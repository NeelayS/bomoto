Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=202, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 11312-11367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467542
Iteration 2/25 | Loss: 0.00120710
Iteration 3/25 | Loss: 0.00109830
Iteration 4/25 | Loss: 0.00108395
Iteration 5/25 | Loss: 0.00108104
Iteration 6/25 | Loss: 0.00108104
Iteration 7/25 | Loss: 0.00108104
Iteration 8/25 | Loss: 0.00108104
Iteration 9/25 | Loss: 0.00108104
Iteration 10/25 | Loss: 0.00108104
Iteration 11/25 | Loss: 0.00108104
Iteration 12/25 | Loss: 0.00108104
Iteration 13/25 | Loss: 0.00108104
Iteration 14/25 | Loss: 0.00108104
Iteration 15/25 | Loss: 0.00108104
Iteration 16/25 | Loss: 0.00108104
Iteration 17/25 | Loss: 0.00108104
Iteration 18/25 | Loss: 0.00108104
Iteration 19/25 | Loss: 0.00108104
Iteration 20/25 | Loss: 0.00108104
Iteration 21/25 | Loss: 0.00108104
Iteration 22/25 | Loss: 0.00108104
Iteration 23/25 | Loss: 0.00108104
Iteration 24/25 | Loss: 0.00108104
Iteration 25/25 | Loss: 0.00108104

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.85169506
Iteration 2/25 | Loss: 0.00067604
Iteration 3/25 | Loss: 0.00067604
Iteration 4/25 | Loss: 0.00067604
Iteration 5/25 | Loss: 0.00067604
Iteration 6/25 | Loss: 0.00067604
Iteration 7/25 | Loss: 0.00067604
Iteration 8/25 | Loss: 0.00067604
Iteration 9/25 | Loss: 0.00067604
Iteration 10/25 | Loss: 0.00067604
Iteration 11/25 | Loss: 0.00067604
Iteration 12/25 | Loss: 0.00067604
Iteration 13/25 | Loss: 0.00067604
Iteration 14/25 | Loss: 0.00067604
Iteration 15/25 | Loss: 0.00067604
Iteration 16/25 | Loss: 0.00067604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000676036230288446, 0.000676036230288446, 0.000676036230288446, 0.000676036230288446, 0.000676036230288446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000676036230288446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067604
Iteration 2/1000 | Loss: 0.00001856
Iteration 3/1000 | Loss: 0.00001502
Iteration 4/1000 | Loss: 0.00001392
Iteration 5/1000 | Loss: 0.00001326
Iteration 6/1000 | Loss: 0.00001284
Iteration 7/1000 | Loss: 0.00001252
Iteration 8/1000 | Loss: 0.00001225
Iteration 9/1000 | Loss: 0.00001219
Iteration 10/1000 | Loss: 0.00001200
Iteration 11/1000 | Loss: 0.00001197
Iteration 12/1000 | Loss: 0.00001195
Iteration 13/1000 | Loss: 0.00001191
Iteration 14/1000 | Loss: 0.00001190
Iteration 15/1000 | Loss: 0.00001190
Iteration 16/1000 | Loss: 0.00001189
Iteration 17/1000 | Loss: 0.00001188
Iteration 18/1000 | Loss: 0.00001185
Iteration 19/1000 | Loss: 0.00001180
Iteration 20/1000 | Loss: 0.00001174
Iteration 21/1000 | Loss: 0.00001172
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001169
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00001158
Iteration 26/1000 | Loss: 0.00001154
Iteration 27/1000 | Loss: 0.00001153
Iteration 28/1000 | Loss: 0.00001152
Iteration 29/1000 | Loss: 0.00001151
Iteration 30/1000 | Loss: 0.00001150
Iteration 31/1000 | Loss: 0.00001150
Iteration 32/1000 | Loss: 0.00001149
Iteration 33/1000 | Loss: 0.00001149
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001148
Iteration 36/1000 | Loss: 0.00001148
Iteration 37/1000 | Loss: 0.00001147
Iteration 38/1000 | Loss: 0.00001147
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001141
Iteration 42/1000 | Loss: 0.00001140
Iteration 43/1000 | Loss: 0.00001138
Iteration 44/1000 | Loss: 0.00001138
Iteration 45/1000 | Loss: 0.00001137
Iteration 46/1000 | Loss: 0.00001137
Iteration 47/1000 | Loss: 0.00001136
Iteration 48/1000 | Loss: 0.00001136
Iteration 49/1000 | Loss: 0.00001136
Iteration 50/1000 | Loss: 0.00001136
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001133
Iteration 56/1000 | Loss: 0.00001133
Iteration 57/1000 | Loss: 0.00001132
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001132
Iteration 60/1000 | Loss: 0.00001132
Iteration 61/1000 | Loss: 0.00001132
Iteration 62/1000 | Loss: 0.00001132
Iteration 63/1000 | Loss: 0.00001132
Iteration 64/1000 | Loss: 0.00001132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [1.1323950275254901e-05, 1.1323950275254901e-05, 1.1323950275254901e-05, 1.1323950275254901e-05, 1.1323950275254901e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1323950275254901e-05

Optimization complete. Final v2v error: 2.886608600616455 mm

Highest mean error: 3.0632681846618652 mm for frame 89

Lowest mean error: 2.7063498497009277 mm for frame 209

Saving results

Total time: 35.85523247718811
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01057766
Iteration 2/25 | Loss: 0.00581623
Iteration 3/25 | Loss: 0.00373737
Iteration 4/25 | Loss: 0.00397482
Iteration 5/25 | Loss: 0.00334523
Iteration 6/25 | Loss: 0.00284478
Iteration 7/25 | Loss: 0.00270160
Iteration 8/25 | Loss: 0.00240472
Iteration 9/25 | Loss: 0.00233955
Iteration 10/25 | Loss: 0.00255328
Iteration 11/25 | Loss: 0.00253812
Iteration 12/25 | Loss: 0.00247096
Iteration 13/25 | Loss: 0.00214615
Iteration 14/25 | Loss: 0.00200467
Iteration 15/25 | Loss: 0.00187463
Iteration 16/25 | Loss: 0.00185550
Iteration 17/25 | Loss: 0.00179011
Iteration 18/25 | Loss: 0.00175965
Iteration 19/25 | Loss: 0.00173298
Iteration 20/25 | Loss: 0.00170918
Iteration 21/25 | Loss: 0.00169941
Iteration 22/25 | Loss: 0.00169221
Iteration 23/25 | Loss: 0.00169116
Iteration 24/25 | Loss: 0.00168132
Iteration 25/25 | Loss: 0.00167075

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.43324625
Iteration 2/25 | Loss: 0.00190048
Iteration 3/25 | Loss: 0.00190048
Iteration 4/25 | Loss: 0.00190048
Iteration 5/25 | Loss: 0.00190048
Iteration 6/25 | Loss: 0.00190048
Iteration 7/25 | Loss: 0.00190048
Iteration 8/25 | Loss: 0.00190048
Iteration 9/25 | Loss: 0.00190048
Iteration 10/25 | Loss: 0.00190048
Iteration 11/25 | Loss: 0.00190047
Iteration 12/25 | Loss: 0.00190047
Iteration 13/25 | Loss: 0.00190047
Iteration 14/25 | Loss: 0.00190047
Iteration 15/25 | Loss: 0.00190047
Iteration 16/25 | Loss: 0.00190047
Iteration 17/25 | Loss: 0.00190047
Iteration 18/25 | Loss: 0.00190047
Iteration 19/25 | Loss: 0.00190047
Iteration 20/25 | Loss: 0.00190047
Iteration 21/25 | Loss: 0.00190047
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0019004743080586195, 0.0019004743080586195, 0.0019004743080586195, 0.0019004743080586195, 0.0019004743080586195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019004743080586195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00190047
Iteration 2/1000 | Loss: 0.00035343
Iteration 3/1000 | Loss: 0.00024968
Iteration 4/1000 | Loss: 0.00020911
Iteration 5/1000 | Loss: 0.00041505
Iteration 6/1000 | Loss: 0.00023042
Iteration 7/1000 | Loss: 0.00013867
Iteration 8/1000 | Loss: 0.00014864
Iteration 9/1000 | Loss: 0.00013884
Iteration 10/1000 | Loss: 0.00024108
Iteration 11/1000 | Loss: 0.00208629
Iteration 12/1000 | Loss: 0.00140603
Iteration 13/1000 | Loss: 0.00198496
Iteration 14/1000 | Loss: 0.00116918
Iteration 15/1000 | Loss: 0.00141960
Iteration 16/1000 | Loss: 0.00024058
Iteration 17/1000 | Loss: 0.00013292
Iteration 18/1000 | Loss: 0.00014867
Iteration 19/1000 | Loss: 0.00013321
Iteration 20/1000 | Loss: 0.00012899
Iteration 21/1000 | Loss: 0.00012517
Iteration 22/1000 | Loss: 0.00013341
Iteration 23/1000 | Loss: 0.00013925
Iteration 24/1000 | Loss: 0.00013933
Iteration 25/1000 | Loss: 0.00014357
Iteration 26/1000 | Loss: 0.00013392
Iteration 27/1000 | Loss: 0.00012044
Iteration 28/1000 | Loss: 0.00012557
Iteration 29/1000 | Loss: 0.00012505
Iteration 30/1000 | Loss: 0.00012954
Iteration 31/1000 | Loss: 0.00013202
Iteration 32/1000 | Loss: 0.00012115
Iteration 33/1000 | Loss: 0.00013726
Iteration 34/1000 | Loss: 0.00011959
Iteration 35/1000 | Loss: 0.00012202
Iteration 36/1000 | Loss: 0.00012691
Iteration 37/1000 | Loss: 0.00013533
Iteration 38/1000 | Loss: 0.00013913
Iteration 39/1000 | Loss: 0.00012215
Iteration 40/1000 | Loss: 0.00012240
Iteration 41/1000 | Loss: 0.00012008
Iteration 42/1000 | Loss: 0.00026969
Iteration 43/1000 | Loss: 0.00038841
Iteration 44/1000 | Loss: 0.00030280
Iteration 45/1000 | Loss: 0.00013952
Iteration 46/1000 | Loss: 0.00014068
Iteration 47/1000 | Loss: 0.00012989
Iteration 48/1000 | Loss: 0.00012743
Iteration 49/1000 | Loss: 0.00013075
Iteration 50/1000 | Loss: 0.00012858
Iteration 51/1000 | Loss: 0.00027841
Iteration 52/1000 | Loss: 0.00029976
Iteration 53/1000 | Loss: 0.00159071
Iteration 54/1000 | Loss: 0.00023093
Iteration 55/1000 | Loss: 0.00013533
Iteration 56/1000 | Loss: 0.00042144
Iteration 57/1000 | Loss: 0.00017206
Iteration 58/1000 | Loss: 0.00014290
Iteration 59/1000 | Loss: 0.00013541
Iteration 60/1000 | Loss: 0.00014548
Iteration 61/1000 | Loss: 0.00032765
Iteration 62/1000 | Loss: 0.00029066
Iteration 63/1000 | Loss: 0.00035247
Iteration 64/1000 | Loss: 0.00028298
Iteration 65/1000 | Loss: 0.00032076
Iteration 66/1000 | Loss: 0.00026453
Iteration 67/1000 | Loss: 0.00023156
Iteration 68/1000 | Loss: 0.00024252
Iteration 69/1000 | Loss: 0.00022721
Iteration 70/1000 | Loss: 0.00011613
Iteration 71/1000 | Loss: 0.00017919
Iteration 72/1000 | Loss: 0.00012929
Iteration 73/1000 | Loss: 0.00011778
Iteration 74/1000 | Loss: 0.00028203
Iteration 75/1000 | Loss: 0.00019298
Iteration 76/1000 | Loss: 0.00014914
Iteration 77/1000 | Loss: 0.00011677
Iteration 78/1000 | Loss: 0.00029766
Iteration 79/1000 | Loss: 0.00019617
Iteration 80/1000 | Loss: 0.00027741
Iteration 81/1000 | Loss: 0.00024788
Iteration 82/1000 | Loss: 0.00013456
Iteration 83/1000 | Loss: 0.00014110
Iteration 84/1000 | Loss: 0.00013935
Iteration 85/1000 | Loss: 0.00013124
Iteration 86/1000 | Loss: 0.00030999
Iteration 87/1000 | Loss: 0.00031926
Iteration 88/1000 | Loss: 0.00012785
Iteration 89/1000 | Loss: 0.00011404
Iteration 90/1000 | Loss: 0.00011520
Iteration 91/1000 | Loss: 0.00011281
Iteration 92/1000 | Loss: 0.00012920
Iteration 93/1000 | Loss: 0.00012671
Iteration 94/1000 | Loss: 0.00013260
Iteration 95/1000 | Loss: 0.00011740
Iteration 96/1000 | Loss: 0.00011248
Iteration 97/1000 | Loss: 0.00012594
Iteration 98/1000 | Loss: 0.00011746
Iteration 99/1000 | Loss: 0.00011220
Iteration 100/1000 | Loss: 0.00084211
Iteration 101/1000 | Loss: 0.00164015
Iteration 102/1000 | Loss: 0.00082464
Iteration 103/1000 | Loss: 0.00024286
Iteration 104/1000 | Loss: 0.00014424
Iteration 105/1000 | Loss: 0.00035123
Iteration 106/1000 | Loss: 0.00034548
Iteration 107/1000 | Loss: 0.00033596
Iteration 108/1000 | Loss: 0.00028135
Iteration 109/1000 | Loss: 0.00027699
Iteration 110/1000 | Loss: 0.00024816
Iteration 111/1000 | Loss: 0.00025980
Iteration 112/1000 | Loss: 0.00024724
Iteration 113/1000 | Loss: 0.00009496
Iteration 114/1000 | Loss: 0.00009419
Iteration 115/1000 | Loss: 0.00009001
Iteration 116/1000 | Loss: 0.00027011
Iteration 117/1000 | Loss: 0.00026011
Iteration 118/1000 | Loss: 0.00010522
Iteration 119/1000 | Loss: 0.00009489
Iteration 120/1000 | Loss: 0.00009693
Iteration 121/1000 | Loss: 0.00029476
Iteration 122/1000 | Loss: 0.00011264
Iteration 123/1000 | Loss: 0.00009277
Iteration 124/1000 | Loss: 0.00009545
Iteration 125/1000 | Loss: 0.00009035
Iteration 126/1000 | Loss: 0.00009202
Iteration 127/1000 | Loss: 0.00009308
Iteration 128/1000 | Loss: 0.00008983
Iteration 129/1000 | Loss: 0.00009105
Iteration 130/1000 | Loss: 0.00008727
Iteration 131/1000 | Loss: 0.00008612
Iteration 132/1000 | Loss: 0.00008561
Iteration 133/1000 | Loss: 0.00008522
Iteration 134/1000 | Loss: 0.00008733
Iteration 135/1000 | Loss: 0.00008724
Iteration 136/1000 | Loss: 0.00008929
Iteration 137/1000 | Loss: 0.00008644
Iteration 138/1000 | Loss: 0.00008951
Iteration 139/1000 | Loss: 0.00009125
Iteration 140/1000 | Loss: 0.00008542
Iteration 141/1000 | Loss: 0.00008948
Iteration 142/1000 | Loss: 0.00008735
Iteration 143/1000 | Loss: 0.00008906
Iteration 144/1000 | Loss: 0.00008520
Iteration 145/1000 | Loss: 0.00008674
Iteration 146/1000 | Loss: 0.00008912
Iteration 147/1000 | Loss: 0.00008524
Iteration 148/1000 | Loss: 0.00008704
Iteration 149/1000 | Loss: 0.00008918
Iteration 150/1000 | Loss: 0.00008463
Iteration 151/1000 | Loss: 0.00008990
Iteration 152/1000 | Loss: 0.00009268
Iteration 153/1000 | Loss: 0.00008435
Iteration 154/1000 | Loss: 0.00008729
Iteration 155/1000 | Loss: 0.00008968
Iteration 156/1000 | Loss: 0.00008452
Iteration 157/1000 | Loss: 0.00008755
Iteration 158/1000 | Loss: 0.00008959
Iteration 159/1000 | Loss: 0.00008431
Iteration 160/1000 | Loss: 0.00008793
Iteration 161/1000 | Loss: 0.00008987
Iteration 162/1000 | Loss: 0.00008418
Iteration 163/1000 | Loss: 0.00008518
Iteration 164/1000 | Loss: 0.00008933
Iteration 165/1000 | Loss: 0.00008552
Iteration 166/1000 | Loss: 0.00027972
Iteration 167/1000 | Loss: 0.00031255
Iteration 168/1000 | Loss: 0.00018764
Iteration 169/1000 | Loss: 0.00018920
Iteration 170/1000 | Loss: 0.00009757
Iteration 171/1000 | Loss: 0.00009167
Iteration 172/1000 | Loss: 0.00008940
Iteration 173/1000 | Loss: 0.00008566
Iteration 174/1000 | Loss: 0.00008506
Iteration 175/1000 | Loss: 0.00009058
Iteration 176/1000 | Loss: 0.00008460
Iteration 177/1000 | Loss: 0.00008407
Iteration 178/1000 | Loss: 0.00008398
Iteration 179/1000 | Loss: 0.00008360
Iteration 180/1000 | Loss: 0.00008336
Iteration 181/1000 | Loss: 0.00008320
Iteration 182/1000 | Loss: 0.00025138
Iteration 183/1000 | Loss: 0.00039000
Iteration 184/1000 | Loss: 0.00017177
Iteration 185/1000 | Loss: 0.00021355
Iteration 186/1000 | Loss: 0.00010110
Iteration 187/1000 | Loss: 0.00022040
Iteration 188/1000 | Loss: 0.00045356
Iteration 189/1000 | Loss: 0.00026278
Iteration 190/1000 | Loss: 0.00045805
Iteration 191/1000 | Loss: 0.00018733
Iteration 192/1000 | Loss: 0.00010251
Iteration 193/1000 | Loss: 0.00009302
Iteration 194/1000 | Loss: 0.00008922
Iteration 195/1000 | Loss: 0.00008651
Iteration 196/1000 | Loss: 0.00011066
Iteration 197/1000 | Loss: 0.00008504
Iteration 198/1000 | Loss: 0.00008450
Iteration 199/1000 | Loss: 0.00008414
Iteration 200/1000 | Loss: 0.00010472
Iteration 201/1000 | Loss: 0.00008376
Iteration 202/1000 | Loss: 0.00008354
Iteration 203/1000 | Loss: 0.00010131
Iteration 204/1000 | Loss: 0.00008327
Iteration 205/1000 | Loss: 0.00008309
Iteration 206/1000 | Loss: 0.00008279
Iteration 207/1000 | Loss: 0.00009710
Iteration 208/1000 | Loss: 0.00008253
Iteration 209/1000 | Loss: 0.00008227
Iteration 210/1000 | Loss: 0.00008218
Iteration 211/1000 | Loss: 0.00008199
Iteration 212/1000 | Loss: 0.00008191
Iteration 213/1000 | Loss: 0.00008190
Iteration 214/1000 | Loss: 0.00008190
Iteration 215/1000 | Loss: 0.00008190
Iteration 216/1000 | Loss: 0.00008189
Iteration 217/1000 | Loss: 0.00008189
Iteration 218/1000 | Loss: 0.00008189
Iteration 219/1000 | Loss: 0.00008189
Iteration 220/1000 | Loss: 0.00008189
Iteration 221/1000 | Loss: 0.00008189
Iteration 222/1000 | Loss: 0.00008189
Iteration 223/1000 | Loss: 0.00008189
Iteration 224/1000 | Loss: 0.00008189
Iteration 225/1000 | Loss: 0.00008189
Iteration 226/1000 | Loss: 0.00008189
Iteration 227/1000 | Loss: 0.00008189
Iteration 228/1000 | Loss: 0.00008189
Iteration 229/1000 | Loss: 0.00008188
Iteration 230/1000 | Loss: 0.00008188
Iteration 231/1000 | Loss: 0.00008188
Iteration 232/1000 | Loss: 0.00008187
Iteration 233/1000 | Loss: 0.00008187
Iteration 234/1000 | Loss: 0.00008187
Iteration 235/1000 | Loss: 0.00008187
Iteration 236/1000 | Loss: 0.00008186
Iteration 237/1000 | Loss: 0.00008189
Iteration 238/1000 | Loss: 0.00008189
Iteration 239/1000 | Loss: 0.00008189
Iteration 240/1000 | Loss: 0.00008189
Iteration 241/1000 | Loss: 0.00008189
Iteration 242/1000 | Loss: 0.00008189
Iteration 243/1000 | Loss: 0.00008189
Iteration 244/1000 | Loss: 0.00008189
Iteration 245/1000 | Loss: 0.00008189
Iteration 246/1000 | Loss: 0.00008187
Iteration 247/1000 | Loss: 0.00008187
Iteration 248/1000 | Loss: 0.00008187
Iteration 249/1000 | Loss: 0.00008187
Iteration 250/1000 | Loss: 0.00008187
Iteration 251/1000 | Loss: 0.00008187
Iteration 252/1000 | Loss: 0.00008187
Iteration 253/1000 | Loss: 0.00008187
Iteration 254/1000 | Loss: 0.00008187
Iteration 255/1000 | Loss: 0.00008187
Iteration 256/1000 | Loss: 0.00008187
Iteration 257/1000 | Loss: 0.00008187
Iteration 258/1000 | Loss: 0.00008187
Iteration 259/1000 | Loss: 0.00008187
Iteration 260/1000 | Loss: 0.00008186
Iteration 261/1000 | Loss: 0.00008186
Iteration 262/1000 | Loss: 0.00008186
Iteration 263/1000 | Loss: 0.00008185
Iteration 264/1000 | Loss: 0.00008185
Iteration 265/1000 | Loss: 0.00008185
Iteration 266/1000 | Loss: 0.00008185
Iteration 267/1000 | Loss: 0.00008185
Iteration 268/1000 | Loss: 0.00008185
Iteration 269/1000 | Loss: 0.00008184
Iteration 270/1000 | Loss: 0.00008184
Iteration 271/1000 | Loss: 0.00008184
Iteration 272/1000 | Loss: 0.00008184
Iteration 273/1000 | Loss: 0.00008183
Iteration 274/1000 | Loss: 0.00008183
Iteration 275/1000 | Loss: 0.00008183
Iteration 276/1000 | Loss: 0.00008183
Iteration 277/1000 | Loss: 0.00008183
Iteration 278/1000 | Loss: 0.00008182
Iteration 279/1000 | Loss: 0.00008182
Iteration 280/1000 | Loss: 0.00008182
Iteration 281/1000 | Loss: 0.00008181
Iteration 282/1000 | Loss: 0.00008180
Iteration 283/1000 | Loss: 0.00008179
Iteration 284/1000 | Loss: 0.00008187
Iteration 285/1000 | Loss: 0.00008187
Iteration 286/1000 | Loss: 0.00008187
Iteration 287/1000 | Loss: 0.00009466
Iteration 288/1000 | Loss: 0.00008180
Iteration 289/1000 | Loss: 0.00008179
Iteration 290/1000 | Loss: 0.00008179
Iteration 291/1000 | Loss: 0.00008175
Iteration 292/1000 | Loss: 0.00008175
Iteration 293/1000 | Loss: 0.00008175
Iteration 294/1000 | Loss: 0.00008175
Iteration 295/1000 | Loss: 0.00008175
Iteration 296/1000 | Loss: 0.00008175
Iteration 297/1000 | Loss: 0.00008175
Iteration 298/1000 | Loss: 0.00008175
Iteration 299/1000 | Loss: 0.00008175
Iteration 300/1000 | Loss: 0.00008175
Iteration 301/1000 | Loss: 0.00008174
Iteration 302/1000 | Loss: 0.00008174
Iteration 303/1000 | Loss: 0.00008174
Iteration 304/1000 | Loss: 0.00008174
Iteration 305/1000 | Loss: 0.00008174
Iteration 306/1000 | Loss: 0.00008174
Iteration 307/1000 | Loss: 0.00008174
Iteration 308/1000 | Loss: 0.00008174
Iteration 309/1000 | Loss: 0.00008174
Iteration 310/1000 | Loss: 0.00008174
Iteration 311/1000 | Loss: 0.00008174
Iteration 312/1000 | Loss: 0.00008172
Iteration 313/1000 | Loss: 0.00008172
Iteration 314/1000 | Loss: 0.00008172
Iteration 315/1000 | Loss: 0.00008172
Iteration 316/1000 | Loss: 0.00008171
Iteration 317/1000 | Loss: 0.00008171
Iteration 318/1000 | Loss: 0.00008171
Iteration 319/1000 | Loss: 0.00008171
Iteration 320/1000 | Loss: 0.00008171
Iteration 321/1000 | Loss: 0.00008171
Iteration 322/1000 | Loss: 0.00008171
Iteration 323/1000 | Loss: 0.00008171
Iteration 324/1000 | Loss: 0.00008171
Iteration 325/1000 | Loss: 0.00008171
Iteration 326/1000 | Loss: 0.00008170
Iteration 327/1000 | Loss: 0.00008170
Iteration 328/1000 | Loss: 0.00008170
Iteration 329/1000 | Loss: 0.00008170
Iteration 330/1000 | Loss: 0.00008170
Iteration 331/1000 | Loss: 0.00008170
Iteration 332/1000 | Loss: 0.00008170
Iteration 333/1000 | Loss: 0.00008170
Iteration 334/1000 | Loss: 0.00008170
Iteration 335/1000 | Loss: 0.00008170
Iteration 336/1000 | Loss: 0.00008170
Iteration 337/1000 | Loss: 0.00008170
Iteration 338/1000 | Loss: 0.00008170
Iteration 339/1000 | Loss: 0.00008170
Iteration 340/1000 | Loss: 0.00008170
Iteration 341/1000 | Loss: 0.00008170
Iteration 342/1000 | Loss: 0.00008170
Iteration 343/1000 | Loss: 0.00008170
Iteration 344/1000 | Loss: 0.00008170
Iteration 345/1000 | Loss: 0.00008170
Iteration 346/1000 | Loss: 0.00008170
Iteration 347/1000 | Loss: 0.00008170
Iteration 348/1000 | Loss: 0.00008170
Iteration 349/1000 | Loss: 0.00008170
Iteration 350/1000 | Loss: 0.00008170
Iteration 351/1000 | Loss: 0.00008170
Iteration 352/1000 | Loss: 0.00008170
Iteration 353/1000 | Loss: 0.00008170
Iteration 354/1000 | Loss: 0.00008170
Iteration 355/1000 | Loss: 0.00008170
Iteration 356/1000 | Loss: 0.00008170
Iteration 357/1000 | Loss: 0.00008170
Iteration 358/1000 | Loss: 0.00008170
Iteration 359/1000 | Loss: 0.00008170
Iteration 360/1000 | Loss: 0.00008170
Iteration 361/1000 | Loss: 0.00008170
Iteration 362/1000 | Loss: 0.00008170
Iteration 363/1000 | Loss: 0.00008170
Iteration 364/1000 | Loss: 0.00008170
Iteration 365/1000 | Loss: 0.00008170
Iteration 366/1000 | Loss: 0.00008170
Iteration 367/1000 | Loss: 0.00008170
Iteration 368/1000 | Loss: 0.00008170
Iteration 369/1000 | Loss: 0.00008170
Iteration 370/1000 | Loss: 0.00008170
Iteration 371/1000 | Loss: 0.00008170
Iteration 372/1000 | Loss: 0.00008170
Iteration 373/1000 | Loss: 0.00008170
Iteration 374/1000 | Loss: 0.00008170
Iteration 375/1000 | Loss: 0.00008170
Iteration 376/1000 | Loss: 0.00008170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 376. Stopping optimization.
Last 5 losses: [8.169728243956342e-05, 8.169728243956342e-05, 8.169728243956342e-05, 8.169728243956342e-05, 8.169728243956342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.169728243956342e-05

Optimization complete. Final v2v error: 6.25045108795166 mm

Highest mean error: 11.860552787780762 mm for frame 46

Lowest mean error: 4.190723896026611 mm for frame 11

Saving results

Total time: 391.46299600601196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024161
Iteration 2/25 | Loss: 0.00159071
Iteration 3/25 | Loss: 0.00128902
Iteration 4/25 | Loss: 0.00125959
Iteration 5/25 | Loss: 0.00125048
Iteration 6/25 | Loss: 0.00124809
Iteration 7/25 | Loss: 0.00124809
Iteration 8/25 | Loss: 0.00124809
Iteration 9/25 | Loss: 0.00124809
Iteration 10/25 | Loss: 0.00124809
Iteration 11/25 | Loss: 0.00124809
Iteration 12/25 | Loss: 0.00124809
Iteration 13/25 | Loss: 0.00124809
Iteration 14/25 | Loss: 0.00124809
Iteration 15/25 | Loss: 0.00124809
Iteration 16/25 | Loss: 0.00124809
Iteration 17/25 | Loss: 0.00124809
Iteration 18/25 | Loss: 0.00124809
Iteration 19/25 | Loss: 0.00124809
Iteration 20/25 | Loss: 0.00124809
Iteration 21/25 | Loss: 0.00124809
Iteration 22/25 | Loss: 0.00124809
Iteration 23/25 | Loss: 0.00124809
Iteration 24/25 | Loss: 0.00124809
Iteration 25/25 | Loss: 0.00124809
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012480915756896138, 0.0012480915756896138, 0.0012480915756896138, 0.0012480915756896138, 0.0012480915756896138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012480915756896138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95649076
Iteration 2/25 | Loss: 0.00102712
Iteration 3/25 | Loss: 0.00102709
Iteration 4/25 | Loss: 0.00102709
Iteration 5/25 | Loss: 0.00102709
Iteration 6/25 | Loss: 0.00102709
Iteration 7/25 | Loss: 0.00102709
Iteration 8/25 | Loss: 0.00102709
Iteration 9/25 | Loss: 0.00102709
Iteration 10/25 | Loss: 0.00102709
Iteration 11/25 | Loss: 0.00102709
Iteration 12/25 | Loss: 0.00102709
Iteration 13/25 | Loss: 0.00102709
Iteration 14/25 | Loss: 0.00102709
Iteration 15/25 | Loss: 0.00102709
Iteration 16/25 | Loss: 0.00102709
Iteration 17/25 | Loss: 0.00102709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010270900093019009, 0.0010270900093019009, 0.0010270900093019009, 0.0010270900093019009, 0.0010270900093019009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010270900093019009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102709
Iteration 2/1000 | Loss: 0.00008601
Iteration 3/1000 | Loss: 0.00004916
Iteration 4/1000 | Loss: 0.00003520
Iteration 5/1000 | Loss: 0.00003245
Iteration 6/1000 | Loss: 0.00003078
Iteration 7/1000 | Loss: 0.00002996
Iteration 8/1000 | Loss: 0.00002928
Iteration 9/1000 | Loss: 0.00002875
Iteration 10/1000 | Loss: 0.00002839
Iteration 11/1000 | Loss: 0.00002803
Iteration 12/1000 | Loss: 0.00002781
Iteration 13/1000 | Loss: 0.00002761
Iteration 14/1000 | Loss: 0.00002742
Iteration 15/1000 | Loss: 0.00002731
Iteration 16/1000 | Loss: 0.00002729
Iteration 17/1000 | Loss: 0.00002729
Iteration 18/1000 | Loss: 0.00002728
Iteration 19/1000 | Loss: 0.00002728
Iteration 20/1000 | Loss: 0.00002727
Iteration 21/1000 | Loss: 0.00002727
Iteration 22/1000 | Loss: 0.00002726
Iteration 23/1000 | Loss: 0.00002726
Iteration 24/1000 | Loss: 0.00002725
Iteration 25/1000 | Loss: 0.00002725
Iteration 26/1000 | Loss: 0.00002724
Iteration 27/1000 | Loss: 0.00002722
Iteration 28/1000 | Loss: 0.00002721
Iteration 29/1000 | Loss: 0.00002717
Iteration 30/1000 | Loss: 0.00002709
Iteration 31/1000 | Loss: 0.00002707
Iteration 32/1000 | Loss: 0.00002707
Iteration 33/1000 | Loss: 0.00002706
Iteration 34/1000 | Loss: 0.00002705
Iteration 35/1000 | Loss: 0.00002701
Iteration 36/1000 | Loss: 0.00002701
Iteration 37/1000 | Loss: 0.00002700
Iteration 38/1000 | Loss: 0.00002698
Iteration 39/1000 | Loss: 0.00002697
Iteration 40/1000 | Loss: 0.00002697
Iteration 41/1000 | Loss: 0.00002697
Iteration 42/1000 | Loss: 0.00002696
Iteration 43/1000 | Loss: 0.00002696
Iteration 44/1000 | Loss: 0.00002696
Iteration 45/1000 | Loss: 0.00002695
Iteration 46/1000 | Loss: 0.00002694
Iteration 47/1000 | Loss: 0.00002693
Iteration 48/1000 | Loss: 0.00002693
Iteration 49/1000 | Loss: 0.00002693
Iteration 50/1000 | Loss: 0.00002693
Iteration 51/1000 | Loss: 0.00002693
Iteration 52/1000 | Loss: 0.00002690
Iteration 53/1000 | Loss: 0.00002690
Iteration 54/1000 | Loss: 0.00002689
Iteration 55/1000 | Loss: 0.00002689
Iteration 56/1000 | Loss: 0.00002687
Iteration 57/1000 | Loss: 0.00002687
Iteration 58/1000 | Loss: 0.00002687
Iteration 59/1000 | Loss: 0.00002687
Iteration 60/1000 | Loss: 0.00002687
Iteration 61/1000 | Loss: 0.00002687
Iteration 62/1000 | Loss: 0.00002687
Iteration 63/1000 | Loss: 0.00002687
Iteration 64/1000 | Loss: 0.00002687
Iteration 65/1000 | Loss: 0.00002687
Iteration 66/1000 | Loss: 0.00002686
Iteration 67/1000 | Loss: 0.00002686
Iteration 68/1000 | Loss: 0.00002686
Iteration 69/1000 | Loss: 0.00002686
Iteration 70/1000 | Loss: 0.00002686
Iteration 71/1000 | Loss: 0.00002686
Iteration 72/1000 | Loss: 0.00002686
Iteration 73/1000 | Loss: 0.00002686
Iteration 74/1000 | Loss: 0.00002686
Iteration 75/1000 | Loss: 0.00002686
Iteration 76/1000 | Loss: 0.00002686
Iteration 77/1000 | Loss: 0.00002685
Iteration 78/1000 | Loss: 0.00002685
Iteration 79/1000 | Loss: 0.00002684
Iteration 80/1000 | Loss: 0.00002684
Iteration 81/1000 | Loss: 0.00002684
Iteration 82/1000 | Loss: 0.00002683
Iteration 83/1000 | Loss: 0.00002683
Iteration 84/1000 | Loss: 0.00002683
Iteration 85/1000 | Loss: 0.00002683
Iteration 86/1000 | Loss: 0.00002683
Iteration 87/1000 | Loss: 0.00002683
Iteration 88/1000 | Loss: 0.00002682
Iteration 89/1000 | Loss: 0.00002682
Iteration 90/1000 | Loss: 0.00002682
Iteration 91/1000 | Loss: 0.00002681
Iteration 92/1000 | Loss: 0.00002681
Iteration 93/1000 | Loss: 0.00002681
Iteration 94/1000 | Loss: 0.00002681
Iteration 95/1000 | Loss: 0.00002680
Iteration 96/1000 | Loss: 0.00002680
Iteration 97/1000 | Loss: 0.00002680
Iteration 98/1000 | Loss: 0.00002679
Iteration 99/1000 | Loss: 0.00002679
Iteration 100/1000 | Loss: 0.00002678
Iteration 101/1000 | Loss: 0.00002678
Iteration 102/1000 | Loss: 0.00002678
Iteration 103/1000 | Loss: 0.00002678
Iteration 104/1000 | Loss: 0.00002677
Iteration 105/1000 | Loss: 0.00002677
Iteration 106/1000 | Loss: 0.00002677
Iteration 107/1000 | Loss: 0.00002677
Iteration 108/1000 | Loss: 0.00002676
Iteration 109/1000 | Loss: 0.00002676
Iteration 110/1000 | Loss: 0.00002675
Iteration 111/1000 | Loss: 0.00002675
Iteration 112/1000 | Loss: 0.00002675
Iteration 113/1000 | Loss: 0.00002675
Iteration 114/1000 | Loss: 0.00002675
Iteration 115/1000 | Loss: 0.00002675
Iteration 116/1000 | Loss: 0.00002675
Iteration 117/1000 | Loss: 0.00002675
Iteration 118/1000 | Loss: 0.00002675
Iteration 119/1000 | Loss: 0.00002675
Iteration 120/1000 | Loss: 0.00002675
Iteration 121/1000 | Loss: 0.00002675
Iteration 122/1000 | Loss: 0.00002675
Iteration 123/1000 | Loss: 0.00002675
Iteration 124/1000 | Loss: 0.00002675
Iteration 125/1000 | Loss: 0.00002674
Iteration 126/1000 | Loss: 0.00002674
Iteration 127/1000 | Loss: 0.00002674
Iteration 128/1000 | Loss: 0.00002674
Iteration 129/1000 | Loss: 0.00002674
Iteration 130/1000 | Loss: 0.00002673
Iteration 131/1000 | Loss: 0.00002673
Iteration 132/1000 | Loss: 0.00002673
Iteration 133/1000 | Loss: 0.00002673
Iteration 134/1000 | Loss: 0.00002673
Iteration 135/1000 | Loss: 0.00002672
Iteration 136/1000 | Loss: 0.00002672
Iteration 137/1000 | Loss: 0.00002672
Iteration 138/1000 | Loss: 0.00002672
Iteration 139/1000 | Loss: 0.00002672
Iteration 140/1000 | Loss: 0.00002672
Iteration 141/1000 | Loss: 0.00002672
Iteration 142/1000 | Loss: 0.00002672
Iteration 143/1000 | Loss: 0.00002672
Iteration 144/1000 | Loss: 0.00002672
Iteration 145/1000 | Loss: 0.00002672
Iteration 146/1000 | Loss: 0.00002672
Iteration 147/1000 | Loss: 0.00002672
Iteration 148/1000 | Loss: 0.00002672
Iteration 149/1000 | Loss: 0.00002672
Iteration 150/1000 | Loss: 0.00002672
Iteration 151/1000 | Loss: 0.00002672
Iteration 152/1000 | Loss: 0.00002672
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.6721931135398336e-05, 2.6721931135398336e-05, 2.6721931135398336e-05, 2.6721931135398336e-05, 2.6721931135398336e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6721931135398336e-05

Optimization complete. Final v2v error: 4.181982517242432 mm

Highest mean error: 5.111505031585693 mm for frame 52

Lowest mean error: 3.403355836868286 mm for frame 27

Saving results

Total time: 45.28118634223938
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00438715
Iteration 2/25 | Loss: 0.00127783
Iteration 3/25 | Loss: 0.00111208
Iteration 4/25 | Loss: 0.00108951
Iteration 5/25 | Loss: 0.00108431
Iteration 6/25 | Loss: 0.00108245
Iteration 7/25 | Loss: 0.00108245
Iteration 8/25 | Loss: 0.00108245
Iteration 9/25 | Loss: 0.00108245
Iteration 10/25 | Loss: 0.00108245
Iteration 11/25 | Loss: 0.00108245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010824488708749413, 0.0010824488708749413, 0.0010824488708749413, 0.0010824488708749413, 0.0010824488708749413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010824488708749413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37684500
Iteration 2/25 | Loss: 0.00075586
Iteration 3/25 | Loss: 0.00075585
Iteration 4/25 | Loss: 0.00075585
Iteration 5/25 | Loss: 0.00075585
Iteration 6/25 | Loss: 0.00075585
Iteration 7/25 | Loss: 0.00075585
Iteration 8/25 | Loss: 0.00075585
Iteration 9/25 | Loss: 0.00075585
Iteration 10/25 | Loss: 0.00075585
Iteration 11/25 | Loss: 0.00075585
Iteration 12/25 | Loss: 0.00075585
Iteration 13/25 | Loss: 0.00075585
Iteration 14/25 | Loss: 0.00075585
Iteration 15/25 | Loss: 0.00075585
Iteration 16/25 | Loss: 0.00075585
Iteration 17/25 | Loss: 0.00075585
Iteration 18/25 | Loss: 0.00075585
Iteration 19/25 | Loss: 0.00075585
Iteration 20/25 | Loss: 0.00075585
Iteration 21/25 | Loss: 0.00075585
Iteration 22/25 | Loss: 0.00075585
Iteration 23/25 | Loss: 0.00075585
Iteration 24/25 | Loss: 0.00075585
Iteration 25/25 | Loss: 0.00075585
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007558465586043894, 0.0007558465586043894, 0.0007558465586043894, 0.0007558465586043894, 0.0007558465586043894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007558465586043894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075585
Iteration 2/1000 | Loss: 0.00003112
Iteration 3/1000 | Loss: 0.00001878
Iteration 4/1000 | Loss: 0.00001539
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001373
Iteration 7/1000 | Loss: 0.00001317
Iteration 8/1000 | Loss: 0.00001284
Iteration 9/1000 | Loss: 0.00001267
Iteration 10/1000 | Loss: 0.00001260
Iteration 11/1000 | Loss: 0.00001244
Iteration 12/1000 | Loss: 0.00001233
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001219
Iteration 17/1000 | Loss: 0.00001218
Iteration 18/1000 | Loss: 0.00001217
Iteration 19/1000 | Loss: 0.00001213
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001212
Iteration 22/1000 | Loss: 0.00001211
Iteration 23/1000 | Loss: 0.00001211
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001209
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001207
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001206
Iteration 32/1000 | Loss: 0.00001206
Iteration 33/1000 | Loss: 0.00001205
Iteration 34/1000 | Loss: 0.00001205
Iteration 35/1000 | Loss: 0.00001205
Iteration 36/1000 | Loss: 0.00001205
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001204
Iteration 39/1000 | Loss: 0.00001204
Iteration 40/1000 | Loss: 0.00001202
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001201
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001201
Iteration 45/1000 | Loss: 0.00001201
Iteration 46/1000 | Loss: 0.00001201
Iteration 47/1000 | Loss: 0.00001201
Iteration 48/1000 | Loss: 0.00001201
Iteration 49/1000 | Loss: 0.00001201
Iteration 50/1000 | Loss: 0.00001201
Iteration 51/1000 | Loss: 0.00001201
Iteration 52/1000 | Loss: 0.00001201
Iteration 53/1000 | Loss: 0.00001200
Iteration 54/1000 | Loss: 0.00001200
Iteration 55/1000 | Loss: 0.00001200
Iteration 56/1000 | Loss: 0.00001200
Iteration 57/1000 | Loss: 0.00001200
Iteration 58/1000 | Loss: 0.00001199
Iteration 59/1000 | Loss: 0.00001199
Iteration 60/1000 | Loss: 0.00001198
Iteration 61/1000 | Loss: 0.00001198
Iteration 62/1000 | Loss: 0.00001197
Iteration 63/1000 | Loss: 0.00001197
Iteration 64/1000 | Loss: 0.00001197
Iteration 65/1000 | Loss: 0.00001197
Iteration 66/1000 | Loss: 0.00001196
Iteration 67/1000 | Loss: 0.00001196
Iteration 68/1000 | Loss: 0.00001196
Iteration 69/1000 | Loss: 0.00001196
Iteration 70/1000 | Loss: 0.00001196
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001195
Iteration 73/1000 | Loss: 0.00001195
Iteration 74/1000 | Loss: 0.00001194
Iteration 75/1000 | Loss: 0.00001194
Iteration 76/1000 | Loss: 0.00001194
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001192
Iteration 85/1000 | Loss: 0.00001192
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001191
Iteration 88/1000 | Loss: 0.00001191
Iteration 89/1000 | Loss: 0.00001190
Iteration 90/1000 | Loss: 0.00001190
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001189
Iteration 98/1000 | Loss: 0.00001189
Iteration 99/1000 | Loss: 0.00001189
Iteration 100/1000 | Loss: 0.00001189
Iteration 101/1000 | Loss: 0.00001189
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001188
Iteration 104/1000 | Loss: 0.00001188
Iteration 105/1000 | Loss: 0.00001188
Iteration 106/1000 | Loss: 0.00001188
Iteration 107/1000 | Loss: 0.00001188
Iteration 108/1000 | Loss: 0.00001188
Iteration 109/1000 | Loss: 0.00001188
Iteration 110/1000 | Loss: 0.00001187
Iteration 111/1000 | Loss: 0.00001187
Iteration 112/1000 | Loss: 0.00001187
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001186
Iteration 116/1000 | Loss: 0.00001186
Iteration 117/1000 | Loss: 0.00001186
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001185
Iteration 120/1000 | Loss: 0.00001185
Iteration 121/1000 | Loss: 0.00001185
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001184
Iteration 126/1000 | Loss: 0.00001184
Iteration 127/1000 | Loss: 0.00001184
Iteration 128/1000 | Loss: 0.00001184
Iteration 129/1000 | Loss: 0.00001184
Iteration 130/1000 | Loss: 0.00001184
Iteration 131/1000 | Loss: 0.00001184
Iteration 132/1000 | Loss: 0.00001184
Iteration 133/1000 | Loss: 0.00001183
Iteration 134/1000 | Loss: 0.00001183
Iteration 135/1000 | Loss: 0.00001183
Iteration 136/1000 | Loss: 0.00001183
Iteration 137/1000 | Loss: 0.00001183
Iteration 138/1000 | Loss: 0.00001183
Iteration 139/1000 | Loss: 0.00001183
Iteration 140/1000 | Loss: 0.00001182
Iteration 141/1000 | Loss: 0.00001182
Iteration 142/1000 | Loss: 0.00001182
Iteration 143/1000 | Loss: 0.00001181
Iteration 144/1000 | Loss: 0.00001181
Iteration 145/1000 | Loss: 0.00001181
Iteration 146/1000 | Loss: 0.00001180
Iteration 147/1000 | Loss: 0.00001180
Iteration 148/1000 | Loss: 0.00001179
Iteration 149/1000 | Loss: 0.00001179
Iteration 150/1000 | Loss: 0.00001179
Iteration 151/1000 | Loss: 0.00001179
Iteration 152/1000 | Loss: 0.00001179
Iteration 153/1000 | Loss: 0.00001178
Iteration 154/1000 | Loss: 0.00001178
Iteration 155/1000 | Loss: 0.00001178
Iteration 156/1000 | Loss: 0.00001177
Iteration 157/1000 | Loss: 0.00001177
Iteration 158/1000 | Loss: 0.00001177
Iteration 159/1000 | Loss: 0.00001177
Iteration 160/1000 | Loss: 0.00001177
Iteration 161/1000 | Loss: 0.00001177
Iteration 162/1000 | Loss: 0.00001177
Iteration 163/1000 | Loss: 0.00001177
Iteration 164/1000 | Loss: 0.00001177
Iteration 165/1000 | Loss: 0.00001177
Iteration 166/1000 | Loss: 0.00001177
Iteration 167/1000 | Loss: 0.00001177
Iteration 168/1000 | Loss: 0.00001177
Iteration 169/1000 | Loss: 0.00001176
Iteration 170/1000 | Loss: 0.00001176
Iteration 171/1000 | Loss: 0.00001176
Iteration 172/1000 | Loss: 0.00001176
Iteration 173/1000 | Loss: 0.00001176
Iteration 174/1000 | Loss: 0.00001176
Iteration 175/1000 | Loss: 0.00001176
Iteration 176/1000 | Loss: 0.00001176
Iteration 177/1000 | Loss: 0.00001176
Iteration 178/1000 | Loss: 0.00001176
Iteration 179/1000 | Loss: 0.00001176
Iteration 180/1000 | Loss: 0.00001176
Iteration 181/1000 | Loss: 0.00001176
Iteration 182/1000 | Loss: 0.00001176
Iteration 183/1000 | Loss: 0.00001176
Iteration 184/1000 | Loss: 0.00001176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.1756806088669691e-05, 1.1756806088669691e-05, 1.1756806088669691e-05, 1.1756806088669691e-05, 1.1756806088669691e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1756806088669691e-05

Optimization complete. Final v2v error: 2.925091505050659 mm

Highest mean error: 3.779144287109375 mm for frame 79

Lowest mean error: 2.5208897590637207 mm for frame 56

Saving results

Total time: 43.571401596069336
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810634
Iteration 2/25 | Loss: 0.00150910
Iteration 3/25 | Loss: 0.00121690
Iteration 4/25 | Loss: 0.00119009
Iteration 5/25 | Loss: 0.00118580
Iteration 6/25 | Loss: 0.00118487
Iteration 7/25 | Loss: 0.00118487
Iteration 8/25 | Loss: 0.00118487
Iteration 9/25 | Loss: 0.00118487
Iteration 10/25 | Loss: 0.00118487
Iteration 11/25 | Loss: 0.00118487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001184868160635233, 0.001184868160635233, 0.001184868160635233, 0.001184868160635233, 0.001184868160635233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001184868160635233

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37605667
Iteration 2/25 | Loss: 0.00033704
Iteration 3/25 | Loss: 0.00033704
Iteration 4/25 | Loss: 0.00033704
Iteration 5/25 | Loss: 0.00033704
Iteration 6/25 | Loss: 0.00033704
Iteration 7/25 | Loss: 0.00033704
Iteration 8/25 | Loss: 0.00033704
Iteration 9/25 | Loss: 0.00033704
Iteration 10/25 | Loss: 0.00033704
Iteration 11/25 | Loss: 0.00033704
Iteration 12/25 | Loss: 0.00033704
Iteration 13/25 | Loss: 0.00033704
Iteration 14/25 | Loss: 0.00033704
Iteration 15/25 | Loss: 0.00033704
Iteration 16/25 | Loss: 0.00033704
Iteration 17/25 | Loss: 0.00033704
Iteration 18/25 | Loss: 0.00033704
Iteration 19/25 | Loss: 0.00033704
Iteration 20/25 | Loss: 0.00033704
Iteration 21/25 | Loss: 0.00033704
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00033704284578561783, 0.00033704284578561783, 0.00033704284578561783, 0.00033704284578561783, 0.00033704284578561783]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00033704284578561783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033704
Iteration 2/1000 | Loss: 0.00005614
Iteration 3/1000 | Loss: 0.00004241
Iteration 4/1000 | Loss: 0.00004011
Iteration 5/1000 | Loss: 0.00003874
Iteration 6/1000 | Loss: 0.00003745
Iteration 7/1000 | Loss: 0.00003681
Iteration 8/1000 | Loss: 0.00003622
Iteration 9/1000 | Loss: 0.00003575
Iteration 10/1000 | Loss: 0.00003544
Iteration 11/1000 | Loss: 0.00003530
Iteration 12/1000 | Loss: 0.00003509
Iteration 13/1000 | Loss: 0.00003505
Iteration 14/1000 | Loss: 0.00003503
Iteration 15/1000 | Loss: 0.00003502
Iteration 16/1000 | Loss: 0.00003500
Iteration 17/1000 | Loss: 0.00003499
Iteration 18/1000 | Loss: 0.00003498
Iteration 19/1000 | Loss: 0.00003497
Iteration 20/1000 | Loss: 0.00003492
Iteration 21/1000 | Loss: 0.00003486
Iteration 22/1000 | Loss: 0.00003486
Iteration 23/1000 | Loss: 0.00003486
Iteration 24/1000 | Loss: 0.00003486
Iteration 25/1000 | Loss: 0.00003486
Iteration 26/1000 | Loss: 0.00003486
Iteration 27/1000 | Loss: 0.00003485
Iteration 28/1000 | Loss: 0.00003485
Iteration 29/1000 | Loss: 0.00003485
Iteration 30/1000 | Loss: 0.00003485
Iteration 31/1000 | Loss: 0.00003485
Iteration 32/1000 | Loss: 0.00003483
Iteration 33/1000 | Loss: 0.00003483
Iteration 34/1000 | Loss: 0.00003482
Iteration 35/1000 | Loss: 0.00003482
Iteration 36/1000 | Loss: 0.00003481
Iteration 37/1000 | Loss: 0.00003481
Iteration 38/1000 | Loss: 0.00003481
Iteration 39/1000 | Loss: 0.00003481
Iteration 40/1000 | Loss: 0.00003481
Iteration 41/1000 | Loss: 0.00003481
Iteration 42/1000 | Loss: 0.00003481
Iteration 43/1000 | Loss: 0.00003481
Iteration 44/1000 | Loss: 0.00003481
Iteration 45/1000 | Loss: 0.00003480
Iteration 46/1000 | Loss: 0.00003480
Iteration 47/1000 | Loss: 0.00003480
Iteration 48/1000 | Loss: 0.00003480
Iteration 49/1000 | Loss: 0.00003480
Iteration 50/1000 | Loss: 0.00003480
Iteration 51/1000 | Loss: 0.00003480
Iteration 52/1000 | Loss: 0.00003480
Iteration 53/1000 | Loss: 0.00003480
Iteration 54/1000 | Loss: 0.00003480
Iteration 55/1000 | Loss: 0.00003480
Iteration 56/1000 | Loss: 0.00003480
Iteration 57/1000 | Loss: 0.00003480
Iteration 58/1000 | Loss: 0.00003480
Iteration 59/1000 | Loss: 0.00003480
Iteration 60/1000 | Loss: 0.00003480
Iteration 61/1000 | Loss: 0.00003480
Iteration 62/1000 | Loss: 0.00003480
Iteration 63/1000 | Loss: 0.00003480
Iteration 64/1000 | Loss: 0.00003480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 64. Stopping optimization.
Last 5 losses: [3.480122177279554e-05, 3.480122177279554e-05, 3.480122177279554e-05, 3.480122177279554e-05, 3.480122177279554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.480122177279554e-05

Optimization complete. Final v2v error: 4.806656837463379 mm

Highest mean error: 5.1295857429504395 mm for frame 42

Lowest mean error: 4.245664596557617 mm for frame 120

Saving results

Total time: 29.90813684463501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875234
Iteration 2/25 | Loss: 0.00120837
Iteration 3/25 | Loss: 0.00109175
Iteration 4/25 | Loss: 0.00107986
Iteration 5/25 | Loss: 0.00107681
Iteration 6/25 | Loss: 0.00107617
Iteration 7/25 | Loss: 0.00107617
Iteration 8/25 | Loss: 0.00107617
Iteration 9/25 | Loss: 0.00107617
Iteration 10/25 | Loss: 0.00107617
Iteration 11/25 | Loss: 0.00107617
Iteration 12/25 | Loss: 0.00107617
Iteration 13/25 | Loss: 0.00107617
Iteration 14/25 | Loss: 0.00107617
Iteration 15/25 | Loss: 0.00107617
Iteration 16/25 | Loss: 0.00107617
Iteration 17/25 | Loss: 0.00107617
Iteration 18/25 | Loss: 0.00107617
Iteration 19/25 | Loss: 0.00107617
Iteration 20/25 | Loss: 0.00107617
Iteration 21/25 | Loss: 0.00107617
Iteration 22/25 | Loss: 0.00107617
Iteration 23/25 | Loss: 0.00107617
Iteration 24/25 | Loss: 0.00107617
Iteration 25/25 | Loss: 0.00107617

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43838775
Iteration 2/25 | Loss: 0.00078024
Iteration 3/25 | Loss: 0.00078023
Iteration 4/25 | Loss: 0.00078023
Iteration 5/25 | Loss: 0.00078023
Iteration 6/25 | Loss: 0.00078023
Iteration 7/25 | Loss: 0.00078023
Iteration 8/25 | Loss: 0.00078023
Iteration 9/25 | Loss: 0.00078023
Iteration 10/25 | Loss: 0.00078023
Iteration 11/25 | Loss: 0.00078023
Iteration 12/25 | Loss: 0.00078023
Iteration 13/25 | Loss: 0.00078023
Iteration 14/25 | Loss: 0.00078023
Iteration 15/25 | Loss: 0.00078023
Iteration 16/25 | Loss: 0.00078023
Iteration 17/25 | Loss: 0.00078023
Iteration 18/25 | Loss: 0.00078023
Iteration 19/25 | Loss: 0.00078023
Iteration 20/25 | Loss: 0.00078023
Iteration 21/25 | Loss: 0.00078023
Iteration 22/25 | Loss: 0.00078023
Iteration 23/25 | Loss: 0.00078023
Iteration 24/25 | Loss: 0.00078023
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007802295149303973, 0.0007802295149303973, 0.0007802295149303973, 0.0007802295149303973, 0.0007802295149303973]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007802295149303973

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078023
Iteration 2/1000 | Loss: 0.00002585
Iteration 3/1000 | Loss: 0.00001645
Iteration 4/1000 | Loss: 0.00001375
Iteration 5/1000 | Loss: 0.00001257
Iteration 6/1000 | Loss: 0.00001180
Iteration 7/1000 | Loss: 0.00001146
Iteration 8/1000 | Loss: 0.00001112
Iteration 9/1000 | Loss: 0.00001090
Iteration 10/1000 | Loss: 0.00001080
Iteration 11/1000 | Loss: 0.00001071
Iteration 12/1000 | Loss: 0.00001062
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001051
Iteration 15/1000 | Loss: 0.00001051
Iteration 16/1000 | Loss: 0.00001050
Iteration 17/1000 | Loss: 0.00001045
Iteration 18/1000 | Loss: 0.00001045
Iteration 19/1000 | Loss: 0.00001044
Iteration 20/1000 | Loss: 0.00001043
Iteration 21/1000 | Loss: 0.00001042
Iteration 22/1000 | Loss: 0.00001042
Iteration 23/1000 | Loss: 0.00001042
Iteration 24/1000 | Loss: 0.00001040
Iteration 25/1000 | Loss: 0.00001040
Iteration 26/1000 | Loss: 0.00001039
Iteration 27/1000 | Loss: 0.00001035
Iteration 28/1000 | Loss: 0.00001034
Iteration 29/1000 | Loss: 0.00001030
Iteration 30/1000 | Loss: 0.00001029
Iteration 31/1000 | Loss: 0.00001027
Iteration 32/1000 | Loss: 0.00001027
Iteration 33/1000 | Loss: 0.00001026
Iteration 34/1000 | Loss: 0.00001023
Iteration 35/1000 | Loss: 0.00001023
Iteration 36/1000 | Loss: 0.00001023
Iteration 37/1000 | Loss: 0.00001023
Iteration 38/1000 | Loss: 0.00001023
Iteration 39/1000 | Loss: 0.00001023
Iteration 40/1000 | Loss: 0.00001022
Iteration 41/1000 | Loss: 0.00001022
Iteration 42/1000 | Loss: 0.00001022
Iteration 43/1000 | Loss: 0.00001021
Iteration 44/1000 | Loss: 0.00001021
Iteration 45/1000 | Loss: 0.00001021
Iteration 46/1000 | Loss: 0.00001021
Iteration 47/1000 | Loss: 0.00001021
Iteration 48/1000 | Loss: 0.00001020
Iteration 49/1000 | Loss: 0.00001020
Iteration 50/1000 | Loss: 0.00001020
Iteration 51/1000 | Loss: 0.00001019
Iteration 52/1000 | Loss: 0.00001019
Iteration 53/1000 | Loss: 0.00001019
Iteration 54/1000 | Loss: 0.00001019
Iteration 55/1000 | Loss: 0.00001019
Iteration 56/1000 | Loss: 0.00001019
Iteration 57/1000 | Loss: 0.00001018
Iteration 58/1000 | Loss: 0.00001018
Iteration 59/1000 | Loss: 0.00001018
Iteration 60/1000 | Loss: 0.00001018
Iteration 61/1000 | Loss: 0.00001017
Iteration 62/1000 | Loss: 0.00001017
Iteration 63/1000 | Loss: 0.00001017
Iteration 64/1000 | Loss: 0.00001016
Iteration 65/1000 | Loss: 0.00001016
Iteration 66/1000 | Loss: 0.00001016
Iteration 67/1000 | Loss: 0.00001016
Iteration 68/1000 | Loss: 0.00001016
Iteration 69/1000 | Loss: 0.00001016
Iteration 70/1000 | Loss: 0.00001015
Iteration 71/1000 | Loss: 0.00001015
Iteration 72/1000 | Loss: 0.00001015
Iteration 73/1000 | Loss: 0.00001014
Iteration 74/1000 | Loss: 0.00001014
Iteration 75/1000 | Loss: 0.00001014
Iteration 76/1000 | Loss: 0.00001013
Iteration 77/1000 | Loss: 0.00001013
Iteration 78/1000 | Loss: 0.00001013
Iteration 79/1000 | Loss: 0.00001013
Iteration 80/1000 | Loss: 0.00001013
Iteration 81/1000 | Loss: 0.00001012
Iteration 82/1000 | Loss: 0.00001012
Iteration 83/1000 | Loss: 0.00001012
Iteration 84/1000 | Loss: 0.00001012
Iteration 85/1000 | Loss: 0.00001012
Iteration 86/1000 | Loss: 0.00001012
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001012
Iteration 89/1000 | Loss: 0.00001011
Iteration 90/1000 | Loss: 0.00001011
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001010
Iteration 93/1000 | Loss: 0.00001010
Iteration 94/1000 | Loss: 0.00001010
Iteration 95/1000 | Loss: 0.00001009
Iteration 96/1000 | Loss: 0.00001009
Iteration 97/1000 | Loss: 0.00001009
Iteration 98/1000 | Loss: 0.00001009
Iteration 99/1000 | Loss: 0.00001009
Iteration 100/1000 | Loss: 0.00001009
Iteration 101/1000 | Loss: 0.00001009
Iteration 102/1000 | Loss: 0.00001009
Iteration 103/1000 | Loss: 0.00001008
Iteration 104/1000 | Loss: 0.00001008
Iteration 105/1000 | Loss: 0.00001008
Iteration 106/1000 | Loss: 0.00001007
Iteration 107/1000 | Loss: 0.00001007
Iteration 108/1000 | Loss: 0.00001007
Iteration 109/1000 | Loss: 0.00001007
Iteration 110/1000 | Loss: 0.00001007
Iteration 111/1000 | Loss: 0.00001006
Iteration 112/1000 | Loss: 0.00001006
Iteration 113/1000 | Loss: 0.00001006
Iteration 114/1000 | Loss: 0.00001005
Iteration 115/1000 | Loss: 0.00001005
Iteration 116/1000 | Loss: 0.00001005
Iteration 117/1000 | Loss: 0.00001004
Iteration 118/1000 | Loss: 0.00001004
Iteration 119/1000 | Loss: 0.00001003
Iteration 120/1000 | Loss: 0.00001003
Iteration 121/1000 | Loss: 0.00001003
Iteration 122/1000 | Loss: 0.00001003
Iteration 123/1000 | Loss: 0.00001003
Iteration 124/1000 | Loss: 0.00001003
Iteration 125/1000 | Loss: 0.00001002
Iteration 126/1000 | Loss: 0.00001002
Iteration 127/1000 | Loss: 0.00001001
Iteration 128/1000 | Loss: 0.00001001
Iteration 129/1000 | Loss: 0.00001001
Iteration 130/1000 | Loss: 0.00001000
Iteration 131/1000 | Loss: 0.00001000
Iteration 132/1000 | Loss: 0.00001000
Iteration 133/1000 | Loss: 0.00001000
Iteration 134/1000 | Loss: 0.00001000
Iteration 135/1000 | Loss: 0.00001000
Iteration 136/1000 | Loss: 0.00000999
Iteration 137/1000 | Loss: 0.00000999
Iteration 138/1000 | Loss: 0.00000999
Iteration 139/1000 | Loss: 0.00000999
Iteration 140/1000 | Loss: 0.00000999
Iteration 141/1000 | Loss: 0.00000999
Iteration 142/1000 | Loss: 0.00000999
Iteration 143/1000 | Loss: 0.00000999
Iteration 144/1000 | Loss: 0.00000998
Iteration 145/1000 | Loss: 0.00000998
Iteration 146/1000 | Loss: 0.00000998
Iteration 147/1000 | Loss: 0.00000997
Iteration 148/1000 | Loss: 0.00000997
Iteration 149/1000 | Loss: 0.00000997
Iteration 150/1000 | Loss: 0.00000997
Iteration 151/1000 | Loss: 0.00000997
Iteration 152/1000 | Loss: 0.00000997
Iteration 153/1000 | Loss: 0.00000997
Iteration 154/1000 | Loss: 0.00000997
Iteration 155/1000 | Loss: 0.00000996
Iteration 156/1000 | Loss: 0.00000996
Iteration 157/1000 | Loss: 0.00000996
Iteration 158/1000 | Loss: 0.00000996
Iteration 159/1000 | Loss: 0.00000996
Iteration 160/1000 | Loss: 0.00000996
Iteration 161/1000 | Loss: 0.00000996
Iteration 162/1000 | Loss: 0.00000995
Iteration 163/1000 | Loss: 0.00000995
Iteration 164/1000 | Loss: 0.00000995
Iteration 165/1000 | Loss: 0.00000994
Iteration 166/1000 | Loss: 0.00000994
Iteration 167/1000 | Loss: 0.00000994
Iteration 168/1000 | Loss: 0.00000994
Iteration 169/1000 | Loss: 0.00000994
Iteration 170/1000 | Loss: 0.00000994
Iteration 171/1000 | Loss: 0.00000993
Iteration 172/1000 | Loss: 0.00000993
Iteration 173/1000 | Loss: 0.00000993
Iteration 174/1000 | Loss: 0.00000993
Iteration 175/1000 | Loss: 0.00000993
Iteration 176/1000 | Loss: 0.00000993
Iteration 177/1000 | Loss: 0.00000993
Iteration 178/1000 | Loss: 0.00000993
Iteration 179/1000 | Loss: 0.00000993
Iteration 180/1000 | Loss: 0.00000993
Iteration 181/1000 | Loss: 0.00000992
Iteration 182/1000 | Loss: 0.00000992
Iteration 183/1000 | Loss: 0.00000992
Iteration 184/1000 | Loss: 0.00000992
Iteration 185/1000 | Loss: 0.00000992
Iteration 186/1000 | Loss: 0.00000991
Iteration 187/1000 | Loss: 0.00000991
Iteration 188/1000 | Loss: 0.00000991
Iteration 189/1000 | Loss: 0.00000991
Iteration 190/1000 | Loss: 0.00000991
Iteration 191/1000 | Loss: 0.00000991
Iteration 192/1000 | Loss: 0.00000991
Iteration 193/1000 | Loss: 0.00000991
Iteration 194/1000 | Loss: 0.00000991
Iteration 195/1000 | Loss: 0.00000991
Iteration 196/1000 | Loss: 0.00000991
Iteration 197/1000 | Loss: 0.00000991
Iteration 198/1000 | Loss: 0.00000990
Iteration 199/1000 | Loss: 0.00000990
Iteration 200/1000 | Loss: 0.00000990
Iteration 201/1000 | Loss: 0.00000990
Iteration 202/1000 | Loss: 0.00000990
Iteration 203/1000 | Loss: 0.00000989
Iteration 204/1000 | Loss: 0.00000989
Iteration 205/1000 | Loss: 0.00000989
Iteration 206/1000 | Loss: 0.00000989
Iteration 207/1000 | Loss: 0.00000989
Iteration 208/1000 | Loss: 0.00000989
Iteration 209/1000 | Loss: 0.00000989
Iteration 210/1000 | Loss: 0.00000989
Iteration 211/1000 | Loss: 0.00000989
Iteration 212/1000 | Loss: 0.00000989
Iteration 213/1000 | Loss: 0.00000989
Iteration 214/1000 | Loss: 0.00000989
Iteration 215/1000 | Loss: 0.00000989
Iteration 216/1000 | Loss: 0.00000989
Iteration 217/1000 | Loss: 0.00000988
Iteration 218/1000 | Loss: 0.00000988
Iteration 219/1000 | Loss: 0.00000988
Iteration 220/1000 | Loss: 0.00000988
Iteration 221/1000 | Loss: 0.00000988
Iteration 222/1000 | Loss: 0.00000988
Iteration 223/1000 | Loss: 0.00000988
Iteration 224/1000 | Loss: 0.00000988
Iteration 225/1000 | Loss: 0.00000988
Iteration 226/1000 | Loss: 0.00000988
Iteration 227/1000 | Loss: 0.00000988
Iteration 228/1000 | Loss: 0.00000988
Iteration 229/1000 | Loss: 0.00000988
Iteration 230/1000 | Loss: 0.00000988
Iteration 231/1000 | Loss: 0.00000988
Iteration 232/1000 | Loss: 0.00000988
Iteration 233/1000 | Loss: 0.00000988
Iteration 234/1000 | Loss: 0.00000988
Iteration 235/1000 | Loss: 0.00000988
Iteration 236/1000 | Loss: 0.00000988
Iteration 237/1000 | Loss: 0.00000988
Iteration 238/1000 | Loss: 0.00000988
Iteration 239/1000 | Loss: 0.00000988
Iteration 240/1000 | Loss: 0.00000988
Iteration 241/1000 | Loss: 0.00000988
Iteration 242/1000 | Loss: 0.00000988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [9.884415703709237e-06, 9.884415703709237e-06, 9.884415703709237e-06, 9.884415703709237e-06, 9.884415703709237e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.884415703709237e-06

Optimization complete. Final v2v error: 2.625756025314331 mm

Highest mean error: 3.4965829849243164 mm for frame 57

Lowest mean error: 2.38942813873291 mm for frame 130

Saving results

Total time: 43.71988272666931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787770
Iteration 2/25 | Loss: 0.00129095
Iteration 3/25 | Loss: 0.00116216
Iteration 4/25 | Loss: 0.00114518
Iteration 5/25 | Loss: 0.00114086
Iteration 6/25 | Loss: 0.00114014
Iteration 7/25 | Loss: 0.00114014
Iteration 8/25 | Loss: 0.00114014
Iteration 9/25 | Loss: 0.00114014
Iteration 10/25 | Loss: 0.00114014
Iteration 11/25 | Loss: 0.00114014
Iteration 12/25 | Loss: 0.00114014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011401429073885083, 0.0011401429073885083, 0.0011401429073885083, 0.0011401429073885083, 0.0011401429073885083]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011401429073885083

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.26737714
Iteration 2/25 | Loss: 0.00084685
Iteration 3/25 | Loss: 0.00084685
Iteration 4/25 | Loss: 0.00084685
Iteration 5/25 | Loss: 0.00084685
Iteration 6/25 | Loss: 0.00084685
Iteration 7/25 | Loss: 0.00084685
Iteration 8/25 | Loss: 0.00084685
Iteration 9/25 | Loss: 0.00084685
Iteration 10/25 | Loss: 0.00084685
Iteration 11/25 | Loss: 0.00084685
Iteration 12/25 | Loss: 0.00084685
Iteration 13/25 | Loss: 0.00084685
Iteration 14/25 | Loss: 0.00084685
Iteration 15/25 | Loss: 0.00084685
Iteration 16/25 | Loss: 0.00084685
Iteration 17/25 | Loss: 0.00084685
Iteration 18/25 | Loss: 0.00084685
Iteration 19/25 | Loss: 0.00084685
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00084685068577528, 0.00084685068577528, 0.00084685068577528, 0.00084685068577528, 0.00084685068577528]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00084685068577528

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084685
Iteration 2/1000 | Loss: 0.00003183
Iteration 3/1000 | Loss: 0.00002080
Iteration 4/1000 | Loss: 0.00001875
Iteration 5/1000 | Loss: 0.00001795
Iteration 6/1000 | Loss: 0.00001760
Iteration 7/1000 | Loss: 0.00001731
Iteration 8/1000 | Loss: 0.00001704
Iteration 9/1000 | Loss: 0.00001691
Iteration 10/1000 | Loss: 0.00001674
Iteration 11/1000 | Loss: 0.00001660
Iteration 12/1000 | Loss: 0.00001660
Iteration 13/1000 | Loss: 0.00001659
Iteration 14/1000 | Loss: 0.00001656
Iteration 15/1000 | Loss: 0.00001645
Iteration 16/1000 | Loss: 0.00001644
Iteration 17/1000 | Loss: 0.00001634
Iteration 18/1000 | Loss: 0.00001634
Iteration 19/1000 | Loss: 0.00001633
Iteration 20/1000 | Loss: 0.00001632
Iteration 21/1000 | Loss: 0.00001628
Iteration 22/1000 | Loss: 0.00001626
Iteration 23/1000 | Loss: 0.00001625
Iteration 24/1000 | Loss: 0.00001622
Iteration 25/1000 | Loss: 0.00001622
Iteration 26/1000 | Loss: 0.00001622
Iteration 27/1000 | Loss: 0.00001622
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00001621
Iteration 30/1000 | Loss: 0.00001621
Iteration 31/1000 | Loss: 0.00001620
Iteration 32/1000 | Loss: 0.00001619
Iteration 33/1000 | Loss: 0.00001617
Iteration 34/1000 | Loss: 0.00001616
Iteration 35/1000 | Loss: 0.00001616
Iteration 36/1000 | Loss: 0.00001616
Iteration 37/1000 | Loss: 0.00001616
Iteration 38/1000 | Loss: 0.00001616
Iteration 39/1000 | Loss: 0.00001616
Iteration 40/1000 | Loss: 0.00001616
Iteration 41/1000 | Loss: 0.00001615
Iteration 42/1000 | Loss: 0.00001615
Iteration 43/1000 | Loss: 0.00001615
Iteration 44/1000 | Loss: 0.00001615
Iteration 45/1000 | Loss: 0.00001614
Iteration 46/1000 | Loss: 0.00001614
Iteration 47/1000 | Loss: 0.00001614
Iteration 48/1000 | Loss: 0.00001614
Iteration 49/1000 | Loss: 0.00001613
Iteration 50/1000 | Loss: 0.00001613
Iteration 51/1000 | Loss: 0.00001612
Iteration 52/1000 | Loss: 0.00001612
Iteration 53/1000 | Loss: 0.00001612
Iteration 54/1000 | Loss: 0.00001612
Iteration 55/1000 | Loss: 0.00001612
Iteration 56/1000 | Loss: 0.00001611
Iteration 57/1000 | Loss: 0.00001611
Iteration 58/1000 | Loss: 0.00001611
Iteration 59/1000 | Loss: 0.00001611
Iteration 60/1000 | Loss: 0.00001610
Iteration 61/1000 | Loss: 0.00001610
Iteration 62/1000 | Loss: 0.00001609
Iteration 63/1000 | Loss: 0.00001609
Iteration 64/1000 | Loss: 0.00001609
Iteration 65/1000 | Loss: 0.00001609
Iteration 66/1000 | Loss: 0.00001609
Iteration 67/1000 | Loss: 0.00001608
Iteration 68/1000 | Loss: 0.00001608
Iteration 69/1000 | Loss: 0.00001608
Iteration 70/1000 | Loss: 0.00001608
Iteration 71/1000 | Loss: 0.00001608
Iteration 72/1000 | Loss: 0.00001608
Iteration 73/1000 | Loss: 0.00001608
Iteration 74/1000 | Loss: 0.00001608
Iteration 75/1000 | Loss: 0.00001608
Iteration 76/1000 | Loss: 0.00001608
Iteration 77/1000 | Loss: 0.00001608
Iteration 78/1000 | Loss: 0.00001608
Iteration 79/1000 | Loss: 0.00001608
Iteration 80/1000 | Loss: 0.00001608
Iteration 81/1000 | Loss: 0.00001608
Iteration 82/1000 | Loss: 0.00001608
Iteration 83/1000 | Loss: 0.00001607
Iteration 84/1000 | Loss: 0.00001607
Iteration 85/1000 | Loss: 0.00001607
Iteration 86/1000 | Loss: 0.00001607
Iteration 87/1000 | Loss: 0.00001607
Iteration 88/1000 | Loss: 0.00001607
Iteration 89/1000 | Loss: 0.00001607
Iteration 90/1000 | Loss: 0.00001606
Iteration 91/1000 | Loss: 0.00001606
Iteration 92/1000 | Loss: 0.00001605
Iteration 93/1000 | Loss: 0.00001605
Iteration 94/1000 | Loss: 0.00001605
Iteration 95/1000 | Loss: 0.00001605
Iteration 96/1000 | Loss: 0.00001605
Iteration 97/1000 | Loss: 0.00001604
Iteration 98/1000 | Loss: 0.00001604
Iteration 99/1000 | Loss: 0.00001604
Iteration 100/1000 | Loss: 0.00001604
Iteration 101/1000 | Loss: 0.00001604
Iteration 102/1000 | Loss: 0.00001604
Iteration 103/1000 | Loss: 0.00001604
Iteration 104/1000 | Loss: 0.00001604
Iteration 105/1000 | Loss: 0.00001603
Iteration 106/1000 | Loss: 0.00001603
Iteration 107/1000 | Loss: 0.00001603
Iteration 108/1000 | Loss: 0.00001603
Iteration 109/1000 | Loss: 0.00001602
Iteration 110/1000 | Loss: 0.00001602
Iteration 111/1000 | Loss: 0.00001601
Iteration 112/1000 | Loss: 0.00001601
Iteration 113/1000 | Loss: 0.00001601
Iteration 114/1000 | Loss: 0.00001601
Iteration 115/1000 | Loss: 0.00001601
Iteration 116/1000 | Loss: 0.00001600
Iteration 117/1000 | Loss: 0.00001599
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001599
Iteration 120/1000 | Loss: 0.00001599
Iteration 121/1000 | Loss: 0.00001599
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001598
Iteration 124/1000 | Loss: 0.00001598
Iteration 125/1000 | Loss: 0.00001598
Iteration 126/1000 | Loss: 0.00001598
Iteration 127/1000 | Loss: 0.00001598
Iteration 128/1000 | Loss: 0.00001598
Iteration 129/1000 | Loss: 0.00001598
Iteration 130/1000 | Loss: 0.00001598
Iteration 131/1000 | Loss: 0.00001598
Iteration 132/1000 | Loss: 0.00001598
Iteration 133/1000 | Loss: 0.00001597
Iteration 134/1000 | Loss: 0.00001597
Iteration 135/1000 | Loss: 0.00001597
Iteration 136/1000 | Loss: 0.00001597
Iteration 137/1000 | Loss: 0.00001597
Iteration 138/1000 | Loss: 0.00001597
Iteration 139/1000 | Loss: 0.00001597
Iteration 140/1000 | Loss: 0.00001597
Iteration 141/1000 | Loss: 0.00001597
Iteration 142/1000 | Loss: 0.00001597
Iteration 143/1000 | Loss: 0.00001596
Iteration 144/1000 | Loss: 0.00001596
Iteration 145/1000 | Loss: 0.00001596
Iteration 146/1000 | Loss: 0.00001596
Iteration 147/1000 | Loss: 0.00001595
Iteration 148/1000 | Loss: 0.00001595
Iteration 149/1000 | Loss: 0.00001595
Iteration 150/1000 | Loss: 0.00001595
Iteration 151/1000 | Loss: 0.00001595
Iteration 152/1000 | Loss: 0.00001595
Iteration 153/1000 | Loss: 0.00001595
Iteration 154/1000 | Loss: 0.00001594
Iteration 155/1000 | Loss: 0.00001594
Iteration 156/1000 | Loss: 0.00001594
Iteration 157/1000 | Loss: 0.00001594
Iteration 158/1000 | Loss: 0.00001594
Iteration 159/1000 | Loss: 0.00001594
Iteration 160/1000 | Loss: 0.00001594
Iteration 161/1000 | Loss: 0.00001594
Iteration 162/1000 | Loss: 0.00001594
Iteration 163/1000 | Loss: 0.00001594
Iteration 164/1000 | Loss: 0.00001594
Iteration 165/1000 | Loss: 0.00001594
Iteration 166/1000 | Loss: 0.00001594
Iteration 167/1000 | Loss: 0.00001594
Iteration 168/1000 | Loss: 0.00001594
Iteration 169/1000 | Loss: 0.00001594
Iteration 170/1000 | Loss: 0.00001594
Iteration 171/1000 | Loss: 0.00001594
Iteration 172/1000 | Loss: 0.00001594
Iteration 173/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.5938499927869998e-05, 1.5938499927869998e-05, 1.5938499927869998e-05, 1.5938499927869998e-05, 1.5938499927869998e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5938499927869998e-05

Optimization complete. Final v2v error: 3.343845844268799 mm

Highest mean error: 3.5184271335601807 mm for frame 122

Lowest mean error: 3.1906728744506836 mm for frame 43

Saving results

Total time: 36.87256479263306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966194
Iteration 2/25 | Loss: 0.00244667
Iteration 3/25 | Loss: 0.00197313
Iteration 4/25 | Loss: 0.00187305
Iteration 5/25 | Loss: 0.00171331
Iteration 6/25 | Loss: 0.00182203
Iteration 7/25 | Loss: 0.00136081
Iteration 8/25 | Loss: 0.00124603
Iteration 9/25 | Loss: 0.00117837
Iteration 10/25 | Loss: 0.00114145
Iteration 11/25 | Loss: 0.00112113
Iteration 12/25 | Loss: 0.00110530
Iteration 13/25 | Loss: 0.00110046
Iteration 14/25 | Loss: 0.00109987
Iteration 15/25 | Loss: 0.00109974
Iteration 16/25 | Loss: 0.00109971
Iteration 17/25 | Loss: 0.00109971
Iteration 18/25 | Loss: 0.00109971
Iteration 19/25 | Loss: 0.00109971
Iteration 20/25 | Loss: 0.00109971
Iteration 21/25 | Loss: 0.00109971
Iteration 22/25 | Loss: 0.00109971
Iteration 23/25 | Loss: 0.00109970
Iteration 24/25 | Loss: 0.00109970
Iteration 25/25 | Loss: 0.00109970

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37164235
Iteration 2/25 | Loss: 0.00047179
Iteration 3/25 | Loss: 0.00047179
Iteration 4/25 | Loss: 0.00047179
Iteration 5/25 | Loss: 0.00047179
Iteration 6/25 | Loss: 0.00047179
Iteration 7/25 | Loss: 0.00047179
Iteration 8/25 | Loss: 0.00047179
Iteration 9/25 | Loss: 0.00047179
Iteration 10/25 | Loss: 0.00047179
Iteration 11/25 | Loss: 0.00047179
Iteration 12/25 | Loss: 0.00047179
Iteration 13/25 | Loss: 0.00047179
Iteration 14/25 | Loss: 0.00047179
Iteration 15/25 | Loss: 0.00047179
Iteration 16/25 | Loss: 0.00047179
Iteration 17/25 | Loss: 0.00047179
Iteration 18/25 | Loss: 0.00047179
Iteration 19/25 | Loss: 0.00047179
Iteration 20/25 | Loss: 0.00047179
Iteration 21/25 | Loss: 0.00047179
Iteration 22/25 | Loss: 0.00047179
Iteration 23/25 | Loss: 0.00047179
Iteration 24/25 | Loss: 0.00047179
Iteration 25/25 | Loss: 0.00047179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047179
Iteration 2/1000 | Loss: 0.00002549
Iteration 3/1000 | Loss: 0.00001586
Iteration 4/1000 | Loss: 0.00001359
Iteration 5/1000 | Loss: 0.00001254
Iteration 6/1000 | Loss: 0.00001211
Iteration 7/1000 | Loss: 0.00001176
Iteration 8/1000 | Loss: 0.00001155
Iteration 9/1000 | Loss: 0.00001152
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001139
Iteration 12/1000 | Loss: 0.00001138
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001129
Iteration 15/1000 | Loss: 0.00001128
Iteration 16/1000 | Loss: 0.00001127
Iteration 17/1000 | Loss: 0.00001116
Iteration 18/1000 | Loss: 0.00001109
Iteration 19/1000 | Loss: 0.00001105
Iteration 20/1000 | Loss: 0.00001105
Iteration 21/1000 | Loss: 0.00001104
Iteration 22/1000 | Loss: 0.00001104
Iteration 23/1000 | Loss: 0.00001103
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001099
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001097
Iteration 34/1000 | Loss: 0.00001097
Iteration 35/1000 | Loss: 0.00001096
Iteration 36/1000 | Loss: 0.00001096
Iteration 37/1000 | Loss: 0.00001096
Iteration 38/1000 | Loss: 0.00001096
Iteration 39/1000 | Loss: 0.00001094
Iteration 40/1000 | Loss: 0.00001093
Iteration 41/1000 | Loss: 0.00001093
Iteration 42/1000 | Loss: 0.00001092
Iteration 43/1000 | Loss: 0.00001092
Iteration 44/1000 | Loss: 0.00001089
Iteration 45/1000 | Loss: 0.00001089
Iteration 46/1000 | Loss: 0.00001089
Iteration 47/1000 | Loss: 0.00001089
Iteration 48/1000 | Loss: 0.00001088
Iteration 49/1000 | Loss: 0.00001088
Iteration 50/1000 | Loss: 0.00001088
Iteration 51/1000 | Loss: 0.00001088
Iteration 52/1000 | Loss: 0.00001088
Iteration 53/1000 | Loss: 0.00001088
Iteration 54/1000 | Loss: 0.00001088
Iteration 55/1000 | Loss: 0.00001088
Iteration 56/1000 | Loss: 0.00001088
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001086
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001085
Iteration 61/1000 | Loss: 0.00001085
Iteration 62/1000 | Loss: 0.00001085
Iteration 63/1000 | Loss: 0.00001084
Iteration 64/1000 | Loss: 0.00001084
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001083
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001083
Iteration 71/1000 | Loss: 0.00001083
Iteration 72/1000 | Loss: 0.00001083
Iteration 73/1000 | Loss: 0.00001083
Iteration 74/1000 | Loss: 0.00001083
Iteration 75/1000 | Loss: 0.00001083
Iteration 76/1000 | Loss: 0.00001082
Iteration 77/1000 | Loss: 0.00001082
Iteration 78/1000 | Loss: 0.00001082
Iteration 79/1000 | Loss: 0.00001082
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001082
Iteration 83/1000 | Loss: 0.00001082
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001082
Iteration 87/1000 | Loss: 0.00001082
Iteration 88/1000 | Loss: 0.00001081
Iteration 89/1000 | Loss: 0.00001081
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001080
Iteration 92/1000 | Loss: 0.00001080
Iteration 93/1000 | Loss: 0.00001079
Iteration 94/1000 | Loss: 0.00001079
Iteration 95/1000 | Loss: 0.00001079
Iteration 96/1000 | Loss: 0.00001079
Iteration 97/1000 | Loss: 0.00001079
Iteration 98/1000 | Loss: 0.00001079
Iteration 99/1000 | Loss: 0.00001079
Iteration 100/1000 | Loss: 0.00001079
Iteration 101/1000 | Loss: 0.00001079
Iteration 102/1000 | Loss: 0.00001079
Iteration 103/1000 | Loss: 0.00001079
Iteration 104/1000 | Loss: 0.00001079
Iteration 105/1000 | Loss: 0.00001077
Iteration 106/1000 | Loss: 0.00001077
Iteration 107/1000 | Loss: 0.00001076
Iteration 108/1000 | Loss: 0.00001076
Iteration 109/1000 | Loss: 0.00001076
Iteration 110/1000 | Loss: 0.00001076
Iteration 111/1000 | Loss: 0.00001076
Iteration 112/1000 | Loss: 0.00001076
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001075
Iteration 116/1000 | Loss: 0.00001075
Iteration 117/1000 | Loss: 0.00001075
Iteration 118/1000 | Loss: 0.00001075
Iteration 119/1000 | Loss: 0.00001075
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001074
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001074
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001074
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001074
Iteration 130/1000 | Loss: 0.00001074
Iteration 131/1000 | Loss: 0.00001074
Iteration 132/1000 | Loss: 0.00001074
Iteration 133/1000 | Loss: 0.00001074
Iteration 134/1000 | Loss: 0.00001074
Iteration 135/1000 | Loss: 0.00001074
Iteration 136/1000 | Loss: 0.00001074
Iteration 137/1000 | Loss: 0.00001073
Iteration 138/1000 | Loss: 0.00001073
Iteration 139/1000 | Loss: 0.00001073
Iteration 140/1000 | Loss: 0.00001073
Iteration 141/1000 | Loss: 0.00001073
Iteration 142/1000 | Loss: 0.00001073
Iteration 143/1000 | Loss: 0.00001073
Iteration 144/1000 | Loss: 0.00001072
Iteration 145/1000 | Loss: 0.00001071
Iteration 146/1000 | Loss: 0.00001071
Iteration 147/1000 | Loss: 0.00001071
Iteration 148/1000 | Loss: 0.00001071
Iteration 149/1000 | Loss: 0.00001071
Iteration 150/1000 | Loss: 0.00001070
Iteration 151/1000 | Loss: 0.00001070
Iteration 152/1000 | Loss: 0.00001070
Iteration 153/1000 | Loss: 0.00001069
Iteration 154/1000 | Loss: 0.00001069
Iteration 155/1000 | Loss: 0.00001069
Iteration 156/1000 | Loss: 0.00001069
Iteration 157/1000 | Loss: 0.00001069
Iteration 158/1000 | Loss: 0.00001069
Iteration 159/1000 | Loss: 0.00001069
Iteration 160/1000 | Loss: 0.00001069
Iteration 161/1000 | Loss: 0.00001069
Iteration 162/1000 | Loss: 0.00001068
Iteration 163/1000 | Loss: 0.00001068
Iteration 164/1000 | Loss: 0.00001068
Iteration 165/1000 | Loss: 0.00001068
Iteration 166/1000 | Loss: 0.00001068
Iteration 167/1000 | Loss: 0.00001068
Iteration 168/1000 | Loss: 0.00001068
Iteration 169/1000 | Loss: 0.00001068
Iteration 170/1000 | Loss: 0.00001068
Iteration 171/1000 | Loss: 0.00001068
Iteration 172/1000 | Loss: 0.00001068
Iteration 173/1000 | Loss: 0.00001068
Iteration 174/1000 | Loss: 0.00001067
Iteration 175/1000 | Loss: 0.00001067
Iteration 176/1000 | Loss: 0.00001067
Iteration 177/1000 | Loss: 0.00001067
Iteration 178/1000 | Loss: 0.00001067
Iteration 179/1000 | Loss: 0.00001067
Iteration 180/1000 | Loss: 0.00001067
Iteration 181/1000 | Loss: 0.00001067
Iteration 182/1000 | Loss: 0.00001067
Iteration 183/1000 | Loss: 0.00001067
Iteration 184/1000 | Loss: 0.00001067
Iteration 185/1000 | Loss: 0.00001067
Iteration 186/1000 | Loss: 0.00001067
Iteration 187/1000 | Loss: 0.00001067
Iteration 188/1000 | Loss: 0.00001067
Iteration 189/1000 | Loss: 0.00001067
Iteration 190/1000 | Loss: 0.00001067
Iteration 191/1000 | Loss: 0.00001066
Iteration 192/1000 | Loss: 0.00001066
Iteration 193/1000 | Loss: 0.00001066
Iteration 194/1000 | Loss: 0.00001066
Iteration 195/1000 | Loss: 0.00001066
Iteration 196/1000 | Loss: 0.00001066
Iteration 197/1000 | Loss: 0.00001066
Iteration 198/1000 | Loss: 0.00001066
Iteration 199/1000 | Loss: 0.00001066
Iteration 200/1000 | Loss: 0.00001066
Iteration 201/1000 | Loss: 0.00001066
Iteration 202/1000 | Loss: 0.00001066
Iteration 203/1000 | Loss: 0.00001066
Iteration 204/1000 | Loss: 0.00001066
Iteration 205/1000 | Loss: 0.00001066
Iteration 206/1000 | Loss: 0.00001066
Iteration 207/1000 | Loss: 0.00001066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.0655497135303449e-05, 1.0655497135303449e-05, 1.0655497135303449e-05, 1.0655497135303449e-05, 1.0655497135303449e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0655497135303449e-05

Optimization complete. Final v2v error: 2.8331551551818848 mm

Highest mean error: 3.000786781311035 mm for frame 30

Lowest mean error: 2.6893410682678223 mm for frame 8

Saving results

Total time: 54.81573843955994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009902
Iteration 2/25 | Loss: 0.01009902
Iteration 3/25 | Loss: 0.01009902
Iteration 4/25 | Loss: 0.00448248
Iteration 5/25 | Loss: 0.00332458
Iteration 6/25 | Loss: 0.00228709
Iteration 7/25 | Loss: 0.00209826
Iteration 8/25 | Loss: 0.00215377
Iteration 9/25 | Loss: 0.00199321
Iteration 10/25 | Loss: 0.00190403
Iteration 11/25 | Loss: 0.00183722
Iteration 12/25 | Loss: 0.00177121
Iteration 13/25 | Loss: 0.00172014
Iteration 14/25 | Loss: 0.00162110
Iteration 15/25 | Loss: 0.00161304
Iteration 16/25 | Loss: 0.00153860
Iteration 17/25 | Loss: 0.00151926
Iteration 18/25 | Loss: 0.00150423
Iteration 19/25 | Loss: 0.00148446
Iteration 20/25 | Loss: 0.00146784
Iteration 21/25 | Loss: 0.00146211
Iteration 22/25 | Loss: 0.00145101
Iteration 23/25 | Loss: 0.00145039
Iteration 24/25 | Loss: 0.00144940
Iteration 25/25 | Loss: 0.00144433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32152903
Iteration 2/25 | Loss: 0.00420389
Iteration 3/25 | Loss: 0.00300049
Iteration 4/25 | Loss: 0.00298633
Iteration 5/25 | Loss: 0.00298633
Iteration 6/25 | Loss: 0.00298633
Iteration 7/25 | Loss: 0.00298633
Iteration 8/25 | Loss: 0.00298633
Iteration 9/25 | Loss: 0.00298633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0029863317031413317, 0.0029863317031413317, 0.0029863317031413317, 0.0029863317031413317, 0.0029863317031413317]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029863317031413317

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298633
Iteration 2/1000 | Loss: 0.01016527
Iteration 3/1000 | Loss: 0.00096177
Iteration 4/1000 | Loss: 0.00129642
Iteration 5/1000 | Loss: 0.00085090
Iteration 6/1000 | Loss: 0.00061972
Iteration 7/1000 | Loss: 0.00116360
Iteration 8/1000 | Loss: 0.00056157
Iteration 9/1000 | Loss: 0.00038495
Iteration 10/1000 | Loss: 0.00035826
Iteration 11/1000 | Loss: 0.00077428
Iteration 12/1000 | Loss: 0.00088244
Iteration 13/1000 | Loss: 0.00015724
Iteration 14/1000 | Loss: 0.00054287
Iteration 15/1000 | Loss: 0.00120783
Iteration 16/1000 | Loss: 0.00200354
Iteration 17/1000 | Loss: 0.00081674
Iteration 18/1000 | Loss: 0.00057323
Iteration 19/1000 | Loss: 0.00030911
Iteration 20/1000 | Loss: 0.00040457
Iteration 21/1000 | Loss: 0.00038395
Iteration 22/1000 | Loss: 0.00043264
Iteration 23/1000 | Loss: 0.00054341
Iteration 24/1000 | Loss: 0.00029185
Iteration 25/1000 | Loss: 0.00201399
Iteration 26/1000 | Loss: 0.00402386
Iteration 27/1000 | Loss: 0.00289046
Iteration 28/1000 | Loss: 0.00192100
Iteration 29/1000 | Loss: 0.00226397
Iteration 30/1000 | Loss: 0.00163471
Iteration 31/1000 | Loss: 0.00086897
Iteration 32/1000 | Loss: 0.00038468
Iteration 33/1000 | Loss: 0.00034625
Iteration 34/1000 | Loss: 0.00031349
Iteration 35/1000 | Loss: 0.00024523
Iteration 36/1000 | Loss: 0.00034424
Iteration 37/1000 | Loss: 0.00031930
Iteration 38/1000 | Loss: 0.00013362
Iteration 39/1000 | Loss: 0.00184324
Iteration 40/1000 | Loss: 0.00118180
Iteration 41/1000 | Loss: 0.00009595
Iteration 42/1000 | Loss: 0.00011065
Iteration 43/1000 | Loss: 0.00046559
Iteration 44/1000 | Loss: 0.00025181
Iteration 45/1000 | Loss: 0.00027764
Iteration 46/1000 | Loss: 0.00021986
Iteration 47/1000 | Loss: 0.00017096
Iteration 48/1000 | Loss: 0.00008851
Iteration 49/1000 | Loss: 0.00033198
Iteration 50/1000 | Loss: 0.00039569
Iteration 51/1000 | Loss: 0.00240259
Iteration 52/1000 | Loss: 0.00338121
Iteration 53/1000 | Loss: 0.00246888
Iteration 54/1000 | Loss: 0.00210293
Iteration 55/1000 | Loss: 0.00145857
Iteration 56/1000 | Loss: 0.00104174
Iteration 57/1000 | Loss: 0.00158548
Iteration 58/1000 | Loss: 0.00102238
Iteration 59/1000 | Loss: 0.00014391
Iteration 60/1000 | Loss: 0.00009309
Iteration 61/1000 | Loss: 0.00012503
Iteration 62/1000 | Loss: 0.00034701
Iteration 63/1000 | Loss: 0.00041959
Iteration 64/1000 | Loss: 0.00048086
Iteration 65/1000 | Loss: 0.00019080
Iteration 66/1000 | Loss: 0.00007135
Iteration 67/1000 | Loss: 0.00009830
Iteration 68/1000 | Loss: 0.00007731
Iteration 69/1000 | Loss: 0.00014840
Iteration 70/1000 | Loss: 0.00008624
Iteration 71/1000 | Loss: 0.00033788
Iteration 72/1000 | Loss: 0.00024936
Iteration 73/1000 | Loss: 0.00009293
Iteration 74/1000 | Loss: 0.00008469
Iteration 75/1000 | Loss: 0.00005921
Iteration 76/1000 | Loss: 0.00006761
Iteration 77/1000 | Loss: 0.00006685
Iteration 78/1000 | Loss: 0.00048376
Iteration 79/1000 | Loss: 0.00044963
Iteration 80/1000 | Loss: 0.00040908
Iteration 81/1000 | Loss: 0.00031750
Iteration 82/1000 | Loss: 0.00012968
Iteration 83/1000 | Loss: 0.00015264
Iteration 84/1000 | Loss: 0.00006465
Iteration 85/1000 | Loss: 0.00011654
Iteration 86/1000 | Loss: 0.00005127
Iteration 87/1000 | Loss: 0.00005216
Iteration 88/1000 | Loss: 0.00017178
Iteration 89/1000 | Loss: 0.00021396
Iteration 90/1000 | Loss: 0.00018954
Iteration 91/1000 | Loss: 0.00018690
Iteration 92/1000 | Loss: 0.00010487
Iteration 93/1000 | Loss: 0.00005227
Iteration 94/1000 | Loss: 0.00007532
Iteration 95/1000 | Loss: 0.00026422
Iteration 96/1000 | Loss: 0.00006118
Iteration 97/1000 | Loss: 0.00005148
Iteration 98/1000 | Loss: 0.00004970
Iteration 99/1000 | Loss: 0.00005366
Iteration 100/1000 | Loss: 0.00005300
Iteration 101/1000 | Loss: 0.00005118
Iteration 102/1000 | Loss: 0.00005553
Iteration 103/1000 | Loss: 0.00036750
Iteration 104/1000 | Loss: 0.00013214
Iteration 105/1000 | Loss: 0.00006737
Iteration 106/1000 | Loss: 0.00016603
Iteration 107/1000 | Loss: 0.00007625
Iteration 108/1000 | Loss: 0.00007559
Iteration 109/1000 | Loss: 0.00044825
Iteration 110/1000 | Loss: 0.00011604
Iteration 111/1000 | Loss: 0.00015823
Iteration 112/1000 | Loss: 0.00005494
Iteration 113/1000 | Loss: 0.00011929
Iteration 114/1000 | Loss: 0.00004886
Iteration 115/1000 | Loss: 0.00005633
Iteration 116/1000 | Loss: 0.00004246
Iteration 117/1000 | Loss: 0.00006798
Iteration 118/1000 | Loss: 0.00015948
Iteration 119/1000 | Loss: 0.00015785
Iteration 120/1000 | Loss: 0.00004068
Iteration 121/1000 | Loss: 0.00009733
Iteration 122/1000 | Loss: 0.00060879
Iteration 123/1000 | Loss: 0.00035879
Iteration 124/1000 | Loss: 0.00024248
Iteration 125/1000 | Loss: 0.00011321
Iteration 126/1000 | Loss: 0.00003823
Iteration 127/1000 | Loss: 0.00016199
Iteration 128/1000 | Loss: 0.00031913
Iteration 129/1000 | Loss: 0.00020555
Iteration 130/1000 | Loss: 0.00011054
Iteration 131/1000 | Loss: 0.00009554
Iteration 132/1000 | Loss: 0.00008294
Iteration 133/1000 | Loss: 0.00004171
Iteration 134/1000 | Loss: 0.00026472
Iteration 135/1000 | Loss: 0.00017618
Iteration 136/1000 | Loss: 0.00007375
Iteration 137/1000 | Loss: 0.00018698
Iteration 138/1000 | Loss: 0.00015363
Iteration 139/1000 | Loss: 0.00009554
Iteration 140/1000 | Loss: 0.00012155
Iteration 141/1000 | Loss: 0.00018539
Iteration 142/1000 | Loss: 0.00019802
Iteration 143/1000 | Loss: 0.00005292
Iteration 144/1000 | Loss: 0.00014294
Iteration 145/1000 | Loss: 0.00004945
Iteration 146/1000 | Loss: 0.00004929
Iteration 147/1000 | Loss: 0.00003787
Iteration 148/1000 | Loss: 0.00014428
Iteration 149/1000 | Loss: 0.00009963
Iteration 150/1000 | Loss: 0.00003472
Iteration 151/1000 | Loss: 0.00014896
Iteration 152/1000 | Loss: 0.00007417
Iteration 153/1000 | Loss: 0.00006881
Iteration 154/1000 | Loss: 0.00006871
Iteration 155/1000 | Loss: 0.00003382
Iteration 156/1000 | Loss: 0.00005020
Iteration 157/1000 | Loss: 0.00004082
Iteration 158/1000 | Loss: 0.00033076
Iteration 159/1000 | Loss: 0.00006449
Iteration 160/1000 | Loss: 0.00003121
Iteration 161/1000 | Loss: 0.00005706
Iteration 162/1000 | Loss: 0.00033951
Iteration 163/1000 | Loss: 0.00099227
Iteration 164/1000 | Loss: 0.00004615
Iteration 165/1000 | Loss: 0.00023614
Iteration 166/1000 | Loss: 0.00004307
Iteration 167/1000 | Loss: 0.00003234
Iteration 168/1000 | Loss: 0.00005634
Iteration 169/1000 | Loss: 0.00002964
Iteration 170/1000 | Loss: 0.00002799
Iteration 171/1000 | Loss: 0.00007156
Iteration 172/1000 | Loss: 0.00002801
Iteration 173/1000 | Loss: 0.00004150
Iteration 174/1000 | Loss: 0.00004019
Iteration 175/1000 | Loss: 0.00002520
Iteration 176/1000 | Loss: 0.00003651
Iteration 177/1000 | Loss: 0.00002502
Iteration 178/1000 | Loss: 0.00002491
Iteration 179/1000 | Loss: 0.00002480
Iteration 180/1000 | Loss: 0.00002480
Iteration 181/1000 | Loss: 0.00002480
Iteration 182/1000 | Loss: 0.00002478
Iteration 183/1000 | Loss: 0.00002478
Iteration 184/1000 | Loss: 0.00002477
Iteration 185/1000 | Loss: 0.00002477
Iteration 186/1000 | Loss: 0.00002476
Iteration 187/1000 | Loss: 0.00002476
Iteration 188/1000 | Loss: 0.00002475
Iteration 189/1000 | Loss: 0.00003630
Iteration 190/1000 | Loss: 0.00002474
Iteration 191/1000 | Loss: 0.00002473
Iteration 192/1000 | Loss: 0.00002471
Iteration 193/1000 | Loss: 0.00002470
Iteration 194/1000 | Loss: 0.00002470
Iteration 195/1000 | Loss: 0.00002470
Iteration 196/1000 | Loss: 0.00002470
Iteration 197/1000 | Loss: 0.00002470
Iteration 198/1000 | Loss: 0.00002470
Iteration 199/1000 | Loss: 0.00002470
Iteration 200/1000 | Loss: 0.00002470
Iteration 201/1000 | Loss: 0.00002469
Iteration 202/1000 | Loss: 0.00002469
Iteration 203/1000 | Loss: 0.00002469
Iteration 204/1000 | Loss: 0.00002469
Iteration 205/1000 | Loss: 0.00002469
Iteration 206/1000 | Loss: 0.00002468
Iteration 207/1000 | Loss: 0.00002468
Iteration 208/1000 | Loss: 0.00002468
Iteration 209/1000 | Loss: 0.00002467
Iteration 210/1000 | Loss: 0.00004544
Iteration 211/1000 | Loss: 0.00004544
Iteration 212/1000 | Loss: 0.00004544
Iteration 213/1000 | Loss: 0.00004544
Iteration 214/1000 | Loss: 0.00011390
Iteration 215/1000 | Loss: 0.00007255
Iteration 216/1000 | Loss: 0.00002967
Iteration 217/1000 | Loss: 0.00003994
Iteration 218/1000 | Loss: 0.00002737
Iteration 219/1000 | Loss: 0.00003057
Iteration 220/1000 | Loss: 0.00002787
Iteration 221/1000 | Loss: 0.00002605
Iteration 222/1000 | Loss: 0.00002473
Iteration 223/1000 | Loss: 0.00002473
Iteration 224/1000 | Loss: 0.00002470
Iteration 225/1000 | Loss: 0.00002469
Iteration 226/1000 | Loss: 0.00002457
Iteration 227/1000 | Loss: 0.00002457
Iteration 228/1000 | Loss: 0.00002457
Iteration 229/1000 | Loss: 0.00002457
Iteration 230/1000 | Loss: 0.00002457
Iteration 231/1000 | Loss: 0.00002456
Iteration 232/1000 | Loss: 0.00002456
Iteration 233/1000 | Loss: 0.00002456
Iteration 234/1000 | Loss: 0.00002456
Iteration 235/1000 | Loss: 0.00002456
Iteration 236/1000 | Loss: 0.00002456
Iteration 237/1000 | Loss: 0.00002456
Iteration 238/1000 | Loss: 0.00002456
Iteration 239/1000 | Loss: 0.00002456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.456352194712963e-05, 2.456352194712963e-05, 2.456352194712963e-05, 2.456352194712963e-05, 2.456352194712963e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.456352194712963e-05

Optimization complete. Final v2v error: 3.4298219680786133 mm

Highest mean error: 11.23144245147705 mm for frame 200

Lowest mean error: 2.9444057941436768 mm for frame 36

Saving results

Total time: 357.83923983573914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00358251
Iteration 2/25 | Loss: 0.00121747
Iteration 3/25 | Loss: 0.00108644
Iteration 4/25 | Loss: 0.00105965
Iteration 5/25 | Loss: 0.00104995
Iteration 6/25 | Loss: 0.00104739
Iteration 7/25 | Loss: 0.00104654
Iteration 8/25 | Loss: 0.00104652
Iteration 9/25 | Loss: 0.00104652
Iteration 10/25 | Loss: 0.00104652
Iteration 11/25 | Loss: 0.00104652
Iteration 12/25 | Loss: 0.00104652
Iteration 13/25 | Loss: 0.00104652
Iteration 14/25 | Loss: 0.00104652
Iteration 15/25 | Loss: 0.00104652
Iteration 16/25 | Loss: 0.00104652
Iteration 17/25 | Loss: 0.00104652
Iteration 18/25 | Loss: 0.00104652
Iteration 19/25 | Loss: 0.00104652
Iteration 20/25 | Loss: 0.00104652
Iteration 21/25 | Loss: 0.00104652
Iteration 22/25 | Loss: 0.00104652
Iteration 23/25 | Loss: 0.00104652
Iteration 24/25 | Loss: 0.00104652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001046519260853529, 0.001046519260853529, 0.001046519260853529, 0.001046519260853529, 0.001046519260853529]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001046519260853529

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38576818
Iteration 2/25 | Loss: 0.00085455
Iteration 3/25 | Loss: 0.00085455
Iteration 4/25 | Loss: 0.00085455
Iteration 5/25 | Loss: 0.00085455
Iteration 6/25 | Loss: 0.00085455
Iteration 7/25 | Loss: 0.00085455
Iteration 8/25 | Loss: 0.00085455
Iteration 9/25 | Loss: 0.00085455
Iteration 10/25 | Loss: 0.00085455
Iteration 11/25 | Loss: 0.00085455
Iteration 12/25 | Loss: 0.00085455
Iteration 13/25 | Loss: 0.00085455
Iteration 14/25 | Loss: 0.00085455
Iteration 15/25 | Loss: 0.00085455
Iteration 16/25 | Loss: 0.00085455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000854545331094414, 0.000854545331094414, 0.000854545331094414, 0.000854545331094414, 0.000854545331094414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000854545331094414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085455
Iteration 2/1000 | Loss: 0.00004679
Iteration 3/1000 | Loss: 0.00002905
Iteration 4/1000 | Loss: 0.00002083
Iteration 5/1000 | Loss: 0.00001903
Iteration 6/1000 | Loss: 0.00001770
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001550
Iteration 10/1000 | Loss: 0.00001519
Iteration 11/1000 | Loss: 0.00001498
Iteration 12/1000 | Loss: 0.00001480
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001460
Iteration 15/1000 | Loss: 0.00001456
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001452
Iteration 18/1000 | Loss: 0.00001450
Iteration 19/1000 | Loss: 0.00001449
Iteration 20/1000 | Loss: 0.00001448
Iteration 21/1000 | Loss: 0.00001447
Iteration 22/1000 | Loss: 0.00001447
Iteration 23/1000 | Loss: 0.00001446
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001445
Iteration 27/1000 | Loss: 0.00001444
Iteration 28/1000 | Loss: 0.00001444
Iteration 29/1000 | Loss: 0.00001444
Iteration 30/1000 | Loss: 0.00001443
Iteration 31/1000 | Loss: 0.00001442
Iteration 32/1000 | Loss: 0.00001442
Iteration 33/1000 | Loss: 0.00001441
Iteration 34/1000 | Loss: 0.00001441
Iteration 35/1000 | Loss: 0.00001441
Iteration 36/1000 | Loss: 0.00001440
Iteration 37/1000 | Loss: 0.00001439
Iteration 38/1000 | Loss: 0.00001436
Iteration 39/1000 | Loss: 0.00001436
Iteration 40/1000 | Loss: 0.00001435
Iteration 41/1000 | Loss: 0.00001430
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001426
Iteration 44/1000 | Loss: 0.00001426
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001425
Iteration 47/1000 | Loss: 0.00001424
Iteration 48/1000 | Loss: 0.00001424
Iteration 49/1000 | Loss: 0.00001424
Iteration 50/1000 | Loss: 0.00001423
Iteration 51/1000 | Loss: 0.00001423
Iteration 52/1000 | Loss: 0.00001423
Iteration 53/1000 | Loss: 0.00001423
Iteration 54/1000 | Loss: 0.00001422
Iteration 55/1000 | Loss: 0.00001422
Iteration 56/1000 | Loss: 0.00001422
Iteration 57/1000 | Loss: 0.00001422
Iteration 58/1000 | Loss: 0.00001421
Iteration 59/1000 | Loss: 0.00001421
Iteration 60/1000 | Loss: 0.00001421
Iteration 61/1000 | Loss: 0.00001421
Iteration 62/1000 | Loss: 0.00001421
Iteration 63/1000 | Loss: 0.00001420
Iteration 64/1000 | Loss: 0.00001420
Iteration 65/1000 | Loss: 0.00001420
Iteration 66/1000 | Loss: 0.00001419
Iteration 67/1000 | Loss: 0.00001419
Iteration 68/1000 | Loss: 0.00001418
Iteration 69/1000 | Loss: 0.00001418
Iteration 70/1000 | Loss: 0.00001418
Iteration 71/1000 | Loss: 0.00001418
Iteration 72/1000 | Loss: 0.00001418
Iteration 73/1000 | Loss: 0.00001418
Iteration 74/1000 | Loss: 0.00001418
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001418
Iteration 77/1000 | Loss: 0.00001417
Iteration 78/1000 | Loss: 0.00001417
Iteration 79/1000 | Loss: 0.00001417
Iteration 80/1000 | Loss: 0.00001417
Iteration 81/1000 | Loss: 0.00001417
Iteration 82/1000 | Loss: 0.00001417
Iteration 83/1000 | Loss: 0.00001416
Iteration 84/1000 | Loss: 0.00001416
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001416
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001415
Iteration 91/1000 | Loss: 0.00001415
Iteration 92/1000 | Loss: 0.00001415
Iteration 93/1000 | Loss: 0.00001415
Iteration 94/1000 | Loss: 0.00001415
Iteration 95/1000 | Loss: 0.00001414
Iteration 96/1000 | Loss: 0.00001414
Iteration 97/1000 | Loss: 0.00001414
Iteration 98/1000 | Loss: 0.00001414
Iteration 99/1000 | Loss: 0.00001413
Iteration 100/1000 | Loss: 0.00001413
Iteration 101/1000 | Loss: 0.00001413
Iteration 102/1000 | Loss: 0.00001413
Iteration 103/1000 | Loss: 0.00001413
Iteration 104/1000 | Loss: 0.00001413
Iteration 105/1000 | Loss: 0.00001413
Iteration 106/1000 | Loss: 0.00001413
Iteration 107/1000 | Loss: 0.00001412
Iteration 108/1000 | Loss: 0.00001412
Iteration 109/1000 | Loss: 0.00001412
Iteration 110/1000 | Loss: 0.00001412
Iteration 111/1000 | Loss: 0.00001412
Iteration 112/1000 | Loss: 0.00001411
Iteration 113/1000 | Loss: 0.00001411
Iteration 114/1000 | Loss: 0.00001411
Iteration 115/1000 | Loss: 0.00001411
Iteration 116/1000 | Loss: 0.00001411
Iteration 117/1000 | Loss: 0.00001411
Iteration 118/1000 | Loss: 0.00001411
Iteration 119/1000 | Loss: 0.00001411
Iteration 120/1000 | Loss: 0.00001410
Iteration 121/1000 | Loss: 0.00001410
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001409
Iteration 125/1000 | Loss: 0.00001409
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001408
Iteration 129/1000 | Loss: 0.00001408
Iteration 130/1000 | Loss: 0.00001408
Iteration 131/1000 | Loss: 0.00001408
Iteration 132/1000 | Loss: 0.00001408
Iteration 133/1000 | Loss: 0.00001407
Iteration 134/1000 | Loss: 0.00001407
Iteration 135/1000 | Loss: 0.00001407
Iteration 136/1000 | Loss: 0.00001407
Iteration 137/1000 | Loss: 0.00001407
Iteration 138/1000 | Loss: 0.00001407
Iteration 139/1000 | Loss: 0.00001407
Iteration 140/1000 | Loss: 0.00001406
Iteration 141/1000 | Loss: 0.00001406
Iteration 142/1000 | Loss: 0.00001406
Iteration 143/1000 | Loss: 0.00001406
Iteration 144/1000 | Loss: 0.00001406
Iteration 145/1000 | Loss: 0.00001406
Iteration 146/1000 | Loss: 0.00001406
Iteration 147/1000 | Loss: 0.00001406
Iteration 148/1000 | Loss: 0.00001406
Iteration 149/1000 | Loss: 0.00001405
Iteration 150/1000 | Loss: 0.00001405
Iteration 151/1000 | Loss: 0.00001405
Iteration 152/1000 | Loss: 0.00001405
Iteration 153/1000 | Loss: 0.00001405
Iteration 154/1000 | Loss: 0.00001405
Iteration 155/1000 | Loss: 0.00001405
Iteration 156/1000 | Loss: 0.00001405
Iteration 157/1000 | Loss: 0.00001405
Iteration 158/1000 | Loss: 0.00001404
Iteration 159/1000 | Loss: 0.00001404
Iteration 160/1000 | Loss: 0.00001404
Iteration 161/1000 | Loss: 0.00001404
Iteration 162/1000 | Loss: 0.00001404
Iteration 163/1000 | Loss: 0.00001404
Iteration 164/1000 | Loss: 0.00001404
Iteration 165/1000 | Loss: 0.00001404
Iteration 166/1000 | Loss: 0.00001404
Iteration 167/1000 | Loss: 0.00001404
Iteration 168/1000 | Loss: 0.00001404
Iteration 169/1000 | Loss: 0.00001404
Iteration 170/1000 | Loss: 0.00001403
Iteration 171/1000 | Loss: 0.00001403
Iteration 172/1000 | Loss: 0.00001403
Iteration 173/1000 | Loss: 0.00001403
Iteration 174/1000 | Loss: 0.00001403
Iteration 175/1000 | Loss: 0.00001403
Iteration 176/1000 | Loss: 0.00001403
Iteration 177/1000 | Loss: 0.00001403
Iteration 178/1000 | Loss: 0.00001403
Iteration 179/1000 | Loss: 0.00001402
Iteration 180/1000 | Loss: 0.00001402
Iteration 181/1000 | Loss: 0.00001402
Iteration 182/1000 | Loss: 0.00001402
Iteration 183/1000 | Loss: 0.00001402
Iteration 184/1000 | Loss: 0.00001402
Iteration 185/1000 | Loss: 0.00001402
Iteration 186/1000 | Loss: 0.00001402
Iteration 187/1000 | Loss: 0.00001402
Iteration 188/1000 | Loss: 0.00001402
Iteration 189/1000 | Loss: 0.00001402
Iteration 190/1000 | Loss: 0.00001402
Iteration 191/1000 | Loss: 0.00001401
Iteration 192/1000 | Loss: 0.00001401
Iteration 193/1000 | Loss: 0.00001401
Iteration 194/1000 | Loss: 0.00001401
Iteration 195/1000 | Loss: 0.00001401
Iteration 196/1000 | Loss: 0.00001401
Iteration 197/1000 | Loss: 0.00001401
Iteration 198/1000 | Loss: 0.00001401
Iteration 199/1000 | Loss: 0.00001401
Iteration 200/1000 | Loss: 0.00001401
Iteration 201/1000 | Loss: 0.00001401
Iteration 202/1000 | Loss: 0.00001401
Iteration 203/1000 | Loss: 0.00001401
Iteration 204/1000 | Loss: 0.00001400
Iteration 205/1000 | Loss: 0.00001400
Iteration 206/1000 | Loss: 0.00001400
Iteration 207/1000 | Loss: 0.00001400
Iteration 208/1000 | Loss: 0.00001400
Iteration 209/1000 | Loss: 0.00001400
Iteration 210/1000 | Loss: 0.00001400
Iteration 211/1000 | Loss: 0.00001400
Iteration 212/1000 | Loss: 0.00001400
Iteration 213/1000 | Loss: 0.00001400
Iteration 214/1000 | Loss: 0.00001400
Iteration 215/1000 | Loss: 0.00001400
Iteration 216/1000 | Loss: 0.00001400
Iteration 217/1000 | Loss: 0.00001400
Iteration 218/1000 | Loss: 0.00001400
Iteration 219/1000 | Loss: 0.00001400
Iteration 220/1000 | Loss: 0.00001399
Iteration 221/1000 | Loss: 0.00001399
Iteration 222/1000 | Loss: 0.00001399
Iteration 223/1000 | Loss: 0.00001399
Iteration 224/1000 | Loss: 0.00001399
Iteration 225/1000 | Loss: 0.00001399
Iteration 226/1000 | Loss: 0.00001399
Iteration 227/1000 | Loss: 0.00001399
Iteration 228/1000 | Loss: 0.00001399
Iteration 229/1000 | Loss: 0.00001399
Iteration 230/1000 | Loss: 0.00001399
Iteration 231/1000 | Loss: 0.00001399
Iteration 232/1000 | Loss: 0.00001399
Iteration 233/1000 | Loss: 0.00001398
Iteration 234/1000 | Loss: 0.00001398
Iteration 235/1000 | Loss: 0.00001398
Iteration 236/1000 | Loss: 0.00001398
Iteration 237/1000 | Loss: 0.00001398
Iteration 238/1000 | Loss: 0.00001398
Iteration 239/1000 | Loss: 0.00001398
Iteration 240/1000 | Loss: 0.00001398
Iteration 241/1000 | Loss: 0.00001398
Iteration 242/1000 | Loss: 0.00001398
Iteration 243/1000 | Loss: 0.00001398
Iteration 244/1000 | Loss: 0.00001398
Iteration 245/1000 | Loss: 0.00001398
Iteration 246/1000 | Loss: 0.00001398
Iteration 247/1000 | Loss: 0.00001398
Iteration 248/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [1.3983329154143576e-05, 1.3983329154143576e-05, 1.3983329154143576e-05, 1.3983329154143576e-05, 1.3983329154143576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3983329154143576e-05

Optimization complete. Final v2v error: 3.0978245735168457 mm

Highest mean error: 4.650249004364014 mm for frame 151

Lowest mean error: 2.158083915710449 mm for frame 165

Saving results

Total time: 48.70668888092041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00461912
Iteration 2/25 | Loss: 0.00118526
Iteration 3/25 | Loss: 0.00109316
Iteration 4/25 | Loss: 0.00108182
Iteration 5/25 | Loss: 0.00107915
Iteration 6/25 | Loss: 0.00107915
Iteration 7/25 | Loss: 0.00107915
Iteration 8/25 | Loss: 0.00107915
Iteration 9/25 | Loss: 0.00107915
Iteration 10/25 | Loss: 0.00107915
Iteration 11/25 | Loss: 0.00107915
Iteration 12/25 | Loss: 0.00107915
Iteration 13/25 | Loss: 0.00107915
Iteration 14/25 | Loss: 0.00107915
Iteration 15/25 | Loss: 0.00107915
Iteration 16/25 | Loss: 0.00107915
Iteration 17/25 | Loss: 0.00107915
Iteration 18/25 | Loss: 0.00107915
Iteration 19/25 | Loss: 0.00107915
Iteration 20/25 | Loss: 0.00107915
Iteration 21/25 | Loss: 0.00107915
Iteration 22/25 | Loss: 0.00107915
Iteration 23/25 | Loss: 0.00107915
Iteration 24/25 | Loss: 0.00107915
Iteration 25/25 | Loss: 0.00107915

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89700627
Iteration 2/25 | Loss: 0.00068987
Iteration 3/25 | Loss: 0.00068987
Iteration 4/25 | Loss: 0.00068987
Iteration 5/25 | Loss: 0.00068987
Iteration 6/25 | Loss: 0.00068987
Iteration 7/25 | Loss: 0.00068987
Iteration 8/25 | Loss: 0.00068987
Iteration 9/25 | Loss: 0.00068987
Iteration 10/25 | Loss: 0.00068987
Iteration 11/25 | Loss: 0.00068987
Iteration 12/25 | Loss: 0.00068987
Iteration 13/25 | Loss: 0.00068987
Iteration 14/25 | Loss: 0.00068987
Iteration 15/25 | Loss: 0.00068987
Iteration 16/25 | Loss: 0.00068987
Iteration 17/25 | Loss: 0.00068987
Iteration 18/25 | Loss: 0.00068987
Iteration 19/25 | Loss: 0.00068987
Iteration 20/25 | Loss: 0.00068987
Iteration 21/25 | Loss: 0.00068987
Iteration 22/25 | Loss: 0.00068987
Iteration 23/25 | Loss: 0.00068987
Iteration 24/25 | Loss: 0.00068987
Iteration 25/25 | Loss: 0.00068987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068987
Iteration 2/1000 | Loss: 0.00001820
Iteration 3/1000 | Loss: 0.00001470
Iteration 4/1000 | Loss: 0.00001387
Iteration 5/1000 | Loss: 0.00001316
Iteration 6/1000 | Loss: 0.00001274
Iteration 7/1000 | Loss: 0.00001250
Iteration 8/1000 | Loss: 0.00001216
Iteration 9/1000 | Loss: 0.00001196
Iteration 10/1000 | Loss: 0.00001196
Iteration 11/1000 | Loss: 0.00001176
Iteration 12/1000 | Loss: 0.00001172
Iteration 13/1000 | Loss: 0.00001168
Iteration 14/1000 | Loss: 0.00001162
Iteration 15/1000 | Loss: 0.00001161
Iteration 16/1000 | Loss: 0.00001161
Iteration 17/1000 | Loss: 0.00001160
Iteration 18/1000 | Loss: 0.00001159
Iteration 19/1000 | Loss: 0.00001159
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001149
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001141
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001136
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001135
Iteration 29/1000 | Loss: 0.00001135
Iteration 30/1000 | Loss: 0.00001134
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001130
Iteration 40/1000 | Loss: 0.00001130
Iteration 41/1000 | Loss: 0.00001130
Iteration 42/1000 | Loss: 0.00001129
Iteration 43/1000 | Loss: 0.00001129
Iteration 44/1000 | Loss: 0.00001128
Iteration 45/1000 | Loss: 0.00001128
Iteration 46/1000 | Loss: 0.00001128
Iteration 47/1000 | Loss: 0.00001127
Iteration 48/1000 | Loss: 0.00001126
Iteration 49/1000 | Loss: 0.00001126
Iteration 50/1000 | Loss: 0.00001125
Iteration 51/1000 | Loss: 0.00001125
Iteration 52/1000 | Loss: 0.00001125
Iteration 53/1000 | Loss: 0.00001125
Iteration 54/1000 | Loss: 0.00001124
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001122
Iteration 60/1000 | Loss: 0.00001122
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001121
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001121
Iteration 65/1000 | Loss: 0.00001121
Iteration 66/1000 | Loss: 0.00001121
Iteration 67/1000 | Loss: 0.00001121
Iteration 68/1000 | Loss: 0.00001120
Iteration 69/1000 | Loss: 0.00001120
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001119
Iteration 72/1000 | Loss: 0.00001119
Iteration 73/1000 | Loss: 0.00001119
Iteration 74/1000 | Loss: 0.00001118
Iteration 75/1000 | Loss: 0.00001118
Iteration 76/1000 | Loss: 0.00001118
Iteration 77/1000 | Loss: 0.00001117
Iteration 78/1000 | Loss: 0.00001117
Iteration 79/1000 | Loss: 0.00001117
Iteration 80/1000 | Loss: 0.00001117
Iteration 81/1000 | Loss: 0.00001116
Iteration 82/1000 | Loss: 0.00001116
Iteration 83/1000 | Loss: 0.00001116
Iteration 84/1000 | Loss: 0.00001115
Iteration 85/1000 | Loss: 0.00001114
Iteration 86/1000 | Loss: 0.00001114
Iteration 87/1000 | Loss: 0.00001114
Iteration 88/1000 | Loss: 0.00001114
Iteration 89/1000 | Loss: 0.00001114
Iteration 90/1000 | Loss: 0.00001114
Iteration 91/1000 | Loss: 0.00001114
Iteration 92/1000 | Loss: 0.00001114
Iteration 93/1000 | Loss: 0.00001114
Iteration 94/1000 | Loss: 0.00001114
Iteration 95/1000 | Loss: 0.00001114
Iteration 96/1000 | Loss: 0.00001114
Iteration 97/1000 | Loss: 0.00001114
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [1.1142731636937242e-05, 1.1142731636937242e-05, 1.1142731636937242e-05, 1.1142731636937242e-05, 1.1142731636937242e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1142731636937242e-05

Optimization complete. Final v2v error: 2.857032299041748 mm

Highest mean error: 3.0960774421691895 mm for frame 121

Lowest mean error: 2.69266676902771 mm for frame 160

Saving results

Total time: 37.56344246864319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825335
Iteration 2/25 | Loss: 0.00113358
Iteration 3/25 | Loss: 0.00106435
Iteration 4/25 | Loss: 0.00105303
Iteration 5/25 | Loss: 0.00104937
Iteration 6/25 | Loss: 0.00104891
Iteration 7/25 | Loss: 0.00104891
Iteration 8/25 | Loss: 0.00104891
Iteration 9/25 | Loss: 0.00104891
Iteration 10/25 | Loss: 0.00104891
Iteration 11/25 | Loss: 0.00104891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001048912527039647, 0.001048912527039647, 0.001048912527039647, 0.001048912527039647, 0.001048912527039647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001048912527039647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.24630165
Iteration 2/25 | Loss: 0.00072567
Iteration 3/25 | Loss: 0.00072566
Iteration 4/25 | Loss: 0.00072566
Iteration 5/25 | Loss: 0.00072566
Iteration 6/25 | Loss: 0.00072566
Iteration 7/25 | Loss: 0.00072566
Iteration 8/25 | Loss: 0.00072566
Iteration 9/25 | Loss: 0.00072566
Iteration 10/25 | Loss: 0.00072566
Iteration 11/25 | Loss: 0.00072566
Iteration 12/25 | Loss: 0.00072566
Iteration 13/25 | Loss: 0.00072566
Iteration 14/25 | Loss: 0.00072566
Iteration 15/25 | Loss: 0.00072566
Iteration 16/25 | Loss: 0.00072566
Iteration 17/25 | Loss: 0.00072566
Iteration 18/25 | Loss: 0.00072566
Iteration 19/25 | Loss: 0.00072566
Iteration 20/25 | Loss: 0.00072566
Iteration 21/25 | Loss: 0.00072566
Iteration 22/25 | Loss: 0.00072566
Iteration 23/25 | Loss: 0.00072566
Iteration 24/25 | Loss: 0.00072566
Iteration 25/25 | Loss: 0.00072566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072566
Iteration 2/1000 | Loss: 0.00001914
Iteration 3/1000 | Loss: 0.00001462
Iteration 4/1000 | Loss: 0.00001350
Iteration 5/1000 | Loss: 0.00001283
Iteration 6/1000 | Loss: 0.00001250
Iteration 7/1000 | Loss: 0.00001211
Iteration 8/1000 | Loss: 0.00001194
Iteration 9/1000 | Loss: 0.00001192
Iteration 10/1000 | Loss: 0.00001187
Iteration 11/1000 | Loss: 0.00001170
Iteration 12/1000 | Loss: 0.00001165
Iteration 13/1000 | Loss: 0.00001160
Iteration 14/1000 | Loss: 0.00001155
Iteration 15/1000 | Loss: 0.00001154
Iteration 16/1000 | Loss: 0.00001153
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001145
Iteration 19/1000 | Loss: 0.00001133
Iteration 20/1000 | Loss: 0.00001131
Iteration 21/1000 | Loss: 0.00001125
Iteration 22/1000 | Loss: 0.00001121
Iteration 23/1000 | Loss: 0.00001120
Iteration 24/1000 | Loss: 0.00001120
Iteration 25/1000 | Loss: 0.00001120
Iteration 26/1000 | Loss: 0.00001119
Iteration 27/1000 | Loss: 0.00001118
Iteration 28/1000 | Loss: 0.00001117
Iteration 29/1000 | Loss: 0.00001116
Iteration 30/1000 | Loss: 0.00001116
Iteration 31/1000 | Loss: 0.00001116
Iteration 32/1000 | Loss: 0.00001116
Iteration 33/1000 | Loss: 0.00001116
Iteration 34/1000 | Loss: 0.00001115
Iteration 35/1000 | Loss: 0.00001115
Iteration 36/1000 | Loss: 0.00001115
Iteration 37/1000 | Loss: 0.00001114
Iteration 38/1000 | Loss: 0.00001113
Iteration 39/1000 | Loss: 0.00001113
Iteration 40/1000 | Loss: 0.00001112
Iteration 41/1000 | Loss: 0.00001112
Iteration 42/1000 | Loss: 0.00001111
Iteration 43/1000 | Loss: 0.00001111
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001108
Iteration 47/1000 | Loss: 0.00001107
Iteration 48/1000 | Loss: 0.00001107
Iteration 49/1000 | Loss: 0.00001106
Iteration 50/1000 | Loss: 0.00001105
Iteration 51/1000 | Loss: 0.00001104
Iteration 52/1000 | Loss: 0.00001103
Iteration 53/1000 | Loss: 0.00001103
Iteration 54/1000 | Loss: 0.00001103
Iteration 55/1000 | Loss: 0.00001102
Iteration 56/1000 | Loss: 0.00001102
Iteration 57/1000 | Loss: 0.00001102
Iteration 58/1000 | Loss: 0.00001101
Iteration 59/1000 | Loss: 0.00001101
Iteration 60/1000 | Loss: 0.00001100
Iteration 61/1000 | Loss: 0.00001099
Iteration 62/1000 | Loss: 0.00001098
Iteration 63/1000 | Loss: 0.00001098
Iteration 64/1000 | Loss: 0.00001098
Iteration 65/1000 | Loss: 0.00001098
Iteration 66/1000 | Loss: 0.00001098
Iteration 67/1000 | Loss: 0.00001098
Iteration 68/1000 | Loss: 0.00001098
Iteration 69/1000 | Loss: 0.00001098
Iteration 70/1000 | Loss: 0.00001097
Iteration 71/1000 | Loss: 0.00001097
Iteration 72/1000 | Loss: 0.00001095
Iteration 73/1000 | Loss: 0.00001094
Iteration 74/1000 | Loss: 0.00001094
Iteration 75/1000 | Loss: 0.00001093
Iteration 76/1000 | Loss: 0.00001093
Iteration 77/1000 | Loss: 0.00001093
Iteration 78/1000 | Loss: 0.00001093
Iteration 79/1000 | Loss: 0.00001092
Iteration 80/1000 | Loss: 0.00001092
Iteration 81/1000 | Loss: 0.00001091
Iteration 82/1000 | Loss: 0.00001090
Iteration 83/1000 | Loss: 0.00001090
Iteration 84/1000 | Loss: 0.00001090
Iteration 85/1000 | Loss: 0.00001090
Iteration 86/1000 | Loss: 0.00001090
Iteration 87/1000 | Loss: 0.00001089
Iteration 88/1000 | Loss: 0.00001089
Iteration 89/1000 | Loss: 0.00001089
Iteration 90/1000 | Loss: 0.00001088
Iteration 91/1000 | Loss: 0.00001088
Iteration 92/1000 | Loss: 0.00001087
Iteration 93/1000 | Loss: 0.00001087
Iteration 94/1000 | Loss: 0.00001086
Iteration 95/1000 | Loss: 0.00001086
Iteration 96/1000 | Loss: 0.00001086
Iteration 97/1000 | Loss: 0.00001086
Iteration 98/1000 | Loss: 0.00001086
Iteration 99/1000 | Loss: 0.00001086
Iteration 100/1000 | Loss: 0.00001086
Iteration 101/1000 | Loss: 0.00001086
Iteration 102/1000 | Loss: 0.00001086
Iteration 103/1000 | Loss: 0.00001086
Iteration 104/1000 | Loss: 0.00001086
Iteration 105/1000 | Loss: 0.00001086
Iteration 106/1000 | Loss: 0.00001086
Iteration 107/1000 | Loss: 0.00001086
Iteration 108/1000 | Loss: 0.00001086
Iteration 109/1000 | Loss: 0.00001085
Iteration 110/1000 | Loss: 0.00001085
Iteration 111/1000 | Loss: 0.00001085
Iteration 112/1000 | Loss: 0.00001084
Iteration 113/1000 | Loss: 0.00001084
Iteration 114/1000 | Loss: 0.00001083
Iteration 115/1000 | Loss: 0.00001083
Iteration 116/1000 | Loss: 0.00001082
Iteration 117/1000 | Loss: 0.00001082
Iteration 118/1000 | Loss: 0.00001082
Iteration 119/1000 | Loss: 0.00001082
Iteration 120/1000 | Loss: 0.00001082
Iteration 121/1000 | Loss: 0.00001082
Iteration 122/1000 | Loss: 0.00001082
Iteration 123/1000 | Loss: 0.00001082
Iteration 124/1000 | Loss: 0.00001082
Iteration 125/1000 | Loss: 0.00001081
Iteration 126/1000 | Loss: 0.00001081
Iteration 127/1000 | Loss: 0.00001081
Iteration 128/1000 | Loss: 0.00001080
Iteration 129/1000 | Loss: 0.00001080
Iteration 130/1000 | Loss: 0.00001080
Iteration 131/1000 | Loss: 0.00001080
Iteration 132/1000 | Loss: 0.00001079
Iteration 133/1000 | Loss: 0.00001079
Iteration 134/1000 | Loss: 0.00001079
Iteration 135/1000 | Loss: 0.00001079
Iteration 136/1000 | Loss: 0.00001079
Iteration 137/1000 | Loss: 0.00001079
Iteration 138/1000 | Loss: 0.00001079
Iteration 139/1000 | Loss: 0.00001079
Iteration 140/1000 | Loss: 0.00001079
Iteration 141/1000 | Loss: 0.00001079
Iteration 142/1000 | Loss: 0.00001079
Iteration 143/1000 | Loss: 0.00001078
Iteration 144/1000 | Loss: 0.00001078
Iteration 145/1000 | Loss: 0.00001078
Iteration 146/1000 | Loss: 0.00001078
Iteration 147/1000 | Loss: 0.00001078
Iteration 148/1000 | Loss: 0.00001077
Iteration 149/1000 | Loss: 0.00001077
Iteration 150/1000 | Loss: 0.00001077
Iteration 151/1000 | Loss: 0.00001077
Iteration 152/1000 | Loss: 0.00001077
Iteration 153/1000 | Loss: 0.00001077
Iteration 154/1000 | Loss: 0.00001077
Iteration 155/1000 | Loss: 0.00001077
Iteration 156/1000 | Loss: 0.00001076
Iteration 157/1000 | Loss: 0.00001076
Iteration 158/1000 | Loss: 0.00001076
Iteration 159/1000 | Loss: 0.00001076
Iteration 160/1000 | Loss: 0.00001076
Iteration 161/1000 | Loss: 0.00001076
Iteration 162/1000 | Loss: 0.00001076
Iteration 163/1000 | Loss: 0.00001076
Iteration 164/1000 | Loss: 0.00001076
Iteration 165/1000 | Loss: 0.00001076
Iteration 166/1000 | Loss: 0.00001076
Iteration 167/1000 | Loss: 0.00001076
Iteration 168/1000 | Loss: 0.00001076
Iteration 169/1000 | Loss: 0.00001076
Iteration 170/1000 | Loss: 0.00001076
Iteration 171/1000 | Loss: 0.00001076
Iteration 172/1000 | Loss: 0.00001076
Iteration 173/1000 | Loss: 0.00001076
Iteration 174/1000 | Loss: 0.00001076
Iteration 175/1000 | Loss: 0.00001076
Iteration 176/1000 | Loss: 0.00001076
Iteration 177/1000 | Loss: 0.00001076
Iteration 178/1000 | Loss: 0.00001076
Iteration 179/1000 | Loss: 0.00001076
Iteration 180/1000 | Loss: 0.00001076
Iteration 181/1000 | Loss: 0.00001076
Iteration 182/1000 | Loss: 0.00001076
Iteration 183/1000 | Loss: 0.00001076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.0760468285297975e-05, 1.0760468285297975e-05, 1.0760468285297975e-05, 1.0760468285297975e-05, 1.0760468285297975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0760468285297975e-05

Optimization complete. Final v2v error: 2.800079822540283 mm

Highest mean error: 3.1343796253204346 mm for frame 114

Lowest mean error: 2.4522485733032227 mm for frame 139

Saving results

Total time: 37.78330087661743
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00930770
Iteration 2/25 | Loss: 0.00216973
Iteration 3/25 | Loss: 0.00148505
Iteration 4/25 | Loss: 0.00137127
Iteration 5/25 | Loss: 0.00132567
Iteration 6/25 | Loss: 0.00129970
Iteration 7/25 | Loss: 0.00127953
Iteration 8/25 | Loss: 0.00127150
Iteration 9/25 | Loss: 0.00126057
Iteration 10/25 | Loss: 0.00125912
Iteration 11/25 | Loss: 0.00125768
Iteration 12/25 | Loss: 0.00125710
Iteration 13/25 | Loss: 0.00125641
Iteration 14/25 | Loss: 0.00125630
Iteration 15/25 | Loss: 0.00125629
Iteration 16/25 | Loss: 0.00125628
Iteration 17/25 | Loss: 0.00125628
Iteration 18/25 | Loss: 0.00125628
Iteration 19/25 | Loss: 0.00125628
Iteration 20/25 | Loss: 0.00125628
Iteration 21/25 | Loss: 0.00125628
Iteration 22/25 | Loss: 0.00125628
Iteration 23/25 | Loss: 0.00125628
Iteration 24/25 | Loss: 0.00125627
Iteration 25/25 | Loss: 0.00125627

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.88817692
Iteration 2/25 | Loss: 0.00078086
Iteration 3/25 | Loss: 0.00066945
Iteration 4/25 | Loss: 0.00066945
Iteration 5/25 | Loss: 0.00066945
Iteration 6/25 | Loss: 0.00066945
Iteration 7/25 | Loss: 0.00066945
Iteration 8/25 | Loss: 0.00066945
Iteration 9/25 | Loss: 0.00066945
Iteration 10/25 | Loss: 0.00066945
Iteration 11/25 | Loss: 0.00066945
Iteration 12/25 | Loss: 0.00066945
Iteration 13/25 | Loss: 0.00066945
Iteration 14/25 | Loss: 0.00066945
Iteration 15/25 | Loss: 0.00066945
Iteration 16/25 | Loss: 0.00066945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006694468320347369, 0.0006694468320347369, 0.0006694468320347369, 0.0006694468320347369, 0.0006694468320347369]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006694468320347369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066945
Iteration 2/1000 | Loss: 0.00012359
Iteration 3/1000 | Loss: 0.00016877
Iteration 4/1000 | Loss: 0.00002983
Iteration 5/1000 | Loss: 0.00002800
Iteration 6/1000 | Loss: 0.00002666
Iteration 7/1000 | Loss: 0.00002598
Iteration 8/1000 | Loss: 0.00014460
Iteration 9/1000 | Loss: 0.00012117
Iteration 10/1000 | Loss: 0.00003410
Iteration 11/1000 | Loss: 0.00002876
Iteration 12/1000 | Loss: 0.00002573
Iteration 13/1000 | Loss: 0.00002462
Iteration 14/1000 | Loss: 0.00012465
Iteration 15/1000 | Loss: 0.00003763
Iteration 16/1000 | Loss: 0.00003196
Iteration 17/1000 | Loss: 0.00002327
Iteration 18/1000 | Loss: 0.00002291
Iteration 19/1000 | Loss: 0.00002291
Iteration 20/1000 | Loss: 0.00008658
Iteration 21/1000 | Loss: 0.00002392
Iteration 22/1000 | Loss: 0.00002276
Iteration 23/1000 | Loss: 0.00004656
Iteration 24/1000 | Loss: 0.00002267
Iteration 25/1000 | Loss: 0.00002251
Iteration 26/1000 | Loss: 0.00002250
Iteration 27/1000 | Loss: 0.00002245
Iteration 28/1000 | Loss: 0.00002245
Iteration 29/1000 | Loss: 0.00002243
Iteration 30/1000 | Loss: 0.00002241
Iteration 31/1000 | Loss: 0.00002241
Iteration 32/1000 | Loss: 0.00002241
Iteration 33/1000 | Loss: 0.00002240
Iteration 34/1000 | Loss: 0.00002239
Iteration 35/1000 | Loss: 0.00002239
Iteration 36/1000 | Loss: 0.00002239
Iteration 37/1000 | Loss: 0.00002238
Iteration 38/1000 | Loss: 0.00002238
Iteration 39/1000 | Loss: 0.00002237
Iteration 40/1000 | Loss: 0.00002237
Iteration 41/1000 | Loss: 0.00002236
Iteration 42/1000 | Loss: 0.00002233
Iteration 43/1000 | Loss: 0.00002233
Iteration 44/1000 | Loss: 0.00002231
Iteration 45/1000 | Loss: 0.00002231
Iteration 46/1000 | Loss: 0.00002230
Iteration 47/1000 | Loss: 0.00002230
Iteration 48/1000 | Loss: 0.00002230
Iteration 49/1000 | Loss: 0.00002230
Iteration 50/1000 | Loss: 0.00002230
Iteration 51/1000 | Loss: 0.00002230
Iteration 52/1000 | Loss: 0.00002230
Iteration 53/1000 | Loss: 0.00002230
Iteration 54/1000 | Loss: 0.00002230
Iteration 55/1000 | Loss: 0.00002230
Iteration 56/1000 | Loss: 0.00002230
Iteration 57/1000 | Loss: 0.00002229
Iteration 58/1000 | Loss: 0.00002229
Iteration 59/1000 | Loss: 0.00002229
Iteration 60/1000 | Loss: 0.00002229
Iteration 61/1000 | Loss: 0.00002229
Iteration 62/1000 | Loss: 0.00002228
Iteration 63/1000 | Loss: 0.00002228
Iteration 64/1000 | Loss: 0.00002228
Iteration 65/1000 | Loss: 0.00002228
Iteration 66/1000 | Loss: 0.00002228
Iteration 67/1000 | Loss: 0.00002228
Iteration 68/1000 | Loss: 0.00002228
Iteration 69/1000 | Loss: 0.00002227
Iteration 70/1000 | Loss: 0.00002227
Iteration 71/1000 | Loss: 0.00002227
Iteration 72/1000 | Loss: 0.00002227
Iteration 73/1000 | Loss: 0.00002227
Iteration 74/1000 | Loss: 0.00002227
Iteration 75/1000 | Loss: 0.00002227
Iteration 76/1000 | Loss: 0.00002227
Iteration 77/1000 | Loss: 0.00002227
Iteration 78/1000 | Loss: 0.00002227
Iteration 79/1000 | Loss: 0.00002227
Iteration 80/1000 | Loss: 0.00002226
Iteration 81/1000 | Loss: 0.00002226
Iteration 82/1000 | Loss: 0.00002226
Iteration 83/1000 | Loss: 0.00002226
Iteration 84/1000 | Loss: 0.00002226
Iteration 85/1000 | Loss: 0.00002226
Iteration 86/1000 | Loss: 0.00002226
Iteration 87/1000 | Loss: 0.00002226
Iteration 88/1000 | Loss: 0.00002226
Iteration 89/1000 | Loss: 0.00002226
Iteration 90/1000 | Loss: 0.00002226
Iteration 91/1000 | Loss: 0.00002226
Iteration 92/1000 | Loss: 0.00002226
Iteration 93/1000 | Loss: 0.00002226
Iteration 94/1000 | Loss: 0.00002226
Iteration 95/1000 | Loss: 0.00002226
Iteration 96/1000 | Loss: 0.00002226
Iteration 97/1000 | Loss: 0.00002226
Iteration 98/1000 | Loss: 0.00002226
Iteration 99/1000 | Loss: 0.00002226
Iteration 100/1000 | Loss: 0.00002226
Iteration 101/1000 | Loss: 0.00002226
Iteration 102/1000 | Loss: 0.00002226
Iteration 103/1000 | Loss: 0.00002226
Iteration 104/1000 | Loss: 0.00002226
Iteration 105/1000 | Loss: 0.00002226
Iteration 106/1000 | Loss: 0.00002226
Iteration 107/1000 | Loss: 0.00002225
Iteration 108/1000 | Loss: 0.00002225
Iteration 109/1000 | Loss: 0.00002225
Iteration 110/1000 | Loss: 0.00002225
Iteration 111/1000 | Loss: 0.00002225
Iteration 112/1000 | Loss: 0.00002225
Iteration 113/1000 | Loss: 0.00002225
Iteration 114/1000 | Loss: 0.00002225
Iteration 115/1000 | Loss: 0.00002225
Iteration 116/1000 | Loss: 0.00002225
Iteration 117/1000 | Loss: 0.00002225
Iteration 118/1000 | Loss: 0.00002225
Iteration 119/1000 | Loss: 0.00002225
Iteration 120/1000 | Loss: 0.00002225
Iteration 121/1000 | Loss: 0.00002225
Iteration 122/1000 | Loss: 0.00002225
Iteration 123/1000 | Loss: 0.00002225
Iteration 124/1000 | Loss: 0.00002225
Iteration 125/1000 | Loss: 0.00002225
Iteration 126/1000 | Loss: 0.00002225
Iteration 127/1000 | Loss: 0.00002225
Iteration 128/1000 | Loss: 0.00002225
Iteration 129/1000 | Loss: 0.00002225
Iteration 130/1000 | Loss: 0.00002225
Iteration 131/1000 | Loss: 0.00002225
Iteration 132/1000 | Loss: 0.00002225
Iteration 133/1000 | Loss: 0.00002225
Iteration 134/1000 | Loss: 0.00002225
Iteration 135/1000 | Loss: 0.00002225
Iteration 136/1000 | Loss: 0.00002225
Iteration 137/1000 | Loss: 0.00002225
Iteration 138/1000 | Loss: 0.00002225
Iteration 139/1000 | Loss: 0.00002225
Iteration 140/1000 | Loss: 0.00002225
Iteration 141/1000 | Loss: 0.00002225
Iteration 142/1000 | Loss: 0.00002225
Iteration 143/1000 | Loss: 0.00002225
Iteration 144/1000 | Loss: 0.00002225
Iteration 145/1000 | Loss: 0.00002225
Iteration 146/1000 | Loss: 0.00002225
Iteration 147/1000 | Loss: 0.00002225
Iteration 148/1000 | Loss: 0.00002225
Iteration 149/1000 | Loss: 0.00002225
Iteration 150/1000 | Loss: 0.00002225
Iteration 151/1000 | Loss: 0.00002225
Iteration 152/1000 | Loss: 0.00002225
Iteration 153/1000 | Loss: 0.00002225
Iteration 154/1000 | Loss: 0.00002225
Iteration 155/1000 | Loss: 0.00002225
Iteration 156/1000 | Loss: 0.00002225
Iteration 157/1000 | Loss: 0.00002225
Iteration 158/1000 | Loss: 0.00002225
Iteration 159/1000 | Loss: 0.00002225
Iteration 160/1000 | Loss: 0.00002225
Iteration 161/1000 | Loss: 0.00002225
Iteration 162/1000 | Loss: 0.00002225
Iteration 163/1000 | Loss: 0.00002225
Iteration 164/1000 | Loss: 0.00002225
Iteration 165/1000 | Loss: 0.00002225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [2.2253429051488638e-05, 2.2253429051488638e-05, 2.2253429051488638e-05, 2.2253429051488638e-05, 2.2253429051488638e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2253429051488638e-05

Optimization complete. Final v2v error: 3.636915683746338 mm

Highest mean error: 11.277881622314453 mm for frame 7

Lowest mean error: 3.2939696311950684 mm for frame 151

Saving results

Total time: 75.89796543121338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029334
Iteration 2/25 | Loss: 0.00390869
Iteration 3/25 | Loss: 0.00373876
Iteration 4/25 | Loss: 0.00197328
Iteration 5/25 | Loss: 0.00163129
Iteration 6/25 | Loss: 0.00155884
Iteration 7/25 | Loss: 0.00139266
Iteration 8/25 | Loss: 0.00127360
Iteration 9/25 | Loss: 0.00124302
Iteration 10/25 | Loss: 0.00118947
Iteration 11/25 | Loss: 0.00116221
Iteration 12/25 | Loss: 0.00116362
Iteration 13/25 | Loss: 0.00115702
Iteration 14/25 | Loss: 0.00116029
Iteration 15/25 | Loss: 0.00116164
Iteration 16/25 | Loss: 0.00115991
Iteration 17/25 | Loss: 0.00115869
Iteration 18/25 | Loss: 0.00115652
Iteration 19/25 | Loss: 0.00115650
Iteration 20/25 | Loss: 0.00115650
Iteration 21/25 | Loss: 0.00115650
Iteration 22/25 | Loss: 0.00115650
Iteration 23/25 | Loss: 0.00115937
Iteration 24/25 | Loss: 0.00115651
Iteration 25/25 | Loss: 0.00115648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36200500
Iteration 2/25 | Loss: 0.00065427
Iteration 3/25 | Loss: 0.00065427
Iteration 4/25 | Loss: 0.00065427
Iteration 5/25 | Loss: 0.00065427
Iteration 6/25 | Loss: 0.00065427
Iteration 7/25 | Loss: 0.00065427
Iteration 8/25 | Loss: 0.00065427
Iteration 9/25 | Loss: 0.00065427
Iteration 10/25 | Loss: 0.00065427
Iteration 11/25 | Loss: 0.00065427
Iteration 12/25 | Loss: 0.00065427
Iteration 13/25 | Loss: 0.00065427
Iteration 14/25 | Loss: 0.00065427
Iteration 15/25 | Loss: 0.00065427
Iteration 16/25 | Loss: 0.00065427
Iteration 17/25 | Loss: 0.00065427
Iteration 18/25 | Loss: 0.00065427
Iteration 19/25 | Loss: 0.00065427
Iteration 20/25 | Loss: 0.00065427
Iteration 21/25 | Loss: 0.00065427
Iteration 22/25 | Loss: 0.00065427
Iteration 23/25 | Loss: 0.00065427
Iteration 24/25 | Loss: 0.00065427
Iteration 25/25 | Loss: 0.00065427

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065427
Iteration 2/1000 | Loss: 0.00003186
Iteration 3/1000 | Loss: 0.00015387
Iteration 4/1000 | Loss: 0.00002310
Iteration 5/1000 | Loss: 0.00002232
Iteration 6/1000 | Loss: 0.00010659
Iteration 7/1000 | Loss: 0.00013831
Iteration 8/1000 | Loss: 0.00005737
Iteration 9/1000 | Loss: 0.00010736
Iteration 10/1000 | Loss: 0.00002111
Iteration 11/1000 | Loss: 0.00002070
Iteration 12/1000 | Loss: 0.00002033
Iteration 13/1000 | Loss: 0.00010580
Iteration 14/1000 | Loss: 0.00020449
Iteration 15/1000 | Loss: 0.00002757
Iteration 16/1000 | Loss: 0.00002103
Iteration 17/1000 | Loss: 0.00002008
Iteration 18/1000 | Loss: 0.00011911
Iteration 19/1000 | Loss: 0.00049208
Iteration 20/1000 | Loss: 0.00003027
Iteration 21/1000 | Loss: 0.00002347
Iteration 22/1000 | Loss: 0.00002074
Iteration 23/1000 | Loss: 0.00001981
Iteration 24/1000 | Loss: 0.00001951
Iteration 25/1000 | Loss: 0.00011872
Iteration 26/1000 | Loss: 0.00026632
Iteration 27/1000 | Loss: 0.00002251
Iteration 28/1000 | Loss: 0.00008701
Iteration 29/1000 | Loss: 0.00001945
Iteration 30/1000 | Loss: 0.00001924
Iteration 31/1000 | Loss: 0.00001924
Iteration 32/1000 | Loss: 0.00001916
Iteration 33/1000 | Loss: 0.00001913
Iteration 34/1000 | Loss: 0.00001913
Iteration 35/1000 | Loss: 0.00001913
Iteration 36/1000 | Loss: 0.00001913
Iteration 37/1000 | Loss: 0.00001912
Iteration 38/1000 | Loss: 0.00011172
Iteration 39/1000 | Loss: 0.00001961
Iteration 40/1000 | Loss: 0.00001904
Iteration 41/1000 | Loss: 0.00001900
Iteration 42/1000 | Loss: 0.00001899
Iteration 43/1000 | Loss: 0.00001898
Iteration 44/1000 | Loss: 0.00001898
Iteration 45/1000 | Loss: 0.00001897
Iteration 46/1000 | Loss: 0.00001897
Iteration 47/1000 | Loss: 0.00001896
Iteration 48/1000 | Loss: 0.00001896
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001895
Iteration 52/1000 | Loss: 0.00001895
Iteration 53/1000 | Loss: 0.00001895
Iteration 54/1000 | Loss: 0.00001894
Iteration 55/1000 | Loss: 0.00001894
Iteration 56/1000 | Loss: 0.00001894
Iteration 57/1000 | Loss: 0.00001894
Iteration 58/1000 | Loss: 0.00001894
Iteration 59/1000 | Loss: 0.00001894
Iteration 60/1000 | Loss: 0.00001893
Iteration 61/1000 | Loss: 0.00001893
Iteration 62/1000 | Loss: 0.00001893
Iteration 63/1000 | Loss: 0.00001893
Iteration 64/1000 | Loss: 0.00001892
Iteration 65/1000 | Loss: 0.00001892
Iteration 66/1000 | Loss: 0.00001892
Iteration 67/1000 | Loss: 0.00001891
Iteration 68/1000 | Loss: 0.00001891
Iteration 69/1000 | Loss: 0.00001891
Iteration 70/1000 | Loss: 0.00001890
Iteration 71/1000 | Loss: 0.00001890
Iteration 72/1000 | Loss: 0.00001890
Iteration 73/1000 | Loss: 0.00001890
Iteration 74/1000 | Loss: 0.00001890
Iteration 75/1000 | Loss: 0.00001889
Iteration 76/1000 | Loss: 0.00001889
Iteration 77/1000 | Loss: 0.00001889
Iteration 78/1000 | Loss: 0.00001889
Iteration 79/1000 | Loss: 0.00001888
Iteration 80/1000 | Loss: 0.00001888
Iteration 81/1000 | Loss: 0.00001887
Iteration 82/1000 | Loss: 0.00001887
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001886
Iteration 85/1000 | Loss: 0.00001885
Iteration 86/1000 | Loss: 0.00001884
Iteration 87/1000 | Loss: 0.00001883
Iteration 88/1000 | Loss: 0.00022926
Iteration 89/1000 | Loss: 0.00007405
Iteration 90/1000 | Loss: 0.00002098
Iteration 91/1000 | Loss: 0.00006239
Iteration 92/1000 | Loss: 0.00001905
Iteration 93/1000 | Loss: 0.00001886
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001881
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001880
Iteration 100/1000 | Loss: 0.00001880
Iteration 101/1000 | Loss: 0.00001880
Iteration 102/1000 | Loss: 0.00001880
Iteration 103/1000 | Loss: 0.00001880
Iteration 104/1000 | Loss: 0.00001880
Iteration 105/1000 | Loss: 0.00001880
Iteration 106/1000 | Loss: 0.00011581
Iteration 107/1000 | Loss: 0.00007186
Iteration 108/1000 | Loss: 0.00005010
Iteration 109/1000 | Loss: 0.00001925
Iteration 110/1000 | Loss: 0.00011702
Iteration 111/1000 | Loss: 0.00001957
Iteration 112/1000 | Loss: 0.00001890
Iteration 113/1000 | Loss: 0.00001886
Iteration 114/1000 | Loss: 0.00001882
Iteration 115/1000 | Loss: 0.00008369
Iteration 116/1000 | Loss: 0.00003985
Iteration 117/1000 | Loss: 0.00001880
Iteration 118/1000 | Loss: 0.00001880
Iteration 119/1000 | Loss: 0.00001880
Iteration 120/1000 | Loss: 0.00001880
Iteration 121/1000 | Loss: 0.00001880
Iteration 122/1000 | Loss: 0.00001880
Iteration 123/1000 | Loss: 0.00001880
Iteration 124/1000 | Loss: 0.00001880
Iteration 125/1000 | Loss: 0.00001879
Iteration 126/1000 | Loss: 0.00001879
Iteration 127/1000 | Loss: 0.00001879
Iteration 128/1000 | Loss: 0.00001879
Iteration 129/1000 | Loss: 0.00001879
Iteration 130/1000 | Loss: 0.00006225
Iteration 131/1000 | Loss: 0.00012411
Iteration 132/1000 | Loss: 0.00006064
Iteration 133/1000 | Loss: 0.00003676
Iteration 134/1000 | Loss: 0.00001911
Iteration 135/1000 | Loss: 0.00006149
Iteration 136/1000 | Loss: 0.00001896
Iteration 137/1000 | Loss: 0.00001879
Iteration 138/1000 | Loss: 0.00001879
Iteration 139/1000 | Loss: 0.00001879
Iteration 140/1000 | Loss: 0.00001879
Iteration 141/1000 | Loss: 0.00001879
Iteration 142/1000 | Loss: 0.00001876
Iteration 143/1000 | Loss: 0.00001876
Iteration 144/1000 | Loss: 0.00001874
Iteration 145/1000 | Loss: 0.00001874
Iteration 146/1000 | Loss: 0.00001874
Iteration 147/1000 | Loss: 0.00001874
Iteration 148/1000 | Loss: 0.00001874
Iteration 149/1000 | Loss: 0.00001874
Iteration 150/1000 | Loss: 0.00001874
Iteration 151/1000 | Loss: 0.00001874
Iteration 152/1000 | Loss: 0.00001874
Iteration 153/1000 | Loss: 0.00001874
Iteration 154/1000 | Loss: 0.00001874
Iteration 155/1000 | Loss: 0.00001874
Iteration 156/1000 | Loss: 0.00001874
Iteration 157/1000 | Loss: 0.00001874
Iteration 158/1000 | Loss: 0.00001874
Iteration 159/1000 | Loss: 0.00001874
Iteration 160/1000 | Loss: 0.00001874
Iteration 161/1000 | Loss: 0.00001874
Iteration 162/1000 | Loss: 0.00001874
Iteration 163/1000 | Loss: 0.00001874
Iteration 164/1000 | Loss: 0.00001874
Iteration 165/1000 | Loss: 0.00001874
Iteration 166/1000 | Loss: 0.00001874
Iteration 167/1000 | Loss: 0.00001874
Iteration 168/1000 | Loss: 0.00001874
Iteration 169/1000 | Loss: 0.00001874
Iteration 170/1000 | Loss: 0.00001874
Iteration 171/1000 | Loss: 0.00001874
Iteration 172/1000 | Loss: 0.00001874
Iteration 173/1000 | Loss: 0.00001874
Iteration 174/1000 | Loss: 0.00001874
Iteration 175/1000 | Loss: 0.00001874
Iteration 176/1000 | Loss: 0.00001874
Iteration 177/1000 | Loss: 0.00001874
Iteration 178/1000 | Loss: 0.00001874
Iteration 179/1000 | Loss: 0.00001874
Iteration 180/1000 | Loss: 0.00001874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.8739958250080235e-05, 1.8739958250080235e-05, 1.8739958250080235e-05, 1.8739958250080235e-05, 1.8739958250080235e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8739958250080235e-05

Optimization complete. Final v2v error: 3.6975927352905273 mm

Highest mean error: 4.048759460449219 mm for frame 20

Lowest mean error: 3.527134418487549 mm for frame 58

Saving results

Total time: 137.60461044311523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814203
Iteration 2/25 | Loss: 0.00142667
Iteration 3/25 | Loss: 0.00117873
Iteration 4/25 | Loss: 0.00116285
Iteration 5/25 | Loss: 0.00116152
Iteration 6/25 | Loss: 0.00116130
Iteration 7/25 | Loss: 0.00116130
Iteration 8/25 | Loss: 0.00116130
Iteration 9/25 | Loss: 0.00116130
Iteration 10/25 | Loss: 0.00116130
Iteration 11/25 | Loss: 0.00116130
Iteration 12/25 | Loss: 0.00116130
Iteration 13/25 | Loss: 0.00116130
Iteration 14/25 | Loss: 0.00116130
Iteration 15/25 | Loss: 0.00116130
Iteration 16/25 | Loss: 0.00116130
Iteration 17/25 | Loss: 0.00116130
Iteration 18/25 | Loss: 0.00116130
Iteration 19/25 | Loss: 0.00116130
Iteration 20/25 | Loss: 0.00116130
Iteration 21/25 | Loss: 0.00116130
Iteration 22/25 | Loss: 0.00116130
Iteration 23/25 | Loss: 0.00116130
Iteration 24/25 | Loss: 0.00116130
Iteration 25/25 | Loss: 0.00116130

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27293086
Iteration 2/25 | Loss: 0.00064712
Iteration 3/25 | Loss: 0.00064711
Iteration 4/25 | Loss: 0.00064710
Iteration 5/25 | Loss: 0.00064710
Iteration 6/25 | Loss: 0.00064710
Iteration 7/25 | Loss: 0.00064710
Iteration 8/25 | Loss: 0.00064710
Iteration 9/25 | Loss: 0.00064710
Iteration 10/25 | Loss: 0.00064710
Iteration 11/25 | Loss: 0.00064710
Iteration 12/25 | Loss: 0.00064710
Iteration 13/25 | Loss: 0.00064710
Iteration 14/25 | Loss: 0.00064710
Iteration 15/25 | Loss: 0.00064710
Iteration 16/25 | Loss: 0.00064710
Iteration 17/25 | Loss: 0.00064710
Iteration 18/25 | Loss: 0.00064710
Iteration 19/25 | Loss: 0.00064710
Iteration 20/25 | Loss: 0.00064710
Iteration 21/25 | Loss: 0.00064710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006471024826169014, 0.0006471024826169014, 0.0006471024826169014, 0.0006471024826169014, 0.0006471024826169014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006471024826169014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064710
Iteration 2/1000 | Loss: 0.00003265
Iteration 3/1000 | Loss: 0.00002222
Iteration 4/1000 | Loss: 0.00001945
Iteration 5/1000 | Loss: 0.00001848
Iteration 6/1000 | Loss: 0.00001801
Iteration 7/1000 | Loss: 0.00001755
Iteration 8/1000 | Loss: 0.00001734
Iteration 9/1000 | Loss: 0.00001707
Iteration 10/1000 | Loss: 0.00001690
Iteration 11/1000 | Loss: 0.00001683
Iteration 12/1000 | Loss: 0.00001672
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00001667
Iteration 15/1000 | Loss: 0.00001664
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001662
Iteration 19/1000 | Loss: 0.00001660
Iteration 20/1000 | Loss: 0.00001660
Iteration 21/1000 | Loss: 0.00001658
Iteration 22/1000 | Loss: 0.00001657
Iteration 23/1000 | Loss: 0.00001657
Iteration 24/1000 | Loss: 0.00001657
Iteration 25/1000 | Loss: 0.00001657
Iteration 26/1000 | Loss: 0.00001657
Iteration 27/1000 | Loss: 0.00001657
Iteration 28/1000 | Loss: 0.00001657
Iteration 29/1000 | Loss: 0.00001657
Iteration 30/1000 | Loss: 0.00001647
Iteration 31/1000 | Loss: 0.00001647
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001644
Iteration 34/1000 | Loss: 0.00001643
Iteration 35/1000 | Loss: 0.00001643
Iteration 36/1000 | Loss: 0.00001642
Iteration 37/1000 | Loss: 0.00001642
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001642
Iteration 41/1000 | Loss: 0.00001642
Iteration 42/1000 | Loss: 0.00001642
Iteration 43/1000 | Loss: 0.00001641
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001640
Iteration 46/1000 | Loss: 0.00001640
Iteration 47/1000 | Loss: 0.00001640
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001640
Iteration 50/1000 | Loss: 0.00001639
Iteration 51/1000 | Loss: 0.00001639
Iteration 52/1000 | Loss: 0.00001639
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001638
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001636
Iteration 58/1000 | Loss: 0.00001636
Iteration 59/1000 | Loss: 0.00001636
Iteration 60/1000 | Loss: 0.00001636
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001635
Iteration 65/1000 | Loss: 0.00001635
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001635
Iteration 68/1000 | Loss: 0.00001635
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001634
Iteration 72/1000 | Loss: 0.00001634
Iteration 73/1000 | Loss: 0.00001634
Iteration 74/1000 | Loss: 0.00001634
Iteration 75/1000 | Loss: 0.00001634
Iteration 76/1000 | Loss: 0.00001634
Iteration 77/1000 | Loss: 0.00001634
Iteration 78/1000 | Loss: 0.00001634
Iteration 79/1000 | Loss: 0.00001634
Iteration 80/1000 | Loss: 0.00001633
Iteration 81/1000 | Loss: 0.00001633
Iteration 82/1000 | Loss: 0.00001633
Iteration 83/1000 | Loss: 0.00001632
Iteration 84/1000 | Loss: 0.00001632
Iteration 85/1000 | Loss: 0.00001632
Iteration 86/1000 | Loss: 0.00001632
Iteration 87/1000 | Loss: 0.00001632
Iteration 88/1000 | Loss: 0.00001632
Iteration 89/1000 | Loss: 0.00001632
Iteration 90/1000 | Loss: 0.00001631
Iteration 91/1000 | Loss: 0.00001631
Iteration 92/1000 | Loss: 0.00001631
Iteration 93/1000 | Loss: 0.00001631
Iteration 94/1000 | Loss: 0.00001631
Iteration 95/1000 | Loss: 0.00001631
Iteration 96/1000 | Loss: 0.00001631
Iteration 97/1000 | Loss: 0.00001631
Iteration 98/1000 | Loss: 0.00001631
Iteration 99/1000 | Loss: 0.00001630
Iteration 100/1000 | Loss: 0.00001630
Iteration 101/1000 | Loss: 0.00001630
Iteration 102/1000 | Loss: 0.00001630
Iteration 103/1000 | Loss: 0.00001630
Iteration 104/1000 | Loss: 0.00001630
Iteration 105/1000 | Loss: 0.00001630
Iteration 106/1000 | Loss: 0.00001630
Iteration 107/1000 | Loss: 0.00001629
Iteration 108/1000 | Loss: 0.00001629
Iteration 109/1000 | Loss: 0.00001629
Iteration 110/1000 | Loss: 0.00001629
Iteration 111/1000 | Loss: 0.00001629
Iteration 112/1000 | Loss: 0.00001629
Iteration 113/1000 | Loss: 0.00001628
Iteration 114/1000 | Loss: 0.00001628
Iteration 115/1000 | Loss: 0.00001628
Iteration 116/1000 | Loss: 0.00001628
Iteration 117/1000 | Loss: 0.00001628
Iteration 118/1000 | Loss: 0.00001628
Iteration 119/1000 | Loss: 0.00001628
Iteration 120/1000 | Loss: 0.00001628
Iteration 121/1000 | Loss: 0.00001628
Iteration 122/1000 | Loss: 0.00001628
Iteration 123/1000 | Loss: 0.00001627
Iteration 124/1000 | Loss: 0.00001627
Iteration 125/1000 | Loss: 0.00001627
Iteration 126/1000 | Loss: 0.00001627
Iteration 127/1000 | Loss: 0.00001627
Iteration 128/1000 | Loss: 0.00001627
Iteration 129/1000 | Loss: 0.00001627
Iteration 130/1000 | Loss: 0.00001627
Iteration 131/1000 | Loss: 0.00001627
Iteration 132/1000 | Loss: 0.00001627
Iteration 133/1000 | Loss: 0.00001627
Iteration 134/1000 | Loss: 0.00001627
Iteration 135/1000 | Loss: 0.00001626
Iteration 136/1000 | Loss: 0.00001626
Iteration 137/1000 | Loss: 0.00001626
Iteration 138/1000 | Loss: 0.00001626
Iteration 139/1000 | Loss: 0.00001626
Iteration 140/1000 | Loss: 0.00001626
Iteration 141/1000 | Loss: 0.00001626
Iteration 142/1000 | Loss: 0.00001626
Iteration 143/1000 | Loss: 0.00001626
Iteration 144/1000 | Loss: 0.00001626
Iteration 145/1000 | Loss: 0.00001626
Iteration 146/1000 | Loss: 0.00001625
Iteration 147/1000 | Loss: 0.00001625
Iteration 148/1000 | Loss: 0.00001625
Iteration 149/1000 | Loss: 0.00001625
Iteration 150/1000 | Loss: 0.00001625
Iteration 151/1000 | Loss: 0.00001625
Iteration 152/1000 | Loss: 0.00001625
Iteration 153/1000 | Loss: 0.00001625
Iteration 154/1000 | Loss: 0.00001625
Iteration 155/1000 | Loss: 0.00001624
Iteration 156/1000 | Loss: 0.00001624
Iteration 157/1000 | Loss: 0.00001624
Iteration 158/1000 | Loss: 0.00001624
Iteration 159/1000 | Loss: 0.00001624
Iteration 160/1000 | Loss: 0.00001624
Iteration 161/1000 | Loss: 0.00001624
Iteration 162/1000 | Loss: 0.00001624
Iteration 163/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.624418655410409e-05, 1.624418655410409e-05, 1.624418655410409e-05, 1.624418655410409e-05, 1.624418655410409e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.624418655410409e-05

Optimization complete. Final v2v error: 3.3896121978759766 mm

Highest mean error: 3.8146848678588867 mm for frame 57

Lowest mean error: 3.157578229904175 mm for frame 5

Saving results

Total time: 36.16953802108765
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00339219
Iteration 2/25 | Loss: 0.00135334
Iteration 3/25 | Loss: 0.00112457
Iteration 4/25 | Loss: 0.00107842
Iteration 5/25 | Loss: 0.00107059
Iteration 6/25 | Loss: 0.00106774
Iteration 7/25 | Loss: 0.00106578
Iteration 8/25 | Loss: 0.00106419
Iteration 9/25 | Loss: 0.00106196
Iteration 10/25 | Loss: 0.00106165
Iteration 11/25 | Loss: 0.00106162
Iteration 12/25 | Loss: 0.00106161
Iteration 13/25 | Loss: 0.00106161
Iteration 14/25 | Loss: 0.00106161
Iteration 15/25 | Loss: 0.00106159
Iteration 16/25 | Loss: 0.00106159
Iteration 17/25 | Loss: 0.00106159
Iteration 18/25 | Loss: 0.00106158
Iteration 19/25 | Loss: 0.00106158
Iteration 20/25 | Loss: 0.00106158
Iteration 21/25 | Loss: 0.00106158
Iteration 22/25 | Loss: 0.00106158
Iteration 23/25 | Loss: 0.00106158
Iteration 24/25 | Loss: 0.00106158
Iteration 25/25 | Loss: 0.00106158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37523186
Iteration 2/25 | Loss: 0.00084760
Iteration 3/25 | Loss: 0.00084760
Iteration 4/25 | Loss: 0.00084760
Iteration 5/25 | Loss: 0.00084760
Iteration 6/25 | Loss: 0.00084760
Iteration 7/25 | Loss: 0.00084760
Iteration 8/25 | Loss: 0.00084760
Iteration 9/25 | Loss: 0.00084760
Iteration 10/25 | Loss: 0.00084760
Iteration 11/25 | Loss: 0.00084760
Iteration 12/25 | Loss: 0.00084760
Iteration 13/25 | Loss: 0.00084760
Iteration 14/25 | Loss: 0.00084760
Iteration 15/25 | Loss: 0.00084760
Iteration 16/25 | Loss: 0.00084760
Iteration 17/25 | Loss: 0.00084760
Iteration 18/25 | Loss: 0.00084760
Iteration 19/25 | Loss: 0.00084760
Iteration 20/25 | Loss: 0.00084760
Iteration 21/25 | Loss: 0.00084760
Iteration 22/25 | Loss: 0.00084760
Iteration 23/25 | Loss: 0.00084760
Iteration 24/25 | Loss: 0.00084760
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008476002840325236, 0.0008476002840325236, 0.0008476002840325236, 0.0008476002840325236, 0.0008476002840325236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008476002840325236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084760
Iteration 2/1000 | Loss: 0.00004429
Iteration 3/1000 | Loss: 0.00002483
Iteration 4/1000 | Loss: 0.00001868
Iteration 5/1000 | Loss: 0.00001722
Iteration 6/1000 | Loss: 0.00001631
Iteration 7/1000 | Loss: 0.00001546
Iteration 8/1000 | Loss: 0.00001495
Iteration 9/1000 | Loss: 0.00001451
Iteration 10/1000 | Loss: 0.00001425
Iteration 11/1000 | Loss: 0.00001400
Iteration 12/1000 | Loss: 0.00001386
Iteration 13/1000 | Loss: 0.00001379
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001365
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001361
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001356
Iteration 21/1000 | Loss: 0.00001355
Iteration 22/1000 | Loss: 0.00001354
Iteration 23/1000 | Loss: 0.00001350
Iteration 24/1000 | Loss: 0.00001350
Iteration 25/1000 | Loss: 0.00001349
Iteration 26/1000 | Loss: 0.00001348
Iteration 27/1000 | Loss: 0.00001347
Iteration 28/1000 | Loss: 0.00001345
Iteration 29/1000 | Loss: 0.00001345
Iteration 30/1000 | Loss: 0.00001344
Iteration 31/1000 | Loss: 0.00001343
Iteration 32/1000 | Loss: 0.00001342
Iteration 33/1000 | Loss: 0.00001342
Iteration 34/1000 | Loss: 0.00001342
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001341
Iteration 37/1000 | Loss: 0.00001341
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001341
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001340
Iteration 43/1000 | Loss: 0.00001340
Iteration 44/1000 | Loss: 0.00001340
Iteration 45/1000 | Loss: 0.00001340
Iteration 46/1000 | Loss: 0.00001340
Iteration 47/1000 | Loss: 0.00001340
Iteration 48/1000 | Loss: 0.00001340
Iteration 49/1000 | Loss: 0.00001340
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001339
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001338
Iteration 55/1000 | Loss: 0.00001338
Iteration 56/1000 | Loss: 0.00001338
Iteration 57/1000 | Loss: 0.00001338
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001337
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001336
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001335
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001334
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001333
Iteration 76/1000 | Loss: 0.00001333
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001333
Iteration 81/1000 | Loss: 0.00001333
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001332
Iteration 84/1000 | Loss: 0.00001332
Iteration 85/1000 | Loss: 0.00001332
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001330
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001330
Iteration 97/1000 | Loss: 0.00001330
Iteration 98/1000 | Loss: 0.00001330
Iteration 99/1000 | Loss: 0.00001330
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001329
Iteration 107/1000 | Loss: 0.00001329
Iteration 108/1000 | Loss: 0.00001329
Iteration 109/1000 | Loss: 0.00001329
Iteration 110/1000 | Loss: 0.00001329
Iteration 111/1000 | Loss: 0.00001329
Iteration 112/1000 | Loss: 0.00001329
Iteration 113/1000 | Loss: 0.00001329
Iteration 114/1000 | Loss: 0.00001329
Iteration 115/1000 | Loss: 0.00001329
Iteration 116/1000 | Loss: 0.00001328
Iteration 117/1000 | Loss: 0.00001328
Iteration 118/1000 | Loss: 0.00001328
Iteration 119/1000 | Loss: 0.00001328
Iteration 120/1000 | Loss: 0.00001328
Iteration 121/1000 | Loss: 0.00001328
Iteration 122/1000 | Loss: 0.00001328
Iteration 123/1000 | Loss: 0.00001328
Iteration 124/1000 | Loss: 0.00001327
Iteration 125/1000 | Loss: 0.00001327
Iteration 126/1000 | Loss: 0.00001327
Iteration 127/1000 | Loss: 0.00001327
Iteration 128/1000 | Loss: 0.00001327
Iteration 129/1000 | Loss: 0.00001327
Iteration 130/1000 | Loss: 0.00001327
Iteration 131/1000 | Loss: 0.00001327
Iteration 132/1000 | Loss: 0.00001327
Iteration 133/1000 | Loss: 0.00001327
Iteration 134/1000 | Loss: 0.00001326
Iteration 135/1000 | Loss: 0.00001326
Iteration 136/1000 | Loss: 0.00001326
Iteration 137/1000 | Loss: 0.00001326
Iteration 138/1000 | Loss: 0.00001326
Iteration 139/1000 | Loss: 0.00001326
Iteration 140/1000 | Loss: 0.00001326
Iteration 141/1000 | Loss: 0.00001326
Iteration 142/1000 | Loss: 0.00001326
Iteration 143/1000 | Loss: 0.00001326
Iteration 144/1000 | Loss: 0.00001326
Iteration 145/1000 | Loss: 0.00001326
Iteration 146/1000 | Loss: 0.00001326
Iteration 147/1000 | Loss: 0.00001325
Iteration 148/1000 | Loss: 0.00001325
Iteration 149/1000 | Loss: 0.00001325
Iteration 150/1000 | Loss: 0.00001325
Iteration 151/1000 | Loss: 0.00001325
Iteration 152/1000 | Loss: 0.00001325
Iteration 153/1000 | Loss: 0.00001325
Iteration 154/1000 | Loss: 0.00001325
Iteration 155/1000 | Loss: 0.00001325
Iteration 156/1000 | Loss: 0.00001325
Iteration 157/1000 | Loss: 0.00001325
Iteration 158/1000 | Loss: 0.00001325
Iteration 159/1000 | Loss: 0.00001325
Iteration 160/1000 | Loss: 0.00001325
Iteration 161/1000 | Loss: 0.00001324
Iteration 162/1000 | Loss: 0.00001324
Iteration 163/1000 | Loss: 0.00001324
Iteration 164/1000 | Loss: 0.00001324
Iteration 165/1000 | Loss: 0.00001324
Iteration 166/1000 | Loss: 0.00001324
Iteration 167/1000 | Loss: 0.00001324
Iteration 168/1000 | Loss: 0.00001324
Iteration 169/1000 | Loss: 0.00001324
Iteration 170/1000 | Loss: 0.00001324
Iteration 171/1000 | Loss: 0.00001324
Iteration 172/1000 | Loss: 0.00001324
Iteration 173/1000 | Loss: 0.00001324
Iteration 174/1000 | Loss: 0.00001324
Iteration 175/1000 | Loss: 0.00001324
Iteration 176/1000 | Loss: 0.00001324
Iteration 177/1000 | Loss: 0.00001324
Iteration 178/1000 | Loss: 0.00001324
Iteration 179/1000 | Loss: 0.00001324
Iteration 180/1000 | Loss: 0.00001324
Iteration 181/1000 | Loss: 0.00001324
Iteration 182/1000 | Loss: 0.00001324
Iteration 183/1000 | Loss: 0.00001324
Iteration 184/1000 | Loss: 0.00001324
Iteration 185/1000 | Loss: 0.00001324
Iteration 186/1000 | Loss: 0.00001324
Iteration 187/1000 | Loss: 0.00001324
Iteration 188/1000 | Loss: 0.00001324
Iteration 189/1000 | Loss: 0.00001324
Iteration 190/1000 | Loss: 0.00001324
Iteration 191/1000 | Loss: 0.00001324
Iteration 192/1000 | Loss: 0.00001324
Iteration 193/1000 | Loss: 0.00001324
Iteration 194/1000 | Loss: 0.00001324
Iteration 195/1000 | Loss: 0.00001324
Iteration 196/1000 | Loss: 0.00001324
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.3240266525826883e-05, 1.3240266525826883e-05, 1.3240266525826883e-05, 1.3240266525826883e-05, 1.3240266525826883e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3240266525826883e-05

Optimization complete. Final v2v error: 3.0673012733459473 mm

Highest mean error: 3.8699464797973633 mm for frame 70

Lowest mean error: 2.4883220195770264 mm for frame 2

Saving results

Total time: 50.039682149887085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558592
Iteration 2/25 | Loss: 0.00119589
Iteration 3/25 | Loss: 0.00113433
Iteration 4/25 | Loss: 0.00112449
Iteration 5/25 | Loss: 0.00112092
Iteration 6/25 | Loss: 0.00112026
Iteration 7/25 | Loss: 0.00112026
Iteration 8/25 | Loss: 0.00112026
Iteration 9/25 | Loss: 0.00112026
Iteration 10/25 | Loss: 0.00112026
Iteration 11/25 | Loss: 0.00112026
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001120257657021284, 0.001120257657021284, 0.001120257657021284, 0.001120257657021284, 0.001120257657021284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001120257657021284

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35745418
Iteration 2/25 | Loss: 0.00065082
Iteration 3/25 | Loss: 0.00065078
Iteration 4/25 | Loss: 0.00065078
Iteration 5/25 | Loss: 0.00065078
Iteration 6/25 | Loss: 0.00065078
Iteration 7/25 | Loss: 0.00065078
Iteration 8/25 | Loss: 0.00065078
Iteration 9/25 | Loss: 0.00065078
Iteration 10/25 | Loss: 0.00065078
Iteration 11/25 | Loss: 0.00065078
Iteration 12/25 | Loss: 0.00065078
Iteration 13/25 | Loss: 0.00065078
Iteration 14/25 | Loss: 0.00065078
Iteration 15/25 | Loss: 0.00065078
Iteration 16/25 | Loss: 0.00065078
Iteration 17/25 | Loss: 0.00065078
Iteration 18/25 | Loss: 0.00065078
Iteration 19/25 | Loss: 0.00065078
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006507810321636498, 0.0006507810321636498, 0.0006507810321636498, 0.0006507810321636498, 0.0006507810321636498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006507810321636498

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065078
Iteration 2/1000 | Loss: 0.00003253
Iteration 3/1000 | Loss: 0.00002037
Iteration 4/1000 | Loss: 0.00001828
Iteration 5/1000 | Loss: 0.00001741
Iteration 6/1000 | Loss: 0.00001694
Iteration 7/1000 | Loss: 0.00001658
Iteration 8/1000 | Loss: 0.00001627
Iteration 9/1000 | Loss: 0.00001599
Iteration 10/1000 | Loss: 0.00001579
Iteration 11/1000 | Loss: 0.00001557
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001537
Iteration 14/1000 | Loss: 0.00001536
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001526
Iteration 17/1000 | Loss: 0.00001526
Iteration 18/1000 | Loss: 0.00001518
Iteration 19/1000 | Loss: 0.00001508
Iteration 20/1000 | Loss: 0.00001502
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001498
Iteration 25/1000 | Loss: 0.00001497
Iteration 26/1000 | Loss: 0.00001496
Iteration 27/1000 | Loss: 0.00001496
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001494
Iteration 31/1000 | Loss: 0.00001493
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001493
Iteration 36/1000 | Loss: 0.00001491
Iteration 37/1000 | Loss: 0.00001490
Iteration 38/1000 | Loss: 0.00001486
Iteration 39/1000 | Loss: 0.00001486
Iteration 40/1000 | Loss: 0.00001481
Iteration 41/1000 | Loss: 0.00001481
Iteration 42/1000 | Loss: 0.00001480
Iteration 43/1000 | Loss: 0.00001480
Iteration 44/1000 | Loss: 0.00001480
Iteration 45/1000 | Loss: 0.00001480
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001479
Iteration 48/1000 | Loss: 0.00001479
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001479
Iteration 51/1000 | Loss: 0.00001479
Iteration 52/1000 | Loss: 0.00001479
Iteration 53/1000 | Loss: 0.00001479
Iteration 54/1000 | Loss: 0.00001479
Iteration 55/1000 | Loss: 0.00001479
Iteration 56/1000 | Loss: 0.00001479
Iteration 57/1000 | Loss: 0.00001479
Iteration 58/1000 | Loss: 0.00001478
Iteration 59/1000 | Loss: 0.00001478
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001478
Iteration 62/1000 | Loss: 0.00001478
Iteration 63/1000 | Loss: 0.00001478
Iteration 64/1000 | Loss: 0.00001478
Iteration 65/1000 | Loss: 0.00001478
Iteration 66/1000 | Loss: 0.00001478
Iteration 67/1000 | Loss: 0.00001477
Iteration 68/1000 | Loss: 0.00001477
Iteration 69/1000 | Loss: 0.00001477
Iteration 70/1000 | Loss: 0.00001476
Iteration 71/1000 | Loss: 0.00001476
Iteration 72/1000 | Loss: 0.00001476
Iteration 73/1000 | Loss: 0.00001476
Iteration 74/1000 | Loss: 0.00001475
Iteration 75/1000 | Loss: 0.00001475
Iteration 76/1000 | Loss: 0.00001475
Iteration 77/1000 | Loss: 0.00001475
Iteration 78/1000 | Loss: 0.00001475
Iteration 79/1000 | Loss: 0.00001474
Iteration 80/1000 | Loss: 0.00001474
Iteration 81/1000 | Loss: 0.00001473
Iteration 82/1000 | Loss: 0.00001473
Iteration 83/1000 | Loss: 0.00001472
Iteration 84/1000 | Loss: 0.00001472
Iteration 85/1000 | Loss: 0.00001472
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001472
Iteration 88/1000 | Loss: 0.00001471
Iteration 89/1000 | Loss: 0.00001471
Iteration 90/1000 | Loss: 0.00001471
Iteration 91/1000 | Loss: 0.00001470
Iteration 92/1000 | Loss: 0.00001470
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001469
Iteration 98/1000 | Loss: 0.00001469
Iteration 99/1000 | Loss: 0.00001469
Iteration 100/1000 | Loss: 0.00001469
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001468
Iteration 108/1000 | Loss: 0.00001468
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001467
Iteration 114/1000 | Loss: 0.00001467
Iteration 115/1000 | Loss: 0.00001467
Iteration 116/1000 | Loss: 0.00001467
Iteration 117/1000 | Loss: 0.00001467
Iteration 118/1000 | Loss: 0.00001467
Iteration 119/1000 | Loss: 0.00001467
Iteration 120/1000 | Loss: 0.00001467
Iteration 121/1000 | Loss: 0.00001467
Iteration 122/1000 | Loss: 0.00001467
Iteration 123/1000 | Loss: 0.00001467
Iteration 124/1000 | Loss: 0.00001466
Iteration 125/1000 | Loss: 0.00001466
Iteration 126/1000 | Loss: 0.00001466
Iteration 127/1000 | Loss: 0.00001466
Iteration 128/1000 | Loss: 0.00001466
Iteration 129/1000 | Loss: 0.00001466
Iteration 130/1000 | Loss: 0.00001466
Iteration 131/1000 | Loss: 0.00001466
Iteration 132/1000 | Loss: 0.00001465
Iteration 133/1000 | Loss: 0.00001465
Iteration 134/1000 | Loss: 0.00001465
Iteration 135/1000 | Loss: 0.00001465
Iteration 136/1000 | Loss: 0.00001465
Iteration 137/1000 | Loss: 0.00001465
Iteration 138/1000 | Loss: 0.00001465
Iteration 139/1000 | Loss: 0.00001465
Iteration 140/1000 | Loss: 0.00001465
Iteration 141/1000 | Loss: 0.00001465
Iteration 142/1000 | Loss: 0.00001465
Iteration 143/1000 | Loss: 0.00001465
Iteration 144/1000 | Loss: 0.00001465
Iteration 145/1000 | Loss: 0.00001465
Iteration 146/1000 | Loss: 0.00001465
Iteration 147/1000 | Loss: 0.00001464
Iteration 148/1000 | Loss: 0.00001464
Iteration 149/1000 | Loss: 0.00001464
Iteration 150/1000 | Loss: 0.00001464
Iteration 151/1000 | Loss: 0.00001464
Iteration 152/1000 | Loss: 0.00001464
Iteration 153/1000 | Loss: 0.00001464
Iteration 154/1000 | Loss: 0.00001464
Iteration 155/1000 | Loss: 0.00001464
Iteration 156/1000 | Loss: 0.00001464
Iteration 157/1000 | Loss: 0.00001464
Iteration 158/1000 | Loss: 0.00001464
Iteration 159/1000 | Loss: 0.00001464
Iteration 160/1000 | Loss: 0.00001464
Iteration 161/1000 | Loss: 0.00001464
Iteration 162/1000 | Loss: 0.00001464
Iteration 163/1000 | Loss: 0.00001464
Iteration 164/1000 | Loss: 0.00001463
Iteration 165/1000 | Loss: 0.00001463
Iteration 166/1000 | Loss: 0.00001463
Iteration 167/1000 | Loss: 0.00001463
Iteration 168/1000 | Loss: 0.00001463
Iteration 169/1000 | Loss: 0.00001463
Iteration 170/1000 | Loss: 0.00001463
Iteration 171/1000 | Loss: 0.00001463
Iteration 172/1000 | Loss: 0.00001463
Iteration 173/1000 | Loss: 0.00001463
Iteration 174/1000 | Loss: 0.00001463
Iteration 175/1000 | Loss: 0.00001463
Iteration 176/1000 | Loss: 0.00001463
Iteration 177/1000 | Loss: 0.00001463
Iteration 178/1000 | Loss: 0.00001463
Iteration 179/1000 | Loss: 0.00001463
Iteration 180/1000 | Loss: 0.00001463
Iteration 181/1000 | Loss: 0.00001463
Iteration 182/1000 | Loss: 0.00001463
Iteration 183/1000 | Loss: 0.00001462
Iteration 184/1000 | Loss: 0.00001462
Iteration 185/1000 | Loss: 0.00001462
Iteration 186/1000 | Loss: 0.00001462
Iteration 187/1000 | Loss: 0.00001462
Iteration 188/1000 | Loss: 0.00001462
Iteration 189/1000 | Loss: 0.00001462
Iteration 190/1000 | Loss: 0.00001462
Iteration 191/1000 | Loss: 0.00001462
Iteration 192/1000 | Loss: 0.00001462
Iteration 193/1000 | Loss: 0.00001462
Iteration 194/1000 | Loss: 0.00001462
Iteration 195/1000 | Loss: 0.00001462
Iteration 196/1000 | Loss: 0.00001462
Iteration 197/1000 | Loss: 0.00001462
Iteration 198/1000 | Loss: 0.00001462
Iteration 199/1000 | Loss: 0.00001462
Iteration 200/1000 | Loss: 0.00001462
Iteration 201/1000 | Loss: 0.00001462
Iteration 202/1000 | Loss: 0.00001462
Iteration 203/1000 | Loss: 0.00001462
Iteration 204/1000 | Loss: 0.00001461
Iteration 205/1000 | Loss: 0.00001461
Iteration 206/1000 | Loss: 0.00001461
Iteration 207/1000 | Loss: 0.00001461
Iteration 208/1000 | Loss: 0.00001461
Iteration 209/1000 | Loss: 0.00001461
Iteration 210/1000 | Loss: 0.00001461
Iteration 211/1000 | Loss: 0.00001461
Iteration 212/1000 | Loss: 0.00001461
Iteration 213/1000 | Loss: 0.00001461
Iteration 214/1000 | Loss: 0.00001461
Iteration 215/1000 | Loss: 0.00001461
Iteration 216/1000 | Loss: 0.00001461
Iteration 217/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.4612765880883671e-05, 1.4612765880883671e-05, 1.4612765880883671e-05, 1.4612765880883671e-05, 1.4612765880883671e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4612765880883671e-05

Optimization complete. Final v2v error: 3.1943559646606445 mm

Highest mean error: 3.572479248046875 mm for frame 52

Lowest mean error: 2.7103359699249268 mm for frame 38

Saving results

Total time: 43.29802942276001
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817355
Iteration 2/25 | Loss: 0.00135065
Iteration 3/25 | Loss: 0.00113353
Iteration 4/25 | Loss: 0.00112666
Iteration 5/25 | Loss: 0.00112430
Iteration 6/25 | Loss: 0.00112396
Iteration 7/25 | Loss: 0.00112396
Iteration 8/25 | Loss: 0.00112396
Iteration 9/25 | Loss: 0.00112396
Iteration 10/25 | Loss: 0.00112396
Iteration 11/25 | Loss: 0.00112396
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011239558225497603, 0.0011239558225497603, 0.0011239558225497603, 0.0011239558225497603, 0.0011239558225497603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011239558225497603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91935265
Iteration 2/25 | Loss: 0.00041491
Iteration 3/25 | Loss: 0.00041491
Iteration 4/25 | Loss: 0.00041491
Iteration 5/25 | Loss: 0.00041491
Iteration 6/25 | Loss: 0.00041491
Iteration 7/25 | Loss: 0.00041491
Iteration 8/25 | Loss: 0.00041491
Iteration 9/25 | Loss: 0.00041491
Iteration 10/25 | Loss: 0.00041491
Iteration 11/25 | Loss: 0.00041491
Iteration 12/25 | Loss: 0.00041491
Iteration 13/25 | Loss: 0.00041491
Iteration 14/25 | Loss: 0.00041491
Iteration 15/25 | Loss: 0.00041491
Iteration 16/25 | Loss: 0.00041491
Iteration 17/25 | Loss: 0.00041491
Iteration 18/25 | Loss: 0.00041491
Iteration 19/25 | Loss: 0.00041491
Iteration 20/25 | Loss: 0.00041491
Iteration 21/25 | Loss: 0.00041491
Iteration 22/25 | Loss: 0.00041491
Iteration 23/25 | Loss: 0.00041491
Iteration 24/25 | Loss: 0.00041491
Iteration 25/25 | Loss: 0.00041491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004149051965214312, 0.0004149051965214312, 0.0004149051965214312, 0.0004149051965214312, 0.0004149051965214312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004149051965214312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041491
Iteration 2/1000 | Loss: 0.00003099
Iteration 3/1000 | Loss: 0.00002387
Iteration 4/1000 | Loss: 0.00002199
Iteration 5/1000 | Loss: 0.00002110
Iteration 6/1000 | Loss: 0.00002060
Iteration 7/1000 | Loss: 0.00002022
Iteration 8/1000 | Loss: 0.00002000
Iteration 9/1000 | Loss: 0.00001984
Iteration 10/1000 | Loss: 0.00001982
Iteration 11/1000 | Loss: 0.00001969
Iteration 12/1000 | Loss: 0.00001967
Iteration 13/1000 | Loss: 0.00001964
Iteration 14/1000 | Loss: 0.00001964
Iteration 15/1000 | Loss: 0.00001964
Iteration 16/1000 | Loss: 0.00001963
Iteration 17/1000 | Loss: 0.00001962
Iteration 18/1000 | Loss: 0.00001962
Iteration 19/1000 | Loss: 0.00001962
Iteration 20/1000 | Loss: 0.00001962
Iteration 21/1000 | Loss: 0.00001961
Iteration 22/1000 | Loss: 0.00001960
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001956
Iteration 25/1000 | Loss: 0.00001956
Iteration 26/1000 | Loss: 0.00001956
Iteration 27/1000 | Loss: 0.00001956
Iteration 28/1000 | Loss: 0.00001956
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001954
Iteration 32/1000 | Loss: 0.00001951
Iteration 33/1000 | Loss: 0.00001951
Iteration 34/1000 | Loss: 0.00001951
Iteration 35/1000 | Loss: 0.00001951
Iteration 36/1000 | Loss: 0.00001951
Iteration 37/1000 | Loss: 0.00001950
Iteration 38/1000 | Loss: 0.00001950
Iteration 39/1000 | Loss: 0.00001950
Iteration 40/1000 | Loss: 0.00001950
Iteration 41/1000 | Loss: 0.00001950
Iteration 42/1000 | Loss: 0.00001950
Iteration 43/1000 | Loss: 0.00001950
Iteration 44/1000 | Loss: 0.00001950
Iteration 45/1000 | Loss: 0.00001949
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001946
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001944
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001943
Iteration 56/1000 | Loss: 0.00001943
Iteration 57/1000 | Loss: 0.00001943
Iteration 58/1000 | Loss: 0.00001943
Iteration 59/1000 | Loss: 0.00001943
Iteration 60/1000 | Loss: 0.00001943
Iteration 61/1000 | Loss: 0.00001943
Iteration 62/1000 | Loss: 0.00001943
Iteration 63/1000 | Loss: 0.00001943
Iteration 64/1000 | Loss: 0.00001943
Iteration 65/1000 | Loss: 0.00001943
Iteration 66/1000 | Loss: 0.00001943
Iteration 67/1000 | Loss: 0.00001943
Iteration 68/1000 | Loss: 0.00001943
Iteration 69/1000 | Loss: 0.00001943
Iteration 70/1000 | Loss: 0.00001943
Iteration 71/1000 | Loss: 0.00001943
Iteration 72/1000 | Loss: 0.00001943
Iteration 73/1000 | Loss: 0.00001943
Iteration 74/1000 | Loss: 0.00001943
Iteration 75/1000 | Loss: 0.00001943
Iteration 76/1000 | Loss: 0.00001943
Iteration 77/1000 | Loss: 0.00001943
Iteration 78/1000 | Loss: 0.00001943
Iteration 79/1000 | Loss: 0.00001943
Iteration 80/1000 | Loss: 0.00001943
Iteration 81/1000 | Loss: 0.00001943
Iteration 82/1000 | Loss: 0.00001943
Iteration 83/1000 | Loss: 0.00001943
Iteration 84/1000 | Loss: 0.00001943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [1.9427985535003245e-05, 1.9427985535003245e-05, 1.9427985535003245e-05, 1.9427985535003245e-05, 1.9427985535003245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9427985535003245e-05

Optimization complete. Final v2v error: 3.69671368598938 mm

Highest mean error: 3.8041703701019287 mm for frame 67

Lowest mean error: 3.574648141860962 mm for frame 149

Saving results

Total time: 28.21320104598999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995753
Iteration 2/25 | Loss: 0.00995753
Iteration 3/25 | Loss: 0.00995753
Iteration 4/25 | Loss: 0.00995753
Iteration 5/25 | Loss: 0.00250805
Iteration 6/25 | Loss: 0.00194737
Iteration 7/25 | Loss: 0.00182530
Iteration 8/25 | Loss: 0.00186833
Iteration 9/25 | Loss: 0.00173026
Iteration 10/25 | Loss: 0.00166432
Iteration 11/25 | Loss: 0.00162606
Iteration 12/25 | Loss: 0.00155299
Iteration 13/25 | Loss: 0.00148747
Iteration 14/25 | Loss: 0.00146524
Iteration 15/25 | Loss: 0.00146273
Iteration 16/25 | Loss: 0.00144868
Iteration 17/25 | Loss: 0.00143255
Iteration 18/25 | Loss: 0.00142339
Iteration 19/25 | Loss: 0.00141076
Iteration 20/25 | Loss: 0.00140253
Iteration 21/25 | Loss: 0.00138616
Iteration 22/25 | Loss: 0.00137994
Iteration 23/25 | Loss: 0.00137615
Iteration 24/25 | Loss: 0.00137172
Iteration 25/25 | Loss: 0.00137301

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53941071
Iteration 2/25 | Loss: 0.00371704
Iteration 3/25 | Loss: 0.00229948
Iteration 4/25 | Loss: 0.00229947
Iteration 5/25 | Loss: 0.00229947
Iteration 6/25 | Loss: 0.00229947
Iteration 7/25 | Loss: 0.00229947
Iteration 8/25 | Loss: 0.00229947
Iteration 9/25 | Loss: 0.00229947
Iteration 10/25 | Loss: 0.00229947
Iteration 11/25 | Loss: 0.00229947
Iteration 12/25 | Loss: 0.00229947
Iteration 13/25 | Loss: 0.00229947
Iteration 14/25 | Loss: 0.00229947
Iteration 15/25 | Loss: 0.00229947
Iteration 16/25 | Loss: 0.00229947
Iteration 17/25 | Loss: 0.00229947
Iteration 18/25 | Loss: 0.00229947
Iteration 19/25 | Loss: 0.00229947
Iteration 20/25 | Loss: 0.00229947
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00229947199113667, 0.00229947199113667, 0.00229947199113667, 0.00229947199113667, 0.00229947199113667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00229947199113667

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00229947
Iteration 2/1000 | Loss: 0.00149853
Iteration 3/1000 | Loss: 0.00106236
Iteration 4/1000 | Loss: 0.00083341
Iteration 5/1000 | Loss: 0.00111421
Iteration 6/1000 | Loss: 0.00089448
Iteration 7/1000 | Loss: 0.00061975
Iteration 8/1000 | Loss: 0.00038173
Iteration 9/1000 | Loss: 0.00023951
Iteration 10/1000 | Loss: 0.00075658
Iteration 11/1000 | Loss: 0.00069355
Iteration 12/1000 | Loss: 0.00070047
Iteration 13/1000 | Loss: 0.00085192
Iteration 14/1000 | Loss: 0.00036117
Iteration 15/1000 | Loss: 0.00056802
Iteration 16/1000 | Loss: 0.00041722
Iteration 17/1000 | Loss: 0.00026645
Iteration 18/1000 | Loss: 0.00060871
Iteration 19/1000 | Loss: 0.00023643
Iteration 20/1000 | Loss: 0.00024335
Iteration 21/1000 | Loss: 0.00026737
Iteration 22/1000 | Loss: 0.00038750
Iteration 23/1000 | Loss: 0.00026223
Iteration 24/1000 | Loss: 0.00033062
Iteration 25/1000 | Loss: 0.00023568
Iteration 26/1000 | Loss: 0.00020948
Iteration 27/1000 | Loss: 0.00027629
Iteration 28/1000 | Loss: 0.00026671
Iteration 29/1000 | Loss: 0.00024231
Iteration 30/1000 | Loss: 0.00023139
Iteration 31/1000 | Loss: 0.00024768
Iteration 32/1000 | Loss: 0.00023739
Iteration 33/1000 | Loss: 0.00035275
Iteration 34/1000 | Loss: 0.00018372
Iteration 35/1000 | Loss: 0.00022555
Iteration 36/1000 | Loss: 0.00019119
Iteration 37/1000 | Loss: 0.00024159
Iteration 38/1000 | Loss: 0.00065146
Iteration 39/1000 | Loss: 0.00057737
Iteration 40/1000 | Loss: 0.00032273
Iteration 41/1000 | Loss: 0.00023428
Iteration 42/1000 | Loss: 0.00019883
Iteration 43/1000 | Loss: 0.00026793
Iteration 44/1000 | Loss: 0.00027493
Iteration 45/1000 | Loss: 0.00027823
Iteration 46/1000 | Loss: 0.00022963
Iteration 47/1000 | Loss: 0.00030475
Iteration 48/1000 | Loss: 0.00049395
Iteration 49/1000 | Loss: 0.00023180
Iteration 50/1000 | Loss: 0.00012519
Iteration 51/1000 | Loss: 0.00027138
Iteration 52/1000 | Loss: 0.00022371
Iteration 53/1000 | Loss: 0.00048453
Iteration 54/1000 | Loss: 0.00215748
Iteration 55/1000 | Loss: 0.00071685
Iteration 56/1000 | Loss: 0.00154206
Iteration 57/1000 | Loss: 0.00045359
Iteration 58/1000 | Loss: 0.00049795
Iteration 59/1000 | Loss: 0.00038104
Iteration 60/1000 | Loss: 0.00075390
Iteration 61/1000 | Loss: 0.00022228
Iteration 62/1000 | Loss: 0.00027415
Iteration 63/1000 | Loss: 0.00039698
Iteration 64/1000 | Loss: 0.00036478
Iteration 65/1000 | Loss: 0.00020886
Iteration 66/1000 | Loss: 0.00021037
Iteration 67/1000 | Loss: 0.00024227
Iteration 68/1000 | Loss: 0.00027154
Iteration 69/1000 | Loss: 0.00027677
Iteration 70/1000 | Loss: 0.00028990
Iteration 71/1000 | Loss: 0.00026560
Iteration 72/1000 | Loss: 0.00063095
Iteration 73/1000 | Loss: 0.00051578
Iteration 74/1000 | Loss: 0.00031982
Iteration 75/1000 | Loss: 0.00036601
Iteration 76/1000 | Loss: 0.00035117
Iteration 77/1000 | Loss: 0.00039039
Iteration 78/1000 | Loss: 0.00031359
Iteration 79/1000 | Loss: 0.00029717
Iteration 80/1000 | Loss: 0.00027833
Iteration 81/1000 | Loss: 0.00029452
Iteration 82/1000 | Loss: 0.00027374
Iteration 83/1000 | Loss: 0.00028653
Iteration 84/1000 | Loss: 0.00065636
Iteration 85/1000 | Loss: 0.00018741
Iteration 86/1000 | Loss: 0.00011444
Iteration 87/1000 | Loss: 0.00011201
Iteration 88/1000 | Loss: 0.00010754
Iteration 89/1000 | Loss: 0.00010205
Iteration 90/1000 | Loss: 0.00017184
Iteration 91/1000 | Loss: 0.00010652
Iteration 92/1000 | Loss: 0.00010532
Iteration 93/1000 | Loss: 0.00010249
Iteration 94/1000 | Loss: 0.00032820
Iteration 95/1000 | Loss: 0.00040420
Iteration 96/1000 | Loss: 0.00018640
Iteration 97/1000 | Loss: 0.00009954
Iteration 98/1000 | Loss: 0.00009320
Iteration 99/1000 | Loss: 0.00032195
Iteration 100/1000 | Loss: 0.00058800
Iteration 101/1000 | Loss: 0.00026266
Iteration 102/1000 | Loss: 0.00022778
Iteration 103/1000 | Loss: 0.00016428
Iteration 104/1000 | Loss: 0.00028261
Iteration 105/1000 | Loss: 0.00041159
Iteration 106/1000 | Loss: 0.00021714
Iteration 107/1000 | Loss: 0.00015714
Iteration 108/1000 | Loss: 0.00010310
Iteration 109/1000 | Loss: 0.00009053
Iteration 110/1000 | Loss: 0.00029421
Iteration 111/1000 | Loss: 0.00017297
Iteration 112/1000 | Loss: 0.00053013
Iteration 113/1000 | Loss: 0.00026261
Iteration 114/1000 | Loss: 0.00016652
Iteration 115/1000 | Loss: 0.00037799
Iteration 116/1000 | Loss: 0.00073188
Iteration 117/1000 | Loss: 0.00028887
Iteration 118/1000 | Loss: 0.00017571
Iteration 119/1000 | Loss: 0.00032455
Iteration 120/1000 | Loss: 0.00023437
Iteration 121/1000 | Loss: 0.00013401
Iteration 122/1000 | Loss: 0.00015321
Iteration 123/1000 | Loss: 0.00022005
Iteration 124/1000 | Loss: 0.00016607
Iteration 125/1000 | Loss: 0.00037559
Iteration 126/1000 | Loss: 0.00018369
Iteration 127/1000 | Loss: 0.00018783
Iteration 128/1000 | Loss: 0.00017648
Iteration 129/1000 | Loss: 0.00016587
Iteration 130/1000 | Loss: 0.00013962
Iteration 131/1000 | Loss: 0.00021153
Iteration 132/1000 | Loss: 0.00014953
Iteration 133/1000 | Loss: 0.00023964
Iteration 134/1000 | Loss: 0.00010995
Iteration 135/1000 | Loss: 0.00020096
Iteration 136/1000 | Loss: 0.00044620
Iteration 137/1000 | Loss: 0.00020319
Iteration 138/1000 | Loss: 0.00014957
Iteration 139/1000 | Loss: 0.00010631
Iteration 140/1000 | Loss: 0.00008684
Iteration 141/1000 | Loss: 0.00013249
Iteration 142/1000 | Loss: 0.00013762
Iteration 143/1000 | Loss: 0.00013260
Iteration 144/1000 | Loss: 0.00015577
Iteration 145/1000 | Loss: 0.00035903
Iteration 146/1000 | Loss: 0.00019980
Iteration 147/1000 | Loss: 0.00015292
Iteration 148/1000 | Loss: 0.00009642
Iteration 149/1000 | Loss: 0.00009678
Iteration 150/1000 | Loss: 0.00016060
Iteration 151/1000 | Loss: 0.00012656
Iteration 152/1000 | Loss: 0.00010101
Iteration 153/1000 | Loss: 0.00009107
Iteration 154/1000 | Loss: 0.00046100
Iteration 155/1000 | Loss: 0.00030300
Iteration 156/1000 | Loss: 0.00011145
Iteration 157/1000 | Loss: 0.00034471
Iteration 158/1000 | Loss: 0.00020447
Iteration 159/1000 | Loss: 0.00011675
Iteration 160/1000 | Loss: 0.00034911
Iteration 161/1000 | Loss: 0.00009597
Iteration 162/1000 | Loss: 0.00009256
Iteration 163/1000 | Loss: 0.00009110
Iteration 164/1000 | Loss: 0.00008925
Iteration 165/1000 | Loss: 0.00008774
Iteration 166/1000 | Loss: 0.00008810
Iteration 167/1000 | Loss: 0.00008705
Iteration 168/1000 | Loss: 0.00008738
Iteration 169/1000 | Loss: 0.00008711
Iteration 170/1000 | Loss: 0.00008661
Iteration 171/1000 | Loss: 0.00014876
Iteration 172/1000 | Loss: 0.00009866
Iteration 173/1000 | Loss: 0.00007876
Iteration 174/1000 | Loss: 0.00007825
Iteration 175/1000 | Loss: 0.00008030
Iteration 176/1000 | Loss: 0.00008690
Iteration 177/1000 | Loss: 0.00014575
Iteration 178/1000 | Loss: 0.00012173
Iteration 179/1000 | Loss: 0.00008723
Iteration 180/1000 | Loss: 0.00008735
Iteration 181/1000 | Loss: 0.00016277
Iteration 182/1000 | Loss: 0.00022215
Iteration 183/1000 | Loss: 0.00017576
Iteration 184/1000 | Loss: 0.00030050
Iteration 185/1000 | Loss: 0.00013016
Iteration 186/1000 | Loss: 0.00010145
Iteration 187/1000 | Loss: 0.00023545
Iteration 188/1000 | Loss: 0.00026502
Iteration 189/1000 | Loss: 0.00009195
Iteration 190/1000 | Loss: 0.00007762
Iteration 191/1000 | Loss: 0.00007527
Iteration 192/1000 | Loss: 0.00007456
Iteration 193/1000 | Loss: 0.00016721
Iteration 194/1000 | Loss: 0.00025928
Iteration 195/1000 | Loss: 0.00008009
Iteration 196/1000 | Loss: 0.00007661
Iteration 197/1000 | Loss: 0.00007527
Iteration 198/1000 | Loss: 0.00033674
Iteration 199/1000 | Loss: 0.00022124
Iteration 200/1000 | Loss: 0.00024113
Iteration 201/1000 | Loss: 0.00008777
Iteration 202/1000 | Loss: 0.00008057
Iteration 203/1000 | Loss: 0.00020793
Iteration 204/1000 | Loss: 0.00014440
Iteration 205/1000 | Loss: 0.00011878
Iteration 206/1000 | Loss: 0.00022767
Iteration 207/1000 | Loss: 0.00019480
Iteration 208/1000 | Loss: 0.00012464
Iteration 209/1000 | Loss: 0.00010126
Iteration 210/1000 | Loss: 0.00023701
Iteration 211/1000 | Loss: 0.00016097
Iteration 212/1000 | Loss: 0.00010082
Iteration 213/1000 | Loss: 0.00007713
Iteration 214/1000 | Loss: 0.00007649
Iteration 215/1000 | Loss: 0.00044186
Iteration 216/1000 | Loss: 0.00029472
Iteration 217/1000 | Loss: 0.00009034
Iteration 218/1000 | Loss: 0.00038502
Iteration 219/1000 | Loss: 0.00014705
Iteration 220/1000 | Loss: 0.00010306
Iteration 221/1000 | Loss: 0.00041162
Iteration 222/1000 | Loss: 0.00010555
Iteration 223/1000 | Loss: 0.00007514
Iteration 224/1000 | Loss: 0.00007502
Iteration 225/1000 | Loss: 0.00023103
Iteration 226/1000 | Loss: 0.00010312
Iteration 227/1000 | Loss: 0.00030260
Iteration 228/1000 | Loss: 0.00049186
Iteration 229/1000 | Loss: 0.00024857
Iteration 230/1000 | Loss: 0.00007716
Iteration 231/1000 | Loss: 0.00007470
Iteration 232/1000 | Loss: 0.00023291
Iteration 233/1000 | Loss: 0.00023290
Iteration 234/1000 | Loss: 0.00028037
Iteration 235/1000 | Loss: 0.00014266
Iteration 236/1000 | Loss: 0.00008384
Iteration 237/1000 | Loss: 0.00007791
Iteration 238/1000 | Loss: 0.00007635
Iteration 239/1000 | Loss: 0.00007598
Iteration 240/1000 | Loss: 0.00018304
Iteration 241/1000 | Loss: 0.00014620
Iteration 242/1000 | Loss: 0.00010555
Iteration 243/1000 | Loss: 0.00064946
Iteration 244/1000 | Loss: 0.00020721
Iteration 245/1000 | Loss: 0.00051710
Iteration 246/1000 | Loss: 0.00045457
Iteration 247/1000 | Loss: 0.00055726
Iteration 248/1000 | Loss: 0.00024591
Iteration 249/1000 | Loss: 0.00021999
Iteration 250/1000 | Loss: 0.00019224
Iteration 251/1000 | Loss: 0.00020229
Iteration 252/1000 | Loss: 0.00025968
Iteration 253/1000 | Loss: 0.00023995
Iteration 254/1000 | Loss: 0.00009490
Iteration 255/1000 | Loss: 0.00013092
Iteration 256/1000 | Loss: 0.00015238
Iteration 257/1000 | Loss: 0.00008116
Iteration 258/1000 | Loss: 0.00007634
Iteration 259/1000 | Loss: 0.00007439
Iteration 260/1000 | Loss: 0.00007375
Iteration 261/1000 | Loss: 0.00007314
Iteration 262/1000 | Loss: 0.00021519
Iteration 263/1000 | Loss: 0.00023838
Iteration 264/1000 | Loss: 0.00021883
Iteration 265/1000 | Loss: 0.00014678
Iteration 266/1000 | Loss: 0.00014654
Iteration 267/1000 | Loss: 0.00034845
Iteration 268/1000 | Loss: 0.00014321
Iteration 269/1000 | Loss: 0.00012801
Iteration 270/1000 | Loss: 0.00007618
Iteration 271/1000 | Loss: 0.00007433
Iteration 272/1000 | Loss: 0.00007323
Iteration 273/1000 | Loss: 0.00007242
Iteration 274/1000 | Loss: 0.00007200
Iteration 275/1000 | Loss: 0.00007182
Iteration 276/1000 | Loss: 0.00013802
Iteration 277/1000 | Loss: 0.00009052
Iteration 278/1000 | Loss: 0.00013200
Iteration 279/1000 | Loss: 0.00009821
Iteration 280/1000 | Loss: 0.00007380
Iteration 281/1000 | Loss: 0.00007181
Iteration 282/1000 | Loss: 0.00007120
Iteration 283/1000 | Loss: 0.00007104
Iteration 284/1000 | Loss: 0.00007102
Iteration 285/1000 | Loss: 0.00013053
Iteration 286/1000 | Loss: 0.00009293
Iteration 287/1000 | Loss: 0.00007298
Iteration 288/1000 | Loss: 0.00012272
Iteration 289/1000 | Loss: 0.00008956
Iteration 290/1000 | Loss: 0.00007127
Iteration 291/1000 | Loss: 0.00007093
Iteration 292/1000 | Loss: 0.00007089
Iteration 293/1000 | Loss: 0.00007088
Iteration 294/1000 | Loss: 0.00007086
Iteration 295/1000 | Loss: 0.00007082
Iteration 296/1000 | Loss: 0.00007080
Iteration 297/1000 | Loss: 0.00007079
Iteration 298/1000 | Loss: 0.00007078
Iteration 299/1000 | Loss: 0.00007071
Iteration 300/1000 | Loss: 0.00012597
Iteration 301/1000 | Loss: 0.00007341
Iteration 302/1000 | Loss: 0.00007202
Iteration 303/1000 | Loss: 0.00017144
Iteration 304/1000 | Loss: 0.00009872
Iteration 305/1000 | Loss: 0.00007688
Iteration 306/1000 | Loss: 0.00011826
Iteration 307/1000 | Loss: 0.00009265
Iteration 308/1000 | Loss: 0.00007505
Iteration 309/1000 | Loss: 0.00007348
Iteration 310/1000 | Loss: 0.00021929
Iteration 311/1000 | Loss: 0.00018192
Iteration 312/1000 | Loss: 0.00016037
Iteration 313/1000 | Loss: 0.00008363
Iteration 314/1000 | Loss: 0.00030050
Iteration 315/1000 | Loss: 0.00015993
Iteration 316/1000 | Loss: 0.00021179
Iteration 317/1000 | Loss: 0.00028147
Iteration 318/1000 | Loss: 0.00037361
Iteration 319/1000 | Loss: 0.00013878
Iteration 320/1000 | Loss: 0.00007377
Iteration 321/1000 | Loss: 0.00007215
Iteration 322/1000 | Loss: 0.00007149
Iteration 323/1000 | Loss: 0.00007117
Iteration 324/1000 | Loss: 0.00007096
Iteration 325/1000 | Loss: 0.00007076
Iteration 326/1000 | Loss: 0.00007058
Iteration 327/1000 | Loss: 0.00047403
Iteration 328/1000 | Loss: 0.00056794
Iteration 329/1000 | Loss: 0.00045374
Iteration 330/1000 | Loss: 0.00030963
Iteration 331/1000 | Loss: 0.00032782
Iteration 332/1000 | Loss: 0.00012900
Iteration 333/1000 | Loss: 0.00016636
Iteration 334/1000 | Loss: 0.00012286
Iteration 335/1000 | Loss: 0.00008049
Iteration 336/1000 | Loss: 0.00014268
Iteration 337/1000 | Loss: 0.00013201
Iteration 338/1000 | Loss: 0.00013316
Iteration 339/1000 | Loss: 0.00050189
Iteration 340/1000 | Loss: 0.00016446
Iteration 341/1000 | Loss: 0.00029419
Iteration 342/1000 | Loss: 0.00023976
Iteration 343/1000 | Loss: 0.00096661
Iteration 344/1000 | Loss: 0.00026254
Iteration 345/1000 | Loss: 0.00019952
Iteration 346/1000 | Loss: 0.00043468
Iteration 347/1000 | Loss: 0.00041187
Iteration 348/1000 | Loss: 0.00011720
Iteration 349/1000 | Loss: 0.00062118
Iteration 350/1000 | Loss: 0.00047248
Iteration 351/1000 | Loss: 0.00024857
Iteration 352/1000 | Loss: 0.00065556
Iteration 353/1000 | Loss: 0.00032050
Iteration 354/1000 | Loss: 0.00009076
Iteration 355/1000 | Loss: 0.00007424
Iteration 356/1000 | Loss: 0.00010316
Iteration 357/1000 | Loss: 0.00007297
Iteration 358/1000 | Loss: 0.00007204
Iteration 359/1000 | Loss: 0.00007143
Iteration 360/1000 | Loss: 0.00036075
Iteration 361/1000 | Loss: 0.00021012
Iteration 362/1000 | Loss: 0.00023491
Iteration 363/1000 | Loss: 0.00008291
Iteration 364/1000 | Loss: 0.00015526
Iteration 365/1000 | Loss: 0.00024488
Iteration 366/1000 | Loss: 0.00034484
Iteration 367/1000 | Loss: 0.00008353
Iteration 368/1000 | Loss: 0.00028488
Iteration 369/1000 | Loss: 0.00008496
Iteration 370/1000 | Loss: 0.00007540
Iteration 371/1000 | Loss: 0.00007249
Iteration 372/1000 | Loss: 0.00018847
Iteration 373/1000 | Loss: 0.00007093
Iteration 374/1000 | Loss: 0.00018475
Iteration 375/1000 | Loss: 0.00009186
Iteration 376/1000 | Loss: 0.00006934
Iteration 377/1000 | Loss: 0.00016371
Iteration 378/1000 | Loss: 0.00006937
Iteration 379/1000 | Loss: 0.00006874
Iteration 380/1000 | Loss: 0.00006848
Iteration 381/1000 | Loss: 0.00006829
Iteration 382/1000 | Loss: 0.00006829
Iteration 383/1000 | Loss: 0.00006828
Iteration 384/1000 | Loss: 0.00006828
Iteration 385/1000 | Loss: 0.00006828
Iteration 386/1000 | Loss: 0.00006827
Iteration 387/1000 | Loss: 0.00006827
Iteration 388/1000 | Loss: 0.00006825
Iteration 389/1000 | Loss: 0.00006825
Iteration 390/1000 | Loss: 0.00006825
Iteration 391/1000 | Loss: 0.00006825
Iteration 392/1000 | Loss: 0.00006825
Iteration 393/1000 | Loss: 0.00006824
Iteration 394/1000 | Loss: 0.00006824
Iteration 395/1000 | Loss: 0.00006824
Iteration 396/1000 | Loss: 0.00006824
Iteration 397/1000 | Loss: 0.00006823
Iteration 398/1000 | Loss: 0.00006823
Iteration 399/1000 | Loss: 0.00006823
Iteration 400/1000 | Loss: 0.00006823
Iteration 401/1000 | Loss: 0.00006823
Iteration 402/1000 | Loss: 0.00006823
Iteration 403/1000 | Loss: 0.00006823
Iteration 404/1000 | Loss: 0.00006822
Iteration 405/1000 | Loss: 0.00006822
Iteration 406/1000 | Loss: 0.00006822
Iteration 407/1000 | Loss: 0.00006821
Iteration 408/1000 | Loss: 0.00018877
Iteration 409/1000 | Loss: 0.00007800
Iteration 410/1000 | Loss: 0.00007898
Iteration 411/1000 | Loss: 0.00006833
Iteration 412/1000 | Loss: 0.00006822
Iteration 413/1000 | Loss: 0.00006820
Iteration 414/1000 | Loss: 0.00006820
Iteration 415/1000 | Loss: 0.00006820
Iteration 416/1000 | Loss: 0.00006820
Iteration 417/1000 | Loss: 0.00006820
Iteration 418/1000 | Loss: 0.00006820
Iteration 419/1000 | Loss: 0.00006820
Iteration 420/1000 | Loss: 0.00006820
Iteration 421/1000 | Loss: 0.00006820
Iteration 422/1000 | Loss: 0.00006820
Iteration 423/1000 | Loss: 0.00006820
Iteration 424/1000 | Loss: 0.00006819
Iteration 425/1000 | Loss: 0.00006819
Iteration 426/1000 | Loss: 0.00006818
Iteration 427/1000 | Loss: 0.00006817
Iteration 428/1000 | Loss: 0.00006816
Iteration 429/1000 | Loss: 0.00006816
Iteration 430/1000 | Loss: 0.00006816
Iteration 431/1000 | Loss: 0.00006815
Iteration 432/1000 | Loss: 0.00006815
Iteration 433/1000 | Loss: 0.00006815
Iteration 434/1000 | Loss: 0.00006815
Iteration 435/1000 | Loss: 0.00006815
Iteration 436/1000 | Loss: 0.00006815
Iteration 437/1000 | Loss: 0.00006814
Iteration 438/1000 | Loss: 0.00006814
Iteration 439/1000 | Loss: 0.00006814
Iteration 440/1000 | Loss: 0.00006814
Iteration 441/1000 | Loss: 0.00006814
Iteration 442/1000 | Loss: 0.00006814
Iteration 443/1000 | Loss: 0.00006814
Iteration 444/1000 | Loss: 0.00006813
Iteration 445/1000 | Loss: 0.00006813
Iteration 446/1000 | Loss: 0.00006813
Iteration 447/1000 | Loss: 0.00006813
Iteration 448/1000 | Loss: 0.00006813
Iteration 449/1000 | Loss: 0.00006813
Iteration 450/1000 | Loss: 0.00006813
Iteration 451/1000 | Loss: 0.00006813
Iteration 452/1000 | Loss: 0.00006813
Iteration 453/1000 | Loss: 0.00006813
Iteration 454/1000 | Loss: 0.00006813
Iteration 455/1000 | Loss: 0.00006813
Iteration 456/1000 | Loss: 0.00006813
Iteration 457/1000 | Loss: 0.00006813
Iteration 458/1000 | Loss: 0.00006813
Iteration 459/1000 | Loss: 0.00006813
Iteration 460/1000 | Loss: 0.00006813
Iteration 461/1000 | Loss: 0.00006813
Iteration 462/1000 | Loss: 0.00006813
Iteration 463/1000 | Loss: 0.00006813
Iteration 464/1000 | Loss: 0.00006813
Iteration 465/1000 | Loss: 0.00006813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 465. Stopping optimization.
Last 5 losses: [6.812869833083823e-05, 6.812869833083823e-05, 6.812869833083823e-05, 6.812869833083823e-05, 6.812869833083823e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.812869833083823e-05

Optimization complete. Final v2v error: 5.018501281738281 mm

Highest mean error: 11.966572761535645 mm for frame 133

Lowest mean error: 3.0212085247039795 mm for frame 36

Saving results

Total time: 663.2374691963196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00575755
Iteration 2/25 | Loss: 0.00135461
Iteration 3/25 | Loss: 0.00119942
Iteration 4/25 | Loss: 0.00116635
Iteration 5/25 | Loss: 0.00115472
Iteration 6/25 | Loss: 0.00115183
Iteration 7/25 | Loss: 0.00115278
Iteration 8/25 | Loss: 0.00115109
Iteration 9/25 | Loss: 0.00114906
Iteration 10/25 | Loss: 0.00115386
Iteration 11/25 | Loss: 0.00114804
Iteration 12/25 | Loss: 0.00114705
Iteration 13/25 | Loss: 0.00114655
Iteration 14/25 | Loss: 0.00114587
Iteration 15/25 | Loss: 0.00114564
Iteration 16/25 | Loss: 0.00114551
Iteration 17/25 | Loss: 0.00114542
Iteration 18/25 | Loss: 0.00114533
Iteration 19/25 | Loss: 0.00114991
Iteration 20/25 | Loss: 0.00114741
Iteration 21/25 | Loss: 0.00114628
Iteration 22/25 | Loss: 0.00114529
Iteration 23/25 | Loss: 0.00114467
Iteration 24/25 | Loss: 0.00114235
Iteration 25/25 | Loss: 0.00114149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17833662
Iteration 2/25 | Loss: 0.00071715
Iteration 3/25 | Loss: 0.00071715
Iteration 4/25 | Loss: 0.00071715
Iteration 5/25 | Loss: 0.00071715
Iteration 6/25 | Loss: 0.00071715
Iteration 7/25 | Loss: 0.00071715
Iteration 8/25 | Loss: 0.00071715
Iteration 9/25 | Loss: 0.00071715
Iteration 10/25 | Loss: 0.00071715
Iteration 11/25 | Loss: 0.00071715
Iteration 12/25 | Loss: 0.00071715
Iteration 13/25 | Loss: 0.00071715
Iteration 14/25 | Loss: 0.00071715
Iteration 15/25 | Loss: 0.00071715
Iteration 16/25 | Loss: 0.00071715
Iteration 17/25 | Loss: 0.00071715
Iteration 18/25 | Loss: 0.00071715
Iteration 19/25 | Loss: 0.00071715
Iteration 20/25 | Loss: 0.00071715
Iteration 21/25 | Loss: 0.00071715
Iteration 22/25 | Loss: 0.00071715
Iteration 23/25 | Loss: 0.00071715
Iteration 24/25 | Loss: 0.00071715
Iteration 25/25 | Loss: 0.00071715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071715
Iteration 2/1000 | Loss: 0.00004830
Iteration 3/1000 | Loss: 0.00003007
Iteration 4/1000 | Loss: 0.00002613
Iteration 5/1000 | Loss: 0.00002495
Iteration 6/1000 | Loss: 0.00002415
Iteration 7/1000 | Loss: 0.00002371
Iteration 8/1000 | Loss: 0.00002319
Iteration 9/1000 | Loss: 0.00002282
Iteration 10/1000 | Loss: 0.00002260
Iteration 11/1000 | Loss: 0.00002233
Iteration 12/1000 | Loss: 0.00002216
Iteration 13/1000 | Loss: 0.00002214
Iteration 14/1000 | Loss: 0.00002204
Iteration 15/1000 | Loss: 0.00002202
Iteration 16/1000 | Loss: 0.00002200
Iteration 17/1000 | Loss: 0.00002197
Iteration 18/1000 | Loss: 0.00002186
Iteration 19/1000 | Loss: 0.00002183
Iteration 20/1000 | Loss: 0.00002181
Iteration 21/1000 | Loss: 0.00002181
Iteration 22/1000 | Loss: 0.00002177
Iteration 23/1000 | Loss: 0.00002174
Iteration 24/1000 | Loss: 0.00002173
Iteration 25/1000 | Loss: 0.00002173
Iteration 26/1000 | Loss: 0.00002172
Iteration 27/1000 | Loss: 0.00002168
Iteration 28/1000 | Loss: 0.00002166
Iteration 29/1000 | Loss: 0.00002165
Iteration 30/1000 | Loss: 0.00002164
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002163
Iteration 33/1000 | Loss: 0.00002163
Iteration 34/1000 | Loss: 0.00002163
Iteration 35/1000 | Loss: 0.00002163
Iteration 36/1000 | Loss: 0.00002162
Iteration 37/1000 | Loss: 0.00002162
Iteration 38/1000 | Loss: 0.00002162
Iteration 39/1000 | Loss: 0.00002162
Iteration 40/1000 | Loss: 0.00002162
Iteration 41/1000 | Loss: 0.00002161
Iteration 42/1000 | Loss: 0.00002161
Iteration 43/1000 | Loss: 0.00002161
Iteration 44/1000 | Loss: 0.00002160
Iteration 45/1000 | Loss: 0.00002160
Iteration 46/1000 | Loss: 0.00002160
Iteration 47/1000 | Loss: 0.00002159
Iteration 48/1000 | Loss: 0.00002159
Iteration 49/1000 | Loss: 0.00002159
Iteration 50/1000 | Loss: 0.00002159
Iteration 51/1000 | Loss: 0.00002158
Iteration 52/1000 | Loss: 0.00002158
Iteration 53/1000 | Loss: 0.00002158
Iteration 54/1000 | Loss: 0.00002157
Iteration 55/1000 | Loss: 0.00002156
Iteration 56/1000 | Loss: 0.00002156
Iteration 57/1000 | Loss: 0.00002156
Iteration 58/1000 | Loss: 0.00002156
Iteration 59/1000 | Loss: 0.00002155
Iteration 60/1000 | Loss: 0.00002155
Iteration 61/1000 | Loss: 0.00002155
Iteration 62/1000 | Loss: 0.00002155
Iteration 63/1000 | Loss: 0.00002154
Iteration 64/1000 | Loss: 0.00002154
Iteration 65/1000 | Loss: 0.00002154
Iteration 66/1000 | Loss: 0.00002154
Iteration 67/1000 | Loss: 0.00002153
Iteration 68/1000 | Loss: 0.00002153
Iteration 69/1000 | Loss: 0.00002153
Iteration 70/1000 | Loss: 0.00002153
Iteration 71/1000 | Loss: 0.00002153
Iteration 72/1000 | Loss: 0.00002153
Iteration 73/1000 | Loss: 0.00002152
Iteration 74/1000 | Loss: 0.00002152
Iteration 75/1000 | Loss: 0.00002152
Iteration 76/1000 | Loss: 0.00002152
Iteration 77/1000 | Loss: 0.00002152
Iteration 78/1000 | Loss: 0.00002152
Iteration 79/1000 | Loss: 0.00002152
Iteration 80/1000 | Loss: 0.00002152
Iteration 81/1000 | Loss: 0.00002152
Iteration 82/1000 | Loss: 0.00002152
Iteration 83/1000 | Loss: 0.00002151
Iteration 84/1000 | Loss: 0.00002151
Iteration 85/1000 | Loss: 0.00002151
Iteration 86/1000 | Loss: 0.00002151
Iteration 87/1000 | Loss: 0.00002150
Iteration 88/1000 | Loss: 0.00002150
Iteration 89/1000 | Loss: 0.00002150
Iteration 90/1000 | Loss: 0.00002149
Iteration 91/1000 | Loss: 0.00002149
Iteration 92/1000 | Loss: 0.00002149
Iteration 93/1000 | Loss: 0.00002148
Iteration 94/1000 | Loss: 0.00002148
Iteration 95/1000 | Loss: 0.00002148
Iteration 96/1000 | Loss: 0.00002147
Iteration 97/1000 | Loss: 0.00002147
Iteration 98/1000 | Loss: 0.00002147
Iteration 99/1000 | Loss: 0.00002147
Iteration 100/1000 | Loss: 0.00002147
Iteration 101/1000 | Loss: 0.00002147
Iteration 102/1000 | Loss: 0.00002146
Iteration 103/1000 | Loss: 0.00002146
Iteration 104/1000 | Loss: 0.00002146
Iteration 105/1000 | Loss: 0.00002146
Iteration 106/1000 | Loss: 0.00002146
Iteration 107/1000 | Loss: 0.00002146
Iteration 108/1000 | Loss: 0.00002146
Iteration 109/1000 | Loss: 0.00002146
Iteration 110/1000 | Loss: 0.00002146
Iteration 111/1000 | Loss: 0.00002146
Iteration 112/1000 | Loss: 0.00002146
Iteration 113/1000 | Loss: 0.00002146
Iteration 114/1000 | Loss: 0.00002146
Iteration 115/1000 | Loss: 0.00002145
Iteration 116/1000 | Loss: 0.00002145
Iteration 117/1000 | Loss: 0.00002145
Iteration 118/1000 | Loss: 0.00002145
Iteration 119/1000 | Loss: 0.00002145
Iteration 120/1000 | Loss: 0.00002145
Iteration 121/1000 | Loss: 0.00002144
Iteration 122/1000 | Loss: 0.00002144
Iteration 123/1000 | Loss: 0.00002144
Iteration 124/1000 | Loss: 0.00002144
Iteration 125/1000 | Loss: 0.00002143
Iteration 126/1000 | Loss: 0.00002143
Iteration 127/1000 | Loss: 0.00002143
Iteration 128/1000 | Loss: 0.00002143
Iteration 129/1000 | Loss: 0.00002143
Iteration 130/1000 | Loss: 0.00002142
Iteration 131/1000 | Loss: 0.00002142
Iteration 132/1000 | Loss: 0.00002142
Iteration 133/1000 | Loss: 0.00002142
Iteration 134/1000 | Loss: 0.00002142
Iteration 135/1000 | Loss: 0.00002141
Iteration 136/1000 | Loss: 0.00002141
Iteration 137/1000 | Loss: 0.00002141
Iteration 138/1000 | Loss: 0.00002141
Iteration 139/1000 | Loss: 0.00002141
Iteration 140/1000 | Loss: 0.00002141
Iteration 141/1000 | Loss: 0.00002141
Iteration 142/1000 | Loss: 0.00002140
Iteration 143/1000 | Loss: 0.00002140
Iteration 144/1000 | Loss: 0.00002140
Iteration 145/1000 | Loss: 0.00002140
Iteration 146/1000 | Loss: 0.00002140
Iteration 147/1000 | Loss: 0.00002140
Iteration 148/1000 | Loss: 0.00002140
Iteration 149/1000 | Loss: 0.00002140
Iteration 150/1000 | Loss: 0.00002139
Iteration 151/1000 | Loss: 0.00002139
Iteration 152/1000 | Loss: 0.00002139
Iteration 153/1000 | Loss: 0.00002139
Iteration 154/1000 | Loss: 0.00002139
Iteration 155/1000 | Loss: 0.00002139
Iteration 156/1000 | Loss: 0.00002139
Iteration 157/1000 | Loss: 0.00002139
Iteration 158/1000 | Loss: 0.00002139
Iteration 159/1000 | Loss: 0.00002139
Iteration 160/1000 | Loss: 0.00002139
Iteration 161/1000 | Loss: 0.00002139
Iteration 162/1000 | Loss: 0.00002139
Iteration 163/1000 | Loss: 0.00002139
Iteration 164/1000 | Loss: 0.00002138
Iteration 165/1000 | Loss: 0.00002138
Iteration 166/1000 | Loss: 0.00002138
Iteration 167/1000 | Loss: 0.00002138
Iteration 168/1000 | Loss: 0.00002138
Iteration 169/1000 | Loss: 0.00002138
Iteration 170/1000 | Loss: 0.00002138
Iteration 171/1000 | Loss: 0.00002138
Iteration 172/1000 | Loss: 0.00002138
Iteration 173/1000 | Loss: 0.00002138
Iteration 174/1000 | Loss: 0.00002138
Iteration 175/1000 | Loss: 0.00002138
Iteration 176/1000 | Loss: 0.00002138
Iteration 177/1000 | Loss: 0.00002138
Iteration 178/1000 | Loss: 0.00002138
Iteration 179/1000 | Loss: 0.00002138
Iteration 180/1000 | Loss: 0.00002138
Iteration 181/1000 | Loss: 0.00002138
Iteration 182/1000 | Loss: 0.00002138
Iteration 183/1000 | Loss: 0.00002138
Iteration 184/1000 | Loss: 0.00002138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.137730916729197e-05, 2.137730916729197e-05, 2.137730916729197e-05, 2.137730916729197e-05, 2.137730916729197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.137730916729197e-05

Optimization complete. Final v2v error: 3.7475502490997314 mm

Highest mean error: 5.879256725311279 mm for frame 114

Lowest mean error: 3.1043028831481934 mm for frame 0

Saving results

Total time: 79.14889883995056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375873
Iteration 2/25 | Loss: 0.00124554
Iteration 3/25 | Loss: 0.00110101
Iteration 4/25 | Loss: 0.00107931
Iteration 5/25 | Loss: 0.00107225
Iteration 6/25 | Loss: 0.00106994
Iteration 7/25 | Loss: 0.00106890
Iteration 8/25 | Loss: 0.00106881
Iteration 9/25 | Loss: 0.00106881
Iteration 10/25 | Loss: 0.00106881
Iteration 11/25 | Loss: 0.00106881
Iteration 12/25 | Loss: 0.00106881
Iteration 13/25 | Loss: 0.00106881
Iteration 14/25 | Loss: 0.00106881
Iteration 15/25 | Loss: 0.00106881
Iteration 16/25 | Loss: 0.00106881
Iteration 17/25 | Loss: 0.00106881
Iteration 18/25 | Loss: 0.00106881
Iteration 19/25 | Loss: 0.00106881
Iteration 20/25 | Loss: 0.00106881
Iteration 21/25 | Loss: 0.00106881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010688098846003413, 0.0010688098846003413, 0.0010688098846003413, 0.0010688098846003413, 0.0010688098846003413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010688098846003413

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45147943
Iteration 2/25 | Loss: 0.00096485
Iteration 3/25 | Loss: 0.00096485
Iteration 4/25 | Loss: 0.00096485
Iteration 5/25 | Loss: 0.00096484
Iteration 6/25 | Loss: 0.00096484
Iteration 7/25 | Loss: 0.00096484
Iteration 8/25 | Loss: 0.00096484
Iteration 9/25 | Loss: 0.00096484
Iteration 10/25 | Loss: 0.00096484
Iteration 11/25 | Loss: 0.00096484
Iteration 12/25 | Loss: 0.00096484
Iteration 13/25 | Loss: 0.00096484
Iteration 14/25 | Loss: 0.00096484
Iteration 15/25 | Loss: 0.00096484
Iteration 16/25 | Loss: 0.00096484
Iteration 17/25 | Loss: 0.00096484
Iteration 18/25 | Loss: 0.00096484
Iteration 19/25 | Loss: 0.00096484
Iteration 20/25 | Loss: 0.00096484
Iteration 21/25 | Loss: 0.00096484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000964843260589987, 0.000964843260589987, 0.000964843260589987, 0.000964843260589987, 0.000964843260589987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000964843260589987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096484
Iteration 2/1000 | Loss: 0.00003044
Iteration 3/1000 | Loss: 0.00002024
Iteration 4/1000 | Loss: 0.00001628
Iteration 5/1000 | Loss: 0.00001540
Iteration 6/1000 | Loss: 0.00001485
Iteration 7/1000 | Loss: 0.00001445
Iteration 8/1000 | Loss: 0.00001413
Iteration 9/1000 | Loss: 0.00001412
Iteration 10/1000 | Loss: 0.00001393
Iteration 11/1000 | Loss: 0.00001388
Iteration 12/1000 | Loss: 0.00001371
Iteration 13/1000 | Loss: 0.00001365
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001355
Iteration 16/1000 | Loss: 0.00001355
Iteration 17/1000 | Loss: 0.00001355
Iteration 18/1000 | Loss: 0.00001351
Iteration 19/1000 | Loss: 0.00001346
Iteration 20/1000 | Loss: 0.00001346
Iteration 21/1000 | Loss: 0.00001346
Iteration 22/1000 | Loss: 0.00001344
Iteration 23/1000 | Loss: 0.00001343
Iteration 24/1000 | Loss: 0.00001342
Iteration 25/1000 | Loss: 0.00001342
Iteration 26/1000 | Loss: 0.00001342
Iteration 27/1000 | Loss: 0.00001341
Iteration 28/1000 | Loss: 0.00001340
Iteration 29/1000 | Loss: 0.00001340
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001339
Iteration 32/1000 | Loss: 0.00001339
Iteration 33/1000 | Loss: 0.00001338
Iteration 34/1000 | Loss: 0.00001338
Iteration 35/1000 | Loss: 0.00001337
Iteration 36/1000 | Loss: 0.00001337
Iteration 37/1000 | Loss: 0.00001337
Iteration 38/1000 | Loss: 0.00001337
Iteration 39/1000 | Loss: 0.00001337
Iteration 40/1000 | Loss: 0.00001336
Iteration 41/1000 | Loss: 0.00001335
Iteration 42/1000 | Loss: 0.00001334
Iteration 43/1000 | Loss: 0.00001334
Iteration 44/1000 | Loss: 0.00001334
Iteration 45/1000 | Loss: 0.00001334
Iteration 46/1000 | Loss: 0.00001334
Iteration 47/1000 | Loss: 0.00001333
Iteration 48/1000 | Loss: 0.00001333
Iteration 49/1000 | Loss: 0.00001333
Iteration 50/1000 | Loss: 0.00001333
Iteration 51/1000 | Loss: 0.00001332
Iteration 52/1000 | Loss: 0.00001332
Iteration 53/1000 | Loss: 0.00001331
Iteration 54/1000 | Loss: 0.00001331
Iteration 55/1000 | Loss: 0.00001331
Iteration 56/1000 | Loss: 0.00001331
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001330
Iteration 59/1000 | Loss: 0.00001329
Iteration 60/1000 | Loss: 0.00001329
Iteration 61/1000 | Loss: 0.00001329
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001328
Iteration 65/1000 | Loss: 0.00001328
Iteration 66/1000 | Loss: 0.00001328
Iteration 67/1000 | Loss: 0.00001328
Iteration 68/1000 | Loss: 0.00001327
Iteration 69/1000 | Loss: 0.00001327
Iteration 70/1000 | Loss: 0.00001327
Iteration 71/1000 | Loss: 0.00001327
Iteration 72/1000 | Loss: 0.00001327
Iteration 73/1000 | Loss: 0.00001327
Iteration 74/1000 | Loss: 0.00001326
Iteration 75/1000 | Loss: 0.00001326
Iteration 76/1000 | Loss: 0.00001326
Iteration 77/1000 | Loss: 0.00001326
Iteration 78/1000 | Loss: 0.00001326
Iteration 79/1000 | Loss: 0.00001326
Iteration 80/1000 | Loss: 0.00001326
Iteration 81/1000 | Loss: 0.00001326
Iteration 82/1000 | Loss: 0.00001326
Iteration 83/1000 | Loss: 0.00001325
Iteration 84/1000 | Loss: 0.00001325
Iteration 85/1000 | Loss: 0.00001325
Iteration 86/1000 | Loss: 0.00001325
Iteration 87/1000 | Loss: 0.00001325
Iteration 88/1000 | Loss: 0.00001325
Iteration 89/1000 | Loss: 0.00001324
Iteration 90/1000 | Loss: 0.00001324
Iteration 91/1000 | Loss: 0.00001324
Iteration 92/1000 | Loss: 0.00001324
Iteration 93/1000 | Loss: 0.00001324
Iteration 94/1000 | Loss: 0.00001323
Iteration 95/1000 | Loss: 0.00001323
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001323
Iteration 98/1000 | Loss: 0.00001323
Iteration 99/1000 | Loss: 0.00001323
Iteration 100/1000 | Loss: 0.00001323
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001322
Iteration 104/1000 | Loss: 0.00001322
Iteration 105/1000 | Loss: 0.00001321
Iteration 106/1000 | Loss: 0.00001321
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001320
Iteration 110/1000 | Loss: 0.00001320
Iteration 111/1000 | Loss: 0.00001320
Iteration 112/1000 | Loss: 0.00001320
Iteration 113/1000 | Loss: 0.00001320
Iteration 114/1000 | Loss: 0.00001320
Iteration 115/1000 | Loss: 0.00001320
Iteration 116/1000 | Loss: 0.00001320
Iteration 117/1000 | Loss: 0.00001320
Iteration 118/1000 | Loss: 0.00001319
Iteration 119/1000 | Loss: 0.00001319
Iteration 120/1000 | Loss: 0.00001319
Iteration 121/1000 | Loss: 0.00001318
Iteration 122/1000 | Loss: 0.00001318
Iteration 123/1000 | Loss: 0.00001318
Iteration 124/1000 | Loss: 0.00001318
Iteration 125/1000 | Loss: 0.00001317
Iteration 126/1000 | Loss: 0.00001317
Iteration 127/1000 | Loss: 0.00001317
Iteration 128/1000 | Loss: 0.00001317
Iteration 129/1000 | Loss: 0.00001316
Iteration 130/1000 | Loss: 0.00001316
Iteration 131/1000 | Loss: 0.00001316
Iteration 132/1000 | Loss: 0.00001316
Iteration 133/1000 | Loss: 0.00001316
Iteration 134/1000 | Loss: 0.00001316
Iteration 135/1000 | Loss: 0.00001316
Iteration 136/1000 | Loss: 0.00001315
Iteration 137/1000 | Loss: 0.00001315
Iteration 138/1000 | Loss: 0.00001315
Iteration 139/1000 | Loss: 0.00001315
Iteration 140/1000 | Loss: 0.00001315
Iteration 141/1000 | Loss: 0.00001315
Iteration 142/1000 | Loss: 0.00001315
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001314
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001314
Iteration 150/1000 | Loss: 0.00001314
Iteration 151/1000 | Loss: 0.00001313
Iteration 152/1000 | Loss: 0.00001313
Iteration 153/1000 | Loss: 0.00001313
Iteration 154/1000 | Loss: 0.00001313
Iteration 155/1000 | Loss: 0.00001312
Iteration 156/1000 | Loss: 0.00001312
Iteration 157/1000 | Loss: 0.00001312
Iteration 158/1000 | Loss: 0.00001312
Iteration 159/1000 | Loss: 0.00001312
Iteration 160/1000 | Loss: 0.00001312
Iteration 161/1000 | Loss: 0.00001312
Iteration 162/1000 | Loss: 0.00001312
Iteration 163/1000 | Loss: 0.00001311
Iteration 164/1000 | Loss: 0.00001311
Iteration 165/1000 | Loss: 0.00001311
Iteration 166/1000 | Loss: 0.00001311
Iteration 167/1000 | Loss: 0.00001311
Iteration 168/1000 | Loss: 0.00001310
Iteration 169/1000 | Loss: 0.00001310
Iteration 170/1000 | Loss: 0.00001310
Iteration 171/1000 | Loss: 0.00001310
Iteration 172/1000 | Loss: 0.00001310
Iteration 173/1000 | Loss: 0.00001310
Iteration 174/1000 | Loss: 0.00001310
Iteration 175/1000 | Loss: 0.00001310
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001309
Iteration 178/1000 | Loss: 0.00001309
Iteration 179/1000 | Loss: 0.00001309
Iteration 180/1000 | Loss: 0.00001309
Iteration 181/1000 | Loss: 0.00001309
Iteration 182/1000 | Loss: 0.00001309
Iteration 183/1000 | Loss: 0.00001309
Iteration 184/1000 | Loss: 0.00001309
Iteration 185/1000 | Loss: 0.00001309
Iteration 186/1000 | Loss: 0.00001309
Iteration 187/1000 | Loss: 0.00001309
Iteration 188/1000 | Loss: 0.00001308
Iteration 189/1000 | Loss: 0.00001308
Iteration 190/1000 | Loss: 0.00001308
Iteration 191/1000 | Loss: 0.00001308
Iteration 192/1000 | Loss: 0.00001308
Iteration 193/1000 | Loss: 0.00001308
Iteration 194/1000 | Loss: 0.00001308
Iteration 195/1000 | Loss: 0.00001308
Iteration 196/1000 | Loss: 0.00001308
Iteration 197/1000 | Loss: 0.00001308
Iteration 198/1000 | Loss: 0.00001308
Iteration 199/1000 | Loss: 0.00001308
Iteration 200/1000 | Loss: 0.00001308
Iteration 201/1000 | Loss: 0.00001308
Iteration 202/1000 | Loss: 0.00001308
Iteration 203/1000 | Loss: 0.00001307
Iteration 204/1000 | Loss: 0.00001307
Iteration 205/1000 | Loss: 0.00001307
Iteration 206/1000 | Loss: 0.00001307
Iteration 207/1000 | Loss: 0.00001307
Iteration 208/1000 | Loss: 0.00001307
Iteration 209/1000 | Loss: 0.00001307
Iteration 210/1000 | Loss: 0.00001307
Iteration 211/1000 | Loss: 0.00001307
Iteration 212/1000 | Loss: 0.00001307
Iteration 213/1000 | Loss: 0.00001307
Iteration 214/1000 | Loss: 0.00001307
Iteration 215/1000 | Loss: 0.00001307
Iteration 216/1000 | Loss: 0.00001307
Iteration 217/1000 | Loss: 0.00001307
Iteration 218/1000 | Loss: 0.00001307
Iteration 219/1000 | Loss: 0.00001307
Iteration 220/1000 | Loss: 0.00001307
Iteration 221/1000 | Loss: 0.00001307
Iteration 222/1000 | Loss: 0.00001307
Iteration 223/1000 | Loss: 0.00001307
Iteration 224/1000 | Loss: 0.00001307
Iteration 225/1000 | Loss: 0.00001307
Iteration 226/1000 | Loss: 0.00001307
Iteration 227/1000 | Loss: 0.00001307
Iteration 228/1000 | Loss: 0.00001307
Iteration 229/1000 | Loss: 0.00001307
Iteration 230/1000 | Loss: 0.00001307
Iteration 231/1000 | Loss: 0.00001307
Iteration 232/1000 | Loss: 0.00001307
Iteration 233/1000 | Loss: 0.00001307
Iteration 234/1000 | Loss: 0.00001307
Iteration 235/1000 | Loss: 0.00001307
Iteration 236/1000 | Loss: 0.00001307
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 236. Stopping optimization.
Last 5 losses: [1.3066057363175787e-05, 1.3066057363175787e-05, 1.3066057363175787e-05, 1.3066057363175787e-05, 1.3066057363175787e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3066057363175787e-05

Optimization complete. Final v2v error: 2.9884908199310303 mm

Highest mean error: 3.7046256065368652 mm for frame 143

Lowest mean error: 2.31457781791687 mm for frame 7

Saving results

Total time: 44.72408699989319
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00762230
Iteration 2/25 | Loss: 0.00139364
Iteration 3/25 | Loss: 0.00124134
Iteration 4/25 | Loss: 0.00120995
Iteration 5/25 | Loss: 0.00120502
Iteration 6/25 | Loss: 0.00119529
Iteration 7/25 | Loss: 0.00119134
Iteration 8/25 | Loss: 0.00118958
Iteration 9/25 | Loss: 0.00118919
Iteration 10/25 | Loss: 0.00118907
Iteration 11/25 | Loss: 0.00118899
Iteration 12/25 | Loss: 0.00118890
Iteration 13/25 | Loss: 0.00118880
Iteration 14/25 | Loss: 0.00118859
Iteration 15/25 | Loss: 0.00118828
Iteration 16/25 | Loss: 0.00118763
Iteration 17/25 | Loss: 0.00118958
Iteration 18/25 | Loss: 0.00119038
Iteration 19/25 | Loss: 0.00118891
Iteration 20/25 | Loss: 0.00118902
Iteration 21/25 | Loss: 0.00118872
Iteration 22/25 | Loss: 0.00118889
Iteration 23/25 | Loss: 0.00118863
Iteration 24/25 | Loss: 0.00118882
Iteration 25/25 | Loss: 0.00118895

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85135698
Iteration 2/25 | Loss: 0.00084900
Iteration 3/25 | Loss: 0.00084900
Iteration 4/25 | Loss: 0.00084900
Iteration 5/25 | Loss: 0.00084900
Iteration 6/25 | Loss: 0.00084900
Iteration 7/25 | Loss: 0.00084900
Iteration 8/25 | Loss: 0.00084900
Iteration 9/25 | Loss: 0.00084900
Iteration 10/25 | Loss: 0.00084900
Iteration 11/25 | Loss: 0.00084900
Iteration 12/25 | Loss: 0.00084900
Iteration 13/25 | Loss: 0.00084900
Iteration 14/25 | Loss: 0.00084900
Iteration 15/25 | Loss: 0.00084900
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008490006439387798, 0.0008490006439387798, 0.0008490006439387798, 0.0008490006439387798, 0.0008490006439387798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008490006439387798

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084900
Iteration 2/1000 | Loss: 0.00006177
Iteration 3/1000 | Loss: 0.00013112
Iteration 4/1000 | Loss: 0.00005046
Iteration 5/1000 | Loss: 0.00012816
Iteration 6/1000 | Loss: 0.00013587
Iteration 7/1000 | Loss: 0.00008398
Iteration 8/1000 | Loss: 0.00010731
Iteration 9/1000 | Loss: 0.00008983
Iteration 10/1000 | Loss: 0.00005355
Iteration 11/1000 | Loss: 0.00007173
Iteration 12/1000 | Loss: 0.00004270
Iteration 13/1000 | Loss: 0.00021896
Iteration 14/1000 | Loss: 0.00015360
Iteration 15/1000 | Loss: 0.00068526
Iteration 16/1000 | Loss: 0.00032970
Iteration 17/1000 | Loss: 0.00010920
Iteration 18/1000 | Loss: 0.00009921
Iteration 19/1000 | Loss: 0.00005098
Iteration 20/1000 | Loss: 0.00005494
Iteration 21/1000 | Loss: 0.00006642
Iteration 22/1000 | Loss: 0.00013485
Iteration 23/1000 | Loss: 0.00011780
Iteration 24/1000 | Loss: 0.00004296
Iteration 25/1000 | Loss: 0.00009860
Iteration 26/1000 | Loss: 0.00011202
Iteration 27/1000 | Loss: 0.00007153
Iteration 28/1000 | Loss: 0.00003646
Iteration 29/1000 | Loss: 0.00010899
Iteration 30/1000 | Loss: 0.00024784
Iteration 31/1000 | Loss: 0.00012911
Iteration 32/1000 | Loss: 0.00010402
Iteration 33/1000 | Loss: 0.00025617
Iteration 34/1000 | Loss: 0.00023494
Iteration 35/1000 | Loss: 0.00017578
Iteration 36/1000 | Loss: 0.00011119
Iteration 37/1000 | Loss: 0.00014560
Iteration 38/1000 | Loss: 0.00010874
Iteration 39/1000 | Loss: 0.00016333
Iteration 40/1000 | Loss: 0.00010767
Iteration 41/1000 | Loss: 0.00011995
Iteration 42/1000 | Loss: 0.00016554
Iteration 43/1000 | Loss: 0.00016631
Iteration 44/1000 | Loss: 0.00020829
Iteration 45/1000 | Loss: 0.00021457
Iteration 46/1000 | Loss: 0.00003800
Iteration 47/1000 | Loss: 0.00003150
Iteration 48/1000 | Loss: 0.00003034
Iteration 49/1000 | Loss: 0.00002992
Iteration 50/1000 | Loss: 0.00002905
Iteration 51/1000 | Loss: 0.00002859
Iteration 52/1000 | Loss: 0.00002828
Iteration 53/1000 | Loss: 0.00002812
Iteration 54/1000 | Loss: 0.00002801
Iteration 55/1000 | Loss: 0.00002800
Iteration 56/1000 | Loss: 0.00002800
Iteration 57/1000 | Loss: 0.00002799
Iteration 58/1000 | Loss: 0.00002799
Iteration 59/1000 | Loss: 0.00002796
Iteration 60/1000 | Loss: 0.00002794
Iteration 61/1000 | Loss: 0.00002793
Iteration 62/1000 | Loss: 0.00002792
Iteration 63/1000 | Loss: 0.00002791
Iteration 64/1000 | Loss: 0.00002791
Iteration 65/1000 | Loss: 0.00002790
Iteration 66/1000 | Loss: 0.00002790
Iteration 67/1000 | Loss: 0.00002789
Iteration 68/1000 | Loss: 0.00002788
Iteration 69/1000 | Loss: 0.00002788
Iteration 70/1000 | Loss: 0.00002787
Iteration 71/1000 | Loss: 0.00002787
Iteration 72/1000 | Loss: 0.00002786
Iteration 73/1000 | Loss: 0.00002785
Iteration 74/1000 | Loss: 0.00002785
Iteration 75/1000 | Loss: 0.00002784
Iteration 76/1000 | Loss: 0.00002784
Iteration 77/1000 | Loss: 0.00002784
Iteration 78/1000 | Loss: 0.00002783
Iteration 79/1000 | Loss: 0.00002783
Iteration 80/1000 | Loss: 0.00002769
Iteration 81/1000 | Loss: 0.00002769
Iteration 82/1000 | Loss: 0.00002767
Iteration 83/1000 | Loss: 0.00002764
Iteration 84/1000 | Loss: 0.00002763
Iteration 85/1000 | Loss: 0.00002763
Iteration 86/1000 | Loss: 0.00002762
Iteration 87/1000 | Loss: 0.00002762
Iteration 88/1000 | Loss: 0.00002761
Iteration 89/1000 | Loss: 0.00002761
Iteration 90/1000 | Loss: 0.00002760
Iteration 91/1000 | Loss: 0.00002760
Iteration 92/1000 | Loss: 0.00002760
Iteration 93/1000 | Loss: 0.00002759
Iteration 94/1000 | Loss: 0.00002758
Iteration 95/1000 | Loss: 0.00002757
Iteration 96/1000 | Loss: 0.00002757
Iteration 97/1000 | Loss: 0.00002756
Iteration 98/1000 | Loss: 0.00002756
Iteration 99/1000 | Loss: 0.00002756
Iteration 100/1000 | Loss: 0.00002755
Iteration 101/1000 | Loss: 0.00002755
Iteration 102/1000 | Loss: 0.00002754
Iteration 103/1000 | Loss: 0.00002754
Iteration 104/1000 | Loss: 0.00002752
Iteration 105/1000 | Loss: 0.00002751
Iteration 106/1000 | Loss: 0.00002748
Iteration 107/1000 | Loss: 0.00002748
Iteration 108/1000 | Loss: 0.00002747
Iteration 109/1000 | Loss: 0.00002746
Iteration 110/1000 | Loss: 0.00002745
Iteration 111/1000 | Loss: 0.00002745
Iteration 112/1000 | Loss: 0.00002744
Iteration 113/1000 | Loss: 0.00002744
Iteration 114/1000 | Loss: 0.00002744
Iteration 115/1000 | Loss: 0.00002743
Iteration 116/1000 | Loss: 0.00002742
Iteration 117/1000 | Loss: 0.00002742
Iteration 118/1000 | Loss: 0.00002741
Iteration 119/1000 | Loss: 0.00002741
Iteration 120/1000 | Loss: 0.00002740
Iteration 121/1000 | Loss: 0.00002740
Iteration 122/1000 | Loss: 0.00002740
Iteration 123/1000 | Loss: 0.00002739
Iteration 124/1000 | Loss: 0.00002739
Iteration 125/1000 | Loss: 0.00002738
Iteration 126/1000 | Loss: 0.00002738
Iteration 127/1000 | Loss: 0.00002738
Iteration 128/1000 | Loss: 0.00002737
Iteration 129/1000 | Loss: 0.00002737
Iteration 130/1000 | Loss: 0.00002736
Iteration 131/1000 | Loss: 0.00002736
Iteration 132/1000 | Loss: 0.00002736
Iteration 133/1000 | Loss: 0.00002736
Iteration 134/1000 | Loss: 0.00002735
Iteration 135/1000 | Loss: 0.00002735
Iteration 136/1000 | Loss: 0.00002735
Iteration 137/1000 | Loss: 0.00002735
Iteration 138/1000 | Loss: 0.00002735
Iteration 139/1000 | Loss: 0.00002735
Iteration 140/1000 | Loss: 0.00002735
Iteration 141/1000 | Loss: 0.00002735
Iteration 142/1000 | Loss: 0.00002734
Iteration 143/1000 | Loss: 0.00002734
Iteration 144/1000 | Loss: 0.00002734
Iteration 145/1000 | Loss: 0.00002734
Iteration 146/1000 | Loss: 0.00002733
Iteration 147/1000 | Loss: 0.00002733
Iteration 148/1000 | Loss: 0.00002733
Iteration 149/1000 | Loss: 0.00002732
Iteration 150/1000 | Loss: 0.00002732
Iteration 151/1000 | Loss: 0.00002732
Iteration 152/1000 | Loss: 0.00002732
Iteration 153/1000 | Loss: 0.00002731
Iteration 154/1000 | Loss: 0.00002731
Iteration 155/1000 | Loss: 0.00002731
Iteration 156/1000 | Loss: 0.00002731
Iteration 157/1000 | Loss: 0.00002731
Iteration 158/1000 | Loss: 0.00002731
Iteration 159/1000 | Loss: 0.00002731
Iteration 160/1000 | Loss: 0.00002731
Iteration 161/1000 | Loss: 0.00002731
Iteration 162/1000 | Loss: 0.00002731
Iteration 163/1000 | Loss: 0.00002730
Iteration 164/1000 | Loss: 0.00002730
Iteration 165/1000 | Loss: 0.00002730
Iteration 166/1000 | Loss: 0.00002730
Iteration 167/1000 | Loss: 0.00002730
Iteration 168/1000 | Loss: 0.00002730
Iteration 169/1000 | Loss: 0.00002730
Iteration 170/1000 | Loss: 0.00002730
Iteration 171/1000 | Loss: 0.00002729
Iteration 172/1000 | Loss: 0.00002729
Iteration 173/1000 | Loss: 0.00002729
Iteration 174/1000 | Loss: 0.00002729
Iteration 175/1000 | Loss: 0.00002729
Iteration 176/1000 | Loss: 0.00002729
Iteration 177/1000 | Loss: 0.00002729
Iteration 178/1000 | Loss: 0.00002729
Iteration 179/1000 | Loss: 0.00002729
Iteration 180/1000 | Loss: 0.00002729
Iteration 181/1000 | Loss: 0.00002729
Iteration 182/1000 | Loss: 0.00002729
Iteration 183/1000 | Loss: 0.00002728
Iteration 184/1000 | Loss: 0.00002728
Iteration 185/1000 | Loss: 0.00002728
Iteration 186/1000 | Loss: 0.00002728
Iteration 187/1000 | Loss: 0.00002728
Iteration 188/1000 | Loss: 0.00002728
Iteration 189/1000 | Loss: 0.00002728
Iteration 190/1000 | Loss: 0.00002727
Iteration 191/1000 | Loss: 0.00002727
Iteration 192/1000 | Loss: 0.00002727
Iteration 193/1000 | Loss: 0.00002727
Iteration 194/1000 | Loss: 0.00002726
Iteration 195/1000 | Loss: 0.00002726
Iteration 196/1000 | Loss: 0.00002726
Iteration 197/1000 | Loss: 0.00002726
Iteration 198/1000 | Loss: 0.00002725
Iteration 199/1000 | Loss: 0.00002725
Iteration 200/1000 | Loss: 0.00002725
Iteration 201/1000 | Loss: 0.00002725
Iteration 202/1000 | Loss: 0.00002725
Iteration 203/1000 | Loss: 0.00002725
Iteration 204/1000 | Loss: 0.00002725
Iteration 205/1000 | Loss: 0.00002725
Iteration 206/1000 | Loss: 0.00002725
Iteration 207/1000 | Loss: 0.00002725
Iteration 208/1000 | Loss: 0.00002725
Iteration 209/1000 | Loss: 0.00002725
Iteration 210/1000 | Loss: 0.00002725
Iteration 211/1000 | Loss: 0.00002724
Iteration 212/1000 | Loss: 0.00002724
Iteration 213/1000 | Loss: 0.00002724
Iteration 214/1000 | Loss: 0.00002724
Iteration 215/1000 | Loss: 0.00002724
Iteration 216/1000 | Loss: 0.00002724
Iteration 217/1000 | Loss: 0.00002724
Iteration 218/1000 | Loss: 0.00002724
Iteration 219/1000 | Loss: 0.00002724
Iteration 220/1000 | Loss: 0.00002724
Iteration 221/1000 | Loss: 0.00002724
Iteration 222/1000 | Loss: 0.00002724
Iteration 223/1000 | Loss: 0.00002724
Iteration 224/1000 | Loss: 0.00002724
Iteration 225/1000 | Loss: 0.00002724
Iteration 226/1000 | Loss: 0.00002724
Iteration 227/1000 | Loss: 0.00002724
Iteration 228/1000 | Loss: 0.00002724
Iteration 229/1000 | Loss: 0.00002723
Iteration 230/1000 | Loss: 0.00002723
Iteration 231/1000 | Loss: 0.00002723
Iteration 232/1000 | Loss: 0.00002723
Iteration 233/1000 | Loss: 0.00002723
Iteration 234/1000 | Loss: 0.00002723
Iteration 235/1000 | Loss: 0.00002723
Iteration 236/1000 | Loss: 0.00002723
Iteration 237/1000 | Loss: 0.00002723
Iteration 238/1000 | Loss: 0.00002723
Iteration 239/1000 | Loss: 0.00002723
Iteration 240/1000 | Loss: 0.00002723
Iteration 241/1000 | Loss: 0.00002723
Iteration 242/1000 | Loss: 0.00002723
Iteration 243/1000 | Loss: 0.00002723
Iteration 244/1000 | Loss: 0.00002723
Iteration 245/1000 | Loss: 0.00002723
Iteration 246/1000 | Loss: 0.00002723
Iteration 247/1000 | Loss: 0.00002723
Iteration 248/1000 | Loss: 0.00002723
Iteration 249/1000 | Loss: 0.00002723
Iteration 250/1000 | Loss: 0.00002723
Iteration 251/1000 | Loss: 0.00002723
Iteration 252/1000 | Loss: 0.00002723
Iteration 253/1000 | Loss: 0.00002723
Iteration 254/1000 | Loss: 0.00002723
Iteration 255/1000 | Loss: 0.00002723
Iteration 256/1000 | Loss: 0.00002723
Iteration 257/1000 | Loss: 0.00002723
Iteration 258/1000 | Loss: 0.00002723
Iteration 259/1000 | Loss: 0.00002723
Iteration 260/1000 | Loss: 0.00002723
Iteration 261/1000 | Loss: 0.00002723
Iteration 262/1000 | Loss: 0.00002723
Iteration 263/1000 | Loss: 0.00002723
Iteration 264/1000 | Loss: 0.00002723
Iteration 265/1000 | Loss: 0.00002723
Iteration 266/1000 | Loss: 0.00002723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [2.7229920306126587e-05, 2.7229920306126587e-05, 2.7229920306126587e-05, 2.7229920306126587e-05, 2.7229920306126587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7229920306126587e-05

Optimization complete. Final v2v error: 4.3503193855285645 mm

Highest mean error: 5.818368434906006 mm for frame 106

Lowest mean error: 3.1415116786956787 mm for frame 231

Saving results

Total time: 156.87935042381287
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406695
Iteration 2/25 | Loss: 0.00120382
Iteration 3/25 | Loss: 0.00111583
Iteration 4/25 | Loss: 0.00110341
Iteration 5/25 | Loss: 0.00109933
Iteration 6/25 | Loss: 0.00109865
Iteration 7/25 | Loss: 0.00109865
Iteration 8/25 | Loss: 0.00109865
Iteration 9/25 | Loss: 0.00109865
Iteration 10/25 | Loss: 0.00109865
Iteration 11/25 | Loss: 0.00109865
Iteration 12/25 | Loss: 0.00109865
Iteration 13/25 | Loss: 0.00109865
Iteration 14/25 | Loss: 0.00109865
Iteration 15/25 | Loss: 0.00109865
Iteration 16/25 | Loss: 0.00109865
Iteration 17/25 | Loss: 0.00109865
Iteration 18/25 | Loss: 0.00109865
Iteration 19/25 | Loss: 0.00109865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001098646316677332, 0.001098646316677332, 0.001098646316677332, 0.001098646316677332, 0.001098646316677332]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001098646316677332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01384449
Iteration 2/25 | Loss: 0.00068406
Iteration 3/25 | Loss: 0.00068406
Iteration 4/25 | Loss: 0.00068406
Iteration 5/25 | Loss: 0.00068406
Iteration 6/25 | Loss: 0.00068406
Iteration 7/25 | Loss: 0.00068406
Iteration 8/25 | Loss: 0.00068406
Iteration 9/25 | Loss: 0.00068406
Iteration 10/25 | Loss: 0.00068406
Iteration 11/25 | Loss: 0.00068406
Iteration 12/25 | Loss: 0.00068406
Iteration 13/25 | Loss: 0.00068406
Iteration 14/25 | Loss: 0.00068406
Iteration 15/25 | Loss: 0.00068406
Iteration 16/25 | Loss: 0.00068406
Iteration 17/25 | Loss: 0.00068406
Iteration 18/25 | Loss: 0.00068406
Iteration 19/25 | Loss: 0.00068406
Iteration 20/25 | Loss: 0.00068406
Iteration 21/25 | Loss: 0.00068406
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006840578862465918, 0.0006840578862465918, 0.0006840578862465918, 0.0006840578862465918, 0.0006840578862465918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006840578862465918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068406
Iteration 2/1000 | Loss: 0.00002571
Iteration 3/1000 | Loss: 0.00002055
Iteration 4/1000 | Loss: 0.00001934
Iteration 5/1000 | Loss: 0.00001847
Iteration 6/1000 | Loss: 0.00001796
Iteration 7/1000 | Loss: 0.00001752
Iteration 8/1000 | Loss: 0.00001719
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00001665
Iteration 11/1000 | Loss: 0.00001647
Iteration 12/1000 | Loss: 0.00001635
Iteration 13/1000 | Loss: 0.00001630
Iteration 14/1000 | Loss: 0.00001629
Iteration 15/1000 | Loss: 0.00001629
Iteration 16/1000 | Loss: 0.00001628
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00001624
Iteration 19/1000 | Loss: 0.00001623
Iteration 20/1000 | Loss: 0.00001622
Iteration 21/1000 | Loss: 0.00001621
Iteration 22/1000 | Loss: 0.00001621
Iteration 23/1000 | Loss: 0.00001621
Iteration 24/1000 | Loss: 0.00001617
Iteration 25/1000 | Loss: 0.00001617
Iteration 26/1000 | Loss: 0.00001617
Iteration 27/1000 | Loss: 0.00001615
Iteration 28/1000 | Loss: 0.00001615
Iteration 29/1000 | Loss: 0.00001614
Iteration 30/1000 | Loss: 0.00001611
Iteration 31/1000 | Loss: 0.00001609
Iteration 32/1000 | Loss: 0.00001608
Iteration 33/1000 | Loss: 0.00001608
Iteration 34/1000 | Loss: 0.00001607
Iteration 35/1000 | Loss: 0.00001607
Iteration 36/1000 | Loss: 0.00001606
Iteration 37/1000 | Loss: 0.00001606
Iteration 38/1000 | Loss: 0.00001606
Iteration 39/1000 | Loss: 0.00001606
Iteration 40/1000 | Loss: 0.00001606
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001605
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001603
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00001602
Iteration 51/1000 | Loss: 0.00001601
Iteration 52/1000 | Loss: 0.00001601
Iteration 53/1000 | Loss: 0.00001601
Iteration 54/1000 | Loss: 0.00001600
Iteration 55/1000 | Loss: 0.00001599
Iteration 56/1000 | Loss: 0.00001599
Iteration 57/1000 | Loss: 0.00001599
Iteration 58/1000 | Loss: 0.00001599
Iteration 59/1000 | Loss: 0.00001599
Iteration 60/1000 | Loss: 0.00001599
Iteration 61/1000 | Loss: 0.00001599
Iteration 62/1000 | Loss: 0.00001598
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001598
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001598
Iteration 69/1000 | Loss: 0.00001598
Iteration 70/1000 | Loss: 0.00001598
Iteration 71/1000 | Loss: 0.00001597
Iteration 72/1000 | Loss: 0.00001597
Iteration 73/1000 | Loss: 0.00001597
Iteration 74/1000 | Loss: 0.00001596
Iteration 75/1000 | Loss: 0.00001596
Iteration 76/1000 | Loss: 0.00001595
Iteration 77/1000 | Loss: 0.00001595
Iteration 78/1000 | Loss: 0.00001595
Iteration 79/1000 | Loss: 0.00001595
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001594
Iteration 83/1000 | Loss: 0.00001594
Iteration 84/1000 | Loss: 0.00001594
Iteration 85/1000 | Loss: 0.00001594
Iteration 86/1000 | Loss: 0.00001594
Iteration 87/1000 | Loss: 0.00001594
Iteration 88/1000 | Loss: 0.00001593
Iteration 89/1000 | Loss: 0.00001593
Iteration 90/1000 | Loss: 0.00001593
Iteration 91/1000 | Loss: 0.00001592
Iteration 92/1000 | Loss: 0.00001592
Iteration 93/1000 | Loss: 0.00001592
Iteration 94/1000 | Loss: 0.00001592
Iteration 95/1000 | Loss: 0.00001592
Iteration 96/1000 | Loss: 0.00001591
Iteration 97/1000 | Loss: 0.00001591
Iteration 98/1000 | Loss: 0.00001590
Iteration 99/1000 | Loss: 0.00001590
Iteration 100/1000 | Loss: 0.00001590
Iteration 101/1000 | Loss: 0.00001590
Iteration 102/1000 | Loss: 0.00001590
Iteration 103/1000 | Loss: 0.00001590
Iteration 104/1000 | Loss: 0.00001590
Iteration 105/1000 | Loss: 0.00001589
Iteration 106/1000 | Loss: 0.00001589
Iteration 107/1000 | Loss: 0.00001589
Iteration 108/1000 | Loss: 0.00001589
Iteration 109/1000 | Loss: 0.00001589
Iteration 110/1000 | Loss: 0.00001589
Iteration 111/1000 | Loss: 0.00001589
Iteration 112/1000 | Loss: 0.00001589
Iteration 113/1000 | Loss: 0.00001589
Iteration 114/1000 | Loss: 0.00001589
Iteration 115/1000 | Loss: 0.00001589
Iteration 116/1000 | Loss: 0.00001589
Iteration 117/1000 | Loss: 0.00001589
Iteration 118/1000 | Loss: 0.00001588
Iteration 119/1000 | Loss: 0.00001588
Iteration 120/1000 | Loss: 0.00001588
Iteration 121/1000 | Loss: 0.00001588
Iteration 122/1000 | Loss: 0.00001588
Iteration 123/1000 | Loss: 0.00001588
Iteration 124/1000 | Loss: 0.00001587
Iteration 125/1000 | Loss: 0.00001587
Iteration 126/1000 | Loss: 0.00001587
Iteration 127/1000 | Loss: 0.00001586
Iteration 128/1000 | Loss: 0.00001586
Iteration 129/1000 | Loss: 0.00001586
Iteration 130/1000 | Loss: 0.00001586
Iteration 131/1000 | Loss: 0.00001586
Iteration 132/1000 | Loss: 0.00001586
Iteration 133/1000 | Loss: 0.00001586
Iteration 134/1000 | Loss: 0.00001585
Iteration 135/1000 | Loss: 0.00001585
Iteration 136/1000 | Loss: 0.00001585
Iteration 137/1000 | Loss: 0.00001585
Iteration 138/1000 | Loss: 0.00001585
Iteration 139/1000 | Loss: 0.00001585
Iteration 140/1000 | Loss: 0.00001585
Iteration 141/1000 | Loss: 0.00001585
Iteration 142/1000 | Loss: 0.00001585
Iteration 143/1000 | Loss: 0.00001585
Iteration 144/1000 | Loss: 0.00001585
Iteration 145/1000 | Loss: 0.00001585
Iteration 146/1000 | Loss: 0.00001585
Iteration 147/1000 | Loss: 0.00001584
Iteration 148/1000 | Loss: 0.00001584
Iteration 149/1000 | Loss: 0.00001584
Iteration 150/1000 | Loss: 0.00001584
Iteration 151/1000 | Loss: 0.00001584
Iteration 152/1000 | Loss: 0.00001584
Iteration 153/1000 | Loss: 0.00001584
Iteration 154/1000 | Loss: 0.00001584
Iteration 155/1000 | Loss: 0.00001584
Iteration 156/1000 | Loss: 0.00001584
Iteration 157/1000 | Loss: 0.00001584
Iteration 158/1000 | Loss: 0.00001584
Iteration 159/1000 | Loss: 0.00001583
Iteration 160/1000 | Loss: 0.00001583
Iteration 161/1000 | Loss: 0.00001583
Iteration 162/1000 | Loss: 0.00001583
Iteration 163/1000 | Loss: 0.00001583
Iteration 164/1000 | Loss: 0.00001583
Iteration 165/1000 | Loss: 0.00001583
Iteration 166/1000 | Loss: 0.00001583
Iteration 167/1000 | Loss: 0.00001583
Iteration 168/1000 | Loss: 0.00001583
Iteration 169/1000 | Loss: 0.00001583
Iteration 170/1000 | Loss: 0.00001583
Iteration 171/1000 | Loss: 0.00001583
Iteration 172/1000 | Loss: 0.00001583
Iteration 173/1000 | Loss: 0.00001582
Iteration 174/1000 | Loss: 0.00001582
Iteration 175/1000 | Loss: 0.00001582
Iteration 176/1000 | Loss: 0.00001582
Iteration 177/1000 | Loss: 0.00001582
Iteration 178/1000 | Loss: 0.00001582
Iteration 179/1000 | Loss: 0.00001582
Iteration 180/1000 | Loss: 0.00001582
Iteration 181/1000 | Loss: 0.00001582
Iteration 182/1000 | Loss: 0.00001582
Iteration 183/1000 | Loss: 0.00001582
Iteration 184/1000 | Loss: 0.00001582
Iteration 185/1000 | Loss: 0.00001582
Iteration 186/1000 | Loss: 0.00001582
Iteration 187/1000 | Loss: 0.00001582
Iteration 188/1000 | Loss: 0.00001582
Iteration 189/1000 | Loss: 0.00001582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.5824700312805362e-05, 1.5824700312805362e-05, 1.5824700312805362e-05, 1.5824700312805362e-05, 1.5824700312805362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5824700312805362e-05

Optimization complete. Final v2v error: 3.349888324737549 mm

Highest mean error: 3.9394521713256836 mm for frame 29

Lowest mean error: 3.0571067333221436 mm for frame 61

Saving results

Total time: 40.35748505592346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809519
Iteration 2/25 | Loss: 0.00157211
Iteration 3/25 | Loss: 0.00124140
Iteration 4/25 | Loss: 0.00120767
Iteration 5/25 | Loss: 0.00119818
Iteration 6/25 | Loss: 0.00119642
Iteration 7/25 | Loss: 0.00119642
Iteration 8/25 | Loss: 0.00119642
Iteration 9/25 | Loss: 0.00119642
Iteration 10/25 | Loss: 0.00119642
Iteration 11/25 | Loss: 0.00119642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011964208679273725, 0.0011964208679273725, 0.0011964208679273725, 0.0011964208679273725, 0.0011964208679273725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011964208679273725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03676033
Iteration 2/25 | Loss: 0.00074387
Iteration 3/25 | Loss: 0.00074387
Iteration 4/25 | Loss: 0.00074387
Iteration 5/25 | Loss: 0.00074387
Iteration 6/25 | Loss: 0.00074387
Iteration 7/25 | Loss: 0.00074387
Iteration 8/25 | Loss: 0.00074387
Iteration 9/25 | Loss: 0.00074386
Iteration 10/25 | Loss: 0.00074386
Iteration 11/25 | Loss: 0.00074386
Iteration 12/25 | Loss: 0.00074386
Iteration 13/25 | Loss: 0.00074386
Iteration 14/25 | Loss: 0.00074386
Iteration 15/25 | Loss: 0.00074386
Iteration 16/25 | Loss: 0.00074386
Iteration 17/25 | Loss: 0.00074386
Iteration 18/25 | Loss: 0.00074386
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007438645698130131, 0.0007438645698130131, 0.0007438645698130131, 0.0007438645698130131, 0.0007438645698130131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007438645698130131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074386
Iteration 2/1000 | Loss: 0.00008319
Iteration 3/1000 | Loss: 0.00004861
Iteration 4/1000 | Loss: 0.00003874
Iteration 5/1000 | Loss: 0.00003660
Iteration 6/1000 | Loss: 0.00003527
Iteration 7/1000 | Loss: 0.00003423
Iteration 8/1000 | Loss: 0.00003341
Iteration 9/1000 | Loss: 0.00003274
Iteration 10/1000 | Loss: 0.00003224
Iteration 11/1000 | Loss: 0.00003181
Iteration 12/1000 | Loss: 0.00003147
Iteration 13/1000 | Loss: 0.00003128
Iteration 14/1000 | Loss: 0.00003122
Iteration 15/1000 | Loss: 0.00003102
Iteration 16/1000 | Loss: 0.00003081
Iteration 17/1000 | Loss: 0.00003078
Iteration 18/1000 | Loss: 0.00003060
Iteration 19/1000 | Loss: 0.00003045
Iteration 20/1000 | Loss: 0.00003043
Iteration 21/1000 | Loss: 0.00003040
Iteration 22/1000 | Loss: 0.00003030
Iteration 23/1000 | Loss: 0.00003030
Iteration 24/1000 | Loss: 0.00003030
Iteration 25/1000 | Loss: 0.00003029
Iteration 26/1000 | Loss: 0.00003028
Iteration 27/1000 | Loss: 0.00003025
Iteration 28/1000 | Loss: 0.00003025
Iteration 29/1000 | Loss: 0.00003025
Iteration 30/1000 | Loss: 0.00003025
Iteration 31/1000 | Loss: 0.00003024
Iteration 32/1000 | Loss: 0.00003024
Iteration 33/1000 | Loss: 0.00003024
Iteration 34/1000 | Loss: 0.00003024
Iteration 35/1000 | Loss: 0.00003024
Iteration 36/1000 | Loss: 0.00003023
Iteration 37/1000 | Loss: 0.00003022
Iteration 38/1000 | Loss: 0.00003019
Iteration 39/1000 | Loss: 0.00003019
Iteration 40/1000 | Loss: 0.00003019
Iteration 41/1000 | Loss: 0.00003019
Iteration 42/1000 | Loss: 0.00003019
Iteration 43/1000 | Loss: 0.00003019
Iteration 44/1000 | Loss: 0.00003016
Iteration 45/1000 | Loss: 0.00003014
Iteration 46/1000 | Loss: 0.00003013
Iteration 47/1000 | Loss: 0.00003013
Iteration 48/1000 | Loss: 0.00003013
Iteration 49/1000 | Loss: 0.00003012
Iteration 50/1000 | Loss: 0.00003012
Iteration 51/1000 | Loss: 0.00003011
Iteration 52/1000 | Loss: 0.00003011
Iteration 53/1000 | Loss: 0.00003011
Iteration 54/1000 | Loss: 0.00003010
Iteration 55/1000 | Loss: 0.00003009
Iteration 56/1000 | Loss: 0.00003009
Iteration 57/1000 | Loss: 0.00003009
Iteration 58/1000 | Loss: 0.00003008
Iteration 59/1000 | Loss: 0.00003008
Iteration 60/1000 | Loss: 0.00003008
Iteration 61/1000 | Loss: 0.00003008
Iteration 62/1000 | Loss: 0.00003008
Iteration 63/1000 | Loss: 0.00003007
Iteration 64/1000 | Loss: 0.00003007
Iteration 65/1000 | Loss: 0.00003006
Iteration 66/1000 | Loss: 0.00003006
Iteration 67/1000 | Loss: 0.00003006
Iteration 68/1000 | Loss: 0.00003005
Iteration 69/1000 | Loss: 0.00003005
Iteration 70/1000 | Loss: 0.00003005
Iteration 71/1000 | Loss: 0.00003005
Iteration 72/1000 | Loss: 0.00003005
Iteration 73/1000 | Loss: 0.00003005
Iteration 74/1000 | Loss: 0.00003005
Iteration 75/1000 | Loss: 0.00003005
Iteration 76/1000 | Loss: 0.00003004
Iteration 77/1000 | Loss: 0.00003004
Iteration 78/1000 | Loss: 0.00003004
Iteration 79/1000 | Loss: 0.00003004
Iteration 80/1000 | Loss: 0.00003004
Iteration 81/1000 | Loss: 0.00003003
Iteration 82/1000 | Loss: 0.00003003
Iteration 83/1000 | Loss: 0.00003002
Iteration 84/1000 | Loss: 0.00003001
Iteration 85/1000 | Loss: 0.00003000
Iteration 86/1000 | Loss: 0.00003000
Iteration 87/1000 | Loss: 0.00003000
Iteration 88/1000 | Loss: 0.00002999
Iteration 89/1000 | Loss: 0.00002998
Iteration 90/1000 | Loss: 0.00002998
Iteration 91/1000 | Loss: 0.00002998
Iteration 92/1000 | Loss: 0.00002997
Iteration 93/1000 | Loss: 0.00002997
Iteration 94/1000 | Loss: 0.00002997
Iteration 95/1000 | Loss: 0.00002997
Iteration 96/1000 | Loss: 0.00002996
Iteration 97/1000 | Loss: 0.00002996
Iteration 98/1000 | Loss: 0.00002996
Iteration 99/1000 | Loss: 0.00002995
Iteration 100/1000 | Loss: 0.00002995
Iteration 101/1000 | Loss: 0.00002995
Iteration 102/1000 | Loss: 0.00002995
Iteration 103/1000 | Loss: 0.00002995
Iteration 104/1000 | Loss: 0.00002994
Iteration 105/1000 | Loss: 0.00002994
Iteration 106/1000 | Loss: 0.00002994
Iteration 107/1000 | Loss: 0.00002993
Iteration 108/1000 | Loss: 0.00002993
Iteration 109/1000 | Loss: 0.00002993
Iteration 110/1000 | Loss: 0.00002992
Iteration 111/1000 | Loss: 0.00002992
Iteration 112/1000 | Loss: 0.00002992
Iteration 113/1000 | Loss: 0.00002992
Iteration 114/1000 | Loss: 0.00002992
Iteration 115/1000 | Loss: 0.00002992
Iteration 116/1000 | Loss: 0.00002992
Iteration 117/1000 | Loss: 0.00002992
Iteration 118/1000 | Loss: 0.00002992
Iteration 119/1000 | Loss: 0.00002992
Iteration 120/1000 | Loss: 0.00002992
Iteration 121/1000 | Loss: 0.00002992
Iteration 122/1000 | Loss: 0.00002992
Iteration 123/1000 | Loss: 0.00002992
Iteration 124/1000 | Loss: 0.00002992
Iteration 125/1000 | Loss: 0.00002992
Iteration 126/1000 | Loss: 0.00002992
Iteration 127/1000 | Loss: 0.00002992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.992037479998544e-05, 2.992037479998544e-05, 2.992037479998544e-05, 2.992037479998544e-05, 2.992037479998544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.992037479998544e-05

Optimization complete. Final v2v error: 4.324465274810791 mm

Highest mean error: 5.834348201751709 mm for frame 154

Lowest mean error: 3.6370208263397217 mm for frame 120

Saving results

Total time: 51.180962324142456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00611369
Iteration 2/25 | Loss: 0.00127859
Iteration 3/25 | Loss: 0.00115162
Iteration 4/25 | Loss: 0.00112521
Iteration 5/25 | Loss: 0.00112205
Iteration 6/25 | Loss: 0.00112115
Iteration 7/25 | Loss: 0.00112115
Iteration 8/25 | Loss: 0.00112115
Iteration 9/25 | Loss: 0.00112115
Iteration 10/25 | Loss: 0.00112115
Iteration 11/25 | Loss: 0.00112115
Iteration 12/25 | Loss: 0.00112115
Iteration 13/25 | Loss: 0.00112115
Iteration 14/25 | Loss: 0.00112115
Iteration 15/25 | Loss: 0.00112115
Iteration 16/25 | Loss: 0.00112115
Iteration 17/25 | Loss: 0.00112115
Iteration 18/25 | Loss: 0.00112115
Iteration 19/25 | Loss: 0.00112115
Iteration 20/25 | Loss: 0.00112115
Iteration 21/25 | Loss: 0.00112115
Iteration 22/25 | Loss: 0.00112115
Iteration 23/25 | Loss: 0.00112115
Iteration 24/25 | Loss: 0.00112115
Iteration 25/25 | Loss: 0.00112115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011211509117856622, 0.0011211509117856622, 0.0011211509117856622, 0.0011211509117856622, 0.0011211509117856622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011211509117856622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.57298422
Iteration 2/25 | Loss: 0.00071610
Iteration 3/25 | Loss: 0.00071610
Iteration 4/25 | Loss: 0.00071609
Iteration 5/25 | Loss: 0.00071609
Iteration 6/25 | Loss: 0.00071609
Iteration 7/25 | Loss: 0.00071609
Iteration 8/25 | Loss: 0.00071609
Iteration 9/25 | Loss: 0.00071609
Iteration 10/25 | Loss: 0.00071609
Iteration 11/25 | Loss: 0.00071609
Iteration 12/25 | Loss: 0.00071609
Iteration 13/25 | Loss: 0.00071609
Iteration 14/25 | Loss: 0.00071609
Iteration 15/25 | Loss: 0.00071609
Iteration 16/25 | Loss: 0.00071609
Iteration 17/25 | Loss: 0.00071609
Iteration 18/25 | Loss: 0.00071609
Iteration 19/25 | Loss: 0.00071609
Iteration 20/25 | Loss: 0.00071609
Iteration 21/25 | Loss: 0.00071609
Iteration 22/25 | Loss: 0.00071609
Iteration 23/25 | Loss: 0.00071609
Iteration 24/25 | Loss: 0.00071609
Iteration 25/25 | Loss: 0.00071609

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071609
Iteration 2/1000 | Loss: 0.00002939
Iteration 3/1000 | Loss: 0.00002318
Iteration 4/1000 | Loss: 0.00002191
Iteration 5/1000 | Loss: 0.00002125
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00002044
Iteration 8/1000 | Loss: 0.00002027
Iteration 9/1000 | Loss: 0.00001999
Iteration 10/1000 | Loss: 0.00001996
Iteration 11/1000 | Loss: 0.00001983
Iteration 12/1000 | Loss: 0.00001982
Iteration 13/1000 | Loss: 0.00001979
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001972
Iteration 16/1000 | Loss: 0.00001968
Iteration 17/1000 | Loss: 0.00001965
Iteration 18/1000 | Loss: 0.00001964
Iteration 19/1000 | Loss: 0.00001964
Iteration 20/1000 | Loss: 0.00001963
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001960
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001959
Iteration 25/1000 | Loss: 0.00001959
Iteration 26/1000 | Loss: 0.00001959
Iteration 27/1000 | Loss: 0.00001958
Iteration 28/1000 | Loss: 0.00001958
Iteration 29/1000 | Loss: 0.00001958
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001957
Iteration 32/1000 | Loss: 0.00001956
Iteration 33/1000 | Loss: 0.00001956
Iteration 34/1000 | Loss: 0.00001955
Iteration 35/1000 | Loss: 0.00001955
Iteration 36/1000 | Loss: 0.00001955
Iteration 37/1000 | Loss: 0.00001955
Iteration 38/1000 | Loss: 0.00001954
Iteration 39/1000 | Loss: 0.00001954
Iteration 40/1000 | Loss: 0.00001953
Iteration 41/1000 | Loss: 0.00001952
Iteration 42/1000 | Loss: 0.00001952
Iteration 43/1000 | Loss: 0.00001952
Iteration 44/1000 | Loss: 0.00001951
Iteration 45/1000 | Loss: 0.00001951
Iteration 46/1000 | Loss: 0.00001951
Iteration 47/1000 | Loss: 0.00001950
Iteration 48/1000 | Loss: 0.00001950
Iteration 49/1000 | Loss: 0.00001949
Iteration 50/1000 | Loss: 0.00001948
Iteration 51/1000 | Loss: 0.00001948
Iteration 52/1000 | Loss: 0.00001948
Iteration 53/1000 | Loss: 0.00001948
Iteration 54/1000 | Loss: 0.00001948
Iteration 55/1000 | Loss: 0.00001948
Iteration 56/1000 | Loss: 0.00001948
Iteration 57/1000 | Loss: 0.00001947
Iteration 58/1000 | Loss: 0.00001947
Iteration 59/1000 | Loss: 0.00001947
Iteration 60/1000 | Loss: 0.00001947
Iteration 61/1000 | Loss: 0.00001947
Iteration 62/1000 | Loss: 0.00001947
Iteration 63/1000 | Loss: 0.00001947
Iteration 64/1000 | Loss: 0.00001947
Iteration 65/1000 | Loss: 0.00001946
Iteration 66/1000 | Loss: 0.00001946
Iteration 67/1000 | Loss: 0.00001945
Iteration 68/1000 | Loss: 0.00001944
Iteration 69/1000 | Loss: 0.00001944
Iteration 70/1000 | Loss: 0.00001944
Iteration 71/1000 | Loss: 0.00001944
Iteration 72/1000 | Loss: 0.00001943
Iteration 73/1000 | Loss: 0.00001943
Iteration 74/1000 | Loss: 0.00001942
Iteration 75/1000 | Loss: 0.00001942
Iteration 76/1000 | Loss: 0.00001941
Iteration 77/1000 | Loss: 0.00001941
Iteration 78/1000 | Loss: 0.00001940
Iteration 79/1000 | Loss: 0.00001940
Iteration 80/1000 | Loss: 0.00001940
Iteration 81/1000 | Loss: 0.00001939
Iteration 82/1000 | Loss: 0.00001939
Iteration 83/1000 | Loss: 0.00001939
Iteration 84/1000 | Loss: 0.00001937
Iteration 85/1000 | Loss: 0.00001937
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001936
Iteration 88/1000 | Loss: 0.00001936
Iteration 89/1000 | Loss: 0.00001936
Iteration 90/1000 | Loss: 0.00001936
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001935
Iteration 95/1000 | Loss: 0.00001935
Iteration 96/1000 | Loss: 0.00001935
Iteration 97/1000 | Loss: 0.00001935
Iteration 98/1000 | Loss: 0.00001935
Iteration 99/1000 | Loss: 0.00001935
Iteration 100/1000 | Loss: 0.00001935
Iteration 101/1000 | Loss: 0.00001935
Iteration 102/1000 | Loss: 0.00001935
Iteration 103/1000 | Loss: 0.00001935
Iteration 104/1000 | Loss: 0.00001935
Iteration 105/1000 | Loss: 0.00001935
Iteration 106/1000 | Loss: 0.00001935
Iteration 107/1000 | Loss: 0.00001935
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.935310683620628e-05, 1.935310683620628e-05, 1.935310683620628e-05, 1.935310683620628e-05, 1.935310683620628e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.935310683620628e-05

Optimization complete. Final v2v error: 3.7291576862335205 mm

Highest mean error: 3.9625823497772217 mm for frame 65

Lowest mean error: 3.6181704998016357 mm for frame 118

Saving results

Total time: 32.924781799316406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972890
Iteration 2/25 | Loss: 0.00365279
Iteration 3/25 | Loss: 0.00289076
Iteration 4/25 | Loss: 0.00227656
Iteration 5/25 | Loss: 0.00208482
Iteration 6/25 | Loss: 0.00197484
Iteration 7/25 | Loss: 0.00189506
Iteration 8/25 | Loss: 0.00184489
Iteration 9/25 | Loss: 0.00182258
Iteration 10/25 | Loss: 0.00180662
Iteration 11/25 | Loss: 0.00179572
Iteration 12/25 | Loss: 0.00179208
Iteration 13/25 | Loss: 0.00179186
Iteration 14/25 | Loss: 0.00179152
Iteration 15/25 | Loss: 0.00179194
Iteration 16/25 | Loss: 0.00179150
Iteration 17/25 | Loss: 0.00179150
Iteration 18/25 | Loss: 0.00179150
Iteration 19/25 | Loss: 0.00179150
Iteration 20/25 | Loss: 0.00179150
Iteration 21/25 | Loss: 0.00179149
Iteration 22/25 | Loss: 0.00179149
Iteration 23/25 | Loss: 0.00179149
Iteration 24/25 | Loss: 0.00179149
Iteration 25/25 | Loss: 0.00179149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31424236
Iteration 2/25 | Loss: 0.00440925
Iteration 3/25 | Loss: 0.00437616
Iteration 4/25 | Loss: 0.00437563
Iteration 5/25 | Loss: 0.00436719
Iteration 6/25 | Loss: 0.00436718
Iteration 7/25 | Loss: 0.00436718
Iteration 8/25 | Loss: 0.00436718
Iteration 9/25 | Loss: 0.00436718
Iteration 10/25 | Loss: 0.00436718
Iteration 11/25 | Loss: 0.00436718
Iteration 12/25 | Loss: 0.00436718
Iteration 13/25 | Loss: 0.00436718
Iteration 14/25 | Loss: 0.00436718
Iteration 15/25 | Loss: 0.00436718
Iteration 16/25 | Loss: 0.00436718
Iteration 17/25 | Loss: 0.00436718
Iteration 18/25 | Loss: 0.00436718
Iteration 19/25 | Loss: 0.00436718
Iteration 20/25 | Loss: 0.00436718
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004367181099951267, 0.004367181099951267, 0.004367181099951267, 0.004367181099951267, 0.004367181099951267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004367181099951267

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00436718
Iteration 2/1000 | Loss: 0.00071611
Iteration 3/1000 | Loss: 0.00060365
Iteration 4/1000 | Loss: 0.00067292
Iteration 5/1000 | Loss: 0.00077067
Iteration 6/1000 | Loss: 0.00082281
Iteration 7/1000 | Loss: 0.00040164
Iteration 8/1000 | Loss: 0.00038582
Iteration 9/1000 | Loss: 0.00040313
Iteration 10/1000 | Loss: 0.00034698
Iteration 11/1000 | Loss: 0.00033813
Iteration 12/1000 | Loss: 0.00304121
Iteration 13/1000 | Loss: 0.00041211
Iteration 14/1000 | Loss: 0.00077598
Iteration 15/1000 | Loss: 0.01889612
Iteration 16/1000 | Loss: 0.00877084
Iteration 17/1000 | Loss: 0.01993852
Iteration 18/1000 | Loss: 0.00811977
Iteration 19/1000 | Loss: 0.00397210
Iteration 20/1000 | Loss: 0.00449642
Iteration 21/1000 | Loss: 0.00332407
Iteration 22/1000 | Loss: 0.00333462
Iteration 23/1000 | Loss: 0.00080656
Iteration 24/1000 | Loss: 0.00086121
Iteration 25/1000 | Loss: 0.00032679
Iteration 26/1000 | Loss: 0.00051304
Iteration 27/1000 | Loss: 0.00027632
Iteration 28/1000 | Loss: 0.00008768
Iteration 29/1000 | Loss: 0.00014170
Iteration 30/1000 | Loss: 0.00006410
Iteration 31/1000 | Loss: 0.00011291
Iteration 32/1000 | Loss: 0.00026533
Iteration 33/1000 | Loss: 0.00008249
Iteration 34/1000 | Loss: 0.00006644
Iteration 35/1000 | Loss: 0.00011815
Iteration 36/1000 | Loss: 0.00008811
Iteration 37/1000 | Loss: 0.00006973
Iteration 38/1000 | Loss: 0.00009254
Iteration 39/1000 | Loss: 0.00009514
Iteration 40/1000 | Loss: 0.00002877
Iteration 41/1000 | Loss: 0.00010122
Iteration 42/1000 | Loss: 0.00003809
Iteration 43/1000 | Loss: 0.00003911
Iteration 44/1000 | Loss: 0.00002282
Iteration 45/1000 | Loss: 0.00002614
Iteration 46/1000 | Loss: 0.00002527
Iteration 47/1000 | Loss: 0.00002001
Iteration 48/1000 | Loss: 0.00001996
Iteration 49/1000 | Loss: 0.00002809
Iteration 50/1000 | Loss: 0.00016089
Iteration 51/1000 | Loss: 0.00003100
Iteration 52/1000 | Loss: 0.00001735
Iteration 53/1000 | Loss: 0.00001545
Iteration 54/1000 | Loss: 0.00001545
Iteration 55/1000 | Loss: 0.00001545
Iteration 56/1000 | Loss: 0.00001545
Iteration 57/1000 | Loss: 0.00001545
Iteration 58/1000 | Loss: 0.00001743
Iteration 59/1000 | Loss: 0.00001544
Iteration 60/1000 | Loss: 0.00001544
Iteration 61/1000 | Loss: 0.00001543
Iteration 62/1000 | Loss: 0.00001543
Iteration 63/1000 | Loss: 0.00001543
Iteration 64/1000 | Loss: 0.00001900
Iteration 65/1000 | Loss: 0.00002051
Iteration 66/1000 | Loss: 0.00001603
Iteration 67/1000 | Loss: 0.00001601
Iteration 68/1000 | Loss: 0.00002866
Iteration 69/1000 | Loss: 0.00001516
Iteration 70/1000 | Loss: 0.00001614
Iteration 71/1000 | Loss: 0.00001645
Iteration 72/1000 | Loss: 0.00001644
Iteration 73/1000 | Loss: 0.00002316
Iteration 74/1000 | Loss: 0.00001502
Iteration 75/1000 | Loss: 0.00001704
Iteration 76/1000 | Loss: 0.00001504
Iteration 77/1000 | Loss: 0.00001506
Iteration 78/1000 | Loss: 0.00001506
Iteration 79/1000 | Loss: 0.00001615
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00003013
Iteration 82/1000 | Loss: 0.00002560
Iteration 83/1000 | Loss: 0.00001922
Iteration 84/1000 | Loss: 0.00001740
Iteration 85/1000 | Loss: 0.00001509
Iteration 86/1000 | Loss: 0.00002244
Iteration 87/1000 | Loss: 0.00001500
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001499
Iteration 93/1000 | Loss: 0.00001499
Iteration 94/1000 | Loss: 0.00001499
Iteration 95/1000 | Loss: 0.00001499
Iteration 96/1000 | Loss: 0.00001499
Iteration 97/1000 | Loss: 0.00001499
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001498
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001498
Iteration 107/1000 | Loss: 0.00001498
Iteration 108/1000 | Loss: 0.00001498
Iteration 109/1000 | Loss: 0.00001498
Iteration 110/1000 | Loss: 0.00001498
Iteration 111/1000 | Loss: 0.00001498
Iteration 112/1000 | Loss: 0.00001498
Iteration 113/1000 | Loss: 0.00001498
Iteration 114/1000 | Loss: 0.00001498
Iteration 115/1000 | Loss: 0.00001498
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001498
Iteration 120/1000 | Loss: 0.00001498
Iteration 121/1000 | Loss: 0.00001498
Iteration 122/1000 | Loss: 0.00001498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.4976690181356389e-05, 1.4976690181356389e-05, 1.4976690181356389e-05, 1.4976690181356389e-05, 1.4976690181356389e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4976690181356389e-05

Optimization complete. Final v2v error: 3.303309679031372 mm

Highest mean error: 3.448458671569824 mm for frame 28

Lowest mean error: 2.8654983043670654 mm for frame 135

Saving results

Total time: 124.0174012184143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486761
Iteration 2/25 | Loss: 0.00116391
Iteration 3/25 | Loss: 0.00108017
Iteration 4/25 | Loss: 0.00106864
Iteration 5/25 | Loss: 0.00106521
Iteration 6/25 | Loss: 0.00106416
Iteration 7/25 | Loss: 0.00106416
Iteration 8/25 | Loss: 0.00106416
Iteration 9/25 | Loss: 0.00106416
Iteration 10/25 | Loss: 0.00106416
Iteration 11/25 | Loss: 0.00106416
Iteration 12/25 | Loss: 0.00106416
Iteration 13/25 | Loss: 0.00106416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010641610715538263, 0.0010641610715538263, 0.0010641610715538263, 0.0010641610715538263, 0.0010641610715538263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010641610715538263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38006878
Iteration 2/25 | Loss: 0.00072142
Iteration 3/25 | Loss: 0.00072138
Iteration 4/25 | Loss: 0.00072138
Iteration 5/25 | Loss: 0.00072138
Iteration 6/25 | Loss: 0.00072138
Iteration 7/25 | Loss: 0.00072138
Iteration 8/25 | Loss: 0.00072138
Iteration 9/25 | Loss: 0.00072138
Iteration 10/25 | Loss: 0.00072138
Iteration 11/25 | Loss: 0.00072138
Iteration 12/25 | Loss: 0.00072138
Iteration 13/25 | Loss: 0.00072138
Iteration 14/25 | Loss: 0.00072138
Iteration 15/25 | Loss: 0.00072138
Iteration 16/25 | Loss: 0.00072138
Iteration 17/25 | Loss: 0.00072138
Iteration 18/25 | Loss: 0.00072138
Iteration 19/25 | Loss: 0.00072138
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007213809294626117, 0.0007213809294626117, 0.0007213809294626117, 0.0007213809294626117, 0.0007213809294626117]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007213809294626117

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072138
Iteration 2/1000 | Loss: 0.00002916
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00001392
Iteration 5/1000 | Loss: 0.00001241
Iteration 6/1000 | Loss: 0.00001165
Iteration 7/1000 | Loss: 0.00001127
Iteration 8/1000 | Loss: 0.00001101
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001081
Iteration 11/1000 | Loss: 0.00001080
Iteration 12/1000 | Loss: 0.00001073
Iteration 13/1000 | Loss: 0.00001067
Iteration 14/1000 | Loss: 0.00001066
Iteration 15/1000 | Loss: 0.00001058
Iteration 16/1000 | Loss: 0.00001056
Iteration 17/1000 | Loss: 0.00001055
Iteration 18/1000 | Loss: 0.00001055
Iteration 19/1000 | Loss: 0.00001055
Iteration 20/1000 | Loss: 0.00001048
Iteration 21/1000 | Loss: 0.00001048
Iteration 22/1000 | Loss: 0.00001048
Iteration 23/1000 | Loss: 0.00001047
Iteration 24/1000 | Loss: 0.00001043
Iteration 25/1000 | Loss: 0.00001043
Iteration 26/1000 | Loss: 0.00001042
Iteration 27/1000 | Loss: 0.00001041
Iteration 28/1000 | Loss: 0.00001041
Iteration 29/1000 | Loss: 0.00001041
Iteration 30/1000 | Loss: 0.00001040
Iteration 31/1000 | Loss: 0.00001040
Iteration 32/1000 | Loss: 0.00001039
Iteration 33/1000 | Loss: 0.00001038
Iteration 34/1000 | Loss: 0.00001038
Iteration 35/1000 | Loss: 0.00001038
Iteration 36/1000 | Loss: 0.00001037
Iteration 37/1000 | Loss: 0.00001037
Iteration 38/1000 | Loss: 0.00001036
Iteration 39/1000 | Loss: 0.00001036
Iteration 40/1000 | Loss: 0.00001035
Iteration 41/1000 | Loss: 0.00001034
Iteration 42/1000 | Loss: 0.00001033
Iteration 43/1000 | Loss: 0.00001033
Iteration 44/1000 | Loss: 0.00001032
Iteration 45/1000 | Loss: 0.00001032
Iteration 46/1000 | Loss: 0.00001031
Iteration 47/1000 | Loss: 0.00001031
Iteration 48/1000 | Loss: 0.00001030
Iteration 49/1000 | Loss: 0.00001029
Iteration 50/1000 | Loss: 0.00001029
Iteration 51/1000 | Loss: 0.00001028
Iteration 52/1000 | Loss: 0.00001028
Iteration 53/1000 | Loss: 0.00001028
Iteration 54/1000 | Loss: 0.00001027
Iteration 55/1000 | Loss: 0.00001027
Iteration 56/1000 | Loss: 0.00001026
Iteration 57/1000 | Loss: 0.00001024
Iteration 58/1000 | Loss: 0.00001024
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001023
Iteration 61/1000 | Loss: 0.00001023
Iteration 62/1000 | Loss: 0.00001022
Iteration 63/1000 | Loss: 0.00001022
Iteration 64/1000 | Loss: 0.00001022
Iteration 65/1000 | Loss: 0.00001022
Iteration 66/1000 | Loss: 0.00001021
Iteration 67/1000 | Loss: 0.00001020
Iteration 68/1000 | Loss: 0.00001019
Iteration 69/1000 | Loss: 0.00001018
Iteration 70/1000 | Loss: 0.00001018
Iteration 71/1000 | Loss: 0.00001017
Iteration 72/1000 | Loss: 0.00001017
Iteration 73/1000 | Loss: 0.00001016
Iteration 74/1000 | Loss: 0.00001016
Iteration 75/1000 | Loss: 0.00001016
Iteration 76/1000 | Loss: 0.00001016
Iteration 77/1000 | Loss: 0.00001015
Iteration 78/1000 | Loss: 0.00001015
Iteration 79/1000 | Loss: 0.00001015
Iteration 80/1000 | Loss: 0.00001014
Iteration 81/1000 | Loss: 0.00001014
Iteration 82/1000 | Loss: 0.00001014
Iteration 83/1000 | Loss: 0.00001013
Iteration 84/1000 | Loss: 0.00001013
Iteration 85/1000 | Loss: 0.00001013
Iteration 86/1000 | Loss: 0.00001013
Iteration 87/1000 | Loss: 0.00001012
Iteration 88/1000 | Loss: 0.00001012
Iteration 89/1000 | Loss: 0.00001012
Iteration 90/1000 | Loss: 0.00001012
Iteration 91/1000 | Loss: 0.00001011
Iteration 92/1000 | Loss: 0.00001011
Iteration 93/1000 | Loss: 0.00001011
Iteration 94/1000 | Loss: 0.00001011
Iteration 95/1000 | Loss: 0.00001011
Iteration 96/1000 | Loss: 0.00001011
Iteration 97/1000 | Loss: 0.00001011
Iteration 98/1000 | Loss: 0.00001011
Iteration 99/1000 | Loss: 0.00001011
Iteration 100/1000 | Loss: 0.00001011
Iteration 101/1000 | Loss: 0.00001011
Iteration 102/1000 | Loss: 0.00001011
Iteration 103/1000 | Loss: 0.00001010
Iteration 104/1000 | Loss: 0.00001010
Iteration 105/1000 | Loss: 0.00001010
Iteration 106/1000 | Loss: 0.00001010
Iteration 107/1000 | Loss: 0.00001009
Iteration 108/1000 | Loss: 0.00001009
Iteration 109/1000 | Loss: 0.00001008
Iteration 110/1000 | Loss: 0.00001008
Iteration 111/1000 | Loss: 0.00001008
Iteration 112/1000 | Loss: 0.00001007
Iteration 113/1000 | Loss: 0.00001007
Iteration 114/1000 | Loss: 0.00001007
Iteration 115/1000 | Loss: 0.00001007
Iteration 116/1000 | Loss: 0.00001006
Iteration 117/1000 | Loss: 0.00001006
Iteration 118/1000 | Loss: 0.00001006
Iteration 119/1000 | Loss: 0.00001005
Iteration 120/1000 | Loss: 0.00001005
Iteration 121/1000 | Loss: 0.00001005
Iteration 122/1000 | Loss: 0.00001005
Iteration 123/1000 | Loss: 0.00001004
Iteration 124/1000 | Loss: 0.00001004
Iteration 125/1000 | Loss: 0.00001004
Iteration 126/1000 | Loss: 0.00001003
Iteration 127/1000 | Loss: 0.00001003
Iteration 128/1000 | Loss: 0.00001002
Iteration 129/1000 | Loss: 0.00001002
Iteration 130/1000 | Loss: 0.00001002
Iteration 131/1000 | Loss: 0.00001001
Iteration 132/1000 | Loss: 0.00001001
Iteration 133/1000 | Loss: 0.00001001
Iteration 134/1000 | Loss: 0.00001001
Iteration 135/1000 | Loss: 0.00001001
Iteration 136/1000 | Loss: 0.00001000
Iteration 137/1000 | Loss: 0.00001000
Iteration 138/1000 | Loss: 0.00001000
Iteration 139/1000 | Loss: 0.00001000
Iteration 140/1000 | Loss: 0.00001000
Iteration 141/1000 | Loss: 0.00001000
Iteration 142/1000 | Loss: 0.00001000
Iteration 143/1000 | Loss: 0.00000999
Iteration 144/1000 | Loss: 0.00000999
Iteration 145/1000 | Loss: 0.00000999
Iteration 146/1000 | Loss: 0.00000998
Iteration 147/1000 | Loss: 0.00000998
Iteration 148/1000 | Loss: 0.00000998
Iteration 149/1000 | Loss: 0.00000998
Iteration 150/1000 | Loss: 0.00000998
Iteration 151/1000 | Loss: 0.00000998
Iteration 152/1000 | Loss: 0.00000998
Iteration 153/1000 | Loss: 0.00000997
Iteration 154/1000 | Loss: 0.00000997
Iteration 155/1000 | Loss: 0.00000997
Iteration 156/1000 | Loss: 0.00000997
Iteration 157/1000 | Loss: 0.00000997
Iteration 158/1000 | Loss: 0.00000996
Iteration 159/1000 | Loss: 0.00000996
Iteration 160/1000 | Loss: 0.00000996
Iteration 161/1000 | Loss: 0.00000996
Iteration 162/1000 | Loss: 0.00000996
Iteration 163/1000 | Loss: 0.00000996
Iteration 164/1000 | Loss: 0.00000995
Iteration 165/1000 | Loss: 0.00000995
Iteration 166/1000 | Loss: 0.00000995
Iteration 167/1000 | Loss: 0.00000995
Iteration 168/1000 | Loss: 0.00000995
Iteration 169/1000 | Loss: 0.00000994
Iteration 170/1000 | Loss: 0.00000994
Iteration 171/1000 | Loss: 0.00000994
Iteration 172/1000 | Loss: 0.00000994
Iteration 173/1000 | Loss: 0.00000994
Iteration 174/1000 | Loss: 0.00000994
Iteration 175/1000 | Loss: 0.00000993
Iteration 176/1000 | Loss: 0.00000993
Iteration 177/1000 | Loss: 0.00000993
Iteration 178/1000 | Loss: 0.00000993
Iteration 179/1000 | Loss: 0.00000993
Iteration 180/1000 | Loss: 0.00000993
Iteration 181/1000 | Loss: 0.00000993
Iteration 182/1000 | Loss: 0.00000993
Iteration 183/1000 | Loss: 0.00000993
Iteration 184/1000 | Loss: 0.00000993
Iteration 185/1000 | Loss: 0.00000992
Iteration 186/1000 | Loss: 0.00000992
Iteration 187/1000 | Loss: 0.00000992
Iteration 188/1000 | Loss: 0.00000992
Iteration 189/1000 | Loss: 0.00000992
Iteration 190/1000 | Loss: 0.00000992
Iteration 191/1000 | Loss: 0.00000992
Iteration 192/1000 | Loss: 0.00000992
Iteration 193/1000 | Loss: 0.00000992
Iteration 194/1000 | Loss: 0.00000991
Iteration 195/1000 | Loss: 0.00000991
Iteration 196/1000 | Loss: 0.00000991
Iteration 197/1000 | Loss: 0.00000991
Iteration 198/1000 | Loss: 0.00000991
Iteration 199/1000 | Loss: 0.00000991
Iteration 200/1000 | Loss: 0.00000991
Iteration 201/1000 | Loss: 0.00000991
Iteration 202/1000 | Loss: 0.00000991
Iteration 203/1000 | Loss: 0.00000991
Iteration 204/1000 | Loss: 0.00000991
Iteration 205/1000 | Loss: 0.00000991
Iteration 206/1000 | Loss: 0.00000991
Iteration 207/1000 | Loss: 0.00000991
Iteration 208/1000 | Loss: 0.00000991
Iteration 209/1000 | Loss: 0.00000991
Iteration 210/1000 | Loss: 0.00000991
Iteration 211/1000 | Loss: 0.00000990
Iteration 212/1000 | Loss: 0.00000990
Iteration 213/1000 | Loss: 0.00000990
Iteration 214/1000 | Loss: 0.00000990
Iteration 215/1000 | Loss: 0.00000990
Iteration 216/1000 | Loss: 0.00000990
Iteration 217/1000 | Loss: 0.00000990
Iteration 218/1000 | Loss: 0.00000990
Iteration 219/1000 | Loss: 0.00000990
Iteration 220/1000 | Loss: 0.00000990
Iteration 221/1000 | Loss: 0.00000990
Iteration 222/1000 | Loss: 0.00000990
Iteration 223/1000 | Loss: 0.00000990
Iteration 224/1000 | Loss: 0.00000990
Iteration 225/1000 | Loss: 0.00000990
Iteration 226/1000 | Loss: 0.00000990
Iteration 227/1000 | Loss: 0.00000990
Iteration 228/1000 | Loss: 0.00000990
Iteration 229/1000 | Loss: 0.00000990
Iteration 230/1000 | Loss: 0.00000990
Iteration 231/1000 | Loss: 0.00000989
Iteration 232/1000 | Loss: 0.00000989
Iteration 233/1000 | Loss: 0.00000989
Iteration 234/1000 | Loss: 0.00000989
Iteration 235/1000 | Loss: 0.00000989
Iteration 236/1000 | Loss: 0.00000989
Iteration 237/1000 | Loss: 0.00000989
Iteration 238/1000 | Loss: 0.00000989
Iteration 239/1000 | Loss: 0.00000989
Iteration 240/1000 | Loss: 0.00000989
Iteration 241/1000 | Loss: 0.00000989
Iteration 242/1000 | Loss: 0.00000989
Iteration 243/1000 | Loss: 0.00000989
Iteration 244/1000 | Loss: 0.00000989
Iteration 245/1000 | Loss: 0.00000989
Iteration 246/1000 | Loss: 0.00000989
Iteration 247/1000 | Loss: 0.00000989
Iteration 248/1000 | Loss: 0.00000989
Iteration 249/1000 | Loss: 0.00000989
Iteration 250/1000 | Loss: 0.00000988
Iteration 251/1000 | Loss: 0.00000988
Iteration 252/1000 | Loss: 0.00000988
Iteration 253/1000 | Loss: 0.00000988
Iteration 254/1000 | Loss: 0.00000988
Iteration 255/1000 | Loss: 0.00000988
Iteration 256/1000 | Loss: 0.00000988
Iteration 257/1000 | Loss: 0.00000988
Iteration 258/1000 | Loss: 0.00000988
Iteration 259/1000 | Loss: 0.00000988
Iteration 260/1000 | Loss: 0.00000988
Iteration 261/1000 | Loss: 0.00000988
Iteration 262/1000 | Loss: 0.00000988
Iteration 263/1000 | Loss: 0.00000988
Iteration 264/1000 | Loss: 0.00000988
Iteration 265/1000 | Loss: 0.00000988
Iteration 266/1000 | Loss: 0.00000988
Iteration 267/1000 | Loss: 0.00000988
Iteration 268/1000 | Loss: 0.00000988
Iteration 269/1000 | Loss: 0.00000988
Iteration 270/1000 | Loss: 0.00000988
Iteration 271/1000 | Loss: 0.00000988
Iteration 272/1000 | Loss: 0.00000988
Iteration 273/1000 | Loss: 0.00000988
Iteration 274/1000 | Loss: 0.00000988
Iteration 275/1000 | Loss: 0.00000988
Iteration 276/1000 | Loss: 0.00000988
Iteration 277/1000 | Loss: 0.00000988
Iteration 278/1000 | Loss: 0.00000988
Iteration 279/1000 | Loss: 0.00000988
Iteration 280/1000 | Loss: 0.00000988
Iteration 281/1000 | Loss: 0.00000988
Iteration 282/1000 | Loss: 0.00000988
Iteration 283/1000 | Loss: 0.00000988
Iteration 284/1000 | Loss: 0.00000988
Iteration 285/1000 | Loss: 0.00000988
Iteration 286/1000 | Loss: 0.00000988
Iteration 287/1000 | Loss: 0.00000988
Iteration 288/1000 | Loss: 0.00000988
Iteration 289/1000 | Loss: 0.00000988
Iteration 290/1000 | Loss: 0.00000988
Iteration 291/1000 | Loss: 0.00000988
Iteration 292/1000 | Loss: 0.00000988
Iteration 293/1000 | Loss: 0.00000988
Iteration 294/1000 | Loss: 0.00000988
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 294. Stopping optimization.
Last 5 losses: [9.882246558845509e-06, 9.882246558845509e-06, 9.882246558845509e-06, 9.882246558845509e-06, 9.882246558845509e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.882246558845509e-06

Optimization complete. Final v2v error: 2.6462395191192627 mm

Highest mean error: 2.972896099090576 mm for frame 41

Lowest mean error: 2.3034517765045166 mm for frame 13

Saving results

Total time: 44.54133319854736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886920
Iteration 2/25 | Loss: 0.00139718
Iteration 3/25 | Loss: 0.00119807
Iteration 4/25 | Loss: 0.00117488
Iteration 5/25 | Loss: 0.00117354
Iteration 6/25 | Loss: 0.00117798
Iteration 7/25 | Loss: 0.00117199
Iteration 8/25 | Loss: 0.00116824
Iteration 9/25 | Loss: 0.00117185
Iteration 10/25 | Loss: 0.00116647
Iteration 11/25 | Loss: 0.00116638
Iteration 12/25 | Loss: 0.00116637
Iteration 13/25 | Loss: 0.00116637
Iteration 14/25 | Loss: 0.00116637
Iteration 15/25 | Loss: 0.00116637
Iteration 16/25 | Loss: 0.00116637
Iteration 17/25 | Loss: 0.00116637
Iteration 18/25 | Loss: 0.00116637
Iteration 19/25 | Loss: 0.00116637
Iteration 20/25 | Loss: 0.00116637
Iteration 21/25 | Loss: 0.00116636
Iteration 22/25 | Loss: 0.00116636
Iteration 23/25 | Loss: 0.00116636
Iteration 24/25 | Loss: 0.00116636
Iteration 25/25 | Loss: 0.00116636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 13.36454964
Iteration 2/25 | Loss: 0.00061770
Iteration 3/25 | Loss: 0.00061758
Iteration 4/25 | Loss: 0.00061758
Iteration 5/25 | Loss: 0.00061758
Iteration 6/25 | Loss: 0.00061758
Iteration 7/25 | Loss: 0.00061758
Iteration 8/25 | Loss: 0.00061758
Iteration 9/25 | Loss: 0.00061757
Iteration 10/25 | Loss: 0.00061757
Iteration 11/25 | Loss: 0.00061757
Iteration 12/25 | Loss: 0.00061757
Iteration 13/25 | Loss: 0.00061757
Iteration 14/25 | Loss: 0.00061757
Iteration 15/25 | Loss: 0.00061757
Iteration 16/25 | Loss: 0.00061757
Iteration 17/25 | Loss: 0.00061757
Iteration 18/25 | Loss: 0.00061757
Iteration 19/25 | Loss: 0.00061757
Iteration 20/25 | Loss: 0.00061757
Iteration 21/25 | Loss: 0.00061757
Iteration 22/25 | Loss: 0.00061757
Iteration 23/25 | Loss: 0.00061757
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006175742018967867, 0.0006175742018967867, 0.0006175742018967867, 0.0006175742018967867, 0.0006175742018967867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006175742018967867

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061757
Iteration 2/1000 | Loss: 0.00004077
Iteration 3/1000 | Loss: 0.00011009
Iteration 4/1000 | Loss: 0.00002607
Iteration 5/1000 | Loss: 0.00002458
Iteration 6/1000 | Loss: 0.00002367
Iteration 7/1000 | Loss: 0.00002298
Iteration 8/1000 | Loss: 0.00002247
Iteration 9/1000 | Loss: 0.00002206
Iteration 10/1000 | Loss: 0.00002172
Iteration 11/1000 | Loss: 0.00002152
Iteration 12/1000 | Loss: 0.00002136
Iteration 13/1000 | Loss: 0.00002130
Iteration 14/1000 | Loss: 0.00002130
Iteration 15/1000 | Loss: 0.00002128
Iteration 16/1000 | Loss: 0.00002128
Iteration 17/1000 | Loss: 0.00002127
Iteration 18/1000 | Loss: 0.00002126
Iteration 19/1000 | Loss: 0.00002126
Iteration 20/1000 | Loss: 0.00002125
Iteration 21/1000 | Loss: 0.00002125
Iteration 22/1000 | Loss: 0.00002124
Iteration 23/1000 | Loss: 0.00002124
Iteration 24/1000 | Loss: 0.00002123
Iteration 25/1000 | Loss: 0.00002123
Iteration 26/1000 | Loss: 0.00002123
Iteration 27/1000 | Loss: 0.00002123
Iteration 28/1000 | Loss: 0.00002122
Iteration 29/1000 | Loss: 0.00002122
Iteration 30/1000 | Loss: 0.00002118
Iteration 31/1000 | Loss: 0.00002118
Iteration 32/1000 | Loss: 0.00002117
Iteration 33/1000 | Loss: 0.00002117
Iteration 34/1000 | Loss: 0.00002116
Iteration 35/1000 | Loss: 0.00002115
Iteration 36/1000 | Loss: 0.00002114
Iteration 37/1000 | Loss: 0.00002114
Iteration 38/1000 | Loss: 0.00002114
Iteration 39/1000 | Loss: 0.00002114
Iteration 40/1000 | Loss: 0.00002114
Iteration 41/1000 | Loss: 0.00002114
Iteration 42/1000 | Loss: 0.00002114
Iteration 43/1000 | Loss: 0.00002114
Iteration 44/1000 | Loss: 0.00002113
Iteration 45/1000 | Loss: 0.00002113
Iteration 46/1000 | Loss: 0.00002113
Iteration 47/1000 | Loss: 0.00002113
Iteration 48/1000 | Loss: 0.00002113
Iteration 49/1000 | Loss: 0.00002113
Iteration 50/1000 | Loss: 0.00002113
Iteration 51/1000 | Loss: 0.00002113
Iteration 52/1000 | Loss: 0.00002113
Iteration 53/1000 | Loss: 0.00002113
Iteration 54/1000 | Loss: 0.00002112
Iteration 55/1000 | Loss: 0.00002112
Iteration 56/1000 | Loss: 0.00002111
Iteration 57/1000 | Loss: 0.00002111
Iteration 58/1000 | Loss: 0.00002111
Iteration 59/1000 | Loss: 0.00002110
Iteration 60/1000 | Loss: 0.00002110
Iteration 61/1000 | Loss: 0.00002110
Iteration 62/1000 | Loss: 0.00002110
Iteration 63/1000 | Loss: 0.00002109
Iteration 64/1000 | Loss: 0.00002109
Iteration 65/1000 | Loss: 0.00002109
Iteration 66/1000 | Loss: 0.00002108
Iteration 67/1000 | Loss: 0.00002108
Iteration 68/1000 | Loss: 0.00002108
Iteration 69/1000 | Loss: 0.00002107
Iteration 70/1000 | Loss: 0.00002107
Iteration 71/1000 | Loss: 0.00002107
Iteration 72/1000 | Loss: 0.00002106
Iteration 73/1000 | Loss: 0.00002106
Iteration 74/1000 | Loss: 0.00002106
Iteration 75/1000 | Loss: 0.00002106
Iteration 76/1000 | Loss: 0.00002105
Iteration 77/1000 | Loss: 0.00002105
Iteration 78/1000 | Loss: 0.00002105
Iteration 79/1000 | Loss: 0.00002105
Iteration 80/1000 | Loss: 0.00002104
Iteration 81/1000 | Loss: 0.00002104
Iteration 82/1000 | Loss: 0.00002104
Iteration 83/1000 | Loss: 0.00002104
Iteration 84/1000 | Loss: 0.00002104
Iteration 85/1000 | Loss: 0.00002103
Iteration 86/1000 | Loss: 0.00002102
Iteration 87/1000 | Loss: 0.00002102
Iteration 88/1000 | Loss: 0.00002101
Iteration 89/1000 | Loss: 0.00002101
Iteration 90/1000 | Loss: 0.00002101
Iteration 91/1000 | Loss: 0.00002101
Iteration 92/1000 | Loss: 0.00002100
Iteration 93/1000 | Loss: 0.00002100
Iteration 94/1000 | Loss: 0.00002100
Iteration 95/1000 | Loss: 0.00002099
Iteration 96/1000 | Loss: 0.00002099
Iteration 97/1000 | Loss: 0.00002099
Iteration 98/1000 | Loss: 0.00002099
Iteration 99/1000 | Loss: 0.00002098
Iteration 100/1000 | Loss: 0.00002098
Iteration 101/1000 | Loss: 0.00002098
Iteration 102/1000 | Loss: 0.00002097
Iteration 103/1000 | Loss: 0.00002097
Iteration 104/1000 | Loss: 0.00002097
Iteration 105/1000 | Loss: 0.00002097
Iteration 106/1000 | Loss: 0.00002097
Iteration 107/1000 | Loss: 0.00002096
Iteration 108/1000 | Loss: 0.00002096
Iteration 109/1000 | Loss: 0.00002096
Iteration 110/1000 | Loss: 0.00002096
Iteration 111/1000 | Loss: 0.00002095
Iteration 112/1000 | Loss: 0.00002095
Iteration 113/1000 | Loss: 0.00002095
Iteration 114/1000 | Loss: 0.00002095
Iteration 115/1000 | Loss: 0.00002095
Iteration 116/1000 | Loss: 0.00002095
Iteration 117/1000 | Loss: 0.00002095
Iteration 118/1000 | Loss: 0.00002095
Iteration 119/1000 | Loss: 0.00002095
Iteration 120/1000 | Loss: 0.00002094
Iteration 121/1000 | Loss: 0.00002094
Iteration 122/1000 | Loss: 0.00002094
Iteration 123/1000 | Loss: 0.00002094
Iteration 124/1000 | Loss: 0.00002094
Iteration 125/1000 | Loss: 0.00002094
Iteration 126/1000 | Loss: 0.00002094
Iteration 127/1000 | Loss: 0.00002093
Iteration 128/1000 | Loss: 0.00002093
Iteration 129/1000 | Loss: 0.00002093
Iteration 130/1000 | Loss: 0.00002093
Iteration 131/1000 | Loss: 0.00002093
Iteration 132/1000 | Loss: 0.00002093
Iteration 133/1000 | Loss: 0.00002093
Iteration 134/1000 | Loss: 0.00002093
Iteration 135/1000 | Loss: 0.00002093
Iteration 136/1000 | Loss: 0.00002093
Iteration 137/1000 | Loss: 0.00002093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [2.0928950107190758e-05, 2.0928950107190758e-05, 2.0928950107190758e-05, 2.0928950107190758e-05, 2.0928950107190758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0928950107190758e-05

Optimization complete. Final v2v error: 3.7813727855682373 mm

Highest mean error: 6.034397125244141 mm for frame 112

Lowest mean error: 3.138760566711426 mm for frame 206

Saving results

Total time: 52.426172733306885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01025383
Iteration 2/25 | Loss: 0.00295349
Iteration 3/25 | Loss: 0.00254428
Iteration 4/25 | Loss: 0.00235401
Iteration 5/25 | Loss: 0.00210688
Iteration 6/25 | Loss: 0.00198986
Iteration 7/25 | Loss: 0.00191500
Iteration 8/25 | Loss: 0.00184709
Iteration 9/25 | Loss: 0.00178056
Iteration 10/25 | Loss: 0.00167592
Iteration 11/25 | Loss: 0.00162941
Iteration 12/25 | Loss: 0.00154510
Iteration 13/25 | Loss: 0.00151196
Iteration 14/25 | Loss: 0.00148733
Iteration 15/25 | Loss: 0.00146925
Iteration 16/25 | Loss: 0.00143674
Iteration 17/25 | Loss: 0.00142788
Iteration 18/25 | Loss: 0.00141838
Iteration 19/25 | Loss: 0.00140948
Iteration 20/25 | Loss: 0.00140803
Iteration 21/25 | Loss: 0.00140565
Iteration 22/25 | Loss: 0.00140343
Iteration 23/25 | Loss: 0.00140408
Iteration 24/25 | Loss: 0.00140303
Iteration 25/25 | Loss: 0.00139663

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32700312
Iteration 2/25 | Loss: 0.00458067
Iteration 3/25 | Loss: 0.00458067
Iteration 4/25 | Loss: 0.00458067
Iteration 5/25 | Loss: 0.00458066
Iteration 6/25 | Loss: 0.00458066
Iteration 7/25 | Loss: 0.00458066
Iteration 8/25 | Loss: 0.00458066
Iteration 9/25 | Loss: 0.00458066
Iteration 10/25 | Loss: 0.00458066
Iteration 11/25 | Loss: 0.00458066
Iteration 12/25 | Loss: 0.00458066
Iteration 13/25 | Loss: 0.00458066
Iteration 14/25 | Loss: 0.00458066
Iteration 15/25 | Loss: 0.00458066
Iteration 16/25 | Loss: 0.00458066
Iteration 17/25 | Loss: 0.00458066
Iteration 18/25 | Loss: 0.00458066
Iteration 19/25 | Loss: 0.00458066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004580662585794926, 0.004580662585794926, 0.004580662585794926, 0.004580662585794926, 0.004580662585794926]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004580662585794926

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00458066
Iteration 2/1000 | Loss: 0.00497952
Iteration 3/1000 | Loss: 0.00039061
Iteration 4/1000 | Loss: 0.00024569
Iteration 5/1000 | Loss: 0.00015927
Iteration 6/1000 | Loss: 0.00010016
Iteration 7/1000 | Loss: 0.00046171
Iteration 8/1000 | Loss: 0.00009080
Iteration 9/1000 | Loss: 0.00181413
Iteration 10/1000 | Loss: 0.00058565
Iteration 11/1000 | Loss: 0.00021417
Iteration 12/1000 | Loss: 0.00008878
Iteration 13/1000 | Loss: 0.00006635
Iteration 14/1000 | Loss: 0.00005128
Iteration 15/1000 | Loss: 0.00004329
Iteration 16/1000 | Loss: 0.00003791
Iteration 17/1000 | Loss: 0.00040718
Iteration 18/1000 | Loss: 0.00004829
Iteration 19/1000 | Loss: 0.00003664
Iteration 20/1000 | Loss: 0.00003168
Iteration 21/1000 | Loss: 0.00038343
Iteration 22/1000 | Loss: 0.00006293
Iteration 23/1000 | Loss: 0.00004047
Iteration 24/1000 | Loss: 0.00003258
Iteration 25/1000 | Loss: 0.00002906
Iteration 26/1000 | Loss: 0.00002710
Iteration 27/1000 | Loss: 0.00002623
Iteration 28/1000 | Loss: 0.00002564
Iteration 29/1000 | Loss: 0.00002507
Iteration 30/1000 | Loss: 0.00002454
Iteration 31/1000 | Loss: 0.00002392
Iteration 32/1000 | Loss: 0.00002364
Iteration 33/1000 | Loss: 0.00003413
Iteration 34/1000 | Loss: 0.00002490
Iteration 35/1000 | Loss: 0.00002363
Iteration 36/1000 | Loss: 0.00002313
Iteration 37/1000 | Loss: 0.00002257
Iteration 38/1000 | Loss: 0.00002228
Iteration 39/1000 | Loss: 0.00002195
Iteration 40/1000 | Loss: 0.00002166
Iteration 41/1000 | Loss: 0.00002136
Iteration 42/1000 | Loss: 0.00002118
Iteration 43/1000 | Loss: 0.00002103
Iteration 44/1000 | Loss: 0.00002103
Iteration 45/1000 | Loss: 0.00002098
Iteration 46/1000 | Loss: 0.00002098
Iteration 47/1000 | Loss: 0.00002097
Iteration 48/1000 | Loss: 0.00002095
Iteration 49/1000 | Loss: 0.00002095
Iteration 50/1000 | Loss: 0.00002093
Iteration 51/1000 | Loss: 0.00002093
Iteration 52/1000 | Loss: 0.00002093
Iteration 53/1000 | Loss: 0.00002093
Iteration 54/1000 | Loss: 0.00002092
Iteration 55/1000 | Loss: 0.00002091
Iteration 56/1000 | Loss: 0.00002091
Iteration 57/1000 | Loss: 0.00002091
Iteration 58/1000 | Loss: 0.00002090
Iteration 59/1000 | Loss: 0.00002090
Iteration 60/1000 | Loss: 0.00002090
Iteration 61/1000 | Loss: 0.00002089
Iteration 62/1000 | Loss: 0.00002089
Iteration 63/1000 | Loss: 0.00002088
Iteration 64/1000 | Loss: 0.00002088
Iteration 65/1000 | Loss: 0.00002088
Iteration 66/1000 | Loss: 0.00002087
Iteration 67/1000 | Loss: 0.00002087
Iteration 68/1000 | Loss: 0.00002086
Iteration 69/1000 | Loss: 0.00002085
Iteration 70/1000 | Loss: 0.00002085
Iteration 71/1000 | Loss: 0.00002085
Iteration 72/1000 | Loss: 0.00002085
Iteration 73/1000 | Loss: 0.00002085
Iteration 74/1000 | Loss: 0.00002085
Iteration 75/1000 | Loss: 0.00002085
Iteration 76/1000 | Loss: 0.00002084
Iteration 77/1000 | Loss: 0.00002084
Iteration 78/1000 | Loss: 0.00002084
Iteration 79/1000 | Loss: 0.00002083
Iteration 80/1000 | Loss: 0.00002083
Iteration 81/1000 | Loss: 0.00002083
Iteration 82/1000 | Loss: 0.00002082
Iteration 83/1000 | Loss: 0.00002081
Iteration 84/1000 | Loss: 0.00002081
Iteration 85/1000 | Loss: 0.00002081
Iteration 86/1000 | Loss: 0.00002080
Iteration 87/1000 | Loss: 0.00002080
Iteration 88/1000 | Loss: 0.00002080
Iteration 89/1000 | Loss: 0.00002079
Iteration 90/1000 | Loss: 0.00002079
Iteration 91/1000 | Loss: 0.00002078
Iteration 92/1000 | Loss: 0.00002078
Iteration 93/1000 | Loss: 0.00002078
Iteration 94/1000 | Loss: 0.00002078
Iteration 95/1000 | Loss: 0.00002077
Iteration 96/1000 | Loss: 0.00002077
Iteration 97/1000 | Loss: 0.00002077
Iteration 98/1000 | Loss: 0.00002077
Iteration 99/1000 | Loss: 0.00002076
Iteration 100/1000 | Loss: 0.00002076
Iteration 101/1000 | Loss: 0.00002076
Iteration 102/1000 | Loss: 0.00002076
Iteration 103/1000 | Loss: 0.00002076
Iteration 104/1000 | Loss: 0.00002076
Iteration 105/1000 | Loss: 0.00002076
Iteration 106/1000 | Loss: 0.00002075
Iteration 107/1000 | Loss: 0.00002075
Iteration 108/1000 | Loss: 0.00002075
Iteration 109/1000 | Loss: 0.00002075
Iteration 110/1000 | Loss: 0.00002075
Iteration 111/1000 | Loss: 0.00002075
Iteration 112/1000 | Loss: 0.00002075
Iteration 113/1000 | Loss: 0.00002075
Iteration 114/1000 | Loss: 0.00002075
Iteration 115/1000 | Loss: 0.00002075
Iteration 116/1000 | Loss: 0.00002075
Iteration 117/1000 | Loss: 0.00002075
Iteration 118/1000 | Loss: 0.00002075
Iteration 119/1000 | Loss: 0.00002075
Iteration 120/1000 | Loss: 0.00002075
Iteration 121/1000 | Loss: 0.00002075
Iteration 122/1000 | Loss: 0.00002075
Iteration 123/1000 | Loss: 0.00002075
Iteration 124/1000 | Loss: 0.00002075
Iteration 125/1000 | Loss: 0.00002075
Iteration 126/1000 | Loss: 0.00002075
Iteration 127/1000 | Loss: 0.00002075
Iteration 128/1000 | Loss: 0.00002075
Iteration 129/1000 | Loss: 0.00002075
Iteration 130/1000 | Loss: 0.00002075
Iteration 131/1000 | Loss: 0.00002075
Iteration 132/1000 | Loss: 0.00002075
Iteration 133/1000 | Loss: 0.00002075
Iteration 134/1000 | Loss: 0.00002075
Iteration 135/1000 | Loss: 0.00002075
Iteration 136/1000 | Loss: 0.00002075
Iteration 137/1000 | Loss: 0.00002075
Iteration 138/1000 | Loss: 0.00002075
Iteration 139/1000 | Loss: 0.00002075
Iteration 140/1000 | Loss: 0.00002075
Iteration 141/1000 | Loss: 0.00002075
Iteration 142/1000 | Loss: 0.00002075
Iteration 143/1000 | Loss: 0.00002075
Iteration 144/1000 | Loss: 0.00002075
Iteration 145/1000 | Loss: 0.00002075
Iteration 146/1000 | Loss: 0.00002075
Iteration 147/1000 | Loss: 0.00002075
Iteration 148/1000 | Loss: 0.00002075
Iteration 149/1000 | Loss: 0.00002075
Iteration 150/1000 | Loss: 0.00002075
Iteration 151/1000 | Loss: 0.00002075
Iteration 152/1000 | Loss: 0.00002075
Iteration 153/1000 | Loss: 0.00002075
Iteration 154/1000 | Loss: 0.00002075
Iteration 155/1000 | Loss: 0.00002075
Iteration 156/1000 | Loss: 0.00002075
Iteration 157/1000 | Loss: 0.00002075
Iteration 158/1000 | Loss: 0.00002075
Iteration 159/1000 | Loss: 0.00002075
Iteration 160/1000 | Loss: 0.00002075
Iteration 161/1000 | Loss: 0.00002075
Iteration 162/1000 | Loss: 0.00002075
Iteration 163/1000 | Loss: 0.00002075
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.0753701392095536e-05, 2.0753701392095536e-05, 2.0753701392095536e-05, 2.0753701392095536e-05, 2.0753701392095536e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0753701392095536e-05

Optimization complete. Final v2v error: 3.2872321605682373 mm

Highest mean error: 10.361233711242676 mm for frame 127

Lowest mean error: 2.813389301300049 mm for frame 52

Saving results

Total time: 114.75536370277405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431437
Iteration 2/25 | Loss: 0.00138790
Iteration 3/25 | Loss: 0.00113341
Iteration 4/25 | Loss: 0.00111171
Iteration 5/25 | Loss: 0.00110942
Iteration 6/25 | Loss: 0.00110876
Iteration 7/25 | Loss: 0.00110876
Iteration 8/25 | Loss: 0.00110876
Iteration 9/25 | Loss: 0.00110876
Iteration 10/25 | Loss: 0.00110876
Iteration 11/25 | Loss: 0.00110876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011087569873780012, 0.0011087569873780012, 0.0011087569873780012, 0.0011087569873780012, 0.0011087569873780012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011087569873780012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.29653120
Iteration 2/25 | Loss: 0.00066533
Iteration 3/25 | Loss: 0.00066529
Iteration 4/25 | Loss: 0.00066529
Iteration 5/25 | Loss: 0.00066529
Iteration 6/25 | Loss: 0.00066529
Iteration 7/25 | Loss: 0.00066529
Iteration 8/25 | Loss: 0.00066529
Iteration 9/25 | Loss: 0.00066529
Iteration 10/25 | Loss: 0.00066529
Iteration 11/25 | Loss: 0.00066529
Iteration 12/25 | Loss: 0.00066529
Iteration 13/25 | Loss: 0.00066529
Iteration 14/25 | Loss: 0.00066529
Iteration 15/25 | Loss: 0.00066529
Iteration 16/25 | Loss: 0.00066529
Iteration 17/25 | Loss: 0.00066529
Iteration 18/25 | Loss: 0.00066529
Iteration 19/25 | Loss: 0.00066529
Iteration 20/25 | Loss: 0.00066529
Iteration 21/25 | Loss: 0.00066529
Iteration 22/25 | Loss: 0.00066529
Iteration 23/25 | Loss: 0.00066529
Iteration 24/25 | Loss: 0.00066529
Iteration 25/25 | Loss: 0.00066529

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066529
Iteration 2/1000 | Loss: 0.00002696
Iteration 3/1000 | Loss: 0.00001781
Iteration 4/1000 | Loss: 0.00001593
Iteration 5/1000 | Loss: 0.00001476
Iteration 6/1000 | Loss: 0.00001413
Iteration 7/1000 | Loss: 0.00001364
Iteration 8/1000 | Loss: 0.00001327
Iteration 9/1000 | Loss: 0.00001322
Iteration 10/1000 | Loss: 0.00001299
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00001275
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001264
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001262
Iteration 17/1000 | Loss: 0.00001261
Iteration 18/1000 | Loss: 0.00001261
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001256
Iteration 21/1000 | Loss: 0.00001256
Iteration 22/1000 | Loss: 0.00001250
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001246
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001244
Iteration 32/1000 | Loss: 0.00001244
Iteration 33/1000 | Loss: 0.00001244
Iteration 34/1000 | Loss: 0.00001243
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001243
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001238
Iteration 46/1000 | Loss: 0.00001238
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001238
Iteration 54/1000 | Loss: 0.00001238
Iteration 55/1000 | Loss: 0.00001237
Iteration 56/1000 | Loss: 0.00001237
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001236
Iteration 59/1000 | Loss: 0.00001236
Iteration 60/1000 | Loss: 0.00001236
Iteration 61/1000 | Loss: 0.00001235
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001234
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001234
Iteration 67/1000 | Loss: 0.00001234
Iteration 68/1000 | Loss: 0.00001233
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001233
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001230
Iteration 82/1000 | Loss: 0.00001230
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001229
Iteration 87/1000 | Loss: 0.00001229
Iteration 88/1000 | Loss: 0.00001229
Iteration 89/1000 | Loss: 0.00001228
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001227
Iteration 98/1000 | Loss: 0.00001227
Iteration 99/1000 | Loss: 0.00001227
Iteration 100/1000 | Loss: 0.00001227
Iteration 101/1000 | Loss: 0.00001226
Iteration 102/1000 | Loss: 0.00001226
Iteration 103/1000 | Loss: 0.00001226
Iteration 104/1000 | Loss: 0.00001226
Iteration 105/1000 | Loss: 0.00001225
Iteration 106/1000 | Loss: 0.00001225
Iteration 107/1000 | Loss: 0.00001225
Iteration 108/1000 | Loss: 0.00001225
Iteration 109/1000 | Loss: 0.00001225
Iteration 110/1000 | Loss: 0.00001225
Iteration 111/1000 | Loss: 0.00001225
Iteration 112/1000 | Loss: 0.00001225
Iteration 113/1000 | Loss: 0.00001224
Iteration 114/1000 | Loss: 0.00001224
Iteration 115/1000 | Loss: 0.00001224
Iteration 116/1000 | Loss: 0.00001224
Iteration 117/1000 | Loss: 0.00001224
Iteration 118/1000 | Loss: 0.00001224
Iteration 119/1000 | Loss: 0.00001224
Iteration 120/1000 | Loss: 0.00001223
Iteration 121/1000 | Loss: 0.00001223
Iteration 122/1000 | Loss: 0.00001223
Iteration 123/1000 | Loss: 0.00001223
Iteration 124/1000 | Loss: 0.00001223
Iteration 125/1000 | Loss: 0.00001222
Iteration 126/1000 | Loss: 0.00001222
Iteration 127/1000 | Loss: 0.00001222
Iteration 128/1000 | Loss: 0.00001222
Iteration 129/1000 | Loss: 0.00001222
Iteration 130/1000 | Loss: 0.00001222
Iteration 131/1000 | Loss: 0.00001222
Iteration 132/1000 | Loss: 0.00001221
Iteration 133/1000 | Loss: 0.00001221
Iteration 134/1000 | Loss: 0.00001221
Iteration 135/1000 | Loss: 0.00001221
Iteration 136/1000 | Loss: 0.00001221
Iteration 137/1000 | Loss: 0.00001220
Iteration 138/1000 | Loss: 0.00001220
Iteration 139/1000 | Loss: 0.00001220
Iteration 140/1000 | Loss: 0.00001220
Iteration 141/1000 | Loss: 0.00001220
Iteration 142/1000 | Loss: 0.00001219
Iteration 143/1000 | Loss: 0.00001219
Iteration 144/1000 | Loss: 0.00001219
Iteration 145/1000 | Loss: 0.00001219
Iteration 146/1000 | Loss: 0.00001219
Iteration 147/1000 | Loss: 0.00001219
Iteration 148/1000 | Loss: 0.00001219
Iteration 149/1000 | Loss: 0.00001219
Iteration 150/1000 | Loss: 0.00001219
Iteration 151/1000 | Loss: 0.00001219
Iteration 152/1000 | Loss: 0.00001219
Iteration 153/1000 | Loss: 0.00001219
Iteration 154/1000 | Loss: 0.00001219
Iteration 155/1000 | Loss: 0.00001219
Iteration 156/1000 | Loss: 0.00001219
Iteration 157/1000 | Loss: 0.00001219
Iteration 158/1000 | Loss: 0.00001219
Iteration 159/1000 | Loss: 0.00001219
Iteration 160/1000 | Loss: 0.00001219
Iteration 161/1000 | Loss: 0.00001219
Iteration 162/1000 | Loss: 0.00001219
Iteration 163/1000 | Loss: 0.00001219
Iteration 164/1000 | Loss: 0.00001219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.2193240763735957e-05, 1.2193240763735957e-05, 1.2193240763735957e-05, 1.2193240763735957e-05, 1.2193240763735957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2193240763735957e-05

Optimization complete. Final v2v error: 2.947772741317749 mm

Highest mean error: 3.669565200805664 mm for frame 71

Lowest mean error: 2.545743465423584 mm for frame 101

Saving results

Total time: 37.60473823547363
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00273306
Iteration 2/25 | Loss: 0.00130676
Iteration 3/25 | Loss: 0.00112848
Iteration 4/25 | Loss: 0.00108596
Iteration 5/25 | Loss: 0.00107730
Iteration 6/25 | Loss: 0.00107546
Iteration 7/25 | Loss: 0.00107485
Iteration 8/25 | Loss: 0.00107485
Iteration 9/25 | Loss: 0.00107485
Iteration 10/25 | Loss: 0.00107485
Iteration 11/25 | Loss: 0.00107485
Iteration 12/25 | Loss: 0.00107485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010748450877144933, 0.0010748450877144933, 0.0010748450877144933, 0.0010748450877144933, 0.0010748450877144933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010748450877144933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36103594
Iteration 2/25 | Loss: 0.00088031
Iteration 3/25 | Loss: 0.00088030
Iteration 4/25 | Loss: 0.00088030
Iteration 5/25 | Loss: 0.00088030
Iteration 6/25 | Loss: 0.00088030
Iteration 7/25 | Loss: 0.00088030
Iteration 8/25 | Loss: 0.00088030
Iteration 9/25 | Loss: 0.00088030
Iteration 10/25 | Loss: 0.00088030
Iteration 11/25 | Loss: 0.00088030
Iteration 12/25 | Loss: 0.00088030
Iteration 13/25 | Loss: 0.00088030
Iteration 14/25 | Loss: 0.00088030
Iteration 15/25 | Loss: 0.00088030
Iteration 16/25 | Loss: 0.00088030
Iteration 17/25 | Loss: 0.00088030
Iteration 18/25 | Loss: 0.00088030
Iteration 19/25 | Loss: 0.00088030
Iteration 20/25 | Loss: 0.00088030
Iteration 21/25 | Loss: 0.00088030
Iteration 22/25 | Loss: 0.00088030
Iteration 23/25 | Loss: 0.00088030
Iteration 24/25 | Loss: 0.00088030
Iteration 25/25 | Loss: 0.00088030

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088030
Iteration 2/1000 | Loss: 0.00004671
Iteration 3/1000 | Loss: 0.00003014
Iteration 4/1000 | Loss: 0.00001993
Iteration 5/1000 | Loss: 0.00001814
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001671
Iteration 8/1000 | Loss: 0.00001629
Iteration 9/1000 | Loss: 0.00001598
Iteration 10/1000 | Loss: 0.00001556
Iteration 11/1000 | Loss: 0.00001524
Iteration 12/1000 | Loss: 0.00001505
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001499
Iteration 15/1000 | Loss: 0.00001481
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001473
Iteration 19/1000 | Loss: 0.00001473
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001472
Iteration 22/1000 | Loss: 0.00001471
Iteration 23/1000 | Loss: 0.00001471
Iteration 24/1000 | Loss: 0.00001471
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001470
Iteration 33/1000 | Loss: 0.00001470
Iteration 34/1000 | Loss: 0.00001469
Iteration 35/1000 | Loss: 0.00001468
Iteration 36/1000 | Loss: 0.00001468
Iteration 37/1000 | Loss: 0.00001465
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001460
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001457
Iteration 51/1000 | Loss: 0.00001457
Iteration 52/1000 | Loss: 0.00001457
Iteration 53/1000 | Loss: 0.00001456
Iteration 54/1000 | Loss: 0.00001456
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001454
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001453
Iteration 65/1000 | Loss: 0.00001453
Iteration 66/1000 | Loss: 0.00001453
Iteration 67/1000 | Loss: 0.00001453
Iteration 68/1000 | Loss: 0.00001453
Iteration 69/1000 | Loss: 0.00001452
Iteration 70/1000 | Loss: 0.00001452
Iteration 71/1000 | Loss: 0.00001452
Iteration 72/1000 | Loss: 0.00001452
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001452
Iteration 76/1000 | Loss: 0.00001452
Iteration 77/1000 | Loss: 0.00001452
Iteration 78/1000 | Loss: 0.00001451
Iteration 79/1000 | Loss: 0.00001451
Iteration 80/1000 | Loss: 0.00001451
Iteration 81/1000 | Loss: 0.00001451
Iteration 82/1000 | Loss: 0.00001451
Iteration 83/1000 | Loss: 0.00001451
Iteration 84/1000 | Loss: 0.00001451
Iteration 85/1000 | Loss: 0.00001450
Iteration 86/1000 | Loss: 0.00001450
Iteration 87/1000 | Loss: 0.00001450
Iteration 88/1000 | Loss: 0.00001449
Iteration 89/1000 | Loss: 0.00001449
Iteration 90/1000 | Loss: 0.00001449
Iteration 91/1000 | Loss: 0.00001448
Iteration 92/1000 | Loss: 0.00001448
Iteration 93/1000 | Loss: 0.00001448
Iteration 94/1000 | Loss: 0.00001448
Iteration 95/1000 | Loss: 0.00001447
Iteration 96/1000 | Loss: 0.00001447
Iteration 97/1000 | Loss: 0.00001447
Iteration 98/1000 | Loss: 0.00001447
Iteration 99/1000 | Loss: 0.00001446
Iteration 100/1000 | Loss: 0.00001446
Iteration 101/1000 | Loss: 0.00001446
Iteration 102/1000 | Loss: 0.00001446
Iteration 103/1000 | Loss: 0.00001445
Iteration 104/1000 | Loss: 0.00001445
Iteration 105/1000 | Loss: 0.00001445
Iteration 106/1000 | Loss: 0.00001445
Iteration 107/1000 | Loss: 0.00001445
Iteration 108/1000 | Loss: 0.00001445
Iteration 109/1000 | Loss: 0.00001445
Iteration 110/1000 | Loss: 0.00001445
Iteration 111/1000 | Loss: 0.00001444
Iteration 112/1000 | Loss: 0.00001444
Iteration 113/1000 | Loss: 0.00001444
Iteration 114/1000 | Loss: 0.00001444
Iteration 115/1000 | Loss: 0.00001443
Iteration 116/1000 | Loss: 0.00001443
Iteration 117/1000 | Loss: 0.00001443
Iteration 118/1000 | Loss: 0.00001443
Iteration 119/1000 | Loss: 0.00001442
Iteration 120/1000 | Loss: 0.00001442
Iteration 121/1000 | Loss: 0.00001442
Iteration 122/1000 | Loss: 0.00001442
Iteration 123/1000 | Loss: 0.00001442
Iteration 124/1000 | Loss: 0.00001442
Iteration 125/1000 | Loss: 0.00001442
Iteration 126/1000 | Loss: 0.00001442
Iteration 127/1000 | Loss: 0.00001442
Iteration 128/1000 | Loss: 0.00001442
Iteration 129/1000 | Loss: 0.00001442
Iteration 130/1000 | Loss: 0.00001441
Iteration 131/1000 | Loss: 0.00001441
Iteration 132/1000 | Loss: 0.00001441
Iteration 133/1000 | Loss: 0.00001441
Iteration 134/1000 | Loss: 0.00001441
Iteration 135/1000 | Loss: 0.00001441
Iteration 136/1000 | Loss: 0.00001440
Iteration 137/1000 | Loss: 0.00001440
Iteration 138/1000 | Loss: 0.00001440
Iteration 139/1000 | Loss: 0.00001440
Iteration 140/1000 | Loss: 0.00001440
Iteration 141/1000 | Loss: 0.00001440
Iteration 142/1000 | Loss: 0.00001440
Iteration 143/1000 | Loss: 0.00001440
Iteration 144/1000 | Loss: 0.00001440
Iteration 145/1000 | Loss: 0.00001440
Iteration 146/1000 | Loss: 0.00001440
Iteration 147/1000 | Loss: 0.00001440
Iteration 148/1000 | Loss: 0.00001440
Iteration 149/1000 | Loss: 0.00001440
Iteration 150/1000 | Loss: 0.00001440
Iteration 151/1000 | Loss: 0.00001440
Iteration 152/1000 | Loss: 0.00001439
Iteration 153/1000 | Loss: 0.00001439
Iteration 154/1000 | Loss: 0.00001439
Iteration 155/1000 | Loss: 0.00001439
Iteration 156/1000 | Loss: 0.00001439
Iteration 157/1000 | Loss: 0.00001439
Iteration 158/1000 | Loss: 0.00001439
Iteration 159/1000 | Loss: 0.00001439
Iteration 160/1000 | Loss: 0.00001439
Iteration 161/1000 | Loss: 0.00001439
Iteration 162/1000 | Loss: 0.00001439
Iteration 163/1000 | Loss: 0.00001439
Iteration 164/1000 | Loss: 0.00001439
Iteration 165/1000 | Loss: 0.00001439
Iteration 166/1000 | Loss: 0.00001439
Iteration 167/1000 | Loss: 0.00001438
Iteration 168/1000 | Loss: 0.00001438
Iteration 169/1000 | Loss: 0.00001438
Iteration 170/1000 | Loss: 0.00001438
Iteration 171/1000 | Loss: 0.00001438
Iteration 172/1000 | Loss: 0.00001438
Iteration 173/1000 | Loss: 0.00001438
Iteration 174/1000 | Loss: 0.00001438
Iteration 175/1000 | Loss: 0.00001438
Iteration 176/1000 | Loss: 0.00001438
Iteration 177/1000 | Loss: 0.00001438
Iteration 178/1000 | Loss: 0.00001438
Iteration 179/1000 | Loss: 0.00001438
Iteration 180/1000 | Loss: 0.00001438
Iteration 181/1000 | Loss: 0.00001438
Iteration 182/1000 | Loss: 0.00001438
Iteration 183/1000 | Loss: 0.00001438
Iteration 184/1000 | Loss: 0.00001438
Iteration 185/1000 | Loss: 0.00001438
Iteration 186/1000 | Loss: 0.00001438
Iteration 187/1000 | Loss: 0.00001438
Iteration 188/1000 | Loss: 0.00001438
Iteration 189/1000 | Loss: 0.00001438
Iteration 190/1000 | Loss: 0.00001438
Iteration 191/1000 | Loss: 0.00001438
Iteration 192/1000 | Loss: 0.00001438
Iteration 193/1000 | Loss: 0.00001438
Iteration 194/1000 | Loss: 0.00001438
Iteration 195/1000 | Loss: 0.00001438
Iteration 196/1000 | Loss: 0.00001438
Iteration 197/1000 | Loss: 0.00001438
Iteration 198/1000 | Loss: 0.00001438
Iteration 199/1000 | Loss: 0.00001438
Iteration 200/1000 | Loss: 0.00001438
Iteration 201/1000 | Loss: 0.00001438
Iteration 202/1000 | Loss: 0.00001438
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.4383108464244287e-05, 1.4383108464244287e-05, 1.4383108464244287e-05, 1.4383108464244287e-05, 1.4383108464244287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4383108464244287e-05

Optimization complete. Final v2v error: 3.165299892425537 mm

Highest mean error: 3.6306822299957275 mm for frame 88

Lowest mean error: 2.885784864425659 mm for frame 0

Saving results

Total time: 41.69861817359924
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019409
Iteration 2/25 | Loss: 0.00172895
Iteration 3/25 | Loss: 0.00151177
Iteration 4/25 | Loss: 0.00132566
Iteration 5/25 | Loss: 0.00132246
Iteration 6/25 | Loss: 0.00125617
Iteration 7/25 | Loss: 0.00117633
Iteration 8/25 | Loss: 0.00115441
Iteration 9/25 | Loss: 0.00113515
Iteration 10/25 | Loss: 0.00113760
Iteration 11/25 | Loss: 0.00113750
Iteration 12/25 | Loss: 0.00112431
Iteration 13/25 | Loss: 0.00111533
Iteration 14/25 | Loss: 0.00111315
Iteration 15/25 | Loss: 0.00110792
Iteration 16/25 | Loss: 0.00110502
Iteration 17/25 | Loss: 0.00110095
Iteration 18/25 | Loss: 0.00109972
Iteration 19/25 | Loss: 0.00109908
Iteration 20/25 | Loss: 0.00109971
Iteration 21/25 | Loss: 0.00110299
Iteration 22/25 | Loss: 0.00110124
Iteration 23/25 | Loss: 0.00109857
Iteration 24/25 | Loss: 0.00109827
Iteration 25/25 | Loss: 0.00109815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43508220
Iteration 2/25 | Loss: 0.00081298
Iteration 3/25 | Loss: 0.00081298
Iteration 4/25 | Loss: 0.00081298
Iteration 5/25 | Loss: 0.00081297
Iteration 6/25 | Loss: 0.00081297
Iteration 7/25 | Loss: 0.00081297
Iteration 8/25 | Loss: 0.00081297
Iteration 9/25 | Loss: 0.00081297
Iteration 10/25 | Loss: 0.00081297
Iteration 11/25 | Loss: 0.00081297
Iteration 12/25 | Loss: 0.00081297
Iteration 13/25 | Loss: 0.00081297
Iteration 14/25 | Loss: 0.00081297
Iteration 15/25 | Loss: 0.00081297
Iteration 16/25 | Loss: 0.00081297
Iteration 17/25 | Loss: 0.00081297
Iteration 18/25 | Loss: 0.00081297
Iteration 19/25 | Loss: 0.00081297
Iteration 20/25 | Loss: 0.00081297
Iteration 21/25 | Loss: 0.00081297
Iteration 22/25 | Loss: 0.00081297
Iteration 23/25 | Loss: 0.00081297
Iteration 24/25 | Loss: 0.00081297
Iteration 25/25 | Loss: 0.00081297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081297
Iteration 2/1000 | Loss: 0.00002850
Iteration 3/1000 | Loss: 0.00001788
Iteration 4/1000 | Loss: 0.00001578
Iteration 5/1000 | Loss: 0.00001449
Iteration 6/1000 | Loss: 0.00016926
Iteration 7/1000 | Loss: 0.00001400
Iteration 8/1000 | Loss: 0.00001344
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001295
Iteration 11/1000 | Loss: 0.00001276
Iteration 12/1000 | Loss: 0.00001264
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001256
Iteration 15/1000 | Loss: 0.00001250
Iteration 16/1000 | Loss: 0.00001241
Iteration 17/1000 | Loss: 0.00001239
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001239
Iteration 20/1000 | Loss: 0.00001238
Iteration 21/1000 | Loss: 0.00001238
Iteration 22/1000 | Loss: 0.00001238
Iteration 23/1000 | Loss: 0.00001238
Iteration 24/1000 | Loss: 0.00001238
Iteration 25/1000 | Loss: 0.00001237
Iteration 26/1000 | Loss: 0.00001237
Iteration 27/1000 | Loss: 0.00001236
Iteration 28/1000 | Loss: 0.00001236
Iteration 29/1000 | Loss: 0.00001236
Iteration 30/1000 | Loss: 0.00001235
Iteration 31/1000 | Loss: 0.00001234
Iteration 32/1000 | Loss: 0.00001234
Iteration 33/1000 | Loss: 0.00001234
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001233
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001232
Iteration 38/1000 | Loss: 0.00001232
Iteration 39/1000 | Loss: 0.00001232
Iteration 40/1000 | Loss: 0.00001232
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001231
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001230
Iteration 45/1000 | Loss: 0.00001230
Iteration 46/1000 | Loss: 0.00001229
Iteration 47/1000 | Loss: 0.00001229
Iteration 48/1000 | Loss: 0.00001229
Iteration 49/1000 | Loss: 0.00001229
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001226
Iteration 56/1000 | Loss: 0.00001226
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001226
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001225
Iteration 63/1000 | Loss: 0.00001225
Iteration 64/1000 | Loss: 0.00001225
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001224
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001223
Iteration 69/1000 | Loss: 0.00001223
Iteration 70/1000 | Loss: 0.00001223
Iteration 71/1000 | Loss: 0.00001222
Iteration 72/1000 | Loss: 0.00001222
Iteration 73/1000 | Loss: 0.00001222
Iteration 74/1000 | Loss: 0.00001222
Iteration 75/1000 | Loss: 0.00001222
Iteration 76/1000 | Loss: 0.00001222
Iteration 77/1000 | Loss: 0.00001222
Iteration 78/1000 | Loss: 0.00001222
Iteration 79/1000 | Loss: 0.00001222
Iteration 80/1000 | Loss: 0.00001221
Iteration 81/1000 | Loss: 0.00001220
Iteration 82/1000 | Loss: 0.00001220
Iteration 83/1000 | Loss: 0.00001220
Iteration 84/1000 | Loss: 0.00001219
Iteration 85/1000 | Loss: 0.00001219
Iteration 86/1000 | Loss: 0.00001219
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001217
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001216
Iteration 101/1000 | Loss: 0.00001216
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001215
Iteration 107/1000 | Loss: 0.00001215
Iteration 108/1000 | Loss: 0.00001214
Iteration 109/1000 | Loss: 0.00001214
Iteration 110/1000 | Loss: 0.00001213
Iteration 111/1000 | Loss: 0.00001213
Iteration 112/1000 | Loss: 0.00001213
Iteration 113/1000 | Loss: 0.00001213
Iteration 114/1000 | Loss: 0.00001212
Iteration 115/1000 | Loss: 0.00001212
Iteration 116/1000 | Loss: 0.00001210
Iteration 117/1000 | Loss: 0.00001210
Iteration 118/1000 | Loss: 0.00001210
Iteration 119/1000 | Loss: 0.00001209
Iteration 120/1000 | Loss: 0.00001209
Iteration 121/1000 | Loss: 0.00001209
Iteration 122/1000 | Loss: 0.00001209
Iteration 123/1000 | Loss: 0.00001209
Iteration 124/1000 | Loss: 0.00001208
Iteration 125/1000 | Loss: 0.00001208
Iteration 126/1000 | Loss: 0.00001208
Iteration 127/1000 | Loss: 0.00001207
Iteration 128/1000 | Loss: 0.00001207
Iteration 129/1000 | Loss: 0.00001207
Iteration 130/1000 | Loss: 0.00001206
Iteration 131/1000 | Loss: 0.00001206
Iteration 132/1000 | Loss: 0.00001206
Iteration 133/1000 | Loss: 0.00001206
Iteration 134/1000 | Loss: 0.00001205
Iteration 135/1000 | Loss: 0.00001205
Iteration 136/1000 | Loss: 0.00001205
Iteration 137/1000 | Loss: 0.00001205
Iteration 138/1000 | Loss: 0.00001204
Iteration 139/1000 | Loss: 0.00001204
Iteration 140/1000 | Loss: 0.00001203
Iteration 141/1000 | Loss: 0.00001203
Iteration 142/1000 | Loss: 0.00001203
Iteration 143/1000 | Loss: 0.00001202
Iteration 144/1000 | Loss: 0.00001202
Iteration 145/1000 | Loss: 0.00001202
Iteration 146/1000 | Loss: 0.00001202
Iteration 147/1000 | Loss: 0.00001201
Iteration 148/1000 | Loss: 0.00001201
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001200
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001199
Iteration 153/1000 | Loss: 0.00001199
Iteration 154/1000 | Loss: 0.00001199
Iteration 155/1000 | Loss: 0.00001199
Iteration 156/1000 | Loss: 0.00001199
Iteration 157/1000 | Loss: 0.00001198
Iteration 158/1000 | Loss: 0.00001198
Iteration 159/1000 | Loss: 0.00001198
Iteration 160/1000 | Loss: 0.00001198
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001196
Iteration 169/1000 | Loss: 0.00001196
Iteration 170/1000 | Loss: 0.00001196
Iteration 171/1000 | Loss: 0.00001196
Iteration 172/1000 | Loss: 0.00001195
Iteration 173/1000 | Loss: 0.00001195
Iteration 174/1000 | Loss: 0.00001195
Iteration 175/1000 | Loss: 0.00001195
Iteration 176/1000 | Loss: 0.00001195
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001195
Iteration 179/1000 | Loss: 0.00001194
Iteration 180/1000 | Loss: 0.00001194
Iteration 181/1000 | Loss: 0.00001194
Iteration 182/1000 | Loss: 0.00001194
Iteration 183/1000 | Loss: 0.00001194
Iteration 184/1000 | Loss: 0.00001194
Iteration 185/1000 | Loss: 0.00001194
Iteration 186/1000 | Loss: 0.00001194
Iteration 187/1000 | Loss: 0.00001194
Iteration 188/1000 | Loss: 0.00001194
Iteration 189/1000 | Loss: 0.00001193
Iteration 190/1000 | Loss: 0.00001193
Iteration 191/1000 | Loss: 0.00001193
Iteration 192/1000 | Loss: 0.00001193
Iteration 193/1000 | Loss: 0.00001193
Iteration 194/1000 | Loss: 0.00001193
Iteration 195/1000 | Loss: 0.00001193
Iteration 196/1000 | Loss: 0.00001193
Iteration 197/1000 | Loss: 0.00001193
Iteration 198/1000 | Loss: 0.00001192
Iteration 199/1000 | Loss: 0.00001192
Iteration 200/1000 | Loss: 0.00001192
Iteration 201/1000 | Loss: 0.00001192
Iteration 202/1000 | Loss: 0.00001192
Iteration 203/1000 | Loss: 0.00001192
Iteration 204/1000 | Loss: 0.00001192
Iteration 205/1000 | Loss: 0.00001192
Iteration 206/1000 | Loss: 0.00001192
Iteration 207/1000 | Loss: 0.00001191
Iteration 208/1000 | Loss: 0.00001191
Iteration 209/1000 | Loss: 0.00001191
Iteration 210/1000 | Loss: 0.00001191
Iteration 211/1000 | Loss: 0.00001191
Iteration 212/1000 | Loss: 0.00001191
Iteration 213/1000 | Loss: 0.00001191
Iteration 214/1000 | Loss: 0.00001191
Iteration 215/1000 | Loss: 0.00001190
Iteration 216/1000 | Loss: 0.00001190
Iteration 217/1000 | Loss: 0.00001190
Iteration 218/1000 | Loss: 0.00001190
Iteration 219/1000 | Loss: 0.00001190
Iteration 220/1000 | Loss: 0.00001190
Iteration 221/1000 | Loss: 0.00001189
Iteration 222/1000 | Loss: 0.00001189
Iteration 223/1000 | Loss: 0.00001189
Iteration 224/1000 | Loss: 0.00001189
Iteration 225/1000 | Loss: 0.00001188
Iteration 226/1000 | Loss: 0.00001188
Iteration 227/1000 | Loss: 0.00001188
Iteration 228/1000 | Loss: 0.00001188
Iteration 229/1000 | Loss: 0.00001188
Iteration 230/1000 | Loss: 0.00001187
Iteration 231/1000 | Loss: 0.00001187
Iteration 232/1000 | Loss: 0.00001187
Iteration 233/1000 | Loss: 0.00001187
Iteration 234/1000 | Loss: 0.00001187
Iteration 235/1000 | Loss: 0.00001187
Iteration 236/1000 | Loss: 0.00001187
Iteration 237/1000 | Loss: 0.00001187
Iteration 238/1000 | Loss: 0.00001187
Iteration 239/1000 | Loss: 0.00001186
Iteration 240/1000 | Loss: 0.00001186
Iteration 241/1000 | Loss: 0.00001186
Iteration 242/1000 | Loss: 0.00001186
Iteration 243/1000 | Loss: 0.00001186
Iteration 244/1000 | Loss: 0.00001186
Iteration 245/1000 | Loss: 0.00001185
Iteration 246/1000 | Loss: 0.00001185
Iteration 247/1000 | Loss: 0.00001185
Iteration 248/1000 | Loss: 0.00001185
Iteration 249/1000 | Loss: 0.00001185
Iteration 250/1000 | Loss: 0.00001185
Iteration 251/1000 | Loss: 0.00001185
Iteration 252/1000 | Loss: 0.00001184
Iteration 253/1000 | Loss: 0.00001184
Iteration 254/1000 | Loss: 0.00001184
Iteration 255/1000 | Loss: 0.00001184
Iteration 256/1000 | Loss: 0.00001183
Iteration 257/1000 | Loss: 0.00001183
Iteration 258/1000 | Loss: 0.00001183
Iteration 259/1000 | Loss: 0.00001183
Iteration 260/1000 | Loss: 0.00001183
Iteration 261/1000 | Loss: 0.00001182
Iteration 262/1000 | Loss: 0.00001182
Iteration 263/1000 | Loss: 0.00001182
Iteration 264/1000 | Loss: 0.00001182
Iteration 265/1000 | Loss: 0.00001182
Iteration 266/1000 | Loss: 0.00001182
Iteration 267/1000 | Loss: 0.00001182
Iteration 268/1000 | Loss: 0.00001181
Iteration 269/1000 | Loss: 0.00001181
Iteration 270/1000 | Loss: 0.00001181
Iteration 271/1000 | Loss: 0.00001181
Iteration 272/1000 | Loss: 0.00001181
Iteration 273/1000 | Loss: 0.00001181
Iteration 274/1000 | Loss: 0.00001180
Iteration 275/1000 | Loss: 0.00001180
Iteration 276/1000 | Loss: 0.00001180
Iteration 277/1000 | Loss: 0.00001180
Iteration 278/1000 | Loss: 0.00001180
Iteration 279/1000 | Loss: 0.00001180
Iteration 280/1000 | Loss: 0.00001180
Iteration 281/1000 | Loss: 0.00001180
Iteration 282/1000 | Loss: 0.00001180
Iteration 283/1000 | Loss: 0.00001180
Iteration 284/1000 | Loss: 0.00001180
Iteration 285/1000 | Loss: 0.00001180
Iteration 286/1000 | Loss: 0.00001180
Iteration 287/1000 | Loss: 0.00001180
Iteration 288/1000 | Loss: 0.00001179
Iteration 289/1000 | Loss: 0.00001179
Iteration 290/1000 | Loss: 0.00001179
Iteration 291/1000 | Loss: 0.00001179
Iteration 292/1000 | Loss: 0.00001179
Iteration 293/1000 | Loss: 0.00001179
Iteration 294/1000 | Loss: 0.00001179
Iteration 295/1000 | Loss: 0.00001179
Iteration 296/1000 | Loss: 0.00001179
Iteration 297/1000 | Loss: 0.00001179
Iteration 298/1000 | Loss: 0.00001179
Iteration 299/1000 | Loss: 0.00001179
Iteration 300/1000 | Loss: 0.00001178
Iteration 301/1000 | Loss: 0.00001178
Iteration 302/1000 | Loss: 0.00001178
Iteration 303/1000 | Loss: 0.00001178
Iteration 304/1000 | Loss: 0.00001178
Iteration 305/1000 | Loss: 0.00001178
Iteration 306/1000 | Loss: 0.00001178
Iteration 307/1000 | Loss: 0.00001178
Iteration 308/1000 | Loss: 0.00001178
Iteration 309/1000 | Loss: 0.00001178
Iteration 310/1000 | Loss: 0.00001177
Iteration 311/1000 | Loss: 0.00001177
Iteration 312/1000 | Loss: 0.00001177
Iteration 313/1000 | Loss: 0.00001177
Iteration 314/1000 | Loss: 0.00001177
Iteration 315/1000 | Loss: 0.00001177
Iteration 316/1000 | Loss: 0.00001177
Iteration 317/1000 | Loss: 0.00001177
Iteration 318/1000 | Loss: 0.00001177
Iteration 319/1000 | Loss: 0.00001177
Iteration 320/1000 | Loss: 0.00001177
Iteration 321/1000 | Loss: 0.00001176
Iteration 322/1000 | Loss: 0.00001176
Iteration 323/1000 | Loss: 0.00001176
Iteration 324/1000 | Loss: 0.00001176
Iteration 325/1000 | Loss: 0.00001176
Iteration 326/1000 | Loss: 0.00001176
Iteration 327/1000 | Loss: 0.00001176
Iteration 328/1000 | Loss: 0.00001176
Iteration 329/1000 | Loss: 0.00001176
Iteration 330/1000 | Loss: 0.00001176
Iteration 331/1000 | Loss: 0.00001176
Iteration 332/1000 | Loss: 0.00001176
Iteration 333/1000 | Loss: 0.00001176
Iteration 334/1000 | Loss: 0.00001175
Iteration 335/1000 | Loss: 0.00001175
Iteration 336/1000 | Loss: 0.00001175
Iteration 337/1000 | Loss: 0.00001175
Iteration 338/1000 | Loss: 0.00001175
Iteration 339/1000 | Loss: 0.00001175
Iteration 340/1000 | Loss: 0.00001175
Iteration 341/1000 | Loss: 0.00001175
Iteration 342/1000 | Loss: 0.00001175
Iteration 343/1000 | Loss: 0.00001175
Iteration 344/1000 | Loss: 0.00001175
Iteration 345/1000 | Loss: 0.00001175
Iteration 346/1000 | Loss: 0.00001175
Iteration 347/1000 | Loss: 0.00001175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 347. Stopping optimization.
Last 5 losses: [1.1754622391890734e-05, 1.1754622391890734e-05, 1.1754622391890734e-05, 1.1754622391890734e-05, 1.1754622391890734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1754622391890734e-05

Optimization complete. Final v2v error: 2.8231897354125977 mm

Highest mean error: 5.882772445678711 mm for frame 31

Lowest mean error: 2.522454023361206 mm for frame 21

Saving results

Total time: 87.43496251106262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00767250
Iteration 2/25 | Loss: 0.00150219
Iteration 3/25 | Loss: 0.00121198
Iteration 4/25 | Loss: 0.00116157
Iteration 5/25 | Loss: 0.00115225
Iteration 6/25 | Loss: 0.00114970
Iteration 7/25 | Loss: 0.00114918
Iteration 8/25 | Loss: 0.00114918
Iteration 9/25 | Loss: 0.00114918
Iteration 10/25 | Loss: 0.00114918
Iteration 11/25 | Loss: 0.00114918
Iteration 12/25 | Loss: 0.00114918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001149178366176784, 0.001149178366176784, 0.001149178366176784, 0.001149178366176784, 0.001149178366176784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001149178366176784

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41818213
Iteration 2/25 | Loss: 0.00051424
Iteration 3/25 | Loss: 0.00051424
Iteration 4/25 | Loss: 0.00051424
Iteration 5/25 | Loss: 0.00051424
Iteration 6/25 | Loss: 0.00051424
Iteration 7/25 | Loss: 0.00051424
Iteration 8/25 | Loss: 0.00051424
Iteration 9/25 | Loss: 0.00051424
Iteration 10/25 | Loss: 0.00051424
Iteration 11/25 | Loss: 0.00051424
Iteration 12/25 | Loss: 0.00051424
Iteration 13/25 | Loss: 0.00051424
Iteration 14/25 | Loss: 0.00051424
Iteration 15/25 | Loss: 0.00051424
Iteration 16/25 | Loss: 0.00051424
Iteration 17/25 | Loss: 0.00051424
Iteration 18/25 | Loss: 0.00051424
Iteration 19/25 | Loss: 0.00051424
Iteration 20/25 | Loss: 0.00051424
Iteration 21/25 | Loss: 0.00051424
Iteration 22/25 | Loss: 0.00051424
Iteration 23/25 | Loss: 0.00051424
Iteration 24/25 | Loss: 0.00051424
Iteration 25/25 | Loss: 0.00051424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051424
Iteration 2/1000 | Loss: 0.00006047
Iteration 3/1000 | Loss: 0.00004059
Iteration 4/1000 | Loss: 0.00003156
Iteration 5/1000 | Loss: 0.00003000
Iteration 6/1000 | Loss: 0.00002899
Iteration 7/1000 | Loss: 0.00002811
Iteration 8/1000 | Loss: 0.00002762
Iteration 9/1000 | Loss: 0.00002717
Iteration 10/1000 | Loss: 0.00002690
Iteration 11/1000 | Loss: 0.00002668
Iteration 12/1000 | Loss: 0.00002662
Iteration 13/1000 | Loss: 0.00002639
Iteration 14/1000 | Loss: 0.00002633
Iteration 15/1000 | Loss: 0.00002621
Iteration 16/1000 | Loss: 0.00002617
Iteration 17/1000 | Loss: 0.00002615
Iteration 18/1000 | Loss: 0.00002615
Iteration 19/1000 | Loss: 0.00002614
Iteration 20/1000 | Loss: 0.00002613
Iteration 21/1000 | Loss: 0.00002612
Iteration 22/1000 | Loss: 0.00002609
Iteration 23/1000 | Loss: 0.00002602
Iteration 24/1000 | Loss: 0.00002598
Iteration 25/1000 | Loss: 0.00002598
Iteration 26/1000 | Loss: 0.00002597
Iteration 27/1000 | Loss: 0.00002597
Iteration 28/1000 | Loss: 0.00002596
Iteration 29/1000 | Loss: 0.00002594
Iteration 30/1000 | Loss: 0.00002592
Iteration 31/1000 | Loss: 0.00002591
Iteration 32/1000 | Loss: 0.00002591
Iteration 33/1000 | Loss: 0.00002590
Iteration 34/1000 | Loss: 0.00002590
Iteration 35/1000 | Loss: 0.00002589
Iteration 36/1000 | Loss: 0.00002589
Iteration 37/1000 | Loss: 0.00002588
Iteration 38/1000 | Loss: 0.00002588
Iteration 39/1000 | Loss: 0.00002587
Iteration 40/1000 | Loss: 0.00002586
Iteration 41/1000 | Loss: 0.00002586
Iteration 42/1000 | Loss: 0.00002584
Iteration 43/1000 | Loss: 0.00002584
Iteration 44/1000 | Loss: 0.00002583
Iteration 45/1000 | Loss: 0.00002583
Iteration 46/1000 | Loss: 0.00002583
Iteration 47/1000 | Loss: 0.00002582
Iteration 48/1000 | Loss: 0.00002582
Iteration 49/1000 | Loss: 0.00002582
Iteration 50/1000 | Loss: 0.00002581
Iteration 51/1000 | Loss: 0.00002581
Iteration 52/1000 | Loss: 0.00002581
Iteration 53/1000 | Loss: 0.00002580
Iteration 54/1000 | Loss: 0.00002580
Iteration 55/1000 | Loss: 0.00002580
Iteration 56/1000 | Loss: 0.00002579
Iteration 57/1000 | Loss: 0.00002579
Iteration 58/1000 | Loss: 0.00002579
Iteration 59/1000 | Loss: 0.00002579
Iteration 60/1000 | Loss: 0.00002579
Iteration 61/1000 | Loss: 0.00002578
Iteration 62/1000 | Loss: 0.00002578
Iteration 63/1000 | Loss: 0.00002578
Iteration 64/1000 | Loss: 0.00002578
Iteration 65/1000 | Loss: 0.00002578
Iteration 66/1000 | Loss: 0.00002577
Iteration 67/1000 | Loss: 0.00002577
Iteration 68/1000 | Loss: 0.00002577
Iteration 69/1000 | Loss: 0.00002576
Iteration 70/1000 | Loss: 0.00002576
Iteration 71/1000 | Loss: 0.00002576
Iteration 72/1000 | Loss: 0.00002576
Iteration 73/1000 | Loss: 0.00002576
Iteration 74/1000 | Loss: 0.00002576
Iteration 75/1000 | Loss: 0.00002576
Iteration 76/1000 | Loss: 0.00002576
Iteration 77/1000 | Loss: 0.00002576
Iteration 78/1000 | Loss: 0.00002575
Iteration 79/1000 | Loss: 0.00002575
Iteration 80/1000 | Loss: 0.00002575
Iteration 81/1000 | Loss: 0.00002575
Iteration 82/1000 | Loss: 0.00002575
Iteration 83/1000 | Loss: 0.00002575
Iteration 84/1000 | Loss: 0.00002575
Iteration 85/1000 | Loss: 0.00002575
Iteration 86/1000 | Loss: 0.00002575
Iteration 87/1000 | Loss: 0.00002575
Iteration 88/1000 | Loss: 0.00002575
Iteration 89/1000 | Loss: 0.00002574
Iteration 90/1000 | Loss: 0.00002574
Iteration 91/1000 | Loss: 0.00002574
Iteration 92/1000 | Loss: 0.00002573
Iteration 93/1000 | Loss: 0.00002573
Iteration 94/1000 | Loss: 0.00002573
Iteration 95/1000 | Loss: 0.00002573
Iteration 96/1000 | Loss: 0.00002573
Iteration 97/1000 | Loss: 0.00002572
Iteration 98/1000 | Loss: 0.00002572
Iteration 99/1000 | Loss: 0.00002572
Iteration 100/1000 | Loss: 0.00002572
Iteration 101/1000 | Loss: 0.00002572
Iteration 102/1000 | Loss: 0.00002572
Iteration 103/1000 | Loss: 0.00002572
Iteration 104/1000 | Loss: 0.00002571
Iteration 105/1000 | Loss: 0.00002571
Iteration 106/1000 | Loss: 0.00002571
Iteration 107/1000 | Loss: 0.00002571
Iteration 108/1000 | Loss: 0.00002571
Iteration 109/1000 | Loss: 0.00002571
Iteration 110/1000 | Loss: 0.00002571
Iteration 111/1000 | Loss: 0.00002571
Iteration 112/1000 | Loss: 0.00002571
Iteration 113/1000 | Loss: 0.00002570
Iteration 114/1000 | Loss: 0.00002570
Iteration 115/1000 | Loss: 0.00002570
Iteration 116/1000 | Loss: 0.00002570
Iteration 117/1000 | Loss: 0.00002570
Iteration 118/1000 | Loss: 0.00002570
Iteration 119/1000 | Loss: 0.00002570
Iteration 120/1000 | Loss: 0.00002569
Iteration 121/1000 | Loss: 0.00002569
Iteration 122/1000 | Loss: 0.00002569
Iteration 123/1000 | Loss: 0.00002569
Iteration 124/1000 | Loss: 0.00002569
Iteration 125/1000 | Loss: 0.00002569
Iteration 126/1000 | Loss: 0.00002569
Iteration 127/1000 | Loss: 0.00002569
Iteration 128/1000 | Loss: 0.00002569
Iteration 129/1000 | Loss: 0.00002569
Iteration 130/1000 | Loss: 0.00002569
Iteration 131/1000 | Loss: 0.00002568
Iteration 132/1000 | Loss: 0.00002568
Iteration 133/1000 | Loss: 0.00002568
Iteration 134/1000 | Loss: 0.00002568
Iteration 135/1000 | Loss: 0.00002568
Iteration 136/1000 | Loss: 0.00002568
Iteration 137/1000 | Loss: 0.00002568
Iteration 138/1000 | Loss: 0.00002568
Iteration 139/1000 | Loss: 0.00002568
Iteration 140/1000 | Loss: 0.00002568
Iteration 141/1000 | Loss: 0.00002568
Iteration 142/1000 | Loss: 0.00002568
Iteration 143/1000 | Loss: 0.00002568
Iteration 144/1000 | Loss: 0.00002568
Iteration 145/1000 | Loss: 0.00002568
Iteration 146/1000 | Loss: 0.00002568
Iteration 147/1000 | Loss: 0.00002568
Iteration 148/1000 | Loss: 0.00002568
Iteration 149/1000 | Loss: 0.00002568
Iteration 150/1000 | Loss: 0.00002568
Iteration 151/1000 | Loss: 0.00002568
Iteration 152/1000 | Loss: 0.00002568
Iteration 153/1000 | Loss: 0.00002567
Iteration 154/1000 | Loss: 0.00002567
Iteration 155/1000 | Loss: 0.00002567
Iteration 156/1000 | Loss: 0.00002567
Iteration 157/1000 | Loss: 0.00002567
Iteration 158/1000 | Loss: 0.00002567
Iteration 159/1000 | Loss: 0.00002567
Iteration 160/1000 | Loss: 0.00002567
Iteration 161/1000 | Loss: 0.00002567
Iteration 162/1000 | Loss: 0.00002567
Iteration 163/1000 | Loss: 0.00002567
Iteration 164/1000 | Loss: 0.00002567
Iteration 165/1000 | Loss: 0.00002567
Iteration 166/1000 | Loss: 0.00002567
Iteration 167/1000 | Loss: 0.00002567
Iteration 168/1000 | Loss: 0.00002567
Iteration 169/1000 | Loss: 0.00002567
Iteration 170/1000 | Loss: 0.00002567
Iteration 171/1000 | Loss: 0.00002566
Iteration 172/1000 | Loss: 0.00002566
Iteration 173/1000 | Loss: 0.00002566
Iteration 174/1000 | Loss: 0.00002566
Iteration 175/1000 | Loss: 0.00002566
Iteration 176/1000 | Loss: 0.00002566
Iteration 177/1000 | Loss: 0.00002566
Iteration 178/1000 | Loss: 0.00002566
Iteration 179/1000 | Loss: 0.00002566
Iteration 180/1000 | Loss: 0.00002566
Iteration 181/1000 | Loss: 0.00002566
Iteration 182/1000 | Loss: 0.00002566
Iteration 183/1000 | Loss: 0.00002566
Iteration 184/1000 | Loss: 0.00002566
Iteration 185/1000 | Loss: 0.00002566
Iteration 186/1000 | Loss: 0.00002566
Iteration 187/1000 | Loss: 0.00002566
Iteration 188/1000 | Loss: 0.00002566
Iteration 189/1000 | Loss: 0.00002566
Iteration 190/1000 | Loss: 0.00002566
Iteration 191/1000 | Loss: 0.00002566
Iteration 192/1000 | Loss: 0.00002566
Iteration 193/1000 | Loss: 0.00002566
Iteration 194/1000 | Loss: 0.00002566
Iteration 195/1000 | Loss: 0.00002566
Iteration 196/1000 | Loss: 0.00002566
Iteration 197/1000 | Loss: 0.00002566
Iteration 198/1000 | Loss: 0.00002566
Iteration 199/1000 | Loss: 0.00002566
Iteration 200/1000 | Loss: 0.00002566
Iteration 201/1000 | Loss: 0.00002566
Iteration 202/1000 | Loss: 0.00002566
Iteration 203/1000 | Loss: 0.00002566
Iteration 204/1000 | Loss: 0.00002566
Iteration 205/1000 | Loss: 0.00002566
Iteration 206/1000 | Loss: 0.00002566
Iteration 207/1000 | Loss: 0.00002566
Iteration 208/1000 | Loss: 0.00002566
Iteration 209/1000 | Loss: 0.00002566
Iteration 210/1000 | Loss: 0.00002566
Iteration 211/1000 | Loss: 0.00002566
Iteration 212/1000 | Loss: 0.00002566
Iteration 213/1000 | Loss: 0.00002566
Iteration 214/1000 | Loss: 0.00002566
Iteration 215/1000 | Loss: 0.00002566
Iteration 216/1000 | Loss: 0.00002566
Iteration 217/1000 | Loss: 0.00002566
Iteration 218/1000 | Loss: 0.00002566
Iteration 219/1000 | Loss: 0.00002566
Iteration 220/1000 | Loss: 0.00002566
Iteration 221/1000 | Loss: 0.00002566
Iteration 222/1000 | Loss: 0.00002566
Iteration 223/1000 | Loss: 0.00002566
Iteration 224/1000 | Loss: 0.00002566
Iteration 225/1000 | Loss: 0.00002566
Iteration 226/1000 | Loss: 0.00002566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.565793511166703e-05, 2.565793511166703e-05, 2.565793511166703e-05, 2.565793511166703e-05, 2.565793511166703e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.565793511166703e-05

Optimization complete. Final v2v error: 4.1564483642578125 mm

Highest mean error: 5.745500087738037 mm for frame 27

Lowest mean error: 3.1206679344177246 mm for frame 179

Saving results

Total time: 45.68074035644531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00742617
Iteration 2/25 | Loss: 0.00121065
Iteration 3/25 | Loss: 0.00111345
Iteration 4/25 | Loss: 0.00110499
Iteration 5/25 | Loss: 0.00110223
Iteration 6/25 | Loss: 0.00110188
Iteration 7/25 | Loss: 0.00110188
Iteration 8/25 | Loss: 0.00110188
Iteration 9/25 | Loss: 0.00110188
Iteration 10/25 | Loss: 0.00110188
Iteration 11/25 | Loss: 0.00110188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011018807999789715, 0.0011018807999789715, 0.0011018807999789715, 0.0011018807999789715, 0.0011018807999789715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011018807999789715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36565447
Iteration 2/25 | Loss: 0.00058422
Iteration 3/25 | Loss: 0.00058414
Iteration 4/25 | Loss: 0.00058414
Iteration 5/25 | Loss: 0.00058414
Iteration 6/25 | Loss: 0.00058414
Iteration 7/25 | Loss: 0.00058414
Iteration 8/25 | Loss: 0.00058414
Iteration 9/25 | Loss: 0.00058414
Iteration 10/25 | Loss: 0.00058414
Iteration 11/25 | Loss: 0.00058414
Iteration 12/25 | Loss: 0.00058414
Iteration 13/25 | Loss: 0.00058414
Iteration 14/25 | Loss: 0.00058414
Iteration 15/25 | Loss: 0.00058414
Iteration 16/25 | Loss: 0.00058414
Iteration 17/25 | Loss: 0.00058414
Iteration 18/25 | Loss: 0.00058414
Iteration 19/25 | Loss: 0.00058414
Iteration 20/25 | Loss: 0.00058414
Iteration 21/25 | Loss: 0.00058414
Iteration 22/25 | Loss: 0.00058414
Iteration 23/25 | Loss: 0.00058414
Iteration 24/25 | Loss: 0.00058414
Iteration 25/25 | Loss: 0.00058414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058414
Iteration 2/1000 | Loss: 0.00004004
Iteration 3/1000 | Loss: 0.00002473
Iteration 4/1000 | Loss: 0.00002214
Iteration 5/1000 | Loss: 0.00002085
Iteration 6/1000 | Loss: 0.00001995
Iteration 7/1000 | Loss: 0.00001948
Iteration 8/1000 | Loss: 0.00001886
Iteration 9/1000 | Loss: 0.00001843
Iteration 10/1000 | Loss: 0.00001816
Iteration 11/1000 | Loss: 0.00001800
Iteration 12/1000 | Loss: 0.00001796
Iteration 13/1000 | Loss: 0.00001775
Iteration 14/1000 | Loss: 0.00001753
Iteration 15/1000 | Loss: 0.00001748
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001724
Iteration 18/1000 | Loss: 0.00001722
Iteration 19/1000 | Loss: 0.00001721
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001717
Iteration 22/1000 | Loss: 0.00001717
Iteration 23/1000 | Loss: 0.00001716
Iteration 24/1000 | Loss: 0.00001715
Iteration 25/1000 | Loss: 0.00001711
Iteration 26/1000 | Loss: 0.00001711
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001711
Iteration 29/1000 | Loss: 0.00001710
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001709
Iteration 32/1000 | Loss: 0.00001708
Iteration 33/1000 | Loss: 0.00001708
Iteration 34/1000 | Loss: 0.00001707
Iteration 35/1000 | Loss: 0.00001707
Iteration 36/1000 | Loss: 0.00001706
Iteration 37/1000 | Loss: 0.00001706
Iteration 38/1000 | Loss: 0.00001705
Iteration 39/1000 | Loss: 0.00001705
Iteration 40/1000 | Loss: 0.00001704
Iteration 41/1000 | Loss: 0.00001704
Iteration 42/1000 | Loss: 0.00001703
Iteration 43/1000 | Loss: 0.00001703
Iteration 44/1000 | Loss: 0.00001703
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001701
Iteration 50/1000 | Loss: 0.00001701
Iteration 51/1000 | Loss: 0.00001700
Iteration 52/1000 | Loss: 0.00001700
Iteration 53/1000 | Loss: 0.00001700
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001699
Iteration 56/1000 | Loss: 0.00001698
Iteration 57/1000 | Loss: 0.00001698
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001697
Iteration 61/1000 | Loss: 0.00001696
Iteration 62/1000 | Loss: 0.00001696
Iteration 63/1000 | Loss: 0.00001696
Iteration 64/1000 | Loss: 0.00001696
Iteration 65/1000 | Loss: 0.00001696
Iteration 66/1000 | Loss: 0.00001696
Iteration 67/1000 | Loss: 0.00001696
Iteration 68/1000 | Loss: 0.00001696
Iteration 69/1000 | Loss: 0.00001696
Iteration 70/1000 | Loss: 0.00001695
Iteration 71/1000 | Loss: 0.00001695
Iteration 72/1000 | Loss: 0.00001695
Iteration 73/1000 | Loss: 0.00001695
Iteration 74/1000 | Loss: 0.00001695
Iteration 75/1000 | Loss: 0.00001695
Iteration 76/1000 | Loss: 0.00001695
Iteration 77/1000 | Loss: 0.00001694
Iteration 78/1000 | Loss: 0.00001694
Iteration 79/1000 | Loss: 0.00001694
Iteration 80/1000 | Loss: 0.00001694
Iteration 81/1000 | Loss: 0.00001694
Iteration 82/1000 | Loss: 0.00001694
Iteration 83/1000 | Loss: 0.00001693
Iteration 84/1000 | Loss: 0.00001693
Iteration 85/1000 | Loss: 0.00001693
Iteration 86/1000 | Loss: 0.00001693
Iteration 87/1000 | Loss: 0.00001693
Iteration 88/1000 | Loss: 0.00001692
Iteration 89/1000 | Loss: 0.00001692
Iteration 90/1000 | Loss: 0.00001692
Iteration 91/1000 | Loss: 0.00001692
Iteration 92/1000 | Loss: 0.00001692
Iteration 93/1000 | Loss: 0.00001692
Iteration 94/1000 | Loss: 0.00001692
Iteration 95/1000 | Loss: 0.00001692
Iteration 96/1000 | Loss: 0.00001692
Iteration 97/1000 | Loss: 0.00001691
Iteration 98/1000 | Loss: 0.00001691
Iteration 99/1000 | Loss: 0.00001691
Iteration 100/1000 | Loss: 0.00001691
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001690
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001688
Iteration 121/1000 | Loss: 0.00001688
Iteration 122/1000 | Loss: 0.00001688
Iteration 123/1000 | Loss: 0.00001688
Iteration 124/1000 | Loss: 0.00001688
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001688
Iteration 129/1000 | Loss: 0.00001688
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001687
Iteration 132/1000 | Loss: 0.00001687
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001686
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001686
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001685
Iteration 152/1000 | Loss: 0.00001685
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001684
Iteration 158/1000 | Loss: 0.00001684
Iteration 159/1000 | Loss: 0.00001684
Iteration 160/1000 | Loss: 0.00001684
Iteration 161/1000 | Loss: 0.00001684
Iteration 162/1000 | Loss: 0.00001683
Iteration 163/1000 | Loss: 0.00001683
Iteration 164/1000 | Loss: 0.00001683
Iteration 165/1000 | Loss: 0.00001683
Iteration 166/1000 | Loss: 0.00001683
Iteration 167/1000 | Loss: 0.00001683
Iteration 168/1000 | Loss: 0.00001682
Iteration 169/1000 | Loss: 0.00001682
Iteration 170/1000 | Loss: 0.00001682
Iteration 171/1000 | Loss: 0.00001682
Iteration 172/1000 | Loss: 0.00001681
Iteration 173/1000 | Loss: 0.00001681
Iteration 174/1000 | Loss: 0.00001681
Iteration 175/1000 | Loss: 0.00001681
Iteration 176/1000 | Loss: 0.00001681
Iteration 177/1000 | Loss: 0.00001681
Iteration 178/1000 | Loss: 0.00001681
Iteration 179/1000 | Loss: 0.00001681
Iteration 180/1000 | Loss: 0.00001681
Iteration 181/1000 | Loss: 0.00001681
Iteration 182/1000 | Loss: 0.00001680
Iteration 183/1000 | Loss: 0.00001680
Iteration 184/1000 | Loss: 0.00001680
Iteration 185/1000 | Loss: 0.00001680
Iteration 186/1000 | Loss: 0.00001679
Iteration 187/1000 | Loss: 0.00001679
Iteration 188/1000 | Loss: 0.00001679
Iteration 189/1000 | Loss: 0.00001679
Iteration 190/1000 | Loss: 0.00001679
Iteration 191/1000 | Loss: 0.00001679
Iteration 192/1000 | Loss: 0.00001679
Iteration 193/1000 | Loss: 0.00001679
Iteration 194/1000 | Loss: 0.00001679
Iteration 195/1000 | Loss: 0.00001679
Iteration 196/1000 | Loss: 0.00001678
Iteration 197/1000 | Loss: 0.00001678
Iteration 198/1000 | Loss: 0.00001678
Iteration 199/1000 | Loss: 0.00001678
Iteration 200/1000 | Loss: 0.00001678
Iteration 201/1000 | Loss: 0.00001678
Iteration 202/1000 | Loss: 0.00001678
Iteration 203/1000 | Loss: 0.00001677
Iteration 204/1000 | Loss: 0.00001677
Iteration 205/1000 | Loss: 0.00001677
Iteration 206/1000 | Loss: 0.00001677
Iteration 207/1000 | Loss: 0.00001677
Iteration 208/1000 | Loss: 0.00001677
Iteration 209/1000 | Loss: 0.00001677
Iteration 210/1000 | Loss: 0.00001677
Iteration 211/1000 | Loss: 0.00001677
Iteration 212/1000 | Loss: 0.00001677
Iteration 213/1000 | Loss: 0.00001677
Iteration 214/1000 | Loss: 0.00001677
Iteration 215/1000 | Loss: 0.00001677
Iteration 216/1000 | Loss: 0.00001677
Iteration 217/1000 | Loss: 0.00001676
Iteration 218/1000 | Loss: 0.00001676
Iteration 219/1000 | Loss: 0.00001676
Iteration 220/1000 | Loss: 0.00001676
Iteration 221/1000 | Loss: 0.00001676
Iteration 222/1000 | Loss: 0.00001676
Iteration 223/1000 | Loss: 0.00001676
Iteration 224/1000 | Loss: 0.00001676
Iteration 225/1000 | Loss: 0.00001676
Iteration 226/1000 | Loss: 0.00001676
Iteration 227/1000 | Loss: 0.00001676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.676336432865355e-05, 1.676336432865355e-05, 1.676336432865355e-05, 1.676336432865355e-05, 1.676336432865355e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.676336432865355e-05

Optimization complete. Final v2v error: 3.420328378677368 mm

Highest mean error: 3.6963584423065186 mm for frame 45

Lowest mean error: 3.2766671180725098 mm for frame 97

Saving results

Total time: 44.39710736274719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883292
Iteration 2/25 | Loss: 0.00122050
Iteration 3/25 | Loss: 0.00116854
Iteration 4/25 | Loss: 0.00116169
Iteration 5/25 | Loss: 0.00115990
Iteration 6/25 | Loss: 0.00115990
Iteration 7/25 | Loss: 0.00115990
Iteration 8/25 | Loss: 0.00115990
Iteration 9/25 | Loss: 0.00115990
Iteration 10/25 | Loss: 0.00115990
Iteration 11/25 | Loss: 0.00115990
Iteration 12/25 | Loss: 0.00115990
Iteration 13/25 | Loss: 0.00115990
Iteration 14/25 | Loss: 0.00115990
Iteration 15/25 | Loss: 0.00115990
Iteration 16/25 | Loss: 0.00115990
Iteration 17/25 | Loss: 0.00115990
Iteration 18/25 | Loss: 0.00115990
Iteration 19/25 | Loss: 0.00115990
Iteration 20/25 | Loss: 0.00115990
Iteration 21/25 | Loss: 0.00115990
Iteration 22/25 | Loss: 0.00115990
Iteration 23/25 | Loss: 0.00115990
Iteration 24/25 | Loss: 0.00115990
Iteration 25/25 | Loss: 0.00115990

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32125020
Iteration 2/25 | Loss: 0.00087378
Iteration 3/25 | Loss: 0.00087378
Iteration 4/25 | Loss: 0.00087378
Iteration 5/25 | Loss: 0.00087378
Iteration 6/25 | Loss: 0.00087378
Iteration 7/25 | Loss: 0.00087378
Iteration 8/25 | Loss: 0.00087377
Iteration 9/25 | Loss: 0.00087377
Iteration 10/25 | Loss: 0.00087377
Iteration 11/25 | Loss: 0.00087377
Iteration 12/25 | Loss: 0.00087377
Iteration 13/25 | Loss: 0.00087377
Iteration 14/25 | Loss: 0.00087377
Iteration 15/25 | Loss: 0.00087377
Iteration 16/25 | Loss: 0.00087377
Iteration 17/25 | Loss: 0.00087377
Iteration 18/25 | Loss: 0.00087377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008737737080082297, 0.0008737737080082297, 0.0008737737080082297, 0.0008737737080082297, 0.0008737737080082297]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008737737080082297

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087377
Iteration 2/1000 | Loss: 0.00002841
Iteration 3/1000 | Loss: 0.00001750
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001522
Iteration 7/1000 | Loss: 0.00001507
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001480
Iteration 10/1000 | Loss: 0.00001479
Iteration 11/1000 | Loss: 0.00001462
Iteration 12/1000 | Loss: 0.00001451
Iteration 13/1000 | Loss: 0.00001436
Iteration 14/1000 | Loss: 0.00001436
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001432
Iteration 17/1000 | Loss: 0.00001429
Iteration 18/1000 | Loss: 0.00001429
Iteration 19/1000 | Loss: 0.00001428
Iteration 20/1000 | Loss: 0.00001428
Iteration 21/1000 | Loss: 0.00001425
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001414
Iteration 25/1000 | Loss: 0.00001414
Iteration 26/1000 | Loss: 0.00001413
Iteration 27/1000 | Loss: 0.00001413
Iteration 28/1000 | Loss: 0.00001413
Iteration 29/1000 | Loss: 0.00001413
Iteration 30/1000 | Loss: 0.00001412
Iteration 31/1000 | Loss: 0.00001412
Iteration 32/1000 | Loss: 0.00001411
Iteration 33/1000 | Loss: 0.00001411
Iteration 34/1000 | Loss: 0.00001408
Iteration 35/1000 | Loss: 0.00001408
Iteration 36/1000 | Loss: 0.00001408
Iteration 37/1000 | Loss: 0.00001408
Iteration 38/1000 | Loss: 0.00001408
Iteration 39/1000 | Loss: 0.00001407
Iteration 40/1000 | Loss: 0.00001407
Iteration 41/1000 | Loss: 0.00001407
Iteration 42/1000 | Loss: 0.00001407
Iteration 43/1000 | Loss: 0.00001407
Iteration 44/1000 | Loss: 0.00001407
Iteration 45/1000 | Loss: 0.00001407
Iteration 46/1000 | Loss: 0.00001407
Iteration 47/1000 | Loss: 0.00001406
Iteration 48/1000 | Loss: 0.00001406
Iteration 49/1000 | Loss: 0.00001405
Iteration 50/1000 | Loss: 0.00001405
Iteration 51/1000 | Loss: 0.00001405
Iteration 52/1000 | Loss: 0.00001404
Iteration 53/1000 | Loss: 0.00001404
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001404
Iteration 57/1000 | Loss: 0.00001403
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001403
Iteration 61/1000 | Loss: 0.00001403
Iteration 62/1000 | Loss: 0.00001403
Iteration 63/1000 | Loss: 0.00001403
Iteration 64/1000 | Loss: 0.00001403
Iteration 65/1000 | Loss: 0.00001403
Iteration 66/1000 | Loss: 0.00001403
Iteration 67/1000 | Loss: 0.00001403
Iteration 68/1000 | Loss: 0.00001403
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001402
Iteration 72/1000 | Loss: 0.00001402
Iteration 73/1000 | Loss: 0.00001402
Iteration 74/1000 | Loss: 0.00001402
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001401
Iteration 79/1000 | Loss: 0.00001400
Iteration 80/1000 | Loss: 0.00001400
Iteration 81/1000 | Loss: 0.00001400
Iteration 82/1000 | Loss: 0.00001400
Iteration 83/1000 | Loss: 0.00001400
Iteration 84/1000 | Loss: 0.00001400
Iteration 85/1000 | Loss: 0.00001400
Iteration 86/1000 | Loss: 0.00001400
Iteration 87/1000 | Loss: 0.00001400
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001399
Iteration 92/1000 | Loss: 0.00001399
Iteration 93/1000 | Loss: 0.00001399
Iteration 94/1000 | Loss: 0.00001399
Iteration 95/1000 | Loss: 0.00001399
Iteration 96/1000 | Loss: 0.00001399
Iteration 97/1000 | Loss: 0.00001399
Iteration 98/1000 | Loss: 0.00001399
Iteration 99/1000 | Loss: 0.00001399
Iteration 100/1000 | Loss: 0.00001399
Iteration 101/1000 | Loss: 0.00001398
Iteration 102/1000 | Loss: 0.00001398
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001398
Iteration 109/1000 | Loss: 0.00001398
Iteration 110/1000 | Loss: 0.00001398
Iteration 111/1000 | Loss: 0.00001398
Iteration 112/1000 | Loss: 0.00001398
Iteration 113/1000 | Loss: 0.00001398
Iteration 114/1000 | Loss: 0.00001398
Iteration 115/1000 | Loss: 0.00001398
Iteration 116/1000 | Loss: 0.00001398
Iteration 117/1000 | Loss: 0.00001398
Iteration 118/1000 | Loss: 0.00001398
Iteration 119/1000 | Loss: 0.00001398
Iteration 120/1000 | Loss: 0.00001398
Iteration 121/1000 | Loss: 0.00001398
Iteration 122/1000 | Loss: 0.00001398
Iteration 123/1000 | Loss: 0.00001398
Iteration 124/1000 | Loss: 0.00001398
Iteration 125/1000 | Loss: 0.00001398
Iteration 126/1000 | Loss: 0.00001398
Iteration 127/1000 | Loss: 0.00001398
Iteration 128/1000 | Loss: 0.00001398
Iteration 129/1000 | Loss: 0.00001398
Iteration 130/1000 | Loss: 0.00001398
Iteration 131/1000 | Loss: 0.00001398
Iteration 132/1000 | Loss: 0.00001398
Iteration 133/1000 | Loss: 0.00001398
Iteration 134/1000 | Loss: 0.00001398
Iteration 135/1000 | Loss: 0.00001398
Iteration 136/1000 | Loss: 0.00001398
Iteration 137/1000 | Loss: 0.00001398
Iteration 138/1000 | Loss: 0.00001398
Iteration 139/1000 | Loss: 0.00001398
Iteration 140/1000 | Loss: 0.00001398
Iteration 141/1000 | Loss: 0.00001398
Iteration 142/1000 | Loss: 0.00001398
Iteration 143/1000 | Loss: 0.00001398
Iteration 144/1000 | Loss: 0.00001398
Iteration 145/1000 | Loss: 0.00001398
Iteration 146/1000 | Loss: 0.00001398
Iteration 147/1000 | Loss: 0.00001398
Iteration 148/1000 | Loss: 0.00001398
Iteration 149/1000 | Loss: 0.00001398
Iteration 150/1000 | Loss: 0.00001398
Iteration 151/1000 | Loss: 0.00001398
Iteration 152/1000 | Loss: 0.00001398
Iteration 153/1000 | Loss: 0.00001398
Iteration 154/1000 | Loss: 0.00001398
Iteration 155/1000 | Loss: 0.00001398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.397564210492419e-05, 1.397564210492419e-05, 1.397564210492419e-05, 1.397564210492419e-05, 1.397564210492419e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.397564210492419e-05

Optimization complete. Final v2v error: 3.1567881107330322 mm

Highest mean error: 3.3386709690093994 mm for frame 45

Lowest mean error: 3.001844882965088 mm for frame 196

Saving results

Total time: 33.7472140789032
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404674
Iteration 2/25 | Loss: 0.00113694
Iteration 3/25 | Loss: 0.00105131
Iteration 4/25 | Loss: 0.00103864
Iteration 5/25 | Loss: 0.00103389
Iteration 6/25 | Loss: 0.00103268
Iteration 7/25 | Loss: 0.00103264
Iteration 8/25 | Loss: 0.00103264
Iteration 9/25 | Loss: 0.00103264
Iteration 10/25 | Loss: 0.00103264
Iteration 11/25 | Loss: 0.00103264
Iteration 12/25 | Loss: 0.00103264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010326431365683675, 0.0010326431365683675, 0.0010326431365683675, 0.0010326431365683675, 0.0010326431365683675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010326431365683675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.85860395
Iteration 2/25 | Loss: 0.00078219
Iteration 3/25 | Loss: 0.00078219
Iteration 4/25 | Loss: 0.00078219
Iteration 5/25 | Loss: 0.00078219
Iteration 6/25 | Loss: 0.00078219
Iteration 7/25 | Loss: 0.00078219
Iteration 8/25 | Loss: 0.00078219
Iteration 9/25 | Loss: 0.00078219
Iteration 10/25 | Loss: 0.00078219
Iteration 11/25 | Loss: 0.00078219
Iteration 12/25 | Loss: 0.00078219
Iteration 13/25 | Loss: 0.00078219
Iteration 14/25 | Loss: 0.00078219
Iteration 15/25 | Loss: 0.00078219
Iteration 16/25 | Loss: 0.00078219
Iteration 17/25 | Loss: 0.00078219
Iteration 18/25 | Loss: 0.00078219
Iteration 19/25 | Loss: 0.00078219
Iteration 20/25 | Loss: 0.00078219
Iteration 21/25 | Loss: 0.00078219
Iteration 22/25 | Loss: 0.00078219
Iteration 23/25 | Loss: 0.00078219
Iteration 24/25 | Loss: 0.00078219
Iteration 25/25 | Loss: 0.00078219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078219
Iteration 2/1000 | Loss: 0.00001477
Iteration 3/1000 | Loss: 0.00001075
Iteration 4/1000 | Loss: 0.00001005
Iteration 5/1000 | Loss: 0.00000964
Iteration 6/1000 | Loss: 0.00000927
Iteration 7/1000 | Loss: 0.00000904
Iteration 8/1000 | Loss: 0.00000902
Iteration 9/1000 | Loss: 0.00000902
Iteration 10/1000 | Loss: 0.00000883
Iteration 11/1000 | Loss: 0.00000867
Iteration 12/1000 | Loss: 0.00000867
Iteration 13/1000 | Loss: 0.00000857
Iteration 14/1000 | Loss: 0.00000855
Iteration 15/1000 | Loss: 0.00000851
Iteration 16/1000 | Loss: 0.00000851
Iteration 17/1000 | Loss: 0.00000848
Iteration 18/1000 | Loss: 0.00000844
Iteration 19/1000 | Loss: 0.00000844
Iteration 20/1000 | Loss: 0.00000844
Iteration 21/1000 | Loss: 0.00000844
Iteration 22/1000 | Loss: 0.00000844
Iteration 23/1000 | Loss: 0.00000843
Iteration 24/1000 | Loss: 0.00000842
Iteration 25/1000 | Loss: 0.00000841
Iteration 26/1000 | Loss: 0.00000841
Iteration 27/1000 | Loss: 0.00000840
Iteration 28/1000 | Loss: 0.00000840
Iteration 29/1000 | Loss: 0.00000837
Iteration 30/1000 | Loss: 0.00000837
Iteration 31/1000 | Loss: 0.00000836
Iteration 32/1000 | Loss: 0.00000835
Iteration 33/1000 | Loss: 0.00000835
Iteration 34/1000 | Loss: 0.00000835
Iteration 35/1000 | Loss: 0.00000835
Iteration 36/1000 | Loss: 0.00000835
Iteration 37/1000 | Loss: 0.00000835
Iteration 38/1000 | Loss: 0.00000835
Iteration 39/1000 | Loss: 0.00000835
Iteration 40/1000 | Loss: 0.00000835
Iteration 41/1000 | Loss: 0.00000835
Iteration 42/1000 | Loss: 0.00000835
Iteration 43/1000 | Loss: 0.00000834
Iteration 44/1000 | Loss: 0.00000834
Iteration 45/1000 | Loss: 0.00000832
Iteration 46/1000 | Loss: 0.00000832
Iteration 47/1000 | Loss: 0.00000832
Iteration 48/1000 | Loss: 0.00000831
Iteration 49/1000 | Loss: 0.00000831
Iteration 50/1000 | Loss: 0.00000831
Iteration 51/1000 | Loss: 0.00000831
Iteration 52/1000 | Loss: 0.00000830
Iteration 53/1000 | Loss: 0.00000830
Iteration 54/1000 | Loss: 0.00000830
Iteration 55/1000 | Loss: 0.00000830
Iteration 56/1000 | Loss: 0.00000830
Iteration 57/1000 | Loss: 0.00000829
Iteration 58/1000 | Loss: 0.00000829
Iteration 59/1000 | Loss: 0.00000828
Iteration 60/1000 | Loss: 0.00000828
Iteration 61/1000 | Loss: 0.00000828
Iteration 62/1000 | Loss: 0.00000828
Iteration 63/1000 | Loss: 0.00000827
Iteration 64/1000 | Loss: 0.00000827
Iteration 65/1000 | Loss: 0.00000827
Iteration 66/1000 | Loss: 0.00000827
Iteration 67/1000 | Loss: 0.00000827
Iteration 68/1000 | Loss: 0.00000826
Iteration 69/1000 | Loss: 0.00000826
Iteration 70/1000 | Loss: 0.00000826
Iteration 71/1000 | Loss: 0.00000825
Iteration 72/1000 | Loss: 0.00000825
Iteration 73/1000 | Loss: 0.00000824
Iteration 74/1000 | Loss: 0.00000824
Iteration 75/1000 | Loss: 0.00000823
Iteration 76/1000 | Loss: 0.00000823
Iteration 77/1000 | Loss: 0.00000823
Iteration 78/1000 | Loss: 0.00000822
Iteration 79/1000 | Loss: 0.00000821
Iteration 80/1000 | Loss: 0.00000821
Iteration 81/1000 | Loss: 0.00000820
Iteration 82/1000 | Loss: 0.00000820
Iteration 83/1000 | Loss: 0.00000820
Iteration 84/1000 | Loss: 0.00000819
Iteration 85/1000 | Loss: 0.00000819
Iteration 86/1000 | Loss: 0.00000819
Iteration 87/1000 | Loss: 0.00000818
Iteration 88/1000 | Loss: 0.00000818
Iteration 89/1000 | Loss: 0.00000817
Iteration 90/1000 | Loss: 0.00000817
Iteration 91/1000 | Loss: 0.00000817
Iteration 92/1000 | Loss: 0.00000816
Iteration 93/1000 | Loss: 0.00000816
Iteration 94/1000 | Loss: 0.00000816
Iteration 95/1000 | Loss: 0.00000815
Iteration 96/1000 | Loss: 0.00000815
Iteration 97/1000 | Loss: 0.00000815
Iteration 98/1000 | Loss: 0.00000815
Iteration 99/1000 | Loss: 0.00000815
Iteration 100/1000 | Loss: 0.00000814
Iteration 101/1000 | Loss: 0.00000814
Iteration 102/1000 | Loss: 0.00000814
Iteration 103/1000 | Loss: 0.00000814
Iteration 104/1000 | Loss: 0.00000814
Iteration 105/1000 | Loss: 0.00000813
Iteration 106/1000 | Loss: 0.00000813
Iteration 107/1000 | Loss: 0.00000813
Iteration 108/1000 | Loss: 0.00000813
Iteration 109/1000 | Loss: 0.00000813
Iteration 110/1000 | Loss: 0.00000813
Iteration 111/1000 | Loss: 0.00000813
Iteration 112/1000 | Loss: 0.00000813
Iteration 113/1000 | Loss: 0.00000813
Iteration 114/1000 | Loss: 0.00000813
Iteration 115/1000 | Loss: 0.00000813
Iteration 116/1000 | Loss: 0.00000813
Iteration 117/1000 | Loss: 0.00000813
Iteration 118/1000 | Loss: 0.00000812
Iteration 119/1000 | Loss: 0.00000812
Iteration 120/1000 | Loss: 0.00000812
Iteration 121/1000 | Loss: 0.00000812
Iteration 122/1000 | Loss: 0.00000812
Iteration 123/1000 | Loss: 0.00000812
Iteration 124/1000 | Loss: 0.00000812
Iteration 125/1000 | Loss: 0.00000812
Iteration 126/1000 | Loss: 0.00000812
Iteration 127/1000 | Loss: 0.00000812
Iteration 128/1000 | Loss: 0.00000812
Iteration 129/1000 | Loss: 0.00000812
Iteration 130/1000 | Loss: 0.00000812
Iteration 131/1000 | Loss: 0.00000811
Iteration 132/1000 | Loss: 0.00000811
Iteration 133/1000 | Loss: 0.00000811
Iteration 134/1000 | Loss: 0.00000811
Iteration 135/1000 | Loss: 0.00000811
Iteration 136/1000 | Loss: 0.00000811
Iteration 137/1000 | Loss: 0.00000811
Iteration 138/1000 | Loss: 0.00000811
Iteration 139/1000 | Loss: 0.00000811
Iteration 140/1000 | Loss: 0.00000810
Iteration 141/1000 | Loss: 0.00000810
Iteration 142/1000 | Loss: 0.00000810
Iteration 143/1000 | Loss: 0.00000810
Iteration 144/1000 | Loss: 0.00000810
Iteration 145/1000 | Loss: 0.00000810
Iteration 146/1000 | Loss: 0.00000810
Iteration 147/1000 | Loss: 0.00000810
Iteration 148/1000 | Loss: 0.00000810
Iteration 149/1000 | Loss: 0.00000810
Iteration 150/1000 | Loss: 0.00000810
Iteration 151/1000 | Loss: 0.00000810
Iteration 152/1000 | Loss: 0.00000810
Iteration 153/1000 | Loss: 0.00000809
Iteration 154/1000 | Loss: 0.00000809
Iteration 155/1000 | Loss: 0.00000809
Iteration 156/1000 | Loss: 0.00000809
Iteration 157/1000 | Loss: 0.00000808
Iteration 158/1000 | Loss: 0.00000808
Iteration 159/1000 | Loss: 0.00000808
Iteration 160/1000 | Loss: 0.00000808
Iteration 161/1000 | Loss: 0.00000808
Iteration 162/1000 | Loss: 0.00000808
Iteration 163/1000 | Loss: 0.00000808
Iteration 164/1000 | Loss: 0.00000808
Iteration 165/1000 | Loss: 0.00000808
Iteration 166/1000 | Loss: 0.00000808
Iteration 167/1000 | Loss: 0.00000808
Iteration 168/1000 | Loss: 0.00000808
Iteration 169/1000 | Loss: 0.00000807
Iteration 170/1000 | Loss: 0.00000807
Iteration 171/1000 | Loss: 0.00000807
Iteration 172/1000 | Loss: 0.00000807
Iteration 173/1000 | Loss: 0.00000807
Iteration 174/1000 | Loss: 0.00000807
Iteration 175/1000 | Loss: 0.00000806
Iteration 176/1000 | Loss: 0.00000806
Iteration 177/1000 | Loss: 0.00000806
Iteration 178/1000 | Loss: 0.00000805
Iteration 179/1000 | Loss: 0.00000805
Iteration 180/1000 | Loss: 0.00000805
Iteration 181/1000 | Loss: 0.00000805
Iteration 182/1000 | Loss: 0.00000805
Iteration 183/1000 | Loss: 0.00000805
Iteration 184/1000 | Loss: 0.00000805
Iteration 185/1000 | Loss: 0.00000805
Iteration 186/1000 | Loss: 0.00000805
Iteration 187/1000 | Loss: 0.00000805
Iteration 188/1000 | Loss: 0.00000805
Iteration 189/1000 | Loss: 0.00000805
Iteration 190/1000 | Loss: 0.00000805
Iteration 191/1000 | Loss: 0.00000805
Iteration 192/1000 | Loss: 0.00000805
Iteration 193/1000 | Loss: 0.00000804
Iteration 194/1000 | Loss: 0.00000804
Iteration 195/1000 | Loss: 0.00000804
Iteration 196/1000 | Loss: 0.00000804
Iteration 197/1000 | Loss: 0.00000804
Iteration 198/1000 | Loss: 0.00000804
Iteration 199/1000 | Loss: 0.00000804
Iteration 200/1000 | Loss: 0.00000804
Iteration 201/1000 | Loss: 0.00000804
Iteration 202/1000 | Loss: 0.00000804
Iteration 203/1000 | Loss: 0.00000804
Iteration 204/1000 | Loss: 0.00000804
Iteration 205/1000 | Loss: 0.00000804
Iteration 206/1000 | Loss: 0.00000803
Iteration 207/1000 | Loss: 0.00000803
Iteration 208/1000 | Loss: 0.00000803
Iteration 209/1000 | Loss: 0.00000803
Iteration 210/1000 | Loss: 0.00000803
Iteration 211/1000 | Loss: 0.00000803
Iteration 212/1000 | Loss: 0.00000803
Iteration 213/1000 | Loss: 0.00000803
Iteration 214/1000 | Loss: 0.00000803
Iteration 215/1000 | Loss: 0.00000803
Iteration 216/1000 | Loss: 0.00000803
Iteration 217/1000 | Loss: 0.00000803
Iteration 218/1000 | Loss: 0.00000803
Iteration 219/1000 | Loss: 0.00000803
Iteration 220/1000 | Loss: 0.00000803
Iteration 221/1000 | Loss: 0.00000803
Iteration 222/1000 | Loss: 0.00000803
Iteration 223/1000 | Loss: 0.00000803
Iteration 224/1000 | Loss: 0.00000803
Iteration 225/1000 | Loss: 0.00000803
Iteration 226/1000 | Loss: 0.00000803
Iteration 227/1000 | Loss: 0.00000803
Iteration 228/1000 | Loss: 0.00000803
Iteration 229/1000 | Loss: 0.00000803
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [8.027148396649864e-06, 8.027148396649864e-06, 8.027148396649864e-06, 8.027148396649864e-06, 8.027148396649864e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.027148396649864e-06

Optimization complete. Final v2v error: 2.447978973388672 mm

Highest mean error: 2.8316731452941895 mm for frame 120

Lowest mean error: 2.267502784729004 mm for frame 156

Saving results

Total time: 41.12991762161255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489595
Iteration 2/25 | Loss: 0.00133774
Iteration 3/25 | Loss: 0.00119632
Iteration 4/25 | Loss: 0.00117983
Iteration 5/25 | Loss: 0.00117609
Iteration 6/25 | Loss: 0.00117550
Iteration 7/25 | Loss: 0.00117550
Iteration 8/25 | Loss: 0.00117550
Iteration 9/25 | Loss: 0.00117550
Iteration 10/25 | Loss: 0.00117550
Iteration 11/25 | Loss: 0.00117550
Iteration 12/25 | Loss: 0.00117550
Iteration 13/25 | Loss: 0.00117550
Iteration 14/25 | Loss: 0.00117550
Iteration 15/25 | Loss: 0.00117550
Iteration 16/25 | Loss: 0.00117550
Iteration 17/25 | Loss: 0.00117550
Iteration 18/25 | Loss: 0.00117550
Iteration 19/25 | Loss: 0.00117550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011754968436434865, 0.0011754968436434865, 0.0011754968436434865, 0.0011754968436434865, 0.0011754968436434865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011754968436434865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38103628
Iteration 2/25 | Loss: 0.00075912
Iteration 3/25 | Loss: 0.00075910
Iteration 4/25 | Loss: 0.00075910
Iteration 5/25 | Loss: 0.00075910
Iteration 6/25 | Loss: 0.00075910
Iteration 7/25 | Loss: 0.00075910
Iteration 8/25 | Loss: 0.00075910
Iteration 9/25 | Loss: 0.00075910
Iteration 10/25 | Loss: 0.00075910
Iteration 11/25 | Loss: 0.00075910
Iteration 12/25 | Loss: 0.00075910
Iteration 13/25 | Loss: 0.00075910
Iteration 14/25 | Loss: 0.00075910
Iteration 15/25 | Loss: 0.00075910
Iteration 16/25 | Loss: 0.00075910
Iteration 17/25 | Loss: 0.00075910
Iteration 18/25 | Loss: 0.00075910
Iteration 19/25 | Loss: 0.00075910
Iteration 20/25 | Loss: 0.00075910
Iteration 21/25 | Loss: 0.00075910
Iteration 22/25 | Loss: 0.00075910
Iteration 23/25 | Loss: 0.00075910
Iteration 24/25 | Loss: 0.00075910
Iteration 25/25 | Loss: 0.00075910

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075910
Iteration 2/1000 | Loss: 0.00003809
Iteration 3/1000 | Loss: 0.00002572
Iteration 4/1000 | Loss: 0.00002318
Iteration 5/1000 | Loss: 0.00002182
Iteration 6/1000 | Loss: 0.00002107
Iteration 7/1000 | Loss: 0.00002055
Iteration 8/1000 | Loss: 0.00002015
Iteration 9/1000 | Loss: 0.00001988
Iteration 10/1000 | Loss: 0.00001975
Iteration 11/1000 | Loss: 0.00001963
Iteration 12/1000 | Loss: 0.00001960
Iteration 13/1000 | Loss: 0.00001957
Iteration 14/1000 | Loss: 0.00001955
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001948
Iteration 17/1000 | Loss: 0.00001947
Iteration 18/1000 | Loss: 0.00001946
Iteration 19/1000 | Loss: 0.00001945
Iteration 20/1000 | Loss: 0.00001945
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001943
Iteration 23/1000 | Loss: 0.00001942
Iteration 24/1000 | Loss: 0.00001941
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001939
Iteration 27/1000 | Loss: 0.00001938
Iteration 28/1000 | Loss: 0.00001937
Iteration 29/1000 | Loss: 0.00001936
Iteration 30/1000 | Loss: 0.00001936
Iteration 31/1000 | Loss: 0.00001935
Iteration 32/1000 | Loss: 0.00001934
Iteration 33/1000 | Loss: 0.00001933
Iteration 34/1000 | Loss: 0.00001933
Iteration 35/1000 | Loss: 0.00001933
Iteration 36/1000 | Loss: 0.00001932
Iteration 37/1000 | Loss: 0.00001930
Iteration 38/1000 | Loss: 0.00001930
Iteration 39/1000 | Loss: 0.00001929
Iteration 40/1000 | Loss: 0.00001928
Iteration 41/1000 | Loss: 0.00001928
Iteration 42/1000 | Loss: 0.00001928
Iteration 43/1000 | Loss: 0.00001928
Iteration 44/1000 | Loss: 0.00001928
Iteration 45/1000 | Loss: 0.00001928
Iteration 46/1000 | Loss: 0.00001927
Iteration 47/1000 | Loss: 0.00001927
Iteration 48/1000 | Loss: 0.00001926
Iteration 49/1000 | Loss: 0.00001926
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00001925
Iteration 52/1000 | Loss: 0.00001925
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001924
Iteration 55/1000 | Loss: 0.00001924
Iteration 56/1000 | Loss: 0.00001924
Iteration 57/1000 | Loss: 0.00001924
Iteration 58/1000 | Loss: 0.00001924
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001922
Iteration 61/1000 | Loss: 0.00001922
Iteration 62/1000 | Loss: 0.00001922
Iteration 63/1000 | Loss: 0.00001922
Iteration 64/1000 | Loss: 0.00001922
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001921
Iteration 67/1000 | Loss: 0.00001921
Iteration 68/1000 | Loss: 0.00001921
Iteration 69/1000 | Loss: 0.00001920
Iteration 70/1000 | Loss: 0.00001920
Iteration 71/1000 | Loss: 0.00001920
Iteration 72/1000 | Loss: 0.00001920
Iteration 73/1000 | Loss: 0.00001920
Iteration 74/1000 | Loss: 0.00001919
Iteration 75/1000 | Loss: 0.00001919
Iteration 76/1000 | Loss: 0.00001919
Iteration 77/1000 | Loss: 0.00001919
Iteration 78/1000 | Loss: 0.00001919
Iteration 79/1000 | Loss: 0.00001918
Iteration 80/1000 | Loss: 0.00001918
Iteration 81/1000 | Loss: 0.00001918
Iteration 82/1000 | Loss: 0.00001918
Iteration 83/1000 | Loss: 0.00001917
Iteration 84/1000 | Loss: 0.00001917
Iteration 85/1000 | Loss: 0.00001917
Iteration 86/1000 | Loss: 0.00001916
Iteration 87/1000 | Loss: 0.00001916
Iteration 88/1000 | Loss: 0.00001916
Iteration 89/1000 | Loss: 0.00001916
Iteration 90/1000 | Loss: 0.00001916
Iteration 91/1000 | Loss: 0.00001916
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001916
Iteration 94/1000 | Loss: 0.00001916
Iteration 95/1000 | Loss: 0.00001916
Iteration 96/1000 | Loss: 0.00001916
Iteration 97/1000 | Loss: 0.00001915
Iteration 98/1000 | Loss: 0.00001915
Iteration 99/1000 | Loss: 0.00001915
Iteration 100/1000 | Loss: 0.00001914
Iteration 101/1000 | Loss: 0.00001914
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001914
Iteration 106/1000 | Loss: 0.00001914
Iteration 107/1000 | Loss: 0.00001914
Iteration 108/1000 | Loss: 0.00001914
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001913
Iteration 111/1000 | Loss: 0.00001913
Iteration 112/1000 | Loss: 0.00001913
Iteration 113/1000 | Loss: 0.00001912
Iteration 114/1000 | Loss: 0.00001912
Iteration 115/1000 | Loss: 0.00001912
Iteration 116/1000 | Loss: 0.00001912
Iteration 117/1000 | Loss: 0.00001912
Iteration 118/1000 | Loss: 0.00001912
Iteration 119/1000 | Loss: 0.00001912
Iteration 120/1000 | Loss: 0.00001912
Iteration 121/1000 | Loss: 0.00001912
Iteration 122/1000 | Loss: 0.00001912
Iteration 123/1000 | Loss: 0.00001912
Iteration 124/1000 | Loss: 0.00001911
Iteration 125/1000 | Loss: 0.00001911
Iteration 126/1000 | Loss: 0.00001911
Iteration 127/1000 | Loss: 0.00001911
Iteration 128/1000 | Loss: 0.00001911
Iteration 129/1000 | Loss: 0.00001911
Iteration 130/1000 | Loss: 0.00001911
Iteration 131/1000 | Loss: 0.00001911
Iteration 132/1000 | Loss: 0.00001911
Iteration 133/1000 | Loss: 0.00001911
Iteration 134/1000 | Loss: 0.00001911
Iteration 135/1000 | Loss: 0.00001910
Iteration 136/1000 | Loss: 0.00001910
Iteration 137/1000 | Loss: 0.00001910
Iteration 138/1000 | Loss: 0.00001910
Iteration 139/1000 | Loss: 0.00001910
Iteration 140/1000 | Loss: 0.00001910
Iteration 141/1000 | Loss: 0.00001910
Iteration 142/1000 | Loss: 0.00001909
Iteration 143/1000 | Loss: 0.00001909
Iteration 144/1000 | Loss: 0.00001909
Iteration 145/1000 | Loss: 0.00001909
Iteration 146/1000 | Loss: 0.00001909
Iteration 147/1000 | Loss: 0.00001909
Iteration 148/1000 | Loss: 0.00001909
Iteration 149/1000 | Loss: 0.00001909
Iteration 150/1000 | Loss: 0.00001909
Iteration 151/1000 | Loss: 0.00001909
Iteration 152/1000 | Loss: 0.00001908
Iteration 153/1000 | Loss: 0.00001908
Iteration 154/1000 | Loss: 0.00001908
Iteration 155/1000 | Loss: 0.00001908
Iteration 156/1000 | Loss: 0.00001908
Iteration 157/1000 | Loss: 0.00001908
Iteration 158/1000 | Loss: 0.00001908
Iteration 159/1000 | Loss: 0.00001908
Iteration 160/1000 | Loss: 0.00001908
Iteration 161/1000 | Loss: 0.00001907
Iteration 162/1000 | Loss: 0.00001907
Iteration 163/1000 | Loss: 0.00001907
Iteration 164/1000 | Loss: 0.00001907
Iteration 165/1000 | Loss: 0.00001907
Iteration 166/1000 | Loss: 0.00001907
Iteration 167/1000 | Loss: 0.00001907
Iteration 168/1000 | Loss: 0.00001907
Iteration 169/1000 | Loss: 0.00001907
Iteration 170/1000 | Loss: 0.00001906
Iteration 171/1000 | Loss: 0.00001906
Iteration 172/1000 | Loss: 0.00001906
Iteration 173/1000 | Loss: 0.00001906
Iteration 174/1000 | Loss: 0.00001906
Iteration 175/1000 | Loss: 0.00001906
Iteration 176/1000 | Loss: 0.00001906
Iteration 177/1000 | Loss: 0.00001906
Iteration 178/1000 | Loss: 0.00001906
Iteration 179/1000 | Loss: 0.00001905
Iteration 180/1000 | Loss: 0.00001905
Iteration 181/1000 | Loss: 0.00001905
Iteration 182/1000 | Loss: 0.00001905
Iteration 183/1000 | Loss: 0.00001905
Iteration 184/1000 | Loss: 0.00001905
Iteration 185/1000 | Loss: 0.00001905
Iteration 186/1000 | Loss: 0.00001905
Iteration 187/1000 | Loss: 0.00001904
Iteration 188/1000 | Loss: 0.00001904
Iteration 189/1000 | Loss: 0.00001904
Iteration 190/1000 | Loss: 0.00001904
Iteration 191/1000 | Loss: 0.00001904
Iteration 192/1000 | Loss: 0.00001904
Iteration 193/1000 | Loss: 0.00001904
Iteration 194/1000 | Loss: 0.00001904
Iteration 195/1000 | Loss: 0.00001904
Iteration 196/1000 | Loss: 0.00001904
Iteration 197/1000 | Loss: 0.00001904
Iteration 198/1000 | Loss: 0.00001904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.9043161955778487e-05, 1.9043161955778487e-05, 1.9043161955778487e-05, 1.9043161955778487e-05, 1.9043161955778487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9043161955778487e-05

Optimization complete. Final v2v error: 3.533907651901245 mm

Highest mean error: 4.164698123931885 mm for frame 132

Lowest mean error: 3.026735305786133 mm for frame 2

Saving results

Total time: 40.08837270736694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093125
Iteration 2/25 | Loss: 0.00210794
Iteration 3/25 | Loss: 0.00171621
Iteration 4/25 | Loss: 0.00158135
Iteration 5/25 | Loss: 0.00164705
Iteration 6/25 | Loss: 0.00148927
Iteration 7/25 | Loss: 0.00143305
Iteration 8/25 | Loss: 0.00140268
Iteration 9/25 | Loss: 0.00139091
Iteration 10/25 | Loss: 0.00138434
Iteration 11/25 | Loss: 0.00137968
Iteration 12/25 | Loss: 0.00137156
Iteration 13/25 | Loss: 0.00137146
Iteration 14/25 | Loss: 0.00136386
Iteration 15/25 | Loss: 0.00135735
Iteration 16/25 | Loss: 0.00135535
Iteration 17/25 | Loss: 0.00135486
Iteration 18/25 | Loss: 0.00135341
Iteration 19/25 | Loss: 0.00135368
Iteration 20/25 | Loss: 0.00136019
Iteration 21/25 | Loss: 0.00135305
Iteration 22/25 | Loss: 0.00135350
Iteration 23/25 | Loss: 0.00135543
Iteration 24/25 | Loss: 0.00134270
Iteration 25/25 | Loss: 0.00134474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99999291
Iteration 2/25 | Loss: 0.00179814
Iteration 3/25 | Loss: 0.00179813
Iteration 4/25 | Loss: 0.00179813
Iteration 5/25 | Loss: 0.00179813
Iteration 6/25 | Loss: 0.00179813
Iteration 7/25 | Loss: 0.00179813
Iteration 8/25 | Loss: 0.00179813
Iteration 9/25 | Loss: 0.00179813
Iteration 10/25 | Loss: 0.00179813
Iteration 11/25 | Loss: 0.00179813
Iteration 12/25 | Loss: 0.00179813
Iteration 13/25 | Loss: 0.00179813
Iteration 14/25 | Loss: 0.00179813
Iteration 15/25 | Loss: 0.00179813
Iteration 16/25 | Loss: 0.00179813
Iteration 17/25 | Loss: 0.00179813
Iteration 18/25 | Loss: 0.00179813
Iteration 19/25 | Loss: 0.00179813
Iteration 20/25 | Loss: 0.00179813
Iteration 21/25 | Loss: 0.00179813
Iteration 22/25 | Loss: 0.00179813
Iteration 23/25 | Loss: 0.00179813
Iteration 24/25 | Loss: 0.00179813
Iteration 25/25 | Loss: 0.00179813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00179813
Iteration 2/1000 | Loss: 0.00085619
Iteration 3/1000 | Loss: 0.00044690
Iteration 4/1000 | Loss: 0.00073022
Iteration 5/1000 | Loss: 0.00066640
Iteration 6/1000 | Loss: 0.00058745
Iteration 7/1000 | Loss: 0.00046359
Iteration 8/1000 | Loss: 0.00027337
Iteration 9/1000 | Loss: 0.00048910
Iteration 10/1000 | Loss: 0.00033993
Iteration 11/1000 | Loss: 0.00055681
Iteration 12/1000 | Loss: 0.00035236
Iteration 13/1000 | Loss: 0.00054930
Iteration 14/1000 | Loss: 0.00049886
Iteration 15/1000 | Loss: 0.00053667
Iteration 16/1000 | Loss: 0.00048732
Iteration 17/1000 | Loss: 0.00045151
Iteration 18/1000 | Loss: 0.00048313
Iteration 19/1000 | Loss: 0.00058784
Iteration 20/1000 | Loss: 0.00034377
Iteration 21/1000 | Loss: 0.00046471
Iteration 22/1000 | Loss: 0.00056021
Iteration 23/1000 | Loss: 0.00048513
Iteration 24/1000 | Loss: 0.00044372
Iteration 25/1000 | Loss: 0.00040868
Iteration 26/1000 | Loss: 0.00035032
Iteration 27/1000 | Loss: 0.00030717
Iteration 28/1000 | Loss: 0.00029172
Iteration 29/1000 | Loss: 0.00031394
Iteration 30/1000 | Loss: 0.00019914
Iteration 31/1000 | Loss: 0.00030870
Iteration 32/1000 | Loss: 0.00027550
Iteration 33/1000 | Loss: 0.00062882
Iteration 34/1000 | Loss: 0.00025293
Iteration 35/1000 | Loss: 0.00032340
Iteration 36/1000 | Loss: 0.00029429
Iteration 37/1000 | Loss: 0.00056454
Iteration 38/1000 | Loss: 0.00052833
Iteration 39/1000 | Loss: 0.00048950
Iteration 40/1000 | Loss: 0.00050116
Iteration 41/1000 | Loss: 0.00052406
Iteration 42/1000 | Loss: 0.00043015
Iteration 43/1000 | Loss: 0.00057092
Iteration 44/1000 | Loss: 0.00021842
Iteration 45/1000 | Loss: 0.00045096
Iteration 46/1000 | Loss: 0.00053800
Iteration 47/1000 | Loss: 0.00048357
Iteration 48/1000 | Loss: 0.00092626
Iteration 49/1000 | Loss: 0.00074216
Iteration 50/1000 | Loss: 0.00055742
Iteration 51/1000 | Loss: 0.00077567
Iteration 52/1000 | Loss: 0.00078330
Iteration 53/1000 | Loss: 0.00124913
Iteration 54/1000 | Loss: 0.00045686
Iteration 55/1000 | Loss: 0.00100850
Iteration 56/1000 | Loss: 0.00058487
Iteration 57/1000 | Loss: 0.00081102
Iteration 58/1000 | Loss: 0.00041473
Iteration 59/1000 | Loss: 0.00047463
Iteration 60/1000 | Loss: 0.00071011
Iteration 61/1000 | Loss: 0.00041317
Iteration 62/1000 | Loss: 0.00047746
Iteration 63/1000 | Loss: 0.00075962
Iteration 64/1000 | Loss: 0.00042937
Iteration 65/1000 | Loss: 0.00046277
Iteration 66/1000 | Loss: 0.00049435
Iteration 67/1000 | Loss: 0.00048932
Iteration 68/1000 | Loss: 0.00039234
Iteration 69/1000 | Loss: 0.00036970
Iteration 70/1000 | Loss: 0.00097917
Iteration 71/1000 | Loss: 0.00074175
Iteration 72/1000 | Loss: 0.00128772
Iteration 73/1000 | Loss: 0.00086955
Iteration 74/1000 | Loss: 0.00119549
Iteration 75/1000 | Loss: 0.00073310
Iteration 76/1000 | Loss: 0.00110106
Iteration 77/1000 | Loss: 0.00090146
Iteration 78/1000 | Loss: 0.00054524
Iteration 79/1000 | Loss: 0.00051842
Iteration 80/1000 | Loss: 0.00047166
Iteration 81/1000 | Loss: 0.00050236
Iteration 82/1000 | Loss: 0.00045931
Iteration 83/1000 | Loss: 0.00048975
Iteration 84/1000 | Loss: 0.00033235
Iteration 85/1000 | Loss: 0.00035002
Iteration 86/1000 | Loss: 0.00025010
Iteration 87/1000 | Loss: 0.00032510
Iteration 88/1000 | Loss: 0.00071967
Iteration 89/1000 | Loss: 0.00048752
Iteration 90/1000 | Loss: 0.00049751
Iteration 91/1000 | Loss: 0.00015312
Iteration 92/1000 | Loss: 0.00026175
Iteration 93/1000 | Loss: 0.00021900
Iteration 94/1000 | Loss: 0.00029804
Iteration 95/1000 | Loss: 0.00023589
Iteration 96/1000 | Loss: 0.00013516
Iteration 97/1000 | Loss: 0.00018538
Iteration 98/1000 | Loss: 0.00020173
Iteration 99/1000 | Loss: 0.00019399
Iteration 100/1000 | Loss: 0.00012484
Iteration 101/1000 | Loss: 0.00055023
Iteration 102/1000 | Loss: 0.00036846
Iteration 103/1000 | Loss: 0.00051055
Iteration 104/1000 | Loss: 0.00071635
Iteration 105/1000 | Loss: 0.00130654
Iteration 106/1000 | Loss: 0.00049829
Iteration 107/1000 | Loss: 0.00154385
Iteration 108/1000 | Loss: 0.00141893
Iteration 109/1000 | Loss: 0.00073899
Iteration 110/1000 | Loss: 0.00068634
Iteration 111/1000 | Loss: 0.00053074
Iteration 112/1000 | Loss: 0.00111068
Iteration 113/1000 | Loss: 0.00054305
Iteration 114/1000 | Loss: 0.00102725
Iteration 115/1000 | Loss: 0.00101732
Iteration 116/1000 | Loss: 0.00078264
Iteration 117/1000 | Loss: 0.00089272
Iteration 118/1000 | Loss: 0.00042356
Iteration 119/1000 | Loss: 0.00038869
Iteration 120/1000 | Loss: 0.00031008
Iteration 121/1000 | Loss: 0.00042995
Iteration 122/1000 | Loss: 0.00022153
Iteration 123/1000 | Loss: 0.00066528
Iteration 124/1000 | Loss: 0.00110231
Iteration 125/1000 | Loss: 0.00047598
Iteration 126/1000 | Loss: 0.00056756
Iteration 127/1000 | Loss: 0.00035693
Iteration 128/1000 | Loss: 0.00041689
Iteration 129/1000 | Loss: 0.00077610
Iteration 130/1000 | Loss: 0.00046799
Iteration 131/1000 | Loss: 0.00045912
Iteration 132/1000 | Loss: 0.00065555
Iteration 133/1000 | Loss: 0.00062612
Iteration 134/1000 | Loss: 0.00102516
Iteration 135/1000 | Loss: 0.00121964
Iteration 136/1000 | Loss: 0.00131703
Iteration 137/1000 | Loss: 0.00128297
Iteration 138/1000 | Loss: 0.00051618
Iteration 139/1000 | Loss: 0.00058581
Iteration 140/1000 | Loss: 0.00043683
Iteration 141/1000 | Loss: 0.00043330
Iteration 142/1000 | Loss: 0.00046574
Iteration 143/1000 | Loss: 0.00066117
Iteration 144/1000 | Loss: 0.00069031
Iteration 145/1000 | Loss: 0.00072255
Iteration 146/1000 | Loss: 0.00045845
Iteration 147/1000 | Loss: 0.00012704
Iteration 148/1000 | Loss: 0.00048822
Iteration 149/1000 | Loss: 0.00081524
Iteration 150/1000 | Loss: 0.00052012
Iteration 151/1000 | Loss: 0.00080011
Iteration 152/1000 | Loss: 0.00046129
Iteration 153/1000 | Loss: 0.00080856
Iteration 154/1000 | Loss: 0.00027279
Iteration 155/1000 | Loss: 0.00078544
Iteration 156/1000 | Loss: 0.00101505
Iteration 157/1000 | Loss: 0.00052357
Iteration 158/1000 | Loss: 0.00121424
Iteration 159/1000 | Loss: 0.00097397
Iteration 160/1000 | Loss: 0.00053206
Iteration 161/1000 | Loss: 0.00055243
Iteration 162/1000 | Loss: 0.00060106
Iteration 163/1000 | Loss: 0.00038836
Iteration 164/1000 | Loss: 0.00044904
Iteration 165/1000 | Loss: 0.00076622
Iteration 166/1000 | Loss: 0.00098836
Iteration 167/1000 | Loss: 0.00038219
Iteration 168/1000 | Loss: 0.00076330
Iteration 169/1000 | Loss: 0.00054670
Iteration 170/1000 | Loss: 0.00035700
Iteration 171/1000 | Loss: 0.00066992
Iteration 172/1000 | Loss: 0.00066933
Iteration 173/1000 | Loss: 0.00060439
Iteration 174/1000 | Loss: 0.00072549
Iteration 175/1000 | Loss: 0.00054388
Iteration 176/1000 | Loss: 0.00024628
Iteration 177/1000 | Loss: 0.00030026
Iteration 178/1000 | Loss: 0.00093666
Iteration 179/1000 | Loss: 0.00061859
Iteration 180/1000 | Loss: 0.00017816
Iteration 181/1000 | Loss: 0.00024498
Iteration 182/1000 | Loss: 0.00014577
Iteration 183/1000 | Loss: 0.00016823
Iteration 184/1000 | Loss: 0.00034349
Iteration 185/1000 | Loss: 0.00053962
Iteration 186/1000 | Loss: 0.00027280
Iteration 187/1000 | Loss: 0.00022048
Iteration 188/1000 | Loss: 0.00038595
Iteration 189/1000 | Loss: 0.00028541
Iteration 190/1000 | Loss: 0.00016116
Iteration 191/1000 | Loss: 0.00022248
Iteration 192/1000 | Loss: 0.00018748
Iteration 193/1000 | Loss: 0.00025047
Iteration 194/1000 | Loss: 0.00051366
Iteration 195/1000 | Loss: 0.00040302
Iteration 196/1000 | Loss: 0.00070834
Iteration 197/1000 | Loss: 0.00045818
Iteration 198/1000 | Loss: 0.00054062
Iteration 199/1000 | Loss: 0.00054468
Iteration 200/1000 | Loss: 0.00056081
Iteration 201/1000 | Loss: 0.00048135
Iteration 202/1000 | Loss: 0.00087931
Iteration 203/1000 | Loss: 0.00095827
Iteration 204/1000 | Loss: 0.00080373
Iteration 205/1000 | Loss: 0.00044284
Iteration 206/1000 | Loss: 0.00055771
Iteration 207/1000 | Loss: 0.00030610
Iteration 208/1000 | Loss: 0.00034922
Iteration 209/1000 | Loss: 0.00014238
Iteration 210/1000 | Loss: 0.00022368
Iteration 211/1000 | Loss: 0.00029480
Iteration 212/1000 | Loss: 0.00020268
Iteration 213/1000 | Loss: 0.00032754
Iteration 214/1000 | Loss: 0.00025973
Iteration 215/1000 | Loss: 0.00019663
Iteration 216/1000 | Loss: 0.00021057
Iteration 217/1000 | Loss: 0.00019558
Iteration 218/1000 | Loss: 0.00013991
Iteration 219/1000 | Loss: 0.00044398
Iteration 220/1000 | Loss: 0.00038666
Iteration 221/1000 | Loss: 0.00061847
Iteration 222/1000 | Loss: 0.00035817
Iteration 223/1000 | Loss: 0.00047845
Iteration 224/1000 | Loss: 0.00036570
Iteration 225/1000 | Loss: 0.00033612
Iteration 226/1000 | Loss: 0.00050843
Iteration 227/1000 | Loss: 0.00052777
Iteration 228/1000 | Loss: 0.00072489
Iteration 229/1000 | Loss: 0.00057667
Iteration 230/1000 | Loss: 0.00053113
Iteration 231/1000 | Loss: 0.00023271
Iteration 232/1000 | Loss: 0.00038619
Iteration 233/1000 | Loss: 0.00043944
Iteration 234/1000 | Loss: 0.00045432
Iteration 235/1000 | Loss: 0.00020192
Iteration 236/1000 | Loss: 0.00020183
Iteration 237/1000 | Loss: 0.00020312
Iteration 238/1000 | Loss: 0.00037890
Iteration 239/1000 | Loss: 0.00018039
Iteration 240/1000 | Loss: 0.00029703
Iteration 241/1000 | Loss: 0.00027627
Iteration 242/1000 | Loss: 0.00023709
Iteration 243/1000 | Loss: 0.00029417
Iteration 244/1000 | Loss: 0.00029118
Iteration 245/1000 | Loss: 0.00027089
Iteration 246/1000 | Loss: 0.00036764
Iteration 247/1000 | Loss: 0.00077509
Iteration 248/1000 | Loss: 0.00063761
Iteration 249/1000 | Loss: 0.00027130
Iteration 250/1000 | Loss: 0.00023924
Iteration 251/1000 | Loss: 0.00031046
Iteration 252/1000 | Loss: 0.00014176
Iteration 253/1000 | Loss: 0.00019664
Iteration 254/1000 | Loss: 0.00046757
Iteration 255/1000 | Loss: 0.00049456
Iteration 256/1000 | Loss: 0.00027477
Iteration 257/1000 | Loss: 0.00018442
Iteration 258/1000 | Loss: 0.00023362
Iteration 259/1000 | Loss: 0.00019919
Iteration 260/1000 | Loss: 0.00018265
Iteration 261/1000 | Loss: 0.00018858
Iteration 262/1000 | Loss: 0.00020763
Iteration 263/1000 | Loss: 0.00019285
Iteration 264/1000 | Loss: 0.00036204
Iteration 265/1000 | Loss: 0.00024578
Iteration 266/1000 | Loss: 0.00045441
Iteration 267/1000 | Loss: 0.00034126
Iteration 268/1000 | Loss: 0.00021032
Iteration 269/1000 | Loss: 0.00019142
Iteration 270/1000 | Loss: 0.00020150
Iteration 271/1000 | Loss: 0.00010439
Iteration 272/1000 | Loss: 0.00012335
Iteration 273/1000 | Loss: 0.00009022
Iteration 274/1000 | Loss: 0.00016485
Iteration 275/1000 | Loss: 0.00014914
Iteration 276/1000 | Loss: 0.00008010
Iteration 277/1000 | Loss: 0.00016521
Iteration 278/1000 | Loss: 0.00038944
Iteration 279/1000 | Loss: 0.00065118
Iteration 280/1000 | Loss: 0.00041409
Iteration 281/1000 | Loss: 0.00037902
Iteration 282/1000 | Loss: 0.00053903
Iteration 283/1000 | Loss: 0.00064446
Iteration 284/1000 | Loss: 0.00043944
Iteration 285/1000 | Loss: 0.00035888
Iteration 286/1000 | Loss: 0.00026997
Iteration 287/1000 | Loss: 0.00031309
Iteration 288/1000 | Loss: 0.00024989
Iteration 289/1000 | Loss: 0.00018087
Iteration 290/1000 | Loss: 0.00048349
Iteration 291/1000 | Loss: 0.00013770
Iteration 292/1000 | Loss: 0.00027280
Iteration 293/1000 | Loss: 0.00099159
Iteration 294/1000 | Loss: 0.00077262
Iteration 295/1000 | Loss: 0.00056125
Iteration 296/1000 | Loss: 0.00099100
Iteration 297/1000 | Loss: 0.00074124
Iteration 298/1000 | Loss: 0.00029770
Iteration 299/1000 | Loss: 0.00014075
Iteration 300/1000 | Loss: 0.00032758
Iteration 301/1000 | Loss: 0.00022137
Iteration 302/1000 | Loss: 0.00024265
Iteration 303/1000 | Loss: 0.00020732
Iteration 304/1000 | Loss: 0.00062042
Iteration 305/1000 | Loss: 0.00040267
Iteration 306/1000 | Loss: 0.00024957
Iteration 307/1000 | Loss: 0.00014305
Iteration 308/1000 | Loss: 0.00021723
Iteration 309/1000 | Loss: 0.00027137
Iteration 310/1000 | Loss: 0.00085230
Iteration 311/1000 | Loss: 0.00073688
Iteration 312/1000 | Loss: 0.00021264
Iteration 313/1000 | Loss: 0.00015676
Iteration 314/1000 | Loss: 0.00019206
Iteration 315/1000 | Loss: 0.00018925
Iteration 316/1000 | Loss: 0.00019929
Iteration 317/1000 | Loss: 0.00025046
Iteration 318/1000 | Loss: 0.00029444
Iteration 319/1000 | Loss: 0.00017016
Iteration 320/1000 | Loss: 0.00034225
Iteration 321/1000 | Loss: 0.00034496
Iteration 322/1000 | Loss: 0.00024253
Iteration 323/1000 | Loss: 0.00006150
Iteration 324/1000 | Loss: 0.00010882
Iteration 325/1000 | Loss: 0.00047195
Iteration 326/1000 | Loss: 0.00018670
Iteration 327/1000 | Loss: 0.00039374
Iteration 328/1000 | Loss: 0.00028594
Iteration 329/1000 | Loss: 0.00041090
Iteration 330/1000 | Loss: 0.00032630
Iteration 331/1000 | Loss: 0.00024441
Iteration 332/1000 | Loss: 0.00093174
Iteration 333/1000 | Loss: 0.00066049
Iteration 334/1000 | Loss: 0.00026400
Iteration 335/1000 | Loss: 0.00053394
Iteration 336/1000 | Loss: 0.00035507
Iteration 337/1000 | Loss: 0.00053332
Iteration 338/1000 | Loss: 0.00018736
Iteration 339/1000 | Loss: 0.00048097
Iteration 340/1000 | Loss: 0.00021538
Iteration 341/1000 | Loss: 0.00053056
Iteration 342/1000 | Loss: 0.00022182
Iteration 343/1000 | Loss: 0.00057192
Iteration 344/1000 | Loss: 0.00019152
Iteration 345/1000 | Loss: 0.00005293
Iteration 346/1000 | Loss: 0.00006901
Iteration 347/1000 | Loss: 0.00006324
Iteration 348/1000 | Loss: 0.00005458
Iteration 349/1000 | Loss: 0.00005833
Iteration 350/1000 | Loss: 0.00004969
Iteration 351/1000 | Loss: 0.00005997
Iteration 352/1000 | Loss: 0.00041301
Iteration 353/1000 | Loss: 0.00025286
Iteration 354/1000 | Loss: 0.00033418
Iteration 355/1000 | Loss: 0.00029715
Iteration 356/1000 | Loss: 0.00031388
Iteration 357/1000 | Loss: 0.00005845
Iteration 358/1000 | Loss: 0.00005357
Iteration 359/1000 | Loss: 0.00031687
Iteration 360/1000 | Loss: 0.00035323
Iteration 361/1000 | Loss: 0.00022840
Iteration 362/1000 | Loss: 0.00066462
Iteration 363/1000 | Loss: 0.00073637
Iteration 364/1000 | Loss: 0.00091199
Iteration 365/1000 | Loss: 0.00035807
Iteration 366/1000 | Loss: 0.00014957
Iteration 367/1000 | Loss: 0.00004884
Iteration 368/1000 | Loss: 0.00053760
Iteration 369/1000 | Loss: 0.00017820
Iteration 370/1000 | Loss: 0.00004287
Iteration 371/1000 | Loss: 0.00004016
Iteration 372/1000 | Loss: 0.00045601
Iteration 373/1000 | Loss: 0.00031566
Iteration 374/1000 | Loss: 0.00004695
Iteration 375/1000 | Loss: 0.00004135
Iteration 376/1000 | Loss: 0.00003810
Iteration 377/1000 | Loss: 0.00003740
Iteration 378/1000 | Loss: 0.00118664
Iteration 379/1000 | Loss: 0.00064578
Iteration 380/1000 | Loss: 0.00079576
Iteration 381/1000 | Loss: 0.00055187
Iteration 382/1000 | Loss: 0.00079437
Iteration 383/1000 | Loss: 0.00011887
Iteration 384/1000 | Loss: 0.00006492
Iteration 385/1000 | Loss: 0.00003719
Iteration 386/1000 | Loss: 0.00003480
Iteration 387/1000 | Loss: 0.00003339
Iteration 388/1000 | Loss: 0.00003251
Iteration 389/1000 | Loss: 0.00013955
Iteration 390/1000 | Loss: 0.00036972
Iteration 391/1000 | Loss: 0.00021963
Iteration 392/1000 | Loss: 0.00044627
Iteration 393/1000 | Loss: 0.00028784
Iteration 394/1000 | Loss: 0.00010985
Iteration 395/1000 | Loss: 0.00029410
Iteration 396/1000 | Loss: 0.00036482
Iteration 397/1000 | Loss: 0.00017797
Iteration 398/1000 | Loss: 0.00004359
Iteration 399/1000 | Loss: 0.00015265
Iteration 400/1000 | Loss: 0.00031520
Iteration 401/1000 | Loss: 0.00022421
Iteration 402/1000 | Loss: 0.00003651
Iteration 403/1000 | Loss: 0.00022586
Iteration 404/1000 | Loss: 0.00018318
Iteration 405/1000 | Loss: 0.00010929
Iteration 406/1000 | Loss: 0.00009715
Iteration 407/1000 | Loss: 0.00003510
Iteration 408/1000 | Loss: 0.00012188
Iteration 409/1000 | Loss: 0.00003820
Iteration 410/1000 | Loss: 0.00003453
Iteration 411/1000 | Loss: 0.00003323
Iteration 412/1000 | Loss: 0.00021599
Iteration 413/1000 | Loss: 0.00010668
Iteration 414/1000 | Loss: 0.00010774
Iteration 415/1000 | Loss: 0.00010651
Iteration 416/1000 | Loss: 0.00016258
Iteration 417/1000 | Loss: 0.00015471
Iteration 418/1000 | Loss: 0.00032743
Iteration 419/1000 | Loss: 0.00003632
Iteration 420/1000 | Loss: 0.00006976
Iteration 421/1000 | Loss: 0.00015642
Iteration 422/1000 | Loss: 0.00016814
Iteration 423/1000 | Loss: 0.00003740
Iteration 424/1000 | Loss: 0.00014928
Iteration 425/1000 | Loss: 0.00015243
Iteration 426/1000 | Loss: 0.00003706
Iteration 427/1000 | Loss: 0.00003473
Iteration 428/1000 | Loss: 0.00003292
Iteration 429/1000 | Loss: 0.00004606
Iteration 430/1000 | Loss: 0.00003120
Iteration 431/1000 | Loss: 0.00016781
Iteration 432/1000 | Loss: 0.00003410
Iteration 433/1000 | Loss: 0.00017754
Iteration 434/1000 | Loss: 0.00020653
Iteration 435/1000 | Loss: 0.00016815
Iteration 436/1000 | Loss: 0.00003820
Iteration 437/1000 | Loss: 0.00003403
Iteration 438/1000 | Loss: 0.00017194
Iteration 439/1000 | Loss: 0.00003251
Iteration 440/1000 | Loss: 0.00016599
Iteration 441/1000 | Loss: 0.00031017
Iteration 442/1000 | Loss: 0.00032493
Iteration 443/1000 | Loss: 0.00033049
Iteration 444/1000 | Loss: 0.00041814
Iteration 445/1000 | Loss: 0.00050186
Iteration 446/1000 | Loss: 0.00069041
Iteration 447/1000 | Loss: 0.00073672
Iteration 448/1000 | Loss: 0.00018314
Iteration 449/1000 | Loss: 0.00011215
Iteration 450/1000 | Loss: 0.00007550
Iteration 451/1000 | Loss: 0.00003668
Iteration 452/1000 | Loss: 0.00003299
Iteration 453/1000 | Loss: 0.00003148
Iteration 454/1000 | Loss: 0.00003042
Iteration 455/1000 | Loss: 0.00002962
Iteration 456/1000 | Loss: 0.00002916
Iteration 457/1000 | Loss: 0.00002876
Iteration 458/1000 | Loss: 0.00002848
Iteration 459/1000 | Loss: 0.00003430
Iteration 460/1000 | Loss: 0.00003054
Iteration 461/1000 | Loss: 0.00002985
Iteration 462/1000 | Loss: 0.00002945
Iteration 463/1000 | Loss: 0.00003486
Iteration 464/1000 | Loss: 0.00003119
Iteration 465/1000 | Loss: 0.00003414
Iteration 466/1000 | Loss: 0.00003194
Iteration 467/1000 | Loss: 0.00003324
Iteration 468/1000 | Loss: 0.00003112
Iteration 469/1000 | Loss: 0.00003443
Iteration 470/1000 | Loss: 0.00003201
Iteration 471/1000 | Loss: 0.00003456
Iteration 472/1000 | Loss: 0.00003312
Iteration 473/1000 | Loss: 0.00003455
Iteration 474/1000 | Loss: 0.00003146
Iteration 475/1000 | Loss: 0.00003107
Iteration 476/1000 | Loss: 0.00003085
Iteration 477/1000 | Loss: 0.00066270
Iteration 478/1000 | Loss: 0.00033496
Iteration 479/1000 | Loss: 0.00045664
Iteration 480/1000 | Loss: 0.00070529
Iteration 481/1000 | Loss: 0.00056640
Iteration 482/1000 | Loss: 0.00065265
Iteration 483/1000 | Loss: 0.00057337
Iteration 484/1000 | Loss: 0.00072419
Iteration 485/1000 | Loss: 0.00065395
Iteration 486/1000 | Loss: 0.00134000
Iteration 487/1000 | Loss: 0.00076511
Iteration 488/1000 | Loss: 0.00015271
Iteration 489/1000 | Loss: 0.00018244
Iteration 490/1000 | Loss: 0.00012706
Iteration 491/1000 | Loss: 0.00008663
Iteration 492/1000 | Loss: 0.00014318
Iteration 493/1000 | Loss: 0.00012772
Iteration 494/1000 | Loss: 0.00006221
Iteration 495/1000 | Loss: 0.00013259
Iteration 496/1000 | Loss: 0.00015218
Iteration 497/1000 | Loss: 0.00015707
Iteration 498/1000 | Loss: 0.00014931
Iteration 499/1000 | Loss: 0.00015110
Iteration 500/1000 | Loss: 0.00009410
Iteration 501/1000 | Loss: 0.00015933
Iteration 502/1000 | Loss: 0.00009560
Iteration 503/1000 | Loss: 0.00011022
Iteration 504/1000 | Loss: 0.00008623
Iteration 505/1000 | Loss: 0.00011260
Iteration 506/1000 | Loss: 0.00005262
Iteration 507/1000 | Loss: 0.00005320
Iteration 508/1000 | Loss: 0.00014049
Iteration 509/1000 | Loss: 0.00013827
Iteration 510/1000 | Loss: 0.00014671
Iteration 511/1000 | Loss: 0.00012955
Iteration 512/1000 | Loss: 0.00014779
Iteration 513/1000 | Loss: 0.00006943
Iteration 514/1000 | Loss: 0.00017166
Iteration 515/1000 | Loss: 0.00012622
Iteration 516/1000 | Loss: 0.00012356
Iteration 517/1000 | Loss: 0.00003425
Iteration 518/1000 | Loss: 0.00003151
Iteration 519/1000 | Loss: 0.00002965
Iteration 520/1000 | Loss: 0.00002826
Iteration 521/1000 | Loss: 0.00002767
Iteration 522/1000 | Loss: 0.00002675
Iteration 523/1000 | Loss: 0.00002604
Iteration 524/1000 | Loss: 0.00002567
Iteration 525/1000 | Loss: 0.00002545
Iteration 526/1000 | Loss: 0.00002507
Iteration 527/1000 | Loss: 0.00002480
Iteration 528/1000 | Loss: 0.00002468
Iteration 529/1000 | Loss: 0.00002467
Iteration 530/1000 | Loss: 0.00002466
Iteration 531/1000 | Loss: 0.00002463
Iteration 532/1000 | Loss: 0.00002458
Iteration 533/1000 | Loss: 0.00002452
Iteration 534/1000 | Loss: 0.00002450
Iteration 535/1000 | Loss: 0.00002449
Iteration 536/1000 | Loss: 0.00002449
Iteration 537/1000 | Loss: 0.00002448
Iteration 538/1000 | Loss: 0.00002448
Iteration 539/1000 | Loss: 0.00002448
Iteration 540/1000 | Loss: 0.00002448
Iteration 541/1000 | Loss: 0.00002448
Iteration 542/1000 | Loss: 0.00002448
Iteration 543/1000 | Loss: 0.00002448
Iteration 544/1000 | Loss: 0.00002448
Iteration 545/1000 | Loss: 0.00002448
Iteration 546/1000 | Loss: 0.00002448
Iteration 547/1000 | Loss: 0.00002447
Iteration 548/1000 | Loss: 0.00002447
Iteration 549/1000 | Loss: 0.00002443
Iteration 550/1000 | Loss: 0.00002443
Iteration 551/1000 | Loss: 0.00002442
Iteration 552/1000 | Loss: 0.00002442
Iteration 553/1000 | Loss: 0.00002438
Iteration 554/1000 | Loss: 0.00002433
Iteration 555/1000 | Loss: 0.00002432
Iteration 556/1000 | Loss: 0.00002432
Iteration 557/1000 | Loss: 0.00002431
Iteration 558/1000 | Loss: 0.00002430
Iteration 559/1000 | Loss: 0.00002430
Iteration 560/1000 | Loss: 0.00002429
Iteration 561/1000 | Loss: 0.00002429
Iteration 562/1000 | Loss: 0.00002429
Iteration 563/1000 | Loss: 0.00002429
Iteration 564/1000 | Loss: 0.00002428
Iteration 565/1000 | Loss: 0.00002428
Iteration 566/1000 | Loss: 0.00002427
Iteration 567/1000 | Loss: 0.00002427
Iteration 568/1000 | Loss: 0.00002427
Iteration 569/1000 | Loss: 0.00002425
Iteration 570/1000 | Loss: 0.00003462
Iteration 571/1000 | Loss: 0.00003462
Iteration 572/1000 | Loss: 0.00003098
Iteration 573/1000 | Loss: 0.00002863
Iteration 574/1000 | Loss: 0.00002664
Iteration 575/1000 | Loss: 0.00002455
Iteration 576/1000 | Loss: 0.00002367
Iteration 577/1000 | Loss: 0.00002329
Iteration 578/1000 | Loss: 0.00002320
Iteration 579/1000 | Loss: 0.00002314
Iteration 580/1000 | Loss: 0.00002314
Iteration 581/1000 | Loss: 0.00002312
Iteration 582/1000 | Loss: 0.00002310
Iteration 583/1000 | Loss: 0.00002309
Iteration 584/1000 | Loss: 0.00002301
Iteration 585/1000 | Loss: 0.00002299
Iteration 586/1000 | Loss: 0.00002298
Iteration 587/1000 | Loss: 0.00002297
Iteration 588/1000 | Loss: 0.00002297
Iteration 589/1000 | Loss: 0.00002297
Iteration 590/1000 | Loss: 0.00002297
Iteration 591/1000 | Loss: 0.00002297
Iteration 592/1000 | Loss: 0.00002297
Iteration 593/1000 | Loss: 0.00002297
Iteration 594/1000 | Loss: 0.00002297
Iteration 595/1000 | Loss: 0.00002297
Iteration 596/1000 | Loss: 0.00002297
Iteration 597/1000 | Loss: 0.00002297
Iteration 598/1000 | Loss: 0.00002297
Iteration 599/1000 | Loss: 0.00002296
Iteration 600/1000 | Loss: 0.00002296
Iteration 601/1000 | Loss: 0.00002296
Iteration 602/1000 | Loss: 0.00002296
Iteration 603/1000 | Loss: 0.00002295
Iteration 604/1000 | Loss: 0.00002295
Iteration 605/1000 | Loss: 0.00002294
Iteration 606/1000 | Loss: 0.00002294
Iteration 607/1000 | Loss: 0.00002294
Iteration 608/1000 | Loss: 0.00002294
Iteration 609/1000 | Loss: 0.00002294
Iteration 610/1000 | Loss: 0.00002294
Iteration 611/1000 | Loss: 0.00002293
Iteration 612/1000 | Loss: 0.00002293
Iteration 613/1000 | Loss: 0.00002293
Iteration 614/1000 | Loss: 0.00002293
Iteration 615/1000 | Loss: 0.00002292
Iteration 616/1000 | Loss: 0.00002292
Iteration 617/1000 | Loss: 0.00002292
Iteration 618/1000 | Loss: 0.00002292
Iteration 619/1000 | Loss: 0.00002292
Iteration 620/1000 | Loss: 0.00002292
Iteration 621/1000 | Loss: 0.00002292
Iteration 622/1000 | Loss: 0.00002292
Iteration 623/1000 | Loss: 0.00002292
Iteration 624/1000 | Loss: 0.00002292
Iteration 625/1000 | Loss: 0.00002292
Iteration 626/1000 | Loss: 0.00002292
Iteration 627/1000 | Loss: 0.00002292
Iteration 628/1000 | Loss: 0.00002292
Iteration 629/1000 | Loss: 0.00002292
Iteration 630/1000 | Loss: 0.00002292
Iteration 631/1000 | Loss: 0.00002292
Iteration 632/1000 | Loss: 0.00002292
Iteration 633/1000 | Loss: 0.00002292
Iteration 634/1000 | Loss: 0.00002292
Iteration 635/1000 | Loss: 0.00002292
Iteration 636/1000 | Loss: 0.00002292
Iteration 637/1000 | Loss: 0.00002292
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 637. Stopping optimization.
Last 5 losses: [2.2918935428606346e-05, 2.2918935428606346e-05, 2.2918935428606346e-05, 2.2918935428606346e-05, 2.2918935428606346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2918935428606346e-05

Optimization complete. Final v2v error: 3.9285366535186768 mm

Highest mean error: 5.90408182144165 mm for frame 143

Lowest mean error: 3.244117498397827 mm for frame 22

Saving results

Total time: 883.6591832637787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976069
Iteration 2/25 | Loss: 0.00380209
Iteration 3/25 | Loss: 0.00239764
Iteration 4/25 | Loss: 0.00222570
Iteration 5/25 | Loss: 0.00214307
Iteration 6/25 | Loss: 0.00213307
Iteration 7/25 | Loss: 0.00204933
Iteration 8/25 | Loss: 0.00201695
Iteration 9/25 | Loss: 0.00189274
Iteration 10/25 | Loss: 0.00188578
Iteration 11/25 | Loss: 0.00186499
Iteration 12/25 | Loss: 0.00184416
Iteration 13/25 | Loss: 0.00183287
Iteration 14/25 | Loss: 0.00182675
Iteration 15/25 | Loss: 0.00181969
Iteration 16/25 | Loss: 0.00182185
Iteration 17/25 | Loss: 0.00182496
Iteration 18/25 | Loss: 0.00182132
Iteration 19/25 | Loss: 0.00181434
Iteration 20/25 | Loss: 0.00181264
Iteration 21/25 | Loss: 0.00181095
Iteration 22/25 | Loss: 0.00181364
Iteration 23/25 | Loss: 0.00180862
Iteration 24/25 | Loss: 0.00180805
Iteration 25/25 | Loss: 0.00180780

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34779239
Iteration 2/25 | Loss: 0.00377535
Iteration 3/25 | Loss: 0.00377535
Iteration 4/25 | Loss: 0.00377534
Iteration 5/25 | Loss: 0.00377534
Iteration 6/25 | Loss: 0.00377534
Iteration 7/25 | Loss: 0.00377534
Iteration 8/25 | Loss: 0.00377534
Iteration 9/25 | Loss: 0.00377534
Iteration 10/25 | Loss: 0.00377534
Iteration 11/25 | Loss: 0.00377534
Iteration 12/25 | Loss: 0.00377534
Iteration 13/25 | Loss: 0.00377534
Iteration 14/25 | Loss: 0.00377534
Iteration 15/25 | Loss: 0.00377534
Iteration 16/25 | Loss: 0.00377534
Iteration 17/25 | Loss: 0.00377534
Iteration 18/25 | Loss: 0.00377534
Iteration 19/25 | Loss: 0.00377534
Iteration 20/25 | Loss: 0.00377534
Iteration 21/25 | Loss: 0.00377534
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0037753430660814047, 0.0037753430660814047, 0.0037753430660814047, 0.0037753430660814047, 0.0037753430660814047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037753430660814047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00377534
Iteration 2/1000 | Loss: 0.00134770
Iteration 3/1000 | Loss: 0.00102939
Iteration 4/1000 | Loss: 0.00121509
Iteration 5/1000 | Loss: 0.00121088
Iteration 6/1000 | Loss: 0.00096195
Iteration 7/1000 | Loss: 0.00059084
Iteration 8/1000 | Loss: 0.00063304
Iteration 9/1000 | Loss: 0.00043996
Iteration 10/1000 | Loss: 0.00038585
Iteration 11/1000 | Loss: 0.00060226
Iteration 12/1000 | Loss: 0.00043734
Iteration 13/1000 | Loss: 0.00033385
Iteration 14/1000 | Loss: 0.00071869
Iteration 15/1000 | Loss: 0.00130512
Iteration 16/1000 | Loss: 0.00559497
Iteration 17/1000 | Loss: 0.00674164
Iteration 18/1000 | Loss: 0.00058561
Iteration 19/1000 | Loss: 0.00029954
Iteration 20/1000 | Loss: 0.00026698
Iteration 21/1000 | Loss: 0.00022609
Iteration 22/1000 | Loss: 0.00016464
Iteration 23/1000 | Loss: 0.00018920
Iteration 24/1000 | Loss: 0.00024317
Iteration 25/1000 | Loss: 0.00011883
Iteration 26/1000 | Loss: 0.00015053
Iteration 27/1000 | Loss: 0.00006224
Iteration 28/1000 | Loss: 0.00005048
Iteration 29/1000 | Loss: 0.00004020
Iteration 30/1000 | Loss: 0.00003377
Iteration 31/1000 | Loss: 0.00002893
Iteration 32/1000 | Loss: 0.00002548
Iteration 33/1000 | Loss: 0.00002291
Iteration 34/1000 | Loss: 0.00002123
Iteration 35/1000 | Loss: 0.00002003
Iteration 36/1000 | Loss: 0.00001922
Iteration 37/1000 | Loss: 0.00001858
Iteration 38/1000 | Loss: 0.00001807
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001735
Iteration 41/1000 | Loss: 0.00001713
Iteration 42/1000 | Loss: 0.00001706
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001692
Iteration 45/1000 | Loss: 0.00001692
Iteration 46/1000 | Loss: 0.00001691
Iteration 47/1000 | Loss: 0.00001688
Iteration 48/1000 | Loss: 0.00001684
Iteration 49/1000 | Loss: 0.00001684
Iteration 50/1000 | Loss: 0.00001679
Iteration 51/1000 | Loss: 0.00001679
Iteration 52/1000 | Loss: 0.00001678
Iteration 53/1000 | Loss: 0.00001678
Iteration 54/1000 | Loss: 0.00001678
Iteration 55/1000 | Loss: 0.00001677
Iteration 56/1000 | Loss: 0.00001677
Iteration 57/1000 | Loss: 0.00001676
Iteration 58/1000 | Loss: 0.00001676
Iteration 59/1000 | Loss: 0.00001676
Iteration 60/1000 | Loss: 0.00001676
Iteration 61/1000 | Loss: 0.00001676
Iteration 62/1000 | Loss: 0.00001676
Iteration 63/1000 | Loss: 0.00001676
Iteration 64/1000 | Loss: 0.00001676
Iteration 65/1000 | Loss: 0.00001675
Iteration 66/1000 | Loss: 0.00001675
Iteration 67/1000 | Loss: 0.00001675
Iteration 68/1000 | Loss: 0.00001675
Iteration 69/1000 | Loss: 0.00001675
Iteration 70/1000 | Loss: 0.00001675
Iteration 71/1000 | Loss: 0.00001673
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001672
Iteration 74/1000 | Loss: 0.00001672
Iteration 75/1000 | Loss: 0.00001671
Iteration 76/1000 | Loss: 0.00001671
Iteration 77/1000 | Loss: 0.00001671
Iteration 78/1000 | Loss: 0.00001671
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001669
Iteration 82/1000 | Loss: 0.00001669
Iteration 83/1000 | Loss: 0.00001668
Iteration 84/1000 | Loss: 0.00001668
Iteration 85/1000 | Loss: 0.00001667
Iteration 86/1000 | Loss: 0.00001667
Iteration 87/1000 | Loss: 0.00001666
Iteration 88/1000 | Loss: 0.00001666
Iteration 89/1000 | Loss: 0.00001666
Iteration 90/1000 | Loss: 0.00001666
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001665
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001664
Iteration 95/1000 | Loss: 0.00001664
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001663
Iteration 98/1000 | Loss: 0.00001663
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001662
Iteration 102/1000 | Loss: 0.00001662
Iteration 103/1000 | Loss: 0.00001662
Iteration 104/1000 | Loss: 0.00001662
Iteration 105/1000 | Loss: 0.00001661
Iteration 106/1000 | Loss: 0.00001661
Iteration 107/1000 | Loss: 0.00001661
Iteration 108/1000 | Loss: 0.00001661
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001660
Iteration 112/1000 | Loss: 0.00001660
Iteration 113/1000 | Loss: 0.00001659
Iteration 114/1000 | Loss: 0.00001659
Iteration 115/1000 | Loss: 0.00001659
Iteration 116/1000 | Loss: 0.00001659
Iteration 117/1000 | Loss: 0.00001659
Iteration 118/1000 | Loss: 0.00001659
Iteration 119/1000 | Loss: 0.00001659
Iteration 120/1000 | Loss: 0.00001659
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001658
Iteration 123/1000 | Loss: 0.00001658
Iteration 124/1000 | Loss: 0.00001658
Iteration 125/1000 | Loss: 0.00001658
Iteration 126/1000 | Loss: 0.00001658
Iteration 127/1000 | Loss: 0.00001658
Iteration 128/1000 | Loss: 0.00001658
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001658
Iteration 134/1000 | Loss: 0.00001658
Iteration 135/1000 | Loss: 0.00001658
Iteration 136/1000 | Loss: 0.00001657
Iteration 137/1000 | Loss: 0.00001657
Iteration 138/1000 | Loss: 0.00001657
Iteration 139/1000 | Loss: 0.00001657
Iteration 140/1000 | Loss: 0.00001657
Iteration 141/1000 | Loss: 0.00001657
Iteration 142/1000 | Loss: 0.00001657
Iteration 143/1000 | Loss: 0.00001657
Iteration 144/1000 | Loss: 0.00001657
Iteration 145/1000 | Loss: 0.00001657
Iteration 146/1000 | Loss: 0.00001657
Iteration 147/1000 | Loss: 0.00001657
Iteration 148/1000 | Loss: 0.00001656
Iteration 149/1000 | Loss: 0.00001656
Iteration 150/1000 | Loss: 0.00001656
Iteration 151/1000 | Loss: 0.00001656
Iteration 152/1000 | Loss: 0.00001656
Iteration 153/1000 | Loss: 0.00001656
Iteration 154/1000 | Loss: 0.00001656
Iteration 155/1000 | Loss: 0.00001656
Iteration 156/1000 | Loss: 0.00001656
Iteration 157/1000 | Loss: 0.00001656
Iteration 158/1000 | Loss: 0.00001656
Iteration 159/1000 | Loss: 0.00001656
Iteration 160/1000 | Loss: 0.00001656
Iteration 161/1000 | Loss: 0.00001656
Iteration 162/1000 | Loss: 0.00001656
Iteration 163/1000 | Loss: 0.00001656
Iteration 164/1000 | Loss: 0.00001656
Iteration 165/1000 | Loss: 0.00001656
Iteration 166/1000 | Loss: 0.00001656
Iteration 167/1000 | Loss: 0.00001655
Iteration 168/1000 | Loss: 0.00001655
Iteration 169/1000 | Loss: 0.00001655
Iteration 170/1000 | Loss: 0.00001655
Iteration 171/1000 | Loss: 0.00001655
Iteration 172/1000 | Loss: 0.00001655
Iteration 173/1000 | Loss: 0.00001655
Iteration 174/1000 | Loss: 0.00001655
Iteration 175/1000 | Loss: 0.00001655
Iteration 176/1000 | Loss: 0.00001655
Iteration 177/1000 | Loss: 0.00001655
Iteration 178/1000 | Loss: 0.00001655
Iteration 179/1000 | Loss: 0.00001654
Iteration 180/1000 | Loss: 0.00001654
Iteration 181/1000 | Loss: 0.00001654
Iteration 182/1000 | Loss: 0.00001654
Iteration 183/1000 | Loss: 0.00001654
Iteration 184/1000 | Loss: 0.00001654
Iteration 185/1000 | Loss: 0.00001654
Iteration 186/1000 | Loss: 0.00001654
Iteration 187/1000 | Loss: 0.00001654
Iteration 188/1000 | Loss: 0.00001654
Iteration 189/1000 | Loss: 0.00001654
Iteration 190/1000 | Loss: 0.00001654
Iteration 191/1000 | Loss: 0.00001654
Iteration 192/1000 | Loss: 0.00001654
Iteration 193/1000 | Loss: 0.00001654
Iteration 194/1000 | Loss: 0.00001654
Iteration 195/1000 | Loss: 0.00001654
Iteration 196/1000 | Loss: 0.00001654
Iteration 197/1000 | Loss: 0.00001654
Iteration 198/1000 | Loss: 0.00001654
Iteration 199/1000 | Loss: 0.00001654
Iteration 200/1000 | Loss: 0.00001654
Iteration 201/1000 | Loss: 0.00001654
Iteration 202/1000 | Loss: 0.00001654
Iteration 203/1000 | Loss: 0.00001654
Iteration 204/1000 | Loss: 0.00001654
Iteration 205/1000 | Loss: 0.00001654
Iteration 206/1000 | Loss: 0.00001654
Iteration 207/1000 | Loss: 0.00001654
Iteration 208/1000 | Loss: 0.00001654
Iteration 209/1000 | Loss: 0.00001654
Iteration 210/1000 | Loss: 0.00001654
Iteration 211/1000 | Loss: 0.00001654
Iteration 212/1000 | Loss: 0.00001654
Iteration 213/1000 | Loss: 0.00001654
Iteration 214/1000 | Loss: 0.00001654
Iteration 215/1000 | Loss: 0.00001654
Iteration 216/1000 | Loss: 0.00001654
Iteration 217/1000 | Loss: 0.00001654
Iteration 218/1000 | Loss: 0.00001654
Iteration 219/1000 | Loss: 0.00001654
Iteration 220/1000 | Loss: 0.00001654
Iteration 221/1000 | Loss: 0.00001654
Iteration 222/1000 | Loss: 0.00001654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.6544712707400322e-05, 1.6544712707400322e-05, 1.6544712707400322e-05, 1.6544712707400322e-05, 1.6544712707400322e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6544712707400322e-05

Optimization complete. Final v2v error: 3.2957842350006104 mm

Highest mean error: 11.394901275634766 mm for frame 96

Lowest mean error: 3.0553338527679443 mm for frame 2

Saving results

Total time: 135.60761547088623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00490830
Iteration 2/25 | Loss: 0.00152557
Iteration 3/25 | Loss: 0.00119487
Iteration 4/25 | Loss: 0.00114585
Iteration 5/25 | Loss: 0.00113886
Iteration 6/25 | Loss: 0.00113689
Iteration 7/25 | Loss: 0.00113667
Iteration 8/25 | Loss: 0.00113667
Iteration 9/25 | Loss: 0.00113667
Iteration 10/25 | Loss: 0.00113667
Iteration 11/25 | Loss: 0.00113667
Iteration 12/25 | Loss: 0.00113667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011366676772013307, 0.0011366676772013307, 0.0011366676772013307, 0.0011366676772013307, 0.0011366676772013307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011366676772013307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35319078
Iteration 2/25 | Loss: 0.00057638
Iteration 3/25 | Loss: 0.00057638
Iteration 4/25 | Loss: 0.00057638
Iteration 5/25 | Loss: 0.00057638
Iteration 6/25 | Loss: 0.00057638
Iteration 7/25 | Loss: 0.00057638
Iteration 8/25 | Loss: 0.00057638
Iteration 9/25 | Loss: 0.00057638
Iteration 10/25 | Loss: 0.00057638
Iteration 11/25 | Loss: 0.00057638
Iteration 12/25 | Loss: 0.00057638
Iteration 13/25 | Loss: 0.00057638
Iteration 14/25 | Loss: 0.00057638
Iteration 15/25 | Loss: 0.00057638
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005763775552622974, 0.0005763775552622974, 0.0005763775552622974, 0.0005763775552622974, 0.0005763775552622974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005763775552622974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057638
Iteration 2/1000 | Loss: 0.00005184
Iteration 3/1000 | Loss: 0.00002454
Iteration 4/1000 | Loss: 0.00001821
Iteration 5/1000 | Loss: 0.00001705
Iteration 6/1000 | Loss: 0.00001641
Iteration 7/1000 | Loss: 0.00001595
Iteration 8/1000 | Loss: 0.00001571
Iteration 9/1000 | Loss: 0.00001570
Iteration 10/1000 | Loss: 0.00001557
Iteration 11/1000 | Loss: 0.00001556
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001542
Iteration 14/1000 | Loss: 0.00001541
Iteration 15/1000 | Loss: 0.00001541
Iteration 16/1000 | Loss: 0.00001537
Iteration 17/1000 | Loss: 0.00001535
Iteration 18/1000 | Loss: 0.00001534
Iteration 19/1000 | Loss: 0.00001534
Iteration 20/1000 | Loss: 0.00001530
Iteration 21/1000 | Loss: 0.00001530
Iteration 22/1000 | Loss: 0.00001529
Iteration 23/1000 | Loss: 0.00001529
Iteration 24/1000 | Loss: 0.00001528
Iteration 25/1000 | Loss: 0.00001527
Iteration 26/1000 | Loss: 0.00001527
Iteration 27/1000 | Loss: 0.00001526
Iteration 28/1000 | Loss: 0.00001526
Iteration 29/1000 | Loss: 0.00001526
Iteration 30/1000 | Loss: 0.00001525
Iteration 31/1000 | Loss: 0.00001525
Iteration 32/1000 | Loss: 0.00001525
Iteration 33/1000 | Loss: 0.00001525
Iteration 34/1000 | Loss: 0.00001524
Iteration 35/1000 | Loss: 0.00001523
Iteration 36/1000 | Loss: 0.00001522
Iteration 37/1000 | Loss: 0.00001521
Iteration 38/1000 | Loss: 0.00001521
Iteration 39/1000 | Loss: 0.00001521
Iteration 40/1000 | Loss: 0.00001517
Iteration 41/1000 | Loss: 0.00001514
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001512
Iteration 46/1000 | Loss: 0.00001512
Iteration 47/1000 | Loss: 0.00001512
Iteration 48/1000 | Loss: 0.00001512
Iteration 49/1000 | Loss: 0.00001511
Iteration 50/1000 | Loss: 0.00001511
Iteration 51/1000 | Loss: 0.00001511
Iteration 52/1000 | Loss: 0.00001511
Iteration 53/1000 | Loss: 0.00001511
Iteration 54/1000 | Loss: 0.00001511
Iteration 55/1000 | Loss: 0.00001511
Iteration 56/1000 | Loss: 0.00001510
Iteration 57/1000 | Loss: 0.00001510
Iteration 58/1000 | Loss: 0.00001510
Iteration 59/1000 | Loss: 0.00001510
Iteration 60/1000 | Loss: 0.00001510
Iteration 61/1000 | Loss: 0.00001510
Iteration 62/1000 | Loss: 0.00001509
Iteration 63/1000 | Loss: 0.00001509
Iteration 64/1000 | Loss: 0.00001509
Iteration 65/1000 | Loss: 0.00001508
Iteration 66/1000 | Loss: 0.00001508
Iteration 67/1000 | Loss: 0.00001508
Iteration 68/1000 | Loss: 0.00001508
Iteration 69/1000 | Loss: 0.00001508
Iteration 70/1000 | Loss: 0.00001507
Iteration 71/1000 | Loss: 0.00001507
Iteration 72/1000 | Loss: 0.00001507
Iteration 73/1000 | Loss: 0.00001506
Iteration 74/1000 | Loss: 0.00001506
Iteration 75/1000 | Loss: 0.00001506
Iteration 76/1000 | Loss: 0.00001506
Iteration 77/1000 | Loss: 0.00001505
Iteration 78/1000 | Loss: 0.00001504
Iteration 79/1000 | Loss: 0.00001504
Iteration 80/1000 | Loss: 0.00001504
Iteration 81/1000 | Loss: 0.00001504
Iteration 82/1000 | Loss: 0.00001504
Iteration 83/1000 | Loss: 0.00001504
Iteration 84/1000 | Loss: 0.00001504
Iteration 85/1000 | Loss: 0.00001504
Iteration 86/1000 | Loss: 0.00001503
Iteration 87/1000 | Loss: 0.00001503
Iteration 88/1000 | Loss: 0.00001503
Iteration 89/1000 | Loss: 0.00001503
Iteration 90/1000 | Loss: 0.00001503
Iteration 91/1000 | Loss: 0.00001503
Iteration 92/1000 | Loss: 0.00001503
Iteration 93/1000 | Loss: 0.00001503
Iteration 94/1000 | Loss: 0.00001503
Iteration 95/1000 | Loss: 0.00001503
Iteration 96/1000 | Loss: 0.00001503
Iteration 97/1000 | Loss: 0.00001502
Iteration 98/1000 | Loss: 0.00001502
Iteration 99/1000 | Loss: 0.00001502
Iteration 100/1000 | Loss: 0.00001502
Iteration 101/1000 | Loss: 0.00001502
Iteration 102/1000 | Loss: 0.00001502
Iteration 103/1000 | Loss: 0.00001502
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001501
Iteration 106/1000 | Loss: 0.00001501
Iteration 107/1000 | Loss: 0.00001501
Iteration 108/1000 | Loss: 0.00001501
Iteration 109/1000 | Loss: 0.00001501
Iteration 110/1000 | Loss: 0.00001501
Iteration 111/1000 | Loss: 0.00001501
Iteration 112/1000 | Loss: 0.00001501
Iteration 113/1000 | Loss: 0.00001501
Iteration 114/1000 | Loss: 0.00001501
Iteration 115/1000 | Loss: 0.00001500
Iteration 116/1000 | Loss: 0.00001500
Iteration 117/1000 | Loss: 0.00001500
Iteration 118/1000 | Loss: 0.00001500
Iteration 119/1000 | Loss: 0.00001499
Iteration 120/1000 | Loss: 0.00001499
Iteration 121/1000 | Loss: 0.00001499
Iteration 122/1000 | Loss: 0.00001499
Iteration 123/1000 | Loss: 0.00001499
Iteration 124/1000 | Loss: 0.00001499
Iteration 125/1000 | Loss: 0.00001498
Iteration 126/1000 | Loss: 0.00001498
Iteration 127/1000 | Loss: 0.00001498
Iteration 128/1000 | Loss: 0.00001498
Iteration 129/1000 | Loss: 0.00001498
Iteration 130/1000 | Loss: 0.00001498
Iteration 131/1000 | Loss: 0.00001498
Iteration 132/1000 | Loss: 0.00001497
Iteration 133/1000 | Loss: 0.00001497
Iteration 134/1000 | Loss: 0.00001497
Iteration 135/1000 | Loss: 0.00001497
Iteration 136/1000 | Loss: 0.00001496
Iteration 137/1000 | Loss: 0.00001496
Iteration 138/1000 | Loss: 0.00001496
Iteration 139/1000 | Loss: 0.00001496
Iteration 140/1000 | Loss: 0.00001496
Iteration 141/1000 | Loss: 0.00001496
Iteration 142/1000 | Loss: 0.00001496
Iteration 143/1000 | Loss: 0.00001496
Iteration 144/1000 | Loss: 0.00001496
Iteration 145/1000 | Loss: 0.00001496
Iteration 146/1000 | Loss: 0.00001495
Iteration 147/1000 | Loss: 0.00001495
Iteration 148/1000 | Loss: 0.00001495
Iteration 149/1000 | Loss: 0.00001495
Iteration 150/1000 | Loss: 0.00001495
Iteration 151/1000 | Loss: 0.00001495
Iteration 152/1000 | Loss: 0.00001495
Iteration 153/1000 | Loss: 0.00001495
Iteration 154/1000 | Loss: 0.00001495
Iteration 155/1000 | Loss: 0.00001495
Iteration 156/1000 | Loss: 0.00001495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.4953249774407595e-05, 1.4953249774407595e-05, 1.4953249774407595e-05, 1.4953249774407595e-05, 1.4953249774407595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4953249774407595e-05

Optimization complete. Final v2v error: 3.349541425704956 mm

Highest mean error: 3.4711525440216064 mm for frame 45

Lowest mean error: 3.0301923751831055 mm for frame 2

Saving results

Total time: 35.979327917099
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979765
Iteration 2/25 | Loss: 0.00362179
Iteration 3/25 | Loss: 0.00228853
Iteration 4/25 | Loss: 0.00208334
Iteration 5/25 | Loss: 0.00179579
Iteration 6/25 | Loss: 0.00181943
Iteration 7/25 | Loss: 0.00165986
Iteration 8/25 | Loss: 0.00160393
Iteration 9/25 | Loss: 0.00154582
Iteration 10/25 | Loss: 0.00148566
Iteration 11/25 | Loss: 0.00146293
Iteration 12/25 | Loss: 0.00144106
Iteration 13/25 | Loss: 0.00142614
Iteration 14/25 | Loss: 0.00142609
Iteration 15/25 | Loss: 0.00141187
Iteration 16/25 | Loss: 0.00139041
Iteration 17/25 | Loss: 0.00137866
Iteration 18/25 | Loss: 0.00136930
Iteration 19/25 | Loss: 0.00137241
Iteration 20/25 | Loss: 0.00136936
Iteration 21/25 | Loss: 0.00135674
Iteration 22/25 | Loss: 0.00135298
Iteration 23/25 | Loss: 0.00134983
Iteration 24/25 | Loss: 0.00134789
Iteration 25/25 | Loss: 0.00135998

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36871576
Iteration 2/25 | Loss: 0.00372102
Iteration 3/25 | Loss: 0.00284238
Iteration 4/25 | Loss: 0.00284234
Iteration 5/25 | Loss: 0.00284233
Iteration 6/25 | Loss: 0.00284233
Iteration 7/25 | Loss: 0.00284233
Iteration 8/25 | Loss: 0.00284233
Iteration 9/25 | Loss: 0.00284233
Iteration 10/25 | Loss: 0.00284233
Iteration 11/25 | Loss: 0.00284233
Iteration 12/25 | Loss: 0.00284233
Iteration 13/25 | Loss: 0.00284233
Iteration 14/25 | Loss: 0.00284233
Iteration 15/25 | Loss: 0.00284233
Iteration 16/25 | Loss: 0.00284233
Iteration 17/25 | Loss: 0.00284233
Iteration 18/25 | Loss: 0.00284233
Iteration 19/25 | Loss: 0.00284233
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0028423310723155737, 0.0028423310723155737, 0.0028423310723155737, 0.0028423310723155737, 0.0028423310723155737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028423310723155737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00284233
Iteration 2/1000 | Loss: 0.00514394
Iteration 3/1000 | Loss: 0.00234782
Iteration 4/1000 | Loss: 0.00120024
Iteration 5/1000 | Loss: 0.00207358
Iteration 6/1000 | Loss: 0.00022965
Iteration 7/1000 | Loss: 0.00091011
Iteration 8/1000 | Loss: 0.00022509
Iteration 9/1000 | Loss: 0.00206153
Iteration 10/1000 | Loss: 0.00021996
Iteration 11/1000 | Loss: 0.00202628
Iteration 12/1000 | Loss: 0.00046806
Iteration 13/1000 | Loss: 0.00038995
Iteration 14/1000 | Loss: 0.00213225
Iteration 15/1000 | Loss: 0.00014298
Iteration 16/1000 | Loss: 0.00033541
Iteration 17/1000 | Loss: 0.00012671
Iteration 18/1000 | Loss: 0.00013358
Iteration 19/1000 | Loss: 0.00030702
Iteration 20/1000 | Loss: 0.00090934
Iteration 21/1000 | Loss: 0.00021391
Iteration 22/1000 | Loss: 0.00009678
Iteration 23/1000 | Loss: 0.00014932
Iteration 24/1000 | Loss: 0.00009385
Iteration 25/1000 | Loss: 0.00018595
Iteration 26/1000 | Loss: 0.00024379
Iteration 27/1000 | Loss: 0.00013589
Iteration 28/1000 | Loss: 0.00059818
Iteration 29/1000 | Loss: 0.00008544
Iteration 30/1000 | Loss: 0.00006497
Iteration 31/1000 | Loss: 0.00006224
Iteration 32/1000 | Loss: 0.00059969
Iteration 33/1000 | Loss: 0.00062981
Iteration 34/1000 | Loss: 0.00016316
Iteration 35/1000 | Loss: 0.00010345
Iteration 36/1000 | Loss: 0.00030395
Iteration 37/1000 | Loss: 0.00013196
Iteration 38/1000 | Loss: 0.00008424
Iteration 39/1000 | Loss: 0.00006781
Iteration 40/1000 | Loss: 0.00013008
Iteration 41/1000 | Loss: 0.00010002
Iteration 42/1000 | Loss: 0.00011059
Iteration 43/1000 | Loss: 0.00038552
Iteration 44/1000 | Loss: 0.00007706
Iteration 45/1000 | Loss: 0.00005090
Iteration 46/1000 | Loss: 0.00005012
Iteration 47/1000 | Loss: 0.00007694
Iteration 48/1000 | Loss: 0.00006714
Iteration 49/1000 | Loss: 0.00053299
Iteration 50/1000 | Loss: 0.00014961
Iteration 51/1000 | Loss: 0.00043129
Iteration 52/1000 | Loss: 0.00038715
Iteration 53/1000 | Loss: 0.00019660
Iteration 54/1000 | Loss: 0.00005951
Iteration 55/1000 | Loss: 0.00025948
Iteration 56/1000 | Loss: 0.00006991
Iteration 57/1000 | Loss: 0.00005318
Iteration 58/1000 | Loss: 0.00005819
Iteration 59/1000 | Loss: 0.00007511
Iteration 60/1000 | Loss: 0.00024493
Iteration 61/1000 | Loss: 0.00092742
Iteration 62/1000 | Loss: 0.00026672
Iteration 63/1000 | Loss: 0.00005163
Iteration 64/1000 | Loss: 0.00004745
Iteration 65/1000 | Loss: 0.00005949
Iteration 66/1000 | Loss: 0.00004961
Iteration 67/1000 | Loss: 0.00004770
Iteration 68/1000 | Loss: 0.00065284
Iteration 69/1000 | Loss: 0.00044031
Iteration 70/1000 | Loss: 0.00022631
Iteration 71/1000 | Loss: 0.00024305
Iteration 72/1000 | Loss: 0.00015565
Iteration 73/1000 | Loss: 0.00005081
Iteration 74/1000 | Loss: 0.00004294
Iteration 75/1000 | Loss: 0.00008251
Iteration 76/1000 | Loss: 0.00015353
Iteration 77/1000 | Loss: 0.00005417
Iteration 78/1000 | Loss: 0.00004363
Iteration 79/1000 | Loss: 0.00006257
Iteration 80/1000 | Loss: 0.00033556
Iteration 81/1000 | Loss: 0.00021977
Iteration 82/1000 | Loss: 0.00027694
Iteration 83/1000 | Loss: 0.00011860
Iteration 84/1000 | Loss: 0.00017252
Iteration 85/1000 | Loss: 0.00024605
Iteration 86/1000 | Loss: 0.00029289
Iteration 87/1000 | Loss: 0.00004550
Iteration 88/1000 | Loss: 0.00004217
Iteration 89/1000 | Loss: 0.00005829
Iteration 90/1000 | Loss: 0.00005208
Iteration 91/1000 | Loss: 0.00006730
Iteration 92/1000 | Loss: 0.00052797
Iteration 93/1000 | Loss: 0.00121427
Iteration 94/1000 | Loss: 0.00059187
Iteration 95/1000 | Loss: 0.00035562
Iteration 96/1000 | Loss: 0.00004731
Iteration 97/1000 | Loss: 0.00004371
Iteration 98/1000 | Loss: 0.00004855
Iteration 99/1000 | Loss: 0.00003838
Iteration 100/1000 | Loss: 0.00012175
Iteration 101/1000 | Loss: 0.00004508
Iteration 102/1000 | Loss: 0.00008855
Iteration 103/1000 | Loss: 0.00003790
Iteration 104/1000 | Loss: 0.00003790
Iteration 105/1000 | Loss: 0.00003789
Iteration 106/1000 | Loss: 0.00003789
Iteration 107/1000 | Loss: 0.00003788
Iteration 108/1000 | Loss: 0.00003788
Iteration 109/1000 | Loss: 0.00006575
Iteration 110/1000 | Loss: 0.00003810
Iteration 111/1000 | Loss: 0.00006316
Iteration 112/1000 | Loss: 0.00034258
Iteration 113/1000 | Loss: 0.00007088
Iteration 114/1000 | Loss: 0.00004347
Iteration 115/1000 | Loss: 0.00003980
Iteration 116/1000 | Loss: 0.00007281
Iteration 117/1000 | Loss: 0.00009864
Iteration 118/1000 | Loss: 0.00009774
Iteration 119/1000 | Loss: 0.00004552
Iteration 120/1000 | Loss: 0.00004970
Iteration 121/1000 | Loss: 0.00003882
Iteration 122/1000 | Loss: 0.00003873
Iteration 123/1000 | Loss: 0.00003872
Iteration 124/1000 | Loss: 0.00003871
Iteration 125/1000 | Loss: 0.00003871
Iteration 126/1000 | Loss: 0.00003870
Iteration 127/1000 | Loss: 0.00003870
Iteration 128/1000 | Loss: 0.00003868
Iteration 129/1000 | Loss: 0.00006344
Iteration 130/1000 | Loss: 0.00010187
Iteration 131/1000 | Loss: 0.00012522
Iteration 132/1000 | Loss: 0.00003928
Iteration 133/1000 | Loss: 0.00004424
Iteration 134/1000 | Loss: 0.00006779
Iteration 135/1000 | Loss: 0.00003855
Iteration 136/1000 | Loss: 0.00004833
Iteration 137/1000 | Loss: 0.00003846
Iteration 138/1000 | Loss: 0.00003846
Iteration 139/1000 | Loss: 0.00003846
Iteration 140/1000 | Loss: 0.00003846
Iteration 141/1000 | Loss: 0.00003846
Iteration 142/1000 | Loss: 0.00003845
Iteration 143/1000 | Loss: 0.00003845
Iteration 144/1000 | Loss: 0.00003845
Iteration 145/1000 | Loss: 0.00003845
Iteration 146/1000 | Loss: 0.00003845
Iteration 147/1000 | Loss: 0.00005190
Iteration 148/1000 | Loss: 0.00004371
Iteration 149/1000 | Loss: 0.00003841
Iteration 150/1000 | Loss: 0.00003840
Iteration 151/1000 | Loss: 0.00003840
Iteration 152/1000 | Loss: 0.00003840
Iteration 153/1000 | Loss: 0.00003839
Iteration 154/1000 | Loss: 0.00003839
Iteration 155/1000 | Loss: 0.00003838
Iteration 156/1000 | Loss: 0.00003838
Iteration 157/1000 | Loss: 0.00005246
Iteration 158/1000 | Loss: 0.00006645
Iteration 159/1000 | Loss: 0.00003844
Iteration 160/1000 | Loss: 0.00003838
Iteration 161/1000 | Loss: 0.00003837
Iteration 162/1000 | Loss: 0.00004893
Iteration 163/1000 | Loss: 0.00003835
Iteration 164/1000 | Loss: 0.00003834
Iteration 165/1000 | Loss: 0.00003833
Iteration 166/1000 | Loss: 0.00003832
Iteration 167/1000 | Loss: 0.00003832
Iteration 168/1000 | Loss: 0.00003829
Iteration 169/1000 | Loss: 0.00006587
Iteration 170/1000 | Loss: 0.00003828
Iteration 171/1000 | Loss: 0.00003828
Iteration 172/1000 | Loss: 0.00003827
Iteration 173/1000 | Loss: 0.00003824
Iteration 174/1000 | Loss: 0.00003823
Iteration 175/1000 | Loss: 0.00006668
Iteration 176/1000 | Loss: 0.00005731
Iteration 177/1000 | Loss: 0.00005316
Iteration 178/1000 | Loss: 0.00028758
Iteration 179/1000 | Loss: 0.00016032
Iteration 180/1000 | Loss: 0.00010391
Iteration 181/1000 | Loss: 0.00004854
Iteration 182/1000 | Loss: 0.00004210
Iteration 183/1000 | Loss: 0.00025783
Iteration 184/1000 | Loss: 0.00018748
Iteration 185/1000 | Loss: 0.00004492
Iteration 186/1000 | Loss: 0.00006015
Iteration 187/1000 | Loss: 0.00003843
Iteration 188/1000 | Loss: 0.00004196
Iteration 189/1000 | Loss: 0.00004426
Iteration 190/1000 | Loss: 0.00025819
Iteration 191/1000 | Loss: 0.00018868
Iteration 192/1000 | Loss: 0.00003972
Iteration 193/1000 | Loss: 0.00004291
Iteration 194/1000 | Loss: 0.00026765
Iteration 195/1000 | Loss: 0.00025047
Iteration 196/1000 | Loss: 0.00012343
Iteration 197/1000 | Loss: 0.00053824
Iteration 198/1000 | Loss: 0.00004131
Iteration 199/1000 | Loss: 0.00013453
Iteration 200/1000 | Loss: 0.00034249
Iteration 201/1000 | Loss: 0.00003905
Iteration 202/1000 | Loss: 0.00007449
Iteration 203/1000 | Loss: 0.00005193
Iteration 204/1000 | Loss: 0.00006846
Iteration 205/1000 | Loss: 0.00009549
Iteration 206/1000 | Loss: 0.00004360
Iteration 207/1000 | Loss: 0.00003866
Iteration 208/1000 | Loss: 0.00003704
Iteration 209/1000 | Loss: 0.00003704
Iteration 210/1000 | Loss: 0.00003702
Iteration 211/1000 | Loss: 0.00003700
Iteration 212/1000 | Loss: 0.00005033
Iteration 213/1000 | Loss: 0.00006747
Iteration 214/1000 | Loss: 0.00011155
Iteration 215/1000 | Loss: 0.00003675
Iteration 216/1000 | Loss: 0.00003669
Iteration 217/1000 | Loss: 0.00003669
Iteration 218/1000 | Loss: 0.00003669
Iteration 219/1000 | Loss: 0.00003669
Iteration 220/1000 | Loss: 0.00003669
Iteration 221/1000 | Loss: 0.00003669
Iteration 222/1000 | Loss: 0.00003668
Iteration 223/1000 | Loss: 0.00003668
Iteration 224/1000 | Loss: 0.00003668
Iteration 225/1000 | Loss: 0.00003668
Iteration 226/1000 | Loss: 0.00003668
Iteration 227/1000 | Loss: 0.00003668
Iteration 228/1000 | Loss: 0.00003668
Iteration 229/1000 | Loss: 0.00003665
Iteration 230/1000 | Loss: 0.00003664
Iteration 231/1000 | Loss: 0.00003663
Iteration 232/1000 | Loss: 0.00003663
Iteration 233/1000 | Loss: 0.00003663
Iteration 234/1000 | Loss: 0.00003662
Iteration 235/1000 | Loss: 0.00003662
Iteration 236/1000 | Loss: 0.00003662
Iteration 237/1000 | Loss: 0.00003661
Iteration 238/1000 | Loss: 0.00003661
Iteration 239/1000 | Loss: 0.00003661
Iteration 240/1000 | Loss: 0.00003656
Iteration 241/1000 | Loss: 0.00003656
Iteration 242/1000 | Loss: 0.00003656
Iteration 243/1000 | Loss: 0.00003655
Iteration 244/1000 | Loss: 0.00003655
Iteration 245/1000 | Loss: 0.00003654
Iteration 246/1000 | Loss: 0.00003654
Iteration 247/1000 | Loss: 0.00006123
Iteration 248/1000 | Loss: 0.00003653
Iteration 249/1000 | Loss: 0.00003649
Iteration 250/1000 | Loss: 0.00003645
Iteration 251/1000 | Loss: 0.00003645
Iteration 252/1000 | Loss: 0.00003645
Iteration 253/1000 | Loss: 0.00003645
Iteration 254/1000 | Loss: 0.00003644
Iteration 255/1000 | Loss: 0.00003644
Iteration 256/1000 | Loss: 0.00003644
Iteration 257/1000 | Loss: 0.00003644
Iteration 258/1000 | Loss: 0.00003644
Iteration 259/1000 | Loss: 0.00003644
Iteration 260/1000 | Loss: 0.00003644
Iteration 261/1000 | Loss: 0.00003644
Iteration 262/1000 | Loss: 0.00003644
Iteration 263/1000 | Loss: 0.00003644
Iteration 264/1000 | Loss: 0.00003644
Iteration 265/1000 | Loss: 0.00003644
Iteration 266/1000 | Loss: 0.00003643
Iteration 267/1000 | Loss: 0.00003643
Iteration 268/1000 | Loss: 0.00003643
Iteration 269/1000 | Loss: 0.00003643
Iteration 270/1000 | Loss: 0.00003643
Iteration 271/1000 | Loss: 0.00003643
Iteration 272/1000 | Loss: 0.00003643
Iteration 273/1000 | Loss: 0.00003643
Iteration 274/1000 | Loss: 0.00003643
Iteration 275/1000 | Loss: 0.00003643
Iteration 276/1000 | Loss: 0.00003642
Iteration 277/1000 | Loss: 0.00003642
Iteration 278/1000 | Loss: 0.00003642
Iteration 279/1000 | Loss: 0.00004762
Iteration 280/1000 | Loss: 0.00003713
Iteration 281/1000 | Loss: 0.00003680
Iteration 282/1000 | Loss: 0.00005767
Iteration 283/1000 | Loss: 0.00003726
Iteration 284/1000 | Loss: 0.00003636
Iteration 285/1000 | Loss: 0.00003634
Iteration 286/1000 | Loss: 0.00004494
Iteration 287/1000 | Loss: 0.00005293
Iteration 288/1000 | Loss: 0.00003696
Iteration 289/1000 | Loss: 0.00004379
Iteration 290/1000 | Loss: 0.00003669
Iteration 291/1000 | Loss: 0.00003679
Iteration 292/1000 | Loss: 0.00003626
Iteration 293/1000 | Loss: 0.00003626
Iteration 294/1000 | Loss: 0.00003626
Iteration 295/1000 | Loss: 0.00003626
Iteration 296/1000 | Loss: 0.00003626
Iteration 297/1000 | Loss: 0.00003626
Iteration 298/1000 | Loss: 0.00003626
Iteration 299/1000 | Loss: 0.00003626
Iteration 300/1000 | Loss: 0.00003626
Iteration 301/1000 | Loss: 0.00003626
Iteration 302/1000 | Loss: 0.00003626
Iteration 303/1000 | Loss: 0.00003626
Iteration 304/1000 | Loss: 0.00003626
Iteration 305/1000 | Loss: 0.00003626
Iteration 306/1000 | Loss: 0.00003626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [3.625547105912119e-05, 3.625547105912119e-05, 3.625547105912119e-05, 3.625547105912119e-05, 3.625547105912119e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.625547105912119e-05

Optimization complete. Final v2v error: 4.155435562133789 mm

Highest mean error: 13.193655014038086 mm for frame 81

Lowest mean error: 2.967151641845703 mm for frame 75

Saving results

Total time: 360.1063606739044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00703983
Iteration 2/25 | Loss: 0.00122194
Iteration 3/25 | Loss: 0.00111277
Iteration 4/25 | Loss: 0.00107987
Iteration 5/25 | Loss: 0.00106843
Iteration 6/25 | Loss: 0.00106711
Iteration 7/25 | Loss: 0.00106587
Iteration 8/25 | Loss: 0.00106565
Iteration 9/25 | Loss: 0.00106543
Iteration 10/25 | Loss: 0.00106525
Iteration 11/25 | Loss: 0.00106498
Iteration 12/25 | Loss: 0.00106473
Iteration 13/25 | Loss: 0.00106461
Iteration 14/25 | Loss: 0.00106460
Iteration 15/25 | Loss: 0.00106460
Iteration 16/25 | Loss: 0.00106460
Iteration 17/25 | Loss: 0.00106460
Iteration 18/25 | Loss: 0.00106460
Iteration 19/25 | Loss: 0.00106460
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010645982110872865, 0.0010645982110872865, 0.0010645982110872865, 0.0010645982110872865, 0.0010645982110872865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010645982110872865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58462262
Iteration 2/25 | Loss: 0.00072736
Iteration 3/25 | Loss: 0.00072736
Iteration 4/25 | Loss: 0.00072736
Iteration 5/25 | Loss: 0.00072736
Iteration 6/25 | Loss: 0.00072736
Iteration 7/25 | Loss: 0.00072736
Iteration 8/25 | Loss: 0.00072736
Iteration 9/25 | Loss: 0.00072736
Iteration 10/25 | Loss: 0.00072735
Iteration 11/25 | Loss: 0.00072735
Iteration 12/25 | Loss: 0.00072735
Iteration 13/25 | Loss: 0.00072735
Iteration 14/25 | Loss: 0.00072735
Iteration 15/25 | Loss: 0.00072735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007273549563251436, 0.0007273549563251436, 0.0007273549563251436, 0.0007273549563251436, 0.0007273549563251436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007273549563251436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072735
Iteration 2/1000 | Loss: 0.00002043
Iteration 3/1000 | Loss: 0.00001582
Iteration 4/1000 | Loss: 0.00001460
Iteration 5/1000 | Loss: 0.00001406
Iteration 6/1000 | Loss: 0.00001348
Iteration 7/1000 | Loss: 0.00001320
Iteration 8/1000 | Loss: 0.00028288
Iteration 9/1000 | Loss: 0.00001337
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001178
Iteration 12/1000 | Loss: 0.00001143
Iteration 13/1000 | Loss: 0.00001142
Iteration 14/1000 | Loss: 0.00001130
Iteration 15/1000 | Loss: 0.00001126
Iteration 16/1000 | Loss: 0.00001125
Iteration 17/1000 | Loss: 0.00001122
Iteration 18/1000 | Loss: 0.00001122
Iteration 19/1000 | Loss: 0.00001120
Iteration 20/1000 | Loss: 0.00001120
Iteration 21/1000 | Loss: 0.00001119
Iteration 22/1000 | Loss: 0.00001118
Iteration 23/1000 | Loss: 0.00001117
Iteration 24/1000 | Loss: 0.00001116
Iteration 25/1000 | Loss: 0.00001115
Iteration 26/1000 | Loss: 0.00001115
Iteration 27/1000 | Loss: 0.00001114
Iteration 28/1000 | Loss: 0.00001113
Iteration 29/1000 | Loss: 0.00001113
Iteration 30/1000 | Loss: 0.00001113
Iteration 31/1000 | Loss: 0.00001113
Iteration 32/1000 | Loss: 0.00001113
Iteration 33/1000 | Loss: 0.00001112
Iteration 34/1000 | Loss: 0.00001112
Iteration 35/1000 | Loss: 0.00001111
Iteration 36/1000 | Loss: 0.00001111
Iteration 37/1000 | Loss: 0.00001110
Iteration 38/1000 | Loss: 0.00001109
Iteration 39/1000 | Loss: 0.00001109
Iteration 40/1000 | Loss: 0.00001108
Iteration 41/1000 | Loss: 0.00001107
Iteration 42/1000 | Loss: 0.00001107
Iteration 43/1000 | Loss: 0.00001107
Iteration 44/1000 | Loss: 0.00001106
Iteration 45/1000 | Loss: 0.00001106
Iteration 46/1000 | Loss: 0.00001105
Iteration 47/1000 | Loss: 0.00001105
Iteration 48/1000 | Loss: 0.00001102
Iteration 49/1000 | Loss: 0.00001098
Iteration 50/1000 | Loss: 0.00001097
Iteration 51/1000 | Loss: 0.00001097
Iteration 52/1000 | Loss: 0.00001096
Iteration 53/1000 | Loss: 0.00001096
Iteration 54/1000 | Loss: 0.00001096
Iteration 55/1000 | Loss: 0.00001096
Iteration 56/1000 | Loss: 0.00001095
Iteration 57/1000 | Loss: 0.00001095
Iteration 58/1000 | Loss: 0.00001092
Iteration 59/1000 | Loss: 0.00001090
Iteration 60/1000 | Loss: 0.00001090
Iteration 61/1000 | Loss: 0.00001090
Iteration 62/1000 | Loss: 0.00001090
Iteration 63/1000 | Loss: 0.00001090
Iteration 64/1000 | Loss: 0.00001089
Iteration 65/1000 | Loss: 0.00001089
Iteration 66/1000 | Loss: 0.00001089
Iteration 67/1000 | Loss: 0.00001089
Iteration 68/1000 | Loss: 0.00001089
Iteration 69/1000 | Loss: 0.00001089
Iteration 70/1000 | Loss: 0.00001089
Iteration 71/1000 | Loss: 0.00001089
Iteration 72/1000 | Loss: 0.00001088
Iteration 73/1000 | Loss: 0.00001087
Iteration 74/1000 | Loss: 0.00001086
Iteration 75/1000 | Loss: 0.00001086
Iteration 76/1000 | Loss: 0.00001086
Iteration 77/1000 | Loss: 0.00001086
Iteration 78/1000 | Loss: 0.00001086
Iteration 79/1000 | Loss: 0.00001086
Iteration 80/1000 | Loss: 0.00001086
Iteration 81/1000 | Loss: 0.00001086
Iteration 82/1000 | Loss: 0.00001086
Iteration 83/1000 | Loss: 0.00001085
Iteration 84/1000 | Loss: 0.00001085
Iteration 85/1000 | Loss: 0.00001085
Iteration 86/1000 | Loss: 0.00001085
Iteration 87/1000 | Loss: 0.00001085
Iteration 88/1000 | Loss: 0.00001085
Iteration 89/1000 | Loss: 0.00001085
Iteration 90/1000 | Loss: 0.00001084
Iteration 91/1000 | Loss: 0.00001084
Iteration 92/1000 | Loss: 0.00001083
Iteration 93/1000 | Loss: 0.00001083
Iteration 94/1000 | Loss: 0.00001083
Iteration 95/1000 | Loss: 0.00001082
Iteration 96/1000 | Loss: 0.00001082
Iteration 97/1000 | Loss: 0.00001082
Iteration 98/1000 | Loss: 0.00001081
Iteration 99/1000 | Loss: 0.00001081
Iteration 100/1000 | Loss: 0.00001081
Iteration 101/1000 | Loss: 0.00001080
Iteration 102/1000 | Loss: 0.00001080
Iteration 103/1000 | Loss: 0.00001080
Iteration 104/1000 | Loss: 0.00001080
Iteration 105/1000 | Loss: 0.00001079
Iteration 106/1000 | Loss: 0.00001079
Iteration 107/1000 | Loss: 0.00001078
Iteration 108/1000 | Loss: 0.00001077
Iteration 109/1000 | Loss: 0.00001077
Iteration 110/1000 | Loss: 0.00001077
Iteration 111/1000 | Loss: 0.00001077
Iteration 112/1000 | Loss: 0.00001077
Iteration 113/1000 | Loss: 0.00001076
Iteration 114/1000 | Loss: 0.00001076
Iteration 115/1000 | Loss: 0.00001076
Iteration 116/1000 | Loss: 0.00001076
Iteration 117/1000 | Loss: 0.00001076
Iteration 118/1000 | Loss: 0.00001076
Iteration 119/1000 | Loss: 0.00001076
Iteration 120/1000 | Loss: 0.00001075
Iteration 121/1000 | Loss: 0.00001075
Iteration 122/1000 | Loss: 0.00001074
Iteration 123/1000 | Loss: 0.00001074
Iteration 124/1000 | Loss: 0.00001074
Iteration 125/1000 | Loss: 0.00001074
Iteration 126/1000 | Loss: 0.00001074
Iteration 127/1000 | Loss: 0.00001074
Iteration 128/1000 | Loss: 0.00001074
Iteration 129/1000 | Loss: 0.00001074
Iteration 130/1000 | Loss: 0.00001073
Iteration 131/1000 | Loss: 0.00001073
Iteration 132/1000 | Loss: 0.00001073
Iteration 133/1000 | Loss: 0.00001073
Iteration 134/1000 | Loss: 0.00001073
Iteration 135/1000 | Loss: 0.00001073
Iteration 136/1000 | Loss: 0.00001073
Iteration 137/1000 | Loss: 0.00001073
Iteration 138/1000 | Loss: 0.00001073
Iteration 139/1000 | Loss: 0.00001073
Iteration 140/1000 | Loss: 0.00001072
Iteration 141/1000 | Loss: 0.00001072
Iteration 142/1000 | Loss: 0.00001072
Iteration 143/1000 | Loss: 0.00001072
Iteration 144/1000 | Loss: 0.00001072
Iteration 145/1000 | Loss: 0.00001072
Iteration 146/1000 | Loss: 0.00001072
Iteration 147/1000 | Loss: 0.00001072
Iteration 148/1000 | Loss: 0.00001072
Iteration 149/1000 | Loss: 0.00001072
Iteration 150/1000 | Loss: 0.00001072
Iteration 151/1000 | Loss: 0.00001072
Iteration 152/1000 | Loss: 0.00001072
Iteration 153/1000 | Loss: 0.00001072
Iteration 154/1000 | Loss: 0.00001071
Iteration 155/1000 | Loss: 0.00001071
Iteration 156/1000 | Loss: 0.00001071
Iteration 157/1000 | Loss: 0.00001071
Iteration 158/1000 | Loss: 0.00001071
Iteration 159/1000 | Loss: 0.00001070
Iteration 160/1000 | Loss: 0.00001070
Iteration 161/1000 | Loss: 0.00001070
Iteration 162/1000 | Loss: 0.00001070
Iteration 163/1000 | Loss: 0.00001070
Iteration 164/1000 | Loss: 0.00001070
Iteration 165/1000 | Loss: 0.00001070
Iteration 166/1000 | Loss: 0.00001070
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.0701958672143519e-05, 1.0701958672143519e-05, 1.0701958672143519e-05, 1.0701958672143519e-05, 1.0701958672143519e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0701958672143519e-05

Optimization complete. Final v2v error: 2.794020175933838 mm

Highest mean error: 3.6463143825531006 mm for frame 140

Lowest mean error: 2.5255448818206787 mm for frame 32

Saving results

Total time: 62.4131646156311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471459
Iteration 2/25 | Loss: 0.00111340
Iteration 3/25 | Loss: 0.00105608
Iteration 4/25 | Loss: 0.00104804
Iteration 5/25 | Loss: 0.00104527
Iteration 6/25 | Loss: 0.00104467
Iteration 7/25 | Loss: 0.00104467
Iteration 8/25 | Loss: 0.00104467
Iteration 9/25 | Loss: 0.00104467
Iteration 10/25 | Loss: 0.00104467
Iteration 11/25 | Loss: 0.00104467
Iteration 12/25 | Loss: 0.00104467
Iteration 13/25 | Loss: 0.00104467
Iteration 14/25 | Loss: 0.00104467
Iteration 15/25 | Loss: 0.00104467
Iteration 16/25 | Loss: 0.00104467
Iteration 17/25 | Loss: 0.00104467
Iteration 18/25 | Loss: 0.00104467
Iteration 19/25 | Loss: 0.00104467
Iteration 20/25 | Loss: 0.00104467
Iteration 21/25 | Loss: 0.00104467
Iteration 22/25 | Loss: 0.00104467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010446652304381132, 0.0010446652304381132, 0.0010446652304381132, 0.0010446652304381132, 0.0010446652304381132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010446652304381132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.78704143
Iteration 2/25 | Loss: 0.00069696
Iteration 3/25 | Loss: 0.00069694
Iteration 4/25 | Loss: 0.00069694
Iteration 5/25 | Loss: 0.00069694
Iteration 6/25 | Loss: 0.00069694
Iteration 7/25 | Loss: 0.00069694
Iteration 8/25 | Loss: 0.00069694
Iteration 9/25 | Loss: 0.00069694
Iteration 10/25 | Loss: 0.00069694
Iteration 11/25 | Loss: 0.00069694
Iteration 12/25 | Loss: 0.00069694
Iteration 13/25 | Loss: 0.00069694
Iteration 14/25 | Loss: 0.00069694
Iteration 15/25 | Loss: 0.00069694
Iteration 16/25 | Loss: 0.00069694
Iteration 17/25 | Loss: 0.00069694
Iteration 18/25 | Loss: 0.00069694
Iteration 19/25 | Loss: 0.00069694
Iteration 20/25 | Loss: 0.00069694
Iteration 21/25 | Loss: 0.00069694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006969361566007137, 0.0006969361566007137, 0.0006969361566007137, 0.0006969361566007137, 0.0006969361566007137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006969361566007137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069694
Iteration 2/1000 | Loss: 0.00001952
Iteration 3/1000 | Loss: 0.00001412
Iteration 4/1000 | Loss: 0.00001272
Iteration 5/1000 | Loss: 0.00001170
Iteration 6/1000 | Loss: 0.00001115
Iteration 7/1000 | Loss: 0.00001080
Iteration 8/1000 | Loss: 0.00001055
Iteration 9/1000 | Loss: 0.00001018
Iteration 10/1000 | Loss: 0.00001002
Iteration 11/1000 | Loss: 0.00000996
Iteration 12/1000 | Loss: 0.00000995
Iteration 13/1000 | Loss: 0.00000993
Iteration 14/1000 | Loss: 0.00000986
Iteration 15/1000 | Loss: 0.00000986
Iteration 16/1000 | Loss: 0.00000983
Iteration 17/1000 | Loss: 0.00000981
Iteration 18/1000 | Loss: 0.00000980
Iteration 19/1000 | Loss: 0.00000980
Iteration 20/1000 | Loss: 0.00000979
Iteration 21/1000 | Loss: 0.00000978
Iteration 22/1000 | Loss: 0.00000977
Iteration 23/1000 | Loss: 0.00000976
Iteration 24/1000 | Loss: 0.00000976
Iteration 25/1000 | Loss: 0.00000976
Iteration 26/1000 | Loss: 0.00000976
Iteration 27/1000 | Loss: 0.00000976
Iteration 28/1000 | Loss: 0.00000976
Iteration 29/1000 | Loss: 0.00000976
Iteration 30/1000 | Loss: 0.00000976
Iteration 31/1000 | Loss: 0.00000975
Iteration 32/1000 | Loss: 0.00000975
Iteration 33/1000 | Loss: 0.00000975
Iteration 34/1000 | Loss: 0.00000973
Iteration 35/1000 | Loss: 0.00000971
Iteration 36/1000 | Loss: 0.00000971
Iteration 37/1000 | Loss: 0.00000970
Iteration 38/1000 | Loss: 0.00000970
Iteration 39/1000 | Loss: 0.00000970
Iteration 40/1000 | Loss: 0.00000970
Iteration 41/1000 | Loss: 0.00000969
Iteration 42/1000 | Loss: 0.00000966
Iteration 43/1000 | Loss: 0.00000965
Iteration 44/1000 | Loss: 0.00000965
Iteration 45/1000 | Loss: 0.00000965
Iteration 46/1000 | Loss: 0.00000965
Iteration 47/1000 | Loss: 0.00000964
Iteration 48/1000 | Loss: 0.00000964
Iteration 49/1000 | Loss: 0.00000962
Iteration 50/1000 | Loss: 0.00000961
Iteration 51/1000 | Loss: 0.00000961
Iteration 52/1000 | Loss: 0.00000961
Iteration 53/1000 | Loss: 0.00000961
Iteration 54/1000 | Loss: 0.00000960
Iteration 55/1000 | Loss: 0.00000957
Iteration 56/1000 | Loss: 0.00000957
Iteration 57/1000 | Loss: 0.00000956
Iteration 58/1000 | Loss: 0.00000956
Iteration 59/1000 | Loss: 0.00000956
Iteration 60/1000 | Loss: 0.00000955
Iteration 61/1000 | Loss: 0.00000954
Iteration 62/1000 | Loss: 0.00000953
Iteration 63/1000 | Loss: 0.00000953
Iteration 64/1000 | Loss: 0.00000952
Iteration 65/1000 | Loss: 0.00000952
Iteration 66/1000 | Loss: 0.00000952
Iteration 67/1000 | Loss: 0.00000952
Iteration 68/1000 | Loss: 0.00000952
Iteration 69/1000 | Loss: 0.00000952
Iteration 70/1000 | Loss: 0.00000952
Iteration 71/1000 | Loss: 0.00000951
Iteration 72/1000 | Loss: 0.00000951
Iteration 73/1000 | Loss: 0.00000951
Iteration 74/1000 | Loss: 0.00000951
Iteration 75/1000 | Loss: 0.00000951
Iteration 76/1000 | Loss: 0.00000951
Iteration 77/1000 | Loss: 0.00000951
Iteration 78/1000 | Loss: 0.00000951
Iteration 79/1000 | Loss: 0.00000951
Iteration 80/1000 | Loss: 0.00000950
Iteration 81/1000 | Loss: 0.00000950
Iteration 82/1000 | Loss: 0.00000950
Iteration 83/1000 | Loss: 0.00000949
Iteration 84/1000 | Loss: 0.00000949
Iteration 85/1000 | Loss: 0.00000948
Iteration 86/1000 | Loss: 0.00000948
Iteration 87/1000 | Loss: 0.00000948
Iteration 88/1000 | Loss: 0.00000948
Iteration 89/1000 | Loss: 0.00000948
Iteration 90/1000 | Loss: 0.00000947
Iteration 91/1000 | Loss: 0.00000947
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000947
Iteration 94/1000 | Loss: 0.00000947
Iteration 95/1000 | Loss: 0.00000947
Iteration 96/1000 | Loss: 0.00000947
Iteration 97/1000 | Loss: 0.00000947
Iteration 98/1000 | Loss: 0.00000946
Iteration 99/1000 | Loss: 0.00000946
Iteration 100/1000 | Loss: 0.00000946
Iteration 101/1000 | Loss: 0.00000945
Iteration 102/1000 | Loss: 0.00000945
Iteration 103/1000 | Loss: 0.00000945
Iteration 104/1000 | Loss: 0.00000945
Iteration 105/1000 | Loss: 0.00000945
Iteration 106/1000 | Loss: 0.00000945
Iteration 107/1000 | Loss: 0.00000944
Iteration 108/1000 | Loss: 0.00000944
Iteration 109/1000 | Loss: 0.00000943
Iteration 110/1000 | Loss: 0.00000943
Iteration 111/1000 | Loss: 0.00000943
Iteration 112/1000 | Loss: 0.00000943
Iteration 113/1000 | Loss: 0.00000942
Iteration 114/1000 | Loss: 0.00000942
Iteration 115/1000 | Loss: 0.00000942
Iteration 116/1000 | Loss: 0.00000942
Iteration 117/1000 | Loss: 0.00000942
Iteration 118/1000 | Loss: 0.00000942
Iteration 119/1000 | Loss: 0.00000941
Iteration 120/1000 | Loss: 0.00000941
Iteration 121/1000 | Loss: 0.00000941
Iteration 122/1000 | Loss: 0.00000940
Iteration 123/1000 | Loss: 0.00000940
Iteration 124/1000 | Loss: 0.00000939
Iteration 125/1000 | Loss: 0.00000939
Iteration 126/1000 | Loss: 0.00000939
Iteration 127/1000 | Loss: 0.00000939
Iteration 128/1000 | Loss: 0.00000939
Iteration 129/1000 | Loss: 0.00000939
Iteration 130/1000 | Loss: 0.00000939
Iteration 131/1000 | Loss: 0.00000939
Iteration 132/1000 | Loss: 0.00000939
Iteration 133/1000 | Loss: 0.00000939
Iteration 134/1000 | Loss: 0.00000938
Iteration 135/1000 | Loss: 0.00000938
Iteration 136/1000 | Loss: 0.00000938
Iteration 137/1000 | Loss: 0.00000938
Iteration 138/1000 | Loss: 0.00000937
Iteration 139/1000 | Loss: 0.00000937
Iteration 140/1000 | Loss: 0.00000937
Iteration 141/1000 | Loss: 0.00000937
Iteration 142/1000 | Loss: 0.00000937
Iteration 143/1000 | Loss: 0.00000937
Iteration 144/1000 | Loss: 0.00000937
Iteration 145/1000 | Loss: 0.00000937
Iteration 146/1000 | Loss: 0.00000936
Iteration 147/1000 | Loss: 0.00000936
Iteration 148/1000 | Loss: 0.00000936
Iteration 149/1000 | Loss: 0.00000936
Iteration 150/1000 | Loss: 0.00000936
Iteration 151/1000 | Loss: 0.00000936
Iteration 152/1000 | Loss: 0.00000936
Iteration 153/1000 | Loss: 0.00000935
Iteration 154/1000 | Loss: 0.00000935
Iteration 155/1000 | Loss: 0.00000935
Iteration 156/1000 | Loss: 0.00000935
Iteration 157/1000 | Loss: 0.00000935
Iteration 158/1000 | Loss: 0.00000934
Iteration 159/1000 | Loss: 0.00000934
Iteration 160/1000 | Loss: 0.00000934
Iteration 161/1000 | Loss: 0.00000934
Iteration 162/1000 | Loss: 0.00000934
Iteration 163/1000 | Loss: 0.00000934
Iteration 164/1000 | Loss: 0.00000934
Iteration 165/1000 | Loss: 0.00000934
Iteration 166/1000 | Loss: 0.00000934
Iteration 167/1000 | Loss: 0.00000934
Iteration 168/1000 | Loss: 0.00000934
Iteration 169/1000 | Loss: 0.00000933
Iteration 170/1000 | Loss: 0.00000933
Iteration 171/1000 | Loss: 0.00000933
Iteration 172/1000 | Loss: 0.00000933
Iteration 173/1000 | Loss: 0.00000933
Iteration 174/1000 | Loss: 0.00000933
Iteration 175/1000 | Loss: 0.00000933
Iteration 176/1000 | Loss: 0.00000933
Iteration 177/1000 | Loss: 0.00000932
Iteration 178/1000 | Loss: 0.00000932
Iteration 179/1000 | Loss: 0.00000932
Iteration 180/1000 | Loss: 0.00000932
Iteration 181/1000 | Loss: 0.00000932
Iteration 182/1000 | Loss: 0.00000932
Iteration 183/1000 | Loss: 0.00000932
Iteration 184/1000 | Loss: 0.00000932
Iteration 185/1000 | Loss: 0.00000932
Iteration 186/1000 | Loss: 0.00000932
Iteration 187/1000 | Loss: 0.00000932
Iteration 188/1000 | Loss: 0.00000932
Iteration 189/1000 | Loss: 0.00000932
Iteration 190/1000 | Loss: 0.00000932
Iteration 191/1000 | Loss: 0.00000932
Iteration 192/1000 | Loss: 0.00000932
Iteration 193/1000 | Loss: 0.00000932
Iteration 194/1000 | Loss: 0.00000932
Iteration 195/1000 | Loss: 0.00000932
Iteration 196/1000 | Loss: 0.00000932
Iteration 197/1000 | Loss: 0.00000931
Iteration 198/1000 | Loss: 0.00000931
Iteration 199/1000 | Loss: 0.00000931
Iteration 200/1000 | Loss: 0.00000931
Iteration 201/1000 | Loss: 0.00000931
Iteration 202/1000 | Loss: 0.00000931
Iteration 203/1000 | Loss: 0.00000931
Iteration 204/1000 | Loss: 0.00000931
Iteration 205/1000 | Loss: 0.00000931
Iteration 206/1000 | Loss: 0.00000931
Iteration 207/1000 | Loss: 0.00000931
Iteration 208/1000 | Loss: 0.00000930
Iteration 209/1000 | Loss: 0.00000930
Iteration 210/1000 | Loss: 0.00000930
Iteration 211/1000 | Loss: 0.00000930
Iteration 212/1000 | Loss: 0.00000930
Iteration 213/1000 | Loss: 0.00000930
Iteration 214/1000 | Loss: 0.00000930
Iteration 215/1000 | Loss: 0.00000930
Iteration 216/1000 | Loss: 0.00000930
Iteration 217/1000 | Loss: 0.00000929
Iteration 218/1000 | Loss: 0.00000929
Iteration 219/1000 | Loss: 0.00000929
Iteration 220/1000 | Loss: 0.00000929
Iteration 221/1000 | Loss: 0.00000929
Iteration 222/1000 | Loss: 0.00000929
Iteration 223/1000 | Loss: 0.00000929
Iteration 224/1000 | Loss: 0.00000929
Iteration 225/1000 | Loss: 0.00000929
Iteration 226/1000 | Loss: 0.00000928
Iteration 227/1000 | Loss: 0.00000928
Iteration 228/1000 | Loss: 0.00000928
Iteration 229/1000 | Loss: 0.00000928
Iteration 230/1000 | Loss: 0.00000927
Iteration 231/1000 | Loss: 0.00000927
Iteration 232/1000 | Loss: 0.00000927
Iteration 233/1000 | Loss: 0.00000927
Iteration 234/1000 | Loss: 0.00000927
Iteration 235/1000 | Loss: 0.00000927
Iteration 236/1000 | Loss: 0.00000927
Iteration 237/1000 | Loss: 0.00000927
Iteration 238/1000 | Loss: 0.00000927
Iteration 239/1000 | Loss: 0.00000927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [9.27341534406878e-06, 9.27341534406878e-06, 9.27341534406878e-06, 9.27341534406878e-06, 9.27341534406878e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.27341534406878e-06

Optimization complete. Final v2v error: 2.620633840560913 mm

Highest mean error: 3.322049617767334 mm for frame 187

Lowest mean error: 2.3443238735198975 mm for frame 34

Saving results

Total time: 47.964163064956665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01063766
Iteration 2/25 | Loss: 0.00238370
Iteration 3/25 | Loss: 0.00198062
Iteration 4/25 | Loss: 0.00183263
Iteration 5/25 | Loss: 0.00141126
Iteration 6/25 | Loss: 0.00133571
Iteration 7/25 | Loss: 0.00135550
Iteration 8/25 | Loss: 0.00126512
Iteration 9/25 | Loss: 0.00128759
Iteration 10/25 | Loss: 0.00122082
Iteration 11/25 | Loss: 0.00121466
Iteration 12/25 | Loss: 0.00118944
Iteration 13/25 | Loss: 0.00118343
Iteration 14/25 | Loss: 0.00117443
Iteration 15/25 | Loss: 0.00117143
Iteration 16/25 | Loss: 0.00116867
Iteration 17/25 | Loss: 0.00116882
Iteration 18/25 | Loss: 0.00117280
Iteration 19/25 | Loss: 0.00116939
Iteration 20/25 | Loss: 0.00116827
Iteration 21/25 | Loss: 0.00116826
Iteration 22/25 | Loss: 0.00116826
Iteration 23/25 | Loss: 0.00116826
Iteration 24/25 | Loss: 0.00116826
Iteration 25/25 | Loss: 0.00116826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89573717
Iteration 2/25 | Loss: 0.00104170
Iteration 3/25 | Loss: 0.00104170
Iteration 4/25 | Loss: 0.00104170
Iteration 5/25 | Loss: 0.00104170
Iteration 6/25 | Loss: 0.00104170
Iteration 7/25 | Loss: 0.00104169
Iteration 8/25 | Loss: 0.00104169
Iteration 9/25 | Loss: 0.00102317
Iteration 10/25 | Loss: 0.00102315
Iteration 11/25 | Loss: 0.00102315
Iteration 12/25 | Loss: 0.00102315
Iteration 13/25 | Loss: 0.00102315
Iteration 14/25 | Loss: 0.00102315
Iteration 15/25 | Loss: 0.00102315
Iteration 16/25 | Loss: 0.00102315
Iteration 17/25 | Loss: 0.00102315
Iteration 18/25 | Loss: 0.00102315
Iteration 19/25 | Loss: 0.00102315
Iteration 20/25 | Loss: 0.00102315
Iteration 21/25 | Loss: 0.00102315
Iteration 22/25 | Loss: 0.00102315
Iteration 23/25 | Loss: 0.00102315
Iteration 24/25 | Loss: 0.00102315
Iteration 25/25 | Loss: 0.00102315

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102315
Iteration 2/1000 | Loss: 0.00003928
Iteration 3/1000 | Loss: 0.00003635
Iteration 4/1000 | Loss: 0.00009492
Iteration 5/1000 | Loss: 0.00005087
Iteration 6/1000 | Loss: 0.00001630
Iteration 7/1000 | Loss: 0.00004985
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00002653
Iteration 10/1000 | Loss: 0.00003826
Iteration 11/1000 | Loss: 0.00002882
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00002866
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001582
Iteration 16/1000 | Loss: 0.00001537
Iteration 17/1000 | Loss: 0.00001535
Iteration 18/1000 | Loss: 0.00001531
Iteration 19/1000 | Loss: 0.00001531
Iteration 20/1000 | Loss: 0.00001530
Iteration 21/1000 | Loss: 0.00001530
Iteration 22/1000 | Loss: 0.00001530
Iteration 23/1000 | Loss: 0.00001528
Iteration 24/1000 | Loss: 0.00001528
Iteration 25/1000 | Loss: 0.00001594
Iteration 26/1000 | Loss: 0.00001521
Iteration 27/1000 | Loss: 0.00001514
Iteration 28/1000 | Loss: 0.00001514
Iteration 29/1000 | Loss: 0.00001512
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001503
Iteration 32/1000 | Loss: 0.00001501
Iteration 33/1000 | Loss: 0.00001500
Iteration 34/1000 | Loss: 0.00001500
Iteration 35/1000 | Loss: 0.00001499
Iteration 36/1000 | Loss: 0.00001499
Iteration 37/1000 | Loss: 0.00001498
Iteration 38/1000 | Loss: 0.00001498
Iteration 39/1000 | Loss: 0.00001498
Iteration 40/1000 | Loss: 0.00001497
Iteration 41/1000 | Loss: 0.00001494
Iteration 42/1000 | Loss: 0.00001494
Iteration 43/1000 | Loss: 0.00001493
Iteration 44/1000 | Loss: 0.00001493
Iteration 45/1000 | Loss: 0.00001493
Iteration 46/1000 | Loss: 0.00001493
Iteration 47/1000 | Loss: 0.00001493
Iteration 48/1000 | Loss: 0.00001493
Iteration 49/1000 | Loss: 0.00001493
Iteration 50/1000 | Loss: 0.00001493
Iteration 51/1000 | Loss: 0.00001493
Iteration 52/1000 | Loss: 0.00001493
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001492
Iteration 55/1000 | Loss: 0.00001492
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001491
Iteration 58/1000 | Loss: 0.00001490
Iteration 59/1000 | Loss: 0.00001490
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001490
Iteration 62/1000 | Loss: 0.00001489
Iteration 63/1000 | Loss: 0.00001489
Iteration 64/1000 | Loss: 0.00001489
Iteration 65/1000 | Loss: 0.00001489
Iteration 66/1000 | Loss: 0.00001489
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00001489
Iteration 69/1000 | Loss: 0.00001489
Iteration 70/1000 | Loss: 0.00001489
Iteration 71/1000 | Loss: 0.00001488
Iteration 72/1000 | Loss: 0.00001488
Iteration 73/1000 | Loss: 0.00001488
Iteration 74/1000 | Loss: 0.00001488
Iteration 75/1000 | Loss: 0.00001488
Iteration 76/1000 | Loss: 0.00001488
Iteration 77/1000 | Loss: 0.00001488
Iteration 78/1000 | Loss: 0.00001487
Iteration 79/1000 | Loss: 0.00001487
Iteration 80/1000 | Loss: 0.00001487
Iteration 81/1000 | Loss: 0.00001487
Iteration 82/1000 | Loss: 0.00001487
Iteration 83/1000 | Loss: 0.00001487
Iteration 84/1000 | Loss: 0.00001487
Iteration 85/1000 | Loss: 0.00001487
Iteration 86/1000 | Loss: 0.00001487
Iteration 87/1000 | Loss: 0.00001487
Iteration 88/1000 | Loss: 0.00001487
Iteration 89/1000 | Loss: 0.00001487
Iteration 90/1000 | Loss: 0.00001487
Iteration 91/1000 | Loss: 0.00001487
Iteration 92/1000 | Loss: 0.00001487
Iteration 93/1000 | Loss: 0.00001487
Iteration 94/1000 | Loss: 0.00001487
Iteration 95/1000 | Loss: 0.00001487
Iteration 96/1000 | Loss: 0.00001487
Iteration 97/1000 | Loss: 0.00001487
Iteration 98/1000 | Loss: 0.00001487
Iteration 99/1000 | Loss: 0.00001487
Iteration 100/1000 | Loss: 0.00001487
Iteration 101/1000 | Loss: 0.00001487
Iteration 102/1000 | Loss: 0.00001487
Iteration 103/1000 | Loss: 0.00001487
Iteration 104/1000 | Loss: 0.00001487
Iteration 105/1000 | Loss: 0.00001487
Iteration 106/1000 | Loss: 0.00001487
Iteration 107/1000 | Loss: 0.00001487
Iteration 108/1000 | Loss: 0.00001487
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001487
Iteration 113/1000 | Loss: 0.00001487
Iteration 114/1000 | Loss: 0.00001487
Iteration 115/1000 | Loss: 0.00001487
Iteration 116/1000 | Loss: 0.00001487
Iteration 117/1000 | Loss: 0.00001487
Iteration 118/1000 | Loss: 0.00001487
Iteration 119/1000 | Loss: 0.00001487
Iteration 120/1000 | Loss: 0.00001487
Iteration 121/1000 | Loss: 0.00001487
Iteration 122/1000 | Loss: 0.00001487
Iteration 123/1000 | Loss: 0.00001487
Iteration 124/1000 | Loss: 0.00001487
Iteration 125/1000 | Loss: 0.00001487
Iteration 126/1000 | Loss: 0.00001487
Iteration 127/1000 | Loss: 0.00001487
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.4869130609440617e-05, 1.4869130609440617e-05, 1.4869130609440617e-05, 1.4869130609440617e-05, 1.4869130609440617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4869130609440617e-05

Optimization complete. Final v2v error: 3.1362719535827637 mm

Highest mean error: 8.890813827514648 mm for frame 16

Lowest mean error: 2.8549420833587646 mm for frame 26

Saving results

Total time: 70.21335649490356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00606644
Iteration 2/25 | Loss: 0.00158731
Iteration 3/25 | Loss: 0.00127021
Iteration 4/25 | Loss: 0.00125824
Iteration 5/25 | Loss: 0.00125418
Iteration 6/25 | Loss: 0.00125320
Iteration 7/25 | Loss: 0.00125320
Iteration 8/25 | Loss: 0.00125320
Iteration 9/25 | Loss: 0.00125320
Iteration 10/25 | Loss: 0.00125320
Iteration 11/25 | Loss: 0.00125320
Iteration 12/25 | Loss: 0.00125320
Iteration 13/25 | Loss: 0.00125320
Iteration 14/25 | Loss: 0.00125320
Iteration 15/25 | Loss: 0.00125320
Iteration 16/25 | Loss: 0.00125320
Iteration 17/25 | Loss: 0.00125320
Iteration 18/25 | Loss: 0.00125320
Iteration 19/25 | Loss: 0.00125320
Iteration 20/25 | Loss: 0.00125320
Iteration 21/25 | Loss: 0.00125320
Iteration 22/25 | Loss: 0.00125320
Iteration 23/25 | Loss: 0.00125320
Iteration 24/25 | Loss: 0.00125320
Iteration 25/25 | Loss: 0.00125320

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89449221
Iteration 2/25 | Loss: 0.00091005
Iteration 3/25 | Loss: 0.00091004
Iteration 4/25 | Loss: 0.00091004
Iteration 5/25 | Loss: 0.00091004
Iteration 6/25 | Loss: 0.00091004
Iteration 7/25 | Loss: 0.00091004
Iteration 8/25 | Loss: 0.00091004
Iteration 9/25 | Loss: 0.00091004
Iteration 10/25 | Loss: 0.00091004
Iteration 11/25 | Loss: 0.00091004
Iteration 12/25 | Loss: 0.00091004
Iteration 13/25 | Loss: 0.00091004
Iteration 14/25 | Loss: 0.00091004
Iteration 15/25 | Loss: 0.00091004
Iteration 16/25 | Loss: 0.00091004
Iteration 17/25 | Loss: 0.00091004
Iteration 18/25 | Loss: 0.00091004
Iteration 19/25 | Loss: 0.00091004
Iteration 20/25 | Loss: 0.00091004
Iteration 21/25 | Loss: 0.00091004
Iteration 22/25 | Loss: 0.00091004
Iteration 23/25 | Loss: 0.00091004
Iteration 24/25 | Loss: 0.00091004
Iteration 25/25 | Loss: 0.00091004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091004
Iteration 2/1000 | Loss: 0.00005590
Iteration 3/1000 | Loss: 0.00003589
Iteration 4/1000 | Loss: 0.00003252
Iteration 5/1000 | Loss: 0.00003112
Iteration 6/1000 | Loss: 0.00003038
Iteration 7/1000 | Loss: 0.00002975
Iteration 8/1000 | Loss: 0.00002924
Iteration 9/1000 | Loss: 0.00002887
Iteration 10/1000 | Loss: 0.00002856
Iteration 11/1000 | Loss: 0.00002829
Iteration 12/1000 | Loss: 0.00002801
Iteration 13/1000 | Loss: 0.00002775
Iteration 14/1000 | Loss: 0.00002751
Iteration 15/1000 | Loss: 0.00002736
Iteration 16/1000 | Loss: 0.00002718
Iteration 17/1000 | Loss: 0.00002705
Iteration 18/1000 | Loss: 0.00002701
Iteration 19/1000 | Loss: 0.00002689
Iteration 20/1000 | Loss: 0.00002687
Iteration 21/1000 | Loss: 0.00002684
Iteration 22/1000 | Loss: 0.00002684
Iteration 23/1000 | Loss: 0.00002683
Iteration 24/1000 | Loss: 0.00002683
Iteration 25/1000 | Loss: 0.00002682
Iteration 26/1000 | Loss: 0.00002681
Iteration 27/1000 | Loss: 0.00002681
Iteration 28/1000 | Loss: 0.00002680
Iteration 29/1000 | Loss: 0.00002680
Iteration 30/1000 | Loss: 0.00002680
Iteration 31/1000 | Loss: 0.00002680
Iteration 32/1000 | Loss: 0.00002679
Iteration 33/1000 | Loss: 0.00002679
Iteration 34/1000 | Loss: 0.00002678
Iteration 35/1000 | Loss: 0.00002678
Iteration 36/1000 | Loss: 0.00002678
Iteration 37/1000 | Loss: 0.00002678
Iteration 38/1000 | Loss: 0.00002678
Iteration 39/1000 | Loss: 0.00002678
Iteration 40/1000 | Loss: 0.00002677
Iteration 41/1000 | Loss: 0.00002677
Iteration 42/1000 | Loss: 0.00002677
Iteration 43/1000 | Loss: 0.00002676
Iteration 44/1000 | Loss: 0.00002676
Iteration 45/1000 | Loss: 0.00002676
Iteration 46/1000 | Loss: 0.00002675
Iteration 47/1000 | Loss: 0.00002675
Iteration 48/1000 | Loss: 0.00002675
Iteration 49/1000 | Loss: 0.00002675
Iteration 50/1000 | Loss: 0.00002675
Iteration 51/1000 | Loss: 0.00002675
Iteration 52/1000 | Loss: 0.00002675
Iteration 53/1000 | Loss: 0.00002675
Iteration 54/1000 | Loss: 0.00002675
Iteration 55/1000 | Loss: 0.00002675
Iteration 56/1000 | Loss: 0.00002674
Iteration 57/1000 | Loss: 0.00002674
Iteration 58/1000 | Loss: 0.00002674
Iteration 59/1000 | Loss: 0.00002674
Iteration 60/1000 | Loss: 0.00002674
Iteration 61/1000 | Loss: 0.00002674
Iteration 62/1000 | Loss: 0.00002674
Iteration 63/1000 | Loss: 0.00002673
Iteration 64/1000 | Loss: 0.00002673
Iteration 65/1000 | Loss: 0.00002673
Iteration 66/1000 | Loss: 0.00002672
Iteration 67/1000 | Loss: 0.00002672
Iteration 68/1000 | Loss: 0.00002672
Iteration 69/1000 | Loss: 0.00002671
Iteration 70/1000 | Loss: 0.00002671
Iteration 71/1000 | Loss: 0.00002671
Iteration 72/1000 | Loss: 0.00002671
Iteration 73/1000 | Loss: 0.00002671
Iteration 74/1000 | Loss: 0.00002671
Iteration 75/1000 | Loss: 0.00002671
Iteration 76/1000 | Loss: 0.00002671
Iteration 77/1000 | Loss: 0.00002670
Iteration 78/1000 | Loss: 0.00002670
Iteration 79/1000 | Loss: 0.00002670
Iteration 80/1000 | Loss: 0.00002670
Iteration 81/1000 | Loss: 0.00002670
Iteration 82/1000 | Loss: 0.00002669
Iteration 83/1000 | Loss: 0.00002669
Iteration 84/1000 | Loss: 0.00002669
Iteration 85/1000 | Loss: 0.00002669
Iteration 86/1000 | Loss: 0.00002668
Iteration 87/1000 | Loss: 0.00002668
Iteration 88/1000 | Loss: 0.00002668
Iteration 89/1000 | Loss: 0.00002668
Iteration 90/1000 | Loss: 0.00002668
Iteration 91/1000 | Loss: 0.00002667
Iteration 92/1000 | Loss: 0.00002667
Iteration 93/1000 | Loss: 0.00002667
Iteration 94/1000 | Loss: 0.00002667
Iteration 95/1000 | Loss: 0.00002667
Iteration 96/1000 | Loss: 0.00002667
Iteration 97/1000 | Loss: 0.00002667
Iteration 98/1000 | Loss: 0.00002667
Iteration 99/1000 | Loss: 0.00002667
Iteration 100/1000 | Loss: 0.00002666
Iteration 101/1000 | Loss: 0.00002666
Iteration 102/1000 | Loss: 0.00002666
Iteration 103/1000 | Loss: 0.00002666
Iteration 104/1000 | Loss: 0.00002666
Iteration 105/1000 | Loss: 0.00002666
Iteration 106/1000 | Loss: 0.00002666
Iteration 107/1000 | Loss: 0.00002666
Iteration 108/1000 | Loss: 0.00002666
Iteration 109/1000 | Loss: 0.00002666
Iteration 110/1000 | Loss: 0.00002666
Iteration 111/1000 | Loss: 0.00002666
Iteration 112/1000 | Loss: 0.00002666
Iteration 113/1000 | Loss: 0.00002666
Iteration 114/1000 | Loss: 0.00002666
Iteration 115/1000 | Loss: 0.00002666
Iteration 116/1000 | Loss: 0.00002666
Iteration 117/1000 | Loss: 0.00002666
Iteration 118/1000 | Loss: 0.00002666
Iteration 119/1000 | Loss: 0.00002665
Iteration 120/1000 | Loss: 0.00002665
Iteration 121/1000 | Loss: 0.00002665
Iteration 122/1000 | Loss: 0.00002665
Iteration 123/1000 | Loss: 0.00002665
Iteration 124/1000 | Loss: 0.00002664
Iteration 125/1000 | Loss: 0.00002664
Iteration 126/1000 | Loss: 0.00002664
Iteration 127/1000 | Loss: 0.00002664
Iteration 128/1000 | Loss: 0.00002664
Iteration 129/1000 | Loss: 0.00002664
Iteration 130/1000 | Loss: 0.00002664
Iteration 131/1000 | Loss: 0.00002664
Iteration 132/1000 | Loss: 0.00002664
Iteration 133/1000 | Loss: 0.00002664
Iteration 134/1000 | Loss: 0.00002664
Iteration 135/1000 | Loss: 0.00002664
Iteration 136/1000 | Loss: 0.00002663
Iteration 137/1000 | Loss: 0.00002663
Iteration 138/1000 | Loss: 0.00002663
Iteration 139/1000 | Loss: 0.00002663
Iteration 140/1000 | Loss: 0.00002663
Iteration 141/1000 | Loss: 0.00002663
Iteration 142/1000 | Loss: 0.00002663
Iteration 143/1000 | Loss: 0.00002663
Iteration 144/1000 | Loss: 0.00002663
Iteration 145/1000 | Loss: 0.00002663
Iteration 146/1000 | Loss: 0.00002663
Iteration 147/1000 | Loss: 0.00002663
Iteration 148/1000 | Loss: 0.00002663
Iteration 149/1000 | Loss: 0.00002663
Iteration 150/1000 | Loss: 0.00002662
Iteration 151/1000 | Loss: 0.00002662
Iteration 152/1000 | Loss: 0.00002662
Iteration 153/1000 | Loss: 0.00002662
Iteration 154/1000 | Loss: 0.00002662
Iteration 155/1000 | Loss: 0.00002662
Iteration 156/1000 | Loss: 0.00002662
Iteration 157/1000 | Loss: 0.00002662
Iteration 158/1000 | Loss: 0.00002662
Iteration 159/1000 | Loss: 0.00002662
Iteration 160/1000 | Loss: 0.00002662
Iteration 161/1000 | Loss: 0.00002662
Iteration 162/1000 | Loss: 0.00002662
Iteration 163/1000 | Loss: 0.00002662
Iteration 164/1000 | Loss: 0.00002662
Iteration 165/1000 | Loss: 0.00002662
Iteration 166/1000 | Loss: 0.00002662
Iteration 167/1000 | Loss: 0.00002662
Iteration 168/1000 | Loss: 0.00002662
Iteration 169/1000 | Loss: 0.00002661
Iteration 170/1000 | Loss: 0.00002661
Iteration 171/1000 | Loss: 0.00002661
Iteration 172/1000 | Loss: 0.00002661
Iteration 173/1000 | Loss: 0.00002661
Iteration 174/1000 | Loss: 0.00002661
Iteration 175/1000 | Loss: 0.00002661
Iteration 176/1000 | Loss: 0.00002661
Iteration 177/1000 | Loss: 0.00002661
Iteration 178/1000 | Loss: 0.00002661
Iteration 179/1000 | Loss: 0.00002661
Iteration 180/1000 | Loss: 0.00002661
Iteration 181/1000 | Loss: 0.00002661
Iteration 182/1000 | Loss: 0.00002661
Iteration 183/1000 | Loss: 0.00002661
Iteration 184/1000 | Loss: 0.00002661
Iteration 185/1000 | Loss: 0.00002661
Iteration 186/1000 | Loss: 0.00002661
Iteration 187/1000 | Loss: 0.00002661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [2.661298458406236e-05, 2.661298458406236e-05, 2.661298458406236e-05, 2.661298458406236e-05, 2.661298458406236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.661298458406236e-05

Optimization complete. Final v2v error: 3.95463228225708 mm

Highest mean error: 4.824062824249268 mm for frame 95

Lowest mean error: 3.0735974311828613 mm for frame 41

Saving results

Total time: 46.04002094268799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873661
Iteration 2/25 | Loss: 0.00238730
Iteration 3/25 | Loss: 0.00180811
Iteration 4/25 | Loss: 0.00152341
Iteration 5/25 | Loss: 0.00151677
Iteration 6/25 | Loss: 0.00150006
Iteration 7/25 | Loss: 0.00147173
Iteration 8/25 | Loss: 0.00147177
Iteration 9/25 | Loss: 0.00145670
Iteration 10/25 | Loss: 0.00143143
Iteration 11/25 | Loss: 0.00142405
Iteration 12/25 | Loss: 0.00141697
Iteration 13/25 | Loss: 0.00141718
Iteration 14/25 | Loss: 0.00141499
Iteration 15/25 | Loss: 0.00141967
Iteration 16/25 | Loss: 0.00141957
Iteration 17/25 | Loss: 0.00141721
Iteration 18/25 | Loss: 0.00141233
Iteration 19/25 | Loss: 0.00140868
Iteration 20/25 | Loss: 0.00140755
Iteration 21/25 | Loss: 0.00140794
Iteration 22/25 | Loss: 0.00141226
Iteration 23/25 | Loss: 0.00141172
Iteration 24/25 | Loss: 0.00141050
Iteration 25/25 | Loss: 0.00140686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.29752684
Iteration 2/25 | Loss: 0.00355662
Iteration 3/25 | Loss: 0.00355662
Iteration 4/25 | Loss: 0.00355662
Iteration 5/25 | Loss: 0.00355661
Iteration 6/25 | Loss: 0.00355661
Iteration 7/25 | Loss: 0.00355661
Iteration 8/25 | Loss: 0.00355661
Iteration 9/25 | Loss: 0.00355661
Iteration 10/25 | Loss: 0.00355661
Iteration 11/25 | Loss: 0.00355661
Iteration 12/25 | Loss: 0.00355661
Iteration 13/25 | Loss: 0.00355661
Iteration 14/25 | Loss: 0.00355661
Iteration 15/25 | Loss: 0.00355661
Iteration 16/25 | Loss: 0.00355661
Iteration 17/25 | Loss: 0.00355661
Iteration 18/25 | Loss: 0.00355661
Iteration 19/25 | Loss: 0.00355661
Iteration 20/25 | Loss: 0.00355661
Iteration 21/25 | Loss: 0.00355661
Iteration 22/25 | Loss: 0.00355661
Iteration 23/25 | Loss: 0.00355661
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.003556611714884639, 0.003556611714884639, 0.003556611714884639, 0.003556611714884639, 0.003556611714884639]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003556611714884639

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00355661
Iteration 2/1000 | Loss: 0.00044048
Iteration 3/1000 | Loss: 0.00170463
Iteration 4/1000 | Loss: 0.00091089
Iteration 5/1000 | Loss: 0.00061722
Iteration 6/1000 | Loss: 0.00020829
Iteration 7/1000 | Loss: 0.00067723
Iteration 8/1000 | Loss: 0.00016247
Iteration 9/1000 | Loss: 0.00007406
Iteration 10/1000 | Loss: 0.00013145
Iteration 11/1000 | Loss: 0.00004786
Iteration 12/1000 | Loss: 0.00004291
Iteration 13/1000 | Loss: 0.00003900
Iteration 14/1000 | Loss: 0.00011622
Iteration 15/1000 | Loss: 0.00028771
Iteration 16/1000 | Loss: 0.00022279
Iteration 17/1000 | Loss: 0.00004382
Iteration 18/1000 | Loss: 0.00003539
Iteration 19/1000 | Loss: 0.00003297
Iteration 20/1000 | Loss: 0.00003165
Iteration 21/1000 | Loss: 0.00024751
Iteration 22/1000 | Loss: 0.00018912
Iteration 23/1000 | Loss: 0.00023382
Iteration 24/1000 | Loss: 0.00003969
Iteration 25/1000 | Loss: 0.00003428
Iteration 26/1000 | Loss: 0.00020183
Iteration 27/1000 | Loss: 0.00003885
Iteration 28/1000 | Loss: 0.00003245
Iteration 29/1000 | Loss: 0.00003024
Iteration 30/1000 | Loss: 0.00002911
Iteration 31/1000 | Loss: 0.00002825
Iteration 32/1000 | Loss: 0.00002758
Iteration 33/1000 | Loss: 0.00002696
Iteration 34/1000 | Loss: 0.00002646
Iteration 35/1000 | Loss: 0.00002596
Iteration 36/1000 | Loss: 0.00027271
Iteration 37/1000 | Loss: 0.00014368
Iteration 38/1000 | Loss: 0.00026397
Iteration 39/1000 | Loss: 0.00011486
Iteration 40/1000 | Loss: 0.00020209
Iteration 41/1000 | Loss: 0.00010720
Iteration 42/1000 | Loss: 0.00022996
Iteration 43/1000 | Loss: 0.00009941
Iteration 44/1000 | Loss: 0.00022230
Iteration 45/1000 | Loss: 0.00016742
Iteration 46/1000 | Loss: 0.00019737
Iteration 47/1000 | Loss: 0.00014428
Iteration 48/1000 | Loss: 0.00002959
Iteration 49/1000 | Loss: 0.00002787
Iteration 50/1000 | Loss: 0.00002654
Iteration 51/1000 | Loss: 0.00002542
Iteration 52/1000 | Loss: 0.00002482
Iteration 53/1000 | Loss: 0.00002440
Iteration 54/1000 | Loss: 0.00002414
Iteration 55/1000 | Loss: 0.00002396
Iteration 56/1000 | Loss: 0.00002375
Iteration 57/1000 | Loss: 0.00002355
Iteration 58/1000 | Loss: 0.00002334
Iteration 59/1000 | Loss: 0.00002325
Iteration 60/1000 | Loss: 0.00002301
Iteration 61/1000 | Loss: 0.00002283
Iteration 62/1000 | Loss: 0.00026321
Iteration 63/1000 | Loss: 0.00011410
Iteration 64/1000 | Loss: 0.00024184
Iteration 65/1000 | Loss: 0.00010436
Iteration 66/1000 | Loss: 0.00024044
Iteration 67/1000 | Loss: 0.00003896
Iteration 68/1000 | Loss: 0.00003073
Iteration 69/1000 | Loss: 0.00002560
Iteration 70/1000 | Loss: 0.00002362
Iteration 71/1000 | Loss: 0.00002252
Iteration 72/1000 | Loss: 0.00002219
Iteration 73/1000 | Loss: 0.00002196
Iteration 74/1000 | Loss: 0.00002189
Iteration 75/1000 | Loss: 0.00002189
Iteration 76/1000 | Loss: 0.00002188
Iteration 77/1000 | Loss: 0.00002188
Iteration 78/1000 | Loss: 0.00002187
Iteration 79/1000 | Loss: 0.00002187
Iteration 80/1000 | Loss: 0.00002187
Iteration 81/1000 | Loss: 0.00002186
Iteration 82/1000 | Loss: 0.00002186
Iteration 83/1000 | Loss: 0.00002185
Iteration 84/1000 | Loss: 0.00002184
Iteration 85/1000 | Loss: 0.00002184
Iteration 86/1000 | Loss: 0.00002183
Iteration 87/1000 | Loss: 0.00002183
Iteration 88/1000 | Loss: 0.00002182
Iteration 89/1000 | Loss: 0.00002181
Iteration 90/1000 | Loss: 0.00002181
Iteration 91/1000 | Loss: 0.00002181
Iteration 92/1000 | Loss: 0.00002180
Iteration 93/1000 | Loss: 0.00002180
Iteration 94/1000 | Loss: 0.00002179
Iteration 95/1000 | Loss: 0.00002178
Iteration 96/1000 | Loss: 0.00002177
Iteration 97/1000 | Loss: 0.00002177
Iteration 98/1000 | Loss: 0.00002177
Iteration 99/1000 | Loss: 0.00002177
Iteration 100/1000 | Loss: 0.00002176
Iteration 101/1000 | Loss: 0.00002176
Iteration 102/1000 | Loss: 0.00002176
Iteration 103/1000 | Loss: 0.00002176
Iteration 104/1000 | Loss: 0.00002176
Iteration 105/1000 | Loss: 0.00002175
Iteration 106/1000 | Loss: 0.00002175
Iteration 107/1000 | Loss: 0.00002175
Iteration 108/1000 | Loss: 0.00002175
Iteration 109/1000 | Loss: 0.00002175
Iteration 110/1000 | Loss: 0.00002175
Iteration 111/1000 | Loss: 0.00002174
Iteration 112/1000 | Loss: 0.00002174
Iteration 113/1000 | Loss: 0.00002174
Iteration 114/1000 | Loss: 0.00002174
Iteration 115/1000 | Loss: 0.00002174
Iteration 116/1000 | Loss: 0.00002174
Iteration 117/1000 | Loss: 0.00002174
Iteration 118/1000 | Loss: 0.00002174
Iteration 119/1000 | Loss: 0.00002174
Iteration 120/1000 | Loss: 0.00002174
Iteration 121/1000 | Loss: 0.00002173
Iteration 122/1000 | Loss: 0.00002173
Iteration 123/1000 | Loss: 0.00002173
Iteration 124/1000 | Loss: 0.00002173
Iteration 125/1000 | Loss: 0.00002173
Iteration 126/1000 | Loss: 0.00002173
Iteration 127/1000 | Loss: 0.00002173
Iteration 128/1000 | Loss: 0.00002173
Iteration 129/1000 | Loss: 0.00002173
Iteration 130/1000 | Loss: 0.00002173
Iteration 131/1000 | Loss: 0.00002173
Iteration 132/1000 | Loss: 0.00002172
Iteration 133/1000 | Loss: 0.00002172
Iteration 134/1000 | Loss: 0.00002172
Iteration 135/1000 | Loss: 0.00002172
Iteration 136/1000 | Loss: 0.00002172
Iteration 137/1000 | Loss: 0.00002172
Iteration 138/1000 | Loss: 0.00002171
Iteration 139/1000 | Loss: 0.00002171
Iteration 140/1000 | Loss: 0.00002171
Iteration 141/1000 | Loss: 0.00002171
Iteration 142/1000 | Loss: 0.00002170
Iteration 143/1000 | Loss: 0.00002170
Iteration 144/1000 | Loss: 0.00002170
Iteration 145/1000 | Loss: 0.00002170
Iteration 146/1000 | Loss: 0.00002170
Iteration 147/1000 | Loss: 0.00002169
Iteration 148/1000 | Loss: 0.00002169
Iteration 149/1000 | Loss: 0.00002169
Iteration 150/1000 | Loss: 0.00002169
Iteration 151/1000 | Loss: 0.00002169
Iteration 152/1000 | Loss: 0.00002169
Iteration 153/1000 | Loss: 0.00002169
Iteration 154/1000 | Loss: 0.00002169
Iteration 155/1000 | Loss: 0.00002168
Iteration 156/1000 | Loss: 0.00002168
Iteration 157/1000 | Loss: 0.00002168
Iteration 158/1000 | Loss: 0.00002168
Iteration 159/1000 | Loss: 0.00002168
Iteration 160/1000 | Loss: 0.00002168
Iteration 161/1000 | Loss: 0.00002168
Iteration 162/1000 | Loss: 0.00002168
Iteration 163/1000 | Loss: 0.00002168
Iteration 164/1000 | Loss: 0.00002168
Iteration 165/1000 | Loss: 0.00002168
Iteration 166/1000 | Loss: 0.00002168
Iteration 167/1000 | Loss: 0.00002168
Iteration 168/1000 | Loss: 0.00002168
Iteration 169/1000 | Loss: 0.00002168
Iteration 170/1000 | Loss: 0.00002168
Iteration 171/1000 | Loss: 0.00002168
Iteration 172/1000 | Loss: 0.00002168
Iteration 173/1000 | Loss: 0.00002168
Iteration 174/1000 | Loss: 0.00002168
Iteration 175/1000 | Loss: 0.00002168
Iteration 176/1000 | Loss: 0.00002168
Iteration 177/1000 | Loss: 0.00002168
Iteration 178/1000 | Loss: 0.00002168
Iteration 179/1000 | Loss: 0.00002168
Iteration 180/1000 | Loss: 0.00002168
Iteration 181/1000 | Loss: 0.00002168
Iteration 182/1000 | Loss: 0.00002168
Iteration 183/1000 | Loss: 0.00002168
Iteration 184/1000 | Loss: 0.00002168
Iteration 185/1000 | Loss: 0.00002168
Iteration 186/1000 | Loss: 0.00002168
Iteration 187/1000 | Loss: 0.00002168
Iteration 188/1000 | Loss: 0.00002168
Iteration 189/1000 | Loss: 0.00002168
Iteration 190/1000 | Loss: 0.00002168
Iteration 191/1000 | Loss: 0.00002168
Iteration 192/1000 | Loss: 0.00002168
Iteration 193/1000 | Loss: 0.00002168
Iteration 194/1000 | Loss: 0.00002168
Iteration 195/1000 | Loss: 0.00002168
Iteration 196/1000 | Loss: 0.00002168
Iteration 197/1000 | Loss: 0.00002168
Iteration 198/1000 | Loss: 0.00002168
Iteration 199/1000 | Loss: 0.00002168
Iteration 200/1000 | Loss: 0.00002168
Iteration 201/1000 | Loss: 0.00002168
Iteration 202/1000 | Loss: 0.00002168
Iteration 203/1000 | Loss: 0.00002168
Iteration 204/1000 | Loss: 0.00002168
Iteration 205/1000 | Loss: 0.00002168
Iteration 206/1000 | Loss: 0.00002168
Iteration 207/1000 | Loss: 0.00002168
Iteration 208/1000 | Loss: 0.00002168
Iteration 209/1000 | Loss: 0.00002168
Iteration 210/1000 | Loss: 0.00002168
Iteration 211/1000 | Loss: 0.00002168
Iteration 212/1000 | Loss: 0.00002168
Iteration 213/1000 | Loss: 0.00002168
Iteration 214/1000 | Loss: 0.00002168
Iteration 215/1000 | Loss: 0.00002168
Iteration 216/1000 | Loss: 0.00002168
Iteration 217/1000 | Loss: 0.00002168
Iteration 218/1000 | Loss: 0.00002168
Iteration 219/1000 | Loss: 0.00002168
Iteration 220/1000 | Loss: 0.00002168
Iteration 221/1000 | Loss: 0.00002168
Iteration 222/1000 | Loss: 0.00002168
Iteration 223/1000 | Loss: 0.00002168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 223. Stopping optimization.
Last 5 losses: [2.1684249077225104e-05, 2.1684249077225104e-05, 2.1684249077225104e-05, 2.1684249077225104e-05, 2.1684249077225104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1684249077225104e-05

Optimization complete. Final v2v error: 3.5482568740844727 mm

Highest mean error: 12.6495943069458 mm for frame 56

Lowest mean error: 2.726247787475586 mm for frame 44

Saving results

Total time: 169.63399577140808
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383017
Iteration 2/25 | Loss: 0.00116221
Iteration 3/25 | Loss: 0.00109638
Iteration 4/25 | Loss: 0.00108492
Iteration 5/25 | Loss: 0.00108115
Iteration 6/25 | Loss: 0.00108064
Iteration 7/25 | Loss: 0.00108064
Iteration 8/25 | Loss: 0.00108064
Iteration 9/25 | Loss: 0.00108064
Iteration 10/25 | Loss: 0.00108064
Iteration 11/25 | Loss: 0.00108064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010806412901729345, 0.0010806412901729345, 0.0010806412901729345, 0.0010806412901729345, 0.0010806412901729345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010806412901729345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.42367744
Iteration 2/25 | Loss: 0.00084144
Iteration 3/25 | Loss: 0.00084142
Iteration 4/25 | Loss: 0.00084142
Iteration 5/25 | Loss: 0.00084142
Iteration 6/25 | Loss: 0.00084142
Iteration 7/25 | Loss: 0.00084142
Iteration 8/25 | Loss: 0.00084142
Iteration 9/25 | Loss: 0.00084142
Iteration 10/25 | Loss: 0.00084142
Iteration 11/25 | Loss: 0.00084142
Iteration 12/25 | Loss: 0.00084142
Iteration 13/25 | Loss: 0.00084142
Iteration 14/25 | Loss: 0.00084142
Iteration 15/25 | Loss: 0.00084142
Iteration 16/25 | Loss: 0.00084142
Iteration 17/25 | Loss: 0.00084142
Iteration 18/25 | Loss: 0.00084142
Iteration 19/25 | Loss: 0.00084142
Iteration 20/25 | Loss: 0.00084142
Iteration 21/25 | Loss: 0.00084142
Iteration 22/25 | Loss: 0.00084142
Iteration 23/25 | Loss: 0.00084142
Iteration 24/25 | Loss: 0.00084142
Iteration 25/25 | Loss: 0.00084142

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084142
Iteration 2/1000 | Loss: 0.00002446
Iteration 3/1000 | Loss: 0.00001524
Iteration 4/1000 | Loss: 0.00001394
Iteration 5/1000 | Loss: 0.00001341
Iteration 6/1000 | Loss: 0.00001304
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001257
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001208
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001207
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001203
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001199
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001192
Iteration 20/1000 | Loss: 0.00001192
Iteration 21/1000 | Loss: 0.00001192
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001191
Iteration 24/1000 | Loss: 0.00001191
Iteration 25/1000 | Loss: 0.00001191
Iteration 26/1000 | Loss: 0.00001190
Iteration 27/1000 | Loss: 0.00001190
Iteration 28/1000 | Loss: 0.00001190
Iteration 29/1000 | Loss: 0.00001190
Iteration 30/1000 | Loss: 0.00001188
Iteration 31/1000 | Loss: 0.00001187
Iteration 32/1000 | Loss: 0.00001187
Iteration 33/1000 | Loss: 0.00001186
Iteration 34/1000 | Loss: 0.00001186
Iteration 35/1000 | Loss: 0.00001186
Iteration 36/1000 | Loss: 0.00001185
Iteration 37/1000 | Loss: 0.00001185
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001184
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001183
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001182
Iteration 45/1000 | Loss: 0.00001182
Iteration 46/1000 | Loss: 0.00001182
Iteration 47/1000 | Loss: 0.00001181
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001177
Iteration 60/1000 | Loss: 0.00001177
Iteration 61/1000 | Loss: 0.00001177
Iteration 62/1000 | Loss: 0.00001177
Iteration 63/1000 | Loss: 0.00001177
Iteration 64/1000 | Loss: 0.00001176
Iteration 65/1000 | Loss: 0.00001176
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001173
Iteration 71/1000 | Loss: 0.00001173
Iteration 72/1000 | Loss: 0.00001173
Iteration 73/1000 | Loss: 0.00001173
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001171
Iteration 77/1000 | Loss: 0.00001171
Iteration 78/1000 | Loss: 0.00001171
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001170
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001170
Iteration 83/1000 | Loss: 0.00001170
Iteration 84/1000 | Loss: 0.00001170
Iteration 85/1000 | Loss: 0.00001170
Iteration 86/1000 | Loss: 0.00001170
Iteration 87/1000 | Loss: 0.00001170
Iteration 88/1000 | Loss: 0.00001170
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001170
Iteration 94/1000 | Loss: 0.00001170
Iteration 95/1000 | Loss: 0.00001170
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.1701255061780103e-05, 1.1701255061780103e-05, 1.1701255061780103e-05, 1.1701255061780103e-05, 1.1701255061780103e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1701255061780103e-05

Optimization complete. Final v2v error: 2.897228956222534 mm

Highest mean error: 3.460679531097412 mm for frame 78

Lowest mean error: 2.5331192016601562 mm for frame 53

Saving results

Total time: 31.139021635055542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823783
Iteration 2/25 | Loss: 0.00131583
Iteration 3/25 | Loss: 0.00111817
Iteration 4/25 | Loss: 0.00108619
Iteration 5/25 | Loss: 0.00108211
Iteration 6/25 | Loss: 0.00108102
Iteration 7/25 | Loss: 0.00108076
Iteration 8/25 | Loss: 0.00108076
Iteration 9/25 | Loss: 0.00108076
Iteration 10/25 | Loss: 0.00108076
Iteration 11/25 | Loss: 0.00108076
Iteration 12/25 | Loss: 0.00108076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010807617800310254, 0.0010807617800310254, 0.0010807617800310254, 0.0010807617800310254, 0.0010807617800310254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010807617800310254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.57619166
Iteration 2/25 | Loss: 0.00078042
Iteration 3/25 | Loss: 0.00069359
Iteration 4/25 | Loss: 0.00069359
Iteration 5/25 | Loss: 0.00069359
Iteration 6/25 | Loss: 0.00069359
Iteration 7/25 | Loss: 0.00069359
Iteration 8/25 | Loss: 0.00069359
Iteration 9/25 | Loss: 0.00069359
Iteration 10/25 | Loss: 0.00069359
Iteration 11/25 | Loss: 0.00069359
Iteration 12/25 | Loss: 0.00069359
Iteration 13/25 | Loss: 0.00069359
Iteration 14/25 | Loss: 0.00069359
Iteration 15/25 | Loss: 0.00069359
Iteration 16/25 | Loss: 0.00069359
Iteration 17/25 | Loss: 0.00069359
Iteration 18/25 | Loss: 0.00069359
Iteration 19/25 | Loss: 0.00069359
Iteration 20/25 | Loss: 0.00069359
Iteration 21/25 | Loss: 0.00069359
Iteration 22/25 | Loss: 0.00069359
Iteration 23/25 | Loss: 0.00069359
Iteration 24/25 | Loss: 0.00069359
Iteration 25/25 | Loss: 0.00069359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069359
Iteration 2/1000 | Loss: 0.00002095
Iteration 3/1000 | Loss: 0.00009425
Iteration 4/1000 | Loss: 0.00001498
Iteration 5/1000 | Loss: 0.00001387
Iteration 6/1000 | Loss: 0.00001335
Iteration 7/1000 | Loss: 0.00001285
Iteration 8/1000 | Loss: 0.00001255
Iteration 9/1000 | Loss: 0.00011103
Iteration 10/1000 | Loss: 0.00001561
Iteration 11/1000 | Loss: 0.00001316
Iteration 12/1000 | Loss: 0.00001243
Iteration 13/1000 | Loss: 0.00001197
Iteration 14/1000 | Loss: 0.00001157
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001132
Iteration 17/1000 | Loss: 0.00001132
Iteration 18/1000 | Loss: 0.00001115
Iteration 19/1000 | Loss: 0.00001104
Iteration 20/1000 | Loss: 0.00001104
Iteration 21/1000 | Loss: 0.00001103
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001098
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001097
Iteration 26/1000 | Loss: 0.00001092
Iteration 27/1000 | Loss: 0.00001090
Iteration 28/1000 | Loss: 0.00001089
Iteration 29/1000 | Loss: 0.00001088
Iteration 30/1000 | Loss: 0.00001086
Iteration 31/1000 | Loss: 0.00001084
Iteration 32/1000 | Loss: 0.00001083
Iteration 33/1000 | Loss: 0.00001083
Iteration 34/1000 | Loss: 0.00001083
Iteration 35/1000 | Loss: 0.00001083
Iteration 36/1000 | Loss: 0.00001081
Iteration 37/1000 | Loss: 0.00001081
Iteration 38/1000 | Loss: 0.00001080
Iteration 39/1000 | Loss: 0.00001080
Iteration 40/1000 | Loss: 0.00001079
Iteration 41/1000 | Loss: 0.00001079
Iteration 42/1000 | Loss: 0.00001079
Iteration 43/1000 | Loss: 0.00001079
Iteration 44/1000 | Loss: 0.00001078
Iteration 45/1000 | Loss: 0.00001078
Iteration 46/1000 | Loss: 0.00001078
Iteration 47/1000 | Loss: 0.00001077
Iteration 48/1000 | Loss: 0.00001077
Iteration 49/1000 | Loss: 0.00001077
Iteration 50/1000 | Loss: 0.00001077
Iteration 51/1000 | Loss: 0.00001077
Iteration 52/1000 | Loss: 0.00001077
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001077
Iteration 55/1000 | Loss: 0.00001077
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001076
Iteration 58/1000 | Loss: 0.00001076
Iteration 59/1000 | Loss: 0.00001076
Iteration 60/1000 | Loss: 0.00001076
Iteration 61/1000 | Loss: 0.00001075
Iteration 62/1000 | Loss: 0.00001075
Iteration 63/1000 | Loss: 0.00001075
Iteration 64/1000 | Loss: 0.00001075
Iteration 65/1000 | Loss: 0.00001075
Iteration 66/1000 | Loss: 0.00001075
Iteration 67/1000 | Loss: 0.00001075
Iteration 68/1000 | Loss: 0.00001074
Iteration 69/1000 | Loss: 0.00001074
Iteration 70/1000 | Loss: 0.00001074
Iteration 71/1000 | Loss: 0.00001074
Iteration 72/1000 | Loss: 0.00001074
Iteration 73/1000 | Loss: 0.00001073
Iteration 74/1000 | Loss: 0.00001073
Iteration 75/1000 | Loss: 0.00001073
Iteration 76/1000 | Loss: 0.00001073
Iteration 77/1000 | Loss: 0.00001073
Iteration 78/1000 | Loss: 0.00001072
Iteration 79/1000 | Loss: 0.00001072
Iteration 80/1000 | Loss: 0.00001072
Iteration 81/1000 | Loss: 0.00001072
Iteration 82/1000 | Loss: 0.00001071
Iteration 83/1000 | Loss: 0.00001071
Iteration 84/1000 | Loss: 0.00001071
Iteration 85/1000 | Loss: 0.00001071
Iteration 86/1000 | Loss: 0.00001071
Iteration 87/1000 | Loss: 0.00001070
Iteration 88/1000 | Loss: 0.00001070
Iteration 89/1000 | Loss: 0.00001070
Iteration 90/1000 | Loss: 0.00001070
Iteration 91/1000 | Loss: 0.00001069
Iteration 92/1000 | Loss: 0.00001069
Iteration 93/1000 | Loss: 0.00001069
Iteration 94/1000 | Loss: 0.00001068
Iteration 95/1000 | Loss: 0.00001068
Iteration 96/1000 | Loss: 0.00001068
Iteration 97/1000 | Loss: 0.00001067
Iteration 98/1000 | Loss: 0.00001067
Iteration 99/1000 | Loss: 0.00001067
Iteration 100/1000 | Loss: 0.00001067
Iteration 101/1000 | Loss: 0.00001066
Iteration 102/1000 | Loss: 0.00001066
Iteration 103/1000 | Loss: 0.00001066
Iteration 104/1000 | Loss: 0.00001065
Iteration 105/1000 | Loss: 0.00001065
Iteration 106/1000 | Loss: 0.00001065
Iteration 107/1000 | Loss: 0.00001065
Iteration 108/1000 | Loss: 0.00001065
Iteration 109/1000 | Loss: 0.00001065
Iteration 110/1000 | Loss: 0.00001065
Iteration 111/1000 | Loss: 0.00001065
Iteration 112/1000 | Loss: 0.00001065
Iteration 113/1000 | Loss: 0.00001065
Iteration 114/1000 | Loss: 0.00001064
Iteration 115/1000 | Loss: 0.00001064
Iteration 116/1000 | Loss: 0.00001064
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001064
Iteration 119/1000 | Loss: 0.00001064
Iteration 120/1000 | Loss: 0.00001064
Iteration 121/1000 | Loss: 0.00001064
Iteration 122/1000 | Loss: 0.00001064
Iteration 123/1000 | Loss: 0.00001064
Iteration 124/1000 | Loss: 0.00001064
Iteration 125/1000 | Loss: 0.00001064
Iteration 126/1000 | Loss: 0.00001064
Iteration 127/1000 | Loss: 0.00001064
Iteration 128/1000 | Loss: 0.00001064
Iteration 129/1000 | Loss: 0.00001064
Iteration 130/1000 | Loss: 0.00001064
Iteration 131/1000 | Loss: 0.00001064
Iteration 132/1000 | Loss: 0.00001064
Iteration 133/1000 | Loss: 0.00001064
Iteration 134/1000 | Loss: 0.00001064
Iteration 135/1000 | Loss: 0.00001064
Iteration 136/1000 | Loss: 0.00001064
Iteration 137/1000 | Loss: 0.00001064
Iteration 138/1000 | Loss: 0.00001064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.0636505066941027e-05, 1.0636505066941027e-05, 1.0636505066941027e-05, 1.0636505066941027e-05, 1.0636505066941027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0636505066941027e-05

Optimization complete. Final v2v error: 2.7930288314819336 mm

Highest mean error: 4.1551899909973145 mm for frame 58

Lowest mean error: 2.507890224456787 mm for frame 41

Saving results

Total time: 53.80973172187805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00729248
Iteration 2/25 | Loss: 0.00124985
Iteration 3/25 | Loss: 0.00111317
Iteration 4/25 | Loss: 0.00109511
Iteration 5/25 | Loss: 0.00109296
Iteration 6/25 | Loss: 0.00109296
Iteration 7/25 | Loss: 0.00109296
Iteration 8/25 | Loss: 0.00109296
Iteration 9/25 | Loss: 0.00109296
Iteration 10/25 | Loss: 0.00109296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010929551208391786, 0.0010929551208391786, 0.0010929551208391786, 0.0010929551208391786, 0.0010929551208391786]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010929551208391786

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37190175
Iteration 2/25 | Loss: 0.00059344
Iteration 3/25 | Loss: 0.00059339
Iteration 4/25 | Loss: 0.00059338
Iteration 5/25 | Loss: 0.00059338
Iteration 6/25 | Loss: 0.00059338
Iteration 7/25 | Loss: 0.00059338
Iteration 8/25 | Loss: 0.00059338
Iteration 9/25 | Loss: 0.00059338
Iteration 10/25 | Loss: 0.00059338
Iteration 11/25 | Loss: 0.00059338
Iteration 12/25 | Loss: 0.00059338
Iteration 13/25 | Loss: 0.00059338
Iteration 14/25 | Loss: 0.00059338
Iteration 15/25 | Loss: 0.00059338
Iteration 16/25 | Loss: 0.00059338
Iteration 17/25 | Loss: 0.00059338
Iteration 18/25 | Loss: 0.00059338
Iteration 19/25 | Loss: 0.00059338
Iteration 20/25 | Loss: 0.00059338
Iteration 21/25 | Loss: 0.00059338
Iteration 22/25 | Loss: 0.00059338
Iteration 23/25 | Loss: 0.00059338
Iteration 24/25 | Loss: 0.00059338
Iteration 25/25 | Loss: 0.00059338

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059338
Iteration 2/1000 | Loss: 0.00002384
Iteration 3/1000 | Loss: 0.00001634
Iteration 4/1000 | Loss: 0.00001467
Iteration 5/1000 | Loss: 0.00001372
Iteration 6/1000 | Loss: 0.00001300
Iteration 7/1000 | Loss: 0.00001258
Iteration 8/1000 | Loss: 0.00001217
Iteration 9/1000 | Loss: 0.00001174
Iteration 10/1000 | Loss: 0.00001154
Iteration 11/1000 | Loss: 0.00001152
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001132
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001119
Iteration 16/1000 | Loss: 0.00001109
Iteration 17/1000 | Loss: 0.00001108
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001100
Iteration 20/1000 | Loss: 0.00001099
Iteration 21/1000 | Loss: 0.00001098
Iteration 22/1000 | Loss: 0.00001091
Iteration 23/1000 | Loss: 0.00001090
Iteration 24/1000 | Loss: 0.00001086
Iteration 25/1000 | Loss: 0.00001085
Iteration 26/1000 | Loss: 0.00001085
Iteration 27/1000 | Loss: 0.00001085
Iteration 28/1000 | Loss: 0.00001084
Iteration 29/1000 | Loss: 0.00001083
Iteration 30/1000 | Loss: 0.00001083
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001075
Iteration 37/1000 | Loss: 0.00001075
Iteration 38/1000 | Loss: 0.00001075
Iteration 39/1000 | Loss: 0.00001074
Iteration 40/1000 | Loss: 0.00001074
Iteration 41/1000 | Loss: 0.00001073
Iteration 42/1000 | Loss: 0.00001073
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001072
Iteration 45/1000 | Loss: 0.00001072
Iteration 46/1000 | Loss: 0.00001071
Iteration 47/1000 | Loss: 0.00001071
Iteration 48/1000 | Loss: 0.00001070
Iteration 49/1000 | Loss: 0.00001070
Iteration 50/1000 | Loss: 0.00001070
Iteration 51/1000 | Loss: 0.00001069
Iteration 52/1000 | Loss: 0.00001069
Iteration 53/1000 | Loss: 0.00001069
Iteration 54/1000 | Loss: 0.00001068
Iteration 55/1000 | Loss: 0.00001068
Iteration 56/1000 | Loss: 0.00001067
Iteration 57/1000 | Loss: 0.00001066
Iteration 58/1000 | Loss: 0.00001066
Iteration 59/1000 | Loss: 0.00001066
Iteration 60/1000 | Loss: 0.00001066
Iteration 61/1000 | Loss: 0.00001065
Iteration 62/1000 | Loss: 0.00001065
Iteration 63/1000 | Loss: 0.00001065
Iteration 64/1000 | Loss: 0.00001065
Iteration 65/1000 | Loss: 0.00001065
Iteration 66/1000 | Loss: 0.00001065
Iteration 67/1000 | Loss: 0.00001065
Iteration 68/1000 | Loss: 0.00001065
Iteration 69/1000 | Loss: 0.00001064
Iteration 70/1000 | Loss: 0.00001064
Iteration 71/1000 | Loss: 0.00001064
Iteration 72/1000 | Loss: 0.00001063
Iteration 73/1000 | Loss: 0.00001063
Iteration 74/1000 | Loss: 0.00001063
Iteration 75/1000 | Loss: 0.00001063
Iteration 76/1000 | Loss: 0.00001063
Iteration 77/1000 | Loss: 0.00001063
Iteration 78/1000 | Loss: 0.00001063
Iteration 79/1000 | Loss: 0.00001063
Iteration 80/1000 | Loss: 0.00001063
Iteration 81/1000 | Loss: 0.00001062
Iteration 82/1000 | Loss: 0.00001062
Iteration 83/1000 | Loss: 0.00001062
Iteration 84/1000 | Loss: 0.00001062
Iteration 85/1000 | Loss: 0.00001061
Iteration 86/1000 | Loss: 0.00001061
Iteration 87/1000 | Loss: 0.00001061
Iteration 88/1000 | Loss: 0.00001060
Iteration 89/1000 | Loss: 0.00001060
Iteration 90/1000 | Loss: 0.00001060
Iteration 91/1000 | Loss: 0.00001059
Iteration 92/1000 | Loss: 0.00001059
Iteration 93/1000 | Loss: 0.00001059
Iteration 94/1000 | Loss: 0.00001058
Iteration 95/1000 | Loss: 0.00001058
Iteration 96/1000 | Loss: 0.00001057
Iteration 97/1000 | Loss: 0.00001057
Iteration 98/1000 | Loss: 0.00001057
Iteration 99/1000 | Loss: 0.00001056
Iteration 100/1000 | Loss: 0.00001056
Iteration 101/1000 | Loss: 0.00001056
Iteration 102/1000 | Loss: 0.00001056
Iteration 103/1000 | Loss: 0.00001056
Iteration 104/1000 | Loss: 0.00001055
Iteration 105/1000 | Loss: 0.00001055
Iteration 106/1000 | Loss: 0.00001055
Iteration 107/1000 | Loss: 0.00001055
Iteration 108/1000 | Loss: 0.00001055
Iteration 109/1000 | Loss: 0.00001054
Iteration 110/1000 | Loss: 0.00001053
Iteration 111/1000 | Loss: 0.00001053
Iteration 112/1000 | Loss: 0.00001053
Iteration 113/1000 | Loss: 0.00001052
Iteration 114/1000 | Loss: 0.00001052
Iteration 115/1000 | Loss: 0.00001052
Iteration 116/1000 | Loss: 0.00001051
Iteration 117/1000 | Loss: 0.00001050
Iteration 118/1000 | Loss: 0.00001050
Iteration 119/1000 | Loss: 0.00001049
Iteration 120/1000 | Loss: 0.00001049
Iteration 121/1000 | Loss: 0.00001049
Iteration 122/1000 | Loss: 0.00001049
Iteration 123/1000 | Loss: 0.00001049
Iteration 124/1000 | Loss: 0.00001049
Iteration 125/1000 | Loss: 0.00001048
Iteration 126/1000 | Loss: 0.00001048
Iteration 127/1000 | Loss: 0.00001048
Iteration 128/1000 | Loss: 0.00001048
Iteration 129/1000 | Loss: 0.00001048
Iteration 130/1000 | Loss: 0.00001048
Iteration 131/1000 | Loss: 0.00001048
Iteration 132/1000 | Loss: 0.00001048
Iteration 133/1000 | Loss: 0.00001047
Iteration 134/1000 | Loss: 0.00001047
Iteration 135/1000 | Loss: 0.00001046
Iteration 136/1000 | Loss: 0.00001046
Iteration 137/1000 | Loss: 0.00001046
Iteration 138/1000 | Loss: 0.00001045
Iteration 139/1000 | Loss: 0.00001045
Iteration 140/1000 | Loss: 0.00001045
Iteration 141/1000 | Loss: 0.00001045
Iteration 142/1000 | Loss: 0.00001044
Iteration 143/1000 | Loss: 0.00001044
Iteration 144/1000 | Loss: 0.00001044
Iteration 145/1000 | Loss: 0.00001043
Iteration 146/1000 | Loss: 0.00001043
Iteration 147/1000 | Loss: 0.00001043
Iteration 148/1000 | Loss: 0.00001043
Iteration 149/1000 | Loss: 0.00001043
Iteration 150/1000 | Loss: 0.00001043
Iteration 151/1000 | Loss: 0.00001042
Iteration 152/1000 | Loss: 0.00001042
Iteration 153/1000 | Loss: 0.00001042
Iteration 154/1000 | Loss: 0.00001042
Iteration 155/1000 | Loss: 0.00001042
Iteration 156/1000 | Loss: 0.00001042
Iteration 157/1000 | Loss: 0.00001042
Iteration 158/1000 | Loss: 0.00001042
Iteration 159/1000 | Loss: 0.00001042
Iteration 160/1000 | Loss: 0.00001042
Iteration 161/1000 | Loss: 0.00001042
Iteration 162/1000 | Loss: 0.00001041
Iteration 163/1000 | Loss: 0.00001041
Iteration 164/1000 | Loss: 0.00001040
Iteration 165/1000 | Loss: 0.00001040
Iteration 166/1000 | Loss: 0.00001040
Iteration 167/1000 | Loss: 0.00001040
Iteration 168/1000 | Loss: 0.00001040
Iteration 169/1000 | Loss: 0.00001039
Iteration 170/1000 | Loss: 0.00001039
Iteration 171/1000 | Loss: 0.00001039
Iteration 172/1000 | Loss: 0.00001039
Iteration 173/1000 | Loss: 0.00001039
Iteration 174/1000 | Loss: 0.00001039
Iteration 175/1000 | Loss: 0.00001039
Iteration 176/1000 | Loss: 0.00001039
Iteration 177/1000 | Loss: 0.00001039
Iteration 178/1000 | Loss: 0.00001039
Iteration 179/1000 | Loss: 0.00001039
Iteration 180/1000 | Loss: 0.00001039
Iteration 181/1000 | Loss: 0.00001039
Iteration 182/1000 | Loss: 0.00001039
Iteration 183/1000 | Loss: 0.00001039
Iteration 184/1000 | Loss: 0.00001039
Iteration 185/1000 | Loss: 0.00001039
Iteration 186/1000 | Loss: 0.00001039
Iteration 187/1000 | Loss: 0.00001039
Iteration 188/1000 | Loss: 0.00001039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.0391519936092664e-05, 1.0391519936092664e-05, 1.0391519936092664e-05, 1.0391519936092664e-05, 1.0391519936092664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0391519936092664e-05

Optimization complete. Final v2v error: 2.746429681777954 mm

Highest mean error: 2.9942686557769775 mm for frame 91

Lowest mean error: 2.556527614593506 mm for frame 157

Saving results

Total time: 48.14897847175598
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00785270
Iteration 2/25 | Loss: 0.00130488
Iteration 3/25 | Loss: 0.00110247
Iteration 4/25 | Loss: 0.00109360
Iteration 5/25 | Loss: 0.00109172
Iteration 6/25 | Loss: 0.00109167
Iteration 7/25 | Loss: 0.00109167
Iteration 8/25 | Loss: 0.00109167
Iteration 9/25 | Loss: 0.00109167
Iteration 10/25 | Loss: 0.00109167
Iteration 11/25 | Loss: 0.00109167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010916650062426925, 0.0010916650062426925, 0.0010916650062426925, 0.0010916650062426925, 0.0010916650062426925]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010916650062426925

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37008893
Iteration 2/25 | Loss: 0.00065381
Iteration 3/25 | Loss: 0.00065381
Iteration 4/25 | Loss: 0.00065381
Iteration 5/25 | Loss: 0.00065381
Iteration 6/25 | Loss: 0.00065381
Iteration 7/25 | Loss: 0.00065381
Iteration 8/25 | Loss: 0.00065381
Iteration 9/25 | Loss: 0.00065380
Iteration 10/25 | Loss: 0.00065380
Iteration 11/25 | Loss: 0.00065380
Iteration 12/25 | Loss: 0.00065380
Iteration 13/25 | Loss: 0.00065380
Iteration 14/25 | Loss: 0.00065380
Iteration 15/25 | Loss: 0.00065380
Iteration 16/25 | Loss: 0.00065380
Iteration 17/25 | Loss: 0.00065380
Iteration 18/25 | Loss: 0.00065380
Iteration 19/25 | Loss: 0.00065380
Iteration 20/25 | Loss: 0.00065380
Iteration 21/25 | Loss: 0.00065380
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006538039888255298, 0.0006538039888255298, 0.0006538039888255298, 0.0006538039888255298, 0.0006538039888255298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006538039888255298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065380
Iteration 2/1000 | Loss: 0.00002697
Iteration 3/1000 | Loss: 0.00001912
Iteration 4/1000 | Loss: 0.00001564
Iteration 5/1000 | Loss: 0.00001443
Iteration 6/1000 | Loss: 0.00001354
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001268
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001218
Iteration 11/1000 | Loss: 0.00001206
Iteration 12/1000 | Loss: 0.00001203
Iteration 13/1000 | Loss: 0.00001194
Iteration 14/1000 | Loss: 0.00001193
Iteration 15/1000 | Loss: 0.00001192
Iteration 16/1000 | Loss: 0.00001191
Iteration 17/1000 | Loss: 0.00001190
Iteration 18/1000 | Loss: 0.00001188
Iteration 19/1000 | Loss: 0.00001188
Iteration 20/1000 | Loss: 0.00001187
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001184
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001184
Iteration 27/1000 | Loss: 0.00001184
Iteration 28/1000 | Loss: 0.00001184
Iteration 29/1000 | Loss: 0.00001183
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001182
Iteration 34/1000 | Loss: 0.00001182
Iteration 35/1000 | Loss: 0.00001182
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001180
Iteration 40/1000 | Loss: 0.00001180
Iteration 41/1000 | Loss: 0.00001179
Iteration 42/1000 | Loss: 0.00001179
Iteration 43/1000 | Loss: 0.00001179
Iteration 44/1000 | Loss: 0.00001179
Iteration 45/1000 | Loss: 0.00001179
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001179
Iteration 48/1000 | Loss: 0.00001179
Iteration 49/1000 | Loss: 0.00001178
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001178
Iteration 54/1000 | Loss: 0.00001178
Iteration 55/1000 | Loss: 0.00001178
Iteration 56/1000 | Loss: 0.00001177
Iteration 57/1000 | Loss: 0.00001177
Iteration 58/1000 | Loss: 0.00001177
Iteration 59/1000 | Loss: 0.00001176
Iteration 60/1000 | Loss: 0.00001176
Iteration 61/1000 | Loss: 0.00001176
Iteration 62/1000 | Loss: 0.00001176
Iteration 63/1000 | Loss: 0.00001176
Iteration 64/1000 | Loss: 0.00001176
Iteration 65/1000 | Loss: 0.00001176
Iteration 66/1000 | Loss: 0.00001175
Iteration 67/1000 | Loss: 0.00001175
Iteration 68/1000 | Loss: 0.00001175
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001173
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001173
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001172
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001170
Iteration 95/1000 | Loss: 0.00001170
Iteration 96/1000 | Loss: 0.00001170
Iteration 97/1000 | Loss: 0.00001169
Iteration 98/1000 | Loss: 0.00001169
Iteration 99/1000 | Loss: 0.00001169
Iteration 100/1000 | Loss: 0.00001168
Iteration 101/1000 | Loss: 0.00001168
Iteration 102/1000 | Loss: 0.00001168
Iteration 103/1000 | Loss: 0.00001168
Iteration 104/1000 | Loss: 0.00001168
Iteration 105/1000 | Loss: 0.00001168
Iteration 106/1000 | Loss: 0.00001168
Iteration 107/1000 | Loss: 0.00001167
Iteration 108/1000 | Loss: 0.00001167
Iteration 109/1000 | Loss: 0.00001167
Iteration 110/1000 | Loss: 0.00001167
Iteration 111/1000 | Loss: 0.00001167
Iteration 112/1000 | Loss: 0.00001167
Iteration 113/1000 | Loss: 0.00001167
Iteration 114/1000 | Loss: 0.00001167
Iteration 115/1000 | Loss: 0.00001167
Iteration 116/1000 | Loss: 0.00001166
Iteration 117/1000 | Loss: 0.00001166
Iteration 118/1000 | Loss: 0.00001166
Iteration 119/1000 | Loss: 0.00001165
Iteration 120/1000 | Loss: 0.00001165
Iteration 121/1000 | Loss: 0.00001165
Iteration 122/1000 | Loss: 0.00001165
Iteration 123/1000 | Loss: 0.00001164
Iteration 124/1000 | Loss: 0.00001164
Iteration 125/1000 | Loss: 0.00001164
Iteration 126/1000 | Loss: 0.00001164
Iteration 127/1000 | Loss: 0.00001164
Iteration 128/1000 | Loss: 0.00001164
Iteration 129/1000 | Loss: 0.00001163
Iteration 130/1000 | Loss: 0.00001163
Iteration 131/1000 | Loss: 0.00001162
Iteration 132/1000 | Loss: 0.00001162
Iteration 133/1000 | Loss: 0.00001162
Iteration 134/1000 | Loss: 0.00001162
Iteration 135/1000 | Loss: 0.00001161
Iteration 136/1000 | Loss: 0.00001161
Iteration 137/1000 | Loss: 0.00001161
Iteration 138/1000 | Loss: 0.00001161
Iteration 139/1000 | Loss: 0.00001161
Iteration 140/1000 | Loss: 0.00001161
Iteration 141/1000 | Loss: 0.00001161
Iteration 142/1000 | Loss: 0.00001161
Iteration 143/1000 | Loss: 0.00001160
Iteration 144/1000 | Loss: 0.00001160
Iteration 145/1000 | Loss: 0.00001160
Iteration 146/1000 | Loss: 0.00001160
Iteration 147/1000 | Loss: 0.00001160
Iteration 148/1000 | Loss: 0.00001160
Iteration 149/1000 | Loss: 0.00001160
Iteration 150/1000 | Loss: 0.00001160
Iteration 151/1000 | Loss: 0.00001160
Iteration 152/1000 | Loss: 0.00001160
Iteration 153/1000 | Loss: 0.00001160
Iteration 154/1000 | Loss: 0.00001160
Iteration 155/1000 | Loss: 0.00001160
Iteration 156/1000 | Loss: 0.00001160
Iteration 157/1000 | Loss: 0.00001160
Iteration 158/1000 | Loss: 0.00001160
Iteration 159/1000 | Loss: 0.00001160
Iteration 160/1000 | Loss: 0.00001160
Iteration 161/1000 | Loss: 0.00001160
Iteration 162/1000 | Loss: 0.00001160
Iteration 163/1000 | Loss: 0.00001160
Iteration 164/1000 | Loss: 0.00001160
Iteration 165/1000 | Loss: 0.00001160
Iteration 166/1000 | Loss: 0.00001160
Iteration 167/1000 | Loss: 0.00001160
Iteration 168/1000 | Loss: 0.00001160
Iteration 169/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.1598521268751938e-05, 1.1598521268751938e-05, 1.1598521268751938e-05, 1.1598521268751938e-05, 1.1598521268751938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1598521268751938e-05

Optimization complete. Final v2v error: 2.8618147373199463 mm

Highest mean error: 3.2127552032470703 mm for frame 123

Lowest mean error: 2.5923025608062744 mm for frame 51

Saving results

Total time: 36.792001485824585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384266
Iteration 2/25 | Loss: 0.00121638
Iteration 3/25 | Loss: 0.00109768
Iteration 4/25 | Loss: 0.00107750
Iteration 5/25 | Loss: 0.00107099
Iteration 6/25 | Loss: 0.00106918
Iteration 7/25 | Loss: 0.00106880
Iteration 8/25 | Loss: 0.00106880
Iteration 9/25 | Loss: 0.00106880
Iteration 10/25 | Loss: 0.00106880
Iteration 11/25 | Loss: 0.00106880
Iteration 12/25 | Loss: 0.00106880
Iteration 13/25 | Loss: 0.00106880
Iteration 14/25 | Loss: 0.00106880
Iteration 15/25 | Loss: 0.00106880
Iteration 16/25 | Loss: 0.00106879
Iteration 17/25 | Loss: 0.00106878
Iteration 18/25 | Loss: 0.00106878
Iteration 19/25 | Loss: 0.00106878
Iteration 20/25 | Loss: 0.00106878
Iteration 21/25 | Loss: 0.00106878
Iteration 22/25 | Loss: 0.00106878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010687760077416897, 0.0010687760077416897, 0.0010687760077416897, 0.0010687760077416897, 0.0010687760077416897]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010687760077416897

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.75436091
Iteration 2/25 | Loss: 0.00089376
Iteration 3/25 | Loss: 0.00089371
Iteration 4/25 | Loss: 0.00089371
Iteration 5/25 | Loss: 0.00089371
Iteration 6/25 | Loss: 0.00089371
Iteration 7/25 | Loss: 0.00089370
Iteration 8/25 | Loss: 0.00089370
Iteration 9/25 | Loss: 0.00089370
Iteration 10/25 | Loss: 0.00089370
Iteration 11/25 | Loss: 0.00089370
Iteration 12/25 | Loss: 0.00089370
Iteration 13/25 | Loss: 0.00089370
Iteration 14/25 | Loss: 0.00089370
Iteration 15/25 | Loss: 0.00089370
Iteration 16/25 | Loss: 0.00089370
Iteration 17/25 | Loss: 0.00089370
Iteration 18/25 | Loss: 0.00089370
Iteration 19/25 | Loss: 0.00089370
Iteration 20/25 | Loss: 0.00089370
Iteration 21/25 | Loss: 0.00089370
Iteration 22/25 | Loss: 0.00089370
Iteration 23/25 | Loss: 0.00089370
Iteration 24/25 | Loss: 0.00089370
Iteration 25/25 | Loss: 0.00089370

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089370
Iteration 2/1000 | Loss: 0.00003380
Iteration 3/1000 | Loss: 0.00002027
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001449
Iteration 7/1000 | Loss: 0.00001395
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001337
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001295
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001295
Iteration 14/1000 | Loss: 0.00001294
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001291
Iteration 17/1000 | Loss: 0.00001291
Iteration 18/1000 | Loss: 0.00001284
Iteration 19/1000 | Loss: 0.00001284
Iteration 20/1000 | Loss: 0.00001281
Iteration 21/1000 | Loss: 0.00001281
Iteration 22/1000 | Loss: 0.00001281
Iteration 23/1000 | Loss: 0.00001281
Iteration 24/1000 | Loss: 0.00001279
Iteration 25/1000 | Loss: 0.00001276
Iteration 26/1000 | Loss: 0.00001273
Iteration 27/1000 | Loss: 0.00001269
Iteration 28/1000 | Loss: 0.00001269
Iteration 29/1000 | Loss: 0.00001267
Iteration 30/1000 | Loss: 0.00001266
Iteration 31/1000 | Loss: 0.00001266
Iteration 32/1000 | Loss: 0.00001265
Iteration 33/1000 | Loss: 0.00001265
Iteration 34/1000 | Loss: 0.00001264
Iteration 35/1000 | Loss: 0.00001264
Iteration 36/1000 | Loss: 0.00001263
Iteration 37/1000 | Loss: 0.00001263
Iteration 38/1000 | Loss: 0.00001263
Iteration 39/1000 | Loss: 0.00001262
Iteration 40/1000 | Loss: 0.00001262
Iteration 41/1000 | Loss: 0.00001261
Iteration 42/1000 | Loss: 0.00001261
Iteration 43/1000 | Loss: 0.00001261
Iteration 44/1000 | Loss: 0.00001260
Iteration 45/1000 | Loss: 0.00001260
Iteration 46/1000 | Loss: 0.00001260
Iteration 47/1000 | Loss: 0.00001259
Iteration 48/1000 | Loss: 0.00001259
Iteration 49/1000 | Loss: 0.00001258
Iteration 50/1000 | Loss: 0.00001258
Iteration 51/1000 | Loss: 0.00001258
Iteration 52/1000 | Loss: 0.00001257
Iteration 53/1000 | Loss: 0.00001257
Iteration 54/1000 | Loss: 0.00001257
Iteration 55/1000 | Loss: 0.00001256
Iteration 56/1000 | Loss: 0.00001256
Iteration 57/1000 | Loss: 0.00001255
Iteration 58/1000 | Loss: 0.00001255
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001254
Iteration 63/1000 | Loss: 0.00001253
Iteration 64/1000 | Loss: 0.00001253
Iteration 65/1000 | Loss: 0.00001253
Iteration 66/1000 | Loss: 0.00001252
Iteration 67/1000 | Loss: 0.00001252
Iteration 68/1000 | Loss: 0.00001252
Iteration 69/1000 | Loss: 0.00001252
Iteration 70/1000 | Loss: 0.00001252
Iteration 71/1000 | Loss: 0.00001252
Iteration 72/1000 | Loss: 0.00001251
Iteration 73/1000 | Loss: 0.00001251
Iteration 74/1000 | Loss: 0.00001251
Iteration 75/1000 | Loss: 0.00001250
Iteration 76/1000 | Loss: 0.00001250
Iteration 77/1000 | Loss: 0.00001250
Iteration 78/1000 | Loss: 0.00001249
Iteration 79/1000 | Loss: 0.00001249
Iteration 80/1000 | Loss: 0.00001246
Iteration 81/1000 | Loss: 0.00001245
Iteration 82/1000 | Loss: 0.00001244
Iteration 83/1000 | Loss: 0.00001244
Iteration 84/1000 | Loss: 0.00001243
Iteration 85/1000 | Loss: 0.00001241
Iteration 86/1000 | Loss: 0.00001241
Iteration 87/1000 | Loss: 0.00001241
Iteration 88/1000 | Loss: 0.00001241
Iteration 89/1000 | Loss: 0.00001241
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001240
Iteration 92/1000 | Loss: 0.00001240
Iteration 93/1000 | Loss: 0.00001240
Iteration 94/1000 | Loss: 0.00001240
Iteration 95/1000 | Loss: 0.00001240
Iteration 96/1000 | Loss: 0.00001240
Iteration 97/1000 | Loss: 0.00001239
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001238
Iteration 104/1000 | Loss: 0.00001238
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001237
Iteration 109/1000 | Loss: 0.00001237
Iteration 110/1000 | Loss: 0.00001237
Iteration 111/1000 | Loss: 0.00001236
Iteration 112/1000 | Loss: 0.00001236
Iteration 113/1000 | Loss: 0.00001236
Iteration 114/1000 | Loss: 0.00001236
Iteration 115/1000 | Loss: 0.00001236
Iteration 116/1000 | Loss: 0.00001236
Iteration 117/1000 | Loss: 0.00001235
Iteration 118/1000 | Loss: 0.00001235
Iteration 119/1000 | Loss: 0.00001235
Iteration 120/1000 | Loss: 0.00001235
Iteration 121/1000 | Loss: 0.00001235
Iteration 122/1000 | Loss: 0.00001235
Iteration 123/1000 | Loss: 0.00001235
Iteration 124/1000 | Loss: 0.00001235
Iteration 125/1000 | Loss: 0.00001235
Iteration 126/1000 | Loss: 0.00001235
Iteration 127/1000 | Loss: 0.00001234
Iteration 128/1000 | Loss: 0.00001234
Iteration 129/1000 | Loss: 0.00001234
Iteration 130/1000 | Loss: 0.00001234
Iteration 131/1000 | Loss: 0.00001234
Iteration 132/1000 | Loss: 0.00001234
Iteration 133/1000 | Loss: 0.00001234
Iteration 134/1000 | Loss: 0.00001234
Iteration 135/1000 | Loss: 0.00001234
Iteration 136/1000 | Loss: 0.00001234
Iteration 137/1000 | Loss: 0.00001234
Iteration 138/1000 | Loss: 0.00001234
Iteration 139/1000 | Loss: 0.00001234
Iteration 140/1000 | Loss: 0.00001234
Iteration 141/1000 | Loss: 0.00001233
Iteration 142/1000 | Loss: 0.00001233
Iteration 143/1000 | Loss: 0.00001233
Iteration 144/1000 | Loss: 0.00001233
Iteration 145/1000 | Loss: 0.00001233
Iteration 146/1000 | Loss: 0.00001233
Iteration 147/1000 | Loss: 0.00001233
Iteration 148/1000 | Loss: 0.00001233
Iteration 149/1000 | Loss: 0.00001233
Iteration 150/1000 | Loss: 0.00001232
Iteration 151/1000 | Loss: 0.00001232
Iteration 152/1000 | Loss: 0.00001232
Iteration 153/1000 | Loss: 0.00001232
Iteration 154/1000 | Loss: 0.00001232
Iteration 155/1000 | Loss: 0.00001232
Iteration 156/1000 | Loss: 0.00001232
Iteration 157/1000 | Loss: 0.00001232
Iteration 158/1000 | Loss: 0.00001232
Iteration 159/1000 | Loss: 0.00001232
Iteration 160/1000 | Loss: 0.00001232
Iteration 161/1000 | Loss: 0.00001232
Iteration 162/1000 | Loss: 0.00001232
Iteration 163/1000 | Loss: 0.00001232
Iteration 164/1000 | Loss: 0.00001232
Iteration 165/1000 | Loss: 0.00001232
Iteration 166/1000 | Loss: 0.00001232
Iteration 167/1000 | Loss: 0.00001232
Iteration 168/1000 | Loss: 0.00001232
Iteration 169/1000 | Loss: 0.00001232
Iteration 170/1000 | Loss: 0.00001232
Iteration 171/1000 | Loss: 0.00001231
Iteration 172/1000 | Loss: 0.00001231
Iteration 173/1000 | Loss: 0.00001231
Iteration 174/1000 | Loss: 0.00001231
Iteration 175/1000 | Loss: 0.00001231
Iteration 176/1000 | Loss: 0.00001231
Iteration 177/1000 | Loss: 0.00001231
Iteration 178/1000 | Loss: 0.00001231
Iteration 179/1000 | Loss: 0.00001231
Iteration 180/1000 | Loss: 0.00001231
Iteration 181/1000 | Loss: 0.00001231
Iteration 182/1000 | Loss: 0.00001231
Iteration 183/1000 | Loss: 0.00001231
Iteration 184/1000 | Loss: 0.00001231
Iteration 185/1000 | Loss: 0.00001231
Iteration 186/1000 | Loss: 0.00001231
Iteration 187/1000 | Loss: 0.00001230
Iteration 188/1000 | Loss: 0.00001230
Iteration 189/1000 | Loss: 0.00001230
Iteration 190/1000 | Loss: 0.00001230
Iteration 191/1000 | Loss: 0.00001230
Iteration 192/1000 | Loss: 0.00001230
Iteration 193/1000 | Loss: 0.00001230
Iteration 194/1000 | Loss: 0.00001230
Iteration 195/1000 | Loss: 0.00001230
Iteration 196/1000 | Loss: 0.00001230
Iteration 197/1000 | Loss: 0.00001230
Iteration 198/1000 | Loss: 0.00001230
Iteration 199/1000 | Loss: 0.00001230
Iteration 200/1000 | Loss: 0.00001230
Iteration 201/1000 | Loss: 0.00001230
Iteration 202/1000 | Loss: 0.00001229
Iteration 203/1000 | Loss: 0.00001229
Iteration 204/1000 | Loss: 0.00001229
Iteration 205/1000 | Loss: 0.00001229
Iteration 206/1000 | Loss: 0.00001229
Iteration 207/1000 | Loss: 0.00001229
Iteration 208/1000 | Loss: 0.00001229
Iteration 209/1000 | Loss: 0.00001229
Iteration 210/1000 | Loss: 0.00001229
Iteration 211/1000 | Loss: 0.00001229
Iteration 212/1000 | Loss: 0.00001229
Iteration 213/1000 | Loss: 0.00001229
Iteration 214/1000 | Loss: 0.00001229
Iteration 215/1000 | Loss: 0.00001229
Iteration 216/1000 | Loss: 0.00001229
Iteration 217/1000 | Loss: 0.00001228
Iteration 218/1000 | Loss: 0.00001228
Iteration 219/1000 | Loss: 0.00001228
Iteration 220/1000 | Loss: 0.00001228
Iteration 221/1000 | Loss: 0.00001228
Iteration 222/1000 | Loss: 0.00001228
Iteration 223/1000 | Loss: 0.00001228
Iteration 224/1000 | Loss: 0.00001228
Iteration 225/1000 | Loss: 0.00001228
Iteration 226/1000 | Loss: 0.00001228
Iteration 227/1000 | Loss: 0.00001228
Iteration 228/1000 | Loss: 0.00001228
Iteration 229/1000 | Loss: 0.00001228
Iteration 230/1000 | Loss: 0.00001228
Iteration 231/1000 | Loss: 0.00001228
Iteration 232/1000 | Loss: 0.00001228
Iteration 233/1000 | Loss: 0.00001228
Iteration 234/1000 | Loss: 0.00001228
Iteration 235/1000 | Loss: 0.00001228
Iteration 236/1000 | Loss: 0.00001228
Iteration 237/1000 | Loss: 0.00001228
Iteration 238/1000 | Loss: 0.00001228
Iteration 239/1000 | Loss: 0.00001228
Iteration 240/1000 | Loss: 0.00001228
Iteration 241/1000 | Loss: 0.00001228
Iteration 242/1000 | Loss: 0.00001228
Iteration 243/1000 | Loss: 0.00001228
Iteration 244/1000 | Loss: 0.00001228
Iteration 245/1000 | Loss: 0.00001228
Iteration 246/1000 | Loss: 0.00001228
Iteration 247/1000 | Loss: 0.00001228
Iteration 248/1000 | Loss: 0.00001228
Iteration 249/1000 | Loss: 0.00001228
Iteration 250/1000 | Loss: 0.00001228
Iteration 251/1000 | Loss: 0.00001228
Iteration 252/1000 | Loss: 0.00001228
Iteration 253/1000 | Loss: 0.00001228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.2276632332941517e-05, 1.2276632332941517e-05, 1.2276632332941517e-05, 1.2276632332941517e-05, 1.2276632332941517e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2276632332941517e-05

Optimization complete. Final v2v error: 2.910971164703369 mm

Highest mean error: 3.6722910404205322 mm for frame 10

Lowest mean error: 2.3816235065460205 mm for frame 138

Saving results

Total time: 44.86561298370361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060149
Iteration 2/25 | Loss: 0.01060149
Iteration 3/25 | Loss: 0.01060149
Iteration 4/25 | Loss: 0.01060149
Iteration 5/25 | Loss: 0.01060149
Iteration 6/25 | Loss: 0.01060149
Iteration 7/25 | Loss: 0.01060149
Iteration 8/25 | Loss: 0.01060149
Iteration 9/25 | Loss: 0.01060149
Iteration 10/25 | Loss: 0.01060149
Iteration 11/25 | Loss: 0.01060148
Iteration 12/25 | Loss: 0.01060148
Iteration 13/25 | Loss: 0.01060148
Iteration 14/25 | Loss: 0.01060148
Iteration 15/25 | Loss: 0.01060148
Iteration 16/25 | Loss: 0.01060148
Iteration 17/25 | Loss: 0.01060148
Iteration 18/25 | Loss: 0.01060148
Iteration 19/25 | Loss: 0.01060148
Iteration 20/25 | Loss: 0.01060148
Iteration 21/25 | Loss: 0.01060148
Iteration 22/25 | Loss: 0.01060148
Iteration 23/25 | Loss: 0.01060147
Iteration 24/25 | Loss: 0.01060147
Iteration 25/25 | Loss: 0.01060147

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62174392
Iteration 2/25 | Loss: 0.08258039
Iteration 3/25 | Loss: 0.08258032
Iteration 4/25 | Loss: 0.08258031
Iteration 5/25 | Loss: 0.08258031
Iteration 6/25 | Loss: 0.08258031
Iteration 7/25 | Loss: 0.08258031
Iteration 8/25 | Loss: 0.08258031
Iteration 9/25 | Loss: 0.08258031
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.08258030563592911, 0.08258030563592911, 0.08258030563592911, 0.08258030563592911, 0.08258030563592911]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08258030563592911

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08258031
Iteration 2/1000 | Loss: 0.00046353
Iteration 3/1000 | Loss: 0.00014558
Iteration 4/1000 | Loss: 0.00006227
Iteration 5/1000 | Loss: 0.00003342
Iteration 6/1000 | Loss: 0.00002584
Iteration 7/1000 | Loss: 0.00002218
Iteration 8/1000 | Loss: 0.00002006
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001677
Iteration 11/1000 | Loss: 0.00001564
Iteration 12/1000 | Loss: 0.00001477
Iteration 13/1000 | Loss: 0.00001406
Iteration 14/1000 | Loss: 0.00001343
Iteration 15/1000 | Loss: 0.00001290
Iteration 16/1000 | Loss: 0.00001236
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001099
Iteration 20/1000 | Loss: 0.00001067
Iteration 21/1000 | Loss: 0.00001039
Iteration 22/1000 | Loss: 0.00001006
Iteration 23/1000 | Loss: 0.00000981
Iteration 24/1000 | Loss: 0.00000972
Iteration 25/1000 | Loss: 0.00000957
Iteration 26/1000 | Loss: 0.00000956
Iteration 27/1000 | Loss: 0.00000947
Iteration 28/1000 | Loss: 0.00000945
Iteration 29/1000 | Loss: 0.00000941
Iteration 30/1000 | Loss: 0.00000936
Iteration 31/1000 | Loss: 0.00000932
Iteration 32/1000 | Loss: 0.00000930
Iteration 33/1000 | Loss: 0.00000929
Iteration 34/1000 | Loss: 0.00000929
Iteration 35/1000 | Loss: 0.00000929
Iteration 36/1000 | Loss: 0.00000926
Iteration 37/1000 | Loss: 0.00000926
Iteration 38/1000 | Loss: 0.00000922
Iteration 39/1000 | Loss: 0.00000922
Iteration 40/1000 | Loss: 0.00000921
Iteration 41/1000 | Loss: 0.00000921
Iteration 42/1000 | Loss: 0.00000920
Iteration 43/1000 | Loss: 0.00000919
Iteration 44/1000 | Loss: 0.00000919
Iteration 45/1000 | Loss: 0.00000918
Iteration 46/1000 | Loss: 0.00000918
Iteration 47/1000 | Loss: 0.00000918
Iteration 48/1000 | Loss: 0.00000917
Iteration 49/1000 | Loss: 0.00000917
Iteration 50/1000 | Loss: 0.00000917
Iteration 51/1000 | Loss: 0.00000916
Iteration 52/1000 | Loss: 0.00000915
Iteration 53/1000 | Loss: 0.00000915
Iteration 54/1000 | Loss: 0.00000915
Iteration 55/1000 | Loss: 0.00000914
Iteration 56/1000 | Loss: 0.00000914
Iteration 57/1000 | Loss: 0.00000914
Iteration 58/1000 | Loss: 0.00000914
Iteration 59/1000 | Loss: 0.00000914
Iteration 60/1000 | Loss: 0.00000914
Iteration 61/1000 | Loss: 0.00000914
Iteration 62/1000 | Loss: 0.00000914
Iteration 63/1000 | Loss: 0.00000914
Iteration 64/1000 | Loss: 0.00000914
Iteration 65/1000 | Loss: 0.00000914
Iteration 66/1000 | Loss: 0.00000914
Iteration 67/1000 | Loss: 0.00000914
Iteration 68/1000 | Loss: 0.00000914
Iteration 69/1000 | Loss: 0.00000914
Iteration 70/1000 | Loss: 0.00000914
Iteration 71/1000 | Loss: 0.00000914
Iteration 72/1000 | Loss: 0.00000914
Iteration 73/1000 | Loss: 0.00000914
Iteration 74/1000 | Loss: 0.00000914
Iteration 75/1000 | Loss: 0.00000914
Iteration 76/1000 | Loss: 0.00000914
Iteration 77/1000 | Loss: 0.00000914
Iteration 78/1000 | Loss: 0.00000914
Iteration 79/1000 | Loss: 0.00000914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [9.135136679105926e-06, 9.135136679105926e-06, 9.135136679105926e-06, 9.135136679105926e-06, 9.135136679105926e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.135136679105926e-06

Optimization complete. Final v2v error: 2.593294620513916 mm

Highest mean error: 2.7954530715942383 mm for frame 158

Lowest mean error: 2.3875362873077393 mm for frame 190

Saving results

Total time: 47.62635087966919
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056975
Iteration 2/25 | Loss: 0.00278941
Iteration 3/25 | Loss: 0.00183889
Iteration 4/25 | Loss: 0.00165069
Iteration 5/25 | Loss: 0.00164304
Iteration 6/25 | Loss: 0.00157414
Iteration 7/25 | Loss: 0.00155072
Iteration 8/25 | Loss: 0.00153753
Iteration 9/25 | Loss: 0.00153099
Iteration 10/25 | Loss: 0.00152718
Iteration 11/25 | Loss: 0.00151497
Iteration 12/25 | Loss: 0.00151308
Iteration 13/25 | Loss: 0.00150408
Iteration 14/25 | Loss: 0.00150736
Iteration 15/25 | Loss: 0.00150908
Iteration 16/25 | Loss: 0.00150050
Iteration 17/25 | Loss: 0.00149739
Iteration 18/25 | Loss: 0.00149588
Iteration 19/25 | Loss: 0.00149559
Iteration 20/25 | Loss: 0.00149539
Iteration 21/25 | Loss: 0.00149518
Iteration 22/25 | Loss: 0.00149502
Iteration 23/25 | Loss: 0.00149493
Iteration 24/25 | Loss: 0.00149478
Iteration 25/25 | Loss: 0.00149462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.17148232
Iteration 2/25 | Loss: 0.00397511
Iteration 3/25 | Loss: 0.00397511
Iteration 4/25 | Loss: 0.00397511
Iteration 5/25 | Loss: 0.00397511
Iteration 6/25 | Loss: 0.00397511
Iteration 7/25 | Loss: 0.00397511
Iteration 8/25 | Loss: 0.00397511
Iteration 9/25 | Loss: 0.00397511
Iteration 10/25 | Loss: 0.00397511
Iteration 11/25 | Loss: 0.00397511
Iteration 12/25 | Loss: 0.00397511
Iteration 13/25 | Loss: 0.00397511
Iteration 14/25 | Loss: 0.00397511
Iteration 15/25 | Loss: 0.00397511
Iteration 16/25 | Loss: 0.00397511
Iteration 17/25 | Loss: 0.00397511
Iteration 18/25 | Loss: 0.00397511
Iteration 19/25 | Loss: 0.00397511
Iteration 20/25 | Loss: 0.00397511
Iteration 21/25 | Loss: 0.00397511
Iteration 22/25 | Loss: 0.00397511
Iteration 23/25 | Loss: 0.00397511
Iteration 24/25 | Loss: 0.00397511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.003975109197199345, 0.003975109197199345, 0.003975109197199345, 0.003975109197199345, 0.003975109197199345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003975109197199345

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00397511
Iteration 2/1000 | Loss: 0.00285323
Iteration 3/1000 | Loss: 0.00244615
Iteration 4/1000 | Loss: 0.00500789
Iteration 5/1000 | Loss: 0.00044048
Iteration 6/1000 | Loss: 0.00201647
Iteration 7/1000 | Loss: 0.00068220
Iteration 8/1000 | Loss: 0.00161845
Iteration 9/1000 | Loss: 0.00024193
Iteration 10/1000 | Loss: 0.00017286
Iteration 11/1000 | Loss: 0.00025232
Iteration 12/1000 | Loss: 0.00013275
Iteration 13/1000 | Loss: 0.00018209
Iteration 14/1000 | Loss: 0.00011183
Iteration 15/1000 | Loss: 0.00023336
Iteration 16/1000 | Loss: 0.00030591
Iteration 17/1000 | Loss: 0.00012557
Iteration 18/1000 | Loss: 0.00063291
Iteration 19/1000 | Loss: 0.00137457
Iteration 20/1000 | Loss: 0.00010908
Iteration 21/1000 | Loss: 0.00009429
Iteration 22/1000 | Loss: 0.00008768
Iteration 23/1000 | Loss: 0.00081397
Iteration 24/1000 | Loss: 0.00051860
Iteration 25/1000 | Loss: 0.00076001
Iteration 26/1000 | Loss: 0.00054084
Iteration 27/1000 | Loss: 0.00072028
Iteration 28/1000 | Loss: 0.00008433
Iteration 29/1000 | Loss: 0.00007773
Iteration 30/1000 | Loss: 0.00007383
Iteration 31/1000 | Loss: 0.00007176
Iteration 32/1000 | Loss: 0.00396234
Iteration 33/1000 | Loss: 0.00142130
Iteration 34/1000 | Loss: 0.00109213
Iteration 35/1000 | Loss: 0.00012367
Iteration 36/1000 | Loss: 0.00008036
Iteration 37/1000 | Loss: 0.00006531
Iteration 38/1000 | Loss: 0.00093132
Iteration 39/1000 | Loss: 0.00021175
Iteration 40/1000 | Loss: 0.00005306
Iteration 41/1000 | Loss: 0.00004574
Iteration 42/1000 | Loss: 0.00004271
Iteration 43/1000 | Loss: 0.00003948
Iteration 44/1000 | Loss: 0.00003681
Iteration 45/1000 | Loss: 0.00003546
Iteration 46/1000 | Loss: 0.00003457
Iteration 47/1000 | Loss: 0.00003385
Iteration 48/1000 | Loss: 0.00003338
Iteration 49/1000 | Loss: 0.00003282
Iteration 50/1000 | Loss: 0.00003238
Iteration 51/1000 | Loss: 0.00003205
Iteration 52/1000 | Loss: 0.00003187
Iteration 53/1000 | Loss: 0.00003174
Iteration 54/1000 | Loss: 0.00003173
Iteration 55/1000 | Loss: 0.00003167
Iteration 56/1000 | Loss: 0.00003167
Iteration 57/1000 | Loss: 0.00003161
Iteration 58/1000 | Loss: 0.00003159
Iteration 59/1000 | Loss: 0.00003158
Iteration 60/1000 | Loss: 0.00003158
Iteration 61/1000 | Loss: 0.00003157
Iteration 62/1000 | Loss: 0.00003152
Iteration 63/1000 | Loss: 0.00003152
Iteration 64/1000 | Loss: 0.00003151
Iteration 65/1000 | Loss: 0.00003151
Iteration 66/1000 | Loss: 0.00003150
Iteration 67/1000 | Loss: 0.00003150
Iteration 68/1000 | Loss: 0.00003150
Iteration 69/1000 | Loss: 0.00003150
Iteration 70/1000 | Loss: 0.00003149
Iteration 71/1000 | Loss: 0.00003149
Iteration 72/1000 | Loss: 0.00003149
Iteration 73/1000 | Loss: 0.00003148
Iteration 74/1000 | Loss: 0.00003148
Iteration 75/1000 | Loss: 0.00003148
Iteration 76/1000 | Loss: 0.00003147
Iteration 77/1000 | Loss: 0.00003147
Iteration 78/1000 | Loss: 0.00003147
Iteration 79/1000 | Loss: 0.00003146
Iteration 80/1000 | Loss: 0.00003146
Iteration 81/1000 | Loss: 0.00003146
Iteration 82/1000 | Loss: 0.00003146
Iteration 83/1000 | Loss: 0.00003146
Iteration 84/1000 | Loss: 0.00003145
Iteration 85/1000 | Loss: 0.00003145
Iteration 86/1000 | Loss: 0.00003145
Iteration 87/1000 | Loss: 0.00003145
Iteration 88/1000 | Loss: 0.00003144
Iteration 89/1000 | Loss: 0.00003144
Iteration 90/1000 | Loss: 0.00003144
Iteration 91/1000 | Loss: 0.00003144
Iteration 92/1000 | Loss: 0.00003144
Iteration 93/1000 | Loss: 0.00003143
Iteration 94/1000 | Loss: 0.00003143
Iteration 95/1000 | Loss: 0.00003143
Iteration 96/1000 | Loss: 0.00003143
Iteration 97/1000 | Loss: 0.00003143
Iteration 98/1000 | Loss: 0.00003143
Iteration 99/1000 | Loss: 0.00003142
Iteration 100/1000 | Loss: 0.00003142
Iteration 101/1000 | Loss: 0.00003142
Iteration 102/1000 | Loss: 0.00003142
Iteration 103/1000 | Loss: 0.00003141
Iteration 104/1000 | Loss: 0.00003141
Iteration 105/1000 | Loss: 0.00003141
Iteration 106/1000 | Loss: 0.00003141
Iteration 107/1000 | Loss: 0.00003140
Iteration 108/1000 | Loss: 0.00003140
Iteration 109/1000 | Loss: 0.00003139
Iteration 110/1000 | Loss: 0.00003139
Iteration 111/1000 | Loss: 0.00003139
Iteration 112/1000 | Loss: 0.00003139
Iteration 113/1000 | Loss: 0.00003139
Iteration 114/1000 | Loss: 0.00003138
Iteration 115/1000 | Loss: 0.00003138
Iteration 116/1000 | Loss: 0.00003138
Iteration 117/1000 | Loss: 0.00003138
Iteration 118/1000 | Loss: 0.00003138
Iteration 119/1000 | Loss: 0.00003137
Iteration 120/1000 | Loss: 0.00003137
Iteration 121/1000 | Loss: 0.00003137
Iteration 122/1000 | Loss: 0.00003137
Iteration 123/1000 | Loss: 0.00003137
Iteration 124/1000 | Loss: 0.00003137
Iteration 125/1000 | Loss: 0.00003136
Iteration 126/1000 | Loss: 0.00003136
Iteration 127/1000 | Loss: 0.00003136
Iteration 128/1000 | Loss: 0.00003136
Iteration 129/1000 | Loss: 0.00003136
Iteration 130/1000 | Loss: 0.00003136
Iteration 131/1000 | Loss: 0.00003136
Iteration 132/1000 | Loss: 0.00003136
Iteration 133/1000 | Loss: 0.00003136
Iteration 134/1000 | Loss: 0.00003136
Iteration 135/1000 | Loss: 0.00003136
Iteration 136/1000 | Loss: 0.00003136
Iteration 137/1000 | Loss: 0.00003135
Iteration 138/1000 | Loss: 0.00003135
Iteration 139/1000 | Loss: 0.00003135
Iteration 140/1000 | Loss: 0.00003135
Iteration 141/1000 | Loss: 0.00003135
Iteration 142/1000 | Loss: 0.00003135
Iteration 143/1000 | Loss: 0.00003134
Iteration 144/1000 | Loss: 0.00003134
Iteration 145/1000 | Loss: 0.00003134
Iteration 146/1000 | Loss: 0.00003133
Iteration 147/1000 | Loss: 0.00003133
Iteration 148/1000 | Loss: 0.00003133
Iteration 149/1000 | Loss: 0.00003133
Iteration 150/1000 | Loss: 0.00003133
Iteration 151/1000 | Loss: 0.00003133
Iteration 152/1000 | Loss: 0.00003133
Iteration 153/1000 | Loss: 0.00003133
Iteration 154/1000 | Loss: 0.00003132
Iteration 155/1000 | Loss: 0.00003132
Iteration 156/1000 | Loss: 0.00003132
Iteration 157/1000 | Loss: 0.00003132
Iteration 158/1000 | Loss: 0.00003132
Iteration 159/1000 | Loss: 0.00003132
Iteration 160/1000 | Loss: 0.00003132
Iteration 161/1000 | Loss: 0.00003132
Iteration 162/1000 | Loss: 0.00003132
Iteration 163/1000 | Loss: 0.00003132
Iteration 164/1000 | Loss: 0.00003132
Iteration 165/1000 | Loss: 0.00003132
Iteration 166/1000 | Loss: 0.00003132
Iteration 167/1000 | Loss: 0.00003131
Iteration 168/1000 | Loss: 0.00003131
Iteration 169/1000 | Loss: 0.00003131
Iteration 170/1000 | Loss: 0.00003131
Iteration 171/1000 | Loss: 0.00003131
Iteration 172/1000 | Loss: 0.00003131
Iteration 173/1000 | Loss: 0.00003131
Iteration 174/1000 | Loss: 0.00003131
Iteration 175/1000 | Loss: 0.00003131
Iteration 176/1000 | Loss: 0.00003131
Iteration 177/1000 | Loss: 0.00003131
Iteration 178/1000 | Loss: 0.00003131
Iteration 179/1000 | Loss: 0.00003131
Iteration 180/1000 | Loss: 0.00003131
Iteration 181/1000 | Loss: 0.00003131
Iteration 182/1000 | Loss: 0.00003131
Iteration 183/1000 | Loss: 0.00003130
Iteration 184/1000 | Loss: 0.00003130
Iteration 185/1000 | Loss: 0.00003130
Iteration 186/1000 | Loss: 0.00003130
Iteration 187/1000 | Loss: 0.00003130
Iteration 188/1000 | Loss: 0.00003130
Iteration 189/1000 | Loss: 0.00003130
Iteration 190/1000 | Loss: 0.00003130
Iteration 191/1000 | Loss: 0.00003130
Iteration 192/1000 | Loss: 0.00003130
Iteration 193/1000 | Loss: 0.00003129
Iteration 194/1000 | Loss: 0.00003129
Iteration 195/1000 | Loss: 0.00003129
Iteration 196/1000 | Loss: 0.00003129
Iteration 197/1000 | Loss: 0.00003129
Iteration 198/1000 | Loss: 0.00003129
Iteration 199/1000 | Loss: 0.00003129
Iteration 200/1000 | Loss: 0.00003129
Iteration 201/1000 | Loss: 0.00003129
Iteration 202/1000 | Loss: 0.00003129
Iteration 203/1000 | Loss: 0.00003129
Iteration 204/1000 | Loss: 0.00003129
Iteration 205/1000 | Loss: 0.00003129
Iteration 206/1000 | Loss: 0.00003129
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [3.129375181742944e-05, 3.129375181742944e-05, 3.129375181742944e-05, 3.129375181742944e-05, 3.129375181742944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.129375181742944e-05

Optimization complete. Final v2v error: 3.7799651622772217 mm

Highest mean error: 11.971219062805176 mm for frame 67

Lowest mean error: 2.737809896469116 mm for frame 139

Saving results

Total time: 132.73078536987305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439702
Iteration 2/25 | Loss: 0.00114850
Iteration 3/25 | Loss: 0.00108989
Iteration 4/25 | Loss: 0.00107608
Iteration 5/25 | Loss: 0.00107186
Iteration 6/25 | Loss: 0.00107132
Iteration 7/25 | Loss: 0.00107132
Iteration 8/25 | Loss: 0.00107132
Iteration 9/25 | Loss: 0.00107132
Iteration 10/25 | Loss: 0.00107132
Iteration 11/25 | Loss: 0.00107132
Iteration 12/25 | Loss: 0.00107132
Iteration 13/25 | Loss: 0.00107132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010713172378018498, 0.0010713172378018498, 0.0010713172378018498, 0.0010713172378018498, 0.0010713172378018498]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010713172378018498

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38403082
Iteration 2/25 | Loss: 0.00073086
Iteration 3/25 | Loss: 0.00073086
Iteration 4/25 | Loss: 0.00073086
Iteration 5/25 | Loss: 0.00073086
Iteration 6/25 | Loss: 0.00073086
Iteration 7/25 | Loss: 0.00073086
Iteration 8/25 | Loss: 0.00073086
Iteration 9/25 | Loss: 0.00073086
Iteration 10/25 | Loss: 0.00073086
Iteration 11/25 | Loss: 0.00073086
Iteration 12/25 | Loss: 0.00073086
Iteration 13/25 | Loss: 0.00073086
Iteration 14/25 | Loss: 0.00073086
Iteration 15/25 | Loss: 0.00073086
Iteration 16/25 | Loss: 0.00073086
Iteration 17/25 | Loss: 0.00073086
Iteration 18/25 | Loss: 0.00073086
Iteration 19/25 | Loss: 0.00073086
Iteration 20/25 | Loss: 0.00073086
Iteration 21/25 | Loss: 0.00073086
Iteration 22/25 | Loss: 0.00073086
Iteration 23/25 | Loss: 0.00073086
Iteration 24/25 | Loss: 0.00073086
Iteration 25/25 | Loss: 0.00073086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073086
Iteration 2/1000 | Loss: 0.00002396
Iteration 3/1000 | Loss: 0.00001819
Iteration 4/1000 | Loss: 0.00001682
Iteration 5/1000 | Loss: 0.00001596
Iteration 6/1000 | Loss: 0.00001556
Iteration 7/1000 | Loss: 0.00001505
Iteration 8/1000 | Loss: 0.00001467
Iteration 9/1000 | Loss: 0.00001441
Iteration 10/1000 | Loss: 0.00001410
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001396
Iteration 13/1000 | Loss: 0.00001387
Iteration 14/1000 | Loss: 0.00001387
Iteration 15/1000 | Loss: 0.00001383
Iteration 16/1000 | Loss: 0.00001380
Iteration 17/1000 | Loss: 0.00001380
Iteration 18/1000 | Loss: 0.00001372
Iteration 19/1000 | Loss: 0.00001367
Iteration 20/1000 | Loss: 0.00001366
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001359
Iteration 23/1000 | Loss: 0.00001359
Iteration 24/1000 | Loss: 0.00001359
Iteration 25/1000 | Loss: 0.00001359
Iteration 26/1000 | Loss: 0.00001359
Iteration 27/1000 | Loss: 0.00001359
Iteration 28/1000 | Loss: 0.00001359
Iteration 29/1000 | Loss: 0.00001359
Iteration 30/1000 | Loss: 0.00001359
Iteration 31/1000 | Loss: 0.00001359
Iteration 32/1000 | Loss: 0.00001358
Iteration 33/1000 | Loss: 0.00001358
Iteration 34/1000 | Loss: 0.00001358
Iteration 35/1000 | Loss: 0.00001357
Iteration 36/1000 | Loss: 0.00001357
Iteration 37/1000 | Loss: 0.00001356
Iteration 38/1000 | Loss: 0.00001355
Iteration 39/1000 | Loss: 0.00001355
Iteration 40/1000 | Loss: 0.00001353
Iteration 41/1000 | Loss: 0.00001353
Iteration 42/1000 | Loss: 0.00001352
Iteration 43/1000 | Loss: 0.00001352
Iteration 44/1000 | Loss: 0.00001352
Iteration 45/1000 | Loss: 0.00001352
Iteration 46/1000 | Loss: 0.00001351
Iteration 47/1000 | Loss: 0.00001351
Iteration 48/1000 | Loss: 0.00001351
Iteration 49/1000 | Loss: 0.00001348
Iteration 50/1000 | Loss: 0.00001348
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001346
Iteration 54/1000 | Loss: 0.00001345
Iteration 55/1000 | Loss: 0.00001345
Iteration 56/1000 | Loss: 0.00001344
Iteration 57/1000 | Loss: 0.00001344
Iteration 58/1000 | Loss: 0.00001344
Iteration 59/1000 | Loss: 0.00001343
Iteration 60/1000 | Loss: 0.00001342
Iteration 61/1000 | Loss: 0.00001342
Iteration 62/1000 | Loss: 0.00001342
Iteration 63/1000 | Loss: 0.00001342
Iteration 64/1000 | Loss: 0.00001342
Iteration 65/1000 | Loss: 0.00001341
Iteration 66/1000 | Loss: 0.00001341
Iteration 67/1000 | Loss: 0.00001341
Iteration 68/1000 | Loss: 0.00001341
Iteration 69/1000 | Loss: 0.00001341
Iteration 70/1000 | Loss: 0.00001341
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001341
Iteration 74/1000 | Loss: 0.00001341
Iteration 75/1000 | Loss: 0.00001340
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001340
Iteration 78/1000 | Loss: 0.00001340
Iteration 79/1000 | Loss: 0.00001340
Iteration 80/1000 | Loss: 0.00001340
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001337
Iteration 83/1000 | Loss: 0.00001336
Iteration 84/1000 | Loss: 0.00001336
Iteration 85/1000 | Loss: 0.00001335
Iteration 86/1000 | Loss: 0.00001335
Iteration 87/1000 | Loss: 0.00001335
Iteration 88/1000 | Loss: 0.00001330
Iteration 89/1000 | Loss: 0.00001330
Iteration 90/1000 | Loss: 0.00001330
Iteration 91/1000 | Loss: 0.00001330
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001325
Iteration 94/1000 | Loss: 0.00001324
Iteration 95/1000 | Loss: 0.00001324
Iteration 96/1000 | Loss: 0.00001323
Iteration 97/1000 | Loss: 0.00001322
Iteration 98/1000 | Loss: 0.00001322
Iteration 99/1000 | Loss: 0.00001322
Iteration 100/1000 | Loss: 0.00001322
Iteration 101/1000 | Loss: 0.00001322
Iteration 102/1000 | Loss: 0.00001322
Iteration 103/1000 | Loss: 0.00001321
Iteration 104/1000 | Loss: 0.00001321
Iteration 105/1000 | Loss: 0.00001321
Iteration 106/1000 | Loss: 0.00001321
Iteration 107/1000 | Loss: 0.00001321
Iteration 108/1000 | Loss: 0.00001321
Iteration 109/1000 | Loss: 0.00001321
Iteration 110/1000 | Loss: 0.00001321
Iteration 111/1000 | Loss: 0.00001321
Iteration 112/1000 | Loss: 0.00001321
Iteration 113/1000 | Loss: 0.00001321
Iteration 114/1000 | Loss: 0.00001320
Iteration 115/1000 | Loss: 0.00001320
Iteration 116/1000 | Loss: 0.00001320
Iteration 117/1000 | Loss: 0.00001320
Iteration 118/1000 | Loss: 0.00001320
Iteration 119/1000 | Loss: 0.00001320
Iteration 120/1000 | Loss: 0.00001320
Iteration 121/1000 | Loss: 0.00001320
Iteration 122/1000 | Loss: 0.00001320
Iteration 123/1000 | Loss: 0.00001320
Iteration 124/1000 | Loss: 0.00001320
Iteration 125/1000 | Loss: 0.00001320
Iteration 126/1000 | Loss: 0.00001319
Iteration 127/1000 | Loss: 0.00001319
Iteration 128/1000 | Loss: 0.00001319
Iteration 129/1000 | Loss: 0.00001319
Iteration 130/1000 | Loss: 0.00001319
Iteration 131/1000 | Loss: 0.00001319
Iteration 132/1000 | Loss: 0.00001318
Iteration 133/1000 | Loss: 0.00001318
Iteration 134/1000 | Loss: 0.00001318
Iteration 135/1000 | Loss: 0.00001318
Iteration 136/1000 | Loss: 0.00001318
Iteration 137/1000 | Loss: 0.00001318
Iteration 138/1000 | Loss: 0.00001318
Iteration 139/1000 | Loss: 0.00001318
Iteration 140/1000 | Loss: 0.00001317
Iteration 141/1000 | Loss: 0.00001317
Iteration 142/1000 | Loss: 0.00001317
Iteration 143/1000 | Loss: 0.00001317
Iteration 144/1000 | Loss: 0.00001317
Iteration 145/1000 | Loss: 0.00001317
Iteration 146/1000 | Loss: 0.00001317
Iteration 147/1000 | Loss: 0.00001316
Iteration 148/1000 | Loss: 0.00001316
Iteration 149/1000 | Loss: 0.00001316
Iteration 150/1000 | Loss: 0.00001316
Iteration 151/1000 | Loss: 0.00001316
Iteration 152/1000 | Loss: 0.00001316
Iteration 153/1000 | Loss: 0.00001316
Iteration 154/1000 | Loss: 0.00001316
Iteration 155/1000 | Loss: 0.00001316
Iteration 156/1000 | Loss: 0.00001316
Iteration 157/1000 | Loss: 0.00001316
Iteration 158/1000 | Loss: 0.00001316
Iteration 159/1000 | Loss: 0.00001316
Iteration 160/1000 | Loss: 0.00001316
Iteration 161/1000 | Loss: 0.00001316
Iteration 162/1000 | Loss: 0.00001316
Iteration 163/1000 | Loss: 0.00001316
Iteration 164/1000 | Loss: 0.00001316
Iteration 165/1000 | Loss: 0.00001316
Iteration 166/1000 | Loss: 0.00001316
Iteration 167/1000 | Loss: 0.00001316
Iteration 168/1000 | Loss: 0.00001316
Iteration 169/1000 | Loss: 0.00001316
Iteration 170/1000 | Loss: 0.00001316
Iteration 171/1000 | Loss: 0.00001316
Iteration 172/1000 | Loss: 0.00001316
Iteration 173/1000 | Loss: 0.00001316
Iteration 174/1000 | Loss: 0.00001316
Iteration 175/1000 | Loss: 0.00001316
Iteration 176/1000 | Loss: 0.00001316
Iteration 177/1000 | Loss: 0.00001316
Iteration 178/1000 | Loss: 0.00001316
Iteration 179/1000 | Loss: 0.00001316
Iteration 180/1000 | Loss: 0.00001316
Iteration 181/1000 | Loss: 0.00001316
Iteration 182/1000 | Loss: 0.00001316
Iteration 183/1000 | Loss: 0.00001316
Iteration 184/1000 | Loss: 0.00001316
Iteration 185/1000 | Loss: 0.00001316
Iteration 186/1000 | Loss: 0.00001316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.3158492038201075e-05, 1.3158492038201075e-05, 1.3158492038201075e-05, 1.3158492038201075e-05, 1.3158492038201075e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3158492038201075e-05

Optimization complete. Final v2v error: 3.0888898372650146 mm

Highest mean error: 3.2389895915985107 mm for frame 139

Lowest mean error: 2.961000442504883 mm for frame 46

Saving results

Total time: 40.83272361755371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799633
Iteration 2/25 | Loss: 0.00114201
Iteration 3/25 | Loss: 0.00106026
Iteration 4/25 | Loss: 0.00104544
Iteration 5/25 | Loss: 0.00104300
Iteration 6/25 | Loss: 0.00104300
Iteration 7/25 | Loss: 0.00104300
Iteration 8/25 | Loss: 0.00104300
Iteration 9/25 | Loss: 0.00104300
Iteration 10/25 | Loss: 0.00104300
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001042995136231184, 0.001042995136231184, 0.001042995136231184, 0.001042995136231184, 0.001042995136231184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001042995136231184

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42422354
Iteration 2/25 | Loss: 0.00067432
Iteration 3/25 | Loss: 0.00067431
Iteration 4/25 | Loss: 0.00067431
Iteration 5/25 | Loss: 0.00067431
Iteration 6/25 | Loss: 0.00067431
Iteration 7/25 | Loss: 0.00067431
Iteration 8/25 | Loss: 0.00067431
Iteration 9/25 | Loss: 0.00067431
Iteration 10/25 | Loss: 0.00067431
Iteration 11/25 | Loss: 0.00067431
Iteration 12/25 | Loss: 0.00067431
Iteration 13/25 | Loss: 0.00067431
Iteration 14/25 | Loss: 0.00067431
Iteration 15/25 | Loss: 0.00067431
Iteration 16/25 | Loss: 0.00067431
Iteration 17/25 | Loss: 0.00067431
Iteration 18/25 | Loss: 0.00067431
Iteration 19/25 | Loss: 0.00067431
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006743100238963962, 0.0006743100238963962, 0.0006743100238963962, 0.0006743100238963962, 0.0006743100238963962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006743100238963962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067431
Iteration 2/1000 | Loss: 0.00002355
Iteration 3/1000 | Loss: 0.00001702
Iteration 4/1000 | Loss: 0.00001503
Iteration 5/1000 | Loss: 0.00001421
Iteration 6/1000 | Loss: 0.00001355
Iteration 7/1000 | Loss: 0.00001311
Iteration 8/1000 | Loss: 0.00001285
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001234
Iteration 12/1000 | Loss: 0.00001230
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001215
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001214
Iteration 19/1000 | Loss: 0.00001213
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001210
Iteration 22/1000 | Loss: 0.00001209
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001195
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001194
Iteration 33/1000 | Loss: 0.00001193
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001192
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001192
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001192
Iteration 40/1000 | Loss: 0.00001192
Iteration 41/1000 | Loss: 0.00001191
Iteration 42/1000 | Loss: 0.00001191
Iteration 43/1000 | Loss: 0.00001191
Iteration 44/1000 | Loss: 0.00001191
Iteration 45/1000 | Loss: 0.00001191
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001187
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001186
Iteration 52/1000 | Loss: 0.00001183
Iteration 53/1000 | Loss: 0.00001183
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001181
Iteration 63/1000 | Loss: 0.00001181
Iteration 64/1000 | Loss: 0.00001181
Iteration 65/1000 | Loss: 0.00001181
Iteration 66/1000 | Loss: 0.00001181
Iteration 67/1000 | Loss: 0.00001181
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001179
Iteration 70/1000 | Loss: 0.00001179
Iteration 71/1000 | Loss: 0.00001179
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001178
Iteration 74/1000 | Loss: 0.00001178
Iteration 75/1000 | Loss: 0.00001178
Iteration 76/1000 | Loss: 0.00001178
Iteration 77/1000 | Loss: 0.00001178
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001178
Iteration 82/1000 | Loss: 0.00001177
Iteration 83/1000 | Loss: 0.00001177
Iteration 84/1000 | Loss: 0.00001176
Iteration 85/1000 | Loss: 0.00001176
Iteration 86/1000 | Loss: 0.00001176
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001175
Iteration 95/1000 | Loss: 0.00001175
Iteration 96/1000 | Loss: 0.00001175
Iteration 97/1000 | Loss: 0.00001174
Iteration 98/1000 | Loss: 0.00001174
Iteration 99/1000 | Loss: 0.00001174
Iteration 100/1000 | Loss: 0.00001174
Iteration 101/1000 | Loss: 0.00001174
Iteration 102/1000 | Loss: 0.00001174
Iteration 103/1000 | Loss: 0.00001174
Iteration 104/1000 | Loss: 0.00001174
Iteration 105/1000 | Loss: 0.00001173
Iteration 106/1000 | Loss: 0.00001173
Iteration 107/1000 | Loss: 0.00001173
Iteration 108/1000 | Loss: 0.00001173
Iteration 109/1000 | Loss: 0.00001173
Iteration 110/1000 | Loss: 0.00001173
Iteration 111/1000 | Loss: 0.00001173
Iteration 112/1000 | Loss: 0.00001173
Iteration 113/1000 | Loss: 0.00001173
Iteration 114/1000 | Loss: 0.00001173
Iteration 115/1000 | Loss: 0.00001173
Iteration 116/1000 | Loss: 0.00001172
Iteration 117/1000 | Loss: 0.00001172
Iteration 118/1000 | Loss: 0.00001172
Iteration 119/1000 | Loss: 0.00001172
Iteration 120/1000 | Loss: 0.00001172
Iteration 121/1000 | Loss: 0.00001172
Iteration 122/1000 | Loss: 0.00001172
Iteration 123/1000 | Loss: 0.00001172
Iteration 124/1000 | Loss: 0.00001172
Iteration 125/1000 | Loss: 0.00001172
Iteration 126/1000 | Loss: 0.00001171
Iteration 127/1000 | Loss: 0.00001171
Iteration 128/1000 | Loss: 0.00001171
Iteration 129/1000 | Loss: 0.00001171
Iteration 130/1000 | Loss: 0.00001171
Iteration 131/1000 | Loss: 0.00001171
Iteration 132/1000 | Loss: 0.00001171
Iteration 133/1000 | Loss: 0.00001171
Iteration 134/1000 | Loss: 0.00001170
Iteration 135/1000 | Loss: 0.00001170
Iteration 136/1000 | Loss: 0.00001170
Iteration 137/1000 | Loss: 0.00001170
Iteration 138/1000 | Loss: 0.00001170
Iteration 139/1000 | Loss: 0.00001170
Iteration 140/1000 | Loss: 0.00001170
Iteration 141/1000 | Loss: 0.00001170
Iteration 142/1000 | Loss: 0.00001170
Iteration 143/1000 | Loss: 0.00001170
Iteration 144/1000 | Loss: 0.00001170
Iteration 145/1000 | Loss: 0.00001170
Iteration 146/1000 | Loss: 0.00001170
Iteration 147/1000 | Loss: 0.00001169
Iteration 148/1000 | Loss: 0.00001169
Iteration 149/1000 | Loss: 0.00001169
Iteration 150/1000 | Loss: 0.00001169
Iteration 151/1000 | Loss: 0.00001168
Iteration 152/1000 | Loss: 0.00001168
Iteration 153/1000 | Loss: 0.00001168
Iteration 154/1000 | Loss: 0.00001167
Iteration 155/1000 | Loss: 0.00001167
Iteration 156/1000 | Loss: 0.00001167
Iteration 157/1000 | Loss: 0.00001167
Iteration 158/1000 | Loss: 0.00001167
Iteration 159/1000 | Loss: 0.00001167
Iteration 160/1000 | Loss: 0.00001167
Iteration 161/1000 | Loss: 0.00001167
Iteration 162/1000 | Loss: 0.00001167
Iteration 163/1000 | Loss: 0.00001167
Iteration 164/1000 | Loss: 0.00001167
Iteration 165/1000 | Loss: 0.00001166
Iteration 166/1000 | Loss: 0.00001166
Iteration 167/1000 | Loss: 0.00001166
Iteration 168/1000 | Loss: 0.00001166
Iteration 169/1000 | Loss: 0.00001166
Iteration 170/1000 | Loss: 0.00001166
Iteration 171/1000 | Loss: 0.00001166
Iteration 172/1000 | Loss: 0.00001166
Iteration 173/1000 | Loss: 0.00001165
Iteration 174/1000 | Loss: 0.00001165
Iteration 175/1000 | Loss: 0.00001165
Iteration 176/1000 | Loss: 0.00001165
Iteration 177/1000 | Loss: 0.00001165
Iteration 178/1000 | Loss: 0.00001165
Iteration 179/1000 | Loss: 0.00001165
Iteration 180/1000 | Loss: 0.00001165
Iteration 181/1000 | Loss: 0.00001164
Iteration 182/1000 | Loss: 0.00001164
Iteration 183/1000 | Loss: 0.00001164
Iteration 184/1000 | Loss: 0.00001164
Iteration 185/1000 | Loss: 0.00001164
Iteration 186/1000 | Loss: 0.00001164
Iteration 187/1000 | Loss: 0.00001164
Iteration 188/1000 | Loss: 0.00001164
Iteration 189/1000 | Loss: 0.00001163
Iteration 190/1000 | Loss: 0.00001163
Iteration 191/1000 | Loss: 0.00001163
Iteration 192/1000 | Loss: 0.00001163
Iteration 193/1000 | Loss: 0.00001163
Iteration 194/1000 | Loss: 0.00001163
Iteration 195/1000 | Loss: 0.00001163
Iteration 196/1000 | Loss: 0.00001163
Iteration 197/1000 | Loss: 0.00001163
Iteration 198/1000 | Loss: 0.00001163
Iteration 199/1000 | Loss: 0.00001163
Iteration 200/1000 | Loss: 0.00001162
Iteration 201/1000 | Loss: 0.00001162
Iteration 202/1000 | Loss: 0.00001162
Iteration 203/1000 | Loss: 0.00001162
Iteration 204/1000 | Loss: 0.00001162
Iteration 205/1000 | Loss: 0.00001162
Iteration 206/1000 | Loss: 0.00001162
Iteration 207/1000 | Loss: 0.00001161
Iteration 208/1000 | Loss: 0.00001161
Iteration 209/1000 | Loss: 0.00001161
Iteration 210/1000 | Loss: 0.00001161
Iteration 211/1000 | Loss: 0.00001161
Iteration 212/1000 | Loss: 0.00001161
Iteration 213/1000 | Loss: 0.00001161
Iteration 214/1000 | Loss: 0.00001161
Iteration 215/1000 | Loss: 0.00001161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.1608142813201994e-05, 1.1608142813201994e-05, 1.1608142813201994e-05, 1.1608142813201994e-05, 1.1608142813201994e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1608142813201994e-05

Optimization complete. Final v2v error: 2.943793296813965 mm

Highest mean error: 3.1533753871917725 mm for frame 155

Lowest mean error: 2.770012617111206 mm for frame 65

Saving results

Total time: 46.35998892784119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_fiona_posed_004/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_fiona_posed_004/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833780
Iteration 2/25 | Loss: 0.00120465
Iteration 3/25 | Loss: 0.00107752
Iteration 4/25 | Loss: 0.00106208
Iteration 5/25 | Loss: 0.00105701
Iteration 6/25 | Loss: 0.00105599
Iteration 7/25 | Loss: 0.00105599
Iteration 8/25 | Loss: 0.00105599
Iteration 9/25 | Loss: 0.00105599
Iteration 10/25 | Loss: 0.00105599
Iteration 11/25 | Loss: 0.00105599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010559933725744486, 0.0010559933725744486, 0.0010559933725744486, 0.0010559933725744486, 0.0010559933725744486]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010559933725744486

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02696800
Iteration 2/25 | Loss: 0.00076880
Iteration 3/25 | Loss: 0.00076880
Iteration 4/25 | Loss: 0.00076880
Iteration 5/25 | Loss: 0.00076880
Iteration 6/25 | Loss: 0.00076880
Iteration 7/25 | Loss: 0.00076880
Iteration 8/25 | Loss: 0.00076880
Iteration 9/25 | Loss: 0.00076880
Iteration 10/25 | Loss: 0.00076880
Iteration 11/25 | Loss: 0.00076880
Iteration 12/25 | Loss: 0.00076880
Iteration 13/25 | Loss: 0.00076880
Iteration 14/25 | Loss: 0.00076880
Iteration 15/25 | Loss: 0.00076880
Iteration 16/25 | Loss: 0.00076880
Iteration 17/25 | Loss: 0.00076880
Iteration 18/25 | Loss: 0.00076880
Iteration 19/25 | Loss: 0.00076880
Iteration 20/25 | Loss: 0.00076880
Iteration 21/25 | Loss: 0.00076880
Iteration 22/25 | Loss: 0.00076880
Iteration 23/25 | Loss: 0.00076880
Iteration 24/25 | Loss: 0.00076880
Iteration 25/25 | Loss: 0.00076880

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076880
Iteration 2/1000 | Loss: 0.00002210
Iteration 3/1000 | Loss: 0.00001493
Iteration 4/1000 | Loss: 0.00001261
Iteration 5/1000 | Loss: 0.00001172
Iteration 6/1000 | Loss: 0.00001115
Iteration 7/1000 | Loss: 0.00001078
Iteration 8/1000 | Loss: 0.00001062
Iteration 9/1000 | Loss: 0.00001053
Iteration 10/1000 | Loss: 0.00001049
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001020
Iteration 14/1000 | Loss: 0.00001019
Iteration 15/1000 | Loss: 0.00001018
Iteration 16/1000 | Loss: 0.00001013
Iteration 17/1000 | Loss: 0.00001013
Iteration 18/1000 | Loss: 0.00001011
Iteration 19/1000 | Loss: 0.00001011
Iteration 20/1000 | Loss: 0.00001009
Iteration 21/1000 | Loss: 0.00001009
Iteration 22/1000 | Loss: 0.00001008
Iteration 23/1000 | Loss: 0.00001006
Iteration 24/1000 | Loss: 0.00001006
Iteration 25/1000 | Loss: 0.00001005
Iteration 26/1000 | Loss: 0.00001002
Iteration 27/1000 | Loss: 0.00001000
Iteration 28/1000 | Loss: 0.00001000
Iteration 29/1000 | Loss: 0.00000999
Iteration 30/1000 | Loss: 0.00000999
Iteration 31/1000 | Loss: 0.00000999
Iteration 32/1000 | Loss: 0.00000999
Iteration 33/1000 | Loss: 0.00000999
Iteration 34/1000 | Loss: 0.00000998
Iteration 35/1000 | Loss: 0.00000998
Iteration 36/1000 | Loss: 0.00000995
Iteration 37/1000 | Loss: 0.00000995
Iteration 38/1000 | Loss: 0.00000995
Iteration 39/1000 | Loss: 0.00000994
Iteration 40/1000 | Loss: 0.00000994
Iteration 41/1000 | Loss: 0.00000994
Iteration 42/1000 | Loss: 0.00000994
Iteration 43/1000 | Loss: 0.00000994
Iteration 44/1000 | Loss: 0.00000994
Iteration 45/1000 | Loss: 0.00000994
Iteration 46/1000 | Loss: 0.00000994
Iteration 47/1000 | Loss: 0.00000994
Iteration 48/1000 | Loss: 0.00000994
Iteration 49/1000 | Loss: 0.00000992
Iteration 50/1000 | Loss: 0.00000992
Iteration 51/1000 | Loss: 0.00000992
Iteration 52/1000 | Loss: 0.00000991
Iteration 53/1000 | Loss: 0.00000991
Iteration 54/1000 | Loss: 0.00000991
Iteration 55/1000 | Loss: 0.00000991
Iteration 56/1000 | Loss: 0.00000990
Iteration 57/1000 | Loss: 0.00000990
Iteration 58/1000 | Loss: 0.00000990
Iteration 59/1000 | Loss: 0.00000990
Iteration 60/1000 | Loss: 0.00000990
Iteration 61/1000 | Loss: 0.00000989
Iteration 62/1000 | Loss: 0.00000989
Iteration 63/1000 | Loss: 0.00000989
Iteration 64/1000 | Loss: 0.00000988
Iteration 65/1000 | Loss: 0.00000988
Iteration 66/1000 | Loss: 0.00000988
Iteration 67/1000 | Loss: 0.00000988
Iteration 68/1000 | Loss: 0.00000988
Iteration 69/1000 | Loss: 0.00000987
Iteration 70/1000 | Loss: 0.00000987
Iteration 71/1000 | Loss: 0.00000987
Iteration 72/1000 | Loss: 0.00000987
Iteration 73/1000 | Loss: 0.00000987
Iteration 74/1000 | Loss: 0.00000987
Iteration 75/1000 | Loss: 0.00000987
Iteration 76/1000 | Loss: 0.00000987
Iteration 77/1000 | Loss: 0.00000987
Iteration 78/1000 | Loss: 0.00000987
Iteration 79/1000 | Loss: 0.00000986
Iteration 80/1000 | Loss: 0.00000986
Iteration 81/1000 | Loss: 0.00000985
Iteration 82/1000 | Loss: 0.00000985
Iteration 83/1000 | Loss: 0.00000985
Iteration 84/1000 | Loss: 0.00000985
Iteration 85/1000 | Loss: 0.00000984
Iteration 86/1000 | Loss: 0.00000984
Iteration 87/1000 | Loss: 0.00000984
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000983
Iteration 90/1000 | Loss: 0.00000983
Iteration 91/1000 | Loss: 0.00000983
Iteration 92/1000 | Loss: 0.00000983
Iteration 93/1000 | Loss: 0.00000982
Iteration 94/1000 | Loss: 0.00000982
Iteration 95/1000 | Loss: 0.00000982
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000981
Iteration 98/1000 | Loss: 0.00000980
Iteration 99/1000 | Loss: 0.00000980
Iteration 100/1000 | Loss: 0.00000979
Iteration 101/1000 | Loss: 0.00000979
Iteration 102/1000 | Loss: 0.00000979
Iteration 103/1000 | Loss: 0.00000978
Iteration 104/1000 | Loss: 0.00000978
Iteration 105/1000 | Loss: 0.00000978
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000977
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000976
Iteration 113/1000 | Loss: 0.00000976
Iteration 114/1000 | Loss: 0.00000976
Iteration 115/1000 | Loss: 0.00000975
Iteration 116/1000 | Loss: 0.00000975
Iteration 117/1000 | Loss: 0.00000975
Iteration 118/1000 | Loss: 0.00000975
Iteration 119/1000 | Loss: 0.00000975
Iteration 120/1000 | Loss: 0.00000975
Iteration 121/1000 | Loss: 0.00000974
Iteration 122/1000 | Loss: 0.00000974
Iteration 123/1000 | Loss: 0.00000974
Iteration 124/1000 | Loss: 0.00000974
Iteration 125/1000 | Loss: 0.00000974
Iteration 126/1000 | Loss: 0.00000974
Iteration 127/1000 | Loss: 0.00000974
Iteration 128/1000 | Loss: 0.00000974
Iteration 129/1000 | Loss: 0.00000973
Iteration 130/1000 | Loss: 0.00000973
Iteration 131/1000 | Loss: 0.00000973
Iteration 132/1000 | Loss: 0.00000973
Iteration 133/1000 | Loss: 0.00000973
Iteration 134/1000 | Loss: 0.00000973
Iteration 135/1000 | Loss: 0.00000973
Iteration 136/1000 | Loss: 0.00000973
Iteration 137/1000 | Loss: 0.00000972
Iteration 138/1000 | Loss: 0.00000972
Iteration 139/1000 | Loss: 0.00000972
Iteration 140/1000 | Loss: 0.00000972
Iteration 141/1000 | Loss: 0.00000972
Iteration 142/1000 | Loss: 0.00000972
Iteration 143/1000 | Loss: 0.00000972
Iteration 144/1000 | Loss: 0.00000972
Iteration 145/1000 | Loss: 0.00000972
Iteration 146/1000 | Loss: 0.00000972
Iteration 147/1000 | Loss: 0.00000972
Iteration 148/1000 | Loss: 0.00000972
Iteration 149/1000 | Loss: 0.00000972
Iteration 150/1000 | Loss: 0.00000972
Iteration 151/1000 | Loss: 0.00000971
Iteration 152/1000 | Loss: 0.00000971
Iteration 153/1000 | Loss: 0.00000971
Iteration 154/1000 | Loss: 0.00000971
Iteration 155/1000 | Loss: 0.00000971
Iteration 156/1000 | Loss: 0.00000971
Iteration 157/1000 | Loss: 0.00000971
Iteration 158/1000 | Loss: 0.00000971
Iteration 159/1000 | Loss: 0.00000971
Iteration 160/1000 | Loss: 0.00000971
Iteration 161/1000 | Loss: 0.00000970
Iteration 162/1000 | Loss: 0.00000970
Iteration 163/1000 | Loss: 0.00000970
Iteration 164/1000 | Loss: 0.00000970
Iteration 165/1000 | Loss: 0.00000970
Iteration 166/1000 | Loss: 0.00000970
Iteration 167/1000 | Loss: 0.00000970
Iteration 168/1000 | Loss: 0.00000970
Iteration 169/1000 | Loss: 0.00000969
Iteration 170/1000 | Loss: 0.00000969
Iteration 171/1000 | Loss: 0.00000969
Iteration 172/1000 | Loss: 0.00000969
Iteration 173/1000 | Loss: 0.00000969
Iteration 174/1000 | Loss: 0.00000969
Iteration 175/1000 | Loss: 0.00000969
Iteration 176/1000 | Loss: 0.00000968
Iteration 177/1000 | Loss: 0.00000968
Iteration 178/1000 | Loss: 0.00000968
Iteration 179/1000 | Loss: 0.00000968
Iteration 180/1000 | Loss: 0.00000968
Iteration 181/1000 | Loss: 0.00000968
Iteration 182/1000 | Loss: 0.00000968
Iteration 183/1000 | Loss: 0.00000968
Iteration 184/1000 | Loss: 0.00000968
Iteration 185/1000 | Loss: 0.00000968
Iteration 186/1000 | Loss: 0.00000967
Iteration 187/1000 | Loss: 0.00000967
Iteration 188/1000 | Loss: 0.00000967
Iteration 189/1000 | Loss: 0.00000967
Iteration 190/1000 | Loss: 0.00000967
Iteration 191/1000 | Loss: 0.00000966
Iteration 192/1000 | Loss: 0.00000966
Iteration 193/1000 | Loss: 0.00000966
Iteration 194/1000 | Loss: 0.00000966
Iteration 195/1000 | Loss: 0.00000966
Iteration 196/1000 | Loss: 0.00000966
Iteration 197/1000 | Loss: 0.00000966
Iteration 198/1000 | Loss: 0.00000966
Iteration 199/1000 | Loss: 0.00000966
Iteration 200/1000 | Loss: 0.00000966
Iteration 201/1000 | Loss: 0.00000966
Iteration 202/1000 | Loss: 0.00000966
Iteration 203/1000 | Loss: 0.00000966
Iteration 204/1000 | Loss: 0.00000966
Iteration 205/1000 | Loss: 0.00000965
Iteration 206/1000 | Loss: 0.00000965
Iteration 207/1000 | Loss: 0.00000965
Iteration 208/1000 | Loss: 0.00000965
Iteration 209/1000 | Loss: 0.00000965
Iteration 210/1000 | Loss: 0.00000965
Iteration 211/1000 | Loss: 0.00000965
Iteration 212/1000 | Loss: 0.00000965
Iteration 213/1000 | Loss: 0.00000965
Iteration 214/1000 | Loss: 0.00000965
Iteration 215/1000 | Loss: 0.00000965
Iteration 216/1000 | Loss: 0.00000965
Iteration 217/1000 | Loss: 0.00000965
Iteration 218/1000 | Loss: 0.00000965
Iteration 219/1000 | Loss: 0.00000965
Iteration 220/1000 | Loss: 0.00000965
Iteration 221/1000 | Loss: 0.00000965
Iteration 222/1000 | Loss: 0.00000965
Iteration 223/1000 | Loss: 0.00000965
Iteration 224/1000 | Loss: 0.00000965
Iteration 225/1000 | Loss: 0.00000965
Iteration 226/1000 | Loss: 0.00000964
Iteration 227/1000 | Loss: 0.00000964
Iteration 228/1000 | Loss: 0.00000964
Iteration 229/1000 | Loss: 0.00000964
Iteration 230/1000 | Loss: 0.00000964
Iteration 231/1000 | Loss: 0.00000964
Iteration 232/1000 | Loss: 0.00000964
Iteration 233/1000 | Loss: 0.00000964
Iteration 234/1000 | Loss: 0.00000964
Iteration 235/1000 | Loss: 0.00000964
Iteration 236/1000 | Loss: 0.00000964
Iteration 237/1000 | Loss: 0.00000964
Iteration 238/1000 | Loss: 0.00000964
Iteration 239/1000 | Loss: 0.00000964
Iteration 240/1000 | Loss: 0.00000964
Iteration 241/1000 | Loss: 0.00000964
Iteration 242/1000 | Loss: 0.00000964
Iteration 243/1000 | Loss: 0.00000963
Iteration 244/1000 | Loss: 0.00000963
Iteration 245/1000 | Loss: 0.00000963
Iteration 246/1000 | Loss: 0.00000963
Iteration 247/1000 | Loss: 0.00000963
Iteration 248/1000 | Loss: 0.00000963
Iteration 249/1000 | Loss: 0.00000963
Iteration 250/1000 | Loss: 0.00000963
Iteration 251/1000 | Loss: 0.00000963
Iteration 252/1000 | Loss: 0.00000963
Iteration 253/1000 | Loss: 0.00000963
Iteration 254/1000 | Loss: 0.00000963
Iteration 255/1000 | Loss: 0.00000963
Iteration 256/1000 | Loss: 0.00000963
Iteration 257/1000 | Loss: 0.00000963
Iteration 258/1000 | Loss: 0.00000963
Iteration 259/1000 | Loss: 0.00000963
Iteration 260/1000 | Loss: 0.00000963
Iteration 261/1000 | Loss: 0.00000963
Iteration 262/1000 | Loss: 0.00000963
Iteration 263/1000 | Loss: 0.00000963
Iteration 264/1000 | Loss: 0.00000963
Iteration 265/1000 | Loss: 0.00000963
Iteration 266/1000 | Loss: 0.00000963
Iteration 267/1000 | Loss: 0.00000963
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [9.62719332164852e-06, 9.62719332164852e-06, 9.62719332164852e-06, 9.62719332164852e-06, 9.62719332164852e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.62719332164852e-06

Optimization complete. Final v2v error: 2.5957794189453125 mm

Highest mean error: 3.5251243114471436 mm for frame 92

Lowest mean error: 2.3078644275665283 mm for frame 127

Saving results

Total time: 43.818642377853394
