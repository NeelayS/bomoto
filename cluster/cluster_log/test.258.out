Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=258, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14448-14503
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00710138
Iteration 2/25 | Loss: 0.00158727
Iteration 3/25 | Loss: 0.00137070
Iteration 4/25 | Loss: 0.00121587
Iteration 5/25 | Loss: 0.00119997
Iteration 6/25 | Loss: 0.00119564
Iteration 7/25 | Loss: 0.00116984
Iteration 8/25 | Loss: 0.00116124
Iteration 9/25 | Loss: 0.00115267
Iteration 10/25 | Loss: 0.00115247
Iteration 11/25 | Loss: 0.00115266
Iteration 12/25 | Loss: 0.00115060
Iteration 13/25 | Loss: 0.00114979
Iteration 14/25 | Loss: 0.00114959
Iteration 15/25 | Loss: 0.00114956
Iteration 16/25 | Loss: 0.00114955
Iteration 17/25 | Loss: 0.00114955
Iteration 18/25 | Loss: 0.00114955
Iteration 19/25 | Loss: 0.00114953
Iteration 20/25 | Loss: 0.00114953
Iteration 21/25 | Loss: 0.00114952
Iteration 22/25 | Loss: 0.00114952
Iteration 23/25 | Loss: 0.00114952
Iteration 24/25 | Loss: 0.00114952
Iteration 25/25 | Loss: 0.00114952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.75801134
Iteration 2/25 | Loss: 0.00094628
Iteration 3/25 | Loss: 0.00092880
Iteration 4/25 | Loss: 0.00092880
Iteration 5/25 | Loss: 0.00092880
Iteration 6/25 | Loss: 0.00092880
Iteration 7/25 | Loss: 0.00092880
Iteration 8/25 | Loss: 0.00092880
Iteration 9/25 | Loss: 0.00092880
Iteration 10/25 | Loss: 0.00092880
Iteration 11/25 | Loss: 0.00092880
Iteration 12/25 | Loss: 0.00092880
Iteration 13/25 | Loss: 0.00092880
Iteration 14/25 | Loss: 0.00092880
Iteration 15/25 | Loss: 0.00092880
Iteration 16/25 | Loss: 0.00092880
Iteration 17/25 | Loss: 0.00092880
Iteration 18/25 | Loss: 0.00092880
Iteration 19/25 | Loss: 0.00092880
Iteration 20/25 | Loss: 0.00092880
Iteration 21/25 | Loss: 0.00092880
Iteration 22/25 | Loss: 0.00092880
Iteration 23/25 | Loss: 0.00092880
Iteration 24/25 | Loss: 0.00092880
Iteration 25/25 | Loss: 0.00092880

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092880
Iteration 2/1000 | Loss: 0.00003943
Iteration 3/1000 | Loss: 0.00005184
Iteration 4/1000 | Loss: 0.00008506
Iteration 5/1000 | Loss: 0.00002424
Iteration 6/1000 | Loss: 0.00006546
Iteration 7/1000 | Loss: 0.00001947
Iteration 8/1000 | Loss: 0.00001436
Iteration 9/1000 | Loss: 0.00001398
Iteration 10/1000 | Loss: 0.00001362
Iteration 11/1000 | Loss: 0.00001329
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00002543
Iteration 14/1000 | Loss: 0.00001454
Iteration 15/1000 | Loss: 0.00001509
Iteration 16/1000 | Loss: 0.00001311
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001255
Iteration 21/1000 | Loss: 0.00001251
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001246
Iteration 25/1000 | Loss: 0.00001246
Iteration 26/1000 | Loss: 0.00001241
Iteration 27/1000 | Loss: 0.00001239
Iteration 28/1000 | Loss: 0.00001239
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001238
Iteration 31/1000 | Loss: 0.00001237
Iteration 32/1000 | Loss: 0.00001235
Iteration 33/1000 | Loss: 0.00001235
Iteration 34/1000 | Loss: 0.00001234
Iteration 35/1000 | Loss: 0.00001234
Iteration 36/1000 | Loss: 0.00001234
Iteration 37/1000 | Loss: 0.00001234
Iteration 38/1000 | Loss: 0.00001233
Iteration 39/1000 | Loss: 0.00001232
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001231
Iteration 42/1000 | Loss: 0.00001231
Iteration 43/1000 | Loss: 0.00001231
Iteration 44/1000 | Loss: 0.00001231
Iteration 45/1000 | Loss: 0.00001228
Iteration 46/1000 | Loss: 0.00001227
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001225
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001224
Iteration 52/1000 | Loss: 0.00001224
Iteration 53/1000 | Loss: 0.00001224
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001222
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001221
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001219
Iteration 67/1000 | Loss: 0.00001219
Iteration 68/1000 | Loss: 0.00001219
Iteration 69/1000 | Loss: 0.00001219
Iteration 70/1000 | Loss: 0.00002206
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001215
Iteration 73/1000 | Loss: 0.00001215
Iteration 74/1000 | Loss: 0.00001215
Iteration 75/1000 | Loss: 0.00001215
Iteration 76/1000 | Loss: 0.00001215
Iteration 77/1000 | Loss: 0.00001215
Iteration 78/1000 | Loss: 0.00001215
Iteration 79/1000 | Loss: 0.00001214
Iteration 80/1000 | Loss: 0.00001214
Iteration 81/1000 | Loss: 0.00001214
Iteration 82/1000 | Loss: 0.00001719
Iteration 83/1000 | Loss: 0.00002303
Iteration 84/1000 | Loss: 0.00001756
Iteration 85/1000 | Loss: 0.00001208
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001208
Iteration 88/1000 | Loss: 0.00001208
Iteration 89/1000 | Loss: 0.00001208
Iteration 90/1000 | Loss: 0.00001208
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001208
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00002203
Iteration 101/1000 | Loss: 0.00001206
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001204
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001204
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001204
Iteration 112/1000 | Loss: 0.00001204
Iteration 113/1000 | Loss: 0.00001204
Iteration 114/1000 | Loss: 0.00001204
Iteration 115/1000 | Loss: 0.00001204
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001203
Iteration 118/1000 | Loss: 0.00001203
Iteration 119/1000 | Loss: 0.00001203
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001203
Iteration 124/1000 | Loss: 0.00001203
Iteration 125/1000 | Loss: 0.00001203
Iteration 126/1000 | Loss: 0.00001203
Iteration 127/1000 | Loss: 0.00001203
Iteration 128/1000 | Loss: 0.00001203
Iteration 129/1000 | Loss: 0.00001203
Iteration 130/1000 | Loss: 0.00001203
Iteration 131/1000 | Loss: 0.00001203
Iteration 132/1000 | Loss: 0.00001203
Iteration 133/1000 | Loss: 0.00001203
Iteration 134/1000 | Loss: 0.00001203
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2028498531435616e-05, 1.2028498531435616e-05, 1.2028498531435616e-05, 1.2028498531435616e-05, 1.2028498531435616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2028498531435616e-05

Optimization complete. Final v2v error: 2.9382336139678955 mm

Highest mean error: 3.6344361305236816 mm for frame 43

Lowest mean error: 2.6030237674713135 mm for frame 197

Saving results

Total time: 80.25957822799683
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819084
Iteration 2/25 | Loss: 0.00130897
Iteration 3/25 | Loss: 0.00119488
Iteration 4/25 | Loss: 0.00116959
Iteration 5/25 | Loss: 0.00116755
Iteration 6/25 | Loss: 0.00116534
Iteration 7/25 | Loss: 0.00116456
Iteration 8/25 | Loss: 0.00116389
Iteration 9/25 | Loss: 0.00116342
Iteration 10/25 | Loss: 0.00116324
Iteration 11/25 | Loss: 0.00116316
Iteration 12/25 | Loss: 0.00116313
Iteration 13/25 | Loss: 0.00116313
Iteration 14/25 | Loss: 0.00116312
Iteration 15/25 | Loss: 0.00116312
Iteration 16/25 | Loss: 0.00116312
Iteration 17/25 | Loss: 0.00116312
Iteration 18/25 | Loss: 0.00116312
Iteration 19/25 | Loss: 0.00116312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011631246889010072, 0.0011631246889010072, 0.0011631246889010072, 0.0011631246889010072, 0.0011631246889010072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011631246889010072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.56681180
Iteration 2/25 | Loss: 0.00083369
Iteration 3/25 | Loss: 0.00083369
Iteration 4/25 | Loss: 0.00083369
Iteration 5/25 | Loss: 0.00083369
Iteration 6/25 | Loss: 0.00083369
Iteration 7/25 | Loss: 0.00083369
Iteration 8/25 | Loss: 0.00083369
Iteration 9/25 | Loss: 0.00083369
Iteration 10/25 | Loss: 0.00083369
Iteration 11/25 | Loss: 0.00083369
Iteration 12/25 | Loss: 0.00083369
Iteration 13/25 | Loss: 0.00083369
Iteration 14/25 | Loss: 0.00083369
Iteration 15/25 | Loss: 0.00083369
Iteration 16/25 | Loss: 0.00083369
Iteration 17/25 | Loss: 0.00083369
Iteration 18/25 | Loss: 0.00083369
Iteration 19/25 | Loss: 0.00083369
Iteration 20/25 | Loss: 0.00083369
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008336854516528547, 0.0008336854516528547, 0.0008336854516528547, 0.0008336854516528547, 0.0008336854516528547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008336854516528547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083369
Iteration 2/1000 | Loss: 0.00002042
Iteration 3/1000 | Loss: 0.00001617
Iteration 4/1000 | Loss: 0.00001461
Iteration 5/1000 | Loss: 0.00001399
Iteration 6/1000 | Loss: 0.00001340
Iteration 7/1000 | Loss: 0.00006295
Iteration 8/1000 | Loss: 0.00001296
Iteration 9/1000 | Loss: 0.00001263
Iteration 10/1000 | Loss: 0.00001234
Iteration 11/1000 | Loss: 0.00001215
Iteration 12/1000 | Loss: 0.00001204
Iteration 13/1000 | Loss: 0.00001202
Iteration 14/1000 | Loss: 0.00001202
Iteration 15/1000 | Loss: 0.00001201
Iteration 16/1000 | Loss: 0.00001201
Iteration 17/1000 | Loss: 0.00001200
Iteration 18/1000 | Loss: 0.00001198
Iteration 19/1000 | Loss: 0.00001191
Iteration 20/1000 | Loss: 0.00001191
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001190
Iteration 23/1000 | Loss: 0.00001190
Iteration 24/1000 | Loss: 0.00001189
Iteration 25/1000 | Loss: 0.00001189
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001185
Iteration 29/1000 | Loss: 0.00001185
Iteration 30/1000 | Loss: 0.00001183
Iteration 31/1000 | Loss: 0.00001183
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00005353
Iteration 34/1000 | Loss: 0.00001178
Iteration 35/1000 | Loss: 0.00001174
Iteration 36/1000 | Loss: 0.00001174
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001174
Iteration 39/1000 | Loss: 0.00001173
Iteration 40/1000 | Loss: 0.00001173
Iteration 41/1000 | Loss: 0.00001173
Iteration 42/1000 | Loss: 0.00001173
Iteration 43/1000 | Loss: 0.00001173
Iteration 44/1000 | Loss: 0.00001173
Iteration 45/1000 | Loss: 0.00001172
Iteration 46/1000 | Loss: 0.00001172
Iteration 47/1000 | Loss: 0.00001171
Iteration 48/1000 | Loss: 0.00001171
Iteration 49/1000 | Loss: 0.00001170
Iteration 50/1000 | Loss: 0.00001170
Iteration 51/1000 | Loss: 0.00001169
Iteration 52/1000 | Loss: 0.00001167
Iteration 53/1000 | Loss: 0.00001167
Iteration 54/1000 | Loss: 0.00001167
Iteration 55/1000 | Loss: 0.00001167
Iteration 56/1000 | Loss: 0.00001166
Iteration 57/1000 | Loss: 0.00001166
Iteration 58/1000 | Loss: 0.00001166
Iteration 59/1000 | Loss: 0.00001166
Iteration 60/1000 | Loss: 0.00001166
Iteration 61/1000 | Loss: 0.00001166
Iteration 62/1000 | Loss: 0.00001166
Iteration 63/1000 | Loss: 0.00001166
Iteration 64/1000 | Loss: 0.00001166
Iteration 65/1000 | Loss: 0.00001166
Iteration 66/1000 | Loss: 0.00001166
Iteration 67/1000 | Loss: 0.00001166
Iteration 68/1000 | Loss: 0.00001166
Iteration 69/1000 | Loss: 0.00001165
Iteration 70/1000 | Loss: 0.00001165
Iteration 71/1000 | Loss: 0.00001164
Iteration 72/1000 | Loss: 0.00001163
Iteration 73/1000 | Loss: 0.00001163
Iteration 74/1000 | Loss: 0.00001163
Iteration 75/1000 | Loss: 0.00001162
Iteration 76/1000 | Loss: 0.00001162
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001162
Iteration 81/1000 | Loss: 0.00001162
Iteration 82/1000 | Loss: 0.00001162
Iteration 83/1000 | Loss: 0.00001162
Iteration 84/1000 | Loss: 0.00001161
Iteration 85/1000 | Loss: 0.00001161
Iteration 86/1000 | Loss: 0.00001161
Iteration 87/1000 | Loss: 0.00001161
Iteration 88/1000 | Loss: 0.00001161
Iteration 89/1000 | Loss: 0.00001161
Iteration 90/1000 | Loss: 0.00001161
Iteration 91/1000 | Loss: 0.00001161
Iteration 92/1000 | Loss: 0.00001161
Iteration 93/1000 | Loss: 0.00001161
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001160
Iteration 96/1000 | Loss: 0.00001160
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001160
Iteration 100/1000 | Loss: 0.00001160
Iteration 101/1000 | Loss: 0.00001160
Iteration 102/1000 | Loss: 0.00001160
Iteration 103/1000 | Loss: 0.00001160
Iteration 104/1000 | Loss: 0.00001160
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001158
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001157
Iteration 120/1000 | Loss: 0.00001157
Iteration 121/1000 | Loss: 0.00001156
Iteration 122/1000 | Loss: 0.00001155
Iteration 123/1000 | Loss: 0.00001155
Iteration 124/1000 | Loss: 0.00001155
Iteration 125/1000 | Loss: 0.00001154
Iteration 126/1000 | Loss: 0.00001154
Iteration 127/1000 | Loss: 0.00001154
Iteration 128/1000 | Loss: 0.00001154
Iteration 129/1000 | Loss: 0.00001154
Iteration 130/1000 | Loss: 0.00001154
Iteration 131/1000 | Loss: 0.00001154
Iteration 132/1000 | Loss: 0.00001154
Iteration 133/1000 | Loss: 0.00001154
Iteration 134/1000 | Loss: 0.00001154
Iteration 135/1000 | Loss: 0.00001154
Iteration 136/1000 | Loss: 0.00001154
Iteration 137/1000 | Loss: 0.00001154
Iteration 138/1000 | Loss: 0.00001153
Iteration 139/1000 | Loss: 0.00001153
Iteration 140/1000 | Loss: 0.00001153
Iteration 141/1000 | Loss: 0.00001153
Iteration 142/1000 | Loss: 0.00001153
Iteration 143/1000 | Loss: 0.00001153
Iteration 144/1000 | Loss: 0.00001153
Iteration 145/1000 | Loss: 0.00001152
Iteration 146/1000 | Loss: 0.00001152
Iteration 147/1000 | Loss: 0.00001152
Iteration 148/1000 | Loss: 0.00001152
Iteration 149/1000 | Loss: 0.00001152
Iteration 150/1000 | Loss: 0.00001152
Iteration 151/1000 | Loss: 0.00001152
Iteration 152/1000 | Loss: 0.00001152
Iteration 153/1000 | Loss: 0.00001152
Iteration 154/1000 | Loss: 0.00001151
Iteration 155/1000 | Loss: 0.00001151
Iteration 156/1000 | Loss: 0.00001151
Iteration 157/1000 | Loss: 0.00001151
Iteration 158/1000 | Loss: 0.00001151
Iteration 159/1000 | Loss: 0.00001151
Iteration 160/1000 | Loss: 0.00001150
Iteration 161/1000 | Loss: 0.00001150
Iteration 162/1000 | Loss: 0.00001150
Iteration 163/1000 | Loss: 0.00001150
Iteration 164/1000 | Loss: 0.00001149
Iteration 165/1000 | Loss: 0.00001149
Iteration 166/1000 | Loss: 0.00001149
Iteration 167/1000 | Loss: 0.00001149
Iteration 168/1000 | Loss: 0.00001149
Iteration 169/1000 | Loss: 0.00001149
Iteration 170/1000 | Loss: 0.00001149
Iteration 171/1000 | Loss: 0.00001149
Iteration 172/1000 | Loss: 0.00001149
Iteration 173/1000 | Loss: 0.00001149
Iteration 174/1000 | Loss: 0.00001149
Iteration 175/1000 | Loss: 0.00001149
Iteration 176/1000 | Loss: 0.00001149
Iteration 177/1000 | Loss: 0.00001149
Iteration 178/1000 | Loss: 0.00001148
Iteration 179/1000 | Loss: 0.00001148
Iteration 180/1000 | Loss: 0.00001148
Iteration 181/1000 | Loss: 0.00001148
Iteration 182/1000 | Loss: 0.00001148
Iteration 183/1000 | Loss: 0.00001148
Iteration 184/1000 | Loss: 0.00001148
Iteration 185/1000 | Loss: 0.00001148
Iteration 186/1000 | Loss: 0.00001148
Iteration 187/1000 | Loss: 0.00001148
Iteration 188/1000 | Loss: 0.00001148
Iteration 189/1000 | Loss: 0.00001148
Iteration 190/1000 | Loss: 0.00001148
Iteration 191/1000 | Loss: 0.00001148
Iteration 192/1000 | Loss: 0.00001148
Iteration 193/1000 | Loss: 0.00001148
Iteration 194/1000 | Loss: 0.00001148
Iteration 195/1000 | Loss: 0.00001147
Iteration 196/1000 | Loss: 0.00001147
Iteration 197/1000 | Loss: 0.00001147
Iteration 198/1000 | Loss: 0.00001147
Iteration 199/1000 | Loss: 0.00001147
Iteration 200/1000 | Loss: 0.00001147
Iteration 201/1000 | Loss: 0.00001147
Iteration 202/1000 | Loss: 0.00001147
Iteration 203/1000 | Loss: 0.00001147
Iteration 204/1000 | Loss: 0.00001147
Iteration 205/1000 | Loss: 0.00001147
Iteration 206/1000 | Loss: 0.00001147
Iteration 207/1000 | Loss: 0.00001147
Iteration 208/1000 | Loss: 0.00001147
Iteration 209/1000 | Loss: 0.00001147
Iteration 210/1000 | Loss: 0.00001147
Iteration 211/1000 | Loss: 0.00001147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.1471534890006296e-05, 1.1471534890006296e-05, 1.1471534890006296e-05, 1.1471534890006296e-05, 1.1471534890006296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1471534890006296e-05

Optimization complete. Final v2v error: 2.9058778285980225 mm

Highest mean error: 3.161715030670166 mm for frame 219

Lowest mean error: 2.6542093753814697 mm for frame 179

Saving results

Total time: 61.450456380844116
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404855
Iteration 2/25 | Loss: 0.00127746
Iteration 3/25 | Loss: 0.00117472
Iteration 4/25 | Loss: 0.00115907
Iteration 5/25 | Loss: 0.00115450
Iteration 6/25 | Loss: 0.00115307
Iteration 7/25 | Loss: 0.00115287
Iteration 8/25 | Loss: 0.00115287
Iteration 9/25 | Loss: 0.00115287
Iteration 10/25 | Loss: 0.00115287
Iteration 11/25 | Loss: 0.00115287
Iteration 12/25 | Loss: 0.00115287
Iteration 13/25 | Loss: 0.00115287
Iteration 14/25 | Loss: 0.00115287
Iteration 15/25 | Loss: 0.00115287
Iteration 16/25 | Loss: 0.00115287
Iteration 17/25 | Loss: 0.00115287
Iteration 18/25 | Loss: 0.00115287
Iteration 19/25 | Loss: 0.00115287
Iteration 20/25 | Loss: 0.00115287
Iteration 21/25 | Loss: 0.00115287
Iteration 22/25 | Loss: 0.00115287
Iteration 23/25 | Loss: 0.00115287
Iteration 24/25 | Loss: 0.00115287
Iteration 25/25 | Loss: 0.00115287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34720922
Iteration 2/25 | Loss: 0.00097302
Iteration 3/25 | Loss: 0.00097300
Iteration 4/25 | Loss: 0.00097300
Iteration 5/25 | Loss: 0.00097300
Iteration 6/25 | Loss: 0.00097300
Iteration 7/25 | Loss: 0.00097300
Iteration 8/25 | Loss: 0.00097300
Iteration 9/25 | Loss: 0.00097300
Iteration 10/25 | Loss: 0.00097300
Iteration 11/25 | Loss: 0.00097300
Iteration 12/25 | Loss: 0.00097300
Iteration 13/25 | Loss: 0.00097300
Iteration 14/25 | Loss: 0.00097300
Iteration 15/25 | Loss: 0.00097300
Iteration 16/25 | Loss: 0.00097300
Iteration 17/25 | Loss: 0.00097300
Iteration 18/25 | Loss: 0.00097300
Iteration 19/25 | Loss: 0.00097300
Iteration 20/25 | Loss: 0.00097300
Iteration 21/25 | Loss: 0.00097300
Iteration 22/25 | Loss: 0.00097300
Iteration 23/25 | Loss: 0.00097300
Iteration 24/25 | Loss: 0.00097300
Iteration 25/25 | Loss: 0.00097300
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009729962912388146, 0.0009729962912388146, 0.0009729962912388146, 0.0009729962912388146, 0.0009729962912388146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009729962912388146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097300
Iteration 2/1000 | Loss: 0.00004982
Iteration 3/1000 | Loss: 0.00003185
Iteration 4/1000 | Loss: 0.00002461
Iteration 5/1000 | Loss: 0.00002013
Iteration 6/1000 | Loss: 0.00001849
Iteration 7/1000 | Loss: 0.00001714
Iteration 8/1000 | Loss: 0.00001647
Iteration 9/1000 | Loss: 0.00001601
Iteration 10/1000 | Loss: 0.00001554
Iteration 11/1000 | Loss: 0.00001530
Iteration 12/1000 | Loss: 0.00001529
Iteration 13/1000 | Loss: 0.00001514
Iteration 14/1000 | Loss: 0.00001490
Iteration 15/1000 | Loss: 0.00001489
Iteration 16/1000 | Loss: 0.00001487
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001479
Iteration 21/1000 | Loss: 0.00001478
Iteration 22/1000 | Loss: 0.00001478
Iteration 23/1000 | Loss: 0.00001478
Iteration 24/1000 | Loss: 0.00001477
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001472
Iteration 28/1000 | Loss: 0.00001471
Iteration 29/1000 | Loss: 0.00001471
Iteration 30/1000 | Loss: 0.00001471
Iteration 31/1000 | Loss: 0.00001471
Iteration 32/1000 | Loss: 0.00001471
Iteration 33/1000 | Loss: 0.00001471
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001471
Iteration 38/1000 | Loss: 0.00001471
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001470
Iteration 42/1000 | Loss: 0.00001470
Iteration 43/1000 | Loss: 0.00001470
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001470
Iteration 47/1000 | Loss: 0.00001470
Iteration 48/1000 | Loss: 0.00001470
Iteration 49/1000 | Loss: 0.00001470
Iteration 50/1000 | Loss: 0.00001470
Iteration 51/1000 | Loss: 0.00001470
Iteration 52/1000 | Loss: 0.00001469
Iteration 53/1000 | Loss: 0.00001469
Iteration 54/1000 | Loss: 0.00001469
Iteration 55/1000 | Loss: 0.00001469
Iteration 56/1000 | Loss: 0.00001468
Iteration 57/1000 | Loss: 0.00001468
Iteration 58/1000 | Loss: 0.00001468
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001465
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001463
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001458
Iteration 69/1000 | Loss: 0.00001457
Iteration 70/1000 | Loss: 0.00001457
Iteration 71/1000 | Loss: 0.00001457
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001452
Iteration 74/1000 | Loss: 0.00001452
Iteration 75/1000 | Loss: 0.00001451
Iteration 76/1000 | Loss: 0.00001451
Iteration 77/1000 | Loss: 0.00001450
Iteration 78/1000 | Loss: 0.00001450
Iteration 79/1000 | Loss: 0.00001450
Iteration 80/1000 | Loss: 0.00001449
Iteration 81/1000 | Loss: 0.00001449
Iteration 82/1000 | Loss: 0.00001449
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001448
Iteration 85/1000 | Loss: 0.00001446
Iteration 86/1000 | Loss: 0.00001446
Iteration 87/1000 | Loss: 0.00001446
Iteration 88/1000 | Loss: 0.00001446
Iteration 89/1000 | Loss: 0.00001446
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001445
Iteration 99/1000 | Loss: 0.00001445
Iteration 100/1000 | Loss: 0.00001445
Iteration 101/1000 | Loss: 0.00001445
Iteration 102/1000 | Loss: 0.00001445
Iteration 103/1000 | Loss: 0.00001445
Iteration 104/1000 | Loss: 0.00001444
Iteration 105/1000 | Loss: 0.00001444
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00001443
Iteration 108/1000 | Loss: 0.00001443
Iteration 109/1000 | Loss: 0.00001442
Iteration 110/1000 | Loss: 0.00001442
Iteration 111/1000 | Loss: 0.00001442
Iteration 112/1000 | Loss: 0.00001442
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001441
Iteration 116/1000 | Loss: 0.00001441
Iteration 117/1000 | Loss: 0.00001441
Iteration 118/1000 | Loss: 0.00001441
Iteration 119/1000 | Loss: 0.00001441
Iteration 120/1000 | Loss: 0.00001441
Iteration 121/1000 | Loss: 0.00001441
Iteration 122/1000 | Loss: 0.00001440
Iteration 123/1000 | Loss: 0.00001440
Iteration 124/1000 | Loss: 0.00001440
Iteration 125/1000 | Loss: 0.00001440
Iteration 126/1000 | Loss: 0.00001440
Iteration 127/1000 | Loss: 0.00001440
Iteration 128/1000 | Loss: 0.00001439
Iteration 129/1000 | Loss: 0.00001439
Iteration 130/1000 | Loss: 0.00001439
Iteration 131/1000 | Loss: 0.00001439
Iteration 132/1000 | Loss: 0.00001439
Iteration 133/1000 | Loss: 0.00001439
Iteration 134/1000 | Loss: 0.00001439
Iteration 135/1000 | Loss: 0.00001438
Iteration 136/1000 | Loss: 0.00001438
Iteration 137/1000 | Loss: 0.00001438
Iteration 138/1000 | Loss: 0.00001438
Iteration 139/1000 | Loss: 0.00001438
Iteration 140/1000 | Loss: 0.00001438
Iteration 141/1000 | Loss: 0.00001437
Iteration 142/1000 | Loss: 0.00001437
Iteration 143/1000 | Loss: 0.00001437
Iteration 144/1000 | Loss: 0.00001437
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001436
Iteration 147/1000 | Loss: 0.00001436
Iteration 148/1000 | Loss: 0.00001436
Iteration 149/1000 | Loss: 0.00001436
Iteration 150/1000 | Loss: 0.00001436
Iteration 151/1000 | Loss: 0.00001436
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00001435
Iteration 157/1000 | Loss: 0.00001435
Iteration 158/1000 | Loss: 0.00001434
Iteration 159/1000 | Loss: 0.00001434
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001434
Iteration 162/1000 | Loss: 0.00001434
Iteration 163/1000 | Loss: 0.00001434
Iteration 164/1000 | Loss: 0.00001434
Iteration 165/1000 | Loss: 0.00001433
Iteration 166/1000 | Loss: 0.00001433
Iteration 167/1000 | Loss: 0.00001433
Iteration 168/1000 | Loss: 0.00001433
Iteration 169/1000 | Loss: 0.00001433
Iteration 170/1000 | Loss: 0.00001433
Iteration 171/1000 | Loss: 0.00001433
Iteration 172/1000 | Loss: 0.00001433
Iteration 173/1000 | Loss: 0.00001433
Iteration 174/1000 | Loss: 0.00001433
Iteration 175/1000 | Loss: 0.00001433
Iteration 176/1000 | Loss: 0.00001433
Iteration 177/1000 | Loss: 0.00001433
Iteration 178/1000 | Loss: 0.00001432
Iteration 179/1000 | Loss: 0.00001432
Iteration 180/1000 | Loss: 0.00001432
Iteration 181/1000 | Loss: 0.00001432
Iteration 182/1000 | Loss: 0.00001432
Iteration 183/1000 | Loss: 0.00001432
Iteration 184/1000 | Loss: 0.00001432
Iteration 185/1000 | Loss: 0.00001432
Iteration 186/1000 | Loss: 0.00001432
Iteration 187/1000 | Loss: 0.00001432
Iteration 188/1000 | Loss: 0.00001432
Iteration 189/1000 | Loss: 0.00001432
Iteration 190/1000 | Loss: 0.00001432
Iteration 191/1000 | Loss: 0.00001432
Iteration 192/1000 | Loss: 0.00001432
Iteration 193/1000 | Loss: 0.00001432
Iteration 194/1000 | Loss: 0.00001432
Iteration 195/1000 | Loss: 0.00001432
Iteration 196/1000 | Loss: 0.00001432
Iteration 197/1000 | Loss: 0.00001432
Iteration 198/1000 | Loss: 0.00001432
Iteration 199/1000 | Loss: 0.00001432
Iteration 200/1000 | Loss: 0.00001432
Iteration 201/1000 | Loss: 0.00001432
Iteration 202/1000 | Loss: 0.00001432
Iteration 203/1000 | Loss: 0.00001432
Iteration 204/1000 | Loss: 0.00001432
Iteration 205/1000 | Loss: 0.00001432
Iteration 206/1000 | Loss: 0.00001432
Iteration 207/1000 | Loss: 0.00001432
Iteration 208/1000 | Loss: 0.00001432
Iteration 209/1000 | Loss: 0.00001432
Iteration 210/1000 | Loss: 0.00001432
Iteration 211/1000 | Loss: 0.00001432
Iteration 212/1000 | Loss: 0.00001432
Iteration 213/1000 | Loss: 0.00001432
Iteration 214/1000 | Loss: 0.00001432
Iteration 215/1000 | Loss: 0.00001432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.4315763110062107e-05, 1.4315763110062107e-05, 1.4315763110062107e-05, 1.4315763110062107e-05, 1.4315763110062107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4315763110062107e-05

Optimization complete. Final v2v error: 3.0669949054718018 mm

Highest mean error: 5.1507062911987305 mm for frame 74

Lowest mean error: 2.4946320056915283 mm for frame 138

Saving results

Total time: 44.78888750076294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01070677
Iteration 2/25 | Loss: 0.00628242
Iteration 3/25 | Loss: 0.00384365
Iteration 4/25 | Loss: 0.00328412
Iteration 5/25 | Loss: 0.00292759
Iteration 6/25 | Loss: 0.00239489
Iteration 7/25 | Loss: 0.00197394
Iteration 8/25 | Loss: 0.00173761
Iteration 9/25 | Loss: 0.00160523
Iteration 10/25 | Loss: 0.00149598
Iteration 11/25 | Loss: 0.00144473
Iteration 12/25 | Loss: 0.00143416
Iteration 13/25 | Loss: 0.00139512
Iteration 14/25 | Loss: 0.00140422
Iteration 15/25 | Loss: 0.00135351
Iteration 16/25 | Loss: 0.00134259
Iteration 17/25 | Loss: 0.00133885
Iteration 18/25 | Loss: 0.00134030
Iteration 19/25 | Loss: 0.00133805
Iteration 20/25 | Loss: 0.00133412
Iteration 21/25 | Loss: 0.00133351
Iteration 22/25 | Loss: 0.00133332
Iteration 23/25 | Loss: 0.00133313
Iteration 24/25 | Loss: 0.00133266
Iteration 25/25 | Loss: 0.00133266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56192279
Iteration 2/25 | Loss: 0.00098256
Iteration 3/25 | Loss: 0.00091271
Iteration 4/25 | Loss: 0.00090577
Iteration 5/25 | Loss: 0.00090576
Iteration 6/25 | Loss: 0.00090576
Iteration 7/25 | Loss: 0.00090576
Iteration 8/25 | Loss: 0.00090576
Iteration 9/25 | Loss: 0.00090576
Iteration 10/25 | Loss: 0.00090576
Iteration 11/25 | Loss: 0.00090576
Iteration 12/25 | Loss: 0.00090576
Iteration 13/25 | Loss: 0.00090576
Iteration 14/25 | Loss: 0.00090576
Iteration 15/25 | Loss: 0.00090576
Iteration 16/25 | Loss: 0.00090576
Iteration 17/25 | Loss: 0.00090576
Iteration 18/25 | Loss: 0.00090576
Iteration 19/25 | Loss: 0.00090576
Iteration 20/25 | Loss: 0.00090576
Iteration 21/25 | Loss: 0.00090576
Iteration 22/25 | Loss: 0.00090576
Iteration 23/25 | Loss: 0.00090576
Iteration 24/25 | Loss: 0.00090576
Iteration 25/25 | Loss: 0.00090576

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090576
Iteration 2/1000 | Loss: 0.00022699
Iteration 3/1000 | Loss: 0.00005233
Iteration 4/1000 | Loss: 0.00003527
Iteration 5/1000 | Loss: 0.00003738
Iteration 6/1000 | Loss: 0.00003245
Iteration 7/1000 | Loss: 0.00003296
Iteration 8/1000 | Loss: 0.00003138
Iteration 9/1000 | Loss: 0.00003088
Iteration 10/1000 | Loss: 0.00003046
Iteration 11/1000 | Loss: 0.00003016
Iteration 12/1000 | Loss: 0.00240346
Iteration 13/1000 | Loss: 0.00025738
Iteration 14/1000 | Loss: 0.00003612
Iteration 15/1000 | Loss: 0.00004354
Iteration 16/1000 | Loss: 0.00002916
Iteration 17/1000 | Loss: 0.00002562
Iteration 18/1000 | Loss: 0.00002384
Iteration 19/1000 | Loss: 0.00002236
Iteration 20/1000 | Loss: 0.00002694
Iteration 21/1000 | Loss: 0.00004381
Iteration 22/1000 | Loss: 0.00005815
Iteration 23/1000 | Loss: 0.00002026
Iteration 24/1000 | Loss: 0.00004631
Iteration 25/1000 | Loss: 0.00001977
Iteration 26/1000 | Loss: 0.00002798
Iteration 27/1000 | Loss: 0.00001909
Iteration 28/1000 | Loss: 0.00001879
Iteration 29/1000 | Loss: 0.00001860
Iteration 30/1000 | Loss: 0.00001848
Iteration 31/1000 | Loss: 0.00001848
Iteration 32/1000 | Loss: 0.00001846
Iteration 33/1000 | Loss: 0.00001846
Iteration 34/1000 | Loss: 0.00001838
Iteration 35/1000 | Loss: 0.00001838
Iteration 36/1000 | Loss: 0.00001838
Iteration 37/1000 | Loss: 0.00001837
Iteration 38/1000 | Loss: 0.00001837
Iteration 39/1000 | Loss: 0.00001837
Iteration 40/1000 | Loss: 0.00001837
Iteration 41/1000 | Loss: 0.00001837
Iteration 42/1000 | Loss: 0.00001837
Iteration 43/1000 | Loss: 0.00001837
Iteration 44/1000 | Loss: 0.00001837
Iteration 45/1000 | Loss: 0.00001837
Iteration 46/1000 | Loss: 0.00001836
Iteration 47/1000 | Loss: 0.00001835
Iteration 48/1000 | Loss: 0.00001835
Iteration 49/1000 | Loss: 0.00001835
Iteration 50/1000 | Loss: 0.00001835
Iteration 51/1000 | Loss: 0.00001835
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001834
Iteration 54/1000 | Loss: 0.00001834
Iteration 55/1000 | Loss: 0.00001834
Iteration 56/1000 | Loss: 0.00001834
Iteration 57/1000 | Loss: 0.00001834
Iteration 58/1000 | Loss: 0.00001834
Iteration 59/1000 | Loss: 0.00001834
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001832
Iteration 70/1000 | Loss: 0.00001832
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001830
Iteration 76/1000 | Loss: 0.00001830
Iteration 77/1000 | Loss: 0.00001829
Iteration 78/1000 | Loss: 0.00001829
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001828
Iteration 81/1000 | Loss: 0.00001828
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001827
Iteration 88/1000 | Loss: 0.00001827
Iteration 89/1000 | Loss: 0.00001827
Iteration 90/1000 | Loss: 0.00001827
Iteration 91/1000 | Loss: 0.00001827
Iteration 92/1000 | Loss: 0.00001827
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001827
Iteration 95/1000 | Loss: 0.00001827
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001826
Iteration 98/1000 | Loss: 0.00001826
Iteration 99/1000 | Loss: 0.00001826
Iteration 100/1000 | Loss: 0.00001826
Iteration 101/1000 | Loss: 0.00001826
Iteration 102/1000 | Loss: 0.00001826
Iteration 103/1000 | Loss: 0.00001826
Iteration 104/1000 | Loss: 0.00001826
Iteration 105/1000 | Loss: 0.00001826
Iteration 106/1000 | Loss: 0.00001826
Iteration 107/1000 | Loss: 0.00001826
Iteration 108/1000 | Loss: 0.00001826
Iteration 109/1000 | Loss: 0.00001826
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.826345396693796e-05, 1.826345396693796e-05, 1.826345396693796e-05, 1.826345396693796e-05, 1.826345396693796e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.826345396693796e-05

Optimization complete. Final v2v error: 3.6888601779937744 mm

Highest mean error: 3.975008249282837 mm for frame 143

Lowest mean error: 3.5717263221740723 mm for frame 44

Saving results

Total time: 101.76125979423523
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080278
Iteration 2/25 | Loss: 0.00179785
Iteration 3/25 | Loss: 0.00132691
Iteration 4/25 | Loss: 0.00130776
Iteration 5/25 | Loss: 0.00131102
Iteration 6/25 | Loss: 0.00128967
Iteration 7/25 | Loss: 0.00127633
Iteration 8/25 | Loss: 0.00127286
Iteration 9/25 | Loss: 0.00126414
Iteration 10/25 | Loss: 0.00127176
Iteration 11/25 | Loss: 0.00126449
Iteration 12/25 | Loss: 0.00126054
Iteration 13/25 | Loss: 0.00126239
Iteration 14/25 | Loss: 0.00125400
Iteration 15/25 | Loss: 0.00125792
Iteration 16/25 | Loss: 0.00125266
Iteration 17/25 | Loss: 0.00125434
Iteration 18/25 | Loss: 0.00125297
Iteration 19/25 | Loss: 0.00125368
Iteration 20/25 | Loss: 0.00125124
Iteration 21/25 | Loss: 0.00125440
Iteration 22/25 | Loss: 0.00125376
Iteration 23/25 | Loss: 0.00125258
Iteration 24/25 | Loss: 0.00125220
Iteration 25/25 | Loss: 0.00124850

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06530154
Iteration 2/25 | Loss: 0.00079775
Iteration 3/25 | Loss: 0.00077897
Iteration 4/25 | Loss: 0.00077897
Iteration 5/25 | Loss: 0.00077897
Iteration 6/25 | Loss: 0.00077897
Iteration 7/25 | Loss: 0.00077897
Iteration 8/25 | Loss: 0.00077897
Iteration 9/25 | Loss: 0.00077897
Iteration 10/25 | Loss: 0.00077897
Iteration 11/25 | Loss: 0.00077897
Iteration 12/25 | Loss: 0.00077897
Iteration 13/25 | Loss: 0.00077897
Iteration 14/25 | Loss: 0.00077897
Iteration 15/25 | Loss: 0.00077897
Iteration 16/25 | Loss: 0.00077897
Iteration 17/25 | Loss: 0.00077897
Iteration 18/25 | Loss: 0.00077897
Iteration 19/25 | Loss: 0.00077897
Iteration 20/25 | Loss: 0.00077897
Iteration 21/25 | Loss: 0.00077897
Iteration 22/25 | Loss: 0.00077897
Iteration 23/25 | Loss: 0.00077897
Iteration 24/25 | Loss: 0.00077897
Iteration 25/25 | Loss: 0.00077897

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077897
Iteration 2/1000 | Loss: 0.00006223
Iteration 3/1000 | Loss: 0.00003258
Iteration 4/1000 | Loss: 0.00003054
Iteration 5/1000 | Loss: 0.00003752
Iteration 6/1000 | Loss: 0.00002286
Iteration 7/1000 | Loss: 0.00004535
Iteration 8/1000 | Loss: 0.00002191
Iteration 9/1000 | Loss: 0.00003681
Iteration 10/1000 | Loss: 0.00002106
Iteration 11/1000 | Loss: 0.00021473
Iteration 12/1000 | Loss: 0.00024883
Iteration 13/1000 | Loss: 0.00015479
Iteration 14/1000 | Loss: 0.00002629
Iteration 15/1000 | Loss: 0.00002404
Iteration 16/1000 | Loss: 0.00002300
Iteration 17/1000 | Loss: 0.00002230
Iteration 18/1000 | Loss: 0.00004701
Iteration 19/1000 | Loss: 0.00002192
Iteration 20/1000 | Loss: 0.00026306
Iteration 21/1000 | Loss: 0.00019951
Iteration 22/1000 | Loss: 0.00021933
Iteration 23/1000 | Loss: 0.00003349
Iteration 24/1000 | Loss: 0.00009467
Iteration 25/1000 | Loss: 0.00002601
Iteration 26/1000 | Loss: 0.00018740
Iteration 27/1000 | Loss: 0.00006896
Iteration 28/1000 | Loss: 0.00018592
Iteration 29/1000 | Loss: 0.00007523
Iteration 30/1000 | Loss: 0.00021007
Iteration 31/1000 | Loss: 0.00019435
Iteration 32/1000 | Loss: 0.00043809
Iteration 33/1000 | Loss: 0.00024672
Iteration 34/1000 | Loss: 0.00015885
Iteration 35/1000 | Loss: 0.00003787
Iteration 36/1000 | Loss: 0.00002783
Iteration 37/1000 | Loss: 0.00002453
Iteration 38/1000 | Loss: 0.00002277
Iteration 39/1000 | Loss: 0.00002682
Iteration 40/1000 | Loss: 0.00002084
Iteration 41/1000 | Loss: 0.00002005
Iteration 42/1000 | Loss: 0.00004310
Iteration 43/1000 | Loss: 0.00001915
Iteration 44/1000 | Loss: 0.00001915
Iteration 45/1000 | Loss: 0.00001915
Iteration 46/1000 | Loss: 0.00001914
Iteration 47/1000 | Loss: 0.00001913
Iteration 48/1000 | Loss: 0.00001913
Iteration 49/1000 | Loss: 0.00001912
Iteration 50/1000 | Loss: 0.00001912
Iteration 51/1000 | Loss: 0.00001893
Iteration 52/1000 | Loss: 0.00001878
Iteration 53/1000 | Loss: 0.00002528
Iteration 54/1000 | Loss: 0.00001864
Iteration 55/1000 | Loss: 0.00001858
Iteration 56/1000 | Loss: 0.00001858
Iteration 57/1000 | Loss: 0.00001858
Iteration 58/1000 | Loss: 0.00001858
Iteration 59/1000 | Loss: 0.00001858
Iteration 60/1000 | Loss: 0.00001858
Iteration 61/1000 | Loss: 0.00001858
Iteration 62/1000 | Loss: 0.00001858
Iteration 63/1000 | Loss: 0.00001858
Iteration 64/1000 | Loss: 0.00001857
Iteration 65/1000 | Loss: 0.00001857
Iteration 66/1000 | Loss: 0.00001857
Iteration 67/1000 | Loss: 0.00001857
Iteration 68/1000 | Loss: 0.00001857
Iteration 69/1000 | Loss: 0.00001857
Iteration 70/1000 | Loss: 0.00001857
Iteration 71/1000 | Loss: 0.00001857
Iteration 72/1000 | Loss: 0.00002280
Iteration 73/1000 | Loss: 0.00002811
Iteration 74/1000 | Loss: 0.00002055
Iteration 75/1000 | Loss: 0.00003436
Iteration 76/1000 | Loss: 0.00001967
Iteration 77/1000 | Loss: 0.00001852
Iteration 78/1000 | Loss: 0.00001852
Iteration 79/1000 | Loss: 0.00001852
Iteration 80/1000 | Loss: 0.00001851
Iteration 81/1000 | Loss: 0.00001851
Iteration 82/1000 | Loss: 0.00001851
Iteration 83/1000 | Loss: 0.00001851
Iteration 84/1000 | Loss: 0.00001851
Iteration 85/1000 | Loss: 0.00001851
Iteration 86/1000 | Loss: 0.00001851
Iteration 87/1000 | Loss: 0.00001851
Iteration 88/1000 | Loss: 0.00001851
Iteration 89/1000 | Loss: 0.00001851
Iteration 90/1000 | Loss: 0.00001850
Iteration 91/1000 | Loss: 0.00001850
Iteration 92/1000 | Loss: 0.00001850
Iteration 93/1000 | Loss: 0.00001850
Iteration 94/1000 | Loss: 0.00001850
Iteration 95/1000 | Loss: 0.00001850
Iteration 96/1000 | Loss: 0.00001850
Iteration 97/1000 | Loss: 0.00001850
Iteration 98/1000 | Loss: 0.00001850
Iteration 99/1000 | Loss: 0.00001850
Iteration 100/1000 | Loss: 0.00001850
Iteration 101/1000 | Loss: 0.00001850
Iteration 102/1000 | Loss: 0.00001850
Iteration 103/1000 | Loss: 0.00001850
Iteration 104/1000 | Loss: 0.00001850
Iteration 105/1000 | Loss: 0.00001850
Iteration 106/1000 | Loss: 0.00001850
Iteration 107/1000 | Loss: 0.00001850
Iteration 108/1000 | Loss: 0.00001850
Iteration 109/1000 | Loss: 0.00001850
Iteration 110/1000 | Loss: 0.00001850
Iteration 111/1000 | Loss: 0.00001850
Iteration 112/1000 | Loss: 0.00001850
Iteration 113/1000 | Loss: 0.00001850
Iteration 114/1000 | Loss: 0.00001850
Iteration 115/1000 | Loss: 0.00001850
Iteration 116/1000 | Loss: 0.00001850
Iteration 117/1000 | Loss: 0.00001850
Iteration 118/1000 | Loss: 0.00001850
Iteration 119/1000 | Loss: 0.00001850
Iteration 120/1000 | Loss: 0.00001850
Iteration 121/1000 | Loss: 0.00001850
Iteration 122/1000 | Loss: 0.00001850
Iteration 123/1000 | Loss: 0.00001850
Iteration 124/1000 | Loss: 0.00001850
Iteration 125/1000 | Loss: 0.00001850
Iteration 126/1000 | Loss: 0.00001850
Iteration 127/1000 | Loss: 0.00001850
Iteration 128/1000 | Loss: 0.00001850
Iteration 129/1000 | Loss: 0.00001850
Iteration 130/1000 | Loss: 0.00001850
Iteration 131/1000 | Loss: 0.00001850
Iteration 132/1000 | Loss: 0.00001850
Iteration 133/1000 | Loss: 0.00001850
Iteration 134/1000 | Loss: 0.00001850
Iteration 135/1000 | Loss: 0.00001850
Iteration 136/1000 | Loss: 0.00001850
Iteration 137/1000 | Loss: 0.00001850
Iteration 138/1000 | Loss: 0.00001850
Iteration 139/1000 | Loss: 0.00001850
Iteration 140/1000 | Loss: 0.00001850
Iteration 141/1000 | Loss: 0.00001850
Iteration 142/1000 | Loss: 0.00001850
Iteration 143/1000 | Loss: 0.00001850
Iteration 144/1000 | Loss: 0.00001850
Iteration 145/1000 | Loss: 0.00001850
Iteration 146/1000 | Loss: 0.00001850
Iteration 147/1000 | Loss: 0.00001850
Iteration 148/1000 | Loss: 0.00001850
Iteration 149/1000 | Loss: 0.00001850
Iteration 150/1000 | Loss: 0.00001850
Iteration 151/1000 | Loss: 0.00001850
Iteration 152/1000 | Loss: 0.00001850
Iteration 153/1000 | Loss: 0.00001850
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.8498103599995375e-05, 1.8498103599995375e-05, 1.8498103599995375e-05, 1.8498103599995375e-05, 1.8498103599995375e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8498103599995375e-05

Optimization complete. Final v2v error: 3.5734174251556396 mm

Highest mean error: 4.740808010101318 mm for frame 22

Lowest mean error: 3.40185546875 mm for frame 1

Saving results

Total time: 126.79352688789368
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416059
Iteration 2/25 | Loss: 0.00123447
Iteration 3/25 | Loss: 0.00115961
Iteration 4/25 | Loss: 0.00114947
Iteration 5/25 | Loss: 0.00114650
Iteration 6/25 | Loss: 0.00114577
Iteration 7/25 | Loss: 0.00114577
Iteration 8/25 | Loss: 0.00114577
Iteration 9/25 | Loss: 0.00114577
Iteration 10/25 | Loss: 0.00114577
Iteration 11/25 | Loss: 0.00114577
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011457684449851513, 0.0011457684449851513, 0.0011457684449851513, 0.0011457684449851513, 0.0011457684449851513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011457684449851513

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.74758673
Iteration 2/25 | Loss: 0.00081944
Iteration 3/25 | Loss: 0.00081942
Iteration 4/25 | Loss: 0.00081942
Iteration 5/25 | Loss: 0.00081942
Iteration 6/25 | Loss: 0.00081942
Iteration 7/25 | Loss: 0.00081942
Iteration 8/25 | Loss: 0.00081942
Iteration 9/25 | Loss: 0.00081942
Iteration 10/25 | Loss: 0.00081942
Iteration 11/25 | Loss: 0.00081942
Iteration 12/25 | Loss: 0.00081942
Iteration 13/25 | Loss: 0.00081942
Iteration 14/25 | Loss: 0.00081942
Iteration 15/25 | Loss: 0.00081942
Iteration 16/25 | Loss: 0.00081942
Iteration 17/25 | Loss: 0.00081942
Iteration 18/25 | Loss: 0.00081942
Iteration 19/25 | Loss: 0.00081942
Iteration 20/25 | Loss: 0.00081942
Iteration 21/25 | Loss: 0.00081942
Iteration 22/25 | Loss: 0.00081942
Iteration 23/25 | Loss: 0.00081942
Iteration 24/25 | Loss: 0.00081942
Iteration 25/25 | Loss: 0.00081942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081942
Iteration 2/1000 | Loss: 0.00002537
Iteration 3/1000 | Loss: 0.00001809
Iteration 4/1000 | Loss: 0.00001507
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001349
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001267
Iteration 9/1000 | Loss: 0.00001252
Iteration 10/1000 | Loss: 0.00001228
Iteration 11/1000 | Loss: 0.00001210
Iteration 12/1000 | Loss: 0.00001206
Iteration 13/1000 | Loss: 0.00001201
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001195
Iteration 18/1000 | Loss: 0.00001194
Iteration 19/1000 | Loss: 0.00001194
Iteration 20/1000 | Loss: 0.00001193
Iteration 21/1000 | Loss: 0.00001192
Iteration 22/1000 | Loss: 0.00001191
Iteration 23/1000 | Loss: 0.00001183
Iteration 24/1000 | Loss: 0.00001181
Iteration 25/1000 | Loss: 0.00001180
Iteration 26/1000 | Loss: 0.00001180
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001177
Iteration 32/1000 | Loss: 0.00001176
Iteration 33/1000 | Loss: 0.00001176
Iteration 34/1000 | Loss: 0.00001176
Iteration 35/1000 | Loss: 0.00001176
Iteration 36/1000 | Loss: 0.00001175
Iteration 37/1000 | Loss: 0.00001174
Iteration 38/1000 | Loss: 0.00001172
Iteration 39/1000 | Loss: 0.00001172
Iteration 40/1000 | Loss: 0.00001172
Iteration 41/1000 | Loss: 0.00001170
Iteration 42/1000 | Loss: 0.00001170
Iteration 43/1000 | Loss: 0.00001170
Iteration 44/1000 | Loss: 0.00001169
Iteration 45/1000 | Loss: 0.00001168
Iteration 46/1000 | Loss: 0.00001168
Iteration 47/1000 | Loss: 0.00001167
Iteration 48/1000 | Loss: 0.00001166
Iteration 49/1000 | Loss: 0.00001166
Iteration 50/1000 | Loss: 0.00001166
Iteration 51/1000 | Loss: 0.00001165
Iteration 52/1000 | Loss: 0.00001165
Iteration 53/1000 | Loss: 0.00001165
Iteration 54/1000 | Loss: 0.00001165
Iteration 55/1000 | Loss: 0.00001165
Iteration 56/1000 | Loss: 0.00001165
Iteration 57/1000 | Loss: 0.00001165
Iteration 58/1000 | Loss: 0.00001164
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001163
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001161
Iteration 63/1000 | Loss: 0.00001161
Iteration 64/1000 | Loss: 0.00001161
Iteration 65/1000 | Loss: 0.00001160
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001157
Iteration 74/1000 | Loss: 0.00001157
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001157
Iteration 78/1000 | Loss: 0.00001156
Iteration 79/1000 | Loss: 0.00001155
Iteration 80/1000 | Loss: 0.00001155
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001151
Iteration 89/1000 | Loss: 0.00001151
Iteration 90/1000 | Loss: 0.00001151
Iteration 91/1000 | Loss: 0.00001151
Iteration 92/1000 | Loss: 0.00001150
Iteration 93/1000 | Loss: 0.00001150
Iteration 94/1000 | Loss: 0.00001149
Iteration 95/1000 | Loss: 0.00001149
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001148
Iteration 98/1000 | Loss: 0.00001148
Iteration 99/1000 | Loss: 0.00001148
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001147
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001147
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001146
Iteration 112/1000 | Loss: 0.00001146
Iteration 113/1000 | Loss: 0.00001146
Iteration 114/1000 | Loss: 0.00001146
Iteration 115/1000 | Loss: 0.00001146
Iteration 116/1000 | Loss: 0.00001146
Iteration 117/1000 | Loss: 0.00001146
Iteration 118/1000 | Loss: 0.00001146
Iteration 119/1000 | Loss: 0.00001146
Iteration 120/1000 | Loss: 0.00001146
Iteration 121/1000 | Loss: 0.00001146
Iteration 122/1000 | Loss: 0.00001145
Iteration 123/1000 | Loss: 0.00001145
Iteration 124/1000 | Loss: 0.00001145
Iteration 125/1000 | Loss: 0.00001145
Iteration 126/1000 | Loss: 0.00001144
Iteration 127/1000 | Loss: 0.00001144
Iteration 128/1000 | Loss: 0.00001144
Iteration 129/1000 | Loss: 0.00001144
Iteration 130/1000 | Loss: 0.00001144
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Iteration 137/1000 | Loss: 0.00001143
Iteration 138/1000 | Loss: 0.00001143
Iteration 139/1000 | Loss: 0.00001143
Iteration 140/1000 | Loss: 0.00001142
Iteration 141/1000 | Loss: 0.00001142
Iteration 142/1000 | Loss: 0.00001142
Iteration 143/1000 | Loss: 0.00001142
Iteration 144/1000 | Loss: 0.00001142
Iteration 145/1000 | Loss: 0.00001142
Iteration 146/1000 | Loss: 0.00001141
Iteration 147/1000 | Loss: 0.00001141
Iteration 148/1000 | Loss: 0.00001141
Iteration 149/1000 | Loss: 0.00001141
Iteration 150/1000 | Loss: 0.00001141
Iteration 151/1000 | Loss: 0.00001141
Iteration 152/1000 | Loss: 0.00001141
Iteration 153/1000 | Loss: 0.00001141
Iteration 154/1000 | Loss: 0.00001141
Iteration 155/1000 | Loss: 0.00001141
Iteration 156/1000 | Loss: 0.00001141
Iteration 157/1000 | Loss: 0.00001141
Iteration 158/1000 | Loss: 0.00001141
Iteration 159/1000 | Loss: 0.00001140
Iteration 160/1000 | Loss: 0.00001140
Iteration 161/1000 | Loss: 0.00001140
Iteration 162/1000 | Loss: 0.00001140
Iteration 163/1000 | Loss: 0.00001140
Iteration 164/1000 | Loss: 0.00001140
Iteration 165/1000 | Loss: 0.00001140
Iteration 166/1000 | Loss: 0.00001140
Iteration 167/1000 | Loss: 0.00001140
Iteration 168/1000 | Loss: 0.00001140
Iteration 169/1000 | Loss: 0.00001140
Iteration 170/1000 | Loss: 0.00001140
Iteration 171/1000 | Loss: 0.00001140
Iteration 172/1000 | Loss: 0.00001139
Iteration 173/1000 | Loss: 0.00001139
Iteration 174/1000 | Loss: 0.00001139
Iteration 175/1000 | Loss: 0.00001139
Iteration 176/1000 | Loss: 0.00001139
Iteration 177/1000 | Loss: 0.00001139
Iteration 178/1000 | Loss: 0.00001139
Iteration 179/1000 | Loss: 0.00001139
Iteration 180/1000 | Loss: 0.00001139
Iteration 181/1000 | Loss: 0.00001139
Iteration 182/1000 | Loss: 0.00001139
Iteration 183/1000 | Loss: 0.00001139
Iteration 184/1000 | Loss: 0.00001139
Iteration 185/1000 | Loss: 0.00001139
Iteration 186/1000 | Loss: 0.00001139
Iteration 187/1000 | Loss: 0.00001139
Iteration 188/1000 | Loss: 0.00001139
Iteration 189/1000 | Loss: 0.00001139
Iteration 190/1000 | Loss: 0.00001139
Iteration 191/1000 | Loss: 0.00001139
Iteration 192/1000 | Loss: 0.00001139
Iteration 193/1000 | Loss: 0.00001139
Iteration 194/1000 | Loss: 0.00001139
Iteration 195/1000 | Loss: 0.00001139
Iteration 196/1000 | Loss: 0.00001139
Iteration 197/1000 | Loss: 0.00001139
Iteration 198/1000 | Loss: 0.00001139
Iteration 199/1000 | Loss: 0.00001139
Iteration 200/1000 | Loss: 0.00001139
Iteration 201/1000 | Loss: 0.00001139
Iteration 202/1000 | Loss: 0.00001139
Iteration 203/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.1389688552299049e-05, 1.1389688552299049e-05, 1.1389688552299049e-05, 1.1389688552299049e-05, 1.1389688552299049e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1389688552299049e-05

Optimization complete. Final v2v error: 2.884035348892212 mm

Highest mean error: 3.542271852493286 mm for frame 86

Lowest mean error: 2.5722525119781494 mm for frame 33

Saving results

Total time: 42.04119372367859
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971162
Iteration 2/25 | Loss: 0.00297057
Iteration 3/25 | Loss: 0.00209901
Iteration 4/25 | Loss: 0.00197878
Iteration 5/25 | Loss: 0.00184444
Iteration 6/25 | Loss: 0.00175761
Iteration 7/25 | Loss: 0.00170086
Iteration 8/25 | Loss: 0.00165697
Iteration 9/25 | Loss: 0.00160414
Iteration 10/25 | Loss: 0.00155163
Iteration 11/25 | Loss: 0.00151478
Iteration 12/25 | Loss: 0.00150428
Iteration 13/25 | Loss: 0.00149947
Iteration 14/25 | Loss: 0.00148934
Iteration 15/25 | Loss: 0.00147929
Iteration 16/25 | Loss: 0.00147563
Iteration 17/25 | Loss: 0.00148216
Iteration 18/25 | Loss: 0.00147933
Iteration 19/25 | Loss: 0.00147256
Iteration 20/25 | Loss: 0.00147183
Iteration 21/25 | Loss: 0.00147013
Iteration 22/25 | Loss: 0.00146799
Iteration 23/25 | Loss: 0.00146968
Iteration 24/25 | Loss: 0.00146639
Iteration 25/25 | Loss: 0.00146667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34157670
Iteration 2/25 | Loss: 0.00374157
Iteration 3/25 | Loss: 0.00289807
Iteration 4/25 | Loss: 0.00289806
Iteration 5/25 | Loss: 0.00289806
Iteration 6/25 | Loss: 0.00289805
Iteration 7/25 | Loss: 0.00289805
Iteration 8/25 | Loss: 0.00289805
Iteration 9/25 | Loss: 0.00289805
Iteration 10/25 | Loss: 0.00289805
Iteration 11/25 | Loss: 0.00289805
Iteration 12/25 | Loss: 0.00289805
Iteration 13/25 | Loss: 0.00289805
Iteration 14/25 | Loss: 0.00289805
Iteration 15/25 | Loss: 0.00289805
Iteration 16/25 | Loss: 0.00289805
Iteration 17/25 | Loss: 0.00289805
Iteration 18/25 | Loss: 0.00289805
Iteration 19/25 | Loss: 0.00289805
Iteration 20/25 | Loss: 0.00289805
Iteration 21/25 | Loss: 0.00289805
Iteration 22/25 | Loss: 0.00289805
Iteration 23/25 | Loss: 0.00289805
Iteration 24/25 | Loss: 0.00289805
Iteration 25/25 | Loss: 0.00289805
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0028980528004467487, 0.0028980528004467487, 0.0028980528004467487, 0.0028980528004467487, 0.0028980528004467487]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028980528004467487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00289805
Iteration 2/1000 | Loss: 0.00157614
Iteration 3/1000 | Loss: 0.00710664
Iteration 4/1000 | Loss: 0.00894658
Iteration 5/1000 | Loss: 0.00928205
Iteration 6/1000 | Loss: 0.00507388
Iteration 7/1000 | Loss: 0.00060437
Iteration 8/1000 | Loss: 0.00090197
Iteration 9/1000 | Loss: 0.00178497
Iteration 10/1000 | Loss: 0.00054529
Iteration 11/1000 | Loss: 0.00028803
Iteration 12/1000 | Loss: 0.00037184
Iteration 13/1000 | Loss: 0.00057263
Iteration 14/1000 | Loss: 0.00164425
Iteration 15/1000 | Loss: 0.00100482
Iteration 16/1000 | Loss: 0.00068071
Iteration 17/1000 | Loss: 0.00027425
Iteration 18/1000 | Loss: 0.00065296
Iteration 19/1000 | Loss: 0.00028133
Iteration 20/1000 | Loss: 0.00029836
Iteration 21/1000 | Loss: 0.00038534
Iteration 22/1000 | Loss: 0.00035598
Iteration 23/1000 | Loss: 0.00127585
Iteration 24/1000 | Loss: 0.00024252
Iteration 25/1000 | Loss: 0.00016120
Iteration 26/1000 | Loss: 0.00030021
Iteration 27/1000 | Loss: 0.00010259
Iteration 28/1000 | Loss: 0.00029090
Iteration 29/1000 | Loss: 0.00103589
Iteration 30/1000 | Loss: 0.00038340
Iteration 31/1000 | Loss: 0.00014770
Iteration 32/1000 | Loss: 0.00015594
Iteration 33/1000 | Loss: 0.00010479
Iteration 34/1000 | Loss: 0.00046793
Iteration 35/1000 | Loss: 0.00149245
Iteration 36/1000 | Loss: 0.00136429
Iteration 37/1000 | Loss: 0.00026690
Iteration 38/1000 | Loss: 0.00013067
Iteration 39/1000 | Loss: 0.00010143
Iteration 40/1000 | Loss: 0.00027644
Iteration 41/1000 | Loss: 0.00008359
Iteration 42/1000 | Loss: 0.00007033
Iteration 43/1000 | Loss: 0.00014438
Iteration 44/1000 | Loss: 0.00039232
Iteration 45/1000 | Loss: 0.00014964
Iteration 46/1000 | Loss: 0.00051557
Iteration 47/1000 | Loss: 0.00104985
Iteration 48/1000 | Loss: 0.00066269
Iteration 49/1000 | Loss: 0.00067680
Iteration 50/1000 | Loss: 0.00058246
Iteration 51/1000 | Loss: 0.00051774
Iteration 52/1000 | Loss: 0.00028593
Iteration 53/1000 | Loss: 0.00007552
Iteration 54/1000 | Loss: 0.00014521
Iteration 55/1000 | Loss: 0.00006599
Iteration 56/1000 | Loss: 0.00010887
Iteration 57/1000 | Loss: 0.00032825
Iteration 58/1000 | Loss: 0.00007299
Iteration 59/1000 | Loss: 0.00018489
Iteration 60/1000 | Loss: 0.00093952
Iteration 61/1000 | Loss: 0.00360775
Iteration 62/1000 | Loss: 0.00120391
Iteration 63/1000 | Loss: 0.00110028
Iteration 64/1000 | Loss: 0.00046533
Iteration 65/1000 | Loss: 0.00085021
Iteration 66/1000 | Loss: 0.00069216
Iteration 67/1000 | Loss: 0.00040543
Iteration 68/1000 | Loss: 0.00047375
Iteration 69/1000 | Loss: 0.00027668
Iteration 70/1000 | Loss: 0.00008019
Iteration 71/1000 | Loss: 0.00009604
Iteration 72/1000 | Loss: 0.00005327
Iteration 73/1000 | Loss: 0.00004344
Iteration 74/1000 | Loss: 0.00004306
Iteration 75/1000 | Loss: 0.00003430
Iteration 76/1000 | Loss: 0.00003099
Iteration 77/1000 | Loss: 0.00002857
Iteration 78/1000 | Loss: 0.00002636
Iteration 79/1000 | Loss: 0.00002520
Iteration 80/1000 | Loss: 0.00002403
Iteration 81/1000 | Loss: 0.00010002
Iteration 82/1000 | Loss: 0.00014820
Iteration 83/1000 | Loss: 0.00019421
Iteration 84/1000 | Loss: 0.00008900
Iteration 85/1000 | Loss: 0.00010116
Iteration 86/1000 | Loss: 0.00009359
Iteration 87/1000 | Loss: 0.00005011
Iteration 88/1000 | Loss: 0.00021038
Iteration 89/1000 | Loss: 0.00013137
Iteration 90/1000 | Loss: 0.00003508
Iteration 91/1000 | Loss: 0.00003143
Iteration 92/1000 | Loss: 0.00002946
Iteration 93/1000 | Loss: 0.00002745
Iteration 94/1000 | Loss: 0.00031290
Iteration 95/1000 | Loss: 0.00002923
Iteration 96/1000 | Loss: 0.00002533
Iteration 97/1000 | Loss: 0.00002426
Iteration 98/1000 | Loss: 0.00002350
Iteration 99/1000 | Loss: 0.00002254
Iteration 100/1000 | Loss: 0.00002199
Iteration 101/1000 | Loss: 0.00002161
Iteration 102/1000 | Loss: 0.00002152
Iteration 103/1000 | Loss: 0.00002150
Iteration 104/1000 | Loss: 0.00002149
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002144
Iteration 107/1000 | Loss: 0.00002143
Iteration 108/1000 | Loss: 0.00002143
Iteration 109/1000 | Loss: 0.00002140
Iteration 110/1000 | Loss: 0.00002140
Iteration 111/1000 | Loss: 0.00002140
Iteration 112/1000 | Loss: 0.00002138
Iteration 113/1000 | Loss: 0.00002138
Iteration 114/1000 | Loss: 0.00002137
Iteration 115/1000 | Loss: 0.00002137
Iteration 116/1000 | Loss: 0.00002136
Iteration 117/1000 | Loss: 0.00002136
Iteration 118/1000 | Loss: 0.00002128
Iteration 119/1000 | Loss: 0.00002122
Iteration 120/1000 | Loss: 0.00002119
Iteration 121/1000 | Loss: 0.00002119
Iteration 122/1000 | Loss: 0.00002117
Iteration 123/1000 | Loss: 0.00002116
Iteration 124/1000 | Loss: 0.00002116
Iteration 125/1000 | Loss: 0.00002115
Iteration 126/1000 | Loss: 0.00002115
Iteration 127/1000 | Loss: 0.00002114
Iteration 128/1000 | Loss: 0.00002112
Iteration 129/1000 | Loss: 0.00002111
Iteration 130/1000 | Loss: 0.00002111
Iteration 131/1000 | Loss: 0.00002110
Iteration 132/1000 | Loss: 0.00002109
Iteration 133/1000 | Loss: 0.00002109
Iteration 134/1000 | Loss: 0.00002108
Iteration 135/1000 | Loss: 0.00002108
Iteration 136/1000 | Loss: 0.00002107
Iteration 137/1000 | Loss: 0.00002106
Iteration 138/1000 | Loss: 0.00002106
Iteration 139/1000 | Loss: 0.00002105
Iteration 140/1000 | Loss: 0.00002105
Iteration 141/1000 | Loss: 0.00002104
Iteration 142/1000 | Loss: 0.00010198
Iteration 143/1000 | Loss: 0.00010198
Iteration 144/1000 | Loss: 0.00046745
Iteration 145/1000 | Loss: 0.00011430
Iteration 146/1000 | Loss: 0.00011887
Iteration 147/1000 | Loss: 0.00003562
Iteration 148/1000 | Loss: 0.00013486
Iteration 149/1000 | Loss: 0.00012990
Iteration 150/1000 | Loss: 0.00014450
Iteration 151/1000 | Loss: 0.00002891
Iteration 152/1000 | Loss: 0.00013270
Iteration 153/1000 | Loss: 0.00014598
Iteration 154/1000 | Loss: 0.00013191
Iteration 155/1000 | Loss: 0.00012929
Iteration 156/1000 | Loss: 0.00031009
Iteration 157/1000 | Loss: 0.00026397
Iteration 158/1000 | Loss: 0.00066664
Iteration 159/1000 | Loss: 0.00004007
Iteration 160/1000 | Loss: 0.00002846
Iteration 161/1000 | Loss: 0.00002582
Iteration 162/1000 | Loss: 0.00002492
Iteration 163/1000 | Loss: 0.00002405
Iteration 164/1000 | Loss: 0.00002335
Iteration 165/1000 | Loss: 0.00002279
Iteration 166/1000 | Loss: 0.00002254
Iteration 167/1000 | Loss: 0.00002210
Iteration 168/1000 | Loss: 0.00002156
Iteration 169/1000 | Loss: 0.00002094
Iteration 170/1000 | Loss: 0.00002066
Iteration 171/1000 | Loss: 0.00002053
Iteration 172/1000 | Loss: 0.00002029
Iteration 173/1000 | Loss: 0.00002014
Iteration 174/1000 | Loss: 0.00002010
Iteration 175/1000 | Loss: 0.00002003
Iteration 176/1000 | Loss: 0.00001994
Iteration 177/1000 | Loss: 0.00001991
Iteration 178/1000 | Loss: 0.00001983
Iteration 179/1000 | Loss: 0.00001980
Iteration 180/1000 | Loss: 0.00032172
Iteration 181/1000 | Loss: 0.00015233
Iteration 182/1000 | Loss: 0.00001992
Iteration 183/1000 | Loss: 0.00024998
Iteration 184/1000 | Loss: 0.00009885
Iteration 185/1000 | Loss: 0.00024813
Iteration 186/1000 | Loss: 0.00054347
Iteration 187/1000 | Loss: 0.00009104
Iteration 188/1000 | Loss: 0.00020024
Iteration 189/1000 | Loss: 0.00002239
Iteration 190/1000 | Loss: 0.00010240
Iteration 191/1000 | Loss: 0.00001999
Iteration 192/1000 | Loss: 0.00001931
Iteration 193/1000 | Loss: 0.00001924
Iteration 194/1000 | Loss: 0.00001908
Iteration 195/1000 | Loss: 0.00001904
Iteration 196/1000 | Loss: 0.00001903
Iteration 197/1000 | Loss: 0.00013921
Iteration 198/1000 | Loss: 0.00004344
Iteration 199/1000 | Loss: 0.00001912
Iteration 200/1000 | Loss: 0.00006235
Iteration 201/1000 | Loss: 0.00003129
Iteration 202/1000 | Loss: 0.00001905
Iteration 203/1000 | Loss: 0.00007188
Iteration 204/1000 | Loss: 0.00001900
Iteration 205/1000 | Loss: 0.00001887
Iteration 206/1000 | Loss: 0.00001885
Iteration 207/1000 | Loss: 0.00001884
Iteration 208/1000 | Loss: 0.00001884
Iteration 209/1000 | Loss: 0.00001884
Iteration 210/1000 | Loss: 0.00001884
Iteration 211/1000 | Loss: 0.00001883
Iteration 212/1000 | Loss: 0.00001883
Iteration 213/1000 | Loss: 0.00001883
Iteration 214/1000 | Loss: 0.00001875
Iteration 215/1000 | Loss: 0.00001873
Iteration 216/1000 | Loss: 0.00001872
Iteration 217/1000 | Loss: 0.00001872
Iteration 218/1000 | Loss: 0.00001871
Iteration 219/1000 | Loss: 0.00001871
Iteration 220/1000 | Loss: 0.00001871
Iteration 221/1000 | Loss: 0.00001871
Iteration 222/1000 | Loss: 0.00001870
Iteration 223/1000 | Loss: 0.00001870
Iteration 224/1000 | Loss: 0.00001870
Iteration 225/1000 | Loss: 0.00001870
Iteration 226/1000 | Loss: 0.00001870
Iteration 227/1000 | Loss: 0.00001870
Iteration 228/1000 | Loss: 0.00001870
Iteration 229/1000 | Loss: 0.00001870
Iteration 230/1000 | Loss: 0.00001870
Iteration 231/1000 | Loss: 0.00001870
Iteration 232/1000 | Loss: 0.00001869
Iteration 233/1000 | Loss: 0.00001869
Iteration 234/1000 | Loss: 0.00001869
Iteration 235/1000 | Loss: 0.00001869
Iteration 236/1000 | Loss: 0.00001869
Iteration 237/1000 | Loss: 0.00001868
Iteration 238/1000 | Loss: 0.00001868
Iteration 239/1000 | Loss: 0.00001868
Iteration 240/1000 | Loss: 0.00001868
Iteration 241/1000 | Loss: 0.00001868
Iteration 242/1000 | Loss: 0.00001868
Iteration 243/1000 | Loss: 0.00001868
Iteration 244/1000 | Loss: 0.00001867
Iteration 245/1000 | Loss: 0.00001867
Iteration 246/1000 | Loss: 0.00001867
Iteration 247/1000 | Loss: 0.00001867
Iteration 248/1000 | Loss: 0.00001866
Iteration 249/1000 | Loss: 0.00001866
Iteration 250/1000 | Loss: 0.00001866
Iteration 251/1000 | Loss: 0.00001866
Iteration 252/1000 | Loss: 0.00001866
Iteration 253/1000 | Loss: 0.00001866
Iteration 254/1000 | Loss: 0.00001866
Iteration 255/1000 | Loss: 0.00001866
Iteration 256/1000 | Loss: 0.00001866
Iteration 257/1000 | Loss: 0.00001866
Iteration 258/1000 | Loss: 0.00001865
Iteration 259/1000 | Loss: 0.00001865
Iteration 260/1000 | Loss: 0.00001865
Iteration 261/1000 | Loss: 0.00001865
Iteration 262/1000 | Loss: 0.00001865
Iteration 263/1000 | Loss: 0.00001865
Iteration 264/1000 | Loss: 0.00001865
Iteration 265/1000 | Loss: 0.00001865
Iteration 266/1000 | Loss: 0.00001865
Iteration 267/1000 | Loss: 0.00001864
Iteration 268/1000 | Loss: 0.00001864
Iteration 269/1000 | Loss: 0.00001864
Iteration 270/1000 | Loss: 0.00001864
Iteration 271/1000 | Loss: 0.00001864
Iteration 272/1000 | Loss: 0.00001864
Iteration 273/1000 | Loss: 0.00001864
Iteration 274/1000 | Loss: 0.00001864
Iteration 275/1000 | Loss: 0.00001864
Iteration 276/1000 | Loss: 0.00001864
Iteration 277/1000 | Loss: 0.00001864
Iteration 278/1000 | Loss: 0.00001864
Iteration 279/1000 | Loss: 0.00001864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [1.8642018403625116e-05, 1.8642018403625116e-05, 1.8642018403625116e-05, 1.8642018403625116e-05, 1.8642018403625116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8642018403625116e-05

Optimization complete. Final v2v error: 3.4963254928588867 mm

Highest mean error: 4.962263107299805 mm for frame 2

Lowest mean error: 3.1134490966796875 mm for frame 127

Saving results

Total time: 301.11071610450745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059622
Iteration 2/25 | Loss: 0.00525158
Iteration 3/25 | Loss: 0.00342893
Iteration 4/25 | Loss: 0.00354286
Iteration 5/25 | Loss: 0.00291708
Iteration 6/25 | Loss: 0.00250960
Iteration 7/25 | Loss: 0.00246915
Iteration 8/25 | Loss: 0.00229861
Iteration 9/25 | Loss: 0.00218539
Iteration 10/25 | Loss: 0.00220444
Iteration 11/25 | Loss: 0.00204763
Iteration 12/25 | Loss: 0.00190811
Iteration 13/25 | Loss: 0.00191698
Iteration 14/25 | Loss: 0.00191009
Iteration 15/25 | Loss: 0.00186875
Iteration 16/25 | Loss: 0.00183722
Iteration 17/25 | Loss: 0.00181590
Iteration 18/25 | Loss: 0.00179901
Iteration 19/25 | Loss: 0.00178569
Iteration 20/25 | Loss: 0.00175231
Iteration 21/25 | Loss: 0.00173109
Iteration 22/25 | Loss: 0.00173041
Iteration 23/25 | Loss: 0.00172866
Iteration 24/25 | Loss: 0.00171978
Iteration 25/25 | Loss: 0.00172121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.38895866
Iteration 2/25 | Loss: 0.00447515
Iteration 3/25 | Loss: 0.00335611
Iteration 4/25 | Loss: 0.00341373
Iteration 5/25 | Loss: 0.00365680
Iteration 6/25 | Loss: 0.00185846
Iteration 7/25 | Loss: 0.00320270
Iteration 8/25 | Loss: 0.00367631
Iteration 9/25 | Loss: 0.00185333
Iteration 10/25 | Loss: 0.00185329
Iteration 11/25 | Loss: 0.00185328
Iteration 12/25 | Loss: 0.00185328
Iteration 13/25 | Loss: 0.00185328
Iteration 14/25 | Loss: 0.00185328
Iteration 15/25 | Loss: 0.00185328
Iteration 16/25 | Loss: 0.00185328
Iteration 17/25 | Loss: 0.00185328
Iteration 18/25 | Loss: 0.00185328
Iteration 19/25 | Loss: 0.00185328
Iteration 20/25 | Loss: 0.00185328
Iteration 21/25 | Loss: 0.00185328
Iteration 22/25 | Loss: 0.00185328
Iteration 23/25 | Loss: 0.00185328
Iteration 24/25 | Loss: 0.00185328
Iteration 25/25 | Loss: 0.00185328

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185328
Iteration 2/1000 | Loss: 0.00040570
Iteration 3/1000 | Loss: 0.00073091
Iteration 4/1000 | Loss: 0.00053541
Iteration 5/1000 | Loss: 0.00041974
Iteration 6/1000 | Loss: 0.00036983
Iteration 7/1000 | Loss: 0.00037445
Iteration 8/1000 | Loss: 0.00026485
Iteration 9/1000 | Loss: 0.00079316
Iteration 10/1000 | Loss: 0.00026626
Iteration 11/1000 | Loss: 0.00021123
Iteration 12/1000 | Loss: 0.00070413
Iteration 13/1000 | Loss: 0.00035577
Iteration 14/1000 | Loss: 0.00017495
Iteration 15/1000 | Loss: 0.00026177
Iteration 16/1000 | Loss: 0.00025199
Iteration 17/1000 | Loss: 0.00022526
Iteration 18/1000 | Loss: 0.00020565
Iteration 19/1000 | Loss: 0.00024656
Iteration 20/1000 | Loss: 0.00027345
Iteration 21/1000 | Loss: 0.00028910
Iteration 22/1000 | Loss: 0.00024880
Iteration 23/1000 | Loss: 0.00019449
Iteration 24/1000 | Loss: 0.00022988
Iteration 25/1000 | Loss: 0.00020402
Iteration 26/1000 | Loss: 0.00037772
Iteration 27/1000 | Loss: 0.00030575
Iteration 28/1000 | Loss: 0.00030040
Iteration 29/1000 | Loss: 0.00029525
Iteration 30/1000 | Loss: 0.00032910
Iteration 31/1000 | Loss: 0.00029035
Iteration 32/1000 | Loss: 0.00032258
Iteration 33/1000 | Loss: 0.00043276
Iteration 34/1000 | Loss: 0.00037361
Iteration 35/1000 | Loss: 0.00043547
Iteration 36/1000 | Loss: 0.00032176
Iteration 37/1000 | Loss: 0.00017044
Iteration 38/1000 | Loss: 0.00016611
Iteration 39/1000 | Loss: 0.00018248
Iteration 40/1000 | Loss: 0.00025447
Iteration 41/1000 | Loss: 0.00021296
Iteration 42/1000 | Loss: 0.00014361
Iteration 43/1000 | Loss: 0.00022965
Iteration 44/1000 | Loss: 0.00021255
Iteration 45/1000 | Loss: 0.00017040
Iteration 46/1000 | Loss: 0.00020965
Iteration 47/1000 | Loss: 0.00020292
Iteration 48/1000 | Loss: 0.00019948
Iteration 49/1000 | Loss: 0.00016097
Iteration 50/1000 | Loss: 0.00072165
Iteration 51/1000 | Loss: 0.00035759
Iteration 52/1000 | Loss: 0.00108597
Iteration 53/1000 | Loss: 0.00047197
Iteration 54/1000 | Loss: 0.00026266
Iteration 55/1000 | Loss: 0.00023960
Iteration 56/1000 | Loss: 0.00035984
Iteration 57/1000 | Loss: 0.00039316
Iteration 58/1000 | Loss: 0.00038309
Iteration 59/1000 | Loss: 0.00024666
Iteration 60/1000 | Loss: 0.00065318
Iteration 61/1000 | Loss: 0.00035666
Iteration 62/1000 | Loss: 0.00032886
Iteration 63/1000 | Loss: 0.00024626
Iteration 64/1000 | Loss: 0.00038249
Iteration 65/1000 | Loss: 0.00036464
Iteration 66/1000 | Loss: 0.00036840
Iteration 67/1000 | Loss: 0.00040716
Iteration 68/1000 | Loss: 0.00037397
Iteration 69/1000 | Loss: 0.00031688
Iteration 70/1000 | Loss: 0.00026746
Iteration 71/1000 | Loss: 0.00023090
Iteration 72/1000 | Loss: 0.00030885
Iteration 73/1000 | Loss: 0.00027079
Iteration 74/1000 | Loss: 0.00043196
Iteration 75/1000 | Loss: 0.00023230
Iteration 76/1000 | Loss: 0.00087658
Iteration 77/1000 | Loss: 0.00029522
Iteration 78/1000 | Loss: 0.00011478
Iteration 79/1000 | Loss: 0.00043740
Iteration 80/1000 | Loss: 0.00011062
Iteration 81/1000 | Loss: 0.00010220
Iteration 82/1000 | Loss: 0.00009820
Iteration 83/1000 | Loss: 0.00009973
Iteration 84/1000 | Loss: 0.00012532
Iteration 85/1000 | Loss: 0.00011541
Iteration 86/1000 | Loss: 0.00012399
Iteration 87/1000 | Loss: 0.00009856
Iteration 88/1000 | Loss: 0.00009831
Iteration 89/1000 | Loss: 0.00010590
Iteration 90/1000 | Loss: 0.00019510
Iteration 91/1000 | Loss: 0.00015693
Iteration 92/1000 | Loss: 0.00012449
Iteration 93/1000 | Loss: 0.00013421
Iteration 94/1000 | Loss: 0.00010463
Iteration 95/1000 | Loss: 0.00009020
Iteration 96/1000 | Loss: 0.00009494
Iteration 97/1000 | Loss: 0.00008782
Iteration 98/1000 | Loss: 0.00008460
Iteration 99/1000 | Loss: 0.00021134
Iteration 100/1000 | Loss: 0.00013711
Iteration 101/1000 | Loss: 0.00016283
Iteration 102/1000 | Loss: 0.00015577
Iteration 103/1000 | Loss: 0.00010524
Iteration 104/1000 | Loss: 0.00015786
Iteration 105/1000 | Loss: 0.00012437
Iteration 106/1000 | Loss: 0.00013799
Iteration 107/1000 | Loss: 0.00011784
Iteration 108/1000 | Loss: 0.00015680
Iteration 109/1000 | Loss: 0.00011041
Iteration 110/1000 | Loss: 0.00009274
Iteration 111/1000 | Loss: 0.00008153
Iteration 112/1000 | Loss: 0.00009320
Iteration 113/1000 | Loss: 0.00008703
Iteration 114/1000 | Loss: 0.00008388
Iteration 115/1000 | Loss: 0.00008498
Iteration 116/1000 | Loss: 0.00008981
Iteration 117/1000 | Loss: 0.00008919
Iteration 118/1000 | Loss: 0.00008982
Iteration 119/1000 | Loss: 0.00008796
Iteration 120/1000 | Loss: 0.00009067
Iteration 121/1000 | Loss: 0.00008880
Iteration 122/1000 | Loss: 0.00008684
Iteration 123/1000 | Loss: 0.00008740
Iteration 124/1000 | Loss: 0.00008997
Iteration 125/1000 | Loss: 0.00009339
Iteration 126/1000 | Loss: 0.00008877
Iteration 127/1000 | Loss: 0.00009548
Iteration 128/1000 | Loss: 0.00008850
Iteration 129/1000 | Loss: 0.00009493
Iteration 130/1000 | Loss: 0.00008951
Iteration 131/1000 | Loss: 0.00009094
Iteration 132/1000 | Loss: 0.00009017
Iteration 133/1000 | Loss: 0.00009076
Iteration 134/1000 | Loss: 0.00008890
Iteration 135/1000 | Loss: 0.00009379
Iteration 136/1000 | Loss: 0.00008814
Iteration 137/1000 | Loss: 0.00009090
Iteration 138/1000 | Loss: 0.00008980
Iteration 139/1000 | Loss: 0.00009249
Iteration 140/1000 | Loss: 0.00008188
Iteration 141/1000 | Loss: 0.00009385
Iteration 142/1000 | Loss: 0.00009122
Iteration 143/1000 | Loss: 0.00008985
Iteration 144/1000 | Loss: 0.00009023
Iteration 145/1000 | Loss: 0.00009275
Iteration 146/1000 | Loss: 0.00008928
Iteration 147/1000 | Loss: 0.00009213
Iteration 148/1000 | Loss: 0.00008716
Iteration 149/1000 | Loss: 0.00009255
Iteration 150/1000 | Loss: 0.00008698
Iteration 151/1000 | Loss: 0.00009140
Iteration 152/1000 | Loss: 0.00008703
Iteration 153/1000 | Loss: 0.00009097
Iteration 154/1000 | Loss: 0.00009106
Iteration 155/1000 | Loss: 0.00009033
Iteration 156/1000 | Loss: 0.00009452
Iteration 157/1000 | Loss: 0.00009020
Iteration 158/1000 | Loss: 0.00008684
Iteration 159/1000 | Loss: 0.00009029
Iteration 160/1000 | Loss: 0.00009369
Iteration 161/1000 | Loss: 0.00009056
Iteration 162/1000 | Loss: 0.00009051
Iteration 163/1000 | Loss: 0.00009362
Iteration 164/1000 | Loss: 0.00008970
Iteration 165/1000 | Loss: 0.00009352
Iteration 166/1000 | Loss: 0.00008561
Iteration 167/1000 | Loss: 0.00008493
Iteration 168/1000 | Loss: 0.00008855
Iteration 169/1000 | Loss: 0.00008892
Iteration 170/1000 | Loss: 0.00008634
Iteration 171/1000 | Loss: 0.00008749
Iteration 172/1000 | Loss: 0.00008596
Iteration 173/1000 | Loss: 0.00009026
Iteration 174/1000 | Loss: 0.00009432
Iteration 175/1000 | Loss: 0.00009180
Iteration 176/1000 | Loss: 0.00008172
Iteration 177/1000 | Loss: 0.00009448
Iteration 178/1000 | Loss: 0.00008545
Iteration 179/1000 | Loss: 0.00008617
Iteration 180/1000 | Loss: 0.00008796
Iteration 181/1000 | Loss: 0.00008487
Iteration 182/1000 | Loss: 0.00008812
Iteration 183/1000 | Loss: 0.00009172
Iteration 184/1000 | Loss: 0.00007796
Iteration 185/1000 | Loss: 0.00007658
Iteration 186/1000 | Loss: 0.00007587
Iteration 187/1000 | Loss: 0.00007562
Iteration 188/1000 | Loss: 0.00007532
Iteration 189/1000 | Loss: 0.00007513
Iteration 190/1000 | Loss: 0.00007509
Iteration 191/1000 | Loss: 0.00007496
Iteration 192/1000 | Loss: 0.00007493
Iteration 193/1000 | Loss: 0.00007491
Iteration 194/1000 | Loss: 0.00007486
Iteration 195/1000 | Loss: 0.00007471
Iteration 196/1000 | Loss: 0.00007470
Iteration 197/1000 | Loss: 0.00007469
Iteration 198/1000 | Loss: 0.00007458
Iteration 199/1000 | Loss: 0.00007444
Iteration 200/1000 | Loss: 0.00007429
Iteration 201/1000 | Loss: 0.00007425
Iteration 202/1000 | Loss: 0.00007411
Iteration 203/1000 | Loss: 0.00007409
Iteration 204/1000 | Loss: 0.00007409
Iteration 205/1000 | Loss: 0.00007404
Iteration 206/1000 | Loss: 0.00007397
Iteration 207/1000 | Loss: 0.00007397
Iteration 208/1000 | Loss: 0.00007397
Iteration 209/1000 | Loss: 0.00007397
Iteration 210/1000 | Loss: 0.00007397
Iteration 211/1000 | Loss: 0.00007397
Iteration 212/1000 | Loss: 0.00007397
Iteration 213/1000 | Loss: 0.00007395
Iteration 214/1000 | Loss: 0.00007394
Iteration 215/1000 | Loss: 0.00007394
Iteration 216/1000 | Loss: 0.00007393
Iteration 217/1000 | Loss: 0.00007388
Iteration 218/1000 | Loss: 0.00007388
Iteration 219/1000 | Loss: 0.00007388
Iteration 220/1000 | Loss: 0.00007388
Iteration 221/1000 | Loss: 0.00007387
Iteration 222/1000 | Loss: 0.00007387
Iteration 223/1000 | Loss: 0.00007387
Iteration 224/1000 | Loss: 0.00007387
Iteration 225/1000 | Loss: 0.00007387
Iteration 226/1000 | Loss: 0.00007387
Iteration 227/1000 | Loss: 0.00007387
Iteration 228/1000 | Loss: 0.00007387
Iteration 229/1000 | Loss: 0.00007387
Iteration 230/1000 | Loss: 0.00007387
Iteration 231/1000 | Loss: 0.00007387
Iteration 232/1000 | Loss: 0.00007387
Iteration 233/1000 | Loss: 0.00007386
Iteration 234/1000 | Loss: 0.00007386
Iteration 235/1000 | Loss: 0.00007386
Iteration 236/1000 | Loss: 0.00007386
Iteration 237/1000 | Loss: 0.00007386
Iteration 238/1000 | Loss: 0.00007386
Iteration 239/1000 | Loss: 0.00007386
Iteration 240/1000 | Loss: 0.00007385
Iteration 241/1000 | Loss: 0.00007385
Iteration 242/1000 | Loss: 0.00007385
Iteration 243/1000 | Loss: 0.00007385
Iteration 244/1000 | Loss: 0.00007385
Iteration 245/1000 | Loss: 0.00007384
Iteration 246/1000 | Loss: 0.00007384
Iteration 247/1000 | Loss: 0.00007384
Iteration 248/1000 | Loss: 0.00007384
Iteration 249/1000 | Loss: 0.00007384
Iteration 250/1000 | Loss: 0.00007384
Iteration 251/1000 | Loss: 0.00007384
Iteration 252/1000 | Loss: 0.00007384
Iteration 253/1000 | Loss: 0.00007384
Iteration 254/1000 | Loss: 0.00007384
Iteration 255/1000 | Loss: 0.00007383
Iteration 256/1000 | Loss: 0.00007383
Iteration 257/1000 | Loss: 0.00007383
Iteration 258/1000 | Loss: 0.00007383
Iteration 259/1000 | Loss: 0.00007383
Iteration 260/1000 | Loss: 0.00007383
Iteration 261/1000 | Loss: 0.00007382
Iteration 262/1000 | Loss: 0.00007382
Iteration 263/1000 | Loss: 0.00007382
Iteration 264/1000 | Loss: 0.00007382
Iteration 265/1000 | Loss: 0.00007382
Iteration 266/1000 | Loss: 0.00007382
Iteration 267/1000 | Loss: 0.00007382
Iteration 268/1000 | Loss: 0.00007382
Iteration 269/1000 | Loss: 0.00007382
Iteration 270/1000 | Loss: 0.00007381
Iteration 271/1000 | Loss: 0.00007381
Iteration 272/1000 | Loss: 0.00007381
Iteration 273/1000 | Loss: 0.00007380
Iteration 274/1000 | Loss: 0.00007380
Iteration 275/1000 | Loss: 0.00007380
Iteration 276/1000 | Loss: 0.00007379
Iteration 277/1000 | Loss: 0.00007379
Iteration 278/1000 | Loss: 0.00007379
Iteration 279/1000 | Loss: 0.00007379
Iteration 280/1000 | Loss: 0.00007379
Iteration 281/1000 | Loss: 0.00007379
Iteration 282/1000 | Loss: 0.00007379
Iteration 283/1000 | Loss: 0.00007379
Iteration 284/1000 | Loss: 0.00007379
Iteration 285/1000 | Loss: 0.00007379
Iteration 286/1000 | Loss: 0.00007379
Iteration 287/1000 | Loss: 0.00007379
Iteration 288/1000 | Loss: 0.00007379
Iteration 289/1000 | Loss: 0.00007378
Iteration 290/1000 | Loss: 0.00007378
Iteration 291/1000 | Loss: 0.00007378
Iteration 292/1000 | Loss: 0.00007378
Iteration 293/1000 | Loss: 0.00007378
Iteration 294/1000 | Loss: 0.00007378
Iteration 295/1000 | Loss: 0.00007378
Iteration 296/1000 | Loss: 0.00007377
Iteration 297/1000 | Loss: 0.00007377
Iteration 298/1000 | Loss: 0.00007377
Iteration 299/1000 | Loss: 0.00007377
Iteration 300/1000 | Loss: 0.00007377
Iteration 301/1000 | Loss: 0.00007377
Iteration 302/1000 | Loss: 0.00007377
Iteration 303/1000 | Loss: 0.00007377
Iteration 304/1000 | Loss: 0.00007377
Iteration 305/1000 | Loss: 0.00007377
Iteration 306/1000 | Loss: 0.00007377
Iteration 307/1000 | Loss: 0.00007377
Iteration 308/1000 | Loss: 0.00007376
Iteration 309/1000 | Loss: 0.00007376
Iteration 310/1000 | Loss: 0.00007376
Iteration 311/1000 | Loss: 0.00007376
Iteration 312/1000 | Loss: 0.00007376
Iteration 313/1000 | Loss: 0.00007376
Iteration 314/1000 | Loss: 0.00007376
Iteration 315/1000 | Loss: 0.00007376
Iteration 316/1000 | Loss: 0.00007376
Iteration 317/1000 | Loss: 0.00007376
Iteration 318/1000 | Loss: 0.00007376
Iteration 319/1000 | Loss: 0.00007376
Iteration 320/1000 | Loss: 0.00007376
Iteration 321/1000 | Loss: 0.00007376
Iteration 322/1000 | Loss: 0.00007376
Iteration 323/1000 | Loss: 0.00007375
Iteration 324/1000 | Loss: 0.00007375
Iteration 325/1000 | Loss: 0.00007375
Iteration 326/1000 | Loss: 0.00007375
Iteration 327/1000 | Loss: 0.00007375
Iteration 328/1000 | Loss: 0.00007375
Iteration 329/1000 | Loss: 0.00007375
Iteration 330/1000 | Loss: 0.00007375
Iteration 331/1000 | Loss: 0.00007375
Iteration 332/1000 | Loss: 0.00007375
Iteration 333/1000 | Loss: 0.00007375
Iteration 334/1000 | Loss: 0.00007375
Iteration 335/1000 | Loss: 0.00007375
Iteration 336/1000 | Loss: 0.00007375
Iteration 337/1000 | Loss: 0.00007374
Iteration 338/1000 | Loss: 0.00007374
Iteration 339/1000 | Loss: 0.00007374
Iteration 340/1000 | Loss: 0.00007374
Iteration 341/1000 | Loss: 0.00007374
Iteration 342/1000 | Loss: 0.00007374
Iteration 343/1000 | Loss: 0.00007374
Iteration 344/1000 | Loss: 0.00007374
Iteration 345/1000 | Loss: 0.00007374
Iteration 346/1000 | Loss: 0.00007374
Iteration 347/1000 | Loss: 0.00007374
Iteration 348/1000 | Loss: 0.00007374
Iteration 349/1000 | Loss: 0.00007374
Iteration 350/1000 | Loss: 0.00007374
Iteration 351/1000 | Loss: 0.00007374
Iteration 352/1000 | Loss: 0.00007374
Iteration 353/1000 | Loss: 0.00007374
Iteration 354/1000 | Loss: 0.00007374
Iteration 355/1000 | Loss: 0.00007374
Iteration 356/1000 | Loss: 0.00007374
Iteration 357/1000 | Loss: 0.00007374
Iteration 358/1000 | Loss: 0.00007374
Iteration 359/1000 | Loss: 0.00007374
Iteration 360/1000 | Loss: 0.00007374
Iteration 361/1000 | Loss: 0.00007374
Iteration 362/1000 | Loss: 0.00007374
Iteration 363/1000 | Loss: 0.00007374
Iteration 364/1000 | Loss: 0.00007374
Iteration 365/1000 | Loss: 0.00007374
Iteration 366/1000 | Loss: 0.00007374
Iteration 367/1000 | Loss: 0.00007374
Iteration 368/1000 | Loss: 0.00007374
Iteration 369/1000 | Loss: 0.00007374
Iteration 370/1000 | Loss: 0.00007374
Iteration 371/1000 | Loss: 0.00007374
Iteration 372/1000 | Loss: 0.00007374
Iteration 373/1000 | Loss: 0.00007374
Iteration 374/1000 | Loss: 0.00007374
Iteration 375/1000 | Loss: 0.00007374
Iteration 376/1000 | Loss: 0.00007374
Iteration 377/1000 | Loss: 0.00007374
Iteration 378/1000 | Loss: 0.00007374
Iteration 379/1000 | Loss: 0.00007374
Iteration 380/1000 | Loss: 0.00007374
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 380. Stopping optimization.
Last 5 losses: [7.37378213671036e-05, 7.37378213671036e-05, 7.37378213671036e-05, 7.37378213671036e-05, 7.37378213671036e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.37378213671036e-05

Optimization complete. Final v2v error: 6.2152204513549805 mm

Highest mean error: 12.582592010498047 mm for frame 150

Lowest mean error: 4.398911952972412 mm for frame 10

Saving results

Total time: 399.86929178237915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823504
Iteration 2/25 | Loss: 0.00122995
Iteration 3/25 | Loss: 0.00113478
Iteration 4/25 | Loss: 0.00112233
Iteration 5/25 | Loss: 0.00111983
Iteration 6/25 | Loss: 0.00111983
Iteration 7/25 | Loss: 0.00111983
Iteration 8/25 | Loss: 0.00111983
Iteration 9/25 | Loss: 0.00111983
Iteration 10/25 | Loss: 0.00111983
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011198330903425813, 0.0011198330903425813, 0.0011198330903425813, 0.0011198330903425813, 0.0011198330903425813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011198330903425813

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35841548
Iteration 2/25 | Loss: 0.00077279
Iteration 3/25 | Loss: 0.00077278
Iteration 4/25 | Loss: 0.00077278
Iteration 5/25 | Loss: 0.00077278
Iteration 6/25 | Loss: 0.00077278
Iteration 7/25 | Loss: 0.00077278
Iteration 8/25 | Loss: 0.00077278
Iteration 9/25 | Loss: 0.00077278
Iteration 10/25 | Loss: 0.00077278
Iteration 11/25 | Loss: 0.00077278
Iteration 12/25 | Loss: 0.00077278
Iteration 13/25 | Loss: 0.00077278
Iteration 14/25 | Loss: 0.00077278
Iteration 15/25 | Loss: 0.00077278
Iteration 16/25 | Loss: 0.00077278
Iteration 17/25 | Loss: 0.00077278
Iteration 18/25 | Loss: 0.00077278
Iteration 19/25 | Loss: 0.00077278
Iteration 20/25 | Loss: 0.00077278
Iteration 21/25 | Loss: 0.00077278
Iteration 22/25 | Loss: 0.00077278
Iteration 23/25 | Loss: 0.00077278
Iteration 24/25 | Loss: 0.00077278
Iteration 25/25 | Loss: 0.00077278

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077278
Iteration 2/1000 | Loss: 0.00001874
Iteration 3/1000 | Loss: 0.00001382
Iteration 4/1000 | Loss: 0.00001247
Iteration 5/1000 | Loss: 0.00001151
Iteration 6/1000 | Loss: 0.00001099
Iteration 7/1000 | Loss: 0.00001070
Iteration 8/1000 | Loss: 0.00001046
Iteration 9/1000 | Loss: 0.00001022
Iteration 10/1000 | Loss: 0.00001015
Iteration 11/1000 | Loss: 0.00001014
Iteration 12/1000 | Loss: 0.00001013
Iteration 13/1000 | Loss: 0.00001012
Iteration 14/1000 | Loss: 0.00001007
Iteration 15/1000 | Loss: 0.00001003
Iteration 16/1000 | Loss: 0.00001002
Iteration 17/1000 | Loss: 0.00001002
Iteration 18/1000 | Loss: 0.00001002
Iteration 19/1000 | Loss: 0.00001002
Iteration 20/1000 | Loss: 0.00001001
Iteration 21/1000 | Loss: 0.00001001
Iteration 22/1000 | Loss: 0.00000998
Iteration 23/1000 | Loss: 0.00000997
Iteration 24/1000 | Loss: 0.00000997
Iteration 25/1000 | Loss: 0.00000997
Iteration 26/1000 | Loss: 0.00000996
Iteration 27/1000 | Loss: 0.00000996
Iteration 28/1000 | Loss: 0.00000996
Iteration 29/1000 | Loss: 0.00000993
Iteration 30/1000 | Loss: 0.00000991
Iteration 31/1000 | Loss: 0.00000991
Iteration 32/1000 | Loss: 0.00000987
Iteration 33/1000 | Loss: 0.00000987
Iteration 34/1000 | Loss: 0.00000985
Iteration 35/1000 | Loss: 0.00000985
Iteration 36/1000 | Loss: 0.00000985
Iteration 37/1000 | Loss: 0.00000984
Iteration 38/1000 | Loss: 0.00000983
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000981
Iteration 41/1000 | Loss: 0.00000981
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000978
Iteration 44/1000 | Loss: 0.00000975
Iteration 45/1000 | Loss: 0.00000974
Iteration 46/1000 | Loss: 0.00000974
Iteration 47/1000 | Loss: 0.00000973
Iteration 48/1000 | Loss: 0.00000973
Iteration 49/1000 | Loss: 0.00000972
Iteration 50/1000 | Loss: 0.00000971
Iteration 51/1000 | Loss: 0.00000970
Iteration 52/1000 | Loss: 0.00000970
Iteration 53/1000 | Loss: 0.00000969
Iteration 54/1000 | Loss: 0.00000969
Iteration 55/1000 | Loss: 0.00000968
Iteration 56/1000 | Loss: 0.00000968
Iteration 57/1000 | Loss: 0.00000967
Iteration 58/1000 | Loss: 0.00000967
Iteration 59/1000 | Loss: 0.00000967
Iteration 60/1000 | Loss: 0.00000966
Iteration 61/1000 | Loss: 0.00000966
Iteration 62/1000 | Loss: 0.00000966
Iteration 63/1000 | Loss: 0.00000965
Iteration 64/1000 | Loss: 0.00000965
Iteration 65/1000 | Loss: 0.00000965
Iteration 66/1000 | Loss: 0.00000965
Iteration 67/1000 | Loss: 0.00000965
Iteration 68/1000 | Loss: 0.00000965
Iteration 69/1000 | Loss: 0.00000965
Iteration 70/1000 | Loss: 0.00000964
Iteration 71/1000 | Loss: 0.00000963
Iteration 72/1000 | Loss: 0.00000963
Iteration 73/1000 | Loss: 0.00000963
Iteration 74/1000 | Loss: 0.00000962
Iteration 75/1000 | Loss: 0.00000962
Iteration 76/1000 | Loss: 0.00000961
Iteration 77/1000 | Loss: 0.00000961
Iteration 78/1000 | Loss: 0.00000961
Iteration 79/1000 | Loss: 0.00000961
Iteration 80/1000 | Loss: 0.00000961
Iteration 81/1000 | Loss: 0.00000960
Iteration 82/1000 | Loss: 0.00000960
Iteration 83/1000 | Loss: 0.00000960
Iteration 84/1000 | Loss: 0.00000958
Iteration 85/1000 | Loss: 0.00000958
Iteration 86/1000 | Loss: 0.00000958
Iteration 87/1000 | Loss: 0.00000957
Iteration 88/1000 | Loss: 0.00000957
Iteration 89/1000 | Loss: 0.00000956
Iteration 90/1000 | Loss: 0.00000956
Iteration 91/1000 | Loss: 0.00000956
Iteration 92/1000 | Loss: 0.00000956
Iteration 93/1000 | Loss: 0.00000956
Iteration 94/1000 | Loss: 0.00000956
Iteration 95/1000 | Loss: 0.00000956
Iteration 96/1000 | Loss: 0.00000955
Iteration 97/1000 | Loss: 0.00000955
Iteration 98/1000 | Loss: 0.00000955
Iteration 99/1000 | Loss: 0.00000955
Iteration 100/1000 | Loss: 0.00000955
Iteration 101/1000 | Loss: 0.00000955
Iteration 102/1000 | Loss: 0.00000955
Iteration 103/1000 | Loss: 0.00000955
Iteration 104/1000 | Loss: 0.00000954
Iteration 105/1000 | Loss: 0.00000954
Iteration 106/1000 | Loss: 0.00000954
Iteration 107/1000 | Loss: 0.00000954
Iteration 108/1000 | Loss: 0.00000954
Iteration 109/1000 | Loss: 0.00000953
Iteration 110/1000 | Loss: 0.00000953
Iteration 111/1000 | Loss: 0.00000953
Iteration 112/1000 | Loss: 0.00000953
Iteration 113/1000 | Loss: 0.00000953
Iteration 114/1000 | Loss: 0.00000952
Iteration 115/1000 | Loss: 0.00000952
Iteration 116/1000 | Loss: 0.00000952
Iteration 117/1000 | Loss: 0.00000951
Iteration 118/1000 | Loss: 0.00000951
Iteration 119/1000 | Loss: 0.00000951
Iteration 120/1000 | Loss: 0.00000951
Iteration 121/1000 | Loss: 0.00000951
Iteration 122/1000 | Loss: 0.00000951
Iteration 123/1000 | Loss: 0.00000951
Iteration 124/1000 | Loss: 0.00000951
Iteration 125/1000 | Loss: 0.00000951
Iteration 126/1000 | Loss: 0.00000951
Iteration 127/1000 | Loss: 0.00000951
Iteration 128/1000 | Loss: 0.00000950
Iteration 129/1000 | Loss: 0.00000950
Iteration 130/1000 | Loss: 0.00000949
Iteration 131/1000 | Loss: 0.00000949
Iteration 132/1000 | Loss: 0.00000949
Iteration 133/1000 | Loss: 0.00000949
Iteration 134/1000 | Loss: 0.00000949
Iteration 135/1000 | Loss: 0.00000948
Iteration 136/1000 | Loss: 0.00000948
Iteration 137/1000 | Loss: 0.00000948
Iteration 138/1000 | Loss: 0.00000948
Iteration 139/1000 | Loss: 0.00000948
Iteration 140/1000 | Loss: 0.00000947
Iteration 141/1000 | Loss: 0.00000946
Iteration 142/1000 | Loss: 0.00000946
Iteration 143/1000 | Loss: 0.00000946
Iteration 144/1000 | Loss: 0.00000946
Iteration 145/1000 | Loss: 0.00000945
Iteration 146/1000 | Loss: 0.00000945
Iteration 147/1000 | Loss: 0.00000945
Iteration 148/1000 | Loss: 0.00000944
Iteration 149/1000 | Loss: 0.00000944
Iteration 150/1000 | Loss: 0.00000944
Iteration 151/1000 | Loss: 0.00000943
Iteration 152/1000 | Loss: 0.00000943
Iteration 153/1000 | Loss: 0.00000943
Iteration 154/1000 | Loss: 0.00000942
Iteration 155/1000 | Loss: 0.00000941
Iteration 156/1000 | Loss: 0.00000941
Iteration 157/1000 | Loss: 0.00000941
Iteration 158/1000 | Loss: 0.00000940
Iteration 159/1000 | Loss: 0.00000940
Iteration 160/1000 | Loss: 0.00000940
Iteration 161/1000 | Loss: 0.00000940
Iteration 162/1000 | Loss: 0.00000939
Iteration 163/1000 | Loss: 0.00000939
Iteration 164/1000 | Loss: 0.00000939
Iteration 165/1000 | Loss: 0.00000938
Iteration 166/1000 | Loss: 0.00000938
Iteration 167/1000 | Loss: 0.00000938
Iteration 168/1000 | Loss: 0.00000938
Iteration 169/1000 | Loss: 0.00000938
Iteration 170/1000 | Loss: 0.00000938
Iteration 171/1000 | Loss: 0.00000938
Iteration 172/1000 | Loss: 0.00000938
Iteration 173/1000 | Loss: 0.00000938
Iteration 174/1000 | Loss: 0.00000938
Iteration 175/1000 | Loss: 0.00000937
Iteration 176/1000 | Loss: 0.00000937
Iteration 177/1000 | Loss: 0.00000937
Iteration 178/1000 | Loss: 0.00000937
Iteration 179/1000 | Loss: 0.00000937
Iteration 180/1000 | Loss: 0.00000937
Iteration 181/1000 | Loss: 0.00000937
Iteration 182/1000 | Loss: 0.00000937
Iteration 183/1000 | Loss: 0.00000937
Iteration 184/1000 | Loss: 0.00000937
Iteration 185/1000 | Loss: 0.00000937
Iteration 186/1000 | Loss: 0.00000937
Iteration 187/1000 | Loss: 0.00000937
Iteration 188/1000 | Loss: 0.00000936
Iteration 189/1000 | Loss: 0.00000936
Iteration 190/1000 | Loss: 0.00000936
Iteration 191/1000 | Loss: 0.00000936
Iteration 192/1000 | Loss: 0.00000936
Iteration 193/1000 | Loss: 0.00000936
Iteration 194/1000 | Loss: 0.00000936
Iteration 195/1000 | Loss: 0.00000936
Iteration 196/1000 | Loss: 0.00000936
Iteration 197/1000 | Loss: 0.00000936
Iteration 198/1000 | Loss: 0.00000936
Iteration 199/1000 | Loss: 0.00000936
Iteration 200/1000 | Loss: 0.00000936
Iteration 201/1000 | Loss: 0.00000936
Iteration 202/1000 | Loss: 0.00000936
Iteration 203/1000 | Loss: 0.00000936
Iteration 204/1000 | Loss: 0.00000935
Iteration 205/1000 | Loss: 0.00000935
Iteration 206/1000 | Loss: 0.00000935
Iteration 207/1000 | Loss: 0.00000935
Iteration 208/1000 | Loss: 0.00000935
Iteration 209/1000 | Loss: 0.00000935
Iteration 210/1000 | Loss: 0.00000935
Iteration 211/1000 | Loss: 0.00000935
Iteration 212/1000 | Loss: 0.00000935
Iteration 213/1000 | Loss: 0.00000935
Iteration 214/1000 | Loss: 0.00000935
Iteration 215/1000 | Loss: 0.00000935
Iteration 216/1000 | Loss: 0.00000935
Iteration 217/1000 | Loss: 0.00000935
Iteration 218/1000 | Loss: 0.00000934
Iteration 219/1000 | Loss: 0.00000934
Iteration 220/1000 | Loss: 0.00000934
Iteration 221/1000 | Loss: 0.00000934
Iteration 222/1000 | Loss: 0.00000934
Iteration 223/1000 | Loss: 0.00000934
Iteration 224/1000 | Loss: 0.00000934
Iteration 225/1000 | Loss: 0.00000934
Iteration 226/1000 | Loss: 0.00000934
Iteration 227/1000 | Loss: 0.00000934
Iteration 228/1000 | Loss: 0.00000934
Iteration 229/1000 | Loss: 0.00000934
Iteration 230/1000 | Loss: 0.00000934
Iteration 231/1000 | Loss: 0.00000934
Iteration 232/1000 | Loss: 0.00000934
Iteration 233/1000 | Loss: 0.00000934
Iteration 234/1000 | Loss: 0.00000934
Iteration 235/1000 | Loss: 0.00000934
Iteration 236/1000 | Loss: 0.00000934
Iteration 237/1000 | Loss: 0.00000934
Iteration 238/1000 | Loss: 0.00000934
Iteration 239/1000 | Loss: 0.00000934
Iteration 240/1000 | Loss: 0.00000934
Iteration 241/1000 | Loss: 0.00000933
Iteration 242/1000 | Loss: 0.00000933
Iteration 243/1000 | Loss: 0.00000933
Iteration 244/1000 | Loss: 0.00000933
Iteration 245/1000 | Loss: 0.00000933
Iteration 246/1000 | Loss: 0.00000933
Iteration 247/1000 | Loss: 0.00000933
Iteration 248/1000 | Loss: 0.00000933
Iteration 249/1000 | Loss: 0.00000933
Iteration 250/1000 | Loss: 0.00000933
Iteration 251/1000 | Loss: 0.00000933
Iteration 252/1000 | Loss: 0.00000933
Iteration 253/1000 | Loss: 0.00000933
Iteration 254/1000 | Loss: 0.00000933
Iteration 255/1000 | Loss: 0.00000933
Iteration 256/1000 | Loss: 0.00000933
Iteration 257/1000 | Loss: 0.00000932
Iteration 258/1000 | Loss: 0.00000932
Iteration 259/1000 | Loss: 0.00000932
Iteration 260/1000 | Loss: 0.00000932
Iteration 261/1000 | Loss: 0.00000932
Iteration 262/1000 | Loss: 0.00000932
Iteration 263/1000 | Loss: 0.00000932
Iteration 264/1000 | Loss: 0.00000932
Iteration 265/1000 | Loss: 0.00000932
Iteration 266/1000 | Loss: 0.00000932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [9.323299309471622e-06, 9.323299309471622e-06, 9.323299309471622e-06, 9.323299309471622e-06, 9.323299309471622e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.323299309471622e-06

Optimization complete. Final v2v error: 2.6149237155914307 mm

Highest mean error: 2.746845006942749 mm for frame 144

Lowest mean error: 2.4380927085876465 mm for frame 260

Saving results

Total time: 50.292861223220825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871921
Iteration 2/25 | Loss: 0.00139663
Iteration 3/25 | Loss: 0.00122138
Iteration 4/25 | Loss: 0.00117126
Iteration 5/25 | Loss: 0.00115852
Iteration 6/25 | Loss: 0.00115600
Iteration 7/25 | Loss: 0.00115493
Iteration 8/25 | Loss: 0.00115459
Iteration 9/25 | Loss: 0.00115444
Iteration 10/25 | Loss: 0.00115443
Iteration 11/25 | Loss: 0.00115443
Iteration 12/25 | Loss: 0.00115443
Iteration 13/25 | Loss: 0.00115442
Iteration 14/25 | Loss: 0.00115442
Iteration 15/25 | Loss: 0.00115442
Iteration 16/25 | Loss: 0.00115442
Iteration 17/25 | Loss: 0.00115442
Iteration 18/25 | Loss: 0.00115442
Iteration 19/25 | Loss: 0.00115442
Iteration 20/25 | Loss: 0.00115442
Iteration 21/25 | Loss: 0.00115441
Iteration 22/25 | Loss: 0.00115441
Iteration 23/25 | Loss: 0.00115441
Iteration 24/25 | Loss: 0.00115441
Iteration 25/25 | Loss: 0.00115441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.66962337
Iteration 2/25 | Loss: 0.00097727
Iteration 3/25 | Loss: 0.00097724
Iteration 4/25 | Loss: 0.00097724
Iteration 5/25 | Loss: 0.00097723
Iteration 6/25 | Loss: 0.00097723
Iteration 7/25 | Loss: 0.00097723
Iteration 8/25 | Loss: 0.00097723
Iteration 9/25 | Loss: 0.00097723
Iteration 10/25 | Loss: 0.00097723
Iteration 11/25 | Loss: 0.00097723
Iteration 12/25 | Loss: 0.00097723
Iteration 13/25 | Loss: 0.00097723
Iteration 14/25 | Loss: 0.00097723
Iteration 15/25 | Loss: 0.00097723
Iteration 16/25 | Loss: 0.00097723
Iteration 17/25 | Loss: 0.00097723
Iteration 18/25 | Loss: 0.00097723
Iteration 19/25 | Loss: 0.00097723
Iteration 20/25 | Loss: 0.00097723
Iteration 21/25 | Loss: 0.00097723
Iteration 22/25 | Loss: 0.00097723
Iteration 23/25 | Loss: 0.00097723
Iteration 24/25 | Loss: 0.00097723
Iteration 25/25 | Loss: 0.00097723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097723
Iteration 2/1000 | Loss: 0.00002301
Iteration 3/1000 | Loss: 0.00001598
Iteration 4/1000 | Loss: 0.00001382
Iteration 5/1000 | Loss: 0.00001306
Iteration 6/1000 | Loss: 0.00001240
Iteration 7/1000 | Loss: 0.00003690
Iteration 8/1000 | Loss: 0.00001197
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00003266
Iteration 11/1000 | Loss: 0.00001178
Iteration 12/1000 | Loss: 0.00004559
Iteration 13/1000 | Loss: 0.00001159
Iteration 14/1000 | Loss: 0.00001145
Iteration 15/1000 | Loss: 0.00001144
Iteration 16/1000 | Loss: 0.00001144
Iteration 17/1000 | Loss: 0.00001144
Iteration 18/1000 | Loss: 0.00001139
Iteration 19/1000 | Loss: 0.00001137
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001137
Iteration 22/1000 | Loss: 0.00001136
Iteration 23/1000 | Loss: 0.00001135
Iteration 24/1000 | Loss: 0.00001135
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001130
Iteration 29/1000 | Loss: 0.00001123
Iteration 30/1000 | Loss: 0.00001123
Iteration 31/1000 | Loss: 0.00001123
Iteration 32/1000 | Loss: 0.00001122
Iteration 33/1000 | Loss: 0.00001121
Iteration 34/1000 | Loss: 0.00001121
Iteration 35/1000 | Loss: 0.00001120
Iteration 36/1000 | Loss: 0.00001120
Iteration 37/1000 | Loss: 0.00001120
Iteration 38/1000 | Loss: 0.00001120
Iteration 39/1000 | Loss: 0.00001119
Iteration 40/1000 | Loss: 0.00001116
Iteration 41/1000 | Loss: 0.00001116
Iteration 42/1000 | Loss: 0.00001115
Iteration 43/1000 | Loss: 0.00001115
Iteration 44/1000 | Loss: 0.00001115
Iteration 45/1000 | Loss: 0.00001114
Iteration 46/1000 | Loss: 0.00001114
Iteration 47/1000 | Loss: 0.00001114
Iteration 48/1000 | Loss: 0.00001112
Iteration 49/1000 | Loss: 0.00001112
Iteration 50/1000 | Loss: 0.00001111
Iteration 51/1000 | Loss: 0.00001111
Iteration 52/1000 | Loss: 0.00001110
Iteration 53/1000 | Loss: 0.00001110
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001107
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001106
Iteration 60/1000 | Loss: 0.00001105
Iteration 61/1000 | Loss: 0.00001105
Iteration 62/1000 | Loss: 0.00001104
Iteration 63/1000 | Loss: 0.00004754
Iteration 64/1000 | Loss: 0.00001100
Iteration 65/1000 | Loss: 0.00001100
Iteration 66/1000 | Loss: 0.00001100
Iteration 67/1000 | Loss: 0.00001100
Iteration 68/1000 | Loss: 0.00001099
Iteration 69/1000 | Loss: 0.00001099
Iteration 70/1000 | Loss: 0.00001099
Iteration 71/1000 | Loss: 0.00001099
Iteration 72/1000 | Loss: 0.00001099
Iteration 73/1000 | Loss: 0.00001099
Iteration 74/1000 | Loss: 0.00001099
Iteration 75/1000 | Loss: 0.00001099
Iteration 76/1000 | Loss: 0.00001099
Iteration 77/1000 | Loss: 0.00001099
Iteration 78/1000 | Loss: 0.00001098
Iteration 79/1000 | Loss: 0.00001098
Iteration 80/1000 | Loss: 0.00001098
Iteration 81/1000 | Loss: 0.00001098
Iteration 82/1000 | Loss: 0.00001098
Iteration 83/1000 | Loss: 0.00001098
Iteration 84/1000 | Loss: 0.00001098
Iteration 85/1000 | Loss: 0.00001098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.098344182537403e-05, 1.098344182537403e-05, 1.098344182537403e-05, 1.098344182537403e-05, 1.098344182537403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.098344182537403e-05

Optimization complete. Final v2v error: 2.7899014949798584 mm

Highest mean error: 3.40964937210083 mm for frame 84

Lowest mean error: 2.496380090713501 mm for frame 176

Saving results

Total time: 49.69278645515442
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810575
Iteration 2/25 | Loss: 0.00176076
Iteration 3/25 | Loss: 0.00127492
Iteration 4/25 | Loss: 0.00122535
Iteration 5/25 | Loss: 0.00121788
Iteration 6/25 | Loss: 0.00121768
Iteration 7/25 | Loss: 0.00121768
Iteration 8/25 | Loss: 0.00121768
Iteration 9/25 | Loss: 0.00121768
Iteration 10/25 | Loss: 0.00121768
Iteration 11/25 | Loss: 0.00121768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012176834279671311, 0.0012176834279671311, 0.0012176834279671311, 0.0012176834279671311, 0.0012176834279671311]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012176834279671311

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35382068
Iteration 2/25 | Loss: 0.00066832
Iteration 3/25 | Loss: 0.00066832
Iteration 4/25 | Loss: 0.00066832
Iteration 5/25 | Loss: 0.00066832
Iteration 6/25 | Loss: 0.00066832
Iteration 7/25 | Loss: 0.00066832
Iteration 8/25 | Loss: 0.00066832
Iteration 9/25 | Loss: 0.00066832
Iteration 10/25 | Loss: 0.00066832
Iteration 11/25 | Loss: 0.00066832
Iteration 12/25 | Loss: 0.00066832
Iteration 13/25 | Loss: 0.00066832
Iteration 14/25 | Loss: 0.00066832
Iteration 15/25 | Loss: 0.00066832
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006683154497295618, 0.0006683154497295618, 0.0006683154497295618, 0.0006683154497295618, 0.0006683154497295618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006683154497295618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066832
Iteration 2/1000 | Loss: 0.00003089
Iteration 3/1000 | Loss: 0.00002313
Iteration 4/1000 | Loss: 0.00002197
Iteration 5/1000 | Loss: 0.00002119
Iteration 6/1000 | Loss: 0.00002052
Iteration 7/1000 | Loss: 0.00002017
Iteration 8/1000 | Loss: 0.00001989
Iteration 9/1000 | Loss: 0.00001957
Iteration 10/1000 | Loss: 0.00001953
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00001945
Iteration 13/1000 | Loss: 0.00001944
Iteration 14/1000 | Loss: 0.00001932
Iteration 15/1000 | Loss: 0.00001914
Iteration 16/1000 | Loss: 0.00001907
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001897
Iteration 20/1000 | Loss: 0.00001897
Iteration 21/1000 | Loss: 0.00001897
Iteration 22/1000 | Loss: 0.00001896
Iteration 23/1000 | Loss: 0.00001896
Iteration 24/1000 | Loss: 0.00001895
Iteration 25/1000 | Loss: 0.00001894
Iteration 26/1000 | Loss: 0.00001893
Iteration 27/1000 | Loss: 0.00001890
Iteration 28/1000 | Loss: 0.00001890
Iteration 29/1000 | Loss: 0.00001889
Iteration 30/1000 | Loss: 0.00001881
Iteration 31/1000 | Loss: 0.00001879
Iteration 32/1000 | Loss: 0.00001877
Iteration 33/1000 | Loss: 0.00001877
Iteration 34/1000 | Loss: 0.00001876
Iteration 35/1000 | Loss: 0.00001876
Iteration 36/1000 | Loss: 0.00001876
Iteration 37/1000 | Loss: 0.00001876
Iteration 38/1000 | Loss: 0.00001876
Iteration 39/1000 | Loss: 0.00001876
Iteration 40/1000 | Loss: 0.00001875
Iteration 41/1000 | Loss: 0.00001875
Iteration 42/1000 | Loss: 0.00001875
Iteration 43/1000 | Loss: 0.00001875
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001875
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001874
Iteration 48/1000 | Loss: 0.00001874
Iteration 49/1000 | Loss: 0.00001873
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001872
Iteration 52/1000 | Loss: 0.00001871
Iteration 53/1000 | Loss: 0.00001871
Iteration 54/1000 | Loss: 0.00001871
Iteration 55/1000 | Loss: 0.00001870
Iteration 56/1000 | Loss: 0.00001870
Iteration 57/1000 | Loss: 0.00001870
Iteration 58/1000 | Loss: 0.00001870
Iteration 59/1000 | Loss: 0.00001870
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001869
Iteration 63/1000 | Loss: 0.00001869
Iteration 64/1000 | Loss: 0.00001869
Iteration 65/1000 | Loss: 0.00001868
Iteration 66/1000 | Loss: 0.00001868
Iteration 67/1000 | Loss: 0.00001868
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001868
Iteration 72/1000 | Loss: 0.00001868
Iteration 73/1000 | Loss: 0.00001868
Iteration 74/1000 | Loss: 0.00001868
Iteration 75/1000 | Loss: 0.00001867
Iteration 76/1000 | Loss: 0.00001867
Iteration 77/1000 | Loss: 0.00001867
Iteration 78/1000 | Loss: 0.00001867
Iteration 79/1000 | Loss: 0.00001867
Iteration 80/1000 | Loss: 0.00001867
Iteration 81/1000 | Loss: 0.00001867
Iteration 82/1000 | Loss: 0.00001867
Iteration 83/1000 | Loss: 0.00001867
Iteration 84/1000 | Loss: 0.00001867
Iteration 85/1000 | Loss: 0.00001867
Iteration 86/1000 | Loss: 0.00001867
Iteration 87/1000 | Loss: 0.00001866
Iteration 88/1000 | Loss: 0.00001866
Iteration 89/1000 | Loss: 0.00001866
Iteration 90/1000 | Loss: 0.00001866
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001865
Iteration 93/1000 | Loss: 0.00001865
Iteration 94/1000 | Loss: 0.00001865
Iteration 95/1000 | Loss: 0.00001865
Iteration 96/1000 | Loss: 0.00001865
Iteration 97/1000 | Loss: 0.00001865
Iteration 98/1000 | Loss: 0.00001864
Iteration 99/1000 | Loss: 0.00001864
Iteration 100/1000 | Loss: 0.00001864
Iteration 101/1000 | Loss: 0.00001864
Iteration 102/1000 | Loss: 0.00001864
Iteration 103/1000 | Loss: 0.00001864
Iteration 104/1000 | Loss: 0.00001863
Iteration 105/1000 | Loss: 0.00001863
Iteration 106/1000 | Loss: 0.00001863
Iteration 107/1000 | Loss: 0.00001863
Iteration 108/1000 | Loss: 0.00001863
Iteration 109/1000 | Loss: 0.00001863
Iteration 110/1000 | Loss: 0.00001863
Iteration 111/1000 | Loss: 0.00001863
Iteration 112/1000 | Loss: 0.00001863
Iteration 113/1000 | Loss: 0.00001862
Iteration 114/1000 | Loss: 0.00001862
Iteration 115/1000 | Loss: 0.00001862
Iteration 116/1000 | Loss: 0.00001862
Iteration 117/1000 | Loss: 0.00001862
Iteration 118/1000 | Loss: 0.00001862
Iteration 119/1000 | Loss: 0.00001862
Iteration 120/1000 | Loss: 0.00001862
Iteration 121/1000 | Loss: 0.00001862
Iteration 122/1000 | Loss: 0.00001862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.862499448179733e-05, 1.862499448179733e-05, 1.862499448179733e-05, 1.862499448179733e-05, 1.862499448179733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.862499448179733e-05

Optimization complete. Final v2v error: 3.592573642730713 mm

Highest mean error: 3.6682536602020264 mm for frame 76

Lowest mean error: 3.2120463848114014 mm for frame 5

Saving results

Total time: 39.375622272491455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009506
Iteration 2/25 | Loss: 0.00169914
Iteration 3/25 | Loss: 0.00149437
Iteration 4/25 | Loss: 0.00135353
Iteration 5/25 | Loss: 0.00131428
Iteration 6/25 | Loss: 0.00133439
Iteration 7/25 | Loss: 0.00131052
Iteration 8/25 | Loss: 0.00128576
Iteration 9/25 | Loss: 0.00127110
Iteration 10/25 | Loss: 0.00128181
Iteration 11/25 | Loss: 0.00127026
Iteration 12/25 | Loss: 0.00126785
Iteration 13/25 | Loss: 0.00127294
Iteration 14/25 | Loss: 0.00127744
Iteration 15/25 | Loss: 0.00127025
Iteration 16/25 | Loss: 0.00125415
Iteration 17/25 | Loss: 0.00125291
Iteration 18/25 | Loss: 0.00125063
Iteration 19/25 | Loss: 0.00125288
Iteration 20/25 | Loss: 0.00124800
Iteration 21/25 | Loss: 0.00123641
Iteration 22/25 | Loss: 0.00123737
Iteration 23/25 | Loss: 0.00123651
Iteration 24/25 | Loss: 0.00123121
Iteration 25/25 | Loss: 0.00122698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36895394
Iteration 2/25 | Loss: 0.00127001
Iteration 3/25 | Loss: 0.00127001
Iteration 4/25 | Loss: 0.00127001
Iteration 5/25 | Loss: 0.00127001
Iteration 6/25 | Loss: 0.00127001
Iteration 7/25 | Loss: 0.00127001
Iteration 8/25 | Loss: 0.00127001
Iteration 9/25 | Loss: 0.00127001
Iteration 10/25 | Loss: 0.00127001
Iteration 11/25 | Loss: 0.00127001
Iteration 12/25 | Loss: 0.00127001
Iteration 13/25 | Loss: 0.00127001
Iteration 14/25 | Loss: 0.00127001
Iteration 15/25 | Loss: 0.00127001
Iteration 16/25 | Loss: 0.00127001
Iteration 17/25 | Loss: 0.00127001
Iteration 18/25 | Loss: 0.00127001
Iteration 19/25 | Loss: 0.00127001
Iteration 20/25 | Loss: 0.00127001
Iteration 21/25 | Loss: 0.00127001
Iteration 22/25 | Loss: 0.00127001
Iteration 23/25 | Loss: 0.00127001
Iteration 24/25 | Loss: 0.00127001
Iteration 25/25 | Loss: 0.00127001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127001
Iteration 2/1000 | Loss: 0.00041369
Iteration 3/1000 | Loss: 0.00067355
Iteration 4/1000 | Loss: 0.00057513
Iteration 5/1000 | Loss: 0.00039275
Iteration 6/1000 | Loss: 0.00073084
Iteration 7/1000 | Loss: 0.00069895
Iteration 8/1000 | Loss: 0.00054038
Iteration 9/1000 | Loss: 0.00049334
Iteration 10/1000 | Loss: 0.00048670
Iteration 11/1000 | Loss: 0.00062229
Iteration 12/1000 | Loss: 0.00121839
Iteration 13/1000 | Loss: 0.00086696
Iteration 14/1000 | Loss: 0.00091865
Iteration 15/1000 | Loss: 0.00078371
Iteration 16/1000 | Loss: 0.00031197
Iteration 17/1000 | Loss: 0.00040157
Iteration 18/1000 | Loss: 0.00042624
Iteration 19/1000 | Loss: 0.00052518
Iteration 20/1000 | Loss: 0.00051623
Iteration 21/1000 | Loss: 0.00056618
Iteration 22/1000 | Loss: 0.00062652
Iteration 23/1000 | Loss: 0.00054038
Iteration 24/1000 | Loss: 0.00062572
Iteration 25/1000 | Loss: 0.00064688
Iteration 26/1000 | Loss: 0.00064643
Iteration 27/1000 | Loss: 0.00067752
Iteration 28/1000 | Loss: 0.00064803
Iteration 29/1000 | Loss: 0.00066691
Iteration 30/1000 | Loss: 0.00057801
Iteration 31/1000 | Loss: 0.00054648
Iteration 32/1000 | Loss: 0.00055013
Iteration 33/1000 | Loss: 0.00049766
Iteration 34/1000 | Loss: 0.00062879
Iteration 35/1000 | Loss: 0.00059804
Iteration 36/1000 | Loss: 0.00064559
Iteration 37/1000 | Loss: 0.00038544
Iteration 38/1000 | Loss: 0.00026307
Iteration 39/1000 | Loss: 0.00025195
Iteration 40/1000 | Loss: 0.00058869
Iteration 41/1000 | Loss: 0.00043006
Iteration 42/1000 | Loss: 0.00037101
Iteration 43/1000 | Loss: 0.00038208
Iteration 44/1000 | Loss: 0.00077617
Iteration 45/1000 | Loss: 0.00034745
Iteration 46/1000 | Loss: 0.00067479
Iteration 47/1000 | Loss: 0.00052339
Iteration 48/1000 | Loss: 0.00091090
Iteration 49/1000 | Loss: 0.00077339
Iteration 50/1000 | Loss: 0.00070290
Iteration 51/1000 | Loss: 0.00071015
Iteration 52/1000 | Loss: 0.00076292
Iteration 53/1000 | Loss: 0.00035964
Iteration 54/1000 | Loss: 0.00045814
Iteration 55/1000 | Loss: 0.00045745
Iteration 56/1000 | Loss: 0.00060487
Iteration 57/1000 | Loss: 0.00079252
Iteration 58/1000 | Loss: 0.00110060
Iteration 59/1000 | Loss: 0.00129530
Iteration 60/1000 | Loss: 0.00057912
Iteration 61/1000 | Loss: 0.00015222
Iteration 62/1000 | Loss: 0.00029294
Iteration 63/1000 | Loss: 0.00014110
Iteration 64/1000 | Loss: 0.00021271
Iteration 65/1000 | Loss: 0.00027426
Iteration 66/1000 | Loss: 0.00024449
Iteration 67/1000 | Loss: 0.00042631
Iteration 68/1000 | Loss: 0.00026527
Iteration 69/1000 | Loss: 0.00022529
Iteration 70/1000 | Loss: 0.00033297
Iteration 71/1000 | Loss: 0.00035568
Iteration 72/1000 | Loss: 0.00030738
Iteration 73/1000 | Loss: 0.00033927
Iteration 74/1000 | Loss: 0.00033190
Iteration 75/1000 | Loss: 0.00035871
Iteration 76/1000 | Loss: 0.00024876
Iteration 77/1000 | Loss: 0.00036964
Iteration 78/1000 | Loss: 0.00024935
Iteration 79/1000 | Loss: 0.00026392
Iteration 80/1000 | Loss: 0.00022166
Iteration 81/1000 | Loss: 0.00027245
Iteration 82/1000 | Loss: 0.00026658
Iteration 83/1000 | Loss: 0.00028224
Iteration 84/1000 | Loss: 0.00030782
Iteration 85/1000 | Loss: 0.00038465
Iteration 86/1000 | Loss: 0.00032662
Iteration 87/1000 | Loss: 0.00036803
Iteration 88/1000 | Loss: 0.00029804
Iteration 89/1000 | Loss: 0.00047427
Iteration 90/1000 | Loss: 0.00025435
Iteration 91/1000 | Loss: 0.00026024
Iteration 92/1000 | Loss: 0.00031411
Iteration 93/1000 | Loss: 0.00026560
Iteration 94/1000 | Loss: 0.00014679
Iteration 95/1000 | Loss: 0.00009939
Iteration 96/1000 | Loss: 0.00016157
Iteration 97/1000 | Loss: 0.00023085
Iteration 98/1000 | Loss: 0.00024266
Iteration 99/1000 | Loss: 0.00022547
Iteration 100/1000 | Loss: 0.00012675
Iteration 101/1000 | Loss: 0.00025517
Iteration 102/1000 | Loss: 0.00023873
Iteration 103/1000 | Loss: 0.00039568
Iteration 104/1000 | Loss: 0.00036004
Iteration 105/1000 | Loss: 0.00030474
Iteration 106/1000 | Loss: 0.00018467
Iteration 107/1000 | Loss: 0.00043176
Iteration 108/1000 | Loss: 0.00029177
Iteration 109/1000 | Loss: 0.00018480
Iteration 110/1000 | Loss: 0.00037606
Iteration 111/1000 | Loss: 0.00032900
Iteration 112/1000 | Loss: 0.00037132
Iteration 113/1000 | Loss: 0.00032333
Iteration 114/1000 | Loss: 0.00028323
Iteration 115/1000 | Loss: 0.00023710
Iteration 116/1000 | Loss: 0.00033082
Iteration 117/1000 | Loss: 0.00034733
Iteration 118/1000 | Loss: 0.00027578
Iteration 119/1000 | Loss: 0.00028528
Iteration 120/1000 | Loss: 0.00036828
Iteration 121/1000 | Loss: 0.00038158
Iteration 122/1000 | Loss: 0.00018313
Iteration 123/1000 | Loss: 0.00004784
Iteration 124/1000 | Loss: 0.00008158
Iteration 125/1000 | Loss: 0.00006006
Iteration 126/1000 | Loss: 0.00017147
Iteration 127/1000 | Loss: 0.00005790
Iteration 128/1000 | Loss: 0.00017342
Iteration 129/1000 | Loss: 0.00018361
Iteration 130/1000 | Loss: 0.00013984
Iteration 131/1000 | Loss: 0.00013246
Iteration 132/1000 | Loss: 0.00009402
Iteration 133/1000 | Loss: 0.00013376
Iteration 134/1000 | Loss: 0.00028565
Iteration 135/1000 | Loss: 0.00022596
Iteration 136/1000 | Loss: 0.00022393
Iteration 137/1000 | Loss: 0.00016537
Iteration 138/1000 | Loss: 0.00012924
Iteration 139/1000 | Loss: 0.00015625
Iteration 140/1000 | Loss: 0.00015180
Iteration 141/1000 | Loss: 0.00021864
Iteration 142/1000 | Loss: 0.00017175
Iteration 143/1000 | Loss: 0.00011495
Iteration 144/1000 | Loss: 0.00008297
Iteration 145/1000 | Loss: 0.00016742
Iteration 146/1000 | Loss: 0.00013252
Iteration 147/1000 | Loss: 0.00014633
Iteration 148/1000 | Loss: 0.00021198
Iteration 149/1000 | Loss: 0.00020374
Iteration 150/1000 | Loss: 0.00022683
Iteration 151/1000 | Loss: 0.00015651
Iteration 152/1000 | Loss: 0.00021545
Iteration 153/1000 | Loss: 0.00021730
Iteration 154/1000 | Loss: 0.00021430
Iteration 155/1000 | Loss: 0.00020005
Iteration 156/1000 | Loss: 0.00022598
Iteration 157/1000 | Loss: 0.00023888
Iteration 158/1000 | Loss: 0.00018333
Iteration 159/1000 | Loss: 0.00015834
Iteration 160/1000 | Loss: 0.00019853
Iteration 161/1000 | Loss: 0.00015010
Iteration 162/1000 | Loss: 0.00013888
Iteration 163/1000 | Loss: 0.00021306
Iteration 164/1000 | Loss: 0.00015293
Iteration 165/1000 | Loss: 0.00004119
Iteration 166/1000 | Loss: 0.00003018
Iteration 167/1000 | Loss: 0.00002352
Iteration 168/1000 | Loss: 0.00002100
Iteration 169/1000 | Loss: 0.00001938
Iteration 170/1000 | Loss: 0.00001813
Iteration 171/1000 | Loss: 0.00001732
Iteration 172/1000 | Loss: 0.00001668
Iteration 173/1000 | Loss: 0.00001629
Iteration 174/1000 | Loss: 0.00001593
Iteration 175/1000 | Loss: 0.00001570
Iteration 176/1000 | Loss: 0.00001546
Iteration 177/1000 | Loss: 0.00001545
Iteration 178/1000 | Loss: 0.00001542
Iteration 179/1000 | Loss: 0.00001518
Iteration 180/1000 | Loss: 0.00001510
Iteration 181/1000 | Loss: 0.00001507
Iteration 182/1000 | Loss: 0.00001496
Iteration 183/1000 | Loss: 0.00001494
Iteration 184/1000 | Loss: 0.00001494
Iteration 185/1000 | Loss: 0.00001493
Iteration 186/1000 | Loss: 0.00001492
Iteration 187/1000 | Loss: 0.00001491
Iteration 188/1000 | Loss: 0.00001490
Iteration 189/1000 | Loss: 0.00001489
Iteration 190/1000 | Loss: 0.00001488
Iteration 191/1000 | Loss: 0.00001488
Iteration 192/1000 | Loss: 0.00001488
Iteration 193/1000 | Loss: 0.00001488
Iteration 194/1000 | Loss: 0.00001488
Iteration 195/1000 | Loss: 0.00001487
Iteration 196/1000 | Loss: 0.00001487
Iteration 197/1000 | Loss: 0.00001487
Iteration 198/1000 | Loss: 0.00001487
Iteration 199/1000 | Loss: 0.00001486
Iteration 200/1000 | Loss: 0.00001486
Iteration 201/1000 | Loss: 0.00001486
Iteration 202/1000 | Loss: 0.00001486
Iteration 203/1000 | Loss: 0.00001486
Iteration 204/1000 | Loss: 0.00001485
Iteration 205/1000 | Loss: 0.00001485
Iteration 206/1000 | Loss: 0.00001485
Iteration 207/1000 | Loss: 0.00001485
Iteration 208/1000 | Loss: 0.00001485
Iteration 209/1000 | Loss: 0.00001485
Iteration 210/1000 | Loss: 0.00001484
Iteration 211/1000 | Loss: 0.00001484
Iteration 212/1000 | Loss: 0.00001484
Iteration 213/1000 | Loss: 0.00001483
Iteration 214/1000 | Loss: 0.00001483
Iteration 215/1000 | Loss: 0.00001482
Iteration 216/1000 | Loss: 0.00001482
Iteration 217/1000 | Loss: 0.00001482
Iteration 218/1000 | Loss: 0.00001482
Iteration 219/1000 | Loss: 0.00001482
Iteration 220/1000 | Loss: 0.00001482
Iteration 221/1000 | Loss: 0.00001482
Iteration 222/1000 | Loss: 0.00001482
Iteration 223/1000 | Loss: 0.00001482
Iteration 224/1000 | Loss: 0.00001482
Iteration 225/1000 | Loss: 0.00001482
Iteration 226/1000 | Loss: 0.00001482
Iteration 227/1000 | Loss: 0.00001481
Iteration 228/1000 | Loss: 0.00001481
Iteration 229/1000 | Loss: 0.00001481
Iteration 230/1000 | Loss: 0.00001480
Iteration 231/1000 | Loss: 0.00001480
Iteration 232/1000 | Loss: 0.00001480
Iteration 233/1000 | Loss: 0.00001480
Iteration 234/1000 | Loss: 0.00001479
Iteration 235/1000 | Loss: 0.00001479
Iteration 236/1000 | Loss: 0.00001479
Iteration 237/1000 | Loss: 0.00001479
Iteration 238/1000 | Loss: 0.00001479
Iteration 239/1000 | Loss: 0.00001478
Iteration 240/1000 | Loss: 0.00001478
Iteration 241/1000 | Loss: 0.00001478
Iteration 242/1000 | Loss: 0.00001478
Iteration 243/1000 | Loss: 0.00001478
Iteration 244/1000 | Loss: 0.00001477
Iteration 245/1000 | Loss: 0.00001477
Iteration 246/1000 | Loss: 0.00001477
Iteration 247/1000 | Loss: 0.00001476
Iteration 248/1000 | Loss: 0.00001476
Iteration 249/1000 | Loss: 0.00001476
Iteration 250/1000 | Loss: 0.00001476
Iteration 251/1000 | Loss: 0.00001476
Iteration 252/1000 | Loss: 0.00001476
Iteration 253/1000 | Loss: 0.00001476
Iteration 254/1000 | Loss: 0.00001475
Iteration 255/1000 | Loss: 0.00001475
Iteration 256/1000 | Loss: 0.00001475
Iteration 257/1000 | Loss: 0.00001475
Iteration 258/1000 | Loss: 0.00001474
Iteration 259/1000 | Loss: 0.00001474
Iteration 260/1000 | Loss: 0.00001474
Iteration 261/1000 | Loss: 0.00001474
Iteration 262/1000 | Loss: 0.00001473
Iteration 263/1000 | Loss: 0.00001473
Iteration 264/1000 | Loss: 0.00001473
Iteration 265/1000 | Loss: 0.00001472
Iteration 266/1000 | Loss: 0.00001472
Iteration 267/1000 | Loss: 0.00001471
Iteration 268/1000 | Loss: 0.00001471
Iteration 269/1000 | Loss: 0.00001471
Iteration 270/1000 | Loss: 0.00001470
Iteration 271/1000 | Loss: 0.00001470
Iteration 272/1000 | Loss: 0.00001470
Iteration 273/1000 | Loss: 0.00001470
Iteration 274/1000 | Loss: 0.00001470
Iteration 275/1000 | Loss: 0.00001470
Iteration 276/1000 | Loss: 0.00001470
Iteration 277/1000 | Loss: 0.00001470
Iteration 278/1000 | Loss: 0.00001469
Iteration 279/1000 | Loss: 0.00001469
Iteration 280/1000 | Loss: 0.00001469
Iteration 281/1000 | Loss: 0.00001469
Iteration 282/1000 | Loss: 0.00001469
Iteration 283/1000 | Loss: 0.00001469
Iteration 284/1000 | Loss: 0.00001469
Iteration 285/1000 | Loss: 0.00001469
Iteration 286/1000 | Loss: 0.00001469
Iteration 287/1000 | Loss: 0.00001469
Iteration 288/1000 | Loss: 0.00001469
Iteration 289/1000 | Loss: 0.00001468
Iteration 290/1000 | Loss: 0.00001468
Iteration 291/1000 | Loss: 0.00001468
Iteration 292/1000 | Loss: 0.00001468
Iteration 293/1000 | Loss: 0.00001468
Iteration 294/1000 | Loss: 0.00001468
Iteration 295/1000 | Loss: 0.00001468
Iteration 296/1000 | Loss: 0.00001468
Iteration 297/1000 | Loss: 0.00001467
Iteration 298/1000 | Loss: 0.00001467
Iteration 299/1000 | Loss: 0.00001467
Iteration 300/1000 | Loss: 0.00001467
Iteration 301/1000 | Loss: 0.00001467
Iteration 302/1000 | Loss: 0.00001467
Iteration 303/1000 | Loss: 0.00001466
Iteration 304/1000 | Loss: 0.00001466
Iteration 305/1000 | Loss: 0.00001466
Iteration 306/1000 | Loss: 0.00001466
Iteration 307/1000 | Loss: 0.00001466
Iteration 308/1000 | Loss: 0.00001466
Iteration 309/1000 | Loss: 0.00001466
Iteration 310/1000 | Loss: 0.00001466
Iteration 311/1000 | Loss: 0.00001465
Iteration 312/1000 | Loss: 0.00001465
Iteration 313/1000 | Loss: 0.00001465
Iteration 314/1000 | Loss: 0.00001465
Iteration 315/1000 | Loss: 0.00001465
Iteration 316/1000 | Loss: 0.00001465
Iteration 317/1000 | Loss: 0.00001465
Iteration 318/1000 | Loss: 0.00001465
Iteration 319/1000 | Loss: 0.00001465
Iteration 320/1000 | Loss: 0.00001465
Iteration 321/1000 | Loss: 0.00001465
Iteration 322/1000 | Loss: 0.00001464
Iteration 323/1000 | Loss: 0.00001464
Iteration 324/1000 | Loss: 0.00001464
Iteration 325/1000 | Loss: 0.00001464
Iteration 326/1000 | Loss: 0.00001464
Iteration 327/1000 | Loss: 0.00001464
Iteration 328/1000 | Loss: 0.00001464
Iteration 329/1000 | Loss: 0.00001464
Iteration 330/1000 | Loss: 0.00001464
Iteration 331/1000 | Loss: 0.00001464
Iteration 332/1000 | Loss: 0.00001464
Iteration 333/1000 | Loss: 0.00001464
Iteration 334/1000 | Loss: 0.00001464
Iteration 335/1000 | Loss: 0.00001464
Iteration 336/1000 | Loss: 0.00001464
Iteration 337/1000 | Loss: 0.00001464
Iteration 338/1000 | Loss: 0.00001464
Iteration 339/1000 | Loss: 0.00001464
Iteration 340/1000 | Loss: 0.00001464
Iteration 341/1000 | Loss: 0.00001464
Iteration 342/1000 | Loss: 0.00001464
Iteration 343/1000 | Loss: 0.00001464
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 343. Stopping optimization.
Last 5 losses: [1.4641766028944403e-05, 1.4641766028944403e-05, 1.4641766028944403e-05, 1.4641766028944403e-05, 1.4641766028944403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4641766028944403e-05

Optimization complete. Final v2v error: 3.1568610668182373 mm

Highest mean error: 4.815678119659424 mm for frame 67

Lowest mean error: 2.708375930786133 mm for frame 2

Saving results

Total time: 314.19554376602173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436608
Iteration 2/25 | Loss: 0.00127829
Iteration 3/25 | Loss: 0.00118499
Iteration 4/25 | Loss: 0.00117209
Iteration 5/25 | Loss: 0.00116863
Iteration 6/25 | Loss: 0.00116785
Iteration 7/25 | Loss: 0.00116774
Iteration 8/25 | Loss: 0.00116774
Iteration 9/25 | Loss: 0.00116774
Iteration 10/25 | Loss: 0.00116774
Iteration 11/25 | Loss: 0.00116774
Iteration 12/25 | Loss: 0.00116774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001167741371318698, 0.001167741371318698, 0.001167741371318698, 0.001167741371318698, 0.001167741371318698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001167741371318698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48223519
Iteration 2/25 | Loss: 0.00078545
Iteration 3/25 | Loss: 0.00078544
Iteration 4/25 | Loss: 0.00078544
Iteration 5/25 | Loss: 0.00078544
Iteration 6/25 | Loss: 0.00078544
Iteration 7/25 | Loss: 0.00078544
Iteration 8/25 | Loss: 0.00078544
Iteration 9/25 | Loss: 0.00078544
Iteration 10/25 | Loss: 0.00078544
Iteration 11/25 | Loss: 0.00078544
Iteration 12/25 | Loss: 0.00078544
Iteration 13/25 | Loss: 0.00078544
Iteration 14/25 | Loss: 0.00078544
Iteration 15/25 | Loss: 0.00078544
Iteration 16/25 | Loss: 0.00078544
Iteration 17/25 | Loss: 0.00078544
Iteration 18/25 | Loss: 0.00078544
Iteration 19/25 | Loss: 0.00078544
Iteration 20/25 | Loss: 0.00078544
Iteration 21/25 | Loss: 0.00078544
Iteration 22/25 | Loss: 0.00078544
Iteration 23/25 | Loss: 0.00078544
Iteration 24/25 | Loss: 0.00078544
Iteration 25/25 | Loss: 0.00078544

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078544
Iteration 2/1000 | Loss: 0.00002782
Iteration 3/1000 | Loss: 0.00001800
Iteration 4/1000 | Loss: 0.00001611
Iteration 5/1000 | Loss: 0.00001526
Iteration 6/1000 | Loss: 0.00001475
Iteration 7/1000 | Loss: 0.00001434
Iteration 8/1000 | Loss: 0.00001407
Iteration 9/1000 | Loss: 0.00001387
Iteration 10/1000 | Loss: 0.00001370
Iteration 11/1000 | Loss: 0.00001354
Iteration 12/1000 | Loss: 0.00001342
Iteration 13/1000 | Loss: 0.00001340
Iteration 14/1000 | Loss: 0.00001335
Iteration 15/1000 | Loss: 0.00001334
Iteration 16/1000 | Loss: 0.00001334
Iteration 17/1000 | Loss: 0.00001332
Iteration 18/1000 | Loss: 0.00001330
Iteration 19/1000 | Loss: 0.00001329
Iteration 20/1000 | Loss: 0.00001327
Iteration 21/1000 | Loss: 0.00001325
Iteration 22/1000 | Loss: 0.00001324
Iteration 23/1000 | Loss: 0.00001324
Iteration 24/1000 | Loss: 0.00001324
Iteration 25/1000 | Loss: 0.00001324
Iteration 26/1000 | Loss: 0.00001323
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001319
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001313
Iteration 33/1000 | Loss: 0.00001312
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001311
Iteration 36/1000 | Loss: 0.00001309
Iteration 37/1000 | Loss: 0.00001308
Iteration 38/1000 | Loss: 0.00001308
Iteration 39/1000 | Loss: 0.00001308
Iteration 40/1000 | Loss: 0.00001308
Iteration 41/1000 | Loss: 0.00001308
Iteration 42/1000 | Loss: 0.00001308
Iteration 43/1000 | Loss: 0.00001307
Iteration 44/1000 | Loss: 0.00001306
Iteration 45/1000 | Loss: 0.00001306
Iteration 46/1000 | Loss: 0.00001304
Iteration 47/1000 | Loss: 0.00001304
Iteration 48/1000 | Loss: 0.00001304
Iteration 49/1000 | Loss: 0.00001303
Iteration 50/1000 | Loss: 0.00001303
Iteration 51/1000 | Loss: 0.00001303
Iteration 52/1000 | Loss: 0.00001303
Iteration 53/1000 | Loss: 0.00001303
Iteration 54/1000 | Loss: 0.00001303
Iteration 55/1000 | Loss: 0.00001303
Iteration 56/1000 | Loss: 0.00001303
Iteration 57/1000 | Loss: 0.00001302
Iteration 58/1000 | Loss: 0.00001302
Iteration 59/1000 | Loss: 0.00001302
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001299
Iteration 78/1000 | Loss: 0.00001299
Iteration 79/1000 | Loss: 0.00001299
Iteration 80/1000 | Loss: 0.00001299
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001298
Iteration 83/1000 | Loss: 0.00001298
Iteration 84/1000 | Loss: 0.00001298
Iteration 85/1000 | Loss: 0.00001298
Iteration 86/1000 | Loss: 0.00001297
Iteration 87/1000 | Loss: 0.00001297
Iteration 88/1000 | Loss: 0.00001297
Iteration 89/1000 | Loss: 0.00001297
Iteration 90/1000 | Loss: 0.00001297
Iteration 91/1000 | Loss: 0.00001296
Iteration 92/1000 | Loss: 0.00001296
Iteration 93/1000 | Loss: 0.00001295
Iteration 94/1000 | Loss: 0.00001295
Iteration 95/1000 | Loss: 0.00001295
Iteration 96/1000 | Loss: 0.00001295
Iteration 97/1000 | Loss: 0.00001295
Iteration 98/1000 | Loss: 0.00001295
Iteration 99/1000 | Loss: 0.00001295
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001295
Iteration 102/1000 | Loss: 0.00001295
Iteration 103/1000 | Loss: 0.00001295
Iteration 104/1000 | Loss: 0.00001295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.2947243703820277e-05, 1.2947243703820277e-05, 1.2947243703820277e-05, 1.2947243703820277e-05, 1.2947243703820277e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2947243703820277e-05

Optimization complete. Final v2v error: 3.048297166824341 mm

Highest mean error: 3.8824849128723145 mm for frame 52

Lowest mean error: 2.6459407806396484 mm for frame 105

Saving results

Total time: 34.478100061416626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00348231
Iteration 2/25 | Loss: 0.00149246
Iteration 3/25 | Loss: 0.00120952
Iteration 4/25 | Loss: 0.00116631
Iteration 5/25 | Loss: 0.00115891
Iteration 6/25 | Loss: 0.00115637
Iteration 7/25 | Loss: 0.00115562
Iteration 8/25 | Loss: 0.00115562
Iteration 9/25 | Loss: 0.00115562
Iteration 10/25 | Loss: 0.00115562
Iteration 11/25 | Loss: 0.00115562
Iteration 12/25 | Loss: 0.00115562
Iteration 13/25 | Loss: 0.00115562
Iteration 14/25 | Loss: 0.00115562
Iteration 15/25 | Loss: 0.00115562
Iteration 16/25 | Loss: 0.00115562
Iteration 17/25 | Loss: 0.00115562
Iteration 18/25 | Loss: 0.00115562
Iteration 19/25 | Loss: 0.00115562
Iteration 20/25 | Loss: 0.00115562
Iteration 21/25 | Loss: 0.00115562
Iteration 22/25 | Loss: 0.00115562
Iteration 23/25 | Loss: 0.00115562
Iteration 24/25 | Loss: 0.00115562
Iteration 25/25 | Loss: 0.00115562

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38803351
Iteration 2/25 | Loss: 0.00091989
Iteration 3/25 | Loss: 0.00091989
Iteration 4/25 | Loss: 0.00091989
Iteration 5/25 | Loss: 0.00091989
Iteration 6/25 | Loss: 0.00091989
Iteration 7/25 | Loss: 0.00091989
Iteration 8/25 | Loss: 0.00091989
Iteration 9/25 | Loss: 0.00091989
Iteration 10/25 | Loss: 0.00091989
Iteration 11/25 | Loss: 0.00091989
Iteration 12/25 | Loss: 0.00091989
Iteration 13/25 | Loss: 0.00091989
Iteration 14/25 | Loss: 0.00091989
Iteration 15/25 | Loss: 0.00091989
Iteration 16/25 | Loss: 0.00091989
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009198859916068614, 0.0009198859916068614, 0.0009198859916068614, 0.0009198859916068614, 0.0009198859916068614]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009198859916068614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091989
Iteration 2/1000 | Loss: 0.00004472
Iteration 3/1000 | Loss: 0.00002439
Iteration 4/1000 | Loss: 0.00001722
Iteration 5/1000 | Loss: 0.00001593
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001442
Iteration 8/1000 | Loss: 0.00001393
Iteration 9/1000 | Loss: 0.00001356
Iteration 10/1000 | Loss: 0.00001335
Iteration 11/1000 | Loss: 0.00001333
Iteration 12/1000 | Loss: 0.00001315
Iteration 13/1000 | Loss: 0.00001308
Iteration 14/1000 | Loss: 0.00001300
Iteration 15/1000 | Loss: 0.00001298
Iteration 16/1000 | Loss: 0.00001296
Iteration 17/1000 | Loss: 0.00001294
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001294
Iteration 20/1000 | Loss: 0.00001294
Iteration 21/1000 | Loss: 0.00001294
Iteration 22/1000 | Loss: 0.00001294
Iteration 23/1000 | Loss: 0.00001293
Iteration 24/1000 | Loss: 0.00001293
Iteration 25/1000 | Loss: 0.00001293
Iteration 26/1000 | Loss: 0.00001293
Iteration 27/1000 | Loss: 0.00001293
Iteration 28/1000 | Loss: 0.00001293
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001292
Iteration 31/1000 | Loss: 0.00001292
Iteration 32/1000 | Loss: 0.00001291
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001289
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001289
Iteration 39/1000 | Loss: 0.00001288
Iteration 40/1000 | Loss: 0.00001288
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001285
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001285
Iteration 51/1000 | Loss: 0.00001284
Iteration 52/1000 | Loss: 0.00001284
Iteration 53/1000 | Loss: 0.00001283
Iteration 54/1000 | Loss: 0.00001283
Iteration 55/1000 | Loss: 0.00001283
Iteration 56/1000 | Loss: 0.00001282
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001281
Iteration 59/1000 | Loss: 0.00001281
Iteration 60/1000 | Loss: 0.00001281
Iteration 61/1000 | Loss: 0.00001280
Iteration 62/1000 | Loss: 0.00001280
Iteration 63/1000 | Loss: 0.00001280
Iteration 64/1000 | Loss: 0.00001280
Iteration 65/1000 | Loss: 0.00001280
Iteration 66/1000 | Loss: 0.00001280
Iteration 67/1000 | Loss: 0.00001279
Iteration 68/1000 | Loss: 0.00001279
Iteration 69/1000 | Loss: 0.00001279
Iteration 70/1000 | Loss: 0.00001278
Iteration 71/1000 | Loss: 0.00001278
Iteration 72/1000 | Loss: 0.00001278
Iteration 73/1000 | Loss: 0.00001278
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001277
Iteration 76/1000 | Loss: 0.00001277
Iteration 77/1000 | Loss: 0.00001277
Iteration 78/1000 | Loss: 0.00001276
Iteration 79/1000 | Loss: 0.00001276
Iteration 80/1000 | Loss: 0.00001276
Iteration 81/1000 | Loss: 0.00001276
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001275
Iteration 87/1000 | Loss: 0.00001275
Iteration 88/1000 | Loss: 0.00001275
Iteration 89/1000 | Loss: 0.00001275
Iteration 90/1000 | Loss: 0.00001275
Iteration 91/1000 | Loss: 0.00001275
Iteration 92/1000 | Loss: 0.00001275
Iteration 93/1000 | Loss: 0.00001275
Iteration 94/1000 | Loss: 0.00001275
Iteration 95/1000 | Loss: 0.00001275
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001275
Iteration 98/1000 | Loss: 0.00001275
Iteration 99/1000 | Loss: 0.00001274
Iteration 100/1000 | Loss: 0.00001274
Iteration 101/1000 | Loss: 0.00001274
Iteration 102/1000 | Loss: 0.00001274
Iteration 103/1000 | Loss: 0.00001274
Iteration 104/1000 | Loss: 0.00001274
Iteration 105/1000 | Loss: 0.00001274
Iteration 106/1000 | Loss: 0.00001274
Iteration 107/1000 | Loss: 0.00001274
Iteration 108/1000 | Loss: 0.00001274
Iteration 109/1000 | Loss: 0.00001274
Iteration 110/1000 | Loss: 0.00001274
Iteration 111/1000 | Loss: 0.00001274
Iteration 112/1000 | Loss: 0.00001274
Iteration 113/1000 | Loss: 0.00001274
Iteration 114/1000 | Loss: 0.00001274
Iteration 115/1000 | Loss: 0.00001274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.2743468687403947e-05, 1.2743468687403947e-05, 1.2743468687403947e-05, 1.2743468687403947e-05, 1.2743468687403947e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2743468687403947e-05

Optimization complete. Final v2v error: 3.066157102584839 mm

Highest mean error: 3.4712743759155273 mm for frame 28

Lowest mean error: 2.6049985885620117 mm for frame 13

Saving results

Total time: 35.02430057525635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853220
Iteration 2/25 | Loss: 0.00130237
Iteration 3/25 | Loss: 0.00119894
Iteration 4/25 | Loss: 0.00118252
Iteration 5/25 | Loss: 0.00117724
Iteration 6/25 | Loss: 0.00117716
Iteration 7/25 | Loss: 0.00117716
Iteration 8/25 | Loss: 0.00117716
Iteration 9/25 | Loss: 0.00117716
Iteration 10/25 | Loss: 0.00117716
Iteration 11/25 | Loss: 0.00117716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011771608842536807, 0.0011771608842536807, 0.0011771608842536807, 0.0011771608842536807, 0.0011771608842536807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011771608842536807

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56807911
Iteration 2/25 | Loss: 0.00073291
Iteration 3/25 | Loss: 0.00073291
Iteration 4/25 | Loss: 0.00073291
Iteration 5/25 | Loss: 0.00073291
Iteration 6/25 | Loss: 0.00073291
Iteration 7/25 | Loss: 0.00073291
Iteration 8/25 | Loss: 0.00073291
Iteration 9/25 | Loss: 0.00073291
Iteration 10/25 | Loss: 0.00073290
Iteration 11/25 | Loss: 0.00073290
Iteration 12/25 | Loss: 0.00073290
Iteration 13/25 | Loss: 0.00073290
Iteration 14/25 | Loss: 0.00073290
Iteration 15/25 | Loss: 0.00073290
Iteration 16/25 | Loss: 0.00073290
Iteration 17/25 | Loss: 0.00073290
Iteration 18/25 | Loss: 0.00073290
Iteration 19/25 | Loss: 0.00073290
Iteration 20/25 | Loss: 0.00073290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007329044747166336, 0.0007329044747166336, 0.0007329044747166336, 0.0007329044747166336, 0.0007329044747166336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007329044747166336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073290
Iteration 2/1000 | Loss: 0.00002252
Iteration 3/1000 | Loss: 0.00001742
Iteration 4/1000 | Loss: 0.00001603
Iteration 5/1000 | Loss: 0.00001528
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001480
Iteration 8/1000 | Loss: 0.00001447
Iteration 9/1000 | Loss: 0.00001439
Iteration 10/1000 | Loss: 0.00001438
Iteration 11/1000 | Loss: 0.00001437
Iteration 12/1000 | Loss: 0.00001424
Iteration 13/1000 | Loss: 0.00001421
Iteration 14/1000 | Loss: 0.00001421
Iteration 15/1000 | Loss: 0.00001420
Iteration 16/1000 | Loss: 0.00001418
Iteration 17/1000 | Loss: 0.00001417
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001416
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001416
Iteration 23/1000 | Loss: 0.00001416
Iteration 24/1000 | Loss: 0.00001415
Iteration 25/1000 | Loss: 0.00001415
Iteration 26/1000 | Loss: 0.00001409
Iteration 27/1000 | Loss: 0.00001409
Iteration 28/1000 | Loss: 0.00001406
Iteration 29/1000 | Loss: 0.00001394
Iteration 30/1000 | Loss: 0.00001391
Iteration 31/1000 | Loss: 0.00001391
Iteration 32/1000 | Loss: 0.00001387
Iteration 33/1000 | Loss: 0.00001385
Iteration 34/1000 | Loss: 0.00001385
Iteration 35/1000 | Loss: 0.00001384
Iteration 36/1000 | Loss: 0.00001384
Iteration 37/1000 | Loss: 0.00001384
Iteration 38/1000 | Loss: 0.00001384
Iteration 39/1000 | Loss: 0.00001383
Iteration 40/1000 | Loss: 0.00001383
Iteration 41/1000 | Loss: 0.00001381
Iteration 42/1000 | Loss: 0.00001381
Iteration 43/1000 | Loss: 0.00001381
Iteration 44/1000 | Loss: 0.00001381
Iteration 45/1000 | Loss: 0.00001381
Iteration 46/1000 | Loss: 0.00001380
Iteration 47/1000 | Loss: 0.00001380
Iteration 48/1000 | Loss: 0.00001379
Iteration 49/1000 | Loss: 0.00001378
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001376
Iteration 53/1000 | Loss: 0.00001376
Iteration 54/1000 | Loss: 0.00001375
Iteration 55/1000 | Loss: 0.00001375
Iteration 56/1000 | Loss: 0.00001375
Iteration 57/1000 | Loss: 0.00001375
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00001375
Iteration 60/1000 | Loss: 0.00001373
Iteration 61/1000 | Loss: 0.00001373
Iteration 62/1000 | Loss: 0.00001373
Iteration 63/1000 | Loss: 0.00001372
Iteration 64/1000 | Loss: 0.00001372
Iteration 65/1000 | Loss: 0.00001372
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001372
Iteration 68/1000 | Loss: 0.00001372
Iteration 69/1000 | Loss: 0.00001372
Iteration 70/1000 | Loss: 0.00001372
Iteration 71/1000 | Loss: 0.00001371
Iteration 72/1000 | Loss: 0.00001371
Iteration 73/1000 | Loss: 0.00001371
Iteration 74/1000 | Loss: 0.00001371
Iteration 75/1000 | Loss: 0.00001370
Iteration 76/1000 | Loss: 0.00001369
Iteration 77/1000 | Loss: 0.00001368
Iteration 78/1000 | Loss: 0.00001368
Iteration 79/1000 | Loss: 0.00001368
Iteration 80/1000 | Loss: 0.00001368
Iteration 81/1000 | Loss: 0.00001368
Iteration 82/1000 | Loss: 0.00001368
Iteration 83/1000 | Loss: 0.00001367
Iteration 84/1000 | Loss: 0.00001367
Iteration 85/1000 | Loss: 0.00001367
Iteration 86/1000 | Loss: 0.00001367
Iteration 87/1000 | Loss: 0.00001367
Iteration 88/1000 | Loss: 0.00001366
Iteration 89/1000 | Loss: 0.00001366
Iteration 90/1000 | Loss: 0.00001366
Iteration 91/1000 | Loss: 0.00001366
Iteration 92/1000 | Loss: 0.00001366
Iteration 93/1000 | Loss: 0.00001366
Iteration 94/1000 | Loss: 0.00001366
Iteration 95/1000 | Loss: 0.00001365
Iteration 96/1000 | Loss: 0.00001365
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001364
Iteration 100/1000 | Loss: 0.00001364
Iteration 101/1000 | Loss: 0.00001364
Iteration 102/1000 | Loss: 0.00001364
Iteration 103/1000 | Loss: 0.00001363
Iteration 104/1000 | Loss: 0.00001363
Iteration 105/1000 | Loss: 0.00001363
Iteration 106/1000 | Loss: 0.00001362
Iteration 107/1000 | Loss: 0.00001362
Iteration 108/1000 | Loss: 0.00001362
Iteration 109/1000 | Loss: 0.00001362
Iteration 110/1000 | Loss: 0.00001362
Iteration 111/1000 | Loss: 0.00001362
Iteration 112/1000 | Loss: 0.00001362
Iteration 113/1000 | Loss: 0.00001362
Iteration 114/1000 | Loss: 0.00001362
Iteration 115/1000 | Loss: 0.00001362
Iteration 116/1000 | Loss: 0.00001361
Iteration 117/1000 | Loss: 0.00001361
Iteration 118/1000 | Loss: 0.00001361
Iteration 119/1000 | Loss: 0.00001361
Iteration 120/1000 | Loss: 0.00001361
Iteration 121/1000 | Loss: 0.00001361
Iteration 122/1000 | Loss: 0.00001361
Iteration 123/1000 | Loss: 0.00001361
Iteration 124/1000 | Loss: 0.00001361
Iteration 125/1000 | Loss: 0.00001360
Iteration 126/1000 | Loss: 0.00001360
Iteration 127/1000 | Loss: 0.00001360
Iteration 128/1000 | Loss: 0.00001360
Iteration 129/1000 | Loss: 0.00001360
Iteration 130/1000 | Loss: 0.00001359
Iteration 131/1000 | Loss: 0.00001359
Iteration 132/1000 | Loss: 0.00001359
Iteration 133/1000 | Loss: 0.00001359
Iteration 134/1000 | Loss: 0.00001359
Iteration 135/1000 | Loss: 0.00001358
Iteration 136/1000 | Loss: 0.00001358
Iteration 137/1000 | Loss: 0.00001358
Iteration 138/1000 | Loss: 0.00001358
Iteration 139/1000 | Loss: 0.00001358
Iteration 140/1000 | Loss: 0.00001358
Iteration 141/1000 | Loss: 0.00001358
Iteration 142/1000 | Loss: 0.00001358
Iteration 143/1000 | Loss: 0.00001358
Iteration 144/1000 | Loss: 0.00001358
Iteration 145/1000 | Loss: 0.00001357
Iteration 146/1000 | Loss: 0.00001357
Iteration 147/1000 | Loss: 0.00001357
Iteration 148/1000 | Loss: 0.00001357
Iteration 149/1000 | Loss: 0.00001357
Iteration 150/1000 | Loss: 0.00001357
Iteration 151/1000 | Loss: 0.00001357
Iteration 152/1000 | Loss: 0.00001357
Iteration 153/1000 | Loss: 0.00001357
Iteration 154/1000 | Loss: 0.00001357
Iteration 155/1000 | Loss: 0.00001356
Iteration 156/1000 | Loss: 0.00001356
Iteration 157/1000 | Loss: 0.00001356
Iteration 158/1000 | Loss: 0.00001356
Iteration 159/1000 | Loss: 0.00001356
Iteration 160/1000 | Loss: 0.00001356
Iteration 161/1000 | Loss: 0.00001355
Iteration 162/1000 | Loss: 0.00001355
Iteration 163/1000 | Loss: 0.00001355
Iteration 164/1000 | Loss: 0.00001355
Iteration 165/1000 | Loss: 0.00001355
Iteration 166/1000 | Loss: 0.00001354
Iteration 167/1000 | Loss: 0.00001354
Iteration 168/1000 | Loss: 0.00001354
Iteration 169/1000 | Loss: 0.00001354
Iteration 170/1000 | Loss: 0.00001354
Iteration 171/1000 | Loss: 0.00001354
Iteration 172/1000 | Loss: 0.00001354
Iteration 173/1000 | Loss: 0.00001354
Iteration 174/1000 | Loss: 0.00001354
Iteration 175/1000 | Loss: 0.00001354
Iteration 176/1000 | Loss: 0.00001354
Iteration 177/1000 | Loss: 0.00001354
Iteration 178/1000 | Loss: 0.00001354
Iteration 179/1000 | Loss: 0.00001353
Iteration 180/1000 | Loss: 0.00001353
Iteration 181/1000 | Loss: 0.00001353
Iteration 182/1000 | Loss: 0.00001353
Iteration 183/1000 | Loss: 0.00001353
Iteration 184/1000 | Loss: 0.00001353
Iteration 185/1000 | Loss: 0.00001353
Iteration 186/1000 | Loss: 0.00001353
Iteration 187/1000 | Loss: 0.00001353
Iteration 188/1000 | Loss: 0.00001353
Iteration 189/1000 | Loss: 0.00001352
Iteration 190/1000 | Loss: 0.00001352
Iteration 191/1000 | Loss: 0.00001352
Iteration 192/1000 | Loss: 0.00001352
Iteration 193/1000 | Loss: 0.00001352
Iteration 194/1000 | Loss: 0.00001352
Iteration 195/1000 | Loss: 0.00001352
Iteration 196/1000 | Loss: 0.00001352
Iteration 197/1000 | Loss: 0.00001352
Iteration 198/1000 | Loss: 0.00001352
Iteration 199/1000 | Loss: 0.00001352
Iteration 200/1000 | Loss: 0.00001352
Iteration 201/1000 | Loss: 0.00001352
Iteration 202/1000 | Loss: 0.00001352
Iteration 203/1000 | Loss: 0.00001352
Iteration 204/1000 | Loss: 0.00001351
Iteration 205/1000 | Loss: 0.00001351
Iteration 206/1000 | Loss: 0.00001351
Iteration 207/1000 | Loss: 0.00001351
Iteration 208/1000 | Loss: 0.00001351
Iteration 209/1000 | Loss: 0.00001351
Iteration 210/1000 | Loss: 0.00001351
Iteration 211/1000 | Loss: 0.00001351
Iteration 212/1000 | Loss: 0.00001351
Iteration 213/1000 | Loss: 0.00001351
Iteration 214/1000 | Loss: 0.00001351
Iteration 215/1000 | Loss: 0.00001351
Iteration 216/1000 | Loss: 0.00001351
Iteration 217/1000 | Loss: 0.00001351
Iteration 218/1000 | Loss: 0.00001351
Iteration 219/1000 | Loss: 0.00001351
Iteration 220/1000 | Loss: 0.00001351
Iteration 221/1000 | Loss: 0.00001351
Iteration 222/1000 | Loss: 0.00001351
Iteration 223/1000 | Loss: 0.00001351
Iteration 224/1000 | Loss: 0.00001351
Iteration 225/1000 | Loss: 0.00001351
Iteration 226/1000 | Loss: 0.00001351
Iteration 227/1000 | Loss: 0.00001351
Iteration 228/1000 | Loss: 0.00001351
Iteration 229/1000 | Loss: 0.00001351
Iteration 230/1000 | Loss: 0.00001351
Iteration 231/1000 | Loss: 0.00001351
Iteration 232/1000 | Loss: 0.00001351
Iteration 233/1000 | Loss: 0.00001351
Iteration 234/1000 | Loss: 0.00001351
Iteration 235/1000 | Loss: 0.00001351
Iteration 236/1000 | Loss: 0.00001351
Iteration 237/1000 | Loss: 0.00001351
Iteration 238/1000 | Loss: 0.00001351
Iteration 239/1000 | Loss: 0.00001351
Iteration 240/1000 | Loss: 0.00001351
Iteration 241/1000 | Loss: 0.00001351
Iteration 242/1000 | Loss: 0.00001351
Iteration 243/1000 | Loss: 0.00001351
Iteration 244/1000 | Loss: 0.00001351
Iteration 245/1000 | Loss: 0.00001351
Iteration 246/1000 | Loss: 0.00001351
Iteration 247/1000 | Loss: 0.00001351
Iteration 248/1000 | Loss: 0.00001351
Iteration 249/1000 | Loss: 0.00001351
Iteration 250/1000 | Loss: 0.00001351
Iteration 251/1000 | Loss: 0.00001351
Iteration 252/1000 | Loss: 0.00001351
Iteration 253/1000 | Loss: 0.00001351
Iteration 254/1000 | Loss: 0.00001351
Iteration 255/1000 | Loss: 0.00001351
Iteration 256/1000 | Loss: 0.00001351
Iteration 257/1000 | Loss: 0.00001351
Iteration 258/1000 | Loss: 0.00001351
Iteration 259/1000 | Loss: 0.00001351
Iteration 260/1000 | Loss: 0.00001351
Iteration 261/1000 | Loss: 0.00001351
Iteration 262/1000 | Loss: 0.00001351
Iteration 263/1000 | Loss: 0.00001351
Iteration 264/1000 | Loss: 0.00001351
Iteration 265/1000 | Loss: 0.00001351
Iteration 266/1000 | Loss: 0.00001351
Iteration 267/1000 | Loss: 0.00001351
Iteration 268/1000 | Loss: 0.00001351
Iteration 269/1000 | Loss: 0.00001351
Iteration 270/1000 | Loss: 0.00001351
Iteration 271/1000 | Loss: 0.00001351
Iteration 272/1000 | Loss: 0.00001351
Iteration 273/1000 | Loss: 0.00001351
Iteration 274/1000 | Loss: 0.00001351
Iteration 275/1000 | Loss: 0.00001351
Iteration 276/1000 | Loss: 0.00001351
Iteration 277/1000 | Loss: 0.00001351
Iteration 278/1000 | Loss: 0.00001351
Iteration 279/1000 | Loss: 0.00001351
Iteration 280/1000 | Loss: 0.00001351
Iteration 281/1000 | Loss: 0.00001351
Iteration 282/1000 | Loss: 0.00001351
Iteration 283/1000 | Loss: 0.00001351
Iteration 284/1000 | Loss: 0.00001351
Iteration 285/1000 | Loss: 0.00001351
Iteration 286/1000 | Loss: 0.00001351
Iteration 287/1000 | Loss: 0.00001351
Iteration 288/1000 | Loss: 0.00001351
Iteration 289/1000 | Loss: 0.00001351
Iteration 290/1000 | Loss: 0.00001351
Iteration 291/1000 | Loss: 0.00001351
Iteration 292/1000 | Loss: 0.00001351
Iteration 293/1000 | Loss: 0.00001351
Iteration 294/1000 | Loss: 0.00001351
Iteration 295/1000 | Loss: 0.00001351
Iteration 296/1000 | Loss: 0.00001351
Iteration 297/1000 | Loss: 0.00001351
Iteration 298/1000 | Loss: 0.00001351
Iteration 299/1000 | Loss: 0.00001351
Iteration 300/1000 | Loss: 0.00001351
Iteration 301/1000 | Loss: 0.00001351
Iteration 302/1000 | Loss: 0.00001351
Iteration 303/1000 | Loss: 0.00001351
Iteration 304/1000 | Loss: 0.00001351
Iteration 305/1000 | Loss: 0.00001351
Iteration 306/1000 | Loss: 0.00001351
Iteration 307/1000 | Loss: 0.00001351
Iteration 308/1000 | Loss: 0.00001351
Iteration 309/1000 | Loss: 0.00001351
Iteration 310/1000 | Loss: 0.00001351
Iteration 311/1000 | Loss: 0.00001351
Iteration 312/1000 | Loss: 0.00001351
Iteration 313/1000 | Loss: 0.00001351
Iteration 314/1000 | Loss: 0.00001351
Iteration 315/1000 | Loss: 0.00001351
Iteration 316/1000 | Loss: 0.00001351
Iteration 317/1000 | Loss: 0.00001351
Iteration 318/1000 | Loss: 0.00001351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 318. Stopping optimization.
Last 5 losses: [1.3510765711544082e-05, 1.3510765711544082e-05, 1.3510765711544082e-05, 1.3510765711544082e-05, 1.3510765711544082e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3510765711544082e-05

Optimization complete. Final v2v error: 3.1275949478149414 mm

Highest mean error: 3.51653790473938 mm for frame 232

Lowest mean error: 2.762140989303589 mm for frame 60

Saving results

Total time: 48.439473390579224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462517
Iteration 2/25 | Loss: 0.00140672
Iteration 3/25 | Loss: 0.00124307
Iteration 4/25 | Loss: 0.00121482
Iteration 5/25 | Loss: 0.00120795
Iteration 6/25 | Loss: 0.00120678
Iteration 7/25 | Loss: 0.00120652
Iteration 8/25 | Loss: 0.00120652
Iteration 9/25 | Loss: 0.00120652
Iteration 10/25 | Loss: 0.00120652
Iteration 11/25 | Loss: 0.00120652
Iteration 12/25 | Loss: 0.00120652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001206517219543457, 0.001206517219543457, 0.001206517219543457, 0.001206517219543457, 0.001206517219543457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001206517219543457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18493879
Iteration 2/25 | Loss: 0.00101678
Iteration 3/25 | Loss: 0.00101676
Iteration 4/25 | Loss: 0.00101676
Iteration 5/25 | Loss: 0.00101676
Iteration 6/25 | Loss: 0.00101676
Iteration 7/25 | Loss: 0.00101676
Iteration 8/25 | Loss: 0.00101676
Iteration 9/25 | Loss: 0.00101676
Iteration 10/25 | Loss: 0.00101676
Iteration 11/25 | Loss: 0.00101676
Iteration 12/25 | Loss: 0.00101676
Iteration 13/25 | Loss: 0.00101676
Iteration 14/25 | Loss: 0.00101676
Iteration 15/25 | Loss: 0.00101676
Iteration 16/25 | Loss: 0.00101676
Iteration 17/25 | Loss: 0.00101676
Iteration 18/25 | Loss: 0.00101675
Iteration 19/25 | Loss: 0.00101676
Iteration 20/25 | Loss: 0.00101675
Iteration 21/25 | Loss: 0.00101675
Iteration 22/25 | Loss: 0.00101676
Iteration 23/25 | Loss: 0.00101675
Iteration 24/25 | Loss: 0.00101675
Iteration 25/25 | Loss: 0.00101675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101675
Iteration 2/1000 | Loss: 0.00006649
Iteration 3/1000 | Loss: 0.00004315
Iteration 4/1000 | Loss: 0.00003170
Iteration 5/1000 | Loss: 0.00002943
Iteration 6/1000 | Loss: 0.00002792
Iteration 7/1000 | Loss: 0.00002694
Iteration 8/1000 | Loss: 0.00002624
Iteration 9/1000 | Loss: 0.00002571
Iteration 10/1000 | Loss: 0.00002521
Iteration 11/1000 | Loss: 0.00002476
Iteration 12/1000 | Loss: 0.00002454
Iteration 13/1000 | Loss: 0.00002447
Iteration 14/1000 | Loss: 0.00002444
Iteration 15/1000 | Loss: 0.00002423
Iteration 16/1000 | Loss: 0.00002409
Iteration 17/1000 | Loss: 0.00002408
Iteration 18/1000 | Loss: 0.00002403
Iteration 19/1000 | Loss: 0.00002401
Iteration 20/1000 | Loss: 0.00002396
Iteration 21/1000 | Loss: 0.00002394
Iteration 22/1000 | Loss: 0.00002392
Iteration 23/1000 | Loss: 0.00002392
Iteration 24/1000 | Loss: 0.00002391
Iteration 25/1000 | Loss: 0.00002391
Iteration 26/1000 | Loss: 0.00002390
Iteration 27/1000 | Loss: 0.00002390
Iteration 28/1000 | Loss: 0.00002389
Iteration 29/1000 | Loss: 0.00002389
Iteration 30/1000 | Loss: 0.00002389
Iteration 31/1000 | Loss: 0.00002384
Iteration 32/1000 | Loss: 0.00002379
Iteration 33/1000 | Loss: 0.00002368
Iteration 34/1000 | Loss: 0.00002367
Iteration 35/1000 | Loss: 0.00002367
Iteration 36/1000 | Loss: 0.00002366
Iteration 37/1000 | Loss: 0.00002365
Iteration 38/1000 | Loss: 0.00002365
Iteration 39/1000 | Loss: 0.00002364
Iteration 40/1000 | Loss: 0.00002363
Iteration 41/1000 | Loss: 0.00002363
Iteration 42/1000 | Loss: 0.00002362
Iteration 43/1000 | Loss: 0.00002362
Iteration 44/1000 | Loss: 0.00002362
Iteration 45/1000 | Loss: 0.00002362
Iteration 46/1000 | Loss: 0.00002362
Iteration 47/1000 | Loss: 0.00002361
Iteration 48/1000 | Loss: 0.00002360
Iteration 49/1000 | Loss: 0.00002358
Iteration 50/1000 | Loss: 0.00002357
Iteration 51/1000 | Loss: 0.00002357
Iteration 52/1000 | Loss: 0.00002356
Iteration 53/1000 | Loss: 0.00002356
Iteration 54/1000 | Loss: 0.00002356
Iteration 55/1000 | Loss: 0.00002354
Iteration 56/1000 | Loss: 0.00002354
Iteration 57/1000 | Loss: 0.00002354
Iteration 58/1000 | Loss: 0.00002354
Iteration 59/1000 | Loss: 0.00002354
Iteration 60/1000 | Loss: 0.00002354
Iteration 61/1000 | Loss: 0.00002353
Iteration 62/1000 | Loss: 0.00002352
Iteration 63/1000 | Loss: 0.00002349
Iteration 64/1000 | Loss: 0.00002349
Iteration 65/1000 | Loss: 0.00002349
Iteration 66/1000 | Loss: 0.00002349
Iteration 67/1000 | Loss: 0.00002348
Iteration 68/1000 | Loss: 0.00002348
Iteration 69/1000 | Loss: 0.00002347
Iteration 70/1000 | Loss: 0.00002346
Iteration 71/1000 | Loss: 0.00002346
Iteration 72/1000 | Loss: 0.00002346
Iteration 73/1000 | Loss: 0.00002343
Iteration 74/1000 | Loss: 0.00002342
Iteration 75/1000 | Loss: 0.00002342
Iteration 76/1000 | Loss: 0.00002342
Iteration 77/1000 | Loss: 0.00002341
Iteration 78/1000 | Loss: 0.00002340
Iteration 79/1000 | Loss: 0.00002340
Iteration 80/1000 | Loss: 0.00002340
Iteration 81/1000 | Loss: 0.00002339
Iteration 82/1000 | Loss: 0.00002339
Iteration 83/1000 | Loss: 0.00002339
Iteration 84/1000 | Loss: 0.00002339
Iteration 85/1000 | Loss: 0.00002338
Iteration 86/1000 | Loss: 0.00002338
Iteration 87/1000 | Loss: 0.00002338
Iteration 88/1000 | Loss: 0.00002338
Iteration 89/1000 | Loss: 0.00002338
Iteration 90/1000 | Loss: 0.00002338
Iteration 91/1000 | Loss: 0.00002337
Iteration 92/1000 | Loss: 0.00002337
Iteration 93/1000 | Loss: 0.00002336
Iteration 94/1000 | Loss: 0.00002336
Iteration 95/1000 | Loss: 0.00002336
Iteration 96/1000 | Loss: 0.00002336
Iteration 97/1000 | Loss: 0.00002336
Iteration 98/1000 | Loss: 0.00002336
Iteration 99/1000 | Loss: 0.00002336
Iteration 100/1000 | Loss: 0.00002336
Iteration 101/1000 | Loss: 0.00002336
Iteration 102/1000 | Loss: 0.00002336
Iteration 103/1000 | Loss: 0.00002336
Iteration 104/1000 | Loss: 0.00002335
Iteration 105/1000 | Loss: 0.00002335
Iteration 106/1000 | Loss: 0.00002335
Iteration 107/1000 | Loss: 0.00002334
Iteration 108/1000 | Loss: 0.00002334
Iteration 109/1000 | Loss: 0.00002334
Iteration 110/1000 | Loss: 0.00002333
Iteration 111/1000 | Loss: 0.00002333
Iteration 112/1000 | Loss: 0.00002333
Iteration 113/1000 | Loss: 0.00002333
Iteration 114/1000 | Loss: 0.00002333
Iteration 115/1000 | Loss: 0.00002333
Iteration 116/1000 | Loss: 0.00002333
Iteration 117/1000 | Loss: 0.00002333
Iteration 118/1000 | Loss: 0.00002333
Iteration 119/1000 | Loss: 0.00002333
Iteration 120/1000 | Loss: 0.00002333
Iteration 121/1000 | Loss: 0.00002332
Iteration 122/1000 | Loss: 0.00002332
Iteration 123/1000 | Loss: 0.00002332
Iteration 124/1000 | Loss: 0.00002332
Iteration 125/1000 | Loss: 0.00002332
Iteration 126/1000 | Loss: 0.00002332
Iteration 127/1000 | Loss: 0.00002332
Iteration 128/1000 | Loss: 0.00002332
Iteration 129/1000 | Loss: 0.00002332
Iteration 130/1000 | Loss: 0.00002332
Iteration 131/1000 | Loss: 0.00002332
Iteration 132/1000 | Loss: 0.00002332
Iteration 133/1000 | Loss: 0.00002331
Iteration 134/1000 | Loss: 0.00002331
Iteration 135/1000 | Loss: 0.00002331
Iteration 136/1000 | Loss: 0.00002331
Iteration 137/1000 | Loss: 0.00002330
Iteration 138/1000 | Loss: 0.00002330
Iteration 139/1000 | Loss: 0.00002330
Iteration 140/1000 | Loss: 0.00002330
Iteration 141/1000 | Loss: 0.00002330
Iteration 142/1000 | Loss: 0.00002330
Iteration 143/1000 | Loss: 0.00002330
Iteration 144/1000 | Loss: 0.00002330
Iteration 145/1000 | Loss: 0.00002329
Iteration 146/1000 | Loss: 0.00002329
Iteration 147/1000 | Loss: 0.00002329
Iteration 148/1000 | Loss: 0.00002329
Iteration 149/1000 | Loss: 0.00002329
Iteration 150/1000 | Loss: 0.00002329
Iteration 151/1000 | Loss: 0.00002329
Iteration 152/1000 | Loss: 0.00002329
Iteration 153/1000 | Loss: 0.00002329
Iteration 154/1000 | Loss: 0.00002329
Iteration 155/1000 | Loss: 0.00002329
Iteration 156/1000 | Loss: 0.00002328
Iteration 157/1000 | Loss: 0.00002328
Iteration 158/1000 | Loss: 0.00002328
Iteration 159/1000 | Loss: 0.00002328
Iteration 160/1000 | Loss: 0.00002328
Iteration 161/1000 | Loss: 0.00002328
Iteration 162/1000 | Loss: 0.00002328
Iteration 163/1000 | Loss: 0.00002328
Iteration 164/1000 | Loss: 0.00002328
Iteration 165/1000 | Loss: 0.00002328
Iteration 166/1000 | Loss: 0.00002328
Iteration 167/1000 | Loss: 0.00002327
Iteration 168/1000 | Loss: 0.00002327
Iteration 169/1000 | Loss: 0.00002327
Iteration 170/1000 | Loss: 0.00002327
Iteration 171/1000 | Loss: 0.00002327
Iteration 172/1000 | Loss: 0.00002327
Iteration 173/1000 | Loss: 0.00002327
Iteration 174/1000 | Loss: 0.00002326
Iteration 175/1000 | Loss: 0.00002326
Iteration 176/1000 | Loss: 0.00002326
Iteration 177/1000 | Loss: 0.00002326
Iteration 178/1000 | Loss: 0.00002326
Iteration 179/1000 | Loss: 0.00002326
Iteration 180/1000 | Loss: 0.00002326
Iteration 181/1000 | Loss: 0.00002326
Iteration 182/1000 | Loss: 0.00002326
Iteration 183/1000 | Loss: 0.00002325
Iteration 184/1000 | Loss: 0.00002325
Iteration 185/1000 | Loss: 0.00002325
Iteration 186/1000 | Loss: 0.00002325
Iteration 187/1000 | Loss: 0.00002324
Iteration 188/1000 | Loss: 0.00002324
Iteration 189/1000 | Loss: 0.00002324
Iteration 190/1000 | Loss: 0.00002324
Iteration 191/1000 | Loss: 0.00002324
Iteration 192/1000 | Loss: 0.00002324
Iteration 193/1000 | Loss: 0.00002324
Iteration 194/1000 | Loss: 0.00002324
Iteration 195/1000 | Loss: 0.00002324
Iteration 196/1000 | Loss: 0.00002324
Iteration 197/1000 | Loss: 0.00002323
Iteration 198/1000 | Loss: 0.00002323
Iteration 199/1000 | Loss: 0.00002323
Iteration 200/1000 | Loss: 0.00002323
Iteration 201/1000 | Loss: 0.00002323
Iteration 202/1000 | Loss: 0.00002323
Iteration 203/1000 | Loss: 0.00002323
Iteration 204/1000 | Loss: 0.00002323
Iteration 205/1000 | Loss: 0.00002323
Iteration 206/1000 | Loss: 0.00002323
Iteration 207/1000 | Loss: 0.00002323
Iteration 208/1000 | Loss: 0.00002323
Iteration 209/1000 | Loss: 0.00002323
Iteration 210/1000 | Loss: 0.00002322
Iteration 211/1000 | Loss: 0.00002322
Iteration 212/1000 | Loss: 0.00002322
Iteration 213/1000 | Loss: 0.00002322
Iteration 214/1000 | Loss: 0.00002322
Iteration 215/1000 | Loss: 0.00002322
Iteration 216/1000 | Loss: 0.00002322
Iteration 217/1000 | Loss: 0.00002322
Iteration 218/1000 | Loss: 0.00002322
Iteration 219/1000 | Loss: 0.00002322
Iteration 220/1000 | Loss: 0.00002322
Iteration 221/1000 | Loss: 0.00002322
Iteration 222/1000 | Loss: 0.00002322
Iteration 223/1000 | Loss: 0.00002322
Iteration 224/1000 | Loss: 0.00002322
Iteration 225/1000 | Loss: 0.00002322
Iteration 226/1000 | Loss: 0.00002322
Iteration 227/1000 | Loss: 0.00002321
Iteration 228/1000 | Loss: 0.00002321
Iteration 229/1000 | Loss: 0.00002321
Iteration 230/1000 | Loss: 0.00002321
Iteration 231/1000 | Loss: 0.00002321
Iteration 232/1000 | Loss: 0.00002321
Iteration 233/1000 | Loss: 0.00002321
Iteration 234/1000 | Loss: 0.00002321
Iteration 235/1000 | Loss: 0.00002321
Iteration 236/1000 | Loss: 0.00002321
Iteration 237/1000 | Loss: 0.00002321
Iteration 238/1000 | Loss: 0.00002321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [2.3213960957946256e-05, 2.3213960957946256e-05, 2.3213960957946256e-05, 2.3213960957946256e-05, 2.3213960957946256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3213960957946256e-05

Optimization complete. Final v2v error: 3.8691556453704834 mm

Highest mean error: 5.534526348114014 mm for frame 73

Lowest mean error: 3.031715154647827 mm for frame 36

Saving results

Total time: 51.0671923160553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740082
Iteration 2/25 | Loss: 0.00176039
Iteration 3/25 | Loss: 0.00132376
Iteration 4/25 | Loss: 0.00124593
Iteration 5/25 | Loss: 0.00123531
Iteration 6/25 | Loss: 0.00123478
Iteration 7/25 | Loss: 0.00123478
Iteration 8/25 | Loss: 0.00123478
Iteration 9/25 | Loss: 0.00123478
Iteration 10/25 | Loss: 0.00123478
Iteration 11/25 | Loss: 0.00123478
Iteration 12/25 | Loss: 0.00123478
Iteration 13/25 | Loss: 0.00123478
Iteration 14/25 | Loss: 0.00123478
Iteration 15/25 | Loss: 0.00123478
Iteration 16/25 | Loss: 0.00123478
Iteration 17/25 | Loss: 0.00123478
Iteration 18/25 | Loss: 0.00123478
Iteration 19/25 | Loss: 0.00123478
Iteration 20/25 | Loss: 0.00123478
Iteration 21/25 | Loss: 0.00123478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001234780065715313, 0.001234780065715313, 0.001234780065715313, 0.001234780065715313, 0.001234780065715313]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001234780065715313

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71161342
Iteration 2/25 | Loss: 0.00073528
Iteration 3/25 | Loss: 0.00073524
Iteration 4/25 | Loss: 0.00073523
Iteration 5/25 | Loss: 0.00073523
Iteration 6/25 | Loss: 0.00073523
Iteration 7/25 | Loss: 0.00073523
Iteration 8/25 | Loss: 0.00073523
Iteration 9/25 | Loss: 0.00073523
Iteration 10/25 | Loss: 0.00073523
Iteration 11/25 | Loss: 0.00073523
Iteration 12/25 | Loss: 0.00073523
Iteration 13/25 | Loss: 0.00073523
Iteration 14/25 | Loss: 0.00073523
Iteration 15/25 | Loss: 0.00073523
Iteration 16/25 | Loss: 0.00073523
Iteration 17/25 | Loss: 0.00073523
Iteration 18/25 | Loss: 0.00073523
Iteration 19/25 | Loss: 0.00073523
Iteration 20/25 | Loss: 0.00073523
Iteration 21/25 | Loss: 0.00073523
Iteration 22/25 | Loss: 0.00073523
Iteration 23/25 | Loss: 0.00073523
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007352327229455113, 0.0007352327229455113, 0.0007352327229455113, 0.0007352327229455113, 0.0007352327229455113]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007352327229455113

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073523
Iteration 2/1000 | Loss: 0.00003688
Iteration 3/1000 | Loss: 0.00002529
Iteration 4/1000 | Loss: 0.00002369
Iteration 5/1000 | Loss: 0.00002268
Iteration 6/1000 | Loss: 0.00002207
Iteration 7/1000 | Loss: 0.00002167
Iteration 8/1000 | Loss: 0.00002131
Iteration 9/1000 | Loss: 0.00002106
Iteration 10/1000 | Loss: 0.00002086
Iteration 11/1000 | Loss: 0.00002062
Iteration 12/1000 | Loss: 0.00002040
Iteration 13/1000 | Loss: 0.00002032
Iteration 14/1000 | Loss: 0.00002028
Iteration 15/1000 | Loss: 0.00002028
Iteration 16/1000 | Loss: 0.00002025
Iteration 17/1000 | Loss: 0.00002024
Iteration 18/1000 | Loss: 0.00002024
Iteration 19/1000 | Loss: 0.00002023
Iteration 20/1000 | Loss: 0.00002022
Iteration 21/1000 | Loss: 0.00002020
Iteration 22/1000 | Loss: 0.00002012
Iteration 23/1000 | Loss: 0.00002012
Iteration 24/1000 | Loss: 0.00002011
Iteration 25/1000 | Loss: 0.00002011
Iteration 26/1000 | Loss: 0.00002011
Iteration 27/1000 | Loss: 0.00002010
Iteration 28/1000 | Loss: 0.00002010
Iteration 29/1000 | Loss: 0.00002009
Iteration 30/1000 | Loss: 0.00002009
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00002008
Iteration 33/1000 | Loss: 0.00002008
Iteration 34/1000 | Loss: 0.00002008
Iteration 35/1000 | Loss: 0.00002007
Iteration 36/1000 | Loss: 0.00002007
Iteration 37/1000 | Loss: 0.00002007
Iteration 38/1000 | Loss: 0.00002006
Iteration 39/1000 | Loss: 0.00002006
Iteration 40/1000 | Loss: 0.00002006
Iteration 41/1000 | Loss: 0.00002006
Iteration 42/1000 | Loss: 0.00002005
Iteration 43/1000 | Loss: 0.00002005
Iteration 44/1000 | Loss: 0.00002005
Iteration 45/1000 | Loss: 0.00002004
Iteration 46/1000 | Loss: 0.00002004
Iteration 47/1000 | Loss: 0.00002004
Iteration 48/1000 | Loss: 0.00002004
Iteration 49/1000 | Loss: 0.00002004
Iteration 50/1000 | Loss: 0.00002004
Iteration 51/1000 | Loss: 0.00002004
Iteration 52/1000 | Loss: 0.00002003
Iteration 53/1000 | Loss: 0.00002003
Iteration 54/1000 | Loss: 0.00002003
Iteration 55/1000 | Loss: 0.00002003
Iteration 56/1000 | Loss: 0.00002003
Iteration 57/1000 | Loss: 0.00002002
Iteration 58/1000 | Loss: 0.00002002
Iteration 59/1000 | Loss: 0.00002002
Iteration 60/1000 | Loss: 0.00002002
Iteration 61/1000 | Loss: 0.00002002
Iteration 62/1000 | Loss: 0.00002001
Iteration 63/1000 | Loss: 0.00002001
Iteration 64/1000 | Loss: 0.00002001
Iteration 65/1000 | Loss: 0.00002001
Iteration 66/1000 | Loss: 0.00002001
Iteration 67/1000 | Loss: 0.00002001
Iteration 68/1000 | Loss: 0.00002001
Iteration 69/1000 | Loss: 0.00002001
Iteration 70/1000 | Loss: 0.00002001
Iteration 71/1000 | Loss: 0.00002001
Iteration 72/1000 | Loss: 0.00002001
Iteration 73/1000 | Loss: 0.00002001
Iteration 74/1000 | Loss: 0.00002001
Iteration 75/1000 | Loss: 0.00002001
Iteration 76/1000 | Loss: 0.00002001
Iteration 77/1000 | Loss: 0.00002001
Iteration 78/1000 | Loss: 0.00002001
Iteration 79/1000 | Loss: 0.00002001
Iteration 80/1000 | Loss: 0.00002000
Iteration 81/1000 | Loss: 0.00002000
Iteration 82/1000 | Loss: 0.00002000
Iteration 83/1000 | Loss: 0.00002000
Iteration 84/1000 | Loss: 0.00002000
Iteration 85/1000 | Loss: 0.00002000
Iteration 86/1000 | Loss: 0.00002000
Iteration 87/1000 | Loss: 0.00002000
Iteration 88/1000 | Loss: 0.00002000
Iteration 89/1000 | Loss: 0.00002000
Iteration 90/1000 | Loss: 0.00002000
Iteration 91/1000 | Loss: 0.00002000
Iteration 92/1000 | Loss: 0.00002000
Iteration 93/1000 | Loss: 0.00002000
Iteration 94/1000 | Loss: 0.00002000
Iteration 95/1000 | Loss: 0.00002000
Iteration 96/1000 | Loss: 0.00002000
Iteration 97/1000 | Loss: 0.00002000
Iteration 98/1000 | Loss: 0.00002000
Iteration 99/1000 | Loss: 0.00002000
Iteration 100/1000 | Loss: 0.00002000
Iteration 101/1000 | Loss: 0.00002000
Iteration 102/1000 | Loss: 0.00002000
Iteration 103/1000 | Loss: 0.00002000
Iteration 104/1000 | Loss: 0.00002000
Iteration 105/1000 | Loss: 0.00002000
Iteration 106/1000 | Loss: 0.00002000
Iteration 107/1000 | Loss: 0.00002000
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [2.0002369637950324e-05, 2.0002369637950324e-05, 2.0002369637950324e-05, 2.0002369637950324e-05, 2.0002369637950324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0002369637950324e-05

Optimization complete. Final v2v error: 3.702610731124878 mm

Highest mean error: 4.034556865692139 mm for frame 235

Lowest mean error: 3.450889825820923 mm for frame 125

Saving results

Total time: 37.75701117515564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923010
Iteration 2/25 | Loss: 0.00216200
Iteration 3/25 | Loss: 0.00145318
Iteration 4/25 | Loss: 0.00133940
Iteration 5/25 | Loss: 0.00131029
Iteration 6/25 | Loss: 0.00130506
Iteration 7/25 | Loss: 0.00130054
Iteration 8/25 | Loss: 0.00129023
Iteration 9/25 | Loss: 0.00128260
Iteration 10/25 | Loss: 0.00127584
Iteration 11/25 | Loss: 0.00127290
Iteration 12/25 | Loss: 0.00127674
Iteration 13/25 | Loss: 0.00128012
Iteration 14/25 | Loss: 0.00128732
Iteration 15/25 | Loss: 0.00128110
Iteration 16/25 | Loss: 0.00126845
Iteration 17/25 | Loss: 0.00126348
Iteration 18/25 | Loss: 0.00126428
Iteration 19/25 | Loss: 0.00126183
Iteration 20/25 | Loss: 0.00126018
Iteration 21/25 | Loss: 0.00125956
Iteration 22/25 | Loss: 0.00125951
Iteration 23/25 | Loss: 0.00125908
Iteration 24/25 | Loss: 0.00125906
Iteration 25/25 | Loss: 0.00126014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28303587
Iteration 2/25 | Loss: 0.00086628
Iteration 3/25 | Loss: 0.00086628
Iteration 4/25 | Loss: 0.00086627
Iteration 5/25 | Loss: 0.00086627
Iteration 6/25 | Loss: 0.00086627
Iteration 7/25 | Loss: 0.00086627
Iteration 8/25 | Loss: 0.00086627
Iteration 9/25 | Loss: 0.00086627
Iteration 10/25 | Loss: 0.00086627
Iteration 11/25 | Loss: 0.00086627
Iteration 12/25 | Loss: 0.00086627
Iteration 13/25 | Loss: 0.00086627
Iteration 14/25 | Loss: 0.00086627
Iteration 15/25 | Loss: 0.00086627
Iteration 16/25 | Loss: 0.00086627
Iteration 17/25 | Loss: 0.00086627
Iteration 18/25 | Loss: 0.00086627
Iteration 19/25 | Loss: 0.00086627
Iteration 20/25 | Loss: 0.00086627
Iteration 21/25 | Loss: 0.00086627
Iteration 22/25 | Loss: 0.00086627
Iteration 23/25 | Loss: 0.00086627
Iteration 24/25 | Loss: 0.00086627
Iteration 25/25 | Loss: 0.00086627

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086627
Iteration 2/1000 | Loss: 0.00004744
Iteration 3/1000 | Loss: 0.00003209
Iteration 4/1000 | Loss: 0.00002387
Iteration 5/1000 | Loss: 0.00002187
Iteration 6/1000 | Loss: 0.00002066
Iteration 7/1000 | Loss: 0.00001979
Iteration 8/1000 | Loss: 0.00001924
Iteration 9/1000 | Loss: 0.00001885
Iteration 10/1000 | Loss: 0.00001854
Iteration 11/1000 | Loss: 0.00001825
Iteration 12/1000 | Loss: 0.00001808
Iteration 13/1000 | Loss: 0.00001803
Iteration 14/1000 | Loss: 0.00001801
Iteration 15/1000 | Loss: 0.00001797
Iteration 16/1000 | Loss: 0.00001796
Iteration 17/1000 | Loss: 0.00001795
Iteration 18/1000 | Loss: 0.00001794
Iteration 19/1000 | Loss: 0.00001794
Iteration 20/1000 | Loss: 0.00001793
Iteration 21/1000 | Loss: 0.00001788
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00001783
Iteration 25/1000 | Loss: 0.00001781
Iteration 26/1000 | Loss: 0.00001781
Iteration 27/1000 | Loss: 0.00001781
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001781
Iteration 31/1000 | Loss: 0.00001781
Iteration 32/1000 | Loss: 0.00001781
Iteration 33/1000 | Loss: 0.00001780
Iteration 34/1000 | Loss: 0.00001780
Iteration 35/1000 | Loss: 0.00001778
Iteration 36/1000 | Loss: 0.00001778
Iteration 37/1000 | Loss: 0.00001778
Iteration 38/1000 | Loss: 0.00001778
Iteration 39/1000 | Loss: 0.00001777
Iteration 40/1000 | Loss: 0.00001777
Iteration 41/1000 | Loss: 0.00001777
Iteration 42/1000 | Loss: 0.00001777
Iteration 43/1000 | Loss: 0.00001777
Iteration 44/1000 | Loss: 0.00001777
Iteration 45/1000 | Loss: 0.00001777
Iteration 46/1000 | Loss: 0.00001777
Iteration 47/1000 | Loss: 0.00001776
Iteration 48/1000 | Loss: 0.00001776
Iteration 49/1000 | Loss: 0.00001774
Iteration 50/1000 | Loss: 0.00001773
Iteration 51/1000 | Loss: 0.00001773
Iteration 52/1000 | Loss: 0.00001773
Iteration 53/1000 | Loss: 0.00001771
Iteration 54/1000 | Loss: 0.00001771
Iteration 55/1000 | Loss: 0.00001771
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001771
Iteration 60/1000 | Loss: 0.00001771
Iteration 61/1000 | Loss: 0.00001770
Iteration 62/1000 | Loss: 0.00001770
Iteration 63/1000 | Loss: 0.00001770
Iteration 64/1000 | Loss: 0.00001770
Iteration 65/1000 | Loss: 0.00001770
Iteration 66/1000 | Loss: 0.00001770
Iteration 67/1000 | Loss: 0.00001770
Iteration 68/1000 | Loss: 0.00001770
Iteration 69/1000 | Loss: 0.00001770
Iteration 70/1000 | Loss: 0.00001770
Iteration 71/1000 | Loss: 0.00001770
Iteration 72/1000 | Loss: 0.00001769
Iteration 73/1000 | Loss: 0.00001768
Iteration 74/1000 | Loss: 0.00001768
Iteration 75/1000 | Loss: 0.00001768
Iteration 76/1000 | Loss: 0.00001768
Iteration 77/1000 | Loss: 0.00001768
Iteration 78/1000 | Loss: 0.00001768
Iteration 79/1000 | Loss: 0.00001768
Iteration 80/1000 | Loss: 0.00001768
Iteration 81/1000 | Loss: 0.00001767
Iteration 82/1000 | Loss: 0.00001767
Iteration 83/1000 | Loss: 0.00001767
Iteration 84/1000 | Loss: 0.00001766
Iteration 85/1000 | Loss: 0.00001766
Iteration 86/1000 | Loss: 0.00001766
Iteration 87/1000 | Loss: 0.00001765
Iteration 88/1000 | Loss: 0.00001765
Iteration 89/1000 | Loss: 0.00001765
Iteration 90/1000 | Loss: 0.00001765
Iteration 91/1000 | Loss: 0.00001765
Iteration 92/1000 | Loss: 0.00001765
Iteration 93/1000 | Loss: 0.00001764
Iteration 94/1000 | Loss: 0.00001764
Iteration 95/1000 | Loss: 0.00001764
Iteration 96/1000 | Loss: 0.00001764
Iteration 97/1000 | Loss: 0.00001764
Iteration 98/1000 | Loss: 0.00001763
Iteration 99/1000 | Loss: 0.00001763
Iteration 100/1000 | Loss: 0.00001763
Iteration 101/1000 | Loss: 0.00001762
Iteration 102/1000 | Loss: 0.00001762
Iteration 103/1000 | Loss: 0.00001762
Iteration 104/1000 | Loss: 0.00001762
Iteration 105/1000 | Loss: 0.00001762
Iteration 106/1000 | Loss: 0.00001761
Iteration 107/1000 | Loss: 0.00001761
Iteration 108/1000 | Loss: 0.00001761
Iteration 109/1000 | Loss: 0.00001761
Iteration 110/1000 | Loss: 0.00001761
Iteration 111/1000 | Loss: 0.00001761
Iteration 112/1000 | Loss: 0.00001761
Iteration 113/1000 | Loss: 0.00001761
Iteration 114/1000 | Loss: 0.00001760
Iteration 115/1000 | Loss: 0.00001760
Iteration 116/1000 | Loss: 0.00001760
Iteration 117/1000 | Loss: 0.00001760
Iteration 118/1000 | Loss: 0.00001760
Iteration 119/1000 | Loss: 0.00001760
Iteration 120/1000 | Loss: 0.00001760
Iteration 121/1000 | Loss: 0.00001759
Iteration 122/1000 | Loss: 0.00001759
Iteration 123/1000 | Loss: 0.00001759
Iteration 124/1000 | Loss: 0.00001759
Iteration 125/1000 | Loss: 0.00001759
Iteration 126/1000 | Loss: 0.00001759
Iteration 127/1000 | Loss: 0.00001759
Iteration 128/1000 | Loss: 0.00001759
Iteration 129/1000 | Loss: 0.00001759
Iteration 130/1000 | Loss: 0.00001759
Iteration 131/1000 | Loss: 0.00001759
Iteration 132/1000 | Loss: 0.00001759
Iteration 133/1000 | Loss: 0.00001759
Iteration 134/1000 | Loss: 0.00001759
Iteration 135/1000 | Loss: 0.00001759
Iteration 136/1000 | Loss: 0.00001759
Iteration 137/1000 | Loss: 0.00001759
Iteration 138/1000 | Loss: 0.00001759
Iteration 139/1000 | Loss: 0.00001759
Iteration 140/1000 | Loss: 0.00001759
Iteration 141/1000 | Loss: 0.00001759
Iteration 142/1000 | Loss: 0.00001759
Iteration 143/1000 | Loss: 0.00001759
Iteration 144/1000 | Loss: 0.00001759
Iteration 145/1000 | Loss: 0.00001759
Iteration 146/1000 | Loss: 0.00001759
Iteration 147/1000 | Loss: 0.00001759
Iteration 148/1000 | Loss: 0.00001759
Iteration 149/1000 | Loss: 0.00001759
Iteration 150/1000 | Loss: 0.00001759
Iteration 151/1000 | Loss: 0.00001759
Iteration 152/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.7592898075236008e-05, 1.7592898075236008e-05, 1.7592898075236008e-05, 1.7592898075236008e-05, 1.7592898075236008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7592898075236008e-05

Optimization complete. Final v2v error: 3.5076234340667725 mm

Highest mean error: 4.440821647644043 mm for frame 94

Lowest mean error: 3.0203073024749756 mm for frame 103

Saving results

Total time: 80.10250687599182
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430639
Iteration 2/25 | Loss: 0.00123930
Iteration 3/25 | Loss: 0.00117499
Iteration 4/25 | Loss: 0.00116680
Iteration 5/25 | Loss: 0.00116472
Iteration 6/25 | Loss: 0.00116472
Iteration 7/25 | Loss: 0.00116472
Iteration 8/25 | Loss: 0.00116472
Iteration 9/25 | Loss: 0.00116472
Iteration 10/25 | Loss: 0.00116472
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011647188803181052, 0.0011647188803181052, 0.0011647188803181052, 0.0011647188803181052, 0.0011647188803181052]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011647188803181052

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40038943
Iteration 2/25 | Loss: 0.00075477
Iteration 3/25 | Loss: 0.00075476
Iteration 4/25 | Loss: 0.00075476
Iteration 5/25 | Loss: 0.00075476
Iteration 6/25 | Loss: 0.00075476
Iteration 7/25 | Loss: 0.00075476
Iteration 8/25 | Loss: 0.00075476
Iteration 9/25 | Loss: 0.00075476
Iteration 10/25 | Loss: 0.00075476
Iteration 11/25 | Loss: 0.00075476
Iteration 12/25 | Loss: 0.00075476
Iteration 13/25 | Loss: 0.00075476
Iteration 14/25 | Loss: 0.00075476
Iteration 15/25 | Loss: 0.00075476
Iteration 16/25 | Loss: 0.00075476
Iteration 17/25 | Loss: 0.00075476
Iteration 18/25 | Loss: 0.00075476
Iteration 19/25 | Loss: 0.00075476
Iteration 20/25 | Loss: 0.00075476
Iteration 21/25 | Loss: 0.00075476
Iteration 22/25 | Loss: 0.00075476
Iteration 23/25 | Loss: 0.00075476
Iteration 24/25 | Loss: 0.00075476
Iteration 25/25 | Loss: 0.00075476

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075476
Iteration 2/1000 | Loss: 0.00002958
Iteration 3/1000 | Loss: 0.00002065
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001747
Iteration 6/1000 | Loss: 0.00001676
Iteration 7/1000 | Loss: 0.00001627
Iteration 8/1000 | Loss: 0.00001593
Iteration 9/1000 | Loss: 0.00001566
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001516
Iteration 12/1000 | Loss: 0.00001516
Iteration 13/1000 | Loss: 0.00001507
Iteration 14/1000 | Loss: 0.00001498
Iteration 15/1000 | Loss: 0.00001488
Iteration 16/1000 | Loss: 0.00001486
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001477
Iteration 19/1000 | Loss: 0.00001468
Iteration 20/1000 | Loss: 0.00001466
Iteration 21/1000 | Loss: 0.00001466
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001465
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001459
Iteration 26/1000 | Loss: 0.00001459
Iteration 27/1000 | Loss: 0.00001455
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001446
Iteration 30/1000 | Loss: 0.00001445
Iteration 31/1000 | Loss: 0.00001445
Iteration 32/1000 | Loss: 0.00001444
Iteration 33/1000 | Loss: 0.00001441
Iteration 34/1000 | Loss: 0.00001440
Iteration 35/1000 | Loss: 0.00001439
Iteration 36/1000 | Loss: 0.00001436
Iteration 37/1000 | Loss: 0.00001433
Iteration 38/1000 | Loss: 0.00001433
Iteration 39/1000 | Loss: 0.00001432
Iteration 40/1000 | Loss: 0.00001431
Iteration 41/1000 | Loss: 0.00001431
Iteration 42/1000 | Loss: 0.00001431
Iteration 43/1000 | Loss: 0.00001430
Iteration 44/1000 | Loss: 0.00001430
Iteration 45/1000 | Loss: 0.00001429
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001429
Iteration 48/1000 | Loss: 0.00001428
Iteration 49/1000 | Loss: 0.00001428
Iteration 50/1000 | Loss: 0.00001428
Iteration 51/1000 | Loss: 0.00001427
Iteration 52/1000 | Loss: 0.00001425
Iteration 53/1000 | Loss: 0.00001425
Iteration 54/1000 | Loss: 0.00001425
Iteration 55/1000 | Loss: 0.00001425
Iteration 56/1000 | Loss: 0.00001425
Iteration 57/1000 | Loss: 0.00001425
Iteration 58/1000 | Loss: 0.00001425
Iteration 59/1000 | Loss: 0.00001425
Iteration 60/1000 | Loss: 0.00001425
Iteration 61/1000 | Loss: 0.00001424
Iteration 62/1000 | Loss: 0.00001424
Iteration 63/1000 | Loss: 0.00001424
Iteration 64/1000 | Loss: 0.00001423
Iteration 65/1000 | Loss: 0.00001423
Iteration 66/1000 | Loss: 0.00001423
Iteration 67/1000 | Loss: 0.00001422
Iteration 68/1000 | Loss: 0.00001422
Iteration 69/1000 | Loss: 0.00001421
Iteration 70/1000 | Loss: 0.00001421
Iteration 71/1000 | Loss: 0.00001421
Iteration 72/1000 | Loss: 0.00001421
Iteration 73/1000 | Loss: 0.00001420
Iteration 74/1000 | Loss: 0.00001420
Iteration 75/1000 | Loss: 0.00001420
Iteration 76/1000 | Loss: 0.00001420
Iteration 77/1000 | Loss: 0.00001420
Iteration 78/1000 | Loss: 0.00001420
Iteration 79/1000 | Loss: 0.00001420
Iteration 80/1000 | Loss: 0.00001420
Iteration 81/1000 | Loss: 0.00001419
Iteration 82/1000 | Loss: 0.00001419
Iteration 83/1000 | Loss: 0.00001419
Iteration 84/1000 | Loss: 0.00001419
Iteration 85/1000 | Loss: 0.00001418
Iteration 86/1000 | Loss: 0.00001418
Iteration 87/1000 | Loss: 0.00001418
Iteration 88/1000 | Loss: 0.00001417
Iteration 89/1000 | Loss: 0.00001417
Iteration 90/1000 | Loss: 0.00001417
Iteration 91/1000 | Loss: 0.00001417
Iteration 92/1000 | Loss: 0.00001417
Iteration 93/1000 | Loss: 0.00001416
Iteration 94/1000 | Loss: 0.00001416
Iteration 95/1000 | Loss: 0.00001416
Iteration 96/1000 | Loss: 0.00001416
Iteration 97/1000 | Loss: 0.00001415
Iteration 98/1000 | Loss: 0.00001415
Iteration 99/1000 | Loss: 0.00001415
Iteration 100/1000 | Loss: 0.00001414
Iteration 101/1000 | Loss: 0.00001414
Iteration 102/1000 | Loss: 0.00001414
Iteration 103/1000 | Loss: 0.00001413
Iteration 104/1000 | Loss: 0.00001413
Iteration 105/1000 | Loss: 0.00001413
Iteration 106/1000 | Loss: 0.00001413
Iteration 107/1000 | Loss: 0.00001413
Iteration 108/1000 | Loss: 0.00001413
Iteration 109/1000 | Loss: 0.00001412
Iteration 110/1000 | Loss: 0.00001412
Iteration 111/1000 | Loss: 0.00001412
Iteration 112/1000 | Loss: 0.00001412
Iteration 113/1000 | Loss: 0.00001412
Iteration 114/1000 | Loss: 0.00001411
Iteration 115/1000 | Loss: 0.00001411
Iteration 116/1000 | Loss: 0.00001411
Iteration 117/1000 | Loss: 0.00001411
Iteration 118/1000 | Loss: 0.00001411
Iteration 119/1000 | Loss: 0.00001411
Iteration 120/1000 | Loss: 0.00001411
Iteration 121/1000 | Loss: 0.00001411
Iteration 122/1000 | Loss: 0.00001411
Iteration 123/1000 | Loss: 0.00001411
Iteration 124/1000 | Loss: 0.00001411
Iteration 125/1000 | Loss: 0.00001411
Iteration 126/1000 | Loss: 0.00001411
Iteration 127/1000 | Loss: 0.00001411
Iteration 128/1000 | Loss: 0.00001411
Iteration 129/1000 | Loss: 0.00001411
Iteration 130/1000 | Loss: 0.00001411
Iteration 131/1000 | Loss: 0.00001411
Iteration 132/1000 | Loss: 0.00001411
Iteration 133/1000 | Loss: 0.00001411
Iteration 134/1000 | Loss: 0.00001411
Iteration 135/1000 | Loss: 0.00001411
Iteration 136/1000 | Loss: 0.00001411
Iteration 137/1000 | Loss: 0.00001411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.4106242815614678e-05, 1.4106242815614678e-05, 1.4106242815614678e-05, 1.4106242815614678e-05, 1.4106242815614678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4106242815614678e-05

Optimization complete. Final v2v error: 3.210066795349121 mm

Highest mean error: 3.564924955368042 mm for frame 69

Lowest mean error: 2.8871445655822754 mm for frame 83

Saving results

Total time: 45.74971914291382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001436
Iteration 2/25 | Loss: 0.01001435
Iteration 3/25 | Loss: 0.00351113
Iteration 4/25 | Loss: 0.00261417
Iteration 5/25 | Loss: 0.00218566
Iteration 6/25 | Loss: 0.00204645
Iteration 7/25 | Loss: 0.00199230
Iteration 8/25 | Loss: 0.00191785
Iteration 9/25 | Loss: 0.00174210
Iteration 10/25 | Loss: 0.00161305
Iteration 11/25 | Loss: 0.00155780
Iteration 12/25 | Loss: 0.00154335
Iteration 13/25 | Loss: 0.00152649
Iteration 14/25 | Loss: 0.00151872
Iteration 15/25 | Loss: 0.00151656
Iteration 16/25 | Loss: 0.00151577
Iteration 17/25 | Loss: 0.00151548
Iteration 18/25 | Loss: 0.00151540
Iteration 19/25 | Loss: 0.00151720
Iteration 20/25 | Loss: 0.00151424
Iteration 21/25 | Loss: 0.00151300
Iteration 22/25 | Loss: 0.00151275
Iteration 23/25 | Loss: 0.00151268
Iteration 24/25 | Loss: 0.00151268
Iteration 25/25 | Loss: 0.00151268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37223709
Iteration 2/25 | Loss: 0.00288922
Iteration 3/25 | Loss: 0.00166003
Iteration 4/25 | Loss: 0.00152122
Iteration 5/25 | Loss: 0.00152122
Iteration 6/25 | Loss: 0.00152122
Iteration 7/25 | Loss: 0.00152122
Iteration 8/25 | Loss: 0.00152122
Iteration 9/25 | Loss: 0.00152122
Iteration 10/25 | Loss: 0.00152122
Iteration 11/25 | Loss: 0.00152122
Iteration 12/25 | Loss: 0.00152122
Iteration 13/25 | Loss: 0.00152122
Iteration 14/25 | Loss: 0.00152122
Iteration 15/25 | Loss: 0.00152122
Iteration 16/25 | Loss: 0.00152122
Iteration 17/25 | Loss: 0.00152122
Iteration 18/25 | Loss: 0.00152122
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015212154248729348, 0.0015212154248729348, 0.0015212154248729348, 0.0015212154248729348, 0.0015212154248729348]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015212154248729348

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152122
Iteration 2/1000 | Loss: 0.00151612
Iteration 3/1000 | Loss: 0.00320108
Iteration 4/1000 | Loss: 0.00011913
Iteration 5/1000 | Loss: 0.00052380
Iteration 6/1000 | Loss: 0.00045473
Iteration 7/1000 | Loss: 0.00327749
Iteration 8/1000 | Loss: 0.00187899
Iteration 9/1000 | Loss: 0.00035035
Iteration 10/1000 | Loss: 0.00149711
Iteration 11/1000 | Loss: 0.00106072
Iteration 12/1000 | Loss: 0.00254087
Iteration 13/1000 | Loss: 0.00011476
Iteration 14/1000 | Loss: 0.00021950
Iteration 15/1000 | Loss: 0.00029458
Iteration 16/1000 | Loss: 0.00029284
Iteration 17/1000 | Loss: 0.00046374
Iteration 18/1000 | Loss: 0.00090218
Iteration 19/1000 | Loss: 0.00012854
Iteration 20/1000 | Loss: 0.00009284
Iteration 21/1000 | Loss: 0.00008662
Iteration 22/1000 | Loss: 0.00008504
Iteration 23/1000 | Loss: 0.00053918
Iteration 24/1000 | Loss: 0.00008358
Iteration 25/1000 | Loss: 0.00008259
Iteration 26/1000 | Loss: 0.00008184
Iteration 27/1000 | Loss: 0.00008116
Iteration 28/1000 | Loss: 0.00039145
Iteration 29/1000 | Loss: 0.00048567
Iteration 30/1000 | Loss: 0.00079673
Iteration 31/1000 | Loss: 0.00095647
Iteration 32/1000 | Loss: 0.00229749
Iteration 33/1000 | Loss: 0.00320780
Iteration 34/1000 | Loss: 0.00276750
Iteration 35/1000 | Loss: 0.00203858
Iteration 36/1000 | Loss: 0.00012701
Iteration 37/1000 | Loss: 0.00010688
Iteration 38/1000 | Loss: 0.00038213
Iteration 39/1000 | Loss: 0.00110215
Iteration 40/1000 | Loss: 0.00128959
Iteration 41/1000 | Loss: 0.00084450
Iteration 42/1000 | Loss: 0.00011821
Iteration 43/1000 | Loss: 0.00010569
Iteration 44/1000 | Loss: 0.00019640
Iteration 45/1000 | Loss: 0.00215656
Iteration 46/1000 | Loss: 0.00016022
Iteration 47/1000 | Loss: 0.00009416
Iteration 48/1000 | Loss: 0.00095882
Iteration 49/1000 | Loss: 0.00032737
Iteration 50/1000 | Loss: 0.00182485
Iteration 51/1000 | Loss: 0.00038349
Iteration 52/1000 | Loss: 0.00072753
Iteration 53/1000 | Loss: 0.00096953
Iteration 54/1000 | Loss: 0.00010053
Iteration 55/1000 | Loss: 0.00049378
Iteration 56/1000 | Loss: 0.00181792
Iteration 57/1000 | Loss: 0.00228201
Iteration 58/1000 | Loss: 0.00386681
Iteration 59/1000 | Loss: 0.00008965
Iteration 60/1000 | Loss: 0.00019107
Iteration 61/1000 | Loss: 0.00008394
Iteration 62/1000 | Loss: 0.00008195
Iteration 63/1000 | Loss: 0.00008107
Iteration 64/1000 | Loss: 0.00008035
Iteration 65/1000 | Loss: 0.00021687
Iteration 66/1000 | Loss: 0.00023937
Iteration 67/1000 | Loss: 0.00017563
Iteration 68/1000 | Loss: 0.00009375
Iteration 69/1000 | Loss: 0.00007975
Iteration 70/1000 | Loss: 0.00032932
Iteration 71/1000 | Loss: 0.00008648
Iteration 72/1000 | Loss: 0.00040716
Iteration 73/1000 | Loss: 0.00175617
Iteration 74/1000 | Loss: 0.00400558
Iteration 75/1000 | Loss: 0.00068592
Iteration 76/1000 | Loss: 0.00012520
Iteration 77/1000 | Loss: 0.00012363
Iteration 78/1000 | Loss: 0.00007883
Iteration 79/1000 | Loss: 0.00110303
Iteration 80/1000 | Loss: 0.00041766
Iteration 81/1000 | Loss: 0.00057329
Iteration 82/1000 | Loss: 0.00009813
Iteration 83/1000 | Loss: 0.00007820
Iteration 84/1000 | Loss: 0.00007778
Iteration 85/1000 | Loss: 0.00044101
Iteration 86/1000 | Loss: 0.00008323
Iteration 87/1000 | Loss: 0.00007765
Iteration 88/1000 | Loss: 0.00032980
Iteration 89/1000 | Loss: 0.00009216
Iteration 90/1000 | Loss: 0.00011391
Iteration 91/1000 | Loss: 0.00013869
Iteration 92/1000 | Loss: 0.00007707
Iteration 93/1000 | Loss: 0.00007644
Iteration 94/1000 | Loss: 0.00020750
Iteration 95/1000 | Loss: 0.00016834
Iteration 96/1000 | Loss: 0.00012271
Iteration 97/1000 | Loss: 0.00010339
Iteration 98/1000 | Loss: 0.00011264
Iteration 99/1000 | Loss: 0.00007689
Iteration 100/1000 | Loss: 0.00007592
Iteration 101/1000 | Loss: 0.00014588
Iteration 102/1000 | Loss: 0.00007535
Iteration 103/1000 | Loss: 0.00007501
Iteration 104/1000 | Loss: 0.00007471
Iteration 105/1000 | Loss: 0.00007445
Iteration 106/1000 | Loss: 0.00007445
Iteration 107/1000 | Loss: 0.00007425
Iteration 108/1000 | Loss: 0.00007416
Iteration 109/1000 | Loss: 0.00007412
Iteration 110/1000 | Loss: 0.00007405
Iteration 111/1000 | Loss: 0.00007403
Iteration 112/1000 | Loss: 0.00007399
Iteration 113/1000 | Loss: 0.00007399
Iteration 114/1000 | Loss: 0.00007398
Iteration 115/1000 | Loss: 0.00007396
Iteration 116/1000 | Loss: 0.00007382
Iteration 117/1000 | Loss: 0.00007365
Iteration 118/1000 | Loss: 0.00007363
Iteration 119/1000 | Loss: 0.00007362
Iteration 120/1000 | Loss: 0.00007360
Iteration 121/1000 | Loss: 0.00007360
Iteration 122/1000 | Loss: 0.00007357
Iteration 123/1000 | Loss: 0.00007357
Iteration 124/1000 | Loss: 0.00007356
Iteration 125/1000 | Loss: 0.00007356
Iteration 126/1000 | Loss: 0.00007356
Iteration 127/1000 | Loss: 0.00007356
Iteration 128/1000 | Loss: 0.00007356
Iteration 129/1000 | Loss: 0.00007356
Iteration 130/1000 | Loss: 0.00007356
Iteration 131/1000 | Loss: 0.00007355
Iteration 132/1000 | Loss: 0.00007355
Iteration 133/1000 | Loss: 0.00007355
Iteration 134/1000 | Loss: 0.00007355
Iteration 135/1000 | Loss: 0.00007355
Iteration 136/1000 | Loss: 0.00007355
Iteration 137/1000 | Loss: 0.00007352
Iteration 138/1000 | Loss: 0.00007351
Iteration 139/1000 | Loss: 0.00007349
Iteration 140/1000 | Loss: 0.00007346
Iteration 141/1000 | Loss: 0.00007346
Iteration 142/1000 | Loss: 0.00007346
Iteration 143/1000 | Loss: 0.00007346
Iteration 144/1000 | Loss: 0.00007346
Iteration 145/1000 | Loss: 0.00007346
Iteration 146/1000 | Loss: 0.00007346
Iteration 147/1000 | Loss: 0.00007345
Iteration 148/1000 | Loss: 0.00007345
Iteration 149/1000 | Loss: 0.00007345
Iteration 150/1000 | Loss: 0.00007345
Iteration 151/1000 | Loss: 0.00007345
Iteration 152/1000 | Loss: 0.00007345
Iteration 153/1000 | Loss: 0.00007345
Iteration 154/1000 | Loss: 0.00007344
Iteration 155/1000 | Loss: 0.00007344
Iteration 156/1000 | Loss: 0.00007344
Iteration 157/1000 | Loss: 0.00007344
Iteration 158/1000 | Loss: 0.00007341
Iteration 159/1000 | Loss: 0.00007340
Iteration 160/1000 | Loss: 0.00007340
Iteration 161/1000 | Loss: 0.00007338
Iteration 162/1000 | Loss: 0.00007335
Iteration 163/1000 | Loss: 0.00007335
Iteration 164/1000 | Loss: 0.00007335
Iteration 165/1000 | Loss: 0.00007335
Iteration 166/1000 | Loss: 0.00007334
Iteration 167/1000 | Loss: 0.00007334
Iteration 168/1000 | Loss: 0.00007334
Iteration 169/1000 | Loss: 0.00007334
Iteration 170/1000 | Loss: 0.00007334
Iteration 171/1000 | Loss: 0.00007334
Iteration 172/1000 | Loss: 0.00007334
Iteration 173/1000 | Loss: 0.00007331
Iteration 174/1000 | Loss: 0.00007331
Iteration 175/1000 | Loss: 0.00007330
Iteration 176/1000 | Loss: 0.00007329
Iteration 177/1000 | Loss: 0.00007328
Iteration 178/1000 | Loss: 0.00007328
Iteration 179/1000 | Loss: 0.00007326
Iteration 180/1000 | Loss: 0.00007326
Iteration 181/1000 | Loss: 0.00007326
Iteration 182/1000 | Loss: 0.00007326
Iteration 183/1000 | Loss: 0.00007326
Iteration 184/1000 | Loss: 0.00007326
Iteration 185/1000 | Loss: 0.00007326
Iteration 186/1000 | Loss: 0.00007326
Iteration 187/1000 | Loss: 0.00007325
Iteration 188/1000 | Loss: 0.00007325
Iteration 189/1000 | Loss: 0.00007325
Iteration 190/1000 | Loss: 0.00007325
Iteration 191/1000 | Loss: 0.00007325
Iteration 192/1000 | Loss: 0.00007325
Iteration 193/1000 | Loss: 0.00007325
Iteration 194/1000 | Loss: 0.00007325
Iteration 195/1000 | Loss: 0.00007324
Iteration 196/1000 | Loss: 0.00007323
Iteration 197/1000 | Loss: 0.00007323
Iteration 198/1000 | Loss: 0.00007323
Iteration 199/1000 | Loss: 0.00007323
Iteration 200/1000 | Loss: 0.00007323
Iteration 201/1000 | Loss: 0.00007323
Iteration 202/1000 | Loss: 0.00007323
Iteration 203/1000 | Loss: 0.00007323
Iteration 204/1000 | Loss: 0.00007323
Iteration 205/1000 | Loss: 0.00007322
Iteration 206/1000 | Loss: 0.00007322
Iteration 207/1000 | Loss: 0.00007322
Iteration 208/1000 | Loss: 0.00007322
Iteration 209/1000 | Loss: 0.00007322
Iteration 210/1000 | Loss: 0.00007320
Iteration 211/1000 | Loss: 0.00007320
Iteration 212/1000 | Loss: 0.00007319
Iteration 213/1000 | Loss: 0.00007319
Iteration 214/1000 | Loss: 0.00007319
Iteration 215/1000 | Loss: 0.00007318
Iteration 216/1000 | Loss: 0.00007318
Iteration 217/1000 | Loss: 0.00007318
Iteration 218/1000 | Loss: 0.00007318
Iteration 219/1000 | Loss: 0.00007318
Iteration 220/1000 | Loss: 0.00007318
Iteration 221/1000 | Loss: 0.00007318
Iteration 222/1000 | Loss: 0.00007318
Iteration 223/1000 | Loss: 0.00007318
Iteration 224/1000 | Loss: 0.00007318
Iteration 225/1000 | Loss: 0.00007317
Iteration 226/1000 | Loss: 0.00007317
Iteration 227/1000 | Loss: 0.00007317
Iteration 228/1000 | Loss: 0.00007316
Iteration 229/1000 | Loss: 0.00007316
Iteration 230/1000 | Loss: 0.00007315
Iteration 231/1000 | Loss: 0.00007315
Iteration 232/1000 | Loss: 0.00007315
Iteration 233/1000 | Loss: 0.00007315
Iteration 234/1000 | Loss: 0.00007314
Iteration 235/1000 | Loss: 0.00007314
Iteration 236/1000 | Loss: 0.00007314
Iteration 237/1000 | Loss: 0.00007313
Iteration 238/1000 | Loss: 0.00007313
Iteration 239/1000 | Loss: 0.00007313
Iteration 240/1000 | Loss: 0.00007313
Iteration 241/1000 | Loss: 0.00007313
Iteration 242/1000 | Loss: 0.00007313
Iteration 243/1000 | Loss: 0.00007313
Iteration 244/1000 | Loss: 0.00007313
Iteration 245/1000 | Loss: 0.00007313
Iteration 246/1000 | Loss: 0.00007313
Iteration 247/1000 | Loss: 0.00007313
Iteration 248/1000 | Loss: 0.00007313
Iteration 249/1000 | Loss: 0.00007313
Iteration 250/1000 | Loss: 0.00007312
Iteration 251/1000 | Loss: 0.00007312
Iteration 252/1000 | Loss: 0.00007312
Iteration 253/1000 | Loss: 0.00007312
Iteration 254/1000 | Loss: 0.00007312
Iteration 255/1000 | Loss: 0.00007312
Iteration 256/1000 | Loss: 0.00007312
Iteration 257/1000 | Loss: 0.00007312
Iteration 258/1000 | Loss: 0.00007311
Iteration 259/1000 | Loss: 0.00007311
Iteration 260/1000 | Loss: 0.00007311
Iteration 261/1000 | Loss: 0.00007311
Iteration 262/1000 | Loss: 0.00007311
Iteration 263/1000 | Loss: 0.00007311
Iteration 264/1000 | Loss: 0.00007311
Iteration 265/1000 | Loss: 0.00007311
Iteration 266/1000 | Loss: 0.00007311
Iteration 267/1000 | Loss: 0.00007311
Iteration 268/1000 | Loss: 0.00007311
Iteration 269/1000 | Loss: 0.00007310
Iteration 270/1000 | Loss: 0.00007310
Iteration 271/1000 | Loss: 0.00007310
Iteration 272/1000 | Loss: 0.00007310
Iteration 273/1000 | Loss: 0.00007310
Iteration 274/1000 | Loss: 0.00007309
Iteration 275/1000 | Loss: 0.00007309
Iteration 276/1000 | Loss: 0.00007309
Iteration 277/1000 | Loss: 0.00007309
Iteration 278/1000 | Loss: 0.00007309
Iteration 279/1000 | Loss: 0.00007309
Iteration 280/1000 | Loss: 0.00007309
Iteration 281/1000 | Loss: 0.00007309
Iteration 282/1000 | Loss: 0.00007309
Iteration 283/1000 | Loss: 0.00007309
Iteration 284/1000 | Loss: 0.00007309
Iteration 285/1000 | Loss: 0.00007309
Iteration 286/1000 | Loss: 0.00007308
Iteration 287/1000 | Loss: 0.00007308
Iteration 288/1000 | Loss: 0.00007308
Iteration 289/1000 | Loss: 0.00007308
Iteration 290/1000 | Loss: 0.00007308
Iteration 291/1000 | Loss: 0.00007308
Iteration 292/1000 | Loss: 0.00007308
Iteration 293/1000 | Loss: 0.00007308
Iteration 294/1000 | Loss: 0.00007308
Iteration 295/1000 | Loss: 0.00007308
Iteration 296/1000 | Loss: 0.00007308
Iteration 297/1000 | Loss: 0.00007308
Iteration 298/1000 | Loss: 0.00007308
Iteration 299/1000 | Loss: 0.00007308
Iteration 300/1000 | Loss: 0.00007308
Iteration 301/1000 | Loss: 0.00007307
Iteration 302/1000 | Loss: 0.00007307
Iteration 303/1000 | Loss: 0.00007307
Iteration 304/1000 | Loss: 0.00007307
Iteration 305/1000 | Loss: 0.00007307
Iteration 306/1000 | Loss: 0.00007307
Iteration 307/1000 | Loss: 0.00007307
Iteration 308/1000 | Loss: 0.00007307
Iteration 309/1000 | Loss: 0.00007307
Iteration 310/1000 | Loss: 0.00007307
Iteration 311/1000 | Loss: 0.00007307
Iteration 312/1000 | Loss: 0.00007307
Iteration 313/1000 | Loss: 0.00007307
Iteration 314/1000 | Loss: 0.00007307
Iteration 315/1000 | Loss: 0.00007307
Iteration 316/1000 | Loss: 0.00007307
Iteration 317/1000 | Loss: 0.00007307
Iteration 318/1000 | Loss: 0.00007306
Iteration 319/1000 | Loss: 0.00007306
Iteration 320/1000 | Loss: 0.00007306
Iteration 321/1000 | Loss: 0.00007306
Iteration 322/1000 | Loss: 0.00007306
Iteration 323/1000 | Loss: 0.00007306
Iteration 324/1000 | Loss: 0.00007306
Iteration 325/1000 | Loss: 0.00007306
Iteration 326/1000 | Loss: 0.00007306
Iteration 327/1000 | Loss: 0.00007306
Iteration 328/1000 | Loss: 0.00007306
Iteration 329/1000 | Loss: 0.00007306
Iteration 330/1000 | Loss: 0.00007306
Iteration 331/1000 | Loss: 0.00007306
Iteration 332/1000 | Loss: 0.00007306
Iteration 333/1000 | Loss: 0.00007306
Iteration 334/1000 | Loss: 0.00007306
Iteration 335/1000 | Loss: 0.00007306
Iteration 336/1000 | Loss: 0.00007306
Iteration 337/1000 | Loss: 0.00007306
Iteration 338/1000 | Loss: 0.00007306
Iteration 339/1000 | Loss: 0.00007306
Iteration 340/1000 | Loss: 0.00007306
Iteration 341/1000 | Loss: 0.00007306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 341. Stopping optimization.
Last 5 losses: [7.30556930648163e-05, 7.30556930648163e-05, 7.30556930648163e-05, 7.30556930648163e-05, 7.30556930648163e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.30556930648163e-05

Optimization complete. Final v2v error: 5.40674352645874 mm

Highest mean error: 11.44576358795166 mm for frame 54

Lowest mean error: 3.683934450149536 mm for frame 109

Saving results

Total time: 211.33626127243042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906858
Iteration 2/25 | Loss: 0.00158784
Iteration 3/25 | Loss: 0.00138734
Iteration 4/25 | Loss: 0.00136750
Iteration 5/25 | Loss: 0.00136372
Iteration 6/25 | Loss: 0.00136273
Iteration 7/25 | Loss: 0.00136273
Iteration 8/25 | Loss: 0.00136273
Iteration 9/25 | Loss: 0.00136273
Iteration 10/25 | Loss: 0.00136273
Iteration 11/25 | Loss: 0.00136273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013627290027216077, 0.0013627290027216077, 0.0013627290027216077, 0.0013627290027216077, 0.0013627290027216077]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013627290027216077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55911440
Iteration 2/25 | Loss: 0.00102022
Iteration 3/25 | Loss: 0.00102021
Iteration 4/25 | Loss: 0.00102021
Iteration 5/25 | Loss: 0.00102021
Iteration 6/25 | Loss: 0.00102021
Iteration 7/25 | Loss: 0.00102021
Iteration 8/25 | Loss: 0.00102021
Iteration 9/25 | Loss: 0.00102021
Iteration 10/25 | Loss: 0.00102021
Iteration 11/25 | Loss: 0.00102021
Iteration 12/25 | Loss: 0.00102021
Iteration 13/25 | Loss: 0.00102021
Iteration 14/25 | Loss: 0.00102021
Iteration 15/25 | Loss: 0.00102021
Iteration 16/25 | Loss: 0.00102021
Iteration 17/25 | Loss: 0.00102021
Iteration 18/25 | Loss: 0.00102021
Iteration 19/25 | Loss: 0.00102021
Iteration 20/25 | Loss: 0.00102021
Iteration 21/25 | Loss: 0.00102021
Iteration 22/25 | Loss: 0.00102021
Iteration 23/25 | Loss: 0.00102021
Iteration 24/25 | Loss: 0.00102021
Iteration 25/25 | Loss: 0.00102021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102021
Iteration 2/1000 | Loss: 0.00006238
Iteration 3/1000 | Loss: 0.00003769
Iteration 4/1000 | Loss: 0.00002959
Iteration 5/1000 | Loss: 0.00002767
Iteration 6/1000 | Loss: 0.00002655
Iteration 7/1000 | Loss: 0.00002597
Iteration 8/1000 | Loss: 0.00002546
Iteration 9/1000 | Loss: 0.00002497
Iteration 10/1000 | Loss: 0.00002470
Iteration 11/1000 | Loss: 0.00002453
Iteration 12/1000 | Loss: 0.00002436
Iteration 13/1000 | Loss: 0.00002432
Iteration 14/1000 | Loss: 0.00002415
Iteration 15/1000 | Loss: 0.00002403
Iteration 16/1000 | Loss: 0.00002394
Iteration 17/1000 | Loss: 0.00002384
Iteration 18/1000 | Loss: 0.00002376
Iteration 19/1000 | Loss: 0.00002376
Iteration 20/1000 | Loss: 0.00002375
Iteration 21/1000 | Loss: 0.00002374
Iteration 22/1000 | Loss: 0.00002374
Iteration 23/1000 | Loss: 0.00002373
Iteration 24/1000 | Loss: 0.00002373
Iteration 25/1000 | Loss: 0.00002372
Iteration 26/1000 | Loss: 0.00002372
Iteration 27/1000 | Loss: 0.00002371
Iteration 28/1000 | Loss: 0.00002371
Iteration 29/1000 | Loss: 0.00002371
Iteration 30/1000 | Loss: 0.00002370
Iteration 31/1000 | Loss: 0.00002369
Iteration 32/1000 | Loss: 0.00002369
Iteration 33/1000 | Loss: 0.00002369
Iteration 34/1000 | Loss: 0.00002368
Iteration 35/1000 | Loss: 0.00002368
Iteration 36/1000 | Loss: 0.00002368
Iteration 37/1000 | Loss: 0.00002367
Iteration 38/1000 | Loss: 0.00002367
Iteration 39/1000 | Loss: 0.00002367
Iteration 40/1000 | Loss: 0.00002366
Iteration 41/1000 | Loss: 0.00002366
Iteration 42/1000 | Loss: 0.00002366
Iteration 43/1000 | Loss: 0.00002366
Iteration 44/1000 | Loss: 0.00002366
Iteration 45/1000 | Loss: 0.00002366
Iteration 46/1000 | Loss: 0.00002365
Iteration 47/1000 | Loss: 0.00002365
Iteration 48/1000 | Loss: 0.00002365
Iteration 49/1000 | Loss: 0.00002365
Iteration 50/1000 | Loss: 0.00002365
Iteration 51/1000 | Loss: 0.00002365
Iteration 52/1000 | Loss: 0.00002365
Iteration 53/1000 | Loss: 0.00002365
Iteration 54/1000 | Loss: 0.00002365
Iteration 55/1000 | Loss: 0.00002365
Iteration 56/1000 | Loss: 0.00002365
Iteration 57/1000 | Loss: 0.00002365
Iteration 58/1000 | Loss: 0.00002364
Iteration 59/1000 | Loss: 0.00002364
Iteration 60/1000 | Loss: 0.00002364
Iteration 61/1000 | Loss: 0.00002363
Iteration 62/1000 | Loss: 0.00002363
Iteration 63/1000 | Loss: 0.00002363
Iteration 64/1000 | Loss: 0.00002363
Iteration 65/1000 | Loss: 0.00002363
Iteration 66/1000 | Loss: 0.00002363
Iteration 67/1000 | Loss: 0.00002363
Iteration 68/1000 | Loss: 0.00002363
Iteration 69/1000 | Loss: 0.00002363
Iteration 70/1000 | Loss: 0.00002363
Iteration 71/1000 | Loss: 0.00002363
Iteration 72/1000 | Loss: 0.00002363
Iteration 73/1000 | Loss: 0.00002362
Iteration 74/1000 | Loss: 0.00002362
Iteration 75/1000 | Loss: 0.00002362
Iteration 76/1000 | Loss: 0.00002362
Iteration 77/1000 | Loss: 0.00002362
Iteration 78/1000 | Loss: 0.00002361
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002361
Iteration 81/1000 | Loss: 0.00002361
Iteration 82/1000 | Loss: 0.00002361
Iteration 83/1000 | Loss: 0.00002360
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002360
Iteration 90/1000 | Loss: 0.00002359
Iteration 91/1000 | Loss: 0.00002359
Iteration 92/1000 | Loss: 0.00002359
Iteration 93/1000 | Loss: 0.00002359
Iteration 94/1000 | Loss: 0.00002359
Iteration 95/1000 | Loss: 0.00002358
Iteration 96/1000 | Loss: 0.00002358
Iteration 97/1000 | Loss: 0.00002358
Iteration 98/1000 | Loss: 0.00002358
Iteration 99/1000 | Loss: 0.00002358
Iteration 100/1000 | Loss: 0.00002358
Iteration 101/1000 | Loss: 0.00002357
Iteration 102/1000 | Loss: 0.00002357
Iteration 103/1000 | Loss: 0.00002357
Iteration 104/1000 | Loss: 0.00002357
Iteration 105/1000 | Loss: 0.00002357
Iteration 106/1000 | Loss: 0.00002357
Iteration 107/1000 | Loss: 0.00002357
Iteration 108/1000 | Loss: 0.00002357
Iteration 109/1000 | Loss: 0.00002357
Iteration 110/1000 | Loss: 0.00002357
Iteration 111/1000 | Loss: 0.00002357
Iteration 112/1000 | Loss: 0.00002357
Iteration 113/1000 | Loss: 0.00002356
Iteration 114/1000 | Loss: 0.00002356
Iteration 115/1000 | Loss: 0.00002356
Iteration 116/1000 | Loss: 0.00002356
Iteration 117/1000 | Loss: 0.00002356
Iteration 118/1000 | Loss: 0.00002356
Iteration 119/1000 | Loss: 0.00002356
Iteration 120/1000 | Loss: 0.00002356
Iteration 121/1000 | Loss: 0.00002356
Iteration 122/1000 | Loss: 0.00002356
Iteration 123/1000 | Loss: 0.00002356
Iteration 124/1000 | Loss: 0.00002356
Iteration 125/1000 | Loss: 0.00002356
Iteration 126/1000 | Loss: 0.00002356
Iteration 127/1000 | Loss: 0.00002356
Iteration 128/1000 | Loss: 0.00002356
Iteration 129/1000 | Loss: 0.00002355
Iteration 130/1000 | Loss: 0.00002355
Iteration 131/1000 | Loss: 0.00002355
Iteration 132/1000 | Loss: 0.00002355
Iteration 133/1000 | Loss: 0.00002355
Iteration 134/1000 | Loss: 0.00002355
Iteration 135/1000 | Loss: 0.00002355
Iteration 136/1000 | Loss: 0.00002355
Iteration 137/1000 | Loss: 0.00002355
Iteration 138/1000 | Loss: 0.00002355
Iteration 139/1000 | Loss: 0.00002355
Iteration 140/1000 | Loss: 0.00002355
Iteration 141/1000 | Loss: 0.00002355
Iteration 142/1000 | Loss: 0.00002355
Iteration 143/1000 | Loss: 0.00002355
Iteration 144/1000 | Loss: 0.00002354
Iteration 145/1000 | Loss: 0.00002354
Iteration 146/1000 | Loss: 0.00002354
Iteration 147/1000 | Loss: 0.00002354
Iteration 148/1000 | Loss: 0.00002354
Iteration 149/1000 | Loss: 0.00002354
Iteration 150/1000 | Loss: 0.00002354
Iteration 151/1000 | Loss: 0.00002354
Iteration 152/1000 | Loss: 0.00002354
Iteration 153/1000 | Loss: 0.00002354
Iteration 154/1000 | Loss: 0.00002354
Iteration 155/1000 | Loss: 0.00002354
Iteration 156/1000 | Loss: 0.00002354
Iteration 157/1000 | Loss: 0.00002354
Iteration 158/1000 | Loss: 0.00002354
Iteration 159/1000 | Loss: 0.00002354
Iteration 160/1000 | Loss: 0.00002354
Iteration 161/1000 | Loss: 0.00002354
Iteration 162/1000 | Loss: 0.00002354
Iteration 163/1000 | Loss: 0.00002354
Iteration 164/1000 | Loss: 0.00002354
Iteration 165/1000 | Loss: 0.00002354
Iteration 166/1000 | Loss: 0.00002354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.3539596440969035e-05, 2.3539596440969035e-05, 2.3539596440969035e-05, 2.3539596440969035e-05, 2.3539596440969035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3539596440969035e-05

Optimization complete. Final v2v error: 4.08866548538208 mm

Highest mean error: 4.56451416015625 mm for frame 20

Lowest mean error: 3.682744026184082 mm for frame 42

Saving results

Total time: 41.25883650779724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077497
Iteration 2/25 | Loss: 0.01077497
Iteration 3/25 | Loss: 0.01077497
Iteration 4/25 | Loss: 0.00323776
Iteration 5/25 | Loss: 0.00181667
Iteration 6/25 | Loss: 0.00171499
Iteration 7/25 | Loss: 0.00204536
Iteration 8/25 | Loss: 0.00189813
Iteration 9/25 | Loss: 0.00161868
Iteration 10/25 | Loss: 0.00137620
Iteration 11/25 | Loss: 0.00138605
Iteration 12/25 | Loss: 0.00130431
Iteration 13/25 | Loss: 0.00128897
Iteration 14/25 | Loss: 0.00127523
Iteration 15/25 | Loss: 0.00127146
Iteration 16/25 | Loss: 0.00126839
Iteration 17/25 | Loss: 0.00126506
Iteration 18/25 | Loss: 0.00126397
Iteration 19/25 | Loss: 0.00126455
Iteration 20/25 | Loss: 0.00126363
Iteration 21/25 | Loss: 0.00126474
Iteration 22/25 | Loss: 0.00126252
Iteration 23/25 | Loss: 0.00126230
Iteration 24/25 | Loss: 0.00126221
Iteration 25/25 | Loss: 0.00126221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.54221237
Iteration 2/25 | Loss: 0.00076325
Iteration 3/25 | Loss: 0.00067084
Iteration 4/25 | Loss: 0.00067084
Iteration 5/25 | Loss: 0.00067084
Iteration 6/25 | Loss: 0.00067084
Iteration 7/25 | Loss: 0.00067084
Iteration 8/25 | Loss: 0.00067084
Iteration 9/25 | Loss: 0.00067084
Iteration 10/25 | Loss: 0.00067084
Iteration 11/25 | Loss: 0.00067084
Iteration 12/25 | Loss: 0.00067084
Iteration 13/25 | Loss: 0.00067084
Iteration 14/25 | Loss: 0.00067084
Iteration 15/25 | Loss: 0.00067084
Iteration 16/25 | Loss: 0.00067084
Iteration 17/25 | Loss: 0.00067084
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006708373548462987, 0.0006708373548462987, 0.0006708373548462987, 0.0006708373548462987, 0.0006708373548462987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006708373548462987

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067084
Iteration 2/1000 | Loss: 0.00021224
Iteration 3/1000 | Loss: 0.00005601
Iteration 4/1000 | Loss: 0.00025423
Iteration 5/1000 | Loss: 0.00003219
Iteration 6/1000 | Loss: 0.00009772
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001842
Iteration 9/1000 | Loss: 0.00002920
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00001738
Iteration 12/1000 | Loss: 0.00001737
Iteration 13/1000 | Loss: 0.00001736
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001736
Iteration 16/1000 | Loss: 0.00001734
Iteration 17/1000 | Loss: 0.00001748
Iteration 18/1000 | Loss: 0.00001945
Iteration 19/1000 | Loss: 0.00001696
Iteration 20/1000 | Loss: 0.00001843
Iteration 21/1000 | Loss: 0.00001759
Iteration 22/1000 | Loss: 0.00001806
Iteration 23/1000 | Loss: 0.00001805
Iteration 24/1000 | Loss: 0.00001805
Iteration 25/1000 | Loss: 0.00001805
Iteration 26/1000 | Loss: 0.00002021
Iteration 27/1000 | Loss: 0.00001859
Iteration 28/1000 | Loss: 0.00001717
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00001919
Iteration 31/1000 | Loss: 0.00001774
Iteration 32/1000 | Loss: 0.00001684
Iteration 33/1000 | Loss: 0.00001670
Iteration 34/1000 | Loss: 0.00001689
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 40. Stopping optimization.
Last 5 losses: [1.6411429896834306e-05, 1.6411429896834306e-05, 1.6411429896834306e-05, 1.6411429896834306e-05, 1.6411429896834306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6411429896834306e-05

Optimization complete. Final v2v error: 3.4699926376342773 mm

Highest mean error: 3.727341651916504 mm for frame 103

Lowest mean error: 3.3001551628112793 mm for frame 172

Saving results

Total time: 82.0630271434784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416189
Iteration 2/25 | Loss: 0.00128159
Iteration 3/25 | Loss: 0.00118960
Iteration 4/25 | Loss: 0.00117343
Iteration 5/25 | Loss: 0.00116823
Iteration 6/25 | Loss: 0.00116752
Iteration 7/25 | Loss: 0.00116752
Iteration 8/25 | Loss: 0.00116752
Iteration 9/25 | Loss: 0.00116752
Iteration 10/25 | Loss: 0.00116752
Iteration 11/25 | Loss: 0.00116752
Iteration 12/25 | Loss: 0.00116752
Iteration 13/25 | Loss: 0.00116752
Iteration 14/25 | Loss: 0.00116752
Iteration 15/25 | Loss: 0.00116752
Iteration 16/25 | Loss: 0.00116752
Iteration 17/25 | Loss: 0.00116752
Iteration 18/25 | Loss: 0.00116752
Iteration 19/25 | Loss: 0.00116752
Iteration 20/25 | Loss: 0.00116752
Iteration 21/25 | Loss: 0.00116752
Iteration 22/25 | Loss: 0.00116752
Iteration 23/25 | Loss: 0.00116752
Iteration 24/25 | Loss: 0.00116752
Iteration 25/25 | Loss: 0.00116752

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40301192
Iteration 2/25 | Loss: 0.00084538
Iteration 3/25 | Loss: 0.00084538
Iteration 4/25 | Loss: 0.00084538
Iteration 5/25 | Loss: 0.00084538
Iteration 6/25 | Loss: 0.00084538
Iteration 7/25 | Loss: 0.00084538
Iteration 8/25 | Loss: 0.00084538
Iteration 9/25 | Loss: 0.00084538
Iteration 10/25 | Loss: 0.00084538
Iteration 11/25 | Loss: 0.00084538
Iteration 12/25 | Loss: 0.00084538
Iteration 13/25 | Loss: 0.00084538
Iteration 14/25 | Loss: 0.00084538
Iteration 15/25 | Loss: 0.00084538
Iteration 16/25 | Loss: 0.00084538
Iteration 17/25 | Loss: 0.00084538
Iteration 18/25 | Loss: 0.00084538
Iteration 19/25 | Loss: 0.00084538
Iteration 20/25 | Loss: 0.00084538
Iteration 21/25 | Loss: 0.00084538
Iteration 22/25 | Loss: 0.00084538
Iteration 23/25 | Loss: 0.00084538
Iteration 24/25 | Loss: 0.00084538
Iteration 25/25 | Loss: 0.00084538

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084538
Iteration 2/1000 | Loss: 0.00003218
Iteration 3/1000 | Loss: 0.00002353
Iteration 4/1000 | Loss: 0.00002027
Iteration 5/1000 | Loss: 0.00001920
Iteration 6/1000 | Loss: 0.00001856
Iteration 7/1000 | Loss: 0.00001804
Iteration 8/1000 | Loss: 0.00001757
Iteration 9/1000 | Loss: 0.00001727
Iteration 10/1000 | Loss: 0.00001701
Iteration 11/1000 | Loss: 0.00001673
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001640
Iteration 14/1000 | Loss: 0.00001635
Iteration 15/1000 | Loss: 0.00001633
Iteration 16/1000 | Loss: 0.00001631
Iteration 17/1000 | Loss: 0.00001627
Iteration 18/1000 | Loss: 0.00001626
Iteration 19/1000 | Loss: 0.00001624
Iteration 20/1000 | Loss: 0.00001624
Iteration 21/1000 | Loss: 0.00001623
Iteration 22/1000 | Loss: 0.00001623
Iteration 23/1000 | Loss: 0.00001622
Iteration 24/1000 | Loss: 0.00001613
Iteration 25/1000 | Loss: 0.00001613
Iteration 26/1000 | Loss: 0.00001613
Iteration 27/1000 | Loss: 0.00001613
Iteration 28/1000 | Loss: 0.00001608
Iteration 29/1000 | Loss: 0.00001608
Iteration 30/1000 | Loss: 0.00001605
Iteration 31/1000 | Loss: 0.00001604
Iteration 32/1000 | Loss: 0.00001604
Iteration 33/1000 | Loss: 0.00001603
Iteration 34/1000 | Loss: 0.00001601
Iteration 35/1000 | Loss: 0.00001599
Iteration 36/1000 | Loss: 0.00001598
Iteration 37/1000 | Loss: 0.00001598
Iteration 38/1000 | Loss: 0.00001597
Iteration 39/1000 | Loss: 0.00001597
Iteration 40/1000 | Loss: 0.00001597
Iteration 41/1000 | Loss: 0.00001596
Iteration 42/1000 | Loss: 0.00001596
Iteration 43/1000 | Loss: 0.00001595
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001593
Iteration 46/1000 | Loss: 0.00001593
Iteration 47/1000 | Loss: 0.00001593
Iteration 48/1000 | Loss: 0.00001593
Iteration 49/1000 | Loss: 0.00001592
Iteration 50/1000 | Loss: 0.00001592
Iteration 51/1000 | Loss: 0.00001591
Iteration 52/1000 | Loss: 0.00001591
Iteration 53/1000 | Loss: 0.00001591
Iteration 54/1000 | Loss: 0.00001590
Iteration 55/1000 | Loss: 0.00001590
Iteration 56/1000 | Loss: 0.00001590
Iteration 57/1000 | Loss: 0.00001590
Iteration 58/1000 | Loss: 0.00001589
Iteration 59/1000 | Loss: 0.00001589
Iteration 60/1000 | Loss: 0.00001588
Iteration 61/1000 | Loss: 0.00001588
Iteration 62/1000 | Loss: 0.00001588
Iteration 63/1000 | Loss: 0.00001588
Iteration 64/1000 | Loss: 0.00001588
Iteration 65/1000 | Loss: 0.00001588
Iteration 66/1000 | Loss: 0.00001587
Iteration 67/1000 | Loss: 0.00001587
Iteration 68/1000 | Loss: 0.00001587
Iteration 69/1000 | Loss: 0.00001587
Iteration 70/1000 | Loss: 0.00001586
Iteration 71/1000 | Loss: 0.00001586
Iteration 72/1000 | Loss: 0.00001586
Iteration 73/1000 | Loss: 0.00001586
Iteration 74/1000 | Loss: 0.00001586
Iteration 75/1000 | Loss: 0.00001586
Iteration 76/1000 | Loss: 0.00001586
Iteration 77/1000 | Loss: 0.00001586
Iteration 78/1000 | Loss: 0.00001586
Iteration 79/1000 | Loss: 0.00001586
Iteration 80/1000 | Loss: 0.00001586
Iteration 81/1000 | Loss: 0.00001585
Iteration 82/1000 | Loss: 0.00001585
Iteration 83/1000 | Loss: 0.00001585
Iteration 84/1000 | Loss: 0.00001585
Iteration 85/1000 | Loss: 0.00001585
Iteration 86/1000 | Loss: 0.00001585
Iteration 87/1000 | Loss: 0.00001585
Iteration 88/1000 | Loss: 0.00001585
Iteration 89/1000 | Loss: 0.00001585
Iteration 90/1000 | Loss: 0.00001585
Iteration 91/1000 | Loss: 0.00001584
Iteration 92/1000 | Loss: 0.00001584
Iteration 93/1000 | Loss: 0.00001584
Iteration 94/1000 | Loss: 0.00001583
Iteration 95/1000 | Loss: 0.00001583
Iteration 96/1000 | Loss: 0.00001583
Iteration 97/1000 | Loss: 0.00001583
Iteration 98/1000 | Loss: 0.00001583
Iteration 99/1000 | Loss: 0.00001583
Iteration 100/1000 | Loss: 0.00001583
Iteration 101/1000 | Loss: 0.00001582
Iteration 102/1000 | Loss: 0.00001582
Iteration 103/1000 | Loss: 0.00001582
Iteration 104/1000 | Loss: 0.00001582
Iteration 105/1000 | Loss: 0.00001582
Iteration 106/1000 | Loss: 0.00001582
Iteration 107/1000 | Loss: 0.00001582
Iteration 108/1000 | Loss: 0.00001581
Iteration 109/1000 | Loss: 0.00001581
Iteration 110/1000 | Loss: 0.00001581
Iteration 111/1000 | Loss: 0.00001581
Iteration 112/1000 | Loss: 0.00001581
Iteration 113/1000 | Loss: 0.00001581
Iteration 114/1000 | Loss: 0.00001581
Iteration 115/1000 | Loss: 0.00001581
Iteration 116/1000 | Loss: 0.00001581
Iteration 117/1000 | Loss: 0.00001581
Iteration 118/1000 | Loss: 0.00001581
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.5806603187229484e-05, 1.5806603187229484e-05, 1.5806603187229484e-05, 1.5806603187229484e-05, 1.5806603187229484e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5806603187229484e-05

Optimization complete. Final v2v error: 3.3988349437713623 mm

Highest mean error: 3.8195910453796387 mm for frame 24

Lowest mean error: 3.1303677558898926 mm for frame 46

Saving results

Total time: 39.703757524490356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378653
Iteration 2/25 | Loss: 0.00117633
Iteration 3/25 | Loss: 0.00112783
Iteration 4/25 | Loss: 0.00112078
Iteration 5/25 | Loss: 0.00111868
Iteration 6/25 | Loss: 0.00111837
Iteration 7/25 | Loss: 0.00111837
Iteration 8/25 | Loss: 0.00111837
Iteration 9/25 | Loss: 0.00111837
Iteration 10/25 | Loss: 0.00111837
Iteration 11/25 | Loss: 0.00111837
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001118374872021377, 0.001118374872021377, 0.001118374872021377, 0.001118374872021377, 0.001118374872021377]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001118374872021377

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.93187261
Iteration 2/25 | Loss: 0.00088701
Iteration 3/25 | Loss: 0.00088701
Iteration 4/25 | Loss: 0.00088700
Iteration 5/25 | Loss: 0.00088700
Iteration 6/25 | Loss: 0.00088700
Iteration 7/25 | Loss: 0.00088700
Iteration 8/25 | Loss: 0.00088700
Iteration 9/25 | Loss: 0.00088700
Iteration 10/25 | Loss: 0.00088700
Iteration 11/25 | Loss: 0.00088700
Iteration 12/25 | Loss: 0.00088700
Iteration 13/25 | Loss: 0.00088700
Iteration 14/25 | Loss: 0.00088700
Iteration 15/25 | Loss: 0.00088700
Iteration 16/25 | Loss: 0.00088700
Iteration 17/25 | Loss: 0.00088700
Iteration 18/25 | Loss: 0.00088700
Iteration 19/25 | Loss: 0.00088700
Iteration 20/25 | Loss: 0.00088700
Iteration 21/25 | Loss: 0.00088700
Iteration 22/25 | Loss: 0.00088700
Iteration 23/25 | Loss: 0.00088700
Iteration 24/25 | Loss: 0.00088700
Iteration 25/25 | Loss: 0.00088700

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088700
Iteration 2/1000 | Loss: 0.00002070
Iteration 3/1000 | Loss: 0.00001411
Iteration 4/1000 | Loss: 0.00001229
Iteration 5/1000 | Loss: 0.00001133
Iteration 6/1000 | Loss: 0.00001083
Iteration 7/1000 | Loss: 0.00001050
Iteration 8/1000 | Loss: 0.00001024
Iteration 9/1000 | Loss: 0.00001023
Iteration 10/1000 | Loss: 0.00001022
Iteration 11/1000 | Loss: 0.00001022
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001001
Iteration 14/1000 | Loss: 0.00000981
Iteration 15/1000 | Loss: 0.00000981
Iteration 16/1000 | Loss: 0.00000974
Iteration 17/1000 | Loss: 0.00000972
Iteration 18/1000 | Loss: 0.00000970
Iteration 19/1000 | Loss: 0.00000965
Iteration 20/1000 | Loss: 0.00000963
Iteration 21/1000 | Loss: 0.00000957
Iteration 22/1000 | Loss: 0.00000955
Iteration 23/1000 | Loss: 0.00000954
Iteration 24/1000 | Loss: 0.00000953
Iteration 25/1000 | Loss: 0.00000950
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000949
Iteration 28/1000 | Loss: 0.00000949
Iteration 29/1000 | Loss: 0.00000948
Iteration 30/1000 | Loss: 0.00000948
Iteration 31/1000 | Loss: 0.00000947
Iteration 32/1000 | Loss: 0.00000947
Iteration 33/1000 | Loss: 0.00000946
Iteration 34/1000 | Loss: 0.00000945
Iteration 35/1000 | Loss: 0.00000944
Iteration 36/1000 | Loss: 0.00000943
Iteration 37/1000 | Loss: 0.00000942
Iteration 38/1000 | Loss: 0.00000942
Iteration 39/1000 | Loss: 0.00000942
Iteration 40/1000 | Loss: 0.00000941
Iteration 41/1000 | Loss: 0.00000941
Iteration 42/1000 | Loss: 0.00000940
Iteration 43/1000 | Loss: 0.00000938
Iteration 44/1000 | Loss: 0.00000937
Iteration 45/1000 | Loss: 0.00000937
Iteration 46/1000 | Loss: 0.00000936
Iteration 47/1000 | Loss: 0.00000935
Iteration 48/1000 | Loss: 0.00000935
Iteration 49/1000 | Loss: 0.00000935
Iteration 50/1000 | Loss: 0.00000935
Iteration 51/1000 | Loss: 0.00000934
Iteration 52/1000 | Loss: 0.00000934
Iteration 53/1000 | Loss: 0.00000934
Iteration 54/1000 | Loss: 0.00000934
Iteration 55/1000 | Loss: 0.00000933
Iteration 56/1000 | Loss: 0.00000933
Iteration 57/1000 | Loss: 0.00000933
Iteration 58/1000 | Loss: 0.00000933
Iteration 59/1000 | Loss: 0.00000932
Iteration 60/1000 | Loss: 0.00000930
Iteration 61/1000 | Loss: 0.00000929
Iteration 62/1000 | Loss: 0.00000929
Iteration 63/1000 | Loss: 0.00000928
Iteration 64/1000 | Loss: 0.00000928
Iteration 65/1000 | Loss: 0.00000927
Iteration 66/1000 | Loss: 0.00000926
Iteration 67/1000 | Loss: 0.00000926
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000925
Iteration 71/1000 | Loss: 0.00000925
Iteration 72/1000 | Loss: 0.00000925
Iteration 73/1000 | Loss: 0.00000924
Iteration 74/1000 | Loss: 0.00000923
Iteration 75/1000 | Loss: 0.00000923
Iteration 76/1000 | Loss: 0.00000919
Iteration 77/1000 | Loss: 0.00000919
Iteration 78/1000 | Loss: 0.00000919
Iteration 79/1000 | Loss: 0.00000919
Iteration 80/1000 | Loss: 0.00000919
Iteration 81/1000 | Loss: 0.00000919
Iteration 82/1000 | Loss: 0.00000919
Iteration 83/1000 | Loss: 0.00000919
Iteration 84/1000 | Loss: 0.00000918
Iteration 85/1000 | Loss: 0.00000917
Iteration 86/1000 | Loss: 0.00000917
Iteration 87/1000 | Loss: 0.00000916
Iteration 88/1000 | Loss: 0.00000916
Iteration 89/1000 | Loss: 0.00000916
Iteration 90/1000 | Loss: 0.00000916
Iteration 91/1000 | Loss: 0.00000916
Iteration 92/1000 | Loss: 0.00000916
Iteration 93/1000 | Loss: 0.00000916
Iteration 94/1000 | Loss: 0.00000916
Iteration 95/1000 | Loss: 0.00000916
Iteration 96/1000 | Loss: 0.00000915
Iteration 97/1000 | Loss: 0.00000915
Iteration 98/1000 | Loss: 0.00000915
Iteration 99/1000 | Loss: 0.00000914
Iteration 100/1000 | Loss: 0.00000914
Iteration 101/1000 | Loss: 0.00000913
Iteration 102/1000 | Loss: 0.00000913
Iteration 103/1000 | Loss: 0.00000913
Iteration 104/1000 | Loss: 0.00000913
Iteration 105/1000 | Loss: 0.00000913
Iteration 106/1000 | Loss: 0.00000912
Iteration 107/1000 | Loss: 0.00000912
Iteration 108/1000 | Loss: 0.00000912
Iteration 109/1000 | Loss: 0.00000912
Iteration 110/1000 | Loss: 0.00000912
Iteration 111/1000 | Loss: 0.00000911
Iteration 112/1000 | Loss: 0.00000911
Iteration 113/1000 | Loss: 0.00000911
Iteration 114/1000 | Loss: 0.00000911
Iteration 115/1000 | Loss: 0.00000910
Iteration 116/1000 | Loss: 0.00000910
Iteration 117/1000 | Loss: 0.00000910
Iteration 118/1000 | Loss: 0.00000910
Iteration 119/1000 | Loss: 0.00000910
Iteration 120/1000 | Loss: 0.00000910
Iteration 121/1000 | Loss: 0.00000910
Iteration 122/1000 | Loss: 0.00000909
Iteration 123/1000 | Loss: 0.00000909
Iteration 124/1000 | Loss: 0.00000909
Iteration 125/1000 | Loss: 0.00000909
Iteration 126/1000 | Loss: 0.00000909
Iteration 127/1000 | Loss: 0.00000909
Iteration 128/1000 | Loss: 0.00000909
Iteration 129/1000 | Loss: 0.00000909
Iteration 130/1000 | Loss: 0.00000908
Iteration 131/1000 | Loss: 0.00000908
Iteration 132/1000 | Loss: 0.00000907
Iteration 133/1000 | Loss: 0.00000907
Iteration 134/1000 | Loss: 0.00000907
Iteration 135/1000 | Loss: 0.00000907
Iteration 136/1000 | Loss: 0.00000907
Iteration 137/1000 | Loss: 0.00000906
Iteration 138/1000 | Loss: 0.00000906
Iteration 139/1000 | Loss: 0.00000906
Iteration 140/1000 | Loss: 0.00000906
Iteration 141/1000 | Loss: 0.00000906
Iteration 142/1000 | Loss: 0.00000906
Iteration 143/1000 | Loss: 0.00000906
Iteration 144/1000 | Loss: 0.00000906
Iteration 145/1000 | Loss: 0.00000906
Iteration 146/1000 | Loss: 0.00000906
Iteration 147/1000 | Loss: 0.00000906
Iteration 148/1000 | Loss: 0.00000906
Iteration 149/1000 | Loss: 0.00000905
Iteration 150/1000 | Loss: 0.00000905
Iteration 151/1000 | Loss: 0.00000905
Iteration 152/1000 | Loss: 0.00000905
Iteration 153/1000 | Loss: 0.00000905
Iteration 154/1000 | Loss: 0.00000905
Iteration 155/1000 | Loss: 0.00000905
Iteration 156/1000 | Loss: 0.00000905
Iteration 157/1000 | Loss: 0.00000904
Iteration 158/1000 | Loss: 0.00000904
Iteration 159/1000 | Loss: 0.00000904
Iteration 160/1000 | Loss: 0.00000904
Iteration 161/1000 | Loss: 0.00000904
Iteration 162/1000 | Loss: 0.00000904
Iteration 163/1000 | Loss: 0.00000904
Iteration 164/1000 | Loss: 0.00000904
Iteration 165/1000 | Loss: 0.00000904
Iteration 166/1000 | Loss: 0.00000904
Iteration 167/1000 | Loss: 0.00000904
Iteration 168/1000 | Loss: 0.00000904
Iteration 169/1000 | Loss: 0.00000904
Iteration 170/1000 | Loss: 0.00000904
Iteration 171/1000 | Loss: 0.00000904
Iteration 172/1000 | Loss: 0.00000904
Iteration 173/1000 | Loss: 0.00000904
Iteration 174/1000 | Loss: 0.00000904
Iteration 175/1000 | Loss: 0.00000904
Iteration 176/1000 | Loss: 0.00000904
Iteration 177/1000 | Loss: 0.00000904
Iteration 178/1000 | Loss: 0.00000904
Iteration 179/1000 | Loss: 0.00000904
Iteration 180/1000 | Loss: 0.00000904
Iteration 181/1000 | Loss: 0.00000904
Iteration 182/1000 | Loss: 0.00000904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [9.04130229173461e-06, 9.04130229173461e-06, 9.04130229173461e-06, 9.04130229173461e-06, 9.04130229173461e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.04130229173461e-06

Optimization complete. Final v2v error: 2.5993707180023193 mm

Highest mean error: 2.9330217838287354 mm for frame 95

Lowest mean error: 2.4395575523376465 mm for frame 166

Saving results

Total time: 39.605812788009644
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012383
Iteration 2/25 | Loss: 0.00234520
Iteration 3/25 | Loss: 0.00303927
Iteration 4/25 | Loss: 0.00214394
Iteration 5/25 | Loss: 0.00169248
Iteration 6/25 | Loss: 0.00170476
Iteration 7/25 | Loss: 0.00141834
Iteration 8/25 | Loss: 0.00137226
Iteration 9/25 | Loss: 0.00129410
Iteration 10/25 | Loss: 0.00125759
Iteration 11/25 | Loss: 0.00128730
Iteration 12/25 | Loss: 0.00123706
Iteration 13/25 | Loss: 0.00123374
Iteration 14/25 | Loss: 0.00123111
Iteration 15/25 | Loss: 0.00123111
Iteration 16/25 | Loss: 0.00121919
Iteration 17/25 | Loss: 0.00121468
Iteration 18/25 | Loss: 0.00121397
Iteration 19/25 | Loss: 0.00121691
Iteration 20/25 | Loss: 0.00121314
Iteration 21/25 | Loss: 0.00121193
Iteration 22/25 | Loss: 0.00121136
Iteration 23/25 | Loss: 0.00121124
Iteration 24/25 | Loss: 0.00121124
Iteration 25/25 | Loss: 0.00121124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43791163
Iteration 2/25 | Loss: 0.00097106
Iteration 3/25 | Loss: 0.00097106
Iteration 4/25 | Loss: 0.00097106
Iteration 5/25 | Loss: 0.00097106
Iteration 6/25 | Loss: 0.00097106
Iteration 7/25 | Loss: 0.00097106
Iteration 8/25 | Loss: 0.00097106
Iteration 9/25 | Loss: 0.00097106
Iteration 10/25 | Loss: 0.00097106
Iteration 11/25 | Loss: 0.00097106
Iteration 12/25 | Loss: 0.00097106
Iteration 13/25 | Loss: 0.00097106
Iteration 14/25 | Loss: 0.00097106
Iteration 15/25 | Loss: 0.00097106
Iteration 16/25 | Loss: 0.00097106
Iteration 17/25 | Loss: 0.00097106
Iteration 18/25 | Loss: 0.00097106
Iteration 19/25 | Loss: 0.00097106
Iteration 20/25 | Loss: 0.00097106
Iteration 21/25 | Loss: 0.00097106
Iteration 22/25 | Loss: 0.00097106
Iteration 23/25 | Loss: 0.00097106
Iteration 24/25 | Loss: 0.00097106
Iteration 25/25 | Loss: 0.00097106

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097106
Iteration 2/1000 | Loss: 0.00004792
Iteration 3/1000 | Loss: 0.00109892
Iteration 4/1000 | Loss: 0.00006575
Iteration 5/1000 | Loss: 0.00003867
Iteration 6/1000 | Loss: 0.00002703
Iteration 7/1000 | Loss: 0.00002471
Iteration 8/1000 | Loss: 0.00002347
Iteration 9/1000 | Loss: 0.00002273
Iteration 10/1000 | Loss: 0.00024462
Iteration 11/1000 | Loss: 0.00002208
Iteration 12/1000 | Loss: 0.00002166
Iteration 13/1000 | Loss: 0.00002130
Iteration 14/1000 | Loss: 0.00002110
Iteration 15/1000 | Loss: 0.00002088
Iteration 16/1000 | Loss: 0.00002069
Iteration 17/1000 | Loss: 0.00002067
Iteration 18/1000 | Loss: 0.00002064
Iteration 19/1000 | Loss: 0.00002063
Iteration 20/1000 | Loss: 0.00002062
Iteration 21/1000 | Loss: 0.00002062
Iteration 22/1000 | Loss: 0.00002061
Iteration 23/1000 | Loss: 0.00002060
Iteration 24/1000 | Loss: 0.00002056
Iteration 25/1000 | Loss: 0.00002055
Iteration 26/1000 | Loss: 0.00002053
Iteration 27/1000 | Loss: 0.00002053
Iteration 28/1000 | Loss: 0.00002053
Iteration 29/1000 | Loss: 0.00002053
Iteration 30/1000 | Loss: 0.00002052
Iteration 31/1000 | Loss: 0.00002048
Iteration 32/1000 | Loss: 0.00002044
Iteration 33/1000 | Loss: 0.00002044
Iteration 34/1000 | Loss: 0.00002044
Iteration 35/1000 | Loss: 0.00002041
Iteration 36/1000 | Loss: 0.00002041
Iteration 37/1000 | Loss: 0.00002041
Iteration 38/1000 | Loss: 0.00002041
Iteration 39/1000 | Loss: 0.00002041
Iteration 40/1000 | Loss: 0.00002040
Iteration 41/1000 | Loss: 0.00002040
Iteration 42/1000 | Loss: 0.00002039
Iteration 43/1000 | Loss: 0.00002039
Iteration 44/1000 | Loss: 0.00002038
Iteration 45/1000 | Loss: 0.00002038
Iteration 46/1000 | Loss: 0.00002038
Iteration 47/1000 | Loss: 0.00002038
Iteration 48/1000 | Loss: 0.00002037
Iteration 49/1000 | Loss: 0.00002037
Iteration 50/1000 | Loss: 0.00002037
Iteration 51/1000 | Loss: 0.00002036
Iteration 52/1000 | Loss: 0.00002036
Iteration 53/1000 | Loss: 0.00002036
Iteration 54/1000 | Loss: 0.00002036
Iteration 55/1000 | Loss: 0.00002035
Iteration 56/1000 | Loss: 0.00002035
Iteration 57/1000 | Loss: 0.00002035
Iteration 58/1000 | Loss: 0.00002035
Iteration 59/1000 | Loss: 0.00002035
Iteration 60/1000 | Loss: 0.00002035
Iteration 61/1000 | Loss: 0.00002035
Iteration 62/1000 | Loss: 0.00002035
Iteration 63/1000 | Loss: 0.00002034
Iteration 64/1000 | Loss: 0.00002034
Iteration 65/1000 | Loss: 0.00002034
Iteration 66/1000 | Loss: 0.00002034
Iteration 67/1000 | Loss: 0.00002034
Iteration 68/1000 | Loss: 0.00002034
Iteration 69/1000 | Loss: 0.00002033
Iteration 70/1000 | Loss: 0.00002033
Iteration 71/1000 | Loss: 0.00002032
Iteration 72/1000 | Loss: 0.00002032
Iteration 73/1000 | Loss: 0.00002032
Iteration 74/1000 | Loss: 0.00002032
Iteration 75/1000 | Loss: 0.00002032
Iteration 76/1000 | Loss: 0.00002032
Iteration 77/1000 | Loss: 0.00002032
Iteration 78/1000 | Loss: 0.00002031
Iteration 79/1000 | Loss: 0.00002031
Iteration 80/1000 | Loss: 0.00002031
Iteration 81/1000 | Loss: 0.00002031
Iteration 82/1000 | Loss: 0.00002030
Iteration 83/1000 | Loss: 0.00002030
Iteration 84/1000 | Loss: 0.00002030
Iteration 85/1000 | Loss: 0.00002030
Iteration 86/1000 | Loss: 0.00002030
Iteration 87/1000 | Loss: 0.00002030
Iteration 88/1000 | Loss: 0.00002030
Iteration 89/1000 | Loss: 0.00002030
Iteration 90/1000 | Loss: 0.00002029
Iteration 91/1000 | Loss: 0.00002029
Iteration 92/1000 | Loss: 0.00002029
Iteration 93/1000 | Loss: 0.00002029
Iteration 94/1000 | Loss: 0.00002028
Iteration 95/1000 | Loss: 0.00002028
Iteration 96/1000 | Loss: 0.00002028
Iteration 97/1000 | Loss: 0.00002027
Iteration 98/1000 | Loss: 0.00002027
Iteration 99/1000 | Loss: 0.00002027
Iteration 100/1000 | Loss: 0.00002027
Iteration 101/1000 | Loss: 0.00002027
Iteration 102/1000 | Loss: 0.00002026
Iteration 103/1000 | Loss: 0.00002026
Iteration 104/1000 | Loss: 0.00002026
Iteration 105/1000 | Loss: 0.00002026
Iteration 106/1000 | Loss: 0.00002026
Iteration 107/1000 | Loss: 0.00002025
Iteration 108/1000 | Loss: 0.00002025
Iteration 109/1000 | Loss: 0.00002025
Iteration 110/1000 | Loss: 0.00002024
Iteration 111/1000 | Loss: 0.00002024
Iteration 112/1000 | Loss: 0.00002024
Iteration 113/1000 | Loss: 0.00002024
Iteration 114/1000 | Loss: 0.00002023
Iteration 115/1000 | Loss: 0.00002023
Iteration 116/1000 | Loss: 0.00002022
Iteration 117/1000 | Loss: 0.00002022
Iteration 118/1000 | Loss: 0.00002022
Iteration 119/1000 | Loss: 0.00002022
Iteration 120/1000 | Loss: 0.00002022
Iteration 121/1000 | Loss: 0.00002022
Iteration 122/1000 | Loss: 0.00002022
Iteration 123/1000 | Loss: 0.00002022
Iteration 124/1000 | Loss: 0.00002022
Iteration 125/1000 | Loss: 0.00002022
Iteration 126/1000 | Loss: 0.00002022
Iteration 127/1000 | Loss: 0.00002021
Iteration 128/1000 | Loss: 0.00002021
Iteration 129/1000 | Loss: 0.00002021
Iteration 130/1000 | Loss: 0.00002020
Iteration 131/1000 | Loss: 0.00002020
Iteration 132/1000 | Loss: 0.00002020
Iteration 133/1000 | Loss: 0.00002020
Iteration 134/1000 | Loss: 0.00002019
Iteration 135/1000 | Loss: 0.00002019
Iteration 136/1000 | Loss: 0.00002019
Iteration 137/1000 | Loss: 0.00002018
Iteration 138/1000 | Loss: 0.00002018
Iteration 139/1000 | Loss: 0.00002018
Iteration 140/1000 | Loss: 0.00002018
Iteration 141/1000 | Loss: 0.00002018
Iteration 142/1000 | Loss: 0.00002018
Iteration 143/1000 | Loss: 0.00002018
Iteration 144/1000 | Loss: 0.00002017
Iteration 145/1000 | Loss: 0.00002017
Iteration 146/1000 | Loss: 0.00002017
Iteration 147/1000 | Loss: 0.00002016
Iteration 148/1000 | Loss: 0.00002016
Iteration 149/1000 | Loss: 0.00002016
Iteration 150/1000 | Loss: 0.00002016
Iteration 151/1000 | Loss: 0.00002016
Iteration 152/1000 | Loss: 0.00002016
Iteration 153/1000 | Loss: 0.00002015
Iteration 154/1000 | Loss: 0.00002015
Iteration 155/1000 | Loss: 0.00002015
Iteration 156/1000 | Loss: 0.00002015
Iteration 157/1000 | Loss: 0.00002014
Iteration 158/1000 | Loss: 0.00002014
Iteration 159/1000 | Loss: 0.00002014
Iteration 160/1000 | Loss: 0.00002014
Iteration 161/1000 | Loss: 0.00002014
Iteration 162/1000 | Loss: 0.00002014
Iteration 163/1000 | Loss: 0.00002013
Iteration 164/1000 | Loss: 0.00002013
Iteration 165/1000 | Loss: 0.00002013
Iteration 166/1000 | Loss: 0.00002013
Iteration 167/1000 | Loss: 0.00002013
Iteration 168/1000 | Loss: 0.00002013
Iteration 169/1000 | Loss: 0.00002012
Iteration 170/1000 | Loss: 0.00002012
Iteration 171/1000 | Loss: 0.00002012
Iteration 172/1000 | Loss: 0.00002012
Iteration 173/1000 | Loss: 0.00002012
Iteration 174/1000 | Loss: 0.00002012
Iteration 175/1000 | Loss: 0.00002012
Iteration 176/1000 | Loss: 0.00002012
Iteration 177/1000 | Loss: 0.00002012
Iteration 178/1000 | Loss: 0.00002012
Iteration 179/1000 | Loss: 0.00002011
Iteration 180/1000 | Loss: 0.00002011
Iteration 181/1000 | Loss: 0.00002011
Iteration 182/1000 | Loss: 0.00002011
Iteration 183/1000 | Loss: 0.00002011
Iteration 184/1000 | Loss: 0.00002010
Iteration 185/1000 | Loss: 0.00002010
Iteration 186/1000 | Loss: 0.00002010
Iteration 187/1000 | Loss: 0.00002009
Iteration 188/1000 | Loss: 0.00002009
Iteration 189/1000 | Loss: 0.00002009
Iteration 190/1000 | Loss: 0.00002009
Iteration 191/1000 | Loss: 0.00002008
Iteration 192/1000 | Loss: 0.00002008
Iteration 193/1000 | Loss: 0.00002008
Iteration 194/1000 | Loss: 0.00002008
Iteration 195/1000 | Loss: 0.00002008
Iteration 196/1000 | Loss: 0.00002008
Iteration 197/1000 | Loss: 0.00002008
Iteration 198/1000 | Loss: 0.00002008
Iteration 199/1000 | Loss: 0.00002007
Iteration 200/1000 | Loss: 0.00002007
Iteration 201/1000 | Loss: 0.00002007
Iteration 202/1000 | Loss: 0.00002006
Iteration 203/1000 | Loss: 0.00002006
Iteration 204/1000 | Loss: 0.00002006
Iteration 205/1000 | Loss: 0.00002005
Iteration 206/1000 | Loss: 0.00002005
Iteration 207/1000 | Loss: 0.00002005
Iteration 208/1000 | Loss: 0.00002004
Iteration 209/1000 | Loss: 0.00002004
Iteration 210/1000 | Loss: 0.00002004
Iteration 211/1000 | Loss: 0.00002004
Iteration 212/1000 | Loss: 0.00002003
Iteration 213/1000 | Loss: 0.00002003
Iteration 214/1000 | Loss: 0.00002003
Iteration 215/1000 | Loss: 0.00002003
Iteration 216/1000 | Loss: 0.00002003
Iteration 217/1000 | Loss: 0.00002002
Iteration 218/1000 | Loss: 0.00002002
Iteration 219/1000 | Loss: 0.00002002
Iteration 220/1000 | Loss: 0.00002002
Iteration 221/1000 | Loss: 0.00002002
Iteration 222/1000 | Loss: 0.00002002
Iteration 223/1000 | Loss: 0.00002002
Iteration 224/1000 | Loss: 0.00002001
Iteration 225/1000 | Loss: 0.00002001
Iteration 226/1000 | Loss: 0.00002001
Iteration 227/1000 | Loss: 0.00002000
Iteration 228/1000 | Loss: 0.00002000
Iteration 229/1000 | Loss: 0.00002000
Iteration 230/1000 | Loss: 0.00001999
Iteration 231/1000 | Loss: 0.00001998
Iteration 232/1000 | Loss: 0.00001998
Iteration 233/1000 | Loss: 0.00001998
Iteration 234/1000 | Loss: 0.00001997
Iteration 235/1000 | Loss: 0.00001997
Iteration 236/1000 | Loss: 0.00001997
Iteration 237/1000 | Loss: 0.00001997
Iteration 238/1000 | Loss: 0.00001996
Iteration 239/1000 | Loss: 0.00001996
Iteration 240/1000 | Loss: 0.00001996
Iteration 241/1000 | Loss: 0.00001995
Iteration 242/1000 | Loss: 0.00001994
Iteration 243/1000 | Loss: 0.00001993
Iteration 244/1000 | Loss: 0.00001993
Iteration 245/1000 | Loss: 0.00001980
Iteration 246/1000 | Loss: 0.00001968
Iteration 247/1000 | Loss: 0.00001967
Iteration 248/1000 | Loss: 0.00001966
Iteration 249/1000 | Loss: 0.00001965
Iteration 250/1000 | Loss: 0.00001962
Iteration 251/1000 | Loss: 0.00001962
Iteration 252/1000 | Loss: 0.00001962
Iteration 253/1000 | Loss: 0.00001961
Iteration 254/1000 | Loss: 0.00001960
Iteration 255/1000 | Loss: 0.00001957
Iteration 256/1000 | Loss: 0.00001956
Iteration 257/1000 | Loss: 0.00001956
Iteration 258/1000 | Loss: 0.00001954
Iteration 259/1000 | Loss: 0.00001953
Iteration 260/1000 | Loss: 0.00001953
Iteration 261/1000 | Loss: 0.00001953
Iteration 262/1000 | Loss: 0.00001952
Iteration 263/1000 | Loss: 0.00001952
Iteration 264/1000 | Loss: 0.00001952
Iteration 265/1000 | Loss: 0.00001950
Iteration 266/1000 | Loss: 0.00001950
Iteration 267/1000 | Loss: 0.00001950
Iteration 268/1000 | Loss: 0.00001950
Iteration 269/1000 | Loss: 0.00001950
Iteration 270/1000 | Loss: 0.00001950
Iteration 271/1000 | Loss: 0.00001950
Iteration 272/1000 | Loss: 0.00001950
Iteration 273/1000 | Loss: 0.00001949
Iteration 274/1000 | Loss: 0.00001949
Iteration 275/1000 | Loss: 0.00001949
Iteration 276/1000 | Loss: 0.00001948
Iteration 277/1000 | Loss: 0.00001948
Iteration 278/1000 | Loss: 0.00001947
Iteration 279/1000 | Loss: 0.00001947
Iteration 280/1000 | Loss: 0.00001947
Iteration 281/1000 | Loss: 0.00001947
Iteration 282/1000 | Loss: 0.00001946
Iteration 283/1000 | Loss: 0.00001946
Iteration 284/1000 | Loss: 0.00001946
Iteration 285/1000 | Loss: 0.00001946
Iteration 286/1000 | Loss: 0.00001946
Iteration 287/1000 | Loss: 0.00001946
Iteration 288/1000 | Loss: 0.00001946
Iteration 289/1000 | Loss: 0.00001946
Iteration 290/1000 | Loss: 0.00001946
Iteration 291/1000 | Loss: 0.00001946
Iteration 292/1000 | Loss: 0.00001945
Iteration 293/1000 | Loss: 0.00001945
Iteration 294/1000 | Loss: 0.00001945
Iteration 295/1000 | Loss: 0.00001945
Iteration 296/1000 | Loss: 0.00001945
Iteration 297/1000 | Loss: 0.00001944
Iteration 298/1000 | Loss: 0.00001944
Iteration 299/1000 | Loss: 0.00001944
Iteration 300/1000 | Loss: 0.00001943
Iteration 301/1000 | Loss: 0.00001943
Iteration 302/1000 | Loss: 0.00001943
Iteration 303/1000 | Loss: 0.00001942
Iteration 304/1000 | Loss: 0.00001942
Iteration 305/1000 | Loss: 0.00001942
Iteration 306/1000 | Loss: 0.00001942
Iteration 307/1000 | Loss: 0.00001941
Iteration 308/1000 | Loss: 0.00001941
Iteration 309/1000 | Loss: 0.00001940
Iteration 310/1000 | Loss: 0.00001940
Iteration 311/1000 | Loss: 0.00001940
Iteration 312/1000 | Loss: 0.00001939
Iteration 313/1000 | Loss: 0.00001939
Iteration 314/1000 | Loss: 0.00001939
Iteration 315/1000 | Loss: 0.00001938
Iteration 316/1000 | Loss: 0.00001938
Iteration 317/1000 | Loss: 0.00001938
Iteration 318/1000 | Loss: 0.00001937
Iteration 319/1000 | Loss: 0.00001937
Iteration 320/1000 | Loss: 0.00001937
Iteration 321/1000 | Loss: 0.00001936
Iteration 322/1000 | Loss: 0.00001936
Iteration 323/1000 | Loss: 0.00001935
Iteration 324/1000 | Loss: 0.00001935
Iteration 325/1000 | Loss: 0.00001935
Iteration 326/1000 | Loss: 0.00001935
Iteration 327/1000 | Loss: 0.00001935
Iteration 328/1000 | Loss: 0.00001935
Iteration 329/1000 | Loss: 0.00001934
Iteration 330/1000 | Loss: 0.00001934
Iteration 331/1000 | Loss: 0.00001934
Iteration 332/1000 | Loss: 0.00001933
Iteration 333/1000 | Loss: 0.00001933
Iteration 334/1000 | Loss: 0.00001933
Iteration 335/1000 | Loss: 0.00001932
Iteration 336/1000 | Loss: 0.00001932
Iteration 337/1000 | Loss: 0.00001931
Iteration 338/1000 | Loss: 0.00001931
Iteration 339/1000 | Loss: 0.00001931
Iteration 340/1000 | Loss: 0.00001931
Iteration 341/1000 | Loss: 0.00001931
Iteration 342/1000 | Loss: 0.00001931
Iteration 343/1000 | Loss: 0.00001930
Iteration 344/1000 | Loss: 0.00001930
Iteration 345/1000 | Loss: 0.00001930
Iteration 346/1000 | Loss: 0.00001930
Iteration 347/1000 | Loss: 0.00001930
Iteration 348/1000 | Loss: 0.00001930
Iteration 349/1000 | Loss: 0.00001929
Iteration 350/1000 | Loss: 0.00001929
Iteration 351/1000 | Loss: 0.00001929
Iteration 352/1000 | Loss: 0.00001929
Iteration 353/1000 | Loss: 0.00001929
Iteration 354/1000 | Loss: 0.00001928
Iteration 355/1000 | Loss: 0.00001928
Iteration 356/1000 | Loss: 0.00001928
Iteration 357/1000 | Loss: 0.00001928
Iteration 358/1000 | Loss: 0.00001928
Iteration 359/1000 | Loss: 0.00001928
Iteration 360/1000 | Loss: 0.00001928
Iteration 361/1000 | Loss: 0.00001928
Iteration 362/1000 | Loss: 0.00001928
Iteration 363/1000 | Loss: 0.00001928
Iteration 364/1000 | Loss: 0.00001928
Iteration 365/1000 | Loss: 0.00001927
Iteration 366/1000 | Loss: 0.00001927
Iteration 367/1000 | Loss: 0.00001927
Iteration 368/1000 | Loss: 0.00001927
Iteration 369/1000 | Loss: 0.00001927
Iteration 370/1000 | Loss: 0.00001927
Iteration 371/1000 | Loss: 0.00001927
Iteration 372/1000 | Loss: 0.00001927
Iteration 373/1000 | Loss: 0.00001927
Iteration 374/1000 | Loss: 0.00001926
Iteration 375/1000 | Loss: 0.00001926
Iteration 376/1000 | Loss: 0.00001926
Iteration 377/1000 | Loss: 0.00001926
Iteration 378/1000 | Loss: 0.00001926
Iteration 379/1000 | Loss: 0.00001926
Iteration 380/1000 | Loss: 0.00001926
Iteration 381/1000 | Loss: 0.00001926
Iteration 382/1000 | Loss: 0.00001926
Iteration 383/1000 | Loss: 0.00001926
Iteration 384/1000 | Loss: 0.00001926
Iteration 385/1000 | Loss: 0.00001926
Iteration 386/1000 | Loss: 0.00001926
Iteration 387/1000 | Loss: 0.00001926
Iteration 388/1000 | Loss: 0.00001926
Iteration 389/1000 | Loss: 0.00001926
Iteration 390/1000 | Loss: 0.00001926
Iteration 391/1000 | Loss: 0.00001925
Iteration 392/1000 | Loss: 0.00001925
Iteration 393/1000 | Loss: 0.00001925
Iteration 394/1000 | Loss: 0.00001925
Iteration 395/1000 | Loss: 0.00001925
Iteration 396/1000 | Loss: 0.00001925
Iteration 397/1000 | Loss: 0.00001925
Iteration 398/1000 | Loss: 0.00001925
Iteration 399/1000 | Loss: 0.00001925
Iteration 400/1000 | Loss: 0.00001925
Iteration 401/1000 | Loss: 0.00001924
Iteration 402/1000 | Loss: 0.00001924
Iteration 403/1000 | Loss: 0.00001924
Iteration 404/1000 | Loss: 0.00001924
Iteration 405/1000 | Loss: 0.00001924
Iteration 406/1000 | Loss: 0.00001924
Iteration 407/1000 | Loss: 0.00001924
Iteration 408/1000 | Loss: 0.00001924
Iteration 409/1000 | Loss: 0.00001924
Iteration 410/1000 | Loss: 0.00001924
Iteration 411/1000 | Loss: 0.00001924
Iteration 412/1000 | Loss: 0.00001924
Iteration 413/1000 | Loss: 0.00001924
Iteration 414/1000 | Loss: 0.00001924
Iteration 415/1000 | Loss: 0.00001923
Iteration 416/1000 | Loss: 0.00001923
Iteration 417/1000 | Loss: 0.00001923
Iteration 418/1000 | Loss: 0.00001923
Iteration 419/1000 | Loss: 0.00001923
Iteration 420/1000 | Loss: 0.00001923
Iteration 421/1000 | Loss: 0.00001923
Iteration 422/1000 | Loss: 0.00001923
Iteration 423/1000 | Loss: 0.00001923
Iteration 424/1000 | Loss: 0.00001923
Iteration 425/1000 | Loss: 0.00001923
Iteration 426/1000 | Loss: 0.00001923
Iteration 427/1000 | Loss: 0.00001923
Iteration 428/1000 | Loss: 0.00001923
Iteration 429/1000 | Loss: 0.00001923
Iteration 430/1000 | Loss: 0.00001923
Iteration 431/1000 | Loss: 0.00001923
Iteration 432/1000 | Loss: 0.00001923
Iteration 433/1000 | Loss: 0.00001923
Iteration 434/1000 | Loss: 0.00001923
Iteration 435/1000 | Loss: 0.00001923
Iteration 436/1000 | Loss: 0.00001922
Iteration 437/1000 | Loss: 0.00001922
Iteration 438/1000 | Loss: 0.00001922
Iteration 439/1000 | Loss: 0.00001922
Iteration 440/1000 | Loss: 0.00001922
Iteration 441/1000 | Loss: 0.00001922
Iteration 442/1000 | Loss: 0.00001922
Iteration 443/1000 | Loss: 0.00001922
Iteration 444/1000 | Loss: 0.00001922
Iteration 445/1000 | Loss: 0.00001922
Iteration 446/1000 | Loss: 0.00001922
Iteration 447/1000 | Loss: 0.00001922
Iteration 448/1000 | Loss: 0.00001922
Iteration 449/1000 | Loss: 0.00001921
Iteration 450/1000 | Loss: 0.00001921
Iteration 451/1000 | Loss: 0.00001921
Iteration 452/1000 | Loss: 0.00001921
Iteration 453/1000 | Loss: 0.00001921
Iteration 454/1000 | Loss: 0.00001921
Iteration 455/1000 | Loss: 0.00001921
Iteration 456/1000 | Loss: 0.00001921
Iteration 457/1000 | Loss: 0.00001921
Iteration 458/1000 | Loss: 0.00001921
Iteration 459/1000 | Loss: 0.00001921
Iteration 460/1000 | Loss: 0.00001921
Iteration 461/1000 | Loss: 0.00001921
Iteration 462/1000 | Loss: 0.00001921
Iteration 463/1000 | Loss: 0.00001921
Iteration 464/1000 | Loss: 0.00001921
Iteration 465/1000 | Loss: 0.00001921
Iteration 466/1000 | Loss: 0.00001921
Iteration 467/1000 | Loss: 0.00001921
Iteration 468/1000 | Loss: 0.00001921
Iteration 469/1000 | Loss: 0.00001921
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 469. Stopping optimization.
Last 5 losses: [1.9212033294024877e-05, 1.9212033294024877e-05, 1.9212033294024877e-05, 1.9212033294024877e-05, 1.9212033294024877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9212033294024877e-05

Optimization complete. Final v2v error: 3.542327642440796 mm

Highest mean error: 12.209872245788574 mm for frame 155

Lowest mean error: 2.917801856994629 mm for frame 170

Saving results

Total time: 119.05557894706726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821876
Iteration 2/25 | Loss: 0.00124445
Iteration 3/25 | Loss: 0.00114599
Iteration 4/25 | Loss: 0.00113705
Iteration 5/25 | Loss: 0.00113540
Iteration 6/25 | Loss: 0.00113540
Iteration 7/25 | Loss: 0.00113540
Iteration 8/25 | Loss: 0.00113540
Iteration 9/25 | Loss: 0.00113540
Iteration 10/25 | Loss: 0.00113540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001135395490564406, 0.001135395490564406, 0.001135395490564406, 0.001135395490564406, 0.001135395490564406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001135395490564406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35338700
Iteration 2/25 | Loss: 0.00073933
Iteration 3/25 | Loss: 0.00073932
Iteration 4/25 | Loss: 0.00073932
Iteration 5/25 | Loss: 0.00073932
Iteration 6/25 | Loss: 0.00073932
Iteration 7/25 | Loss: 0.00073932
Iteration 8/25 | Loss: 0.00073932
Iteration 9/25 | Loss: 0.00073932
Iteration 10/25 | Loss: 0.00073931
Iteration 11/25 | Loss: 0.00073931
Iteration 12/25 | Loss: 0.00073931
Iteration 13/25 | Loss: 0.00073931
Iteration 14/25 | Loss: 0.00073931
Iteration 15/25 | Loss: 0.00073931
Iteration 16/25 | Loss: 0.00073931
Iteration 17/25 | Loss: 0.00073931
Iteration 18/25 | Loss: 0.00073931
Iteration 19/25 | Loss: 0.00073931
Iteration 20/25 | Loss: 0.00073931
Iteration 21/25 | Loss: 0.00073931
Iteration 22/25 | Loss: 0.00073931
Iteration 23/25 | Loss: 0.00073931
Iteration 24/25 | Loss: 0.00073931
Iteration 25/25 | Loss: 0.00073931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073931
Iteration 2/1000 | Loss: 0.00002227
Iteration 3/1000 | Loss: 0.00001533
Iteration 4/1000 | Loss: 0.00001355
Iteration 5/1000 | Loss: 0.00001277
Iteration 6/1000 | Loss: 0.00001215
Iteration 7/1000 | Loss: 0.00001185
Iteration 8/1000 | Loss: 0.00001162
Iteration 9/1000 | Loss: 0.00001135
Iteration 10/1000 | Loss: 0.00001113
Iteration 11/1000 | Loss: 0.00001109
Iteration 12/1000 | Loss: 0.00001105
Iteration 13/1000 | Loss: 0.00001098
Iteration 14/1000 | Loss: 0.00001097
Iteration 15/1000 | Loss: 0.00001097
Iteration 16/1000 | Loss: 0.00001083
Iteration 17/1000 | Loss: 0.00001078
Iteration 18/1000 | Loss: 0.00001077
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001076
Iteration 21/1000 | Loss: 0.00001074
Iteration 22/1000 | Loss: 0.00001070
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001069
Iteration 25/1000 | Loss: 0.00001068
Iteration 26/1000 | Loss: 0.00001064
Iteration 27/1000 | Loss: 0.00001064
Iteration 28/1000 | Loss: 0.00001063
Iteration 29/1000 | Loss: 0.00001063
Iteration 30/1000 | Loss: 0.00001062
Iteration 31/1000 | Loss: 0.00001062
Iteration 32/1000 | Loss: 0.00001062
Iteration 33/1000 | Loss: 0.00001061
Iteration 34/1000 | Loss: 0.00001061
Iteration 35/1000 | Loss: 0.00001060
Iteration 36/1000 | Loss: 0.00001058
Iteration 37/1000 | Loss: 0.00001056
Iteration 38/1000 | Loss: 0.00001055
Iteration 39/1000 | Loss: 0.00001055
Iteration 40/1000 | Loss: 0.00001050
Iteration 41/1000 | Loss: 0.00001050
Iteration 42/1000 | Loss: 0.00001047
Iteration 43/1000 | Loss: 0.00001047
Iteration 44/1000 | Loss: 0.00001045
Iteration 45/1000 | Loss: 0.00001044
Iteration 46/1000 | Loss: 0.00001043
Iteration 47/1000 | Loss: 0.00001043
Iteration 48/1000 | Loss: 0.00001043
Iteration 49/1000 | Loss: 0.00001042
Iteration 50/1000 | Loss: 0.00001042
Iteration 51/1000 | Loss: 0.00001040
Iteration 52/1000 | Loss: 0.00001039
Iteration 53/1000 | Loss: 0.00001039
Iteration 54/1000 | Loss: 0.00001035
Iteration 55/1000 | Loss: 0.00001035
Iteration 56/1000 | Loss: 0.00001034
Iteration 57/1000 | Loss: 0.00001034
Iteration 58/1000 | Loss: 0.00001031
Iteration 59/1000 | Loss: 0.00001028
Iteration 60/1000 | Loss: 0.00001027
Iteration 61/1000 | Loss: 0.00001027
Iteration 62/1000 | Loss: 0.00001026
Iteration 63/1000 | Loss: 0.00001024
Iteration 64/1000 | Loss: 0.00001024
Iteration 65/1000 | Loss: 0.00001024
Iteration 66/1000 | Loss: 0.00001023
Iteration 67/1000 | Loss: 0.00001023
Iteration 68/1000 | Loss: 0.00001023
Iteration 69/1000 | Loss: 0.00001023
Iteration 70/1000 | Loss: 0.00001023
Iteration 71/1000 | Loss: 0.00001022
Iteration 72/1000 | Loss: 0.00001022
Iteration 73/1000 | Loss: 0.00001022
Iteration 74/1000 | Loss: 0.00001022
Iteration 75/1000 | Loss: 0.00001022
Iteration 76/1000 | Loss: 0.00001021
Iteration 77/1000 | Loss: 0.00001021
Iteration 78/1000 | Loss: 0.00001021
Iteration 79/1000 | Loss: 0.00001021
Iteration 80/1000 | Loss: 0.00001021
Iteration 81/1000 | Loss: 0.00001021
Iteration 82/1000 | Loss: 0.00001021
Iteration 83/1000 | Loss: 0.00001020
Iteration 84/1000 | Loss: 0.00001020
Iteration 85/1000 | Loss: 0.00001020
Iteration 86/1000 | Loss: 0.00001020
Iteration 87/1000 | Loss: 0.00001020
Iteration 88/1000 | Loss: 0.00001019
Iteration 89/1000 | Loss: 0.00001019
Iteration 90/1000 | Loss: 0.00001019
Iteration 91/1000 | Loss: 0.00001019
Iteration 92/1000 | Loss: 0.00001019
Iteration 93/1000 | Loss: 0.00001019
Iteration 94/1000 | Loss: 0.00001019
Iteration 95/1000 | Loss: 0.00001019
Iteration 96/1000 | Loss: 0.00001019
Iteration 97/1000 | Loss: 0.00001019
Iteration 98/1000 | Loss: 0.00001019
Iteration 99/1000 | Loss: 0.00001019
Iteration 100/1000 | Loss: 0.00001019
Iteration 101/1000 | Loss: 0.00001019
Iteration 102/1000 | Loss: 0.00001019
Iteration 103/1000 | Loss: 0.00001019
Iteration 104/1000 | Loss: 0.00001019
Iteration 105/1000 | Loss: 0.00001018
Iteration 106/1000 | Loss: 0.00001018
Iteration 107/1000 | Loss: 0.00001018
Iteration 108/1000 | Loss: 0.00001018
Iteration 109/1000 | Loss: 0.00001018
Iteration 110/1000 | Loss: 0.00001018
Iteration 111/1000 | Loss: 0.00001017
Iteration 112/1000 | Loss: 0.00001017
Iteration 113/1000 | Loss: 0.00001017
Iteration 114/1000 | Loss: 0.00001017
Iteration 115/1000 | Loss: 0.00001017
Iteration 116/1000 | Loss: 0.00001017
Iteration 117/1000 | Loss: 0.00001017
Iteration 118/1000 | Loss: 0.00001017
Iteration 119/1000 | Loss: 0.00001017
Iteration 120/1000 | Loss: 0.00001017
Iteration 121/1000 | Loss: 0.00001017
Iteration 122/1000 | Loss: 0.00001017
Iteration 123/1000 | Loss: 0.00001017
Iteration 124/1000 | Loss: 0.00001016
Iteration 125/1000 | Loss: 0.00001016
Iteration 126/1000 | Loss: 0.00001016
Iteration 127/1000 | Loss: 0.00001016
Iteration 128/1000 | Loss: 0.00001016
Iteration 129/1000 | Loss: 0.00001016
Iteration 130/1000 | Loss: 0.00001015
Iteration 131/1000 | Loss: 0.00001015
Iteration 132/1000 | Loss: 0.00001015
Iteration 133/1000 | Loss: 0.00001015
Iteration 134/1000 | Loss: 0.00001015
Iteration 135/1000 | Loss: 0.00001015
Iteration 136/1000 | Loss: 0.00001015
Iteration 137/1000 | Loss: 0.00001015
Iteration 138/1000 | Loss: 0.00001015
Iteration 139/1000 | Loss: 0.00001014
Iteration 140/1000 | Loss: 0.00001014
Iteration 141/1000 | Loss: 0.00001014
Iteration 142/1000 | Loss: 0.00001014
Iteration 143/1000 | Loss: 0.00001014
Iteration 144/1000 | Loss: 0.00001014
Iteration 145/1000 | Loss: 0.00001014
Iteration 146/1000 | Loss: 0.00001014
Iteration 147/1000 | Loss: 0.00001014
Iteration 148/1000 | Loss: 0.00001014
Iteration 149/1000 | Loss: 0.00001014
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001014
Iteration 152/1000 | Loss: 0.00001013
Iteration 153/1000 | Loss: 0.00001013
Iteration 154/1000 | Loss: 0.00001013
Iteration 155/1000 | Loss: 0.00001013
Iteration 156/1000 | Loss: 0.00001013
Iteration 157/1000 | Loss: 0.00001012
Iteration 158/1000 | Loss: 0.00001012
Iteration 159/1000 | Loss: 0.00001012
Iteration 160/1000 | Loss: 0.00001012
Iteration 161/1000 | Loss: 0.00001012
Iteration 162/1000 | Loss: 0.00001012
Iteration 163/1000 | Loss: 0.00001012
Iteration 164/1000 | Loss: 0.00001012
Iteration 165/1000 | Loss: 0.00001011
Iteration 166/1000 | Loss: 0.00001011
Iteration 167/1000 | Loss: 0.00001011
Iteration 168/1000 | Loss: 0.00001011
Iteration 169/1000 | Loss: 0.00001011
Iteration 170/1000 | Loss: 0.00001010
Iteration 171/1000 | Loss: 0.00001010
Iteration 172/1000 | Loss: 0.00001010
Iteration 173/1000 | Loss: 0.00001010
Iteration 174/1000 | Loss: 0.00001010
Iteration 175/1000 | Loss: 0.00001010
Iteration 176/1000 | Loss: 0.00001010
Iteration 177/1000 | Loss: 0.00001010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.0103918612003326e-05, 1.0103918612003326e-05, 1.0103918612003326e-05, 1.0103918612003326e-05, 1.0103918612003326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0103918612003326e-05

Optimization complete. Final v2v error: 2.7222695350646973 mm

Highest mean error: 2.8076558113098145 mm for frame 62

Lowest mean error: 2.638359546661377 mm for frame 121

Saving results

Total time: 40.67233657836914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399944
Iteration 2/25 | Loss: 0.00125023
Iteration 3/25 | Loss: 0.00115992
Iteration 4/25 | Loss: 0.00115547
Iteration 5/25 | Loss: 0.00115547
Iteration 6/25 | Loss: 0.00115547
Iteration 7/25 | Loss: 0.00115547
Iteration 8/25 | Loss: 0.00115547
Iteration 9/25 | Loss: 0.00115547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.0011554707307368517, 0.0011554707307368517, 0.0011554707307368517, 0.0011554707307368517, 0.0011554707307368517]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011554707307368517

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54061806
Iteration 2/25 | Loss: 0.00069765
Iteration 3/25 | Loss: 0.00069765
Iteration 4/25 | Loss: 0.00069765
Iteration 5/25 | Loss: 0.00069765
Iteration 6/25 | Loss: 0.00069765
Iteration 7/25 | Loss: 0.00069765
Iteration 8/25 | Loss: 0.00069765
Iteration 9/25 | Loss: 0.00069765
Iteration 10/25 | Loss: 0.00069765
Iteration 11/25 | Loss: 0.00069765
Iteration 12/25 | Loss: 0.00069765
Iteration 13/25 | Loss: 0.00069765
Iteration 14/25 | Loss: 0.00069765
Iteration 15/25 | Loss: 0.00069765
Iteration 16/25 | Loss: 0.00069765
Iteration 17/25 | Loss: 0.00069765
Iteration 18/25 | Loss: 0.00069765
Iteration 19/25 | Loss: 0.00069765
Iteration 20/25 | Loss: 0.00069765
Iteration 21/25 | Loss: 0.00069765
Iteration 22/25 | Loss: 0.00069765
Iteration 23/25 | Loss: 0.00069765
Iteration 24/25 | Loss: 0.00069765
Iteration 25/25 | Loss: 0.00069765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069765
Iteration 2/1000 | Loss: 0.00002574
Iteration 3/1000 | Loss: 0.00001678
Iteration 4/1000 | Loss: 0.00001427
Iteration 5/1000 | Loss: 0.00001321
Iteration 6/1000 | Loss: 0.00001258
Iteration 7/1000 | Loss: 0.00001218
Iteration 8/1000 | Loss: 0.00001202
Iteration 9/1000 | Loss: 0.00001184
Iteration 10/1000 | Loss: 0.00001147
Iteration 11/1000 | Loss: 0.00001118
Iteration 12/1000 | Loss: 0.00001100
Iteration 13/1000 | Loss: 0.00001084
Iteration 14/1000 | Loss: 0.00001082
Iteration 15/1000 | Loss: 0.00001080
Iteration 16/1000 | Loss: 0.00001079
Iteration 17/1000 | Loss: 0.00001077
Iteration 18/1000 | Loss: 0.00001076
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001075
Iteration 21/1000 | Loss: 0.00001073
Iteration 22/1000 | Loss: 0.00001072
Iteration 23/1000 | Loss: 0.00001069
Iteration 24/1000 | Loss: 0.00001068
Iteration 25/1000 | Loss: 0.00001068
Iteration 26/1000 | Loss: 0.00001068
Iteration 27/1000 | Loss: 0.00001066
Iteration 28/1000 | Loss: 0.00001066
Iteration 29/1000 | Loss: 0.00001065
Iteration 30/1000 | Loss: 0.00001063
Iteration 31/1000 | Loss: 0.00001063
Iteration 32/1000 | Loss: 0.00001063
Iteration 33/1000 | Loss: 0.00001063
Iteration 34/1000 | Loss: 0.00001063
Iteration 35/1000 | Loss: 0.00001063
Iteration 36/1000 | Loss: 0.00001062
Iteration 37/1000 | Loss: 0.00001062
Iteration 38/1000 | Loss: 0.00001061
Iteration 39/1000 | Loss: 0.00001059
Iteration 40/1000 | Loss: 0.00001059
Iteration 41/1000 | Loss: 0.00001059
Iteration 42/1000 | Loss: 0.00001059
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001057
Iteration 47/1000 | Loss: 0.00001054
Iteration 48/1000 | Loss: 0.00001051
Iteration 49/1000 | Loss: 0.00001051
Iteration 50/1000 | Loss: 0.00001050
Iteration 51/1000 | Loss: 0.00001050
Iteration 52/1000 | Loss: 0.00001049
Iteration 53/1000 | Loss: 0.00001048
Iteration 54/1000 | Loss: 0.00001048
Iteration 55/1000 | Loss: 0.00001048
Iteration 56/1000 | Loss: 0.00001047
Iteration 57/1000 | Loss: 0.00001047
Iteration 58/1000 | Loss: 0.00001047
Iteration 59/1000 | Loss: 0.00001047
Iteration 60/1000 | Loss: 0.00001047
Iteration 61/1000 | Loss: 0.00001047
Iteration 62/1000 | Loss: 0.00001047
Iteration 63/1000 | Loss: 0.00001047
Iteration 64/1000 | Loss: 0.00001047
Iteration 65/1000 | Loss: 0.00001047
Iteration 66/1000 | Loss: 0.00001047
Iteration 67/1000 | Loss: 0.00001046
Iteration 68/1000 | Loss: 0.00001046
Iteration 69/1000 | Loss: 0.00001046
Iteration 70/1000 | Loss: 0.00001046
Iteration 71/1000 | Loss: 0.00001046
Iteration 72/1000 | Loss: 0.00001046
Iteration 73/1000 | Loss: 0.00001046
Iteration 74/1000 | Loss: 0.00001046
Iteration 75/1000 | Loss: 0.00001045
Iteration 76/1000 | Loss: 0.00001044
Iteration 77/1000 | Loss: 0.00001044
Iteration 78/1000 | Loss: 0.00001043
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001040
Iteration 81/1000 | Loss: 0.00001040
Iteration 82/1000 | Loss: 0.00001039
Iteration 83/1000 | Loss: 0.00001039
Iteration 84/1000 | Loss: 0.00001037
Iteration 85/1000 | Loss: 0.00001034
Iteration 86/1000 | Loss: 0.00001034
Iteration 87/1000 | Loss: 0.00001033
Iteration 88/1000 | Loss: 0.00001033
Iteration 89/1000 | Loss: 0.00001032
Iteration 90/1000 | Loss: 0.00001030
Iteration 91/1000 | Loss: 0.00001030
Iteration 92/1000 | Loss: 0.00001029
Iteration 93/1000 | Loss: 0.00001029
Iteration 94/1000 | Loss: 0.00001028
Iteration 95/1000 | Loss: 0.00001028
Iteration 96/1000 | Loss: 0.00001028
Iteration 97/1000 | Loss: 0.00001027
Iteration 98/1000 | Loss: 0.00001027
Iteration 99/1000 | Loss: 0.00001026
Iteration 100/1000 | Loss: 0.00001026
Iteration 101/1000 | Loss: 0.00001026
Iteration 102/1000 | Loss: 0.00001026
Iteration 103/1000 | Loss: 0.00001025
Iteration 104/1000 | Loss: 0.00001025
Iteration 105/1000 | Loss: 0.00001025
Iteration 106/1000 | Loss: 0.00001025
Iteration 107/1000 | Loss: 0.00001024
Iteration 108/1000 | Loss: 0.00001024
Iteration 109/1000 | Loss: 0.00001024
Iteration 110/1000 | Loss: 0.00001024
Iteration 111/1000 | Loss: 0.00001024
Iteration 112/1000 | Loss: 0.00001023
Iteration 113/1000 | Loss: 0.00001023
Iteration 114/1000 | Loss: 0.00001023
Iteration 115/1000 | Loss: 0.00001023
Iteration 116/1000 | Loss: 0.00001022
Iteration 117/1000 | Loss: 0.00001022
Iteration 118/1000 | Loss: 0.00001022
Iteration 119/1000 | Loss: 0.00001022
Iteration 120/1000 | Loss: 0.00001021
Iteration 121/1000 | Loss: 0.00001021
Iteration 122/1000 | Loss: 0.00001021
Iteration 123/1000 | Loss: 0.00001021
Iteration 124/1000 | Loss: 0.00001020
Iteration 125/1000 | Loss: 0.00001019
Iteration 126/1000 | Loss: 0.00001019
Iteration 127/1000 | Loss: 0.00001019
Iteration 128/1000 | Loss: 0.00001018
Iteration 129/1000 | Loss: 0.00001018
Iteration 130/1000 | Loss: 0.00001018
Iteration 131/1000 | Loss: 0.00001018
Iteration 132/1000 | Loss: 0.00001018
Iteration 133/1000 | Loss: 0.00001018
Iteration 134/1000 | Loss: 0.00001018
Iteration 135/1000 | Loss: 0.00001018
Iteration 136/1000 | Loss: 0.00001018
Iteration 137/1000 | Loss: 0.00001018
Iteration 138/1000 | Loss: 0.00001018
Iteration 139/1000 | Loss: 0.00001018
Iteration 140/1000 | Loss: 0.00001018
Iteration 141/1000 | Loss: 0.00001017
Iteration 142/1000 | Loss: 0.00001017
Iteration 143/1000 | Loss: 0.00001017
Iteration 144/1000 | Loss: 0.00001017
Iteration 145/1000 | Loss: 0.00001017
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001016
Iteration 148/1000 | Loss: 0.00001016
Iteration 149/1000 | Loss: 0.00001016
Iteration 150/1000 | Loss: 0.00001016
Iteration 151/1000 | Loss: 0.00001016
Iteration 152/1000 | Loss: 0.00001016
Iteration 153/1000 | Loss: 0.00001016
Iteration 154/1000 | Loss: 0.00001016
Iteration 155/1000 | Loss: 0.00001016
Iteration 156/1000 | Loss: 0.00001015
Iteration 157/1000 | Loss: 0.00001015
Iteration 158/1000 | Loss: 0.00001015
Iteration 159/1000 | Loss: 0.00001015
Iteration 160/1000 | Loss: 0.00001015
Iteration 161/1000 | Loss: 0.00001015
Iteration 162/1000 | Loss: 0.00001015
Iteration 163/1000 | Loss: 0.00001015
Iteration 164/1000 | Loss: 0.00001015
Iteration 165/1000 | Loss: 0.00001015
Iteration 166/1000 | Loss: 0.00001015
Iteration 167/1000 | Loss: 0.00001015
Iteration 168/1000 | Loss: 0.00001015
Iteration 169/1000 | Loss: 0.00001014
Iteration 170/1000 | Loss: 0.00001014
Iteration 171/1000 | Loss: 0.00001014
Iteration 172/1000 | Loss: 0.00001014
Iteration 173/1000 | Loss: 0.00001013
Iteration 174/1000 | Loss: 0.00001013
Iteration 175/1000 | Loss: 0.00001013
Iteration 176/1000 | Loss: 0.00001012
Iteration 177/1000 | Loss: 0.00001012
Iteration 178/1000 | Loss: 0.00001012
Iteration 179/1000 | Loss: 0.00001012
Iteration 180/1000 | Loss: 0.00001012
Iteration 181/1000 | Loss: 0.00001011
Iteration 182/1000 | Loss: 0.00001011
Iteration 183/1000 | Loss: 0.00001011
Iteration 184/1000 | Loss: 0.00001011
Iteration 185/1000 | Loss: 0.00001011
Iteration 186/1000 | Loss: 0.00001010
Iteration 187/1000 | Loss: 0.00001010
Iteration 188/1000 | Loss: 0.00001010
Iteration 189/1000 | Loss: 0.00001010
Iteration 190/1000 | Loss: 0.00001010
Iteration 191/1000 | Loss: 0.00001010
Iteration 192/1000 | Loss: 0.00001010
Iteration 193/1000 | Loss: 0.00001010
Iteration 194/1000 | Loss: 0.00001010
Iteration 195/1000 | Loss: 0.00001010
Iteration 196/1000 | Loss: 0.00001010
Iteration 197/1000 | Loss: 0.00001010
Iteration 198/1000 | Loss: 0.00001010
Iteration 199/1000 | Loss: 0.00001010
Iteration 200/1000 | Loss: 0.00001010
Iteration 201/1000 | Loss: 0.00001010
Iteration 202/1000 | Loss: 0.00001010
Iteration 203/1000 | Loss: 0.00001010
Iteration 204/1000 | Loss: 0.00001010
Iteration 205/1000 | Loss: 0.00001010
Iteration 206/1000 | Loss: 0.00001010
Iteration 207/1000 | Loss: 0.00001010
Iteration 208/1000 | Loss: 0.00001010
Iteration 209/1000 | Loss: 0.00001010
Iteration 210/1000 | Loss: 0.00001010
Iteration 211/1000 | Loss: 0.00001010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [1.0100230610987637e-05, 1.0100230610987637e-05, 1.0100230610987637e-05, 1.0100230610987637e-05, 1.0100230610987637e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0100230610987637e-05

Optimization complete. Final v2v error: 2.7288808822631836 mm

Highest mean error: 2.955626964569092 mm for frame 153

Lowest mean error: 2.5908243656158447 mm for frame 186

Saving results

Total time: 47.376402616500854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824464
Iteration 2/25 | Loss: 0.00173105
Iteration 3/25 | Loss: 0.00152268
Iteration 4/25 | Loss: 0.00145689
Iteration 5/25 | Loss: 0.00144820
Iteration 6/25 | Loss: 0.00144477
Iteration 7/25 | Loss: 0.00146406
Iteration 8/25 | Loss: 0.00144059
Iteration 9/25 | Loss: 0.00137415
Iteration 10/25 | Loss: 0.00136465
Iteration 11/25 | Loss: 0.00136763
Iteration 12/25 | Loss: 0.00135499
Iteration 13/25 | Loss: 0.00135791
Iteration 14/25 | Loss: 0.00133419
Iteration 15/25 | Loss: 0.00130285
Iteration 16/25 | Loss: 0.00128400
Iteration 17/25 | Loss: 0.00127413
Iteration 18/25 | Loss: 0.00126903
Iteration 19/25 | Loss: 0.00126576
Iteration 20/25 | Loss: 0.00126306
Iteration 21/25 | Loss: 0.00126252
Iteration 22/25 | Loss: 0.00125773
Iteration 23/25 | Loss: 0.00125660
Iteration 24/25 | Loss: 0.00125428
Iteration 25/25 | Loss: 0.00125378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32168221
Iteration 2/25 | Loss: 0.00087193
Iteration 3/25 | Loss: 0.00087190
Iteration 4/25 | Loss: 0.00087190
Iteration 5/25 | Loss: 0.00087190
Iteration 6/25 | Loss: 0.00087190
Iteration 7/25 | Loss: 0.00087190
Iteration 8/25 | Loss: 0.00087190
Iteration 9/25 | Loss: 0.00087190
Iteration 10/25 | Loss: 0.00087190
Iteration 11/25 | Loss: 0.00087190
Iteration 12/25 | Loss: 0.00087190
Iteration 13/25 | Loss: 0.00087190
Iteration 14/25 | Loss: 0.00087190
Iteration 15/25 | Loss: 0.00087190
Iteration 16/25 | Loss: 0.00087190
Iteration 17/25 | Loss: 0.00087190
Iteration 18/25 | Loss: 0.00087190
Iteration 19/25 | Loss: 0.00087190
Iteration 20/25 | Loss: 0.00087190
Iteration 21/25 | Loss: 0.00087190
Iteration 22/25 | Loss: 0.00087190
Iteration 23/25 | Loss: 0.00087190
Iteration 24/25 | Loss: 0.00087190
Iteration 25/25 | Loss: 0.00087190

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087190
Iteration 2/1000 | Loss: 0.00008176
Iteration 3/1000 | Loss: 0.00005607
Iteration 4/1000 | Loss: 0.00013881
Iteration 5/1000 | Loss: 0.00017199
Iteration 6/1000 | Loss: 0.00004481
Iteration 7/1000 | Loss: 0.00004734
Iteration 8/1000 | Loss: 0.00003742
Iteration 9/1000 | Loss: 0.00067091
Iteration 10/1000 | Loss: 0.00042468
Iteration 11/1000 | Loss: 0.00004771
Iteration 12/1000 | Loss: 0.00003705
Iteration 13/1000 | Loss: 0.00003418
Iteration 14/1000 | Loss: 0.00066027
Iteration 15/1000 | Loss: 0.00016128
Iteration 16/1000 | Loss: 0.00004201
Iteration 17/1000 | Loss: 0.00003739
Iteration 18/1000 | Loss: 0.00003465
Iteration 19/1000 | Loss: 0.00023548
Iteration 20/1000 | Loss: 0.00005461
Iteration 21/1000 | Loss: 0.00015594
Iteration 22/1000 | Loss: 0.00026486
Iteration 23/1000 | Loss: 0.00017890
Iteration 24/1000 | Loss: 0.00011226
Iteration 25/1000 | Loss: 0.00008561
Iteration 26/1000 | Loss: 0.00010043
Iteration 27/1000 | Loss: 0.00012113
Iteration 28/1000 | Loss: 0.00006685
Iteration 29/1000 | Loss: 0.00006246
Iteration 30/1000 | Loss: 0.00004226
Iteration 31/1000 | Loss: 0.00003635
Iteration 32/1000 | Loss: 0.00005271
Iteration 33/1000 | Loss: 0.00004373
Iteration 34/1000 | Loss: 0.00004196
Iteration 35/1000 | Loss: 0.00004355
Iteration 36/1000 | Loss: 0.00003982
Iteration 37/1000 | Loss: 0.00003964
Iteration 38/1000 | Loss: 0.00003873
Iteration 39/1000 | Loss: 0.00004107
Iteration 40/1000 | Loss: 0.00003898
Iteration 41/1000 | Loss: 0.00088862
Iteration 42/1000 | Loss: 0.00047668
Iteration 43/1000 | Loss: 0.00005172
Iteration 44/1000 | Loss: 0.00003760
Iteration 45/1000 | Loss: 0.00003249
Iteration 46/1000 | Loss: 0.00002906
Iteration 47/1000 | Loss: 0.00003263
Iteration 48/1000 | Loss: 0.00002798
Iteration 49/1000 | Loss: 0.00002648
Iteration 50/1000 | Loss: 0.00002559
Iteration 51/1000 | Loss: 0.00002485
Iteration 52/1000 | Loss: 0.00002694
Iteration 53/1000 | Loss: 0.00002527
Iteration 54/1000 | Loss: 0.00002686
Iteration 55/1000 | Loss: 0.00002481
Iteration 56/1000 | Loss: 0.00002568
Iteration 57/1000 | Loss: 0.00002419
Iteration 58/1000 | Loss: 0.00002505
Iteration 59/1000 | Loss: 0.00002395
Iteration 60/1000 | Loss: 0.00002367
Iteration 61/1000 | Loss: 0.00002356
Iteration 62/1000 | Loss: 0.00002344
Iteration 63/1000 | Loss: 0.00002344
Iteration 64/1000 | Loss: 0.00002343
Iteration 65/1000 | Loss: 0.00021244
Iteration 66/1000 | Loss: 0.00002351
Iteration 67/1000 | Loss: 0.00002259
Iteration 68/1000 | Loss: 0.00002228
Iteration 69/1000 | Loss: 0.00002196
Iteration 70/1000 | Loss: 0.00002174
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002156
Iteration 73/1000 | Loss: 0.00002155
Iteration 74/1000 | Loss: 0.00002149
Iteration 75/1000 | Loss: 0.00002149
Iteration 76/1000 | Loss: 0.00002148
Iteration 77/1000 | Loss: 0.00002148
Iteration 78/1000 | Loss: 0.00002148
Iteration 79/1000 | Loss: 0.00002147
Iteration 80/1000 | Loss: 0.00002146
Iteration 81/1000 | Loss: 0.00002144
Iteration 82/1000 | Loss: 0.00002144
Iteration 83/1000 | Loss: 0.00002144
Iteration 84/1000 | Loss: 0.00002144
Iteration 85/1000 | Loss: 0.00002143
Iteration 86/1000 | Loss: 0.00002143
Iteration 87/1000 | Loss: 0.00002143
Iteration 88/1000 | Loss: 0.00002141
Iteration 89/1000 | Loss: 0.00002137
Iteration 90/1000 | Loss: 0.00002136
Iteration 91/1000 | Loss: 0.00002135
Iteration 92/1000 | Loss: 0.00002134
Iteration 93/1000 | Loss: 0.00002133
Iteration 94/1000 | Loss: 0.00002132
Iteration 95/1000 | Loss: 0.00002129
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002125
Iteration 98/1000 | Loss: 0.00002124
Iteration 99/1000 | Loss: 0.00002124
Iteration 100/1000 | Loss: 0.00002124
Iteration 101/1000 | Loss: 0.00002123
Iteration 102/1000 | Loss: 0.00002123
Iteration 103/1000 | Loss: 0.00002123
Iteration 104/1000 | Loss: 0.00002122
Iteration 105/1000 | Loss: 0.00002122
Iteration 106/1000 | Loss: 0.00002122
Iteration 107/1000 | Loss: 0.00002122
Iteration 108/1000 | Loss: 0.00002121
Iteration 109/1000 | Loss: 0.00002120
Iteration 110/1000 | Loss: 0.00002120
Iteration 111/1000 | Loss: 0.00002120
Iteration 112/1000 | Loss: 0.00002119
Iteration 113/1000 | Loss: 0.00002118
Iteration 114/1000 | Loss: 0.00002118
Iteration 115/1000 | Loss: 0.00002118
Iteration 116/1000 | Loss: 0.00002117
Iteration 117/1000 | Loss: 0.00002117
Iteration 118/1000 | Loss: 0.00002117
Iteration 119/1000 | Loss: 0.00002117
Iteration 120/1000 | Loss: 0.00002117
Iteration 121/1000 | Loss: 0.00002117
Iteration 122/1000 | Loss: 0.00002117
Iteration 123/1000 | Loss: 0.00002117
Iteration 124/1000 | Loss: 0.00002116
Iteration 125/1000 | Loss: 0.00002116
Iteration 126/1000 | Loss: 0.00002116
Iteration 127/1000 | Loss: 0.00002116
Iteration 128/1000 | Loss: 0.00002116
Iteration 129/1000 | Loss: 0.00002116
Iteration 130/1000 | Loss: 0.00002116
Iteration 131/1000 | Loss: 0.00002116
Iteration 132/1000 | Loss: 0.00002116
Iteration 133/1000 | Loss: 0.00002116
Iteration 134/1000 | Loss: 0.00002116
Iteration 135/1000 | Loss: 0.00002115
Iteration 136/1000 | Loss: 0.00002115
Iteration 137/1000 | Loss: 0.00002115
Iteration 138/1000 | Loss: 0.00002115
Iteration 139/1000 | Loss: 0.00002115
Iteration 140/1000 | Loss: 0.00002115
Iteration 141/1000 | Loss: 0.00002115
Iteration 142/1000 | Loss: 0.00002115
Iteration 143/1000 | Loss: 0.00002115
Iteration 144/1000 | Loss: 0.00002114
Iteration 145/1000 | Loss: 0.00002114
Iteration 146/1000 | Loss: 0.00002114
Iteration 147/1000 | Loss: 0.00002114
Iteration 148/1000 | Loss: 0.00002114
Iteration 149/1000 | Loss: 0.00002114
Iteration 150/1000 | Loss: 0.00002114
Iteration 151/1000 | Loss: 0.00002114
Iteration 152/1000 | Loss: 0.00002114
Iteration 153/1000 | Loss: 0.00002114
Iteration 154/1000 | Loss: 0.00002114
Iteration 155/1000 | Loss: 0.00002114
Iteration 156/1000 | Loss: 0.00002114
Iteration 157/1000 | Loss: 0.00002114
Iteration 158/1000 | Loss: 0.00002114
Iteration 159/1000 | Loss: 0.00002114
Iteration 160/1000 | Loss: 0.00002113
Iteration 161/1000 | Loss: 0.00002113
Iteration 162/1000 | Loss: 0.00002113
Iteration 163/1000 | Loss: 0.00002113
Iteration 164/1000 | Loss: 0.00002113
Iteration 165/1000 | Loss: 0.00002113
Iteration 166/1000 | Loss: 0.00002113
Iteration 167/1000 | Loss: 0.00002113
Iteration 168/1000 | Loss: 0.00002113
Iteration 169/1000 | Loss: 0.00002113
Iteration 170/1000 | Loss: 0.00002113
Iteration 171/1000 | Loss: 0.00002113
Iteration 172/1000 | Loss: 0.00002113
Iteration 173/1000 | Loss: 0.00002113
Iteration 174/1000 | Loss: 0.00002113
Iteration 175/1000 | Loss: 0.00002113
Iteration 176/1000 | Loss: 0.00002113
Iteration 177/1000 | Loss: 0.00002113
Iteration 178/1000 | Loss: 0.00002113
Iteration 179/1000 | Loss: 0.00002113
Iteration 180/1000 | Loss: 0.00002113
Iteration 181/1000 | Loss: 0.00002113
Iteration 182/1000 | Loss: 0.00002113
Iteration 183/1000 | Loss: 0.00002113
Iteration 184/1000 | Loss: 0.00002113
Iteration 185/1000 | Loss: 0.00002113
Iteration 186/1000 | Loss: 0.00002113
Iteration 187/1000 | Loss: 0.00002113
Iteration 188/1000 | Loss: 0.00002113
Iteration 189/1000 | Loss: 0.00002113
Iteration 190/1000 | Loss: 0.00002113
Iteration 191/1000 | Loss: 0.00002113
Iteration 192/1000 | Loss: 0.00002113
Iteration 193/1000 | Loss: 0.00002113
Iteration 194/1000 | Loss: 0.00002113
Iteration 195/1000 | Loss: 0.00002113
Iteration 196/1000 | Loss: 0.00002113
Iteration 197/1000 | Loss: 0.00002113
Iteration 198/1000 | Loss: 0.00002113
Iteration 199/1000 | Loss: 0.00002113
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.1130234017618932e-05, 2.1130234017618932e-05, 2.1130234017618932e-05, 2.1130234017618932e-05, 2.1130234017618932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1130234017618932e-05

Optimization complete. Final v2v error: 3.823289632797241 mm

Highest mean error: 5.467752933502197 mm for frame 83

Lowest mean error: 3.5116701126098633 mm for frame 31

Saving results

Total time: 176.15395975112915
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00567128
Iteration 2/25 | Loss: 0.00129577
Iteration 3/25 | Loss: 0.00120754
Iteration 4/25 | Loss: 0.00119026
Iteration 5/25 | Loss: 0.00118486
Iteration 6/25 | Loss: 0.00118325
Iteration 7/25 | Loss: 0.00118265
Iteration 8/25 | Loss: 0.00118265
Iteration 9/25 | Loss: 0.00118265
Iteration 10/25 | Loss: 0.00118265
Iteration 11/25 | Loss: 0.00118265
Iteration 12/25 | Loss: 0.00118265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011826478876173496, 0.0011826478876173496, 0.0011826478876173496, 0.0011826478876173496, 0.0011826478876173496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011826478876173496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47597158
Iteration 2/25 | Loss: 0.00096728
Iteration 3/25 | Loss: 0.00096728
Iteration 4/25 | Loss: 0.00096728
Iteration 5/25 | Loss: 0.00096728
Iteration 6/25 | Loss: 0.00096728
Iteration 7/25 | Loss: 0.00096728
Iteration 8/25 | Loss: 0.00096728
Iteration 9/25 | Loss: 0.00096728
Iteration 10/25 | Loss: 0.00096728
Iteration 11/25 | Loss: 0.00096728
Iteration 12/25 | Loss: 0.00096728
Iteration 13/25 | Loss: 0.00096728
Iteration 14/25 | Loss: 0.00096728
Iteration 15/25 | Loss: 0.00096728
Iteration 16/25 | Loss: 0.00096728
Iteration 17/25 | Loss: 0.00096728
Iteration 18/25 | Loss: 0.00096728
Iteration 19/25 | Loss: 0.00096728
Iteration 20/25 | Loss: 0.00096728
Iteration 21/25 | Loss: 0.00096728
Iteration 22/25 | Loss: 0.00096728
Iteration 23/25 | Loss: 0.00096728
Iteration 24/25 | Loss: 0.00096728
Iteration 25/25 | Loss: 0.00096728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096728
Iteration 2/1000 | Loss: 0.00003209
Iteration 3/1000 | Loss: 0.00002111
Iteration 4/1000 | Loss: 0.00001691
Iteration 5/1000 | Loss: 0.00001571
Iteration 6/1000 | Loss: 0.00001515
Iteration 7/1000 | Loss: 0.00001484
Iteration 8/1000 | Loss: 0.00001444
Iteration 9/1000 | Loss: 0.00001418
Iteration 10/1000 | Loss: 0.00001411
Iteration 11/1000 | Loss: 0.00001395
Iteration 12/1000 | Loss: 0.00001378
Iteration 13/1000 | Loss: 0.00001376
Iteration 14/1000 | Loss: 0.00001366
Iteration 15/1000 | Loss: 0.00001364
Iteration 16/1000 | Loss: 0.00001347
Iteration 17/1000 | Loss: 0.00001344
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001335
Iteration 20/1000 | Loss: 0.00001334
Iteration 21/1000 | Loss: 0.00001334
Iteration 22/1000 | Loss: 0.00001330
Iteration 23/1000 | Loss: 0.00001326
Iteration 24/1000 | Loss: 0.00001325
Iteration 25/1000 | Loss: 0.00001325
Iteration 26/1000 | Loss: 0.00001322
Iteration 27/1000 | Loss: 0.00001318
Iteration 28/1000 | Loss: 0.00001318
Iteration 29/1000 | Loss: 0.00001317
Iteration 30/1000 | Loss: 0.00001317
Iteration 31/1000 | Loss: 0.00001317
Iteration 32/1000 | Loss: 0.00001317
Iteration 33/1000 | Loss: 0.00001316
Iteration 34/1000 | Loss: 0.00001315
Iteration 35/1000 | Loss: 0.00001315
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001312
Iteration 39/1000 | Loss: 0.00001312
Iteration 40/1000 | Loss: 0.00001312
Iteration 41/1000 | Loss: 0.00001312
Iteration 42/1000 | Loss: 0.00001312
Iteration 43/1000 | Loss: 0.00001312
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001312
Iteration 48/1000 | Loss: 0.00001312
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001311
Iteration 52/1000 | Loss: 0.00001310
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001309
Iteration 56/1000 | Loss: 0.00001309
Iteration 57/1000 | Loss: 0.00001308
Iteration 58/1000 | Loss: 0.00001308
Iteration 59/1000 | Loss: 0.00001308
Iteration 60/1000 | Loss: 0.00001308
Iteration 61/1000 | Loss: 0.00001308
Iteration 62/1000 | Loss: 0.00001307
Iteration 63/1000 | Loss: 0.00001307
Iteration 64/1000 | Loss: 0.00001306
Iteration 65/1000 | Loss: 0.00001306
Iteration 66/1000 | Loss: 0.00001306
Iteration 67/1000 | Loss: 0.00001305
Iteration 68/1000 | Loss: 0.00001305
Iteration 69/1000 | Loss: 0.00001305
Iteration 70/1000 | Loss: 0.00001305
Iteration 71/1000 | Loss: 0.00001304
Iteration 72/1000 | Loss: 0.00001304
Iteration 73/1000 | Loss: 0.00001304
Iteration 74/1000 | Loss: 0.00001304
Iteration 75/1000 | Loss: 0.00001304
Iteration 76/1000 | Loss: 0.00001304
Iteration 77/1000 | Loss: 0.00001303
Iteration 78/1000 | Loss: 0.00001303
Iteration 79/1000 | Loss: 0.00001303
Iteration 80/1000 | Loss: 0.00001303
Iteration 81/1000 | Loss: 0.00001302
Iteration 82/1000 | Loss: 0.00001302
Iteration 83/1000 | Loss: 0.00001302
Iteration 84/1000 | Loss: 0.00001301
Iteration 85/1000 | Loss: 0.00001301
Iteration 86/1000 | Loss: 0.00001301
Iteration 87/1000 | Loss: 0.00001301
Iteration 88/1000 | Loss: 0.00001301
Iteration 89/1000 | Loss: 0.00001300
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001299
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001298
Iteration 99/1000 | Loss: 0.00001298
Iteration 100/1000 | Loss: 0.00001298
Iteration 101/1000 | Loss: 0.00001298
Iteration 102/1000 | Loss: 0.00001298
Iteration 103/1000 | Loss: 0.00001298
Iteration 104/1000 | Loss: 0.00001298
Iteration 105/1000 | Loss: 0.00001298
Iteration 106/1000 | Loss: 0.00001298
Iteration 107/1000 | Loss: 0.00001298
Iteration 108/1000 | Loss: 0.00001297
Iteration 109/1000 | Loss: 0.00001297
Iteration 110/1000 | Loss: 0.00001297
Iteration 111/1000 | Loss: 0.00001297
Iteration 112/1000 | Loss: 0.00001297
Iteration 113/1000 | Loss: 0.00001297
Iteration 114/1000 | Loss: 0.00001297
Iteration 115/1000 | Loss: 0.00001297
Iteration 116/1000 | Loss: 0.00001297
Iteration 117/1000 | Loss: 0.00001296
Iteration 118/1000 | Loss: 0.00001296
Iteration 119/1000 | Loss: 0.00001296
Iteration 120/1000 | Loss: 0.00001296
Iteration 121/1000 | Loss: 0.00001296
Iteration 122/1000 | Loss: 0.00001296
Iteration 123/1000 | Loss: 0.00001296
Iteration 124/1000 | Loss: 0.00001296
Iteration 125/1000 | Loss: 0.00001296
Iteration 126/1000 | Loss: 0.00001296
Iteration 127/1000 | Loss: 0.00001296
Iteration 128/1000 | Loss: 0.00001296
Iteration 129/1000 | Loss: 0.00001296
Iteration 130/1000 | Loss: 0.00001296
Iteration 131/1000 | Loss: 0.00001296
Iteration 132/1000 | Loss: 0.00001296
Iteration 133/1000 | Loss: 0.00001296
Iteration 134/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.2963534572918434e-05, 1.2963534572918434e-05, 1.2963534572918434e-05, 1.2963534572918434e-05, 1.2963534572918434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2963534572918434e-05

Optimization complete. Final v2v error: 3.0697457790374756 mm

Highest mean error: 3.5115020275115967 mm for frame 1

Lowest mean error: 2.750943183898926 mm for frame 22

Saving results

Total time: 38.870771169662476
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827435
Iteration 2/25 | Loss: 0.00146198
Iteration 3/25 | Loss: 0.00123058
Iteration 4/25 | Loss: 0.00121631
Iteration 5/25 | Loss: 0.00122368
Iteration 6/25 | Loss: 0.00121098
Iteration 7/25 | Loss: 0.00120347
Iteration 8/25 | Loss: 0.00120022
Iteration 9/25 | Loss: 0.00119986
Iteration 10/25 | Loss: 0.00119975
Iteration 11/25 | Loss: 0.00119975
Iteration 12/25 | Loss: 0.00119975
Iteration 13/25 | Loss: 0.00119975
Iteration 14/25 | Loss: 0.00119975
Iteration 15/25 | Loss: 0.00119975
Iteration 16/25 | Loss: 0.00119975
Iteration 17/25 | Loss: 0.00119975
Iteration 18/25 | Loss: 0.00119974
Iteration 19/25 | Loss: 0.00119974
Iteration 20/25 | Loss: 0.00119974
Iteration 21/25 | Loss: 0.00119974
Iteration 22/25 | Loss: 0.00119974
Iteration 23/25 | Loss: 0.00119974
Iteration 24/25 | Loss: 0.00119974
Iteration 25/25 | Loss: 0.00119974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33667123
Iteration 2/25 | Loss: 0.00068373
Iteration 3/25 | Loss: 0.00068370
Iteration 4/25 | Loss: 0.00068370
Iteration 5/25 | Loss: 0.00068370
Iteration 6/25 | Loss: 0.00068370
Iteration 7/25 | Loss: 0.00068370
Iteration 8/25 | Loss: 0.00068370
Iteration 9/25 | Loss: 0.00068370
Iteration 10/25 | Loss: 0.00068369
Iteration 11/25 | Loss: 0.00068369
Iteration 12/25 | Loss: 0.00068369
Iteration 13/25 | Loss: 0.00068369
Iteration 14/25 | Loss: 0.00068369
Iteration 15/25 | Loss: 0.00068369
Iteration 16/25 | Loss: 0.00068369
Iteration 17/25 | Loss: 0.00068369
Iteration 18/25 | Loss: 0.00068369
Iteration 19/25 | Loss: 0.00068369
Iteration 20/25 | Loss: 0.00068369
Iteration 21/25 | Loss: 0.00068369
Iteration 22/25 | Loss: 0.00068369
Iteration 23/25 | Loss: 0.00068369
Iteration 24/25 | Loss: 0.00068369
Iteration 25/25 | Loss: 0.00068369

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068369
Iteration 2/1000 | Loss: 0.00003661
Iteration 3/1000 | Loss: 0.00002701
Iteration 4/1000 | Loss: 0.00002419
Iteration 5/1000 | Loss: 0.00002283
Iteration 6/1000 | Loss: 0.00002182
Iteration 7/1000 | Loss: 0.00002113
Iteration 8/1000 | Loss: 0.00002065
Iteration 9/1000 | Loss: 0.00002024
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001964
Iteration 12/1000 | Loss: 0.00001940
Iteration 13/1000 | Loss: 0.00001922
Iteration 14/1000 | Loss: 0.00001920
Iteration 15/1000 | Loss: 0.00001915
Iteration 16/1000 | Loss: 0.00001914
Iteration 17/1000 | Loss: 0.00001914
Iteration 18/1000 | Loss: 0.00001908
Iteration 19/1000 | Loss: 0.00001904
Iteration 20/1000 | Loss: 0.00001904
Iteration 21/1000 | Loss: 0.00001899
Iteration 22/1000 | Loss: 0.00001896
Iteration 23/1000 | Loss: 0.00001896
Iteration 24/1000 | Loss: 0.00001891
Iteration 25/1000 | Loss: 0.00001890
Iteration 26/1000 | Loss: 0.00001889
Iteration 27/1000 | Loss: 0.00001888
Iteration 28/1000 | Loss: 0.00001888
Iteration 29/1000 | Loss: 0.00001888
Iteration 30/1000 | Loss: 0.00001887
Iteration 31/1000 | Loss: 0.00001887
Iteration 32/1000 | Loss: 0.00001884
Iteration 33/1000 | Loss: 0.00001884
Iteration 34/1000 | Loss: 0.00001883
Iteration 35/1000 | Loss: 0.00001881
Iteration 36/1000 | Loss: 0.00001881
Iteration 37/1000 | Loss: 0.00001880
Iteration 38/1000 | Loss: 0.00001880
Iteration 39/1000 | Loss: 0.00001879
Iteration 40/1000 | Loss: 0.00001879
Iteration 41/1000 | Loss: 0.00001879
Iteration 42/1000 | Loss: 0.00001876
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001876
Iteration 45/1000 | Loss: 0.00001876
Iteration 46/1000 | Loss: 0.00001876
Iteration 47/1000 | Loss: 0.00001876
Iteration 48/1000 | Loss: 0.00001876
Iteration 49/1000 | Loss: 0.00001876
Iteration 50/1000 | Loss: 0.00001876
Iteration 51/1000 | Loss: 0.00001875
Iteration 52/1000 | Loss: 0.00001875
Iteration 53/1000 | Loss: 0.00001875
Iteration 54/1000 | Loss: 0.00001875
Iteration 55/1000 | Loss: 0.00001875
Iteration 56/1000 | Loss: 0.00001875
Iteration 57/1000 | Loss: 0.00001875
Iteration 58/1000 | Loss: 0.00001875
Iteration 59/1000 | Loss: 0.00001875
Iteration 60/1000 | Loss: 0.00001875
Iteration 61/1000 | Loss: 0.00001875
Iteration 62/1000 | Loss: 0.00001874
Iteration 63/1000 | Loss: 0.00001874
Iteration 64/1000 | Loss: 0.00001874
Iteration 65/1000 | Loss: 0.00001873
Iteration 66/1000 | Loss: 0.00001873
Iteration 67/1000 | Loss: 0.00001872
Iteration 68/1000 | Loss: 0.00001872
Iteration 69/1000 | Loss: 0.00001871
Iteration 70/1000 | Loss: 0.00001871
Iteration 71/1000 | Loss: 0.00001871
Iteration 72/1000 | Loss: 0.00001871
Iteration 73/1000 | Loss: 0.00001870
Iteration 74/1000 | Loss: 0.00001870
Iteration 75/1000 | Loss: 0.00001869
Iteration 76/1000 | Loss: 0.00001869
Iteration 77/1000 | Loss: 0.00001868
Iteration 78/1000 | Loss: 0.00001868
Iteration 79/1000 | Loss: 0.00001868
Iteration 80/1000 | Loss: 0.00001867
Iteration 81/1000 | Loss: 0.00001867
Iteration 82/1000 | Loss: 0.00001867
Iteration 83/1000 | Loss: 0.00001867
Iteration 84/1000 | Loss: 0.00001866
Iteration 85/1000 | Loss: 0.00001866
Iteration 86/1000 | Loss: 0.00001866
Iteration 87/1000 | Loss: 0.00001866
Iteration 88/1000 | Loss: 0.00001865
Iteration 89/1000 | Loss: 0.00001865
Iteration 90/1000 | Loss: 0.00001865
Iteration 91/1000 | Loss: 0.00001865
Iteration 92/1000 | Loss: 0.00001864
Iteration 93/1000 | Loss: 0.00001864
Iteration 94/1000 | Loss: 0.00001864
Iteration 95/1000 | Loss: 0.00001864
Iteration 96/1000 | Loss: 0.00001864
Iteration 97/1000 | Loss: 0.00001864
Iteration 98/1000 | Loss: 0.00001863
Iteration 99/1000 | Loss: 0.00001863
Iteration 100/1000 | Loss: 0.00001863
Iteration 101/1000 | Loss: 0.00001863
Iteration 102/1000 | Loss: 0.00001863
Iteration 103/1000 | Loss: 0.00001862
Iteration 104/1000 | Loss: 0.00001862
Iteration 105/1000 | Loss: 0.00001862
Iteration 106/1000 | Loss: 0.00001862
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001860
Iteration 113/1000 | Loss: 0.00001859
Iteration 114/1000 | Loss: 0.00001859
Iteration 115/1000 | Loss: 0.00001859
Iteration 116/1000 | Loss: 0.00001859
Iteration 117/1000 | Loss: 0.00001859
Iteration 118/1000 | Loss: 0.00001859
Iteration 119/1000 | Loss: 0.00001858
Iteration 120/1000 | Loss: 0.00001858
Iteration 121/1000 | Loss: 0.00001858
Iteration 122/1000 | Loss: 0.00001858
Iteration 123/1000 | Loss: 0.00001858
Iteration 124/1000 | Loss: 0.00001857
Iteration 125/1000 | Loss: 0.00001857
Iteration 126/1000 | Loss: 0.00001857
Iteration 127/1000 | Loss: 0.00001857
Iteration 128/1000 | Loss: 0.00001857
Iteration 129/1000 | Loss: 0.00001857
Iteration 130/1000 | Loss: 0.00001857
Iteration 131/1000 | Loss: 0.00001857
Iteration 132/1000 | Loss: 0.00001857
Iteration 133/1000 | Loss: 0.00001857
Iteration 134/1000 | Loss: 0.00001857
Iteration 135/1000 | Loss: 0.00001857
Iteration 136/1000 | Loss: 0.00001857
Iteration 137/1000 | Loss: 0.00001856
Iteration 138/1000 | Loss: 0.00001856
Iteration 139/1000 | Loss: 0.00001856
Iteration 140/1000 | Loss: 0.00001856
Iteration 141/1000 | Loss: 0.00001855
Iteration 142/1000 | Loss: 0.00001855
Iteration 143/1000 | Loss: 0.00001855
Iteration 144/1000 | Loss: 0.00001854
Iteration 145/1000 | Loss: 0.00001854
Iteration 146/1000 | Loss: 0.00001854
Iteration 147/1000 | Loss: 0.00001854
Iteration 148/1000 | Loss: 0.00001854
Iteration 149/1000 | Loss: 0.00001853
Iteration 150/1000 | Loss: 0.00001853
Iteration 151/1000 | Loss: 0.00001853
Iteration 152/1000 | Loss: 0.00001853
Iteration 153/1000 | Loss: 0.00001853
Iteration 154/1000 | Loss: 0.00001853
Iteration 155/1000 | Loss: 0.00001852
Iteration 156/1000 | Loss: 0.00001852
Iteration 157/1000 | Loss: 0.00001852
Iteration 158/1000 | Loss: 0.00001852
Iteration 159/1000 | Loss: 0.00001852
Iteration 160/1000 | Loss: 0.00001852
Iteration 161/1000 | Loss: 0.00001852
Iteration 162/1000 | Loss: 0.00001852
Iteration 163/1000 | Loss: 0.00001852
Iteration 164/1000 | Loss: 0.00001852
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001851
Iteration 169/1000 | Loss: 0.00001851
Iteration 170/1000 | Loss: 0.00001851
Iteration 171/1000 | Loss: 0.00001851
Iteration 172/1000 | Loss: 0.00001851
Iteration 173/1000 | Loss: 0.00001851
Iteration 174/1000 | Loss: 0.00001851
Iteration 175/1000 | Loss: 0.00001851
Iteration 176/1000 | Loss: 0.00001851
Iteration 177/1000 | Loss: 0.00001851
Iteration 178/1000 | Loss: 0.00001851
Iteration 179/1000 | Loss: 0.00001851
Iteration 180/1000 | Loss: 0.00001851
Iteration 181/1000 | Loss: 0.00001851
Iteration 182/1000 | Loss: 0.00001851
Iteration 183/1000 | Loss: 0.00001851
Iteration 184/1000 | Loss: 0.00001851
Iteration 185/1000 | Loss: 0.00001851
Iteration 186/1000 | Loss: 0.00001850
Iteration 187/1000 | Loss: 0.00001850
Iteration 188/1000 | Loss: 0.00001850
Iteration 189/1000 | Loss: 0.00001850
Iteration 190/1000 | Loss: 0.00001850
Iteration 191/1000 | Loss: 0.00001850
Iteration 192/1000 | Loss: 0.00001850
Iteration 193/1000 | Loss: 0.00001850
Iteration 194/1000 | Loss: 0.00001850
Iteration 195/1000 | Loss: 0.00001850
Iteration 196/1000 | Loss: 0.00001850
Iteration 197/1000 | Loss: 0.00001850
Iteration 198/1000 | Loss: 0.00001850
Iteration 199/1000 | Loss: 0.00001850
Iteration 200/1000 | Loss: 0.00001850
Iteration 201/1000 | Loss: 0.00001850
Iteration 202/1000 | Loss: 0.00001850
Iteration 203/1000 | Loss: 0.00001850
Iteration 204/1000 | Loss: 0.00001850
Iteration 205/1000 | Loss: 0.00001850
Iteration 206/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.850387161539402e-05, 1.850387161539402e-05, 1.850387161539402e-05, 1.850387161539402e-05, 1.850387161539402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.850387161539402e-05

Optimization complete. Final v2v error: 3.5610291957855225 mm

Highest mean error: 4.563669681549072 mm for frame 32

Lowest mean error: 2.922374963760376 mm for frame 72

Saving results

Total time: 61.58844304084778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023162
Iteration 2/25 | Loss: 0.00389395
Iteration 3/25 | Loss: 0.00303592
Iteration 4/25 | Loss: 0.00282606
Iteration 5/25 | Loss: 0.00233543
Iteration 6/25 | Loss: 0.00225084
Iteration 7/25 | Loss: 0.00213940
Iteration 8/25 | Loss: 0.00207387
Iteration 9/25 | Loss: 0.00194564
Iteration 10/25 | Loss: 0.00195735
Iteration 11/25 | Loss: 0.00189236
Iteration 12/25 | Loss: 0.00188041
Iteration 13/25 | Loss: 0.00184816
Iteration 14/25 | Loss: 0.00183228
Iteration 15/25 | Loss: 0.00179804
Iteration 16/25 | Loss: 0.00178906
Iteration 17/25 | Loss: 0.00179717
Iteration 18/25 | Loss: 0.00177985
Iteration 19/25 | Loss: 0.00176557
Iteration 20/25 | Loss: 0.00177424
Iteration 21/25 | Loss: 0.00177002
Iteration 22/25 | Loss: 0.00175861
Iteration 23/25 | Loss: 0.00174303
Iteration 24/25 | Loss: 0.00173974
Iteration 25/25 | Loss: 0.00173953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34670794
Iteration 2/25 | Loss: 0.00464500
Iteration 3/25 | Loss: 0.00374153
Iteration 4/25 | Loss: 0.00374152
Iteration 5/25 | Loss: 0.00374152
Iteration 6/25 | Loss: 0.00374152
Iteration 7/25 | Loss: 0.00374152
Iteration 8/25 | Loss: 0.00374152
Iteration 9/25 | Loss: 0.00374152
Iteration 10/25 | Loss: 0.00374152
Iteration 11/25 | Loss: 0.00374152
Iteration 12/25 | Loss: 0.00374152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0037415206898003817, 0.0037415206898003817, 0.0037415206898003817, 0.0037415206898003817, 0.0037415206898003817]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037415206898003817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00374152
Iteration 2/1000 | Loss: 0.00159377
Iteration 3/1000 | Loss: 0.00148363
Iteration 4/1000 | Loss: 0.00180155
Iteration 5/1000 | Loss: 0.00159938
Iteration 6/1000 | Loss: 0.00075293
Iteration 7/1000 | Loss: 0.00028876
Iteration 8/1000 | Loss: 0.00088810
Iteration 9/1000 | Loss: 0.00060289
Iteration 10/1000 | Loss: 0.00095664
Iteration 11/1000 | Loss: 0.00037692
Iteration 12/1000 | Loss: 0.00058757
Iteration 13/1000 | Loss: 0.00036865
Iteration 14/1000 | Loss: 0.00021355
Iteration 15/1000 | Loss: 0.00019620
Iteration 16/1000 | Loss: 0.00024447
Iteration 17/1000 | Loss: 0.00037079
Iteration 18/1000 | Loss: 0.00023010
Iteration 19/1000 | Loss: 0.00018067
Iteration 20/1000 | Loss: 0.00039893
Iteration 21/1000 | Loss: 0.00021652
Iteration 22/1000 | Loss: 0.00017494
Iteration 23/1000 | Loss: 0.00032090
Iteration 24/1000 | Loss: 0.00058928
Iteration 25/1000 | Loss: 0.00154087
Iteration 26/1000 | Loss: 0.00484401
Iteration 27/1000 | Loss: 0.00076883
Iteration 28/1000 | Loss: 0.00095264
Iteration 29/1000 | Loss: 0.00137182
Iteration 30/1000 | Loss: 0.00040024
Iteration 31/1000 | Loss: 0.00017263
Iteration 32/1000 | Loss: 0.00016857
Iteration 33/1000 | Loss: 0.00032002
Iteration 34/1000 | Loss: 0.00037025
Iteration 35/1000 | Loss: 0.00020334
Iteration 36/1000 | Loss: 0.00043030
Iteration 37/1000 | Loss: 0.00041503
Iteration 38/1000 | Loss: 0.00020705
Iteration 39/1000 | Loss: 0.00027761
Iteration 40/1000 | Loss: 0.00016575
Iteration 41/1000 | Loss: 0.00018950
Iteration 42/1000 | Loss: 0.00016359
Iteration 43/1000 | Loss: 0.00053615
Iteration 44/1000 | Loss: 0.00016667
Iteration 45/1000 | Loss: 0.00030780
Iteration 46/1000 | Loss: 0.00111009
Iteration 47/1000 | Loss: 0.00127504
Iteration 48/1000 | Loss: 0.00050960
Iteration 49/1000 | Loss: 0.00016219
Iteration 50/1000 | Loss: 0.00016517
Iteration 51/1000 | Loss: 0.00048142
Iteration 52/1000 | Loss: 0.00120426
Iteration 53/1000 | Loss: 0.00021620
Iteration 54/1000 | Loss: 0.00018760
Iteration 55/1000 | Loss: 0.00015394
Iteration 56/1000 | Loss: 0.00037740
Iteration 57/1000 | Loss: 0.00045154
Iteration 58/1000 | Loss: 0.00075475
Iteration 59/1000 | Loss: 0.00096296
Iteration 60/1000 | Loss: 0.00111232
Iteration 61/1000 | Loss: 0.00491107
Iteration 62/1000 | Loss: 0.00832043
Iteration 63/1000 | Loss: 0.00196132
Iteration 64/1000 | Loss: 0.00261078
Iteration 65/1000 | Loss: 0.00177086
Iteration 66/1000 | Loss: 0.00048179
Iteration 67/1000 | Loss: 0.00033334
Iteration 68/1000 | Loss: 0.00027917
Iteration 69/1000 | Loss: 0.00019972
Iteration 70/1000 | Loss: 0.00119746
Iteration 71/1000 | Loss: 0.00025146
Iteration 72/1000 | Loss: 0.00084328
Iteration 73/1000 | Loss: 0.00013522
Iteration 74/1000 | Loss: 0.00010026
Iteration 75/1000 | Loss: 0.00136577
Iteration 76/1000 | Loss: 0.00120623
Iteration 77/1000 | Loss: 0.00047943
Iteration 78/1000 | Loss: 0.00083304
Iteration 79/1000 | Loss: 0.00184881
Iteration 80/1000 | Loss: 0.00013150
Iteration 81/1000 | Loss: 0.00022551
Iteration 82/1000 | Loss: 0.00007884
Iteration 83/1000 | Loss: 0.00040067
Iteration 84/1000 | Loss: 0.00007309
Iteration 85/1000 | Loss: 0.00006945
Iteration 86/1000 | Loss: 0.00048630
Iteration 87/1000 | Loss: 0.00010747
Iteration 88/1000 | Loss: 0.00006471
Iteration 89/1000 | Loss: 0.00006290
Iteration 90/1000 | Loss: 0.00010237
Iteration 91/1000 | Loss: 0.00006062
Iteration 92/1000 | Loss: 0.00005960
Iteration 93/1000 | Loss: 0.00009424
Iteration 94/1000 | Loss: 0.00040971
Iteration 95/1000 | Loss: 0.00006740
Iteration 96/1000 | Loss: 0.00061232
Iteration 97/1000 | Loss: 0.00019244
Iteration 98/1000 | Loss: 0.00005760
Iteration 99/1000 | Loss: 0.00037473
Iteration 100/1000 | Loss: 0.00005772
Iteration 101/1000 | Loss: 0.00005694
Iteration 102/1000 | Loss: 0.00005665
Iteration 103/1000 | Loss: 0.00005640
Iteration 104/1000 | Loss: 0.00005622
Iteration 105/1000 | Loss: 0.00009998
Iteration 106/1000 | Loss: 0.00056744
Iteration 107/1000 | Loss: 0.00006321
Iteration 108/1000 | Loss: 0.00024537
Iteration 109/1000 | Loss: 0.00005780
Iteration 110/1000 | Loss: 0.00007033
Iteration 111/1000 | Loss: 0.00005598
Iteration 112/1000 | Loss: 0.00005570
Iteration 113/1000 | Loss: 0.00005569
Iteration 114/1000 | Loss: 0.00005568
Iteration 115/1000 | Loss: 0.00005562
Iteration 116/1000 | Loss: 0.00005561
Iteration 117/1000 | Loss: 0.00005560
Iteration 118/1000 | Loss: 0.00005560
Iteration 119/1000 | Loss: 0.00005560
Iteration 120/1000 | Loss: 0.00005560
Iteration 121/1000 | Loss: 0.00005559
Iteration 122/1000 | Loss: 0.00005559
Iteration 123/1000 | Loss: 0.00005558
Iteration 124/1000 | Loss: 0.00005557
Iteration 125/1000 | Loss: 0.00005552
Iteration 126/1000 | Loss: 0.00005540
Iteration 127/1000 | Loss: 0.00005531
Iteration 128/1000 | Loss: 0.00005526
Iteration 129/1000 | Loss: 0.00005526
Iteration 130/1000 | Loss: 0.00005525
Iteration 131/1000 | Loss: 0.00005525
Iteration 132/1000 | Loss: 0.00005520
Iteration 133/1000 | Loss: 0.00005519
Iteration 134/1000 | Loss: 0.00005518
Iteration 135/1000 | Loss: 0.00005517
Iteration 136/1000 | Loss: 0.00005517
Iteration 137/1000 | Loss: 0.00005515
Iteration 138/1000 | Loss: 0.00005515
Iteration 139/1000 | Loss: 0.00005512
Iteration 140/1000 | Loss: 0.00005512
Iteration 141/1000 | Loss: 0.00005512
Iteration 142/1000 | Loss: 0.00005512
Iteration 143/1000 | Loss: 0.00005512
Iteration 144/1000 | Loss: 0.00005511
Iteration 145/1000 | Loss: 0.00005511
Iteration 146/1000 | Loss: 0.00005511
Iteration 147/1000 | Loss: 0.00005511
Iteration 148/1000 | Loss: 0.00005511
Iteration 149/1000 | Loss: 0.00005511
Iteration 150/1000 | Loss: 0.00005511
Iteration 151/1000 | Loss: 0.00005511
Iteration 152/1000 | Loss: 0.00005511
Iteration 153/1000 | Loss: 0.00005510
Iteration 154/1000 | Loss: 0.00005509
Iteration 155/1000 | Loss: 0.00005509
Iteration 156/1000 | Loss: 0.00005508
Iteration 157/1000 | Loss: 0.00005508
Iteration 158/1000 | Loss: 0.00005508
Iteration 159/1000 | Loss: 0.00005508
Iteration 160/1000 | Loss: 0.00005508
Iteration 161/1000 | Loss: 0.00005508
Iteration 162/1000 | Loss: 0.00005508
Iteration 163/1000 | Loss: 0.00005507
Iteration 164/1000 | Loss: 0.00005507
Iteration 165/1000 | Loss: 0.00005506
Iteration 166/1000 | Loss: 0.00005506
Iteration 167/1000 | Loss: 0.00005506
Iteration 168/1000 | Loss: 0.00005506
Iteration 169/1000 | Loss: 0.00005506
Iteration 170/1000 | Loss: 0.00005506
Iteration 171/1000 | Loss: 0.00005506
Iteration 172/1000 | Loss: 0.00005506
Iteration 173/1000 | Loss: 0.00005506
Iteration 174/1000 | Loss: 0.00005506
Iteration 175/1000 | Loss: 0.00005505
Iteration 176/1000 | Loss: 0.00005505
Iteration 177/1000 | Loss: 0.00005505
Iteration 178/1000 | Loss: 0.00005505
Iteration 179/1000 | Loss: 0.00005505
Iteration 180/1000 | Loss: 0.00005505
Iteration 181/1000 | Loss: 0.00005504
Iteration 182/1000 | Loss: 0.00005504
Iteration 183/1000 | Loss: 0.00005504
Iteration 184/1000 | Loss: 0.00005504
Iteration 185/1000 | Loss: 0.00005504
Iteration 186/1000 | Loss: 0.00005504
Iteration 187/1000 | Loss: 0.00005504
Iteration 188/1000 | Loss: 0.00005504
Iteration 189/1000 | Loss: 0.00005504
Iteration 190/1000 | Loss: 0.00005504
Iteration 191/1000 | Loss: 0.00005503
Iteration 192/1000 | Loss: 0.00005503
Iteration 193/1000 | Loss: 0.00005503
Iteration 194/1000 | Loss: 0.00005503
Iteration 195/1000 | Loss: 0.00005503
Iteration 196/1000 | Loss: 0.00005503
Iteration 197/1000 | Loss: 0.00005502
Iteration 198/1000 | Loss: 0.00005502
Iteration 199/1000 | Loss: 0.00005502
Iteration 200/1000 | Loss: 0.00005502
Iteration 201/1000 | Loss: 0.00005502
Iteration 202/1000 | Loss: 0.00005502
Iteration 203/1000 | Loss: 0.00005501
Iteration 204/1000 | Loss: 0.00005501
Iteration 205/1000 | Loss: 0.00005501
Iteration 206/1000 | Loss: 0.00005501
Iteration 207/1000 | Loss: 0.00005501
Iteration 208/1000 | Loss: 0.00005501
Iteration 209/1000 | Loss: 0.00005501
Iteration 210/1000 | Loss: 0.00005501
Iteration 211/1000 | Loss: 0.00005501
Iteration 212/1000 | Loss: 0.00005501
Iteration 213/1000 | Loss: 0.00005500
Iteration 214/1000 | Loss: 0.00005500
Iteration 215/1000 | Loss: 0.00005500
Iteration 216/1000 | Loss: 0.00005500
Iteration 217/1000 | Loss: 0.00005500
Iteration 218/1000 | Loss: 0.00005499
Iteration 219/1000 | Loss: 0.00005499
Iteration 220/1000 | Loss: 0.00005499
Iteration 221/1000 | Loss: 0.00005499
Iteration 222/1000 | Loss: 0.00005499
Iteration 223/1000 | Loss: 0.00005499
Iteration 224/1000 | Loss: 0.00005499
Iteration 225/1000 | Loss: 0.00005499
Iteration 226/1000 | Loss: 0.00005499
Iteration 227/1000 | Loss: 0.00005499
Iteration 228/1000 | Loss: 0.00005499
Iteration 229/1000 | Loss: 0.00005499
Iteration 230/1000 | Loss: 0.00005499
Iteration 231/1000 | Loss: 0.00005499
Iteration 232/1000 | Loss: 0.00005499
Iteration 233/1000 | Loss: 0.00005499
Iteration 234/1000 | Loss: 0.00005499
Iteration 235/1000 | Loss: 0.00005498
Iteration 236/1000 | Loss: 0.00005498
Iteration 237/1000 | Loss: 0.00005498
Iteration 238/1000 | Loss: 0.00005498
Iteration 239/1000 | Loss: 0.00005498
Iteration 240/1000 | Loss: 0.00005498
Iteration 241/1000 | Loss: 0.00005498
Iteration 242/1000 | Loss: 0.00005498
Iteration 243/1000 | Loss: 0.00005498
Iteration 244/1000 | Loss: 0.00005498
Iteration 245/1000 | Loss: 0.00005498
Iteration 246/1000 | Loss: 0.00005498
Iteration 247/1000 | Loss: 0.00005498
Iteration 248/1000 | Loss: 0.00005498
Iteration 249/1000 | Loss: 0.00005498
Iteration 250/1000 | Loss: 0.00005498
Iteration 251/1000 | Loss: 0.00005498
Iteration 252/1000 | Loss: 0.00005498
Iteration 253/1000 | Loss: 0.00005498
Iteration 254/1000 | Loss: 0.00005498
Iteration 255/1000 | Loss: 0.00005498
Iteration 256/1000 | Loss: 0.00005498
Iteration 257/1000 | Loss: 0.00005498
Iteration 258/1000 | Loss: 0.00005498
Iteration 259/1000 | Loss: 0.00005498
Iteration 260/1000 | Loss: 0.00005497
Iteration 261/1000 | Loss: 0.00005497
Iteration 262/1000 | Loss: 0.00005497
Iteration 263/1000 | Loss: 0.00005497
Iteration 264/1000 | Loss: 0.00005497
Iteration 265/1000 | Loss: 0.00005497
Iteration 266/1000 | Loss: 0.00005497
Iteration 267/1000 | Loss: 0.00005497
Iteration 268/1000 | Loss: 0.00005497
Iteration 269/1000 | Loss: 0.00005497
Iteration 270/1000 | Loss: 0.00005497
Iteration 271/1000 | Loss: 0.00005497
Iteration 272/1000 | Loss: 0.00005497
Iteration 273/1000 | Loss: 0.00005497
Iteration 274/1000 | Loss: 0.00005497
Iteration 275/1000 | Loss: 0.00005497
Iteration 276/1000 | Loss: 0.00005497
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [5.4973272199276835e-05, 5.4973272199276835e-05, 5.4973272199276835e-05, 5.4973272199276835e-05, 5.4973272199276835e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.4973272199276835e-05

Optimization complete. Final v2v error: 4.7106547355651855 mm

Highest mean error: 11.199122428894043 mm for frame 5

Lowest mean error: 3.5543437004089355 mm for frame 115

Saving results

Total time: 222.0917613506317
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851347
Iteration 2/25 | Loss: 0.00158628
Iteration 3/25 | Loss: 0.00127294
Iteration 4/25 | Loss: 0.00124537
Iteration 5/25 | Loss: 0.00123573
Iteration 6/25 | Loss: 0.00123262
Iteration 7/25 | Loss: 0.00123164
Iteration 8/25 | Loss: 0.00123141
Iteration 9/25 | Loss: 0.00123141
Iteration 10/25 | Loss: 0.00123141
Iteration 11/25 | Loss: 0.00123141
Iteration 12/25 | Loss: 0.00123141
Iteration 13/25 | Loss: 0.00123141
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012314136838540435, 0.0012314136838540435, 0.0012314136838540435, 0.0012314136838540435, 0.0012314136838540435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012314136838540435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16608357
Iteration 2/25 | Loss: 0.00095671
Iteration 3/25 | Loss: 0.00095671
Iteration 4/25 | Loss: 0.00095671
Iteration 5/25 | Loss: 0.00095671
Iteration 6/25 | Loss: 0.00095671
Iteration 7/25 | Loss: 0.00095671
Iteration 8/25 | Loss: 0.00095671
Iteration 9/25 | Loss: 0.00095671
Iteration 10/25 | Loss: 0.00095671
Iteration 11/25 | Loss: 0.00095671
Iteration 12/25 | Loss: 0.00095671
Iteration 13/25 | Loss: 0.00095671
Iteration 14/25 | Loss: 0.00095671
Iteration 15/25 | Loss: 0.00095671
Iteration 16/25 | Loss: 0.00095671
Iteration 17/25 | Loss: 0.00095671
Iteration 18/25 | Loss: 0.00095671
Iteration 19/25 | Loss: 0.00095671
Iteration 20/25 | Loss: 0.00095671
Iteration 21/25 | Loss: 0.00095671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009567085071466863, 0.0009567085071466863, 0.0009567085071466863, 0.0009567085071466863, 0.0009567085071466863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009567085071466863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095671
Iteration 2/1000 | Loss: 0.00005532
Iteration 3/1000 | Loss: 0.00003476
Iteration 4/1000 | Loss: 0.00002739
Iteration 5/1000 | Loss: 0.00002503
Iteration 6/1000 | Loss: 0.00002352
Iteration 7/1000 | Loss: 0.00002263
Iteration 8/1000 | Loss: 0.00002211
Iteration 9/1000 | Loss: 0.00002165
Iteration 10/1000 | Loss: 0.00002128
Iteration 11/1000 | Loss: 0.00002105
Iteration 12/1000 | Loss: 0.00002090
Iteration 13/1000 | Loss: 0.00002071
Iteration 14/1000 | Loss: 0.00002051
Iteration 15/1000 | Loss: 0.00002041
Iteration 16/1000 | Loss: 0.00002035
Iteration 17/1000 | Loss: 0.00002031
Iteration 18/1000 | Loss: 0.00002030
Iteration 19/1000 | Loss: 0.00002028
Iteration 20/1000 | Loss: 0.00002026
Iteration 21/1000 | Loss: 0.00002022
Iteration 22/1000 | Loss: 0.00002018
Iteration 23/1000 | Loss: 0.00002016
Iteration 24/1000 | Loss: 0.00002015
Iteration 25/1000 | Loss: 0.00002015
Iteration 26/1000 | Loss: 0.00002014
Iteration 27/1000 | Loss: 0.00002014
Iteration 28/1000 | Loss: 0.00002013
Iteration 29/1000 | Loss: 0.00002012
Iteration 30/1000 | Loss: 0.00002009
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00002007
Iteration 33/1000 | Loss: 0.00002006
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002004
Iteration 36/1000 | Loss: 0.00002003
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00002002
Iteration 39/1000 | Loss: 0.00002002
Iteration 40/1000 | Loss: 0.00002001
Iteration 41/1000 | Loss: 0.00002001
Iteration 42/1000 | Loss: 0.00002001
Iteration 43/1000 | Loss: 0.00002000
Iteration 44/1000 | Loss: 0.00002000
Iteration 45/1000 | Loss: 0.00002000
Iteration 46/1000 | Loss: 0.00001999
Iteration 47/1000 | Loss: 0.00001999
Iteration 48/1000 | Loss: 0.00001999
Iteration 49/1000 | Loss: 0.00001999
Iteration 50/1000 | Loss: 0.00001998
Iteration 51/1000 | Loss: 0.00001998
Iteration 52/1000 | Loss: 0.00001998
Iteration 53/1000 | Loss: 0.00001997
Iteration 54/1000 | Loss: 0.00001997
Iteration 55/1000 | Loss: 0.00001997
Iteration 56/1000 | Loss: 0.00001996
Iteration 57/1000 | Loss: 0.00001996
Iteration 58/1000 | Loss: 0.00001996
Iteration 59/1000 | Loss: 0.00001995
Iteration 60/1000 | Loss: 0.00001995
Iteration 61/1000 | Loss: 0.00001995
Iteration 62/1000 | Loss: 0.00001995
Iteration 63/1000 | Loss: 0.00001994
Iteration 64/1000 | Loss: 0.00001994
Iteration 65/1000 | Loss: 0.00001994
Iteration 66/1000 | Loss: 0.00001994
Iteration 67/1000 | Loss: 0.00001993
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001992
Iteration 71/1000 | Loss: 0.00001992
Iteration 72/1000 | Loss: 0.00001992
Iteration 73/1000 | Loss: 0.00001992
Iteration 74/1000 | Loss: 0.00001991
Iteration 75/1000 | Loss: 0.00001991
Iteration 76/1000 | Loss: 0.00001991
Iteration 77/1000 | Loss: 0.00001991
Iteration 78/1000 | Loss: 0.00001991
Iteration 79/1000 | Loss: 0.00001991
Iteration 80/1000 | Loss: 0.00001991
Iteration 81/1000 | Loss: 0.00001991
Iteration 82/1000 | Loss: 0.00001990
Iteration 83/1000 | Loss: 0.00001990
Iteration 84/1000 | Loss: 0.00001990
Iteration 85/1000 | Loss: 0.00001990
Iteration 86/1000 | Loss: 0.00001990
Iteration 87/1000 | Loss: 0.00001990
Iteration 88/1000 | Loss: 0.00001989
Iteration 89/1000 | Loss: 0.00001989
Iteration 90/1000 | Loss: 0.00001989
Iteration 91/1000 | Loss: 0.00001989
Iteration 92/1000 | Loss: 0.00001989
Iteration 93/1000 | Loss: 0.00001989
Iteration 94/1000 | Loss: 0.00001989
Iteration 95/1000 | Loss: 0.00001989
Iteration 96/1000 | Loss: 0.00001989
Iteration 97/1000 | Loss: 0.00001988
Iteration 98/1000 | Loss: 0.00001988
Iteration 99/1000 | Loss: 0.00001988
Iteration 100/1000 | Loss: 0.00001988
Iteration 101/1000 | Loss: 0.00001987
Iteration 102/1000 | Loss: 0.00001987
Iteration 103/1000 | Loss: 0.00001987
Iteration 104/1000 | Loss: 0.00001987
Iteration 105/1000 | Loss: 0.00001986
Iteration 106/1000 | Loss: 0.00001986
Iteration 107/1000 | Loss: 0.00001986
Iteration 108/1000 | Loss: 0.00001986
Iteration 109/1000 | Loss: 0.00001986
Iteration 110/1000 | Loss: 0.00001986
Iteration 111/1000 | Loss: 0.00001986
Iteration 112/1000 | Loss: 0.00001986
Iteration 113/1000 | Loss: 0.00001986
Iteration 114/1000 | Loss: 0.00001985
Iteration 115/1000 | Loss: 0.00001985
Iteration 116/1000 | Loss: 0.00001985
Iteration 117/1000 | Loss: 0.00001985
Iteration 118/1000 | Loss: 0.00001984
Iteration 119/1000 | Loss: 0.00001984
Iteration 120/1000 | Loss: 0.00001984
Iteration 121/1000 | Loss: 0.00001984
Iteration 122/1000 | Loss: 0.00001984
Iteration 123/1000 | Loss: 0.00001984
Iteration 124/1000 | Loss: 0.00001984
Iteration 125/1000 | Loss: 0.00001983
Iteration 126/1000 | Loss: 0.00001983
Iteration 127/1000 | Loss: 0.00001983
Iteration 128/1000 | Loss: 0.00001983
Iteration 129/1000 | Loss: 0.00001983
Iteration 130/1000 | Loss: 0.00001983
Iteration 131/1000 | Loss: 0.00001983
Iteration 132/1000 | Loss: 0.00001983
Iteration 133/1000 | Loss: 0.00001982
Iteration 134/1000 | Loss: 0.00001982
Iteration 135/1000 | Loss: 0.00001982
Iteration 136/1000 | Loss: 0.00001982
Iteration 137/1000 | Loss: 0.00001982
Iteration 138/1000 | Loss: 0.00001982
Iteration 139/1000 | Loss: 0.00001982
Iteration 140/1000 | Loss: 0.00001982
Iteration 141/1000 | Loss: 0.00001982
Iteration 142/1000 | Loss: 0.00001981
Iteration 143/1000 | Loss: 0.00001981
Iteration 144/1000 | Loss: 0.00001981
Iteration 145/1000 | Loss: 0.00001981
Iteration 146/1000 | Loss: 0.00001981
Iteration 147/1000 | Loss: 0.00001981
Iteration 148/1000 | Loss: 0.00001981
Iteration 149/1000 | Loss: 0.00001981
Iteration 150/1000 | Loss: 0.00001981
Iteration 151/1000 | Loss: 0.00001981
Iteration 152/1000 | Loss: 0.00001980
Iteration 153/1000 | Loss: 0.00001980
Iteration 154/1000 | Loss: 0.00001980
Iteration 155/1000 | Loss: 0.00001980
Iteration 156/1000 | Loss: 0.00001980
Iteration 157/1000 | Loss: 0.00001980
Iteration 158/1000 | Loss: 0.00001980
Iteration 159/1000 | Loss: 0.00001980
Iteration 160/1000 | Loss: 0.00001980
Iteration 161/1000 | Loss: 0.00001980
Iteration 162/1000 | Loss: 0.00001979
Iteration 163/1000 | Loss: 0.00001979
Iteration 164/1000 | Loss: 0.00001979
Iteration 165/1000 | Loss: 0.00001979
Iteration 166/1000 | Loss: 0.00001979
Iteration 167/1000 | Loss: 0.00001979
Iteration 168/1000 | Loss: 0.00001979
Iteration 169/1000 | Loss: 0.00001978
Iteration 170/1000 | Loss: 0.00001978
Iteration 171/1000 | Loss: 0.00001978
Iteration 172/1000 | Loss: 0.00001978
Iteration 173/1000 | Loss: 0.00001978
Iteration 174/1000 | Loss: 0.00001978
Iteration 175/1000 | Loss: 0.00001978
Iteration 176/1000 | Loss: 0.00001978
Iteration 177/1000 | Loss: 0.00001977
Iteration 178/1000 | Loss: 0.00001977
Iteration 179/1000 | Loss: 0.00001977
Iteration 180/1000 | Loss: 0.00001977
Iteration 181/1000 | Loss: 0.00001977
Iteration 182/1000 | Loss: 0.00001977
Iteration 183/1000 | Loss: 0.00001977
Iteration 184/1000 | Loss: 0.00001976
Iteration 185/1000 | Loss: 0.00001976
Iteration 186/1000 | Loss: 0.00001976
Iteration 187/1000 | Loss: 0.00001976
Iteration 188/1000 | Loss: 0.00001976
Iteration 189/1000 | Loss: 0.00001976
Iteration 190/1000 | Loss: 0.00001976
Iteration 191/1000 | Loss: 0.00001976
Iteration 192/1000 | Loss: 0.00001976
Iteration 193/1000 | Loss: 0.00001976
Iteration 194/1000 | Loss: 0.00001975
Iteration 195/1000 | Loss: 0.00001975
Iteration 196/1000 | Loss: 0.00001975
Iteration 197/1000 | Loss: 0.00001975
Iteration 198/1000 | Loss: 0.00001975
Iteration 199/1000 | Loss: 0.00001975
Iteration 200/1000 | Loss: 0.00001975
Iteration 201/1000 | Loss: 0.00001974
Iteration 202/1000 | Loss: 0.00001974
Iteration 203/1000 | Loss: 0.00001974
Iteration 204/1000 | Loss: 0.00001974
Iteration 205/1000 | Loss: 0.00001974
Iteration 206/1000 | Loss: 0.00001974
Iteration 207/1000 | Loss: 0.00001973
Iteration 208/1000 | Loss: 0.00001973
Iteration 209/1000 | Loss: 0.00001973
Iteration 210/1000 | Loss: 0.00001973
Iteration 211/1000 | Loss: 0.00001973
Iteration 212/1000 | Loss: 0.00001973
Iteration 213/1000 | Loss: 0.00001973
Iteration 214/1000 | Loss: 0.00001973
Iteration 215/1000 | Loss: 0.00001973
Iteration 216/1000 | Loss: 0.00001973
Iteration 217/1000 | Loss: 0.00001973
Iteration 218/1000 | Loss: 0.00001972
Iteration 219/1000 | Loss: 0.00001972
Iteration 220/1000 | Loss: 0.00001972
Iteration 221/1000 | Loss: 0.00001972
Iteration 222/1000 | Loss: 0.00001972
Iteration 223/1000 | Loss: 0.00001972
Iteration 224/1000 | Loss: 0.00001972
Iteration 225/1000 | Loss: 0.00001972
Iteration 226/1000 | Loss: 0.00001972
Iteration 227/1000 | Loss: 0.00001972
Iteration 228/1000 | Loss: 0.00001972
Iteration 229/1000 | Loss: 0.00001972
Iteration 230/1000 | Loss: 0.00001971
Iteration 231/1000 | Loss: 0.00001971
Iteration 232/1000 | Loss: 0.00001971
Iteration 233/1000 | Loss: 0.00001971
Iteration 234/1000 | Loss: 0.00001971
Iteration 235/1000 | Loss: 0.00001971
Iteration 236/1000 | Loss: 0.00001971
Iteration 237/1000 | Loss: 0.00001971
Iteration 238/1000 | Loss: 0.00001971
Iteration 239/1000 | Loss: 0.00001971
Iteration 240/1000 | Loss: 0.00001971
Iteration 241/1000 | Loss: 0.00001971
Iteration 242/1000 | Loss: 0.00001971
Iteration 243/1000 | Loss: 0.00001971
Iteration 244/1000 | Loss: 0.00001971
Iteration 245/1000 | Loss: 0.00001970
Iteration 246/1000 | Loss: 0.00001970
Iteration 247/1000 | Loss: 0.00001970
Iteration 248/1000 | Loss: 0.00001970
Iteration 249/1000 | Loss: 0.00001970
Iteration 250/1000 | Loss: 0.00001970
Iteration 251/1000 | Loss: 0.00001970
Iteration 252/1000 | Loss: 0.00001970
Iteration 253/1000 | Loss: 0.00001970
Iteration 254/1000 | Loss: 0.00001970
Iteration 255/1000 | Loss: 0.00001969
Iteration 256/1000 | Loss: 0.00001969
Iteration 257/1000 | Loss: 0.00001969
Iteration 258/1000 | Loss: 0.00001969
Iteration 259/1000 | Loss: 0.00001969
Iteration 260/1000 | Loss: 0.00001969
Iteration 261/1000 | Loss: 0.00001969
Iteration 262/1000 | Loss: 0.00001969
Iteration 263/1000 | Loss: 0.00001969
Iteration 264/1000 | Loss: 0.00001969
Iteration 265/1000 | Loss: 0.00001969
Iteration 266/1000 | Loss: 0.00001969
Iteration 267/1000 | Loss: 0.00001968
Iteration 268/1000 | Loss: 0.00001968
Iteration 269/1000 | Loss: 0.00001968
Iteration 270/1000 | Loss: 0.00001968
Iteration 271/1000 | Loss: 0.00001968
Iteration 272/1000 | Loss: 0.00001968
Iteration 273/1000 | Loss: 0.00001968
Iteration 274/1000 | Loss: 0.00001968
Iteration 275/1000 | Loss: 0.00001968
Iteration 276/1000 | Loss: 0.00001968
Iteration 277/1000 | Loss: 0.00001968
Iteration 278/1000 | Loss: 0.00001968
Iteration 279/1000 | Loss: 0.00001968
Iteration 280/1000 | Loss: 0.00001968
Iteration 281/1000 | Loss: 0.00001968
Iteration 282/1000 | Loss: 0.00001968
Iteration 283/1000 | Loss: 0.00001968
Iteration 284/1000 | Loss: 0.00001968
Iteration 285/1000 | Loss: 0.00001967
Iteration 286/1000 | Loss: 0.00001967
Iteration 287/1000 | Loss: 0.00001967
Iteration 288/1000 | Loss: 0.00001967
Iteration 289/1000 | Loss: 0.00001967
Iteration 290/1000 | Loss: 0.00001967
Iteration 291/1000 | Loss: 0.00001967
Iteration 292/1000 | Loss: 0.00001967
Iteration 293/1000 | Loss: 0.00001967
Iteration 294/1000 | Loss: 0.00001967
Iteration 295/1000 | Loss: 0.00001967
Iteration 296/1000 | Loss: 0.00001967
Iteration 297/1000 | Loss: 0.00001967
Iteration 298/1000 | Loss: 0.00001967
Iteration 299/1000 | Loss: 0.00001967
Iteration 300/1000 | Loss: 0.00001967
Iteration 301/1000 | Loss: 0.00001967
Iteration 302/1000 | Loss: 0.00001967
Iteration 303/1000 | Loss: 0.00001967
Iteration 304/1000 | Loss: 0.00001967
Iteration 305/1000 | Loss: 0.00001967
Iteration 306/1000 | Loss: 0.00001967
Iteration 307/1000 | Loss: 0.00001967
Iteration 308/1000 | Loss: 0.00001967
Iteration 309/1000 | Loss: 0.00001967
Iteration 310/1000 | Loss: 0.00001967
Iteration 311/1000 | Loss: 0.00001967
Iteration 312/1000 | Loss: 0.00001967
Iteration 313/1000 | Loss: 0.00001967
Iteration 314/1000 | Loss: 0.00001967
Iteration 315/1000 | Loss: 0.00001967
Iteration 316/1000 | Loss: 0.00001967
Iteration 317/1000 | Loss: 0.00001967
Iteration 318/1000 | Loss: 0.00001967
Iteration 319/1000 | Loss: 0.00001967
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 319. Stopping optimization.
Last 5 losses: [1.9669896573759615e-05, 1.9669896573759615e-05, 1.9669896573759615e-05, 1.9669896573759615e-05, 1.9669896573759615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9669896573759615e-05

Optimization complete. Final v2v error: 3.6314287185668945 mm

Highest mean error: 5.586639881134033 mm for frame 91

Lowest mean error: 2.688657760620117 mm for frame 12

Saving results

Total time: 54.135135650634766
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042403
Iteration 2/25 | Loss: 0.01042403
Iteration 3/25 | Loss: 0.01042402
Iteration 4/25 | Loss: 0.01042402
Iteration 5/25 | Loss: 0.01042402
Iteration 6/25 | Loss: 0.01042402
Iteration 7/25 | Loss: 0.01042402
Iteration 8/25 | Loss: 0.01042402
Iteration 9/25 | Loss: 0.01042402
Iteration 10/25 | Loss: 0.01042402
Iteration 11/25 | Loss: 0.01042402
Iteration 12/25 | Loss: 0.01042401
Iteration 13/25 | Loss: 0.01042401
Iteration 14/25 | Loss: 0.01042401
Iteration 15/25 | Loss: 0.01042401
Iteration 16/25 | Loss: 0.01042401
Iteration 17/25 | Loss: 0.01042401
Iteration 18/25 | Loss: 0.01042401
Iteration 19/25 | Loss: 0.01042401
Iteration 20/25 | Loss: 0.01042401
Iteration 21/25 | Loss: 0.01042401
Iteration 22/25 | Loss: 0.01042400
Iteration 23/25 | Loss: 0.01042400
Iteration 24/25 | Loss: 0.01042400
Iteration 25/25 | Loss: 0.01042400

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32604253
Iteration 2/25 | Loss: 0.17664386
Iteration 3/25 | Loss: 0.17451559
Iteration 4/25 | Loss: 0.17057657
Iteration 5/25 | Loss: 0.17049612
Iteration 6/25 | Loss: 0.17049612
Iteration 7/25 | Loss: 0.17049612
Iteration 8/25 | Loss: 0.17049608
Iteration 9/25 | Loss: 0.17049608
Iteration 10/25 | Loss: 0.17049608
Iteration 11/25 | Loss: 0.17049608
Iteration 12/25 | Loss: 0.17049608
Iteration 13/25 | Loss: 0.17049608
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.17049607634544373, 0.17049607634544373, 0.17049607634544373, 0.17049607634544373, 0.17049607634544373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17049607634544373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17049608
Iteration 2/1000 | Loss: 0.00238324
Iteration 3/1000 | Loss: 0.00116278
Iteration 4/1000 | Loss: 0.00245777
Iteration 5/1000 | Loss: 0.00194563
Iteration 6/1000 | Loss: 0.00054608
Iteration 7/1000 | Loss: 0.00045830
Iteration 8/1000 | Loss: 0.00027020
Iteration 9/1000 | Loss: 0.00068739
Iteration 10/1000 | Loss: 0.00026081
Iteration 11/1000 | Loss: 0.00005479
Iteration 12/1000 | Loss: 0.00011070
Iteration 13/1000 | Loss: 0.00003185
Iteration 14/1000 | Loss: 0.00002845
Iteration 15/1000 | Loss: 0.00038034
Iteration 16/1000 | Loss: 0.00002332
Iteration 17/1000 | Loss: 0.00002144
Iteration 18/1000 | Loss: 0.00013926
Iteration 19/1000 | Loss: 0.00001898
Iteration 20/1000 | Loss: 0.00001801
Iteration 21/1000 | Loss: 0.00013941
Iteration 22/1000 | Loss: 0.00001690
Iteration 23/1000 | Loss: 0.00012227
Iteration 24/1000 | Loss: 0.00001606
Iteration 25/1000 | Loss: 0.00013569
Iteration 26/1000 | Loss: 0.00001523
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001454
Iteration 29/1000 | Loss: 0.00011552
Iteration 30/1000 | Loss: 0.00004419
Iteration 31/1000 | Loss: 0.00001398
Iteration 32/1000 | Loss: 0.00001385
Iteration 33/1000 | Loss: 0.00004788
Iteration 34/1000 | Loss: 0.00001836
Iteration 35/1000 | Loss: 0.00001582
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001354
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001347
Iteration 40/1000 | Loss: 0.00009443
Iteration 41/1000 | Loss: 0.00003474
Iteration 42/1000 | Loss: 0.00003299
Iteration 43/1000 | Loss: 0.00003700
Iteration 44/1000 | Loss: 0.00002520
Iteration 45/1000 | Loss: 0.00001336
Iteration 46/1000 | Loss: 0.00001335
Iteration 47/1000 | Loss: 0.00001334
Iteration 48/1000 | Loss: 0.00001333
Iteration 49/1000 | Loss: 0.00001332
Iteration 50/1000 | Loss: 0.00001322
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001320
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001316
Iteration 60/1000 | Loss: 0.00001311
Iteration 61/1000 | Loss: 0.00001310
Iteration 62/1000 | Loss: 0.00001310
Iteration 63/1000 | Loss: 0.00001309
Iteration 64/1000 | Loss: 0.00001307
Iteration 65/1000 | Loss: 0.00001307
Iteration 66/1000 | Loss: 0.00001306
Iteration 67/1000 | Loss: 0.00001306
Iteration 68/1000 | Loss: 0.00001306
Iteration 69/1000 | Loss: 0.00001306
Iteration 70/1000 | Loss: 0.00001306
Iteration 71/1000 | Loss: 0.00001306
Iteration 72/1000 | Loss: 0.00001306
Iteration 73/1000 | Loss: 0.00001306
Iteration 74/1000 | Loss: 0.00001306
Iteration 75/1000 | Loss: 0.00001306
Iteration 76/1000 | Loss: 0.00001306
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001305
Iteration 79/1000 | Loss: 0.00001305
Iteration 80/1000 | Loss: 0.00001304
Iteration 81/1000 | Loss: 0.00001304
Iteration 82/1000 | Loss: 0.00001304
Iteration 83/1000 | Loss: 0.00001304
Iteration 84/1000 | Loss: 0.00001304
Iteration 85/1000 | Loss: 0.00001303
Iteration 86/1000 | Loss: 0.00001303
Iteration 87/1000 | Loss: 0.00001303
Iteration 88/1000 | Loss: 0.00001303
Iteration 89/1000 | Loss: 0.00001301
Iteration 90/1000 | Loss: 0.00001300
Iteration 91/1000 | Loss: 0.00001300
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001299
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001298
Iteration 98/1000 | Loss: 0.00001297
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001296
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001296
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001296
Iteration 110/1000 | Loss: 0.00001296
Iteration 111/1000 | Loss: 0.00001295
Iteration 112/1000 | Loss: 0.00001295
Iteration 113/1000 | Loss: 0.00001295
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001294
Iteration 116/1000 | Loss: 0.00001294
Iteration 117/1000 | Loss: 0.00001294
Iteration 118/1000 | Loss: 0.00001293
Iteration 119/1000 | Loss: 0.00001293
Iteration 120/1000 | Loss: 0.00001293
Iteration 121/1000 | Loss: 0.00001293
Iteration 122/1000 | Loss: 0.00001293
Iteration 123/1000 | Loss: 0.00001293
Iteration 124/1000 | Loss: 0.00001293
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001292
Iteration 127/1000 | Loss: 0.00001292
Iteration 128/1000 | Loss: 0.00001292
Iteration 129/1000 | Loss: 0.00001292
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001290
Iteration 134/1000 | Loss: 0.00001290
Iteration 135/1000 | Loss: 0.00001290
Iteration 136/1000 | Loss: 0.00001289
Iteration 137/1000 | Loss: 0.00001289
Iteration 138/1000 | Loss: 0.00001289
Iteration 139/1000 | Loss: 0.00001289
Iteration 140/1000 | Loss: 0.00001289
Iteration 141/1000 | Loss: 0.00001289
Iteration 142/1000 | Loss: 0.00001289
Iteration 143/1000 | Loss: 0.00001289
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001288
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001288
Iteration 151/1000 | Loss: 0.00001288
Iteration 152/1000 | Loss: 0.00001288
Iteration 153/1000 | Loss: 0.00001288
Iteration 154/1000 | Loss: 0.00001288
Iteration 155/1000 | Loss: 0.00001287
Iteration 156/1000 | Loss: 0.00001287
Iteration 157/1000 | Loss: 0.00001287
Iteration 158/1000 | Loss: 0.00001287
Iteration 159/1000 | Loss: 0.00001287
Iteration 160/1000 | Loss: 0.00001287
Iteration 161/1000 | Loss: 0.00001287
Iteration 162/1000 | Loss: 0.00001287
Iteration 163/1000 | Loss: 0.00001287
Iteration 164/1000 | Loss: 0.00001287
Iteration 165/1000 | Loss: 0.00001286
Iteration 166/1000 | Loss: 0.00001286
Iteration 167/1000 | Loss: 0.00001286
Iteration 168/1000 | Loss: 0.00001286
Iteration 169/1000 | Loss: 0.00001286
Iteration 170/1000 | Loss: 0.00001286
Iteration 171/1000 | Loss: 0.00001286
Iteration 172/1000 | Loss: 0.00001286
Iteration 173/1000 | Loss: 0.00001286
Iteration 174/1000 | Loss: 0.00001286
Iteration 175/1000 | Loss: 0.00001286
Iteration 176/1000 | Loss: 0.00001286
Iteration 177/1000 | Loss: 0.00001286
Iteration 178/1000 | Loss: 0.00001286
Iteration 179/1000 | Loss: 0.00001286
Iteration 180/1000 | Loss: 0.00001286
Iteration 181/1000 | Loss: 0.00001286
Iteration 182/1000 | Loss: 0.00001286
Iteration 183/1000 | Loss: 0.00001286
Iteration 184/1000 | Loss: 0.00001286
Iteration 185/1000 | Loss: 0.00001286
Iteration 186/1000 | Loss: 0.00001286
Iteration 187/1000 | Loss: 0.00001286
Iteration 188/1000 | Loss: 0.00001285
Iteration 189/1000 | Loss: 0.00001285
Iteration 190/1000 | Loss: 0.00001285
Iteration 191/1000 | Loss: 0.00001285
Iteration 192/1000 | Loss: 0.00001285
Iteration 193/1000 | Loss: 0.00001285
Iteration 194/1000 | Loss: 0.00001285
Iteration 195/1000 | Loss: 0.00001285
Iteration 196/1000 | Loss: 0.00001285
Iteration 197/1000 | Loss: 0.00001285
Iteration 198/1000 | Loss: 0.00001285
Iteration 199/1000 | Loss: 0.00001285
Iteration 200/1000 | Loss: 0.00001285
Iteration 201/1000 | Loss: 0.00001285
Iteration 202/1000 | Loss: 0.00001285
Iteration 203/1000 | Loss: 0.00001285
Iteration 204/1000 | Loss: 0.00001285
Iteration 205/1000 | Loss: 0.00001285
Iteration 206/1000 | Loss: 0.00001285
Iteration 207/1000 | Loss: 0.00001285
Iteration 208/1000 | Loss: 0.00001285
Iteration 209/1000 | Loss: 0.00001285
Iteration 210/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [1.2846700883528683e-05, 1.2846700883528683e-05, 1.2846700883528683e-05, 1.2846700883528683e-05, 1.2846700883528683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2846700883528683e-05

Optimization complete. Final v2v error: 3.001582145690918 mm

Highest mean error: 3.229185104370117 mm for frame 128

Lowest mean error: 2.6645612716674805 mm for frame 2

Saving results

Total time: 91.79529070854187
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00936570
Iteration 2/25 | Loss: 0.00507093
Iteration 3/25 | Loss: 0.00373753
Iteration 4/25 | Loss: 0.00304573
Iteration 5/25 | Loss: 0.00255885
Iteration 6/25 | Loss: 0.00214672
Iteration 7/25 | Loss: 0.00189471
Iteration 8/25 | Loss: 0.00181242
Iteration 9/25 | Loss: 0.00179887
Iteration 10/25 | Loss: 0.00182488
Iteration 11/25 | Loss: 0.00166240
Iteration 12/25 | Loss: 0.00157915
Iteration 13/25 | Loss: 0.00155205
Iteration 14/25 | Loss: 0.00156052
Iteration 15/25 | Loss: 0.00153366
Iteration 16/25 | Loss: 0.00152164
Iteration 17/25 | Loss: 0.00153166
Iteration 18/25 | Loss: 0.00149627
Iteration 19/25 | Loss: 0.00149268
Iteration 20/25 | Loss: 0.00147947
Iteration 21/25 | Loss: 0.00147286
Iteration 22/25 | Loss: 0.00146013
Iteration 23/25 | Loss: 0.00145975
Iteration 24/25 | Loss: 0.00145469
Iteration 25/25 | Loss: 0.00145268

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34152901
Iteration 2/25 | Loss: 0.00147878
Iteration 3/25 | Loss: 0.00147878
Iteration 4/25 | Loss: 0.00147878
Iteration 5/25 | Loss: 0.00147878
Iteration 6/25 | Loss: 0.00147878
Iteration 7/25 | Loss: 0.00147878
Iteration 8/25 | Loss: 0.00147878
Iteration 9/25 | Loss: 0.00147878
Iteration 10/25 | Loss: 0.00147878
Iteration 11/25 | Loss: 0.00147878
Iteration 12/25 | Loss: 0.00147878
Iteration 13/25 | Loss: 0.00147878
Iteration 14/25 | Loss: 0.00147878
Iteration 15/25 | Loss: 0.00147878
Iteration 16/25 | Loss: 0.00147878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014787815744057298, 0.0014787815744057298, 0.0014787815744057298, 0.0014787815744057298, 0.0014787815744057298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014787815744057298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147878
Iteration 2/1000 | Loss: 0.00042980
Iteration 3/1000 | Loss: 0.00047593
Iteration 4/1000 | Loss: 0.00046525
Iteration 5/1000 | Loss: 0.00092960
Iteration 6/1000 | Loss: 0.00077784
Iteration 7/1000 | Loss: 0.00098697
Iteration 8/1000 | Loss: 0.00163711
Iteration 9/1000 | Loss: 0.00341705
Iteration 10/1000 | Loss: 0.00165589
Iteration 11/1000 | Loss: 0.00218083
Iteration 12/1000 | Loss: 0.00043953
Iteration 13/1000 | Loss: 0.00119291
Iteration 14/1000 | Loss: 0.00081434
Iteration 15/1000 | Loss: 0.00051777
Iteration 16/1000 | Loss: 0.00219611
Iteration 17/1000 | Loss: 0.00698341
Iteration 18/1000 | Loss: 0.00458092
Iteration 19/1000 | Loss: 0.00246827
Iteration 20/1000 | Loss: 0.00246398
Iteration 21/1000 | Loss: 0.00198765
Iteration 22/1000 | Loss: 0.00188767
Iteration 23/1000 | Loss: 0.00098265
Iteration 24/1000 | Loss: 0.00152097
Iteration 25/1000 | Loss: 0.00091176
Iteration 26/1000 | Loss: 0.00109793
Iteration 27/1000 | Loss: 0.00065100
Iteration 28/1000 | Loss: 0.00142178
Iteration 29/1000 | Loss: 0.00060207
Iteration 30/1000 | Loss: 0.00088797
Iteration 31/1000 | Loss: 0.00084394
Iteration 32/1000 | Loss: 0.00098971
Iteration 33/1000 | Loss: 0.00063660
Iteration 34/1000 | Loss: 0.00095615
Iteration 35/1000 | Loss: 0.00013890
Iteration 36/1000 | Loss: 0.00054195
Iteration 37/1000 | Loss: 0.00011950
Iteration 38/1000 | Loss: 0.00009499
Iteration 39/1000 | Loss: 0.00077761
Iteration 40/1000 | Loss: 0.00026469
Iteration 41/1000 | Loss: 0.00023723
Iteration 42/1000 | Loss: 0.00068437
Iteration 43/1000 | Loss: 0.00025452
Iteration 44/1000 | Loss: 0.00005881
Iteration 45/1000 | Loss: 0.00031837
Iteration 46/1000 | Loss: 0.00014623
Iteration 47/1000 | Loss: 0.00004671
Iteration 48/1000 | Loss: 0.00004188
Iteration 49/1000 | Loss: 0.00064745
Iteration 50/1000 | Loss: 0.00008929
Iteration 51/1000 | Loss: 0.00005602
Iteration 52/1000 | Loss: 0.00003589
Iteration 53/1000 | Loss: 0.00021848
Iteration 54/1000 | Loss: 0.00004057
Iteration 55/1000 | Loss: 0.00003285
Iteration 56/1000 | Loss: 0.00019106
Iteration 57/1000 | Loss: 0.00015930
Iteration 58/1000 | Loss: 0.00009420
Iteration 59/1000 | Loss: 0.00012997
Iteration 60/1000 | Loss: 0.00006083
Iteration 61/1000 | Loss: 0.00013808
Iteration 62/1000 | Loss: 0.00004209
Iteration 63/1000 | Loss: 0.00030106
Iteration 64/1000 | Loss: 0.00008617
Iteration 65/1000 | Loss: 0.00018974
Iteration 66/1000 | Loss: 0.00003445
Iteration 67/1000 | Loss: 0.00024268
Iteration 68/1000 | Loss: 0.00003055
Iteration 69/1000 | Loss: 0.00015288
Iteration 70/1000 | Loss: 0.00020823
Iteration 71/1000 | Loss: 0.00002861
Iteration 72/1000 | Loss: 0.00002797
Iteration 73/1000 | Loss: 0.00002717
Iteration 74/1000 | Loss: 0.00016908
Iteration 75/1000 | Loss: 0.00005490
Iteration 76/1000 | Loss: 0.00009970
Iteration 77/1000 | Loss: 0.00008566
Iteration 78/1000 | Loss: 0.00002608
Iteration 79/1000 | Loss: 0.00002531
Iteration 80/1000 | Loss: 0.00002499
Iteration 81/1000 | Loss: 0.00002461
Iteration 82/1000 | Loss: 0.00002439
Iteration 83/1000 | Loss: 0.00002430
Iteration 84/1000 | Loss: 0.00020353
Iteration 85/1000 | Loss: 0.00005712
Iteration 86/1000 | Loss: 0.00018155
Iteration 87/1000 | Loss: 0.00003833
Iteration 88/1000 | Loss: 0.00002637
Iteration 89/1000 | Loss: 0.00024117
Iteration 90/1000 | Loss: 0.00018241
Iteration 91/1000 | Loss: 0.00004181
Iteration 92/1000 | Loss: 0.00007003
Iteration 93/1000 | Loss: 0.00016120
Iteration 94/1000 | Loss: 0.00005703
Iteration 95/1000 | Loss: 0.00002525
Iteration 96/1000 | Loss: 0.00013369
Iteration 97/1000 | Loss: 0.00007215
Iteration 98/1000 | Loss: 0.00002608
Iteration 99/1000 | Loss: 0.00010021
Iteration 100/1000 | Loss: 0.00007627
Iteration 101/1000 | Loss: 0.00002479
Iteration 102/1000 | Loss: 0.00009602
Iteration 103/1000 | Loss: 0.00009157
Iteration 104/1000 | Loss: 0.00010401
Iteration 105/1000 | Loss: 0.00007747
Iteration 106/1000 | Loss: 0.00011877
Iteration 107/1000 | Loss: 0.00010106
Iteration 108/1000 | Loss: 0.00002506
Iteration 109/1000 | Loss: 0.00002468
Iteration 110/1000 | Loss: 0.00002455
Iteration 111/1000 | Loss: 0.00002455
Iteration 112/1000 | Loss: 0.00002454
Iteration 113/1000 | Loss: 0.00002454
Iteration 114/1000 | Loss: 0.00002454
Iteration 115/1000 | Loss: 0.00002454
Iteration 116/1000 | Loss: 0.00002454
Iteration 117/1000 | Loss: 0.00002454
Iteration 118/1000 | Loss: 0.00002454
Iteration 119/1000 | Loss: 0.00002449
Iteration 120/1000 | Loss: 0.00012472
Iteration 121/1000 | Loss: 0.00020556
Iteration 122/1000 | Loss: 0.00012476
Iteration 123/1000 | Loss: 0.00014721
Iteration 124/1000 | Loss: 0.00035833
Iteration 125/1000 | Loss: 0.00009344
Iteration 126/1000 | Loss: 0.00007132
Iteration 127/1000 | Loss: 0.00005214
Iteration 128/1000 | Loss: 0.00007448
Iteration 129/1000 | Loss: 0.00015995
Iteration 130/1000 | Loss: 0.00016849
Iteration 131/1000 | Loss: 0.00020850
Iteration 132/1000 | Loss: 0.00016514
Iteration 133/1000 | Loss: 0.00018414
Iteration 134/1000 | Loss: 0.00016025
Iteration 135/1000 | Loss: 0.00032831
Iteration 136/1000 | Loss: 0.00014625
Iteration 137/1000 | Loss: 0.00038451
Iteration 138/1000 | Loss: 0.00007508
Iteration 139/1000 | Loss: 0.00007128
Iteration 140/1000 | Loss: 0.00007149
Iteration 141/1000 | Loss: 0.00007568
Iteration 142/1000 | Loss: 0.00006530
Iteration 143/1000 | Loss: 0.00006914
Iteration 144/1000 | Loss: 0.00007063
Iteration 145/1000 | Loss: 0.00006832
Iteration 146/1000 | Loss: 0.00027330
Iteration 147/1000 | Loss: 0.00006818
Iteration 148/1000 | Loss: 0.00006563
Iteration 149/1000 | Loss: 0.00006844
Iteration 150/1000 | Loss: 0.00007008
Iteration 151/1000 | Loss: 0.00006790
Iteration 152/1000 | Loss: 0.00007236
Iteration 153/1000 | Loss: 0.00129128
Iteration 154/1000 | Loss: 0.00215166
Iteration 155/1000 | Loss: 0.00151376
Iteration 156/1000 | Loss: 0.00064821
Iteration 157/1000 | Loss: 0.00023141
Iteration 158/1000 | Loss: 0.00018046
Iteration 159/1000 | Loss: 0.00035994
Iteration 160/1000 | Loss: 0.00005620
Iteration 161/1000 | Loss: 0.00006279
Iteration 162/1000 | Loss: 0.00006897
Iteration 163/1000 | Loss: 0.00021033
Iteration 164/1000 | Loss: 0.00015189
Iteration 165/1000 | Loss: 0.00019344
Iteration 166/1000 | Loss: 0.00009117
Iteration 167/1000 | Loss: 0.00019946
Iteration 168/1000 | Loss: 0.00015128
Iteration 169/1000 | Loss: 0.00020011
Iteration 170/1000 | Loss: 0.00029523
Iteration 171/1000 | Loss: 0.00155917
Iteration 172/1000 | Loss: 0.00007282
Iteration 173/1000 | Loss: 0.00031777
Iteration 174/1000 | Loss: 0.00003699
Iteration 175/1000 | Loss: 0.00016335
Iteration 176/1000 | Loss: 0.00003056
Iteration 177/1000 | Loss: 0.00009280
Iteration 178/1000 | Loss: 0.00011382
Iteration 179/1000 | Loss: 0.00045979
Iteration 180/1000 | Loss: 0.00024852
Iteration 181/1000 | Loss: 0.00016703
Iteration 182/1000 | Loss: 0.00027899
Iteration 183/1000 | Loss: 0.00066646
Iteration 184/1000 | Loss: 0.00009516
Iteration 185/1000 | Loss: 0.00003921
Iteration 186/1000 | Loss: 0.00002736
Iteration 187/1000 | Loss: 0.00002564
Iteration 188/1000 | Loss: 0.00002436
Iteration 189/1000 | Loss: 0.00002354
Iteration 190/1000 | Loss: 0.00002287
Iteration 191/1000 | Loss: 0.00002234
Iteration 192/1000 | Loss: 0.00002210
Iteration 193/1000 | Loss: 0.00025067
Iteration 194/1000 | Loss: 0.00002610
Iteration 195/1000 | Loss: 0.00002339
Iteration 196/1000 | Loss: 0.00002225
Iteration 197/1000 | Loss: 0.00002173
Iteration 198/1000 | Loss: 0.00002135
Iteration 199/1000 | Loss: 0.00002129
Iteration 200/1000 | Loss: 0.00002126
Iteration 201/1000 | Loss: 0.00002125
Iteration 202/1000 | Loss: 0.00002125
Iteration 203/1000 | Loss: 0.00002125
Iteration 204/1000 | Loss: 0.00002125
Iteration 205/1000 | Loss: 0.00002125
Iteration 206/1000 | Loss: 0.00002125
Iteration 207/1000 | Loss: 0.00002125
Iteration 208/1000 | Loss: 0.00002125
Iteration 209/1000 | Loss: 0.00002124
Iteration 210/1000 | Loss: 0.00002124
Iteration 211/1000 | Loss: 0.00002124
Iteration 212/1000 | Loss: 0.00002124
Iteration 213/1000 | Loss: 0.00002124
Iteration 214/1000 | Loss: 0.00002123
Iteration 215/1000 | Loss: 0.00002123
Iteration 216/1000 | Loss: 0.00002123
Iteration 217/1000 | Loss: 0.00002123
Iteration 218/1000 | Loss: 0.00002123
Iteration 219/1000 | Loss: 0.00002123
Iteration 220/1000 | Loss: 0.00002123
Iteration 221/1000 | Loss: 0.00002123
Iteration 222/1000 | Loss: 0.00002122
Iteration 223/1000 | Loss: 0.00002122
Iteration 224/1000 | Loss: 0.00002122
Iteration 225/1000 | Loss: 0.00002122
Iteration 226/1000 | Loss: 0.00002122
Iteration 227/1000 | Loss: 0.00002122
Iteration 228/1000 | Loss: 0.00002122
Iteration 229/1000 | Loss: 0.00002122
Iteration 230/1000 | Loss: 0.00002121
Iteration 231/1000 | Loss: 0.00002121
Iteration 232/1000 | Loss: 0.00002121
Iteration 233/1000 | Loss: 0.00002120
Iteration 234/1000 | Loss: 0.00002120
Iteration 235/1000 | Loss: 0.00002120
Iteration 236/1000 | Loss: 0.00002120
Iteration 237/1000 | Loss: 0.00002119
Iteration 238/1000 | Loss: 0.00002119
Iteration 239/1000 | Loss: 0.00002119
Iteration 240/1000 | Loss: 0.00002119
Iteration 241/1000 | Loss: 0.00002119
Iteration 242/1000 | Loss: 0.00002118
Iteration 243/1000 | Loss: 0.00002118
Iteration 244/1000 | Loss: 0.00002118
Iteration 245/1000 | Loss: 0.00002118
Iteration 246/1000 | Loss: 0.00002118
Iteration 247/1000 | Loss: 0.00002117
Iteration 248/1000 | Loss: 0.00002117
Iteration 249/1000 | Loss: 0.00002117
Iteration 250/1000 | Loss: 0.00002117
Iteration 251/1000 | Loss: 0.00002117
Iteration 252/1000 | Loss: 0.00002116
Iteration 253/1000 | Loss: 0.00002116
Iteration 254/1000 | Loss: 0.00002115
Iteration 255/1000 | Loss: 0.00002115
Iteration 256/1000 | Loss: 0.00002115
Iteration 257/1000 | Loss: 0.00002115
Iteration 258/1000 | Loss: 0.00002115
Iteration 259/1000 | Loss: 0.00002115
Iteration 260/1000 | Loss: 0.00002115
Iteration 261/1000 | Loss: 0.00002115
Iteration 262/1000 | Loss: 0.00002115
Iteration 263/1000 | Loss: 0.00002115
Iteration 264/1000 | Loss: 0.00002115
Iteration 265/1000 | Loss: 0.00002114
Iteration 266/1000 | Loss: 0.00002114
Iteration 267/1000 | Loss: 0.00002114
Iteration 268/1000 | Loss: 0.00002114
Iteration 269/1000 | Loss: 0.00002114
Iteration 270/1000 | Loss: 0.00002114
Iteration 271/1000 | Loss: 0.00002113
Iteration 272/1000 | Loss: 0.00002113
Iteration 273/1000 | Loss: 0.00002112
Iteration 274/1000 | Loss: 0.00002112
Iteration 275/1000 | Loss: 0.00002112
Iteration 276/1000 | Loss: 0.00002112
Iteration 277/1000 | Loss: 0.00002112
Iteration 278/1000 | Loss: 0.00002112
Iteration 279/1000 | Loss: 0.00002112
Iteration 280/1000 | Loss: 0.00002112
Iteration 281/1000 | Loss: 0.00002112
Iteration 282/1000 | Loss: 0.00002112
Iteration 283/1000 | Loss: 0.00002112
Iteration 284/1000 | Loss: 0.00002112
Iteration 285/1000 | Loss: 0.00002111
Iteration 286/1000 | Loss: 0.00002111
Iteration 287/1000 | Loss: 0.00002111
Iteration 288/1000 | Loss: 0.00002111
Iteration 289/1000 | Loss: 0.00002111
Iteration 290/1000 | Loss: 0.00002110
Iteration 291/1000 | Loss: 0.00002110
Iteration 292/1000 | Loss: 0.00002110
Iteration 293/1000 | Loss: 0.00002110
Iteration 294/1000 | Loss: 0.00002110
Iteration 295/1000 | Loss: 0.00002110
Iteration 296/1000 | Loss: 0.00002110
Iteration 297/1000 | Loss: 0.00002110
Iteration 298/1000 | Loss: 0.00002110
Iteration 299/1000 | Loss: 0.00002110
Iteration 300/1000 | Loss: 0.00002110
Iteration 301/1000 | Loss: 0.00002110
Iteration 302/1000 | Loss: 0.00002110
Iteration 303/1000 | Loss: 0.00002110
Iteration 304/1000 | Loss: 0.00002109
Iteration 305/1000 | Loss: 0.00002109
Iteration 306/1000 | Loss: 0.00002109
Iteration 307/1000 | Loss: 0.00002109
Iteration 308/1000 | Loss: 0.00002109
Iteration 309/1000 | Loss: 0.00002109
Iteration 310/1000 | Loss: 0.00002109
Iteration 311/1000 | Loss: 0.00002109
Iteration 312/1000 | Loss: 0.00002109
Iteration 313/1000 | Loss: 0.00002109
Iteration 314/1000 | Loss: 0.00002109
Iteration 315/1000 | Loss: 0.00002108
Iteration 316/1000 | Loss: 0.00002108
Iteration 317/1000 | Loss: 0.00002108
Iteration 318/1000 | Loss: 0.00002108
Iteration 319/1000 | Loss: 0.00002108
Iteration 320/1000 | Loss: 0.00002108
Iteration 321/1000 | Loss: 0.00002108
Iteration 322/1000 | Loss: 0.00002108
Iteration 323/1000 | Loss: 0.00002108
Iteration 324/1000 | Loss: 0.00002108
Iteration 325/1000 | Loss: 0.00002108
Iteration 326/1000 | Loss: 0.00002108
Iteration 327/1000 | Loss: 0.00002107
Iteration 328/1000 | Loss: 0.00002107
Iteration 329/1000 | Loss: 0.00002107
Iteration 330/1000 | Loss: 0.00002107
Iteration 331/1000 | Loss: 0.00002107
Iteration 332/1000 | Loss: 0.00002107
Iteration 333/1000 | Loss: 0.00002107
Iteration 334/1000 | Loss: 0.00002107
Iteration 335/1000 | Loss: 0.00002107
Iteration 336/1000 | Loss: 0.00002106
Iteration 337/1000 | Loss: 0.00002106
Iteration 338/1000 | Loss: 0.00002106
Iteration 339/1000 | Loss: 0.00002106
Iteration 340/1000 | Loss: 0.00002106
Iteration 341/1000 | Loss: 0.00002106
Iteration 342/1000 | Loss: 0.00002106
Iteration 343/1000 | Loss: 0.00002106
Iteration 344/1000 | Loss: 0.00002105
Iteration 345/1000 | Loss: 0.00002105
Iteration 346/1000 | Loss: 0.00002105
Iteration 347/1000 | Loss: 0.00002105
Iteration 348/1000 | Loss: 0.00002105
Iteration 349/1000 | Loss: 0.00002104
Iteration 350/1000 | Loss: 0.00002104
Iteration 351/1000 | Loss: 0.00002104
Iteration 352/1000 | Loss: 0.00002104
Iteration 353/1000 | Loss: 0.00002104
Iteration 354/1000 | Loss: 0.00002104
Iteration 355/1000 | Loss: 0.00002104
Iteration 356/1000 | Loss: 0.00002104
Iteration 357/1000 | Loss: 0.00002104
Iteration 358/1000 | Loss: 0.00002104
Iteration 359/1000 | Loss: 0.00002103
Iteration 360/1000 | Loss: 0.00002103
Iteration 361/1000 | Loss: 0.00002103
Iteration 362/1000 | Loss: 0.00002103
Iteration 363/1000 | Loss: 0.00002103
Iteration 364/1000 | Loss: 0.00002103
Iteration 365/1000 | Loss: 0.00002103
Iteration 366/1000 | Loss: 0.00002103
Iteration 367/1000 | Loss: 0.00002103
Iteration 368/1000 | Loss: 0.00002103
Iteration 369/1000 | Loss: 0.00002103
Iteration 370/1000 | Loss: 0.00002103
Iteration 371/1000 | Loss: 0.00002103
Iteration 372/1000 | Loss: 0.00002103
Iteration 373/1000 | Loss: 0.00002103
Iteration 374/1000 | Loss: 0.00002103
Iteration 375/1000 | Loss: 0.00002103
Iteration 376/1000 | Loss: 0.00002103
Iteration 377/1000 | Loss: 0.00002103
Iteration 378/1000 | Loss: 0.00002103
Iteration 379/1000 | Loss: 0.00002103
Iteration 380/1000 | Loss: 0.00002103
Iteration 381/1000 | Loss: 0.00002103
Iteration 382/1000 | Loss: 0.00002103
Iteration 383/1000 | Loss: 0.00002103
Iteration 384/1000 | Loss: 0.00002103
Iteration 385/1000 | Loss: 0.00002103
Iteration 386/1000 | Loss: 0.00002103
Iteration 387/1000 | Loss: 0.00002103
Iteration 388/1000 | Loss: 0.00002103
Iteration 389/1000 | Loss: 0.00002103
Iteration 390/1000 | Loss: 0.00002103
Iteration 391/1000 | Loss: 0.00002103
Iteration 392/1000 | Loss: 0.00002103
Iteration 393/1000 | Loss: 0.00002103
Iteration 394/1000 | Loss: 0.00002103
Iteration 395/1000 | Loss: 0.00002103
Iteration 396/1000 | Loss: 0.00002103
Iteration 397/1000 | Loss: 0.00002103
Iteration 398/1000 | Loss: 0.00002103
Iteration 399/1000 | Loss: 0.00002103
Iteration 400/1000 | Loss: 0.00002103
Iteration 401/1000 | Loss: 0.00002103
Iteration 402/1000 | Loss: 0.00002103
Iteration 403/1000 | Loss: 0.00002103
Iteration 404/1000 | Loss: 0.00002103
Iteration 405/1000 | Loss: 0.00002103
Iteration 406/1000 | Loss: 0.00002103
Iteration 407/1000 | Loss: 0.00002103
Iteration 408/1000 | Loss: 0.00002103
Iteration 409/1000 | Loss: 0.00002103
Iteration 410/1000 | Loss: 0.00002103
Iteration 411/1000 | Loss: 0.00002103
Iteration 412/1000 | Loss: 0.00002103
Iteration 413/1000 | Loss: 0.00002103
Iteration 414/1000 | Loss: 0.00002103
Iteration 415/1000 | Loss: 0.00002103
Iteration 416/1000 | Loss: 0.00002103
Iteration 417/1000 | Loss: 0.00002103
Iteration 418/1000 | Loss: 0.00002103
Iteration 419/1000 | Loss: 0.00002103
Iteration 420/1000 | Loss: 0.00002103
Iteration 421/1000 | Loss: 0.00002103
Iteration 422/1000 | Loss: 0.00002103
Iteration 423/1000 | Loss: 0.00002103
Iteration 424/1000 | Loss: 0.00002103
Iteration 425/1000 | Loss: 0.00002103
Iteration 426/1000 | Loss: 0.00002103
Iteration 427/1000 | Loss: 0.00002103
Iteration 428/1000 | Loss: 0.00002103
Iteration 429/1000 | Loss: 0.00002103
Iteration 430/1000 | Loss: 0.00002103
Iteration 431/1000 | Loss: 0.00002103
Iteration 432/1000 | Loss: 0.00002103
Iteration 433/1000 | Loss: 0.00002103
Iteration 434/1000 | Loss: 0.00002103
Iteration 435/1000 | Loss: 0.00002103
Iteration 436/1000 | Loss: 0.00002103
Iteration 437/1000 | Loss: 0.00002103
Iteration 438/1000 | Loss: 0.00002103
Iteration 439/1000 | Loss: 0.00002103
Iteration 440/1000 | Loss: 0.00002103
Iteration 441/1000 | Loss: 0.00002103
Iteration 442/1000 | Loss: 0.00002103
Iteration 443/1000 | Loss: 0.00002103
Iteration 444/1000 | Loss: 0.00002103
Iteration 445/1000 | Loss: 0.00002103
Iteration 446/1000 | Loss: 0.00002103
Iteration 447/1000 | Loss: 0.00002103
Iteration 448/1000 | Loss: 0.00002103
Iteration 449/1000 | Loss: 0.00002103
Iteration 450/1000 | Loss: 0.00002103
Iteration 451/1000 | Loss: 0.00002103
Iteration 452/1000 | Loss: 0.00002103
Iteration 453/1000 | Loss: 0.00002103
Iteration 454/1000 | Loss: 0.00002103
Iteration 455/1000 | Loss: 0.00002103
Iteration 456/1000 | Loss: 0.00002103
Iteration 457/1000 | Loss: 0.00002103
Iteration 458/1000 | Loss: 0.00002103
Iteration 459/1000 | Loss: 0.00002103
Iteration 460/1000 | Loss: 0.00002103
Iteration 461/1000 | Loss: 0.00002103
Iteration 462/1000 | Loss: 0.00002103
Iteration 463/1000 | Loss: 0.00002103
Iteration 464/1000 | Loss: 0.00002103
Iteration 465/1000 | Loss: 0.00002103
Iteration 466/1000 | Loss: 0.00002103
Iteration 467/1000 | Loss: 0.00002103
Iteration 468/1000 | Loss: 0.00002103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 468. Stopping optimization.
Last 5 losses: [2.1027899492764845e-05, 2.1027899492764845e-05, 2.1027899492764845e-05, 2.1027899492764845e-05, 2.1027899492764845e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1027899492764845e-05

Optimization complete. Final v2v error: 3.874122142791748 mm

Highest mean error: 6.173070907592773 mm for frame 172

Lowest mean error: 3.3006279468536377 mm for frame 24

Saving results

Total time: 351.724821805954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768424
Iteration 2/25 | Loss: 0.00214403
Iteration 3/25 | Loss: 0.00160110
Iteration 4/25 | Loss: 0.00151362
Iteration 5/25 | Loss: 0.00146692
Iteration 6/25 | Loss: 0.00144646
Iteration 7/25 | Loss: 0.00136506
Iteration 8/25 | Loss: 0.00132843
Iteration 9/25 | Loss: 0.00131137
Iteration 10/25 | Loss: 0.00129491
Iteration 11/25 | Loss: 0.00129028
Iteration 12/25 | Loss: 0.00129151
Iteration 13/25 | Loss: 0.00128824
Iteration 14/25 | Loss: 0.00128518
Iteration 15/25 | Loss: 0.00128541
Iteration 16/25 | Loss: 0.00127797
Iteration 17/25 | Loss: 0.00127342
Iteration 18/25 | Loss: 0.00127283
Iteration 19/25 | Loss: 0.00127272
Iteration 20/25 | Loss: 0.00127272
Iteration 21/25 | Loss: 0.00127272
Iteration 22/25 | Loss: 0.00127272
Iteration 23/25 | Loss: 0.00127272
Iteration 24/25 | Loss: 0.00127272
Iteration 25/25 | Loss: 0.00127271

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 11.13234615
Iteration 2/25 | Loss: 0.00084466
Iteration 3/25 | Loss: 0.00084452
Iteration 4/25 | Loss: 0.00084452
Iteration 5/25 | Loss: 0.00084452
Iteration 6/25 | Loss: 0.00084452
Iteration 7/25 | Loss: 0.00084452
Iteration 8/25 | Loss: 0.00084452
Iteration 9/25 | Loss: 0.00084452
Iteration 10/25 | Loss: 0.00084452
Iteration 11/25 | Loss: 0.00084452
Iteration 12/25 | Loss: 0.00084452
Iteration 13/25 | Loss: 0.00084452
Iteration 14/25 | Loss: 0.00084452
Iteration 15/25 | Loss: 0.00084452
Iteration 16/25 | Loss: 0.00084452
Iteration 17/25 | Loss: 0.00084452
Iteration 18/25 | Loss: 0.00084452
Iteration 19/25 | Loss: 0.00084452
Iteration 20/25 | Loss: 0.00084452
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008445177809335291, 0.0008445177809335291, 0.0008445177809335291, 0.0008445177809335291, 0.0008445177809335291]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008445177809335291

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084452
Iteration 2/1000 | Loss: 0.00003868
Iteration 3/1000 | Loss: 0.00017790
Iteration 4/1000 | Loss: 0.00002571
Iteration 5/1000 | Loss: 0.00002423
Iteration 6/1000 | Loss: 0.00002338
Iteration 7/1000 | Loss: 0.00002277
Iteration 8/1000 | Loss: 0.00002241
Iteration 9/1000 | Loss: 0.00010343
Iteration 10/1000 | Loss: 0.00002404
Iteration 11/1000 | Loss: 0.00002212
Iteration 12/1000 | Loss: 0.00002137
Iteration 13/1000 | Loss: 0.00002059
Iteration 14/1000 | Loss: 0.00002013
Iteration 15/1000 | Loss: 0.00001992
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001988
Iteration 18/1000 | Loss: 0.00001977
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001956
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001942
Iteration 23/1000 | Loss: 0.00001942
Iteration 24/1000 | Loss: 0.00001942
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001941
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001941
Iteration 31/1000 | Loss: 0.00001941
Iteration 32/1000 | Loss: 0.00001940
Iteration 33/1000 | Loss: 0.00001938
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001936
Iteration 36/1000 | Loss: 0.00001930
Iteration 37/1000 | Loss: 0.00001928
Iteration 38/1000 | Loss: 0.00001927
Iteration 39/1000 | Loss: 0.00001927
Iteration 40/1000 | Loss: 0.00001927
Iteration 41/1000 | Loss: 0.00001927
Iteration 42/1000 | Loss: 0.00001927
Iteration 43/1000 | Loss: 0.00001926
Iteration 44/1000 | Loss: 0.00001926
Iteration 45/1000 | Loss: 0.00001926
Iteration 46/1000 | Loss: 0.00001926
Iteration 47/1000 | Loss: 0.00001926
Iteration 48/1000 | Loss: 0.00001925
Iteration 49/1000 | Loss: 0.00001925
Iteration 50/1000 | Loss: 0.00001925
Iteration 51/1000 | Loss: 0.00001924
Iteration 52/1000 | Loss: 0.00001924
Iteration 53/1000 | Loss: 0.00001924
Iteration 54/1000 | Loss: 0.00001924
Iteration 55/1000 | Loss: 0.00001924
Iteration 56/1000 | Loss: 0.00001923
Iteration 57/1000 | Loss: 0.00001923
Iteration 58/1000 | Loss: 0.00001923
Iteration 59/1000 | Loss: 0.00001923
Iteration 60/1000 | Loss: 0.00001923
Iteration 61/1000 | Loss: 0.00001923
Iteration 62/1000 | Loss: 0.00001923
Iteration 63/1000 | Loss: 0.00001923
Iteration 64/1000 | Loss: 0.00001923
Iteration 65/1000 | Loss: 0.00001923
Iteration 66/1000 | Loss: 0.00001923
Iteration 67/1000 | Loss: 0.00001923
Iteration 68/1000 | Loss: 0.00001922
Iteration 69/1000 | Loss: 0.00001922
Iteration 70/1000 | Loss: 0.00001922
Iteration 71/1000 | Loss: 0.00001922
Iteration 72/1000 | Loss: 0.00001922
Iteration 73/1000 | Loss: 0.00001922
Iteration 74/1000 | Loss: 0.00001922
Iteration 75/1000 | Loss: 0.00001922
Iteration 76/1000 | Loss: 0.00001922
Iteration 77/1000 | Loss: 0.00001922
Iteration 78/1000 | Loss: 0.00001922
Iteration 79/1000 | Loss: 0.00001922
Iteration 80/1000 | Loss: 0.00001922
Iteration 81/1000 | Loss: 0.00001922
Iteration 82/1000 | Loss: 0.00001922
Iteration 83/1000 | Loss: 0.00001921
Iteration 84/1000 | Loss: 0.00001921
Iteration 85/1000 | Loss: 0.00001921
Iteration 86/1000 | Loss: 0.00001921
Iteration 87/1000 | Loss: 0.00001921
Iteration 88/1000 | Loss: 0.00001921
Iteration 89/1000 | Loss: 0.00001921
Iteration 90/1000 | Loss: 0.00001921
Iteration 91/1000 | Loss: 0.00001921
Iteration 92/1000 | Loss: 0.00001921
Iteration 93/1000 | Loss: 0.00001920
Iteration 94/1000 | Loss: 0.00001920
Iteration 95/1000 | Loss: 0.00001920
Iteration 96/1000 | Loss: 0.00001920
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001920
Iteration 99/1000 | Loss: 0.00001920
Iteration 100/1000 | Loss: 0.00001919
Iteration 101/1000 | Loss: 0.00001919
Iteration 102/1000 | Loss: 0.00001919
Iteration 103/1000 | Loss: 0.00001919
Iteration 104/1000 | Loss: 0.00001919
Iteration 105/1000 | Loss: 0.00001919
Iteration 106/1000 | Loss: 0.00001919
Iteration 107/1000 | Loss: 0.00001918
Iteration 108/1000 | Loss: 0.00001918
Iteration 109/1000 | Loss: 0.00001918
Iteration 110/1000 | Loss: 0.00001918
Iteration 111/1000 | Loss: 0.00001918
Iteration 112/1000 | Loss: 0.00001917
Iteration 113/1000 | Loss: 0.00001917
Iteration 114/1000 | Loss: 0.00001917
Iteration 115/1000 | Loss: 0.00001917
Iteration 116/1000 | Loss: 0.00001917
Iteration 117/1000 | Loss: 0.00001917
Iteration 118/1000 | Loss: 0.00001917
Iteration 119/1000 | Loss: 0.00001917
Iteration 120/1000 | Loss: 0.00001917
Iteration 121/1000 | Loss: 0.00001917
Iteration 122/1000 | Loss: 0.00001917
Iteration 123/1000 | Loss: 0.00001917
Iteration 124/1000 | Loss: 0.00001917
Iteration 125/1000 | Loss: 0.00001916
Iteration 126/1000 | Loss: 0.00001916
Iteration 127/1000 | Loss: 0.00001916
Iteration 128/1000 | Loss: 0.00001916
Iteration 129/1000 | Loss: 0.00001916
Iteration 130/1000 | Loss: 0.00001916
Iteration 131/1000 | Loss: 0.00001916
Iteration 132/1000 | Loss: 0.00001916
Iteration 133/1000 | Loss: 0.00001916
Iteration 134/1000 | Loss: 0.00001916
Iteration 135/1000 | Loss: 0.00001915
Iteration 136/1000 | Loss: 0.00001915
Iteration 137/1000 | Loss: 0.00001915
Iteration 138/1000 | Loss: 0.00001915
Iteration 139/1000 | Loss: 0.00001915
Iteration 140/1000 | Loss: 0.00001915
Iteration 141/1000 | Loss: 0.00001915
Iteration 142/1000 | Loss: 0.00001915
Iteration 143/1000 | Loss: 0.00001915
Iteration 144/1000 | Loss: 0.00001915
Iteration 145/1000 | Loss: 0.00001915
Iteration 146/1000 | Loss: 0.00001915
Iteration 147/1000 | Loss: 0.00001915
Iteration 148/1000 | Loss: 0.00001915
Iteration 149/1000 | Loss: 0.00001915
Iteration 150/1000 | Loss: 0.00001914
Iteration 151/1000 | Loss: 0.00001914
Iteration 152/1000 | Loss: 0.00001914
Iteration 153/1000 | Loss: 0.00001914
Iteration 154/1000 | Loss: 0.00001914
Iteration 155/1000 | Loss: 0.00001914
Iteration 156/1000 | Loss: 0.00001914
Iteration 157/1000 | Loss: 0.00001914
Iteration 158/1000 | Loss: 0.00001914
Iteration 159/1000 | Loss: 0.00001914
Iteration 160/1000 | Loss: 0.00001914
Iteration 161/1000 | Loss: 0.00001914
Iteration 162/1000 | Loss: 0.00001914
Iteration 163/1000 | Loss: 0.00001914
Iteration 164/1000 | Loss: 0.00001914
Iteration 165/1000 | Loss: 0.00001914
Iteration 166/1000 | Loss: 0.00001914
Iteration 167/1000 | Loss: 0.00001914
Iteration 168/1000 | Loss: 0.00001913
Iteration 169/1000 | Loss: 0.00001913
Iteration 170/1000 | Loss: 0.00001913
Iteration 171/1000 | Loss: 0.00001913
Iteration 172/1000 | Loss: 0.00001913
Iteration 173/1000 | Loss: 0.00001913
Iteration 174/1000 | Loss: 0.00001913
Iteration 175/1000 | Loss: 0.00001913
Iteration 176/1000 | Loss: 0.00001913
Iteration 177/1000 | Loss: 0.00001913
Iteration 178/1000 | Loss: 0.00001913
Iteration 179/1000 | Loss: 0.00001913
Iteration 180/1000 | Loss: 0.00001913
Iteration 181/1000 | Loss: 0.00001913
Iteration 182/1000 | Loss: 0.00001913
Iteration 183/1000 | Loss: 0.00001913
Iteration 184/1000 | Loss: 0.00001913
Iteration 185/1000 | Loss: 0.00001913
Iteration 186/1000 | Loss: 0.00001913
Iteration 187/1000 | Loss: 0.00001913
Iteration 188/1000 | Loss: 0.00001913
Iteration 189/1000 | Loss: 0.00001913
Iteration 190/1000 | Loss: 0.00001913
Iteration 191/1000 | Loss: 0.00001913
Iteration 192/1000 | Loss: 0.00001913
Iteration 193/1000 | Loss: 0.00001913
Iteration 194/1000 | Loss: 0.00001913
Iteration 195/1000 | Loss: 0.00001913
Iteration 196/1000 | Loss: 0.00001913
Iteration 197/1000 | Loss: 0.00001913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.9127171981381252e-05, 1.9127171981381252e-05, 1.9127171981381252e-05, 1.9127171981381252e-05, 1.9127171981381252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9127171981381252e-05

Optimization complete. Final v2v error: 3.617814302444458 mm

Highest mean error: 4.165993690490723 mm for frame 150

Lowest mean error: 3.2779393196105957 mm for frame 148

Saving results

Total time: 73.82085824012756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858872
Iteration 2/25 | Loss: 0.00217663
Iteration 3/25 | Loss: 0.00184914
Iteration 4/25 | Loss: 0.00166212
Iteration 5/25 | Loss: 0.00147717
Iteration 6/25 | Loss: 0.00138983
Iteration 7/25 | Loss: 0.00136662
Iteration 8/25 | Loss: 0.00135867
Iteration 9/25 | Loss: 0.00135408
Iteration 10/25 | Loss: 0.00135370
Iteration 11/25 | Loss: 0.00135116
Iteration 12/25 | Loss: 0.00135008
Iteration 13/25 | Loss: 0.00135143
Iteration 14/25 | Loss: 0.00134769
Iteration 15/25 | Loss: 0.00134713
Iteration 16/25 | Loss: 0.00134593
Iteration 17/25 | Loss: 0.00134422
Iteration 18/25 | Loss: 0.00134350
Iteration 19/25 | Loss: 0.00134311
Iteration 20/25 | Loss: 0.00134342
Iteration 21/25 | Loss: 0.00134370
Iteration 22/25 | Loss: 0.00134259
Iteration 23/25 | Loss: 0.00134321
Iteration 24/25 | Loss: 0.00134295
Iteration 25/25 | Loss: 0.00134321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33834493
Iteration 2/25 | Loss: 0.00067252
Iteration 3/25 | Loss: 0.00067249
Iteration 4/25 | Loss: 0.00067249
Iteration 5/25 | Loss: 0.00067249
Iteration 6/25 | Loss: 0.00067249
Iteration 7/25 | Loss: 0.00067249
Iteration 8/25 | Loss: 0.00067249
Iteration 9/25 | Loss: 0.00067249
Iteration 10/25 | Loss: 0.00067249
Iteration 11/25 | Loss: 0.00067249
Iteration 12/25 | Loss: 0.00067249
Iteration 13/25 | Loss: 0.00067249
Iteration 14/25 | Loss: 0.00067249
Iteration 15/25 | Loss: 0.00067249
Iteration 16/25 | Loss: 0.00067249
Iteration 17/25 | Loss: 0.00067249
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000672488531563431, 0.000672488531563431, 0.000672488531563431, 0.000672488531563431, 0.000672488531563431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000672488531563431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067249
Iteration 2/1000 | Loss: 0.00004215
Iteration 3/1000 | Loss: 0.00003807
Iteration 4/1000 | Loss: 0.00004049
Iteration 5/1000 | Loss: 0.00003893
Iteration 6/1000 | Loss: 0.00003508
Iteration 7/1000 | Loss: 0.00003160
Iteration 8/1000 | Loss: 0.00002606
Iteration 9/1000 | Loss: 0.00003285
Iteration 10/1000 | Loss: 0.00002745
Iteration 11/1000 | Loss: 0.00002524
Iteration 12/1000 | Loss: 0.00002327
Iteration 13/1000 | Loss: 0.00002880
Iteration 14/1000 | Loss: 0.00003000
Iteration 15/1000 | Loss: 0.00003002
Iteration 16/1000 | Loss: 0.00002569
Iteration 17/1000 | Loss: 0.00004401
Iteration 18/1000 | Loss: 0.00003203
Iteration 19/1000 | Loss: 0.00002875
Iteration 20/1000 | Loss: 0.00002609
Iteration 21/1000 | Loss: 0.00003232
Iteration 22/1000 | Loss: 0.00002799
Iteration 23/1000 | Loss: 0.00003185
Iteration 24/1000 | Loss: 0.00002766
Iteration 25/1000 | Loss: 0.00003181
Iteration 26/1000 | Loss: 0.00002708
Iteration 27/1000 | Loss: 0.00002881
Iteration 28/1000 | Loss: 0.00003580
Iteration 29/1000 | Loss: 0.00003340
Iteration 30/1000 | Loss: 0.00003386
Iteration 31/1000 | Loss: 0.00002692
Iteration 32/1000 | Loss: 0.00002904
Iteration 33/1000 | Loss: 0.00003393
Iteration 34/1000 | Loss: 0.00003066
Iteration 35/1000 | Loss: 0.00002691
Iteration 36/1000 | Loss: 0.00002612
Iteration 37/1000 | Loss: 0.00002719
Iteration 38/1000 | Loss: 0.00002652
Iteration 39/1000 | Loss: 0.00003204
Iteration 40/1000 | Loss: 0.00002804
Iteration 41/1000 | Loss: 0.00002736
Iteration 42/1000 | Loss: 0.00003086
Iteration 43/1000 | Loss: 0.00003725
Iteration 44/1000 | Loss: 0.00002852
Iteration 45/1000 | Loss: 0.00002333
Iteration 46/1000 | Loss: 0.00002249
Iteration 47/1000 | Loss: 0.00002180
Iteration 48/1000 | Loss: 0.00002145
Iteration 49/1000 | Loss: 0.00002115
Iteration 50/1000 | Loss: 0.00002107
Iteration 51/1000 | Loss: 0.00002105
Iteration 52/1000 | Loss: 0.00002105
Iteration 53/1000 | Loss: 0.00002105
Iteration 54/1000 | Loss: 0.00002105
Iteration 55/1000 | Loss: 0.00002105
Iteration 56/1000 | Loss: 0.00002104
Iteration 57/1000 | Loss: 0.00002103
Iteration 58/1000 | Loss: 0.00002103
Iteration 59/1000 | Loss: 0.00002102
Iteration 60/1000 | Loss: 0.00002102
Iteration 61/1000 | Loss: 0.00002102
Iteration 62/1000 | Loss: 0.00002101
Iteration 63/1000 | Loss: 0.00002100
Iteration 64/1000 | Loss: 0.00002100
Iteration 65/1000 | Loss: 0.00002100
Iteration 66/1000 | Loss: 0.00002100
Iteration 67/1000 | Loss: 0.00002099
Iteration 68/1000 | Loss: 0.00002099
Iteration 69/1000 | Loss: 0.00002099
Iteration 70/1000 | Loss: 0.00002099
Iteration 71/1000 | Loss: 0.00002099
Iteration 72/1000 | Loss: 0.00002099
Iteration 73/1000 | Loss: 0.00002099
Iteration 74/1000 | Loss: 0.00002099
Iteration 75/1000 | Loss: 0.00002099
Iteration 76/1000 | Loss: 0.00002098
Iteration 77/1000 | Loss: 0.00002098
Iteration 78/1000 | Loss: 0.00002098
Iteration 79/1000 | Loss: 0.00002098
Iteration 80/1000 | Loss: 0.00002098
Iteration 81/1000 | Loss: 0.00002098
Iteration 82/1000 | Loss: 0.00002098
Iteration 83/1000 | Loss: 0.00002098
Iteration 84/1000 | Loss: 0.00002098
Iteration 85/1000 | Loss: 0.00002098
Iteration 86/1000 | Loss: 0.00002098
Iteration 87/1000 | Loss: 0.00002097
Iteration 88/1000 | Loss: 0.00002097
Iteration 89/1000 | Loss: 0.00002097
Iteration 90/1000 | Loss: 0.00002097
Iteration 91/1000 | Loss: 0.00002097
Iteration 92/1000 | Loss: 0.00002097
Iteration 93/1000 | Loss: 0.00002096
Iteration 94/1000 | Loss: 0.00002096
Iteration 95/1000 | Loss: 0.00002096
Iteration 96/1000 | Loss: 0.00002096
Iteration 97/1000 | Loss: 0.00002095
Iteration 98/1000 | Loss: 0.00002095
Iteration 99/1000 | Loss: 0.00002095
Iteration 100/1000 | Loss: 0.00002095
Iteration 101/1000 | Loss: 0.00002095
Iteration 102/1000 | Loss: 0.00002095
Iteration 103/1000 | Loss: 0.00002095
Iteration 104/1000 | Loss: 0.00002095
Iteration 105/1000 | Loss: 0.00002095
Iteration 106/1000 | Loss: 0.00002095
Iteration 107/1000 | Loss: 0.00002094
Iteration 108/1000 | Loss: 0.00002094
Iteration 109/1000 | Loss: 0.00002094
Iteration 110/1000 | Loss: 0.00002094
Iteration 111/1000 | Loss: 0.00002094
Iteration 112/1000 | Loss: 0.00002094
Iteration 113/1000 | Loss: 0.00002093
Iteration 114/1000 | Loss: 0.00002093
Iteration 115/1000 | Loss: 0.00002093
Iteration 116/1000 | Loss: 0.00002093
Iteration 117/1000 | Loss: 0.00002092
Iteration 118/1000 | Loss: 0.00002092
Iteration 119/1000 | Loss: 0.00002092
Iteration 120/1000 | Loss: 0.00002092
Iteration 121/1000 | Loss: 0.00002092
Iteration 122/1000 | Loss: 0.00002092
Iteration 123/1000 | Loss: 0.00002091
Iteration 124/1000 | Loss: 0.00002091
Iteration 125/1000 | Loss: 0.00002091
Iteration 126/1000 | Loss: 0.00002091
Iteration 127/1000 | Loss: 0.00002091
Iteration 128/1000 | Loss: 0.00002091
Iteration 129/1000 | Loss: 0.00002091
Iteration 130/1000 | Loss: 0.00002091
Iteration 131/1000 | Loss: 0.00002091
Iteration 132/1000 | Loss: 0.00002090
Iteration 133/1000 | Loss: 0.00002090
Iteration 134/1000 | Loss: 0.00002090
Iteration 135/1000 | Loss: 0.00002089
Iteration 136/1000 | Loss: 0.00002089
Iteration 137/1000 | Loss: 0.00002088
Iteration 138/1000 | Loss: 0.00002088
Iteration 139/1000 | Loss: 0.00002088
Iteration 140/1000 | Loss: 0.00002087
Iteration 141/1000 | Loss: 0.00002087
Iteration 142/1000 | Loss: 0.00002087
Iteration 143/1000 | Loss: 0.00002087
Iteration 144/1000 | Loss: 0.00002087
Iteration 145/1000 | Loss: 0.00002087
Iteration 146/1000 | Loss: 0.00002087
Iteration 147/1000 | Loss: 0.00002087
Iteration 148/1000 | Loss: 0.00002087
Iteration 149/1000 | Loss: 0.00002087
Iteration 150/1000 | Loss: 0.00002087
Iteration 151/1000 | Loss: 0.00002087
Iteration 152/1000 | Loss: 0.00002086
Iteration 153/1000 | Loss: 0.00002086
Iteration 154/1000 | Loss: 0.00002086
Iteration 155/1000 | Loss: 0.00002086
Iteration 156/1000 | Loss: 0.00002086
Iteration 157/1000 | Loss: 0.00002086
Iteration 158/1000 | Loss: 0.00002085
Iteration 159/1000 | Loss: 0.00002085
Iteration 160/1000 | Loss: 0.00002085
Iteration 161/1000 | Loss: 0.00002085
Iteration 162/1000 | Loss: 0.00002085
Iteration 163/1000 | Loss: 0.00002085
Iteration 164/1000 | Loss: 0.00002085
Iteration 165/1000 | Loss: 0.00002084
Iteration 166/1000 | Loss: 0.00002084
Iteration 167/1000 | Loss: 0.00002084
Iteration 168/1000 | Loss: 0.00002084
Iteration 169/1000 | Loss: 0.00002084
Iteration 170/1000 | Loss: 0.00002084
Iteration 171/1000 | Loss: 0.00002084
Iteration 172/1000 | Loss: 0.00002083
Iteration 173/1000 | Loss: 0.00002083
Iteration 174/1000 | Loss: 0.00002083
Iteration 175/1000 | Loss: 0.00002083
Iteration 176/1000 | Loss: 0.00002083
Iteration 177/1000 | Loss: 0.00002083
Iteration 178/1000 | Loss: 0.00002083
Iteration 179/1000 | Loss: 0.00002082
Iteration 180/1000 | Loss: 0.00002082
Iteration 181/1000 | Loss: 0.00002081
Iteration 182/1000 | Loss: 0.00002081
Iteration 183/1000 | Loss: 0.00002081
Iteration 184/1000 | Loss: 0.00002081
Iteration 185/1000 | Loss: 0.00002080
Iteration 186/1000 | Loss: 0.00002080
Iteration 187/1000 | Loss: 0.00002079
Iteration 188/1000 | Loss: 0.00002079
Iteration 189/1000 | Loss: 0.00002079
Iteration 190/1000 | Loss: 0.00002079
Iteration 191/1000 | Loss: 0.00002079
Iteration 192/1000 | Loss: 0.00002079
Iteration 193/1000 | Loss: 0.00002078
Iteration 194/1000 | Loss: 0.00002078
Iteration 195/1000 | Loss: 0.00002078
Iteration 196/1000 | Loss: 0.00002078
Iteration 197/1000 | Loss: 0.00002078
Iteration 198/1000 | Loss: 0.00002078
Iteration 199/1000 | Loss: 0.00002078
Iteration 200/1000 | Loss: 0.00002078
Iteration 201/1000 | Loss: 0.00002078
Iteration 202/1000 | Loss: 0.00002078
Iteration 203/1000 | Loss: 0.00002078
Iteration 204/1000 | Loss: 0.00002077
Iteration 205/1000 | Loss: 0.00002077
Iteration 206/1000 | Loss: 0.00002077
Iteration 207/1000 | Loss: 0.00002077
Iteration 208/1000 | Loss: 0.00002077
Iteration 209/1000 | Loss: 0.00002077
Iteration 210/1000 | Loss: 0.00002076
Iteration 211/1000 | Loss: 0.00002076
Iteration 212/1000 | Loss: 0.00002076
Iteration 213/1000 | Loss: 0.00002076
Iteration 214/1000 | Loss: 0.00002076
Iteration 215/1000 | Loss: 0.00002076
Iteration 216/1000 | Loss: 0.00002076
Iteration 217/1000 | Loss: 0.00002076
Iteration 218/1000 | Loss: 0.00002076
Iteration 219/1000 | Loss: 0.00002076
Iteration 220/1000 | Loss: 0.00002076
Iteration 221/1000 | Loss: 0.00002076
Iteration 222/1000 | Loss: 0.00002076
Iteration 223/1000 | Loss: 0.00002076
Iteration 224/1000 | Loss: 0.00002076
Iteration 225/1000 | Loss: 0.00002076
Iteration 226/1000 | Loss: 0.00002076
Iteration 227/1000 | Loss: 0.00002076
Iteration 228/1000 | Loss: 0.00002076
Iteration 229/1000 | Loss: 0.00002076
Iteration 230/1000 | Loss: 0.00002076
Iteration 231/1000 | Loss: 0.00002076
Iteration 232/1000 | Loss: 0.00002076
Iteration 233/1000 | Loss: 0.00002076
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.0759944163728505e-05, 2.0759944163728505e-05, 2.0759944163728505e-05, 2.0759944163728505e-05, 2.0759944163728505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0759944163728505e-05

Optimization complete. Final v2v error: 3.8410849571228027 mm

Highest mean error: 4.473325729370117 mm for frame 138

Lowest mean error: 3.7024056911468506 mm for frame 114

Saving results

Total time: 146.6971893310547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051062
Iteration 2/25 | Loss: 0.01051062
Iteration 3/25 | Loss: 0.00150918
Iteration 4/25 | Loss: 0.00122012
Iteration 5/25 | Loss: 0.00117387
Iteration 6/25 | Loss: 0.00114698
Iteration 7/25 | Loss: 0.00114396
Iteration 8/25 | Loss: 0.00110657
Iteration 9/25 | Loss: 0.00110298
Iteration 10/25 | Loss: 0.00110584
Iteration 11/25 | Loss: 0.00109994
Iteration 12/25 | Loss: 0.00109559
Iteration 13/25 | Loss: 0.00109334
Iteration 14/25 | Loss: 0.00109160
Iteration 15/25 | Loss: 0.00109154
Iteration 16/25 | Loss: 0.00109153
Iteration 17/25 | Loss: 0.00109153
Iteration 18/25 | Loss: 0.00109343
Iteration 19/25 | Loss: 0.00109342
Iteration 20/25 | Loss: 0.00109602
Iteration 21/25 | Loss: 0.00109312
Iteration 22/25 | Loss: 0.00109174
Iteration 23/25 | Loss: 0.00108936
Iteration 24/25 | Loss: 0.00108623
Iteration 25/25 | Loss: 0.00108629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28084147
Iteration 2/25 | Loss: 0.00117786
Iteration 3/25 | Loss: 0.00106068
Iteration 4/25 | Loss: 0.00106068
Iteration 5/25 | Loss: 0.00106068
Iteration 6/25 | Loss: 0.00106068
Iteration 7/25 | Loss: 0.00106068
Iteration 8/25 | Loss: 0.00106068
Iteration 9/25 | Loss: 0.00106068
Iteration 10/25 | Loss: 0.00106068
Iteration 11/25 | Loss: 0.00106068
Iteration 12/25 | Loss: 0.00106068
Iteration 13/25 | Loss: 0.00106068
Iteration 14/25 | Loss: 0.00106068
Iteration 15/25 | Loss: 0.00106068
Iteration 16/25 | Loss: 0.00106067
Iteration 17/25 | Loss: 0.00106067
Iteration 18/25 | Loss: 0.00106068
Iteration 19/25 | Loss: 0.00106067
Iteration 20/25 | Loss: 0.00106068
Iteration 21/25 | Loss: 0.00106068
Iteration 22/25 | Loss: 0.00106067
Iteration 23/25 | Loss: 0.00106068
Iteration 24/25 | Loss: 0.00106068
Iteration 25/25 | Loss: 0.00106067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106067
Iteration 2/1000 | Loss: 0.00016960
Iteration 3/1000 | Loss: 0.00009500
Iteration 4/1000 | Loss: 0.00002492
Iteration 5/1000 | Loss: 0.00001527
Iteration 6/1000 | Loss: 0.00001906
Iteration 7/1000 | Loss: 0.00001959
Iteration 8/1000 | Loss: 0.00001338
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001264
Iteration 11/1000 | Loss: 0.00001576
Iteration 12/1000 | Loss: 0.00001416
Iteration 13/1000 | Loss: 0.00002240
Iteration 14/1000 | Loss: 0.00001214
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001213
Iteration 17/1000 | Loss: 0.00001212
Iteration 18/1000 | Loss: 0.00001208
Iteration 19/1000 | Loss: 0.00001208
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001208
Iteration 22/1000 | Loss: 0.00001207
Iteration 23/1000 | Loss: 0.00001207
Iteration 24/1000 | Loss: 0.00001207
Iteration 25/1000 | Loss: 0.00001207
Iteration 26/1000 | Loss: 0.00001207
Iteration 27/1000 | Loss: 0.00001207
Iteration 28/1000 | Loss: 0.00001205
Iteration 29/1000 | Loss: 0.00001194
Iteration 30/1000 | Loss: 0.00001194
Iteration 31/1000 | Loss: 0.00001194
Iteration 32/1000 | Loss: 0.00001193
Iteration 33/1000 | Loss: 0.00001193
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001187
Iteration 37/1000 | Loss: 0.00001187
Iteration 38/1000 | Loss: 0.00001184
Iteration 39/1000 | Loss: 0.00001184
Iteration 40/1000 | Loss: 0.00001183
Iteration 41/1000 | Loss: 0.00001183
Iteration 42/1000 | Loss: 0.00001183
Iteration 43/1000 | Loss: 0.00001182
Iteration 44/1000 | Loss: 0.00001182
Iteration 45/1000 | Loss: 0.00001181
Iteration 46/1000 | Loss: 0.00001181
Iteration 47/1000 | Loss: 0.00001180
Iteration 48/1000 | Loss: 0.00001180
Iteration 49/1000 | Loss: 0.00001179
Iteration 50/1000 | Loss: 0.00001179
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001178
Iteration 53/1000 | Loss: 0.00001177
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001177
Iteration 56/1000 | Loss: 0.00001176
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001175
Iteration 59/1000 | Loss: 0.00001174
Iteration 60/1000 | Loss: 0.00001174
Iteration 61/1000 | Loss: 0.00001174
Iteration 62/1000 | Loss: 0.00001173
Iteration 63/1000 | Loss: 0.00001173
Iteration 64/1000 | Loss: 0.00001173
Iteration 65/1000 | Loss: 0.00001173
Iteration 66/1000 | Loss: 0.00001173
Iteration 67/1000 | Loss: 0.00001172
Iteration 68/1000 | Loss: 0.00001172
Iteration 69/1000 | Loss: 0.00001172
Iteration 70/1000 | Loss: 0.00001172
Iteration 71/1000 | Loss: 0.00001172
Iteration 72/1000 | Loss: 0.00001172
Iteration 73/1000 | Loss: 0.00001172
Iteration 74/1000 | Loss: 0.00001172
Iteration 75/1000 | Loss: 0.00001171
Iteration 76/1000 | Loss: 0.00001171
Iteration 77/1000 | Loss: 0.00001171
Iteration 78/1000 | Loss: 0.00001171
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001171
Iteration 82/1000 | Loss: 0.00001171
Iteration 83/1000 | Loss: 0.00001171
Iteration 84/1000 | Loss: 0.00001171
Iteration 85/1000 | Loss: 0.00001171
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001171
Iteration 98/1000 | Loss: 0.00001171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.1712139894370921e-05, 1.1712139894370921e-05, 1.1712139894370921e-05, 1.1712139894370921e-05, 1.1712139894370921e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1712139894370921e-05

Optimization complete. Final v2v error: 2.92033052444458 mm

Highest mean error: 4.6305131912231445 mm for frame 147

Lowest mean error: 2.7635819911956787 mm for frame 132

Saving results

Total time: 67.52414536476135
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042161
Iteration 2/25 | Loss: 0.00137904
Iteration 3/25 | Loss: 0.00119469
Iteration 4/25 | Loss: 0.00117733
Iteration 5/25 | Loss: 0.00116845
Iteration 6/25 | Loss: 0.00117367
Iteration 7/25 | Loss: 0.00116742
Iteration 8/25 | Loss: 0.00116531
Iteration 9/25 | Loss: 0.00116990
Iteration 10/25 | Loss: 0.00116367
Iteration 11/25 | Loss: 0.00116334
Iteration 12/25 | Loss: 0.00115777
Iteration 13/25 | Loss: 0.00115438
Iteration 14/25 | Loss: 0.00115558
Iteration 15/25 | Loss: 0.00115626
Iteration 16/25 | Loss: 0.00115509
Iteration 17/25 | Loss: 0.00115358
Iteration 18/25 | Loss: 0.00116031
Iteration 19/25 | Loss: 0.00115528
Iteration 20/25 | Loss: 0.00115425
Iteration 21/25 | Loss: 0.00115453
Iteration 22/25 | Loss: 0.00115417
Iteration 23/25 | Loss: 0.00115293
Iteration 24/25 | Loss: 0.00115416
Iteration 25/25 | Loss: 0.00115279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.28894615
Iteration 2/25 | Loss: 0.00086882
Iteration 3/25 | Loss: 0.00086882
Iteration 4/25 | Loss: 0.00086882
Iteration 5/25 | Loss: 0.00086882
Iteration 6/25 | Loss: 0.00086882
Iteration 7/25 | Loss: 0.00086882
Iteration 8/25 | Loss: 0.00086882
Iteration 9/25 | Loss: 0.00086882
Iteration 10/25 | Loss: 0.00086882
Iteration 11/25 | Loss: 0.00086882
Iteration 12/25 | Loss: 0.00086882
Iteration 13/25 | Loss: 0.00086882
Iteration 14/25 | Loss: 0.00086882
Iteration 15/25 | Loss: 0.00086882
Iteration 16/25 | Loss: 0.00086882
Iteration 17/25 | Loss: 0.00086882
Iteration 18/25 | Loss: 0.00086882
Iteration 19/25 | Loss: 0.00086882
Iteration 20/25 | Loss: 0.00086882
Iteration 21/25 | Loss: 0.00086882
Iteration 22/25 | Loss: 0.00086882
Iteration 23/25 | Loss: 0.00086882
Iteration 24/25 | Loss: 0.00086882
Iteration 25/25 | Loss: 0.00086882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086882
Iteration 2/1000 | Loss: 0.00013047
Iteration 3/1000 | Loss: 0.00010403
Iteration 4/1000 | Loss: 0.00006053
Iteration 5/1000 | Loss: 0.00002667
Iteration 6/1000 | Loss: 0.00002908
Iteration 7/1000 | Loss: 0.00010590
Iteration 8/1000 | Loss: 0.00004007
Iteration 9/1000 | Loss: 0.00021600
Iteration 10/1000 | Loss: 0.00008893
Iteration 11/1000 | Loss: 0.00008974
Iteration 12/1000 | Loss: 0.00009995
Iteration 13/1000 | Loss: 0.00002807
Iteration 14/1000 | Loss: 0.00002867
Iteration 15/1000 | Loss: 0.00003180
Iteration 16/1000 | Loss: 0.00008532
Iteration 17/1000 | Loss: 0.00011131
Iteration 18/1000 | Loss: 0.00016326
Iteration 19/1000 | Loss: 0.00007763
Iteration 20/1000 | Loss: 0.00007110
Iteration 21/1000 | Loss: 0.00013453
Iteration 22/1000 | Loss: 0.00024631
Iteration 23/1000 | Loss: 0.00013459
Iteration 24/1000 | Loss: 0.00019753
Iteration 25/1000 | Loss: 0.00016982
Iteration 26/1000 | Loss: 0.00019289
Iteration 27/1000 | Loss: 0.00015598
Iteration 28/1000 | Loss: 0.00019151
Iteration 29/1000 | Loss: 0.00053412
Iteration 30/1000 | Loss: 0.00067629
Iteration 31/1000 | Loss: 0.00016401
Iteration 32/1000 | Loss: 0.00007030
Iteration 33/1000 | Loss: 0.00005382
Iteration 34/1000 | Loss: 0.00005575
Iteration 35/1000 | Loss: 0.00004609
Iteration 36/1000 | Loss: 0.00005938
Iteration 37/1000 | Loss: 0.00003851
Iteration 38/1000 | Loss: 0.00004832
Iteration 39/1000 | Loss: 0.00002179
Iteration 40/1000 | Loss: 0.00007636
Iteration 41/1000 | Loss: 0.00003471
Iteration 42/1000 | Loss: 0.00010605
Iteration 43/1000 | Loss: 0.00018750
Iteration 44/1000 | Loss: 0.00010850
Iteration 45/1000 | Loss: 0.00013312
Iteration 46/1000 | Loss: 0.00005997
Iteration 47/1000 | Loss: 0.00014535
Iteration 48/1000 | Loss: 0.00014420
Iteration 49/1000 | Loss: 0.00016657
Iteration 50/1000 | Loss: 0.00010749
Iteration 51/1000 | Loss: 0.00017798
Iteration 52/1000 | Loss: 0.00002814
Iteration 53/1000 | Loss: 0.00002049
Iteration 54/1000 | Loss: 0.00002328
Iteration 55/1000 | Loss: 0.00002112
Iteration 56/1000 | Loss: 0.00002629
Iteration 57/1000 | Loss: 0.00002704
Iteration 58/1000 | Loss: 0.00002351
Iteration 59/1000 | Loss: 0.00002500
Iteration 60/1000 | Loss: 0.00003011
Iteration 61/1000 | Loss: 0.00001902
Iteration 62/1000 | Loss: 0.00001594
Iteration 63/1000 | Loss: 0.00002227
Iteration 64/1000 | Loss: 0.00002099
Iteration 65/1000 | Loss: 0.00002106
Iteration 66/1000 | Loss: 0.00002048
Iteration 67/1000 | Loss: 0.00002054
Iteration 68/1000 | Loss: 0.00002437
Iteration 69/1000 | Loss: 0.00002351
Iteration 70/1000 | Loss: 0.00002528
Iteration 71/1000 | Loss: 0.00002421
Iteration 72/1000 | Loss: 0.00002394
Iteration 73/1000 | Loss: 0.00002349
Iteration 74/1000 | Loss: 0.00002012
Iteration 75/1000 | Loss: 0.00002255
Iteration 76/1000 | Loss: 0.00001660
Iteration 77/1000 | Loss: 0.00001537
Iteration 78/1000 | Loss: 0.00001612
Iteration 79/1000 | Loss: 0.00002169
Iteration 80/1000 | Loss: 0.00002904
Iteration 81/1000 | Loss: 0.00002573
Iteration 82/1000 | Loss: 0.00002375
Iteration 83/1000 | Loss: 0.00001851
Iteration 84/1000 | Loss: 0.00002404
Iteration 85/1000 | Loss: 0.00002634
Iteration 86/1000 | Loss: 0.00002415
Iteration 87/1000 | Loss: 0.00002593
Iteration 88/1000 | Loss: 0.00002439
Iteration 89/1000 | Loss: 0.00002541
Iteration 90/1000 | Loss: 0.00002213
Iteration 91/1000 | Loss: 0.00001362
Iteration 92/1000 | Loss: 0.00001255
Iteration 93/1000 | Loss: 0.00001206
Iteration 94/1000 | Loss: 0.00001167
Iteration 95/1000 | Loss: 0.00001140
Iteration 96/1000 | Loss: 0.00003127
Iteration 97/1000 | Loss: 0.00001207
Iteration 98/1000 | Loss: 0.00001145
Iteration 99/1000 | Loss: 0.00001120
Iteration 100/1000 | Loss: 0.00001118
Iteration 101/1000 | Loss: 0.00001115
Iteration 102/1000 | Loss: 0.00001114
Iteration 103/1000 | Loss: 0.00001114
Iteration 104/1000 | Loss: 0.00001113
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001110
Iteration 107/1000 | Loss: 0.00001110
Iteration 108/1000 | Loss: 0.00001110
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001107
Iteration 112/1000 | Loss: 0.00001107
Iteration 113/1000 | Loss: 0.00001107
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001106
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001103
Iteration 118/1000 | Loss: 0.00001102
Iteration 119/1000 | Loss: 0.00001102
Iteration 120/1000 | Loss: 0.00001102
Iteration 121/1000 | Loss: 0.00001102
Iteration 122/1000 | Loss: 0.00001101
Iteration 123/1000 | Loss: 0.00001101
Iteration 124/1000 | Loss: 0.00001101
Iteration 125/1000 | Loss: 0.00001100
Iteration 126/1000 | Loss: 0.00001100
Iteration 127/1000 | Loss: 0.00001100
Iteration 128/1000 | Loss: 0.00001100
Iteration 129/1000 | Loss: 0.00001100
Iteration 130/1000 | Loss: 0.00001099
Iteration 131/1000 | Loss: 0.00001099
Iteration 132/1000 | Loss: 0.00001098
Iteration 133/1000 | Loss: 0.00001098
Iteration 134/1000 | Loss: 0.00001098
Iteration 135/1000 | Loss: 0.00001097
Iteration 136/1000 | Loss: 0.00001097
Iteration 137/1000 | Loss: 0.00001096
Iteration 138/1000 | Loss: 0.00001096
Iteration 139/1000 | Loss: 0.00001096
Iteration 140/1000 | Loss: 0.00001096
Iteration 141/1000 | Loss: 0.00001095
Iteration 142/1000 | Loss: 0.00001095
Iteration 143/1000 | Loss: 0.00001095
Iteration 144/1000 | Loss: 0.00001095
Iteration 145/1000 | Loss: 0.00001095
Iteration 146/1000 | Loss: 0.00001094
Iteration 147/1000 | Loss: 0.00001094
Iteration 148/1000 | Loss: 0.00001094
Iteration 149/1000 | Loss: 0.00001094
Iteration 150/1000 | Loss: 0.00001094
Iteration 151/1000 | Loss: 0.00001094
Iteration 152/1000 | Loss: 0.00001093
Iteration 153/1000 | Loss: 0.00001093
Iteration 154/1000 | Loss: 0.00001093
Iteration 155/1000 | Loss: 0.00001093
Iteration 156/1000 | Loss: 0.00001093
Iteration 157/1000 | Loss: 0.00001093
Iteration 158/1000 | Loss: 0.00001093
Iteration 159/1000 | Loss: 0.00001093
Iteration 160/1000 | Loss: 0.00001093
Iteration 161/1000 | Loss: 0.00001093
Iteration 162/1000 | Loss: 0.00001092
Iteration 163/1000 | Loss: 0.00001092
Iteration 164/1000 | Loss: 0.00001092
Iteration 165/1000 | Loss: 0.00001092
Iteration 166/1000 | Loss: 0.00001091
Iteration 167/1000 | Loss: 0.00001091
Iteration 168/1000 | Loss: 0.00001090
Iteration 169/1000 | Loss: 0.00001090
Iteration 170/1000 | Loss: 0.00001089
Iteration 171/1000 | Loss: 0.00001089
Iteration 172/1000 | Loss: 0.00001088
Iteration 173/1000 | Loss: 0.00001088
Iteration 174/1000 | Loss: 0.00001088
Iteration 175/1000 | Loss: 0.00001088
Iteration 176/1000 | Loss: 0.00001087
Iteration 177/1000 | Loss: 0.00001087
Iteration 178/1000 | Loss: 0.00001087
Iteration 179/1000 | Loss: 0.00001086
Iteration 180/1000 | Loss: 0.00001086
Iteration 181/1000 | Loss: 0.00001085
Iteration 182/1000 | Loss: 0.00001085
Iteration 183/1000 | Loss: 0.00001085
Iteration 184/1000 | Loss: 0.00001085
Iteration 185/1000 | Loss: 0.00001085
Iteration 186/1000 | Loss: 0.00001083
Iteration 187/1000 | Loss: 0.00001083
Iteration 188/1000 | Loss: 0.00001083
Iteration 189/1000 | Loss: 0.00001083
Iteration 190/1000 | Loss: 0.00001083
Iteration 191/1000 | Loss: 0.00001083
Iteration 192/1000 | Loss: 0.00001083
Iteration 193/1000 | Loss: 0.00001083
Iteration 194/1000 | Loss: 0.00001083
Iteration 195/1000 | Loss: 0.00001083
Iteration 196/1000 | Loss: 0.00001083
Iteration 197/1000 | Loss: 0.00001083
Iteration 198/1000 | Loss: 0.00001083
Iteration 199/1000 | Loss: 0.00001083
Iteration 200/1000 | Loss: 0.00001083
Iteration 201/1000 | Loss: 0.00001083
Iteration 202/1000 | Loss: 0.00001083
Iteration 203/1000 | Loss: 0.00001083
Iteration 204/1000 | Loss: 0.00001083
Iteration 205/1000 | Loss: 0.00001083
Iteration 206/1000 | Loss: 0.00001083
Iteration 207/1000 | Loss: 0.00001083
Iteration 208/1000 | Loss: 0.00001083
Iteration 209/1000 | Loss: 0.00001083
Iteration 210/1000 | Loss: 0.00001083
Iteration 211/1000 | Loss: 0.00001083
Iteration 212/1000 | Loss: 0.00001083
Iteration 213/1000 | Loss: 0.00001083
Iteration 214/1000 | Loss: 0.00001083
Iteration 215/1000 | Loss: 0.00001083
Iteration 216/1000 | Loss: 0.00001083
Iteration 217/1000 | Loss: 0.00001083
Iteration 218/1000 | Loss: 0.00001083
Iteration 219/1000 | Loss: 0.00001083
Iteration 220/1000 | Loss: 0.00001083
Iteration 221/1000 | Loss: 0.00001083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.0827189726114739e-05, 1.0827189726114739e-05, 1.0827189726114739e-05, 1.0827189726114739e-05, 1.0827189726114739e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0827189726114739e-05

Optimization complete. Final v2v error: 2.7656705379486084 mm

Highest mean error: 6.310921669006348 mm for frame 6

Lowest mean error: 2.5765268802642822 mm for frame 254

Saving results

Total time: 229.01283979415894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402056
Iteration 2/25 | Loss: 0.00126931
Iteration 3/25 | Loss: 0.00115135
Iteration 4/25 | Loss: 0.00113772
Iteration 5/25 | Loss: 0.00113543
Iteration 6/25 | Loss: 0.00113508
Iteration 7/25 | Loss: 0.00113508
Iteration 8/25 | Loss: 0.00113508
Iteration 9/25 | Loss: 0.00113508
Iteration 10/25 | Loss: 0.00113508
Iteration 11/25 | Loss: 0.00113508
Iteration 12/25 | Loss: 0.00113508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011350827990099788, 0.0011350827990099788, 0.0011350827990099788, 0.0011350827990099788, 0.0011350827990099788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011350827990099788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35565233
Iteration 2/25 | Loss: 0.00066258
Iteration 3/25 | Loss: 0.00066257
Iteration 4/25 | Loss: 0.00066257
Iteration 5/25 | Loss: 0.00066257
Iteration 6/25 | Loss: 0.00066257
Iteration 7/25 | Loss: 0.00066257
Iteration 8/25 | Loss: 0.00066257
Iteration 9/25 | Loss: 0.00066257
Iteration 10/25 | Loss: 0.00066257
Iteration 11/25 | Loss: 0.00066257
Iteration 12/25 | Loss: 0.00066257
Iteration 13/25 | Loss: 0.00066257
Iteration 14/25 | Loss: 0.00066257
Iteration 15/25 | Loss: 0.00066257
Iteration 16/25 | Loss: 0.00066257
Iteration 17/25 | Loss: 0.00066257
Iteration 18/25 | Loss: 0.00066257
Iteration 19/25 | Loss: 0.00066257
Iteration 20/25 | Loss: 0.00066257
Iteration 21/25 | Loss: 0.00066257
Iteration 22/25 | Loss: 0.00066257
Iteration 23/25 | Loss: 0.00066257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006625704700127244, 0.0006625704700127244, 0.0006625704700127244, 0.0006625704700127244, 0.0006625704700127244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006625704700127244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066257
Iteration 2/1000 | Loss: 0.00002316
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001570
Iteration 5/1000 | Loss: 0.00001470
Iteration 6/1000 | Loss: 0.00001400
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001331
Iteration 9/1000 | Loss: 0.00001300
Iteration 10/1000 | Loss: 0.00001284
Iteration 11/1000 | Loss: 0.00001278
Iteration 12/1000 | Loss: 0.00001278
Iteration 13/1000 | Loss: 0.00001277
Iteration 14/1000 | Loss: 0.00001274
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00001271
Iteration 17/1000 | Loss: 0.00001263
Iteration 18/1000 | Loss: 0.00001254
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001251
Iteration 23/1000 | Loss: 0.00001250
Iteration 24/1000 | Loss: 0.00001249
Iteration 25/1000 | Loss: 0.00001248
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001244
Iteration 28/1000 | Loss: 0.00001243
Iteration 29/1000 | Loss: 0.00001243
Iteration 30/1000 | Loss: 0.00001243
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001238
Iteration 43/1000 | Loss: 0.00001237
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001236
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001213
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001209
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001209
Iteration 71/1000 | Loss: 0.00001209
Iteration 72/1000 | Loss: 0.00001209
Iteration 73/1000 | Loss: 0.00001208
Iteration 74/1000 | Loss: 0.00001208
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001207
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001206
Iteration 79/1000 | Loss: 0.00001205
Iteration 80/1000 | Loss: 0.00001205
Iteration 81/1000 | Loss: 0.00001205
Iteration 82/1000 | Loss: 0.00001205
Iteration 83/1000 | Loss: 0.00001205
Iteration 84/1000 | Loss: 0.00001205
Iteration 85/1000 | Loss: 0.00001205
Iteration 86/1000 | Loss: 0.00001204
Iteration 87/1000 | Loss: 0.00001204
Iteration 88/1000 | Loss: 0.00001204
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001202
Iteration 94/1000 | Loss: 0.00001202
Iteration 95/1000 | Loss: 0.00001202
Iteration 96/1000 | Loss: 0.00001202
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001201
Iteration 99/1000 | Loss: 0.00001201
Iteration 100/1000 | Loss: 0.00001201
Iteration 101/1000 | Loss: 0.00001201
Iteration 102/1000 | Loss: 0.00001201
Iteration 103/1000 | Loss: 0.00001201
Iteration 104/1000 | Loss: 0.00001201
Iteration 105/1000 | Loss: 0.00001201
Iteration 106/1000 | Loss: 0.00001201
Iteration 107/1000 | Loss: 0.00001201
Iteration 108/1000 | Loss: 0.00001201
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001198
Iteration 118/1000 | Loss: 0.00001198
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001197
Iteration 124/1000 | Loss: 0.00001197
Iteration 125/1000 | Loss: 0.00001197
Iteration 126/1000 | Loss: 0.00001197
Iteration 127/1000 | Loss: 0.00001197
Iteration 128/1000 | Loss: 0.00001197
Iteration 129/1000 | Loss: 0.00001196
Iteration 130/1000 | Loss: 0.00001196
Iteration 131/1000 | Loss: 0.00001196
Iteration 132/1000 | Loss: 0.00001196
Iteration 133/1000 | Loss: 0.00001196
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001195
Iteration 136/1000 | Loss: 0.00001195
Iteration 137/1000 | Loss: 0.00001195
Iteration 138/1000 | Loss: 0.00001195
Iteration 139/1000 | Loss: 0.00001195
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001195
Iteration 142/1000 | Loss: 0.00001195
Iteration 143/1000 | Loss: 0.00001195
Iteration 144/1000 | Loss: 0.00001195
Iteration 145/1000 | Loss: 0.00001195
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001194
Iteration 148/1000 | Loss: 0.00001194
Iteration 149/1000 | Loss: 0.00001194
Iteration 150/1000 | Loss: 0.00001194
Iteration 151/1000 | Loss: 0.00001193
Iteration 152/1000 | Loss: 0.00001193
Iteration 153/1000 | Loss: 0.00001193
Iteration 154/1000 | Loss: 0.00001193
Iteration 155/1000 | Loss: 0.00001193
Iteration 156/1000 | Loss: 0.00001193
Iteration 157/1000 | Loss: 0.00001193
Iteration 158/1000 | Loss: 0.00001193
Iteration 159/1000 | Loss: 0.00001193
Iteration 160/1000 | Loss: 0.00001193
Iteration 161/1000 | Loss: 0.00001192
Iteration 162/1000 | Loss: 0.00001192
Iteration 163/1000 | Loss: 0.00001192
Iteration 164/1000 | Loss: 0.00001192
Iteration 165/1000 | Loss: 0.00001192
Iteration 166/1000 | Loss: 0.00001192
Iteration 167/1000 | Loss: 0.00001192
Iteration 168/1000 | Loss: 0.00001191
Iteration 169/1000 | Loss: 0.00001191
Iteration 170/1000 | Loss: 0.00001191
Iteration 171/1000 | Loss: 0.00001191
Iteration 172/1000 | Loss: 0.00001191
Iteration 173/1000 | Loss: 0.00001191
Iteration 174/1000 | Loss: 0.00001191
Iteration 175/1000 | Loss: 0.00001191
Iteration 176/1000 | Loss: 0.00001191
Iteration 177/1000 | Loss: 0.00001191
Iteration 178/1000 | Loss: 0.00001191
Iteration 179/1000 | Loss: 0.00001191
Iteration 180/1000 | Loss: 0.00001191
Iteration 181/1000 | Loss: 0.00001190
Iteration 182/1000 | Loss: 0.00001190
Iteration 183/1000 | Loss: 0.00001190
Iteration 184/1000 | Loss: 0.00001190
Iteration 185/1000 | Loss: 0.00001190
Iteration 186/1000 | Loss: 0.00001190
Iteration 187/1000 | Loss: 0.00001190
Iteration 188/1000 | Loss: 0.00001190
Iteration 189/1000 | Loss: 0.00001190
Iteration 190/1000 | Loss: 0.00001190
Iteration 191/1000 | Loss: 0.00001190
Iteration 192/1000 | Loss: 0.00001190
Iteration 193/1000 | Loss: 0.00001190
Iteration 194/1000 | Loss: 0.00001190
Iteration 195/1000 | Loss: 0.00001190
Iteration 196/1000 | Loss: 0.00001189
Iteration 197/1000 | Loss: 0.00001189
Iteration 198/1000 | Loss: 0.00001189
Iteration 199/1000 | Loss: 0.00001189
Iteration 200/1000 | Loss: 0.00001189
Iteration 201/1000 | Loss: 0.00001189
Iteration 202/1000 | Loss: 0.00001189
Iteration 203/1000 | Loss: 0.00001189
Iteration 204/1000 | Loss: 0.00001189
Iteration 205/1000 | Loss: 0.00001189
Iteration 206/1000 | Loss: 0.00001189
Iteration 207/1000 | Loss: 0.00001189
Iteration 208/1000 | Loss: 0.00001189
Iteration 209/1000 | Loss: 0.00001189
Iteration 210/1000 | Loss: 0.00001189
Iteration 211/1000 | Loss: 0.00001189
Iteration 212/1000 | Loss: 0.00001189
Iteration 213/1000 | Loss: 0.00001189
Iteration 214/1000 | Loss: 0.00001189
Iteration 215/1000 | Loss: 0.00001189
Iteration 216/1000 | Loss: 0.00001189
Iteration 217/1000 | Loss: 0.00001188
Iteration 218/1000 | Loss: 0.00001188
Iteration 219/1000 | Loss: 0.00001188
Iteration 220/1000 | Loss: 0.00001188
Iteration 221/1000 | Loss: 0.00001188
Iteration 222/1000 | Loss: 0.00001188
Iteration 223/1000 | Loss: 0.00001188
Iteration 224/1000 | Loss: 0.00001188
Iteration 225/1000 | Loss: 0.00001188
Iteration 226/1000 | Loss: 0.00001188
Iteration 227/1000 | Loss: 0.00001188
Iteration 228/1000 | Loss: 0.00001188
Iteration 229/1000 | Loss: 0.00001188
Iteration 230/1000 | Loss: 0.00001188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.1880719284818042e-05, 1.1880719284818042e-05, 1.1880719284818042e-05, 1.1880719284818042e-05, 1.1880719284818042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1880719284818042e-05

Optimization complete. Final v2v error: 2.942182779312134 mm

Highest mean error: 3.1970081329345703 mm for frame 115

Lowest mean error: 2.6958649158477783 mm for frame 30

Saving results

Total time: 44.93673920631409
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885637
Iteration 2/25 | Loss: 0.00131981
Iteration 3/25 | Loss: 0.00121014
Iteration 4/25 | Loss: 0.00119109
Iteration 5/25 | Loss: 0.00118447
Iteration 6/25 | Loss: 0.00118281
Iteration 7/25 | Loss: 0.00118281
Iteration 8/25 | Loss: 0.00118281
Iteration 9/25 | Loss: 0.00118281
Iteration 10/25 | Loss: 0.00118281
Iteration 11/25 | Loss: 0.00118281
Iteration 12/25 | Loss: 0.00118281
Iteration 13/25 | Loss: 0.00118281
Iteration 14/25 | Loss: 0.00118281
Iteration 15/25 | Loss: 0.00118281
Iteration 16/25 | Loss: 0.00118281
Iteration 17/25 | Loss: 0.00118281
Iteration 18/25 | Loss: 0.00118281
Iteration 19/25 | Loss: 0.00118281
Iteration 20/25 | Loss: 0.00118281
Iteration 21/25 | Loss: 0.00118281
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011828087735921144, 0.0011828087735921144, 0.0011828087735921144, 0.0011828087735921144, 0.0011828087735921144]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011828087735921144

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37897956
Iteration 2/25 | Loss: 0.00084164
Iteration 3/25 | Loss: 0.00084163
Iteration 4/25 | Loss: 0.00084163
Iteration 5/25 | Loss: 0.00084163
Iteration 6/25 | Loss: 0.00084163
Iteration 7/25 | Loss: 0.00084163
Iteration 8/25 | Loss: 0.00084162
Iteration 9/25 | Loss: 0.00084162
Iteration 10/25 | Loss: 0.00084162
Iteration 11/25 | Loss: 0.00084162
Iteration 12/25 | Loss: 0.00084162
Iteration 13/25 | Loss: 0.00084162
Iteration 14/25 | Loss: 0.00084162
Iteration 15/25 | Loss: 0.00084162
Iteration 16/25 | Loss: 0.00084162
Iteration 17/25 | Loss: 0.00084162
Iteration 18/25 | Loss: 0.00084162
Iteration 19/25 | Loss: 0.00084162
Iteration 20/25 | Loss: 0.00084162
Iteration 21/25 | Loss: 0.00084162
Iteration 22/25 | Loss: 0.00084162
Iteration 23/25 | Loss: 0.00084162
Iteration 24/25 | Loss: 0.00084162
Iteration 25/25 | Loss: 0.00084162

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084162
Iteration 2/1000 | Loss: 0.00004376
Iteration 3/1000 | Loss: 0.00002824
Iteration 4/1000 | Loss: 0.00002362
Iteration 5/1000 | Loss: 0.00002235
Iteration 6/1000 | Loss: 0.00002144
Iteration 7/1000 | Loss: 0.00002076
Iteration 8/1000 | Loss: 0.00002031
Iteration 9/1000 | Loss: 0.00001978
Iteration 10/1000 | Loss: 0.00001949
Iteration 11/1000 | Loss: 0.00001922
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001900
Iteration 14/1000 | Loss: 0.00001898
Iteration 15/1000 | Loss: 0.00001896
Iteration 16/1000 | Loss: 0.00001884
Iteration 17/1000 | Loss: 0.00001879
Iteration 18/1000 | Loss: 0.00001876
Iteration 19/1000 | Loss: 0.00001875
Iteration 20/1000 | Loss: 0.00001873
Iteration 21/1000 | Loss: 0.00001873
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001870
Iteration 24/1000 | Loss: 0.00001870
Iteration 25/1000 | Loss: 0.00001869
Iteration 26/1000 | Loss: 0.00001868
Iteration 27/1000 | Loss: 0.00001862
Iteration 28/1000 | Loss: 0.00001855
Iteration 29/1000 | Loss: 0.00001850
Iteration 30/1000 | Loss: 0.00001850
Iteration 31/1000 | Loss: 0.00001848
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001847
Iteration 34/1000 | Loss: 0.00001847
Iteration 35/1000 | Loss: 0.00001846
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001844
Iteration 38/1000 | Loss: 0.00001843
Iteration 39/1000 | Loss: 0.00001843
Iteration 40/1000 | Loss: 0.00001842
Iteration 41/1000 | Loss: 0.00001842
Iteration 42/1000 | Loss: 0.00001841
Iteration 43/1000 | Loss: 0.00001841
Iteration 44/1000 | Loss: 0.00001841
Iteration 45/1000 | Loss: 0.00001840
Iteration 46/1000 | Loss: 0.00001840
Iteration 47/1000 | Loss: 0.00001839
Iteration 48/1000 | Loss: 0.00001839
Iteration 49/1000 | Loss: 0.00001838
Iteration 50/1000 | Loss: 0.00001838
Iteration 51/1000 | Loss: 0.00001837
Iteration 52/1000 | Loss: 0.00001837
Iteration 53/1000 | Loss: 0.00001836
Iteration 54/1000 | Loss: 0.00001836
Iteration 55/1000 | Loss: 0.00001835
Iteration 56/1000 | Loss: 0.00001835
Iteration 57/1000 | Loss: 0.00001835
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001834
Iteration 60/1000 | Loss: 0.00001834
Iteration 61/1000 | Loss: 0.00001834
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001832
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001830
Iteration 70/1000 | Loss: 0.00001830
Iteration 71/1000 | Loss: 0.00001828
Iteration 72/1000 | Loss: 0.00001828
Iteration 73/1000 | Loss: 0.00001828
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001827
Iteration 76/1000 | Loss: 0.00001826
Iteration 77/1000 | Loss: 0.00001826
Iteration 78/1000 | Loss: 0.00001826
Iteration 79/1000 | Loss: 0.00001826
Iteration 80/1000 | Loss: 0.00001826
Iteration 81/1000 | Loss: 0.00001826
Iteration 82/1000 | Loss: 0.00001825
Iteration 83/1000 | Loss: 0.00001825
Iteration 84/1000 | Loss: 0.00001825
Iteration 85/1000 | Loss: 0.00001825
Iteration 86/1000 | Loss: 0.00001825
Iteration 87/1000 | Loss: 0.00001825
Iteration 88/1000 | Loss: 0.00001825
Iteration 89/1000 | Loss: 0.00001825
Iteration 90/1000 | Loss: 0.00001825
Iteration 91/1000 | Loss: 0.00001825
Iteration 92/1000 | Loss: 0.00001825
Iteration 93/1000 | Loss: 0.00001825
Iteration 94/1000 | Loss: 0.00001824
Iteration 95/1000 | Loss: 0.00001824
Iteration 96/1000 | Loss: 0.00001823
Iteration 97/1000 | Loss: 0.00001823
Iteration 98/1000 | Loss: 0.00001822
Iteration 99/1000 | Loss: 0.00001822
Iteration 100/1000 | Loss: 0.00001822
Iteration 101/1000 | Loss: 0.00001821
Iteration 102/1000 | Loss: 0.00001821
Iteration 103/1000 | Loss: 0.00001821
Iteration 104/1000 | Loss: 0.00001820
Iteration 105/1000 | Loss: 0.00001820
Iteration 106/1000 | Loss: 0.00001820
Iteration 107/1000 | Loss: 0.00001819
Iteration 108/1000 | Loss: 0.00001819
Iteration 109/1000 | Loss: 0.00001819
Iteration 110/1000 | Loss: 0.00001818
Iteration 111/1000 | Loss: 0.00001818
Iteration 112/1000 | Loss: 0.00001818
Iteration 113/1000 | Loss: 0.00001817
Iteration 114/1000 | Loss: 0.00001817
Iteration 115/1000 | Loss: 0.00001817
Iteration 116/1000 | Loss: 0.00001817
Iteration 117/1000 | Loss: 0.00001817
Iteration 118/1000 | Loss: 0.00001817
Iteration 119/1000 | Loss: 0.00001816
Iteration 120/1000 | Loss: 0.00001816
Iteration 121/1000 | Loss: 0.00001816
Iteration 122/1000 | Loss: 0.00001815
Iteration 123/1000 | Loss: 0.00001815
Iteration 124/1000 | Loss: 0.00001815
Iteration 125/1000 | Loss: 0.00001815
Iteration 126/1000 | Loss: 0.00001814
Iteration 127/1000 | Loss: 0.00001814
Iteration 128/1000 | Loss: 0.00001814
Iteration 129/1000 | Loss: 0.00001814
Iteration 130/1000 | Loss: 0.00001813
Iteration 131/1000 | Loss: 0.00001813
Iteration 132/1000 | Loss: 0.00001813
Iteration 133/1000 | Loss: 0.00001812
Iteration 134/1000 | Loss: 0.00001812
Iteration 135/1000 | Loss: 0.00001812
Iteration 136/1000 | Loss: 0.00001812
Iteration 137/1000 | Loss: 0.00001812
Iteration 138/1000 | Loss: 0.00001812
Iteration 139/1000 | Loss: 0.00001812
Iteration 140/1000 | Loss: 0.00001812
Iteration 141/1000 | Loss: 0.00001812
Iteration 142/1000 | Loss: 0.00001811
Iteration 143/1000 | Loss: 0.00001811
Iteration 144/1000 | Loss: 0.00001811
Iteration 145/1000 | Loss: 0.00001811
Iteration 146/1000 | Loss: 0.00001811
Iteration 147/1000 | Loss: 0.00001811
Iteration 148/1000 | Loss: 0.00001811
Iteration 149/1000 | Loss: 0.00001811
Iteration 150/1000 | Loss: 0.00001811
Iteration 151/1000 | Loss: 0.00001811
Iteration 152/1000 | Loss: 0.00001810
Iteration 153/1000 | Loss: 0.00001810
Iteration 154/1000 | Loss: 0.00001810
Iteration 155/1000 | Loss: 0.00001810
Iteration 156/1000 | Loss: 0.00001810
Iteration 157/1000 | Loss: 0.00001810
Iteration 158/1000 | Loss: 0.00001810
Iteration 159/1000 | Loss: 0.00001809
Iteration 160/1000 | Loss: 0.00001809
Iteration 161/1000 | Loss: 0.00001809
Iteration 162/1000 | Loss: 0.00001809
Iteration 163/1000 | Loss: 0.00001808
Iteration 164/1000 | Loss: 0.00001808
Iteration 165/1000 | Loss: 0.00001808
Iteration 166/1000 | Loss: 0.00001808
Iteration 167/1000 | Loss: 0.00001808
Iteration 168/1000 | Loss: 0.00001808
Iteration 169/1000 | Loss: 0.00001808
Iteration 170/1000 | Loss: 0.00001808
Iteration 171/1000 | Loss: 0.00001808
Iteration 172/1000 | Loss: 0.00001808
Iteration 173/1000 | Loss: 0.00001807
Iteration 174/1000 | Loss: 0.00001807
Iteration 175/1000 | Loss: 0.00001807
Iteration 176/1000 | Loss: 0.00001807
Iteration 177/1000 | Loss: 0.00001807
Iteration 178/1000 | Loss: 0.00001807
Iteration 179/1000 | Loss: 0.00001807
Iteration 180/1000 | Loss: 0.00001806
Iteration 181/1000 | Loss: 0.00001806
Iteration 182/1000 | Loss: 0.00001806
Iteration 183/1000 | Loss: 0.00001806
Iteration 184/1000 | Loss: 0.00001806
Iteration 185/1000 | Loss: 0.00001806
Iteration 186/1000 | Loss: 0.00001806
Iteration 187/1000 | Loss: 0.00001806
Iteration 188/1000 | Loss: 0.00001806
Iteration 189/1000 | Loss: 0.00001806
Iteration 190/1000 | Loss: 0.00001806
Iteration 191/1000 | Loss: 0.00001806
Iteration 192/1000 | Loss: 0.00001806
Iteration 193/1000 | Loss: 0.00001806
Iteration 194/1000 | Loss: 0.00001806
Iteration 195/1000 | Loss: 0.00001805
Iteration 196/1000 | Loss: 0.00001805
Iteration 197/1000 | Loss: 0.00001805
Iteration 198/1000 | Loss: 0.00001805
Iteration 199/1000 | Loss: 0.00001805
Iteration 200/1000 | Loss: 0.00001805
Iteration 201/1000 | Loss: 0.00001805
Iteration 202/1000 | Loss: 0.00001805
Iteration 203/1000 | Loss: 0.00001805
Iteration 204/1000 | Loss: 0.00001805
Iteration 205/1000 | Loss: 0.00001805
Iteration 206/1000 | Loss: 0.00001804
Iteration 207/1000 | Loss: 0.00001804
Iteration 208/1000 | Loss: 0.00001804
Iteration 209/1000 | Loss: 0.00001804
Iteration 210/1000 | Loss: 0.00001804
Iteration 211/1000 | Loss: 0.00001804
Iteration 212/1000 | Loss: 0.00001804
Iteration 213/1000 | Loss: 0.00001804
Iteration 214/1000 | Loss: 0.00001804
Iteration 215/1000 | Loss: 0.00001804
Iteration 216/1000 | Loss: 0.00001804
Iteration 217/1000 | Loss: 0.00001804
Iteration 218/1000 | Loss: 0.00001804
Iteration 219/1000 | Loss: 0.00001804
Iteration 220/1000 | Loss: 0.00001804
Iteration 221/1000 | Loss: 0.00001804
Iteration 222/1000 | Loss: 0.00001804
Iteration 223/1000 | Loss: 0.00001804
Iteration 224/1000 | Loss: 0.00001804
Iteration 225/1000 | Loss: 0.00001804
Iteration 226/1000 | Loss: 0.00001804
Iteration 227/1000 | Loss: 0.00001804
Iteration 228/1000 | Loss: 0.00001804
Iteration 229/1000 | Loss: 0.00001804
Iteration 230/1000 | Loss: 0.00001804
Iteration 231/1000 | Loss: 0.00001804
Iteration 232/1000 | Loss: 0.00001804
Iteration 233/1000 | Loss: 0.00001804
Iteration 234/1000 | Loss: 0.00001804
Iteration 235/1000 | Loss: 0.00001804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.8042859665001743e-05, 1.8042859665001743e-05, 1.8042859665001743e-05, 1.8042859665001743e-05, 1.8042859665001743e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8042859665001743e-05

Optimization complete. Final v2v error: 3.5869221687316895 mm

Highest mean error: 5.208703994750977 mm for frame 70

Lowest mean error: 3.127964735031128 mm for frame 44

Saving results

Total time: 47.31231331825256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00523658
Iteration 2/25 | Loss: 0.00133585
Iteration 3/25 | Loss: 0.00121435
Iteration 4/25 | Loss: 0.00120414
Iteration 5/25 | Loss: 0.00120240
Iteration 6/25 | Loss: 0.00120204
Iteration 7/25 | Loss: 0.00120204
Iteration 8/25 | Loss: 0.00120204
Iteration 9/25 | Loss: 0.00120204
Iteration 10/25 | Loss: 0.00120204
Iteration 11/25 | Loss: 0.00120204
Iteration 12/25 | Loss: 0.00120204
Iteration 13/25 | Loss: 0.00120204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012020394206047058, 0.0012020394206047058, 0.0012020394206047058, 0.0012020394206047058, 0.0012020394206047058]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012020394206047058

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34650970
Iteration 2/25 | Loss: 0.00067994
Iteration 3/25 | Loss: 0.00067992
Iteration 4/25 | Loss: 0.00067992
Iteration 5/25 | Loss: 0.00067992
Iteration 6/25 | Loss: 0.00067992
Iteration 7/25 | Loss: 0.00067992
Iteration 8/25 | Loss: 0.00067992
Iteration 9/25 | Loss: 0.00067992
Iteration 10/25 | Loss: 0.00067992
Iteration 11/25 | Loss: 0.00067992
Iteration 12/25 | Loss: 0.00067992
Iteration 13/25 | Loss: 0.00067992
Iteration 14/25 | Loss: 0.00067992
Iteration 15/25 | Loss: 0.00067992
Iteration 16/25 | Loss: 0.00067992
Iteration 17/25 | Loss: 0.00067992
Iteration 18/25 | Loss: 0.00067992
Iteration 19/25 | Loss: 0.00067992
Iteration 20/25 | Loss: 0.00067992
Iteration 21/25 | Loss: 0.00067992
Iteration 22/25 | Loss: 0.00067992
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006799167022109032, 0.0006799167022109032, 0.0006799167022109032, 0.0006799167022109032, 0.0006799167022109032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006799167022109032

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067992
Iteration 2/1000 | Loss: 0.00003444
Iteration 3/1000 | Loss: 0.00002329
Iteration 4/1000 | Loss: 0.00001958
Iteration 5/1000 | Loss: 0.00001808
Iteration 6/1000 | Loss: 0.00001744
Iteration 7/1000 | Loss: 0.00001698
Iteration 8/1000 | Loss: 0.00001659
Iteration 9/1000 | Loss: 0.00001632
Iteration 10/1000 | Loss: 0.00001621
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001586
Iteration 14/1000 | Loss: 0.00001577
Iteration 15/1000 | Loss: 0.00001564
Iteration 16/1000 | Loss: 0.00001557
Iteration 17/1000 | Loss: 0.00001551
Iteration 18/1000 | Loss: 0.00001551
Iteration 19/1000 | Loss: 0.00001551
Iteration 20/1000 | Loss: 0.00001551
Iteration 21/1000 | Loss: 0.00001549
Iteration 22/1000 | Loss: 0.00001548
Iteration 23/1000 | Loss: 0.00001548
Iteration 24/1000 | Loss: 0.00001548
Iteration 25/1000 | Loss: 0.00001547
Iteration 26/1000 | Loss: 0.00001547
Iteration 27/1000 | Loss: 0.00001546
Iteration 28/1000 | Loss: 0.00001545
Iteration 29/1000 | Loss: 0.00001545
Iteration 30/1000 | Loss: 0.00001545
Iteration 31/1000 | Loss: 0.00001542
Iteration 32/1000 | Loss: 0.00001542
Iteration 33/1000 | Loss: 0.00001542
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001542
Iteration 36/1000 | Loss: 0.00001542
Iteration 37/1000 | Loss: 0.00001541
Iteration 38/1000 | Loss: 0.00001540
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00001540
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00001539
Iteration 43/1000 | Loss: 0.00001539
Iteration 44/1000 | Loss: 0.00001539
Iteration 45/1000 | Loss: 0.00001539
Iteration 46/1000 | Loss: 0.00001539
Iteration 47/1000 | Loss: 0.00001539
Iteration 48/1000 | Loss: 0.00001538
Iteration 49/1000 | Loss: 0.00001538
Iteration 50/1000 | Loss: 0.00001538
Iteration 51/1000 | Loss: 0.00001537
Iteration 52/1000 | Loss: 0.00001537
Iteration 53/1000 | Loss: 0.00001536
Iteration 54/1000 | Loss: 0.00001536
Iteration 55/1000 | Loss: 0.00001536
Iteration 56/1000 | Loss: 0.00001535
Iteration 57/1000 | Loss: 0.00001534
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001533
Iteration 61/1000 | Loss: 0.00001533
Iteration 62/1000 | Loss: 0.00001532
Iteration 63/1000 | Loss: 0.00001532
Iteration 64/1000 | Loss: 0.00001531
Iteration 65/1000 | Loss: 0.00001531
Iteration 66/1000 | Loss: 0.00001531
Iteration 67/1000 | Loss: 0.00001531
Iteration 68/1000 | Loss: 0.00001530
Iteration 69/1000 | Loss: 0.00001530
Iteration 70/1000 | Loss: 0.00001530
Iteration 71/1000 | Loss: 0.00001529
Iteration 72/1000 | Loss: 0.00001529
Iteration 73/1000 | Loss: 0.00001529
Iteration 74/1000 | Loss: 0.00001529
Iteration 75/1000 | Loss: 0.00001528
Iteration 76/1000 | Loss: 0.00001528
Iteration 77/1000 | Loss: 0.00001527
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001527
Iteration 80/1000 | Loss: 0.00001526
Iteration 81/1000 | Loss: 0.00001526
Iteration 82/1000 | Loss: 0.00001526
Iteration 83/1000 | Loss: 0.00001526
Iteration 84/1000 | Loss: 0.00001525
Iteration 85/1000 | Loss: 0.00001525
Iteration 86/1000 | Loss: 0.00001525
Iteration 87/1000 | Loss: 0.00001525
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001524
Iteration 92/1000 | Loss: 0.00001524
Iteration 93/1000 | Loss: 0.00001524
Iteration 94/1000 | Loss: 0.00001524
Iteration 95/1000 | Loss: 0.00001524
Iteration 96/1000 | Loss: 0.00001524
Iteration 97/1000 | Loss: 0.00001524
Iteration 98/1000 | Loss: 0.00001523
Iteration 99/1000 | Loss: 0.00001523
Iteration 100/1000 | Loss: 0.00001523
Iteration 101/1000 | Loss: 0.00001522
Iteration 102/1000 | Loss: 0.00001522
Iteration 103/1000 | Loss: 0.00001522
Iteration 104/1000 | Loss: 0.00001522
Iteration 105/1000 | Loss: 0.00001522
Iteration 106/1000 | Loss: 0.00001521
Iteration 107/1000 | Loss: 0.00001521
Iteration 108/1000 | Loss: 0.00001521
Iteration 109/1000 | Loss: 0.00001521
Iteration 110/1000 | Loss: 0.00001521
Iteration 111/1000 | Loss: 0.00001521
Iteration 112/1000 | Loss: 0.00001520
Iteration 113/1000 | Loss: 0.00001520
Iteration 114/1000 | Loss: 0.00001520
Iteration 115/1000 | Loss: 0.00001520
Iteration 116/1000 | Loss: 0.00001520
Iteration 117/1000 | Loss: 0.00001519
Iteration 118/1000 | Loss: 0.00001519
Iteration 119/1000 | Loss: 0.00001519
Iteration 120/1000 | Loss: 0.00001519
Iteration 121/1000 | Loss: 0.00001519
Iteration 122/1000 | Loss: 0.00001519
Iteration 123/1000 | Loss: 0.00001519
Iteration 124/1000 | Loss: 0.00001519
Iteration 125/1000 | Loss: 0.00001519
Iteration 126/1000 | Loss: 0.00001519
Iteration 127/1000 | Loss: 0.00001519
Iteration 128/1000 | Loss: 0.00001518
Iteration 129/1000 | Loss: 0.00001518
Iteration 130/1000 | Loss: 0.00001518
Iteration 131/1000 | Loss: 0.00001518
Iteration 132/1000 | Loss: 0.00001518
Iteration 133/1000 | Loss: 0.00001518
Iteration 134/1000 | Loss: 0.00001518
Iteration 135/1000 | Loss: 0.00001518
Iteration 136/1000 | Loss: 0.00001518
Iteration 137/1000 | Loss: 0.00001518
Iteration 138/1000 | Loss: 0.00001518
Iteration 139/1000 | Loss: 0.00001518
Iteration 140/1000 | Loss: 0.00001518
Iteration 141/1000 | Loss: 0.00001518
Iteration 142/1000 | Loss: 0.00001518
Iteration 143/1000 | Loss: 0.00001518
Iteration 144/1000 | Loss: 0.00001518
Iteration 145/1000 | Loss: 0.00001518
Iteration 146/1000 | Loss: 0.00001517
Iteration 147/1000 | Loss: 0.00001517
Iteration 148/1000 | Loss: 0.00001517
Iteration 149/1000 | Loss: 0.00001517
Iteration 150/1000 | Loss: 0.00001517
Iteration 151/1000 | Loss: 0.00001517
Iteration 152/1000 | Loss: 0.00001517
Iteration 153/1000 | Loss: 0.00001517
Iteration 154/1000 | Loss: 0.00001517
Iteration 155/1000 | Loss: 0.00001517
Iteration 156/1000 | Loss: 0.00001517
Iteration 157/1000 | Loss: 0.00001517
Iteration 158/1000 | Loss: 0.00001517
Iteration 159/1000 | Loss: 0.00001516
Iteration 160/1000 | Loss: 0.00001516
Iteration 161/1000 | Loss: 0.00001516
Iteration 162/1000 | Loss: 0.00001516
Iteration 163/1000 | Loss: 0.00001516
Iteration 164/1000 | Loss: 0.00001516
Iteration 165/1000 | Loss: 0.00001516
Iteration 166/1000 | Loss: 0.00001516
Iteration 167/1000 | Loss: 0.00001515
Iteration 168/1000 | Loss: 0.00001515
Iteration 169/1000 | Loss: 0.00001515
Iteration 170/1000 | Loss: 0.00001515
Iteration 171/1000 | Loss: 0.00001515
Iteration 172/1000 | Loss: 0.00001515
Iteration 173/1000 | Loss: 0.00001515
Iteration 174/1000 | Loss: 0.00001514
Iteration 175/1000 | Loss: 0.00001514
Iteration 176/1000 | Loss: 0.00001514
Iteration 177/1000 | Loss: 0.00001514
Iteration 178/1000 | Loss: 0.00001514
Iteration 179/1000 | Loss: 0.00001514
Iteration 180/1000 | Loss: 0.00001514
Iteration 181/1000 | Loss: 0.00001514
Iteration 182/1000 | Loss: 0.00001514
Iteration 183/1000 | Loss: 0.00001514
Iteration 184/1000 | Loss: 0.00001514
Iteration 185/1000 | Loss: 0.00001514
Iteration 186/1000 | Loss: 0.00001514
Iteration 187/1000 | Loss: 0.00001514
Iteration 188/1000 | Loss: 0.00001514
Iteration 189/1000 | Loss: 0.00001514
Iteration 190/1000 | Loss: 0.00001513
Iteration 191/1000 | Loss: 0.00001513
Iteration 192/1000 | Loss: 0.00001513
Iteration 193/1000 | Loss: 0.00001513
Iteration 194/1000 | Loss: 0.00001513
Iteration 195/1000 | Loss: 0.00001513
Iteration 196/1000 | Loss: 0.00001513
Iteration 197/1000 | Loss: 0.00001513
Iteration 198/1000 | Loss: 0.00001513
Iteration 199/1000 | Loss: 0.00001513
Iteration 200/1000 | Loss: 0.00001513
Iteration 201/1000 | Loss: 0.00001513
Iteration 202/1000 | Loss: 0.00001513
Iteration 203/1000 | Loss: 0.00001513
Iteration 204/1000 | Loss: 0.00001513
Iteration 205/1000 | Loss: 0.00001513
Iteration 206/1000 | Loss: 0.00001513
Iteration 207/1000 | Loss: 0.00001513
Iteration 208/1000 | Loss: 0.00001513
Iteration 209/1000 | Loss: 0.00001513
Iteration 210/1000 | Loss: 0.00001513
Iteration 211/1000 | Loss: 0.00001513
Iteration 212/1000 | Loss: 0.00001513
Iteration 213/1000 | Loss: 0.00001513
Iteration 214/1000 | Loss: 0.00001513
Iteration 215/1000 | Loss: 0.00001513
Iteration 216/1000 | Loss: 0.00001513
Iteration 217/1000 | Loss: 0.00001513
Iteration 218/1000 | Loss: 0.00001513
Iteration 219/1000 | Loss: 0.00001513
Iteration 220/1000 | Loss: 0.00001513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.5129972780414391e-05, 1.5129972780414391e-05, 1.5129972780414391e-05, 1.5129972780414391e-05, 1.5129972780414391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5129972780414391e-05

Optimization complete. Final v2v error: 3.2332475185394287 mm

Highest mean error: 3.6829965114593506 mm for frame 47

Lowest mean error: 2.8143961429595947 mm for frame 104

Saving results

Total time: 43.66039705276489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00342768
Iteration 2/25 | Loss: 0.00143392
Iteration 3/25 | Loss: 0.00120262
Iteration 4/25 | Loss: 0.00116060
Iteration 5/25 | Loss: 0.00115041
Iteration 6/25 | Loss: 0.00114784
Iteration 7/25 | Loss: 0.00114997
Iteration 8/25 | Loss: 0.00114718
Iteration 9/25 | Loss: 0.00114647
Iteration 10/25 | Loss: 0.00114604
Iteration 11/25 | Loss: 0.00114419
Iteration 12/25 | Loss: 0.00114377
Iteration 13/25 | Loss: 0.00114369
Iteration 14/25 | Loss: 0.00114369
Iteration 15/25 | Loss: 0.00114369
Iteration 16/25 | Loss: 0.00114369
Iteration 17/25 | Loss: 0.00114369
Iteration 18/25 | Loss: 0.00114369
Iteration 19/25 | Loss: 0.00114368
Iteration 20/25 | Loss: 0.00114368
Iteration 21/25 | Loss: 0.00114368
Iteration 22/25 | Loss: 0.00114368
Iteration 23/25 | Loss: 0.00114368
Iteration 24/25 | Loss: 0.00114368
Iteration 25/25 | Loss: 0.00114368

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36410713
Iteration 2/25 | Loss: 0.00101096
Iteration 3/25 | Loss: 0.00101095
Iteration 4/25 | Loss: 0.00101095
Iteration 5/25 | Loss: 0.00101095
Iteration 6/25 | Loss: 0.00101095
Iteration 7/25 | Loss: 0.00101095
Iteration 8/25 | Loss: 0.00101095
Iteration 9/25 | Loss: 0.00101095
Iteration 10/25 | Loss: 0.00101095
Iteration 11/25 | Loss: 0.00101095
Iteration 12/25 | Loss: 0.00101095
Iteration 13/25 | Loss: 0.00101095
Iteration 14/25 | Loss: 0.00101095
Iteration 15/25 | Loss: 0.00101095
Iteration 16/25 | Loss: 0.00101095
Iteration 17/25 | Loss: 0.00101095
Iteration 18/25 | Loss: 0.00101095
Iteration 19/25 | Loss: 0.00101095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010109514696523547, 0.0010109514696523547, 0.0010109514696523547, 0.0010109514696523547, 0.0010109514696523547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010109514696523547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101095
Iteration 2/1000 | Loss: 0.00004558
Iteration 3/1000 | Loss: 0.00002640
Iteration 4/1000 | Loss: 0.00002055
Iteration 5/1000 | Loss: 0.00001873
Iteration 6/1000 | Loss: 0.00001766
Iteration 7/1000 | Loss: 0.00001677
Iteration 8/1000 | Loss: 0.00001623
Iteration 9/1000 | Loss: 0.00001573
Iteration 10/1000 | Loss: 0.00001543
Iteration 11/1000 | Loss: 0.00001525
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001501
Iteration 14/1000 | Loss: 0.00001488
Iteration 15/1000 | Loss: 0.00001478
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001475
Iteration 21/1000 | Loss: 0.00001474
Iteration 22/1000 | Loss: 0.00001473
Iteration 23/1000 | Loss: 0.00001472
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001472
Iteration 26/1000 | Loss: 0.00001471
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001469
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001462
Iteration 32/1000 | Loss: 0.00001461
Iteration 33/1000 | Loss: 0.00001458
Iteration 34/1000 | Loss: 0.00001456
Iteration 35/1000 | Loss: 0.00001456
Iteration 36/1000 | Loss: 0.00001455
Iteration 37/1000 | Loss: 0.00001455
Iteration 38/1000 | Loss: 0.00001455
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001454
Iteration 41/1000 | Loss: 0.00001454
Iteration 42/1000 | Loss: 0.00001454
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001453
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001453
Iteration 47/1000 | Loss: 0.00001452
Iteration 48/1000 | Loss: 0.00001452
Iteration 49/1000 | Loss: 0.00001452
Iteration 50/1000 | Loss: 0.00001451
Iteration 51/1000 | Loss: 0.00001451
Iteration 52/1000 | Loss: 0.00001451
Iteration 53/1000 | Loss: 0.00001450
Iteration 54/1000 | Loss: 0.00001450
Iteration 55/1000 | Loss: 0.00001450
Iteration 56/1000 | Loss: 0.00001450
Iteration 57/1000 | Loss: 0.00001450
Iteration 58/1000 | Loss: 0.00001450
Iteration 59/1000 | Loss: 0.00001449
Iteration 60/1000 | Loss: 0.00001449
Iteration 61/1000 | Loss: 0.00001449
Iteration 62/1000 | Loss: 0.00001449
Iteration 63/1000 | Loss: 0.00001449
Iteration 64/1000 | Loss: 0.00001449
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001448
Iteration 68/1000 | Loss: 0.00001448
Iteration 69/1000 | Loss: 0.00001448
Iteration 70/1000 | Loss: 0.00001447
Iteration 71/1000 | Loss: 0.00001447
Iteration 72/1000 | Loss: 0.00001447
Iteration 73/1000 | Loss: 0.00001446
Iteration 74/1000 | Loss: 0.00001446
Iteration 75/1000 | Loss: 0.00001446
Iteration 76/1000 | Loss: 0.00001446
Iteration 77/1000 | Loss: 0.00001445
Iteration 78/1000 | Loss: 0.00001445
Iteration 79/1000 | Loss: 0.00001445
Iteration 80/1000 | Loss: 0.00001445
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001444
Iteration 85/1000 | Loss: 0.00001444
Iteration 86/1000 | Loss: 0.00001444
Iteration 87/1000 | Loss: 0.00001444
Iteration 88/1000 | Loss: 0.00001444
Iteration 89/1000 | Loss: 0.00001444
Iteration 90/1000 | Loss: 0.00001444
Iteration 91/1000 | Loss: 0.00001444
Iteration 92/1000 | Loss: 0.00001443
Iteration 93/1000 | Loss: 0.00001443
Iteration 94/1000 | Loss: 0.00001443
Iteration 95/1000 | Loss: 0.00001443
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001443
Iteration 98/1000 | Loss: 0.00001443
Iteration 99/1000 | Loss: 0.00001442
Iteration 100/1000 | Loss: 0.00001442
Iteration 101/1000 | Loss: 0.00001442
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001441
Iteration 105/1000 | Loss: 0.00001441
Iteration 106/1000 | Loss: 0.00001440
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001440
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001440
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001439
Iteration 114/1000 | Loss: 0.00001439
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001438
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001438
Iteration 123/1000 | Loss: 0.00001438
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001437
Iteration 133/1000 | Loss: 0.00001437
Iteration 134/1000 | Loss: 0.00001437
Iteration 135/1000 | Loss: 0.00001437
Iteration 136/1000 | Loss: 0.00001436
Iteration 137/1000 | Loss: 0.00001436
Iteration 138/1000 | Loss: 0.00001436
Iteration 139/1000 | Loss: 0.00001436
Iteration 140/1000 | Loss: 0.00001436
Iteration 141/1000 | Loss: 0.00001436
Iteration 142/1000 | Loss: 0.00001435
Iteration 143/1000 | Loss: 0.00001435
Iteration 144/1000 | Loss: 0.00001435
Iteration 145/1000 | Loss: 0.00001435
Iteration 146/1000 | Loss: 0.00001435
Iteration 147/1000 | Loss: 0.00001435
Iteration 148/1000 | Loss: 0.00001435
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001434
Iteration 156/1000 | Loss: 0.00001434
Iteration 157/1000 | Loss: 0.00001434
Iteration 158/1000 | Loss: 0.00001434
Iteration 159/1000 | Loss: 0.00001434
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001434
Iteration 162/1000 | Loss: 0.00001434
Iteration 163/1000 | Loss: 0.00001434
Iteration 164/1000 | Loss: 0.00001434
Iteration 165/1000 | Loss: 0.00001434
Iteration 166/1000 | Loss: 0.00001434
Iteration 167/1000 | Loss: 0.00001434
Iteration 168/1000 | Loss: 0.00001433
Iteration 169/1000 | Loss: 0.00001433
Iteration 170/1000 | Loss: 0.00001433
Iteration 171/1000 | Loss: 0.00001433
Iteration 172/1000 | Loss: 0.00001433
Iteration 173/1000 | Loss: 0.00001433
Iteration 174/1000 | Loss: 0.00001433
Iteration 175/1000 | Loss: 0.00001433
Iteration 176/1000 | Loss: 0.00001433
Iteration 177/1000 | Loss: 0.00001433
Iteration 178/1000 | Loss: 0.00001433
Iteration 179/1000 | Loss: 0.00001433
Iteration 180/1000 | Loss: 0.00001433
Iteration 181/1000 | Loss: 0.00001433
Iteration 182/1000 | Loss: 0.00001433
Iteration 183/1000 | Loss: 0.00001433
Iteration 184/1000 | Loss: 0.00001433
Iteration 185/1000 | Loss: 0.00001433
Iteration 186/1000 | Loss: 0.00001432
Iteration 187/1000 | Loss: 0.00001432
Iteration 188/1000 | Loss: 0.00001432
Iteration 189/1000 | Loss: 0.00001432
Iteration 190/1000 | Loss: 0.00001432
Iteration 191/1000 | Loss: 0.00001432
Iteration 192/1000 | Loss: 0.00001432
Iteration 193/1000 | Loss: 0.00001432
Iteration 194/1000 | Loss: 0.00001432
Iteration 195/1000 | Loss: 0.00001432
Iteration 196/1000 | Loss: 0.00001432
Iteration 197/1000 | Loss: 0.00001432
Iteration 198/1000 | Loss: 0.00001432
Iteration 199/1000 | Loss: 0.00001432
Iteration 200/1000 | Loss: 0.00001432
Iteration 201/1000 | Loss: 0.00001432
Iteration 202/1000 | Loss: 0.00001432
Iteration 203/1000 | Loss: 0.00001432
Iteration 204/1000 | Loss: 0.00001432
Iteration 205/1000 | Loss: 0.00001432
Iteration 206/1000 | Loss: 0.00001432
Iteration 207/1000 | Loss: 0.00001432
Iteration 208/1000 | Loss: 0.00001432
Iteration 209/1000 | Loss: 0.00001432
Iteration 210/1000 | Loss: 0.00001432
Iteration 211/1000 | Loss: 0.00001432
Iteration 212/1000 | Loss: 0.00001432
Iteration 213/1000 | Loss: 0.00001432
Iteration 214/1000 | Loss: 0.00001432
Iteration 215/1000 | Loss: 0.00001432
Iteration 216/1000 | Loss: 0.00001432
Iteration 217/1000 | Loss: 0.00001432
Iteration 218/1000 | Loss: 0.00001432
Iteration 219/1000 | Loss: 0.00001432
Iteration 220/1000 | Loss: 0.00001432
Iteration 221/1000 | Loss: 0.00001432
Iteration 222/1000 | Loss: 0.00001432
Iteration 223/1000 | Loss: 0.00001432
Iteration 224/1000 | Loss: 0.00001432
Iteration 225/1000 | Loss: 0.00001432
Iteration 226/1000 | Loss: 0.00001432
Iteration 227/1000 | Loss: 0.00001432
Iteration 228/1000 | Loss: 0.00001432
Iteration 229/1000 | Loss: 0.00001432
Iteration 230/1000 | Loss: 0.00001432
Iteration 231/1000 | Loss: 0.00001432
Iteration 232/1000 | Loss: 0.00001432
Iteration 233/1000 | Loss: 0.00001432
Iteration 234/1000 | Loss: 0.00001432
Iteration 235/1000 | Loss: 0.00001432
Iteration 236/1000 | Loss: 0.00001432
Iteration 237/1000 | Loss: 0.00001432
Iteration 238/1000 | Loss: 0.00001432
Iteration 239/1000 | Loss: 0.00001432
Iteration 240/1000 | Loss: 0.00001432
Iteration 241/1000 | Loss: 0.00001432
Iteration 242/1000 | Loss: 0.00001432
Iteration 243/1000 | Loss: 0.00001432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [1.4323724826681428e-05, 1.4323724826681428e-05, 1.4323724826681428e-05, 1.4323724826681428e-05, 1.4323724826681428e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4323724826681428e-05

Optimization complete. Final v2v error: 3.203021287918091 mm

Highest mean error: 3.9924144744873047 mm for frame 70

Lowest mean error: 2.635526418685913 mm for frame 0

Saving results

Total time: 56.31404399871826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979974
Iteration 2/25 | Loss: 0.00182880
Iteration 3/25 | Loss: 0.00146292
Iteration 4/25 | Loss: 0.00138920
Iteration 5/25 | Loss: 0.00134968
Iteration 6/25 | Loss: 0.00137315
Iteration 7/25 | Loss: 0.00133276
Iteration 8/25 | Loss: 0.00130215
Iteration 9/25 | Loss: 0.00127424
Iteration 10/25 | Loss: 0.00127153
Iteration 11/25 | Loss: 0.00126455
Iteration 12/25 | Loss: 0.00125319
Iteration 13/25 | Loss: 0.00125752
Iteration 14/25 | Loss: 0.00124699
Iteration 15/25 | Loss: 0.00124722
Iteration 16/25 | Loss: 0.00124481
Iteration 17/25 | Loss: 0.00124130
Iteration 18/25 | Loss: 0.00124418
Iteration 19/25 | Loss: 0.00124449
Iteration 20/25 | Loss: 0.00123992
Iteration 21/25 | Loss: 0.00123992
Iteration 22/25 | Loss: 0.00124432
Iteration 23/25 | Loss: 0.00123988
Iteration 24/25 | Loss: 0.00123816
Iteration 25/25 | Loss: 0.00124004

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34357524
Iteration 2/25 | Loss: 0.00110303
Iteration 3/25 | Loss: 0.00095716
Iteration 4/25 | Loss: 0.00095716
Iteration 5/25 | Loss: 0.00095716
Iteration 6/25 | Loss: 0.00095716
Iteration 7/25 | Loss: 0.00095716
Iteration 8/25 | Loss: 0.00095716
Iteration 9/25 | Loss: 0.00095716
Iteration 10/25 | Loss: 0.00095716
Iteration 11/25 | Loss: 0.00095716
Iteration 12/25 | Loss: 0.00095716
Iteration 13/25 | Loss: 0.00095716
Iteration 14/25 | Loss: 0.00095716
Iteration 15/25 | Loss: 0.00095716
Iteration 16/25 | Loss: 0.00095716
Iteration 17/25 | Loss: 0.00095716
Iteration 18/25 | Loss: 0.00095716
Iteration 19/25 | Loss: 0.00095716
Iteration 20/25 | Loss: 0.00095716
Iteration 21/25 | Loss: 0.00095716
Iteration 22/25 | Loss: 0.00095716
Iteration 23/25 | Loss: 0.00095716
Iteration 24/25 | Loss: 0.00095716
Iteration 25/25 | Loss: 0.00095716

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095716
Iteration 2/1000 | Loss: 0.00013822
Iteration 3/1000 | Loss: 0.00020941
Iteration 4/1000 | Loss: 0.00025499
Iteration 5/1000 | Loss: 0.00021154
Iteration 6/1000 | Loss: 0.00013792
Iteration 7/1000 | Loss: 0.00019248
Iteration 8/1000 | Loss: 0.00014674
Iteration 9/1000 | Loss: 0.00015244
Iteration 10/1000 | Loss: 0.00006494
Iteration 11/1000 | Loss: 0.00011139
Iteration 12/1000 | Loss: 0.00010032
Iteration 13/1000 | Loss: 0.00009072
Iteration 14/1000 | Loss: 0.00003631
Iteration 15/1000 | Loss: 0.00004370
Iteration 16/1000 | Loss: 0.00003286
Iteration 17/1000 | Loss: 0.00003728
Iteration 18/1000 | Loss: 0.00003213
Iteration 19/1000 | Loss: 0.00017402
Iteration 20/1000 | Loss: 0.00006020
Iteration 21/1000 | Loss: 0.00003712
Iteration 22/1000 | Loss: 0.00014208
Iteration 23/1000 | Loss: 0.00045597
Iteration 24/1000 | Loss: 0.00023391
Iteration 25/1000 | Loss: 0.00022380
Iteration 26/1000 | Loss: 0.00019231
Iteration 27/1000 | Loss: 0.00020770
Iteration 28/1000 | Loss: 0.00018331
Iteration 29/1000 | Loss: 0.00005667
Iteration 30/1000 | Loss: 0.00003046
Iteration 31/1000 | Loss: 0.00002950
Iteration 32/1000 | Loss: 0.00015508
Iteration 33/1000 | Loss: 0.00011357
Iteration 34/1000 | Loss: 0.00015802
Iteration 35/1000 | Loss: 0.00010777
Iteration 36/1000 | Loss: 0.00013036
Iteration 37/1000 | Loss: 0.00017040
Iteration 38/1000 | Loss: 0.00015865
Iteration 39/1000 | Loss: 0.00002962
Iteration 40/1000 | Loss: 0.00002849
Iteration 41/1000 | Loss: 0.00002775
Iteration 42/1000 | Loss: 0.00002712
Iteration 43/1000 | Loss: 0.00011891
Iteration 44/1000 | Loss: 0.00008986
Iteration 45/1000 | Loss: 0.00014184
Iteration 46/1000 | Loss: 0.00015027
Iteration 47/1000 | Loss: 0.00006715
Iteration 48/1000 | Loss: 0.00012265
Iteration 49/1000 | Loss: 0.00007816
Iteration 50/1000 | Loss: 0.00011924
Iteration 51/1000 | Loss: 0.00013450
Iteration 52/1000 | Loss: 0.00008517
Iteration 53/1000 | Loss: 0.00005869
Iteration 54/1000 | Loss: 0.00014395
Iteration 55/1000 | Loss: 0.00009810
Iteration 56/1000 | Loss: 0.00012860
Iteration 57/1000 | Loss: 0.00012738
Iteration 58/1000 | Loss: 0.00005137
Iteration 59/1000 | Loss: 0.00006934
Iteration 60/1000 | Loss: 0.00002749
Iteration 61/1000 | Loss: 0.00002674
Iteration 62/1000 | Loss: 0.00002637
Iteration 63/1000 | Loss: 0.00002625
Iteration 64/1000 | Loss: 0.00002621
Iteration 65/1000 | Loss: 0.00002620
Iteration 66/1000 | Loss: 0.00002619
Iteration 67/1000 | Loss: 0.00002618
Iteration 68/1000 | Loss: 0.00002616
Iteration 69/1000 | Loss: 0.00002614
Iteration 70/1000 | Loss: 0.00002614
Iteration 71/1000 | Loss: 0.00002613
Iteration 72/1000 | Loss: 0.00002613
Iteration 73/1000 | Loss: 0.00002613
Iteration 74/1000 | Loss: 0.00002612
Iteration 75/1000 | Loss: 0.00002612
Iteration 76/1000 | Loss: 0.00002610
Iteration 77/1000 | Loss: 0.00002610
Iteration 78/1000 | Loss: 0.00002609
Iteration 79/1000 | Loss: 0.00002609
Iteration 80/1000 | Loss: 0.00002608
Iteration 81/1000 | Loss: 0.00002607
Iteration 82/1000 | Loss: 0.00002606
Iteration 83/1000 | Loss: 0.00002601
Iteration 84/1000 | Loss: 0.00010076
Iteration 85/1000 | Loss: 0.00002689
Iteration 86/1000 | Loss: 0.00002590
Iteration 87/1000 | Loss: 0.00002584
Iteration 88/1000 | Loss: 0.00002581
Iteration 89/1000 | Loss: 0.00002581
Iteration 90/1000 | Loss: 0.00002580
Iteration 91/1000 | Loss: 0.00002579
Iteration 92/1000 | Loss: 0.00002579
Iteration 93/1000 | Loss: 0.00002579
Iteration 94/1000 | Loss: 0.00002579
Iteration 95/1000 | Loss: 0.00002579
Iteration 96/1000 | Loss: 0.00002579
Iteration 97/1000 | Loss: 0.00002579
Iteration 98/1000 | Loss: 0.00002578
Iteration 99/1000 | Loss: 0.00002577
Iteration 100/1000 | Loss: 0.00002577
Iteration 101/1000 | Loss: 0.00002576
Iteration 102/1000 | Loss: 0.00002576
Iteration 103/1000 | Loss: 0.00002576
Iteration 104/1000 | Loss: 0.00002575
Iteration 105/1000 | Loss: 0.00002575
Iteration 106/1000 | Loss: 0.00008662
Iteration 107/1000 | Loss: 0.00005098
Iteration 108/1000 | Loss: 0.00002591
Iteration 109/1000 | Loss: 0.00012691
Iteration 110/1000 | Loss: 0.00014808
Iteration 111/1000 | Loss: 0.00007163
Iteration 112/1000 | Loss: 0.00006557
Iteration 113/1000 | Loss: 0.00006121
Iteration 114/1000 | Loss: 0.00003431
Iteration 115/1000 | Loss: 0.00002783
Iteration 116/1000 | Loss: 0.00002695
Iteration 117/1000 | Loss: 0.00015485
Iteration 118/1000 | Loss: 0.00014757
Iteration 119/1000 | Loss: 0.00003771
Iteration 120/1000 | Loss: 0.00003070
Iteration 121/1000 | Loss: 0.00002867
Iteration 122/1000 | Loss: 0.00002798
Iteration 123/1000 | Loss: 0.00002744
Iteration 124/1000 | Loss: 0.00012679
Iteration 125/1000 | Loss: 0.00012734
Iteration 126/1000 | Loss: 0.00043689
Iteration 127/1000 | Loss: 0.00003498
Iteration 128/1000 | Loss: 0.00002604
Iteration 129/1000 | Loss: 0.00002569
Iteration 130/1000 | Loss: 0.00002542
Iteration 131/1000 | Loss: 0.00002540
Iteration 132/1000 | Loss: 0.00002539
Iteration 133/1000 | Loss: 0.00002523
Iteration 134/1000 | Loss: 0.00002520
Iteration 135/1000 | Loss: 0.00002518
Iteration 136/1000 | Loss: 0.00002517
Iteration 137/1000 | Loss: 0.00002516
Iteration 138/1000 | Loss: 0.00002515
Iteration 139/1000 | Loss: 0.00002503
Iteration 140/1000 | Loss: 0.00002502
Iteration 141/1000 | Loss: 0.00002501
Iteration 142/1000 | Loss: 0.00002489
Iteration 143/1000 | Loss: 0.00002489
Iteration 144/1000 | Loss: 0.00002481
Iteration 145/1000 | Loss: 0.00002481
Iteration 146/1000 | Loss: 0.00002477
Iteration 147/1000 | Loss: 0.00002475
Iteration 148/1000 | Loss: 0.00002474
Iteration 149/1000 | Loss: 0.00002474
Iteration 150/1000 | Loss: 0.00014457
Iteration 151/1000 | Loss: 0.00008280
Iteration 152/1000 | Loss: 0.00012017
Iteration 153/1000 | Loss: 0.00004384
Iteration 154/1000 | Loss: 0.00005904
Iteration 155/1000 | Loss: 0.00002578
Iteration 156/1000 | Loss: 0.00015264
Iteration 157/1000 | Loss: 0.00012725
Iteration 158/1000 | Loss: 0.00010383
Iteration 159/1000 | Loss: 0.00016659
Iteration 160/1000 | Loss: 0.00094334
Iteration 161/1000 | Loss: 0.00036560
Iteration 162/1000 | Loss: 0.00048043
Iteration 163/1000 | Loss: 0.00003622
Iteration 164/1000 | Loss: 0.00012975
Iteration 165/1000 | Loss: 0.00009440
Iteration 166/1000 | Loss: 0.00007407
Iteration 167/1000 | Loss: 0.00019451
Iteration 168/1000 | Loss: 0.00007053
Iteration 169/1000 | Loss: 0.00030499
Iteration 170/1000 | Loss: 0.00006793
Iteration 171/1000 | Loss: 0.00015135
Iteration 172/1000 | Loss: 0.00036607
Iteration 173/1000 | Loss: 0.00030599
Iteration 174/1000 | Loss: 0.00009331
Iteration 175/1000 | Loss: 0.00008172
Iteration 176/1000 | Loss: 0.00018366
Iteration 177/1000 | Loss: 0.00018445
Iteration 178/1000 | Loss: 0.00021298
Iteration 179/1000 | Loss: 0.00003338
Iteration 180/1000 | Loss: 0.00017017
Iteration 181/1000 | Loss: 0.00019300
Iteration 182/1000 | Loss: 0.00018198
Iteration 183/1000 | Loss: 0.00009170
Iteration 184/1000 | Loss: 0.00002668
Iteration 185/1000 | Loss: 0.00006926
Iteration 186/1000 | Loss: 0.00007612
Iteration 187/1000 | Loss: 0.00006891
Iteration 188/1000 | Loss: 0.00006316
Iteration 189/1000 | Loss: 0.00006354
Iteration 190/1000 | Loss: 0.00015201
Iteration 191/1000 | Loss: 0.00012057
Iteration 192/1000 | Loss: 0.00015440
Iteration 193/1000 | Loss: 0.00012953
Iteration 194/1000 | Loss: 0.00009224
Iteration 195/1000 | Loss: 0.00002587
Iteration 196/1000 | Loss: 0.00018834
Iteration 197/1000 | Loss: 0.00011815
Iteration 198/1000 | Loss: 0.00007640
Iteration 199/1000 | Loss: 0.00020192
Iteration 200/1000 | Loss: 0.00030707
Iteration 201/1000 | Loss: 0.00018246
Iteration 202/1000 | Loss: 0.00014108
Iteration 203/1000 | Loss: 0.00016253
Iteration 204/1000 | Loss: 0.00012627
Iteration 205/1000 | Loss: 0.00011265
Iteration 206/1000 | Loss: 0.00011538
Iteration 207/1000 | Loss: 0.00007194
Iteration 208/1000 | Loss: 0.00002409
Iteration 209/1000 | Loss: 0.00002348
Iteration 210/1000 | Loss: 0.00002315
Iteration 211/1000 | Loss: 0.00002298
Iteration 212/1000 | Loss: 0.00002290
Iteration 213/1000 | Loss: 0.00002271
Iteration 214/1000 | Loss: 0.00002271
Iteration 215/1000 | Loss: 0.00002267
Iteration 216/1000 | Loss: 0.00002265
Iteration 217/1000 | Loss: 0.00002264
Iteration 218/1000 | Loss: 0.00002257
Iteration 219/1000 | Loss: 0.00002257
Iteration 220/1000 | Loss: 0.00002255
Iteration 221/1000 | Loss: 0.00002253
Iteration 222/1000 | Loss: 0.00002251
Iteration 223/1000 | Loss: 0.00002251
Iteration 224/1000 | Loss: 0.00002250
Iteration 225/1000 | Loss: 0.00002248
Iteration 226/1000 | Loss: 0.00002248
Iteration 227/1000 | Loss: 0.00002248
Iteration 228/1000 | Loss: 0.00002248
Iteration 229/1000 | Loss: 0.00002248
Iteration 230/1000 | Loss: 0.00002248
Iteration 231/1000 | Loss: 0.00002248
Iteration 232/1000 | Loss: 0.00002246
Iteration 233/1000 | Loss: 0.00002245
Iteration 234/1000 | Loss: 0.00002244
Iteration 235/1000 | Loss: 0.00002244
Iteration 236/1000 | Loss: 0.00002244
Iteration 237/1000 | Loss: 0.00002243
Iteration 238/1000 | Loss: 0.00002243
Iteration 239/1000 | Loss: 0.00002243
Iteration 240/1000 | Loss: 0.00002243
Iteration 241/1000 | Loss: 0.00002242
Iteration 242/1000 | Loss: 0.00002242
Iteration 243/1000 | Loss: 0.00002241
Iteration 244/1000 | Loss: 0.00002241
Iteration 245/1000 | Loss: 0.00002241
Iteration 246/1000 | Loss: 0.00002241
Iteration 247/1000 | Loss: 0.00002241
Iteration 248/1000 | Loss: 0.00002241
Iteration 249/1000 | Loss: 0.00002241
Iteration 250/1000 | Loss: 0.00002241
Iteration 251/1000 | Loss: 0.00002241
Iteration 252/1000 | Loss: 0.00002241
Iteration 253/1000 | Loss: 0.00002241
Iteration 254/1000 | Loss: 0.00002240
Iteration 255/1000 | Loss: 0.00002240
Iteration 256/1000 | Loss: 0.00002240
Iteration 257/1000 | Loss: 0.00002240
Iteration 258/1000 | Loss: 0.00002240
Iteration 259/1000 | Loss: 0.00002240
Iteration 260/1000 | Loss: 0.00002240
Iteration 261/1000 | Loss: 0.00002239
Iteration 262/1000 | Loss: 0.00002239
Iteration 263/1000 | Loss: 0.00002239
Iteration 264/1000 | Loss: 0.00002239
Iteration 265/1000 | Loss: 0.00002239
Iteration 266/1000 | Loss: 0.00002239
Iteration 267/1000 | Loss: 0.00002239
Iteration 268/1000 | Loss: 0.00002239
Iteration 269/1000 | Loss: 0.00002239
Iteration 270/1000 | Loss: 0.00002239
Iteration 271/1000 | Loss: 0.00002239
Iteration 272/1000 | Loss: 0.00002239
Iteration 273/1000 | Loss: 0.00002239
Iteration 274/1000 | Loss: 0.00002239
Iteration 275/1000 | Loss: 0.00002239
Iteration 276/1000 | Loss: 0.00002239
Iteration 277/1000 | Loss: 0.00002239
Iteration 278/1000 | Loss: 0.00002239
Iteration 279/1000 | Loss: 0.00002239
Iteration 280/1000 | Loss: 0.00002239
Iteration 281/1000 | Loss: 0.00002239
Iteration 282/1000 | Loss: 0.00002239
Iteration 283/1000 | Loss: 0.00002239
Iteration 284/1000 | Loss: 0.00002239
Iteration 285/1000 | Loss: 0.00002239
Iteration 286/1000 | Loss: 0.00002239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 286. Stopping optimization.
Last 5 losses: [2.239333298348356e-05, 2.239333298348356e-05, 2.239333298348356e-05, 2.239333298348356e-05, 2.239333298348356e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.239333298348356e-05

Optimization complete. Final v2v error: 3.9887163639068604 mm

Highest mean error: 5.503320217132568 mm for frame 134

Lowest mean error: 3.3823697566986084 mm for frame 25

Saving results

Total time: 322.51521825790405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821467
Iteration 2/25 | Loss: 0.00146137
Iteration 3/25 | Loss: 0.00127951
Iteration 4/25 | Loss: 0.00126864
Iteration 5/25 | Loss: 0.00126721
Iteration 6/25 | Loss: 0.00126717
Iteration 7/25 | Loss: 0.00126717
Iteration 8/25 | Loss: 0.00126717
Iteration 9/25 | Loss: 0.00126717
Iteration 10/25 | Loss: 0.00126717
Iteration 11/25 | Loss: 0.00126717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001267170999199152, 0.001267170999199152, 0.001267170999199152, 0.001267170999199152, 0.001267170999199152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001267170999199152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98861003
Iteration 2/25 | Loss: 0.00057396
Iteration 3/25 | Loss: 0.00057395
Iteration 4/25 | Loss: 0.00057395
Iteration 5/25 | Loss: 0.00057395
Iteration 6/25 | Loss: 0.00057395
Iteration 7/25 | Loss: 0.00057395
Iteration 8/25 | Loss: 0.00057395
Iteration 9/25 | Loss: 0.00057395
Iteration 10/25 | Loss: 0.00057395
Iteration 11/25 | Loss: 0.00057395
Iteration 12/25 | Loss: 0.00057395
Iteration 13/25 | Loss: 0.00057395
Iteration 14/25 | Loss: 0.00057395
Iteration 15/25 | Loss: 0.00057395
Iteration 16/25 | Loss: 0.00057395
Iteration 17/25 | Loss: 0.00057395
Iteration 18/25 | Loss: 0.00057395
Iteration 19/25 | Loss: 0.00057395
Iteration 20/25 | Loss: 0.00057395
Iteration 21/25 | Loss: 0.00057395
Iteration 22/25 | Loss: 0.00057395
Iteration 23/25 | Loss: 0.00057395
Iteration 24/25 | Loss: 0.00057395
Iteration 25/25 | Loss: 0.00057395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057395
Iteration 2/1000 | Loss: 0.00004068
Iteration 3/1000 | Loss: 0.00003061
Iteration 4/1000 | Loss: 0.00002758
Iteration 5/1000 | Loss: 0.00002589
Iteration 6/1000 | Loss: 0.00002495
Iteration 7/1000 | Loss: 0.00002438
Iteration 8/1000 | Loss: 0.00002398
Iteration 9/1000 | Loss: 0.00002359
Iteration 10/1000 | Loss: 0.00002332
Iteration 11/1000 | Loss: 0.00002308
Iteration 12/1000 | Loss: 0.00002274
Iteration 13/1000 | Loss: 0.00002244
Iteration 14/1000 | Loss: 0.00002237
Iteration 15/1000 | Loss: 0.00002223
Iteration 16/1000 | Loss: 0.00002222
Iteration 17/1000 | Loss: 0.00002222
Iteration 18/1000 | Loss: 0.00002222
Iteration 19/1000 | Loss: 0.00002221
Iteration 20/1000 | Loss: 0.00002207
Iteration 21/1000 | Loss: 0.00002206
Iteration 22/1000 | Loss: 0.00002193
Iteration 23/1000 | Loss: 0.00002192
Iteration 24/1000 | Loss: 0.00002192
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002191
Iteration 27/1000 | Loss: 0.00002190
Iteration 28/1000 | Loss: 0.00002189
Iteration 29/1000 | Loss: 0.00002188
Iteration 30/1000 | Loss: 0.00002188
Iteration 31/1000 | Loss: 0.00002187
Iteration 32/1000 | Loss: 0.00002184
Iteration 33/1000 | Loss: 0.00002184
Iteration 34/1000 | Loss: 0.00002184
Iteration 35/1000 | Loss: 0.00002184
Iteration 36/1000 | Loss: 0.00002184
Iteration 37/1000 | Loss: 0.00002184
Iteration 38/1000 | Loss: 0.00002184
Iteration 39/1000 | Loss: 0.00002184
Iteration 40/1000 | Loss: 0.00002184
Iteration 41/1000 | Loss: 0.00002183
Iteration 42/1000 | Loss: 0.00002183
Iteration 43/1000 | Loss: 0.00002178
Iteration 44/1000 | Loss: 0.00002177
Iteration 45/1000 | Loss: 0.00002177
Iteration 46/1000 | Loss: 0.00002176
Iteration 47/1000 | Loss: 0.00002171
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002169
Iteration 50/1000 | Loss: 0.00002169
Iteration 51/1000 | Loss: 0.00002169
Iteration 52/1000 | Loss: 0.00002169
Iteration 53/1000 | Loss: 0.00002168
Iteration 54/1000 | Loss: 0.00002168
Iteration 55/1000 | Loss: 0.00002168
Iteration 56/1000 | Loss: 0.00002168
Iteration 57/1000 | Loss: 0.00002167
Iteration 58/1000 | Loss: 0.00002167
Iteration 59/1000 | Loss: 0.00002167
Iteration 60/1000 | Loss: 0.00002167
Iteration 61/1000 | Loss: 0.00002166
Iteration 62/1000 | Loss: 0.00002166
Iteration 63/1000 | Loss: 0.00002166
Iteration 64/1000 | Loss: 0.00002166
Iteration 65/1000 | Loss: 0.00002166
Iteration 66/1000 | Loss: 0.00002166
Iteration 67/1000 | Loss: 0.00002165
Iteration 68/1000 | Loss: 0.00002165
Iteration 69/1000 | Loss: 0.00002164
Iteration 70/1000 | Loss: 0.00002164
Iteration 71/1000 | Loss: 0.00002164
Iteration 72/1000 | Loss: 0.00002164
Iteration 73/1000 | Loss: 0.00002164
Iteration 74/1000 | Loss: 0.00002164
Iteration 75/1000 | Loss: 0.00002164
Iteration 76/1000 | Loss: 0.00002164
Iteration 77/1000 | Loss: 0.00002164
Iteration 78/1000 | Loss: 0.00002164
Iteration 79/1000 | Loss: 0.00002164
Iteration 80/1000 | Loss: 0.00002164
Iteration 81/1000 | Loss: 0.00002164
Iteration 82/1000 | Loss: 0.00002164
Iteration 83/1000 | Loss: 0.00002164
Iteration 84/1000 | Loss: 0.00002164
Iteration 85/1000 | Loss: 0.00002164
Iteration 86/1000 | Loss: 0.00002164
Iteration 87/1000 | Loss: 0.00002164
Iteration 88/1000 | Loss: 0.00002164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [2.1639660189975984e-05, 2.1639660189975984e-05, 2.1639660189975984e-05, 2.1639660189975984e-05, 2.1639660189975984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1639660189975984e-05

Optimization complete. Final v2v error: 3.9054431915283203 mm

Highest mean error: 3.933576822280884 mm for frame 140

Lowest mean error: 3.881669044494629 mm for frame 78

Saving results

Total time: 37.06730246543884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00564852
Iteration 2/25 | Loss: 0.00133631
Iteration 3/25 | Loss: 0.00125287
Iteration 4/25 | Loss: 0.00124508
Iteration 5/25 | Loss: 0.00124346
Iteration 6/25 | Loss: 0.00124346
Iteration 7/25 | Loss: 0.00124346
Iteration 8/25 | Loss: 0.00124346
Iteration 9/25 | Loss: 0.00124346
Iteration 10/25 | Loss: 0.00124346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012434583622962236, 0.0012434583622962236, 0.0012434583622962236, 0.0012434583622962236, 0.0012434583622962236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012434583622962236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.63173616
Iteration 2/25 | Loss: 0.00085167
Iteration 3/25 | Loss: 0.00085166
Iteration 4/25 | Loss: 0.00085166
Iteration 5/25 | Loss: 0.00085166
Iteration 6/25 | Loss: 0.00085166
Iteration 7/25 | Loss: 0.00085166
Iteration 8/25 | Loss: 0.00085166
Iteration 9/25 | Loss: 0.00085166
Iteration 10/25 | Loss: 0.00085166
Iteration 11/25 | Loss: 0.00085166
Iteration 12/25 | Loss: 0.00085166
Iteration 13/25 | Loss: 0.00085166
Iteration 14/25 | Loss: 0.00085166
Iteration 15/25 | Loss: 0.00085166
Iteration 16/25 | Loss: 0.00085166
Iteration 17/25 | Loss: 0.00085166
Iteration 18/25 | Loss: 0.00085166
Iteration 19/25 | Loss: 0.00085166
Iteration 20/25 | Loss: 0.00085166
Iteration 21/25 | Loss: 0.00085166
Iteration 22/25 | Loss: 0.00085166
Iteration 23/25 | Loss: 0.00085166
Iteration 24/25 | Loss: 0.00085166
Iteration 25/25 | Loss: 0.00085166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085166
Iteration 2/1000 | Loss: 0.00003381
Iteration 3/1000 | Loss: 0.00002567
Iteration 4/1000 | Loss: 0.00002426
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002229
Iteration 7/1000 | Loss: 0.00002185
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002086
Iteration 10/1000 | Loss: 0.00002053
Iteration 11/1000 | Loss: 0.00002017
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001963
Iteration 14/1000 | Loss: 0.00001941
Iteration 15/1000 | Loss: 0.00001925
Iteration 16/1000 | Loss: 0.00001925
Iteration 17/1000 | Loss: 0.00001924
Iteration 18/1000 | Loss: 0.00001924
Iteration 19/1000 | Loss: 0.00001923
Iteration 20/1000 | Loss: 0.00001922
Iteration 21/1000 | Loss: 0.00001916
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001908
Iteration 24/1000 | Loss: 0.00001906
Iteration 25/1000 | Loss: 0.00001901
Iteration 26/1000 | Loss: 0.00001894
Iteration 27/1000 | Loss: 0.00001892
Iteration 28/1000 | Loss: 0.00001892
Iteration 29/1000 | Loss: 0.00001891
Iteration 30/1000 | Loss: 0.00001891
Iteration 31/1000 | Loss: 0.00001891
Iteration 32/1000 | Loss: 0.00001891
Iteration 33/1000 | Loss: 0.00001891
Iteration 34/1000 | Loss: 0.00001891
Iteration 35/1000 | Loss: 0.00001891
Iteration 36/1000 | Loss: 0.00001891
Iteration 37/1000 | Loss: 0.00001891
Iteration 38/1000 | Loss: 0.00001891
Iteration 39/1000 | Loss: 0.00001891
Iteration 40/1000 | Loss: 0.00001891
Iteration 41/1000 | Loss: 0.00001891
Iteration 42/1000 | Loss: 0.00001891
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001891
Iteration 46/1000 | Loss: 0.00001891
Iteration 47/1000 | Loss: 0.00001891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 47. Stopping optimization.
Last 5 losses: [1.8913178791990504e-05, 1.8913178791990504e-05, 1.8913178791990504e-05, 1.8913178791990504e-05, 1.8913178791990504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8913178791990504e-05

Optimization complete. Final v2v error: 3.52698016166687 mm

Highest mean error: 3.6217095851898193 mm for frame 27

Lowest mean error: 3.4712047576904297 mm for frame 139

Saving results

Total time: 40.05032253265381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971907
Iteration 2/25 | Loss: 0.00325803
Iteration 3/25 | Loss: 0.00251736
Iteration 4/25 | Loss: 0.00238636
Iteration 5/25 | Loss: 0.00227209
Iteration 6/25 | Loss: 0.00212211
Iteration 7/25 | Loss: 0.00183883
Iteration 8/25 | Loss: 0.00170670
Iteration 9/25 | Loss: 0.00160184
Iteration 10/25 | Loss: 0.00155340
Iteration 11/25 | Loss: 0.00151337
Iteration 12/25 | Loss: 0.00148222
Iteration 13/25 | Loss: 0.00147517
Iteration 14/25 | Loss: 0.00145043
Iteration 15/25 | Loss: 0.00145086
Iteration 16/25 | Loss: 0.00146060
Iteration 17/25 | Loss: 0.00146893
Iteration 18/25 | Loss: 0.00145258
Iteration 19/25 | Loss: 0.00144387
Iteration 20/25 | Loss: 0.00143501
Iteration 21/25 | Loss: 0.00142596
Iteration 22/25 | Loss: 0.00143028
Iteration 23/25 | Loss: 0.00142191
Iteration 24/25 | Loss: 0.00141198
Iteration 25/25 | Loss: 0.00140064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34699500
Iteration 2/25 | Loss: 0.00277605
Iteration 3/25 | Loss: 0.00242965
Iteration 4/25 | Loss: 0.00242965
Iteration 5/25 | Loss: 0.00242965
Iteration 6/25 | Loss: 0.00242965
Iteration 7/25 | Loss: 0.00242965
Iteration 8/25 | Loss: 0.00242965
Iteration 9/25 | Loss: 0.00242965
Iteration 10/25 | Loss: 0.00242965
Iteration 11/25 | Loss: 0.00242965
Iteration 12/25 | Loss: 0.00242965
Iteration 13/25 | Loss: 0.00242965
Iteration 14/25 | Loss: 0.00242965
Iteration 15/25 | Loss: 0.00242965
Iteration 16/25 | Loss: 0.00242965
Iteration 17/25 | Loss: 0.00242965
Iteration 18/25 | Loss: 0.00242965
Iteration 19/25 | Loss: 0.00242965
Iteration 20/25 | Loss: 0.00242965
Iteration 21/25 | Loss: 0.00242965
Iteration 22/25 | Loss: 0.00242965
Iteration 23/25 | Loss: 0.00242965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0024296464398503304, 0.0024296464398503304, 0.0024296464398503304, 0.0024296464398503304, 0.0024296464398503304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024296464398503304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00242965
Iteration 2/1000 | Loss: 0.00068640
Iteration 3/1000 | Loss: 0.00048894
Iteration 4/1000 | Loss: 0.00139680
Iteration 5/1000 | Loss: 0.00017019
Iteration 6/1000 | Loss: 0.00047480
Iteration 7/1000 | Loss: 0.00018376
Iteration 8/1000 | Loss: 0.00014979
Iteration 9/1000 | Loss: 0.00014315
Iteration 10/1000 | Loss: 0.00014602
Iteration 11/1000 | Loss: 0.00015558
Iteration 12/1000 | Loss: 0.00012769
Iteration 13/1000 | Loss: 0.00029597
Iteration 14/1000 | Loss: 0.00026556
Iteration 15/1000 | Loss: 0.00030192
Iteration 16/1000 | Loss: 0.00025010
Iteration 17/1000 | Loss: 0.00011646
Iteration 18/1000 | Loss: 0.00014639
Iteration 19/1000 | Loss: 0.00011406
Iteration 20/1000 | Loss: 0.00012692
Iteration 21/1000 | Loss: 0.00027014
Iteration 22/1000 | Loss: 0.00037022
Iteration 23/1000 | Loss: 0.00026113
Iteration 24/1000 | Loss: 0.00022411
Iteration 25/1000 | Loss: 0.00029950
Iteration 26/1000 | Loss: 0.00028077
Iteration 27/1000 | Loss: 0.00053700
Iteration 28/1000 | Loss: 0.00081779
Iteration 29/1000 | Loss: 0.00035782
Iteration 30/1000 | Loss: 0.00120101
Iteration 31/1000 | Loss: 0.00074712
Iteration 32/1000 | Loss: 0.00053911
Iteration 33/1000 | Loss: 0.00028072
Iteration 34/1000 | Loss: 0.00033241
Iteration 35/1000 | Loss: 0.00035087
Iteration 36/1000 | Loss: 0.00017479
Iteration 37/1000 | Loss: 0.00018343
Iteration 38/1000 | Loss: 0.00039137
Iteration 39/1000 | Loss: 0.00017106
Iteration 40/1000 | Loss: 0.00040476
Iteration 41/1000 | Loss: 0.00064065
Iteration 42/1000 | Loss: 0.00109975
Iteration 43/1000 | Loss: 0.00027641
Iteration 44/1000 | Loss: 0.00012700
Iteration 45/1000 | Loss: 0.00018337
Iteration 46/1000 | Loss: 0.00022416
Iteration 47/1000 | Loss: 0.00013347
Iteration 48/1000 | Loss: 0.00015768
Iteration 49/1000 | Loss: 0.00028994
Iteration 50/1000 | Loss: 0.00013499
Iteration 51/1000 | Loss: 0.00040374
Iteration 52/1000 | Loss: 0.00023876
Iteration 53/1000 | Loss: 0.00049676
Iteration 54/1000 | Loss: 0.00010634
Iteration 55/1000 | Loss: 0.00030792
Iteration 56/1000 | Loss: 0.00025595
Iteration 57/1000 | Loss: 0.00008950
Iteration 58/1000 | Loss: 0.00009090
Iteration 59/1000 | Loss: 0.00019706
Iteration 60/1000 | Loss: 0.00030307
Iteration 61/1000 | Loss: 0.00030682
Iteration 62/1000 | Loss: 0.00009974
Iteration 63/1000 | Loss: 0.00015438
Iteration 64/1000 | Loss: 0.00031161
Iteration 65/1000 | Loss: 0.00009743
Iteration 66/1000 | Loss: 0.00029396
Iteration 67/1000 | Loss: 0.00008120
Iteration 68/1000 | Loss: 0.00007794
Iteration 69/1000 | Loss: 0.00007564
Iteration 70/1000 | Loss: 0.00008980
Iteration 71/1000 | Loss: 0.00006849
Iteration 72/1000 | Loss: 0.00008421
Iteration 73/1000 | Loss: 0.00008358
Iteration 74/1000 | Loss: 0.00008581
Iteration 75/1000 | Loss: 0.00008602
Iteration 76/1000 | Loss: 0.00010813
Iteration 77/1000 | Loss: 0.00033367
Iteration 78/1000 | Loss: 0.00056905
Iteration 79/1000 | Loss: 0.00007999
Iteration 80/1000 | Loss: 0.00005743
Iteration 81/1000 | Loss: 0.00006602
Iteration 82/1000 | Loss: 0.00007568
Iteration 83/1000 | Loss: 0.00006533
Iteration 84/1000 | Loss: 0.00006906
Iteration 85/1000 | Loss: 0.00006600
Iteration 86/1000 | Loss: 0.00006606
Iteration 87/1000 | Loss: 0.00006211
Iteration 88/1000 | Loss: 0.00007545
Iteration 89/1000 | Loss: 0.00008278
Iteration 90/1000 | Loss: 0.00008117
Iteration 91/1000 | Loss: 0.00007824
Iteration 92/1000 | Loss: 0.00009169
Iteration 93/1000 | Loss: 0.00006552
Iteration 94/1000 | Loss: 0.00007521
Iteration 95/1000 | Loss: 0.00007413
Iteration 96/1000 | Loss: 0.00007385
Iteration 97/1000 | Loss: 0.00007617
Iteration 98/1000 | Loss: 0.00008561
Iteration 99/1000 | Loss: 0.00008520
Iteration 100/1000 | Loss: 0.00008838
Iteration 101/1000 | Loss: 0.00015134
Iteration 102/1000 | Loss: 0.00014893
Iteration 103/1000 | Loss: 0.00005737
Iteration 104/1000 | Loss: 0.00004876
Iteration 105/1000 | Loss: 0.00004585
Iteration 106/1000 | Loss: 0.00004399
Iteration 107/1000 | Loss: 0.00004327
Iteration 108/1000 | Loss: 0.00004236
Iteration 109/1000 | Loss: 0.00005279
Iteration 110/1000 | Loss: 0.00004256
Iteration 111/1000 | Loss: 0.00004105
Iteration 112/1000 | Loss: 0.00004031
Iteration 113/1000 | Loss: 0.00004111
Iteration 114/1000 | Loss: 0.00004015
Iteration 115/1000 | Loss: 0.00003987
Iteration 116/1000 | Loss: 0.00003961
Iteration 117/1000 | Loss: 0.00003937
Iteration 118/1000 | Loss: 0.00003922
Iteration 119/1000 | Loss: 0.00003922
Iteration 120/1000 | Loss: 0.00003920
Iteration 121/1000 | Loss: 0.00003919
Iteration 122/1000 | Loss: 0.00003914
Iteration 123/1000 | Loss: 0.00003914
Iteration 124/1000 | Loss: 0.00003907
Iteration 125/1000 | Loss: 0.00003906
Iteration 126/1000 | Loss: 0.00003904
Iteration 127/1000 | Loss: 0.00003903
Iteration 128/1000 | Loss: 0.00003901
Iteration 129/1000 | Loss: 0.00003900
Iteration 130/1000 | Loss: 0.00003900
Iteration 131/1000 | Loss: 0.00003900
Iteration 132/1000 | Loss: 0.00003899
Iteration 133/1000 | Loss: 0.00003899
Iteration 134/1000 | Loss: 0.00003897
Iteration 135/1000 | Loss: 0.00003896
Iteration 136/1000 | Loss: 0.00003896
Iteration 137/1000 | Loss: 0.00003896
Iteration 138/1000 | Loss: 0.00003890
Iteration 139/1000 | Loss: 0.00003890
Iteration 140/1000 | Loss: 0.00003890
Iteration 141/1000 | Loss: 0.00003890
Iteration 142/1000 | Loss: 0.00003890
Iteration 143/1000 | Loss: 0.00003890
Iteration 144/1000 | Loss: 0.00003890
Iteration 145/1000 | Loss: 0.00003890
Iteration 146/1000 | Loss: 0.00003890
Iteration 147/1000 | Loss: 0.00003890
Iteration 148/1000 | Loss: 0.00003889
Iteration 149/1000 | Loss: 0.00003889
Iteration 150/1000 | Loss: 0.00003889
Iteration 151/1000 | Loss: 0.00003886
Iteration 152/1000 | Loss: 0.00003885
Iteration 153/1000 | Loss: 0.00003885
Iteration 154/1000 | Loss: 0.00003885
Iteration 155/1000 | Loss: 0.00003880
Iteration 156/1000 | Loss: 0.00003880
Iteration 157/1000 | Loss: 0.00003879
Iteration 158/1000 | Loss: 0.00003879
Iteration 159/1000 | Loss: 0.00003879
Iteration 160/1000 | Loss: 0.00003878
Iteration 161/1000 | Loss: 0.00003878
Iteration 162/1000 | Loss: 0.00003877
Iteration 163/1000 | Loss: 0.00003877
Iteration 164/1000 | Loss: 0.00003877
Iteration 165/1000 | Loss: 0.00003877
Iteration 166/1000 | Loss: 0.00003877
Iteration 167/1000 | Loss: 0.00003877
Iteration 168/1000 | Loss: 0.00003877
Iteration 169/1000 | Loss: 0.00003877
Iteration 170/1000 | Loss: 0.00003877
Iteration 171/1000 | Loss: 0.00003876
Iteration 172/1000 | Loss: 0.00003876
Iteration 173/1000 | Loss: 0.00003876
Iteration 174/1000 | Loss: 0.00003876
Iteration 175/1000 | Loss: 0.00003876
Iteration 176/1000 | Loss: 0.00003876
Iteration 177/1000 | Loss: 0.00003875
Iteration 178/1000 | Loss: 0.00003875
Iteration 179/1000 | Loss: 0.00003874
Iteration 180/1000 | Loss: 0.00003873
Iteration 181/1000 | Loss: 0.00003873
Iteration 182/1000 | Loss: 0.00003873
Iteration 183/1000 | Loss: 0.00003873
Iteration 184/1000 | Loss: 0.00003873
Iteration 185/1000 | Loss: 0.00003873
Iteration 186/1000 | Loss: 0.00003873
Iteration 187/1000 | Loss: 0.00003873
Iteration 188/1000 | Loss: 0.00003873
Iteration 189/1000 | Loss: 0.00003873
Iteration 190/1000 | Loss: 0.00003873
Iteration 191/1000 | Loss: 0.00003873
Iteration 192/1000 | Loss: 0.00003872
Iteration 193/1000 | Loss: 0.00003872
Iteration 194/1000 | Loss: 0.00003872
Iteration 195/1000 | Loss: 0.00003872
Iteration 196/1000 | Loss: 0.00003872
Iteration 197/1000 | Loss: 0.00003872
Iteration 198/1000 | Loss: 0.00003872
Iteration 199/1000 | Loss: 0.00003872
Iteration 200/1000 | Loss: 0.00003872
Iteration 201/1000 | Loss: 0.00003872
Iteration 202/1000 | Loss: 0.00003872
Iteration 203/1000 | Loss: 0.00003871
Iteration 204/1000 | Loss: 0.00003871
Iteration 205/1000 | Loss: 0.00003871
Iteration 206/1000 | Loss: 0.00003871
Iteration 207/1000 | Loss: 0.00003871
Iteration 208/1000 | Loss: 0.00003871
Iteration 209/1000 | Loss: 0.00003871
Iteration 210/1000 | Loss: 0.00003870
Iteration 211/1000 | Loss: 0.00003870
Iteration 212/1000 | Loss: 0.00003870
Iteration 213/1000 | Loss: 0.00003870
Iteration 214/1000 | Loss: 0.00003870
Iteration 215/1000 | Loss: 0.00003870
Iteration 216/1000 | Loss: 0.00003870
Iteration 217/1000 | Loss: 0.00003870
Iteration 218/1000 | Loss: 0.00003870
Iteration 219/1000 | Loss: 0.00003870
Iteration 220/1000 | Loss: 0.00003870
Iteration 221/1000 | Loss: 0.00003869
Iteration 222/1000 | Loss: 0.00003869
Iteration 223/1000 | Loss: 0.00003869
Iteration 224/1000 | Loss: 0.00003869
Iteration 225/1000 | Loss: 0.00003869
Iteration 226/1000 | Loss: 0.00003869
Iteration 227/1000 | Loss: 0.00003869
Iteration 228/1000 | Loss: 0.00003869
Iteration 229/1000 | Loss: 0.00003869
Iteration 230/1000 | Loss: 0.00003869
Iteration 231/1000 | Loss: 0.00003869
Iteration 232/1000 | Loss: 0.00003869
Iteration 233/1000 | Loss: 0.00003869
Iteration 234/1000 | Loss: 0.00003869
Iteration 235/1000 | Loss: 0.00003869
Iteration 236/1000 | Loss: 0.00003869
Iteration 237/1000 | Loss: 0.00003869
Iteration 238/1000 | Loss: 0.00003869
Iteration 239/1000 | Loss: 0.00003869
Iteration 240/1000 | Loss: 0.00003869
Iteration 241/1000 | Loss: 0.00003869
Iteration 242/1000 | Loss: 0.00003869
Iteration 243/1000 | Loss: 0.00003869
Iteration 244/1000 | Loss: 0.00003869
Iteration 245/1000 | Loss: 0.00003869
Iteration 246/1000 | Loss: 0.00003869
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [3.868573912768625e-05, 3.868573912768625e-05, 3.868573912768625e-05, 3.868573912768625e-05, 3.868573912768625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.868573912768625e-05

Optimization complete. Final v2v error: 4.0077619552612305 mm

Highest mean error: 10.744410514831543 mm for frame 199

Lowest mean error: 3.2486014366149902 mm for frame 97

Saving results

Total time: 259.04657649993896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00569998
Iteration 2/25 | Loss: 0.00119238
Iteration 3/25 | Loss: 0.00112955
Iteration 4/25 | Loss: 0.00112108
Iteration 5/25 | Loss: 0.00111803
Iteration 6/25 | Loss: 0.00111750
Iteration 7/25 | Loss: 0.00111750
Iteration 8/25 | Loss: 0.00111750
Iteration 9/25 | Loss: 0.00111750
Iteration 10/25 | Loss: 0.00111750
Iteration 11/25 | Loss: 0.00111750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011175047839060426, 0.0011175047839060426, 0.0011175047839060426, 0.0011175047839060426, 0.0011175047839060426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011175047839060426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.84667277
Iteration 2/25 | Loss: 0.00083510
Iteration 3/25 | Loss: 0.00083510
Iteration 4/25 | Loss: 0.00083510
Iteration 5/25 | Loss: 0.00083510
Iteration 6/25 | Loss: 0.00083510
Iteration 7/25 | Loss: 0.00083510
Iteration 8/25 | Loss: 0.00083510
Iteration 9/25 | Loss: 0.00083509
Iteration 10/25 | Loss: 0.00083509
Iteration 11/25 | Loss: 0.00083509
Iteration 12/25 | Loss: 0.00083509
Iteration 13/25 | Loss: 0.00083509
Iteration 14/25 | Loss: 0.00083509
Iteration 15/25 | Loss: 0.00083509
Iteration 16/25 | Loss: 0.00083509
Iteration 17/25 | Loss: 0.00083509
Iteration 18/25 | Loss: 0.00083509
Iteration 19/25 | Loss: 0.00083509
Iteration 20/25 | Loss: 0.00083509
Iteration 21/25 | Loss: 0.00083509
Iteration 22/25 | Loss: 0.00083509
Iteration 23/25 | Loss: 0.00083509
Iteration 24/25 | Loss: 0.00083509
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008350934949703515, 0.0008350934949703515, 0.0008350934949703515, 0.0008350934949703515, 0.0008350934949703515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008350934949703515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083509
Iteration 2/1000 | Loss: 0.00001860
Iteration 3/1000 | Loss: 0.00001336
Iteration 4/1000 | Loss: 0.00001198
Iteration 5/1000 | Loss: 0.00001131
Iteration 6/1000 | Loss: 0.00001085
Iteration 7/1000 | Loss: 0.00001061
Iteration 8/1000 | Loss: 0.00001036
Iteration 9/1000 | Loss: 0.00001018
Iteration 10/1000 | Loss: 0.00001016
Iteration 11/1000 | Loss: 0.00001014
Iteration 12/1000 | Loss: 0.00001000
Iteration 13/1000 | Loss: 0.00000987
Iteration 14/1000 | Loss: 0.00000984
Iteration 15/1000 | Loss: 0.00000977
Iteration 16/1000 | Loss: 0.00000977
Iteration 17/1000 | Loss: 0.00000975
Iteration 18/1000 | Loss: 0.00000975
Iteration 19/1000 | Loss: 0.00000974
Iteration 20/1000 | Loss: 0.00000974
Iteration 21/1000 | Loss: 0.00000972
Iteration 22/1000 | Loss: 0.00000969
Iteration 23/1000 | Loss: 0.00000968
Iteration 24/1000 | Loss: 0.00000967
Iteration 25/1000 | Loss: 0.00000966
Iteration 26/1000 | Loss: 0.00000966
Iteration 27/1000 | Loss: 0.00000966
Iteration 28/1000 | Loss: 0.00000965
Iteration 29/1000 | Loss: 0.00000965
Iteration 30/1000 | Loss: 0.00000965
Iteration 31/1000 | Loss: 0.00000964
Iteration 32/1000 | Loss: 0.00000964
Iteration 33/1000 | Loss: 0.00000964
Iteration 34/1000 | Loss: 0.00000963
Iteration 35/1000 | Loss: 0.00000963
Iteration 36/1000 | Loss: 0.00000963
Iteration 37/1000 | Loss: 0.00000963
Iteration 38/1000 | Loss: 0.00000963
Iteration 39/1000 | Loss: 0.00000963
Iteration 40/1000 | Loss: 0.00000963
Iteration 41/1000 | Loss: 0.00000962
Iteration 42/1000 | Loss: 0.00000962
Iteration 43/1000 | Loss: 0.00000962
Iteration 44/1000 | Loss: 0.00000962
Iteration 45/1000 | Loss: 0.00000961
Iteration 46/1000 | Loss: 0.00000961
Iteration 47/1000 | Loss: 0.00000960
Iteration 48/1000 | Loss: 0.00000960
Iteration 49/1000 | Loss: 0.00000959
Iteration 50/1000 | Loss: 0.00000959
Iteration 51/1000 | Loss: 0.00000959
Iteration 52/1000 | Loss: 0.00000958
Iteration 53/1000 | Loss: 0.00000956
Iteration 54/1000 | Loss: 0.00000956
Iteration 55/1000 | Loss: 0.00000956
Iteration 56/1000 | Loss: 0.00000956
Iteration 57/1000 | Loss: 0.00000955
Iteration 58/1000 | Loss: 0.00000953
Iteration 59/1000 | Loss: 0.00000953
Iteration 60/1000 | Loss: 0.00000952
Iteration 61/1000 | Loss: 0.00000951
Iteration 62/1000 | Loss: 0.00000950
Iteration 63/1000 | Loss: 0.00000950
Iteration 64/1000 | Loss: 0.00000949
Iteration 65/1000 | Loss: 0.00000949
Iteration 66/1000 | Loss: 0.00000949
Iteration 67/1000 | Loss: 0.00000948
Iteration 68/1000 | Loss: 0.00000948
Iteration 69/1000 | Loss: 0.00000948
Iteration 70/1000 | Loss: 0.00000948
Iteration 71/1000 | Loss: 0.00000947
Iteration 72/1000 | Loss: 0.00000947
Iteration 73/1000 | Loss: 0.00000947
Iteration 74/1000 | Loss: 0.00000947
Iteration 75/1000 | Loss: 0.00000947
Iteration 76/1000 | Loss: 0.00000946
Iteration 77/1000 | Loss: 0.00000946
Iteration 78/1000 | Loss: 0.00000946
Iteration 79/1000 | Loss: 0.00000945
Iteration 80/1000 | Loss: 0.00000945
Iteration 81/1000 | Loss: 0.00000944
Iteration 82/1000 | Loss: 0.00000944
Iteration 83/1000 | Loss: 0.00000944
Iteration 84/1000 | Loss: 0.00000944
Iteration 85/1000 | Loss: 0.00000943
Iteration 86/1000 | Loss: 0.00000943
Iteration 87/1000 | Loss: 0.00000943
Iteration 88/1000 | Loss: 0.00000943
Iteration 89/1000 | Loss: 0.00000943
Iteration 90/1000 | Loss: 0.00000943
Iteration 91/1000 | Loss: 0.00000943
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000942
Iteration 95/1000 | Loss: 0.00000942
Iteration 96/1000 | Loss: 0.00000942
Iteration 97/1000 | Loss: 0.00000942
Iteration 98/1000 | Loss: 0.00000942
Iteration 99/1000 | Loss: 0.00000942
Iteration 100/1000 | Loss: 0.00000942
Iteration 101/1000 | Loss: 0.00000942
Iteration 102/1000 | Loss: 0.00000942
Iteration 103/1000 | Loss: 0.00000942
Iteration 104/1000 | Loss: 0.00000942
Iteration 105/1000 | Loss: 0.00000942
Iteration 106/1000 | Loss: 0.00000941
Iteration 107/1000 | Loss: 0.00000941
Iteration 108/1000 | Loss: 0.00000941
Iteration 109/1000 | Loss: 0.00000941
Iteration 110/1000 | Loss: 0.00000941
Iteration 111/1000 | Loss: 0.00000941
Iteration 112/1000 | Loss: 0.00000940
Iteration 113/1000 | Loss: 0.00000940
Iteration 114/1000 | Loss: 0.00000940
Iteration 115/1000 | Loss: 0.00000940
Iteration 116/1000 | Loss: 0.00000940
Iteration 117/1000 | Loss: 0.00000940
Iteration 118/1000 | Loss: 0.00000940
Iteration 119/1000 | Loss: 0.00000940
Iteration 120/1000 | Loss: 0.00000939
Iteration 121/1000 | Loss: 0.00000939
Iteration 122/1000 | Loss: 0.00000939
Iteration 123/1000 | Loss: 0.00000939
Iteration 124/1000 | Loss: 0.00000939
Iteration 125/1000 | Loss: 0.00000939
Iteration 126/1000 | Loss: 0.00000939
Iteration 127/1000 | Loss: 0.00000939
Iteration 128/1000 | Loss: 0.00000939
Iteration 129/1000 | Loss: 0.00000939
Iteration 130/1000 | Loss: 0.00000939
Iteration 131/1000 | Loss: 0.00000938
Iteration 132/1000 | Loss: 0.00000938
Iteration 133/1000 | Loss: 0.00000938
Iteration 134/1000 | Loss: 0.00000937
Iteration 135/1000 | Loss: 0.00000937
Iteration 136/1000 | Loss: 0.00000937
Iteration 137/1000 | Loss: 0.00000937
Iteration 138/1000 | Loss: 0.00000937
Iteration 139/1000 | Loss: 0.00000937
Iteration 140/1000 | Loss: 0.00000937
Iteration 141/1000 | Loss: 0.00000937
Iteration 142/1000 | Loss: 0.00000937
Iteration 143/1000 | Loss: 0.00000936
Iteration 144/1000 | Loss: 0.00000936
Iteration 145/1000 | Loss: 0.00000936
Iteration 146/1000 | Loss: 0.00000935
Iteration 147/1000 | Loss: 0.00000935
Iteration 148/1000 | Loss: 0.00000935
Iteration 149/1000 | Loss: 0.00000935
Iteration 150/1000 | Loss: 0.00000934
Iteration 151/1000 | Loss: 0.00000934
Iteration 152/1000 | Loss: 0.00000934
Iteration 153/1000 | Loss: 0.00000934
Iteration 154/1000 | Loss: 0.00000934
Iteration 155/1000 | Loss: 0.00000934
Iteration 156/1000 | Loss: 0.00000934
Iteration 157/1000 | Loss: 0.00000934
Iteration 158/1000 | Loss: 0.00000934
Iteration 159/1000 | Loss: 0.00000933
Iteration 160/1000 | Loss: 0.00000933
Iteration 161/1000 | Loss: 0.00000933
Iteration 162/1000 | Loss: 0.00000933
Iteration 163/1000 | Loss: 0.00000933
Iteration 164/1000 | Loss: 0.00000933
Iteration 165/1000 | Loss: 0.00000933
Iteration 166/1000 | Loss: 0.00000933
Iteration 167/1000 | Loss: 0.00000933
Iteration 168/1000 | Loss: 0.00000933
Iteration 169/1000 | Loss: 0.00000933
Iteration 170/1000 | Loss: 0.00000933
Iteration 171/1000 | Loss: 0.00000933
Iteration 172/1000 | Loss: 0.00000933
Iteration 173/1000 | Loss: 0.00000932
Iteration 174/1000 | Loss: 0.00000932
Iteration 175/1000 | Loss: 0.00000932
Iteration 176/1000 | Loss: 0.00000932
Iteration 177/1000 | Loss: 0.00000932
Iteration 178/1000 | Loss: 0.00000932
Iteration 179/1000 | Loss: 0.00000932
Iteration 180/1000 | Loss: 0.00000932
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [9.323514859715942e-06, 9.323514859715942e-06, 9.323514859715942e-06, 9.323514859715942e-06, 9.323514859715942e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.323514859715942e-06

Optimization complete. Final v2v error: 2.6495954990386963 mm

Highest mean error: 2.95288348197937 mm for frame 109

Lowest mean error: 2.5003879070281982 mm for frame 172

Saving results

Total time: 38.94191098213196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423693
Iteration 2/25 | Loss: 0.00124031
Iteration 3/25 | Loss: 0.00116112
Iteration 4/25 | Loss: 0.00114618
Iteration 5/25 | Loss: 0.00114197
Iteration 6/25 | Loss: 0.00114125
Iteration 7/25 | Loss: 0.00114125
Iteration 8/25 | Loss: 0.00114125
Iteration 9/25 | Loss: 0.00114125
Iteration 10/25 | Loss: 0.00114125
Iteration 11/25 | Loss: 0.00114125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011412540916353464, 0.0011412540916353464, 0.0011412540916353464, 0.0011412540916353464, 0.0011412540916353464]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011412540916353464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36364949
Iteration 2/25 | Loss: 0.00079755
Iteration 3/25 | Loss: 0.00079755
Iteration 4/25 | Loss: 0.00079755
Iteration 5/25 | Loss: 0.00079755
Iteration 6/25 | Loss: 0.00079755
Iteration 7/25 | Loss: 0.00079755
Iteration 8/25 | Loss: 0.00079755
Iteration 9/25 | Loss: 0.00079755
Iteration 10/25 | Loss: 0.00079755
Iteration 11/25 | Loss: 0.00079755
Iteration 12/25 | Loss: 0.00079755
Iteration 13/25 | Loss: 0.00079755
Iteration 14/25 | Loss: 0.00079755
Iteration 15/25 | Loss: 0.00079755
Iteration 16/25 | Loss: 0.00079755
Iteration 17/25 | Loss: 0.00079755
Iteration 18/25 | Loss: 0.00079755
Iteration 19/25 | Loss: 0.00079755
Iteration 20/25 | Loss: 0.00079755
Iteration 21/25 | Loss: 0.00079755
Iteration 22/25 | Loss: 0.00079755
Iteration 23/25 | Loss: 0.00079755
Iteration 24/25 | Loss: 0.00079755
Iteration 25/25 | Loss: 0.00079755

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079755
Iteration 2/1000 | Loss: 0.00002671
Iteration 3/1000 | Loss: 0.00001815
Iteration 4/1000 | Loss: 0.00001548
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001383
Iteration 8/1000 | Loss: 0.00001358
Iteration 9/1000 | Loss: 0.00001333
Iteration 10/1000 | Loss: 0.00001311
Iteration 11/1000 | Loss: 0.00001304
Iteration 12/1000 | Loss: 0.00001296
Iteration 13/1000 | Loss: 0.00001291
Iteration 14/1000 | Loss: 0.00001290
Iteration 15/1000 | Loss: 0.00001283
Iteration 16/1000 | Loss: 0.00001282
Iteration 17/1000 | Loss: 0.00001278
Iteration 18/1000 | Loss: 0.00001278
Iteration 19/1000 | Loss: 0.00001275
Iteration 20/1000 | Loss: 0.00001270
Iteration 21/1000 | Loss: 0.00001270
Iteration 22/1000 | Loss: 0.00001269
Iteration 23/1000 | Loss: 0.00001269
Iteration 24/1000 | Loss: 0.00001268
Iteration 25/1000 | Loss: 0.00001264
Iteration 26/1000 | Loss: 0.00001264
Iteration 27/1000 | Loss: 0.00001262
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001257
Iteration 30/1000 | Loss: 0.00001257
Iteration 31/1000 | Loss: 0.00001257
Iteration 32/1000 | Loss: 0.00001257
Iteration 33/1000 | Loss: 0.00001256
Iteration 34/1000 | Loss: 0.00001256
Iteration 35/1000 | Loss: 0.00001254
Iteration 36/1000 | Loss: 0.00001254
Iteration 37/1000 | Loss: 0.00001253
Iteration 38/1000 | Loss: 0.00001253
Iteration 39/1000 | Loss: 0.00001253
Iteration 40/1000 | Loss: 0.00001253
Iteration 41/1000 | Loss: 0.00001252
Iteration 42/1000 | Loss: 0.00001252
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001250
Iteration 46/1000 | Loss: 0.00001250
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001249
Iteration 52/1000 | Loss: 0.00001249
Iteration 53/1000 | Loss: 0.00001249
Iteration 54/1000 | Loss: 0.00001249
Iteration 55/1000 | Loss: 0.00001249
Iteration 56/1000 | Loss: 0.00001249
Iteration 57/1000 | Loss: 0.00001249
Iteration 58/1000 | Loss: 0.00001247
Iteration 59/1000 | Loss: 0.00001244
Iteration 60/1000 | Loss: 0.00001244
Iteration 61/1000 | Loss: 0.00001244
Iteration 62/1000 | Loss: 0.00001244
Iteration 63/1000 | Loss: 0.00001244
Iteration 64/1000 | Loss: 0.00001243
Iteration 65/1000 | Loss: 0.00001243
Iteration 66/1000 | Loss: 0.00001243
Iteration 67/1000 | Loss: 0.00001243
Iteration 68/1000 | Loss: 0.00001243
Iteration 69/1000 | Loss: 0.00001243
Iteration 70/1000 | Loss: 0.00001243
Iteration 71/1000 | Loss: 0.00001241
Iteration 72/1000 | Loss: 0.00001241
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001238
Iteration 76/1000 | Loss: 0.00001238
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001237
Iteration 79/1000 | Loss: 0.00001237
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001235
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001235
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001234
Iteration 90/1000 | Loss: 0.00001234
Iteration 91/1000 | Loss: 0.00001234
Iteration 92/1000 | Loss: 0.00001234
Iteration 93/1000 | Loss: 0.00001234
Iteration 94/1000 | Loss: 0.00001234
Iteration 95/1000 | Loss: 0.00001234
Iteration 96/1000 | Loss: 0.00001234
Iteration 97/1000 | Loss: 0.00001233
Iteration 98/1000 | Loss: 0.00001233
Iteration 99/1000 | Loss: 0.00001233
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001232
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001231
Iteration 105/1000 | Loss: 0.00001231
Iteration 106/1000 | Loss: 0.00001231
Iteration 107/1000 | Loss: 0.00001231
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001230
Iteration 112/1000 | Loss: 0.00001230
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001229
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001228
Iteration 127/1000 | Loss: 0.00001228
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001227
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001225
Iteration 135/1000 | Loss: 0.00001225
Iteration 136/1000 | Loss: 0.00001225
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001225
Iteration 141/1000 | Loss: 0.00001225
Iteration 142/1000 | Loss: 0.00001225
Iteration 143/1000 | Loss: 0.00001225
Iteration 144/1000 | Loss: 0.00001225
Iteration 145/1000 | Loss: 0.00001225
Iteration 146/1000 | Loss: 0.00001225
Iteration 147/1000 | Loss: 0.00001224
Iteration 148/1000 | Loss: 0.00001224
Iteration 149/1000 | Loss: 0.00001224
Iteration 150/1000 | Loss: 0.00001224
Iteration 151/1000 | Loss: 0.00001224
Iteration 152/1000 | Loss: 0.00001223
Iteration 153/1000 | Loss: 0.00001223
Iteration 154/1000 | Loss: 0.00001223
Iteration 155/1000 | Loss: 0.00001223
Iteration 156/1000 | Loss: 0.00001223
Iteration 157/1000 | Loss: 0.00001223
Iteration 158/1000 | Loss: 0.00001223
Iteration 159/1000 | Loss: 0.00001223
Iteration 160/1000 | Loss: 0.00001223
Iteration 161/1000 | Loss: 0.00001223
Iteration 162/1000 | Loss: 0.00001223
Iteration 163/1000 | Loss: 0.00001223
Iteration 164/1000 | Loss: 0.00001223
Iteration 165/1000 | Loss: 0.00001223
Iteration 166/1000 | Loss: 0.00001223
Iteration 167/1000 | Loss: 0.00001223
Iteration 168/1000 | Loss: 0.00001223
Iteration 169/1000 | Loss: 0.00001223
Iteration 170/1000 | Loss: 0.00001223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.223182425746927e-05, 1.223182425746927e-05, 1.223182425746927e-05, 1.223182425746927e-05, 1.223182425746927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.223182425746927e-05

Optimization complete. Final v2v error: 3.0234832763671875 mm

Highest mean error: 3.2343904972076416 mm for frame 106

Lowest mean error: 2.8704826831817627 mm for frame 156

Saving results

Total time: 39.16710376739502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00950617
Iteration 2/25 | Loss: 0.00133035
Iteration 3/25 | Loss: 0.00120779
Iteration 4/25 | Loss: 0.00119225
Iteration 5/25 | Loss: 0.00118754
Iteration 6/25 | Loss: 0.00118682
Iteration 7/25 | Loss: 0.00118682
Iteration 8/25 | Loss: 0.00118682
Iteration 9/25 | Loss: 0.00118682
Iteration 10/25 | Loss: 0.00118682
Iteration 11/25 | Loss: 0.00118682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011868190485984087, 0.0011868190485984087, 0.0011868190485984087, 0.0011868190485984087, 0.0011868190485984087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011868190485984087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66001153
Iteration 2/25 | Loss: 0.00081256
Iteration 3/25 | Loss: 0.00081256
Iteration 4/25 | Loss: 0.00081256
Iteration 5/25 | Loss: 0.00081256
Iteration 6/25 | Loss: 0.00081256
Iteration 7/25 | Loss: 0.00081256
Iteration 8/25 | Loss: 0.00081256
Iteration 9/25 | Loss: 0.00081256
Iteration 10/25 | Loss: 0.00081256
Iteration 11/25 | Loss: 0.00081256
Iteration 12/25 | Loss: 0.00081256
Iteration 13/25 | Loss: 0.00081256
Iteration 14/25 | Loss: 0.00081256
Iteration 15/25 | Loss: 0.00081256
Iteration 16/25 | Loss: 0.00081256
Iteration 17/25 | Loss: 0.00081256
Iteration 18/25 | Loss: 0.00081256
Iteration 19/25 | Loss: 0.00081256
Iteration 20/25 | Loss: 0.00081256
Iteration 21/25 | Loss: 0.00081256
Iteration 22/25 | Loss: 0.00081256
Iteration 23/25 | Loss: 0.00081256
Iteration 24/25 | Loss: 0.00081256
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008125556050799787, 0.0008125556050799787, 0.0008125556050799787, 0.0008125556050799787, 0.0008125556050799787]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008125556050799787

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081256
Iteration 2/1000 | Loss: 0.00002445
Iteration 3/1000 | Loss: 0.00001699
Iteration 4/1000 | Loss: 0.00001567
Iteration 5/1000 | Loss: 0.00001500
Iteration 6/1000 | Loss: 0.00001493
Iteration 7/1000 | Loss: 0.00001467
Iteration 8/1000 | Loss: 0.00001439
Iteration 9/1000 | Loss: 0.00001418
Iteration 10/1000 | Loss: 0.00001405
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001395
Iteration 13/1000 | Loss: 0.00001391
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001390
Iteration 16/1000 | Loss: 0.00001389
Iteration 17/1000 | Loss: 0.00001389
Iteration 18/1000 | Loss: 0.00001384
Iteration 19/1000 | Loss: 0.00001383
Iteration 20/1000 | Loss: 0.00001383
Iteration 21/1000 | Loss: 0.00001382
Iteration 22/1000 | Loss: 0.00001381
Iteration 23/1000 | Loss: 0.00001379
Iteration 24/1000 | Loss: 0.00001378
Iteration 25/1000 | Loss: 0.00001377
Iteration 26/1000 | Loss: 0.00001377
Iteration 27/1000 | Loss: 0.00001377
Iteration 28/1000 | Loss: 0.00001376
Iteration 29/1000 | Loss: 0.00001376
Iteration 30/1000 | Loss: 0.00001372
Iteration 31/1000 | Loss: 0.00001369
Iteration 32/1000 | Loss: 0.00001368
Iteration 33/1000 | Loss: 0.00001368
Iteration 34/1000 | Loss: 0.00001367
Iteration 35/1000 | Loss: 0.00001366
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001364
Iteration 38/1000 | Loss: 0.00001364
Iteration 39/1000 | Loss: 0.00001363
Iteration 40/1000 | Loss: 0.00001362
Iteration 41/1000 | Loss: 0.00001360
Iteration 42/1000 | Loss: 0.00001360
Iteration 43/1000 | Loss: 0.00001358
Iteration 44/1000 | Loss: 0.00001357
Iteration 45/1000 | Loss: 0.00001357
Iteration 46/1000 | Loss: 0.00001357
Iteration 47/1000 | Loss: 0.00001356
Iteration 48/1000 | Loss: 0.00001356
Iteration 49/1000 | Loss: 0.00001355
Iteration 50/1000 | Loss: 0.00001355
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001353
Iteration 53/1000 | Loss: 0.00001353
Iteration 54/1000 | Loss: 0.00001353
Iteration 55/1000 | Loss: 0.00001353
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001353
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001352
Iteration 61/1000 | Loss: 0.00001352
Iteration 62/1000 | Loss: 0.00001352
Iteration 63/1000 | Loss: 0.00001351
Iteration 64/1000 | Loss: 0.00001351
Iteration 65/1000 | Loss: 0.00001350
Iteration 66/1000 | Loss: 0.00001350
Iteration 67/1000 | Loss: 0.00001350
Iteration 68/1000 | Loss: 0.00001350
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001349
Iteration 71/1000 | Loss: 0.00001349
Iteration 72/1000 | Loss: 0.00001348
Iteration 73/1000 | Loss: 0.00001348
Iteration 74/1000 | Loss: 0.00001348
Iteration 75/1000 | Loss: 0.00001348
Iteration 76/1000 | Loss: 0.00001348
Iteration 77/1000 | Loss: 0.00001348
Iteration 78/1000 | Loss: 0.00001347
Iteration 79/1000 | Loss: 0.00001347
Iteration 80/1000 | Loss: 0.00001347
Iteration 81/1000 | Loss: 0.00001347
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001346
Iteration 86/1000 | Loss: 0.00001345
Iteration 87/1000 | Loss: 0.00001345
Iteration 88/1000 | Loss: 0.00001345
Iteration 89/1000 | Loss: 0.00001345
Iteration 90/1000 | Loss: 0.00001345
Iteration 91/1000 | Loss: 0.00001345
Iteration 92/1000 | Loss: 0.00001345
Iteration 93/1000 | Loss: 0.00001345
Iteration 94/1000 | Loss: 0.00001344
Iteration 95/1000 | Loss: 0.00001344
Iteration 96/1000 | Loss: 0.00001344
Iteration 97/1000 | Loss: 0.00001344
Iteration 98/1000 | Loss: 0.00001343
Iteration 99/1000 | Loss: 0.00001343
Iteration 100/1000 | Loss: 0.00001343
Iteration 101/1000 | Loss: 0.00001343
Iteration 102/1000 | Loss: 0.00001342
Iteration 103/1000 | Loss: 0.00001342
Iteration 104/1000 | Loss: 0.00001342
Iteration 105/1000 | Loss: 0.00001342
Iteration 106/1000 | Loss: 0.00001341
Iteration 107/1000 | Loss: 0.00001341
Iteration 108/1000 | Loss: 0.00001340
Iteration 109/1000 | Loss: 0.00001340
Iteration 110/1000 | Loss: 0.00001340
Iteration 111/1000 | Loss: 0.00001340
Iteration 112/1000 | Loss: 0.00001340
Iteration 113/1000 | Loss: 0.00001340
Iteration 114/1000 | Loss: 0.00001339
Iteration 115/1000 | Loss: 0.00001339
Iteration 116/1000 | Loss: 0.00001339
Iteration 117/1000 | Loss: 0.00001339
Iteration 118/1000 | Loss: 0.00001339
Iteration 119/1000 | Loss: 0.00001339
Iteration 120/1000 | Loss: 0.00001339
Iteration 121/1000 | Loss: 0.00001339
Iteration 122/1000 | Loss: 0.00001339
Iteration 123/1000 | Loss: 0.00001339
Iteration 124/1000 | Loss: 0.00001339
Iteration 125/1000 | Loss: 0.00001339
Iteration 126/1000 | Loss: 0.00001339
Iteration 127/1000 | Loss: 0.00001339
Iteration 128/1000 | Loss: 0.00001339
Iteration 129/1000 | Loss: 0.00001339
Iteration 130/1000 | Loss: 0.00001339
Iteration 131/1000 | Loss: 0.00001339
Iteration 132/1000 | Loss: 0.00001339
Iteration 133/1000 | Loss: 0.00001339
Iteration 134/1000 | Loss: 0.00001339
Iteration 135/1000 | Loss: 0.00001339
Iteration 136/1000 | Loss: 0.00001339
Iteration 137/1000 | Loss: 0.00001339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.33892481244402e-05, 1.33892481244402e-05, 1.33892481244402e-05, 1.33892481244402e-05, 1.33892481244402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.33892481244402e-05

Optimization complete. Final v2v error: 3.0999722480773926 mm

Highest mean error: 3.2540128231048584 mm for frame 88

Lowest mean error: 2.9037325382232666 mm for frame 111

Saving results

Total time: 33.71717047691345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00726550
Iteration 2/25 | Loss: 0.00140033
Iteration 3/25 | Loss: 0.00121968
Iteration 4/25 | Loss: 0.00114885
Iteration 5/25 | Loss: 0.00113507
Iteration 6/25 | Loss: 0.00113174
Iteration 7/25 | Loss: 0.00112998
Iteration 8/25 | Loss: 0.00112933
Iteration 9/25 | Loss: 0.00112871
Iteration 10/25 | Loss: 0.00112826
Iteration 11/25 | Loss: 0.00112812
Iteration 12/25 | Loss: 0.00112812
Iteration 13/25 | Loss: 0.00112812
Iteration 14/25 | Loss: 0.00112811
Iteration 15/25 | Loss: 0.00112811
Iteration 16/25 | Loss: 0.00112811
Iteration 17/25 | Loss: 0.00112811
Iteration 18/25 | Loss: 0.00112811
Iteration 19/25 | Loss: 0.00112811
Iteration 20/25 | Loss: 0.00112811
Iteration 21/25 | Loss: 0.00112811
Iteration 22/25 | Loss: 0.00112810
Iteration 23/25 | Loss: 0.00112810
Iteration 24/25 | Loss: 0.00112810
Iteration 25/25 | Loss: 0.00112810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.71049809
Iteration 2/25 | Loss: 0.00118681
Iteration 3/25 | Loss: 0.00118677
Iteration 4/25 | Loss: 0.00118677
Iteration 5/25 | Loss: 0.00118677
Iteration 6/25 | Loss: 0.00118677
Iteration 7/25 | Loss: 0.00118677
Iteration 8/25 | Loss: 0.00118677
Iteration 9/25 | Loss: 0.00118677
Iteration 10/25 | Loss: 0.00118677
Iteration 11/25 | Loss: 0.00118677
Iteration 12/25 | Loss: 0.00118677
Iteration 13/25 | Loss: 0.00118677
Iteration 14/25 | Loss: 0.00118677
Iteration 15/25 | Loss: 0.00118677
Iteration 16/25 | Loss: 0.00118677
Iteration 17/25 | Loss: 0.00118677
Iteration 18/25 | Loss: 0.00118677
Iteration 19/25 | Loss: 0.00118677
Iteration 20/25 | Loss: 0.00118677
Iteration 21/25 | Loss: 0.00118677
Iteration 22/25 | Loss: 0.00118677
Iteration 23/25 | Loss: 0.00118677
Iteration 24/25 | Loss: 0.00118677
Iteration 25/25 | Loss: 0.00118677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118677
Iteration 2/1000 | Loss: 0.00003701
Iteration 3/1000 | Loss: 0.00002447
Iteration 4/1000 | Loss: 0.00001798
Iteration 5/1000 | Loss: 0.00001684
Iteration 6/1000 | Loss: 0.00001589
Iteration 7/1000 | Loss: 0.00001513
Iteration 8/1000 | Loss: 0.00001462
Iteration 9/1000 | Loss: 0.00001434
Iteration 10/1000 | Loss: 0.00001424
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001382
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001359
Iteration 15/1000 | Loss: 0.00001358
Iteration 16/1000 | Loss: 0.00001350
Iteration 17/1000 | Loss: 0.00001342
Iteration 18/1000 | Loss: 0.00001338
Iteration 19/1000 | Loss: 0.00001337
Iteration 20/1000 | Loss: 0.00001337
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001336
Iteration 23/1000 | Loss: 0.00001335
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001334
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001332
Iteration 30/1000 | Loss: 0.00001332
Iteration 31/1000 | Loss: 0.00001332
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001331
Iteration 34/1000 | Loss: 0.00001331
Iteration 35/1000 | Loss: 0.00001331
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001331
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00001331
Iteration 40/1000 | Loss: 0.00001331
Iteration 41/1000 | Loss: 0.00001331
Iteration 42/1000 | Loss: 0.00001331
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001330
Iteration 45/1000 | Loss: 0.00001330
Iteration 46/1000 | Loss: 0.00001330
Iteration 47/1000 | Loss: 0.00001330
Iteration 48/1000 | Loss: 0.00001330
Iteration 49/1000 | Loss: 0.00001330
Iteration 50/1000 | Loss: 0.00001330
Iteration 51/1000 | Loss: 0.00001330
Iteration 52/1000 | Loss: 0.00001330
Iteration 53/1000 | Loss: 0.00001330
Iteration 54/1000 | Loss: 0.00001330
Iteration 55/1000 | Loss: 0.00001330
Iteration 56/1000 | Loss: 0.00001330
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001330
Iteration 59/1000 | Loss: 0.00001330
Iteration 60/1000 | Loss: 0.00001330
Iteration 61/1000 | Loss: 0.00001330
Iteration 62/1000 | Loss: 0.00001330
Iteration 63/1000 | Loss: 0.00001330
Iteration 64/1000 | Loss: 0.00001330
Iteration 65/1000 | Loss: 0.00001330
Iteration 66/1000 | Loss: 0.00001330
Iteration 67/1000 | Loss: 0.00001330
Iteration 68/1000 | Loss: 0.00001330
Iteration 69/1000 | Loss: 0.00001330
Iteration 70/1000 | Loss: 0.00001330
Iteration 71/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.3304833373695146e-05, 1.3304833373695146e-05, 1.3304833373695146e-05, 1.3304833373695146e-05, 1.3304833373695146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3304833373695146e-05

Optimization complete. Final v2v error: 3.123854398727417 mm

Highest mean error: 3.6915929317474365 mm for frame 9

Lowest mean error: 2.8034214973449707 mm for frame 126

Saving results

Total time: 43.09833216667175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034888
Iteration 2/25 | Loss: 0.01034888
Iteration 3/25 | Loss: 0.01034887
Iteration 4/25 | Loss: 0.01034887
Iteration 5/25 | Loss: 0.01034887
Iteration 6/25 | Loss: 0.01034887
Iteration 7/25 | Loss: 0.01034886
Iteration 8/25 | Loss: 0.01034886
Iteration 9/25 | Loss: 0.01034886
Iteration 10/25 | Loss: 0.01034886
Iteration 11/25 | Loss: 0.01034886
Iteration 12/25 | Loss: 0.01034885
Iteration 13/25 | Loss: 0.01034885
Iteration 14/25 | Loss: 0.01034885
Iteration 15/25 | Loss: 0.01034885
Iteration 16/25 | Loss: 0.01034885
Iteration 17/25 | Loss: 0.01034884
Iteration 18/25 | Loss: 0.01034884
Iteration 19/25 | Loss: 0.01034884
Iteration 20/25 | Loss: 0.01034884
Iteration 21/25 | Loss: 0.01034884
Iteration 22/25 | Loss: 0.01034884
Iteration 23/25 | Loss: 0.01034883
Iteration 24/25 | Loss: 0.01034883
Iteration 25/25 | Loss: 0.01034883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63781536
Iteration 2/25 | Loss: 0.08222357
Iteration 3/25 | Loss: 0.08212490
Iteration 4/25 | Loss: 0.08212487
Iteration 5/25 | Loss: 0.08212486
Iteration 6/25 | Loss: 0.08212486
Iteration 7/25 | Loss: 0.08212486
Iteration 8/25 | Loss: 0.08212486
Iteration 9/25 | Loss: 0.08212486
Iteration 10/25 | Loss: 0.08212486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.08212485909461975, 0.08212485909461975, 0.08212485909461975, 0.08212485909461975, 0.08212485909461975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08212485909461975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08212486
Iteration 2/1000 | Loss: 0.00076249
Iteration 3/1000 | Loss: 0.00028978
Iteration 4/1000 | Loss: 0.00017009
Iteration 5/1000 | Loss: 0.00012132
Iteration 6/1000 | Loss: 0.00010705
Iteration 7/1000 | Loss: 0.00016117
Iteration 8/1000 | Loss: 0.00005204
Iteration 9/1000 | Loss: 0.00004298
Iteration 10/1000 | Loss: 0.00004105
Iteration 11/1000 | Loss: 0.00009407
Iteration 12/1000 | Loss: 0.00015747
Iteration 13/1000 | Loss: 0.00009052
Iteration 14/1000 | Loss: 0.00005020
Iteration 15/1000 | Loss: 0.00004582
Iteration 16/1000 | Loss: 0.00002055
Iteration 17/1000 | Loss: 0.00004920
Iteration 18/1000 | Loss: 0.00001909
Iteration 19/1000 | Loss: 0.00005109
Iteration 20/1000 | Loss: 0.00018572
Iteration 21/1000 | Loss: 0.00003207
Iteration 22/1000 | Loss: 0.00004335
Iteration 23/1000 | Loss: 0.00002389
Iteration 24/1000 | Loss: 0.00005614
Iteration 25/1000 | Loss: 0.00010716
Iteration 26/1000 | Loss: 0.00002257
Iteration 27/1000 | Loss: 0.00001652
Iteration 28/1000 | Loss: 0.00001573
Iteration 29/1000 | Loss: 0.00003284
Iteration 30/1000 | Loss: 0.00014248
Iteration 31/1000 | Loss: 0.00004819
Iteration 32/1000 | Loss: 0.00004020
Iteration 33/1000 | Loss: 0.00021671
Iteration 34/1000 | Loss: 0.00012391
Iteration 35/1000 | Loss: 0.00002568
Iteration 36/1000 | Loss: 0.00001375
Iteration 37/1000 | Loss: 0.00001796
Iteration 38/1000 | Loss: 0.00001447
Iteration 39/1000 | Loss: 0.00010822
Iteration 40/1000 | Loss: 0.00019569
Iteration 41/1000 | Loss: 0.00001785
Iteration 42/1000 | Loss: 0.00004290
Iteration 43/1000 | Loss: 0.00002546
Iteration 44/1000 | Loss: 0.00001276
Iteration 45/1000 | Loss: 0.00001261
Iteration 46/1000 | Loss: 0.00001857
Iteration 47/1000 | Loss: 0.00004455
Iteration 48/1000 | Loss: 0.00001230
Iteration 49/1000 | Loss: 0.00001279
Iteration 50/1000 | Loss: 0.00001220
Iteration 51/1000 | Loss: 0.00001220
Iteration 52/1000 | Loss: 0.00001220
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001213
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001209
Iteration 60/1000 | Loss: 0.00001209
Iteration 61/1000 | Loss: 0.00001206
Iteration 62/1000 | Loss: 0.00001205
Iteration 63/1000 | Loss: 0.00001203
Iteration 64/1000 | Loss: 0.00001202
Iteration 65/1000 | Loss: 0.00003492
Iteration 66/1000 | Loss: 0.00002785
Iteration 67/1000 | Loss: 0.00001188
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001716
Iteration 70/1000 | Loss: 0.00001177
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001176
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001174
Iteration 77/1000 | Loss: 0.00001174
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001174
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001173
Iteration 83/1000 | Loss: 0.00001173
Iteration 84/1000 | Loss: 0.00001173
Iteration 85/1000 | Loss: 0.00001173
Iteration 86/1000 | Loss: 0.00001173
Iteration 87/1000 | Loss: 0.00001173
Iteration 88/1000 | Loss: 0.00001173
Iteration 89/1000 | Loss: 0.00001173
Iteration 90/1000 | Loss: 0.00001172
Iteration 91/1000 | Loss: 0.00001172
Iteration 92/1000 | Loss: 0.00001172
Iteration 93/1000 | Loss: 0.00001172
Iteration 94/1000 | Loss: 0.00001172
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001444
Iteration 98/1000 | Loss: 0.00001171
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001171
Iteration 103/1000 | Loss: 0.00001171
Iteration 104/1000 | Loss: 0.00001171
Iteration 105/1000 | Loss: 0.00001171
Iteration 106/1000 | Loss: 0.00001171
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00001169
Iteration 116/1000 | Loss: 0.00001169
Iteration 117/1000 | Loss: 0.00001169
Iteration 118/1000 | Loss: 0.00001169
Iteration 119/1000 | Loss: 0.00001169
Iteration 120/1000 | Loss: 0.00001169
Iteration 121/1000 | Loss: 0.00001169
Iteration 122/1000 | Loss: 0.00001169
Iteration 123/1000 | Loss: 0.00001169
Iteration 124/1000 | Loss: 0.00001169
Iteration 125/1000 | Loss: 0.00001169
Iteration 126/1000 | Loss: 0.00001169
Iteration 127/1000 | Loss: 0.00001169
Iteration 128/1000 | Loss: 0.00001169
Iteration 129/1000 | Loss: 0.00001168
Iteration 130/1000 | Loss: 0.00001168
Iteration 131/1000 | Loss: 0.00001168
Iteration 132/1000 | Loss: 0.00001288
Iteration 133/1000 | Loss: 0.00002001
Iteration 134/1000 | Loss: 0.00001274
Iteration 135/1000 | Loss: 0.00001538
Iteration 136/1000 | Loss: 0.00001200
Iteration 137/1000 | Loss: 0.00001162
Iteration 138/1000 | Loss: 0.00001162
Iteration 139/1000 | Loss: 0.00001162
Iteration 140/1000 | Loss: 0.00001162
Iteration 141/1000 | Loss: 0.00001162
Iteration 142/1000 | Loss: 0.00001162
Iteration 143/1000 | Loss: 0.00001162
Iteration 144/1000 | Loss: 0.00001162
Iteration 145/1000 | Loss: 0.00001162
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001162
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Iteration 154/1000 | Loss: 0.00001162
Iteration 155/1000 | Loss: 0.00001162
Iteration 156/1000 | Loss: 0.00001162
Iteration 157/1000 | Loss: 0.00001162
Iteration 158/1000 | Loss: 0.00001162
Iteration 159/1000 | Loss: 0.00001162
Iteration 160/1000 | Loss: 0.00001162
Iteration 161/1000 | Loss: 0.00001162
Iteration 162/1000 | Loss: 0.00001162
Iteration 163/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.1617061318247579e-05, 1.1617061318247579e-05, 1.1617061318247579e-05, 1.1617061318247579e-05, 1.1617061318247579e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1617061318247579e-05

Optimization complete. Final v2v error: 2.96368670463562 mm

Highest mean error: 3.306063652038574 mm for frame 153

Lowest mean error: 2.6745150089263916 mm for frame 22

Saving results

Total time: 99.00937795639038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_002/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_002/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398884
Iteration 2/25 | Loss: 0.00130066
Iteration 3/25 | Loss: 0.00116421
Iteration 4/25 | Loss: 0.00115510
Iteration 5/25 | Loss: 0.00115467
Iteration 6/25 | Loss: 0.00115467
Iteration 7/25 | Loss: 0.00115467
Iteration 8/25 | Loss: 0.00115467
Iteration 9/25 | Loss: 0.00115467
Iteration 10/25 | Loss: 0.00115467
Iteration 11/25 | Loss: 0.00115467
Iteration 12/25 | Loss: 0.00115467
Iteration 13/25 | Loss: 0.00115467
Iteration 14/25 | Loss: 0.00115467
Iteration 15/25 | Loss: 0.00115467
Iteration 16/25 | Loss: 0.00115467
Iteration 17/25 | Loss: 0.00115467
Iteration 18/25 | Loss: 0.00115467
Iteration 19/25 | Loss: 0.00115467
Iteration 20/25 | Loss: 0.00115467
Iteration 21/25 | Loss: 0.00115467
Iteration 22/25 | Loss: 0.00115467
Iteration 23/25 | Loss: 0.00115467
Iteration 24/25 | Loss: 0.00115467
Iteration 25/25 | Loss: 0.00115467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57244778
Iteration 2/25 | Loss: 0.00069864
Iteration 3/25 | Loss: 0.00069863
Iteration 4/25 | Loss: 0.00069863
Iteration 5/25 | Loss: 0.00069863
Iteration 6/25 | Loss: 0.00069863
Iteration 7/25 | Loss: 0.00069863
Iteration 8/25 | Loss: 0.00069863
Iteration 9/25 | Loss: 0.00069863
Iteration 10/25 | Loss: 0.00069863
Iteration 11/25 | Loss: 0.00069863
Iteration 12/25 | Loss: 0.00069863
Iteration 13/25 | Loss: 0.00069863
Iteration 14/25 | Loss: 0.00069863
Iteration 15/25 | Loss: 0.00069863
Iteration 16/25 | Loss: 0.00069863
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006986323860473931, 0.0006986323860473931, 0.0006986323860473931, 0.0006986323860473931, 0.0006986323860473931]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006986323860473931

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00069863
Iteration 2/1000 | Loss: 0.00002254
Iteration 3/1000 | Loss: 0.00001502
Iteration 4/1000 | Loss: 0.00001352
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001227
Iteration 7/1000 | Loss: 0.00001199
Iteration 8/1000 | Loss: 0.00001160
Iteration 9/1000 | Loss: 0.00001127
Iteration 10/1000 | Loss: 0.00001117
Iteration 11/1000 | Loss: 0.00001117
Iteration 12/1000 | Loss: 0.00001101
Iteration 13/1000 | Loss: 0.00001094
Iteration 14/1000 | Loss: 0.00001094
Iteration 15/1000 | Loss: 0.00001080
Iteration 16/1000 | Loss: 0.00001076
Iteration 17/1000 | Loss: 0.00001076
Iteration 18/1000 | Loss: 0.00001075
Iteration 19/1000 | Loss: 0.00001075
Iteration 20/1000 | Loss: 0.00001074
Iteration 21/1000 | Loss: 0.00001074
Iteration 22/1000 | Loss: 0.00001070
Iteration 23/1000 | Loss: 0.00001070
Iteration 24/1000 | Loss: 0.00001070
Iteration 25/1000 | Loss: 0.00001070
Iteration 26/1000 | Loss: 0.00001070
Iteration 27/1000 | Loss: 0.00001070
Iteration 28/1000 | Loss: 0.00001070
Iteration 29/1000 | Loss: 0.00001070
Iteration 30/1000 | Loss: 0.00001070
Iteration 31/1000 | Loss: 0.00001069
Iteration 32/1000 | Loss: 0.00001069
Iteration 33/1000 | Loss: 0.00001069
Iteration 34/1000 | Loss: 0.00001069
Iteration 35/1000 | Loss: 0.00001069
Iteration 36/1000 | Loss: 0.00001067
Iteration 37/1000 | Loss: 0.00001066
Iteration 38/1000 | Loss: 0.00001066
Iteration 39/1000 | Loss: 0.00001066
Iteration 40/1000 | Loss: 0.00001066
Iteration 41/1000 | Loss: 0.00001066
Iteration 42/1000 | Loss: 0.00001066
Iteration 43/1000 | Loss: 0.00001065
Iteration 44/1000 | Loss: 0.00001065
Iteration 45/1000 | Loss: 0.00001065
Iteration 46/1000 | Loss: 0.00001065
Iteration 47/1000 | Loss: 0.00001065
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001065
Iteration 50/1000 | Loss: 0.00001063
Iteration 51/1000 | Loss: 0.00001063
Iteration 52/1000 | Loss: 0.00001063
Iteration 53/1000 | Loss: 0.00001063
Iteration 54/1000 | Loss: 0.00001063
Iteration 55/1000 | Loss: 0.00001060
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001059
Iteration 58/1000 | Loss: 0.00001059
Iteration 59/1000 | Loss: 0.00001059
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001057
Iteration 62/1000 | Loss: 0.00001056
Iteration 63/1000 | Loss: 0.00001055
Iteration 64/1000 | Loss: 0.00001055
Iteration 65/1000 | Loss: 0.00001055
Iteration 66/1000 | Loss: 0.00001054
Iteration 67/1000 | Loss: 0.00001054
Iteration 68/1000 | Loss: 0.00001054
Iteration 69/1000 | Loss: 0.00001054
Iteration 70/1000 | Loss: 0.00001054
Iteration 71/1000 | Loss: 0.00001053
Iteration 72/1000 | Loss: 0.00001053
Iteration 73/1000 | Loss: 0.00001053
Iteration 74/1000 | Loss: 0.00001053
Iteration 75/1000 | Loss: 0.00001053
Iteration 76/1000 | Loss: 0.00001052
Iteration 77/1000 | Loss: 0.00001052
Iteration 78/1000 | Loss: 0.00001051
Iteration 79/1000 | Loss: 0.00001050
Iteration 80/1000 | Loss: 0.00001050
Iteration 81/1000 | Loss: 0.00001049
Iteration 82/1000 | Loss: 0.00001048
Iteration 83/1000 | Loss: 0.00001048
Iteration 84/1000 | Loss: 0.00001048
Iteration 85/1000 | Loss: 0.00001048
Iteration 86/1000 | Loss: 0.00001048
Iteration 87/1000 | Loss: 0.00001048
Iteration 88/1000 | Loss: 0.00001048
Iteration 89/1000 | Loss: 0.00001048
Iteration 90/1000 | Loss: 0.00001047
Iteration 91/1000 | Loss: 0.00001047
Iteration 92/1000 | Loss: 0.00001047
Iteration 93/1000 | Loss: 0.00001043
Iteration 94/1000 | Loss: 0.00001043
Iteration 95/1000 | Loss: 0.00001043
Iteration 96/1000 | Loss: 0.00001043
Iteration 97/1000 | Loss: 0.00001043
Iteration 98/1000 | Loss: 0.00001043
Iteration 99/1000 | Loss: 0.00001043
Iteration 100/1000 | Loss: 0.00001043
Iteration 101/1000 | Loss: 0.00001043
Iteration 102/1000 | Loss: 0.00001043
Iteration 103/1000 | Loss: 0.00001042
Iteration 104/1000 | Loss: 0.00001042
Iteration 105/1000 | Loss: 0.00001042
Iteration 106/1000 | Loss: 0.00001042
Iteration 107/1000 | Loss: 0.00001037
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001035
Iteration 111/1000 | Loss: 0.00001034
Iteration 112/1000 | Loss: 0.00001034
Iteration 113/1000 | Loss: 0.00001033
Iteration 114/1000 | Loss: 0.00001033
Iteration 115/1000 | Loss: 0.00001033
Iteration 116/1000 | Loss: 0.00001033
Iteration 117/1000 | Loss: 0.00001032
Iteration 118/1000 | Loss: 0.00001032
Iteration 119/1000 | Loss: 0.00001032
Iteration 120/1000 | Loss: 0.00001032
Iteration 121/1000 | Loss: 0.00001032
Iteration 122/1000 | Loss: 0.00001032
Iteration 123/1000 | Loss: 0.00001032
Iteration 124/1000 | Loss: 0.00001032
Iteration 125/1000 | Loss: 0.00001032
Iteration 126/1000 | Loss: 0.00001032
Iteration 127/1000 | Loss: 0.00001031
Iteration 128/1000 | Loss: 0.00001030
Iteration 129/1000 | Loss: 0.00001030
Iteration 130/1000 | Loss: 0.00001030
Iteration 131/1000 | Loss: 0.00001029
Iteration 132/1000 | Loss: 0.00001029
Iteration 133/1000 | Loss: 0.00001029
Iteration 134/1000 | Loss: 0.00001029
Iteration 135/1000 | Loss: 0.00001028
Iteration 136/1000 | Loss: 0.00001028
Iteration 137/1000 | Loss: 0.00001028
Iteration 138/1000 | Loss: 0.00001027
Iteration 139/1000 | Loss: 0.00001027
Iteration 140/1000 | Loss: 0.00001027
Iteration 141/1000 | Loss: 0.00001027
Iteration 142/1000 | Loss: 0.00001027
Iteration 143/1000 | Loss: 0.00001027
Iteration 144/1000 | Loss: 0.00001027
Iteration 145/1000 | Loss: 0.00001027
Iteration 146/1000 | Loss: 0.00001027
Iteration 147/1000 | Loss: 0.00001027
Iteration 148/1000 | Loss: 0.00001027
Iteration 149/1000 | Loss: 0.00001027
Iteration 150/1000 | Loss: 0.00001027
Iteration 151/1000 | Loss: 0.00001027
Iteration 152/1000 | Loss: 0.00001027
Iteration 153/1000 | Loss: 0.00001026
Iteration 154/1000 | Loss: 0.00001026
Iteration 155/1000 | Loss: 0.00001026
Iteration 156/1000 | Loss: 0.00001026
Iteration 157/1000 | Loss: 0.00001025
Iteration 158/1000 | Loss: 0.00001025
Iteration 159/1000 | Loss: 0.00001025
Iteration 160/1000 | Loss: 0.00001024
Iteration 161/1000 | Loss: 0.00001024
Iteration 162/1000 | Loss: 0.00001024
Iteration 163/1000 | Loss: 0.00001023
Iteration 164/1000 | Loss: 0.00001023
Iteration 165/1000 | Loss: 0.00001022
Iteration 166/1000 | Loss: 0.00001022
Iteration 167/1000 | Loss: 0.00001022
Iteration 168/1000 | Loss: 0.00001022
Iteration 169/1000 | Loss: 0.00001022
Iteration 170/1000 | Loss: 0.00001022
Iteration 171/1000 | Loss: 0.00001022
Iteration 172/1000 | Loss: 0.00001022
Iteration 173/1000 | Loss: 0.00001021
Iteration 174/1000 | Loss: 0.00001021
Iteration 175/1000 | Loss: 0.00001021
Iteration 176/1000 | Loss: 0.00001021
Iteration 177/1000 | Loss: 0.00001021
Iteration 178/1000 | Loss: 0.00001021
Iteration 179/1000 | Loss: 0.00001021
Iteration 180/1000 | Loss: 0.00001021
Iteration 181/1000 | Loss: 0.00001021
Iteration 182/1000 | Loss: 0.00001021
Iteration 183/1000 | Loss: 0.00001021
Iteration 184/1000 | Loss: 0.00001021
Iteration 185/1000 | Loss: 0.00001021
Iteration 186/1000 | Loss: 0.00001020
Iteration 187/1000 | Loss: 0.00001020
Iteration 188/1000 | Loss: 0.00001020
Iteration 189/1000 | Loss: 0.00001020
Iteration 190/1000 | Loss: 0.00001020
Iteration 191/1000 | Loss: 0.00001020
Iteration 192/1000 | Loss: 0.00001020
Iteration 193/1000 | Loss: 0.00001020
Iteration 194/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.020365107251564e-05, 1.020365107251564e-05, 1.020365107251564e-05, 1.020365107251564e-05, 1.020365107251564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.020365107251564e-05

Optimization complete. Final v2v error: 2.7407829761505127 mm

Highest mean error: 2.9737863540649414 mm for frame 170

Lowest mean error: 2.596287965774536 mm for frame 203

Saving results

Total time: 44.37233114242554
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00372174
Iteration 2/25 | Loss: 0.00125166
Iteration 3/25 | Loss: 0.00114862
Iteration 4/25 | Loss: 0.00113803
Iteration 5/25 | Loss: 0.00113505
Iteration 6/25 | Loss: 0.00113493
Iteration 7/25 | Loss: 0.00113493
Iteration 8/25 | Loss: 0.00113493
Iteration 9/25 | Loss: 0.00113493
Iteration 10/25 | Loss: 0.00113493
Iteration 11/25 | Loss: 0.00113493
Iteration 12/25 | Loss: 0.00113493
Iteration 13/25 | Loss: 0.00113493
Iteration 14/25 | Loss: 0.00113493
Iteration 15/25 | Loss: 0.00113493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011349264532327652, 0.0011349264532327652, 0.0011349264532327652, 0.0011349264532327652, 0.0011349264532327652]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011349264532327652

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78318655
Iteration 2/25 | Loss: 0.00092395
Iteration 3/25 | Loss: 0.00092395
Iteration 4/25 | Loss: 0.00092395
Iteration 5/25 | Loss: 0.00092395
Iteration 6/25 | Loss: 0.00092394
Iteration 7/25 | Loss: 0.00092394
Iteration 8/25 | Loss: 0.00092394
Iteration 9/25 | Loss: 0.00092394
Iteration 10/25 | Loss: 0.00092394
Iteration 11/25 | Loss: 0.00092394
Iteration 12/25 | Loss: 0.00092394
Iteration 13/25 | Loss: 0.00092394
Iteration 14/25 | Loss: 0.00092394
Iteration 15/25 | Loss: 0.00092394
Iteration 16/25 | Loss: 0.00092394
Iteration 17/25 | Loss: 0.00092394
Iteration 18/25 | Loss: 0.00092394
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009239430073648691, 0.0009239430073648691, 0.0009239430073648691, 0.0009239430073648691, 0.0009239430073648691]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009239430073648691

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092394
Iteration 2/1000 | Loss: 0.00002941
Iteration 3/1000 | Loss: 0.00001756
Iteration 4/1000 | Loss: 0.00001519
Iteration 5/1000 | Loss: 0.00001410
Iteration 6/1000 | Loss: 0.00001334
Iteration 7/1000 | Loss: 0.00001278
Iteration 8/1000 | Loss: 0.00001241
Iteration 9/1000 | Loss: 0.00001216
Iteration 10/1000 | Loss: 0.00001182
Iteration 11/1000 | Loss: 0.00001157
Iteration 12/1000 | Loss: 0.00001136
Iteration 13/1000 | Loss: 0.00001119
Iteration 14/1000 | Loss: 0.00001109
Iteration 15/1000 | Loss: 0.00001109
Iteration 16/1000 | Loss: 0.00001104
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001102
Iteration 20/1000 | Loss: 0.00001101
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001099
Iteration 24/1000 | Loss: 0.00001098
Iteration 25/1000 | Loss: 0.00001098
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001096
Iteration 29/1000 | Loss: 0.00001095
Iteration 30/1000 | Loss: 0.00001094
Iteration 31/1000 | Loss: 0.00001094
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001092
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001090
Iteration 36/1000 | Loss: 0.00001089
Iteration 37/1000 | Loss: 0.00001089
Iteration 38/1000 | Loss: 0.00001089
Iteration 39/1000 | Loss: 0.00001089
Iteration 40/1000 | Loss: 0.00001088
Iteration 41/1000 | Loss: 0.00001088
Iteration 42/1000 | Loss: 0.00001088
Iteration 43/1000 | Loss: 0.00001087
Iteration 44/1000 | Loss: 0.00001086
Iteration 45/1000 | Loss: 0.00001086
Iteration 46/1000 | Loss: 0.00001086
Iteration 47/1000 | Loss: 0.00001086
Iteration 48/1000 | Loss: 0.00001085
Iteration 49/1000 | Loss: 0.00001085
Iteration 50/1000 | Loss: 0.00001085
Iteration 51/1000 | Loss: 0.00001085
Iteration 52/1000 | Loss: 0.00001084
Iteration 53/1000 | Loss: 0.00001084
Iteration 54/1000 | Loss: 0.00001084
Iteration 55/1000 | Loss: 0.00001083
Iteration 56/1000 | Loss: 0.00001083
Iteration 57/1000 | Loss: 0.00001083
Iteration 58/1000 | Loss: 0.00001083
Iteration 59/1000 | Loss: 0.00001083
Iteration 60/1000 | Loss: 0.00001082
Iteration 61/1000 | Loss: 0.00001082
Iteration 62/1000 | Loss: 0.00001082
Iteration 63/1000 | Loss: 0.00001082
Iteration 64/1000 | Loss: 0.00001082
Iteration 65/1000 | Loss: 0.00001082
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001081
Iteration 71/1000 | Loss: 0.00001081
Iteration 72/1000 | Loss: 0.00001081
Iteration 73/1000 | Loss: 0.00001080
Iteration 74/1000 | Loss: 0.00001080
Iteration 75/1000 | Loss: 0.00001080
Iteration 76/1000 | Loss: 0.00001080
Iteration 77/1000 | Loss: 0.00001080
Iteration 78/1000 | Loss: 0.00001080
Iteration 79/1000 | Loss: 0.00001079
Iteration 80/1000 | Loss: 0.00001079
Iteration 81/1000 | Loss: 0.00001079
Iteration 82/1000 | Loss: 0.00001079
Iteration 83/1000 | Loss: 0.00001079
Iteration 84/1000 | Loss: 0.00001079
Iteration 85/1000 | Loss: 0.00001078
Iteration 86/1000 | Loss: 0.00001078
Iteration 87/1000 | Loss: 0.00001077
Iteration 88/1000 | Loss: 0.00001077
Iteration 89/1000 | Loss: 0.00001077
Iteration 90/1000 | Loss: 0.00001077
Iteration 91/1000 | Loss: 0.00001077
Iteration 92/1000 | Loss: 0.00001077
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001076
Iteration 95/1000 | Loss: 0.00001076
Iteration 96/1000 | Loss: 0.00001076
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001075
Iteration 99/1000 | Loss: 0.00001075
Iteration 100/1000 | Loss: 0.00001075
Iteration 101/1000 | Loss: 0.00001075
Iteration 102/1000 | Loss: 0.00001075
Iteration 103/1000 | Loss: 0.00001074
Iteration 104/1000 | Loss: 0.00001074
Iteration 105/1000 | Loss: 0.00001074
Iteration 106/1000 | Loss: 0.00001074
Iteration 107/1000 | Loss: 0.00001074
Iteration 108/1000 | Loss: 0.00001074
Iteration 109/1000 | Loss: 0.00001074
Iteration 110/1000 | Loss: 0.00001074
Iteration 111/1000 | Loss: 0.00001074
Iteration 112/1000 | Loss: 0.00001073
Iteration 113/1000 | Loss: 0.00001073
Iteration 114/1000 | Loss: 0.00001073
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001073
Iteration 118/1000 | Loss: 0.00001073
Iteration 119/1000 | Loss: 0.00001073
Iteration 120/1000 | Loss: 0.00001072
Iteration 121/1000 | Loss: 0.00001072
Iteration 122/1000 | Loss: 0.00001072
Iteration 123/1000 | Loss: 0.00001072
Iteration 124/1000 | Loss: 0.00001072
Iteration 125/1000 | Loss: 0.00001072
Iteration 126/1000 | Loss: 0.00001071
Iteration 127/1000 | Loss: 0.00001071
Iteration 128/1000 | Loss: 0.00001071
Iteration 129/1000 | Loss: 0.00001071
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001070
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001069
Iteration 137/1000 | Loss: 0.00001069
Iteration 138/1000 | Loss: 0.00001069
Iteration 139/1000 | Loss: 0.00001069
Iteration 140/1000 | Loss: 0.00001069
Iteration 141/1000 | Loss: 0.00001069
Iteration 142/1000 | Loss: 0.00001069
Iteration 143/1000 | Loss: 0.00001069
Iteration 144/1000 | Loss: 0.00001069
Iteration 145/1000 | Loss: 0.00001069
Iteration 146/1000 | Loss: 0.00001069
Iteration 147/1000 | Loss: 0.00001069
Iteration 148/1000 | Loss: 0.00001069
Iteration 149/1000 | Loss: 0.00001069
Iteration 150/1000 | Loss: 0.00001069
Iteration 151/1000 | Loss: 0.00001069
Iteration 152/1000 | Loss: 0.00001069
Iteration 153/1000 | Loss: 0.00001069
Iteration 154/1000 | Loss: 0.00001069
Iteration 155/1000 | Loss: 0.00001069
Iteration 156/1000 | Loss: 0.00001069
Iteration 157/1000 | Loss: 0.00001069
Iteration 158/1000 | Loss: 0.00001069
Iteration 159/1000 | Loss: 0.00001069
Iteration 160/1000 | Loss: 0.00001069
Iteration 161/1000 | Loss: 0.00001069
Iteration 162/1000 | Loss: 0.00001069
Iteration 163/1000 | Loss: 0.00001069
Iteration 164/1000 | Loss: 0.00001069
Iteration 165/1000 | Loss: 0.00001069
Iteration 166/1000 | Loss: 0.00001069
Iteration 167/1000 | Loss: 0.00001069
Iteration 168/1000 | Loss: 0.00001069
Iteration 169/1000 | Loss: 0.00001069
Iteration 170/1000 | Loss: 0.00001069
Iteration 171/1000 | Loss: 0.00001069
Iteration 172/1000 | Loss: 0.00001069
Iteration 173/1000 | Loss: 0.00001069
Iteration 174/1000 | Loss: 0.00001069
Iteration 175/1000 | Loss: 0.00001069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.0689177543099504e-05, 1.0689177543099504e-05, 1.0689177543099504e-05, 1.0689177543099504e-05, 1.0689177543099504e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0689177543099504e-05

Optimization complete. Final v2v error: 2.8310794830322266 mm

Highest mean error: 3.3134005069732666 mm for frame 75

Lowest mean error: 2.3924672603607178 mm for frame 10

Saving results

Total time: 45.095905780792236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424793
Iteration 2/25 | Loss: 0.00123853
Iteration 3/25 | Loss: 0.00114768
Iteration 4/25 | Loss: 0.00113270
Iteration 5/25 | Loss: 0.00112775
Iteration 6/25 | Loss: 0.00112643
Iteration 7/25 | Loss: 0.00112632
Iteration 8/25 | Loss: 0.00112632
Iteration 9/25 | Loss: 0.00112632
Iteration 10/25 | Loss: 0.00112632
Iteration 11/25 | Loss: 0.00112632
Iteration 12/25 | Loss: 0.00112632
Iteration 13/25 | Loss: 0.00112632
Iteration 14/25 | Loss: 0.00112632
Iteration 15/25 | Loss: 0.00112632
Iteration 16/25 | Loss: 0.00112632
Iteration 17/25 | Loss: 0.00112632
Iteration 18/25 | Loss: 0.00112632
Iteration 19/25 | Loss: 0.00112632
Iteration 20/25 | Loss: 0.00112632
Iteration 21/25 | Loss: 0.00112632
Iteration 22/25 | Loss: 0.00112632
Iteration 23/25 | Loss: 0.00112632
Iteration 24/25 | Loss: 0.00112632
Iteration 25/25 | Loss: 0.00112632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.80667996
Iteration 2/25 | Loss: 0.00088928
Iteration 3/25 | Loss: 0.00088926
Iteration 4/25 | Loss: 0.00088926
Iteration 5/25 | Loss: 0.00088926
Iteration 6/25 | Loss: 0.00088926
Iteration 7/25 | Loss: 0.00088925
Iteration 8/25 | Loss: 0.00088925
Iteration 9/25 | Loss: 0.00088925
Iteration 10/25 | Loss: 0.00088925
Iteration 11/25 | Loss: 0.00088925
Iteration 12/25 | Loss: 0.00088925
Iteration 13/25 | Loss: 0.00088925
Iteration 14/25 | Loss: 0.00088925
Iteration 15/25 | Loss: 0.00088925
Iteration 16/25 | Loss: 0.00088925
Iteration 17/25 | Loss: 0.00088925
Iteration 18/25 | Loss: 0.00088925
Iteration 19/25 | Loss: 0.00088925
Iteration 20/25 | Loss: 0.00088925
Iteration 21/25 | Loss: 0.00088925
Iteration 22/25 | Loss: 0.00088925
Iteration 23/25 | Loss: 0.00088925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00088925298769027, 0.00088925298769027, 0.00088925298769027, 0.00088925298769027, 0.00088925298769027]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00088925298769027

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088925
Iteration 2/1000 | Loss: 0.00003007
Iteration 3/1000 | Loss: 0.00001818
Iteration 4/1000 | Loss: 0.00001518
Iteration 5/1000 | Loss: 0.00001428
Iteration 6/1000 | Loss: 0.00001356
Iteration 7/1000 | Loss: 0.00001316
Iteration 8/1000 | Loss: 0.00001274
Iteration 9/1000 | Loss: 0.00001250
Iteration 10/1000 | Loss: 0.00001226
Iteration 11/1000 | Loss: 0.00001201
Iteration 12/1000 | Loss: 0.00001191
Iteration 13/1000 | Loss: 0.00001186
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001184
Iteration 16/1000 | Loss: 0.00001183
Iteration 17/1000 | Loss: 0.00001182
Iteration 18/1000 | Loss: 0.00001171
Iteration 19/1000 | Loss: 0.00001165
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001163
Iteration 22/1000 | Loss: 0.00001162
Iteration 23/1000 | Loss: 0.00001162
Iteration 24/1000 | Loss: 0.00001161
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001161
Iteration 27/1000 | Loss: 0.00001160
Iteration 28/1000 | Loss: 0.00001160
Iteration 29/1000 | Loss: 0.00001160
Iteration 30/1000 | Loss: 0.00001160
Iteration 31/1000 | Loss: 0.00001160
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001159
Iteration 35/1000 | Loss: 0.00001159
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001158
Iteration 38/1000 | Loss: 0.00001157
Iteration 39/1000 | Loss: 0.00001157
Iteration 40/1000 | Loss: 0.00001157
Iteration 41/1000 | Loss: 0.00001156
Iteration 42/1000 | Loss: 0.00001156
Iteration 43/1000 | Loss: 0.00001155
Iteration 44/1000 | Loss: 0.00001155
Iteration 45/1000 | Loss: 0.00001155
Iteration 46/1000 | Loss: 0.00001155
Iteration 47/1000 | Loss: 0.00001155
Iteration 48/1000 | Loss: 0.00001155
Iteration 49/1000 | Loss: 0.00001153
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001151
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001150
Iteration 57/1000 | Loss: 0.00001150
Iteration 58/1000 | Loss: 0.00001149
Iteration 59/1000 | Loss: 0.00001149
Iteration 60/1000 | Loss: 0.00001148
Iteration 61/1000 | Loss: 0.00001148
Iteration 62/1000 | Loss: 0.00001147
Iteration 63/1000 | Loss: 0.00001147
Iteration 64/1000 | Loss: 0.00001147
Iteration 65/1000 | Loss: 0.00001146
Iteration 66/1000 | Loss: 0.00001146
Iteration 67/1000 | Loss: 0.00001146
Iteration 68/1000 | Loss: 0.00001146
Iteration 69/1000 | Loss: 0.00001146
Iteration 70/1000 | Loss: 0.00001146
Iteration 71/1000 | Loss: 0.00001146
Iteration 72/1000 | Loss: 0.00001146
Iteration 73/1000 | Loss: 0.00001145
Iteration 74/1000 | Loss: 0.00001145
Iteration 75/1000 | Loss: 0.00001145
Iteration 76/1000 | Loss: 0.00001145
Iteration 77/1000 | Loss: 0.00001145
Iteration 78/1000 | Loss: 0.00001144
Iteration 79/1000 | Loss: 0.00001144
Iteration 80/1000 | Loss: 0.00001143
Iteration 81/1000 | Loss: 0.00001143
Iteration 82/1000 | Loss: 0.00001143
Iteration 83/1000 | Loss: 0.00001142
Iteration 84/1000 | Loss: 0.00001142
Iteration 85/1000 | Loss: 0.00001142
Iteration 86/1000 | Loss: 0.00001141
Iteration 87/1000 | Loss: 0.00001141
Iteration 88/1000 | Loss: 0.00001141
Iteration 89/1000 | Loss: 0.00001140
Iteration 90/1000 | Loss: 0.00001140
Iteration 91/1000 | Loss: 0.00001140
Iteration 92/1000 | Loss: 0.00001139
Iteration 93/1000 | Loss: 0.00001139
Iteration 94/1000 | Loss: 0.00001139
Iteration 95/1000 | Loss: 0.00001138
Iteration 96/1000 | Loss: 0.00001138
Iteration 97/1000 | Loss: 0.00001138
Iteration 98/1000 | Loss: 0.00001137
Iteration 99/1000 | Loss: 0.00001137
Iteration 100/1000 | Loss: 0.00001137
Iteration 101/1000 | Loss: 0.00001137
Iteration 102/1000 | Loss: 0.00001136
Iteration 103/1000 | Loss: 0.00001136
Iteration 104/1000 | Loss: 0.00001135
Iteration 105/1000 | Loss: 0.00001135
Iteration 106/1000 | Loss: 0.00001135
Iteration 107/1000 | Loss: 0.00001135
Iteration 108/1000 | Loss: 0.00001135
Iteration 109/1000 | Loss: 0.00001135
Iteration 110/1000 | Loss: 0.00001134
Iteration 111/1000 | Loss: 0.00001134
Iteration 112/1000 | Loss: 0.00001134
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001133
Iteration 117/1000 | Loss: 0.00001133
Iteration 118/1000 | Loss: 0.00001133
Iteration 119/1000 | Loss: 0.00001132
Iteration 120/1000 | Loss: 0.00001132
Iteration 121/1000 | Loss: 0.00001132
Iteration 122/1000 | Loss: 0.00001132
Iteration 123/1000 | Loss: 0.00001132
Iteration 124/1000 | Loss: 0.00001131
Iteration 125/1000 | Loss: 0.00001131
Iteration 126/1000 | Loss: 0.00001131
Iteration 127/1000 | Loss: 0.00001131
Iteration 128/1000 | Loss: 0.00001131
Iteration 129/1000 | Loss: 0.00001131
Iteration 130/1000 | Loss: 0.00001131
Iteration 131/1000 | Loss: 0.00001131
Iteration 132/1000 | Loss: 0.00001130
Iteration 133/1000 | Loss: 0.00001130
Iteration 134/1000 | Loss: 0.00001130
Iteration 135/1000 | Loss: 0.00001130
Iteration 136/1000 | Loss: 0.00001130
Iteration 137/1000 | Loss: 0.00001130
Iteration 138/1000 | Loss: 0.00001130
Iteration 139/1000 | Loss: 0.00001130
Iteration 140/1000 | Loss: 0.00001130
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001129
Iteration 146/1000 | Loss: 0.00001129
Iteration 147/1000 | Loss: 0.00001129
Iteration 148/1000 | Loss: 0.00001129
Iteration 149/1000 | Loss: 0.00001129
Iteration 150/1000 | Loss: 0.00001129
Iteration 151/1000 | Loss: 0.00001129
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001128
Iteration 154/1000 | Loss: 0.00001128
Iteration 155/1000 | Loss: 0.00001128
Iteration 156/1000 | Loss: 0.00001128
Iteration 157/1000 | Loss: 0.00001128
Iteration 158/1000 | Loss: 0.00001128
Iteration 159/1000 | Loss: 0.00001128
Iteration 160/1000 | Loss: 0.00001128
Iteration 161/1000 | Loss: 0.00001128
Iteration 162/1000 | Loss: 0.00001128
Iteration 163/1000 | Loss: 0.00001127
Iteration 164/1000 | Loss: 0.00001127
Iteration 165/1000 | Loss: 0.00001127
Iteration 166/1000 | Loss: 0.00001127
Iteration 167/1000 | Loss: 0.00001127
Iteration 168/1000 | Loss: 0.00001127
Iteration 169/1000 | Loss: 0.00001127
Iteration 170/1000 | Loss: 0.00001127
Iteration 171/1000 | Loss: 0.00001127
Iteration 172/1000 | Loss: 0.00001127
Iteration 173/1000 | Loss: 0.00001127
Iteration 174/1000 | Loss: 0.00001127
Iteration 175/1000 | Loss: 0.00001127
Iteration 176/1000 | Loss: 0.00001127
Iteration 177/1000 | Loss: 0.00001127
Iteration 178/1000 | Loss: 0.00001127
Iteration 179/1000 | Loss: 0.00001127
Iteration 180/1000 | Loss: 0.00001127
Iteration 181/1000 | Loss: 0.00001127
Iteration 182/1000 | Loss: 0.00001126
Iteration 183/1000 | Loss: 0.00001126
Iteration 184/1000 | Loss: 0.00001126
Iteration 185/1000 | Loss: 0.00001126
Iteration 186/1000 | Loss: 0.00001126
Iteration 187/1000 | Loss: 0.00001126
Iteration 188/1000 | Loss: 0.00001126
Iteration 189/1000 | Loss: 0.00001126
Iteration 190/1000 | Loss: 0.00001126
Iteration 191/1000 | Loss: 0.00001125
Iteration 192/1000 | Loss: 0.00001125
Iteration 193/1000 | Loss: 0.00001125
Iteration 194/1000 | Loss: 0.00001125
Iteration 195/1000 | Loss: 0.00001125
Iteration 196/1000 | Loss: 0.00001125
Iteration 197/1000 | Loss: 0.00001125
Iteration 198/1000 | Loss: 0.00001125
Iteration 199/1000 | Loss: 0.00001125
Iteration 200/1000 | Loss: 0.00001125
Iteration 201/1000 | Loss: 0.00001125
Iteration 202/1000 | Loss: 0.00001125
Iteration 203/1000 | Loss: 0.00001125
Iteration 204/1000 | Loss: 0.00001125
Iteration 205/1000 | Loss: 0.00001125
Iteration 206/1000 | Loss: 0.00001125
Iteration 207/1000 | Loss: 0.00001125
Iteration 208/1000 | Loss: 0.00001125
Iteration 209/1000 | Loss: 0.00001125
Iteration 210/1000 | Loss: 0.00001125
Iteration 211/1000 | Loss: 0.00001125
Iteration 212/1000 | Loss: 0.00001125
Iteration 213/1000 | Loss: 0.00001125
Iteration 214/1000 | Loss: 0.00001125
Iteration 215/1000 | Loss: 0.00001125
Iteration 216/1000 | Loss: 0.00001125
Iteration 217/1000 | Loss: 0.00001125
Iteration 218/1000 | Loss: 0.00001125
Iteration 219/1000 | Loss: 0.00001125
Iteration 220/1000 | Loss: 0.00001125
Iteration 221/1000 | Loss: 0.00001125
Iteration 222/1000 | Loss: 0.00001125
Iteration 223/1000 | Loss: 0.00001125
Iteration 224/1000 | Loss: 0.00001125
Iteration 225/1000 | Loss: 0.00001125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.1247312613704707e-05, 1.1247312613704707e-05, 1.1247312613704707e-05, 1.1247312613704707e-05, 1.1247312613704707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1247312613704707e-05

Optimization complete. Final v2v error: 2.85469913482666 mm

Highest mean error: 3.2717084884643555 mm for frame 82

Lowest mean error: 2.440370798110962 mm for frame 7

Saving results

Total time: 43.843257665634155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789992
Iteration 2/25 | Loss: 0.00138606
Iteration 3/25 | Loss: 0.00115755
Iteration 4/25 | Loss: 0.00114196
Iteration 5/25 | Loss: 0.00113647
Iteration 6/25 | Loss: 0.00113475
Iteration 7/25 | Loss: 0.00113461
Iteration 8/25 | Loss: 0.00113461
Iteration 9/25 | Loss: 0.00113461
Iteration 10/25 | Loss: 0.00113461
Iteration 11/25 | Loss: 0.00113461
Iteration 12/25 | Loss: 0.00113461
Iteration 13/25 | Loss: 0.00113461
Iteration 14/25 | Loss: 0.00113461
Iteration 15/25 | Loss: 0.00113461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011346099199727178, 0.0011346099199727178, 0.0011346099199727178, 0.0011346099199727178, 0.0011346099199727178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011346099199727178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16767526
Iteration 2/25 | Loss: 0.00091723
Iteration 3/25 | Loss: 0.00091723
Iteration 4/25 | Loss: 0.00091723
Iteration 5/25 | Loss: 0.00091723
Iteration 6/25 | Loss: 0.00091723
Iteration 7/25 | Loss: 0.00091723
Iteration 8/25 | Loss: 0.00091723
Iteration 9/25 | Loss: 0.00091723
Iteration 10/25 | Loss: 0.00091723
Iteration 11/25 | Loss: 0.00091723
Iteration 12/25 | Loss: 0.00091723
Iteration 13/25 | Loss: 0.00091723
Iteration 14/25 | Loss: 0.00091723
Iteration 15/25 | Loss: 0.00091723
Iteration 16/25 | Loss: 0.00091723
Iteration 17/25 | Loss: 0.00091723
Iteration 18/25 | Loss: 0.00091723
Iteration 19/25 | Loss: 0.00091723
Iteration 20/25 | Loss: 0.00091723
Iteration 21/25 | Loss: 0.00091723
Iteration 22/25 | Loss: 0.00091723
Iteration 23/25 | Loss: 0.00091723
Iteration 24/25 | Loss: 0.00091723
Iteration 25/25 | Loss: 0.00091723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091723
Iteration 2/1000 | Loss: 0.00004580
Iteration 3/1000 | Loss: 0.00002848
Iteration 4/1000 | Loss: 0.00002221
Iteration 5/1000 | Loss: 0.00001888
Iteration 6/1000 | Loss: 0.00001745
Iteration 7/1000 | Loss: 0.00001610
Iteration 8/1000 | Loss: 0.00001552
Iteration 9/1000 | Loss: 0.00001501
Iteration 10/1000 | Loss: 0.00001451
Iteration 11/1000 | Loss: 0.00001424
Iteration 12/1000 | Loss: 0.00001400
Iteration 13/1000 | Loss: 0.00001386
Iteration 14/1000 | Loss: 0.00001375
Iteration 15/1000 | Loss: 0.00001371
Iteration 16/1000 | Loss: 0.00001367
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001360
Iteration 19/1000 | Loss: 0.00001351
Iteration 20/1000 | Loss: 0.00001349
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001346
Iteration 24/1000 | Loss: 0.00001341
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001336
Iteration 27/1000 | Loss: 0.00001336
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001334
Iteration 31/1000 | Loss: 0.00001333
Iteration 32/1000 | Loss: 0.00001333
Iteration 33/1000 | Loss: 0.00001333
Iteration 34/1000 | Loss: 0.00001333
Iteration 35/1000 | Loss: 0.00001333
Iteration 36/1000 | Loss: 0.00001333
Iteration 37/1000 | Loss: 0.00001332
Iteration 38/1000 | Loss: 0.00001331
Iteration 39/1000 | Loss: 0.00001330
Iteration 40/1000 | Loss: 0.00001330
Iteration 41/1000 | Loss: 0.00001330
Iteration 42/1000 | Loss: 0.00001329
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001322
Iteration 45/1000 | Loss: 0.00001322
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001321
Iteration 48/1000 | Loss: 0.00001321
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001320
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001318
Iteration 54/1000 | Loss: 0.00001318
Iteration 55/1000 | Loss: 0.00001318
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001317
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001316
Iteration 64/1000 | Loss: 0.00001315
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001312
Iteration 69/1000 | Loss: 0.00001312
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001312
Iteration 72/1000 | Loss: 0.00001312
Iteration 73/1000 | Loss: 0.00001311
Iteration 74/1000 | Loss: 0.00001311
Iteration 75/1000 | Loss: 0.00001311
Iteration 76/1000 | Loss: 0.00001311
Iteration 77/1000 | Loss: 0.00001311
Iteration 78/1000 | Loss: 0.00001311
Iteration 79/1000 | Loss: 0.00001311
Iteration 80/1000 | Loss: 0.00001310
Iteration 81/1000 | Loss: 0.00001310
Iteration 82/1000 | Loss: 0.00001310
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001309
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001309
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001307
Iteration 97/1000 | Loss: 0.00001307
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001307
Iteration 101/1000 | Loss: 0.00001307
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001305
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001304
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001304
Iteration 114/1000 | Loss: 0.00001304
Iteration 115/1000 | Loss: 0.00001304
Iteration 116/1000 | Loss: 0.00001304
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001303
Iteration 120/1000 | Loss: 0.00001303
Iteration 121/1000 | Loss: 0.00001303
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001302
Iteration 126/1000 | Loss: 0.00001302
Iteration 127/1000 | Loss: 0.00001302
Iteration 128/1000 | Loss: 0.00001302
Iteration 129/1000 | Loss: 0.00001302
Iteration 130/1000 | Loss: 0.00001302
Iteration 131/1000 | Loss: 0.00001302
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001301
Iteration 134/1000 | Loss: 0.00001301
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001301
Iteration 137/1000 | Loss: 0.00001301
Iteration 138/1000 | Loss: 0.00001301
Iteration 139/1000 | Loss: 0.00001300
Iteration 140/1000 | Loss: 0.00001300
Iteration 141/1000 | Loss: 0.00001300
Iteration 142/1000 | Loss: 0.00001299
Iteration 143/1000 | Loss: 0.00001299
Iteration 144/1000 | Loss: 0.00001299
Iteration 145/1000 | Loss: 0.00001299
Iteration 146/1000 | Loss: 0.00001299
Iteration 147/1000 | Loss: 0.00001299
Iteration 148/1000 | Loss: 0.00001299
Iteration 149/1000 | Loss: 0.00001299
Iteration 150/1000 | Loss: 0.00001299
Iteration 151/1000 | Loss: 0.00001299
Iteration 152/1000 | Loss: 0.00001298
Iteration 153/1000 | Loss: 0.00001298
Iteration 154/1000 | Loss: 0.00001298
Iteration 155/1000 | Loss: 0.00001298
Iteration 156/1000 | Loss: 0.00001298
Iteration 157/1000 | Loss: 0.00001298
Iteration 158/1000 | Loss: 0.00001298
Iteration 159/1000 | Loss: 0.00001298
Iteration 160/1000 | Loss: 0.00001298
Iteration 161/1000 | Loss: 0.00001298
Iteration 162/1000 | Loss: 0.00001298
Iteration 163/1000 | Loss: 0.00001298
Iteration 164/1000 | Loss: 0.00001298
Iteration 165/1000 | Loss: 0.00001298
Iteration 166/1000 | Loss: 0.00001298
Iteration 167/1000 | Loss: 0.00001298
Iteration 168/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.2976787729712669e-05, 1.2976787729712669e-05, 1.2976787729712669e-05, 1.2976787729712669e-05, 1.2976787729712669e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2976787729712669e-05

Optimization complete. Final v2v error: 2.970184803009033 mm

Highest mean error: 4.259571075439453 mm for frame 69

Lowest mean error: 2.4503371715545654 mm for frame 102

Saving results

Total time: 44.91143989562988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_emma_posed_018/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_emma_posed_018/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902706
Iteration 2/25 | Loss: 0.00152097
Iteration 3/25 | Loss: 0.00124483
Iteration 4/25 | Loss: 0.00121646
Iteration 5/25 | Loss: 0.00120860
Iteration 6/25 | Loss: 0.00120636
Iteration 7/25 | Loss: 0.00120636
Iteration 8/25 | Loss: 0.00120636
Iteration 9/25 | Loss: 0.00120636
Iteration 10/25 | Loss: 0.00120636
Iteration 11/25 | Loss: 0.00120636
Iteration 12/25 | Loss: 0.00120636
Iteration 13/25 | Loss: 0.00120636
Iteration 14/25 | Loss: 0.00120636
Iteration 15/25 | Loss: 0.00120636
Iteration 16/25 | Loss: 0.00120636
Iteration 17/25 | Loss: 0.00120636
Iteration 18/25 | Loss: 0.00120636
Iteration 19/25 | Loss: 0.00120636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012063562171533704, 0.0012063562171533704, 0.0012063562171533704, 0.0012063562171533704, 0.0012063562171533704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012063562171533704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32576752
Iteration 2/25 | Loss: 0.00072222
Iteration 3/25 | Loss: 0.00072220
Iteration 4/25 | Loss: 0.00072220
Iteration 5/25 | Loss: 0.00072220
Iteration 6/25 | Loss: 0.00072220
Iteration 7/25 | Loss: 0.00072220
Iteration 8/25 | Loss: 0.00072219
Iteration 9/25 | Loss: 0.00072219
Iteration 10/25 | Loss: 0.00072219
Iteration 11/25 | Loss: 0.00072219
Iteration 12/25 | Loss: 0.00072219
Iteration 13/25 | Loss: 0.00072219
Iteration 14/25 | Loss: 0.00072219
Iteration 15/25 | Loss: 0.00072219
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000722194614354521, 0.000722194614354521, 0.000722194614354521, 0.000722194614354521, 0.000722194614354521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000722194614354521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072219
Iteration 2/1000 | Loss: 0.00004801
Iteration 3/1000 | Loss: 0.00002972
Iteration 4/1000 | Loss: 0.00002454
Iteration 5/1000 | Loss: 0.00002276
Iteration 6/1000 | Loss: 0.00002173
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002035
Iteration 9/1000 | Loss: 0.00001982
Iteration 10/1000 | Loss: 0.00001951
Iteration 11/1000 | Loss: 0.00001928
Iteration 12/1000 | Loss: 0.00001913
Iteration 13/1000 | Loss: 0.00001905
Iteration 14/1000 | Loss: 0.00001904
Iteration 15/1000 | Loss: 0.00001895
Iteration 16/1000 | Loss: 0.00001894
Iteration 17/1000 | Loss: 0.00001888
Iteration 18/1000 | Loss: 0.00001885
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001884
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00001883
Iteration 23/1000 | Loss: 0.00001882
Iteration 24/1000 | Loss: 0.00001881
Iteration 25/1000 | Loss: 0.00001881
Iteration 26/1000 | Loss: 0.00001880
Iteration 27/1000 | Loss: 0.00001877
Iteration 28/1000 | Loss: 0.00001876
Iteration 29/1000 | Loss: 0.00001876
Iteration 30/1000 | Loss: 0.00001874
Iteration 31/1000 | Loss: 0.00001874
Iteration 32/1000 | Loss: 0.00001874
Iteration 33/1000 | Loss: 0.00001874
Iteration 34/1000 | Loss: 0.00001873
Iteration 35/1000 | Loss: 0.00001873
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001873
Iteration 38/1000 | Loss: 0.00001873
Iteration 39/1000 | Loss: 0.00001873
Iteration 40/1000 | Loss: 0.00001873
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001872
Iteration 43/1000 | Loss: 0.00001872
Iteration 44/1000 | Loss: 0.00001872
Iteration 45/1000 | Loss: 0.00001872
Iteration 46/1000 | Loss: 0.00001872
Iteration 47/1000 | Loss: 0.00001872
Iteration 48/1000 | Loss: 0.00001872
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001872
Iteration 52/1000 | Loss: 0.00001871
Iteration 53/1000 | Loss: 0.00001871
Iteration 54/1000 | Loss: 0.00001871
Iteration 55/1000 | Loss: 0.00001870
Iteration 56/1000 | Loss: 0.00001870
Iteration 57/1000 | Loss: 0.00001869
Iteration 58/1000 | Loss: 0.00001869
Iteration 59/1000 | Loss: 0.00001869
Iteration 60/1000 | Loss: 0.00001869
Iteration 61/1000 | Loss: 0.00001869
Iteration 62/1000 | Loss: 0.00001868
Iteration 63/1000 | Loss: 0.00001868
Iteration 64/1000 | Loss: 0.00001868
Iteration 65/1000 | Loss: 0.00001867
Iteration 66/1000 | Loss: 0.00001867
Iteration 67/1000 | Loss: 0.00001866
Iteration 68/1000 | Loss: 0.00001866
Iteration 69/1000 | Loss: 0.00001866
Iteration 70/1000 | Loss: 0.00001866
Iteration 71/1000 | Loss: 0.00001866
Iteration 72/1000 | Loss: 0.00001866
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001866
Iteration 75/1000 | Loss: 0.00001866
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001864
Iteration 80/1000 | Loss: 0.00001864
Iteration 81/1000 | Loss: 0.00001864
Iteration 82/1000 | Loss: 0.00001863
Iteration 83/1000 | Loss: 0.00001863
Iteration 84/1000 | Loss: 0.00001863
Iteration 85/1000 | Loss: 0.00001863
Iteration 86/1000 | Loss: 0.00001862
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00001862
Iteration 89/1000 | Loss: 0.00001862
Iteration 90/1000 | Loss: 0.00001861
Iteration 91/1000 | Loss: 0.00001861
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001860
Iteration 94/1000 | Loss: 0.00001860
Iteration 95/1000 | Loss: 0.00001860
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001860
Iteration 98/1000 | Loss: 0.00001860
Iteration 99/1000 | Loss: 0.00001859
Iteration 100/1000 | Loss: 0.00001859
Iteration 101/1000 | Loss: 0.00001859
Iteration 102/1000 | Loss: 0.00001859
Iteration 103/1000 | Loss: 0.00001859
Iteration 104/1000 | Loss: 0.00001859
Iteration 105/1000 | Loss: 0.00001858
Iteration 106/1000 | Loss: 0.00001858
Iteration 107/1000 | Loss: 0.00001858
Iteration 108/1000 | Loss: 0.00001858
Iteration 109/1000 | Loss: 0.00001857
Iteration 110/1000 | Loss: 0.00001857
Iteration 111/1000 | Loss: 0.00001857
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001856
Iteration 114/1000 | Loss: 0.00001856
Iteration 115/1000 | Loss: 0.00001856
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001854
Iteration 121/1000 | Loss: 0.00001854
Iteration 122/1000 | Loss: 0.00001854
Iteration 123/1000 | Loss: 0.00001854
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001853
Iteration 126/1000 | Loss: 0.00001853
Iteration 127/1000 | Loss: 0.00001853
Iteration 128/1000 | Loss: 0.00001853
Iteration 129/1000 | Loss: 0.00001853
Iteration 130/1000 | Loss: 0.00001853
Iteration 131/1000 | Loss: 0.00001853
Iteration 132/1000 | Loss: 0.00001853
Iteration 133/1000 | Loss: 0.00001853
Iteration 134/1000 | Loss: 0.00001852
Iteration 135/1000 | Loss: 0.00001852
Iteration 136/1000 | Loss: 0.00001852
Iteration 137/1000 | Loss: 0.00001852
Iteration 138/1000 | Loss: 0.00001852
Iteration 139/1000 | Loss: 0.00001852
Iteration 140/1000 | Loss: 0.00001852
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001851
Iteration 143/1000 | Loss: 0.00001851
Iteration 144/1000 | Loss: 0.00001851
Iteration 145/1000 | Loss: 0.00001851
Iteration 146/1000 | Loss: 0.00001851
Iteration 147/1000 | Loss: 0.00001850
Iteration 148/1000 | Loss: 0.00001850
Iteration 149/1000 | Loss: 0.00001850
Iteration 150/1000 | Loss: 0.00001850
Iteration 151/1000 | Loss: 0.00001850
Iteration 152/1000 | Loss: 0.00001850
Iteration 153/1000 | Loss: 0.00001850
Iteration 154/1000 | Loss: 0.00001850
Iteration 155/1000 | Loss: 0.00001850
Iteration 156/1000 | Loss: 0.00001850
Iteration 157/1000 | Loss: 0.00001850
Iteration 158/1000 | Loss: 0.00001850
Iteration 159/1000 | Loss: 0.00001849
Iteration 160/1000 | Loss: 0.00001849
Iteration 161/1000 | Loss: 0.00001849
Iteration 162/1000 | Loss: 0.00001849
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001849
Iteration 165/1000 | Loss: 0.00001849
Iteration 166/1000 | Loss: 0.00001848
Iteration 167/1000 | Loss: 0.00001848
Iteration 168/1000 | Loss: 0.00001848
Iteration 169/1000 | Loss: 0.00001848
Iteration 170/1000 | Loss: 0.00001848
Iteration 171/1000 | Loss: 0.00001847
Iteration 172/1000 | Loss: 0.00001847
Iteration 173/1000 | Loss: 0.00001847
Iteration 174/1000 | Loss: 0.00001847
Iteration 175/1000 | Loss: 0.00001847
Iteration 176/1000 | Loss: 0.00001847
Iteration 177/1000 | Loss: 0.00001847
Iteration 178/1000 | Loss: 0.00001846
Iteration 179/1000 | Loss: 0.00001846
Iteration 180/1000 | Loss: 0.00001846
Iteration 181/1000 | Loss: 0.00001845
Iteration 182/1000 | Loss: 0.00001845
Iteration 183/1000 | Loss: 0.00001845
Iteration 184/1000 | Loss: 0.00001845
Iteration 185/1000 | Loss: 0.00001845
Iteration 186/1000 | Loss: 0.00001845
Iteration 187/1000 | Loss: 0.00001845
Iteration 188/1000 | Loss: 0.00001844
Iteration 189/1000 | Loss: 0.00001844
Iteration 190/1000 | Loss: 0.00001844
Iteration 191/1000 | Loss: 0.00001844
Iteration 192/1000 | Loss: 0.00001843
Iteration 193/1000 | Loss: 0.00001843
Iteration 194/1000 | Loss: 0.00001843
Iteration 195/1000 | Loss: 0.00001843
Iteration 196/1000 | Loss: 0.00001843
Iteration 197/1000 | Loss: 0.00001843
Iteration 198/1000 | Loss: 0.00001843
Iteration 199/1000 | Loss: 0.00001843
Iteration 200/1000 | Loss: 0.00001843
Iteration 201/1000 | Loss: 0.00001842
Iteration 202/1000 | Loss: 0.00001842
Iteration 203/1000 | Loss: 0.00001842
Iteration 204/1000 | Loss: 0.00001842
Iteration 205/1000 | Loss: 0.00001842
Iteration 206/1000 | Loss: 0.00001842
Iteration 207/1000 | Loss: 0.00001842
Iteration 208/1000 | Loss: 0.00001841
Iteration 209/1000 | Loss: 0.00001841
Iteration 210/1000 | Loss: 0.00001841
Iteration 211/1000 | Loss: 0.00001841
Iteration 212/1000 | Loss: 0.00001841
Iteration 213/1000 | Loss: 0.00001841
Iteration 214/1000 | Loss: 0.00001840
Iteration 215/1000 | Loss: 0.00001840
Iteration 216/1000 | Loss: 0.00001840
Iteration 217/1000 | Loss: 0.00001840
Iteration 218/1000 | Loss: 0.00001840
Iteration 219/1000 | Loss: 0.00001840
Iteration 220/1000 | Loss: 0.00001840
Iteration 221/1000 | Loss: 0.00001840
Iteration 222/1000 | Loss: 0.00001840
Iteration 223/1000 | Loss: 0.00001840
Iteration 224/1000 | Loss: 0.00001840
Iteration 225/1000 | Loss: 0.00001840
Iteration 226/1000 | Loss: 0.00001840
Iteration 227/1000 | Loss: 0.00001840
Iteration 228/1000 | Loss: 0.00001839
Iteration 229/1000 | Loss: 0.00001839
Iteration 230/1000 | Loss: 0.00001839
Iteration 231/1000 | Loss: 0.00001839
Iteration 232/1000 | Loss: 0.00001839
Iteration 233/1000 | Loss: 0.00001839
Iteration 234/1000 | Loss: 0.00001839
Iteration 235/1000 | Loss: 0.00001839
Iteration 236/1000 | Loss: 0.00001838
Iteration 237/1000 | Loss: 0.00001838
Iteration 238/1000 | Loss: 0.00001838
Iteration 239/1000 | Loss: 0.00001838
Iteration 240/1000 | Loss: 0.00001838
Iteration 241/1000 | Loss: 0.00001838
Iteration 242/1000 | Loss: 0.00001838
Iteration 243/1000 | Loss: 0.00001838
Iteration 244/1000 | Loss: 0.00001838
Iteration 245/1000 | Loss: 0.00001838
Iteration 246/1000 | Loss: 0.00001838
Iteration 247/1000 | Loss: 0.00001838
Iteration 248/1000 | Loss: 0.00001838
Iteration 249/1000 | Loss: 0.00001838
Iteration 250/1000 | Loss: 0.00001838
Iteration 251/1000 | Loss: 0.00001838
Iteration 252/1000 | Loss: 0.00001838
Iteration 253/1000 | Loss: 0.00001838
Iteration 254/1000 | Loss: 0.00001838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.8379838365945034e-05, 1.8379838365945034e-05, 1.8379838365945034e-05, 1.8379838365945034e-05, 1.8379838365945034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8379838365945034e-05

Optimization complete. Final v2v error: 3.6137168407440186 mm

Highest mean error: 4.686139106750488 mm for frame 68

Lowest mean error: 3.0152618885040283 mm for frame 46

Saving results

Total time: 46.00179672241211
