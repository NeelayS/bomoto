Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=224, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12544-12599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393114
Iteration 2/25 | Loss: 0.00093681
Iteration 3/25 | Loss: 0.00077935
Iteration 4/25 | Loss: 0.00075061
Iteration 5/25 | Loss: 0.00074631
Iteration 6/25 | Loss: 0.00074491
Iteration 7/25 | Loss: 0.00074443
Iteration 8/25 | Loss: 0.00074443
Iteration 9/25 | Loss: 0.00074443
Iteration 10/25 | Loss: 0.00074443
Iteration 11/25 | Loss: 0.00074443
Iteration 12/25 | Loss: 0.00074443
Iteration 13/25 | Loss: 0.00074443
Iteration 14/25 | Loss: 0.00074443
Iteration 15/25 | Loss: 0.00074443
Iteration 16/25 | Loss: 0.00074443
Iteration 17/25 | Loss: 0.00074443
Iteration 18/25 | Loss: 0.00074443
Iteration 19/25 | Loss: 0.00074443
Iteration 20/25 | Loss: 0.00074443
Iteration 21/25 | Loss: 0.00074443
Iteration 22/25 | Loss: 0.00074443
Iteration 23/25 | Loss: 0.00074443
Iteration 24/25 | Loss: 0.00074443
Iteration 25/25 | Loss: 0.00074443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51057768
Iteration 2/25 | Loss: 0.00053038
Iteration 3/25 | Loss: 0.00053038
Iteration 4/25 | Loss: 0.00053038
Iteration 5/25 | Loss: 0.00053038
Iteration 6/25 | Loss: 0.00053038
Iteration 7/25 | Loss: 0.00053038
Iteration 8/25 | Loss: 0.00053038
Iteration 9/25 | Loss: 0.00053038
Iteration 10/25 | Loss: 0.00053038
Iteration 11/25 | Loss: 0.00053038
Iteration 12/25 | Loss: 0.00053038
Iteration 13/25 | Loss: 0.00053038
Iteration 14/25 | Loss: 0.00053038
Iteration 15/25 | Loss: 0.00053038
Iteration 16/25 | Loss: 0.00053038
Iteration 17/25 | Loss: 0.00053038
Iteration 18/25 | Loss: 0.00053038
Iteration 19/25 | Loss: 0.00053038
Iteration 20/25 | Loss: 0.00053038
Iteration 21/25 | Loss: 0.00053038
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005303766229189932, 0.0005303766229189932, 0.0005303766229189932, 0.0005303766229189932, 0.0005303766229189932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005303766229189932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053038
Iteration 2/1000 | Loss: 0.00002982
Iteration 3/1000 | Loss: 0.00001930
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001664
Iteration 6/1000 | Loss: 0.00001620
Iteration 7/1000 | Loss: 0.00001583
Iteration 8/1000 | Loss: 0.00001556
Iteration 9/1000 | Loss: 0.00001544
Iteration 10/1000 | Loss: 0.00001527
Iteration 11/1000 | Loss: 0.00001505
Iteration 12/1000 | Loss: 0.00001494
Iteration 13/1000 | Loss: 0.00001485
Iteration 14/1000 | Loss: 0.00001484
Iteration 15/1000 | Loss: 0.00001483
Iteration 16/1000 | Loss: 0.00001482
Iteration 17/1000 | Loss: 0.00001481
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001476
Iteration 20/1000 | Loss: 0.00001471
Iteration 21/1000 | Loss: 0.00001469
Iteration 22/1000 | Loss: 0.00001469
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001468
Iteration 25/1000 | Loss: 0.00001467
Iteration 26/1000 | Loss: 0.00001467
Iteration 27/1000 | Loss: 0.00001467
Iteration 28/1000 | Loss: 0.00001466
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001465
Iteration 32/1000 | Loss: 0.00001465
Iteration 33/1000 | Loss: 0.00001465
Iteration 34/1000 | Loss: 0.00001465
Iteration 35/1000 | Loss: 0.00001465
Iteration 36/1000 | Loss: 0.00001465
Iteration 37/1000 | Loss: 0.00001465
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001464
Iteration 40/1000 | Loss: 0.00001464
Iteration 41/1000 | Loss: 0.00001464
Iteration 42/1000 | Loss: 0.00001464
Iteration 43/1000 | Loss: 0.00001464
Iteration 44/1000 | Loss: 0.00001464
Iteration 45/1000 | Loss: 0.00001464
Iteration 46/1000 | Loss: 0.00001464
Iteration 47/1000 | Loss: 0.00001464
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001463
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001463
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001462
Iteration 56/1000 | Loss: 0.00001462
Iteration 57/1000 | Loss: 0.00001462
Iteration 58/1000 | Loss: 0.00001461
Iteration 59/1000 | Loss: 0.00001461
Iteration 60/1000 | Loss: 0.00001461
Iteration 61/1000 | Loss: 0.00001460
Iteration 62/1000 | Loss: 0.00001460
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001459
Iteration 67/1000 | Loss: 0.00001459
Iteration 68/1000 | Loss: 0.00001459
Iteration 69/1000 | Loss: 0.00001458
Iteration 70/1000 | Loss: 0.00001458
Iteration 71/1000 | Loss: 0.00001458
Iteration 72/1000 | Loss: 0.00001458
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001457
Iteration 76/1000 | Loss: 0.00001457
Iteration 77/1000 | Loss: 0.00001457
Iteration 78/1000 | Loss: 0.00001456
Iteration 79/1000 | Loss: 0.00001456
Iteration 80/1000 | Loss: 0.00001455
Iteration 81/1000 | Loss: 0.00001455
Iteration 82/1000 | Loss: 0.00001455
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001454
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001454
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001453
Iteration 100/1000 | Loss: 0.00001453
Iteration 101/1000 | Loss: 0.00001453
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001453
Iteration 106/1000 | Loss: 0.00001453
Iteration 107/1000 | Loss: 0.00001453
Iteration 108/1000 | Loss: 0.00001453
Iteration 109/1000 | Loss: 0.00001453
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001453
Iteration 112/1000 | Loss: 0.00001453
Iteration 113/1000 | Loss: 0.00001453
Iteration 114/1000 | Loss: 0.00001452
Iteration 115/1000 | Loss: 0.00001452
Iteration 116/1000 | Loss: 0.00001452
Iteration 117/1000 | Loss: 0.00001452
Iteration 118/1000 | Loss: 0.00001452
Iteration 119/1000 | Loss: 0.00001452
Iteration 120/1000 | Loss: 0.00001452
Iteration 121/1000 | Loss: 0.00001452
Iteration 122/1000 | Loss: 0.00001452
Iteration 123/1000 | Loss: 0.00001452
Iteration 124/1000 | Loss: 0.00001452
Iteration 125/1000 | Loss: 0.00001452
Iteration 126/1000 | Loss: 0.00001452
Iteration 127/1000 | Loss: 0.00001452
Iteration 128/1000 | Loss: 0.00001451
Iteration 129/1000 | Loss: 0.00001451
Iteration 130/1000 | Loss: 0.00001451
Iteration 131/1000 | Loss: 0.00001451
Iteration 132/1000 | Loss: 0.00001451
Iteration 133/1000 | Loss: 0.00001451
Iteration 134/1000 | Loss: 0.00001451
Iteration 135/1000 | Loss: 0.00001451
Iteration 136/1000 | Loss: 0.00001451
Iteration 137/1000 | Loss: 0.00001451
Iteration 138/1000 | Loss: 0.00001451
Iteration 139/1000 | Loss: 0.00001451
Iteration 140/1000 | Loss: 0.00001451
Iteration 141/1000 | Loss: 0.00001450
Iteration 142/1000 | Loss: 0.00001450
Iteration 143/1000 | Loss: 0.00001450
Iteration 144/1000 | Loss: 0.00001450
Iteration 145/1000 | Loss: 0.00001450
Iteration 146/1000 | Loss: 0.00001450
Iteration 147/1000 | Loss: 0.00001450
Iteration 148/1000 | Loss: 0.00001450
Iteration 149/1000 | Loss: 0.00001450
Iteration 150/1000 | Loss: 0.00001450
Iteration 151/1000 | Loss: 0.00001450
Iteration 152/1000 | Loss: 0.00001449
Iteration 153/1000 | Loss: 0.00001449
Iteration 154/1000 | Loss: 0.00001449
Iteration 155/1000 | Loss: 0.00001449
Iteration 156/1000 | Loss: 0.00001449
Iteration 157/1000 | Loss: 0.00001449
Iteration 158/1000 | Loss: 0.00001449
Iteration 159/1000 | Loss: 0.00001449
Iteration 160/1000 | Loss: 0.00001449
Iteration 161/1000 | Loss: 0.00001449
Iteration 162/1000 | Loss: 0.00001449
Iteration 163/1000 | Loss: 0.00001449
Iteration 164/1000 | Loss: 0.00001449
Iteration 165/1000 | Loss: 0.00001449
Iteration 166/1000 | Loss: 0.00001449
Iteration 167/1000 | Loss: 0.00001449
Iteration 168/1000 | Loss: 0.00001449
Iteration 169/1000 | Loss: 0.00001449
Iteration 170/1000 | Loss: 0.00001449
Iteration 171/1000 | Loss: 0.00001449
Iteration 172/1000 | Loss: 0.00001449
Iteration 173/1000 | Loss: 0.00001449
Iteration 174/1000 | Loss: 0.00001449
Iteration 175/1000 | Loss: 0.00001449
Iteration 176/1000 | Loss: 0.00001449
Iteration 177/1000 | Loss: 0.00001449
Iteration 178/1000 | Loss: 0.00001449
Iteration 179/1000 | Loss: 0.00001449
Iteration 180/1000 | Loss: 0.00001449
Iteration 181/1000 | Loss: 0.00001449
Iteration 182/1000 | Loss: 0.00001449
Iteration 183/1000 | Loss: 0.00001449
Iteration 184/1000 | Loss: 0.00001449
Iteration 185/1000 | Loss: 0.00001449
Iteration 186/1000 | Loss: 0.00001449
Iteration 187/1000 | Loss: 0.00001449
Iteration 188/1000 | Loss: 0.00001449
Iteration 189/1000 | Loss: 0.00001449
Iteration 190/1000 | Loss: 0.00001449
Iteration 191/1000 | Loss: 0.00001449
Iteration 192/1000 | Loss: 0.00001449
Iteration 193/1000 | Loss: 0.00001449
Iteration 194/1000 | Loss: 0.00001449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.4492716218228452e-05, 1.4492716218228452e-05, 1.4492716218228452e-05, 1.4492716218228452e-05, 1.4492716218228452e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4492716218228452e-05

Optimization complete. Final v2v error: 3.129054546356201 mm

Highest mean error: 3.6675400733947754 mm for frame 21

Lowest mean error: 2.605459213256836 mm for frame 36

Saving results

Total time: 41.551302909851074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00423680
Iteration 2/25 | Loss: 0.00095485
Iteration 3/25 | Loss: 0.00085384
Iteration 4/25 | Loss: 0.00081953
Iteration 5/25 | Loss: 0.00080844
Iteration 6/25 | Loss: 0.00080606
Iteration 7/25 | Loss: 0.00080496
Iteration 8/25 | Loss: 0.00080494
Iteration 9/25 | Loss: 0.00080494
Iteration 10/25 | Loss: 0.00080494
Iteration 11/25 | Loss: 0.00080494
Iteration 12/25 | Loss: 0.00080494
Iteration 13/25 | Loss: 0.00080494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008049383759498596, 0.0008049383759498596, 0.0008049383759498596, 0.0008049383759498596, 0.0008049383759498596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008049383759498596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.24246240
Iteration 2/25 | Loss: 0.00049646
Iteration 3/25 | Loss: 0.00049645
Iteration 4/25 | Loss: 0.00049645
Iteration 5/25 | Loss: 0.00049645
Iteration 6/25 | Loss: 0.00049645
Iteration 7/25 | Loss: 0.00049645
Iteration 8/25 | Loss: 0.00049645
Iteration 9/25 | Loss: 0.00049645
Iteration 10/25 | Loss: 0.00049645
Iteration 11/25 | Loss: 0.00049645
Iteration 12/25 | Loss: 0.00049645
Iteration 13/25 | Loss: 0.00049645
Iteration 14/25 | Loss: 0.00049645
Iteration 15/25 | Loss: 0.00049645
Iteration 16/25 | Loss: 0.00049645
Iteration 17/25 | Loss: 0.00049645
Iteration 18/25 | Loss: 0.00049645
Iteration 19/25 | Loss: 0.00049645
Iteration 20/25 | Loss: 0.00049645
Iteration 21/25 | Loss: 0.00049645
Iteration 22/25 | Loss: 0.00049645
Iteration 23/25 | Loss: 0.00049645
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0004964512772858143, 0.0004964512772858143, 0.0004964512772858143, 0.0004964512772858143, 0.0004964512772858143]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004964512772858143

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049645
Iteration 2/1000 | Loss: 0.00004259
Iteration 3/1000 | Loss: 0.00002890
Iteration 4/1000 | Loss: 0.00002710
Iteration 5/1000 | Loss: 0.00002590
Iteration 6/1000 | Loss: 0.00002522
Iteration 7/1000 | Loss: 0.00002460
Iteration 8/1000 | Loss: 0.00002418
Iteration 9/1000 | Loss: 0.00002385
Iteration 10/1000 | Loss: 0.00002359
Iteration 11/1000 | Loss: 0.00002344
Iteration 12/1000 | Loss: 0.00002331
Iteration 13/1000 | Loss: 0.00002325
Iteration 14/1000 | Loss: 0.00002324
Iteration 15/1000 | Loss: 0.00002322
Iteration 16/1000 | Loss: 0.00002319
Iteration 17/1000 | Loss: 0.00002319
Iteration 18/1000 | Loss: 0.00002319
Iteration 19/1000 | Loss: 0.00002319
Iteration 20/1000 | Loss: 0.00002319
Iteration 21/1000 | Loss: 0.00002318
Iteration 22/1000 | Loss: 0.00002308
Iteration 23/1000 | Loss: 0.00002304
Iteration 24/1000 | Loss: 0.00002296
Iteration 25/1000 | Loss: 0.00002294
Iteration 26/1000 | Loss: 0.00002294
Iteration 27/1000 | Loss: 0.00002294
Iteration 28/1000 | Loss: 0.00002293
Iteration 29/1000 | Loss: 0.00002292
Iteration 30/1000 | Loss: 0.00002291
Iteration 31/1000 | Loss: 0.00002291
Iteration 32/1000 | Loss: 0.00002291
Iteration 33/1000 | Loss: 0.00002290
Iteration 34/1000 | Loss: 0.00002290
Iteration 35/1000 | Loss: 0.00002290
Iteration 36/1000 | Loss: 0.00002290
Iteration 37/1000 | Loss: 0.00002290
Iteration 38/1000 | Loss: 0.00002290
Iteration 39/1000 | Loss: 0.00002290
Iteration 40/1000 | Loss: 0.00002290
Iteration 41/1000 | Loss: 0.00002289
Iteration 42/1000 | Loss: 0.00002289
Iteration 43/1000 | Loss: 0.00002289
Iteration 44/1000 | Loss: 0.00002288
Iteration 45/1000 | Loss: 0.00002288
Iteration 46/1000 | Loss: 0.00002287
Iteration 47/1000 | Loss: 0.00002287
Iteration 48/1000 | Loss: 0.00002287
Iteration 49/1000 | Loss: 0.00002287
Iteration 50/1000 | Loss: 0.00002287
Iteration 51/1000 | Loss: 0.00002287
Iteration 52/1000 | Loss: 0.00002287
Iteration 53/1000 | Loss: 0.00002287
Iteration 54/1000 | Loss: 0.00002287
Iteration 55/1000 | Loss: 0.00002287
Iteration 56/1000 | Loss: 0.00002287
Iteration 57/1000 | Loss: 0.00002287
Iteration 58/1000 | Loss: 0.00002286
Iteration 59/1000 | Loss: 0.00002286
Iteration 60/1000 | Loss: 0.00002286
Iteration 61/1000 | Loss: 0.00002286
Iteration 62/1000 | Loss: 0.00002286
Iteration 63/1000 | Loss: 0.00002285
Iteration 64/1000 | Loss: 0.00002285
Iteration 65/1000 | Loss: 0.00002285
Iteration 66/1000 | Loss: 0.00002284
Iteration 67/1000 | Loss: 0.00002284
Iteration 68/1000 | Loss: 0.00002284
Iteration 69/1000 | Loss: 0.00002284
Iteration 70/1000 | Loss: 0.00002284
Iteration 71/1000 | Loss: 0.00002283
Iteration 72/1000 | Loss: 0.00002283
Iteration 73/1000 | Loss: 0.00002283
Iteration 74/1000 | Loss: 0.00002283
Iteration 75/1000 | Loss: 0.00002282
Iteration 76/1000 | Loss: 0.00002282
Iteration 77/1000 | Loss: 0.00002282
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002281
Iteration 80/1000 | Loss: 0.00002281
Iteration 81/1000 | Loss: 0.00002281
Iteration 82/1000 | Loss: 0.00002281
Iteration 83/1000 | Loss: 0.00002281
Iteration 84/1000 | Loss: 0.00002280
Iteration 85/1000 | Loss: 0.00002280
Iteration 86/1000 | Loss: 0.00002280
Iteration 87/1000 | Loss: 0.00002280
Iteration 88/1000 | Loss: 0.00002279
Iteration 89/1000 | Loss: 0.00002279
Iteration 90/1000 | Loss: 0.00002279
Iteration 91/1000 | Loss: 0.00002279
Iteration 92/1000 | Loss: 0.00002279
Iteration 93/1000 | Loss: 0.00002279
Iteration 94/1000 | Loss: 0.00002278
Iteration 95/1000 | Loss: 0.00002278
Iteration 96/1000 | Loss: 0.00002278
Iteration 97/1000 | Loss: 0.00002278
Iteration 98/1000 | Loss: 0.00002278
Iteration 99/1000 | Loss: 0.00002278
Iteration 100/1000 | Loss: 0.00002278
Iteration 101/1000 | Loss: 0.00002278
Iteration 102/1000 | Loss: 0.00002278
Iteration 103/1000 | Loss: 0.00002278
Iteration 104/1000 | Loss: 0.00002278
Iteration 105/1000 | Loss: 0.00002278
Iteration 106/1000 | Loss: 0.00002278
Iteration 107/1000 | Loss: 0.00002277
Iteration 108/1000 | Loss: 0.00002277
Iteration 109/1000 | Loss: 0.00002277
Iteration 110/1000 | Loss: 0.00002277
Iteration 111/1000 | Loss: 0.00002277
Iteration 112/1000 | Loss: 0.00002277
Iteration 113/1000 | Loss: 0.00002277
Iteration 114/1000 | Loss: 0.00002277
Iteration 115/1000 | Loss: 0.00002277
Iteration 116/1000 | Loss: 0.00002277
Iteration 117/1000 | Loss: 0.00002277
Iteration 118/1000 | Loss: 0.00002276
Iteration 119/1000 | Loss: 0.00002276
Iteration 120/1000 | Loss: 0.00002276
Iteration 121/1000 | Loss: 0.00002276
Iteration 122/1000 | Loss: 0.00002276
Iteration 123/1000 | Loss: 0.00002276
Iteration 124/1000 | Loss: 0.00002276
Iteration 125/1000 | Loss: 0.00002276
Iteration 126/1000 | Loss: 0.00002276
Iteration 127/1000 | Loss: 0.00002276
Iteration 128/1000 | Loss: 0.00002276
Iteration 129/1000 | Loss: 0.00002276
Iteration 130/1000 | Loss: 0.00002276
Iteration 131/1000 | Loss: 0.00002276
Iteration 132/1000 | Loss: 0.00002276
Iteration 133/1000 | Loss: 0.00002276
Iteration 134/1000 | Loss: 0.00002276
Iteration 135/1000 | Loss: 0.00002276
Iteration 136/1000 | Loss: 0.00002276
Iteration 137/1000 | Loss: 0.00002276
Iteration 138/1000 | Loss: 0.00002276
Iteration 139/1000 | Loss: 0.00002276
Iteration 140/1000 | Loss: 0.00002276
Iteration 141/1000 | Loss: 0.00002276
Iteration 142/1000 | Loss: 0.00002276
Iteration 143/1000 | Loss: 0.00002276
Iteration 144/1000 | Loss: 0.00002276
Iteration 145/1000 | Loss: 0.00002276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [2.276256782351993e-05, 2.276256782351993e-05, 2.276256782351993e-05, 2.276256782351993e-05, 2.276256782351993e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.276256782351993e-05

Optimization complete. Final v2v error: 3.977019786834717 mm

Highest mean error: 4.376089096069336 mm for frame 87

Lowest mean error: 3.6047160625457764 mm for frame 171

Saving results

Total time: 41.61375904083252
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559083
Iteration 2/25 | Loss: 0.00112891
Iteration 3/25 | Loss: 0.00086206
Iteration 4/25 | Loss: 0.00077854
Iteration 5/25 | Loss: 0.00076034
Iteration 6/25 | Loss: 0.00076388
Iteration 7/25 | Loss: 0.00075650
Iteration 8/25 | Loss: 0.00075449
Iteration 9/25 | Loss: 0.00075342
Iteration 10/25 | Loss: 0.00075160
Iteration 11/25 | Loss: 0.00075025
Iteration 12/25 | Loss: 0.00074975
Iteration 13/25 | Loss: 0.00074960
Iteration 14/25 | Loss: 0.00074960
Iteration 15/25 | Loss: 0.00074959
Iteration 16/25 | Loss: 0.00074959
Iteration 17/25 | Loss: 0.00074959
Iteration 18/25 | Loss: 0.00074959
Iteration 19/25 | Loss: 0.00074959
Iteration 20/25 | Loss: 0.00074959
Iteration 21/25 | Loss: 0.00074959
Iteration 22/25 | Loss: 0.00074959
Iteration 23/25 | Loss: 0.00074959
Iteration 24/25 | Loss: 0.00074959
Iteration 25/25 | Loss: 0.00074959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77018762
Iteration 2/25 | Loss: 0.00046138
Iteration 3/25 | Loss: 0.00046138
Iteration 4/25 | Loss: 0.00046138
Iteration 5/25 | Loss: 0.00046138
Iteration 6/25 | Loss: 0.00046138
Iteration 7/25 | Loss: 0.00046138
Iteration 8/25 | Loss: 0.00046138
Iteration 9/25 | Loss: 0.00046138
Iteration 10/25 | Loss: 0.00046138
Iteration 11/25 | Loss: 0.00046138
Iteration 12/25 | Loss: 0.00046138
Iteration 13/25 | Loss: 0.00046138
Iteration 14/25 | Loss: 0.00046138
Iteration 15/25 | Loss: 0.00046138
Iteration 16/25 | Loss: 0.00046138
Iteration 17/25 | Loss: 0.00046138
Iteration 18/25 | Loss: 0.00046138
Iteration 19/25 | Loss: 0.00046138
Iteration 20/25 | Loss: 0.00046138
Iteration 21/25 | Loss: 0.00046138
Iteration 22/25 | Loss: 0.00046138
Iteration 23/25 | Loss: 0.00046138
Iteration 24/25 | Loss: 0.00046138
Iteration 25/25 | Loss: 0.00046138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046138
Iteration 2/1000 | Loss: 0.00003187
Iteration 3/1000 | Loss: 0.00002255
Iteration 4/1000 | Loss: 0.00002111
Iteration 5/1000 | Loss: 0.00001995
Iteration 6/1000 | Loss: 0.00001933
Iteration 7/1000 | Loss: 0.00001872
Iteration 8/1000 | Loss: 0.00001838
Iteration 9/1000 | Loss: 0.00001811
Iteration 10/1000 | Loss: 0.00001790
Iteration 11/1000 | Loss: 0.00001788
Iteration 12/1000 | Loss: 0.00001779
Iteration 13/1000 | Loss: 0.00001777
Iteration 14/1000 | Loss: 0.00001772
Iteration 15/1000 | Loss: 0.00001764
Iteration 16/1000 | Loss: 0.00001761
Iteration 17/1000 | Loss: 0.00001760
Iteration 18/1000 | Loss: 0.00001760
Iteration 19/1000 | Loss: 0.00001756
Iteration 20/1000 | Loss: 0.00001755
Iteration 21/1000 | Loss: 0.00001755
Iteration 22/1000 | Loss: 0.00001753
Iteration 23/1000 | Loss: 0.00001752
Iteration 24/1000 | Loss: 0.00001752
Iteration 25/1000 | Loss: 0.00001752
Iteration 26/1000 | Loss: 0.00001752
Iteration 27/1000 | Loss: 0.00001752
Iteration 28/1000 | Loss: 0.00001752
Iteration 29/1000 | Loss: 0.00001752
Iteration 30/1000 | Loss: 0.00001752
Iteration 31/1000 | Loss: 0.00001752
Iteration 32/1000 | Loss: 0.00001752
Iteration 33/1000 | Loss: 0.00001751
Iteration 34/1000 | Loss: 0.00001751
Iteration 35/1000 | Loss: 0.00001751
Iteration 36/1000 | Loss: 0.00001751
Iteration 37/1000 | Loss: 0.00001749
Iteration 38/1000 | Loss: 0.00001748
Iteration 39/1000 | Loss: 0.00001747
Iteration 40/1000 | Loss: 0.00001747
Iteration 41/1000 | Loss: 0.00001746
Iteration 42/1000 | Loss: 0.00001745
Iteration 43/1000 | Loss: 0.00001745
Iteration 44/1000 | Loss: 0.00001744
Iteration 45/1000 | Loss: 0.00001744
Iteration 46/1000 | Loss: 0.00001743
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001737
Iteration 49/1000 | Loss: 0.00001736
Iteration 50/1000 | Loss: 0.00001733
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001728
Iteration 53/1000 | Loss: 0.00001726
Iteration 54/1000 | Loss: 0.00001726
Iteration 55/1000 | Loss: 0.00001726
Iteration 56/1000 | Loss: 0.00001726
Iteration 57/1000 | Loss: 0.00001726
Iteration 58/1000 | Loss: 0.00001726
Iteration 59/1000 | Loss: 0.00001726
Iteration 60/1000 | Loss: 0.00001726
Iteration 61/1000 | Loss: 0.00001726
Iteration 62/1000 | Loss: 0.00001726
Iteration 63/1000 | Loss: 0.00001726
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001725
Iteration 67/1000 | Loss: 0.00001725
Iteration 68/1000 | Loss: 0.00001725
Iteration 69/1000 | Loss: 0.00001724
Iteration 70/1000 | Loss: 0.00001724
Iteration 71/1000 | Loss: 0.00001724
Iteration 72/1000 | Loss: 0.00001724
Iteration 73/1000 | Loss: 0.00001724
Iteration 74/1000 | Loss: 0.00001724
Iteration 75/1000 | Loss: 0.00001724
Iteration 76/1000 | Loss: 0.00001724
Iteration 77/1000 | Loss: 0.00001724
Iteration 78/1000 | Loss: 0.00001724
Iteration 79/1000 | Loss: 0.00001724
Iteration 80/1000 | Loss: 0.00001724
Iteration 81/1000 | Loss: 0.00001724
Iteration 82/1000 | Loss: 0.00001724
Iteration 83/1000 | Loss: 0.00001724
Iteration 84/1000 | Loss: 0.00001724
Iteration 85/1000 | Loss: 0.00001724
Iteration 86/1000 | Loss: 0.00001724
Iteration 87/1000 | Loss: 0.00001724
Iteration 88/1000 | Loss: 0.00001724
Iteration 89/1000 | Loss: 0.00001724
Iteration 90/1000 | Loss: 0.00001724
Iteration 91/1000 | Loss: 0.00001724
Iteration 92/1000 | Loss: 0.00001724
Iteration 93/1000 | Loss: 0.00001724
Iteration 94/1000 | Loss: 0.00001724
Iteration 95/1000 | Loss: 0.00001724
Iteration 96/1000 | Loss: 0.00001724
Iteration 97/1000 | Loss: 0.00001724
Iteration 98/1000 | Loss: 0.00001724
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.7235503037227318e-05, 1.7235503037227318e-05, 1.7235503037227318e-05, 1.7235503037227318e-05, 1.7235503037227318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7235503037227318e-05

Optimization complete. Final v2v error: 3.512270450592041 mm

Highest mean error: 4.226999759674072 mm for frame 186

Lowest mean error: 3.21663236618042 mm for frame 214

Saving results

Total time: 55.25371503829956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006907
Iteration 2/25 | Loss: 0.00154773
Iteration 3/25 | Loss: 0.00121256
Iteration 4/25 | Loss: 0.00112227
Iteration 5/25 | Loss: 0.00107665
Iteration 6/25 | Loss: 0.00106416
Iteration 7/25 | Loss: 0.00105958
Iteration 8/25 | Loss: 0.00105817
Iteration 9/25 | Loss: 0.00105786
Iteration 10/25 | Loss: 0.00105786
Iteration 11/25 | Loss: 0.00105786
Iteration 12/25 | Loss: 0.00105786
Iteration 13/25 | Loss: 0.00105786
Iteration 14/25 | Loss: 0.00105786
Iteration 15/25 | Loss: 0.00105786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0010578561341390014, 0.0010578561341390014, 0.0010578561341390014, 0.0010578561341390014, 0.0010578561341390014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010578561341390014

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41220045
Iteration 2/25 | Loss: 0.00123529
Iteration 3/25 | Loss: 0.00123491
Iteration 4/25 | Loss: 0.00123491
Iteration 5/25 | Loss: 0.00123491
Iteration 6/25 | Loss: 0.00123491
Iteration 7/25 | Loss: 0.00123491
Iteration 8/25 | Loss: 0.00123491
Iteration 9/25 | Loss: 0.00123491
Iteration 10/25 | Loss: 0.00123491
Iteration 11/25 | Loss: 0.00123491
Iteration 12/25 | Loss: 0.00123491
Iteration 13/25 | Loss: 0.00123491
Iteration 14/25 | Loss: 0.00123491
Iteration 15/25 | Loss: 0.00123491
Iteration 16/25 | Loss: 0.00123491
Iteration 17/25 | Loss: 0.00123491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012349113821983337, 0.0012349113821983337, 0.0012349113821983337, 0.0012349113821983337, 0.0012349113821983337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012349113821983337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123491
Iteration 2/1000 | Loss: 0.00056176
Iteration 3/1000 | Loss: 0.00016266
Iteration 4/1000 | Loss: 0.00012472
Iteration 5/1000 | Loss: 0.00010592
Iteration 6/1000 | Loss: 0.00063278
Iteration 7/1000 | Loss: 0.00052452
Iteration 8/1000 | Loss: 0.00052873
Iteration 9/1000 | Loss: 0.00039674
Iteration 10/1000 | Loss: 0.00023662
Iteration 11/1000 | Loss: 0.00009386
Iteration 12/1000 | Loss: 0.00046191
Iteration 13/1000 | Loss: 0.00024190
Iteration 14/1000 | Loss: 0.00008837
Iteration 15/1000 | Loss: 0.00008655
Iteration 16/1000 | Loss: 0.00041552
Iteration 17/1000 | Loss: 0.00060163
Iteration 18/1000 | Loss: 0.00144694
Iteration 19/1000 | Loss: 0.00058795
Iteration 20/1000 | Loss: 0.00071020
Iteration 21/1000 | Loss: 0.00186729
Iteration 22/1000 | Loss: 0.00166797
Iteration 23/1000 | Loss: 0.00117633
Iteration 24/1000 | Loss: 0.00085334
Iteration 25/1000 | Loss: 0.00019481
Iteration 26/1000 | Loss: 0.00008002
Iteration 27/1000 | Loss: 0.00007684
Iteration 28/1000 | Loss: 0.00007349
Iteration 29/1000 | Loss: 0.00096710
Iteration 30/1000 | Loss: 0.00040961
Iteration 31/1000 | Loss: 0.00026884
Iteration 32/1000 | Loss: 0.00012435
Iteration 33/1000 | Loss: 0.00006606
Iteration 34/1000 | Loss: 0.00025613
Iteration 35/1000 | Loss: 0.00006248
Iteration 36/1000 | Loss: 0.00023534
Iteration 37/1000 | Loss: 0.00006980
Iteration 38/1000 | Loss: 0.00005698
Iteration 39/1000 | Loss: 0.00005525
Iteration 40/1000 | Loss: 0.00005410
Iteration 41/1000 | Loss: 0.00005322
Iteration 42/1000 | Loss: 0.00005267
Iteration 43/1000 | Loss: 0.00005231
Iteration 44/1000 | Loss: 0.00005196
Iteration 45/1000 | Loss: 0.00005175
Iteration 46/1000 | Loss: 0.00005162
Iteration 47/1000 | Loss: 0.00005159
Iteration 48/1000 | Loss: 0.00005152
Iteration 49/1000 | Loss: 0.00005148
Iteration 50/1000 | Loss: 0.00005147
Iteration 51/1000 | Loss: 0.00005145
Iteration 52/1000 | Loss: 0.00005143
Iteration 53/1000 | Loss: 0.00005143
Iteration 54/1000 | Loss: 0.00005142
Iteration 55/1000 | Loss: 0.00005139
Iteration 56/1000 | Loss: 0.00005136
Iteration 57/1000 | Loss: 0.00005136
Iteration 58/1000 | Loss: 0.00005136
Iteration 59/1000 | Loss: 0.00005135
Iteration 60/1000 | Loss: 0.00005134
Iteration 61/1000 | Loss: 0.00005134
Iteration 62/1000 | Loss: 0.00005133
Iteration 63/1000 | Loss: 0.00005133
Iteration 64/1000 | Loss: 0.00005133
Iteration 65/1000 | Loss: 0.00005133
Iteration 66/1000 | Loss: 0.00005132
Iteration 67/1000 | Loss: 0.00005130
Iteration 68/1000 | Loss: 0.00005130
Iteration 69/1000 | Loss: 0.00005129
Iteration 70/1000 | Loss: 0.00005128
Iteration 71/1000 | Loss: 0.00005128
Iteration 72/1000 | Loss: 0.00005127
Iteration 73/1000 | Loss: 0.00005127
Iteration 74/1000 | Loss: 0.00005127
Iteration 75/1000 | Loss: 0.00005126
Iteration 76/1000 | Loss: 0.00005125
Iteration 77/1000 | Loss: 0.00005125
Iteration 78/1000 | Loss: 0.00005124
Iteration 79/1000 | Loss: 0.00005124
Iteration 80/1000 | Loss: 0.00005123
Iteration 81/1000 | Loss: 0.00005123
Iteration 82/1000 | Loss: 0.00005122
Iteration 83/1000 | Loss: 0.00005121
Iteration 84/1000 | Loss: 0.00005121
Iteration 85/1000 | Loss: 0.00005120
Iteration 86/1000 | Loss: 0.00005120
Iteration 87/1000 | Loss: 0.00005119
Iteration 88/1000 | Loss: 0.00005119
Iteration 89/1000 | Loss: 0.00005119
Iteration 90/1000 | Loss: 0.00005118
Iteration 91/1000 | Loss: 0.00005118
Iteration 92/1000 | Loss: 0.00005118
Iteration 93/1000 | Loss: 0.00005117
Iteration 94/1000 | Loss: 0.00005117
Iteration 95/1000 | Loss: 0.00005117
Iteration 96/1000 | Loss: 0.00005116
Iteration 97/1000 | Loss: 0.00005116
Iteration 98/1000 | Loss: 0.00005116
Iteration 99/1000 | Loss: 0.00005115
Iteration 100/1000 | Loss: 0.00005115
Iteration 101/1000 | Loss: 0.00005114
Iteration 102/1000 | Loss: 0.00005114
Iteration 103/1000 | Loss: 0.00005114
Iteration 104/1000 | Loss: 0.00005113
Iteration 105/1000 | Loss: 0.00005113
Iteration 106/1000 | Loss: 0.00005113
Iteration 107/1000 | Loss: 0.00005112
Iteration 108/1000 | Loss: 0.00005112
Iteration 109/1000 | Loss: 0.00005112
Iteration 110/1000 | Loss: 0.00005112
Iteration 111/1000 | Loss: 0.00005112
Iteration 112/1000 | Loss: 0.00005112
Iteration 113/1000 | Loss: 0.00005111
Iteration 114/1000 | Loss: 0.00005111
Iteration 115/1000 | Loss: 0.00005111
Iteration 116/1000 | Loss: 0.00005111
Iteration 117/1000 | Loss: 0.00005110
Iteration 118/1000 | Loss: 0.00005110
Iteration 119/1000 | Loss: 0.00005110
Iteration 120/1000 | Loss: 0.00005110
Iteration 121/1000 | Loss: 0.00005110
Iteration 122/1000 | Loss: 0.00005110
Iteration 123/1000 | Loss: 0.00005109
Iteration 124/1000 | Loss: 0.00005109
Iteration 125/1000 | Loss: 0.00005109
Iteration 126/1000 | Loss: 0.00005109
Iteration 127/1000 | Loss: 0.00005109
Iteration 128/1000 | Loss: 0.00005109
Iteration 129/1000 | Loss: 0.00005109
Iteration 130/1000 | Loss: 0.00005109
Iteration 131/1000 | Loss: 0.00005109
Iteration 132/1000 | Loss: 0.00005109
Iteration 133/1000 | Loss: 0.00005109
Iteration 134/1000 | Loss: 0.00005109
Iteration 135/1000 | Loss: 0.00005108
Iteration 136/1000 | Loss: 0.00005108
Iteration 137/1000 | Loss: 0.00005108
Iteration 138/1000 | Loss: 0.00005108
Iteration 139/1000 | Loss: 0.00005108
Iteration 140/1000 | Loss: 0.00005108
Iteration 141/1000 | Loss: 0.00005108
Iteration 142/1000 | Loss: 0.00005108
Iteration 143/1000 | Loss: 0.00005108
Iteration 144/1000 | Loss: 0.00005108
Iteration 145/1000 | Loss: 0.00005108
Iteration 146/1000 | Loss: 0.00005108
Iteration 147/1000 | Loss: 0.00005108
Iteration 148/1000 | Loss: 0.00005108
Iteration 149/1000 | Loss: 0.00005108
Iteration 150/1000 | Loss: 0.00005108
Iteration 151/1000 | Loss: 0.00005108
Iteration 152/1000 | Loss: 0.00005108
Iteration 153/1000 | Loss: 0.00005108
Iteration 154/1000 | Loss: 0.00005108
Iteration 155/1000 | Loss: 0.00005108
Iteration 156/1000 | Loss: 0.00005108
Iteration 157/1000 | Loss: 0.00005108
Iteration 158/1000 | Loss: 0.00005108
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [5.1083145081065595e-05, 5.1083145081065595e-05, 5.1083145081065595e-05, 5.1083145081065595e-05, 5.1083145081065595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.1083145081065595e-05

Optimization complete. Final v2v error: 5.665143013000488 mm

Highest mean error: 7.829646587371826 mm for frame 181

Lowest mean error: 3.7012710571289062 mm for frame 39

Saving results

Total time: 104.79541778564453
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00517476
Iteration 2/25 | Loss: 0.00096685
Iteration 3/25 | Loss: 0.00081021
Iteration 4/25 | Loss: 0.00078063
Iteration 5/25 | Loss: 0.00077231
Iteration 6/25 | Loss: 0.00077080
Iteration 7/25 | Loss: 0.00077080
Iteration 8/25 | Loss: 0.00077080
Iteration 9/25 | Loss: 0.00077080
Iteration 10/25 | Loss: 0.00077080
Iteration 11/25 | Loss: 0.00077080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007708044140599668, 0.0007708044140599668, 0.0007708044140599668, 0.0007708044140599668, 0.0007708044140599668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007708044140599668

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47910249
Iteration 2/25 | Loss: 0.00039392
Iteration 3/25 | Loss: 0.00039388
Iteration 4/25 | Loss: 0.00039388
Iteration 5/25 | Loss: 0.00039388
Iteration 6/25 | Loss: 0.00039388
Iteration 7/25 | Loss: 0.00039388
Iteration 8/25 | Loss: 0.00039388
Iteration 9/25 | Loss: 0.00039388
Iteration 10/25 | Loss: 0.00039388
Iteration 11/25 | Loss: 0.00039388
Iteration 12/25 | Loss: 0.00039388
Iteration 13/25 | Loss: 0.00039388
Iteration 14/25 | Loss: 0.00039388
Iteration 15/25 | Loss: 0.00039388
Iteration 16/25 | Loss: 0.00039388
Iteration 17/25 | Loss: 0.00039388
Iteration 18/25 | Loss: 0.00039388
Iteration 19/25 | Loss: 0.00039388
Iteration 20/25 | Loss: 0.00039388
Iteration 21/25 | Loss: 0.00039388
Iteration 22/25 | Loss: 0.00039388
Iteration 23/25 | Loss: 0.00039388
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0003938803856726736, 0.0003938803856726736, 0.0003938803856726736, 0.0003938803856726736, 0.0003938803856726736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003938803856726736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039388
Iteration 2/1000 | Loss: 0.00002883
Iteration 3/1000 | Loss: 0.00001865
Iteration 4/1000 | Loss: 0.00001619
Iteration 5/1000 | Loss: 0.00001477
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001354
Iteration 8/1000 | Loss: 0.00001318
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001313
Iteration 11/1000 | Loss: 0.00001313
Iteration 12/1000 | Loss: 0.00001289
Iteration 13/1000 | Loss: 0.00001269
Iteration 14/1000 | Loss: 0.00001264
Iteration 15/1000 | Loss: 0.00001261
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001258
Iteration 18/1000 | Loss: 0.00001257
Iteration 19/1000 | Loss: 0.00001257
Iteration 20/1000 | Loss: 0.00001256
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001251
Iteration 24/1000 | Loss: 0.00001251
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001250
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001249
Iteration 29/1000 | Loss: 0.00001248
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001241
Iteration 35/1000 | Loss: 0.00001240
Iteration 36/1000 | Loss: 0.00001240
Iteration 37/1000 | Loss: 0.00001240
Iteration 38/1000 | Loss: 0.00001239
Iteration 39/1000 | Loss: 0.00001238
Iteration 40/1000 | Loss: 0.00001238
Iteration 41/1000 | Loss: 0.00001237
Iteration 42/1000 | Loss: 0.00001237
Iteration 43/1000 | Loss: 0.00001237
Iteration 44/1000 | Loss: 0.00001236
Iteration 45/1000 | Loss: 0.00001235
Iteration 46/1000 | Loss: 0.00001234
Iteration 47/1000 | Loss: 0.00001234
Iteration 48/1000 | Loss: 0.00001234
Iteration 49/1000 | Loss: 0.00001234
Iteration 50/1000 | Loss: 0.00001233
Iteration 51/1000 | Loss: 0.00001233
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001232
Iteration 54/1000 | Loss: 0.00001232
Iteration 55/1000 | Loss: 0.00001232
Iteration 56/1000 | Loss: 0.00001232
Iteration 57/1000 | Loss: 0.00001232
Iteration 58/1000 | Loss: 0.00001231
Iteration 59/1000 | Loss: 0.00001231
Iteration 60/1000 | Loss: 0.00001231
Iteration 61/1000 | Loss: 0.00001231
Iteration 62/1000 | Loss: 0.00001230
Iteration 63/1000 | Loss: 0.00001230
Iteration 64/1000 | Loss: 0.00001230
Iteration 65/1000 | Loss: 0.00001229
Iteration 66/1000 | Loss: 0.00001229
Iteration 67/1000 | Loss: 0.00001229
Iteration 68/1000 | Loss: 0.00001229
Iteration 69/1000 | Loss: 0.00001228
Iteration 70/1000 | Loss: 0.00001228
Iteration 71/1000 | Loss: 0.00001228
Iteration 72/1000 | Loss: 0.00001228
Iteration 73/1000 | Loss: 0.00001228
Iteration 74/1000 | Loss: 0.00001228
Iteration 75/1000 | Loss: 0.00001228
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001227
Iteration 79/1000 | Loss: 0.00001227
Iteration 80/1000 | Loss: 0.00001227
Iteration 81/1000 | Loss: 0.00001227
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001226
Iteration 86/1000 | Loss: 0.00001226
Iteration 87/1000 | Loss: 0.00001226
Iteration 88/1000 | Loss: 0.00001226
Iteration 89/1000 | Loss: 0.00001225
Iteration 90/1000 | Loss: 0.00001225
Iteration 91/1000 | Loss: 0.00001225
Iteration 92/1000 | Loss: 0.00001225
Iteration 93/1000 | Loss: 0.00001225
Iteration 94/1000 | Loss: 0.00001225
Iteration 95/1000 | Loss: 0.00001225
Iteration 96/1000 | Loss: 0.00001224
Iteration 97/1000 | Loss: 0.00001224
Iteration 98/1000 | Loss: 0.00001224
Iteration 99/1000 | Loss: 0.00001224
Iteration 100/1000 | Loss: 0.00001223
Iteration 101/1000 | Loss: 0.00001223
Iteration 102/1000 | Loss: 0.00001223
Iteration 103/1000 | Loss: 0.00001223
Iteration 104/1000 | Loss: 0.00001223
Iteration 105/1000 | Loss: 0.00001223
Iteration 106/1000 | Loss: 0.00001223
Iteration 107/1000 | Loss: 0.00001223
Iteration 108/1000 | Loss: 0.00001223
Iteration 109/1000 | Loss: 0.00001223
Iteration 110/1000 | Loss: 0.00001222
Iteration 111/1000 | Loss: 0.00001222
Iteration 112/1000 | Loss: 0.00001222
Iteration 113/1000 | Loss: 0.00001222
Iteration 114/1000 | Loss: 0.00001221
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001221
Iteration 117/1000 | Loss: 0.00001221
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001221
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001220
Iteration 127/1000 | Loss: 0.00001220
Iteration 128/1000 | Loss: 0.00001220
Iteration 129/1000 | Loss: 0.00001220
Iteration 130/1000 | Loss: 0.00001220
Iteration 131/1000 | Loss: 0.00001220
Iteration 132/1000 | Loss: 0.00001219
Iteration 133/1000 | Loss: 0.00001219
Iteration 134/1000 | Loss: 0.00001219
Iteration 135/1000 | Loss: 0.00001219
Iteration 136/1000 | Loss: 0.00001219
Iteration 137/1000 | Loss: 0.00001219
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001218
Iteration 141/1000 | Loss: 0.00001218
Iteration 142/1000 | Loss: 0.00001218
Iteration 143/1000 | Loss: 0.00001218
Iteration 144/1000 | Loss: 0.00001218
Iteration 145/1000 | Loss: 0.00001218
Iteration 146/1000 | Loss: 0.00001218
Iteration 147/1000 | Loss: 0.00001218
Iteration 148/1000 | Loss: 0.00001218
Iteration 149/1000 | Loss: 0.00001218
Iteration 150/1000 | Loss: 0.00001218
Iteration 151/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.2180415069451556e-05, 1.2180415069451556e-05, 1.2180415069451556e-05, 1.2180415069451556e-05, 1.2180415069451556e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2180415069451556e-05

Optimization complete. Final v2v error: 2.95236873626709 mm

Highest mean error: 3.116156816482544 mm for frame 238

Lowest mean error: 2.8150742053985596 mm for frame 207

Saving results

Total time: 41.98362731933594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_eric_posed_018/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_eric_posed_018/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00872014
Iteration 2/25 | Loss: 0.00129662
Iteration 3/25 | Loss: 0.00086393
Iteration 4/25 | Loss: 0.00077353
Iteration 5/25 | Loss: 0.00075860
Iteration 6/25 | Loss: 0.00075574
Iteration 7/25 | Loss: 0.00075517
Iteration 8/25 | Loss: 0.00075517
Iteration 9/25 | Loss: 0.00075517
Iteration 10/25 | Loss: 0.00075517
Iteration 11/25 | Loss: 0.00075517
Iteration 12/25 | Loss: 0.00075517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007551659364253283, 0.0007551659364253283, 0.0007551659364253283, 0.0007551659364253283, 0.0007551659364253283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007551659364253283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46647871
Iteration 2/25 | Loss: 0.00050599
Iteration 3/25 | Loss: 0.00050598
Iteration 4/25 | Loss: 0.00050598
Iteration 5/25 | Loss: 0.00050598
Iteration 6/25 | Loss: 0.00050598
Iteration 7/25 | Loss: 0.00050598
Iteration 8/25 | Loss: 0.00050598
Iteration 9/25 | Loss: 0.00050598
Iteration 10/25 | Loss: 0.00050598
Iteration 11/25 | Loss: 0.00050598
Iteration 12/25 | Loss: 0.00050598
Iteration 13/25 | Loss: 0.00050598
Iteration 14/25 | Loss: 0.00050598
Iteration 15/25 | Loss: 0.00050598
Iteration 16/25 | Loss: 0.00050598
Iteration 17/25 | Loss: 0.00050598
Iteration 18/25 | Loss: 0.00050598
Iteration 19/25 | Loss: 0.00050598
Iteration 20/25 | Loss: 0.00050598
Iteration 21/25 | Loss: 0.00050598
Iteration 22/25 | Loss: 0.00050598
Iteration 23/25 | Loss: 0.00050598
Iteration 24/25 | Loss: 0.00050598
Iteration 25/25 | Loss: 0.00050598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050598
Iteration 2/1000 | Loss: 0.00002689
Iteration 3/1000 | Loss: 0.00001889
Iteration 4/1000 | Loss: 0.00001732
Iteration 5/1000 | Loss: 0.00001635
Iteration 6/1000 | Loss: 0.00001556
Iteration 7/1000 | Loss: 0.00001519
Iteration 8/1000 | Loss: 0.00001481
Iteration 9/1000 | Loss: 0.00001459
Iteration 10/1000 | Loss: 0.00001450
Iteration 11/1000 | Loss: 0.00001436
Iteration 12/1000 | Loss: 0.00001434
Iteration 13/1000 | Loss: 0.00001425
Iteration 14/1000 | Loss: 0.00001418
Iteration 15/1000 | Loss: 0.00001417
Iteration 16/1000 | Loss: 0.00001416
Iteration 17/1000 | Loss: 0.00001416
Iteration 18/1000 | Loss: 0.00001415
Iteration 19/1000 | Loss: 0.00001414
Iteration 20/1000 | Loss: 0.00001413
Iteration 21/1000 | Loss: 0.00001413
Iteration 22/1000 | Loss: 0.00001411
Iteration 23/1000 | Loss: 0.00001411
Iteration 24/1000 | Loss: 0.00001411
Iteration 25/1000 | Loss: 0.00001407
Iteration 26/1000 | Loss: 0.00001404
Iteration 27/1000 | Loss: 0.00001403
Iteration 28/1000 | Loss: 0.00001402
Iteration 29/1000 | Loss: 0.00001402
Iteration 30/1000 | Loss: 0.00001401
Iteration 31/1000 | Loss: 0.00001401
Iteration 32/1000 | Loss: 0.00001401
Iteration 33/1000 | Loss: 0.00001398
Iteration 34/1000 | Loss: 0.00001398
Iteration 35/1000 | Loss: 0.00001398
Iteration 36/1000 | Loss: 0.00001398
Iteration 37/1000 | Loss: 0.00001397
Iteration 38/1000 | Loss: 0.00001396
Iteration 39/1000 | Loss: 0.00001396
Iteration 40/1000 | Loss: 0.00001395
Iteration 41/1000 | Loss: 0.00001395
Iteration 42/1000 | Loss: 0.00001395
Iteration 43/1000 | Loss: 0.00001394
Iteration 44/1000 | Loss: 0.00001394
Iteration 45/1000 | Loss: 0.00001394
Iteration 46/1000 | Loss: 0.00001393
Iteration 47/1000 | Loss: 0.00001393
Iteration 48/1000 | Loss: 0.00001393
Iteration 49/1000 | Loss: 0.00001393
Iteration 50/1000 | Loss: 0.00001393
Iteration 51/1000 | Loss: 0.00001392
Iteration 52/1000 | Loss: 0.00001392
Iteration 53/1000 | Loss: 0.00001392
Iteration 54/1000 | Loss: 0.00001392
Iteration 55/1000 | Loss: 0.00001392
Iteration 56/1000 | Loss: 0.00001392
Iteration 57/1000 | Loss: 0.00001392
Iteration 58/1000 | Loss: 0.00001392
Iteration 59/1000 | Loss: 0.00001392
Iteration 60/1000 | Loss: 0.00001392
Iteration 61/1000 | Loss: 0.00001391
Iteration 62/1000 | Loss: 0.00001391
Iteration 63/1000 | Loss: 0.00001391
Iteration 64/1000 | Loss: 0.00001391
Iteration 65/1000 | Loss: 0.00001391
Iteration 66/1000 | Loss: 0.00001391
Iteration 67/1000 | Loss: 0.00001391
Iteration 68/1000 | Loss: 0.00001391
Iteration 69/1000 | Loss: 0.00001391
Iteration 70/1000 | Loss: 0.00001390
Iteration 71/1000 | Loss: 0.00001390
Iteration 72/1000 | Loss: 0.00001390
Iteration 73/1000 | Loss: 0.00001390
Iteration 74/1000 | Loss: 0.00001390
Iteration 75/1000 | Loss: 0.00001390
Iteration 76/1000 | Loss: 0.00001390
Iteration 77/1000 | Loss: 0.00001390
Iteration 78/1000 | Loss: 0.00001390
Iteration 79/1000 | Loss: 0.00001390
Iteration 80/1000 | Loss: 0.00001389
Iteration 81/1000 | Loss: 0.00001389
Iteration 82/1000 | Loss: 0.00001389
Iteration 83/1000 | Loss: 0.00001389
Iteration 84/1000 | Loss: 0.00001389
Iteration 85/1000 | Loss: 0.00001389
Iteration 86/1000 | Loss: 0.00001389
Iteration 87/1000 | Loss: 0.00001389
Iteration 88/1000 | Loss: 0.00001389
Iteration 89/1000 | Loss: 0.00001389
Iteration 90/1000 | Loss: 0.00001389
Iteration 91/1000 | Loss: 0.00001389
Iteration 92/1000 | Loss: 0.00001388
Iteration 93/1000 | Loss: 0.00001388
Iteration 94/1000 | Loss: 0.00001388
Iteration 95/1000 | Loss: 0.00001388
Iteration 96/1000 | Loss: 0.00001388
Iteration 97/1000 | Loss: 0.00001388
Iteration 98/1000 | Loss: 0.00001388
Iteration 99/1000 | Loss: 0.00001388
Iteration 100/1000 | Loss: 0.00001387
Iteration 101/1000 | Loss: 0.00001387
Iteration 102/1000 | Loss: 0.00001387
Iteration 103/1000 | Loss: 0.00001387
Iteration 104/1000 | Loss: 0.00001387
Iteration 105/1000 | Loss: 0.00001387
Iteration 106/1000 | Loss: 0.00001387
Iteration 107/1000 | Loss: 0.00001387
Iteration 108/1000 | Loss: 0.00001387
Iteration 109/1000 | Loss: 0.00001387
Iteration 110/1000 | Loss: 0.00001387
Iteration 111/1000 | Loss: 0.00001387
Iteration 112/1000 | Loss: 0.00001387
Iteration 113/1000 | Loss: 0.00001387
Iteration 114/1000 | Loss: 0.00001387
Iteration 115/1000 | Loss: 0.00001386
Iteration 116/1000 | Loss: 0.00001386
Iteration 117/1000 | Loss: 0.00001386
Iteration 118/1000 | Loss: 0.00001386
Iteration 119/1000 | Loss: 0.00001386
Iteration 120/1000 | Loss: 0.00001386
Iteration 121/1000 | Loss: 0.00001386
Iteration 122/1000 | Loss: 0.00001386
Iteration 123/1000 | Loss: 0.00001386
Iteration 124/1000 | Loss: 0.00001386
Iteration 125/1000 | Loss: 0.00001386
Iteration 126/1000 | Loss: 0.00001386
Iteration 127/1000 | Loss: 0.00001385
Iteration 128/1000 | Loss: 0.00001385
Iteration 129/1000 | Loss: 0.00001385
Iteration 130/1000 | Loss: 0.00001385
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001385
Iteration 133/1000 | Loss: 0.00001385
Iteration 134/1000 | Loss: 0.00001385
Iteration 135/1000 | Loss: 0.00001385
Iteration 136/1000 | Loss: 0.00001385
Iteration 137/1000 | Loss: 0.00001385
Iteration 138/1000 | Loss: 0.00001385
Iteration 139/1000 | Loss: 0.00001385
Iteration 140/1000 | Loss: 0.00001384
Iteration 141/1000 | Loss: 0.00001384
Iteration 142/1000 | Loss: 0.00001384
Iteration 143/1000 | Loss: 0.00001384
Iteration 144/1000 | Loss: 0.00001384
Iteration 145/1000 | Loss: 0.00001384
Iteration 146/1000 | Loss: 0.00001384
Iteration 147/1000 | Loss: 0.00001384
Iteration 148/1000 | Loss: 0.00001384
Iteration 149/1000 | Loss: 0.00001384
Iteration 150/1000 | Loss: 0.00001384
Iteration 151/1000 | Loss: 0.00001384
Iteration 152/1000 | Loss: 0.00001384
Iteration 153/1000 | Loss: 0.00001384
Iteration 154/1000 | Loss: 0.00001384
Iteration 155/1000 | Loss: 0.00001384
Iteration 156/1000 | Loss: 0.00001383
Iteration 157/1000 | Loss: 0.00001383
Iteration 158/1000 | Loss: 0.00001383
Iteration 159/1000 | Loss: 0.00001383
Iteration 160/1000 | Loss: 0.00001383
Iteration 161/1000 | Loss: 0.00001383
Iteration 162/1000 | Loss: 0.00001383
Iteration 163/1000 | Loss: 0.00001383
Iteration 164/1000 | Loss: 0.00001382
Iteration 165/1000 | Loss: 0.00001382
Iteration 166/1000 | Loss: 0.00001382
Iteration 167/1000 | Loss: 0.00001382
Iteration 168/1000 | Loss: 0.00001382
Iteration 169/1000 | Loss: 0.00001382
Iteration 170/1000 | Loss: 0.00001381
Iteration 171/1000 | Loss: 0.00001381
Iteration 172/1000 | Loss: 0.00001381
Iteration 173/1000 | Loss: 0.00001381
Iteration 174/1000 | Loss: 0.00001381
Iteration 175/1000 | Loss: 0.00001381
Iteration 176/1000 | Loss: 0.00001381
Iteration 177/1000 | Loss: 0.00001381
Iteration 178/1000 | Loss: 0.00001381
Iteration 179/1000 | Loss: 0.00001381
Iteration 180/1000 | Loss: 0.00001381
Iteration 181/1000 | Loss: 0.00001381
Iteration 182/1000 | Loss: 0.00001380
Iteration 183/1000 | Loss: 0.00001380
Iteration 184/1000 | Loss: 0.00001380
Iteration 185/1000 | Loss: 0.00001380
Iteration 186/1000 | Loss: 0.00001380
Iteration 187/1000 | Loss: 0.00001380
Iteration 188/1000 | Loss: 0.00001380
Iteration 189/1000 | Loss: 0.00001380
Iteration 190/1000 | Loss: 0.00001380
Iteration 191/1000 | Loss: 0.00001380
Iteration 192/1000 | Loss: 0.00001380
Iteration 193/1000 | Loss: 0.00001380
Iteration 194/1000 | Loss: 0.00001380
Iteration 195/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.3798384316032752e-05, 1.3798384316032752e-05, 1.3798384316032752e-05, 1.3798384316032752e-05, 1.3798384316032752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3798384316032752e-05

Optimization complete. Final v2v error: 3.1544535160064697 mm

Highest mean error: 3.8560309410095215 mm for frame 53

Lowest mean error: 2.672989845275879 mm for frame 239

Saving results

Total time: 47.243659257888794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873613
Iteration 2/25 | Loss: 0.00164964
Iteration 3/25 | Loss: 0.00137998
Iteration 4/25 | Loss: 0.00134966
Iteration 5/25 | Loss: 0.00134498
Iteration 6/25 | Loss: 0.00134391
Iteration 7/25 | Loss: 0.00134391
Iteration 8/25 | Loss: 0.00134391
Iteration 9/25 | Loss: 0.00134391
Iteration 10/25 | Loss: 0.00134391
Iteration 11/25 | Loss: 0.00134391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013439131435006857, 0.0013439131435006857, 0.0013439131435006857, 0.0013439131435006857, 0.0013439131435006857]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013439131435006857

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17517245
Iteration 2/25 | Loss: 0.00144109
Iteration 3/25 | Loss: 0.00144109
Iteration 4/25 | Loss: 0.00144108
Iteration 5/25 | Loss: 0.00144108
Iteration 6/25 | Loss: 0.00144108
Iteration 7/25 | Loss: 0.00144108
Iteration 8/25 | Loss: 0.00144108
Iteration 9/25 | Loss: 0.00144108
Iteration 10/25 | Loss: 0.00144108
Iteration 11/25 | Loss: 0.00144108
Iteration 12/25 | Loss: 0.00144108
Iteration 13/25 | Loss: 0.00144108
Iteration 14/25 | Loss: 0.00144108
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014410828007385135, 0.0014410828007385135, 0.0014410828007385135, 0.0014410828007385135, 0.0014410828007385135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014410828007385135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144108
Iteration 2/1000 | Loss: 0.00006373
Iteration 3/1000 | Loss: 0.00003266
Iteration 4/1000 | Loss: 0.00002261
Iteration 5/1000 | Loss: 0.00001968
Iteration 6/1000 | Loss: 0.00001851
Iteration 7/1000 | Loss: 0.00001796
Iteration 8/1000 | Loss: 0.00001755
Iteration 9/1000 | Loss: 0.00001720
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001684
Iteration 12/1000 | Loss: 0.00001678
Iteration 13/1000 | Loss: 0.00001674
Iteration 14/1000 | Loss: 0.00001667
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001665
Iteration 17/1000 | Loss: 0.00001663
Iteration 18/1000 | Loss: 0.00001663
Iteration 19/1000 | Loss: 0.00001662
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001650
Iteration 22/1000 | Loss: 0.00001649
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001646
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001645
Iteration 28/1000 | Loss: 0.00001645
Iteration 29/1000 | Loss: 0.00001644
Iteration 30/1000 | Loss: 0.00001643
Iteration 31/1000 | Loss: 0.00001643
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001643
Iteration 35/1000 | Loss: 0.00001643
Iteration 36/1000 | Loss: 0.00001642
Iteration 37/1000 | Loss: 0.00001640
Iteration 38/1000 | Loss: 0.00001639
Iteration 39/1000 | Loss: 0.00001639
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001638
Iteration 42/1000 | Loss: 0.00001638
Iteration 43/1000 | Loss: 0.00001638
Iteration 44/1000 | Loss: 0.00001637
Iteration 45/1000 | Loss: 0.00001637
Iteration 46/1000 | Loss: 0.00001636
Iteration 47/1000 | Loss: 0.00001636
Iteration 48/1000 | Loss: 0.00001636
Iteration 49/1000 | Loss: 0.00001636
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001635
Iteration 52/1000 | Loss: 0.00001635
Iteration 53/1000 | Loss: 0.00001634
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001634
Iteration 57/1000 | Loss: 0.00001634
Iteration 58/1000 | Loss: 0.00001634
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001633
Iteration 62/1000 | Loss: 0.00001633
Iteration 63/1000 | Loss: 0.00001632
Iteration 64/1000 | Loss: 0.00001632
Iteration 65/1000 | Loss: 0.00001632
Iteration 66/1000 | Loss: 0.00001631
Iteration 67/1000 | Loss: 0.00001631
Iteration 68/1000 | Loss: 0.00001631
Iteration 69/1000 | Loss: 0.00001630
Iteration 70/1000 | Loss: 0.00001630
Iteration 71/1000 | Loss: 0.00001630
Iteration 72/1000 | Loss: 0.00001630
Iteration 73/1000 | Loss: 0.00001630
Iteration 74/1000 | Loss: 0.00001630
Iteration 75/1000 | Loss: 0.00001629
Iteration 76/1000 | Loss: 0.00001629
Iteration 77/1000 | Loss: 0.00001629
Iteration 78/1000 | Loss: 0.00001627
Iteration 79/1000 | Loss: 0.00001627
Iteration 80/1000 | Loss: 0.00001627
Iteration 81/1000 | Loss: 0.00001627
Iteration 82/1000 | Loss: 0.00001627
Iteration 83/1000 | Loss: 0.00001627
Iteration 84/1000 | Loss: 0.00001627
Iteration 85/1000 | Loss: 0.00001627
Iteration 86/1000 | Loss: 0.00001627
Iteration 87/1000 | Loss: 0.00001626
Iteration 88/1000 | Loss: 0.00001625
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001621
Iteration 95/1000 | Loss: 0.00001620
Iteration 96/1000 | Loss: 0.00001620
Iteration 97/1000 | Loss: 0.00001620
Iteration 98/1000 | Loss: 0.00001619
Iteration 99/1000 | Loss: 0.00001619
Iteration 100/1000 | Loss: 0.00001618
Iteration 101/1000 | Loss: 0.00001618
Iteration 102/1000 | Loss: 0.00001618
Iteration 103/1000 | Loss: 0.00001618
Iteration 104/1000 | Loss: 0.00001617
Iteration 105/1000 | Loss: 0.00001617
Iteration 106/1000 | Loss: 0.00001617
Iteration 107/1000 | Loss: 0.00001617
Iteration 108/1000 | Loss: 0.00001616
Iteration 109/1000 | Loss: 0.00001616
Iteration 110/1000 | Loss: 0.00001616
Iteration 111/1000 | Loss: 0.00001616
Iteration 112/1000 | Loss: 0.00001616
Iteration 113/1000 | Loss: 0.00001616
Iteration 114/1000 | Loss: 0.00001616
Iteration 115/1000 | Loss: 0.00001616
Iteration 116/1000 | Loss: 0.00001616
Iteration 117/1000 | Loss: 0.00001616
Iteration 118/1000 | Loss: 0.00001616
Iteration 119/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.615793189557735e-05, 1.615793189557735e-05, 1.615793189557735e-05, 1.615793189557735e-05, 1.615793189557735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.615793189557735e-05

Optimization complete. Final v2v error: 3.4123709201812744 mm

Highest mean error: 4.101418972015381 mm for frame 89

Lowest mean error: 3.0517988204956055 mm for frame 69

Saving results

Total time: 42.30418801307678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957480
Iteration 2/25 | Loss: 0.00211065
Iteration 3/25 | Loss: 0.00156786
Iteration 4/25 | Loss: 0.00148602
Iteration 5/25 | Loss: 0.00146559
Iteration 6/25 | Loss: 0.00141716
Iteration 7/25 | Loss: 0.00139694
Iteration 8/25 | Loss: 0.00139299
Iteration 9/25 | Loss: 0.00139178
Iteration 10/25 | Loss: 0.00139142
Iteration 11/25 | Loss: 0.00139117
Iteration 12/25 | Loss: 0.00139104
Iteration 13/25 | Loss: 0.00139090
Iteration 14/25 | Loss: 0.00139159
Iteration 15/25 | Loss: 0.00138911
Iteration 16/25 | Loss: 0.00138774
Iteration 17/25 | Loss: 0.00138743
Iteration 18/25 | Loss: 0.00138740
Iteration 19/25 | Loss: 0.00138739
Iteration 20/25 | Loss: 0.00138739
Iteration 21/25 | Loss: 0.00138739
Iteration 22/25 | Loss: 0.00138739
Iteration 23/25 | Loss: 0.00138739
Iteration 24/25 | Loss: 0.00138739
Iteration 25/25 | Loss: 0.00138739

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43947101
Iteration 2/25 | Loss: 0.00124067
Iteration 3/25 | Loss: 0.00124066
Iteration 4/25 | Loss: 0.00124066
Iteration 5/25 | Loss: 0.00124066
Iteration 6/25 | Loss: 0.00124066
Iteration 7/25 | Loss: 0.00124066
Iteration 8/25 | Loss: 0.00124066
Iteration 9/25 | Loss: 0.00124066
Iteration 10/25 | Loss: 0.00124066
Iteration 11/25 | Loss: 0.00124066
Iteration 12/25 | Loss: 0.00124066
Iteration 13/25 | Loss: 0.00124066
Iteration 14/25 | Loss: 0.00124066
Iteration 15/25 | Loss: 0.00124066
Iteration 16/25 | Loss: 0.00124066
Iteration 17/25 | Loss: 0.00124066
Iteration 18/25 | Loss: 0.00124066
Iteration 19/25 | Loss: 0.00124066
Iteration 20/25 | Loss: 0.00124066
Iteration 21/25 | Loss: 0.00124066
Iteration 22/25 | Loss: 0.00124066
Iteration 23/25 | Loss: 0.00124066
Iteration 24/25 | Loss: 0.00124066
Iteration 25/25 | Loss: 0.00124066

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124066
Iteration 2/1000 | Loss: 0.00006218
Iteration 3/1000 | Loss: 0.00003939
Iteration 4/1000 | Loss: 0.00003236
Iteration 5/1000 | Loss: 0.00002950
Iteration 6/1000 | Loss: 0.00002827
Iteration 7/1000 | Loss: 0.00002735
Iteration 8/1000 | Loss: 0.00002691
Iteration 9/1000 | Loss: 0.00002659
Iteration 10/1000 | Loss: 0.00002637
Iteration 11/1000 | Loss: 0.00002609
Iteration 12/1000 | Loss: 0.00002594
Iteration 13/1000 | Loss: 0.00002575
Iteration 14/1000 | Loss: 0.00002559
Iteration 15/1000 | Loss: 0.00002559
Iteration 16/1000 | Loss: 0.00002547
Iteration 17/1000 | Loss: 0.00002539
Iteration 18/1000 | Loss: 0.00002538
Iteration 19/1000 | Loss: 0.00002537
Iteration 20/1000 | Loss: 0.00002535
Iteration 21/1000 | Loss: 0.00002535
Iteration 22/1000 | Loss: 0.00002533
Iteration 23/1000 | Loss: 0.00002532
Iteration 24/1000 | Loss: 0.00002529
Iteration 25/1000 | Loss: 0.00002528
Iteration 26/1000 | Loss: 0.00002526
Iteration 27/1000 | Loss: 0.00002526
Iteration 28/1000 | Loss: 0.00002524
Iteration 29/1000 | Loss: 0.00002523
Iteration 30/1000 | Loss: 0.00002523
Iteration 31/1000 | Loss: 0.00002523
Iteration 32/1000 | Loss: 0.00002522
Iteration 33/1000 | Loss: 0.00002521
Iteration 34/1000 | Loss: 0.00002520
Iteration 35/1000 | Loss: 0.00002519
Iteration 36/1000 | Loss: 0.00002519
Iteration 37/1000 | Loss: 0.00002519
Iteration 38/1000 | Loss: 0.00002519
Iteration 39/1000 | Loss: 0.00002519
Iteration 40/1000 | Loss: 0.00002519
Iteration 41/1000 | Loss: 0.00002519
Iteration 42/1000 | Loss: 0.00002519
Iteration 43/1000 | Loss: 0.00002519
Iteration 44/1000 | Loss: 0.00002519
Iteration 45/1000 | Loss: 0.00002519
Iteration 46/1000 | Loss: 0.00002519
Iteration 47/1000 | Loss: 0.00002518
Iteration 48/1000 | Loss: 0.00002517
Iteration 49/1000 | Loss: 0.00002517
Iteration 50/1000 | Loss: 0.00002516
Iteration 51/1000 | Loss: 0.00002516
Iteration 52/1000 | Loss: 0.00002516
Iteration 53/1000 | Loss: 0.00002516
Iteration 54/1000 | Loss: 0.00002516
Iteration 55/1000 | Loss: 0.00002516
Iteration 56/1000 | Loss: 0.00002516
Iteration 57/1000 | Loss: 0.00002516
Iteration 58/1000 | Loss: 0.00002516
Iteration 59/1000 | Loss: 0.00002516
Iteration 60/1000 | Loss: 0.00002516
Iteration 61/1000 | Loss: 0.00002515
Iteration 62/1000 | Loss: 0.00002515
Iteration 63/1000 | Loss: 0.00002515
Iteration 64/1000 | Loss: 0.00002513
Iteration 65/1000 | Loss: 0.00002513
Iteration 66/1000 | Loss: 0.00002513
Iteration 67/1000 | Loss: 0.00002513
Iteration 68/1000 | Loss: 0.00002513
Iteration 69/1000 | Loss: 0.00002513
Iteration 70/1000 | Loss: 0.00002512
Iteration 71/1000 | Loss: 0.00002512
Iteration 72/1000 | Loss: 0.00002512
Iteration 73/1000 | Loss: 0.00002512
Iteration 74/1000 | Loss: 0.00002512
Iteration 75/1000 | Loss: 0.00002512
Iteration 76/1000 | Loss: 0.00002512
Iteration 77/1000 | Loss: 0.00002510
Iteration 78/1000 | Loss: 0.00002510
Iteration 79/1000 | Loss: 0.00002510
Iteration 80/1000 | Loss: 0.00002510
Iteration 81/1000 | Loss: 0.00002510
Iteration 82/1000 | Loss: 0.00002510
Iteration 83/1000 | Loss: 0.00002510
Iteration 84/1000 | Loss: 0.00002509
Iteration 85/1000 | Loss: 0.00002509
Iteration 86/1000 | Loss: 0.00002509
Iteration 87/1000 | Loss: 0.00002509
Iteration 88/1000 | Loss: 0.00002509
Iteration 89/1000 | Loss: 0.00002509
Iteration 90/1000 | Loss: 0.00002509
Iteration 91/1000 | Loss: 0.00002508
Iteration 92/1000 | Loss: 0.00002508
Iteration 93/1000 | Loss: 0.00002508
Iteration 94/1000 | Loss: 0.00002508
Iteration 95/1000 | Loss: 0.00002508
Iteration 96/1000 | Loss: 0.00002508
Iteration 97/1000 | Loss: 0.00002508
Iteration 98/1000 | Loss: 0.00002508
Iteration 99/1000 | Loss: 0.00002507
Iteration 100/1000 | Loss: 0.00002507
Iteration 101/1000 | Loss: 0.00002507
Iteration 102/1000 | Loss: 0.00002507
Iteration 103/1000 | Loss: 0.00002506
Iteration 104/1000 | Loss: 0.00002506
Iteration 105/1000 | Loss: 0.00002506
Iteration 106/1000 | Loss: 0.00002506
Iteration 107/1000 | Loss: 0.00002506
Iteration 108/1000 | Loss: 0.00002506
Iteration 109/1000 | Loss: 0.00002505
Iteration 110/1000 | Loss: 0.00002505
Iteration 111/1000 | Loss: 0.00002505
Iteration 112/1000 | Loss: 0.00002505
Iteration 113/1000 | Loss: 0.00002505
Iteration 114/1000 | Loss: 0.00002505
Iteration 115/1000 | Loss: 0.00002505
Iteration 116/1000 | Loss: 0.00002504
Iteration 117/1000 | Loss: 0.00002504
Iteration 118/1000 | Loss: 0.00002504
Iteration 119/1000 | Loss: 0.00002504
Iteration 120/1000 | Loss: 0.00002504
Iteration 121/1000 | Loss: 0.00002504
Iteration 122/1000 | Loss: 0.00002504
Iteration 123/1000 | Loss: 0.00002504
Iteration 124/1000 | Loss: 0.00002504
Iteration 125/1000 | Loss: 0.00002504
Iteration 126/1000 | Loss: 0.00002504
Iteration 127/1000 | Loss: 0.00002504
Iteration 128/1000 | Loss: 0.00002504
Iteration 129/1000 | Loss: 0.00002504
Iteration 130/1000 | Loss: 0.00002503
Iteration 131/1000 | Loss: 0.00002503
Iteration 132/1000 | Loss: 0.00002503
Iteration 133/1000 | Loss: 0.00002503
Iteration 134/1000 | Loss: 0.00002502
Iteration 135/1000 | Loss: 0.00002502
Iteration 136/1000 | Loss: 0.00002502
Iteration 137/1000 | Loss: 0.00002502
Iteration 138/1000 | Loss: 0.00002502
Iteration 139/1000 | Loss: 0.00002502
Iteration 140/1000 | Loss: 0.00002501
Iteration 141/1000 | Loss: 0.00002501
Iteration 142/1000 | Loss: 0.00002501
Iteration 143/1000 | Loss: 0.00002501
Iteration 144/1000 | Loss: 0.00002501
Iteration 145/1000 | Loss: 0.00002501
Iteration 146/1000 | Loss: 0.00002500
Iteration 147/1000 | Loss: 0.00002500
Iteration 148/1000 | Loss: 0.00002500
Iteration 149/1000 | Loss: 0.00002500
Iteration 150/1000 | Loss: 0.00002500
Iteration 151/1000 | Loss: 0.00002500
Iteration 152/1000 | Loss: 0.00002500
Iteration 153/1000 | Loss: 0.00002500
Iteration 154/1000 | Loss: 0.00002500
Iteration 155/1000 | Loss: 0.00002500
Iteration 156/1000 | Loss: 0.00002500
Iteration 157/1000 | Loss: 0.00002499
Iteration 158/1000 | Loss: 0.00002499
Iteration 159/1000 | Loss: 0.00002499
Iteration 160/1000 | Loss: 0.00002499
Iteration 161/1000 | Loss: 0.00002499
Iteration 162/1000 | Loss: 0.00002499
Iteration 163/1000 | Loss: 0.00002499
Iteration 164/1000 | Loss: 0.00002499
Iteration 165/1000 | Loss: 0.00002498
Iteration 166/1000 | Loss: 0.00002498
Iteration 167/1000 | Loss: 0.00002498
Iteration 168/1000 | Loss: 0.00002498
Iteration 169/1000 | Loss: 0.00002498
Iteration 170/1000 | Loss: 0.00002498
Iteration 171/1000 | Loss: 0.00002498
Iteration 172/1000 | Loss: 0.00002498
Iteration 173/1000 | Loss: 0.00002498
Iteration 174/1000 | Loss: 0.00002498
Iteration 175/1000 | Loss: 0.00002498
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.498225512681529e-05, 2.498225512681529e-05, 2.498225512681529e-05, 2.498225512681529e-05, 2.498225512681529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.498225512681529e-05

Optimization complete. Final v2v error: 4.040708541870117 mm

Highest mean error: 5.85576057434082 mm for frame 90

Lowest mean error: 2.866590976715088 mm for frame 8

Saving results

Total time: 63.510656118392944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00440005
Iteration 2/25 | Loss: 0.00153821
Iteration 3/25 | Loss: 0.00136214
Iteration 4/25 | Loss: 0.00133889
Iteration 5/25 | Loss: 0.00133206
Iteration 6/25 | Loss: 0.00132945
Iteration 7/25 | Loss: 0.00132876
Iteration 8/25 | Loss: 0.00132876
Iteration 9/25 | Loss: 0.00132876
Iteration 10/25 | Loss: 0.00132876
Iteration 11/25 | Loss: 0.00132876
Iteration 12/25 | Loss: 0.00132876
Iteration 13/25 | Loss: 0.00132876
Iteration 14/25 | Loss: 0.00132876
Iteration 15/25 | Loss: 0.00132876
Iteration 16/25 | Loss: 0.00132876
Iteration 17/25 | Loss: 0.00132876
Iteration 18/25 | Loss: 0.00132876
Iteration 19/25 | Loss: 0.00132876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013287555193528533, 0.0013287555193528533, 0.0013287555193528533, 0.0013287555193528533, 0.0013287555193528533]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013287555193528533

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.76937914
Iteration 2/25 | Loss: 0.00153592
Iteration 3/25 | Loss: 0.00153589
Iteration 4/25 | Loss: 0.00153589
Iteration 5/25 | Loss: 0.00153589
Iteration 6/25 | Loss: 0.00153589
Iteration 7/25 | Loss: 0.00153589
Iteration 8/25 | Loss: 0.00153589
Iteration 9/25 | Loss: 0.00153589
Iteration 10/25 | Loss: 0.00153589
Iteration 11/25 | Loss: 0.00153589
Iteration 12/25 | Loss: 0.00153589
Iteration 13/25 | Loss: 0.00153589
Iteration 14/25 | Loss: 0.00153589
Iteration 15/25 | Loss: 0.00153589
Iteration 16/25 | Loss: 0.00153589
Iteration 17/25 | Loss: 0.00153589
Iteration 18/25 | Loss: 0.00153589
Iteration 19/25 | Loss: 0.00153589
Iteration 20/25 | Loss: 0.00153589
Iteration 21/25 | Loss: 0.00153589
Iteration 22/25 | Loss: 0.00153589
Iteration 23/25 | Loss: 0.00153589
Iteration 24/25 | Loss: 0.00153589
Iteration 25/25 | Loss: 0.00153589

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153589
Iteration 2/1000 | Loss: 0.00004355
Iteration 3/1000 | Loss: 0.00003271
Iteration 4/1000 | Loss: 0.00002843
Iteration 5/1000 | Loss: 0.00002669
Iteration 6/1000 | Loss: 0.00002583
Iteration 7/1000 | Loss: 0.00002515
Iteration 8/1000 | Loss: 0.00002468
Iteration 9/1000 | Loss: 0.00002440
Iteration 10/1000 | Loss: 0.00002421
Iteration 11/1000 | Loss: 0.00002415
Iteration 12/1000 | Loss: 0.00002408
Iteration 13/1000 | Loss: 0.00002407
Iteration 14/1000 | Loss: 0.00002407
Iteration 15/1000 | Loss: 0.00002406
Iteration 16/1000 | Loss: 0.00002405
Iteration 17/1000 | Loss: 0.00002405
Iteration 18/1000 | Loss: 0.00002403
Iteration 19/1000 | Loss: 0.00002402
Iteration 20/1000 | Loss: 0.00002401
Iteration 21/1000 | Loss: 0.00002393
Iteration 22/1000 | Loss: 0.00002389
Iteration 23/1000 | Loss: 0.00002389
Iteration 24/1000 | Loss: 0.00002389
Iteration 25/1000 | Loss: 0.00002388
Iteration 26/1000 | Loss: 0.00002388
Iteration 27/1000 | Loss: 0.00002388
Iteration 28/1000 | Loss: 0.00002388
Iteration 29/1000 | Loss: 0.00002388
Iteration 30/1000 | Loss: 0.00002388
Iteration 31/1000 | Loss: 0.00002388
Iteration 32/1000 | Loss: 0.00002388
Iteration 33/1000 | Loss: 0.00002388
Iteration 34/1000 | Loss: 0.00002387
Iteration 35/1000 | Loss: 0.00002387
Iteration 36/1000 | Loss: 0.00002385
Iteration 37/1000 | Loss: 0.00002384
Iteration 38/1000 | Loss: 0.00002384
Iteration 39/1000 | Loss: 0.00002384
Iteration 40/1000 | Loss: 0.00002384
Iteration 41/1000 | Loss: 0.00002384
Iteration 42/1000 | Loss: 0.00002384
Iteration 43/1000 | Loss: 0.00002383
Iteration 44/1000 | Loss: 0.00002383
Iteration 45/1000 | Loss: 0.00002383
Iteration 46/1000 | Loss: 0.00002383
Iteration 47/1000 | Loss: 0.00002383
Iteration 48/1000 | Loss: 0.00002383
Iteration 49/1000 | Loss: 0.00002382
Iteration 50/1000 | Loss: 0.00002382
Iteration 51/1000 | Loss: 0.00002381
Iteration 52/1000 | Loss: 0.00002381
Iteration 53/1000 | Loss: 0.00002381
Iteration 54/1000 | Loss: 0.00002381
Iteration 55/1000 | Loss: 0.00002381
Iteration 56/1000 | Loss: 0.00002381
Iteration 57/1000 | Loss: 0.00002381
Iteration 58/1000 | Loss: 0.00002381
Iteration 59/1000 | Loss: 0.00002381
Iteration 60/1000 | Loss: 0.00002381
Iteration 61/1000 | Loss: 0.00002380
Iteration 62/1000 | Loss: 0.00002380
Iteration 63/1000 | Loss: 0.00002380
Iteration 64/1000 | Loss: 0.00002379
Iteration 65/1000 | Loss: 0.00002379
Iteration 66/1000 | Loss: 0.00002379
Iteration 67/1000 | Loss: 0.00002378
Iteration 68/1000 | Loss: 0.00002378
Iteration 69/1000 | Loss: 0.00002378
Iteration 70/1000 | Loss: 0.00002378
Iteration 71/1000 | Loss: 0.00002378
Iteration 72/1000 | Loss: 0.00002378
Iteration 73/1000 | Loss: 0.00002378
Iteration 74/1000 | Loss: 0.00002377
Iteration 75/1000 | Loss: 0.00002377
Iteration 76/1000 | Loss: 0.00002377
Iteration 77/1000 | Loss: 0.00002377
Iteration 78/1000 | Loss: 0.00002377
Iteration 79/1000 | Loss: 0.00002377
Iteration 80/1000 | Loss: 0.00002377
Iteration 81/1000 | Loss: 0.00002376
Iteration 82/1000 | Loss: 0.00002376
Iteration 83/1000 | Loss: 0.00002376
Iteration 84/1000 | Loss: 0.00002376
Iteration 85/1000 | Loss: 0.00002376
Iteration 86/1000 | Loss: 0.00002375
Iteration 87/1000 | Loss: 0.00002375
Iteration 88/1000 | Loss: 0.00002374
Iteration 89/1000 | Loss: 0.00002374
Iteration 90/1000 | Loss: 0.00002374
Iteration 91/1000 | Loss: 0.00002374
Iteration 92/1000 | Loss: 0.00002374
Iteration 93/1000 | Loss: 0.00002374
Iteration 94/1000 | Loss: 0.00002373
Iteration 95/1000 | Loss: 0.00002373
Iteration 96/1000 | Loss: 0.00002373
Iteration 97/1000 | Loss: 0.00002373
Iteration 98/1000 | Loss: 0.00002373
Iteration 99/1000 | Loss: 0.00002373
Iteration 100/1000 | Loss: 0.00002372
Iteration 101/1000 | Loss: 0.00002372
Iteration 102/1000 | Loss: 0.00002372
Iteration 103/1000 | Loss: 0.00002372
Iteration 104/1000 | Loss: 0.00002371
Iteration 105/1000 | Loss: 0.00002371
Iteration 106/1000 | Loss: 0.00002371
Iteration 107/1000 | Loss: 0.00002370
Iteration 108/1000 | Loss: 0.00002370
Iteration 109/1000 | Loss: 0.00002370
Iteration 110/1000 | Loss: 0.00002370
Iteration 111/1000 | Loss: 0.00002370
Iteration 112/1000 | Loss: 0.00002370
Iteration 113/1000 | Loss: 0.00002370
Iteration 114/1000 | Loss: 0.00002370
Iteration 115/1000 | Loss: 0.00002369
Iteration 116/1000 | Loss: 0.00002369
Iteration 117/1000 | Loss: 0.00002369
Iteration 118/1000 | Loss: 0.00002369
Iteration 119/1000 | Loss: 0.00002368
Iteration 120/1000 | Loss: 0.00002368
Iteration 121/1000 | Loss: 0.00002368
Iteration 122/1000 | Loss: 0.00002368
Iteration 123/1000 | Loss: 0.00002368
Iteration 124/1000 | Loss: 0.00002368
Iteration 125/1000 | Loss: 0.00002368
Iteration 126/1000 | Loss: 0.00002368
Iteration 127/1000 | Loss: 0.00002368
Iteration 128/1000 | Loss: 0.00002368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.368239984207321e-05, 2.368239984207321e-05, 2.368239984207321e-05, 2.368239984207321e-05, 2.368239984207321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.368239984207321e-05

Optimization complete. Final v2v error: 4.096721649169922 mm

Highest mean error: 4.375389575958252 mm for frame 24

Lowest mean error: 3.862170696258545 mm for frame 88

Saving results

Total time: 35.14721465110779
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945239
Iteration 2/25 | Loss: 0.00164870
Iteration 3/25 | Loss: 0.00142923
Iteration 4/25 | Loss: 0.00140184
Iteration 5/25 | Loss: 0.00139472
Iteration 6/25 | Loss: 0.00139354
Iteration 7/25 | Loss: 0.00139354
Iteration 8/25 | Loss: 0.00139354
Iteration 9/25 | Loss: 0.00139354
Iteration 10/25 | Loss: 0.00139354
Iteration 11/25 | Loss: 0.00139354
Iteration 12/25 | Loss: 0.00139354
Iteration 13/25 | Loss: 0.00139354
Iteration 14/25 | Loss: 0.00139354
Iteration 15/25 | Loss: 0.00139354
Iteration 16/25 | Loss: 0.00139354
Iteration 17/25 | Loss: 0.00139354
Iteration 18/25 | Loss: 0.00139354
Iteration 19/25 | Loss: 0.00139354
Iteration 20/25 | Loss: 0.00139354
Iteration 21/25 | Loss: 0.00139354
Iteration 22/25 | Loss: 0.00139354
Iteration 23/25 | Loss: 0.00139354
Iteration 24/25 | Loss: 0.00139354
Iteration 25/25 | Loss: 0.00139354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07349408
Iteration 2/25 | Loss: 0.00206655
Iteration 3/25 | Loss: 0.00206655
Iteration 4/25 | Loss: 0.00206654
Iteration 5/25 | Loss: 0.00206654
Iteration 6/25 | Loss: 0.00206654
Iteration 7/25 | Loss: 0.00206654
Iteration 8/25 | Loss: 0.00206654
Iteration 9/25 | Loss: 0.00206654
Iteration 10/25 | Loss: 0.00206654
Iteration 11/25 | Loss: 0.00206654
Iteration 12/25 | Loss: 0.00206654
Iteration 13/25 | Loss: 0.00206654
Iteration 14/25 | Loss: 0.00206654
Iteration 15/25 | Loss: 0.00206654
Iteration 16/25 | Loss: 0.00206654
Iteration 17/25 | Loss: 0.00206654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002066542860120535, 0.002066542860120535, 0.002066542860120535, 0.002066542860120535, 0.002066542860120535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002066542860120535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206654
Iteration 2/1000 | Loss: 0.00008437
Iteration 3/1000 | Loss: 0.00004181
Iteration 4/1000 | Loss: 0.00003305
Iteration 5/1000 | Loss: 0.00002940
Iteration 6/1000 | Loss: 0.00002768
Iteration 7/1000 | Loss: 0.00002660
Iteration 8/1000 | Loss: 0.00002585
Iteration 9/1000 | Loss: 0.00002533
Iteration 10/1000 | Loss: 0.00002491
Iteration 11/1000 | Loss: 0.00002460
Iteration 12/1000 | Loss: 0.00002437
Iteration 13/1000 | Loss: 0.00002432
Iteration 14/1000 | Loss: 0.00002413
Iteration 15/1000 | Loss: 0.00002394
Iteration 16/1000 | Loss: 0.00002388
Iteration 17/1000 | Loss: 0.00002381
Iteration 18/1000 | Loss: 0.00002376
Iteration 19/1000 | Loss: 0.00002372
Iteration 20/1000 | Loss: 0.00002369
Iteration 21/1000 | Loss: 0.00002368
Iteration 22/1000 | Loss: 0.00002367
Iteration 23/1000 | Loss: 0.00002367
Iteration 24/1000 | Loss: 0.00002366
Iteration 25/1000 | Loss: 0.00002366
Iteration 26/1000 | Loss: 0.00002365
Iteration 27/1000 | Loss: 0.00002364
Iteration 28/1000 | Loss: 0.00002364
Iteration 29/1000 | Loss: 0.00002364
Iteration 30/1000 | Loss: 0.00002363
Iteration 31/1000 | Loss: 0.00002363
Iteration 32/1000 | Loss: 0.00002362
Iteration 33/1000 | Loss: 0.00002362
Iteration 34/1000 | Loss: 0.00002361
Iteration 35/1000 | Loss: 0.00002360
Iteration 36/1000 | Loss: 0.00002359
Iteration 37/1000 | Loss: 0.00002357
Iteration 38/1000 | Loss: 0.00002356
Iteration 39/1000 | Loss: 0.00002354
Iteration 40/1000 | Loss: 0.00002354
Iteration 41/1000 | Loss: 0.00002354
Iteration 42/1000 | Loss: 0.00002354
Iteration 43/1000 | Loss: 0.00002354
Iteration 44/1000 | Loss: 0.00002354
Iteration 45/1000 | Loss: 0.00002354
Iteration 46/1000 | Loss: 0.00002353
Iteration 47/1000 | Loss: 0.00002353
Iteration 48/1000 | Loss: 0.00002353
Iteration 49/1000 | Loss: 0.00002353
Iteration 50/1000 | Loss: 0.00002352
Iteration 51/1000 | Loss: 0.00002352
Iteration 52/1000 | Loss: 0.00002351
Iteration 53/1000 | Loss: 0.00002351
Iteration 54/1000 | Loss: 0.00002351
Iteration 55/1000 | Loss: 0.00002350
Iteration 56/1000 | Loss: 0.00002350
Iteration 57/1000 | Loss: 0.00002350
Iteration 58/1000 | Loss: 0.00002350
Iteration 59/1000 | Loss: 0.00002350
Iteration 60/1000 | Loss: 0.00002349
Iteration 61/1000 | Loss: 0.00002349
Iteration 62/1000 | Loss: 0.00002349
Iteration 63/1000 | Loss: 0.00002349
Iteration 64/1000 | Loss: 0.00002348
Iteration 65/1000 | Loss: 0.00002347
Iteration 66/1000 | Loss: 0.00002347
Iteration 67/1000 | Loss: 0.00002347
Iteration 68/1000 | Loss: 0.00002347
Iteration 69/1000 | Loss: 0.00002347
Iteration 70/1000 | Loss: 0.00002347
Iteration 71/1000 | Loss: 0.00002347
Iteration 72/1000 | Loss: 0.00002346
Iteration 73/1000 | Loss: 0.00002346
Iteration 74/1000 | Loss: 0.00002346
Iteration 75/1000 | Loss: 0.00002346
Iteration 76/1000 | Loss: 0.00002345
Iteration 77/1000 | Loss: 0.00002345
Iteration 78/1000 | Loss: 0.00002345
Iteration 79/1000 | Loss: 0.00002345
Iteration 80/1000 | Loss: 0.00002345
Iteration 81/1000 | Loss: 0.00002345
Iteration 82/1000 | Loss: 0.00002345
Iteration 83/1000 | Loss: 0.00002345
Iteration 84/1000 | Loss: 0.00002345
Iteration 85/1000 | Loss: 0.00002344
Iteration 86/1000 | Loss: 0.00002344
Iteration 87/1000 | Loss: 0.00002344
Iteration 88/1000 | Loss: 0.00002344
Iteration 89/1000 | Loss: 0.00002344
Iteration 90/1000 | Loss: 0.00002344
Iteration 91/1000 | Loss: 0.00002344
Iteration 92/1000 | Loss: 0.00002344
Iteration 93/1000 | Loss: 0.00002344
Iteration 94/1000 | Loss: 0.00002343
Iteration 95/1000 | Loss: 0.00002343
Iteration 96/1000 | Loss: 0.00002343
Iteration 97/1000 | Loss: 0.00002343
Iteration 98/1000 | Loss: 0.00002343
Iteration 99/1000 | Loss: 0.00002343
Iteration 100/1000 | Loss: 0.00002343
Iteration 101/1000 | Loss: 0.00002343
Iteration 102/1000 | Loss: 0.00002342
Iteration 103/1000 | Loss: 0.00002342
Iteration 104/1000 | Loss: 0.00002342
Iteration 105/1000 | Loss: 0.00002342
Iteration 106/1000 | Loss: 0.00002342
Iteration 107/1000 | Loss: 0.00002342
Iteration 108/1000 | Loss: 0.00002342
Iteration 109/1000 | Loss: 0.00002342
Iteration 110/1000 | Loss: 0.00002342
Iteration 111/1000 | Loss: 0.00002342
Iteration 112/1000 | Loss: 0.00002342
Iteration 113/1000 | Loss: 0.00002342
Iteration 114/1000 | Loss: 0.00002342
Iteration 115/1000 | Loss: 0.00002342
Iteration 116/1000 | Loss: 0.00002342
Iteration 117/1000 | Loss: 0.00002342
Iteration 118/1000 | Loss: 0.00002342
Iteration 119/1000 | Loss: 0.00002342
Iteration 120/1000 | Loss: 0.00002342
Iteration 121/1000 | Loss: 0.00002342
Iteration 122/1000 | Loss: 0.00002342
Iteration 123/1000 | Loss: 0.00002342
Iteration 124/1000 | Loss: 0.00002342
Iteration 125/1000 | Loss: 0.00002342
Iteration 126/1000 | Loss: 0.00002342
Iteration 127/1000 | Loss: 0.00002342
Iteration 128/1000 | Loss: 0.00002342
Iteration 129/1000 | Loss: 0.00002342
Iteration 130/1000 | Loss: 0.00002342
Iteration 131/1000 | Loss: 0.00002342
Iteration 132/1000 | Loss: 0.00002342
Iteration 133/1000 | Loss: 0.00002342
Iteration 134/1000 | Loss: 0.00002342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.3415079340338707e-05, 2.3415079340338707e-05, 2.3415079340338707e-05, 2.3415079340338707e-05, 2.3415079340338707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3415079340338707e-05

Optimization complete. Final v2v error: 4.092001914978027 mm

Highest mean error: 4.579864501953125 mm for frame 208

Lowest mean error: 3.7600624561309814 mm for frame 0

Saving results

Total time: 45.1469144821167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500316
Iteration 2/25 | Loss: 0.00142343
Iteration 3/25 | Loss: 0.00132636
Iteration 4/25 | Loss: 0.00131857
Iteration 5/25 | Loss: 0.00131638
Iteration 6/25 | Loss: 0.00131630
Iteration 7/25 | Loss: 0.00131630
Iteration 8/25 | Loss: 0.00131630
Iteration 9/25 | Loss: 0.00131630
Iteration 10/25 | Loss: 0.00131630
Iteration 11/25 | Loss: 0.00131630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013163018738850951, 0.0013163018738850951, 0.0013163018738850951, 0.0013163018738850951, 0.0013163018738850951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013163018738850951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.79120600
Iteration 2/25 | Loss: 0.00151011
Iteration 3/25 | Loss: 0.00151011
Iteration 4/25 | Loss: 0.00151011
Iteration 5/25 | Loss: 0.00151011
Iteration 6/25 | Loss: 0.00151011
Iteration 7/25 | Loss: 0.00151011
Iteration 8/25 | Loss: 0.00151011
Iteration 9/25 | Loss: 0.00151011
Iteration 10/25 | Loss: 0.00151011
Iteration 11/25 | Loss: 0.00151011
Iteration 12/25 | Loss: 0.00151011
Iteration 13/25 | Loss: 0.00151011
Iteration 14/25 | Loss: 0.00151011
Iteration 15/25 | Loss: 0.00151011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0015101067256182432, 0.0015101067256182432, 0.0015101067256182432, 0.0015101067256182432, 0.0015101067256182432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015101067256182432

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151011
Iteration 2/1000 | Loss: 0.00004730
Iteration 3/1000 | Loss: 0.00003052
Iteration 4/1000 | Loss: 0.00002611
Iteration 5/1000 | Loss: 0.00002466
Iteration 6/1000 | Loss: 0.00002358
Iteration 7/1000 | Loss: 0.00002294
Iteration 8/1000 | Loss: 0.00002242
Iteration 9/1000 | Loss: 0.00002199
Iteration 10/1000 | Loss: 0.00002173
Iteration 11/1000 | Loss: 0.00002145
Iteration 12/1000 | Loss: 0.00002133
Iteration 13/1000 | Loss: 0.00002115
Iteration 14/1000 | Loss: 0.00002098
Iteration 15/1000 | Loss: 0.00002097
Iteration 16/1000 | Loss: 0.00002087
Iteration 17/1000 | Loss: 0.00002084
Iteration 18/1000 | Loss: 0.00002081
Iteration 19/1000 | Loss: 0.00002079
Iteration 20/1000 | Loss: 0.00002078
Iteration 21/1000 | Loss: 0.00002078
Iteration 22/1000 | Loss: 0.00002077
Iteration 23/1000 | Loss: 0.00002076
Iteration 24/1000 | Loss: 0.00002075
Iteration 25/1000 | Loss: 0.00002074
Iteration 26/1000 | Loss: 0.00002074
Iteration 27/1000 | Loss: 0.00002070
Iteration 28/1000 | Loss: 0.00002068
Iteration 29/1000 | Loss: 0.00002065
Iteration 30/1000 | Loss: 0.00002062
Iteration 31/1000 | Loss: 0.00002062
Iteration 32/1000 | Loss: 0.00002061
Iteration 33/1000 | Loss: 0.00002061
Iteration 34/1000 | Loss: 0.00002058
Iteration 35/1000 | Loss: 0.00002051
Iteration 36/1000 | Loss: 0.00002051
Iteration 37/1000 | Loss: 0.00002051
Iteration 38/1000 | Loss: 0.00002049
Iteration 39/1000 | Loss: 0.00002049
Iteration 40/1000 | Loss: 0.00002048
Iteration 41/1000 | Loss: 0.00002047
Iteration 42/1000 | Loss: 0.00002045
Iteration 43/1000 | Loss: 0.00002044
Iteration 44/1000 | Loss: 0.00002043
Iteration 45/1000 | Loss: 0.00002043
Iteration 46/1000 | Loss: 0.00002042
Iteration 47/1000 | Loss: 0.00002041
Iteration 48/1000 | Loss: 0.00002040
Iteration 49/1000 | Loss: 0.00002033
Iteration 50/1000 | Loss: 0.00002026
Iteration 51/1000 | Loss: 0.00002025
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002017
Iteration 55/1000 | Loss: 0.00002017
Iteration 56/1000 | Loss: 0.00002017
Iteration 57/1000 | Loss: 0.00002016
Iteration 58/1000 | Loss: 0.00002016
Iteration 59/1000 | Loss: 0.00002016
Iteration 60/1000 | Loss: 0.00002016
Iteration 61/1000 | Loss: 0.00002016
Iteration 62/1000 | Loss: 0.00002014
Iteration 63/1000 | Loss: 0.00002014
Iteration 64/1000 | Loss: 0.00002013
Iteration 65/1000 | Loss: 0.00002012
Iteration 66/1000 | Loss: 0.00002012
Iteration 67/1000 | Loss: 0.00002012
Iteration 68/1000 | Loss: 0.00002011
Iteration 69/1000 | Loss: 0.00002011
Iteration 70/1000 | Loss: 0.00002011
Iteration 71/1000 | Loss: 0.00002010
Iteration 72/1000 | Loss: 0.00002009
Iteration 73/1000 | Loss: 0.00002008
Iteration 74/1000 | Loss: 0.00002008
Iteration 75/1000 | Loss: 0.00002007
Iteration 76/1000 | Loss: 0.00002007
Iteration 77/1000 | Loss: 0.00002006
Iteration 78/1000 | Loss: 0.00002006
Iteration 79/1000 | Loss: 0.00002006
Iteration 80/1000 | Loss: 0.00002005
Iteration 81/1000 | Loss: 0.00002003
Iteration 82/1000 | Loss: 0.00002002
Iteration 83/1000 | Loss: 0.00002002
Iteration 84/1000 | Loss: 0.00002002
Iteration 85/1000 | Loss: 0.00002001
Iteration 86/1000 | Loss: 0.00002001
Iteration 87/1000 | Loss: 0.00002000
Iteration 88/1000 | Loss: 0.00001999
Iteration 89/1000 | Loss: 0.00001998
Iteration 90/1000 | Loss: 0.00001998
Iteration 91/1000 | Loss: 0.00001998
Iteration 92/1000 | Loss: 0.00001997
Iteration 93/1000 | Loss: 0.00001996
Iteration 94/1000 | Loss: 0.00001996
Iteration 95/1000 | Loss: 0.00001995
Iteration 96/1000 | Loss: 0.00001995
Iteration 97/1000 | Loss: 0.00001995
Iteration 98/1000 | Loss: 0.00001994
Iteration 99/1000 | Loss: 0.00001994
Iteration 100/1000 | Loss: 0.00001993
Iteration 101/1000 | Loss: 0.00001993
Iteration 102/1000 | Loss: 0.00001993
Iteration 103/1000 | Loss: 0.00001993
Iteration 104/1000 | Loss: 0.00001992
Iteration 105/1000 | Loss: 0.00001992
Iteration 106/1000 | Loss: 0.00001992
Iteration 107/1000 | Loss: 0.00001992
Iteration 108/1000 | Loss: 0.00001992
Iteration 109/1000 | Loss: 0.00001992
Iteration 110/1000 | Loss: 0.00001991
Iteration 111/1000 | Loss: 0.00001991
Iteration 112/1000 | Loss: 0.00001990
Iteration 113/1000 | Loss: 0.00001989
Iteration 114/1000 | Loss: 0.00001989
Iteration 115/1000 | Loss: 0.00001988
Iteration 116/1000 | Loss: 0.00001985
Iteration 117/1000 | Loss: 0.00001985
Iteration 118/1000 | Loss: 0.00001984
Iteration 119/1000 | Loss: 0.00001983
Iteration 120/1000 | Loss: 0.00001982
Iteration 121/1000 | Loss: 0.00001982
Iteration 122/1000 | Loss: 0.00001982
Iteration 123/1000 | Loss: 0.00001981
Iteration 124/1000 | Loss: 0.00001981
Iteration 125/1000 | Loss: 0.00001981
Iteration 126/1000 | Loss: 0.00001980
Iteration 127/1000 | Loss: 0.00001980
Iteration 128/1000 | Loss: 0.00001980
Iteration 129/1000 | Loss: 0.00001979
Iteration 130/1000 | Loss: 0.00001979
Iteration 131/1000 | Loss: 0.00001978
Iteration 132/1000 | Loss: 0.00001978
Iteration 133/1000 | Loss: 0.00001978
Iteration 134/1000 | Loss: 0.00001977
Iteration 135/1000 | Loss: 0.00001977
Iteration 136/1000 | Loss: 0.00001977
Iteration 137/1000 | Loss: 0.00001976
Iteration 138/1000 | Loss: 0.00001976
Iteration 139/1000 | Loss: 0.00001974
Iteration 140/1000 | Loss: 0.00001974
Iteration 141/1000 | Loss: 0.00001974
Iteration 142/1000 | Loss: 0.00001974
Iteration 143/1000 | Loss: 0.00001974
Iteration 144/1000 | Loss: 0.00001974
Iteration 145/1000 | Loss: 0.00001974
Iteration 146/1000 | Loss: 0.00001973
Iteration 147/1000 | Loss: 0.00001973
Iteration 148/1000 | Loss: 0.00001972
Iteration 149/1000 | Loss: 0.00001972
Iteration 150/1000 | Loss: 0.00001972
Iteration 151/1000 | Loss: 0.00001972
Iteration 152/1000 | Loss: 0.00001971
Iteration 153/1000 | Loss: 0.00001971
Iteration 154/1000 | Loss: 0.00001971
Iteration 155/1000 | Loss: 0.00001971
Iteration 156/1000 | Loss: 0.00001970
Iteration 157/1000 | Loss: 0.00001970
Iteration 158/1000 | Loss: 0.00001970
Iteration 159/1000 | Loss: 0.00001970
Iteration 160/1000 | Loss: 0.00001970
Iteration 161/1000 | Loss: 0.00001970
Iteration 162/1000 | Loss: 0.00001969
Iteration 163/1000 | Loss: 0.00001969
Iteration 164/1000 | Loss: 0.00001969
Iteration 165/1000 | Loss: 0.00001969
Iteration 166/1000 | Loss: 0.00001969
Iteration 167/1000 | Loss: 0.00001969
Iteration 168/1000 | Loss: 0.00001969
Iteration 169/1000 | Loss: 0.00001969
Iteration 170/1000 | Loss: 0.00001969
Iteration 171/1000 | Loss: 0.00001969
Iteration 172/1000 | Loss: 0.00001969
Iteration 173/1000 | Loss: 0.00001969
Iteration 174/1000 | Loss: 0.00001969
Iteration 175/1000 | Loss: 0.00001969
Iteration 176/1000 | Loss: 0.00001969
Iteration 177/1000 | Loss: 0.00001969
Iteration 178/1000 | Loss: 0.00001968
Iteration 179/1000 | Loss: 0.00001968
Iteration 180/1000 | Loss: 0.00001968
Iteration 181/1000 | Loss: 0.00001968
Iteration 182/1000 | Loss: 0.00001968
Iteration 183/1000 | Loss: 0.00001968
Iteration 184/1000 | Loss: 0.00001967
Iteration 185/1000 | Loss: 0.00001967
Iteration 186/1000 | Loss: 0.00001967
Iteration 187/1000 | Loss: 0.00001967
Iteration 188/1000 | Loss: 0.00001967
Iteration 189/1000 | Loss: 0.00001967
Iteration 190/1000 | Loss: 0.00001966
Iteration 191/1000 | Loss: 0.00001966
Iteration 192/1000 | Loss: 0.00001966
Iteration 193/1000 | Loss: 0.00001966
Iteration 194/1000 | Loss: 0.00001966
Iteration 195/1000 | Loss: 0.00001966
Iteration 196/1000 | Loss: 0.00001966
Iteration 197/1000 | Loss: 0.00001966
Iteration 198/1000 | Loss: 0.00001966
Iteration 199/1000 | Loss: 0.00001966
Iteration 200/1000 | Loss: 0.00001966
Iteration 201/1000 | Loss: 0.00001966
Iteration 202/1000 | Loss: 0.00001966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.9656738004414365e-05, 1.9656738004414365e-05, 1.9656738004414365e-05, 1.9656738004414365e-05, 1.9656738004414365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9656738004414365e-05

Optimization complete. Final v2v error: 3.7307655811309814 mm

Highest mean error: 3.9415905475616455 mm for frame 9

Lowest mean error: 3.530109405517578 mm for frame 209

Saving results

Total time: 61.14274477958679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037368
Iteration 2/25 | Loss: 0.00165994
Iteration 3/25 | Loss: 0.00135438
Iteration 4/25 | Loss: 0.00132371
Iteration 5/25 | Loss: 0.00130872
Iteration 6/25 | Loss: 0.00129684
Iteration 7/25 | Loss: 0.00129436
Iteration 8/25 | Loss: 0.00127379
Iteration 9/25 | Loss: 0.00127145
Iteration 10/25 | Loss: 0.00126113
Iteration 11/25 | Loss: 0.00125366
Iteration 12/25 | Loss: 0.00124294
Iteration 13/25 | Loss: 0.00124042
Iteration 14/25 | Loss: 0.00124090
Iteration 15/25 | Loss: 0.00123626
Iteration 16/25 | Loss: 0.00123222
Iteration 17/25 | Loss: 0.00123565
Iteration 18/25 | Loss: 0.00122562
Iteration 19/25 | Loss: 0.00122371
Iteration 20/25 | Loss: 0.00122385
Iteration 21/25 | Loss: 0.00122335
Iteration 22/25 | Loss: 0.00122285
Iteration 23/25 | Loss: 0.00122442
Iteration 24/25 | Loss: 0.00122268
Iteration 25/25 | Loss: 0.00122265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34827375
Iteration 2/25 | Loss: 0.00215849
Iteration 3/25 | Loss: 0.00215849
Iteration 4/25 | Loss: 0.00215849
Iteration 5/25 | Loss: 0.00215849
Iteration 6/25 | Loss: 0.00215849
Iteration 7/25 | Loss: 0.00215849
Iteration 8/25 | Loss: 0.00215849
Iteration 9/25 | Loss: 0.00213788
Iteration 10/25 | Loss: 0.00213787
Iteration 11/25 | Loss: 0.00213786
Iteration 12/25 | Loss: 0.00213786
Iteration 13/25 | Loss: 0.00213786
Iteration 14/25 | Loss: 0.00213786
Iteration 15/25 | Loss: 0.00213786
Iteration 16/25 | Loss: 0.00213786
Iteration 17/25 | Loss: 0.00213786
Iteration 18/25 | Loss: 0.00213786
Iteration 19/25 | Loss: 0.00213786
Iteration 20/25 | Loss: 0.00213786
Iteration 21/25 | Loss: 0.00213786
Iteration 22/25 | Loss: 0.00213786
Iteration 23/25 | Loss: 0.00213786
Iteration 24/25 | Loss: 0.00213786
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00213786237873137, 0.00213786237873137, 0.00213786237873137, 0.00213786237873137, 0.00213786237873137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00213786237873137

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00213786
Iteration 2/1000 | Loss: 0.00006298
Iteration 3/1000 | Loss: 0.00027161
Iteration 4/1000 | Loss: 0.00002868
Iteration 5/1000 | Loss: 0.00003531
Iteration 6/1000 | Loss: 0.00001984
Iteration 7/1000 | Loss: 0.00004951
Iteration 8/1000 | Loss: 0.00003761
Iteration 9/1000 | Loss: 0.00002439
Iteration 10/1000 | Loss: 0.00001982
Iteration 11/1000 | Loss: 0.00004069
Iteration 12/1000 | Loss: 0.00001714
Iteration 13/1000 | Loss: 0.00002509
Iteration 14/1000 | Loss: 0.00008480
Iteration 15/1000 | Loss: 0.00001622
Iteration 16/1000 | Loss: 0.00001492
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001481
Iteration 19/1000 | Loss: 0.00001479
Iteration 20/1000 | Loss: 0.00001479
Iteration 21/1000 | Loss: 0.00001475
Iteration 22/1000 | Loss: 0.00001475
Iteration 23/1000 | Loss: 0.00001475
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001472
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00004663
Iteration 31/1000 | Loss: 0.00004540
Iteration 32/1000 | Loss: 0.00002708
Iteration 33/1000 | Loss: 0.00001462
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001461
Iteration 36/1000 | Loss: 0.00001461
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001461
Iteration 40/1000 | Loss: 0.00001461
Iteration 41/1000 | Loss: 0.00001461
Iteration 42/1000 | Loss: 0.00001461
Iteration 43/1000 | Loss: 0.00001461
Iteration 44/1000 | Loss: 0.00001461
Iteration 45/1000 | Loss: 0.00001461
Iteration 46/1000 | Loss: 0.00001460
Iteration 47/1000 | Loss: 0.00001460
Iteration 48/1000 | Loss: 0.00002382
Iteration 49/1000 | Loss: 0.00003445
Iteration 50/1000 | Loss: 0.00001667
Iteration 51/1000 | Loss: 0.00001511
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001458
Iteration 55/1000 | Loss: 0.00001457
Iteration 56/1000 | Loss: 0.00001457
Iteration 57/1000 | Loss: 0.00001456
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001452
Iteration 60/1000 | Loss: 0.00001452
Iteration 61/1000 | Loss: 0.00001451
Iteration 62/1000 | Loss: 0.00001450
Iteration 63/1000 | Loss: 0.00001450
Iteration 64/1000 | Loss: 0.00001450
Iteration 65/1000 | Loss: 0.00001448
Iteration 66/1000 | Loss: 0.00001448
Iteration 67/1000 | Loss: 0.00001447
Iteration 68/1000 | Loss: 0.00001447
Iteration 69/1000 | Loss: 0.00001446
Iteration 70/1000 | Loss: 0.00001446
Iteration 71/1000 | Loss: 0.00001446
Iteration 72/1000 | Loss: 0.00001446
Iteration 73/1000 | Loss: 0.00001446
Iteration 74/1000 | Loss: 0.00001445
Iteration 75/1000 | Loss: 0.00001445
Iteration 76/1000 | Loss: 0.00001444
Iteration 77/1000 | Loss: 0.00001444
Iteration 78/1000 | Loss: 0.00001443
Iteration 79/1000 | Loss: 0.00001443
Iteration 80/1000 | Loss: 0.00001443
Iteration 81/1000 | Loss: 0.00001442
Iteration 82/1000 | Loss: 0.00001442
Iteration 83/1000 | Loss: 0.00001442
Iteration 84/1000 | Loss: 0.00001442
Iteration 85/1000 | Loss: 0.00001441
Iteration 86/1000 | Loss: 0.00001441
Iteration 87/1000 | Loss: 0.00001441
Iteration 88/1000 | Loss: 0.00001441
Iteration 89/1000 | Loss: 0.00001440
Iteration 90/1000 | Loss: 0.00001440
Iteration 91/1000 | Loss: 0.00001440
Iteration 92/1000 | Loss: 0.00001440
Iteration 93/1000 | Loss: 0.00001440
Iteration 94/1000 | Loss: 0.00001440
Iteration 95/1000 | Loss: 0.00001440
Iteration 96/1000 | Loss: 0.00001440
Iteration 97/1000 | Loss: 0.00001440
Iteration 98/1000 | Loss: 0.00001440
Iteration 99/1000 | Loss: 0.00001440
Iteration 100/1000 | Loss: 0.00001440
Iteration 101/1000 | Loss: 0.00001440
Iteration 102/1000 | Loss: 0.00001440
Iteration 103/1000 | Loss: 0.00001440
Iteration 104/1000 | Loss: 0.00001439
Iteration 105/1000 | Loss: 0.00001439
Iteration 106/1000 | Loss: 0.00001439
Iteration 107/1000 | Loss: 0.00001439
Iteration 108/1000 | Loss: 0.00001439
Iteration 109/1000 | Loss: 0.00001439
Iteration 110/1000 | Loss: 0.00001439
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001439
Iteration 114/1000 | Loss: 0.00001438
Iteration 115/1000 | Loss: 0.00001438
Iteration 116/1000 | Loss: 0.00001438
Iteration 117/1000 | Loss: 0.00001438
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001438
Iteration 123/1000 | Loss: 0.00001438
Iteration 124/1000 | Loss: 0.00001438
Iteration 125/1000 | Loss: 0.00001438
Iteration 126/1000 | Loss: 0.00001438
Iteration 127/1000 | Loss: 0.00001437
Iteration 128/1000 | Loss: 0.00001437
Iteration 129/1000 | Loss: 0.00001437
Iteration 130/1000 | Loss: 0.00001437
Iteration 131/1000 | Loss: 0.00001437
Iteration 132/1000 | Loss: 0.00001437
Iteration 133/1000 | Loss: 0.00001437
Iteration 134/1000 | Loss: 0.00001437
Iteration 135/1000 | Loss: 0.00001437
Iteration 136/1000 | Loss: 0.00001437
Iteration 137/1000 | Loss: 0.00001437
Iteration 138/1000 | Loss: 0.00001437
Iteration 139/1000 | Loss: 0.00001437
Iteration 140/1000 | Loss: 0.00001437
Iteration 141/1000 | Loss: 0.00001437
Iteration 142/1000 | Loss: 0.00001436
Iteration 143/1000 | Loss: 0.00001436
Iteration 144/1000 | Loss: 0.00001436
Iteration 145/1000 | Loss: 0.00001436
Iteration 146/1000 | Loss: 0.00001436
Iteration 147/1000 | Loss: 0.00001436
Iteration 148/1000 | Loss: 0.00001436
Iteration 149/1000 | Loss: 0.00001436
Iteration 150/1000 | Loss: 0.00001436
Iteration 151/1000 | Loss: 0.00001436
Iteration 152/1000 | Loss: 0.00001436
Iteration 153/1000 | Loss: 0.00001436
Iteration 154/1000 | Loss: 0.00001436
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00003174
Iteration 157/1000 | Loss: 0.00001437
Iteration 158/1000 | Loss: 0.00001435
Iteration 159/1000 | Loss: 0.00001435
Iteration 160/1000 | Loss: 0.00001435
Iteration 161/1000 | Loss: 0.00001435
Iteration 162/1000 | Loss: 0.00001435
Iteration 163/1000 | Loss: 0.00001435
Iteration 164/1000 | Loss: 0.00001435
Iteration 165/1000 | Loss: 0.00004559
Iteration 166/1000 | Loss: 0.00002953
Iteration 167/1000 | Loss: 0.00003636
Iteration 168/1000 | Loss: 0.00001441
Iteration 169/1000 | Loss: 0.00002021
Iteration 170/1000 | Loss: 0.00001441
Iteration 171/1000 | Loss: 0.00001435
Iteration 172/1000 | Loss: 0.00001435
Iteration 173/1000 | Loss: 0.00001435
Iteration 174/1000 | Loss: 0.00001435
Iteration 175/1000 | Loss: 0.00001435
Iteration 176/1000 | Loss: 0.00001435
Iteration 177/1000 | Loss: 0.00001435
Iteration 178/1000 | Loss: 0.00001435
Iteration 179/1000 | Loss: 0.00001435
Iteration 180/1000 | Loss: 0.00001435
Iteration 181/1000 | Loss: 0.00001435
Iteration 182/1000 | Loss: 0.00001435
Iteration 183/1000 | Loss: 0.00001435
Iteration 184/1000 | Loss: 0.00001434
Iteration 185/1000 | Loss: 0.00001434
Iteration 186/1000 | Loss: 0.00001434
Iteration 187/1000 | Loss: 0.00001434
Iteration 188/1000 | Loss: 0.00001433
Iteration 189/1000 | Loss: 0.00001433
Iteration 190/1000 | Loss: 0.00001433
Iteration 191/1000 | Loss: 0.00001433
Iteration 192/1000 | Loss: 0.00001433
Iteration 193/1000 | Loss: 0.00001433
Iteration 194/1000 | Loss: 0.00001433
Iteration 195/1000 | Loss: 0.00001433
Iteration 196/1000 | Loss: 0.00001433
Iteration 197/1000 | Loss: 0.00001433
Iteration 198/1000 | Loss: 0.00001433
Iteration 199/1000 | Loss: 0.00001433
Iteration 200/1000 | Loss: 0.00001433
Iteration 201/1000 | Loss: 0.00001433
Iteration 202/1000 | Loss: 0.00001433
Iteration 203/1000 | Loss: 0.00001433
Iteration 204/1000 | Loss: 0.00001433
Iteration 205/1000 | Loss: 0.00001433
Iteration 206/1000 | Loss: 0.00001433
Iteration 207/1000 | Loss: 0.00001433
Iteration 208/1000 | Loss: 0.00001433
Iteration 209/1000 | Loss: 0.00001433
Iteration 210/1000 | Loss: 0.00001433
Iteration 211/1000 | Loss: 0.00001433
Iteration 212/1000 | Loss: 0.00001433
Iteration 213/1000 | Loss: 0.00001433
Iteration 214/1000 | Loss: 0.00001433
Iteration 215/1000 | Loss: 0.00001433
Iteration 216/1000 | Loss: 0.00001433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.433113357052207e-05, 1.433113357052207e-05, 1.433113357052207e-05, 1.433113357052207e-05, 1.433113357052207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.433113357052207e-05

Optimization complete. Final v2v error: 3.1139490604400635 mm

Highest mean error: 8.532771110534668 mm for frame 144

Lowest mean error: 2.630047082901001 mm for frame 91

Saving results

Total time: 98.86798787117004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849058
Iteration 2/25 | Loss: 0.00169689
Iteration 3/25 | Loss: 0.00154653
Iteration 4/25 | Loss: 0.00138111
Iteration 5/25 | Loss: 0.00134795
Iteration 6/25 | Loss: 0.00132116
Iteration 7/25 | Loss: 0.00130598
Iteration 8/25 | Loss: 0.00130939
Iteration 9/25 | Loss: 0.00130636
Iteration 10/25 | Loss: 0.00129978
Iteration 11/25 | Loss: 0.00130766
Iteration 12/25 | Loss: 0.00129914
Iteration 13/25 | Loss: 0.00129801
Iteration 14/25 | Loss: 0.00130265
Iteration 15/25 | Loss: 0.00129642
Iteration 16/25 | Loss: 0.00129368
Iteration 17/25 | Loss: 0.00129258
Iteration 18/25 | Loss: 0.00129231
Iteration 19/25 | Loss: 0.00129223
Iteration 20/25 | Loss: 0.00129223
Iteration 21/25 | Loss: 0.00129223
Iteration 22/25 | Loss: 0.00129223
Iteration 23/25 | Loss: 0.00129223
Iteration 24/25 | Loss: 0.00129223
Iteration 25/25 | Loss: 0.00129223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75681949
Iteration 2/25 | Loss: 0.00259572
Iteration 3/25 | Loss: 0.00211936
Iteration 4/25 | Loss: 0.00211936
Iteration 5/25 | Loss: 0.00211936
Iteration 6/25 | Loss: 0.00211936
Iteration 7/25 | Loss: 0.00211936
Iteration 8/25 | Loss: 0.00211936
Iteration 9/25 | Loss: 0.00211936
Iteration 10/25 | Loss: 0.00211936
Iteration 11/25 | Loss: 0.00211936
Iteration 12/25 | Loss: 0.00211936
Iteration 13/25 | Loss: 0.00211936
Iteration 14/25 | Loss: 0.00211936
Iteration 15/25 | Loss: 0.00211936
Iteration 16/25 | Loss: 0.00211936
Iteration 17/25 | Loss: 0.00211936
Iteration 18/25 | Loss: 0.00211936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021193556021898985, 0.0021193556021898985, 0.0021193556021898985, 0.0021193556021898985, 0.0021193556021898985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021193556021898985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211936
Iteration 2/1000 | Loss: 0.00014076
Iteration 3/1000 | Loss: 0.00013518
Iteration 4/1000 | Loss: 0.00003198
Iteration 5/1000 | Loss: 0.00003023
Iteration 6/1000 | Loss: 0.00044941
Iteration 7/1000 | Loss: 0.00052146
Iteration 8/1000 | Loss: 0.00003315
Iteration 9/1000 | Loss: 0.00050367
Iteration 10/1000 | Loss: 0.00002988
Iteration 11/1000 | Loss: 0.00002718
Iteration 12/1000 | Loss: 0.00010386
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002441
Iteration 15/1000 | Loss: 0.00002404
Iteration 16/1000 | Loss: 0.00002381
Iteration 17/1000 | Loss: 0.00002364
Iteration 18/1000 | Loss: 0.00002363
Iteration 19/1000 | Loss: 0.00002359
Iteration 20/1000 | Loss: 0.00002347
Iteration 21/1000 | Loss: 0.00002334
Iteration 22/1000 | Loss: 0.00002332
Iteration 23/1000 | Loss: 0.00002331
Iteration 24/1000 | Loss: 0.00002330
Iteration 25/1000 | Loss: 0.00002329
Iteration 26/1000 | Loss: 0.00002328
Iteration 27/1000 | Loss: 0.00002328
Iteration 28/1000 | Loss: 0.00002327
Iteration 29/1000 | Loss: 0.00002327
Iteration 30/1000 | Loss: 0.00002326
Iteration 31/1000 | Loss: 0.00002326
Iteration 32/1000 | Loss: 0.00002325
Iteration 33/1000 | Loss: 0.00002325
Iteration 34/1000 | Loss: 0.00002325
Iteration 35/1000 | Loss: 0.00002324
Iteration 36/1000 | Loss: 0.00002324
Iteration 37/1000 | Loss: 0.00002323
Iteration 38/1000 | Loss: 0.00002323
Iteration 39/1000 | Loss: 0.00002322
Iteration 40/1000 | Loss: 0.00002322
Iteration 41/1000 | Loss: 0.00002321
Iteration 42/1000 | Loss: 0.00002321
Iteration 43/1000 | Loss: 0.00002321
Iteration 44/1000 | Loss: 0.00002321
Iteration 45/1000 | Loss: 0.00002321
Iteration 46/1000 | Loss: 0.00002321
Iteration 47/1000 | Loss: 0.00002320
Iteration 48/1000 | Loss: 0.00002320
Iteration 49/1000 | Loss: 0.00002320
Iteration 50/1000 | Loss: 0.00002320
Iteration 51/1000 | Loss: 0.00002319
Iteration 52/1000 | Loss: 0.00002319
Iteration 53/1000 | Loss: 0.00002319
Iteration 54/1000 | Loss: 0.00002319
Iteration 55/1000 | Loss: 0.00002319
Iteration 56/1000 | Loss: 0.00002319
Iteration 57/1000 | Loss: 0.00002319
Iteration 58/1000 | Loss: 0.00002318
Iteration 59/1000 | Loss: 0.00002318
Iteration 60/1000 | Loss: 0.00002318
Iteration 61/1000 | Loss: 0.00002317
Iteration 62/1000 | Loss: 0.00002317
Iteration 63/1000 | Loss: 0.00002317
Iteration 64/1000 | Loss: 0.00002317
Iteration 65/1000 | Loss: 0.00002317
Iteration 66/1000 | Loss: 0.00002316
Iteration 67/1000 | Loss: 0.00002316
Iteration 68/1000 | Loss: 0.00002316
Iteration 69/1000 | Loss: 0.00002316
Iteration 70/1000 | Loss: 0.00002316
Iteration 71/1000 | Loss: 0.00002316
Iteration 72/1000 | Loss: 0.00002316
Iteration 73/1000 | Loss: 0.00002316
Iteration 74/1000 | Loss: 0.00002316
Iteration 75/1000 | Loss: 0.00002315
Iteration 76/1000 | Loss: 0.00002315
Iteration 77/1000 | Loss: 0.00002315
Iteration 78/1000 | Loss: 0.00002315
Iteration 79/1000 | Loss: 0.00002315
Iteration 80/1000 | Loss: 0.00002315
Iteration 81/1000 | Loss: 0.00002315
Iteration 82/1000 | Loss: 0.00002314
Iteration 83/1000 | Loss: 0.00002314
Iteration 84/1000 | Loss: 0.00002314
Iteration 85/1000 | Loss: 0.00002314
Iteration 86/1000 | Loss: 0.00002314
Iteration 87/1000 | Loss: 0.00002314
Iteration 88/1000 | Loss: 0.00002314
Iteration 89/1000 | Loss: 0.00002314
Iteration 90/1000 | Loss: 0.00002314
Iteration 91/1000 | Loss: 0.00002314
Iteration 92/1000 | Loss: 0.00002314
Iteration 93/1000 | Loss: 0.00002314
Iteration 94/1000 | Loss: 0.00002314
Iteration 95/1000 | Loss: 0.00002314
Iteration 96/1000 | Loss: 0.00002314
Iteration 97/1000 | Loss: 0.00002314
Iteration 98/1000 | Loss: 0.00002314
Iteration 99/1000 | Loss: 0.00002314
Iteration 100/1000 | Loss: 0.00002314
Iteration 101/1000 | Loss: 0.00002314
Iteration 102/1000 | Loss: 0.00002313
Iteration 103/1000 | Loss: 0.00002313
Iteration 104/1000 | Loss: 0.00002313
Iteration 105/1000 | Loss: 0.00002313
Iteration 106/1000 | Loss: 0.00002313
Iteration 107/1000 | Loss: 0.00002313
Iteration 108/1000 | Loss: 0.00002313
Iteration 109/1000 | Loss: 0.00002313
Iteration 110/1000 | Loss: 0.00002313
Iteration 111/1000 | Loss: 0.00002313
Iteration 112/1000 | Loss: 0.00002313
Iteration 113/1000 | Loss: 0.00002313
Iteration 114/1000 | Loss: 0.00002312
Iteration 115/1000 | Loss: 0.00002312
Iteration 116/1000 | Loss: 0.00002312
Iteration 117/1000 | Loss: 0.00002312
Iteration 118/1000 | Loss: 0.00002312
Iteration 119/1000 | Loss: 0.00002312
Iteration 120/1000 | Loss: 0.00002312
Iteration 121/1000 | Loss: 0.00002312
Iteration 122/1000 | Loss: 0.00002312
Iteration 123/1000 | Loss: 0.00002312
Iteration 124/1000 | Loss: 0.00002312
Iteration 125/1000 | Loss: 0.00002312
Iteration 126/1000 | Loss: 0.00002312
Iteration 127/1000 | Loss: 0.00002312
Iteration 128/1000 | Loss: 0.00002312
Iteration 129/1000 | Loss: 0.00002311
Iteration 130/1000 | Loss: 0.00002311
Iteration 131/1000 | Loss: 0.00002311
Iteration 132/1000 | Loss: 0.00002311
Iteration 133/1000 | Loss: 0.00002311
Iteration 134/1000 | Loss: 0.00002311
Iteration 135/1000 | Loss: 0.00002311
Iteration 136/1000 | Loss: 0.00002311
Iteration 137/1000 | Loss: 0.00002311
Iteration 138/1000 | Loss: 0.00002311
Iteration 139/1000 | Loss: 0.00002311
Iteration 140/1000 | Loss: 0.00002311
Iteration 141/1000 | Loss: 0.00002311
Iteration 142/1000 | Loss: 0.00002311
Iteration 143/1000 | Loss: 0.00002311
Iteration 144/1000 | Loss: 0.00002311
Iteration 145/1000 | Loss: 0.00002311
Iteration 146/1000 | Loss: 0.00002311
Iteration 147/1000 | Loss: 0.00002311
Iteration 148/1000 | Loss: 0.00002311
Iteration 149/1000 | Loss: 0.00002311
Iteration 150/1000 | Loss: 0.00002311
Iteration 151/1000 | Loss: 0.00002311
Iteration 152/1000 | Loss: 0.00002311
Iteration 153/1000 | Loss: 0.00002311
Iteration 154/1000 | Loss: 0.00002311
Iteration 155/1000 | Loss: 0.00002311
Iteration 156/1000 | Loss: 0.00002311
Iteration 157/1000 | Loss: 0.00002311
Iteration 158/1000 | Loss: 0.00002311
Iteration 159/1000 | Loss: 0.00002311
Iteration 160/1000 | Loss: 0.00002311
Iteration 161/1000 | Loss: 0.00002311
Iteration 162/1000 | Loss: 0.00002311
Iteration 163/1000 | Loss: 0.00002311
Iteration 164/1000 | Loss: 0.00002311
Iteration 165/1000 | Loss: 0.00002311
Iteration 166/1000 | Loss: 0.00002311
Iteration 167/1000 | Loss: 0.00002311
Iteration 168/1000 | Loss: 0.00002311
Iteration 169/1000 | Loss: 0.00002311
Iteration 170/1000 | Loss: 0.00002311
Iteration 171/1000 | Loss: 0.00002311
Iteration 172/1000 | Loss: 0.00002311
Iteration 173/1000 | Loss: 0.00002311
Iteration 174/1000 | Loss: 0.00002311
Iteration 175/1000 | Loss: 0.00002311
Iteration 176/1000 | Loss: 0.00002311
Iteration 177/1000 | Loss: 0.00002311
Iteration 178/1000 | Loss: 0.00002311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [2.3109483663574792e-05, 2.3109483663574792e-05, 2.3109483663574792e-05, 2.3109483663574792e-05, 2.3109483663574792e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3109483663574792e-05

Optimization complete. Final v2v error: 3.9746081829071045 mm

Highest mean error: 9.003783226013184 mm for frame 152

Lowest mean error: 3.7497620582580566 mm for frame 140

Saving results

Total time: 69.99821925163269
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468836
Iteration 2/25 | Loss: 0.00190380
Iteration 3/25 | Loss: 0.00164822
Iteration 4/25 | Loss: 0.00168924
Iteration 5/25 | Loss: 0.00179372
Iteration 6/25 | Loss: 0.00153830
Iteration 7/25 | Loss: 0.00147423
Iteration 8/25 | Loss: 0.00147039
Iteration 9/25 | Loss: 0.00148451
Iteration 10/25 | Loss: 0.00150729
Iteration 11/25 | Loss: 0.00149081
Iteration 12/25 | Loss: 0.00146312
Iteration 13/25 | Loss: 0.00144833
Iteration 14/25 | Loss: 0.00144518
Iteration 15/25 | Loss: 0.00144764
Iteration 16/25 | Loss: 0.00144578
Iteration 17/25 | Loss: 0.00144306
Iteration 18/25 | Loss: 0.00144239
Iteration 19/25 | Loss: 0.00144222
Iteration 20/25 | Loss: 0.00144219
Iteration 21/25 | Loss: 0.00144219
Iteration 22/25 | Loss: 0.00144219
Iteration 23/25 | Loss: 0.00144218
Iteration 24/25 | Loss: 0.00144218
Iteration 25/25 | Loss: 0.00144218

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22535622
Iteration 2/25 | Loss: 0.00212051
Iteration 3/25 | Loss: 0.00212051
Iteration 4/25 | Loss: 0.00212051
Iteration 5/25 | Loss: 0.00212051
Iteration 6/25 | Loss: 0.00212051
Iteration 7/25 | Loss: 0.00212051
Iteration 8/25 | Loss: 0.00212051
Iteration 9/25 | Loss: 0.00212051
Iteration 10/25 | Loss: 0.00212051
Iteration 11/25 | Loss: 0.00212051
Iteration 12/25 | Loss: 0.00212051
Iteration 13/25 | Loss: 0.00212051
Iteration 14/25 | Loss: 0.00212051
Iteration 15/25 | Loss: 0.00212051
Iteration 16/25 | Loss: 0.00212051
Iteration 17/25 | Loss: 0.00212051
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002120510209351778, 0.002120510209351778, 0.002120510209351778, 0.002120510209351778, 0.002120510209351778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002120510209351778

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212051
Iteration 2/1000 | Loss: 0.00072861
Iteration 3/1000 | Loss: 0.00103729
Iteration 4/1000 | Loss: 0.00032035
Iteration 5/1000 | Loss: 0.00010382
Iteration 6/1000 | Loss: 0.00008354
Iteration 7/1000 | Loss: 0.00006718
Iteration 8/1000 | Loss: 0.00005812
Iteration 9/1000 | Loss: 0.00013607
Iteration 10/1000 | Loss: 0.00005425
Iteration 11/1000 | Loss: 0.00004983
Iteration 12/1000 | Loss: 0.00031550
Iteration 13/1000 | Loss: 0.00008274
Iteration 14/1000 | Loss: 0.00013695
Iteration 15/1000 | Loss: 0.00006528
Iteration 16/1000 | Loss: 0.00005516
Iteration 17/1000 | Loss: 0.00004976
Iteration 18/1000 | Loss: 0.00004764
Iteration 19/1000 | Loss: 0.00004604
Iteration 20/1000 | Loss: 0.00004462
Iteration 21/1000 | Loss: 0.00004320
Iteration 22/1000 | Loss: 0.00004225
Iteration 23/1000 | Loss: 0.00008520
Iteration 24/1000 | Loss: 0.00013663
Iteration 25/1000 | Loss: 0.00006309
Iteration 26/1000 | Loss: 0.00004053
Iteration 27/1000 | Loss: 0.00003981
Iteration 28/1000 | Loss: 0.00003882
Iteration 29/1000 | Loss: 0.00003812
Iteration 30/1000 | Loss: 0.00003752
Iteration 31/1000 | Loss: 0.00003710
Iteration 32/1000 | Loss: 0.00003680
Iteration 33/1000 | Loss: 0.00003646
Iteration 34/1000 | Loss: 0.00004546
Iteration 35/1000 | Loss: 0.00003683
Iteration 36/1000 | Loss: 0.00003636
Iteration 37/1000 | Loss: 0.00003599
Iteration 38/1000 | Loss: 0.00003578
Iteration 39/1000 | Loss: 0.00009152
Iteration 40/1000 | Loss: 0.00005261
Iteration 41/1000 | Loss: 0.00007248
Iteration 42/1000 | Loss: 0.00009893
Iteration 43/1000 | Loss: 0.00007143
Iteration 44/1000 | Loss: 0.00009486
Iteration 45/1000 | Loss: 0.00006934
Iteration 46/1000 | Loss: 0.00010519
Iteration 47/1000 | Loss: 0.00007480
Iteration 48/1000 | Loss: 0.00009046
Iteration 49/1000 | Loss: 0.00008303
Iteration 50/1000 | Loss: 0.00009405
Iteration 51/1000 | Loss: 0.00003904
Iteration 52/1000 | Loss: 0.00003839
Iteration 53/1000 | Loss: 0.00003748
Iteration 54/1000 | Loss: 0.00003687
Iteration 55/1000 | Loss: 0.00003603
Iteration 56/1000 | Loss: 0.00003551
Iteration 57/1000 | Loss: 0.00003531
Iteration 58/1000 | Loss: 0.00003512
Iteration 59/1000 | Loss: 0.00003504
Iteration 60/1000 | Loss: 0.00003503
Iteration 61/1000 | Loss: 0.00003502
Iteration 62/1000 | Loss: 0.00003502
Iteration 63/1000 | Loss: 0.00003501
Iteration 64/1000 | Loss: 0.00003501
Iteration 65/1000 | Loss: 0.00003501
Iteration 66/1000 | Loss: 0.00003501
Iteration 67/1000 | Loss: 0.00003501
Iteration 68/1000 | Loss: 0.00003501
Iteration 69/1000 | Loss: 0.00003500
Iteration 70/1000 | Loss: 0.00003500
Iteration 71/1000 | Loss: 0.00003500
Iteration 72/1000 | Loss: 0.00003500
Iteration 73/1000 | Loss: 0.00003500
Iteration 74/1000 | Loss: 0.00003500
Iteration 75/1000 | Loss: 0.00003500
Iteration 76/1000 | Loss: 0.00003499
Iteration 77/1000 | Loss: 0.00003499
Iteration 78/1000 | Loss: 0.00003499
Iteration 79/1000 | Loss: 0.00003499
Iteration 80/1000 | Loss: 0.00003499
Iteration 81/1000 | Loss: 0.00003499
Iteration 82/1000 | Loss: 0.00003499
Iteration 83/1000 | Loss: 0.00003499
Iteration 84/1000 | Loss: 0.00003499
Iteration 85/1000 | Loss: 0.00003499
Iteration 86/1000 | Loss: 0.00003499
Iteration 87/1000 | Loss: 0.00003499
Iteration 88/1000 | Loss: 0.00003499
Iteration 89/1000 | Loss: 0.00003499
Iteration 90/1000 | Loss: 0.00003499
Iteration 91/1000 | Loss: 0.00003499
Iteration 92/1000 | Loss: 0.00003499
Iteration 93/1000 | Loss: 0.00003499
Iteration 94/1000 | Loss: 0.00003499
Iteration 95/1000 | Loss: 0.00003499
Iteration 96/1000 | Loss: 0.00003499
Iteration 97/1000 | Loss: 0.00003499
Iteration 98/1000 | Loss: 0.00003499
Iteration 99/1000 | Loss: 0.00003499
Iteration 100/1000 | Loss: 0.00003499
Iteration 101/1000 | Loss: 0.00003499
Iteration 102/1000 | Loss: 0.00003499
Iteration 103/1000 | Loss: 0.00003499
Iteration 104/1000 | Loss: 0.00003499
Iteration 105/1000 | Loss: 0.00003499
Iteration 106/1000 | Loss: 0.00003499
Iteration 107/1000 | Loss: 0.00003499
Iteration 108/1000 | Loss: 0.00003499
Iteration 109/1000 | Loss: 0.00003499
Iteration 110/1000 | Loss: 0.00003499
Iteration 111/1000 | Loss: 0.00003499
Iteration 112/1000 | Loss: 0.00003499
Iteration 113/1000 | Loss: 0.00003499
Iteration 114/1000 | Loss: 0.00003499
Iteration 115/1000 | Loss: 0.00003499
Iteration 116/1000 | Loss: 0.00003499
Iteration 117/1000 | Loss: 0.00003499
Iteration 118/1000 | Loss: 0.00003499
Iteration 119/1000 | Loss: 0.00003499
Iteration 120/1000 | Loss: 0.00003499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [3.498801015666686e-05, 3.498801015666686e-05, 3.498801015666686e-05, 3.498801015666686e-05, 3.498801015666686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.498801015666686e-05

Optimization complete. Final v2v error: 4.285717010498047 mm

Highest mean error: 6.01552152633667 mm for frame 106

Lowest mean error: 3.613968849182129 mm for frame 5

Saving results

Total time: 137.97588443756104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409311
Iteration 2/25 | Loss: 0.00137881
Iteration 3/25 | Loss: 0.00128694
Iteration 4/25 | Loss: 0.00128305
Iteration 5/25 | Loss: 0.00128251
Iteration 6/25 | Loss: 0.00128251
Iteration 7/25 | Loss: 0.00128251
Iteration 8/25 | Loss: 0.00128251
Iteration 9/25 | Loss: 0.00128251
Iteration 10/25 | Loss: 0.00128251
Iteration 11/25 | Loss: 0.00128251
Iteration 12/25 | Loss: 0.00128251
Iteration 13/25 | Loss: 0.00128225
Iteration 14/25 | Loss: 0.00128225
Iteration 15/25 | Loss: 0.00128225
Iteration 16/25 | Loss: 0.00128225
Iteration 17/25 | Loss: 0.00128225
Iteration 18/25 | Loss: 0.00128225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012822512071579695, 0.0012822512071579695, 0.0012822512071579695, 0.0012822512071579695, 0.0012822512071579695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012822512071579695

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26867723
Iteration 2/25 | Loss: 0.00191711
Iteration 3/25 | Loss: 0.00191711
Iteration 4/25 | Loss: 0.00191711
Iteration 5/25 | Loss: 0.00191711
Iteration 6/25 | Loss: 0.00191711
Iteration 7/25 | Loss: 0.00191711
Iteration 8/25 | Loss: 0.00191711
Iteration 9/25 | Loss: 0.00191711
Iteration 10/25 | Loss: 0.00191711
Iteration 11/25 | Loss: 0.00191711
Iteration 12/25 | Loss: 0.00191711
Iteration 13/25 | Loss: 0.00191711
Iteration 14/25 | Loss: 0.00191711
Iteration 15/25 | Loss: 0.00191711
Iteration 16/25 | Loss: 0.00191711
Iteration 17/25 | Loss: 0.00191711
Iteration 18/25 | Loss: 0.00191711
Iteration 19/25 | Loss: 0.00191711
Iteration 20/25 | Loss: 0.00191711
Iteration 21/25 | Loss: 0.00191711
Iteration 22/25 | Loss: 0.00191711
Iteration 23/25 | Loss: 0.00191711
Iteration 24/25 | Loss: 0.00191711
Iteration 25/25 | Loss: 0.00191711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191711
Iteration 2/1000 | Loss: 0.00003286
Iteration 3/1000 | Loss: 0.00001796
Iteration 4/1000 | Loss: 0.00001575
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001441
Iteration 7/1000 | Loss: 0.00001403
Iteration 8/1000 | Loss: 0.00001387
Iteration 9/1000 | Loss: 0.00001381
Iteration 10/1000 | Loss: 0.00001381
Iteration 11/1000 | Loss: 0.00001378
Iteration 12/1000 | Loss: 0.00001377
Iteration 13/1000 | Loss: 0.00001377
Iteration 14/1000 | Loss: 0.00001376
Iteration 15/1000 | Loss: 0.00001376
Iteration 16/1000 | Loss: 0.00001372
Iteration 17/1000 | Loss: 0.00001364
Iteration 18/1000 | Loss: 0.00001361
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001359
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001356
Iteration 23/1000 | Loss: 0.00001356
Iteration 24/1000 | Loss: 0.00001356
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001352
Iteration 28/1000 | Loss: 0.00001352
Iteration 29/1000 | Loss: 0.00001352
Iteration 30/1000 | Loss: 0.00001351
Iteration 31/1000 | Loss: 0.00001351
Iteration 32/1000 | Loss: 0.00001351
Iteration 33/1000 | Loss: 0.00001351
Iteration 34/1000 | Loss: 0.00001351
Iteration 35/1000 | Loss: 0.00001351
Iteration 36/1000 | Loss: 0.00001351
Iteration 37/1000 | Loss: 0.00001351
Iteration 38/1000 | Loss: 0.00001351
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001346
Iteration 42/1000 | Loss: 0.00001346
Iteration 43/1000 | Loss: 0.00001345
Iteration 44/1000 | Loss: 0.00001345
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001341
Iteration 48/1000 | Loss: 0.00001341
Iteration 49/1000 | Loss: 0.00001341
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001341
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001341
Iteration 56/1000 | Loss: 0.00001340
Iteration 57/1000 | Loss: 0.00001340
Iteration 58/1000 | Loss: 0.00001339
Iteration 59/1000 | Loss: 0.00001339
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00001338
Iteration 62/1000 | Loss: 0.00001338
Iteration 63/1000 | Loss: 0.00001337
Iteration 64/1000 | Loss: 0.00001337
Iteration 65/1000 | Loss: 0.00001336
Iteration 66/1000 | Loss: 0.00001336
Iteration 67/1000 | Loss: 0.00001336
Iteration 68/1000 | Loss: 0.00001335
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001335
Iteration 71/1000 | Loss: 0.00001334
Iteration 72/1000 | Loss: 0.00001334
Iteration 73/1000 | Loss: 0.00001334
Iteration 74/1000 | Loss: 0.00001334
Iteration 75/1000 | Loss: 0.00001334
Iteration 76/1000 | Loss: 0.00001334
Iteration 77/1000 | Loss: 0.00001333
Iteration 78/1000 | Loss: 0.00001333
Iteration 79/1000 | Loss: 0.00001333
Iteration 80/1000 | Loss: 0.00001332
Iteration 81/1000 | Loss: 0.00001332
Iteration 82/1000 | Loss: 0.00001332
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001330
Iteration 87/1000 | Loss: 0.00001330
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001328
Iteration 91/1000 | Loss: 0.00001328
Iteration 92/1000 | Loss: 0.00001328
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001327
Iteration 96/1000 | Loss: 0.00001327
Iteration 97/1000 | Loss: 0.00001327
Iteration 98/1000 | Loss: 0.00001327
Iteration 99/1000 | Loss: 0.00001327
Iteration 100/1000 | Loss: 0.00001327
Iteration 101/1000 | Loss: 0.00001327
Iteration 102/1000 | Loss: 0.00001327
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001326
Iteration 106/1000 | Loss: 0.00001326
Iteration 107/1000 | Loss: 0.00001326
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001325
Iteration 111/1000 | Loss: 0.00001325
Iteration 112/1000 | Loss: 0.00001325
Iteration 113/1000 | Loss: 0.00001325
Iteration 114/1000 | Loss: 0.00001325
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001325
Iteration 117/1000 | Loss: 0.00001324
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001324
Iteration 120/1000 | Loss: 0.00001324
Iteration 121/1000 | Loss: 0.00001324
Iteration 122/1000 | Loss: 0.00001324
Iteration 123/1000 | Loss: 0.00001324
Iteration 124/1000 | Loss: 0.00001324
Iteration 125/1000 | Loss: 0.00001324
Iteration 126/1000 | Loss: 0.00001323
Iteration 127/1000 | Loss: 0.00001323
Iteration 128/1000 | Loss: 0.00001323
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001322
Iteration 131/1000 | Loss: 0.00001322
Iteration 132/1000 | Loss: 0.00001322
Iteration 133/1000 | Loss: 0.00001322
Iteration 134/1000 | Loss: 0.00001322
Iteration 135/1000 | Loss: 0.00001322
Iteration 136/1000 | Loss: 0.00001322
Iteration 137/1000 | Loss: 0.00001322
Iteration 138/1000 | Loss: 0.00001322
Iteration 139/1000 | Loss: 0.00001322
Iteration 140/1000 | Loss: 0.00001322
Iteration 141/1000 | Loss: 0.00001322
Iteration 142/1000 | Loss: 0.00001322
Iteration 143/1000 | Loss: 0.00001321
Iteration 144/1000 | Loss: 0.00001321
Iteration 145/1000 | Loss: 0.00001321
Iteration 146/1000 | Loss: 0.00001321
Iteration 147/1000 | Loss: 0.00001321
Iteration 148/1000 | Loss: 0.00001321
Iteration 149/1000 | Loss: 0.00001321
Iteration 150/1000 | Loss: 0.00001321
Iteration 151/1000 | Loss: 0.00001321
Iteration 152/1000 | Loss: 0.00001321
Iteration 153/1000 | Loss: 0.00001321
Iteration 154/1000 | Loss: 0.00001321
Iteration 155/1000 | Loss: 0.00001321
Iteration 156/1000 | Loss: 0.00001321
Iteration 157/1000 | Loss: 0.00001321
Iteration 158/1000 | Loss: 0.00001321
Iteration 159/1000 | Loss: 0.00001321
Iteration 160/1000 | Loss: 0.00001321
Iteration 161/1000 | Loss: 0.00001321
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.3211988516559359e-05, 1.3211988516559359e-05, 1.3211988516559359e-05, 1.3211988516559359e-05, 1.3211988516559359e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3211988516559359e-05

Optimization complete. Final v2v error: 3.0416457653045654 mm

Highest mean error: 3.143994092941284 mm for frame 75

Lowest mean error: 2.909177303314209 mm for frame 133

Saving results

Total time: 34.212305784225464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481308
Iteration 2/25 | Loss: 0.00142783
Iteration 3/25 | Loss: 0.00136457
Iteration 4/25 | Loss: 0.00135762
Iteration 5/25 | Loss: 0.00135568
Iteration 6/25 | Loss: 0.00135551
Iteration 7/25 | Loss: 0.00135551
Iteration 8/25 | Loss: 0.00135551
Iteration 9/25 | Loss: 0.00135551
Iteration 10/25 | Loss: 0.00135551
Iteration 11/25 | Loss: 0.00135551
Iteration 12/25 | Loss: 0.00135551
Iteration 13/25 | Loss: 0.00135551
Iteration 14/25 | Loss: 0.00135551
Iteration 15/25 | Loss: 0.00135551
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001355508342385292, 0.001355508342385292, 0.001355508342385292, 0.001355508342385292, 0.001355508342385292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001355508342385292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99780566
Iteration 2/25 | Loss: 0.00150169
Iteration 3/25 | Loss: 0.00150169
Iteration 4/25 | Loss: 0.00150169
Iteration 5/25 | Loss: 0.00150169
Iteration 6/25 | Loss: 0.00150169
Iteration 7/25 | Loss: 0.00150169
Iteration 8/25 | Loss: 0.00150169
Iteration 9/25 | Loss: 0.00150169
Iteration 10/25 | Loss: 0.00150169
Iteration 11/25 | Loss: 0.00150169
Iteration 12/25 | Loss: 0.00150169
Iteration 13/25 | Loss: 0.00150169
Iteration 14/25 | Loss: 0.00150169
Iteration 15/25 | Loss: 0.00150169
Iteration 16/25 | Loss: 0.00150169
Iteration 17/25 | Loss: 0.00150169
Iteration 18/25 | Loss: 0.00150169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015016874531283975, 0.0015016874531283975, 0.0015016874531283975, 0.0015016874531283975, 0.0015016874531283975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015016874531283975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150169
Iteration 2/1000 | Loss: 0.00006798
Iteration 3/1000 | Loss: 0.00003802
Iteration 4/1000 | Loss: 0.00002944
Iteration 5/1000 | Loss: 0.00002852
Iteration 6/1000 | Loss: 0.00002747
Iteration 7/1000 | Loss: 0.00002669
Iteration 8/1000 | Loss: 0.00002616
Iteration 9/1000 | Loss: 0.00002552
Iteration 10/1000 | Loss: 0.00002514
Iteration 11/1000 | Loss: 0.00002491
Iteration 12/1000 | Loss: 0.00002475
Iteration 13/1000 | Loss: 0.00002475
Iteration 14/1000 | Loss: 0.00002470
Iteration 15/1000 | Loss: 0.00002470
Iteration 16/1000 | Loss: 0.00002469
Iteration 17/1000 | Loss: 0.00002469
Iteration 18/1000 | Loss: 0.00002468
Iteration 19/1000 | Loss: 0.00002468
Iteration 20/1000 | Loss: 0.00002467
Iteration 21/1000 | Loss: 0.00002461
Iteration 22/1000 | Loss: 0.00002451
Iteration 23/1000 | Loss: 0.00002446
Iteration 24/1000 | Loss: 0.00002441
Iteration 25/1000 | Loss: 0.00002441
Iteration 26/1000 | Loss: 0.00002440
Iteration 27/1000 | Loss: 0.00002440
Iteration 28/1000 | Loss: 0.00002440
Iteration 29/1000 | Loss: 0.00002437
Iteration 30/1000 | Loss: 0.00002437
Iteration 31/1000 | Loss: 0.00002435
Iteration 32/1000 | Loss: 0.00002435
Iteration 33/1000 | Loss: 0.00002435
Iteration 34/1000 | Loss: 0.00002434
Iteration 35/1000 | Loss: 0.00002433
Iteration 36/1000 | Loss: 0.00002431
Iteration 37/1000 | Loss: 0.00002431
Iteration 38/1000 | Loss: 0.00002431
Iteration 39/1000 | Loss: 0.00002431
Iteration 40/1000 | Loss: 0.00002431
Iteration 41/1000 | Loss: 0.00002431
Iteration 42/1000 | Loss: 0.00002431
Iteration 43/1000 | Loss: 0.00002431
Iteration 44/1000 | Loss: 0.00002431
Iteration 45/1000 | Loss: 0.00002431
Iteration 46/1000 | Loss: 0.00002430
Iteration 47/1000 | Loss: 0.00002430
Iteration 48/1000 | Loss: 0.00002429
Iteration 49/1000 | Loss: 0.00002428
Iteration 50/1000 | Loss: 0.00002428
Iteration 51/1000 | Loss: 0.00002428
Iteration 52/1000 | Loss: 0.00002428
Iteration 53/1000 | Loss: 0.00002427
Iteration 54/1000 | Loss: 0.00002427
Iteration 55/1000 | Loss: 0.00002427
Iteration 56/1000 | Loss: 0.00002427
Iteration 57/1000 | Loss: 0.00002427
Iteration 58/1000 | Loss: 0.00002427
Iteration 59/1000 | Loss: 0.00002426
Iteration 60/1000 | Loss: 0.00002426
Iteration 61/1000 | Loss: 0.00002425
Iteration 62/1000 | Loss: 0.00002423
Iteration 63/1000 | Loss: 0.00002422
Iteration 64/1000 | Loss: 0.00002422
Iteration 65/1000 | Loss: 0.00002422
Iteration 66/1000 | Loss: 0.00002419
Iteration 67/1000 | Loss: 0.00002416
Iteration 68/1000 | Loss: 0.00002415
Iteration 69/1000 | Loss: 0.00002414
Iteration 70/1000 | Loss: 0.00002414
Iteration 71/1000 | Loss: 0.00002414
Iteration 72/1000 | Loss: 0.00002414
Iteration 73/1000 | Loss: 0.00002414
Iteration 74/1000 | Loss: 0.00002414
Iteration 75/1000 | Loss: 0.00002414
Iteration 76/1000 | Loss: 0.00002414
Iteration 77/1000 | Loss: 0.00002414
Iteration 78/1000 | Loss: 0.00002414
Iteration 79/1000 | Loss: 0.00002414
Iteration 80/1000 | Loss: 0.00002414
Iteration 81/1000 | Loss: 0.00002413
Iteration 82/1000 | Loss: 0.00002413
Iteration 83/1000 | Loss: 0.00002413
Iteration 84/1000 | Loss: 0.00002413
Iteration 85/1000 | Loss: 0.00002413
Iteration 86/1000 | Loss: 0.00002412
Iteration 87/1000 | Loss: 0.00002412
Iteration 88/1000 | Loss: 0.00002412
Iteration 89/1000 | Loss: 0.00002412
Iteration 90/1000 | Loss: 0.00002412
Iteration 91/1000 | Loss: 0.00002412
Iteration 92/1000 | Loss: 0.00002412
Iteration 93/1000 | Loss: 0.00002412
Iteration 94/1000 | Loss: 0.00002412
Iteration 95/1000 | Loss: 0.00002412
Iteration 96/1000 | Loss: 0.00002412
Iteration 97/1000 | Loss: 0.00002412
Iteration 98/1000 | Loss: 0.00002411
Iteration 99/1000 | Loss: 0.00002411
Iteration 100/1000 | Loss: 0.00002411
Iteration 101/1000 | Loss: 0.00002411
Iteration 102/1000 | Loss: 0.00002411
Iteration 103/1000 | Loss: 0.00002411
Iteration 104/1000 | Loss: 0.00002410
Iteration 105/1000 | Loss: 0.00002410
Iteration 106/1000 | Loss: 0.00002409
Iteration 107/1000 | Loss: 0.00002409
Iteration 108/1000 | Loss: 0.00002408
Iteration 109/1000 | Loss: 0.00002408
Iteration 110/1000 | Loss: 0.00002408
Iteration 111/1000 | Loss: 0.00002407
Iteration 112/1000 | Loss: 0.00002407
Iteration 113/1000 | Loss: 0.00002407
Iteration 114/1000 | Loss: 0.00002406
Iteration 115/1000 | Loss: 0.00002404
Iteration 116/1000 | Loss: 0.00002404
Iteration 117/1000 | Loss: 0.00002404
Iteration 118/1000 | Loss: 0.00002403
Iteration 119/1000 | Loss: 0.00002403
Iteration 120/1000 | Loss: 0.00002403
Iteration 121/1000 | Loss: 0.00002403
Iteration 122/1000 | Loss: 0.00002403
Iteration 123/1000 | Loss: 0.00002403
Iteration 124/1000 | Loss: 0.00002403
Iteration 125/1000 | Loss: 0.00002403
Iteration 126/1000 | Loss: 0.00002403
Iteration 127/1000 | Loss: 0.00002403
Iteration 128/1000 | Loss: 0.00002402
Iteration 129/1000 | Loss: 0.00002402
Iteration 130/1000 | Loss: 0.00002402
Iteration 131/1000 | Loss: 0.00002402
Iteration 132/1000 | Loss: 0.00002401
Iteration 133/1000 | Loss: 0.00002401
Iteration 134/1000 | Loss: 0.00002400
Iteration 135/1000 | Loss: 0.00002400
Iteration 136/1000 | Loss: 0.00002399
Iteration 137/1000 | Loss: 0.00002399
Iteration 138/1000 | Loss: 0.00002399
Iteration 139/1000 | Loss: 0.00002398
Iteration 140/1000 | Loss: 0.00002397
Iteration 141/1000 | Loss: 0.00002397
Iteration 142/1000 | Loss: 0.00002396
Iteration 143/1000 | Loss: 0.00002395
Iteration 144/1000 | Loss: 0.00002395
Iteration 145/1000 | Loss: 0.00002394
Iteration 146/1000 | Loss: 0.00002393
Iteration 147/1000 | Loss: 0.00002392
Iteration 148/1000 | Loss: 0.00002392
Iteration 149/1000 | Loss: 0.00002391
Iteration 150/1000 | Loss: 0.00002391
Iteration 151/1000 | Loss: 0.00002391
Iteration 152/1000 | Loss: 0.00002391
Iteration 153/1000 | Loss: 0.00002391
Iteration 154/1000 | Loss: 0.00002391
Iteration 155/1000 | Loss: 0.00002391
Iteration 156/1000 | Loss: 0.00002391
Iteration 157/1000 | Loss: 0.00002390
Iteration 158/1000 | Loss: 0.00002390
Iteration 159/1000 | Loss: 0.00002390
Iteration 160/1000 | Loss: 0.00002390
Iteration 161/1000 | Loss: 0.00002389
Iteration 162/1000 | Loss: 0.00002389
Iteration 163/1000 | Loss: 0.00002389
Iteration 164/1000 | Loss: 0.00002389
Iteration 165/1000 | Loss: 0.00002389
Iteration 166/1000 | Loss: 0.00002389
Iteration 167/1000 | Loss: 0.00002389
Iteration 168/1000 | Loss: 0.00002389
Iteration 169/1000 | Loss: 0.00002389
Iteration 170/1000 | Loss: 0.00002389
Iteration 171/1000 | Loss: 0.00002388
Iteration 172/1000 | Loss: 0.00002388
Iteration 173/1000 | Loss: 0.00002388
Iteration 174/1000 | Loss: 0.00002388
Iteration 175/1000 | Loss: 0.00002388
Iteration 176/1000 | Loss: 0.00002388
Iteration 177/1000 | Loss: 0.00002388
Iteration 178/1000 | Loss: 0.00002388
Iteration 179/1000 | Loss: 0.00002388
Iteration 180/1000 | Loss: 0.00002388
Iteration 181/1000 | Loss: 0.00002388
Iteration 182/1000 | Loss: 0.00002388
Iteration 183/1000 | Loss: 0.00002388
Iteration 184/1000 | Loss: 0.00002388
Iteration 185/1000 | Loss: 0.00002387
Iteration 186/1000 | Loss: 0.00002387
Iteration 187/1000 | Loss: 0.00002387
Iteration 188/1000 | Loss: 0.00002387
Iteration 189/1000 | Loss: 0.00002387
Iteration 190/1000 | Loss: 0.00002387
Iteration 191/1000 | Loss: 0.00002387
Iteration 192/1000 | Loss: 0.00002387
Iteration 193/1000 | Loss: 0.00002387
Iteration 194/1000 | Loss: 0.00002387
Iteration 195/1000 | Loss: 0.00002387
Iteration 196/1000 | Loss: 0.00002387
Iteration 197/1000 | Loss: 0.00002387
Iteration 198/1000 | Loss: 0.00002386
Iteration 199/1000 | Loss: 0.00002386
Iteration 200/1000 | Loss: 0.00002386
Iteration 201/1000 | Loss: 0.00002386
Iteration 202/1000 | Loss: 0.00002386
Iteration 203/1000 | Loss: 0.00002386
Iteration 204/1000 | Loss: 0.00002386
Iteration 205/1000 | Loss: 0.00002386
Iteration 206/1000 | Loss: 0.00002386
Iteration 207/1000 | Loss: 0.00002386
Iteration 208/1000 | Loss: 0.00002386
Iteration 209/1000 | Loss: 0.00002386
Iteration 210/1000 | Loss: 0.00002386
Iteration 211/1000 | Loss: 0.00002386
Iteration 212/1000 | Loss: 0.00002386
Iteration 213/1000 | Loss: 0.00002385
Iteration 214/1000 | Loss: 0.00002385
Iteration 215/1000 | Loss: 0.00002385
Iteration 216/1000 | Loss: 0.00002385
Iteration 217/1000 | Loss: 0.00002385
Iteration 218/1000 | Loss: 0.00002385
Iteration 219/1000 | Loss: 0.00002385
Iteration 220/1000 | Loss: 0.00002385
Iteration 221/1000 | Loss: 0.00002385
Iteration 222/1000 | Loss: 0.00002384
Iteration 223/1000 | Loss: 0.00002384
Iteration 224/1000 | Loss: 0.00002384
Iteration 225/1000 | Loss: 0.00002384
Iteration 226/1000 | Loss: 0.00002384
Iteration 227/1000 | Loss: 0.00002384
Iteration 228/1000 | Loss: 0.00002384
Iteration 229/1000 | Loss: 0.00002384
Iteration 230/1000 | Loss: 0.00002384
Iteration 231/1000 | Loss: 0.00002384
Iteration 232/1000 | Loss: 0.00002383
Iteration 233/1000 | Loss: 0.00002383
Iteration 234/1000 | Loss: 0.00002383
Iteration 235/1000 | Loss: 0.00002383
Iteration 236/1000 | Loss: 0.00002382
Iteration 237/1000 | Loss: 0.00002382
Iteration 238/1000 | Loss: 0.00002382
Iteration 239/1000 | Loss: 0.00002382
Iteration 240/1000 | Loss: 0.00002381
Iteration 241/1000 | Loss: 0.00002381
Iteration 242/1000 | Loss: 0.00002381
Iteration 243/1000 | Loss: 0.00002380
Iteration 244/1000 | Loss: 0.00002380
Iteration 245/1000 | Loss: 0.00002380
Iteration 246/1000 | Loss: 0.00002380
Iteration 247/1000 | Loss: 0.00002380
Iteration 248/1000 | Loss: 0.00002380
Iteration 249/1000 | Loss: 0.00002380
Iteration 250/1000 | Loss: 0.00002380
Iteration 251/1000 | Loss: 0.00002380
Iteration 252/1000 | Loss: 0.00002380
Iteration 253/1000 | Loss: 0.00002380
Iteration 254/1000 | Loss: 0.00002380
Iteration 255/1000 | Loss: 0.00002380
Iteration 256/1000 | Loss: 0.00002380
Iteration 257/1000 | Loss: 0.00002380
Iteration 258/1000 | Loss: 0.00002380
Iteration 259/1000 | Loss: 0.00002380
Iteration 260/1000 | Loss: 0.00002380
Iteration 261/1000 | Loss: 0.00002380
Iteration 262/1000 | Loss: 0.00002380
Iteration 263/1000 | Loss: 0.00002380
Iteration 264/1000 | Loss: 0.00002380
Iteration 265/1000 | Loss: 0.00002380
Iteration 266/1000 | Loss: 0.00002380
Iteration 267/1000 | Loss: 0.00002380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 267. Stopping optimization.
Last 5 losses: [2.3801325369277038e-05, 2.3801325369277038e-05, 2.3801325369277038e-05, 2.3801325369277038e-05, 2.3801325369277038e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3801325369277038e-05

Optimization complete. Final v2v error: 4.021539688110352 mm

Highest mean error: 4.18993616104126 mm for frame 119

Lowest mean error: 3.905550718307495 mm for frame 90

Saving results

Total time: 44.24764060974121
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973688
Iteration 2/25 | Loss: 0.00146438
Iteration 3/25 | Loss: 0.00137457
Iteration 4/25 | Loss: 0.00136380
Iteration 5/25 | Loss: 0.00136075
Iteration 6/25 | Loss: 0.00135990
Iteration 7/25 | Loss: 0.00135990
Iteration 8/25 | Loss: 0.00135990
Iteration 9/25 | Loss: 0.00135990
Iteration 10/25 | Loss: 0.00135990
Iteration 11/25 | Loss: 0.00135990
Iteration 12/25 | Loss: 0.00135990
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013599037192761898, 0.0013599037192761898, 0.0013599037192761898, 0.0013599037192761898, 0.0013599037192761898]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013599037192761898

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.32401991
Iteration 2/25 | Loss: 0.00238592
Iteration 3/25 | Loss: 0.00238588
Iteration 4/25 | Loss: 0.00238588
Iteration 5/25 | Loss: 0.00238588
Iteration 6/25 | Loss: 0.00238588
Iteration 7/25 | Loss: 0.00238588
Iteration 8/25 | Loss: 0.00238588
Iteration 9/25 | Loss: 0.00238588
Iteration 10/25 | Loss: 0.00238588
Iteration 11/25 | Loss: 0.00238588
Iteration 12/25 | Loss: 0.00238588
Iteration 13/25 | Loss: 0.00238588
Iteration 14/25 | Loss: 0.00238588
Iteration 15/25 | Loss: 0.00238588
Iteration 16/25 | Loss: 0.00238588
Iteration 17/25 | Loss: 0.00238588
Iteration 18/25 | Loss: 0.00238588
Iteration 19/25 | Loss: 0.00238588
Iteration 20/25 | Loss: 0.00238588
Iteration 21/25 | Loss: 0.00238588
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0023858759086579084, 0.0023858759086579084, 0.0023858759086579084, 0.0023858759086579084, 0.0023858759086579084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023858759086579084

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238588
Iteration 2/1000 | Loss: 0.00004848
Iteration 3/1000 | Loss: 0.00003087
Iteration 4/1000 | Loss: 0.00002555
Iteration 5/1000 | Loss: 0.00002234
Iteration 6/1000 | Loss: 0.00002125
Iteration 7/1000 | Loss: 0.00002068
Iteration 8/1000 | Loss: 0.00002023
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001967
Iteration 11/1000 | Loss: 0.00001957
Iteration 12/1000 | Loss: 0.00001938
Iteration 13/1000 | Loss: 0.00001935
Iteration 14/1000 | Loss: 0.00001935
Iteration 15/1000 | Loss: 0.00001929
Iteration 16/1000 | Loss: 0.00001929
Iteration 17/1000 | Loss: 0.00001926
Iteration 18/1000 | Loss: 0.00001926
Iteration 19/1000 | Loss: 0.00001925
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001915
Iteration 22/1000 | Loss: 0.00001914
Iteration 23/1000 | Loss: 0.00001914
Iteration 24/1000 | Loss: 0.00001913
Iteration 25/1000 | Loss: 0.00001913
Iteration 26/1000 | Loss: 0.00001912
Iteration 27/1000 | Loss: 0.00001912
Iteration 28/1000 | Loss: 0.00001912
Iteration 29/1000 | Loss: 0.00001912
Iteration 30/1000 | Loss: 0.00001911
Iteration 31/1000 | Loss: 0.00001910
Iteration 32/1000 | Loss: 0.00001910
Iteration 33/1000 | Loss: 0.00001910
Iteration 34/1000 | Loss: 0.00001910
Iteration 35/1000 | Loss: 0.00001909
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001909
Iteration 38/1000 | Loss: 0.00001909
Iteration 39/1000 | Loss: 0.00001909
Iteration 40/1000 | Loss: 0.00001909
Iteration 41/1000 | Loss: 0.00001909
Iteration 42/1000 | Loss: 0.00001909
Iteration 43/1000 | Loss: 0.00001909
Iteration 44/1000 | Loss: 0.00001909
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 48. Stopping optimization.
Last 5 losses: [1.9092429283773527e-05, 1.9092429283773527e-05, 1.9092429283773527e-05, 1.9092429283773527e-05, 1.9092429283773527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9092429283773527e-05

Optimization complete. Final v2v error: 3.761220932006836 mm

Highest mean error: 4.121586322784424 mm for frame 68

Lowest mean error: 3.3825907707214355 mm for frame 24

Saving results

Total time: 30.61555027961731
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819996
Iteration 2/25 | Loss: 0.00169346
Iteration 3/25 | Loss: 0.00145345
Iteration 4/25 | Loss: 0.00143452
Iteration 5/25 | Loss: 0.00142894
Iteration 6/25 | Loss: 0.00142753
Iteration 7/25 | Loss: 0.00142749
Iteration 8/25 | Loss: 0.00142749
Iteration 9/25 | Loss: 0.00142749
Iteration 10/25 | Loss: 0.00142749
Iteration 11/25 | Loss: 0.00142749
Iteration 12/25 | Loss: 0.00142749
Iteration 13/25 | Loss: 0.00142749
Iteration 14/25 | Loss: 0.00142749
Iteration 15/25 | Loss: 0.00142739
Iteration 16/25 | Loss: 0.00142739
Iteration 17/25 | Loss: 0.00142739
Iteration 18/25 | Loss: 0.00142739
Iteration 19/25 | Loss: 0.00142739
Iteration 20/25 | Loss: 0.00142739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014273908454924822, 0.0014273908454924822, 0.0014273908454924822, 0.0014273908454924822, 0.0014273908454924822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014273908454924822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26816761
Iteration 2/25 | Loss: 0.00163160
Iteration 3/25 | Loss: 0.00163157
Iteration 4/25 | Loss: 0.00163157
Iteration 5/25 | Loss: 0.00163157
Iteration 6/25 | Loss: 0.00163157
Iteration 7/25 | Loss: 0.00163157
Iteration 8/25 | Loss: 0.00163157
Iteration 9/25 | Loss: 0.00163157
Iteration 10/25 | Loss: 0.00163157
Iteration 11/25 | Loss: 0.00163157
Iteration 12/25 | Loss: 0.00163157
Iteration 13/25 | Loss: 0.00163157
Iteration 14/25 | Loss: 0.00163157
Iteration 15/25 | Loss: 0.00163157
Iteration 16/25 | Loss: 0.00163157
Iteration 17/25 | Loss: 0.00163157
Iteration 18/25 | Loss: 0.00163157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016315695829689503, 0.0016315695829689503, 0.0016315695829689503, 0.0016315695829689503, 0.0016315695829689503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016315695829689503

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163157
Iteration 2/1000 | Loss: 0.00007855
Iteration 3/1000 | Loss: 0.00005138
Iteration 4/1000 | Loss: 0.00004478
Iteration 5/1000 | Loss: 0.00004223
Iteration 6/1000 | Loss: 0.00004132
Iteration 7/1000 | Loss: 0.00004033
Iteration 8/1000 | Loss: 0.00003968
Iteration 9/1000 | Loss: 0.00003926
Iteration 10/1000 | Loss: 0.00003892
Iteration 11/1000 | Loss: 0.00003854
Iteration 12/1000 | Loss: 0.00003824
Iteration 13/1000 | Loss: 0.00003797
Iteration 14/1000 | Loss: 0.00003778
Iteration 15/1000 | Loss: 0.00003757
Iteration 16/1000 | Loss: 0.00003739
Iteration 17/1000 | Loss: 0.00003728
Iteration 18/1000 | Loss: 0.00003714
Iteration 19/1000 | Loss: 0.00003700
Iteration 20/1000 | Loss: 0.00003689
Iteration 21/1000 | Loss: 0.00003683
Iteration 22/1000 | Loss: 0.00003681
Iteration 23/1000 | Loss: 0.00003677
Iteration 24/1000 | Loss: 0.00003676
Iteration 25/1000 | Loss: 0.00003676
Iteration 26/1000 | Loss: 0.00003673
Iteration 27/1000 | Loss: 0.00003672
Iteration 28/1000 | Loss: 0.00003672
Iteration 29/1000 | Loss: 0.00003672
Iteration 30/1000 | Loss: 0.00003672
Iteration 31/1000 | Loss: 0.00003672
Iteration 32/1000 | Loss: 0.00003672
Iteration 33/1000 | Loss: 0.00003672
Iteration 34/1000 | Loss: 0.00003672
Iteration 35/1000 | Loss: 0.00003672
Iteration 36/1000 | Loss: 0.00003671
Iteration 37/1000 | Loss: 0.00003671
Iteration 38/1000 | Loss: 0.00003671
Iteration 39/1000 | Loss: 0.00003670
Iteration 40/1000 | Loss: 0.00003670
Iteration 41/1000 | Loss: 0.00003670
Iteration 42/1000 | Loss: 0.00003670
Iteration 43/1000 | Loss: 0.00003670
Iteration 44/1000 | Loss: 0.00003670
Iteration 45/1000 | Loss: 0.00003670
Iteration 46/1000 | Loss: 0.00003670
Iteration 47/1000 | Loss: 0.00003669
Iteration 48/1000 | Loss: 0.00003669
Iteration 49/1000 | Loss: 0.00003669
Iteration 50/1000 | Loss: 0.00003669
Iteration 51/1000 | Loss: 0.00003669
Iteration 52/1000 | Loss: 0.00003669
Iteration 53/1000 | Loss: 0.00003669
Iteration 54/1000 | Loss: 0.00003669
Iteration 55/1000 | Loss: 0.00003669
Iteration 56/1000 | Loss: 0.00003669
Iteration 57/1000 | Loss: 0.00003669
Iteration 58/1000 | Loss: 0.00003669
Iteration 59/1000 | Loss: 0.00003669
Iteration 60/1000 | Loss: 0.00003669
Iteration 61/1000 | Loss: 0.00003669
Iteration 62/1000 | Loss: 0.00003669
Iteration 63/1000 | Loss: 0.00003669
Iteration 64/1000 | Loss: 0.00003669
Iteration 65/1000 | Loss: 0.00003669
Iteration 66/1000 | Loss: 0.00003669
Iteration 67/1000 | Loss: 0.00003669
Iteration 68/1000 | Loss: 0.00003669
Iteration 69/1000 | Loss: 0.00003669
Iteration 70/1000 | Loss: 0.00003669
Iteration 71/1000 | Loss: 0.00003669
Iteration 72/1000 | Loss: 0.00003669
Iteration 73/1000 | Loss: 0.00003669
Iteration 74/1000 | Loss: 0.00003669
Iteration 75/1000 | Loss: 0.00003669
Iteration 76/1000 | Loss: 0.00003669
Iteration 77/1000 | Loss: 0.00003669
Iteration 78/1000 | Loss: 0.00003669
Iteration 79/1000 | Loss: 0.00003669
Iteration 80/1000 | Loss: 0.00003669
Iteration 81/1000 | Loss: 0.00003669
Iteration 82/1000 | Loss: 0.00003669
Iteration 83/1000 | Loss: 0.00003669
Iteration 84/1000 | Loss: 0.00003669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 84. Stopping optimization.
Last 5 losses: [3.668724821181968e-05, 3.668724821181968e-05, 3.668724821181968e-05, 3.668724821181968e-05, 3.668724821181968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.668724821181968e-05

Optimization complete. Final v2v error: 4.807823181152344 mm

Highest mean error: 5.30129861831665 mm for frame 84

Lowest mean error: 3.3417809009552 mm for frame 1

Saving results

Total time: 42.37360095977783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00515201
Iteration 2/25 | Loss: 0.00155041
Iteration 3/25 | Loss: 0.00136949
Iteration 4/25 | Loss: 0.00135310
Iteration 5/25 | Loss: 0.00134689
Iteration 6/25 | Loss: 0.00134526
Iteration 7/25 | Loss: 0.00134526
Iteration 8/25 | Loss: 0.00134526
Iteration 9/25 | Loss: 0.00134526
Iteration 10/25 | Loss: 0.00134526
Iteration 11/25 | Loss: 0.00134526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001345261000096798, 0.001345261000096798, 0.001345261000096798, 0.001345261000096798, 0.001345261000096798]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001345261000096798

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74183768
Iteration 2/25 | Loss: 0.00176607
Iteration 3/25 | Loss: 0.00176607
Iteration 4/25 | Loss: 0.00176607
Iteration 5/25 | Loss: 0.00176607
Iteration 6/25 | Loss: 0.00176607
Iteration 7/25 | Loss: 0.00176607
Iteration 8/25 | Loss: 0.00176607
Iteration 9/25 | Loss: 0.00176607
Iteration 10/25 | Loss: 0.00176607
Iteration 11/25 | Loss: 0.00176607
Iteration 12/25 | Loss: 0.00176607
Iteration 13/25 | Loss: 0.00176607
Iteration 14/25 | Loss: 0.00176607
Iteration 15/25 | Loss: 0.00176607
Iteration 16/25 | Loss: 0.00176607
Iteration 17/25 | Loss: 0.00176607
Iteration 18/25 | Loss: 0.00176607
Iteration 19/25 | Loss: 0.00176607
Iteration 20/25 | Loss: 0.00176607
Iteration 21/25 | Loss: 0.00176607
Iteration 22/25 | Loss: 0.00176607
Iteration 23/25 | Loss: 0.00176607
Iteration 24/25 | Loss: 0.00176607
Iteration 25/25 | Loss: 0.00176607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176607
Iteration 2/1000 | Loss: 0.00005577
Iteration 3/1000 | Loss: 0.00004222
Iteration 4/1000 | Loss: 0.00003672
Iteration 5/1000 | Loss: 0.00003476
Iteration 6/1000 | Loss: 0.00003383
Iteration 7/1000 | Loss: 0.00003309
Iteration 8/1000 | Loss: 0.00003248
Iteration 9/1000 | Loss: 0.00003188
Iteration 10/1000 | Loss: 0.00003156
Iteration 11/1000 | Loss: 0.00003127
Iteration 12/1000 | Loss: 0.00003104
Iteration 13/1000 | Loss: 0.00003083
Iteration 14/1000 | Loss: 0.00003062
Iteration 15/1000 | Loss: 0.00003047
Iteration 16/1000 | Loss: 0.00003046
Iteration 17/1000 | Loss: 0.00003025
Iteration 18/1000 | Loss: 0.00003023
Iteration 19/1000 | Loss: 0.00003013
Iteration 20/1000 | Loss: 0.00003011
Iteration 21/1000 | Loss: 0.00003008
Iteration 22/1000 | Loss: 0.00003004
Iteration 23/1000 | Loss: 0.00003000
Iteration 24/1000 | Loss: 0.00002975
Iteration 25/1000 | Loss: 0.00002957
Iteration 26/1000 | Loss: 0.00002941
Iteration 27/1000 | Loss: 0.00002931
Iteration 28/1000 | Loss: 0.00002931
Iteration 29/1000 | Loss: 0.00002931
Iteration 30/1000 | Loss: 0.00002931
Iteration 31/1000 | Loss: 0.00002930
Iteration 32/1000 | Loss: 0.00002930
Iteration 33/1000 | Loss: 0.00002930
Iteration 34/1000 | Loss: 0.00002930
Iteration 35/1000 | Loss: 0.00002930
Iteration 36/1000 | Loss: 0.00002924
Iteration 37/1000 | Loss: 0.00002924
Iteration 38/1000 | Loss: 0.00002922
Iteration 39/1000 | Loss: 0.00002921
Iteration 40/1000 | Loss: 0.00002920
Iteration 41/1000 | Loss: 0.00002920
Iteration 42/1000 | Loss: 0.00002919
Iteration 43/1000 | Loss: 0.00002919
Iteration 44/1000 | Loss: 0.00002918
Iteration 45/1000 | Loss: 0.00002918
Iteration 46/1000 | Loss: 0.00002917
Iteration 47/1000 | Loss: 0.00002916
Iteration 48/1000 | Loss: 0.00002915
Iteration 49/1000 | Loss: 0.00002915
Iteration 50/1000 | Loss: 0.00002914
Iteration 51/1000 | Loss: 0.00002913
Iteration 52/1000 | Loss: 0.00002913
Iteration 53/1000 | Loss: 0.00002912
Iteration 54/1000 | Loss: 0.00002912
Iteration 55/1000 | Loss: 0.00002912
Iteration 56/1000 | Loss: 0.00002912
Iteration 57/1000 | Loss: 0.00002912
Iteration 58/1000 | Loss: 0.00002912
Iteration 59/1000 | Loss: 0.00002912
Iteration 60/1000 | Loss: 0.00002910
Iteration 61/1000 | Loss: 0.00002909
Iteration 62/1000 | Loss: 0.00002908
Iteration 63/1000 | Loss: 0.00002905
Iteration 64/1000 | Loss: 0.00002903
Iteration 65/1000 | Loss: 0.00002903
Iteration 66/1000 | Loss: 0.00002903
Iteration 67/1000 | Loss: 0.00002897
Iteration 68/1000 | Loss: 0.00002897
Iteration 69/1000 | Loss: 0.00002897
Iteration 70/1000 | Loss: 0.00002896
Iteration 71/1000 | Loss: 0.00002895
Iteration 72/1000 | Loss: 0.00002895
Iteration 73/1000 | Loss: 0.00002895
Iteration 74/1000 | Loss: 0.00002894
Iteration 75/1000 | Loss: 0.00002894
Iteration 76/1000 | Loss: 0.00002894
Iteration 77/1000 | Loss: 0.00002893
Iteration 78/1000 | Loss: 0.00002893
Iteration 79/1000 | Loss: 0.00002893
Iteration 80/1000 | Loss: 0.00002893
Iteration 81/1000 | Loss: 0.00002893
Iteration 82/1000 | Loss: 0.00002893
Iteration 83/1000 | Loss: 0.00002892
Iteration 84/1000 | Loss: 0.00002891
Iteration 85/1000 | Loss: 0.00002891
Iteration 86/1000 | Loss: 0.00002891
Iteration 87/1000 | Loss: 0.00002891
Iteration 88/1000 | Loss: 0.00002891
Iteration 89/1000 | Loss: 0.00002891
Iteration 90/1000 | Loss: 0.00002891
Iteration 91/1000 | Loss: 0.00002891
Iteration 92/1000 | Loss: 0.00002891
Iteration 93/1000 | Loss: 0.00002891
Iteration 94/1000 | Loss: 0.00002890
Iteration 95/1000 | Loss: 0.00002890
Iteration 96/1000 | Loss: 0.00002890
Iteration 97/1000 | Loss: 0.00002889
Iteration 98/1000 | Loss: 0.00002889
Iteration 99/1000 | Loss: 0.00002889
Iteration 100/1000 | Loss: 0.00002889
Iteration 101/1000 | Loss: 0.00002889
Iteration 102/1000 | Loss: 0.00002888
Iteration 103/1000 | Loss: 0.00002888
Iteration 104/1000 | Loss: 0.00002888
Iteration 105/1000 | Loss: 0.00002888
Iteration 106/1000 | Loss: 0.00002888
Iteration 107/1000 | Loss: 0.00002888
Iteration 108/1000 | Loss: 0.00002888
Iteration 109/1000 | Loss: 0.00002888
Iteration 110/1000 | Loss: 0.00002888
Iteration 111/1000 | Loss: 0.00002888
Iteration 112/1000 | Loss: 0.00002887
Iteration 113/1000 | Loss: 0.00002887
Iteration 114/1000 | Loss: 0.00002887
Iteration 115/1000 | Loss: 0.00002887
Iteration 116/1000 | Loss: 0.00002887
Iteration 117/1000 | Loss: 0.00002887
Iteration 118/1000 | Loss: 0.00002887
Iteration 119/1000 | Loss: 0.00002886
Iteration 120/1000 | Loss: 0.00002886
Iteration 121/1000 | Loss: 0.00002886
Iteration 122/1000 | Loss: 0.00002886
Iteration 123/1000 | Loss: 0.00002886
Iteration 124/1000 | Loss: 0.00002886
Iteration 125/1000 | Loss: 0.00002886
Iteration 126/1000 | Loss: 0.00002886
Iteration 127/1000 | Loss: 0.00002886
Iteration 128/1000 | Loss: 0.00002886
Iteration 129/1000 | Loss: 0.00002886
Iteration 130/1000 | Loss: 0.00002886
Iteration 131/1000 | Loss: 0.00002886
Iteration 132/1000 | Loss: 0.00002886
Iteration 133/1000 | Loss: 0.00002886
Iteration 134/1000 | Loss: 0.00002886
Iteration 135/1000 | Loss: 0.00002886
Iteration 136/1000 | Loss: 0.00002886
Iteration 137/1000 | Loss: 0.00002886
Iteration 138/1000 | Loss: 0.00002886
Iteration 139/1000 | Loss: 0.00002886
Iteration 140/1000 | Loss: 0.00002886
Iteration 141/1000 | Loss: 0.00002886
Iteration 142/1000 | Loss: 0.00002886
Iteration 143/1000 | Loss: 0.00002886
Iteration 144/1000 | Loss: 0.00002886
Iteration 145/1000 | Loss: 0.00002886
Iteration 146/1000 | Loss: 0.00002886
Iteration 147/1000 | Loss: 0.00002886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.886188667616807e-05, 2.886188667616807e-05, 2.886188667616807e-05, 2.886188667616807e-05, 2.886188667616807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.886188667616807e-05

Optimization complete. Final v2v error: 4.488107681274414 mm

Highest mean error: 4.772154331207275 mm for frame 23

Lowest mean error: 4.28464937210083 mm for frame 104

Saving results

Total time: 60.33816075325012
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088418
Iteration 2/25 | Loss: 0.00259620
Iteration 3/25 | Loss: 0.00221728
Iteration 4/25 | Loss: 0.00210920
Iteration 5/25 | Loss: 0.00183303
Iteration 6/25 | Loss: 0.00153708
Iteration 7/25 | Loss: 0.00142995
Iteration 8/25 | Loss: 0.00140784
Iteration 9/25 | Loss: 0.00140517
Iteration 10/25 | Loss: 0.00140455
Iteration 11/25 | Loss: 0.00140455
Iteration 12/25 | Loss: 0.00140455
Iteration 13/25 | Loss: 0.00140455
Iteration 14/25 | Loss: 0.00140455
Iteration 15/25 | Loss: 0.00140455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001404549810104072, 0.001404549810104072, 0.001404549810104072, 0.001404549810104072, 0.001404549810104072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001404549810104072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23813498
Iteration 2/25 | Loss: 0.00138645
Iteration 3/25 | Loss: 0.00138644
Iteration 4/25 | Loss: 0.00138644
Iteration 5/25 | Loss: 0.00138644
Iteration 6/25 | Loss: 0.00138644
Iteration 7/25 | Loss: 0.00138644
Iteration 8/25 | Loss: 0.00138644
Iteration 9/25 | Loss: 0.00138644
Iteration 10/25 | Loss: 0.00138644
Iteration 11/25 | Loss: 0.00138644
Iteration 12/25 | Loss: 0.00138644
Iteration 13/25 | Loss: 0.00138644
Iteration 14/25 | Loss: 0.00138644
Iteration 15/25 | Loss: 0.00138644
Iteration 16/25 | Loss: 0.00138644
Iteration 17/25 | Loss: 0.00138644
Iteration 18/25 | Loss: 0.00138644
Iteration 19/25 | Loss: 0.00138644
Iteration 20/25 | Loss: 0.00138644
Iteration 21/25 | Loss: 0.00138644
Iteration 22/25 | Loss: 0.00138644
Iteration 23/25 | Loss: 0.00138644
Iteration 24/25 | Loss: 0.00138644
Iteration 25/25 | Loss: 0.00138644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138644
Iteration 2/1000 | Loss: 0.00004529
Iteration 3/1000 | Loss: 0.00002965
Iteration 4/1000 | Loss: 0.00002663
Iteration 5/1000 | Loss: 0.00002518
Iteration 6/1000 | Loss: 0.00002430
Iteration 7/1000 | Loss: 0.00002332
Iteration 8/1000 | Loss: 0.00002286
Iteration 9/1000 | Loss: 0.00002248
Iteration 10/1000 | Loss: 0.00002221
Iteration 11/1000 | Loss: 0.00002212
Iteration 12/1000 | Loss: 0.00002197
Iteration 13/1000 | Loss: 0.00002194
Iteration 14/1000 | Loss: 0.00002194
Iteration 15/1000 | Loss: 0.00002193
Iteration 16/1000 | Loss: 0.00002191
Iteration 17/1000 | Loss: 0.00002188
Iteration 18/1000 | Loss: 0.00002188
Iteration 19/1000 | Loss: 0.00002187
Iteration 20/1000 | Loss: 0.00002186
Iteration 21/1000 | Loss: 0.00002186
Iteration 22/1000 | Loss: 0.00002186
Iteration 23/1000 | Loss: 0.00002185
Iteration 24/1000 | Loss: 0.00002185
Iteration 25/1000 | Loss: 0.00002184
Iteration 26/1000 | Loss: 0.00002184
Iteration 27/1000 | Loss: 0.00002184
Iteration 28/1000 | Loss: 0.00002183
Iteration 29/1000 | Loss: 0.00002183
Iteration 30/1000 | Loss: 0.00002183
Iteration 31/1000 | Loss: 0.00002182
Iteration 32/1000 | Loss: 0.00002182
Iteration 33/1000 | Loss: 0.00002182
Iteration 34/1000 | Loss: 0.00002182
Iteration 35/1000 | Loss: 0.00002181
Iteration 36/1000 | Loss: 0.00002181
Iteration 37/1000 | Loss: 0.00002181
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002180
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002180
Iteration 42/1000 | Loss: 0.00002179
Iteration 43/1000 | Loss: 0.00002179
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002178
Iteration 46/1000 | Loss: 0.00002178
Iteration 47/1000 | Loss: 0.00002178
Iteration 48/1000 | Loss: 0.00002177
Iteration 49/1000 | Loss: 0.00002177
Iteration 50/1000 | Loss: 0.00002177
Iteration 51/1000 | Loss: 0.00002177
Iteration 52/1000 | Loss: 0.00002177
Iteration 53/1000 | Loss: 0.00002177
Iteration 54/1000 | Loss: 0.00002176
Iteration 55/1000 | Loss: 0.00002175
Iteration 56/1000 | Loss: 0.00002175
Iteration 57/1000 | Loss: 0.00002175
Iteration 58/1000 | Loss: 0.00002174
Iteration 59/1000 | Loss: 0.00002174
Iteration 60/1000 | Loss: 0.00002174
Iteration 61/1000 | Loss: 0.00002173
Iteration 62/1000 | Loss: 0.00002173
Iteration 63/1000 | Loss: 0.00002173
Iteration 64/1000 | Loss: 0.00002172
Iteration 65/1000 | Loss: 0.00002172
Iteration 66/1000 | Loss: 0.00002172
Iteration 67/1000 | Loss: 0.00002171
Iteration 68/1000 | Loss: 0.00002171
Iteration 69/1000 | Loss: 0.00002171
Iteration 70/1000 | Loss: 0.00002171
Iteration 71/1000 | Loss: 0.00002170
Iteration 72/1000 | Loss: 0.00002170
Iteration 73/1000 | Loss: 0.00002170
Iteration 74/1000 | Loss: 0.00002170
Iteration 75/1000 | Loss: 0.00002170
Iteration 76/1000 | Loss: 0.00002170
Iteration 77/1000 | Loss: 0.00002170
Iteration 78/1000 | Loss: 0.00002170
Iteration 79/1000 | Loss: 0.00002170
Iteration 80/1000 | Loss: 0.00002170
Iteration 81/1000 | Loss: 0.00002170
Iteration 82/1000 | Loss: 0.00002169
Iteration 83/1000 | Loss: 0.00002169
Iteration 84/1000 | Loss: 0.00002169
Iteration 85/1000 | Loss: 0.00002169
Iteration 86/1000 | Loss: 0.00002169
Iteration 87/1000 | Loss: 0.00002169
Iteration 88/1000 | Loss: 0.00002169
Iteration 89/1000 | Loss: 0.00002169
Iteration 90/1000 | Loss: 0.00002169
Iteration 91/1000 | Loss: 0.00002169
Iteration 92/1000 | Loss: 0.00002169
Iteration 93/1000 | Loss: 0.00002169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [2.1688943888875656e-05, 2.1688943888875656e-05, 2.1688943888875656e-05, 2.1688943888875656e-05, 2.1688943888875656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1688943888875656e-05

Optimization complete. Final v2v error: 4.015326023101807 mm

Highest mean error: 4.32158088684082 mm for frame 70

Lowest mean error: 3.8110501766204834 mm for frame 83

Saving results

Total time: 45.209341526031494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896502
Iteration 2/25 | Loss: 0.00160205
Iteration 3/25 | Loss: 0.00133638
Iteration 4/25 | Loss: 0.00131057
Iteration 5/25 | Loss: 0.00130877
Iteration 6/25 | Loss: 0.00130877
Iteration 7/25 | Loss: 0.00130877
Iteration 8/25 | Loss: 0.00130877
Iteration 9/25 | Loss: 0.00130877
Iteration 10/25 | Loss: 0.00130877
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013087742263451219, 0.0013087742263451219, 0.0013087742263451219, 0.0013087742263451219, 0.0013087742263451219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013087742263451219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25699389
Iteration 2/25 | Loss: 0.00159455
Iteration 3/25 | Loss: 0.00159454
Iteration 4/25 | Loss: 0.00159454
Iteration 5/25 | Loss: 0.00159453
Iteration 6/25 | Loss: 0.00159453
Iteration 7/25 | Loss: 0.00159453
Iteration 8/25 | Loss: 0.00159453
Iteration 9/25 | Loss: 0.00159453
Iteration 10/25 | Loss: 0.00159453
Iteration 11/25 | Loss: 0.00159453
Iteration 12/25 | Loss: 0.00159453
Iteration 13/25 | Loss: 0.00159453
Iteration 14/25 | Loss: 0.00159453
Iteration 15/25 | Loss: 0.00159453
Iteration 16/25 | Loss: 0.00159453
Iteration 17/25 | Loss: 0.00159453
Iteration 18/25 | Loss: 0.00159453
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015945332124829292, 0.0015945332124829292, 0.0015945332124829292, 0.0015945332124829292, 0.0015945332124829292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015945332124829292

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159453
Iteration 2/1000 | Loss: 0.00003085
Iteration 3/1000 | Loss: 0.00002240
Iteration 4/1000 | Loss: 0.00001987
Iteration 5/1000 | Loss: 0.00001861
Iteration 6/1000 | Loss: 0.00001786
Iteration 7/1000 | Loss: 0.00001734
Iteration 8/1000 | Loss: 0.00001696
Iteration 9/1000 | Loss: 0.00001688
Iteration 10/1000 | Loss: 0.00001674
Iteration 11/1000 | Loss: 0.00001656
Iteration 12/1000 | Loss: 0.00001649
Iteration 13/1000 | Loss: 0.00001643
Iteration 14/1000 | Loss: 0.00001630
Iteration 15/1000 | Loss: 0.00001623
Iteration 16/1000 | Loss: 0.00001620
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001619
Iteration 19/1000 | Loss: 0.00001617
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001616
Iteration 22/1000 | Loss: 0.00001616
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001615
Iteration 25/1000 | Loss: 0.00001614
Iteration 26/1000 | Loss: 0.00001614
Iteration 27/1000 | Loss: 0.00001613
Iteration 28/1000 | Loss: 0.00001612
Iteration 29/1000 | Loss: 0.00001611
Iteration 30/1000 | Loss: 0.00001611
Iteration 31/1000 | Loss: 0.00001610
Iteration 32/1000 | Loss: 0.00001610
Iteration 33/1000 | Loss: 0.00001609
Iteration 34/1000 | Loss: 0.00001609
Iteration 35/1000 | Loss: 0.00001608
Iteration 36/1000 | Loss: 0.00001608
Iteration 37/1000 | Loss: 0.00001607
Iteration 38/1000 | Loss: 0.00001607
Iteration 39/1000 | Loss: 0.00001607
Iteration 40/1000 | Loss: 0.00001607
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001606
Iteration 43/1000 | Loss: 0.00001606
Iteration 44/1000 | Loss: 0.00001606
Iteration 45/1000 | Loss: 0.00001606
Iteration 46/1000 | Loss: 0.00001606
Iteration 47/1000 | Loss: 0.00001606
Iteration 48/1000 | Loss: 0.00001605
Iteration 49/1000 | Loss: 0.00001605
Iteration 50/1000 | Loss: 0.00001604
Iteration 51/1000 | Loss: 0.00001604
Iteration 52/1000 | Loss: 0.00001604
Iteration 53/1000 | Loss: 0.00001604
Iteration 54/1000 | Loss: 0.00001603
Iteration 55/1000 | Loss: 0.00001603
Iteration 56/1000 | Loss: 0.00001602
Iteration 57/1000 | Loss: 0.00001602
Iteration 58/1000 | Loss: 0.00001602
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001601
Iteration 61/1000 | Loss: 0.00001600
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001598
Iteration 64/1000 | Loss: 0.00001598
Iteration 65/1000 | Loss: 0.00001597
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001594
Iteration 68/1000 | Loss: 0.00001594
Iteration 69/1000 | Loss: 0.00001593
Iteration 70/1000 | Loss: 0.00001593
Iteration 71/1000 | Loss: 0.00001592
Iteration 72/1000 | Loss: 0.00001592
Iteration 73/1000 | Loss: 0.00001590
Iteration 74/1000 | Loss: 0.00001590
Iteration 75/1000 | Loss: 0.00001590
Iteration 76/1000 | Loss: 0.00001590
Iteration 77/1000 | Loss: 0.00001589
Iteration 78/1000 | Loss: 0.00001589
Iteration 79/1000 | Loss: 0.00001589
Iteration 80/1000 | Loss: 0.00001589
Iteration 81/1000 | Loss: 0.00001588
Iteration 82/1000 | Loss: 0.00001588
Iteration 83/1000 | Loss: 0.00001588
Iteration 84/1000 | Loss: 0.00001588
Iteration 85/1000 | Loss: 0.00001588
Iteration 86/1000 | Loss: 0.00001588
Iteration 87/1000 | Loss: 0.00001587
Iteration 88/1000 | Loss: 0.00001587
Iteration 89/1000 | Loss: 0.00001587
Iteration 90/1000 | Loss: 0.00001587
Iteration 91/1000 | Loss: 0.00001587
Iteration 92/1000 | Loss: 0.00001587
Iteration 93/1000 | Loss: 0.00001587
Iteration 94/1000 | Loss: 0.00001587
Iteration 95/1000 | Loss: 0.00001587
Iteration 96/1000 | Loss: 0.00001587
Iteration 97/1000 | Loss: 0.00001587
Iteration 98/1000 | Loss: 0.00001586
Iteration 99/1000 | Loss: 0.00001586
Iteration 100/1000 | Loss: 0.00001586
Iteration 101/1000 | Loss: 0.00001586
Iteration 102/1000 | Loss: 0.00001586
Iteration 103/1000 | Loss: 0.00001585
Iteration 104/1000 | Loss: 0.00001585
Iteration 105/1000 | Loss: 0.00001585
Iteration 106/1000 | Loss: 0.00001585
Iteration 107/1000 | Loss: 0.00001584
Iteration 108/1000 | Loss: 0.00001584
Iteration 109/1000 | Loss: 0.00001584
Iteration 110/1000 | Loss: 0.00001584
Iteration 111/1000 | Loss: 0.00001584
Iteration 112/1000 | Loss: 0.00001584
Iteration 113/1000 | Loss: 0.00001584
Iteration 114/1000 | Loss: 0.00001584
Iteration 115/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.5844974768697284e-05, 1.5844974768697284e-05, 1.5844974768697284e-05, 1.5844974768697284e-05, 1.5844974768697284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5844974768697284e-05

Optimization complete. Final v2v error: 3.3365232944488525 mm

Highest mean error: 3.756155490875244 mm for frame 135

Lowest mean error: 2.9398956298828125 mm for frame 5

Saving results

Total time: 39.144481897354126
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00454006
Iteration 2/25 | Loss: 0.00146666
Iteration 3/25 | Loss: 0.00136873
Iteration 4/25 | Loss: 0.00135979
Iteration 5/25 | Loss: 0.00135780
Iteration 6/25 | Loss: 0.00135766
Iteration 7/25 | Loss: 0.00135766
Iteration 8/25 | Loss: 0.00135766
Iteration 9/25 | Loss: 0.00135766
Iteration 10/25 | Loss: 0.00135766
Iteration 11/25 | Loss: 0.00135766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013576605124399066, 0.0013576605124399066, 0.0013576605124399066, 0.0013576605124399066, 0.0013576605124399066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013576605124399066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26360369
Iteration 2/25 | Loss: 0.00216958
Iteration 3/25 | Loss: 0.00216957
Iteration 4/25 | Loss: 0.00216957
Iteration 5/25 | Loss: 0.00216957
Iteration 6/25 | Loss: 0.00216957
Iteration 7/25 | Loss: 0.00216957
Iteration 8/25 | Loss: 0.00216957
Iteration 9/25 | Loss: 0.00216957
Iteration 10/25 | Loss: 0.00216957
Iteration 11/25 | Loss: 0.00216957
Iteration 12/25 | Loss: 0.00216957
Iteration 13/25 | Loss: 0.00216957
Iteration 14/25 | Loss: 0.00216957
Iteration 15/25 | Loss: 0.00216957
Iteration 16/25 | Loss: 0.00216957
Iteration 17/25 | Loss: 0.00216957
Iteration 18/25 | Loss: 0.00216957
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002169568557292223, 0.002169568557292223, 0.002169568557292223, 0.002169568557292223, 0.002169568557292223]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002169568557292223

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00216957
Iteration 2/1000 | Loss: 0.00005888
Iteration 3/1000 | Loss: 0.00003051
Iteration 4/1000 | Loss: 0.00002571
Iteration 5/1000 | Loss: 0.00002328
Iteration 6/1000 | Loss: 0.00002227
Iteration 7/1000 | Loss: 0.00002154
Iteration 8/1000 | Loss: 0.00002107
Iteration 9/1000 | Loss: 0.00002057
Iteration 10/1000 | Loss: 0.00002028
Iteration 11/1000 | Loss: 0.00002004
Iteration 12/1000 | Loss: 0.00001997
Iteration 13/1000 | Loss: 0.00001994
Iteration 14/1000 | Loss: 0.00001993
Iteration 15/1000 | Loss: 0.00001993
Iteration 16/1000 | Loss: 0.00001991
Iteration 17/1000 | Loss: 0.00001991
Iteration 18/1000 | Loss: 0.00001991
Iteration 19/1000 | Loss: 0.00001990
Iteration 20/1000 | Loss: 0.00001989
Iteration 21/1000 | Loss: 0.00001989
Iteration 22/1000 | Loss: 0.00001989
Iteration 23/1000 | Loss: 0.00001988
Iteration 24/1000 | Loss: 0.00001988
Iteration 25/1000 | Loss: 0.00001988
Iteration 26/1000 | Loss: 0.00001987
Iteration 27/1000 | Loss: 0.00001987
Iteration 28/1000 | Loss: 0.00001985
Iteration 29/1000 | Loss: 0.00001985
Iteration 30/1000 | Loss: 0.00001983
Iteration 31/1000 | Loss: 0.00001979
Iteration 32/1000 | Loss: 0.00001964
Iteration 33/1000 | Loss: 0.00001959
Iteration 34/1000 | Loss: 0.00001959
Iteration 35/1000 | Loss: 0.00001957
Iteration 36/1000 | Loss: 0.00001957
Iteration 37/1000 | Loss: 0.00001956
Iteration 38/1000 | Loss: 0.00001955
Iteration 39/1000 | Loss: 0.00001955
Iteration 40/1000 | Loss: 0.00001955
Iteration 41/1000 | Loss: 0.00001955
Iteration 42/1000 | Loss: 0.00001955
Iteration 43/1000 | Loss: 0.00001955
Iteration 44/1000 | Loss: 0.00001955
Iteration 45/1000 | Loss: 0.00001954
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00001953
Iteration 48/1000 | Loss: 0.00001953
Iteration 49/1000 | Loss: 0.00001953
Iteration 50/1000 | Loss: 0.00001952
Iteration 51/1000 | Loss: 0.00001952
Iteration 52/1000 | Loss: 0.00001952
Iteration 53/1000 | Loss: 0.00001952
Iteration 54/1000 | Loss: 0.00001952
Iteration 55/1000 | Loss: 0.00001952
Iteration 56/1000 | Loss: 0.00001951
Iteration 57/1000 | Loss: 0.00001951
Iteration 58/1000 | Loss: 0.00001951
Iteration 59/1000 | Loss: 0.00001951
Iteration 60/1000 | Loss: 0.00001951
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001950
Iteration 63/1000 | Loss: 0.00001950
Iteration 64/1000 | Loss: 0.00001950
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00001948
Iteration 73/1000 | Loss: 0.00001948
Iteration 74/1000 | Loss: 0.00001948
Iteration 75/1000 | Loss: 0.00001948
Iteration 76/1000 | Loss: 0.00001948
Iteration 77/1000 | Loss: 0.00001948
Iteration 78/1000 | Loss: 0.00001948
Iteration 79/1000 | Loss: 0.00001948
Iteration 80/1000 | Loss: 0.00001948
Iteration 81/1000 | Loss: 0.00001947
Iteration 82/1000 | Loss: 0.00001946
Iteration 83/1000 | Loss: 0.00001946
Iteration 84/1000 | Loss: 0.00001945
Iteration 85/1000 | Loss: 0.00001945
Iteration 86/1000 | Loss: 0.00001944
Iteration 87/1000 | Loss: 0.00001944
Iteration 88/1000 | Loss: 0.00001944
Iteration 89/1000 | Loss: 0.00001944
Iteration 90/1000 | Loss: 0.00001944
Iteration 91/1000 | Loss: 0.00001944
Iteration 92/1000 | Loss: 0.00001944
Iteration 93/1000 | Loss: 0.00001944
Iteration 94/1000 | Loss: 0.00001944
Iteration 95/1000 | Loss: 0.00001944
Iteration 96/1000 | Loss: 0.00001943
Iteration 97/1000 | Loss: 0.00001943
Iteration 98/1000 | Loss: 0.00001943
Iteration 99/1000 | Loss: 0.00001943
Iteration 100/1000 | Loss: 0.00001943
Iteration 101/1000 | Loss: 0.00001942
Iteration 102/1000 | Loss: 0.00001942
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001941
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001940
Iteration 107/1000 | Loss: 0.00001940
Iteration 108/1000 | Loss: 0.00001940
Iteration 109/1000 | Loss: 0.00001939
Iteration 110/1000 | Loss: 0.00001939
Iteration 111/1000 | Loss: 0.00001939
Iteration 112/1000 | Loss: 0.00001939
Iteration 113/1000 | Loss: 0.00001938
Iteration 114/1000 | Loss: 0.00001938
Iteration 115/1000 | Loss: 0.00001938
Iteration 116/1000 | Loss: 0.00001937
Iteration 117/1000 | Loss: 0.00001937
Iteration 118/1000 | Loss: 0.00001937
Iteration 119/1000 | Loss: 0.00001937
Iteration 120/1000 | Loss: 0.00001937
Iteration 121/1000 | Loss: 0.00001937
Iteration 122/1000 | Loss: 0.00001937
Iteration 123/1000 | Loss: 0.00001937
Iteration 124/1000 | Loss: 0.00001936
Iteration 125/1000 | Loss: 0.00001936
Iteration 126/1000 | Loss: 0.00001936
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001936
Iteration 130/1000 | Loss: 0.00001936
Iteration 131/1000 | Loss: 0.00001936
Iteration 132/1000 | Loss: 0.00001936
Iteration 133/1000 | Loss: 0.00001935
Iteration 134/1000 | Loss: 0.00001935
Iteration 135/1000 | Loss: 0.00001935
Iteration 136/1000 | Loss: 0.00001935
Iteration 137/1000 | Loss: 0.00001935
Iteration 138/1000 | Loss: 0.00001935
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001934
Iteration 144/1000 | Loss: 0.00001934
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001934
Iteration 147/1000 | Loss: 0.00001934
Iteration 148/1000 | Loss: 0.00001934
Iteration 149/1000 | Loss: 0.00001934
Iteration 150/1000 | Loss: 0.00001934
Iteration 151/1000 | Loss: 0.00001934
Iteration 152/1000 | Loss: 0.00001934
Iteration 153/1000 | Loss: 0.00001934
Iteration 154/1000 | Loss: 0.00001934
Iteration 155/1000 | Loss: 0.00001934
Iteration 156/1000 | Loss: 0.00001934
Iteration 157/1000 | Loss: 0.00001934
Iteration 158/1000 | Loss: 0.00001934
Iteration 159/1000 | Loss: 0.00001934
Iteration 160/1000 | Loss: 0.00001934
Iteration 161/1000 | Loss: 0.00001934
Iteration 162/1000 | Loss: 0.00001934
Iteration 163/1000 | Loss: 0.00001934
Iteration 164/1000 | Loss: 0.00001934
Iteration 165/1000 | Loss: 0.00001934
Iteration 166/1000 | Loss: 0.00001934
Iteration 167/1000 | Loss: 0.00001934
Iteration 168/1000 | Loss: 0.00001934
Iteration 169/1000 | Loss: 0.00001934
Iteration 170/1000 | Loss: 0.00001934
Iteration 171/1000 | Loss: 0.00001934
Iteration 172/1000 | Loss: 0.00001934
Iteration 173/1000 | Loss: 0.00001934
Iteration 174/1000 | Loss: 0.00001934
Iteration 175/1000 | Loss: 0.00001934
Iteration 176/1000 | Loss: 0.00001934
Iteration 177/1000 | Loss: 0.00001934
Iteration 178/1000 | Loss: 0.00001934
Iteration 179/1000 | Loss: 0.00001934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.933650673890952e-05, 1.933650673890952e-05, 1.933650673890952e-05, 1.933650673890952e-05, 1.933650673890952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.933650673890952e-05

Optimization complete. Final v2v error: 3.7713191509246826 mm

Highest mean error: 4.39140510559082 mm for frame 159

Lowest mean error: 3.256511688232422 mm for frame 9

Saving results

Total time: 39.081655979156494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00885067
Iteration 2/25 | Loss: 0.00169765
Iteration 3/25 | Loss: 0.00146075
Iteration 4/25 | Loss: 0.00144108
Iteration 5/25 | Loss: 0.00143824
Iteration 6/25 | Loss: 0.00143762
Iteration 7/25 | Loss: 0.00143762
Iteration 8/25 | Loss: 0.00143762
Iteration 9/25 | Loss: 0.00143762
Iteration 10/25 | Loss: 0.00143762
Iteration 11/25 | Loss: 0.00143762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014376193284988403, 0.0014376193284988403, 0.0014376193284988403, 0.0014376193284988403, 0.0014376193284988403]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014376193284988403

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18267548
Iteration 2/25 | Loss: 0.00124677
Iteration 3/25 | Loss: 0.00124671
Iteration 4/25 | Loss: 0.00124671
Iteration 5/25 | Loss: 0.00124671
Iteration 6/25 | Loss: 0.00124671
Iteration 7/25 | Loss: 0.00124671
Iteration 8/25 | Loss: 0.00124671
Iteration 9/25 | Loss: 0.00124671
Iteration 10/25 | Loss: 0.00124671
Iteration 11/25 | Loss: 0.00124671
Iteration 12/25 | Loss: 0.00124671
Iteration 13/25 | Loss: 0.00124671
Iteration 14/25 | Loss: 0.00124671
Iteration 15/25 | Loss: 0.00124671
Iteration 16/25 | Loss: 0.00124671
Iteration 17/25 | Loss: 0.00124671
Iteration 18/25 | Loss: 0.00124671
Iteration 19/25 | Loss: 0.00124671
Iteration 20/25 | Loss: 0.00124671
Iteration 21/25 | Loss: 0.00124671
Iteration 22/25 | Loss: 0.00124671
Iteration 23/25 | Loss: 0.00124671
Iteration 24/25 | Loss: 0.00124671
Iteration 25/25 | Loss: 0.00124671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124671
Iteration 2/1000 | Loss: 0.00004761
Iteration 3/1000 | Loss: 0.00003083
Iteration 4/1000 | Loss: 0.00002595
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00002330
Iteration 7/1000 | Loss: 0.00002266
Iteration 8/1000 | Loss: 0.00002235
Iteration 9/1000 | Loss: 0.00002203
Iteration 10/1000 | Loss: 0.00002181
Iteration 11/1000 | Loss: 0.00002156
Iteration 12/1000 | Loss: 0.00002136
Iteration 13/1000 | Loss: 0.00002123
Iteration 14/1000 | Loss: 0.00002109
Iteration 15/1000 | Loss: 0.00002094
Iteration 16/1000 | Loss: 0.00002079
Iteration 17/1000 | Loss: 0.00002071
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002071
Iteration 20/1000 | Loss: 0.00002071
Iteration 21/1000 | Loss: 0.00002071
Iteration 22/1000 | Loss: 0.00002071
Iteration 23/1000 | Loss: 0.00002071
Iteration 24/1000 | Loss: 0.00002071
Iteration 25/1000 | Loss: 0.00002070
Iteration 26/1000 | Loss: 0.00002070
Iteration 27/1000 | Loss: 0.00002068
Iteration 28/1000 | Loss: 0.00002068
Iteration 29/1000 | Loss: 0.00002068
Iteration 30/1000 | Loss: 0.00002068
Iteration 31/1000 | Loss: 0.00002068
Iteration 32/1000 | Loss: 0.00002068
Iteration 33/1000 | Loss: 0.00002068
Iteration 34/1000 | Loss: 0.00002068
Iteration 35/1000 | Loss: 0.00002068
Iteration 36/1000 | Loss: 0.00002067
Iteration 37/1000 | Loss: 0.00002067
Iteration 38/1000 | Loss: 0.00002067
Iteration 39/1000 | Loss: 0.00002066
Iteration 40/1000 | Loss: 0.00002066
Iteration 41/1000 | Loss: 0.00002065
Iteration 42/1000 | Loss: 0.00002065
Iteration 43/1000 | Loss: 0.00002065
Iteration 44/1000 | Loss: 0.00002065
Iteration 45/1000 | Loss: 0.00002064
Iteration 46/1000 | Loss: 0.00002064
Iteration 47/1000 | Loss: 0.00002064
Iteration 48/1000 | Loss: 0.00002064
Iteration 49/1000 | Loss: 0.00002064
Iteration 50/1000 | Loss: 0.00002064
Iteration 51/1000 | Loss: 0.00002063
Iteration 52/1000 | Loss: 0.00002063
Iteration 53/1000 | Loss: 0.00002062
Iteration 54/1000 | Loss: 0.00002061
Iteration 55/1000 | Loss: 0.00002060
Iteration 56/1000 | Loss: 0.00002060
Iteration 57/1000 | Loss: 0.00002059
Iteration 58/1000 | Loss: 0.00002058
Iteration 59/1000 | Loss: 0.00002058
Iteration 60/1000 | Loss: 0.00002058
Iteration 61/1000 | Loss: 0.00002057
Iteration 62/1000 | Loss: 0.00002057
Iteration 63/1000 | Loss: 0.00002057
Iteration 64/1000 | Loss: 0.00002057
Iteration 65/1000 | Loss: 0.00002057
Iteration 66/1000 | Loss: 0.00002057
Iteration 67/1000 | Loss: 0.00002057
Iteration 68/1000 | Loss: 0.00002056
Iteration 69/1000 | Loss: 0.00002056
Iteration 70/1000 | Loss: 0.00002056
Iteration 71/1000 | Loss: 0.00002055
Iteration 72/1000 | Loss: 0.00002055
Iteration 73/1000 | Loss: 0.00002054
Iteration 74/1000 | Loss: 0.00002054
Iteration 75/1000 | Loss: 0.00002053
Iteration 76/1000 | Loss: 0.00002053
Iteration 77/1000 | Loss: 0.00002052
Iteration 78/1000 | Loss: 0.00002052
Iteration 79/1000 | Loss: 0.00002052
Iteration 80/1000 | Loss: 0.00002052
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00002051
Iteration 83/1000 | Loss: 0.00002051
Iteration 84/1000 | Loss: 0.00002051
Iteration 85/1000 | Loss: 0.00002051
Iteration 86/1000 | Loss: 0.00002050
Iteration 87/1000 | Loss: 0.00002050
Iteration 88/1000 | Loss: 0.00002050
Iteration 89/1000 | Loss: 0.00002050
Iteration 90/1000 | Loss: 0.00002050
Iteration 91/1000 | Loss: 0.00002050
Iteration 92/1000 | Loss: 0.00002050
Iteration 93/1000 | Loss: 0.00002049
Iteration 94/1000 | Loss: 0.00002049
Iteration 95/1000 | Loss: 0.00002049
Iteration 96/1000 | Loss: 0.00002049
Iteration 97/1000 | Loss: 0.00002048
Iteration 98/1000 | Loss: 0.00002048
Iteration 99/1000 | Loss: 0.00002048
Iteration 100/1000 | Loss: 0.00002048
Iteration 101/1000 | Loss: 0.00002047
Iteration 102/1000 | Loss: 0.00002047
Iteration 103/1000 | Loss: 0.00002047
Iteration 104/1000 | Loss: 0.00002047
Iteration 105/1000 | Loss: 0.00002047
Iteration 106/1000 | Loss: 0.00002046
Iteration 107/1000 | Loss: 0.00002046
Iteration 108/1000 | Loss: 0.00002046
Iteration 109/1000 | Loss: 0.00002046
Iteration 110/1000 | Loss: 0.00002046
Iteration 111/1000 | Loss: 0.00002046
Iteration 112/1000 | Loss: 0.00002046
Iteration 113/1000 | Loss: 0.00002046
Iteration 114/1000 | Loss: 0.00002046
Iteration 115/1000 | Loss: 0.00002046
Iteration 116/1000 | Loss: 0.00002046
Iteration 117/1000 | Loss: 0.00002046
Iteration 118/1000 | Loss: 0.00002046
Iteration 119/1000 | Loss: 0.00002046
Iteration 120/1000 | Loss: 0.00002046
Iteration 121/1000 | Loss: 0.00002046
Iteration 122/1000 | Loss: 0.00002046
Iteration 123/1000 | Loss: 0.00002046
Iteration 124/1000 | Loss: 0.00002046
Iteration 125/1000 | Loss: 0.00002046
Iteration 126/1000 | Loss: 0.00002046
Iteration 127/1000 | Loss: 0.00002046
Iteration 128/1000 | Loss: 0.00002046
Iteration 129/1000 | Loss: 0.00002046
Iteration 130/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.0458255676203407e-05, 2.0458255676203407e-05, 2.0458255676203407e-05, 2.0458255676203407e-05, 2.0458255676203407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0458255676203407e-05

Optimization complete. Final v2v error: 3.86985182762146 mm

Highest mean error: 4.290658473968506 mm for frame 138

Lowest mean error: 3.4692130088806152 mm for frame 14

Saving results

Total time: 40.317787647247314
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479736
Iteration 2/25 | Loss: 0.00143317
Iteration 3/25 | Loss: 0.00133601
Iteration 4/25 | Loss: 0.00132658
Iteration 5/25 | Loss: 0.00132297
Iteration 6/25 | Loss: 0.00132236
Iteration 7/25 | Loss: 0.00132236
Iteration 8/25 | Loss: 0.00132236
Iteration 9/25 | Loss: 0.00132236
Iteration 10/25 | Loss: 0.00132236
Iteration 11/25 | Loss: 0.00132236
Iteration 12/25 | Loss: 0.00132236
Iteration 13/25 | Loss: 0.00132236
Iteration 14/25 | Loss: 0.00132236
Iteration 15/25 | Loss: 0.00132236
Iteration 16/25 | Loss: 0.00132236
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013223604764789343, 0.0013223604764789343, 0.0013223604764789343, 0.0013223604764789343, 0.0013223604764789343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013223604764789343

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31350994
Iteration 2/25 | Loss: 0.00192086
Iteration 3/25 | Loss: 0.00192086
Iteration 4/25 | Loss: 0.00192086
Iteration 5/25 | Loss: 0.00192086
Iteration 6/25 | Loss: 0.00192086
Iteration 7/25 | Loss: 0.00192086
Iteration 8/25 | Loss: 0.00192086
Iteration 9/25 | Loss: 0.00192086
Iteration 10/25 | Loss: 0.00192085
Iteration 11/25 | Loss: 0.00192085
Iteration 12/25 | Loss: 0.00192085
Iteration 13/25 | Loss: 0.00192085
Iteration 14/25 | Loss: 0.00192085
Iteration 15/25 | Loss: 0.00192085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0019208547892048955, 0.0019208547892048955, 0.0019208547892048955, 0.0019208547892048955, 0.0019208547892048955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019208547892048955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192085
Iteration 2/1000 | Loss: 0.00003825
Iteration 3/1000 | Loss: 0.00002955
Iteration 4/1000 | Loss: 0.00002595
Iteration 5/1000 | Loss: 0.00002464
Iteration 6/1000 | Loss: 0.00002382
Iteration 7/1000 | Loss: 0.00002309
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002226
Iteration 10/1000 | Loss: 0.00002220
Iteration 11/1000 | Loss: 0.00002208
Iteration 12/1000 | Loss: 0.00002203
Iteration 13/1000 | Loss: 0.00002201
Iteration 14/1000 | Loss: 0.00002201
Iteration 15/1000 | Loss: 0.00002198
Iteration 16/1000 | Loss: 0.00002196
Iteration 17/1000 | Loss: 0.00002196
Iteration 18/1000 | Loss: 0.00002195
Iteration 19/1000 | Loss: 0.00002195
Iteration 20/1000 | Loss: 0.00002194
Iteration 21/1000 | Loss: 0.00002194
Iteration 22/1000 | Loss: 0.00002193
Iteration 23/1000 | Loss: 0.00002193
Iteration 24/1000 | Loss: 0.00002192
Iteration 25/1000 | Loss: 0.00002192
Iteration 26/1000 | Loss: 0.00002192
Iteration 27/1000 | Loss: 0.00002192
Iteration 28/1000 | Loss: 0.00002192
Iteration 29/1000 | Loss: 0.00002191
Iteration 30/1000 | Loss: 0.00002191
Iteration 31/1000 | Loss: 0.00002189
Iteration 32/1000 | Loss: 0.00002189
Iteration 33/1000 | Loss: 0.00002189
Iteration 34/1000 | Loss: 0.00002188
Iteration 35/1000 | Loss: 0.00002188
Iteration 36/1000 | Loss: 0.00002188
Iteration 37/1000 | Loss: 0.00002188
Iteration 38/1000 | Loss: 0.00002188
Iteration 39/1000 | Loss: 0.00002188
Iteration 40/1000 | Loss: 0.00002188
Iteration 41/1000 | Loss: 0.00002187
Iteration 42/1000 | Loss: 0.00002187
Iteration 43/1000 | Loss: 0.00002187
Iteration 44/1000 | Loss: 0.00002186
Iteration 45/1000 | Loss: 0.00002186
Iteration 46/1000 | Loss: 0.00002186
Iteration 47/1000 | Loss: 0.00002186
Iteration 48/1000 | Loss: 0.00002186
Iteration 49/1000 | Loss: 0.00002185
Iteration 50/1000 | Loss: 0.00002185
Iteration 51/1000 | Loss: 0.00002185
Iteration 52/1000 | Loss: 0.00002185
Iteration 53/1000 | Loss: 0.00002185
Iteration 54/1000 | Loss: 0.00002185
Iteration 55/1000 | Loss: 0.00002185
Iteration 56/1000 | Loss: 0.00002185
Iteration 57/1000 | Loss: 0.00002185
Iteration 58/1000 | Loss: 0.00002185
Iteration 59/1000 | Loss: 0.00002185
Iteration 60/1000 | Loss: 0.00002184
Iteration 61/1000 | Loss: 0.00002184
Iteration 62/1000 | Loss: 0.00002184
Iteration 63/1000 | Loss: 0.00002184
Iteration 64/1000 | Loss: 0.00002184
Iteration 65/1000 | Loss: 0.00002184
Iteration 66/1000 | Loss: 0.00002184
Iteration 67/1000 | Loss: 0.00002184
Iteration 68/1000 | Loss: 0.00002184
Iteration 69/1000 | Loss: 0.00002184
Iteration 70/1000 | Loss: 0.00002183
Iteration 71/1000 | Loss: 0.00002183
Iteration 72/1000 | Loss: 0.00002183
Iteration 73/1000 | Loss: 0.00002183
Iteration 74/1000 | Loss: 0.00002183
Iteration 75/1000 | Loss: 0.00002183
Iteration 76/1000 | Loss: 0.00002182
Iteration 77/1000 | Loss: 0.00002182
Iteration 78/1000 | Loss: 0.00002182
Iteration 79/1000 | Loss: 0.00002182
Iteration 80/1000 | Loss: 0.00002182
Iteration 81/1000 | Loss: 0.00002182
Iteration 82/1000 | Loss: 0.00002182
Iteration 83/1000 | Loss: 0.00002182
Iteration 84/1000 | Loss: 0.00002181
Iteration 85/1000 | Loss: 0.00002181
Iteration 86/1000 | Loss: 0.00002181
Iteration 87/1000 | Loss: 0.00002181
Iteration 88/1000 | Loss: 0.00002181
Iteration 89/1000 | Loss: 0.00002181
Iteration 90/1000 | Loss: 0.00002181
Iteration 91/1000 | Loss: 0.00002181
Iteration 92/1000 | Loss: 0.00002181
Iteration 93/1000 | Loss: 0.00002181
Iteration 94/1000 | Loss: 0.00002181
Iteration 95/1000 | Loss: 0.00002180
Iteration 96/1000 | Loss: 0.00002180
Iteration 97/1000 | Loss: 0.00002180
Iteration 98/1000 | Loss: 0.00002180
Iteration 99/1000 | Loss: 0.00002180
Iteration 100/1000 | Loss: 0.00002180
Iteration 101/1000 | Loss: 0.00002180
Iteration 102/1000 | Loss: 0.00002180
Iteration 103/1000 | Loss: 0.00002180
Iteration 104/1000 | Loss: 0.00002180
Iteration 105/1000 | Loss: 0.00002180
Iteration 106/1000 | Loss: 0.00002180
Iteration 107/1000 | Loss: 0.00002180
Iteration 108/1000 | Loss: 0.00002180
Iteration 109/1000 | Loss: 0.00002180
Iteration 110/1000 | Loss: 0.00002180
Iteration 111/1000 | Loss: 0.00002180
Iteration 112/1000 | Loss: 0.00002180
Iteration 113/1000 | Loss: 0.00002180
Iteration 114/1000 | Loss: 0.00002180
Iteration 115/1000 | Loss: 0.00002180
Iteration 116/1000 | Loss: 0.00002180
Iteration 117/1000 | Loss: 0.00002180
Iteration 118/1000 | Loss: 0.00002180
Iteration 119/1000 | Loss: 0.00002180
Iteration 120/1000 | Loss: 0.00002180
Iteration 121/1000 | Loss: 0.00002180
Iteration 122/1000 | Loss: 0.00002180
Iteration 123/1000 | Loss: 0.00002180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [2.180396404583007e-05, 2.180396404583007e-05, 2.180396404583007e-05, 2.180396404583007e-05, 2.180396404583007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.180396404583007e-05

Optimization complete. Final v2v error: 3.9228196144104004 mm

Highest mean error: 4.375535488128662 mm for frame 13

Lowest mean error: 3.4685072898864746 mm for frame 56

Saving results

Total time: 31.192760229110718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00376490
Iteration 2/25 | Loss: 0.00145900
Iteration 3/25 | Loss: 0.00137309
Iteration 4/25 | Loss: 0.00136479
Iteration 5/25 | Loss: 0.00136120
Iteration 6/25 | Loss: 0.00136063
Iteration 7/25 | Loss: 0.00136063
Iteration 8/25 | Loss: 0.00136063
Iteration 9/25 | Loss: 0.00136063
Iteration 10/25 | Loss: 0.00136063
Iteration 11/25 | Loss: 0.00136063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013606292195618153, 0.0013606292195618153, 0.0013606292195618153, 0.0013606292195618153, 0.0013606292195618153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013606292195618153

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19147241
Iteration 2/25 | Loss: 0.00255327
Iteration 3/25 | Loss: 0.00255327
Iteration 4/25 | Loss: 0.00255327
Iteration 5/25 | Loss: 0.00255327
Iteration 6/25 | Loss: 0.00255327
Iteration 7/25 | Loss: 0.00255327
Iteration 8/25 | Loss: 0.00255327
Iteration 9/25 | Loss: 0.00255327
Iteration 10/25 | Loss: 0.00255327
Iteration 11/25 | Loss: 0.00255327
Iteration 12/25 | Loss: 0.00255327
Iteration 13/25 | Loss: 0.00255327
Iteration 14/25 | Loss: 0.00255327
Iteration 15/25 | Loss: 0.00255327
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0025532664731144905, 0.0025532664731144905, 0.0025532664731144905, 0.0025532664731144905, 0.0025532664731144905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025532664731144905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255327
Iteration 2/1000 | Loss: 0.00004894
Iteration 3/1000 | Loss: 0.00003016
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002240
Iteration 6/1000 | Loss: 0.00002138
Iteration 7/1000 | Loss: 0.00002090
Iteration 8/1000 | Loss: 0.00002046
Iteration 9/1000 | Loss: 0.00002008
Iteration 10/1000 | Loss: 0.00001976
Iteration 11/1000 | Loss: 0.00001955
Iteration 12/1000 | Loss: 0.00001953
Iteration 13/1000 | Loss: 0.00001937
Iteration 14/1000 | Loss: 0.00001935
Iteration 15/1000 | Loss: 0.00001934
Iteration 16/1000 | Loss: 0.00001931
Iteration 17/1000 | Loss: 0.00001926
Iteration 18/1000 | Loss: 0.00001924
Iteration 19/1000 | Loss: 0.00001922
Iteration 20/1000 | Loss: 0.00001921
Iteration 21/1000 | Loss: 0.00001920
Iteration 22/1000 | Loss: 0.00001920
Iteration 23/1000 | Loss: 0.00001919
Iteration 24/1000 | Loss: 0.00001919
Iteration 25/1000 | Loss: 0.00001918
Iteration 26/1000 | Loss: 0.00001917
Iteration 27/1000 | Loss: 0.00001917
Iteration 28/1000 | Loss: 0.00001916
Iteration 29/1000 | Loss: 0.00001916
Iteration 30/1000 | Loss: 0.00001915
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001915
Iteration 33/1000 | Loss: 0.00001914
Iteration 34/1000 | Loss: 0.00001913
Iteration 35/1000 | Loss: 0.00001913
Iteration 36/1000 | Loss: 0.00001912
Iteration 37/1000 | Loss: 0.00001912
Iteration 38/1000 | Loss: 0.00001912
Iteration 39/1000 | Loss: 0.00001911
Iteration 40/1000 | Loss: 0.00001911
Iteration 41/1000 | Loss: 0.00001911
Iteration 42/1000 | Loss: 0.00001911
Iteration 43/1000 | Loss: 0.00001910
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001909
Iteration 46/1000 | Loss: 0.00001909
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001908
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001908
Iteration 56/1000 | Loss: 0.00001908
Iteration 57/1000 | Loss: 0.00001908
Iteration 58/1000 | Loss: 0.00001908
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001907
Iteration 61/1000 | Loss: 0.00001906
Iteration 62/1000 | Loss: 0.00001906
Iteration 63/1000 | Loss: 0.00001906
Iteration 64/1000 | Loss: 0.00001905
Iteration 65/1000 | Loss: 0.00001905
Iteration 66/1000 | Loss: 0.00001905
Iteration 67/1000 | Loss: 0.00001905
Iteration 68/1000 | Loss: 0.00001905
Iteration 69/1000 | Loss: 0.00001904
Iteration 70/1000 | Loss: 0.00001904
Iteration 71/1000 | Loss: 0.00001904
Iteration 72/1000 | Loss: 0.00001904
Iteration 73/1000 | Loss: 0.00001904
Iteration 74/1000 | Loss: 0.00001904
Iteration 75/1000 | Loss: 0.00001903
Iteration 76/1000 | Loss: 0.00001903
Iteration 77/1000 | Loss: 0.00001903
Iteration 78/1000 | Loss: 0.00001903
Iteration 79/1000 | Loss: 0.00001903
Iteration 80/1000 | Loss: 0.00001903
Iteration 81/1000 | Loss: 0.00001903
Iteration 82/1000 | Loss: 0.00001903
Iteration 83/1000 | Loss: 0.00001903
Iteration 84/1000 | Loss: 0.00001903
Iteration 85/1000 | Loss: 0.00001903
Iteration 86/1000 | Loss: 0.00001903
Iteration 87/1000 | Loss: 0.00001903
Iteration 88/1000 | Loss: 0.00001903
Iteration 89/1000 | Loss: 0.00001903
Iteration 90/1000 | Loss: 0.00001903
Iteration 91/1000 | Loss: 0.00001903
Iteration 92/1000 | Loss: 0.00001903
Iteration 93/1000 | Loss: 0.00001903
Iteration 94/1000 | Loss: 0.00001903
Iteration 95/1000 | Loss: 0.00001903
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001903
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001903
Iteration 100/1000 | Loss: 0.00001903
Iteration 101/1000 | Loss: 0.00001903
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001903
Iteration 104/1000 | Loss: 0.00001903
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001903
Iteration 107/1000 | Loss: 0.00001903
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001903
Iteration 110/1000 | Loss: 0.00001903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [1.902619806060102e-05, 1.902619806060102e-05, 1.902619806060102e-05, 1.902619806060102e-05, 1.902619806060102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.902619806060102e-05

Optimization complete. Final v2v error: 3.6871144771575928 mm

Highest mean error: 4.064809799194336 mm for frame 188

Lowest mean error: 3.2978954315185547 mm for frame 36

Saving results

Total time: 37.575231075286865
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864164
Iteration 2/25 | Loss: 0.00137645
Iteration 3/25 | Loss: 0.00126701
Iteration 4/25 | Loss: 0.00125950
Iteration 5/25 | Loss: 0.00125820
Iteration 6/25 | Loss: 0.00125820
Iteration 7/25 | Loss: 0.00125820
Iteration 8/25 | Loss: 0.00125820
Iteration 9/25 | Loss: 0.00125820
Iteration 10/25 | Loss: 0.00125820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012582006165757775, 0.0012582006165757775, 0.0012582006165757775, 0.0012582006165757775, 0.0012582006165757775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012582006165757775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26062560
Iteration 2/25 | Loss: 0.00171160
Iteration 3/25 | Loss: 0.00171160
Iteration 4/25 | Loss: 0.00171160
Iteration 5/25 | Loss: 0.00171160
Iteration 6/25 | Loss: 0.00171160
Iteration 7/25 | Loss: 0.00171160
Iteration 8/25 | Loss: 0.00171160
Iteration 9/25 | Loss: 0.00171160
Iteration 10/25 | Loss: 0.00171160
Iteration 11/25 | Loss: 0.00171160
Iteration 12/25 | Loss: 0.00171160
Iteration 13/25 | Loss: 0.00171160
Iteration 14/25 | Loss: 0.00171160
Iteration 15/25 | Loss: 0.00171160
Iteration 16/25 | Loss: 0.00171160
Iteration 17/25 | Loss: 0.00171160
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0017115962691605091, 0.0017115962691605091, 0.0017115962691605091, 0.0017115962691605091, 0.0017115962691605091]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017115962691605091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171160
Iteration 2/1000 | Loss: 0.00002564
Iteration 3/1000 | Loss: 0.00001920
Iteration 4/1000 | Loss: 0.00001742
Iteration 5/1000 | Loss: 0.00001648
Iteration 6/1000 | Loss: 0.00001606
Iteration 7/1000 | Loss: 0.00001556
Iteration 8/1000 | Loss: 0.00001531
Iteration 9/1000 | Loss: 0.00001513
Iteration 10/1000 | Loss: 0.00001512
Iteration 11/1000 | Loss: 0.00001500
Iteration 12/1000 | Loss: 0.00001499
Iteration 13/1000 | Loss: 0.00001499
Iteration 14/1000 | Loss: 0.00001498
Iteration 15/1000 | Loss: 0.00001497
Iteration 16/1000 | Loss: 0.00001497
Iteration 17/1000 | Loss: 0.00001497
Iteration 18/1000 | Loss: 0.00001496
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001494
Iteration 21/1000 | Loss: 0.00001489
Iteration 22/1000 | Loss: 0.00001488
Iteration 23/1000 | Loss: 0.00001487
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001484
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001482
Iteration 31/1000 | Loss: 0.00001478
Iteration 32/1000 | Loss: 0.00001474
Iteration 33/1000 | Loss: 0.00001474
Iteration 34/1000 | Loss: 0.00001474
Iteration 35/1000 | Loss: 0.00001474
Iteration 36/1000 | Loss: 0.00001472
Iteration 37/1000 | Loss: 0.00001471
Iteration 38/1000 | Loss: 0.00001471
Iteration 39/1000 | Loss: 0.00001471
Iteration 40/1000 | Loss: 0.00001471
Iteration 41/1000 | Loss: 0.00001471
Iteration 42/1000 | Loss: 0.00001471
Iteration 43/1000 | Loss: 0.00001471
Iteration 44/1000 | Loss: 0.00001470
Iteration 45/1000 | Loss: 0.00001470
Iteration 46/1000 | Loss: 0.00001469
Iteration 47/1000 | Loss: 0.00001468
Iteration 48/1000 | Loss: 0.00001468
Iteration 49/1000 | Loss: 0.00001468
Iteration 50/1000 | Loss: 0.00001468
Iteration 51/1000 | Loss: 0.00001468
Iteration 52/1000 | Loss: 0.00001468
Iteration 53/1000 | Loss: 0.00001468
Iteration 54/1000 | Loss: 0.00001467
Iteration 55/1000 | Loss: 0.00001467
Iteration 56/1000 | Loss: 0.00001467
Iteration 57/1000 | Loss: 0.00001467
Iteration 58/1000 | Loss: 0.00001466
Iteration 59/1000 | Loss: 0.00001465
Iteration 60/1000 | Loss: 0.00001465
Iteration 61/1000 | Loss: 0.00001465
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001465
Iteration 64/1000 | Loss: 0.00001464
Iteration 65/1000 | Loss: 0.00001464
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001463
Iteration 69/1000 | Loss: 0.00001462
Iteration 70/1000 | Loss: 0.00001462
Iteration 71/1000 | Loss: 0.00001462
Iteration 72/1000 | Loss: 0.00001462
Iteration 73/1000 | Loss: 0.00001461
Iteration 74/1000 | Loss: 0.00001461
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001459
Iteration 78/1000 | Loss: 0.00001458
Iteration 79/1000 | Loss: 0.00001458
Iteration 80/1000 | Loss: 0.00001458
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001457
Iteration 85/1000 | Loss: 0.00001456
Iteration 86/1000 | Loss: 0.00001455
Iteration 87/1000 | Loss: 0.00001455
Iteration 88/1000 | Loss: 0.00001455
Iteration 89/1000 | Loss: 0.00001455
Iteration 90/1000 | Loss: 0.00001454
Iteration 91/1000 | Loss: 0.00001454
Iteration 92/1000 | Loss: 0.00001454
Iteration 93/1000 | Loss: 0.00001454
Iteration 94/1000 | Loss: 0.00001454
Iteration 95/1000 | Loss: 0.00001454
Iteration 96/1000 | Loss: 0.00001454
Iteration 97/1000 | Loss: 0.00001454
Iteration 98/1000 | Loss: 0.00001454
Iteration 99/1000 | Loss: 0.00001454
Iteration 100/1000 | Loss: 0.00001454
Iteration 101/1000 | Loss: 0.00001454
Iteration 102/1000 | Loss: 0.00001453
Iteration 103/1000 | Loss: 0.00001453
Iteration 104/1000 | Loss: 0.00001453
Iteration 105/1000 | Loss: 0.00001452
Iteration 106/1000 | Loss: 0.00001452
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001452
Iteration 109/1000 | Loss: 0.00001452
Iteration 110/1000 | Loss: 0.00001452
Iteration 111/1000 | Loss: 0.00001451
Iteration 112/1000 | Loss: 0.00001451
Iteration 113/1000 | Loss: 0.00001451
Iteration 114/1000 | Loss: 0.00001451
Iteration 115/1000 | Loss: 0.00001451
Iteration 116/1000 | Loss: 0.00001450
Iteration 117/1000 | Loss: 0.00001450
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001450
Iteration 121/1000 | Loss: 0.00001450
Iteration 122/1000 | Loss: 0.00001450
Iteration 123/1000 | Loss: 0.00001450
Iteration 124/1000 | Loss: 0.00001450
Iteration 125/1000 | Loss: 0.00001450
Iteration 126/1000 | Loss: 0.00001450
Iteration 127/1000 | Loss: 0.00001450
Iteration 128/1000 | Loss: 0.00001450
Iteration 129/1000 | Loss: 0.00001450
Iteration 130/1000 | Loss: 0.00001450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.4500228644465096e-05, 1.4500228644465096e-05, 1.4500228644465096e-05, 1.4500228644465096e-05, 1.4500228644465096e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4500228644465096e-05

Optimization complete. Final v2v error: 3.18882417678833 mm

Highest mean error: 3.4012556076049805 mm for frame 68

Lowest mean error: 2.9700677394866943 mm for frame 135

Saving results

Total time: 33.061970233917236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429522
Iteration 2/25 | Loss: 0.00148149
Iteration 3/25 | Loss: 0.00133737
Iteration 4/25 | Loss: 0.00132728
Iteration 5/25 | Loss: 0.00132568
Iteration 6/25 | Loss: 0.00132514
Iteration 7/25 | Loss: 0.00132507
Iteration 8/25 | Loss: 0.00132507
Iteration 9/25 | Loss: 0.00132507
Iteration 10/25 | Loss: 0.00132507
Iteration 11/25 | Loss: 0.00132507
Iteration 12/25 | Loss: 0.00132507
Iteration 13/25 | Loss: 0.00132507
Iteration 14/25 | Loss: 0.00132507
Iteration 15/25 | Loss: 0.00132507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013250652700662613, 0.0013250652700662613, 0.0013250652700662613, 0.0013250652700662613, 0.0013250652700662613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013250652700662613

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34211075
Iteration 2/25 | Loss: 0.00164315
Iteration 3/25 | Loss: 0.00164314
Iteration 4/25 | Loss: 0.00164314
Iteration 5/25 | Loss: 0.00164314
Iteration 6/25 | Loss: 0.00164314
Iteration 7/25 | Loss: 0.00164314
Iteration 8/25 | Loss: 0.00164314
Iteration 9/25 | Loss: 0.00164314
Iteration 10/25 | Loss: 0.00164314
Iteration 11/25 | Loss: 0.00164314
Iteration 12/25 | Loss: 0.00164314
Iteration 13/25 | Loss: 0.00164314
Iteration 14/25 | Loss: 0.00164314
Iteration 15/25 | Loss: 0.00164314
Iteration 16/25 | Loss: 0.00164314
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001643139636144042, 0.001643139636144042, 0.001643139636144042, 0.001643139636144042, 0.001643139636144042]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001643139636144042

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164314
Iteration 2/1000 | Loss: 0.00004937
Iteration 3/1000 | Loss: 0.00002540
Iteration 4/1000 | Loss: 0.00002119
Iteration 5/1000 | Loss: 0.00001985
Iteration 6/1000 | Loss: 0.00001896
Iteration 7/1000 | Loss: 0.00001849
Iteration 8/1000 | Loss: 0.00001811
Iteration 9/1000 | Loss: 0.00001788
Iteration 10/1000 | Loss: 0.00001781
Iteration 11/1000 | Loss: 0.00001762
Iteration 12/1000 | Loss: 0.00001757
Iteration 13/1000 | Loss: 0.00001742
Iteration 14/1000 | Loss: 0.00001736
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001730
Iteration 20/1000 | Loss: 0.00001730
Iteration 21/1000 | Loss: 0.00001730
Iteration 22/1000 | Loss: 0.00001729
Iteration 23/1000 | Loss: 0.00001729
Iteration 24/1000 | Loss: 0.00001728
Iteration 25/1000 | Loss: 0.00001725
Iteration 26/1000 | Loss: 0.00001725
Iteration 27/1000 | Loss: 0.00001725
Iteration 28/1000 | Loss: 0.00001725
Iteration 29/1000 | Loss: 0.00001724
Iteration 30/1000 | Loss: 0.00001724
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001724
Iteration 33/1000 | Loss: 0.00001724
Iteration 34/1000 | Loss: 0.00001724
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001723
Iteration 37/1000 | Loss: 0.00001722
Iteration 38/1000 | Loss: 0.00001722
Iteration 39/1000 | Loss: 0.00001720
Iteration 40/1000 | Loss: 0.00001720
Iteration 41/1000 | Loss: 0.00001720
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001719
Iteration 45/1000 | Loss: 0.00001718
Iteration 46/1000 | Loss: 0.00001717
Iteration 47/1000 | Loss: 0.00001717
Iteration 48/1000 | Loss: 0.00001717
Iteration 49/1000 | Loss: 0.00001717
Iteration 50/1000 | Loss: 0.00001716
Iteration 51/1000 | Loss: 0.00001716
Iteration 52/1000 | Loss: 0.00001716
Iteration 53/1000 | Loss: 0.00001716
Iteration 54/1000 | Loss: 0.00001716
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001715
Iteration 58/1000 | Loss: 0.00001715
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001715
Iteration 63/1000 | Loss: 0.00001715
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001713
Iteration 69/1000 | Loss: 0.00001713
Iteration 70/1000 | Loss: 0.00001713
Iteration 71/1000 | Loss: 0.00001713
Iteration 72/1000 | Loss: 0.00001713
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001712
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001711
Iteration 78/1000 | Loss: 0.00001711
Iteration 79/1000 | Loss: 0.00001711
Iteration 80/1000 | Loss: 0.00001710
Iteration 81/1000 | Loss: 0.00001710
Iteration 82/1000 | Loss: 0.00001710
Iteration 83/1000 | Loss: 0.00001710
Iteration 84/1000 | Loss: 0.00001710
Iteration 85/1000 | Loss: 0.00001710
Iteration 86/1000 | Loss: 0.00001710
Iteration 87/1000 | Loss: 0.00001709
Iteration 88/1000 | Loss: 0.00001709
Iteration 89/1000 | Loss: 0.00001709
Iteration 90/1000 | Loss: 0.00001709
Iteration 91/1000 | Loss: 0.00001709
Iteration 92/1000 | Loss: 0.00001709
Iteration 93/1000 | Loss: 0.00001709
Iteration 94/1000 | Loss: 0.00001709
Iteration 95/1000 | Loss: 0.00001709
Iteration 96/1000 | Loss: 0.00001709
Iteration 97/1000 | Loss: 0.00001709
Iteration 98/1000 | Loss: 0.00001708
Iteration 99/1000 | Loss: 0.00001708
Iteration 100/1000 | Loss: 0.00001708
Iteration 101/1000 | Loss: 0.00001708
Iteration 102/1000 | Loss: 0.00001708
Iteration 103/1000 | Loss: 0.00001708
Iteration 104/1000 | Loss: 0.00001708
Iteration 105/1000 | Loss: 0.00001708
Iteration 106/1000 | Loss: 0.00001708
Iteration 107/1000 | Loss: 0.00001708
Iteration 108/1000 | Loss: 0.00001708
Iteration 109/1000 | Loss: 0.00001708
Iteration 110/1000 | Loss: 0.00001708
Iteration 111/1000 | Loss: 0.00001708
Iteration 112/1000 | Loss: 0.00001708
Iteration 113/1000 | Loss: 0.00001708
Iteration 114/1000 | Loss: 0.00001708
Iteration 115/1000 | Loss: 0.00001708
Iteration 116/1000 | Loss: 0.00001707
Iteration 117/1000 | Loss: 0.00001707
Iteration 118/1000 | Loss: 0.00001707
Iteration 119/1000 | Loss: 0.00001707
Iteration 120/1000 | Loss: 0.00001706
Iteration 121/1000 | Loss: 0.00001706
Iteration 122/1000 | Loss: 0.00001706
Iteration 123/1000 | Loss: 0.00001706
Iteration 124/1000 | Loss: 0.00001706
Iteration 125/1000 | Loss: 0.00001706
Iteration 126/1000 | Loss: 0.00001706
Iteration 127/1000 | Loss: 0.00001706
Iteration 128/1000 | Loss: 0.00001706
Iteration 129/1000 | Loss: 0.00001706
Iteration 130/1000 | Loss: 0.00001705
Iteration 131/1000 | Loss: 0.00001705
Iteration 132/1000 | Loss: 0.00001705
Iteration 133/1000 | Loss: 0.00001705
Iteration 134/1000 | Loss: 0.00001705
Iteration 135/1000 | Loss: 0.00001705
Iteration 136/1000 | Loss: 0.00001705
Iteration 137/1000 | Loss: 0.00001705
Iteration 138/1000 | Loss: 0.00001704
Iteration 139/1000 | Loss: 0.00001704
Iteration 140/1000 | Loss: 0.00001704
Iteration 141/1000 | Loss: 0.00001704
Iteration 142/1000 | Loss: 0.00001704
Iteration 143/1000 | Loss: 0.00001704
Iteration 144/1000 | Loss: 0.00001703
Iteration 145/1000 | Loss: 0.00001703
Iteration 146/1000 | Loss: 0.00001703
Iteration 147/1000 | Loss: 0.00001703
Iteration 148/1000 | Loss: 0.00001702
Iteration 149/1000 | Loss: 0.00001702
Iteration 150/1000 | Loss: 0.00001702
Iteration 151/1000 | Loss: 0.00001702
Iteration 152/1000 | Loss: 0.00001702
Iteration 153/1000 | Loss: 0.00001702
Iteration 154/1000 | Loss: 0.00001702
Iteration 155/1000 | Loss: 0.00001702
Iteration 156/1000 | Loss: 0.00001702
Iteration 157/1000 | Loss: 0.00001702
Iteration 158/1000 | Loss: 0.00001702
Iteration 159/1000 | Loss: 0.00001702
Iteration 160/1000 | Loss: 0.00001702
Iteration 161/1000 | Loss: 0.00001702
Iteration 162/1000 | Loss: 0.00001702
Iteration 163/1000 | Loss: 0.00001702
Iteration 164/1000 | Loss: 0.00001702
Iteration 165/1000 | Loss: 0.00001702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.701704786682967e-05, 1.701704786682967e-05, 1.701704786682967e-05, 1.701704786682967e-05, 1.701704786682967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.701704786682967e-05

Optimization complete. Final v2v error: 3.366659164428711 mm

Highest mean error: 4.952174663543701 mm for frame 37

Lowest mean error: 2.879897117614746 mm for frame 0

Saving results

Total time: 36.96305251121521
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479253
Iteration 2/25 | Loss: 0.00146189
Iteration 3/25 | Loss: 0.00133812
Iteration 4/25 | Loss: 0.00132307
Iteration 5/25 | Loss: 0.00132072
Iteration 6/25 | Loss: 0.00132007
Iteration 7/25 | Loss: 0.00132007
Iteration 8/25 | Loss: 0.00132007
Iteration 9/25 | Loss: 0.00132007
Iteration 10/25 | Loss: 0.00132007
Iteration 11/25 | Loss: 0.00132007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013200683752074838, 0.0013200683752074838, 0.0013200683752074838, 0.0013200683752074838, 0.0013200683752074838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013200683752074838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35246503
Iteration 2/25 | Loss: 0.00154846
Iteration 3/25 | Loss: 0.00154844
Iteration 4/25 | Loss: 0.00154844
Iteration 5/25 | Loss: 0.00154844
Iteration 6/25 | Loss: 0.00154844
Iteration 7/25 | Loss: 0.00154844
Iteration 8/25 | Loss: 0.00154844
Iteration 9/25 | Loss: 0.00154844
Iteration 10/25 | Loss: 0.00154844
Iteration 11/25 | Loss: 0.00154844
Iteration 12/25 | Loss: 0.00154844
Iteration 13/25 | Loss: 0.00154844
Iteration 14/25 | Loss: 0.00154844
Iteration 15/25 | Loss: 0.00154844
Iteration 16/25 | Loss: 0.00154844
Iteration 17/25 | Loss: 0.00154844
Iteration 18/25 | Loss: 0.00154844
Iteration 19/25 | Loss: 0.00154844
Iteration 20/25 | Loss: 0.00154844
Iteration 21/25 | Loss: 0.00154844
Iteration 22/25 | Loss: 0.00154844
Iteration 23/25 | Loss: 0.00154844
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0015484389150515199, 0.0015484389150515199, 0.0015484389150515199, 0.0015484389150515199, 0.0015484389150515199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015484389150515199

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154844
Iteration 2/1000 | Loss: 0.00003832
Iteration 3/1000 | Loss: 0.00002699
Iteration 4/1000 | Loss: 0.00002310
Iteration 5/1000 | Loss: 0.00002202
Iteration 6/1000 | Loss: 0.00002137
Iteration 7/1000 | Loss: 0.00002105
Iteration 8/1000 | Loss: 0.00002075
Iteration 9/1000 | Loss: 0.00002058
Iteration 10/1000 | Loss: 0.00002052
Iteration 11/1000 | Loss: 0.00002037
Iteration 12/1000 | Loss: 0.00002026
Iteration 13/1000 | Loss: 0.00002024
Iteration 14/1000 | Loss: 0.00002023
Iteration 15/1000 | Loss: 0.00002019
Iteration 16/1000 | Loss: 0.00002019
Iteration 17/1000 | Loss: 0.00002018
Iteration 18/1000 | Loss: 0.00002012
Iteration 19/1000 | Loss: 0.00002007
Iteration 20/1000 | Loss: 0.00002002
Iteration 21/1000 | Loss: 0.00002002
Iteration 22/1000 | Loss: 0.00002000
Iteration 23/1000 | Loss: 0.00002000
Iteration 24/1000 | Loss: 0.00001999
Iteration 25/1000 | Loss: 0.00001997
Iteration 26/1000 | Loss: 0.00001996
Iteration 27/1000 | Loss: 0.00001996
Iteration 28/1000 | Loss: 0.00001996
Iteration 29/1000 | Loss: 0.00001995
Iteration 30/1000 | Loss: 0.00001995
Iteration 31/1000 | Loss: 0.00001995
Iteration 32/1000 | Loss: 0.00001995
Iteration 33/1000 | Loss: 0.00001995
Iteration 34/1000 | Loss: 0.00001993
Iteration 35/1000 | Loss: 0.00001993
Iteration 36/1000 | Loss: 0.00001993
Iteration 37/1000 | Loss: 0.00001993
Iteration 38/1000 | Loss: 0.00001993
Iteration 39/1000 | Loss: 0.00001993
Iteration 40/1000 | Loss: 0.00001992
Iteration 41/1000 | Loss: 0.00001992
Iteration 42/1000 | Loss: 0.00001992
Iteration 43/1000 | Loss: 0.00001992
Iteration 44/1000 | Loss: 0.00001992
Iteration 45/1000 | Loss: 0.00001992
Iteration 46/1000 | Loss: 0.00001990
Iteration 47/1000 | Loss: 0.00001990
Iteration 48/1000 | Loss: 0.00001990
Iteration 49/1000 | Loss: 0.00001990
Iteration 50/1000 | Loss: 0.00001990
Iteration 51/1000 | Loss: 0.00001990
Iteration 52/1000 | Loss: 0.00001990
Iteration 53/1000 | Loss: 0.00001990
Iteration 54/1000 | Loss: 0.00001990
Iteration 55/1000 | Loss: 0.00001990
Iteration 56/1000 | Loss: 0.00001990
Iteration 57/1000 | Loss: 0.00001989
Iteration 58/1000 | Loss: 0.00001989
Iteration 59/1000 | Loss: 0.00001989
Iteration 60/1000 | Loss: 0.00001989
Iteration 61/1000 | Loss: 0.00001989
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001988
Iteration 65/1000 | Loss: 0.00001988
Iteration 66/1000 | Loss: 0.00001988
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001988
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001985
Iteration 74/1000 | Loss: 0.00001985
Iteration 75/1000 | Loss: 0.00001985
Iteration 76/1000 | Loss: 0.00001985
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00001985
Iteration 79/1000 | Loss: 0.00001985
Iteration 80/1000 | Loss: 0.00001985
Iteration 81/1000 | Loss: 0.00001985
Iteration 82/1000 | Loss: 0.00001984
Iteration 83/1000 | Loss: 0.00001984
Iteration 84/1000 | Loss: 0.00001984
Iteration 85/1000 | Loss: 0.00001983
Iteration 86/1000 | Loss: 0.00001983
Iteration 87/1000 | Loss: 0.00001982
Iteration 88/1000 | Loss: 0.00001982
Iteration 89/1000 | Loss: 0.00001982
Iteration 90/1000 | Loss: 0.00001982
Iteration 91/1000 | Loss: 0.00001981
Iteration 92/1000 | Loss: 0.00001981
Iteration 93/1000 | Loss: 0.00001981
Iteration 94/1000 | Loss: 0.00001981
Iteration 95/1000 | Loss: 0.00001981
Iteration 96/1000 | Loss: 0.00001980
Iteration 97/1000 | Loss: 0.00001980
Iteration 98/1000 | Loss: 0.00001979
Iteration 99/1000 | Loss: 0.00001979
Iteration 100/1000 | Loss: 0.00001978
Iteration 101/1000 | Loss: 0.00001976
Iteration 102/1000 | Loss: 0.00001976
Iteration 103/1000 | Loss: 0.00001976
Iteration 104/1000 | Loss: 0.00001976
Iteration 105/1000 | Loss: 0.00001976
Iteration 106/1000 | Loss: 0.00001976
Iteration 107/1000 | Loss: 0.00001975
Iteration 108/1000 | Loss: 0.00001975
Iteration 109/1000 | Loss: 0.00001975
Iteration 110/1000 | Loss: 0.00001975
Iteration 111/1000 | Loss: 0.00001975
Iteration 112/1000 | Loss: 0.00001975
Iteration 113/1000 | Loss: 0.00001975
Iteration 114/1000 | Loss: 0.00001975
Iteration 115/1000 | Loss: 0.00001975
Iteration 116/1000 | Loss: 0.00001975
Iteration 117/1000 | Loss: 0.00001974
Iteration 118/1000 | Loss: 0.00001973
Iteration 119/1000 | Loss: 0.00001973
Iteration 120/1000 | Loss: 0.00001973
Iteration 121/1000 | Loss: 0.00001973
Iteration 122/1000 | Loss: 0.00001973
Iteration 123/1000 | Loss: 0.00001972
Iteration 124/1000 | Loss: 0.00001972
Iteration 125/1000 | Loss: 0.00001972
Iteration 126/1000 | Loss: 0.00001972
Iteration 127/1000 | Loss: 0.00001972
Iteration 128/1000 | Loss: 0.00001972
Iteration 129/1000 | Loss: 0.00001972
Iteration 130/1000 | Loss: 0.00001971
Iteration 131/1000 | Loss: 0.00001971
Iteration 132/1000 | Loss: 0.00001971
Iteration 133/1000 | Loss: 0.00001971
Iteration 134/1000 | Loss: 0.00001971
Iteration 135/1000 | Loss: 0.00001971
Iteration 136/1000 | Loss: 0.00001971
Iteration 137/1000 | Loss: 0.00001971
Iteration 138/1000 | Loss: 0.00001971
Iteration 139/1000 | Loss: 0.00001971
Iteration 140/1000 | Loss: 0.00001971
Iteration 141/1000 | Loss: 0.00001971
Iteration 142/1000 | Loss: 0.00001971
Iteration 143/1000 | Loss: 0.00001970
Iteration 144/1000 | Loss: 0.00001970
Iteration 145/1000 | Loss: 0.00001970
Iteration 146/1000 | Loss: 0.00001970
Iteration 147/1000 | Loss: 0.00001970
Iteration 148/1000 | Loss: 0.00001970
Iteration 149/1000 | Loss: 0.00001970
Iteration 150/1000 | Loss: 0.00001970
Iteration 151/1000 | Loss: 0.00001970
Iteration 152/1000 | Loss: 0.00001970
Iteration 153/1000 | Loss: 0.00001969
Iteration 154/1000 | Loss: 0.00001969
Iteration 155/1000 | Loss: 0.00001969
Iteration 156/1000 | Loss: 0.00001969
Iteration 157/1000 | Loss: 0.00001969
Iteration 158/1000 | Loss: 0.00001969
Iteration 159/1000 | Loss: 0.00001969
Iteration 160/1000 | Loss: 0.00001969
Iteration 161/1000 | Loss: 0.00001969
Iteration 162/1000 | Loss: 0.00001969
Iteration 163/1000 | Loss: 0.00001969
Iteration 164/1000 | Loss: 0.00001969
Iteration 165/1000 | Loss: 0.00001969
Iteration 166/1000 | Loss: 0.00001969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.9689814507728443e-05, 1.9689814507728443e-05, 1.9689814507728443e-05, 1.9689814507728443e-05, 1.9689814507728443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9689814507728443e-05

Optimization complete. Final v2v error: 3.769960403442383 mm

Highest mean error: 4.151542663574219 mm for frame 127

Lowest mean error: 3.436997413635254 mm for frame 60

Saving results

Total time: 38.925171852111816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403761
Iteration 2/25 | Loss: 0.00143181
Iteration 3/25 | Loss: 0.00131128
Iteration 4/25 | Loss: 0.00129622
Iteration 5/25 | Loss: 0.00129227
Iteration 6/25 | Loss: 0.00129150
Iteration 7/25 | Loss: 0.00129150
Iteration 8/25 | Loss: 0.00129150
Iteration 9/25 | Loss: 0.00129150
Iteration 10/25 | Loss: 0.00129150
Iteration 11/25 | Loss: 0.00129150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012915018014609814, 0.0012915018014609814, 0.0012915018014609814, 0.0012915018014609814, 0.0012915018014609814]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012915018014609814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24591756
Iteration 2/25 | Loss: 0.00176876
Iteration 3/25 | Loss: 0.00176875
Iteration 4/25 | Loss: 0.00176875
Iteration 5/25 | Loss: 0.00176875
Iteration 6/25 | Loss: 0.00176875
Iteration 7/25 | Loss: 0.00176875
Iteration 8/25 | Loss: 0.00176875
Iteration 9/25 | Loss: 0.00176875
Iteration 10/25 | Loss: 0.00176875
Iteration 11/25 | Loss: 0.00176875
Iteration 12/25 | Loss: 0.00176875
Iteration 13/25 | Loss: 0.00176875
Iteration 14/25 | Loss: 0.00176875
Iteration 15/25 | Loss: 0.00176875
Iteration 16/25 | Loss: 0.00176875
Iteration 17/25 | Loss: 0.00176875
Iteration 18/25 | Loss: 0.00176875
Iteration 19/25 | Loss: 0.00176875
Iteration 20/25 | Loss: 0.00176875
Iteration 21/25 | Loss: 0.00176875
Iteration 22/25 | Loss: 0.00176875
Iteration 23/25 | Loss: 0.00176875
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017687514191493392, 0.0017687514191493392, 0.0017687514191493392, 0.0017687514191493392, 0.0017687514191493392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017687514191493392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176875
Iteration 2/1000 | Loss: 0.00002590
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001664
Iteration 5/1000 | Loss: 0.00001556
Iteration 6/1000 | Loss: 0.00001494
Iteration 7/1000 | Loss: 0.00001444
Iteration 8/1000 | Loss: 0.00001417
Iteration 9/1000 | Loss: 0.00001406
Iteration 10/1000 | Loss: 0.00001405
Iteration 11/1000 | Loss: 0.00001401
Iteration 12/1000 | Loss: 0.00001397
Iteration 13/1000 | Loss: 0.00001392
Iteration 14/1000 | Loss: 0.00001390
Iteration 15/1000 | Loss: 0.00001389
Iteration 16/1000 | Loss: 0.00001389
Iteration 17/1000 | Loss: 0.00001388
Iteration 18/1000 | Loss: 0.00001388
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001387
Iteration 21/1000 | Loss: 0.00001386
Iteration 22/1000 | Loss: 0.00001386
Iteration 23/1000 | Loss: 0.00001386
Iteration 24/1000 | Loss: 0.00001385
Iteration 25/1000 | Loss: 0.00001383
Iteration 26/1000 | Loss: 0.00001383
Iteration 27/1000 | Loss: 0.00001383
Iteration 28/1000 | Loss: 0.00001383
Iteration 29/1000 | Loss: 0.00001383
Iteration 30/1000 | Loss: 0.00001383
Iteration 31/1000 | Loss: 0.00001383
Iteration 32/1000 | Loss: 0.00001383
Iteration 33/1000 | Loss: 0.00001382
Iteration 34/1000 | Loss: 0.00001382
Iteration 35/1000 | Loss: 0.00001382
Iteration 36/1000 | Loss: 0.00001382
Iteration 37/1000 | Loss: 0.00001381
Iteration 38/1000 | Loss: 0.00001381
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001380
Iteration 41/1000 | Loss: 0.00001380
Iteration 42/1000 | Loss: 0.00001379
Iteration 43/1000 | Loss: 0.00001379
Iteration 44/1000 | Loss: 0.00001379
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001378
Iteration 47/1000 | Loss: 0.00001378
Iteration 48/1000 | Loss: 0.00001378
Iteration 49/1000 | Loss: 0.00001377
Iteration 50/1000 | Loss: 0.00001377
Iteration 51/1000 | Loss: 0.00001377
Iteration 52/1000 | Loss: 0.00001377
Iteration 53/1000 | Loss: 0.00001377
Iteration 54/1000 | Loss: 0.00001377
Iteration 55/1000 | Loss: 0.00001377
Iteration 56/1000 | Loss: 0.00001377
Iteration 57/1000 | Loss: 0.00001376
Iteration 58/1000 | Loss: 0.00001376
Iteration 59/1000 | Loss: 0.00001376
Iteration 60/1000 | Loss: 0.00001376
Iteration 61/1000 | Loss: 0.00001376
Iteration 62/1000 | Loss: 0.00001376
Iteration 63/1000 | Loss: 0.00001376
Iteration 64/1000 | Loss: 0.00001375
Iteration 65/1000 | Loss: 0.00001375
Iteration 66/1000 | Loss: 0.00001375
Iteration 67/1000 | Loss: 0.00001375
Iteration 68/1000 | Loss: 0.00001375
Iteration 69/1000 | Loss: 0.00001375
Iteration 70/1000 | Loss: 0.00001375
Iteration 71/1000 | Loss: 0.00001375
Iteration 72/1000 | Loss: 0.00001375
Iteration 73/1000 | Loss: 0.00001375
Iteration 74/1000 | Loss: 0.00001375
Iteration 75/1000 | Loss: 0.00001375
Iteration 76/1000 | Loss: 0.00001375
Iteration 77/1000 | Loss: 0.00001375
Iteration 78/1000 | Loss: 0.00001375
Iteration 79/1000 | Loss: 0.00001375
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.3751800906902645e-05, 1.3751800906902645e-05, 1.3751800906902645e-05, 1.3751800906902645e-05, 1.3751800906902645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3751800906902645e-05

Optimization complete. Final v2v error: 3.118036985397339 mm

Highest mean error: 3.4814321994781494 mm for frame 102

Lowest mean error: 2.6970319747924805 mm for frame 3

Saving results

Total time: 28.78350281715393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862390
Iteration 2/25 | Loss: 0.00204936
Iteration 3/25 | Loss: 0.00148506
Iteration 4/25 | Loss: 0.00141511
Iteration 5/25 | Loss: 0.00140897
Iteration 6/25 | Loss: 0.00140824
Iteration 7/25 | Loss: 0.00140824
Iteration 8/25 | Loss: 0.00140824
Iteration 9/25 | Loss: 0.00140824
Iteration 10/25 | Loss: 0.00140824
Iteration 11/25 | Loss: 0.00140824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014082433190196753, 0.0014082433190196753, 0.0014082433190196753, 0.0014082433190196753, 0.0014082433190196753]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014082433190196753

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11082876
Iteration 2/25 | Loss: 0.00084150
Iteration 3/25 | Loss: 0.00084147
Iteration 4/25 | Loss: 0.00084147
Iteration 5/25 | Loss: 0.00084147
Iteration 6/25 | Loss: 0.00084147
Iteration 7/25 | Loss: 0.00084147
Iteration 8/25 | Loss: 0.00084147
Iteration 9/25 | Loss: 0.00084147
Iteration 10/25 | Loss: 0.00084147
Iteration 11/25 | Loss: 0.00084147
Iteration 12/25 | Loss: 0.00084147
Iteration 13/25 | Loss: 0.00084147
Iteration 14/25 | Loss: 0.00084147
Iteration 15/25 | Loss: 0.00084147
Iteration 16/25 | Loss: 0.00084147
Iteration 17/25 | Loss: 0.00084147
Iteration 18/25 | Loss: 0.00084147
Iteration 19/25 | Loss: 0.00084147
Iteration 20/25 | Loss: 0.00084147
Iteration 21/25 | Loss: 0.00084147
Iteration 22/25 | Loss: 0.00084147
Iteration 23/25 | Loss: 0.00084147
Iteration 24/25 | Loss: 0.00084147
Iteration 25/25 | Loss: 0.00084147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084147
Iteration 2/1000 | Loss: 0.00004177
Iteration 3/1000 | Loss: 0.00002814
Iteration 4/1000 | Loss: 0.00002336
Iteration 5/1000 | Loss: 0.00002152
Iteration 6/1000 | Loss: 0.00002068
Iteration 7/1000 | Loss: 0.00002009
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001917
Iteration 10/1000 | Loss: 0.00001891
Iteration 11/1000 | Loss: 0.00001869
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001857
Iteration 14/1000 | Loss: 0.00001834
Iteration 15/1000 | Loss: 0.00001834
Iteration 16/1000 | Loss: 0.00001829
Iteration 17/1000 | Loss: 0.00001829
Iteration 18/1000 | Loss: 0.00001828
Iteration 19/1000 | Loss: 0.00001825
Iteration 20/1000 | Loss: 0.00001819
Iteration 21/1000 | Loss: 0.00001818
Iteration 22/1000 | Loss: 0.00001817
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001817
Iteration 25/1000 | Loss: 0.00001816
Iteration 26/1000 | Loss: 0.00001815
Iteration 27/1000 | Loss: 0.00001815
Iteration 28/1000 | Loss: 0.00001814
Iteration 29/1000 | Loss: 0.00001814
Iteration 30/1000 | Loss: 0.00001813
Iteration 31/1000 | Loss: 0.00001812
Iteration 32/1000 | Loss: 0.00001812
Iteration 33/1000 | Loss: 0.00001811
Iteration 34/1000 | Loss: 0.00001811
Iteration 35/1000 | Loss: 0.00001811
Iteration 36/1000 | Loss: 0.00001811
Iteration 37/1000 | Loss: 0.00001811
Iteration 38/1000 | Loss: 0.00001811
Iteration 39/1000 | Loss: 0.00001810
Iteration 40/1000 | Loss: 0.00001810
Iteration 41/1000 | Loss: 0.00001810
Iteration 42/1000 | Loss: 0.00001810
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001809
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001808
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001808
Iteration 50/1000 | Loss: 0.00001808
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001807
Iteration 53/1000 | Loss: 0.00001807
Iteration 54/1000 | Loss: 0.00001807
Iteration 55/1000 | Loss: 0.00001807
Iteration 56/1000 | Loss: 0.00001807
Iteration 57/1000 | Loss: 0.00001807
Iteration 58/1000 | Loss: 0.00001807
Iteration 59/1000 | Loss: 0.00001807
Iteration 60/1000 | Loss: 0.00001807
Iteration 61/1000 | Loss: 0.00001807
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001806
Iteration 64/1000 | Loss: 0.00001806
Iteration 65/1000 | Loss: 0.00001806
Iteration 66/1000 | Loss: 0.00001806
Iteration 67/1000 | Loss: 0.00001806
Iteration 68/1000 | Loss: 0.00001806
Iteration 69/1000 | Loss: 0.00001806
Iteration 70/1000 | Loss: 0.00001806
Iteration 71/1000 | Loss: 0.00001806
Iteration 72/1000 | Loss: 0.00001806
Iteration 73/1000 | Loss: 0.00001806
Iteration 74/1000 | Loss: 0.00001806
Iteration 75/1000 | Loss: 0.00001806
Iteration 76/1000 | Loss: 0.00001806
Iteration 77/1000 | Loss: 0.00001805
Iteration 78/1000 | Loss: 0.00001805
Iteration 79/1000 | Loss: 0.00001805
Iteration 80/1000 | Loss: 0.00001805
Iteration 81/1000 | Loss: 0.00001805
Iteration 82/1000 | Loss: 0.00001805
Iteration 83/1000 | Loss: 0.00001804
Iteration 84/1000 | Loss: 0.00001804
Iteration 85/1000 | Loss: 0.00001804
Iteration 86/1000 | Loss: 0.00001804
Iteration 87/1000 | Loss: 0.00001804
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001804
Iteration 90/1000 | Loss: 0.00001804
Iteration 91/1000 | Loss: 0.00001803
Iteration 92/1000 | Loss: 0.00001803
Iteration 93/1000 | Loss: 0.00001803
Iteration 94/1000 | Loss: 0.00001803
Iteration 95/1000 | Loss: 0.00001803
Iteration 96/1000 | Loss: 0.00001803
Iteration 97/1000 | Loss: 0.00001803
Iteration 98/1000 | Loss: 0.00001802
Iteration 99/1000 | Loss: 0.00001802
Iteration 100/1000 | Loss: 0.00001802
Iteration 101/1000 | Loss: 0.00001802
Iteration 102/1000 | Loss: 0.00001802
Iteration 103/1000 | Loss: 0.00001802
Iteration 104/1000 | Loss: 0.00001802
Iteration 105/1000 | Loss: 0.00001802
Iteration 106/1000 | Loss: 0.00001802
Iteration 107/1000 | Loss: 0.00001802
Iteration 108/1000 | Loss: 0.00001802
Iteration 109/1000 | Loss: 0.00001802
Iteration 110/1000 | Loss: 0.00001802
Iteration 111/1000 | Loss: 0.00001801
Iteration 112/1000 | Loss: 0.00001801
Iteration 113/1000 | Loss: 0.00001801
Iteration 114/1000 | Loss: 0.00001801
Iteration 115/1000 | Loss: 0.00001801
Iteration 116/1000 | Loss: 0.00001801
Iteration 117/1000 | Loss: 0.00001801
Iteration 118/1000 | Loss: 0.00001801
Iteration 119/1000 | Loss: 0.00001801
Iteration 120/1000 | Loss: 0.00001801
Iteration 121/1000 | Loss: 0.00001801
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001801
Iteration 130/1000 | Loss: 0.00001801
Iteration 131/1000 | Loss: 0.00001801
Iteration 132/1000 | Loss: 0.00001801
Iteration 133/1000 | Loss: 0.00001801
Iteration 134/1000 | Loss: 0.00001801
Iteration 135/1000 | Loss: 0.00001801
Iteration 136/1000 | Loss: 0.00001801
Iteration 137/1000 | Loss: 0.00001801
Iteration 138/1000 | Loss: 0.00001801
Iteration 139/1000 | Loss: 0.00001801
Iteration 140/1000 | Loss: 0.00001801
Iteration 141/1000 | Loss: 0.00001801
Iteration 142/1000 | Loss: 0.00001801
Iteration 143/1000 | Loss: 0.00001801
Iteration 144/1000 | Loss: 0.00001801
Iteration 145/1000 | Loss: 0.00001801
Iteration 146/1000 | Loss: 0.00001801
Iteration 147/1000 | Loss: 0.00001801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.8013806766248308e-05, 1.8013806766248308e-05, 1.8013806766248308e-05, 1.8013806766248308e-05, 1.8013806766248308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8013806766248308e-05

Optimization complete. Final v2v error: 3.647324562072754 mm

Highest mean error: 4.103395938873291 mm for frame 119

Lowest mean error: 3.374851942062378 mm for frame 43

Saving results

Total time: 35.951539278030396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_39_us_1861/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_39_us_1861/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401611
Iteration 2/25 | Loss: 0.00138429
Iteration 3/25 | Loss: 0.00130968
Iteration 4/25 | Loss: 0.00130428
Iteration 5/25 | Loss: 0.00130339
Iteration 6/25 | Loss: 0.00130339
Iteration 7/25 | Loss: 0.00130339
Iteration 8/25 | Loss: 0.00130339
Iteration 9/25 | Loss: 0.00130339
Iteration 10/25 | Loss: 0.00130339
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013033892028033733, 0.0013033892028033733, 0.0013033892028033733, 0.0013033892028033733, 0.0013033892028033733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013033892028033733

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94065547
Iteration 2/25 | Loss: 0.00189672
Iteration 3/25 | Loss: 0.00189671
Iteration 4/25 | Loss: 0.00189671
Iteration 5/25 | Loss: 0.00189671
Iteration 6/25 | Loss: 0.00189671
Iteration 7/25 | Loss: 0.00189671
Iteration 8/25 | Loss: 0.00189671
Iteration 9/25 | Loss: 0.00189671
Iteration 10/25 | Loss: 0.00189671
Iteration 11/25 | Loss: 0.00189671
Iteration 12/25 | Loss: 0.00189671
Iteration 13/25 | Loss: 0.00189671
Iteration 14/25 | Loss: 0.00189671
Iteration 15/25 | Loss: 0.00189671
Iteration 16/25 | Loss: 0.00189671
Iteration 17/25 | Loss: 0.00189671
Iteration 18/25 | Loss: 0.00189671
Iteration 19/25 | Loss: 0.00189671
Iteration 20/25 | Loss: 0.00189671
Iteration 21/25 | Loss: 0.00189671
Iteration 22/25 | Loss: 0.00189671
Iteration 23/25 | Loss: 0.00189671
Iteration 24/25 | Loss: 0.00189671
Iteration 25/25 | Loss: 0.00189671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189671
Iteration 2/1000 | Loss: 0.00002947
Iteration 3/1000 | Loss: 0.00002023
Iteration 4/1000 | Loss: 0.00001846
Iteration 5/1000 | Loss: 0.00001768
Iteration 6/1000 | Loss: 0.00001710
Iteration 7/1000 | Loss: 0.00001675
Iteration 8/1000 | Loss: 0.00001671
Iteration 9/1000 | Loss: 0.00001654
Iteration 10/1000 | Loss: 0.00001653
Iteration 11/1000 | Loss: 0.00001653
Iteration 12/1000 | Loss: 0.00001643
Iteration 13/1000 | Loss: 0.00001643
Iteration 14/1000 | Loss: 0.00001643
Iteration 15/1000 | Loss: 0.00001643
Iteration 16/1000 | Loss: 0.00001643
Iteration 17/1000 | Loss: 0.00001643
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001643
Iteration 20/1000 | Loss: 0.00001642
Iteration 21/1000 | Loss: 0.00001642
Iteration 22/1000 | Loss: 0.00001642
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001642
Iteration 25/1000 | Loss: 0.00001642
Iteration 26/1000 | Loss: 0.00001642
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001638
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001638
Iteration 32/1000 | Loss: 0.00001638
Iteration 33/1000 | Loss: 0.00001637
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001635
Iteration 36/1000 | Loss: 0.00001634
Iteration 37/1000 | Loss: 0.00001634
Iteration 38/1000 | Loss: 0.00001634
Iteration 39/1000 | Loss: 0.00001634
Iteration 40/1000 | Loss: 0.00001634
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001634
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001633
Iteration 48/1000 | Loss: 0.00001633
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001629
Iteration 52/1000 | Loss: 0.00001629
Iteration 53/1000 | Loss: 0.00001629
Iteration 54/1000 | Loss: 0.00001629
Iteration 55/1000 | Loss: 0.00001629
Iteration 56/1000 | Loss: 0.00001628
Iteration 57/1000 | Loss: 0.00001628
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001624
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001622
Iteration 73/1000 | Loss: 0.00001622
Iteration 74/1000 | Loss: 0.00001621
Iteration 75/1000 | Loss: 0.00001621
Iteration 76/1000 | Loss: 0.00001621
Iteration 77/1000 | Loss: 0.00001621
Iteration 78/1000 | Loss: 0.00001621
Iteration 79/1000 | Loss: 0.00001621
Iteration 80/1000 | Loss: 0.00001620
Iteration 81/1000 | Loss: 0.00001620
Iteration 82/1000 | Loss: 0.00001620
Iteration 83/1000 | Loss: 0.00001620
Iteration 84/1000 | Loss: 0.00001620
Iteration 85/1000 | Loss: 0.00001620
Iteration 86/1000 | Loss: 0.00001620
Iteration 87/1000 | Loss: 0.00001620
Iteration 88/1000 | Loss: 0.00001620
Iteration 89/1000 | Loss: 0.00001619
Iteration 90/1000 | Loss: 0.00001619
Iteration 91/1000 | Loss: 0.00001619
Iteration 92/1000 | Loss: 0.00001619
Iteration 93/1000 | Loss: 0.00001619
Iteration 94/1000 | Loss: 0.00001619
Iteration 95/1000 | Loss: 0.00001619
Iteration 96/1000 | Loss: 0.00001619
Iteration 97/1000 | Loss: 0.00001619
Iteration 98/1000 | Loss: 0.00001619
Iteration 99/1000 | Loss: 0.00001619
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001618
Iteration 102/1000 | Loss: 0.00001618
Iteration 103/1000 | Loss: 0.00001618
Iteration 104/1000 | Loss: 0.00001618
Iteration 105/1000 | Loss: 0.00001618
Iteration 106/1000 | Loss: 0.00001618
Iteration 107/1000 | Loss: 0.00001618
Iteration 108/1000 | Loss: 0.00001618
Iteration 109/1000 | Loss: 0.00001618
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001617
Iteration 112/1000 | Loss: 0.00001617
Iteration 113/1000 | Loss: 0.00001617
Iteration 114/1000 | Loss: 0.00001617
Iteration 115/1000 | Loss: 0.00001617
Iteration 116/1000 | Loss: 0.00001617
Iteration 117/1000 | Loss: 0.00001616
Iteration 118/1000 | Loss: 0.00001616
Iteration 119/1000 | Loss: 0.00001616
Iteration 120/1000 | Loss: 0.00001616
Iteration 121/1000 | Loss: 0.00001616
Iteration 122/1000 | Loss: 0.00001616
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001615
Iteration 125/1000 | Loss: 0.00001615
Iteration 126/1000 | Loss: 0.00001615
Iteration 127/1000 | Loss: 0.00001615
Iteration 128/1000 | Loss: 0.00001615
Iteration 129/1000 | Loss: 0.00001615
Iteration 130/1000 | Loss: 0.00001615
Iteration 131/1000 | Loss: 0.00001615
Iteration 132/1000 | Loss: 0.00001615
Iteration 133/1000 | Loss: 0.00001615
Iteration 134/1000 | Loss: 0.00001614
Iteration 135/1000 | Loss: 0.00001614
Iteration 136/1000 | Loss: 0.00001614
Iteration 137/1000 | Loss: 0.00001614
Iteration 138/1000 | Loss: 0.00001614
Iteration 139/1000 | Loss: 0.00001613
Iteration 140/1000 | Loss: 0.00001613
Iteration 141/1000 | Loss: 0.00001613
Iteration 142/1000 | Loss: 0.00001613
Iteration 143/1000 | Loss: 0.00001613
Iteration 144/1000 | Loss: 0.00001613
Iteration 145/1000 | Loss: 0.00001612
Iteration 146/1000 | Loss: 0.00001612
Iteration 147/1000 | Loss: 0.00001612
Iteration 148/1000 | Loss: 0.00001612
Iteration 149/1000 | Loss: 0.00001612
Iteration 150/1000 | Loss: 0.00001612
Iteration 151/1000 | Loss: 0.00001612
Iteration 152/1000 | Loss: 0.00001612
Iteration 153/1000 | Loss: 0.00001612
Iteration 154/1000 | Loss: 0.00001612
Iteration 155/1000 | Loss: 0.00001612
Iteration 156/1000 | Loss: 0.00001612
Iteration 157/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.612343839951791e-05, 1.612343839951791e-05, 1.612343839951791e-05, 1.612343839951791e-05, 1.612343839951791e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.612343839951791e-05

Optimization complete. Final v2v error: 3.474203586578369 mm

Highest mean error: 3.73276686668396 mm for frame 109

Lowest mean error: 3.054309129714966 mm for frame 0

Saving results

Total time: 30.53676748275757
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400975
Iteration 2/25 | Loss: 0.00133178
Iteration 3/25 | Loss: 0.00123305
Iteration 4/25 | Loss: 0.00121740
Iteration 5/25 | Loss: 0.00121285
Iteration 6/25 | Loss: 0.00121181
Iteration 7/25 | Loss: 0.00121181
Iteration 8/25 | Loss: 0.00121181
Iteration 9/25 | Loss: 0.00121181
Iteration 10/25 | Loss: 0.00121181
Iteration 11/25 | Loss: 0.00121181
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012118149315938354, 0.0012118149315938354, 0.0012118149315938354, 0.0012118149315938354, 0.0012118149315938354]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012118149315938354

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28412259
Iteration 2/25 | Loss: 0.00165906
Iteration 3/25 | Loss: 0.00165904
Iteration 4/25 | Loss: 0.00165904
Iteration 5/25 | Loss: 0.00165903
Iteration 6/25 | Loss: 0.00165903
Iteration 7/25 | Loss: 0.00165903
Iteration 8/25 | Loss: 0.00165903
Iteration 9/25 | Loss: 0.00165903
Iteration 10/25 | Loss: 0.00165903
Iteration 11/25 | Loss: 0.00165903
Iteration 12/25 | Loss: 0.00165903
Iteration 13/25 | Loss: 0.00165903
Iteration 14/25 | Loss: 0.00165903
Iteration 15/25 | Loss: 0.00165903
Iteration 16/25 | Loss: 0.00165903
Iteration 17/25 | Loss: 0.00165903
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016590321902185678, 0.0016590321902185678, 0.0016590321902185678, 0.0016590321902185678, 0.0016590321902185678]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016590321902185678

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00165903
Iteration 2/1000 | Loss: 0.00005045
Iteration 3/1000 | Loss: 0.00003197
Iteration 4/1000 | Loss: 0.00002513
Iteration 5/1000 | Loss: 0.00002174
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001823
Iteration 8/1000 | Loss: 0.00001723
Iteration 9/1000 | Loss: 0.00001667
Iteration 10/1000 | Loss: 0.00001613
Iteration 11/1000 | Loss: 0.00001584
Iteration 12/1000 | Loss: 0.00001558
Iteration 13/1000 | Loss: 0.00001527
Iteration 14/1000 | Loss: 0.00001514
Iteration 15/1000 | Loss: 0.00001513
Iteration 16/1000 | Loss: 0.00001511
Iteration 17/1000 | Loss: 0.00001511
Iteration 18/1000 | Loss: 0.00001511
Iteration 19/1000 | Loss: 0.00001510
Iteration 20/1000 | Loss: 0.00001508
Iteration 21/1000 | Loss: 0.00001507
Iteration 22/1000 | Loss: 0.00001505
Iteration 23/1000 | Loss: 0.00001504
Iteration 24/1000 | Loss: 0.00001502
Iteration 25/1000 | Loss: 0.00001501
Iteration 26/1000 | Loss: 0.00001501
Iteration 27/1000 | Loss: 0.00001500
Iteration 28/1000 | Loss: 0.00001499
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001493
Iteration 31/1000 | Loss: 0.00001490
Iteration 32/1000 | Loss: 0.00001487
Iteration 33/1000 | Loss: 0.00001482
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001481
Iteration 36/1000 | Loss: 0.00001480
Iteration 37/1000 | Loss: 0.00001477
Iteration 38/1000 | Loss: 0.00001475
Iteration 39/1000 | Loss: 0.00001473
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001471
Iteration 42/1000 | Loss: 0.00001471
Iteration 43/1000 | Loss: 0.00001471
Iteration 44/1000 | Loss: 0.00001471
Iteration 45/1000 | Loss: 0.00001469
Iteration 46/1000 | Loss: 0.00001469
Iteration 47/1000 | Loss: 0.00001463
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001459
Iteration 51/1000 | Loss: 0.00001458
Iteration 52/1000 | Loss: 0.00001458
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001456
Iteration 55/1000 | Loss: 0.00001456
Iteration 56/1000 | Loss: 0.00001455
Iteration 57/1000 | Loss: 0.00001455
Iteration 58/1000 | Loss: 0.00001454
Iteration 59/1000 | Loss: 0.00001454
Iteration 60/1000 | Loss: 0.00001453
Iteration 61/1000 | Loss: 0.00001453
Iteration 62/1000 | Loss: 0.00001452
Iteration 63/1000 | Loss: 0.00001452
Iteration 64/1000 | Loss: 0.00001451
Iteration 65/1000 | Loss: 0.00001451
Iteration 66/1000 | Loss: 0.00001451
Iteration 67/1000 | Loss: 0.00001450
Iteration 68/1000 | Loss: 0.00001450
Iteration 69/1000 | Loss: 0.00001449
Iteration 70/1000 | Loss: 0.00001449
Iteration 71/1000 | Loss: 0.00001448
Iteration 72/1000 | Loss: 0.00001448
Iteration 73/1000 | Loss: 0.00001447
Iteration 74/1000 | Loss: 0.00001447
Iteration 75/1000 | Loss: 0.00001446
Iteration 76/1000 | Loss: 0.00001446
Iteration 77/1000 | Loss: 0.00001446
Iteration 78/1000 | Loss: 0.00001446
Iteration 79/1000 | Loss: 0.00001446
Iteration 80/1000 | Loss: 0.00001445
Iteration 81/1000 | Loss: 0.00001445
Iteration 82/1000 | Loss: 0.00001445
Iteration 83/1000 | Loss: 0.00001445
Iteration 84/1000 | Loss: 0.00001445
Iteration 85/1000 | Loss: 0.00001444
Iteration 86/1000 | Loss: 0.00001444
Iteration 87/1000 | Loss: 0.00001444
Iteration 88/1000 | Loss: 0.00001444
Iteration 89/1000 | Loss: 0.00001443
Iteration 90/1000 | Loss: 0.00001443
Iteration 91/1000 | Loss: 0.00001442
Iteration 92/1000 | Loss: 0.00001442
Iteration 93/1000 | Loss: 0.00001442
Iteration 94/1000 | Loss: 0.00001442
Iteration 95/1000 | Loss: 0.00001442
Iteration 96/1000 | Loss: 0.00001441
Iteration 97/1000 | Loss: 0.00001441
Iteration 98/1000 | Loss: 0.00001441
Iteration 99/1000 | Loss: 0.00001441
Iteration 100/1000 | Loss: 0.00001441
Iteration 101/1000 | Loss: 0.00001441
Iteration 102/1000 | Loss: 0.00001441
Iteration 103/1000 | Loss: 0.00001441
Iteration 104/1000 | Loss: 0.00001440
Iteration 105/1000 | Loss: 0.00001440
Iteration 106/1000 | Loss: 0.00001440
Iteration 107/1000 | Loss: 0.00001440
Iteration 108/1000 | Loss: 0.00001440
Iteration 109/1000 | Loss: 0.00001440
Iteration 110/1000 | Loss: 0.00001440
Iteration 111/1000 | Loss: 0.00001439
Iteration 112/1000 | Loss: 0.00001439
Iteration 113/1000 | Loss: 0.00001439
Iteration 114/1000 | Loss: 0.00001439
Iteration 115/1000 | Loss: 0.00001439
Iteration 116/1000 | Loss: 0.00001439
Iteration 117/1000 | Loss: 0.00001438
Iteration 118/1000 | Loss: 0.00001438
Iteration 119/1000 | Loss: 0.00001438
Iteration 120/1000 | Loss: 0.00001438
Iteration 121/1000 | Loss: 0.00001438
Iteration 122/1000 | Loss: 0.00001437
Iteration 123/1000 | Loss: 0.00001437
Iteration 124/1000 | Loss: 0.00001437
Iteration 125/1000 | Loss: 0.00001437
Iteration 126/1000 | Loss: 0.00001437
Iteration 127/1000 | Loss: 0.00001436
Iteration 128/1000 | Loss: 0.00001436
Iteration 129/1000 | Loss: 0.00001436
Iteration 130/1000 | Loss: 0.00001436
Iteration 131/1000 | Loss: 0.00001436
Iteration 132/1000 | Loss: 0.00001436
Iteration 133/1000 | Loss: 0.00001436
Iteration 134/1000 | Loss: 0.00001435
Iteration 135/1000 | Loss: 0.00001435
Iteration 136/1000 | Loss: 0.00001435
Iteration 137/1000 | Loss: 0.00001435
Iteration 138/1000 | Loss: 0.00001435
Iteration 139/1000 | Loss: 0.00001434
Iteration 140/1000 | Loss: 0.00001434
Iteration 141/1000 | Loss: 0.00001434
Iteration 142/1000 | Loss: 0.00001434
Iteration 143/1000 | Loss: 0.00001433
Iteration 144/1000 | Loss: 0.00001433
Iteration 145/1000 | Loss: 0.00001432
Iteration 146/1000 | Loss: 0.00001432
Iteration 147/1000 | Loss: 0.00001431
Iteration 148/1000 | Loss: 0.00001431
Iteration 149/1000 | Loss: 0.00001431
Iteration 150/1000 | Loss: 0.00001430
Iteration 151/1000 | Loss: 0.00001430
Iteration 152/1000 | Loss: 0.00001430
Iteration 153/1000 | Loss: 0.00001430
Iteration 154/1000 | Loss: 0.00001429
Iteration 155/1000 | Loss: 0.00001429
Iteration 156/1000 | Loss: 0.00001429
Iteration 157/1000 | Loss: 0.00001429
Iteration 158/1000 | Loss: 0.00001428
Iteration 159/1000 | Loss: 0.00001428
Iteration 160/1000 | Loss: 0.00001428
Iteration 161/1000 | Loss: 0.00001428
Iteration 162/1000 | Loss: 0.00001428
Iteration 163/1000 | Loss: 0.00001428
Iteration 164/1000 | Loss: 0.00001428
Iteration 165/1000 | Loss: 0.00001428
Iteration 166/1000 | Loss: 0.00001428
Iteration 167/1000 | Loss: 0.00001427
Iteration 168/1000 | Loss: 0.00001427
Iteration 169/1000 | Loss: 0.00001427
Iteration 170/1000 | Loss: 0.00001426
Iteration 171/1000 | Loss: 0.00001426
Iteration 172/1000 | Loss: 0.00001426
Iteration 173/1000 | Loss: 0.00001426
Iteration 174/1000 | Loss: 0.00001426
Iteration 175/1000 | Loss: 0.00001426
Iteration 176/1000 | Loss: 0.00001426
Iteration 177/1000 | Loss: 0.00001426
Iteration 178/1000 | Loss: 0.00001426
Iteration 179/1000 | Loss: 0.00001426
Iteration 180/1000 | Loss: 0.00001426
Iteration 181/1000 | Loss: 0.00001426
Iteration 182/1000 | Loss: 0.00001426
Iteration 183/1000 | Loss: 0.00001425
Iteration 184/1000 | Loss: 0.00001425
Iteration 185/1000 | Loss: 0.00001425
Iteration 186/1000 | Loss: 0.00001425
Iteration 187/1000 | Loss: 0.00001425
Iteration 188/1000 | Loss: 0.00001425
Iteration 189/1000 | Loss: 0.00001424
Iteration 190/1000 | Loss: 0.00001424
Iteration 191/1000 | Loss: 0.00001424
Iteration 192/1000 | Loss: 0.00001424
Iteration 193/1000 | Loss: 0.00001424
Iteration 194/1000 | Loss: 0.00001424
Iteration 195/1000 | Loss: 0.00001424
Iteration 196/1000 | Loss: 0.00001424
Iteration 197/1000 | Loss: 0.00001424
Iteration 198/1000 | Loss: 0.00001424
Iteration 199/1000 | Loss: 0.00001424
Iteration 200/1000 | Loss: 0.00001424
Iteration 201/1000 | Loss: 0.00001423
Iteration 202/1000 | Loss: 0.00001423
Iteration 203/1000 | Loss: 0.00001423
Iteration 204/1000 | Loss: 0.00001423
Iteration 205/1000 | Loss: 0.00001423
Iteration 206/1000 | Loss: 0.00001423
Iteration 207/1000 | Loss: 0.00001423
Iteration 208/1000 | Loss: 0.00001422
Iteration 209/1000 | Loss: 0.00001422
Iteration 210/1000 | Loss: 0.00001422
Iteration 211/1000 | Loss: 0.00001422
Iteration 212/1000 | Loss: 0.00001422
Iteration 213/1000 | Loss: 0.00001422
Iteration 214/1000 | Loss: 0.00001422
Iteration 215/1000 | Loss: 0.00001422
Iteration 216/1000 | Loss: 0.00001422
Iteration 217/1000 | Loss: 0.00001422
Iteration 218/1000 | Loss: 0.00001422
Iteration 219/1000 | Loss: 0.00001422
Iteration 220/1000 | Loss: 0.00001422
Iteration 221/1000 | Loss: 0.00001421
Iteration 222/1000 | Loss: 0.00001421
Iteration 223/1000 | Loss: 0.00001421
Iteration 224/1000 | Loss: 0.00001421
Iteration 225/1000 | Loss: 0.00001421
Iteration 226/1000 | Loss: 0.00001421
Iteration 227/1000 | Loss: 0.00001421
Iteration 228/1000 | Loss: 0.00001421
Iteration 229/1000 | Loss: 0.00001421
Iteration 230/1000 | Loss: 0.00001421
Iteration 231/1000 | Loss: 0.00001421
Iteration 232/1000 | Loss: 0.00001420
Iteration 233/1000 | Loss: 0.00001420
Iteration 234/1000 | Loss: 0.00001420
Iteration 235/1000 | Loss: 0.00001420
Iteration 236/1000 | Loss: 0.00001420
Iteration 237/1000 | Loss: 0.00001420
Iteration 238/1000 | Loss: 0.00001420
Iteration 239/1000 | Loss: 0.00001420
Iteration 240/1000 | Loss: 0.00001420
Iteration 241/1000 | Loss: 0.00001420
Iteration 242/1000 | Loss: 0.00001420
Iteration 243/1000 | Loss: 0.00001420
Iteration 244/1000 | Loss: 0.00001420
Iteration 245/1000 | Loss: 0.00001420
Iteration 246/1000 | Loss: 0.00001420
Iteration 247/1000 | Loss: 0.00001420
Iteration 248/1000 | Loss: 0.00001420
Iteration 249/1000 | Loss: 0.00001419
Iteration 250/1000 | Loss: 0.00001419
Iteration 251/1000 | Loss: 0.00001419
Iteration 252/1000 | Loss: 0.00001419
Iteration 253/1000 | Loss: 0.00001419
Iteration 254/1000 | Loss: 0.00001419
Iteration 255/1000 | Loss: 0.00001419
Iteration 256/1000 | Loss: 0.00001419
Iteration 257/1000 | Loss: 0.00001419
Iteration 258/1000 | Loss: 0.00001419
Iteration 259/1000 | Loss: 0.00001419
Iteration 260/1000 | Loss: 0.00001419
Iteration 261/1000 | Loss: 0.00001419
Iteration 262/1000 | Loss: 0.00001418
Iteration 263/1000 | Loss: 0.00001418
Iteration 264/1000 | Loss: 0.00001418
Iteration 265/1000 | Loss: 0.00001418
Iteration 266/1000 | Loss: 0.00001418
Iteration 267/1000 | Loss: 0.00001418
Iteration 268/1000 | Loss: 0.00001418
Iteration 269/1000 | Loss: 0.00001418
Iteration 270/1000 | Loss: 0.00001418
Iteration 271/1000 | Loss: 0.00001418
Iteration 272/1000 | Loss: 0.00001418
Iteration 273/1000 | Loss: 0.00001418
Iteration 274/1000 | Loss: 0.00001417
Iteration 275/1000 | Loss: 0.00001417
Iteration 276/1000 | Loss: 0.00001417
Iteration 277/1000 | Loss: 0.00001417
Iteration 278/1000 | Loss: 0.00001417
Iteration 279/1000 | Loss: 0.00001417
Iteration 280/1000 | Loss: 0.00001417
Iteration 281/1000 | Loss: 0.00001417
Iteration 282/1000 | Loss: 0.00001417
Iteration 283/1000 | Loss: 0.00001417
Iteration 284/1000 | Loss: 0.00001417
Iteration 285/1000 | Loss: 0.00001416
Iteration 286/1000 | Loss: 0.00001416
Iteration 287/1000 | Loss: 0.00001416
Iteration 288/1000 | Loss: 0.00001416
Iteration 289/1000 | Loss: 0.00001416
Iteration 290/1000 | Loss: 0.00001416
Iteration 291/1000 | Loss: 0.00001416
Iteration 292/1000 | Loss: 0.00001416
Iteration 293/1000 | Loss: 0.00001416
Iteration 294/1000 | Loss: 0.00001415
Iteration 295/1000 | Loss: 0.00001415
Iteration 296/1000 | Loss: 0.00001415
Iteration 297/1000 | Loss: 0.00001415
Iteration 298/1000 | Loss: 0.00001415
Iteration 299/1000 | Loss: 0.00001415
Iteration 300/1000 | Loss: 0.00001415
Iteration 301/1000 | Loss: 0.00001415
Iteration 302/1000 | Loss: 0.00001415
Iteration 303/1000 | Loss: 0.00001415
Iteration 304/1000 | Loss: 0.00001415
Iteration 305/1000 | Loss: 0.00001415
Iteration 306/1000 | Loss: 0.00001415
Iteration 307/1000 | Loss: 0.00001415
Iteration 308/1000 | Loss: 0.00001415
Iteration 309/1000 | Loss: 0.00001415
Iteration 310/1000 | Loss: 0.00001415
Iteration 311/1000 | Loss: 0.00001415
Iteration 312/1000 | Loss: 0.00001415
Iteration 313/1000 | Loss: 0.00001415
Iteration 314/1000 | Loss: 0.00001415
Iteration 315/1000 | Loss: 0.00001415
Iteration 316/1000 | Loss: 0.00001415
Iteration 317/1000 | Loss: 0.00001415
Iteration 318/1000 | Loss: 0.00001415
Iteration 319/1000 | Loss: 0.00001415
Iteration 320/1000 | Loss: 0.00001415
Iteration 321/1000 | Loss: 0.00001415
Iteration 322/1000 | Loss: 0.00001415
Iteration 323/1000 | Loss: 0.00001415
Iteration 324/1000 | Loss: 0.00001415
Iteration 325/1000 | Loss: 0.00001415
Iteration 326/1000 | Loss: 0.00001415
Iteration 327/1000 | Loss: 0.00001415
Iteration 328/1000 | Loss: 0.00001415
Iteration 329/1000 | Loss: 0.00001415
Iteration 330/1000 | Loss: 0.00001415
Iteration 331/1000 | Loss: 0.00001415
Iteration 332/1000 | Loss: 0.00001415
Iteration 333/1000 | Loss: 0.00001415
Iteration 334/1000 | Loss: 0.00001415
Iteration 335/1000 | Loss: 0.00001415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 335. Stopping optimization.
Last 5 losses: [1.4145098248263821e-05, 1.4145098248263821e-05, 1.4145098248263821e-05, 1.4145098248263821e-05, 1.4145098248263821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4145098248263821e-05

Optimization complete. Final v2v error: 3.0726723670959473 mm

Highest mean error: 5.089756488800049 mm for frame 88

Lowest mean error: 2.493788719177246 mm for frame 21

Saving results

Total time: 54.43409824371338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787658
Iteration 2/25 | Loss: 0.00130477
Iteration 3/25 | Loss: 0.00121147
Iteration 4/25 | Loss: 0.00119932
Iteration 5/25 | Loss: 0.00119600
Iteration 6/25 | Loss: 0.00119600
Iteration 7/25 | Loss: 0.00119600
Iteration 8/25 | Loss: 0.00119600
Iteration 9/25 | Loss: 0.00119600
Iteration 10/25 | Loss: 0.00119600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011960010742768645, 0.0011960010742768645, 0.0011960010742768645, 0.0011960010742768645, 0.0011960010742768645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011960010742768645

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28067696
Iteration 2/25 | Loss: 0.00174438
Iteration 3/25 | Loss: 0.00174438
Iteration 4/25 | Loss: 0.00174437
Iteration 5/25 | Loss: 0.00174437
Iteration 6/25 | Loss: 0.00174437
Iteration 7/25 | Loss: 0.00174437
Iteration 8/25 | Loss: 0.00174437
Iteration 9/25 | Loss: 0.00174437
Iteration 10/25 | Loss: 0.00174437
Iteration 11/25 | Loss: 0.00174437
Iteration 12/25 | Loss: 0.00174437
Iteration 13/25 | Loss: 0.00174437
Iteration 14/25 | Loss: 0.00174437
Iteration 15/25 | Loss: 0.00174437
Iteration 16/25 | Loss: 0.00174437
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0017443726537749171, 0.0017443726537749171, 0.0017443726537749171, 0.0017443726537749171, 0.0017443726537749171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017443726537749171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00174437
Iteration 2/1000 | Loss: 0.00003870
Iteration 3/1000 | Loss: 0.00002398
Iteration 4/1000 | Loss: 0.00001833
Iteration 5/1000 | Loss: 0.00001576
Iteration 6/1000 | Loss: 0.00001440
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001288
Iteration 9/1000 | Loss: 0.00001253
Iteration 10/1000 | Loss: 0.00001227
Iteration 11/1000 | Loss: 0.00001224
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001192
Iteration 14/1000 | Loss: 0.00001175
Iteration 15/1000 | Loss: 0.00001168
Iteration 16/1000 | Loss: 0.00001165
Iteration 17/1000 | Loss: 0.00001164
Iteration 18/1000 | Loss: 0.00001162
Iteration 19/1000 | Loss: 0.00001157
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001152
Iteration 22/1000 | Loss: 0.00001152
Iteration 23/1000 | Loss: 0.00001151
Iteration 24/1000 | Loss: 0.00001151
Iteration 25/1000 | Loss: 0.00001151
Iteration 26/1000 | Loss: 0.00001150
Iteration 27/1000 | Loss: 0.00001150
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001148
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001146
Iteration 38/1000 | Loss: 0.00001146
Iteration 39/1000 | Loss: 0.00001145
Iteration 40/1000 | Loss: 0.00001144
Iteration 41/1000 | Loss: 0.00001144
Iteration 42/1000 | Loss: 0.00001141
Iteration 43/1000 | Loss: 0.00001138
Iteration 44/1000 | Loss: 0.00001137
Iteration 45/1000 | Loss: 0.00001136
Iteration 46/1000 | Loss: 0.00001136
Iteration 47/1000 | Loss: 0.00001135
Iteration 48/1000 | Loss: 0.00001135
Iteration 49/1000 | Loss: 0.00001135
Iteration 50/1000 | Loss: 0.00001134
Iteration 51/1000 | Loss: 0.00001134
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001128
Iteration 55/1000 | Loss: 0.00001126
Iteration 56/1000 | Loss: 0.00001122
Iteration 57/1000 | Loss: 0.00001122
Iteration 58/1000 | Loss: 0.00001120
Iteration 59/1000 | Loss: 0.00001120
Iteration 60/1000 | Loss: 0.00001119
Iteration 61/1000 | Loss: 0.00001119
Iteration 62/1000 | Loss: 0.00001118
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001117
Iteration 66/1000 | Loss: 0.00001116
Iteration 67/1000 | Loss: 0.00001115
Iteration 68/1000 | Loss: 0.00001115
Iteration 69/1000 | Loss: 0.00001115
Iteration 70/1000 | Loss: 0.00001115
Iteration 71/1000 | Loss: 0.00001114
Iteration 72/1000 | Loss: 0.00001114
Iteration 73/1000 | Loss: 0.00001114
Iteration 74/1000 | Loss: 0.00001113
Iteration 75/1000 | Loss: 0.00001113
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001111
Iteration 83/1000 | Loss: 0.00001111
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001110
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001109
Iteration 93/1000 | Loss: 0.00001109
Iteration 94/1000 | Loss: 0.00001109
Iteration 95/1000 | Loss: 0.00001109
Iteration 96/1000 | Loss: 0.00001109
Iteration 97/1000 | Loss: 0.00001109
Iteration 98/1000 | Loss: 0.00001108
Iteration 99/1000 | Loss: 0.00001108
Iteration 100/1000 | Loss: 0.00001108
Iteration 101/1000 | Loss: 0.00001108
Iteration 102/1000 | Loss: 0.00001107
Iteration 103/1000 | Loss: 0.00001107
Iteration 104/1000 | Loss: 0.00001107
Iteration 105/1000 | Loss: 0.00001107
Iteration 106/1000 | Loss: 0.00001107
Iteration 107/1000 | Loss: 0.00001107
Iteration 108/1000 | Loss: 0.00001107
Iteration 109/1000 | Loss: 0.00001107
Iteration 110/1000 | Loss: 0.00001106
Iteration 111/1000 | Loss: 0.00001106
Iteration 112/1000 | Loss: 0.00001106
Iteration 113/1000 | Loss: 0.00001106
Iteration 114/1000 | Loss: 0.00001106
Iteration 115/1000 | Loss: 0.00001105
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001104
Iteration 119/1000 | Loss: 0.00001104
Iteration 120/1000 | Loss: 0.00001104
Iteration 121/1000 | Loss: 0.00001103
Iteration 122/1000 | Loss: 0.00001103
Iteration 123/1000 | Loss: 0.00001103
Iteration 124/1000 | Loss: 0.00001102
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001101
Iteration 127/1000 | Loss: 0.00001101
Iteration 128/1000 | Loss: 0.00001101
Iteration 129/1000 | Loss: 0.00001100
Iteration 130/1000 | Loss: 0.00001100
Iteration 131/1000 | Loss: 0.00001100
Iteration 132/1000 | Loss: 0.00001100
Iteration 133/1000 | Loss: 0.00001100
Iteration 134/1000 | Loss: 0.00001100
Iteration 135/1000 | Loss: 0.00001100
Iteration 136/1000 | Loss: 0.00001099
Iteration 137/1000 | Loss: 0.00001099
Iteration 138/1000 | Loss: 0.00001099
Iteration 139/1000 | Loss: 0.00001099
Iteration 140/1000 | Loss: 0.00001099
Iteration 141/1000 | Loss: 0.00001099
Iteration 142/1000 | Loss: 0.00001099
Iteration 143/1000 | Loss: 0.00001099
Iteration 144/1000 | Loss: 0.00001099
Iteration 145/1000 | Loss: 0.00001099
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.0991744602506515e-05, 1.0991744602506515e-05, 1.0991744602506515e-05, 1.0991744602506515e-05, 1.0991744602506515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0991744602506515e-05

Optimization complete. Final v2v error: 2.8772621154785156 mm

Highest mean error: 3.446722984313965 mm for frame 239

Lowest mean error: 2.5875706672668457 mm for frame 128

Saving results

Total time: 45.28711652755737
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396287
Iteration 2/25 | Loss: 0.00125796
Iteration 3/25 | Loss: 0.00119141
Iteration 4/25 | Loss: 0.00118012
Iteration 5/25 | Loss: 0.00117655
Iteration 6/25 | Loss: 0.00117581
Iteration 7/25 | Loss: 0.00117581
Iteration 8/25 | Loss: 0.00117581
Iteration 9/25 | Loss: 0.00117581
Iteration 10/25 | Loss: 0.00117581
Iteration 11/25 | Loss: 0.00117581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001175812678411603, 0.001175812678411603, 0.001175812678411603, 0.001175812678411603, 0.001175812678411603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001175812678411603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.45726037
Iteration 2/25 | Loss: 0.00136230
Iteration 3/25 | Loss: 0.00136230
Iteration 4/25 | Loss: 0.00136229
Iteration 5/25 | Loss: 0.00136229
Iteration 6/25 | Loss: 0.00136229
Iteration 7/25 | Loss: 0.00136229
Iteration 8/25 | Loss: 0.00136229
Iteration 9/25 | Loss: 0.00136229
Iteration 10/25 | Loss: 0.00136229
Iteration 11/25 | Loss: 0.00136229
Iteration 12/25 | Loss: 0.00136229
Iteration 13/25 | Loss: 0.00136229
Iteration 14/25 | Loss: 0.00136229
Iteration 15/25 | Loss: 0.00136229
Iteration 16/25 | Loss: 0.00136229
Iteration 17/25 | Loss: 0.00136229
Iteration 18/25 | Loss: 0.00136229
Iteration 19/25 | Loss: 0.00136229
Iteration 20/25 | Loss: 0.00136229
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013622923288494349, 0.0013622923288494349, 0.0013622923288494349, 0.0013622923288494349, 0.0013622923288494349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013622923288494349

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136229
Iteration 2/1000 | Loss: 0.00002228
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001352
Iteration 5/1000 | Loss: 0.00001264
Iteration 6/1000 | Loss: 0.00001213
Iteration 7/1000 | Loss: 0.00001167
Iteration 8/1000 | Loss: 0.00001136
Iteration 9/1000 | Loss: 0.00001134
Iteration 10/1000 | Loss: 0.00001100
Iteration 11/1000 | Loss: 0.00001075
Iteration 12/1000 | Loss: 0.00001067
Iteration 13/1000 | Loss: 0.00001067
Iteration 14/1000 | Loss: 0.00001065
Iteration 15/1000 | Loss: 0.00001063
Iteration 16/1000 | Loss: 0.00001061
Iteration 17/1000 | Loss: 0.00001048
Iteration 18/1000 | Loss: 0.00001048
Iteration 19/1000 | Loss: 0.00001047
Iteration 20/1000 | Loss: 0.00001046
Iteration 21/1000 | Loss: 0.00001039
Iteration 22/1000 | Loss: 0.00001038
Iteration 23/1000 | Loss: 0.00001036
Iteration 24/1000 | Loss: 0.00001033
Iteration 25/1000 | Loss: 0.00001032
Iteration 26/1000 | Loss: 0.00001030
Iteration 27/1000 | Loss: 0.00001026
Iteration 28/1000 | Loss: 0.00001026
Iteration 29/1000 | Loss: 0.00001024
Iteration 30/1000 | Loss: 0.00001024
Iteration 31/1000 | Loss: 0.00001024
Iteration 32/1000 | Loss: 0.00001023
Iteration 33/1000 | Loss: 0.00001022
Iteration 34/1000 | Loss: 0.00001020
Iteration 35/1000 | Loss: 0.00001015
Iteration 36/1000 | Loss: 0.00001009
Iteration 37/1000 | Loss: 0.00001005
Iteration 38/1000 | Loss: 0.00001004
Iteration 39/1000 | Loss: 0.00001004
Iteration 40/1000 | Loss: 0.00001003
Iteration 41/1000 | Loss: 0.00001003
Iteration 42/1000 | Loss: 0.00001002
Iteration 43/1000 | Loss: 0.00001001
Iteration 44/1000 | Loss: 0.00001000
Iteration 45/1000 | Loss: 0.00001000
Iteration 46/1000 | Loss: 0.00000999
Iteration 47/1000 | Loss: 0.00000999
Iteration 48/1000 | Loss: 0.00000998
Iteration 49/1000 | Loss: 0.00000998
Iteration 50/1000 | Loss: 0.00000998
Iteration 51/1000 | Loss: 0.00000998
Iteration 52/1000 | Loss: 0.00000996
Iteration 53/1000 | Loss: 0.00000995
Iteration 54/1000 | Loss: 0.00000995
Iteration 55/1000 | Loss: 0.00000994
Iteration 56/1000 | Loss: 0.00000994
Iteration 57/1000 | Loss: 0.00000994
Iteration 58/1000 | Loss: 0.00000993
Iteration 59/1000 | Loss: 0.00000993
Iteration 60/1000 | Loss: 0.00000992
Iteration 61/1000 | Loss: 0.00000992
Iteration 62/1000 | Loss: 0.00000991
Iteration 63/1000 | Loss: 0.00000991
Iteration 64/1000 | Loss: 0.00000991
Iteration 65/1000 | Loss: 0.00000991
Iteration 66/1000 | Loss: 0.00000991
Iteration 67/1000 | Loss: 0.00000991
Iteration 68/1000 | Loss: 0.00000991
Iteration 69/1000 | Loss: 0.00000991
Iteration 70/1000 | Loss: 0.00000990
Iteration 71/1000 | Loss: 0.00000990
Iteration 72/1000 | Loss: 0.00000990
Iteration 73/1000 | Loss: 0.00000990
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000989
Iteration 76/1000 | Loss: 0.00000989
Iteration 77/1000 | Loss: 0.00000989
Iteration 78/1000 | Loss: 0.00000989
Iteration 79/1000 | Loss: 0.00000989
Iteration 80/1000 | Loss: 0.00000989
Iteration 81/1000 | Loss: 0.00000989
Iteration 82/1000 | Loss: 0.00000989
Iteration 83/1000 | Loss: 0.00000989
Iteration 84/1000 | Loss: 0.00000989
Iteration 85/1000 | Loss: 0.00000988
Iteration 86/1000 | Loss: 0.00000988
Iteration 87/1000 | Loss: 0.00000987
Iteration 88/1000 | Loss: 0.00000987
Iteration 89/1000 | Loss: 0.00000987
Iteration 90/1000 | Loss: 0.00000986
Iteration 91/1000 | Loss: 0.00000985
Iteration 92/1000 | Loss: 0.00000985
Iteration 93/1000 | Loss: 0.00000985
Iteration 94/1000 | Loss: 0.00000985
Iteration 95/1000 | Loss: 0.00000985
Iteration 96/1000 | Loss: 0.00000985
Iteration 97/1000 | Loss: 0.00000984
Iteration 98/1000 | Loss: 0.00000984
Iteration 99/1000 | Loss: 0.00000983
Iteration 100/1000 | Loss: 0.00000983
Iteration 101/1000 | Loss: 0.00000983
Iteration 102/1000 | Loss: 0.00000983
Iteration 103/1000 | Loss: 0.00000983
Iteration 104/1000 | Loss: 0.00000983
Iteration 105/1000 | Loss: 0.00000983
Iteration 106/1000 | Loss: 0.00000982
Iteration 107/1000 | Loss: 0.00000982
Iteration 108/1000 | Loss: 0.00000982
Iteration 109/1000 | Loss: 0.00000982
Iteration 110/1000 | Loss: 0.00000981
Iteration 111/1000 | Loss: 0.00000981
Iteration 112/1000 | Loss: 0.00000981
Iteration 113/1000 | Loss: 0.00000981
Iteration 114/1000 | Loss: 0.00000980
Iteration 115/1000 | Loss: 0.00000980
Iteration 116/1000 | Loss: 0.00000980
Iteration 117/1000 | Loss: 0.00000980
Iteration 118/1000 | Loss: 0.00000980
Iteration 119/1000 | Loss: 0.00000980
Iteration 120/1000 | Loss: 0.00000980
Iteration 121/1000 | Loss: 0.00000979
Iteration 122/1000 | Loss: 0.00000979
Iteration 123/1000 | Loss: 0.00000979
Iteration 124/1000 | Loss: 0.00000979
Iteration 125/1000 | Loss: 0.00000979
Iteration 126/1000 | Loss: 0.00000979
Iteration 127/1000 | Loss: 0.00000979
Iteration 128/1000 | Loss: 0.00000979
Iteration 129/1000 | Loss: 0.00000979
Iteration 130/1000 | Loss: 0.00000978
Iteration 131/1000 | Loss: 0.00000978
Iteration 132/1000 | Loss: 0.00000977
Iteration 133/1000 | Loss: 0.00000977
Iteration 134/1000 | Loss: 0.00000977
Iteration 135/1000 | Loss: 0.00000976
Iteration 136/1000 | Loss: 0.00000976
Iteration 137/1000 | Loss: 0.00000976
Iteration 138/1000 | Loss: 0.00000976
Iteration 139/1000 | Loss: 0.00000975
Iteration 140/1000 | Loss: 0.00000975
Iteration 141/1000 | Loss: 0.00000975
Iteration 142/1000 | Loss: 0.00000975
Iteration 143/1000 | Loss: 0.00000975
Iteration 144/1000 | Loss: 0.00000975
Iteration 145/1000 | Loss: 0.00000975
Iteration 146/1000 | Loss: 0.00000975
Iteration 147/1000 | Loss: 0.00000975
Iteration 148/1000 | Loss: 0.00000975
Iteration 149/1000 | Loss: 0.00000975
Iteration 150/1000 | Loss: 0.00000975
Iteration 151/1000 | Loss: 0.00000975
Iteration 152/1000 | Loss: 0.00000975
Iteration 153/1000 | Loss: 0.00000975
Iteration 154/1000 | Loss: 0.00000975
Iteration 155/1000 | Loss: 0.00000975
Iteration 156/1000 | Loss: 0.00000975
Iteration 157/1000 | Loss: 0.00000974
Iteration 158/1000 | Loss: 0.00000974
Iteration 159/1000 | Loss: 0.00000974
Iteration 160/1000 | Loss: 0.00000974
Iteration 161/1000 | Loss: 0.00000974
Iteration 162/1000 | Loss: 0.00000974
Iteration 163/1000 | Loss: 0.00000973
Iteration 164/1000 | Loss: 0.00000973
Iteration 165/1000 | Loss: 0.00000973
Iteration 166/1000 | Loss: 0.00000973
Iteration 167/1000 | Loss: 0.00000973
Iteration 168/1000 | Loss: 0.00000973
Iteration 169/1000 | Loss: 0.00000973
Iteration 170/1000 | Loss: 0.00000973
Iteration 171/1000 | Loss: 0.00000973
Iteration 172/1000 | Loss: 0.00000973
Iteration 173/1000 | Loss: 0.00000973
Iteration 174/1000 | Loss: 0.00000973
Iteration 175/1000 | Loss: 0.00000973
Iteration 176/1000 | Loss: 0.00000973
Iteration 177/1000 | Loss: 0.00000973
Iteration 178/1000 | Loss: 0.00000973
Iteration 179/1000 | Loss: 0.00000973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [9.731953468872234e-06, 9.731953468872234e-06, 9.731953468872234e-06, 9.731953468872234e-06, 9.731953468872234e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.731953468872234e-06

Optimization complete. Final v2v error: 2.720625638961792 mm

Highest mean error: 3.0168864727020264 mm for frame 77

Lowest mean error: 2.5973868370056152 mm for frame 140

Saving results

Total time: 40.86526322364807
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382730
Iteration 2/25 | Loss: 0.00132273
Iteration 3/25 | Loss: 0.00122696
Iteration 4/25 | Loss: 0.00120742
Iteration 5/25 | Loss: 0.00119950
Iteration 6/25 | Loss: 0.00119740
Iteration 7/25 | Loss: 0.00119680
Iteration 8/25 | Loss: 0.00119680
Iteration 9/25 | Loss: 0.00119680
Iteration 10/25 | Loss: 0.00119680
Iteration 11/25 | Loss: 0.00119680
Iteration 12/25 | Loss: 0.00119680
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011968001490458846, 0.0011968001490458846, 0.0011968001490458846, 0.0011968001490458846, 0.0011968001490458846]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011968001490458846

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39673507
Iteration 2/25 | Loss: 0.00202585
Iteration 3/25 | Loss: 0.00202585
Iteration 4/25 | Loss: 0.00202585
Iteration 5/25 | Loss: 0.00202585
Iteration 6/25 | Loss: 0.00202585
Iteration 7/25 | Loss: 0.00202585
Iteration 8/25 | Loss: 0.00202585
Iteration 9/25 | Loss: 0.00202585
Iteration 10/25 | Loss: 0.00202585
Iteration 11/25 | Loss: 0.00202585
Iteration 12/25 | Loss: 0.00202585
Iteration 13/25 | Loss: 0.00202585
Iteration 14/25 | Loss: 0.00202585
Iteration 15/25 | Loss: 0.00202585
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002025849651545286, 0.002025849651545286, 0.002025849651545286, 0.002025849651545286, 0.002025849651545286]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002025849651545286

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202585
Iteration 2/1000 | Loss: 0.00005691
Iteration 3/1000 | Loss: 0.00004033
Iteration 4/1000 | Loss: 0.00003156
Iteration 5/1000 | Loss: 0.00002837
Iteration 6/1000 | Loss: 0.00002628
Iteration 7/1000 | Loss: 0.00002459
Iteration 8/1000 | Loss: 0.00002360
Iteration 9/1000 | Loss: 0.00002277
Iteration 10/1000 | Loss: 0.00002220
Iteration 11/1000 | Loss: 0.00002178
Iteration 12/1000 | Loss: 0.00002145
Iteration 13/1000 | Loss: 0.00002125
Iteration 14/1000 | Loss: 0.00002121
Iteration 15/1000 | Loss: 0.00002101
Iteration 16/1000 | Loss: 0.00002083
Iteration 17/1000 | Loss: 0.00002077
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002069
Iteration 20/1000 | Loss: 0.00002069
Iteration 21/1000 | Loss: 0.00002067
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002063
Iteration 24/1000 | Loss: 0.00002053
Iteration 25/1000 | Loss: 0.00002047
Iteration 26/1000 | Loss: 0.00002045
Iteration 27/1000 | Loss: 0.00002044
Iteration 28/1000 | Loss: 0.00002044
Iteration 29/1000 | Loss: 0.00002043
Iteration 30/1000 | Loss: 0.00002043
Iteration 31/1000 | Loss: 0.00002041
Iteration 32/1000 | Loss: 0.00002038
Iteration 33/1000 | Loss: 0.00002037
Iteration 34/1000 | Loss: 0.00002037
Iteration 35/1000 | Loss: 0.00002036
Iteration 36/1000 | Loss: 0.00002036
Iteration 37/1000 | Loss: 0.00002036
Iteration 38/1000 | Loss: 0.00002035
Iteration 39/1000 | Loss: 0.00002035
Iteration 40/1000 | Loss: 0.00002035
Iteration 41/1000 | Loss: 0.00002035
Iteration 42/1000 | Loss: 0.00002035
Iteration 43/1000 | Loss: 0.00002035
Iteration 44/1000 | Loss: 0.00002034
Iteration 45/1000 | Loss: 0.00002034
Iteration 46/1000 | Loss: 0.00002034
Iteration 47/1000 | Loss: 0.00002034
Iteration 48/1000 | Loss: 0.00002034
Iteration 49/1000 | Loss: 0.00002033
Iteration 50/1000 | Loss: 0.00002033
Iteration 51/1000 | Loss: 0.00002033
Iteration 52/1000 | Loss: 0.00002033
Iteration 53/1000 | Loss: 0.00002032
Iteration 54/1000 | Loss: 0.00002032
Iteration 55/1000 | Loss: 0.00002032
Iteration 56/1000 | Loss: 0.00002032
Iteration 57/1000 | Loss: 0.00002032
Iteration 58/1000 | Loss: 0.00002031
Iteration 59/1000 | Loss: 0.00002031
Iteration 60/1000 | Loss: 0.00002031
Iteration 61/1000 | Loss: 0.00002030
Iteration 62/1000 | Loss: 0.00002030
Iteration 63/1000 | Loss: 0.00002030
Iteration 64/1000 | Loss: 0.00002030
Iteration 65/1000 | Loss: 0.00002030
Iteration 66/1000 | Loss: 0.00002030
Iteration 67/1000 | Loss: 0.00002030
Iteration 68/1000 | Loss: 0.00002030
Iteration 69/1000 | Loss: 0.00002030
Iteration 70/1000 | Loss: 0.00002030
Iteration 71/1000 | Loss: 0.00002030
Iteration 72/1000 | Loss: 0.00002030
Iteration 73/1000 | Loss: 0.00002030
Iteration 74/1000 | Loss: 0.00002029
Iteration 75/1000 | Loss: 0.00002029
Iteration 76/1000 | Loss: 0.00002029
Iteration 77/1000 | Loss: 0.00002029
Iteration 78/1000 | Loss: 0.00002029
Iteration 79/1000 | Loss: 0.00002029
Iteration 80/1000 | Loss: 0.00002029
Iteration 81/1000 | Loss: 0.00002029
Iteration 82/1000 | Loss: 0.00002029
Iteration 83/1000 | Loss: 0.00002028
Iteration 84/1000 | Loss: 0.00002028
Iteration 85/1000 | Loss: 0.00002028
Iteration 86/1000 | Loss: 0.00002028
Iteration 87/1000 | Loss: 0.00002027
Iteration 88/1000 | Loss: 0.00002027
Iteration 89/1000 | Loss: 0.00002027
Iteration 90/1000 | Loss: 0.00002027
Iteration 91/1000 | Loss: 0.00002027
Iteration 92/1000 | Loss: 0.00002027
Iteration 93/1000 | Loss: 0.00002026
Iteration 94/1000 | Loss: 0.00002026
Iteration 95/1000 | Loss: 0.00002026
Iteration 96/1000 | Loss: 0.00002026
Iteration 97/1000 | Loss: 0.00002026
Iteration 98/1000 | Loss: 0.00002026
Iteration 99/1000 | Loss: 0.00002026
Iteration 100/1000 | Loss: 0.00002025
Iteration 101/1000 | Loss: 0.00002025
Iteration 102/1000 | Loss: 0.00002025
Iteration 103/1000 | Loss: 0.00002025
Iteration 104/1000 | Loss: 0.00002025
Iteration 105/1000 | Loss: 0.00002025
Iteration 106/1000 | Loss: 0.00002025
Iteration 107/1000 | Loss: 0.00002025
Iteration 108/1000 | Loss: 0.00002025
Iteration 109/1000 | Loss: 0.00002025
Iteration 110/1000 | Loss: 0.00002025
Iteration 111/1000 | Loss: 0.00002025
Iteration 112/1000 | Loss: 0.00002025
Iteration 113/1000 | Loss: 0.00002025
Iteration 114/1000 | Loss: 0.00002024
Iteration 115/1000 | Loss: 0.00002024
Iteration 116/1000 | Loss: 0.00002024
Iteration 117/1000 | Loss: 0.00002024
Iteration 118/1000 | Loss: 0.00002024
Iteration 119/1000 | Loss: 0.00002024
Iteration 120/1000 | Loss: 0.00002024
Iteration 121/1000 | Loss: 0.00002023
Iteration 122/1000 | Loss: 0.00002023
Iteration 123/1000 | Loss: 0.00002023
Iteration 124/1000 | Loss: 0.00002023
Iteration 125/1000 | Loss: 0.00002023
Iteration 126/1000 | Loss: 0.00002022
Iteration 127/1000 | Loss: 0.00002022
Iteration 128/1000 | Loss: 0.00002022
Iteration 129/1000 | Loss: 0.00002022
Iteration 130/1000 | Loss: 0.00002022
Iteration 131/1000 | Loss: 0.00002022
Iteration 132/1000 | Loss: 0.00002022
Iteration 133/1000 | Loss: 0.00002022
Iteration 134/1000 | Loss: 0.00002021
Iteration 135/1000 | Loss: 0.00002021
Iteration 136/1000 | Loss: 0.00002021
Iteration 137/1000 | Loss: 0.00002021
Iteration 138/1000 | Loss: 0.00002021
Iteration 139/1000 | Loss: 0.00002021
Iteration 140/1000 | Loss: 0.00002021
Iteration 141/1000 | Loss: 0.00002021
Iteration 142/1000 | Loss: 0.00002021
Iteration 143/1000 | Loss: 0.00002021
Iteration 144/1000 | Loss: 0.00002021
Iteration 145/1000 | Loss: 0.00002020
Iteration 146/1000 | Loss: 0.00002020
Iteration 147/1000 | Loss: 0.00002020
Iteration 148/1000 | Loss: 0.00002020
Iteration 149/1000 | Loss: 0.00002020
Iteration 150/1000 | Loss: 0.00002020
Iteration 151/1000 | Loss: 0.00002019
Iteration 152/1000 | Loss: 0.00002019
Iteration 153/1000 | Loss: 0.00002019
Iteration 154/1000 | Loss: 0.00002019
Iteration 155/1000 | Loss: 0.00002019
Iteration 156/1000 | Loss: 0.00002019
Iteration 157/1000 | Loss: 0.00002018
Iteration 158/1000 | Loss: 0.00002018
Iteration 159/1000 | Loss: 0.00002018
Iteration 160/1000 | Loss: 0.00002018
Iteration 161/1000 | Loss: 0.00002018
Iteration 162/1000 | Loss: 0.00002018
Iteration 163/1000 | Loss: 0.00002018
Iteration 164/1000 | Loss: 0.00002018
Iteration 165/1000 | Loss: 0.00002018
Iteration 166/1000 | Loss: 0.00002018
Iteration 167/1000 | Loss: 0.00002018
Iteration 168/1000 | Loss: 0.00002018
Iteration 169/1000 | Loss: 0.00002018
Iteration 170/1000 | Loss: 0.00002017
Iteration 171/1000 | Loss: 0.00002017
Iteration 172/1000 | Loss: 0.00002017
Iteration 173/1000 | Loss: 0.00002017
Iteration 174/1000 | Loss: 0.00002017
Iteration 175/1000 | Loss: 0.00002017
Iteration 176/1000 | Loss: 0.00002017
Iteration 177/1000 | Loss: 0.00002017
Iteration 178/1000 | Loss: 0.00002017
Iteration 179/1000 | Loss: 0.00002016
Iteration 180/1000 | Loss: 0.00002016
Iteration 181/1000 | Loss: 0.00002016
Iteration 182/1000 | Loss: 0.00002016
Iteration 183/1000 | Loss: 0.00002016
Iteration 184/1000 | Loss: 0.00002016
Iteration 185/1000 | Loss: 0.00002016
Iteration 186/1000 | Loss: 0.00002016
Iteration 187/1000 | Loss: 0.00002016
Iteration 188/1000 | Loss: 0.00002016
Iteration 189/1000 | Loss: 0.00002016
Iteration 190/1000 | Loss: 0.00002016
Iteration 191/1000 | Loss: 0.00002016
Iteration 192/1000 | Loss: 0.00002016
Iteration 193/1000 | Loss: 0.00002016
Iteration 194/1000 | Loss: 0.00002016
Iteration 195/1000 | Loss: 0.00002016
Iteration 196/1000 | Loss: 0.00002016
Iteration 197/1000 | Loss: 0.00002016
Iteration 198/1000 | Loss: 0.00002016
Iteration 199/1000 | Loss: 0.00002016
Iteration 200/1000 | Loss: 0.00002016
Iteration 201/1000 | Loss: 0.00002016
Iteration 202/1000 | Loss: 0.00002016
Iteration 203/1000 | Loss: 0.00002016
Iteration 204/1000 | Loss: 0.00002016
Iteration 205/1000 | Loss: 0.00002016
Iteration 206/1000 | Loss: 0.00002016
Iteration 207/1000 | Loss: 0.00002016
Iteration 208/1000 | Loss: 0.00002016
Iteration 209/1000 | Loss: 0.00002016
Iteration 210/1000 | Loss: 0.00002016
Iteration 211/1000 | Loss: 0.00002016
Iteration 212/1000 | Loss: 0.00002016
Iteration 213/1000 | Loss: 0.00002016
Iteration 214/1000 | Loss: 0.00002016
Iteration 215/1000 | Loss: 0.00002016
Iteration 216/1000 | Loss: 0.00002016
Iteration 217/1000 | Loss: 0.00002016
Iteration 218/1000 | Loss: 0.00002016
Iteration 219/1000 | Loss: 0.00002016
Iteration 220/1000 | Loss: 0.00002016
Iteration 221/1000 | Loss: 0.00002016
Iteration 222/1000 | Loss: 0.00002016
Iteration 223/1000 | Loss: 0.00002016
Iteration 224/1000 | Loss: 0.00002016
Iteration 225/1000 | Loss: 0.00002016
Iteration 226/1000 | Loss: 0.00002016
Iteration 227/1000 | Loss: 0.00002016
Iteration 228/1000 | Loss: 0.00002016
Iteration 229/1000 | Loss: 0.00002016
Iteration 230/1000 | Loss: 0.00002016
Iteration 231/1000 | Loss: 0.00002016
Iteration 232/1000 | Loss: 0.00002016
Iteration 233/1000 | Loss: 0.00002016
Iteration 234/1000 | Loss: 0.00002016
Iteration 235/1000 | Loss: 0.00002016
Iteration 236/1000 | Loss: 0.00002016
Iteration 237/1000 | Loss: 0.00002016
Iteration 238/1000 | Loss: 0.00002016
Iteration 239/1000 | Loss: 0.00002016
Iteration 240/1000 | Loss: 0.00002016
Iteration 241/1000 | Loss: 0.00002016
Iteration 242/1000 | Loss: 0.00002016
Iteration 243/1000 | Loss: 0.00002016
Iteration 244/1000 | Loss: 0.00002016
Iteration 245/1000 | Loss: 0.00002016
Iteration 246/1000 | Loss: 0.00002016
Iteration 247/1000 | Loss: 0.00002016
Iteration 248/1000 | Loss: 0.00002016
Iteration 249/1000 | Loss: 0.00002016
Iteration 250/1000 | Loss: 0.00002016
Iteration 251/1000 | Loss: 0.00002016
Iteration 252/1000 | Loss: 0.00002016
Iteration 253/1000 | Loss: 0.00002016
Iteration 254/1000 | Loss: 0.00002016
Iteration 255/1000 | Loss: 0.00002016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [2.0155242964392528e-05, 2.0155242964392528e-05, 2.0155242964392528e-05, 2.0155242964392528e-05, 2.0155242964392528e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0155242964392528e-05

Optimization complete. Final v2v error: 3.652743101119995 mm

Highest mean error: 5.308891296386719 mm for frame 45

Lowest mean error: 2.5318944454193115 mm for frame 33

Saving results

Total time: 48.337563276290894
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00676326
Iteration 2/25 | Loss: 0.00133936
Iteration 3/25 | Loss: 0.00124218
Iteration 4/25 | Loss: 0.00120531
Iteration 5/25 | Loss: 0.00120268
Iteration 6/25 | Loss: 0.00119656
Iteration 7/25 | Loss: 0.00119450
Iteration 8/25 | Loss: 0.00119353
Iteration 9/25 | Loss: 0.00119291
Iteration 10/25 | Loss: 0.00119243
Iteration 11/25 | Loss: 0.00119222
Iteration 12/25 | Loss: 0.00119215
Iteration 13/25 | Loss: 0.00119215
Iteration 14/25 | Loss: 0.00119215
Iteration 15/25 | Loss: 0.00119215
Iteration 16/25 | Loss: 0.00119215
Iteration 17/25 | Loss: 0.00119215
Iteration 18/25 | Loss: 0.00119215
Iteration 19/25 | Loss: 0.00119215
Iteration 20/25 | Loss: 0.00119215
Iteration 21/25 | Loss: 0.00119215
Iteration 22/25 | Loss: 0.00119214
Iteration 23/25 | Loss: 0.00119214
Iteration 24/25 | Loss: 0.00119214
Iteration 25/25 | Loss: 0.00119214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.24095225
Iteration 2/25 | Loss: 0.00148219
Iteration 3/25 | Loss: 0.00147249
Iteration 4/25 | Loss: 0.00147249
Iteration 5/25 | Loss: 0.00147248
Iteration 6/25 | Loss: 0.00147248
Iteration 7/25 | Loss: 0.00147248
Iteration 8/25 | Loss: 0.00147248
Iteration 9/25 | Loss: 0.00147248
Iteration 10/25 | Loss: 0.00147248
Iteration 11/25 | Loss: 0.00147248
Iteration 12/25 | Loss: 0.00147248
Iteration 13/25 | Loss: 0.00147248
Iteration 14/25 | Loss: 0.00147248
Iteration 15/25 | Loss: 0.00147248
Iteration 16/25 | Loss: 0.00147248
Iteration 17/25 | Loss: 0.00147248
Iteration 18/25 | Loss: 0.00147248
Iteration 19/25 | Loss: 0.00147248
Iteration 20/25 | Loss: 0.00147248
Iteration 21/25 | Loss: 0.00147248
Iteration 22/25 | Loss: 0.00147248
Iteration 23/25 | Loss: 0.00147248
Iteration 24/25 | Loss: 0.00147248
Iteration 25/25 | Loss: 0.00147248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147248
Iteration 2/1000 | Loss: 0.00001719
Iteration 3/1000 | Loss: 0.00001335
Iteration 4/1000 | Loss: 0.00001236
Iteration 5/1000 | Loss: 0.00001179
Iteration 6/1000 | Loss: 0.00001127
Iteration 7/1000 | Loss: 0.00001102
Iteration 8/1000 | Loss: 0.00001064
Iteration 9/1000 | Loss: 0.00001041
Iteration 10/1000 | Loss: 0.00001031
Iteration 11/1000 | Loss: 0.00001017
Iteration 12/1000 | Loss: 0.00001012
Iteration 13/1000 | Loss: 0.00001012
Iteration 14/1000 | Loss: 0.00001004
Iteration 15/1000 | Loss: 0.00001003
Iteration 16/1000 | Loss: 0.00001003
Iteration 17/1000 | Loss: 0.00000998
Iteration 18/1000 | Loss: 0.00000997
Iteration 19/1000 | Loss: 0.00000997
Iteration 20/1000 | Loss: 0.00000996
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000994
Iteration 23/1000 | Loss: 0.00000990
Iteration 24/1000 | Loss: 0.00000986
Iteration 25/1000 | Loss: 0.00000978
Iteration 26/1000 | Loss: 0.00000977
Iteration 27/1000 | Loss: 0.00000975
Iteration 28/1000 | Loss: 0.00000973
Iteration 29/1000 | Loss: 0.00000972
Iteration 30/1000 | Loss: 0.00000972
Iteration 31/1000 | Loss: 0.00000972
Iteration 32/1000 | Loss: 0.00000971
Iteration 33/1000 | Loss: 0.00000971
Iteration 34/1000 | Loss: 0.00000971
Iteration 35/1000 | Loss: 0.00000970
Iteration 36/1000 | Loss: 0.00000970
Iteration 37/1000 | Loss: 0.00000970
Iteration 38/1000 | Loss: 0.00000969
Iteration 39/1000 | Loss: 0.00000969
Iteration 40/1000 | Loss: 0.00000968
Iteration 41/1000 | Loss: 0.00000967
Iteration 42/1000 | Loss: 0.00000967
Iteration 43/1000 | Loss: 0.00000967
Iteration 44/1000 | Loss: 0.00000967
Iteration 45/1000 | Loss: 0.00000966
Iteration 46/1000 | Loss: 0.00000966
Iteration 47/1000 | Loss: 0.00000966
Iteration 48/1000 | Loss: 0.00000966
Iteration 49/1000 | Loss: 0.00000966
Iteration 50/1000 | Loss: 0.00000965
Iteration 51/1000 | Loss: 0.00000965
Iteration 52/1000 | Loss: 0.00000965
Iteration 53/1000 | Loss: 0.00000965
Iteration 54/1000 | Loss: 0.00000965
Iteration 55/1000 | Loss: 0.00000965
Iteration 56/1000 | Loss: 0.00000965
Iteration 57/1000 | Loss: 0.00000965
Iteration 58/1000 | Loss: 0.00000965
Iteration 59/1000 | Loss: 0.00000965
Iteration 60/1000 | Loss: 0.00000964
Iteration 61/1000 | Loss: 0.00000964
Iteration 62/1000 | Loss: 0.00000964
Iteration 63/1000 | Loss: 0.00000964
Iteration 64/1000 | Loss: 0.00000963
Iteration 65/1000 | Loss: 0.00000963
Iteration 66/1000 | Loss: 0.00000963
Iteration 67/1000 | Loss: 0.00000963
Iteration 68/1000 | Loss: 0.00000963
Iteration 69/1000 | Loss: 0.00000962
Iteration 70/1000 | Loss: 0.00000962
Iteration 71/1000 | Loss: 0.00000962
Iteration 72/1000 | Loss: 0.00000962
Iteration 73/1000 | Loss: 0.00000961
Iteration 74/1000 | Loss: 0.00000961
Iteration 75/1000 | Loss: 0.00000961
Iteration 76/1000 | Loss: 0.00000961
Iteration 77/1000 | Loss: 0.00000960
Iteration 78/1000 | Loss: 0.00000960
Iteration 79/1000 | Loss: 0.00000960
Iteration 80/1000 | Loss: 0.00000959
Iteration 81/1000 | Loss: 0.00000959
Iteration 82/1000 | Loss: 0.00000959
Iteration 83/1000 | Loss: 0.00000959
Iteration 84/1000 | Loss: 0.00000959
Iteration 85/1000 | Loss: 0.00000958
Iteration 86/1000 | Loss: 0.00000958
Iteration 87/1000 | Loss: 0.00000958
Iteration 88/1000 | Loss: 0.00000957
Iteration 89/1000 | Loss: 0.00000957
Iteration 90/1000 | Loss: 0.00000957
Iteration 91/1000 | Loss: 0.00000957
Iteration 92/1000 | Loss: 0.00000956
Iteration 93/1000 | Loss: 0.00000956
Iteration 94/1000 | Loss: 0.00000956
Iteration 95/1000 | Loss: 0.00000956
Iteration 96/1000 | Loss: 0.00000955
Iteration 97/1000 | Loss: 0.00000954
Iteration 98/1000 | Loss: 0.00000954
Iteration 99/1000 | Loss: 0.00000954
Iteration 100/1000 | Loss: 0.00000953
Iteration 101/1000 | Loss: 0.00000953
Iteration 102/1000 | Loss: 0.00000953
Iteration 103/1000 | Loss: 0.00000953
Iteration 104/1000 | Loss: 0.00000953
Iteration 105/1000 | Loss: 0.00000953
Iteration 106/1000 | Loss: 0.00000953
Iteration 107/1000 | Loss: 0.00000953
Iteration 108/1000 | Loss: 0.00000953
Iteration 109/1000 | Loss: 0.00000953
Iteration 110/1000 | Loss: 0.00000953
Iteration 111/1000 | Loss: 0.00000953
Iteration 112/1000 | Loss: 0.00000953
Iteration 113/1000 | Loss: 0.00000953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [9.526551366434433e-06, 9.526551366434433e-06, 9.526551366434433e-06, 9.526551366434433e-06, 9.526551366434433e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.526551366434433e-06

Optimization complete. Final v2v error: 2.685058355331421 mm

Highest mean error: 3.0490570068359375 mm for frame 94

Lowest mean error: 2.4720821380615234 mm for frame 20

Saving results

Total time: 53.35361409187317
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416172
Iteration 2/25 | Loss: 0.00126927
Iteration 3/25 | Loss: 0.00120679
Iteration 4/25 | Loss: 0.00119763
Iteration 5/25 | Loss: 0.00119548
Iteration 6/25 | Loss: 0.00119548
Iteration 7/25 | Loss: 0.00119548
Iteration 8/25 | Loss: 0.00119548
Iteration 9/25 | Loss: 0.00119548
Iteration 10/25 | Loss: 0.00119548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011954766232520342, 0.0011954766232520342, 0.0011954766232520342, 0.0011954766232520342, 0.0011954766232520342]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011954766232520342

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40172267
Iteration 2/25 | Loss: 0.00128437
Iteration 3/25 | Loss: 0.00128437
Iteration 4/25 | Loss: 0.00128437
Iteration 5/25 | Loss: 0.00128437
Iteration 6/25 | Loss: 0.00128437
Iteration 7/25 | Loss: 0.00128437
Iteration 8/25 | Loss: 0.00128437
Iteration 9/25 | Loss: 0.00128437
Iteration 10/25 | Loss: 0.00128437
Iteration 11/25 | Loss: 0.00128437
Iteration 12/25 | Loss: 0.00128437
Iteration 13/25 | Loss: 0.00128437
Iteration 14/25 | Loss: 0.00128437
Iteration 15/25 | Loss: 0.00128437
Iteration 16/25 | Loss: 0.00128437
Iteration 17/25 | Loss: 0.00128437
Iteration 18/25 | Loss: 0.00128437
Iteration 19/25 | Loss: 0.00128437
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012843655422329903, 0.0012843655422329903, 0.0012843655422329903, 0.0012843655422329903, 0.0012843655422329903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012843655422329903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128437
Iteration 2/1000 | Loss: 0.00002117
Iteration 3/1000 | Loss: 0.00001748
Iteration 4/1000 | Loss: 0.00001624
Iteration 5/1000 | Loss: 0.00001514
Iteration 6/1000 | Loss: 0.00001450
Iteration 7/1000 | Loss: 0.00001408
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001308
Iteration 11/1000 | Loss: 0.00001285
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001263
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001235
Iteration 16/1000 | Loss: 0.00001227
Iteration 17/1000 | Loss: 0.00001226
Iteration 18/1000 | Loss: 0.00001226
Iteration 19/1000 | Loss: 0.00001220
Iteration 20/1000 | Loss: 0.00001219
Iteration 21/1000 | Loss: 0.00001219
Iteration 22/1000 | Loss: 0.00001219
Iteration 23/1000 | Loss: 0.00001218
Iteration 24/1000 | Loss: 0.00001218
Iteration 25/1000 | Loss: 0.00001218
Iteration 26/1000 | Loss: 0.00001217
Iteration 27/1000 | Loss: 0.00001216
Iteration 28/1000 | Loss: 0.00001215
Iteration 29/1000 | Loss: 0.00001210
Iteration 30/1000 | Loss: 0.00001204
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001193
Iteration 36/1000 | Loss: 0.00001192
Iteration 37/1000 | Loss: 0.00001192
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001191
Iteration 41/1000 | Loss: 0.00001191
Iteration 42/1000 | Loss: 0.00001190
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001190
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001185
Iteration 50/1000 | Loss: 0.00001185
Iteration 51/1000 | Loss: 0.00001185
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001183
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001181
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001181
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001179
Iteration 66/1000 | Loss: 0.00001179
Iteration 67/1000 | Loss: 0.00001178
Iteration 68/1000 | Loss: 0.00001178
Iteration 69/1000 | Loss: 0.00001177
Iteration 70/1000 | Loss: 0.00001177
Iteration 71/1000 | Loss: 0.00001177
Iteration 72/1000 | Loss: 0.00001177
Iteration 73/1000 | Loss: 0.00001176
Iteration 74/1000 | Loss: 0.00001176
Iteration 75/1000 | Loss: 0.00001176
Iteration 76/1000 | Loss: 0.00001176
Iteration 77/1000 | Loss: 0.00001176
Iteration 78/1000 | Loss: 0.00001176
Iteration 79/1000 | Loss: 0.00001175
Iteration 80/1000 | Loss: 0.00001175
Iteration 81/1000 | Loss: 0.00001175
Iteration 82/1000 | Loss: 0.00001175
Iteration 83/1000 | Loss: 0.00001174
Iteration 84/1000 | Loss: 0.00001174
Iteration 85/1000 | Loss: 0.00001174
Iteration 86/1000 | Loss: 0.00001174
Iteration 87/1000 | Loss: 0.00001174
Iteration 88/1000 | Loss: 0.00001174
Iteration 89/1000 | Loss: 0.00001174
Iteration 90/1000 | Loss: 0.00001174
Iteration 91/1000 | Loss: 0.00001174
Iteration 92/1000 | Loss: 0.00001174
Iteration 93/1000 | Loss: 0.00001174
Iteration 94/1000 | Loss: 0.00001174
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.1740563422790729e-05, 1.1740563422790729e-05, 1.1740563422790729e-05, 1.1740563422790729e-05, 1.1740563422790729e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1740563422790729e-05

Optimization complete. Final v2v error: 2.9897923469543457 mm

Highest mean error: 3.159860849380493 mm for frame 68

Lowest mean error: 2.831446647644043 mm for frame 11

Saving results

Total time: 42.20452666282654
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00993853
Iteration 2/25 | Loss: 0.00196837
Iteration 3/25 | Loss: 0.00141886
Iteration 4/25 | Loss: 0.00134247
Iteration 5/25 | Loss: 0.00134697
Iteration 6/25 | Loss: 0.00130588
Iteration 7/25 | Loss: 0.00128443
Iteration 8/25 | Loss: 0.00128821
Iteration 9/25 | Loss: 0.00127789
Iteration 10/25 | Loss: 0.00128991
Iteration 11/25 | Loss: 0.00128423
Iteration 12/25 | Loss: 0.00128180
Iteration 13/25 | Loss: 0.00127443
Iteration 14/25 | Loss: 0.00126196
Iteration 15/25 | Loss: 0.00125724
Iteration 16/25 | Loss: 0.00125200
Iteration 17/25 | Loss: 0.00125153
Iteration 18/25 | Loss: 0.00124853
Iteration 19/25 | Loss: 0.00124503
Iteration 20/25 | Loss: 0.00124294
Iteration 21/25 | Loss: 0.00124120
Iteration 22/25 | Loss: 0.00124015
Iteration 23/25 | Loss: 0.00123984
Iteration 24/25 | Loss: 0.00123977
Iteration 25/25 | Loss: 0.00123977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30135500
Iteration 2/25 | Loss: 0.00143870
Iteration 3/25 | Loss: 0.00143870
Iteration 4/25 | Loss: 0.00143870
Iteration 5/25 | Loss: 0.00143870
Iteration 6/25 | Loss: 0.00143870
Iteration 7/25 | Loss: 0.00143870
Iteration 8/25 | Loss: 0.00143870
Iteration 9/25 | Loss: 0.00143870
Iteration 10/25 | Loss: 0.00143870
Iteration 11/25 | Loss: 0.00143870
Iteration 12/25 | Loss: 0.00143870
Iteration 13/25 | Loss: 0.00143870
Iteration 14/25 | Loss: 0.00143870
Iteration 15/25 | Loss: 0.00143870
Iteration 16/25 | Loss: 0.00143870
Iteration 17/25 | Loss: 0.00143870
Iteration 18/25 | Loss: 0.00143870
Iteration 19/25 | Loss: 0.00143870
Iteration 20/25 | Loss: 0.00143870
Iteration 21/25 | Loss: 0.00143870
Iteration 22/25 | Loss: 0.00143870
Iteration 23/25 | Loss: 0.00143870
Iteration 24/25 | Loss: 0.00143870
Iteration 25/25 | Loss: 0.00143870

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143870
Iteration 2/1000 | Loss: 0.00004118
Iteration 3/1000 | Loss: 0.00026833
Iteration 4/1000 | Loss: 0.00021247
Iteration 5/1000 | Loss: 0.00027389
Iteration 6/1000 | Loss: 0.00025241
Iteration 7/1000 | Loss: 0.00021869
Iteration 8/1000 | Loss: 0.00003576
Iteration 9/1000 | Loss: 0.00003620
Iteration 10/1000 | Loss: 0.00011361
Iteration 11/1000 | Loss: 0.00012184
Iteration 12/1000 | Loss: 0.00006829
Iteration 13/1000 | Loss: 0.00002072
Iteration 14/1000 | Loss: 0.00028032
Iteration 15/1000 | Loss: 0.00014883
Iteration 16/1000 | Loss: 0.00025554
Iteration 17/1000 | Loss: 0.00014094
Iteration 18/1000 | Loss: 0.00001907
Iteration 19/1000 | Loss: 0.00028642
Iteration 20/1000 | Loss: 0.00012783
Iteration 21/1000 | Loss: 0.00006737
Iteration 22/1000 | Loss: 0.00003156
Iteration 23/1000 | Loss: 0.00008920
Iteration 24/1000 | Loss: 0.00011652
Iteration 25/1000 | Loss: 0.00002755
Iteration 26/1000 | Loss: 0.00013895
Iteration 27/1000 | Loss: 0.00002428
Iteration 28/1000 | Loss: 0.00003833
Iteration 29/1000 | Loss: 0.00002270
Iteration 30/1000 | Loss: 0.00001636
Iteration 31/1000 | Loss: 0.00012658
Iteration 32/1000 | Loss: 0.00002731
Iteration 33/1000 | Loss: 0.00001569
Iteration 34/1000 | Loss: 0.00001542
Iteration 35/1000 | Loss: 0.00001519
Iteration 36/1000 | Loss: 0.00001518
Iteration 37/1000 | Loss: 0.00001517
Iteration 38/1000 | Loss: 0.00001517
Iteration 39/1000 | Loss: 0.00001514
Iteration 40/1000 | Loss: 0.00001513
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001502
Iteration 43/1000 | Loss: 0.00001501
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001496
Iteration 47/1000 | Loss: 0.00001491
Iteration 48/1000 | Loss: 0.00001487
Iteration 49/1000 | Loss: 0.00001479
Iteration 50/1000 | Loss: 0.00001475
Iteration 51/1000 | Loss: 0.00001474
Iteration 52/1000 | Loss: 0.00001474
Iteration 53/1000 | Loss: 0.00001473
Iteration 54/1000 | Loss: 0.00001473
Iteration 55/1000 | Loss: 0.00001472
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001469
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001464
Iteration 62/1000 | Loss: 0.00001463
Iteration 63/1000 | Loss: 0.00001463
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001460
Iteration 66/1000 | Loss: 0.00001459
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001457
Iteration 69/1000 | Loss: 0.00001457
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001455
Iteration 74/1000 | Loss: 0.00001454
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001453
Iteration 78/1000 | Loss: 0.00001453
Iteration 79/1000 | Loss: 0.00001453
Iteration 80/1000 | Loss: 0.00001453
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001453
Iteration 83/1000 | Loss: 0.00001453
Iteration 84/1000 | Loss: 0.00001453
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001452
Iteration 88/1000 | Loss: 0.00001452
Iteration 89/1000 | Loss: 0.00001452
Iteration 90/1000 | Loss: 0.00001452
Iteration 91/1000 | Loss: 0.00001452
Iteration 92/1000 | Loss: 0.00001452
Iteration 93/1000 | Loss: 0.00001451
Iteration 94/1000 | Loss: 0.00001450
Iteration 95/1000 | Loss: 0.00001450
Iteration 96/1000 | Loss: 0.00001450
Iteration 97/1000 | Loss: 0.00001450
Iteration 98/1000 | Loss: 0.00001449
Iteration 99/1000 | Loss: 0.00001449
Iteration 100/1000 | Loss: 0.00001449
Iteration 101/1000 | Loss: 0.00001449
Iteration 102/1000 | Loss: 0.00001448
Iteration 103/1000 | Loss: 0.00001448
Iteration 104/1000 | Loss: 0.00001448
Iteration 105/1000 | Loss: 0.00001448
Iteration 106/1000 | Loss: 0.00001448
Iteration 107/1000 | Loss: 0.00001448
Iteration 108/1000 | Loss: 0.00001447
Iteration 109/1000 | Loss: 0.00001447
Iteration 110/1000 | Loss: 0.00001447
Iteration 111/1000 | Loss: 0.00001447
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001447
Iteration 115/1000 | Loss: 0.00001447
Iteration 116/1000 | Loss: 0.00001447
Iteration 117/1000 | Loss: 0.00001447
Iteration 118/1000 | Loss: 0.00001446
Iteration 119/1000 | Loss: 0.00001446
Iteration 120/1000 | Loss: 0.00001446
Iteration 121/1000 | Loss: 0.00001446
Iteration 122/1000 | Loss: 0.00001446
Iteration 123/1000 | Loss: 0.00001446
Iteration 124/1000 | Loss: 0.00001446
Iteration 125/1000 | Loss: 0.00001446
Iteration 126/1000 | Loss: 0.00001445
Iteration 127/1000 | Loss: 0.00001445
Iteration 128/1000 | Loss: 0.00001445
Iteration 129/1000 | Loss: 0.00001445
Iteration 130/1000 | Loss: 0.00001445
Iteration 131/1000 | Loss: 0.00001445
Iteration 132/1000 | Loss: 0.00001445
Iteration 133/1000 | Loss: 0.00001444
Iteration 134/1000 | Loss: 0.00001444
Iteration 135/1000 | Loss: 0.00001444
Iteration 136/1000 | Loss: 0.00001444
Iteration 137/1000 | Loss: 0.00001444
Iteration 138/1000 | Loss: 0.00001444
Iteration 139/1000 | Loss: 0.00001444
Iteration 140/1000 | Loss: 0.00001444
Iteration 141/1000 | Loss: 0.00001444
Iteration 142/1000 | Loss: 0.00001444
Iteration 143/1000 | Loss: 0.00001444
Iteration 144/1000 | Loss: 0.00001444
Iteration 145/1000 | Loss: 0.00001443
Iteration 146/1000 | Loss: 0.00001443
Iteration 147/1000 | Loss: 0.00001443
Iteration 148/1000 | Loss: 0.00001443
Iteration 149/1000 | Loss: 0.00001443
Iteration 150/1000 | Loss: 0.00001443
Iteration 151/1000 | Loss: 0.00001443
Iteration 152/1000 | Loss: 0.00001443
Iteration 153/1000 | Loss: 0.00001443
Iteration 154/1000 | Loss: 0.00001443
Iteration 155/1000 | Loss: 0.00001443
Iteration 156/1000 | Loss: 0.00001442
Iteration 157/1000 | Loss: 0.00001442
Iteration 158/1000 | Loss: 0.00001442
Iteration 159/1000 | Loss: 0.00001442
Iteration 160/1000 | Loss: 0.00001442
Iteration 161/1000 | Loss: 0.00001441
Iteration 162/1000 | Loss: 0.00001441
Iteration 163/1000 | Loss: 0.00001441
Iteration 164/1000 | Loss: 0.00001441
Iteration 165/1000 | Loss: 0.00001441
Iteration 166/1000 | Loss: 0.00001441
Iteration 167/1000 | Loss: 0.00001440
Iteration 168/1000 | Loss: 0.00001440
Iteration 169/1000 | Loss: 0.00001440
Iteration 170/1000 | Loss: 0.00001440
Iteration 171/1000 | Loss: 0.00001440
Iteration 172/1000 | Loss: 0.00001440
Iteration 173/1000 | Loss: 0.00001440
Iteration 174/1000 | Loss: 0.00001440
Iteration 175/1000 | Loss: 0.00001440
Iteration 176/1000 | Loss: 0.00001440
Iteration 177/1000 | Loss: 0.00001440
Iteration 178/1000 | Loss: 0.00001440
Iteration 179/1000 | Loss: 0.00001440
Iteration 180/1000 | Loss: 0.00001440
Iteration 181/1000 | Loss: 0.00001440
Iteration 182/1000 | Loss: 0.00001440
Iteration 183/1000 | Loss: 0.00001439
Iteration 184/1000 | Loss: 0.00001439
Iteration 185/1000 | Loss: 0.00001439
Iteration 186/1000 | Loss: 0.00001439
Iteration 187/1000 | Loss: 0.00001439
Iteration 188/1000 | Loss: 0.00001439
Iteration 189/1000 | Loss: 0.00001439
Iteration 190/1000 | Loss: 0.00001439
Iteration 191/1000 | Loss: 0.00001439
Iteration 192/1000 | Loss: 0.00001439
Iteration 193/1000 | Loss: 0.00001439
Iteration 194/1000 | Loss: 0.00001439
Iteration 195/1000 | Loss: 0.00001439
Iteration 196/1000 | Loss: 0.00001439
Iteration 197/1000 | Loss: 0.00001438
Iteration 198/1000 | Loss: 0.00001438
Iteration 199/1000 | Loss: 0.00001438
Iteration 200/1000 | Loss: 0.00001438
Iteration 201/1000 | Loss: 0.00001438
Iteration 202/1000 | Loss: 0.00001438
Iteration 203/1000 | Loss: 0.00001437
Iteration 204/1000 | Loss: 0.00001437
Iteration 205/1000 | Loss: 0.00001437
Iteration 206/1000 | Loss: 0.00001437
Iteration 207/1000 | Loss: 0.00001437
Iteration 208/1000 | Loss: 0.00001437
Iteration 209/1000 | Loss: 0.00001437
Iteration 210/1000 | Loss: 0.00001437
Iteration 211/1000 | Loss: 0.00001437
Iteration 212/1000 | Loss: 0.00001437
Iteration 213/1000 | Loss: 0.00001437
Iteration 214/1000 | Loss: 0.00001437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.4373735211847816e-05, 1.4373735211847816e-05, 1.4373735211847816e-05, 1.4373735211847816e-05, 1.4373735211847816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4373735211847816e-05

Optimization complete. Final v2v error: 3.036414623260498 mm

Highest mean error: 5.5049214363098145 mm for frame 42

Lowest mean error: 2.6057612895965576 mm for frame 130

Saving results

Total time: 107.91095662117004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804720
Iteration 2/25 | Loss: 0.00132313
Iteration 3/25 | Loss: 0.00123627
Iteration 4/25 | Loss: 0.00122753
Iteration 5/25 | Loss: 0.00122492
Iteration 6/25 | Loss: 0.00122446
Iteration 7/25 | Loss: 0.00122446
Iteration 8/25 | Loss: 0.00122446
Iteration 9/25 | Loss: 0.00122446
Iteration 10/25 | Loss: 0.00122446
Iteration 11/25 | Loss: 0.00122446
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001224464038386941, 0.001224464038386941, 0.001224464038386941, 0.001224464038386941, 0.001224464038386941]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001224464038386941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.30966282
Iteration 2/25 | Loss: 0.00144461
Iteration 3/25 | Loss: 0.00144461
Iteration 4/25 | Loss: 0.00144461
Iteration 5/25 | Loss: 0.00144460
Iteration 6/25 | Loss: 0.00144460
Iteration 7/25 | Loss: 0.00144460
Iteration 8/25 | Loss: 0.00144460
Iteration 9/25 | Loss: 0.00144460
Iteration 10/25 | Loss: 0.00144460
Iteration 11/25 | Loss: 0.00144460
Iteration 12/25 | Loss: 0.00144460
Iteration 13/25 | Loss: 0.00144460
Iteration 14/25 | Loss: 0.00144460
Iteration 15/25 | Loss: 0.00144460
Iteration 16/25 | Loss: 0.00144460
Iteration 17/25 | Loss: 0.00144460
Iteration 18/25 | Loss: 0.00144460
Iteration 19/25 | Loss: 0.00144460
Iteration 20/25 | Loss: 0.00144460
Iteration 21/25 | Loss: 0.00144460
Iteration 22/25 | Loss: 0.00144460
Iteration 23/25 | Loss: 0.00144460
Iteration 24/25 | Loss: 0.00144460
Iteration 25/25 | Loss: 0.00144460

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144460
Iteration 2/1000 | Loss: 0.00002929
Iteration 3/1000 | Loss: 0.00002021
Iteration 4/1000 | Loss: 0.00001684
Iteration 5/1000 | Loss: 0.00001530
Iteration 6/1000 | Loss: 0.00001455
Iteration 7/1000 | Loss: 0.00001391
Iteration 8/1000 | Loss: 0.00001335
Iteration 9/1000 | Loss: 0.00001306
Iteration 10/1000 | Loss: 0.00001279
Iteration 11/1000 | Loss: 0.00001254
Iteration 12/1000 | Loss: 0.00001236
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001209
Iteration 15/1000 | Loss: 0.00001208
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001203
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001197
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001185
Iteration 22/1000 | Loss: 0.00001183
Iteration 23/1000 | Loss: 0.00001182
Iteration 24/1000 | Loss: 0.00001182
Iteration 25/1000 | Loss: 0.00001181
Iteration 26/1000 | Loss: 0.00001181
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001180
Iteration 29/1000 | Loss: 0.00001179
Iteration 30/1000 | Loss: 0.00001178
Iteration 31/1000 | Loss: 0.00001178
Iteration 32/1000 | Loss: 0.00001174
Iteration 33/1000 | Loss: 0.00001174
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001171
Iteration 37/1000 | Loss: 0.00001167
Iteration 38/1000 | Loss: 0.00001166
Iteration 39/1000 | Loss: 0.00001165
Iteration 40/1000 | Loss: 0.00001165
Iteration 41/1000 | Loss: 0.00001164
Iteration 42/1000 | Loss: 0.00001163
Iteration 43/1000 | Loss: 0.00001162
Iteration 44/1000 | Loss: 0.00001162
Iteration 45/1000 | Loss: 0.00001162
Iteration 46/1000 | Loss: 0.00001161
Iteration 47/1000 | Loss: 0.00001161
Iteration 48/1000 | Loss: 0.00001161
Iteration 49/1000 | Loss: 0.00001160
Iteration 50/1000 | Loss: 0.00001160
Iteration 51/1000 | Loss: 0.00001159
Iteration 52/1000 | Loss: 0.00001159
Iteration 53/1000 | Loss: 0.00001159
Iteration 54/1000 | Loss: 0.00001158
Iteration 55/1000 | Loss: 0.00001158
Iteration 56/1000 | Loss: 0.00001158
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001156
Iteration 61/1000 | Loss: 0.00001156
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001155
Iteration 64/1000 | Loss: 0.00001155
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001154
Iteration 67/1000 | Loss: 0.00001154
Iteration 68/1000 | Loss: 0.00001153
Iteration 69/1000 | Loss: 0.00001153
Iteration 70/1000 | Loss: 0.00001153
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001151
Iteration 75/1000 | Loss: 0.00001151
Iteration 76/1000 | Loss: 0.00001150
Iteration 77/1000 | Loss: 0.00001150
Iteration 78/1000 | Loss: 0.00001149
Iteration 79/1000 | Loss: 0.00001149
Iteration 80/1000 | Loss: 0.00001149
Iteration 81/1000 | Loss: 0.00001149
Iteration 82/1000 | Loss: 0.00001149
Iteration 83/1000 | Loss: 0.00001149
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001148
Iteration 88/1000 | Loss: 0.00001148
Iteration 89/1000 | Loss: 0.00001148
Iteration 90/1000 | Loss: 0.00001148
Iteration 91/1000 | Loss: 0.00001148
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001147
Iteration 94/1000 | Loss: 0.00001147
Iteration 95/1000 | Loss: 0.00001147
Iteration 96/1000 | Loss: 0.00001147
Iteration 97/1000 | Loss: 0.00001146
Iteration 98/1000 | Loss: 0.00001146
Iteration 99/1000 | Loss: 0.00001146
Iteration 100/1000 | Loss: 0.00001146
Iteration 101/1000 | Loss: 0.00001145
Iteration 102/1000 | Loss: 0.00001145
Iteration 103/1000 | Loss: 0.00001145
Iteration 104/1000 | Loss: 0.00001145
Iteration 105/1000 | Loss: 0.00001145
Iteration 106/1000 | Loss: 0.00001145
Iteration 107/1000 | Loss: 0.00001145
Iteration 108/1000 | Loss: 0.00001145
Iteration 109/1000 | Loss: 0.00001145
Iteration 110/1000 | Loss: 0.00001145
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001145
Iteration 113/1000 | Loss: 0.00001145
Iteration 114/1000 | Loss: 0.00001145
Iteration 115/1000 | Loss: 0.00001145
Iteration 116/1000 | Loss: 0.00001145
Iteration 117/1000 | Loss: 0.00001145
Iteration 118/1000 | Loss: 0.00001145
Iteration 119/1000 | Loss: 0.00001145
Iteration 120/1000 | Loss: 0.00001145
Iteration 121/1000 | Loss: 0.00001145
Iteration 122/1000 | Loss: 0.00001145
Iteration 123/1000 | Loss: 0.00001145
Iteration 124/1000 | Loss: 0.00001145
Iteration 125/1000 | Loss: 0.00001145
Iteration 126/1000 | Loss: 0.00001145
Iteration 127/1000 | Loss: 0.00001145
Iteration 128/1000 | Loss: 0.00001145
Iteration 129/1000 | Loss: 0.00001145
Iteration 130/1000 | Loss: 0.00001145
Iteration 131/1000 | Loss: 0.00001145
Iteration 132/1000 | Loss: 0.00001145
Iteration 133/1000 | Loss: 0.00001145
Iteration 134/1000 | Loss: 0.00001145
Iteration 135/1000 | Loss: 0.00001145
Iteration 136/1000 | Loss: 0.00001145
Iteration 137/1000 | Loss: 0.00001145
Iteration 138/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.1450845704530366e-05, 1.1450845704530366e-05, 1.1450845704530366e-05, 1.1450845704530366e-05, 1.1450845704530366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1450845704530366e-05

Optimization complete. Final v2v error: 2.900601625442505 mm

Highest mean error: 3.9378647804260254 mm for frame 86

Lowest mean error: 2.6154661178588867 mm for frame 114

Saving results

Total time: 40.52012538909912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793710
Iteration 2/25 | Loss: 0.00127482
Iteration 3/25 | Loss: 0.00119754
Iteration 4/25 | Loss: 0.00118873
Iteration 5/25 | Loss: 0.00118711
Iteration 6/25 | Loss: 0.00118711
Iteration 7/25 | Loss: 0.00118711
Iteration 8/25 | Loss: 0.00118711
Iteration 9/25 | Loss: 0.00118711
Iteration 10/25 | Loss: 0.00118711
Iteration 11/25 | Loss: 0.00118711
Iteration 12/25 | Loss: 0.00118711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011871142778545618, 0.0011871142778545618, 0.0011871142778545618, 0.0011871142778545618, 0.0011871142778545618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011871142778545618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29279852
Iteration 2/25 | Loss: 0.00142383
Iteration 3/25 | Loss: 0.00142383
Iteration 4/25 | Loss: 0.00142383
Iteration 5/25 | Loss: 0.00142383
Iteration 6/25 | Loss: 0.00142383
Iteration 7/25 | Loss: 0.00142383
Iteration 8/25 | Loss: 0.00142383
Iteration 9/25 | Loss: 0.00142383
Iteration 10/25 | Loss: 0.00142383
Iteration 11/25 | Loss: 0.00142383
Iteration 12/25 | Loss: 0.00142383
Iteration 13/25 | Loss: 0.00142383
Iteration 14/25 | Loss: 0.00142383
Iteration 15/25 | Loss: 0.00142383
Iteration 16/25 | Loss: 0.00142383
Iteration 17/25 | Loss: 0.00142383
Iteration 18/25 | Loss: 0.00142383
Iteration 19/25 | Loss: 0.00142383
Iteration 20/25 | Loss: 0.00142383
Iteration 21/25 | Loss: 0.00142383
Iteration 22/25 | Loss: 0.00142383
Iteration 23/25 | Loss: 0.00142383
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001423825160600245, 0.001423825160600245, 0.001423825160600245, 0.001423825160600245, 0.001423825160600245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001423825160600245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142383
Iteration 2/1000 | Loss: 0.00002243
Iteration 3/1000 | Loss: 0.00001419
Iteration 4/1000 | Loss: 0.00001249
Iteration 5/1000 | Loss: 0.00001168
Iteration 6/1000 | Loss: 0.00001103
Iteration 7/1000 | Loss: 0.00001056
Iteration 8/1000 | Loss: 0.00001028
Iteration 9/1000 | Loss: 0.00001003
Iteration 10/1000 | Loss: 0.00000970
Iteration 11/1000 | Loss: 0.00000965
Iteration 12/1000 | Loss: 0.00000964
Iteration 13/1000 | Loss: 0.00000956
Iteration 14/1000 | Loss: 0.00000955
Iteration 15/1000 | Loss: 0.00000953
Iteration 16/1000 | Loss: 0.00000942
Iteration 17/1000 | Loss: 0.00000940
Iteration 18/1000 | Loss: 0.00000939
Iteration 19/1000 | Loss: 0.00000933
Iteration 20/1000 | Loss: 0.00000933
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000930
Iteration 23/1000 | Loss: 0.00000930
Iteration 24/1000 | Loss: 0.00000930
Iteration 25/1000 | Loss: 0.00000930
Iteration 26/1000 | Loss: 0.00000928
Iteration 27/1000 | Loss: 0.00000924
Iteration 28/1000 | Loss: 0.00000924
Iteration 29/1000 | Loss: 0.00000924
Iteration 30/1000 | Loss: 0.00000923
Iteration 31/1000 | Loss: 0.00000922
Iteration 32/1000 | Loss: 0.00000918
Iteration 33/1000 | Loss: 0.00000918
Iteration 34/1000 | Loss: 0.00000917
Iteration 35/1000 | Loss: 0.00000917
Iteration 36/1000 | Loss: 0.00000915
Iteration 37/1000 | Loss: 0.00000914
Iteration 38/1000 | Loss: 0.00000913
Iteration 39/1000 | Loss: 0.00000913
Iteration 40/1000 | Loss: 0.00000913
Iteration 41/1000 | Loss: 0.00000912
Iteration 42/1000 | Loss: 0.00000912
Iteration 43/1000 | Loss: 0.00000903
Iteration 44/1000 | Loss: 0.00000902
Iteration 45/1000 | Loss: 0.00000900
Iteration 46/1000 | Loss: 0.00000899
Iteration 47/1000 | Loss: 0.00000898
Iteration 48/1000 | Loss: 0.00000898
Iteration 49/1000 | Loss: 0.00000898
Iteration 50/1000 | Loss: 0.00000897
Iteration 51/1000 | Loss: 0.00000897
Iteration 52/1000 | Loss: 0.00000897
Iteration 53/1000 | Loss: 0.00000897
Iteration 54/1000 | Loss: 0.00000897
Iteration 55/1000 | Loss: 0.00000897
Iteration 56/1000 | Loss: 0.00000897
Iteration 57/1000 | Loss: 0.00000897
Iteration 58/1000 | Loss: 0.00000897
Iteration 59/1000 | Loss: 0.00000897
Iteration 60/1000 | Loss: 0.00000896
Iteration 61/1000 | Loss: 0.00000896
Iteration 62/1000 | Loss: 0.00000896
Iteration 63/1000 | Loss: 0.00000896
Iteration 64/1000 | Loss: 0.00000896
Iteration 65/1000 | Loss: 0.00000895
Iteration 66/1000 | Loss: 0.00000895
Iteration 67/1000 | Loss: 0.00000895
Iteration 68/1000 | Loss: 0.00000895
Iteration 69/1000 | Loss: 0.00000894
Iteration 70/1000 | Loss: 0.00000894
Iteration 71/1000 | Loss: 0.00000894
Iteration 72/1000 | Loss: 0.00000894
Iteration 73/1000 | Loss: 0.00000893
Iteration 74/1000 | Loss: 0.00000893
Iteration 75/1000 | Loss: 0.00000893
Iteration 76/1000 | Loss: 0.00000892
Iteration 77/1000 | Loss: 0.00000892
Iteration 78/1000 | Loss: 0.00000891
Iteration 79/1000 | Loss: 0.00000891
Iteration 80/1000 | Loss: 0.00000890
Iteration 81/1000 | Loss: 0.00000890
Iteration 82/1000 | Loss: 0.00000889
Iteration 83/1000 | Loss: 0.00000889
Iteration 84/1000 | Loss: 0.00000889
Iteration 85/1000 | Loss: 0.00000889
Iteration 86/1000 | Loss: 0.00000889
Iteration 87/1000 | Loss: 0.00000889
Iteration 88/1000 | Loss: 0.00000889
Iteration 89/1000 | Loss: 0.00000888
Iteration 90/1000 | Loss: 0.00000888
Iteration 91/1000 | Loss: 0.00000888
Iteration 92/1000 | Loss: 0.00000888
Iteration 93/1000 | Loss: 0.00000887
Iteration 94/1000 | Loss: 0.00000887
Iteration 95/1000 | Loss: 0.00000886
Iteration 96/1000 | Loss: 0.00000886
Iteration 97/1000 | Loss: 0.00000886
Iteration 98/1000 | Loss: 0.00000886
Iteration 99/1000 | Loss: 0.00000886
Iteration 100/1000 | Loss: 0.00000886
Iteration 101/1000 | Loss: 0.00000885
Iteration 102/1000 | Loss: 0.00000885
Iteration 103/1000 | Loss: 0.00000885
Iteration 104/1000 | Loss: 0.00000885
Iteration 105/1000 | Loss: 0.00000884
Iteration 106/1000 | Loss: 0.00000884
Iteration 107/1000 | Loss: 0.00000884
Iteration 108/1000 | Loss: 0.00000884
Iteration 109/1000 | Loss: 0.00000884
Iteration 110/1000 | Loss: 0.00000883
Iteration 111/1000 | Loss: 0.00000883
Iteration 112/1000 | Loss: 0.00000883
Iteration 113/1000 | Loss: 0.00000883
Iteration 114/1000 | Loss: 0.00000882
Iteration 115/1000 | Loss: 0.00000882
Iteration 116/1000 | Loss: 0.00000882
Iteration 117/1000 | Loss: 0.00000882
Iteration 118/1000 | Loss: 0.00000882
Iteration 119/1000 | Loss: 0.00000882
Iteration 120/1000 | Loss: 0.00000882
Iteration 121/1000 | Loss: 0.00000881
Iteration 122/1000 | Loss: 0.00000881
Iteration 123/1000 | Loss: 0.00000881
Iteration 124/1000 | Loss: 0.00000881
Iteration 125/1000 | Loss: 0.00000881
Iteration 126/1000 | Loss: 0.00000881
Iteration 127/1000 | Loss: 0.00000881
Iteration 128/1000 | Loss: 0.00000881
Iteration 129/1000 | Loss: 0.00000880
Iteration 130/1000 | Loss: 0.00000880
Iteration 131/1000 | Loss: 0.00000880
Iteration 132/1000 | Loss: 0.00000880
Iteration 133/1000 | Loss: 0.00000880
Iteration 134/1000 | Loss: 0.00000880
Iteration 135/1000 | Loss: 0.00000880
Iteration 136/1000 | Loss: 0.00000880
Iteration 137/1000 | Loss: 0.00000880
Iteration 138/1000 | Loss: 0.00000880
Iteration 139/1000 | Loss: 0.00000880
Iteration 140/1000 | Loss: 0.00000880
Iteration 141/1000 | Loss: 0.00000880
Iteration 142/1000 | Loss: 0.00000880
Iteration 143/1000 | Loss: 0.00000880
Iteration 144/1000 | Loss: 0.00000880
Iteration 145/1000 | Loss: 0.00000880
Iteration 146/1000 | Loss: 0.00000880
Iteration 147/1000 | Loss: 0.00000880
Iteration 148/1000 | Loss: 0.00000880
Iteration 149/1000 | Loss: 0.00000880
Iteration 150/1000 | Loss: 0.00000880
Iteration 151/1000 | Loss: 0.00000880
Iteration 152/1000 | Loss: 0.00000880
Iteration 153/1000 | Loss: 0.00000880
Iteration 154/1000 | Loss: 0.00000880
Iteration 155/1000 | Loss: 0.00000880
Iteration 156/1000 | Loss: 0.00000880
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [8.803087439446244e-06, 8.803087439446244e-06, 8.803087439446244e-06, 8.803087439446244e-06, 8.803087439446244e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.803087439446244e-06

Optimization complete. Final v2v error: 2.549833297729492 mm

Highest mean error: 2.7621231079101562 mm for frame 31

Lowest mean error: 2.4409377574920654 mm for frame 15

Saving results

Total time: 38.16879987716675
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998985
Iteration 2/25 | Loss: 0.00998985
Iteration 3/25 | Loss: 0.00250982
Iteration 4/25 | Loss: 0.00200847
Iteration 5/25 | Loss: 0.00197626
Iteration 6/25 | Loss: 0.00185989
Iteration 7/25 | Loss: 0.00157077
Iteration 8/25 | Loss: 0.00153282
Iteration 9/25 | Loss: 0.00146524
Iteration 10/25 | Loss: 0.00136446
Iteration 11/25 | Loss: 0.00129102
Iteration 12/25 | Loss: 0.00125649
Iteration 13/25 | Loss: 0.00124850
Iteration 14/25 | Loss: 0.00124054
Iteration 15/25 | Loss: 0.00124459
Iteration 16/25 | Loss: 0.00124361
Iteration 17/25 | Loss: 0.00124192
Iteration 18/25 | Loss: 0.00123738
Iteration 19/25 | Loss: 0.00123742
Iteration 20/25 | Loss: 0.00123644
Iteration 21/25 | Loss: 0.00123575
Iteration 22/25 | Loss: 0.00123637
Iteration 23/25 | Loss: 0.00123590
Iteration 24/25 | Loss: 0.00124060
Iteration 25/25 | Loss: 0.00123931

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25442803
Iteration 2/25 | Loss: 0.00103521
Iteration 3/25 | Loss: 0.00103520
Iteration 4/25 | Loss: 0.00103520
Iteration 5/25 | Loss: 0.00103520
Iteration 6/25 | Loss: 0.00103520
Iteration 7/25 | Loss: 0.00103520
Iteration 8/25 | Loss: 0.00103520
Iteration 9/25 | Loss: 0.00103520
Iteration 10/25 | Loss: 0.00103520
Iteration 11/25 | Loss: 0.00103520
Iteration 12/25 | Loss: 0.00103520
Iteration 13/25 | Loss: 0.00103520
Iteration 14/25 | Loss: 0.00103520
Iteration 15/25 | Loss: 0.00103520
Iteration 16/25 | Loss: 0.00103520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010352028766646981, 0.0010352028766646981, 0.0010352028766646981, 0.0010352028766646981, 0.0010352028766646981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010352028766646981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103520
Iteration 2/1000 | Loss: 0.00002823
Iteration 3/1000 | Loss: 0.00002755
Iteration 4/1000 | Loss: 0.00002273
Iteration 5/1000 | Loss: 0.00002759
Iteration 6/1000 | Loss: 0.00002549
Iteration 7/1000 | Loss: 0.00001910
Iteration 8/1000 | Loss: 0.00002470
Iteration 9/1000 | Loss: 0.00002409
Iteration 10/1000 | Loss: 0.00002438
Iteration 11/1000 | Loss: 0.00002220
Iteration 12/1000 | Loss: 0.00002362
Iteration 13/1000 | Loss: 0.00002211
Iteration 14/1000 | Loss: 0.00002492
Iteration 15/1000 | Loss: 0.00002110
Iteration 16/1000 | Loss: 0.00002429
Iteration 17/1000 | Loss: 0.00002552
Iteration 18/1000 | Loss: 0.00002430
Iteration 19/1000 | Loss: 0.00002364
Iteration 20/1000 | Loss: 0.00002184
Iteration 21/1000 | Loss: 0.00002823
Iteration 22/1000 | Loss: 0.00002653
Iteration 23/1000 | Loss: 0.00002795
Iteration 24/1000 | Loss: 0.00002897
Iteration 25/1000 | Loss: 0.00002911
Iteration 26/1000 | Loss: 0.00002803
Iteration 27/1000 | Loss: 0.00002875
Iteration 28/1000 | Loss: 0.00002765
Iteration 29/1000 | Loss: 0.00002867
Iteration 30/1000 | Loss: 0.00002828
Iteration 31/1000 | Loss: 0.00001999
Iteration 32/1000 | Loss: 0.00002612
Iteration 33/1000 | Loss: 0.00002048
Iteration 34/1000 | Loss: 0.00002310
Iteration 35/1000 | Loss: 0.00002931
Iteration 36/1000 | Loss: 0.00002824
Iteration 37/1000 | Loss: 0.00002563
Iteration 38/1000 | Loss: 0.00002649
Iteration 39/1000 | Loss: 0.00002230
Iteration 40/1000 | Loss: 0.00002322
Iteration 41/1000 | Loss: 0.00002872
Iteration 42/1000 | Loss: 0.00002133
Iteration 43/1000 | Loss: 0.00001964
Iteration 44/1000 | Loss: 0.00001769
Iteration 45/1000 | Loss: 0.00001709
Iteration 46/1000 | Loss: 0.00001663
Iteration 47/1000 | Loss: 0.00001635
Iteration 48/1000 | Loss: 0.00002207
Iteration 49/1000 | Loss: 0.00003021
Iteration 50/1000 | Loss: 0.00001809
Iteration 51/1000 | Loss: 0.00002220
Iteration 52/1000 | Loss: 0.00002196
Iteration 53/1000 | Loss: 0.00003388
Iteration 54/1000 | Loss: 0.00002710
Iteration 55/1000 | Loss: 0.00001766
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00002746
Iteration 58/1000 | Loss: 0.00002872
Iteration 59/1000 | Loss: 0.00002767
Iteration 60/1000 | Loss: 0.00002809
Iteration 61/1000 | Loss: 0.00002737
Iteration 62/1000 | Loss: 0.00003552
Iteration 63/1000 | Loss: 0.00002105
Iteration 64/1000 | Loss: 0.00002987
Iteration 65/1000 | Loss: 0.00001853
Iteration 66/1000 | Loss: 0.00003896
Iteration 67/1000 | Loss: 0.00003246
Iteration 68/1000 | Loss: 0.00004179
Iteration 69/1000 | Loss: 0.00001729
Iteration 70/1000 | Loss: 0.00002121
Iteration 71/1000 | Loss: 0.00001751
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00004295
Iteration 76/1000 | Loss: 0.00001717
Iteration 77/1000 | Loss: 0.00002913
Iteration 78/1000 | Loss: 0.00001869
Iteration 79/1000 | Loss: 0.00004116
Iteration 80/1000 | Loss: 0.00001761
Iteration 81/1000 | Loss: 0.00002436
Iteration 82/1000 | Loss: 0.00001746
Iteration 83/1000 | Loss: 0.00002778
Iteration 84/1000 | Loss: 0.00001775
Iteration 85/1000 | Loss: 0.00001643
Iteration 86/1000 | Loss: 0.00003290
Iteration 87/1000 | Loss: 0.00002564
Iteration 88/1000 | Loss: 0.00003204
Iteration 89/1000 | Loss: 0.00001881
Iteration 90/1000 | Loss: 0.00002066
Iteration 91/1000 | Loss: 0.00001956
Iteration 92/1000 | Loss: 0.00002947
Iteration 93/1000 | Loss: 0.00001930
Iteration 94/1000 | Loss: 0.00003620
Iteration 95/1000 | Loss: 0.00002420
Iteration 96/1000 | Loss: 0.00003195
Iteration 97/1000 | Loss: 0.00002480
Iteration 98/1000 | Loss: 0.00003354
Iteration 99/1000 | Loss: 0.00001717
Iteration 100/1000 | Loss: 0.00002620
Iteration 101/1000 | Loss: 0.00003600
Iteration 102/1000 | Loss: 0.00001882
Iteration 103/1000 | Loss: 0.00002770
Iteration 104/1000 | Loss: 0.00001834
Iteration 105/1000 | Loss: 0.00002830
Iteration 106/1000 | Loss: 0.00002636
Iteration 107/1000 | Loss: 0.00002774
Iteration 108/1000 | Loss: 0.00001610
Iteration 109/1000 | Loss: 0.00001609
Iteration 110/1000 | Loss: 0.00001604
Iteration 111/1000 | Loss: 0.00001601
Iteration 112/1000 | Loss: 0.00001601
Iteration 113/1000 | Loss: 0.00001598
Iteration 114/1000 | Loss: 0.00001596
Iteration 115/1000 | Loss: 0.00001596
Iteration 116/1000 | Loss: 0.00001595
Iteration 117/1000 | Loss: 0.00001595
Iteration 118/1000 | Loss: 0.00001595
Iteration 119/1000 | Loss: 0.00001592
Iteration 120/1000 | Loss: 0.00001591
Iteration 121/1000 | Loss: 0.00001591
Iteration 122/1000 | Loss: 0.00001591
Iteration 123/1000 | Loss: 0.00001590
Iteration 124/1000 | Loss: 0.00001589
Iteration 125/1000 | Loss: 0.00001586
Iteration 126/1000 | Loss: 0.00001586
Iteration 127/1000 | Loss: 0.00001585
Iteration 128/1000 | Loss: 0.00001585
Iteration 129/1000 | Loss: 0.00001584
Iteration 130/1000 | Loss: 0.00001583
Iteration 131/1000 | Loss: 0.00001582
Iteration 132/1000 | Loss: 0.00001581
Iteration 133/1000 | Loss: 0.00001581
Iteration 134/1000 | Loss: 0.00001580
Iteration 135/1000 | Loss: 0.00001578
Iteration 136/1000 | Loss: 0.00001578
Iteration 137/1000 | Loss: 0.00001578
Iteration 138/1000 | Loss: 0.00001578
Iteration 139/1000 | Loss: 0.00001578
Iteration 140/1000 | Loss: 0.00001577
Iteration 141/1000 | Loss: 0.00001577
Iteration 142/1000 | Loss: 0.00001577
Iteration 143/1000 | Loss: 0.00001577
Iteration 144/1000 | Loss: 0.00001577
Iteration 145/1000 | Loss: 0.00001577
Iteration 146/1000 | Loss: 0.00001577
Iteration 147/1000 | Loss: 0.00001577
Iteration 148/1000 | Loss: 0.00001576
Iteration 149/1000 | Loss: 0.00001576
Iteration 150/1000 | Loss: 0.00001576
Iteration 151/1000 | Loss: 0.00001576
Iteration 152/1000 | Loss: 0.00001576
Iteration 153/1000 | Loss: 0.00001576
Iteration 154/1000 | Loss: 0.00001576
Iteration 155/1000 | Loss: 0.00001576
Iteration 156/1000 | Loss: 0.00001575
Iteration 157/1000 | Loss: 0.00001575
Iteration 158/1000 | Loss: 0.00001575
Iteration 159/1000 | Loss: 0.00001575
Iteration 160/1000 | Loss: 0.00001575
Iteration 161/1000 | Loss: 0.00001575
Iteration 162/1000 | Loss: 0.00001575
Iteration 163/1000 | Loss: 0.00001575
Iteration 164/1000 | Loss: 0.00001574
Iteration 165/1000 | Loss: 0.00001574
Iteration 166/1000 | Loss: 0.00001574
Iteration 167/1000 | Loss: 0.00001574
Iteration 168/1000 | Loss: 0.00001574
Iteration 169/1000 | Loss: 0.00001574
Iteration 170/1000 | Loss: 0.00001574
Iteration 171/1000 | Loss: 0.00001574
Iteration 172/1000 | Loss: 0.00001574
Iteration 173/1000 | Loss: 0.00001574
Iteration 174/1000 | Loss: 0.00001574
Iteration 175/1000 | Loss: 0.00001573
Iteration 176/1000 | Loss: 0.00001573
Iteration 177/1000 | Loss: 0.00001573
Iteration 178/1000 | Loss: 0.00001573
Iteration 179/1000 | Loss: 0.00001573
Iteration 180/1000 | Loss: 0.00001573
Iteration 181/1000 | Loss: 0.00001573
Iteration 182/1000 | Loss: 0.00001573
Iteration 183/1000 | Loss: 0.00001573
Iteration 184/1000 | Loss: 0.00001573
Iteration 185/1000 | Loss: 0.00001573
Iteration 186/1000 | Loss: 0.00001573
Iteration 187/1000 | Loss: 0.00001573
Iteration 188/1000 | Loss: 0.00001573
Iteration 189/1000 | Loss: 0.00001572
Iteration 190/1000 | Loss: 0.00001572
Iteration 191/1000 | Loss: 0.00001572
Iteration 192/1000 | Loss: 0.00001572
Iteration 193/1000 | Loss: 0.00001572
Iteration 194/1000 | Loss: 0.00001572
Iteration 195/1000 | Loss: 0.00001572
Iteration 196/1000 | Loss: 0.00001572
Iteration 197/1000 | Loss: 0.00001572
Iteration 198/1000 | Loss: 0.00001572
Iteration 199/1000 | Loss: 0.00001572
Iteration 200/1000 | Loss: 0.00001572
Iteration 201/1000 | Loss: 0.00001572
Iteration 202/1000 | Loss: 0.00001572
Iteration 203/1000 | Loss: 0.00001571
Iteration 204/1000 | Loss: 0.00001571
Iteration 205/1000 | Loss: 0.00001571
Iteration 206/1000 | Loss: 0.00001571
Iteration 207/1000 | Loss: 0.00001571
Iteration 208/1000 | Loss: 0.00001571
Iteration 209/1000 | Loss: 0.00001571
Iteration 210/1000 | Loss: 0.00001571
Iteration 211/1000 | Loss: 0.00001571
Iteration 212/1000 | Loss: 0.00001571
Iteration 213/1000 | Loss: 0.00001571
Iteration 214/1000 | Loss: 0.00001571
Iteration 215/1000 | Loss: 0.00001571
Iteration 216/1000 | Loss: 0.00001571
Iteration 217/1000 | Loss: 0.00001571
Iteration 218/1000 | Loss: 0.00001571
Iteration 219/1000 | Loss: 0.00001571
Iteration 220/1000 | Loss: 0.00001571
Iteration 221/1000 | Loss: 0.00001571
Iteration 222/1000 | Loss: 0.00001571
Iteration 223/1000 | Loss: 0.00001571
Iteration 224/1000 | Loss: 0.00001571
Iteration 225/1000 | Loss: 0.00001571
Iteration 226/1000 | Loss: 0.00001571
Iteration 227/1000 | Loss: 0.00001571
Iteration 228/1000 | Loss: 0.00001571
Iteration 229/1000 | Loss: 0.00001571
Iteration 230/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.5711766536696814e-05, 1.5711766536696814e-05, 1.5711766536696814e-05, 1.5711766536696814e-05, 1.5711766536696814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5711766536696814e-05

Optimization complete. Final v2v error: 3.2505857944488525 mm

Highest mean error: 8.568268775939941 mm for frame 50

Lowest mean error: 3.0769524574279785 mm for frame 9

Saving results

Total time: 232.610004901886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601361
Iteration 2/25 | Loss: 0.00171748
Iteration 3/25 | Loss: 0.00138673
Iteration 4/25 | Loss: 0.00137256
Iteration 5/25 | Loss: 0.00136810
Iteration 6/25 | Loss: 0.00136672
Iteration 7/25 | Loss: 0.00136669
Iteration 8/25 | Loss: 0.00136669
Iteration 9/25 | Loss: 0.00136669
Iteration 10/25 | Loss: 0.00136669
Iteration 11/25 | Loss: 0.00136669
Iteration 12/25 | Loss: 0.00136669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013666902668774128, 0.0013666902668774128, 0.0013666902668774128, 0.0013666902668774128, 0.0013666902668774128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013666902668774128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87244540
Iteration 2/25 | Loss: 0.00151388
Iteration 3/25 | Loss: 0.00151387
Iteration 4/25 | Loss: 0.00151387
Iteration 5/25 | Loss: 0.00151386
Iteration 6/25 | Loss: 0.00151386
Iteration 7/25 | Loss: 0.00151386
Iteration 8/25 | Loss: 0.00151386
Iteration 9/25 | Loss: 0.00151386
Iteration 10/25 | Loss: 0.00151386
Iteration 11/25 | Loss: 0.00151386
Iteration 12/25 | Loss: 0.00151386
Iteration 13/25 | Loss: 0.00151386
Iteration 14/25 | Loss: 0.00151386
Iteration 15/25 | Loss: 0.00151386
Iteration 16/25 | Loss: 0.00151386
Iteration 17/25 | Loss: 0.00151386
Iteration 18/25 | Loss: 0.00151386
Iteration 19/25 | Loss: 0.00151386
Iteration 20/25 | Loss: 0.00151386
Iteration 21/25 | Loss: 0.00151386
Iteration 22/25 | Loss: 0.00151386
Iteration 23/25 | Loss: 0.00151386
Iteration 24/25 | Loss: 0.00151386
Iteration 25/25 | Loss: 0.00151386

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151386
Iteration 2/1000 | Loss: 0.00005518
Iteration 3/1000 | Loss: 0.00003633
Iteration 4/1000 | Loss: 0.00003223
Iteration 5/1000 | Loss: 0.00003078
Iteration 6/1000 | Loss: 0.00003021
Iteration 7/1000 | Loss: 0.00002951
Iteration 8/1000 | Loss: 0.00002903
Iteration 9/1000 | Loss: 0.00002866
Iteration 10/1000 | Loss: 0.00002830
Iteration 11/1000 | Loss: 0.00002796
Iteration 12/1000 | Loss: 0.00002768
Iteration 13/1000 | Loss: 0.00002739
Iteration 14/1000 | Loss: 0.00002714
Iteration 15/1000 | Loss: 0.00002693
Iteration 16/1000 | Loss: 0.00002672
Iteration 17/1000 | Loss: 0.00002667
Iteration 18/1000 | Loss: 0.00002654
Iteration 19/1000 | Loss: 0.00002649
Iteration 20/1000 | Loss: 0.00002642
Iteration 21/1000 | Loss: 0.00002640
Iteration 22/1000 | Loss: 0.00002636
Iteration 23/1000 | Loss: 0.00002635
Iteration 24/1000 | Loss: 0.00002634
Iteration 25/1000 | Loss: 0.00002633
Iteration 26/1000 | Loss: 0.00002633
Iteration 27/1000 | Loss: 0.00002633
Iteration 28/1000 | Loss: 0.00002632
Iteration 29/1000 | Loss: 0.00002632
Iteration 30/1000 | Loss: 0.00002632
Iteration 31/1000 | Loss: 0.00002631
Iteration 32/1000 | Loss: 0.00002631
Iteration 33/1000 | Loss: 0.00002631
Iteration 34/1000 | Loss: 0.00002631
Iteration 35/1000 | Loss: 0.00002631
Iteration 36/1000 | Loss: 0.00002631
Iteration 37/1000 | Loss: 0.00002630
Iteration 38/1000 | Loss: 0.00002630
Iteration 39/1000 | Loss: 0.00002630
Iteration 40/1000 | Loss: 0.00002630
Iteration 41/1000 | Loss: 0.00002629
Iteration 42/1000 | Loss: 0.00002629
Iteration 43/1000 | Loss: 0.00002629
Iteration 44/1000 | Loss: 0.00002629
Iteration 45/1000 | Loss: 0.00002629
Iteration 46/1000 | Loss: 0.00002629
Iteration 47/1000 | Loss: 0.00002628
Iteration 48/1000 | Loss: 0.00002628
Iteration 49/1000 | Loss: 0.00002628
Iteration 50/1000 | Loss: 0.00002628
Iteration 51/1000 | Loss: 0.00002628
Iteration 52/1000 | Loss: 0.00002628
Iteration 53/1000 | Loss: 0.00002628
Iteration 54/1000 | Loss: 0.00002628
Iteration 55/1000 | Loss: 0.00002628
Iteration 56/1000 | Loss: 0.00002628
Iteration 57/1000 | Loss: 0.00002628
Iteration 58/1000 | Loss: 0.00002628
Iteration 59/1000 | Loss: 0.00002627
Iteration 60/1000 | Loss: 0.00002627
Iteration 61/1000 | Loss: 0.00002627
Iteration 62/1000 | Loss: 0.00002627
Iteration 63/1000 | Loss: 0.00002627
Iteration 64/1000 | Loss: 0.00002627
Iteration 65/1000 | Loss: 0.00002627
Iteration 66/1000 | Loss: 0.00002627
Iteration 67/1000 | Loss: 0.00002627
Iteration 68/1000 | Loss: 0.00002627
Iteration 69/1000 | Loss: 0.00002627
Iteration 70/1000 | Loss: 0.00002627
Iteration 71/1000 | Loss: 0.00002627
Iteration 72/1000 | Loss: 0.00002627
Iteration 73/1000 | Loss: 0.00002627
Iteration 74/1000 | Loss: 0.00002627
Iteration 75/1000 | Loss: 0.00002627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.6270145099260844e-05, 2.6270145099260844e-05, 2.6270145099260844e-05, 2.6270145099260844e-05, 2.6270145099260844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6270145099260844e-05

Optimization complete. Final v2v error: 3.96767258644104 mm

Highest mean error: 4.84293794631958 mm for frame 121

Lowest mean error: 3.1136374473571777 mm for frame 36

Saving results

Total time: 39.642723083496094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029501
Iteration 2/25 | Loss: 0.01029501
Iteration 3/25 | Loss: 0.01029501
Iteration 4/25 | Loss: 0.00302444
Iteration 5/25 | Loss: 0.00177679
Iteration 6/25 | Loss: 0.00201132
Iteration 7/25 | Loss: 0.00163247
Iteration 8/25 | Loss: 0.00152646
Iteration 9/25 | Loss: 0.00144849
Iteration 10/25 | Loss: 0.00137998
Iteration 11/25 | Loss: 0.00127921
Iteration 12/25 | Loss: 0.00126918
Iteration 13/25 | Loss: 0.00123319
Iteration 14/25 | Loss: 0.00122399
Iteration 15/25 | Loss: 0.00121529
Iteration 16/25 | Loss: 0.00121425
Iteration 17/25 | Loss: 0.00121520
Iteration 18/25 | Loss: 0.00121939
Iteration 19/25 | Loss: 0.00121360
Iteration 20/25 | Loss: 0.00120745
Iteration 21/25 | Loss: 0.00120971
Iteration 22/25 | Loss: 0.00119333
Iteration 23/25 | Loss: 0.00119704
Iteration 24/25 | Loss: 0.00119629
Iteration 25/25 | Loss: 0.00119326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32454598
Iteration 2/25 | Loss: 0.00167033
Iteration 3/25 | Loss: 0.00151442
Iteration 4/25 | Loss: 0.00151442
Iteration 5/25 | Loss: 0.00151442
Iteration 6/25 | Loss: 0.00151442
Iteration 7/25 | Loss: 0.00151442
Iteration 8/25 | Loss: 0.00151442
Iteration 9/25 | Loss: 0.00151442
Iteration 10/25 | Loss: 0.00151442
Iteration 11/25 | Loss: 0.00151442
Iteration 12/25 | Loss: 0.00151442
Iteration 13/25 | Loss: 0.00151442
Iteration 14/25 | Loss: 0.00151442
Iteration 15/25 | Loss: 0.00151442
Iteration 16/25 | Loss: 0.00151442
Iteration 17/25 | Loss: 0.00151442
Iteration 18/25 | Loss: 0.00151442
Iteration 19/25 | Loss: 0.00151442
Iteration 20/25 | Loss: 0.00151442
Iteration 21/25 | Loss: 0.00151442
Iteration 22/25 | Loss: 0.00151442
Iteration 23/25 | Loss: 0.00151442
Iteration 24/25 | Loss: 0.00151442
Iteration 25/25 | Loss: 0.00151442

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00151442
Iteration 2/1000 | Loss: 0.00008607
Iteration 3/1000 | Loss: 0.00017264
Iteration 4/1000 | Loss: 0.00019075
Iteration 5/1000 | Loss: 0.00013602
Iteration 6/1000 | Loss: 0.00043136
Iteration 7/1000 | Loss: 0.00003097
Iteration 8/1000 | Loss: 0.00001483
Iteration 9/1000 | Loss: 0.00329231
Iteration 10/1000 | Loss: 0.00008330
Iteration 11/1000 | Loss: 0.00005691
Iteration 12/1000 | Loss: 0.00001893
Iteration 13/1000 | Loss: 0.00002006
Iteration 14/1000 | Loss: 0.00001292
Iteration 15/1000 | Loss: 0.00001613
Iteration 16/1000 | Loss: 0.00001365
Iteration 17/1000 | Loss: 0.00001510
Iteration 18/1000 | Loss: 0.00001113
Iteration 19/1000 | Loss: 0.00008631
Iteration 20/1000 | Loss: 0.00002430
Iteration 21/1000 | Loss: 0.00019310
Iteration 22/1000 | Loss: 0.00001695
Iteration 23/1000 | Loss: 0.00001086
Iteration 24/1000 | Loss: 0.00002062
Iteration 25/1000 | Loss: 0.00001058
Iteration 26/1000 | Loss: 0.00001049
Iteration 27/1000 | Loss: 0.00001049
Iteration 28/1000 | Loss: 0.00001049
Iteration 29/1000 | Loss: 0.00001045
Iteration 30/1000 | Loss: 0.00001045
Iteration 31/1000 | Loss: 0.00001045
Iteration 32/1000 | Loss: 0.00001044
Iteration 33/1000 | Loss: 0.00001043
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001033
Iteration 36/1000 | Loss: 0.00001032
Iteration 37/1000 | Loss: 0.00004959
Iteration 38/1000 | Loss: 0.00001459
Iteration 39/1000 | Loss: 0.00001022
Iteration 40/1000 | Loss: 0.00002692
Iteration 41/1000 | Loss: 0.00001015
Iteration 42/1000 | Loss: 0.00001012
Iteration 43/1000 | Loss: 0.00001012
Iteration 44/1000 | Loss: 0.00001011
Iteration 45/1000 | Loss: 0.00001010
Iteration 46/1000 | Loss: 0.00001010
Iteration 47/1000 | Loss: 0.00001010
Iteration 48/1000 | Loss: 0.00001010
Iteration 49/1000 | Loss: 0.00001010
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001010
Iteration 52/1000 | Loss: 0.00001010
Iteration 53/1000 | Loss: 0.00001010
Iteration 54/1000 | Loss: 0.00001010
Iteration 55/1000 | Loss: 0.00001010
Iteration 56/1000 | Loss: 0.00001009
Iteration 57/1000 | Loss: 0.00001009
Iteration 58/1000 | Loss: 0.00001009
Iteration 59/1000 | Loss: 0.00002173
Iteration 60/1000 | Loss: 0.00001009
Iteration 61/1000 | Loss: 0.00001339
Iteration 62/1000 | Loss: 0.00001003
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00001000
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000999
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000998
Iteration 72/1000 | Loss: 0.00000998
Iteration 73/1000 | Loss: 0.00000998
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000998
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000997
Iteration 80/1000 | Loss: 0.00000997
Iteration 81/1000 | Loss: 0.00000997
Iteration 82/1000 | Loss: 0.00001105
Iteration 83/1000 | Loss: 0.00001002
Iteration 84/1000 | Loss: 0.00000995
Iteration 85/1000 | Loss: 0.00000994
Iteration 86/1000 | Loss: 0.00000994
Iteration 87/1000 | Loss: 0.00000994
Iteration 88/1000 | Loss: 0.00000994
Iteration 89/1000 | Loss: 0.00000994
Iteration 90/1000 | Loss: 0.00000994
Iteration 91/1000 | Loss: 0.00000994
Iteration 92/1000 | Loss: 0.00000994
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001043
Iteration 95/1000 | Loss: 0.00001572
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00000991
Iteration 98/1000 | Loss: 0.00000991
Iteration 99/1000 | Loss: 0.00000990
Iteration 100/1000 | Loss: 0.00000990
Iteration 101/1000 | Loss: 0.00000990
Iteration 102/1000 | Loss: 0.00000990
Iteration 103/1000 | Loss: 0.00000990
Iteration 104/1000 | Loss: 0.00000990
Iteration 105/1000 | Loss: 0.00000990
Iteration 106/1000 | Loss: 0.00000990
Iteration 107/1000 | Loss: 0.00000990
Iteration 108/1000 | Loss: 0.00000990
Iteration 109/1000 | Loss: 0.00000990
Iteration 110/1000 | Loss: 0.00000989
Iteration 111/1000 | Loss: 0.00000989
Iteration 112/1000 | Loss: 0.00001363
Iteration 113/1000 | Loss: 0.00001005
Iteration 114/1000 | Loss: 0.00000991
Iteration 115/1000 | Loss: 0.00000991
Iteration 116/1000 | Loss: 0.00000991
Iteration 117/1000 | Loss: 0.00000991
Iteration 118/1000 | Loss: 0.00000991
Iteration 119/1000 | Loss: 0.00000991
Iteration 120/1000 | Loss: 0.00000991
Iteration 121/1000 | Loss: 0.00000991
Iteration 122/1000 | Loss: 0.00000991
Iteration 123/1000 | Loss: 0.00000991
Iteration 124/1000 | Loss: 0.00000990
Iteration 125/1000 | Loss: 0.00000990
Iteration 126/1000 | Loss: 0.00000990
Iteration 127/1000 | Loss: 0.00000990
Iteration 128/1000 | Loss: 0.00001051
Iteration 129/1000 | Loss: 0.00001015
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001008
Iteration 132/1000 | Loss: 0.00000992
Iteration 133/1000 | Loss: 0.00000988
Iteration 134/1000 | Loss: 0.00000988
Iteration 135/1000 | Loss: 0.00000988
Iteration 136/1000 | Loss: 0.00000988
Iteration 137/1000 | Loss: 0.00000988
Iteration 138/1000 | Loss: 0.00000988
Iteration 139/1000 | Loss: 0.00000987
Iteration 140/1000 | Loss: 0.00000987
Iteration 141/1000 | Loss: 0.00000987
Iteration 142/1000 | Loss: 0.00000987
Iteration 143/1000 | Loss: 0.00000987
Iteration 144/1000 | Loss: 0.00000987
Iteration 145/1000 | Loss: 0.00000987
Iteration 146/1000 | Loss: 0.00000987
Iteration 147/1000 | Loss: 0.00000987
Iteration 148/1000 | Loss: 0.00000987
Iteration 149/1000 | Loss: 0.00000987
Iteration 150/1000 | Loss: 0.00000987
Iteration 151/1000 | Loss: 0.00000987
Iteration 152/1000 | Loss: 0.00000987
Iteration 153/1000 | Loss: 0.00000987
Iteration 154/1000 | Loss: 0.00000987
Iteration 155/1000 | Loss: 0.00000987
Iteration 156/1000 | Loss: 0.00000987
Iteration 157/1000 | Loss: 0.00000987
Iteration 158/1000 | Loss: 0.00000987
Iteration 159/1000 | Loss: 0.00000987
Iteration 160/1000 | Loss: 0.00000987
Iteration 161/1000 | Loss: 0.00000987
Iteration 162/1000 | Loss: 0.00000987
Iteration 163/1000 | Loss: 0.00000987
Iteration 164/1000 | Loss: 0.00000987
Iteration 165/1000 | Loss: 0.00000987
Iteration 166/1000 | Loss: 0.00000987
Iteration 167/1000 | Loss: 0.00000987
Iteration 168/1000 | Loss: 0.00000987
Iteration 169/1000 | Loss: 0.00000987
Iteration 170/1000 | Loss: 0.00000987
Iteration 171/1000 | Loss: 0.00000987
Iteration 172/1000 | Loss: 0.00000987
Iteration 173/1000 | Loss: 0.00000987
Iteration 174/1000 | Loss: 0.00000987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [9.873859198705759e-06, 9.873859198705759e-06, 9.873859198705759e-06, 9.873859198705759e-06, 9.873859198705759e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.873859198705759e-06

Optimization complete. Final v2v error: 2.6608471870422363 mm

Highest mean error: 4.246599197387695 mm for frame 71

Lowest mean error: 2.3071248531341553 mm for frame 38

Saving results

Total time: 106.17813324928284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951362
Iteration 2/25 | Loss: 0.00293165
Iteration 3/25 | Loss: 0.00219600
Iteration 4/25 | Loss: 0.00194346
Iteration 5/25 | Loss: 0.00183914
Iteration 6/25 | Loss: 0.00165090
Iteration 7/25 | Loss: 0.00161298
Iteration 8/25 | Loss: 0.00154733
Iteration 9/25 | Loss: 0.00150938
Iteration 10/25 | Loss: 0.00149733
Iteration 11/25 | Loss: 0.00148685
Iteration 12/25 | Loss: 0.00148467
Iteration 13/25 | Loss: 0.00148341
Iteration 14/25 | Loss: 0.00148259
Iteration 15/25 | Loss: 0.00148216
Iteration 16/25 | Loss: 0.00148528
Iteration 17/25 | Loss: 0.00148156
Iteration 18/25 | Loss: 0.00148104
Iteration 19/25 | Loss: 0.00148086
Iteration 20/25 | Loss: 0.00148081
Iteration 21/25 | Loss: 0.00148081
Iteration 22/25 | Loss: 0.00148081
Iteration 23/25 | Loss: 0.00148081
Iteration 24/25 | Loss: 0.00148080
Iteration 25/25 | Loss: 0.00148080

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25622439
Iteration 2/25 | Loss: 0.00176317
Iteration 3/25 | Loss: 0.00169200
Iteration 4/25 | Loss: 0.00169200
Iteration 5/25 | Loss: 0.00169200
Iteration 6/25 | Loss: 0.00169200
Iteration 7/25 | Loss: 0.00169200
Iteration 8/25 | Loss: 0.00169200
Iteration 9/25 | Loss: 0.00169200
Iteration 10/25 | Loss: 0.00169200
Iteration 11/25 | Loss: 0.00169200
Iteration 12/25 | Loss: 0.00169200
Iteration 13/25 | Loss: 0.00169200
Iteration 14/25 | Loss: 0.00169200
Iteration 15/25 | Loss: 0.00169200
Iteration 16/25 | Loss: 0.00169200
Iteration 17/25 | Loss: 0.00169200
Iteration 18/25 | Loss: 0.00169200
Iteration 19/25 | Loss: 0.00169200
Iteration 20/25 | Loss: 0.00169200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016919984482228756, 0.0016919984482228756, 0.0016919984482228756, 0.0016919984482228756, 0.0016919984482228756]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016919984482228756

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169200
Iteration 2/1000 | Loss: 0.00013427
Iteration 3/1000 | Loss: 0.00017784
Iteration 4/1000 | Loss: 0.00007107
Iteration 5/1000 | Loss: 0.00004758
Iteration 6/1000 | Loss: 0.00004324
Iteration 7/1000 | Loss: 0.00004049
Iteration 8/1000 | Loss: 0.00008298
Iteration 9/1000 | Loss: 0.00003922
Iteration 10/1000 | Loss: 0.00003745
Iteration 11/1000 | Loss: 0.00003655
Iteration 12/1000 | Loss: 0.00006707
Iteration 13/1000 | Loss: 0.00003574
Iteration 14/1000 | Loss: 0.00003537
Iteration 15/1000 | Loss: 0.00003499
Iteration 16/1000 | Loss: 0.00003471
Iteration 17/1000 | Loss: 0.00003442
Iteration 18/1000 | Loss: 0.00003429
Iteration 19/1000 | Loss: 0.00003428
Iteration 20/1000 | Loss: 0.00003425
Iteration 21/1000 | Loss: 0.00003424
Iteration 22/1000 | Loss: 0.00003416
Iteration 23/1000 | Loss: 0.00003415
Iteration 24/1000 | Loss: 0.00003410
Iteration 25/1000 | Loss: 0.00003410
Iteration 26/1000 | Loss: 0.00003409
Iteration 27/1000 | Loss: 0.00003409
Iteration 28/1000 | Loss: 0.00003409
Iteration 29/1000 | Loss: 0.00003408
Iteration 30/1000 | Loss: 0.00003408
Iteration 31/1000 | Loss: 0.00003407
Iteration 32/1000 | Loss: 0.00003406
Iteration 33/1000 | Loss: 0.00003405
Iteration 34/1000 | Loss: 0.00003405
Iteration 35/1000 | Loss: 0.00003405
Iteration 36/1000 | Loss: 0.00003405
Iteration 37/1000 | Loss: 0.00003404
Iteration 38/1000 | Loss: 0.00003404
Iteration 39/1000 | Loss: 0.00003404
Iteration 40/1000 | Loss: 0.00003404
Iteration 41/1000 | Loss: 0.00003404
Iteration 42/1000 | Loss: 0.00003403
Iteration 43/1000 | Loss: 0.00003402
Iteration 44/1000 | Loss: 0.00003402
Iteration 45/1000 | Loss: 0.00003400
Iteration 46/1000 | Loss: 0.00003400
Iteration 47/1000 | Loss: 0.00003400
Iteration 48/1000 | Loss: 0.00003399
Iteration 49/1000 | Loss: 0.00003399
Iteration 50/1000 | Loss: 0.00003399
Iteration 51/1000 | Loss: 0.00003399
Iteration 52/1000 | Loss: 0.00003399
Iteration 53/1000 | Loss: 0.00003399
Iteration 54/1000 | Loss: 0.00003399
Iteration 55/1000 | Loss: 0.00003399
Iteration 56/1000 | Loss: 0.00003398
Iteration 57/1000 | Loss: 0.00003398
Iteration 58/1000 | Loss: 0.00003398
Iteration 59/1000 | Loss: 0.00003398
Iteration 60/1000 | Loss: 0.00003398
Iteration 61/1000 | Loss: 0.00003398
Iteration 62/1000 | Loss: 0.00003398
Iteration 63/1000 | Loss: 0.00003398
Iteration 64/1000 | Loss: 0.00003397
Iteration 65/1000 | Loss: 0.00003397
Iteration 66/1000 | Loss: 0.00003397
Iteration 67/1000 | Loss: 0.00003397
Iteration 68/1000 | Loss: 0.00003397
Iteration 69/1000 | Loss: 0.00003397
Iteration 70/1000 | Loss: 0.00003397
Iteration 71/1000 | Loss: 0.00003397
Iteration 72/1000 | Loss: 0.00003397
Iteration 73/1000 | Loss: 0.00003397
Iteration 74/1000 | Loss: 0.00003397
Iteration 75/1000 | Loss: 0.00003397
Iteration 76/1000 | Loss: 0.00003396
Iteration 77/1000 | Loss: 0.00003396
Iteration 78/1000 | Loss: 0.00003396
Iteration 79/1000 | Loss: 0.00003395
Iteration 80/1000 | Loss: 0.00003395
Iteration 81/1000 | Loss: 0.00003394
Iteration 82/1000 | Loss: 0.00003394
Iteration 83/1000 | Loss: 0.00003394
Iteration 84/1000 | Loss: 0.00003394
Iteration 85/1000 | Loss: 0.00003394
Iteration 86/1000 | Loss: 0.00003394
Iteration 87/1000 | Loss: 0.00003394
Iteration 88/1000 | Loss: 0.00003393
Iteration 89/1000 | Loss: 0.00003393
Iteration 90/1000 | Loss: 0.00003393
Iteration 91/1000 | Loss: 0.00003393
Iteration 92/1000 | Loss: 0.00003393
Iteration 93/1000 | Loss: 0.00003393
Iteration 94/1000 | Loss: 0.00003393
Iteration 95/1000 | Loss: 0.00003393
Iteration 96/1000 | Loss: 0.00003393
Iteration 97/1000 | Loss: 0.00003393
Iteration 98/1000 | Loss: 0.00003393
Iteration 99/1000 | Loss: 0.00003393
Iteration 100/1000 | Loss: 0.00003392
Iteration 101/1000 | Loss: 0.00003392
Iteration 102/1000 | Loss: 0.00003392
Iteration 103/1000 | Loss: 0.00003392
Iteration 104/1000 | Loss: 0.00003392
Iteration 105/1000 | Loss: 0.00003392
Iteration 106/1000 | Loss: 0.00003392
Iteration 107/1000 | Loss: 0.00003392
Iteration 108/1000 | Loss: 0.00003392
Iteration 109/1000 | Loss: 0.00003392
Iteration 110/1000 | Loss: 0.00003392
Iteration 111/1000 | Loss: 0.00003392
Iteration 112/1000 | Loss: 0.00003392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [3.39172147505451e-05, 3.39172147505451e-05, 3.39172147505451e-05, 3.39172147505451e-05, 3.39172147505451e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.39172147505451e-05

Optimization complete. Final v2v error: 4.7985734939575195 mm

Highest mean error: 11.471525192260742 mm for frame 232

Lowest mean error: 3.633800506591797 mm for frame 155

Saving results

Total time: 78.7223470211029
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398303
Iteration 2/25 | Loss: 0.00127538
Iteration 3/25 | Loss: 0.00121292
Iteration 4/25 | Loss: 0.00120562
Iteration 5/25 | Loss: 0.00120323
Iteration 6/25 | Loss: 0.00120323
Iteration 7/25 | Loss: 0.00120323
Iteration 8/25 | Loss: 0.00120323
Iteration 9/25 | Loss: 0.00120323
Iteration 10/25 | Loss: 0.00120323
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012032348895445466, 0.0012032348895445466, 0.0012032348895445466, 0.0012032348895445466, 0.0012032348895445466]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012032348895445466

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53609836
Iteration 2/25 | Loss: 0.00126347
Iteration 3/25 | Loss: 0.00126347
Iteration 4/25 | Loss: 0.00126347
Iteration 5/25 | Loss: 0.00126347
Iteration 6/25 | Loss: 0.00126347
Iteration 7/25 | Loss: 0.00126347
Iteration 8/25 | Loss: 0.00126347
Iteration 9/25 | Loss: 0.00126347
Iteration 10/25 | Loss: 0.00126347
Iteration 11/25 | Loss: 0.00126347
Iteration 12/25 | Loss: 0.00126347
Iteration 13/25 | Loss: 0.00126347
Iteration 14/25 | Loss: 0.00126347
Iteration 15/25 | Loss: 0.00126347
Iteration 16/25 | Loss: 0.00126347
Iteration 17/25 | Loss: 0.00126347
Iteration 18/25 | Loss: 0.00126347
Iteration 19/25 | Loss: 0.00126347
Iteration 20/25 | Loss: 0.00126347
Iteration 21/25 | Loss: 0.00126347
Iteration 22/25 | Loss: 0.00126347
Iteration 23/25 | Loss: 0.00126347
Iteration 24/25 | Loss: 0.00126347
Iteration 25/25 | Loss: 0.00126347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126347
Iteration 2/1000 | Loss: 0.00002831
Iteration 3/1000 | Loss: 0.00001642
Iteration 4/1000 | Loss: 0.00001355
Iteration 5/1000 | Loss: 0.00001259
Iteration 6/1000 | Loss: 0.00001201
Iteration 7/1000 | Loss: 0.00001163
Iteration 8/1000 | Loss: 0.00001139
Iteration 9/1000 | Loss: 0.00001098
Iteration 10/1000 | Loss: 0.00001059
Iteration 11/1000 | Loss: 0.00001038
Iteration 12/1000 | Loss: 0.00001022
Iteration 13/1000 | Loss: 0.00001021
Iteration 14/1000 | Loss: 0.00001021
Iteration 15/1000 | Loss: 0.00001017
Iteration 16/1000 | Loss: 0.00001009
Iteration 17/1000 | Loss: 0.00001009
Iteration 18/1000 | Loss: 0.00001008
Iteration 19/1000 | Loss: 0.00001003
Iteration 20/1000 | Loss: 0.00001002
Iteration 21/1000 | Loss: 0.00001000
Iteration 22/1000 | Loss: 0.00000998
Iteration 23/1000 | Loss: 0.00000994
Iteration 24/1000 | Loss: 0.00000994
Iteration 25/1000 | Loss: 0.00000992
Iteration 26/1000 | Loss: 0.00000989
Iteration 27/1000 | Loss: 0.00000988
Iteration 28/1000 | Loss: 0.00000988
Iteration 29/1000 | Loss: 0.00000987
Iteration 30/1000 | Loss: 0.00000987
Iteration 31/1000 | Loss: 0.00000983
Iteration 32/1000 | Loss: 0.00000983
Iteration 33/1000 | Loss: 0.00000983
Iteration 34/1000 | Loss: 0.00000983
Iteration 35/1000 | Loss: 0.00000983
Iteration 36/1000 | Loss: 0.00000983
Iteration 37/1000 | Loss: 0.00000983
Iteration 38/1000 | Loss: 0.00000982
Iteration 39/1000 | Loss: 0.00000982
Iteration 40/1000 | Loss: 0.00000982
Iteration 41/1000 | Loss: 0.00000982
Iteration 42/1000 | Loss: 0.00000981
Iteration 43/1000 | Loss: 0.00000980
Iteration 44/1000 | Loss: 0.00000979
Iteration 45/1000 | Loss: 0.00000979
Iteration 46/1000 | Loss: 0.00000979
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000977
Iteration 49/1000 | Loss: 0.00000977
Iteration 50/1000 | Loss: 0.00000977
Iteration 51/1000 | Loss: 0.00000977
Iteration 52/1000 | Loss: 0.00000976
Iteration 53/1000 | Loss: 0.00000976
Iteration 54/1000 | Loss: 0.00000975
Iteration 55/1000 | Loss: 0.00000975
Iteration 56/1000 | Loss: 0.00000975
Iteration 57/1000 | Loss: 0.00000975
Iteration 58/1000 | Loss: 0.00000975
Iteration 59/1000 | Loss: 0.00000974
Iteration 60/1000 | Loss: 0.00000974
Iteration 61/1000 | Loss: 0.00000973
Iteration 62/1000 | Loss: 0.00000973
Iteration 63/1000 | Loss: 0.00000973
Iteration 64/1000 | Loss: 0.00000973
Iteration 65/1000 | Loss: 0.00000973
Iteration 66/1000 | Loss: 0.00000972
Iteration 67/1000 | Loss: 0.00000972
Iteration 68/1000 | Loss: 0.00000972
Iteration 69/1000 | Loss: 0.00000972
Iteration 70/1000 | Loss: 0.00000971
Iteration 71/1000 | Loss: 0.00000971
Iteration 72/1000 | Loss: 0.00000970
Iteration 73/1000 | Loss: 0.00000970
Iteration 74/1000 | Loss: 0.00000969
Iteration 75/1000 | Loss: 0.00000969
Iteration 76/1000 | Loss: 0.00000969
Iteration 77/1000 | Loss: 0.00000969
Iteration 78/1000 | Loss: 0.00000968
Iteration 79/1000 | Loss: 0.00000968
Iteration 80/1000 | Loss: 0.00000968
Iteration 81/1000 | Loss: 0.00000967
Iteration 82/1000 | Loss: 0.00000967
Iteration 83/1000 | Loss: 0.00000967
Iteration 84/1000 | Loss: 0.00000966
Iteration 85/1000 | Loss: 0.00000966
Iteration 86/1000 | Loss: 0.00000966
Iteration 87/1000 | Loss: 0.00000966
Iteration 88/1000 | Loss: 0.00000966
Iteration 89/1000 | Loss: 0.00000965
Iteration 90/1000 | Loss: 0.00000965
Iteration 91/1000 | Loss: 0.00000965
Iteration 92/1000 | Loss: 0.00000965
Iteration 93/1000 | Loss: 0.00000965
Iteration 94/1000 | Loss: 0.00000964
Iteration 95/1000 | Loss: 0.00000964
Iteration 96/1000 | Loss: 0.00000964
Iteration 97/1000 | Loss: 0.00000963
Iteration 98/1000 | Loss: 0.00000962
Iteration 99/1000 | Loss: 0.00000962
Iteration 100/1000 | Loss: 0.00000961
Iteration 101/1000 | Loss: 0.00000961
Iteration 102/1000 | Loss: 0.00000960
Iteration 103/1000 | Loss: 0.00000960
Iteration 104/1000 | Loss: 0.00000959
Iteration 105/1000 | Loss: 0.00000959
Iteration 106/1000 | Loss: 0.00000959
Iteration 107/1000 | Loss: 0.00000959
Iteration 108/1000 | Loss: 0.00000959
Iteration 109/1000 | Loss: 0.00000959
Iteration 110/1000 | Loss: 0.00000959
Iteration 111/1000 | Loss: 0.00000959
Iteration 112/1000 | Loss: 0.00000959
Iteration 113/1000 | Loss: 0.00000959
Iteration 114/1000 | Loss: 0.00000958
Iteration 115/1000 | Loss: 0.00000958
Iteration 116/1000 | Loss: 0.00000957
Iteration 117/1000 | Loss: 0.00000957
Iteration 118/1000 | Loss: 0.00000957
Iteration 119/1000 | Loss: 0.00000956
Iteration 120/1000 | Loss: 0.00000956
Iteration 121/1000 | Loss: 0.00000956
Iteration 122/1000 | Loss: 0.00000956
Iteration 123/1000 | Loss: 0.00000956
Iteration 124/1000 | Loss: 0.00000956
Iteration 125/1000 | Loss: 0.00000956
Iteration 126/1000 | Loss: 0.00000956
Iteration 127/1000 | Loss: 0.00000956
Iteration 128/1000 | Loss: 0.00000955
Iteration 129/1000 | Loss: 0.00000955
Iteration 130/1000 | Loss: 0.00000955
Iteration 131/1000 | Loss: 0.00000954
Iteration 132/1000 | Loss: 0.00000954
Iteration 133/1000 | Loss: 0.00000953
Iteration 134/1000 | Loss: 0.00000953
Iteration 135/1000 | Loss: 0.00000953
Iteration 136/1000 | Loss: 0.00000953
Iteration 137/1000 | Loss: 0.00000953
Iteration 138/1000 | Loss: 0.00000953
Iteration 139/1000 | Loss: 0.00000953
Iteration 140/1000 | Loss: 0.00000952
Iteration 141/1000 | Loss: 0.00000952
Iteration 142/1000 | Loss: 0.00000952
Iteration 143/1000 | Loss: 0.00000952
Iteration 144/1000 | Loss: 0.00000952
Iteration 145/1000 | Loss: 0.00000952
Iteration 146/1000 | Loss: 0.00000951
Iteration 147/1000 | Loss: 0.00000951
Iteration 148/1000 | Loss: 0.00000951
Iteration 149/1000 | Loss: 0.00000951
Iteration 150/1000 | Loss: 0.00000950
Iteration 151/1000 | Loss: 0.00000950
Iteration 152/1000 | Loss: 0.00000950
Iteration 153/1000 | Loss: 0.00000950
Iteration 154/1000 | Loss: 0.00000950
Iteration 155/1000 | Loss: 0.00000950
Iteration 156/1000 | Loss: 0.00000950
Iteration 157/1000 | Loss: 0.00000949
Iteration 158/1000 | Loss: 0.00000949
Iteration 159/1000 | Loss: 0.00000949
Iteration 160/1000 | Loss: 0.00000949
Iteration 161/1000 | Loss: 0.00000948
Iteration 162/1000 | Loss: 0.00000948
Iteration 163/1000 | Loss: 0.00000948
Iteration 164/1000 | Loss: 0.00000948
Iteration 165/1000 | Loss: 0.00000948
Iteration 166/1000 | Loss: 0.00000947
Iteration 167/1000 | Loss: 0.00000947
Iteration 168/1000 | Loss: 0.00000947
Iteration 169/1000 | Loss: 0.00000947
Iteration 170/1000 | Loss: 0.00000947
Iteration 171/1000 | Loss: 0.00000946
Iteration 172/1000 | Loss: 0.00000946
Iteration 173/1000 | Loss: 0.00000946
Iteration 174/1000 | Loss: 0.00000946
Iteration 175/1000 | Loss: 0.00000945
Iteration 176/1000 | Loss: 0.00000945
Iteration 177/1000 | Loss: 0.00000945
Iteration 178/1000 | Loss: 0.00000945
Iteration 179/1000 | Loss: 0.00000945
Iteration 180/1000 | Loss: 0.00000945
Iteration 181/1000 | Loss: 0.00000945
Iteration 182/1000 | Loss: 0.00000944
Iteration 183/1000 | Loss: 0.00000944
Iteration 184/1000 | Loss: 0.00000944
Iteration 185/1000 | Loss: 0.00000944
Iteration 186/1000 | Loss: 0.00000943
Iteration 187/1000 | Loss: 0.00000943
Iteration 188/1000 | Loss: 0.00000943
Iteration 189/1000 | Loss: 0.00000943
Iteration 190/1000 | Loss: 0.00000943
Iteration 191/1000 | Loss: 0.00000943
Iteration 192/1000 | Loss: 0.00000943
Iteration 193/1000 | Loss: 0.00000943
Iteration 194/1000 | Loss: 0.00000943
Iteration 195/1000 | Loss: 0.00000943
Iteration 196/1000 | Loss: 0.00000943
Iteration 197/1000 | Loss: 0.00000943
Iteration 198/1000 | Loss: 0.00000943
Iteration 199/1000 | Loss: 0.00000943
Iteration 200/1000 | Loss: 0.00000942
Iteration 201/1000 | Loss: 0.00000942
Iteration 202/1000 | Loss: 0.00000942
Iteration 203/1000 | Loss: 0.00000942
Iteration 204/1000 | Loss: 0.00000941
Iteration 205/1000 | Loss: 0.00000941
Iteration 206/1000 | Loss: 0.00000941
Iteration 207/1000 | Loss: 0.00000940
Iteration 208/1000 | Loss: 0.00000940
Iteration 209/1000 | Loss: 0.00000940
Iteration 210/1000 | Loss: 0.00000940
Iteration 211/1000 | Loss: 0.00000940
Iteration 212/1000 | Loss: 0.00000940
Iteration 213/1000 | Loss: 0.00000940
Iteration 214/1000 | Loss: 0.00000940
Iteration 215/1000 | Loss: 0.00000940
Iteration 216/1000 | Loss: 0.00000940
Iteration 217/1000 | Loss: 0.00000940
Iteration 218/1000 | Loss: 0.00000939
Iteration 219/1000 | Loss: 0.00000939
Iteration 220/1000 | Loss: 0.00000939
Iteration 221/1000 | Loss: 0.00000939
Iteration 222/1000 | Loss: 0.00000939
Iteration 223/1000 | Loss: 0.00000939
Iteration 224/1000 | Loss: 0.00000939
Iteration 225/1000 | Loss: 0.00000939
Iteration 226/1000 | Loss: 0.00000939
Iteration 227/1000 | Loss: 0.00000939
Iteration 228/1000 | Loss: 0.00000939
Iteration 229/1000 | Loss: 0.00000938
Iteration 230/1000 | Loss: 0.00000938
Iteration 231/1000 | Loss: 0.00000938
Iteration 232/1000 | Loss: 0.00000938
Iteration 233/1000 | Loss: 0.00000938
Iteration 234/1000 | Loss: 0.00000938
Iteration 235/1000 | Loss: 0.00000938
Iteration 236/1000 | Loss: 0.00000938
Iteration 237/1000 | Loss: 0.00000938
Iteration 238/1000 | Loss: 0.00000938
Iteration 239/1000 | Loss: 0.00000938
Iteration 240/1000 | Loss: 0.00000938
Iteration 241/1000 | Loss: 0.00000937
Iteration 242/1000 | Loss: 0.00000937
Iteration 243/1000 | Loss: 0.00000937
Iteration 244/1000 | Loss: 0.00000937
Iteration 245/1000 | Loss: 0.00000937
Iteration 246/1000 | Loss: 0.00000937
Iteration 247/1000 | Loss: 0.00000937
Iteration 248/1000 | Loss: 0.00000937
Iteration 249/1000 | Loss: 0.00000936
Iteration 250/1000 | Loss: 0.00000936
Iteration 251/1000 | Loss: 0.00000936
Iteration 252/1000 | Loss: 0.00000936
Iteration 253/1000 | Loss: 0.00000936
Iteration 254/1000 | Loss: 0.00000936
Iteration 255/1000 | Loss: 0.00000936
Iteration 256/1000 | Loss: 0.00000936
Iteration 257/1000 | Loss: 0.00000936
Iteration 258/1000 | Loss: 0.00000936
Iteration 259/1000 | Loss: 0.00000935
Iteration 260/1000 | Loss: 0.00000935
Iteration 261/1000 | Loss: 0.00000935
Iteration 262/1000 | Loss: 0.00000935
Iteration 263/1000 | Loss: 0.00000935
Iteration 264/1000 | Loss: 0.00000935
Iteration 265/1000 | Loss: 0.00000935
Iteration 266/1000 | Loss: 0.00000935
Iteration 267/1000 | Loss: 0.00000935
Iteration 268/1000 | Loss: 0.00000935
Iteration 269/1000 | Loss: 0.00000935
Iteration 270/1000 | Loss: 0.00000935
Iteration 271/1000 | Loss: 0.00000935
Iteration 272/1000 | Loss: 0.00000934
Iteration 273/1000 | Loss: 0.00000934
Iteration 274/1000 | Loss: 0.00000934
Iteration 275/1000 | Loss: 0.00000934
Iteration 276/1000 | Loss: 0.00000934
Iteration 277/1000 | Loss: 0.00000934
Iteration 278/1000 | Loss: 0.00000934
Iteration 279/1000 | Loss: 0.00000934
Iteration 280/1000 | Loss: 0.00000934
Iteration 281/1000 | Loss: 0.00000934
Iteration 282/1000 | Loss: 0.00000934
Iteration 283/1000 | Loss: 0.00000934
Iteration 284/1000 | Loss: 0.00000934
Iteration 285/1000 | Loss: 0.00000934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [9.343108104076236e-06, 9.343108104076236e-06, 9.343108104076236e-06, 9.343108104076236e-06, 9.343108104076236e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.343108104076236e-06

Optimization complete. Final v2v error: 2.612779140472412 mm

Highest mean error: 3.0571401119232178 mm for frame 204

Lowest mean error: 2.364117383956909 mm for frame 108

Saving results

Total time: 52.069759368896484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485392
Iteration 2/25 | Loss: 0.00136489
Iteration 3/25 | Loss: 0.00127661
Iteration 4/25 | Loss: 0.00126205
Iteration 5/25 | Loss: 0.00125796
Iteration 6/25 | Loss: 0.00125741
Iteration 7/25 | Loss: 0.00125741
Iteration 8/25 | Loss: 0.00125741
Iteration 9/25 | Loss: 0.00125741
Iteration 10/25 | Loss: 0.00125741
Iteration 11/25 | Loss: 0.00125741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012574100401252508, 0.0012574100401252508, 0.0012574100401252508, 0.0012574100401252508, 0.0012574100401252508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012574100401252508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28191876
Iteration 2/25 | Loss: 0.00150712
Iteration 3/25 | Loss: 0.00150712
Iteration 4/25 | Loss: 0.00150712
Iteration 5/25 | Loss: 0.00150712
Iteration 6/25 | Loss: 0.00150712
Iteration 7/25 | Loss: 0.00150712
Iteration 8/25 | Loss: 0.00150711
Iteration 9/25 | Loss: 0.00150711
Iteration 10/25 | Loss: 0.00150711
Iteration 11/25 | Loss: 0.00150711
Iteration 12/25 | Loss: 0.00150711
Iteration 13/25 | Loss: 0.00150711
Iteration 14/25 | Loss: 0.00150711
Iteration 15/25 | Loss: 0.00150711
Iteration 16/25 | Loss: 0.00150711
Iteration 17/25 | Loss: 0.00150711
Iteration 18/25 | Loss: 0.00150711
Iteration 19/25 | Loss: 0.00150711
Iteration 20/25 | Loss: 0.00150711
Iteration 21/25 | Loss: 0.00150711
Iteration 22/25 | Loss: 0.00150711
Iteration 23/25 | Loss: 0.00150711
Iteration 24/25 | Loss: 0.00150711
Iteration 25/25 | Loss: 0.00150711

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150711
Iteration 2/1000 | Loss: 0.00004500
Iteration 3/1000 | Loss: 0.00003145
Iteration 4/1000 | Loss: 0.00002673
Iteration 5/1000 | Loss: 0.00002499
Iteration 6/1000 | Loss: 0.00002415
Iteration 7/1000 | Loss: 0.00002338
Iteration 8/1000 | Loss: 0.00002275
Iteration 9/1000 | Loss: 0.00002236
Iteration 10/1000 | Loss: 0.00002205
Iteration 11/1000 | Loss: 0.00002177
Iteration 12/1000 | Loss: 0.00002167
Iteration 13/1000 | Loss: 0.00002153
Iteration 14/1000 | Loss: 0.00002142
Iteration 15/1000 | Loss: 0.00002142
Iteration 16/1000 | Loss: 0.00002136
Iteration 17/1000 | Loss: 0.00002133
Iteration 18/1000 | Loss: 0.00002131
Iteration 19/1000 | Loss: 0.00002127
Iteration 20/1000 | Loss: 0.00002126
Iteration 21/1000 | Loss: 0.00002117
Iteration 22/1000 | Loss: 0.00002114
Iteration 23/1000 | Loss: 0.00002113
Iteration 24/1000 | Loss: 0.00002112
Iteration 25/1000 | Loss: 0.00002112
Iteration 26/1000 | Loss: 0.00002111
Iteration 27/1000 | Loss: 0.00002104
Iteration 28/1000 | Loss: 0.00002096
Iteration 29/1000 | Loss: 0.00002094
Iteration 30/1000 | Loss: 0.00002092
Iteration 31/1000 | Loss: 0.00002091
Iteration 32/1000 | Loss: 0.00002090
Iteration 33/1000 | Loss: 0.00002089
Iteration 34/1000 | Loss: 0.00002088
Iteration 35/1000 | Loss: 0.00002088
Iteration 36/1000 | Loss: 0.00002088
Iteration 37/1000 | Loss: 0.00002086
Iteration 38/1000 | Loss: 0.00002085
Iteration 39/1000 | Loss: 0.00002085
Iteration 40/1000 | Loss: 0.00002085
Iteration 41/1000 | Loss: 0.00002085
Iteration 42/1000 | Loss: 0.00002084
Iteration 43/1000 | Loss: 0.00002084
Iteration 44/1000 | Loss: 0.00002084
Iteration 45/1000 | Loss: 0.00002083
Iteration 46/1000 | Loss: 0.00002083
Iteration 47/1000 | Loss: 0.00002082
Iteration 48/1000 | Loss: 0.00002081
Iteration 49/1000 | Loss: 0.00002081
Iteration 50/1000 | Loss: 0.00002080
Iteration 51/1000 | Loss: 0.00002080
Iteration 52/1000 | Loss: 0.00002078
Iteration 53/1000 | Loss: 0.00002078
Iteration 54/1000 | Loss: 0.00002078
Iteration 55/1000 | Loss: 0.00002077
Iteration 56/1000 | Loss: 0.00002077
Iteration 57/1000 | Loss: 0.00002076
Iteration 58/1000 | Loss: 0.00002075
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002074
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002072
Iteration 65/1000 | Loss: 0.00002071
Iteration 66/1000 | Loss: 0.00002071
Iteration 67/1000 | Loss: 0.00002070
Iteration 68/1000 | Loss: 0.00002070
Iteration 69/1000 | Loss: 0.00002069
Iteration 70/1000 | Loss: 0.00002069
Iteration 71/1000 | Loss: 0.00002069
Iteration 72/1000 | Loss: 0.00002068
Iteration 73/1000 | Loss: 0.00002068
Iteration 74/1000 | Loss: 0.00002068
Iteration 75/1000 | Loss: 0.00002068
Iteration 76/1000 | Loss: 0.00002068
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002068
Iteration 79/1000 | Loss: 0.00002068
Iteration 80/1000 | Loss: 0.00002068
Iteration 81/1000 | Loss: 0.00002068
Iteration 82/1000 | Loss: 0.00002067
Iteration 83/1000 | Loss: 0.00002067
Iteration 84/1000 | Loss: 0.00002067
Iteration 85/1000 | Loss: 0.00002066
Iteration 86/1000 | Loss: 0.00002066
Iteration 87/1000 | Loss: 0.00002066
Iteration 88/1000 | Loss: 0.00002066
Iteration 89/1000 | Loss: 0.00002065
Iteration 90/1000 | Loss: 0.00002065
Iteration 91/1000 | Loss: 0.00002065
Iteration 92/1000 | Loss: 0.00002064
Iteration 93/1000 | Loss: 0.00002064
Iteration 94/1000 | Loss: 0.00002064
Iteration 95/1000 | Loss: 0.00002063
Iteration 96/1000 | Loss: 0.00002063
Iteration 97/1000 | Loss: 0.00002063
Iteration 98/1000 | Loss: 0.00002063
Iteration 99/1000 | Loss: 0.00002062
Iteration 100/1000 | Loss: 0.00002062
Iteration 101/1000 | Loss: 0.00002062
Iteration 102/1000 | Loss: 0.00002062
Iteration 103/1000 | Loss: 0.00002062
Iteration 104/1000 | Loss: 0.00002062
Iteration 105/1000 | Loss: 0.00002062
Iteration 106/1000 | Loss: 0.00002062
Iteration 107/1000 | Loss: 0.00002061
Iteration 108/1000 | Loss: 0.00002061
Iteration 109/1000 | Loss: 0.00002061
Iteration 110/1000 | Loss: 0.00002061
Iteration 111/1000 | Loss: 0.00002061
Iteration 112/1000 | Loss: 0.00002061
Iteration 113/1000 | Loss: 0.00002061
Iteration 114/1000 | Loss: 0.00002060
Iteration 115/1000 | Loss: 0.00002060
Iteration 116/1000 | Loss: 0.00002060
Iteration 117/1000 | Loss: 0.00002060
Iteration 118/1000 | Loss: 0.00002060
Iteration 119/1000 | Loss: 0.00002060
Iteration 120/1000 | Loss: 0.00002060
Iteration 121/1000 | Loss: 0.00002059
Iteration 122/1000 | Loss: 0.00002059
Iteration 123/1000 | Loss: 0.00002059
Iteration 124/1000 | Loss: 0.00002059
Iteration 125/1000 | Loss: 0.00002059
Iteration 126/1000 | Loss: 0.00002059
Iteration 127/1000 | Loss: 0.00002058
Iteration 128/1000 | Loss: 0.00002058
Iteration 129/1000 | Loss: 0.00002058
Iteration 130/1000 | Loss: 0.00002058
Iteration 131/1000 | Loss: 0.00002058
Iteration 132/1000 | Loss: 0.00002058
Iteration 133/1000 | Loss: 0.00002057
Iteration 134/1000 | Loss: 0.00002057
Iteration 135/1000 | Loss: 0.00002057
Iteration 136/1000 | Loss: 0.00002057
Iteration 137/1000 | Loss: 0.00002057
Iteration 138/1000 | Loss: 0.00002056
Iteration 139/1000 | Loss: 0.00002056
Iteration 140/1000 | Loss: 0.00002055
Iteration 141/1000 | Loss: 0.00002055
Iteration 142/1000 | Loss: 0.00002055
Iteration 143/1000 | Loss: 0.00002055
Iteration 144/1000 | Loss: 0.00002055
Iteration 145/1000 | Loss: 0.00002055
Iteration 146/1000 | Loss: 0.00002055
Iteration 147/1000 | Loss: 0.00002055
Iteration 148/1000 | Loss: 0.00002055
Iteration 149/1000 | Loss: 0.00002055
Iteration 150/1000 | Loss: 0.00002055
Iteration 151/1000 | Loss: 0.00002055
Iteration 152/1000 | Loss: 0.00002054
Iteration 153/1000 | Loss: 0.00002054
Iteration 154/1000 | Loss: 0.00002054
Iteration 155/1000 | Loss: 0.00002054
Iteration 156/1000 | Loss: 0.00002054
Iteration 157/1000 | Loss: 0.00002054
Iteration 158/1000 | Loss: 0.00002054
Iteration 159/1000 | Loss: 0.00002054
Iteration 160/1000 | Loss: 0.00002054
Iteration 161/1000 | Loss: 0.00002054
Iteration 162/1000 | Loss: 0.00002054
Iteration 163/1000 | Loss: 0.00002054
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.0541980120469816e-05, 2.0541980120469816e-05, 2.0541980120469816e-05, 2.0541980120469816e-05, 2.0541980120469816e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0541980120469816e-05

Optimization complete. Final v2v error: 3.5411267280578613 mm

Highest mean error: 4.572939395904541 mm for frame 76

Lowest mean error: 3.0918335914611816 mm for frame 115

Saving results

Total time: 44.37176465988159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00997434
Iteration 2/25 | Loss: 0.00435461
Iteration 3/25 | Loss: 0.00230471
Iteration 4/25 | Loss: 0.00195548
Iteration 5/25 | Loss: 0.00199825
Iteration 6/25 | Loss: 0.00175936
Iteration 7/25 | Loss: 0.00144180
Iteration 8/25 | Loss: 0.00134264
Iteration 9/25 | Loss: 0.00131459
Iteration 10/25 | Loss: 0.00129185
Iteration 11/25 | Loss: 0.00129576
Iteration 12/25 | Loss: 0.00128050
Iteration 13/25 | Loss: 0.00127270
Iteration 14/25 | Loss: 0.00127024
Iteration 15/25 | Loss: 0.00128043
Iteration 16/25 | Loss: 0.00125392
Iteration 17/25 | Loss: 0.00124858
Iteration 18/25 | Loss: 0.00124821
Iteration 19/25 | Loss: 0.00124546
Iteration 20/25 | Loss: 0.00124679
Iteration 21/25 | Loss: 0.00124525
Iteration 22/25 | Loss: 0.00124510
Iteration 23/25 | Loss: 0.00124510
Iteration 24/25 | Loss: 0.00124509
Iteration 25/25 | Loss: 0.00124509

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27284455
Iteration 2/25 | Loss: 0.00188723
Iteration 3/25 | Loss: 0.00188674
Iteration 4/25 | Loss: 0.00188784
Iteration 5/25 | Loss: 0.00187533
Iteration 6/25 | Loss: 0.00188125
Iteration 7/25 | Loss: 0.00186997
Iteration 8/25 | Loss: 0.00187048
Iteration 9/25 | Loss: 0.00186999
Iteration 10/25 | Loss: 0.00186762
Iteration 11/25 | Loss: 0.00186757
Iteration 12/25 | Loss: 0.00186757
Iteration 13/25 | Loss: 0.00186757
Iteration 14/25 | Loss: 0.00186757
Iteration 15/25 | Loss: 0.00186757
Iteration 16/25 | Loss: 0.00186757
Iteration 17/25 | Loss: 0.00186757
Iteration 18/25 | Loss: 0.00186757
Iteration 19/25 | Loss: 0.00186757
Iteration 20/25 | Loss: 0.00186757
Iteration 21/25 | Loss: 0.00186757
Iteration 22/25 | Loss: 0.00186757
Iteration 23/25 | Loss: 0.00186757
Iteration 24/25 | Loss: 0.00186757
Iteration 25/25 | Loss: 0.00186757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00186757
Iteration 2/1000 | Loss: 0.00010104
Iteration 3/1000 | Loss: 0.00005763
Iteration 4/1000 | Loss: 0.00004951
Iteration 5/1000 | Loss: 0.00004448
Iteration 6/1000 | Loss: 0.00004420
Iteration 7/1000 | Loss: 0.00004159
Iteration 8/1000 | Loss: 0.00004086
Iteration 9/1000 | Loss: 0.00003997
Iteration 10/1000 | Loss: 0.00003890
Iteration 11/1000 | Loss: 0.00004000
Iteration 12/1000 | Loss: 0.00003859
Iteration 13/1000 | Loss: 0.00003808
Iteration 14/1000 | Loss: 0.00003916
Iteration 15/1000 | Loss: 0.00004393
Iteration 16/1000 | Loss: 0.00003762
Iteration 17/1000 | Loss: 0.00004201
Iteration 18/1000 | Loss: 0.00003793
Iteration 19/1000 | Loss: 0.00006823
Iteration 20/1000 | Loss: 0.00003823
Iteration 21/1000 | Loss: 0.00004849
Iteration 22/1000 | Loss: 0.00003723
Iteration 23/1000 | Loss: 0.00006643
Iteration 24/1000 | Loss: 0.00004009
Iteration 25/1000 | Loss: 0.00003879
Iteration 26/1000 | Loss: 0.00003815
Iteration 27/1000 | Loss: 0.00003631
Iteration 28/1000 | Loss: 0.00003631
Iteration 29/1000 | Loss: 0.00003631
Iteration 30/1000 | Loss: 0.00003631
Iteration 31/1000 | Loss: 0.00003631
Iteration 32/1000 | Loss: 0.00003631
Iteration 33/1000 | Loss: 0.00003630
Iteration 34/1000 | Loss: 0.00003630
Iteration 35/1000 | Loss: 0.00003630
Iteration 36/1000 | Loss: 0.00003630
Iteration 37/1000 | Loss: 0.00003630
Iteration 38/1000 | Loss: 0.00003630
Iteration 39/1000 | Loss: 0.00003630
Iteration 40/1000 | Loss: 0.00003727
Iteration 41/1000 | Loss: 0.00003651
Iteration 42/1000 | Loss: 0.00003625
Iteration 43/1000 | Loss: 0.00003625
Iteration 44/1000 | Loss: 0.00003619
Iteration 45/1000 | Loss: 0.00003905
Iteration 46/1000 | Loss: 0.00004328
Iteration 47/1000 | Loss: 0.00003608
Iteration 48/1000 | Loss: 0.00003608
Iteration 49/1000 | Loss: 0.00003608
Iteration 50/1000 | Loss: 0.00003608
Iteration 51/1000 | Loss: 0.00003608
Iteration 52/1000 | Loss: 0.00003608
Iteration 53/1000 | Loss: 0.00003604
Iteration 54/1000 | Loss: 0.00003603
Iteration 55/1000 | Loss: 0.00003603
Iteration 56/1000 | Loss: 0.00003603
Iteration 57/1000 | Loss: 0.00003603
Iteration 58/1000 | Loss: 0.00003602
Iteration 59/1000 | Loss: 0.00003602
Iteration 60/1000 | Loss: 0.00003602
Iteration 61/1000 | Loss: 0.00003602
Iteration 62/1000 | Loss: 0.00003616
Iteration 63/1000 | Loss: 0.00003599
Iteration 64/1000 | Loss: 0.00003598
Iteration 65/1000 | Loss: 0.00003598
Iteration 66/1000 | Loss: 0.00003752
Iteration 67/1000 | Loss: 0.00003752
Iteration 68/1000 | Loss: 0.00003752
Iteration 69/1000 | Loss: 0.00004265
Iteration 70/1000 | Loss: 0.00003939
Iteration 71/1000 | Loss: 0.00004455
Iteration 72/1000 | Loss: 0.00003659
Iteration 73/1000 | Loss: 0.00005120
Iteration 74/1000 | Loss: 0.00003725
Iteration 75/1000 | Loss: 0.00003770
Iteration 76/1000 | Loss: 0.00003593
Iteration 77/1000 | Loss: 0.00003753
Iteration 78/1000 | Loss: 0.00003584
Iteration 79/1000 | Loss: 0.00003584
Iteration 80/1000 | Loss: 0.00003584
Iteration 81/1000 | Loss: 0.00003584
Iteration 82/1000 | Loss: 0.00003584
Iteration 83/1000 | Loss: 0.00003584
Iteration 84/1000 | Loss: 0.00003584
Iteration 85/1000 | Loss: 0.00003583
Iteration 86/1000 | Loss: 0.00003583
Iteration 87/1000 | Loss: 0.00003583
Iteration 88/1000 | Loss: 0.00003583
Iteration 89/1000 | Loss: 0.00003583
Iteration 90/1000 | Loss: 0.00003583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [3.5834738810081035e-05, 3.5834738810081035e-05, 3.5834738810081035e-05, 3.5834738810081035e-05, 3.5834738810081035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5834738810081035e-05

Optimization complete. Final v2v error: 3.343377113342285 mm

Highest mean error: 9.832856178283691 mm for frame 29

Lowest mean error: 2.3436970710754395 mm for frame 50

Saving results

Total time: 102.49506378173828
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823116
Iteration 2/25 | Loss: 0.00159215
Iteration 3/25 | Loss: 0.00129717
Iteration 4/25 | Loss: 0.00127117
Iteration 5/25 | Loss: 0.00126697
Iteration 6/25 | Loss: 0.00127086
Iteration 7/25 | Loss: 0.00126405
Iteration 8/25 | Loss: 0.00126122
Iteration 9/25 | Loss: 0.00126051
Iteration 10/25 | Loss: 0.00126037
Iteration 11/25 | Loss: 0.00126036
Iteration 12/25 | Loss: 0.00126036
Iteration 13/25 | Loss: 0.00126036
Iteration 14/25 | Loss: 0.00126036
Iteration 15/25 | Loss: 0.00126036
Iteration 16/25 | Loss: 0.00126036
Iteration 17/25 | Loss: 0.00126035
Iteration 18/25 | Loss: 0.00126035
Iteration 19/25 | Loss: 0.00126035
Iteration 20/25 | Loss: 0.00126035
Iteration 21/25 | Loss: 0.00126035
Iteration 22/25 | Loss: 0.00126035
Iteration 23/25 | Loss: 0.00126035
Iteration 24/25 | Loss: 0.00126035
Iteration 25/25 | Loss: 0.00126035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26408041
Iteration 2/25 | Loss: 0.00126809
Iteration 3/25 | Loss: 0.00126806
Iteration 4/25 | Loss: 0.00126806
Iteration 5/25 | Loss: 0.00126806
Iteration 6/25 | Loss: 0.00126806
Iteration 7/25 | Loss: 0.00126806
Iteration 8/25 | Loss: 0.00126806
Iteration 9/25 | Loss: 0.00126806
Iteration 10/25 | Loss: 0.00126806
Iteration 11/25 | Loss: 0.00126806
Iteration 12/25 | Loss: 0.00126806
Iteration 13/25 | Loss: 0.00126806
Iteration 14/25 | Loss: 0.00126806
Iteration 15/25 | Loss: 0.00126806
Iteration 16/25 | Loss: 0.00126806
Iteration 17/25 | Loss: 0.00126806
Iteration 18/25 | Loss: 0.00126806
Iteration 19/25 | Loss: 0.00126806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012680619256570935, 0.0012680619256570935, 0.0012680619256570935, 0.0012680619256570935, 0.0012680619256570935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012680619256570935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126806
Iteration 2/1000 | Loss: 0.00003489
Iteration 3/1000 | Loss: 0.00002656
Iteration 4/1000 | Loss: 0.00002412
Iteration 5/1000 | Loss: 0.00002269
Iteration 6/1000 | Loss: 0.00002176
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00002067
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00001983
Iteration 11/1000 | Loss: 0.00001955
Iteration 12/1000 | Loss: 0.00001932
Iteration 13/1000 | Loss: 0.00001910
Iteration 14/1000 | Loss: 0.00001906
Iteration 15/1000 | Loss: 0.00001900
Iteration 16/1000 | Loss: 0.00001900
Iteration 17/1000 | Loss: 0.00001898
Iteration 18/1000 | Loss: 0.00001893
Iteration 19/1000 | Loss: 0.00001888
Iteration 20/1000 | Loss: 0.00001883
Iteration 21/1000 | Loss: 0.00001881
Iteration 22/1000 | Loss: 0.00001878
Iteration 23/1000 | Loss: 0.00001877
Iteration 24/1000 | Loss: 0.00001876
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001874
Iteration 27/1000 | Loss: 0.00001874
Iteration 28/1000 | Loss: 0.00001869
Iteration 29/1000 | Loss: 0.00001868
Iteration 30/1000 | Loss: 0.00001866
Iteration 31/1000 | Loss: 0.00001865
Iteration 32/1000 | Loss: 0.00001865
Iteration 33/1000 | Loss: 0.00001864
Iteration 34/1000 | Loss: 0.00001864
Iteration 35/1000 | Loss: 0.00001864
Iteration 36/1000 | Loss: 0.00001863
Iteration 37/1000 | Loss: 0.00001862
Iteration 38/1000 | Loss: 0.00001861
Iteration 39/1000 | Loss: 0.00001861
Iteration 40/1000 | Loss: 0.00001859
Iteration 41/1000 | Loss: 0.00001858
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001858
Iteration 44/1000 | Loss: 0.00001857
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001854
Iteration 50/1000 | Loss: 0.00001854
Iteration 51/1000 | Loss: 0.00001853
Iteration 52/1000 | Loss: 0.00001852
Iteration 53/1000 | Loss: 0.00001852
Iteration 54/1000 | Loss: 0.00001852
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001851
Iteration 58/1000 | Loss: 0.00001851
Iteration 59/1000 | Loss: 0.00001851
Iteration 60/1000 | Loss: 0.00001851
Iteration 61/1000 | Loss: 0.00001850
Iteration 62/1000 | Loss: 0.00001850
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001848
Iteration 67/1000 | Loss: 0.00001847
Iteration 68/1000 | Loss: 0.00001847
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001845
Iteration 73/1000 | Loss: 0.00001845
Iteration 74/1000 | Loss: 0.00001845
Iteration 75/1000 | Loss: 0.00001844
Iteration 76/1000 | Loss: 0.00001844
Iteration 77/1000 | Loss: 0.00001844
Iteration 78/1000 | Loss: 0.00001843
Iteration 79/1000 | Loss: 0.00001843
Iteration 80/1000 | Loss: 0.00001843
Iteration 81/1000 | Loss: 0.00001842
Iteration 82/1000 | Loss: 0.00001842
Iteration 83/1000 | Loss: 0.00001842
Iteration 84/1000 | Loss: 0.00001841
Iteration 85/1000 | Loss: 0.00001841
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001840
Iteration 94/1000 | Loss: 0.00001839
Iteration 95/1000 | Loss: 0.00001839
Iteration 96/1000 | Loss: 0.00001839
Iteration 97/1000 | Loss: 0.00001839
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001838
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001836
Iteration 107/1000 | Loss: 0.00001836
Iteration 108/1000 | Loss: 0.00001836
Iteration 109/1000 | Loss: 0.00001836
Iteration 110/1000 | Loss: 0.00001836
Iteration 111/1000 | Loss: 0.00001836
Iteration 112/1000 | Loss: 0.00001835
Iteration 113/1000 | Loss: 0.00001835
Iteration 114/1000 | Loss: 0.00001835
Iteration 115/1000 | Loss: 0.00001835
Iteration 116/1000 | Loss: 0.00001835
Iteration 117/1000 | Loss: 0.00001835
Iteration 118/1000 | Loss: 0.00001835
Iteration 119/1000 | Loss: 0.00001835
Iteration 120/1000 | Loss: 0.00001835
Iteration 121/1000 | Loss: 0.00001835
Iteration 122/1000 | Loss: 0.00001835
Iteration 123/1000 | Loss: 0.00001835
Iteration 124/1000 | Loss: 0.00001835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.8345652279094793e-05, 1.8345652279094793e-05, 1.8345652279094793e-05, 1.8345652279094793e-05, 1.8345652279094793e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8345652279094793e-05

Optimization complete. Final v2v error: 3.568894624710083 mm

Highest mean error: 4.519386291503906 mm for frame 31

Lowest mean error: 2.946357250213623 mm for frame 72

Saving results

Total time: 55.28993892669678
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516759
Iteration 2/25 | Loss: 0.00164850
Iteration 3/25 | Loss: 0.00132144
Iteration 4/25 | Loss: 0.00130111
Iteration 5/25 | Loss: 0.00129550
Iteration 6/25 | Loss: 0.00129367
Iteration 7/25 | Loss: 0.00129325
Iteration 8/25 | Loss: 0.00129325
Iteration 9/25 | Loss: 0.00129325
Iteration 10/25 | Loss: 0.00129325
Iteration 11/25 | Loss: 0.00129325
Iteration 12/25 | Loss: 0.00129325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012932511745020747, 0.0012932511745020747, 0.0012932511745020747, 0.0012932511745020747, 0.0012932511745020747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012932511745020747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24307239
Iteration 2/25 | Loss: 0.00132522
Iteration 3/25 | Loss: 0.00132520
Iteration 4/25 | Loss: 0.00132520
Iteration 5/25 | Loss: 0.00132520
Iteration 6/25 | Loss: 0.00132520
Iteration 7/25 | Loss: 0.00132520
Iteration 8/25 | Loss: 0.00132520
Iteration 9/25 | Loss: 0.00132520
Iteration 10/25 | Loss: 0.00132520
Iteration 11/25 | Loss: 0.00132520
Iteration 12/25 | Loss: 0.00132520
Iteration 13/25 | Loss: 0.00132520
Iteration 14/25 | Loss: 0.00132520
Iteration 15/25 | Loss: 0.00132520
Iteration 16/25 | Loss: 0.00132520
Iteration 17/25 | Loss: 0.00132520
Iteration 18/25 | Loss: 0.00132520
Iteration 19/25 | Loss: 0.00132520
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013251997297629714, 0.0013251997297629714, 0.0013251997297629714, 0.0013251997297629714, 0.0013251997297629714]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013251997297629714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132520
Iteration 2/1000 | Loss: 0.00005990
Iteration 3/1000 | Loss: 0.00003782
Iteration 4/1000 | Loss: 0.00002746
Iteration 5/1000 | Loss: 0.00002536
Iteration 6/1000 | Loss: 0.00002404
Iteration 7/1000 | Loss: 0.00002331
Iteration 8/1000 | Loss: 0.00002266
Iteration 9/1000 | Loss: 0.00002212
Iteration 10/1000 | Loss: 0.00002177
Iteration 11/1000 | Loss: 0.00002148
Iteration 12/1000 | Loss: 0.00002124
Iteration 13/1000 | Loss: 0.00002096
Iteration 14/1000 | Loss: 0.00002075
Iteration 15/1000 | Loss: 0.00002054
Iteration 16/1000 | Loss: 0.00002041
Iteration 17/1000 | Loss: 0.00002036
Iteration 18/1000 | Loss: 0.00002024
Iteration 19/1000 | Loss: 0.00002018
Iteration 20/1000 | Loss: 0.00002016
Iteration 21/1000 | Loss: 0.00002013
Iteration 22/1000 | Loss: 0.00002012
Iteration 23/1000 | Loss: 0.00002008
Iteration 24/1000 | Loss: 0.00002007
Iteration 25/1000 | Loss: 0.00002006
Iteration 26/1000 | Loss: 0.00002006
Iteration 27/1000 | Loss: 0.00002002
Iteration 28/1000 | Loss: 0.00002000
Iteration 29/1000 | Loss: 0.00001995
Iteration 30/1000 | Loss: 0.00001995
Iteration 31/1000 | Loss: 0.00001990
Iteration 32/1000 | Loss: 0.00001990
Iteration 33/1000 | Loss: 0.00001988
Iteration 34/1000 | Loss: 0.00001988
Iteration 35/1000 | Loss: 0.00001988
Iteration 36/1000 | Loss: 0.00001988
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001987
Iteration 40/1000 | Loss: 0.00001987
Iteration 41/1000 | Loss: 0.00001987
Iteration 42/1000 | Loss: 0.00001987
Iteration 43/1000 | Loss: 0.00001987
Iteration 44/1000 | Loss: 0.00001987
Iteration 45/1000 | Loss: 0.00001986
Iteration 46/1000 | Loss: 0.00001986
Iteration 47/1000 | Loss: 0.00001985
Iteration 48/1000 | Loss: 0.00001985
Iteration 49/1000 | Loss: 0.00001985
Iteration 50/1000 | Loss: 0.00001984
Iteration 51/1000 | Loss: 0.00001984
Iteration 52/1000 | Loss: 0.00001984
Iteration 53/1000 | Loss: 0.00001984
Iteration 54/1000 | Loss: 0.00001984
Iteration 55/1000 | Loss: 0.00001983
Iteration 56/1000 | Loss: 0.00001983
Iteration 57/1000 | Loss: 0.00001983
Iteration 58/1000 | Loss: 0.00001983
Iteration 59/1000 | Loss: 0.00001983
Iteration 60/1000 | Loss: 0.00001983
Iteration 61/1000 | Loss: 0.00001983
Iteration 62/1000 | Loss: 0.00001982
Iteration 63/1000 | Loss: 0.00001982
Iteration 64/1000 | Loss: 0.00001982
Iteration 65/1000 | Loss: 0.00001982
Iteration 66/1000 | Loss: 0.00001982
Iteration 67/1000 | Loss: 0.00001982
Iteration 68/1000 | Loss: 0.00001982
Iteration 69/1000 | Loss: 0.00001982
Iteration 70/1000 | Loss: 0.00001982
Iteration 71/1000 | Loss: 0.00001981
Iteration 72/1000 | Loss: 0.00001981
Iteration 73/1000 | Loss: 0.00001981
Iteration 74/1000 | Loss: 0.00001981
Iteration 75/1000 | Loss: 0.00001981
Iteration 76/1000 | Loss: 0.00001981
Iteration 77/1000 | Loss: 0.00001981
Iteration 78/1000 | Loss: 0.00001981
Iteration 79/1000 | Loss: 0.00001981
Iteration 80/1000 | Loss: 0.00001981
Iteration 81/1000 | Loss: 0.00001981
Iteration 82/1000 | Loss: 0.00001981
Iteration 83/1000 | Loss: 0.00001981
Iteration 84/1000 | Loss: 0.00001980
Iteration 85/1000 | Loss: 0.00001980
Iteration 86/1000 | Loss: 0.00001980
Iteration 87/1000 | Loss: 0.00001980
Iteration 88/1000 | Loss: 0.00001980
Iteration 89/1000 | Loss: 0.00001980
Iteration 90/1000 | Loss: 0.00001979
Iteration 91/1000 | Loss: 0.00001979
Iteration 92/1000 | Loss: 0.00001979
Iteration 93/1000 | Loss: 0.00001979
Iteration 94/1000 | Loss: 0.00001979
Iteration 95/1000 | Loss: 0.00001979
Iteration 96/1000 | Loss: 0.00001979
Iteration 97/1000 | Loss: 0.00001979
Iteration 98/1000 | Loss: 0.00001979
Iteration 99/1000 | Loss: 0.00001978
Iteration 100/1000 | Loss: 0.00001978
Iteration 101/1000 | Loss: 0.00001978
Iteration 102/1000 | Loss: 0.00001978
Iteration 103/1000 | Loss: 0.00001978
Iteration 104/1000 | Loss: 0.00001977
Iteration 105/1000 | Loss: 0.00001977
Iteration 106/1000 | Loss: 0.00001977
Iteration 107/1000 | Loss: 0.00001977
Iteration 108/1000 | Loss: 0.00001977
Iteration 109/1000 | Loss: 0.00001977
Iteration 110/1000 | Loss: 0.00001976
Iteration 111/1000 | Loss: 0.00001976
Iteration 112/1000 | Loss: 0.00001976
Iteration 113/1000 | Loss: 0.00001976
Iteration 114/1000 | Loss: 0.00001976
Iteration 115/1000 | Loss: 0.00001976
Iteration 116/1000 | Loss: 0.00001976
Iteration 117/1000 | Loss: 0.00001976
Iteration 118/1000 | Loss: 0.00001976
Iteration 119/1000 | Loss: 0.00001976
Iteration 120/1000 | Loss: 0.00001976
Iteration 121/1000 | Loss: 0.00001976
Iteration 122/1000 | Loss: 0.00001976
Iteration 123/1000 | Loss: 0.00001976
Iteration 124/1000 | Loss: 0.00001976
Iteration 125/1000 | Loss: 0.00001975
Iteration 126/1000 | Loss: 0.00001975
Iteration 127/1000 | Loss: 0.00001975
Iteration 128/1000 | Loss: 0.00001975
Iteration 129/1000 | Loss: 0.00001975
Iteration 130/1000 | Loss: 0.00001975
Iteration 131/1000 | Loss: 0.00001975
Iteration 132/1000 | Loss: 0.00001975
Iteration 133/1000 | Loss: 0.00001975
Iteration 134/1000 | Loss: 0.00001975
Iteration 135/1000 | Loss: 0.00001975
Iteration 136/1000 | Loss: 0.00001975
Iteration 137/1000 | Loss: 0.00001975
Iteration 138/1000 | Loss: 0.00001975
Iteration 139/1000 | Loss: 0.00001975
Iteration 140/1000 | Loss: 0.00001975
Iteration 141/1000 | Loss: 0.00001975
Iteration 142/1000 | Loss: 0.00001974
Iteration 143/1000 | Loss: 0.00001974
Iteration 144/1000 | Loss: 0.00001974
Iteration 145/1000 | Loss: 0.00001974
Iteration 146/1000 | Loss: 0.00001974
Iteration 147/1000 | Loss: 0.00001974
Iteration 148/1000 | Loss: 0.00001974
Iteration 149/1000 | Loss: 0.00001974
Iteration 150/1000 | Loss: 0.00001974
Iteration 151/1000 | Loss: 0.00001974
Iteration 152/1000 | Loss: 0.00001974
Iteration 153/1000 | Loss: 0.00001974
Iteration 154/1000 | Loss: 0.00001974
Iteration 155/1000 | Loss: 0.00001974
Iteration 156/1000 | Loss: 0.00001973
Iteration 157/1000 | Loss: 0.00001973
Iteration 158/1000 | Loss: 0.00001973
Iteration 159/1000 | Loss: 0.00001973
Iteration 160/1000 | Loss: 0.00001973
Iteration 161/1000 | Loss: 0.00001973
Iteration 162/1000 | Loss: 0.00001973
Iteration 163/1000 | Loss: 0.00001973
Iteration 164/1000 | Loss: 0.00001973
Iteration 165/1000 | Loss: 0.00001973
Iteration 166/1000 | Loss: 0.00001973
Iteration 167/1000 | Loss: 0.00001973
Iteration 168/1000 | Loss: 0.00001973
Iteration 169/1000 | Loss: 0.00001973
Iteration 170/1000 | Loss: 0.00001973
Iteration 171/1000 | Loss: 0.00001973
Iteration 172/1000 | Loss: 0.00001973
Iteration 173/1000 | Loss: 0.00001973
Iteration 174/1000 | Loss: 0.00001973
Iteration 175/1000 | Loss: 0.00001973
Iteration 176/1000 | Loss: 0.00001972
Iteration 177/1000 | Loss: 0.00001972
Iteration 178/1000 | Loss: 0.00001972
Iteration 179/1000 | Loss: 0.00001972
Iteration 180/1000 | Loss: 0.00001972
Iteration 181/1000 | Loss: 0.00001972
Iteration 182/1000 | Loss: 0.00001972
Iteration 183/1000 | Loss: 0.00001972
Iteration 184/1000 | Loss: 0.00001972
Iteration 185/1000 | Loss: 0.00001972
Iteration 186/1000 | Loss: 0.00001972
Iteration 187/1000 | Loss: 0.00001972
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.972142854356207e-05, 1.972142854356207e-05, 1.972142854356207e-05, 1.972142854356207e-05, 1.972142854356207e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.972142854356207e-05

Optimization complete. Final v2v error: 3.6153128147125244 mm

Highest mean error: 5.61697244644165 mm for frame 58

Lowest mean error: 2.716750383377075 mm for frame 3

Saving results

Total time: 49.50078010559082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00626337
Iteration 2/25 | Loss: 0.00137617
Iteration 3/25 | Loss: 0.00124733
Iteration 4/25 | Loss: 0.00122460
Iteration 5/25 | Loss: 0.00121781
Iteration 6/25 | Loss: 0.00121608
Iteration 7/25 | Loss: 0.00121608
Iteration 8/25 | Loss: 0.00121608
Iteration 9/25 | Loss: 0.00121608
Iteration 10/25 | Loss: 0.00121608
Iteration 11/25 | Loss: 0.00121608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001216075848788023, 0.001216075848788023, 0.001216075848788023, 0.001216075848788023, 0.001216075848788023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001216075848788023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30909276
Iteration 2/25 | Loss: 0.00196367
Iteration 3/25 | Loss: 0.00196367
Iteration 4/25 | Loss: 0.00196367
Iteration 5/25 | Loss: 0.00196367
Iteration 6/25 | Loss: 0.00196367
Iteration 7/25 | Loss: 0.00196367
Iteration 8/25 | Loss: 0.00196367
Iteration 9/25 | Loss: 0.00196367
Iteration 10/25 | Loss: 0.00196367
Iteration 11/25 | Loss: 0.00196367
Iteration 12/25 | Loss: 0.00196367
Iteration 13/25 | Loss: 0.00196367
Iteration 14/25 | Loss: 0.00196367
Iteration 15/25 | Loss: 0.00196367
Iteration 16/25 | Loss: 0.00196367
Iteration 17/25 | Loss: 0.00196367
Iteration 18/25 | Loss: 0.00196367
Iteration 19/25 | Loss: 0.00196367
Iteration 20/25 | Loss: 0.00196367
Iteration 21/25 | Loss: 0.00196367
Iteration 22/25 | Loss: 0.00196367
Iteration 23/25 | Loss: 0.00196367
Iteration 24/25 | Loss: 0.00196367
Iteration 25/25 | Loss: 0.00196367

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196367
Iteration 2/1000 | Loss: 0.00004156
Iteration 3/1000 | Loss: 0.00002600
Iteration 4/1000 | Loss: 0.00001873
Iteration 5/1000 | Loss: 0.00001658
Iteration 6/1000 | Loss: 0.00001511
Iteration 7/1000 | Loss: 0.00001402
Iteration 8/1000 | Loss: 0.00001333
Iteration 9/1000 | Loss: 0.00001280
Iteration 10/1000 | Loss: 0.00001250
Iteration 11/1000 | Loss: 0.00001247
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001212
Iteration 14/1000 | Loss: 0.00001207
Iteration 15/1000 | Loss: 0.00001193
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001185
Iteration 18/1000 | Loss: 0.00001179
Iteration 19/1000 | Loss: 0.00001179
Iteration 20/1000 | Loss: 0.00001178
Iteration 21/1000 | Loss: 0.00001175
Iteration 22/1000 | Loss: 0.00001171
Iteration 23/1000 | Loss: 0.00001170
Iteration 24/1000 | Loss: 0.00001168
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001168
Iteration 27/1000 | Loss: 0.00001165
Iteration 28/1000 | Loss: 0.00001161
Iteration 29/1000 | Loss: 0.00001161
Iteration 30/1000 | Loss: 0.00001160
Iteration 31/1000 | Loss: 0.00001160
Iteration 32/1000 | Loss: 0.00001159
Iteration 33/1000 | Loss: 0.00001159
Iteration 34/1000 | Loss: 0.00001159
Iteration 35/1000 | Loss: 0.00001158
Iteration 36/1000 | Loss: 0.00001158
Iteration 37/1000 | Loss: 0.00001157
Iteration 38/1000 | Loss: 0.00001157
Iteration 39/1000 | Loss: 0.00001156
Iteration 40/1000 | Loss: 0.00001156
Iteration 41/1000 | Loss: 0.00001156
Iteration 42/1000 | Loss: 0.00001155
Iteration 43/1000 | Loss: 0.00001155
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001154
Iteration 46/1000 | Loss: 0.00001154
Iteration 47/1000 | Loss: 0.00001154
Iteration 48/1000 | Loss: 0.00001154
Iteration 49/1000 | Loss: 0.00001154
Iteration 50/1000 | Loss: 0.00001153
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001152
Iteration 57/1000 | Loss: 0.00001152
Iteration 58/1000 | Loss: 0.00001151
Iteration 59/1000 | Loss: 0.00001151
Iteration 60/1000 | Loss: 0.00001151
Iteration 61/1000 | Loss: 0.00001151
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001149
Iteration 66/1000 | Loss: 0.00001149
Iteration 67/1000 | Loss: 0.00001149
Iteration 68/1000 | Loss: 0.00001149
Iteration 69/1000 | Loss: 0.00001149
Iteration 70/1000 | Loss: 0.00001149
Iteration 71/1000 | Loss: 0.00001148
Iteration 72/1000 | Loss: 0.00001148
Iteration 73/1000 | Loss: 0.00001148
Iteration 74/1000 | Loss: 0.00001148
Iteration 75/1000 | Loss: 0.00001147
Iteration 76/1000 | Loss: 0.00001147
Iteration 77/1000 | Loss: 0.00001147
Iteration 78/1000 | Loss: 0.00001147
Iteration 79/1000 | Loss: 0.00001147
Iteration 80/1000 | Loss: 0.00001147
Iteration 81/1000 | Loss: 0.00001146
Iteration 82/1000 | Loss: 0.00001146
Iteration 83/1000 | Loss: 0.00001146
Iteration 84/1000 | Loss: 0.00001146
Iteration 85/1000 | Loss: 0.00001146
Iteration 86/1000 | Loss: 0.00001146
Iteration 87/1000 | Loss: 0.00001146
Iteration 88/1000 | Loss: 0.00001145
Iteration 89/1000 | Loss: 0.00001145
Iteration 90/1000 | Loss: 0.00001145
Iteration 91/1000 | Loss: 0.00001145
Iteration 92/1000 | Loss: 0.00001145
Iteration 93/1000 | Loss: 0.00001144
Iteration 94/1000 | Loss: 0.00001144
Iteration 95/1000 | Loss: 0.00001144
Iteration 96/1000 | Loss: 0.00001144
Iteration 97/1000 | Loss: 0.00001144
Iteration 98/1000 | Loss: 0.00001144
Iteration 99/1000 | Loss: 0.00001144
Iteration 100/1000 | Loss: 0.00001144
Iteration 101/1000 | Loss: 0.00001143
Iteration 102/1000 | Loss: 0.00001143
Iteration 103/1000 | Loss: 0.00001142
Iteration 104/1000 | Loss: 0.00001142
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001142
Iteration 107/1000 | Loss: 0.00001142
Iteration 108/1000 | Loss: 0.00001142
Iteration 109/1000 | Loss: 0.00001141
Iteration 110/1000 | Loss: 0.00001141
Iteration 111/1000 | Loss: 0.00001141
Iteration 112/1000 | Loss: 0.00001140
Iteration 113/1000 | Loss: 0.00001140
Iteration 114/1000 | Loss: 0.00001140
Iteration 115/1000 | Loss: 0.00001140
Iteration 116/1000 | Loss: 0.00001139
Iteration 117/1000 | Loss: 0.00001139
Iteration 118/1000 | Loss: 0.00001139
Iteration 119/1000 | Loss: 0.00001139
Iteration 120/1000 | Loss: 0.00001139
Iteration 121/1000 | Loss: 0.00001139
Iteration 122/1000 | Loss: 0.00001138
Iteration 123/1000 | Loss: 0.00001138
Iteration 124/1000 | Loss: 0.00001138
Iteration 125/1000 | Loss: 0.00001138
Iteration 126/1000 | Loss: 0.00001138
Iteration 127/1000 | Loss: 0.00001138
Iteration 128/1000 | Loss: 0.00001137
Iteration 129/1000 | Loss: 0.00001137
Iteration 130/1000 | Loss: 0.00001137
Iteration 131/1000 | Loss: 0.00001137
Iteration 132/1000 | Loss: 0.00001137
Iteration 133/1000 | Loss: 0.00001137
Iteration 134/1000 | Loss: 0.00001137
Iteration 135/1000 | Loss: 0.00001137
Iteration 136/1000 | Loss: 0.00001137
Iteration 137/1000 | Loss: 0.00001137
Iteration 138/1000 | Loss: 0.00001137
Iteration 139/1000 | Loss: 0.00001136
Iteration 140/1000 | Loss: 0.00001136
Iteration 141/1000 | Loss: 0.00001136
Iteration 142/1000 | Loss: 0.00001136
Iteration 143/1000 | Loss: 0.00001136
Iteration 144/1000 | Loss: 0.00001136
Iteration 145/1000 | Loss: 0.00001136
Iteration 146/1000 | Loss: 0.00001136
Iteration 147/1000 | Loss: 0.00001136
Iteration 148/1000 | Loss: 0.00001135
Iteration 149/1000 | Loss: 0.00001135
Iteration 150/1000 | Loss: 0.00001135
Iteration 151/1000 | Loss: 0.00001135
Iteration 152/1000 | Loss: 0.00001135
Iteration 153/1000 | Loss: 0.00001134
Iteration 154/1000 | Loss: 0.00001134
Iteration 155/1000 | Loss: 0.00001134
Iteration 156/1000 | Loss: 0.00001134
Iteration 157/1000 | Loss: 0.00001134
Iteration 158/1000 | Loss: 0.00001134
Iteration 159/1000 | Loss: 0.00001133
Iteration 160/1000 | Loss: 0.00001133
Iteration 161/1000 | Loss: 0.00001133
Iteration 162/1000 | Loss: 0.00001133
Iteration 163/1000 | Loss: 0.00001133
Iteration 164/1000 | Loss: 0.00001133
Iteration 165/1000 | Loss: 0.00001133
Iteration 166/1000 | Loss: 0.00001133
Iteration 167/1000 | Loss: 0.00001133
Iteration 168/1000 | Loss: 0.00001133
Iteration 169/1000 | Loss: 0.00001133
Iteration 170/1000 | Loss: 0.00001132
Iteration 171/1000 | Loss: 0.00001132
Iteration 172/1000 | Loss: 0.00001132
Iteration 173/1000 | Loss: 0.00001132
Iteration 174/1000 | Loss: 0.00001132
Iteration 175/1000 | Loss: 0.00001132
Iteration 176/1000 | Loss: 0.00001131
Iteration 177/1000 | Loss: 0.00001131
Iteration 178/1000 | Loss: 0.00001131
Iteration 179/1000 | Loss: 0.00001131
Iteration 180/1000 | Loss: 0.00001131
Iteration 181/1000 | Loss: 0.00001131
Iteration 182/1000 | Loss: 0.00001131
Iteration 183/1000 | Loss: 0.00001130
Iteration 184/1000 | Loss: 0.00001130
Iteration 185/1000 | Loss: 0.00001130
Iteration 186/1000 | Loss: 0.00001130
Iteration 187/1000 | Loss: 0.00001129
Iteration 188/1000 | Loss: 0.00001129
Iteration 189/1000 | Loss: 0.00001129
Iteration 190/1000 | Loss: 0.00001129
Iteration 191/1000 | Loss: 0.00001129
Iteration 192/1000 | Loss: 0.00001129
Iteration 193/1000 | Loss: 0.00001129
Iteration 194/1000 | Loss: 0.00001129
Iteration 195/1000 | Loss: 0.00001128
Iteration 196/1000 | Loss: 0.00001128
Iteration 197/1000 | Loss: 0.00001128
Iteration 198/1000 | Loss: 0.00001128
Iteration 199/1000 | Loss: 0.00001128
Iteration 200/1000 | Loss: 0.00001128
Iteration 201/1000 | Loss: 0.00001128
Iteration 202/1000 | Loss: 0.00001127
Iteration 203/1000 | Loss: 0.00001127
Iteration 204/1000 | Loss: 0.00001127
Iteration 205/1000 | Loss: 0.00001127
Iteration 206/1000 | Loss: 0.00001127
Iteration 207/1000 | Loss: 0.00001127
Iteration 208/1000 | Loss: 0.00001127
Iteration 209/1000 | Loss: 0.00001127
Iteration 210/1000 | Loss: 0.00001127
Iteration 211/1000 | Loss: 0.00001127
Iteration 212/1000 | Loss: 0.00001126
Iteration 213/1000 | Loss: 0.00001126
Iteration 214/1000 | Loss: 0.00001126
Iteration 215/1000 | Loss: 0.00001126
Iteration 216/1000 | Loss: 0.00001125
Iteration 217/1000 | Loss: 0.00001125
Iteration 218/1000 | Loss: 0.00001125
Iteration 219/1000 | Loss: 0.00001125
Iteration 220/1000 | Loss: 0.00001124
Iteration 221/1000 | Loss: 0.00001124
Iteration 222/1000 | Loss: 0.00001124
Iteration 223/1000 | Loss: 0.00001124
Iteration 224/1000 | Loss: 0.00001123
Iteration 225/1000 | Loss: 0.00001123
Iteration 226/1000 | Loss: 0.00001123
Iteration 227/1000 | Loss: 0.00001123
Iteration 228/1000 | Loss: 0.00001123
Iteration 229/1000 | Loss: 0.00001123
Iteration 230/1000 | Loss: 0.00001123
Iteration 231/1000 | Loss: 0.00001122
Iteration 232/1000 | Loss: 0.00001122
Iteration 233/1000 | Loss: 0.00001122
Iteration 234/1000 | Loss: 0.00001122
Iteration 235/1000 | Loss: 0.00001122
Iteration 236/1000 | Loss: 0.00001122
Iteration 237/1000 | Loss: 0.00001121
Iteration 238/1000 | Loss: 0.00001121
Iteration 239/1000 | Loss: 0.00001121
Iteration 240/1000 | Loss: 0.00001121
Iteration 241/1000 | Loss: 0.00001121
Iteration 242/1000 | Loss: 0.00001121
Iteration 243/1000 | Loss: 0.00001121
Iteration 244/1000 | Loss: 0.00001121
Iteration 245/1000 | Loss: 0.00001120
Iteration 246/1000 | Loss: 0.00001120
Iteration 247/1000 | Loss: 0.00001120
Iteration 248/1000 | Loss: 0.00001120
Iteration 249/1000 | Loss: 0.00001120
Iteration 250/1000 | Loss: 0.00001120
Iteration 251/1000 | Loss: 0.00001120
Iteration 252/1000 | Loss: 0.00001120
Iteration 253/1000 | Loss: 0.00001119
Iteration 254/1000 | Loss: 0.00001119
Iteration 255/1000 | Loss: 0.00001119
Iteration 256/1000 | Loss: 0.00001119
Iteration 257/1000 | Loss: 0.00001119
Iteration 258/1000 | Loss: 0.00001119
Iteration 259/1000 | Loss: 0.00001119
Iteration 260/1000 | Loss: 0.00001118
Iteration 261/1000 | Loss: 0.00001118
Iteration 262/1000 | Loss: 0.00001118
Iteration 263/1000 | Loss: 0.00001118
Iteration 264/1000 | Loss: 0.00001118
Iteration 265/1000 | Loss: 0.00001118
Iteration 266/1000 | Loss: 0.00001118
Iteration 267/1000 | Loss: 0.00001118
Iteration 268/1000 | Loss: 0.00001118
Iteration 269/1000 | Loss: 0.00001118
Iteration 270/1000 | Loss: 0.00001117
Iteration 271/1000 | Loss: 0.00001117
Iteration 272/1000 | Loss: 0.00001117
Iteration 273/1000 | Loss: 0.00001117
Iteration 274/1000 | Loss: 0.00001117
Iteration 275/1000 | Loss: 0.00001117
Iteration 276/1000 | Loss: 0.00001117
Iteration 277/1000 | Loss: 0.00001117
Iteration 278/1000 | Loss: 0.00001117
Iteration 279/1000 | Loss: 0.00001117
Iteration 280/1000 | Loss: 0.00001117
Iteration 281/1000 | Loss: 0.00001117
Iteration 282/1000 | Loss: 0.00001117
Iteration 283/1000 | Loss: 0.00001116
Iteration 284/1000 | Loss: 0.00001116
Iteration 285/1000 | Loss: 0.00001116
Iteration 286/1000 | Loss: 0.00001116
Iteration 287/1000 | Loss: 0.00001116
Iteration 288/1000 | Loss: 0.00001116
Iteration 289/1000 | Loss: 0.00001116
Iteration 290/1000 | Loss: 0.00001116
Iteration 291/1000 | Loss: 0.00001116
Iteration 292/1000 | Loss: 0.00001116
Iteration 293/1000 | Loss: 0.00001116
Iteration 294/1000 | Loss: 0.00001116
Iteration 295/1000 | Loss: 0.00001116
Iteration 296/1000 | Loss: 0.00001116
Iteration 297/1000 | Loss: 0.00001116
Iteration 298/1000 | Loss: 0.00001115
Iteration 299/1000 | Loss: 0.00001115
Iteration 300/1000 | Loss: 0.00001115
Iteration 301/1000 | Loss: 0.00001115
Iteration 302/1000 | Loss: 0.00001115
Iteration 303/1000 | Loss: 0.00001115
Iteration 304/1000 | Loss: 0.00001115
Iteration 305/1000 | Loss: 0.00001115
Iteration 306/1000 | Loss: 0.00001115
Iteration 307/1000 | Loss: 0.00001115
Iteration 308/1000 | Loss: 0.00001114
Iteration 309/1000 | Loss: 0.00001114
Iteration 310/1000 | Loss: 0.00001114
Iteration 311/1000 | Loss: 0.00001114
Iteration 312/1000 | Loss: 0.00001114
Iteration 313/1000 | Loss: 0.00001114
Iteration 314/1000 | Loss: 0.00001114
Iteration 315/1000 | Loss: 0.00001114
Iteration 316/1000 | Loss: 0.00001114
Iteration 317/1000 | Loss: 0.00001114
Iteration 318/1000 | Loss: 0.00001113
Iteration 319/1000 | Loss: 0.00001113
Iteration 320/1000 | Loss: 0.00001113
Iteration 321/1000 | Loss: 0.00001113
Iteration 322/1000 | Loss: 0.00001113
Iteration 323/1000 | Loss: 0.00001113
Iteration 324/1000 | Loss: 0.00001113
Iteration 325/1000 | Loss: 0.00001113
Iteration 326/1000 | Loss: 0.00001113
Iteration 327/1000 | Loss: 0.00001113
Iteration 328/1000 | Loss: 0.00001112
Iteration 329/1000 | Loss: 0.00001112
Iteration 330/1000 | Loss: 0.00001112
Iteration 331/1000 | Loss: 0.00001112
Iteration 332/1000 | Loss: 0.00001112
Iteration 333/1000 | Loss: 0.00001112
Iteration 334/1000 | Loss: 0.00001112
Iteration 335/1000 | Loss: 0.00001112
Iteration 336/1000 | Loss: 0.00001112
Iteration 337/1000 | Loss: 0.00001112
Iteration 338/1000 | Loss: 0.00001112
Iteration 339/1000 | Loss: 0.00001112
Iteration 340/1000 | Loss: 0.00001112
Iteration 341/1000 | Loss: 0.00001112
Iteration 342/1000 | Loss: 0.00001112
Iteration 343/1000 | Loss: 0.00001112
Iteration 344/1000 | Loss: 0.00001112
Iteration 345/1000 | Loss: 0.00001112
Iteration 346/1000 | Loss: 0.00001112
Iteration 347/1000 | Loss: 0.00001112
Iteration 348/1000 | Loss: 0.00001112
Iteration 349/1000 | Loss: 0.00001112
Iteration 350/1000 | Loss: 0.00001112
Iteration 351/1000 | Loss: 0.00001112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 351. Stopping optimization.
Last 5 losses: [1.1118140719190706e-05, 1.1118140719190706e-05, 1.1118140719190706e-05, 1.1118140719190706e-05, 1.1118140719190706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1118140719190706e-05

Optimization complete. Final v2v error: 2.83447265625 mm

Highest mean error: 3.448831558227539 mm for frame 74

Lowest mean error: 2.4690613746643066 mm for frame 31

Saving results

Total time: 54.16466450691223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765799
Iteration 2/25 | Loss: 0.00185411
Iteration 3/25 | Loss: 0.00136802
Iteration 4/25 | Loss: 0.00130846
Iteration 5/25 | Loss: 0.00132213
Iteration 6/25 | Loss: 0.00130561
Iteration 7/25 | Loss: 0.00130169
Iteration 8/25 | Loss: 0.00129414
Iteration 9/25 | Loss: 0.00128981
Iteration 10/25 | Loss: 0.00128886
Iteration 11/25 | Loss: 0.00128868
Iteration 12/25 | Loss: 0.00128865
Iteration 13/25 | Loss: 0.00128865
Iteration 14/25 | Loss: 0.00128864
Iteration 15/25 | Loss: 0.00128864
Iteration 16/25 | Loss: 0.00128864
Iteration 17/25 | Loss: 0.00128864
Iteration 18/25 | Loss: 0.00128864
Iteration 19/25 | Loss: 0.00128864
Iteration 20/25 | Loss: 0.00128864
Iteration 21/25 | Loss: 0.00128864
Iteration 22/25 | Loss: 0.00128864
Iteration 23/25 | Loss: 0.00128864
Iteration 24/25 | Loss: 0.00128864
Iteration 25/25 | Loss: 0.00128864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43002105
Iteration 2/25 | Loss: 0.00104483
Iteration 3/25 | Loss: 0.00104483
Iteration 4/25 | Loss: 0.00104483
Iteration 5/25 | Loss: 0.00104483
Iteration 6/25 | Loss: 0.00104483
Iteration 7/25 | Loss: 0.00104483
Iteration 8/25 | Loss: 0.00104483
Iteration 9/25 | Loss: 0.00104483
Iteration 10/25 | Loss: 0.00104483
Iteration 11/25 | Loss: 0.00104483
Iteration 12/25 | Loss: 0.00104483
Iteration 13/25 | Loss: 0.00104483
Iteration 14/25 | Loss: 0.00104483
Iteration 15/25 | Loss: 0.00104483
Iteration 16/25 | Loss: 0.00104483
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010448304237797856, 0.0010448304237797856, 0.0010448304237797856, 0.0010448304237797856, 0.0010448304237797856]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010448304237797856

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104483
Iteration 2/1000 | Loss: 0.00002639
Iteration 3/1000 | Loss: 0.00013624
Iteration 4/1000 | Loss: 0.00014104
Iteration 5/1000 | Loss: 0.00002690
Iteration 6/1000 | Loss: 0.00001965
Iteration 7/1000 | Loss: 0.00001781
Iteration 8/1000 | Loss: 0.00012354
Iteration 9/1000 | Loss: 0.00001751
Iteration 10/1000 | Loss: 0.00001687
Iteration 11/1000 | Loss: 0.00011502
Iteration 12/1000 | Loss: 0.00001825
Iteration 13/1000 | Loss: 0.00001931
Iteration 14/1000 | Loss: 0.00003438
Iteration 15/1000 | Loss: 0.00001602
Iteration 16/1000 | Loss: 0.00003327
Iteration 17/1000 | Loss: 0.00001584
Iteration 18/1000 | Loss: 0.00001559
Iteration 19/1000 | Loss: 0.00001545
Iteration 20/1000 | Loss: 0.00001544
Iteration 21/1000 | Loss: 0.00001544
Iteration 22/1000 | Loss: 0.00001543
Iteration 23/1000 | Loss: 0.00001542
Iteration 24/1000 | Loss: 0.00001541
Iteration 25/1000 | Loss: 0.00001532
Iteration 26/1000 | Loss: 0.00001532
Iteration 27/1000 | Loss: 0.00001532
Iteration 28/1000 | Loss: 0.00001532
Iteration 29/1000 | Loss: 0.00001532
Iteration 30/1000 | Loss: 0.00001532
Iteration 31/1000 | Loss: 0.00001532
Iteration 32/1000 | Loss: 0.00001531
Iteration 33/1000 | Loss: 0.00001531
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001531
Iteration 37/1000 | Loss: 0.00001529
Iteration 38/1000 | Loss: 0.00001528
Iteration 39/1000 | Loss: 0.00001527
Iteration 40/1000 | Loss: 0.00001526
Iteration 41/1000 | Loss: 0.00001524
Iteration 42/1000 | Loss: 0.00001522
Iteration 43/1000 | Loss: 0.00001518
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001518
Iteration 46/1000 | Loss: 0.00001518
Iteration 47/1000 | Loss: 0.00001518
Iteration 48/1000 | Loss: 0.00001517
Iteration 49/1000 | Loss: 0.00001517
Iteration 50/1000 | Loss: 0.00001517
Iteration 51/1000 | Loss: 0.00001517
Iteration 52/1000 | Loss: 0.00001516
Iteration 53/1000 | Loss: 0.00001516
Iteration 54/1000 | Loss: 0.00001516
Iteration 55/1000 | Loss: 0.00001516
Iteration 56/1000 | Loss: 0.00001516
Iteration 57/1000 | Loss: 0.00001515
Iteration 58/1000 | Loss: 0.00001515
Iteration 59/1000 | Loss: 0.00001515
Iteration 60/1000 | Loss: 0.00001514
Iteration 61/1000 | Loss: 0.00001514
Iteration 62/1000 | Loss: 0.00001514
Iteration 63/1000 | Loss: 0.00001513
Iteration 64/1000 | Loss: 0.00001512
Iteration 65/1000 | Loss: 0.00001512
Iteration 66/1000 | Loss: 0.00001511
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001510
Iteration 71/1000 | Loss: 0.00001510
Iteration 72/1000 | Loss: 0.00001510
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001509
Iteration 75/1000 | Loss: 0.00001509
Iteration 76/1000 | Loss: 0.00001509
Iteration 77/1000 | Loss: 0.00001509
Iteration 78/1000 | Loss: 0.00001509
Iteration 79/1000 | Loss: 0.00001509
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001509
Iteration 82/1000 | Loss: 0.00001509
Iteration 83/1000 | Loss: 0.00001509
Iteration 84/1000 | Loss: 0.00001509
Iteration 85/1000 | Loss: 0.00001508
Iteration 86/1000 | Loss: 0.00001508
Iteration 87/1000 | Loss: 0.00001508
Iteration 88/1000 | Loss: 0.00001508
Iteration 89/1000 | Loss: 0.00001507
Iteration 90/1000 | Loss: 0.00001507
Iteration 91/1000 | Loss: 0.00001507
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001506
Iteration 94/1000 | Loss: 0.00001506
Iteration 95/1000 | Loss: 0.00001505
Iteration 96/1000 | Loss: 0.00001504
Iteration 97/1000 | Loss: 0.00001504
Iteration 98/1000 | Loss: 0.00001504
Iteration 99/1000 | Loss: 0.00001503
Iteration 100/1000 | Loss: 0.00001502
Iteration 101/1000 | Loss: 0.00001502
Iteration 102/1000 | Loss: 0.00001502
Iteration 103/1000 | Loss: 0.00001501
Iteration 104/1000 | Loss: 0.00001501
Iteration 105/1000 | Loss: 0.00001501
Iteration 106/1000 | Loss: 0.00001500
Iteration 107/1000 | Loss: 0.00001499
Iteration 108/1000 | Loss: 0.00001499
Iteration 109/1000 | Loss: 0.00001499
Iteration 110/1000 | Loss: 0.00001499
Iteration 111/1000 | Loss: 0.00001499
Iteration 112/1000 | Loss: 0.00001499
Iteration 113/1000 | Loss: 0.00001498
Iteration 114/1000 | Loss: 0.00001498
Iteration 115/1000 | Loss: 0.00001498
Iteration 116/1000 | Loss: 0.00001498
Iteration 117/1000 | Loss: 0.00001498
Iteration 118/1000 | Loss: 0.00001498
Iteration 119/1000 | Loss: 0.00001498
Iteration 120/1000 | Loss: 0.00001498
Iteration 121/1000 | Loss: 0.00001498
Iteration 122/1000 | Loss: 0.00001497
Iteration 123/1000 | Loss: 0.00001497
Iteration 124/1000 | Loss: 0.00001496
Iteration 125/1000 | Loss: 0.00001496
Iteration 126/1000 | Loss: 0.00001496
Iteration 127/1000 | Loss: 0.00001496
Iteration 128/1000 | Loss: 0.00001496
Iteration 129/1000 | Loss: 0.00001496
Iteration 130/1000 | Loss: 0.00001495
Iteration 131/1000 | Loss: 0.00001495
Iteration 132/1000 | Loss: 0.00001495
Iteration 133/1000 | Loss: 0.00001495
Iteration 134/1000 | Loss: 0.00001495
Iteration 135/1000 | Loss: 0.00001495
Iteration 136/1000 | Loss: 0.00001495
Iteration 137/1000 | Loss: 0.00001494
Iteration 138/1000 | Loss: 0.00001494
Iteration 139/1000 | Loss: 0.00001494
Iteration 140/1000 | Loss: 0.00001494
Iteration 141/1000 | Loss: 0.00001494
Iteration 142/1000 | Loss: 0.00001493
Iteration 143/1000 | Loss: 0.00001493
Iteration 144/1000 | Loss: 0.00001493
Iteration 145/1000 | Loss: 0.00001493
Iteration 146/1000 | Loss: 0.00001493
Iteration 147/1000 | Loss: 0.00001493
Iteration 148/1000 | Loss: 0.00001492
Iteration 149/1000 | Loss: 0.00001492
Iteration 150/1000 | Loss: 0.00001492
Iteration 151/1000 | Loss: 0.00001492
Iteration 152/1000 | Loss: 0.00001492
Iteration 153/1000 | Loss: 0.00001492
Iteration 154/1000 | Loss: 0.00001492
Iteration 155/1000 | Loss: 0.00001492
Iteration 156/1000 | Loss: 0.00001492
Iteration 157/1000 | Loss: 0.00001492
Iteration 158/1000 | Loss: 0.00001491
Iteration 159/1000 | Loss: 0.00001491
Iteration 160/1000 | Loss: 0.00001491
Iteration 161/1000 | Loss: 0.00001491
Iteration 162/1000 | Loss: 0.00001491
Iteration 163/1000 | Loss: 0.00001491
Iteration 164/1000 | Loss: 0.00001491
Iteration 165/1000 | Loss: 0.00001491
Iteration 166/1000 | Loss: 0.00001490
Iteration 167/1000 | Loss: 0.00001490
Iteration 168/1000 | Loss: 0.00001490
Iteration 169/1000 | Loss: 0.00001489
Iteration 170/1000 | Loss: 0.00001489
Iteration 171/1000 | Loss: 0.00001489
Iteration 172/1000 | Loss: 0.00001489
Iteration 173/1000 | Loss: 0.00001489
Iteration 174/1000 | Loss: 0.00001489
Iteration 175/1000 | Loss: 0.00001489
Iteration 176/1000 | Loss: 0.00001489
Iteration 177/1000 | Loss: 0.00001489
Iteration 178/1000 | Loss: 0.00001489
Iteration 179/1000 | Loss: 0.00001489
Iteration 180/1000 | Loss: 0.00001489
Iteration 181/1000 | Loss: 0.00001489
Iteration 182/1000 | Loss: 0.00001488
Iteration 183/1000 | Loss: 0.00001488
Iteration 184/1000 | Loss: 0.00001488
Iteration 185/1000 | Loss: 0.00001488
Iteration 186/1000 | Loss: 0.00001488
Iteration 187/1000 | Loss: 0.00001488
Iteration 188/1000 | Loss: 0.00001488
Iteration 189/1000 | Loss: 0.00001488
Iteration 190/1000 | Loss: 0.00001488
Iteration 191/1000 | Loss: 0.00001488
Iteration 192/1000 | Loss: 0.00001488
Iteration 193/1000 | Loss: 0.00001488
Iteration 194/1000 | Loss: 0.00001488
Iteration 195/1000 | Loss: 0.00001488
Iteration 196/1000 | Loss: 0.00001488
Iteration 197/1000 | Loss: 0.00001488
Iteration 198/1000 | Loss: 0.00001488
Iteration 199/1000 | Loss: 0.00001488
Iteration 200/1000 | Loss: 0.00001488
Iteration 201/1000 | Loss: 0.00001488
Iteration 202/1000 | Loss: 0.00001488
Iteration 203/1000 | Loss: 0.00001488
Iteration 204/1000 | Loss: 0.00001488
Iteration 205/1000 | Loss: 0.00001488
Iteration 206/1000 | Loss: 0.00001488
Iteration 207/1000 | Loss: 0.00001488
Iteration 208/1000 | Loss: 0.00001488
Iteration 209/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.4882499272061978e-05, 1.4882499272061978e-05, 1.4882499272061978e-05, 1.4882499272061978e-05, 1.4882499272061978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4882499272061978e-05

Optimization complete. Final v2v error: 3.2496798038482666 mm

Highest mean error: 3.5458688735961914 mm for frame 62

Lowest mean error: 3.0105960369110107 mm for frame 195

Saving results

Total time: 70.25190687179565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00488556
Iteration 2/25 | Loss: 0.00140170
Iteration 3/25 | Loss: 0.00131588
Iteration 4/25 | Loss: 0.00130923
Iteration 5/25 | Loss: 0.00130715
Iteration 6/25 | Loss: 0.00130715
Iteration 7/25 | Loss: 0.00130715
Iteration 8/25 | Loss: 0.00130715
Iteration 9/25 | Loss: 0.00130715
Iteration 10/25 | Loss: 0.00130715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013071502326056361, 0.0013071502326056361, 0.0013071502326056361, 0.0013071502326056361, 0.0013071502326056361]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013071502326056361

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29978251
Iteration 2/25 | Loss: 0.00143803
Iteration 3/25 | Loss: 0.00143802
Iteration 4/25 | Loss: 0.00143802
Iteration 5/25 | Loss: 0.00143802
Iteration 6/25 | Loss: 0.00143802
Iteration 7/25 | Loss: 0.00143802
Iteration 8/25 | Loss: 0.00143802
Iteration 9/25 | Loss: 0.00143801
Iteration 10/25 | Loss: 0.00143801
Iteration 11/25 | Loss: 0.00143801
Iteration 12/25 | Loss: 0.00143801
Iteration 13/25 | Loss: 0.00143801
Iteration 14/25 | Loss: 0.00143801
Iteration 15/25 | Loss: 0.00143801
Iteration 16/25 | Loss: 0.00143801
Iteration 17/25 | Loss: 0.00143801
Iteration 18/25 | Loss: 0.00143801
Iteration 19/25 | Loss: 0.00143801
Iteration 20/25 | Loss: 0.00143801
Iteration 21/25 | Loss: 0.00143801
Iteration 22/25 | Loss: 0.00143801
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014380138600245118, 0.0014380138600245118, 0.0014380138600245118, 0.0014380138600245118, 0.0014380138600245118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014380138600245118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143801
Iteration 2/1000 | Loss: 0.00004115
Iteration 3/1000 | Loss: 0.00002673
Iteration 4/1000 | Loss: 0.00002462
Iteration 5/1000 | Loss: 0.00002307
Iteration 6/1000 | Loss: 0.00002228
Iteration 7/1000 | Loss: 0.00002151
Iteration 8/1000 | Loss: 0.00002105
Iteration 9/1000 | Loss: 0.00002070
Iteration 10/1000 | Loss: 0.00002038
Iteration 11/1000 | Loss: 0.00002024
Iteration 12/1000 | Loss: 0.00002019
Iteration 13/1000 | Loss: 0.00002003
Iteration 14/1000 | Loss: 0.00001990
Iteration 15/1000 | Loss: 0.00001984
Iteration 16/1000 | Loss: 0.00001980
Iteration 17/1000 | Loss: 0.00001977
Iteration 18/1000 | Loss: 0.00001976
Iteration 19/1000 | Loss: 0.00001974
Iteration 20/1000 | Loss: 0.00001974
Iteration 21/1000 | Loss: 0.00001972
Iteration 22/1000 | Loss: 0.00001971
Iteration 23/1000 | Loss: 0.00001968
Iteration 24/1000 | Loss: 0.00001967
Iteration 25/1000 | Loss: 0.00001967
Iteration 26/1000 | Loss: 0.00001961
Iteration 27/1000 | Loss: 0.00001961
Iteration 28/1000 | Loss: 0.00001961
Iteration 29/1000 | Loss: 0.00001960
Iteration 30/1000 | Loss: 0.00001956
Iteration 31/1000 | Loss: 0.00001956
Iteration 32/1000 | Loss: 0.00001956
Iteration 33/1000 | Loss: 0.00001955
Iteration 34/1000 | Loss: 0.00001955
Iteration 35/1000 | Loss: 0.00001953
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001952
Iteration 38/1000 | Loss: 0.00001951
Iteration 39/1000 | Loss: 0.00001951
Iteration 40/1000 | Loss: 0.00001951
Iteration 41/1000 | Loss: 0.00001950
Iteration 42/1000 | Loss: 0.00001950
Iteration 43/1000 | Loss: 0.00001950
Iteration 44/1000 | Loss: 0.00001949
Iteration 45/1000 | Loss: 0.00001949
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001948
Iteration 48/1000 | Loss: 0.00001948
Iteration 49/1000 | Loss: 0.00001947
Iteration 50/1000 | Loss: 0.00001947
Iteration 51/1000 | Loss: 0.00001947
Iteration 52/1000 | Loss: 0.00001946
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001946
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001945
Iteration 59/1000 | Loss: 0.00001945
Iteration 60/1000 | Loss: 0.00001945
Iteration 61/1000 | Loss: 0.00001945
Iteration 62/1000 | Loss: 0.00001945
Iteration 63/1000 | Loss: 0.00001944
Iteration 64/1000 | Loss: 0.00001944
Iteration 65/1000 | Loss: 0.00001943
Iteration 66/1000 | Loss: 0.00001943
Iteration 67/1000 | Loss: 0.00001943
Iteration 68/1000 | Loss: 0.00001943
Iteration 69/1000 | Loss: 0.00001943
Iteration 70/1000 | Loss: 0.00001943
Iteration 71/1000 | Loss: 0.00001942
Iteration 72/1000 | Loss: 0.00001942
Iteration 73/1000 | Loss: 0.00001942
Iteration 74/1000 | Loss: 0.00001941
Iteration 75/1000 | Loss: 0.00001940
Iteration 76/1000 | Loss: 0.00001940
Iteration 77/1000 | Loss: 0.00001940
Iteration 78/1000 | Loss: 0.00001940
Iteration 79/1000 | Loss: 0.00001940
Iteration 80/1000 | Loss: 0.00001939
Iteration 81/1000 | Loss: 0.00001939
Iteration 82/1000 | Loss: 0.00001939
Iteration 83/1000 | Loss: 0.00001939
Iteration 84/1000 | Loss: 0.00001938
Iteration 85/1000 | Loss: 0.00001938
Iteration 86/1000 | Loss: 0.00001937
Iteration 87/1000 | Loss: 0.00001937
Iteration 88/1000 | Loss: 0.00001937
Iteration 89/1000 | Loss: 0.00001937
Iteration 90/1000 | Loss: 0.00001936
Iteration 91/1000 | Loss: 0.00001936
Iteration 92/1000 | Loss: 0.00001936
Iteration 93/1000 | Loss: 0.00001936
Iteration 94/1000 | Loss: 0.00001936
Iteration 95/1000 | Loss: 0.00001935
Iteration 96/1000 | Loss: 0.00001935
Iteration 97/1000 | Loss: 0.00001935
Iteration 98/1000 | Loss: 0.00001934
Iteration 99/1000 | Loss: 0.00001934
Iteration 100/1000 | Loss: 0.00001934
Iteration 101/1000 | Loss: 0.00001934
Iteration 102/1000 | Loss: 0.00001933
Iteration 103/1000 | Loss: 0.00001933
Iteration 104/1000 | Loss: 0.00001933
Iteration 105/1000 | Loss: 0.00001933
Iteration 106/1000 | Loss: 0.00001933
Iteration 107/1000 | Loss: 0.00001933
Iteration 108/1000 | Loss: 0.00001933
Iteration 109/1000 | Loss: 0.00001933
Iteration 110/1000 | Loss: 0.00001933
Iteration 111/1000 | Loss: 0.00001933
Iteration 112/1000 | Loss: 0.00001933
Iteration 113/1000 | Loss: 0.00001933
Iteration 114/1000 | Loss: 0.00001933
Iteration 115/1000 | Loss: 0.00001933
Iteration 116/1000 | Loss: 0.00001933
Iteration 117/1000 | Loss: 0.00001933
Iteration 118/1000 | Loss: 0.00001933
Iteration 119/1000 | Loss: 0.00001933
Iteration 120/1000 | Loss: 0.00001933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.932855047925841e-05, 1.932855047925841e-05, 1.932855047925841e-05, 1.932855047925841e-05, 1.932855047925841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.932855047925841e-05

Optimization complete. Final v2v error: 3.6005570888519287 mm

Highest mean error: 4.199817180633545 mm for frame 132

Lowest mean error: 3.089526653289795 mm for frame 2

Saving results

Total time: 38.161370277404785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00976768
Iteration 2/25 | Loss: 0.00369669
Iteration 3/25 | Loss: 0.00237747
Iteration 4/25 | Loss: 0.00214191
Iteration 5/25 | Loss: 0.00202737
Iteration 6/25 | Loss: 0.00180230
Iteration 7/25 | Loss: 0.00180609
Iteration 8/25 | Loss: 0.00172032
Iteration 9/25 | Loss: 0.00161549
Iteration 10/25 | Loss: 0.00158937
Iteration 11/25 | Loss: 0.00158872
Iteration 12/25 | Loss: 0.00157256
Iteration 13/25 | Loss: 0.00155665
Iteration 14/25 | Loss: 0.00155235
Iteration 15/25 | Loss: 0.00152824
Iteration 16/25 | Loss: 0.00151044
Iteration 17/25 | Loss: 0.00150827
Iteration 18/25 | Loss: 0.00150523
Iteration 19/25 | Loss: 0.00149545
Iteration 20/25 | Loss: 0.00148868
Iteration 21/25 | Loss: 0.00148671
Iteration 22/25 | Loss: 0.00148696
Iteration 23/25 | Loss: 0.00148325
Iteration 24/25 | Loss: 0.00148479
Iteration 25/25 | Loss: 0.00148201

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31034744
Iteration 2/25 | Loss: 0.00364217
Iteration 3/25 | Loss: 0.00300223
Iteration 4/25 | Loss: 0.00300223
Iteration 5/25 | Loss: 0.00300222
Iteration 6/25 | Loss: 0.00300222
Iteration 7/25 | Loss: 0.00300222
Iteration 8/25 | Loss: 0.00300222
Iteration 9/25 | Loss: 0.00300222
Iteration 10/25 | Loss: 0.00300222
Iteration 11/25 | Loss: 0.00300222
Iteration 12/25 | Loss: 0.00300222
Iteration 13/25 | Loss: 0.00300222
Iteration 14/25 | Loss: 0.00300222
Iteration 15/25 | Loss: 0.00300222
Iteration 16/25 | Loss: 0.00300222
Iteration 17/25 | Loss: 0.00300222
Iteration 18/25 | Loss: 0.00300222
Iteration 19/25 | Loss: 0.00300222
Iteration 20/25 | Loss: 0.00300222
Iteration 21/25 | Loss: 0.00300222
Iteration 22/25 | Loss: 0.00300222
Iteration 23/25 | Loss: 0.00300222
Iteration 24/25 | Loss: 0.00300222
Iteration 25/25 | Loss: 0.00300222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00300222
Iteration 2/1000 | Loss: 0.00102833
Iteration 3/1000 | Loss: 0.00226876
Iteration 4/1000 | Loss: 0.00163144
Iteration 5/1000 | Loss: 0.00062188
Iteration 6/1000 | Loss: 0.00024265
Iteration 7/1000 | Loss: 0.00047474
Iteration 8/1000 | Loss: 0.00094682
Iteration 9/1000 | Loss: 0.00024931
Iteration 10/1000 | Loss: 0.00028224
Iteration 11/1000 | Loss: 0.00105946
Iteration 12/1000 | Loss: 0.00024518
Iteration 13/1000 | Loss: 0.00019928
Iteration 14/1000 | Loss: 0.00016111
Iteration 15/1000 | Loss: 0.00014917
Iteration 16/1000 | Loss: 0.00051690
Iteration 17/1000 | Loss: 0.00030187
Iteration 18/1000 | Loss: 0.00092199
Iteration 19/1000 | Loss: 0.00014476
Iteration 20/1000 | Loss: 0.00023540
Iteration 21/1000 | Loss: 0.00016261
Iteration 22/1000 | Loss: 0.00055034
Iteration 23/1000 | Loss: 0.00066397
Iteration 24/1000 | Loss: 0.00025417
Iteration 25/1000 | Loss: 0.00043822
Iteration 26/1000 | Loss: 0.00018242
Iteration 27/1000 | Loss: 0.00024906
Iteration 28/1000 | Loss: 0.00040824
Iteration 29/1000 | Loss: 0.00045249
Iteration 30/1000 | Loss: 0.00037791
Iteration 31/1000 | Loss: 0.00018735
Iteration 32/1000 | Loss: 0.00016738
Iteration 33/1000 | Loss: 0.00062102
Iteration 34/1000 | Loss: 0.00056086
Iteration 35/1000 | Loss: 0.00200673
Iteration 36/1000 | Loss: 0.00085906
Iteration 37/1000 | Loss: 0.00085376
Iteration 38/1000 | Loss: 0.00214453
Iteration 39/1000 | Loss: 0.00078598
Iteration 40/1000 | Loss: 0.00066885
Iteration 41/1000 | Loss: 0.00064334
Iteration 42/1000 | Loss: 0.00050647
Iteration 43/1000 | Loss: 0.00048665
Iteration 44/1000 | Loss: 0.00020458
Iteration 45/1000 | Loss: 0.00024839
Iteration 46/1000 | Loss: 0.00036113
Iteration 47/1000 | Loss: 0.00029919
Iteration 48/1000 | Loss: 0.00028146
Iteration 49/1000 | Loss: 0.00039566
Iteration 50/1000 | Loss: 0.00023229
Iteration 51/1000 | Loss: 0.00009432
Iteration 52/1000 | Loss: 0.00039260
Iteration 53/1000 | Loss: 0.00051585
Iteration 54/1000 | Loss: 0.00064386
Iteration 55/1000 | Loss: 0.00088587
Iteration 56/1000 | Loss: 0.00022309
Iteration 57/1000 | Loss: 0.00023906
Iteration 58/1000 | Loss: 0.00132329
Iteration 59/1000 | Loss: 0.00017005
Iteration 60/1000 | Loss: 0.00008115
Iteration 61/1000 | Loss: 0.00023033
Iteration 62/1000 | Loss: 0.00023119
Iteration 63/1000 | Loss: 0.00147484
Iteration 64/1000 | Loss: 0.00036027
Iteration 65/1000 | Loss: 0.00023745
Iteration 66/1000 | Loss: 0.00018062
Iteration 67/1000 | Loss: 0.00007537
Iteration 68/1000 | Loss: 0.00010408
Iteration 69/1000 | Loss: 0.00016941
Iteration 70/1000 | Loss: 0.00034336
Iteration 71/1000 | Loss: 0.00008000
Iteration 72/1000 | Loss: 0.00035712
Iteration 73/1000 | Loss: 0.00028628
Iteration 74/1000 | Loss: 0.00016117
Iteration 75/1000 | Loss: 0.00012869
Iteration 76/1000 | Loss: 0.00015667
Iteration 77/1000 | Loss: 0.00016707
Iteration 78/1000 | Loss: 0.00011006
Iteration 79/1000 | Loss: 0.00008555
Iteration 80/1000 | Loss: 0.00094552
Iteration 81/1000 | Loss: 0.00014739
Iteration 82/1000 | Loss: 0.00017148
Iteration 83/1000 | Loss: 0.00005066
Iteration 84/1000 | Loss: 0.00026399
Iteration 85/1000 | Loss: 0.00004204
Iteration 86/1000 | Loss: 0.00016547
Iteration 87/1000 | Loss: 0.00019158
Iteration 88/1000 | Loss: 0.00005916
Iteration 89/1000 | Loss: 0.00006449
Iteration 90/1000 | Loss: 0.00007989
Iteration 91/1000 | Loss: 0.00004152
Iteration 92/1000 | Loss: 0.00003706
Iteration 93/1000 | Loss: 0.00004897
Iteration 94/1000 | Loss: 0.00004837
Iteration 95/1000 | Loss: 0.00002922
Iteration 96/1000 | Loss: 0.00014846
Iteration 97/1000 | Loss: 0.00006362
Iteration 98/1000 | Loss: 0.00034606
Iteration 99/1000 | Loss: 0.00003409
Iteration 100/1000 | Loss: 0.00011006
Iteration 101/1000 | Loss: 0.00003330
Iteration 102/1000 | Loss: 0.00002921
Iteration 103/1000 | Loss: 0.00021911
Iteration 104/1000 | Loss: 0.00024633
Iteration 105/1000 | Loss: 0.00002639
Iteration 106/1000 | Loss: 0.00004728
Iteration 107/1000 | Loss: 0.00002601
Iteration 108/1000 | Loss: 0.00003347
Iteration 109/1000 | Loss: 0.00003106
Iteration 110/1000 | Loss: 0.00002365
Iteration 111/1000 | Loss: 0.00005359
Iteration 112/1000 | Loss: 0.00002410
Iteration 113/1000 | Loss: 0.00002333
Iteration 114/1000 | Loss: 0.00002884
Iteration 115/1000 | Loss: 0.00017866
Iteration 116/1000 | Loss: 0.00003128
Iteration 117/1000 | Loss: 0.00002330
Iteration 118/1000 | Loss: 0.00002630
Iteration 119/1000 | Loss: 0.00002301
Iteration 120/1000 | Loss: 0.00002300
Iteration 121/1000 | Loss: 0.00002300
Iteration 122/1000 | Loss: 0.00002299
Iteration 123/1000 | Loss: 0.00002296
Iteration 124/1000 | Loss: 0.00002295
Iteration 125/1000 | Loss: 0.00002290
Iteration 126/1000 | Loss: 0.00004857
Iteration 127/1000 | Loss: 0.00002286
Iteration 128/1000 | Loss: 0.00002281
Iteration 129/1000 | Loss: 0.00002277
Iteration 130/1000 | Loss: 0.00002276
Iteration 131/1000 | Loss: 0.00002815
Iteration 132/1000 | Loss: 0.00004835
Iteration 133/1000 | Loss: 0.00004834
Iteration 134/1000 | Loss: 0.00021047
Iteration 135/1000 | Loss: 0.00002759
Iteration 136/1000 | Loss: 0.00002469
Iteration 137/1000 | Loss: 0.00002271
Iteration 138/1000 | Loss: 0.00002270
Iteration 139/1000 | Loss: 0.00004944
Iteration 140/1000 | Loss: 0.00002319
Iteration 141/1000 | Loss: 0.00002253
Iteration 142/1000 | Loss: 0.00002730
Iteration 143/1000 | Loss: 0.00004895
Iteration 144/1000 | Loss: 0.00003731
Iteration 145/1000 | Loss: 0.00002245
Iteration 146/1000 | Loss: 0.00002244
Iteration 147/1000 | Loss: 0.00002244
Iteration 148/1000 | Loss: 0.00002243
Iteration 149/1000 | Loss: 0.00002243
Iteration 150/1000 | Loss: 0.00002243
Iteration 151/1000 | Loss: 0.00002243
Iteration 152/1000 | Loss: 0.00002243
Iteration 153/1000 | Loss: 0.00002243
Iteration 154/1000 | Loss: 0.00002243
Iteration 155/1000 | Loss: 0.00002243
Iteration 156/1000 | Loss: 0.00002243
Iteration 157/1000 | Loss: 0.00002243
Iteration 158/1000 | Loss: 0.00002243
Iteration 159/1000 | Loss: 0.00002242
Iteration 160/1000 | Loss: 0.00002242
Iteration 161/1000 | Loss: 0.00002242
Iteration 162/1000 | Loss: 0.00002242
Iteration 163/1000 | Loss: 0.00002236
Iteration 164/1000 | Loss: 0.00002232
Iteration 165/1000 | Loss: 0.00002232
Iteration 166/1000 | Loss: 0.00002232
Iteration 167/1000 | Loss: 0.00002230
Iteration 168/1000 | Loss: 0.00002222
Iteration 169/1000 | Loss: 0.00002219
Iteration 170/1000 | Loss: 0.00004651
Iteration 171/1000 | Loss: 0.00004421
Iteration 172/1000 | Loss: 0.00004457
Iteration 173/1000 | Loss: 0.00002754
Iteration 174/1000 | Loss: 0.00002212
Iteration 175/1000 | Loss: 0.00004366
Iteration 176/1000 | Loss: 0.00002276
Iteration 177/1000 | Loss: 0.00002234
Iteration 178/1000 | Loss: 0.00002198
Iteration 179/1000 | Loss: 0.00002198
Iteration 180/1000 | Loss: 0.00002197
Iteration 181/1000 | Loss: 0.00002197
Iteration 182/1000 | Loss: 0.00002197
Iteration 183/1000 | Loss: 0.00002195
Iteration 184/1000 | Loss: 0.00010220
Iteration 185/1000 | Loss: 0.00002890
Iteration 186/1000 | Loss: 0.00010906
Iteration 187/1000 | Loss: 0.00022921
Iteration 188/1000 | Loss: 0.00004580
Iteration 189/1000 | Loss: 0.00002563
Iteration 190/1000 | Loss: 0.00005792
Iteration 191/1000 | Loss: 0.00002296
Iteration 192/1000 | Loss: 0.00004596
Iteration 193/1000 | Loss: 0.00008669
Iteration 194/1000 | Loss: 0.00009308
Iteration 195/1000 | Loss: 0.00001869
Iteration 196/1000 | Loss: 0.00001831
Iteration 197/1000 | Loss: 0.00003281
Iteration 198/1000 | Loss: 0.00006051
Iteration 199/1000 | Loss: 0.00001911
Iteration 200/1000 | Loss: 0.00002277
Iteration 201/1000 | Loss: 0.00001853
Iteration 202/1000 | Loss: 0.00001942
Iteration 203/1000 | Loss: 0.00001756
Iteration 204/1000 | Loss: 0.00001752
Iteration 205/1000 | Loss: 0.00001747
Iteration 206/1000 | Loss: 0.00001747
Iteration 207/1000 | Loss: 0.00001746
Iteration 208/1000 | Loss: 0.00001742
Iteration 209/1000 | Loss: 0.00001999
Iteration 210/1000 | Loss: 0.00001794
Iteration 211/1000 | Loss: 0.00001744
Iteration 212/1000 | Loss: 0.00002310
Iteration 213/1000 | Loss: 0.00001765
Iteration 214/1000 | Loss: 0.00001719
Iteration 215/1000 | Loss: 0.00001718
Iteration 216/1000 | Loss: 0.00001718
Iteration 217/1000 | Loss: 0.00001717
Iteration 218/1000 | Loss: 0.00001717
Iteration 219/1000 | Loss: 0.00001716
Iteration 220/1000 | Loss: 0.00001716
Iteration 221/1000 | Loss: 0.00001716
Iteration 222/1000 | Loss: 0.00001716
Iteration 223/1000 | Loss: 0.00001716
Iteration 224/1000 | Loss: 0.00001716
Iteration 225/1000 | Loss: 0.00001716
Iteration 226/1000 | Loss: 0.00001716
Iteration 227/1000 | Loss: 0.00001716
Iteration 228/1000 | Loss: 0.00001716
Iteration 229/1000 | Loss: 0.00001715
Iteration 230/1000 | Loss: 0.00001715
Iteration 231/1000 | Loss: 0.00001715
Iteration 232/1000 | Loss: 0.00001715
Iteration 233/1000 | Loss: 0.00001715
Iteration 234/1000 | Loss: 0.00001715
Iteration 235/1000 | Loss: 0.00001715
Iteration 236/1000 | Loss: 0.00001714
Iteration 237/1000 | Loss: 0.00001714
Iteration 238/1000 | Loss: 0.00001714
Iteration 239/1000 | Loss: 0.00001714
Iteration 240/1000 | Loss: 0.00001714
Iteration 241/1000 | Loss: 0.00001714
Iteration 242/1000 | Loss: 0.00001713
Iteration 243/1000 | Loss: 0.00001713
Iteration 244/1000 | Loss: 0.00001713
Iteration 245/1000 | Loss: 0.00001713
Iteration 246/1000 | Loss: 0.00001713
Iteration 247/1000 | Loss: 0.00001713
Iteration 248/1000 | Loss: 0.00001713
Iteration 249/1000 | Loss: 0.00001713
Iteration 250/1000 | Loss: 0.00001713
Iteration 251/1000 | Loss: 0.00001713
Iteration 252/1000 | Loss: 0.00001713
Iteration 253/1000 | Loss: 0.00001713
Iteration 254/1000 | Loss: 0.00001713
Iteration 255/1000 | Loss: 0.00001713
Iteration 256/1000 | Loss: 0.00001713
Iteration 257/1000 | Loss: 0.00001713
Iteration 258/1000 | Loss: 0.00001713
Iteration 259/1000 | Loss: 0.00001713
Iteration 260/1000 | Loss: 0.00001713
Iteration 261/1000 | Loss: 0.00001713
Iteration 262/1000 | Loss: 0.00001713
Iteration 263/1000 | Loss: 0.00001713
Iteration 264/1000 | Loss: 0.00001713
Iteration 265/1000 | Loss: 0.00001713
Iteration 266/1000 | Loss: 0.00001713
Iteration 267/1000 | Loss: 0.00001713
Iteration 268/1000 | Loss: 0.00001713
Iteration 269/1000 | Loss: 0.00001713
Iteration 270/1000 | Loss: 0.00001713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [1.7126547390944324e-05, 1.7126547390944324e-05, 1.7126547390944324e-05, 1.7126547390944324e-05, 1.7126547390944324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7126547390944324e-05

Optimization complete. Final v2v error: 3.093726396560669 mm

Highest mean error: 11.148713111877441 mm for frame 168

Lowest mean error: 2.629958152770996 mm for frame 27

Saving results

Total time: 325.18543219566345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968389
Iteration 2/25 | Loss: 0.00350200
Iteration 3/25 | Loss: 0.00230556
Iteration 4/25 | Loss: 0.00212815
Iteration 5/25 | Loss: 0.00195266
Iteration 6/25 | Loss: 0.00179682
Iteration 7/25 | Loss: 0.00171891
Iteration 8/25 | Loss: 0.00165152
Iteration 9/25 | Loss: 0.00164308
Iteration 10/25 | Loss: 0.00158139
Iteration 11/25 | Loss: 0.00156004
Iteration 12/25 | Loss: 0.00153401
Iteration 13/25 | Loss: 0.00153677
Iteration 14/25 | Loss: 0.00153703
Iteration 15/25 | Loss: 0.00151563
Iteration 16/25 | Loss: 0.00149995
Iteration 17/25 | Loss: 0.00148743
Iteration 18/25 | Loss: 0.00148866
Iteration 19/25 | Loss: 0.00149066
Iteration 20/25 | Loss: 0.00147935
Iteration 21/25 | Loss: 0.00147508
Iteration 22/25 | Loss: 0.00147815
Iteration 23/25 | Loss: 0.00147915
Iteration 24/25 | Loss: 0.00142931
Iteration 25/25 | Loss: 0.00139564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27391565
Iteration 2/25 | Loss: 0.00352907
Iteration 3/25 | Loss: 0.00315982
Iteration 4/25 | Loss: 0.00315981
Iteration 5/25 | Loss: 0.00315981
Iteration 6/25 | Loss: 0.00315981
Iteration 7/25 | Loss: 0.00315981
Iteration 8/25 | Loss: 0.00315981
Iteration 9/25 | Loss: 0.00315981
Iteration 10/25 | Loss: 0.00315981
Iteration 11/25 | Loss: 0.00315981
Iteration 12/25 | Loss: 0.00315981
Iteration 13/25 | Loss: 0.00315981
Iteration 14/25 | Loss: 0.00315981
Iteration 15/25 | Loss: 0.00315981
Iteration 16/25 | Loss: 0.00315981
Iteration 17/25 | Loss: 0.00315981
Iteration 18/25 | Loss: 0.00315981
Iteration 19/25 | Loss: 0.00315981
Iteration 20/25 | Loss: 0.00315981
Iteration 21/25 | Loss: 0.00315981
Iteration 22/25 | Loss: 0.00315981
Iteration 23/25 | Loss: 0.00315981
Iteration 24/25 | Loss: 0.00315981
Iteration 25/25 | Loss: 0.00315981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00315981
Iteration 2/1000 | Loss: 0.00467270
Iteration 3/1000 | Loss: 0.00161913
Iteration 4/1000 | Loss: 0.00028117
Iteration 5/1000 | Loss: 0.00062770
Iteration 6/1000 | Loss: 0.00067062
Iteration 7/1000 | Loss: 0.00033061
Iteration 8/1000 | Loss: 0.00067752
Iteration 9/1000 | Loss: 0.00041460
Iteration 10/1000 | Loss: 0.00389021
Iteration 11/1000 | Loss: 0.00093194
Iteration 12/1000 | Loss: 0.00019181
Iteration 13/1000 | Loss: 0.00015599
Iteration 14/1000 | Loss: 0.00033666
Iteration 15/1000 | Loss: 0.00120224
Iteration 16/1000 | Loss: 0.00024215
Iteration 17/1000 | Loss: 0.00024215
Iteration 18/1000 | Loss: 0.00102442
Iteration 19/1000 | Loss: 0.00058553
Iteration 20/1000 | Loss: 0.00011455
Iteration 21/1000 | Loss: 0.00112424
Iteration 22/1000 | Loss: 0.00019227
Iteration 23/1000 | Loss: 0.00010381
Iteration 24/1000 | Loss: 0.00110206
Iteration 25/1000 | Loss: 0.00017365
Iteration 26/1000 | Loss: 0.00176538
Iteration 27/1000 | Loss: 0.00072152
Iteration 28/1000 | Loss: 0.00021108
Iteration 29/1000 | Loss: 0.00087743
Iteration 30/1000 | Loss: 0.00022419
Iteration 31/1000 | Loss: 0.00008909
Iteration 32/1000 | Loss: 0.00012696
Iteration 33/1000 | Loss: 0.00030587
Iteration 34/1000 | Loss: 0.00020447
Iteration 35/1000 | Loss: 0.00020288
Iteration 36/1000 | Loss: 0.00008823
Iteration 37/1000 | Loss: 0.00028393
Iteration 38/1000 | Loss: 0.00008806
Iteration 39/1000 | Loss: 0.00018945
Iteration 40/1000 | Loss: 0.00220600
Iteration 41/1000 | Loss: 0.00105508
Iteration 42/1000 | Loss: 0.00138734
Iteration 43/1000 | Loss: 0.00013502
Iteration 44/1000 | Loss: 0.00034372
Iteration 45/1000 | Loss: 0.00007949
Iteration 46/1000 | Loss: 0.00007156
Iteration 47/1000 | Loss: 0.00011452
Iteration 48/1000 | Loss: 0.00013366
Iteration 49/1000 | Loss: 0.00007452
Iteration 50/1000 | Loss: 0.00006847
Iteration 51/1000 | Loss: 0.00012992
Iteration 52/1000 | Loss: 0.00146589
Iteration 53/1000 | Loss: 0.00016021
Iteration 54/1000 | Loss: 0.00051306
Iteration 55/1000 | Loss: 0.00055911
Iteration 56/1000 | Loss: 0.00143754
Iteration 57/1000 | Loss: 0.00050100
Iteration 58/1000 | Loss: 0.00066072
Iteration 59/1000 | Loss: 0.00062695
Iteration 60/1000 | Loss: 0.00024748
Iteration 61/1000 | Loss: 0.00040876
Iteration 62/1000 | Loss: 0.00009777
Iteration 63/1000 | Loss: 0.00008302
Iteration 64/1000 | Loss: 0.00007080
Iteration 65/1000 | Loss: 0.00007098
Iteration 66/1000 | Loss: 0.00007625
Iteration 67/1000 | Loss: 0.00008497
Iteration 68/1000 | Loss: 0.00010476
Iteration 69/1000 | Loss: 0.00007213
Iteration 70/1000 | Loss: 0.00024681
Iteration 71/1000 | Loss: 0.00008158
Iteration 72/1000 | Loss: 0.00009966
Iteration 73/1000 | Loss: 0.00007687
Iteration 74/1000 | Loss: 0.00009203
Iteration 75/1000 | Loss: 0.00008803
Iteration 76/1000 | Loss: 0.00007214
Iteration 77/1000 | Loss: 0.00007939
Iteration 78/1000 | Loss: 0.00008819
Iteration 79/1000 | Loss: 0.00006319
Iteration 80/1000 | Loss: 0.00007199
Iteration 81/1000 | Loss: 0.00006925
Iteration 82/1000 | Loss: 0.00099001
Iteration 83/1000 | Loss: 0.00009928
Iteration 84/1000 | Loss: 0.00008556
Iteration 85/1000 | Loss: 0.00007114
Iteration 86/1000 | Loss: 0.00045446
Iteration 87/1000 | Loss: 0.00044118
Iteration 88/1000 | Loss: 0.00010319
Iteration 89/1000 | Loss: 0.00007313
Iteration 90/1000 | Loss: 0.00006872
Iteration 91/1000 | Loss: 0.00012758
Iteration 92/1000 | Loss: 0.00158539
Iteration 93/1000 | Loss: 0.00217607
Iteration 94/1000 | Loss: 0.00011743
Iteration 95/1000 | Loss: 0.00022113
Iteration 96/1000 | Loss: 0.00015016
Iteration 97/1000 | Loss: 0.00010479
Iteration 98/1000 | Loss: 0.00018164
Iteration 99/1000 | Loss: 0.00021958
Iteration 100/1000 | Loss: 0.00012137
Iteration 101/1000 | Loss: 0.00022487
Iteration 102/1000 | Loss: 0.00017101
Iteration 103/1000 | Loss: 0.00016192
Iteration 104/1000 | Loss: 0.00015375
Iteration 105/1000 | Loss: 0.00012531
Iteration 106/1000 | Loss: 0.00009279
Iteration 107/1000 | Loss: 0.00020820
Iteration 108/1000 | Loss: 0.00011816
Iteration 109/1000 | Loss: 0.00018579
Iteration 110/1000 | Loss: 0.00009272
Iteration 111/1000 | Loss: 0.00010810
Iteration 112/1000 | Loss: 0.00005326
Iteration 113/1000 | Loss: 0.00012770
Iteration 114/1000 | Loss: 0.00026058
Iteration 115/1000 | Loss: 0.00009038
Iteration 116/1000 | Loss: 0.00033103
Iteration 117/1000 | Loss: 0.00029172
Iteration 118/1000 | Loss: 0.00030508
Iteration 119/1000 | Loss: 0.00018927
Iteration 120/1000 | Loss: 0.00013466
Iteration 121/1000 | Loss: 0.00005255
Iteration 122/1000 | Loss: 0.00005063
Iteration 123/1000 | Loss: 0.00010117
Iteration 124/1000 | Loss: 0.00004692
Iteration 125/1000 | Loss: 0.00006832
Iteration 126/1000 | Loss: 0.00012606
Iteration 127/1000 | Loss: 0.00004588
Iteration 128/1000 | Loss: 0.00008336
Iteration 129/1000 | Loss: 0.00004554
Iteration 130/1000 | Loss: 0.00008667
Iteration 131/1000 | Loss: 0.00038634
Iteration 132/1000 | Loss: 0.00005706
Iteration 133/1000 | Loss: 0.00004853
Iteration 134/1000 | Loss: 0.00005193
Iteration 135/1000 | Loss: 0.00008800
Iteration 136/1000 | Loss: 0.00004475
Iteration 137/1000 | Loss: 0.00004461
Iteration 138/1000 | Loss: 0.00005535
Iteration 139/1000 | Loss: 0.00004439
Iteration 140/1000 | Loss: 0.00008282
Iteration 141/1000 | Loss: 0.00017668
Iteration 142/1000 | Loss: 0.00011991
Iteration 143/1000 | Loss: 0.00005100
Iteration 144/1000 | Loss: 0.00007594
Iteration 145/1000 | Loss: 0.00004421
Iteration 146/1000 | Loss: 0.00004405
Iteration 147/1000 | Loss: 0.00012559
Iteration 148/1000 | Loss: 0.00005279
Iteration 149/1000 | Loss: 0.00029047
Iteration 150/1000 | Loss: 0.00014498
Iteration 151/1000 | Loss: 0.00004775
Iteration 152/1000 | Loss: 0.00005896
Iteration 153/1000 | Loss: 0.00004397
Iteration 154/1000 | Loss: 0.00024761
Iteration 155/1000 | Loss: 0.00017219
Iteration 156/1000 | Loss: 0.00006590
Iteration 157/1000 | Loss: 0.00023032
Iteration 158/1000 | Loss: 0.00035186
Iteration 159/1000 | Loss: 0.00024172
Iteration 160/1000 | Loss: 0.00014619
Iteration 161/1000 | Loss: 0.00024182
Iteration 162/1000 | Loss: 0.00016502
Iteration 163/1000 | Loss: 0.00023797
Iteration 164/1000 | Loss: 0.00008321
Iteration 165/1000 | Loss: 0.00006336
Iteration 166/1000 | Loss: 0.00005293
Iteration 167/1000 | Loss: 0.00006009
Iteration 168/1000 | Loss: 0.00005346
Iteration 169/1000 | Loss: 0.00004484
Iteration 170/1000 | Loss: 0.00006822
Iteration 171/1000 | Loss: 0.00004758
Iteration 172/1000 | Loss: 0.00006150
Iteration 173/1000 | Loss: 0.00004359
Iteration 174/1000 | Loss: 0.00004345
Iteration 175/1000 | Loss: 0.00004344
Iteration 176/1000 | Loss: 0.00007110
Iteration 177/1000 | Loss: 0.00008003
Iteration 178/1000 | Loss: 0.00004327
Iteration 179/1000 | Loss: 0.00008450
Iteration 180/1000 | Loss: 0.00004301
Iteration 181/1000 | Loss: 0.00004286
Iteration 182/1000 | Loss: 0.00004280
Iteration 183/1000 | Loss: 0.00039027
Iteration 184/1000 | Loss: 0.00041059
Iteration 185/1000 | Loss: 0.00024947
Iteration 186/1000 | Loss: 0.00016288
Iteration 187/1000 | Loss: 0.00024972
Iteration 188/1000 | Loss: 0.00020022
Iteration 189/1000 | Loss: 0.00018710
Iteration 190/1000 | Loss: 0.00017726
Iteration 191/1000 | Loss: 0.00008923
Iteration 192/1000 | Loss: 0.00013853
Iteration 193/1000 | Loss: 0.00004903
Iteration 194/1000 | Loss: 0.00004785
Iteration 195/1000 | Loss: 0.00004647
Iteration 196/1000 | Loss: 0.00004016
Iteration 197/1000 | Loss: 0.00003962
Iteration 198/1000 | Loss: 0.00013518
Iteration 199/1000 | Loss: 0.00003906
Iteration 200/1000 | Loss: 0.00003867
Iteration 201/1000 | Loss: 0.00003834
Iteration 202/1000 | Loss: 0.00008751
Iteration 203/1000 | Loss: 0.00003817
Iteration 204/1000 | Loss: 0.00004915
Iteration 205/1000 | Loss: 0.00003791
Iteration 206/1000 | Loss: 0.00003789
Iteration 207/1000 | Loss: 0.00003785
Iteration 208/1000 | Loss: 0.00007275
Iteration 209/1000 | Loss: 0.00005640
Iteration 210/1000 | Loss: 0.00006454
Iteration 211/1000 | Loss: 0.00004305
Iteration 212/1000 | Loss: 0.00003966
Iteration 213/1000 | Loss: 0.00003761
Iteration 214/1000 | Loss: 0.00003756
Iteration 215/1000 | Loss: 0.00003756
Iteration 216/1000 | Loss: 0.00003755
Iteration 217/1000 | Loss: 0.00003755
Iteration 218/1000 | Loss: 0.00003755
Iteration 219/1000 | Loss: 0.00003755
Iteration 220/1000 | Loss: 0.00003754
Iteration 221/1000 | Loss: 0.00003754
Iteration 222/1000 | Loss: 0.00003754
Iteration 223/1000 | Loss: 0.00003754
Iteration 224/1000 | Loss: 0.00003754
Iteration 225/1000 | Loss: 0.00003754
Iteration 226/1000 | Loss: 0.00003753
Iteration 227/1000 | Loss: 0.00003753
Iteration 228/1000 | Loss: 0.00003753
Iteration 229/1000 | Loss: 0.00003753
Iteration 230/1000 | Loss: 0.00003752
Iteration 231/1000 | Loss: 0.00003752
Iteration 232/1000 | Loss: 0.00003752
Iteration 233/1000 | Loss: 0.00003751
Iteration 234/1000 | Loss: 0.00006080
Iteration 235/1000 | Loss: 0.00003768
Iteration 236/1000 | Loss: 0.00003744
Iteration 237/1000 | Loss: 0.00003735
Iteration 238/1000 | Loss: 0.00003733
Iteration 239/1000 | Loss: 0.00003732
Iteration 240/1000 | Loss: 0.00003732
Iteration 241/1000 | Loss: 0.00024829
Iteration 242/1000 | Loss: 0.00014226
Iteration 243/1000 | Loss: 0.00004343
Iteration 244/1000 | Loss: 0.00003746
Iteration 245/1000 | Loss: 0.00007495
Iteration 246/1000 | Loss: 0.00003793
Iteration 247/1000 | Loss: 0.00004929
Iteration 248/1000 | Loss: 0.00003731
Iteration 249/1000 | Loss: 0.00025325
Iteration 250/1000 | Loss: 0.00027104
Iteration 251/1000 | Loss: 0.00022855
Iteration 252/1000 | Loss: 0.00005658
Iteration 253/1000 | Loss: 0.00004892
Iteration 254/1000 | Loss: 0.00003969
Iteration 255/1000 | Loss: 0.00006113
Iteration 256/1000 | Loss: 0.00004942
Iteration 257/1000 | Loss: 0.00003992
Iteration 258/1000 | Loss: 0.00003853
Iteration 259/1000 | Loss: 0.00003926
Iteration 260/1000 | Loss: 0.00003838
Iteration 261/1000 | Loss: 0.00004354
Iteration 262/1000 | Loss: 0.00003820
Iteration 263/1000 | Loss: 0.00006865
Iteration 264/1000 | Loss: 0.00003826
Iteration 265/1000 | Loss: 0.00005027
Iteration 266/1000 | Loss: 0.00003813
Iteration 267/1000 | Loss: 0.00003811
Iteration 268/1000 | Loss: 0.00003811
Iteration 269/1000 | Loss: 0.00003811
Iteration 270/1000 | Loss: 0.00003811
Iteration 271/1000 | Loss: 0.00003811
Iteration 272/1000 | Loss: 0.00003811
Iteration 273/1000 | Loss: 0.00003811
Iteration 274/1000 | Loss: 0.00003811
Iteration 275/1000 | Loss: 0.00003809
Iteration 276/1000 | Loss: 0.00003809
Iteration 277/1000 | Loss: 0.00003809
Iteration 278/1000 | Loss: 0.00003806
Iteration 279/1000 | Loss: 0.00003805
Iteration 280/1000 | Loss: 0.00004663
Iteration 281/1000 | Loss: 0.00012786
Iteration 282/1000 | Loss: 0.00011221
Iteration 283/1000 | Loss: 0.00005910
Iteration 284/1000 | Loss: 0.00004062
Iteration 285/1000 | Loss: 0.00007060
Iteration 286/1000 | Loss: 0.00003891
Iteration 287/1000 | Loss: 0.00003788
Iteration 288/1000 | Loss: 0.00004186
Iteration 289/1000 | Loss: 0.00003779
Iteration 290/1000 | Loss: 0.00003779
Iteration 291/1000 | Loss: 0.00003779
Iteration 292/1000 | Loss: 0.00003779
Iteration 293/1000 | Loss: 0.00003779
Iteration 294/1000 | Loss: 0.00003779
Iteration 295/1000 | Loss: 0.00003779
Iteration 296/1000 | Loss: 0.00003779
Iteration 297/1000 | Loss: 0.00003779
Iteration 298/1000 | Loss: 0.00003779
Iteration 299/1000 | Loss: 0.00003779
Iteration 300/1000 | Loss: 0.00003778
Iteration 301/1000 | Loss: 0.00003778
Iteration 302/1000 | Loss: 0.00003778
Iteration 303/1000 | Loss: 0.00003777
Iteration 304/1000 | Loss: 0.00003777
Iteration 305/1000 | Loss: 0.00003776
Iteration 306/1000 | Loss: 0.00003776
Iteration 307/1000 | Loss: 0.00003775
Iteration 308/1000 | Loss: 0.00003775
Iteration 309/1000 | Loss: 0.00003775
Iteration 310/1000 | Loss: 0.00003774
Iteration 311/1000 | Loss: 0.00003774
Iteration 312/1000 | Loss: 0.00003774
Iteration 313/1000 | Loss: 0.00003773
Iteration 314/1000 | Loss: 0.00003773
Iteration 315/1000 | Loss: 0.00003773
Iteration 316/1000 | Loss: 0.00003773
Iteration 317/1000 | Loss: 0.00003772
Iteration 318/1000 | Loss: 0.00003771
Iteration 319/1000 | Loss: 0.00003771
Iteration 320/1000 | Loss: 0.00003771
Iteration 321/1000 | Loss: 0.00003771
Iteration 322/1000 | Loss: 0.00003771
Iteration 323/1000 | Loss: 0.00006082
Iteration 324/1000 | Loss: 0.00004296
Iteration 325/1000 | Loss: 0.00003846
Iteration 326/1000 | Loss: 0.00003784
Iteration 327/1000 | Loss: 0.00003768
Iteration 328/1000 | Loss: 0.00003768
Iteration 329/1000 | Loss: 0.00003768
Iteration 330/1000 | Loss: 0.00003768
Iteration 331/1000 | Loss: 0.00003768
Iteration 332/1000 | Loss: 0.00003767
Iteration 333/1000 | Loss: 0.00003767
Iteration 334/1000 | Loss: 0.00003767
Iteration 335/1000 | Loss: 0.00003767
Iteration 336/1000 | Loss: 0.00003767
Iteration 337/1000 | Loss: 0.00003767
Iteration 338/1000 | Loss: 0.00003767
Iteration 339/1000 | Loss: 0.00003767
Iteration 340/1000 | Loss: 0.00003767
Iteration 341/1000 | Loss: 0.00003767
Iteration 342/1000 | Loss: 0.00003767
Iteration 343/1000 | Loss: 0.00003767
Iteration 344/1000 | Loss: 0.00003767
Iteration 345/1000 | Loss: 0.00003767
Iteration 346/1000 | Loss: 0.00003846
Iteration 347/1000 | Loss: 0.00003766
Iteration 348/1000 | Loss: 0.00003764
Iteration 349/1000 | Loss: 0.00003764
Iteration 350/1000 | Loss: 0.00003764
Iteration 351/1000 | Loss: 0.00003764
Iteration 352/1000 | Loss: 0.00003764
Iteration 353/1000 | Loss: 0.00003763
Iteration 354/1000 | Loss: 0.00003763
Iteration 355/1000 | Loss: 0.00003763
Iteration 356/1000 | Loss: 0.00003763
Iteration 357/1000 | Loss: 0.00003763
Iteration 358/1000 | Loss: 0.00003763
Iteration 359/1000 | Loss: 0.00003763
Iteration 360/1000 | Loss: 0.00003763
Iteration 361/1000 | Loss: 0.00003763
Iteration 362/1000 | Loss: 0.00003763
Iteration 363/1000 | Loss: 0.00003761
Iteration 364/1000 | Loss: 0.00003761
Iteration 365/1000 | Loss: 0.00003760
Iteration 366/1000 | Loss: 0.00003760
Iteration 367/1000 | Loss: 0.00003759
Iteration 368/1000 | Loss: 0.00003759
Iteration 369/1000 | Loss: 0.00003759
Iteration 370/1000 | Loss: 0.00003759
Iteration 371/1000 | Loss: 0.00003758
Iteration 372/1000 | Loss: 0.00003758
Iteration 373/1000 | Loss: 0.00003758
Iteration 374/1000 | Loss: 0.00003758
Iteration 375/1000 | Loss: 0.00003758
Iteration 376/1000 | Loss: 0.00003758
Iteration 377/1000 | Loss: 0.00003758
Iteration 378/1000 | Loss: 0.00003758
Iteration 379/1000 | Loss: 0.00003758
Iteration 380/1000 | Loss: 0.00003757
Iteration 381/1000 | Loss: 0.00003757
Iteration 382/1000 | Loss: 0.00003756
Iteration 383/1000 | Loss: 0.00003756
Iteration 384/1000 | Loss: 0.00003755
Iteration 385/1000 | Loss: 0.00003755
Iteration 386/1000 | Loss: 0.00003754
Iteration 387/1000 | Loss: 0.00003754
Iteration 388/1000 | Loss: 0.00003754
Iteration 389/1000 | Loss: 0.00003753
Iteration 390/1000 | Loss: 0.00003753
Iteration 391/1000 | Loss: 0.00003753
Iteration 392/1000 | Loss: 0.00003752
Iteration 393/1000 | Loss: 0.00003752
Iteration 394/1000 | Loss: 0.00003752
Iteration 395/1000 | Loss: 0.00003752
Iteration 396/1000 | Loss: 0.00003751
Iteration 397/1000 | Loss: 0.00003751
Iteration 398/1000 | Loss: 0.00003751
Iteration 399/1000 | Loss: 0.00003751
Iteration 400/1000 | Loss: 0.00003750
Iteration 401/1000 | Loss: 0.00003750
Iteration 402/1000 | Loss: 0.00003750
Iteration 403/1000 | Loss: 0.00003750
Iteration 404/1000 | Loss: 0.00003749
Iteration 405/1000 | Loss: 0.00003749
Iteration 406/1000 | Loss: 0.00003748
Iteration 407/1000 | Loss: 0.00003747
Iteration 408/1000 | Loss: 0.00003747
Iteration 409/1000 | Loss: 0.00003747
Iteration 410/1000 | Loss: 0.00003747
Iteration 411/1000 | Loss: 0.00004328
Iteration 412/1000 | Loss: 0.00003757
Iteration 413/1000 | Loss: 0.00003740
Iteration 414/1000 | Loss: 0.00003740
Iteration 415/1000 | Loss: 0.00003740
Iteration 416/1000 | Loss: 0.00003740
Iteration 417/1000 | Loss: 0.00003740
Iteration 418/1000 | Loss: 0.00003740
Iteration 419/1000 | Loss: 0.00003740
Iteration 420/1000 | Loss: 0.00003740
Iteration 421/1000 | Loss: 0.00003739
Iteration 422/1000 | Loss: 0.00003739
Iteration 423/1000 | Loss: 0.00003739
Iteration 424/1000 | Loss: 0.00003738
Iteration 425/1000 | Loss: 0.00003734
Iteration 426/1000 | Loss: 0.00014792
Iteration 427/1000 | Loss: 0.00005142
Iteration 428/1000 | Loss: 0.00003831
Iteration 429/1000 | Loss: 0.00004141
Iteration 430/1000 | Loss: 0.00005853
Iteration 431/1000 | Loss: 0.00003762
Iteration 432/1000 | Loss: 0.00004500
Iteration 433/1000 | Loss: 0.00004925
Iteration 434/1000 | Loss: 0.00004909
Iteration 435/1000 | Loss: 0.00010468
Iteration 436/1000 | Loss: 0.00003732
Iteration 437/1000 | Loss: 0.00004020
Iteration 438/1000 | Loss: 0.00004020
Iteration 439/1000 | Loss: 0.00005302
Iteration 440/1000 | Loss: 0.00005982
Iteration 441/1000 | Loss: 0.00004531
Iteration 442/1000 | Loss: 0.00003761
Iteration 443/1000 | Loss: 0.00003970
Iteration 444/1000 | Loss: 0.00003696
Iteration 445/1000 | Loss: 0.00004301
Iteration 446/1000 | Loss: 0.00003684
Iteration 447/1000 | Loss: 0.00003681
Iteration 448/1000 | Loss: 0.00017909
Iteration 449/1000 | Loss: 0.00016399
Iteration 450/1000 | Loss: 0.00003697
Iteration 451/1000 | Loss: 0.00003752
Iteration 452/1000 | Loss: 0.00003679
Iteration 453/1000 | Loss: 0.00003678
Iteration 454/1000 | Loss: 0.00003678
Iteration 455/1000 | Loss: 0.00003676
Iteration 456/1000 | Loss: 0.00003834
Iteration 457/1000 | Loss: 0.00017936
Iteration 458/1000 | Loss: 0.00016958
Iteration 459/1000 | Loss: 0.00023882
Iteration 460/1000 | Loss: 0.00004051
Iteration 461/1000 | Loss: 0.00003867
Iteration 462/1000 | Loss: 0.00006561
Iteration 463/1000 | Loss: 0.00003836
Iteration 464/1000 | Loss: 0.00003802
Iteration 465/1000 | Loss: 0.00003799
Iteration 466/1000 | Loss: 0.00003796
Iteration 467/1000 | Loss: 0.00003796
Iteration 468/1000 | Loss: 0.00004959
Iteration 469/1000 | Loss: 0.00003774
Iteration 470/1000 | Loss: 0.00003770
Iteration 471/1000 | Loss: 0.00003770
Iteration 472/1000 | Loss: 0.00003769
Iteration 473/1000 | Loss: 0.00003769
Iteration 474/1000 | Loss: 0.00003768
Iteration 475/1000 | Loss: 0.00003768
Iteration 476/1000 | Loss: 0.00004880
Iteration 477/1000 | Loss: 0.00003762
Iteration 478/1000 | Loss: 0.00003760
Iteration 479/1000 | Loss: 0.00003760
Iteration 480/1000 | Loss: 0.00003758
Iteration 481/1000 | Loss: 0.00003758
Iteration 482/1000 | Loss: 0.00004011
Iteration 483/1000 | Loss: 0.00003756
Iteration 484/1000 | Loss: 0.00003755
Iteration 485/1000 | Loss: 0.00003755
Iteration 486/1000 | Loss: 0.00003755
Iteration 487/1000 | Loss: 0.00003755
Iteration 488/1000 | Loss: 0.00003755
Iteration 489/1000 | Loss: 0.00003755
Iteration 490/1000 | Loss: 0.00003755
Iteration 491/1000 | Loss: 0.00003755
Iteration 492/1000 | Loss: 0.00003755
Iteration 493/1000 | Loss: 0.00003755
Iteration 494/1000 | Loss: 0.00003755
Iteration 495/1000 | Loss: 0.00003755
Iteration 496/1000 | Loss: 0.00003755
Iteration 497/1000 | Loss: 0.00003755
Iteration 498/1000 | Loss: 0.00003755
Iteration 499/1000 | Loss: 0.00003755
Iteration 500/1000 | Loss: 0.00003755
Iteration 501/1000 | Loss: 0.00003755
Iteration 502/1000 | Loss: 0.00003755
Iteration 503/1000 | Loss: 0.00003755
Iteration 504/1000 | Loss: 0.00003755
Iteration 505/1000 | Loss: 0.00003755
Iteration 506/1000 | Loss: 0.00003755
Iteration 507/1000 | Loss: 0.00003755
Iteration 508/1000 | Loss: 0.00003755
Iteration 509/1000 | Loss: 0.00003755
Iteration 510/1000 | Loss: 0.00003755
Iteration 511/1000 | Loss: 0.00003755
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 511. Stopping optimization.
Last 5 losses: [3.7549598346231505e-05, 3.7549598346231505e-05, 3.7549598346231505e-05, 3.7549598346231505e-05, 3.7549598346231505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7549598346231505e-05

Optimization complete. Final v2v error: 4.2319655418396 mm

Highest mean error: 12.737037658691406 mm for frame 84

Lowest mean error: 2.989750862121582 mm for frame 216

Saving results

Total time: 535.2002310752869
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044316
Iteration 2/25 | Loss: 0.01044315
Iteration 3/25 | Loss: 0.01044315
Iteration 4/25 | Loss: 0.01044314
Iteration 5/25 | Loss: 0.01044314
Iteration 6/25 | Loss: 0.00563239
Iteration 7/25 | Loss: 0.00406653
Iteration 8/25 | Loss: 0.00395910
Iteration 9/25 | Loss: 0.00380504
Iteration 10/25 | Loss: 0.00339405
Iteration 11/25 | Loss: 0.00312178
Iteration 12/25 | Loss: 0.00261853
Iteration 13/25 | Loss: 0.00236996
Iteration 14/25 | Loss: 0.00261007
Iteration 15/25 | Loss: 0.00248124
Iteration 16/25 | Loss: 0.00239414
Iteration 17/25 | Loss: 0.00219683
Iteration 18/25 | Loss: 0.00208434
Iteration 19/25 | Loss: 0.00204232
Iteration 20/25 | Loss: 0.00196524
Iteration 21/25 | Loss: 0.00190900
Iteration 22/25 | Loss: 0.00188302
Iteration 23/25 | Loss: 0.00185996
Iteration 24/25 | Loss: 0.00185925
Iteration 25/25 | Loss: 0.00184121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.39088151
Iteration 2/25 | Loss: 0.00210132
Iteration 3/25 | Loss: 0.00210132
Iteration 4/25 | Loss: 0.00210132
Iteration 5/25 | Loss: 0.00210132
Iteration 6/25 | Loss: 0.00210132
Iteration 7/25 | Loss: 0.00210132
Iteration 8/25 | Loss: 0.00210132
Iteration 9/25 | Loss: 0.00210132
Iteration 10/25 | Loss: 0.00210132
Iteration 11/25 | Loss: 0.00210132
Iteration 12/25 | Loss: 0.00210132
Iteration 13/25 | Loss: 0.00210132
Iteration 14/25 | Loss: 0.00210132
Iteration 15/25 | Loss: 0.00210132
Iteration 16/25 | Loss: 0.00210132
Iteration 17/25 | Loss: 0.00210132
Iteration 18/25 | Loss: 0.00210132
Iteration 19/25 | Loss: 0.00210132
Iteration 20/25 | Loss: 0.00210132
Iteration 21/25 | Loss: 0.00210132
Iteration 22/25 | Loss: 0.00210132
Iteration 23/25 | Loss: 0.00210132
Iteration 24/25 | Loss: 0.00210132
Iteration 25/25 | Loss: 0.00210132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210132
Iteration 2/1000 | Loss: 0.00028928
Iteration 3/1000 | Loss: 0.00059403
Iteration 4/1000 | Loss: 0.00036983
Iteration 5/1000 | Loss: 0.00018365
Iteration 6/1000 | Loss: 0.00016359
Iteration 7/1000 | Loss: 0.00045699
Iteration 8/1000 | Loss: 0.00054931
Iteration 9/1000 | Loss: 0.00056726
Iteration 10/1000 | Loss: 0.00043550
Iteration 11/1000 | Loss: 0.00049434
Iteration 12/1000 | Loss: 0.00030304
Iteration 13/1000 | Loss: 0.00026866
Iteration 14/1000 | Loss: 0.00035181
Iteration 15/1000 | Loss: 0.00034193
Iteration 16/1000 | Loss: 0.00036242
Iteration 17/1000 | Loss: 0.00025941
Iteration 18/1000 | Loss: 0.00015869
Iteration 19/1000 | Loss: 0.00015915
Iteration 20/1000 | Loss: 0.00046815
Iteration 21/1000 | Loss: 0.00021984
Iteration 22/1000 | Loss: 0.00015321
Iteration 23/1000 | Loss: 0.00016876
Iteration 24/1000 | Loss: 0.00014972
Iteration 25/1000 | Loss: 0.00013590
Iteration 26/1000 | Loss: 0.00033309
Iteration 27/1000 | Loss: 0.00014979
Iteration 28/1000 | Loss: 0.00028844
Iteration 29/1000 | Loss: 0.00068453
Iteration 30/1000 | Loss: 0.00049937
Iteration 31/1000 | Loss: 0.00042137
Iteration 32/1000 | Loss: 0.00019562
Iteration 33/1000 | Loss: 0.00017457
Iteration 34/1000 | Loss: 0.00016695
Iteration 35/1000 | Loss: 0.00022204
Iteration 36/1000 | Loss: 0.00012833
Iteration 37/1000 | Loss: 0.00013626
Iteration 38/1000 | Loss: 0.00014024
Iteration 39/1000 | Loss: 0.00025155
Iteration 40/1000 | Loss: 0.00057399
Iteration 41/1000 | Loss: 0.00046195
Iteration 42/1000 | Loss: 0.00063354
Iteration 43/1000 | Loss: 0.00044507
Iteration 44/1000 | Loss: 0.00017353
Iteration 45/1000 | Loss: 0.00051883
Iteration 46/1000 | Loss: 0.00019860
Iteration 47/1000 | Loss: 0.00023260
Iteration 48/1000 | Loss: 0.00011798
Iteration 49/1000 | Loss: 0.00025870
Iteration 50/1000 | Loss: 0.00012912
Iteration 51/1000 | Loss: 0.00024788
Iteration 52/1000 | Loss: 0.00021226
Iteration 53/1000 | Loss: 0.00037372
Iteration 54/1000 | Loss: 0.00028756
Iteration 55/1000 | Loss: 0.00068501
Iteration 56/1000 | Loss: 0.00078908
Iteration 57/1000 | Loss: 0.00028576
Iteration 58/1000 | Loss: 0.00028525
Iteration 59/1000 | Loss: 0.00035342
Iteration 60/1000 | Loss: 0.00041241
Iteration 61/1000 | Loss: 0.00035788
Iteration 62/1000 | Loss: 0.00027007
Iteration 63/1000 | Loss: 0.00025362
Iteration 64/1000 | Loss: 0.00020868
Iteration 65/1000 | Loss: 0.00014879
Iteration 66/1000 | Loss: 0.00022016
Iteration 67/1000 | Loss: 0.00024289
Iteration 68/1000 | Loss: 0.00020555
Iteration 69/1000 | Loss: 0.00019789
Iteration 70/1000 | Loss: 0.00033850
Iteration 71/1000 | Loss: 0.00019364
Iteration 72/1000 | Loss: 0.00011804
Iteration 73/1000 | Loss: 0.00023642
Iteration 74/1000 | Loss: 0.00021245
Iteration 75/1000 | Loss: 0.00039256
Iteration 76/1000 | Loss: 0.00025939
Iteration 77/1000 | Loss: 0.00039209
Iteration 78/1000 | Loss: 0.00030417
Iteration 79/1000 | Loss: 0.00031953
Iteration 80/1000 | Loss: 0.00020631
Iteration 81/1000 | Loss: 0.00013883
Iteration 82/1000 | Loss: 0.00011317
Iteration 83/1000 | Loss: 0.00029984
Iteration 84/1000 | Loss: 0.00010961
Iteration 85/1000 | Loss: 0.00011128
Iteration 86/1000 | Loss: 0.00013337
Iteration 87/1000 | Loss: 0.00012104
Iteration 88/1000 | Loss: 0.00026083
Iteration 89/1000 | Loss: 0.00020725
Iteration 90/1000 | Loss: 0.00012256
Iteration 91/1000 | Loss: 0.00011346
Iteration 92/1000 | Loss: 0.00010641
Iteration 93/1000 | Loss: 0.00010313
Iteration 94/1000 | Loss: 0.00010122
Iteration 95/1000 | Loss: 0.00024120
Iteration 96/1000 | Loss: 0.00011520
Iteration 97/1000 | Loss: 0.00010245
Iteration 98/1000 | Loss: 0.00027059
Iteration 99/1000 | Loss: 0.00010574
Iteration 100/1000 | Loss: 0.00026238
Iteration 101/1000 | Loss: 0.00096747
Iteration 102/1000 | Loss: 0.00035786
Iteration 103/1000 | Loss: 0.00067394
Iteration 104/1000 | Loss: 0.00036583
Iteration 105/1000 | Loss: 0.00033241
Iteration 106/1000 | Loss: 0.00025717
Iteration 107/1000 | Loss: 0.00021290
Iteration 108/1000 | Loss: 0.00054738
Iteration 109/1000 | Loss: 0.00032976
Iteration 110/1000 | Loss: 0.00011227
Iteration 111/1000 | Loss: 0.00029876
Iteration 112/1000 | Loss: 0.00025843
Iteration 113/1000 | Loss: 0.00020427
Iteration 114/1000 | Loss: 0.00010765
Iteration 115/1000 | Loss: 0.00009842
Iteration 116/1000 | Loss: 0.00009536
Iteration 117/1000 | Loss: 0.00009347
Iteration 118/1000 | Loss: 0.00009190
Iteration 119/1000 | Loss: 0.00009089
Iteration 120/1000 | Loss: 0.00009009
Iteration 121/1000 | Loss: 0.00008962
Iteration 122/1000 | Loss: 0.00008920
Iteration 123/1000 | Loss: 0.00008886
Iteration 124/1000 | Loss: 0.00008854
Iteration 125/1000 | Loss: 0.00020103
Iteration 126/1000 | Loss: 0.00042115
Iteration 127/1000 | Loss: 0.00051243
Iteration 128/1000 | Loss: 0.00008908
Iteration 129/1000 | Loss: 0.00008718
Iteration 130/1000 | Loss: 0.00008636
Iteration 131/1000 | Loss: 0.00008599
Iteration 132/1000 | Loss: 0.00008579
Iteration 133/1000 | Loss: 0.00008572
Iteration 134/1000 | Loss: 0.00008571
Iteration 135/1000 | Loss: 0.00008571
Iteration 136/1000 | Loss: 0.00008571
Iteration 137/1000 | Loss: 0.00008571
Iteration 138/1000 | Loss: 0.00008565
Iteration 139/1000 | Loss: 0.00008556
Iteration 140/1000 | Loss: 0.00008550
Iteration 141/1000 | Loss: 0.00008545
Iteration 142/1000 | Loss: 0.00008545
Iteration 143/1000 | Loss: 0.00008545
Iteration 144/1000 | Loss: 0.00008544
Iteration 145/1000 | Loss: 0.00008544
Iteration 146/1000 | Loss: 0.00008543
Iteration 147/1000 | Loss: 0.00008543
Iteration 148/1000 | Loss: 0.00008540
Iteration 149/1000 | Loss: 0.00008539
Iteration 150/1000 | Loss: 0.00008536
Iteration 151/1000 | Loss: 0.00008536
Iteration 152/1000 | Loss: 0.00008536
Iteration 153/1000 | Loss: 0.00008536
Iteration 154/1000 | Loss: 0.00008536
Iteration 155/1000 | Loss: 0.00008536
Iteration 156/1000 | Loss: 0.00008536
Iteration 157/1000 | Loss: 0.00008536
Iteration 158/1000 | Loss: 0.00008536
Iteration 159/1000 | Loss: 0.00008536
Iteration 160/1000 | Loss: 0.00008535
Iteration 161/1000 | Loss: 0.00008533
Iteration 162/1000 | Loss: 0.00008532
Iteration 163/1000 | Loss: 0.00008532
Iteration 164/1000 | Loss: 0.00008532
Iteration 165/1000 | Loss: 0.00008532
Iteration 166/1000 | Loss: 0.00008531
Iteration 167/1000 | Loss: 0.00008530
Iteration 168/1000 | Loss: 0.00008529
Iteration 169/1000 | Loss: 0.00008529
Iteration 170/1000 | Loss: 0.00008529
Iteration 171/1000 | Loss: 0.00008529
Iteration 172/1000 | Loss: 0.00008527
Iteration 173/1000 | Loss: 0.00008527
Iteration 174/1000 | Loss: 0.00008527
Iteration 175/1000 | Loss: 0.00008526
Iteration 176/1000 | Loss: 0.00008526
Iteration 177/1000 | Loss: 0.00008526
Iteration 178/1000 | Loss: 0.00008526
Iteration 179/1000 | Loss: 0.00008526
Iteration 180/1000 | Loss: 0.00008525
Iteration 181/1000 | Loss: 0.00008525
Iteration 182/1000 | Loss: 0.00008525
Iteration 183/1000 | Loss: 0.00008524
Iteration 184/1000 | Loss: 0.00008524
Iteration 185/1000 | Loss: 0.00008524
Iteration 186/1000 | Loss: 0.00008524
Iteration 187/1000 | Loss: 0.00008524
Iteration 188/1000 | Loss: 0.00008524
Iteration 189/1000 | Loss: 0.00008524
Iteration 190/1000 | Loss: 0.00008523
Iteration 191/1000 | Loss: 0.00008523
Iteration 192/1000 | Loss: 0.00008523
Iteration 193/1000 | Loss: 0.00008522
Iteration 194/1000 | Loss: 0.00008521
Iteration 195/1000 | Loss: 0.00008521
Iteration 196/1000 | Loss: 0.00008520
Iteration 197/1000 | Loss: 0.00008519
Iteration 198/1000 | Loss: 0.00008518
Iteration 199/1000 | Loss: 0.00008518
Iteration 200/1000 | Loss: 0.00008517
Iteration 201/1000 | Loss: 0.00008517
Iteration 202/1000 | Loss: 0.00008517
Iteration 203/1000 | Loss: 0.00008516
Iteration 204/1000 | Loss: 0.00008516
Iteration 205/1000 | Loss: 0.00008515
Iteration 206/1000 | Loss: 0.00008514
Iteration 207/1000 | Loss: 0.00008514
Iteration 208/1000 | Loss: 0.00008514
Iteration 209/1000 | Loss: 0.00008514
Iteration 210/1000 | Loss: 0.00008514
Iteration 211/1000 | Loss: 0.00008514
Iteration 212/1000 | Loss: 0.00008514
Iteration 213/1000 | Loss: 0.00008514
Iteration 214/1000 | Loss: 0.00008513
Iteration 215/1000 | Loss: 0.00008513
Iteration 216/1000 | Loss: 0.00008513
Iteration 217/1000 | Loss: 0.00008513
Iteration 218/1000 | Loss: 0.00008512
Iteration 219/1000 | Loss: 0.00008511
Iteration 220/1000 | Loss: 0.00008511
Iteration 221/1000 | Loss: 0.00008511
Iteration 222/1000 | Loss: 0.00008511
Iteration 223/1000 | Loss: 0.00008510
Iteration 224/1000 | Loss: 0.00008510
Iteration 225/1000 | Loss: 0.00008510
Iteration 226/1000 | Loss: 0.00008509
Iteration 227/1000 | Loss: 0.00008508
Iteration 228/1000 | Loss: 0.00008508
Iteration 229/1000 | Loss: 0.00008508
Iteration 230/1000 | Loss: 0.00008508
Iteration 231/1000 | Loss: 0.00008508
Iteration 232/1000 | Loss: 0.00008508
Iteration 233/1000 | Loss: 0.00008507
Iteration 234/1000 | Loss: 0.00008505
Iteration 235/1000 | Loss: 0.00008505
Iteration 236/1000 | Loss: 0.00008505
Iteration 237/1000 | Loss: 0.00008505
Iteration 238/1000 | Loss: 0.00008505
Iteration 239/1000 | Loss: 0.00008504
Iteration 240/1000 | Loss: 0.00008504
Iteration 241/1000 | Loss: 0.00008504
Iteration 242/1000 | Loss: 0.00008503
Iteration 243/1000 | Loss: 0.00008503
Iteration 244/1000 | Loss: 0.00008503
Iteration 245/1000 | Loss: 0.00008503
Iteration 246/1000 | Loss: 0.00008503
Iteration 247/1000 | Loss: 0.00008503
Iteration 248/1000 | Loss: 0.00008502
Iteration 249/1000 | Loss: 0.00008502
Iteration 250/1000 | Loss: 0.00008502
Iteration 251/1000 | Loss: 0.00008502
Iteration 252/1000 | Loss: 0.00008501
Iteration 253/1000 | Loss: 0.00008501
Iteration 254/1000 | Loss: 0.00008500
Iteration 255/1000 | Loss: 0.00008500
Iteration 256/1000 | Loss: 0.00008499
Iteration 257/1000 | Loss: 0.00008499
Iteration 258/1000 | Loss: 0.00008499
Iteration 259/1000 | Loss: 0.00008498
Iteration 260/1000 | Loss: 0.00008498
Iteration 261/1000 | Loss: 0.00008498
Iteration 262/1000 | Loss: 0.00008498
Iteration 263/1000 | Loss: 0.00008498
Iteration 264/1000 | Loss: 0.00008498
Iteration 265/1000 | Loss: 0.00008498
Iteration 266/1000 | Loss: 0.00008497
Iteration 267/1000 | Loss: 0.00008497
Iteration 268/1000 | Loss: 0.00008497
Iteration 269/1000 | Loss: 0.00008497
Iteration 270/1000 | Loss: 0.00008497
Iteration 271/1000 | Loss: 0.00008497
Iteration 272/1000 | Loss: 0.00008497
Iteration 273/1000 | Loss: 0.00008497
Iteration 274/1000 | Loss: 0.00008497
Iteration 275/1000 | Loss: 0.00008496
Iteration 276/1000 | Loss: 0.00008496
Iteration 277/1000 | Loss: 0.00008496
Iteration 278/1000 | Loss: 0.00008496
Iteration 279/1000 | Loss: 0.00008496
Iteration 280/1000 | Loss: 0.00008495
Iteration 281/1000 | Loss: 0.00008495
Iteration 282/1000 | Loss: 0.00008495
Iteration 283/1000 | Loss: 0.00008495
Iteration 284/1000 | Loss: 0.00008494
Iteration 285/1000 | Loss: 0.00008494
Iteration 286/1000 | Loss: 0.00008493
Iteration 287/1000 | Loss: 0.00008493
Iteration 288/1000 | Loss: 0.00008492
Iteration 289/1000 | Loss: 0.00008492
Iteration 290/1000 | Loss: 0.00008492
Iteration 291/1000 | Loss: 0.00008492
Iteration 292/1000 | Loss: 0.00008492
Iteration 293/1000 | Loss: 0.00008492
Iteration 294/1000 | Loss: 0.00008492
Iteration 295/1000 | Loss: 0.00008492
Iteration 296/1000 | Loss: 0.00008492
Iteration 297/1000 | Loss: 0.00008492
Iteration 298/1000 | Loss: 0.00008492
Iteration 299/1000 | Loss: 0.00008492
Iteration 300/1000 | Loss: 0.00008492
Iteration 301/1000 | Loss: 0.00008492
Iteration 302/1000 | Loss: 0.00008491
Iteration 303/1000 | Loss: 0.00008491
Iteration 304/1000 | Loss: 0.00008491
Iteration 305/1000 | Loss: 0.00008491
Iteration 306/1000 | Loss: 0.00008490
Iteration 307/1000 | Loss: 0.00008490
Iteration 308/1000 | Loss: 0.00008490
Iteration 309/1000 | Loss: 0.00008490
Iteration 310/1000 | Loss: 0.00008490
Iteration 311/1000 | Loss: 0.00008490
Iteration 312/1000 | Loss: 0.00008490
Iteration 313/1000 | Loss: 0.00008490
Iteration 314/1000 | Loss: 0.00008490
Iteration 315/1000 | Loss: 0.00008489
Iteration 316/1000 | Loss: 0.00008489
Iteration 317/1000 | Loss: 0.00008489
Iteration 318/1000 | Loss: 0.00008489
Iteration 319/1000 | Loss: 0.00008489
Iteration 320/1000 | Loss: 0.00008489
Iteration 321/1000 | Loss: 0.00008489
Iteration 322/1000 | Loss: 0.00008489
Iteration 323/1000 | Loss: 0.00008489
Iteration 324/1000 | Loss: 0.00008489
Iteration 325/1000 | Loss: 0.00008489
Iteration 326/1000 | Loss: 0.00008489
Iteration 327/1000 | Loss: 0.00008488
Iteration 328/1000 | Loss: 0.00008488
Iteration 329/1000 | Loss: 0.00008488
Iteration 330/1000 | Loss: 0.00008488
Iteration 331/1000 | Loss: 0.00008488
Iteration 332/1000 | Loss: 0.00008488
Iteration 333/1000 | Loss: 0.00008488
Iteration 334/1000 | Loss: 0.00008488
Iteration 335/1000 | Loss: 0.00008488
Iteration 336/1000 | Loss: 0.00008488
Iteration 337/1000 | Loss: 0.00008488
Iteration 338/1000 | Loss: 0.00008488
Iteration 339/1000 | Loss: 0.00008488
Iteration 340/1000 | Loss: 0.00008488
Iteration 341/1000 | Loss: 0.00008488
Iteration 342/1000 | Loss: 0.00008488
Iteration 343/1000 | Loss: 0.00008488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 343. Stopping optimization.
Last 5 losses: [8.487654849886894e-05, 8.487654849886894e-05, 8.487654849886894e-05, 8.487654849886894e-05, 8.487654849886894e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.487654849886894e-05

Optimization complete. Final v2v error: 6.321693420410156 mm

Highest mean error: 12.402009010314941 mm for frame 48

Lowest mean error: 4.158199787139893 mm for frame 11

Saving results

Total time: 265.2498617172241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_alexandra_posed_002/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_alexandra_posed_002/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962276
Iteration 2/25 | Loss: 0.00253375
Iteration 3/25 | Loss: 0.00212401
Iteration 4/25 | Loss: 0.00205726
Iteration 5/25 | Loss: 0.00201735
Iteration 6/25 | Loss: 0.00202108
Iteration 7/25 | Loss: 0.00199491
Iteration 8/25 | Loss: 0.00199840
Iteration 9/25 | Loss: 0.00198486
Iteration 10/25 | Loss: 0.00197875
Iteration 11/25 | Loss: 0.00196423
Iteration 12/25 | Loss: 0.00195898
Iteration 13/25 | Loss: 0.00194694
Iteration 14/25 | Loss: 0.00194958
Iteration 15/25 | Loss: 0.00194436
Iteration 16/25 | Loss: 0.00194361
Iteration 17/25 | Loss: 0.00194341
Iteration 18/25 | Loss: 0.00194335
Iteration 19/25 | Loss: 0.00194335
Iteration 20/25 | Loss: 0.00194335
Iteration 21/25 | Loss: 0.00194332
Iteration 22/25 | Loss: 0.00194332
Iteration 23/25 | Loss: 0.00194332
Iteration 24/25 | Loss: 0.00194332
Iteration 25/25 | Loss: 0.00194332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26221824
Iteration 2/25 | Loss: 0.00625935
Iteration 3/25 | Loss: 0.00593192
Iteration 4/25 | Loss: 0.00593192
Iteration 5/25 | Loss: 0.00593192
Iteration 6/25 | Loss: 0.00593191
Iteration 7/25 | Loss: 0.00593191
Iteration 8/25 | Loss: 0.00593191
Iteration 9/25 | Loss: 0.00593191
Iteration 10/25 | Loss: 0.00593191
Iteration 11/25 | Loss: 0.00593191
Iteration 12/25 | Loss: 0.00593191
Iteration 13/25 | Loss: 0.00593191
Iteration 14/25 | Loss: 0.00593191
Iteration 15/25 | Loss: 0.00593191
Iteration 16/25 | Loss: 0.00593191
Iteration 17/25 | Loss: 0.00593191
Iteration 18/25 | Loss: 0.00593191
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.005931912921369076, 0.005931912921369076, 0.005931912921369076, 0.005931912921369076, 0.005931912921369076]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005931912921369076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00593191
Iteration 2/1000 | Loss: 0.00078556
Iteration 3/1000 | Loss: 0.00101647
Iteration 4/1000 | Loss: 0.00071549
Iteration 5/1000 | Loss: 0.00051965
Iteration 6/1000 | Loss: 0.00306281
Iteration 7/1000 | Loss: 0.00994775
Iteration 8/1000 | Loss: 0.01570597
Iteration 9/1000 | Loss: 0.00191583
Iteration 10/1000 | Loss: 0.00202731
Iteration 11/1000 | Loss: 0.00162656
Iteration 12/1000 | Loss: 0.00089126
Iteration 13/1000 | Loss: 0.00194578
Iteration 14/1000 | Loss: 0.00070397
Iteration 15/1000 | Loss: 0.00087111
Iteration 16/1000 | Loss: 0.00039229
Iteration 17/1000 | Loss: 0.00253464
Iteration 18/1000 | Loss: 0.00041573
Iteration 19/1000 | Loss: 0.00258548
Iteration 20/1000 | Loss: 0.00037237
Iteration 21/1000 | Loss: 0.00060253
Iteration 22/1000 | Loss: 0.00067395
Iteration 23/1000 | Loss: 0.00465122
Iteration 24/1000 | Loss: 0.00379636
Iteration 25/1000 | Loss: 0.02414487
Iteration 26/1000 | Loss: 0.00181695
Iteration 27/1000 | Loss: 0.00420222
Iteration 28/1000 | Loss: 0.00138667
Iteration 29/1000 | Loss: 0.00068491
Iteration 30/1000 | Loss: 0.00085927
Iteration 31/1000 | Loss: 0.00173466
Iteration 32/1000 | Loss: 0.00030717
Iteration 33/1000 | Loss: 0.00074302
Iteration 34/1000 | Loss: 0.00020542
Iteration 35/1000 | Loss: 0.00011943
Iteration 36/1000 | Loss: 0.00006901
Iteration 37/1000 | Loss: 0.00005456
Iteration 38/1000 | Loss: 0.00045380
Iteration 39/1000 | Loss: 0.00004028
Iteration 40/1000 | Loss: 0.00008474
Iteration 41/1000 | Loss: 0.00009241
Iteration 42/1000 | Loss: 0.00002952
Iteration 43/1000 | Loss: 0.00002692
Iteration 44/1000 | Loss: 0.00017501
Iteration 45/1000 | Loss: 0.00030661
Iteration 46/1000 | Loss: 0.00002331
Iteration 47/1000 | Loss: 0.00005515
Iteration 48/1000 | Loss: 0.00002089
Iteration 49/1000 | Loss: 0.00012266
Iteration 50/1000 | Loss: 0.00002129
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001816
Iteration 53/1000 | Loss: 0.00001767
Iteration 54/1000 | Loss: 0.00005654
Iteration 55/1000 | Loss: 0.00001755
Iteration 56/1000 | Loss: 0.00001715
Iteration 57/1000 | Loss: 0.00001698
Iteration 58/1000 | Loss: 0.00001694
Iteration 59/1000 | Loss: 0.00001690
Iteration 60/1000 | Loss: 0.00001677
Iteration 61/1000 | Loss: 0.00001676
Iteration 62/1000 | Loss: 0.00001675
Iteration 63/1000 | Loss: 0.00001675
Iteration 64/1000 | Loss: 0.00001672
Iteration 65/1000 | Loss: 0.00001671
Iteration 66/1000 | Loss: 0.00001671
Iteration 67/1000 | Loss: 0.00001671
Iteration 68/1000 | Loss: 0.00001671
Iteration 69/1000 | Loss: 0.00001671
Iteration 70/1000 | Loss: 0.00001671
Iteration 71/1000 | Loss: 0.00001670
Iteration 72/1000 | Loss: 0.00001670
Iteration 73/1000 | Loss: 0.00001670
Iteration 74/1000 | Loss: 0.00001670
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001669
Iteration 77/1000 | Loss: 0.00001669
Iteration 78/1000 | Loss: 0.00001669
Iteration 79/1000 | Loss: 0.00001668
Iteration 80/1000 | Loss: 0.00001668
Iteration 81/1000 | Loss: 0.00001668
Iteration 82/1000 | Loss: 0.00001667
Iteration 83/1000 | Loss: 0.00001667
Iteration 84/1000 | Loss: 0.00001667
Iteration 85/1000 | Loss: 0.00001666
Iteration 86/1000 | Loss: 0.00001666
Iteration 87/1000 | Loss: 0.00001666
Iteration 88/1000 | Loss: 0.00001666
Iteration 89/1000 | Loss: 0.00001666
Iteration 90/1000 | Loss: 0.00001666
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001665
Iteration 93/1000 | Loss: 0.00001665
Iteration 94/1000 | Loss: 0.00001665
Iteration 95/1000 | Loss: 0.00001664
Iteration 96/1000 | Loss: 0.00001664
Iteration 97/1000 | Loss: 0.00001664
Iteration 98/1000 | Loss: 0.00001664
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001663
Iteration 102/1000 | Loss: 0.00001663
Iteration 103/1000 | Loss: 0.00001663
Iteration 104/1000 | Loss: 0.00001663
Iteration 105/1000 | Loss: 0.00001662
Iteration 106/1000 | Loss: 0.00001662
Iteration 107/1000 | Loss: 0.00001661
Iteration 108/1000 | Loss: 0.00001661
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001660
Iteration 111/1000 | Loss: 0.00001660
Iteration 112/1000 | Loss: 0.00001660
Iteration 113/1000 | Loss: 0.00001660
Iteration 114/1000 | Loss: 0.00001660
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001660
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001660
Iteration 121/1000 | Loss: 0.00001660
Iteration 122/1000 | Loss: 0.00001660
Iteration 123/1000 | Loss: 0.00001660
Iteration 124/1000 | Loss: 0.00001660
Iteration 125/1000 | Loss: 0.00001660
Iteration 126/1000 | Loss: 0.00001660
Iteration 127/1000 | Loss: 0.00001660
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001660
Iteration 130/1000 | Loss: 0.00001660
Iteration 131/1000 | Loss: 0.00001660
Iteration 132/1000 | Loss: 0.00001660
Iteration 133/1000 | Loss: 0.00001660
Iteration 134/1000 | Loss: 0.00001660
Iteration 135/1000 | Loss: 0.00001660
Iteration 136/1000 | Loss: 0.00001660
Iteration 137/1000 | Loss: 0.00001660
Iteration 138/1000 | Loss: 0.00001660
Iteration 139/1000 | Loss: 0.00001660
Iteration 140/1000 | Loss: 0.00001660
Iteration 141/1000 | Loss: 0.00001660
Iteration 142/1000 | Loss: 0.00001660
Iteration 143/1000 | Loss: 0.00001660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.6601590687059797e-05, 1.6601590687059797e-05, 1.6601590687059797e-05, 1.6601590687059797e-05, 1.6601590687059797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6601590687059797e-05

Optimization complete. Final v2v error: 3.5242161750793457 mm

Highest mean error: 3.6499195098876953 mm for frame 35

Lowest mean error: 3.3249897956848145 mm for frame 112

Saving results

Total time: 123.12216854095459
