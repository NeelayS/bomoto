Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=24, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 1344-1399
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838920
Iteration 2/25 | Loss: 0.00132121
Iteration 3/25 | Loss: 0.00088793
Iteration 4/25 | Loss: 0.00083939
Iteration 5/25 | Loss: 0.00082545
Iteration 6/25 | Loss: 0.00082208
Iteration 7/25 | Loss: 0.00082142
Iteration 8/25 | Loss: 0.00082142
Iteration 9/25 | Loss: 0.00082142
Iteration 10/25 | Loss: 0.00082142
Iteration 11/25 | Loss: 0.00082142
Iteration 12/25 | Loss: 0.00082142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008214159170165658, 0.0008214159170165658, 0.0008214159170165658, 0.0008214159170165658, 0.0008214159170165658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008214159170165658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99242115
Iteration 2/25 | Loss: 0.00029891
Iteration 3/25 | Loss: 0.00029886
Iteration 4/25 | Loss: 0.00029886
Iteration 5/25 | Loss: 0.00029886
Iteration 6/25 | Loss: 0.00029886
Iteration 7/25 | Loss: 0.00029886
Iteration 8/25 | Loss: 0.00029886
Iteration 9/25 | Loss: 0.00029886
Iteration 10/25 | Loss: 0.00029886
Iteration 11/25 | Loss: 0.00029886
Iteration 12/25 | Loss: 0.00029886
Iteration 13/25 | Loss: 0.00029886
Iteration 14/25 | Loss: 0.00029886
Iteration 15/25 | Loss: 0.00029886
Iteration 16/25 | Loss: 0.00029886
Iteration 17/25 | Loss: 0.00029886
Iteration 18/25 | Loss: 0.00029886
Iteration 19/25 | Loss: 0.00029886
Iteration 20/25 | Loss: 0.00029886
Iteration 21/25 | Loss: 0.00029886
Iteration 22/25 | Loss: 0.00029886
Iteration 23/25 | Loss: 0.00029886
Iteration 24/25 | Loss: 0.00029886
Iteration 25/25 | Loss: 0.00029886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029886
Iteration 2/1000 | Loss: 0.00006997
Iteration 3/1000 | Loss: 0.00005139
Iteration 4/1000 | Loss: 0.00004441
Iteration 5/1000 | Loss: 0.00004178
Iteration 6/1000 | Loss: 0.00004053
Iteration 7/1000 | Loss: 0.00003969
Iteration 8/1000 | Loss: 0.00003884
Iteration 9/1000 | Loss: 0.00003817
Iteration 10/1000 | Loss: 0.00003772
Iteration 11/1000 | Loss: 0.00003729
Iteration 12/1000 | Loss: 0.00003705
Iteration 13/1000 | Loss: 0.00003689
Iteration 14/1000 | Loss: 0.00003675
Iteration 15/1000 | Loss: 0.00003665
Iteration 16/1000 | Loss: 0.00003656
Iteration 17/1000 | Loss: 0.00003655
Iteration 18/1000 | Loss: 0.00003655
Iteration 19/1000 | Loss: 0.00003649
Iteration 20/1000 | Loss: 0.00003647
Iteration 21/1000 | Loss: 0.00003647
Iteration 22/1000 | Loss: 0.00003647
Iteration 23/1000 | Loss: 0.00003646
Iteration 24/1000 | Loss: 0.00003645
Iteration 25/1000 | Loss: 0.00003645
Iteration 26/1000 | Loss: 0.00003644
Iteration 27/1000 | Loss: 0.00003644
Iteration 28/1000 | Loss: 0.00003643
Iteration 29/1000 | Loss: 0.00003643
Iteration 30/1000 | Loss: 0.00003643
Iteration 31/1000 | Loss: 0.00003642
Iteration 32/1000 | Loss: 0.00003642
Iteration 33/1000 | Loss: 0.00003642
Iteration 34/1000 | Loss: 0.00003642
Iteration 35/1000 | Loss: 0.00003642
Iteration 36/1000 | Loss: 0.00003642
Iteration 37/1000 | Loss: 0.00003642
Iteration 38/1000 | Loss: 0.00003642
Iteration 39/1000 | Loss: 0.00003642
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 39. Stopping optimization.
Last 5 losses: [3.641704824985936e-05, 3.641704824985936e-05, 3.641704824985936e-05, 3.641704824985936e-05, 3.641704824985936e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.641704824985936e-05

Optimization complete. Final v2v error: 4.904390811920166 mm

Highest mean error: 6.519172191619873 mm for frame 216

Lowest mean error: 4.154055595397949 mm for frame 104

Saving results

Total time: 40.92475962638855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919391
Iteration 2/25 | Loss: 0.00092919
Iteration 3/25 | Loss: 0.00078205
Iteration 4/25 | Loss: 0.00074997
Iteration 5/25 | Loss: 0.00074333
Iteration 6/25 | Loss: 0.00074193
Iteration 7/25 | Loss: 0.00074156
Iteration 8/25 | Loss: 0.00074156
Iteration 9/25 | Loss: 0.00074156
Iteration 10/25 | Loss: 0.00074156
Iteration 11/25 | Loss: 0.00074156
Iteration 12/25 | Loss: 0.00074156
Iteration 13/25 | Loss: 0.00074156
Iteration 14/25 | Loss: 0.00074156
Iteration 15/25 | Loss: 0.00074156
Iteration 16/25 | Loss: 0.00074156
Iteration 17/25 | Loss: 0.00074156
Iteration 18/25 | Loss: 0.00074156
Iteration 19/25 | Loss: 0.00074156
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007415561703965068, 0.0007415561703965068, 0.0007415561703965068, 0.0007415561703965068, 0.0007415561703965068]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007415561703965068

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28912210
Iteration 2/25 | Loss: 0.00047770
Iteration 3/25 | Loss: 0.00047770
Iteration 4/25 | Loss: 0.00047770
Iteration 5/25 | Loss: 0.00047770
Iteration 6/25 | Loss: 0.00047770
Iteration 7/25 | Loss: 0.00047770
Iteration 8/25 | Loss: 0.00047770
Iteration 9/25 | Loss: 0.00047770
Iteration 10/25 | Loss: 0.00047770
Iteration 11/25 | Loss: 0.00047770
Iteration 12/25 | Loss: 0.00047770
Iteration 13/25 | Loss: 0.00047770
Iteration 14/25 | Loss: 0.00047770
Iteration 15/25 | Loss: 0.00047770
Iteration 16/25 | Loss: 0.00047770
Iteration 17/25 | Loss: 0.00047770
Iteration 18/25 | Loss: 0.00047770
Iteration 19/25 | Loss: 0.00047770
Iteration 20/25 | Loss: 0.00047770
Iteration 21/25 | Loss: 0.00047770
Iteration 22/25 | Loss: 0.00047770
Iteration 23/25 | Loss: 0.00047770
Iteration 24/25 | Loss: 0.00047770
Iteration 25/25 | Loss: 0.00047770

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047770
Iteration 2/1000 | Loss: 0.00005857
Iteration 3/1000 | Loss: 0.00004541
Iteration 4/1000 | Loss: 0.00004119
Iteration 5/1000 | Loss: 0.00003790
Iteration 6/1000 | Loss: 0.00003613
Iteration 7/1000 | Loss: 0.00003527
Iteration 8/1000 | Loss: 0.00003444
Iteration 9/1000 | Loss: 0.00003412
Iteration 10/1000 | Loss: 0.00003384
Iteration 11/1000 | Loss: 0.00003371
Iteration 12/1000 | Loss: 0.00003367
Iteration 13/1000 | Loss: 0.00003366
Iteration 14/1000 | Loss: 0.00003351
Iteration 15/1000 | Loss: 0.00003342
Iteration 16/1000 | Loss: 0.00003339
Iteration 17/1000 | Loss: 0.00003323
Iteration 18/1000 | Loss: 0.00003317
Iteration 19/1000 | Loss: 0.00003308
Iteration 20/1000 | Loss: 0.00003308
Iteration 21/1000 | Loss: 0.00003303
Iteration 22/1000 | Loss: 0.00003303
Iteration 23/1000 | Loss: 0.00003303
Iteration 24/1000 | Loss: 0.00003303
Iteration 25/1000 | Loss: 0.00003303
Iteration 26/1000 | Loss: 0.00003302
Iteration 27/1000 | Loss: 0.00003302
Iteration 28/1000 | Loss: 0.00003301
Iteration 29/1000 | Loss: 0.00003299
Iteration 30/1000 | Loss: 0.00003299
Iteration 31/1000 | Loss: 0.00003298
Iteration 32/1000 | Loss: 0.00003298
Iteration 33/1000 | Loss: 0.00003298
Iteration 34/1000 | Loss: 0.00003298
Iteration 35/1000 | Loss: 0.00003298
Iteration 36/1000 | Loss: 0.00003298
Iteration 37/1000 | Loss: 0.00003298
Iteration 38/1000 | Loss: 0.00003298
Iteration 39/1000 | Loss: 0.00003297
Iteration 40/1000 | Loss: 0.00003297
Iteration 41/1000 | Loss: 0.00003297
Iteration 42/1000 | Loss: 0.00003296
Iteration 43/1000 | Loss: 0.00003296
Iteration 44/1000 | Loss: 0.00003296
Iteration 45/1000 | Loss: 0.00003296
Iteration 46/1000 | Loss: 0.00003296
Iteration 47/1000 | Loss: 0.00003296
Iteration 48/1000 | Loss: 0.00003295
Iteration 49/1000 | Loss: 0.00003295
Iteration 50/1000 | Loss: 0.00003295
Iteration 51/1000 | Loss: 0.00003295
Iteration 52/1000 | Loss: 0.00003295
Iteration 53/1000 | Loss: 0.00003294
Iteration 54/1000 | Loss: 0.00003293
Iteration 55/1000 | Loss: 0.00003293
Iteration 56/1000 | Loss: 0.00003293
Iteration 57/1000 | Loss: 0.00003293
Iteration 58/1000 | Loss: 0.00003293
Iteration 59/1000 | Loss: 0.00003293
Iteration 60/1000 | Loss: 0.00003293
Iteration 61/1000 | Loss: 0.00003293
Iteration 62/1000 | Loss: 0.00003293
Iteration 63/1000 | Loss: 0.00003293
Iteration 64/1000 | Loss: 0.00003293
Iteration 65/1000 | Loss: 0.00003292
Iteration 66/1000 | Loss: 0.00003292
Iteration 67/1000 | Loss: 0.00003292
Iteration 68/1000 | Loss: 0.00003292
Iteration 69/1000 | Loss: 0.00003292
Iteration 70/1000 | Loss: 0.00003292
Iteration 71/1000 | Loss: 0.00003292
Iteration 72/1000 | Loss: 0.00003291
Iteration 73/1000 | Loss: 0.00003291
Iteration 74/1000 | Loss: 0.00003291
Iteration 75/1000 | Loss: 0.00003291
Iteration 76/1000 | Loss: 0.00003291
Iteration 77/1000 | Loss: 0.00003290
Iteration 78/1000 | Loss: 0.00003290
Iteration 79/1000 | Loss: 0.00003290
Iteration 80/1000 | Loss: 0.00003290
Iteration 81/1000 | Loss: 0.00003290
Iteration 82/1000 | Loss: 0.00003290
Iteration 83/1000 | Loss: 0.00003290
Iteration 84/1000 | Loss: 0.00003290
Iteration 85/1000 | Loss: 0.00003290
Iteration 86/1000 | Loss: 0.00003290
Iteration 87/1000 | Loss: 0.00003290
Iteration 88/1000 | Loss: 0.00003290
Iteration 89/1000 | Loss: 0.00003290
Iteration 90/1000 | Loss: 0.00003290
Iteration 91/1000 | Loss: 0.00003290
Iteration 92/1000 | Loss: 0.00003290
Iteration 93/1000 | Loss: 0.00003290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [3.2904114050325006e-05, 3.2904114050325006e-05, 3.2904114050325006e-05, 3.2904114050325006e-05, 3.2904114050325006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2904114050325006e-05

Optimization complete. Final v2v error: 4.863656044006348 mm

Highest mean error: 5.161003589630127 mm for frame 101

Lowest mean error: 4.485485553741455 mm for frame 25

Saving results

Total time: 35.325281620025635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00749690
Iteration 2/25 | Loss: 0.00186592
Iteration 3/25 | Loss: 0.00134176
Iteration 4/25 | Loss: 0.00122488
Iteration 5/25 | Loss: 0.00114063
Iteration 6/25 | Loss: 0.00110211
Iteration 7/25 | Loss: 0.00106731
Iteration 8/25 | Loss: 0.00133064
Iteration 9/25 | Loss: 0.00133975
Iteration 10/25 | Loss: 0.00086406
Iteration 11/25 | Loss: 0.00080068
Iteration 12/25 | Loss: 0.00074480
Iteration 13/25 | Loss: 0.00074235
Iteration 14/25 | Loss: 0.00074650
Iteration 15/25 | Loss: 0.00075179
Iteration 16/25 | Loss: 0.00072529
Iteration 17/25 | Loss: 0.00072336
Iteration 18/25 | Loss: 0.00072309
Iteration 19/25 | Loss: 0.00073500
Iteration 20/25 | Loss: 0.00072109
Iteration 21/25 | Loss: 0.00072585
Iteration 22/25 | Loss: 0.00072770
Iteration 23/25 | Loss: 0.00073002
Iteration 24/25 | Loss: 0.00072035
Iteration 25/25 | Loss: 0.00072634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52315807
Iteration 2/25 | Loss: 0.00040429
Iteration 3/25 | Loss: 0.00035756
Iteration 4/25 | Loss: 0.00035732
Iteration 5/25 | Loss: 0.00035732
Iteration 6/25 | Loss: 0.00035732
Iteration 7/25 | Loss: 0.00035732
Iteration 8/25 | Loss: 0.00035732
Iteration 9/25 | Loss: 0.00035732
Iteration 10/25 | Loss: 0.00035732
Iteration 11/25 | Loss: 0.00035732
Iteration 12/25 | Loss: 0.00035732
Iteration 13/25 | Loss: 0.00035732
Iteration 14/25 | Loss: 0.00035732
Iteration 15/25 | Loss: 0.00035732
Iteration 16/25 | Loss: 0.00035732
Iteration 17/25 | Loss: 0.00035732
Iteration 18/25 | Loss: 0.00035732
Iteration 19/25 | Loss: 0.00035732
Iteration 20/25 | Loss: 0.00035732
Iteration 21/25 | Loss: 0.00035732
Iteration 22/25 | Loss: 0.00035732
Iteration 23/25 | Loss: 0.00035732
Iteration 24/25 | Loss: 0.00035732
Iteration 25/25 | Loss: 0.00035732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00035732
Iteration 2/1000 | Loss: 0.00036698
Iteration 3/1000 | Loss: 0.00005460
Iteration 4/1000 | Loss: 0.00004123
Iteration 5/1000 | Loss: 0.00003645
Iteration 6/1000 | Loss: 0.00003363
Iteration 7/1000 | Loss: 0.00003134
Iteration 8/1000 | Loss: 0.00003024
Iteration 9/1000 | Loss: 0.00002938
Iteration 10/1000 | Loss: 0.00002877
Iteration 11/1000 | Loss: 0.00002805
Iteration 12/1000 | Loss: 0.00002768
Iteration 13/1000 | Loss: 0.00002734
Iteration 14/1000 | Loss: 0.00002722
Iteration 15/1000 | Loss: 0.00002720
Iteration 16/1000 | Loss: 0.00002709
Iteration 17/1000 | Loss: 0.00002702
Iteration 18/1000 | Loss: 0.00002702
Iteration 19/1000 | Loss: 0.00002701
Iteration 20/1000 | Loss: 0.00002692
Iteration 21/1000 | Loss: 0.00002686
Iteration 22/1000 | Loss: 0.00002685
Iteration 23/1000 | Loss: 0.00002671
Iteration 24/1000 | Loss: 0.00002665
Iteration 25/1000 | Loss: 0.00002665
Iteration 26/1000 | Loss: 0.00002662
Iteration 27/1000 | Loss: 0.00002662
Iteration 28/1000 | Loss: 0.00002661
Iteration 29/1000 | Loss: 0.00002659
Iteration 30/1000 | Loss: 0.00002658
Iteration 31/1000 | Loss: 0.00002657
Iteration 32/1000 | Loss: 0.00002657
Iteration 33/1000 | Loss: 0.00002656
Iteration 34/1000 | Loss: 0.00002656
Iteration 35/1000 | Loss: 0.00002655
Iteration 36/1000 | Loss: 0.00002654
Iteration 37/1000 | Loss: 0.00002649
Iteration 38/1000 | Loss: 0.00002648
Iteration 39/1000 | Loss: 0.00002648
Iteration 40/1000 | Loss: 0.00002647
Iteration 41/1000 | Loss: 0.00002647
Iteration 42/1000 | Loss: 0.00002647
Iteration 43/1000 | Loss: 0.00002644
Iteration 44/1000 | Loss: 0.00002643
Iteration 45/1000 | Loss: 0.00002642
Iteration 46/1000 | Loss: 0.00002642
Iteration 47/1000 | Loss: 0.00002641
Iteration 48/1000 | Loss: 0.00002640
Iteration 49/1000 | Loss: 0.00002640
Iteration 50/1000 | Loss: 0.00002640
Iteration 51/1000 | Loss: 0.00002640
Iteration 52/1000 | Loss: 0.00002640
Iteration 53/1000 | Loss: 0.00002640
Iteration 54/1000 | Loss: 0.00002639
Iteration 55/1000 | Loss: 0.00002639
Iteration 56/1000 | Loss: 0.00002639
Iteration 57/1000 | Loss: 0.00002639
Iteration 58/1000 | Loss: 0.00002639
Iteration 59/1000 | Loss: 0.00002639
Iteration 60/1000 | Loss: 0.00002639
Iteration 61/1000 | Loss: 0.00002639
Iteration 62/1000 | Loss: 0.00002639
Iteration 63/1000 | Loss: 0.00002639
Iteration 64/1000 | Loss: 0.00002639
Iteration 65/1000 | Loss: 0.00002639
Iteration 66/1000 | Loss: 0.00002638
Iteration 67/1000 | Loss: 0.00002638
Iteration 68/1000 | Loss: 0.00002638
Iteration 69/1000 | Loss: 0.00002637
Iteration 70/1000 | Loss: 0.00002637
Iteration 71/1000 | Loss: 0.00002636
Iteration 72/1000 | Loss: 0.00002636
Iteration 73/1000 | Loss: 0.00002635
Iteration 74/1000 | Loss: 0.00002635
Iteration 75/1000 | Loss: 0.00002635
Iteration 76/1000 | Loss: 0.00002634
Iteration 77/1000 | Loss: 0.00002634
Iteration 78/1000 | Loss: 0.00002634
Iteration 79/1000 | Loss: 0.00002633
Iteration 80/1000 | Loss: 0.00002633
Iteration 81/1000 | Loss: 0.00002633
Iteration 82/1000 | Loss: 0.00002633
Iteration 83/1000 | Loss: 0.00002632
Iteration 84/1000 | Loss: 0.00002632
Iteration 85/1000 | Loss: 0.00002632
Iteration 86/1000 | Loss: 0.00002632
Iteration 87/1000 | Loss: 0.00002631
Iteration 88/1000 | Loss: 0.00002631
Iteration 89/1000 | Loss: 0.00002630
Iteration 90/1000 | Loss: 0.00002630
Iteration 91/1000 | Loss: 0.00002630
Iteration 92/1000 | Loss: 0.00002630
Iteration 93/1000 | Loss: 0.00002629
Iteration 94/1000 | Loss: 0.00002629
Iteration 95/1000 | Loss: 0.00002629
Iteration 96/1000 | Loss: 0.00002629
Iteration 97/1000 | Loss: 0.00002628
Iteration 98/1000 | Loss: 0.00002628
Iteration 99/1000 | Loss: 0.00002627
Iteration 100/1000 | Loss: 0.00002627
Iteration 101/1000 | Loss: 0.00002627
Iteration 102/1000 | Loss: 0.00002626
Iteration 103/1000 | Loss: 0.00002626
Iteration 104/1000 | Loss: 0.00002626
Iteration 105/1000 | Loss: 0.00002626
Iteration 106/1000 | Loss: 0.00002625
Iteration 107/1000 | Loss: 0.00002625
Iteration 108/1000 | Loss: 0.00002625
Iteration 109/1000 | Loss: 0.00002625
Iteration 110/1000 | Loss: 0.00002625
Iteration 111/1000 | Loss: 0.00002625
Iteration 112/1000 | Loss: 0.00002625
Iteration 113/1000 | Loss: 0.00002625
Iteration 114/1000 | Loss: 0.00002624
Iteration 115/1000 | Loss: 0.00002624
Iteration 116/1000 | Loss: 0.00002624
Iteration 117/1000 | Loss: 0.00002624
Iteration 118/1000 | Loss: 0.00002624
Iteration 119/1000 | Loss: 0.00002624
Iteration 120/1000 | Loss: 0.00002624
Iteration 121/1000 | Loss: 0.00002624
Iteration 122/1000 | Loss: 0.00002623
Iteration 123/1000 | Loss: 0.00002623
Iteration 124/1000 | Loss: 0.00002623
Iteration 125/1000 | Loss: 0.00002622
Iteration 126/1000 | Loss: 0.00002622
Iteration 127/1000 | Loss: 0.00002622
Iteration 128/1000 | Loss: 0.00002622
Iteration 129/1000 | Loss: 0.00002621
Iteration 130/1000 | Loss: 0.00002621
Iteration 131/1000 | Loss: 0.00002621
Iteration 132/1000 | Loss: 0.00002621
Iteration 133/1000 | Loss: 0.00002621
Iteration 134/1000 | Loss: 0.00002620
Iteration 135/1000 | Loss: 0.00002620
Iteration 136/1000 | Loss: 0.00002620
Iteration 137/1000 | Loss: 0.00002620
Iteration 138/1000 | Loss: 0.00002620
Iteration 139/1000 | Loss: 0.00002620
Iteration 140/1000 | Loss: 0.00002620
Iteration 141/1000 | Loss: 0.00002619
Iteration 142/1000 | Loss: 0.00002619
Iteration 143/1000 | Loss: 0.00002619
Iteration 144/1000 | Loss: 0.00002619
Iteration 145/1000 | Loss: 0.00002619
Iteration 146/1000 | Loss: 0.00002618
Iteration 147/1000 | Loss: 0.00002618
Iteration 148/1000 | Loss: 0.00002618
Iteration 149/1000 | Loss: 0.00002618
Iteration 150/1000 | Loss: 0.00002617
Iteration 151/1000 | Loss: 0.00002617
Iteration 152/1000 | Loss: 0.00002617
Iteration 153/1000 | Loss: 0.00002617
Iteration 154/1000 | Loss: 0.00002617
Iteration 155/1000 | Loss: 0.00002617
Iteration 156/1000 | Loss: 0.00002617
Iteration 157/1000 | Loss: 0.00002616
Iteration 158/1000 | Loss: 0.00002616
Iteration 159/1000 | Loss: 0.00002616
Iteration 160/1000 | Loss: 0.00002616
Iteration 161/1000 | Loss: 0.00002616
Iteration 162/1000 | Loss: 0.00002616
Iteration 163/1000 | Loss: 0.00002616
Iteration 164/1000 | Loss: 0.00002616
Iteration 165/1000 | Loss: 0.00002616
Iteration 166/1000 | Loss: 0.00002615
Iteration 167/1000 | Loss: 0.00002615
Iteration 168/1000 | Loss: 0.00002615
Iteration 169/1000 | Loss: 0.00002615
Iteration 170/1000 | Loss: 0.00002615
Iteration 171/1000 | Loss: 0.00002615
Iteration 172/1000 | Loss: 0.00002615
Iteration 173/1000 | Loss: 0.00002615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.6151070414925925e-05, 2.6151070414925925e-05, 2.6151070414925925e-05, 2.6151070414925925e-05, 2.6151070414925925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6151070414925925e-05

Optimization complete. Final v2v error: 4.282830238342285 mm

Highest mean error: 6.133380889892578 mm for frame 155

Lowest mean error: 3.3655202388763428 mm for frame 249

Saving results

Total time: 101.01334571838379
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851416
Iteration 2/25 | Loss: 0.00113805
Iteration 3/25 | Loss: 0.00077994
Iteration 4/25 | Loss: 0.00073197
Iteration 5/25 | Loss: 0.00072505
Iteration 6/25 | Loss: 0.00072456
Iteration 7/25 | Loss: 0.00072456
Iteration 8/25 | Loss: 0.00072456
Iteration 9/25 | Loss: 0.00072456
Iteration 10/25 | Loss: 0.00072456
Iteration 11/25 | Loss: 0.00072456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007245604647323489, 0.0007245604647323489, 0.0007245604647323489, 0.0007245604647323489, 0.0007245604647323489]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007245604647323489

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91026938
Iteration 2/25 | Loss: 0.00041822
Iteration 3/25 | Loss: 0.00041821
Iteration 4/25 | Loss: 0.00041821
Iteration 5/25 | Loss: 0.00041821
Iteration 6/25 | Loss: 0.00041821
Iteration 7/25 | Loss: 0.00041821
Iteration 8/25 | Loss: 0.00041821
Iteration 9/25 | Loss: 0.00041821
Iteration 10/25 | Loss: 0.00041821
Iteration 11/25 | Loss: 0.00041821
Iteration 12/25 | Loss: 0.00041821
Iteration 13/25 | Loss: 0.00041821
Iteration 14/25 | Loss: 0.00041821
Iteration 15/25 | Loss: 0.00041821
Iteration 16/25 | Loss: 0.00041821
Iteration 17/25 | Loss: 0.00041821
Iteration 18/25 | Loss: 0.00041821
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004182090051472187, 0.0004182090051472187, 0.0004182090051472187, 0.0004182090051472187, 0.0004182090051472187]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004182090051472187

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041821
Iteration 2/1000 | Loss: 0.00004926
Iteration 3/1000 | Loss: 0.00003760
Iteration 4/1000 | Loss: 0.00003399
Iteration 5/1000 | Loss: 0.00003190
Iteration 6/1000 | Loss: 0.00003047
Iteration 7/1000 | Loss: 0.00002947
Iteration 8/1000 | Loss: 0.00002886
Iteration 9/1000 | Loss: 0.00002854
Iteration 10/1000 | Loss: 0.00002824
Iteration 11/1000 | Loss: 0.00002818
Iteration 12/1000 | Loss: 0.00002810
Iteration 13/1000 | Loss: 0.00002805
Iteration 14/1000 | Loss: 0.00002793
Iteration 15/1000 | Loss: 0.00002790
Iteration 16/1000 | Loss: 0.00002790
Iteration 17/1000 | Loss: 0.00002790
Iteration 18/1000 | Loss: 0.00002790
Iteration 19/1000 | Loss: 0.00002790
Iteration 20/1000 | Loss: 0.00002790
Iteration 21/1000 | Loss: 0.00002789
Iteration 22/1000 | Loss: 0.00002787
Iteration 23/1000 | Loss: 0.00002786
Iteration 24/1000 | Loss: 0.00002786
Iteration 25/1000 | Loss: 0.00002785
Iteration 26/1000 | Loss: 0.00002785
Iteration 27/1000 | Loss: 0.00002785
Iteration 28/1000 | Loss: 0.00002785
Iteration 29/1000 | Loss: 0.00002785
Iteration 30/1000 | Loss: 0.00002784
Iteration 31/1000 | Loss: 0.00002784
Iteration 32/1000 | Loss: 0.00002784
Iteration 33/1000 | Loss: 0.00002784
Iteration 34/1000 | Loss: 0.00002782
Iteration 35/1000 | Loss: 0.00002779
Iteration 36/1000 | Loss: 0.00002778
Iteration 37/1000 | Loss: 0.00002778
Iteration 38/1000 | Loss: 0.00002775
Iteration 39/1000 | Loss: 0.00002775
Iteration 40/1000 | Loss: 0.00002774
Iteration 41/1000 | Loss: 0.00002771
Iteration 42/1000 | Loss: 0.00002771
Iteration 43/1000 | Loss: 0.00002771
Iteration 44/1000 | Loss: 0.00002771
Iteration 45/1000 | Loss: 0.00002770
Iteration 46/1000 | Loss: 0.00002770
Iteration 47/1000 | Loss: 0.00002770
Iteration 48/1000 | Loss: 0.00002770
Iteration 49/1000 | Loss: 0.00002770
Iteration 50/1000 | Loss: 0.00002770
Iteration 51/1000 | Loss: 0.00002770
Iteration 52/1000 | Loss: 0.00002770
Iteration 53/1000 | Loss: 0.00002770
Iteration 54/1000 | Loss: 0.00002770
Iteration 55/1000 | Loss: 0.00002770
Iteration 56/1000 | Loss: 0.00002770
Iteration 57/1000 | Loss: 0.00002770
Iteration 58/1000 | Loss: 0.00002769
Iteration 59/1000 | Loss: 0.00002769
Iteration 60/1000 | Loss: 0.00002769
Iteration 61/1000 | Loss: 0.00002768
Iteration 62/1000 | Loss: 0.00002768
Iteration 63/1000 | Loss: 0.00002768
Iteration 64/1000 | Loss: 0.00002768
Iteration 65/1000 | Loss: 0.00002767
Iteration 66/1000 | Loss: 0.00002767
Iteration 67/1000 | Loss: 0.00002767
Iteration 68/1000 | Loss: 0.00002766
Iteration 69/1000 | Loss: 0.00002766
Iteration 70/1000 | Loss: 0.00002766
Iteration 71/1000 | Loss: 0.00002766
Iteration 72/1000 | Loss: 0.00002766
Iteration 73/1000 | Loss: 0.00002766
Iteration 74/1000 | Loss: 0.00002765
Iteration 75/1000 | Loss: 0.00002764
Iteration 76/1000 | Loss: 0.00002764
Iteration 77/1000 | Loss: 0.00002763
Iteration 78/1000 | Loss: 0.00002763
Iteration 79/1000 | Loss: 0.00002763
Iteration 80/1000 | Loss: 0.00002762
Iteration 81/1000 | Loss: 0.00002762
Iteration 82/1000 | Loss: 0.00002762
Iteration 83/1000 | Loss: 0.00002762
Iteration 84/1000 | Loss: 0.00002762
Iteration 85/1000 | Loss: 0.00002762
Iteration 86/1000 | Loss: 0.00002761
Iteration 87/1000 | Loss: 0.00002761
Iteration 88/1000 | Loss: 0.00002761
Iteration 89/1000 | Loss: 0.00002761
Iteration 90/1000 | Loss: 0.00002761
Iteration 91/1000 | Loss: 0.00002761
Iteration 92/1000 | Loss: 0.00002761
Iteration 93/1000 | Loss: 0.00002760
Iteration 94/1000 | Loss: 0.00002760
Iteration 95/1000 | Loss: 0.00002760
Iteration 96/1000 | Loss: 0.00002760
Iteration 97/1000 | Loss: 0.00002760
Iteration 98/1000 | Loss: 0.00002760
Iteration 99/1000 | Loss: 0.00002760
Iteration 100/1000 | Loss: 0.00002759
Iteration 101/1000 | Loss: 0.00002759
Iteration 102/1000 | Loss: 0.00002759
Iteration 103/1000 | Loss: 0.00002759
Iteration 104/1000 | Loss: 0.00002759
Iteration 105/1000 | Loss: 0.00002759
Iteration 106/1000 | Loss: 0.00002759
Iteration 107/1000 | Loss: 0.00002759
Iteration 108/1000 | Loss: 0.00002759
Iteration 109/1000 | Loss: 0.00002759
Iteration 110/1000 | Loss: 0.00002759
Iteration 111/1000 | Loss: 0.00002759
Iteration 112/1000 | Loss: 0.00002759
Iteration 113/1000 | Loss: 0.00002758
Iteration 114/1000 | Loss: 0.00002758
Iteration 115/1000 | Loss: 0.00002758
Iteration 116/1000 | Loss: 0.00002758
Iteration 117/1000 | Loss: 0.00002758
Iteration 118/1000 | Loss: 0.00002758
Iteration 119/1000 | Loss: 0.00002758
Iteration 120/1000 | Loss: 0.00002758
Iteration 121/1000 | Loss: 0.00002758
Iteration 122/1000 | Loss: 0.00002758
Iteration 123/1000 | Loss: 0.00002758
Iteration 124/1000 | Loss: 0.00002758
Iteration 125/1000 | Loss: 0.00002758
Iteration 126/1000 | Loss: 0.00002758
Iteration 127/1000 | Loss: 0.00002758
Iteration 128/1000 | Loss: 0.00002758
Iteration 129/1000 | Loss: 0.00002758
Iteration 130/1000 | Loss: 0.00002757
Iteration 131/1000 | Loss: 0.00002757
Iteration 132/1000 | Loss: 0.00002757
Iteration 133/1000 | Loss: 0.00002757
Iteration 134/1000 | Loss: 0.00002757
Iteration 135/1000 | Loss: 0.00002757
Iteration 136/1000 | Loss: 0.00002757
Iteration 137/1000 | Loss: 0.00002757
Iteration 138/1000 | Loss: 0.00002757
Iteration 139/1000 | Loss: 0.00002757
Iteration 140/1000 | Loss: 0.00002757
Iteration 141/1000 | Loss: 0.00002757
Iteration 142/1000 | Loss: 0.00002757
Iteration 143/1000 | Loss: 0.00002757
Iteration 144/1000 | Loss: 0.00002757
Iteration 145/1000 | Loss: 0.00002757
Iteration 146/1000 | Loss: 0.00002757
Iteration 147/1000 | Loss: 0.00002757
Iteration 148/1000 | Loss: 0.00002757
Iteration 149/1000 | Loss: 0.00002757
Iteration 150/1000 | Loss: 0.00002757
Iteration 151/1000 | Loss: 0.00002757
Iteration 152/1000 | Loss: 0.00002757
Iteration 153/1000 | Loss: 0.00002757
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.7574467821978033e-05, 2.7574467821978033e-05, 2.7574467821978033e-05, 2.7574467821978033e-05, 2.7574467821978033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7574467821978033e-05

Optimization complete. Final v2v error: 4.482585430145264 mm

Highest mean error: 4.97131872177124 mm for frame 9

Lowest mean error: 4.0756964683532715 mm for frame 201

Saving results

Total time: 40.679919719696045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403701
Iteration 2/25 | Loss: 0.00102330
Iteration 3/25 | Loss: 0.00071935
Iteration 4/25 | Loss: 0.00064011
Iteration 5/25 | Loss: 0.00062205
Iteration 6/25 | Loss: 0.00061687
Iteration 7/25 | Loss: 0.00061497
Iteration 8/25 | Loss: 0.00061439
Iteration 9/25 | Loss: 0.00061439
Iteration 10/25 | Loss: 0.00061439
Iteration 11/25 | Loss: 0.00061439
Iteration 12/25 | Loss: 0.00061439
Iteration 13/25 | Loss: 0.00061439
Iteration 14/25 | Loss: 0.00061439
Iteration 15/25 | Loss: 0.00061439
Iteration 16/25 | Loss: 0.00061439
Iteration 17/25 | Loss: 0.00061439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000614388263784349, 0.000614388263784349, 0.000614388263784349, 0.000614388263784349, 0.000614388263784349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000614388263784349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28873038
Iteration 2/25 | Loss: 0.00029655
Iteration 3/25 | Loss: 0.00029655
Iteration 4/25 | Loss: 0.00029655
Iteration 5/25 | Loss: 0.00029655
Iteration 6/25 | Loss: 0.00029655
Iteration 7/25 | Loss: 0.00029655
Iteration 8/25 | Loss: 0.00029655
Iteration 9/25 | Loss: 0.00029655
Iteration 10/25 | Loss: 0.00029654
Iteration 11/25 | Loss: 0.00029654
Iteration 12/25 | Loss: 0.00029654
Iteration 13/25 | Loss: 0.00029654
Iteration 14/25 | Loss: 0.00029654
Iteration 15/25 | Loss: 0.00029654
Iteration 16/25 | Loss: 0.00029654
Iteration 17/25 | Loss: 0.00029654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0002965443127322942, 0.0002965443127322942, 0.0002965443127322942, 0.0002965443127322942, 0.0002965443127322942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002965443127322942

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00029654
Iteration 2/1000 | Loss: 0.00002775
Iteration 3/1000 | Loss: 0.00002278
Iteration 4/1000 | Loss: 0.00001936
Iteration 5/1000 | Loss: 0.00001794
Iteration 6/1000 | Loss: 0.00001713
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001611
Iteration 9/1000 | Loss: 0.00001581
Iteration 10/1000 | Loss: 0.00001557
Iteration 11/1000 | Loss: 0.00001538
Iteration 12/1000 | Loss: 0.00001526
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001506
Iteration 15/1000 | Loss: 0.00001505
Iteration 16/1000 | Loss: 0.00001501
Iteration 17/1000 | Loss: 0.00001501
Iteration 18/1000 | Loss: 0.00001500
Iteration 19/1000 | Loss: 0.00001500
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001497
Iteration 22/1000 | Loss: 0.00001496
Iteration 23/1000 | Loss: 0.00001495
Iteration 24/1000 | Loss: 0.00001495
Iteration 25/1000 | Loss: 0.00001495
Iteration 26/1000 | Loss: 0.00001494
Iteration 27/1000 | Loss: 0.00001494
Iteration 28/1000 | Loss: 0.00001493
Iteration 29/1000 | Loss: 0.00001493
Iteration 30/1000 | Loss: 0.00001491
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001490
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001490
Iteration 35/1000 | Loss: 0.00001489
Iteration 36/1000 | Loss: 0.00001489
Iteration 37/1000 | Loss: 0.00001489
Iteration 38/1000 | Loss: 0.00001489
Iteration 39/1000 | Loss: 0.00001489
Iteration 40/1000 | Loss: 0.00001489
Iteration 41/1000 | Loss: 0.00001488
Iteration 42/1000 | Loss: 0.00001488
Iteration 43/1000 | Loss: 0.00001487
Iteration 44/1000 | Loss: 0.00001486
Iteration 45/1000 | Loss: 0.00001486
Iteration 46/1000 | Loss: 0.00001485
Iteration 47/1000 | Loss: 0.00001485
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001484
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001481
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001480
Iteration 57/1000 | Loss: 0.00001480
Iteration 58/1000 | Loss: 0.00001480
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001479
Iteration 61/1000 | Loss: 0.00001472
Iteration 62/1000 | Loss: 0.00001472
Iteration 63/1000 | Loss: 0.00001471
Iteration 64/1000 | Loss: 0.00001471
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001470
Iteration 67/1000 | Loss: 0.00001470
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001469
Iteration 70/1000 | Loss: 0.00001469
Iteration 71/1000 | Loss: 0.00001468
Iteration 72/1000 | Loss: 0.00001468
Iteration 73/1000 | Loss: 0.00001467
Iteration 74/1000 | Loss: 0.00001467
Iteration 75/1000 | Loss: 0.00001467
Iteration 76/1000 | Loss: 0.00001466
Iteration 77/1000 | Loss: 0.00001466
Iteration 78/1000 | Loss: 0.00001466
Iteration 79/1000 | Loss: 0.00001465
Iteration 80/1000 | Loss: 0.00001465
Iteration 81/1000 | Loss: 0.00001464
Iteration 82/1000 | Loss: 0.00001464
Iteration 83/1000 | Loss: 0.00001464
Iteration 84/1000 | Loss: 0.00001464
Iteration 85/1000 | Loss: 0.00001463
Iteration 86/1000 | Loss: 0.00001463
Iteration 87/1000 | Loss: 0.00001463
Iteration 88/1000 | Loss: 0.00001463
Iteration 89/1000 | Loss: 0.00001463
Iteration 90/1000 | Loss: 0.00001463
Iteration 91/1000 | Loss: 0.00001463
Iteration 92/1000 | Loss: 0.00001463
Iteration 93/1000 | Loss: 0.00001463
Iteration 94/1000 | Loss: 0.00001463
Iteration 95/1000 | Loss: 0.00001462
Iteration 96/1000 | Loss: 0.00001462
Iteration 97/1000 | Loss: 0.00001462
Iteration 98/1000 | Loss: 0.00001462
Iteration 99/1000 | Loss: 0.00001462
Iteration 100/1000 | Loss: 0.00001462
Iteration 101/1000 | Loss: 0.00001461
Iteration 102/1000 | Loss: 0.00001461
Iteration 103/1000 | Loss: 0.00001461
Iteration 104/1000 | Loss: 0.00001461
Iteration 105/1000 | Loss: 0.00001461
Iteration 106/1000 | Loss: 0.00001461
Iteration 107/1000 | Loss: 0.00001461
Iteration 108/1000 | Loss: 0.00001461
Iteration 109/1000 | Loss: 0.00001461
Iteration 110/1000 | Loss: 0.00001461
Iteration 111/1000 | Loss: 0.00001461
Iteration 112/1000 | Loss: 0.00001461
Iteration 113/1000 | Loss: 0.00001461
Iteration 114/1000 | Loss: 0.00001461
Iteration 115/1000 | Loss: 0.00001460
Iteration 116/1000 | Loss: 0.00001460
Iteration 117/1000 | Loss: 0.00001460
Iteration 118/1000 | Loss: 0.00001460
Iteration 119/1000 | Loss: 0.00001460
Iteration 120/1000 | Loss: 0.00001460
Iteration 121/1000 | Loss: 0.00001460
Iteration 122/1000 | Loss: 0.00001460
Iteration 123/1000 | Loss: 0.00001460
Iteration 124/1000 | Loss: 0.00001460
Iteration 125/1000 | Loss: 0.00001460
Iteration 126/1000 | Loss: 0.00001460
Iteration 127/1000 | Loss: 0.00001460
Iteration 128/1000 | Loss: 0.00001460
Iteration 129/1000 | Loss: 0.00001460
Iteration 130/1000 | Loss: 0.00001460
Iteration 131/1000 | Loss: 0.00001460
Iteration 132/1000 | Loss: 0.00001460
Iteration 133/1000 | Loss: 0.00001459
Iteration 134/1000 | Loss: 0.00001459
Iteration 135/1000 | Loss: 0.00001459
Iteration 136/1000 | Loss: 0.00001459
Iteration 137/1000 | Loss: 0.00001459
Iteration 138/1000 | Loss: 0.00001459
Iteration 139/1000 | Loss: 0.00001459
Iteration 140/1000 | Loss: 0.00001459
Iteration 141/1000 | Loss: 0.00001459
Iteration 142/1000 | Loss: 0.00001459
Iteration 143/1000 | Loss: 0.00001459
Iteration 144/1000 | Loss: 0.00001459
Iteration 145/1000 | Loss: 0.00001459
Iteration 146/1000 | Loss: 0.00001459
Iteration 147/1000 | Loss: 0.00001459
Iteration 148/1000 | Loss: 0.00001458
Iteration 149/1000 | Loss: 0.00001458
Iteration 150/1000 | Loss: 0.00001458
Iteration 151/1000 | Loss: 0.00001458
Iteration 152/1000 | Loss: 0.00001458
Iteration 153/1000 | Loss: 0.00001458
Iteration 154/1000 | Loss: 0.00001458
Iteration 155/1000 | Loss: 0.00001458
Iteration 156/1000 | Loss: 0.00001458
Iteration 157/1000 | Loss: 0.00001457
Iteration 158/1000 | Loss: 0.00001457
Iteration 159/1000 | Loss: 0.00001457
Iteration 160/1000 | Loss: 0.00001457
Iteration 161/1000 | Loss: 0.00001457
Iteration 162/1000 | Loss: 0.00001456
Iteration 163/1000 | Loss: 0.00001456
Iteration 164/1000 | Loss: 0.00001456
Iteration 165/1000 | Loss: 0.00001456
Iteration 166/1000 | Loss: 0.00001456
Iteration 167/1000 | Loss: 0.00001456
Iteration 168/1000 | Loss: 0.00001456
Iteration 169/1000 | Loss: 0.00001455
Iteration 170/1000 | Loss: 0.00001455
Iteration 171/1000 | Loss: 0.00001455
Iteration 172/1000 | Loss: 0.00001455
Iteration 173/1000 | Loss: 0.00001455
Iteration 174/1000 | Loss: 0.00001454
Iteration 175/1000 | Loss: 0.00001454
Iteration 176/1000 | Loss: 0.00001454
Iteration 177/1000 | Loss: 0.00001454
Iteration 178/1000 | Loss: 0.00001454
Iteration 179/1000 | Loss: 0.00001453
Iteration 180/1000 | Loss: 0.00001453
Iteration 181/1000 | Loss: 0.00001453
Iteration 182/1000 | Loss: 0.00001453
Iteration 183/1000 | Loss: 0.00001453
Iteration 184/1000 | Loss: 0.00001453
Iteration 185/1000 | Loss: 0.00001453
Iteration 186/1000 | Loss: 0.00001453
Iteration 187/1000 | Loss: 0.00001453
Iteration 188/1000 | Loss: 0.00001453
Iteration 189/1000 | Loss: 0.00001453
Iteration 190/1000 | Loss: 0.00001453
Iteration 191/1000 | Loss: 0.00001453
Iteration 192/1000 | Loss: 0.00001453
Iteration 193/1000 | Loss: 0.00001453
Iteration 194/1000 | Loss: 0.00001453
Iteration 195/1000 | Loss: 0.00001453
Iteration 196/1000 | Loss: 0.00001453
Iteration 197/1000 | Loss: 0.00001453
Iteration 198/1000 | Loss: 0.00001453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [1.4530709449900314e-05, 1.4530709449900314e-05, 1.4530709449900314e-05, 1.4530709449900314e-05, 1.4530709449900314e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4530709449900314e-05

Optimization complete. Final v2v error: 3.2905046939849854 mm

Highest mean error: 4.092045783996582 mm for frame 78

Lowest mean error: 2.9505562782287598 mm for frame 163

Saving results

Total time: 47.2588791847229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_nl_5739/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_nl_5739/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404063
Iteration 2/25 | Loss: 0.00090498
Iteration 3/25 | Loss: 0.00076045
Iteration 4/25 | Loss: 0.00072862
Iteration 5/25 | Loss: 0.00071821
Iteration 6/25 | Loss: 0.00071657
Iteration 7/25 | Loss: 0.00071644
Iteration 8/25 | Loss: 0.00071644
Iteration 9/25 | Loss: 0.00071644
Iteration 10/25 | Loss: 0.00071644
Iteration 11/25 | Loss: 0.00071644
Iteration 12/25 | Loss: 0.00071644
Iteration 13/25 | Loss: 0.00071644
Iteration 14/25 | Loss: 0.00071644
Iteration 15/25 | Loss: 0.00071644
Iteration 16/25 | Loss: 0.00071644
Iteration 17/25 | Loss: 0.00071644
Iteration 18/25 | Loss: 0.00071644
Iteration 19/25 | Loss: 0.00071644
Iteration 20/25 | Loss: 0.00071644
Iteration 21/25 | Loss: 0.00071644
Iteration 22/25 | Loss: 0.00071644
Iteration 23/25 | Loss: 0.00071644
Iteration 24/25 | Loss: 0.00071644
Iteration 25/25 | Loss: 0.00071644
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007164368871599436, 0.0007164368871599436, 0.0007164368871599436, 0.0007164368871599436, 0.0007164368871599436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007164368871599436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36247611
Iteration 2/25 | Loss: 0.00033846
Iteration 3/25 | Loss: 0.00033842
Iteration 4/25 | Loss: 0.00033842
Iteration 5/25 | Loss: 0.00033842
Iteration 6/25 | Loss: 0.00033842
Iteration 7/25 | Loss: 0.00033842
Iteration 8/25 | Loss: 0.00033842
Iteration 9/25 | Loss: 0.00033842
Iteration 10/25 | Loss: 0.00033842
Iteration 11/25 | Loss: 0.00033842
Iteration 12/25 | Loss: 0.00033842
Iteration 13/25 | Loss: 0.00033842
Iteration 14/25 | Loss: 0.00033842
Iteration 15/25 | Loss: 0.00033842
Iteration 16/25 | Loss: 0.00033842
Iteration 17/25 | Loss: 0.00033842
Iteration 18/25 | Loss: 0.00033842
Iteration 19/25 | Loss: 0.00033842
Iteration 20/25 | Loss: 0.00033842
Iteration 21/25 | Loss: 0.00033842
Iteration 22/25 | Loss: 0.00033842
Iteration 23/25 | Loss: 0.00033842
Iteration 24/25 | Loss: 0.00033842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0003384202718734741, 0.0003384202718734741, 0.0003384202718734741, 0.0003384202718734741, 0.0003384202718734741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003384202718734741

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00033842
Iteration 2/1000 | Loss: 0.00004860
Iteration 3/1000 | Loss: 0.00003707
Iteration 4/1000 | Loss: 0.00003426
Iteration 5/1000 | Loss: 0.00003290
Iteration 6/1000 | Loss: 0.00003207
Iteration 7/1000 | Loss: 0.00003143
Iteration 8/1000 | Loss: 0.00003092
Iteration 9/1000 | Loss: 0.00003048
Iteration 10/1000 | Loss: 0.00003023
Iteration 11/1000 | Loss: 0.00003003
Iteration 12/1000 | Loss: 0.00002997
Iteration 13/1000 | Loss: 0.00002992
Iteration 14/1000 | Loss: 0.00002987
Iteration 15/1000 | Loss: 0.00002985
Iteration 16/1000 | Loss: 0.00002984
Iteration 17/1000 | Loss: 0.00002984
Iteration 18/1000 | Loss: 0.00002983
Iteration 19/1000 | Loss: 0.00002983
Iteration 20/1000 | Loss: 0.00002983
Iteration 21/1000 | Loss: 0.00002982
Iteration 22/1000 | Loss: 0.00002982
Iteration 23/1000 | Loss: 0.00002981
Iteration 24/1000 | Loss: 0.00002981
Iteration 25/1000 | Loss: 0.00002980
Iteration 26/1000 | Loss: 0.00002980
Iteration 27/1000 | Loss: 0.00002979
Iteration 28/1000 | Loss: 0.00002979
Iteration 29/1000 | Loss: 0.00002979
Iteration 30/1000 | Loss: 0.00002978
Iteration 31/1000 | Loss: 0.00002978
Iteration 32/1000 | Loss: 0.00002975
Iteration 33/1000 | Loss: 0.00002974
Iteration 34/1000 | Loss: 0.00002974
Iteration 35/1000 | Loss: 0.00002974
Iteration 36/1000 | Loss: 0.00002974
Iteration 37/1000 | Loss: 0.00002974
Iteration 38/1000 | Loss: 0.00002974
Iteration 39/1000 | Loss: 0.00002974
Iteration 40/1000 | Loss: 0.00002973
Iteration 41/1000 | Loss: 0.00002972
Iteration 42/1000 | Loss: 0.00002972
Iteration 43/1000 | Loss: 0.00002972
Iteration 44/1000 | Loss: 0.00002971
Iteration 45/1000 | Loss: 0.00002971
Iteration 46/1000 | Loss: 0.00002970
Iteration 47/1000 | Loss: 0.00002970
Iteration 48/1000 | Loss: 0.00002970
Iteration 49/1000 | Loss: 0.00002969
Iteration 50/1000 | Loss: 0.00002969
Iteration 51/1000 | Loss: 0.00002969
Iteration 52/1000 | Loss: 0.00002969
Iteration 53/1000 | Loss: 0.00002969
Iteration 54/1000 | Loss: 0.00002969
Iteration 55/1000 | Loss: 0.00002969
Iteration 56/1000 | Loss: 0.00002969
Iteration 57/1000 | Loss: 0.00002969
Iteration 58/1000 | Loss: 0.00002969
Iteration 59/1000 | Loss: 0.00002969
Iteration 60/1000 | Loss: 0.00002969
Iteration 61/1000 | Loss: 0.00002969
Iteration 62/1000 | Loss: 0.00002969
Iteration 63/1000 | Loss: 0.00002969
Iteration 64/1000 | Loss: 0.00002969
Iteration 65/1000 | Loss: 0.00002969
Iteration 66/1000 | Loss: 0.00002969
Iteration 67/1000 | Loss: 0.00002969
Iteration 68/1000 | Loss: 0.00002969
Iteration 69/1000 | Loss: 0.00002969
Iteration 70/1000 | Loss: 0.00002969
Iteration 71/1000 | Loss: 0.00002969
Iteration 72/1000 | Loss: 0.00002969
Iteration 73/1000 | Loss: 0.00002969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.9686976631637663e-05, 2.9686976631637663e-05, 2.9686976631637663e-05, 2.9686976631637663e-05, 2.9686976631637663e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9686976631637663e-05

Optimization complete. Final v2v error: 4.652078628540039 mm

Highest mean error: 4.878530502319336 mm for frame 106

Lowest mean error: 4.239980697631836 mm for frame 181

Saving results

Total time: 32.762211084365845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598864
Iteration 2/25 | Loss: 0.00151361
Iteration 3/25 | Loss: 0.00137736
Iteration 4/25 | Loss: 0.00135723
Iteration 5/25 | Loss: 0.00135362
Iteration 6/25 | Loss: 0.00135301
Iteration 7/25 | Loss: 0.00135286
Iteration 8/25 | Loss: 0.00135286
Iteration 9/25 | Loss: 0.00135286
Iteration 10/25 | Loss: 0.00135286
Iteration 11/25 | Loss: 0.00135286
Iteration 12/25 | Loss: 0.00135286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001352861407212913, 0.001352861407212913, 0.001352861407212913, 0.001352861407212913, 0.001352861407212913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001352861407212913

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42128909
Iteration 2/25 | Loss: 0.00088371
Iteration 3/25 | Loss: 0.00088369
Iteration 4/25 | Loss: 0.00088369
Iteration 5/25 | Loss: 0.00088369
Iteration 6/25 | Loss: 0.00088369
Iteration 7/25 | Loss: 0.00088369
Iteration 8/25 | Loss: 0.00088369
Iteration 9/25 | Loss: 0.00088369
Iteration 10/25 | Loss: 0.00088369
Iteration 11/25 | Loss: 0.00088369
Iteration 12/25 | Loss: 0.00088369
Iteration 13/25 | Loss: 0.00088369
Iteration 14/25 | Loss: 0.00088369
Iteration 15/25 | Loss: 0.00088369
Iteration 16/25 | Loss: 0.00088369
Iteration 17/25 | Loss: 0.00088369
Iteration 18/25 | Loss: 0.00088369
Iteration 19/25 | Loss: 0.00088369
Iteration 20/25 | Loss: 0.00088369
Iteration 21/25 | Loss: 0.00088369
Iteration 22/25 | Loss: 0.00088369
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008836902561597526, 0.0008836902561597526, 0.0008836902561597526, 0.0008836902561597526, 0.0008836902561597526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008836902561597526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088369
Iteration 2/1000 | Loss: 0.00004130
Iteration 3/1000 | Loss: 0.00002882
Iteration 4/1000 | Loss: 0.00002653
Iteration 5/1000 | Loss: 0.00002552
Iteration 6/1000 | Loss: 0.00002457
Iteration 7/1000 | Loss: 0.00002404
Iteration 8/1000 | Loss: 0.00002371
Iteration 9/1000 | Loss: 0.00002335
Iteration 10/1000 | Loss: 0.00002311
Iteration 11/1000 | Loss: 0.00002293
Iteration 12/1000 | Loss: 0.00002287
Iteration 13/1000 | Loss: 0.00002282
Iteration 14/1000 | Loss: 0.00002280
Iteration 15/1000 | Loss: 0.00002279
Iteration 16/1000 | Loss: 0.00002277
Iteration 17/1000 | Loss: 0.00002273
Iteration 18/1000 | Loss: 0.00002272
Iteration 19/1000 | Loss: 0.00002272
Iteration 20/1000 | Loss: 0.00002271
Iteration 21/1000 | Loss: 0.00002270
Iteration 22/1000 | Loss: 0.00002270
Iteration 23/1000 | Loss: 0.00002269
Iteration 24/1000 | Loss: 0.00002269
Iteration 25/1000 | Loss: 0.00002269
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00002268
Iteration 28/1000 | Loss: 0.00002267
Iteration 29/1000 | Loss: 0.00002267
Iteration 30/1000 | Loss: 0.00002265
Iteration 31/1000 | Loss: 0.00002260
Iteration 32/1000 | Loss: 0.00002259
Iteration 33/1000 | Loss: 0.00002258
Iteration 34/1000 | Loss: 0.00002258
Iteration 35/1000 | Loss: 0.00002256
Iteration 36/1000 | Loss: 0.00002254
Iteration 37/1000 | Loss: 0.00002254
Iteration 38/1000 | Loss: 0.00002253
Iteration 39/1000 | Loss: 0.00002253
Iteration 40/1000 | Loss: 0.00002253
Iteration 41/1000 | Loss: 0.00002253
Iteration 42/1000 | Loss: 0.00002253
Iteration 43/1000 | Loss: 0.00002253
Iteration 44/1000 | Loss: 0.00002253
Iteration 45/1000 | Loss: 0.00002253
Iteration 46/1000 | Loss: 0.00002253
Iteration 47/1000 | Loss: 0.00002253
Iteration 48/1000 | Loss: 0.00002252
Iteration 49/1000 | Loss: 0.00002252
Iteration 50/1000 | Loss: 0.00002252
Iteration 51/1000 | Loss: 0.00002252
Iteration 52/1000 | Loss: 0.00002251
Iteration 53/1000 | Loss: 0.00002251
Iteration 54/1000 | Loss: 0.00002249
Iteration 55/1000 | Loss: 0.00002249
Iteration 56/1000 | Loss: 0.00002248
Iteration 57/1000 | Loss: 0.00002248
Iteration 58/1000 | Loss: 0.00002247
Iteration 59/1000 | Loss: 0.00002247
Iteration 60/1000 | Loss: 0.00002247
Iteration 61/1000 | Loss: 0.00002247
Iteration 62/1000 | Loss: 0.00002247
Iteration 63/1000 | Loss: 0.00002247
Iteration 64/1000 | Loss: 0.00002247
Iteration 65/1000 | Loss: 0.00002247
Iteration 66/1000 | Loss: 0.00002247
Iteration 67/1000 | Loss: 0.00002247
Iteration 68/1000 | Loss: 0.00002247
Iteration 69/1000 | Loss: 0.00002247
Iteration 70/1000 | Loss: 0.00002247
Iteration 71/1000 | Loss: 0.00002246
Iteration 72/1000 | Loss: 0.00002246
Iteration 73/1000 | Loss: 0.00002246
Iteration 74/1000 | Loss: 0.00002246
Iteration 75/1000 | Loss: 0.00002246
Iteration 76/1000 | Loss: 0.00002246
Iteration 77/1000 | Loss: 0.00002246
Iteration 78/1000 | Loss: 0.00002246
Iteration 79/1000 | Loss: 0.00002245
Iteration 80/1000 | Loss: 0.00002245
Iteration 81/1000 | Loss: 0.00002245
Iteration 82/1000 | Loss: 0.00002244
Iteration 83/1000 | Loss: 0.00002244
Iteration 84/1000 | Loss: 0.00002244
Iteration 85/1000 | Loss: 0.00002243
Iteration 86/1000 | Loss: 0.00002243
Iteration 87/1000 | Loss: 0.00002243
Iteration 88/1000 | Loss: 0.00002243
Iteration 89/1000 | Loss: 0.00002242
Iteration 90/1000 | Loss: 0.00002242
Iteration 91/1000 | Loss: 0.00002242
Iteration 92/1000 | Loss: 0.00002242
Iteration 93/1000 | Loss: 0.00002242
Iteration 94/1000 | Loss: 0.00002242
Iteration 95/1000 | Loss: 0.00002242
Iteration 96/1000 | Loss: 0.00002241
Iteration 97/1000 | Loss: 0.00002241
Iteration 98/1000 | Loss: 0.00002241
Iteration 99/1000 | Loss: 0.00002241
Iteration 100/1000 | Loss: 0.00002241
Iteration 101/1000 | Loss: 0.00002241
Iteration 102/1000 | Loss: 0.00002241
Iteration 103/1000 | Loss: 0.00002241
Iteration 104/1000 | Loss: 0.00002240
Iteration 105/1000 | Loss: 0.00002240
Iteration 106/1000 | Loss: 0.00002240
Iteration 107/1000 | Loss: 0.00002240
Iteration 108/1000 | Loss: 0.00002240
Iteration 109/1000 | Loss: 0.00002240
Iteration 110/1000 | Loss: 0.00002240
Iteration 111/1000 | Loss: 0.00002239
Iteration 112/1000 | Loss: 0.00002239
Iteration 113/1000 | Loss: 0.00002239
Iteration 114/1000 | Loss: 0.00002239
Iteration 115/1000 | Loss: 0.00002239
Iteration 116/1000 | Loss: 0.00002239
Iteration 117/1000 | Loss: 0.00002239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.239213790744543e-05, 2.239213790744543e-05, 2.239213790744543e-05, 2.239213790744543e-05, 2.239213790744543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.239213790744543e-05

Optimization complete. Final v2v error: 3.952179193496704 mm

Highest mean error: 6.1510162353515625 mm for frame 60

Lowest mean error: 3.605893850326538 mm for frame 106

Saving results

Total time: 41.39303779602051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020126
Iteration 2/25 | Loss: 0.00259504
Iteration 3/25 | Loss: 0.00211631
Iteration 4/25 | Loss: 0.00213244
Iteration 5/25 | Loss: 0.00180678
Iteration 6/25 | Loss: 0.00164837
Iteration 7/25 | Loss: 0.00157825
Iteration 8/25 | Loss: 0.00155414
Iteration 9/25 | Loss: 0.00152477
Iteration 10/25 | Loss: 0.00152040
Iteration 11/25 | Loss: 0.00150378
Iteration 12/25 | Loss: 0.00149371
Iteration 13/25 | Loss: 0.00149183
Iteration 14/25 | Loss: 0.00148724
Iteration 15/25 | Loss: 0.00149039
Iteration 16/25 | Loss: 0.00149632
Iteration 17/25 | Loss: 0.00149267
Iteration 18/25 | Loss: 0.00149075
Iteration 19/25 | Loss: 0.00148834
Iteration 20/25 | Loss: 0.00149247
Iteration 21/25 | Loss: 0.00149158
Iteration 22/25 | Loss: 0.00149263
Iteration 23/25 | Loss: 0.00149096
Iteration 24/25 | Loss: 0.00148958
Iteration 25/25 | Loss: 0.00148675

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41310263
Iteration 2/25 | Loss: 0.00319205
Iteration 3/25 | Loss: 0.00291300
Iteration 4/25 | Loss: 0.00291299
Iteration 5/25 | Loss: 0.00288230
Iteration 6/25 | Loss: 0.00288230
Iteration 7/25 | Loss: 0.00288230
Iteration 8/25 | Loss: 0.00288230
Iteration 9/25 | Loss: 0.00288230
Iteration 10/25 | Loss: 0.00288230
Iteration 11/25 | Loss: 0.00288230
Iteration 12/25 | Loss: 0.00288230
Iteration 13/25 | Loss: 0.00288230
Iteration 14/25 | Loss: 0.00288230
Iteration 15/25 | Loss: 0.00288230
Iteration 16/25 | Loss: 0.00288230
Iteration 17/25 | Loss: 0.00288230
Iteration 18/25 | Loss: 0.00288230
Iteration 19/25 | Loss: 0.00288230
Iteration 20/25 | Loss: 0.00288230
Iteration 21/25 | Loss: 0.00288230
Iteration 22/25 | Loss: 0.00288230
Iteration 23/25 | Loss: 0.00288230
Iteration 24/25 | Loss: 0.00288230
Iteration 25/25 | Loss: 0.00288230

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00288230
Iteration 2/1000 | Loss: 0.00038906
Iteration 3/1000 | Loss: 0.00044352
Iteration 4/1000 | Loss: 0.00024327
Iteration 5/1000 | Loss: 0.00022928
Iteration 6/1000 | Loss: 0.00021632
Iteration 7/1000 | Loss: 0.00127104
Iteration 8/1000 | Loss: 0.00067070
Iteration 9/1000 | Loss: 0.00021170
Iteration 10/1000 | Loss: 0.00020665
Iteration 11/1000 | Loss: 0.00016017
Iteration 12/1000 | Loss: 0.00070148
Iteration 13/1000 | Loss: 0.00174793
Iteration 14/1000 | Loss: 0.00069679
Iteration 15/1000 | Loss: 0.00022033
Iteration 16/1000 | Loss: 0.00031993
Iteration 17/1000 | Loss: 0.00026279
Iteration 18/1000 | Loss: 0.00023976
Iteration 19/1000 | Loss: 0.00029100
Iteration 20/1000 | Loss: 0.00018117
Iteration 21/1000 | Loss: 0.00018567
Iteration 22/1000 | Loss: 0.00034825
Iteration 23/1000 | Loss: 0.00027417
Iteration 24/1000 | Loss: 0.00058712
Iteration 25/1000 | Loss: 0.00022325
Iteration 26/1000 | Loss: 0.00029295
Iteration 27/1000 | Loss: 0.00011479
Iteration 28/1000 | Loss: 0.00026293
Iteration 29/1000 | Loss: 0.00016485
Iteration 30/1000 | Loss: 0.00024131
Iteration 31/1000 | Loss: 0.00050140
Iteration 32/1000 | Loss: 0.00038557
Iteration 33/1000 | Loss: 0.00033142
Iteration 34/1000 | Loss: 0.00060956
Iteration 35/1000 | Loss: 0.00050418
Iteration 36/1000 | Loss: 0.00026959
Iteration 37/1000 | Loss: 0.00012357
Iteration 38/1000 | Loss: 0.00011895
Iteration 39/1000 | Loss: 0.00010694
Iteration 40/1000 | Loss: 0.00010424
Iteration 41/1000 | Loss: 0.00013576
Iteration 42/1000 | Loss: 0.00014327
Iteration 43/1000 | Loss: 0.00010703
Iteration 44/1000 | Loss: 0.00029093
Iteration 45/1000 | Loss: 0.00010292
Iteration 46/1000 | Loss: 0.00009880
Iteration 47/1000 | Loss: 0.00009630
Iteration 48/1000 | Loss: 0.00011226
Iteration 49/1000 | Loss: 0.00009844
Iteration 50/1000 | Loss: 0.00036387
Iteration 51/1000 | Loss: 0.00046480
Iteration 52/1000 | Loss: 0.00010034
Iteration 53/1000 | Loss: 0.00009491
Iteration 54/1000 | Loss: 0.00009230
Iteration 55/1000 | Loss: 0.00010085
Iteration 56/1000 | Loss: 0.00009276
Iteration 57/1000 | Loss: 0.00009522
Iteration 58/1000 | Loss: 0.00009994
Iteration 59/1000 | Loss: 0.00009211
Iteration 60/1000 | Loss: 0.00008656
Iteration 61/1000 | Loss: 0.00008501
Iteration 62/1000 | Loss: 0.00008697
Iteration 63/1000 | Loss: 0.00008225
Iteration 64/1000 | Loss: 0.00012352
Iteration 65/1000 | Loss: 0.00007985
Iteration 66/1000 | Loss: 0.00009383
Iteration 67/1000 | Loss: 0.00007740
Iteration 68/1000 | Loss: 0.00009983
Iteration 69/1000 | Loss: 0.00007614
Iteration 70/1000 | Loss: 0.00015317
Iteration 71/1000 | Loss: 0.00011270
Iteration 72/1000 | Loss: 0.00008064
Iteration 73/1000 | Loss: 0.00007445
Iteration 74/1000 | Loss: 0.00007372
Iteration 75/1000 | Loss: 0.00007312
Iteration 76/1000 | Loss: 0.00014627
Iteration 77/1000 | Loss: 0.00008220
Iteration 78/1000 | Loss: 0.00007935
Iteration 79/1000 | Loss: 0.00007225
Iteration 80/1000 | Loss: 0.00007218
Iteration 81/1000 | Loss: 0.00007186
Iteration 82/1000 | Loss: 0.00007160
Iteration 83/1000 | Loss: 0.00011772
Iteration 84/1000 | Loss: 0.00007454
Iteration 85/1000 | Loss: 0.00007131
Iteration 86/1000 | Loss: 0.00007130
Iteration 87/1000 | Loss: 0.00007112
Iteration 88/1000 | Loss: 0.00008671
Iteration 89/1000 | Loss: 0.00007118
Iteration 90/1000 | Loss: 0.00007092
Iteration 91/1000 | Loss: 0.00007088
Iteration 92/1000 | Loss: 0.00007087
Iteration 93/1000 | Loss: 0.00007086
Iteration 94/1000 | Loss: 0.00007086
Iteration 95/1000 | Loss: 0.00007085
Iteration 96/1000 | Loss: 0.00007085
Iteration 97/1000 | Loss: 0.00009495
Iteration 98/1000 | Loss: 0.00007080
Iteration 99/1000 | Loss: 0.00007079
Iteration 100/1000 | Loss: 0.00007078
Iteration 101/1000 | Loss: 0.00007078
Iteration 102/1000 | Loss: 0.00007077
Iteration 103/1000 | Loss: 0.00007077
Iteration 104/1000 | Loss: 0.00007077
Iteration 105/1000 | Loss: 0.00007076
Iteration 106/1000 | Loss: 0.00007076
Iteration 107/1000 | Loss: 0.00007075
Iteration 108/1000 | Loss: 0.00007075
Iteration 109/1000 | Loss: 0.00007075
Iteration 110/1000 | Loss: 0.00007074
Iteration 111/1000 | Loss: 0.00007074
Iteration 112/1000 | Loss: 0.00007074
Iteration 113/1000 | Loss: 0.00007074
Iteration 114/1000 | Loss: 0.00007074
Iteration 115/1000 | Loss: 0.00007074
Iteration 116/1000 | Loss: 0.00007073
Iteration 117/1000 | Loss: 0.00007073
Iteration 118/1000 | Loss: 0.00007073
Iteration 119/1000 | Loss: 0.00007073
Iteration 120/1000 | Loss: 0.00007073
Iteration 121/1000 | Loss: 0.00007071
Iteration 122/1000 | Loss: 0.00007071
Iteration 123/1000 | Loss: 0.00007071
Iteration 124/1000 | Loss: 0.00007071
Iteration 125/1000 | Loss: 0.00007071
Iteration 126/1000 | Loss: 0.00007071
Iteration 127/1000 | Loss: 0.00007071
Iteration 128/1000 | Loss: 0.00007070
Iteration 129/1000 | Loss: 0.00007070
Iteration 130/1000 | Loss: 0.00007070
Iteration 131/1000 | Loss: 0.00007070
Iteration 132/1000 | Loss: 0.00007070
Iteration 133/1000 | Loss: 0.00007070
Iteration 134/1000 | Loss: 0.00007070
Iteration 135/1000 | Loss: 0.00007069
Iteration 136/1000 | Loss: 0.00007069
Iteration 137/1000 | Loss: 0.00007069
Iteration 138/1000 | Loss: 0.00007069
Iteration 139/1000 | Loss: 0.00007069
Iteration 140/1000 | Loss: 0.00007068
Iteration 141/1000 | Loss: 0.00007068
Iteration 142/1000 | Loss: 0.00007068
Iteration 143/1000 | Loss: 0.00007068
Iteration 144/1000 | Loss: 0.00007067
Iteration 145/1000 | Loss: 0.00007067
Iteration 146/1000 | Loss: 0.00007067
Iteration 147/1000 | Loss: 0.00007067
Iteration 148/1000 | Loss: 0.00007067
Iteration 149/1000 | Loss: 0.00007067
Iteration 150/1000 | Loss: 0.00007067
Iteration 151/1000 | Loss: 0.00007067
Iteration 152/1000 | Loss: 0.00007066
Iteration 153/1000 | Loss: 0.00007066
Iteration 154/1000 | Loss: 0.00007066
Iteration 155/1000 | Loss: 0.00007066
Iteration 156/1000 | Loss: 0.00007066
Iteration 157/1000 | Loss: 0.00007066
Iteration 158/1000 | Loss: 0.00007066
Iteration 159/1000 | Loss: 0.00007066
Iteration 160/1000 | Loss: 0.00007066
Iteration 161/1000 | Loss: 0.00007066
Iteration 162/1000 | Loss: 0.00007066
Iteration 163/1000 | Loss: 0.00007066
Iteration 164/1000 | Loss: 0.00007066
Iteration 165/1000 | Loss: 0.00007066
Iteration 166/1000 | Loss: 0.00007066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [7.066388934617862e-05, 7.066388934617862e-05, 7.066388934617862e-05, 7.066388934617862e-05, 7.066388934617862e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.066388934617862e-05

Optimization complete. Final v2v error: 4.6210150718688965 mm

Highest mean error: 10.761209487915039 mm for frame 35

Lowest mean error: 3.2667698860168457 mm for frame 89

Saving results

Total time: 171.91866612434387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447063
Iteration 2/25 | Loss: 0.00130000
Iteration 3/25 | Loss: 0.00124618
Iteration 4/25 | Loss: 0.00124001
Iteration 5/25 | Loss: 0.00123759
Iteration 6/25 | Loss: 0.00123723
Iteration 7/25 | Loss: 0.00123723
Iteration 8/25 | Loss: 0.00123722
Iteration 9/25 | Loss: 0.00123722
Iteration 10/25 | Loss: 0.00123722
Iteration 11/25 | Loss: 0.00123722
Iteration 12/25 | Loss: 0.00123722
Iteration 13/25 | Loss: 0.00123722
Iteration 14/25 | Loss: 0.00123722
Iteration 15/25 | Loss: 0.00123722
Iteration 16/25 | Loss: 0.00123722
Iteration 17/25 | Loss: 0.00123722
Iteration 18/25 | Loss: 0.00123722
Iteration 19/25 | Loss: 0.00123722
Iteration 20/25 | Loss: 0.00123722
Iteration 21/25 | Loss: 0.00123722
Iteration 22/25 | Loss: 0.00123722
Iteration 23/25 | Loss: 0.00123722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012372175697237253, 0.0012372175697237253, 0.0012372175697237253, 0.0012372175697237253, 0.0012372175697237253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012372175697237253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44399667
Iteration 2/25 | Loss: 0.00085476
Iteration 3/25 | Loss: 0.00085475
Iteration 4/25 | Loss: 0.00085475
Iteration 5/25 | Loss: 0.00085475
Iteration 6/25 | Loss: 0.00085475
Iteration 7/25 | Loss: 0.00085475
Iteration 8/25 | Loss: 0.00085475
Iteration 9/25 | Loss: 0.00085475
Iteration 10/25 | Loss: 0.00085475
Iteration 11/25 | Loss: 0.00085475
Iteration 12/25 | Loss: 0.00085475
Iteration 13/25 | Loss: 0.00085475
Iteration 14/25 | Loss: 0.00085475
Iteration 15/25 | Loss: 0.00085475
Iteration 16/25 | Loss: 0.00085475
Iteration 17/25 | Loss: 0.00085475
Iteration 18/25 | Loss: 0.00085475
Iteration 19/25 | Loss: 0.00085475
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008547462057322264, 0.0008547462057322264, 0.0008547462057322264, 0.0008547462057322264, 0.0008547462057322264]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008547462057322264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085475
Iteration 2/1000 | Loss: 0.00002911
Iteration 3/1000 | Loss: 0.00001951
Iteration 4/1000 | Loss: 0.00001572
Iteration 5/1000 | Loss: 0.00001461
Iteration 6/1000 | Loss: 0.00001404
Iteration 7/1000 | Loss: 0.00001349
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001292
Iteration 10/1000 | Loss: 0.00001289
Iteration 11/1000 | Loss: 0.00001284
Iteration 12/1000 | Loss: 0.00001269
Iteration 13/1000 | Loss: 0.00001266
Iteration 14/1000 | Loss: 0.00001263
Iteration 15/1000 | Loss: 0.00001262
Iteration 16/1000 | Loss: 0.00001261
Iteration 17/1000 | Loss: 0.00001256
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001248
Iteration 20/1000 | Loss: 0.00001247
Iteration 21/1000 | Loss: 0.00001246
Iteration 22/1000 | Loss: 0.00001246
Iteration 23/1000 | Loss: 0.00001246
Iteration 24/1000 | Loss: 0.00001245
Iteration 25/1000 | Loss: 0.00001245
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001243
Iteration 32/1000 | Loss: 0.00001243
Iteration 33/1000 | Loss: 0.00001243
Iteration 34/1000 | Loss: 0.00001242
Iteration 35/1000 | Loss: 0.00001242
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001239
Iteration 43/1000 | Loss: 0.00001239
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001239
Iteration 46/1000 | Loss: 0.00001239
Iteration 47/1000 | Loss: 0.00001238
Iteration 48/1000 | Loss: 0.00001238
Iteration 49/1000 | Loss: 0.00001238
Iteration 50/1000 | Loss: 0.00001238
Iteration 51/1000 | Loss: 0.00001238
Iteration 52/1000 | Loss: 0.00001238
Iteration 53/1000 | Loss: 0.00001237
Iteration 54/1000 | Loss: 0.00001237
Iteration 55/1000 | Loss: 0.00001235
Iteration 56/1000 | Loss: 0.00001235
Iteration 57/1000 | Loss: 0.00001235
Iteration 58/1000 | Loss: 0.00001235
Iteration 59/1000 | Loss: 0.00001235
Iteration 60/1000 | Loss: 0.00001235
Iteration 61/1000 | Loss: 0.00001235
Iteration 62/1000 | Loss: 0.00001235
Iteration 63/1000 | Loss: 0.00001235
Iteration 64/1000 | Loss: 0.00001234
Iteration 65/1000 | Loss: 0.00001234
Iteration 66/1000 | Loss: 0.00001234
Iteration 67/1000 | Loss: 0.00001234
Iteration 68/1000 | Loss: 0.00001233
Iteration 69/1000 | Loss: 0.00001233
Iteration 70/1000 | Loss: 0.00001233
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001233
Iteration 75/1000 | Loss: 0.00001233
Iteration 76/1000 | Loss: 0.00001232
Iteration 77/1000 | Loss: 0.00001232
Iteration 78/1000 | Loss: 0.00001232
Iteration 79/1000 | Loss: 0.00001232
Iteration 80/1000 | Loss: 0.00001232
Iteration 81/1000 | Loss: 0.00001232
Iteration 82/1000 | Loss: 0.00001232
Iteration 83/1000 | Loss: 0.00001232
Iteration 84/1000 | Loss: 0.00001232
Iteration 85/1000 | Loss: 0.00001232
Iteration 86/1000 | Loss: 0.00001231
Iteration 87/1000 | Loss: 0.00001231
Iteration 88/1000 | Loss: 0.00001231
Iteration 89/1000 | Loss: 0.00001231
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001231
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001230
Iteration 100/1000 | Loss: 0.00001230
Iteration 101/1000 | Loss: 0.00001230
Iteration 102/1000 | Loss: 0.00001230
Iteration 103/1000 | Loss: 0.00001230
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001230
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001229
Iteration 110/1000 | Loss: 0.00001229
Iteration 111/1000 | Loss: 0.00001229
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001229
Iteration 116/1000 | Loss: 0.00001229
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001228
Iteration 120/1000 | Loss: 0.00001228
Iteration 121/1000 | Loss: 0.00001228
Iteration 122/1000 | Loss: 0.00001228
Iteration 123/1000 | Loss: 0.00001228
Iteration 124/1000 | Loss: 0.00001228
Iteration 125/1000 | Loss: 0.00001228
Iteration 126/1000 | Loss: 0.00001228
Iteration 127/1000 | Loss: 0.00001228
Iteration 128/1000 | Loss: 0.00001228
Iteration 129/1000 | Loss: 0.00001228
Iteration 130/1000 | Loss: 0.00001228
Iteration 131/1000 | Loss: 0.00001228
Iteration 132/1000 | Loss: 0.00001228
Iteration 133/1000 | Loss: 0.00001228
Iteration 134/1000 | Loss: 0.00001228
Iteration 135/1000 | Loss: 0.00001227
Iteration 136/1000 | Loss: 0.00001227
Iteration 137/1000 | Loss: 0.00001227
Iteration 138/1000 | Loss: 0.00001227
Iteration 139/1000 | Loss: 0.00001226
Iteration 140/1000 | Loss: 0.00001226
Iteration 141/1000 | Loss: 0.00001226
Iteration 142/1000 | Loss: 0.00001226
Iteration 143/1000 | Loss: 0.00001226
Iteration 144/1000 | Loss: 0.00001226
Iteration 145/1000 | Loss: 0.00001226
Iteration 146/1000 | Loss: 0.00001226
Iteration 147/1000 | Loss: 0.00001226
Iteration 148/1000 | Loss: 0.00001226
Iteration 149/1000 | Loss: 0.00001226
Iteration 150/1000 | Loss: 0.00001226
Iteration 151/1000 | Loss: 0.00001226
Iteration 152/1000 | Loss: 0.00001225
Iteration 153/1000 | Loss: 0.00001225
Iteration 154/1000 | Loss: 0.00001225
Iteration 155/1000 | Loss: 0.00001225
Iteration 156/1000 | Loss: 0.00001225
Iteration 157/1000 | Loss: 0.00001225
Iteration 158/1000 | Loss: 0.00001225
Iteration 159/1000 | Loss: 0.00001225
Iteration 160/1000 | Loss: 0.00001225
Iteration 161/1000 | Loss: 0.00001225
Iteration 162/1000 | Loss: 0.00001225
Iteration 163/1000 | Loss: 0.00001225
Iteration 164/1000 | Loss: 0.00001225
Iteration 165/1000 | Loss: 0.00001225
Iteration 166/1000 | Loss: 0.00001225
Iteration 167/1000 | Loss: 0.00001225
Iteration 168/1000 | Loss: 0.00001225
Iteration 169/1000 | Loss: 0.00001225
Iteration 170/1000 | Loss: 0.00001225
Iteration 171/1000 | Loss: 0.00001225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.2253002751094755e-05, 1.2253002751094755e-05, 1.2253002751094755e-05, 1.2253002751094755e-05, 1.2253002751094755e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2253002751094755e-05

Optimization complete. Final v2v error: 2.969820737838745 mm

Highest mean error: 3.324824571609497 mm for frame 110

Lowest mean error: 2.7184524536132812 mm for frame 17

Saving results

Total time: 34.21729326248169
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015500
Iteration 2/25 | Loss: 0.00295099
Iteration 3/25 | Loss: 0.00225771
Iteration 4/25 | Loss: 0.00198279
Iteration 5/25 | Loss: 0.00192740
Iteration 6/25 | Loss: 0.00162979
Iteration 7/25 | Loss: 0.00143318
Iteration 8/25 | Loss: 0.00141825
Iteration 9/25 | Loss: 0.00139651
Iteration 10/25 | Loss: 0.00137653
Iteration 11/25 | Loss: 0.00136901
Iteration 12/25 | Loss: 0.00136668
Iteration 13/25 | Loss: 0.00136352
Iteration 14/25 | Loss: 0.00136233
Iteration 15/25 | Loss: 0.00136200
Iteration 16/25 | Loss: 0.00136190
Iteration 17/25 | Loss: 0.00136189
Iteration 18/25 | Loss: 0.00136189
Iteration 19/25 | Loss: 0.00136189
Iteration 20/25 | Loss: 0.00136189
Iteration 21/25 | Loss: 0.00136189
Iteration 22/25 | Loss: 0.00136189
Iteration 23/25 | Loss: 0.00136189
Iteration 24/25 | Loss: 0.00136189
Iteration 25/25 | Loss: 0.00136189

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47571003
Iteration 2/25 | Loss: 0.00066663
Iteration 3/25 | Loss: 0.00066663
Iteration 4/25 | Loss: 0.00066663
Iteration 5/25 | Loss: 0.00066663
Iteration 6/25 | Loss: 0.00066663
Iteration 7/25 | Loss: 0.00066663
Iteration 8/25 | Loss: 0.00066663
Iteration 9/25 | Loss: 0.00066663
Iteration 10/25 | Loss: 0.00066663
Iteration 11/25 | Loss: 0.00066663
Iteration 12/25 | Loss: 0.00066663
Iteration 13/25 | Loss: 0.00066663
Iteration 14/25 | Loss: 0.00066663
Iteration 15/25 | Loss: 0.00066663
Iteration 16/25 | Loss: 0.00066663
Iteration 17/25 | Loss: 0.00066663
Iteration 18/25 | Loss: 0.00066663
Iteration 19/25 | Loss: 0.00066663
Iteration 20/25 | Loss: 0.00066663
Iteration 21/25 | Loss: 0.00066663
Iteration 22/25 | Loss: 0.00066663
Iteration 23/25 | Loss: 0.00066663
Iteration 24/25 | Loss: 0.00066663
Iteration 25/25 | Loss: 0.00066663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066663
Iteration 2/1000 | Loss: 0.00005250
Iteration 3/1000 | Loss: 0.00003717
Iteration 4/1000 | Loss: 0.00003171
Iteration 5/1000 | Loss: 0.00003047
Iteration 6/1000 | Loss: 0.00002945
Iteration 7/1000 | Loss: 0.00002891
Iteration 8/1000 | Loss: 0.00002830
Iteration 9/1000 | Loss: 0.00002807
Iteration 10/1000 | Loss: 0.00002786
Iteration 11/1000 | Loss: 0.00002765
Iteration 12/1000 | Loss: 0.00002761
Iteration 13/1000 | Loss: 0.00002758
Iteration 14/1000 | Loss: 0.00002754
Iteration 15/1000 | Loss: 0.00002754
Iteration 16/1000 | Loss: 0.00002753
Iteration 17/1000 | Loss: 0.00002753
Iteration 18/1000 | Loss: 0.00002752
Iteration 19/1000 | Loss: 0.00002752
Iteration 20/1000 | Loss: 0.00002749
Iteration 21/1000 | Loss: 0.00002736
Iteration 22/1000 | Loss: 0.00002730
Iteration 23/1000 | Loss: 0.00002729
Iteration 24/1000 | Loss: 0.00002727
Iteration 25/1000 | Loss: 0.00002727
Iteration 26/1000 | Loss: 0.00002726
Iteration 27/1000 | Loss: 0.00002726
Iteration 28/1000 | Loss: 0.00002726
Iteration 29/1000 | Loss: 0.00002726
Iteration 30/1000 | Loss: 0.00002725
Iteration 31/1000 | Loss: 0.00002725
Iteration 32/1000 | Loss: 0.00002723
Iteration 33/1000 | Loss: 0.00002723
Iteration 34/1000 | Loss: 0.00002722
Iteration 35/1000 | Loss: 0.00002722
Iteration 36/1000 | Loss: 0.00002722
Iteration 37/1000 | Loss: 0.00002722
Iteration 38/1000 | Loss: 0.00002722
Iteration 39/1000 | Loss: 0.00002722
Iteration 40/1000 | Loss: 0.00002722
Iteration 41/1000 | Loss: 0.00002722
Iteration 42/1000 | Loss: 0.00002721
Iteration 43/1000 | Loss: 0.00002721
Iteration 44/1000 | Loss: 0.00002721
Iteration 45/1000 | Loss: 0.00002721
Iteration 46/1000 | Loss: 0.00002721
Iteration 47/1000 | Loss: 0.00002721
Iteration 48/1000 | Loss: 0.00002721
Iteration 49/1000 | Loss: 0.00002721
Iteration 50/1000 | Loss: 0.00002720
Iteration 51/1000 | Loss: 0.00002720
Iteration 52/1000 | Loss: 0.00002720
Iteration 53/1000 | Loss: 0.00002720
Iteration 54/1000 | Loss: 0.00002720
Iteration 55/1000 | Loss: 0.00002719
Iteration 56/1000 | Loss: 0.00002719
Iteration 57/1000 | Loss: 0.00002719
Iteration 58/1000 | Loss: 0.00002718
Iteration 59/1000 | Loss: 0.00002718
Iteration 60/1000 | Loss: 0.00002718
Iteration 61/1000 | Loss: 0.00002717
Iteration 62/1000 | Loss: 0.00002717
Iteration 63/1000 | Loss: 0.00002717
Iteration 64/1000 | Loss: 0.00002717
Iteration 65/1000 | Loss: 0.00002717
Iteration 66/1000 | Loss: 0.00002717
Iteration 67/1000 | Loss: 0.00002717
Iteration 68/1000 | Loss: 0.00002717
Iteration 69/1000 | Loss: 0.00002717
Iteration 70/1000 | Loss: 0.00002716
Iteration 71/1000 | Loss: 0.00002716
Iteration 72/1000 | Loss: 0.00002716
Iteration 73/1000 | Loss: 0.00002715
Iteration 74/1000 | Loss: 0.00002715
Iteration 75/1000 | Loss: 0.00002715
Iteration 76/1000 | Loss: 0.00002715
Iteration 77/1000 | Loss: 0.00002714
Iteration 78/1000 | Loss: 0.00002714
Iteration 79/1000 | Loss: 0.00002714
Iteration 80/1000 | Loss: 0.00002714
Iteration 81/1000 | Loss: 0.00002714
Iteration 82/1000 | Loss: 0.00002713
Iteration 83/1000 | Loss: 0.00002713
Iteration 84/1000 | Loss: 0.00002713
Iteration 85/1000 | Loss: 0.00002713
Iteration 86/1000 | Loss: 0.00002713
Iteration 87/1000 | Loss: 0.00002712
Iteration 88/1000 | Loss: 0.00002712
Iteration 89/1000 | Loss: 0.00002712
Iteration 90/1000 | Loss: 0.00002712
Iteration 91/1000 | Loss: 0.00002712
Iteration 92/1000 | Loss: 0.00002712
Iteration 93/1000 | Loss: 0.00002712
Iteration 94/1000 | Loss: 0.00002712
Iteration 95/1000 | Loss: 0.00002712
Iteration 96/1000 | Loss: 0.00002711
Iteration 97/1000 | Loss: 0.00002711
Iteration 98/1000 | Loss: 0.00002711
Iteration 99/1000 | Loss: 0.00002711
Iteration 100/1000 | Loss: 0.00002711
Iteration 101/1000 | Loss: 0.00002711
Iteration 102/1000 | Loss: 0.00002711
Iteration 103/1000 | Loss: 0.00002711
Iteration 104/1000 | Loss: 0.00002711
Iteration 105/1000 | Loss: 0.00002710
Iteration 106/1000 | Loss: 0.00002710
Iteration 107/1000 | Loss: 0.00002710
Iteration 108/1000 | Loss: 0.00002710
Iteration 109/1000 | Loss: 0.00002710
Iteration 110/1000 | Loss: 0.00002710
Iteration 111/1000 | Loss: 0.00002710
Iteration 112/1000 | Loss: 0.00002710
Iteration 113/1000 | Loss: 0.00002710
Iteration 114/1000 | Loss: 0.00002710
Iteration 115/1000 | Loss: 0.00002710
Iteration 116/1000 | Loss: 0.00002710
Iteration 117/1000 | Loss: 0.00002709
Iteration 118/1000 | Loss: 0.00002709
Iteration 119/1000 | Loss: 0.00002709
Iteration 120/1000 | Loss: 0.00002709
Iteration 121/1000 | Loss: 0.00002709
Iteration 122/1000 | Loss: 0.00002709
Iteration 123/1000 | Loss: 0.00002709
Iteration 124/1000 | Loss: 0.00002709
Iteration 125/1000 | Loss: 0.00002709
Iteration 126/1000 | Loss: 0.00002709
Iteration 127/1000 | Loss: 0.00002709
Iteration 128/1000 | Loss: 0.00002709
Iteration 129/1000 | Loss: 0.00002708
Iteration 130/1000 | Loss: 0.00002708
Iteration 131/1000 | Loss: 0.00002708
Iteration 132/1000 | Loss: 0.00002708
Iteration 133/1000 | Loss: 0.00002708
Iteration 134/1000 | Loss: 0.00002708
Iteration 135/1000 | Loss: 0.00002708
Iteration 136/1000 | Loss: 0.00002708
Iteration 137/1000 | Loss: 0.00002708
Iteration 138/1000 | Loss: 0.00002708
Iteration 139/1000 | Loss: 0.00002707
Iteration 140/1000 | Loss: 0.00002707
Iteration 141/1000 | Loss: 0.00002707
Iteration 142/1000 | Loss: 0.00002707
Iteration 143/1000 | Loss: 0.00002707
Iteration 144/1000 | Loss: 0.00002707
Iteration 145/1000 | Loss: 0.00002707
Iteration 146/1000 | Loss: 0.00002707
Iteration 147/1000 | Loss: 0.00002707
Iteration 148/1000 | Loss: 0.00002707
Iteration 149/1000 | Loss: 0.00002707
Iteration 150/1000 | Loss: 0.00002707
Iteration 151/1000 | Loss: 0.00002707
Iteration 152/1000 | Loss: 0.00002707
Iteration 153/1000 | Loss: 0.00002707
Iteration 154/1000 | Loss: 0.00002707
Iteration 155/1000 | Loss: 0.00002707
Iteration 156/1000 | Loss: 0.00002707
Iteration 157/1000 | Loss: 0.00002707
Iteration 158/1000 | Loss: 0.00002707
Iteration 159/1000 | Loss: 0.00002707
Iteration 160/1000 | Loss: 0.00002707
Iteration 161/1000 | Loss: 0.00002707
Iteration 162/1000 | Loss: 0.00002707
Iteration 163/1000 | Loss: 0.00002707
Iteration 164/1000 | Loss: 0.00002707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [2.7066542315878905e-05, 2.7066542315878905e-05, 2.7066542315878905e-05, 2.7066542315878905e-05, 2.7066542315878905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7066542315878905e-05

Optimization complete. Final v2v error: 4.442704677581787 mm

Highest mean error: 4.700428009033203 mm for frame 145

Lowest mean error: 3.9324207305908203 mm for frame 6

Saving results

Total time: 57.46111607551575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00519928
Iteration 2/25 | Loss: 0.00171579
Iteration 3/25 | Loss: 0.00144356
Iteration 4/25 | Loss: 0.00140280
Iteration 5/25 | Loss: 0.00139514
Iteration 6/25 | Loss: 0.00139734
Iteration 7/25 | Loss: 0.00138173
Iteration 8/25 | Loss: 0.00137799
Iteration 9/25 | Loss: 0.00137744
Iteration 10/25 | Loss: 0.00137728
Iteration 11/25 | Loss: 0.00137728
Iteration 12/25 | Loss: 0.00137728
Iteration 13/25 | Loss: 0.00137728
Iteration 14/25 | Loss: 0.00137728
Iteration 15/25 | Loss: 0.00137728
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013772815000265837, 0.0013772815000265837, 0.0013772815000265837, 0.0013772815000265837, 0.0013772815000265837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013772815000265837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42462444
Iteration 2/25 | Loss: 0.00083881
Iteration 3/25 | Loss: 0.00083878
Iteration 4/25 | Loss: 0.00083878
Iteration 5/25 | Loss: 0.00083878
Iteration 6/25 | Loss: 0.00083878
Iteration 7/25 | Loss: 0.00083878
Iteration 8/25 | Loss: 0.00083878
Iteration 9/25 | Loss: 0.00083878
Iteration 10/25 | Loss: 0.00083878
Iteration 11/25 | Loss: 0.00083878
Iteration 12/25 | Loss: 0.00083878
Iteration 13/25 | Loss: 0.00083878
Iteration 14/25 | Loss: 0.00083878
Iteration 15/25 | Loss: 0.00083878
Iteration 16/25 | Loss: 0.00083878
Iteration 17/25 | Loss: 0.00083878
Iteration 18/25 | Loss: 0.00083878
Iteration 19/25 | Loss: 0.00083878
Iteration 20/25 | Loss: 0.00083878
Iteration 21/25 | Loss: 0.00083878
Iteration 22/25 | Loss: 0.00083878
Iteration 23/25 | Loss: 0.00083878
Iteration 24/25 | Loss: 0.00083878
Iteration 25/25 | Loss: 0.00083878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083878
Iteration 2/1000 | Loss: 0.00061394
Iteration 3/1000 | Loss: 0.00084381
Iteration 4/1000 | Loss: 0.00027349
Iteration 5/1000 | Loss: 0.00019980
Iteration 6/1000 | Loss: 0.00020191
Iteration 7/1000 | Loss: 0.00021009
Iteration 8/1000 | Loss: 0.00019893
Iteration 9/1000 | Loss: 0.00019037
Iteration 10/1000 | Loss: 0.00019854
Iteration 11/1000 | Loss: 0.00019032
Iteration 12/1000 | Loss: 0.00026977
Iteration 13/1000 | Loss: 0.00020139
Iteration 14/1000 | Loss: 0.00020489
Iteration 15/1000 | Loss: 0.00039839
Iteration 16/1000 | Loss: 0.00036437
Iteration 17/1000 | Loss: 0.00019119
Iteration 18/1000 | Loss: 0.00018740
Iteration 19/1000 | Loss: 0.00022496
Iteration 20/1000 | Loss: 0.00016027
Iteration 21/1000 | Loss: 0.00008221
Iteration 22/1000 | Loss: 0.00005035
Iteration 23/1000 | Loss: 0.00004665
Iteration 24/1000 | Loss: 0.00004525
Iteration 25/1000 | Loss: 0.00004403
Iteration 26/1000 | Loss: 0.00007897
Iteration 27/1000 | Loss: 0.00004898
Iteration 28/1000 | Loss: 0.00007554
Iteration 29/1000 | Loss: 0.00005028
Iteration 30/1000 | Loss: 0.00004318
Iteration 31/1000 | Loss: 0.00011259
Iteration 32/1000 | Loss: 0.00008220
Iteration 33/1000 | Loss: 0.00012480
Iteration 34/1000 | Loss: 0.00007878
Iteration 35/1000 | Loss: 0.00007731
Iteration 36/1000 | Loss: 0.00006487
Iteration 37/1000 | Loss: 0.00004405
Iteration 38/1000 | Loss: 0.00004271
Iteration 39/1000 | Loss: 0.00004261
Iteration 40/1000 | Loss: 0.00023505
Iteration 41/1000 | Loss: 0.00010556
Iteration 42/1000 | Loss: 0.00005066
Iteration 43/1000 | Loss: 0.00004523
Iteration 44/1000 | Loss: 0.00022315
Iteration 45/1000 | Loss: 0.00008773
Iteration 46/1000 | Loss: 0.00004251
Iteration 47/1000 | Loss: 0.00023797
Iteration 48/1000 | Loss: 0.00031067
Iteration 49/1000 | Loss: 0.00029066
Iteration 50/1000 | Loss: 0.00011795
Iteration 51/1000 | Loss: 0.00010327
Iteration 52/1000 | Loss: 0.00012915
Iteration 53/1000 | Loss: 0.00007004
Iteration 54/1000 | Loss: 0.00008852
Iteration 55/1000 | Loss: 0.00007056
Iteration 56/1000 | Loss: 0.00004699
Iteration 57/1000 | Loss: 0.00004546
Iteration 58/1000 | Loss: 0.00004336
Iteration 59/1000 | Loss: 0.00004259
Iteration 60/1000 | Loss: 0.00004199
Iteration 61/1000 | Loss: 0.00004163
Iteration 62/1000 | Loss: 0.00004146
Iteration 63/1000 | Loss: 0.00004145
Iteration 64/1000 | Loss: 0.00004144
Iteration 65/1000 | Loss: 0.00004142
Iteration 66/1000 | Loss: 0.00004137
Iteration 67/1000 | Loss: 0.00004135
Iteration 68/1000 | Loss: 0.00004134
Iteration 69/1000 | Loss: 0.00004134
Iteration 70/1000 | Loss: 0.00004134
Iteration 71/1000 | Loss: 0.00004133
Iteration 72/1000 | Loss: 0.00004133
Iteration 73/1000 | Loss: 0.00004132
Iteration 74/1000 | Loss: 0.00004132
Iteration 75/1000 | Loss: 0.00004131
Iteration 76/1000 | Loss: 0.00004130
Iteration 77/1000 | Loss: 0.00022613
Iteration 78/1000 | Loss: 0.00022612
Iteration 79/1000 | Loss: 0.00022612
Iteration 80/1000 | Loss: 0.00023414
Iteration 81/1000 | Loss: 0.00019273
Iteration 82/1000 | Loss: 0.00010137
Iteration 83/1000 | Loss: 0.00008034
Iteration 84/1000 | Loss: 0.00004314
Iteration 85/1000 | Loss: 0.00004200
Iteration 86/1000 | Loss: 0.00004160
Iteration 87/1000 | Loss: 0.00004133
Iteration 88/1000 | Loss: 0.00004100
Iteration 89/1000 | Loss: 0.00004096
Iteration 90/1000 | Loss: 0.00004091
Iteration 91/1000 | Loss: 0.00004090
Iteration 92/1000 | Loss: 0.00004090
Iteration 93/1000 | Loss: 0.00004089
Iteration 94/1000 | Loss: 0.00008110
Iteration 95/1000 | Loss: 0.00008110
Iteration 96/1000 | Loss: 0.00017015
Iteration 97/1000 | Loss: 0.00009843
Iteration 98/1000 | Loss: 0.00011189
Iteration 99/1000 | Loss: 0.00007769
Iteration 100/1000 | Loss: 0.00013608
Iteration 101/1000 | Loss: 0.00007717
Iteration 102/1000 | Loss: 0.00004726
Iteration 103/1000 | Loss: 0.00004489
Iteration 104/1000 | Loss: 0.00004340
Iteration 105/1000 | Loss: 0.00004122
Iteration 106/1000 | Loss: 0.00004092
Iteration 107/1000 | Loss: 0.00004074
Iteration 108/1000 | Loss: 0.00004066
Iteration 109/1000 | Loss: 0.00004060
Iteration 110/1000 | Loss: 0.00004060
Iteration 111/1000 | Loss: 0.00004059
Iteration 112/1000 | Loss: 0.00004057
Iteration 113/1000 | Loss: 0.00004057
Iteration 114/1000 | Loss: 0.00004057
Iteration 115/1000 | Loss: 0.00004057
Iteration 116/1000 | Loss: 0.00004057
Iteration 117/1000 | Loss: 0.00004057
Iteration 118/1000 | Loss: 0.00004056
Iteration 119/1000 | Loss: 0.00004056
Iteration 120/1000 | Loss: 0.00004056
Iteration 121/1000 | Loss: 0.00004056
Iteration 122/1000 | Loss: 0.00004056
Iteration 123/1000 | Loss: 0.00004056
Iteration 124/1000 | Loss: 0.00004056
Iteration 125/1000 | Loss: 0.00004056
Iteration 126/1000 | Loss: 0.00004056
Iteration 127/1000 | Loss: 0.00004056
Iteration 128/1000 | Loss: 0.00004054
Iteration 129/1000 | Loss: 0.00004054
Iteration 130/1000 | Loss: 0.00004054
Iteration 131/1000 | Loss: 0.00004053
Iteration 132/1000 | Loss: 0.00004053
Iteration 133/1000 | Loss: 0.00004053
Iteration 134/1000 | Loss: 0.00004053
Iteration 135/1000 | Loss: 0.00004052
Iteration 136/1000 | Loss: 0.00004052
Iteration 137/1000 | Loss: 0.00004052
Iteration 138/1000 | Loss: 0.00004052
Iteration 139/1000 | Loss: 0.00004051
Iteration 140/1000 | Loss: 0.00004051
Iteration 141/1000 | Loss: 0.00004051
Iteration 142/1000 | Loss: 0.00004051
Iteration 143/1000 | Loss: 0.00004051
Iteration 144/1000 | Loss: 0.00004051
Iteration 145/1000 | Loss: 0.00004051
Iteration 146/1000 | Loss: 0.00004051
Iteration 147/1000 | Loss: 0.00004051
Iteration 148/1000 | Loss: 0.00004050
Iteration 149/1000 | Loss: 0.00004050
Iteration 150/1000 | Loss: 0.00004050
Iteration 151/1000 | Loss: 0.00004050
Iteration 152/1000 | Loss: 0.00004050
Iteration 153/1000 | Loss: 0.00004050
Iteration 154/1000 | Loss: 0.00004050
Iteration 155/1000 | Loss: 0.00004049
Iteration 156/1000 | Loss: 0.00004049
Iteration 157/1000 | Loss: 0.00004049
Iteration 158/1000 | Loss: 0.00004049
Iteration 159/1000 | Loss: 0.00004049
Iteration 160/1000 | Loss: 0.00004049
Iteration 161/1000 | Loss: 0.00004049
Iteration 162/1000 | Loss: 0.00004049
Iteration 163/1000 | Loss: 0.00004048
Iteration 164/1000 | Loss: 0.00004048
Iteration 165/1000 | Loss: 0.00004048
Iteration 166/1000 | Loss: 0.00004048
Iteration 167/1000 | Loss: 0.00004048
Iteration 168/1000 | Loss: 0.00004048
Iteration 169/1000 | Loss: 0.00004048
Iteration 170/1000 | Loss: 0.00004048
Iteration 171/1000 | Loss: 0.00004048
Iteration 172/1000 | Loss: 0.00004048
Iteration 173/1000 | Loss: 0.00004048
Iteration 174/1000 | Loss: 0.00004048
Iteration 175/1000 | Loss: 0.00004048
Iteration 176/1000 | Loss: 0.00004048
Iteration 177/1000 | Loss: 0.00004048
Iteration 178/1000 | Loss: 0.00004048
Iteration 179/1000 | Loss: 0.00004048
Iteration 180/1000 | Loss: 0.00004048
Iteration 181/1000 | Loss: 0.00004048
Iteration 182/1000 | Loss: 0.00004048
Iteration 183/1000 | Loss: 0.00004048
Iteration 184/1000 | Loss: 0.00004048
Iteration 185/1000 | Loss: 0.00004048
Iteration 186/1000 | Loss: 0.00004048
Iteration 187/1000 | Loss: 0.00004048
Iteration 188/1000 | Loss: 0.00004048
Iteration 189/1000 | Loss: 0.00004048
Iteration 190/1000 | Loss: 0.00004048
Iteration 191/1000 | Loss: 0.00004048
Iteration 192/1000 | Loss: 0.00004048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [4.047713446198031e-05, 4.047713446198031e-05, 4.047713446198031e-05, 4.047713446198031e-05, 4.047713446198031e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.047713446198031e-05

Optimization complete. Final v2v error: 4.858532428741455 mm

Highest mean error: 6.389414310455322 mm for frame 180

Lowest mean error: 4.403411865234375 mm for frame 194

Saving results

Total time: 159.62061762809753
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056750
Iteration 2/25 | Loss: 0.01056750
Iteration 3/25 | Loss: 0.01056750
Iteration 4/25 | Loss: 0.01056750
Iteration 5/25 | Loss: 0.01056750
Iteration 6/25 | Loss: 0.01056749
Iteration 7/25 | Loss: 0.01056749
Iteration 8/25 | Loss: 0.00163733
Iteration 9/25 | Loss: 0.00140935
Iteration 10/25 | Loss: 0.00131585
Iteration 11/25 | Loss: 0.00127713
Iteration 12/25 | Loss: 0.00127583
Iteration 13/25 | Loss: 0.00127687
Iteration 14/25 | Loss: 0.00126942
Iteration 15/25 | Loss: 0.00127613
Iteration 16/25 | Loss: 0.00127019
Iteration 17/25 | Loss: 0.00127125
Iteration 18/25 | Loss: 0.00126907
Iteration 19/25 | Loss: 0.00126727
Iteration 20/25 | Loss: 0.00127076
Iteration 21/25 | Loss: 0.00127057
Iteration 22/25 | Loss: 0.00126707
Iteration 23/25 | Loss: 0.00126677
Iteration 24/25 | Loss: 0.00126548
Iteration 25/25 | Loss: 0.00126467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.51885128
Iteration 2/25 | Loss: 0.00094601
Iteration 3/25 | Loss: 0.00088509
Iteration 4/25 | Loss: 0.00088509
Iteration 5/25 | Loss: 0.00088509
Iteration 6/25 | Loss: 0.00088509
Iteration 7/25 | Loss: 0.00088509
Iteration 8/25 | Loss: 0.00088509
Iteration 9/25 | Loss: 0.00088509
Iteration 10/25 | Loss: 0.00088509
Iteration 11/25 | Loss: 0.00088509
Iteration 12/25 | Loss: 0.00088509
Iteration 13/25 | Loss: 0.00088509
Iteration 14/25 | Loss: 0.00088509
Iteration 15/25 | Loss: 0.00088509
Iteration 16/25 | Loss: 0.00088509
Iteration 17/25 | Loss: 0.00088509
Iteration 18/25 | Loss: 0.00088509
Iteration 19/25 | Loss: 0.00088509
Iteration 20/25 | Loss: 0.00088509
Iteration 21/25 | Loss: 0.00088509
Iteration 22/25 | Loss: 0.00088509
Iteration 23/25 | Loss: 0.00088509
Iteration 24/25 | Loss: 0.00088509
Iteration 25/25 | Loss: 0.00088509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088509
Iteration 2/1000 | Loss: 0.00007985
Iteration 3/1000 | Loss: 0.00001920
Iteration 4/1000 | Loss: 0.00005089
Iteration 5/1000 | Loss: 0.00012599
Iteration 6/1000 | Loss: 0.00002150
Iteration 7/1000 | Loss: 0.00003288
Iteration 8/1000 | Loss: 0.00001595
Iteration 9/1000 | Loss: 0.00006908
Iteration 10/1000 | Loss: 0.00012108
Iteration 11/1000 | Loss: 0.00009064
Iteration 12/1000 | Loss: 0.00004691
Iteration 13/1000 | Loss: 0.00002061
Iteration 14/1000 | Loss: 0.00001914
Iteration 15/1000 | Loss: 0.00001591
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00015276
Iteration 19/1000 | Loss: 0.00001885
Iteration 20/1000 | Loss: 0.00001465
Iteration 21/1000 | Loss: 0.00001441
Iteration 22/1000 | Loss: 0.00001440
Iteration 23/1000 | Loss: 0.00001425
Iteration 24/1000 | Loss: 0.00001425
Iteration 25/1000 | Loss: 0.00001422
Iteration 26/1000 | Loss: 0.00001421
Iteration 27/1000 | Loss: 0.00001420
Iteration 28/1000 | Loss: 0.00001415
Iteration 29/1000 | Loss: 0.00001415
Iteration 30/1000 | Loss: 0.00001414
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001413
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001413
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00002221
Iteration 37/1000 | Loss: 0.00001404
Iteration 38/1000 | Loss: 0.00001402
Iteration 39/1000 | Loss: 0.00001402
Iteration 40/1000 | Loss: 0.00001400
Iteration 41/1000 | Loss: 0.00001399
Iteration 42/1000 | Loss: 0.00001395
Iteration 43/1000 | Loss: 0.00001394
Iteration 44/1000 | Loss: 0.00001390
Iteration 45/1000 | Loss: 0.00001390
Iteration 46/1000 | Loss: 0.00001389
Iteration 47/1000 | Loss: 0.00001388
Iteration 48/1000 | Loss: 0.00001388
Iteration 49/1000 | Loss: 0.00001387
Iteration 50/1000 | Loss: 0.00001386
Iteration 51/1000 | Loss: 0.00001384
Iteration 52/1000 | Loss: 0.00001384
Iteration 53/1000 | Loss: 0.00001384
Iteration 54/1000 | Loss: 0.00001384
Iteration 55/1000 | Loss: 0.00001384
Iteration 56/1000 | Loss: 0.00001383
Iteration 57/1000 | Loss: 0.00001383
Iteration 58/1000 | Loss: 0.00001382
Iteration 59/1000 | Loss: 0.00001381
Iteration 60/1000 | Loss: 0.00001380
Iteration 61/1000 | Loss: 0.00001379
Iteration 62/1000 | Loss: 0.00010312
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001513
Iteration 65/1000 | Loss: 0.00001368
Iteration 66/1000 | Loss: 0.00001364
Iteration 67/1000 | Loss: 0.00001362
Iteration 68/1000 | Loss: 0.00001359
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00001342
Iteration 71/1000 | Loss: 0.00001341
Iteration 72/1000 | Loss: 0.00001341
Iteration 73/1000 | Loss: 0.00001340
Iteration 74/1000 | Loss: 0.00001340
Iteration 75/1000 | Loss: 0.00001340
Iteration 76/1000 | Loss: 0.00001340
Iteration 77/1000 | Loss: 0.00001339
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001336
Iteration 80/1000 | Loss: 0.00001336
Iteration 81/1000 | Loss: 0.00001336
Iteration 82/1000 | Loss: 0.00001335
Iteration 83/1000 | Loss: 0.00001335
Iteration 84/1000 | Loss: 0.00001334
Iteration 85/1000 | Loss: 0.00001334
Iteration 86/1000 | Loss: 0.00001334
Iteration 87/1000 | Loss: 0.00001333
Iteration 88/1000 | Loss: 0.00001333
Iteration 89/1000 | Loss: 0.00001332
Iteration 90/1000 | Loss: 0.00001332
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001330
Iteration 95/1000 | Loss: 0.00001330
Iteration 96/1000 | Loss: 0.00001329
Iteration 97/1000 | Loss: 0.00005696
Iteration 98/1000 | Loss: 0.00001321
Iteration 99/1000 | Loss: 0.00001315
Iteration 100/1000 | Loss: 0.00001315
Iteration 101/1000 | Loss: 0.00001315
Iteration 102/1000 | Loss: 0.00001314
Iteration 103/1000 | Loss: 0.00001313
Iteration 104/1000 | Loss: 0.00001313
Iteration 105/1000 | Loss: 0.00001313
Iteration 106/1000 | Loss: 0.00001313
Iteration 107/1000 | Loss: 0.00001313
Iteration 108/1000 | Loss: 0.00001313
Iteration 109/1000 | Loss: 0.00001313
Iteration 110/1000 | Loss: 0.00003518
Iteration 111/1000 | Loss: 0.00002549
Iteration 112/1000 | Loss: 0.00001312
Iteration 113/1000 | Loss: 0.00001312
Iteration 114/1000 | Loss: 0.00001312
Iteration 115/1000 | Loss: 0.00001312
Iteration 116/1000 | Loss: 0.00001312
Iteration 117/1000 | Loss: 0.00001312
Iteration 118/1000 | Loss: 0.00001312
Iteration 119/1000 | Loss: 0.00001312
Iteration 120/1000 | Loss: 0.00001311
Iteration 121/1000 | Loss: 0.00001311
Iteration 122/1000 | Loss: 0.00002671
Iteration 123/1000 | Loss: 0.00001320
Iteration 124/1000 | Loss: 0.00001313
Iteration 125/1000 | Loss: 0.00001312
Iteration 126/1000 | Loss: 0.00001311
Iteration 127/1000 | Loss: 0.00001311
Iteration 128/1000 | Loss: 0.00001311
Iteration 129/1000 | Loss: 0.00001311
Iteration 130/1000 | Loss: 0.00001311
Iteration 131/1000 | Loss: 0.00001310
Iteration 132/1000 | Loss: 0.00001310
Iteration 133/1000 | Loss: 0.00001310
Iteration 134/1000 | Loss: 0.00001310
Iteration 135/1000 | Loss: 0.00001310
Iteration 136/1000 | Loss: 0.00001309
Iteration 137/1000 | Loss: 0.00001309
Iteration 138/1000 | Loss: 0.00001309
Iteration 139/1000 | Loss: 0.00001309
Iteration 140/1000 | Loss: 0.00001309
Iteration 141/1000 | Loss: 0.00001309
Iteration 142/1000 | Loss: 0.00001309
Iteration 143/1000 | Loss: 0.00001309
Iteration 144/1000 | Loss: 0.00001309
Iteration 145/1000 | Loss: 0.00001309
Iteration 146/1000 | Loss: 0.00001309
Iteration 147/1000 | Loss: 0.00001309
Iteration 148/1000 | Loss: 0.00001309
Iteration 149/1000 | Loss: 0.00001309
Iteration 150/1000 | Loss: 0.00001309
Iteration 151/1000 | Loss: 0.00001309
Iteration 152/1000 | Loss: 0.00001309
Iteration 153/1000 | Loss: 0.00001309
Iteration 154/1000 | Loss: 0.00001309
Iteration 155/1000 | Loss: 0.00001309
Iteration 156/1000 | Loss: 0.00001309
Iteration 157/1000 | Loss: 0.00001309
Iteration 158/1000 | Loss: 0.00001309
Iteration 159/1000 | Loss: 0.00001309
Iteration 160/1000 | Loss: 0.00001309
Iteration 161/1000 | Loss: 0.00001309
Iteration 162/1000 | Loss: 0.00001309
Iteration 163/1000 | Loss: 0.00001309
Iteration 164/1000 | Loss: 0.00001309
Iteration 165/1000 | Loss: 0.00001309
Iteration 166/1000 | Loss: 0.00001309
Iteration 167/1000 | Loss: 0.00001309
Iteration 168/1000 | Loss: 0.00001309
Iteration 169/1000 | Loss: 0.00001309
Iteration 170/1000 | Loss: 0.00001309
Iteration 171/1000 | Loss: 0.00001309
Iteration 172/1000 | Loss: 0.00001309
Iteration 173/1000 | Loss: 0.00001309
Iteration 174/1000 | Loss: 0.00001309
Iteration 175/1000 | Loss: 0.00001309
Iteration 176/1000 | Loss: 0.00001309
Iteration 177/1000 | Loss: 0.00001309
Iteration 178/1000 | Loss: 0.00001309
Iteration 179/1000 | Loss: 0.00001309
Iteration 180/1000 | Loss: 0.00001309
Iteration 181/1000 | Loss: 0.00001309
Iteration 182/1000 | Loss: 0.00001309
Iteration 183/1000 | Loss: 0.00001309
Iteration 184/1000 | Loss: 0.00001309
Iteration 185/1000 | Loss: 0.00001309
Iteration 186/1000 | Loss: 0.00001309
Iteration 187/1000 | Loss: 0.00001309
Iteration 188/1000 | Loss: 0.00001309
Iteration 189/1000 | Loss: 0.00001309
Iteration 190/1000 | Loss: 0.00001309
Iteration 191/1000 | Loss: 0.00001309
Iteration 192/1000 | Loss: 0.00001309
Iteration 193/1000 | Loss: 0.00001309
Iteration 194/1000 | Loss: 0.00001309
Iteration 195/1000 | Loss: 0.00001309
Iteration 196/1000 | Loss: 0.00001309
Iteration 197/1000 | Loss: 0.00001309
Iteration 198/1000 | Loss: 0.00001309
Iteration 199/1000 | Loss: 0.00001309
Iteration 200/1000 | Loss: 0.00001309
Iteration 201/1000 | Loss: 0.00001309
Iteration 202/1000 | Loss: 0.00001309
Iteration 203/1000 | Loss: 0.00001309
Iteration 204/1000 | Loss: 0.00001309
Iteration 205/1000 | Loss: 0.00001309
Iteration 206/1000 | Loss: 0.00001309
Iteration 207/1000 | Loss: 0.00001309
Iteration 208/1000 | Loss: 0.00001309
Iteration 209/1000 | Loss: 0.00001309
Iteration 210/1000 | Loss: 0.00001309
Iteration 211/1000 | Loss: 0.00001309
Iteration 212/1000 | Loss: 0.00001309
Iteration 213/1000 | Loss: 0.00001309
Iteration 214/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.3085960745229386e-05, 1.3085960745229386e-05, 1.3085960745229386e-05, 1.3085960745229386e-05, 1.3085960745229386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3085960745229386e-05

Optimization complete. Final v2v error: 3.034245491027832 mm

Highest mean error: 4.170266628265381 mm for frame 78

Lowest mean error: 2.8235883712768555 mm for frame 159

Saving results

Total time: 102.26020622253418
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957289
Iteration 2/25 | Loss: 0.00332186
Iteration 3/25 | Loss: 0.00239766
Iteration 4/25 | Loss: 0.00209202
Iteration 5/25 | Loss: 0.00208353
Iteration 6/25 | Loss: 0.00187744
Iteration 7/25 | Loss: 0.00193641
Iteration 8/25 | Loss: 0.00246391
Iteration 9/25 | Loss: 0.00197927
Iteration 10/25 | Loss: 0.00146530
Iteration 11/25 | Loss: 0.00139193
Iteration 12/25 | Loss: 0.00134980
Iteration 13/25 | Loss: 0.00134597
Iteration 14/25 | Loss: 0.00134189
Iteration 15/25 | Loss: 0.00134204
Iteration 16/25 | Loss: 0.00133635
Iteration 17/25 | Loss: 0.00132861
Iteration 18/25 | Loss: 0.00132754
Iteration 19/25 | Loss: 0.00132731
Iteration 20/25 | Loss: 0.00132726
Iteration 21/25 | Loss: 0.00132724
Iteration 22/25 | Loss: 0.00132723
Iteration 23/25 | Loss: 0.00132723
Iteration 24/25 | Loss: 0.00132723
Iteration 25/25 | Loss: 0.00132722

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.13492107
Iteration 2/25 | Loss: 0.00107893
Iteration 3/25 | Loss: 0.00107893
Iteration 4/25 | Loss: 0.00068487
Iteration 5/25 | Loss: 0.00068486
Iteration 6/25 | Loss: 0.00068486
Iteration 7/25 | Loss: 0.00068486
Iteration 8/25 | Loss: 0.00068486
Iteration 9/25 | Loss: 0.00068486
Iteration 10/25 | Loss: 0.00068486
Iteration 11/25 | Loss: 0.00068486
Iteration 12/25 | Loss: 0.00068486
Iteration 13/25 | Loss: 0.00068486
Iteration 14/25 | Loss: 0.00068486
Iteration 15/25 | Loss: 0.00068486
Iteration 16/25 | Loss: 0.00068486
Iteration 17/25 | Loss: 0.00068486
Iteration 18/25 | Loss: 0.00068486
Iteration 19/25 | Loss: 0.00068486
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006848585908301175, 0.0006848585908301175, 0.0006848585908301175, 0.0006848585908301175, 0.0006848585908301175]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006848585908301175

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068486
Iteration 2/1000 | Loss: 0.00057357
Iteration 3/1000 | Loss: 0.00004647
Iteration 4/1000 | Loss: 0.00003945
Iteration 5/1000 | Loss: 0.00003702
Iteration 6/1000 | Loss: 0.00003527
Iteration 7/1000 | Loss: 0.00033666
Iteration 8/1000 | Loss: 0.00062270
Iteration 9/1000 | Loss: 0.00268477
Iteration 10/1000 | Loss: 0.00009882
Iteration 11/1000 | Loss: 0.00006784
Iteration 12/1000 | Loss: 0.00050342
Iteration 13/1000 | Loss: 0.00003178
Iteration 14/1000 | Loss: 0.00002795
Iteration 15/1000 | Loss: 0.00002633
Iteration 16/1000 | Loss: 0.00002476
Iteration 17/1000 | Loss: 0.00002407
Iteration 18/1000 | Loss: 0.00002335
Iteration 19/1000 | Loss: 0.00002290
Iteration 20/1000 | Loss: 0.00002248
Iteration 21/1000 | Loss: 0.00002226
Iteration 22/1000 | Loss: 0.00002198
Iteration 23/1000 | Loss: 0.00002192
Iteration 24/1000 | Loss: 0.00002190
Iteration 25/1000 | Loss: 0.00002184
Iteration 26/1000 | Loss: 0.00002182
Iteration 27/1000 | Loss: 0.00002182
Iteration 28/1000 | Loss: 0.00002182
Iteration 29/1000 | Loss: 0.00002181
Iteration 30/1000 | Loss: 0.00002180
Iteration 31/1000 | Loss: 0.00002179
Iteration 32/1000 | Loss: 0.00002172
Iteration 33/1000 | Loss: 0.00002168
Iteration 34/1000 | Loss: 0.00002167
Iteration 35/1000 | Loss: 0.00002166
Iteration 36/1000 | Loss: 0.00002166
Iteration 37/1000 | Loss: 0.00002166
Iteration 38/1000 | Loss: 0.00002165
Iteration 39/1000 | Loss: 0.00002164
Iteration 40/1000 | Loss: 0.00002163
Iteration 41/1000 | Loss: 0.00002163
Iteration 42/1000 | Loss: 0.00002162
Iteration 43/1000 | Loss: 0.00002162
Iteration 44/1000 | Loss: 0.00002161
Iteration 45/1000 | Loss: 0.00002161
Iteration 46/1000 | Loss: 0.00002160
Iteration 47/1000 | Loss: 0.00002159
Iteration 48/1000 | Loss: 0.00002159
Iteration 49/1000 | Loss: 0.00002159
Iteration 50/1000 | Loss: 0.00002159
Iteration 51/1000 | Loss: 0.00002158
Iteration 52/1000 | Loss: 0.00002158
Iteration 53/1000 | Loss: 0.00002158
Iteration 54/1000 | Loss: 0.00002158
Iteration 55/1000 | Loss: 0.00002157
Iteration 56/1000 | Loss: 0.00002157
Iteration 57/1000 | Loss: 0.00002157
Iteration 58/1000 | Loss: 0.00002157
Iteration 59/1000 | Loss: 0.00002157
Iteration 60/1000 | Loss: 0.00002157
Iteration 61/1000 | Loss: 0.00002156
Iteration 62/1000 | Loss: 0.00002156
Iteration 63/1000 | Loss: 0.00002156
Iteration 64/1000 | Loss: 0.00002156
Iteration 65/1000 | Loss: 0.00002156
Iteration 66/1000 | Loss: 0.00002156
Iteration 67/1000 | Loss: 0.00002156
Iteration 68/1000 | Loss: 0.00002156
Iteration 69/1000 | Loss: 0.00002156
Iteration 70/1000 | Loss: 0.00002155
Iteration 71/1000 | Loss: 0.00002155
Iteration 72/1000 | Loss: 0.00002155
Iteration 73/1000 | Loss: 0.00002155
Iteration 74/1000 | Loss: 0.00002154
Iteration 75/1000 | Loss: 0.00002154
Iteration 76/1000 | Loss: 0.00002154
Iteration 77/1000 | Loss: 0.00002153
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002153
Iteration 80/1000 | Loss: 0.00002153
Iteration 81/1000 | Loss: 0.00002153
Iteration 82/1000 | Loss: 0.00002153
Iteration 83/1000 | Loss: 0.00002153
Iteration 84/1000 | Loss: 0.00002153
Iteration 85/1000 | Loss: 0.00002153
Iteration 86/1000 | Loss: 0.00002153
Iteration 87/1000 | Loss: 0.00002153
Iteration 88/1000 | Loss: 0.00002153
Iteration 89/1000 | Loss: 0.00002153
Iteration 90/1000 | Loss: 0.00002153
Iteration 91/1000 | Loss: 0.00002152
Iteration 92/1000 | Loss: 0.00002152
Iteration 93/1000 | Loss: 0.00002152
Iteration 94/1000 | Loss: 0.00002152
Iteration 95/1000 | Loss: 0.00002152
Iteration 96/1000 | Loss: 0.00002151
Iteration 97/1000 | Loss: 0.00002151
Iteration 98/1000 | Loss: 0.00002151
Iteration 99/1000 | Loss: 0.00002151
Iteration 100/1000 | Loss: 0.00002151
Iteration 101/1000 | Loss: 0.00002151
Iteration 102/1000 | Loss: 0.00002150
Iteration 103/1000 | Loss: 0.00002150
Iteration 104/1000 | Loss: 0.00002150
Iteration 105/1000 | Loss: 0.00002150
Iteration 106/1000 | Loss: 0.00002149
Iteration 107/1000 | Loss: 0.00002149
Iteration 108/1000 | Loss: 0.00002149
Iteration 109/1000 | Loss: 0.00002148
Iteration 110/1000 | Loss: 0.00002148
Iteration 111/1000 | Loss: 0.00002148
Iteration 112/1000 | Loss: 0.00002148
Iteration 113/1000 | Loss: 0.00002147
Iteration 114/1000 | Loss: 0.00002147
Iteration 115/1000 | Loss: 0.00002147
Iteration 116/1000 | Loss: 0.00002147
Iteration 117/1000 | Loss: 0.00002147
Iteration 118/1000 | Loss: 0.00002147
Iteration 119/1000 | Loss: 0.00002147
Iteration 120/1000 | Loss: 0.00002147
Iteration 121/1000 | Loss: 0.00002147
Iteration 122/1000 | Loss: 0.00002147
Iteration 123/1000 | Loss: 0.00002147
Iteration 124/1000 | Loss: 0.00002146
Iteration 125/1000 | Loss: 0.00002146
Iteration 126/1000 | Loss: 0.00002146
Iteration 127/1000 | Loss: 0.00002146
Iteration 128/1000 | Loss: 0.00002146
Iteration 129/1000 | Loss: 0.00002146
Iteration 130/1000 | Loss: 0.00002146
Iteration 131/1000 | Loss: 0.00002146
Iteration 132/1000 | Loss: 0.00002146
Iteration 133/1000 | Loss: 0.00002146
Iteration 134/1000 | Loss: 0.00002146
Iteration 135/1000 | Loss: 0.00002146
Iteration 136/1000 | Loss: 0.00002146
Iteration 137/1000 | Loss: 0.00002146
Iteration 138/1000 | Loss: 0.00002146
Iteration 139/1000 | Loss: 0.00002146
Iteration 140/1000 | Loss: 0.00002146
Iteration 141/1000 | Loss: 0.00002146
Iteration 142/1000 | Loss: 0.00002146
Iteration 143/1000 | Loss: 0.00002146
Iteration 144/1000 | Loss: 0.00002146
Iteration 145/1000 | Loss: 0.00002146
Iteration 146/1000 | Loss: 0.00002146
Iteration 147/1000 | Loss: 0.00002146
Iteration 148/1000 | Loss: 0.00002146
Iteration 149/1000 | Loss: 0.00002146
Iteration 150/1000 | Loss: 0.00002146
Iteration 151/1000 | Loss: 0.00002146
Iteration 152/1000 | Loss: 0.00002146
Iteration 153/1000 | Loss: 0.00002146
Iteration 154/1000 | Loss: 0.00002146
Iteration 155/1000 | Loss: 0.00002146
Iteration 156/1000 | Loss: 0.00002146
Iteration 157/1000 | Loss: 0.00002146
Iteration 158/1000 | Loss: 0.00002146
Iteration 159/1000 | Loss: 0.00002146
Iteration 160/1000 | Loss: 0.00002146
Iteration 161/1000 | Loss: 0.00002146
Iteration 162/1000 | Loss: 0.00002146
Iteration 163/1000 | Loss: 0.00002146
Iteration 164/1000 | Loss: 0.00002146
Iteration 165/1000 | Loss: 0.00002146
Iteration 166/1000 | Loss: 0.00002146
Iteration 167/1000 | Loss: 0.00002146
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [2.145837243006099e-05, 2.145837243006099e-05, 2.145837243006099e-05, 2.145837243006099e-05, 2.145837243006099e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.145837243006099e-05

Optimization complete. Final v2v error: 3.8947741985321045 mm

Highest mean error: 4.38155460357666 mm for frame 58

Lowest mean error: 3.4346272945404053 mm for frame 124

Saving results

Total time: 76.60625338554382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596211
Iteration 2/25 | Loss: 0.00133766
Iteration 3/25 | Loss: 0.00125832
Iteration 4/25 | Loss: 0.00124243
Iteration 5/25 | Loss: 0.00123680
Iteration 6/25 | Loss: 0.00123632
Iteration 7/25 | Loss: 0.00123632
Iteration 8/25 | Loss: 0.00123632
Iteration 9/25 | Loss: 0.00123632
Iteration 10/25 | Loss: 0.00123632
Iteration 11/25 | Loss: 0.00123632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001236319076269865, 0.001236319076269865, 0.001236319076269865, 0.001236319076269865, 0.001236319076269865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001236319076269865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.22544861
Iteration 2/25 | Loss: 0.00075130
Iteration 3/25 | Loss: 0.00075130
Iteration 4/25 | Loss: 0.00075130
Iteration 5/25 | Loss: 0.00075130
Iteration 6/25 | Loss: 0.00075130
Iteration 7/25 | Loss: 0.00075130
Iteration 8/25 | Loss: 0.00075129
Iteration 9/25 | Loss: 0.00075129
Iteration 10/25 | Loss: 0.00075129
Iteration 11/25 | Loss: 0.00075129
Iteration 12/25 | Loss: 0.00075129
Iteration 13/25 | Loss: 0.00075129
Iteration 14/25 | Loss: 0.00075129
Iteration 15/25 | Loss: 0.00075129
Iteration 16/25 | Loss: 0.00075129
Iteration 17/25 | Loss: 0.00075129
Iteration 18/25 | Loss: 0.00075129
Iteration 19/25 | Loss: 0.00075129
Iteration 20/25 | Loss: 0.00075129
Iteration 21/25 | Loss: 0.00075129
Iteration 22/25 | Loss: 0.00075129
Iteration 23/25 | Loss: 0.00075129
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007512939628213644, 0.0007512939628213644, 0.0007512939628213644, 0.0007512939628213644, 0.0007512939628213644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007512939628213644

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075129
Iteration 2/1000 | Loss: 0.00003522
Iteration 3/1000 | Loss: 0.00002565
Iteration 4/1000 | Loss: 0.00002314
Iteration 5/1000 | Loss: 0.00002195
Iteration 6/1000 | Loss: 0.00002118
Iteration 7/1000 | Loss: 0.00002086
Iteration 8/1000 | Loss: 0.00002054
Iteration 9/1000 | Loss: 0.00002022
Iteration 10/1000 | Loss: 0.00001989
Iteration 11/1000 | Loss: 0.00001984
Iteration 12/1000 | Loss: 0.00001971
Iteration 13/1000 | Loss: 0.00001952
Iteration 14/1000 | Loss: 0.00001950
Iteration 15/1000 | Loss: 0.00001946
Iteration 16/1000 | Loss: 0.00001945
Iteration 17/1000 | Loss: 0.00001944
Iteration 18/1000 | Loss: 0.00001944
Iteration 19/1000 | Loss: 0.00001943
Iteration 20/1000 | Loss: 0.00001942
Iteration 21/1000 | Loss: 0.00001936
Iteration 22/1000 | Loss: 0.00001935
Iteration 23/1000 | Loss: 0.00001934
Iteration 24/1000 | Loss: 0.00001933
Iteration 25/1000 | Loss: 0.00001925
Iteration 26/1000 | Loss: 0.00001918
Iteration 27/1000 | Loss: 0.00001916
Iteration 28/1000 | Loss: 0.00001915
Iteration 29/1000 | Loss: 0.00001913
Iteration 30/1000 | Loss: 0.00001913
Iteration 31/1000 | Loss: 0.00001913
Iteration 32/1000 | Loss: 0.00001913
Iteration 33/1000 | Loss: 0.00001912
Iteration 34/1000 | Loss: 0.00001912
Iteration 35/1000 | Loss: 0.00001912
Iteration 36/1000 | Loss: 0.00001909
Iteration 37/1000 | Loss: 0.00001906
Iteration 38/1000 | Loss: 0.00001906
Iteration 39/1000 | Loss: 0.00001902
Iteration 40/1000 | Loss: 0.00001901
Iteration 41/1000 | Loss: 0.00001898
Iteration 42/1000 | Loss: 0.00001897
Iteration 43/1000 | Loss: 0.00001896
Iteration 44/1000 | Loss: 0.00001896
Iteration 45/1000 | Loss: 0.00001895
Iteration 46/1000 | Loss: 0.00001895
Iteration 47/1000 | Loss: 0.00001895
Iteration 48/1000 | Loss: 0.00001895
Iteration 49/1000 | Loss: 0.00001895
Iteration 50/1000 | Loss: 0.00001894
Iteration 51/1000 | Loss: 0.00001894
Iteration 52/1000 | Loss: 0.00001894
Iteration 53/1000 | Loss: 0.00001894
Iteration 54/1000 | Loss: 0.00001893
Iteration 55/1000 | Loss: 0.00001893
Iteration 56/1000 | Loss: 0.00001892
Iteration 57/1000 | Loss: 0.00001892
Iteration 58/1000 | Loss: 0.00001892
Iteration 59/1000 | Loss: 0.00001891
Iteration 60/1000 | Loss: 0.00001891
Iteration 61/1000 | Loss: 0.00001890
Iteration 62/1000 | Loss: 0.00001890
Iteration 63/1000 | Loss: 0.00001890
Iteration 64/1000 | Loss: 0.00001890
Iteration 65/1000 | Loss: 0.00001889
Iteration 66/1000 | Loss: 0.00001889
Iteration 67/1000 | Loss: 0.00001889
Iteration 68/1000 | Loss: 0.00001889
Iteration 69/1000 | Loss: 0.00001889
Iteration 70/1000 | Loss: 0.00001889
Iteration 71/1000 | Loss: 0.00001889
Iteration 72/1000 | Loss: 0.00001888
Iteration 73/1000 | Loss: 0.00001888
Iteration 74/1000 | Loss: 0.00001888
Iteration 75/1000 | Loss: 0.00001887
Iteration 76/1000 | Loss: 0.00001887
Iteration 77/1000 | Loss: 0.00001886
Iteration 78/1000 | Loss: 0.00001886
Iteration 79/1000 | Loss: 0.00001886
Iteration 80/1000 | Loss: 0.00001885
Iteration 81/1000 | Loss: 0.00001885
Iteration 82/1000 | Loss: 0.00001885
Iteration 83/1000 | Loss: 0.00001884
Iteration 84/1000 | Loss: 0.00001884
Iteration 85/1000 | Loss: 0.00001884
Iteration 86/1000 | Loss: 0.00001883
Iteration 87/1000 | Loss: 0.00001883
Iteration 88/1000 | Loss: 0.00001883
Iteration 89/1000 | Loss: 0.00001883
Iteration 90/1000 | Loss: 0.00001882
Iteration 91/1000 | Loss: 0.00001882
Iteration 92/1000 | Loss: 0.00001882
Iteration 93/1000 | Loss: 0.00001881
Iteration 94/1000 | Loss: 0.00001881
Iteration 95/1000 | Loss: 0.00001881
Iteration 96/1000 | Loss: 0.00001881
Iteration 97/1000 | Loss: 0.00001881
Iteration 98/1000 | Loss: 0.00001881
Iteration 99/1000 | Loss: 0.00001881
Iteration 100/1000 | Loss: 0.00001881
Iteration 101/1000 | Loss: 0.00001881
Iteration 102/1000 | Loss: 0.00001881
Iteration 103/1000 | Loss: 0.00001881
Iteration 104/1000 | Loss: 0.00001881
Iteration 105/1000 | Loss: 0.00001881
Iteration 106/1000 | Loss: 0.00001881
Iteration 107/1000 | Loss: 0.00001880
Iteration 108/1000 | Loss: 0.00001880
Iteration 109/1000 | Loss: 0.00001880
Iteration 110/1000 | Loss: 0.00001880
Iteration 111/1000 | Loss: 0.00001880
Iteration 112/1000 | Loss: 0.00001880
Iteration 113/1000 | Loss: 0.00001880
Iteration 114/1000 | Loss: 0.00001880
Iteration 115/1000 | Loss: 0.00001880
Iteration 116/1000 | Loss: 0.00001880
Iteration 117/1000 | Loss: 0.00001880
Iteration 118/1000 | Loss: 0.00001879
Iteration 119/1000 | Loss: 0.00001879
Iteration 120/1000 | Loss: 0.00001879
Iteration 121/1000 | Loss: 0.00001879
Iteration 122/1000 | Loss: 0.00001879
Iteration 123/1000 | Loss: 0.00001879
Iteration 124/1000 | Loss: 0.00001879
Iteration 125/1000 | Loss: 0.00001879
Iteration 126/1000 | Loss: 0.00001879
Iteration 127/1000 | Loss: 0.00001879
Iteration 128/1000 | Loss: 0.00001879
Iteration 129/1000 | Loss: 0.00001879
Iteration 130/1000 | Loss: 0.00001879
Iteration 131/1000 | Loss: 0.00001879
Iteration 132/1000 | Loss: 0.00001879
Iteration 133/1000 | Loss: 0.00001879
Iteration 134/1000 | Loss: 0.00001879
Iteration 135/1000 | Loss: 0.00001879
Iteration 136/1000 | Loss: 0.00001879
Iteration 137/1000 | Loss: 0.00001879
Iteration 138/1000 | Loss: 0.00001879
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.8788728993968107e-05, 1.8788728993968107e-05, 1.8788728993968107e-05, 1.8788728993968107e-05, 1.8788728993968107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8788728993968107e-05

Optimization complete. Final v2v error: 3.571547031402588 mm

Highest mean error: 4.357447624206543 mm for frame 127

Lowest mean error: 3.0102012157440186 mm for frame 2

Saving results

Total time: 40.05983543395996
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902583
Iteration 2/25 | Loss: 0.00180986
Iteration 3/25 | Loss: 0.00149552
Iteration 4/25 | Loss: 0.00147238
Iteration 5/25 | Loss: 0.00142341
Iteration 6/25 | Loss: 0.00142420
Iteration 7/25 | Loss: 0.00142653
Iteration 8/25 | Loss: 0.00141643
Iteration 9/25 | Loss: 0.00141925
Iteration 10/25 | Loss: 0.00141736
Iteration 11/25 | Loss: 0.00140344
Iteration 12/25 | Loss: 0.00140781
Iteration 13/25 | Loss: 0.00141055
Iteration 14/25 | Loss: 0.00141408
Iteration 15/25 | Loss: 0.00139955
Iteration 16/25 | Loss: 0.00139178
Iteration 17/25 | Loss: 0.00139308
Iteration 18/25 | Loss: 0.00138732
Iteration 19/25 | Loss: 0.00138022
Iteration 20/25 | Loss: 0.00138035
Iteration 21/25 | Loss: 0.00138108
Iteration 22/25 | Loss: 0.00138220
Iteration 23/25 | Loss: 0.00137876
Iteration 24/25 | Loss: 0.00137862
Iteration 25/25 | Loss: 0.00138973

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37367654
Iteration 2/25 | Loss: 0.00112716
Iteration 3/25 | Loss: 0.00112693
Iteration 4/25 | Loss: 0.00112693
Iteration 5/25 | Loss: 0.00112693
Iteration 6/25 | Loss: 0.00112693
Iteration 7/25 | Loss: 0.00112693
Iteration 8/25 | Loss: 0.00112693
Iteration 9/25 | Loss: 0.00112693
Iteration 10/25 | Loss: 0.00112693
Iteration 11/25 | Loss: 0.00112693
Iteration 12/25 | Loss: 0.00112693
Iteration 13/25 | Loss: 0.00112693
Iteration 14/25 | Loss: 0.00112693
Iteration 15/25 | Loss: 0.00112693
Iteration 16/25 | Loss: 0.00112693
Iteration 17/25 | Loss: 0.00112693
Iteration 18/25 | Loss: 0.00112693
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011269306996837258, 0.0011269306996837258, 0.0011269306996837258, 0.0011269306996837258, 0.0011269306996837258]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011269306996837258

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112693
Iteration 2/1000 | Loss: 0.00687623
Iteration 3/1000 | Loss: 0.00013216
Iteration 4/1000 | Loss: 0.00279089
Iteration 5/1000 | Loss: 0.00256230
Iteration 6/1000 | Loss: 0.00056523
Iteration 7/1000 | Loss: 0.00042087
Iteration 8/1000 | Loss: 0.00031656
Iteration 9/1000 | Loss: 0.00036313
Iteration 10/1000 | Loss: 0.00022686
Iteration 11/1000 | Loss: 0.00007367
Iteration 12/1000 | Loss: 0.00004449
Iteration 13/1000 | Loss: 0.00004088
Iteration 14/1000 | Loss: 0.00023044
Iteration 15/1000 | Loss: 0.00005030
Iteration 16/1000 | Loss: 0.00004255
Iteration 17/1000 | Loss: 0.00003949
Iteration 18/1000 | Loss: 0.00003808
Iteration 19/1000 | Loss: 0.00006378
Iteration 20/1000 | Loss: 0.00003528
Iteration 21/1000 | Loss: 0.00003383
Iteration 22/1000 | Loss: 0.00008388
Iteration 23/1000 | Loss: 0.00008759
Iteration 24/1000 | Loss: 0.00009812
Iteration 25/1000 | Loss: 0.00007868
Iteration 26/1000 | Loss: 0.00004932
Iteration 27/1000 | Loss: 0.00005279
Iteration 28/1000 | Loss: 0.00004820
Iteration 29/1000 | Loss: 0.00035452
Iteration 30/1000 | Loss: 0.00034325
Iteration 31/1000 | Loss: 0.00024828
Iteration 32/1000 | Loss: 0.00030295
Iteration 33/1000 | Loss: 0.00042481
Iteration 34/1000 | Loss: 0.00028454
Iteration 35/1000 | Loss: 0.00060320
Iteration 36/1000 | Loss: 0.00006856
Iteration 37/1000 | Loss: 0.00032629
Iteration 38/1000 | Loss: 0.00013923
Iteration 39/1000 | Loss: 0.00015176
Iteration 40/1000 | Loss: 0.00010049
Iteration 41/1000 | Loss: 0.00006003
Iteration 42/1000 | Loss: 0.00004308
Iteration 43/1000 | Loss: 0.00004789
Iteration 44/1000 | Loss: 0.00006094
Iteration 45/1000 | Loss: 0.00018584
Iteration 46/1000 | Loss: 0.00014762
Iteration 47/1000 | Loss: 0.00010402
Iteration 48/1000 | Loss: 0.00015149
Iteration 49/1000 | Loss: 0.00013397
Iteration 50/1000 | Loss: 0.00007189
Iteration 51/1000 | Loss: 0.00017082
Iteration 52/1000 | Loss: 0.00021641
Iteration 53/1000 | Loss: 0.00025754
Iteration 54/1000 | Loss: 0.00124009
Iteration 55/1000 | Loss: 0.00085574
Iteration 56/1000 | Loss: 0.00016245
Iteration 57/1000 | Loss: 0.00089565
Iteration 58/1000 | Loss: 0.00083422
Iteration 59/1000 | Loss: 0.00041207
Iteration 60/1000 | Loss: 0.00018721
Iteration 61/1000 | Loss: 0.00057371
Iteration 62/1000 | Loss: 0.00028289
Iteration 63/1000 | Loss: 0.00077599
Iteration 64/1000 | Loss: 0.00064477
Iteration 65/1000 | Loss: 0.00054665
Iteration 66/1000 | Loss: 0.00089799
Iteration 67/1000 | Loss: 0.00131016
Iteration 68/1000 | Loss: 0.00096069
Iteration 69/1000 | Loss: 0.00005896
Iteration 70/1000 | Loss: 0.00004584
Iteration 71/1000 | Loss: 0.00003837
Iteration 72/1000 | Loss: 0.00003537
Iteration 73/1000 | Loss: 0.00003310
Iteration 74/1000 | Loss: 0.00039975
Iteration 75/1000 | Loss: 0.00003854
Iteration 76/1000 | Loss: 0.00003075
Iteration 77/1000 | Loss: 0.00002965
Iteration 78/1000 | Loss: 0.00002895
Iteration 79/1000 | Loss: 0.00002833
Iteration 80/1000 | Loss: 0.00002798
Iteration 81/1000 | Loss: 0.00002777
Iteration 82/1000 | Loss: 0.00032415
Iteration 83/1000 | Loss: 0.00004943
Iteration 84/1000 | Loss: 0.00003128
Iteration 85/1000 | Loss: 0.00002864
Iteration 86/1000 | Loss: 0.00002787
Iteration 87/1000 | Loss: 0.00002742
Iteration 88/1000 | Loss: 0.00002711
Iteration 89/1000 | Loss: 0.00002708
Iteration 90/1000 | Loss: 0.00002690
Iteration 91/1000 | Loss: 0.00002675
Iteration 92/1000 | Loss: 0.00002675
Iteration 93/1000 | Loss: 0.00002672
Iteration 94/1000 | Loss: 0.00002669
Iteration 95/1000 | Loss: 0.00002666
Iteration 96/1000 | Loss: 0.00002665
Iteration 97/1000 | Loss: 0.00002665
Iteration 98/1000 | Loss: 0.00002664
Iteration 99/1000 | Loss: 0.00002664
Iteration 100/1000 | Loss: 0.00002664
Iteration 101/1000 | Loss: 0.00002664
Iteration 102/1000 | Loss: 0.00002664
Iteration 103/1000 | Loss: 0.00002664
Iteration 104/1000 | Loss: 0.00002664
Iteration 105/1000 | Loss: 0.00002663
Iteration 106/1000 | Loss: 0.00002663
Iteration 107/1000 | Loss: 0.00002663
Iteration 108/1000 | Loss: 0.00002663
Iteration 109/1000 | Loss: 0.00002663
Iteration 110/1000 | Loss: 0.00002663
Iteration 111/1000 | Loss: 0.00002663
Iteration 112/1000 | Loss: 0.00002663
Iteration 113/1000 | Loss: 0.00002662
Iteration 114/1000 | Loss: 0.00002662
Iteration 115/1000 | Loss: 0.00002662
Iteration 116/1000 | Loss: 0.00002662
Iteration 117/1000 | Loss: 0.00002662
Iteration 118/1000 | Loss: 0.00002662
Iteration 119/1000 | Loss: 0.00002662
Iteration 120/1000 | Loss: 0.00002661
Iteration 121/1000 | Loss: 0.00002661
Iteration 122/1000 | Loss: 0.00002661
Iteration 123/1000 | Loss: 0.00002661
Iteration 124/1000 | Loss: 0.00002661
Iteration 125/1000 | Loss: 0.00002661
Iteration 126/1000 | Loss: 0.00002661
Iteration 127/1000 | Loss: 0.00002661
Iteration 128/1000 | Loss: 0.00002661
Iteration 129/1000 | Loss: 0.00002661
Iteration 130/1000 | Loss: 0.00002661
Iteration 131/1000 | Loss: 0.00002661
Iteration 132/1000 | Loss: 0.00002660
Iteration 133/1000 | Loss: 0.00002660
Iteration 134/1000 | Loss: 0.00002659
Iteration 135/1000 | Loss: 0.00002659
Iteration 136/1000 | Loss: 0.00002659
Iteration 137/1000 | Loss: 0.00002658
Iteration 138/1000 | Loss: 0.00002658
Iteration 139/1000 | Loss: 0.00002658
Iteration 140/1000 | Loss: 0.00002657
Iteration 141/1000 | Loss: 0.00002657
Iteration 142/1000 | Loss: 0.00002657
Iteration 143/1000 | Loss: 0.00002657
Iteration 144/1000 | Loss: 0.00002656
Iteration 145/1000 | Loss: 0.00002655
Iteration 146/1000 | Loss: 0.00002655
Iteration 147/1000 | Loss: 0.00002654
Iteration 148/1000 | Loss: 0.00002654
Iteration 149/1000 | Loss: 0.00002653
Iteration 150/1000 | Loss: 0.00002653
Iteration 151/1000 | Loss: 0.00002652
Iteration 152/1000 | Loss: 0.00002652
Iteration 153/1000 | Loss: 0.00002652
Iteration 154/1000 | Loss: 0.00002651
Iteration 155/1000 | Loss: 0.00002651
Iteration 156/1000 | Loss: 0.00002651
Iteration 157/1000 | Loss: 0.00002651
Iteration 158/1000 | Loss: 0.00002651
Iteration 159/1000 | Loss: 0.00002650
Iteration 160/1000 | Loss: 0.00002650
Iteration 161/1000 | Loss: 0.00002650
Iteration 162/1000 | Loss: 0.00002650
Iteration 163/1000 | Loss: 0.00002650
Iteration 164/1000 | Loss: 0.00002650
Iteration 165/1000 | Loss: 0.00002650
Iteration 166/1000 | Loss: 0.00002650
Iteration 167/1000 | Loss: 0.00002649
Iteration 168/1000 | Loss: 0.00002649
Iteration 169/1000 | Loss: 0.00002649
Iteration 170/1000 | Loss: 0.00002649
Iteration 171/1000 | Loss: 0.00002649
Iteration 172/1000 | Loss: 0.00002649
Iteration 173/1000 | Loss: 0.00002649
Iteration 174/1000 | Loss: 0.00002649
Iteration 175/1000 | Loss: 0.00002648
Iteration 176/1000 | Loss: 0.00002648
Iteration 177/1000 | Loss: 0.00002648
Iteration 178/1000 | Loss: 0.00002648
Iteration 179/1000 | Loss: 0.00002648
Iteration 180/1000 | Loss: 0.00002648
Iteration 181/1000 | Loss: 0.00002648
Iteration 182/1000 | Loss: 0.00002648
Iteration 183/1000 | Loss: 0.00002648
Iteration 184/1000 | Loss: 0.00002648
Iteration 185/1000 | Loss: 0.00002647
Iteration 186/1000 | Loss: 0.00002647
Iteration 187/1000 | Loss: 0.00002647
Iteration 188/1000 | Loss: 0.00002647
Iteration 189/1000 | Loss: 0.00002647
Iteration 190/1000 | Loss: 0.00002647
Iteration 191/1000 | Loss: 0.00002647
Iteration 192/1000 | Loss: 0.00002647
Iteration 193/1000 | Loss: 0.00002647
Iteration 194/1000 | Loss: 0.00002647
Iteration 195/1000 | Loss: 0.00002646
Iteration 196/1000 | Loss: 0.00002646
Iteration 197/1000 | Loss: 0.00002646
Iteration 198/1000 | Loss: 0.00002646
Iteration 199/1000 | Loss: 0.00002646
Iteration 200/1000 | Loss: 0.00002646
Iteration 201/1000 | Loss: 0.00002646
Iteration 202/1000 | Loss: 0.00002646
Iteration 203/1000 | Loss: 0.00002646
Iteration 204/1000 | Loss: 0.00002646
Iteration 205/1000 | Loss: 0.00002646
Iteration 206/1000 | Loss: 0.00002645
Iteration 207/1000 | Loss: 0.00002645
Iteration 208/1000 | Loss: 0.00002645
Iteration 209/1000 | Loss: 0.00002645
Iteration 210/1000 | Loss: 0.00002645
Iteration 211/1000 | Loss: 0.00002645
Iteration 212/1000 | Loss: 0.00002645
Iteration 213/1000 | Loss: 0.00002645
Iteration 214/1000 | Loss: 0.00002645
Iteration 215/1000 | Loss: 0.00002645
Iteration 216/1000 | Loss: 0.00002645
Iteration 217/1000 | Loss: 0.00002645
Iteration 218/1000 | Loss: 0.00002645
Iteration 219/1000 | Loss: 0.00002645
Iteration 220/1000 | Loss: 0.00002645
Iteration 221/1000 | Loss: 0.00002645
Iteration 222/1000 | Loss: 0.00002645
Iteration 223/1000 | Loss: 0.00002645
Iteration 224/1000 | Loss: 0.00002645
Iteration 225/1000 | Loss: 0.00002645
Iteration 226/1000 | Loss: 0.00002645
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.644609776325524e-05, 2.644609776325524e-05, 2.644609776325524e-05, 2.644609776325524e-05, 2.644609776325524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.644609776325524e-05

Optimization complete. Final v2v error: 4.258845329284668 mm

Highest mean error: 6.029605388641357 mm for frame 98

Lowest mean error: 3.297074317932129 mm for frame 144

Saving results

Total time: 181.96935725212097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402470
Iteration 2/25 | Loss: 0.00127994
Iteration 3/25 | Loss: 0.00122581
Iteration 4/25 | Loss: 0.00121709
Iteration 5/25 | Loss: 0.00121368
Iteration 6/25 | Loss: 0.00121347
Iteration 7/25 | Loss: 0.00121347
Iteration 8/25 | Loss: 0.00121347
Iteration 9/25 | Loss: 0.00121347
Iteration 10/25 | Loss: 0.00121347
Iteration 11/25 | Loss: 0.00121347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012134724529460073, 0.0012134724529460073, 0.0012134724529460073, 0.0012134724529460073, 0.0012134724529460073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012134724529460073

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43988168
Iteration 2/25 | Loss: 0.00080080
Iteration 3/25 | Loss: 0.00080080
Iteration 4/25 | Loss: 0.00080080
Iteration 5/25 | Loss: 0.00080080
Iteration 6/25 | Loss: 0.00080080
Iteration 7/25 | Loss: 0.00080080
Iteration 8/25 | Loss: 0.00080080
Iteration 9/25 | Loss: 0.00080080
Iteration 10/25 | Loss: 0.00080080
Iteration 11/25 | Loss: 0.00080080
Iteration 12/25 | Loss: 0.00080080
Iteration 13/25 | Loss: 0.00080080
Iteration 14/25 | Loss: 0.00080080
Iteration 15/25 | Loss: 0.00080080
Iteration 16/25 | Loss: 0.00080080
Iteration 17/25 | Loss: 0.00080080
Iteration 18/25 | Loss: 0.00080080
Iteration 19/25 | Loss: 0.00080080
Iteration 20/25 | Loss: 0.00080080
Iteration 21/25 | Loss: 0.00080080
Iteration 22/25 | Loss: 0.00080080
Iteration 23/25 | Loss: 0.00080080
Iteration 24/25 | Loss: 0.00080080
Iteration 25/25 | Loss: 0.00080080

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080080
Iteration 2/1000 | Loss: 0.00002305
Iteration 3/1000 | Loss: 0.00001554
Iteration 4/1000 | Loss: 0.00001424
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001319
Iteration 7/1000 | Loss: 0.00001282
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001273
Iteration 10/1000 | Loss: 0.00001270
Iteration 11/1000 | Loss: 0.00001249
Iteration 12/1000 | Loss: 0.00001246
Iteration 13/1000 | Loss: 0.00001246
Iteration 14/1000 | Loss: 0.00001245
Iteration 15/1000 | Loss: 0.00001244
Iteration 16/1000 | Loss: 0.00001236
Iteration 17/1000 | Loss: 0.00001235
Iteration 18/1000 | Loss: 0.00001234
Iteration 19/1000 | Loss: 0.00001223
Iteration 20/1000 | Loss: 0.00001218
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001216
Iteration 23/1000 | Loss: 0.00001215
Iteration 24/1000 | Loss: 0.00001215
Iteration 25/1000 | Loss: 0.00001213
Iteration 26/1000 | Loss: 0.00001213
Iteration 27/1000 | Loss: 0.00001212
Iteration 28/1000 | Loss: 0.00001209
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001203
Iteration 31/1000 | Loss: 0.00001203
Iteration 32/1000 | Loss: 0.00001202
Iteration 33/1000 | Loss: 0.00001202
Iteration 34/1000 | Loss: 0.00001201
Iteration 35/1000 | Loss: 0.00001201
Iteration 36/1000 | Loss: 0.00001201
Iteration 37/1000 | Loss: 0.00001201
Iteration 38/1000 | Loss: 0.00001201
Iteration 39/1000 | Loss: 0.00001201
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001196
Iteration 44/1000 | Loss: 0.00001196
Iteration 45/1000 | Loss: 0.00001196
Iteration 46/1000 | Loss: 0.00001196
Iteration 47/1000 | Loss: 0.00001196
Iteration 48/1000 | Loss: 0.00001196
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001195
Iteration 51/1000 | Loss: 0.00001195
Iteration 52/1000 | Loss: 0.00001195
Iteration 53/1000 | Loss: 0.00001195
Iteration 54/1000 | Loss: 0.00001195
Iteration 55/1000 | Loss: 0.00001195
Iteration 56/1000 | Loss: 0.00001194
Iteration 57/1000 | Loss: 0.00001194
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001192
Iteration 61/1000 | Loss: 0.00001192
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001188
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001186
Iteration 68/1000 | Loss: 0.00001184
Iteration 69/1000 | Loss: 0.00001183
Iteration 70/1000 | Loss: 0.00001182
Iteration 71/1000 | Loss: 0.00001182
Iteration 72/1000 | Loss: 0.00001182
Iteration 73/1000 | Loss: 0.00001181
Iteration 74/1000 | Loss: 0.00001181
Iteration 75/1000 | Loss: 0.00001180
Iteration 76/1000 | Loss: 0.00001179
Iteration 77/1000 | Loss: 0.00001179
Iteration 78/1000 | Loss: 0.00001178
Iteration 79/1000 | Loss: 0.00001178
Iteration 80/1000 | Loss: 0.00001178
Iteration 81/1000 | Loss: 0.00001178
Iteration 82/1000 | Loss: 0.00001178
Iteration 83/1000 | Loss: 0.00001178
Iteration 84/1000 | Loss: 0.00001177
Iteration 85/1000 | Loss: 0.00001177
Iteration 86/1000 | Loss: 0.00001177
Iteration 87/1000 | Loss: 0.00001176
Iteration 88/1000 | Loss: 0.00001176
Iteration 89/1000 | Loss: 0.00001176
Iteration 90/1000 | Loss: 0.00001176
Iteration 91/1000 | Loss: 0.00001175
Iteration 92/1000 | Loss: 0.00001175
Iteration 93/1000 | Loss: 0.00001175
Iteration 94/1000 | Loss: 0.00001174
Iteration 95/1000 | Loss: 0.00001174
Iteration 96/1000 | Loss: 0.00001173
Iteration 97/1000 | Loss: 0.00001173
Iteration 98/1000 | Loss: 0.00001172
Iteration 99/1000 | Loss: 0.00001171
Iteration 100/1000 | Loss: 0.00001171
Iteration 101/1000 | Loss: 0.00001171
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001169
Iteration 108/1000 | Loss: 0.00001169
Iteration 109/1000 | Loss: 0.00001169
Iteration 110/1000 | Loss: 0.00001169
Iteration 111/1000 | Loss: 0.00001169
Iteration 112/1000 | Loss: 0.00001169
Iteration 113/1000 | Loss: 0.00001169
Iteration 114/1000 | Loss: 0.00001169
Iteration 115/1000 | Loss: 0.00001169
Iteration 116/1000 | Loss: 0.00001168
Iteration 117/1000 | Loss: 0.00001168
Iteration 118/1000 | Loss: 0.00001167
Iteration 119/1000 | Loss: 0.00001167
Iteration 120/1000 | Loss: 0.00001166
Iteration 121/1000 | Loss: 0.00001165
Iteration 122/1000 | Loss: 0.00001165
Iteration 123/1000 | Loss: 0.00001165
Iteration 124/1000 | Loss: 0.00001165
Iteration 125/1000 | Loss: 0.00001165
Iteration 126/1000 | Loss: 0.00001165
Iteration 127/1000 | Loss: 0.00001165
Iteration 128/1000 | Loss: 0.00001165
Iteration 129/1000 | Loss: 0.00001165
Iteration 130/1000 | Loss: 0.00001164
Iteration 131/1000 | Loss: 0.00001164
Iteration 132/1000 | Loss: 0.00001164
Iteration 133/1000 | Loss: 0.00001163
Iteration 134/1000 | Loss: 0.00001163
Iteration 135/1000 | Loss: 0.00001163
Iteration 136/1000 | Loss: 0.00001162
Iteration 137/1000 | Loss: 0.00001162
Iteration 138/1000 | Loss: 0.00001162
Iteration 139/1000 | Loss: 0.00001162
Iteration 140/1000 | Loss: 0.00001162
Iteration 141/1000 | Loss: 0.00001162
Iteration 142/1000 | Loss: 0.00001162
Iteration 143/1000 | Loss: 0.00001162
Iteration 144/1000 | Loss: 0.00001162
Iteration 145/1000 | Loss: 0.00001162
Iteration 146/1000 | Loss: 0.00001162
Iteration 147/1000 | Loss: 0.00001162
Iteration 148/1000 | Loss: 0.00001162
Iteration 149/1000 | Loss: 0.00001162
Iteration 150/1000 | Loss: 0.00001162
Iteration 151/1000 | Loss: 0.00001162
Iteration 152/1000 | Loss: 0.00001162
Iteration 153/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.1617133168329019e-05, 1.1617133168329019e-05, 1.1617133168329019e-05, 1.1617133168329019e-05, 1.1617133168329019e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1617133168329019e-05

Optimization complete. Final v2v error: 2.957387924194336 mm

Highest mean error: 3.052116870880127 mm for frame 104

Lowest mean error: 2.9023635387420654 mm for frame 0

Saving results

Total time: 35.825363636016846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476780
Iteration 2/25 | Loss: 0.00129227
Iteration 3/25 | Loss: 0.00121213
Iteration 4/25 | Loss: 0.00120030
Iteration 5/25 | Loss: 0.00119656
Iteration 6/25 | Loss: 0.00119653
Iteration 7/25 | Loss: 0.00119653
Iteration 8/25 | Loss: 0.00119653
Iteration 9/25 | Loss: 0.00119653
Iteration 10/25 | Loss: 0.00119653
Iteration 11/25 | Loss: 0.00119653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011965284356847405, 0.0011965284356847405, 0.0011965284356847405, 0.0011965284356847405, 0.0011965284356847405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011965284356847405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89856672
Iteration 2/25 | Loss: 0.00074464
Iteration 3/25 | Loss: 0.00074464
Iteration 4/25 | Loss: 0.00074464
Iteration 5/25 | Loss: 0.00074464
Iteration 6/25 | Loss: 0.00074464
Iteration 7/25 | Loss: 0.00074464
Iteration 8/25 | Loss: 0.00074464
Iteration 9/25 | Loss: 0.00074463
Iteration 10/25 | Loss: 0.00074463
Iteration 11/25 | Loss: 0.00074463
Iteration 12/25 | Loss: 0.00074463
Iteration 13/25 | Loss: 0.00074463
Iteration 14/25 | Loss: 0.00074463
Iteration 15/25 | Loss: 0.00074463
Iteration 16/25 | Loss: 0.00074463
Iteration 17/25 | Loss: 0.00074463
Iteration 18/25 | Loss: 0.00074463
Iteration 19/25 | Loss: 0.00074463
Iteration 20/25 | Loss: 0.00074463
Iteration 21/25 | Loss: 0.00074463
Iteration 22/25 | Loss: 0.00074463
Iteration 23/25 | Loss: 0.00074463
Iteration 24/25 | Loss: 0.00074463
Iteration 25/25 | Loss: 0.00074463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074463
Iteration 2/1000 | Loss: 0.00002559
Iteration 3/1000 | Loss: 0.00001807
Iteration 4/1000 | Loss: 0.00001612
Iteration 5/1000 | Loss: 0.00001502
Iteration 6/1000 | Loss: 0.00001428
Iteration 7/1000 | Loss: 0.00001387
Iteration 8/1000 | Loss: 0.00001359
Iteration 9/1000 | Loss: 0.00001325
Iteration 10/1000 | Loss: 0.00001312
Iteration 11/1000 | Loss: 0.00001312
Iteration 12/1000 | Loss: 0.00001312
Iteration 13/1000 | Loss: 0.00001305
Iteration 14/1000 | Loss: 0.00001304
Iteration 15/1000 | Loss: 0.00001291
Iteration 16/1000 | Loss: 0.00001282
Iteration 17/1000 | Loss: 0.00001282
Iteration 18/1000 | Loss: 0.00001282
Iteration 19/1000 | Loss: 0.00001278
Iteration 20/1000 | Loss: 0.00001276
Iteration 21/1000 | Loss: 0.00001269
Iteration 22/1000 | Loss: 0.00001267
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001262
Iteration 26/1000 | Loss: 0.00001261
Iteration 27/1000 | Loss: 0.00001260
Iteration 28/1000 | Loss: 0.00001255
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001253
Iteration 32/1000 | Loss: 0.00001252
Iteration 33/1000 | Loss: 0.00001250
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001247
Iteration 37/1000 | Loss: 0.00001247
Iteration 38/1000 | Loss: 0.00001246
Iteration 39/1000 | Loss: 0.00001245
Iteration 40/1000 | Loss: 0.00001239
Iteration 41/1000 | Loss: 0.00001238
Iteration 42/1000 | Loss: 0.00001234
Iteration 43/1000 | Loss: 0.00001234
Iteration 44/1000 | Loss: 0.00001234
Iteration 45/1000 | Loss: 0.00001233
Iteration 46/1000 | Loss: 0.00001233
Iteration 47/1000 | Loss: 0.00001229
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001225
Iteration 54/1000 | Loss: 0.00001225
Iteration 55/1000 | Loss: 0.00001225
Iteration 56/1000 | Loss: 0.00001225
Iteration 57/1000 | Loss: 0.00001225
Iteration 58/1000 | Loss: 0.00001224
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001210
Iteration 66/1000 | Loss: 0.00001210
Iteration 67/1000 | Loss: 0.00001209
Iteration 68/1000 | Loss: 0.00001209
Iteration 69/1000 | Loss: 0.00001209
Iteration 70/1000 | Loss: 0.00001208
Iteration 71/1000 | Loss: 0.00001208
Iteration 72/1000 | Loss: 0.00001207
Iteration 73/1000 | Loss: 0.00001207
Iteration 74/1000 | Loss: 0.00001207
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001204
Iteration 80/1000 | Loss: 0.00001204
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001203
Iteration 83/1000 | Loss: 0.00001203
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001200
Iteration 99/1000 | Loss: 0.00001200
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001198
Iteration 103/1000 | Loss: 0.00001198
Iteration 104/1000 | Loss: 0.00001197
Iteration 105/1000 | Loss: 0.00001197
Iteration 106/1000 | Loss: 0.00001197
Iteration 107/1000 | Loss: 0.00001197
Iteration 108/1000 | Loss: 0.00001196
Iteration 109/1000 | Loss: 0.00001196
Iteration 110/1000 | Loss: 0.00001196
Iteration 111/1000 | Loss: 0.00001195
Iteration 112/1000 | Loss: 0.00001195
Iteration 113/1000 | Loss: 0.00001195
Iteration 114/1000 | Loss: 0.00001195
Iteration 115/1000 | Loss: 0.00001195
Iteration 116/1000 | Loss: 0.00001195
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001194
Iteration 119/1000 | Loss: 0.00001194
Iteration 120/1000 | Loss: 0.00001194
Iteration 121/1000 | Loss: 0.00001194
Iteration 122/1000 | Loss: 0.00001194
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001193
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.192759009427391e-05, 1.192759009427391e-05, 1.192759009427391e-05, 1.192759009427391e-05, 1.192759009427391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.192759009427391e-05

Optimization complete. Final v2v error: 2.9863438606262207 mm

Highest mean error: 3.2225284576416016 mm for frame 110

Lowest mean error: 2.8213443756103516 mm for frame 0

Saving results

Total time: 44.381280183792114
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848291
Iteration 2/25 | Loss: 0.00157608
Iteration 3/25 | Loss: 0.00137775
Iteration 4/25 | Loss: 0.00135735
Iteration 5/25 | Loss: 0.00135261
Iteration 6/25 | Loss: 0.00135163
Iteration 7/25 | Loss: 0.00135163
Iteration 8/25 | Loss: 0.00135163
Iteration 9/25 | Loss: 0.00135163
Iteration 10/25 | Loss: 0.00135163
Iteration 11/25 | Loss: 0.00135163
Iteration 12/25 | Loss: 0.00135163
Iteration 13/25 | Loss: 0.00135163
Iteration 14/25 | Loss: 0.00135163
Iteration 15/25 | Loss: 0.00135163
Iteration 16/25 | Loss: 0.00135163
Iteration 17/25 | Loss: 0.00135163
Iteration 18/25 | Loss: 0.00135163
Iteration 19/25 | Loss: 0.00135163
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013516346225515008, 0.0013516346225515008, 0.0013516346225515008, 0.0013516346225515008, 0.0013516346225515008]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013516346225515008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36530674
Iteration 2/25 | Loss: 0.00088580
Iteration 3/25 | Loss: 0.00088579
Iteration 4/25 | Loss: 0.00088579
Iteration 5/25 | Loss: 0.00088579
Iteration 6/25 | Loss: 0.00088579
Iteration 7/25 | Loss: 0.00088579
Iteration 8/25 | Loss: 0.00088579
Iteration 9/25 | Loss: 0.00088579
Iteration 10/25 | Loss: 0.00088579
Iteration 11/25 | Loss: 0.00088579
Iteration 12/25 | Loss: 0.00088579
Iteration 13/25 | Loss: 0.00088579
Iteration 14/25 | Loss: 0.00088579
Iteration 15/25 | Loss: 0.00088579
Iteration 16/25 | Loss: 0.00088579
Iteration 17/25 | Loss: 0.00088579
Iteration 18/25 | Loss: 0.00088579
Iteration 19/25 | Loss: 0.00088579
Iteration 20/25 | Loss: 0.00088579
Iteration 21/25 | Loss: 0.00088579
Iteration 22/25 | Loss: 0.00088579
Iteration 23/25 | Loss: 0.00088579
Iteration 24/25 | Loss: 0.00088579
Iteration 25/25 | Loss: 0.00088579

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088579
Iteration 2/1000 | Loss: 0.00006878
Iteration 3/1000 | Loss: 0.00004185
Iteration 4/1000 | Loss: 0.00003594
Iteration 5/1000 | Loss: 0.00003372
Iteration 6/1000 | Loss: 0.00003199
Iteration 7/1000 | Loss: 0.00003097
Iteration 8/1000 | Loss: 0.00003001
Iteration 9/1000 | Loss: 0.00002943
Iteration 10/1000 | Loss: 0.00002912
Iteration 11/1000 | Loss: 0.00002885
Iteration 12/1000 | Loss: 0.00002861
Iteration 13/1000 | Loss: 0.00002851
Iteration 14/1000 | Loss: 0.00002841
Iteration 15/1000 | Loss: 0.00002837
Iteration 16/1000 | Loss: 0.00002836
Iteration 17/1000 | Loss: 0.00002822
Iteration 18/1000 | Loss: 0.00002818
Iteration 19/1000 | Loss: 0.00002818
Iteration 20/1000 | Loss: 0.00002815
Iteration 21/1000 | Loss: 0.00002814
Iteration 22/1000 | Loss: 0.00002813
Iteration 23/1000 | Loss: 0.00002813
Iteration 24/1000 | Loss: 0.00002812
Iteration 25/1000 | Loss: 0.00002812
Iteration 26/1000 | Loss: 0.00002810
Iteration 27/1000 | Loss: 0.00002809
Iteration 28/1000 | Loss: 0.00002809
Iteration 29/1000 | Loss: 0.00002807
Iteration 30/1000 | Loss: 0.00002807
Iteration 31/1000 | Loss: 0.00002806
Iteration 32/1000 | Loss: 0.00002805
Iteration 33/1000 | Loss: 0.00002805
Iteration 34/1000 | Loss: 0.00002805
Iteration 35/1000 | Loss: 0.00002805
Iteration 36/1000 | Loss: 0.00002805
Iteration 37/1000 | Loss: 0.00002805
Iteration 38/1000 | Loss: 0.00002805
Iteration 39/1000 | Loss: 0.00002804
Iteration 40/1000 | Loss: 0.00002804
Iteration 41/1000 | Loss: 0.00002804
Iteration 42/1000 | Loss: 0.00002804
Iteration 43/1000 | Loss: 0.00002802
Iteration 44/1000 | Loss: 0.00002802
Iteration 45/1000 | Loss: 0.00002802
Iteration 46/1000 | Loss: 0.00002801
Iteration 47/1000 | Loss: 0.00002801
Iteration 48/1000 | Loss: 0.00002800
Iteration 49/1000 | Loss: 0.00002800
Iteration 50/1000 | Loss: 0.00002800
Iteration 51/1000 | Loss: 0.00002800
Iteration 52/1000 | Loss: 0.00002800
Iteration 53/1000 | Loss: 0.00002800
Iteration 54/1000 | Loss: 0.00002799
Iteration 55/1000 | Loss: 0.00002799
Iteration 56/1000 | Loss: 0.00002799
Iteration 57/1000 | Loss: 0.00002799
Iteration 58/1000 | Loss: 0.00002799
Iteration 59/1000 | Loss: 0.00002799
Iteration 60/1000 | Loss: 0.00002799
Iteration 61/1000 | Loss: 0.00002798
Iteration 62/1000 | Loss: 0.00002798
Iteration 63/1000 | Loss: 0.00002798
Iteration 64/1000 | Loss: 0.00002797
Iteration 65/1000 | Loss: 0.00002797
Iteration 66/1000 | Loss: 0.00002797
Iteration 67/1000 | Loss: 0.00002797
Iteration 68/1000 | Loss: 0.00002797
Iteration 69/1000 | Loss: 0.00002796
Iteration 70/1000 | Loss: 0.00002796
Iteration 71/1000 | Loss: 0.00002796
Iteration 72/1000 | Loss: 0.00002796
Iteration 73/1000 | Loss: 0.00002796
Iteration 74/1000 | Loss: 0.00002796
Iteration 75/1000 | Loss: 0.00002795
Iteration 76/1000 | Loss: 0.00002795
Iteration 77/1000 | Loss: 0.00002795
Iteration 78/1000 | Loss: 0.00002795
Iteration 79/1000 | Loss: 0.00002795
Iteration 80/1000 | Loss: 0.00002794
Iteration 81/1000 | Loss: 0.00002794
Iteration 82/1000 | Loss: 0.00002793
Iteration 83/1000 | Loss: 0.00002793
Iteration 84/1000 | Loss: 0.00002793
Iteration 85/1000 | Loss: 0.00002793
Iteration 86/1000 | Loss: 0.00002792
Iteration 87/1000 | Loss: 0.00002792
Iteration 88/1000 | Loss: 0.00002792
Iteration 89/1000 | Loss: 0.00002792
Iteration 90/1000 | Loss: 0.00002792
Iteration 91/1000 | Loss: 0.00002792
Iteration 92/1000 | Loss: 0.00002791
Iteration 93/1000 | Loss: 0.00002791
Iteration 94/1000 | Loss: 0.00002791
Iteration 95/1000 | Loss: 0.00002791
Iteration 96/1000 | Loss: 0.00002791
Iteration 97/1000 | Loss: 0.00002790
Iteration 98/1000 | Loss: 0.00002790
Iteration 99/1000 | Loss: 0.00002790
Iteration 100/1000 | Loss: 0.00002790
Iteration 101/1000 | Loss: 0.00002790
Iteration 102/1000 | Loss: 0.00002790
Iteration 103/1000 | Loss: 0.00002790
Iteration 104/1000 | Loss: 0.00002789
Iteration 105/1000 | Loss: 0.00002789
Iteration 106/1000 | Loss: 0.00002789
Iteration 107/1000 | Loss: 0.00002789
Iteration 108/1000 | Loss: 0.00002789
Iteration 109/1000 | Loss: 0.00002789
Iteration 110/1000 | Loss: 0.00002788
Iteration 111/1000 | Loss: 0.00002788
Iteration 112/1000 | Loss: 0.00002788
Iteration 113/1000 | Loss: 0.00002788
Iteration 114/1000 | Loss: 0.00002788
Iteration 115/1000 | Loss: 0.00002788
Iteration 116/1000 | Loss: 0.00002788
Iteration 117/1000 | Loss: 0.00002788
Iteration 118/1000 | Loss: 0.00002787
Iteration 119/1000 | Loss: 0.00002787
Iteration 120/1000 | Loss: 0.00002787
Iteration 121/1000 | Loss: 0.00002787
Iteration 122/1000 | Loss: 0.00002787
Iteration 123/1000 | Loss: 0.00002787
Iteration 124/1000 | Loss: 0.00002786
Iteration 125/1000 | Loss: 0.00002786
Iteration 126/1000 | Loss: 0.00002786
Iteration 127/1000 | Loss: 0.00002786
Iteration 128/1000 | Loss: 0.00002786
Iteration 129/1000 | Loss: 0.00002786
Iteration 130/1000 | Loss: 0.00002786
Iteration 131/1000 | Loss: 0.00002785
Iteration 132/1000 | Loss: 0.00002785
Iteration 133/1000 | Loss: 0.00002785
Iteration 134/1000 | Loss: 0.00002785
Iteration 135/1000 | Loss: 0.00002785
Iteration 136/1000 | Loss: 0.00002785
Iteration 137/1000 | Loss: 0.00002785
Iteration 138/1000 | Loss: 0.00002785
Iteration 139/1000 | Loss: 0.00002785
Iteration 140/1000 | Loss: 0.00002785
Iteration 141/1000 | Loss: 0.00002785
Iteration 142/1000 | Loss: 0.00002785
Iteration 143/1000 | Loss: 0.00002785
Iteration 144/1000 | Loss: 0.00002785
Iteration 145/1000 | Loss: 0.00002785
Iteration 146/1000 | Loss: 0.00002784
Iteration 147/1000 | Loss: 0.00002784
Iteration 148/1000 | Loss: 0.00002784
Iteration 149/1000 | Loss: 0.00002784
Iteration 150/1000 | Loss: 0.00002784
Iteration 151/1000 | Loss: 0.00002784
Iteration 152/1000 | Loss: 0.00002784
Iteration 153/1000 | Loss: 0.00002784
Iteration 154/1000 | Loss: 0.00002784
Iteration 155/1000 | Loss: 0.00002784
Iteration 156/1000 | Loss: 0.00002783
Iteration 157/1000 | Loss: 0.00002783
Iteration 158/1000 | Loss: 0.00002783
Iteration 159/1000 | Loss: 0.00002783
Iteration 160/1000 | Loss: 0.00002783
Iteration 161/1000 | Loss: 0.00002783
Iteration 162/1000 | Loss: 0.00002783
Iteration 163/1000 | Loss: 0.00002783
Iteration 164/1000 | Loss: 0.00002783
Iteration 165/1000 | Loss: 0.00002783
Iteration 166/1000 | Loss: 0.00002782
Iteration 167/1000 | Loss: 0.00002782
Iteration 168/1000 | Loss: 0.00002782
Iteration 169/1000 | Loss: 0.00002782
Iteration 170/1000 | Loss: 0.00002782
Iteration 171/1000 | Loss: 0.00002782
Iteration 172/1000 | Loss: 0.00002782
Iteration 173/1000 | Loss: 0.00002782
Iteration 174/1000 | Loss: 0.00002781
Iteration 175/1000 | Loss: 0.00002781
Iteration 176/1000 | Loss: 0.00002781
Iteration 177/1000 | Loss: 0.00002781
Iteration 178/1000 | Loss: 0.00002781
Iteration 179/1000 | Loss: 0.00002781
Iteration 180/1000 | Loss: 0.00002781
Iteration 181/1000 | Loss: 0.00002781
Iteration 182/1000 | Loss: 0.00002781
Iteration 183/1000 | Loss: 0.00002781
Iteration 184/1000 | Loss: 0.00002781
Iteration 185/1000 | Loss: 0.00002780
Iteration 186/1000 | Loss: 0.00002780
Iteration 187/1000 | Loss: 0.00002780
Iteration 188/1000 | Loss: 0.00002780
Iteration 189/1000 | Loss: 0.00002780
Iteration 190/1000 | Loss: 0.00002780
Iteration 191/1000 | Loss: 0.00002780
Iteration 192/1000 | Loss: 0.00002780
Iteration 193/1000 | Loss: 0.00002780
Iteration 194/1000 | Loss: 0.00002780
Iteration 195/1000 | Loss: 0.00002780
Iteration 196/1000 | Loss: 0.00002780
Iteration 197/1000 | Loss: 0.00002780
Iteration 198/1000 | Loss: 0.00002780
Iteration 199/1000 | Loss: 0.00002780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [2.779934038699139e-05, 2.779934038699139e-05, 2.779934038699139e-05, 2.779934038699139e-05, 2.779934038699139e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.779934038699139e-05

Optimization complete. Final v2v error: 4.167169094085693 mm

Highest mean error: 5.759510517120361 mm for frame 160

Lowest mean error: 3.1890947818756104 mm for frame 124

Saving results

Total time: 47.07321906089783
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017614
Iteration 2/25 | Loss: 0.01017614
Iteration 3/25 | Loss: 0.01017614
Iteration 4/25 | Loss: 0.01017614
Iteration 5/25 | Loss: 0.01017613
Iteration 6/25 | Loss: 0.00279545
Iteration 7/25 | Loss: 0.00203186
Iteration 8/25 | Loss: 0.00174418
Iteration 9/25 | Loss: 0.00168028
Iteration 10/25 | Loss: 0.00157257
Iteration 11/25 | Loss: 0.00150199
Iteration 12/25 | Loss: 0.00144834
Iteration 13/25 | Loss: 0.00140763
Iteration 14/25 | Loss: 0.00140181
Iteration 15/25 | Loss: 0.00138851
Iteration 16/25 | Loss: 0.00137737
Iteration 17/25 | Loss: 0.00137118
Iteration 18/25 | Loss: 0.00137395
Iteration 19/25 | Loss: 0.00137413
Iteration 20/25 | Loss: 0.00136638
Iteration 21/25 | Loss: 0.00136082
Iteration 22/25 | Loss: 0.00135728
Iteration 23/25 | Loss: 0.00135902
Iteration 24/25 | Loss: 0.00134177
Iteration 25/25 | Loss: 0.00133276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46000636
Iteration 2/25 | Loss: 0.00287876
Iteration 3/25 | Loss: 0.00205068
Iteration 4/25 | Loss: 0.00205065
Iteration 5/25 | Loss: 0.00205065
Iteration 6/25 | Loss: 0.00205065
Iteration 7/25 | Loss: 0.00205065
Iteration 8/25 | Loss: 0.00205065
Iteration 9/25 | Loss: 0.00205065
Iteration 10/25 | Loss: 0.00205065
Iteration 11/25 | Loss: 0.00205065
Iteration 12/25 | Loss: 0.00205065
Iteration 13/25 | Loss: 0.00205065
Iteration 14/25 | Loss: 0.00205065
Iteration 15/25 | Loss: 0.00205065
Iteration 16/25 | Loss: 0.00205065
Iteration 17/25 | Loss: 0.00205065
Iteration 18/25 | Loss: 0.00205065
Iteration 19/25 | Loss: 0.00205065
Iteration 20/25 | Loss: 0.00205065
Iteration 21/25 | Loss: 0.00205065
Iteration 22/25 | Loss: 0.00205065
Iteration 23/25 | Loss: 0.00205065
Iteration 24/25 | Loss: 0.00205065
Iteration 25/25 | Loss: 0.00205065

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205065
Iteration 2/1000 | Loss: 0.00100782
Iteration 3/1000 | Loss: 0.00059965
Iteration 4/1000 | Loss: 0.00020044
Iteration 5/1000 | Loss: 0.00043550
Iteration 6/1000 | Loss: 0.00013456
Iteration 7/1000 | Loss: 0.00016697
Iteration 8/1000 | Loss: 0.00142591
Iteration 9/1000 | Loss: 0.00013059
Iteration 10/1000 | Loss: 0.00020403
Iteration 11/1000 | Loss: 0.00052647
Iteration 12/1000 | Loss: 0.00008609
Iteration 13/1000 | Loss: 0.00010603
Iteration 14/1000 | Loss: 0.00007375
Iteration 15/1000 | Loss: 0.00015660
Iteration 16/1000 | Loss: 0.00068244
Iteration 17/1000 | Loss: 0.00073446
Iteration 18/1000 | Loss: 0.00038770
Iteration 19/1000 | Loss: 0.00028266
Iteration 20/1000 | Loss: 0.00023938
Iteration 21/1000 | Loss: 0.00006950
Iteration 22/1000 | Loss: 0.00009295
Iteration 23/1000 | Loss: 0.00018281
Iteration 24/1000 | Loss: 0.00026204
Iteration 25/1000 | Loss: 0.00027658
Iteration 26/1000 | Loss: 0.00017244
Iteration 27/1000 | Loss: 0.00012427
Iteration 28/1000 | Loss: 0.00007574
Iteration 29/1000 | Loss: 0.00006414
Iteration 30/1000 | Loss: 0.00028053
Iteration 31/1000 | Loss: 0.00014151
Iteration 32/1000 | Loss: 0.00022023
Iteration 33/1000 | Loss: 0.00026417
Iteration 34/1000 | Loss: 0.00023774
Iteration 35/1000 | Loss: 0.00016341
Iteration 36/1000 | Loss: 0.00006599
Iteration 37/1000 | Loss: 0.00006410
Iteration 38/1000 | Loss: 0.00006333
Iteration 39/1000 | Loss: 0.00011184
Iteration 40/1000 | Loss: 0.00036175
Iteration 41/1000 | Loss: 0.00017249
Iteration 42/1000 | Loss: 0.00007362
Iteration 43/1000 | Loss: 0.00006565
Iteration 44/1000 | Loss: 0.00006201
Iteration 45/1000 | Loss: 0.00010983
Iteration 46/1000 | Loss: 0.00005944
Iteration 47/1000 | Loss: 0.00019529
Iteration 48/1000 | Loss: 0.00015759
Iteration 49/1000 | Loss: 0.00006067
Iteration 50/1000 | Loss: 0.00005864
Iteration 51/1000 | Loss: 0.00016446
Iteration 52/1000 | Loss: 0.00010843
Iteration 53/1000 | Loss: 0.00006686
Iteration 54/1000 | Loss: 0.00006047
Iteration 55/1000 | Loss: 0.00005843
Iteration 56/1000 | Loss: 0.00005783
Iteration 57/1000 | Loss: 0.00005745
Iteration 58/1000 | Loss: 0.00005701
Iteration 59/1000 | Loss: 0.00019602
Iteration 60/1000 | Loss: 0.00016589
Iteration 61/1000 | Loss: 0.00005800
Iteration 62/1000 | Loss: 0.00020312
Iteration 63/1000 | Loss: 0.00013249
Iteration 64/1000 | Loss: 0.00020458
Iteration 65/1000 | Loss: 0.00010394
Iteration 66/1000 | Loss: 0.00019420
Iteration 67/1000 | Loss: 0.00012241
Iteration 68/1000 | Loss: 0.00027829
Iteration 69/1000 | Loss: 0.00011740
Iteration 70/1000 | Loss: 0.00024820
Iteration 71/1000 | Loss: 0.00006428
Iteration 72/1000 | Loss: 0.00006050
Iteration 73/1000 | Loss: 0.00014474
Iteration 74/1000 | Loss: 0.00005845
Iteration 75/1000 | Loss: 0.00005720
Iteration 76/1000 | Loss: 0.00024089
Iteration 77/1000 | Loss: 0.00035756
Iteration 78/1000 | Loss: 0.00056169
Iteration 79/1000 | Loss: 0.00038240
Iteration 80/1000 | Loss: 0.00015445
Iteration 81/1000 | Loss: 0.00010798
Iteration 82/1000 | Loss: 0.00018980
Iteration 83/1000 | Loss: 0.00010111
Iteration 84/1000 | Loss: 0.00006376
Iteration 85/1000 | Loss: 0.00005671
Iteration 86/1000 | Loss: 0.00010038
Iteration 87/1000 | Loss: 0.00005315
Iteration 88/1000 | Loss: 0.00013123
Iteration 89/1000 | Loss: 0.00005139
Iteration 90/1000 | Loss: 0.00005046
Iteration 91/1000 | Loss: 0.00004986
Iteration 92/1000 | Loss: 0.00004948
Iteration 93/1000 | Loss: 0.00004913
Iteration 94/1000 | Loss: 0.00004866
Iteration 95/1000 | Loss: 0.00012733
Iteration 96/1000 | Loss: 0.00004831
Iteration 97/1000 | Loss: 0.00004978
Iteration 98/1000 | Loss: 0.00004804
Iteration 99/1000 | Loss: 0.00004783
Iteration 100/1000 | Loss: 0.00004778
Iteration 101/1000 | Loss: 0.00004758
Iteration 102/1000 | Loss: 0.00004727
Iteration 103/1000 | Loss: 0.00004691
Iteration 104/1000 | Loss: 0.00004643
Iteration 105/1000 | Loss: 0.00004587
Iteration 106/1000 | Loss: 0.00004537
Iteration 107/1000 | Loss: 0.00018241
Iteration 108/1000 | Loss: 0.00023403
Iteration 109/1000 | Loss: 0.00024785
Iteration 110/1000 | Loss: 0.00030860
Iteration 111/1000 | Loss: 0.00023554
Iteration 112/1000 | Loss: 0.00031347
Iteration 113/1000 | Loss: 0.00024001
Iteration 114/1000 | Loss: 0.00029099
Iteration 115/1000 | Loss: 0.00023321
Iteration 116/1000 | Loss: 0.00004782
Iteration 117/1000 | Loss: 0.00004518
Iteration 118/1000 | Loss: 0.00016207
Iteration 119/1000 | Loss: 0.00020657
Iteration 120/1000 | Loss: 0.00023268
Iteration 121/1000 | Loss: 0.00006480
Iteration 122/1000 | Loss: 0.00013729
Iteration 123/1000 | Loss: 0.00004847
Iteration 124/1000 | Loss: 0.00004678
Iteration 125/1000 | Loss: 0.00004573
Iteration 126/1000 | Loss: 0.00004531
Iteration 127/1000 | Loss: 0.00024225
Iteration 128/1000 | Loss: 0.00017430
Iteration 129/1000 | Loss: 0.00006191
Iteration 130/1000 | Loss: 0.00005319
Iteration 131/1000 | Loss: 0.00004529
Iteration 132/1000 | Loss: 0.00004430
Iteration 133/1000 | Loss: 0.00017461
Iteration 134/1000 | Loss: 0.00026084
Iteration 135/1000 | Loss: 0.00022345
Iteration 136/1000 | Loss: 0.00018090
Iteration 137/1000 | Loss: 0.00005557
Iteration 138/1000 | Loss: 0.00004903
Iteration 139/1000 | Loss: 0.00022060
Iteration 140/1000 | Loss: 0.00005037
Iteration 141/1000 | Loss: 0.00004737
Iteration 142/1000 | Loss: 0.00004576
Iteration 143/1000 | Loss: 0.00004438
Iteration 144/1000 | Loss: 0.00005296
Iteration 145/1000 | Loss: 0.00004372
Iteration 146/1000 | Loss: 0.00004264
Iteration 147/1000 | Loss: 0.00004210
Iteration 148/1000 | Loss: 0.00025043
Iteration 149/1000 | Loss: 0.00019329
Iteration 150/1000 | Loss: 0.00015809
Iteration 151/1000 | Loss: 0.00004499
Iteration 152/1000 | Loss: 0.00004382
Iteration 153/1000 | Loss: 0.00004314
Iteration 154/1000 | Loss: 0.00004227
Iteration 155/1000 | Loss: 0.00005381
Iteration 156/1000 | Loss: 0.00004260
Iteration 157/1000 | Loss: 0.00004184
Iteration 158/1000 | Loss: 0.00004147
Iteration 159/1000 | Loss: 0.00004121
Iteration 160/1000 | Loss: 0.00004101
Iteration 161/1000 | Loss: 0.00004071
Iteration 162/1000 | Loss: 0.00004035
Iteration 163/1000 | Loss: 0.00024978
Iteration 164/1000 | Loss: 0.00016311
Iteration 165/1000 | Loss: 0.00023090
Iteration 166/1000 | Loss: 0.00011848
Iteration 167/1000 | Loss: 0.00011528
Iteration 168/1000 | Loss: 0.00016456
Iteration 169/1000 | Loss: 0.00021223
Iteration 170/1000 | Loss: 0.00005030
Iteration 171/1000 | Loss: 0.00004309
Iteration 172/1000 | Loss: 0.00004063
Iteration 173/1000 | Loss: 0.00003944
Iteration 174/1000 | Loss: 0.00003872
Iteration 175/1000 | Loss: 0.00003827
Iteration 176/1000 | Loss: 0.00003778
Iteration 177/1000 | Loss: 0.00003740
Iteration 178/1000 | Loss: 0.00003687
Iteration 179/1000 | Loss: 0.00003636
Iteration 180/1000 | Loss: 0.00109244
Iteration 181/1000 | Loss: 0.00059191
Iteration 182/1000 | Loss: 0.00004276
Iteration 183/1000 | Loss: 0.00003837
Iteration 184/1000 | Loss: 0.00003692
Iteration 185/1000 | Loss: 0.00064105
Iteration 186/1000 | Loss: 0.00044349
Iteration 187/1000 | Loss: 0.00004114
Iteration 188/1000 | Loss: 0.00003728
Iteration 189/1000 | Loss: 0.00044715
Iteration 190/1000 | Loss: 0.00016109
Iteration 191/1000 | Loss: 0.00041772
Iteration 192/1000 | Loss: 0.00015732
Iteration 193/1000 | Loss: 0.00040489
Iteration 194/1000 | Loss: 0.00013823
Iteration 195/1000 | Loss: 0.00020837
Iteration 196/1000 | Loss: 0.00093585
Iteration 197/1000 | Loss: 0.00028705
Iteration 198/1000 | Loss: 0.00017102
Iteration 199/1000 | Loss: 0.00028234
Iteration 200/1000 | Loss: 0.00036691
Iteration 201/1000 | Loss: 0.00010472
Iteration 202/1000 | Loss: 0.00005305
Iteration 203/1000 | Loss: 0.00004840
Iteration 204/1000 | Loss: 0.00029877
Iteration 205/1000 | Loss: 0.00011959
Iteration 206/1000 | Loss: 0.00023737
Iteration 207/1000 | Loss: 0.00004189
Iteration 208/1000 | Loss: 0.00003709
Iteration 209/1000 | Loss: 0.00003596
Iteration 210/1000 | Loss: 0.00003469
Iteration 211/1000 | Loss: 0.00004724
Iteration 212/1000 | Loss: 0.00003404
Iteration 213/1000 | Loss: 0.00024387
Iteration 214/1000 | Loss: 0.00003891
Iteration 215/1000 | Loss: 0.00003521
Iteration 216/1000 | Loss: 0.00003425
Iteration 217/1000 | Loss: 0.00003433
Iteration 218/1000 | Loss: 0.00003287
Iteration 219/1000 | Loss: 0.00024768
Iteration 220/1000 | Loss: 0.00016994
Iteration 221/1000 | Loss: 0.00003225
Iteration 222/1000 | Loss: 0.00025412
Iteration 223/1000 | Loss: 0.00019984
Iteration 224/1000 | Loss: 0.00036354
Iteration 225/1000 | Loss: 0.00019190
Iteration 226/1000 | Loss: 0.00019051
Iteration 227/1000 | Loss: 0.00003695
Iteration 228/1000 | Loss: 0.00003535
Iteration 229/1000 | Loss: 0.00003443
Iteration 230/1000 | Loss: 0.00023013
Iteration 231/1000 | Loss: 0.00003817
Iteration 232/1000 | Loss: 0.00003560
Iteration 233/1000 | Loss: 0.00003450
Iteration 234/1000 | Loss: 0.00003373
Iteration 235/1000 | Loss: 0.00026079
Iteration 236/1000 | Loss: 0.00018869
Iteration 237/1000 | Loss: 0.00048151
Iteration 238/1000 | Loss: 0.00043308
Iteration 239/1000 | Loss: 0.00023134
Iteration 240/1000 | Loss: 0.00014333
Iteration 241/1000 | Loss: 0.00003471
Iteration 242/1000 | Loss: 0.00022538
Iteration 243/1000 | Loss: 0.00004952
Iteration 244/1000 | Loss: 0.00031321
Iteration 245/1000 | Loss: 0.00005295
Iteration 246/1000 | Loss: 0.00004215
Iteration 247/1000 | Loss: 0.00005160
Iteration 248/1000 | Loss: 0.00003967
Iteration 249/1000 | Loss: 0.00003564
Iteration 250/1000 | Loss: 0.00003439
Iteration 251/1000 | Loss: 0.00003349
Iteration 252/1000 | Loss: 0.00016488
Iteration 253/1000 | Loss: 0.00012775
Iteration 254/1000 | Loss: 0.00016161
Iteration 255/1000 | Loss: 0.00026934
Iteration 256/1000 | Loss: 0.00017665
Iteration 257/1000 | Loss: 0.00003991
Iteration 258/1000 | Loss: 0.00003664
Iteration 259/1000 | Loss: 0.00003572
Iteration 260/1000 | Loss: 0.00004443
Iteration 261/1000 | Loss: 0.00003488
Iteration 262/1000 | Loss: 0.00003399
Iteration 263/1000 | Loss: 0.00003312
Iteration 264/1000 | Loss: 0.00003257
Iteration 265/1000 | Loss: 0.00022416
Iteration 266/1000 | Loss: 0.00003865
Iteration 267/1000 | Loss: 0.00003470
Iteration 268/1000 | Loss: 0.00003382
Iteration 269/1000 | Loss: 0.00004546
Iteration 270/1000 | Loss: 0.00003374
Iteration 271/1000 | Loss: 0.00003283
Iteration 272/1000 | Loss: 0.00021914
Iteration 273/1000 | Loss: 0.00017916
Iteration 274/1000 | Loss: 0.00021433
Iteration 275/1000 | Loss: 0.00029059
Iteration 276/1000 | Loss: 0.00022053
Iteration 277/1000 | Loss: 0.00008357
Iteration 278/1000 | Loss: 0.00006832
Iteration 279/1000 | Loss: 0.00003456
Iteration 280/1000 | Loss: 0.00003382
Iteration 281/1000 | Loss: 0.00003323
Iteration 282/1000 | Loss: 0.00003283
Iteration 283/1000 | Loss: 0.00003260
Iteration 284/1000 | Loss: 0.00003219
Iteration 285/1000 | Loss: 0.00003181
Iteration 286/1000 | Loss: 0.00015533
Iteration 287/1000 | Loss: 0.00022918
Iteration 288/1000 | Loss: 0.00018710
Iteration 289/1000 | Loss: 0.00018520
Iteration 290/1000 | Loss: 0.00023683
Iteration 291/1000 | Loss: 0.00016225
Iteration 292/1000 | Loss: 0.00003701
Iteration 293/1000 | Loss: 0.00003460
Iteration 294/1000 | Loss: 0.00003324
Iteration 295/1000 | Loss: 0.00003248
Iteration 296/1000 | Loss: 0.00003196
Iteration 297/1000 | Loss: 0.00003144
Iteration 298/1000 | Loss: 0.00003091
Iteration 299/1000 | Loss: 0.00025246
Iteration 300/1000 | Loss: 0.00016551
Iteration 301/1000 | Loss: 0.00060860
Iteration 302/1000 | Loss: 0.00033146
Iteration 303/1000 | Loss: 0.00020541
Iteration 304/1000 | Loss: 0.00014812
Iteration 305/1000 | Loss: 0.00014254
Iteration 306/1000 | Loss: 0.00009199
Iteration 307/1000 | Loss: 0.00055274
Iteration 308/1000 | Loss: 0.00038885
Iteration 309/1000 | Loss: 0.00004286
Iteration 310/1000 | Loss: 0.00003529
Iteration 311/1000 | Loss: 0.00003324
Iteration 312/1000 | Loss: 0.00003178
Iteration 313/1000 | Loss: 0.00040746
Iteration 314/1000 | Loss: 0.00027359
Iteration 315/1000 | Loss: 0.00033358
Iteration 316/1000 | Loss: 0.00011247
Iteration 317/1000 | Loss: 0.00014933
Iteration 318/1000 | Loss: 0.00009469
Iteration 319/1000 | Loss: 0.00047679
Iteration 320/1000 | Loss: 0.00033469
Iteration 321/1000 | Loss: 0.00007543
Iteration 322/1000 | Loss: 0.00005948
Iteration 323/1000 | Loss: 0.00007589
Iteration 324/1000 | Loss: 0.00005961
Iteration 325/1000 | Loss: 0.00020188
Iteration 326/1000 | Loss: 0.00039314
Iteration 327/1000 | Loss: 0.00007864
Iteration 328/1000 | Loss: 0.00047672
Iteration 329/1000 | Loss: 0.00022316
Iteration 330/1000 | Loss: 0.00004128
Iteration 331/1000 | Loss: 0.00003611
Iteration 332/1000 | Loss: 0.00003304
Iteration 333/1000 | Loss: 0.00003138
Iteration 334/1000 | Loss: 0.00003019
Iteration 335/1000 | Loss: 0.00002946
Iteration 336/1000 | Loss: 0.00023750
Iteration 337/1000 | Loss: 0.00021052
Iteration 338/1000 | Loss: 0.00002887
Iteration 339/1000 | Loss: 0.00030956
Iteration 340/1000 | Loss: 0.00003532
Iteration 341/1000 | Loss: 0.00003263
Iteration 342/1000 | Loss: 0.00003142
Iteration 343/1000 | Loss: 0.00003039
Iteration 344/1000 | Loss: 0.00002965
Iteration 345/1000 | Loss: 0.00002915
Iteration 346/1000 | Loss: 0.00004363
Iteration 347/1000 | Loss: 0.00023250
Iteration 348/1000 | Loss: 0.00019275
Iteration 349/1000 | Loss: 0.00014102
Iteration 350/1000 | Loss: 0.00003517
Iteration 351/1000 | Loss: 0.00032843
Iteration 352/1000 | Loss: 0.00003339
Iteration 353/1000 | Loss: 0.00002959
Iteration 354/1000 | Loss: 0.00002831
Iteration 355/1000 | Loss: 0.00002762
Iteration 356/1000 | Loss: 0.00002710
Iteration 357/1000 | Loss: 0.00002683
Iteration 358/1000 | Loss: 0.00002647
Iteration 359/1000 | Loss: 0.00023612
Iteration 360/1000 | Loss: 0.00022983
Iteration 361/1000 | Loss: 0.00016721
Iteration 362/1000 | Loss: 0.00020550
Iteration 363/1000 | Loss: 0.00003106
Iteration 364/1000 | Loss: 0.00002857
Iteration 365/1000 | Loss: 0.00002703
Iteration 366/1000 | Loss: 0.00002606
Iteration 367/1000 | Loss: 0.00002554
Iteration 368/1000 | Loss: 0.00002532
Iteration 369/1000 | Loss: 0.00002532
Iteration 370/1000 | Loss: 0.00002526
Iteration 371/1000 | Loss: 0.00002501
Iteration 372/1000 | Loss: 0.00002480
Iteration 373/1000 | Loss: 0.00002479
Iteration 374/1000 | Loss: 0.00018903
Iteration 375/1000 | Loss: 0.00012279
Iteration 376/1000 | Loss: 0.00018796
Iteration 377/1000 | Loss: 0.00003206
Iteration 378/1000 | Loss: 0.00002662
Iteration 379/1000 | Loss: 0.00002449
Iteration 380/1000 | Loss: 0.00002375
Iteration 381/1000 | Loss: 0.00002329
Iteration 382/1000 | Loss: 0.00002310
Iteration 383/1000 | Loss: 0.00002309
Iteration 384/1000 | Loss: 0.00002309
Iteration 385/1000 | Loss: 0.00002307
Iteration 386/1000 | Loss: 0.00002305
Iteration 387/1000 | Loss: 0.00002305
Iteration 388/1000 | Loss: 0.00002286
Iteration 389/1000 | Loss: 0.00023933
Iteration 390/1000 | Loss: 0.00015462
Iteration 391/1000 | Loss: 0.00021777
Iteration 392/1000 | Loss: 0.00003173
Iteration 393/1000 | Loss: 0.00002634
Iteration 394/1000 | Loss: 0.00002474
Iteration 395/1000 | Loss: 0.00002405
Iteration 396/1000 | Loss: 0.00002369
Iteration 397/1000 | Loss: 0.00002329
Iteration 398/1000 | Loss: 0.00002301
Iteration 399/1000 | Loss: 0.00021721
Iteration 400/1000 | Loss: 0.00015772
Iteration 401/1000 | Loss: 0.00002300
Iteration 402/1000 | Loss: 0.00022370
Iteration 403/1000 | Loss: 0.00019307
Iteration 404/1000 | Loss: 0.00026052
Iteration 405/1000 | Loss: 0.00004058
Iteration 406/1000 | Loss: 0.00003098
Iteration 407/1000 | Loss: 0.00002907
Iteration 408/1000 | Loss: 0.00002735
Iteration 409/1000 | Loss: 0.00002556
Iteration 410/1000 | Loss: 0.00002356
Iteration 411/1000 | Loss: 0.00020865
Iteration 412/1000 | Loss: 0.00018273
Iteration 413/1000 | Loss: 0.00014476
Iteration 414/1000 | Loss: 0.00015646
Iteration 415/1000 | Loss: 0.00016011
Iteration 416/1000 | Loss: 0.00002649
Iteration 417/1000 | Loss: 0.00002461
Iteration 418/1000 | Loss: 0.00002398
Iteration 419/1000 | Loss: 0.00002338
Iteration 420/1000 | Loss: 0.00002290
Iteration 421/1000 | Loss: 0.00024122
Iteration 422/1000 | Loss: 0.00003748
Iteration 423/1000 | Loss: 0.00002770
Iteration 424/1000 | Loss: 0.00002489
Iteration 425/1000 | Loss: 0.00002316
Iteration 426/1000 | Loss: 0.00002198
Iteration 427/1000 | Loss: 0.00002151
Iteration 428/1000 | Loss: 0.00002134
Iteration 429/1000 | Loss: 0.00002126
Iteration 430/1000 | Loss: 0.00002124
Iteration 431/1000 | Loss: 0.00002124
Iteration 432/1000 | Loss: 0.00002124
Iteration 433/1000 | Loss: 0.00002123
Iteration 434/1000 | Loss: 0.00002123
Iteration 435/1000 | Loss: 0.00002123
Iteration 436/1000 | Loss: 0.00002122
Iteration 437/1000 | Loss: 0.00002122
Iteration 438/1000 | Loss: 0.00002122
Iteration 439/1000 | Loss: 0.00002122
Iteration 440/1000 | Loss: 0.00002122
Iteration 441/1000 | Loss: 0.00002122
Iteration 442/1000 | Loss: 0.00002122
Iteration 443/1000 | Loss: 0.00002122
Iteration 444/1000 | Loss: 0.00002122
Iteration 445/1000 | Loss: 0.00002122
Iteration 446/1000 | Loss: 0.00002121
Iteration 447/1000 | Loss: 0.00002121
Iteration 448/1000 | Loss: 0.00002119
Iteration 449/1000 | Loss: 0.00002119
Iteration 450/1000 | Loss: 0.00002119
Iteration 451/1000 | Loss: 0.00002119
Iteration 452/1000 | Loss: 0.00002118
Iteration 453/1000 | Loss: 0.00002118
Iteration 454/1000 | Loss: 0.00002118
Iteration 455/1000 | Loss: 0.00002118
Iteration 456/1000 | Loss: 0.00002118
Iteration 457/1000 | Loss: 0.00002117
Iteration 458/1000 | Loss: 0.00002109
Iteration 459/1000 | Loss: 0.00002102
Iteration 460/1000 | Loss: 0.00002097
Iteration 461/1000 | Loss: 0.00002095
Iteration 462/1000 | Loss: 0.00002087
Iteration 463/1000 | Loss: 0.00002086
Iteration 464/1000 | Loss: 0.00002085
Iteration 465/1000 | Loss: 0.00002084
Iteration 466/1000 | Loss: 0.00002084
Iteration 467/1000 | Loss: 0.00002084
Iteration 468/1000 | Loss: 0.00002084
Iteration 469/1000 | Loss: 0.00002084
Iteration 470/1000 | Loss: 0.00002083
Iteration 471/1000 | Loss: 0.00002083
Iteration 472/1000 | Loss: 0.00002083
Iteration 473/1000 | Loss: 0.00002082
Iteration 474/1000 | Loss: 0.00002082
Iteration 475/1000 | Loss: 0.00002082
Iteration 476/1000 | Loss: 0.00002082
Iteration 477/1000 | Loss: 0.00002082
Iteration 478/1000 | Loss: 0.00002082
Iteration 479/1000 | Loss: 0.00002082
Iteration 480/1000 | Loss: 0.00002081
Iteration 481/1000 | Loss: 0.00002081
Iteration 482/1000 | Loss: 0.00002081
Iteration 483/1000 | Loss: 0.00002080
Iteration 484/1000 | Loss: 0.00002080
Iteration 485/1000 | Loss: 0.00002078
Iteration 486/1000 | Loss: 0.00002078
Iteration 487/1000 | Loss: 0.00002075
Iteration 488/1000 | Loss: 0.00002075
Iteration 489/1000 | Loss: 0.00002075
Iteration 490/1000 | Loss: 0.00002075
Iteration 491/1000 | Loss: 0.00002075
Iteration 492/1000 | Loss: 0.00002075
Iteration 493/1000 | Loss: 0.00002075
Iteration 494/1000 | Loss: 0.00002074
Iteration 495/1000 | Loss: 0.00002074
Iteration 496/1000 | Loss: 0.00002074
Iteration 497/1000 | Loss: 0.00002073
Iteration 498/1000 | Loss: 0.00002073
Iteration 499/1000 | Loss: 0.00002073
Iteration 500/1000 | Loss: 0.00002072
Iteration 501/1000 | Loss: 0.00002072
Iteration 502/1000 | Loss: 0.00002072
Iteration 503/1000 | Loss: 0.00002072
Iteration 504/1000 | Loss: 0.00002072
Iteration 505/1000 | Loss: 0.00002072
Iteration 506/1000 | Loss: 0.00002072
Iteration 507/1000 | Loss: 0.00002072
Iteration 508/1000 | Loss: 0.00002072
Iteration 509/1000 | Loss: 0.00002072
Iteration 510/1000 | Loss: 0.00002071
Iteration 511/1000 | Loss: 0.00002071
Iteration 512/1000 | Loss: 0.00002071
Iteration 513/1000 | Loss: 0.00002071
Iteration 514/1000 | Loss: 0.00002071
Iteration 515/1000 | Loss: 0.00002071
Iteration 516/1000 | Loss: 0.00002071
Iteration 517/1000 | Loss: 0.00002071
Iteration 518/1000 | Loss: 0.00002071
Iteration 519/1000 | Loss: 0.00002071
Iteration 520/1000 | Loss: 0.00002071
Iteration 521/1000 | Loss: 0.00002071
Iteration 522/1000 | Loss: 0.00002071
Iteration 523/1000 | Loss: 0.00002071
Iteration 524/1000 | Loss: 0.00002071
Iteration 525/1000 | Loss: 0.00002071
Iteration 526/1000 | Loss: 0.00002071
Iteration 527/1000 | Loss: 0.00002070
Iteration 528/1000 | Loss: 0.00002070
Iteration 529/1000 | Loss: 0.00002070
Iteration 530/1000 | Loss: 0.00002070
Iteration 531/1000 | Loss: 0.00002070
Iteration 532/1000 | Loss: 0.00002069
Iteration 533/1000 | Loss: 0.00002069
Iteration 534/1000 | Loss: 0.00002069
Iteration 535/1000 | Loss: 0.00002069
Iteration 536/1000 | Loss: 0.00002069
Iteration 537/1000 | Loss: 0.00002069
Iteration 538/1000 | Loss: 0.00002069
Iteration 539/1000 | Loss: 0.00002068
Iteration 540/1000 | Loss: 0.00002068
Iteration 541/1000 | Loss: 0.00002068
Iteration 542/1000 | Loss: 0.00002068
Iteration 543/1000 | Loss: 0.00002068
Iteration 544/1000 | Loss: 0.00002068
Iteration 545/1000 | Loss: 0.00002068
Iteration 546/1000 | Loss: 0.00002068
Iteration 547/1000 | Loss: 0.00002068
Iteration 548/1000 | Loss: 0.00002068
Iteration 549/1000 | Loss: 0.00002068
Iteration 550/1000 | Loss: 0.00002068
Iteration 551/1000 | Loss: 0.00002068
Iteration 552/1000 | Loss: 0.00002067
Iteration 553/1000 | Loss: 0.00002067
Iteration 554/1000 | Loss: 0.00002067
Iteration 555/1000 | Loss: 0.00002067
Iteration 556/1000 | Loss: 0.00002067
Iteration 557/1000 | Loss: 0.00002067
Iteration 558/1000 | Loss: 0.00002067
Iteration 559/1000 | Loss: 0.00002067
Iteration 560/1000 | Loss: 0.00002067
Iteration 561/1000 | Loss: 0.00002067
Iteration 562/1000 | Loss: 0.00002066
Iteration 563/1000 | Loss: 0.00002066
Iteration 564/1000 | Loss: 0.00002066
Iteration 565/1000 | Loss: 0.00002066
Iteration 566/1000 | Loss: 0.00002066
Iteration 567/1000 | Loss: 0.00002065
Iteration 568/1000 | Loss: 0.00002065
Iteration 569/1000 | Loss: 0.00002065
Iteration 570/1000 | Loss: 0.00002065
Iteration 571/1000 | Loss: 0.00002065
Iteration 572/1000 | Loss: 0.00002065
Iteration 573/1000 | Loss: 0.00002065
Iteration 574/1000 | Loss: 0.00002065
Iteration 575/1000 | Loss: 0.00002065
Iteration 576/1000 | Loss: 0.00002064
Iteration 577/1000 | Loss: 0.00002064
Iteration 578/1000 | Loss: 0.00002064
Iteration 579/1000 | Loss: 0.00002064
Iteration 580/1000 | Loss: 0.00002064
Iteration 581/1000 | Loss: 0.00002064
Iteration 582/1000 | Loss: 0.00002064
Iteration 583/1000 | Loss: 0.00002064
Iteration 584/1000 | Loss: 0.00002064
Iteration 585/1000 | Loss: 0.00002064
Iteration 586/1000 | Loss: 0.00002064
Iteration 587/1000 | Loss: 0.00002064
Iteration 588/1000 | Loss: 0.00002064
Iteration 589/1000 | Loss: 0.00002064
Iteration 590/1000 | Loss: 0.00002064
Iteration 591/1000 | Loss: 0.00002063
Iteration 592/1000 | Loss: 0.00002063
Iteration 593/1000 | Loss: 0.00002063
Iteration 594/1000 | Loss: 0.00002063
Iteration 595/1000 | Loss: 0.00002063
Iteration 596/1000 | Loss: 0.00002063
Iteration 597/1000 | Loss: 0.00002063
Iteration 598/1000 | Loss: 0.00002063
Iteration 599/1000 | Loss: 0.00002063
Iteration 600/1000 | Loss: 0.00002063
Iteration 601/1000 | Loss: 0.00002063
Iteration 602/1000 | Loss: 0.00002063
Iteration 603/1000 | Loss: 0.00002063
Iteration 604/1000 | Loss: 0.00002063
Iteration 605/1000 | Loss: 0.00002063
Iteration 606/1000 | Loss: 0.00002063
Iteration 607/1000 | Loss: 0.00002063
Iteration 608/1000 | Loss: 0.00002063
Iteration 609/1000 | Loss: 0.00002063
Iteration 610/1000 | Loss: 0.00002063
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 610. Stopping optimization.
Last 5 losses: [2.0632693122024648e-05, 2.0632693122024648e-05, 2.0632693122024648e-05, 2.0632693122024648e-05, 2.0632693122024648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0632693122024648e-05

Optimization complete. Final v2v error: 3.3253540992736816 mm

Highest mean error: 10.86297607421875 mm for frame 142

Lowest mean error: 2.9217169284820557 mm for frame 65

Saving results

Total time: 727.4669091701508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00876956
Iteration 2/25 | Loss: 0.00174811
Iteration 3/25 | Loss: 0.00140622
Iteration 4/25 | Loss: 0.00136940
Iteration 5/25 | Loss: 0.00135730
Iteration 6/25 | Loss: 0.00133736
Iteration 7/25 | Loss: 0.00133290
Iteration 8/25 | Loss: 0.00133101
Iteration 9/25 | Loss: 0.00133093
Iteration 10/25 | Loss: 0.00132027
Iteration 11/25 | Loss: 0.00131732
Iteration 12/25 | Loss: 0.00131673
Iteration 13/25 | Loss: 0.00131653
Iteration 14/25 | Loss: 0.00131649
Iteration 15/25 | Loss: 0.00131649
Iteration 16/25 | Loss: 0.00131649
Iteration 17/25 | Loss: 0.00131649
Iteration 18/25 | Loss: 0.00131649
Iteration 19/25 | Loss: 0.00131649
Iteration 20/25 | Loss: 0.00131649
Iteration 21/25 | Loss: 0.00131649
Iteration 22/25 | Loss: 0.00131649
Iteration 23/25 | Loss: 0.00131649
Iteration 24/25 | Loss: 0.00131648
Iteration 25/25 | Loss: 0.00131648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.78479385
Iteration 2/25 | Loss: 0.00078480
Iteration 3/25 | Loss: 0.00078470
Iteration 4/25 | Loss: 0.00078470
Iteration 5/25 | Loss: 0.00078469
Iteration 6/25 | Loss: 0.00078469
Iteration 7/25 | Loss: 0.00078469
Iteration 8/25 | Loss: 0.00078469
Iteration 9/25 | Loss: 0.00078469
Iteration 10/25 | Loss: 0.00078469
Iteration 11/25 | Loss: 0.00078469
Iteration 12/25 | Loss: 0.00078469
Iteration 13/25 | Loss: 0.00078469
Iteration 14/25 | Loss: 0.00078469
Iteration 15/25 | Loss: 0.00078469
Iteration 16/25 | Loss: 0.00078469
Iteration 17/25 | Loss: 0.00078469
Iteration 18/25 | Loss: 0.00078469
Iteration 19/25 | Loss: 0.00078469
Iteration 20/25 | Loss: 0.00078469
Iteration 21/25 | Loss: 0.00078469
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007846927037462592, 0.0007846927037462592, 0.0007846927037462592, 0.0007846927037462592, 0.0007846927037462592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007846927037462592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078469
Iteration 2/1000 | Loss: 0.00034668
Iteration 3/1000 | Loss: 0.00006346
Iteration 4/1000 | Loss: 0.00004172
Iteration 5/1000 | Loss: 0.00003254
Iteration 6/1000 | Loss: 0.00002876
Iteration 7/1000 | Loss: 0.00002689
Iteration 8/1000 | Loss: 0.00002618
Iteration 9/1000 | Loss: 0.00002563
Iteration 10/1000 | Loss: 0.00030823
Iteration 11/1000 | Loss: 0.00014612
Iteration 12/1000 | Loss: 0.00002545
Iteration 13/1000 | Loss: 0.00033373
Iteration 14/1000 | Loss: 0.00003642
Iteration 15/1000 | Loss: 0.00003244
Iteration 16/1000 | Loss: 0.00022971
Iteration 17/1000 | Loss: 0.00032837
Iteration 18/1000 | Loss: 0.00019072
Iteration 19/1000 | Loss: 0.00006908
Iteration 20/1000 | Loss: 0.00006161
Iteration 21/1000 | Loss: 0.00003220
Iteration 22/1000 | Loss: 0.00002896
Iteration 23/1000 | Loss: 0.00009034
Iteration 24/1000 | Loss: 0.00010164
Iteration 25/1000 | Loss: 0.00002692
Iteration 26/1000 | Loss: 0.00002652
Iteration 27/1000 | Loss: 0.00002617
Iteration 28/1000 | Loss: 0.00010302
Iteration 29/1000 | Loss: 0.00005610
Iteration 30/1000 | Loss: 0.00002584
Iteration 31/1000 | Loss: 0.00002562
Iteration 32/1000 | Loss: 0.00002538
Iteration 33/1000 | Loss: 0.00012749
Iteration 34/1000 | Loss: 0.00009799
Iteration 35/1000 | Loss: 0.00002714
Iteration 36/1000 | Loss: 0.00002464
Iteration 37/1000 | Loss: 0.00002384
Iteration 38/1000 | Loss: 0.00002327
Iteration 39/1000 | Loss: 0.00016073
Iteration 40/1000 | Loss: 0.00003460
Iteration 41/1000 | Loss: 0.00002344
Iteration 42/1000 | Loss: 0.00002246
Iteration 43/1000 | Loss: 0.00002210
Iteration 44/1000 | Loss: 0.00002185
Iteration 45/1000 | Loss: 0.00002162
Iteration 46/1000 | Loss: 0.00002144
Iteration 47/1000 | Loss: 0.00002136
Iteration 48/1000 | Loss: 0.00002135
Iteration 49/1000 | Loss: 0.00002134
Iteration 50/1000 | Loss: 0.00002127
Iteration 51/1000 | Loss: 0.00002124
Iteration 52/1000 | Loss: 0.00002121
Iteration 53/1000 | Loss: 0.00002120
Iteration 54/1000 | Loss: 0.00013457
Iteration 55/1000 | Loss: 0.00003798
Iteration 56/1000 | Loss: 0.00002114
Iteration 57/1000 | Loss: 0.00004063
Iteration 58/1000 | Loss: 0.00002111
Iteration 59/1000 | Loss: 0.00002110
Iteration 60/1000 | Loss: 0.00002110
Iteration 61/1000 | Loss: 0.00002110
Iteration 62/1000 | Loss: 0.00002110
Iteration 63/1000 | Loss: 0.00002110
Iteration 64/1000 | Loss: 0.00002110
Iteration 65/1000 | Loss: 0.00002110
Iteration 66/1000 | Loss: 0.00002110
Iteration 67/1000 | Loss: 0.00002110
Iteration 68/1000 | Loss: 0.00002110
Iteration 69/1000 | Loss: 0.00002110
Iteration 70/1000 | Loss: 0.00002110
Iteration 71/1000 | Loss: 0.00002109
Iteration 72/1000 | Loss: 0.00002109
Iteration 73/1000 | Loss: 0.00002109
Iteration 74/1000 | Loss: 0.00002109
Iteration 75/1000 | Loss: 0.00002109
Iteration 76/1000 | Loss: 0.00002108
Iteration 77/1000 | Loss: 0.00002107
Iteration 78/1000 | Loss: 0.00002107
Iteration 79/1000 | Loss: 0.00002107
Iteration 80/1000 | Loss: 0.00002106
Iteration 81/1000 | Loss: 0.00002106
Iteration 82/1000 | Loss: 0.00002106
Iteration 83/1000 | Loss: 0.00002106
Iteration 84/1000 | Loss: 0.00002106
Iteration 85/1000 | Loss: 0.00002105
Iteration 86/1000 | Loss: 0.00002105
Iteration 87/1000 | Loss: 0.00002105
Iteration 88/1000 | Loss: 0.00002105
Iteration 89/1000 | Loss: 0.00002105
Iteration 90/1000 | Loss: 0.00002104
Iteration 91/1000 | Loss: 0.00002104
Iteration 92/1000 | Loss: 0.00002103
Iteration 93/1000 | Loss: 0.00002103
Iteration 94/1000 | Loss: 0.00002103
Iteration 95/1000 | Loss: 0.00002103
Iteration 96/1000 | Loss: 0.00002102
Iteration 97/1000 | Loss: 0.00002102
Iteration 98/1000 | Loss: 0.00002102
Iteration 99/1000 | Loss: 0.00002102
Iteration 100/1000 | Loss: 0.00002102
Iteration 101/1000 | Loss: 0.00002101
Iteration 102/1000 | Loss: 0.00002101
Iteration 103/1000 | Loss: 0.00002101
Iteration 104/1000 | Loss: 0.00002101
Iteration 105/1000 | Loss: 0.00002101
Iteration 106/1000 | Loss: 0.00002101
Iteration 107/1000 | Loss: 0.00002100
Iteration 108/1000 | Loss: 0.00002100
Iteration 109/1000 | Loss: 0.00002100
Iteration 110/1000 | Loss: 0.00002100
Iteration 111/1000 | Loss: 0.00002099
Iteration 112/1000 | Loss: 0.00002099
Iteration 113/1000 | Loss: 0.00002099
Iteration 114/1000 | Loss: 0.00002099
Iteration 115/1000 | Loss: 0.00002098
Iteration 116/1000 | Loss: 0.00002098
Iteration 117/1000 | Loss: 0.00002098
Iteration 118/1000 | Loss: 0.00002098
Iteration 119/1000 | Loss: 0.00002098
Iteration 120/1000 | Loss: 0.00002098
Iteration 121/1000 | Loss: 0.00002097
Iteration 122/1000 | Loss: 0.00002097
Iteration 123/1000 | Loss: 0.00002097
Iteration 124/1000 | Loss: 0.00002097
Iteration 125/1000 | Loss: 0.00002097
Iteration 126/1000 | Loss: 0.00002096
Iteration 127/1000 | Loss: 0.00002096
Iteration 128/1000 | Loss: 0.00002096
Iteration 129/1000 | Loss: 0.00002096
Iteration 130/1000 | Loss: 0.00002096
Iteration 131/1000 | Loss: 0.00002095
Iteration 132/1000 | Loss: 0.00002095
Iteration 133/1000 | Loss: 0.00002095
Iteration 134/1000 | Loss: 0.00002095
Iteration 135/1000 | Loss: 0.00002095
Iteration 136/1000 | Loss: 0.00002095
Iteration 137/1000 | Loss: 0.00002095
Iteration 138/1000 | Loss: 0.00002094
Iteration 139/1000 | Loss: 0.00002094
Iteration 140/1000 | Loss: 0.00002094
Iteration 141/1000 | Loss: 0.00002094
Iteration 142/1000 | Loss: 0.00002094
Iteration 143/1000 | Loss: 0.00002094
Iteration 144/1000 | Loss: 0.00002094
Iteration 145/1000 | Loss: 0.00002094
Iteration 146/1000 | Loss: 0.00002094
Iteration 147/1000 | Loss: 0.00002094
Iteration 148/1000 | Loss: 0.00002093
Iteration 149/1000 | Loss: 0.00002093
Iteration 150/1000 | Loss: 0.00002093
Iteration 151/1000 | Loss: 0.00002093
Iteration 152/1000 | Loss: 0.00002093
Iteration 153/1000 | Loss: 0.00002093
Iteration 154/1000 | Loss: 0.00002093
Iteration 155/1000 | Loss: 0.00002093
Iteration 156/1000 | Loss: 0.00002093
Iteration 157/1000 | Loss: 0.00002093
Iteration 158/1000 | Loss: 0.00002093
Iteration 159/1000 | Loss: 0.00002093
Iteration 160/1000 | Loss: 0.00002093
Iteration 161/1000 | Loss: 0.00002093
Iteration 162/1000 | Loss: 0.00002093
Iteration 163/1000 | Loss: 0.00002093
Iteration 164/1000 | Loss: 0.00002092
Iteration 165/1000 | Loss: 0.00002092
Iteration 166/1000 | Loss: 0.00002092
Iteration 167/1000 | Loss: 0.00002092
Iteration 168/1000 | Loss: 0.00002092
Iteration 169/1000 | Loss: 0.00002092
Iteration 170/1000 | Loss: 0.00002092
Iteration 171/1000 | Loss: 0.00002092
Iteration 172/1000 | Loss: 0.00002092
Iteration 173/1000 | Loss: 0.00002092
Iteration 174/1000 | Loss: 0.00002092
Iteration 175/1000 | Loss: 0.00002092
Iteration 176/1000 | Loss: 0.00002092
Iteration 177/1000 | Loss: 0.00002092
Iteration 178/1000 | Loss: 0.00002092
Iteration 179/1000 | Loss: 0.00002092
Iteration 180/1000 | Loss: 0.00002092
Iteration 181/1000 | Loss: 0.00002091
Iteration 182/1000 | Loss: 0.00002091
Iteration 183/1000 | Loss: 0.00002091
Iteration 184/1000 | Loss: 0.00002091
Iteration 185/1000 | Loss: 0.00002090
Iteration 186/1000 | Loss: 0.00002090
Iteration 187/1000 | Loss: 0.00002090
Iteration 188/1000 | Loss: 0.00002090
Iteration 189/1000 | Loss: 0.00002090
Iteration 190/1000 | Loss: 0.00002090
Iteration 191/1000 | Loss: 0.00002089
Iteration 192/1000 | Loss: 0.00002089
Iteration 193/1000 | Loss: 0.00002089
Iteration 194/1000 | Loss: 0.00002089
Iteration 195/1000 | Loss: 0.00002089
Iteration 196/1000 | Loss: 0.00002089
Iteration 197/1000 | Loss: 0.00002089
Iteration 198/1000 | Loss: 0.00002089
Iteration 199/1000 | Loss: 0.00002089
Iteration 200/1000 | Loss: 0.00002089
Iteration 201/1000 | Loss: 0.00002089
Iteration 202/1000 | Loss: 0.00002089
Iteration 203/1000 | Loss: 0.00002089
Iteration 204/1000 | Loss: 0.00002089
Iteration 205/1000 | Loss: 0.00002089
Iteration 206/1000 | Loss: 0.00002089
Iteration 207/1000 | Loss: 0.00002089
Iteration 208/1000 | Loss: 0.00002089
Iteration 209/1000 | Loss: 0.00002089
Iteration 210/1000 | Loss: 0.00002089
Iteration 211/1000 | Loss: 0.00002089
Iteration 212/1000 | Loss: 0.00002089
Iteration 213/1000 | Loss: 0.00002089
Iteration 214/1000 | Loss: 0.00002089
Iteration 215/1000 | Loss: 0.00002089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [2.0886247511953115e-05, 2.0886247511953115e-05, 2.0886247511953115e-05, 2.0886247511953115e-05, 2.0886247511953115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0886247511953115e-05

Optimization complete. Final v2v error: 3.795621156692505 mm

Highest mean error: 5.95581579208374 mm for frame 99

Lowest mean error: 3.1709086894989014 mm for frame 18

Saving results

Total time: 104.8461012840271
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404156
Iteration 2/25 | Loss: 0.00134037
Iteration 3/25 | Loss: 0.00123485
Iteration 4/25 | Loss: 0.00122746
Iteration 5/25 | Loss: 0.00122508
Iteration 6/25 | Loss: 0.00122508
Iteration 7/25 | Loss: 0.00122508
Iteration 8/25 | Loss: 0.00122508
Iteration 9/25 | Loss: 0.00122508
Iteration 10/25 | Loss: 0.00122508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001225080224685371, 0.001225080224685371, 0.001225080224685371, 0.001225080224685371, 0.001225080224685371]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001225080224685371

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67525065
Iteration 2/25 | Loss: 0.00070327
Iteration 3/25 | Loss: 0.00070327
Iteration 4/25 | Loss: 0.00070327
Iteration 5/25 | Loss: 0.00070327
Iteration 6/25 | Loss: 0.00070327
Iteration 7/25 | Loss: 0.00070327
Iteration 8/25 | Loss: 0.00070327
Iteration 9/25 | Loss: 0.00070327
Iteration 10/25 | Loss: 0.00070327
Iteration 11/25 | Loss: 0.00070327
Iteration 12/25 | Loss: 0.00070327
Iteration 13/25 | Loss: 0.00070327
Iteration 14/25 | Loss: 0.00070327
Iteration 15/25 | Loss: 0.00070327
Iteration 16/25 | Loss: 0.00070327
Iteration 17/25 | Loss: 0.00070327
Iteration 18/25 | Loss: 0.00070327
Iteration 19/25 | Loss: 0.00070327
Iteration 20/25 | Loss: 0.00070327
Iteration 21/25 | Loss: 0.00070327
Iteration 22/25 | Loss: 0.00070327
Iteration 23/25 | Loss: 0.00070327
Iteration 24/25 | Loss: 0.00070327
Iteration 25/25 | Loss: 0.00070327

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070327
Iteration 2/1000 | Loss: 0.00003077
Iteration 3/1000 | Loss: 0.00001837
Iteration 4/1000 | Loss: 0.00001641
Iteration 5/1000 | Loss: 0.00001503
Iteration 6/1000 | Loss: 0.00001423
Iteration 7/1000 | Loss: 0.00001376
Iteration 8/1000 | Loss: 0.00001349
Iteration 9/1000 | Loss: 0.00001307
Iteration 10/1000 | Loss: 0.00001279
Iteration 11/1000 | Loss: 0.00001277
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001255
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001249
Iteration 16/1000 | Loss: 0.00001243
Iteration 17/1000 | Loss: 0.00001240
Iteration 18/1000 | Loss: 0.00001232
Iteration 19/1000 | Loss: 0.00001232
Iteration 20/1000 | Loss: 0.00001232
Iteration 21/1000 | Loss: 0.00001231
Iteration 22/1000 | Loss: 0.00001231
Iteration 23/1000 | Loss: 0.00001230
Iteration 24/1000 | Loss: 0.00001230
Iteration 25/1000 | Loss: 0.00001229
Iteration 26/1000 | Loss: 0.00001229
Iteration 27/1000 | Loss: 0.00001228
Iteration 28/1000 | Loss: 0.00001227
Iteration 29/1000 | Loss: 0.00001227
Iteration 30/1000 | Loss: 0.00001227
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001227
Iteration 33/1000 | Loss: 0.00001226
Iteration 34/1000 | Loss: 0.00001225
Iteration 35/1000 | Loss: 0.00001225
Iteration 36/1000 | Loss: 0.00001225
Iteration 37/1000 | Loss: 0.00001223
Iteration 38/1000 | Loss: 0.00001223
Iteration 39/1000 | Loss: 0.00001223
Iteration 40/1000 | Loss: 0.00001222
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001222
Iteration 44/1000 | Loss: 0.00001222
Iteration 45/1000 | Loss: 0.00001222
Iteration 46/1000 | Loss: 0.00001222
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001220
Iteration 54/1000 | Loss: 0.00001220
Iteration 55/1000 | Loss: 0.00001219
Iteration 56/1000 | Loss: 0.00001219
Iteration 57/1000 | Loss: 0.00001218
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001218
Iteration 60/1000 | Loss: 0.00001217
Iteration 61/1000 | Loss: 0.00001217
Iteration 62/1000 | Loss: 0.00001217
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001216
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001215
Iteration 67/1000 | Loss: 0.00001215
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001212
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001211
Iteration 83/1000 | Loss: 0.00001211
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001210
Iteration 86/1000 | Loss: 0.00001210
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001209
Iteration 89/1000 | Loss: 0.00001209
Iteration 90/1000 | Loss: 0.00001209
Iteration 91/1000 | Loss: 0.00001208
Iteration 92/1000 | Loss: 0.00001208
Iteration 93/1000 | Loss: 0.00001208
Iteration 94/1000 | Loss: 0.00001208
Iteration 95/1000 | Loss: 0.00001208
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001205
Iteration 107/1000 | Loss: 0.00001205
Iteration 108/1000 | Loss: 0.00001205
Iteration 109/1000 | Loss: 0.00001204
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001204
Iteration 112/1000 | Loss: 0.00001204
Iteration 113/1000 | Loss: 0.00001204
Iteration 114/1000 | Loss: 0.00001204
Iteration 115/1000 | Loss: 0.00001204
Iteration 116/1000 | Loss: 0.00001204
Iteration 117/1000 | Loss: 0.00001204
Iteration 118/1000 | Loss: 0.00001204
Iteration 119/1000 | Loss: 0.00001204
Iteration 120/1000 | Loss: 0.00001204
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001204
Iteration 123/1000 | Loss: 0.00001204
Iteration 124/1000 | Loss: 0.00001204
Iteration 125/1000 | Loss: 0.00001203
Iteration 126/1000 | Loss: 0.00001203
Iteration 127/1000 | Loss: 0.00001203
Iteration 128/1000 | Loss: 0.00001203
Iteration 129/1000 | Loss: 0.00001203
Iteration 130/1000 | Loss: 0.00001203
Iteration 131/1000 | Loss: 0.00001203
Iteration 132/1000 | Loss: 0.00001203
Iteration 133/1000 | Loss: 0.00001203
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001202
Iteration 136/1000 | Loss: 0.00001202
Iteration 137/1000 | Loss: 0.00001202
Iteration 138/1000 | Loss: 0.00001202
Iteration 139/1000 | Loss: 0.00001202
Iteration 140/1000 | Loss: 0.00001202
Iteration 141/1000 | Loss: 0.00001202
Iteration 142/1000 | Loss: 0.00001201
Iteration 143/1000 | Loss: 0.00001201
Iteration 144/1000 | Loss: 0.00001201
Iteration 145/1000 | Loss: 0.00001201
Iteration 146/1000 | Loss: 0.00001201
Iteration 147/1000 | Loss: 0.00001201
Iteration 148/1000 | Loss: 0.00001201
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001200
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001200
Iteration 153/1000 | Loss: 0.00001200
Iteration 154/1000 | Loss: 0.00001200
Iteration 155/1000 | Loss: 0.00001200
Iteration 156/1000 | Loss: 0.00001199
Iteration 157/1000 | Loss: 0.00001199
Iteration 158/1000 | Loss: 0.00001199
Iteration 159/1000 | Loss: 0.00001199
Iteration 160/1000 | Loss: 0.00001199
Iteration 161/1000 | Loss: 0.00001199
Iteration 162/1000 | Loss: 0.00001199
Iteration 163/1000 | Loss: 0.00001199
Iteration 164/1000 | Loss: 0.00001199
Iteration 165/1000 | Loss: 0.00001199
Iteration 166/1000 | Loss: 0.00001199
Iteration 167/1000 | Loss: 0.00001199
Iteration 168/1000 | Loss: 0.00001199
Iteration 169/1000 | Loss: 0.00001199
Iteration 170/1000 | Loss: 0.00001198
Iteration 171/1000 | Loss: 0.00001198
Iteration 172/1000 | Loss: 0.00001198
Iteration 173/1000 | Loss: 0.00001198
Iteration 174/1000 | Loss: 0.00001198
Iteration 175/1000 | Loss: 0.00001198
Iteration 176/1000 | Loss: 0.00001198
Iteration 177/1000 | Loss: 0.00001198
Iteration 178/1000 | Loss: 0.00001198
Iteration 179/1000 | Loss: 0.00001198
Iteration 180/1000 | Loss: 0.00001198
Iteration 181/1000 | Loss: 0.00001198
Iteration 182/1000 | Loss: 0.00001197
Iteration 183/1000 | Loss: 0.00001197
Iteration 184/1000 | Loss: 0.00001197
Iteration 185/1000 | Loss: 0.00001197
Iteration 186/1000 | Loss: 0.00001197
Iteration 187/1000 | Loss: 0.00001197
Iteration 188/1000 | Loss: 0.00001197
Iteration 189/1000 | Loss: 0.00001197
Iteration 190/1000 | Loss: 0.00001197
Iteration 191/1000 | Loss: 0.00001197
Iteration 192/1000 | Loss: 0.00001197
Iteration 193/1000 | Loss: 0.00001197
Iteration 194/1000 | Loss: 0.00001197
Iteration 195/1000 | Loss: 0.00001197
Iteration 196/1000 | Loss: 0.00001197
Iteration 197/1000 | Loss: 0.00001197
Iteration 198/1000 | Loss: 0.00001197
Iteration 199/1000 | Loss: 0.00001197
Iteration 200/1000 | Loss: 0.00001197
Iteration 201/1000 | Loss: 0.00001197
Iteration 202/1000 | Loss: 0.00001197
Iteration 203/1000 | Loss: 0.00001197
Iteration 204/1000 | Loss: 0.00001197
Iteration 205/1000 | Loss: 0.00001197
Iteration 206/1000 | Loss: 0.00001197
Iteration 207/1000 | Loss: 0.00001197
Iteration 208/1000 | Loss: 0.00001197
Iteration 209/1000 | Loss: 0.00001197
Iteration 210/1000 | Loss: 0.00001197
Iteration 211/1000 | Loss: 0.00001197
Iteration 212/1000 | Loss: 0.00001197
Iteration 213/1000 | Loss: 0.00001197
Iteration 214/1000 | Loss: 0.00001197
Iteration 215/1000 | Loss: 0.00001197
Iteration 216/1000 | Loss: 0.00001197
Iteration 217/1000 | Loss: 0.00001197
Iteration 218/1000 | Loss: 0.00001197
Iteration 219/1000 | Loss: 0.00001197
Iteration 220/1000 | Loss: 0.00001197
Iteration 221/1000 | Loss: 0.00001197
Iteration 222/1000 | Loss: 0.00001197
Iteration 223/1000 | Loss: 0.00001197
Iteration 224/1000 | Loss: 0.00001197
Iteration 225/1000 | Loss: 0.00001197
Iteration 226/1000 | Loss: 0.00001197
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.196939865621971e-05, 1.196939865621971e-05, 1.196939865621971e-05, 1.196939865621971e-05, 1.196939865621971e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.196939865621971e-05

Optimization complete. Final v2v error: 2.9615111351013184 mm

Highest mean error: 3.2197768688201904 mm for frame 105

Lowest mean error: 2.8170981407165527 mm for frame 194

Saving results

Total time: 45.214810371398926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00827813
Iteration 2/25 | Loss: 0.00154751
Iteration 3/25 | Loss: 0.00136561
Iteration 4/25 | Loss: 0.00134257
Iteration 5/25 | Loss: 0.00133912
Iteration 6/25 | Loss: 0.00133860
Iteration 7/25 | Loss: 0.00133860
Iteration 8/25 | Loss: 0.00133860
Iteration 9/25 | Loss: 0.00133860
Iteration 10/25 | Loss: 0.00133860
Iteration 11/25 | Loss: 0.00133860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001338602160103619, 0.001338602160103619, 0.001338602160103619, 0.001338602160103619, 0.001338602160103619]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001338602160103619

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.64798319
Iteration 2/25 | Loss: 0.00099161
Iteration 3/25 | Loss: 0.00099155
Iteration 4/25 | Loss: 0.00099155
Iteration 5/25 | Loss: 0.00099155
Iteration 6/25 | Loss: 0.00099155
Iteration 7/25 | Loss: 0.00099155
Iteration 8/25 | Loss: 0.00099155
Iteration 9/25 | Loss: 0.00099155
Iteration 10/25 | Loss: 0.00099155
Iteration 11/25 | Loss: 0.00099155
Iteration 12/25 | Loss: 0.00099155
Iteration 13/25 | Loss: 0.00099155
Iteration 14/25 | Loss: 0.00099155
Iteration 15/25 | Loss: 0.00099155
Iteration 16/25 | Loss: 0.00099155
Iteration 17/25 | Loss: 0.00099155
Iteration 18/25 | Loss: 0.00099155
Iteration 19/25 | Loss: 0.00099155
Iteration 20/25 | Loss: 0.00099155
Iteration 21/25 | Loss: 0.00099155
Iteration 22/25 | Loss: 0.00099155
Iteration 23/25 | Loss: 0.00099155
Iteration 24/25 | Loss: 0.00099155
Iteration 25/25 | Loss: 0.00099155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099155
Iteration 2/1000 | Loss: 0.00008816
Iteration 3/1000 | Loss: 0.00005469
Iteration 4/1000 | Loss: 0.00004143
Iteration 5/1000 | Loss: 0.00003774
Iteration 6/1000 | Loss: 0.00003587
Iteration 7/1000 | Loss: 0.00003434
Iteration 8/1000 | Loss: 0.00003312
Iteration 9/1000 | Loss: 0.00003232
Iteration 10/1000 | Loss: 0.00003170
Iteration 11/1000 | Loss: 0.00003116
Iteration 12/1000 | Loss: 0.00003086
Iteration 13/1000 | Loss: 0.00003052
Iteration 14/1000 | Loss: 0.00003026
Iteration 15/1000 | Loss: 0.00003005
Iteration 16/1000 | Loss: 0.00002992
Iteration 17/1000 | Loss: 0.00002976
Iteration 18/1000 | Loss: 0.00002959
Iteration 19/1000 | Loss: 0.00002948
Iteration 20/1000 | Loss: 0.00002939
Iteration 21/1000 | Loss: 0.00002936
Iteration 22/1000 | Loss: 0.00002933
Iteration 23/1000 | Loss: 0.00002933
Iteration 24/1000 | Loss: 0.00002933
Iteration 25/1000 | Loss: 0.00002931
Iteration 26/1000 | Loss: 0.00002931
Iteration 27/1000 | Loss: 0.00002930
Iteration 28/1000 | Loss: 0.00002930
Iteration 29/1000 | Loss: 0.00002929
Iteration 30/1000 | Loss: 0.00002929
Iteration 31/1000 | Loss: 0.00002929
Iteration 32/1000 | Loss: 0.00002928
Iteration 33/1000 | Loss: 0.00002928
Iteration 34/1000 | Loss: 0.00002927
Iteration 35/1000 | Loss: 0.00002927
Iteration 36/1000 | Loss: 0.00002927
Iteration 37/1000 | Loss: 0.00002926
Iteration 38/1000 | Loss: 0.00002926
Iteration 39/1000 | Loss: 0.00002925
Iteration 40/1000 | Loss: 0.00002925
Iteration 41/1000 | Loss: 0.00002924
Iteration 42/1000 | Loss: 0.00002924
Iteration 43/1000 | Loss: 0.00002923
Iteration 44/1000 | Loss: 0.00002923
Iteration 45/1000 | Loss: 0.00002923
Iteration 46/1000 | Loss: 0.00002922
Iteration 47/1000 | Loss: 0.00002922
Iteration 48/1000 | Loss: 0.00002922
Iteration 49/1000 | Loss: 0.00002922
Iteration 50/1000 | Loss: 0.00002921
Iteration 51/1000 | Loss: 0.00002921
Iteration 52/1000 | Loss: 0.00002921
Iteration 53/1000 | Loss: 0.00002921
Iteration 54/1000 | Loss: 0.00002921
Iteration 55/1000 | Loss: 0.00002920
Iteration 56/1000 | Loss: 0.00002920
Iteration 57/1000 | Loss: 0.00002920
Iteration 58/1000 | Loss: 0.00002920
Iteration 59/1000 | Loss: 0.00002920
Iteration 60/1000 | Loss: 0.00002919
Iteration 61/1000 | Loss: 0.00002919
Iteration 62/1000 | Loss: 0.00002919
Iteration 63/1000 | Loss: 0.00002919
Iteration 64/1000 | Loss: 0.00002919
Iteration 65/1000 | Loss: 0.00002919
Iteration 66/1000 | Loss: 0.00002919
Iteration 67/1000 | Loss: 0.00002919
Iteration 68/1000 | Loss: 0.00002919
Iteration 69/1000 | Loss: 0.00002918
Iteration 70/1000 | Loss: 0.00002918
Iteration 71/1000 | Loss: 0.00002918
Iteration 72/1000 | Loss: 0.00002918
Iteration 73/1000 | Loss: 0.00002918
Iteration 74/1000 | Loss: 0.00002918
Iteration 75/1000 | Loss: 0.00002918
Iteration 76/1000 | Loss: 0.00002918
Iteration 77/1000 | Loss: 0.00002917
Iteration 78/1000 | Loss: 0.00002917
Iteration 79/1000 | Loss: 0.00002917
Iteration 80/1000 | Loss: 0.00002917
Iteration 81/1000 | Loss: 0.00002917
Iteration 82/1000 | Loss: 0.00002917
Iteration 83/1000 | Loss: 0.00002917
Iteration 84/1000 | Loss: 0.00002917
Iteration 85/1000 | Loss: 0.00002917
Iteration 86/1000 | Loss: 0.00002917
Iteration 87/1000 | Loss: 0.00002917
Iteration 88/1000 | Loss: 0.00002917
Iteration 89/1000 | Loss: 0.00002916
Iteration 90/1000 | Loss: 0.00002916
Iteration 91/1000 | Loss: 0.00002916
Iteration 92/1000 | Loss: 0.00002916
Iteration 93/1000 | Loss: 0.00002916
Iteration 94/1000 | Loss: 0.00002916
Iteration 95/1000 | Loss: 0.00002916
Iteration 96/1000 | Loss: 0.00002916
Iteration 97/1000 | Loss: 0.00002916
Iteration 98/1000 | Loss: 0.00002916
Iteration 99/1000 | Loss: 0.00002916
Iteration 100/1000 | Loss: 0.00002916
Iteration 101/1000 | Loss: 0.00002916
Iteration 102/1000 | Loss: 0.00002916
Iteration 103/1000 | Loss: 0.00002915
Iteration 104/1000 | Loss: 0.00002915
Iteration 105/1000 | Loss: 0.00002915
Iteration 106/1000 | Loss: 0.00002915
Iteration 107/1000 | Loss: 0.00002915
Iteration 108/1000 | Loss: 0.00002915
Iteration 109/1000 | Loss: 0.00002915
Iteration 110/1000 | Loss: 0.00002914
Iteration 111/1000 | Loss: 0.00002914
Iteration 112/1000 | Loss: 0.00002914
Iteration 113/1000 | Loss: 0.00002914
Iteration 114/1000 | Loss: 0.00002914
Iteration 115/1000 | Loss: 0.00002913
Iteration 116/1000 | Loss: 0.00002913
Iteration 117/1000 | Loss: 0.00002913
Iteration 118/1000 | Loss: 0.00002913
Iteration 119/1000 | Loss: 0.00002913
Iteration 120/1000 | Loss: 0.00002913
Iteration 121/1000 | Loss: 0.00002913
Iteration 122/1000 | Loss: 0.00002913
Iteration 123/1000 | Loss: 0.00002913
Iteration 124/1000 | Loss: 0.00002913
Iteration 125/1000 | Loss: 0.00002913
Iteration 126/1000 | Loss: 0.00002912
Iteration 127/1000 | Loss: 0.00002912
Iteration 128/1000 | Loss: 0.00002912
Iteration 129/1000 | Loss: 0.00002912
Iteration 130/1000 | Loss: 0.00002912
Iteration 131/1000 | Loss: 0.00002912
Iteration 132/1000 | Loss: 0.00002912
Iteration 133/1000 | Loss: 0.00002912
Iteration 134/1000 | Loss: 0.00002912
Iteration 135/1000 | Loss: 0.00002912
Iteration 136/1000 | Loss: 0.00002912
Iteration 137/1000 | Loss: 0.00002911
Iteration 138/1000 | Loss: 0.00002911
Iteration 139/1000 | Loss: 0.00002911
Iteration 140/1000 | Loss: 0.00002911
Iteration 141/1000 | Loss: 0.00002911
Iteration 142/1000 | Loss: 0.00002911
Iteration 143/1000 | Loss: 0.00002911
Iteration 144/1000 | Loss: 0.00002911
Iteration 145/1000 | Loss: 0.00002911
Iteration 146/1000 | Loss: 0.00002911
Iteration 147/1000 | Loss: 0.00002911
Iteration 148/1000 | Loss: 0.00002911
Iteration 149/1000 | Loss: 0.00002911
Iteration 150/1000 | Loss: 0.00002911
Iteration 151/1000 | Loss: 0.00002911
Iteration 152/1000 | Loss: 0.00002911
Iteration 153/1000 | Loss: 0.00002911
Iteration 154/1000 | Loss: 0.00002911
Iteration 155/1000 | Loss: 0.00002911
Iteration 156/1000 | Loss: 0.00002911
Iteration 157/1000 | Loss: 0.00002911
Iteration 158/1000 | Loss: 0.00002911
Iteration 159/1000 | Loss: 0.00002911
Iteration 160/1000 | Loss: 0.00002911
Iteration 161/1000 | Loss: 0.00002911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.91126525553409e-05, 2.91126525553409e-05, 2.91126525553409e-05, 2.91126525553409e-05, 2.91126525553409e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.91126525553409e-05

Optimization complete. Final v2v error: 4.435770034790039 mm

Highest mean error: 5.6952714920043945 mm for frame 163

Lowest mean error: 3.528237819671631 mm for frame 78

Saving results

Total time: 44.1214234828949
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404799
Iteration 2/25 | Loss: 0.00128615
Iteration 3/25 | Loss: 0.00120247
Iteration 4/25 | Loss: 0.00119438
Iteration 5/25 | Loss: 0.00119263
Iteration 6/25 | Loss: 0.00119227
Iteration 7/25 | Loss: 0.00119227
Iteration 8/25 | Loss: 0.00119227
Iteration 9/25 | Loss: 0.00119227
Iteration 10/25 | Loss: 0.00119227
Iteration 11/25 | Loss: 0.00119227
Iteration 12/25 | Loss: 0.00119227
Iteration 13/25 | Loss: 0.00119227
Iteration 14/25 | Loss: 0.00119227
Iteration 15/25 | Loss: 0.00119227
Iteration 16/25 | Loss: 0.00119227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011922724079340696, 0.0011922724079340696, 0.0011922724079340696, 0.0011922724079340696, 0.0011922724079340696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011922724079340696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43792200
Iteration 2/25 | Loss: 0.00078904
Iteration 3/25 | Loss: 0.00078904
Iteration 4/25 | Loss: 0.00078904
Iteration 5/25 | Loss: 0.00078904
Iteration 6/25 | Loss: 0.00078904
Iteration 7/25 | Loss: 0.00078904
Iteration 8/25 | Loss: 0.00078904
Iteration 9/25 | Loss: 0.00078904
Iteration 10/25 | Loss: 0.00078904
Iteration 11/25 | Loss: 0.00078904
Iteration 12/25 | Loss: 0.00078904
Iteration 13/25 | Loss: 0.00078904
Iteration 14/25 | Loss: 0.00078904
Iteration 15/25 | Loss: 0.00078904
Iteration 16/25 | Loss: 0.00078904
Iteration 17/25 | Loss: 0.00078904
Iteration 18/25 | Loss: 0.00078904
Iteration 19/25 | Loss: 0.00078904
Iteration 20/25 | Loss: 0.00078904
Iteration 21/25 | Loss: 0.00078904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007890370325185359, 0.0007890370325185359, 0.0007890370325185359, 0.0007890370325185359, 0.0007890370325185359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007890370325185359

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078904
Iteration 2/1000 | Loss: 0.00002916
Iteration 3/1000 | Loss: 0.00001627
Iteration 4/1000 | Loss: 0.00001391
Iteration 5/1000 | Loss: 0.00001274
Iteration 6/1000 | Loss: 0.00001198
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001121
Iteration 9/1000 | Loss: 0.00001121
Iteration 10/1000 | Loss: 0.00001117
Iteration 11/1000 | Loss: 0.00001117
Iteration 12/1000 | Loss: 0.00001116
Iteration 13/1000 | Loss: 0.00001116
Iteration 14/1000 | Loss: 0.00001116
Iteration 15/1000 | Loss: 0.00001116
Iteration 16/1000 | Loss: 0.00001116
Iteration 17/1000 | Loss: 0.00001116
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001116
Iteration 20/1000 | Loss: 0.00001115
Iteration 21/1000 | Loss: 0.00001115
Iteration 22/1000 | Loss: 0.00001114
Iteration 23/1000 | Loss: 0.00001112
Iteration 24/1000 | Loss: 0.00001111
Iteration 25/1000 | Loss: 0.00001109
Iteration 26/1000 | Loss: 0.00001108
Iteration 27/1000 | Loss: 0.00001108
Iteration 28/1000 | Loss: 0.00001108
Iteration 29/1000 | Loss: 0.00001107
Iteration 30/1000 | Loss: 0.00001097
Iteration 31/1000 | Loss: 0.00001096
Iteration 32/1000 | Loss: 0.00001096
Iteration 33/1000 | Loss: 0.00001096
Iteration 34/1000 | Loss: 0.00001096
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001092
Iteration 38/1000 | Loss: 0.00001088
Iteration 39/1000 | Loss: 0.00001087
Iteration 40/1000 | Loss: 0.00001087
Iteration 41/1000 | Loss: 0.00001087
Iteration 42/1000 | Loss: 0.00001087
Iteration 43/1000 | Loss: 0.00001086
Iteration 44/1000 | Loss: 0.00001086
Iteration 45/1000 | Loss: 0.00001086
Iteration 46/1000 | Loss: 0.00001086
Iteration 47/1000 | Loss: 0.00001085
Iteration 48/1000 | Loss: 0.00001085
Iteration 49/1000 | Loss: 0.00001084
Iteration 50/1000 | Loss: 0.00001081
Iteration 51/1000 | Loss: 0.00001080
Iteration 52/1000 | Loss: 0.00001077
Iteration 53/1000 | Loss: 0.00001077
Iteration 54/1000 | Loss: 0.00001076
Iteration 55/1000 | Loss: 0.00001076
Iteration 56/1000 | Loss: 0.00001076
Iteration 57/1000 | Loss: 0.00001075
Iteration 58/1000 | Loss: 0.00001075
Iteration 59/1000 | Loss: 0.00001075
Iteration 60/1000 | Loss: 0.00001075
Iteration 61/1000 | Loss: 0.00001075
Iteration 62/1000 | Loss: 0.00001074
Iteration 63/1000 | Loss: 0.00001074
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001071
Iteration 67/1000 | Loss: 0.00001071
Iteration 68/1000 | Loss: 0.00001070
Iteration 69/1000 | Loss: 0.00001070
Iteration 70/1000 | Loss: 0.00001069
Iteration 71/1000 | Loss: 0.00001069
Iteration 72/1000 | Loss: 0.00001069
Iteration 73/1000 | Loss: 0.00001068
Iteration 74/1000 | Loss: 0.00001068
Iteration 75/1000 | Loss: 0.00001068
Iteration 76/1000 | Loss: 0.00001068
Iteration 77/1000 | Loss: 0.00001067
Iteration 78/1000 | Loss: 0.00001067
Iteration 79/1000 | Loss: 0.00001066
Iteration 80/1000 | Loss: 0.00001066
Iteration 81/1000 | Loss: 0.00001066
Iteration 82/1000 | Loss: 0.00001065
Iteration 83/1000 | Loss: 0.00001065
Iteration 84/1000 | Loss: 0.00001065
Iteration 85/1000 | Loss: 0.00001065
Iteration 86/1000 | Loss: 0.00001065
Iteration 87/1000 | Loss: 0.00001064
Iteration 88/1000 | Loss: 0.00001063
Iteration 89/1000 | Loss: 0.00001062
Iteration 90/1000 | Loss: 0.00001061
Iteration 91/1000 | Loss: 0.00001061
Iteration 92/1000 | Loss: 0.00001061
Iteration 93/1000 | Loss: 0.00001060
Iteration 94/1000 | Loss: 0.00001060
Iteration 95/1000 | Loss: 0.00001060
Iteration 96/1000 | Loss: 0.00001060
Iteration 97/1000 | Loss: 0.00001059
Iteration 98/1000 | Loss: 0.00001059
Iteration 99/1000 | Loss: 0.00001059
Iteration 100/1000 | Loss: 0.00001058
Iteration 101/1000 | Loss: 0.00001058
Iteration 102/1000 | Loss: 0.00001058
Iteration 103/1000 | Loss: 0.00001058
Iteration 104/1000 | Loss: 0.00001057
Iteration 105/1000 | Loss: 0.00001057
Iteration 106/1000 | Loss: 0.00001057
Iteration 107/1000 | Loss: 0.00001057
Iteration 108/1000 | Loss: 0.00001057
Iteration 109/1000 | Loss: 0.00001057
Iteration 110/1000 | Loss: 0.00001056
Iteration 111/1000 | Loss: 0.00001056
Iteration 112/1000 | Loss: 0.00001056
Iteration 113/1000 | Loss: 0.00001056
Iteration 114/1000 | Loss: 0.00001056
Iteration 115/1000 | Loss: 0.00001056
Iteration 116/1000 | Loss: 0.00001056
Iteration 117/1000 | Loss: 0.00001055
Iteration 118/1000 | Loss: 0.00001055
Iteration 119/1000 | Loss: 0.00001055
Iteration 120/1000 | Loss: 0.00001055
Iteration 121/1000 | Loss: 0.00001055
Iteration 122/1000 | Loss: 0.00001055
Iteration 123/1000 | Loss: 0.00001055
Iteration 124/1000 | Loss: 0.00001055
Iteration 125/1000 | Loss: 0.00001055
Iteration 126/1000 | Loss: 0.00001055
Iteration 127/1000 | Loss: 0.00001055
Iteration 128/1000 | Loss: 0.00001055
Iteration 129/1000 | Loss: 0.00001055
Iteration 130/1000 | Loss: 0.00001055
Iteration 131/1000 | Loss: 0.00001055
Iteration 132/1000 | Loss: 0.00001054
Iteration 133/1000 | Loss: 0.00001054
Iteration 134/1000 | Loss: 0.00001053
Iteration 135/1000 | Loss: 0.00001053
Iteration 136/1000 | Loss: 0.00001053
Iteration 137/1000 | Loss: 0.00001053
Iteration 138/1000 | Loss: 0.00001053
Iteration 139/1000 | Loss: 0.00001053
Iteration 140/1000 | Loss: 0.00001053
Iteration 141/1000 | Loss: 0.00001052
Iteration 142/1000 | Loss: 0.00001052
Iteration 143/1000 | Loss: 0.00001052
Iteration 144/1000 | Loss: 0.00001052
Iteration 145/1000 | Loss: 0.00001052
Iteration 146/1000 | Loss: 0.00001052
Iteration 147/1000 | Loss: 0.00001052
Iteration 148/1000 | Loss: 0.00001051
Iteration 149/1000 | Loss: 0.00001051
Iteration 150/1000 | Loss: 0.00001051
Iteration 151/1000 | Loss: 0.00001050
Iteration 152/1000 | Loss: 0.00001050
Iteration 153/1000 | Loss: 0.00001050
Iteration 154/1000 | Loss: 0.00001049
Iteration 155/1000 | Loss: 0.00001049
Iteration 156/1000 | Loss: 0.00001049
Iteration 157/1000 | Loss: 0.00001048
Iteration 158/1000 | Loss: 0.00001048
Iteration 159/1000 | Loss: 0.00001048
Iteration 160/1000 | Loss: 0.00001047
Iteration 161/1000 | Loss: 0.00001047
Iteration 162/1000 | Loss: 0.00001047
Iteration 163/1000 | Loss: 0.00001047
Iteration 164/1000 | Loss: 0.00001046
Iteration 165/1000 | Loss: 0.00001046
Iteration 166/1000 | Loss: 0.00001045
Iteration 167/1000 | Loss: 0.00001045
Iteration 168/1000 | Loss: 0.00001045
Iteration 169/1000 | Loss: 0.00001045
Iteration 170/1000 | Loss: 0.00001045
Iteration 171/1000 | Loss: 0.00001044
Iteration 172/1000 | Loss: 0.00001044
Iteration 173/1000 | Loss: 0.00001044
Iteration 174/1000 | Loss: 0.00001044
Iteration 175/1000 | Loss: 0.00001044
Iteration 176/1000 | Loss: 0.00001044
Iteration 177/1000 | Loss: 0.00001044
Iteration 178/1000 | Loss: 0.00001043
Iteration 179/1000 | Loss: 0.00001043
Iteration 180/1000 | Loss: 0.00001042
Iteration 181/1000 | Loss: 0.00001042
Iteration 182/1000 | Loss: 0.00001042
Iteration 183/1000 | Loss: 0.00001041
Iteration 184/1000 | Loss: 0.00001041
Iteration 185/1000 | Loss: 0.00001041
Iteration 186/1000 | Loss: 0.00001041
Iteration 187/1000 | Loss: 0.00001041
Iteration 188/1000 | Loss: 0.00001041
Iteration 189/1000 | Loss: 0.00001040
Iteration 190/1000 | Loss: 0.00001040
Iteration 191/1000 | Loss: 0.00001040
Iteration 192/1000 | Loss: 0.00001040
Iteration 193/1000 | Loss: 0.00001040
Iteration 194/1000 | Loss: 0.00001040
Iteration 195/1000 | Loss: 0.00001040
Iteration 196/1000 | Loss: 0.00001040
Iteration 197/1000 | Loss: 0.00001040
Iteration 198/1000 | Loss: 0.00001040
Iteration 199/1000 | Loss: 0.00001040
Iteration 200/1000 | Loss: 0.00001040
Iteration 201/1000 | Loss: 0.00001039
Iteration 202/1000 | Loss: 0.00001039
Iteration 203/1000 | Loss: 0.00001039
Iteration 204/1000 | Loss: 0.00001039
Iteration 205/1000 | Loss: 0.00001039
Iteration 206/1000 | Loss: 0.00001039
Iteration 207/1000 | Loss: 0.00001038
Iteration 208/1000 | Loss: 0.00001038
Iteration 209/1000 | Loss: 0.00001038
Iteration 210/1000 | Loss: 0.00001038
Iteration 211/1000 | Loss: 0.00001038
Iteration 212/1000 | Loss: 0.00001038
Iteration 213/1000 | Loss: 0.00001038
Iteration 214/1000 | Loss: 0.00001037
Iteration 215/1000 | Loss: 0.00001037
Iteration 216/1000 | Loss: 0.00001037
Iteration 217/1000 | Loss: 0.00001037
Iteration 218/1000 | Loss: 0.00001037
Iteration 219/1000 | Loss: 0.00001037
Iteration 220/1000 | Loss: 0.00001037
Iteration 221/1000 | Loss: 0.00001037
Iteration 222/1000 | Loss: 0.00001037
Iteration 223/1000 | Loss: 0.00001037
Iteration 224/1000 | Loss: 0.00001037
Iteration 225/1000 | Loss: 0.00001037
Iteration 226/1000 | Loss: 0.00001037
Iteration 227/1000 | Loss: 0.00001037
Iteration 228/1000 | Loss: 0.00001037
Iteration 229/1000 | Loss: 0.00001037
Iteration 230/1000 | Loss: 0.00001037
Iteration 231/1000 | Loss: 0.00001037
Iteration 232/1000 | Loss: 0.00001037
Iteration 233/1000 | Loss: 0.00001037
Iteration 234/1000 | Loss: 0.00001037
Iteration 235/1000 | Loss: 0.00001037
Iteration 236/1000 | Loss: 0.00001037
Iteration 237/1000 | Loss: 0.00001037
Iteration 238/1000 | Loss: 0.00001037
Iteration 239/1000 | Loss: 0.00001037
Iteration 240/1000 | Loss: 0.00001037
Iteration 241/1000 | Loss: 0.00001037
Iteration 242/1000 | Loss: 0.00001037
Iteration 243/1000 | Loss: 0.00001037
Iteration 244/1000 | Loss: 0.00001037
Iteration 245/1000 | Loss: 0.00001037
Iteration 246/1000 | Loss: 0.00001037
Iteration 247/1000 | Loss: 0.00001037
Iteration 248/1000 | Loss: 0.00001037
Iteration 249/1000 | Loss: 0.00001037
Iteration 250/1000 | Loss: 0.00001037
Iteration 251/1000 | Loss: 0.00001037
Iteration 252/1000 | Loss: 0.00001037
Iteration 253/1000 | Loss: 0.00001037
Iteration 254/1000 | Loss: 0.00001037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.0367330105509609e-05, 1.0367330105509609e-05, 1.0367330105509609e-05, 1.0367330105509609e-05, 1.0367330105509609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0367330105509609e-05

Optimization complete. Final v2v error: 2.774460792541504 mm

Highest mean error: 2.7979912757873535 mm for frame 123

Lowest mean error: 2.734506607055664 mm for frame 52

Saving results

Total time: 38.45251107215881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020475
Iteration 2/25 | Loss: 0.00181518
Iteration 3/25 | Loss: 0.00142614
Iteration 4/25 | Loss: 0.00139605
Iteration 5/25 | Loss: 0.00138679
Iteration 6/25 | Loss: 0.00138447
Iteration 7/25 | Loss: 0.00138447
Iteration 8/25 | Loss: 0.00138447
Iteration 9/25 | Loss: 0.00138447
Iteration 10/25 | Loss: 0.00138447
Iteration 11/25 | Loss: 0.00138447
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013844718923792243, 0.0013844718923792243, 0.0013844718923792243, 0.0013844718923792243, 0.0013844718923792243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013844718923792243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94096637
Iteration 2/25 | Loss: 0.00103872
Iteration 3/25 | Loss: 0.00103870
Iteration 4/25 | Loss: 0.00103869
Iteration 5/25 | Loss: 0.00103869
Iteration 6/25 | Loss: 0.00103869
Iteration 7/25 | Loss: 0.00103869
Iteration 8/25 | Loss: 0.00103869
Iteration 9/25 | Loss: 0.00103869
Iteration 10/25 | Loss: 0.00103869
Iteration 11/25 | Loss: 0.00103869
Iteration 12/25 | Loss: 0.00103869
Iteration 13/25 | Loss: 0.00103869
Iteration 14/25 | Loss: 0.00103869
Iteration 15/25 | Loss: 0.00103869
Iteration 16/25 | Loss: 0.00103869
Iteration 17/25 | Loss: 0.00103869
Iteration 18/25 | Loss: 0.00103869
Iteration 19/25 | Loss: 0.00103869
Iteration 20/25 | Loss: 0.00103869
Iteration 21/25 | Loss: 0.00103869
Iteration 22/25 | Loss: 0.00103869
Iteration 23/25 | Loss: 0.00103869
Iteration 24/25 | Loss: 0.00103869
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0010386923095211387, 0.0010386923095211387, 0.0010386923095211387, 0.0010386923095211387, 0.0010386923095211387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010386923095211387

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103869
Iteration 2/1000 | Loss: 0.00008429
Iteration 3/1000 | Loss: 0.00004458
Iteration 4/1000 | Loss: 0.00003441
Iteration 5/1000 | Loss: 0.00003192
Iteration 6/1000 | Loss: 0.00003055
Iteration 7/1000 | Loss: 0.00002960
Iteration 8/1000 | Loss: 0.00002885
Iteration 9/1000 | Loss: 0.00002840
Iteration 10/1000 | Loss: 0.00002793
Iteration 11/1000 | Loss: 0.00002766
Iteration 12/1000 | Loss: 0.00002741
Iteration 13/1000 | Loss: 0.00002715
Iteration 14/1000 | Loss: 0.00002696
Iteration 15/1000 | Loss: 0.00002688
Iteration 16/1000 | Loss: 0.00002686
Iteration 17/1000 | Loss: 0.00002678
Iteration 18/1000 | Loss: 0.00002669
Iteration 19/1000 | Loss: 0.00002664
Iteration 20/1000 | Loss: 0.00002664
Iteration 21/1000 | Loss: 0.00002663
Iteration 22/1000 | Loss: 0.00002662
Iteration 23/1000 | Loss: 0.00002662
Iteration 24/1000 | Loss: 0.00002661
Iteration 25/1000 | Loss: 0.00002661
Iteration 26/1000 | Loss: 0.00002661
Iteration 27/1000 | Loss: 0.00002661
Iteration 28/1000 | Loss: 0.00002660
Iteration 29/1000 | Loss: 0.00002656
Iteration 30/1000 | Loss: 0.00002651
Iteration 31/1000 | Loss: 0.00002649
Iteration 32/1000 | Loss: 0.00002648
Iteration 33/1000 | Loss: 0.00002647
Iteration 34/1000 | Loss: 0.00002646
Iteration 35/1000 | Loss: 0.00002642
Iteration 36/1000 | Loss: 0.00002642
Iteration 37/1000 | Loss: 0.00002639
Iteration 38/1000 | Loss: 0.00002639
Iteration 39/1000 | Loss: 0.00002639
Iteration 40/1000 | Loss: 0.00002639
Iteration 41/1000 | Loss: 0.00002639
Iteration 42/1000 | Loss: 0.00002638
Iteration 43/1000 | Loss: 0.00002638
Iteration 44/1000 | Loss: 0.00002638
Iteration 45/1000 | Loss: 0.00002637
Iteration 46/1000 | Loss: 0.00002637
Iteration 47/1000 | Loss: 0.00002636
Iteration 48/1000 | Loss: 0.00002635
Iteration 49/1000 | Loss: 0.00002635
Iteration 50/1000 | Loss: 0.00002635
Iteration 51/1000 | Loss: 0.00002635
Iteration 52/1000 | Loss: 0.00002634
Iteration 53/1000 | Loss: 0.00002634
Iteration 54/1000 | Loss: 0.00002633
Iteration 55/1000 | Loss: 0.00002633
Iteration 56/1000 | Loss: 0.00002632
Iteration 57/1000 | Loss: 0.00002632
Iteration 58/1000 | Loss: 0.00002631
Iteration 59/1000 | Loss: 0.00002630
Iteration 60/1000 | Loss: 0.00002630
Iteration 61/1000 | Loss: 0.00002630
Iteration 62/1000 | Loss: 0.00002630
Iteration 63/1000 | Loss: 0.00002630
Iteration 64/1000 | Loss: 0.00002630
Iteration 65/1000 | Loss: 0.00002630
Iteration 66/1000 | Loss: 0.00002630
Iteration 67/1000 | Loss: 0.00002630
Iteration 68/1000 | Loss: 0.00002629
Iteration 69/1000 | Loss: 0.00002629
Iteration 70/1000 | Loss: 0.00002629
Iteration 71/1000 | Loss: 0.00002629
Iteration 72/1000 | Loss: 0.00002629
Iteration 73/1000 | Loss: 0.00002629
Iteration 74/1000 | Loss: 0.00002628
Iteration 75/1000 | Loss: 0.00002628
Iteration 76/1000 | Loss: 0.00002627
Iteration 77/1000 | Loss: 0.00002626
Iteration 78/1000 | Loss: 0.00002626
Iteration 79/1000 | Loss: 0.00002626
Iteration 80/1000 | Loss: 0.00002625
Iteration 81/1000 | Loss: 0.00002625
Iteration 82/1000 | Loss: 0.00002625
Iteration 83/1000 | Loss: 0.00002625
Iteration 84/1000 | Loss: 0.00002624
Iteration 85/1000 | Loss: 0.00002624
Iteration 86/1000 | Loss: 0.00002624
Iteration 87/1000 | Loss: 0.00002623
Iteration 88/1000 | Loss: 0.00002623
Iteration 89/1000 | Loss: 0.00002623
Iteration 90/1000 | Loss: 0.00002622
Iteration 91/1000 | Loss: 0.00002622
Iteration 92/1000 | Loss: 0.00002622
Iteration 93/1000 | Loss: 0.00002622
Iteration 94/1000 | Loss: 0.00002622
Iteration 95/1000 | Loss: 0.00002622
Iteration 96/1000 | Loss: 0.00002622
Iteration 97/1000 | Loss: 0.00002622
Iteration 98/1000 | Loss: 0.00002622
Iteration 99/1000 | Loss: 0.00002622
Iteration 100/1000 | Loss: 0.00002621
Iteration 101/1000 | Loss: 0.00002621
Iteration 102/1000 | Loss: 0.00002621
Iteration 103/1000 | Loss: 0.00002621
Iteration 104/1000 | Loss: 0.00002621
Iteration 105/1000 | Loss: 0.00002621
Iteration 106/1000 | Loss: 0.00002621
Iteration 107/1000 | Loss: 0.00002621
Iteration 108/1000 | Loss: 0.00002620
Iteration 109/1000 | Loss: 0.00002620
Iteration 110/1000 | Loss: 0.00002620
Iteration 111/1000 | Loss: 0.00002620
Iteration 112/1000 | Loss: 0.00002620
Iteration 113/1000 | Loss: 0.00002619
Iteration 114/1000 | Loss: 0.00002619
Iteration 115/1000 | Loss: 0.00002619
Iteration 116/1000 | Loss: 0.00002619
Iteration 117/1000 | Loss: 0.00002619
Iteration 118/1000 | Loss: 0.00002619
Iteration 119/1000 | Loss: 0.00002619
Iteration 120/1000 | Loss: 0.00002619
Iteration 121/1000 | Loss: 0.00002619
Iteration 122/1000 | Loss: 0.00002619
Iteration 123/1000 | Loss: 0.00002619
Iteration 124/1000 | Loss: 0.00002619
Iteration 125/1000 | Loss: 0.00002619
Iteration 126/1000 | Loss: 0.00002619
Iteration 127/1000 | Loss: 0.00002618
Iteration 128/1000 | Loss: 0.00002618
Iteration 129/1000 | Loss: 0.00002618
Iteration 130/1000 | Loss: 0.00002618
Iteration 131/1000 | Loss: 0.00002618
Iteration 132/1000 | Loss: 0.00002618
Iteration 133/1000 | Loss: 0.00002618
Iteration 134/1000 | Loss: 0.00002618
Iteration 135/1000 | Loss: 0.00002618
Iteration 136/1000 | Loss: 0.00002618
Iteration 137/1000 | Loss: 0.00002618
Iteration 138/1000 | Loss: 0.00002618
Iteration 139/1000 | Loss: 0.00002618
Iteration 140/1000 | Loss: 0.00002617
Iteration 141/1000 | Loss: 0.00002617
Iteration 142/1000 | Loss: 0.00002617
Iteration 143/1000 | Loss: 0.00002617
Iteration 144/1000 | Loss: 0.00002617
Iteration 145/1000 | Loss: 0.00002617
Iteration 146/1000 | Loss: 0.00002617
Iteration 147/1000 | Loss: 0.00002617
Iteration 148/1000 | Loss: 0.00002617
Iteration 149/1000 | Loss: 0.00002616
Iteration 150/1000 | Loss: 0.00002616
Iteration 151/1000 | Loss: 0.00002616
Iteration 152/1000 | Loss: 0.00002616
Iteration 153/1000 | Loss: 0.00002616
Iteration 154/1000 | Loss: 0.00002616
Iteration 155/1000 | Loss: 0.00002616
Iteration 156/1000 | Loss: 0.00002616
Iteration 157/1000 | Loss: 0.00002616
Iteration 158/1000 | Loss: 0.00002616
Iteration 159/1000 | Loss: 0.00002616
Iteration 160/1000 | Loss: 0.00002616
Iteration 161/1000 | Loss: 0.00002616
Iteration 162/1000 | Loss: 0.00002616
Iteration 163/1000 | Loss: 0.00002616
Iteration 164/1000 | Loss: 0.00002616
Iteration 165/1000 | Loss: 0.00002616
Iteration 166/1000 | Loss: 0.00002616
Iteration 167/1000 | Loss: 0.00002616
Iteration 168/1000 | Loss: 0.00002616
Iteration 169/1000 | Loss: 0.00002616
Iteration 170/1000 | Loss: 0.00002616
Iteration 171/1000 | Loss: 0.00002616
Iteration 172/1000 | Loss: 0.00002616
Iteration 173/1000 | Loss: 0.00002616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.6161520509049296e-05, 2.6161520509049296e-05, 2.6161520509049296e-05, 2.6161520509049296e-05, 2.6161520509049296e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6161520509049296e-05

Optimization complete. Final v2v error: 4.274898052215576 mm

Highest mean error: 5.087431907653809 mm for frame 66

Lowest mean error: 3.6706767082214355 mm for frame 28

Saving results

Total time: 47.83070731163025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00982615
Iteration 2/25 | Loss: 0.00175961
Iteration 3/25 | Loss: 0.00149786
Iteration 4/25 | Loss: 0.00145044
Iteration 5/25 | Loss: 0.00144777
Iteration 6/25 | Loss: 0.00141692
Iteration 7/25 | Loss: 0.00141031
Iteration 8/25 | Loss: 0.00140364
Iteration 9/25 | Loss: 0.00139470
Iteration 10/25 | Loss: 0.00139807
Iteration 11/25 | Loss: 0.00138610
Iteration 12/25 | Loss: 0.00138433
Iteration 13/25 | Loss: 0.00139010
Iteration 14/25 | Loss: 0.00138111
Iteration 15/25 | Loss: 0.00138571
Iteration 16/25 | Loss: 0.00138224
Iteration 17/25 | Loss: 0.00137772
Iteration 18/25 | Loss: 0.00137879
Iteration 19/25 | Loss: 0.00137811
Iteration 20/25 | Loss: 0.00137922
Iteration 21/25 | Loss: 0.00138169
Iteration 22/25 | Loss: 0.00138256
Iteration 23/25 | Loss: 0.00138405
Iteration 24/25 | Loss: 0.00138125
Iteration 25/25 | Loss: 0.00138180

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85009670
Iteration 2/25 | Loss: 0.00192454
Iteration 3/25 | Loss: 0.00192454
Iteration 4/25 | Loss: 0.00192454
Iteration 5/25 | Loss: 0.00192454
Iteration 6/25 | Loss: 0.00192454
Iteration 7/25 | Loss: 0.00192454
Iteration 8/25 | Loss: 0.00192454
Iteration 9/25 | Loss: 0.00192454
Iteration 10/25 | Loss: 0.00192454
Iteration 11/25 | Loss: 0.00192454
Iteration 12/25 | Loss: 0.00192454
Iteration 13/25 | Loss: 0.00192454
Iteration 14/25 | Loss: 0.00192454
Iteration 15/25 | Loss: 0.00192454
Iteration 16/25 | Loss: 0.00192454
Iteration 17/25 | Loss: 0.00192454
Iteration 18/25 | Loss: 0.00192454
Iteration 19/25 | Loss: 0.00192454
Iteration 20/25 | Loss: 0.00192454
Iteration 21/25 | Loss: 0.00192454
Iteration 22/25 | Loss: 0.00192454
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0019245409639552236, 0.0019245409639552236, 0.0019245409639552236, 0.0019245409639552236, 0.0019245409639552236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019245409639552236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192454
Iteration 2/1000 | Loss: 0.00038809
Iteration 3/1000 | Loss: 0.00028033
Iteration 4/1000 | Loss: 0.00037291
Iteration 5/1000 | Loss: 0.00031608
Iteration 6/1000 | Loss: 0.00020930
Iteration 7/1000 | Loss: 0.00072885
Iteration 8/1000 | Loss: 0.00030269
Iteration 9/1000 | Loss: 0.00010947
Iteration 10/1000 | Loss: 0.00072916
Iteration 11/1000 | Loss: 0.00048126
Iteration 12/1000 | Loss: 0.00016097
Iteration 13/1000 | Loss: 0.00057468
Iteration 14/1000 | Loss: 0.00062518
Iteration 15/1000 | Loss: 0.00059130
Iteration 16/1000 | Loss: 0.00053818
Iteration 17/1000 | Loss: 0.00032366
Iteration 18/1000 | Loss: 0.00012426
Iteration 19/1000 | Loss: 0.00069709
Iteration 20/1000 | Loss: 0.00028167
Iteration 21/1000 | Loss: 0.00031163
Iteration 22/1000 | Loss: 0.00013169
Iteration 23/1000 | Loss: 0.00059357
Iteration 24/1000 | Loss: 0.00051330
Iteration 25/1000 | Loss: 0.00051372
Iteration 26/1000 | Loss: 0.00113743
Iteration 27/1000 | Loss: 0.00047076
Iteration 28/1000 | Loss: 0.00042881
Iteration 29/1000 | Loss: 0.00038074
Iteration 30/1000 | Loss: 0.00043566
Iteration 31/1000 | Loss: 0.00045536
Iteration 32/1000 | Loss: 0.00032023
Iteration 33/1000 | Loss: 0.00023906
Iteration 34/1000 | Loss: 0.00060975
Iteration 35/1000 | Loss: 0.00023423
Iteration 36/1000 | Loss: 0.00047867
Iteration 37/1000 | Loss: 0.00111844
Iteration 38/1000 | Loss: 0.00116724
Iteration 39/1000 | Loss: 0.00061456
Iteration 40/1000 | Loss: 0.00035681
Iteration 41/1000 | Loss: 0.00028661
Iteration 42/1000 | Loss: 0.00016255
Iteration 43/1000 | Loss: 0.00008920
Iteration 44/1000 | Loss: 0.00009217
Iteration 45/1000 | Loss: 0.00007582
Iteration 46/1000 | Loss: 0.00038811
Iteration 47/1000 | Loss: 0.00047426
Iteration 48/1000 | Loss: 0.00035956
Iteration 49/1000 | Loss: 0.00051903
Iteration 50/1000 | Loss: 0.00062635
Iteration 51/1000 | Loss: 0.00096113
Iteration 52/1000 | Loss: 0.00055975
Iteration 53/1000 | Loss: 0.00060786
Iteration 54/1000 | Loss: 0.00030111
Iteration 55/1000 | Loss: 0.00019577
Iteration 56/1000 | Loss: 0.00013558
Iteration 57/1000 | Loss: 0.00008215
Iteration 58/1000 | Loss: 0.00007447
Iteration 59/1000 | Loss: 0.00007478
Iteration 60/1000 | Loss: 0.00024180
Iteration 61/1000 | Loss: 0.00007479
Iteration 62/1000 | Loss: 0.00007113
Iteration 63/1000 | Loss: 0.00006967
Iteration 64/1000 | Loss: 0.00005965
Iteration 65/1000 | Loss: 0.00011929
Iteration 66/1000 | Loss: 0.00007040
Iteration 67/1000 | Loss: 0.00006250
Iteration 68/1000 | Loss: 0.00005953
Iteration 69/1000 | Loss: 0.00006123
Iteration 70/1000 | Loss: 0.00005384
Iteration 71/1000 | Loss: 0.00006422
Iteration 72/1000 | Loss: 0.00006097
Iteration 73/1000 | Loss: 0.00005901
Iteration 74/1000 | Loss: 0.00006043
Iteration 75/1000 | Loss: 0.00005494
Iteration 76/1000 | Loss: 0.00006236
Iteration 77/1000 | Loss: 0.00007147
Iteration 78/1000 | Loss: 0.00006274
Iteration 79/1000 | Loss: 0.00005367
Iteration 80/1000 | Loss: 0.00005406
Iteration 81/1000 | Loss: 0.00005327
Iteration 82/1000 | Loss: 0.00005804
Iteration 83/1000 | Loss: 0.00005691
Iteration 84/1000 | Loss: 0.00078040
Iteration 85/1000 | Loss: 0.00277212
Iteration 86/1000 | Loss: 0.00064067
Iteration 87/1000 | Loss: 0.00077640
Iteration 88/1000 | Loss: 0.00007501
Iteration 89/1000 | Loss: 0.00009116
Iteration 90/1000 | Loss: 0.00034860
Iteration 91/1000 | Loss: 0.00008418
Iteration 92/1000 | Loss: 0.00008187
Iteration 93/1000 | Loss: 0.00007196
Iteration 94/1000 | Loss: 0.00006751
Iteration 95/1000 | Loss: 0.00043985
Iteration 96/1000 | Loss: 0.00005698
Iteration 97/1000 | Loss: 0.00005141
Iteration 98/1000 | Loss: 0.00027816
Iteration 99/1000 | Loss: 0.00005838
Iteration 100/1000 | Loss: 0.00005398
Iteration 101/1000 | Loss: 0.00004447
Iteration 102/1000 | Loss: 0.00005426
Iteration 103/1000 | Loss: 0.00009959
Iteration 104/1000 | Loss: 0.00007167
Iteration 105/1000 | Loss: 0.00009375
Iteration 106/1000 | Loss: 0.00007517
Iteration 107/1000 | Loss: 0.00009522
Iteration 108/1000 | Loss: 0.00013333
Iteration 109/1000 | Loss: 0.00055289
Iteration 110/1000 | Loss: 0.00014619
Iteration 111/1000 | Loss: 0.00005175
Iteration 112/1000 | Loss: 0.00005192
Iteration 113/1000 | Loss: 0.00006223
Iteration 114/1000 | Loss: 0.00013978
Iteration 115/1000 | Loss: 0.00004052
Iteration 116/1000 | Loss: 0.00004526
Iteration 117/1000 | Loss: 0.00005242
Iteration 118/1000 | Loss: 0.00005214
Iteration 119/1000 | Loss: 0.00005142
Iteration 120/1000 | Loss: 0.00056756
Iteration 121/1000 | Loss: 0.00030528
Iteration 122/1000 | Loss: 0.00045987
Iteration 123/1000 | Loss: 0.00009038
Iteration 124/1000 | Loss: 0.00007193
Iteration 125/1000 | Loss: 0.00004108
Iteration 126/1000 | Loss: 0.00003633
Iteration 127/1000 | Loss: 0.00003581
Iteration 128/1000 | Loss: 0.00003411
Iteration 129/1000 | Loss: 0.00003396
Iteration 130/1000 | Loss: 0.00003461
Iteration 131/1000 | Loss: 0.00003403
Iteration 132/1000 | Loss: 0.00003356
Iteration 133/1000 | Loss: 0.00003371
Iteration 134/1000 | Loss: 0.00003341
Iteration 135/1000 | Loss: 0.00041338
Iteration 136/1000 | Loss: 0.00003495
Iteration 137/1000 | Loss: 0.00003170
Iteration 138/1000 | Loss: 0.00003355
Iteration 139/1000 | Loss: 0.00003166
Iteration 140/1000 | Loss: 0.00003310
Iteration 141/1000 | Loss: 0.00003071
Iteration 142/1000 | Loss: 0.00043994
Iteration 143/1000 | Loss: 0.00003378
Iteration 144/1000 | Loss: 0.00003202
Iteration 145/1000 | Loss: 0.00003079
Iteration 146/1000 | Loss: 0.00048330
Iteration 147/1000 | Loss: 0.00003126
Iteration 148/1000 | Loss: 0.00016400
Iteration 149/1000 | Loss: 0.00012273
Iteration 150/1000 | Loss: 0.00015075
Iteration 151/1000 | Loss: 0.00010113
Iteration 152/1000 | Loss: 0.00012139
Iteration 153/1000 | Loss: 0.00004242
Iteration 154/1000 | Loss: 0.00018523
Iteration 155/1000 | Loss: 0.00028226
Iteration 156/1000 | Loss: 0.00003910
Iteration 157/1000 | Loss: 0.00003243
Iteration 158/1000 | Loss: 0.00002868
Iteration 159/1000 | Loss: 0.00002636
Iteration 160/1000 | Loss: 0.00002551
Iteration 161/1000 | Loss: 0.00002473
Iteration 162/1000 | Loss: 0.00002390
Iteration 163/1000 | Loss: 0.00003213
Iteration 164/1000 | Loss: 0.00016158
Iteration 165/1000 | Loss: 0.00009679
Iteration 166/1000 | Loss: 0.00003215
Iteration 167/1000 | Loss: 0.00017735
Iteration 168/1000 | Loss: 0.00027477
Iteration 169/1000 | Loss: 0.00003670
Iteration 170/1000 | Loss: 0.00002457
Iteration 171/1000 | Loss: 0.00018160
Iteration 172/1000 | Loss: 0.00011628
Iteration 173/1000 | Loss: 0.00004872
Iteration 174/1000 | Loss: 0.00008463
Iteration 175/1000 | Loss: 0.00002665
Iteration 176/1000 | Loss: 0.00002408
Iteration 177/1000 | Loss: 0.00002332
Iteration 178/1000 | Loss: 0.00002297
Iteration 179/1000 | Loss: 0.00002265
Iteration 180/1000 | Loss: 0.00002248
Iteration 181/1000 | Loss: 0.00002238
Iteration 182/1000 | Loss: 0.00002237
Iteration 183/1000 | Loss: 0.00002236
Iteration 184/1000 | Loss: 0.00002235
Iteration 185/1000 | Loss: 0.00002232
Iteration 186/1000 | Loss: 0.00002225
Iteration 187/1000 | Loss: 0.00002223
Iteration 188/1000 | Loss: 0.00002222
Iteration 189/1000 | Loss: 0.00002215
Iteration 190/1000 | Loss: 0.00002212
Iteration 191/1000 | Loss: 0.00002212
Iteration 192/1000 | Loss: 0.00002207
Iteration 193/1000 | Loss: 0.00002200
Iteration 194/1000 | Loss: 0.00002200
Iteration 195/1000 | Loss: 0.00002199
Iteration 196/1000 | Loss: 0.00002198
Iteration 197/1000 | Loss: 0.00002198
Iteration 198/1000 | Loss: 0.00002198
Iteration 199/1000 | Loss: 0.00002198
Iteration 200/1000 | Loss: 0.00002198
Iteration 201/1000 | Loss: 0.00002198
Iteration 202/1000 | Loss: 0.00002198
Iteration 203/1000 | Loss: 0.00002197
Iteration 204/1000 | Loss: 0.00002197
Iteration 205/1000 | Loss: 0.00002197
Iteration 206/1000 | Loss: 0.00002197
Iteration 207/1000 | Loss: 0.00002196
Iteration 208/1000 | Loss: 0.00002196
Iteration 209/1000 | Loss: 0.00002196
Iteration 210/1000 | Loss: 0.00002196
Iteration 211/1000 | Loss: 0.00002196
Iteration 212/1000 | Loss: 0.00002196
Iteration 213/1000 | Loss: 0.00002195
Iteration 214/1000 | Loss: 0.00002195
Iteration 215/1000 | Loss: 0.00002195
Iteration 216/1000 | Loss: 0.00002195
Iteration 217/1000 | Loss: 0.00002195
Iteration 218/1000 | Loss: 0.00002195
Iteration 219/1000 | Loss: 0.00002195
Iteration 220/1000 | Loss: 0.00002195
Iteration 221/1000 | Loss: 0.00002195
Iteration 222/1000 | Loss: 0.00002195
Iteration 223/1000 | Loss: 0.00002195
Iteration 224/1000 | Loss: 0.00002195
Iteration 225/1000 | Loss: 0.00002195
Iteration 226/1000 | Loss: 0.00002194
Iteration 227/1000 | Loss: 0.00002194
Iteration 228/1000 | Loss: 0.00002194
Iteration 229/1000 | Loss: 0.00002194
Iteration 230/1000 | Loss: 0.00002194
Iteration 231/1000 | Loss: 0.00002194
Iteration 232/1000 | Loss: 0.00002194
Iteration 233/1000 | Loss: 0.00002194
Iteration 234/1000 | Loss: 0.00002194
Iteration 235/1000 | Loss: 0.00002194
Iteration 236/1000 | Loss: 0.00002194
Iteration 237/1000 | Loss: 0.00002194
Iteration 238/1000 | Loss: 0.00002194
Iteration 239/1000 | Loss: 0.00002194
Iteration 240/1000 | Loss: 0.00002194
Iteration 241/1000 | Loss: 0.00002194
Iteration 242/1000 | Loss: 0.00002194
Iteration 243/1000 | Loss: 0.00002194
Iteration 244/1000 | Loss: 0.00002194
Iteration 245/1000 | Loss: 0.00002193
Iteration 246/1000 | Loss: 0.00002193
Iteration 247/1000 | Loss: 0.00002193
Iteration 248/1000 | Loss: 0.00002193
Iteration 249/1000 | Loss: 0.00002193
Iteration 250/1000 | Loss: 0.00002193
Iteration 251/1000 | Loss: 0.00002193
Iteration 252/1000 | Loss: 0.00002193
Iteration 253/1000 | Loss: 0.00002193
Iteration 254/1000 | Loss: 0.00002193
Iteration 255/1000 | Loss: 0.00002193
Iteration 256/1000 | Loss: 0.00002193
Iteration 257/1000 | Loss: 0.00002193
Iteration 258/1000 | Loss: 0.00002193
Iteration 259/1000 | Loss: 0.00002193
Iteration 260/1000 | Loss: 0.00002193
Iteration 261/1000 | Loss: 0.00002193
Iteration 262/1000 | Loss: 0.00002193
Iteration 263/1000 | Loss: 0.00002193
Iteration 264/1000 | Loss: 0.00002192
Iteration 265/1000 | Loss: 0.00002192
Iteration 266/1000 | Loss: 0.00002192
Iteration 267/1000 | Loss: 0.00002192
Iteration 268/1000 | Loss: 0.00002192
Iteration 269/1000 | Loss: 0.00002192
Iteration 270/1000 | Loss: 0.00002192
Iteration 271/1000 | Loss: 0.00002192
Iteration 272/1000 | Loss: 0.00002192
Iteration 273/1000 | Loss: 0.00002192
Iteration 274/1000 | Loss: 0.00002192
Iteration 275/1000 | Loss: 0.00002192
Iteration 276/1000 | Loss: 0.00002192
Iteration 277/1000 | Loss: 0.00002192
Iteration 278/1000 | Loss: 0.00002192
Iteration 279/1000 | Loss: 0.00002192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 279. Stopping optimization.
Last 5 losses: [2.1919351638644002e-05, 2.1919351638644002e-05, 2.1919351638644002e-05, 2.1919351638644002e-05, 2.1919351638644002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1919351638644002e-05

Optimization complete. Final v2v error: 3.921335458755493 mm

Highest mean error: 5.82243537902832 mm for frame 29

Lowest mean error: 3.3538386821746826 mm for frame 185

Saving results

Total time: 350.50058460235596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00998672
Iteration 2/25 | Loss: 0.00998672
Iteration 3/25 | Loss: 0.00998671
Iteration 4/25 | Loss: 0.00347905
Iteration 5/25 | Loss: 0.00240088
Iteration 6/25 | Loss: 0.00196131
Iteration 7/25 | Loss: 0.00156931
Iteration 8/25 | Loss: 0.00147569
Iteration 9/25 | Loss: 0.00139737
Iteration 10/25 | Loss: 0.00137660
Iteration 11/25 | Loss: 0.00136433
Iteration 12/25 | Loss: 0.00134455
Iteration 13/25 | Loss: 0.00133718
Iteration 14/25 | Loss: 0.00133494
Iteration 15/25 | Loss: 0.00133433
Iteration 16/25 | Loss: 0.00133390
Iteration 17/25 | Loss: 0.00133354
Iteration 18/25 | Loss: 0.00133328
Iteration 19/25 | Loss: 0.00133286
Iteration 20/25 | Loss: 0.00133258
Iteration 21/25 | Loss: 0.00133247
Iteration 22/25 | Loss: 0.00133231
Iteration 23/25 | Loss: 0.00133193
Iteration 24/25 | Loss: 0.00133171
Iteration 25/25 | Loss: 0.00133163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40376067
Iteration 2/25 | Loss: 0.00080271
Iteration 3/25 | Loss: 0.00080270
Iteration 4/25 | Loss: 0.00080270
Iteration 5/25 | Loss: 0.00080270
Iteration 6/25 | Loss: 0.00080270
Iteration 7/25 | Loss: 0.00080270
Iteration 8/25 | Loss: 0.00080270
Iteration 9/25 | Loss: 0.00080270
Iteration 10/25 | Loss: 0.00080270
Iteration 11/25 | Loss: 0.00080270
Iteration 12/25 | Loss: 0.00080270
Iteration 13/25 | Loss: 0.00080270
Iteration 14/25 | Loss: 0.00080270
Iteration 15/25 | Loss: 0.00080270
Iteration 16/25 | Loss: 0.00080270
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008027000003494322, 0.0008027000003494322, 0.0008027000003494322, 0.0008027000003494322, 0.0008027000003494322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008027000003494322

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080270
Iteration 2/1000 | Loss: 0.00005579
Iteration 3/1000 | Loss: 0.00003623
Iteration 4/1000 | Loss: 0.00003288
Iteration 5/1000 | Loss: 0.00003095
Iteration 6/1000 | Loss: 0.00002997
Iteration 7/1000 | Loss: 0.00002893
Iteration 8/1000 | Loss: 0.00002835
Iteration 9/1000 | Loss: 0.00002795
Iteration 10/1000 | Loss: 0.00002756
Iteration 11/1000 | Loss: 0.00002745
Iteration 12/1000 | Loss: 0.00002719
Iteration 13/1000 | Loss: 0.00002696
Iteration 14/1000 | Loss: 0.00002691
Iteration 15/1000 | Loss: 0.00002686
Iteration 16/1000 | Loss: 0.00002683
Iteration 17/1000 | Loss: 0.00002682
Iteration 18/1000 | Loss: 0.00002681
Iteration 19/1000 | Loss: 0.00002681
Iteration 20/1000 | Loss: 0.00002680
Iteration 21/1000 | Loss: 0.00002680
Iteration 22/1000 | Loss: 0.00002679
Iteration 23/1000 | Loss: 0.00002678
Iteration 24/1000 | Loss: 0.00002678
Iteration 25/1000 | Loss: 0.00002677
Iteration 26/1000 | Loss: 0.00002677
Iteration 27/1000 | Loss: 0.00002677
Iteration 28/1000 | Loss: 0.00002676
Iteration 29/1000 | Loss: 0.00002671
Iteration 30/1000 | Loss: 0.00002666
Iteration 31/1000 | Loss: 0.00002666
Iteration 32/1000 | Loss: 0.00002664
Iteration 33/1000 | Loss: 0.00002664
Iteration 34/1000 | Loss: 0.00002663
Iteration 35/1000 | Loss: 0.00002662
Iteration 36/1000 | Loss: 0.00002662
Iteration 37/1000 | Loss: 0.00002662
Iteration 38/1000 | Loss: 0.00002661
Iteration 39/1000 | Loss: 0.00002661
Iteration 40/1000 | Loss: 0.00002660
Iteration 41/1000 | Loss: 0.00002659
Iteration 42/1000 | Loss: 0.00002659
Iteration 43/1000 | Loss: 0.00002659
Iteration 44/1000 | Loss: 0.00002659
Iteration 45/1000 | Loss: 0.00002658
Iteration 46/1000 | Loss: 0.00002658
Iteration 47/1000 | Loss: 0.00002657
Iteration 48/1000 | Loss: 0.00002656
Iteration 49/1000 | Loss: 0.00002656
Iteration 50/1000 | Loss: 0.00002655
Iteration 51/1000 | Loss: 0.00002655
Iteration 52/1000 | Loss: 0.00002655
Iteration 53/1000 | Loss: 0.00002655
Iteration 54/1000 | Loss: 0.00002655
Iteration 55/1000 | Loss: 0.00002654
Iteration 56/1000 | Loss: 0.00002654
Iteration 57/1000 | Loss: 0.00002654
Iteration 58/1000 | Loss: 0.00002653
Iteration 59/1000 | Loss: 0.00002653
Iteration 60/1000 | Loss: 0.00002653
Iteration 61/1000 | Loss: 0.00002652
Iteration 62/1000 | Loss: 0.00002652
Iteration 63/1000 | Loss: 0.00002652
Iteration 64/1000 | Loss: 0.00002652
Iteration 65/1000 | Loss: 0.00002651
Iteration 66/1000 | Loss: 0.00002651
Iteration 67/1000 | Loss: 0.00002651
Iteration 68/1000 | Loss: 0.00002651
Iteration 69/1000 | Loss: 0.00002651
Iteration 70/1000 | Loss: 0.00002651
Iteration 71/1000 | Loss: 0.00002651
Iteration 72/1000 | Loss: 0.00002651
Iteration 73/1000 | Loss: 0.00002650
Iteration 74/1000 | Loss: 0.00002650
Iteration 75/1000 | Loss: 0.00002650
Iteration 76/1000 | Loss: 0.00002650
Iteration 77/1000 | Loss: 0.00002650
Iteration 78/1000 | Loss: 0.00002649
Iteration 79/1000 | Loss: 0.00002649
Iteration 80/1000 | Loss: 0.00002649
Iteration 81/1000 | Loss: 0.00002649
Iteration 82/1000 | Loss: 0.00002649
Iteration 83/1000 | Loss: 0.00002649
Iteration 84/1000 | Loss: 0.00002649
Iteration 85/1000 | Loss: 0.00002648
Iteration 86/1000 | Loss: 0.00002648
Iteration 87/1000 | Loss: 0.00002648
Iteration 88/1000 | Loss: 0.00002648
Iteration 89/1000 | Loss: 0.00002648
Iteration 90/1000 | Loss: 0.00002648
Iteration 91/1000 | Loss: 0.00002647
Iteration 92/1000 | Loss: 0.00002647
Iteration 93/1000 | Loss: 0.00002647
Iteration 94/1000 | Loss: 0.00002647
Iteration 95/1000 | Loss: 0.00002647
Iteration 96/1000 | Loss: 0.00002646
Iteration 97/1000 | Loss: 0.00002646
Iteration 98/1000 | Loss: 0.00002646
Iteration 99/1000 | Loss: 0.00002646
Iteration 100/1000 | Loss: 0.00002646
Iteration 101/1000 | Loss: 0.00002646
Iteration 102/1000 | Loss: 0.00002645
Iteration 103/1000 | Loss: 0.00002645
Iteration 104/1000 | Loss: 0.00002645
Iteration 105/1000 | Loss: 0.00002645
Iteration 106/1000 | Loss: 0.00002645
Iteration 107/1000 | Loss: 0.00002645
Iteration 108/1000 | Loss: 0.00002645
Iteration 109/1000 | Loss: 0.00002644
Iteration 110/1000 | Loss: 0.00002644
Iteration 111/1000 | Loss: 0.00002644
Iteration 112/1000 | Loss: 0.00002644
Iteration 113/1000 | Loss: 0.00002643
Iteration 114/1000 | Loss: 0.00002643
Iteration 115/1000 | Loss: 0.00002643
Iteration 116/1000 | Loss: 0.00002643
Iteration 117/1000 | Loss: 0.00002643
Iteration 118/1000 | Loss: 0.00002642
Iteration 119/1000 | Loss: 0.00002642
Iteration 120/1000 | Loss: 0.00002642
Iteration 121/1000 | Loss: 0.00002642
Iteration 122/1000 | Loss: 0.00002642
Iteration 123/1000 | Loss: 0.00002642
Iteration 124/1000 | Loss: 0.00002641
Iteration 125/1000 | Loss: 0.00002641
Iteration 126/1000 | Loss: 0.00002641
Iteration 127/1000 | Loss: 0.00002641
Iteration 128/1000 | Loss: 0.00002641
Iteration 129/1000 | Loss: 0.00002641
Iteration 130/1000 | Loss: 0.00002641
Iteration 131/1000 | Loss: 0.00002641
Iteration 132/1000 | Loss: 0.00002640
Iteration 133/1000 | Loss: 0.00002640
Iteration 134/1000 | Loss: 0.00002640
Iteration 135/1000 | Loss: 0.00002640
Iteration 136/1000 | Loss: 0.00002640
Iteration 137/1000 | Loss: 0.00002640
Iteration 138/1000 | Loss: 0.00002640
Iteration 139/1000 | Loss: 0.00002640
Iteration 140/1000 | Loss: 0.00002640
Iteration 141/1000 | Loss: 0.00002639
Iteration 142/1000 | Loss: 0.00002639
Iteration 143/1000 | Loss: 0.00002639
Iteration 144/1000 | Loss: 0.00002639
Iteration 145/1000 | Loss: 0.00002639
Iteration 146/1000 | Loss: 0.00002638
Iteration 147/1000 | Loss: 0.00002638
Iteration 148/1000 | Loss: 0.00002638
Iteration 149/1000 | Loss: 0.00002638
Iteration 150/1000 | Loss: 0.00002638
Iteration 151/1000 | Loss: 0.00002638
Iteration 152/1000 | Loss: 0.00002638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.6383697331766598e-05, 2.6383697331766598e-05, 2.6383697331766598e-05, 2.6383697331766598e-05, 2.6383697331766598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6383697331766598e-05

Optimization complete. Final v2v error: 4.4069437980651855 mm

Highest mean error: 4.712813854217529 mm for frame 18

Lowest mean error: 4.016951560974121 mm for frame 142

Saving results

Total time: 80.81768083572388
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414216
Iteration 2/25 | Loss: 0.00136464
Iteration 3/25 | Loss: 0.00126119
Iteration 4/25 | Loss: 0.00124373
Iteration 5/25 | Loss: 0.00123856
Iteration 6/25 | Loss: 0.00123701
Iteration 7/25 | Loss: 0.00123690
Iteration 8/25 | Loss: 0.00123690
Iteration 9/25 | Loss: 0.00123690
Iteration 10/25 | Loss: 0.00123690
Iteration 11/25 | Loss: 0.00123690
Iteration 12/25 | Loss: 0.00123690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001236898242495954, 0.001236898242495954, 0.001236898242495954, 0.001236898242495954, 0.001236898242495954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001236898242495954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37600374
Iteration 2/25 | Loss: 0.00086980
Iteration 3/25 | Loss: 0.00086977
Iteration 4/25 | Loss: 0.00086977
Iteration 5/25 | Loss: 0.00086977
Iteration 6/25 | Loss: 0.00086977
Iteration 7/25 | Loss: 0.00086977
Iteration 8/25 | Loss: 0.00086977
Iteration 9/25 | Loss: 0.00086977
Iteration 10/25 | Loss: 0.00086977
Iteration 11/25 | Loss: 0.00086977
Iteration 12/25 | Loss: 0.00086977
Iteration 13/25 | Loss: 0.00086977
Iteration 14/25 | Loss: 0.00086977
Iteration 15/25 | Loss: 0.00086977
Iteration 16/25 | Loss: 0.00086977
Iteration 17/25 | Loss: 0.00086977
Iteration 18/25 | Loss: 0.00086977
Iteration 19/25 | Loss: 0.00086977
Iteration 20/25 | Loss: 0.00086977
Iteration 21/25 | Loss: 0.00086977
Iteration 22/25 | Loss: 0.00086977
Iteration 23/25 | Loss: 0.00086977
Iteration 24/25 | Loss: 0.00086977
Iteration 25/25 | Loss: 0.00086977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086977
Iteration 2/1000 | Loss: 0.00006049
Iteration 3/1000 | Loss: 0.00003830
Iteration 4/1000 | Loss: 0.00002966
Iteration 5/1000 | Loss: 0.00002460
Iteration 6/1000 | Loss: 0.00002284
Iteration 7/1000 | Loss: 0.00002151
Iteration 8/1000 | Loss: 0.00002049
Iteration 9/1000 | Loss: 0.00001982
Iteration 10/1000 | Loss: 0.00001927
Iteration 11/1000 | Loss: 0.00001887
Iteration 12/1000 | Loss: 0.00001887
Iteration 13/1000 | Loss: 0.00001867
Iteration 14/1000 | Loss: 0.00001846
Iteration 15/1000 | Loss: 0.00001831
Iteration 16/1000 | Loss: 0.00001831
Iteration 17/1000 | Loss: 0.00001830
Iteration 18/1000 | Loss: 0.00001829
Iteration 19/1000 | Loss: 0.00001825
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001820
Iteration 22/1000 | Loss: 0.00001819
Iteration 23/1000 | Loss: 0.00001818
Iteration 24/1000 | Loss: 0.00001818
Iteration 25/1000 | Loss: 0.00001817
Iteration 26/1000 | Loss: 0.00001817
Iteration 27/1000 | Loss: 0.00001816
Iteration 28/1000 | Loss: 0.00001815
Iteration 29/1000 | Loss: 0.00001814
Iteration 30/1000 | Loss: 0.00001814
Iteration 31/1000 | Loss: 0.00001814
Iteration 32/1000 | Loss: 0.00001813
Iteration 33/1000 | Loss: 0.00001813
Iteration 34/1000 | Loss: 0.00001813
Iteration 35/1000 | Loss: 0.00001813
Iteration 36/1000 | Loss: 0.00001813
Iteration 37/1000 | Loss: 0.00001812
Iteration 38/1000 | Loss: 0.00001812
Iteration 39/1000 | Loss: 0.00001812
Iteration 40/1000 | Loss: 0.00001812
Iteration 41/1000 | Loss: 0.00001812
Iteration 42/1000 | Loss: 0.00001811
Iteration 43/1000 | Loss: 0.00001810
Iteration 44/1000 | Loss: 0.00001810
Iteration 45/1000 | Loss: 0.00001810
Iteration 46/1000 | Loss: 0.00001809
Iteration 47/1000 | Loss: 0.00001809
Iteration 48/1000 | Loss: 0.00001808
Iteration 49/1000 | Loss: 0.00001807
Iteration 50/1000 | Loss: 0.00001807
Iteration 51/1000 | Loss: 0.00001807
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001806
Iteration 54/1000 | Loss: 0.00001805
Iteration 55/1000 | Loss: 0.00001804
Iteration 56/1000 | Loss: 0.00001803
Iteration 57/1000 | Loss: 0.00001803
Iteration 58/1000 | Loss: 0.00001802
Iteration 59/1000 | Loss: 0.00001801
Iteration 60/1000 | Loss: 0.00001801
Iteration 61/1000 | Loss: 0.00001800
Iteration 62/1000 | Loss: 0.00001800
Iteration 63/1000 | Loss: 0.00001799
Iteration 64/1000 | Loss: 0.00001799
Iteration 65/1000 | Loss: 0.00001799
Iteration 66/1000 | Loss: 0.00001798
Iteration 67/1000 | Loss: 0.00001797
Iteration 68/1000 | Loss: 0.00001797
Iteration 69/1000 | Loss: 0.00001797
Iteration 70/1000 | Loss: 0.00001796
Iteration 71/1000 | Loss: 0.00001795
Iteration 72/1000 | Loss: 0.00001794
Iteration 73/1000 | Loss: 0.00001794
Iteration 74/1000 | Loss: 0.00001793
Iteration 75/1000 | Loss: 0.00001793
Iteration 76/1000 | Loss: 0.00001793
Iteration 77/1000 | Loss: 0.00001793
Iteration 78/1000 | Loss: 0.00001792
Iteration 79/1000 | Loss: 0.00001792
Iteration 80/1000 | Loss: 0.00001792
Iteration 81/1000 | Loss: 0.00001792
Iteration 82/1000 | Loss: 0.00001791
Iteration 83/1000 | Loss: 0.00001791
Iteration 84/1000 | Loss: 0.00001791
Iteration 85/1000 | Loss: 0.00001790
Iteration 86/1000 | Loss: 0.00001789
Iteration 87/1000 | Loss: 0.00001789
Iteration 88/1000 | Loss: 0.00001788
Iteration 89/1000 | Loss: 0.00001788
Iteration 90/1000 | Loss: 0.00001788
Iteration 91/1000 | Loss: 0.00001787
Iteration 92/1000 | Loss: 0.00001787
Iteration 93/1000 | Loss: 0.00001787
Iteration 94/1000 | Loss: 0.00001786
Iteration 95/1000 | Loss: 0.00001786
Iteration 96/1000 | Loss: 0.00001786
Iteration 97/1000 | Loss: 0.00001786
Iteration 98/1000 | Loss: 0.00001785
Iteration 99/1000 | Loss: 0.00001785
Iteration 100/1000 | Loss: 0.00001785
Iteration 101/1000 | Loss: 0.00001785
Iteration 102/1000 | Loss: 0.00001785
Iteration 103/1000 | Loss: 0.00001785
Iteration 104/1000 | Loss: 0.00001785
Iteration 105/1000 | Loss: 0.00001785
Iteration 106/1000 | Loss: 0.00001785
Iteration 107/1000 | Loss: 0.00001785
Iteration 108/1000 | Loss: 0.00001785
Iteration 109/1000 | Loss: 0.00001784
Iteration 110/1000 | Loss: 0.00001784
Iteration 111/1000 | Loss: 0.00001783
Iteration 112/1000 | Loss: 0.00001783
Iteration 113/1000 | Loss: 0.00001783
Iteration 114/1000 | Loss: 0.00001783
Iteration 115/1000 | Loss: 0.00001783
Iteration 116/1000 | Loss: 0.00001783
Iteration 117/1000 | Loss: 0.00001782
Iteration 118/1000 | Loss: 0.00001782
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00001782
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001782
Iteration 123/1000 | Loss: 0.00001782
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001780
Iteration 128/1000 | Loss: 0.00001780
Iteration 129/1000 | Loss: 0.00001780
Iteration 130/1000 | Loss: 0.00001779
Iteration 131/1000 | Loss: 0.00001779
Iteration 132/1000 | Loss: 0.00001779
Iteration 133/1000 | Loss: 0.00001779
Iteration 134/1000 | Loss: 0.00001779
Iteration 135/1000 | Loss: 0.00001779
Iteration 136/1000 | Loss: 0.00001778
Iteration 137/1000 | Loss: 0.00001778
Iteration 138/1000 | Loss: 0.00001778
Iteration 139/1000 | Loss: 0.00001778
Iteration 140/1000 | Loss: 0.00001778
Iteration 141/1000 | Loss: 0.00001778
Iteration 142/1000 | Loss: 0.00001778
Iteration 143/1000 | Loss: 0.00001778
Iteration 144/1000 | Loss: 0.00001778
Iteration 145/1000 | Loss: 0.00001778
Iteration 146/1000 | Loss: 0.00001778
Iteration 147/1000 | Loss: 0.00001778
Iteration 148/1000 | Loss: 0.00001777
Iteration 149/1000 | Loss: 0.00001777
Iteration 150/1000 | Loss: 0.00001777
Iteration 151/1000 | Loss: 0.00001777
Iteration 152/1000 | Loss: 0.00001777
Iteration 153/1000 | Loss: 0.00001777
Iteration 154/1000 | Loss: 0.00001777
Iteration 155/1000 | Loss: 0.00001776
Iteration 156/1000 | Loss: 0.00001776
Iteration 157/1000 | Loss: 0.00001776
Iteration 158/1000 | Loss: 0.00001776
Iteration 159/1000 | Loss: 0.00001776
Iteration 160/1000 | Loss: 0.00001776
Iteration 161/1000 | Loss: 0.00001776
Iteration 162/1000 | Loss: 0.00001776
Iteration 163/1000 | Loss: 0.00001776
Iteration 164/1000 | Loss: 0.00001776
Iteration 165/1000 | Loss: 0.00001776
Iteration 166/1000 | Loss: 0.00001775
Iteration 167/1000 | Loss: 0.00001775
Iteration 168/1000 | Loss: 0.00001775
Iteration 169/1000 | Loss: 0.00001775
Iteration 170/1000 | Loss: 0.00001775
Iteration 171/1000 | Loss: 0.00001775
Iteration 172/1000 | Loss: 0.00001775
Iteration 173/1000 | Loss: 0.00001775
Iteration 174/1000 | Loss: 0.00001775
Iteration 175/1000 | Loss: 0.00001775
Iteration 176/1000 | Loss: 0.00001775
Iteration 177/1000 | Loss: 0.00001774
Iteration 178/1000 | Loss: 0.00001774
Iteration 179/1000 | Loss: 0.00001774
Iteration 180/1000 | Loss: 0.00001774
Iteration 181/1000 | Loss: 0.00001774
Iteration 182/1000 | Loss: 0.00001774
Iteration 183/1000 | Loss: 0.00001774
Iteration 184/1000 | Loss: 0.00001773
Iteration 185/1000 | Loss: 0.00001773
Iteration 186/1000 | Loss: 0.00001773
Iteration 187/1000 | Loss: 0.00001773
Iteration 188/1000 | Loss: 0.00001773
Iteration 189/1000 | Loss: 0.00001773
Iteration 190/1000 | Loss: 0.00001773
Iteration 191/1000 | Loss: 0.00001773
Iteration 192/1000 | Loss: 0.00001773
Iteration 193/1000 | Loss: 0.00001773
Iteration 194/1000 | Loss: 0.00001773
Iteration 195/1000 | Loss: 0.00001772
Iteration 196/1000 | Loss: 0.00001772
Iteration 197/1000 | Loss: 0.00001772
Iteration 198/1000 | Loss: 0.00001772
Iteration 199/1000 | Loss: 0.00001772
Iteration 200/1000 | Loss: 0.00001772
Iteration 201/1000 | Loss: 0.00001772
Iteration 202/1000 | Loss: 0.00001772
Iteration 203/1000 | Loss: 0.00001772
Iteration 204/1000 | Loss: 0.00001772
Iteration 205/1000 | Loss: 0.00001772
Iteration 206/1000 | Loss: 0.00001772
Iteration 207/1000 | Loss: 0.00001772
Iteration 208/1000 | Loss: 0.00001772
Iteration 209/1000 | Loss: 0.00001772
Iteration 210/1000 | Loss: 0.00001771
Iteration 211/1000 | Loss: 0.00001771
Iteration 212/1000 | Loss: 0.00001771
Iteration 213/1000 | Loss: 0.00001771
Iteration 214/1000 | Loss: 0.00001771
Iteration 215/1000 | Loss: 0.00001771
Iteration 216/1000 | Loss: 0.00001771
Iteration 217/1000 | Loss: 0.00001771
Iteration 218/1000 | Loss: 0.00001771
Iteration 219/1000 | Loss: 0.00001771
Iteration 220/1000 | Loss: 0.00001771
Iteration 221/1000 | Loss: 0.00001771
Iteration 222/1000 | Loss: 0.00001771
Iteration 223/1000 | Loss: 0.00001771
Iteration 224/1000 | Loss: 0.00001771
Iteration 225/1000 | Loss: 0.00001770
Iteration 226/1000 | Loss: 0.00001770
Iteration 227/1000 | Loss: 0.00001770
Iteration 228/1000 | Loss: 0.00001770
Iteration 229/1000 | Loss: 0.00001769
Iteration 230/1000 | Loss: 0.00001769
Iteration 231/1000 | Loss: 0.00001769
Iteration 232/1000 | Loss: 0.00001769
Iteration 233/1000 | Loss: 0.00001769
Iteration 234/1000 | Loss: 0.00001769
Iteration 235/1000 | Loss: 0.00001768
Iteration 236/1000 | Loss: 0.00001768
Iteration 237/1000 | Loss: 0.00001768
Iteration 238/1000 | Loss: 0.00001768
Iteration 239/1000 | Loss: 0.00001768
Iteration 240/1000 | Loss: 0.00001768
Iteration 241/1000 | Loss: 0.00001768
Iteration 242/1000 | Loss: 0.00001768
Iteration 243/1000 | Loss: 0.00001768
Iteration 244/1000 | Loss: 0.00001768
Iteration 245/1000 | Loss: 0.00001767
Iteration 246/1000 | Loss: 0.00001767
Iteration 247/1000 | Loss: 0.00001767
Iteration 248/1000 | Loss: 0.00001767
Iteration 249/1000 | Loss: 0.00001767
Iteration 250/1000 | Loss: 0.00001767
Iteration 251/1000 | Loss: 0.00001767
Iteration 252/1000 | Loss: 0.00001767
Iteration 253/1000 | Loss: 0.00001766
Iteration 254/1000 | Loss: 0.00001766
Iteration 255/1000 | Loss: 0.00001766
Iteration 256/1000 | Loss: 0.00001766
Iteration 257/1000 | Loss: 0.00001766
Iteration 258/1000 | Loss: 0.00001766
Iteration 259/1000 | Loss: 0.00001765
Iteration 260/1000 | Loss: 0.00001765
Iteration 261/1000 | Loss: 0.00001765
Iteration 262/1000 | Loss: 0.00001765
Iteration 263/1000 | Loss: 0.00001765
Iteration 264/1000 | Loss: 0.00001764
Iteration 265/1000 | Loss: 0.00001764
Iteration 266/1000 | Loss: 0.00001764
Iteration 267/1000 | Loss: 0.00001764
Iteration 268/1000 | Loss: 0.00001764
Iteration 269/1000 | Loss: 0.00001764
Iteration 270/1000 | Loss: 0.00001764
Iteration 271/1000 | Loss: 0.00001764
Iteration 272/1000 | Loss: 0.00001764
Iteration 273/1000 | Loss: 0.00001764
Iteration 274/1000 | Loss: 0.00001764
Iteration 275/1000 | Loss: 0.00001764
Iteration 276/1000 | Loss: 0.00001764
Iteration 277/1000 | Loss: 0.00001764
Iteration 278/1000 | Loss: 0.00001764
Iteration 279/1000 | Loss: 0.00001764
Iteration 280/1000 | Loss: 0.00001764
Iteration 281/1000 | Loss: 0.00001764
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 281. Stopping optimization.
Last 5 losses: [1.7643820683588274e-05, 1.7643820683588274e-05, 1.7643820683588274e-05, 1.7643820683588274e-05, 1.7643820683588274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7643820683588274e-05

Optimization complete. Final v2v error: 3.4129831790924072 mm

Highest mean error: 5.40324592590332 mm for frame 87

Lowest mean error: 2.785376787185669 mm for frame 14

Saving results

Total time: 47.82988667488098
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396755
Iteration 2/25 | Loss: 0.00127611
Iteration 3/25 | Loss: 0.00120041
Iteration 4/25 | Loss: 0.00119200
Iteration 5/25 | Loss: 0.00118990
Iteration 6/25 | Loss: 0.00118944
Iteration 7/25 | Loss: 0.00118944
Iteration 8/25 | Loss: 0.00118944
Iteration 9/25 | Loss: 0.00118944
Iteration 10/25 | Loss: 0.00118944
Iteration 11/25 | Loss: 0.00118944
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011894440976902843, 0.0011894440976902843, 0.0011894440976902843, 0.0011894440976902843, 0.0011894440976902843]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011894440976902843

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43723571
Iteration 2/25 | Loss: 0.00078816
Iteration 3/25 | Loss: 0.00078815
Iteration 4/25 | Loss: 0.00078815
Iteration 5/25 | Loss: 0.00078815
Iteration 6/25 | Loss: 0.00078815
Iteration 7/25 | Loss: 0.00078815
Iteration 8/25 | Loss: 0.00078815
Iteration 9/25 | Loss: 0.00078815
Iteration 10/25 | Loss: 0.00078815
Iteration 11/25 | Loss: 0.00078815
Iteration 12/25 | Loss: 0.00078815
Iteration 13/25 | Loss: 0.00078815
Iteration 14/25 | Loss: 0.00078815
Iteration 15/25 | Loss: 0.00078815
Iteration 16/25 | Loss: 0.00078815
Iteration 17/25 | Loss: 0.00078815
Iteration 18/25 | Loss: 0.00078815
Iteration 19/25 | Loss: 0.00078815
Iteration 20/25 | Loss: 0.00078815
Iteration 21/25 | Loss: 0.00078815
Iteration 22/25 | Loss: 0.00078815
Iteration 23/25 | Loss: 0.00078815
Iteration 24/25 | Loss: 0.00078815
Iteration 25/25 | Loss: 0.00078815

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078815
Iteration 2/1000 | Loss: 0.00002406
Iteration 3/1000 | Loss: 0.00001491
Iteration 4/1000 | Loss: 0.00001302
Iteration 5/1000 | Loss: 0.00001211
Iteration 6/1000 | Loss: 0.00001146
Iteration 7/1000 | Loss: 0.00001100
Iteration 8/1000 | Loss: 0.00001098
Iteration 9/1000 | Loss: 0.00001086
Iteration 10/1000 | Loss: 0.00001079
Iteration 11/1000 | Loss: 0.00001079
Iteration 12/1000 | Loss: 0.00001079
Iteration 13/1000 | Loss: 0.00001078
Iteration 14/1000 | Loss: 0.00001076
Iteration 15/1000 | Loss: 0.00001076
Iteration 16/1000 | Loss: 0.00001076
Iteration 17/1000 | Loss: 0.00001076
Iteration 18/1000 | Loss: 0.00001076
Iteration 19/1000 | Loss: 0.00001076
Iteration 20/1000 | Loss: 0.00001076
Iteration 21/1000 | Loss: 0.00001076
Iteration 22/1000 | Loss: 0.00001076
Iteration 23/1000 | Loss: 0.00001075
Iteration 24/1000 | Loss: 0.00001075
Iteration 25/1000 | Loss: 0.00001075
Iteration 26/1000 | Loss: 0.00001075
Iteration 27/1000 | Loss: 0.00001075
Iteration 28/1000 | Loss: 0.00001074
Iteration 29/1000 | Loss: 0.00001074
Iteration 30/1000 | Loss: 0.00001070
Iteration 31/1000 | Loss: 0.00001069
Iteration 32/1000 | Loss: 0.00001068
Iteration 33/1000 | Loss: 0.00001068
Iteration 34/1000 | Loss: 0.00001067
Iteration 35/1000 | Loss: 0.00001066
Iteration 36/1000 | Loss: 0.00001064
Iteration 37/1000 | Loss: 0.00001060
Iteration 38/1000 | Loss: 0.00001060
Iteration 39/1000 | Loss: 0.00001059
Iteration 40/1000 | Loss: 0.00001058
Iteration 41/1000 | Loss: 0.00001056
Iteration 42/1000 | Loss: 0.00001056
Iteration 43/1000 | Loss: 0.00001056
Iteration 44/1000 | Loss: 0.00001054
Iteration 45/1000 | Loss: 0.00001054
Iteration 46/1000 | Loss: 0.00001053
Iteration 47/1000 | Loss: 0.00001052
Iteration 48/1000 | Loss: 0.00001051
Iteration 49/1000 | Loss: 0.00001051
Iteration 50/1000 | Loss: 0.00001050
Iteration 51/1000 | Loss: 0.00001049
Iteration 52/1000 | Loss: 0.00001049
Iteration 53/1000 | Loss: 0.00001047
Iteration 54/1000 | Loss: 0.00001047
Iteration 55/1000 | Loss: 0.00001047
Iteration 56/1000 | Loss: 0.00001046
Iteration 57/1000 | Loss: 0.00001046
Iteration 58/1000 | Loss: 0.00001042
Iteration 59/1000 | Loss: 0.00001042
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001041
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001037
Iteration 64/1000 | Loss: 0.00001036
Iteration 65/1000 | Loss: 0.00001036
Iteration 66/1000 | Loss: 0.00001035
Iteration 67/1000 | Loss: 0.00001034
Iteration 68/1000 | Loss: 0.00001034
Iteration 69/1000 | Loss: 0.00001034
Iteration 70/1000 | Loss: 0.00001033
Iteration 71/1000 | Loss: 0.00001032
Iteration 72/1000 | Loss: 0.00001032
Iteration 73/1000 | Loss: 0.00001031
Iteration 74/1000 | Loss: 0.00001031
Iteration 75/1000 | Loss: 0.00001031
Iteration 76/1000 | Loss: 0.00001030
Iteration 77/1000 | Loss: 0.00001030
Iteration 78/1000 | Loss: 0.00001030
Iteration 79/1000 | Loss: 0.00001029
Iteration 80/1000 | Loss: 0.00001029
Iteration 81/1000 | Loss: 0.00001029
Iteration 82/1000 | Loss: 0.00001029
Iteration 83/1000 | Loss: 0.00001028
Iteration 84/1000 | Loss: 0.00001028
Iteration 85/1000 | Loss: 0.00001028
Iteration 86/1000 | Loss: 0.00001028
Iteration 87/1000 | Loss: 0.00001028
Iteration 88/1000 | Loss: 0.00001028
Iteration 89/1000 | Loss: 0.00001028
Iteration 90/1000 | Loss: 0.00001027
Iteration 91/1000 | Loss: 0.00001027
Iteration 92/1000 | Loss: 0.00001027
Iteration 93/1000 | Loss: 0.00001027
Iteration 94/1000 | Loss: 0.00001027
Iteration 95/1000 | Loss: 0.00001026
Iteration 96/1000 | Loss: 0.00001026
Iteration 97/1000 | Loss: 0.00001026
Iteration 98/1000 | Loss: 0.00001025
Iteration 99/1000 | Loss: 0.00001024
Iteration 100/1000 | Loss: 0.00001024
Iteration 101/1000 | Loss: 0.00001024
Iteration 102/1000 | Loss: 0.00001023
Iteration 103/1000 | Loss: 0.00001023
Iteration 104/1000 | Loss: 0.00001022
Iteration 105/1000 | Loss: 0.00001022
Iteration 106/1000 | Loss: 0.00001022
Iteration 107/1000 | Loss: 0.00001021
Iteration 108/1000 | Loss: 0.00001021
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001020
Iteration 112/1000 | Loss: 0.00001020
Iteration 113/1000 | Loss: 0.00001020
Iteration 114/1000 | Loss: 0.00001020
Iteration 115/1000 | Loss: 0.00001020
Iteration 116/1000 | Loss: 0.00001020
Iteration 117/1000 | Loss: 0.00001020
Iteration 118/1000 | Loss: 0.00001019
Iteration 119/1000 | Loss: 0.00001019
Iteration 120/1000 | Loss: 0.00001019
Iteration 121/1000 | Loss: 0.00001019
Iteration 122/1000 | Loss: 0.00001019
Iteration 123/1000 | Loss: 0.00001019
Iteration 124/1000 | Loss: 0.00001019
Iteration 125/1000 | Loss: 0.00001019
Iteration 126/1000 | Loss: 0.00001018
Iteration 127/1000 | Loss: 0.00001018
Iteration 128/1000 | Loss: 0.00001018
Iteration 129/1000 | Loss: 0.00001018
Iteration 130/1000 | Loss: 0.00001017
Iteration 131/1000 | Loss: 0.00001017
Iteration 132/1000 | Loss: 0.00001017
Iteration 133/1000 | Loss: 0.00001017
Iteration 134/1000 | Loss: 0.00001017
Iteration 135/1000 | Loss: 0.00001017
Iteration 136/1000 | Loss: 0.00001017
Iteration 137/1000 | Loss: 0.00001017
Iteration 138/1000 | Loss: 0.00001017
Iteration 139/1000 | Loss: 0.00001017
Iteration 140/1000 | Loss: 0.00001017
Iteration 141/1000 | Loss: 0.00001017
Iteration 142/1000 | Loss: 0.00001017
Iteration 143/1000 | Loss: 0.00001017
Iteration 144/1000 | Loss: 0.00001016
Iteration 145/1000 | Loss: 0.00001016
Iteration 146/1000 | Loss: 0.00001016
Iteration 147/1000 | Loss: 0.00001015
Iteration 148/1000 | Loss: 0.00001015
Iteration 149/1000 | Loss: 0.00001015
Iteration 150/1000 | Loss: 0.00001015
Iteration 151/1000 | Loss: 0.00001015
Iteration 152/1000 | Loss: 0.00001015
Iteration 153/1000 | Loss: 0.00001015
Iteration 154/1000 | Loss: 0.00001015
Iteration 155/1000 | Loss: 0.00001015
Iteration 156/1000 | Loss: 0.00001015
Iteration 157/1000 | Loss: 0.00001015
Iteration 158/1000 | Loss: 0.00001014
Iteration 159/1000 | Loss: 0.00001014
Iteration 160/1000 | Loss: 0.00001014
Iteration 161/1000 | Loss: 0.00001014
Iteration 162/1000 | Loss: 0.00001014
Iteration 163/1000 | Loss: 0.00001014
Iteration 164/1000 | Loss: 0.00001014
Iteration 165/1000 | Loss: 0.00001014
Iteration 166/1000 | Loss: 0.00001014
Iteration 167/1000 | Loss: 0.00001013
Iteration 168/1000 | Loss: 0.00001013
Iteration 169/1000 | Loss: 0.00001013
Iteration 170/1000 | Loss: 0.00001012
Iteration 171/1000 | Loss: 0.00001012
Iteration 172/1000 | Loss: 0.00001012
Iteration 173/1000 | Loss: 0.00001012
Iteration 174/1000 | Loss: 0.00001012
Iteration 175/1000 | Loss: 0.00001011
Iteration 176/1000 | Loss: 0.00001011
Iteration 177/1000 | Loss: 0.00001011
Iteration 178/1000 | Loss: 0.00001011
Iteration 179/1000 | Loss: 0.00001011
Iteration 180/1000 | Loss: 0.00001010
Iteration 181/1000 | Loss: 0.00001010
Iteration 182/1000 | Loss: 0.00001010
Iteration 183/1000 | Loss: 0.00001009
Iteration 184/1000 | Loss: 0.00001009
Iteration 185/1000 | Loss: 0.00001009
Iteration 186/1000 | Loss: 0.00001009
Iteration 187/1000 | Loss: 0.00001008
Iteration 188/1000 | Loss: 0.00001008
Iteration 189/1000 | Loss: 0.00001008
Iteration 190/1000 | Loss: 0.00001008
Iteration 191/1000 | Loss: 0.00001007
Iteration 192/1000 | Loss: 0.00001007
Iteration 193/1000 | Loss: 0.00001007
Iteration 194/1000 | Loss: 0.00001007
Iteration 195/1000 | Loss: 0.00001006
Iteration 196/1000 | Loss: 0.00001006
Iteration 197/1000 | Loss: 0.00001006
Iteration 198/1000 | Loss: 0.00001006
Iteration 199/1000 | Loss: 0.00001005
Iteration 200/1000 | Loss: 0.00001005
Iteration 201/1000 | Loss: 0.00001005
Iteration 202/1000 | Loss: 0.00001005
Iteration 203/1000 | Loss: 0.00001005
Iteration 204/1000 | Loss: 0.00001005
Iteration 205/1000 | Loss: 0.00001005
Iteration 206/1000 | Loss: 0.00001005
Iteration 207/1000 | Loss: 0.00001005
Iteration 208/1000 | Loss: 0.00001005
Iteration 209/1000 | Loss: 0.00001005
Iteration 210/1000 | Loss: 0.00001004
Iteration 211/1000 | Loss: 0.00001004
Iteration 212/1000 | Loss: 0.00001004
Iteration 213/1000 | Loss: 0.00001004
Iteration 214/1000 | Loss: 0.00001004
Iteration 215/1000 | Loss: 0.00001003
Iteration 216/1000 | Loss: 0.00001003
Iteration 217/1000 | Loss: 0.00001003
Iteration 218/1000 | Loss: 0.00001003
Iteration 219/1000 | Loss: 0.00001003
Iteration 220/1000 | Loss: 0.00001003
Iteration 221/1000 | Loss: 0.00001003
Iteration 222/1000 | Loss: 0.00001003
Iteration 223/1000 | Loss: 0.00001003
Iteration 224/1000 | Loss: 0.00001003
Iteration 225/1000 | Loss: 0.00001003
Iteration 226/1000 | Loss: 0.00001002
Iteration 227/1000 | Loss: 0.00001002
Iteration 228/1000 | Loss: 0.00001002
Iteration 229/1000 | Loss: 0.00001002
Iteration 230/1000 | Loss: 0.00001002
Iteration 231/1000 | Loss: 0.00001002
Iteration 232/1000 | Loss: 0.00001002
Iteration 233/1000 | Loss: 0.00001002
Iteration 234/1000 | Loss: 0.00001002
Iteration 235/1000 | Loss: 0.00001002
Iteration 236/1000 | Loss: 0.00001002
Iteration 237/1000 | Loss: 0.00001001
Iteration 238/1000 | Loss: 0.00001001
Iteration 239/1000 | Loss: 0.00001001
Iteration 240/1000 | Loss: 0.00001001
Iteration 241/1000 | Loss: 0.00001001
Iteration 242/1000 | Loss: 0.00001001
Iteration 243/1000 | Loss: 0.00001001
Iteration 244/1000 | Loss: 0.00001001
Iteration 245/1000 | Loss: 0.00001001
Iteration 246/1000 | Loss: 0.00001001
Iteration 247/1000 | Loss: 0.00001001
Iteration 248/1000 | Loss: 0.00001001
Iteration 249/1000 | Loss: 0.00001001
Iteration 250/1000 | Loss: 0.00001001
Iteration 251/1000 | Loss: 0.00001001
Iteration 252/1000 | Loss: 0.00001001
Iteration 253/1000 | Loss: 0.00001001
Iteration 254/1000 | Loss: 0.00001001
Iteration 255/1000 | Loss: 0.00001001
Iteration 256/1000 | Loss: 0.00001001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [1.0007518540078308e-05, 1.0007518540078308e-05, 1.0007518540078308e-05, 1.0007518540078308e-05, 1.0007518540078308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0007518540078308e-05

Optimization complete. Final v2v error: 2.724393129348755 mm

Highest mean error: 2.7999558448791504 mm for frame 72

Lowest mean error: 2.665911912918091 mm for frame 117

Saving results

Total time: 39.17144560813904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841682
Iteration 2/25 | Loss: 0.00145176
Iteration 3/25 | Loss: 0.00129361
Iteration 4/25 | Loss: 0.00127392
Iteration 5/25 | Loss: 0.00126771
Iteration 6/25 | Loss: 0.00126702
Iteration 7/25 | Loss: 0.00126702
Iteration 8/25 | Loss: 0.00126702
Iteration 9/25 | Loss: 0.00126702
Iteration 10/25 | Loss: 0.00126702
Iteration 11/25 | Loss: 0.00126702
Iteration 12/25 | Loss: 0.00126702
Iteration 13/25 | Loss: 0.00126702
Iteration 14/25 | Loss: 0.00126702
Iteration 15/25 | Loss: 0.00126702
Iteration 16/25 | Loss: 0.00126702
Iteration 17/25 | Loss: 0.00126702
Iteration 18/25 | Loss: 0.00126702
Iteration 19/25 | Loss: 0.00126702
Iteration 20/25 | Loss: 0.00126702
Iteration 21/25 | Loss: 0.00126702
Iteration 22/25 | Loss: 0.00126702
Iteration 23/25 | Loss: 0.00126702
Iteration 24/25 | Loss: 0.00126702
Iteration 25/25 | Loss: 0.00126702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012670163996517658, 0.0012670163996517658, 0.0012670163996517658, 0.0012670163996517658, 0.0012670163996517658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012670163996517658

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96054167
Iteration 2/25 | Loss: 0.00053672
Iteration 3/25 | Loss: 0.00053672
Iteration 4/25 | Loss: 0.00053672
Iteration 5/25 | Loss: 0.00053672
Iteration 6/25 | Loss: 0.00053671
Iteration 7/25 | Loss: 0.00053671
Iteration 8/25 | Loss: 0.00053671
Iteration 9/25 | Loss: 0.00053671
Iteration 10/25 | Loss: 0.00053671
Iteration 11/25 | Loss: 0.00053671
Iteration 12/25 | Loss: 0.00053671
Iteration 13/25 | Loss: 0.00053671
Iteration 14/25 | Loss: 0.00053671
Iteration 15/25 | Loss: 0.00053671
Iteration 16/25 | Loss: 0.00053671
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005367138655856252, 0.0005367138655856252, 0.0005367138655856252, 0.0005367138655856252, 0.0005367138655856252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005367138655856252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053671
Iteration 2/1000 | Loss: 0.00004102
Iteration 3/1000 | Loss: 0.00003313
Iteration 4/1000 | Loss: 0.00003087
Iteration 5/1000 | Loss: 0.00002961
Iteration 6/1000 | Loss: 0.00002880
Iteration 7/1000 | Loss: 0.00002836
Iteration 8/1000 | Loss: 0.00002816
Iteration 9/1000 | Loss: 0.00002789
Iteration 10/1000 | Loss: 0.00002783
Iteration 11/1000 | Loss: 0.00002766
Iteration 12/1000 | Loss: 0.00002766
Iteration 13/1000 | Loss: 0.00002749
Iteration 14/1000 | Loss: 0.00002747
Iteration 15/1000 | Loss: 0.00002747
Iteration 16/1000 | Loss: 0.00002747
Iteration 17/1000 | Loss: 0.00002747
Iteration 18/1000 | Loss: 0.00002747
Iteration 19/1000 | Loss: 0.00002747
Iteration 20/1000 | Loss: 0.00002747
Iteration 21/1000 | Loss: 0.00002747
Iteration 22/1000 | Loss: 0.00002747
Iteration 23/1000 | Loss: 0.00002746
Iteration 24/1000 | Loss: 0.00002746
Iteration 25/1000 | Loss: 0.00002746
Iteration 26/1000 | Loss: 0.00002746
Iteration 27/1000 | Loss: 0.00002745
Iteration 28/1000 | Loss: 0.00002744
Iteration 29/1000 | Loss: 0.00002736
Iteration 30/1000 | Loss: 0.00002736
Iteration 31/1000 | Loss: 0.00002735
Iteration 32/1000 | Loss: 0.00002735
Iteration 33/1000 | Loss: 0.00002735
Iteration 34/1000 | Loss: 0.00002735
Iteration 35/1000 | Loss: 0.00002734
Iteration 36/1000 | Loss: 0.00002731
Iteration 37/1000 | Loss: 0.00002730
Iteration 38/1000 | Loss: 0.00002730
Iteration 39/1000 | Loss: 0.00002730
Iteration 40/1000 | Loss: 0.00002729
Iteration 41/1000 | Loss: 0.00002729
Iteration 42/1000 | Loss: 0.00002729
Iteration 43/1000 | Loss: 0.00002729
Iteration 44/1000 | Loss: 0.00002729
Iteration 45/1000 | Loss: 0.00002729
Iteration 46/1000 | Loss: 0.00002729
Iteration 47/1000 | Loss: 0.00002729
Iteration 48/1000 | Loss: 0.00002729
Iteration 49/1000 | Loss: 0.00002729
Iteration 50/1000 | Loss: 0.00002729
Iteration 51/1000 | Loss: 0.00002728
Iteration 52/1000 | Loss: 0.00002728
Iteration 53/1000 | Loss: 0.00002728
Iteration 54/1000 | Loss: 0.00002728
Iteration 55/1000 | Loss: 0.00002728
Iteration 56/1000 | Loss: 0.00002728
Iteration 57/1000 | Loss: 0.00002728
Iteration 58/1000 | Loss: 0.00002728
Iteration 59/1000 | Loss: 0.00002728
Iteration 60/1000 | Loss: 0.00002728
Iteration 61/1000 | Loss: 0.00002727
Iteration 62/1000 | Loss: 0.00002727
Iteration 63/1000 | Loss: 0.00002727
Iteration 64/1000 | Loss: 0.00002727
Iteration 65/1000 | Loss: 0.00002727
Iteration 66/1000 | Loss: 0.00002727
Iteration 67/1000 | Loss: 0.00002727
Iteration 68/1000 | Loss: 0.00002727
Iteration 69/1000 | Loss: 0.00002727
Iteration 70/1000 | Loss: 0.00002727
Iteration 71/1000 | Loss: 0.00002727
Iteration 72/1000 | Loss: 0.00002727
Iteration 73/1000 | Loss: 0.00002727
Iteration 74/1000 | Loss: 0.00002726
Iteration 75/1000 | Loss: 0.00002726
Iteration 76/1000 | Loss: 0.00002726
Iteration 77/1000 | Loss: 0.00002726
Iteration 78/1000 | Loss: 0.00002726
Iteration 79/1000 | Loss: 0.00002726
Iteration 80/1000 | Loss: 0.00002726
Iteration 81/1000 | Loss: 0.00002726
Iteration 82/1000 | Loss: 0.00002726
Iteration 83/1000 | Loss: 0.00002726
Iteration 84/1000 | Loss: 0.00002726
Iteration 85/1000 | Loss: 0.00002726
Iteration 86/1000 | Loss: 0.00002726
Iteration 87/1000 | Loss: 0.00002726
Iteration 88/1000 | Loss: 0.00002725
Iteration 89/1000 | Loss: 0.00002725
Iteration 90/1000 | Loss: 0.00002725
Iteration 91/1000 | Loss: 0.00002725
Iteration 92/1000 | Loss: 0.00002725
Iteration 93/1000 | Loss: 0.00002725
Iteration 94/1000 | Loss: 0.00002725
Iteration 95/1000 | Loss: 0.00002725
Iteration 96/1000 | Loss: 0.00002725
Iteration 97/1000 | Loss: 0.00002725
Iteration 98/1000 | Loss: 0.00002725
Iteration 99/1000 | Loss: 0.00002725
Iteration 100/1000 | Loss: 0.00002725
Iteration 101/1000 | Loss: 0.00002725
Iteration 102/1000 | Loss: 0.00002725
Iteration 103/1000 | Loss: 0.00002725
Iteration 104/1000 | Loss: 0.00002725
Iteration 105/1000 | Loss: 0.00002725
Iteration 106/1000 | Loss: 0.00002725
Iteration 107/1000 | Loss: 0.00002725
Iteration 108/1000 | Loss: 0.00002725
Iteration 109/1000 | Loss: 0.00002725
Iteration 110/1000 | Loss: 0.00002725
Iteration 111/1000 | Loss: 0.00002725
Iteration 112/1000 | Loss: 0.00002725
Iteration 113/1000 | Loss: 0.00002725
Iteration 114/1000 | Loss: 0.00002725
Iteration 115/1000 | Loss: 0.00002725
Iteration 116/1000 | Loss: 0.00002725
Iteration 117/1000 | Loss: 0.00002725
Iteration 118/1000 | Loss: 0.00002725
Iteration 119/1000 | Loss: 0.00002725
Iteration 120/1000 | Loss: 0.00002725
Iteration 121/1000 | Loss: 0.00002725
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.725036392803304e-05, 2.725036392803304e-05, 2.725036392803304e-05, 2.725036392803304e-05, 2.725036392803304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.725036392803304e-05

Optimization complete. Final v2v error: 4.363414764404297 mm

Highest mean error: 4.458626747131348 mm for frame 14

Lowest mean error: 4.232129096984863 mm for frame 170

Saving results

Total time: 31.815717458724976
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849105
Iteration 2/25 | Loss: 0.00167992
Iteration 3/25 | Loss: 0.00137743
Iteration 4/25 | Loss: 0.00131934
Iteration 5/25 | Loss: 0.00128042
Iteration 6/25 | Loss: 0.00126992
Iteration 7/25 | Loss: 0.00129560
Iteration 8/25 | Loss: 0.00123985
Iteration 9/25 | Loss: 0.00125167
Iteration 10/25 | Loss: 0.00124340
Iteration 11/25 | Loss: 0.00123349
Iteration 12/25 | Loss: 0.00123707
Iteration 13/25 | Loss: 0.00123432
Iteration 14/25 | Loss: 0.00122863
Iteration 15/25 | Loss: 0.00123398
Iteration 16/25 | Loss: 0.00122829
Iteration 17/25 | Loss: 0.00122824
Iteration 18/25 | Loss: 0.00122824
Iteration 19/25 | Loss: 0.00122824
Iteration 20/25 | Loss: 0.00122824
Iteration 21/25 | Loss: 0.00122824
Iteration 22/25 | Loss: 0.00122824
Iteration 23/25 | Loss: 0.00122824
Iteration 24/25 | Loss: 0.00122824
Iteration 25/25 | Loss: 0.00122824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 12.41609859
Iteration 2/25 | Loss: 0.00084747
Iteration 3/25 | Loss: 0.00084735
Iteration 4/25 | Loss: 0.00084735
Iteration 5/25 | Loss: 0.00084735
Iteration 6/25 | Loss: 0.00084735
Iteration 7/25 | Loss: 0.00084735
Iteration 8/25 | Loss: 0.00084735
Iteration 9/25 | Loss: 0.00084735
Iteration 10/25 | Loss: 0.00084735
Iteration 11/25 | Loss: 0.00084735
Iteration 12/25 | Loss: 0.00084735
Iteration 13/25 | Loss: 0.00084735
Iteration 14/25 | Loss: 0.00084735
Iteration 15/25 | Loss: 0.00084735
Iteration 16/25 | Loss: 0.00084735
Iteration 17/25 | Loss: 0.00084735
Iteration 18/25 | Loss: 0.00084735
Iteration 19/25 | Loss: 0.00084735
Iteration 20/25 | Loss: 0.00084735
Iteration 21/25 | Loss: 0.00084735
Iteration 22/25 | Loss: 0.00084735
Iteration 23/25 | Loss: 0.00084735
Iteration 24/25 | Loss: 0.00084735
Iteration 25/25 | Loss: 0.00084735

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084735
Iteration 2/1000 | Loss: 0.00003767
Iteration 3/1000 | Loss: 0.00017873
Iteration 4/1000 | Loss: 0.00002901
Iteration 5/1000 | Loss: 0.00002298
Iteration 6/1000 | Loss: 0.00002172
Iteration 7/1000 | Loss: 0.00020869
Iteration 8/1000 | Loss: 0.00073705
Iteration 9/1000 | Loss: 0.00004044
Iteration 10/1000 | Loss: 0.00002641
Iteration 11/1000 | Loss: 0.00002175
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00001955
Iteration 14/1000 | Loss: 0.00001922
Iteration 15/1000 | Loss: 0.00001898
Iteration 16/1000 | Loss: 0.00011420
Iteration 17/1000 | Loss: 0.00011419
Iteration 18/1000 | Loss: 0.00039261
Iteration 19/1000 | Loss: 0.00002675
Iteration 20/1000 | Loss: 0.00002894
Iteration 21/1000 | Loss: 0.00001883
Iteration 22/1000 | Loss: 0.00008936
Iteration 23/1000 | Loss: 0.00002505
Iteration 24/1000 | Loss: 0.00001866
Iteration 25/1000 | Loss: 0.00002220
Iteration 26/1000 | Loss: 0.00002220
Iteration 27/1000 | Loss: 0.00004211
Iteration 28/1000 | Loss: 0.00007542
Iteration 29/1000 | Loss: 0.00010631
Iteration 30/1000 | Loss: 0.00021999
Iteration 31/1000 | Loss: 0.00003212
Iteration 32/1000 | Loss: 0.00001924
Iteration 33/1000 | Loss: 0.00001852
Iteration 34/1000 | Loss: 0.00001829
Iteration 35/1000 | Loss: 0.00001827
Iteration 36/1000 | Loss: 0.00001826
Iteration 37/1000 | Loss: 0.00001823
Iteration 38/1000 | Loss: 0.00001820
Iteration 39/1000 | Loss: 0.00001820
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001819
Iteration 42/1000 | Loss: 0.00001818
Iteration 43/1000 | Loss: 0.00001818
Iteration 44/1000 | Loss: 0.00001817
Iteration 45/1000 | Loss: 0.00001817
Iteration 46/1000 | Loss: 0.00001816
Iteration 47/1000 | Loss: 0.00001816
Iteration 48/1000 | Loss: 0.00001815
Iteration 49/1000 | Loss: 0.00001815
Iteration 50/1000 | Loss: 0.00005310
Iteration 51/1000 | Loss: 0.00008167
Iteration 52/1000 | Loss: 0.00001817
Iteration 53/1000 | Loss: 0.00001810
Iteration 54/1000 | Loss: 0.00001810
Iteration 55/1000 | Loss: 0.00001809
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001808
Iteration 58/1000 | Loss: 0.00001808
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001808
Iteration 61/1000 | Loss: 0.00001808
Iteration 62/1000 | Loss: 0.00001808
Iteration 63/1000 | Loss: 0.00001808
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00001807
Iteration 66/1000 | Loss: 0.00001807
Iteration 67/1000 | Loss: 0.00001807
Iteration 68/1000 | Loss: 0.00001807
Iteration 69/1000 | Loss: 0.00001807
Iteration 70/1000 | Loss: 0.00001807
Iteration 71/1000 | Loss: 0.00001807
Iteration 72/1000 | Loss: 0.00001807
Iteration 73/1000 | Loss: 0.00001807
Iteration 74/1000 | Loss: 0.00001807
Iteration 75/1000 | Loss: 0.00001807
Iteration 76/1000 | Loss: 0.00001807
Iteration 77/1000 | Loss: 0.00001806
Iteration 78/1000 | Loss: 0.00001806
Iteration 79/1000 | Loss: 0.00001806
Iteration 80/1000 | Loss: 0.00001806
Iteration 81/1000 | Loss: 0.00001805
Iteration 82/1000 | Loss: 0.00001805
Iteration 83/1000 | Loss: 0.00001805
Iteration 84/1000 | Loss: 0.00001805
Iteration 85/1000 | Loss: 0.00001805
Iteration 86/1000 | Loss: 0.00001805
Iteration 87/1000 | Loss: 0.00001805
Iteration 88/1000 | Loss: 0.00001804
Iteration 89/1000 | Loss: 0.00001804
Iteration 90/1000 | Loss: 0.00001804
Iteration 91/1000 | Loss: 0.00001803
Iteration 92/1000 | Loss: 0.00001803
Iteration 93/1000 | Loss: 0.00001802
Iteration 94/1000 | Loss: 0.00001802
Iteration 95/1000 | Loss: 0.00001801
Iteration 96/1000 | Loss: 0.00001801
Iteration 97/1000 | Loss: 0.00001801
Iteration 98/1000 | Loss: 0.00001801
Iteration 99/1000 | Loss: 0.00001800
Iteration 100/1000 | Loss: 0.00001800
Iteration 101/1000 | Loss: 0.00001800
Iteration 102/1000 | Loss: 0.00001800
Iteration 103/1000 | Loss: 0.00001800
Iteration 104/1000 | Loss: 0.00001800
Iteration 105/1000 | Loss: 0.00001800
Iteration 106/1000 | Loss: 0.00001799
Iteration 107/1000 | Loss: 0.00001799
Iteration 108/1000 | Loss: 0.00001799
Iteration 109/1000 | Loss: 0.00001799
Iteration 110/1000 | Loss: 0.00001799
Iteration 111/1000 | Loss: 0.00001798
Iteration 112/1000 | Loss: 0.00001798
Iteration 113/1000 | Loss: 0.00001797
Iteration 114/1000 | Loss: 0.00001797
Iteration 115/1000 | Loss: 0.00001797
Iteration 116/1000 | Loss: 0.00001797
Iteration 117/1000 | Loss: 0.00001796
Iteration 118/1000 | Loss: 0.00001796
Iteration 119/1000 | Loss: 0.00001796
Iteration 120/1000 | Loss: 0.00001795
Iteration 121/1000 | Loss: 0.00001795
Iteration 122/1000 | Loss: 0.00001795
Iteration 123/1000 | Loss: 0.00001794
Iteration 124/1000 | Loss: 0.00001794
Iteration 125/1000 | Loss: 0.00001794
Iteration 126/1000 | Loss: 0.00001794
Iteration 127/1000 | Loss: 0.00001794
Iteration 128/1000 | Loss: 0.00001794
Iteration 129/1000 | Loss: 0.00001793
Iteration 130/1000 | Loss: 0.00001793
Iteration 131/1000 | Loss: 0.00001793
Iteration 132/1000 | Loss: 0.00001793
Iteration 133/1000 | Loss: 0.00001793
Iteration 134/1000 | Loss: 0.00001793
Iteration 135/1000 | Loss: 0.00001793
Iteration 136/1000 | Loss: 0.00001792
Iteration 137/1000 | Loss: 0.00001792
Iteration 138/1000 | Loss: 0.00001792
Iteration 139/1000 | Loss: 0.00001792
Iteration 140/1000 | Loss: 0.00001792
Iteration 141/1000 | Loss: 0.00001792
Iteration 142/1000 | Loss: 0.00001792
Iteration 143/1000 | Loss: 0.00001791
Iteration 144/1000 | Loss: 0.00001791
Iteration 145/1000 | Loss: 0.00011670
Iteration 146/1000 | Loss: 0.00052275
Iteration 147/1000 | Loss: 0.00003996
Iteration 148/1000 | Loss: 0.00002378
Iteration 149/1000 | Loss: 0.00001965
Iteration 150/1000 | Loss: 0.00001798
Iteration 151/1000 | Loss: 0.00001798
Iteration 152/1000 | Loss: 0.00004015
Iteration 153/1000 | Loss: 0.00001793
Iteration 154/1000 | Loss: 0.00001792
Iteration 155/1000 | Loss: 0.00001792
Iteration 156/1000 | Loss: 0.00001791
Iteration 157/1000 | Loss: 0.00001791
Iteration 158/1000 | Loss: 0.00001790
Iteration 159/1000 | Loss: 0.00001790
Iteration 160/1000 | Loss: 0.00007632
Iteration 161/1000 | Loss: 0.00001816
Iteration 162/1000 | Loss: 0.00003346
Iteration 163/1000 | Loss: 0.00001817
Iteration 164/1000 | Loss: 0.00001794
Iteration 165/1000 | Loss: 0.00001790
Iteration 166/1000 | Loss: 0.00001790
Iteration 167/1000 | Loss: 0.00001790
Iteration 168/1000 | Loss: 0.00001790
Iteration 169/1000 | Loss: 0.00001790
Iteration 170/1000 | Loss: 0.00001790
Iteration 171/1000 | Loss: 0.00001790
Iteration 172/1000 | Loss: 0.00001790
Iteration 173/1000 | Loss: 0.00001790
Iteration 174/1000 | Loss: 0.00001790
Iteration 175/1000 | Loss: 0.00001789
Iteration 176/1000 | Loss: 0.00001789
Iteration 177/1000 | Loss: 0.00001789
Iteration 178/1000 | Loss: 0.00001789
Iteration 179/1000 | Loss: 0.00001789
Iteration 180/1000 | Loss: 0.00001789
Iteration 181/1000 | Loss: 0.00001789
Iteration 182/1000 | Loss: 0.00001789
Iteration 183/1000 | Loss: 0.00001789
Iteration 184/1000 | Loss: 0.00001789
Iteration 185/1000 | Loss: 0.00001789
Iteration 186/1000 | Loss: 0.00001788
Iteration 187/1000 | Loss: 0.00001788
Iteration 188/1000 | Loss: 0.00001786
Iteration 189/1000 | Loss: 0.00001786
Iteration 190/1000 | Loss: 0.00001786
Iteration 191/1000 | Loss: 0.00001785
Iteration 192/1000 | Loss: 0.00001785
Iteration 193/1000 | Loss: 0.00001784
Iteration 194/1000 | Loss: 0.00001784
Iteration 195/1000 | Loss: 0.00001784
Iteration 196/1000 | Loss: 0.00001783
Iteration 197/1000 | Loss: 0.00001783
Iteration 198/1000 | Loss: 0.00001783
Iteration 199/1000 | Loss: 0.00007974
Iteration 200/1000 | Loss: 0.00002028
Iteration 201/1000 | Loss: 0.00008254
Iteration 202/1000 | Loss: 0.00003951
Iteration 203/1000 | Loss: 0.00003608
Iteration 204/1000 | Loss: 0.00001782
Iteration 205/1000 | Loss: 0.00001781
Iteration 206/1000 | Loss: 0.00001781
Iteration 207/1000 | Loss: 0.00001780
Iteration 208/1000 | Loss: 0.00001780
Iteration 209/1000 | Loss: 0.00001780
Iteration 210/1000 | Loss: 0.00001780
Iteration 211/1000 | Loss: 0.00001780
Iteration 212/1000 | Loss: 0.00001780
Iteration 213/1000 | Loss: 0.00001779
Iteration 214/1000 | Loss: 0.00001779
Iteration 215/1000 | Loss: 0.00001779
Iteration 216/1000 | Loss: 0.00001779
Iteration 217/1000 | Loss: 0.00001779
Iteration 218/1000 | Loss: 0.00001779
Iteration 219/1000 | Loss: 0.00001779
Iteration 220/1000 | Loss: 0.00001779
Iteration 221/1000 | Loss: 0.00001779
Iteration 222/1000 | Loss: 0.00001779
Iteration 223/1000 | Loss: 0.00001779
Iteration 224/1000 | Loss: 0.00001778
Iteration 225/1000 | Loss: 0.00001778
Iteration 226/1000 | Loss: 0.00001778
Iteration 227/1000 | Loss: 0.00001778
Iteration 228/1000 | Loss: 0.00001778
Iteration 229/1000 | Loss: 0.00001778
Iteration 230/1000 | Loss: 0.00001778
Iteration 231/1000 | Loss: 0.00001778
Iteration 232/1000 | Loss: 0.00001778
Iteration 233/1000 | Loss: 0.00001778
Iteration 234/1000 | Loss: 0.00001778
Iteration 235/1000 | Loss: 0.00001778
Iteration 236/1000 | Loss: 0.00001778
Iteration 237/1000 | Loss: 0.00001778
Iteration 238/1000 | Loss: 0.00001778
Iteration 239/1000 | Loss: 0.00004625
Iteration 240/1000 | Loss: 0.00001783
Iteration 241/1000 | Loss: 0.00001781
Iteration 242/1000 | Loss: 0.00001781
Iteration 243/1000 | Loss: 0.00001780
Iteration 244/1000 | Loss: 0.00001780
Iteration 245/1000 | Loss: 0.00001780
Iteration 246/1000 | Loss: 0.00001777
Iteration 247/1000 | Loss: 0.00001776
Iteration 248/1000 | Loss: 0.00001776
Iteration 249/1000 | Loss: 0.00001775
Iteration 250/1000 | Loss: 0.00001775
Iteration 251/1000 | Loss: 0.00001775
Iteration 252/1000 | Loss: 0.00001775
Iteration 253/1000 | Loss: 0.00001774
Iteration 254/1000 | Loss: 0.00001774
Iteration 255/1000 | Loss: 0.00001774
Iteration 256/1000 | Loss: 0.00001774
Iteration 257/1000 | Loss: 0.00001774
Iteration 258/1000 | Loss: 0.00001773
Iteration 259/1000 | Loss: 0.00001773
Iteration 260/1000 | Loss: 0.00001773
Iteration 261/1000 | Loss: 0.00001772
Iteration 262/1000 | Loss: 0.00001772
Iteration 263/1000 | Loss: 0.00001772
Iteration 264/1000 | Loss: 0.00001772
Iteration 265/1000 | Loss: 0.00001772
Iteration 266/1000 | Loss: 0.00001772
Iteration 267/1000 | Loss: 0.00001772
Iteration 268/1000 | Loss: 0.00001771
Iteration 269/1000 | Loss: 0.00001771
Iteration 270/1000 | Loss: 0.00001770
Iteration 271/1000 | Loss: 0.00001770
Iteration 272/1000 | Loss: 0.00001769
Iteration 273/1000 | Loss: 0.00001769
Iteration 274/1000 | Loss: 0.00001769
Iteration 275/1000 | Loss: 0.00001769
Iteration 276/1000 | Loss: 0.00001769
Iteration 277/1000 | Loss: 0.00001768
Iteration 278/1000 | Loss: 0.00001768
Iteration 279/1000 | Loss: 0.00001767
Iteration 280/1000 | Loss: 0.00001767
Iteration 281/1000 | Loss: 0.00001762
Iteration 282/1000 | Loss: 0.00001761
Iteration 283/1000 | Loss: 0.00001761
Iteration 284/1000 | Loss: 0.00001760
Iteration 285/1000 | Loss: 0.00001760
Iteration 286/1000 | Loss: 0.00001759
Iteration 287/1000 | Loss: 0.00001759
Iteration 288/1000 | Loss: 0.00001759
Iteration 289/1000 | Loss: 0.00001758
Iteration 290/1000 | Loss: 0.00001758
Iteration 291/1000 | Loss: 0.00001757
Iteration 292/1000 | Loss: 0.00001756
Iteration 293/1000 | Loss: 0.00001756
Iteration 294/1000 | Loss: 0.00001756
Iteration 295/1000 | Loss: 0.00001756
Iteration 296/1000 | Loss: 0.00001755
Iteration 297/1000 | Loss: 0.00001755
Iteration 298/1000 | Loss: 0.00001755
Iteration 299/1000 | Loss: 0.00001754
Iteration 300/1000 | Loss: 0.00001754
Iteration 301/1000 | Loss: 0.00001753
Iteration 302/1000 | Loss: 0.00001752
Iteration 303/1000 | Loss: 0.00001752
Iteration 304/1000 | Loss: 0.00001751
Iteration 305/1000 | Loss: 0.00001751
Iteration 306/1000 | Loss: 0.00001750
Iteration 307/1000 | Loss: 0.00001750
Iteration 308/1000 | Loss: 0.00001749
Iteration 309/1000 | Loss: 0.00001749
Iteration 310/1000 | Loss: 0.00001749
Iteration 311/1000 | Loss: 0.00001748
Iteration 312/1000 | Loss: 0.00001746
Iteration 313/1000 | Loss: 0.00001746
Iteration 314/1000 | Loss: 0.00001745
Iteration 315/1000 | Loss: 0.00002553
Iteration 316/1000 | Loss: 0.00001909
Iteration 317/1000 | Loss: 0.00001843
Iteration 318/1000 | Loss: 0.00006294
Iteration 319/1000 | Loss: 0.00001762
Iteration 320/1000 | Loss: 0.00001753
Iteration 321/1000 | Loss: 0.00001752
Iteration 322/1000 | Loss: 0.00001752
Iteration 323/1000 | Loss: 0.00005594
Iteration 324/1000 | Loss: 0.00002091
Iteration 325/1000 | Loss: 0.00001819
Iteration 326/1000 | Loss: 0.00001746
Iteration 327/1000 | Loss: 0.00003141
Iteration 328/1000 | Loss: 0.00001754
Iteration 329/1000 | Loss: 0.00001739
Iteration 330/1000 | Loss: 0.00001736
Iteration 331/1000 | Loss: 0.00001728
Iteration 332/1000 | Loss: 0.00001728
Iteration 333/1000 | Loss: 0.00001724
Iteration 334/1000 | Loss: 0.00001724
Iteration 335/1000 | Loss: 0.00001724
Iteration 336/1000 | Loss: 0.00001724
Iteration 337/1000 | Loss: 0.00001724
Iteration 338/1000 | Loss: 0.00001724
Iteration 339/1000 | Loss: 0.00001724
Iteration 340/1000 | Loss: 0.00001723
Iteration 341/1000 | Loss: 0.00001723
Iteration 342/1000 | Loss: 0.00001723
Iteration 343/1000 | Loss: 0.00001723
Iteration 344/1000 | Loss: 0.00001723
Iteration 345/1000 | Loss: 0.00001723
Iteration 346/1000 | Loss: 0.00001723
Iteration 347/1000 | Loss: 0.00001722
Iteration 348/1000 | Loss: 0.00001722
Iteration 349/1000 | Loss: 0.00001722
Iteration 350/1000 | Loss: 0.00001722
Iteration 351/1000 | Loss: 0.00001722
Iteration 352/1000 | Loss: 0.00001722
Iteration 353/1000 | Loss: 0.00001722
Iteration 354/1000 | Loss: 0.00001722
Iteration 355/1000 | Loss: 0.00001721
Iteration 356/1000 | Loss: 0.00001721
Iteration 357/1000 | Loss: 0.00001721
Iteration 358/1000 | Loss: 0.00001721
Iteration 359/1000 | Loss: 0.00001720
Iteration 360/1000 | Loss: 0.00001720
Iteration 361/1000 | Loss: 0.00001720
Iteration 362/1000 | Loss: 0.00001720
Iteration 363/1000 | Loss: 0.00001720
Iteration 364/1000 | Loss: 0.00001720
Iteration 365/1000 | Loss: 0.00001720
Iteration 366/1000 | Loss: 0.00001720
Iteration 367/1000 | Loss: 0.00001720
Iteration 368/1000 | Loss: 0.00001719
Iteration 369/1000 | Loss: 0.00001719
Iteration 370/1000 | Loss: 0.00001719
Iteration 371/1000 | Loss: 0.00001718
Iteration 372/1000 | Loss: 0.00001718
Iteration 373/1000 | Loss: 0.00001718
Iteration 374/1000 | Loss: 0.00001717
Iteration 375/1000 | Loss: 0.00001717
Iteration 376/1000 | Loss: 0.00001717
Iteration 377/1000 | Loss: 0.00001716
Iteration 378/1000 | Loss: 0.00001716
Iteration 379/1000 | Loss: 0.00001716
Iteration 380/1000 | Loss: 0.00001716
Iteration 381/1000 | Loss: 0.00001716
Iteration 382/1000 | Loss: 0.00001716
Iteration 383/1000 | Loss: 0.00001716
Iteration 384/1000 | Loss: 0.00001716
Iteration 385/1000 | Loss: 0.00001716
Iteration 386/1000 | Loss: 0.00001716
Iteration 387/1000 | Loss: 0.00001716
Iteration 388/1000 | Loss: 0.00001715
Iteration 389/1000 | Loss: 0.00001715
Iteration 390/1000 | Loss: 0.00001715
Iteration 391/1000 | Loss: 0.00001715
Iteration 392/1000 | Loss: 0.00001715
Iteration 393/1000 | Loss: 0.00001715
Iteration 394/1000 | Loss: 0.00001715
Iteration 395/1000 | Loss: 0.00001715
Iteration 396/1000 | Loss: 0.00001715
Iteration 397/1000 | Loss: 0.00001715
Iteration 398/1000 | Loss: 0.00001715
Iteration 399/1000 | Loss: 0.00001715
Iteration 400/1000 | Loss: 0.00001715
Iteration 401/1000 | Loss: 0.00001715
Iteration 402/1000 | Loss: 0.00001715
Iteration 403/1000 | Loss: 0.00001715
Iteration 404/1000 | Loss: 0.00001715
Iteration 405/1000 | Loss: 0.00001715
Iteration 406/1000 | Loss: 0.00001714
Iteration 407/1000 | Loss: 0.00001714
Iteration 408/1000 | Loss: 0.00001714
Iteration 409/1000 | Loss: 0.00001714
Iteration 410/1000 | Loss: 0.00001714
Iteration 411/1000 | Loss: 0.00001714
Iteration 412/1000 | Loss: 0.00001714
Iteration 413/1000 | Loss: 0.00001713
Iteration 414/1000 | Loss: 0.00001713
Iteration 415/1000 | Loss: 0.00001713
Iteration 416/1000 | Loss: 0.00001713
Iteration 417/1000 | Loss: 0.00001713
Iteration 418/1000 | Loss: 0.00001712
Iteration 419/1000 | Loss: 0.00001712
Iteration 420/1000 | Loss: 0.00001712
Iteration 421/1000 | Loss: 0.00001712
Iteration 422/1000 | Loss: 0.00001712
Iteration 423/1000 | Loss: 0.00001712
Iteration 424/1000 | Loss: 0.00001712
Iteration 425/1000 | Loss: 0.00001712
Iteration 426/1000 | Loss: 0.00001711
Iteration 427/1000 | Loss: 0.00001711
Iteration 428/1000 | Loss: 0.00001711
Iteration 429/1000 | Loss: 0.00001711
Iteration 430/1000 | Loss: 0.00001711
Iteration 431/1000 | Loss: 0.00001711
Iteration 432/1000 | Loss: 0.00001710
Iteration 433/1000 | Loss: 0.00001710
Iteration 434/1000 | Loss: 0.00001710
Iteration 435/1000 | Loss: 0.00001710
Iteration 436/1000 | Loss: 0.00001710
Iteration 437/1000 | Loss: 0.00001710
Iteration 438/1000 | Loss: 0.00001710
Iteration 439/1000 | Loss: 0.00001710
Iteration 440/1000 | Loss: 0.00001709
Iteration 441/1000 | Loss: 0.00001709
Iteration 442/1000 | Loss: 0.00001709
Iteration 443/1000 | Loss: 0.00001709
Iteration 444/1000 | Loss: 0.00001709
Iteration 445/1000 | Loss: 0.00001709
Iteration 446/1000 | Loss: 0.00001709
Iteration 447/1000 | Loss: 0.00001709
Iteration 448/1000 | Loss: 0.00001709
Iteration 449/1000 | Loss: 0.00001709
Iteration 450/1000 | Loss: 0.00001709
Iteration 451/1000 | Loss: 0.00001709
Iteration 452/1000 | Loss: 0.00001709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 452. Stopping optimization.
Last 5 losses: [1.7094984286813997e-05, 1.7094984286813997e-05, 1.7094984286813997e-05, 1.7094984286813997e-05, 1.7094984286813997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7094984286813997e-05

Optimization complete. Final v2v error: 3.4807748794555664 mm

Highest mean error: 4.205934524536133 mm for frame 77

Lowest mean error: 2.846902370452881 mm for frame 2

Saving results

Total time: 147.05625820159912
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797351
Iteration 2/25 | Loss: 0.00137717
Iteration 3/25 | Loss: 0.00126174
Iteration 4/25 | Loss: 0.00125932
Iteration 5/25 | Loss: 0.00123581
Iteration 6/25 | Loss: 0.00123252
Iteration 7/25 | Loss: 0.00123024
Iteration 8/25 | Loss: 0.00122838
Iteration 9/25 | Loss: 0.00123017
Iteration 10/25 | Loss: 0.00122636
Iteration 11/25 | Loss: 0.00122511
Iteration 12/25 | Loss: 0.00122471
Iteration 13/25 | Loss: 0.00122465
Iteration 14/25 | Loss: 0.00122464
Iteration 15/25 | Loss: 0.00122464
Iteration 16/25 | Loss: 0.00122464
Iteration 17/25 | Loss: 0.00122464
Iteration 18/25 | Loss: 0.00122463
Iteration 19/25 | Loss: 0.00122463
Iteration 20/25 | Loss: 0.00122463
Iteration 21/25 | Loss: 0.00122463
Iteration 22/25 | Loss: 0.00122463
Iteration 23/25 | Loss: 0.00122463
Iteration 24/25 | Loss: 0.00122462
Iteration 25/25 | Loss: 0.00122462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44846988
Iteration 2/25 | Loss: 0.00086545
Iteration 3/25 | Loss: 0.00086536
Iteration 4/25 | Loss: 0.00086535
Iteration 5/25 | Loss: 0.00086535
Iteration 6/25 | Loss: 0.00086535
Iteration 7/25 | Loss: 0.00086535
Iteration 8/25 | Loss: 0.00086535
Iteration 9/25 | Loss: 0.00086535
Iteration 10/25 | Loss: 0.00086535
Iteration 11/25 | Loss: 0.00086535
Iteration 12/25 | Loss: 0.00086535
Iteration 13/25 | Loss: 0.00086535
Iteration 14/25 | Loss: 0.00086535
Iteration 15/25 | Loss: 0.00086535
Iteration 16/25 | Loss: 0.00086535
Iteration 17/25 | Loss: 0.00086535
Iteration 18/25 | Loss: 0.00086535
Iteration 19/25 | Loss: 0.00086535
Iteration 20/25 | Loss: 0.00086535
Iteration 21/25 | Loss: 0.00086535
Iteration 22/25 | Loss: 0.00086535
Iteration 23/25 | Loss: 0.00086535
Iteration 24/25 | Loss: 0.00086535
Iteration 25/25 | Loss: 0.00086535

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086535
Iteration 2/1000 | Loss: 0.00003365
Iteration 3/1000 | Loss: 0.00002180
Iteration 4/1000 | Loss: 0.00001888
Iteration 5/1000 | Loss: 0.00001780
Iteration 6/1000 | Loss: 0.00001682
Iteration 7/1000 | Loss: 0.00001628
Iteration 8/1000 | Loss: 0.00001591
Iteration 9/1000 | Loss: 0.00001582
Iteration 10/1000 | Loss: 0.00001560
Iteration 11/1000 | Loss: 0.00001538
Iteration 12/1000 | Loss: 0.00001519
Iteration 13/1000 | Loss: 0.00001518
Iteration 14/1000 | Loss: 0.00001518
Iteration 15/1000 | Loss: 0.00001515
Iteration 16/1000 | Loss: 0.00001514
Iteration 17/1000 | Loss: 0.00001513
Iteration 18/1000 | Loss: 0.00001512
Iteration 19/1000 | Loss: 0.00001512
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001510
Iteration 22/1000 | Loss: 0.00001510
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001510
Iteration 25/1000 | Loss: 0.00001509
Iteration 26/1000 | Loss: 0.00001509
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001508
Iteration 29/1000 | Loss: 0.00001507
Iteration 30/1000 | Loss: 0.00001507
Iteration 31/1000 | Loss: 0.00001506
Iteration 32/1000 | Loss: 0.00001506
Iteration 33/1000 | Loss: 0.00001505
Iteration 34/1000 | Loss: 0.00001505
Iteration 35/1000 | Loss: 0.00001505
Iteration 36/1000 | Loss: 0.00001505
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001503
Iteration 40/1000 | Loss: 0.00001502
Iteration 41/1000 | Loss: 0.00001501
Iteration 42/1000 | Loss: 0.00001501
Iteration 43/1000 | Loss: 0.00001500
Iteration 44/1000 | Loss: 0.00001500
Iteration 45/1000 | Loss: 0.00001500
Iteration 46/1000 | Loss: 0.00001499
Iteration 47/1000 | Loss: 0.00001499
Iteration 48/1000 | Loss: 0.00001499
Iteration 49/1000 | Loss: 0.00001496
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001495
Iteration 53/1000 | Loss: 0.00001495
Iteration 54/1000 | Loss: 0.00001494
Iteration 55/1000 | Loss: 0.00001494
Iteration 56/1000 | Loss: 0.00001493
Iteration 57/1000 | Loss: 0.00001493
Iteration 58/1000 | Loss: 0.00001493
Iteration 59/1000 | Loss: 0.00001492
Iteration 60/1000 | Loss: 0.00001492
Iteration 61/1000 | Loss: 0.00001492
Iteration 62/1000 | Loss: 0.00001492
Iteration 63/1000 | Loss: 0.00001492
Iteration 64/1000 | Loss: 0.00001492
Iteration 65/1000 | Loss: 0.00001492
Iteration 66/1000 | Loss: 0.00001492
Iteration 67/1000 | Loss: 0.00001492
Iteration 68/1000 | Loss: 0.00001491
Iteration 69/1000 | Loss: 0.00001491
Iteration 70/1000 | Loss: 0.00001491
Iteration 71/1000 | Loss: 0.00001491
Iteration 72/1000 | Loss: 0.00001491
Iteration 73/1000 | Loss: 0.00001491
Iteration 74/1000 | Loss: 0.00001491
Iteration 75/1000 | Loss: 0.00001491
Iteration 76/1000 | Loss: 0.00001491
Iteration 77/1000 | Loss: 0.00001491
Iteration 78/1000 | Loss: 0.00001490
Iteration 79/1000 | Loss: 0.00001490
Iteration 80/1000 | Loss: 0.00001490
Iteration 81/1000 | Loss: 0.00001490
Iteration 82/1000 | Loss: 0.00001490
Iteration 83/1000 | Loss: 0.00001490
Iteration 84/1000 | Loss: 0.00001489
Iteration 85/1000 | Loss: 0.00001489
Iteration 86/1000 | Loss: 0.00001489
Iteration 87/1000 | Loss: 0.00001489
Iteration 88/1000 | Loss: 0.00001489
Iteration 89/1000 | Loss: 0.00001488
Iteration 90/1000 | Loss: 0.00001488
Iteration 91/1000 | Loss: 0.00001488
Iteration 92/1000 | Loss: 0.00001488
Iteration 93/1000 | Loss: 0.00001488
Iteration 94/1000 | Loss: 0.00001488
Iteration 95/1000 | Loss: 0.00001488
Iteration 96/1000 | Loss: 0.00001488
Iteration 97/1000 | Loss: 0.00001488
Iteration 98/1000 | Loss: 0.00001488
Iteration 99/1000 | Loss: 0.00001488
Iteration 100/1000 | Loss: 0.00001487
Iteration 101/1000 | Loss: 0.00001487
Iteration 102/1000 | Loss: 0.00001487
Iteration 103/1000 | Loss: 0.00001487
Iteration 104/1000 | Loss: 0.00001487
Iteration 105/1000 | Loss: 0.00001487
Iteration 106/1000 | Loss: 0.00001487
Iteration 107/1000 | Loss: 0.00001487
Iteration 108/1000 | Loss: 0.00001487
Iteration 109/1000 | Loss: 0.00001487
Iteration 110/1000 | Loss: 0.00001487
Iteration 111/1000 | Loss: 0.00001487
Iteration 112/1000 | Loss: 0.00001486
Iteration 113/1000 | Loss: 0.00001486
Iteration 114/1000 | Loss: 0.00001486
Iteration 115/1000 | Loss: 0.00001486
Iteration 116/1000 | Loss: 0.00001486
Iteration 117/1000 | Loss: 0.00001486
Iteration 118/1000 | Loss: 0.00001485
Iteration 119/1000 | Loss: 0.00001485
Iteration 120/1000 | Loss: 0.00001485
Iteration 121/1000 | Loss: 0.00001485
Iteration 122/1000 | Loss: 0.00001485
Iteration 123/1000 | Loss: 0.00001484
Iteration 124/1000 | Loss: 0.00001484
Iteration 125/1000 | Loss: 0.00001484
Iteration 126/1000 | Loss: 0.00001484
Iteration 127/1000 | Loss: 0.00001484
Iteration 128/1000 | Loss: 0.00001484
Iteration 129/1000 | Loss: 0.00001484
Iteration 130/1000 | Loss: 0.00001484
Iteration 131/1000 | Loss: 0.00001483
Iteration 132/1000 | Loss: 0.00001483
Iteration 133/1000 | Loss: 0.00001483
Iteration 134/1000 | Loss: 0.00001482
Iteration 135/1000 | Loss: 0.00001482
Iteration 136/1000 | Loss: 0.00001482
Iteration 137/1000 | Loss: 0.00001481
Iteration 138/1000 | Loss: 0.00001481
Iteration 139/1000 | Loss: 0.00001481
Iteration 140/1000 | Loss: 0.00001480
Iteration 141/1000 | Loss: 0.00001480
Iteration 142/1000 | Loss: 0.00001480
Iteration 143/1000 | Loss: 0.00001480
Iteration 144/1000 | Loss: 0.00001480
Iteration 145/1000 | Loss: 0.00001480
Iteration 146/1000 | Loss: 0.00001480
Iteration 147/1000 | Loss: 0.00001480
Iteration 148/1000 | Loss: 0.00001480
Iteration 149/1000 | Loss: 0.00001480
Iteration 150/1000 | Loss: 0.00001479
Iteration 151/1000 | Loss: 0.00001479
Iteration 152/1000 | Loss: 0.00001479
Iteration 153/1000 | Loss: 0.00001478
Iteration 154/1000 | Loss: 0.00001478
Iteration 155/1000 | Loss: 0.00001478
Iteration 156/1000 | Loss: 0.00001478
Iteration 157/1000 | Loss: 0.00001478
Iteration 158/1000 | Loss: 0.00001478
Iteration 159/1000 | Loss: 0.00001478
Iteration 160/1000 | Loss: 0.00001478
Iteration 161/1000 | Loss: 0.00001477
Iteration 162/1000 | Loss: 0.00001477
Iteration 163/1000 | Loss: 0.00001477
Iteration 164/1000 | Loss: 0.00001477
Iteration 165/1000 | Loss: 0.00001477
Iteration 166/1000 | Loss: 0.00001477
Iteration 167/1000 | Loss: 0.00001477
Iteration 168/1000 | Loss: 0.00001477
Iteration 169/1000 | Loss: 0.00001477
Iteration 170/1000 | Loss: 0.00001477
Iteration 171/1000 | Loss: 0.00001477
Iteration 172/1000 | Loss: 0.00001477
Iteration 173/1000 | Loss: 0.00001477
Iteration 174/1000 | Loss: 0.00001477
Iteration 175/1000 | Loss: 0.00001477
Iteration 176/1000 | Loss: 0.00001477
Iteration 177/1000 | Loss: 0.00001477
Iteration 178/1000 | Loss: 0.00001477
Iteration 179/1000 | Loss: 0.00001477
Iteration 180/1000 | Loss: 0.00001477
Iteration 181/1000 | Loss: 0.00001477
Iteration 182/1000 | Loss: 0.00001477
Iteration 183/1000 | Loss: 0.00001477
Iteration 184/1000 | Loss: 0.00001477
Iteration 185/1000 | Loss: 0.00001476
Iteration 186/1000 | Loss: 0.00001476
Iteration 187/1000 | Loss: 0.00001476
Iteration 188/1000 | Loss: 0.00001476
Iteration 189/1000 | Loss: 0.00001476
Iteration 190/1000 | Loss: 0.00001476
Iteration 191/1000 | Loss: 0.00001476
Iteration 192/1000 | Loss: 0.00001476
Iteration 193/1000 | Loss: 0.00001476
Iteration 194/1000 | Loss: 0.00001476
Iteration 195/1000 | Loss: 0.00001476
Iteration 196/1000 | Loss: 0.00001476
Iteration 197/1000 | Loss: 0.00001476
Iteration 198/1000 | Loss: 0.00001476
Iteration 199/1000 | Loss: 0.00001476
Iteration 200/1000 | Loss: 0.00001476
Iteration 201/1000 | Loss: 0.00001476
Iteration 202/1000 | Loss: 0.00001476
Iteration 203/1000 | Loss: 0.00001476
Iteration 204/1000 | Loss: 0.00001476
Iteration 205/1000 | Loss: 0.00001476
Iteration 206/1000 | Loss: 0.00001476
Iteration 207/1000 | Loss: 0.00001476
Iteration 208/1000 | Loss: 0.00001476
Iteration 209/1000 | Loss: 0.00001476
Iteration 210/1000 | Loss: 0.00001476
Iteration 211/1000 | Loss: 0.00001476
Iteration 212/1000 | Loss: 0.00001476
Iteration 213/1000 | Loss: 0.00001476
Iteration 214/1000 | Loss: 0.00001476
Iteration 215/1000 | Loss: 0.00001476
Iteration 216/1000 | Loss: 0.00001476
Iteration 217/1000 | Loss: 0.00001476
Iteration 218/1000 | Loss: 0.00001476
Iteration 219/1000 | Loss: 0.00001476
Iteration 220/1000 | Loss: 0.00001476
Iteration 221/1000 | Loss: 0.00001476
Iteration 222/1000 | Loss: 0.00001476
Iteration 223/1000 | Loss: 0.00001476
Iteration 224/1000 | Loss: 0.00001476
Iteration 225/1000 | Loss: 0.00001476
Iteration 226/1000 | Loss: 0.00001476
Iteration 227/1000 | Loss: 0.00001476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.4761538295715582e-05, 1.4761538295715582e-05, 1.4761538295715582e-05, 1.4761538295715582e-05, 1.4761538295715582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4761538295715582e-05

Optimization complete. Final v2v error: 3.202571153640747 mm

Highest mean error: 4.496392726898193 mm for frame 224

Lowest mean error: 2.7395405769348145 mm for frame 119

Saving results

Total time: 59.63896203041077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010340
Iteration 2/25 | Loss: 0.01010340
Iteration 3/25 | Loss: 0.01010340
Iteration 4/25 | Loss: 0.00185996
Iteration 5/25 | Loss: 0.00158121
Iteration 6/25 | Loss: 0.00151085
Iteration 7/25 | Loss: 0.00152909
Iteration 8/25 | Loss: 0.00147179
Iteration 9/25 | Loss: 0.00143404
Iteration 10/25 | Loss: 0.00140192
Iteration 11/25 | Loss: 0.00139530
Iteration 12/25 | Loss: 0.00138576
Iteration 13/25 | Loss: 0.00137935
Iteration 14/25 | Loss: 0.00138273
Iteration 15/25 | Loss: 0.00137925
Iteration 16/25 | Loss: 0.00138180
Iteration 17/25 | Loss: 0.00138551
Iteration 18/25 | Loss: 0.00137966
Iteration 19/25 | Loss: 0.00138455
Iteration 20/25 | Loss: 0.00137466
Iteration 21/25 | Loss: 0.00137163
Iteration 22/25 | Loss: 0.00136650
Iteration 23/25 | Loss: 0.00136537
Iteration 24/25 | Loss: 0.00136472
Iteration 25/25 | Loss: 0.00136445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44119680
Iteration 2/25 | Loss: 0.00259925
Iteration 3/25 | Loss: 0.00198037
Iteration 4/25 | Loss: 0.00198037
Iteration 5/25 | Loss: 0.00198037
Iteration 6/25 | Loss: 0.00198037
Iteration 7/25 | Loss: 0.00198037
Iteration 8/25 | Loss: 0.00198037
Iteration 9/25 | Loss: 0.00198037
Iteration 10/25 | Loss: 0.00198037
Iteration 11/25 | Loss: 0.00198037
Iteration 12/25 | Loss: 0.00198037
Iteration 13/25 | Loss: 0.00198037
Iteration 14/25 | Loss: 0.00198037
Iteration 15/25 | Loss: 0.00198037
Iteration 16/25 | Loss: 0.00198037
Iteration 17/25 | Loss: 0.00198037
Iteration 18/25 | Loss: 0.00198037
Iteration 19/25 | Loss: 0.00198037
Iteration 20/25 | Loss: 0.00198037
Iteration 21/25 | Loss: 0.00198037
Iteration 22/25 | Loss: 0.00198037
Iteration 23/25 | Loss: 0.00198037
Iteration 24/25 | Loss: 0.00198037
Iteration 25/25 | Loss: 0.00198037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198037
Iteration 2/1000 | Loss: 0.00043970
Iteration 3/1000 | Loss: 0.00035767
Iteration 4/1000 | Loss: 0.00052491
Iteration 5/1000 | Loss: 0.00044644
Iteration 6/1000 | Loss: 0.00039854
Iteration 7/1000 | Loss: 0.00041145
Iteration 8/1000 | Loss: 0.00033048
Iteration 9/1000 | Loss: 0.00056209
Iteration 10/1000 | Loss: 0.00143393
Iteration 11/1000 | Loss: 0.00136758
Iteration 12/1000 | Loss: 0.00047396
Iteration 13/1000 | Loss: 0.00022560
Iteration 14/1000 | Loss: 0.00070486
Iteration 15/1000 | Loss: 0.00017669
Iteration 16/1000 | Loss: 0.00039515
Iteration 17/1000 | Loss: 0.00067181
Iteration 18/1000 | Loss: 0.00036094
Iteration 19/1000 | Loss: 0.00034076
Iteration 20/1000 | Loss: 0.00012167
Iteration 21/1000 | Loss: 0.00020557
Iteration 22/1000 | Loss: 0.00011345
Iteration 23/1000 | Loss: 0.00020117
Iteration 24/1000 | Loss: 0.00009341
Iteration 25/1000 | Loss: 0.00092297
Iteration 26/1000 | Loss: 0.00010191
Iteration 27/1000 | Loss: 0.00043618
Iteration 28/1000 | Loss: 0.00043441
Iteration 29/1000 | Loss: 0.00065346
Iteration 30/1000 | Loss: 0.00080682
Iteration 31/1000 | Loss: 0.00032955
Iteration 32/1000 | Loss: 0.00043302
Iteration 33/1000 | Loss: 0.00011721
Iteration 34/1000 | Loss: 0.00009882
Iteration 35/1000 | Loss: 0.00019670
Iteration 36/1000 | Loss: 0.00018035
Iteration 37/1000 | Loss: 0.00022516
Iteration 38/1000 | Loss: 0.00022061
Iteration 39/1000 | Loss: 0.00089418
Iteration 40/1000 | Loss: 0.00044508
Iteration 41/1000 | Loss: 0.00047823
Iteration 42/1000 | Loss: 0.00031304
Iteration 43/1000 | Loss: 0.00013245
Iteration 44/1000 | Loss: 0.00023056
Iteration 45/1000 | Loss: 0.00008391
Iteration 46/1000 | Loss: 0.00054741
Iteration 47/1000 | Loss: 0.00027408
Iteration 48/1000 | Loss: 0.00009036
Iteration 49/1000 | Loss: 0.00009244
Iteration 50/1000 | Loss: 0.00007200
Iteration 51/1000 | Loss: 0.00009332
Iteration 52/1000 | Loss: 0.00011891
Iteration 53/1000 | Loss: 0.00006181
Iteration 54/1000 | Loss: 0.00022252
Iteration 55/1000 | Loss: 0.00006696
Iteration 56/1000 | Loss: 0.00008220
Iteration 57/1000 | Loss: 0.00006100
Iteration 58/1000 | Loss: 0.00009563
Iteration 59/1000 | Loss: 0.00005640
Iteration 60/1000 | Loss: 0.00016818
Iteration 61/1000 | Loss: 0.00021611
Iteration 62/1000 | Loss: 0.00027731
Iteration 63/1000 | Loss: 0.00014586
Iteration 64/1000 | Loss: 0.00023391
Iteration 65/1000 | Loss: 0.00005910
Iteration 66/1000 | Loss: 0.00038945
Iteration 67/1000 | Loss: 0.00005384
Iteration 68/1000 | Loss: 0.00042252
Iteration 69/1000 | Loss: 0.00037626
Iteration 70/1000 | Loss: 0.00054047
Iteration 71/1000 | Loss: 0.00086801
Iteration 72/1000 | Loss: 0.00030956
Iteration 73/1000 | Loss: 0.00023823
Iteration 74/1000 | Loss: 0.00042736
Iteration 75/1000 | Loss: 0.00014977
Iteration 76/1000 | Loss: 0.00020628
Iteration 77/1000 | Loss: 0.00012666
Iteration 78/1000 | Loss: 0.00016653
Iteration 79/1000 | Loss: 0.00007165
Iteration 80/1000 | Loss: 0.00011397
Iteration 81/1000 | Loss: 0.00004928
Iteration 82/1000 | Loss: 0.00013575
Iteration 83/1000 | Loss: 0.00044591
Iteration 84/1000 | Loss: 0.00038694
Iteration 85/1000 | Loss: 0.00042206
Iteration 86/1000 | Loss: 0.00004533
Iteration 87/1000 | Loss: 0.00003986
Iteration 88/1000 | Loss: 0.00032828
Iteration 89/1000 | Loss: 0.00003969
Iteration 90/1000 | Loss: 0.00003622
Iteration 91/1000 | Loss: 0.00003475
Iteration 92/1000 | Loss: 0.00021916
Iteration 93/1000 | Loss: 0.00033115
Iteration 94/1000 | Loss: 0.00015729
Iteration 95/1000 | Loss: 0.00007734
Iteration 96/1000 | Loss: 0.00003575
Iteration 97/1000 | Loss: 0.00003891
Iteration 98/1000 | Loss: 0.00032865
Iteration 99/1000 | Loss: 0.00016217
Iteration 100/1000 | Loss: 0.00005394
Iteration 101/1000 | Loss: 0.00003656
Iteration 102/1000 | Loss: 0.00041929
Iteration 103/1000 | Loss: 0.00007255
Iteration 104/1000 | Loss: 0.00004623
Iteration 105/1000 | Loss: 0.00002693
Iteration 106/1000 | Loss: 0.00004939
Iteration 107/1000 | Loss: 0.00010008
Iteration 108/1000 | Loss: 0.00032138
Iteration 109/1000 | Loss: 0.00002979
Iteration 110/1000 | Loss: 0.00002912
Iteration 111/1000 | Loss: 0.00002506
Iteration 112/1000 | Loss: 0.00003477
Iteration 113/1000 | Loss: 0.00006069
Iteration 114/1000 | Loss: 0.00030891
Iteration 115/1000 | Loss: 0.00002351
Iteration 116/1000 | Loss: 0.00003499
Iteration 117/1000 | Loss: 0.00002448
Iteration 118/1000 | Loss: 0.00002065
Iteration 119/1000 | Loss: 0.00002156
Iteration 120/1000 | Loss: 0.00003579
Iteration 121/1000 | Loss: 0.00002244
Iteration 122/1000 | Loss: 0.00002083
Iteration 123/1000 | Loss: 0.00002614
Iteration 124/1000 | Loss: 0.00001927
Iteration 125/1000 | Loss: 0.00002603
Iteration 126/1000 | Loss: 0.00001906
Iteration 127/1000 | Loss: 0.00001903
Iteration 128/1000 | Loss: 0.00001903
Iteration 129/1000 | Loss: 0.00001903
Iteration 130/1000 | Loss: 0.00001902
Iteration 131/1000 | Loss: 0.00001902
Iteration 132/1000 | Loss: 0.00002444
Iteration 133/1000 | Loss: 0.00001896
Iteration 134/1000 | Loss: 0.00001895
Iteration 135/1000 | Loss: 0.00001895
Iteration 136/1000 | Loss: 0.00001987
Iteration 137/1000 | Loss: 0.00002766
Iteration 138/1000 | Loss: 0.00001891
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001889
Iteration 141/1000 | Loss: 0.00001889
Iteration 142/1000 | Loss: 0.00001888
Iteration 143/1000 | Loss: 0.00001888
Iteration 144/1000 | Loss: 0.00001888
Iteration 145/1000 | Loss: 0.00001887
Iteration 146/1000 | Loss: 0.00001886
Iteration 147/1000 | Loss: 0.00001886
Iteration 148/1000 | Loss: 0.00001886
Iteration 149/1000 | Loss: 0.00001886
Iteration 150/1000 | Loss: 0.00002052
Iteration 151/1000 | Loss: 0.00002052
Iteration 152/1000 | Loss: 0.00001884
Iteration 153/1000 | Loss: 0.00001883
Iteration 154/1000 | Loss: 0.00001883
Iteration 155/1000 | Loss: 0.00001883
Iteration 156/1000 | Loss: 0.00001883
Iteration 157/1000 | Loss: 0.00002000
Iteration 158/1000 | Loss: 0.00001885
Iteration 159/1000 | Loss: 0.00001882
Iteration 160/1000 | Loss: 0.00001882
Iteration 161/1000 | Loss: 0.00001882
Iteration 162/1000 | Loss: 0.00001882
Iteration 163/1000 | Loss: 0.00001882
Iteration 164/1000 | Loss: 0.00001882
Iteration 165/1000 | Loss: 0.00001882
Iteration 166/1000 | Loss: 0.00001882
Iteration 167/1000 | Loss: 0.00001882
Iteration 168/1000 | Loss: 0.00001882
Iteration 169/1000 | Loss: 0.00001882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.881849493656773e-05, 1.881849493656773e-05, 1.881849493656773e-05, 1.881849493656773e-05, 1.881849493656773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.881849493656773e-05

Optimization complete. Final v2v error: 3.184790849685669 mm

Highest mean error: 11.52011775970459 mm for frame 133

Lowest mean error: 2.7278263568878174 mm for frame 2

Saving results

Total time: 253.62068843841553
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01049749
Iteration 2/25 | Loss: 0.00195723
Iteration 3/25 | Loss: 0.00143068
Iteration 4/25 | Loss: 0.00139219
Iteration 5/25 | Loss: 0.00137815
Iteration 6/25 | Loss: 0.00137293
Iteration 7/25 | Loss: 0.00136914
Iteration 8/25 | Loss: 0.00136797
Iteration 9/25 | Loss: 0.00136762
Iteration 10/25 | Loss: 0.00136731
Iteration 11/25 | Loss: 0.00136729
Iteration 12/25 | Loss: 0.00136729
Iteration 13/25 | Loss: 0.00136729
Iteration 14/25 | Loss: 0.00136728
Iteration 15/25 | Loss: 0.00136728
Iteration 16/25 | Loss: 0.00136728
Iteration 17/25 | Loss: 0.00136728
Iteration 18/25 | Loss: 0.00136728
Iteration 19/25 | Loss: 0.00136728
Iteration 20/25 | Loss: 0.00136728
Iteration 21/25 | Loss: 0.00136728
Iteration 22/25 | Loss: 0.00136728
Iteration 23/25 | Loss: 0.00136728
Iteration 24/25 | Loss: 0.00136728
Iteration 25/25 | Loss: 0.00136727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.46074748
Iteration 2/25 | Loss: 0.00092073
Iteration 3/25 | Loss: 0.00092073
Iteration 4/25 | Loss: 0.00092073
Iteration 5/25 | Loss: 0.00092073
Iteration 6/25 | Loss: 0.00092073
Iteration 7/25 | Loss: 0.00092073
Iteration 8/25 | Loss: 0.00092073
Iteration 9/25 | Loss: 0.00092073
Iteration 10/25 | Loss: 0.00092073
Iteration 11/25 | Loss: 0.00092073
Iteration 12/25 | Loss: 0.00092073
Iteration 13/25 | Loss: 0.00092073
Iteration 14/25 | Loss: 0.00092073
Iteration 15/25 | Loss: 0.00092073
Iteration 16/25 | Loss: 0.00092073
Iteration 17/25 | Loss: 0.00092073
Iteration 18/25 | Loss: 0.00092073
Iteration 19/25 | Loss: 0.00092073
Iteration 20/25 | Loss: 0.00092073
Iteration 21/25 | Loss: 0.00092073
Iteration 22/25 | Loss: 0.00092073
Iteration 23/25 | Loss: 0.00092073
Iteration 24/25 | Loss: 0.00092073
Iteration 25/25 | Loss: 0.00092073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092073
Iteration 2/1000 | Loss: 0.00005611
Iteration 3/1000 | Loss: 0.00004043
Iteration 4/1000 | Loss: 0.00003685
Iteration 5/1000 | Loss: 0.00003492
Iteration 6/1000 | Loss: 0.00003389
Iteration 7/1000 | Loss: 0.00003294
Iteration 8/1000 | Loss: 0.00003244
Iteration 9/1000 | Loss: 0.00003190
Iteration 10/1000 | Loss: 0.00003151
Iteration 11/1000 | Loss: 0.00003123
Iteration 12/1000 | Loss: 0.00003102
Iteration 13/1000 | Loss: 0.00003083
Iteration 14/1000 | Loss: 0.00003081
Iteration 15/1000 | Loss: 0.00003076
Iteration 16/1000 | Loss: 0.00003074
Iteration 17/1000 | Loss: 0.00003071
Iteration 18/1000 | Loss: 0.00003066
Iteration 19/1000 | Loss: 0.00003063
Iteration 20/1000 | Loss: 0.00003063
Iteration 21/1000 | Loss: 0.00003062
Iteration 22/1000 | Loss: 0.00003062
Iteration 23/1000 | Loss: 0.00003061
Iteration 24/1000 | Loss: 0.00003059
Iteration 25/1000 | Loss: 0.00003058
Iteration 26/1000 | Loss: 0.00003058
Iteration 27/1000 | Loss: 0.00003056
Iteration 28/1000 | Loss: 0.00003056
Iteration 29/1000 | Loss: 0.00003055
Iteration 30/1000 | Loss: 0.00003055
Iteration 31/1000 | Loss: 0.00003053
Iteration 32/1000 | Loss: 0.00003053
Iteration 33/1000 | Loss: 0.00003053
Iteration 34/1000 | Loss: 0.00003053
Iteration 35/1000 | Loss: 0.00003053
Iteration 36/1000 | Loss: 0.00003053
Iteration 37/1000 | Loss: 0.00003053
Iteration 38/1000 | Loss: 0.00003052
Iteration 39/1000 | Loss: 0.00003052
Iteration 40/1000 | Loss: 0.00003052
Iteration 41/1000 | Loss: 0.00003052
Iteration 42/1000 | Loss: 0.00003051
Iteration 43/1000 | Loss: 0.00003051
Iteration 44/1000 | Loss: 0.00003050
Iteration 45/1000 | Loss: 0.00003050
Iteration 46/1000 | Loss: 0.00003049
Iteration 47/1000 | Loss: 0.00003049
Iteration 48/1000 | Loss: 0.00003049
Iteration 49/1000 | Loss: 0.00003049
Iteration 50/1000 | Loss: 0.00003049
Iteration 51/1000 | Loss: 0.00003049
Iteration 52/1000 | Loss: 0.00003049
Iteration 53/1000 | Loss: 0.00003049
Iteration 54/1000 | Loss: 0.00003049
Iteration 55/1000 | Loss: 0.00003049
Iteration 56/1000 | Loss: 0.00003049
Iteration 57/1000 | Loss: 0.00003048
Iteration 58/1000 | Loss: 0.00003048
Iteration 59/1000 | Loss: 0.00003048
Iteration 60/1000 | Loss: 0.00003047
Iteration 61/1000 | Loss: 0.00003047
Iteration 62/1000 | Loss: 0.00003047
Iteration 63/1000 | Loss: 0.00003047
Iteration 64/1000 | Loss: 0.00003046
Iteration 65/1000 | Loss: 0.00003046
Iteration 66/1000 | Loss: 0.00003046
Iteration 67/1000 | Loss: 0.00003046
Iteration 68/1000 | Loss: 0.00003046
Iteration 69/1000 | Loss: 0.00003045
Iteration 70/1000 | Loss: 0.00003045
Iteration 71/1000 | Loss: 0.00003045
Iteration 72/1000 | Loss: 0.00003045
Iteration 73/1000 | Loss: 0.00003045
Iteration 74/1000 | Loss: 0.00003045
Iteration 75/1000 | Loss: 0.00003045
Iteration 76/1000 | Loss: 0.00003045
Iteration 77/1000 | Loss: 0.00003045
Iteration 78/1000 | Loss: 0.00003045
Iteration 79/1000 | Loss: 0.00003045
Iteration 80/1000 | Loss: 0.00003044
Iteration 81/1000 | Loss: 0.00003044
Iteration 82/1000 | Loss: 0.00003044
Iteration 83/1000 | Loss: 0.00003044
Iteration 84/1000 | Loss: 0.00003044
Iteration 85/1000 | Loss: 0.00003044
Iteration 86/1000 | Loss: 0.00003044
Iteration 87/1000 | Loss: 0.00003044
Iteration 88/1000 | Loss: 0.00003044
Iteration 89/1000 | Loss: 0.00003044
Iteration 90/1000 | Loss: 0.00003044
Iteration 91/1000 | Loss: 0.00003044
Iteration 92/1000 | Loss: 0.00003044
Iteration 93/1000 | Loss: 0.00003044
Iteration 94/1000 | Loss: 0.00003044
Iteration 95/1000 | Loss: 0.00003044
Iteration 96/1000 | Loss: 0.00003044
Iteration 97/1000 | Loss: 0.00003044
Iteration 98/1000 | Loss: 0.00003044
Iteration 99/1000 | Loss: 0.00003044
Iteration 100/1000 | Loss: 0.00003044
Iteration 101/1000 | Loss: 0.00003044
Iteration 102/1000 | Loss: 0.00003044
Iteration 103/1000 | Loss: 0.00003044
Iteration 104/1000 | Loss: 0.00003044
Iteration 105/1000 | Loss: 0.00003044
Iteration 106/1000 | Loss: 0.00003044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [3.0435774533543736e-05, 3.0435774533543736e-05, 3.0435774533543736e-05, 3.0435774533543736e-05, 3.0435774533543736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0435774533543736e-05

Optimization complete. Final v2v error: 4.621025085449219 mm

Highest mean error: 4.974860668182373 mm for frame 106

Lowest mean error: 3.702507972717285 mm for frame 4

Saving results

Total time: 51.107794523239136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537737
Iteration 2/25 | Loss: 0.00143177
Iteration 3/25 | Loss: 0.00130530
Iteration 4/25 | Loss: 0.00128727
Iteration 5/25 | Loss: 0.00127904
Iteration 6/25 | Loss: 0.00127775
Iteration 7/25 | Loss: 0.00127775
Iteration 8/25 | Loss: 0.00127775
Iteration 9/25 | Loss: 0.00127775
Iteration 10/25 | Loss: 0.00127775
Iteration 11/25 | Loss: 0.00127775
Iteration 12/25 | Loss: 0.00127775
Iteration 13/25 | Loss: 0.00127775
Iteration 14/25 | Loss: 0.00127775
Iteration 15/25 | Loss: 0.00127775
Iteration 16/25 | Loss: 0.00127775
Iteration 17/25 | Loss: 0.00127775
Iteration 18/25 | Loss: 0.00127775
Iteration 19/25 | Loss: 0.00127775
Iteration 20/25 | Loss: 0.00127775
Iteration 21/25 | Loss: 0.00127775
Iteration 22/25 | Loss: 0.00127775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0012777493102476, 0.0012777493102476, 0.0012777493102476, 0.0012777493102476, 0.0012777493102476]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012777493102476

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80018222
Iteration 2/25 | Loss: 0.00067152
Iteration 3/25 | Loss: 0.00067152
Iteration 4/25 | Loss: 0.00067152
Iteration 5/25 | Loss: 0.00067152
Iteration 6/25 | Loss: 0.00067152
Iteration 7/25 | Loss: 0.00067152
Iteration 8/25 | Loss: 0.00067152
Iteration 9/25 | Loss: 0.00067152
Iteration 10/25 | Loss: 0.00067152
Iteration 11/25 | Loss: 0.00067152
Iteration 12/25 | Loss: 0.00067152
Iteration 13/25 | Loss: 0.00067152
Iteration 14/25 | Loss: 0.00067152
Iteration 15/25 | Loss: 0.00067152
Iteration 16/25 | Loss: 0.00067152
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006715151248499751, 0.0006715151248499751, 0.0006715151248499751, 0.0006715151248499751, 0.0006715151248499751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006715151248499751

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067152
Iteration 2/1000 | Loss: 0.00003998
Iteration 3/1000 | Loss: 0.00003024
Iteration 4/1000 | Loss: 0.00002812
Iteration 5/1000 | Loss: 0.00002716
Iteration 6/1000 | Loss: 0.00002641
Iteration 7/1000 | Loss: 0.00002586
Iteration 8/1000 | Loss: 0.00002538
Iteration 9/1000 | Loss: 0.00002492
Iteration 10/1000 | Loss: 0.00002452
Iteration 11/1000 | Loss: 0.00002414
Iteration 12/1000 | Loss: 0.00002373
Iteration 13/1000 | Loss: 0.00002346
Iteration 14/1000 | Loss: 0.00002321
Iteration 15/1000 | Loss: 0.00002316
Iteration 16/1000 | Loss: 0.00002297
Iteration 17/1000 | Loss: 0.00002281
Iteration 18/1000 | Loss: 0.00002277
Iteration 19/1000 | Loss: 0.00002273
Iteration 20/1000 | Loss: 0.00002273
Iteration 21/1000 | Loss: 0.00002262
Iteration 22/1000 | Loss: 0.00002259
Iteration 23/1000 | Loss: 0.00002259
Iteration 24/1000 | Loss: 0.00002258
Iteration 25/1000 | Loss: 0.00002255
Iteration 26/1000 | Loss: 0.00002255
Iteration 27/1000 | Loss: 0.00002255
Iteration 28/1000 | Loss: 0.00002255
Iteration 29/1000 | Loss: 0.00002255
Iteration 30/1000 | Loss: 0.00002255
Iteration 31/1000 | Loss: 0.00002255
Iteration 32/1000 | Loss: 0.00002255
Iteration 33/1000 | Loss: 0.00002254
Iteration 34/1000 | Loss: 0.00002254
Iteration 35/1000 | Loss: 0.00002251
Iteration 36/1000 | Loss: 0.00002251
Iteration 37/1000 | Loss: 0.00002251
Iteration 38/1000 | Loss: 0.00002250
Iteration 39/1000 | Loss: 0.00002248
Iteration 40/1000 | Loss: 0.00002248
Iteration 41/1000 | Loss: 0.00002248
Iteration 42/1000 | Loss: 0.00002247
Iteration 43/1000 | Loss: 0.00002243
Iteration 44/1000 | Loss: 0.00002243
Iteration 45/1000 | Loss: 0.00002243
Iteration 46/1000 | Loss: 0.00002243
Iteration 47/1000 | Loss: 0.00002243
Iteration 48/1000 | Loss: 0.00002243
Iteration 49/1000 | Loss: 0.00002240
Iteration 50/1000 | Loss: 0.00002240
Iteration 51/1000 | Loss: 0.00002240
Iteration 52/1000 | Loss: 0.00002239
Iteration 53/1000 | Loss: 0.00002239
Iteration 54/1000 | Loss: 0.00002239
Iteration 55/1000 | Loss: 0.00002239
Iteration 56/1000 | Loss: 0.00002239
Iteration 57/1000 | Loss: 0.00002239
Iteration 58/1000 | Loss: 0.00002239
Iteration 59/1000 | Loss: 0.00002239
Iteration 60/1000 | Loss: 0.00002239
Iteration 61/1000 | Loss: 0.00002239
Iteration 62/1000 | Loss: 0.00002239
Iteration 63/1000 | Loss: 0.00002239
Iteration 64/1000 | Loss: 0.00002239
Iteration 65/1000 | Loss: 0.00002238
Iteration 66/1000 | Loss: 0.00002238
Iteration 67/1000 | Loss: 0.00002238
Iteration 68/1000 | Loss: 0.00002237
Iteration 69/1000 | Loss: 0.00002236
Iteration 70/1000 | Loss: 0.00002235
Iteration 71/1000 | Loss: 0.00002235
Iteration 72/1000 | Loss: 0.00002235
Iteration 73/1000 | Loss: 0.00002234
Iteration 74/1000 | Loss: 0.00002234
Iteration 75/1000 | Loss: 0.00002233
Iteration 76/1000 | Loss: 0.00002232
Iteration 77/1000 | Loss: 0.00002232
Iteration 78/1000 | Loss: 0.00002232
Iteration 79/1000 | Loss: 0.00002231
Iteration 80/1000 | Loss: 0.00002231
Iteration 81/1000 | Loss: 0.00002231
Iteration 82/1000 | Loss: 0.00002230
Iteration 83/1000 | Loss: 0.00002230
Iteration 84/1000 | Loss: 0.00002229
Iteration 85/1000 | Loss: 0.00002229
Iteration 86/1000 | Loss: 0.00002228
Iteration 87/1000 | Loss: 0.00002227
Iteration 88/1000 | Loss: 0.00002227
Iteration 89/1000 | Loss: 0.00002227
Iteration 90/1000 | Loss: 0.00002227
Iteration 91/1000 | Loss: 0.00002227
Iteration 92/1000 | Loss: 0.00002227
Iteration 93/1000 | Loss: 0.00002226
Iteration 94/1000 | Loss: 0.00002226
Iteration 95/1000 | Loss: 0.00002226
Iteration 96/1000 | Loss: 0.00002226
Iteration 97/1000 | Loss: 0.00002226
Iteration 98/1000 | Loss: 0.00002226
Iteration 99/1000 | Loss: 0.00002226
Iteration 100/1000 | Loss: 0.00002226
Iteration 101/1000 | Loss: 0.00002225
Iteration 102/1000 | Loss: 0.00002225
Iteration 103/1000 | Loss: 0.00002225
Iteration 104/1000 | Loss: 0.00002225
Iteration 105/1000 | Loss: 0.00002225
Iteration 106/1000 | Loss: 0.00002225
Iteration 107/1000 | Loss: 0.00002225
Iteration 108/1000 | Loss: 0.00002225
Iteration 109/1000 | Loss: 0.00002224
Iteration 110/1000 | Loss: 0.00002224
Iteration 111/1000 | Loss: 0.00002224
Iteration 112/1000 | Loss: 0.00002223
Iteration 113/1000 | Loss: 0.00002223
Iteration 114/1000 | Loss: 0.00002223
Iteration 115/1000 | Loss: 0.00002223
Iteration 116/1000 | Loss: 0.00002222
Iteration 117/1000 | Loss: 0.00002222
Iteration 118/1000 | Loss: 0.00002222
Iteration 119/1000 | Loss: 0.00002222
Iteration 120/1000 | Loss: 0.00002222
Iteration 121/1000 | Loss: 0.00002222
Iteration 122/1000 | Loss: 0.00002222
Iteration 123/1000 | Loss: 0.00002222
Iteration 124/1000 | Loss: 0.00002222
Iteration 125/1000 | Loss: 0.00002222
Iteration 126/1000 | Loss: 0.00002222
Iteration 127/1000 | Loss: 0.00002222
Iteration 128/1000 | Loss: 0.00002222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.2224612621357664e-05, 2.2224612621357664e-05, 2.2224612621357664e-05, 2.2224612621357664e-05, 2.2224612621357664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2224612621357664e-05

Optimization complete. Final v2v error: 4.001379489898682 mm

Highest mean error: 4.557010650634766 mm for frame 266

Lowest mean error: 3.920600652694702 mm for frame 2

Saving results

Total time: 50.002044677734375
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533565
Iteration 2/25 | Loss: 0.00143487
Iteration 3/25 | Loss: 0.00134259
Iteration 4/25 | Loss: 0.00133174
Iteration 5/25 | Loss: 0.00132850
Iteration 6/25 | Loss: 0.00132850
Iteration 7/25 | Loss: 0.00132850
Iteration 8/25 | Loss: 0.00132850
Iteration 9/25 | Loss: 0.00132850
Iteration 10/25 | Loss: 0.00132850
Iteration 11/25 | Loss: 0.00132850
Iteration 12/25 | Loss: 0.00132850
Iteration 13/25 | Loss: 0.00132850
Iteration 14/25 | Loss: 0.00132850
Iteration 15/25 | Loss: 0.00132850
Iteration 16/25 | Loss: 0.00132850
Iteration 17/25 | Loss: 0.00132850
Iteration 18/25 | Loss: 0.00132850
Iteration 19/25 | Loss: 0.00132850
Iteration 20/25 | Loss: 0.00132850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013284961460158229, 0.0013284961460158229, 0.0013284961460158229, 0.0013284961460158229, 0.0013284961460158229]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013284961460158229

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.29345560
Iteration 2/25 | Loss: 0.00080102
Iteration 3/25 | Loss: 0.00080100
Iteration 4/25 | Loss: 0.00080100
Iteration 5/25 | Loss: 0.00080100
Iteration 6/25 | Loss: 0.00080100
Iteration 7/25 | Loss: 0.00080100
Iteration 8/25 | Loss: 0.00080100
Iteration 9/25 | Loss: 0.00080100
Iteration 10/25 | Loss: 0.00080100
Iteration 11/25 | Loss: 0.00080100
Iteration 12/25 | Loss: 0.00080100
Iteration 13/25 | Loss: 0.00080100
Iteration 14/25 | Loss: 0.00080100
Iteration 15/25 | Loss: 0.00080100
Iteration 16/25 | Loss: 0.00080100
Iteration 17/25 | Loss: 0.00080100
Iteration 18/25 | Loss: 0.00080100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008009995799511671, 0.0008009995799511671, 0.0008009995799511671, 0.0008009995799511671, 0.0008009995799511671]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008009995799511671

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080100
Iteration 2/1000 | Loss: 0.00003998
Iteration 3/1000 | Loss: 0.00002722
Iteration 4/1000 | Loss: 0.00002468
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002252
Iteration 7/1000 | Loss: 0.00002192
Iteration 8/1000 | Loss: 0.00002159
Iteration 9/1000 | Loss: 0.00002114
Iteration 10/1000 | Loss: 0.00002085
Iteration 11/1000 | Loss: 0.00002065
Iteration 12/1000 | Loss: 0.00002048
Iteration 13/1000 | Loss: 0.00002041
Iteration 14/1000 | Loss: 0.00002033
Iteration 15/1000 | Loss: 0.00002019
Iteration 16/1000 | Loss: 0.00002018
Iteration 17/1000 | Loss: 0.00002015
Iteration 18/1000 | Loss: 0.00002014
Iteration 19/1000 | Loss: 0.00002014
Iteration 20/1000 | Loss: 0.00002013
Iteration 21/1000 | Loss: 0.00002009
Iteration 22/1000 | Loss: 0.00002009
Iteration 23/1000 | Loss: 0.00002009
Iteration 24/1000 | Loss: 0.00002009
Iteration 25/1000 | Loss: 0.00002009
Iteration 26/1000 | Loss: 0.00002009
Iteration 27/1000 | Loss: 0.00002009
Iteration 28/1000 | Loss: 0.00002009
Iteration 29/1000 | Loss: 0.00002009
Iteration 30/1000 | Loss: 0.00002008
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00002008
Iteration 33/1000 | Loss: 0.00002008
Iteration 34/1000 | Loss: 0.00002004
Iteration 35/1000 | Loss: 0.00002002
Iteration 36/1000 | Loss: 0.00002002
Iteration 37/1000 | Loss: 0.00002001
Iteration 38/1000 | Loss: 0.00002001
Iteration 39/1000 | Loss: 0.00002000
Iteration 40/1000 | Loss: 0.00002000
Iteration 41/1000 | Loss: 0.00001999
Iteration 42/1000 | Loss: 0.00001999
Iteration 43/1000 | Loss: 0.00001999
Iteration 44/1000 | Loss: 0.00001999
Iteration 45/1000 | Loss: 0.00001998
Iteration 46/1000 | Loss: 0.00001998
Iteration 47/1000 | Loss: 0.00001998
Iteration 48/1000 | Loss: 0.00001997
Iteration 49/1000 | Loss: 0.00001997
Iteration 50/1000 | Loss: 0.00001996
Iteration 51/1000 | Loss: 0.00001996
Iteration 52/1000 | Loss: 0.00001996
Iteration 53/1000 | Loss: 0.00001996
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001994
Iteration 56/1000 | Loss: 0.00001994
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001993
Iteration 59/1000 | Loss: 0.00001993
Iteration 60/1000 | Loss: 0.00001993
Iteration 61/1000 | Loss: 0.00001993
Iteration 62/1000 | Loss: 0.00001993
Iteration 63/1000 | Loss: 0.00001993
Iteration 64/1000 | Loss: 0.00001993
Iteration 65/1000 | Loss: 0.00001993
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001991
Iteration 71/1000 | Loss: 0.00001991
Iteration 72/1000 | Loss: 0.00001990
Iteration 73/1000 | Loss: 0.00001990
Iteration 74/1000 | Loss: 0.00001990
Iteration 75/1000 | Loss: 0.00001989
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001989
Iteration 78/1000 | Loss: 0.00001988
Iteration 79/1000 | Loss: 0.00001987
Iteration 80/1000 | Loss: 0.00001987
Iteration 81/1000 | Loss: 0.00001987
Iteration 82/1000 | Loss: 0.00001987
Iteration 83/1000 | Loss: 0.00001987
Iteration 84/1000 | Loss: 0.00001986
Iteration 85/1000 | Loss: 0.00001986
Iteration 86/1000 | Loss: 0.00001986
Iteration 87/1000 | Loss: 0.00001986
Iteration 88/1000 | Loss: 0.00001986
Iteration 89/1000 | Loss: 0.00001986
Iteration 90/1000 | Loss: 0.00001985
Iteration 91/1000 | Loss: 0.00001985
Iteration 92/1000 | Loss: 0.00001985
Iteration 93/1000 | Loss: 0.00001985
Iteration 94/1000 | Loss: 0.00001985
Iteration 95/1000 | Loss: 0.00001985
Iteration 96/1000 | Loss: 0.00001984
Iteration 97/1000 | Loss: 0.00001984
Iteration 98/1000 | Loss: 0.00001984
Iteration 99/1000 | Loss: 0.00001984
Iteration 100/1000 | Loss: 0.00001984
Iteration 101/1000 | Loss: 0.00001984
Iteration 102/1000 | Loss: 0.00001983
Iteration 103/1000 | Loss: 0.00001983
Iteration 104/1000 | Loss: 0.00001983
Iteration 105/1000 | Loss: 0.00001983
Iteration 106/1000 | Loss: 0.00001983
Iteration 107/1000 | Loss: 0.00001983
Iteration 108/1000 | Loss: 0.00001983
Iteration 109/1000 | Loss: 0.00001982
Iteration 110/1000 | Loss: 0.00001982
Iteration 111/1000 | Loss: 0.00001982
Iteration 112/1000 | Loss: 0.00001982
Iteration 113/1000 | Loss: 0.00001981
Iteration 114/1000 | Loss: 0.00001981
Iteration 115/1000 | Loss: 0.00001981
Iteration 116/1000 | Loss: 0.00001981
Iteration 117/1000 | Loss: 0.00001981
Iteration 118/1000 | Loss: 0.00001981
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001980
Iteration 121/1000 | Loss: 0.00001980
Iteration 122/1000 | Loss: 0.00001980
Iteration 123/1000 | Loss: 0.00001980
Iteration 124/1000 | Loss: 0.00001980
Iteration 125/1000 | Loss: 0.00001980
Iteration 126/1000 | Loss: 0.00001980
Iteration 127/1000 | Loss: 0.00001980
Iteration 128/1000 | Loss: 0.00001980
Iteration 129/1000 | Loss: 0.00001980
Iteration 130/1000 | Loss: 0.00001980
Iteration 131/1000 | Loss: 0.00001980
Iteration 132/1000 | Loss: 0.00001980
Iteration 133/1000 | Loss: 0.00001980
Iteration 134/1000 | Loss: 0.00001980
Iteration 135/1000 | Loss: 0.00001980
Iteration 136/1000 | Loss: 0.00001980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.979578701138962e-05, 1.979578701138962e-05, 1.979578701138962e-05, 1.979578701138962e-05, 1.979578701138962e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.979578701138962e-05

Optimization complete. Final v2v error: 3.7190465927124023 mm

Highest mean error: 4.080570697784424 mm for frame 123

Lowest mean error: 3.47409725189209 mm for frame 149

Saving results

Total time: 42.64554786682129
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768949
Iteration 2/25 | Loss: 0.00140252
Iteration 3/25 | Loss: 0.00123536
Iteration 4/25 | Loss: 0.00121762
Iteration 5/25 | Loss: 0.00119990
Iteration 6/25 | Loss: 0.00119858
Iteration 7/25 | Loss: 0.00119829
Iteration 8/25 | Loss: 0.00119814
Iteration 9/25 | Loss: 0.00119810
Iteration 10/25 | Loss: 0.00119809
Iteration 11/25 | Loss: 0.00119809
Iteration 12/25 | Loss: 0.00119808
Iteration 13/25 | Loss: 0.00119808
Iteration 14/25 | Loss: 0.00119807
Iteration 15/25 | Loss: 0.00119807
Iteration 16/25 | Loss: 0.00119807
Iteration 17/25 | Loss: 0.00119807
Iteration 18/25 | Loss: 0.00119807
Iteration 19/25 | Loss: 0.00119807
Iteration 20/25 | Loss: 0.00119807
Iteration 21/25 | Loss: 0.00119807
Iteration 22/25 | Loss: 0.00119807
Iteration 23/25 | Loss: 0.00119807
Iteration 24/25 | Loss: 0.00119807
Iteration 25/25 | Loss: 0.00119806

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98422313
Iteration 2/25 | Loss: 0.00082975
Iteration 3/25 | Loss: 0.00082975
Iteration 4/25 | Loss: 0.00082974
Iteration 5/25 | Loss: 0.00082974
Iteration 6/25 | Loss: 0.00082974
Iteration 7/25 | Loss: 0.00082974
Iteration 8/25 | Loss: 0.00082974
Iteration 9/25 | Loss: 0.00082974
Iteration 10/25 | Loss: 0.00082974
Iteration 11/25 | Loss: 0.00082974
Iteration 12/25 | Loss: 0.00082974
Iteration 13/25 | Loss: 0.00082974
Iteration 14/25 | Loss: 0.00082974
Iteration 15/25 | Loss: 0.00082974
Iteration 16/25 | Loss: 0.00082974
Iteration 17/25 | Loss: 0.00082974
Iteration 18/25 | Loss: 0.00082974
Iteration 19/25 | Loss: 0.00082974
Iteration 20/25 | Loss: 0.00082974
Iteration 21/25 | Loss: 0.00082974
Iteration 22/25 | Loss: 0.00082974
Iteration 23/25 | Loss: 0.00082974
Iteration 24/25 | Loss: 0.00082974
Iteration 25/25 | Loss: 0.00082974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082974
Iteration 2/1000 | Loss: 0.00002179
Iteration 3/1000 | Loss: 0.00001715
Iteration 4/1000 | Loss: 0.00001525
Iteration 5/1000 | Loss: 0.00001442
Iteration 6/1000 | Loss: 0.00001386
Iteration 7/1000 | Loss: 0.00001342
Iteration 8/1000 | Loss: 0.00001315
Iteration 9/1000 | Loss: 0.00001294
Iteration 10/1000 | Loss: 0.00001273
Iteration 11/1000 | Loss: 0.00001272
Iteration 12/1000 | Loss: 0.00001268
Iteration 13/1000 | Loss: 0.00001267
Iteration 14/1000 | Loss: 0.00001266
Iteration 15/1000 | Loss: 0.00001263
Iteration 16/1000 | Loss: 0.00001259
Iteration 17/1000 | Loss: 0.00001258
Iteration 18/1000 | Loss: 0.00001258
Iteration 19/1000 | Loss: 0.00001258
Iteration 20/1000 | Loss: 0.00001241
Iteration 21/1000 | Loss: 0.00001238
Iteration 22/1000 | Loss: 0.00001238
Iteration 23/1000 | Loss: 0.00001237
Iteration 24/1000 | Loss: 0.00001237
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001235
Iteration 27/1000 | Loss: 0.00001232
Iteration 28/1000 | Loss: 0.00001232
Iteration 29/1000 | Loss: 0.00001231
Iteration 30/1000 | Loss: 0.00001231
Iteration 31/1000 | Loss: 0.00001230
Iteration 32/1000 | Loss: 0.00001230
Iteration 33/1000 | Loss: 0.00001229
Iteration 34/1000 | Loss: 0.00001229
Iteration 35/1000 | Loss: 0.00001228
Iteration 36/1000 | Loss: 0.00001228
Iteration 37/1000 | Loss: 0.00001228
Iteration 38/1000 | Loss: 0.00001227
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001225
Iteration 43/1000 | Loss: 0.00001225
Iteration 44/1000 | Loss: 0.00001224
Iteration 45/1000 | Loss: 0.00001223
Iteration 46/1000 | Loss: 0.00001223
Iteration 47/1000 | Loss: 0.00001222
Iteration 48/1000 | Loss: 0.00001221
Iteration 49/1000 | Loss: 0.00001221
Iteration 50/1000 | Loss: 0.00001221
Iteration 51/1000 | Loss: 0.00001221
Iteration 52/1000 | Loss: 0.00001221
Iteration 53/1000 | Loss: 0.00001221
Iteration 54/1000 | Loss: 0.00001221
Iteration 55/1000 | Loss: 0.00001221
Iteration 56/1000 | Loss: 0.00001221
Iteration 57/1000 | Loss: 0.00001220
Iteration 58/1000 | Loss: 0.00001220
Iteration 59/1000 | Loss: 0.00001220
Iteration 60/1000 | Loss: 0.00001220
Iteration 61/1000 | Loss: 0.00001219
Iteration 62/1000 | Loss: 0.00001219
Iteration 63/1000 | Loss: 0.00001218
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001217
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001216
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001214
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001213
Iteration 74/1000 | Loss: 0.00001213
Iteration 75/1000 | Loss: 0.00001213
Iteration 76/1000 | Loss: 0.00001213
Iteration 77/1000 | Loss: 0.00001213
Iteration 78/1000 | Loss: 0.00001212
Iteration 79/1000 | Loss: 0.00001212
Iteration 80/1000 | Loss: 0.00001211
Iteration 81/1000 | Loss: 0.00001211
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001209
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001208
Iteration 86/1000 | Loss: 0.00001208
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001207
Iteration 89/1000 | Loss: 0.00001207
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001205
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001204
Iteration 95/1000 | Loss: 0.00001204
Iteration 96/1000 | Loss: 0.00001204
Iteration 97/1000 | Loss: 0.00001204
Iteration 98/1000 | Loss: 0.00001203
Iteration 99/1000 | Loss: 0.00001203
Iteration 100/1000 | Loss: 0.00001203
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001201
Iteration 105/1000 | Loss: 0.00001201
Iteration 106/1000 | Loss: 0.00001201
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001199
Iteration 110/1000 | Loss: 0.00001199
Iteration 111/1000 | Loss: 0.00001199
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001199
Iteration 118/1000 | Loss: 0.00001199
Iteration 119/1000 | Loss: 0.00001198
Iteration 120/1000 | Loss: 0.00001198
Iteration 121/1000 | Loss: 0.00001198
Iteration 122/1000 | Loss: 0.00001198
Iteration 123/1000 | Loss: 0.00001198
Iteration 124/1000 | Loss: 0.00001198
Iteration 125/1000 | Loss: 0.00001198
Iteration 126/1000 | Loss: 0.00001198
Iteration 127/1000 | Loss: 0.00001198
Iteration 128/1000 | Loss: 0.00001198
Iteration 129/1000 | Loss: 0.00001198
Iteration 130/1000 | Loss: 0.00001198
Iteration 131/1000 | Loss: 0.00001197
Iteration 132/1000 | Loss: 0.00001197
Iteration 133/1000 | Loss: 0.00001197
Iteration 134/1000 | Loss: 0.00001196
Iteration 135/1000 | Loss: 0.00001196
Iteration 136/1000 | Loss: 0.00001196
Iteration 137/1000 | Loss: 0.00001196
Iteration 138/1000 | Loss: 0.00001196
Iteration 139/1000 | Loss: 0.00001195
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001195
Iteration 142/1000 | Loss: 0.00001195
Iteration 143/1000 | Loss: 0.00001195
Iteration 144/1000 | Loss: 0.00001195
Iteration 145/1000 | Loss: 0.00001195
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001194
Iteration 148/1000 | Loss: 0.00001194
Iteration 149/1000 | Loss: 0.00001194
Iteration 150/1000 | Loss: 0.00001194
Iteration 151/1000 | Loss: 0.00001194
Iteration 152/1000 | Loss: 0.00001194
Iteration 153/1000 | Loss: 0.00001194
Iteration 154/1000 | Loss: 0.00001194
Iteration 155/1000 | Loss: 0.00001193
Iteration 156/1000 | Loss: 0.00001193
Iteration 157/1000 | Loss: 0.00001193
Iteration 158/1000 | Loss: 0.00001193
Iteration 159/1000 | Loss: 0.00001193
Iteration 160/1000 | Loss: 0.00001193
Iteration 161/1000 | Loss: 0.00001193
Iteration 162/1000 | Loss: 0.00001193
Iteration 163/1000 | Loss: 0.00001193
Iteration 164/1000 | Loss: 0.00001193
Iteration 165/1000 | Loss: 0.00001193
Iteration 166/1000 | Loss: 0.00001193
Iteration 167/1000 | Loss: 0.00001193
Iteration 168/1000 | Loss: 0.00001192
Iteration 169/1000 | Loss: 0.00001192
Iteration 170/1000 | Loss: 0.00001192
Iteration 171/1000 | Loss: 0.00001192
Iteration 172/1000 | Loss: 0.00001192
Iteration 173/1000 | Loss: 0.00001192
Iteration 174/1000 | Loss: 0.00001192
Iteration 175/1000 | Loss: 0.00001192
Iteration 176/1000 | Loss: 0.00001192
Iteration 177/1000 | Loss: 0.00001192
Iteration 178/1000 | Loss: 0.00001191
Iteration 179/1000 | Loss: 0.00001191
Iteration 180/1000 | Loss: 0.00001191
Iteration 181/1000 | Loss: 0.00001191
Iteration 182/1000 | Loss: 0.00001191
Iteration 183/1000 | Loss: 0.00001191
Iteration 184/1000 | Loss: 0.00001191
Iteration 185/1000 | Loss: 0.00001191
Iteration 186/1000 | Loss: 0.00001191
Iteration 187/1000 | Loss: 0.00001191
Iteration 188/1000 | Loss: 0.00001191
Iteration 189/1000 | Loss: 0.00001191
Iteration 190/1000 | Loss: 0.00001191
Iteration 191/1000 | Loss: 0.00001191
Iteration 192/1000 | Loss: 0.00001191
Iteration 193/1000 | Loss: 0.00001191
Iteration 194/1000 | Loss: 0.00001191
Iteration 195/1000 | Loss: 0.00001191
Iteration 196/1000 | Loss: 0.00001191
Iteration 197/1000 | Loss: 0.00001191
Iteration 198/1000 | Loss: 0.00001191
Iteration 199/1000 | Loss: 0.00001191
Iteration 200/1000 | Loss: 0.00001191
Iteration 201/1000 | Loss: 0.00001191
Iteration 202/1000 | Loss: 0.00001191
Iteration 203/1000 | Loss: 0.00001191
Iteration 204/1000 | Loss: 0.00001191
Iteration 205/1000 | Loss: 0.00001191
Iteration 206/1000 | Loss: 0.00001191
Iteration 207/1000 | Loss: 0.00001191
Iteration 208/1000 | Loss: 0.00001191
Iteration 209/1000 | Loss: 0.00001191
Iteration 210/1000 | Loss: 0.00001191
Iteration 211/1000 | Loss: 0.00001191
Iteration 212/1000 | Loss: 0.00001191
Iteration 213/1000 | Loss: 0.00001191
Iteration 214/1000 | Loss: 0.00001191
Iteration 215/1000 | Loss: 0.00001191
Iteration 216/1000 | Loss: 0.00001191
Iteration 217/1000 | Loss: 0.00001191
Iteration 218/1000 | Loss: 0.00001191
Iteration 219/1000 | Loss: 0.00001191
Iteration 220/1000 | Loss: 0.00001191
Iteration 221/1000 | Loss: 0.00001191
Iteration 222/1000 | Loss: 0.00001191
Iteration 223/1000 | Loss: 0.00001191
Iteration 224/1000 | Loss: 0.00001191
Iteration 225/1000 | Loss: 0.00001191
Iteration 226/1000 | Loss: 0.00001191
Iteration 227/1000 | Loss: 0.00001191
Iteration 228/1000 | Loss: 0.00001191
Iteration 229/1000 | Loss: 0.00001191
Iteration 230/1000 | Loss: 0.00001191
Iteration 231/1000 | Loss: 0.00001191
Iteration 232/1000 | Loss: 0.00001191
Iteration 233/1000 | Loss: 0.00001191
Iteration 234/1000 | Loss: 0.00001191
Iteration 235/1000 | Loss: 0.00001191
Iteration 236/1000 | Loss: 0.00001191
Iteration 237/1000 | Loss: 0.00001191
Iteration 238/1000 | Loss: 0.00001191
Iteration 239/1000 | Loss: 0.00001191
Iteration 240/1000 | Loss: 0.00001191
Iteration 241/1000 | Loss: 0.00001191
Iteration 242/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 242. Stopping optimization.
Last 5 losses: [1.1913037269550841e-05, 1.1913037269550841e-05, 1.1913037269550841e-05, 1.1913037269550841e-05, 1.1913037269550841e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1913037269550841e-05

Optimization complete. Final v2v error: 2.9698102474212646 mm

Highest mean error: 3.163991689682007 mm for frame 64

Lowest mean error: 2.8278748989105225 mm for frame 2

Saving results

Total time: 46.309372663497925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809089
Iteration 2/25 | Loss: 0.00142323
Iteration 3/25 | Loss: 0.00125835
Iteration 4/25 | Loss: 0.00123612
Iteration 5/25 | Loss: 0.00121426
Iteration 6/25 | Loss: 0.00121334
Iteration 7/25 | Loss: 0.00121291
Iteration 8/25 | Loss: 0.00121271
Iteration 9/25 | Loss: 0.00121265
Iteration 10/25 | Loss: 0.00121265
Iteration 11/25 | Loss: 0.00121265
Iteration 12/25 | Loss: 0.00121264
Iteration 13/25 | Loss: 0.00121264
Iteration 14/25 | Loss: 0.00121264
Iteration 15/25 | Loss: 0.00121264
Iteration 16/25 | Loss: 0.00121264
Iteration 17/25 | Loss: 0.00121264
Iteration 18/25 | Loss: 0.00121264
Iteration 19/25 | Loss: 0.00121264
Iteration 20/25 | Loss: 0.00121264
Iteration 21/25 | Loss: 0.00121264
Iteration 22/25 | Loss: 0.00121264
Iteration 23/25 | Loss: 0.00121263
Iteration 24/25 | Loss: 0.00121263
Iteration 25/25 | Loss: 0.00121263

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.55012393
Iteration 2/25 | Loss: 0.00081064
Iteration 3/25 | Loss: 0.00081064
Iteration 4/25 | Loss: 0.00081064
Iteration 5/25 | Loss: 0.00081064
Iteration 6/25 | Loss: 0.00081064
Iteration 7/25 | Loss: 0.00081064
Iteration 8/25 | Loss: 0.00081064
Iteration 9/25 | Loss: 0.00081064
Iteration 10/25 | Loss: 0.00081064
Iteration 11/25 | Loss: 0.00081064
Iteration 12/25 | Loss: 0.00081064
Iteration 13/25 | Loss: 0.00081064
Iteration 14/25 | Loss: 0.00081064
Iteration 15/25 | Loss: 0.00081064
Iteration 16/25 | Loss: 0.00081064
Iteration 17/25 | Loss: 0.00081064
Iteration 18/25 | Loss: 0.00081064
Iteration 19/25 | Loss: 0.00081064
Iteration 20/25 | Loss: 0.00081064
Iteration 21/25 | Loss: 0.00081064
Iteration 22/25 | Loss: 0.00081064
Iteration 23/25 | Loss: 0.00081064
Iteration 24/25 | Loss: 0.00081064
Iteration 25/25 | Loss: 0.00081064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081064
Iteration 2/1000 | Loss: 0.00002509
Iteration 3/1000 | Loss: 0.00001875
Iteration 4/1000 | Loss: 0.00001663
Iteration 5/1000 | Loss: 0.00001584
Iteration 6/1000 | Loss: 0.00001529
Iteration 7/1000 | Loss: 0.00001481
Iteration 8/1000 | Loss: 0.00001458
Iteration 9/1000 | Loss: 0.00001433
Iteration 10/1000 | Loss: 0.00001416
Iteration 11/1000 | Loss: 0.00001406
Iteration 12/1000 | Loss: 0.00001388
Iteration 13/1000 | Loss: 0.00001380
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001368
Iteration 16/1000 | Loss: 0.00001368
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001363
Iteration 20/1000 | Loss: 0.00001362
Iteration 21/1000 | Loss: 0.00001361
Iteration 22/1000 | Loss: 0.00001360
Iteration 23/1000 | Loss: 0.00001360
Iteration 24/1000 | Loss: 0.00001359
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001357
Iteration 28/1000 | Loss: 0.00001356
Iteration 29/1000 | Loss: 0.00001355
Iteration 30/1000 | Loss: 0.00001354
Iteration 31/1000 | Loss: 0.00001354
Iteration 32/1000 | Loss: 0.00001354
Iteration 33/1000 | Loss: 0.00001353
Iteration 34/1000 | Loss: 0.00001353
Iteration 35/1000 | Loss: 0.00001348
Iteration 36/1000 | Loss: 0.00001348
Iteration 37/1000 | Loss: 0.00001347
Iteration 38/1000 | Loss: 0.00001346
Iteration 39/1000 | Loss: 0.00001346
Iteration 40/1000 | Loss: 0.00001345
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001343
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001342
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001339
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001336
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001334
Iteration 55/1000 | Loss: 0.00001333
Iteration 56/1000 | Loss: 0.00001333
Iteration 57/1000 | Loss: 0.00001332
Iteration 58/1000 | Loss: 0.00001330
Iteration 59/1000 | Loss: 0.00001329
Iteration 60/1000 | Loss: 0.00001327
Iteration 61/1000 | Loss: 0.00001327
Iteration 62/1000 | Loss: 0.00001327
Iteration 63/1000 | Loss: 0.00001327
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001327
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001326
Iteration 70/1000 | Loss: 0.00001326
Iteration 71/1000 | Loss: 0.00001325
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001324
Iteration 76/1000 | Loss: 0.00001324
Iteration 77/1000 | Loss: 0.00001324
Iteration 78/1000 | Loss: 0.00001324
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001323
Iteration 82/1000 | Loss: 0.00001323
Iteration 83/1000 | Loss: 0.00001323
Iteration 84/1000 | Loss: 0.00001323
Iteration 85/1000 | Loss: 0.00001322
Iteration 86/1000 | Loss: 0.00001322
Iteration 87/1000 | Loss: 0.00001322
Iteration 88/1000 | Loss: 0.00001321
Iteration 89/1000 | Loss: 0.00001321
Iteration 90/1000 | Loss: 0.00001321
Iteration 91/1000 | Loss: 0.00001320
Iteration 92/1000 | Loss: 0.00001320
Iteration 93/1000 | Loss: 0.00001320
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001319
Iteration 96/1000 | Loss: 0.00001319
Iteration 97/1000 | Loss: 0.00001319
Iteration 98/1000 | Loss: 0.00001319
Iteration 99/1000 | Loss: 0.00001319
Iteration 100/1000 | Loss: 0.00001318
Iteration 101/1000 | Loss: 0.00001318
Iteration 102/1000 | Loss: 0.00001318
Iteration 103/1000 | Loss: 0.00001318
Iteration 104/1000 | Loss: 0.00001317
Iteration 105/1000 | Loss: 0.00001317
Iteration 106/1000 | Loss: 0.00001317
Iteration 107/1000 | Loss: 0.00001317
Iteration 108/1000 | Loss: 0.00001317
Iteration 109/1000 | Loss: 0.00001317
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00001317
Iteration 113/1000 | Loss: 0.00001316
Iteration 114/1000 | Loss: 0.00001316
Iteration 115/1000 | Loss: 0.00001316
Iteration 116/1000 | Loss: 0.00001315
Iteration 117/1000 | Loss: 0.00001315
Iteration 118/1000 | Loss: 0.00001315
Iteration 119/1000 | Loss: 0.00001315
Iteration 120/1000 | Loss: 0.00001315
Iteration 121/1000 | Loss: 0.00001314
Iteration 122/1000 | Loss: 0.00001314
Iteration 123/1000 | Loss: 0.00001314
Iteration 124/1000 | Loss: 0.00001314
Iteration 125/1000 | Loss: 0.00001314
Iteration 126/1000 | Loss: 0.00001314
Iteration 127/1000 | Loss: 0.00001314
Iteration 128/1000 | Loss: 0.00001314
Iteration 129/1000 | Loss: 0.00001313
Iteration 130/1000 | Loss: 0.00001313
Iteration 131/1000 | Loss: 0.00001313
Iteration 132/1000 | Loss: 0.00001313
Iteration 133/1000 | Loss: 0.00001313
Iteration 134/1000 | Loss: 0.00001313
Iteration 135/1000 | Loss: 0.00001313
Iteration 136/1000 | Loss: 0.00001313
Iteration 137/1000 | Loss: 0.00001313
Iteration 138/1000 | Loss: 0.00001312
Iteration 139/1000 | Loss: 0.00001312
Iteration 140/1000 | Loss: 0.00001312
Iteration 141/1000 | Loss: 0.00001312
Iteration 142/1000 | Loss: 0.00001312
Iteration 143/1000 | Loss: 0.00001312
Iteration 144/1000 | Loss: 0.00001311
Iteration 145/1000 | Loss: 0.00001311
Iteration 146/1000 | Loss: 0.00001311
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00001311
Iteration 149/1000 | Loss: 0.00001311
Iteration 150/1000 | Loss: 0.00001311
Iteration 151/1000 | Loss: 0.00001311
Iteration 152/1000 | Loss: 0.00001311
Iteration 153/1000 | Loss: 0.00001311
Iteration 154/1000 | Loss: 0.00001311
Iteration 155/1000 | Loss: 0.00001311
Iteration 156/1000 | Loss: 0.00001311
Iteration 157/1000 | Loss: 0.00001311
Iteration 158/1000 | Loss: 0.00001310
Iteration 159/1000 | Loss: 0.00001310
Iteration 160/1000 | Loss: 0.00001310
Iteration 161/1000 | Loss: 0.00001310
Iteration 162/1000 | Loss: 0.00001310
Iteration 163/1000 | Loss: 0.00001310
Iteration 164/1000 | Loss: 0.00001310
Iteration 165/1000 | Loss: 0.00001310
Iteration 166/1000 | Loss: 0.00001310
Iteration 167/1000 | Loss: 0.00001310
Iteration 168/1000 | Loss: 0.00001310
Iteration 169/1000 | Loss: 0.00001310
Iteration 170/1000 | Loss: 0.00001310
Iteration 171/1000 | Loss: 0.00001310
Iteration 172/1000 | Loss: 0.00001310
Iteration 173/1000 | Loss: 0.00001310
Iteration 174/1000 | Loss: 0.00001310
Iteration 175/1000 | Loss: 0.00001310
Iteration 176/1000 | Loss: 0.00001310
Iteration 177/1000 | Loss: 0.00001310
Iteration 178/1000 | Loss: 0.00001310
Iteration 179/1000 | Loss: 0.00001310
Iteration 180/1000 | Loss: 0.00001310
Iteration 181/1000 | Loss: 0.00001310
Iteration 182/1000 | Loss: 0.00001310
Iteration 183/1000 | Loss: 0.00001310
Iteration 184/1000 | Loss: 0.00001310
Iteration 185/1000 | Loss: 0.00001310
Iteration 186/1000 | Loss: 0.00001310
Iteration 187/1000 | Loss: 0.00001310
Iteration 188/1000 | Loss: 0.00001310
Iteration 189/1000 | Loss: 0.00001310
Iteration 190/1000 | Loss: 0.00001310
Iteration 191/1000 | Loss: 0.00001310
Iteration 192/1000 | Loss: 0.00001310
Iteration 193/1000 | Loss: 0.00001310
Iteration 194/1000 | Loss: 0.00001310
Iteration 195/1000 | Loss: 0.00001310
Iteration 196/1000 | Loss: 0.00001310
Iteration 197/1000 | Loss: 0.00001310
Iteration 198/1000 | Loss: 0.00001310
Iteration 199/1000 | Loss: 0.00001310
Iteration 200/1000 | Loss: 0.00001310
Iteration 201/1000 | Loss: 0.00001310
Iteration 202/1000 | Loss: 0.00001310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.3095920621708501e-05, 1.3095920621708501e-05, 1.3095920621708501e-05, 1.3095920621708501e-05, 1.3095920621708501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3095920621708501e-05

Optimization complete. Final v2v error: 3.092332363128662 mm

Highest mean error: 3.477835178375244 mm for frame 50

Lowest mean error: 2.842780351638794 mm for frame 156

Saving results

Total time: 47.35117316246033
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022314
Iteration 2/25 | Loss: 0.00167374
Iteration 3/25 | Loss: 0.00137257
Iteration 4/25 | Loss: 0.00133737
Iteration 5/25 | Loss: 0.00136717
Iteration 6/25 | Loss: 0.00134139
Iteration 7/25 | Loss: 0.00133163
Iteration 8/25 | Loss: 0.00132149
Iteration 9/25 | Loss: 0.00130588
Iteration 10/25 | Loss: 0.00131561
Iteration 11/25 | Loss: 0.00131366
Iteration 12/25 | Loss: 0.00131548
Iteration 13/25 | Loss: 0.00131669
Iteration 14/25 | Loss: 0.00131839
Iteration 15/25 | Loss: 0.00131781
Iteration 16/25 | Loss: 0.00131859
Iteration 17/25 | Loss: 0.00131596
Iteration 18/25 | Loss: 0.00131274
Iteration 19/25 | Loss: 0.00131149
Iteration 20/25 | Loss: 0.00130671
Iteration 21/25 | Loss: 0.00130472
Iteration 22/25 | Loss: 0.00130368
Iteration 23/25 | Loss: 0.00130240
Iteration 24/25 | Loss: 0.00130213
Iteration 25/25 | Loss: 0.00129855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63010049
Iteration 2/25 | Loss: 0.00105505
Iteration 3/25 | Loss: 0.00105503
Iteration 4/25 | Loss: 0.00105503
Iteration 5/25 | Loss: 0.00105503
Iteration 6/25 | Loss: 0.00105503
Iteration 7/25 | Loss: 0.00105503
Iteration 8/25 | Loss: 0.00105503
Iteration 9/25 | Loss: 0.00105503
Iteration 10/25 | Loss: 0.00105503
Iteration 11/25 | Loss: 0.00105503
Iteration 12/25 | Loss: 0.00105503
Iteration 13/25 | Loss: 0.00105503
Iteration 14/25 | Loss: 0.00105503
Iteration 15/25 | Loss: 0.00105503
Iteration 16/25 | Loss: 0.00105503
Iteration 17/25 | Loss: 0.00105503
Iteration 18/25 | Loss: 0.00105503
Iteration 19/25 | Loss: 0.00105503
Iteration 20/25 | Loss: 0.00105503
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010550309671089053, 0.0010550309671089053, 0.0010550309671089053, 0.0010550309671089053, 0.0010550309671089053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010550309671089053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105503
Iteration 2/1000 | Loss: 0.00021486
Iteration 3/1000 | Loss: 0.00022351
Iteration 4/1000 | Loss: 0.00024701
Iteration 5/1000 | Loss: 0.00027324
Iteration 6/1000 | Loss: 0.00016692
Iteration 7/1000 | Loss: 0.00044838
Iteration 8/1000 | Loss: 0.00037298
Iteration 9/1000 | Loss: 0.00046445
Iteration 10/1000 | Loss: 0.00039036
Iteration 11/1000 | Loss: 0.00020883
Iteration 12/1000 | Loss: 0.00047266
Iteration 13/1000 | Loss: 0.00023063
Iteration 14/1000 | Loss: 0.00038125
Iteration 15/1000 | Loss: 0.00035609
Iteration 16/1000 | Loss: 0.00024030
Iteration 17/1000 | Loss: 0.00032856
Iteration 18/1000 | Loss: 0.00032955
Iteration 19/1000 | Loss: 0.00023213
Iteration 20/1000 | Loss: 0.00022245
Iteration 21/1000 | Loss: 0.00034797
Iteration 22/1000 | Loss: 0.00027158
Iteration 23/1000 | Loss: 0.00036765
Iteration 24/1000 | Loss: 0.00025618
Iteration 25/1000 | Loss: 0.00032938
Iteration 26/1000 | Loss: 0.00041195
Iteration 27/1000 | Loss: 0.00035279
Iteration 28/1000 | Loss: 0.00035353
Iteration 29/1000 | Loss: 0.00025072
Iteration 30/1000 | Loss: 0.00026905
Iteration 31/1000 | Loss: 0.00032686
Iteration 32/1000 | Loss: 0.00027226
Iteration 33/1000 | Loss: 0.00018803
Iteration 34/1000 | Loss: 0.00045153
Iteration 35/1000 | Loss: 0.00006968
Iteration 36/1000 | Loss: 0.00019771
Iteration 37/1000 | Loss: 0.00016056
Iteration 38/1000 | Loss: 0.00019640
Iteration 39/1000 | Loss: 0.00016031
Iteration 40/1000 | Loss: 0.00021247
Iteration 41/1000 | Loss: 0.00015732
Iteration 42/1000 | Loss: 0.00011837
Iteration 43/1000 | Loss: 0.00015233
Iteration 44/1000 | Loss: 0.00016148
Iteration 45/1000 | Loss: 0.00015688
Iteration 46/1000 | Loss: 0.00031904
Iteration 47/1000 | Loss: 0.00021653
Iteration 48/1000 | Loss: 0.00029018
Iteration 49/1000 | Loss: 0.00015420
Iteration 50/1000 | Loss: 0.00016552
Iteration 51/1000 | Loss: 0.00016433
Iteration 52/1000 | Loss: 0.00013182
Iteration 53/1000 | Loss: 0.00014312
Iteration 54/1000 | Loss: 0.00015898
Iteration 55/1000 | Loss: 0.00014974
Iteration 56/1000 | Loss: 0.00005476
Iteration 57/1000 | Loss: 0.00003557
Iteration 58/1000 | Loss: 0.00005461
Iteration 59/1000 | Loss: 0.00005324
Iteration 60/1000 | Loss: 0.00005653
Iteration 61/1000 | Loss: 0.00015065
Iteration 62/1000 | Loss: 0.00038538
Iteration 63/1000 | Loss: 0.00012119
Iteration 64/1000 | Loss: 0.00013095
Iteration 65/1000 | Loss: 0.00022197
Iteration 66/1000 | Loss: 0.00003061
Iteration 67/1000 | Loss: 0.00002387
Iteration 68/1000 | Loss: 0.00002231
Iteration 69/1000 | Loss: 0.00002131
Iteration 70/1000 | Loss: 0.00006006
Iteration 71/1000 | Loss: 0.00002797
Iteration 72/1000 | Loss: 0.00002338
Iteration 73/1000 | Loss: 0.00001916
Iteration 74/1000 | Loss: 0.00001827
Iteration 75/1000 | Loss: 0.00001785
Iteration 76/1000 | Loss: 0.00001768
Iteration 77/1000 | Loss: 0.00001740
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001714
Iteration 80/1000 | Loss: 0.00001714
Iteration 81/1000 | Loss: 0.00001712
Iteration 82/1000 | Loss: 0.00001709
Iteration 83/1000 | Loss: 0.00001708
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001707
Iteration 86/1000 | Loss: 0.00001701
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001696
Iteration 90/1000 | Loss: 0.00001696
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001694
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001694
Iteration 101/1000 | Loss: 0.00001694
Iteration 102/1000 | Loss: 0.00001694
Iteration 103/1000 | Loss: 0.00001694
Iteration 104/1000 | Loss: 0.00001693
Iteration 105/1000 | Loss: 0.00001693
Iteration 106/1000 | Loss: 0.00001693
Iteration 107/1000 | Loss: 0.00001693
Iteration 108/1000 | Loss: 0.00001693
Iteration 109/1000 | Loss: 0.00001693
Iteration 110/1000 | Loss: 0.00001693
Iteration 111/1000 | Loss: 0.00001693
Iteration 112/1000 | Loss: 0.00001693
Iteration 113/1000 | Loss: 0.00001693
Iteration 114/1000 | Loss: 0.00001692
Iteration 115/1000 | Loss: 0.00001692
Iteration 116/1000 | Loss: 0.00001692
Iteration 117/1000 | Loss: 0.00001692
Iteration 118/1000 | Loss: 0.00001692
Iteration 119/1000 | Loss: 0.00001692
Iteration 120/1000 | Loss: 0.00001691
Iteration 121/1000 | Loss: 0.00001691
Iteration 122/1000 | Loss: 0.00001691
Iteration 123/1000 | Loss: 0.00001691
Iteration 124/1000 | Loss: 0.00001691
Iteration 125/1000 | Loss: 0.00001691
Iteration 126/1000 | Loss: 0.00001690
Iteration 127/1000 | Loss: 0.00001690
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001689
Iteration 130/1000 | Loss: 0.00001688
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001688
Iteration 134/1000 | Loss: 0.00001688
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001686
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001685
Iteration 152/1000 | Loss: 0.00001685
Iteration 153/1000 | Loss: 0.00001685
Iteration 154/1000 | Loss: 0.00001685
Iteration 155/1000 | Loss: 0.00001685
Iteration 156/1000 | Loss: 0.00001685
Iteration 157/1000 | Loss: 0.00001685
Iteration 158/1000 | Loss: 0.00001685
Iteration 159/1000 | Loss: 0.00001685
Iteration 160/1000 | Loss: 0.00001685
Iteration 161/1000 | Loss: 0.00001684
Iteration 162/1000 | Loss: 0.00001684
Iteration 163/1000 | Loss: 0.00001684
Iteration 164/1000 | Loss: 0.00001684
Iteration 165/1000 | Loss: 0.00001684
Iteration 166/1000 | Loss: 0.00001684
Iteration 167/1000 | Loss: 0.00001684
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Iteration 170/1000 | Loss: 0.00001684
Iteration 171/1000 | Loss: 0.00001684
Iteration 172/1000 | Loss: 0.00001684
Iteration 173/1000 | Loss: 0.00001684
Iteration 174/1000 | Loss: 0.00001684
Iteration 175/1000 | Loss: 0.00001683
Iteration 176/1000 | Loss: 0.00001683
Iteration 177/1000 | Loss: 0.00001683
Iteration 178/1000 | Loss: 0.00001683
Iteration 179/1000 | Loss: 0.00001682
Iteration 180/1000 | Loss: 0.00001682
Iteration 181/1000 | Loss: 0.00001682
Iteration 182/1000 | Loss: 0.00001682
Iteration 183/1000 | Loss: 0.00001682
Iteration 184/1000 | Loss: 0.00001682
Iteration 185/1000 | Loss: 0.00001682
Iteration 186/1000 | Loss: 0.00001682
Iteration 187/1000 | Loss: 0.00001682
Iteration 188/1000 | Loss: 0.00001681
Iteration 189/1000 | Loss: 0.00001681
Iteration 190/1000 | Loss: 0.00001681
Iteration 191/1000 | Loss: 0.00001680
Iteration 192/1000 | Loss: 0.00001680
Iteration 193/1000 | Loss: 0.00001680
Iteration 194/1000 | Loss: 0.00001679
Iteration 195/1000 | Loss: 0.00001679
Iteration 196/1000 | Loss: 0.00001679
Iteration 197/1000 | Loss: 0.00001679
Iteration 198/1000 | Loss: 0.00001679
Iteration 199/1000 | Loss: 0.00001679
Iteration 200/1000 | Loss: 0.00001679
Iteration 201/1000 | Loss: 0.00001679
Iteration 202/1000 | Loss: 0.00001679
Iteration 203/1000 | Loss: 0.00001679
Iteration 204/1000 | Loss: 0.00001678
Iteration 205/1000 | Loss: 0.00001678
Iteration 206/1000 | Loss: 0.00001678
Iteration 207/1000 | Loss: 0.00001678
Iteration 208/1000 | Loss: 0.00001678
Iteration 209/1000 | Loss: 0.00001677
Iteration 210/1000 | Loss: 0.00001677
Iteration 211/1000 | Loss: 0.00001677
Iteration 212/1000 | Loss: 0.00001677
Iteration 213/1000 | Loss: 0.00001677
Iteration 214/1000 | Loss: 0.00001677
Iteration 215/1000 | Loss: 0.00001676
Iteration 216/1000 | Loss: 0.00001676
Iteration 217/1000 | Loss: 0.00001676
Iteration 218/1000 | Loss: 0.00001676
Iteration 219/1000 | Loss: 0.00001676
Iteration 220/1000 | Loss: 0.00001676
Iteration 221/1000 | Loss: 0.00001676
Iteration 222/1000 | Loss: 0.00001676
Iteration 223/1000 | Loss: 0.00001676
Iteration 224/1000 | Loss: 0.00001676
Iteration 225/1000 | Loss: 0.00001676
Iteration 226/1000 | Loss: 0.00001676
Iteration 227/1000 | Loss: 0.00001675
Iteration 228/1000 | Loss: 0.00001675
Iteration 229/1000 | Loss: 0.00001675
Iteration 230/1000 | Loss: 0.00001675
Iteration 231/1000 | Loss: 0.00001675
Iteration 232/1000 | Loss: 0.00001675
Iteration 233/1000 | Loss: 0.00001675
Iteration 234/1000 | Loss: 0.00001675
Iteration 235/1000 | Loss: 0.00001674
Iteration 236/1000 | Loss: 0.00001674
Iteration 237/1000 | Loss: 0.00001674
Iteration 238/1000 | Loss: 0.00001674
Iteration 239/1000 | Loss: 0.00001673
Iteration 240/1000 | Loss: 0.00001672
Iteration 241/1000 | Loss: 0.00001672
Iteration 242/1000 | Loss: 0.00001671
Iteration 243/1000 | Loss: 0.00001671
Iteration 244/1000 | Loss: 0.00001671
Iteration 245/1000 | Loss: 0.00001670
Iteration 246/1000 | Loss: 0.00001670
Iteration 247/1000 | Loss: 0.00001670
Iteration 248/1000 | Loss: 0.00001670
Iteration 249/1000 | Loss: 0.00001670
Iteration 250/1000 | Loss: 0.00001669
Iteration 251/1000 | Loss: 0.00001669
Iteration 252/1000 | Loss: 0.00001669
Iteration 253/1000 | Loss: 0.00001669
Iteration 254/1000 | Loss: 0.00001669
Iteration 255/1000 | Loss: 0.00001669
Iteration 256/1000 | Loss: 0.00001669
Iteration 257/1000 | Loss: 0.00001669
Iteration 258/1000 | Loss: 0.00001669
Iteration 259/1000 | Loss: 0.00001669
Iteration 260/1000 | Loss: 0.00001669
Iteration 261/1000 | Loss: 0.00001669
Iteration 262/1000 | Loss: 0.00001669
Iteration 263/1000 | Loss: 0.00001669
Iteration 264/1000 | Loss: 0.00001669
Iteration 265/1000 | Loss: 0.00001669
Iteration 266/1000 | Loss: 0.00001669
Iteration 267/1000 | Loss: 0.00001669
Iteration 268/1000 | Loss: 0.00001669
Iteration 269/1000 | Loss: 0.00001669
Iteration 270/1000 | Loss: 0.00001669
Iteration 271/1000 | Loss: 0.00001669
Iteration 272/1000 | Loss: 0.00001669
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 272. Stopping optimization.
Last 5 losses: [1.668726690695621e-05, 1.668726690695621e-05, 1.668726690695621e-05, 1.668726690695621e-05, 1.668726690695621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.668726690695621e-05

Optimization complete. Final v2v error: 3.374903440475464 mm

Highest mean error: 4.968313217163086 mm for frame 0

Lowest mean error: 2.948193311691284 mm for frame 56

Saving results

Total time: 164.07499599456787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382007
Iteration 2/25 | Loss: 0.00141056
Iteration 3/25 | Loss: 0.00127101
Iteration 4/25 | Loss: 0.00124113
Iteration 5/25 | Loss: 0.00123333
Iteration 6/25 | Loss: 0.00123078
Iteration 7/25 | Loss: 0.00123051
Iteration 8/25 | Loss: 0.00123051
Iteration 9/25 | Loss: 0.00123051
Iteration 10/25 | Loss: 0.00123051
Iteration 11/25 | Loss: 0.00123051
Iteration 12/25 | Loss: 0.00123051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001230508554726839, 0.001230508554726839, 0.001230508554726839, 0.001230508554726839, 0.001230508554726839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001230508554726839

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35446417
Iteration 2/25 | Loss: 0.00073633
Iteration 3/25 | Loss: 0.00073633
Iteration 4/25 | Loss: 0.00073633
Iteration 5/25 | Loss: 0.00073633
Iteration 6/25 | Loss: 0.00073633
Iteration 7/25 | Loss: 0.00073633
Iteration 8/25 | Loss: 0.00073633
Iteration 9/25 | Loss: 0.00073633
Iteration 10/25 | Loss: 0.00073633
Iteration 11/25 | Loss: 0.00073633
Iteration 12/25 | Loss: 0.00073633
Iteration 13/25 | Loss: 0.00073633
Iteration 14/25 | Loss: 0.00073633
Iteration 15/25 | Loss: 0.00073633
Iteration 16/25 | Loss: 0.00073633
Iteration 17/25 | Loss: 0.00073633
Iteration 18/25 | Loss: 0.00073633
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007363279000855982, 0.0007363279000855982, 0.0007363279000855982, 0.0007363279000855982, 0.0007363279000855982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007363279000855982

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073633
Iteration 2/1000 | Loss: 0.00003766
Iteration 3/1000 | Loss: 0.00002755
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002338
Iteration 6/1000 | Loss: 0.00002229
Iteration 7/1000 | Loss: 0.00002161
Iteration 8/1000 | Loss: 0.00002104
Iteration 9/1000 | Loss: 0.00002068
Iteration 10/1000 | Loss: 0.00002035
Iteration 11/1000 | Loss: 0.00002010
Iteration 12/1000 | Loss: 0.00001991
Iteration 13/1000 | Loss: 0.00001978
Iteration 14/1000 | Loss: 0.00001973
Iteration 15/1000 | Loss: 0.00001970
Iteration 16/1000 | Loss: 0.00001969
Iteration 17/1000 | Loss: 0.00001969
Iteration 18/1000 | Loss: 0.00001966
Iteration 19/1000 | Loss: 0.00001965
Iteration 20/1000 | Loss: 0.00001965
Iteration 21/1000 | Loss: 0.00001964
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001958
Iteration 26/1000 | Loss: 0.00001958
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001957
Iteration 29/1000 | Loss: 0.00001956
Iteration 30/1000 | Loss: 0.00001955
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001952
Iteration 34/1000 | Loss: 0.00001951
Iteration 35/1000 | Loss: 0.00001951
Iteration 36/1000 | Loss: 0.00001950
Iteration 37/1000 | Loss: 0.00001949
Iteration 38/1000 | Loss: 0.00001949
Iteration 39/1000 | Loss: 0.00001949
Iteration 40/1000 | Loss: 0.00001949
Iteration 41/1000 | Loss: 0.00001949
Iteration 42/1000 | Loss: 0.00001949
Iteration 43/1000 | Loss: 0.00001949
Iteration 44/1000 | Loss: 0.00001948
Iteration 45/1000 | Loss: 0.00001948
Iteration 46/1000 | Loss: 0.00001948
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001947
Iteration 49/1000 | Loss: 0.00001947
Iteration 50/1000 | Loss: 0.00001946
Iteration 51/1000 | Loss: 0.00001946
Iteration 52/1000 | Loss: 0.00001946
Iteration 53/1000 | Loss: 0.00001946
Iteration 54/1000 | Loss: 0.00001946
Iteration 55/1000 | Loss: 0.00001946
Iteration 56/1000 | Loss: 0.00001946
Iteration 57/1000 | Loss: 0.00001946
Iteration 58/1000 | Loss: 0.00001946
Iteration 59/1000 | Loss: 0.00001946
Iteration 60/1000 | Loss: 0.00001946
Iteration 61/1000 | Loss: 0.00001946
Iteration 62/1000 | Loss: 0.00001946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [1.94550993910525e-05, 1.94550993910525e-05, 1.94550993910525e-05, 1.94550993910525e-05, 1.94550993910525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.94550993910525e-05

Optimization complete. Final v2v error: 3.7177438735961914 mm

Highest mean error: 4.298442840576172 mm for frame 179

Lowest mean error: 2.9247915744781494 mm for frame 80

Saving results

Total time: 38.01957130432129
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570166
Iteration 2/25 | Loss: 0.00140953
Iteration 3/25 | Loss: 0.00134749
Iteration 4/25 | Loss: 0.00134108
Iteration 5/25 | Loss: 0.00133901
Iteration 6/25 | Loss: 0.00133901
Iteration 7/25 | Loss: 0.00133901
Iteration 8/25 | Loss: 0.00133901
Iteration 9/25 | Loss: 0.00133901
Iteration 10/25 | Loss: 0.00133901
Iteration 11/25 | Loss: 0.00133901
Iteration 12/25 | Loss: 0.00133901
Iteration 13/25 | Loss: 0.00133901
Iteration 14/25 | Loss: 0.00133901
Iteration 15/25 | Loss: 0.00133901
Iteration 16/25 | Loss: 0.00133901
Iteration 17/25 | Loss: 0.00133901
Iteration 18/25 | Loss: 0.00133901
Iteration 19/25 | Loss: 0.00133901
Iteration 20/25 | Loss: 0.00133901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001339012524113059, 0.001339012524113059, 0.001339012524113059, 0.001339012524113059, 0.001339012524113059]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001339012524113059

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 14.19579601
Iteration 2/25 | Loss: 0.00084815
Iteration 3/25 | Loss: 0.00084808
Iteration 4/25 | Loss: 0.00084808
Iteration 5/25 | Loss: 0.00084808
Iteration 6/25 | Loss: 0.00084808
Iteration 7/25 | Loss: 0.00084807
Iteration 8/25 | Loss: 0.00084807
Iteration 9/25 | Loss: 0.00084807
Iteration 10/25 | Loss: 0.00084807
Iteration 11/25 | Loss: 0.00084807
Iteration 12/25 | Loss: 0.00084807
Iteration 13/25 | Loss: 0.00084807
Iteration 14/25 | Loss: 0.00084807
Iteration 15/25 | Loss: 0.00084807
Iteration 16/25 | Loss: 0.00084807
Iteration 17/25 | Loss: 0.00084807
Iteration 18/25 | Loss: 0.00084807
Iteration 19/25 | Loss: 0.00084807
Iteration 20/25 | Loss: 0.00084807
Iteration 21/25 | Loss: 0.00084807
Iteration 22/25 | Loss: 0.00084807
Iteration 23/25 | Loss: 0.00084807
Iteration 24/25 | Loss: 0.00084807
Iteration 25/25 | Loss: 0.00084807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084807
Iteration 2/1000 | Loss: 0.00003182
Iteration 3/1000 | Loss: 0.00002290
Iteration 4/1000 | Loss: 0.00002108
Iteration 5/1000 | Loss: 0.00002020
Iteration 6/1000 | Loss: 0.00001974
Iteration 7/1000 | Loss: 0.00001951
Iteration 8/1000 | Loss: 0.00001924
Iteration 9/1000 | Loss: 0.00001899
Iteration 10/1000 | Loss: 0.00001887
Iteration 11/1000 | Loss: 0.00001872
Iteration 12/1000 | Loss: 0.00001860
Iteration 13/1000 | Loss: 0.00001859
Iteration 14/1000 | Loss: 0.00001849
Iteration 15/1000 | Loss: 0.00001841
Iteration 16/1000 | Loss: 0.00001837
Iteration 17/1000 | Loss: 0.00001837
Iteration 18/1000 | Loss: 0.00001836
Iteration 19/1000 | Loss: 0.00001836
Iteration 20/1000 | Loss: 0.00001836
Iteration 21/1000 | Loss: 0.00001836
Iteration 22/1000 | Loss: 0.00001836
Iteration 23/1000 | Loss: 0.00001836
Iteration 24/1000 | Loss: 0.00001836
Iteration 25/1000 | Loss: 0.00001836
Iteration 26/1000 | Loss: 0.00001835
Iteration 27/1000 | Loss: 0.00001832
Iteration 28/1000 | Loss: 0.00001832
Iteration 29/1000 | Loss: 0.00001831
Iteration 30/1000 | Loss: 0.00001831
Iteration 31/1000 | Loss: 0.00001831
Iteration 32/1000 | Loss: 0.00001831
Iteration 33/1000 | Loss: 0.00001830
Iteration 34/1000 | Loss: 0.00001827
Iteration 35/1000 | Loss: 0.00001826
Iteration 36/1000 | Loss: 0.00001825
Iteration 37/1000 | Loss: 0.00001823
Iteration 38/1000 | Loss: 0.00001822
Iteration 39/1000 | Loss: 0.00001821
Iteration 40/1000 | Loss: 0.00001819
Iteration 41/1000 | Loss: 0.00001818
Iteration 42/1000 | Loss: 0.00001816
Iteration 43/1000 | Loss: 0.00001814
Iteration 44/1000 | Loss: 0.00001812
Iteration 45/1000 | Loss: 0.00001811
Iteration 46/1000 | Loss: 0.00001810
Iteration 47/1000 | Loss: 0.00001810
Iteration 48/1000 | Loss: 0.00001809
Iteration 49/1000 | Loss: 0.00001809
Iteration 50/1000 | Loss: 0.00001809
Iteration 51/1000 | Loss: 0.00001808
Iteration 52/1000 | Loss: 0.00001808
Iteration 53/1000 | Loss: 0.00001808
Iteration 54/1000 | Loss: 0.00001808
Iteration 55/1000 | Loss: 0.00001808
Iteration 56/1000 | Loss: 0.00001808
Iteration 57/1000 | Loss: 0.00001808
Iteration 58/1000 | Loss: 0.00001807
Iteration 59/1000 | Loss: 0.00001804
Iteration 60/1000 | Loss: 0.00001803
Iteration 61/1000 | Loss: 0.00001803
Iteration 62/1000 | Loss: 0.00001802
Iteration 63/1000 | Loss: 0.00001802
Iteration 64/1000 | Loss: 0.00001799
Iteration 65/1000 | Loss: 0.00001799
Iteration 66/1000 | Loss: 0.00001799
Iteration 67/1000 | Loss: 0.00001799
Iteration 68/1000 | Loss: 0.00001799
Iteration 69/1000 | Loss: 0.00001799
Iteration 70/1000 | Loss: 0.00001799
Iteration 71/1000 | Loss: 0.00001799
Iteration 72/1000 | Loss: 0.00001799
Iteration 73/1000 | Loss: 0.00001799
Iteration 74/1000 | Loss: 0.00001798
Iteration 75/1000 | Loss: 0.00001798
Iteration 76/1000 | Loss: 0.00001798
Iteration 77/1000 | Loss: 0.00001798
Iteration 78/1000 | Loss: 0.00001797
Iteration 79/1000 | Loss: 0.00001797
Iteration 80/1000 | Loss: 0.00001797
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001796
Iteration 83/1000 | Loss: 0.00001795
Iteration 84/1000 | Loss: 0.00001795
Iteration 85/1000 | Loss: 0.00001795
Iteration 86/1000 | Loss: 0.00001794
Iteration 87/1000 | Loss: 0.00001794
Iteration 88/1000 | Loss: 0.00001793
Iteration 89/1000 | Loss: 0.00001793
Iteration 90/1000 | Loss: 0.00001792
Iteration 91/1000 | Loss: 0.00001792
Iteration 92/1000 | Loss: 0.00001792
Iteration 93/1000 | Loss: 0.00001791
Iteration 94/1000 | Loss: 0.00001791
Iteration 95/1000 | Loss: 0.00001791
Iteration 96/1000 | Loss: 0.00001790
Iteration 97/1000 | Loss: 0.00001790
Iteration 98/1000 | Loss: 0.00001790
Iteration 99/1000 | Loss: 0.00001789
Iteration 100/1000 | Loss: 0.00001789
Iteration 101/1000 | Loss: 0.00001788
Iteration 102/1000 | Loss: 0.00001788
Iteration 103/1000 | Loss: 0.00001788
Iteration 104/1000 | Loss: 0.00001788
Iteration 105/1000 | Loss: 0.00001788
Iteration 106/1000 | Loss: 0.00001787
Iteration 107/1000 | Loss: 0.00001787
Iteration 108/1000 | Loss: 0.00001787
Iteration 109/1000 | Loss: 0.00001787
Iteration 110/1000 | Loss: 0.00001787
Iteration 111/1000 | Loss: 0.00001787
Iteration 112/1000 | Loss: 0.00001787
Iteration 113/1000 | Loss: 0.00001787
Iteration 114/1000 | Loss: 0.00001786
Iteration 115/1000 | Loss: 0.00001786
Iteration 116/1000 | Loss: 0.00001786
Iteration 117/1000 | Loss: 0.00001785
Iteration 118/1000 | Loss: 0.00001785
Iteration 119/1000 | Loss: 0.00001785
Iteration 120/1000 | Loss: 0.00001785
Iteration 121/1000 | Loss: 0.00001785
Iteration 122/1000 | Loss: 0.00001784
Iteration 123/1000 | Loss: 0.00001784
Iteration 124/1000 | Loss: 0.00001784
Iteration 125/1000 | Loss: 0.00001784
Iteration 126/1000 | Loss: 0.00001784
Iteration 127/1000 | Loss: 0.00001784
Iteration 128/1000 | Loss: 0.00001784
Iteration 129/1000 | Loss: 0.00001783
Iteration 130/1000 | Loss: 0.00001783
Iteration 131/1000 | Loss: 0.00001783
Iteration 132/1000 | Loss: 0.00001783
Iteration 133/1000 | Loss: 0.00001782
Iteration 134/1000 | Loss: 0.00001782
Iteration 135/1000 | Loss: 0.00001782
Iteration 136/1000 | Loss: 0.00001782
Iteration 137/1000 | Loss: 0.00001782
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001782
Iteration 140/1000 | Loss: 0.00001782
Iteration 141/1000 | Loss: 0.00001781
Iteration 142/1000 | Loss: 0.00001781
Iteration 143/1000 | Loss: 0.00001781
Iteration 144/1000 | Loss: 0.00001781
Iteration 145/1000 | Loss: 0.00001781
Iteration 146/1000 | Loss: 0.00001781
Iteration 147/1000 | Loss: 0.00001781
Iteration 148/1000 | Loss: 0.00001781
Iteration 149/1000 | Loss: 0.00001781
Iteration 150/1000 | Loss: 0.00001781
Iteration 151/1000 | Loss: 0.00001781
Iteration 152/1000 | Loss: 0.00001781
Iteration 153/1000 | Loss: 0.00001781
Iteration 154/1000 | Loss: 0.00001781
Iteration 155/1000 | Loss: 0.00001781
Iteration 156/1000 | Loss: 0.00001781
Iteration 157/1000 | Loss: 0.00001781
Iteration 158/1000 | Loss: 0.00001781
Iteration 159/1000 | Loss: 0.00001780
Iteration 160/1000 | Loss: 0.00001780
Iteration 161/1000 | Loss: 0.00001780
Iteration 162/1000 | Loss: 0.00001780
Iteration 163/1000 | Loss: 0.00001780
Iteration 164/1000 | Loss: 0.00001780
Iteration 165/1000 | Loss: 0.00001780
Iteration 166/1000 | Loss: 0.00001780
Iteration 167/1000 | Loss: 0.00001780
Iteration 168/1000 | Loss: 0.00001780
Iteration 169/1000 | Loss: 0.00001780
Iteration 170/1000 | Loss: 0.00001780
Iteration 171/1000 | Loss: 0.00001780
Iteration 172/1000 | Loss: 0.00001780
Iteration 173/1000 | Loss: 0.00001780
Iteration 174/1000 | Loss: 0.00001780
Iteration 175/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.779930607881397e-05, 1.779930607881397e-05, 1.779930607881397e-05, 1.779930607881397e-05, 1.779930607881397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.779930607881397e-05

Optimization complete. Final v2v error: 3.5526978969573975 mm

Highest mean error: 3.707223653793335 mm for frame 204

Lowest mean error: 3.3938958644866943 mm for frame 74

Saving results

Total time: 44.165218353271484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01150880
Iteration 2/25 | Loss: 0.00289895
Iteration 3/25 | Loss: 0.00232252
Iteration 4/25 | Loss: 0.00213844
Iteration 5/25 | Loss: 0.00201700
Iteration 6/25 | Loss: 0.00198846
Iteration 7/25 | Loss: 0.00207575
Iteration 8/25 | Loss: 0.00221630
Iteration 9/25 | Loss: 0.00219038
Iteration 10/25 | Loss: 0.00201910
Iteration 11/25 | Loss: 0.00187949
Iteration 12/25 | Loss: 0.00181648
Iteration 13/25 | Loss: 0.00178000
Iteration 14/25 | Loss: 0.00177059
Iteration 15/25 | Loss: 0.00177133
Iteration 16/25 | Loss: 0.00176719
Iteration 17/25 | Loss: 0.00175808
Iteration 18/25 | Loss: 0.00174921
Iteration 19/25 | Loss: 0.00174755
Iteration 20/25 | Loss: 0.00174713
Iteration 21/25 | Loss: 0.00174696
Iteration 22/25 | Loss: 0.00174690
Iteration 23/25 | Loss: 0.00174690
Iteration 24/25 | Loss: 0.00174690
Iteration 25/25 | Loss: 0.00174690

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.35800824
Iteration 2/25 | Loss: 0.00134403
Iteration 3/25 | Loss: 0.00134403
Iteration 4/25 | Loss: 0.00134403
Iteration 5/25 | Loss: 0.00134403
Iteration 6/25 | Loss: 0.00134403
Iteration 7/25 | Loss: 0.00134403
Iteration 8/25 | Loss: 0.00134403
Iteration 9/25 | Loss: 0.00134403
Iteration 10/25 | Loss: 0.00134403
Iteration 11/25 | Loss: 0.00134403
Iteration 12/25 | Loss: 0.00134403
Iteration 13/25 | Loss: 0.00134403
Iteration 14/25 | Loss: 0.00134403
Iteration 15/25 | Loss: 0.00134403
Iteration 16/25 | Loss: 0.00134403
Iteration 17/25 | Loss: 0.00134403
Iteration 18/25 | Loss: 0.00134403
Iteration 19/25 | Loss: 0.00134403
Iteration 20/25 | Loss: 0.00134403
Iteration 21/25 | Loss: 0.00134403
Iteration 22/25 | Loss: 0.00134403
Iteration 23/25 | Loss: 0.00134403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001344028627499938, 0.001344028627499938, 0.001344028627499938, 0.001344028627499938, 0.001344028627499938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001344028627499938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134403
Iteration 2/1000 | Loss: 0.00010587
Iteration 3/1000 | Loss: 0.00007525
Iteration 4/1000 | Loss: 0.00006671
Iteration 5/1000 | Loss: 0.00006428
Iteration 6/1000 | Loss: 0.00006268
Iteration 7/1000 | Loss: 0.00006103
Iteration 8/1000 | Loss: 0.00005954
Iteration 9/1000 | Loss: 0.00005820
Iteration 10/1000 | Loss: 0.00005752
Iteration 11/1000 | Loss: 0.00005690
Iteration 12/1000 | Loss: 0.00005643
Iteration 13/1000 | Loss: 0.00005592
Iteration 14/1000 | Loss: 0.00005551
Iteration 15/1000 | Loss: 0.00005524
Iteration 16/1000 | Loss: 0.00005496
Iteration 17/1000 | Loss: 0.00005466
Iteration 18/1000 | Loss: 0.00005442
Iteration 19/1000 | Loss: 0.00005423
Iteration 20/1000 | Loss: 0.00005409
Iteration 21/1000 | Loss: 0.00005394
Iteration 22/1000 | Loss: 0.00005389
Iteration 23/1000 | Loss: 0.00005383
Iteration 24/1000 | Loss: 0.00005377
Iteration 25/1000 | Loss: 0.00005368
Iteration 26/1000 | Loss: 0.00005364
Iteration 27/1000 | Loss: 0.00005364
Iteration 28/1000 | Loss: 0.00005364
Iteration 29/1000 | Loss: 0.00005364
Iteration 30/1000 | Loss: 0.00005364
Iteration 31/1000 | Loss: 0.00005364
Iteration 32/1000 | Loss: 0.00005364
Iteration 33/1000 | Loss: 0.00005363
Iteration 34/1000 | Loss: 0.00005363
Iteration 35/1000 | Loss: 0.00005363
Iteration 36/1000 | Loss: 0.00005361
Iteration 37/1000 | Loss: 0.00005361
Iteration 38/1000 | Loss: 0.00005361
Iteration 39/1000 | Loss: 0.00005360
Iteration 40/1000 | Loss: 0.00005360
Iteration 41/1000 | Loss: 0.00005360
Iteration 42/1000 | Loss: 0.00005360
Iteration 43/1000 | Loss: 0.00005360
Iteration 44/1000 | Loss: 0.00005360
Iteration 45/1000 | Loss: 0.00005360
Iteration 46/1000 | Loss: 0.00005360
Iteration 47/1000 | Loss: 0.00005360
Iteration 48/1000 | Loss: 0.00005356
Iteration 49/1000 | Loss: 0.00005356
Iteration 50/1000 | Loss: 0.00005355
Iteration 51/1000 | Loss: 0.00005355
Iteration 52/1000 | Loss: 0.00005353
Iteration 53/1000 | Loss: 0.00005353
Iteration 54/1000 | Loss: 0.00005353
Iteration 55/1000 | Loss: 0.00005353
Iteration 56/1000 | Loss: 0.00005353
Iteration 57/1000 | Loss: 0.00005353
Iteration 58/1000 | Loss: 0.00005353
Iteration 59/1000 | Loss: 0.00005353
Iteration 60/1000 | Loss: 0.00005353
Iteration 61/1000 | Loss: 0.00005352
Iteration 62/1000 | Loss: 0.00005352
Iteration 63/1000 | Loss: 0.00005352
Iteration 64/1000 | Loss: 0.00005352
Iteration 65/1000 | Loss: 0.00005352
Iteration 66/1000 | Loss: 0.00005352
Iteration 67/1000 | Loss: 0.00005352
Iteration 68/1000 | Loss: 0.00005352
Iteration 69/1000 | Loss: 0.00005352
Iteration 70/1000 | Loss: 0.00005352
Iteration 71/1000 | Loss: 0.00005352
Iteration 72/1000 | Loss: 0.00005352
Iteration 73/1000 | Loss: 0.00005352
Iteration 74/1000 | Loss: 0.00005352
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 74. Stopping optimization.
Last 5 losses: [5.3521864174399525e-05, 5.3521864174399525e-05, 5.3521864174399525e-05, 5.3521864174399525e-05, 5.3521864174399525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.3521864174399525e-05

Optimization complete. Final v2v error: 6.005926609039307 mm

Highest mean error: 6.303958892822266 mm for frame 43

Lowest mean error: 5.806789875030518 mm for frame 166

Saving results

Total time: 85.19326400756836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986968
Iteration 2/25 | Loss: 0.00986968
Iteration 3/25 | Loss: 0.00398724
Iteration 4/25 | Loss: 0.00231728
Iteration 5/25 | Loss: 0.00201044
Iteration 6/25 | Loss: 0.00191790
Iteration 7/25 | Loss: 0.00196974
Iteration 8/25 | Loss: 0.00189018
Iteration 9/25 | Loss: 0.00173632
Iteration 10/25 | Loss: 0.00169060
Iteration 11/25 | Loss: 0.00168229
Iteration 12/25 | Loss: 0.00164370
Iteration 13/25 | Loss: 0.00162964
Iteration 14/25 | Loss: 0.00160998
Iteration 15/25 | Loss: 0.00159532
Iteration 16/25 | Loss: 0.00158270
Iteration 17/25 | Loss: 0.00157035
Iteration 18/25 | Loss: 0.00156410
Iteration 19/25 | Loss: 0.00155450
Iteration 20/25 | Loss: 0.00155270
Iteration 21/25 | Loss: 0.00155232
Iteration 22/25 | Loss: 0.00154966
Iteration 23/25 | Loss: 0.00154854
Iteration 24/25 | Loss: 0.00154947
Iteration 25/25 | Loss: 0.00154655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43734491
Iteration 2/25 | Loss: 0.00576884
Iteration 3/25 | Loss: 0.00378533
Iteration 4/25 | Loss: 0.00378533
Iteration 5/25 | Loss: 0.00378533
Iteration 6/25 | Loss: 0.00378533
Iteration 7/25 | Loss: 0.00378533
Iteration 8/25 | Loss: 0.00378533
Iteration 9/25 | Loss: 0.00378533
Iteration 10/25 | Loss: 0.00378533
Iteration 11/25 | Loss: 0.00378532
Iteration 12/25 | Loss: 0.00378532
Iteration 13/25 | Loss: 0.00378532
Iteration 14/25 | Loss: 0.00378532
Iteration 15/25 | Loss: 0.00378532
Iteration 16/25 | Loss: 0.00378532
Iteration 17/25 | Loss: 0.00378532
Iteration 18/25 | Loss: 0.00378532
Iteration 19/25 | Loss: 0.00378532
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0037853242829442024, 0.0037853242829442024, 0.0037853242829442024, 0.0037853242829442024, 0.0037853242829442024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0037853242829442024

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00378532
Iteration 2/1000 | Loss: 0.00341057
Iteration 3/1000 | Loss: 0.00053795
Iteration 4/1000 | Loss: 0.00053739
Iteration 5/1000 | Loss: 0.00055483
Iteration 6/1000 | Loss: 0.00038706
Iteration 7/1000 | Loss: 0.00067274
Iteration 8/1000 | Loss: 0.00076806
Iteration 9/1000 | Loss: 0.00026360
Iteration 10/1000 | Loss: 0.00078171
Iteration 11/1000 | Loss: 0.00056145
Iteration 12/1000 | Loss: 0.00092431
Iteration 13/1000 | Loss: 0.00045906
Iteration 14/1000 | Loss: 0.00022891
Iteration 15/1000 | Loss: 0.00085789
Iteration 16/1000 | Loss: 0.00219131
Iteration 17/1000 | Loss: 0.00025031
Iteration 18/1000 | Loss: 0.00021680
Iteration 19/1000 | Loss: 0.00020281
Iteration 20/1000 | Loss: 0.00020450
Iteration 21/1000 | Loss: 0.00016626
Iteration 22/1000 | Loss: 0.00017146
Iteration 23/1000 | Loss: 0.00095024
Iteration 24/1000 | Loss: 0.00061182
Iteration 25/1000 | Loss: 0.00047263
Iteration 26/1000 | Loss: 0.00053055
Iteration 27/1000 | Loss: 0.00019069
Iteration 28/1000 | Loss: 0.00018950
Iteration 29/1000 | Loss: 0.00030791
Iteration 30/1000 | Loss: 0.00017364
Iteration 31/1000 | Loss: 0.00013394
Iteration 32/1000 | Loss: 0.00030157
Iteration 33/1000 | Loss: 0.00020643
Iteration 34/1000 | Loss: 0.00014954
Iteration 35/1000 | Loss: 0.00016608
Iteration 36/1000 | Loss: 0.00070142
Iteration 37/1000 | Loss: 0.00035648
Iteration 38/1000 | Loss: 0.00059636
Iteration 39/1000 | Loss: 0.00032594
Iteration 40/1000 | Loss: 0.00023167
Iteration 41/1000 | Loss: 0.00018091
Iteration 42/1000 | Loss: 0.00081739
Iteration 43/1000 | Loss: 0.00059953
Iteration 44/1000 | Loss: 0.00018778
Iteration 45/1000 | Loss: 0.00043620
Iteration 46/1000 | Loss: 0.00063023
Iteration 47/1000 | Loss: 0.00060483
Iteration 48/1000 | Loss: 0.00063264
Iteration 49/1000 | Loss: 0.00020733
Iteration 50/1000 | Loss: 0.00028490
Iteration 51/1000 | Loss: 0.00058092
Iteration 52/1000 | Loss: 0.00048809
Iteration 53/1000 | Loss: 0.00048159
Iteration 54/1000 | Loss: 0.00016095
Iteration 55/1000 | Loss: 0.00013313
Iteration 56/1000 | Loss: 0.00014660
Iteration 57/1000 | Loss: 0.00015828
Iteration 58/1000 | Loss: 0.00047891
Iteration 59/1000 | Loss: 0.00027067
Iteration 60/1000 | Loss: 0.00016346
Iteration 61/1000 | Loss: 0.00016519
Iteration 62/1000 | Loss: 0.00053550
Iteration 63/1000 | Loss: 0.00027373
Iteration 64/1000 | Loss: 0.00016044
Iteration 65/1000 | Loss: 0.00015597
Iteration 66/1000 | Loss: 0.00058532
Iteration 67/1000 | Loss: 0.00036935
Iteration 68/1000 | Loss: 0.00016385
Iteration 69/1000 | Loss: 0.00016539
Iteration 70/1000 | Loss: 0.00015480
Iteration 71/1000 | Loss: 0.00024762
Iteration 72/1000 | Loss: 0.00022498
Iteration 73/1000 | Loss: 0.00016013
Iteration 74/1000 | Loss: 0.00016145
Iteration 75/1000 | Loss: 0.00056317
Iteration 76/1000 | Loss: 0.00016114
Iteration 77/1000 | Loss: 0.00012465
Iteration 78/1000 | Loss: 0.00014972
Iteration 79/1000 | Loss: 0.00012719
Iteration 80/1000 | Loss: 0.00011673
Iteration 81/1000 | Loss: 0.00010893
Iteration 82/1000 | Loss: 0.00015621
Iteration 83/1000 | Loss: 0.00014644
Iteration 84/1000 | Loss: 0.00013786
Iteration 85/1000 | Loss: 0.00052884
Iteration 86/1000 | Loss: 0.00027310
Iteration 87/1000 | Loss: 0.00057521
Iteration 88/1000 | Loss: 0.00013985
Iteration 89/1000 | Loss: 0.00013619
Iteration 90/1000 | Loss: 0.00012098
Iteration 91/1000 | Loss: 0.00012726
Iteration 92/1000 | Loss: 0.00011668
Iteration 93/1000 | Loss: 0.00011051
Iteration 94/1000 | Loss: 0.00011661
Iteration 95/1000 | Loss: 0.00010946
Iteration 96/1000 | Loss: 0.00011208
Iteration 97/1000 | Loss: 0.00012742
Iteration 98/1000 | Loss: 0.00013024
Iteration 99/1000 | Loss: 0.00012114
Iteration 100/1000 | Loss: 0.00012641
Iteration 101/1000 | Loss: 0.00012313
Iteration 102/1000 | Loss: 0.00012402
Iteration 103/1000 | Loss: 0.00011910
Iteration 104/1000 | Loss: 0.00012342
Iteration 105/1000 | Loss: 0.00053661
Iteration 106/1000 | Loss: 0.00011433
Iteration 107/1000 | Loss: 0.00010606
Iteration 108/1000 | Loss: 0.00011805
Iteration 109/1000 | Loss: 0.00011942
Iteration 110/1000 | Loss: 0.00011677
Iteration 111/1000 | Loss: 0.00012582
Iteration 112/1000 | Loss: 0.00011486
Iteration 113/1000 | Loss: 0.00010704
Iteration 114/1000 | Loss: 0.00011593
Iteration 115/1000 | Loss: 0.00012378
Iteration 116/1000 | Loss: 0.00010185
Iteration 117/1000 | Loss: 0.00012491
Iteration 118/1000 | Loss: 0.00009643
Iteration 119/1000 | Loss: 0.00009780
Iteration 120/1000 | Loss: 0.00010895
Iteration 121/1000 | Loss: 0.00010588
Iteration 122/1000 | Loss: 0.00010426
Iteration 123/1000 | Loss: 0.00009988
Iteration 124/1000 | Loss: 0.00013862
Iteration 125/1000 | Loss: 0.00010517
Iteration 126/1000 | Loss: 0.00009642
Iteration 127/1000 | Loss: 0.00010484
Iteration 128/1000 | Loss: 0.00009782
Iteration 129/1000 | Loss: 0.00010448
Iteration 130/1000 | Loss: 0.00010883
Iteration 131/1000 | Loss: 0.00010885
Iteration 132/1000 | Loss: 0.00010884
Iteration 133/1000 | Loss: 0.00010016
Iteration 134/1000 | Loss: 0.00010932
Iteration 135/1000 | Loss: 0.00009746
Iteration 136/1000 | Loss: 0.00009712
Iteration 137/1000 | Loss: 0.00010178
Iteration 138/1000 | Loss: 0.00013319
Iteration 139/1000 | Loss: 0.00010222
Iteration 140/1000 | Loss: 0.00012201
Iteration 141/1000 | Loss: 0.00010093
Iteration 142/1000 | Loss: 0.00011529
Iteration 143/1000 | Loss: 0.00010925
Iteration 144/1000 | Loss: 0.00010810
Iteration 145/1000 | Loss: 0.00009998
Iteration 146/1000 | Loss: 0.00011519
Iteration 147/1000 | Loss: 0.00009840
Iteration 148/1000 | Loss: 0.00011533
Iteration 149/1000 | Loss: 0.00011843
Iteration 150/1000 | Loss: 0.00009520
Iteration 151/1000 | Loss: 0.00011184
Iteration 152/1000 | Loss: 0.00011372
Iteration 153/1000 | Loss: 0.00010487
Iteration 154/1000 | Loss: 0.00011602
Iteration 155/1000 | Loss: 0.00010042
Iteration 156/1000 | Loss: 0.00011055
Iteration 157/1000 | Loss: 0.00010549
Iteration 158/1000 | Loss: 0.00011951
Iteration 159/1000 | Loss: 0.00010524
Iteration 160/1000 | Loss: 0.00012203
Iteration 161/1000 | Loss: 0.00011777
Iteration 162/1000 | Loss: 0.00010722
Iteration 163/1000 | Loss: 0.00011233
Iteration 164/1000 | Loss: 0.00010333
Iteration 165/1000 | Loss: 0.00011854
Iteration 166/1000 | Loss: 0.00010783
Iteration 167/1000 | Loss: 0.00012143
Iteration 168/1000 | Loss: 0.00014467
Iteration 169/1000 | Loss: 0.00010372
Iteration 170/1000 | Loss: 0.00010706
Iteration 171/1000 | Loss: 0.00012570
Iteration 172/1000 | Loss: 0.00017775
Iteration 173/1000 | Loss: 0.00010457
Iteration 174/1000 | Loss: 0.00009500
Iteration 175/1000 | Loss: 0.00010198
Iteration 176/1000 | Loss: 0.00027045
Iteration 177/1000 | Loss: 0.00015296
Iteration 178/1000 | Loss: 0.00010259
Iteration 179/1000 | Loss: 0.00011955
Iteration 180/1000 | Loss: 0.00010528
Iteration 181/1000 | Loss: 0.00009847
Iteration 182/1000 | Loss: 0.00015911
Iteration 183/1000 | Loss: 0.00009750
Iteration 184/1000 | Loss: 0.00009375
Iteration 185/1000 | Loss: 0.00009200
Iteration 186/1000 | Loss: 0.00009042
Iteration 187/1000 | Loss: 0.00020434
Iteration 188/1000 | Loss: 0.00010281
Iteration 189/1000 | Loss: 0.00009164
Iteration 190/1000 | Loss: 0.00009008
Iteration 191/1000 | Loss: 0.00008864
Iteration 192/1000 | Loss: 0.00022839
Iteration 193/1000 | Loss: 0.00014169
Iteration 194/1000 | Loss: 0.00020349
Iteration 195/1000 | Loss: 0.00011157
Iteration 196/1000 | Loss: 0.00008810
Iteration 197/1000 | Loss: 0.00017068
Iteration 198/1000 | Loss: 0.00043703
Iteration 199/1000 | Loss: 0.00031208
Iteration 200/1000 | Loss: 0.00053420
Iteration 201/1000 | Loss: 0.00011654
Iteration 202/1000 | Loss: 0.00010187
Iteration 203/1000 | Loss: 0.00036466
Iteration 204/1000 | Loss: 0.00008809
Iteration 205/1000 | Loss: 0.00008633
Iteration 206/1000 | Loss: 0.00008473
Iteration 207/1000 | Loss: 0.00008369
Iteration 208/1000 | Loss: 0.00008267
Iteration 209/1000 | Loss: 0.00032690
Iteration 210/1000 | Loss: 0.00041630
Iteration 211/1000 | Loss: 0.00010409
Iteration 212/1000 | Loss: 0.00009298
Iteration 213/1000 | Loss: 0.00008236
Iteration 214/1000 | Loss: 0.00032882
Iteration 215/1000 | Loss: 0.00018082
Iteration 216/1000 | Loss: 0.00030999
Iteration 217/1000 | Loss: 0.00020416
Iteration 218/1000 | Loss: 0.00029445
Iteration 219/1000 | Loss: 0.00008314
Iteration 220/1000 | Loss: 0.00011851
Iteration 221/1000 | Loss: 0.00008192
Iteration 222/1000 | Loss: 0.00008046
Iteration 223/1000 | Loss: 0.00007991
Iteration 224/1000 | Loss: 0.00007959
Iteration 225/1000 | Loss: 0.00010770
Iteration 226/1000 | Loss: 0.00007907
Iteration 227/1000 | Loss: 0.00007891
Iteration 228/1000 | Loss: 0.00007865
Iteration 229/1000 | Loss: 0.00007863
Iteration 230/1000 | Loss: 0.00007862
Iteration 231/1000 | Loss: 0.00007851
Iteration 232/1000 | Loss: 0.00029976
Iteration 233/1000 | Loss: 0.00078891
Iteration 234/1000 | Loss: 0.00029661
Iteration 235/1000 | Loss: 0.00011683
Iteration 236/1000 | Loss: 0.00047922
Iteration 237/1000 | Loss: 0.00009230
Iteration 238/1000 | Loss: 0.00008316
Iteration 239/1000 | Loss: 0.00007644
Iteration 240/1000 | Loss: 0.00011371
Iteration 241/1000 | Loss: 0.00007129
Iteration 242/1000 | Loss: 0.00007012
Iteration 243/1000 | Loss: 0.00006901
Iteration 244/1000 | Loss: 0.00012263
Iteration 245/1000 | Loss: 0.00006791
Iteration 246/1000 | Loss: 0.00006732
Iteration 247/1000 | Loss: 0.00006695
Iteration 248/1000 | Loss: 0.00009967
Iteration 249/1000 | Loss: 0.00006658
Iteration 250/1000 | Loss: 0.00006637
Iteration 251/1000 | Loss: 0.00006619
Iteration 252/1000 | Loss: 0.00009151
Iteration 253/1000 | Loss: 0.00006609
Iteration 254/1000 | Loss: 0.00006588
Iteration 255/1000 | Loss: 0.00008233
Iteration 256/1000 | Loss: 0.00006578
Iteration 257/1000 | Loss: 0.00006575
Iteration 258/1000 | Loss: 0.00006573
Iteration 259/1000 | Loss: 0.00006573
Iteration 260/1000 | Loss: 0.00006572
Iteration 261/1000 | Loss: 0.00006572
Iteration 262/1000 | Loss: 0.00006570
Iteration 263/1000 | Loss: 0.00010125
Iteration 264/1000 | Loss: 0.00006632
Iteration 265/1000 | Loss: 0.00006560
Iteration 266/1000 | Loss: 0.00006546
Iteration 267/1000 | Loss: 0.00006545
Iteration 268/1000 | Loss: 0.00009345
Iteration 269/1000 | Loss: 0.00014897
Iteration 270/1000 | Loss: 0.00054767
Iteration 271/1000 | Loss: 0.00013147
Iteration 272/1000 | Loss: 0.00008622
Iteration 273/1000 | Loss: 0.00006917
Iteration 274/1000 | Loss: 0.00015338
Iteration 275/1000 | Loss: 0.00020297
Iteration 276/1000 | Loss: 0.00047435
Iteration 277/1000 | Loss: 0.00024013
Iteration 278/1000 | Loss: 0.00007008
Iteration 279/1000 | Loss: 0.00010976
Iteration 280/1000 | Loss: 0.00008048
Iteration 281/1000 | Loss: 0.00006680
Iteration 282/1000 | Loss: 0.00006457
Iteration 283/1000 | Loss: 0.00008838
Iteration 284/1000 | Loss: 0.00009264
Iteration 285/1000 | Loss: 0.00027730
Iteration 286/1000 | Loss: 0.00031374
Iteration 287/1000 | Loss: 0.00015680
Iteration 288/1000 | Loss: 0.00007715
Iteration 289/1000 | Loss: 0.00006827
Iteration 290/1000 | Loss: 0.00034615
Iteration 291/1000 | Loss: 0.00006254
Iteration 292/1000 | Loss: 0.00014343
Iteration 293/1000 | Loss: 0.00008202
Iteration 294/1000 | Loss: 0.00005967
Iteration 295/1000 | Loss: 0.00005921
Iteration 296/1000 | Loss: 0.00010712
Iteration 297/1000 | Loss: 0.00005848
Iteration 298/1000 | Loss: 0.00005787
Iteration 299/1000 | Loss: 0.00013301
Iteration 300/1000 | Loss: 0.00008489
Iteration 301/1000 | Loss: 0.00008956
Iteration 302/1000 | Loss: 0.00005644
Iteration 303/1000 | Loss: 0.00026786
Iteration 304/1000 | Loss: 0.00020716
Iteration 305/1000 | Loss: 0.00006035
Iteration 306/1000 | Loss: 0.00048840
Iteration 307/1000 | Loss: 0.00034868
Iteration 308/1000 | Loss: 0.00050530
Iteration 309/1000 | Loss: 0.00029878
Iteration 310/1000 | Loss: 0.00014125
Iteration 311/1000 | Loss: 0.00006810
Iteration 312/1000 | Loss: 0.00027464
Iteration 313/1000 | Loss: 0.00008481
Iteration 314/1000 | Loss: 0.00028104
Iteration 315/1000 | Loss: 0.00042531
Iteration 316/1000 | Loss: 0.00093755
Iteration 317/1000 | Loss: 0.00076354
Iteration 318/1000 | Loss: 0.00020785
Iteration 319/1000 | Loss: 0.00134007
Iteration 320/1000 | Loss: 0.00025233
Iteration 321/1000 | Loss: 0.00056401
Iteration 322/1000 | Loss: 0.00048527
Iteration 323/1000 | Loss: 0.00006486
Iteration 324/1000 | Loss: 0.00005905
Iteration 325/1000 | Loss: 0.00037530
Iteration 326/1000 | Loss: 0.00027577
Iteration 327/1000 | Loss: 0.00024254
Iteration 328/1000 | Loss: 0.00008262
Iteration 329/1000 | Loss: 0.00063937
Iteration 330/1000 | Loss: 0.00074242
Iteration 331/1000 | Loss: 0.00089387
Iteration 332/1000 | Loss: 0.00080312
Iteration 333/1000 | Loss: 0.00028710
Iteration 334/1000 | Loss: 0.00007952
Iteration 335/1000 | Loss: 0.00026370
Iteration 336/1000 | Loss: 0.00021904
Iteration 337/1000 | Loss: 0.00026125
Iteration 338/1000 | Loss: 0.00039434
Iteration 339/1000 | Loss: 0.00006273
Iteration 340/1000 | Loss: 0.00024991
Iteration 341/1000 | Loss: 0.00012666
Iteration 342/1000 | Loss: 0.00007019
Iteration 343/1000 | Loss: 0.00006193
Iteration 344/1000 | Loss: 0.00005796
Iteration 345/1000 | Loss: 0.00005604
Iteration 346/1000 | Loss: 0.00005476
Iteration 347/1000 | Loss: 0.00005339
Iteration 348/1000 | Loss: 0.00005252
Iteration 349/1000 | Loss: 0.00005187
Iteration 350/1000 | Loss: 0.00005075
Iteration 351/1000 | Loss: 0.00006670
Iteration 352/1000 | Loss: 0.00005317
Iteration 353/1000 | Loss: 0.00004934
Iteration 354/1000 | Loss: 0.00004887
Iteration 355/1000 | Loss: 0.00028456
Iteration 356/1000 | Loss: 0.00021509
Iteration 357/1000 | Loss: 0.00027063
Iteration 358/1000 | Loss: 0.00071588
Iteration 359/1000 | Loss: 0.00047101
Iteration 360/1000 | Loss: 0.00015812
Iteration 361/1000 | Loss: 0.00017177
Iteration 362/1000 | Loss: 0.00073168
Iteration 363/1000 | Loss: 0.00032277
Iteration 364/1000 | Loss: 0.00028806
Iteration 365/1000 | Loss: 0.00008284
Iteration 366/1000 | Loss: 0.00005875
Iteration 367/1000 | Loss: 0.00008252
Iteration 368/1000 | Loss: 0.00036932
Iteration 369/1000 | Loss: 0.00004961
Iteration 370/1000 | Loss: 0.00004701
Iteration 371/1000 | Loss: 0.00004561
Iteration 372/1000 | Loss: 0.00004469
Iteration 373/1000 | Loss: 0.00004393
Iteration 374/1000 | Loss: 0.00004339
Iteration 375/1000 | Loss: 0.00004292
Iteration 376/1000 | Loss: 0.00027908
Iteration 377/1000 | Loss: 0.00021554
Iteration 378/1000 | Loss: 0.00021105
Iteration 379/1000 | Loss: 0.00025758
Iteration 380/1000 | Loss: 0.00007186
Iteration 381/1000 | Loss: 0.00004803
Iteration 382/1000 | Loss: 0.00011834
Iteration 383/1000 | Loss: 0.00004487
Iteration 384/1000 | Loss: 0.00006433
Iteration 385/1000 | Loss: 0.00004381
Iteration 386/1000 | Loss: 0.00013340
Iteration 387/1000 | Loss: 0.00024486
Iteration 388/1000 | Loss: 0.00021259
Iteration 389/1000 | Loss: 0.00025617
Iteration 390/1000 | Loss: 0.00038319
Iteration 391/1000 | Loss: 0.00004901
Iteration 392/1000 | Loss: 0.00005119
Iteration 393/1000 | Loss: 0.00004566
Iteration 394/1000 | Loss: 0.00017407
Iteration 395/1000 | Loss: 0.00007593
Iteration 396/1000 | Loss: 0.00006081
Iteration 397/1000 | Loss: 0.00004302
Iteration 398/1000 | Loss: 0.00011773
Iteration 399/1000 | Loss: 0.00038256
Iteration 400/1000 | Loss: 0.00006015
Iteration 401/1000 | Loss: 0.00006473
Iteration 402/1000 | Loss: 0.00021254
Iteration 403/1000 | Loss: 0.00004653
Iteration 404/1000 | Loss: 0.00004459
Iteration 405/1000 | Loss: 0.00012034
Iteration 406/1000 | Loss: 0.00012861
Iteration 407/1000 | Loss: 0.00005950
Iteration 408/1000 | Loss: 0.00011418
Iteration 409/1000 | Loss: 0.00026237
Iteration 410/1000 | Loss: 0.00020747
Iteration 411/1000 | Loss: 0.00009853
Iteration 412/1000 | Loss: 0.00004514
Iteration 413/1000 | Loss: 0.00011496
Iteration 414/1000 | Loss: 0.00004177
Iteration 415/1000 | Loss: 0.00038293
Iteration 416/1000 | Loss: 0.00014169
Iteration 417/1000 | Loss: 0.00025479
Iteration 418/1000 | Loss: 0.00007796
Iteration 419/1000 | Loss: 0.00004952
Iteration 420/1000 | Loss: 0.00013894
Iteration 421/1000 | Loss: 0.00005604
Iteration 422/1000 | Loss: 0.00004164
Iteration 423/1000 | Loss: 0.00010329
Iteration 424/1000 | Loss: 0.00011702
Iteration 425/1000 | Loss: 0.00004688
Iteration 426/1000 | Loss: 0.00004254
Iteration 427/1000 | Loss: 0.00004023
Iteration 428/1000 | Loss: 0.00003868
Iteration 429/1000 | Loss: 0.00003774
Iteration 430/1000 | Loss: 0.00006917
Iteration 431/1000 | Loss: 0.00003699
Iteration 432/1000 | Loss: 0.00003660
Iteration 433/1000 | Loss: 0.00003626
Iteration 434/1000 | Loss: 0.00007543
Iteration 435/1000 | Loss: 0.00050374
Iteration 436/1000 | Loss: 0.00007869
Iteration 437/1000 | Loss: 0.00019063
Iteration 438/1000 | Loss: 0.00004125
Iteration 439/1000 | Loss: 0.00003650
Iteration 440/1000 | Loss: 0.00003577
Iteration 441/1000 | Loss: 0.00003555
Iteration 442/1000 | Loss: 0.00006636
Iteration 443/1000 | Loss: 0.00026493
Iteration 444/1000 | Loss: 0.00019156
Iteration 445/1000 | Loss: 0.00030815
Iteration 446/1000 | Loss: 0.00027021
Iteration 447/1000 | Loss: 0.00018706
Iteration 448/1000 | Loss: 0.00026284
Iteration 449/1000 | Loss: 0.00006482
Iteration 450/1000 | Loss: 0.00007267
Iteration 451/1000 | Loss: 0.00004026
Iteration 452/1000 | Loss: 0.00003795
Iteration 453/1000 | Loss: 0.00003643
Iteration 454/1000 | Loss: 0.00003547
Iteration 455/1000 | Loss: 0.00029147
Iteration 456/1000 | Loss: 0.00003490
Iteration 457/1000 | Loss: 0.00003444
Iteration 458/1000 | Loss: 0.00003409
Iteration 459/1000 | Loss: 0.00014377
Iteration 460/1000 | Loss: 0.00016903
Iteration 461/1000 | Loss: 0.00048816
Iteration 462/1000 | Loss: 0.00003809
Iteration 463/1000 | Loss: 0.00005252
Iteration 464/1000 | Loss: 0.00004114
Iteration 465/1000 | Loss: 0.00003354
Iteration 466/1000 | Loss: 0.00026621
Iteration 467/1000 | Loss: 0.00022120
Iteration 468/1000 | Loss: 0.00016955
Iteration 469/1000 | Loss: 0.00006148
Iteration 470/1000 | Loss: 0.00003416
Iteration 471/1000 | Loss: 0.00003352
Iteration 472/1000 | Loss: 0.00003293
Iteration 473/1000 | Loss: 0.00006939
Iteration 474/1000 | Loss: 0.00003223
Iteration 475/1000 | Loss: 0.00003184
Iteration 476/1000 | Loss: 0.00025959
Iteration 477/1000 | Loss: 0.00019417
Iteration 478/1000 | Loss: 0.00027370
Iteration 479/1000 | Loss: 0.00003652
Iteration 480/1000 | Loss: 0.00003403
Iteration 481/1000 | Loss: 0.00003342
Iteration 482/1000 | Loss: 0.00027311
Iteration 483/1000 | Loss: 0.00023653
Iteration 484/1000 | Loss: 0.00020734
Iteration 485/1000 | Loss: 0.00003605
Iteration 486/1000 | Loss: 0.00012344
Iteration 487/1000 | Loss: 0.00003449
Iteration 488/1000 | Loss: 0.00005739
Iteration 489/1000 | Loss: 0.00006438
Iteration 490/1000 | Loss: 0.00022623
Iteration 491/1000 | Loss: 0.00003903
Iteration 492/1000 | Loss: 0.00003801
Iteration 493/1000 | Loss: 0.00004881
Iteration 494/1000 | Loss: 0.00026499
Iteration 495/1000 | Loss: 0.00019374
Iteration 496/1000 | Loss: 0.00019140
Iteration 497/1000 | Loss: 0.00016926
Iteration 498/1000 | Loss: 0.00004962
Iteration 499/1000 | Loss: 0.00010990
Iteration 500/1000 | Loss: 0.00004407
Iteration 501/1000 | Loss: 0.00005566
Iteration 502/1000 | Loss: 0.00010564
Iteration 503/1000 | Loss: 0.00003872
Iteration 504/1000 | Loss: 0.00005340
Iteration 505/1000 | Loss: 0.00003290
Iteration 506/1000 | Loss: 0.00027715
Iteration 507/1000 | Loss: 0.00011390
Iteration 508/1000 | Loss: 0.00006753
Iteration 509/1000 | Loss: 0.00003245
Iteration 510/1000 | Loss: 0.00026007
Iteration 511/1000 | Loss: 0.00014211
Iteration 512/1000 | Loss: 0.00025416
Iteration 513/1000 | Loss: 0.00012006
Iteration 514/1000 | Loss: 0.00007044
Iteration 515/1000 | Loss: 0.00007385
Iteration 516/1000 | Loss: 0.00003835
Iteration 517/1000 | Loss: 0.00003857
Iteration 518/1000 | Loss: 0.00003209
Iteration 519/1000 | Loss: 0.00003483
Iteration 520/1000 | Loss: 0.00003104
Iteration 521/1000 | Loss: 0.00003068
Iteration 522/1000 | Loss: 0.00003058
Iteration 523/1000 | Loss: 0.00003051
Iteration 524/1000 | Loss: 0.00005196
Iteration 525/1000 | Loss: 0.00006657
Iteration 526/1000 | Loss: 0.00003029
Iteration 527/1000 | Loss: 0.00003020
Iteration 528/1000 | Loss: 0.00003013
Iteration 529/1000 | Loss: 0.00030133
Iteration 530/1000 | Loss: 0.00019141
Iteration 531/1000 | Loss: 0.00026046
Iteration 532/1000 | Loss: 0.00018779
Iteration 533/1000 | Loss: 0.00003028
Iteration 534/1000 | Loss: 0.00028361
Iteration 535/1000 | Loss: 0.00018516
Iteration 536/1000 | Loss: 0.00031247
Iteration 537/1000 | Loss: 0.00018094
Iteration 538/1000 | Loss: 0.00024998
Iteration 539/1000 | Loss: 0.00019357
Iteration 540/1000 | Loss: 0.00003078
Iteration 541/1000 | Loss: 0.00023909
Iteration 542/1000 | Loss: 0.00019027
Iteration 543/1000 | Loss: 0.00039974
Iteration 544/1000 | Loss: 0.00003211
Iteration 545/1000 | Loss: 0.00024679
Iteration 546/1000 | Loss: 0.00018341
Iteration 547/1000 | Loss: 0.00018928
Iteration 548/1000 | Loss: 0.00017165
Iteration 549/1000 | Loss: 0.00010433
Iteration 550/1000 | Loss: 0.00003934
Iteration 551/1000 | Loss: 0.00020051
Iteration 552/1000 | Loss: 0.00003462
Iteration 553/1000 | Loss: 0.00003031
Iteration 554/1000 | Loss: 0.00003915
Iteration 555/1000 | Loss: 0.00023468
Iteration 556/1000 | Loss: 0.00028636
Iteration 557/1000 | Loss: 0.00004154
Iteration 558/1000 | Loss: 0.00011776
Iteration 559/1000 | Loss: 0.00003361
Iteration 560/1000 | Loss: 0.00003559
Iteration 561/1000 | Loss: 0.00003194
Iteration 562/1000 | Loss: 0.00012951
Iteration 563/1000 | Loss: 0.00004267
Iteration 564/1000 | Loss: 0.00003116
Iteration 565/1000 | Loss: 0.00003095
Iteration 566/1000 | Loss: 0.00003070
Iteration 567/1000 | Loss: 0.00029745
Iteration 568/1000 | Loss: 0.00019638
Iteration 569/1000 | Loss: 0.00023930
Iteration 570/1000 | Loss: 0.00020654
Iteration 571/1000 | Loss: 0.00006727
Iteration 572/1000 | Loss: 0.00003099
Iteration 573/1000 | Loss: 0.00004257
Iteration 574/1000 | Loss: 0.00028642
Iteration 575/1000 | Loss: 0.00017804
Iteration 576/1000 | Loss: 0.00016628
Iteration 577/1000 | Loss: 0.00004242
Iteration 578/1000 | Loss: 0.00003304
Iteration 579/1000 | Loss: 0.00003244
Iteration 580/1000 | Loss: 0.00004097
Iteration 581/1000 | Loss: 0.00003137
Iteration 582/1000 | Loss: 0.00004785
Iteration 583/1000 | Loss: 0.00003091
Iteration 584/1000 | Loss: 0.00003069
Iteration 585/1000 | Loss: 0.00003041
Iteration 586/1000 | Loss: 0.00007682
Iteration 587/1000 | Loss: 0.00003027
Iteration 588/1000 | Loss: 0.00002985
Iteration 589/1000 | Loss: 0.00026591
Iteration 590/1000 | Loss: 0.00018160
Iteration 591/1000 | Loss: 0.00003041
Iteration 592/1000 | Loss: 0.00023286
Iteration 593/1000 | Loss: 0.00003293
Iteration 594/1000 | Loss: 0.00004099
Iteration 595/1000 | Loss: 0.00004133
Iteration 596/1000 | Loss: 0.00003102
Iteration 597/1000 | Loss: 0.00008437
Iteration 598/1000 | Loss: 0.00003610
Iteration 599/1000 | Loss: 0.00043893
Iteration 600/1000 | Loss: 0.00033496
Iteration 601/1000 | Loss: 0.00039627
Iteration 602/1000 | Loss: 0.00004542
Iteration 603/1000 | Loss: 0.00003500
Iteration 604/1000 | Loss: 0.00003391
Iteration 605/1000 | Loss: 0.00017440
Iteration 606/1000 | Loss: 0.00004987
Iteration 607/1000 | Loss: 0.00004011
Iteration 608/1000 | Loss: 0.00009780
Iteration 609/1000 | Loss: 0.00005311
Iteration 610/1000 | Loss: 0.00005013
Iteration 611/1000 | Loss: 0.00005523
Iteration 612/1000 | Loss: 0.00003222
Iteration 613/1000 | Loss: 0.00003183
Iteration 614/1000 | Loss: 0.00003149
Iteration 615/1000 | Loss: 0.00006371
Iteration 616/1000 | Loss: 0.00025222
Iteration 617/1000 | Loss: 0.00003566
Iteration 618/1000 | Loss: 0.00003327
Iteration 619/1000 | Loss: 0.00003264
Iteration 620/1000 | Loss: 0.00007588
Iteration 621/1000 | Loss: 0.00006262
Iteration 622/1000 | Loss: 0.00003206
Iteration 623/1000 | Loss: 0.00009435
Iteration 624/1000 | Loss: 0.00003315
Iteration 625/1000 | Loss: 0.00003180
Iteration 626/1000 | Loss: 0.00003166
Iteration 627/1000 | Loss: 0.00005643
Iteration 628/1000 | Loss: 0.00003158
Iteration 629/1000 | Loss: 0.00003130
Iteration 630/1000 | Loss: 0.00011183
Iteration 631/1000 | Loss: 0.00003200
Iteration 632/1000 | Loss: 0.00003114
Iteration 633/1000 | Loss: 0.00003078
Iteration 634/1000 | Loss: 0.00003042
Iteration 635/1000 | Loss: 0.00009092
Iteration 636/1000 | Loss: 0.00002986
Iteration 637/1000 | Loss: 0.00024473
Iteration 638/1000 | Loss: 0.00016302
Iteration 639/1000 | Loss: 0.00025571
Iteration 640/1000 | Loss: 0.00015056
Iteration 641/1000 | Loss: 0.00021707
Iteration 642/1000 | Loss: 0.00016642
Iteration 643/1000 | Loss: 0.00004799
Iteration 644/1000 | Loss: 0.00003562
Iteration 645/1000 | Loss: 0.00004047
Iteration 646/1000 | Loss: 0.00005489
Iteration 647/1000 | Loss: 0.00003173
Iteration 648/1000 | Loss: 0.00003100
Iteration 649/1000 | Loss: 0.00003057
Iteration 650/1000 | Loss: 0.00003019
Iteration 651/1000 | Loss: 0.00004665
Iteration 652/1000 | Loss: 0.00003603
Iteration 653/1000 | Loss: 0.00002988
Iteration 654/1000 | Loss: 0.00003117
Iteration 655/1000 | Loss: 0.00002937
Iteration 656/1000 | Loss: 0.00047977
Iteration 657/1000 | Loss: 0.00060714
Iteration 658/1000 | Loss: 0.00030917
Iteration 659/1000 | Loss: 0.00037496
Iteration 660/1000 | Loss: 0.00004963
Iteration 661/1000 | Loss: 0.00003220
Iteration 662/1000 | Loss: 0.00003039
Iteration 663/1000 | Loss: 0.00020560
Iteration 664/1000 | Loss: 0.00002919
Iteration 665/1000 | Loss: 0.00011438
Iteration 666/1000 | Loss: 0.00002874
Iteration 667/1000 | Loss: 0.00002844
Iteration 668/1000 | Loss: 0.00002808
Iteration 669/1000 | Loss: 0.00002779
Iteration 670/1000 | Loss: 0.00026592
Iteration 671/1000 | Loss: 0.00018339
Iteration 672/1000 | Loss: 0.00025138
Iteration 673/1000 | Loss: 0.00015934
Iteration 674/1000 | Loss: 0.00005836
Iteration 675/1000 | Loss: 0.00003526
Iteration 676/1000 | Loss: 0.00053262
Iteration 677/1000 | Loss: 0.00012514
Iteration 678/1000 | Loss: 0.00003621
Iteration 679/1000 | Loss: 0.00002986
Iteration 680/1000 | Loss: 0.00002665
Iteration 681/1000 | Loss: 0.00002588
Iteration 682/1000 | Loss: 0.00002543
Iteration 683/1000 | Loss: 0.00002515
Iteration 684/1000 | Loss: 0.00002499
Iteration 685/1000 | Loss: 0.00002496
Iteration 686/1000 | Loss: 0.00002481
Iteration 687/1000 | Loss: 0.00002476
Iteration 688/1000 | Loss: 0.00002476
Iteration 689/1000 | Loss: 0.00005927
Iteration 690/1000 | Loss: 0.00002476
Iteration 691/1000 | Loss: 0.00002461
Iteration 692/1000 | Loss: 0.00002460
Iteration 693/1000 | Loss: 0.00002460
Iteration 694/1000 | Loss: 0.00002460
Iteration 695/1000 | Loss: 0.00002460
Iteration 696/1000 | Loss: 0.00002458
Iteration 697/1000 | Loss: 0.00002457
Iteration 698/1000 | Loss: 0.00002457
Iteration 699/1000 | Loss: 0.00002457
Iteration 700/1000 | Loss: 0.00002457
Iteration 701/1000 | Loss: 0.00002457
Iteration 702/1000 | Loss: 0.00002456
Iteration 703/1000 | Loss: 0.00002456
Iteration 704/1000 | Loss: 0.00002456
Iteration 705/1000 | Loss: 0.00002456
Iteration 706/1000 | Loss: 0.00002455
Iteration 707/1000 | Loss: 0.00002455
Iteration 708/1000 | Loss: 0.00002455
Iteration 709/1000 | Loss: 0.00002455
Iteration 710/1000 | Loss: 0.00002455
Iteration 711/1000 | Loss: 0.00002455
Iteration 712/1000 | Loss: 0.00002455
Iteration 713/1000 | Loss: 0.00002455
Iteration 714/1000 | Loss: 0.00002455
Iteration 715/1000 | Loss: 0.00002455
Iteration 716/1000 | Loss: 0.00002455
Iteration 717/1000 | Loss: 0.00002455
Iteration 718/1000 | Loss: 0.00002454
Iteration 719/1000 | Loss: 0.00002454
Iteration 720/1000 | Loss: 0.00002454
Iteration 721/1000 | Loss: 0.00002454
Iteration 722/1000 | Loss: 0.00002454
Iteration 723/1000 | Loss: 0.00002453
Iteration 724/1000 | Loss: 0.00002453
Iteration 725/1000 | Loss: 0.00002452
Iteration 726/1000 | Loss: 0.00002451
Iteration 727/1000 | Loss: 0.00002450
Iteration 728/1000 | Loss: 0.00002450
Iteration 729/1000 | Loss: 0.00005264
Iteration 730/1000 | Loss: 0.00002456
Iteration 731/1000 | Loss: 0.00002445
Iteration 732/1000 | Loss: 0.00002445
Iteration 733/1000 | Loss: 0.00002444
Iteration 734/1000 | Loss: 0.00002444
Iteration 735/1000 | Loss: 0.00002444
Iteration 736/1000 | Loss: 0.00002444
Iteration 737/1000 | Loss: 0.00002444
Iteration 738/1000 | Loss: 0.00002444
Iteration 739/1000 | Loss: 0.00002444
Iteration 740/1000 | Loss: 0.00002444
Iteration 741/1000 | Loss: 0.00002444
Iteration 742/1000 | Loss: 0.00002444
Iteration 743/1000 | Loss: 0.00002444
Iteration 744/1000 | Loss: 0.00002443
Iteration 745/1000 | Loss: 0.00002443
Iteration 746/1000 | Loss: 0.00002443
Iteration 747/1000 | Loss: 0.00002443
Iteration 748/1000 | Loss: 0.00002443
Iteration 749/1000 | Loss: 0.00002443
Iteration 750/1000 | Loss: 0.00002443
Iteration 751/1000 | Loss: 0.00002443
Iteration 752/1000 | Loss: 0.00002443
Iteration 753/1000 | Loss: 0.00002443
Iteration 754/1000 | Loss: 0.00002442
Iteration 755/1000 | Loss: 0.00002442
Iteration 756/1000 | Loss: 0.00002442
Iteration 757/1000 | Loss: 0.00002442
Iteration 758/1000 | Loss: 0.00002442
Iteration 759/1000 | Loss: 0.00002441
Iteration 760/1000 | Loss: 0.00002441
Iteration 761/1000 | Loss: 0.00002441
Iteration 762/1000 | Loss: 0.00002441
Iteration 763/1000 | Loss: 0.00002440
Iteration 764/1000 | Loss: 0.00002440
Iteration 765/1000 | Loss: 0.00002440
Iteration 766/1000 | Loss: 0.00002440
Iteration 767/1000 | Loss: 0.00002440
Iteration 768/1000 | Loss: 0.00002440
Iteration 769/1000 | Loss: 0.00002439
Iteration 770/1000 | Loss: 0.00002439
Iteration 771/1000 | Loss: 0.00002439
Iteration 772/1000 | Loss: 0.00002439
Iteration 773/1000 | Loss: 0.00002439
Iteration 774/1000 | Loss: 0.00002438
Iteration 775/1000 | Loss: 0.00002438
Iteration 776/1000 | Loss: 0.00002438
Iteration 777/1000 | Loss: 0.00002438
Iteration 778/1000 | Loss: 0.00002438
Iteration 779/1000 | Loss: 0.00002438
Iteration 780/1000 | Loss: 0.00002438
Iteration 781/1000 | Loss: 0.00002438
Iteration 782/1000 | Loss: 0.00002438
Iteration 783/1000 | Loss: 0.00002438
Iteration 784/1000 | Loss: 0.00002437
Iteration 785/1000 | Loss: 0.00002437
Iteration 786/1000 | Loss: 0.00002437
Iteration 787/1000 | Loss: 0.00002437
Iteration 788/1000 | Loss: 0.00002437
Iteration 789/1000 | Loss: 0.00002437
Iteration 790/1000 | Loss: 0.00002437
Iteration 791/1000 | Loss: 0.00002436
Iteration 792/1000 | Loss: 0.00002436
Iteration 793/1000 | Loss: 0.00002436
Iteration 794/1000 | Loss: 0.00002436
Iteration 795/1000 | Loss: 0.00002435
Iteration 796/1000 | Loss: 0.00002435
Iteration 797/1000 | Loss: 0.00002435
Iteration 798/1000 | Loss: 0.00002435
Iteration 799/1000 | Loss: 0.00002435
Iteration 800/1000 | Loss: 0.00002435
Iteration 801/1000 | Loss: 0.00002435
Iteration 802/1000 | Loss: 0.00002434
Iteration 803/1000 | Loss: 0.00002434
Iteration 804/1000 | Loss: 0.00002434
Iteration 805/1000 | Loss: 0.00002434
Iteration 806/1000 | Loss: 0.00002434
Iteration 807/1000 | Loss: 0.00002433
Iteration 808/1000 | Loss: 0.00002433
Iteration 809/1000 | Loss: 0.00002433
Iteration 810/1000 | Loss: 0.00006105
Iteration 811/1000 | Loss: 0.00002873
Iteration 812/1000 | Loss: 0.00002460
Iteration 813/1000 | Loss: 0.00002435
Iteration 814/1000 | Loss: 0.00002435
Iteration 815/1000 | Loss: 0.00002435
Iteration 816/1000 | Loss: 0.00002435
Iteration 817/1000 | Loss: 0.00002435
Iteration 818/1000 | Loss: 0.00002435
Iteration 819/1000 | Loss: 0.00002433
Iteration 820/1000 | Loss: 0.00002432
Iteration 821/1000 | Loss: 0.00002431
Iteration 822/1000 | Loss: 0.00002431
Iteration 823/1000 | Loss: 0.00002430
Iteration 824/1000 | Loss: 0.00002429
Iteration 825/1000 | Loss: 0.00002429
Iteration 826/1000 | Loss: 0.00002429
Iteration 827/1000 | Loss: 0.00002429
Iteration 828/1000 | Loss: 0.00002428
Iteration 829/1000 | Loss: 0.00002428
Iteration 830/1000 | Loss: 0.00002428
Iteration 831/1000 | Loss: 0.00002428
Iteration 832/1000 | Loss: 0.00002428
Iteration 833/1000 | Loss: 0.00002428
Iteration 834/1000 | Loss: 0.00002428
Iteration 835/1000 | Loss: 0.00002427
Iteration 836/1000 | Loss: 0.00002427
Iteration 837/1000 | Loss: 0.00002426
Iteration 838/1000 | Loss: 0.00002425
Iteration 839/1000 | Loss: 0.00002425
Iteration 840/1000 | Loss: 0.00002424
Iteration 841/1000 | Loss: 0.00002424
Iteration 842/1000 | Loss: 0.00002423
Iteration 843/1000 | Loss: 0.00003338
Iteration 844/1000 | Loss: 0.00002453
Iteration 845/1000 | Loss: 0.00002533
Iteration 846/1000 | Loss: 0.00002411
Iteration 847/1000 | Loss: 0.00002411
Iteration 848/1000 | Loss: 0.00002411
Iteration 849/1000 | Loss: 0.00002411
Iteration 850/1000 | Loss: 0.00002411
Iteration 851/1000 | Loss: 0.00002411
Iteration 852/1000 | Loss: 0.00002411
Iteration 853/1000 | Loss: 0.00002410
Iteration 854/1000 | Loss: 0.00002410
Iteration 855/1000 | Loss: 0.00002410
Iteration 856/1000 | Loss: 0.00002410
Iteration 857/1000 | Loss: 0.00002410
Iteration 858/1000 | Loss: 0.00002410
Iteration 859/1000 | Loss: 0.00002410
Iteration 860/1000 | Loss: 0.00002396
Iteration 861/1000 | Loss: 0.00002394
Iteration 862/1000 | Loss: 0.00002387
Iteration 863/1000 | Loss: 0.00002383
Iteration 864/1000 | Loss: 0.00002382
Iteration 865/1000 | Loss: 0.00002382
Iteration 866/1000 | Loss: 0.00002381
Iteration 867/1000 | Loss: 0.00002381
Iteration 868/1000 | Loss: 0.00002379
Iteration 869/1000 | Loss: 0.00002377
Iteration 870/1000 | Loss: 0.00002376
Iteration 871/1000 | Loss: 0.00030150
Iteration 872/1000 | Loss: 0.00003487
Iteration 873/1000 | Loss: 0.00016805
Iteration 874/1000 | Loss: 0.00002561
Iteration 875/1000 | Loss: 0.00008620
Iteration 876/1000 | Loss: 0.00002375
Iteration 877/1000 | Loss: 0.00002317
Iteration 878/1000 | Loss: 0.00002304
Iteration 879/1000 | Loss: 0.00002293
Iteration 880/1000 | Loss: 0.00002292
Iteration 881/1000 | Loss: 0.00004736
Iteration 882/1000 | Loss: 0.00002319
Iteration 883/1000 | Loss: 0.00002282
Iteration 884/1000 | Loss: 0.00002281
Iteration 885/1000 | Loss: 0.00002281
Iteration 886/1000 | Loss: 0.00002280
Iteration 887/1000 | Loss: 0.00002280
Iteration 888/1000 | Loss: 0.00002280
Iteration 889/1000 | Loss: 0.00002280
Iteration 890/1000 | Loss: 0.00002280
Iteration 891/1000 | Loss: 0.00002280
Iteration 892/1000 | Loss: 0.00002280
Iteration 893/1000 | Loss: 0.00002280
Iteration 894/1000 | Loss: 0.00002280
Iteration 895/1000 | Loss: 0.00002280
Iteration 896/1000 | Loss: 0.00002280
Iteration 897/1000 | Loss: 0.00002280
Iteration 898/1000 | Loss: 0.00002280
Iteration 899/1000 | Loss: 0.00002280
Iteration 900/1000 | Loss: 0.00002279
Iteration 901/1000 | Loss: 0.00002279
Iteration 902/1000 | Loss: 0.00002279
Iteration 903/1000 | Loss: 0.00002279
Iteration 904/1000 | Loss: 0.00002279
Iteration 905/1000 | Loss: 0.00002279
Iteration 906/1000 | Loss: 0.00002279
Iteration 907/1000 | Loss: 0.00002279
Iteration 908/1000 | Loss: 0.00002279
Iteration 909/1000 | Loss: 0.00002279
Iteration 910/1000 | Loss: 0.00002279
Iteration 911/1000 | Loss: 0.00002279
Iteration 912/1000 | Loss: 0.00002279
Iteration 913/1000 | Loss: 0.00002279
Iteration 914/1000 | Loss: 0.00002279
Iteration 915/1000 | Loss: 0.00002279
Iteration 916/1000 | Loss: 0.00002279
Iteration 917/1000 | Loss: 0.00002278
Iteration 918/1000 | Loss: 0.00002278
Iteration 919/1000 | Loss: 0.00002278
Iteration 920/1000 | Loss: 0.00002278
Iteration 921/1000 | Loss: 0.00002278
Iteration 922/1000 | Loss: 0.00002278
Iteration 923/1000 | Loss: 0.00002278
Iteration 924/1000 | Loss: 0.00002278
Iteration 925/1000 | Loss: 0.00002278
Iteration 926/1000 | Loss: 0.00002278
Iteration 927/1000 | Loss: 0.00002278
Iteration 928/1000 | Loss: 0.00002278
Iteration 929/1000 | Loss: 0.00002278
Iteration 930/1000 | Loss: 0.00002278
Iteration 931/1000 | Loss: 0.00002278
Iteration 932/1000 | Loss: 0.00002278
Iteration 933/1000 | Loss: 0.00002277
Iteration 934/1000 | Loss: 0.00002277
Iteration 935/1000 | Loss: 0.00002277
Iteration 936/1000 | Loss: 0.00002277
Iteration 937/1000 | Loss: 0.00002277
Iteration 938/1000 | Loss: 0.00002277
Iteration 939/1000 | Loss: 0.00002277
Iteration 940/1000 | Loss: 0.00002277
Iteration 941/1000 | Loss: 0.00002277
Iteration 942/1000 | Loss: 0.00002277
Iteration 943/1000 | Loss: 0.00002277
Iteration 944/1000 | Loss: 0.00002277
Iteration 945/1000 | Loss: 0.00002277
Iteration 946/1000 | Loss: 0.00002277
Iteration 947/1000 | Loss: 0.00002277
Iteration 948/1000 | Loss: 0.00002277
Iteration 949/1000 | Loss: 0.00002277
Iteration 950/1000 | Loss: 0.00002277
Iteration 951/1000 | Loss: 0.00002277
Iteration 952/1000 | Loss: 0.00002277
Iteration 953/1000 | Loss: 0.00002276
Iteration 954/1000 | Loss: 0.00002276
Iteration 955/1000 | Loss: 0.00002276
Iteration 956/1000 | Loss: 0.00002276
Iteration 957/1000 | Loss: 0.00002276
Iteration 958/1000 | Loss: 0.00002276
Iteration 959/1000 | Loss: 0.00002276
Iteration 960/1000 | Loss: 0.00002276
Iteration 961/1000 | Loss: 0.00002275
Iteration 962/1000 | Loss: 0.00002275
Iteration 963/1000 | Loss: 0.00002275
Iteration 964/1000 | Loss: 0.00002275
Iteration 965/1000 | Loss: 0.00002274
Iteration 966/1000 | Loss: 0.00002274
Iteration 967/1000 | Loss: 0.00002274
Iteration 968/1000 | Loss: 0.00002274
Iteration 969/1000 | Loss: 0.00002273
Iteration 970/1000 | Loss: 0.00002273
Iteration 971/1000 | Loss: 0.00002273
Iteration 972/1000 | Loss: 0.00002273
Iteration 973/1000 | Loss: 0.00002273
Iteration 974/1000 | Loss: 0.00002273
Iteration 975/1000 | Loss: 0.00002273
Iteration 976/1000 | Loss: 0.00002273
Iteration 977/1000 | Loss: 0.00002273
Iteration 978/1000 | Loss: 0.00002273
Iteration 979/1000 | Loss: 0.00002273
Iteration 980/1000 | Loss: 0.00002273
Iteration 981/1000 | Loss: 0.00002273
Iteration 982/1000 | Loss: 0.00002273
Iteration 983/1000 | Loss: 0.00002273
Iteration 984/1000 | Loss: 0.00002273
Iteration 985/1000 | Loss: 0.00002272
Iteration 986/1000 | Loss: 0.00002272
Iteration 987/1000 | Loss: 0.00002272
Iteration 988/1000 | Loss: 0.00002272
Iteration 989/1000 | Loss: 0.00002272
Iteration 990/1000 | Loss: 0.00002272
Iteration 991/1000 | Loss: 0.00002272
Iteration 992/1000 | Loss: 0.00002272
Iteration 993/1000 | Loss: 0.00002272
Iteration 994/1000 | Loss: 0.00002272
Iteration 995/1000 | Loss: 0.00002272
Iteration 996/1000 | Loss: 0.00002272
Iteration 997/1000 | Loss: 0.00002272
Iteration 998/1000 | Loss: 0.00002272
Iteration 999/1000 | Loss: 0.00005791
Iteration 1000/1000 | Loss: 0.00002285

Optimization complete. Final v2v error: 3.456068754196167 mm

Highest mean error: 10.66693115234375 mm for frame 35

Lowest mean error: 2.8978331089019775 mm for frame 71

Saving results

Total time: 1155.2286517620087
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435713
Iteration 2/25 | Loss: 0.00132101
Iteration 3/25 | Loss: 0.00126396
Iteration 4/25 | Loss: 0.00125480
Iteration 5/25 | Loss: 0.00125308
Iteration 6/25 | Loss: 0.00125278
Iteration 7/25 | Loss: 0.00125278
Iteration 8/25 | Loss: 0.00125278
Iteration 9/25 | Loss: 0.00125278
Iteration 10/25 | Loss: 0.00125278
Iteration 11/25 | Loss: 0.00125278
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001252780668437481, 0.001252780668437481, 0.001252780668437481, 0.001252780668437481, 0.001252780668437481]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001252780668437481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85374403
Iteration 2/25 | Loss: 0.00083124
Iteration 3/25 | Loss: 0.00083124
Iteration 4/25 | Loss: 0.00083124
Iteration 5/25 | Loss: 0.00083123
Iteration 6/25 | Loss: 0.00083123
Iteration 7/25 | Loss: 0.00083123
Iteration 8/25 | Loss: 0.00083123
Iteration 9/25 | Loss: 0.00083123
Iteration 10/25 | Loss: 0.00083123
Iteration 11/25 | Loss: 0.00083123
Iteration 12/25 | Loss: 0.00083123
Iteration 13/25 | Loss: 0.00083123
Iteration 14/25 | Loss: 0.00083123
Iteration 15/25 | Loss: 0.00083123
Iteration 16/25 | Loss: 0.00083123
Iteration 17/25 | Loss: 0.00083123
Iteration 18/25 | Loss: 0.00083123
Iteration 19/25 | Loss: 0.00083123
Iteration 20/25 | Loss: 0.00083123
Iteration 21/25 | Loss: 0.00083123
Iteration 22/25 | Loss: 0.00083123
Iteration 23/25 | Loss: 0.00083123
Iteration 24/25 | Loss: 0.00083123
Iteration 25/25 | Loss: 0.00083123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083123
Iteration 2/1000 | Loss: 0.00003147
Iteration 3/1000 | Loss: 0.00002371
Iteration 4/1000 | Loss: 0.00002184
Iteration 5/1000 | Loss: 0.00002113
Iteration 6/1000 | Loss: 0.00002055
Iteration 7/1000 | Loss: 0.00002016
Iteration 8/1000 | Loss: 0.00001980
Iteration 9/1000 | Loss: 0.00001962
Iteration 10/1000 | Loss: 0.00001953
Iteration 11/1000 | Loss: 0.00001950
Iteration 12/1000 | Loss: 0.00001920
Iteration 13/1000 | Loss: 0.00001911
Iteration 14/1000 | Loss: 0.00001909
Iteration 15/1000 | Loss: 0.00001907
Iteration 16/1000 | Loss: 0.00001888
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001873
Iteration 19/1000 | Loss: 0.00001873
Iteration 20/1000 | Loss: 0.00001872
Iteration 21/1000 | Loss: 0.00001871
Iteration 22/1000 | Loss: 0.00001871
Iteration 23/1000 | Loss: 0.00001869
Iteration 24/1000 | Loss: 0.00001867
Iteration 25/1000 | Loss: 0.00001866
Iteration 26/1000 | Loss: 0.00001865
Iteration 27/1000 | Loss: 0.00001865
Iteration 28/1000 | Loss: 0.00001864
Iteration 29/1000 | Loss: 0.00001864
Iteration 30/1000 | Loss: 0.00001862
Iteration 31/1000 | Loss: 0.00001861
Iteration 32/1000 | Loss: 0.00001861
Iteration 33/1000 | Loss: 0.00001860
Iteration 34/1000 | Loss: 0.00001860
Iteration 35/1000 | Loss: 0.00001859
Iteration 36/1000 | Loss: 0.00001859
Iteration 37/1000 | Loss: 0.00001859
Iteration 38/1000 | Loss: 0.00001858
Iteration 39/1000 | Loss: 0.00001855
Iteration 40/1000 | Loss: 0.00001853
Iteration 41/1000 | Loss: 0.00001852
Iteration 42/1000 | Loss: 0.00001852
Iteration 43/1000 | Loss: 0.00001847
Iteration 44/1000 | Loss: 0.00001844
Iteration 45/1000 | Loss: 0.00001843
Iteration 46/1000 | Loss: 0.00001842
Iteration 47/1000 | Loss: 0.00001842
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001841
Iteration 50/1000 | Loss: 0.00001841
Iteration 51/1000 | Loss: 0.00001840
Iteration 52/1000 | Loss: 0.00001840
Iteration 53/1000 | Loss: 0.00001840
Iteration 54/1000 | Loss: 0.00001840
Iteration 55/1000 | Loss: 0.00001840
Iteration 56/1000 | Loss: 0.00001839
Iteration 57/1000 | Loss: 0.00001839
Iteration 58/1000 | Loss: 0.00001839
Iteration 59/1000 | Loss: 0.00001839
Iteration 60/1000 | Loss: 0.00001838
Iteration 61/1000 | Loss: 0.00001838
Iteration 62/1000 | Loss: 0.00001836
Iteration 63/1000 | Loss: 0.00001834
Iteration 64/1000 | Loss: 0.00001834
Iteration 65/1000 | Loss: 0.00001834
Iteration 66/1000 | Loss: 0.00001834
Iteration 67/1000 | Loss: 0.00001834
Iteration 68/1000 | Loss: 0.00001834
Iteration 69/1000 | Loss: 0.00001834
Iteration 70/1000 | Loss: 0.00001832
Iteration 71/1000 | Loss: 0.00001832
Iteration 72/1000 | Loss: 0.00001832
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001831
Iteration 77/1000 | Loss: 0.00001830
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001829
Iteration 82/1000 | Loss: 0.00001829
Iteration 83/1000 | Loss: 0.00001829
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001828
Iteration 88/1000 | Loss: 0.00001828
Iteration 89/1000 | Loss: 0.00001828
Iteration 90/1000 | Loss: 0.00001828
Iteration 91/1000 | Loss: 0.00001828
Iteration 92/1000 | Loss: 0.00001828
Iteration 93/1000 | Loss: 0.00001827
Iteration 94/1000 | Loss: 0.00001827
Iteration 95/1000 | Loss: 0.00001827
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001827
Iteration 98/1000 | Loss: 0.00001827
Iteration 99/1000 | Loss: 0.00001827
Iteration 100/1000 | Loss: 0.00001827
Iteration 101/1000 | Loss: 0.00001827
Iteration 102/1000 | Loss: 0.00001826
Iteration 103/1000 | Loss: 0.00001826
Iteration 104/1000 | Loss: 0.00001826
Iteration 105/1000 | Loss: 0.00001826
Iteration 106/1000 | Loss: 0.00001826
Iteration 107/1000 | Loss: 0.00001826
Iteration 108/1000 | Loss: 0.00001826
Iteration 109/1000 | Loss: 0.00001825
Iteration 110/1000 | Loss: 0.00001825
Iteration 111/1000 | Loss: 0.00001825
Iteration 112/1000 | Loss: 0.00001825
Iteration 113/1000 | Loss: 0.00001824
Iteration 114/1000 | Loss: 0.00001824
Iteration 115/1000 | Loss: 0.00001824
Iteration 116/1000 | Loss: 0.00001824
Iteration 117/1000 | Loss: 0.00001824
Iteration 118/1000 | Loss: 0.00001824
Iteration 119/1000 | Loss: 0.00001824
Iteration 120/1000 | Loss: 0.00001824
Iteration 121/1000 | Loss: 0.00001824
Iteration 122/1000 | Loss: 0.00001824
Iteration 123/1000 | Loss: 0.00001823
Iteration 124/1000 | Loss: 0.00001823
Iteration 125/1000 | Loss: 0.00001823
Iteration 126/1000 | Loss: 0.00001823
Iteration 127/1000 | Loss: 0.00001822
Iteration 128/1000 | Loss: 0.00001822
Iteration 129/1000 | Loss: 0.00001822
Iteration 130/1000 | Loss: 0.00001822
Iteration 131/1000 | Loss: 0.00001822
Iteration 132/1000 | Loss: 0.00001822
Iteration 133/1000 | Loss: 0.00001822
Iteration 134/1000 | Loss: 0.00001822
Iteration 135/1000 | Loss: 0.00001822
Iteration 136/1000 | Loss: 0.00001822
Iteration 137/1000 | Loss: 0.00001822
Iteration 138/1000 | Loss: 0.00001821
Iteration 139/1000 | Loss: 0.00001821
Iteration 140/1000 | Loss: 0.00001821
Iteration 141/1000 | Loss: 0.00001821
Iteration 142/1000 | Loss: 0.00001821
Iteration 143/1000 | Loss: 0.00001821
Iteration 144/1000 | Loss: 0.00001821
Iteration 145/1000 | Loss: 0.00001821
Iteration 146/1000 | Loss: 0.00001821
Iteration 147/1000 | Loss: 0.00001821
Iteration 148/1000 | Loss: 0.00001820
Iteration 149/1000 | Loss: 0.00001820
Iteration 150/1000 | Loss: 0.00001820
Iteration 151/1000 | Loss: 0.00001820
Iteration 152/1000 | Loss: 0.00001820
Iteration 153/1000 | Loss: 0.00001820
Iteration 154/1000 | Loss: 0.00001820
Iteration 155/1000 | Loss: 0.00001820
Iteration 156/1000 | Loss: 0.00001819
Iteration 157/1000 | Loss: 0.00001819
Iteration 158/1000 | Loss: 0.00001819
Iteration 159/1000 | Loss: 0.00001819
Iteration 160/1000 | Loss: 0.00001819
Iteration 161/1000 | Loss: 0.00001819
Iteration 162/1000 | Loss: 0.00001819
Iteration 163/1000 | Loss: 0.00001819
Iteration 164/1000 | Loss: 0.00001819
Iteration 165/1000 | Loss: 0.00001819
Iteration 166/1000 | Loss: 0.00001819
Iteration 167/1000 | Loss: 0.00001819
Iteration 168/1000 | Loss: 0.00001819
Iteration 169/1000 | Loss: 0.00001819
Iteration 170/1000 | Loss: 0.00001819
Iteration 171/1000 | Loss: 0.00001818
Iteration 172/1000 | Loss: 0.00001818
Iteration 173/1000 | Loss: 0.00001818
Iteration 174/1000 | Loss: 0.00001818
Iteration 175/1000 | Loss: 0.00001818
Iteration 176/1000 | Loss: 0.00001818
Iteration 177/1000 | Loss: 0.00001818
Iteration 178/1000 | Loss: 0.00001818
Iteration 179/1000 | Loss: 0.00001818
Iteration 180/1000 | Loss: 0.00001818
Iteration 181/1000 | Loss: 0.00001818
Iteration 182/1000 | Loss: 0.00001818
Iteration 183/1000 | Loss: 0.00001818
Iteration 184/1000 | Loss: 0.00001818
Iteration 185/1000 | Loss: 0.00001817
Iteration 186/1000 | Loss: 0.00001817
Iteration 187/1000 | Loss: 0.00001817
Iteration 188/1000 | Loss: 0.00001817
Iteration 189/1000 | Loss: 0.00001817
Iteration 190/1000 | Loss: 0.00001817
Iteration 191/1000 | Loss: 0.00001817
Iteration 192/1000 | Loss: 0.00001817
Iteration 193/1000 | Loss: 0.00001817
Iteration 194/1000 | Loss: 0.00001817
Iteration 195/1000 | Loss: 0.00001817
Iteration 196/1000 | Loss: 0.00001817
Iteration 197/1000 | Loss: 0.00001817
Iteration 198/1000 | Loss: 0.00001817
Iteration 199/1000 | Loss: 0.00001817
Iteration 200/1000 | Loss: 0.00001817
Iteration 201/1000 | Loss: 0.00001817
Iteration 202/1000 | Loss: 0.00001817
Iteration 203/1000 | Loss: 0.00001816
Iteration 204/1000 | Loss: 0.00001816
Iteration 205/1000 | Loss: 0.00001816
Iteration 206/1000 | Loss: 0.00001816
Iteration 207/1000 | Loss: 0.00001816
Iteration 208/1000 | Loss: 0.00001816
Iteration 209/1000 | Loss: 0.00001816
Iteration 210/1000 | Loss: 0.00001816
Iteration 211/1000 | Loss: 0.00001816
Iteration 212/1000 | Loss: 0.00001816
Iteration 213/1000 | Loss: 0.00001816
Iteration 214/1000 | Loss: 0.00001816
Iteration 215/1000 | Loss: 0.00001816
Iteration 216/1000 | Loss: 0.00001815
Iteration 217/1000 | Loss: 0.00001815
Iteration 218/1000 | Loss: 0.00001815
Iteration 219/1000 | Loss: 0.00001815
Iteration 220/1000 | Loss: 0.00001815
Iteration 221/1000 | Loss: 0.00001815
Iteration 222/1000 | Loss: 0.00001815
Iteration 223/1000 | Loss: 0.00001815
Iteration 224/1000 | Loss: 0.00001815
Iteration 225/1000 | Loss: 0.00001815
Iteration 226/1000 | Loss: 0.00001814
Iteration 227/1000 | Loss: 0.00001814
Iteration 228/1000 | Loss: 0.00001814
Iteration 229/1000 | Loss: 0.00001814
Iteration 230/1000 | Loss: 0.00001814
Iteration 231/1000 | Loss: 0.00001814
Iteration 232/1000 | Loss: 0.00001814
Iteration 233/1000 | Loss: 0.00001814
Iteration 234/1000 | Loss: 0.00001814
Iteration 235/1000 | Loss: 0.00001814
Iteration 236/1000 | Loss: 0.00001814
Iteration 237/1000 | Loss: 0.00001814
Iteration 238/1000 | Loss: 0.00001814
Iteration 239/1000 | Loss: 0.00001814
Iteration 240/1000 | Loss: 0.00001814
Iteration 241/1000 | Loss: 0.00001814
Iteration 242/1000 | Loss: 0.00001814
Iteration 243/1000 | Loss: 0.00001814
Iteration 244/1000 | Loss: 0.00001814
Iteration 245/1000 | Loss: 0.00001814
Iteration 246/1000 | Loss: 0.00001814
Iteration 247/1000 | Loss: 0.00001814
Iteration 248/1000 | Loss: 0.00001814
Iteration 249/1000 | Loss: 0.00001814
Iteration 250/1000 | Loss: 0.00001814
Iteration 251/1000 | Loss: 0.00001814
Iteration 252/1000 | Loss: 0.00001814
Iteration 253/1000 | Loss: 0.00001814
Iteration 254/1000 | Loss: 0.00001814
Iteration 255/1000 | Loss: 0.00001814
Iteration 256/1000 | Loss: 0.00001814
Iteration 257/1000 | Loss: 0.00001814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 257. Stopping optimization.
Last 5 losses: [1.8139116946258582e-05, 1.8139116946258582e-05, 1.8139116946258582e-05, 1.8139116946258582e-05, 1.8139116946258582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8139116946258582e-05

Optimization complete. Final v2v error: 3.613950490951538 mm

Highest mean error: 3.882852554321289 mm for frame 32

Lowest mean error: 3.3897013664245605 mm for frame 48

Saving results

Total time: 43.31391477584839
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00900017
Iteration 2/25 | Loss: 0.00173975
Iteration 3/25 | Loss: 0.00144031
Iteration 4/25 | Loss: 0.00135903
Iteration 5/25 | Loss: 0.00133748
Iteration 6/25 | Loss: 0.00132870
Iteration 7/25 | Loss: 0.00132416
Iteration 8/25 | Loss: 0.00131889
Iteration 9/25 | Loss: 0.00131699
Iteration 10/25 | Loss: 0.00131620
Iteration 11/25 | Loss: 0.00131570
Iteration 12/25 | Loss: 0.00131556
Iteration 13/25 | Loss: 0.00131552
Iteration 14/25 | Loss: 0.00131547
Iteration 15/25 | Loss: 0.00131546
Iteration 16/25 | Loss: 0.00131546
Iteration 17/25 | Loss: 0.00131546
Iteration 18/25 | Loss: 0.00131544
Iteration 19/25 | Loss: 0.00131544
Iteration 20/25 | Loss: 0.00131544
Iteration 21/25 | Loss: 0.00131544
Iteration 22/25 | Loss: 0.00131544
Iteration 23/25 | Loss: 0.00131543
Iteration 24/25 | Loss: 0.00131543
Iteration 25/25 | Loss: 0.00131543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.21168566
Iteration 2/25 | Loss: 0.00081493
Iteration 3/25 | Loss: 0.00081489
Iteration 4/25 | Loss: 0.00081489
Iteration 5/25 | Loss: 0.00081489
Iteration 6/25 | Loss: 0.00081489
Iteration 7/25 | Loss: 0.00081489
Iteration 8/25 | Loss: 0.00081489
Iteration 9/25 | Loss: 0.00081489
Iteration 10/25 | Loss: 0.00081489
Iteration 11/25 | Loss: 0.00081489
Iteration 12/25 | Loss: 0.00081489
Iteration 13/25 | Loss: 0.00081489
Iteration 14/25 | Loss: 0.00081489
Iteration 15/25 | Loss: 0.00081489
Iteration 16/25 | Loss: 0.00081489
Iteration 17/25 | Loss: 0.00081489
Iteration 18/25 | Loss: 0.00081489
Iteration 19/25 | Loss: 0.00081489
Iteration 20/25 | Loss: 0.00081489
Iteration 21/25 | Loss: 0.00081489
Iteration 22/25 | Loss: 0.00081489
Iteration 23/25 | Loss: 0.00081489
Iteration 24/25 | Loss: 0.00081489
Iteration 25/25 | Loss: 0.00081489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081489
Iteration 2/1000 | Loss: 0.00005151
Iteration 3/1000 | Loss: 0.00003292
Iteration 4/1000 | Loss: 0.00002941
Iteration 5/1000 | Loss: 0.00002771
Iteration 6/1000 | Loss: 0.00002640
Iteration 7/1000 | Loss: 0.00007560
Iteration 8/1000 | Loss: 0.00002852
Iteration 9/1000 | Loss: 0.00002614
Iteration 10/1000 | Loss: 0.00002472
Iteration 11/1000 | Loss: 0.00002367
Iteration 12/1000 | Loss: 0.00002313
Iteration 13/1000 | Loss: 0.00002272
Iteration 14/1000 | Loss: 0.00002255
Iteration 15/1000 | Loss: 0.00002237
Iteration 16/1000 | Loss: 0.00002220
Iteration 17/1000 | Loss: 0.00002209
Iteration 18/1000 | Loss: 0.00002207
Iteration 19/1000 | Loss: 0.00002200
Iteration 20/1000 | Loss: 0.00002200
Iteration 21/1000 | Loss: 0.00002198
Iteration 22/1000 | Loss: 0.00002197
Iteration 23/1000 | Loss: 0.00002196
Iteration 24/1000 | Loss: 0.00002196
Iteration 25/1000 | Loss: 0.00002196
Iteration 26/1000 | Loss: 0.00002195
Iteration 27/1000 | Loss: 0.00002195
Iteration 28/1000 | Loss: 0.00002194
Iteration 29/1000 | Loss: 0.00002193
Iteration 30/1000 | Loss: 0.00002193
Iteration 31/1000 | Loss: 0.00002192
Iteration 32/1000 | Loss: 0.00002192
Iteration 33/1000 | Loss: 0.00002191
Iteration 34/1000 | Loss: 0.00002187
Iteration 35/1000 | Loss: 0.00002187
Iteration 36/1000 | Loss: 0.00002187
Iteration 37/1000 | Loss: 0.00002187
Iteration 38/1000 | Loss: 0.00002187
Iteration 39/1000 | Loss: 0.00002187
Iteration 40/1000 | Loss: 0.00002186
Iteration 41/1000 | Loss: 0.00002186
Iteration 42/1000 | Loss: 0.00002185
Iteration 43/1000 | Loss: 0.00002185
Iteration 44/1000 | Loss: 0.00002185
Iteration 45/1000 | Loss: 0.00002184
Iteration 46/1000 | Loss: 0.00002184
Iteration 47/1000 | Loss: 0.00002184
Iteration 48/1000 | Loss: 0.00002183
Iteration 49/1000 | Loss: 0.00002183
Iteration 50/1000 | Loss: 0.00002183
Iteration 51/1000 | Loss: 0.00002182
Iteration 52/1000 | Loss: 0.00002182
Iteration 53/1000 | Loss: 0.00002182
Iteration 54/1000 | Loss: 0.00002182
Iteration 55/1000 | Loss: 0.00002182
Iteration 56/1000 | Loss: 0.00002182
Iteration 57/1000 | Loss: 0.00002182
Iteration 58/1000 | Loss: 0.00002182
Iteration 59/1000 | Loss: 0.00002182
Iteration 60/1000 | Loss: 0.00002182
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002181
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002181
Iteration 65/1000 | Loss: 0.00002180
Iteration 66/1000 | Loss: 0.00002180
Iteration 67/1000 | Loss: 0.00002180
Iteration 68/1000 | Loss: 0.00002180
Iteration 69/1000 | Loss: 0.00002180
Iteration 70/1000 | Loss: 0.00002180
Iteration 71/1000 | Loss: 0.00002180
Iteration 72/1000 | Loss: 0.00002180
Iteration 73/1000 | Loss: 0.00002179
Iteration 74/1000 | Loss: 0.00002179
Iteration 75/1000 | Loss: 0.00002179
Iteration 76/1000 | Loss: 0.00002179
Iteration 77/1000 | Loss: 0.00002179
Iteration 78/1000 | Loss: 0.00002179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [2.1794839994981885e-05, 2.1794839994981885e-05, 2.1794839994981885e-05, 2.1794839994981885e-05, 2.1794839994981885e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1794839994981885e-05

Optimization complete. Final v2v error: 3.8941800594329834 mm

Highest mean error: 4.95343542098999 mm for frame 68

Lowest mean error: 3.4405550956726074 mm for frame 135

Saving results

Total time: 54.470726013183594
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988866
Iteration 2/25 | Loss: 0.00185570
Iteration 3/25 | Loss: 0.00146942
Iteration 4/25 | Loss: 0.00142981
Iteration 5/25 | Loss: 0.00142298
Iteration 6/25 | Loss: 0.00142228
Iteration 7/25 | Loss: 0.00142228
Iteration 8/25 | Loss: 0.00142228
Iteration 9/25 | Loss: 0.00142228
Iteration 10/25 | Loss: 0.00142228
Iteration 11/25 | Loss: 0.00142228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001422278699465096, 0.001422278699465096, 0.001422278699465096, 0.001422278699465096, 0.001422278699465096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001422278699465096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.75144625
Iteration 2/25 | Loss: 0.00077035
Iteration 3/25 | Loss: 0.00077029
Iteration 4/25 | Loss: 0.00077029
Iteration 5/25 | Loss: 0.00077029
Iteration 6/25 | Loss: 0.00077029
Iteration 7/25 | Loss: 0.00077029
Iteration 8/25 | Loss: 0.00077029
Iteration 9/25 | Loss: 0.00077029
Iteration 10/25 | Loss: 0.00077029
Iteration 11/25 | Loss: 0.00077029
Iteration 12/25 | Loss: 0.00077029
Iteration 13/25 | Loss: 0.00077029
Iteration 14/25 | Loss: 0.00077029
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007702889270149171, 0.0007702889270149171, 0.0007702889270149171, 0.0007702889270149171, 0.0007702889270149171]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007702889270149171

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077029
Iteration 2/1000 | Loss: 0.00007504
Iteration 3/1000 | Loss: 0.00004677
Iteration 4/1000 | Loss: 0.00004006
Iteration 5/1000 | Loss: 0.00003867
Iteration 6/1000 | Loss: 0.00003741
Iteration 7/1000 | Loss: 0.00003642
Iteration 8/1000 | Loss: 0.00003584
Iteration 9/1000 | Loss: 0.00003535
Iteration 10/1000 | Loss: 0.00003493
Iteration 11/1000 | Loss: 0.00003466
Iteration 12/1000 | Loss: 0.00003438
Iteration 13/1000 | Loss: 0.00003417
Iteration 14/1000 | Loss: 0.00003400
Iteration 15/1000 | Loss: 0.00003390
Iteration 16/1000 | Loss: 0.00003383
Iteration 17/1000 | Loss: 0.00003379
Iteration 18/1000 | Loss: 0.00003379
Iteration 19/1000 | Loss: 0.00003371
Iteration 20/1000 | Loss: 0.00003360
Iteration 21/1000 | Loss: 0.00003359
Iteration 22/1000 | Loss: 0.00003353
Iteration 23/1000 | Loss: 0.00003353
Iteration 24/1000 | Loss: 0.00003346
Iteration 25/1000 | Loss: 0.00003342
Iteration 26/1000 | Loss: 0.00003340
Iteration 27/1000 | Loss: 0.00003336
Iteration 28/1000 | Loss: 0.00003336
Iteration 29/1000 | Loss: 0.00003335
Iteration 30/1000 | Loss: 0.00003335
Iteration 31/1000 | Loss: 0.00003334
Iteration 32/1000 | Loss: 0.00003334
Iteration 33/1000 | Loss: 0.00003333
Iteration 34/1000 | Loss: 0.00003332
Iteration 35/1000 | Loss: 0.00003332
Iteration 36/1000 | Loss: 0.00003332
Iteration 37/1000 | Loss: 0.00003331
Iteration 38/1000 | Loss: 0.00003331
Iteration 39/1000 | Loss: 0.00003331
Iteration 40/1000 | Loss: 0.00003330
Iteration 41/1000 | Loss: 0.00003330
Iteration 42/1000 | Loss: 0.00003329
Iteration 43/1000 | Loss: 0.00003328
Iteration 44/1000 | Loss: 0.00003328
Iteration 45/1000 | Loss: 0.00003328
Iteration 46/1000 | Loss: 0.00003328
Iteration 47/1000 | Loss: 0.00003328
Iteration 48/1000 | Loss: 0.00003328
Iteration 49/1000 | Loss: 0.00003327
Iteration 50/1000 | Loss: 0.00003327
Iteration 51/1000 | Loss: 0.00003326
Iteration 52/1000 | Loss: 0.00003326
Iteration 53/1000 | Loss: 0.00003326
Iteration 54/1000 | Loss: 0.00003326
Iteration 55/1000 | Loss: 0.00003326
Iteration 56/1000 | Loss: 0.00003325
Iteration 57/1000 | Loss: 0.00003325
Iteration 58/1000 | Loss: 0.00003325
Iteration 59/1000 | Loss: 0.00003325
Iteration 60/1000 | Loss: 0.00003325
Iteration 61/1000 | Loss: 0.00003325
Iteration 62/1000 | Loss: 0.00003324
Iteration 63/1000 | Loss: 0.00003324
Iteration 64/1000 | Loss: 0.00003324
Iteration 65/1000 | Loss: 0.00003324
Iteration 66/1000 | Loss: 0.00003324
Iteration 67/1000 | Loss: 0.00003324
Iteration 68/1000 | Loss: 0.00003323
Iteration 69/1000 | Loss: 0.00003323
Iteration 70/1000 | Loss: 0.00003323
Iteration 71/1000 | Loss: 0.00003323
Iteration 72/1000 | Loss: 0.00003323
Iteration 73/1000 | Loss: 0.00003323
Iteration 74/1000 | Loss: 0.00003323
Iteration 75/1000 | Loss: 0.00003323
Iteration 76/1000 | Loss: 0.00003323
Iteration 77/1000 | Loss: 0.00003323
Iteration 78/1000 | Loss: 0.00003323
Iteration 79/1000 | Loss: 0.00003323
Iteration 80/1000 | Loss: 0.00003323
Iteration 81/1000 | Loss: 0.00003322
Iteration 82/1000 | Loss: 0.00003322
Iteration 83/1000 | Loss: 0.00003322
Iteration 84/1000 | Loss: 0.00003322
Iteration 85/1000 | Loss: 0.00003322
Iteration 86/1000 | Loss: 0.00003322
Iteration 87/1000 | Loss: 0.00003322
Iteration 88/1000 | Loss: 0.00003321
Iteration 89/1000 | Loss: 0.00003321
Iteration 90/1000 | Loss: 0.00003321
Iteration 91/1000 | Loss: 0.00003321
Iteration 92/1000 | Loss: 0.00003321
Iteration 93/1000 | Loss: 0.00003321
Iteration 94/1000 | Loss: 0.00003321
Iteration 95/1000 | Loss: 0.00003320
Iteration 96/1000 | Loss: 0.00003320
Iteration 97/1000 | Loss: 0.00003320
Iteration 98/1000 | Loss: 0.00003320
Iteration 99/1000 | Loss: 0.00003320
Iteration 100/1000 | Loss: 0.00003320
Iteration 101/1000 | Loss: 0.00003320
Iteration 102/1000 | Loss: 0.00003319
Iteration 103/1000 | Loss: 0.00003319
Iteration 104/1000 | Loss: 0.00003319
Iteration 105/1000 | Loss: 0.00003319
Iteration 106/1000 | Loss: 0.00003319
Iteration 107/1000 | Loss: 0.00003319
Iteration 108/1000 | Loss: 0.00003318
Iteration 109/1000 | Loss: 0.00003318
Iteration 110/1000 | Loss: 0.00003318
Iteration 111/1000 | Loss: 0.00003318
Iteration 112/1000 | Loss: 0.00003318
Iteration 113/1000 | Loss: 0.00003317
Iteration 114/1000 | Loss: 0.00003317
Iteration 115/1000 | Loss: 0.00003317
Iteration 116/1000 | Loss: 0.00003317
Iteration 117/1000 | Loss: 0.00003317
Iteration 118/1000 | Loss: 0.00003317
Iteration 119/1000 | Loss: 0.00003316
Iteration 120/1000 | Loss: 0.00003316
Iteration 121/1000 | Loss: 0.00003316
Iteration 122/1000 | Loss: 0.00003316
Iteration 123/1000 | Loss: 0.00003316
Iteration 124/1000 | Loss: 0.00003316
Iteration 125/1000 | Loss: 0.00003316
Iteration 126/1000 | Loss: 0.00003316
Iteration 127/1000 | Loss: 0.00003315
Iteration 128/1000 | Loss: 0.00003315
Iteration 129/1000 | Loss: 0.00003315
Iteration 130/1000 | Loss: 0.00003315
Iteration 131/1000 | Loss: 0.00003315
Iteration 132/1000 | Loss: 0.00003315
Iteration 133/1000 | Loss: 0.00003314
Iteration 134/1000 | Loss: 0.00003314
Iteration 135/1000 | Loss: 0.00003314
Iteration 136/1000 | Loss: 0.00003314
Iteration 137/1000 | Loss: 0.00003314
Iteration 138/1000 | Loss: 0.00003314
Iteration 139/1000 | Loss: 0.00003314
Iteration 140/1000 | Loss: 0.00003314
Iteration 141/1000 | Loss: 0.00003314
Iteration 142/1000 | Loss: 0.00003314
Iteration 143/1000 | Loss: 0.00003314
Iteration 144/1000 | Loss: 0.00003314
Iteration 145/1000 | Loss: 0.00003313
Iteration 146/1000 | Loss: 0.00003313
Iteration 147/1000 | Loss: 0.00003313
Iteration 148/1000 | Loss: 0.00003313
Iteration 149/1000 | Loss: 0.00003313
Iteration 150/1000 | Loss: 0.00003313
Iteration 151/1000 | Loss: 0.00003313
Iteration 152/1000 | Loss: 0.00003313
Iteration 153/1000 | Loss: 0.00003313
Iteration 154/1000 | Loss: 0.00003313
Iteration 155/1000 | Loss: 0.00003313
Iteration 156/1000 | Loss: 0.00003313
Iteration 157/1000 | Loss: 0.00003312
Iteration 158/1000 | Loss: 0.00003312
Iteration 159/1000 | Loss: 0.00003312
Iteration 160/1000 | Loss: 0.00003312
Iteration 161/1000 | Loss: 0.00003312
Iteration 162/1000 | Loss: 0.00003312
Iteration 163/1000 | Loss: 0.00003312
Iteration 164/1000 | Loss: 0.00003312
Iteration 165/1000 | Loss: 0.00003312
Iteration 166/1000 | Loss: 0.00003312
Iteration 167/1000 | Loss: 0.00003312
Iteration 168/1000 | Loss: 0.00003312
Iteration 169/1000 | Loss: 0.00003312
Iteration 170/1000 | Loss: 0.00003312
Iteration 171/1000 | Loss: 0.00003312
Iteration 172/1000 | Loss: 0.00003311
Iteration 173/1000 | Loss: 0.00003311
Iteration 174/1000 | Loss: 0.00003311
Iteration 175/1000 | Loss: 0.00003311
Iteration 176/1000 | Loss: 0.00003311
Iteration 177/1000 | Loss: 0.00003311
Iteration 178/1000 | Loss: 0.00003310
Iteration 179/1000 | Loss: 0.00003310
Iteration 180/1000 | Loss: 0.00003310
Iteration 181/1000 | Loss: 0.00003310
Iteration 182/1000 | Loss: 0.00003310
Iteration 183/1000 | Loss: 0.00003310
Iteration 184/1000 | Loss: 0.00003310
Iteration 185/1000 | Loss: 0.00003310
Iteration 186/1000 | Loss: 0.00003310
Iteration 187/1000 | Loss: 0.00003310
Iteration 188/1000 | Loss: 0.00003310
Iteration 189/1000 | Loss: 0.00003310
Iteration 190/1000 | Loss: 0.00003310
Iteration 191/1000 | Loss: 0.00003310
Iteration 192/1000 | Loss: 0.00003310
Iteration 193/1000 | Loss: 0.00003310
Iteration 194/1000 | Loss: 0.00003310
Iteration 195/1000 | Loss: 0.00003310
Iteration 196/1000 | Loss: 0.00003310
Iteration 197/1000 | Loss: 0.00003310
Iteration 198/1000 | Loss: 0.00003310
Iteration 199/1000 | Loss: 0.00003310
Iteration 200/1000 | Loss: 0.00003310
Iteration 201/1000 | Loss: 0.00003310
Iteration 202/1000 | Loss: 0.00003310
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [3.3101470762630925e-05, 3.3101470762630925e-05, 3.3101470762630925e-05, 3.3101470762630925e-05, 3.3101470762630925e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3101470762630925e-05

Optimization complete. Final v2v error: 4.7166900634765625 mm

Highest mean error: 5.30124568939209 mm for frame 81

Lowest mean error: 4.120270252227783 mm for frame 157

Saving results

Total time: 52.6494357585907
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837002
Iteration 2/25 | Loss: 0.00145321
Iteration 3/25 | Loss: 0.00125503
Iteration 4/25 | Loss: 0.00123808
Iteration 5/25 | Loss: 0.00123498
Iteration 6/25 | Loss: 0.00123476
Iteration 7/25 | Loss: 0.00123476
Iteration 8/25 | Loss: 0.00123476
Iteration 9/25 | Loss: 0.00123476
Iteration 10/25 | Loss: 0.00123476
Iteration 11/25 | Loss: 0.00123476
Iteration 12/25 | Loss: 0.00123476
Iteration 13/25 | Loss: 0.00123476
Iteration 14/25 | Loss: 0.00123476
Iteration 15/25 | Loss: 0.00123476
Iteration 16/25 | Loss: 0.00123476
Iteration 17/25 | Loss: 0.00123476
Iteration 18/25 | Loss: 0.00123476
Iteration 19/25 | Loss: 0.00123476
Iteration 20/25 | Loss: 0.00123476
Iteration 21/25 | Loss: 0.00123476
Iteration 22/25 | Loss: 0.00123476
Iteration 23/25 | Loss: 0.00123476
Iteration 24/25 | Loss: 0.00123476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012347568990662694, 0.0012347568990662694, 0.0012347568990662694, 0.0012347568990662694, 0.0012347568990662694]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012347568990662694

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44386780
Iteration 2/25 | Loss: 0.00073397
Iteration 3/25 | Loss: 0.00073396
Iteration 4/25 | Loss: 0.00073396
Iteration 5/25 | Loss: 0.00073396
Iteration 6/25 | Loss: 0.00073396
Iteration 7/25 | Loss: 0.00073396
Iteration 8/25 | Loss: 0.00073396
Iteration 9/25 | Loss: 0.00073396
Iteration 10/25 | Loss: 0.00073396
Iteration 11/25 | Loss: 0.00073395
Iteration 12/25 | Loss: 0.00073395
Iteration 13/25 | Loss: 0.00073395
Iteration 14/25 | Loss: 0.00073395
Iteration 15/25 | Loss: 0.00073395
Iteration 16/25 | Loss: 0.00073395
Iteration 17/25 | Loss: 0.00073395
Iteration 18/25 | Loss: 0.00073395
Iteration 19/25 | Loss: 0.00073395
Iteration 20/25 | Loss: 0.00073395
Iteration 21/25 | Loss: 0.00073395
Iteration 22/25 | Loss: 0.00073395
Iteration 23/25 | Loss: 0.00073395
Iteration 24/25 | Loss: 0.00073395
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007339547737501562, 0.0007339547737501562, 0.0007339547737501562, 0.0007339547737501562, 0.0007339547737501562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007339547737501562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073395
Iteration 2/1000 | Loss: 0.00003264
Iteration 3/1000 | Loss: 0.00002040
Iteration 4/1000 | Loss: 0.00001819
Iteration 5/1000 | Loss: 0.00001702
Iteration 6/1000 | Loss: 0.00001584
Iteration 7/1000 | Loss: 0.00001529
Iteration 8/1000 | Loss: 0.00001492
Iteration 9/1000 | Loss: 0.00001459
Iteration 10/1000 | Loss: 0.00001428
Iteration 11/1000 | Loss: 0.00001426
Iteration 12/1000 | Loss: 0.00001421
Iteration 13/1000 | Loss: 0.00001411
Iteration 14/1000 | Loss: 0.00001409
Iteration 15/1000 | Loss: 0.00001408
Iteration 16/1000 | Loss: 0.00001398
Iteration 17/1000 | Loss: 0.00001396
Iteration 18/1000 | Loss: 0.00001394
Iteration 19/1000 | Loss: 0.00001393
Iteration 20/1000 | Loss: 0.00001393
Iteration 21/1000 | Loss: 0.00001393
Iteration 22/1000 | Loss: 0.00001392
Iteration 23/1000 | Loss: 0.00001392
Iteration 24/1000 | Loss: 0.00001391
Iteration 25/1000 | Loss: 0.00001391
Iteration 26/1000 | Loss: 0.00001390
Iteration 27/1000 | Loss: 0.00001390
Iteration 28/1000 | Loss: 0.00001388
Iteration 29/1000 | Loss: 0.00001388
Iteration 30/1000 | Loss: 0.00001386
Iteration 31/1000 | Loss: 0.00001385
Iteration 32/1000 | Loss: 0.00001385
Iteration 33/1000 | Loss: 0.00001384
Iteration 34/1000 | Loss: 0.00001382
Iteration 35/1000 | Loss: 0.00001382
Iteration 36/1000 | Loss: 0.00001382
Iteration 37/1000 | Loss: 0.00001381
Iteration 38/1000 | Loss: 0.00001381
Iteration 39/1000 | Loss: 0.00001380
Iteration 40/1000 | Loss: 0.00001380
Iteration 41/1000 | Loss: 0.00001380
Iteration 42/1000 | Loss: 0.00001379
Iteration 43/1000 | Loss: 0.00001378
Iteration 44/1000 | Loss: 0.00001378
Iteration 45/1000 | Loss: 0.00001378
Iteration 46/1000 | Loss: 0.00001377
Iteration 47/1000 | Loss: 0.00001377
Iteration 48/1000 | Loss: 0.00001376
Iteration 49/1000 | Loss: 0.00001376
Iteration 50/1000 | Loss: 0.00001376
Iteration 51/1000 | Loss: 0.00001376
Iteration 52/1000 | Loss: 0.00001376
Iteration 53/1000 | Loss: 0.00001376
Iteration 54/1000 | Loss: 0.00001376
Iteration 55/1000 | Loss: 0.00001376
Iteration 56/1000 | Loss: 0.00001376
Iteration 57/1000 | Loss: 0.00001376
Iteration 58/1000 | Loss: 0.00001375
Iteration 59/1000 | Loss: 0.00001375
Iteration 60/1000 | Loss: 0.00001374
Iteration 61/1000 | Loss: 0.00001374
Iteration 62/1000 | Loss: 0.00001373
Iteration 63/1000 | Loss: 0.00001373
Iteration 64/1000 | Loss: 0.00001372
Iteration 65/1000 | Loss: 0.00001372
Iteration 66/1000 | Loss: 0.00001371
Iteration 67/1000 | Loss: 0.00001371
Iteration 68/1000 | Loss: 0.00001370
Iteration 69/1000 | Loss: 0.00001370
Iteration 70/1000 | Loss: 0.00001370
Iteration 71/1000 | Loss: 0.00001369
Iteration 72/1000 | Loss: 0.00001369
Iteration 73/1000 | Loss: 0.00001369
Iteration 74/1000 | Loss: 0.00001368
Iteration 75/1000 | Loss: 0.00001368
Iteration 76/1000 | Loss: 0.00001368
Iteration 77/1000 | Loss: 0.00001367
Iteration 78/1000 | Loss: 0.00001367
Iteration 79/1000 | Loss: 0.00001366
Iteration 80/1000 | Loss: 0.00001366
Iteration 81/1000 | Loss: 0.00001366
Iteration 82/1000 | Loss: 0.00001365
Iteration 83/1000 | Loss: 0.00001365
Iteration 84/1000 | Loss: 0.00001365
Iteration 85/1000 | Loss: 0.00001365
Iteration 86/1000 | Loss: 0.00001365
Iteration 87/1000 | Loss: 0.00001365
Iteration 88/1000 | Loss: 0.00001365
Iteration 89/1000 | Loss: 0.00001365
Iteration 90/1000 | Loss: 0.00001365
Iteration 91/1000 | Loss: 0.00001365
Iteration 92/1000 | Loss: 0.00001365
Iteration 93/1000 | Loss: 0.00001365
Iteration 94/1000 | Loss: 0.00001365
Iteration 95/1000 | Loss: 0.00001365
Iteration 96/1000 | Loss: 0.00001365
Iteration 97/1000 | Loss: 0.00001365
Iteration 98/1000 | Loss: 0.00001365
Iteration 99/1000 | Loss: 0.00001365
Iteration 100/1000 | Loss: 0.00001365
Iteration 101/1000 | Loss: 0.00001365
Iteration 102/1000 | Loss: 0.00001365
Iteration 103/1000 | Loss: 0.00001365
Iteration 104/1000 | Loss: 0.00001365
Iteration 105/1000 | Loss: 0.00001365
Iteration 106/1000 | Loss: 0.00001365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.3654876966029406e-05, 1.3654876966029406e-05, 1.3654876966029406e-05, 1.3654876966029406e-05, 1.3654876966029406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3654876966029406e-05

Optimization complete. Final v2v error: 3.1723310947418213 mm

Highest mean error: 3.7253518104553223 mm for frame 134

Lowest mean error: 2.861952304840088 mm for frame 30

Saving results

Total time: 36.59586524963379
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871038
Iteration 2/25 | Loss: 0.00142066
Iteration 3/25 | Loss: 0.00129811
Iteration 4/25 | Loss: 0.00127626
Iteration 5/25 | Loss: 0.00126846
Iteration 6/25 | Loss: 0.00126682
Iteration 7/25 | Loss: 0.00126682
Iteration 8/25 | Loss: 0.00126682
Iteration 9/25 | Loss: 0.00126682
Iteration 10/25 | Loss: 0.00126682
Iteration 11/25 | Loss: 0.00126682
Iteration 12/25 | Loss: 0.00126682
Iteration 13/25 | Loss: 0.00126682
Iteration 14/25 | Loss: 0.00126682
Iteration 15/25 | Loss: 0.00126682
Iteration 16/25 | Loss: 0.00126682
Iteration 17/25 | Loss: 0.00126682
Iteration 18/25 | Loss: 0.00126682
Iteration 19/25 | Loss: 0.00126682
Iteration 20/25 | Loss: 0.00126682
Iteration 21/25 | Loss: 0.00126682
Iteration 22/25 | Loss: 0.00126682
Iteration 23/25 | Loss: 0.00126682
Iteration 24/25 | Loss: 0.00126682
Iteration 25/25 | Loss: 0.00126682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43750703
Iteration 2/25 | Loss: 0.00077148
Iteration 3/25 | Loss: 0.00077145
Iteration 4/25 | Loss: 0.00077145
Iteration 5/25 | Loss: 0.00077145
Iteration 6/25 | Loss: 0.00077145
Iteration 7/25 | Loss: 0.00077145
Iteration 8/25 | Loss: 0.00077145
Iteration 9/25 | Loss: 0.00077145
Iteration 10/25 | Loss: 0.00077145
Iteration 11/25 | Loss: 0.00077145
Iteration 12/25 | Loss: 0.00077145
Iteration 13/25 | Loss: 0.00077145
Iteration 14/25 | Loss: 0.00077145
Iteration 15/25 | Loss: 0.00077145
Iteration 16/25 | Loss: 0.00077145
Iteration 17/25 | Loss: 0.00077145
Iteration 18/25 | Loss: 0.00077145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007714489474892616, 0.0007714489474892616, 0.0007714489474892616, 0.0007714489474892616, 0.0007714489474892616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007714489474892616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077145
Iteration 2/1000 | Loss: 0.00004639
Iteration 3/1000 | Loss: 0.00003224
Iteration 4/1000 | Loss: 0.00002661
Iteration 5/1000 | Loss: 0.00002515
Iteration 6/1000 | Loss: 0.00002406
Iteration 7/1000 | Loss: 0.00002330
Iteration 8/1000 | Loss: 0.00002258
Iteration 9/1000 | Loss: 0.00002207
Iteration 10/1000 | Loss: 0.00002174
Iteration 11/1000 | Loss: 0.00002156
Iteration 12/1000 | Loss: 0.00002140
Iteration 13/1000 | Loss: 0.00002122
Iteration 14/1000 | Loss: 0.00002115
Iteration 15/1000 | Loss: 0.00002111
Iteration 16/1000 | Loss: 0.00002110
Iteration 17/1000 | Loss: 0.00002110
Iteration 18/1000 | Loss: 0.00002109
Iteration 19/1000 | Loss: 0.00002105
Iteration 20/1000 | Loss: 0.00002100
Iteration 21/1000 | Loss: 0.00002100
Iteration 22/1000 | Loss: 0.00002099
Iteration 23/1000 | Loss: 0.00002090
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002081
Iteration 26/1000 | Loss: 0.00002081
Iteration 27/1000 | Loss: 0.00002080
Iteration 28/1000 | Loss: 0.00002080
Iteration 29/1000 | Loss: 0.00002080
Iteration 30/1000 | Loss: 0.00002080
Iteration 31/1000 | Loss: 0.00002080
Iteration 32/1000 | Loss: 0.00002079
Iteration 33/1000 | Loss: 0.00002079
Iteration 34/1000 | Loss: 0.00002078
Iteration 35/1000 | Loss: 0.00002078
Iteration 36/1000 | Loss: 0.00002077
Iteration 37/1000 | Loss: 0.00002077
Iteration 38/1000 | Loss: 0.00002076
Iteration 39/1000 | Loss: 0.00002076
Iteration 40/1000 | Loss: 0.00002076
Iteration 41/1000 | Loss: 0.00002075
Iteration 42/1000 | Loss: 0.00002075
Iteration 43/1000 | Loss: 0.00002074
Iteration 44/1000 | Loss: 0.00002074
Iteration 45/1000 | Loss: 0.00002073
Iteration 46/1000 | Loss: 0.00002073
Iteration 47/1000 | Loss: 0.00002073
Iteration 48/1000 | Loss: 0.00002073
Iteration 49/1000 | Loss: 0.00002072
Iteration 50/1000 | Loss: 0.00002072
Iteration 51/1000 | Loss: 0.00002072
Iteration 52/1000 | Loss: 0.00002072
Iteration 53/1000 | Loss: 0.00002071
Iteration 54/1000 | Loss: 0.00002071
Iteration 55/1000 | Loss: 0.00002071
Iteration 56/1000 | Loss: 0.00002070
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002069
Iteration 59/1000 | Loss: 0.00002069
Iteration 60/1000 | Loss: 0.00002069
Iteration 61/1000 | Loss: 0.00002069
Iteration 62/1000 | Loss: 0.00002068
Iteration 63/1000 | Loss: 0.00002068
Iteration 64/1000 | Loss: 0.00002068
Iteration 65/1000 | Loss: 0.00002068
Iteration 66/1000 | Loss: 0.00002067
Iteration 67/1000 | Loss: 0.00002067
Iteration 68/1000 | Loss: 0.00002065
Iteration 69/1000 | Loss: 0.00002065
Iteration 70/1000 | Loss: 0.00002065
Iteration 71/1000 | Loss: 0.00002065
Iteration 72/1000 | Loss: 0.00002065
Iteration 73/1000 | Loss: 0.00002065
Iteration 74/1000 | Loss: 0.00002065
Iteration 75/1000 | Loss: 0.00002065
Iteration 76/1000 | Loss: 0.00002065
Iteration 77/1000 | Loss: 0.00002065
Iteration 78/1000 | Loss: 0.00002064
Iteration 79/1000 | Loss: 0.00002064
Iteration 80/1000 | Loss: 0.00002064
Iteration 81/1000 | Loss: 0.00002064
Iteration 82/1000 | Loss: 0.00002064
Iteration 83/1000 | Loss: 0.00002063
Iteration 84/1000 | Loss: 0.00002063
Iteration 85/1000 | Loss: 0.00002062
Iteration 86/1000 | Loss: 0.00002062
Iteration 87/1000 | Loss: 0.00002061
Iteration 88/1000 | Loss: 0.00002061
Iteration 89/1000 | Loss: 0.00002061
Iteration 90/1000 | Loss: 0.00002061
Iteration 91/1000 | Loss: 0.00002061
Iteration 92/1000 | Loss: 0.00002061
Iteration 93/1000 | Loss: 0.00002061
Iteration 94/1000 | Loss: 0.00002060
Iteration 95/1000 | Loss: 0.00002060
Iteration 96/1000 | Loss: 0.00002060
Iteration 97/1000 | Loss: 0.00002060
Iteration 98/1000 | Loss: 0.00002060
Iteration 99/1000 | Loss: 0.00002060
Iteration 100/1000 | Loss: 0.00002060
Iteration 101/1000 | Loss: 0.00002059
Iteration 102/1000 | Loss: 0.00002059
Iteration 103/1000 | Loss: 0.00002059
Iteration 104/1000 | Loss: 0.00002059
Iteration 105/1000 | Loss: 0.00002059
Iteration 106/1000 | Loss: 0.00002059
Iteration 107/1000 | Loss: 0.00002059
Iteration 108/1000 | Loss: 0.00002059
Iteration 109/1000 | Loss: 0.00002059
Iteration 110/1000 | Loss: 0.00002058
Iteration 111/1000 | Loss: 0.00002058
Iteration 112/1000 | Loss: 0.00002058
Iteration 113/1000 | Loss: 0.00002058
Iteration 114/1000 | Loss: 0.00002058
Iteration 115/1000 | Loss: 0.00002058
Iteration 116/1000 | Loss: 0.00002058
Iteration 117/1000 | Loss: 0.00002058
Iteration 118/1000 | Loss: 0.00002058
Iteration 119/1000 | Loss: 0.00002058
Iteration 120/1000 | Loss: 0.00002057
Iteration 121/1000 | Loss: 0.00002057
Iteration 122/1000 | Loss: 0.00002057
Iteration 123/1000 | Loss: 0.00002057
Iteration 124/1000 | Loss: 0.00002057
Iteration 125/1000 | Loss: 0.00002057
Iteration 126/1000 | Loss: 0.00002057
Iteration 127/1000 | Loss: 0.00002057
Iteration 128/1000 | Loss: 0.00002056
Iteration 129/1000 | Loss: 0.00002056
Iteration 130/1000 | Loss: 0.00002056
Iteration 131/1000 | Loss: 0.00002056
Iteration 132/1000 | Loss: 0.00002056
Iteration 133/1000 | Loss: 0.00002056
Iteration 134/1000 | Loss: 0.00002056
Iteration 135/1000 | Loss: 0.00002056
Iteration 136/1000 | Loss: 0.00002056
Iteration 137/1000 | Loss: 0.00002055
Iteration 138/1000 | Loss: 0.00002055
Iteration 139/1000 | Loss: 0.00002055
Iteration 140/1000 | Loss: 0.00002055
Iteration 141/1000 | Loss: 0.00002055
Iteration 142/1000 | Loss: 0.00002055
Iteration 143/1000 | Loss: 0.00002055
Iteration 144/1000 | Loss: 0.00002055
Iteration 145/1000 | Loss: 0.00002055
Iteration 146/1000 | Loss: 0.00002055
Iteration 147/1000 | Loss: 0.00002055
Iteration 148/1000 | Loss: 0.00002055
Iteration 149/1000 | Loss: 0.00002054
Iteration 150/1000 | Loss: 0.00002054
Iteration 151/1000 | Loss: 0.00002054
Iteration 152/1000 | Loss: 0.00002054
Iteration 153/1000 | Loss: 0.00002054
Iteration 154/1000 | Loss: 0.00002053
Iteration 155/1000 | Loss: 0.00002053
Iteration 156/1000 | Loss: 0.00002053
Iteration 157/1000 | Loss: 0.00002053
Iteration 158/1000 | Loss: 0.00002052
Iteration 159/1000 | Loss: 0.00002052
Iteration 160/1000 | Loss: 0.00002052
Iteration 161/1000 | Loss: 0.00002052
Iteration 162/1000 | Loss: 0.00002052
Iteration 163/1000 | Loss: 0.00002051
Iteration 164/1000 | Loss: 0.00002051
Iteration 165/1000 | Loss: 0.00002051
Iteration 166/1000 | Loss: 0.00002051
Iteration 167/1000 | Loss: 0.00002051
Iteration 168/1000 | Loss: 0.00002051
Iteration 169/1000 | Loss: 0.00002051
Iteration 170/1000 | Loss: 0.00002051
Iteration 171/1000 | Loss: 0.00002051
Iteration 172/1000 | Loss: 0.00002051
Iteration 173/1000 | Loss: 0.00002051
Iteration 174/1000 | Loss: 0.00002051
Iteration 175/1000 | Loss: 0.00002051
Iteration 176/1000 | Loss: 0.00002051
Iteration 177/1000 | Loss: 0.00002051
Iteration 178/1000 | Loss: 0.00002051
Iteration 179/1000 | Loss: 0.00002051
Iteration 180/1000 | Loss: 0.00002051
Iteration 181/1000 | Loss: 0.00002051
Iteration 182/1000 | Loss: 0.00002051
Iteration 183/1000 | Loss: 0.00002051
Iteration 184/1000 | Loss: 0.00002051
Iteration 185/1000 | Loss: 0.00002051
Iteration 186/1000 | Loss: 0.00002051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.0507297449512407e-05, 2.0507297449512407e-05, 2.0507297449512407e-05, 2.0507297449512407e-05, 2.0507297449512407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0507297449512407e-05

Optimization complete. Final v2v error: 3.779193878173828 mm

Highest mean error: 5.283156871795654 mm for frame 67

Lowest mean error: 3.221355676651001 mm for frame 98

Saving results

Total time: 41.813703536987305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058125
Iteration 2/25 | Loss: 0.00187036
Iteration 3/25 | Loss: 0.00143926
Iteration 4/25 | Loss: 0.00132428
Iteration 5/25 | Loss: 0.00132469
Iteration 6/25 | Loss: 0.00130919
Iteration 7/25 | Loss: 0.00130664
Iteration 8/25 | Loss: 0.00128411
Iteration 9/25 | Loss: 0.00126918
Iteration 10/25 | Loss: 0.00124934
Iteration 11/25 | Loss: 0.00125166
Iteration 12/25 | Loss: 0.00124447
Iteration 13/25 | Loss: 0.00124075
Iteration 14/25 | Loss: 0.00123904
Iteration 15/25 | Loss: 0.00123966
Iteration 16/25 | Loss: 0.00123926
Iteration 17/25 | Loss: 0.00124320
Iteration 18/25 | Loss: 0.00124110
Iteration 19/25 | Loss: 0.00123830
Iteration 20/25 | Loss: 0.00123791
Iteration 21/25 | Loss: 0.00124458
Iteration 22/25 | Loss: 0.00124608
Iteration 23/25 | Loss: 0.00124264
Iteration 24/25 | Loss: 0.00124272
Iteration 25/25 | Loss: 0.00124148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.58731747
Iteration 2/25 | Loss: 0.00087240
Iteration 3/25 | Loss: 0.00084678
Iteration 4/25 | Loss: 0.00084678
Iteration 5/25 | Loss: 0.00084678
Iteration 6/25 | Loss: 0.00084678
Iteration 7/25 | Loss: 0.00084678
Iteration 8/25 | Loss: 0.00084678
Iteration 9/25 | Loss: 0.00084678
Iteration 10/25 | Loss: 0.00084678
Iteration 11/25 | Loss: 0.00084678
Iteration 12/25 | Loss: 0.00084678
Iteration 13/25 | Loss: 0.00084678
Iteration 14/25 | Loss: 0.00084678
Iteration 15/25 | Loss: 0.00084678
Iteration 16/25 | Loss: 0.00084678
Iteration 17/25 | Loss: 0.00084678
Iteration 18/25 | Loss: 0.00084678
Iteration 19/25 | Loss: 0.00084678
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008467776933684945, 0.0008467776933684945, 0.0008467776933684945, 0.0008467776933684945, 0.0008467776933684945]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008467776933684945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084678
Iteration 2/1000 | Loss: 0.00029385
Iteration 3/1000 | Loss: 0.00017228
Iteration 4/1000 | Loss: 0.00006201
Iteration 5/1000 | Loss: 0.00008363
Iteration 6/1000 | Loss: 0.00004198
Iteration 7/1000 | Loss: 0.00005765
Iteration 8/1000 | Loss: 0.00009293
Iteration 9/1000 | Loss: 0.00020262
Iteration 10/1000 | Loss: 0.00011045
Iteration 11/1000 | Loss: 0.00013275
Iteration 12/1000 | Loss: 0.00003231
Iteration 13/1000 | Loss: 0.00008631
Iteration 14/1000 | Loss: 0.00003432
Iteration 15/1000 | Loss: 0.00005866
Iteration 16/1000 | Loss: 0.00005091
Iteration 17/1000 | Loss: 0.00010079
Iteration 18/1000 | Loss: 0.00003409
Iteration 19/1000 | Loss: 0.00011126
Iteration 20/1000 | Loss: 0.00035641
Iteration 21/1000 | Loss: 0.00036069
Iteration 22/1000 | Loss: 0.00007704
Iteration 23/1000 | Loss: 0.00006459
Iteration 24/1000 | Loss: 0.00005446
Iteration 25/1000 | Loss: 0.00002246
Iteration 26/1000 | Loss: 0.00002170
Iteration 27/1000 | Loss: 0.00002000
Iteration 28/1000 | Loss: 0.00001927
Iteration 29/1000 | Loss: 0.00001879
Iteration 30/1000 | Loss: 0.00009460
Iteration 31/1000 | Loss: 0.00001961
Iteration 32/1000 | Loss: 0.00003290
Iteration 33/1000 | Loss: 0.00001853
Iteration 34/1000 | Loss: 0.00003311
Iteration 35/1000 | Loss: 0.00003310
Iteration 36/1000 | Loss: 0.00014669
Iteration 37/1000 | Loss: 0.00003709
Iteration 38/1000 | Loss: 0.00001800
Iteration 39/1000 | Loss: 0.00001770
Iteration 40/1000 | Loss: 0.00001761
Iteration 41/1000 | Loss: 0.00001737
Iteration 42/1000 | Loss: 0.00001718
Iteration 43/1000 | Loss: 0.00001707
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001703
Iteration 46/1000 | Loss: 0.00001703
Iteration 47/1000 | Loss: 0.00001702
Iteration 48/1000 | Loss: 0.00001702
Iteration 49/1000 | Loss: 0.00001702
Iteration 50/1000 | Loss: 0.00001701
Iteration 51/1000 | Loss: 0.00001700
Iteration 52/1000 | Loss: 0.00001700
Iteration 53/1000 | Loss: 0.00001699
Iteration 54/1000 | Loss: 0.00001699
Iteration 55/1000 | Loss: 0.00001698
Iteration 56/1000 | Loss: 0.00001698
Iteration 57/1000 | Loss: 0.00001698
Iteration 58/1000 | Loss: 0.00001697
Iteration 59/1000 | Loss: 0.00001697
Iteration 60/1000 | Loss: 0.00001696
Iteration 61/1000 | Loss: 0.00001696
Iteration 62/1000 | Loss: 0.00001696
Iteration 63/1000 | Loss: 0.00001695
Iteration 64/1000 | Loss: 0.00001695
Iteration 65/1000 | Loss: 0.00001695
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001695
Iteration 68/1000 | Loss: 0.00001694
Iteration 69/1000 | Loss: 0.00001694
Iteration 70/1000 | Loss: 0.00001694
Iteration 71/1000 | Loss: 0.00003003
Iteration 72/1000 | Loss: 0.00001694
Iteration 73/1000 | Loss: 0.00001692
Iteration 74/1000 | Loss: 0.00001692
Iteration 75/1000 | Loss: 0.00001691
Iteration 76/1000 | Loss: 0.00001691
Iteration 77/1000 | Loss: 0.00001691
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001690
Iteration 80/1000 | Loss: 0.00001690
Iteration 81/1000 | Loss: 0.00001690
Iteration 82/1000 | Loss: 0.00001689
Iteration 83/1000 | Loss: 0.00001689
Iteration 84/1000 | Loss: 0.00001689
Iteration 85/1000 | Loss: 0.00002350
Iteration 86/1000 | Loss: 0.00002120
Iteration 87/1000 | Loss: 0.00002207
Iteration 88/1000 | Loss: 0.00001687
Iteration 89/1000 | Loss: 0.00001687
Iteration 90/1000 | Loss: 0.00001687
Iteration 91/1000 | Loss: 0.00001687
Iteration 92/1000 | Loss: 0.00001687
Iteration 93/1000 | Loss: 0.00001687
Iteration 94/1000 | Loss: 0.00001687
Iteration 95/1000 | Loss: 0.00001687
Iteration 96/1000 | Loss: 0.00001687
Iteration 97/1000 | Loss: 0.00001687
Iteration 98/1000 | Loss: 0.00001686
Iteration 99/1000 | Loss: 0.00001686
Iteration 100/1000 | Loss: 0.00001685
Iteration 101/1000 | Loss: 0.00001685
Iteration 102/1000 | Loss: 0.00001685
Iteration 103/1000 | Loss: 0.00001685
Iteration 104/1000 | Loss: 0.00001685
Iteration 105/1000 | Loss: 0.00001685
Iteration 106/1000 | Loss: 0.00001685
Iteration 107/1000 | Loss: 0.00001685
Iteration 108/1000 | Loss: 0.00001685
Iteration 109/1000 | Loss: 0.00001684
Iteration 110/1000 | Loss: 0.00001684
Iteration 111/1000 | Loss: 0.00001684
Iteration 112/1000 | Loss: 0.00001684
Iteration 113/1000 | Loss: 0.00001684
Iteration 114/1000 | Loss: 0.00001684
Iteration 115/1000 | Loss: 0.00001684
Iteration 116/1000 | Loss: 0.00001684
Iteration 117/1000 | Loss: 0.00001684
Iteration 118/1000 | Loss: 0.00001683
Iteration 119/1000 | Loss: 0.00001683
Iteration 120/1000 | Loss: 0.00001683
Iteration 121/1000 | Loss: 0.00001683
Iteration 122/1000 | Loss: 0.00001683
Iteration 123/1000 | Loss: 0.00001682
Iteration 124/1000 | Loss: 0.00001682
Iteration 125/1000 | Loss: 0.00001682
Iteration 126/1000 | Loss: 0.00001682
Iteration 127/1000 | Loss: 0.00001682
Iteration 128/1000 | Loss: 0.00001682
Iteration 129/1000 | Loss: 0.00001682
Iteration 130/1000 | Loss: 0.00001681
Iteration 131/1000 | Loss: 0.00001681
Iteration 132/1000 | Loss: 0.00001681
Iteration 133/1000 | Loss: 0.00001681
Iteration 134/1000 | Loss: 0.00001680
Iteration 135/1000 | Loss: 0.00001680
Iteration 136/1000 | Loss: 0.00001680
Iteration 137/1000 | Loss: 0.00001680
Iteration 138/1000 | Loss: 0.00001679
Iteration 139/1000 | Loss: 0.00001679
Iteration 140/1000 | Loss: 0.00001679
Iteration 141/1000 | Loss: 0.00001679
Iteration 142/1000 | Loss: 0.00001679
Iteration 143/1000 | Loss: 0.00001679
Iteration 144/1000 | Loss: 0.00001678
Iteration 145/1000 | Loss: 0.00001678
Iteration 146/1000 | Loss: 0.00001678
Iteration 147/1000 | Loss: 0.00001678
Iteration 148/1000 | Loss: 0.00001678
Iteration 149/1000 | Loss: 0.00001677
Iteration 150/1000 | Loss: 0.00001677
Iteration 151/1000 | Loss: 0.00001677
Iteration 152/1000 | Loss: 0.00001677
Iteration 153/1000 | Loss: 0.00001677
Iteration 154/1000 | Loss: 0.00001676
Iteration 155/1000 | Loss: 0.00001676
Iteration 156/1000 | Loss: 0.00001676
Iteration 157/1000 | Loss: 0.00001676
Iteration 158/1000 | Loss: 0.00001675
Iteration 159/1000 | Loss: 0.00002505
Iteration 160/1000 | Loss: 0.00001675
Iteration 161/1000 | Loss: 0.00001674
Iteration 162/1000 | Loss: 0.00001674
Iteration 163/1000 | Loss: 0.00001674
Iteration 164/1000 | Loss: 0.00001674
Iteration 165/1000 | Loss: 0.00001674
Iteration 166/1000 | Loss: 0.00001674
Iteration 167/1000 | Loss: 0.00001674
Iteration 168/1000 | Loss: 0.00001674
Iteration 169/1000 | Loss: 0.00001674
Iteration 170/1000 | Loss: 0.00001674
Iteration 171/1000 | Loss: 0.00001674
Iteration 172/1000 | Loss: 0.00001674
Iteration 173/1000 | Loss: 0.00001674
Iteration 174/1000 | Loss: 0.00001673
Iteration 175/1000 | Loss: 0.00001673
Iteration 176/1000 | Loss: 0.00003558
Iteration 177/1000 | Loss: 0.00006641
Iteration 178/1000 | Loss: 0.00028652
Iteration 179/1000 | Loss: 0.00004343
Iteration 180/1000 | Loss: 0.00005295
Iteration 181/1000 | Loss: 0.00001821
Iteration 182/1000 | Loss: 0.00001777
Iteration 183/1000 | Loss: 0.00005051
Iteration 184/1000 | Loss: 0.00001725
Iteration 185/1000 | Loss: 0.00003880
Iteration 186/1000 | Loss: 0.00001689
Iteration 187/1000 | Loss: 0.00001669
Iteration 188/1000 | Loss: 0.00001655
Iteration 189/1000 | Loss: 0.00001653
Iteration 190/1000 | Loss: 0.00004776
Iteration 191/1000 | Loss: 0.00001698
Iteration 192/1000 | Loss: 0.00001643
Iteration 193/1000 | Loss: 0.00001642
Iteration 194/1000 | Loss: 0.00001641
Iteration 195/1000 | Loss: 0.00001641
Iteration 196/1000 | Loss: 0.00001640
Iteration 197/1000 | Loss: 0.00001640
Iteration 198/1000 | Loss: 0.00001640
Iteration 199/1000 | Loss: 0.00001640
Iteration 200/1000 | Loss: 0.00001640
Iteration 201/1000 | Loss: 0.00001640
Iteration 202/1000 | Loss: 0.00001639
Iteration 203/1000 | Loss: 0.00001639
Iteration 204/1000 | Loss: 0.00001639
Iteration 205/1000 | Loss: 0.00001639
Iteration 206/1000 | Loss: 0.00001639
Iteration 207/1000 | Loss: 0.00001639
Iteration 208/1000 | Loss: 0.00001639
Iteration 209/1000 | Loss: 0.00001639
Iteration 210/1000 | Loss: 0.00001639
Iteration 211/1000 | Loss: 0.00001639
Iteration 212/1000 | Loss: 0.00001638
Iteration 213/1000 | Loss: 0.00001638
Iteration 214/1000 | Loss: 0.00001638
Iteration 215/1000 | Loss: 0.00001637
Iteration 216/1000 | Loss: 0.00001637
Iteration 217/1000 | Loss: 0.00001636
Iteration 218/1000 | Loss: 0.00001636
Iteration 219/1000 | Loss: 0.00001636
Iteration 220/1000 | Loss: 0.00001636
Iteration 221/1000 | Loss: 0.00001636
Iteration 222/1000 | Loss: 0.00001636
Iteration 223/1000 | Loss: 0.00003817
Iteration 224/1000 | Loss: 0.00001753
Iteration 225/1000 | Loss: 0.00001837
Iteration 226/1000 | Loss: 0.00001634
Iteration 227/1000 | Loss: 0.00001631
Iteration 228/1000 | Loss: 0.00001631
Iteration 229/1000 | Loss: 0.00001631
Iteration 230/1000 | Loss: 0.00001631
Iteration 231/1000 | Loss: 0.00001631
Iteration 232/1000 | Loss: 0.00001631
Iteration 233/1000 | Loss: 0.00001631
Iteration 234/1000 | Loss: 0.00001631
Iteration 235/1000 | Loss: 0.00001630
Iteration 236/1000 | Loss: 0.00001630
Iteration 237/1000 | Loss: 0.00001630
Iteration 238/1000 | Loss: 0.00001630
Iteration 239/1000 | Loss: 0.00001630
Iteration 240/1000 | Loss: 0.00001630
Iteration 241/1000 | Loss: 0.00001630
Iteration 242/1000 | Loss: 0.00001630
Iteration 243/1000 | Loss: 0.00001630
Iteration 244/1000 | Loss: 0.00001630
Iteration 245/1000 | Loss: 0.00001630
Iteration 246/1000 | Loss: 0.00001630
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [1.6300275092362426e-05, 1.6300275092362426e-05, 1.6300275092362426e-05, 1.6300275092362426e-05, 1.6300275092362426e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6300275092362426e-05

Optimization complete. Final v2v error: 3.4137580394744873 mm

Highest mean error: 5.241263389587402 mm for frame 156

Lowest mean error: 3.0106959342956543 mm for frame 44

Saving results

Total time: 171.31251621246338
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00868126
Iteration 2/25 | Loss: 0.00168644
Iteration 3/25 | Loss: 0.00137715
Iteration 4/25 | Loss: 0.00132131
Iteration 5/25 | Loss: 0.00134002
Iteration 6/25 | Loss: 0.00131990
Iteration 7/25 | Loss: 0.00128380
Iteration 8/25 | Loss: 0.00126376
Iteration 9/25 | Loss: 0.00125322
Iteration 10/25 | Loss: 0.00124877
Iteration 11/25 | Loss: 0.00124755
Iteration 12/25 | Loss: 0.00124713
Iteration 13/25 | Loss: 0.00124702
Iteration 14/25 | Loss: 0.00124700
Iteration 15/25 | Loss: 0.00124700
Iteration 16/25 | Loss: 0.00124700
Iteration 17/25 | Loss: 0.00124700
Iteration 18/25 | Loss: 0.00124700
Iteration 19/25 | Loss: 0.00124700
Iteration 20/25 | Loss: 0.00124700
Iteration 21/25 | Loss: 0.00124700
Iteration 22/25 | Loss: 0.00124700
Iteration 23/25 | Loss: 0.00124700
Iteration 24/25 | Loss: 0.00124700
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001246995059773326, 0.001246995059773326, 0.001246995059773326, 0.001246995059773326, 0.001246995059773326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001246995059773326

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29839134
Iteration 2/25 | Loss: 0.00079713
Iteration 3/25 | Loss: 0.00079713
Iteration 4/25 | Loss: 0.00079713
Iteration 5/25 | Loss: 0.00079713
Iteration 6/25 | Loss: 0.00079713
Iteration 7/25 | Loss: 0.00079713
Iteration 8/25 | Loss: 0.00079713
Iteration 9/25 | Loss: 0.00079712
Iteration 10/25 | Loss: 0.00079712
Iteration 11/25 | Loss: 0.00079712
Iteration 12/25 | Loss: 0.00079712
Iteration 13/25 | Loss: 0.00079712
Iteration 14/25 | Loss: 0.00079712
Iteration 15/25 | Loss: 0.00079712
Iteration 16/25 | Loss: 0.00079712
Iteration 17/25 | Loss: 0.00079712
Iteration 18/25 | Loss: 0.00079712
Iteration 19/25 | Loss: 0.00079712
Iteration 20/25 | Loss: 0.00079712
Iteration 21/25 | Loss: 0.00079712
Iteration 22/25 | Loss: 0.00079712
Iteration 23/25 | Loss: 0.00079712
Iteration 24/25 | Loss: 0.00079712
Iteration 25/25 | Loss: 0.00079712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079712
Iteration 2/1000 | Loss: 0.00003388
Iteration 3/1000 | Loss: 0.00002668
Iteration 4/1000 | Loss: 0.00002460
Iteration 5/1000 | Loss: 0.00002342
Iteration 6/1000 | Loss: 0.00002256
Iteration 7/1000 | Loss: 0.00002211
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002125
Iteration 10/1000 | Loss: 0.00002103
Iteration 11/1000 | Loss: 0.00002081
Iteration 12/1000 | Loss: 0.00002066
Iteration 13/1000 | Loss: 0.00002064
Iteration 14/1000 | Loss: 0.00002056
Iteration 15/1000 | Loss: 0.00002053
Iteration 16/1000 | Loss: 0.00002052
Iteration 17/1000 | Loss: 0.00002038
Iteration 18/1000 | Loss: 0.00002038
Iteration 19/1000 | Loss: 0.00002034
Iteration 20/1000 | Loss: 0.00002034
Iteration 21/1000 | Loss: 0.00002033
Iteration 22/1000 | Loss: 0.00002027
Iteration 23/1000 | Loss: 0.00002027
Iteration 24/1000 | Loss: 0.00002027
Iteration 25/1000 | Loss: 0.00002027
Iteration 26/1000 | Loss: 0.00002025
Iteration 27/1000 | Loss: 0.00002025
Iteration 28/1000 | Loss: 0.00002025
Iteration 29/1000 | Loss: 0.00002024
Iteration 30/1000 | Loss: 0.00002023
Iteration 31/1000 | Loss: 0.00002023
Iteration 32/1000 | Loss: 0.00002022
Iteration 33/1000 | Loss: 0.00002022
Iteration 34/1000 | Loss: 0.00002022
Iteration 35/1000 | Loss: 0.00002021
Iteration 36/1000 | Loss: 0.00002021
Iteration 37/1000 | Loss: 0.00002021
Iteration 38/1000 | Loss: 0.00002021
Iteration 39/1000 | Loss: 0.00002021
Iteration 40/1000 | Loss: 0.00002021
Iteration 41/1000 | Loss: 0.00002020
Iteration 42/1000 | Loss: 0.00002020
Iteration 43/1000 | Loss: 0.00002020
Iteration 44/1000 | Loss: 0.00002020
Iteration 45/1000 | Loss: 0.00002020
Iteration 46/1000 | Loss: 0.00002019
Iteration 47/1000 | Loss: 0.00002018
Iteration 48/1000 | Loss: 0.00002018
Iteration 49/1000 | Loss: 0.00002017
Iteration 50/1000 | Loss: 0.00002017
Iteration 51/1000 | Loss: 0.00002016
Iteration 52/1000 | Loss: 0.00002014
Iteration 53/1000 | Loss: 0.00002014
Iteration 54/1000 | Loss: 0.00002013
Iteration 55/1000 | Loss: 0.00002012
Iteration 56/1000 | Loss: 0.00002012
Iteration 57/1000 | Loss: 0.00002012
Iteration 58/1000 | Loss: 0.00002010
Iteration 59/1000 | Loss: 0.00002010
Iteration 60/1000 | Loss: 0.00002009
Iteration 61/1000 | Loss: 0.00002009
Iteration 62/1000 | Loss: 0.00002007
Iteration 63/1000 | Loss: 0.00002007
Iteration 64/1000 | Loss: 0.00002006
Iteration 65/1000 | Loss: 0.00002006
Iteration 66/1000 | Loss: 0.00002005
Iteration 67/1000 | Loss: 0.00002005
Iteration 68/1000 | Loss: 0.00002005
Iteration 69/1000 | Loss: 0.00002005
Iteration 70/1000 | Loss: 0.00002005
Iteration 71/1000 | Loss: 0.00002005
Iteration 72/1000 | Loss: 0.00002004
Iteration 73/1000 | Loss: 0.00002004
Iteration 74/1000 | Loss: 0.00002003
Iteration 75/1000 | Loss: 0.00002003
Iteration 76/1000 | Loss: 0.00002003
Iteration 77/1000 | Loss: 0.00002003
Iteration 78/1000 | Loss: 0.00002003
Iteration 79/1000 | Loss: 0.00002003
Iteration 80/1000 | Loss: 0.00002002
Iteration 81/1000 | Loss: 0.00002002
Iteration 82/1000 | Loss: 0.00002002
Iteration 83/1000 | Loss: 0.00002002
Iteration 84/1000 | Loss: 0.00002002
Iteration 85/1000 | Loss: 0.00002002
Iteration 86/1000 | Loss: 0.00002002
Iteration 87/1000 | Loss: 0.00002002
Iteration 88/1000 | Loss: 0.00002001
Iteration 89/1000 | Loss: 0.00002001
Iteration 90/1000 | Loss: 0.00002001
Iteration 91/1000 | Loss: 0.00002001
Iteration 92/1000 | Loss: 0.00002000
Iteration 93/1000 | Loss: 0.00002000
Iteration 94/1000 | Loss: 0.00002000
Iteration 95/1000 | Loss: 0.00002000
Iteration 96/1000 | Loss: 0.00002000
Iteration 97/1000 | Loss: 0.00002000
Iteration 98/1000 | Loss: 0.00002000
Iteration 99/1000 | Loss: 0.00002000
Iteration 100/1000 | Loss: 0.00002000
Iteration 101/1000 | Loss: 0.00002000
Iteration 102/1000 | Loss: 0.00002000
Iteration 103/1000 | Loss: 0.00001999
Iteration 104/1000 | Loss: 0.00001999
Iteration 105/1000 | Loss: 0.00001999
Iteration 106/1000 | Loss: 0.00001999
Iteration 107/1000 | Loss: 0.00001999
Iteration 108/1000 | Loss: 0.00001998
Iteration 109/1000 | Loss: 0.00001998
Iteration 110/1000 | Loss: 0.00001998
Iteration 111/1000 | Loss: 0.00001998
Iteration 112/1000 | Loss: 0.00001998
Iteration 113/1000 | Loss: 0.00001998
Iteration 114/1000 | Loss: 0.00001997
Iteration 115/1000 | Loss: 0.00001997
Iteration 116/1000 | Loss: 0.00001997
Iteration 117/1000 | Loss: 0.00001997
Iteration 118/1000 | Loss: 0.00001997
Iteration 119/1000 | Loss: 0.00001997
Iteration 120/1000 | Loss: 0.00001997
Iteration 121/1000 | Loss: 0.00001997
Iteration 122/1000 | Loss: 0.00001997
Iteration 123/1000 | Loss: 0.00001997
Iteration 124/1000 | Loss: 0.00001996
Iteration 125/1000 | Loss: 0.00001996
Iteration 126/1000 | Loss: 0.00001996
Iteration 127/1000 | Loss: 0.00001996
Iteration 128/1000 | Loss: 0.00001996
Iteration 129/1000 | Loss: 0.00001996
Iteration 130/1000 | Loss: 0.00001996
Iteration 131/1000 | Loss: 0.00001996
Iteration 132/1000 | Loss: 0.00001996
Iteration 133/1000 | Loss: 0.00001996
Iteration 134/1000 | Loss: 0.00001996
Iteration 135/1000 | Loss: 0.00001996
Iteration 136/1000 | Loss: 0.00001996
Iteration 137/1000 | Loss: 0.00001996
Iteration 138/1000 | Loss: 0.00001996
Iteration 139/1000 | Loss: 0.00001996
Iteration 140/1000 | Loss: 0.00001996
Iteration 141/1000 | Loss: 0.00001996
Iteration 142/1000 | Loss: 0.00001996
Iteration 143/1000 | Loss: 0.00001996
Iteration 144/1000 | Loss: 0.00001996
Iteration 145/1000 | Loss: 0.00001996
Iteration 146/1000 | Loss: 0.00001996
Iteration 147/1000 | Loss: 0.00001996
Iteration 148/1000 | Loss: 0.00001996
Iteration 149/1000 | Loss: 0.00001996
Iteration 150/1000 | Loss: 0.00001996
Iteration 151/1000 | Loss: 0.00001996
Iteration 152/1000 | Loss: 0.00001996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.9959965356974863e-05, 1.9959965356974863e-05, 1.9959965356974863e-05, 1.9959965356974863e-05, 1.9959965356974863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9959965356974863e-05

Optimization complete. Final v2v error: 3.7195072174072266 mm

Highest mean error: 4.215980052947998 mm for frame 94

Lowest mean error: 3.270475387573242 mm for frame 19

Saving results

Total time: 59.706369161605835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00541070
Iteration 2/25 | Loss: 0.00134848
Iteration 3/25 | Loss: 0.00126321
Iteration 4/25 | Loss: 0.00125015
Iteration 5/25 | Loss: 0.00124693
Iteration 6/25 | Loss: 0.00124693
Iteration 7/25 | Loss: 0.00124693
Iteration 8/25 | Loss: 0.00124693
Iteration 9/25 | Loss: 0.00124693
Iteration 10/25 | Loss: 0.00124693
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012469312641769648, 0.0012469312641769648, 0.0012469312641769648, 0.0012469312641769648, 0.0012469312641769648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012469312641769648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.03643036
Iteration 2/25 | Loss: 0.00073726
Iteration 3/25 | Loss: 0.00073725
Iteration 4/25 | Loss: 0.00073725
Iteration 5/25 | Loss: 0.00073725
Iteration 6/25 | Loss: 0.00073725
Iteration 7/25 | Loss: 0.00073725
Iteration 8/25 | Loss: 0.00073725
Iteration 9/25 | Loss: 0.00073725
Iteration 10/25 | Loss: 0.00073725
Iteration 11/25 | Loss: 0.00073725
Iteration 12/25 | Loss: 0.00073725
Iteration 13/25 | Loss: 0.00073725
Iteration 14/25 | Loss: 0.00073725
Iteration 15/25 | Loss: 0.00073725
Iteration 16/25 | Loss: 0.00073725
Iteration 17/25 | Loss: 0.00073725
Iteration 18/25 | Loss: 0.00073725
Iteration 19/25 | Loss: 0.00073725
Iteration 20/25 | Loss: 0.00073725
Iteration 21/25 | Loss: 0.00073725
Iteration 22/25 | Loss: 0.00073725
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0007372494437731802, 0.0007372494437731802, 0.0007372494437731802, 0.0007372494437731802, 0.0007372494437731802]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007372494437731802

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073725
Iteration 2/1000 | Loss: 0.00002962
Iteration 3/1000 | Loss: 0.00002288
Iteration 4/1000 | Loss: 0.00002125
Iteration 5/1000 | Loss: 0.00002022
Iteration 6/1000 | Loss: 0.00001955
Iteration 7/1000 | Loss: 0.00001890
Iteration 8/1000 | Loss: 0.00001862
Iteration 9/1000 | Loss: 0.00001822
Iteration 10/1000 | Loss: 0.00001797
Iteration 11/1000 | Loss: 0.00001796
Iteration 12/1000 | Loss: 0.00001781
Iteration 13/1000 | Loss: 0.00001761
Iteration 14/1000 | Loss: 0.00001752
Iteration 15/1000 | Loss: 0.00001744
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00001742
Iteration 19/1000 | Loss: 0.00001737
Iteration 20/1000 | Loss: 0.00001737
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001733
Iteration 23/1000 | Loss: 0.00001732
Iteration 24/1000 | Loss: 0.00001731
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001730
Iteration 27/1000 | Loss: 0.00001727
Iteration 28/1000 | Loss: 0.00001726
Iteration 29/1000 | Loss: 0.00001725
Iteration 30/1000 | Loss: 0.00001725
Iteration 31/1000 | Loss: 0.00001724
Iteration 32/1000 | Loss: 0.00001723
Iteration 33/1000 | Loss: 0.00001721
Iteration 34/1000 | Loss: 0.00001719
Iteration 35/1000 | Loss: 0.00001719
Iteration 36/1000 | Loss: 0.00001718
Iteration 37/1000 | Loss: 0.00001718
Iteration 38/1000 | Loss: 0.00001717
Iteration 39/1000 | Loss: 0.00001717
Iteration 40/1000 | Loss: 0.00001716
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001714
Iteration 47/1000 | Loss: 0.00001713
Iteration 48/1000 | Loss: 0.00001713
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001712
Iteration 51/1000 | Loss: 0.00001711
Iteration 52/1000 | Loss: 0.00001711
Iteration 53/1000 | Loss: 0.00001710
Iteration 54/1000 | Loss: 0.00001710
Iteration 55/1000 | Loss: 0.00001710
Iteration 56/1000 | Loss: 0.00001710
Iteration 57/1000 | Loss: 0.00001709
Iteration 58/1000 | Loss: 0.00001709
Iteration 59/1000 | Loss: 0.00001709
Iteration 60/1000 | Loss: 0.00001709
Iteration 61/1000 | Loss: 0.00001709
Iteration 62/1000 | Loss: 0.00001709
Iteration 63/1000 | Loss: 0.00001709
Iteration 64/1000 | Loss: 0.00001708
Iteration 65/1000 | Loss: 0.00001708
Iteration 66/1000 | Loss: 0.00001708
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001706
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001706
Iteration 72/1000 | Loss: 0.00001706
Iteration 73/1000 | Loss: 0.00001706
Iteration 74/1000 | Loss: 0.00001706
Iteration 75/1000 | Loss: 0.00001706
Iteration 76/1000 | Loss: 0.00001705
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001705
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001705
Iteration 81/1000 | Loss: 0.00001705
Iteration 82/1000 | Loss: 0.00001705
Iteration 83/1000 | Loss: 0.00001705
Iteration 84/1000 | Loss: 0.00001705
Iteration 85/1000 | Loss: 0.00001705
Iteration 86/1000 | Loss: 0.00001704
Iteration 87/1000 | Loss: 0.00001704
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001701
Iteration 90/1000 | Loss: 0.00001701
Iteration 91/1000 | Loss: 0.00001701
Iteration 92/1000 | Loss: 0.00001701
Iteration 93/1000 | Loss: 0.00001701
Iteration 94/1000 | Loss: 0.00001701
Iteration 95/1000 | Loss: 0.00001701
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001700
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001699
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001698
Iteration 108/1000 | Loss: 0.00001698
Iteration 109/1000 | Loss: 0.00001697
Iteration 110/1000 | Loss: 0.00001697
Iteration 111/1000 | Loss: 0.00001697
Iteration 112/1000 | Loss: 0.00001697
Iteration 113/1000 | Loss: 0.00001697
Iteration 114/1000 | Loss: 0.00001696
Iteration 115/1000 | Loss: 0.00001696
Iteration 116/1000 | Loss: 0.00001696
Iteration 117/1000 | Loss: 0.00001696
Iteration 118/1000 | Loss: 0.00001696
Iteration 119/1000 | Loss: 0.00001696
Iteration 120/1000 | Loss: 0.00001696
Iteration 121/1000 | Loss: 0.00001696
Iteration 122/1000 | Loss: 0.00001696
Iteration 123/1000 | Loss: 0.00001696
Iteration 124/1000 | Loss: 0.00001695
Iteration 125/1000 | Loss: 0.00001695
Iteration 126/1000 | Loss: 0.00001695
Iteration 127/1000 | Loss: 0.00001694
Iteration 128/1000 | Loss: 0.00001694
Iteration 129/1000 | Loss: 0.00001694
Iteration 130/1000 | Loss: 0.00001694
Iteration 131/1000 | Loss: 0.00001694
Iteration 132/1000 | Loss: 0.00001694
Iteration 133/1000 | Loss: 0.00001694
Iteration 134/1000 | Loss: 0.00001694
Iteration 135/1000 | Loss: 0.00001694
Iteration 136/1000 | Loss: 0.00001694
Iteration 137/1000 | Loss: 0.00001694
Iteration 138/1000 | Loss: 0.00001693
Iteration 139/1000 | Loss: 0.00001693
Iteration 140/1000 | Loss: 0.00001693
Iteration 141/1000 | Loss: 0.00001693
Iteration 142/1000 | Loss: 0.00001693
Iteration 143/1000 | Loss: 0.00001693
Iteration 144/1000 | Loss: 0.00001693
Iteration 145/1000 | Loss: 0.00001693
Iteration 146/1000 | Loss: 0.00001693
Iteration 147/1000 | Loss: 0.00001692
Iteration 148/1000 | Loss: 0.00001692
Iteration 149/1000 | Loss: 0.00001692
Iteration 150/1000 | Loss: 0.00001692
Iteration 151/1000 | Loss: 0.00001691
Iteration 152/1000 | Loss: 0.00001691
Iteration 153/1000 | Loss: 0.00001691
Iteration 154/1000 | Loss: 0.00001691
Iteration 155/1000 | Loss: 0.00001691
Iteration 156/1000 | Loss: 0.00001691
Iteration 157/1000 | Loss: 0.00001691
Iteration 158/1000 | Loss: 0.00001690
Iteration 159/1000 | Loss: 0.00001690
Iteration 160/1000 | Loss: 0.00001690
Iteration 161/1000 | Loss: 0.00001690
Iteration 162/1000 | Loss: 0.00001690
Iteration 163/1000 | Loss: 0.00001690
Iteration 164/1000 | Loss: 0.00001690
Iteration 165/1000 | Loss: 0.00001690
Iteration 166/1000 | Loss: 0.00001690
Iteration 167/1000 | Loss: 0.00001690
Iteration 168/1000 | Loss: 0.00001690
Iteration 169/1000 | Loss: 0.00001690
Iteration 170/1000 | Loss: 0.00001690
Iteration 171/1000 | Loss: 0.00001690
Iteration 172/1000 | Loss: 0.00001690
Iteration 173/1000 | Loss: 0.00001690
Iteration 174/1000 | Loss: 0.00001690
Iteration 175/1000 | Loss: 0.00001689
Iteration 176/1000 | Loss: 0.00001689
Iteration 177/1000 | Loss: 0.00001689
Iteration 178/1000 | Loss: 0.00001689
Iteration 179/1000 | Loss: 0.00001689
Iteration 180/1000 | Loss: 0.00001689
Iteration 181/1000 | Loss: 0.00001689
Iteration 182/1000 | Loss: 0.00001689
Iteration 183/1000 | Loss: 0.00001689
Iteration 184/1000 | Loss: 0.00001689
Iteration 185/1000 | Loss: 0.00001689
Iteration 186/1000 | Loss: 0.00001689
Iteration 187/1000 | Loss: 0.00001689
Iteration 188/1000 | Loss: 0.00001689
Iteration 189/1000 | Loss: 0.00001689
Iteration 190/1000 | Loss: 0.00001688
Iteration 191/1000 | Loss: 0.00001688
Iteration 192/1000 | Loss: 0.00001688
Iteration 193/1000 | Loss: 0.00001688
Iteration 194/1000 | Loss: 0.00001688
Iteration 195/1000 | Loss: 0.00001688
Iteration 196/1000 | Loss: 0.00001688
Iteration 197/1000 | Loss: 0.00001688
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.6884307115105912e-05, 1.6884307115105912e-05, 1.6884307115105912e-05, 1.6884307115105912e-05, 1.6884307115105912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6884307115105912e-05

Optimization complete. Final v2v error: 3.4733946323394775 mm

Highest mean error: 3.8509435653686523 mm for frame 124

Lowest mean error: 3.1193344593048096 mm for frame 83

Saving results

Total time: 42.02142095565796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00695640
Iteration 2/25 | Loss: 0.00155967
Iteration 3/25 | Loss: 0.00128074
Iteration 4/25 | Loss: 0.00124875
Iteration 5/25 | Loss: 0.00124314
Iteration 6/25 | Loss: 0.00124235
Iteration 7/25 | Loss: 0.00124235
Iteration 8/25 | Loss: 0.00124235
Iteration 9/25 | Loss: 0.00124235
Iteration 10/25 | Loss: 0.00124235
Iteration 11/25 | Loss: 0.00124235
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012423484586179256, 0.0012423484586179256, 0.0012423484586179256, 0.0012423484586179256, 0.0012423484586179256]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012423484586179256

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45194733
Iteration 2/25 | Loss: 0.00070022
Iteration 3/25 | Loss: 0.00070021
Iteration 4/25 | Loss: 0.00070021
Iteration 5/25 | Loss: 0.00070021
Iteration 6/25 | Loss: 0.00070021
Iteration 7/25 | Loss: 0.00070020
Iteration 8/25 | Loss: 0.00070020
Iteration 9/25 | Loss: 0.00070020
Iteration 10/25 | Loss: 0.00070020
Iteration 11/25 | Loss: 0.00070020
Iteration 12/25 | Loss: 0.00070020
Iteration 13/25 | Loss: 0.00070020
Iteration 14/25 | Loss: 0.00070020
Iteration 15/25 | Loss: 0.00070020
Iteration 16/25 | Loss: 0.00070020
Iteration 17/25 | Loss: 0.00070020
Iteration 18/25 | Loss: 0.00070020
Iteration 19/25 | Loss: 0.00070020
Iteration 20/25 | Loss: 0.00070020
Iteration 21/25 | Loss: 0.00070020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007002032944001257, 0.0007002032944001257, 0.0007002032944001257, 0.0007002032944001257, 0.0007002032944001257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007002032944001257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070020
Iteration 2/1000 | Loss: 0.00003223
Iteration 3/1000 | Loss: 0.00002276
Iteration 4/1000 | Loss: 0.00002070
Iteration 5/1000 | Loss: 0.00001940
Iteration 6/1000 | Loss: 0.00001832
Iteration 7/1000 | Loss: 0.00001771
Iteration 8/1000 | Loss: 0.00001732
Iteration 9/1000 | Loss: 0.00001695
Iteration 10/1000 | Loss: 0.00001668
Iteration 11/1000 | Loss: 0.00001649
Iteration 12/1000 | Loss: 0.00001633
Iteration 13/1000 | Loss: 0.00001631
Iteration 14/1000 | Loss: 0.00001623
Iteration 15/1000 | Loss: 0.00001610
Iteration 16/1000 | Loss: 0.00001605
Iteration 17/1000 | Loss: 0.00001604
Iteration 18/1000 | Loss: 0.00001602
Iteration 19/1000 | Loss: 0.00001600
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001597
Iteration 23/1000 | Loss: 0.00001592
Iteration 24/1000 | Loss: 0.00001589
Iteration 25/1000 | Loss: 0.00001589
Iteration 26/1000 | Loss: 0.00001588
Iteration 27/1000 | Loss: 0.00001586
Iteration 28/1000 | Loss: 0.00001585
Iteration 29/1000 | Loss: 0.00001581
Iteration 30/1000 | Loss: 0.00001581
Iteration 31/1000 | Loss: 0.00001577
Iteration 32/1000 | Loss: 0.00001576
Iteration 33/1000 | Loss: 0.00001576
Iteration 34/1000 | Loss: 0.00001575
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001573
Iteration 37/1000 | Loss: 0.00001573
Iteration 38/1000 | Loss: 0.00001573
Iteration 39/1000 | Loss: 0.00001573
Iteration 40/1000 | Loss: 0.00001572
Iteration 41/1000 | Loss: 0.00001572
Iteration 42/1000 | Loss: 0.00001572
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001571
Iteration 45/1000 | Loss: 0.00001571
Iteration 46/1000 | Loss: 0.00001571
Iteration 47/1000 | Loss: 0.00001571
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001570
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001569
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001568
Iteration 55/1000 | Loss: 0.00001568
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001567
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001567
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001566
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001566
Iteration 66/1000 | Loss: 0.00001566
Iteration 67/1000 | Loss: 0.00001566
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001565
Iteration 70/1000 | Loss: 0.00001564
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001563
Iteration 73/1000 | Loss: 0.00001563
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001563
Iteration 76/1000 | Loss: 0.00001563
Iteration 77/1000 | Loss: 0.00001562
Iteration 78/1000 | Loss: 0.00001561
Iteration 79/1000 | Loss: 0.00001561
Iteration 80/1000 | Loss: 0.00001561
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001560
Iteration 84/1000 | Loss: 0.00001560
Iteration 85/1000 | Loss: 0.00001560
Iteration 86/1000 | Loss: 0.00001559
Iteration 87/1000 | Loss: 0.00001559
Iteration 88/1000 | Loss: 0.00001559
Iteration 89/1000 | Loss: 0.00001558
Iteration 90/1000 | Loss: 0.00001558
Iteration 91/1000 | Loss: 0.00001557
Iteration 92/1000 | Loss: 0.00001557
Iteration 93/1000 | Loss: 0.00001557
Iteration 94/1000 | Loss: 0.00001556
Iteration 95/1000 | Loss: 0.00001556
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001555
Iteration 99/1000 | Loss: 0.00001554
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001553
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001553
Iteration 105/1000 | Loss: 0.00001553
Iteration 106/1000 | Loss: 0.00001553
Iteration 107/1000 | Loss: 0.00001552
Iteration 108/1000 | Loss: 0.00001552
Iteration 109/1000 | Loss: 0.00001552
Iteration 110/1000 | Loss: 0.00001552
Iteration 111/1000 | Loss: 0.00001551
Iteration 112/1000 | Loss: 0.00001551
Iteration 113/1000 | Loss: 0.00001550
Iteration 114/1000 | Loss: 0.00001550
Iteration 115/1000 | Loss: 0.00001550
Iteration 116/1000 | Loss: 0.00001550
Iteration 117/1000 | Loss: 0.00001550
Iteration 118/1000 | Loss: 0.00001550
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001548
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001548
Iteration 126/1000 | Loss: 0.00001548
Iteration 127/1000 | Loss: 0.00001548
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001548
Iteration 130/1000 | Loss: 0.00001548
Iteration 131/1000 | Loss: 0.00001548
Iteration 132/1000 | Loss: 0.00001548
Iteration 133/1000 | Loss: 0.00001548
Iteration 134/1000 | Loss: 0.00001548
Iteration 135/1000 | Loss: 0.00001548
Iteration 136/1000 | Loss: 0.00001548
Iteration 137/1000 | Loss: 0.00001548
Iteration 138/1000 | Loss: 0.00001548
Iteration 139/1000 | Loss: 0.00001548
Iteration 140/1000 | Loss: 0.00001548
Iteration 141/1000 | Loss: 0.00001548
Iteration 142/1000 | Loss: 0.00001548
Iteration 143/1000 | Loss: 0.00001548
Iteration 144/1000 | Loss: 0.00001548
Iteration 145/1000 | Loss: 0.00001548
Iteration 146/1000 | Loss: 0.00001548
Iteration 147/1000 | Loss: 0.00001548
Iteration 148/1000 | Loss: 0.00001548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [1.5479638022952713e-05, 1.5479638022952713e-05, 1.5479638022952713e-05, 1.5479638022952713e-05, 1.5479638022952713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5479638022952713e-05

Optimization complete. Final v2v error: 3.3450417518615723 mm

Highest mean error: 3.826870918273926 mm for frame 235

Lowest mean error: 3.0128836631774902 mm for frame 166

Saving results

Total time: 45.72655272483826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970951
Iteration 2/25 | Loss: 0.00248486
Iteration 3/25 | Loss: 0.00194731
Iteration 4/25 | Loss: 0.00185515
Iteration 5/25 | Loss: 0.00184778
Iteration 6/25 | Loss: 0.00175619
Iteration 7/25 | Loss: 0.00166858
Iteration 8/25 | Loss: 0.00163442
Iteration 9/25 | Loss: 0.00162692
Iteration 10/25 | Loss: 0.00162105
Iteration 11/25 | Loss: 0.00161279
Iteration 12/25 | Loss: 0.00160835
Iteration 13/25 | Loss: 0.00160771
Iteration 14/25 | Loss: 0.00160673
Iteration 15/25 | Loss: 0.00161227
Iteration 16/25 | Loss: 0.00160376
Iteration 17/25 | Loss: 0.00160280
Iteration 18/25 | Loss: 0.00160256
Iteration 19/25 | Loss: 0.00160248
Iteration 20/25 | Loss: 0.00160248
Iteration 21/25 | Loss: 0.00160248
Iteration 22/25 | Loss: 0.00160248
Iteration 23/25 | Loss: 0.00160248
Iteration 24/25 | Loss: 0.00160248
Iteration 25/25 | Loss: 0.00160248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40604150
Iteration 2/25 | Loss: 0.00264508
Iteration 3/25 | Loss: 0.00264507
Iteration 4/25 | Loss: 0.00264507
Iteration 5/25 | Loss: 0.00264507
Iteration 6/25 | Loss: 0.00264507
Iteration 7/25 | Loss: 0.00264507
Iteration 8/25 | Loss: 0.00264507
Iteration 9/25 | Loss: 0.00264507
Iteration 10/25 | Loss: 0.00264507
Iteration 11/25 | Loss: 0.00264507
Iteration 12/25 | Loss: 0.00264507
Iteration 13/25 | Loss: 0.00264507
Iteration 14/25 | Loss: 0.00264507
Iteration 15/25 | Loss: 0.00264507
Iteration 16/25 | Loss: 0.00264507
Iteration 17/25 | Loss: 0.00264507
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002645067870616913, 0.002645067870616913, 0.002645067870616913, 0.002645067870616913, 0.002645067870616913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002645067870616913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264507
Iteration 2/1000 | Loss: 0.00033849
Iteration 3/1000 | Loss: 0.00024755
Iteration 4/1000 | Loss: 0.00022184
Iteration 5/1000 | Loss: 0.00020259
Iteration 6/1000 | Loss: 0.00019452
Iteration 7/1000 | Loss: 0.00018681
Iteration 8/1000 | Loss: 0.00017885
Iteration 9/1000 | Loss: 0.00017318
Iteration 10/1000 | Loss: 0.00017031
Iteration 11/1000 | Loss: 0.00029040
Iteration 12/1000 | Loss: 0.00028281
Iteration 13/1000 | Loss: 0.00040109
Iteration 14/1000 | Loss: 0.00158329
Iteration 15/1000 | Loss: 0.00814457
Iteration 16/1000 | Loss: 0.00076655
Iteration 17/1000 | Loss: 0.00033216
Iteration 18/1000 | Loss: 0.00023567
Iteration 19/1000 | Loss: 0.00014283
Iteration 20/1000 | Loss: 0.00009068
Iteration 21/1000 | Loss: 0.00006654
Iteration 22/1000 | Loss: 0.00005533
Iteration 23/1000 | Loss: 0.00004703
Iteration 24/1000 | Loss: 0.00003987
Iteration 25/1000 | Loss: 0.00003338
Iteration 26/1000 | Loss: 0.00002991
Iteration 27/1000 | Loss: 0.00002695
Iteration 28/1000 | Loss: 0.00002513
Iteration 29/1000 | Loss: 0.00002341
Iteration 30/1000 | Loss: 0.00002184
Iteration 31/1000 | Loss: 0.00002058
Iteration 32/1000 | Loss: 0.00001987
Iteration 33/1000 | Loss: 0.00001939
Iteration 34/1000 | Loss: 0.00001887
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001844
Iteration 37/1000 | Loss: 0.00001841
Iteration 38/1000 | Loss: 0.00001834
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001814
Iteration 41/1000 | Loss: 0.00001814
Iteration 42/1000 | Loss: 0.00001811
Iteration 43/1000 | Loss: 0.00001809
Iteration 44/1000 | Loss: 0.00001809
Iteration 45/1000 | Loss: 0.00001808
Iteration 46/1000 | Loss: 0.00001808
Iteration 47/1000 | Loss: 0.00001807
Iteration 48/1000 | Loss: 0.00001807
Iteration 49/1000 | Loss: 0.00001802
Iteration 50/1000 | Loss: 0.00001801
Iteration 51/1000 | Loss: 0.00001801
Iteration 52/1000 | Loss: 0.00001801
Iteration 53/1000 | Loss: 0.00001801
Iteration 54/1000 | Loss: 0.00001800
Iteration 55/1000 | Loss: 0.00001800
Iteration 56/1000 | Loss: 0.00001799
Iteration 57/1000 | Loss: 0.00001799
Iteration 58/1000 | Loss: 0.00001799
Iteration 59/1000 | Loss: 0.00001798
Iteration 60/1000 | Loss: 0.00001798
Iteration 61/1000 | Loss: 0.00001798
Iteration 62/1000 | Loss: 0.00001797
Iteration 63/1000 | Loss: 0.00001797
Iteration 64/1000 | Loss: 0.00001797
Iteration 65/1000 | Loss: 0.00001796
Iteration 66/1000 | Loss: 0.00001796
Iteration 67/1000 | Loss: 0.00001796
Iteration 68/1000 | Loss: 0.00001793
Iteration 69/1000 | Loss: 0.00001793
Iteration 70/1000 | Loss: 0.00001792
Iteration 71/1000 | Loss: 0.00001792
Iteration 72/1000 | Loss: 0.00001792
Iteration 73/1000 | Loss: 0.00001792
Iteration 74/1000 | Loss: 0.00001792
Iteration 75/1000 | Loss: 0.00001791
Iteration 76/1000 | Loss: 0.00001791
Iteration 77/1000 | Loss: 0.00001791
Iteration 78/1000 | Loss: 0.00001790
Iteration 79/1000 | Loss: 0.00001790
Iteration 80/1000 | Loss: 0.00001790
Iteration 81/1000 | Loss: 0.00001790
Iteration 82/1000 | Loss: 0.00001790
Iteration 83/1000 | Loss: 0.00001790
Iteration 84/1000 | Loss: 0.00001790
Iteration 85/1000 | Loss: 0.00001790
Iteration 86/1000 | Loss: 0.00001790
Iteration 87/1000 | Loss: 0.00001789
Iteration 88/1000 | Loss: 0.00001789
Iteration 89/1000 | Loss: 0.00001789
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001788
Iteration 92/1000 | Loss: 0.00001787
Iteration 93/1000 | Loss: 0.00001787
Iteration 94/1000 | Loss: 0.00001787
Iteration 95/1000 | Loss: 0.00001787
Iteration 96/1000 | Loss: 0.00001787
Iteration 97/1000 | Loss: 0.00001786
Iteration 98/1000 | Loss: 0.00001786
Iteration 99/1000 | Loss: 0.00001786
Iteration 100/1000 | Loss: 0.00001786
Iteration 101/1000 | Loss: 0.00001786
Iteration 102/1000 | Loss: 0.00001786
Iteration 103/1000 | Loss: 0.00001786
Iteration 104/1000 | Loss: 0.00001786
Iteration 105/1000 | Loss: 0.00001786
Iteration 106/1000 | Loss: 0.00001786
Iteration 107/1000 | Loss: 0.00001786
Iteration 108/1000 | Loss: 0.00001786
Iteration 109/1000 | Loss: 0.00001786
Iteration 110/1000 | Loss: 0.00001786
Iteration 111/1000 | Loss: 0.00001786
Iteration 112/1000 | Loss: 0.00001786
Iteration 113/1000 | Loss: 0.00001785
Iteration 114/1000 | Loss: 0.00001785
Iteration 115/1000 | Loss: 0.00001785
Iteration 116/1000 | Loss: 0.00001785
Iteration 117/1000 | Loss: 0.00001785
Iteration 118/1000 | Loss: 0.00001785
Iteration 119/1000 | Loss: 0.00001785
Iteration 120/1000 | Loss: 0.00001785
Iteration 121/1000 | Loss: 0.00001785
Iteration 122/1000 | Loss: 0.00001785
Iteration 123/1000 | Loss: 0.00001785
Iteration 124/1000 | Loss: 0.00001785
Iteration 125/1000 | Loss: 0.00001784
Iteration 126/1000 | Loss: 0.00001784
Iteration 127/1000 | Loss: 0.00001784
Iteration 128/1000 | Loss: 0.00001784
Iteration 129/1000 | Loss: 0.00001784
Iteration 130/1000 | Loss: 0.00001784
Iteration 131/1000 | Loss: 0.00001784
Iteration 132/1000 | Loss: 0.00001784
Iteration 133/1000 | Loss: 0.00001784
Iteration 134/1000 | Loss: 0.00001784
Iteration 135/1000 | Loss: 0.00001784
Iteration 136/1000 | Loss: 0.00001784
Iteration 137/1000 | Loss: 0.00001784
Iteration 138/1000 | Loss: 0.00001784
Iteration 139/1000 | Loss: 0.00001783
Iteration 140/1000 | Loss: 0.00001783
Iteration 141/1000 | Loss: 0.00001783
Iteration 142/1000 | Loss: 0.00001783
Iteration 143/1000 | Loss: 0.00001783
Iteration 144/1000 | Loss: 0.00001783
Iteration 145/1000 | Loss: 0.00001783
Iteration 146/1000 | Loss: 0.00001783
Iteration 147/1000 | Loss: 0.00001783
Iteration 148/1000 | Loss: 0.00001783
Iteration 149/1000 | Loss: 0.00001783
Iteration 150/1000 | Loss: 0.00001783
Iteration 151/1000 | Loss: 0.00001783
Iteration 152/1000 | Loss: 0.00001782
Iteration 153/1000 | Loss: 0.00001782
Iteration 154/1000 | Loss: 0.00001782
Iteration 155/1000 | Loss: 0.00001782
Iteration 156/1000 | Loss: 0.00001782
Iteration 157/1000 | Loss: 0.00001782
Iteration 158/1000 | Loss: 0.00001782
Iteration 159/1000 | Loss: 0.00001782
Iteration 160/1000 | Loss: 0.00001782
Iteration 161/1000 | Loss: 0.00001782
Iteration 162/1000 | Loss: 0.00001782
Iteration 163/1000 | Loss: 0.00001782
Iteration 164/1000 | Loss: 0.00001782
Iteration 165/1000 | Loss: 0.00001782
Iteration 166/1000 | Loss: 0.00001782
Iteration 167/1000 | Loss: 0.00001782
Iteration 168/1000 | Loss: 0.00001782
Iteration 169/1000 | Loss: 0.00001782
Iteration 170/1000 | Loss: 0.00001782
Iteration 171/1000 | Loss: 0.00001782
Iteration 172/1000 | Loss: 0.00001782
Iteration 173/1000 | Loss: 0.00001782
Iteration 174/1000 | Loss: 0.00001782
Iteration 175/1000 | Loss: 0.00001782
Iteration 176/1000 | Loss: 0.00001782
Iteration 177/1000 | Loss: 0.00001782
Iteration 178/1000 | Loss: 0.00001782
Iteration 179/1000 | Loss: 0.00001782
Iteration 180/1000 | Loss: 0.00001782
Iteration 181/1000 | Loss: 0.00001782
Iteration 182/1000 | Loss: 0.00001782
Iteration 183/1000 | Loss: 0.00001782
Iteration 184/1000 | Loss: 0.00001782
Iteration 185/1000 | Loss: 0.00001782
Iteration 186/1000 | Loss: 0.00001782
Iteration 187/1000 | Loss: 0.00001782
Iteration 188/1000 | Loss: 0.00001782
Iteration 189/1000 | Loss: 0.00001782
Iteration 190/1000 | Loss: 0.00001782
Iteration 191/1000 | Loss: 0.00001782
Iteration 192/1000 | Loss: 0.00001782
Iteration 193/1000 | Loss: 0.00001782
Iteration 194/1000 | Loss: 0.00001782
Iteration 195/1000 | Loss: 0.00001782
Iteration 196/1000 | Loss: 0.00001782
Iteration 197/1000 | Loss: 0.00001782
Iteration 198/1000 | Loss: 0.00001782
Iteration 199/1000 | Loss: 0.00001782
Iteration 200/1000 | Loss: 0.00001782
Iteration 201/1000 | Loss: 0.00001782
Iteration 202/1000 | Loss: 0.00001782
Iteration 203/1000 | Loss: 0.00001782
Iteration 204/1000 | Loss: 0.00001782
Iteration 205/1000 | Loss: 0.00001782
Iteration 206/1000 | Loss: 0.00001782
Iteration 207/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [1.7819822460296564e-05, 1.7819822460296564e-05, 1.7819822460296564e-05, 1.7819822460296564e-05, 1.7819822460296564e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7819822460296564e-05

Optimization complete. Final v2v error: 3.572963237762451 mm

Highest mean error: 3.8277337551116943 mm for frame 73

Lowest mean error: 3.2520411014556885 mm for frame 133

Saving results

Total time: 92.29737186431885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820037
Iteration 2/25 | Loss: 0.00129456
Iteration 3/25 | Loss: 0.00120185
Iteration 4/25 | Loss: 0.00119259
Iteration 5/25 | Loss: 0.00119247
Iteration 6/25 | Loss: 0.00119247
Iteration 7/25 | Loss: 0.00119247
Iteration 8/25 | Loss: 0.00119247
Iteration 9/25 | Loss: 0.00119247
Iteration 10/25 | Loss: 0.00119247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011924724094569683, 0.0011924724094569683, 0.0011924724094569683, 0.0011924724094569683, 0.0011924724094569683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011924724094569683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43283165
Iteration 2/25 | Loss: 0.00063949
Iteration 3/25 | Loss: 0.00063948
Iteration 4/25 | Loss: 0.00063948
Iteration 5/25 | Loss: 0.00063948
Iteration 6/25 | Loss: 0.00063948
Iteration 7/25 | Loss: 0.00063948
Iteration 8/25 | Loss: 0.00063948
Iteration 9/25 | Loss: 0.00063948
Iteration 10/25 | Loss: 0.00063948
Iteration 11/25 | Loss: 0.00063948
Iteration 12/25 | Loss: 0.00063948
Iteration 13/25 | Loss: 0.00063948
Iteration 14/25 | Loss: 0.00063948
Iteration 15/25 | Loss: 0.00063948
Iteration 16/25 | Loss: 0.00063948
Iteration 17/25 | Loss: 0.00063948
Iteration 18/25 | Loss: 0.00063948
Iteration 19/25 | Loss: 0.00063948
Iteration 20/25 | Loss: 0.00063948
Iteration 21/25 | Loss: 0.00063948
Iteration 22/25 | Loss: 0.00063948
Iteration 23/25 | Loss: 0.00063948
Iteration 24/25 | Loss: 0.00063948
Iteration 25/25 | Loss: 0.00063948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063948
Iteration 2/1000 | Loss: 0.00002272
Iteration 3/1000 | Loss: 0.00001740
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001444
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001300
Iteration 8/1000 | Loss: 0.00001264
Iteration 9/1000 | Loss: 0.00001239
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001204
Iteration 13/1000 | Loss: 0.00001202
Iteration 14/1000 | Loss: 0.00001202
Iteration 15/1000 | Loss: 0.00001201
Iteration 16/1000 | Loss: 0.00001200
Iteration 17/1000 | Loss: 0.00001200
Iteration 18/1000 | Loss: 0.00001200
Iteration 19/1000 | Loss: 0.00001200
Iteration 20/1000 | Loss: 0.00001199
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001197
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001197
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001196
Iteration 31/1000 | Loss: 0.00001196
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001193
Iteration 35/1000 | Loss: 0.00001193
Iteration 36/1000 | Loss: 0.00001193
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001192
Iteration 39/1000 | Loss: 0.00001192
Iteration 40/1000 | Loss: 0.00001191
Iteration 41/1000 | Loss: 0.00001190
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001188
Iteration 44/1000 | Loss: 0.00001188
Iteration 45/1000 | Loss: 0.00001188
Iteration 46/1000 | Loss: 0.00001188
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001187
Iteration 49/1000 | Loss: 0.00001187
Iteration 50/1000 | Loss: 0.00001187
Iteration 51/1000 | Loss: 0.00001187
Iteration 52/1000 | Loss: 0.00001186
Iteration 53/1000 | Loss: 0.00001179
Iteration 54/1000 | Loss: 0.00001177
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001174
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001170
Iteration 61/1000 | Loss: 0.00001170
Iteration 62/1000 | Loss: 0.00001170
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001169
Iteration 65/1000 | Loss: 0.00001169
Iteration 66/1000 | Loss: 0.00001168
Iteration 67/1000 | Loss: 0.00001168
Iteration 68/1000 | Loss: 0.00001168
Iteration 69/1000 | Loss: 0.00001167
Iteration 70/1000 | Loss: 0.00001167
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001165
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001164
Iteration 75/1000 | Loss: 0.00001164
Iteration 76/1000 | Loss: 0.00001163
Iteration 77/1000 | Loss: 0.00001162
Iteration 78/1000 | Loss: 0.00001162
Iteration 79/1000 | Loss: 0.00001162
Iteration 80/1000 | Loss: 0.00001161
Iteration 81/1000 | Loss: 0.00001160
Iteration 82/1000 | Loss: 0.00001160
Iteration 83/1000 | Loss: 0.00001159
Iteration 84/1000 | Loss: 0.00001157
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001156
Iteration 87/1000 | Loss: 0.00001156
Iteration 88/1000 | Loss: 0.00001156
Iteration 89/1000 | Loss: 0.00001156
Iteration 90/1000 | Loss: 0.00001155
Iteration 91/1000 | Loss: 0.00001154
Iteration 92/1000 | Loss: 0.00001154
Iteration 93/1000 | Loss: 0.00001154
Iteration 94/1000 | Loss: 0.00001154
Iteration 95/1000 | Loss: 0.00001154
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001152
Iteration 105/1000 | Loss: 0.00001152
Iteration 106/1000 | Loss: 0.00001152
Iteration 107/1000 | Loss: 0.00001151
Iteration 108/1000 | Loss: 0.00001151
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001151
Iteration 111/1000 | Loss: 0.00001151
Iteration 112/1000 | Loss: 0.00001151
Iteration 113/1000 | Loss: 0.00001150
Iteration 114/1000 | Loss: 0.00001150
Iteration 115/1000 | Loss: 0.00001150
Iteration 116/1000 | Loss: 0.00001150
Iteration 117/1000 | Loss: 0.00001149
Iteration 118/1000 | Loss: 0.00001149
Iteration 119/1000 | Loss: 0.00001148
Iteration 120/1000 | Loss: 0.00001148
Iteration 121/1000 | Loss: 0.00001147
Iteration 122/1000 | Loss: 0.00001147
Iteration 123/1000 | Loss: 0.00001146
Iteration 124/1000 | Loss: 0.00001146
Iteration 125/1000 | Loss: 0.00001144
Iteration 126/1000 | Loss: 0.00001144
Iteration 127/1000 | Loss: 0.00001143
Iteration 128/1000 | Loss: 0.00001143
Iteration 129/1000 | Loss: 0.00001143
Iteration 130/1000 | Loss: 0.00001143
Iteration 131/1000 | Loss: 0.00001143
Iteration 132/1000 | Loss: 0.00001143
Iteration 133/1000 | Loss: 0.00001143
Iteration 134/1000 | Loss: 0.00001143
Iteration 135/1000 | Loss: 0.00001143
Iteration 136/1000 | Loss: 0.00001143
Iteration 137/1000 | Loss: 0.00001143
Iteration 138/1000 | Loss: 0.00001142
Iteration 139/1000 | Loss: 0.00001141
Iteration 140/1000 | Loss: 0.00001140
Iteration 141/1000 | Loss: 0.00001140
Iteration 142/1000 | Loss: 0.00001140
Iteration 143/1000 | Loss: 0.00001140
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Iteration 150/1000 | Loss: 0.00001138
Iteration 151/1000 | Loss: 0.00001138
Iteration 152/1000 | Loss: 0.00001138
Iteration 153/1000 | Loss: 0.00001138
Iteration 154/1000 | Loss: 0.00001138
Iteration 155/1000 | Loss: 0.00001138
Iteration 156/1000 | Loss: 0.00001138
Iteration 157/1000 | Loss: 0.00001138
Iteration 158/1000 | Loss: 0.00001138
Iteration 159/1000 | Loss: 0.00001138
Iteration 160/1000 | Loss: 0.00001138
Iteration 161/1000 | Loss: 0.00001138
Iteration 162/1000 | Loss: 0.00001137
Iteration 163/1000 | Loss: 0.00001137
Iteration 164/1000 | Loss: 0.00001137
Iteration 165/1000 | Loss: 0.00001137
Iteration 166/1000 | Loss: 0.00001136
Iteration 167/1000 | Loss: 0.00001136
Iteration 168/1000 | Loss: 0.00001136
Iteration 169/1000 | Loss: 0.00001136
Iteration 170/1000 | Loss: 0.00001136
Iteration 171/1000 | Loss: 0.00001136
Iteration 172/1000 | Loss: 0.00001136
Iteration 173/1000 | Loss: 0.00001135
Iteration 174/1000 | Loss: 0.00001135
Iteration 175/1000 | Loss: 0.00001135
Iteration 176/1000 | Loss: 0.00001134
Iteration 177/1000 | Loss: 0.00001134
Iteration 178/1000 | Loss: 0.00001134
Iteration 179/1000 | Loss: 0.00001134
Iteration 180/1000 | Loss: 0.00001134
Iteration 181/1000 | Loss: 0.00001134
Iteration 182/1000 | Loss: 0.00001133
Iteration 183/1000 | Loss: 0.00001133
Iteration 184/1000 | Loss: 0.00001133
Iteration 185/1000 | Loss: 0.00001133
Iteration 186/1000 | Loss: 0.00001133
Iteration 187/1000 | Loss: 0.00001133
Iteration 188/1000 | Loss: 0.00001133
Iteration 189/1000 | Loss: 0.00001133
Iteration 190/1000 | Loss: 0.00001133
Iteration 191/1000 | Loss: 0.00001133
Iteration 192/1000 | Loss: 0.00001133
Iteration 193/1000 | Loss: 0.00001133
Iteration 194/1000 | Loss: 0.00001132
Iteration 195/1000 | Loss: 0.00001132
Iteration 196/1000 | Loss: 0.00001132
Iteration 197/1000 | Loss: 0.00001132
Iteration 198/1000 | Loss: 0.00001132
Iteration 199/1000 | Loss: 0.00001132
Iteration 200/1000 | Loss: 0.00001132
Iteration 201/1000 | Loss: 0.00001132
Iteration 202/1000 | Loss: 0.00001132
Iteration 203/1000 | Loss: 0.00001132
Iteration 204/1000 | Loss: 0.00001132
Iteration 205/1000 | Loss: 0.00001132
Iteration 206/1000 | Loss: 0.00001131
Iteration 207/1000 | Loss: 0.00001131
Iteration 208/1000 | Loss: 0.00001131
Iteration 209/1000 | Loss: 0.00001131
Iteration 210/1000 | Loss: 0.00001131
Iteration 211/1000 | Loss: 0.00001131
Iteration 212/1000 | Loss: 0.00001131
Iteration 213/1000 | Loss: 0.00001131
Iteration 214/1000 | Loss: 0.00001131
Iteration 215/1000 | Loss: 0.00001131
Iteration 216/1000 | Loss: 0.00001131
Iteration 217/1000 | Loss: 0.00001131
Iteration 218/1000 | Loss: 0.00001131
Iteration 219/1000 | Loss: 0.00001131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 219. Stopping optimization.
Last 5 losses: [1.1310121408314444e-05, 1.1310121408314444e-05, 1.1310121408314444e-05, 1.1310121408314444e-05, 1.1310121408314444e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1310121408314444e-05

Optimization complete. Final v2v error: 2.888993740081787 mm

Highest mean error: 3.09829044342041 mm for frame 66

Lowest mean error: 2.722672700881958 mm for frame 16

Saving results

Total time: 47.55977153778076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00794529
Iteration 2/25 | Loss: 0.00136520
Iteration 3/25 | Loss: 0.00127578
Iteration 4/25 | Loss: 0.00126628
Iteration 5/25 | Loss: 0.00126307
Iteration 6/25 | Loss: 0.00126240
Iteration 7/25 | Loss: 0.00126240
Iteration 8/25 | Loss: 0.00126240
Iteration 9/25 | Loss: 0.00126240
Iteration 10/25 | Loss: 0.00126240
Iteration 11/25 | Loss: 0.00126240
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012623979710042477, 0.0012623979710042477, 0.0012623979710042477, 0.0012623979710042477, 0.0012623979710042477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012623979710042477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53235650
Iteration 2/25 | Loss: 0.00072854
Iteration 3/25 | Loss: 0.00072853
Iteration 4/25 | Loss: 0.00072853
Iteration 5/25 | Loss: 0.00072853
Iteration 6/25 | Loss: 0.00072853
Iteration 7/25 | Loss: 0.00072853
Iteration 8/25 | Loss: 0.00072853
Iteration 9/25 | Loss: 0.00072853
Iteration 10/25 | Loss: 0.00072853
Iteration 11/25 | Loss: 0.00072853
Iteration 12/25 | Loss: 0.00072853
Iteration 13/25 | Loss: 0.00072853
Iteration 14/25 | Loss: 0.00072853
Iteration 15/25 | Loss: 0.00072853
Iteration 16/25 | Loss: 0.00072853
Iteration 17/25 | Loss: 0.00072853
Iteration 18/25 | Loss: 0.00072853
Iteration 19/25 | Loss: 0.00072853
Iteration 20/25 | Loss: 0.00072853
Iteration 21/25 | Loss: 0.00072853
Iteration 22/25 | Loss: 0.00072853
Iteration 23/25 | Loss: 0.00072853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007285272586159408, 0.0007285272586159408, 0.0007285272586159408, 0.0007285272586159408, 0.0007285272586159408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007285272586159408

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072853
Iteration 2/1000 | Loss: 0.00004526
Iteration 3/1000 | Loss: 0.00002998
Iteration 4/1000 | Loss: 0.00002436
Iteration 5/1000 | Loss: 0.00002281
Iteration 6/1000 | Loss: 0.00002167
Iteration 7/1000 | Loss: 0.00002106
Iteration 8/1000 | Loss: 0.00002056
Iteration 9/1000 | Loss: 0.00002017
Iteration 10/1000 | Loss: 0.00001989
Iteration 11/1000 | Loss: 0.00001961
Iteration 12/1000 | Loss: 0.00001939
Iteration 13/1000 | Loss: 0.00001934
Iteration 14/1000 | Loss: 0.00001930
Iteration 15/1000 | Loss: 0.00001926
Iteration 16/1000 | Loss: 0.00001923
Iteration 17/1000 | Loss: 0.00001916
Iteration 18/1000 | Loss: 0.00001898
Iteration 19/1000 | Loss: 0.00001892
Iteration 20/1000 | Loss: 0.00001888
Iteration 21/1000 | Loss: 0.00001888
Iteration 22/1000 | Loss: 0.00001887
Iteration 23/1000 | Loss: 0.00001886
Iteration 24/1000 | Loss: 0.00001885
Iteration 25/1000 | Loss: 0.00001885
Iteration 26/1000 | Loss: 0.00001884
Iteration 27/1000 | Loss: 0.00001884
Iteration 28/1000 | Loss: 0.00001883
Iteration 29/1000 | Loss: 0.00001883
Iteration 30/1000 | Loss: 0.00001882
Iteration 31/1000 | Loss: 0.00001882
Iteration 32/1000 | Loss: 0.00001880
Iteration 33/1000 | Loss: 0.00001879
Iteration 34/1000 | Loss: 0.00001879
Iteration 35/1000 | Loss: 0.00001879
Iteration 36/1000 | Loss: 0.00001879
Iteration 37/1000 | Loss: 0.00001878
Iteration 38/1000 | Loss: 0.00001878
Iteration 39/1000 | Loss: 0.00001877
Iteration 40/1000 | Loss: 0.00001877
Iteration 41/1000 | Loss: 0.00001877
Iteration 42/1000 | Loss: 0.00001877
Iteration 43/1000 | Loss: 0.00001876
Iteration 44/1000 | Loss: 0.00001875
Iteration 45/1000 | Loss: 0.00001875
Iteration 46/1000 | Loss: 0.00001875
Iteration 47/1000 | Loss: 0.00001875
Iteration 48/1000 | Loss: 0.00001875
Iteration 49/1000 | Loss: 0.00001875
Iteration 50/1000 | Loss: 0.00001875
Iteration 51/1000 | Loss: 0.00001874
Iteration 52/1000 | Loss: 0.00001874
Iteration 53/1000 | Loss: 0.00001874
Iteration 54/1000 | Loss: 0.00001874
Iteration 55/1000 | Loss: 0.00001873
Iteration 56/1000 | Loss: 0.00001872
Iteration 57/1000 | Loss: 0.00001872
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001870
Iteration 63/1000 | Loss: 0.00001870
Iteration 64/1000 | Loss: 0.00001870
Iteration 65/1000 | Loss: 0.00001870
Iteration 66/1000 | Loss: 0.00001869
Iteration 67/1000 | Loss: 0.00001869
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001867
Iteration 72/1000 | Loss: 0.00001867
Iteration 73/1000 | Loss: 0.00001866
Iteration 74/1000 | Loss: 0.00001866
Iteration 75/1000 | Loss: 0.00001866
Iteration 76/1000 | Loss: 0.00001865
Iteration 77/1000 | Loss: 0.00001865
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001865
Iteration 80/1000 | Loss: 0.00001865
Iteration 81/1000 | Loss: 0.00001864
Iteration 82/1000 | Loss: 0.00001864
Iteration 83/1000 | Loss: 0.00001864
Iteration 84/1000 | Loss: 0.00001864
Iteration 85/1000 | Loss: 0.00001864
Iteration 86/1000 | Loss: 0.00001864
Iteration 87/1000 | Loss: 0.00001864
Iteration 88/1000 | Loss: 0.00001863
Iteration 89/1000 | Loss: 0.00001863
Iteration 90/1000 | Loss: 0.00001863
Iteration 91/1000 | Loss: 0.00001863
Iteration 92/1000 | Loss: 0.00001863
Iteration 93/1000 | Loss: 0.00001863
Iteration 94/1000 | Loss: 0.00001863
Iteration 95/1000 | Loss: 0.00001863
Iteration 96/1000 | Loss: 0.00001863
Iteration 97/1000 | Loss: 0.00001863
Iteration 98/1000 | Loss: 0.00001863
Iteration 99/1000 | Loss: 0.00001863
Iteration 100/1000 | Loss: 0.00001863
Iteration 101/1000 | Loss: 0.00001863
Iteration 102/1000 | Loss: 0.00001862
Iteration 103/1000 | Loss: 0.00001862
Iteration 104/1000 | Loss: 0.00001862
Iteration 105/1000 | Loss: 0.00001862
Iteration 106/1000 | Loss: 0.00001862
Iteration 107/1000 | Loss: 0.00001861
Iteration 108/1000 | Loss: 0.00001861
Iteration 109/1000 | Loss: 0.00001861
Iteration 110/1000 | Loss: 0.00001861
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001860
Iteration 113/1000 | Loss: 0.00001860
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001860
Iteration 116/1000 | Loss: 0.00001859
Iteration 117/1000 | Loss: 0.00001859
Iteration 118/1000 | Loss: 0.00001859
Iteration 119/1000 | Loss: 0.00001859
Iteration 120/1000 | Loss: 0.00001859
Iteration 121/1000 | Loss: 0.00001859
Iteration 122/1000 | Loss: 0.00001859
Iteration 123/1000 | Loss: 0.00001859
Iteration 124/1000 | Loss: 0.00001858
Iteration 125/1000 | Loss: 0.00001858
Iteration 126/1000 | Loss: 0.00001858
Iteration 127/1000 | Loss: 0.00001858
Iteration 128/1000 | Loss: 0.00001858
Iteration 129/1000 | Loss: 0.00001857
Iteration 130/1000 | Loss: 0.00001857
Iteration 131/1000 | Loss: 0.00001857
Iteration 132/1000 | Loss: 0.00001856
Iteration 133/1000 | Loss: 0.00001856
Iteration 134/1000 | Loss: 0.00001856
Iteration 135/1000 | Loss: 0.00001856
Iteration 136/1000 | Loss: 0.00001855
Iteration 137/1000 | Loss: 0.00001855
Iteration 138/1000 | Loss: 0.00001855
Iteration 139/1000 | Loss: 0.00001855
Iteration 140/1000 | Loss: 0.00001855
Iteration 141/1000 | Loss: 0.00001855
Iteration 142/1000 | Loss: 0.00001855
Iteration 143/1000 | Loss: 0.00001855
Iteration 144/1000 | Loss: 0.00001855
Iteration 145/1000 | Loss: 0.00001855
Iteration 146/1000 | Loss: 0.00001854
Iteration 147/1000 | Loss: 0.00001854
Iteration 148/1000 | Loss: 0.00001854
Iteration 149/1000 | Loss: 0.00001854
Iteration 150/1000 | Loss: 0.00001854
Iteration 151/1000 | Loss: 0.00001854
Iteration 152/1000 | Loss: 0.00001854
Iteration 153/1000 | Loss: 0.00001853
Iteration 154/1000 | Loss: 0.00001853
Iteration 155/1000 | Loss: 0.00001853
Iteration 156/1000 | Loss: 0.00001853
Iteration 157/1000 | Loss: 0.00001853
Iteration 158/1000 | Loss: 0.00001853
Iteration 159/1000 | Loss: 0.00001852
Iteration 160/1000 | Loss: 0.00001852
Iteration 161/1000 | Loss: 0.00001852
Iteration 162/1000 | Loss: 0.00001852
Iteration 163/1000 | Loss: 0.00001852
Iteration 164/1000 | Loss: 0.00001852
Iteration 165/1000 | Loss: 0.00001852
Iteration 166/1000 | Loss: 0.00001852
Iteration 167/1000 | Loss: 0.00001852
Iteration 168/1000 | Loss: 0.00001852
Iteration 169/1000 | Loss: 0.00001851
Iteration 170/1000 | Loss: 0.00001851
Iteration 171/1000 | Loss: 0.00001851
Iteration 172/1000 | Loss: 0.00001851
Iteration 173/1000 | Loss: 0.00001851
Iteration 174/1000 | Loss: 0.00001851
Iteration 175/1000 | Loss: 0.00001851
Iteration 176/1000 | Loss: 0.00001851
Iteration 177/1000 | Loss: 0.00001851
Iteration 178/1000 | Loss: 0.00001851
Iteration 179/1000 | Loss: 0.00001851
Iteration 180/1000 | Loss: 0.00001851
Iteration 181/1000 | Loss: 0.00001851
Iteration 182/1000 | Loss: 0.00001851
Iteration 183/1000 | Loss: 0.00001851
Iteration 184/1000 | Loss: 0.00001851
Iteration 185/1000 | Loss: 0.00001851
Iteration 186/1000 | Loss: 0.00001850
Iteration 187/1000 | Loss: 0.00001850
Iteration 188/1000 | Loss: 0.00001850
Iteration 189/1000 | Loss: 0.00001850
Iteration 190/1000 | Loss: 0.00001850
Iteration 191/1000 | Loss: 0.00001850
Iteration 192/1000 | Loss: 0.00001850
Iteration 193/1000 | Loss: 0.00001850
Iteration 194/1000 | Loss: 0.00001850
Iteration 195/1000 | Loss: 0.00001850
Iteration 196/1000 | Loss: 0.00001850
Iteration 197/1000 | Loss: 0.00001850
Iteration 198/1000 | Loss: 0.00001850
Iteration 199/1000 | Loss: 0.00001850
Iteration 200/1000 | Loss: 0.00001850
Iteration 201/1000 | Loss: 0.00001850
Iteration 202/1000 | Loss: 0.00001850
Iteration 203/1000 | Loss: 0.00001849
Iteration 204/1000 | Loss: 0.00001849
Iteration 205/1000 | Loss: 0.00001849
Iteration 206/1000 | Loss: 0.00001849
Iteration 207/1000 | Loss: 0.00001849
Iteration 208/1000 | Loss: 0.00001849
Iteration 209/1000 | Loss: 0.00001849
Iteration 210/1000 | Loss: 0.00001849
Iteration 211/1000 | Loss: 0.00001849
Iteration 212/1000 | Loss: 0.00001849
Iteration 213/1000 | Loss: 0.00001849
Iteration 214/1000 | Loss: 0.00001849
Iteration 215/1000 | Loss: 0.00001849
Iteration 216/1000 | Loss: 0.00001848
Iteration 217/1000 | Loss: 0.00001848
Iteration 218/1000 | Loss: 0.00001848
Iteration 219/1000 | Loss: 0.00001848
Iteration 220/1000 | Loss: 0.00001848
Iteration 221/1000 | Loss: 0.00001848
Iteration 222/1000 | Loss: 0.00001848
Iteration 223/1000 | Loss: 0.00001848
Iteration 224/1000 | Loss: 0.00001848
Iteration 225/1000 | Loss: 0.00001848
Iteration 226/1000 | Loss: 0.00001848
Iteration 227/1000 | Loss: 0.00001848
Iteration 228/1000 | Loss: 0.00001848
Iteration 229/1000 | Loss: 0.00001848
Iteration 230/1000 | Loss: 0.00001848
Iteration 231/1000 | Loss: 0.00001848
Iteration 232/1000 | Loss: 0.00001848
Iteration 233/1000 | Loss: 0.00001848
Iteration 234/1000 | Loss: 0.00001848
Iteration 235/1000 | Loss: 0.00001848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.8481092411093414e-05, 1.8481092411093414e-05, 1.8481092411093414e-05, 1.8481092411093414e-05, 1.8481092411093414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8481092411093414e-05

Optimization complete. Final v2v error: 3.579272508621216 mm

Highest mean error: 4.478818416595459 mm for frame 121

Lowest mean error: 2.8803534507751465 mm for frame 146

Saving results

Total time: 44.41547894477844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830652
Iteration 2/25 | Loss: 0.00142833
Iteration 3/25 | Loss: 0.00129985
Iteration 4/25 | Loss: 0.00128345
Iteration 5/25 | Loss: 0.00127844
Iteration 6/25 | Loss: 0.00127712
Iteration 7/25 | Loss: 0.00127702
Iteration 8/25 | Loss: 0.00127702
Iteration 9/25 | Loss: 0.00127702
Iteration 10/25 | Loss: 0.00127702
Iteration 11/25 | Loss: 0.00127702
Iteration 12/25 | Loss: 0.00127702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012770169414579868, 0.0012770169414579868, 0.0012770169414579868, 0.0012770169414579868, 0.0012770169414579868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012770169414579868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43760216
Iteration 2/25 | Loss: 0.00161570
Iteration 3/25 | Loss: 0.00161570
Iteration 4/25 | Loss: 0.00161570
Iteration 5/25 | Loss: 0.00161570
Iteration 6/25 | Loss: 0.00161570
Iteration 7/25 | Loss: 0.00161570
Iteration 8/25 | Loss: 0.00161570
Iteration 9/25 | Loss: 0.00161570
Iteration 10/25 | Loss: 0.00161570
Iteration 11/25 | Loss: 0.00161570
Iteration 12/25 | Loss: 0.00161570
Iteration 13/25 | Loss: 0.00161570
Iteration 14/25 | Loss: 0.00161570
Iteration 15/25 | Loss: 0.00161570
Iteration 16/25 | Loss: 0.00161570
Iteration 17/25 | Loss: 0.00161570
Iteration 18/25 | Loss: 0.00161570
Iteration 19/25 | Loss: 0.00161570
Iteration 20/25 | Loss: 0.00161570
Iteration 21/25 | Loss: 0.00161570
Iteration 22/25 | Loss: 0.00161570
Iteration 23/25 | Loss: 0.00161570
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0016156983328983188, 0.0016156983328983188, 0.0016156983328983188, 0.0016156983328983188, 0.0016156983328983188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016156983328983188

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161570
Iteration 2/1000 | Loss: 0.00012528
Iteration 3/1000 | Loss: 0.00007775
Iteration 4/1000 | Loss: 0.00006482
Iteration 5/1000 | Loss: 0.00005697
Iteration 6/1000 | Loss: 0.00005411
Iteration 7/1000 | Loss: 0.00005166
Iteration 8/1000 | Loss: 0.00005032
Iteration 9/1000 | Loss: 0.00004866
Iteration 10/1000 | Loss: 0.00004761
Iteration 11/1000 | Loss: 0.00004713
Iteration 12/1000 | Loss: 0.00004684
Iteration 13/1000 | Loss: 0.00004656
Iteration 14/1000 | Loss: 0.00004640
Iteration 15/1000 | Loss: 0.00004627
Iteration 16/1000 | Loss: 0.00004620
Iteration 17/1000 | Loss: 0.00004614
Iteration 18/1000 | Loss: 0.00004609
Iteration 19/1000 | Loss: 0.00004608
Iteration 20/1000 | Loss: 0.00004601
Iteration 21/1000 | Loss: 0.00004597
Iteration 22/1000 | Loss: 0.00004596
Iteration 23/1000 | Loss: 0.00004596
Iteration 24/1000 | Loss: 0.00004594
Iteration 25/1000 | Loss: 0.00004593
Iteration 26/1000 | Loss: 0.00004592
Iteration 27/1000 | Loss: 0.00004592
Iteration 28/1000 | Loss: 0.00004591
Iteration 29/1000 | Loss: 0.00004590
Iteration 30/1000 | Loss: 0.00004589
Iteration 31/1000 | Loss: 0.00004588
Iteration 32/1000 | Loss: 0.00004587
Iteration 33/1000 | Loss: 0.00004587
Iteration 34/1000 | Loss: 0.00004582
Iteration 35/1000 | Loss: 0.00004578
Iteration 36/1000 | Loss: 0.00004577
Iteration 37/1000 | Loss: 0.00004576
Iteration 38/1000 | Loss: 0.00004576
Iteration 39/1000 | Loss: 0.00004560
Iteration 40/1000 | Loss: 0.00004549
Iteration 41/1000 | Loss: 0.00004530
Iteration 42/1000 | Loss: 0.00004508
Iteration 43/1000 | Loss: 0.00018744
Iteration 44/1000 | Loss: 0.00016964
Iteration 45/1000 | Loss: 0.00005559
Iteration 46/1000 | Loss: 0.00014942
Iteration 47/1000 | Loss: 0.00004705
Iteration 48/1000 | Loss: 0.00014479
Iteration 49/1000 | Loss: 0.00005143
Iteration 50/1000 | Loss: 0.00004634
Iteration 51/1000 | Loss: 0.00004507
Iteration 52/1000 | Loss: 0.00004457
Iteration 53/1000 | Loss: 0.00004409
Iteration 54/1000 | Loss: 0.00011328
Iteration 55/1000 | Loss: 0.00004850
Iteration 56/1000 | Loss: 0.00004434
Iteration 57/1000 | Loss: 0.00009665
Iteration 58/1000 | Loss: 0.00011169
Iteration 59/1000 | Loss: 0.00012740
Iteration 60/1000 | Loss: 0.00010456
Iteration 61/1000 | Loss: 0.00008880
Iteration 62/1000 | Loss: 0.00007240
Iteration 63/1000 | Loss: 0.00008766
Iteration 64/1000 | Loss: 0.00006038
Iteration 65/1000 | Loss: 0.00007566
Iteration 66/1000 | Loss: 0.00006700
Iteration 67/1000 | Loss: 0.00007035
Iteration 68/1000 | Loss: 0.00004451
Iteration 69/1000 | Loss: 0.00004373
Iteration 70/1000 | Loss: 0.00004333
Iteration 71/1000 | Loss: 0.00004294
Iteration 72/1000 | Loss: 0.00004263
Iteration 73/1000 | Loss: 0.00004227
Iteration 74/1000 | Loss: 0.00004199
Iteration 75/1000 | Loss: 0.00004192
Iteration 76/1000 | Loss: 0.00004186
Iteration 77/1000 | Loss: 0.00004185
Iteration 78/1000 | Loss: 0.00004182
Iteration 79/1000 | Loss: 0.00004563
Iteration 80/1000 | Loss: 0.00004277
Iteration 81/1000 | Loss: 0.00004177
Iteration 82/1000 | Loss: 0.00004138
Iteration 83/1000 | Loss: 0.00004092
Iteration 84/1000 | Loss: 0.00004039
Iteration 85/1000 | Loss: 0.00004006
Iteration 86/1000 | Loss: 0.00003994
Iteration 87/1000 | Loss: 0.00003988
Iteration 88/1000 | Loss: 0.00003988
Iteration 89/1000 | Loss: 0.00003985
Iteration 90/1000 | Loss: 0.00003985
Iteration 91/1000 | Loss: 0.00003984
Iteration 92/1000 | Loss: 0.00003984
Iteration 93/1000 | Loss: 0.00003983
Iteration 94/1000 | Loss: 0.00003983
Iteration 95/1000 | Loss: 0.00003981
Iteration 96/1000 | Loss: 0.00003977
Iteration 97/1000 | Loss: 0.00003975
Iteration 98/1000 | Loss: 0.00003974
Iteration 99/1000 | Loss: 0.00003973
Iteration 100/1000 | Loss: 0.00003969
Iteration 101/1000 | Loss: 0.00003965
Iteration 102/1000 | Loss: 0.00003964
Iteration 103/1000 | Loss: 0.00003962
Iteration 104/1000 | Loss: 0.00003962
Iteration 105/1000 | Loss: 0.00003961
Iteration 106/1000 | Loss: 0.00003960
Iteration 107/1000 | Loss: 0.00003960
Iteration 108/1000 | Loss: 0.00003959
Iteration 109/1000 | Loss: 0.00003959
Iteration 110/1000 | Loss: 0.00003959
Iteration 111/1000 | Loss: 0.00003958
Iteration 112/1000 | Loss: 0.00003958
Iteration 113/1000 | Loss: 0.00003957
Iteration 114/1000 | Loss: 0.00003957
Iteration 115/1000 | Loss: 0.00003957
Iteration 116/1000 | Loss: 0.00003957
Iteration 117/1000 | Loss: 0.00003956
Iteration 118/1000 | Loss: 0.00003956
Iteration 119/1000 | Loss: 0.00003956
Iteration 120/1000 | Loss: 0.00003955
Iteration 121/1000 | Loss: 0.00003955
Iteration 122/1000 | Loss: 0.00003955
Iteration 123/1000 | Loss: 0.00003955
Iteration 124/1000 | Loss: 0.00003955
Iteration 125/1000 | Loss: 0.00003955
Iteration 126/1000 | Loss: 0.00003954
Iteration 127/1000 | Loss: 0.00003954
Iteration 128/1000 | Loss: 0.00003954
Iteration 129/1000 | Loss: 0.00003954
Iteration 130/1000 | Loss: 0.00003954
Iteration 131/1000 | Loss: 0.00003953
Iteration 132/1000 | Loss: 0.00003953
Iteration 133/1000 | Loss: 0.00003953
Iteration 134/1000 | Loss: 0.00003953
Iteration 135/1000 | Loss: 0.00003953
Iteration 136/1000 | Loss: 0.00003953
Iteration 137/1000 | Loss: 0.00003953
Iteration 138/1000 | Loss: 0.00003953
Iteration 139/1000 | Loss: 0.00003953
Iteration 140/1000 | Loss: 0.00003953
Iteration 141/1000 | Loss: 0.00003953
Iteration 142/1000 | Loss: 0.00003952
Iteration 143/1000 | Loss: 0.00003952
Iteration 144/1000 | Loss: 0.00003952
Iteration 145/1000 | Loss: 0.00003952
Iteration 146/1000 | Loss: 0.00003951
Iteration 147/1000 | Loss: 0.00003951
Iteration 148/1000 | Loss: 0.00003951
Iteration 149/1000 | Loss: 0.00003951
Iteration 150/1000 | Loss: 0.00003951
Iteration 151/1000 | Loss: 0.00003951
Iteration 152/1000 | Loss: 0.00003951
Iteration 153/1000 | Loss: 0.00003951
Iteration 154/1000 | Loss: 0.00003950
Iteration 155/1000 | Loss: 0.00003950
Iteration 156/1000 | Loss: 0.00003950
Iteration 157/1000 | Loss: 0.00003950
Iteration 158/1000 | Loss: 0.00003950
Iteration 159/1000 | Loss: 0.00003950
Iteration 160/1000 | Loss: 0.00003950
Iteration 161/1000 | Loss: 0.00003950
Iteration 162/1000 | Loss: 0.00003949
Iteration 163/1000 | Loss: 0.00003949
Iteration 164/1000 | Loss: 0.00003949
Iteration 165/1000 | Loss: 0.00003948
Iteration 166/1000 | Loss: 0.00003948
Iteration 167/1000 | Loss: 0.00003948
Iteration 168/1000 | Loss: 0.00003948
Iteration 169/1000 | Loss: 0.00003947
Iteration 170/1000 | Loss: 0.00003947
Iteration 171/1000 | Loss: 0.00003947
Iteration 172/1000 | Loss: 0.00003946
Iteration 173/1000 | Loss: 0.00003946
Iteration 174/1000 | Loss: 0.00003946
Iteration 175/1000 | Loss: 0.00003946
Iteration 176/1000 | Loss: 0.00003945
Iteration 177/1000 | Loss: 0.00003944
Iteration 178/1000 | Loss: 0.00003943
Iteration 179/1000 | Loss: 0.00003942
Iteration 180/1000 | Loss: 0.00003942
Iteration 181/1000 | Loss: 0.00003942
Iteration 182/1000 | Loss: 0.00003941
Iteration 183/1000 | Loss: 0.00003941
Iteration 184/1000 | Loss: 0.00003941
Iteration 185/1000 | Loss: 0.00003940
Iteration 186/1000 | Loss: 0.00003940
Iteration 187/1000 | Loss: 0.00003940
Iteration 188/1000 | Loss: 0.00003940
Iteration 189/1000 | Loss: 0.00003939
Iteration 190/1000 | Loss: 0.00003939
Iteration 191/1000 | Loss: 0.00003939
Iteration 192/1000 | Loss: 0.00003939
Iteration 193/1000 | Loss: 0.00003938
Iteration 194/1000 | Loss: 0.00003938
Iteration 195/1000 | Loss: 0.00003938
Iteration 196/1000 | Loss: 0.00003938
Iteration 197/1000 | Loss: 0.00003938
Iteration 198/1000 | Loss: 0.00003938
Iteration 199/1000 | Loss: 0.00003938
Iteration 200/1000 | Loss: 0.00003938
Iteration 201/1000 | Loss: 0.00003937
Iteration 202/1000 | Loss: 0.00003937
Iteration 203/1000 | Loss: 0.00003937
Iteration 204/1000 | Loss: 0.00003937
Iteration 205/1000 | Loss: 0.00003937
Iteration 206/1000 | Loss: 0.00003937
Iteration 207/1000 | Loss: 0.00003937
Iteration 208/1000 | Loss: 0.00003937
Iteration 209/1000 | Loss: 0.00003937
Iteration 210/1000 | Loss: 0.00003937
Iteration 211/1000 | Loss: 0.00003936
Iteration 212/1000 | Loss: 0.00003936
Iteration 213/1000 | Loss: 0.00003936
Iteration 214/1000 | Loss: 0.00003936
Iteration 215/1000 | Loss: 0.00003936
Iteration 216/1000 | Loss: 0.00003936
Iteration 217/1000 | Loss: 0.00003936
Iteration 218/1000 | Loss: 0.00003936
Iteration 219/1000 | Loss: 0.00003936
Iteration 220/1000 | Loss: 0.00003936
Iteration 221/1000 | Loss: 0.00003935
Iteration 222/1000 | Loss: 0.00003935
Iteration 223/1000 | Loss: 0.00003935
Iteration 224/1000 | Loss: 0.00003935
Iteration 225/1000 | Loss: 0.00003935
Iteration 226/1000 | Loss: 0.00003935
Iteration 227/1000 | Loss: 0.00003935
Iteration 228/1000 | Loss: 0.00003935
Iteration 229/1000 | Loss: 0.00003935
Iteration 230/1000 | Loss: 0.00003935
Iteration 231/1000 | Loss: 0.00003935
Iteration 232/1000 | Loss: 0.00003935
Iteration 233/1000 | Loss: 0.00003935
Iteration 234/1000 | Loss: 0.00003934
Iteration 235/1000 | Loss: 0.00003934
Iteration 236/1000 | Loss: 0.00003934
Iteration 237/1000 | Loss: 0.00003934
Iteration 238/1000 | Loss: 0.00003934
Iteration 239/1000 | Loss: 0.00003934
Iteration 240/1000 | Loss: 0.00003934
Iteration 241/1000 | Loss: 0.00003934
Iteration 242/1000 | Loss: 0.00003934
Iteration 243/1000 | Loss: 0.00003934
Iteration 244/1000 | Loss: 0.00003934
Iteration 245/1000 | Loss: 0.00003934
Iteration 246/1000 | Loss: 0.00003934
Iteration 247/1000 | Loss: 0.00003934
Iteration 248/1000 | Loss: 0.00003934
Iteration 249/1000 | Loss: 0.00003934
Iteration 250/1000 | Loss: 0.00003934
Iteration 251/1000 | Loss: 0.00003934
Iteration 252/1000 | Loss: 0.00003933
Iteration 253/1000 | Loss: 0.00003933
Iteration 254/1000 | Loss: 0.00003933
Iteration 255/1000 | Loss: 0.00003933
Iteration 256/1000 | Loss: 0.00003933
Iteration 257/1000 | Loss: 0.00003933
Iteration 258/1000 | Loss: 0.00003933
Iteration 259/1000 | Loss: 0.00003933
Iteration 260/1000 | Loss: 0.00003933
Iteration 261/1000 | Loss: 0.00003933
Iteration 262/1000 | Loss: 0.00003933
Iteration 263/1000 | Loss: 0.00003933
Iteration 264/1000 | Loss: 0.00003933
Iteration 265/1000 | Loss: 0.00003933
Iteration 266/1000 | Loss: 0.00003933
Iteration 267/1000 | Loss: 0.00003933
Iteration 268/1000 | Loss: 0.00003933
Iteration 269/1000 | Loss: 0.00003933
Iteration 270/1000 | Loss: 0.00003933
Iteration 271/1000 | Loss: 0.00003933
Iteration 272/1000 | Loss: 0.00003932
Iteration 273/1000 | Loss: 0.00003932
Iteration 274/1000 | Loss: 0.00003932
Iteration 275/1000 | Loss: 0.00003932
Iteration 276/1000 | Loss: 0.00003932
Iteration 277/1000 | Loss: 0.00003932
Iteration 278/1000 | Loss: 0.00003932
Iteration 279/1000 | Loss: 0.00003932
Iteration 280/1000 | Loss: 0.00003932
Iteration 281/1000 | Loss: 0.00003932
Iteration 282/1000 | Loss: 0.00003932
Iteration 283/1000 | Loss: 0.00003932
Iteration 284/1000 | Loss: 0.00003932
Iteration 285/1000 | Loss: 0.00003932
Iteration 286/1000 | Loss: 0.00003931
Iteration 287/1000 | Loss: 0.00003931
Iteration 288/1000 | Loss: 0.00003931
Iteration 289/1000 | Loss: 0.00003931
Iteration 290/1000 | Loss: 0.00003931
Iteration 291/1000 | Loss: 0.00003931
Iteration 292/1000 | Loss: 0.00003931
Iteration 293/1000 | Loss: 0.00003931
Iteration 294/1000 | Loss: 0.00003931
Iteration 295/1000 | Loss: 0.00003931
Iteration 296/1000 | Loss: 0.00003931
Iteration 297/1000 | Loss: 0.00003931
Iteration 298/1000 | Loss: 0.00003931
Iteration 299/1000 | Loss: 0.00003931
Iteration 300/1000 | Loss: 0.00003931
Iteration 301/1000 | Loss: 0.00003931
Iteration 302/1000 | Loss: 0.00003931
Iteration 303/1000 | Loss: 0.00003931
Iteration 304/1000 | Loss: 0.00003931
Iteration 305/1000 | Loss: 0.00003931
Iteration 306/1000 | Loss: 0.00003931
Iteration 307/1000 | Loss: 0.00003931
Iteration 308/1000 | Loss: 0.00003931
Iteration 309/1000 | Loss: 0.00003931
Iteration 310/1000 | Loss: 0.00003931
Iteration 311/1000 | Loss: 0.00003931
Iteration 312/1000 | Loss: 0.00003931
Iteration 313/1000 | Loss: 0.00003931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 313. Stopping optimization.
Last 5 losses: [3.930552338715643e-05, 3.930552338715643e-05, 3.930552338715643e-05, 3.930552338715643e-05, 3.930552338715643e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.930552338715643e-05

Optimization complete. Final v2v error: 3.413224697113037 mm

Highest mean error: 11.191141128540039 mm for frame 120

Lowest mean error: 2.5320217609405518 mm for frame 80

Saving results

Total time: 136.58313298225403
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_janna_posed_016/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_janna_posed_016/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00692769
Iteration 2/25 | Loss: 0.00155225
Iteration 3/25 | Loss: 0.00135640
Iteration 4/25 | Loss: 0.00132367
Iteration 5/25 | Loss: 0.00131267
Iteration 6/25 | Loss: 0.00131127
Iteration 7/25 | Loss: 0.00131231
Iteration 8/25 | Loss: 0.00130812
Iteration 9/25 | Loss: 0.00131173
Iteration 10/25 | Loss: 0.00130543
Iteration 11/25 | Loss: 0.00130736
Iteration 12/25 | Loss: 0.00130621
Iteration 13/25 | Loss: 0.00130229
Iteration 14/25 | Loss: 0.00130000
Iteration 15/25 | Loss: 0.00129754
Iteration 16/25 | Loss: 0.00129681
Iteration 17/25 | Loss: 0.00129662
Iteration 18/25 | Loss: 0.00129658
Iteration 19/25 | Loss: 0.00129658
Iteration 20/25 | Loss: 0.00129657
Iteration 21/25 | Loss: 0.00129657
Iteration 22/25 | Loss: 0.00129657
Iteration 23/25 | Loss: 0.00129657
Iteration 24/25 | Loss: 0.00129657
Iteration 25/25 | Loss: 0.00129657

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60883057
Iteration 2/25 | Loss: 0.00090002
Iteration 3/25 | Loss: 0.00090002
Iteration 4/25 | Loss: 0.00090002
Iteration 5/25 | Loss: 0.00090002
Iteration 6/25 | Loss: 0.00090002
Iteration 7/25 | Loss: 0.00090002
Iteration 8/25 | Loss: 0.00090002
Iteration 9/25 | Loss: 0.00090002
Iteration 10/25 | Loss: 0.00090002
Iteration 11/25 | Loss: 0.00090002
Iteration 12/25 | Loss: 0.00090002
Iteration 13/25 | Loss: 0.00090002
Iteration 14/25 | Loss: 0.00090002
Iteration 15/25 | Loss: 0.00090002
Iteration 16/25 | Loss: 0.00090002
Iteration 17/25 | Loss: 0.00090002
Iteration 18/25 | Loss: 0.00090002
Iteration 19/25 | Loss: 0.00090002
Iteration 20/25 | Loss: 0.00090002
Iteration 21/25 | Loss: 0.00090002
Iteration 22/25 | Loss: 0.00090002
Iteration 23/25 | Loss: 0.00090002
Iteration 24/25 | Loss: 0.00090002
Iteration 25/25 | Loss: 0.00090002

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090002
Iteration 2/1000 | Loss: 0.00011278
Iteration 3/1000 | Loss: 0.00007926
Iteration 4/1000 | Loss: 0.00005084
Iteration 5/1000 | Loss: 0.00003996
Iteration 6/1000 | Loss: 0.00003309
Iteration 7/1000 | Loss: 0.00003070
Iteration 8/1000 | Loss: 0.00002933
Iteration 9/1000 | Loss: 0.00002839
Iteration 10/1000 | Loss: 0.00002776
Iteration 11/1000 | Loss: 0.00005974
Iteration 12/1000 | Loss: 0.00003028
Iteration 13/1000 | Loss: 0.00002867
Iteration 14/1000 | Loss: 0.00002797
Iteration 15/1000 | Loss: 0.00002742
Iteration 16/1000 | Loss: 0.00002694
Iteration 17/1000 | Loss: 0.00032573
Iteration 18/1000 | Loss: 0.00017410
Iteration 19/1000 | Loss: 0.00004283
Iteration 20/1000 | Loss: 0.00003616
Iteration 21/1000 | Loss: 0.00003215
Iteration 22/1000 | Loss: 0.00002885
Iteration 23/1000 | Loss: 0.00002685
Iteration 24/1000 | Loss: 0.00002649
Iteration 25/1000 | Loss: 0.00002642
Iteration 26/1000 | Loss: 0.00002622
Iteration 27/1000 | Loss: 0.00002606
Iteration 28/1000 | Loss: 0.00030528
Iteration 29/1000 | Loss: 0.00030528
Iteration 30/1000 | Loss: 0.00005788
Iteration 31/1000 | Loss: 0.00002694
Iteration 32/1000 | Loss: 0.00002602
Iteration 33/1000 | Loss: 0.00031236
Iteration 34/1000 | Loss: 0.00005950
Iteration 35/1000 | Loss: 0.00002733
Iteration 36/1000 | Loss: 0.00002607
Iteration 37/1000 | Loss: 0.00030239
Iteration 38/1000 | Loss: 0.00005433
Iteration 39/1000 | Loss: 0.00003609
Iteration 40/1000 | Loss: 0.00003221
Iteration 41/1000 | Loss: 0.00005675
Iteration 42/1000 | Loss: 0.00005178
Iteration 43/1000 | Loss: 0.00005185
Iteration 44/1000 | Loss: 0.00005273
Iteration 45/1000 | Loss: 0.00004892
Iteration 46/1000 | Loss: 0.00002703
Iteration 47/1000 | Loss: 0.00002592
Iteration 48/1000 | Loss: 0.00002497
Iteration 49/1000 | Loss: 0.00002476
Iteration 50/1000 | Loss: 0.00002476
Iteration 51/1000 | Loss: 0.00002471
Iteration 52/1000 | Loss: 0.00002470
Iteration 53/1000 | Loss: 0.00002470
Iteration 54/1000 | Loss: 0.00002463
Iteration 55/1000 | Loss: 0.00002463
Iteration 56/1000 | Loss: 0.00002461
Iteration 57/1000 | Loss: 0.00002461
Iteration 58/1000 | Loss: 0.00002460
Iteration 59/1000 | Loss: 0.00002459
Iteration 60/1000 | Loss: 0.00002459
Iteration 61/1000 | Loss: 0.00002458
Iteration 62/1000 | Loss: 0.00002458
Iteration 63/1000 | Loss: 0.00002457
Iteration 64/1000 | Loss: 0.00002457
Iteration 65/1000 | Loss: 0.00002457
Iteration 66/1000 | Loss: 0.00002456
Iteration 67/1000 | Loss: 0.00002456
Iteration 68/1000 | Loss: 0.00002456
Iteration 69/1000 | Loss: 0.00002456
Iteration 70/1000 | Loss: 0.00002455
Iteration 71/1000 | Loss: 0.00002455
Iteration 72/1000 | Loss: 0.00002455
Iteration 73/1000 | Loss: 0.00002454
Iteration 74/1000 | Loss: 0.00002454
Iteration 75/1000 | Loss: 0.00002454
Iteration 76/1000 | Loss: 0.00002454
Iteration 77/1000 | Loss: 0.00002453
Iteration 78/1000 | Loss: 0.00002453
Iteration 79/1000 | Loss: 0.00002453
Iteration 80/1000 | Loss: 0.00002453
Iteration 81/1000 | Loss: 0.00002453
Iteration 82/1000 | Loss: 0.00002452
Iteration 83/1000 | Loss: 0.00002452
Iteration 84/1000 | Loss: 0.00002452
Iteration 85/1000 | Loss: 0.00002451
Iteration 86/1000 | Loss: 0.00002451
Iteration 87/1000 | Loss: 0.00002451
Iteration 88/1000 | Loss: 0.00002450
Iteration 89/1000 | Loss: 0.00002450
Iteration 90/1000 | Loss: 0.00002446
Iteration 91/1000 | Loss: 0.00002443
Iteration 92/1000 | Loss: 0.00002442
Iteration 93/1000 | Loss: 0.00002442
Iteration 94/1000 | Loss: 0.00002442
Iteration 95/1000 | Loss: 0.00006116
Iteration 96/1000 | Loss: 0.00006160
Iteration 97/1000 | Loss: 0.00003643
Iteration 98/1000 | Loss: 0.00003223
Iteration 99/1000 | Loss: 0.00003014
Iteration 100/1000 | Loss: 0.00002878
Iteration 101/1000 | Loss: 0.00002798
Iteration 102/1000 | Loss: 0.00002738
Iteration 103/1000 | Loss: 0.00002701
Iteration 104/1000 | Loss: 0.00002656
Iteration 105/1000 | Loss: 0.00002606
Iteration 106/1000 | Loss: 0.00002553
Iteration 107/1000 | Loss: 0.00003355
Iteration 108/1000 | Loss: 0.00002767
Iteration 109/1000 | Loss: 0.00003304
Iteration 110/1000 | Loss: 0.00002732
Iteration 111/1000 | Loss: 0.00003035
Iteration 112/1000 | Loss: 0.00002569
Iteration 113/1000 | Loss: 0.00002546
Iteration 114/1000 | Loss: 0.00002542
Iteration 115/1000 | Loss: 0.00002540
Iteration 116/1000 | Loss: 0.00002537
Iteration 117/1000 | Loss: 0.00002534
Iteration 118/1000 | Loss: 0.00002531
Iteration 119/1000 | Loss: 0.00002528
Iteration 120/1000 | Loss: 0.00002525
Iteration 121/1000 | Loss: 0.00002522
Iteration 122/1000 | Loss: 0.00002519
Iteration 123/1000 | Loss: 0.00002518
Iteration 124/1000 | Loss: 0.00002518
Iteration 125/1000 | Loss: 0.00002515
Iteration 126/1000 | Loss: 0.00002514
Iteration 127/1000 | Loss: 0.00002514
Iteration 128/1000 | Loss: 0.00002514
Iteration 129/1000 | Loss: 0.00002514
Iteration 130/1000 | Loss: 0.00002514
Iteration 131/1000 | Loss: 0.00002514
Iteration 132/1000 | Loss: 0.00002513
Iteration 133/1000 | Loss: 0.00002512
Iteration 134/1000 | Loss: 0.00002512
Iteration 135/1000 | Loss: 0.00002512
Iteration 136/1000 | Loss: 0.00002512
Iteration 137/1000 | Loss: 0.00002512
Iteration 138/1000 | Loss: 0.00002511
Iteration 139/1000 | Loss: 0.00002511
Iteration 140/1000 | Loss: 0.00002511
Iteration 141/1000 | Loss: 0.00002511
Iteration 142/1000 | Loss: 0.00002511
Iteration 143/1000 | Loss: 0.00002511
Iteration 144/1000 | Loss: 0.00002511
Iteration 145/1000 | Loss: 0.00002511
Iteration 146/1000 | Loss: 0.00002511
Iteration 147/1000 | Loss: 0.00002511
Iteration 148/1000 | Loss: 0.00002511
Iteration 149/1000 | Loss: 0.00002510
Iteration 150/1000 | Loss: 0.00002510
Iteration 151/1000 | Loss: 0.00002510
Iteration 152/1000 | Loss: 0.00002510
Iteration 153/1000 | Loss: 0.00002510
Iteration 154/1000 | Loss: 0.00002510
Iteration 155/1000 | Loss: 0.00002510
Iteration 156/1000 | Loss: 0.00002509
Iteration 157/1000 | Loss: 0.00002509
Iteration 158/1000 | Loss: 0.00002509
Iteration 159/1000 | Loss: 0.00002509
Iteration 160/1000 | Loss: 0.00002509
Iteration 161/1000 | Loss: 0.00002509
Iteration 162/1000 | Loss: 0.00002509
Iteration 163/1000 | Loss: 0.00002509
Iteration 164/1000 | Loss: 0.00002508
Iteration 165/1000 | Loss: 0.00002508
Iteration 166/1000 | Loss: 0.00002508
Iteration 167/1000 | Loss: 0.00002508
Iteration 168/1000 | Loss: 0.00002508
Iteration 169/1000 | Loss: 0.00002508
Iteration 170/1000 | Loss: 0.00002508
Iteration 171/1000 | Loss: 0.00002508
Iteration 172/1000 | Loss: 0.00002508
Iteration 173/1000 | Loss: 0.00002507
Iteration 174/1000 | Loss: 0.00002507
Iteration 175/1000 | Loss: 0.00002507
Iteration 176/1000 | Loss: 0.00002507
Iteration 177/1000 | Loss: 0.00002507
Iteration 178/1000 | Loss: 0.00002507
Iteration 179/1000 | Loss: 0.00002507
Iteration 180/1000 | Loss: 0.00002507
Iteration 181/1000 | Loss: 0.00002507
Iteration 182/1000 | Loss: 0.00002507
Iteration 183/1000 | Loss: 0.00002507
Iteration 184/1000 | Loss: 0.00002507
Iteration 185/1000 | Loss: 0.00002507
Iteration 186/1000 | Loss: 0.00002507
Iteration 187/1000 | Loss: 0.00002507
Iteration 188/1000 | Loss: 0.00002507
Iteration 189/1000 | Loss: 0.00002507
Iteration 190/1000 | Loss: 0.00002507
Iteration 191/1000 | Loss: 0.00002507
Iteration 192/1000 | Loss: 0.00002507
Iteration 193/1000 | Loss: 0.00002507
Iteration 194/1000 | Loss: 0.00002507
Iteration 195/1000 | Loss: 0.00002507
Iteration 196/1000 | Loss: 0.00002507
Iteration 197/1000 | Loss: 0.00002507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.5065019144676626e-05, 2.5065019144676626e-05, 2.5065019144676626e-05, 2.5065019144676626e-05, 2.5065019144676626e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5065019144676626e-05

Optimization complete. Final v2v error: 4.106546401977539 mm

Highest mean error: 8.174458503723145 mm for frame 108

Lowest mean error: 3.252701997756958 mm for frame 20

Saving results

Total time: 131.32956075668335
