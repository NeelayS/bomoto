Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=267, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14952-15007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386273
Iteration 2/25 | Loss: 0.00131295
Iteration 3/25 | Loss: 0.00126140
Iteration 4/25 | Loss: 0.00125585
Iteration 5/25 | Loss: 0.00125211
Iteration 6/25 | Loss: 0.00125211
Iteration 7/25 | Loss: 0.00125211
Iteration 8/25 | Loss: 0.00125211
Iteration 9/25 | Loss: 0.00125211
Iteration 10/25 | Loss: 0.00125211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012521054595708847, 0.0012521054595708847, 0.0012521054595708847, 0.0012521054595708847, 0.0012521054595708847]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012521054595708847

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57810271
Iteration 2/25 | Loss: 0.00074729
Iteration 3/25 | Loss: 0.00074724
Iteration 4/25 | Loss: 0.00074724
Iteration 5/25 | Loss: 0.00074724
Iteration 6/25 | Loss: 0.00074724
Iteration 7/25 | Loss: 0.00074724
Iteration 8/25 | Loss: 0.00074724
Iteration 9/25 | Loss: 0.00074724
Iteration 10/25 | Loss: 0.00074724
Iteration 11/25 | Loss: 0.00074724
Iteration 12/25 | Loss: 0.00074723
Iteration 13/25 | Loss: 0.00074723
Iteration 14/25 | Loss: 0.00074723
Iteration 15/25 | Loss: 0.00074723
Iteration 16/25 | Loss: 0.00074723
Iteration 17/25 | Loss: 0.00074723
Iteration 18/25 | Loss: 0.00074723
Iteration 19/25 | Loss: 0.00074723
Iteration 20/25 | Loss: 0.00074723
Iteration 21/25 | Loss: 0.00074723
Iteration 22/25 | Loss: 0.00074723
Iteration 23/25 | Loss: 0.00074723
Iteration 24/25 | Loss: 0.00074723
Iteration 25/25 | Loss: 0.00074723

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074723
Iteration 2/1000 | Loss: 0.00002837
Iteration 3/1000 | Loss: 0.00001922
Iteration 4/1000 | Loss: 0.00001743
Iteration 5/1000 | Loss: 0.00001613
Iteration 6/1000 | Loss: 0.00001547
Iteration 7/1000 | Loss: 0.00001510
Iteration 8/1000 | Loss: 0.00001484
Iteration 9/1000 | Loss: 0.00001448
Iteration 10/1000 | Loss: 0.00001417
Iteration 11/1000 | Loss: 0.00001405
Iteration 12/1000 | Loss: 0.00001402
Iteration 13/1000 | Loss: 0.00001395
Iteration 14/1000 | Loss: 0.00001379
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001363
Iteration 17/1000 | Loss: 0.00001363
Iteration 18/1000 | Loss: 0.00001363
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001361
Iteration 21/1000 | Loss: 0.00001356
Iteration 22/1000 | Loss: 0.00001356
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001352
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001348
Iteration 30/1000 | Loss: 0.00001347
Iteration 31/1000 | Loss: 0.00001347
Iteration 32/1000 | Loss: 0.00001347
Iteration 33/1000 | Loss: 0.00001347
Iteration 34/1000 | Loss: 0.00001346
Iteration 35/1000 | Loss: 0.00001346
Iteration 36/1000 | Loss: 0.00001346
Iteration 37/1000 | Loss: 0.00001344
Iteration 38/1000 | Loss: 0.00001344
Iteration 39/1000 | Loss: 0.00001344
Iteration 40/1000 | Loss: 0.00001344
Iteration 41/1000 | Loss: 0.00001344
Iteration 42/1000 | Loss: 0.00001344
Iteration 43/1000 | Loss: 0.00001343
Iteration 44/1000 | Loss: 0.00001343
Iteration 45/1000 | Loss: 0.00001343
Iteration 46/1000 | Loss: 0.00001342
Iteration 47/1000 | Loss: 0.00001342
Iteration 48/1000 | Loss: 0.00001342
Iteration 49/1000 | Loss: 0.00001342
Iteration 50/1000 | Loss: 0.00001341
Iteration 51/1000 | Loss: 0.00001341
Iteration 52/1000 | Loss: 0.00001341
Iteration 53/1000 | Loss: 0.00001341
Iteration 54/1000 | Loss: 0.00001341
Iteration 55/1000 | Loss: 0.00001341
Iteration 56/1000 | Loss: 0.00001341
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00001341
Iteration 59/1000 | Loss: 0.00001340
Iteration 60/1000 | Loss: 0.00001340
Iteration 61/1000 | Loss: 0.00001340
Iteration 62/1000 | Loss: 0.00001340
Iteration 63/1000 | Loss: 0.00001340
Iteration 64/1000 | Loss: 0.00001340
Iteration 65/1000 | Loss: 0.00001340
Iteration 66/1000 | Loss: 0.00001340
Iteration 67/1000 | Loss: 0.00001340
Iteration 68/1000 | Loss: 0.00001340
Iteration 69/1000 | Loss: 0.00001340
Iteration 70/1000 | Loss: 0.00001339
Iteration 71/1000 | Loss: 0.00001339
Iteration 72/1000 | Loss: 0.00001339
Iteration 73/1000 | Loss: 0.00001339
Iteration 74/1000 | Loss: 0.00001339
Iteration 75/1000 | Loss: 0.00001338
Iteration 76/1000 | Loss: 0.00001338
Iteration 77/1000 | Loss: 0.00001338
Iteration 78/1000 | Loss: 0.00001338
Iteration 79/1000 | Loss: 0.00001338
Iteration 80/1000 | Loss: 0.00001338
Iteration 81/1000 | Loss: 0.00001338
Iteration 82/1000 | Loss: 0.00001338
Iteration 83/1000 | Loss: 0.00001337
Iteration 84/1000 | Loss: 0.00001337
Iteration 85/1000 | Loss: 0.00001337
Iteration 86/1000 | Loss: 0.00001337
Iteration 87/1000 | Loss: 0.00001337
Iteration 88/1000 | Loss: 0.00001336
Iteration 89/1000 | Loss: 0.00001336
Iteration 90/1000 | Loss: 0.00001336
Iteration 91/1000 | Loss: 0.00001336
Iteration 92/1000 | Loss: 0.00001336
Iteration 93/1000 | Loss: 0.00001336
Iteration 94/1000 | Loss: 0.00001336
Iteration 95/1000 | Loss: 0.00001336
Iteration 96/1000 | Loss: 0.00001336
Iteration 97/1000 | Loss: 0.00001336
Iteration 98/1000 | Loss: 0.00001335
Iteration 99/1000 | Loss: 0.00001335
Iteration 100/1000 | Loss: 0.00001335
Iteration 101/1000 | Loss: 0.00001335
Iteration 102/1000 | Loss: 0.00001335
Iteration 103/1000 | Loss: 0.00001335
Iteration 104/1000 | Loss: 0.00001335
Iteration 105/1000 | Loss: 0.00001335
Iteration 106/1000 | Loss: 0.00001335
Iteration 107/1000 | Loss: 0.00001334
Iteration 108/1000 | Loss: 0.00001334
Iteration 109/1000 | Loss: 0.00001334
Iteration 110/1000 | Loss: 0.00001334
Iteration 111/1000 | Loss: 0.00001334
Iteration 112/1000 | Loss: 0.00001334
Iteration 113/1000 | Loss: 0.00001334
Iteration 114/1000 | Loss: 0.00001334
Iteration 115/1000 | Loss: 0.00001334
Iteration 116/1000 | Loss: 0.00001333
Iteration 117/1000 | Loss: 0.00001333
Iteration 118/1000 | Loss: 0.00001333
Iteration 119/1000 | Loss: 0.00001333
Iteration 120/1000 | Loss: 0.00001333
Iteration 121/1000 | Loss: 0.00001333
Iteration 122/1000 | Loss: 0.00001333
Iteration 123/1000 | Loss: 0.00001333
Iteration 124/1000 | Loss: 0.00001333
Iteration 125/1000 | Loss: 0.00001333
Iteration 126/1000 | Loss: 0.00001333
Iteration 127/1000 | Loss: 0.00001333
Iteration 128/1000 | Loss: 0.00001332
Iteration 129/1000 | Loss: 0.00001332
Iteration 130/1000 | Loss: 0.00001332
Iteration 131/1000 | Loss: 0.00001332
Iteration 132/1000 | Loss: 0.00001332
Iteration 133/1000 | Loss: 0.00001332
Iteration 134/1000 | Loss: 0.00001332
Iteration 135/1000 | Loss: 0.00001332
Iteration 136/1000 | Loss: 0.00001332
Iteration 137/1000 | Loss: 0.00001332
Iteration 138/1000 | Loss: 0.00001332
Iteration 139/1000 | Loss: 0.00001332
Iteration 140/1000 | Loss: 0.00001331
Iteration 141/1000 | Loss: 0.00001331
Iteration 142/1000 | Loss: 0.00001331
Iteration 143/1000 | Loss: 0.00001331
Iteration 144/1000 | Loss: 0.00001331
Iteration 145/1000 | Loss: 0.00001331
Iteration 146/1000 | Loss: 0.00001331
Iteration 147/1000 | Loss: 0.00001331
Iteration 148/1000 | Loss: 0.00001331
Iteration 149/1000 | Loss: 0.00001331
Iteration 150/1000 | Loss: 0.00001331
Iteration 151/1000 | Loss: 0.00001331
Iteration 152/1000 | Loss: 0.00001331
Iteration 153/1000 | Loss: 0.00001330
Iteration 154/1000 | Loss: 0.00001330
Iteration 155/1000 | Loss: 0.00001330
Iteration 156/1000 | Loss: 0.00001330
Iteration 157/1000 | Loss: 0.00001330
Iteration 158/1000 | Loss: 0.00001330
Iteration 159/1000 | Loss: 0.00001330
Iteration 160/1000 | Loss: 0.00001330
Iteration 161/1000 | Loss: 0.00001330
Iteration 162/1000 | Loss: 0.00001330
Iteration 163/1000 | Loss: 0.00001330
Iteration 164/1000 | Loss: 0.00001330
Iteration 165/1000 | Loss: 0.00001330
Iteration 166/1000 | Loss: 0.00001330
Iteration 167/1000 | Loss: 0.00001330
Iteration 168/1000 | Loss: 0.00001329
Iteration 169/1000 | Loss: 0.00001329
Iteration 170/1000 | Loss: 0.00001329
Iteration 171/1000 | Loss: 0.00001329
Iteration 172/1000 | Loss: 0.00001329
Iteration 173/1000 | Loss: 0.00001329
Iteration 174/1000 | Loss: 0.00001329
Iteration 175/1000 | Loss: 0.00001329
Iteration 176/1000 | Loss: 0.00001329
Iteration 177/1000 | Loss: 0.00001329
Iteration 178/1000 | Loss: 0.00001329
Iteration 179/1000 | Loss: 0.00001329
Iteration 180/1000 | Loss: 0.00001329
Iteration 181/1000 | Loss: 0.00001329
Iteration 182/1000 | Loss: 0.00001329
Iteration 183/1000 | Loss: 0.00001329
Iteration 184/1000 | Loss: 0.00001329
Iteration 185/1000 | Loss: 0.00001329
Iteration 186/1000 | Loss: 0.00001329
Iteration 187/1000 | Loss: 0.00001329
Iteration 188/1000 | Loss: 0.00001329
Iteration 189/1000 | Loss: 0.00001329
Iteration 190/1000 | Loss: 0.00001329
Iteration 191/1000 | Loss: 0.00001329
Iteration 192/1000 | Loss: 0.00001329
Iteration 193/1000 | Loss: 0.00001329
Iteration 194/1000 | Loss: 0.00001329
Iteration 195/1000 | Loss: 0.00001329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.328927646682132e-05, 1.328927646682132e-05, 1.328927646682132e-05, 1.328927646682132e-05, 1.328927646682132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.328927646682132e-05

Optimization complete. Final v2v error: 3.1074471473693848 mm

Highest mean error: 3.503387928009033 mm for frame 128

Lowest mean error: 2.8099112510681152 mm for frame 36

Saving results

Total time: 46.60343837738037
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946179
Iteration 2/25 | Loss: 0.00155164
Iteration 3/25 | Loss: 0.00137594
Iteration 4/25 | Loss: 0.00136191
Iteration 5/25 | Loss: 0.00135716
Iteration 6/25 | Loss: 0.00135599
Iteration 7/25 | Loss: 0.00135599
Iteration 8/25 | Loss: 0.00135599
Iteration 9/25 | Loss: 0.00135599
Iteration 10/25 | Loss: 0.00135599
Iteration 11/25 | Loss: 0.00135599
Iteration 12/25 | Loss: 0.00135599
Iteration 13/25 | Loss: 0.00135599
Iteration 14/25 | Loss: 0.00135599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013559901854023337, 0.0013559901854023337, 0.0013559901854023337, 0.0013559901854023337, 0.0013559901854023337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013559901854023337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40930617
Iteration 2/25 | Loss: 0.00104420
Iteration 3/25 | Loss: 0.00104420
Iteration 4/25 | Loss: 0.00104420
Iteration 5/25 | Loss: 0.00104420
Iteration 6/25 | Loss: 0.00104420
Iteration 7/25 | Loss: 0.00104420
Iteration 8/25 | Loss: 0.00104420
Iteration 9/25 | Loss: 0.00104420
Iteration 10/25 | Loss: 0.00104420
Iteration 11/25 | Loss: 0.00104420
Iteration 12/25 | Loss: 0.00104420
Iteration 13/25 | Loss: 0.00104420
Iteration 14/25 | Loss: 0.00104420
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010442002676427364, 0.0010442002676427364, 0.0010442002676427364, 0.0010442002676427364, 0.0010442002676427364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010442002676427364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104420
Iteration 2/1000 | Loss: 0.00004327
Iteration 3/1000 | Loss: 0.00003301
Iteration 4/1000 | Loss: 0.00002991
Iteration 5/1000 | Loss: 0.00002840
Iteration 6/1000 | Loss: 0.00002698
Iteration 7/1000 | Loss: 0.00002591
Iteration 8/1000 | Loss: 0.00002535
Iteration 9/1000 | Loss: 0.00002490
Iteration 10/1000 | Loss: 0.00002456
Iteration 11/1000 | Loss: 0.00002431
Iteration 12/1000 | Loss: 0.00002425
Iteration 13/1000 | Loss: 0.00002411
Iteration 14/1000 | Loss: 0.00002402
Iteration 15/1000 | Loss: 0.00002400
Iteration 16/1000 | Loss: 0.00002400
Iteration 17/1000 | Loss: 0.00002400
Iteration 18/1000 | Loss: 0.00002400
Iteration 19/1000 | Loss: 0.00002400
Iteration 20/1000 | Loss: 0.00002400
Iteration 21/1000 | Loss: 0.00002400
Iteration 22/1000 | Loss: 0.00002400
Iteration 23/1000 | Loss: 0.00002399
Iteration 24/1000 | Loss: 0.00002398
Iteration 25/1000 | Loss: 0.00002397
Iteration 26/1000 | Loss: 0.00002397
Iteration 27/1000 | Loss: 0.00002395
Iteration 28/1000 | Loss: 0.00002395
Iteration 29/1000 | Loss: 0.00002395
Iteration 30/1000 | Loss: 0.00002395
Iteration 31/1000 | Loss: 0.00002395
Iteration 32/1000 | Loss: 0.00002395
Iteration 33/1000 | Loss: 0.00002395
Iteration 34/1000 | Loss: 0.00002394
Iteration 35/1000 | Loss: 0.00002394
Iteration 36/1000 | Loss: 0.00002394
Iteration 37/1000 | Loss: 0.00002394
Iteration 38/1000 | Loss: 0.00002393
Iteration 39/1000 | Loss: 0.00002392
Iteration 40/1000 | Loss: 0.00002391
Iteration 41/1000 | Loss: 0.00002391
Iteration 42/1000 | Loss: 0.00002390
Iteration 43/1000 | Loss: 0.00002389
Iteration 44/1000 | Loss: 0.00002389
Iteration 45/1000 | Loss: 0.00002389
Iteration 46/1000 | Loss: 0.00002389
Iteration 47/1000 | Loss: 0.00002389
Iteration 48/1000 | Loss: 0.00002389
Iteration 49/1000 | Loss: 0.00002389
Iteration 50/1000 | Loss: 0.00002389
Iteration 51/1000 | Loss: 0.00002389
Iteration 52/1000 | Loss: 0.00002388
Iteration 53/1000 | Loss: 0.00002388
Iteration 54/1000 | Loss: 0.00002388
Iteration 55/1000 | Loss: 0.00002388
Iteration 56/1000 | Loss: 0.00002388
Iteration 57/1000 | Loss: 0.00002388
Iteration 58/1000 | Loss: 0.00002387
Iteration 59/1000 | Loss: 0.00002387
Iteration 60/1000 | Loss: 0.00002386
Iteration 61/1000 | Loss: 0.00002386
Iteration 62/1000 | Loss: 0.00002385
Iteration 63/1000 | Loss: 0.00002385
Iteration 64/1000 | Loss: 0.00002385
Iteration 65/1000 | Loss: 0.00002385
Iteration 66/1000 | Loss: 0.00002384
Iteration 67/1000 | Loss: 0.00002384
Iteration 68/1000 | Loss: 0.00002384
Iteration 69/1000 | Loss: 0.00002383
Iteration 70/1000 | Loss: 0.00002383
Iteration 71/1000 | Loss: 0.00002383
Iteration 72/1000 | Loss: 0.00002383
Iteration 73/1000 | Loss: 0.00002383
Iteration 74/1000 | Loss: 0.00002383
Iteration 75/1000 | Loss: 0.00002382
Iteration 76/1000 | Loss: 0.00002382
Iteration 77/1000 | Loss: 0.00002382
Iteration 78/1000 | Loss: 0.00002382
Iteration 79/1000 | Loss: 0.00002382
Iteration 80/1000 | Loss: 0.00002382
Iteration 81/1000 | Loss: 0.00002381
Iteration 82/1000 | Loss: 0.00002381
Iteration 83/1000 | Loss: 0.00002381
Iteration 84/1000 | Loss: 0.00002381
Iteration 85/1000 | Loss: 0.00002380
Iteration 86/1000 | Loss: 0.00002380
Iteration 87/1000 | Loss: 0.00002380
Iteration 88/1000 | Loss: 0.00002380
Iteration 89/1000 | Loss: 0.00002380
Iteration 90/1000 | Loss: 0.00002380
Iteration 91/1000 | Loss: 0.00002380
Iteration 92/1000 | Loss: 0.00002380
Iteration 93/1000 | Loss: 0.00002380
Iteration 94/1000 | Loss: 0.00002380
Iteration 95/1000 | Loss: 0.00002380
Iteration 96/1000 | Loss: 0.00002380
Iteration 97/1000 | Loss: 0.00002380
Iteration 98/1000 | Loss: 0.00002380
Iteration 99/1000 | Loss: 0.00002380
Iteration 100/1000 | Loss: 0.00002379
Iteration 101/1000 | Loss: 0.00002379
Iteration 102/1000 | Loss: 0.00002379
Iteration 103/1000 | Loss: 0.00002379
Iteration 104/1000 | Loss: 0.00002379
Iteration 105/1000 | Loss: 0.00002378
Iteration 106/1000 | Loss: 0.00002378
Iteration 107/1000 | Loss: 0.00002378
Iteration 108/1000 | Loss: 0.00002378
Iteration 109/1000 | Loss: 0.00002378
Iteration 110/1000 | Loss: 0.00002378
Iteration 111/1000 | Loss: 0.00002378
Iteration 112/1000 | Loss: 0.00002377
Iteration 113/1000 | Loss: 0.00002377
Iteration 114/1000 | Loss: 0.00002377
Iteration 115/1000 | Loss: 0.00002377
Iteration 116/1000 | Loss: 0.00002377
Iteration 117/1000 | Loss: 0.00002377
Iteration 118/1000 | Loss: 0.00002376
Iteration 119/1000 | Loss: 0.00002376
Iteration 120/1000 | Loss: 0.00002376
Iteration 121/1000 | Loss: 0.00002376
Iteration 122/1000 | Loss: 0.00002376
Iteration 123/1000 | Loss: 0.00002376
Iteration 124/1000 | Loss: 0.00002376
Iteration 125/1000 | Loss: 0.00002376
Iteration 126/1000 | Loss: 0.00002376
Iteration 127/1000 | Loss: 0.00002376
Iteration 128/1000 | Loss: 0.00002376
Iteration 129/1000 | Loss: 0.00002376
Iteration 130/1000 | Loss: 0.00002375
Iteration 131/1000 | Loss: 0.00002375
Iteration 132/1000 | Loss: 0.00002375
Iteration 133/1000 | Loss: 0.00002375
Iteration 134/1000 | Loss: 0.00002375
Iteration 135/1000 | Loss: 0.00002375
Iteration 136/1000 | Loss: 0.00002375
Iteration 137/1000 | Loss: 0.00002375
Iteration 138/1000 | Loss: 0.00002375
Iteration 139/1000 | Loss: 0.00002375
Iteration 140/1000 | Loss: 0.00002375
Iteration 141/1000 | Loss: 0.00002375
Iteration 142/1000 | Loss: 0.00002374
Iteration 143/1000 | Loss: 0.00002374
Iteration 144/1000 | Loss: 0.00002374
Iteration 145/1000 | Loss: 0.00002374
Iteration 146/1000 | Loss: 0.00002374
Iteration 147/1000 | Loss: 0.00002374
Iteration 148/1000 | Loss: 0.00002374
Iteration 149/1000 | Loss: 0.00002373
Iteration 150/1000 | Loss: 0.00002373
Iteration 151/1000 | Loss: 0.00002373
Iteration 152/1000 | Loss: 0.00002373
Iteration 153/1000 | Loss: 0.00002373
Iteration 154/1000 | Loss: 0.00002373
Iteration 155/1000 | Loss: 0.00002373
Iteration 156/1000 | Loss: 0.00002373
Iteration 157/1000 | Loss: 0.00002373
Iteration 158/1000 | Loss: 0.00002373
Iteration 159/1000 | Loss: 0.00002373
Iteration 160/1000 | Loss: 0.00002373
Iteration 161/1000 | Loss: 0.00002373
Iteration 162/1000 | Loss: 0.00002373
Iteration 163/1000 | Loss: 0.00002373
Iteration 164/1000 | Loss: 0.00002373
Iteration 165/1000 | Loss: 0.00002373
Iteration 166/1000 | Loss: 0.00002373
Iteration 167/1000 | Loss: 0.00002373
Iteration 168/1000 | Loss: 0.00002373
Iteration 169/1000 | Loss: 0.00002372
Iteration 170/1000 | Loss: 0.00002372
Iteration 171/1000 | Loss: 0.00002372
Iteration 172/1000 | Loss: 0.00002372
Iteration 173/1000 | Loss: 0.00002372
Iteration 174/1000 | Loss: 0.00002372
Iteration 175/1000 | Loss: 0.00002372
Iteration 176/1000 | Loss: 0.00002372
Iteration 177/1000 | Loss: 0.00002372
Iteration 178/1000 | Loss: 0.00002372
Iteration 179/1000 | Loss: 0.00002372
Iteration 180/1000 | Loss: 0.00002372
Iteration 181/1000 | Loss: 0.00002371
Iteration 182/1000 | Loss: 0.00002371
Iteration 183/1000 | Loss: 0.00002371
Iteration 184/1000 | Loss: 0.00002371
Iteration 185/1000 | Loss: 0.00002371
Iteration 186/1000 | Loss: 0.00002371
Iteration 187/1000 | Loss: 0.00002371
Iteration 188/1000 | Loss: 0.00002371
Iteration 189/1000 | Loss: 0.00002371
Iteration 190/1000 | Loss: 0.00002371
Iteration 191/1000 | Loss: 0.00002371
Iteration 192/1000 | Loss: 0.00002371
Iteration 193/1000 | Loss: 0.00002371
Iteration 194/1000 | Loss: 0.00002371
Iteration 195/1000 | Loss: 0.00002371
Iteration 196/1000 | Loss: 0.00002371
Iteration 197/1000 | Loss: 0.00002371
Iteration 198/1000 | Loss: 0.00002371
Iteration 199/1000 | Loss: 0.00002371
Iteration 200/1000 | Loss: 0.00002371
Iteration 201/1000 | Loss: 0.00002371
Iteration 202/1000 | Loss: 0.00002371
Iteration 203/1000 | Loss: 0.00002371
Iteration 204/1000 | Loss: 0.00002371
Iteration 205/1000 | Loss: 0.00002371
Iteration 206/1000 | Loss: 0.00002371
Iteration 207/1000 | Loss: 0.00002371
Iteration 208/1000 | Loss: 0.00002371
Iteration 209/1000 | Loss: 0.00002371
Iteration 210/1000 | Loss: 0.00002371
Iteration 211/1000 | Loss: 0.00002371
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.3708718799753115e-05, 2.3708718799753115e-05, 2.3708718799753115e-05, 2.3708718799753115e-05, 2.3708718799753115e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3708718799753115e-05

Optimization complete. Final v2v error: 4.124099254608154 mm

Highest mean error: 4.199054718017578 mm for frame 21

Lowest mean error: 3.9586844444274902 mm for frame 151

Saving results

Total time: 39.24186158180237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046035
Iteration 2/25 | Loss: 0.01046035
Iteration 3/25 | Loss: 0.01046035
Iteration 4/25 | Loss: 0.01046035
Iteration 5/25 | Loss: 0.01046035
Iteration 6/25 | Loss: 0.01046035
Iteration 7/25 | Loss: 0.01046035
Iteration 8/25 | Loss: 0.01046035
Iteration 9/25 | Loss: 0.01046035
Iteration 10/25 | Loss: 0.01046035
Iteration 11/25 | Loss: 0.01046035
Iteration 12/25 | Loss: 0.01046035
Iteration 13/25 | Loss: 0.01046034
Iteration 14/25 | Loss: 0.01046034
Iteration 15/25 | Loss: 0.01046034
Iteration 16/25 | Loss: 0.01046034
Iteration 17/25 | Loss: 0.01046034
Iteration 18/25 | Loss: 0.01046034
Iteration 19/25 | Loss: 0.01046034
Iteration 20/25 | Loss: 0.01046034
Iteration 21/25 | Loss: 0.01046034
Iteration 22/25 | Loss: 0.01046034
Iteration 23/25 | Loss: 0.01046034
Iteration 24/25 | Loss: 0.01046034
Iteration 25/25 | Loss: 0.01046034

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57985520
Iteration 2/25 | Loss: 0.09077529
Iteration 3/25 | Loss: 0.09075951
Iteration 4/25 | Loss: 0.09075949
Iteration 5/25 | Loss: 0.09075949
Iteration 6/25 | Loss: 0.09075949
Iteration 7/25 | Loss: 0.09075949
Iteration 8/25 | Loss: 0.09075949
Iteration 9/25 | Loss: 0.09075949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.09075949341058731, 0.09075949341058731, 0.09075949341058731, 0.09075949341058731, 0.09075949341058731]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.09075949341058731

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.09075949
Iteration 2/1000 | Loss: 0.00163339
Iteration 3/1000 | Loss: 0.00324191
Iteration 4/1000 | Loss: 0.00097798
Iteration 5/1000 | Loss: 0.00538872
Iteration 6/1000 | Loss: 0.00029643
Iteration 7/1000 | Loss: 0.00142904
Iteration 8/1000 | Loss: 0.00127467
Iteration 9/1000 | Loss: 0.00027886
Iteration 10/1000 | Loss: 0.00010063
Iteration 11/1000 | Loss: 0.00004441
Iteration 12/1000 | Loss: 0.00050633
Iteration 13/1000 | Loss: 0.00025987
Iteration 14/1000 | Loss: 0.00004531
Iteration 15/1000 | Loss: 0.00003177
Iteration 16/1000 | Loss: 0.00154296
Iteration 17/1000 | Loss: 0.00005356
Iteration 18/1000 | Loss: 0.00027514
Iteration 19/1000 | Loss: 0.00002512
Iteration 20/1000 | Loss: 0.00004190
Iteration 21/1000 | Loss: 0.00002781
Iteration 22/1000 | Loss: 0.00011223
Iteration 23/1000 | Loss: 0.00004723
Iteration 24/1000 | Loss: 0.00006303
Iteration 25/1000 | Loss: 0.00002277
Iteration 26/1000 | Loss: 0.00001881
Iteration 27/1000 | Loss: 0.00011572
Iteration 28/1000 | Loss: 0.00002366
Iteration 29/1000 | Loss: 0.00003237
Iteration 30/1000 | Loss: 0.00001776
Iteration 31/1000 | Loss: 0.00011103
Iteration 32/1000 | Loss: 0.00035411
Iteration 33/1000 | Loss: 0.00004579
Iteration 34/1000 | Loss: 0.00003923
Iteration 35/1000 | Loss: 0.00003492
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001699
Iteration 38/1000 | Loss: 0.00003612
Iteration 39/1000 | Loss: 0.00004831
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00004001
Iteration 42/1000 | Loss: 0.00086031
Iteration 43/1000 | Loss: 0.00001771
Iteration 44/1000 | Loss: 0.00001614
Iteration 45/1000 | Loss: 0.00001582
Iteration 46/1000 | Loss: 0.00003956
Iteration 47/1000 | Loss: 0.00004210
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001549
Iteration 50/1000 | Loss: 0.00001534
Iteration 51/1000 | Loss: 0.00001531
Iteration 52/1000 | Loss: 0.00001530
Iteration 53/1000 | Loss: 0.00003550
Iteration 54/1000 | Loss: 0.00001800
Iteration 55/1000 | Loss: 0.00002120
Iteration 56/1000 | Loss: 0.00001492
Iteration 57/1000 | Loss: 0.00002183
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001477
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001465
Iteration 62/1000 | Loss: 0.00001463
Iteration 63/1000 | Loss: 0.00001458
Iteration 64/1000 | Loss: 0.00001457
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001454
Iteration 67/1000 | Loss: 0.00001454
Iteration 68/1000 | Loss: 0.00001451
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001442
Iteration 72/1000 | Loss: 0.00001442
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001441
Iteration 76/1000 | Loss: 0.00003736
Iteration 77/1000 | Loss: 0.00003331
Iteration 78/1000 | Loss: 0.00001674
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001484
Iteration 81/1000 | Loss: 0.00001433
Iteration 82/1000 | Loss: 0.00001432
Iteration 83/1000 | Loss: 0.00002550
Iteration 84/1000 | Loss: 0.00001437
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001435
Iteration 88/1000 | Loss: 0.00001432
Iteration 89/1000 | Loss: 0.00001432
Iteration 90/1000 | Loss: 0.00001431
Iteration 91/1000 | Loss: 0.00001431
Iteration 92/1000 | Loss: 0.00001431
Iteration 93/1000 | Loss: 0.00001431
Iteration 94/1000 | Loss: 0.00001431
Iteration 95/1000 | Loss: 0.00001431
Iteration 96/1000 | Loss: 0.00001431
Iteration 97/1000 | Loss: 0.00001431
Iteration 98/1000 | Loss: 0.00001431
Iteration 99/1000 | Loss: 0.00001430
Iteration 100/1000 | Loss: 0.00001430
Iteration 101/1000 | Loss: 0.00002144
Iteration 102/1000 | Loss: 0.00001429
Iteration 103/1000 | Loss: 0.00001428
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001427
Iteration 106/1000 | Loss: 0.00001427
Iteration 107/1000 | Loss: 0.00001427
Iteration 108/1000 | Loss: 0.00001427
Iteration 109/1000 | Loss: 0.00001427
Iteration 110/1000 | Loss: 0.00001427
Iteration 111/1000 | Loss: 0.00001427
Iteration 112/1000 | Loss: 0.00001427
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001426
Iteration 117/1000 | Loss: 0.00001426
Iteration 118/1000 | Loss: 0.00001426
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00002142
Iteration 121/1000 | Loss: 0.00002878
Iteration 122/1000 | Loss: 0.00001426
Iteration 123/1000 | Loss: 0.00001426
Iteration 124/1000 | Loss: 0.00001426
Iteration 125/1000 | Loss: 0.00001426
Iteration 126/1000 | Loss: 0.00001426
Iteration 127/1000 | Loss: 0.00001426
Iteration 128/1000 | Loss: 0.00001426
Iteration 129/1000 | Loss: 0.00001425
Iteration 130/1000 | Loss: 0.00001425
Iteration 131/1000 | Loss: 0.00001425
Iteration 132/1000 | Loss: 0.00001423
Iteration 133/1000 | Loss: 0.00001423
Iteration 134/1000 | Loss: 0.00001423
Iteration 135/1000 | Loss: 0.00001423
Iteration 136/1000 | Loss: 0.00001422
Iteration 137/1000 | Loss: 0.00001422
Iteration 138/1000 | Loss: 0.00001422
Iteration 139/1000 | Loss: 0.00001422
Iteration 140/1000 | Loss: 0.00001422
Iteration 141/1000 | Loss: 0.00001421
Iteration 142/1000 | Loss: 0.00001421
Iteration 143/1000 | Loss: 0.00001421
Iteration 144/1000 | Loss: 0.00001420
Iteration 145/1000 | Loss: 0.00001420
Iteration 146/1000 | Loss: 0.00001420
Iteration 147/1000 | Loss: 0.00001420
Iteration 148/1000 | Loss: 0.00001420
Iteration 149/1000 | Loss: 0.00001420
Iteration 150/1000 | Loss: 0.00001420
Iteration 151/1000 | Loss: 0.00001420
Iteration 152/1000 | Loss: 0.00001420
Iteration 153/1000 | Loss: 0.00001420
Iteration 154/1000 | Loss: 0.00001419
Iteration 155/1000 | Loss: 0.00001419
Iteration 156/1000 | Loss: 0.00001419
Iteration 157/1000 | Loss: 0.00001419
Iteration 158/1000 | Loss: 0.00001419
Iteration 159/1000 | Loss: 0.00001419
Iteration 160/1000 | Loss: 0.00001419
Iteration 161/1000 | Loss: 0.00001418
Iteration 162/1000 | Loss: 0.00001418
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001417
Iteration 165/1000 | Loss: 0.00001417
Iteration 166/1000 | Loss: 0.00001417
Iteration 167/1000 | Loss: 0.00001416
Iteration 168/1000 | Loss: 0.00001416
Iteration 169/1000 | Loss: 0.00001416
Iteration 170/1000 | Loss: 0.00001416
Iteration 171/1000 | Loss: 0.00001416
Iteration 172/1000 | Loss: 0.00001416
Iteration 173/1000 | Loss: 0.00001416
Iteration 174/1000 | Loss: 0.00001416
Iteration 175/1000 | Loss: 0.00001416
Iteration 176/1000 | Loss: 0.00001416
Iteration 177/1000 | Loss: 0.00001416
Iteration 178/1000 | Loss: 0.00001415
Iteration 179/1000 | Loss: 0.00001415
Iteration 180/1000 | Loss: 0.00001415
Iteration 181/1000 | Loss: 0.00001415
Iteration 182/1000 | Loss: 0.00001415
Iteration 183/1000 | Loss: 0.00001415
Iteration 184/1000 | Loss: 0.00001415
Iteration 185/1000 | Loss: 0.00001415
Iteration 186/1000 | Loss: 0.00001415
Iteration 187/1000 | Loss: 0.00001415
Iteration 188/1000 | Loss: 0.00001415
Iteration 189/1000 | Loss: 0.00001415
Iteration 190/1000 | Loss: 0.00001415
Iteration 191/1000 | Loss: 0.00001415
Iteration 192/1000 | Loss: 0.00001414
Iteration 193/1000 | Loss: 0.00001414
Iteration 194/1000 | Loss: 0.00001414
Iteration 195/1000 | Loss: 0.00001414
Iteration 196/1000 | Loss: 0.00001414
Iteration 197/1000 | Loss: 0.00001414
Iteration 198/1000 | Loss: 0.00001414
Iteration 199/1000 | Loss: 0.00001414
Iteration 200/1000 | Loss: 0.00001414
Iteration 201/1000 | Loss: 0.00001414
Iteration 202/1000 | Loss: 0.00001414
Iteration 203/1000 | Loss: 0.00001414
Iteration 204/1000 | Loss: 0.00001414
Iteration 205/1000 | Loss: 0.00001414
Iteration 206/1000 | Loss: 0.00001414
Iteration 207/1000 | Loss: 0.00001414
Iteration 208/1000 | Loss: 0.00001414
Iteration 209/1000 | Loss: 0.00001414
Iteration 210/1000 | Loss: 0.00001414
Iteration 211/1000 | Loss: 0.00001414
Iteration 212/1000 | Loss: 0.00001414
Iteration 213/1000 | Loss: 0.00001414
Iteration 214/1000 | Loss: 0.00001414
Iteration 215/1000 | Loss: 0.00001413
Iteration 216/1000 | Loss: 0.00001413
Iteration 217/1000 | Loss: 0.00001413
Iteration 218/1000 | Loss: 0.00001413
Iteration 219/1000 | Loss: 0.00001413
Iteration 220/1000 | Loss: 0.00001413
Iteration 221/1000 | Loss: 0.00001413
Iteration 222/1000 | Loss: 0.00001413
Iteration 223/1000 | Loss: 0.00001413
Iteration 224/1000 | Loss: 0.00001413
Iteration 225/1000 | Loss: 0.00001413
Iteration 226/1000 | Loss: 0.00001413
Iteration 227/1000 | Loss: 0.00001413
Iteration 228/1000 | Loss: 0.00001413
Iteration 229/1000 | Loss: 0.00001413
Iteration 230/1000 | Loss: 0.00001413
Iteration 231/1000 | Loss: 0.00001413
Iteration 232/1000 | Loss: 0.00001413
Iteration 233/1000 | Loss: 0.00001413
Iteration 234/1000 | Loss: 0.00001412
Iteration 235/1000 | Loss: 0.00001412
Iteration 236/1000 | Loss: 0.00001412
Iteration 237/1000 | Loss: 0.00001412
Iteration 238/1000 | Loss: 0.00001412
Iteration 239/1000 | Loss: 0.00001412
Iteration 240/1000 | Loss: 0.00001412
Iteration 241/1000 | Loss: 0.00001412
Iteration 242/1000 | Loss: 0.00001412
Iteration 243/1000 | Loss: 0.00001412
Iteration 244/1000 | Loss: 0.00001412
Iteration 245/1000 | Loss: 0.00001412
Iteration 246/1000 | Loss: 0.00001412
Iteration 247/1000 | Loss: 0.00001412
Iteration 248/1000 | Loss: 0.00001412
Iteration 249/1000 | Loss: 0.00001412
Iteration 250/1000 | Loss: 0.00001412
Iteration 251/1000 | Loss: 0.00001412
Iteration 252/1000 | Loss: 0.00001412
Iteration 253/1000 | Loss: 0.00001412
Iteration 254/1000 | Loss: 0.00001412
Iteration 255/1000 | Loss: 0.00001412
Iteration 256/1000 | Loss: 0.00001412
Iteration 257/1000 | Loss: 0.00001411
Iteration 258/1000 | Loss: 0.00001411
Iteration 259/1000 | Loss: 0.00001411
Iteration 260/1000 | Loss: 0.00001411
Iteration 261/1000 | Loss: 0.00001411
Iteration 262/1000 | Loss: 0.00001411
Iteration 263/1000 | Loss: 0.00001411
Iteration 264/1000 | Loss: 0.00001411
Iteration 265/1000 | Loss: 0.00001411
Iteration 266/1000 | Loss: 0.00001411
Iteration 267/1000 | Loss: 0.00001411
Iteration 268/1000 | Loss: 0.00001411
Iteration 269/1000 | Loss: 0.00001411
Iteration 270/1000 | Loss: 0.00001411
Iteration 271/1000 | Loss: 0.00001411
Iteration 272/1000 | Loss: 0.00001410
Iteration 273/1000 | Loss: 0.00001410
Iteration 274/1000 | Loss: 0.00001410
Iteration 275/1000 | Loss: 0.00001410
Iteration 276/1000 | Loss: 0.00001410
Iteration 277/1000 | Loss: 0.00001410
Iteration 278/1000 | Loss: 0.00001410
Iteration 279/1000 | Loss: 0.00001410
Iteration 280/1000 | Loss: 0.00001410
Iteration 281/1000 | Loss: 0.00001410
Iteration 282/1000 | Loss: 0.00001410
Iteration 283/1000 | Loss: 0.00001410
Iteration 284/1000 | Loss: 0.00001410
Iteration 285/1000 | Loss: 0.00001410
Iteration 286/1000 | Loss: 0.00001410
Iteration 287/1000 | Loss: 0.00001410
Iteration 288/1000 | Loss: 0.00001410
Iteration 289/1000 | Loss: 0.00001410
Iteration 290/1000 | Loss: 0.00001410
Iteration 291/1000 | Loss: 0.00001410
Iteration 292/1000 | Loss: 0.00001409
Iteration 293/1000 | Loss: 0.00001409
Iteration 294/1000 | Loss: 0.00001409
Iteration 295/1000 | Loss: 0.00001409
Iteration 296/1000 | Loss: 0.00001409
Iteration 297/1000 | Loss: 0.00001409
Iteration 298/1000 | Loss: 0.00001409
Iteration 299/1000 | Loss: 0.00001409
Iteration 300/1000 | Loss: 0.00001409
Iteration 301/1000 | Loss: 0.00001409
Iteration 302/1000 | Loss: 0.00001409
Iteration 303/1000 | Loss: 0.00001409
Iteration 304/1000 | Loss: 0.00001409
Iteration 305/1000 | Loss: 0.00001409
Iteration 306/1000 | Loss: 0.00001409
Iteration 307/1000 | Loss: 0.00001409
Iteration 308/1000 | Loss: 0.00001409
Iteration 309/1000 | Loss: 0.00001409
Iteration 310/1000 | Loss: 0.00001409
Iteration 311/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [1.4093422578298487e-05, 1.4093422578298487e-05, 1.4093422578298487e-05, 1.4093422578298487e-05, 1.4093422578298487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4093422578298487e-05

Optimization complete. Final v2v error: 3.1826415061950684 mm

Highest mean error: 3.583960771560669 mm for frame 145

Lowest mean error: 2.888571262359619 mm for frame 231

Saving results

Total time: 125.97517013549805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013494
Iteration 2/25 | Loss: 0.00292809
Iteration 3/25 | Loss: 0.00259493
Iteration 4/25 | Loss: 0.00242819
Iteration 5/25 | Loss: 0.00229165
Iteration 6/25 | Loss: 0.00215121
Iteration 7/25 | Loss: 0.00205044
Iteration 8/25 | Loss: 0.00195982
Iteration 9/25 | Loss: 0.00187758
Iteration 10/25 | Loss: 0.00184512
Iteration 11/25 | Loss: 0.00182267
Iteration 12/25 | Loss: 0.00179992
Iteration 13/25 | Loss: 0.00173177
Iteration 14/25 | Loss: 0.00169988
Iteration 15/25 | Loss: 0.00169347
Iteration 16/25 | Loss: 0.00165926
Iteration 17/25 | Loss: 0.00164661
Iteration 18/25 | Loss: 0.00164652
Iteration 19/25 | Loss: 0.00164482
Iteration 20/25 | Loss: 0.00162992
Iteration 21/25 | Loss: 0.00162302
Iteration 22/25 | Loss: 0.00162605
Iteration 23/25 | Loss: 0.00162836
Iteration 24/25 | Loss: 0.00161384
Iteration 25/25 | Loss: 0.00161543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36974072
Iteration 2/25 | Loss: 0.00604537
Iteration 3/25 | Loss: 0.00604534
Iteration 4/25 | Loss: 0.00604534
Iteration 5/25 | Loss: 0.00604534
Iteration 6/25 | Loss: 0.00604534
Iteration 7/25 | Loss: 0.00604534
Iteration 8/25 | Loss: 0.00604534
Iteration 9/25 | Loss: 0.00604534
Iteration 10/25 | Loss: 0.00604534
Iteration 11/25 | Loss: 0.00604534
Iteration 12/25 | Loss: 0.00604534
Iteration 13/25 | Loss: 0.00604534
Iteration 14/25 | Loss: 0.00604534
Iteration 15/25 | Loss: 0.00604534
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.006045338697731495, 0.006045338697731495, 0.006045338697731495, 0.006045338697731495, 0.006045338697731495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006045338697731495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00604534
Iteration 2/1000 | Loss: 0.00425909
Iteration 3/1000 | Loss: 0.00052707
Iteration 4/1000 | Loss: 0.00027093
Iteration 5/1000 | Loss: 0.00018794
Iteration 6/1000 | Loss: 0.00013323
Iteration 7/1000 | Loss: 0.00010680
Iteration 8/1000 | Loss: 0.00009428
Iteration 9/1000 | Loss: 0.00008349
Iteration 10/1000 | Loss: 0.00123529
Iteration 11/1000 | Loss: 0.00010843
Iteration 12/1000 | Loss: 0.00007869
Iteration 13/1000 | Loss: 0.00006352
Iteration 14/1000 | Loss: 0.00005701
Iteration 15/1000 | Loss: 0.00005357
Iteration 16/1000 | Loss: 0.00005031
Iteration 17/1000 | Loss: 0.00004775
Iteration 18/1000 | Loss: 0.00004567
Iteration 19/1000 | Loss: 0.00004439
Iteration 20/1000 | Loss: 0.00004308
Iteration 21/1000 | Loss: 0.00004216
Iteration 22/1000 | Loss: 0.00004124
Iteration 23/1000 | Loss: 0.00004059
Iteration 24/1000 | Loss: 0.00003992
Iteration 25/1000 | Loss: 0.00003944
Iteration 26/1000 | Loss: 0.00003891
Iteration 27/1000 | Loss: 0.00003850
Iteration 28/1000 | Loss: 0.00041552
Iteration 29/1000 | Loss: 0.00127909
Iteration 30/1000 | Loss: 0.00089698
Iteration 31/1000 | Loss: 0.00008495
Iteration 32/1000 | Loss: 0.00004683
Iteration 33/1000 | Loss: 0.00004137
Iteration 34/1000 | Loss: 0.00084594
Iteration 35/1000 | Loss: 0.00045556
Iteration 36/1000 | Loss: 0.00128713
Iteration 37/1000 | Loss: 0.00102792
Iteration 38/1000 | Loss: 0.00042809
Iteration 39/1000 | Loss: 0.00009199
Iteration 40/1000 | Loss: 0.00029274
Iteration 41/1000 | Loss: 0.00020760
Iteration 42/1000 | Loss: 0.00007057
Iteration 43/1000 | Loss: 0.00030216
Iteration 44/1000 | Loss: 0.00023085
Iteration 45/1000 | Loss: 0.00033196
Iteration 46/1000 | Loss: 0.00021516
Iteration 47/1000 | Loss: 0.00034865
Iteration 48/1000 | Loss: 0.00018052
Iteration 49/1000 | Loss: 0.00007317
Iteration 50/1000 | Loss: 0.00006534
Iteration 51/1000 | Loss: 0.00005309
Iteration 52/1000 | Loss: 0.00004440
Iteration 53/1000 | Loss: 0.00004180
Iteration 54/1000 | Loss: 0.00003977
Iteration 55/1000 | Loss: 0.00003763
Iteration 56/1000 | Loss: 0.00003576
Iteration 57/1000 | Loss: 0.00032602
Iteration 58/1000 | Loss: 0.00035242
Iteration 59/1000 | Loss: 0.00005091
Iteration 60/1000 | Loss: 0.00004176
Iteration 61/1000 | Loss: 0.00003707
Iteration 62/1000 | Loss: 0.00003411
Iteration 63/1000 | Loss: 0.00003246
Iteration 64/1000 | Loss: 0.00003148
Iteration 65/1000 | Loss: 0.00003065
Iteration 66/1000 | Loss: 0.00002985
Iteration 67/1000 | Loss: 0.00002900
Iteration 68/1000 | Loss: 0.00002811
Iteration 69/1000 | Loss: 0.00002737
Iteration 70/1000 | Loss: 0.00002669
Iteration 71/1000 | Loss: 0.00002616
Iteration 72/1000 | Loss: 0.00002553
Iteration 73/1000 | Loss: 0.00002519
Iteration 74/1000 | Loss: 0.00002491
Iteration 75/1000 | Loss: 0.00002488
Iteration 76/1000 | Loss: 0.00002472
Iteration 77/1000 | Loss: 0.00002460
Iteration 78/1000 | Loss: 0.00002460
Iteration 79/1000 | Loss: 0.00002459
Iteration 80/1000 | Loss: 0.00002459
Iteration 81/1000 | Loss: 0.00002459
Iteration 82/1000 | Loss: 0.00002459
Iteration 83/1000 | Loss: 0.00002458
Iteration 84/1000 | Loss: 0.00002457
Iteration 85/1000 | Loss: 0.00002457
Iteration 86/1000 | Loss: 0.00002457
Iteration 87/1000 | Loss: 0.00002457
Iteration 88/1000 | Loss: 0.00002457
Iteration 89/1000 | Loss: 0.00002456
Iteration 90/1000 | Loss: 0.00002456
Iteration 91/1000 | Loss: 0.00002456
Iteration 92/1000 | Loss: 0.00002456
Iteration 93/1000 | Loss: 0.00002456
Iteration 94/1000 | Loss: 0.00002455
Iteration 95/1000 | Loss: 0.00002455
Iteration 96/1000 | Loss: 0.00002455
Iteration 97/1000 | Loss: 0.00002454
Iteration 98/1000 | Loss: 0.00002454
Iteration 99/1000 | Loss: 0.00002454
Iteration 100/1000 | Loss: 0.00002454
Iteration 101/1000 | Loss: 0.00002454
Iteration 102/1000 | Loss: 0.00002454
Iteration 103/1000 | Loss: 0.00002454
Iteration 104/1000 | Loss: 0.00002454
Iteration 105/1000 | Loss: 0.00002453
Iteration 106/1000 | Loss: 0.00002453
Iteration 107/1000 | Loss: 0.00002453
Iteration 108/1000 | Loss: 0.00002453
Iteration 109/1000 | Loss: 0.00002452
Iteration 110/1000 | Loss: 0.00002452
Iteration 111/1000 | Loss: 0.00002451
Iteration 112/1000 | Loss: 0.00002451
Iteration 113/1000 | Loss: 0.00002451
Iteration 114/1000 | Loss: 0.00002451
Iteration 115/1000 | Loss: 0.00002451
Iteration 116/1000 | Loss: 0.00002450
Iteration 117/1000 | Loss: 0.00002450
Iteration 118/1000 | Loss: 0.00002450
Iteration 119/1000 | Loss: 0.00002450
Iteration 120/1000 | Loss: 0.00002450
Iteration 121/1000 | Loss: 0.00002450
Iteration 122/1000 | Loss: 0.00002450
Iteration 123/1000 | Loss: 0.00002450
Iteration 124/1000 | Loss: 0.00002450
Iteration 125/1000 | Loss: 0.00002449
Iteration 126/1000 | Loss: 0.00002449
Iteration 127/1000 | Loss: 0.00002449
Iteration 128/1000 | Loss: 0.00002449
Iteration 129/1000 | Loss: 0.00002449
Iteration 130/1000 | Loss: 0.00002449
Iteration 131/1000 | Loss: 0.00002449
Iteration 132/1000 | Loss: 0.00002449
Iteration 133/1000 | Loss: 0.00002449
Iteration 134/1000 | Loss: 0.00002449
Iteration 135/1000 | Loss: 0.00002449
Iteration 136/1000 | Loss: 0.00002449
Iteration 137/1000 | Loss: 0.00002449
Iteration 138/1000 | Loss: 0.00002449
Iteration 139/1000 | Loss: 0.00002449
Iteration 140/1000 | Loss: 0.00002449
Iteration 141/1000 | Loss: 0.00002449
Iteration 142/1000 | Loss: 0.00002448
Iteration 143/1000 | Loss: 0.00002448
Iteration 144/1000 | Loss: 0.00002448
Iteration 145/1000 | Loss: 0.00002448
Iteration 146/1000 | Loss: 0.00002448
Iteration 147/1000 | Loss: 0.00002448
Iteration 148/1000 | Loss: 0.00002448
Iteration 149/1000 | Loss: 0.00002448
Iteration 150/1000 | Loss: 0.00002448
Iteration 151/1000 | Loss: 0.00002448
Iteration 152/1000 | Loss: 0.00002448
Iteration 153/1000 | Loss: 0.00002448
Iteration 154/1000 | Loss: 0.00002447
Iteration 155/1000 | Loss: 0.00002447
Iteration 156/1000 | Loss: 0.00002447
Iteration 157/1000 | Loss: 0.00002447
Iteration 158/1000 | Loss: 0.00002447
Iteration 159/1000 | Loss: 0.00002447
Iteration 160/1000 | Loss: 0.00002447
Iteration 161/1000 | Loss: 0.00002447
Iteration 162/1000 | Loss: 0.00002447
Iteration 163/1000 | Loss: 0.00002446
Iteration 164/1000 | Loss: 0.00002446
Iteration 165/1000 | Loss: 0.00002446
Iteration 166/1000 | Loss: 0.00002446
Iteration 167/1000 | Loss: 0.00002446
Iteration 168/1000 | Loss: 0.00002446
Iteration 169/1000 | Loss: 0.00002446
Iteration 170/1000 | Loss: 0.00002446
Iteration 171/1000 | Loss: 0.00002446
Iteration 172/1000 | Loss: 0.00002445
Iteration 173/1000 | Loss: 0.00002445
Iteration 174/1000 | Loss: 0.00002445
Iteration 175/1000 | Loss: 0.00002445
Iteration 176/1000 | Loss: 0.00002445
Iteration 177/1000 | Loss: 0.00002444
Iteration 178/1000 | Loss: 0.00002444
Iteration 179/1000 | Loss: 0.00002444
Iteration 180/1000 | Loss: 0.00002444
Iteration 181/1000 | Loss: 0.00002444
Iteration 182/1000 | Loss: 0.00002444
Iteration 183/1000 | Loss: 0.00002444
Iteration 184/1000 | Loss: 0.00002444
Iteration 185/1000 | Loss: 0.00002444
Iteration 186/1000 | Loss: 0.00002444
Iteration 187/1000 | Loss: 0.00002444
Iteration 188/1000 | Loss: 0.00002444
Iteration 189/1000 | Loss: 0.00002444
Iteration 190/1000 | Loss: 0.00002444
Iteration 191/1000 | Loss: 0.00002444
Iteration 192/1000 | Loss: 0.00002444
Iteration 193/1000 | Loss: 0.00002444
Iteration 194/1000 | Loss: 0.00002444
Iteration 195/1000 | Loss: 0.00002444
Iteration 196/1000 | Loss: 0.00002444
Iteration 197/1000 | Loss: 0.00002444
Iteration 198/1000 | Loss: 0.00002444
Iteration 199/1000 | Loss: 0.00002444
Iteration 200/1000 | Loss: 0.00002444
Iteration 201/1000 | Loss: 0.00002444
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.4436943931505084e-05, 2.4436943931505084e-05, 2.4436943931505084e-05, 2.4436943931505084e-05, 2.4436943931505084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4436943931505084e-05

Optimization complete. Final v2v error: 3.6381468772888184 mm

Highest mean error: 11.418152809143066 mm for frame 114

Lowest mean error: 3.160696268081665 mm for frame 52

Saving results

Total time: 160.49858808517456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812964
Iteration 2/25 | Loss: 0.00150324
Iteration 3/25 | Loss: 0.00136587
Iteration 4/25 | Loss: 0.00134235
Iteration 5/25 | Loss: 0.00133923
Iteration 6/25 | Loss: 0.00133898
Iteration 7/25 | Loss: 0.00133898
Iteration 8/25 | Loss: 0.00133898
Iteration 9/25 | Loss: 0.00133898
Iteration 10/25 | Loss: 0.00133898
Iteration 11/25 | Loss: 0.00133898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013389780651777983, 0.0013389780651777983, 0.0013389780651777983, 0.0013389780651777983, 0.0013389780651777983]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013389780651777983

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96168607
Iteration 2/25 | Loss: 0.00065149
Iteration 3/25 | Loss: 0.00065149
Iteration 4/25 | Loss: 0.00065149
Iteration 5/25 | Loss: 0.00065149
Iteration 6/25 | Loss: 0.00065149
Iteration 7/25 | Loss: 0.00065149
Iteration 8/25 | Loss: 0.00065149
Iteration 9/25 | Loss: 0.00065149
Iteration 10/25 | Loss: 0.00065149
Iteration 11/25 | Loss: 0.00065149
Iteration 12/25 | Loss: 0.00065149
Iteration 13/25 | Loss: 0.00065149
Iteration 14/25 | Loss: 0.00065149
Iteration 15/25 | Loss: 0.00065149
Iteration 16/25 | Loss: 0.00065149
Iteration 17/25 | Loss: 0.00065149
Iteration 18/25 | Loss: 0.00065149
Iteration 19/25 | Loss: 0.00065149
Iteration 20/25 | Loss: 0.00065149
Iteration 21/25 | Loss: 0.00065149
Iteration 22/25 | Loss: 0.00065149
Iteration 23/25 | Loss: 0.00065149
Iteration 24/25 | Loss: 0.00065149
Iteration 25/25 | Loss: 0.00065149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065149
Iteration 2/1000 | Loss: 0.00004612
Iteration 3/1000 | Loss: 0.00003591
Iteration 4/1000 | Loss: 0.00003272
Iteration 5/1000 | Loss: 0.00003147
Iteration 6/1000 | Loss: 0.00003077
Iteration 7/1000 | Loss: 0.00003017
Iteration 8/1000 | Loss: 0.00002979
Iteration 9/1000 | Loss: 0.00002943
Iteration 10/1000 | Loss: 0.00002906
Iteration 11/1000 | Loss: 0.00002880
Iteration 12/1000 | Loss: 0.00002867
Iteration 13/1000 | Loss: 0.00002864
Iteration 14/1000 | Loss: 0.00002847
Iteration 15/1000 | Loss: 0.00002845
Iteration 16/1000 | Loss: 0.00002832
Iteration 17/1000 | Loss: 0.00002831
Iteration 18/1000 | Loss: 0.00002830
Iteration 19/1000 | Loss: 0.00002830
Iteration 20/1000 | Loss: 0.00002828
Iteration 21/1000 | Loss: 0.00002827
Iteration 22/1000 | Loss: 0.00002827
Iteration 23/1000 | Loss: 0.00002826
Iteration 24/1000 | Loss: 0.00002825
Iteration 25/1000 | Loss: 0.00002825
Iteration 26/1000 | Loss: 0.00002825
Iteration 27/1000 | Loss: 0.00002825
Iteration 28/1000 | Loss: 0.00002825
Iteration 29/1000 | Loss: 0.00002825
Iteration 30/1000 | Loss: 0.00002824
Iteration 31/1000 | Loss: 0.00002824
Iteration 32/1000 | Loss: 0.00002824
Iteration 33/1000 | Loss: 0.00002823
Iteration 34/1000 | Loss: 0.00002823
Iteration 35/1000 | Loss: 0.00002820
Iteration 36/1000 | Loss: 0.00002818
Iteration 37/1000 | Loss: 0.00002817
Iteration 38/1000 | Loss: 0.00002817
Iteration 39/1000 | Loss: 0.00002817
Iteration 40/1000 | Loss: 0.00002816
Iteration 41/1000 | Loss: 0.00002816
Iteration 42/1000 | Loss: 0.00002816
Iteration 43/1000 | Loss: 0.00002816
Iteration 44/1000 | Loss: 0.00002816
Iteration 45/1000 | Loss: 0.00002816
Iteration 46/1000 | Loss: 0.00002816
Iteration 47/1000 | Loss: 0.00002816
Iteration 48/1000 | Loss: 0.00002816
Iteration 49/1000 | Loss: 0.00002815
Iteration 50/1000 | Loss: 0.00002815
Iteration 51/1000 | Loss: 0.00002815
Iteration 52/1000 | Loss: 0.00002815
Iteration 53/1000 | Loss: 0.00002815
Iteration 54/1000 | Loss: 0.00002815
Iteration 55/1000 | Loss: 0.00002814
Iteration 56/1000 | Loss: 0.00002814
Iteration 57/1000 | Loss: 0.00002814
Iteration 58/1000 | Loss: 0.00002814
Iteration 59/1000 | Loss: 0.00002814
Iteration 60/1000 | Loss: 0.00002814
Iteration 61/1000 | Loss: 0.00002814
Iteration 62/1000 | Loss: 0.00002814
Iteration 63/1000 | Loss: 0.00002814
Iteration 64/1000 | Loss: 0.00002813
Iteration 65/1000 | Loss: 0.00002813
Iteration 66/1000 | Loss: 0.00002813
Iteration 67/1000 | Loss: 0.00002813
Iteration 68/1000 | Loss: 0.00002813
Iteration 69/1000 | Loss: 0.00002813
Iteration 70/1000 | Loss: 0.00002813
Iteration 71/1000 | Loss: 0.00002813
Iteration 72/1000 | Loss: 0.00002813
Iteration 73/1000 | Loss: 0.00002813
Iteration 74/1000 | Loss: 0.00002813
Iteration 75/1000 | Loss: 0.00002813
Iteration 76/1000 | Loss: 0.00002813
Iteration 77/1000 | Loss: 0.00002813
Iteration 78/1000 | Loss: 0.00002813
Iteration 79/1000 | Loss: 0.00002813
Iteration 80/1000 | Loss: 0.00002812
Iteration 81/1000 | Loss: 0.00002812
Iteration 82/1000 | Loss: 0.00002812
Iteration 83/1000 | Loss: 0.00002812
Iteration 84/1000 | Loss: 0.00002812
Iteration 85/1000 | Loss: 0.00002812
Iteration 86/1000 | Loss: 0.00002812
Iteration 87/1000 | Loss: 0.00002812
Iteration 88/1000 | Loss: 0.00002812
Iteration 89/1000 | Loss: 0.00002812
Iteration 90/1000 | Loss: 0.00002812
Iteration 91/1000 | Loss: 0.00002812
Iteration 92/1000 | Loss: 0.00002812
Iteration 93/1000 | Loss: 0.00002811
Iteration 94/1000 | Loss: 0.00002811
Iteration 95/1000 | Loss: 0.00002811
Iteration 96/1000 | Loss: 0.00002811
Iteration 97/1000 | Loss: 0.00002811
Iteration 98/1000 | Loss: 0.00002811
Iteration 99/1000 | Loss: 0.00002810
Iteration 100/1000 | Loss: 0.00002810
Iteration 101/1000 | Loss: 0.00002810
Iteration 102/1000 | Loss: 0.00002810
Iteration 103/1000 | Loss: 0.00002810
Iteration 104/1000 | Loss: 0.00002810
Iteration 105/1000 | Loss: 0.00002810
Iteration 106/1000 | Loss: 0.00002809
Iteration 107/1000 | Loss: 0.00002809
Iteration 108/1000 | Loss: 0.00002809
Iteration 109/1000 | Loss: 0.00002809
Iteration 110/1000 | Loss: 0.00002809
Iteration 111/1000 | Loss: 0.00002809
Iteration 112/1000 | Loss: 0.00002808
Iteration 113/1000 | Loss: 0.00002808
Iteration 114/1000 | Loss: 0.00002808
Iteration 115/1000 | Loss: 0.00002808
Iteration 116/1000 | Loss: 0.00002808
Iteration 117/1000 | Loss: 0.00002808
Iteration 118/1000 | Loss: 0.00002808
Iteration 119/1000 | Loss: 0.00002808
Iteration 120/1000 | Loss: 0.00002808
Iteration 121/1000 | Loss: 0.00002808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.807578130159527e-05, 2.807578130159527e-05, 2.807578130159527e-05, 2.807578130159527e-05, 2.807578130159527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.807578130159527e-05

Optimization complete. Final v2v error: 4.524128437042236 mm

Highest mean error: 4.798386096954346 mm for frame 29

Lowest mean error: 4.393975257873535 mm for frame 14

Saving results

Total time: 34.51300311088562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804025
Iteration 2/25 | Loss: 0.00131801
Iteration 3/25 | Loss: 0.00123868
Iteration 4/25 | Loss: 0.00123230
Iteration 5/25 | Loss: 0.00123081
Iteration 6/25 | Loss: 0.00123081
Iteration 7/25 | Loss: 0.00123081
Iteration 8/25 | Loss: 0.00123081
Iteration 9/25 | Loss: 0.00123081
Iteration 10/25 | Loss: 0.00123081
Iteration 11/25 | Loss: 0.00123081
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012308062287047505, 0.0012308062287047505, 0.0012308062287047505, 0.0012308062287047505, 0.0012308062287047505]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012308062287047505

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43037784
Iteration 2/25 | Loss: 0.00074994
Iteration 3/25 | Loss: 0.00074994
Iteration 4/25 | Loss: 0.00074994
Iteration 5/25 | Loss: 0.00074994
Iteration 6/25 | Loss: 0.00074993
Iteration 7/25 | Loss: 0.00074993
Iteration 8/25 | Loss: 0.00074993
Iteration 9/25 | Loss: 0.00074993
Iteration 10/25 | Loss: 0.00074993
Iteration 11/25 | Loss: 0.00074993
Iteration 12/25 | Loss: 0.00074993
Iteration 13/25 | Loss: 0.00074993
Iteration 14/25 | Loss: 0.00074993
Iteration 15/25 | Loss: 0.00074993
Iteration 16/25 | Loss: 0.00074993
Iteration 17/25 | Loss: 0.00074993
Iteration 18/25 | Loss: 0.00074993
Iteration 19/25 | Loss: 0.00074993
Iteration 20/25 | Loss: 0.00074993
Iteration 21/25 | Loss: 0.00074993
Iteration 22/25 | Loss: 0.00074993
Iteration 23/25 | Loss: 0.00074993
Iteration 24/25 | Loss: 0.00074993
Iteration 25/25 | Loss: 0.00074993

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074993
Iteration 2/1000 | Loss: 0.00002575
Iteration 3/1000 | Loss: 0.00001711
Iteration 4/1000 | Loss: 0.00001550
Iteration 5/1000 | Loss: 0.00001448
Iteration 6/1000 | Loss: 0.00001370
Iteration 7/1000 | Loss: 0.00001324
Iteration 8/1000 | Loss: 0.00001303
Iteration 9/1000 | Loss: 0.00001303
Iteration 10/1000 | Loss: 0.00001302
Iteration 11/1000 | Loss: 0.00001292
Iteration 12/1000 | Loss: 0.00001272
Iteration 13/1000 | Loss: 0.00001260
Iteration 14/1000 | Loss: 0.00001258
Iteration 15/1000 | Loss: 0.00001255
Iteration 16/1000 | Loss: 0.00001253
Iteration 17/1000 | Loss: 0.00001248
Iteration 18/1000 | Loss: 0.00001247
Iteration 19/1000 | Loss: 0.00001247
Iteration 20/1000 | Loss: 0.00001247
Iteration 21/1000 | Loss: 0.00001245
Iteration 22/1000 | Loss: 0.00001243
Iteration 23/1000 | Loss: 0.00001242
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001241
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001235
Iteration 28/1000 | Loss: 0.00001235
Iteration 29/1000 | Loss: 0.00001234
Iteration 30/1000 | Loss: 0.00001234
Iteration 31/1000 | Loss: 0.00001234
Iteration 32/1000 | Loss: 0.00001233
Iteration 33/1000 | Loss: 0.00001231
Iteration 34/1000 | Loss: 0.00001230
Iteration 35/1000 | Loss: 0.00001230
Iteration 36/1000 | Loss: 0.00001229
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001227
Iteration 41/1000 | Loss: 0.00001226
Iteration 42/1000 | Loss: 0.00001226
Iteration 43/1000 | Loss: 0.00001226
Iteration 44/1000 | Loss: 0.00001225
Iteration 45/1000 | Loss: 0.00001225
Iteration 46/1000 | Loss: 0.00001225
Iteration 47/1000 | Loss: 0.00001225
Iteration 48/1000 | Loss: 0.00001224
Iteration 49/1000 | Loss: 0.00001224
Iteration 50/1000 | Loss: 0.00001223
Iteration 51/1000 | Loss: 0.00001223
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001222
Iteration 54/1000 | Loss: 0.00001222
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001222
Iteration 57/1000 | Loss: 0.00001221
Iteration 58/1000 | Loss: 0.00001221
Iteration 59/1000 | Loss: 0.00001221
Iteration 60/1000 | Loss: 0.00001221
Iteration 61/1000 | Loss: 0.00001220
Iteration 62/1000 | Loss: 0.00001220
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001219
Iteration 66/1000 | Loss: 0.00001218
Iteration 67/1000 | Loss: 0.00001217
Iteration 68/1000 | Loss: 0.00001217
Iteration 69/1000 | Loss: 0.00001217
Iteration 70/1000 | Loss: 0.00001216
Iteration 71/1000 | Loss: 0.00001216
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001207
Iteration 76/1000 | Loss: 0.00001206
Iteration 77/1000 | Loss: 0.00001206
Iteration 78/1000 | Loss: 0.00001205
Iteration 79/1000 | Loss: 0.00001203
Iteration 80/1000 | Loss: 0.00001203
Iteration 81/1000 | Loss: 0.00001203
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001202
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001200
Iteration 87/1000 | Loss: 0.00001200
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001199
Iteration 90/1000 | Loss: 0.00001199
Iteration 91/1000 | Loss: 0.00001199
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001198
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001197
Iteration 96/1000 | Loss: 0.00001197
Iteration 97/1000 | Loss: 0.00001197
Iteration 98/1000 | Loss: 0.00001196
Iteration 99/1000 | Loss: 0.00001196
Iteration 100/1000 | Loss: 0.00001196
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001195
Iteration 103/1000 | Loss: 0.00001195
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001195
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001194
Iteration 110/1000 | Loss: 0.00001194
Iteration 111/1000 | Loss: 0.00001193
Iteration 112/1000 | Loss: 0.00001193
Iteration 113/1000 | Loss: 0.00001193
Iteration 114/1000 | Loss: 0.00001193
Iteration 115/1000 | Loss: 0.00001193
Iteration 116/1000 | Loss: 0.00001193
Iteration 117/1000 | Loss: 0.00001192
Iteration 118/1000 | Loss: 0.00001192
Iteration 119/1000 | Loss: 0.00001192
Iteration 120/1000 | Loss: 0.00001192
Iteration 121/1000 | Loss: 0.00001192
Iteration 122/1000 | Loss: 0.00001191
Iteration 123/1000 | Loss: 0.00001191
Iteration 124/1000 | Loss: 0.00001191
Iteration 125/1000 | Loss: 0.00001191
Iteration 126/1000 | Loss: 0.00001191
Iteration 127/1000 | Loss: 0.00001190
Iteration 128/1000 | Loss: 0.00001190
Iteration 129/1000 | Loss: 0.00001190
Iteration 130/1000 | Loss: 0.00001190
Iteration 131/1000 | Loss: 0.00001190
Iteration 132/1000 | Loss: 0.00001190
Iteration 133/1000 | Loss: 0.00001189
Iteration 134/1000 | Loss: 0.00001189
Iteration 135/1000 | Loss: 0.00001189
Iteration 136/1000 | Loss: 0.00001189
Iteration 137/1000 | Loss: 0.00001189
Iteration 138/1000 | Loss: 0.00001189
Iteration 139/1000 | Loss: 0.00001189
Iteration 140/1000 | Loss: 0.00001189
Iteration 141/1000 | Loss: 0.00001189
Iteration 142/1000 | Loss: 0.00001189
Iteration 143/1000 | Loss: 0.00001188
Iteration 144/1000 | Loss: 0.00001188
Iteration 145/1000 | Loss: 0.00001188
Iteration 146/1000 | Loss: 0.00001188
Iteration 147/1000 | Loss: 0.00001188
Iteration 148/1000 | Loss: 0.00001188
Iteration 149/1000 | Loss: 0.00001188
Iteration 150/1000 | Loss: 0.00001188
Iteration 151/1000 | Loss: 0.00001187
Iteration 152/1000 | Loss: 0.00001187
Iteration 153/1000 | Loss: 0.00001187
Iteration 154/1000 | Loss: 0.00001187
Iteration 155/1000 | Loss: 0.00001187
Iteration 156/1000 | Loss: 0.00001187
Iteration 157/1000 | Loss: 0.00001186
Iteration 158/1000 | Loss: 0.00001186
Iteration 159/1000 | Loss: 0.00001186
Iteration 160/1000 | Loss: 0.00001186
Iteration 161/1000 | Loss: 0.00001186
Iteration 162/1000 | Loss: 0.00001186
Iteration 163/1000 | Loss: 0.00001185
Iteration 164/1000 | Loss: 0.00001185
Iteration 165/1000 | Loss: 0.00001185
Iteration 166/1000 | Loss: 0.00001185
Iteration 167/1000 | Loss: 0.00001185
Iteration 168/1000 | Loss: 0.00001185
Iteration 169/1000 | Loss: 0.00001185
Iteration 170/1000 | Loss: 0.00001185
Iteration 171/1000 | Loss: 0.00001185
Iteration 172/1000 | Loss: 0.00001185
Iteration 173/1000 | Loss: 0.00001185
Iteration 174/1000 | Loss: 0.00001185
Iteration 175/1000 | Loss: 0.00001185
Iteration 176/1000 | Loss: 0.00001185
Iteration 177/1000 | Loss: 0.00001185
Iteration 178/1000 | Loss: 0.00001185
Iteration 179/1000 | Loss: 0.00001185
Iteration 180/1000 | Loss: 0.00001185
Iteration 181/1000 | Loss: 0.00001185
Iteration 182/1000 | Loss: 0.00001185
Iteration 183/1000 | Loss: 0.00001185
Iteration 184/1000 | Loss: 0.00001185
Iteration 185/1000 | Loss: 0.00001185
Iteration 186/1000 | Loss: 0.00001185
Iteration 187/1000 | Loss: 0.00001185
Iteration 188/1000 | Loss: 0.00001185
Iteration 189/1000 | Loss: 0.00001185
Iteration 190/1000 | Loss: 0.00001185
Iteration 191/1000 | Loss: 0.00001185
Iteration 192/1000 | Loss: 0.00001185
Iteration 193/1000 | Loss: 0.00001185
Iteration 194/1000 | Loss: 0.00001185
Iteration 195/1000 | Loss: 0.00001185
Iteration 196/1000 | Loss: 0.00001185
Iteration 197/1000 | Loss: 0.00001185
Iteration 198/1000 | Loss: 0.00001185
Iteration 199/1000 | Loss: 0.00001185
Iteration 200/1000 | Loss: 0.00001185
Iteration 201/1000 | Loss: 0.00001185
Iteration 202/1000 | Loss: 0.00001185
Iteration 203/1000 | Loss: 0.00001185
Iteration 204/1000 | Loss: 0.00001185
Iteration 205/1000 | Loss: 0.00001185
Iteration 206/1000 | Loss: 0.00001185
Iteration 207/1000 | Loss: 0.00001185
Iteration 208/1000 | Loss: 0.00001185
Iteration 209/1000 | Loss: 0.00001185
Iteration 210/1000 | Loss: 0.00001185
Iteration 211/1000 | Loss: 0.00001185
Iteration 212/1000 | Loss: 0.00001185
Iteration 213/1000 | Loss: 0.00001185
Iteration 214/1000 | Loss: 0.00001185
Iteration 215/1000 | Loss: 0.00001185
Iteration 216/1000 | Loss: 0.00001185
Iteration 217/1000 | Loss: 0.00001185
Iteration 218/1000 | Loss: 0.00001185
Iteration 219/1000 | Loss: 0.00001185
Iteration 220/1000 | Loss: 0.00001185
Iteration 221/1000 | Loss: 0.00001185
Iteration 222/1000 | Loss: 0.00001185
Iteration 223/1000 | Loss: 0.00001185
Iteration 224/1000 | Loss: 0.00001185
Iteration 225/1000 | Loss: 0.00001185
Iteration 226/1000 | Loss: 0.00001185
Iteration 227/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [1.1846087545563933e-05, 1.1846087545563933e-05, 1.1846087545563933e-05, 1.1846087545563933e-05, 1.1846087545563933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1846087545563933e-05

Optimization complete. Final v2v error: 2.938854217529297 mm

Highest mean error: 3.080826997756958 mm for frame 54

Lowest mean error: 2.806044578552246 mm for frame 157

Saving results

Total time: 38.8326621055603
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00883734
Iteration 2/25 | Loss: 0.00176975
Iteration 3/25 | Loss: 0.00152911
Iteration 4/25 | Loss: 0.00151991
Iteration 5/25 | Loss: 0.00151861
Iteration 6/25 | Loss: 0.00151861
Iteration 7/25 | Loss: 0.00151861
Iteration 8/25 | Loss: 0.00151861
Iteration 9/25 | Loss: 0.00151861
Iteration 10/25 | Loss: 0.00151861
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015186090022325516, 0.0015186090022325516, 0.0015186090022325516, 0.0015186090022325516, 0.0015186090022325516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015186090022325516

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.60209036
Iteration 2/25 | Loss: 0.00118273
Iteration 3/25 | Loss: 0.00118273
Iteration 4/25 | Loss: 0.00118273
Iteration 5/25 | Loss: 0.00118273
Iteration 6/25 | Loss: 0.00118273
Iteration 7/25 | Loss: 0.00118273
Iteration 8/25 | Loss: 0.00118273
Iteration 9/25 | Loss: 0.00118273
Iteration 10/25 | Loss: 0.00118273
Iteration 11/25 | Loss: 0.00118273
Iteration 12/25 | Loss: 0.00118273
Iteration 13/25 | Loss: 0.00118273
Iteration 14/25 | Loss: 0.00118273
Iteration 15/25 | Loss: 0.00118273
Iteration 16/25 | Loss: 0.00118273
Iteration 17/25 | Loss: 0.00118273
Iteration 18/25 | Loss: 0.00118273
Iteration 19/25 | Loss: 0.00118273
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011827279813587666, 0.0011827279813587666, 0.0011827279813587666, 0.0011827279813587666, 0.0011827279813587666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011827279813587666

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118273
Iteration 2/1000 | Loss: 0.00006685
Iteration 3/1000 | Loss: 0.00004038
Iteration 4/1000 | Loss: 0.00003593
Iteration 5/1000 | Loss: 0.00003426
Iteration 6/1000 | Loss: 0.00003325
Iteration 7/1000 | Loss: 0.00003249
Iteration 8/1000 | Loss: 0.00003190
Iteration 9/1000 | Loss: 0.00003157
Iteration 10/1000 | Loss: 0.00003130
Iteration 11/1000 | Loss: 0.00003109
Iteration 12/1000 | Loss: 0.00003103
Iteration 13/1000 | Loss: 0.00003091
Iteration 14/1000 | Loss: 0.00003091
Iteration 15/1000 | Loss: 0.00003087
Iteration 16/1000 | Loss: 0.00003084
Iteration 17/1000 | Loss: 0.00003081
Iteration 18/1000 | Loss: 0.00003081
Iteration 19/1000 | Loss: 0.00003081
Iteration 20/1000 | Loss: 0.00003081
Iteration 21/1000 | Loss: 0.00003081
Iteration 22/1000 | Loss: 0.00003081
Iteration 23/1000 | Loss: 0.00003081
Iteration 24/1000 | Loss: 0.00003080
Iteration 25/1000 | Loss: 0.00003080
Iteration 26/1000 | Loss: 0.00003080
Iteration 27/1000 | Loss: 0.00003080
Iteration 28/1000 | Loss: 0.00003080
Iteration 29/1000 | Loss: 0.00003079
Iteration 30/1000 | Loss: 0.00003079
Iteration 31/1000 | Loss: 0.00003079
Iteration 32/1000 | Loss: 0.00003079
Iteration 33/1000 | Loss: 0.00003079
Iteration 34/1000 | Loss: 0.00003079
Iteration 35/1000 | Loss: 0.00003079
Iteration 36/1000 | Loss: 0.00003079
Iteration 37/1000 | Loss: 0.00003079
Iteration 38/1000 | Loss: 0.00003079
Iteration 39/1000 | Loss: 0.00003079
Iteration 40/1000 | Loss: 0.00003079
Iteration 41/1000 | Loss: 0.00003078
Iteration 42/1000 | Loss: 0.00003078
Iteration 43/1000 | Loss: 0.00003078
Iteration 44/1000 | Loss: 0.00003078
Iteration 45/1000 | Loss: 0.00003078
Iteration 46/1000 | Loss: 0.00003078
Iteration 47/1000 | Loss: 0.00003078
Iteration 48/1000 | Loss: 0.00003077
Iteration 49/1000 | Loss: 0.00003077
Iteration 50/1000 | Loss: 0.00003077
Iteration 51/1000 | Loss: 0.00003077
Iteration 52/1000 | Loss: 0.00003077
Iteration 53/1000 | Loss: 0.00003077
Iteration 54/1000 | Loss: 0.00003077
Iteration 55/1000 | Loss: 0.00003077
Iteration 56/1000 | Loss: 0.00003077
Iteration 57/1000 | Loss: 0.00003077
Iteration 58/1000 | Loss: 0.00003076
Iteration 59/1000 | Loss: 0.00003076
Iteration 60/1000 | Loss: 0.00003076
Iteration 61/1000 | Loss: 0.00003076
Iteration 62/1000 | Loss: 0.00003075
Iteration 63/1000 | Loss: 0.00003075
Iteration 64/1000 | Loss: 0.00003075
Iteration 65/1000 | Loss: 0.00003075
Iteration 66/1000 | Loss: 0.00003075
Iteration 67/1000 | Loss: 0.00003075
Iteration 68/1000 | Loss: 0.00003075
Iteration 69/1000 | Loss: 0.00003075
Iteration 70/1000 | Loss: 0.00003075
Iteration 71/1000 | Loss: 0.00003074
Iteration 72/1000 | Loss: 0.00003074
Iteration 73/1000 | Loss: 0.00003074
Iteration 74/1000 | Loss: 0.00003074
Iteration 75/1000 | Loss: 0.00003074
Iteration 76/1000 | Loss: 0.00003074
Iteration 77/1000 | Loss: 0.00003074
Iteration 78/1000 | Loss: 0.00003074
Iteration 79/1000 | Loss: 0.00003074
Iteration 80/1000 | Loss: 0.00003074
Iteration 81/1000 | Loss: 0.00003074
Iteration 82/1000 | Loss: 0.00003074
Iteration 83/1000 | Loss: 0.00003074
Iteration 84/1000 | Loss: 0.00003074
Iteration 85/1000 | Loss: 0.00003074
Iteration 86/1000 | Loss: 0.00003074
Iteration 87/1000 | Loss: 0.00003074
Iteration 88/1000 | Loss: 0.00003074
Iteration 89/1000 | Loss: 0.00003074
Iteration 90/1000 | Loss: 0.00003074
Iteration 91/1000 | Loss: 0.00003074
Iteration 92/1000 | Loss: 0.00003074
Iteration 93/1000 | Loss: 0.00003074
Iteration 94/1000 | Loss: 0.00003074
Iteration 95/1000 | Loss: 0.00003074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [3.0741932278033346e-05, 3.0741932278033346e-05, 3.0741932278033346e-05, 3.0741932278033346e-05, 3.0741932278033346e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0741932278033346e-05

Optimization complete. Final v2v error: 4.642096519470215 mm

Highest mean error: 4.938783645629883 mm for frame 99

Lowest mean error: 4.465425491333008 mm for frame 47

Saving results

Total time: 29.26616144180298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00397215
Iteration 2/25 | Loss: 0.00135528
Iteration 3/25 | Loss: 0.00125199
Iteration 4/25 | Loss: 0.00123601
Iteration 5/25 | Loss: 0.00123127
Iteration 6/25 | Loss: 0.00123067
Iteration 7/25 | Loss: 0.00123067
Iteration 8/25 | Loss: 0.00123067
Iteration 9/25 | Loss: 0.00123067
Iteration 10/25 | Loss: 0.00123067
Iteration 11/25 | Loss: 0.00123067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001230674795806408, 0.001230674795806408, 0.001230674795806408, 0.001230674795806408, 0.001230674795806408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001230674795806408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41348362
Iteration 2/25 | Loss: 0.00083132
Iteration 3/25 | Loss: 0.00083132
Iteration 4/25 | Loss: 0.00083131
Iteration 5/25 | Loss: 0.00083131
Iteration 6/25 | Loss: 0.00083131
Iteration 7/25 | Loss: 0.00083131
Iteration 8/25 | Loss: 0.00083131
Iteration 9/25 | Loss: 0.00083131
Iteration 10/25 | Loss: 0.00083131
Iteration 11/25 | Loss: 0.00083131
Iteration 12/25 | Loss: 0.00083131
Iteration 13/25 | Loss: 0.00083131
Iteration 14/25 | Loss: 0.00083131
Iteration 15/25 | Loss: 0.00083131
Iteration 16/25 | Loss: 0.00083131
Iteration 17/25 | Loss: 0.00083131
Iteration 18/25 | Loss: 0.00083131
Iteration 19/25 | Loss: 0.00083131
Iteration 20/25 | Loss: 0.00083131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008313125581480563, 0.0008313125581480563, 0.0008313125581480563, 0.0008313125581480563, 0.0008313125581480563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008313125581480563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083131
Iteration 2/1000 | Loss: 0.00004404
Iteration 3/1000 | Loss: 0.00002857
Iteration 4/1000 | Loss: 0.00002503
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002200
Iteration 7/1000 | Loss: 0.00002112
Iteration 8/1000 | Loss: 0.00002052
Iteration 9/1000 | Loss: 0.00002012
Iteration 10/1000 | Loss: 0.00001970
Iteration 11/1000 | Loss: 0.00001935
Iteration 12/1000 | Loss: 0.00001913
Iteration 13/1000 | Loss: 0.00001894
Iteration 14/1000 | Loss: 0.00001894
Iteration 15/1000 | Loss: 0.00001887
Iteration 16/1000 | Loss: 0.00001887
Iteration 17/1000 | Loss: 0.00001886
Iteration 18/1000 | Loss: 0.00001886
Iteration 19/1000 | Loss: 0.00001886
Iteration 20/1000 | Loss: 0.00001885
Iteration 21/1000 | Loss: 0.00001884
Iteration 22/1000 | Loss: 0.00001884
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001882
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001879
Iteration 27/1000 | Loss: 0.00001879
Iteration 28/1000 | Loss: 0.00001875
Iteration 29/1000 | Loss: 0.00001874
Iteration 30/1000 | Loss: 0.00001872
Iteration 31/1000 | Loss: 0.00001871
Iteration 32/1000 | Loss: 0.00001869
Iteration 33/1000 | Loss: 0.00001866
Iteration 34/1000 | Loss: 0.00001865
Iteration 35/1000 | Loss: 0.00001865
Iteration 36/1000 | Loss: 0.00001864
Iteration 37/1000 | Loss: 0.00001864
Iteration 38/1000 | Loss: 0.00001863
Iteration 39/1000 | Loss: 0.00001863
Iteration 40/1000 | Loss: 0.00001862
Iteration 41/1000 | Loss: 0.00001862
Iteration 42/1000 | Loss: 0.00001862
Iteration 43/1000 | Loss: 0.00001861
Iteration 44/1000 | Loss: 0.00001860
Iteration 45/1000 | Loss: 0.00001860
Iteration 46/1000 | Loss: 0.00001860
Iteration 47/1000 | Loss: 0.00001859
Iteration 48/1000 | Loss: 0.00001859
Iteration 49/1000 | Loss: 0.00001858
Iteration 50/1000 | Loss: 0.00001858
Iteration 51/1000 | Loss: 0.00001858
Iteration 52/1000 | Loss: 0.00001857
Iteration 53/1000 | Loss: 0.00001857
Iteration 54/1000 | Loss: 0.00001857
Iteration 55/1000 | Loss: 0.00001857
Iteration 56/1000 | Loss: 0.00001857
Iteration 57/1000 | Loss: 0.00001857
Iteration 58/1000 | Loss: 0.00001856
Iteration 59/1000 | Loss: 0.00001856
Iteration 60/1000 | Loss: 0.00001856
Iteration 61/1000 | Loss: 0.00001856
Iteration 62/1000 | Loss: 0.00001856
Iteration 63/1000 | Loss: 0.00001856
Iteration 64/1000 | Loss: 0.00001856
Iteration 65/1000 | Loss: 0.00001856
Iteration 66/1000 | Loss: 0.00001856
Iteration 67/1000 | Loss: 0.00001856
Iteration 68/1000 | Loss: 0.00001856
Iteration 69/1000 | Loss: 0.00001856
Iteration 70/1000 | Loss: 0.00001856
Iteration 71/1000 | Loss: 0.00001856
Iteration 72/1000 | Loss: 0.00001856
Iteration 73/1000 | Loss: 0.00001856
Iteration 74/1000 | Loss: 0.00001856
Iteration 75/1000 | Loss: 0.00001856
Iteration 76/1000 | Loss: 0.00001856
Iteration 77/1000 | Loss: 0.00001856
Iteration 78/1000 | Loss: 0.00001856
Iteration 79/1000 | Loss: 0.00001856
Iteration 80/1000 | Loss: 0.00001856
Iteration 81/1000 | Loss: 0.00001856
Iteration 82/1000 | Loss: 0.00001856
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [1.8562242985353805e-05, 1.8562242985353805e-05, 1.8562242985353805e-05, 1.8562242985353805e-05, 1.8562242985353805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8562242985353805e-05

Optimization complete. Final v2v error: 3.6000113487243652 mm

Highest mean error: 4.500476837158203 mm for frame 96

Lowest mean error: 3.0891387462615967 mm for frame 73

Saving results

Total time: 37.20934510231018
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789063
Iteration 2/25 | Loss: 0.00154646
Iteration 3/25 | Loss: 0.00132526
Iteration 4/25 | Loss: 0.00129785
Iteration 5/25 | Loss: 0.00128798
Iteration 6/25 | Loss: 0.00128625
Iteration 7/25 | Loss: 0.00128625
Iteration 8/25 | Loss: 0.00128625
Iteration 9/25 | Loss: 0.00128625
Iteration 10/25 | Loss: 0.00128625
Iteration 11/25 | Loss: 0.00128625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012862535659223795, 0.0012862535659223795, 0.0012862535659223795, 0.0012862535659223795, 0.0012862535659223795]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012862535659223795

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82438946
Iteration 2/25 | Loss: 0.00061798
Iteration 3/25 | Loss: 0.00061798
Iteration 4/25 | Loss: 0.00061798
Iteration 5/25 | Loss: 0.00061798
Iteration 6/25 | Loss: 0.00061798
Iteration 7/25 | Loss: 0.00061798
Iteration 8/25 | Loss: 0.00061798
Iteration 9/25 | Loss: 0.00061798
Iteration 10/25 | Loss: 0.00061798
Iteration 11/25 | Loss: 0.00061798
Iteration 12/25 | Loss: 0.00061798
Iteration 13/25 | Loss: 0.00061798
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0006179806077852845, 0.0006179806077852845, 0.0006179806077852845, 0.0006179806077852845, 0.0006179806077852845]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006179806077852845

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061798
Iteration 2/1000 | Loss: 0.00004398
Iteration 3/1000 | Loss: 0.00003003
Iteration 4/1000 | Loss: 0.00002693
Iteration 5/1000 | Loss: 0.00002539
Iteration 6/1000 | Loss: 0.00002383
Iteration 7/1000 | Loss: 0.00002322
Iteration 8/1000 | Loss: 0.00002278
Iteration 9/1000 | Loss: 0.00002220
Iteration 10/1000 | Loss: 0.00002182
Iteration 11/1000 | Loss: 0.00002161
Iteration 12/1000 | Loss: 0.00002141
Iteration 13/1000 | Loss: 0.00002133
Iteration 14/1000 | Loss: 0.00002129
Iteration 15/1000 | Loss: 0.00002119
Iteration 16/1000 | Loss: 0.00002114
Iteration 17/1000 | Loss: 0.00002113
Iteration 18/1000 | Loss: 0.00002113
Iteration 19/1000 | Loss: 0.00002110
Iteration 20/1000 | Loss: 0.00002105
Iteration 21/1000 | Loss: 0.00002102
Iteration 22/1000 | Loss: 0.00002102
Iteration 23/1000 | Loss: 0.00002101
Iteration 24/1000 | Loss: 0.00002101
Iteration 25/1000 | Loss: 0.00002100
Iteration 26/1000 | Loss: 0.00002100
Iteration 27/1000 | Loss: 0.00002099
Iteration 28/1000 | Loss: 0.00002099
Iteration 29/1000 | Loss: 0.00002098
Iteration 30/1000 | Loss: 0.00002098
Iteration 31/1000 | Loss: 0.00002098
Iteration 32/1000 | Loss: 0.00002098
Iteration 33/1000 | Loss: 0.00002098
Iteration 34/1000 | Loss: 0.00002098
Iteration 35/1000 | Loss: 0.00002097
Iteration 36/1000 | Loss: 0.00002097
Iteration 37/1000 | Loss: 0.00002097
Iteration 38/1000 | Loss: 0.00002097
Iteration 39/1000 | Loss: 0.00002097
Iteration 40/1000 | Loss: 0.00002097
Iteration 41/1000 | Loss: 0.00002097
Iteration 42/1000 | Loss: 0.00002096
Iteration 43/1000 | Loss: 0.00002096
Iteration 44/1000 | Loss: 0.00002096
Iteration 45/1000 | Loss: 0.00002096
Iteration 46/1000 | Loss: 0.00002095
Iteration 47/1000 | Loss: 0.00002095
Iteration 48/1000 | Loss: 0.00002095
Iteration 49/1000 | Loss: 0.00002095
Iteration 50/1000 | Loss: 0.00002094
Iteration 51/1000 | Loss: 0.00002094
Iteration 52/1000 | Loss: 0.00002094
Iteration 53/1000 | Loss: 0.00002094
Iteration 54/1000 | Loss: 0.00002093
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002093
Iteration 57/1000 | Loss: 0.00002093
Iteration 58/1000 | Loss: 0.00002093
Iteration 59/1000 | Loss: 0.00002093
Iteration 60/1000 | Loss: 0.00002093
Iteration 61/1000 | Loss: 0.00002093
Iteration 62/1000 | Loss: 0.00002093
Iteration 63/1000 | Loss: 0.00002093
Iteration 64/1000 | Loss: 0.00002093
Iteration 65/1000 | Loss: 0.00002093
Iteration 66/1000 | Loss: 0.00002093
Iteration 67/1000 | Loss: 0.00002093
Iteration 68/1000 | Loss: 0.00002093
Iteration 69/1000 | Loss: 0.00002093
Iteration 70/1000 | Loss: 0.00002093
Iteration 71/1000 | Loss: 0.00002093
Iteration 72/1000 | Loss: 0.00002093
Iteration 73/1000 | Loss: 0.00002093
Iteration 74/1000 | Loss: 0.00002093
Iteration 75/1000 | Loss: 0.00002093
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.0925603166688234e-05, 2.0925603166688234e-05, 2.0925603166688234e-05, 2.0925603166688234e-05, 2.0925603166688234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0925603166688234e-05

Optimization complete. Final v2v error: 3.89959979057312 mm

Highest mean error: 4.215841293334961 mm for frame 98

Lowest mean error: 3.5638420581817627 mm for frame 134

Saving results

Total time: 36.33759427070618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432790
Iteration 2/25 | Loss: 0.00137159
Iteration 3/25 | Loss: 0.00129815
Iteration 4/25 | Loss: 0.00128115
Iteration 5/25 | Loss: 0.00127636
Iteration 6/25 | Loss: 0.00127518
Iteration 7/25 | Loss: 0.00127495
Iteration 8/25 | Loss: 0.00127495
Iteration 9/25 | Loss: 0.00127495
Iteration 10/25 | Loss: 0.00127495
Iteration 11/25 | Loss: 0.00127495
Iteration 12/25 | Loss: 0.00127495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001274953712709248, 0.001274953712709248, 0.001274953712709248, 0.001274953712709248, 0.001274953712709248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001274953712709248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44058287
Iteration 2/25 | Loss: 0.00083861
Iteration 3/25 | Loss: 0.00083861
Iteration 4/25 | Loss: 0.00083861
Iteration 5/25 | Loss: 0.00083861
Iteration 6/25 | Loss: 0.00083861
Iteration 7/25 | Loss: 0.00083861
Iteration 8/25 | Loss: 0.00083860
Iteration 9/25 | Loss: 0.00083860
Iteration 10/25 | Loss: 0.00083860
Iteration 11/25 | Loss: 0.00083860
Iteration 12/25 | Loss: 0.00083860
Iteration 13/25 | Loss: 0.00083860
Iteration 14/25 | Loss: 0.00083860
Iteration 15/25 | Loss: 0.00083860
Iteration 16/25 | Loss: 0.00083860
Iteration 17/25 | Loss: 0.00083860
Iteration 18/25 | Loss: 0.00083860
Iteration 19/25 | Loss: 0.00083860
Iteration 20/25 | Loss: 0.00083860
Iteration 21/25 | Loss: 0.00083860
Iteration 22/25 | Loss: 0.00083860
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008386040572077036, 0.0008386040572077036, 0.0008386040572077036, 0.0008386040572077036, 0.0008386040572077036]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008386040572077036

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083860
Iteration 2/1000 | Loss: 0.00003254
Iteration 3/1000 | Loss: 0.00002341
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00002076
Iteration 6/1000 | Loss: 0.00002018
Iteration 7/1000 | Loss: 0.00001976
Iteration 8/1000 | Loss: 0.00001939
Iteration 9/1000 | Loss: 0.00001915
Iteration 10/1000 | Loss: 0.00001895
Iteration 11/1000 | Loss: 0.00001876
Iteration 12/1000 | Loss: 0.00001861
Iteration 13/1000 | Loss: 0.00001843
Iteration 14/1000 | Loss: 0.00001839
Iteration 15/1000 | Loss: 0.00001825
Iteration 16/1000 | Loss: 0.00001824
Iteration 17/1000 | Loss: 0.00001820
Iteration 18/1000 | Loss: 0.00001819
Iteration 19/1000 | Loss: 0.00001818
Iteration 20/1000 | Loss: 0.00001817
Iteration 21/1000 | Loss: 0.00001817
Iteration 22/1000 | Loss: 0.00001813
Iteration 23/1000 | Loss: 0.00001813
Iteration 24/1000 | Loss: 0.00001812
Iteration 25/1000 | Loss: 0.00001809
Iteration 26/1000 | Loss: 0.00001807
Iteration 27/1000 | Loss: 0.00001807
Iteration 28/1000 | Loss: 0.00001806
Iteration 29/1000 | Loss: 0.00001801
Iteration 30/1000 | Loss: 0.00001801
Iteration 31/1000 | Loss: 0.00001799
Iteration 32/1000 | Loss: 0.00001799
Iteration 33/1000 | Loss: 0.00001793
Iteration 34/1000 | Loss: 0.00001792
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001789
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001786
Iteration 43/1000 | Loss: 0.00001786
Iteration 44/1000 | Loss: 0.00001785
Iteration 45/1000 | Loss: 0.00001785
Iteration 46/1000 | Loss: 0.00001783
Iteration 47/1000 | Loss: 0.00001783
Iteration 48/1000 | Loss: 0.00001783
Iteration 49/1000 | Loss: 0.00001783
Iteration 50/1000 | Loss: 0.00001782
Iteration 51/1000 | Loss: 0.00001782
Iteration 52/1000 | Loss: 0.00001782
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001781
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001780
Iteration 58/1000 | Loss: 0.00001780
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001779
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001779
Iteration 64/1000 | Loss: 0.00001779
Iteration 65/1000 | Loss: 0.00001778
Iteration 66/1000 | Loss: 0.00001778
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001778
Iteration 69/1000 | Loss: 0.00001778
Iteration 70/1000 | Loss: 0.00001778
Iteration 71/1000 | Loss: 0.00001778
Iteration 72/1000 | Loss: 0.00001778
Iteration 73/1000 | Loss: 0.00001778
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001777
Iteration 76/1000 | Loss: 0.00001777
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001776
Iteration 83/1000 | Loss: 0.00001776
Iteration 84/1000 | Loss: 0.00001776
Iteration 85/1000 | Loss: 0.00001776
Iteration 86/1000 | Loss: 0.00001776
Iteration 87/1000 | Loss: 0.00001776
Iteration 88/1000 | Loss: 0.00001776
Iteration 89/1000 | Loss: 0.00001776
Iteration 90/1000 | Loss: 0.00001776
Iteration 91/1000 | Loss: 0.00001776
Iteration 92/1000 | Loss: 0.00001776
Iteration 93/1000 | Loss: 0.00001776
Iteration 94/1000 | Loss: 0.00001776
Iteration 95/1000 | Loss: 0.00001776
Iteration 96/1000 | Loss: 0.00001776
Iteration 97/1000 | Loss: 0.00001776
Iteration 98/1000 | Loss: 0.00001776
Iteration 99/1000 | Loss: 0.00001775
Iteration 100/1000 | Loss: 0.00001775
Iteration 101/1000 | Loss: 0.00001775
Iteration 102/1000 | Loss: 0.00001775
Iteration 103/1000 | Loss: 0.00001775
Iteration 104/1000 | Loss: 0.00001775
Iteration 105/1000 | Loss: 0.00001775
Iteration 106/1000 | Loss: 0.00001775
Iteration 107/1000 | Loss: 0.00001775
Iteration 108/1000 | Loss: 0.00001775
Iteration 109/1000 | Loss: 0.00001775
Iteration 110/1000 | Loss: 0.00001775
Iteration 111/1000 | Loss: 0.00001775
Iteration 112/1000 | Loss: 0.00001775
Iteration 113/1000 | Loss: 0.00001775
Iteration 114/1000 | Loss: 0.00001775
Iteration 115/1000 | Loss: 0.00001775
Iteration 116/1000 | Loss: 0.00001775
Iteration 117/1000 | Loss: 0.00001775
Iteration 118/1000 | Loss: 0.00001775
Iteration 119/1000 | Loss: 0.00001775
Iteration 120/1000 | Loss: 0.00001775
Iteration 121/1000 | Loss: 0.00001775
Iteration 122/1000 | Loss: 0.00001775
Iteration 123/1000 | Loss: 0.00001775
Iteration 124/1000 | Loss: 0.00001775
Iteration 125/1000 | Loss: 0.00001775
Iteration 126/1000 | Loss: 0.00001775
Iteration 127/1000 | Loss: 0.00001775
Iteration 128/1000 | Loss: 0.00001775
Iteration 129/1000 | Loss: 0.00001775
Iteration 130/1000 | Loss: 0.00001775
Iteration 131/1000 | Loss: 0.00001775
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.77505880856188e-05, 1.77505880856188e-05, 1.77505880856188e-05, 1.77505880856188e-05, 1.77505880856188e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.77505880856188e-05

Optimization complete. Final v2v error: 3.5415003299713135 mm

Highest mean error: 4.242286682128906 mm for frame 70

Lowest mean error: 3.2169625759124756 mm for frame 44

Saving results

Total time: 39.84034299850464
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970416
Iteration 2/25 | Loss: 0.00424183
Iteration 3/25 | Loss: 0.00270929
Iteration 4/25 | Loss: 0.00212867
Iteration 5/25 | Loss: 0.00212291
Iteration 6/25 | Loss: 0.00198121
Iteration 7/25 | Loss: 0.00182047
Iteration 8/25 | Loss: 0.00171982
Iteration 9/25 | Loss: 0.00165478
Iteration 10/25 | Loss: 0.00161554
Iteration 11/25 | Loss: 0.00160960
Iteration 12/25 | Loss: 0.00160260
Iteration 13/25 | Loss: 0.00159616
Iteration 14/25 | Loss: 0.00159354
Iteration 15/25 | Loss: 0.00158648
Iteration 16/25 | Loss: 0.00158680
Iteration 17/25 | Loss: 0.00158450
Iteration 18/25 | Loss: 0.00158340
Iteration 19/25 | Loss: 0.00158431
Iteration 20/25 | Loss: 0.00158072
Iteration 21/25 | Loss: 0.00158047
Iteration 22/25 | Loss: 0.00158037
Iteration 23/25 | Loss: 0.00158172
Iteration 24/25 | Loss: 0.00158025
Iteration 25/25 | Loss: 0.00157959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38258243
Iteration 2/25 | Loss: 0.00295001
Iteration 3/25 | Loss: 0.00267618
Iteration 4/25 | Loss: 0.00267617
Iteration 5/25 | Loss: 0.00267617
Iteration 6/25 | Loss: 0.00267617
Iteration 7/25 | Loss: 0.00267617
Iteration 8/25 | Loss: 0.00267617
Iteration 9/25 | Loss: 0.00267617
Iteration 10/25 | Loss: 0.00267617
Iteration 11/25 | Loss: 0.00267617
Iteration 12/25 | Loss: 0.00267617
Iteration 13/25 | Loss: 0.00267617
Iteration 14/25 | Loss: 0.00267617
Iteration 15/25 | Loss: 0.00267617
Iteration 16/25 | Loss: 0.00267617
Iteration 17/25 | Loss: 0.00267617
Iteration 18/25 | Loss: 0.00267617
Iteration 19/25 | Loss: 0.00267617
Iteration 20/25 | Loss: 0.00267617
Iteration 21/25 | Loss: 0.00267617
Iteration 22/25 | Loss: 0.00267617
Iteration 23/25 | Loss: 0.00267617
Iteration 24/25 | Loss: 0.00267617
Iteration 25/25 | Loss: 0.00267617

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267617
Iteration 2/1000 | Loss: 0.00075125
Iteration 3/1000 | Loss: 0.00028290
Iteration 4/1000 | Loss: 0.00020259
Iteration 5/1000 | Loss: 0.00029948
Iteration 6/1000 | Loss: 0.00029403
Iteration 7/1000 | Loss: 0.00018574
Iteration 8/1000 | Loss: 0.00025315
Iteration 9/1000 | Loss: 0.00119580
Iteration 10/1000 | Loss: 0.00057054
Iteration 11/1000 | Loss: 0.00026746
Iteration 12/1000 | Loss: 0.00018913
Iteration 13/1000 | Loss: 0.00065120
Iteration 14/1000 | Loss: 0.00026989
Iteration 15/1000 | Loss: 0.00099341
Iteration 16/1000 | Loss: 0.00048841
Iteration 17/1000 | Loss: 0.00014064
Iteration 18/1000 | Loss: 0.00066639
Iteration 19/1000 | Loss: 0.00030564
Iteration 20/1000 | Loss: 0.00051268
Iteration 21/1000 | Loss: 0.00013705
Iteration 22/1000 | Loss: 0.00048942
Iteration 23/1000 | Loss: 0.00051338
Iteration 24/1000 | Loss: 0.00040891
Iteration 25/1000 | Loss: 0.00025796
Iteration 26/1000 | Loss: 0.00037451
Iteration 27/1000 | Loss: 0.00012093
Iteration 28/1000 | Loss: 0.00010754
Iteration 29/1000 | Loss: 0.00010257
Iteration 30/1000 | Loss: 0.00036862
Iteration 31/1000 | Loss: 0.00030165
Iteration 32/1000 | Loss: 0.00009849
Iteration 33/1000 | Loss: 0.00009417
Iteration 34/1000 | Loss: 0.00009015
Iteration 35/1000 | Loss: 0.00012586
Iteration 36/1000 | Loss: 0.00008580
Iteration 37/1000 | Loss: 0.00008827
Iteration 38/1000 | Loss: 0.00011102
Iteration 39/1000 | Loss: 0.00008161
Iteration 40/1000 | Loss: 0.00008125
Iteration 41/1000 | Loss: 0.00007989
Iteration 42/1000 | Loss: 0.00007917
Iteration 43/1000 | Loss: 0.00007853
Iteration 44/1000 | Loss: 0.00007810
Iteration 45/1000 | Loss: 0.00007759
Iteration 46/1000 | Loss: 0.00007718
Iteration 47/1000 | Loss: 0.00007657
Iteration 48/1000 | Loss: 0.00007607
Iteration 49/1000 | Loss: 0.00007562
Iteration 50/1000 | Loss: 0.00007525
Iteration 51/1000 | Loss: 0.00007494
Iteration 52/1000 | Loss: 0.00007474
Iteration 53/1000 | Loss: 0.00007457
Iteration 54/1000 | Loss: 0.00007448
Iteration 55/1000 | Loss: 0.00007448
Iteration 56/1000 | Loss: 0.00007448
Iteration 57/1000 | Loss: 0.00007442
Iteration 58/1000 | Loss: 0.00007442
Iteration 59/1000 | Loss: 0.00007442
Iteration 60/1000 | Loss: 0.00007442
Iteration 61/1000 | Loss: 0.00007442
Iteration 62/1000 | Loss: 0.00007442
Iteration 63/1000 | Loss: 0.00007442
Iteration 64/1000 | Loss: 0.00007441
Iteration 65/1000 | Loss: 0.00007441
Iteration 66/1000 | Loss: 0.00007440
Iteration 67/1000 | Loss: 0.00007440
Iteration 68/1000 | Loss: 0.00007440
Iteration 69/1000 | Loss: 0.00007439
Iteration 70/1000 | Loss: 0.00007439
Iteration 71/1000 | Loss: 0.00007438
Iteration 72/1000 | Loss: 0.00007438
Iteration 73/1000 | Loss: 0.00007438
Iteration 74/1000 | Loss: 0.00007438
Iteration 75/1000 | Loss: 0.00007437
Iteration 76/1000 | Loss: 0.00007437
Iteration 77/1000 | Loss: 0.00007437
Iteration 78/1000 | Loss: 0.00007437
Iteration 79/1000 | Loss: 0.00007437
Iteration 80/1000 | Loss: 0.00007437
Iteration 81/1000 | Loss: 0.00007437
Iteration 82/1000 | Loss: 0.00007437
Iteration 83/1000 | Loss: 0.00007437
Iteration 84/1000 | Loss: 0.00007437
Iteration 85/1000 | Loss: 0.00007437
Iteration 86/1000 | Loss: 0.00007437
Iteration 87/1000 | Loss: 0.00007436
Iteration 88/1000 | Loss: 0.00007436
Iteration 89/1000 | Loss: 0.00007436
Iteration 90/1000 | Loss: 0.00007436
Iteration 91/1000 | Loss: 0.00007435
Iteration 92/1000 | Loss: 0.00007435
Iteration 93/1000 | Loss: 0.00007435
Iteration 94/1000 | Loss: 0.00007435
Iteration 95/1000 | Loss: 0.00007435
Iteration 96/1000 | Loss: 0.00007435
Iteration 97/1000 | Loss: 0.00007435
Iteration 98/1000 | Loss: 0.00007435
Iteration 99/1000 | Loss: 0.00007435
Iteration 100/1000 | Loss: 0.00007435
Iteration 101/1000 | Loss: 0.00007435
Iteration 102/1000 | Loss: 0.00007435
Iteration 103/1000 | Loss: 0.00007434
Iteration 104/1000 | Loss: 0.00007434
Iteration 105/1000 | Loss: 0.00007434
Iteration 106/1000 | Loss: 0.00007434
Iteration 107/1000 | Loss: 0.00007434
Iteration 108/1000 | Loss: 0.00007434
Iteration 109/1000 | Loss: 0.00007434
Iteration 110/1000 | Loss: 0.00007434
Iteration 111/1000 | Loss: 0.00007433
Iteration 112/1000 | Loss: 0.00007433
Iteration 113/1000 | Loss: 0.00007433
Iteration 114/1000 | Loss: 0.00007433
Iteration 115/1000 | Loss: 0.00007432
Iteration 116/1000 | Loss: 0.00007432
Iteration 117/1000 | Loss: 0.00007432
Iteration 118/1000 | Loss: 0.00007432
Iteration 119/1000 | Loss: 0.00007432
Iteration 120/1000 | Loss: 0.00007432
Iteration 121/1000 | Loss: 0.00007432
Iteration 122/1000 | Loss: 0.00007432
Iteration 123/1000 | Loss: 0.00007431
Iteration 124/1000 | Loss: 0.00007431
Iteration 125/1000 | Loss: 0.00007431
Iteration 126/1000 | Loss: 0.00007431
Iteration 127/1000 | Loss: 0.00007431
Iteration 128/1000 | Loss: 0.00007431
Iteration 129/1000 | Loss: 0.00007431
Iteration 130/1000 | Loss: 0.00007430
Iteration 131/1000 | Loss: 0.00007430
Iteration 132/1000 | Loss: 0.00007430
Iteration 133/1000 | Loss: 0.00007430
Iteration 134/1000 | Loss: 0.00007430
Iteration 135/1000 | Loss: 0.00007430
Iteration 136/1000 | Loss: 0.00007430
Iteration 137/1000 | Loss: 0.00007430
Iteration 138/1000 | Loss: 0.00007430
Iteration 139/1000 | Loss: 0.00007429
Iteration 140/1000 | Loss: 0.00007429
Iteration 141/1000 | Loss: 0.00007429
Iteration 142/1000 | Loss: 0.00007429
Iteration 143/1000 | Loss: 0.00007429
Iteration 144/1000 | Loss: 0.00007429
Iteration 145/1000 | Loss: 0.00007429
Iteration 146/1000 | Loss: 0.00007429
Iteration 147/1000 | Loss: 0.00007429
Iteration 148/1000 | Loss: 0.00007429
Iteration 149/1000 | Loss: 0.00007428
Iteration 150/1000 | Loss: 0.00007428
Iteration 151/1000 | Loss: 0.00007428
Iteration 152/1000 | Loss: 0.00007428
Iteration 153/1000 | Loss: 0.00007428
Iteration 154/1000 | Loss: 0.00007428
Iteration 155/1000 | Loss: 0.00007428
Iteration 156/1000 | Loss: 0.00007428
Iteration 157/1000 | Loss: 0.00007428
Iteration 158/1000 | Loss: 0.00007428
Iteration 159/1000 | Loss: 0.00007428
Iteration 160/1000 | Loss: 0.00007428
Iteration 161/1000 | Loss: 0.00007428
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [7.42848715162836e-05, 7.42848715162836e-05, 7.42848715162836e-05, 7.42848715162836e-05, 7.42848715162836e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.42848715162836e-05

Optimization complete. Final v2v error: 5.0970025062561035 mm

Highest mean error: 11.20915412902832 mm for frame 85

Lowest mean error: 3.8094587326049805 mm for frame 8

Saving results

Total time: 142.7345688343048
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479110
Iteration 2/25 | Loss: 0.00132679
Iteration 3/25 | Loss: 0.00126370
Iteration 4/25 | Loss: 0.00125214
Iteration 5/25 | Loss: 0.00124775
Iteration 6/25 | Loss: 0.00124691
Iteration 7/25 | Loss: 0.00124691
Iteration 8/25 | Loss: 0.00124691
Iteration 9/25 | Loss: 0.00124691
Iteration 10/25 | Loss: 0.00124691
Iteration 11/25 | Loss: 0.00124691
Iteration 12/25 | Loss: 0.00124691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012469138018786907, 0.0012469138018786907, 0.0012469138018786907, 0.0012469138018786907, 0.0012469138018786907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012469138018786907

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37040567
Iteration 2/25 | Loss: 0.00082409
Iteration 3/25 | Loss: 0.00082409
Iteration 4/25 | Loss: 0.00082409
Iteration 5/25 | Loss: 0.00082409
Iteration 6/25 | Loss: 0.00082409
Iteration 7/25 | Loss: 0.00082409
Iteration 8/25 | Loss: 0.00082409
Iteration 9/25 | Loss: 0.00082409
Iteration 10/25 | Loss: 0.00082409
Iteration 11/25 | Loss: 0.00082409
Iteration 12/25 | Loss: 0.00082409
Iteration 13/25 | Loss: 0.00082409
Iteration 14/25 | Loss: 0.00082409
Iteration 15/25 | Loss: 0.00082409
Iteration 16/25 | Loss: 0.00082409
Iteration 17/25 | Loss: 0.00082409
Iteration 18/25 | Loss: 0.00082409
Iteration 19/25 | Loss: 0.00082409
Iteration 20/25 | Loss: 0.00082409
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.000824087590444833, 0.000824087590444833, 0.000824087590444833, 0.000824087590444833, 0.000824087590444833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000824087590444833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082409
Iteration 2/1000 | Loss: 0.00003712
Iteration 3/1000 | Loss: 0.00002134
Iteration 4/1000 | Loss: 0.00001810
Iteration 5/1000 | Loss: 0.00001699
Iteration 6/1000 | Loss: 0.00001640
Iteration 7/1000 | Loss: 0.00001584
Iteration 8/1000 | Loss: 0.00001564
Iteration 9/1000 | Loss: 0.00001538
Iteration 10/1000 | Loss: 0.00001515
Iteration 11/1000 | Loss: 0.00001508
Iteration 12/1000 | Loss: 0.00001504
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001485
Iteration 15/1000 | Loss: 0.00001484
Iteration 16/1000 | Loss: 0.00001484
Iteration 17/1000 | Loss: 0.00001483
Iteration 18/1000 | Loss: 0.00001483
Iteration 19/1000 | Loss: 0.00001481
Iteration 20/1000 | Loss: 0.00001475
Iteration 21/1000 | Loss: 0.00001474
Iteration 22/1000 | Loss: 0.00001472
Iteration 23/1000 | Loss: 0.00001468
Iteration 24/1000 | Loss: 0.00001459
Iteration 25/1000 | Loss: 0.00001459
Iteration 26/1000 | Loss: 0.00001457
Iteration 27/1000 | Loss: 0.00001456
Iteration 28/1000 | Loss: 0.00001456
Iteration 29/1000 | Loss: 0.00001455
Iteration 30/1000 | Loss: 0.00001455
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001454
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001452
Iteration 35/1000 | Loss: 0.00001451
Iteration 36/1000 | Loss: 0.00001450
Iteration 37/1000 | Loss: 0.00001450
Iteration 38/1000 | Loss: 0.00001450
Iteration 39/1000 | Loss: 0.00001449
Iteration 40/1000 | Loss: 0.00001449
Iteration 41/1000 | Loss: 0.00001449
Iteration 42/1000 | Loss: 0.00001449
Iteration 43/1000 | Loss: 0.00001449
Iteration 44/1000 | Loss: 0.00001449
Iteration 45/1000 | Loss: 0.00001448
Iteration 46/1000 | Loss: 0.00001448
Iteration 47/1000 | Loss: 0.00001448
Iteration 48/1000 | Loss: 0.00001447
Iteration 49/1000 | Loss: 0.00001447
Iteration 50/1000 | Loss: 0.00001445
Iteration 51/1000 | Loss: 0.00001445
Iteration 52/1000 | Loss: 0.00001445
Iteration 53/1000 | Loss: 0.00001445
Iteration 54/1000 | Loss: 0.00001445
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001445
Iteration 60/1000 | Loss: 0.00001443
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001443
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001441
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00001440
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001438
Iteration 78/1000 | Loss: 0.00001438
Iteration 79/1000 | Loss: 0.00001438
Iteration 80/1000 | Loss: 0.00001438
Iteration 81/1000 | Loss: 0.00001438
Iteration 82/1000 | Loss: 0.00001438
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001437
Iteration 89/1000 | Loss: 0.00001437
Iteration 90/1000 | Loss: 0.00001433
Iteration 91/1000 | Loss: 0.00001433
Iteration 92/1000 | Loss: 0.00001433
Iteration 93/1000 | Loss: 0.00001433
Iteration 94/1000 | Loss: 0.00001433
Iteration 95/1000 | Loss: 0.00001433
Iteration 96/1000 | Loss: 0.00001433
Iteration 97/1000 | Loss: 0.00001433
Iteration 98/1000 | Loss: 0.00001433
Iteration 99/1000 | Loss: 0.00001433
Iteration 100/1000 | Loss: 0.00001432
Iteration 101/1000 | Loss: 0.00001432
Iteration 102/1000 | Loss: 0.00001431
Iteration 103/1000 | Loss: 0.00001429
Iteration 104/1000 | Loss: 0.00001429
Iteration 105/1000 | Loss: 0.00001429
Iteration 106/1000 | Loss: 0.00001429
Iteration 107/1000 | Loss: 0.00001428
Iteration 108/1000 | Loss: 0.00001428
Iteration 109/1000 | Loss: 0.00001428
Iteration 110/1000 | Loss: 0.00001428
Iteration 111/1000 | Loss: 0.00001427
Iteration 112/1000 | Loss: 0.00001427
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001426
Iteration 116/1000 | Loss: 0.00001426
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001425
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001425
Iteration 122/1000 | Loss: 0.00001425
Iteration 123/1000 | Loss: 0.00001425
Iteration 124/1000 | Loss: 0.00001424
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001423
Iteration 129/1000 | Loss: 0.00001423
Iteration 130/1000 | Loss: 0.00001423
Iteration 131/1000 | Loss: 0.00001421
Iteration 132/1000 | Loss: 0.00001421
Iteration 133/1000 | Loss: 0.00001421
Iteration 134/1000 | Loss: 0.00001421
Iteration 135/1000 | Loss: 0.00001421
Iteration 136/1000 | Loss: 0.00001420
Iteration 137/1000 | Loss: 0.00001420
Iteration 138/1000 | Loss: 0.00001420
Iteration 139/1000 | Loss: 0.00001420
Iteration 140/1000 | Loss: 0.00001420
Iteration 141/1000 | Loss: 0.00001420
Iteration 142/1000 | Loss: 0.00001420
Iteration 143/1000 | Loss: 0.00001420
Iteration 144/1000 | Loss: 0.00001420
Iteration 145/1000 | Loss: 0.00001420
Iteration 146/1000 | Loss: 0.00001420
Iteration 147/1000 | Loss: 0.00001419
Iteration 148/1000 | Loss: 0.00001419
Iteration 149/1000 | Loss: 0.00001419
Iteration 150/1000 | Loss: 0.00001419
Iteration 151/1000 | Loss: 0.00001419
Iteration 152/1000 | Loss: 0.00001419
Iteration 153/1000 | Loss: 0.00001419
Iteration 154/1000 | Loss: 0.00001419
Iteration 155/1000 | Loss: 0.00001419
Iteration 156/1000 | Loss: 0.00001419
Iteration 157/1000 | Loss: 0.00001418
Iteration 158/1000 | Loss: 0.00001418
Iteration 159/1000 | Loss: 0.00001418
Iteration 160/1000 | Loss: 0.00001418
Iteration 161/1000 | Loss: 0.00001418
Iteration 162/1000 | Loss: 0.00001418
Iteration 163/1000 | Loss: 0.00001418
Iteration 164/1000 | Loss: 0.00001418
Iteration 165/1000 | Loss: 0.00001418
Iteration 166/1000 | Loss: 0.00001418
Iteration 167/1000 | Loss: 0.00001418
Iteration 168/1000 | Loss: 0.00001418
Iteration 169/1000 | Loss: 0.00001418
Iteration 170/1000 | Loss: 0.00001418
Iteration 171/1000 | Loss: 0.00001418
Iteration 172/1000 | Loss: 0.00001418
Iteration 173/1000 | Loss: 0.00001418
Iteration 174/1000 | Loss: 0.00001418
Iteration 175/1000 | Loss: 0.00001418
Iteration 176/1000 | Loss: 0.00001418
Iteration 177/1000 | Loss: 0.00001418
Iteration 178/1000 | Loss: 0.00001417
Iteration 179/1000 | Loss: 0.00001417
Iteration 180/1000 | Loss: 0.00001417
Iteration 181/1000 | Loss: 0.00001417
Iteration 182/1000 | Loss: 0.00001417
Iteration 183/1000 | Loss: 0.00001417
Iteration 184/1000 | Loss: 0.00001417
Iteration 185/1000 | Loss: 0.00001417
Iteration 186/1000 | Loss: 0.00001417
Iteration 187/1000 | Loss: 0.00001417
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.417324074282078e-05, 1.417324074282078e-05, 1.417324074282078e-05, 1.417324074282078e-05, 1.417324074282078e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.417324074282078e-05

Optimization complete. Final v2v error: 3.244502544403076 mm

Highest mean error: 3.577188491821289 mm for frame 108

Lowest mean error: 3.073183059692383 mm for frame 4

Saving results

Total time: 39.11886501312256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00595577
Iteration 2/25 | Loss: 0.00136371
Iteration 3/25 | Loss: 0.00128590
Iteration 4/25 | Loss: 0.00127073
Iteration 5/25 | Loss: 0.00126532
Iteration 6/25 | Loss: 0.00126500
Iteration 7/25 | Loss: 0.00126500
Iteration 8/25 | Loss: 0.00126500
Iteration 9/25 | Loss: 0.00126500
Iteration 10/25 | Loss: 0.00126500
Iteration 11/25 | Loss: 0.00126500
Iteration 12/25 | Loss: 0.00126500
Iteration 13/25 | Loss: 0.00126500
Iteration 14/25 | Loss: 0.00126500
Iteration 15/25 | Loss: 0.00126500
Iteration 16/25 | Loss: 0.00126500
Iteration 17/25 | Loss: 0.00126500
Iteration 18/25 | Loss: 0.00126500
Iteration 19/25 | Loss: 0.00126500
Iteration 20/25 | Loss: 0.00126500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012649975251406431, 0.0012649975251406431, 0.0012649975251406431, 0.0012649975251406431, 0.0012649975251406431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012649975251406431

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.21522498
Iteration 2/25 | Loss: 0.00079892
Iteration 3/25 | Loss: 0.00079892
Iteration 4/25 | Loss: 0.00079892
Iteration 5/25 | Loss: 0.00079892
Iteration 6/25 | Loss: 0.00079892
Iteration 7/25 | Loss: 0.00079892
Iteration 8/25 | Loss: 0.00079892
Iteration 9/25 | Loss: 0.00079892
Iteration 10/25 | Loss: 0.00079892
Iteration 11/25 | Loss: 0.00079892
Iteration 12/25 | Loss: 0.00079892
Iteration 13/25 | Loss: 0.00079892
Iteration 14/25 | Loss: 0.00079892
Iteration 15/25 | Loss: 0.00079892
Iteration 16/25 | Loss: 0.00079892
Iteration 17/25 | Loss: 0.00079892
Iteration 18/25 | Loss: 0.00079892
Iteration 19/25 | Loss: 0.00079892
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000798917724750936, 0.000798917724750936, 0.000798917724750936, 0.000798917724750936, 0.000798917724750936]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000798917724750936

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079892
Iteration 2/1000 | Loss: 0.00003796
Iteration 3/1000 | Loss: 0.00002507
Iteration 4/1000 | Loss: 0.00002303
Iteration 5/1000 | Loss: 0.00002191
Iteration 6/1000 | Loss: 0.00002128
Iteration 7/1000 | Loss: 0.00002100
Iteration 8/1000 | Loss: 0.00002061
Iteration 9/1000 | Loss: 0.00002029
Iteration 10/1000 | Loss: 0.00002005
Iteration 11/1000 | Loss: 0.00001992
Iteration 12/1000 | Loss: 0.00001991
Iteration 13/1000 | Loss: 0.00001974
Iteration 14/1000 | Loss: 0.00001971
Iteration 15/1000 | Loss: 0.00001954
Iteration 16/1000 | Loss: 0.00001949
Iteration 17/1000 | Loss: 0.00001946
Iteration 18/1000 | Loss: 0.00001945
Iteration 19/1000 | Loss: 0.00001945
Iteration 20/1000 | Loss: 0.00001944
Iteration 21/1000 | Loss: 0.00001944
Iteration 22/1000 | Loss: 0.00001942
Iteration 23/1000 | Loss: 0.00001942
Iteration 24/1000 | Loss: 0.00001941
Iteration 25/1000 | Loss: 0.00001941
Iteration 26/1000 | Loss: 0.00001941
Iteration 27/1000 | Loss: 0.00001940
Iteration 28/1000 | Loss: 0.00001936
Iteration 29/1000 | Loss: 0.00001936
Iteration 30/1000 | Loss: 0.00001935
Iteration 31/1000 | Loss: 0.00001935
Iteration 32/1000 | Loss: 0.00001931
Iteration 33/1000 | Loss: 0.00001930
Iteration 34/1000 | Loss: 0.00001929
Iteration 35/1000 | Loss: 0.00001928
Iteration 36/1000 | Loss: 0.00001925
Iteration 37/1000 | Loss: 0.00001925
Iteration 38/1000 | Loss: 0.00001925
Iteration 39/1000 | Loss: 0.00001920
Iteration 40/1000 | Loss: 0.00001920
Iteration 41/1000 | Loss: 0.00001920
Iteration 42/1000 | Loss: 0.00001920
Iteration 43/1000 | Loss: 0.00001920
Iteration 44/1000 | Loss: 0.00001920
Iteration 45/1000 | Loss: 0.00001919
Iteration 46/1000 | Loss: 0.00001919
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001918
Iteration 49/1000 | Loss: 0.00001917
Iteration 50/1000 | Loss: 0.00001917
Iteration 51/1000 | Loss: 0.00001916
Iteration 52/1000 | Loss: 0.00001916
Iteration 53/1000 | Loss: 0.00001915
Iteration 54/1000 | Loss: 0.00001915
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001914
Iteration 57/1000 | Loss: 0.00001913
Iteration 58/1000 | Loss: 0.00001913
Iteration 59/1000 | Loss: 0.00001912
Iteration 60/1000 | Loss: 0.00001912
Iteration 61/1000 | Loss: 0.00001912
Iteration 62/1000 | Loss: 0.00001912
Iteration 63/1000 | Loss: 0.00001912
Iteration 64/1000 | Loss: 0.00001911
Iteration 65/1000 | Loss: 0.00001911
Iteration 66/1000 | Loss: 0.00001911
Iteration 67/1000 | Loss: 0.00001911
Iteration 68/1000 | Loss: 0.00001911
Iteration 69/1000 | Loss: 0.00001911
Iteration 70/1000 | Loss: 0.00001910
Iteration 71/1000 | Loss: 0.00001910
Iteration 72/1000 | Loss: 0.00001910
Iteration 73/1000 | Loss: 0.00001909
Iteration 74/1000 | Loss: 0.00001909
Iteration 75/1000 | Loss: 0.00001909
Iteration 76/1000 | Loss: 0.00001909
Iteration 77/1000 | Loss: 0.00001908
Iteration 78/1000 | Loss: 0.00001908
Iteration 79/1000 | Loss: 0.00001908
Iteration 80/1000 | Loss: 0.00001908
Iteration 81/1000 | Loss: 0.00001908
Iteration 82/1000 | Loss: 0.00001907
Iteration 83/1000 | Loss: 0.00001907
Iteration 84/1000 | Loss: 0.00001907
Iteration 85/1000 | Loss: 0.00001907
Iteration 86/1000 | Loss: 0.00001907
Iteration 87/1000 | Loss: 0.00001906
Iteration 88/1000 | Loss: 0.00001906
Iteration 89/1000 | Loss: 0.00001906
Iteration 90/1000 | Loss: 0.00001906
Iteration 91/1000 | Loss: 0.00001906
Iteration 92/1000 | Loss: 0.00001906
Iteration 93/1000 | Loss: 0.00001906
Iteration 94/1000 | Loss: 0.00001906
Iteration 95/1000 | Loss: 0.00001906
Iteration 96/1000 | Loss: 0.00001906
Iteration 97/1000 | Loss: 0.00001906
Iteration 98/1000 | Loss: 0.00001906
Iteration 99/1000 | Loss: 0.00001906
Iteration 100/1000 | Loss: 0.00001906
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001905
Iteration 106/1000 | Loss: 0.00001905
Iteration 107/1000 | Loss: 0.00001905
Iteration 108/1000 | Loss: 0.00001905
Iteration 109/1000 | Loss: 0.00001905
Iteration 110/1000 | Loss: 0.00001905
Iteration 111/1000 | Loss: 0.00001905
Iteration 112/1000 | Loss: 0.00001905
Iteration 113/1000 | Loss: 0.00001905
Iteration 114/1000 | Loss: 0.00001905
Iteration 115/1000 | Loss: 0.00001905
Iteration 116/1000 | Loss: 0.00001905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.905270073621068e-05, 1.905270073621068e-05, 1.905270073621068e-05, 1.905270073621068e-05, 1.905270073621068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.905270073621068e-05

Optimization complete. Final v2v error: 3.6052918434143066 mm

Highest mean error: 4.39068603515625 mm for frame 128

Lowest mean error: 3.0415923595428467 mm for frame 6

Saving results

Total time: 37.86077356338501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056881
Iteration 2/25 | Loss: 0.00228723
Iteration 3/25 | Loss: 0.00257843
Iteration 4/25 | Loss: 0.00173023
Iteration 5/25 | Loss: 0.00145349
Iteration 6/25 | Loss: 0.00143453
Iteration 7/25 | Loss: 0.00143095
Iteration 8/25 | Loss: 0.00142931
Iteration 9/25 | Loss: 0.00142882
Iteration 10/25 | Loss: 0.00142866
Iteration 11/25 | Loss: 0.00142865
Iteration 12/25 | Loss: 0.00142865
Iteration 13/25 | Loss: 0.00142865
Iteration 14/25 | Loss: 0.00142864
Iteration 15/25 | Loss: 0.00142864
Iteration 16/25 | Loss: 0.00142864
Iteration 17/25 | Loss: 0.00142864
Iteration 18/25 | Loss: 0.00142864
Iteration 19/25 | Loss: 0.00142864
Iteration 20/25 | Loss: 0.00142864
Iteration 21/25 | Loss: 0.00142864
Iteration 22/25 | Loss: 0.00142864
Iteration 23/25 | Loss: 0.00142863
Iteration 24/25 | Loss: 0.00142863
Iteration 25/25 | Loss: 0.00142863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95960826
Iteration 2/25 | Loss: 0.00098923
Iteration 3/25 | Loss: 0.00098923
Iteration 4/25 | Loss: 0.00098923
Iteration 5/25 | Loss: 0.00098923
Iteration 6/25 | Loss: 0.00098923
Iteration 7/25 | Loss: 0.00098923
Iteration 8/25 | Loss: 0.00098923
Iteration 9/25 | Loss: 0.00098923
Iteration 10/25 | Loss: 0.00098923
Iteration 11/25 | Loss: 0.00098923
Iteration 12/25 | Loss: 0.00098923
Iteration 13/25 | Loss: 0.00098922
Iteration 14/25 | Loss: 0.00098922
Iteration 15/25 | Loss: 0.00098922
Iteration 16/25 | Loss: 0.00098922
Iteration 17/25 | Loss: 0.00098922
Iteration 18/25 | Loss: 0.00098922
Iteration 19/25 | Loss: 0.00098922
Iteration 20/25 | Loss: 0.00098922
Iteration 21/25 | Loss: 0.00098922
Iteration 22/25 | Loss: 0.00098922
Iteration 23/25 | Loss: 0.00098922
Iteration 24/25 | Loss: 0.00098922
Iteration 25/25 | Loss: 0.00098922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098922
Iteration 2/1000 | Loss: 0.00004836
Iteration 3/1000 | Loss: 0.00003611
Iteration 4/1000 | Loss: 0.00003239
Iteration 5/1000 | Loss: 0.00003108
Iteration 6/1000 | Loss: 0.00003017
Iteration 7/1000 | Loss: 0.00002953
Iteration 8/1000 | Loss: 0.00002906
Iteration 9/1000 | Loss: 0.00002869
Iteration 10/1000 | Loss: 0.00002828
Iteration 11/1000 | Loss: 0.00002793
Iteration 12/1000 | Loss: 0.00002761
Iteration 13/1000 | Loss: 0.00002733
Iteration 14/1000 | Loss: 0.00002707
Iteration 15/1000 | Loss: 0.00002682
Iteration 16/1000 | Loss: 0.00002662
Iteration 17/1000 | Loss: 0.00002653
Iteration 18/1000 | Loss: 0.00002651
Iteration 19/1000 | Loss: 0.00002648
Iteration 20/1000 | Loss: 0.00002647
Iteration 21/1000 | Loss: 0.00002640
Iteration 22/1000 | Loss: 0.00002628
Iteration 23/1000 | Loss: 0.00002623
Iteration 24/1000 | Loss: 0.00002622
Iteration 25/1000 | Loss: 0.00002621
Iteration 26/1000 | Loss: 0.00002619
Iteration 27/1000 | Loss: 0.00002619
Iteration 28/1000 | Loss: 0.00002619
Iteration 29/1000 | Loss: 0.00002619
Iteration 30/1000 | Loss: 0.00002619
Iteration 31/1000 | Loss: 0.00002619
Iteration 32/1000 | Loss: 0.00002618
Iteration 33/1000 | Loss: 0.00002618
Iteration 34/1000 | Loss: 0.00002618
Iteration 35/1000 | Loss: 0.00002618
Iteration 36/1000 | Loss: 0.00002618
Iteration 37/1000 | Loss: 0.00002618
Iteration 38/1000 | Loss: 0.00002618
Iteration 39/1000 | Loss: 0.00002618
Iteration 40/1000 | Loss: 0.00002618
Iteration 41/1000 | Loss: 0.00002618
Iteration 42/1000 | Loss: 0.00002618
Iteration 43/1000 | Loss: 0.00002617
Iteration 44/1000 | Loss: 0.00002617
Iteration 45/1000 | Loss: 0.00002617
Iteration 46/1000 | Loss: 0.00002617
Iteration 47/1000 | Loss: 0.00002617
Iteration 48/1000 | Loss: 0.00002617
Iteration 49/1000 | Loss: 0.00002617
Iteration 50/1000 | Loss: 0.00002617
Iteration 51/1000 | Loss: 0.00002615
Iteration 52/1000 | Loss: 0.00002615
Iteration 53/1000 | Loss: 0.00002615
Iteration 54/1000 | Loss: 0.00002615
Iteration 55/1000 | Loss: 0.00002615
Iteration 56/1000 | Loss: 0.00002614
Iteration 57/1000 | Loss: 0.00002614
Iteration 58/1000 | Loss: 0.00002614
Iteration 59/1000 | Loss: 0.00002613
Iteration 60/1000 | Loss: 0.00002613
Iteration 61/1000 | Loss: 0.00002613
Iteration 62/1000 | Loss: 0.00002613
Iteration 63/1000 | Loss: 0.00002612
Iteration 64/1000 | Loss: 0.00002612
Iteration 65/1000 | Loss: 0.00002612
Iteration 66/1000 | Loss: 0.00002612
Iteration 67/1000 | Loss: 0.00002612
Iteration 68/1000 | Loss: 0.00002611
Iteration 69/1000 | Loss: 0.00002611
Iteration 70/1000 | Loss: 0.00002611
Iteration 71/1000 | Loss: 0.00002611
Iteration 72/1000 | Loss: 0.00002611
Iteration 73/1000 | Loss: 0.00002610
Iteration 74/1000 | Loss: 0.00002610
Iteration 75/1000 | Loss: 0.00002610
Iteration 76/1000 | Loss: 0.00002610
Iteration 77/1000 | Loss: 0.00002609
Iteration 78/1000 | Loss: 0.00002609
Iteration 79/1000 | Loss: 0.00002609
Iteration 80/1000 | Loss: 0.00002609
Iteration 81/1000 | Loss: 0.00002609
Iteration 82/1000 | Loss: 0.00002608
Iteration 83/1000 | Loss: 0.00002608
Iteration 84/1000 | Loss: 0.00002608
Iteration 85/1000 | Loss: 0.00002607
Iteration 86/1000 | Loss: 0.00002607
Iteration 87/1000 | Loss: 0.00002607
Iteration 88/1000 | Loss: 0.00002607
Iteration 89/1000 | Loss: 0.00002607
Iteration 90/1000 | Loss: 0.00002607
Iteration 91/1000 | Loss: 0.00002607
Iteration 92/1000 | Loss: 0.00002606
Iteration 93/1000 | Loss: 0.00002606
Iteration 94/1000 | Loss: 0.00002606
Iteration 95/1000 | Loss: 0.00002606
Iteration 96/1000 | Loss: 0.00002606
Iteration 97/1000 | Loss: 0.00002606
Iteration 98/1000 | Loss: 0.00002606
Iteration 99/1000 | Loss: 0.00002606
Iteration 100/1000 | Loss: 0.00002606
Iteration 101/1000 | Loss: 0.00002606
Iteration 102/1000 | Loss: 0.00002605
Iteration 103/1000 | Loss: 0.00002605
Iteration 104/1000 | Loss: 0.00002605
Iteration 105/1000 | Loss: 0.00002605
Iteration 106/1000 | Loss: 0.00002605
Iteration 107/1000 | Loss: 0.00002605
Iteration 108/1000 | Loss: 0.00002605
Iteration 109/1000 | Loss: 0.00002604
Iteration 110/1000 | Loss: 0.00002604
Iteration 111/1000 | Loss: 0.00002604
Iteration 112/1000 | Loss: 0.00002604
Iteration 113/1000 | Loss: 0.00002604
Iteration 114/1000 | Loss: 0.00002604
Iteration 115/1000 | Loss: 0.00002604
Iteration 116/1000 | Loss: 0.00002604
Iteration 117/1000 | Loss: 0.00002604
Iteration 118/1000 | Loss: 0.00002603
Iteration 119/1000 | Loss: 0.00002603
Iteration 120/1000 | Loss: 0.00002603
Iteration 121/1000 | Loss: 0.00002603
Iteration 122/1000 | Loss: 0.00002603
Iteration 123/1000 | Loss: 0.00002603
Iteration 124/1000 | Loss: 0.00002603
Iteration 125/1000 | Loss: 0.00002603
Iteration 126/1000 | Loss: 0.00002602
Iteration 127/1000 | Loss: 0.00002602
Iteration 128/1000 | Loss: 0.00002602
Iteration 129/1000 | Loss: 0.00002602
Iteration 130/1000 | Loss: 0.00002602
Iteration 131/1000 | Loss: 0.00002602
Iteration 132/1000 | Loss: 0.00002602
Iteration 133/1000 | Loss: 0.00002602
Iteration 134/1000 | Loss: 0.00002602
Iteration 135/1000 | Loss: 0.00002602
Iteration 136/1000 | Loss: 0.00002602
Iteration 137/1000 | Loss: 0.00002602
Iteration 138/1000 | Loss: 0.00002602
Iteration 139/1000 | Loss: 0.00002601
Iteration 140/1000 | Loss: 0.00002601
Iteration 141/1000 | Loss: 0.00002601
Iteration 142/1000 | Loss: 0.00002601
Iteration 143/1000 | Loss: 0.00002601
Iteration 144/1000 | Loss: 0.00002601
Iteration 145/1000 | Loss: 0.00002601
Iteration 146/1000 | Loss: 0.00002601
Iteration 147/1000 | Loss: 0.00002601
Iteration 148/1000 | Loss: 0.00002601
Iteration 149/1000 | Loss: 0.00002601
Iteration 150/1000 | Loss: 0.00002601
Iteration 151/1000 | Loss: 0.00002601
Iteration 152/1000 | Loss: 0.00002601
Iteration 153/1000 | Loss: 0.00002601
Iteration 154/1000 | Loss: 0.00002601
Iteration 155/1000 | Loss: 0.00002601
Iteration 156/1000 | Loss: 0.00002601
Iteration 157/1000 | Loss: 0.00002601
Iteration 158/1000 | Loss: 0.00002601
Iteration 159/1000 | Loss: 0.00002601
Iteration 160/1000 | Loss: 0.00002600
Iteration 161/1000 | Loss: 0.00002600
Iteration 162/1000 | Loss: 0.00002600
Iteration 163/1000 | Loss: 0.00002600
Iteration 164/1000 | Loss: 0.00002600
Iteration 165/1000 | Loss: 0.00002600
Iteration 166/1000 | Loss: 0.00002600
Iteration 167/1000 | Loss: 0.00002600
Iteration 168/1000 | Loss: 0.00002600
Iteration 169/1000 | Loss: 0.00002600
Iteration 170/1000 | Loss: 0.00002600
Iteration 171/1000 | Loss: 0.00002600
Iteration 172/1000 | Loss: 0.00002600
Iteration 173/1000 | Loss: 0.00002600
Iteration 174/1000 | Loss: 0.00002600
Iteration 175/1000 | Loss: 0.00002600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.6002117010648362e-05, 2.6002117010648362e-05, 2.6002117010648362e-05, 2.6002117010648362e-05, 2.6002117010648362e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6002117010648362e-05

Optimization complete. Final v2v error: 4.236876487731934 mm

Highest mean error: 5.129278182983398 mm for frame 139

Lowest mean error: 3.5589406490325928 mm for frame 28

Saving results

Total time: 57.540302991867065
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787817
Iteration 2/25 | Loss: 0.00174677
Iteration 3/25 | Loss: 0.00148943
Iteration 4/25 | Loss: 0.00144292
Iteration 5/25 | Loss: 0.00142427
Iteration 6/25 | Loss: 0.00145690
Iteration 7/25 | Loss: 0.00143451
Iteration 8/25 | Loss: 0.00142718
Iteration 9/25 | Loss: 0.00141374
Iteration 10/25 | Loss: 0.00142020
Iteration 11/25 | Loss: 0.00140696
Iteration 12/25 | Loss: 0.00140418
Iteration 13/25 | Loss: 0.00140172
Iteration 14/25 | Loss: 0.00139856
Iteration 15/25 | Loss: 0.00139810
Iteration 16/25 | Loss: 0.00139703
Iteration 17/25 | Loss: 0.00139451
Iteration 18/25 | Loss: 0.00139330
Iteration 19/25 | Loss: 0.00139295
Iteration 20/25 | Loss: 0.00139280
Iteration 21/25 | Loss: 0.00139479
Iteration 22/25 | Loss: 0.00139134
Iteration 23/25 | Loss: 0.00139070
Iteration 24/25 | Loss: 0.00139041
Iteration 25/25 | Loss: 0.00139039

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53172350
Iteration 2/25 | Loss: 0.00092295
Iteration 3/25 | Loss: 0.00092295
Iteration 4/25 | Loss: 0.00092295
Iteration 5/25 | Loss: 0.00092295
Iteration 6/25 | Loss: 0.00092295
Iteration 7/25 | Loss: 0.00092295
Iteration 8/25 | Loss: 0.00092295
Iteration 9/25 | Loss: 0.00092295
Iteration 10/25 | Loss: 0.00092295
Iteration 11/25 | Loss: 0.00092295
Iteration 12/25 | Loss: 0.00092295
Iteration 13/25 | Loss: 0.00092295
Iteration 14/25 | Loss: 0.00092295
Iteration 15/25 | Loss: 0.00092295
Iteration 16/25 | Loss: 0.00092295
Iteration 17/25 | Loss: 0.00092295
Iteration 18/25 | Loss: 0.00092295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009229479474015534, 0.0009229479474015534, 0.0009229479474015534, 0.0009229479474015534, 0.0009229479474015534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009229479474015534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092295
Iteration 2/1000 | Loss: 0.00004473
Iteration 3/1000 | Loss: 0.00003370
Iteration 4/1000 | Loss: 0.00003014
Iteration 5/1000 | Loss: 0.00002870
Iteration 6/1000 | Loss: 0.00002769
Iteration 7/1000 | Loss: 0.00002717
Iteration 8/1000 | Loss: 0.00002662
Iteration 9/1000 | Loss: 0.00002624
Iteration 10/1000 | Loss: 0.00002598
Iteration 11/1000 | Loss: 0.00002568
Iteration 12/1000 | Loss: 0.00002553
Iteration 13/1000 | Loss: 0.00002553
Iteration 14/1000 | Loss: 0.00002549
Iteration 15/1000 | Loss: 0.00002530
Iteration 16/1000 | Loss: 0.00002518
Iteration 17/1000 | Loss: 0.00002508
Iteration 18/1000 | Loss: 0.00002504
Iteration 19/1000 | Loss: 0.00002498
Iteration 20/1000 | Loss: 0.00002498
Iteration 21/1000 | Loss: 0.00002497
Iteration 22/1000 | Loss: 0.00002489
Iteration 23/1000 | Loss: 0.00002483
Iteration 24/1000 | Loss: 0.00002483
Iteration 25/1000 | Loss: 0.00002482
Iteration 26/1000 | Loss: 0.00002482
Iteration 27/1000 | Loss: 0.00002480
Iteration 28/1000 | Loss: 0.00002479
Iteration 29/1000 | Loss: 0.00002468
Iteration 30/1000 | Loss: 0.00002465
Iteration 31/1000 | Loss: 0.00002465
Iteration 32/1000 | Loss: 0.00002462
Iteration 33/1000 | Loss: 0.00002462
Iteration 34/1000 | Loss: 0.00002461
Iteration 35/1000 | Loss: 0.00002461
Iteration 36/1000 | Loss: 0.00002461
Iteration 37/1000 | Loss: 0.00002461
Iteration 38/1000 | Loss: 0.00002460
Iteration 39/1000 | Loss: 0.00002460
Iteration 40/1000 | Loss: 0.00002460
Iteration 41/1000 | Loss: 0.00002460
Iteration 42/1000 | Loss: 0.00002460
Iteration 43/1000 | Loss: 0.00002459
Iteration 44/1000 | Loss: 0.00002459
Iteration 45/1000 | Loss: 0.00002459
Iteration 46/1000 | Loss: 0.00002459
Iteration 47/1000 | Loss: 0.00002459
Iteration 48/1000 | Loss: 0.00002459
Iteration 49/1000 | Loss: 0.00002458
Iteration 50/1000 | Loss: 0.00002458
Iteration 51/1000 | Loss: 0.00002458
Iteration 52/1000 | Loss: 0.00002458
Iteration 53/1000 | Loss: 0.00002458
Iteration 54/1000 | Loss: 0.00002457
Iteration 55/1000 | Loss: 0.00002457
Iteration 56/1000 | Loss: 0.00002457
Iteration 57/1000 | Loss: 0.00002457
Iteration 58/1000 | Loss: 0.00002457
Iteration 59/1000 | Loss: 0.00002457
Iteration 60/1000 | Loss: 0.00002457
Iteration 61/1000 | Loss: 0.00002457
Iteration 62/1000 | Loss: 0.00002457
Iteration 63/1000 | Loss: 0.00002457
Iteration 64/1000 | Loss: 0.00002457
Iteration 65/1000 | Loss: 0.00002457
Iteration 66/1000 | Loss: 0.00002457
Iteration 67/1000 | Loss: 0.00002457
Iteration 68/1000 | Loss: 0.00002457
Iteration 69/1000 | Loss: 0.00002456
Iteration 70/1000 | Loss: 0.00002456
Iteration 71/1000 | Loss: 0.00002456
Iteration 72/1000 | Loss: 0.00002456
Iteration 73/1000 | Loss: 0.00002456
Iteration 74/1000 | Loss: 0.00002456
Iteration 75/1000 | Loss: 0.00002456
Iteration 76/1000 | Loss: 0.00002456
Iteration 77/1000 | Loss: 0.00002456
Iteration 78/1000 | Loss: 0.00002456
Iteration 79/1000 | Loss: 0.00002456
Iteration 80/1000 | Loss: 0.00002456
Iteration 81/1000 | Loss: 0.00002456
Iteration 82/1000 | Loss: 0.00002456
Iteration 83/1000 | Loss: 0.00002456
Iteration 84/1000 | Loss: 0.00002455
Iteration 85/1000 | Loss: 0.00002455
Iteration 86/1000 | Loss: 0.00002455
Iteration 87/1000 | Loss: 0.00002455
Iteration 88/1000 | Loss: 0.00002455
Iteration 89/1000 | Loss: 0.00002455
Iteration 90/1000 | Loss: 0.00002455
Iteration 91/1000 | Loss: 0.00002455
Iteration 92/1000 | Loss: 0.00002455
Iteration 93/1000 | Loss: 0.00002455
Iteration 94/1000 | Loss: 0.00002455
Iteration 95/1000 | Loss: 0.00002455
Iteration 96/1000 | Loss: 0.00002455
Iteration 97/1000 | Loss: 0.00002455
Iteration 98/1000 | Loss: 0.00002454
Iteration 99/1000 | Loss: 0.00002454
Iteration 100/1000 | Loss: 0.00002454
Iteration 101/1000 | Loss: 0.00002454
Iteration 102/1000 | Loss: 0.00002454
Iteration 103/1000 | Loss: 0.00002454
Iteration 104/1000 | Loss: 0.00002454
Iteration 105/1000 | Loss: 0.00002454
Iteration 106/1000 | Loss: 0.00002454
Iteration 107/1000 | Loss: 0.00002454
Iteration 108/1000 | Loss: 0.00002454
Iteration 109/1000 | Loss: 0.00002454
Iteration 110/1000 | Loss: 0.00002454
Iteration 111/1000 | Loss: 0.00002454
Iteration 112/1000 | Loss: 0.00002454
Iteration 113/1000 | Loss: 0.00002453
Iteration 114/1000 | Loss: 0.00002453
Iteration 115/1000 | Loss: 0.00002453
Iteration 116/1000 | Loss: 0.00002453
Iteration 117/1000 | Loss: 0.00002453
Iteration 118/1000 | Loss: 0.00002453
Iteration 119/1000 | Loss: 0.00002453
Iteration 120/1000 | Loss: 0.00002453
Iteration 121/1000 | Loss: 0.00002453
Iteration 122/1000 | Loss: 0.00002453
Iteration 123/1000 | Loss: 0.00002453
Iteration 124/1000 | Loss: 0.00002453
Iteration 125/1000 | Loss: 0.00002453
Iteration 126/1000 | Loss: 0.00002453
Iteration 127/1000 | Loss: 0.00002452
Iteration 128/1000 | Loss: 0.00002452
Iteration 129/1000 | Loss: 0.00002452
Iteration 130/1000 | Loss: 0.00002452
Iteration 131/1000 | Loss: 0.00002452
Iteration 132/1000 | Loss: 0.00002452
Iteration 133/1000 | Loss: 0.00002452
Iteration 134/1000 | Loss: 0.00002452
Iteration 135/1000 | Loss: 0.00002452
Iteration 136/1000 | Loss: 0.00002452
Iteration 137/1000 | Loss: 0.00002452
Iteration 138/1000 | Loss: 0.00002452
Iteration 139/1000 | Loss: 0.00002452
Iteration 140/1000 | Loss: 0.00002452
Iteration 141/1000 | Loss: 0.00002452
Iteration 142/1000 | Loss: 0.00002452
Iteration 143/1000 | Loss: 0.00002452
Iteration 144/1000 | Loss: 0.00002452
Iteration 145/1000 | Loss: 0.00002452
Iteration 146/1000 | Loss: 0.00002452
Iteration 147/1000 | Loss: 0.00002452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.4517452402506024e-05, 2.4517452402506024e-05, 2.4517452402506024e-05, 2.4517452402506024e-05, 2.4517452402506024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4517452402506024e-05

Optimization complete. Final v2v error: 4.043971061706543 mm

Highest mean error: 11.925616264343262 mm for frame 104

Lowest mean error: 3.3950130939483643 mm for frame 92

Saving results

Total time: 85.24251055717468
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047044
Iteration 2/25 | Loss: 0.01047044
Iteration 3/25 | Loss: 0.01047044
Iteration 4/25 | Loss: 0.00249871
Iteration 5/25 | Loss: 0.00163736
Iteration 6/25 | Loss: 0.00152572
Iteration 7/25 | Loss: 0.00150374
Iteration 8/25 | Loss: 0.00144833
Iteration 9/25 | Loss: 0.00140437
Iteration 10/25 | Loss: 0.00138978
Iteration 11/25 | Loss: 0.00135288
Iteration 12/25 | Loss: 0.00134146
Iteration 13/25 | Loss: 0.00133889
Iteration 14/25 | Loss: 0.00133632
Iteration 15/25 | Loss: 0.00133732
Iteration 16/25 | Loss: 0.00132673
Iteration 17/25 | Loss: 0.00131890
Iteration 18/25 | Loss: 0.00131553
Iteration 19/25 | Loss: 0.00131404
Iteration 20/25 | Loss: 0.00131366
Iteration 21/25 | Loss: 0.00131354
Iteration 22/25 | Loss: 0.00131347
Iteration 23/25 | Loss: 0.00131331
Iteration 24/25 | Loss: 0.00131318
Iteration 25/25 | Loss: 0.00131314

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57926404
Iteration 2/25 | Loss: 0.00151912
Iteration 3/25 | Loss: 0.00148399
Iteration 4/25 | Loss: 0.00148399
Iteration 5/25 | Loss: 0.00148399
Iteration 6/25 | Loss: 0.00148399
Iteration 7/25 | Loss: 0.00148399
Iteration 8/25 | Loss: 0.00148399
Iteration 9/25 | Loss: 0.00148399
Iteration 10/25 | Loss: 0.00148399
Iteration 11/25 | Loss: 0.00148399
Iteration 12/25 | Loss: 0.00148399
Iteration 13/25 | Loss: 0.00148399
Iteration 14/25 | Loss: 0.00148399
Iteration 15/25 | Loss: 0.00148399
Iteration 16/25 | Loss: 0.00148399
Iteration 17/25 | Loss: 0.00148399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001483992557041347, 0.001483992557041347, 0.001483992557041347, 0.001483992557041347, 0.001483992557041347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001483992557041347

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148399
Iteration 2/1000 | Loss: 0.00040280
Iteration 3/1000 | Loss: 0.00052388
Iteration 4/1000 | Loss: 0.00028585
Iteration 5/1000 | Loss: 0.00004639
Iteration 6/1000 | Loss: 0.00042410
Iteration 7/1000 | Loss: 0.00037460
Iteration 8/1000 | Loss: 0.00016616
Iteration 9/1000 | Loss: 0.00036784
Iteration 10/1000 | Loss: 0.00022375
Iteration 11/1000 | Loss: 0.00051229
Iteration 12/1000 | Loss: 0.00050273
Iteration 13/1000 | Loss: 0.00020672
Iteration 14/1000 | Loss: 0.00020905
Iteration 15/1000 | Loss: 0.00019694
Iteration 16/1000 | Loss: 0.00031967
Iteration 17/1000 | Loss: 0.00022337
Iteration 18/1000 | Loss: 0.00029626
Iteration 19/1000 | Loss: 0.00020162
Iteration 20/1000 | Loss: 0.00020755
Iteration 21/1000 | Loss: 0.00016425
Iteration 22/1000 | Loss: 0.00015804
Iteration 23/1000 | Loss: 0.00006283
Iteration 24/1000 | Loss: 0.00004267
Iteration 25/1000 | Loss: 0.00006159
Iteration 26/1000 | Loss: 0.00005169
Iteration 27/1000 | Loss: 0.00020700
Iteration 28/1000 | Loss: 0.00020763
Iteration 29/1000 | Loss: 0.00011310
Iteration 30/1000 | Loss: 0.00013615
Iteration 31/1000 | Loss: 0.00019518
Iteration 32/1000 | Loss: 0.00016536
Iteration 33/1000 | Loss: 0.00004803
Iteration 34/1000 | Loss: 0.00003674
Iteration 35/1000 | Loss: 0.00011670
Iteration 36/1000 | Loss: 0.00007912
Iteration 37/1000 | Loss: 0.00007350
Iteration 38/1000 | Loss: 0.00002721
Iteration 39/1000 | Loss: 0.00011043
Iteration 40/1000 | Loss: 0.00007568
Iteration 41/1000 | Loss: 0.00013075
Iteration 42/1000 | Loss: 0.00004669
Iteration 43/1000 | Loss: 0.00002625
Iteration 44/1000 | Loss: 0.00002534
Iteration 45/1000 | Loss: 0.00002477
Iteration 46/1000 | Loss: 0.00030704
Iteration 47/1000 | Loss: 0.00026840
Iteration 48/1000 | Loss: 0.00013772
Iteration 49/1000 | Loss: 0.00008505
Iteration 50/1000 | Loss: 0.00002902
Iteration 51/1000 | Loss: 0.00026460
Iteration 52/1000 | Loss: 0.00024820
Iteration 53/1000 | Loss: 0.00032249
Iteration 54/1000 | Loss: 0.00032750
Iteration 55/1000 | Loss: 0.00119249
Iteration 56/1000 | Loss: 0.00053961
Iteration 57/1000 | Loss: 0.00037329
Iteration 58/1000 | Loss: 0.00026877
Iteration 59/1000 | Loss: 0.00036311
Iteration 60/1000 | Loss: 0.00020392
Iteration 61/1000 | Loss: 0.00015322
Iteration 62/1000 | Loss: 0.00008272
Iteration 63/1000 | Loss: 0.00012829
Iteration 64/1000 | Loss: 0.00027878
Iteration 65/1000 | Loss: 0.00018914
Iteration 66/1000 | Loss: 0.00011242
Iteration 67/1000 | Loss: 0.00010955
Iteration 68/1000 | Loss: 0.00010559
Iteration 69/1000 | Loss: 0.00017932
Iteration 70/1000 | Loss: 0.00018085
Iteration 71/1000 | Loss: 0.00007165
Iteration 72/1000 | Loss: 0.00013306
Iteration 73/1000 | Loss: 0.00005205
Iteration 74/1000 | Loss: 0.00011252
Iteration 75/1000 | Loss: 0.00005404
Iteration 76/1000 | Loss: 0.00010805
Iteration 77/1000 | Loss: 0.00010838
Iteration 78/1000 | Loss: 0.00009240
Iteration 79/1000 | Loss: 0.00025290
Iteration 80/1000 | Loss: 0.00019742
Iteration 81/1000 | Loss: 0.00012246
Iteration 82/1000 | Loss: 0.00010832
Iteration 83/1000 | Loss: 0.00010658
Iteration 84/1000 | Loss: 0.00008056
Iteration 85/1000 | Loss: 0.00009293
Iteration 86/1000 | Loss: 0.00004340
Iteration 87/1000 | Loss: 0.00008511
Iteration 88/1000 | Loss: 0.00010034
Iteration 89/1000 | Loss: 0.00011729
Iteration 90/1000 | Loss: 0.00020906
Iteration 91/1000 | Loss: 0.00026620
Iteration 92/1000 | Loss: 0.00009746
Iteration 93/1000 | Loss: 0.00007319
Iteration 94/1000 | Loss: 0.00022013
Iteration 95/1000 | Loss: 0.00026404
Iteration 96/1000 | Loss: 0.00039926
Iteration 97/1000 | Loss: 0.00025609
Iteration 98/1000 | Loss: 0.00026294
Iteration 99/1000 | Loss: 0.00005043
Iteration 100/1000 | Loss: 0.00020047
Iteration 101/1000 | Loss: 0.00007134
Iteration 102/1000 | Loss: 0.00004485
Iteration 103/1000 | Loss: 0.00003377
Iteration 104/1000 | Loss: 0.00007531
Iteration 105/1000 | Loss: 0.00012168
Iteration 106/1000 | Loss: 0.00010241
Iteration 107/1000 | Loss: 0.00010414
Iteration 108/1000 | Loss: 0.00031019
Iteration 109/1000 | Loss: 0.00048277
Iteration 110/1000 | Loss: 0.00020543
Iteration 111/1000 | Loss: 0.00048185
Iteration 112/1000 | Loss: 0.00045273
Iteration 113/1000 | Loss: 0.00019622
Iteration 114/1000 | Loss: 0.00015287
Iteration 115/1000 | Loss: 0.00010579
Iteration 116/1000 | Loss: 0.00013855
Iteration 117/1000 | Loss: 0.00009470
Iteration 118/1000 | Loss: 0.00014975
Iteration 119/1000 | Loss: 0.00010139
Iteration 120/1000 | Loss: 0.00003164
Iteration 121/1000 | Loss: 0.00017813
Iteration 122/1000 | Loss: 0.00003705
Iteration 123/1000 | Loss: 0.00010601
Iteration 124/1000 | Loss: 0.00015149
Iteration 125/1000 | Loss: 0.00005333
Iteration 126/1000 | Loss: 0.00002877
Iteration 127/1000 | Loss: 0.00002697
Iteration 128/1000 | Loss: 0.00025035
Iteration 129/1000 | Loss: 0.00019650
Iteration 130/1000 | Loss: 0.00051086
Iteration 131/1000 | Loss: 0.00046145
Iteration 132/1000 | Loss: 0.00026017
Iteration 133/1000 | Loss: 0.00008696
Iteration 134/1000 | Loss: 0.00033238
Iteration 135/1000 | Loss: 0.00027149
Iteration 136/1000 | Loss: 0.00016677
Iteration 137/1000 | Loss: 0.00014508
Iteration 138/1000 | Loss: 0.00006862
Iteration 139/1000 | Loss: 0.00005170
Iteration 140/1000 | Loss: 0.00017814
Iteration 141/1000 | Loss: 0.00020591
Iteration 142/1000 | Loss: 0.00012556
Iteration 143/1000 | Loss: 0.00013390
Iteration 144/1000 | Loss: 0.00007707
Iteration 145/1000 | Loss: 0.00011475
Iteration 146/1000 | Loss: 0.00043601
Iteration 147/1000 | Loss: 0.00036450
Iteration 148/1000 | Loss: 0.00011648
Iteration 149/1000 | Loss: 0.00047202
Iteration 150/1000 | Loss: 0.00019209
Iteration 151/1000 | Loss: 0.00024281
Iteration 152/1000 | Loss: 0.00011542
Iteration 153/1000 | Loss: 0.00003601
Iteration 154/1000 | Loss: 0.00010207
Iteration 155/1000 | Loss: 0.00010458
Iteration 156/1000 | Loss: 0.00017186
Iteration 157/1000 | Loss: 0.00012158
Iteration 158/1000 | Loss: 0.00023026
Iteration 159/1000 | Loss: 0.00021735
Iteration 160/1000 | Loss: 0.00023215
Iteration 161/1000 | Loss: 0.00022471
Iteration 162/1000 | Loss: 0.00012676
Iteration 163/1000 | Loss: 0.00018231
Iteration 164/1000 | Loss: 0.00013144
Iteration 165/1000 | Loss: 0.00025608
Iteration 166/1000 | Loss: 0.00032705
Iteration 167/1000 | Loss: 0.00040941
Iteration 168/1000 | Loss: 0.00025450
Iteration 169/1000 | Loss: 0.00009995
Iteration 170/1000 | Loss: 0.00012337
Iteration 171/1000 | Loss: 0.00012946
Iteration 172/1000 | Loss: 0.00005897
Iteration 173/1000 | Loss: 0.00003979
Iteration 174/1000 | Loss: 0.00005035
Iteration 175/1000 | Loss: 0.00003950
Iteration 176/1000 | Loss: 0.00014695
Iteration 177/1000 | Loss: 0.00005990
Iteration 178/1000 | Loss: 0.00018850
Iteration 179/1000 | Loss: 0.00016024
Iteration 180/1000 | Loss: 0.00009411
Iteration 181/1000 | Loss: 0.00003175
Iteration 182/1000 | Loss: 0.00002937
Iteration 183/1000 | Loss: 0.00008336
Iteration 184/1000 | Loss: 0.00002642
Iteration 185/1000 | Loss: 0.00002429
Iteration 186/1000 | Loss: 0.00011471
Iteration 187/1000 | Loss: 0.00028484
Iteration 188/1000 | Loss: 0.00014890
Iteration 189/1000 | Loss: 0.00002524
Iteration 190/1000 | Loss: 0.00010868
Iteration 191/1000 | Loss: 0.00024644
Iteration 192/1000 | Loss: 0.00034310
Iteration 193/1000 | Loss: 0.00010359
Iteration 194/1000 | Loss: 0.00003657
Iteration 195/1000 | Loss: 0.00010761
Iteration 196/1000 | Loss: 0.00008661
Iteration 197/1000 | Loss: 0.00011815
Iteration 198/1000 | Loss: 0.00017924
Iteration 199/1000 | Loss: 0.00007390
Iteration 200/1000 | Loss: 0.00007180
Iteration 201/1000 | Loss: 0.00012949
Iteration 202/1000 | Loss: 0.00008331
Iteration 203/1000 | Loss: 0.00013144
Iteration 204/1000 | Loss: 0.00002952
Iteration 205/1000 | Loss: 0.00002679
Iteration 206/1000 | Loss: 0.00002505
Iteration 207/1000 | Loss: 0.00002395
Iteration 208/1000 | Loss: 0.00002325
Iteration 209/1000 | Loss: 0.00002273
Iteration 210/1000 | Loss: 0.00017804
Iteration 211/1000 | Loss: 0.00011190
Iteration 212/1000 | Loss: 0.00004929
Iteration 213/1000 | Loss: 0.00009360
Iteration 214/1000 | Loss: 0.00007361
Iteration 215/1000 | Loss: 0.00005216
Iteration 216/1000 | Loss: 0.00017477
Iteration 217/1000 | Loss: 0.00023448
Iteration 218/1000 | Loss: 0.00005045
Iteration 219/1000 | Loss: 0.00003150
Iteration 220/1000 | Loss: 0.00016734
Iteration 221/1000 | Loss: 0.00009506
Iteration 222/1000 | Loss: 0.00013921
Iteration 223/1000 | Loss: 0.00007269
Iteration 224/1000 | Loss: 0.00003429
Iteration 225/1000 | Loss: 0.00002485
Iteration 226/1000 | Loss: 0.00002417
Iteration 227/1000 | Loss: 0.00014749
Iteration 228/1000 | Loss: 0.00017608
Iteration 229/1000 | Loss: 0.00010722
Iteration 230/1000 | Loss: 0.00016371
Iteration 231/1000 | Loss: 0.00017721
Iteration 232/1000 | Loss: 0.00012673
Iteration 233/1000 | Loss: 0.00003802
Iteration 234/1000 | Loss: 0.00002916
Iteration 235/1000 | Loss: 0.00002556
Iteration 236/1000 | Loss: 0.00004622
Iteration 237/1000 | Loss: 0.00021604
Iteration 238/1000 | Loss: 0.00003979
Iteration 239/1000 | Loss: 0.00002530
Iteration 240/1000 | Loss: 0.00002216
Iteration 241/1000 | Loss: 0.00013967
Iteration 242/1000 | Loss: 0.00010719
Iteration 243/1000 | Loss: 0.00013292
Iteration 244/1000 | Loss: 0.00009091
Iteration 245/1000 | Loss: 0.00011473
Iteration 246/1000 | Loss: 0.00011136
Iteration 247/1000 | Loss: 0.00011312
Iteration 248/1000 | Loss: 0.00013609
Iteration 249/1000 | Loss: 0.00005104
Iteration 250/1000 | Loss: 0.00013730
Iteration 251/1000 | Loss: 0.00007770
Iteration 252/1000 | Loss: 0.00013922
Iteration 253/1000 | Loss: 0.00008849
Iteration 254/1000 | Loss: 0.00002000
Iteration 255/1000 | Loss: 0.00001958
Iteration 256/1000 | Loss: 0.00001926
Iteration 257/1000 | Loss: 0.00002835
Iteration 258/1000 | Loss: 0.00002086
Iteration 259/1000 | Loss: 0.00002001
Iteration 260/1000 | Loss: 0.00012424
Iteration 261/1000 | Loss: 0.00007455
Iteration 262/1000 | Loss: 0.00002038
Iteration 263/1000 | Loss: 0.00002786
Iteration 264/1000 | Loss: 0.00017588
Iteration 265/1000 | Loss: 0.00008043
Iteration 266/1000 | Loss: 0.00002316
Iteration 267/1000 | Loss: 0.00002800
Iteration 268/1000 | Loss: 0.00002146
Iteration 269/1000 | Loss: 0.00001982
Iteration 270/1000 | Loss: 0.00001979
Iteration 271/1000 | Loss: 0.00002164
Iteration 272/1000 | Loss: 0.00001972
Iteration 273/1000 | Loss: 0.00001970
Iteration 274/1000 | Loss: 0.00001969
Iteration 275/1000 | Loss: 0.00002124
Iteration 276/1000 | Loss: 0.00001959
Iteration 277/1000 | Loss: 0.00001934
Iteration 278/1000 | Loss: 0.00012249
Iteration 279/1000 | Loss: 0.00022644
Iteration 280/1000 | Loss: 0.00013232
Iteration 281/1000 | Loss: 0.00003245
Iteration 282/1000 | Loss: 0.00003320
Iteration 283/1000 | Loss: 0.00015764
Iteration 284/1000 | Loss: 0.00008999
Iteration 285/1000 | Loss: 0.00003362
Iteration 286/1000 | Loss: 0.00002633
Iteration 287/1000 | Loss: 0.00008815
Iteration 288/1000 | Loss: 0.00003032
Iteration 289/1000 | Loss: 0.00007117
Iteration 290/1000 | Loss: 0.00013266
Iteration 291/1000 | Loss: 0.00013812
Iteration 292/1000 | Loss: 0.00002589
Iteration 293/1000 | Loss: 0.00002010
Iteration 294/1000 | Loss: 0.00002919
Iteration 295/1000 | Loss: 0.00002803
Iteration 296/1000 | Loss: 0.00001899
Iteration 297/1000 | Loss: 0.00001962
Iteration 298/1000 | Loss: 0.00001885
Iteration 299/1000 | Loss: 0.00001824
Iteration 300/1000 | Loss: 0.00001926
Iteration 301/1000 | Loss: 0.00001810
Iteration 302/1000 | Loss: 0.00001809
Iteration 303/1000 | Loss: 0.00001807
Iteration 304/1000 | Loss: 0.00001853
Iteration 305/1000 | Loss: 0.00001805
Iteration 306/1000 | Loss: 0.00001805
Iteration 307/1000 | Loss: 0.00001805
Iteration 308/1000 | Loss: 0.00001804
Iteration 309/1000 | Loss: 0.00001804
Iteration 310/1000 | Loss: 0.00001804
Iteration 311/1000 | Loss: 0.00001804
Iteration 312/1000 | Loss: 0.00001804
Iteration 313/1000 | Loss: 0.00001804
Iteration 314/1000 | Loss: 0.00001804
Iteration 315/1000 | Loss: 0.00001804
Iteration 316/1000 | Loss: 0.00001804
Iteration 317/1000 | Loss: 0.00001804
Iteration 318/1000 | Loss: 0.00001804
Iteration 319/1000 | Loss: 0.00001804
Iteration 320/1000 | Loss: 0.00001803
Iteration 321/1000 | Loss: 0.00001803
Iteration 322/1000 | Loss: 0.00001803
Iteration 323/1000 | Loss: 0.00001803
Iteration 324/1000 | Loss: 0.00001803
Iteration 325/1000 | Loss: 0.00001803
Iteration 326/1000 | Loss: 0.00001803
Iteration 327/1000 | Loss: 0.00001803
Iteration 328/1000 | Loss: 0.00001801
Iteration 329/1000 | Loss: 0.00001801
Iteration 330/1000 | Loss: 0.00001801
Iteration 331/1000 | Loss: 0.00001801
Iteration 332/1000 | Loss: 0.00001800
Iteration 333/1000 | Loss: 0.00001800
Iteration 334/1000 | Loss: 0.00001800
Iteration 335/1000 | Loss: 0.00001800
Iteration 336/1000 | Loss: 0.00001800
Iteration 337/1000 | Loss: 0.00001799
Iteration 338/1000 | Loss: 0.00001798
Iteration 339/1000 | Loss: 0.00001798
Iteration 340/1000 | Loss: 0.00001798
Iteration 341/1000 | Loss: 0.00001798
Iteration 342/1000 | Loss: 0.00001798
Iteration 343/1000 | Loss: 0.00001798
Iteration 344/1000 | Loss: 0.00001797
Iteration 345/1000 | Loss: 0.00001797
Iteration 346/1000 | Loss: 0.00001797
Iteration 347/1000 | Loss: 0.00001797
Iteration 348/1000 | Loss: 0.00001797
Iteration 349/1000 | Loss: 0.00001797
Iteration 350/1000 | Loss: 0.00001797
Iteration 351/1000 | Loss: 0.00001797
Iteration 352/1000 | Loss: 0.00001797
Iteration 353/1000 | Loss: 0.00001797
Iteration 354/1000 | Loss: 0.00001797
Iteration 355/1000 | Loss: 0.00001797
Iteration 356/1000 | Loss: 0.00001796
Iteration 357/1000 | Loss: 0.00001796
Iteration 358/1000 | Loss: 0.00001796
Iteration 359/1000 | Loss: 0.00001796
Iteration 360/1000 | Loss: 0.00001796
Iteration 361/1000 | Loss: 0.00001796
Iteration 362/1000 | Loss: 0.00001796
Iteration 363/1000 | Loss: 0.00001795
Iteration 364/1000 | Loss: 0.00001795
Iteration 365/1000 | Loss: 0.00001795
Iteration 366/1000 | Loss: 0.00001794
Iteration 367/1000 | Loss: 0.00001794
Iteration 368/1000 | Loss: 0.00001794
Iteration 369/1000 | Loss: 0.00001794
Iteration 370/1000 | Loss: 0.00001793
Iteration 371/1000 | Loss: 0.00001793
Iteration 372/1000 | Loss: 0.00001793
Iteration 373/1000 | Loss: 0.00001793
Iteration 374/1000 | Loss: 0.00001793
Iteration 375/1000 | Loss: 0.00001793
Iteration 376/1000 | Loss: 0.00001793
Iteration 377/1000 | Loss: 0.00001793
Iteration 378/1000 | Loss: 0.00001793
Iteration 379/1000 | Loss: 0.00001792
Iteration 380/1000 | Loss: 0.00001792
Iteration 381/1000 | Loss: 0.00001792
Iteration 382/1000 | Loss: 0.00001792
Iteration 383/1000 | Loss: 0.00001792
Iteration 384/1000 | Loss: 0.00001792
Iteration 385/1000 | Loss: 0.00001792
Iteration 386/1000 | Loss: 0.00001792
Iteration 387/1000 | Loss: 0.00001792
Iteration 388/1000 | Loss: 0.00001792
Iteration 389/1000 | Loss: 0.00001792
Iteration 390/1000 | Loss: 0.00001792
Iteration 391/1000 | Loss: 0.00001792
Iteration 392/1000 | Loss: 0.00001792
Iteration 393/1000 | Loss: 0.00001792
Iteration 394/1000 | Loss: 0.00001792
Iteration 395/1000 | Loss: 0.00001792
Iteration 396/1000 | Loss: 0.00001792
Iteration 397/1000 | Loss: 0.00001792
Iteration 398/1000 | Loss: 0.00001792
Iteration 399/1000 | Loss: 0.00001792
Iteration 400/1000 | Loss: 0.00001792
Iteration 401/1000 | Loss: 0.00001792
Iteration 402/1000 | Loss: 0.00001792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 402. Stopping optimization.
Last 5 losses: [1.7917007426149212e-05, 1.7917007426149212e-05, 1.7917007426149212e-05, 1.7917007426149212e-05, 1.7917007426149212e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7917007426149212e-05

Optimization complete. Final v2v error: 3.3007476329803467 mm

Highest mean error: 7.0416340827941895 mm for frame 170

Lowest mean error: 2.7870850563049316 mm for frame 162

Saving results

Total time: 528.9339573383331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00338199
Iteration 2/25 | Loss: 0.00136065
Iteration 3/25 | Loss: 0.00125270
Iteration 4/25 | Loss: 0.00123294
Iteration 5/25 | Loss: 0.00122658
Iteration 6/25 | Loss: 0.00122467
Iteration 7/25 | Loss: 0.00122467
Iteration 8/25 | Loss: 0.00122467
Iteration 9/25 | Loss: 0.00122467
Iteration 10/25 | Loss: 0.00122467
Iteration 11/25 | Loss: 0.00122467
Iteration 12/25 | Loss: 0.00122467
Iteration 13/25 | Loss: 0.00122467
Iteration 14/25 | Loss: 0.00122467
Iteration 15/25 | Loss: 0.00122467
Iteration 16/25 | Loss: 0.00122467
Iteration 17/25 | Loss: 0.00122467
Iteration 18/25 | Loss: 0.00122467
Iteration 19/25 | Loss: 0.00122467
Iteration 20/25 | Loss: 0.00122467
Iteration 21/25 | Loss: 0.00122467
Iteration 22/25 | Loss: 0.00122467
Iteration 23/25 | Loss: 0.00122467
Iteration 24/25 | Loss: 0.00122467
Iteration 25/25 | Loss: 0.00122467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41209376
Iteration 2/25 | Loss: 0.00077768
Iteration 3/25 | Loss: 0.00077768
Iteration 4/25 | Loss: 0.00077767
Iteration 5/25 | Loss: 0.00077767
Iteration 6/25 | Loss: 0.00077767
Iteration 7/25 | Loss: 0.00077767
Iteration 8/25 | Loss: 0.00077767
Iteration 9/25 | Loss: 0.00077767
Iteration 10/25 | Loss: 0.00077767
Iteration 11/25 | Loss: 0.00077767
Iteration 12/25 | Loss: 0.00077767
Iteration 13/25 | Loss: 0.00077767
Iteration 14/25 | Loss: 0.00077767
Iteration 15/25 | Loss: 0.00077767
Iteration 16/25 | Loss: 0.00077767
Iteration 17/25 | Loss: 0.00077767
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007776726270094514, 0.0007776726270094514, 0.0007776726270094514, 0.0007776726270094514, 0.0007776726270094514]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007776726270094514

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077767
Iteration 2/1000 | Loss: 0.00003525
Iteration 3/1000 | Loss: 0.00002321
Iteration 4/1000 | Loss: 0.00002070
Iteration 5/1000 | Loss: 0.00001922
Iteration 6/1000 | Loss: 0.00001784
Iteration 7/1000 | Loss: 0.00001703
Iteration 8/1000 | Loss: 0.00001657
Iteration 9/1000 | Loss: 0.00001612
Iteration 10/1000 | Loss: 0.00001571
Iteration 11/1000 | Loss: 0.00001553
Iteration 12/1000 | Loss: 0.00001536
Iteration 13/1000 | Loss: 0.00001527
Iteration 14/1000 | Loss: 0.00001519
Iteration 15/1000 | Loss: 0.00001519
Iteration 16/1000 | Loss: 0.00001518
Iteration 17/1000 | Loss: 0.00001517
Iteration 18/1000 | Loss: 0.00001516
Iteration 19/1000 | Loss: 0.00001515
Iteration 20/1000 | Loss: 0.00001515
Iteration 21/1000 | Loss: 0.00001514
Iteration 22/1000 | Loss: 0.00001513
Iteration 23/1000 | Loss: 0.00001513
Iteration 24/1000 | Loss: 0.00001513
Iteration 25/1000 | Loss: 0.00001511
Iteration 26/1000 | Loss: 0.00001510
Iteration 27/1000 | Loss: 0.00001509
Iteration 28/1000 | Loss: 0.00001509
Iteration 29/1000 | Loss: 0.00001509
Iteration 30/1000 | Loss: 0.00001508
Iteration 31/1000 | Loss: 0.00001504
Iteration 32/1000 | Loss: 0.00001504
Iteration 33/1000 | Loss: 0.00001503
Iteration 34/1000 | Loss: 0.00001503
Iteration 35/1000 | Loss: 0.00001502
Iteration 36/1000 | Loss: 0.00001502
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001501
Iteration 39/1000 | Loss: 0.00001501
Iteration 40/1000 | Loss: 0.00001501
Iteration 41/1000 | Loss: 0.00001500
Iteration 42/1000 | Loss: 0.00001500
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001499
Iteration 45/1000 | Loss: 0.00001499
Iteration 46/1000 | Loss: 0.00001498
Iteration 47/1000 | Loss: 0.00001498
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001495
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001492
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001491
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001491
Iteration 58/1000 | Loss: 0.00001491
Iteration 59/1000 | Loss: 0.00001491
Iteration 60/1000 | Loss: 0.00001490
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001487
Iteration 64/1000 | Loss: 0.00001487
Iteration 65/1000 | Loss: 0.00001487
Iteration 66/1000 | Loss: 0.00001486
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001485
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001485
Iteration 73/1000 | Loss: 0.00001485
Iteration 74/1000 | Loss: 0.00001484
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001483
Iteration 78/1000 | Loss: 0.00001483
Iteration 79/1000 | Loss: 0.00001483
Iteration 80/1000 | Loss: 0.00001483
Iteration 81/1000 | Loss: 0.00001482
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001482
Iteration 84/1000 | Loss: 0.00001482
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001481
Iteration 87/1000 | Loss: 0.00001481
Iteration 88/1000 | Loss: 0.00001481
Iteration 89/1000 | Loss: 0.00001480
Iteration 90/1000 | Loss: 0.00001480
Iteration 91/1000 | Loss: 0.00001480
Iteration 92/1000 | Loss: 0.00001480
Iteration 93/1000 | Loss: 0.00001480
Iteration 94/1000 | Loss: 0.00001479
Iteration 95/1000 | Loss: 0.00001479
Iteration 96/1000 | Loss: 0.00001479
Iteration 97/1000 | Loss: 0.00001479
Iteration 98/1000 | Loss: 0.00001478
Iteration 99/1000 | Loss: 0.00001478
Iteration 100/1000 | Loss: 0.00001478
Iteration 101/1000 | Loss: 0.00001478
Iteration 102/1000 | Loss: 0.00001477
Iteration 103/1000 | Loss: 0.00001477
Iteration 104/1000 | Loss: 0.00001477
Iteration 105/1000 | Loss: 0.00001477
Iteration 106/1000 | Loss: 0.00001477
Iteration 107/1000 | Loss: 0.00001477
Iteration 108/1000 | Loss: 0.00001477
Iteration 109/1000 | Loss: 0.00001477
Iteration 110/1000 | Loss: 0.00001477
Iteration 111/1000 | Loss: 0.00001477
Iteration 112/1000 | Loss: 0.00001477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [1.4768163964617997e-05, 1.4768163964617997e-05, 1.4768163964617997e-05, 1.4768163964617997e-05, 1.4768163964617997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4768163964617997e-05

Optimization complete. Final v2v error: 3.258589506149292 mm

Highest mean error: 3.8676609992980957 mm for frame 95

Lowest mean error: 2.8804612159729004 mm for frame 264

Saving results

Total time: 43.423946380615234
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802204
Iteration 2/25 | Loss: 0.00164181
Iteration 3/25 | Loss: 0.00136949
Iteration 4/25 | Loss: 0.00133895
Iteration 5/25 | Loss: 0.00132984
Iteration 6/25 | Loss: 0.00132760
Iteration 7/25 | Loss: 0.00132760
Iteration 8/25 | Loss: 0.00132760
Iteration 9/25 | Loss: 0.00132760
Iteration 10/25 | Loss: 0.00132760
Iteration 11/25 | Loss: 0.00132760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001327598001807928, 0.001327598001807928, 0.001327598001807928, 0.001327598001807928, 0.001327598001807928]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001327598001807928

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.23799086
Iteration 2/25 | Loss: 0.00084827
Iteration 3/25 | Loss: 0.00084827
Iteration 4/25 | Loss: 0.00084827
Iteration 5/25 | Loss: 0.00084826
Iteration 6/25 | Loss: 0.00084826
Iteration 7/25 | Loss: 0.00084826
Iteration 8/25 | Loss: 0.00084826
Iteration 9/25 | Loss: 0.00084826
Iteration 10/25 | Loss: 0.00084826
Iteration 11/25 | Loss: 0.00084826
Iteration 12/25 | Loss: 0.00084826
Iteration 13/25 | Loss: 0.00084826
Iteration 14/25 | Loss: 0.00084826
Iteration 15/25 | Loss: 0.00084826
Iteration 16/25 | Loss: 0.00084826
Iteration 17/25 | Loss: 0.00084826
Iteration 18/25 | Loss: 0.00084826
Iteration 19/25 | Loss: 0.00084826
Iteration 20/25 | Loss: 0.00084826
Iteration 21/25 | Loss: 0.00084826
Iteration 22/25 | Loss: 0.00084826
Iteration 23/25 | Loss: 0.00084826
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008482642006129026, 0.0008482642006129026, 0.0008482642006129026, 0.0008482642006129026, 0.0008482642006129026]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008482642006129026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00084826
Iteration 2/1000 | Loss: 0.00006074
Iteration 3/1000 | Loss: 0.00003902
Iteration 4/1000 | Loss: 0.00003419
Iteration 5/1000 | Loss: 0.00003240
Iteration 6/1000 | Loss: 0.00003083
Iteration 7/1000 | Loss: 0.00002963
Iteration 8/1000 | Loss: 0.00002865
Iteration 9/1000 | Loss: 0.00002808
Iteration 10/1000 | Loss: 0.00002759
Iteration 11/1000 | Loss: 0.00002712
Iteration 12/1000 | Loss: 0.00002684
Iteration 13/1000 | Loss: 0.00002659
Iteration 14/1000 | Loss: 0.00002640
Iteration 15/1000 | Loss: 0.00002633
Iteration 16/1000 | Loss: 0.00002630
Iteration 17/1000 | Loss: 0.00002629
Iteration 18/1000 | Loss: 0.00002622
Iteration 19/1000 | Loss: 0.00002615
Iteration 20/1000 | Loss: 0.00002614
Iteration 21/1000 | Loss: 0.00002614
Iteration 22/1000 | Loss: 0.00002613
Iteration 23/1000 | Loss: 0.00002612
Iteration 24/1000 | Loss: 0.00002611
Iteration 25/1000 | Loss: 0.00002611
Iteration 26/1000 | Loss: 0.00002607
Iteration 27/1000 | Loss: 0.00002605
Iteration 28/1000 | Loss: 0.00002604
Iteration 29/1000 | Loss: 0.00002604
Iteration 30/1000 | Loss: 0.00002603
Iteration 31/1000 | Loss: 0.00002603
Iteration 32/1000 | Loss: 0.00002602
Iteration 33/1000 | Loss: 0.00002602
Iteration 34/1000 | Loss: 0.00002601
Iteration 35/1000 | Loss: 0.00002601
Iteration 36/1000 | Loss: 0.00002601
Iteration 37/1000 | Loss: 0.00002600
Iteration 38/1000 | Loss: 0.00002600
Iteration 39/1000 | Loss: 0.00002600
Iteration 40/1000 | Loss: 0.00002599
Iteration 41/1000 | Loss: 0.00002599
Iteration 42/1000 | Loss: 0.00002599
Iteration 43/1000 | Loss: 0.00002598
Iteration 44/1000 | Loss: 0.00002598
Iteration 45/1000 | Loss: 0.00002598
Iteration 46/1000 | Loss: 0.00002597
Iteration 47/1000 | Loss: 0.00002597
Iteration 48/1000 | Loss: 0.00002596
Iteration 49/1000 | Loss: 0.00002596
Iteration 50/1000 | Loss: 0.00002596
Iteration 51/1000 | Loss: 0.00002595
Iteration 52/1000 | Loss: 0.00002595
Iteration 53/1000 | Loss: 0.00002595
Iteration 54/1000 | Loss: 0.00002595
Iteration 55/1000 | Loss: 0.00002594
Iteration 56/1000 | Loss: 0.00002594
Iteration 57/1000 | Loss: 0.00002594
Iteration 58/1000 | Loss: 0.00002593
Iteration 59/1000 | Loss: 0.00002593
Iteration 60/1000 | Loss: 0.00002593
Iteration 61/1000 | Loss: 0.00002593
Iteration 62/1000 | Loss: 0.00002593
Iteration 63/1000 | Loss: 0.00002593
Iteration 64/1000 | Loss: 0.00002592
Iteration 65/1000 | Loss: 0.00002592
Iteration 66/1000 | Loss: 0.00002592
Iteration 67/1000 | Loss: 0.00002592
Iteration 68/1000 | Loss: 0.00002592
Iteration 69/1000 | Loss: 0.00002591
Iteration 70/1000 | Loss: 0.00002591
Iteration 71/1000 | Loss: 0.00002591
Iteration 72/1000 | Loss: 0.00002591
Iteration 73/1000 | Loss: 0.00002591
Iteration 74/1000 | Loss: 0.00002591
Iteration 75/1000 | Loss: 0.00002590
Iteration 76/1000 | Loss: 0.00002590
Iteration 77/1000 | Loss: 0.00002590
Iteration 78/1000 | Loss: 0.00002590
Iteration 79/1000 | Loss: 0.00002590
Iteration 80/1000 | Loss: 0.00002590
Iteration 81/1000 | Loss: 0.00002590
Iteration 82/1000 | Loss: 0.00002590
Iteration 83/1000 | Loss: 0.00002590
Iteration 84/1000 | Loss: 0.00002590
Iteration 85/1000 | Loss: 0.00002590
Iteration 86/1000 | Loss: 0.00002589
Iteration 87/1000 | Loss: 0.00002589
Iteration 88/1000 | Loss: 0.00002589
Iteration 89/1000 | Loss: 0.00002589
Iteration 90/1000 | Loss: 0.00002589
Iteration 91/1000 | Loss: 0.00002589
Iteration 92/1000 | Loss: 0.00002589
Iteration 93/1000 | Loss: 0.00002589
Iteration 94/1000 | Loss: 0.00002588
Iteration 95/1000 | Loss: 0.00002588
Iteration 96/1000 | Loss: 0.00002588
Iteration 97/1000 | Loss: 0.00002588
Iteration 98/1000 | Loss: 0.00002588
Iteration 99/1000 | Loss: 0.00002587
Iteration 100/1000 | Loss: 0.00002587
Iteration 101/1000 | Loss: 0.00002587
Iteration 102/1000 | Loss: 0.00002587
Iteration 103/1000 | Loss: 0.00002587
Iteration 104/1000 | Loss: 0.00002587
Iteration 105/1000 | Loss: 0.00002587
Iteration 106/1000 | Loss: 0.00002587
Iteration 107/1000 | Loss: 0.00002587
Iteration 108/1000 | Loss: 0.00002587
Iteration 109/1000 | Loss: 0.00002587
Iteration 110/1000 | Loss: 0.00002587
Iteration 111/1000 | Loss: 0.00002587
Iteration 112/1000 | Loss: 0.00002586
Iteration 113/1000 | Loss: 0.00002586
Iteration 114/1000 | Loss: 0.00002586
Iteration 115/1000 | Loss: 0.00002586
Iteration 116/1000 | Loss: 0.00002586
Iteration 117/1000 | Loss: 0.00002586
Iteration 118/1000 | Loss: 0.00002586
Iteration 119/1000 | Loss: 0.00002586
Iteration 120/1000 | Loss: 0.00002586
Iteration 121/1000 | Loss: 0.00002586
Iteration 122/1000 | Loss: 0.00002586
Iteration 123/1000 | Loss: 0.00002586
Iteration 124/1000 | Loss: 0.00002586
Iteration 125/1000 | Loss: 0.00002586
Iteration 126/1000 | Loss: 0.00002586
Iteration 127/1000 | Loss: 0.00002586
Iteration 128/1000 | Loss: 0.00002586
Iteration 129/1000 | Loss: 0.00002586
Iteration 130/1000 | Loss: 0.00002586
Iteration 131/1000 | Loss: 0.00002586
Iteration 132/1000 | Loss: 0.00002586
Iteration 133/1000 | Loss: 0.00002586
Iteration 134/1000 | Loss: 0.00002586
Iteration 135/1000 | Loss: 0.00002586
Iteration 136/1000 | Loss: 0.00002586
Iteration 137/1000 | Loss: 0.00002586
Iteration 138/1000 | Loss: 0.00002586
Iteration 139/1000 | Loss: 0.00002586
Iteration 140/1000 | Loss: 0.00002586
Iteration 141/1000 | Loss: 0.00002586
Iteration 142/1000 | Loss: 0.00002586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.5861108952085488e-05, 2.5861108952085488e-05, 2.5861108952085488e-05, 2.5861108952085488e-05, 2.5861108952085488e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5861108952085488e-05

Optimization complete. Final v2v error: 4.214554786682129 mm

Highest mean error: 4.991734981536865 mm for frame 182

Lowest mean error: 3.4296834468841553 mm for frame 234

Saving results

Total time: 45.55678868293762
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832399
Iteration 2/25 | Loss: 0.00138768
Iteration 3/25 | Loss: 0.00131667
Iteration 4/25 | Loss: 0.00129675
Iteration 5/25 | Loss: 0.00129056
Iteration 6/25 | Loss: 0.00129019
Iteration 7/25 | Loss: 0.00129019
Iteration 8/25 | Loss: 0.00129019
Iteration 9/25 | Loss: 0.00129019
Iteration 10/25 | Loss: 0.00129019
Iteration 11/25 | Loss: 0.00129019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001290186308324337, 0.001290186308324337, 0.001290186308324337, 0.001290186308324337, 0.001290186308324337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001290186308324337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40884733
Iteration 2/25 | Loss: 0.00096925
Iteration 3/25 | Loss: 0.00096925
Iteration 4/25 | Loss: 0.00096925
Iteration 5/25 | Loss: 0.00096925
Iteration 6/25 | Loss: 0.00096925
Iteration 7/25 | Loss: 0.00096925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 7. Stopping optimization.
Last 5 losses: [0.0009692518506199121, 0.0009692518506199121, 0.0009692518506199121, 0.0009692518506199121, 0.0009692518506199121]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009692518506199121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096925
Iteration 2/1000 | Loss: 0.00002866
Iteration 3/1000 | Loss: 0.00002395
Iteration 4/1000 | Loss: 0.00002274
Iteration 5/1000 | Loss: 0.00002209
Iteration 6/1000 | Loss: 0.00002176
Iteration 7/1000 | Loss: 0.00002140
Iteration 8/1000 | Loss: 0.00002110
Iteration 9/1000 | Loss: 0.00002097
Iteration 10/1000 | Loss: 0.00002083
Iteration 11/1000 | Loss: 0.00002081
Iteration 12/1000 | Loss: 0.00002075
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002059
Iteration 15/1000 | Loss: 0.00002058
Iteration 16/1000 | Loss: 0.00002058
Iteration 17/1000 | Loss: 0.00002053
Iteration 18/1000 | Loss: 0.00002050
Iteration 19/1000 | Loss: 0.00002049
Iteration 20/1000 | Loss: 0.00002048
Iteration 21/1000 | Loss: 0.00002047
Iteration 22/1000 | Loss: 0.00002047
Iteration 23/1000 | Loss: 0.00002047
Iteration 24/1000 | Loss: 0.00002047
Iteration 25/1000 | Loss: 0.00002047
Iteration 26/1000 | Loss: 0.00002047
Iteration 27/1000 | Loss: 0.00002047
Iteration 28/1000 | Loss: 0.00002045
Iteration 29/1000 | Loss: 0.00002043
Iteration 30/1000 | Loss: 0.00002043
Iteration 31/1000 | Loss: 0.00002043
Iteration 32/1000 | Loss: 0.00002043
Iteration 33/1000 | Loss: 0.00002043
Iteration 34/1000 | Loss: 0.00002042
Iteration 35/1000 | Loss: 0.00002042
Iteration 36/1000 | Loss: 0.00002042
Iteration 37/1000 | Loss: 0.00002040
Iteration 38/1000 | Loss: 0.00002039
Iteration 39/1000 | Loss: 0.00002038
Iteration 40/1000 | Loss: 0.00002038
Iteration 41/1000 | Loss: 0.00002038
Iteration 42/1000 | Loss: 0.00002038
Iteration 43/1000 | Loss: 0.00002038
Iteration 44/1000 | Loss: 0.00002038
Iteration 45/1000 | Loss: 0.00002038
Iteration 46/1000 | Loss: 0.00002038
Iteration 47/1000 | Loss: 0.00002038
Iteration 48/1000 | Loss: 0.00002038
Iteration 49/1000 | Loss: 0.00002038
Iteration 50/1000 | Loss: 0.00002037
Iteration 51/1000 | Loss: 0.00002036
Iteration 52/1000 | Loss: 0.00002036
Iteration 53/1000 | Loss: 0.00002036
Iteration 54/1000 | Loss: 0.00002035
Iteration 55/1000 | Loss: 0.00002035
Iteration 56/1000 | Loss: 0.00002035
Iteration 57/1000 | Loss: 0.00002035
Iteration 58/1000 | Loss: 0.00002034
Iteration 59/1000 | Loss: 0.00002034
Iteration 60/1000 | Loss: 0.00002034
Iteration 61/1000 | Loss: 0.00002033
Iteration 62/1000 | Loss: 0.00002033
Iteration 63/1000 | Loss: 0.00002033
Iteration 64/1000 | Loss: 0.00002033
Iteration 65/1000 | Loss: 0.00002033
Iteration 66/1000 | Loss: 0.00002032
Iteration 67/1000 | Loss: 0.00002032
Iteration 68/1000 | Loss: 0.00002032
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002030
Iteration 72/1000 | Loss: 0.00002030
Iteration 73/1000 | Loss: 0.00002030
Iteration 74/1000 | Loss: 0.00002029
Iteration 75/1000 | Loss: 0.00002029
Iteration 76/1000 | Loss: 0.00002028
Iteration 77/1000 | Loss: 0.00002028
Iteration 78/1000 | Loss: 0.00002028
Iteration 79/1000 | Loss: 0.00002028
Iteration 80/1000 | Loss: 0.00002028
Iteration 81/1000 | Loss: 0.00002028
Iteration 82/1000 | Loss: 0.00002028
Iteration 83/1000 | Loss: 0.00002028
Iteration 84/1000 | Loss: 0.00002027
Iteration 85/1000 | Loss: 0.00002027
Iteration 86/1000 | Loss: 0.00002026
Iteration 87/1000 | Loss: 0.00002026
Iteration 88/1000 | Loss: 0.00002026
Iteration 89/1000 | Loss: 0.00002026
Iteration 90/1000 | Loss: 0.00002025
Iteration 91/1000 | Loss: 0.00002025
Iteration 92/1000 | Loss: 0.00002025
Iteration 93/1000 | Loss: 0.00002025
Iteration 94/1000 | Loss: 0.00002025
Iteration 95/1000 | Loss: 0.00002024
Iteration 96/1000 | Loss: 0.00002024
Iteration 97/1000 | Loss: 0.00002024
Iteration 98/1000 | Loss: 0.00002024
Iteration 99/1000 | Loss: 0.00002024
Iteration 100/1000 | Loss: 0.00002024
Iteration 101/1000 | Loss: 0.00002024
Iteration 102/1000 | Loss: 0.00002023
Iteration 103/1000 | Loss: 0.00002023
Iteration 104/1000 | Loss: 0.00002023
Iteration 105/1000 | Loss: 0.00002023
Iteration 106/1000 | Loss: 0.00002023
Iteration 107/1000 | Loss: 0.00002022
Iteration 108/1000 | Loss: 0.00002022
Iteration 109/1000 | Loss: 0.00002022
Iteration 110/1000 | Loss: 0.00002022
Iteration 111/1000 | Loss: 0.00002022
Iteration 112/1000 | Loss: 0.00002022
Iteration 113/1000 | Loss: 0.00002021
Iteration 114/1000 | Loss: 0.00002021
Iteration 115/1000 | Loss: 0.00002021
Iteration 116/1000 | Loss: 0.00002020
Iteration 117/1000 | Loss: 0.00002020
Iteration 118/1000 | Loss: 0.00002020
Iteration 119/1000 | Loss: 0.00002020
Iteration 120/1000 | Loss: 0.00002020
Iteration 121/1000 | Loss: 0.00002020
Iteration 122/1000 | Loss: 0.00002020
Iteration 123/1000 | Loss: 0.00002019
Iteration 124/1000 | Loss: 0.00002019
Iteration 125/1000 | Loss: 0.00002019
Iteration 126/1000 | Loss: 0.00002019
Iteration 127/1000 | Loss: 0.00002019
Iteration 128/1000 | Loss: 0.00002019
Iteration 129/1000 | Loss: 0.00002019
Iteration 130/1000 | Loss: 0.00002019
Iteration 131/1000 | Loss: 0.00002019
Iteration 132/1000 | Loss: 0.00002018
Iteration 133/1000 | Loss: 0.00002018
Iteration 134/1000 | Loss: 0.00002018
Iteration 135/1000 | Loss: 0.00002017
Iteration 136/1000 | Loss: 0.00002017
Iteration 137/1000 | Loss: 0.00002016
Iteration 138/1000 | Loss: 0.00002016
Iteration 139/1000 | Loss: 0.00002016
Iteration 140/1000 | Loss: 0.00002015
Iteration 141/1000 | Loss: 0.00002015
Iteration 142/1000 | Loss: 0.00002014
Iteration 143/1000 | Loss: 0.00002014
Iteration 144/1000 | Loss: 0.00002014
Iteration 145/1000 | Loss: 0.00002013
Iteration 146/1000 | Loss: 0.00002013
Iteration 147/1000 | Loss: 0.00002013
Iteration 148/1000 | Loss: 0.00002012
Iteration 149/1000 | Loss: 0.00002012
Iteration 150/1000 | Loss: 0.00002012
Iteration 151/1000 | Loss: 0.00002011
Iteration 152/1000 | Loss: 0.00002011
Iteration 153/1000 | Loss: 0.00002011
Iteration 154/1000 | Loss: 0.00002011
Iteration 155/1000 | Loss: 0.00002010
Iteration 156/1000 | Loss: 0.00002010
Iteration 157/1000 | Loss: 0.00002010
Iteration 158/1000 | Loss: 0.00002010
Iteration 159/1000 | Loss: 0.00002010
Iteration 160/1000 | Loss: 0.00002010
Iteration 161/1000 | Loss: 0.00002010
Iteration 162/1000 | Loss: 0.00002010
Iteration 163/1000 | Loss: 0.00002010
Iteration 164/1000 | Loss: 0.00002010
Iteration 165/1000 | Loss: 0.00002010
Iteration 166/1000 | Loss: 0.00002010
Iteration 167/1000 | Loss: 0.00002010
Iteration 168/1000 | Loss: 0.00002010
Iteration 169/1000 | Loss: 0.00002010
Iteration 170/1000 | Loss: 0.00002010
Iteration 171/1000 | Loss: 0.00002010
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.009690615523141e-05, 2.009690615523141e-05, 2.009690615523141e-05, 2.009690615523141e-05, 2.009690615523141e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.009690615523141e-05

Optimization complete. Final v2v error: 3.7860922813415527 mm

Highest mean error: 3.878844976425171 mm for frame 155

Lowest mean error: 3.5849146842956543 mm for frame 0

Saving results

Total time: 41.42237734794617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814742
Iteration 2/25 | Loss: 0.00190005
Iteration 3/25 | Loss: 0.00154354
Iteration 4/25 | Loss: 0.00149287
Iteration 5/25 | Loss: 0.00147333
Iteration 6/25 | Loss: 0.00146118
Iteration 7/25 | Loss: 0.00146024
Iteration 8/25 | Loss: 0.00143868
Iteration 9/25 | Loss: 0.00144854
Iteration 10/25 | Loss: 0.00143835
Iteration 11/25 | Loss: 0.00141694
Iteration 12/25 | Loss: 0.00141013
Iteration 13/25 | Loss: 0.00140560
Iteration 14/25 | Loss: 0.00140359
Iteration 15/25 | Loss: 0.00139421
Iteration 16/25 | Loss: 0.00138957
Iteration 17/25 | Loss: 0.00139228
Iteration 18/25 | Loss: 0.00139041
Iteration 19/25 | Loss: 0.00138705
Iteration 20/25 | Loss: 0.00138538
Iteration 21/25 | Loss: 0.00138656
Iteration 22/25 | Loss: 0.00138373
Iteration 23/25 | Loss: 0.00138269
Iteration 24/25 | Loss: 0.00138246
Iteration 25/25 | Loss: 0.00138240

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33107579
Iteration 2/25 | Loss: 0.00093706
Iteration 3/25 | Loss: 0.00093700
Iteration 4/25 | Loss: 0.00093700
Iteration 5/25 | Loss: 0.00093700
Iteration 6/25 | Loss: 0.00093700
Iteration 7/25 | Loss: 0.00093700
Iteration 8/25 | Loss: 0.00093700
Iteration 9/25 | Loss: 0.00093700
Iteration 10/25 | Loss: 0.00093700
Iteration 11/25 | Loss: 0.00093700
Iteration 12/25 | Loss: 0.00093700
Iteration 13/25 | Loss: 0.00093700
Iteration 14/25 | Loss: 0.00093700
Iteration 15/25 | Loss: 0.00093700
Iteration 16/25 | Loss: 0.00093700
Iteration 17/25 | Loss: 0.00093700
Iteration 18/25 | Loss: 0.00093700
Iteration 19/25 | Loss: 0.00093700
Iteration 20/25 | Loss: 0.00093700
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009369993349537253, 0.0009369993349537253, 0.0009369993349537253, 0.0009369993349537253, 0.0009369993349537253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009369993349537253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093700
Iteration 2/1000 | Loss: 0.00008322
Iteration 3/1000 | Loss: 0.00005851
Iteration 4/1000 | Loss: 0.00005170
Iteration 5/1000 | Loss: 0.00004776
Iteration 6/1000 | Loss: 0.00004538
Iteration 7/1000 | Loss: 0.00024257
Iteration 8/1000 | Loss: 0.00005663
Iteration 9/1000 | Loss: 0.00004483
Iteration 10/1000 | Loss: 0.00004303
Iteration 11/1000 | Loss: 0.00004147
Iteration 12/1000 | Loss: 0.00004011
Iteration 13/1000 | Loss: 0.00003893
Iteration 14/1000 | Loss: 0.00003826
Iteration 15/1000 | Loss: 0.00003790
Iteration 16/1000 | Loss: 0.00003758
Iteration 17/1000 | Loss: 0.00003719
Iteration 18/1000 | Loss: 0.00003702
Iteration 19/1000 | Loss: 0.00003682
Iteration 20/1000 | Loss: 0.00003668
Iteration 21/1000 | Loss: 0.00003666
Iteration 22/1000 | Loss: 0.00003664
Iteration 23/1000 | Loss: 0.00003664
Iteration 24/1000 | Loss: 0.00003663
Iteration 25/1000 | Loss: 0.00003661
Iteration 26/1000 | Loss: 0.00003659
Iteration 27/1000 | Loss: 0.00003658
Iteration 28/1000 | Loss: 0.00003653
Iteration 29/1000 | Loss: 0.00003652
Iteration 30/1000 | Loss: 0.00003652
Iteration 31/1000 | Loss: 0.00003651
Iteration 32/1000 | Loss: 0.00003642
Iteration 33/1000 | Loss: 0.00003640
Iteration 34/1000 | Loss: 0.00003639
Iteration 35/1000 | Loss: 0.00003638
Iteration 36/1000 | Loss: 0.00003637
Iteration 37/1000 | Loss: 0.00003632
Iteration 38/1000 | Loss: 0.00003632
Iteration 39/1000 | Loss: 0.00003631
Iteration 40/1000 | Loss: 0.00003629
Iteration 41/1000 | Loss: 0.00003628
Iteration 42/1000 | Loss: 0.00003627
Iteration 43/1000 | Loss: 0.00003627
Iteration 44/1000 | Loss: 0.00003626
Iteration 45/1000 | Loss: 0.00003626
Iteration 46/1000 | Loss: 0.00003626
Iteration 47/1000 | Loss: 0.00003625
Iteration 48/1000 | Loss: 0.00003624
Iteration 49/1000 | Loss: 0.00003624
Iteration 50/1000 | Loss: 0.00003623
Iteration 51/1000 | Loss: 0.00003622
Iteration 52/1000 | Loss: 0.00003622
Iteration 53/1000 | Loss: 0.00003621
Iteration 54/1000 | Loss: 0.00003621
Iteration 55/1000 | Loss: 0.00003620
Iteration 56/1000 | Loss: 0.00003620
Iteration 57/1000 | Loss: 0.00003617
Iteration 58/1000 | Loss: 0.00003617
Iteration 59/1000 | Loss: 0.00003616
Iteration 60/1000 | Loss: 0.00003610
Iteration 61/1000 | Loss: 0.00003606
Iteration 62/1000 | Loss: 0.00003605
Iteration 63/1000 | Loss: 0.00003605
Iteration 64/1000 | Loss: 0.00003604
Iteration 65/1000 | Loss: 0.00003604
Iteration 66/1000 | Loss: 0.00003604
Iteration 67/1000 | Loss: 0.00003603
Iteration 68/1000 | Loss: 0.00046417
Iteration 69/1000 | Loss: 0.00093473
Iteration 70/1000 | Loss: 0.00004595
Iteration 71/1000 | Loss: 0.00003619
Iteration 72/1000 | Loss: 0.00003374
Iteration 73/1000 | Loss: 0.00003193
Iteration 74/1000 | Loss: 0.00003077
Iteration 75/1000 | Loss: 0.00002992
Iteration 76/1000 | Loss: 0.00002890
Iteration 77/1000 | Loss: 0.00002824
Iteration 78/1000 | Loss: 0.00002768
Iteration 79/1000 | Loss: 0.00002733
Iteration 80/1000 | Loss: 0.00002717
Iteration 81/1000 | Loss: 0.00002702
Iteration 82/1000 | Loss: 0.00002701
Iteration 83/1000 | Loss: 0.00002701
Iteration 84/1000 | Loss: 0.00002697
Iteration 85/1000 | Loss: 0.00002690
Iteration 86/1000 | Loss: 0.00002681
Iteration 87/1000 | Loss: 0.00002680
Iteration 88/1000 | Loss: 0.00002679
Iteration 89/1000 | Loss: 0.00002679
Iteration 90/1000 | Loss: 0.00002678
Iteration 91/1000 | Loss: 0.00002678
Iteration 92/1000 | Loss: 0.00002677
Iteration 93/1000 | Loss: 0.00002676
Iteration 94/1000 | Loss: 0.00002676
Iteration 95/1000 | Loss: 0.00002675
Iteration 96/1000 | Loss: 0.00002675
Iteration 97/1000 | Loss: 0.00002675
Iteration 98/1000 | Loss: 0.00002674
Iteration 99/1000 | Loss: 0.00002674
Iteration 100/1000 | Loss: 0.00002674
Iteration 101/1000 | Loss: 0.00002670
Iteration 102/1000 | Loss: 0.00002669
Iteration 103/1000 | Loss: 0.00002668
Iteration 104/1000 | Loss: 0.00002668
Iteration 105/1000 | Loss: 0.00002667
Iteration 106/1000 | Loss: 0.00002665
Iteration 107/1000 | Loss: 0.00002665
Iteration 108/1000 | Loss: 0.00002665
Iteration 109/1000 | Loss: 0.00002664
Iteration 110/1000 | Loss: 0.00002663
Iteration 111/1000 | Loss: 0.00002663
Iteration 112/1000 | Loss: 0.00002662
Iteration 113/1000 | Loss: 0.00002662
Iteration 114/1000 | Loss: 0.00002662
Iteration 115/1000 | Loss: 0.00002662
Iteration 116/1000 | Loss: 0.00002662
Iteration 117/1000 | Loss: 0.00002662
Iteration 118/1000 | Loss: 0.00002662
Iteration 119/1000 | Loss: 0.00002662
Iteration 120/1000 | Loss: 0.00002662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [2.662221777427476e-05, 2.662221777427476e-05, 2.662221777427476e-05, 2.662221777427476e-05, 2.662221777427476e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.662221777427476e-05

Optimization complete. Final v2v error: 4.101465225219727 mm

Highest mean error: 11.466483116149902 mm for frame 170

Lowest mean error: 3.6754086017608643 mm for frame 37

Saving results

Total time: 117.81559133529663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395138
Iteration 2/25 | Loss: 0.00131270
Iteration 3/25 | Loss: 0.00125063
Iteration 4/25 | Loss: 0.00124189
Iteration 5/25 | Loss: 0.00123872
Iteration 6/25 | Loss: 0.00123872
Iteration 7/25 | Loss: 0.00123872
Iteration 8/25 | Loss: 0.00123872
Iteration 9/25 | Loss: 0.00123872
Iteration 10/25 | Loss: 0.00123872
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012387241004034877, 0.0012387241004034877, 0.0012387241004034877, 0.0012387241004034877, 0.0012387241004034877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012387241004034877

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43576980
Iteration 2/25 | Loss: 0.00081284
Iteration 3/25 | Loss: 0.00081284
Iteration 4/25 | Loss: 0.00081283
Iteration 5/25 | Loss: 0.00081283
Iteration 6/25 | Loss: 0.00081283
Iteration 7/25 | Loss: 0.00081283
Iteration 8/25 | Loss: 0.00081283
Iteration 9/25 | Loss: 0.00081283
Iteration 10/25 | Loss: 0.00081283
Iteration 11/25 | Loss: 0.00081283
Iteration 12/25 | Loss: 0.00081283
Iteration 13/25 | Loss: 0.00081283
Iteration 14/25 | Loss: 0.00081283
Iteration 15/25 | Loss: 0.00081283
Iteration 16/25 | Loss: 0.00081283
Iteration 17/25 | Loss: 0.00081283
Iteration 18/25 | Loss: 0.00081283
Iteration 19/25 | Loss: 0.00081283
Iteration 20/25 | Loss: 0.00081283
Iteration 21/25 | Loss: 0.00081283
Iteration 22/25 | Loss: 0.00081283
Iteration 23/25 | Loss: 0.00081283
Iteration 24/25 | Loss: 0.00081283
Iteration 25/25 | Loss: 0.00081283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081283
Iteration 2/1000 | Loss: 0.00001981
Iteration 3/1000 | Loss: 0.00001551
Iteration 4/1000 | Loss: 0.00001419
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001303
Iteration 7/1000 | Loss: 0.00001272
Iteration 8/1000 | Loss: 0.00001271
Iteration 9/1000 | Loss: 0.00001246
Iteration 10/1000 | Loss: 0.00001244
Iteration 11/1000 | Loss: 0.00001235
Iteration 12/1000 | Loss: 0.00001222
Iteration 13/1000 | Loss: 0.00001215
Iteration 14/1000 | Loss: 0.00001213
Iteration 15/1000 | Loss: 0.00001209
Iteration 16/1000 | Loss: 0.00001207
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001205
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001202
Iteration 21/1000 | Loss: 0.00001201
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001199
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001198
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001196
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001194
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001191
Iteration 40/1000 | Loss: 0.00001190
Iteration 41/1000 | Loss: 0.00001187
Iteration 42/1000 | Loss: 0.00001186
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001183
Iteration 46/1000 | Loss: 0.00001183
Iteration 47/1000 | Loss: 0.00001182
Iteration 48/1000 | Loss: 0.00001181
Iteration 49/1000 | Loss: 0.00001180
Iteration 50/1000 | Loss: 0.00001178
Iteration 51/1000 | Loss: 0.00001178
Iteration 52/1000 | Loss: 0.00001177
Iteration 53/1000 | Loss: 0.00001175
Iteration 54/1000 | Loss: 0.00001174
Iteration 55/1000 | Loss: 0.00001173
Iteration 56/1000 | Loss: 0.00001173
Iteration 57/1000 | Loss: 0.00001168
Iteration 58/1000 | Loss: 0.00001165
Iteration 59/1000 | Loss: 0.00001164
Iteration 60/1000 | Loss: 0.00001164
Iteration 61/1000 | Loss: 0.00001163
Iteration 62/1000 | Loss: 0.00001163
Iteration 63/1000 | Loss: 0.00001162
Iteration 64/1000 | Loss: 0.00001162
Iteration 65/1000 | Loss: 0.00001161
Iteration 66/1000 | Loss: 0.00001161
Iteration 67/1000 | Loss: 0.00001161
Iteration 68/1000 | Loss: 0.00001161
Iteration 69/1000 | Loss: 0.00001160
Iteration 70/1000 | Loss: 0.00001160
Iteration 71/1000 | Loss: 0.00001159
Iteration 72/1000 | Loss: 0.00001159
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001157
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001153
Iteration 78/1000 | Loss: 0.00001152
Iteration 79/1000 | Loss: 0.00001152
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001151
Iteration 82/1000 | Loss: 0.00001151
Iteration 83/1000 | Loss: 0.00001151
Iteration 84/1000 | Loss: 0.00001151
Iteration 85/1000 | Loss: 0.00001151
Iteration 86/1000 | Loss: 0.00001151
Iteration 87/1000 | Loss: 0.00001151
Iteration 88/1000 | Loss: 0.00001150
Iteration 89/1000 | Loss: 0.00001150
Iteration 90/1000 | Loss: 0.00001150
Iteration 91/1000 | Loss: 0.00001150
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001147
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001147
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001147
Iteration 106/1000 | Loss: 0.00001147
Iteration 107/1000 | Loss: 0.00001146
Iteration 108/1000 | Loss: 0.00001146
Iteration 109/1000 | Loss: 0.00001146
Iteration 110/1000 | Loss: 0.00001146
Iteration 111/1000 | Loss: 0.00001145
Iteration 112/1000 | Loss: 0.00001145
Iteration 113/1000 | Loss: 0.00001144
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001144
Iteration 116/1000 | Loss: 0.00001144
Iteration 117/1000 | Loss: 0.00001143
Iteration 118/1000 | Loss: 0.00001143
Iteration 119/1000 | Loss: 0.00001143
Iteration 120/1000 | Loss: 0.00001143
Iteration 121/1000 | Loss: 0.00001143
Iteration 122/1000 | Loss: 0.00001143
Iteration 123/1000 | Loss: 0.00001142
Iteration 124/1000 | Loss: 0.00001141
Iteration 125/1000 | Loss: 0.00001141
Iteration 126/1000 | Loss: 0.00001141
Iteration 127/1000 | Loss: 0.00001141
Iteration 128/1000 | Loss: 0.00001141
Iteration 129/1000 | Loss: 0.00001141
Iteration 130/1000 | Loss: 0.00001141
Iteration 131/1000 | Loss: 0.00001141
Iteration 132/1000 | Loss: 0.00001140
Iteration 133/1000 | Loss: 0.00001140
Iteration 134/1000 | Loss: 0.00001140
Iteration 135/1000 | Loss: 0.00001139
Iteration 136/1000 | Loss: 0.00001139
Iteration 137/1000 | Loss: 0.00001139
Iteration 138/1000 | Loss: 0.00001139
Iteration 139/1000 | Loss: 0.00001139
Iteration 140/1000 | Loss: 0.00001139
Iteration 141/1000 | Loss: 0.00001139
Iteration 142/1000 | Loss: 0.00001139
Iteration 143/1000 | Loss: 0.00001139
Iteration 144/1000 | Loss: 0.00001139
Iteration 145/1000 | Loss: 0.00001139
Iteration 146/1000 | Loss: 0.00001139
Iteration 147/1000 | Loss: 0.00001139
Iteration 148/1000 | Loss: 0.00001139
Iteration 149/1000 | Loss: 0.00001139
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.1391667612770107e-05, 1.1391667612770107e-05, 1.1391667612770107e-05, 1.1391667612770107e-05, 1.1391667612770107e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1391667612770107e-05

Optimization complete. Final v2v error: 2.925677537918091 mm

Highest mean error: 2.969940423965454 mm for frame 39

Lowest mean error: 2.8944365978240967 mm for frame 144

Saving results

Total time: 40.13705325126648
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051950
Iteration 2/25 | Loss: 0.00198820
Iteration 3/25 | Loss: 0.00149155
Iteration 4/25 | Loss: 0.00147323
Iteration 5/25 | Loss: 0.00145023
Iteration 6/25 | Loss: 0.00138575
Iteration 7/25 | Loss: 0.00139557
Iteration 8/25 | Loss: 0.00137337
Iteration 9/25 | Loss: 0.00135701
Iteration 10/25 | Loss: 0.00132976
Iteration 11/25 | Loss: 0.00133186
Iteration 12/25 | Loss: 0.00132623
Iteration 13/25 | Loss: 0.00132678
Iteration 14/25 | Loss: 0.00132543
Iteration 15/25 | Loss: 0.00132460
Iteration 16/25 | Loss: 0.00132512
Iteration 17/25 | Loss: 0.00132432
Iteration 18/25 | Loss: 0.00132328
Iteration 19/25 | Loss: 0.00132010
Iteration 20/25 | Loss: 0.00131986
Iteration 21/25 | Loss: 0.00131972
Iteration 22/25 | Loss: 0.00131967
Iteration 23/25 | Loss: 0.00131967
Iteration 24/25 | Loss: 0.00131967
Iteration 25/25 | Loss: 0.00131967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.06527042
Iteration 2/25 | Loss: 0.00108323
Iteration 3/25 | Loss: 0.00104237
Iteration 4/25 | Loss: 0.00104237
Iteration 5/25 | Loss: 0.00104237
Iteration 6/25 | Loss: 0.00104237
Iteration 7/25 | Loss: 0.00104237
Iteration 8/25 | Loss: 0.00104237
Iteration 9/25 | Loss: 0.00104237
Iteration 10/25 | Loss: 0.00104237
Iteration 11/25 | Loss: 0.00104237
Iteration 12/25 | Loss: 0.00104237
Iteration 13/25 | Loss: 0.00104237
Iteration 14/25 | Loss: 0.00104237
Iteration 15/25 | Loss: 0.00104237
Iteration 16/25 | Loss: 0.00104237
Iteration 17/25 | Loss: 0.00104237
Iteration 18/25 | Loss: 0.00104237
Iteration 19/25 | Loss: 0.00104237
Iteration 20/25 | Loss: 0.00104237
Iteration 21/25 | Loss: 0.00104237
Iteration 22/25 | Loss: 0.00104237
Iteration 23/25 | Loss: 0.00104237
Iteration 24/25 | Loss: 0.00104237
Iteration 25/25 | Loss: 0.00104237
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0010423698695376515, 0.0010423698695376515, 0.0010423698695376515, 0.0010423698695376515, 0.0010423698695376515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010423698695376515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104237
Iteration 2/1000 | Loss: 0.00011885
Iteration 3/1000 | Loss: 0.00025546
Iteration 4/1000 | Loss: 0.00005775
Iteration 5/1000 | Loss: 0.00004444
Iteration 6/1000 | Loss: 0.00008694
Iteration 7/1000 | Loss: 0.00008045
Iteration 8/1000 | Loss: 0.00007918
Iteration 9/1000 | Loss: 0.00006732
Iteration 10/1000 | Loss: 0.00004011
Iteration 11/1000 | Loss: 0.00018199
Iteration 12/1000 | Loss: 0.00014765
Iteration 13/1000 | Loss: 0.00024199
Iteration 14/1000 | Loss: 0.00005713
Iteration 15/1000 | Loss: 0.00007399
Iteration 16/1000 | Loss: 0.00005644
Iteration 17/1000 | Loss: 0.00005259
Iteration 18/1000 | Loss: 0.00021514
Iteration 19/1000 | Loss: 0.00006141
Iteration 20/1000 | Loss: 0.00082089
Iteration 21/1000 | Loss: 0.00027590
Iteration 22/1000 | Loss: 0.00018841
Iteration 23/1000 | Loss: 0.00020330
Iteration 24/1000 | Loss: 0.00013607
Iteration 25/1000 | Loss: 0.00029173
Iteration 26/1000 | Loss: 0.00015479
Iteration 27/1000 | Loss: 0.00019416
Iteration 28/1000 | Loss: 0.00014362
Iteration 29/1000 | Loss: 0.00018158
Iteration 30/1000 | Loss: 0.00019239
Iteration 31/1000 | Loss: 0.00023960
Iteration 32/1000 | Loss: 0.00069818
Iteration 33/1000 | Loss: 0.00008438
Iteration 34/1000 | Loss: 0.00004269
Iteration 35/1000 | Loss: 0.00004360
Iteration 36/1000 | Loss: 0.00006752
Iteration 37/1000 | Loss: 0.00006383
Iteration 38/1000 | Loss: 0.00004800
Iteration 39/1000 | Loss: 0.00003721
Iteration 40/1000 | Loss: 0.00005884
Iteration 41/1000 | Loss: 0.00005327
Iteration 42/1000 | Loss: 0.00004807
Iteration 43/1000 | Loss: 0.00006748
Iteration 44/1000 | Loss: 0.00004835
Iteration 45/1000 | Loss: 0.00006815
Iteration 46/1000 | Loss: 0.00004650
Iteration 47/1000 | Loss: 0.00006467
Iteration 48/1000 | Loss: 0.00003657
Iteration 49/1000 | Loss: 0.00003349
Iteration 50/1000 | Loss: 0.00003193
Iteration 51/1000 | Loss: 0.00003106
Iteration 52/1000 | Loss: 0.00003019
Iteration 53/1000 | Loss: 0.00002963
Iteration 54/1000 | Loss: 0.00002925
Iteration 55/1000 | Loss: 0.00002900
Iteration 56/1000 | Loss: 0.00002869
Iteration 57/1000 | Loss: 0.00002843
Iteration 58/1000 | Loss: 0.00002840
Iteration 59/1000 | Loss: 0.00002824
Iteration 60/1000 | Loss: 0.00002808
Iteration 61/1000 | Loss: 0.00002798
Iteration 62/1000 | Loss: 0.00002790
Iteration 63/1000 | Loss: 0.00002786
Iteration 64/1000 | Loss: 0.00002786
Iteration 65/1000 | Loss: 0.00002786
Iteration 66/1000 | Loss: 0.00002786
Iteration 67/1000 | Loss: 0.00002786
Iteration 68/1000 | Loss: 0.00002786
Iteration 69/1000 | Loss: 0.00002786
Iteration 70/1000 | Loss: 0.00002785
Iteration 71/1000 | Loss: 0.00002785
Iteration 72/1000 | Loss: 0.00002785
Iteration 73/1000 | Loss: 0.00002785
Iteration 74/1000 | Loss: 0.00002785
Iteration 75/1000 | Loss: 0.00002785
Iteration 76/1000 | Loss: 0.00002784
Iteration 77/1000 | Loss: 0.00002784
Iteration 78/1000 | Loss: 0.00002784
Iteration 79/1000 | Loss: 0.00002784
Iteration 80/1000 | Loss: 0.00002784
Iteration 81/1000 | Loss: 0.00002784
Iteration 82/1000 | Loss: 0.00002784
Iteration 83/1000 | Loss: 0.00002784
Iteration 84/1000 | Loss: 0.00002784
Iteration 85/1000 | Loss: 0.00002784
Iteration 86/1000 | Loss: 0.00002784
Iteration 87/1000 | Loss: 0.00002783
Iteration 88/1000 | Loss: 0.00002783
Iteration 89/1000 | Loss: 0.00002783
Iteration 90/1000 | Loss: 0.00002782
Iteration 91/1000 | Loss: 0.00002782
Iteration 92/1000 | Loss: 0.00002781
Iteration 93/1000 | Loss: 0.00002781
Iteration 94/1000 | Loss: 0.00002781
Iteration 95/1000 | Loss: 0.00002780
Iteration 96/1000 | Loss: 0.00002780
Iteration 97/1000 | Loss: 0.00002780
Iteration 98/1000 | Loss: 0.00002780
Iteration 99/1000 | Loss: 0.00002780
Iteration 100/1000 | Loss: 0.00002779
Iteration 101/1000 | Loss: 0.00002779
Iteration 102/1000 | Loss: 0.00002779
Iteration 103/1000 | Loss: 0.00002779
Iteration 104/1000 | Loss: 0.00002779
Iteration 105/1000 | Loss: 0.00002779
Iteration 106/1000 | Loss: 0.00002779
Iteration 107/1000 | Loss: 0.00002779
Iteration 108/1000 | Loss: 0.00002779
Iteration 109/1000 | Loss: 0.00002778
Iteration 110/1000 | Loss: 0.00002778
Iteration 111/1000 | Loss: 0.00002778
Iteration 112/1000 | Loss: 0.00002778
Iteration 113/1000 | Loss: 0.00002777
Iteration 114/1000 | Loss: 0.00002777
Iteration 115/1000 | Loss: 0.00002777
Iteration 116/1000 | Loss: 0.00002777
Iteration 117/1000 | Loss: 0.00002777
Iteration 118/1000 | Loss: 0.00002776
Iteration 119/1000 | Loss: 0.00002776
Iteration 120/1000 | Loss: 0.00002776
Iteration 121/1000 | Loss: 0.00002776
Iteration 122/1000 | Loss: 0.00002776
Iteration 123/1000 | Loss: 0.00002776
Iteration 124/1000 | Loss: 0.00002776
Iteration 125/1000 | Loss: 0.00002776
Iteration 126/1000 | Loss: 0.00002776
Iteration 127/1000 | Loss: 0.00002775
Iteration 128/1000 | Loss: 0.00002775
Iteration 129/1000 | Loss: 0.00002775
Iteration 130/1000 | Loss: 0.00002775
Iteration 131/1000 | Loss: 0.00002775
Iteration 132/1000 | Loss: 0.00002775
Iteration 133/1000 | Loss: 0.00002774
Iteration 134/1000 | Loss: 0.00002774
Iteration 135/1000 | Loss: 0.00002774
Iteration 136/1000 | Loss: 0.00002774
Iteration 137/1000 | Loss: 0.00002774
Iteration 138/1000 | Loss: 0.00002774
Iteration 139/1000 | Loss: 0.00002774
Iteration 140/1000 | Loss: 0.00002773
Iteration 141/1000 | Loss: 0.00002773
Iteration 142/1000 | Loss: 0.00002773
Iteration 143/1000 | Loss: 0.00002773
Iteration 144/1000 | Loss: 0.00002773
Iteration 145/1000 | Loss: 0.00002773
Iteration 146/1000 | Loss: 0.00002773
Iteration 147/1000 | Loss: 0.00002773
Iteration 148/1000 | Loss: 0.00002773
Iteration 149/1000 | Loss: 0.00002773
Iteration 150/1000 | Loss: 0.00002772
Iteration 151/1000 | Loss: 0.00002772
Iteration 152/1000 | Loss: 0.00002772
Iteration 153/1000 | Loss: 0.00002772
Iteration 154/1000 | Loss: 0.00002772
Iteration 155/1000 | Loss: 0.00002772
Iteration 156/1000 | Loss: 0.00002772
Iteration 157/1000 | Loss: 0.00002772
Iteration 158/1000 | Loss: 0.00002772
Iteration 159/1000 | Loss: 0.00002772
Iteration 160/1000 | Loss: 0.00002772
Iteration 161/1000 | Loss: 0.00002772
Iteration 162/1000 | Loss: 0.00002772
Iteration 163/1000 | Loss: 0.00002771
Iteration 164/1000 | Loss: 0.00002771
Iteration 165/1000 | Loss: 0.00002771
Iteration 166/1000 | Loss: 0.00002771
Iteration 167/1000 | Loss: 0.00002771
Iteration 168/1000 | Loss: 0.00002771
Iteration 169/1000 | Loss: 0.00002771
Iteration 170/1000 | Loss: 0.00002771
Iteration 171/1000 | Loss: 0.00002771
Iteration 172/1000 | Loss: 0.00002771
Iteration 173/1000 | Loss: 0.00002771
Iteration 174/1000 | Loss: 0.00002770
Iteration 175/1000 | Loss: 0.00002770
Iteration 176/1000 | Loss: 0.00002770
Iteration 177/1000 | Loss: 0.00002770
Iteration 178/1000 | Loss: 0.00002770
Iteration 179/1000 | Loss: 0.00002769
Iteration 180/1000 | Loss: 0.00002769
Iteration 181/1000 | Loss: 0.00002769
Iteration 182/1000 | Loss: 0.00002769
Iteration 183/1000 | Loss: 0.00002769
Iteration 184/1000 | Loss: 0.00002769
Iteration 185/1000 | Loss: 0.00002768
Iteration 186/1000 | Loss: 0.00002768
Iteration 187/1000 | Loss: 0.00002768
Iteration 188/1000 | Loss: 0.00002768
Iteration 189/1000 | Loss: 0.00002768
Iteration 190/1000 | Loss: 0.00002767
Iteration 191/1000 | Loss: 0.00002767
Iteration 192/1000 | Loss: 0.00002767
Iteration 193/1000 | Loss: 0.00002767
Iteration 194/1000 | Loss: 0.00002766
Iteration 195/1000 | Loss: 0.00002766
Iteration 196/1000 | Loss: 0.00002766
Iteration 197/1000 | Loss: 0.00002765
Iteration 198/1000 | Loss: 0.00002765
Iteration 199/1000 | Loss: 0.00002765
Iteration 200/1000 | Loss: 0.00002765
Iteration 201/1000 | Loss: 0.00002765
Iteration 202/1000 | Loss: 0.00002765
Iteration 203/1000 | Loss: 0.00002765
Iteration 204/1000 | Loss: 0.00002764
Iteration 205/1000 | Loss: 0.00002764
Iteration 206/1000 | Loss: 0.00002764
Iteration 207/1000 | Loss: 0.00002764
Iteration 208/1000 | Loss: 0.00002764
Iteration 209/1000 | Loss: 0.00002764
Iteration 210/1000 | Loss: 0.00002764
Iteration 211/1000 | Loss: 0.00002764
Iteration 212/1000 | Loss: 0.00002764
Iteration 213/1000 | Loss: 0.00002764
Iteration 214/1000 | Loss: 0.00002764
Iteration 215/1000 | Loss: 0.00002764
Iteration 216/1000 | Loss: 0.00002764
Iteration 217/1000 | Loss: 0.00002764
Iteration 218/1000 | Loss: 0.00002763
Iteration 219/1000 | Loss: 0.00002763
Iteration 220/1000 | Loss: 0.00002763
Iteration 221/1000 | Loss: 0.00002763
Iteration 222/1000 | Loss: 0.00002763
Iteration 223/1000 | Loss: 0.00002763
Iteration 224/1000 | Loss: 0.00002763
Iteration 225/1000 | Loss: 0.00002763
Iteration 226/1000 | Loss: 0.00002763
Iteration 227/1000 | Loss: 0.00002763
Iteration 228/1000 | Loss: 0.00002762
Iteration 229/1000 | Loss: 0.00002762
Iteration 230/1000 | Loss: 0.00002762
Iteration 231/1000 | Loss: 0.00002762
Iteration 232/1000 | Loss: 0.00002762
Iteration 233/1000 | Loss: 0.00002762
Iteration 234/1000 | Loss: 0.00002762
Iteration 235/1000 | Loss: 0.00002762
Iteration 236/1000 | Loss: 0.00002762
Iteration 237/1000 | Loss: 0.00002762
Iteration 238/1000 | Loss: 0.00002762
Iteration 239/1000 | Loss: 0.00002762
Iteration 240/1000 | Loss: 0.00002762
Iteration 241/1000 | Loss: 0.00002762
Iteration 242/1000 | Loss: 0.00002761
Iteration 243/1000 | Loss: 0.00002761
Iteration 244/1000 | Loss: 0.00002761
Iteration 245/1000 | Loss: 0.00002761
Iteration 246/1000 | Loss: 0.00002761
Iteration 247/1000 | Loss: 0.00002761
Iteration 248/1000 | Loss: 0.00002761
Iteration 249/1000 | Loss: 0.00002761
Iteration 250/1000 | Loss: 0.00002761
Iteration 251/1000 | Loss: 0.00002761
Iteration 252/1000 | Loss: 0.00002761
Iteration 253/1000 | Loss: 0.00002761
Iteration 254/1000 | Loss: 0.00002761
Iteration 255/1000 | Loss: 0.00002761
Iteration 256/1000 | Loss: 0.00002760
Iteration 257/1000 | Loss: 0.00002760
Iteration 258/1000 | Loss: 0.00002760
Iteration 259/1000 | Loss: 0.00002760
Iteration 260/1000 | Loss: 0.00002760
Iteration 261/1000 | Loss: 0.00002760
Iteration 262/1000 | Loss: 0.00002760
Iteration 263/1000 | Loss: 0.00002760
Iteration 264/1000 | Loss: 0.00002760
Iteration 265/1000 | Loss: 0.00002760
Iteration 266/1000 | Loss: 0.00002760
Iteration 267/1000 | Loss: 0.00002760
Iteration 268/1000 | Loss: 0.00002760
Iteration 269/1000 | Loss: 0.00002760
Iteration 270/1000 | Loss: 0.00002760
Iteration 271/1000 | Loss: 0.00002760
Iteration 272/1000 | Loss: 0.00002760
Iteration 273/1000 | Loss: 0.00002760
Iteration 274/1000 | Loss: 0.00002760
Iteration 275/1000 | Loss: 0.00002760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [2.759537528618239e-05, 2.759537528618239e-05, 2.759537528618239e-05, 2.759537528618239e-05, 2.759537528618239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.759537528618239e-05

Optimization complete. Final v2v error: 4.348906517028809 mm

Highest mean error: 5.75202751159668 mm for frame 35

Lowest mean error: 3.1937406063079834 mm for frame 6

Saving results

Total time: 138.54769253730774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048177
Iteration 2/25 | Loss: 0.00420546
Iteration 3/25 | Loss: 0.00345948
Iteration 4/25 | Loss: 0.00330281
Iteration 5/25 | Loss: 0.00362061
Iteration 6/25 | Loss: 0.00339663
Iteration 7/25 | Loss: 0.00267902
Iteration 8/25 | Loss: 0.00245204
Iteration 9/25 | Loss: 0.00239305
Iteration 10/25 | Loss: 0.00239217
Iteration 11/25 | Loss: 0.00225513
Iteration 12/25 | Loss: 0.00215833
Iteration 13/25 | Loss: 0.00216874
Iteration 14/25 | Loss: 0.00194163
Iteration 15/25 | Loss: 0.00184104
Iteration 16/25 | Loss: 0.00183488
Iteration 17/25 | Loss: 0.00176751
Iteration 18/25 | Loss: 0.00176726
Iteration 19/25 | Loss: 0.00177558
Iteration 20/25 | Loss: 0.00174856
Iteration 21/25 | Loss: 0.00176164
Iteration 22/25 | Loss: 0.00172276
Iteration 23/25 | Loss: 0.00171971
Iteration 24/25 | Loss: 0.00174908
Iteration 25/25 | Loss: 0.00172481

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12365854
Iteration 2/25 | Loss: 0.00231732
Iteration 3/25 | Loss: 0.00231731
Iteration 4/25 | Loss: 0.00231731
Iteration 5/25 | Loss: 0.00231731
Iteration 6/25 | Loss: 0.00231731
Iteration 7/25 | Loss: 0.00231731
Iteration 8/25 | Loss: 0.00231731
Iteration 9/25 | Loss: 0.00231731
Iteration 10/25 | Loss: 0.00231731
Iteration 11/25 | Loss: 0.00231731
Iteration 12/25 | Loss: 0.00231731
Iteration 13/25 | Loss: 0.00231731
Iteration 14/25 | Loss: 0.00231731
Iteration 15/25 | Loss: 0.00231731
Iteration 16/25 | Loss: 0.00231731
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0023173068184405565, 0.0023173068184405565, 0.0023173068184405565, 0.0023173068184405565, 0.0023173068184405565]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0023173068184405565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00231731
Iteration 2/1000 | Loss: 0.00139315
Iteration 3/1000 | Loss: 0.00021920
Iteration 4/1000 | Loss: 0.00018499
Iteration 5/1000 | Loss: 0.00016551
Iteration 6/1000 | Loss: 0.00015457
Iteration 7/1000 | Loss: 0.00014945
Iteration 8/1000 | Loss: 0.00122906
Iteration 9/1000 | Loss: 0.00655228
Iteration 10/1000 | Loss: 0.00438087
Iteration 11/1000 | Loss: 0.00198729
Iteration 12/1000 | Loss: 0.00023686
Iteration 13/1000 | Loss: 0.00014407
Iteration 14/1000 | Loss: 0.00010902
Iteration 15/1000 | Loss: 0.00009160
Iteration 16/1000 | Loss: 0.00007697
Iteration 17/1000 | Loss: 0.00006844
Iteration 18/1000 | Loss: 0.00006317
Iteration 19/1000 | Loss: 0.00005941
Iteration 20/1000 | Loss: 0.00128843
Iteration 21/1000 | Loss: 0.00021561
Iteration 22/1000 | Loss: 0.00010955
Iteration 23/1000 | Loss: 0.00020343
Iteration 24/1000 | Loss: 0.00271925
Iteration 25/1000 | Loss: 0.00083337
Iteration 26/1000 | Loss: 0.00172018
Iteration 27/1000 | Loss: 0.00011740
Iteration 28/1000 | Loss: 0.00007329
Iteration 29/1000 | Loss: 0.00006513
Iteration 30/1000 | Loss: 0.00006139
Iteration 31/1000 | Loss: 0.00005918
Iteration 32/1000 | Loss: 0.00005721
Iteration 33/1000 | Loss: 0.00005512
Iteration 34/1000 | Loss: 0.00005379
Iteration 35/1000 | Loss: 0.00005262
Iteration 36/1000 | Loss: 0.00021059
Iteration 37/1000 | Loss: 0.00008668
Iteration 38/1000 | Loss: 0.00007268
Iteration 39/1000 | Loss: 0.00006757
Iteration 40/1000 | Loss: 0.00015172
Iteration 41/1000 | Loss: 0.00017136
Iteration 42/1000 | Loss: 0.00008413
Iteration 43/1000 | Loss: 0.00006969
Iteration 44/1000 | Loss: 0.00010596
Iteration 45/1000 | Loss: 0.00019659
Iteration 46/1000 | Loss: 0.00009377
Iteration 47/1000 | Loss: 0.00009196
Iteration 48/1000 | Loss: 0.00007149
Iteration 49/1000 | Loss: 0.00017383
Iteration 50/1000 | Loss: 0.00007421
Iteration 51/1000 | Loss: 0.00019155
Iteration 52/1000 | Loss: 0.00006818
Iteration 53/1000 | Loss: 0.00007334
Iteration 54/1000 | Loss: 0.00014892
Iteration 55/1000 | Loss: 0.00016891
Iteration 56/1000 | Loss: 0.00007311
Iteration 57/1000 | Loss: 0.00005980
Iteration 58/1000 | Loss: 0.00007163
Iteration 59/1000 | Loss: 0.00005563
Iteration 60/1000 | Loss: 0.00005273
Iteration 61/1000 | Loss: 0.00032772
Iteration 62/1000 | Loss: 0.00070631
Iteration 63/1000 | Loss: 0.00031993
Iteration 64/1000 | Loss: 0.00005263
Iteration 65/1000 | Loss: 0.00005030
Iteration 66/1000 | Loss: 0.00004943
Iteration 67/1000 | Loss: 0.00004885
Iteration 68/1000 | Loss: 0.00004838
Iteration 69/1000 | Loss: 0.00004802
Iteration 70/1000 | Loss: 0.00031242
Iteration 71/1000 | Loss: 0.00006049
Iteration 72/1000 | Loss: 0.00005296
Iteration 73/1000 | Loss: 0.00005037
Iteration 74/1000 | Loss: 0.00004937
Iteration 75/1000 | Loss: 0.00004878
Iteration 76/1000 | Loss: 0.00004836
Iteration 77/1000 | Loss: 0.00004814
Iteration 78/1000 | Loss: 0.00004778
Iteration 79/1000 | Loss: 0.00004758
Iteration 80/1000 | Loss: 0.00004749
Iteration 81/1000 | Loss: 0.00004746
Iteration 82/1000 | Loss: 0.00004741
Iteration 83/1000 | Loss: 0.00004740
Iteration 84/1000 | Loss: 0.00004740
Iteration 85/1000 | Loss: 0.00004738
Iteration 86/1000 | Loss: 0.00004738
Iteration 87/1000 | Loss: 0.00004738
Iteration 88/1000 | Loss: 0.00004738
Iteration 89/1000 | Loss: 0.00004738
Iteration 90/1000 | Loss: 0.00004738
Iteration 91/1000 | Loss: 0.00004738
Iteration 92/1000 | Loss: 0.00004738
Iteration 93/1000 | Loss: 0.00004737
Iteration 94/1000 | Loss: 0.00004734
Iteration 95/1000 | Loss: 0.00004734
Iteration 96/1000 | Loss: 0.00004732
Iteration 97/1000 | Loss: 0.00004730
Iteration 98/1000 | Loss: 0.00023365
Iteration 99/1000 | Loss: 0.00005014
Iteration 100/1000 | Loss: 0.00038824
Iteration 101/1000 | Loss: 0.00146789
Iteration 102/1000 | Loss: 0.00033753
Iteration 103/1000 | Loss: 0.00026921
Iteration 104/1000 | Loss: 0.00015887
Iteration 105/1000 | Loss: 0.00005052
Iteration 106/1000 | Loss: 0.00004428
Iteration 107/1000 | Loss: 0.00004104
Iteration 108/1000 | Loss: 0.00003914
Iteration 109/1000 | Loss: 0.00003797
Iteration 110/1000 | Loss: 0.00003731
Iteration 111/1000 | Loss: 0.00005344
Iteration 112/1000 | Loss: 0.00003931
Iteration 113/1000 | Loss: 0.00003775
Iteration 114/1000 | Loss: 0.00003675
Iteration 115/1000 | Loss: 0.00003598
Iteration 116/1000 | Loss: 0.00003568
Iteration 117/1000 | Loss: 0.00003528
Iteration 118/1000 | Loss: 0.00003500
Iteration 119/1000 | Loss: 0.00003480
Iteration 120/1000 | Loss: 0.00003471
Iteration 121/1000 | Loss: 0.00003463
Iteration 122/1000 | Loss: 0.00003456
Iteration 123/1000 | Loss: 0.00003452
Iteration 124/1000 | Loss: 0.00003440
Iteration 125/1000 | Loss: 0.00003439
Iteration 126/1000 | Loss: 0.00003439
Iteration 127/1000 | Loss: 0.00003437
Iteration 128/1000 | Loss: 0.00003437
Iteration 129/1000 | Loss: 0.00003437
Iteration 130/1000 | Loss: 0.00003436
Iteration 131/1000 | Loss: 0.00003436
Iteration 132/1000 | Loss: 0.00003436
Iteration 133/1000 | Loss: 0.00003436
Iteration 134/1000 | Loss: 0.00003436
Iteration 135/1000 | Loss: 0.00003436
Iteration 136/1000 | Loss: 0.00003436
Iteration 137/1000 | Loss: 0.00003435
Iteration 138/1000 | Loss: 0.00003435
Iteration 139/1000 | Loss: 0.00003435
Iteration 140/1000 | Loss: 0.00003435
Iteration 141/1000 | Loss: 0.00003435
Iteration 142/1000 | Loss: 0.00003435
Iteration 143/1000 | Loss: 0.00003434
Iteration 144/1000 | Loss: 0.00003434
Iteration 145/1000 | Loss: 0.00003434
Iteration 146/1000 | Loss: 0.00003434
Iteration 147/1000 | Loss: 0.00003433
Iteration 148/1000 | Loss: 0.00003433
Iteration 149/1000 | Loss: 0.00003433
Iteration 150/1000 | Loss: 0.00003433
Iteration 151/1000 | Loss: 0.00003433
Iteration 152/1000 | Loss: 0.00003433
Iteration 153/1000 | Loss: 0.00003433
Iteration 154/1000 | Loss: 0.00003433
Iteration 155/1000 | Loss: 0.00003433
Iteration 156/1000 | Loss: 0.00003433
Iteration 157/1000 | Loss: 0.00003433
Iteration 158/1000 | Loss: 0.00003433
Iteration 159/1000 | Loss: 0.00003433
Iteration 160/1000 | Loss: 0.00003433
Iteration 161/1000 | Loss: 0.00003433
Iteration 162/1000 | Loss: 0.00003433
Iteration 163/1000 | Loss: 0.00003433
Iteration 164/1000 | Loss: 0.00003433
Iteration 165/1000 | Loss: 0.00003433
Iteration 166/1000 | Loss: 0.00003433
Iteration 167/1000 | Loss: 0.00003433
Iteration 168/1000 | Loss: 0.00003433
Iteration 169/1000 | Loss: 0.00003433
Iteration 170/1000 | Loss: 0.00003433
Iteration 171/1000 | Loss: 0.00003433
Iteration 172/1000 | Loss: 0.00003433
Iteration 173/1000 | Loss: 0.00003433
Iteration 174/1000 | Loss: 0.00003433
Iteration 175/1000 | Loss: 0.00003433
Iteration 176/1000 | Loss: 0.00003433
Iteration 177/1000 | Loss: 0.00003433
Iteration 178/1000 | Loss: 0.00003433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.43305837304797e-05, 3.43305837304797e-05, 3.43305837304797e-05, 3.43305837304797e-05, 3.43305837304797e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.43305837304797e-05

Optimization complete. Final v2v error: 4.5369157791137695 mm

Highest mean error: 10.58731746673584 mm for frame 39

Lowest mean error: 4.155847072601318 mm for frame 163

Saving results

Total time: 201.5387098789215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00395864
Iteration 2/25 | Loss: 0.00130262
Iteration 3/25 | Loss: 0.00123282
Iteration 4/25 | Loss: 0.00122593
Iteration 5/25 | Loss: 0.00122553
Iteration 6/25 | Loss: 0.00122553
Iteration 7/25 | Loss: 0.00122553
Iteration 8/25 | Loss: 0.00122553
Iteration 9/25 | Loss: 0.00122553
Iteration 10/25 | Loss: 0.00122553
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012255344772711396, 0.0012255344772711396, 0.0012255344772711396, 0.0012255344772711396, 0.0012255344772711396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012255344772711396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41838717
Iteration 2/25 | Loss: 0.00066110
Iteration 3/25 | Loss: 0.00066109
Iteration 4/25 | Loss: 0.00066109
Iteration 5/25 | Loss: 0.00066109
Iteration 6/25 | Loss: 0.00066109
Iteration 7/25 | Loss: 0.00066109
Iteration 8/25 | Loss: 0.00066109
Iteration 9/25 | Loss: 0.00066109
Iteration 10/25 | Loss: 0.00066109
Iteration 11/25 | Loss: 0.00066109
Iteration 12/25 | Loss: 0.00066109
Iteration 13/25 | Loss: 0.00066109
Iteration 14/25 | Loss: 0.00066109
Iteration 15/25 | Loss: 0.00066109
Iteration 16/25 | Loss: 0.00066109
Iteration 17/25 | Loss: 0.00066109
Iteration 18/25 | Loss: 0.00066109
Iteration 19/25 | Loss: 0.00066109
Iteration 20/25 | Loss: 0.00066109
Iteration 21/25 | Loss: 0.00066109
Iteration 22/25 | Loss: 0.00066109
Iteration 23/25 | Loss: 0.00066109
Iteration 24/25 | Loss: 0.00066109
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006610912969335914, 0.0006610912969335914, 0.0006610912969335914, 0.0006610912969335914, 0.0006610912969335914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006610912969335914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066109
Iteration 2/1000 | Loss: 0.00002828
Iteration 3/1000 | Loss: 0.00001937
Iteration 4/1000 | Loss: 0.00001686
Iteration 5/1000 | Loss: 0.00001564
Iteration 6/1000 | Loss: 0.00001473
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001398
Iteration 9/1000 | Loss: 0.00001364
Iteration 10/1000 | Loss: 0.00001346
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001315
Iteration 14/1000 | Loss: 0.00001314
Iteration 15/1000 | Loss: 0.00001313
Iteration 16/1000 | Loss: 0.00001313
Iteration 17/1000 | Loss: 0.00001313
Iteration 18/1000 | Loss: 0.00001312
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001311
Iteration 21/1000 | Loss: 0.00001308
Iteration 22/1000 | Loss: 0.00001307
Iteration 23/1000 | Loss: 0.00001304
Iteration 24/1000 | Loss: 0.00001303
Iteration 25/1000 | Loss: 0.00001296
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001280
Iteration 28/1000 | Loss: 0.00001277
Iteration 29/1000 | Loss: 0.00001275
Iteration 30/1000 | Loss: 0.00001274
Iteration 31/1000 | Loss: 0.00001274
Iteration 32/1000 | Loss: 0.00001273
Iteration 33/1000 | Loss: 0.00001272
Iteration 34/1000 | Loss: 0.00001272
Iteration 35/1000 | Loss: 0.00001267
Iteration 36/1000 | Loss: 0.00001267
Iteration 37/1000 | Loss: 0.00001267
Iteration 38/1000 | Loss: 0.00001267
Iteration 39/1000 | Loss: 0.00001267
Iteration 40/1000 | Loss: 0.00001267
Iteration 41/1000 | Loss: 0.00001267
Iteration 42/1000 | Loss: 0.00001266
Iteration 43/1000 | Loss: 0.00001265
Iteration 44/1000 | Loss: 0.00001264
Iteration 45/1000 | Loss: 0.00001264
Iteration 46/1000 | Loss: 0.00001263
Iteration 47/1000 | Loss: 0.00001263
Iteration 48/1000 | Loss: 0.00001262
Iteration 49/1000 | Loss: 0.00001262
Iteration 50/1000 | Loss: 0.00001262
Iteration 51/1000 | Loss: 0.00001261
Iteration 52/1000 | Loss: 0.00001261
Iteration 53/1000 | Loss: 0.00001260
Iteration 54/1000 | Loss: 0.00001260
Iteration 55/1000 | Loss: 0.00001259
Iteration 56/1000 | Loss: 0.00001259
Iteration 57/1000 | Loss: 0.00001259
Iteration 58/1000 | Loss: 0.00001259
Iteration 59/1000 | Loss: 0.00001259
Iteration 60/1000 | Loss: 0.00001259
Iteration 61/1000 | Loss: 0.00001258
Iteration 62/1000 | Loss: 0.00001258
Iteration 63/1000 | Loss: 0.00001258
Iteration 64/1000 | Loss: 0.00001258
Iteration 65/1000 | Loss: 0.00001258
Iteration 66/1000 | Loss: 0.00001258
Iteration 67/1000 | Loss: 0.00001258
Iteration 68/1000 | Loss: 0.00001258
Iteration 69/1000 | Loss: 0.00001258
Iteration 70/1000 | Loss: 0.00001258
Iteration 71/1000 | Loss: 0.00001258
Iteration 72/1000 | Loss: 0.00001257
Iteration 73/1000 | Loss: 0.00001257
Iteration 74/1000 | Loss: 0.00001257
Iteration 75/1000 | Loss: 0.00001257
Iteration 76/1000 | Loss: 0.00001257
Iteration 77/1000 | Loss: 0.00001257
Iteration 78/1000 | Loss: 0.00001257
Iteration 79/1000 | Loss: 0.00001257
Iteration 80/1000 | Loss: 0.00001257
Iteration 81/1000 | Loss: 0.00001255
Iteration 82/1000 | Loss: 0.00001255
Iteration 83/1000 | Loss: 0.00001255
Iteration 84/1000 | Loss: 0.00001254
Iteration 85/1000 | Loss: 0.00001254
Iteration 86/1000 | Loss: 0.00001254
Iteration 87/1000 | Loss: 0.00001254
Iteration 88/1000 | Loss: 0.00001254
Iteration 89/1000 | Loss: 0.00001254
Iteration 90/1000 | Loss: 0.00001253
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001252
Iteration 94/1000 | Loss: 0.00001252
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001250
Iteration 98/1000 | Loss: 0.00001249
Iteration 99/1000 | Loss: 0.00001249
Iteration 100/1000 | Loss: 0.00001248
Iteration 101/1000 | Loss: 0.00001248
Iteration 102/1000 | Loss: 0.00001247
Iteration 103/1000 | Loss: 0.00001246
Iteration 104/1000 | Loss: 0.00001246
Iteration 105/1000 | Loss: 0.00001246
Iteration 106/1000 | Loss: 0.00001245
Iteration 107/1000 | Loss: 0.00001245
Iteration 108/1000 | Loss: 0.00001245
Iteration 109/1000 | Loss: 0.00001245
Iteration 110/1000 | Loss: 0.00001245
Iteration 111/1000 | Loss: 0.00001245
Iteration 112/1000 | Loss: 0.00001245
Iteration 113/1000 | Loss: 0.00001245
Iteration 114/1000 | Loss: 0.00001245
Iteration 115/1000 | Loss: 0.00001244
Iteration 116/1000 | Loss: 0.00001244
Iteration 117/1000 | Loss: 0.00001244
Iteration 118/1000 | Loss: 0.00001244
Iteration 119/1000 | Loss: 0.00001244
Iteration 120/1000 | Loss: 0.00001244
Iteration 121/1000 | Loss: 0.00001243
Iteration 122/1000 | Loss: 0.00001243
Iteration 123/1000 | Loss: 0.00001243
Iteration 124/1000 | Loss: 0.00001243
Iteration 125/1000 | Loss: 0.00001243
Iteration 126/1000 | Loss: 0.00001243
Iteration 127/1000 | Loss: 0.00001243
Iteration 128/1000 | Loss: 0.00001242
Iteration 129/1000 | Loss: 0.00001242
Iteration 130/1000 | Loss: 0.00001242
Iteration 131/1000 | Loss: 0.00001242
Iteration 132/1000 | Loss: 0.00001242
Iteration 133/1000 | Loss: 0.00001242
Iteration 134/1000 | Loss: 0.00001242
Iteration 135/1000 | Loss: 0.00001242
Iteration 136/1000 | Loss: 0.00001242
Iteration 137/1000 | Loss: 0.00001242
Iteration 138/1000 | Loss: 0.00001242
Iteration 139/1000 | Loss: 0.00001242
Iteration 140/1000 | Loss: 0.00001242
Iteration 141/1000 | Loss: 0.00001242
Iteration 142/1000 | Loss: 0.00001242
Iteration 143/1000 | Loss: 0.00001241
Iteration 144/1000 | Loss: 0.00001241
Iteration 145/1000 | Loss: 0.00001241
Iteration 146/1000 | Loss: 0.00001241
Iteration 147/1000 | Loss: 0.00001241
Iteration 148/1000 | Loss: 0.00001241
Iteration 149/1000 | Loss: 0.00001241
Iteration 150/1000 | Loss: 0.00001241
Iteration 151/1000 | Loss: 0.00001241
Iteration 152/1000 | Loss: 0.00001241
Iteration 153/1000 | Loss: 0.00001241
Iteration 154/1000 | Loss: 0.00001241
Iteration 155/1000 | Loss: 0.00001240
Iteration 156/1000 | Loss: 0.00001240
Iteration 157/1000 | Loss: 0.00001240
Iteration 158/1000 | Loss: 0.00001240
Iteration 159/1000 | Loss: 0.00001240
Iteration 160/1000 | Loss: 0.00001240
Iteration 161/1000 | Loss: 0.00001240
Iteration 162/1000 | Loss: 0.00001240
Iteration 163/1000 | Loss: 0.00001240
Iteration 164/1000 | Loss: 0.00001239
Iteration 165/1000 | Loss: 0.00001239
Iteration 166/1000 | Loss: 0.00001239
Iteration 167/1000 | Loss: 0.00001239
Iteration 168/1000 | Loss: 0.00001239
Iteration 169/1000 | Loss: 0.00001239
Iteration 170/1000 | Loss: 0.00001239
Iteration 171/1000 | Loss: 0.00001239
Iteration 172/1000 | Loss: 0.00001239
Iteration 173/1000 | Loss: 0.00001239
Iteration 174/1000 | Loss: 0.00001239
Iteration 175/1000 | Loss: 0.00001239
Iteration 176/1000 | Loss: 0.00001239
Iteration 177/1000 | Loss: 0.00001239
Iteration 178/1000 | Loss: 0.00001239
Iteration 179/1000 | Loss: 0.00001239
Iteration 180/1000 | Loss: 0.00001239
Iteration 181/1000 | Loss: 0.00001239
Iteration 182/1000 | Loss: 0.00001239
Iteration 183/1000 | Loss: 0.00001239
Iteration 184/1000 | Loss: 0.00001239
Iteration 185/1000 | Loss: 0.00001239
Iteration 186/1000 | Loss: 0.00001239
Iteration 187/1000 | Loss: 0.00001239
Iteration 188/1000 | Loss: 0.00001239
Iteration 189/1000 | Loss: 0.00001239
Iteration 190/1000 | Loss: 0.00001239
Iteration 191/1000 | Loss: 0.00001239
Iteration 192/1000 | Loss: 0.00001239
Iteration 193/1000 | Loss: 0.00001239
Iteration 194/1000 | Loss: 0.00001239
Iteration 195/1000 | Loss: 0.00001239
Iteration 196/1000 | Loss: 0.00001239
Iteration 197/1000 | Loss: 0.00001239
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.23884556160192e-05, 1.23884556160192e-05, 1.23884556160192e-05, 1.23884556160192e-05, 1.23884556160192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.23884556160192e-05

Optimization complete. Final v2v error: 3.022643804550171 mm

Highest mean error: 3.090275287628174 mm for frame 137

Lowest mean error: 2.8806240558624268 mm for frame 239

Saving results

Total time: 43.90277314186096
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906057
Iteration 2/25 | Loss: 0.00175385
Iteration 3/25 | Loss: 0.00142133
Iteration 4/25 | Loss: 0.00136420
Iteration 5/25 | Loss: 0.00135095
Iteration 6/25 | Loss: 0.00134841
Iteration 7/25 | Loss: 0.00134758
Iteration 8/25 | Loss: 0.00134758
Iteration 9/25 | Loss: 0.00134758
Iteration 10/25 | Loss: 0.00134758
Iteration 11/25 | Loss: 0.00134758
Iteration 12/25 | Loss: 0.00134758
Iteration 13/25 | Loss: 0.00134758
Iteration 14/25 | Loss: 0.00134758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013475775485858321, 0.0013475775485858321, 0.0013475775485858321, 0.0013475775485858321, 0.0013475775485858321]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013475775485858321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51632071
Iteration 2/25 | Loss: 0.00091971
Iteration 3/25 | Loss: 0.00091961
Iteration 4/25 | Loss: 0.00091961
Iteration 5/25 | Loss: 0.00091961
Iteration 6/25 | Loss: 0.00091961
Iteration 7/25 | Loss: 0.00091961
Iteration 8/25 | Loss: 0.00091961
Iteration 9/25 | Loss: 0.00091961
Iteration 10/25 | Loss: 0.00091961
Iteration 11/25 | Loss: 0.00091961
Iteration 12/25 | Loss: 0.00091961
Iteration 13/25 | Loss: 0.00091961
Iteration 14/25 | Loss: 0.00091961
Iteration 15/25 | Loss: 0.00091961
Iteration 16/25 | Loss: 0.00091961
Iteration 17/25 | Loss: 0.00091961
Iteration 18/25 | Loss: 0.00091961
Iteration 19/25 | Loss: 0.00091961
Iteration 20/25 | Loss: 0.00091961
Iteration 21/25 | Loss: 0.00091961
Iteration 22/25 | Loss: 0.00091961
Iteration 23/25 | Loss: 0.00091961
Iteration 24/25 | Loss: 0.00091961
Iteration 25/25 | Loss: 0.00091961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091961
Iteration 2/1000 | Loss: 0.00008631
Iteration 3/1000 | Loss: 0.00005517
Iteration 4/1000 | Loss: 0.00004254
Iteration 5/1000 | Loss: 0.00003983
Iteration 6/1000 | Loss: 0.00003729
Iteration 7/1000 | Loss: 0.00003624
Iteration 8/1000 | Loss: 0.00003523
Iteration 9/1000 | Loss: 0.00003454
Iteration 10/1000 | Loss: 0.00003398
Iteration 11/1000 | Loss: 0.00003352
Iteration 12/1000 | Loss: 0.00003320
Iteration 13/1000 | Loss: 0.00003294
Iteration 14/1000 | Loss: 0.00003266
Iteration 15/1000 | Loss: 0.00003246
Iteration 16/1000 | Loss: 0.00003229
Iteration 17/1000 | Loss: 0.00003214
Iteration 18/1000 | Loss: 0.00003207
Iteration 19/1000 | Loss: 0.00003200
Iteration 20/1000 | Loss: 0.00003185
Iteration 21/1000 | Loss: 0.00003183
Iteration 22/1000 | Loss: 0.00003179
Iteration 23/1000 | Loss: 0.00003177
Iteration 24/1000 | Loss: 0.00003174
Iteration 25/1000 | Loss: 0.00003169
Iteration 26/1000 | Loss: 0.00003165
Iteration 27/1000 | Loss: 0.00003160
Iteration 28/1000 | Loss: 0.00003159
Iteration 29/1000 | Loss: 0.00003158
Iteration 30/1000 | Loss: 0.00003158
Iteration 31/1000 | Loss: 0.00003157
Iteration 32/1000 | Loss: 0.00003157
Iteration 33/1000 | Loss: 0.00003157
Iteration 34/1000 | Loss: 0.00003154
Iteration 35/1000 | Loss: 0.00003153
Iteration 36/1000 | Loss: 0.00003149
Iteration 37/1000 | Loss: 0.00003148
Iteration 38/1000 | Loss: 0.00003147
Iteration 39/1000 | Loss: 0.00003147
Iteration 40/1000 | Loss: 0.00003146
Iteration 41/1000 | Loss: 0.00003146
Iteration 42/1000 | Loss: 0.00003145
Iteration 43/1000 | Loss: 0.00003144
Iteration 44/1000 | Loss: 0.00003144
Iteration 45/1000 | Loss: 0.00003143
Iteration 46/1000 | Loss: 0.00003142
Iteration 47/1000 | Loss: 0.00003142
Iteration 48/1000 | Loss: 0.00003142
Iteration 49/1000 | Loss: 0.00003142
Iteration 50/1000 | Loss: 0.00003142
Iteration 51/1000 | Loss: 0.00003142
Iteration 52/1000 | Loss: 0.00003141
Iteration 53/1000 | Loss: 0.00003141
Iteration 54/1000 | Loss: 0.00003141
Iteration 55/1000 | Loss: 0.00003141
Iteration 56/1000 | Loss: 0.00003141
Iteration 57/1000 | Loss: 0.00003141
Iteration 58/1000 | Loss: 0.00003141
Iteration 59/1000 | Loss: 0.00003141
Iteration 60/1000 | Loss: 0.00003141
Iteration 61/1000 | Loss: 0.00003141
Iteration 62/1000 | Loss: 0.00003141
Iteration 63/1000 | Loss: 0.00003141
Iteration 64/1000 | Loss: 0.00003141
Iteration 65/1000 | Loss: 0.00003141
Iteration 66/1000 | Loss: 0.00003141
Iteration 67/1000 | Loss: 0.00003141
Iteration 68/1000 | Loss: 0.00003140
Iteration 69/1000 | Loss: 0.00003140
Iteration 70/1000 | Loss: 0.00003140
Iteration 71/1000 | Loss: 0.00003139
Iteration 72/1000 | Loss: 0.00003139
Iteration 73/1000 | Loss: 0.00003139
Iteration 74/1000 | Loss: 0.00003138
Iteration 75/1000 | Loss: 0.00003138
Iteration 76/1000 | Loss: 0.00003138
Iteration 77/1000 | Loss: 0.00003138
Iteration 78/1000 | Loss: 0.00003137
Iteration 79/1000 | Loss: 0.00003137
Iteration 80/1000 | Loss: 0.00003137
Iteration 81/1000 | Loss: 0.00003137
Iteration 82/1000 | Loss: 0.00003136
Iteration 83/1000 | Loss: 0.00003136
Iteration 84/1000 | Loss: 0.00003136
Iteration 85/1000 | Loss: 0.00003135
Iteration 86/1000 | Loss: 0.00003135
Iteration 87/1000 | Loss: 0.00003135
Iteration 88/1000 | Loss: 0.00003135
Iteration 89/1000 | Loss: 0.00003135
Iteration 90/1000 | Loss: 0.00003135
Iteration 91/1000 | Loss: 0.00003135
Iteration 92/1000 | Loss: 0.00003134
Iteration 93/1000 | Loss: 0.00003134
Iteration 94/1000 | Loss: 0.00003134
Iteration 95/1000 | Loss: 0.00003134
Iteration 96/1000 | Loss: 0.00003134
Iteration 97/1000 | Loss: 0.00003134
Iteration 98/1000 | Loss: 0.00003134
Iteration 99/1000 | Loss: 0.00003133
Iteration 100/1000 | Loss: 0.00003133
Iteration 101/1000 | Loss: 0.00003133
Iteration 102/1000 | Loss: 0.00003133
Iteration 103/1000 | Loss: 0.00003133
Iteration 104/1000 | Loss: 0.00003132
Iteration 105/1000 | Loss: 0.00003132
Iteration 106/1000 | Loss: 0.00003132
Iteration 107/1000 | Loss: 0.00003132
Iteration 108/1000 | Loss: 0.00003132
Iteration 109/1000 | Loss: 0.00003132
Iteration 110/1000 | Loss: 0.00003132
Iteration 111/1000 | Loss: 0.00003132
Iteration 112/1000 | Loss: 0.00003132
Iteration 113/1000 | Loss: 0.00003132
Iteration 114/1000 | Loss: 0.00003132
Iteration 115/1000 | Loss: 0.00003132
Iteration 116/1000 | Loss: 0.00003131
Iteration 117/1000 | Loss: 0.00003131
Iteration 118/1000 | Loss: 0.00003131
Iteration 119/1000 | Loss: 0.00003131
Iteration 120/1000 | Loss: 0.00003131
Iteration 121/1000 | Loss: 0.00003131
Iteration 122/1000 | Loss: 0.00003131
Iteration 123/1000 | Loss: 0.00003131
Iteration 124/1000 | Loss: 0.00003131
Iteration 125/1000 | Loss: 0.00003131
Iteration 126/1000 | Loss: 0.00003131
Iteration 127/1000 | Loss: 0.00003130
Iteration 128/1000 | Loss: 0.00003130
Iteration 129/1000 | Loss: 0.00003130
Iteration 130/1000 | Loss: 0.00003130
Iteration 131/1000 | Loss: 0.00003130
Iteration 132/1000 | Loss: 0.00003130
Iteration 133/1000 | Loss: 0.00003130
Iteration 134/1000 | Loss: 0.00003129
Iteration 135/1000 | Loss: 0.00003129
Iteration 136/1000 | Loss: 0.00003129
Iteration 137/1000 | Loss: 0.00003129
Iteration 138/1000 | Loss: 0.00003129
Iteration 139/1000 | Loss: 0.00003128
Iteration 140/1000 | Loss: 0.00003128
Iteration 141/1000 | Loss: 0.00003128
Iteration 142/1000 | Loss: 0.00003128
Iteration 143/1000 | Loss: 0.00003128
Iteration 144/1000 | Loss: 0.00003128
Iteration 145/1000 | Loss: 0.00003128
Iteration 146/1000 | Loss: 0.00003128
Iteration 147/1000 | Loss: 0.00003128
Iteration 148/1000 | Loss: 0.00003128
Iteration 149/1000 | Loss: 0.00003128
Iteration 150/1000 | Loss: 0.00003128
Iteration 151/1000 | Loss: 0.00003128
Iteration 152/1000 | Loss: 0.00003127
Iteration 153/1000 | Loss: 0.00003127
Iteration 154/1000 | Loss: 0.00003127
Iteration 155/1000 | Loss: 0.00003127
Iteration 156/1000 | Loss: 0.00003126
Iteration 157/1000 | Loss: 0.00003126
Iteration 158/1000 | Loss: 0.00003126
Iteration 159/1000 | Loss: 0.00003126
Iteration 160/1000 | Loss: 0.00003126
Iteration 161/1000 | Loss: 0.00003126
Iteration 162/1000 | Loss: 0.00003126
Iteration 163/1000 | Loss: 0.00003126
Iteration 164/1000 | Loss: 0.00003126
Iteration 165/1000 | Loss: 0.00003126
Iteration 166/1000 | Loss: 0.00003125
Iteration 167/1000 | Loss: 0.00003125
Iteration 168/1000 | Loss: 0.00003125
Iteration 169/1000 | Loss: 0.00003125
Iteration 170/1000 | Loss: 0.00003125
Iteration 171/1000 | Loss: 0.00003125
Iteration 172/1000 | Loss: 0.00003125
Iteration 173/1000 | Loss: 0.00003125
Iteration 174/1000 | Loss: 0.00003125
Iteration 175/1000 | Loss: 0.00003125
Iteration 176/1000 | Loss: 0.00003125
Iteration 177/1000 | Loss: 0.00003124
Iteration 178/1000 | Loss: 0.00003124
Iteration 179/1000 | Loss: 0.00003124
Iteration 180/1000 | Loss: 0.00003124
Iteration 181/1000 | Loss: 0.00003124
Iteration 182/1000 | Loss: 0.00003124
Iteration 183/1000 | Loss: 0.00003124
Iteration 184/1000 | Loss: 0.00003124
Iteration 185/1000 | Loss: 0.00003124
Iteration 186/1000 | Loss: 0.00003124
Iteration 187/1000 | Loss: 0.00003124
Iteration 188/1000 | Loss: 0.00003124
Iteration 189/1000 | Loss: 0.00003124
Iteration 190/1000 | Loss: 0.00003124
Iteration 191/1000 | Loss: 0.00003124
Iteration 192/1000 | Loss: 0.00003124
Iteration 193/1000 | Loss: 0.00003124
Iteration 194/1000 | Loss: 0.00003123
Iteration 195/1000 | Loss: 0.00003123
Iteration 196/1000 | Loss: 0.00003123
Iteration 197/1000 | Loss: 0.00003123
Iteration 198/1000 | Loss: 0.00003123
Iteration 199/1000 | Loss: 0.00003123
Iteration 200/1000 | Loss: 0.00003123
Iteration 201/1000 | Loss: 0.00003123
Iteration 202/1000 | Loss: 0.00003123
Iteration 203/1000 | Loss: 0.00003123
Iteration 204/1000 | Loss: 0.00003123
Iteration 205/1000 | Loss: 0.00003123
Iteration 206/1000 | Loss: 0.00003123
Iteration 207/1000 | Loss: 0.00003123
Iteration 208/1000 | Loss: 0.00003123
Iteration 209/1000 | Loss: 0.00003123
Iteration 210/1000 | Loss: 0.00003123
Iteration 211/1000 | Loss: 0.00003123
Iteration 212/1000 | Loss: 0.00003123
Iteration 213/1000 | Loss: 0.00003123
Iteration 214/1000 | Loss: 0.00003123
Iteration 215/1000 | Loss: 0.00003122
Iteration 216/1000 | Loss: 0.00003122
Iteration 217/1000 | Loss: 0.00003122
Iteration 218/1000 | Loss: 0.00003122
Iteration 219/1000 | Loss: 0.00003122
Iteration 220/1000 | Loss: 0.00003122
Iteration 221/1000 | Loss: 0.00003122
Iteration 222/1000 | Loss: 0.00003122
Iteration 223/1000 | Loss: 0.00003122
Iteration 224/1000 | Loss: 0.00003122
Iteration 225/1000 | Loss: 0.00003122
Iteration 226/1000 | Loss: 0.00003122
Iteration 227/1000 | Loss: 0.00003122
Iteration 228/1000 | Loss: 0.00003122
Iteration 229/1000 | Loss: 0.00003122
Iteration 230/1000 | Loss: 0.00003122
Iteration 231/1000 | Loss: 0.00003122
Iteration 232/1000 | Loss: 0.00003121
Iteration 233/1000 | Loss: 0.00003121
Iteration 234/1000 | Loss: 0.00003121
Iteration 235/1000 | Loss: 0.00003121
Iteration 236/1000 | Loss: 0.00003121
Iteration 237/1000 | Loss: 0.00003121
Iteration 238/1000 | Loss: 0.00003121
Iteration 239/1000 | Loss: 0.00003121
Iteration 240/1000 | Loss: 0.00003121
Iteration 241/1000 | Loss: 0.00003121
Iteration 242/1000 | Loss: 0.00003121
Iteration 243/1000 | Loss: 0.00003121
Iteration 244/1000 | Loss: 0.00003121
Iteration 245/1000 | Loss: 0.00003121
Iteration 246/1000 | Loss: 0.00003121
Iteration 247/1000 | Loss: 0.00003121
Iteration 248/1000 | Loss: 0.00003121
Iteration 249/1000 | Loss: 0.00003121
Iteration 250/1000 | Loss: 0.00003121
Iteration 251/1000 | Loss: 0.00003121
Iteration 252/1000 | Loss: 0.00003121
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [3.120945984846912e-05, 3.120945984846912e-05, 3.120945984846912e-05, 3.120945984846912e-05, 3.120945984846912e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.120945984846912e-05

Optimization complete. Final v2v error: 4.598264217376709 mm

Highest mean error: 6.7933478355407715 mm for frame 82

Lowest mean error: 3.435967445373535 mm for frame 30

Saving results

Total time: 54.16779160499573
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596392
Iteration 2/25 | Loss: 0.00142376
Iteration 3/25 | Loss: 0.00131426
Iteration 4/25 | Loss: 0.00130518
Iteration 5/25 | Loss: 0.00130386
Iteration 6/25 | Loss: 0.00130386
Iteration 7/25 | Loss: 0.00130386
Iteration 8/25 | Loss: 0.00130386
Iteration 9/25 | Loss: 0.00130386
Iteration 10/25 | Loss: 0.00130386
Iteration 11/25 | Loss: 0.00130386
Iteration 12/25 | Loss: 0.00130386
Iteration 13/25 | Loss: 0.00130386
Iteration 14/25 | Loss: 0.00130386
Iteration 15/25 | Loss: 0.00130386
Iteration 16/25 | Loss: 0.00130386
Iteration 17/25 | Loss: 0.00130386
Iteration 18/25 | Loss: 0.00130386
Iteration 19/25 | Loss: 0.00130386
Iteration 20/25 | Loss: 0.00130386
Iteration 21/25 | Loss: 0.00130386
Iteration 22/25 | Loss: 0.00130386
Iteration 23/25 | Loss: 0.00130386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013038619654253125, 0.0013038619654253125, 0.0013038619654253125, 0.0013038619654253125, 0.0013038619654253125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013038619654253125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.48293090
Iteration 2/25 | Loss: 0.00085042
Iteration 3/25 | Loss: 0.00085042
Iteration 4/25 | Loss: 0.00085042
Iteration 5/25 | Loss: 0.00085042
Iteration 6/25 | Loss: 0.00085042
Iteration 7/25 | Loss: 0.00085042
Iteration 8/25 | Loss: 0.00085042
Iteration 9/25 | Loss: 0.00085042
Iteration 10/25 | Loss: 0.00085042
Iteration 11/25 | Loss: 0.00085042
Iteration 12/25 | Loss: 0.00085042
Iteration 13/25 | Loss: 0.00085042
Iteration 14/25 | Loss: 0.00085042
Iteration 15/25 | Loss: 0.00085042
Iteration 16/25 | Loss: 0.00085042
Iteration 17/25 | Loss: 0.00085042
Iteration 18/25 | Loss: 0.00085042
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008504196302965283, 0.0008504196302965283, 0.0008504196302965283, 0.0008504196302965283, 0.0008504196302965283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008504196302965283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085042
Iteration 2/1000 | Loss: 0.00002629
Iteration 3/1000 | Loss: 0.00001954
Iteration 4/1000 | Loss: 0.00001817
Iteration 5/1000 | Loss: 0.00001743
Iteration 6/1000 | Loss: 0.00001690
Iteration 7/1000 | Loss: 0.00001663
Iteration 8/1000 | Loss: 0.00001658
Iteration 9/1000 | Loss: 0.00001633
Iteration 10/1000 | Loss: 0.00001614
Iteration 11/1000 | Loss: 0.00001610
Iteration 12/1000 | Loss: 0.00001594
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001567
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00001566
Iteration 17/1000 | Loss: 0.00001562
Iteration 18/1000 | Loss: 0.00001561
Iteration 19/1000 | Loss: 0.00001559
Iteration 20/1000 | Loss: 0.00001557
Iteration 21/1000 | Loss: 0.00001553
Iteration 22/1000 | Loss: 0.00001549
Iteration 23/1000 | Loss: 0.00001548
Iteration 24/1000 | Loss: 0.00001548
Iteration 25/1000 | Loss: 0.00001544
Iteration 26/1000 | Loss: 0.00001543
Iteration 27/1000 | Loss: 0.00001542
Iteration 28/1000 | Loss: 0.00001542
Iteration 29/1000 | Loss: 0.00001542
Iteration 30/1000 | Loss: 0.00001541
Iteration 31/1000 | Loss: 0.00001541
Iteration 32/1000 | Loss: 0.00001540
Iteration 33/1000 | Loss: 0.00001540
Iteration 34/1000 | Loss: 0.00001539
Iteration 35/1000 | Loss: 0.00001539
Iteration 36/1000 | Loss: 0.00001539
Iteration 37/1000 | Loss: 0.00001538
Iteration 38/1000 | Loss: 0.00001538
Iteration 39/1000 | Loss: 0.00001538
Iteration 40/1000 | Loss: 0.00001537
Iteration 41/1000 | Loss: 0.00001537
Iteration 42/1000 | Loss: 0.00001536
Iteration 43/1000 | Loss: 0.00001536
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001536
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001536
Iteration 48/1000 | Loss: 0.00001536
Iteration 49/1000 | Loss: 0.00001536
Iteration 50/1000 | Loss: 0.00001536
Iteration 51/1000 | Loss: 0.00001535
Iteration 52/1000 | Loss: 0.00001535
Iteration 53/1000 | Loss: 0.00001535
Iteration 54/1000 | Loss: 0.00001535
Iteration 55/1000 | Loss: 0.00001534
Iteration 56/1000 | Loss: 0.00001534
Iteration 57/1000 | Loss: 0.00001534
Iteration 58/1000 | Loss: 0.00001534
Iteration 59/1000 | Loss: 0.00001534
Iteration 60/1000 | Loss: 0.00001534
Iteration 61/1000 | Loss: 0.00001533
Iteration 62/1000 | Loss: 0.00001533
Iteration 63/1000 | Loss: 0.00001533
Iteration 64/1000 | Loss: 0.00001532
Iteration 65/1000 | Loss: 0.00001532
Iteration 66/1000 | Loss: 0.00001532
Iteration 67/1000 | Loss: 0.00001532
Iteration 68/1000 | Loss: 0.00001532
Iteration 69/1000 | Loss: 0.00001532
Iteration 70/1000 | Loss: 0.00001531
Iteration 71/1000 | Loss: 0.00001531
Iteration 72/1000 | Loss: 0.00001531
Iteration 73/1000 | Loss: 0.00001530
Iteration 74/1000 | Loss: 0.00001530
Iteration 75/1000 | Loss: 0.00001530
Iteration 76/1000 | Loss: 0.00001530
Iteration 77/1000 | Loss: 0.00001530
Iteration 78/1000 | Loss: 0.00001530
Iteration 79/1000 | Loss: 0.00001530
Iteration 80/1000 | Loss: 0.00001530
Iteration 81/1000 | Loss: 0.00001530
Iteration 82/1000 | Loss: 0.00001530
Iteration 83/1000 | Loss: 0.00001529
Iteration 84/1000 | Loss: 0.00001529
Iteration 85/1000 | Loss: 0.00001529
Iteration 86/1000 | Loss: 0.00001529
Iteration 87/1000 | Loss: 0.00001529
Iteration 88/1000 | Loss: 0.00001529
Iteration 89/1000 | Loss: 0.00001528
Iteration 90/1000 | Loss: 0.00001528
Iteration 91/1000 | Loss: 0.00001528
Iteration 92/1000 | Loss: 0.00001528
Iteration 93/1000 | Loss: 0.00001528
Iteration 94/1000 | Loss: 0.00001528
Iteration 95/1000 | Loss: 0.00001528
Iteration 96/1000 | Loss: 0.00001528
Iteration 97/1000 | Loss: 0.00001527
Iteration 98/1000 | Loss: 0.00001527
Iteration 99/1000 | Loss: 0.00001527
Iteration 100/1000 | Loss: 0.00001526
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001526
Iteration 103/1000 | Loss: 0.00001526
Iteration 104/1000 | Loss: 0.00001526
Iteration 105/1000 | Loss: 0.00001526
Iteration 106/1000 | Loss: 0.00001526
Iteration 107/1000 | Loss: 0.00001526
Iteration 108/1000 | Loss: 0.00001525
Iteration 109/1000 | Loss: 0.00001525
Iteration 110/1000 | Loss: 0.00001524
Iteration 111/1000 | Loss: 0.00001524
Iteration 112/1000 | Loss: 0.00001524
Iteration 113/1000 | Loss: 0.00001524
Iteration 114/1000 | Loss: 0.00001524
Iteration 115/1000 | Loss: 0.00001524
Iteration 116/1000 | Loss: 0.00001524
Iteration 117/1000 | Loss: 0.00001524
Iteration 118/1000 | Loss: 0.00001523
Iteration 119/1000 | Loss: 0.00001523
Iteration 120/1000 | Loss: 0.00001523
Iteration 121/1000 | Loss: 0.00001523
Iteration 122/1000 | Loss: 0.00001523
Iteration 123/1000 | Loss: 0.00001523
Iteration 124/1000 | Loss: 0.00001523
Iteration 125/1000 | Loss: 0.00001522
Iteration 126/1000 | Loss: 0.00001522
Iteration 127/1000 | Loss: 0.00001522
Iteration 128/1000 | Loss: 0.00001521
Iteration 129/1000 | Loss: 0.00001521
Iteration 130/1000 | Loss: 0.00001521
Iteration 131/1000 | Loss: 0.00001521
Iteration 132/1000 | Loss: 0.00001521
Iteration 133/1000 | Loss: 0.00001520
Iteration 134/1000 | Loss: 0.00001520
Iteration 135/1000 | Loss: 0.00001520
Iteration 136/1000 | Loss: 0.00001520
Iteration 137/1000 | Loss: 0.00001520
Iteration 138/1000 | Loss: 0.00001520
Iteration 139/1000 | Loss: 0.00001520
Iteration 140/1000 | Loss: 0.00001520
Iteration 141/1000 | Loss: 0.00001520
Iteration 142/1000 | Loss: 0.00001520
Iteration 143/1000 | Loss: 0.00001519
Iteration 144/1000 | Loss: 0.00001519
Iteration 145/1000 | Loss: 0.00001519
Iteration 146/1000 | Loss: 0.00001519
Iteration 147/1000 | Loss: 0.00001519
Iteration 148/1000 | Loss: 0.00001519
Iteration 149/1000 | Loss: 0.00001519
Iteration 150/1000 | Loss: 0.00001519
Iteration 151/1000 | Loss: 0.00001519
Iteration 152/1000 | Loss: 0.00001518
Iteration 153/1000 | Loss: 0.00001518
Iteration 154/1000 | Loss: 0.00001518
Iteration 155/1000 | Loss: 0.00001518
Iteration 156/1000 | Loss: 0.00001518
Iteration 157/1000 | Loss: 0.00001518
Iteration 158/1000 | Loss: 0.00001518
Iteration 159/1000 | Loss: 0.00001518
Iteration 160/1000 | Loss: 0.00001517
Iteration 161/1000 | Loss: 0.00001517
Iteration 162/1000 | Loss: 0.00001517
Iteration 163/1000 | Loss: 0.00001517
Iteration 164/1000 | Loss: 0.00001517
Iteration 165/1000 | Loss: 0.00001517
Iteration 166/1000 | Loss: 0.00001517
Iteration 167/1000 | Loss: 0.00001516
Iteration 168/1000 | Loss: 0.00001516
Iteration 169/1000 | Loss: 0.00001516
Iteration 170/1000 | Loss: 0.00001516
Iteration 171/1000 | Loss: 0.00001516
Iteration 172/1000 | Loss: 0.00001516
Iteration 173/1000 | Loss: 0.00001515
Iteration 174/1000 | Loss: 0.00001515
Iteration 175/1000 | Loss: 0.00001515
Iteration 176/1000 | Loss: 0.00001515
Iteration 177/1000 | Loss: 0.00001515
Iteration 178/1000 | Loss: 0.00001515
Iteration 179/1000 | Loss: 0.00001514
Iteration 180/1000 | Loss: 0.00001514
Iteration 181/1000 | Loss: 0.00001514
Iteration 182/1000 | Loss: 0.00001514
Iteration 183/1000 | Loss: 0.00001514
Iteration 184/1000 | Loss: 0.00001514
Iteration 185/1000 | Loss: 0.00001514
Iteration 186/1000 | Loss: 0.00001514
Iteration 187/1000 | Loss: 0.00001514
Iteration 188/1000 | Loss: 0.00001514
Iteration 189/1000 | Loss: 0.00001514
Iteration 190/1000 | Loss: 0.00001514
Iteration 191/1000 | Loss: 0.00001514
Iteration 192/1000 | Loss: 0.00001514
Iteration 193/1000 | Loss: 0.00001514
Iteration 194/1000 | Loss: 0.00001514
Iteration 195/1000 | Loss: 0.00001514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.5139494280447252e-05, 1.5139494280447252e-05, 1.5139494280447252e-05, 1.5139494280447252e-05, 1.5139494280447252e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5139494280447252e-05

Optimization complete. Final v2v error: 3.2803547382354736 mm

Highest mean error: 4.073236465454102 mm for frame 70

Lowest mean error: 2.939025402069092 mm for frame 19

Saving results

Total time: 43.439241886138916
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965004
Iteration 2/25 | Loss: 0.00274870
Iteration 3/25 | Loss: 0.00188906
Iteration 4/25 | Loss: 0.00180172
Iteration 5/25 | Loss: 0.00170796
Iteration 6/25 | Loss: 0.00152703
Iteration 7/25 | Loss: 0.00151268
Iteration 8/25 | Loss: 0.00142527
Iteration 9/25 | Loss: 0.00139892
Iteration 10/25 | Loss: 0.00138650
Iteration 11/25 | Loss: 0.00138188
Iteration 12/25 | Loss: 0.00137795
Iteration 13/25 | Loss: 0.00136957
Iteration 14/25 | Loss: 0.00137272
Iteration 15/25 | Loss: 0.00137542
Iteration 16/25 | Loss: 0.00137130
Iteration 17/25 | Loss: 0.00137124
Iteration 18/25 | Loss: 0.00136834
Iteration 19/25 | Loss: 0.00136721
Iteration 20/25 | Loss: 0.00137412
Iteration 21/25 | Loss: 0.00136755
Iteration 22/25 | Loss: 0.00136198
Iteration 23/25 | Loss: 0.00136403
Iteration 24/25 | Loss: 0.00137159
Iteration 25/25 | Loss: 0.00136499

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39091074
Iteration 2/25 | Loss: 0.00135114
Iteration 3/25 | Loss: 0.00133238
Iteration 4/25 | Loss: 0.00133238
Iteration 5/25 | Loss: 0.00133238
Iteration 6/25 | Loss: 0.00133238
Iteration 7/25 | Loss: 0.00133238
Iteration 8/25 | Loss: 0.00133238
Iteration 9/25 | Loss: 0.00133238
Iteration 10/25 | Loss: 0.00133238
Iteration 11/25 | Loss: 0.00133238
Iteration 12/25 | Loss: 0.00133238
Iteration 13/25 | Loss: 0.00133238
Iteration 14/25 | Loss: 0.00133238
Iteration 15/25 | Loss: 0.00133238
Iteration 16/25 | Loss: 0.00133238
Iteration 17/25 | Loss: 0.00133238
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013323756866157055, 0.0013323756866157055, 0.0013323756866157055, 0.0013323756866157055, 0.0013323756866157055]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013323756866157055

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133238
Iteration 2/1000 | Loss: 0.00062846
Iteration 3/1000 | Loss: 0.00015596
Iteration 4/1000 | Loss: 0.00037044
Iteration 5/1000 | Loss: 0.00051743
Iteration 6/1000 | Loss: 0.00031638
Iteration 7/1000 | Loss: 0.00024757
Iteration 8/1000 | Loss: 0.00034375
Iteration 9/1000 | Loss: 0.00020654
Iteration 10/1000 | Loss: 0.00017310
Iteration 11/1000 | Loss: 0.00017308
Iteration 12/1000 | Loss: 0.00013760
Iteration 13/1000 | Loss: 0.00015486
Iteration 14/1000 | Loss: 0.00013934
Iteration 15/1000 | Loss: 0.00015609
Iteration 16/1000 | Loss: 0.00024052
Iteration 17/1000 | Loss: 0.00015186
Iteration 18/1000 | Loss: 0.00026039
Iteration 19/1000 | Loss: 0.00030890
Iteration 20/1000 | Loss: 0.00020467
Iteration 21/1000 | Loss: 0.00006485
Iteration 22/1000 | Loss: 0.00005975
Iteration 23/1000 | Loss: 0.00005659
Iteration 24/1000 | Loss: 0.00005373
Iteration 25/1000 | Loss: 0.00041812
Iteration 26/1000 | Loss: 0.00037153
Iteration 27/1000 | Loss: 0.00043118
Iteration 28/1000 | Loss: 0.00029863
Iteration 29/1000 | Loss: 0.00021589
Iteration 30/1000 | Loss: 0.00019712
Iteration 31/1000 | Loss: 0.00031476
Iteration 32/1000 | Loss: 0.00022253
Iteration 33/1000 | Loss: 0.00015045
Iteration 34/1000 | Loss: 0.00014926
Iteration 35/1000 | Loss: 0.00006061
Iteration 36/1000 | Loss: 0.00005588
Iteration 37/1000 | Loss: 0.00005315
Iteration 38/1000 | Loss: 0.00027719
Iteration 39/1000 | Loss: 0.00035855
Iteration 40/1000 | Loss: 0.00006433
Iteration 41/1000 | Loss: 0.00005683
Iteration 42/1000 | Loss: 0.00005337
Iteration 43/1000 | Loss: 0.00004988
Iteration 44/1000 | Loss: 0.00015866
Iteration 45/1000 | Loss: 0.00005421
Iteration 46/1000 | Loss: 0.00005063
Iteration 47/1000 | Loss: 0.00005946
Iteration 48/1000 | Loss: 0.00004934
Iteration 49/1000 | Loss: 0.00006082
Iteration 50/1000 | Loss: 0.00005679
Iteration 51/1000 | Loss: 0.00004989
Iteration 52/1000 | Loss: 0.00004660
Iteration 53/1000 | Loss: 0.00004581
Iteration 54/1000 | Loss: 0.00070814
Iteration 55/1000 | Loss: 0.00004677
Iteration 56/1000 | Loss: 0.00004390
Iteration 57/1000 | Loss: 0.00064309
Iteration 58/1000 | Loss: 0.00004631
Iteration 59/1000 | Loss: 0.00004247
Iteration 60/1000 | Loss: 0.00004059
Iteration 61/1000 | Loss: 0.00004827
Iteration 62/1000 | Loss: 0.00004275
Iteration 63/1000 | Loss: 0.00004101
Iteration 64/1000 | Loss: 0.00003918
Iteration 65/1000 | Loss: 0.00003806
Iteration 66/1000 | Loss: 0.00003651
Iteration 67/1000 | Loss: 0.00003512
Iteration 68/1000 | Loss: 0.00003427
Iteration 69/1000 | Loss: 0.00003340
Iteration 70/1000 | Loss: 0.00003245
Iteration 71/1000 | Loss: 0.00004041
Iteration 72/1000 | Loss: 0.00003225
Iteration 73/1000 | Loss: 0.00003052
Iteration 74/1000 | Loss: 0.00003754
Iteration 75/1000 | Loss: 0.00003000
Iteration 76/1000 | Loss: 0.00002978
Iteration 77/1000 | Loss: 0.00002977
Iteration 78/1000 | Loss: 0.00002976
Iteration 79/1000 | Loss: 0.00002971
Iteration 80/1000 | Loss: 0.00002969
Iteration 81/1000 | Loss: 0.00002968
Iteration 82/1000 | Loss: 0.00002968
Iteration 83/1000 | Loss: 0.00002966
Iteration 84/1000 | Loss: 0.00002966
Iteration 85/1000 | Loss: 0.00002961
Iteration 86/1000 | Loss: 0.00002961
Iteration 87/1000 | Loss: 0.00002960
Iteration 88/1000 | Loss: 0.00002960
Iteration 89/1000 | Loss: 0.00002960
Iteration 90/1000 | Loss: 0.00002960
Iteration 91/1000 | Loss: 0.00002959
Iteration 92/1000 | Loss: 0.00002959
Iteration 93/1000 | Loss: 0.00002959
Iteration 94/1000 | Loss: 0.00002959
Iteration 95/1000 | Loss: 0.00002959
Iteration 96/1000 | Loss: 0.00002959
Iteration 97/1000 | Loss: 0.00002959
Iteration 98/1000 | Loss: 0.00002958
Iteration 99/1000 | Loss: 0.00002958
Iteration 100/1000 | Loss: 0.00002958
Iteration 101/1000 | Loss: 0.00002958
Iteration 102/1000 | Loss: 0.00002957
Iteration 103/1000 | Loss: 0.00002957
Iteration 104/1000 | Loss: 0.00002955
Iteration 105/1000 | Loss: 0.00002955
Iteration 106/1000 | Loss: 0.00002954
Iteration 107/1000 | Loss: 0.00002954
Iteration 108/1000 | Loss: 0.00002952
Iteration 109/1000 | Loss: 0.00002952
Iteration 110/1000 | Loss: 0.00002952
Iteration 111/1000 | Loss: 0.00002952
Iteration 112/1000 | Loss: 0.00002951
Iteration 113/1000 | Loss: 0.00002951
Iteration 114/1000 | Loss: 0.00002951
Iteration 115/1000 | Loss: 0.00002950
Iteration 116/1000 | Loss: 0.00002950
Iteration 117/1000 | Loss: 0.00002950
Iteration 118/1000 | Loss: 0.00002950
Iteration 119/1000 | Loss: 0.00002950
Iteration 120/1000 | Loss: 0.00002950
Iteration 121/1000 | Loss: 0.00002949
Iteration 122/1000 | Loss: 0.00002949
Iteration 123/1000 | Loss: 0.00002949
Iteration 124/1000 | Loss: 0.00002949
Iteration 125/1000 | Loss: 0.00002949
Iteration 126/1000 | Loss: 0.00002949
Iteration 127/1000 | Loss: 0.00002949
Iteration 128/1000 | Loss: 0.00002948
Iteration 129/1000 | Loss: 0.00002948
Iteration 130/1000 | Loss: 0.00002948
Iteration 131/1000 | Loss: 0.00002948
Iteration 132/1000 | Loss: 0.00002948
Iteration 133/1000 | Loss: 0.00002948
Iteration 134/1000 | Loss: 0.00002947
Iteration 135/1000 | Loss: 0.00002947
Iteration 136/1000 | Loss: 0.00002947
Iteration 137/1000 | Loss: 0.00002947
Iteration 138/1000 | Loss: 0.00002947
Iteration 139/1000 | Loss: 0.00002947
Iteration 140/1000 | Loss: 0.00002947
Iteration 141/1000 | Loss: 0.00002947
Iteration 142/1000 | Loss: 0.00002947
Iteration 143/1000 | Loss: 0.00002947
Iteration 144/1000 | Loss: 0.00002947
Iteration 145/1000 | Loss: 0.00002946
Iteration 146/1000 | Loss: 0.00002946
Iteration 147/1000 | Loss: 0.00002946
Iteration 148/1000 | Loss: 0.00002945
Iteration 149/1000 | Loss: 0.00002945
Iteration 150/1000 | Loss: 0.00002945
Iteration 151/1000 | Loss: 0.00002944
Iteration 152/1000 | Loss: 0.00002944
Iteration 153/1000 | Loss: 0.00002944
Iteration 154/1000 | Loss: 0.00002944
Iteration 155/1000 | Loss: 0.00002944
Iteration 156/1000 | Loss: 0.00002944
Iteration 157/1000 | Loss: 0.00002944
Iteration 158/1000 | Loss: 0.00002943
Iteration 159/1000 | Loss: 0.00002943
Iteration 160/1000 | Loss: 0.00002943
Iteration 161/1000 | Loss: 0.00002943
Iteration 162/1000 | Loss: 0.00002942
Iteration 163/1000 | Loss: 0.00002942
Iteration 164/1000 | Loss: 0.00002942
Iteration 165/1000 | Loss: 0.00002942
Iteration 166/1000 | Loss: 0.00002942
Iteration 167/1000 | Loss: 0.00002942
Iteration 168/1000 | Loss: 0.00002942
Iteration 169/1000 | Loss: 0.00002942
Iteration 170/1000 | Loss: 0.00002942
Iteration 171/1000 | Loss: 0.00002942
Iteration 172/1000 | Loss: 0.00002942
Iteration 173/1000 | Loss: 0.00002942
Iteration 174/1000 | Loss: 0.00002942
Iteration 175/1000 | Loss: 0.00002941
Iteration 176/1000 | Loss: 0.00002941
Iteration 177/1000 | Loss: 0.00002941
Iteration 178/1000 | Loss: 0.00002941
Iteration 179/1000 | Loss: 0.00002941
Iteration 180/1000 | Loss: 0.00002940
Iteration 181/1000 | Loss: 0.00002940
Iteration 182/1000 | Loss: 0.00002940
Iteration 183/1000 | Loss: 0.00002940
Iteration 184/1000 | Loss: 0.00002940
Iteration 185/1000 | Loss: 0.00002939
Iteration 186/1000 | Loss: 0.00069774
Iteration 187/1000 | Loss: 0.00056293
Iteration 188/1000 | Loss: 0.00055365
Iteration 189/1000 | Loss: 0.00004598
Iteration 190/1000 | Loss: 0.00003392
Iteration 191/1000 | Loss: 0.00003121
Iteration 192/1000 | Loss: 0.00002981
Iteration 193/1000 | Loss: 0.00006251
Iteration 194/1000 | Loss: 0.00036418
Iteration 195/1000 | Loss: 0.00005768
Iteration 196/1000 | Loss: 0.00004239
Iteration 197/1000 | Loss: 0.00014121
Iteration 198/1000 | Loss: 0.00002866
Iteration 199/1000 | Loss: 0.00002812
Iteration 200/1000 | Loss: 0.00002789
Iteration 201/1000 | Loss: 0.00002767
Iteration 202/1000 | Loss: 0.00002757
Iteration 203/1000 | Loss: 0.00002753
Iteration 204/1000 | Loss: 0.00002750
Iteration 205/1000 | Loss: 0.00002749
Iteration 206/1000 | Loss: 0.00003586
Iteration 207/1000 | Loss: 0.00002735
Iteration 208/1000 | Loss: 0.00002733
Iteration 209/1000 | Loss: 0.00002732
Iteration 210/1000 | Loss: 0.00002731
Iteration 211/1000 | Loss: 0.00002731
Iteration 212/1000 | Loss: 0.00002731
Iteration 213/1000 | Loss: 0.00002731
Iteration 214/1000 | Loss: 0.00002731
Iteration 215/1000 | Loss: 0.00002731
Iteration 216/1000 | Loss: 0.00002731
Iteration 217/1000 | Loss: 0.00002731
Iteration 218/1000 | Loss: 0.00002731
Iteration 219/1000 | Loss: 0.00002730
Iteration 220/1000 | Loss: 0.00002730
Iteration 221/1000 | Loss: 0.00002730
Iteration 222/1000 | Loss: 0.00002730
Iteration 223/1000 | Loss: 0.00002730
Iteration 224/1000 | Loss: 0.00002730
Iteration 225/1000 | Loss: 0.00002730
Iteration 226/1000 | Loss: 0.00002729
Iteration 227/1000 | Loss: 0.00002729
Iteration 228/1000 | Loss: 0.00002722
Iteration 229/1000 | Loss: 0.00002721
Iteration 230/1000 | Loss: 0.00002720
Iteration 231/1000 | Loss: 0.00002715
Iteration 232/1000 | Loss: 0.00002714
Iteration 233/1000 | Loss: 0.00002713
Iteration 234/1000 | Loss: 0.00002713
Iteration 235/1000 | Loss: 0.00002712
Iteration 236/1000 | Loss: 0.00002711
Iteration 237/1000 | Loss: 0.00002711
Iteration 238/1000 | Loss: 0.00002711
Iteration 239/1000 | Loss: 0.00002710
Iteration 240/1000 | Loss: 0.00002710
Iteration 241/1000 | Loss: 0.00002710
Iteration 242/1000 | Loss: 0.00002710
Iteration 243/1000 | Loss: 0.00002709
Iteration 244/1000 | Loss: 0.00002709
Iteration 245/1000 | Loss: 0.00002708
Iteration 246/1000 | Loss: 0.00002708
Iteration 247/1000 | Loss: 0.00002707
Iteration 248/1000 | Loss: 0.00002707
Iteration 249/1000 | Loss: 0.00002705
Iteration 250/1000 | Loss: 0.00002704
Iteration 251/1000 | Loss: 0.00002704
Iteration 252/1000 | Loss: 0.00002703
Iteration 253/1000 | Loss: 0.00002703
Iteration 254/1000 | Loss: 0.00002703
Iteration 255/1000 | Loss: 0.00002702
Iteration 256/1000 | Loss: 0.00002702
Iteration 257/1000 | Loss: 0.00002702
Iteration 258/1000 | Loss: 0.00002701
Iteration 259/1000 | Loss: 0.00002701
Iteration 260/1000 | Loss: 0.00002701
Iteration 261/1000 | Loss: 0.00002700
Iteration 262/1000 | Loss: 0.00002700
Iteration 263/1000 | Loss: 0.00002699
Iteration 264/1000 | Loss: 0.00002699
Iteration 265/1000 | Loss: 0.00002699
Iteration 266/1000 | Loss: 0.00002699
Iteration 267/1000 | Loss: 0.00002698
Iteration 268/1000 | Loss: 0.00002698
Iteration 269/1000 | Loss: 0.00002697
Iteration 270/1000 | Loss: 0.00002697
Iteration 271/1000 | Loss: 0.00002697
Iteration 272/1000 | Loss: 0.00002696
Iteration 273/1000 | Loss: 0.00002696
Iteration 274/1000 | Loss: 0.00002696
Iteration 275/1000 | Loss: 0.00002695
Iteration 276/1000 | Loss: 0.00002695
Iteration 277/1000 | Loss: 0.00002695
Iteration 278/1000 | Loss: 0.00002695
Iteration 279/1000 | Loss: 0.00002695
Iteration 280/1000 | Loss: 0.00002694
Iteration 281/1000 | Loss: 0.00002694
Iteration 282/1000 | Loss: 0.00002694
Iteration 283/1000 | Loss: 0.00002694
Iteration 284/1000 | Loss: 0.00002694
Iteration 285/1000 | Loss: 0.00002694
Iteration 286/1000 | Loss: 0.00002694
Iteration 287/1000 | Loss: 0.00002694
Iteration 288/1000 | Loss: 0.00002694
Iteration 289/1000 | Loss: 0.00002694
Iteration 290/1000 | Loss: 0.00002694
Iteration 291/1000 | Loss: 0.00002694
Iteration 292/1000 | Loss: 0.00002694
Iteration 293/1000 | Loss: 0.00002694
Iteration 294/1000 | Loss: 0.00002694
Iteration 295/1000 | Loss: 0.00002694
Iteration 296/1000 | Loss: 0.00002694
Iteration 297/1000 | Loss: 0.00002694
Iteration 298/1000 | Loss: 0.00002694
Iteration 299/1000 | Loss: 0.00002694
Iteration 300/1000 | Loss: 0.00002694
Iteration 301/1000 | Loss: 0.00002694
Iteration 302/1000 | Loss: 0.00002694
Iteration 303/1000 | Loss: 0.00002694
Iteration 304/1000 | Loss: 0.00002694
Iteration 305/1000 | Loss: 0.00002694
Iteration 306/1000 | Loss: 0.00002694
Iteration 307/1000 | Loss: 0.00002694
Iteration 308/1000 | Loss: 0.00002694
Iteration 309/1000 | Loss: 0.00002694
Iteration 310/1000 | Loss: 0.00002694
Iteration 311/1000 | Loss: 0.00002694
Iteration 312/1000 | Loss: 0.00002694
Iteration 313/1000 | Loss: 0.00002694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 313. Stopping optimization.
Last 5 losses: [2.6935838832287118e-05, 2.6935838832287118e-05, 2.6935838832287118e-05, 2.6935838832287118e-05, 2.6935838832287118e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6935838832287118e-05

Optimization complete. Final v2v error: 4.1416096687316895 mm

Highest mean error: 6.0071282386779785 mm for frame 72

Lowest mean error: 2.8287899494171143 mm for frame 1

Saving results

Total time: 191.33092617988586
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996526
Iteration 2/25 | Loss: 0.00288316
Iteration 3/25 | Loss: 0.00231135
Iteration 4/25 | Loss: 0.00216266
Iteration 5/25 | Loss: 0.00210161
Iteration 6/25 | Loss: 0.00204620
Iteration 7/25 | Loss: 0.00201910
Iteration 8/25 | Loss: 0.00200879
Iteration 9/25 | Loss: 0.00198089
Iteration 10/25 | Loss: 0.00197524
Iteration 11/25 | Loss: 0.00197369
Iteration 12/25 | Loss: 0.00197303
Iteration 13/25 | Loss: 0.00197268
Iteration 14/25 | Loss: 0.00197257
Iteration 15/25 | Loss: 0.00197256
Iteration 16/25 | Loss: 0.00197256
Iteration 17/25 | Loss: 0.00197255
Iteration 18/25 | Loss: 0.00197255
Iteration 19/25 | Loss: 0.00197255
Iteration 20/25 | Loss: 0.00197255
Iteration 21/25 | Loss: 0.00197255
Iteration 22/25 | Loss: 0.00197254
Iteration 23/25 | Loss: 0.00197254
Iteration 24/25 | Loss: 0.00197254
Iteration 25/25 | Loss: 0.00197254

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41505241
Iteration 2/25 | Loss: 0.00321776
Iteration 3/25 | Loss: 0.00321776
Iteration 4/25 | Loss: 0.00321776
Iteration 5/25 | Loss: 0.00321776
Iteration 6/25 | Loss: 0.00321776
Iteration 7/25 | Loss: 0.00321776
Iteration 8/25 | Loss: 0.00321776
Iteration 9/25 | Loss: 0.00321775
Iteration 10/25 | Loss: 0.00321775
Iteration 11/25 | Loss: 0.00321775
Iteration 12/25 | Loss: 0.00321775
Iteration 13/25 | Loss: 0.00321775
Iteration 14/25 | Loss: 0.00321775
Iteration 15/25 | Loss: 0.00321775
Iteration 16/25 | Loss: 0.00321775
Iteration 17/25 | Loss: 0.00321775
Iteration 18/25 | Loss: 0.00321775
Iteration 19/25 | Loss: 0.00321775
Iteration 20/25 | Loss: 0.00321775
Iteration 21/25 | Loss: 0.00321775
Iteration 22/25 | Loss: 0.00321775
Iteration 23/25 | Loss: 0.00321775
Iteration 24/25 | Loss: 0.00321775
Iteration 25/25 | Loss: 0.00321775

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00321775
Iteration 2/1000 | Loss: 0.00052645
Iteration 3/1000 | Loss: 0.00042126
Iteration 4/1000 | Loss: 0.00038358
Iteration 5/1000 | Loss: 0.00035497
Iteration 6/1000 | Loss: 0.00033130
Iteration 7/1000 | Loss: 0.00030997
Iteration 8/1000 | Loss: 0.00029607
Iteration 9/1000 | Loss: 0.00364801
Iteration 10/1000 | Loss: 0.01739545
Iteration 11/1000 | Loss: 0.00043703
Iteration 12/1000 | Loss: 0.00030055
Iteration 13/1000 | Loss: 0.00019488
Iteration 14/1000 | Loss: 0.00013694
Iteration 15/1000 | Loss: 0.00009134
Iteration 16/1000 | Loss: 0.00007089
Iteration 17/1000 | Loss: 0.00005475
Iteration 18/1000 | Loss: 0.00004427
Iteration 19/1000 | Loss: 0.00003735
Iteration 20/1000 | Loss: 0.00003364
Iteration 21/1000 | Loss: 0.00002980
Iteration 22/1000 | Loss: 0.00002750
Iteration 23/1000 | Loss: 0.00002562
Iteration 24/1000 | Loss: 0.00002382
Iteration 25/1000 | Loss: 0.00002265
Iteration 26/1000 | Loss: 0.00002168
Iteration 27/1000 | Loss: 0.00002108
Iteration 28/1000 | Loss: 0.00002064
Iteration 29/1000 | Loss: 0.00002036
Iteration 30/1000 | Loss: 0.00002006
Iteration 31/1000 | Loss: 0.00001993
Iteration 32/1000 | Loss: 0.00001991
Iteration 33/1000 | Loss: 0.00001977
Iteration 34/1000 | Loss: 0.00001974
Iteration 35/1000 | Loss: 0.00001973
Iteration 36/1000 | Loss: 0.00001973
Iteration 37/1000 | Loss: 0.00001972
Iteration 38/1000 | Loss: 0.00001963
Iteration 39/1000 | Loss: 0.00001961
Iteration 40/1000 | Loss: 0.00001960
Iteration 41/1000 | Loss: 0.00001960
Iteration 42/1000 | Loss: 0.00001960
Iteration 43/1000 | Loss: 0.00001960
Iteration 44/1000 | Loss: 0.00001960
Iteration 45/1000 | Loss: 0.00001960
Iteration 46/1000 | Loss: 0.00001960
Iteration 47/1000 | Loss: 0.00001959
Iteration 48/1000 | Loss: 0.00001956
Iteration 49/1000 | Loss: 0.00001956
Iteration 50/1000 | Loss: 0.00001956
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00001956
Iteration 53/1000 | Loss: 0.00001956
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001956
Iteration 56/1000 | Loss: 0.00001956
Iteration 57/1000 | Loss: 0.00001956
Iteration 58/1000 | Loss: 0.00001955
Iteration 59/1000 | Loss: 0.00001955
Iteration 60/1000 | Loss: 0.00001953
Iteration 61/1000 | Loss: 0.00001952
Iteration 62/1000 | Loss: 0.00001952
Iteration 63/1000 | Loss: 0.00001952
Iteration 64/1000 | Loss: 0.00001952
Iteration 65/1000 | Loss: 0.00001952
Iteration 66/1000 | Loss: 0.00001952
Iteration 67/1000 | Loss: 0.00001952
Iteration 68/1000 | Loss: 0.00001952
Iteration 69/1000 | Loss: 0.00001952
Iteration 70/1000 | Loss: 0.00001952
Iteration 71/1000 | Loss: 0.00001951
Iteration 72/1000 | Loss: 0.00001951
Iteration 73/1000 | Loss: 0.00001951
Iteration 74/1000 | Loss: 0.00001951
Iteration 75/1000 | Loss: 0.00001951
Iteration 76/1000 | Loss: 0.00001951
Iteration 77/1000 | Loss: 0.00001951
Iteration 78/1000 | Loss: 0.00001951
Iteration 79/1000 | Loss: 0.00001951
Iteration 80/1000 | Loss: 0.00001951
Iteration 81/1000 | Loss: 0.00001951
Iteration 82/1000 | Loss: 0.00001951
Iteration 83/1000 | Loss: 0.00001951
Iteration 84/1000 | Loss: 0.00001951
Iteration 85/1000 | Loss: 0.00001951
Iteration 86/1000 | Loss: 0.00001951
Iteration 87/1000 | Loss: 0.00001951
Iteration 88/1000 | Loss: 0.00001951
Iteration 89/1000 | Loss: 0.00001951
Iteration 90/1000 | Loss: 0.00001951
Iteration 91/1000 | Loss: 0.00001951
Iteration 92/1000 | Loss: 0.00001951
Iteration 93/1000 | Loss: 0.00001951
Iteration 94/1000 | Loss: 0.00001951
Iteration 95/1000 | Loss: 0.00001951
Iteration 96/1000 | Loss: 0.00001951
Iteration 97/1000 | Loss: 0.00001951
Iteration 98/1000 | Loss: 0.00001951
Iteration 99/1000 | Loss: 0.00001951
Iteration 100/1000 | Loss: 0.00001951
Iteration 101/1000 | Loss: 0.00001951
Iteration 102/1000 | Loss: 0.00001951
Iteration 103/1000 | Loss: 0.00001951
Iteration 104/1000 | Loss: 0.00001951
Iteration 105/1000 | Loss: 0.00001951
Iteration 106/1000 | Loss: 0.00001951
Iteration 107/1000 | Loss: 0.00001951
Iteration 108/1000 | Loss: 0.00001951
Iteration 109/1000 | Loss: 0.00001951
Iteration 110/1000 | Loss: 0.00001951
Iteration 111/1000 | Loss: 0.00001951
Iteration 112/1000 | Loss: 0.00001951
Iteration 113/1000 | Loss: 0.00001951
Iteration 114/1000 | Loss: 0.00001951
Iteration 115/1000 | Loss: 0.00001951
Iteration 116/1000 | Loss: 0.00001951
Iteration 117/1000 | Loss: 0.00001951
Iteration 118/1000 | Loss: 0.00001951
Iteration 119/1000 | Loss: 0.00001951
Iteration 120/1000 | Loss: 0.00001951
Iteration 121/1000 | Loss: 0.00001951
Iteration 122/1000 | Loss: 0.00001951
Iteration 123/1000 | Loss: 0.00001951
Iteration 124/1000 | Loss: 0.00001951
Iteration 125/1000 | Loss: 0.00001951
Iteration 126/1000 | Loss: 0.00001951
Iteration 127/1000 | Loss: 0.00001951
Iteration 128/1000 | Loss: 0.00001951
Iteration 129/1000 | Loss: 0.00001951
Iteration 130/1000 | Loss: 0.00001951
Iteration 131/1000 | Loss: 0.00001951
Iteration 132/1000 | Loss: 0.00001951
Iteration 133/1000 | Loss: 0.00001951
Iteration 134/1000 | Loss: 0.00001951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.9513721781549975e-05, 1.9513721781549975e-05, 1.9513721781549975e-05, 1.9513721781549975e-05, 1.9513721781549975e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9513721781549975e-05

Optimization complete. Final v2v error: 3.750563621520996 mm

Highest mean error: 3.8387343883514404 mm for frame 53

Lowest mean error: 3.704373836517334 mm for frame 65

Saving results

Total time: 73.18716144561768
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756752
Iteration 2/25 | Loss: 0.00150429
Iteration 3/25 | Loss: 0.00135755
Iteration 4/25 | Loss: 0.00135302
Iteration 5/25 | Loss: 0.00135190
Iteration 6/25 | Loss: 0.00135188
Iteration 7/25 | Loss: 0.00135188
Iteration 8/25 | Loss: 0.00135188
Iteration 9/25 | Loss: 0.00135188
Iteration 10/25 | Loss: 0.00135188
Iteration 11/25 | Loss: 0.00135188
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013518795603886247, 0.0013518795603886247, 0.0013518795603886247, 0.0013518795603886247, 0.0013518795603886247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013518795603886247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33928072
Iteration 2/25 | Loss: 0.00083698
Iteration 3/25 | Loss: 0.00083695
Iteration 4/25 | Loss: 0.00083695
Iteration 5/25 | Loss: 0.00083695
Iteration 6/25 | Loss: 0.00083695
Iteration 7/25 | Loss: 0.00083695
Iteration 8/25 | Loss: 0.00083695
Iteration 9/25 | Loss: 0.00083695
Iteration 10/25 | Loss: 0.00083695
Iteration 11/25 | Loss: 0.00083695
Iteration 12/25 | Loss: 0.00083695
Iteration 13/25 | Loss: 0.00083695
Iteration 14/25 | Loss: 0.00083695
Iteration 15/25 | Loss: 0.00083695
Iteration 16/25 | Loss: 0.00083695
Iteration 17/25 | Loss: 0.00083695
Iteration 18/25 | Loss: 0.00083695
Iteration 19/25 | Loss: 0.00083695
Iteration 20/25 | Loss: 0.00083695
Iteration 21/25 | Loss: 0.00083695
Iteration 22/25 | Loss: 0.00083695
Iteration 23/25 | Loss: 0.00083695
Iteration 24/25 | Loss: 0.00083695
Iteration 25/25 | Loss: 0.00083695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083695
Iteration 2/1000 | Loss: 0.00003958
Iteration 3/1000 | Loss: 0.00002732
Iteration 4/1000 | Loss: 0.00002436
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002261
Iteration 7/1000 | Loss: 0.00002187
Iteration 8/1000 | Loss: 0.00002152
Iteration 9/1000 | Loss: 0.00002118
Iteration 10/1000 | Loss: 0.00002094
Iteration 11/1000 | Loss: 0.00002076
Iteration 12/1000 | Loss: 0.00002068
Iteration 13/1000 | Loss: 0.00002068
Iteration 14/1000 | Loss: 0.00002057
Iteration 15/1000 | Loss: 0.00002057
Iteration 16/1000 | Loss: 0.00002054
Iteration 17/1000 | Loss: 0.00002053
Iteration 18/1000 | Loss: 0.00002052
Iteration 19/1000 | Loss: 0.00002051
Iteration 20/1000 | Loss: 0.00002048
Iteration 21/1000 | Loss: 0.00002047
Iteration 22/1000 | Loss: 0.00002046
Iteration 23/1000 | Loss: 0.00002045
Iteration 24/1000 | Loss: 0.00002044
Iteration 25/1000 | Loss: 0.00002043
Iteration 26/1000 | Loss: 0.00002041
Iteration 27/1000 | Loss: 0.00002040
Iteration 28/1000 | Loss: 0.00002040
Iteration 29/1000 | Loss: 0.00002036
Iteration 30/1000 | Loss: 0.00002035
Iteration 31/1000 | Loss: 0.00002033
Iteration 32/1000 | Loss: 0.00002033
Iteration 33/1000 | Loss: 0.00002033
Iteration 34/1000 | Loss: 0.00002032
Iteration 35/1000 | Loss: 0.00002032
Iteration 36/1000 | Loss: 0.00002032
Iteration 37/1000 | Loss: 0.00002032
Iteration 38/1000 | Loss: 0.00002031
Iteration 39/1000 | Loss: 0.00002030
Iteration 40/1000 | Loss: 0.00002028
Iteration 41/1000 | Loss: 0.00002028
Iteration 42/1000 | Loss: 0.00002028
Iteration 43/1000 | Loss: 0.00002027
Iteration 44/1000 | Loss: 0.00002025
Iteration 45/1000 | Loss: 0.00002025
Iteration 46/1000 | Loss: 0.00002024
Iteration 47/1000 | Loss: 0.00002024
Iteration 48/1000 | Loss: 0.00002024
Iteration 49/1000 | Loss: 0.00002023
Iteration 50/1000 | Loss: 0.00002023
Iteration 51/1000 | Loss: 0.00002023
Iteration 52/1000 | Loss: 0.00002022
Iteration 53/1000 | Loss: 0.00002022
Iteration 54/1000 | Loss: 0.00002022
Iteration 55/1000 | Loss: 0.00002022
Iteration 56/1000 | Loss: 0.00002022
Iteration 57/1000 | Loss: 0.00002022
Iteration 58/1000 | Loss: 0.00002022
Iteration 59/1000 | Loss: 0.00002022
Iteration 60/1000 | Loss: 0.00002022
Iteration 61/1000 | Loss: 0.00002021
Iteration 62/1000 | Loss: 0.00002021
Iteration 63/1000 | Loss: 0.00002021
Iteration 64/1000 | Loss: 0.00002021
Iteration 65/1000 | Loss: 0.00002021
Iteration 66/1000 | Loss: 0.00002021
Iteration 67/1000 | Loss: 0.00002020
Iteration 68/1000 | Loss: 0.00002020
Iteration 69/1000 | Loss: 0.00002020
Iteration 70/1000 | Loss: 0.00002019
Iteration 71/1000 | Loss: 0.00002017
Iteration 72/1000 | Loss: 0.00002017
Iteration 73/1000 | Loss: 0.00002016
Iteration 74/1000 | Loss: 0.00002016
Iteration 75/1000 | Loss: 0.00002015
Iteration 76/1000 | Loss: 0.00002015
Iteration 77/1000 | Loss: 0.00002014
Iteration 78/1000 | Loss: 0.00002014
Iteration 79/1000 | Loss: 0.00002014
Iteration 80/1000 | Loss: 0.00002014
Iteration 81/1000 | Loss: 0.00002013
Iteration 82/1000 | Loss: 0.00002013
Iteration 83/1000 | Loss: 0.00002013
Iteration 84/1000 | Loss: 0.00002012
Iteration 85/1000 | Loss: 0.00002012
Iteration 86/1000 | Loss: 0.00002012
Iteration 87/1000 | Loss: 0.00002012
Iteration 88/1000 | Loss: 0.00002012
Iteration 89/1000 | Loss: 0.00002012
Iteration 90/1000 | Loss: 0.00002011
Iteration 91/1000 | Loss: 0.00002011
Iteration 92/1000 | Loss: 0.00002011
Iteration 93/1000 | Loss: 0.00002011
Iteration 94/1000 | Loss: 0.00002011
Iteration 95/1000 | Loss: 0.00002011
Iteration 96/1000 | Loss: 0.00002010
Iteration 97/1000 | Loss: 0.00002010
Iteration 98/1000 | Loss: 0.00002009
Iteration 99/1000 | Loss: 0.00002009
Iteration 100/1000 | Loss: 0.00002009
Iteration 101/1000 | Loss: 0.00002009
Iteration 102/1000 | Loss: 0.00002009
Iteration 103/1000 | Loss: 0.00002009
Iteration 104/1000 | Loss: 0.00002009
Iteration 105/1000 | Loss: 0.00002009
Iteration 106/1000 | Loss: 0.00002008
Iteration 107/1000 | Loss: 0.00002008
Iteration 108/1000 | Loss: 0.00002008
Iteration 109/1000 | Loss: 0.00002008
Iteration 110/1000 | Loss: 0.00002008
Iteration 111/1000 | Loss: 0.00002008
Iteration 112/1000 | Loss: 0.00002008
Iteration 113/1000 | Loss: 0.00002008
Iteration 114/1000 | Loss: 0.00002007
Iteration 115/1000 | Loss: 0.00002007
Iteration 116/1000 | Loss: 0.00002007
Iteration 117/1000 | Loss: 0.00002007
Iteration 118/1000 | Loss: 0.00002007
Iteration 119/1000 | Loss: 0.00002007
Iteration 120/1000 | Loss: 0.00002007
Iteration 121/1000 | Loss: 0.00002007
Iteration 122/1000 | Loss: 0.00002007
Iteration 123/1000 | Loss: 0.00002007
Iteration 124/1000 | Loss: 0.00002007
Iteration 125/1000 | Loss: 0.00002007
Iteration 126/1000 | Loss: 0.00002007
Iteration 127/1000 | Loss: 0.00002007
Iteration 128/1000 | Loss: 0.00002007
Iteration 129/1000 | Loss: 0.00002006
Iteration 130/1000 | Loss: 0.00002006
Iteration 131/1000 | Loss: 0.00002006
Iteration 132/1000 | Loss: 0.00002006
Iteration 133/1000 | Loss: 0.00002006
Iteration 134/1000 | Loss: 0.00002006
Iteration 135/1000 | Loss: 0.00002006
Iteration 136/1000 | Loss: 0.00002006
Iteration 137/1000 | Loss: 0.00002006
Iteration 138/1000 | Loss: 0.00002006
Iteration 139/1000 | Loss: 0.00002006
Iteration 140/1000 | Loss: 0.00002006
Iteration 141/1000 | Loss: 0.00002006
Iteration 142/1000 | Loss: 0.00002006
Iteration 143/1000 | Loss: 0.00002006
Iteration 144/1000 | Loss: 0.00002006
Iteration 145/1000 | Loss: 0.00002006
Iteration 146/1000 | Loss: 0.00002006
Iteration 147/1000 | Loss: 0.00002006
Iteration 148/1000 | Loss: 0.00002006
Iteration 149/1000 | Loss: 0.00002005
Iteration 150/1000 | Loss: 0.00002005
Iteration 151/1000 | Loss: 0.00002005
Iteration 152/1000 | Loss: 0.00002005
Iteration 153/1000 | Loss: 0.00002005
Iteration 154/1000 | Loss: 0.00002005
Iteration 155/1000 | Loss: 0.00002005
Iteration 156/1000 | Loss: 0.00002005
Iteration 157/1000 | Loss: 0.00002005
Iteration 158/1000 | Loss: 0.00002005
Iteration 159/1000 | Loss: 0.00002005
Iteration 160/1000 | Loss: 0.00002005
Iteration 161/1000 | Loss: 0.00002005
Iteration 162/1000 | Loss: 0.00002005
Iteration 163/1000 | Loss: 0.00002005
Iteration 164/1000 | Loss: 0.00002005
Iteration 165/1000 | Loss: 0.00002005
Iteration 166/1000 | Loss: 0.00002005
Iteration 167/1000 | Loss: 0.00002005
Iteration 168/1000 | Loss: 0.00002005
Iteration 169/1000 | Loss: 0.00002005
Iteration 170/1000 | Loss: 0.00002004
Iteration 171/1000 | Loss: 0.00002004
Iteration 172/1000 | Loss: 0.00002004
Iteration 173/1000 | Loss: 0.00002004
Iteration 174/1000 | Loss: 0.00002004
Iteration 175/1000 | Loss: 0.00002004
Iteration 176/1000 | Loss: 0.00002004
Iteration 177/1000 | Loss: 0.00002004
Iteration 178/1000 | Loss: 0.00002004
Iteration 179/1000 | Loss: 0.00002004
Iteration 180/1000 | Loss: 0.00002004
Iteration 181/1000 | Loss: 0.00002004
Iteration 182/1000 | Loss: 0.00002004
Iteration 183/1000 | Loss: 0.00002004
Iteration 184/1000 | Loss: 0.00002004
Iteration 185/1000 | Loss: 0.00002004
Iteration 186/1000 | Loss: 0.00002004
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [2.0042425603605807e-05, 2.0042425603605807e-05, 2.0042425603605807e-05, 2.0042425603605807e-05, 2.0042425603605807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0042425603605807e-05

Optimization complete. Final v2v error: 3.7490034103393555 mm

Highest mean error: 3.9926347732543945 mm for frame 30

Lowest mean error: 3.6177585124969482 mm for frame 37

Saving results

Total time: 37.792561531066895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999645
Iteration 2/25 | Loss: 0.00289748
Iteration 3/25 | Loss: 0.00192305
Iteration 4/25 | Loss: 0.00167588
Iteration 5/25 | Loss: 0.00168522
Iteration 6/25 | Loss: 0.00161208
Iteration 7/25 | Loss: 0.00152492
Iteration 8/25 | Loss: 0.00147464
Iteration 9/25 | Loss: 0.00143581
Iteration 10/25 | Loss: 0.00140684
Iteration 11/25 | Loss: 0.00139076
Iteration 12/25 | Loss: 0.00136562
Iteration 13/25 | Loss: 0.00133904
Iteration 14/25 | Loss: 0.00132876
Iteration 15/25 | Loss: 0.00131752
Iteration 16/25 | Loss: 0.00131618
Iteration 17/25 | Loss: 0.00131114
Iteration 18/25 | Loss: 0.00130812
Iteration 19/25 | Loss: 0.00130664
Iteration 20/25 | Loss: 0.00130621
Iteration 21/25 | Loss: 0.00131046
Iteration 22/25 | Loss: 0.00130791
Iteration 23/25 | Loss: 0.00130524
Iteration 24/25 | Loss: 0.00130456
Iteration 25/25 | Loss: 0.00130442

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41530228
Iteration 2/25 | Loss: 0.00084878
Iteration 3/25 | Loss: 0.00078790
Iteration 4/25 | Loss: 0.00078790
Iteration 5/25 | Loss: 0.00078790
Iteration 6/25 | Loss: 0.00078790
Iteration 7/25 | Loss: 0.00078790
Iteration 8/25 | Loss: 0.00078790
Iteration 9/25 | Loss: 0.00078790
Iteration 10/25 | Loss: 0.00078790
Iteration 11/25 | Loss: 0.00078789
Iteration 12/25 | Loss: 0.00078789
Iteration 13/25 | Loss: 0.00078789
Iteration 14/25 | Loss: 0.00078789
Iteration 15/25 | Loss: 0.00078789
Iteration 16/25 | Loss: 0.00078789
Iteration 17/25 | Loss: 0.00078789
Iteration 18/25 | Loss: 0.00078789
Iteration 19/25 | Loss: 0.00078789
Iteration 20/25 | Loss: 0.00078789
Iteration 21/25 | Loss: 0.00078789
Iteration 22/25 | Loss: 0.00078789
Iteration 23/25 | Loss: 0.00078789
Iteration 24/25 | Loss: 0.00078789
Iteration 25/25 | Loss: 0.00078789

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078789
Iteration 2/1000 | Loss: 0.00014972
Iteration 3/1000 | Loss: 0.00004616
Iteration 4/1000 | Loss: 0.00004004
Iteration 5/1000 | Loss: 0.00003748
Iteration 6/1000 | Loss: 0.00015457
Iteration 7/1000 | Loss: 0.00012123
Iteration 8/1000 | Loss: 0.00014034
Iteration 9/1000 | Loss: 0.00011525
Iteration 10/1000 | Loss: 0.00008275
Iteration 11/1000 | Loss: 0.00005088
Iteration 12/1000 | Loss: 0.00003498
Iteration 13/1000 | Loss: 0.00003419
Iteration 14/1000 | Loss: 0.00013241
Iteration 15/1000 | Loss: 0.00008045
Iteration 16/1000 | Loss: 0.00004181
Iteration 17/1000 | Loss: 0.00003763
Iteration 18/1000 | Loss: 0.00003473
Iteration 19/1000 | Loss: 0.00010298
Iteration 20/1000 | Loss: 0.00003925
Iteration 21/1000 | Loss: 0.00009358
Iteration 22/1000 | Loss: 0.00003752
Iteration 23/1000 | Loss: 0.00008861
Iteration 24/1000 | Loss: 0.00003700
Iteration 25/1000 | Loss: 0.00007471
Iteration 26/1000 | Loss: 0.00003451
Iteration 27/1000 | Loss: 0.00003326
Iteration 28/1000 | Loss: 0.00003252
Iteration 29/1000 | Loss: 0.00003202
Iteration 30/1000 | Loss: 0.00035673
Iteration 31/1000 | Loss: 0.00026240
Iteration 32/1000 | Loss: 0.00007255
Iteration 33/1000 | Loss: 0.00007071
Iteration 34/1000 | Loss: 0.00003516
Iteration 35/1000 | Loss: 0.00003185
Iteration 36/1000 | Loss: 0.00015940
Iteration 37/1000 | Loss: 0.00029667
Iteration 38/1000 | Loss: 0.00004188
Iteration 39/1000 | Loss: 0.00005441
Iteration 40/1000 | Loss: 0.00003226
Iteration 41/1000 | Loss: 0.00003145
Iteration 42/1000 | Loss: 0.00020682
Iteration 43/1000 | Loss: 0.00018798
Iteration 44/1000 | Loss: 0.00023216
Iteration 45/1000 | Loss: 0.00030653
Iteration 46/1000 | Loss: 0.00041379
Iteration 47/1000 | Loss: 0.00051070
Iteration 48/1000 | Loss: 0.00004209
Iteration 49/1000 | Loss: 0.00071291
Iteration 50/1000 | Loss: 0.00003463
Iteration 51/1000 | Loss: 0.00003140
Iteration 52/1000 | Loss: 0.00003061
Iteration 53/1000 | Loss: 0.00002995
Iteration 54/1000 | Loss: 0.00002960
Iteration 55/1000 | Loss: 0.00002941
Iteration 56/1000 | Loss: 0.00002933
Iteration 57/1000 | Loss: 0.00002924
Iteration 58/1000 | Loss: 0.00002924
Iteration 59/1000 | Loss: 0.00002921
Iteration 60/1000 | Loss: 0.00002908
Iteration 61/1000 | Loss: 0.00002907
Iteration 62/1000 | Loss: 0.00002900
Iteration 63/1000 | Loss: 0.00034011
Iteration 64/1000 | Loss: 0.00087530
Iteration 65/1000 | Loss: 0.00104595
Iteration 66/1000 | Loss: 0.00030283
Iteration 67/1000 | Loss: 0.00034059
Iteration 68/1000 | Loss: 0.00019275
Iteration 69/1000 | Loss: 0.00015510
Iteration 70/1000 | Loss: 0.00003080
Iteration 71/1000 | Loss: 0.00002761
Iteration 72/1000 | Loss: 0.00002623
Iteration 73/1000 | Loss: 0.00002506
Iteration 74/1000 | Loss: 0.00002432
Iteration 75/1000 | Loss: 0.00002398
Iteration 76/1000 | Loss: 0.00002372
Iteration 77/1000 | Loss: 0.00002346
Iteration 78/1000 | Loss: 0.00002324
Iteration 79/1000 | Loss: 0.00002308
Iteration 80/1000 | Loss: 0.00002297
Iteration 81/1000 | Loss: 0.00002296
Iteration 82/1000 | Loss: 0.00002296
Iteration 83/1000 | Loss: 0.00002295
Iteration 84/1000 | Loss: 0.00002295
Iteration 85/1000 | Loss: 0.00002291
Iteration 86/1000 | Loss: 0.00002290
Iteration 87/1000 | Loss: 0.00002290
Iteration 88/1000 | Loss: 0.00002289
Iteration 89/1000 | Loss: 0.00002289
Iteration 90/1000 | Loss: 0.00002289
Iteration 91/1000 | Loss: 0.00002288
Iteration 92/1000 | Loss: 0.00002286
Iteration 93/1000 | Loss: 0.00002286
Iteration 94/1000 | Loss: 0.00002285
Iteration 95/1000 | Loss: 0.00002284
Iteration 96/1000 | Loss: 0.00002284
Iteration 97/1000 | Loss: 0.00002284
Iteration 98/1000 | Loss: 0.00002283
Iteration 99/1000 | Loss: 0.00002283
Iteration 100/1000 | Loss: 0.00002283
Iteration 101/1000 | Loss: 0.00002283
Iteration 102/1000 | Loss: 0.00002283
Iteration 103/1000 | Loss: 0.00002282
Iteration 104/1000 | Loss: 0.00002282
Iteration 105/1000 | Loss: 0.00002282
Iteration 106/1000 | Loss: 0.00002282
Iteration 107/1000 | Loss: 0.00002282
Iteration 108/1000 | Loss: 0.00002281
Iteration 109/1000 | Loss: 0.00002281
Iteration 110/1000 | Loss: 0.00002281
Iteration 111/1000 | Loss: 0.00002281
Iteration 112/1000 | Loss: 0.00002281
Iteration 113/1000 | Loss: 0.00002280
Iteration 114/1000 | Loss: 0.00002280
Iteration 115/1000 | Loss: 0.00002280
Iteration 116/1000 | Loss: 0.00002279
Iteration 117/1000 | Loss: 0.00002279
Iteration 118/1000 | Loss: 0.00002279
Iteration 119/1000 | Loss: 0.00002279
Iteration 120/1000 | Loss: 0.00002279
Iteration 121/1000 | Loss: 0.00002279
Iteration 122/1000 | Loss: 0.00002279
Iteration 123/1000 | Loss: 0.00002278
Iteration 124/1000 | Loss: 0.00002278
Iteration 125/1000 | Loss: 0.00002278
Iteration 126/1000 | Loss: 0.00002278
Iteration 127/1000 | Loss: 0.00002278
Iteration 128/1000 | Loss: 0.00002278
Iteration 129/1000 | Loss: 0.00002277
Iteration 130/1000 | Loss: 0.00002277
Iteration 131/1000 | Loss: 0.00002277
Iteration 132/1000 | Loss: 0.00002277
Iteration 133/1000 | Loss: 0.00002277
Iteration 134/1000 | Loss: 0.00002276
Iteration 135/1000 | Loss: 0.00002276
Iteration 136/1000 | Loss: 0.00002276
Iteration 137/1000 | Loss: 0.00002276
Iteration 138/1000 | Loss: 0.00002276
Iteration 139/1000 | Loss: 0.00002276
Iteration 140/1000 | Loss: 0.00002276
Iteration 141/1000 | Loss: 0.00002276
Iteration 142/1000 | Loss: 0.00002276
Iteration 143/1000 | Loss: 0.00002276
Iteration 144/1000 | Loss: 0.00002276
Iteration 145/1000 | Loss: 0.00002276
Iteration 146/1000 | Loss: 0.00002276
Iteration 147/1000 | Loss: 0.00002276
Iteration 148/1000 | Loss: 0.00002276
Iteration 149/1000 | Loss: 0.00002276
Iteration 150/1000 | Loss: 0.00002276
Iteration 151/1000 | Loss: 0.00002276
Iteration 152/1000 | Loss: 0.00002276
Iteration 153/1000 | Loss: 0.00002276
Iteration 154/1000 | Loss: 0.00002276
Iteration 155/1000 | Loss: 0.00002276
Iteration 156/1000 | Loss: 0.00002276
Iteration 157/1000 | Loss: 0.00002276
Iteration 158/1000 | Loss: 0.00002276
Iteration 159/1000 | Loss: 0.00002276
Iteration 160/1000 | Loss: 0.00002276
Iteration 161/1000 | Loss: 0.00002276
Iteration 162/1000 | Loss: 0.00002276
Iteration 163/1000 | Loss: 0.00002276
Iteration 164/1000 | Loss: 0.00002276
Iteration 165/1000 | Loss: 0.00002276
Iteration 166/1000 | Loss: 0.00002276
Iteration 167/1000 | Loss: 0.00002276
Iteration 168/1000 | Loss: 0.00002276
Iteration 169/1000 | Loss: 0.00002276
Iteration 170/1000 | Loss: 0.00002276
Iteration 171/1000 | Loss: 0.00002276
Iteration 172/1000 | Loss: 0.00002276
Iteration 173/1000 | Loss: 0.00002276
Iteration 174/1000 | Loss: 0.00002276
Iteration 175/1000 | Loss: 0.00002276
Iteration 176/1000 | Loss: 0.00002276
Iteration 177/1000 | Loss: 0.00002276
Iteration 178/1000 | Loss: 0.00002276
Iteration 179/1000 | Loss: 0.00002276
Iteration 180/1000 | Loss: 0.00002276
Iteration 181/1000 | Loss: 0.00002276
Iteration 182/1000 | Loss: 0.00002276
Iteration 183/1000 | Loss: 0.00002276
Iteration 184/1000 | Loss: 0.00002276
Iteration 185/1000 | Loss: 0.00002276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [2.2762227672501467e-05, 2.2762227672501467e-05, 2.2762227672501467e-05, 2.2762227672501467e-05, 2.2762227672501467e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2762227672501467e-05

Optimization complete. Final v2v error: 4.053630352020264 mm

Highest mean error: 4.571044445037842 mm for frame 132

Lowest mean error: 3.4308066368103027 mm for frame 3

Saving results

Total time: 157.82538890838623
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761842
Iteration 2/25 | Loss: 0.00169971
Iteration 3/25 | Loss: 0.00139232
Iteration 4/25 | Loss: 0.00133835
Iteration 5/25 | Loss: 0.00132862
Iteration 6/25 | Loss: 0.00132613
Iteration 7/25 | Loss: 0.00132605
Iteration 8/25 | Loss: 0.00132605
Iteration 9/25 | Loss: 0.00132605
Iteration 10/25 | Loss: 0.00132605
Iteration 11/25 | Loss: 0.00132605
Iteration 12/25 | Loss: 0.00132605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013260467676445842, 0.0013260467676445842, 0.0013260467676445842, 0.0013260467676445842, 0.0013260467676445842]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013260467676445842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47645330
Iteration 2/25 | Loss: 0.00058482
Iteration 3/25 | Loss: 0.00058482
Iteration 4/25 | Loss: 0.00058482
Iteration 5/25 | Loss: 0.00058482
Iteration 6/25 | Loss: 0.00058482
Iteration 7/25 | Loss: 0.00058482
Iteration 8/25 | Loss: 0.00058482
Iteration 9/25 | Loss: 0.00058482
Iteration 10/25 | Loss: 0.00058482
Iteration 11/25 | Loss: 0.00058482
Iteration 12/25 | Loss: 0.00058482
Iteration 13/25 | Loss: 0.00058482
Iteration 14/25 | Loss: 0.00058482
Iteration 15/25 | Loss: 0.00058482
Iteration 16/25 | Loss: 0.00058482
Iteration 17/25 | Loss: 0.00058482
Iteration 18/25 | Loss: 0.00058482
Iteration 19/25 | Loss: 0.00058482
Iteration 20/25 | Loss: 0.00058482
Iteration 21/25 | Loss: 0.00058482
Iteration 22/25 | Loss: 0.00058482
Iteration 23/25 | Loss: 0.00058482
Iteration 24/25 | Loss: 0.00058482
Iteration 25/25 | Loss: 0.00058482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058482
Iteration 2/1000 | Loss: 0.00007059
Iteration 3/1000 | Loss: 0.00004823
Iteration 4/1000 | Loss: 0.00003861
Iteration 5/1000 | Loss: 0.00003700
Iteration 6/1000 | Loss: 0.00003554
Iteration 7/1000 | Loss: 0.00003437
Iteration 8/1000 | Loss: 0.00003362
Iteration 9/1000 | Loss: 0.00003299
Iteration 10/1000 | Loss: 0.00003259
Iteration 11/1000 | Loss: 0.00003230
Iteration 12/1000 | Loss: 0.00003201
Iteration 13/1000 | Loss: 0.00003173
Iteration 14/1000 | Loss: 0.00003157
Iteration 15/1000 | Loss: 0.00003153
Iteration 16/1000 | Loss: 0.00003148
Iteration 17/1000 | Loss: 0.00003147
Iteration 18/1000 | Loss: 0.00003142
Iteration 19/1000 | Loss: 0.00003133
Iteration 20/1000 | Loss: 0.00003119
Iteration 21/1000 | Loss: 0.00003116
Iteration 22/1000 | Loss: 0.00003109
Iteration 23/1000 | Loss: 0.00003106
Iteration 24/1000 | Loss: 0.00003106
Iteration 25/1000 | Loss: 0.00003105
Iteration 26/1000 | Loss: 0.00003105
Iteration 27/1000 | Loss: 0.00003103
Iteration 28/1000 | Loss: 0.00003102
Iteration 29/1000 | Loss: 0.00003101
Iteration 30/1000 | Loss: 0.00003100
Iteration 31/1000 | Loss: 0.00003099
Iteration 32/1000 | Loss: 0.00003099
Iteration 33/1000 | Loss: 0.00003098
Iteration 34/1000 | Loss: 0.00003098
Iteration 35/1000 | Loss: 0.00003097
Iteration 36/1000 | Loss: 0.00003097
Iteration 37/1000 | Loss: 0.00003096
Iteration 38/1000 | Loss: 0.00003096
Iteration 39/1000 | Loss: 0.00003095
Iteration 40/1000 | Loss: 0.00003095
Iteration 41/1000 | Loss: 0.00003095
Iteration 42/1000 | Loss: 0.00003095
Iteration 43/1000 | Loss: 0.00003094
Iteration 44/1000 | Loss: 0.00003094
Iteration 45/1000 | Loss: 0.00003094
Iteration 46/1000 | Loss: 0.00003093
Iteration 47/1000 | Loss: 0.00003093
Iteration 48/1000 | Loss: 0.00003093
Iteration 49/1000 | Loss: 0.00003093
Iteration 50/1000 | Loss: 0.00003092
Iteration 51/1000 | Loss: 0.00003092
Iteration 52/1000 | Loss: 0.00003092
Iteration 53/1000 | Loss: 0.00003091
Iteration 54/1000 | Loss: 0.00003091
Iteration 55/1000 | Loss: 0.00003091
Iteration 56/1000 | Loss: 0.00003091
Iteration 57/1000 | Loss: 0.00003090
Iteration 58/1000 | Loss: 0.00003090
Iteration 59/1000 | Loss: 0.00003090
Iteration 60/1000 | Loss: 0.00003090
Iteration 61/1000 | Loss: 0.00003090
Iteration 62/1000 | Loss: 0.00003089
Iteration 63/1000 | Loss: 0.00003089
Iteration 64/1000 | Loss: 0.00003089
Iteration 65/1000 | Loss: 0.00003089
Iteration 66/1000 | Loss: 0.00003089
Iteration 67/1000 | Loss: 0.00003089
Iteration 68/1000 | Loss: 0.00003089
Iteration 69/1000 | Loss: 0.00003089
Iteration 70/1000 | Loss: 0.00003089
Iteration 71/1000 | Loss: 0.00003088
Iteration 72/1000 | Loss: 0.00003088
Iteration 73/1000 | Loss: 0.00003088
Iteration 74/1000 | Loss: 0.00003088
Iteration 75/1000 | Loss: 0.00003088
Iteration 76/1000 | Loss: 0.00003088
Iteration 77/1000 | Loss: 0.00003088
Iteration 78/1000 | Loss: 0.00003088
Iteration 79/1000 | Loss: 0.00003088
Iteration 80/1000 | Loss: 0.00003088
Iteration 81/1000 | Loss: 0.00003088
Iteration 82/1000 | Loss: 0.00003088
Iteration 83/1000 | Loss: 0.00003088
Iteration 84/1000 | Loss: 0.00003088
Iteration 85/1000 | Loss: 0.00003088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [3.0879171390552074e-05, 3.0879171390552074e-05, 3.0879171390552074e-05, 3.0879171390552074e-05, 3.0879171390552074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0879171390552074e-05

Optimization complete. Final v2v error: 4.613144397735596 mm

Highest mean error: 6.214130878448486 mm for frame 27

Lowest mean error: 3.5542378425598145 mm for frame 179

Saving results

Total time: 39.1254198551178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017546
Iteration 2/25 | Loss: 0.00246998
Iteration 3/25 | Loss: 0.00205577
Iteration 4/25 | Loss: 0.00196145
Iteration 5/25 | Loss: 0.00195613
Iteration 6/25 | Loss: 0.00190359
Iteration 7/25 | Loss: 0.00185933
Iteration 8/25 | Loss: 0.00185422
Iteration 9/25 | Loss: 0.00188947
Iteration 10/25 | Loss: 0.00190489
Iteration 11/25 | Loss: 0.00185832
Iteration 12/25 | Loss: 0.00182847
Iteration 13/25 | Loss: 0.00180990
Iteration 14/25 | Loss: 0.00179079
Iteration 15/25 | Loss: 0.00177457
Iteration 16/25 | Loss: 0.00177197
Iteration 17/25 | Loss: 0.00176390
Iteration 18/25 | Loss: 0.00176196
Iteration 19/25 | Loss: 0.00175845
Iteration 20/25 | Loss: 0.00175620
Iteration 21/25 | Loss: 0.00175653
Iteration 22/25 | Loss: 0.00175593
Iteration 23/25 | Loss: 0.00175339
Iteration 24/25 | Loss: 0.00175025
Iteration 25/25 | Loss: 0.00175062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93979293
Iteration 2/25 | Loss: 0.00322857
Iteration 3/25 | Loss: 0.00322855
Iteration 4/25 | Loss: 0.00322855
Iteration 5/25 | Loss: 0.00322855
Iteration 6/25 | Loss: 0.00322855
Iteration 7/25 | Loss: 0.00322855
Iteration 8/25 | Loss: 0.00322855
Iteration 9/25 | Loss: 0.00322855
Iteration 10/25 | Loss: 0.00322855
Iteration 11/25 | Loss: 0.00322855
Iteration 12/25 | Loss: 0.00322855
Iteration 13/25 | Loss: 0.00322855
Iteration 14/25 | Loss: 0.00322855
Iteration 15/25 | Loss: 0.00322855
Iteration 16/25 | Loss: 0.00322855
Iteration 17/25 | Loss: 0.00322855
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0032285472843796015, 0.0032285472843796015, 0.0032285472843796015, 0.0032285472843796015, 0.0032285472843796015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032285472843796015

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00322855
Iteration 2/1000 | Loss: 0.00038347
Iteration 3/1000 | Loss: 0.00060264
Iteration 4/1000 | Loss: 0.00041609
Iteration 5/1000 | Loss: 0.00037718
Iteration 6/1000 | Loss: 0.00039476
Iteration 7/1000 | Loss: 0.00025446
Iteration 8/1000 | Loss: 0.00056903
Iteration 9/1000 | Loss: 0.00032163
Iteration 10/1000 | Loss: 0.00032681
Iteration 11/1000 | Loss: 0.00068774
Iteration 12/1000 | Loss: 0.00060021
Iteration 13/1000 | Loss: 0.00064319
Iteration 14/1000 | Loss: 0.00138884
Iteration 15/1000 | Loss: 0.00106472
Iteration 16/1000 | Loss: 0.00050977
Iteration 17/1000 | Loss: 0.00058462
Iteration 18/1000 | Loss: 0.00048950
Iteration 19/1000 | Loss: 0.00050061
Iteration 20/1000 | Loss: 0.00073662
Iteration 21/1000 | Loss: 0.00062209
Iteration 22/1000 | Loss: 0.00054902
Iteration 23/1000 | Loss: 0.00050567
Iteration 24/1000 | Loss: 0.00077234
Iteration 25/1000 | Loss: 0.00060485
Iteration 26/1000 | Loss: 0.00034641
Iteration 27/1000 | Loss: 0.00040268
Iteration 28/1000 | Loss: 0.00059539
Iteration 29/1000 | Loss: 0.00079265
Iteration 30/1000 | Loss: 0.00065559
Iteration 31/1000 | Loss: 0.00072259
Iteration 32/1000 | Loss: 0.00067171
Iteration 33/1000 | Loss: 0.00046756
Iteration 34/1000 | Loss: 0.00066437
Iteration 35/1000 | Loss: 0.00042430
Iteration 36/1000 | Loss: 0.00019275
Iteration 37/1000 | Loss: 0.00018406
Iteration 38/1000 | Loss: 0.00044811
Iteration 39/1000 | Loss: 0.00120273
Iteration 40/1000 | Loss: 0.00063658
Iteration 41/1000 | Loss: 0.00041766
Iteration 42/1000 | Loss: 0.00037183
Iteration 43/1000 | Loss: 0.00082737
Iteration 44/1000 | Loss: 0.00095653
Iteration 45/1000 | Loss: 0.00087896
Iteration 46/1000 | Loss: 0.00053693
Iteration 47/1000 | Loss: 0.00040862
Iteration 48/1000 | Loss: 0.00040643
Iteration 49/1000 | Loss: 0.00044822
Iteration 50/1000 | Loss: 0.00042310
Iteration 51/1000 | Loss: 0.00022139
Iteration 52/1000 | Loss: 0.00018285
Iteration 53/1000 | Loss: 0.00044357
Iteration 54/1000 | Loss: 0.00128092
Iteration 55/1000 | Loss: 0.00115502
Iteration 56/1000 | Loss: 0.00046677
Iteration 57/1000 | Loss: 0.00039168
Iteration 58/1000 | Loss: 0.00118723
Iteration 59/1000 | Loss: 0.00099986
Iteration 60/1000 | Loss: 0.00149505
Iteration 61/1000 | Loss: 0.00148988
Iteration 62/1000 | Loss: 0.00077734
Iteration 63/1000 | Loss: 0.00025190
Iteration 64/1000 | Loss: 0.00041835
Iteration 65/1000 | Loss: 0.00065885
Iteration 66/1000 | Loss: 0.00133056
Iteration 67/1000 | Loss: 0.00083836
Iteration 68/1000 | Loss: 0.00126392
Iteration 69/1000 | Loss: 0.00017448
Iteration 70/1000 | Loss: 0.00044196
Iteration 71/1000 | Loss: 0.00114951
Iteration 72/1000 | Loss: 0.00068225
Iteration 73/1000 | Loss: 0.00040540
Iteration 74/1000 | Loss: 0.00039040
Iteration 75/1000 | Loss: 0.00069495
Iteration 76/1000 | Loss: 0.00040241
Iteration 77/1000 | Loss: 0.00046002
Iteration 78/1000 | Loss: 0.00035845
Iteration 79/1000 | Loss: 0.00070533
Iteration 80/1000 | Loss: 0.00749739
Iteration 81/1000 | Loss: 0.01557757
Iteration 82/1000 | Loss: 0.00620640
Iteration 83/1000 | Loss: 0.00169641
Iteration 84/1000 | Loss: 0.00659512
Iteration 85/1000 | Loss: 0.00848518
Iteration 86/1000 | Loss: 0.00249595
Iteration 87/1000 | Loss: 0.00228029
Iteration 88/1000 | Loss: 0.00593956
Iteration 89/1000 | Loss: 0.00608377
Iteration 90/1000 | Loss: 0.00061599
Iteration 91/1000 | Loss: 0.00483030
Iteration 92/1000 | Loss: 0.00491136
Iteration 93/1000 | Loss: 0.01165877
Iteration 94/1000 | Loss: 0.00187667
Iteration 95/1000 | Loss: 0.00218778
Iteration 96/1000 | Loss: 0.00422734
Iteration 97/1000 | Loss: 0.00302198
Iteration 98/1000 | Loss: 0.00156501
Iteration 99/1000 | Loss: 0.00149662
Iteration 100/1000 | Loss: 0.00299488
Iteration 101/1000 | Loss: 0.00556311
Iteration 102/1000 | Loss: 0.00493050
Iteration 103/1000 | Loss: 0.00532385
Iteration 104/1000 | Loss: 0.00309068
Iteration 105/1000 | Loss: 0.00259980
Iteration 106/1000 | Loss: 0.00043049
Iteration 107/1000 | Loss: 0.00020662
Iteration 108/1000 | Loss: 0.00017945
Iteration 109/1000 | Loss: 0.00017484
Iteration 110/1000 | Loss: 0.00092466
Iteration 111/1000 | Loss: 0.00065542
Iteration 112/1000 | Loss: 0.00049237
Iteration 113/1000 | Loss: 0.00031493
Iteration 114/1000 | Loss: 0.00010210
Iteration 115/1000 | Loss: 0.00007086
Iteration 116/1000 | Loss: 0.00005726
Iteration 117/1000 | Loss: 0.00004872
Iteration 118/1000 | Loss: 0.00004422
Iteration 119/1000 | Loss: 0.00004079
Iteration 120/1000 | Loss: 0.00003814
Iteration 121/1000 | Loss: 0.00003555
Iteration 122/1000 | Loss: 0.00003389
Iteration 123/1000 | Loss: 0.00003256
Iteration 124/1000 | Loss: 0.00003158
Iteration 125/1000 | Loss: 0.00003066
Iteration 126/1000 | Loss: 0.00008030
Iteration 127/1000 | Loss: 0.00003476
Iteration 128/1000 | Loss: 0.00003048
Iteration 129/1000 | Loss: 0.00002934
Iteration 130/1000 | Loss: 0.00002866
Iteration 131/1000 | Loss: 0.00002834
Iteration 132/1000 | Loss: 0.00002802
Iteration 133/1000 | Loss: 0.00002772
Iteration 134/1000 | Loss: 0.00002754
Iteration 135/1000 | Loss: 0.00014564
Iteration 136/1000 | Loss: 0.00017646
Iteration 137/1000 | Loss: 0.00002999
Iteration 138/1000 | Loss: 0.00002777
Iteration 139/1000 | Loss: 0.00002735
Iteration 140/1000 | Loss: 0.00002702
Iteration 141/1000 | Loss: 0.00002700
Iteration 142/1000 | Loss: 0.00002686
Iteration 143/1000 | Loss: 0.00002685
Iteration 144/1000 | Loss: 0.00002684
Iteration 145/1000 | Loss: 0.00002684
Iteration 146/1000 | Loss: 0.00002684
Iteration 147/1000 | Loss: 0.00002684
Iteration 148/1000 | Loss: 0.00002684
Iteration 149/1000 | Loss: 0.00002684
Iteration 150/1000 | Loss: 0.00002684
Iteration 151/1000 | Loss: 0.00002684
Iteration 152/1000 | Loss: 0.00002684
Iteration 153/1000 | Loss: 0.00012461
Iteration 154/1000 | Loss: 0.00002881
Iteration 155/1000 | Loss: 0.00002811
Iteration 156/1000 | Loss: 0.00002767
Iteration 157/1000 | Loss: 0.00002751
Iteration 158/1000 | Loss: 0.00007079
Iteration 159/1000 | Loss: 0.00014846
Iteration 160/1000 | Loss: 0.00011575
Iteration 161/1000 | Loss: 0.00012327
Iteration 162/1000 | Loss: 0.00006603
Iteration 163/1000 | Loss: 0.00014146
Iteration 164/1000 | Loss: 0.00016506
Iteration 165/1000 | Loss: 0.00012388
Iteration 166/1000 | Loss: 0.00012914
Iteration 167/1000 | Loss: 0.00013890
Iteration 168/1000 | Loss: 0.00008939
Iteration 169/1000 | Loss: 0.00027242
Iteration 170/1000 | Loss: 0.00019610
Iteration 171/1000 | Loss: 0.00018396
Iteration 172/1000 | Loss: 0.00021872
Iteration 173/1000 | Loss: 0.00016437
Iteration 174/1000 | Loss: 0.00005884
Iteration 175/1000 | Loss: 0.00017949
Iteration 176/1000 | Loss: 0.00013821
Iteration 177/1000 | Loss: 0.00008189
Iteration 178/1000 | Loss: 0.00010624
Iteration 179/1000 | Loss: 0.00014694
Iteration 180/1000 | Loss: 0.00021404
Iteration 181/1000 | Loss: 0.00003196
Iteration 182/1000 | Loss: 0.00002775
Iteration 183/1000 | Loss: 0.00002684
Iteration 184/1000 | Loss: 0.00002650
Iteration 185/1000 | Loss: 0.00002634
Iteration 186/1000 | Loss: 0.00002630
Iteration 187/1000 | Loss: 0.00002623
Iteration 188/1000 | Loss: 0.00002619
Iteration 189/1000 | Loss: 0.00002619
Iteration 190/1000 | Loss: 0.00002619
Iteration 191/1000 | Loss: 0.00002619
Iteration 192/1000 | Loss: 0.00002619
Iteration 193/1000 | Loss: 0.00002618
Iteration 194/1000 | Loss: 0.00002618
Iteration 195/1000 | Loss: 0.00002618
Iteration 196/1000 | Loss: 0.00002618
Iteration 197/1000 | Loss: 0.00002618
Iteration 198/1000 | Loss: 0.00002618
Iteration 199/1000 | Loss: 0.00002618
Iteration 200/1000 | Loss: 0.00002618
Iteration 201/1000 | Loss: 0.00002614
Iteration 202/1000 | Loss: 0.00002614
Iteration 203/1000 | Loss: 0.00002613
Iteration 204/1000 | Loss: 0.00002613
Iteration 205/1000 | Loss: 0.00002611
Iteration 206/1000 | Loss: 0.00002611
Iteration 207/1000 | Loss: 0.00002611
Iteration 208/1000 | Loss: 0.00002611
Iteration 209/1000 | Loss: 0.00002611
Iteration 210/1000 | Loss: 0.00002611
Iteration 211/1000 | Loss: 0.00002611
Iteration 212/1000 | Loss: 0.00002611
Iteration 213/1000 | Loss: 0.00002611
Iteration 214/1000 | Loss: 0.00002611
Iteration 215/1000 | Loss: 0.00002610
Iteration 216/1000 | Loss: 0.00002610
Iteration 217/1000 | Loss: 0.00002610
Iteration 218/1000 | Loss: 0.00002610
Iteration 219/1000 | Loss: 0.00002610
Iteration 220/1000 | Loss: 0.00002610
Iteration 221/1000 | Loss: 0.00002610
Iteration 222/1000 | Loss: 0.00002610
Iteration 223/1000 | Loss: 0.00002610
Iteration 224/1000 | Loss: 0.00002610
Iteration 225/1000 | Loss: 0.00002609
Iteration 226/1000 | Loss: 0.00002609
Iteration 227/1000 | Loss: 0.00002609
Iteration 228/1000 | Loss: 0.00002609
Iteration 229/1000 | Loss: 0.00002609
Iteration 230/1000 | Loss: 0.00002609
Iteration 231/1000 | Loss: 0.00002609
Iteration 232/1000 | Loss: 0.00002609
Iteration 233/1000 | Loss: 0.00002609
Iteration 234/1000 | Loss: 0.00002609
Iteration 235/1000 | Loss: 0.00002608
Iteration 236/1000 | Loss: 0.00002608
Iteration 237/1000 | Loss: 0.00002608
Iteration 238/1000 | Loss: 0.00002608
Iteration 239/1000 | Loss: 0.00002608
Iteration 240/1000 | Loss: 0.00002607
Iteration 241/1000 | Loss: 0.00002607
Iteration 242/1000 | Loss: 0.00002607
Iteration 243/1000 | Loss: 0.00002606
Iteration 244/1000 | Loss: 0.00002606
Iteration 245/1000 | Loss: 0.00002606
Iteration 246/1000 | Loss: 0.00002605
Iteration 247/1000 | Loss: 0.00002605
Iteration 248/1000 | Loss: 0.00002605
Iteration 249/1000 | Loss: 0.00002604
Iteration 250/1000 | Loss: 0.00002604
Iteration 251/1000 | Loss: 0.00002604
Iteration 252/1000 | Loss: 0.00002604
Iteration 253/1000 | Loss: 0.00002604
Iteration 254/1000 | Loss: 0.00002604
Iteration 255/1000 | Loss: 0.00002604
Iteration 256/1000 | Loss: 0.00002604
Iteration 257/1000 | Loss: 0.00002604
Iteration 258/1000 | Loss: 0.00002603
Iteration 259/1000 | Loss: 0.00002603
Iteration 260/1000 | Loss: 0.00002603
Iteration 261/1000 | Loss: 0.00002603
Iteration 262/1000 | Loss: 0.00002603
Iteration 263/1000 | Loss: 0.00002603
Iteration 264/1000 | Loss: 0.00002603
Iteration 265/1000 | Loss: 0.00002603
Iteration 266/1000 | Loss: 0.00002603
Iteration 267/1000 | Loss: 0.00002603
Iteration 268/1000 | Loss: 0.00002603
Iteration 269/1000 | Loss: 0.00002603
Iteration 270/1000 | Loss: 0.00002603
Iteration 271/1000 | Loss: 0.00002603
Iteration 272/1000 | Loss: 0.00002603
Iteration 273/1000 | Loss: 0.00002602
Iteration 274/1000 | Loss: 0.00002602
Iteration 275/1000 | Loss: 0.00002602
Iteration 276/1000 | Loss: 0.00002602
Iteration 277/1000 | Loss: 0.00002602
Iteration 278/1000 | Loss: 0.00002602
Iteration 279/1000 | Loss: 0.00002602
Iteration 280/1000 | Loss: 0.00002602
Iteration 281/1000 | Loss: 0.00002602
Iteration 282/1000 | Loss: 0.00002602
Iteration 283/1000 | Loss: 0.00002602
Iteration 284/1000 | Loss: 0.00002602
Iteration 285/1000 | Loss: 0.00002602
Iteration 286/1000 | Loss: 0.00002602
Iteration 287/1000 | Loss: 0.00002602
Iteration 288/1000 | Loss: 0.00002602
Iteration 289/1000 | Loss: 0.00002602
Iteration 290/1000 | Loss: 0.00002602
Iteration 291/1000 | Loss: 0.00002602
Iteration 292/1000 | Loss: 0.00002602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [2.6024930775747634e-05, 2.6024930775747634e-05, 2.6024930775747634e-05, 2.6024930775747634e-05, 2.6024930775747634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6024930775747634e-05

Optimization complete. Final v2v error: 4.274906635284424 mm

Highest mean error: 5.9493021965026855 mm for frame 104

Lowest mean error: 3.6704986095428467 mm for frame 28

Saving results

Total time: 318.0780005455017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817853
Iteration 2/25 | Loss: 0.00143576
Iteration 3/25 | Loss: 0.00132735
Iteration 4/25 | Loss: 0.00130635
Iteration 5/25 | Loss: 0.00130030
Iteration 6/25 | Loss: 0.00129916
Iteration 7/25 | Loss: 0.00129916
Iteration 8/25 | Loss: 0.00129916
Iteration 9/25 | Loss: 0.00129916
Iteration 10/25 | Loss: 0.00129916
Iteration 11/25 | Loss: 0.00129916
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012991572730243206, 0.0012991572730243206, 0.0012991572730243206, 0.0012991572730243206, 0.0012991572730243206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012991572730243206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.77739096
Iteration 2/25 | Loss: 0.00101876
Iteration 3/25 | Loss: 0.00101873
Iteration 4/25 | Loss: 0.00101872
Iteration 5/25 | Loss: 0.00101872
Iteration 6/25 | Loss: 0.00101872
Iteration 7/25 | Loss: 0.00101872
Iteration 8/25 | Loss: 0.00101872
Iteration 9/25 | Loss: 0.00101872
Iteration 10/25 | Loss: 0.00101872
Iteration 11/25 | Loss: 0.00101872
Iteration 12/25 | Loss: 0.00101872
Iteration 13/25 | Loss: 0.00101872
Iteration 14/25 | Loss: 0.00101872
Iteration 15/25 | Loss: 0.00101872
Iteration 16/25 | Loss: 0.00101872
Iteration 17/25 | Loss: 0.00101872
Iteration 18/25 | Loss: 0.00101872
Iteration 19/25 | Loss: 0.00101872
Iteration 20/25 | Loss: 0.00101872
Iteration 21/25 | Loss: 0.00101872
Iteration 22/25 | Loss: 0.00101872
Iteration 23/25 | Loss: 0.00101872
Iteration 24/25 | Loss: 0.00101872
Iteration 25/25 | Loss: 0.00101872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101872
Iteration 2/1000 | Loss: 0.00003655
Iteration 3/1000 | Loss: 0.00002325
Iteration 4/1000 | Loss: 0.00002099
Iteration 5/1000 | Loss: 0.00002010
Iteration 6/1000 | Loss: 0.00001930
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001819
Iteration 10/1000 | Loss: 0.00001806
Iteration 11/1000 | Loss: 0.00001791
Iteration 12/1000 | Loss: 0.00001766
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001756
Iteration 15/1000 | Loss: 0.00001752
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00001729
Iteration 19/1000 | Loss: 0.00001725
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001723
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001721
Iteration 26/1000 | Loss: 0.00001720
Iteration 27/1000 | Loss: 0.00001719
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001718
Iteration 30/1000 | Loss: 0.00001718
Iteration 31/1000 | Loss: 0.00001718
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001717
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001715
Iteration 37/1000 | Loss: 0.00001714
Iteration 38/1000 | Loss: 0.00001714
Iteration 39/1000 | Loss: 0.00001712
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001710
Iteration 42/1000 | Loss: 0.00001710
Iteration 43/1000 | Loss: 0.00001708
Iteration 44/1000 | Loss: 0.00001708
Iteration 45/1000 | Loss: 0.00001707
Iteration 46/1000 | Loss: 0.00001707
Iteration 47/1000 | Loss: 0.00001707
Iteration 48/1000 | Loss: 0.00001706
Iteration 49/1000 | Loss: 0.00001705
Iteration 50/1000 | Loss: 0.00001705
Iteration 51/1000 | Loss: 0.00001705
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001704
Iteration 56/1000 | Loss: 0.00001703
Iteration 57/1000 | Loss: 0.00001703
Iteration 58/1000 | Loss: 0.00001703
Iteration 59/1000 | Loss: 0.00001702
Iteration 60/1000 | Loss: 0.00001702
Iteration 61/1000 | Loss: 0.00001702
Iteration 62/1000 | Loss: 0.00001701
Iteration 63/1000 | Loss: 0.00001701
Iteration 64/1000 | Loss: 0.00001700
Iteration 65/1000 | Loss: 0.00001700
Iteration 66/1000 | Loss: 0.00001700
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001700
Iteration 70/1000 | Loss: 0.00001700
Iteration 71/1000 | Loss: 0.00001699
Iteration 72/1000 | Loss: 0.00001699
Iteration 73/1000 | Loss: 0.00001699
Iteration 74/1000 | Loss: 0.00001699
Iteration 75/1000 | Loss: 0.00001699
Iteration 76/1000 | Loss: 0.00001699
Iteration 77/1000 | Loss: 0.00001699
Iteration 78/1000 | Loss: 0.00001698
Iteration 79/1000 | Loss: 0.00001698
Iteration 80/1000 | Loss: 0.00001698
Iteration 81/1000 | Loss: 0.00001698
Iteration 82/1000 | Loss: 0.00001698
Iteration 83/1000 | Loss: 0.00001697
Iteration 84/1000 | Loss: 0.00001697
Iteration 85/1000 | Loss: 0.00001697
Iteration 86/1000 | Loss: 0.00001696
Iteration 87/1000 | Loss: 0.00001696
Iteration 88/1000 | Loss: 0.00001696
Iteration 89/1000 | Loss: 0.00001696
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001692
Iteration 98/1000 | Loss: 0.00001691
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001690
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001690
Iteration 110/1000 | Loss: 0.00001690
Iteration 111/1000 | Loss: 0.00001689
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001689
Iteration 116/1000 | Loss: 0.00001689
Iteration 117/1000 | Loss: 0.00001689
Iteration 118/1000 | Loss: 0.00001689
Iteration 119/1000 | Loss: 0.00001689
Iteration 120/1000 | Loss: 0.00001689
Iteration 121/1000 | Loss: 0.00001689
Iteration 122/1000 | Loss: 0.00001689
Iteration 123/1000 | Loss: 0.00001689
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001689
Iteration 126/1000 | Loss: 0.00001689
Iteration 127/1000 | Loss: 0.00001689
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001688
Iteration 130/1000 | Loss: 0.00001688
Iteration 131/1000 | Loss: 0.00001688
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001688
Iteration 134/1000 | Loss: 0.00001688
Iteration 135/1000 | Loss: 0.00001688
Iteration 136/1000 | Loss: 0.00001688
Iteration 137/1000 | Loss: 0.00001688
Iteration 138/1000 | Loss: 0.00001688
Iteration 139/1000 | Loss: 0.00001688
Iteration 140/1000 | Loss: 0.00001688
Iteration 141/1000 | Loss: 0.00001688
Iteration 142/1000 | Loss: 0.00001688
Iteration 143/1000 | Loss: 0.00001688
Iteration 144/1000 | Loss: 0.00001688
Iteration 145/1000 | Loss: 0.00001688
Iteration 146/1000 | Loss: 0.00001687
Iteration 147/1000 | Loss: 0.00001687
Iteration 148/1000 | Loss: 0.00001687
Iteration 149/1000 | Loss: 0.00001687
Iteration 150/1000 | Loss: 0.00001687
Iteration 151/1000 | Loss: 0.00001687
Iteration 152/1000 | Loss: 0.00001687
Iteration 153/1000 | Loss: 0.00001687
Iteration 154/1000 | Loss: 0.00001687
Iteration 155/1000 | Loss: 0.00001687
Iteration 156/1000 | Loss: 0.00001687
Iteration 157/1000 | Loss: 0.00001687
Iteration 158/1000 | Loss: 0.00001687
Iteration 159/1000 | Loss: 0.00001687
Iteration 160/1000 | Loss: 0.00001687
Iteration 161/1000 | Loss: 0.00001687
Iteration 162/1000 | Loss: 0.00001687
Iteration 163/1000 | Loss: 0.00001687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.6868765669642016e-05, 1.6868765669642016e-05, 1.6868765669642016e-05, 1.6868765669642016e-05, 1.6868765669642016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6868765669642016e-05

Optimization complete. Final v2v error: 3.470792770385742 mm

Highest mean error: 3.7215418815612793 mm for frame 112

Lowest mean error: 3.142859697341919 mm for frame 2

Saving results

Total time: 38.72395181655884
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763006
Iteration 2/25 | Loss: 0.00166668
Iteration 3/25 | Loss: 0.00140764
Iteration 4/25 | Loss: 0.00139197
Iteration 5/25 | Loss: 0.00139176
Iteration 6/25 | Loss: 0.00139176
Iteration 7/25 | Loss: 0.00139176
Iteration 8/25 | Loss: 0.00139176
Iteration 9/25 | Loss: 0.00139176
Iteration 10/25 | Loss: 0.00139176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013917606556788087, 0.0013917606556788087, 0.0013917606556788087, 0.0013917606556788087, 0.0013917606556788087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013917606556788087

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50574195
Iteration 2/25 | Loss: 0.00086560
Iteration 3/25 | Loss: 0.00086557
Iteration 4/25 | Loss: 0.00086557
Iteration 5/25 | Loss: 0.00086557
Iteration 6/25 | Loss: 0.00086557
Iteration 7/25 | Loss: 0.00086557
Iteration 8/25 | Loss: 0.00086557
Iteration 9/25 | Loss: 0.00086557
Iteration 10/25 | Loss: 0.00086557
Iteration 11/25 | Loss: 0.00086557
Iteration 12/25 | Loss: 0.00086557
Iteration 13/25 | Loss: 0.00086557
Iteration 14/25 | Loss: 0.00086557
Iteration 15/25 | Loss: 0.00086557
Iteration 16/25 | Loss: 0.00086557
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008655705023556948, 0.0008655705023556948, 0.0008655705023556948, 0.0008655705023556948, 0.0008655705023556948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008655705023556948

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086557
Iteration 2/1000 | Loss: 0.00003969
Iteration 3/1000 | Loss: 0.00002972
Iteration 4/1000 | Loss: 0.00002774
Iteration 5/1000 | Loss: 0.00002671
Iteration 6/1000 | Loss: 0.00002615
Iteration 7/1000 | Loss: 0.00002569
Iteration 8/1000 | Loss: 0.00002526
Iteration 9/1000 | Loss: 0.00002498
Iteration 10/1000 | Loss: 0.00002493
Iteration 11/1000 | Loss: 0.00002485
Iteration 12/1000 | Loss: 0.00002464
Iteration 13/1000 | Loss: 0.00002443
Iteration 14/1000 | Loss: 0.00002427
Iteration 15/1000 | Loss: 0.00002414
Iteration 16/1000 | Loss: 0.00002410
Iteration 17/1000 | Loss: 0.00002409
Iteration 18/1000 | Loss: 0.00002403
Iteration 19/1000 | Loss: 0.00002402
Iteration 20/1000 | Loss: 0.00002402
Iteration 21/1000 | Loss: 0.00002401
Iteration 22/1000 | Loss: 0.00002401
Iteration 23/1000 | Loss: 0.00002401
Iteration 24/1000 | Loss: 0.00002400
Iteration 25/1000 | Loss: 0.00002392
Iteration 26/1000 | Loss: 0.00002390
Iteration 27/1000 | Loss: 0.00002382
Iteration 28/1000 | Loss: 0.00002378
Iteration 29/1000 | Loss: 0.00002377
Iteration 30/1000 | Loss: 0.00002376
Iteration 31/1000 | Loss: 0.00002374
Iteration 32/1000 | Loss: 0.00002374
Iteration 33/1000 | Loss: 0.00002373
Iteration 34/1000 | Loss: 0.00002373
Iteration 35/1000 | Loss: 0.00002373
Iteration 36/1000 | Loss: 0.00002373
Iteration 37/1000 | Loss: 0.00002373
Iteration 38/1000 | Loss: 0.00002372
Iteration 39/1000 | Loss: 0.00002371
Iteration 40/1000 | Loss: 0.00002371
Iteration 41/1000 | Loss: 0.00002370
Iteration 42/1000 | Loss: 0.00002370
Iteration 43/1000 | Loss: 0.00002370
Iteration 44/1000 | Loss: 0.00002369
Iteration 45/1000 | Loss: 0.00002369
Iteration 46/1000 | Loss: 0.00002368
Iteration 47/1000 | Loss: 0.00002368
Iteration 48/1000 | Loss: 0.00002368
Iteration 49/1000 | Loss: 0.00002368
Iteration 50/1000 | Loss: 0.00002368
Iteration 51/1000 | Loss: 0.00002368
Iteration 52/1000 | Loss: 0.00002368
Iteration 53/1000 | Loss: 0.00002368
Iteration 54/1000 | Loss: 0.00002367
Iteration 55/1000 | Loss: 0.00002366
Iteration 56/1000 | Loss: 0.00002366
Iteration 57/1000 | Loss: 0.00002366
Iteration 58/1000 | Loss: 0.00002366
Iteration 59/1000 | Loss: 0.00002365
Iteration 60/1000 | Loss: 0.00002365
Iteration 61/1000 | Loss: 0.00002365
Iteration 62/1000 | Loss: 0.00002365
Iteration 63/1000 | Loss: 0.00002365
Iteration 64/1000 | Loss: 0.00002365
Iteration 65/1000 | Loss: 0.00002365
Iteration 66/1000 | Loss: 0.00002365
Iteration 67/1000 | Loss: 0.00002365
Iteration 68/1000 | Loss: 0.00002365
Iteration 69/1000 | Loss: 0.00002364
Iteration 70/1000 | Loss: 0.00002364
Iteration 71/1000 | Loss: 0.00002364
Iteration 72/1000 | Loss: 0.00002364
Iteration 73/1000 | Loss: 0.00002363
Iteration 74/1000 | Loss: 0.00002363
Iteration 75/1000 | Loss: 0.00002363
Iteration 76/1000 | Loss: 0.00002363
Iteration 77/1000 | Loss: 0.00002363
Iteration 78/1000 | Loss: 0.00002362
Iteration 79/1000 | Loss: 0.00002362
Iteration 80/1000 | Loss: 0.00002362
Iteration 81/1000 | Loss: 0.00002361
Iteration 82/1000 | Loss: 0.00002361
Iteration 83/1000 | Loss: 0.00002361
Iteration 84/1000 | Loss: 0.00002361
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002360
Iteration 88/1000 | Loss: 0.00002360
Iteration 89/1000 | Loss: 0.00002360
Iteration 90/1000 | Loss: 0.00002360
Iteration 91/1000 | Loss: 0.00002360
Iteration 92/1000 | Loss: 0.00002360
Iteration 93/1000 | Loss: 0.00002360
Iteration 94/1000 | Loss: 0.00002360
Iteration 95/1000 | Loss: 0.00002360
Iteration 96/1000 | Loss: 0.00002360
Iteration 97/1000 | Loss: 0.00002360
Iteration 98/1000 | Loss: 0.00002360
Iteration 99/1000 | Loss: 0.00002360
Iteration 100/1000 | Loss: 0.00002360
Iteration 101/1000 | Loss: 0.00002360
Iteration 102/1000 | Loss: 0.00002360
Iteration 103/1000 | Loss: 0.00002360
Iteration 104/1000 | Loss: 0.00002360
Iteration 105/1000 | Loss: 0.00002360
Iteration 106/1000 | Loss: 0.00002360
Iteration 107/1000 | Loss: 0.00002360
Iteration 108/1000 | Loss: 0.00002360
Iteration 109/1000 | Loss: 0.00002360
Iteration 110/1000 | Loss: 0.00002360
Iteration 111/1000 | Loss: 0.00002360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.3598271582159214e-05, 2.3598271582159214e-05, 2.3598271582159214e-05, 2.3598271582159214e-05, 2.3598271582159214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3598271582159214e-05

Optimization complete. Final v2v error: 4.071320056915283 mm

Highest mean error: 4.189296722412109 mm for frame 49

Lowest mean error: 3.9094724655151367 mm for frame 212

Saving results

Total time: 38.58934807777405
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396430
Iteration 2/25 | Loss: 0.00137683
Iteration 3/25 | Loss: 0.00126289
Iteration 4/25 | Loss: 0.00125334
Iteration 5/25 | Loss: 0.00125040
Iteration 6/25 | Loss: 0.00125040
Iteration 7/25 | Loss: 0.00125040
Iteration 8/25 | Loss: 0.00125040
Iteration 9/25 | Loss: 0.00125040
Iteration 10/25 | Loss: 0.00125040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012504037003964186, 0.0012504037003964186, 0.0012504037003964186, 0.0012504037003964186, 0.0012504037003964186]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012504037003964186

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71135461
Iteration 2/25 | Loss: 0.00079604
Iteration 3/25 | Loss: 0.00079603
Iteration 4/25 | Loss: 0.00079603
Iteration 5/25 | Loss: 0.00079603
Iteration 6/25 | Loss: 0.00079603
Iteration 7/25 | Loss: 0.00079603
Iteration 8/25 | Loss: 0.00079603
Iteration 9/25 | Loss: 0.00079603
Iteration 10/25 | Loss: 0.00079603
Iteration 11/25 | Loss: 0.00079603
Iteration 12/25 | Loss: 0.00079603
Iteration 13/25 | Loss: 0.00079603
Iteration 14/25 | Loss: 0.00079603
Iteration 15/25 | Loss: 0.00079603
Iteration 16/25 | Loss: 0.00079603
Iteration 17/25 | Loss: 0.00079603
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007960302755236626, 0.0007960302755236626, 0.0007960302755236626, 0.0007960302755236626, 0.0007960302755236626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007960302755236626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079603
Iteration 2/1000 | Loss: 0.00003176
Iteration 3/1000 | Loss: 0.00001794
Iteration 4/1000 | Loss: 0.00001626
Iteration 5/1000 | Loss: 0.00001495
Iteration 6/1000 | Loss: 0.00001416
Iteration 7/1000 | Loss: 0.00001359
Iteration 8/1000 | Loss: 0.00001326
Iteration 9/1000 | Loss: 0.00001298
Iteration 10/1000 | Loss: 0.00001257
Iteration 11/1000 | Loss: 0.00001240
Iteration 12/1000 | Loss: 0.00001238
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001227
Iteration 15/1000 | Loss: 0.00001219
Iteration 16/1000 | Loss: 0.00001216
Iteration 17/1000 | Loss: 0.00001215
Iteration 18/1000 | Loss: 0.00001215
Iteration 19/1000 | Loss: 0.00001214
Iteration 20/1000 | Loss: 0.00001213
Iteration 21/1000 | Loss: 0.00001213
Iteration 22/1000 | Loss: 0.00001212
Iteration 23/1000 | Loss: 0.00001212
Iteration 24/1000 | Loss: 0.00001211
Iteration 25/1000 | Loss: 0.00001211
Iteration 26/1000 | Loss: 0.00001210
Iteration 27/1000 | Loss: 0.00001210
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001207
Iteration 32/1000 | Loss: 0.00001207
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001206
Iteration 35/1000 | Loss: 0.00001206
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001205
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001204
Iteration 41/1000 | Loss: 0.00001204
Iteration 42/1000 | Loss: 0.00001203
Iteration 43/1000 | Loss: 0.00001203
Iteration 44/1000 | Loss: 0.00001202
Iteration 45/1000 | Loss: 0.00001202
Iteration 46/1000 | Loss: 0.00001202
Iteration 47/1000 | Loss: 0.00001202
Iteration 48/1000 | Loss: 0.00001202
Iteration 49/1000 | Loss: 0.00001202
Iteration 50/1000 | Loss: 0.00001202
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001202
Iteration 53/1000 | Loss: 0.00001202
Iteration 54/1000 | Loss: 0.00001202
Iteration 55/1000 | Loss: 0.00001201
Iteration 56/1000 | Loss: 0.00001201
Iteration 57/1000 | Loss: 0.00001201
Iteration 58/1000 | Loss: 0.00001201
Iteration 59/1000 | Loss: 0.00001201
Iteration 60/1000 | Loss: 0.00001201
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001201
Iteration 67/1000 | Loss: 0.00001201
Iteration 68/1000 | Loss: 0.00001200
Iteration 69/1000 | Loss: 0.00001200
Iteration 70/1000 | Loss: 0.00001200
Iteration 71/1000 | Loss: 0.00001200
Iteration 72/1000 | Loss: 0.00001199
Iteration 73/1000 | Loss: 0.00001199
Iteration 74/1000 | Loss: 0.00001197
Iteration 75/1000 | Loss: 0.00001196
Iteration 76/1000 | Loss: 0.00001196
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001193
Iteration 80/1000 | Loss: 0.00001193
Iteration 81/1000 | Loss: 0.00001193
Iteration 82/1000 | Loss: 0.00001193
Iteration 83/1000 | Loss: 0.00001193
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001193
Iteration 87/1000 | Loss: 0.00001193
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001190
Iteration 92/1000 | Loss: 0.00001190
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001189
Iteration 96/1000 | Loss: 0.00001189
Iteration 97/1000 | Loss: 0.00001189
Iteration 98/1000 | Loss: 0.00001189
Iteration 99/1000 | Loss: 0.00001188
Iteration 100/1000 | Loss: 0.00001188
Iteration 101/1000 | Loss: 0.00001188
Iteration 102/1000 | Loss: 0.00001188
Iteration 103/1000 | Loss: 0.00001188
Iteration 104/1000 | Loss: 0.00001188
Iteration 105/1000 | Loss: 0.00001187
Iteration 106/1000 | Loss: 0.00001187
Iteration 107/1000 | Loss: 0.00001187
Iteration 108/1000 | Loss: 0.00001187
Iteration 109/1000 | Loss: 0.00001187
Iteration 110/1000 | Loss: 0.00001186
Iteration 111/1000 | Loss: 0.00001186
Iteration 112/1000 | Loss: 0.00001186
Iteration 113/1000 | Loss: 0.00001186
Iteration 114/1000 | Loss: 0.00001186
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001183
Iteration 124/1000 | Loss: 0.00001183
Iteration 125/1000 | Loss: 0.00001182
Iteration 126/1000 | Loss: 0.00001182
Iteration 127/1000 | Loss: 0.00001182
Iteration 128/1000 | Loss: 0.00001182
Iteration 129/1000 | Loss: 0.00001182
Iteration 130/1000 | Loss: 0.00001181
Iteration 131/1000 | Loss: 0.00001181
Iteration 132/1000 | Loss: 0.00001181
Iteration 133/1000 | Loss: 0.00001180
Iteration 134/1000 | Loss: 0.00001180
Iteration 135/1000 | Loss: 0.00001180
Iteration 136/1000 | Loss: 0.00001180
Iteration 137/1000 | Loss: 0.00001180
Iteration 138/1000 | Loss: 0.00001180
Iteration 139/1000 | Loss: 0.00001180
Iteration 140/1000 | Loss: 0.00001180
Iteration 141/1000 | Loss: 0.00001180
Iteration 142/1000 | Loss: 0.00001179
Iteration 143/1000 | Loss: 0.00001179
Iteration 144/1000 | Loss: 0.00001179
Iteration 145/1000 | Loss: 0.00001179
Iteration 146/1000 | Loss: 0.00001179
Iteration 147/1000 | Loss: 0.00001179
Iteration 148/1000 | Loss: 0.00001179
Iteration 149/1000 | Loss: 0.00001179
Iteration 150/1000 | Loss: 0.00001179
Iteration 151/1000 | Loss: 0.00001179
Iteration 152/1000 | Loss: 0.00001178
Iteration 153/1000 | Loss: 0.00001178
Iteration 154/1000 | Loss: 0.00001178
Iteration 155/1000 | Loss: 0.00001178
Iteration 156/1000 | Loss: 0.00001178
Iteration 157/1000 | Loss: 0.00001178
Iteration 158/1000 | Loss: 0.00001178
Iteration 159/1000 | Loss: 0.00001178
Iteration 160/1000 | Loss: 0.00001178
Iteration 161/1000 | Loss: 0.00001178
Iteration 162/1000 | Loss: 0.00001178
Iteration 163/1000 | Loss: 0.00001178
Iteration 164/1000 | Loss: 0.00001178
Iteration 165/1000 | Loss: 0.00001178
Iteration 166/1000 | Loss: 0.00001178
Iteration 167/1000 | Loss: 0.00001178
Iteration 168/1000 | Loss: 0.00001178
Iteration 169/1000 | Loss: 0.00001178
Iteration 170/1000 | Loss: 0.00001178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.177670401375508e-05, 1.177670401375508e-05, 1.177670401375508e-05, 1.177670401375508e-05, 1.177670401375508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.177670401375508e-05

Optimization complete. Final v2v error: 2.9465997219085693 mm

Highest mean error: 3.092127561569214 mm for frame 105

Lowest mean error: 2.8273427486419678 mm for frame 151

Saving results

Total time: 42.195449352264404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402215
Iteration 2/25 | Loss: 0.00132414
Iteration 3/25 | Loss: 0.00125289
Iteration 4/25 | Loss: 0.00124694
Iteration 5/25 | Loss: 0.00124388
Iteration 6/25 | Loss: 0.00124342
Iteration 7/25 | Loss: 0.00124342
Iteration 8/25 | Loss: 0.00124342
Iteration 9/25 | Loss: 0.00124342
Iteration 10/25 | Loss: 0.00124342
Iteration 11/25 | Loss: 0.00124342
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012434190139174461, 0.0012434190139174461, 0.0012434190139174461, 0.0012434190139174461, 0.0012434190139174461]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012434190139174461

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67680359
Iteration 2/25 | Loss: 0.00065202
Iteration 3/25 | Loss: 0.00065202
Iteration 4/25 | Loss: 0.00065202
Iteration 5/25 | Loss: 0.00065202
Iteration 6/25 | Loss: 0.00065202
Iteration 7/25 | Loss: 0.00065202
Iteration 8/25 | Loss: 0.00065201
Iteration 9/25 | Loss: 0.00065201
Iteration 10/25 | Loss: 0.00065201
Iteration 11/25 | Loss: 0.00065201
Iteration 12/25 | Loss: 0.00065201
Iteration 13/25 | Loss: 0.00065201
Iteration 14/25 | Loss: 0.00065201
Iteration 15/25 | Loss: 0.00065201
Iteration 16/25 | Loss: 0.00065201
Iteration 17/25 | Loss: 0.00065201
Iteration 18/25 | Loss: 0.00065201
Iteration 19/25 | Loss: 0.00065201
Iteration 20/25 | Loss: 0.00065201
Iteration 21/25 | Loss: 0.00065201
Iteration 22/25 | Loss: 0.00065201
Iteration 23/25 | Loss: 0.00065201
Iteration 24/25 | Loss: 0.00065201
Iteration 25/25 | Loss: 0.00065201

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065201
Iteration 2/1000 | Loss: 0.00003261
Iteration 3/1000 | Loss: 0.00001972
Iteration 4/1000 | Loss: 0.00001703
Iteration 5/1000 | Loss: 0.00001591
Iteration 6/1000 | Loss: 0.00001517
Iteration 7/1000 | Loss: 0.00001469
Iteration 8/1000 | Loss: 0.00001435
Iteration 9/1000 | Loss: 0.00001394
Iteration 10/1000 | Loss: 0.00001354
Iteration 11/1000 | Loss: 0.00001336
Iteration 12/1000 | Loss: 0.00001334
Iteration 13/1000 | Loss: 0.00001322
Iteration 14/1000 | Loss: 0.00001314
Iteration 15/1000 | Loss: 0.00001309
Iteration 16/1000 | Loss: 0.00001298
Iteration 17/1000 | Loss: 0.00001298
Iteration 18/1000 | Loss: 0.00001285
Iteration 19/1000 | Loss: 0.00001283
Iteration 20/1000 | Loss: 0.00001282
Iteration 21/1000 | Loss: 0.00001282
Iteration 22/1000 | Loss: 0.00001281
Iteration 23/1000 | Loss: 0.00001281
Iteration 24/1000 | Loss: 0.00001280
Iteration 25/1000 | Loss: 0.00001280
Iteration 26/1000 | Loss: 0.00001278
Iteration 27/1000 | Loss: 0.00001277
Iteration 28/1000 | Loss: 0.00001277
Iteration 29/1000 | Loss: 0.00001276
Iteration 30/1000 | Loss: 0.00001276
Iteration 31/1000 | Loss: 0.00001276
Iteration 32/1000 | Loss: 0.00001275
Iteration 33/1000 | Loss: 0.00001274
Iteration 34/1000 | Loss: 0.00001274
Iteration 35/1000 | Loss: 0.00001274
Iteration 36/1000 | Loss: 0.00001273
Iteration 37/1000 | Loss: 0.00001272
Iteration 38/1000 | Loss: 0.00001272
Iteration 39/1000 | Loss: 0.00001271
Iteration 40/1000 | Loss: 0.00001270
Iteration 41/1000 | Loss: 0.00001269
Iteration 42/1000 | Loss: 0.00001269
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001267
Iteration 47/1000 | Loss: 0.00001267
Iteration 48/1000 | Loss: 0.00001267
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001267
Iteration 51/1000 | Loss: 0.00001267
Iteration 52/1000 | Loss: 0.00001267
Iteration 53/1000 | Loss: 0.00001266
Iteration 54/1000 | Loss: 0.00001266
Iteration 55/1000 | Loss: 0.00001266
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001265
Iteration 58/1000 | Loss: 0.00001265
Iteration 59/1000 | Loss: 0.00001265
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001265
Iteration 63/1000 | Loss: 0.00001264
Iteration 64/1000 | Loss: 0.00001264
Iteration 65/1000 | Loss: 0.00001264
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001262
Iteration 72/1000 | Loss: 0.00001262
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001262
Iteration 75/1000 | Loss: 0.00001262
Iteration 76/1000 | Loss: 0.00001262
Iteration 77/1000 | Loss: 0.00001262
Iteration 78/1000 | Loss: 0.00001262
Iteration 79/1000 | Loss: 0.00001261
Iteration 80/1000 | Loss: 0.00001261
Iteration 81/1000 | Loss: 0.00001261
Iteration 82/1000 | Loss: 0.00001260
Iteration 83/1000 | Loss: 0.00001260
Iteration 84/1000 | Loss: 0.00001260
Iteration 85/1000 | Loss: 0.00001260
Iteration 86/1000 | Loss: 0.00001260
Iteration 87/1000 | Loss: 0.00001260
Iteration 88/1000 | Loss: 0.00001259
Iteration 89/1000 | Loss: 0.00001259
Iteration 90/1000 | Loss: 0.00001259
Iteration 91/1000 | Loss: 0.00001259
Iteration 92/1000 | Loss: 0.00001259
Iteration 93/1000 | Loss: 0.00001259
Iteration 94/1000 | Loss: 0.00001258
Iteration 95/1000 | Loss: 0.00001258
Iteration 96/1000 | Loss: 0.00001258
Iteration 97/1000 | Loss: 0.00001258
Iteration 98/1000 | Loss: 0.00001258
Iteration 99/1000 | Loss: 0.00001258
Iteration 100/1000 | Loss: 0.00001258
Iteration 101/1000 | Loss: 0.00001257
Iteration 102/1000 | Loss: 0.00001257
Iteration 103/1000 | Loss: 0.00001257
Iteration 104/1000 | Loss: 0.00001257
Iteration 105/1000 | Loss: 0.00001257
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001256
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001256
Iteration 116/1000 | Loss: 0.00001256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.2563245036290027e-05, 1.2563245036290027e-05, 1.2563245036290027e-05, 1.2563245036290027e-05, 1.2563245036290027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2563245036290027e-05

Optimization complete. Final v2v error: 3.023083209991455 mm

Highest mean error: 3.349684476852417 mm for frame 219

Lowest mean error: 2.7815909385681152 mm for frame 244

Saving results

Total time: 40.3214008808136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00453370
Iteration 2/25 | Loss: 0.00149454
Iteration 3/25 | Loss: 0.00132354
Iteration 4/25 | Loss: 0.00128607
Iteration 5/25 | Loss: 0.00127930
Iteration 6/25 | Loss: 0.00128704
Iteration 7/25 | Loss: 0.00127697
Iteration 8/25 | Loss: 0.00127115
Iteration 9/25 | Loss: 0.00127036
Iteration 10/25 | Loss: 0.00127020
Iteration 11/25 | Loss: 0.00127020
Iteration 12/25 | Loss: 0.00127018
Iteration 13/25 | Loss: 0.00127018
Iteration 14/25 | Loss: 0.00127018
Iteration 15/25 | Loss: 0.00127018
Iteration 16/25 | Loss: 0.00127018
Iteration 17/25 | Loss: 0.00127018
Iteration 18/25 | Loss: 0.00127018
Iteration 19/25 | Loss: 0.00127018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001270183129236102, 0.001270183129236102, 0.001270183129236102, 0.001270183129236102, 0.001270183129236102]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001270183129236102

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42272770
Iteration 2/25 | Loss: 0.00086909
Iteration 3/25 | Loss: 0.00086908
Iteration 4/25 | Loss: 0.00086908
Iteration 5/25 | Loss: 0.00086908
Iteration 6/25 | Loss: 0.00086908
Iteration 7/25 | Loss: 0.00086908
Iteration 8/25 | Loss: 0.00086908
Iteration 9/25 | Loss: 0.00086908
Iteration 10/25 | Loss: 0.00086908
Iteration 11/25 | Loss: 0.00086908
Iteration 12/25 | Loss: 0.00086908
Iteration 13/25 | Loss: 0.00086908
Iteration 14/25 | Loss: 0.00086908
Iteration 15/25 | Loss: 0.00086908
Iteration 16/25 | Loss: 0.00086908
Iteration 17/25 | Loss: 0.00086908
Iteration 18/25 | Loss: 0.00086908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008690812392160296, 0.0008690812392160296, 0.0008690812392160296, 0.0008690812392160296, 0.0008690812392160296]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008690812392160296

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086908
Iteration 2/1000 | Loss: 0.00004609
Iteration 3/1000 | Loss: 0.00002971
Iteration 4/1000 | Loss: 0.00002428
Iteration 5/1000 | Loss: 0.00002251
Iteration 6/1000 | Loss: 0.00002108
Iteration 7/1000 | Loss: 0.00002012
Iteration 8/1000 | Loss: 0.00001942
Iteration 9/1000 | Loss: 0.00001895
Iteration 10/1000 | Loss: 0.00001867
Iteration 11/1000 | Loss: 0.00001831
Iteration 12/1000 | Loss: 0.00001804
Iteration 13/1000 | Loss: 0.00001784
Iteration 14/1000 | Loss: 0.00001765
Iteration 15/1000 | Loss: 0.00001764
Iteration 16/1000 | Loss: 0.00001762
Iteration 17/1000 | Loss: 0.00001747
Iteration 18/1000 | Loss: 0.00001744
Iteration 19/1000 | Loss: 0.00001743
Iteration 20/1000 | Loss: 0.00001739
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001738
Iteration 23/1000 | Loss: 0.00001737
Iteration 24/1000 | Loss: 0.00001736
Iteration 25/1000 | Loss: 0.00001735
Iteration 26/1000 | Loss: 0.00001735
Iteration 27/1000 | Loss: 0.00001734
Iteration 28/1000 | Loss: 0.00001734
Iteration 29/1000 | Loss: 0.00001733
Iteration 30/1000 | Loss: 0.00001733
Iteration 31/1000 | Loss: 0.00001732
Iteration 32/1000 | Loss: 0.00001731
Iteration 33/1000 | Loss: 0.00001731
Iteration 34/1000 | Loss: 0.00001730
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001724
Iteration 37/1000 | Loss: 0.00001723
Iteration 38/1000 | Loss: 0.00001723
Iteration 39/1000 | Loss: 0.00001722
Iteration 40/1000 | Loss: 0.00001722
Iteration 41/1000 | Loss: 0.00001720
Iteration 42/1000 | Loss: 0.00001720
Iteration 43/1000 | Loss: 0.00001720
Iteration 44/1000 | Loss: 0.00001720
Iteration 45/1000 | Loss: 0.00001719
Iteration 46/1000 | Loss: 0.00001719
Iteration 47/1000 | Loss: 0.00001719
Iteration 48/1000 | Loss: 0.00001719
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001717
Iteration 54/1000 | Loss: 0.00001717
Iteration 55/1000 | Loss: 0.00001716
Iteration 56/1000 | Loss: 0.00001716
Iteration 57/1000 | Loss: 0.00001716
Iteration 58/1000 | Loss: 0.00001716
Iteration 59/1000 | Loss: 0.00001715
Iteration 60/1000 | Loss: 0.00001715
Iteration 61/1000 | Loss: 0.00001715
Iteration 62/1000 | Loss: 0.00001714
Iteration 63/1000 | Loss: 0.00001714
Iteration 64/1000 | Loss: 0.00001714
Iteration 65/1000 | Loss: 0.00001714
Iteration 66/1000 | Loss: 0.00001714
Iteration 67/1000 | Loss: 0.00001713
Iteration 68/1000 | Loss: 0.00001713
Iteration 69/1000 | Loss: 0.00001713
Iteration 70/1000 | Loss: 0.00001713
Iteration 71/1000 | Loss: 0.00001713
Iteration 72/1000 | Loss: 0.00001712
Iteration 73/1000 | Loss: 0.00001712
Iteration 74/1000 | Loss: 0.00001712
Iteration 75/1000 | Loss: 0.00001711
Iteration 76/1000 | Loss: 0.00001711
Iteration 77/1000 | Loss: 0.00001710
Iteration 78/1000 | Loss: 0.00001710
Iteration 79/1000 | Loss: 0.00001710
Iteration 80/1000 | Loss: 0.00001709
Iteration 81/1000 | Loss: 0.00001708
Iteration 82/1000 | Loss: 0.00001708
Iteration 83/1000 | Loss: 0.00001708
Iteration 84/1000 | Loss: 0.00001708
Iteration 85/1000 | Loss: 0.00001708
Iteration 86/1000 | Loss: 0.00001708
Iteration 87/1000 | Loss: 0.00001708
Iteration 88/1000 | Loss: 0.00001708
Iteration 89/1000 | Loss: 0.00001708
Iteration 90/1000 | Loss: 0.00001708
Iteration 91/1000 | Loss: 0.00001707
Iteration 92/1000 | Loss: 0.00001707
Iteration 93/1000 | Loss: 0.00001707
Iteration 94/1000 | Loss: 0.00001706
Iteration 95/1000 | Loss: 0.00001706
Iteration 96/1000 | Loss: 0.00001706
Iteration 97/1000 | Loss: 0.00001706
Iteration 98/1000 | Loss: 0.00001706
Iteration 99/1000 | Loss: 0.00001705
Iteration 100/1000 | Loss: 0.00001705
Iteration 101/1000 | Loss: 0.00001705
Iteration 102/1000 | Loss: 0.00001704
Iteration 103/1000 | Loss: 0.00001704
Iteration 104/1000 | Loss: 0.00001704
Iteration 105/1000 | Loss: 0.00001704
Iteration 106/1000 | Loss: 0.00001703
Iteration 107/1000 | Loss: 0.00001703
Iteration 108/1000 | Loss: 0.00001703
Iteration 109/1000 | Loss: 0.00001703
Iteration 110/1000 | Loss: 0.00001703
Iteration 111/1000 | Loss: 0.00001702
Iteration 112/1000 | Loss: 0.00001702
Iteration 113/1000 | Loss: 0.00001702
Iteration 114/1000 | Loss: 0.00001701
Iteration 115/1000 | Loss: 0.00001701
Iteration 116/1000 | Loss: 0.00001701
Iteration 117/1000 | Loss: 0.00001701
Iteration 118/1000 | Loss: 0.00001700
Iteration 119/1000 | Loss: 0.00001700
Iteration 120/1000 | Loss: 0.00001700
Iteration 121/1000 | Loss: 0.00001700
Iteration 122/1000 | Loss: 0.00001700
Iteration 123/1000 | Loss: 0.00001700
Iteration 124/1000 | Loss: 0.00001700
Iteration 125/1000 | Loss: 0.00001699
Iteration 126/1000 | Loss: 0.00001699
Iteration 127/1000 | Loss: 0.00001699
Iteration 128/1000 | Loss: 0.00001698
Iteration 129/1000 | Loss: 0.00001698
Iteration 130/1000 | Loss: 0.00001698
Iteration 131/1000 | Loss: 0.00001698
Iteration 132/1000 | Loss: 0.00001698
Iteration 133/1000 | Loss: 0.00001698
Iteration 134/1000 | Loss: 0.00001698
Iteration 135/1000 | Loss: 0.00001697
Iteration 136/1000 | Loss: 0.00001697
Iteration 137/1000 | Loss: 0.00001697
Iteration 138/1000 | Loss: 0.00001697
Iteration 139/1000 | Loss: 0.00001697
Iteration 140/1000 | Loss: 0.00001696
Iteration 141/1000 | Loss: 0.00001696
Iteration 142/1000 | Loss: 0.00001696
Iteration 143/1000 | Loss: 0.00001696
Iteration 144/1000 | Loss: 0.00001696
Iteration 145/1000 | Loss: 0.00001696
Iteration 146/1000 | Loss: 0.00001696
Iteration 147/1000 | Loss: 0.00001696
Iteration 148/1000 | Loss: 0.00001696
Iteration 149/1000 | Loss: 0.00001696
Iteration 150/1000 | Loss: 0.00001695
Iteration 151/1000 | Loss: 0.00001695
Iteration 152/1000 | Loss: 0.00001695
Iteration 153/1000 | Loss: 0.00001695
Iteration 154/1000 | Loss: 0.00001695
Iteration 155/1000 | Loss: 0.00001695
Iteration 156/1000 | Loss: 0.00001695
Iteration 157/1000 | Loss: 0.00001695
Iteration 158/1000 | Loss: 0.00001695
Iteration 159/1000 | Loss: 0.00001695
Iteration 160/1000 | Loss: 0.00001695
Iteration 161/1000 | Loss: 0.00001695
Iteration 162/1000 | Loss: 0.00001695
Iteration 163/1000 | Loss: 0.00001694
Iteration 164/1000 | Loss: 0.00001694
Iteration 165/1000 | Loss: 0.00001694
Iteration 166/1000 | Loss: 0.00001694
Iteration 167/1000 | Loss: 0.00001694
Iteration 168/1000 | Loss: 0.00001694
Iteration 169/1000 | Loss: 0.00001694
Iteration 170/1000 | Loss: 0.00001694
Iteration 171/1000 | Loss: 0.00001694
Iteration 172/1000 | Loss: 0.00001694
Iteration 173/1000 | Loss: 0.00001694
Iteration 174/1000 | Loss: 0.00001693
Iteration 175/1000 | Loss: 0.00001693
Iteration 176/1000 | Loss: 0.00001693
Iteration 177/1000 | Loss: 0.00001693
Iteration 178/1000 | Loss: 0.00001693
Iteration 179/1000 | Loss: 0.00001693
Iteration 180/1000 | Loss: 0.00001693
Iteration 181/1000 | Loss: 0.00001693
Iteration 182/1000 | Loss: 0.00001693
Iteration 183/1000 | Loss: 0.00001693
Iteration 184/1000 | Loss: 0.00001692
Iteration 185/1000 | Loss: 0.00001692
Iteration 186/1000 | Loss: 0.00001692
Iteration 187/1000 | Loss: 0.00001692
Iteration 188/1000 | Loss: 0.00001692
Iteration 189/1000 | Loss: 0.00001692
Iteration 190/1000 | Loss: 0.00001692
Iteration 191/1000 | Loss: 0.00001692
Iteration 192/1000 | Loss: 0.00001692
Iteration 193/1000 | Loss: 0.00001692
Iteration 194/1000 | Loss: 0.00001691
Iteration 195/1000 | Loss: 0.00001691
Iteration 196/1000 | Loss: 0.00001691
Iteration 197/1000 | Loss: 0.00001691
Iteration 198/1000 | Loss: 0.00001691
Iteration 199/1000 | Loss: 0.00001691
Iteration 200/1000 | Loss: 0.00001691
Iteration 201/1000 | Loss: 0.00001691
Iteration 202/1000 | Loss: 0.00001691
Iteration 203/1000 | Loss: 0.00001691
Iteration 204/1000 | Loss: 0.00001691
Iteration 205/1000 | Loss: 0.00001691
Iteration 206/1000 | Loss: 0.00001691
Iteration 207/1000 | Loss: 0.00001691
Iteration 208/1000 | Loss: 0.00001691
Iteration 209/1000 | Loss: 0.00001691
Iteration 210/1000 | Loss: 0.00001691
Iteration 211/1000 | Loss: 0.00001691
Iteration 212/1000 | Loss: 0.00001691
Iteration 213/1000 | Loss: 0.00001690
Iteration 214/1000 | Loss: 0.00001690
Iteration 215/1000 | Loss: 0.00001690
Iteration 216/1000 | Loss: 0.00001690
Iteration 217/1000 | Loss: 0.00001690
Iteration 218/1000 | Loss: 0.00001690
Iteration 219/1000 | Loss: 0.00001690
Iteration 220/1000 | Loss: 0.00001690
Iteration 221/1000 | Loss: 0.00001690
Iteration 222/1000 | Loss: 0.00001690
Iteration 223/1000 | Loss: 0.00001690
Iteration 224/1000 | Loss: 0.00001690
Iteration 225/1000 | Loss: 0.00001690
Iteration 226/1000 | Loss: 0.00001690
Iteration 227/1000 | Loss: 0.00001690
Iteration 228/1000 | Loss: 0.00001690
Iteration 229/1000 | Loss: 0.00001690
Iteration 230/1000 | Loss: 0.00001690
Iteration 231/1000 | Loss: 0.00001690
Iteration 232/1000 | Loss: 0.00001690
Iteration 233/1000 | Loss: 0.00001690
Iteration 234/1000 | Loss: 0.00001690
Iteration 235/1000 | Loss: 0.00001689
Iteration 236/1000 | Loss: 0.00001689
Iteration 237/1000 | Loss: 0.00001689
Iteration 238/1000 | Loss: 0.00001689
Iteration 239/1000 | Loss: 0.00001689
Iteration 240/1000 | Loss: 0.00001689
Iteration 241/1000 | Loss: 0.00001689
Iteration 242/1000 | Loss: 0.00001689
Iteration 243/1000 | Loss: 0.00001689
Iteration 244/1000 | Loss: 0.00001689
Iteration 245/1000 | Loss: 0.00001689
Iteration 246/1000 | Loss: 0.00001689
Iteration 247/1000 | Loss: 0.00001689
Iteration 248/1000 | Loss: 0.00001689
Iteration 249/1000 | Loss: 0.00001689
Iteration 250/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.689407872618176e-05, 1.689407872618176e-05, 1.689407872618176e-05, 1.689407872618176e-05, 1.689407872618176e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.689407872618176e-05

Optimization complete. Final v2v error: 3.434509515762329 mm

Highest mean error: 4.533380508422852 mm for frame 111

Lowest mean error: 2.973827600479126 mm for frame 0

Saving results

Total time: 56.27333378791809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00920948
Iteration 2/25 | Loss: 0.00161546
Iteration 3/25 | Loss: 0.00154879
Iteration 4/25 | Loss: 0.00135318
Iteration 5/25 | Loss: 0.00134272
Iteration 6/25 | Loss: 0.00134014
Iteration 7/25 | Loss: 0.00132753
Iteration 8/25 | Loss: 0.00131807
Iteration 9/25 | Loss: 0.00131656
Iteration 10/25 | Loss: 0.00131386
Iteration 11/25 | Loss: 0.00131262
Iteration 12/25 | Loss: 0.00131683
Iteration 13/25 | Loss: 0.00131197
Iteration 14/25 | Loss: 0.00131183
Iteration 15/25 | Loss: 0.00131183
Iteration 16/25 | Loss: 0.00131183
Iteration 17/25 | Loss: 0.00131183
Iteration 18/25 | Loss: 0.00131183
Iteration 19/25 | Loss: 0.00131182
Iteration 20/25 | Loss: 0.00131182
Iteration 21/25 | Loss: 0.00131182
Iteration 22/25 | Loss: 0.00131182
Iteration 23/25 | Loss: 0.00131182
Iteration 24/25 | Loss: 0.00131182
Iteration 25/25 | Loss: 0.00131429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54522884
Iteration 2/25 | Loss: 0.00095771
Iteration 3/25 | Loss: 0.00085403
Iteration 4/25 | Loss: 0.00085403
Iteration 5/25 | Loss: 0.00085403
Iteration 6/25 | Loss: 0.00085403
Iteration 7/25 | Loss: 0.00085403
Iteration 8/25 | Loss: 0.00085403
Iteration 9/25 | Loss: 0.00085403
Iteration 10/25 | Loss: 0.00085403
Iteration 11/25 | Loss: 0.00085403
Iteration 12/25 | Loss: 0.00085403
Iteration 13/25 | Loss: 0.00085403
Iteration 14/25 | Loss: 0.00085403
Iteration 15/25 | Loss: 0.00085403
Iteration 16/25 | Loss: 0.00085403
Iteration 17/25 | Loss: 0.00085403
Iteration 18/25 | Loss: 0.00085403
Iteration 19/25 | Loss: 0.00085403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008540265844203532, 0.0008540265844203532, 0.0008540265844203532, 0.0008540265844203532, 0.0008540265844203532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008540265844203532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085403
Iteration 2/1000 | Loss: 0.00013247
Iteration 3/1000 | Loss: 0.00013173
Iteration 4/1000 | Loss: 0.00002969
Iteration 5/1000 | Loss: 0.00003348
Iteration 6/1000 | Loss: 0.00002478
Iteration 7/1000 | Loss: 0.00008453
Iteration 8/1000 | Loss: 0.00004229
Iteration 9/1000 | Loss: 0.00002446
Iteration 10/1000 | Loss: 0.00002270
Iteration 11/1000 | Loss: 0.00002231
Iteration 12/1000 | Loss: 0.00009876
Iteration 13/1000 | Loss: 0.00004454
Iteration 14/1000 | Loss: 0.00002471
Iteration 15/1000 | Loss: 0.00006984
Iteration 16/1000 | Loss: 0.00002541
Iteration 17/1000 | Loss: 0.00002126
Iteration 18/1000 | Loss: 0.00002091
Iteration 19/1000 | Loss: 0.00002065
Iteration 20/1000 | Loss: 0.00044475
Iteration 21/1000 | Loss: 0.00002339
Iteration 22/1000 | Loss: 0.00002098
Iteration 23/1000 | Loss: 0.00003801
Iteration 24/1000 | Loss: 0.00012574
Iteration 25/1000 | Loss: 0.00001971
Iteration 26/1000 | Loss: 0.00001931
Iteration 27/1000 | Loss: 0.00002587
Iteration 28/1000 | Loss: 0.00001922
Iteration 29/1000 | Loss: 0.00001883
Iteration 30/1000 | Loss: 0.00001977
Iteration 31/1000 | Loss: 0.00001872
Iteration 32/1000 | Loss: 0.00001871
Iteration 33/1000 | Loss: 0.00001870
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001870
Iteration 37/1000 | Loss: 0.00001870
Iteration 38/1000 | Loss: 0.00001870
Iteration 39/1000 | Loss: 0.00001870
Iteration 40/1000 | Loss: 0.00001870
Iteration 41/1000 | Loss: 0.00001870
Iteration 42/1000 | Loss: 0.00001870
Iteration 43/1000 | Loss: 0.00001870
Iteration 44/1000 | Loss: 0.00001870
Iteration 45/1000 | Loss: 0.00001869
Iteration 46/1000 | Loss: 0.00001869
Iteration 47/1000 | Loss: 0.00001869
Iteration 48/1000 | Loss: 0.00001869
Iteration 49/1000 | Loss: 0.00001868
Iteration 50/1000 | Loss: 0.00001868
Iteration 51/1000 | Loss: 0.00001991
Iteration 52/1000 | Loss: 0.00001860
Iteration 53/1000 | Loss: 0.00001858
Iteration 54/1000 | Loss: 0.00001854
Iteration 55/1000 | Loss: 0.00001853
Iteration 56/1000 | Loss: 0.00001851
Iteration 57/1000 | Loss: 0.00001847
Iteration 58/1000 | Loss: 0.00001847
Iteration 59/1000 | Loss: 0.00001841
Iteration 60/1000 | Loss: 0.00001841
Iteration 61/1000 | Loss: 0.00001839
Iteration 62/1000 | Loss: 0.00001839
Iteration 63/1000 | Loss: 0.00001838
Iteration 64/1000 | Loss: 0.00001838
Iteration 65/1000 | Loss: 0.00001838
Iteration 66/1000 | Loss: 0.00001837
Iteration 67/1000 | Loss: 0.00001837
Iteration 68/1000 | Loss: 0.00001837
Iteration 69/1000 | Loss: 0.00001837
Iteration 70/1000 | Loss: 0.00001837
Iteration 71/1000 | Loss: 0.00001837
Iteration 72/1000 | Loss: 0.00001836
Iteration 73/1000 | Loss: 0.00001836
Iteration 74/1000 | Loss: 0.00001836
Iteration 75/1000 | Loss: 0.00001836
Iteration 76/1000 | Loss: 0.00001835
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00001834
Iteration 79/1000 | Loss: 0.00001834
Iteration 80/1000 | Loss: 0.00001834
Iteration 81/1000 | Loss: 0.00001834
Iteration 82/1000 | Loss: 0.00001834
Iteration 83/1000 | Loss: 0.00001834
Iteration 84/1000 | Loss: 0.00001834
Iteration 85/1000 | Loss: 0.00001834
Iteration 86/1000 | Loss: 0.00001834
Iteration 87/1000 | Loss: 0.00001833
Iteration 88/1000 | Loss: 0.00001833
Iteration 89/1000 | Loss: 0.00001833
Iteration 90/1000 | Loss: 0.00001833
Iteration 91/1000 | Loss: 0.00001832
Iteration 92/1000 | Loss: 0.00001832
Iteration 93/1000 | Loss: 0.00001832
Iteration 94/1000 | Loss: 0.00001832
Iteration 95/1000 | Loss: 0.00001832
Iteration 96/1000 | Loss: 0.00001832
Iteration 97/1000 | Loss: 0.00001831
Iteration 98/1000 | Loss: 0.00001831
Iteration 99/1000 | Loss: 0.00001830
Iteration 100/1000 | Loss: 0.00001830
Iteration 101/1000 | Loss: 0.00001830
Iteration 102/1000 | Loss: 0.00001829
Iteration 103/1000 | Loss: 0.00001829
Iteration 104/1000 | Loss: 0.00001829
Iteration 105/1000 | Loss: 0.00001829
Iteration 106/1000 | Loss: 0.00001829
Iteration 107/1000 | Loss: 0.00001829
Iteration 108/1000 | Loss: 0.00001829
Iteration 109/1000 | Loss: 0.00001829
Iteration 110/1000 | Loss: 0.00001829
Iteration 111/1000 | Loss: 0.00001829
Iteration 112/1000 | Loss: 0.00001829
Iteration 113/1000 | Loss: 0.00001829
Iteration 114/1000 | Loss: 0.00001828
Iteration 115/1000 | Loss: 0.00001828
Iteration 116/1000 | Loss: 0.00001828
Iteration 117/1000 | Loss: 0.00001828
Iteration 118/1000 | Loss: 0.00001828
Iteration 119/1000 | Loss: 0.00001828
Iteration 120/1000 | Loss: 0.00001828
Iteration 121/1000 | Loss: 0.00001828
Iteration 122/1000 | Loss: 0.00001826
Iteration 123/1000 | Loss: 0.00001826
Iteration 124/1000 | Loss: 0.00001826
Iteration 125/1000 | Loss: 0.00001826
Iteration 126/1000 | Loss: 0.00001826
Iteration 127/1000 | Loss: 0.00001825
Iteration 128/1000 | Loss: 0.00001825
Iteration 129/1000 | Loss: 0.00001825
Iteration 130/1000 | Loss: 0.00001825
Iteration 131/1000 | Loss: 0.00001825
Iteration 132/1000 | Loss: 0.00001825
Iteration 133/1000 | Loss: 0.00001825
Iteration 134/1000 | Loss: 0.00001824
Iteration 135/1000 | Loss: 0.00001824
Iteration 136/1000 | Loss: 0.00001824
Iteration 137/1000 | Loss: 0.00001824
Iteration 138/1000 | Loss: 0.00001824
Iteration 139/1000 | Loss: 0.00001823
Iteration 140/1000 | Loss: 0.00001823
Iteration 141/1000 | Loss: 0.00001823
Iteration 142/1000 | Loss: 0.00001822
Iteration 143/1000 | Loss: 0.00001822
Iteration 144/1000 | Loss: 0.00004183
Iteration 145/1000 | Loss: 0.00001924
Iteration 146/1000 | Loss: 0.00002002
Iteration 147/1000 | Loss: 0.00001823
Iteration 148/1000 | Loss: 0.00001821
Iteration 149/1000 | Loss: 0.00001821
Iteration 150/1000 | Loss: 0.00001821
Iteration 151/1000 | Loss: 0.00001821
Iteration 152/1000 | Loss: 0.00001821
Iteration 153/1000 | Loss: 0.00001821
Iteration 154/1000 | Loss: 0.00001821
Iteration 155/1000 | Loss: 0.00001821
Iteration 156/1000 | Loss: 0.00001820
Iteration 157/1000 | Loss: 0.00001820
Iteration 158/1000 | Loss: 0.00001820
Iteration 159/1000 | Loss: 0.00001820
Iteration 160/1000 | Loss: 0.00001820
Iteration 161/1000 | Loss: 0.00001819
Iteration 162/1000 | Loss: 0.00001819
Iteration 163/1000 | Loss: 0.00001819
Iteration 164/1000 | Loss: 0.00001819
Iteration 165/1000 | Loss: 0.00001819
Iteration 166/1000 | Loss: 0.00001819
Iteration 167/1000 | Loss: 0.00001819
Iteration 168/1000 | Loss: 0.00001819
Iteration 169/1000 | Loss: 0.00001819
Iteration 170/1000 | Loss: 0.00001818
Iteration 171/1000 | Loss: 0.00001818
Iteration 172/1000 | Loss: 0.00001818
Iteration 173/1000 | Loss: 0.00001818
Iteration 174/1000 | Loss: 0.00001818
Iteration 175/1000 | Loss: 0.00001818
Iteration 176/1000 | Loss: 0.00001818
Iteration 177/1000 | Loss: 0.00001818
Iteration 178/1000 | Loss: 0.00001818
Iteration 179/1000 | Loss: 0.00001818
Iteration 180/1000 | Loss: 0.00001818
Iteration 181/1000 | Loss: 0.00001818
Iteration 182/1000 | Loss: 0.00001817
Iteration 183/1000 | Loss: 0.00001817
Iteration 184/1000 | Loss: 0.00001817
Iteration 185/1000 | Loss: 0.00001817
Iteration 186/1000 | Loss: 0.00001817
Iteration 187/1000 | Loss: 0.00001817
Iteration 188/1000 | Loss: 0.00001817
Iteration 189/1000 | Loss: 0.00001817
Iteration 190/1000 | Loss: 0.00001816
Iteration 191/1000 | Loss: 0.00001816
Iteration 192/1000 | Loss: 0.00001816
Iteration 193/1000 | Loss: 0.00001816
Iteration 194/1000 | Loss: 0.00001816
Iteration 195/1000 | Loss: 0.00001816
Iteration 196/1000 | Loss: 0.00001816
Iteration 197/1000 | Loss: 0.00001816
Iteration 198/1000 | Loss: 0.00001816
Iteration 199/1000 | Loss: 0.00001816
Iteration 200/1000 | Loss: 0.00001816
Iteration 201/1000 | Loss: 0.00001816
Iteration 202/1000 | Loss: 0.00001816
Iteration 203/1000 | Loss: 0.00001816
Iteration 204/1000 | Loss: 0.00001816
Iteration 205/1000 | Loss: 0.00001816
Iteration 206/1000 | Loss: 0.00001816
Iteration 207/1000 | Loss: 0.00001816
Iteration 208/1000 | Loss: 0.00001816
Iteration 209/1000 | Loss: 0.00001816
Iteration 210/1000 | Loss: 0.00001816
Iteration 211/1000 | Loss: 0.00001816
Iteration 212/1000 | Loss: 0.00001816
Iteration 213/1000 | Loss: 0.00001816
Iteration 214/1000 | Loss: 0.00001816
Iteration 215/1000 | Loss: 0.00001816
Iteration 216/1000 | Loss: 0.00001816
Iteration 217/1000 | Loss: 0.00001816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.8156781152356416e-05, 1.8156781152356416e-05, 1.8156781152356416e-05, 1.8156781152356416e-05, 1.8156781152356416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8156781152356416e-05

Optimization complete. Final v2v error: 3.593297004699707 mm

Highest mean error: 4.240523338317871 mm for frame 39

Lowest mean error: 3.002218246459961 mm for frame 200

Saving results

Total time: 92.49911332130432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784815
Iteration 2/25 | Loss: 0.00133741
Iteration 3/25 | Loss: 0.00124192
Iteration 4/25 | Loss: 0.00123315
Iteration 5/25 | Loss: 0.00123078
Iteration 6/25 | Loss: 0.00123078
Iteration 7/25 | Loss: 0.00123078
Iteration 8/25 | Loss: 0.00123078
Iteration 9/25 | Loss: 0.00123078
Iteration 10/25 | Loss: 0.00123078
Iteration 11/25 | Loss: 0.00123078
Iteration 12/25 | Loss: 0.00123078
Iteration 13/25 | Loss: 0.00123078
Iteration 14/25 | Loss: 0.00123078
Iteration 15/25 | Loss: 0.00123078
Iteration 16/25 | Loss: 0.00123078
Iteration 17/25 | Loss: 0.00123078
Iteration 18/25 | Loss: 0.00123078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012307788711041212, 0.0012307788711041212, 0.0012307788711041212, 0.0012307788711041212, 0.0012307788711041212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012307788711041212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43594849
Iteration 2/25 | Loss: 0.00077905
Iteration 3/25 | Loss: 0.00077904
Iteration 4/25 | Loss: 0.00077904
Iteration 5/25 | Loss: 0.00077904
Iteration 6/25 | Loss: 0.00077904
Iteration 7/25 | Loss: 0.00077904
Iteration 8/25 | Loss: 0.00077904
Iteration 9/25 | Loss: 0.00077904
Iteration 10/25 | Loss: 0.00077904
Iteration 11/25 | Loss: 0.00077904
Iteration 12/25 | Loss: 0.00077904
Iteration 13/25 | Loss: 0.00077904
Iteration 14/25 | Loss: 0.00077904
Iteration 15/25 | Loss: 0.00077904
Iteration 16/25 | Loss: 0.00077904
Iteration 17/25 | Loss: 0.00077904
Iteration 18/25 | Loss: 0.00077904
Iteration 19/25 | Loss: 0.00077904
Iteration 20/25 | Loss: 0.00077904
Iteration 21/25 | Loss: 0.00077904
Iteration 22/25 | Loss: 0.00077904
Iteration 23/25 | Loss: 0.00077904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0007790402160026133, 0.0007790402160026133, 0.0007790402160026133, 0.0007790402160026133, 0.0007790402160026133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007790402160026133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077904
Iteration 2/1000 | Loss: 0.00002794
Iteration 3/1000 | Loss: 0.00001739
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001443
Iteration 6/1000 | Loss: 0.00001367
Iteration 7/1000 | Loss: 0.00001316
Iteration 8/1000 | Loss: 0.00001278
Iteration 9/1000 | Loss: 0.00001273
Iteration 10/1000 | Loss: 0.00001262
Iteration 11/1000 | Loss: 0.00001239
Iteration 12/1000 | Loss: 0.00001236
Iteration 13/1000 | Loss: 0.00001236
Iteration 14/1000 | Loss: 0.00001236
Iteration 15/1000 | Loss: 0.00001234
Iteration 16/1000 | Loss: 0.00001229
Iteration 17/1000 | Loss: 0.00001227
Iteration 18/1000 | Loss: 0.00001221
Iteration 19/1000 | Loss: 0.00001221
Iteration 20/1000 | Loss: 0.00001220
Iteration 21/1000 | Loss: 0.00001220
Iteration 22/1000 | Loss: 0.00001217
Iteration 23/1000 | Loss: 0.00001216
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001212
Iteration 26/1000 | Loss: 0.00001211
Iteration 27/1000 | Loss: 0.00001202
Iteration 28/1000 | Loss: 0.00001202
Iteration 29/1000 | Loss: 0.00001201
Iteration 30/1000 | Loss: 0.00001200
Iteration 31/1000 | Loss: 0.00001200
Iteration 32/1000 | Loss: 0.00001200
Iteration 33/1000 | Loss: 0.00001199
Iteration 34/1000 | Loss: 0.00001199
Iteration 35/1000 | Loss: 0.00001198
Iteration 36/1000 | Loss: 0.00001198
Iteration 37/1000 | Loss: 0.00001198
Iteration 38/1000 | Loss: 0.00001196
Iteration 39/1000 | Loss: 0.00001194
Iteration 40/1000 | Loss: 0.00001194
Iteration 41/1000 | Loss: 0.00001194
Iteration 42/1000 | Loss: 0.00001193
Iteration 43/1000 | Loss: 0.00001193
Iteration 44/1000 | Loss: 0.00001193
Iteration 45/1000 | Loss: 0.00001193
Iteration 46/1000 | Loss: 0.00001193
Iteration 47/1000 | Loss: 0.00001193
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001193
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001193
Iteration 53/1000 | Loss: 0.00001193
Iteration 54/1000 | Loss: 0.00001193
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001193
Iteration 64/1000 | Loss: 0.00001193
Iteration 65/1000 | Loss: 0.00001193
Iteration 66/1000 | Loss: 0.00001193
Iteration 67/1000 | Loss: 0.00001193
Iteration 68/1000 | Loss: 0.00001193
Iteration 69/1000 | Loss: 0.00001193
Iteration 70/1000 | Loss: 0.00001193
Iteration 71/1000 | Loss: 0.00001193
Iteration 72/1000 | Loss: 0.00001193
Iteration 73/1000 | Loss: 0.00001193
Iteration 74/1000 | Loss: 0.00001193
Iteration 75/1000 | Loss: 0.00001193
Iteration 76/1000 | Loss: 0.00001193
Iteration 77/1000 | Loss: 0.00001193
Iteration 78/1000 | Loss: 0.00001193
Iteration 79/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 79. Stopping optimization.
Last 5 losses: [1.19255328172585e-05, 1.19255328172585e-05, 1.19255328172585e-05, 1.19255328172585e-05, 1.19255328172585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.19255328172585e-05

Optimization complete. Final v2v error: 2.9646413326263428 mm

Highest mean error: 3.1975257396698 mm for frame 46

Lowest mean error: 2.817331552505493 mm for frame 142

Saving results

Total time: 27.98164129257202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396702
Iteration 2/25 | Loss: 0.00138645
Iteration 3/25 | Loss: 0.00129751
Iteration 4/25 | Loss: 0.00128718
Iteration 5/25 | Loss: 0.00128481
Iteration 6/25 | Loss: 0.00128453
Iteration 7/25 | Loss: 0.00128453
Iteration 8/25 | Loss: 0.00128453
Iteration 9/25 | Loss: 0.00128453
Iteration 10/25 | Loss: 0.00128453
Iteration 11/25 | Loss: 0.00128453
Iteration 12/25 | Loss: 0.00128453
Iteration 13/25 | Loss: 0.00128453
Iteration 14/25 | Loss: 0.00128453
Iteration 15/25 | Loss: 0.00128453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012845267774537206, 0.0012845267774537206, 0.0012845267774537206, 0.0012845267774537206, 0.0012845267774537206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012845267774537206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43122637
Iteration 2/25 | Loss: 0.00092066
Iteration 3/25 | Loss: 0.00092066
Iteration 4/25 | Loss: 0.00092066
Iteration 5/25 | Loss: 0.00092066
Iteration 6/25 | Loss: 0.00092066
Iteration 7/25 | Loss: 0.00092066
Iteration 8/25 | Loss: 0.00092066
Iteration 9/25 | Loss: 0.00092066
Iteration 10/25 | Loss: 0.00092066
Iteration 11/25 | Loss: 0.00092066
Iteration 12/25 | Loss: 0.00092066
Iteration 13/25 | Loss: 0.00092066
Iteration 14/25 | Loss: 0.00092066
Iteration 15/25 | Loss: 0.00092066
Iteration 16/25 | Loss: 0.00092066
Iteration 17/25 | Loss: 0.00092066
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009206600952893496, 0.0009206600952893496, 0.0009206600952893496, 0.0009206600952893496, 0.0009206600952893496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009206600952893496

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092066
Iteration 2/1000 | Loss: 0.00003954
Iteration 3/1000 | Loss: 0.00002508
Iteration 4/1000 | Loss: 0.00002171
Iteration 5/1000 | Loss: 0.00001977
Iteration 6/1000 | Loss: 0.00001860
Iteration 7/1000 | Loss: 0.00001785
Iteration 8/1000 | Loss: 0.00001736
Iteration 9/1000 | Loss: 0.00001714
Iteration 10/1000 | Loss: 0.00001692
Iteration 11/1000 | Loss: 0.00001679
Iteration 12/1000 | Loss: 0.00001675
Iteration 13/1000 | Loss: 0.00001665
Iteration 14/1000 | Loss: 0.00001664
Iteration 15/1000 | Loss: 0.00001661
Iteration 16/1000 | Loss: 0.00001660
Iteration 17/1000 | Loss: 0.00001658
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001644
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001638
Iteration 22/1000 | Loss: 0.00001637
Iteration 23/1000 | Loss: 0.00001637
Iteration 24/1000 | Loss: 0.00001636
Iteration 25/1000 | Loss: 0.00001636
Iteration 26/1000 | Loss: 0.00001635
Iteration 27/1000 | Loss: 0.00001634
Iteration 28/1000 | Loss: 0.00001634
Iteration 29/1000 | Loss: 0.00001633
Iteration 30/1000 | Loss: 0.00001633
Iteration 31/1000 | Loss: 0.00001632
Iteration 32/1000 | Loss: 0.00001631
Iteration 33/1000 | Loss: 0.00001631
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001630
Iteration 36/1000 | Loss: 0.00001630
Iteration 37/1000 | Loss: 0.00001629
Iteration 38/1000 | Loss: 0.00001629
Iteration 39/1000 | Loss: 0.00001628
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001627
Iteration 42/1000 | Loss: 0.00001627
Iteration 43/1000 | Loss: 0.00001627
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001625
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001624
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001622
Iteration 56/1000 | Loss: 0.00001622
Iteration 57/1000 | Loss: 0.00001621
Iteration 58/1000 | Loss: 0.00001620
Iteration 59/1000 | Loss: 0.00001620
Iteration 60/1000 | Loss: 0.00001620
Iteration 61/1000 | Loss: 0.00001619
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00001618
Iteration 64/1000 | Loss: 0.00001618
Iteration 65/1000 | Loss: 0.00001618
Iteration 66/1000 | Loss: 0.00001618
Iteration 67/1000 | Loss: 0.00001617
Iteration 68/1000 | Loss: 0.00001617
Iteration 69/1000 | Loss: 0.00001617
Iteration 70/1000 | Loss: 0.00001616
Iteration 71/1000 | Loss: 0.00001616
Iteration 72/1000 | Loss: 0.00001616
Iteration 73/1000 | Loss: 0.00001615
Iteration 74/1000 | Loss: 0.00001615
Iteration 75/1000 | Loss: 0.00001615
Iteration 76/1000 | Loss: 0.00001615
Iteration 77/1000 | Loss: 0.00001615
Iteration 78/1000 | Loss: 0.00001615
Iteration 79/1000 | Loss: 0.00001615
Iteration 80/1000 | Loss: 0.00001615
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001614
Iteration 83/1000 | Loss: 0.00001614
Iteration 84/1000 | Loss: 0.00001614
Iteration 85/1000 | Loss: 0.00001614
Iteration 86/1000 | Loss: 0.00001614
Iteration 87/1000 | Loss: 0.00001614
Iteration 88/1000 | Loss: 0.00001614
Iteration 89/1000 | Loss: 0.00001614
Iteration 90/1000 | Loss: 0.00001614
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001611
Iteration 97/1000 | Loss: 0.00001611
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001609
Iteration 101/1000 | Loss: 0.00001609
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001609
Iteration 107/1000 | Loss: 0.00001609
Iteration 108/1000 | Loss: 0.00001609
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001608
Iteration 111/1000 | Loss: 0.00001608
Iteration 112/1000 | Loss: 0.00001608
Iteration 113/1000 | Loss: 0.00001608
Iteration 114/1000 | Loss: 0.00001608
Iteration 115/1000 | Loss: 0.00001608
Iteration 116/1000 | Loss: 0.00001608
Iteration 117/1000 | Loss: 0.00001607
Iteration 118/1000 | Loss: 0.00001607
Iteration 119/1000 | Loss: 0.00001607
Iteration 120/1000 | Loss: 0.00001607
Iteration 121/1000 | Loss: 0.00001607
Iteration 122/1000 | Loss: 0.00001607
Iteration 123/1000 | Loss: 0.00001607
Iteration 124/1000 | Loss: 0.00001607
Iteration 125/1000 | Loss: 0.00001606
Iteration 126/1000 | Loss: 0.00001606
Iteration 127/1000 | Loss: 0.00001606
Iteration 128/1000 | Loss: 0.00001606
Iteration 129/1000 | Loss: 0.00001606
Iteration 130/1000 | Loss: 0.00001606
Iteration 131/1000 | Loss: 0.00001605
Iteration 132/1000 | Loss: 0.00001605
Iteration 133/1000 | Loss: 0.00001605
Iteration 134/1000 | Loss: 0.00001604
Iteration 135/1000 | Loss: 0.00001604
Iteration 136/1000 | Loss: 0.00001604
Iteration 137/1000 | Loss: 0.00001604
Iteration 138/1000 | Loss: 0.00001603
Iteration 139/1000 | Loss: 0.00001603
Iteration 140/1000 | Loss: 0.00001603
Iteration 141/1000 | Loss: 0.00001603
Iteration 142/1000 | Loss: 0.00001603
Iteration 143/1000 | Loss: 0.00001603
Iteration 144/1000 | Loss: 0.00001602
Iteration 145/1000 | Loss: 0.00001602
Iteration 146/1000 | Loss: 0.00001602
Iteration 147/1000 | Loss: 0.00001602
Iteration 148/1000 | Loss: 0.00001602
Iteration 149/1000 | Loss: 0.00001602
Iteration 150/1000 | Loss: 0.00001602
Iteration 151/1000 | Loss: 0.00001602
Iteration 152/1000 | Loss: 0.00001602
Iteration 153/1000 | Loss: 0.00001602
Iteration 154/1000 | Loss: 0.00001601
Iteration 155/1000 | Loss: 0.00001601
Iteration 156/1000 | Loss: 0.00001601
Iteration 157/1000 | Loss: 0.00001601
Iteration 158/1000 | Loss: 0.00001601
Iteration 159/1000 | Loss: 0.00001601
Iteration 160/1000 | Loss: 0.00001601
Iteration 161/1000 | Loss: 0.00001600
Iteration 162/1000 | Loss: 0.00001600
Iteration 163/1000 | Loss: 0.00001600
Iteration 164/1000 | Loss: 0.00001599
Iteration 165/1000 | Loss: 0.00001599
Iteration 166/1000 | Loss: 0.00001599
Iteration 167/1000 | Loss: 0.00001599
Iteration 168/1000 | Loss: 0.00001599
Iteration 169/1000 | Loss: 0.00001599
Iteration 170/1000 | Loss: 0.00001599
Iteration 171/1000 | Loss: 0.00001599
Iteration 172/1000 | Loss: 0.00001599
Iteration 173/1000 | Loss: 0.00001599
Iteration 174/1000 | Loss: 0.00001599
Iteration 175/1000 | Loss: 0.00001598
Iteration 176/1000 | Loss: 0.00001598
Iteration 177/1000 | Loss: 0.00001598
Iteration 178/1000 | Loss: 0.00001597
Iteration 179/1000 | Loss: 0.00001597
Iteration 180/1000 | Loss: 0.00001597
Iteration 181/1000 | Loss: 0.00001597
Iteration 182/1000 | Loss: 0.00001597
Iteration 183/1000 | Loss: 0.00001596
Iteration 184/1000 | Loss: 0.00001596
Iteration 185/1000 | Loss: 0.00001596
Iteration 186/1000 | Loss: 0.00001596
Iteration 187/1000 | Loss: 0.00001596
Iteration 188/1000 | Loss: 0.00001596
Iteration 189/1000 | Loss: 0.00001596
Iteration 190/1000 | Loss: 0.00001596
Iteration 191/1000 | Loss: 0.00001596
Iteration 192/1000 | Loss: 0.00001595
Iteration 193/1000 | Loss: 0.00001595
Iteration 194/1000 | Loss: 0.00001595
Iteration 195/1000 | Loss: 0.00001595
Iteration 196/1000 | Loss: 0.00001595
Iteration 197/1000 | Loss: 0.00001595
Iteration 198/1000 | Loss: 0.00001595
Iteration 199/1000 | Loss: 0.00001595
Iteration 200/1000 | Loss: 0.00001595
Iteration 201/1000 | Loss: 0.00001594
Iteration 202/1000 | Loss: 0.00001594
Iteration 203/1000 | Loss: 0.00001594
Iteration 204/1000 | Loss: 0.00001594
Iteration 205/1000 | Loss: 0.00001594
Iteration 206/1000 | Loss: 0.00001594
Iteration 207/1000 | Loss: 0.00001594
Iteration 208/1000 | Loss: 0.00001594
Iteration 209/1000 | Loss: 0.00001594
Iteration 210/1000 | Loss: 0.00001594
Iteration 211/1000 | Loss: 0.00001594
Iteration 212/1000 | Loss: 0.00001594
Iteration 213/1000 | Loss: 0.00001594
Iteration 214/1000 | Loss: 0.00001594
Iteration 215/1000 | Loss: 0.00001594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 215. Stopping optimization.
Last 5 losses: [1.5940704543027095e-05, 1.5940704543027095e-05, 1.5940704543027095e-05, 1.5940704543027095e-05, 1.5940704543027095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5940704543027095e-05

Optimization complete. Final v2v error: 3.3119003772735596 mm

Highest mean error: 3.7581586837768555 mm for frame 88

Lowest mean error: 2.8103320598602295 mm for frame 16

Saving results

Total time: 41.70302867889404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969424
Iteration 2/25 | Loss: 0.00152563
Iteration 3/25 | Loss: 0.00141726
Iteration 4/25 | Loss: 0.00139876
Iteration 5/25 | Loss: 0.00139354
Iteration 6/25 | Loss: 0.00139304
Iteration 7/25 | Loss: 0.00139304
Iteration 8/25 | Loss: 0.00139304
Iteration 9/25 | Loss: 0.00139304
Iteration 10/25 | Loss: 0.00139304
Iteration 11/25 | Loss: 0.00139304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013930372660979629, 0.0013930372660979629, 0.0013930372660979629, 0.0013930372660979629, 0.0013930372660979629]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013930372660979629

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.68244624
Iteration 2/25 | Loss: 0.00119243
Iteration 3/25 | Loss: 0.00119242
Iteration 4/25 | Loss: 0.00119242
Iteration 5/25 | Loss: 0.00119242
Iteration 6/25 | Loss: 0.00119242
Iteration 7/25 | Loss: 0.00119242
Iteration 8/25 | Loss: 0.00119242
Iteration 9/25 | Loss: 0.00119241
Iteration 10/25 | Loss: 0.00119241
Iteration 11/25 | Loss: 0.00119241
Iteration 12/25 | Loss: 0.00119241
Iteration 13/25 | Loss: 0.00119241
Iteration 14/25 | Loss: 0.00119241
Iteration 15/25 | Loss: 0.00119241
Iteration 16/25 | Loss: 0.00119241
Iteration 17/25 | Loss: 0.00119241
Iteration 18/25 | Loss: 0.00119241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011924144346266985, 0.0011924144346266985, 0.0011924144346266985, 0.0011924144346266985, 0.0011924144346266985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011924144346266985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119241
Iteration 2/1000 | Loss: 0.00006386
Iteration 3/1000 | Loss: 0.00003703
Iteration 4/1000 | Loss: 0.00002996
Iteration 5/1000 | Loss: 0.00002763
Iteration 6/1000 | Loss: 0.00002630
Iteration 7/1000 | Loss: 0.00002570
Iteration 8/1000 | Loss: 0.00002522
Iteration 9/1000 | Loss: 0.00002483
Iteration 10/1000 | Loss: 0.00002459
Iteration 11/1000 | Loss: 0.00002436
Iteration 12/1000 | Loss: 0.00002420
Iteration 13/1000 | Loss: 0.00002405
Iteration 14/1000 | Loss: 0.00002404
Iteration 15/1000 | Loss: 0.00002404
Iteration 16/1000 | Loss: 0.00002398
Iteration 17/1000 | Loss: 0.00002395
Iteration 18/1000 | Loss: 0.00002393
Iteration 19/1000 | Loss: 0.00002390
Iteration 20/1000 | Loss: 0.00002386
Iteration 21/1000 | Loss: 0.00002385
Iteration 22/1000 | Loss: 0.00002380
Iteration 23/1000 | Loss: 0.00002380
Iteration 24/1000 | Loss: 0.00002379
Iteration 25/1000 | Loss: 0.00002376
Iteration 26/1000 | Loss: 0.00002371
Iteration 27/1000 | Loss: 0.00002367
Iteration 28/1000 | Loss: 0.00002364
Iteration 29/1000 | Loss: 0.00002363
Iteration 30/1000 | Loss: 0.00002363
Iteration 31/1000 | Loss: 0.00002357
Iteration 32/1000 | Loss: 0.00002357
Iteration 33/1000 | Loss: 0.00002355
Iteration 34/1000 | Loss: 0.00002355
Iteration 35/1000 | Loss: 0.00002354
Iteration 36/1000 | Loss: 0.00002354
Iteration 37/1000 | Loss: 0.00002353
Iteration 38/1000 | Loss: 0.00002353
Iteration 39/1000 | Loss: 0.00002352
Iteration 40/1000 | Loss: 0.00002352
Iteration 41/1000 | Loss: 0.00002352
Iteration 42/1000 | Loss: 0.00002351
Iteration 43/1000 | Loss: 0.00002351
Iteration 44/1000 | Loss: 0.00002350
Iteration 45/1000 | Loss: 0.00002350
Iteration 46/1000 | Loss: 0.00002349
Iteration 47/1000 | Loss: 0.00002349
Iteration 48/1000 | Loss: 0.00002349
Iteration 49/1000 | Loss: 0.00002348
Iteration 50/1000 | Loss: 0.00002348
Iteration 51/1000 | Loss: 0.00002348
Iteration 52/1000 | Loss: 0.00002348
Iteration 53/1000 | Loss: 0.00002347
Iteration 54/1000 | Loss: 0.00002347
Iteration 55/1000 | Loss: 0.00002347
Iteration 56/1000 | Loss: 0.00002347
Iteration 57/1000 | Loss: 0.00002346
Iteration 58/1000 | Loss: 0.00002346
Iteration 59/1000 | Loss: 0.00002346
Iteration 60/1000 | Loss: 0.00002346
Iteration 61/1000 | Loss: 0.00002345
Iteration 62/1000 | Loss: 0.00002345
Iteration 63/1000 | Loss: 0.00002345
Iteration 64/1000 | Loss: 0.00002345
Iteration 65/1000 | Loss: 0.00002345
Iteration 66/1000 | Loss: 0.00002345
Iteration 67/1000 | Loss: 0.00002345
Iteration 68/1000 | Loss: 0.00002344
Iteration 69/1000 | Loss: 0.00002344
Iteration 70/1000 | Loss: 0.00002343
Iteration 71/1000 | Loss: 0.00002343
Iteration 72/1000 | Loss: 0.00002343
Iteration 73/1000 | Loss: 0.00002343
Iteration 74/1000 | Loss: 0.00002343
Iteration 75/1000 | Loss: 0.00002343
Iteration 76/1000 | Loss: 0.00002342
Iteration 77/1000 | Loss: 0.00002342
Iteration 78/1000 | Loss: 0.00002342
Iteration 79/1000 | Loss: 0.00002341
Iteration 80/1000 | Loss: 0.00002341
Iteration 81/1000 | Loss: 0.00002341
Iteration 82/1000 | Loss: 0.00002341
Iteration 83/1000 | Loss: 0.00002341
Iteration 84/1000 | Loss: 0.00002341
Iteration 85/1000 | Loss: 0.00002341
Iteration 86/1000 | Loss: 0.00002341
Iteration 87/1000 | Loss: 0.00002341
Iteration 88/1000 | Loss: 0.00002340
Iteration 89/1000 | Loss: 0.00002340
Iteration 90/1000 | Loss: 0.00002340
Iteration 91/1000 | Loss: 0.00002339
Iteration 92/1000 | Loss: 0.00002339
Iteration 93/1000 | Loss: 0.00002338
Iteration 94/1000 | Loss: 0.00002338
Iteration 95/1000 | Loss: 0.00002338
Iteration 96/1000 | Loss: 0.00002337
Iteration 97/1000 | Loss: 0.00002337
Iteration 98/1000 | Loss: 0.00002337
Iteration 99/1000 | Loss: 0.00002336
Iteration 100/1000 | Loss: 0.00002336
Iteration 101/1000 | Loss: 0.00002336
Iteration 102/1000 | Loss: 0.00002336
Iteration 103/1000 | Loss: 0.00002335
Iteration 104/1000 | Loss: 0.00002335
Iteration 105/1000 | Loss: 0.00002335
Iteration 106/1000 | Loss: 0.00002335
Iteration 107/1000 | Loss: 0.00002334
Iteration 108/1000 | Loss: 0.00002334
Iteration 109/1000 | Loss: 0.00002334
Iteration 110/1000 | Loss: 0.00002334
Iteration 111/1000 | Loss: 0.00002334
Iteration 112/1000 | Loss: 0.00002334
Iteration 113/1000 | Loss: 0.00002334
Iteration 114/1000 | Loss: 0.00002334
Iteration 115/1000 | Loss: 0.00002334
Iteration 116/1000 | Loss: 0.00002334
Iteration 117/1000 | Loss: 0.00002334
Iteration 118/1000 | Loss: 0.00002334
Iteration 119/1000 | Loss: 0.00002334
Iteration 120/1000 | Loss: 0.00002334
Iteration 121/1000 | Loss: 0.00002334
Iteration 122/1000 | Loss: 0.00002333
Iteration 123/1000 | Loss: 0.00002333
Iteration 124/1000 | Loss: 0.00002333
Iteration 125/1000 | Loss: 0.00002333
Iteration 126/1000 | Loss: 0.00002333
Iteration 127/1000 | Loss: 0.00002333
Iteration 128/1000 | Loss: 0.00002333
Iteration 129/1000 | Loss: 0.00002333
Iteration 130/1000 | Loss: 0.00002333
Iteration 131/1000 | Loss: 0.00002333
Iteration 132/1000 | Loss: 0.00002333
Iteration 133/1000 | Loss: 0.00002333
Iteration 134/1000 | Loss: 0.00002333
Iteration 135/1000 | Loss: 0.00002333
Iteration 136/1000 | Loss: 0.00002333
Iteration 137/1000 | Loss: 0.00002333
Iteration 138/1000 | Loss: 0.00002333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.333415613975376e-05, 2.333415613975376e-05, 2.333415613975376e-05, 2.333415613975376e-05, 2.333415613975376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.333415613975376e-05

Optimization complete. Final v2v error: 4.041645526885986 mm

Highest mean error: 4.3307881355285645 mm for frame 52

Lowest mean error: 3.736924171447754 mm for frame 120

Saving results

Total time: 37.74077224731445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052598
Iteration 2/25 | Loss: 0.01052598
Iteration 3/25 | Loss: 0.01052598
Iteration 4/25 | Loss: 0.01052598
Iteration 5/25 | Loss: 0.01052598
Iteration 6/25 | Loss: 0.01052598
Iteration 7/25 | Loss: 0.01052598
Iteration 8/25 | Loss: 0.01052598
Iteration 9/25 | Loss: 0.01052598
Iteration 10/25 | Loss: 0.01052598
Iteration 11/25 | Loss: 0.01052598
Iteration 12/25 | Loss: 0.01052597
Iteration 13/25 | Loss: 0.01052597
Iteration 14/25 | Loss: 0.01052597
Iteration 15/25 | Loss: 0.01052597
Iteration 16/25 | Loss: 0.01052597
Iteration 17/25 | Loss: 0.01052597
Iteration 18/25 | Loss: 0.01052597
Iteration 19/25 | Loss: 0.01052597
Iteration 20/25 | Loss: 0.01052597
Iteration 21/25 | Loss: 0.01052597
Iteration 22/25 | Loss: 0.01052597
Iteration 23/25 | Loss: 0.01052597
Iteration 24/25 | Loss: 0.01052597
Iteration 25/25 | Loss: 0.01052597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81603551
Iteration 2/25 | Loss: 0.05903211
Iteration 3/25 | Loss: 0.05892063
Iteration 4/25 | Loss: 0.05886522
Iteration 5/25 | Loss: 0.05884708
Iteration 6/25 | Loss: 0.05884707
Iteration 7/25 | Loss: 0.05884706
Iteration 8/25 | Loss: 0.05884706
Iteration 9/25 | Loss: 0.05884706
Iteration 10/25 | Loss: 0.05884706
Iteration 11/25 | Loss: 0.05884706
Iteration 12/25 | Loss: 0.05884706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.058847058564424515, 0.058847058564424515, 0.058847058564424515, 0.058847058564424515, 0.058847058564424515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.058847058564424515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05884706
Iteration 2/1000 | Loss: 0.00281273
Iteration 3/1000 | Loss: 0.00289407
Iteration 4/1000 | Loss: 0.00214353
Iteration 5/1000 | Loss: 0.00075206
Iteration 6/1000 | Loss: 0.00093847
Iteration 7/1000 | Loss: 0.00048525
Iteration 8/1000 | Loss: 0.00013082
Iteration 9/1000 | Loss: 0.00056096
Iteration 10/1000 | Loss: 0.00026091
Iteration 11/1000 | Loss: 0.00036255
Iteration 12/1000 | Loss: 0.00009902
Iteration 13/1000 | Loss: 0.00013866
Iteration 14/1000 | Loss: 0.00018048
Iteration 15/1000 | Loss: 0.00038269
Iteration 16/1000 | Loss: 0.00004190
Iteration 17/1000 | Loss: 0.00017416
Iteration 18/1000 | Loss: 0.00012525
Iteration 19/1000 | Loss: 0.00024104
Iteration 20/1000 | Loss: 0.00017840
Iteration 21/1000 | Loss: 0.00005999
Iteration 22/1000 | Loss: 0.00041644
Iteration 23/1000 | Loss: 0.00046082
Iteration 24/1000 | Loss: 0.00003789
Iteration 25/1000 | Loss: 0.00011517
Iteration 26/1000 | Loss: 0.00004318
Iteration 27/1000 | Loss: 0.00010888
Iteration 28/1000 | Loss: 0.00015736
Iteration 29/1000 | Loss: 0.00007070
Iteration 30/1000 | Loss: 0.00005377
Iteration 31/1000 | Loss: 0.00004864
Iteration 32/1000 | Loss: 0.00002977
Iteration 33/1000 | Loss: 0.00003999
Iteration 34/1000 | Loss: 0.00002903
Iteration 35/1000 | Loss: 0.00010466
Iteration 36/1000 | Loss: 0.00034672
Iteration 37/1000 | Loss: 0.00018058
Iteration 38/1000 | Loss: 0.00014130
Iteration 39/1000 | Loss: 0.00048506
Iteration 40/1000 | Loss: 0.00018826
Iteration 41/1000 | Loss: 0.00079187
Iteration 42/1000 | Loss: 0.00071171
Iteration 43/1000 | Loss: 0.00084335
Iteration 44/1000 | Loss: 0.00078188
Iteration 45/1000 | Loss: 0.00004420
Iteration 46/1000 | Loss: 0.00025556
Iteration 47/1000 | Loss: 0.00004004
Iteration 48/1000 | Loss: 0.00004632
Iteration 49/1000 | Loss: 0.00002960
Iteration 50/1000 | Loss: 0.00007222
Iteration 51/1000 | Loss: 0.00003223
Iteration 52/1000 | Loss: 0.00002479
Iteration 53/1000 | Loss: 0.00007635
Iteration 54/1000 | Loss: 0.00009408
Iteration 55/1000 | Loss: 0.00002615
Iteration 56/1000 | Loss: 0.00002454
Iteration 57/1000 | Loss: 0.00002286
Iteration 58/1000 | Loss: 0.00002285
Iteration 59/1000 | Loss: 0.00002284
Iteration 60/1000 | Loss: 0.00002283
Iteration 61/1000 | Loss: 0.00013260
Iteration 62/1000 | Loss: 0.00006490
Iteration 63/1000 | Loss: 0.00002269
Iteration 64/1000 | Loss: 0.00002246
Iteration 65/1000 | Loss: 0.00004871
Iteration 66/1000 | Loss: 0.00002229
Iteration 67/1000 | Loss: 0.00002228
Iteration 68/1000 | Loss: 0.00002226
Iteration 69/1000 | Loss: 0.00002224
Iteration 70/1000 | Loss: 0.00002224
Iteration 71/1000 | Loss: 0.00002224
Iteration 72/1000 | Loss: 0.00002224
Iteration 73/1000 | Loss: 0.00002224
Iteration 74/1000 | Loss: 0.00002224
Iteration 75/1000 | Loss: 0.00002223
Iteration 76/1000 | Loss: 0.00002223
Iteration 77/1000 | Loss: 0.00002223
Iteration 78/1000 | Loss: 0.00002223
Iteration 79/1000 | Loss: 0.00002223
Iteration 80/1000 | Loss: 0.00002222
Iteration 81/1000 | Loss: 0.00002221
Iteration 82/1000 | Loss: 0.00007084
Iteration 83/1000 | Loss: 0.00002819
Iteration 84/1000 | Loss: 0.00002196
Iteration 85/1000 | Loss: 0.00002196
Iteration 86/1000 | Loss: 0.00002196
Iteration 87/1000 | Loss: 0.00002193
Iteration 88/1000 | Loss: 0.00002193
Iteration 89/1000 | Loss: 0.00002192
Iteration 90/1000 | Loss: 0.00002192
Iteration 91/1000 | Loss: 0.00002192
Iteration 92/1000 | Loss: 0.00002192
Iteration 93/1000 | Loss: 0.00002192
Iteration 94/1000 | Loss: 0.00003455
Iteration 95/1000 | Loss: 0.00003455
Iteration 96/1000 | Loss: 0.00012831
Iteration 97/1000 | Loss: 0.00006854
Iteration 98/1000 | Loss: 0.00003426
Iteration 99/1000 | Loss: 0.00002191
Iteration 100/1000 | Loss: 0.00003121
Iteration 101/1000 | Loss: 0.00003028
Iteration 102/1000 | Loss: 0.00008935
Iteration 103/1000 | Loss: 0.00149674
Iteration 104/1000 | Loss: 0.00005443
Iteration 105/1000 | Loss: 0.00003632
Iteration 106/1000 | Loss: 0.00003691
Iteration 107/1000 | Loss: 0.00008066
Iteration 108/1000 | Loss: 0.00002253
Iteration 109/1000 | Loss: 0.00002203
Iteration 110/1000 | Loss: 0.00004011
Iteration 111/1000 | Loss: 0.00002190
Iteration 112/1000 | Loss: 0.00002186
Iteration 113/1000 | Loss: 0.00004349
Iteration 114/1000 | Loss: 0.00002174
Iteration 115/1000 | Loss: 0.00002173
Iteration 116/1000 | Loss: 0.00002172
Iteration 117/1000 | Loss: 0.00002172
Iteration 118/1000 | Loss: 0.00002172
Iteration 119/1000 | Loss: 0.00002172
Iteration 120/1000 | Loss: 0.00002172
Iteration 121/1000 | Loss: 0.00002172
Iteration 122/1000 | Loss: 0.00002172
Iteration 123/1000 | Loss: 0.00002172
Iteration 124/1000 | Loss: 0.00002172
Iteration 125/1000 | Loss: 0.00002172
Iteration 126/1000 | Loss: 0.00002172
Iteration 127/1000 | Loss: 0.00002172
Iteration 128/1000 | Loss: 0.00002172
Iteration 129/1000 | Loss: 0.00002172
Iteration 130/1000 | Loss: 0.00002172
Iteration 131/1000 | Loss: 0.00002171
Iteration 132/1000 | Loss: 0.00002171
Iteration 133/1000 | Loss: 0.00002171
Iteration 134/1000 | Loss: 0.00002171
Iteration 135/1000 | Loss: 0.00002171
Iteration 136/1000 | Loss: 0.00002171
Iteration 137/1000 | Loss: 0.00002169
Iteration 138/1000 | Loss: 0.00002169
Iteration 139/1000 | Loss: 0.00002169
Iteration 140/1000 | Loss: 0.00002169
Iteration 141/1000 | Loss: 0.00002169
Iteration 142/1000 | Loss: 0.00002169
Iteration 143/1000 | Loss: 0.00002169
Iteration 144/1000 | Loss: 0.00002169
Iteration 145/1000 | Loss: 0.00002169
Iteration 146/1000 | Loss: 0.00002169
Iteration 147/1000 | Loss: 0.00002169
Iteration 148/1000 | Loss: 0.00002168
Iteration 149/1000 | Loss: 0.00002168
Iteration 150/1000 | Loss: 0.00002168
Iteration 151/1000 | Loss: 0.00002168
Iteration 152/1000 | Loss: 0.00002168
Iteration 153/1000 | Loss: 0.00002168
Iteration 154/1000 | Loss: 0.00002168
Iteration 155/1000 | Loss: 0.00002168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.1684863895643502e-05, 2.1684863895643502e-05, 2.1684863895643502e-05, 2.1684863895643502e-05, 2.1684863895643502e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1684863895643502e-05

Optimization complete. Final v2v error: 3.981172561645508 mm

Highest mean error: 4.887591361999512 mm for frame 38

Lowest mean error: 3.650796413421631 mm for frame 99

Saving results

Total time: 143.9345932006836
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00400050
Iteration 2/25 | Loss: 0.00130094
Iteration 3/25 | Loss: 0.00125244
Iteration 4/25 | Loss: 0.00124609
Iteration 5/25 | Loss: 0.00124402
Iteration 6/25 | Loss: 0.00124402
Iteration 7/25 | Loss: 0.00124402
Iteration 8/25 | Loss: 0.00124402
Iteration 9/25 | Loss: 0.00124402
Iteration 10/25 | Loss: 0.00124402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012440151767805219, 0.0012440151767805219, 0.0012440151767805219, 0.0012440151767805219, 0.0012440151767805219]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012440151767805219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45481277
Iteration 2/25 | Loss: 0.00087583
Iteration 3/25 | Loss: 0.00087583
Iteration 4/25 | Loss: 0.00087583
Iteration 5/25 | Loss: 0.00087583
Iteration 6/25 | Loss: 0.00087583
Iteration 7/25 | Loss: 0.00087583
Iteration 8/25 | Loss: 0.00087583
Iteration 9/25 | Loss: 0.00087583
Iteration 10/25 | Loss: 0.00087583
Iteration 11/25 | Loss: 0.00087583
Iteration 12/25 | Loss: 0.00087583
Iteration 13/25 | Loss: 0.00087583
Iteration 14/25 | Loss: 0.00087583
Iteration 15/25 | Loss: 0.00087583
Iteration 16/25 | Loss: 0.00087583
Iteration 17/25 | Loss: 0.00087583
Iteration 18/25 | Loss: 0.00087583
Iteration 19/25 | Loss: 0.00087583
Iteration 20/25 | Loss: 0.00087583
Iteration 21/25 | Loss: 0.00087583
Iteration 22/25 | Loss: 0.00087583
Iteration 23/25 | Loss: 0.00087583
Iteration 24/25 | Loss: 0.00087583
Iteration 25/25 | Loss: 0.00087583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087583
Iteration 2/1000 | Loss: 0.00002316
Iteration 3/1000 | Loss: 0.00001636
Iteration 4/1000 | Loss: 0.00001501
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001400
Iteration 7/1000 | Loss: 0.00001378
Iteration 8/1000 | Loss: 0.00001371
Iteration 9/1000 | Loss: 0.00001351
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001341
Iteration 12/1000 | Loss: 0.00001339
Iteration 13/1000 | Loss: 0.00001335
Iteration 14/1000 | Loss: 0.00001335
Iteration 15/1000 | Loss: 0.00001326
Iteration 16/1000 | Loss: 0.00001325
Iteration 17/1000 | Loss: 0.00001324
Iteration 18/1000 | Loss: 0.00001323
Iteration 19/1000 | Loss: 0.00001323
Iteration 20/1000 | Loss: 0.00001322
Iteration 21/1000 | Loss: 0.00001322
Iteration 22/1000 | Loss: 0.00001321
Iteration 23/1000 | Loss: 0.00001321
Iteration 24/1000 | Loss: 0.00001320
Iteration 25/1000 | Loss: 0.00001319
Iteration 26/1000 | Loss: 0.00001319
Iteration 27/1000 | Loss: 0.00001314
Iteration 28/1000 | Loss: 0.00001308
Iteration 29/1000 | Loss: 0.00001307
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001307
Iteration 32/1000 | Loss: 0.00001307
Iteration 33/1000 | Loss: 0.00001307
Iteration 34/1000 | Loss: 0.00001307
Iteration 35/1000 | Loss: 0.00001307
Iteration 36/1000 | Loss: 0.00001307
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001306
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001305
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001305
Iteration 45/1000 | Loss: 0.00001305
Iteration 46/1000 | Loss: 0.00001305
Iteration 47/1000 | Loss: 0.00001305
Iteration 48/1000 | Loss: 0.00001305
Iteration 49/1000 | Loss: 0.00001305
Iteration 50/1000 | Loss: 0.00001305
Iteration 51/1000 | Loss: 0.00001304
Iteration 52/1000 | Loss: 0.00001304
Iteration 53/1000 | Loss: 0.00001304
Iteration 54/1000 | Loss: 0.00001302
Iteration 55/1000 | Loss: 0.00001301
Iteration 56/1000 | Loss: 0.00001301
Iteration 57/1000 | Loss: 0.00001301
Iteration 58/1000 | Loss: 0.00001301
Iteration 59/1000 | Loss: 0.00001301
Iteration 60/1000 | Loss: 0.00001301
Iteration 61/1000 | Loss: 0.00001301
Iteration 62/1000 | Loss: 0.00001301
Iteration 63/1000 | Loss: 0.00001301
Iteration 64/1000 | Loss: 0.00001301
Iteration 65/1000 | Loss: 0.00001301
Iteration 66/1000 | Loss: 0.00001301
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001301
Iteration 69/1000 | Loss: 0.00001301
Iteration 70/1000 | Loss: 0.00001301
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001300
Iteration 81/1000 | Loss: 0.00001300
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001298
Iteration 89/1000 | Loss: 0.00001298
Iteration 90/1000 | Loss: 0.00001298
Iteration 91/1000 | Loss: 0.00001298
Iteration 92/1000 | Loss: 0.00001298
Iteration 93/1000 | Loss: 0.00001298
Iteration 94/1000 | Loss: 0.00001298
Iteration 95/1000 | Loss: 0.00001298
Iteration 96/1000 | Loss: 0.00001298
Iteration 97/1000 | Loss: 0.00001297
Iteration 98/1000 | Loss: 0.00001297
Iteration 99/1000 | Loss: 0.00001297
Iteration 100/1000 | Loss: 0.00001297
Iteration 101/1000 | Loss: 0.00001297
Iteration 102/1000 | Loss: 0.00001297
Iteration 103/1000 | Loss: 0.00001296
Iteration 104/1000 | Loss: 0.00001296
Iteration 105/1000 | Loss: 0.00001296
Iteration 106/1000 | Loss: 0.00001296
Iteration 107/1000 | Loss: 0.00001296
Iteration 108/1000 | Loss: 0.00001296
Iteration 109/1000 | Loss: 0.00001295
Iteration 110/1000 | Loss: 0.00001295
Iteration 111/1000 | Loss: 0.00001295
Iteration 112/1000 | Loss: 0.00001295
Iteration 113/1000 | Loss: 0.00001295
Iteration 114/1000 | Loss: 0.00001295
Iteration 115/1000 | Loss: 0.00001295
Iteration 116/1000 | Loss: 0.00001295
Iteration 117/1000 | Loss: 0.00001295
Iteration 118/1000 | Loss: 0.00001294
Iteration 119/1000 | Loss: 0.00001294
Iteration 120/1000 | Loss: 0.00001294
Iteration 121/1000 | Loss: 0.00001294
Iteration 122/1000 | Loss: 0.00001293
Iteration 123/1000 | Loss: 0.00001293
Iteration 124/1000 | Loss: 0.00001293
Iteration 125/1000 | Loss: 0.00001293
Iteration 126/1000 | Loss: 0.00001293
Iteration 127/1000 | Loss: 0.00001292
Iteration 128/1000 | Loss: 0.00001292
Iteration 129/1000 | Loss: 0.00001292
Iteration 130/1000 | Loss: 0.00001291
Iteration 131/1000 | Loss: 0.00001291
Iteration 132/1000 | Loss: 0.00001291
Iteration 133/1000 | Loss: 0.00001291
Iteration 134/1000 | Loss: 0.00001291
Iteration 135/1000 | Loss: 0.00001291
Iteration 136/1000 | Loss: 0.00001291
Iteration 137/1000 | Loss: 0.00001291
Iteration 138/1000 | Loss: 0.00001291
Iteration 139/1000 | Loss: 0.00001291
Iteration 140/1000 | Loss: 0.00001290
Iteration 141/1000 | Loss: 0.00001290
Iteration 142/1000 | Loss: 0.00001290
Iteration 143/1000 | Loss: 0.00001290
Iteration 144/1000 | Loss: 0.00001289
Iteration 145/1000 | Loss: 0.00001289
Iteration 146/1000 | Loss: 0.00001289
Iteration 147/1000 | Loss: 0.00001289
Iteration 148/1000 | Loss: 0.00001288
Iteration 149/1000 | Loss: 0.00001288
Iteration 150/1000 | Loss: 0.00001288
Iteration 151/1000 | Loss: 0.00001288
Iteration 152/1000 | Loss: 0.00001288
Iteration 153/1000 | Loss: 0.00001288
Iteration 154/1000 | Loss: 0.00001288
Iteration 155/1000 | Loss: 0.00001288
Iteration 156/1000 | Loss: 0.00001287
Iteration 157/1000 | Loss: 0.00001287
Iteration 158/1000 | Loss: 0.00001287
Iteration 159/1000 | Loss: 0.00001287
Iteration 160/1000 | Loss: 0.00001287
Iteration 161/1000 | Loss: 0.00001287
Iteration 162/1000 | Loss: 0.00001286
Iteration 163/1000 | Loss: 0.00001286
Iteration 164/1000 | Loss: 0.00001286
Iteration 165/1000 | Loss: 0.00001285
Iteration 166/1000 | Loss: 0.00001285
Iteration 167/1000 | Loss: 0.00001285
Iteration 168/1000 | Loss: 0.00001285
Iteration 169/1000 | Loss: 0.00001285
Iteration 170/1000 | Loss: 0.00001285
Iteration 171/1000 | Loss: 0.00001284
Iteration 172/1000 | Loss: 0.00001284
Iteration 173/1000 | Loss: 0.00001284
Iteration 174/1000 | Loss: 0.00001284
Iteration 175/1000 | Loss: 0.00001284
Iteration 176/1000 | Loss: 0.00001284
Iteration 177/1000 | Loss: 0.00001283
Iteration 178/1000 | Loss: 0.00001283
Iteration 179/1000 | Loss: 0.00001283
Iteration 180/1000 | Loss: 0.00001283
Iteration 181/1000 | Loss: 0.00001283
Iteration 182/1000 | Loss: 0.00001283
Iteration 183/1000 | Loss: 0.00001283
Iteration 184/1000 | Loss: 0.00001283
Iteration 185/1000 | Loss: 0.00001283
Iteration 186/1000 | Loss: 0.00001282
Iteration 187/1000 | Loss: 0.00001282
Iteration 188/1000 | Loss: 0.00001282
Iteration 189/1000 | Loss: 0.00001282
Iteration 190/1000 | Loss: 0.00001282
Iteration 191/1000 | Loss: 0.00001282
Iteration 192/1000 | Loss: 0.00001282
Iteration 193/1000 | Loss: 0.00001282
Iteration 194/1000 | Loss: 0.00001282
Iteration 195/1000 | Loss: 0.00001282
Iteration 196/1000 | Loss: 0.00001282
Iteration 197/1000 | Loss: 0.00001282
Iteration 198/1000 | Loss: 0.00001282
Iteration 199/1000 | Loss: 0.00001282
Iteration 200/1000 | Loss: 0.00001282
Iteration 201/1000 | Loss: 0.00001282
Iteration 202/1000 | Loss: 0.00001282
Iteration 203/1000 | Loss: 0.00001282
Iteration 204/1000 | Loss: 0.00001282
Iteration 205/1000 | Loss: 0.00001282
Iteration 206/1000 | Loss: 0.00001282
Iteration 207/1000 | Loss: 0.00001282
Iteration 208/1000 | Loss: 0.00001282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.2818963114114013e-05, 1.2818963114114013e-05, 1.2818963114114013e-05, 1.2818963114114013e-05, 1.2818963114114013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2818963114114013e-05

Optimization complete. Final v2v error: 3.055142879486084 mm

Highest mean error: 3.2714269161224365 mm for frame 102

Lowest mean error: 2.966010093688965 mm for frame 88

Saving results

Total time: 35.980000495910645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01012758
Iteration 2/25 | Loss: 0.00190249
Iteration 3/25 | Loss: 0.00159999
Iteration 4/25 | Loss: 0.00156786
Iteration 5/25 | Loss: 0.00145273
Iteration 6/25 | Loss: 0.00135857
Iteration 7/25 | Loss: 0.00135461
Iteration 8/25 | Loss: 0.00133481
Iteration 9/25 | Loss: 0.00132944
Iteration 10/25 | Loss: 0.00134872
Iteration 11/25 | Loss: 0.00131670
Iteration 12/25 | Loss: 0.00131433
Iteration 13/25 | Loss: 0.00130947
Iteration 14/25 | Loss: 0.00130751
Iteration 15/25 | Loss: 0.00130700
Iteration 16/25 | Loss: 0.00130688
Iteration 17/25 | Loss: 0.00130688
Iteration 18/25 | Loss: 0.00130688
Iteration 19/25 | Loss: 0.00130687
Iteration 20/25 | Loss: 0.00130687
Iteration 21/25 | Loss: 0.00130687
Iteration 22/25 | Loss: 0.00130687
Iteration 23/25 | Loss: 0.00130687
Iteration 24/25 | Loss: 0.00130686
Iteration 25/25 | Loss: 0.00130686

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53115141
Iteration 2/25 | Loss: 0.00101875
Iteration 3/25 | Loss: 0.00101875
Iteration 4/25 | Loss: 0.00101875
Iteration 5/25 | Loss: 0.00101875
Iteration 6/25 | Loss: 0.00101875
Iteration 7/25 | Loss: 0.00101875
Iteration 8/25 | Loss: 0.00101875
Iteration 9/25 | Loss: 0.00101875
Iteration 10/25 | Loss: 0.00101875
Iteration 11/25 | Loss: 0.00101875
Iteration 12/25 | Loss: 0.00101875
Iteration 13/25 | Loss: 0.00101875
Iteration 14/25 | Loss: 0.00101875
Iteration 15/25 | Loss: 0.00101875
Iteration 16/25 | Loss: 0.00101875
Iteration 17/25 | Loss: 0.00101875
Iteration 18/25 | Loss: 0.00101875
Iteration 19/25 | Loss: 0.00101875
Iteration 20/25 | Loss: 0.00101875
Iteration 21/25 | Loss: 0.00101875
Iteration 22/25 | Loss: 0.00101875
Iteration 23/25 | Loss: 0.00101875
Iteration 24/25 | Loss: 0.00101875
Iteration 25/25 | Loss: 0.00101875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101875
Iteration 2/1000 | Loss: 0.00005335
Iteration 3/1000 | Loss: 0.00003516
Iteration 4/1000 | Loss: 0.00002828
Iteration 5/1000 | Loss: 0.00002621
Iteration 6/1000 | Loss: 0.00002476
Iteration 7/1000 | Loss: 0.00002395
Iteration 8/1000 | Loss: 0.00002322
Iteration 9/1000 | Loss: 0.00002277
Iteration 10/1000 | Loss: 0.00002232
Iteration 11/1000 | Loss: 0.00002216
Iteration 12/1000 | Loss: 0.00002193
Iteration 13/1000 | Loss: 0.00046292
Iteration 14/1000 | Loss: 0.00002383
Iteration 15/1000 | Loss: 0.00002196
Iteration 16/1000 | Loss: 0.00002083
Iteration 17/1000 | Loss: 0.00002018
Iteration 18/1000 | Loss: 0.00001963
Iteration 19/1000 | Loss: 0.00001933
Iteration 20/1000 | Loss: 0.00001931
Iteration 21/1000 | Loss: 0.00001911
Iteration 22/1000 | Loss: 0.00001900
Iteration 23/1000 | Loss: 0.00001885
Iteration 24/1000 | Loss: 0.00001884
Iteration 25/1000 | Loss: 0.00001881
Iteration 26/1000 | Loss: 0.00001875
Iteration 27/1000 | Loss: 0.00001859
Iteration 28/1000 | Loss: 0.00001854
Iteration 29/1000 | Loss: 0.00001847
Iteration 30/1000 | Loss: 0.00001845
Iteration 31/1000 | Loss: 0.00001844
Iteration 32/1000 | Loss: 0.00001843
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00001842
Iteration 35/1000 | Loss: 0.00001839
Iteration 36/1000 | Loss: 0.00001836
Iteration 37/1000 | Loss: 0.00001836
Iteration 38/1000 | Loss: 0.00001836
Iteration 39/1000 | Loss: 0.00001836
Iteration 40/1000 | Loss: 0.00001836
Iteration 41/1000 | Loss: 0.00001836
Iteration 42/1000 | Loss: 0.00001836
Iteration 43/1000 | Loss: 0.00001835
Iteration 44/1000 | Loss: 0.00001835
Iteration 45/1000 | Loss: 0.00001835
Iteration 46/1000 | Loss: 0.00001835
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001834
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001834
Iteration 53/1000 | Loss: 0.00001834
Iteration 54/1000 | Loss: 0.00001833
Iteration 55/1000 | Loss: 0.00001833
Iteration 56/1000 | Loss: 0.00001833
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001833
Iteration 61/1000 | Loss: 0.00001833
Iteration 62/1000 | Loss: 0.00001833
Iteration 63/1000 | Loss: 0.00001833
Iteration 64/1000 | Loss: 0.00001833
Iteration 65/1000 | Loss: 0.00001833
Iteration 66/1000 | Loss: 0.00001832
Iteration 67/1000 | Loss: 0.00001832
Iteration 68/1000 | Loss: 0.00001832
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001831
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001830
Iteration 74/1000 | Loss: 0.00001830
Iteration 75/1000 | Loss: 0.00001830
Iteration 76/1000 | Loss: 0.00001830
Iteration 77/1000 | Loss: 0.00001830
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001830
Iteration 81/1000 | Loss: 0.00001830
Iteration 82/1000 | Loss: 0.00001830
Iteration 83/1000 | Loss: 0.00001829
Iteration 84/1000 | Loss: 0.00001829
Iteration 85/1000 | Loss: 0.00001829
Iteration 86/1000 | Loss: 0.00001829
Iteration 87/1000 | Loss: 0.00001829
Iteration 88/1000 | Loss: 0.00001829
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001829
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001828
Iteration 93/1000 | Loss: 0.00001828
Iteration 94/1000 | Loss: 0.00001828
Iteration 95/1000 | Loss: 0.00001828
Iteration 96/1000 | Loss: 0.00001828
Iteration 97/1000 | Loss: 0.00001827
Iteration 98/1000 | Loss: 0.00001827
Iteration 99/1000 | Loss: 0.00001827
Iteration 100/1000 | Loss: 0.00001827
Iteration 101/1000 | Loss: 0.00001826
Iteration 102/1000 | Loss: 0.00001826
Iteration 103/1000 | Loss: 0.00001826
Iteration 104/1000 | Loss: 0.00001826
Iteration 105/1000 | Loss: 0.00001826
Iteration 106/1000 | Loss: 0.00001826
Iteration 107/1000 | Loss: 0.00001826
Iteration 108/1000 | Loss: 0.00001826
Iteration 109/1000 | Loss: 0.00001825
Iteration 110/1000 | Loss: 0.00001825
Iteration 111/1000 | Loss: 0.00001825
Iteration 112/1000 | Loss: 0.00001825
Iteration 113/1000 | Loss: 0.00001824
Iteration 114/1000 | Loss: 0.00001824
Iteration 115/1000 | Loss: 0.00001824
Iteration 116/1000 | Loss: 0.00001824
Iteration 117/1000 | Loss: 0.00001824
Iteration 118/1000 | Loss: 0.00001824
Iteration 119/1000 | Loss: 0.00001824
Iteration 120/1000 | Loss: 0.00001824
Iteration 121/1000 | Loss: 0.00001824
Iteration 122/1000 | Loss: 0.00001824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.8244167222292162e-05, 1.8244167222292162e-05, 1.8244167222292162e-05, 1.8244167222292162e-05, 1.8244167222292162e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8244167222292162e-05

Optimization complete. Final v2v error: 3.709223508834839 mm

Highest mean error: 4.4871392250061035 mm for frame 3

Lowest mean error: 3.5113542079925537 mm for frame 92

Saving results

Total time: 79.08010172843933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806703
Iteration 2/25 | Loss: 0.00140017
Iteration 3/25 | Loss: 0.00128368
Iteration 4/25 | Loss: 0.00127380
Iteration 5/25 | Loss: 0.00127143
Iteration 6/25 | Loss: 0.00127143
Iteration 7/25 | Loss: 0.00127143
Iteration 8/25 | Loss: 0.00127143
Iteration 9/25 | Loss: 0.00127143
Iteration 10/25 | Loss: 0.00127143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012714280746877193, 0.0012714280746877193, 0.0012714280746877193, 0.0012714280746877193, 0.0012714280746877193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012714280746877193

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42845273
Iteration 2/25 | Loss: 0.00083222
Iteration 3/25 | Loss: 0.00083221
Iteration 4/25 | Loss: 0.00083221
Iteration 5/25 | Loss: 0.00083221
Iteration 6/25 | Loss: 0.00083221
Iteration 7/25 | Loss: 0.00083221
Iteration 8/25 | Loss: 0.00083221
Iteration 9/25 | Loss: 0.00083221
Iteration 10/25 | Loss: 0.00083221
Iteration 11/25 | Loss: 0.00083221
Iteration 12/25 | Loss: 0.00083221
Iteration 13/25 | Loss: 0.00083221
Iteration 14/25 | Loss: 0.00083221
Iteration 15/25 | Loss: 0.00083221
Iteration 16/25 | Loss: 0.00083221
Iteration 17/25 | Loss: 0.00083221
Iteration 18/25 | Loss: 0.00083221
Iteration 19/25 | Loss: 0.00083221
Iteration 20/25 | Loss: 0.00083221
Iteration 21/25 | Loss: 0.00083221
Iteration 22/25 | Loss: 0.00083221
Iteration 23/25 | Loss: 0.00083221
Iteration 24/25 | Loss: 0.00083221
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008322079083882272, 0.0008322079083882272, 0.0008322079083882272, 0.0008322079083882272, 0.0008322079083882272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008322079083882272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083221
Iteration 2/1000 | Loss: 0.00003108
Iteration 3/1000 | Loss: 0.00001963
Iteration 4/1000 | Loss: 0.00001658
Iteration 5/1000 | Loss: 0.00001569
Iteration 6/1000 | Loss: 0.00001491
Iteration 7/1000 | Loss: 0.00001438
Iteration 8/1000 | Loss: 0.00001408
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001340
Iteration 11/1000 | Loss: 0.00001338
Iteration 12/1000 | Loss: 0.00001319
Iteration 13/1000 | Loss: 0.00001314
Iteration 14/1000 | Loss: 0.00001305
Iteration 15/1000 | Loss: 0.00001303
Iteration 16/1000 | Loss: 0.00001303
Iteration 17/1000 | Loss: 0.00001302
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001302
Iteration 20/1000 | Loss: 0.00001300
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001298
Iteration 24/1000 | Loss: 0.00001298
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001297
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001296
Iteration 29/1000 | Loss: 0.00001295
Iteration 30/1000 | Loss: 0.00001292
Iteration 31/1000 | Loss: 0.00001291
Iteration 32/1000 | Loss: 0.00001290
Iteration 33/1000 | Loss: 0.00001289
Iteration 34/1000 | Loss: 0.00001287
Iteration 35/1000 | Loss: 0.00001285
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001283
Iteration 39/1000 | Loss: 0.00001283
Iteration 40/1000 | Loss: 0.00001283
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001283
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001283
Iteration 45/1000 | Loss: 0.00001282
Iteration 46/1000 | Loss: 0.00001281
Iteration 47/1000 | Loss: 0.00001280
Iteration 48/1000 | Loss: 0.00001280
Iteration 49/1000 | Loss: 0.00001280
Iteration 50/1000 | Loss: 0.00001279
Iteration 51/1000 | Loss: 0.00001279
Iteration 52/1000 | Loss: 0.00001278
Iteration 53/1000 | Loss: 0.00001277
Iteration 54/1000 | Loss: 0.00001277
Iteration 55/1000 | Loss: 0.00001276
Iteration 56/1000 | Loss: 0.00001276
Iteration 57/1000 | Loss: 0.00001276
Iteration 58/1000 | Loss: 0.00001275
Iteration 59/1000 | Loss: 0.00001275
Iteration 60/1000 | Loss: 0.00001275
Iteration 61/1000 | Loss: 0.00001275
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001274
Iteration 64/1000 | Loss: 0.00001274
Iteration 65/1000 | Loss: 0.00001273
Iteration 66/1000 | Loss: 0.00001273
Iteration 67/1000 | Loss: 0.00001272
Iteration 68/1000 | Loss: 0.00001272
Iteration 69/1000 | Loss: 0.00001272
Iteration 70/1000 | Loss: 0.00001272
Iteration 71/1000 | Loss: 0.00001272
Iteration 72/1000 | Loss: 0.00001272
Iteration 73/1000 | Loss: 0.00001272
Iteration 74/1000 | Loss: 0.00001272
Iteration 75/1000 | Loss: 0.00001272
Iteration 76/1000 | Loss: 0.00001272
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001271
Iteration 81/1000 | Loss: 0.00001271
Iteration 82/1000 | Loss: 0.00001271
Iteration 83/1000 | Loss: 0.00001271
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001270
Iteration 86/1000 | Loss: 0.00001270
Iteration 87/1000 | Loss: 0.00001270
Iteration 88/1000 | Loss: 0.00001270
Iteration 89/1000 | Loss: 0.00001270
Iteration 90/1000 | Loss: 0.00001269
Iteration 91/1000 | Loss: 0.00001269
Iteration 92/1000 | Loss: 0.00001269
Iteration 93/1000 | Loss: 0.00001268
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001267
Iteration 98/1000 | Loss: 0.00001267
Iteration 99/1000 | Loss: 0.00001267
Iteration 100/1000 | Loss: 0.00001267
Iteration 101/1000 | Loss: 0.00001267
Iteration 102/1000 | Loss: 0.00001267
Iteration 103/1000 | Loss: 0.00001266
Iteration 104/1000 | Loss: 0.00001266
Iteration 105/1000 | Loss: 0.00001266
Iteration 106/1000 | Loss: 0.00001266
Iteration 107/1000 | Loss: 0.00001266
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001266
Iteration 110/1000 | Loss: 0.00001266
Iteration 111/1000 | Loss: 0.00001266
Iteration 112/1000 | Loss: 0.00001266
Iteration 113/1000 | Loss: 0.00001266
Iteration 114/1000 | Loss: 0.00001266
Iteration 115/1000 | Loss: 0.00001265
Iteration 116/1000 | Loss: 0.00001265
Iteration 117/1000 | Loss: 0.00001265
Iteration 118/1000 | Loss: 0.00001265
Iteration 119/1000 | Loss: 0.00001265
Iteration 120/1000 | Loss: 0.00001265
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001265
Iteration 123/1000 | Loss: 0.00001265
Iteration 124/1000 | Loss: 0.00001265
Iteration 125/1000 | Loss: 0.00001265
Iteration 126/1000 | Loss: 0.00001265
Iteration 127/1000 | Loss: 0.00001265
Iteration 128/1000 | Loss: 0.00001265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.2650411918002646e-05, 1.2650411918002646e-05, 1.2650411918002646e-05, 1.2650411918002646e-05, 1.2650411918002646e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2650411918002646e-05

Optimization complete. Final v2v error: 3.0550084114074707 mm

Highest mean error: 3.2949938774108887 mm for frame 114

Lowest mean error: 2.913668632507324 mm for frame 62

Saving results

Total time: 39.99704885482788
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01084260
Iteration 2/25 | Loss: 0.00162236
Iteration 3/25 | Loss: 0.00140245
Iteration 4/25 | Loss: 0.00138990
Iteration 5/25 | Loss: 0.00139397
Iteration 6/25 | Loss: 0.00139018
Iteration 7/25 | Loss: 0.00138756
Iteration 8/25 | Loss: 0.00138704
Iteration 9/25 | Loss: 0.00138704
Iteration 10/25 | Loss: 0.00138704
Iteration 11/25 | Loss: 0.00138704
Iteration 12/25 | Loss: 0.00138704
Iteration 13/25 | Loss: 0.00138704
Iteration 14/25 | Loss: 0.00138703
Iteration 15/25 | Loss: 0.00138703
Iteration 16/25 | Loss: 0.00138703
Iteration 17/25 | Loss: 0.00138703
Iteration 18/25 | Loss: 0.00138703
Iteration 19/25 | Loss: 0.00138703
Iteration 20/25 | Loss: 0.00138703
Iteration 21/25 | Loss: 0.00138703
Iteration 22/25 | Loss: 0.00138703
Iteration 23/25 | Loss: 0.00138703
Iteration 24/25 | Loss: 0.00138703
Iteration 25/25 | Loss: 0.00138703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.90899932
Iteration 2/25 | Loss: 0.00070390
Iteration 3/25 | Loss: 0.00070390
Iteration 4/25 | Loss: 0.00070390
Iteration 5/25 | Loss: 0.00070390
Iteration 6/25 | Loss: 0.00070390
Iteration 7/25 | Loss: 0.00070390
Iteration 8/25 | Loss: 0.00070390
Iteration 9/25 | Loss: 0.00070390
Iteration 10/25 | Loss: 0.00070390
Iteration 11/25 | Loss: 0.00070390
Iteration 12/25 | Loss: 0.00070390
Iteration 13/25 | Loss: 0.00070390
Iteration 14/25 | Loss: 0.00070390
Iteration 15/25 | Loss: 0.00070390
Iteration 16/25 | Loss: 0.00070390
Iteration 17/25 | Loss: 0.00070390
Iteration 18/25 | Loss: 0.00070390
Iteration 19/25 | Loss: 0.00070390
Iteration 20/25 | Loss: 0.00070390
Iteration 21/25 | Loss: 0.00070390
Iteration 22/25 | Loss: 0.00070390
Iteration 23/25 | Loss: 0.00070390
Iteration 24/25 | Loss: 0.00070390
Iteration 25/25 | Loss: 0.00070390

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070390
Iteration 2/1000 | Loss: 0.00004445
Iteration 3/1000 | Loss: 0.00002979
Iteration 4/1000 | Loss: 0.00002603
Iteration 5/1000 | Loss: 0.00002486
Iteration 6/1000 | Loss: 0.00002435
Iteration 7/1000 | Loss: 0.00010574
Iteration 8/1000 | Loss: 0.00003149
Iteration 9/1000 | Loss: 0.00002559
Iteration 10/1000 | Loss: 0.00002353
Iteration 11/1000 | Loss: 0.00002319
Iteration 12/1000 | Loss: 0.00014356
Iteration 13/1000 | Loss: 0.00006998
Iteration 14/1000 | Loss: 0.00011567
Iteration 15/1000 | Loss: 0.00004591
Iteration 16/1000 | Loss: 0.00004568
Iteration 17/1000 | Loss: 0.00003481
Iteration 18/1000 | Loss: 0.00002287
Iteration 19/1000 | Loss: 0.00004130
Iteration 20/1000 | Loss: 0.00002266
Iteration 21/1000 | Loss: 0.00002250
Iteration 22/1000 | Loss: 0.00002249
Iteration 23/1000 | Loss: 0.00002231
Iteration 24/1000 | Loss: 0.00002222
Iteration 25/1000 | Loss: 0.00002221
Iteration 26/1000 | Loss: 0.00002218
Iteration 27/1000 | Loss: 0.00002218
Iteration 28/1000 | Loss: 0.00002218
Iteration 29/1000 | Loss: 0.00002218
Iteration 30/1000 | Loss: 0.00002217
Iteration 31/1000 | Loss: 0.00002214
Iteration 32/1000 | Loss: 0.00002213
Iteration 33/1000 | Loss: 0.00002213
Iteration 34/1000 | Loss: 0.00002213
Iteration 35/1000 | Loss: 0.00002210
Iteration 36/1000 | Loss: 0.00002209
Iteration 37/1000 | Loss: 0.00002209
Iteration 38/1000 | Loss: 0.00002208
Iteration 39/1000 | Loss: 0.00002206
Iteration 40/1000 | Loss: 0.00002205
Iteration 41/1000 | Loss: 0.00002205
Iteration 42/1000 | Loss: 0.00002205
Iteration 43/1000 | Loss: 0.00002204
Iteration 44/1000 | Loss: 0.00002204
Iteration 45/1000 | Loss: 0.00002204
Iteration 46/1000 | Loss: 0.00002203
Iteration 47/1000 | Loss: 0.00002203
Iteration 48/1000 | Loss: 0.00002203
Iteration 49/1000 | Loss: 0.00002203
Iteration 50/1000 | Loss: 0.00002203
Iteration 51/1000 | Loss: 0.00002202
Iteration 52/1000 | Loss: 0.00002202
Iteration 53/1000 | Loss: 0.00002202
Iteration 54/1000 | Loss: 0.00002202
Iteration 55/1000 | Loss: 0.00002202
Iteration 56/1000 | Loss: 0.00002202
Iteration 57/1000 | Loss: 0.00002202
Iteration 58/1000 | Loss: 0.00002202
Iteration 59/1000 | Loss: 0.00002202
Iteration 60/1000 | Loss: 0.00002202
Iteration 61/1000 | Loss: 0.00002202
Iteration 62/1000 | Loss: 0.00002202
Iteration 63/1000 | Loss: 0.00002201
Iteration 64/1000 | Loss: 0.00002201
Iteration 65/1000 | Loss: 0.00002201
Iteration 66/1000 | Loss: 0.00002201
Iteration 67/1000 | Loss: 0.00002201
Iteration 68/1000 | Loss: 0.00002201
Iteration 69/1000 | Loss: 0.00002201
Iteration 70/1000 | Loss: 0.00002201
Iteration 71/1000 | Loss: 0.00002200
Iteration 72/1000 | Loss: 0.00002200
Iteration 73/1000 | Loss: 0.00002200
Iteration 74/1000 | Loss: 0.00002200
Iteration 75/1000 | Loss: 0.00002200
Iteration 76/1000 | Loss: 0.00002200
Iteration 77/1000 | Loss: 0.00002200
Iteration 78/1000 | Loss: 0.00002200
Iteration 79/1000 | Loss: 0.00002199
Iteration 80/1000 | Loss: 0.00002199
Iteration 81/1000 | Loss: 0.00002199
Iteration 82/1000 | Loss: 0.00002199
Iteration 83/1000 | Loss: 0.00002199
Iteration 84/1000 | Loss: 0.00002199
Iteration 85/1000 | Loss: 0.00002199
Iteration 86/1000 | Loss: 0.00002199
Iteration 87/1000 | Loss: 0.00002199
Iteration 88/1000 | Loss: 0.00002199
Iteration 89/1000 | Loss: 0.00002199
Iteration 90/1000 | Loss: 0.00002199
Iteration 91/1000 | Loss: 0.00002199
Iteration 92/1000 | Loss: 0.00002199
Iteration 93/1000 | Loss: 0.00002199
Iteration 94/1000 | Loss: 0.00002199
Iteration 95/1000 | Loss: 0.00002199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.198991751356516e-05, 2.198991751356516e-05, 2.198991751356516e-05, 2.198991751356516e-05, 2.198991751356516e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.198991751356516e-05

Optimization complete. Final v2v error: 3.8121635913848877 mm

Highest mean error: 4.264753341674805 mm for frame 104

Lowest mean error: 3.4900848865509033 mm for frame 33

Saving results

Total time: 50.841808557510376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902721
Iteration 2/25 | Loss: 0.00196105
Iteration 3/25 | Loss: 0.00162047
Iteration 4/25 | Loss: 0.00155411
Iteration 5/25 | Loss: 0.00154008
Iteration 6/25 | Loss: 0.00145466
Iteration 7/25 | Loss: 0.00143006
Iteration 8/25 | Loss: 0.00141082
Iteration 9/25 | Loss: 0.00140510
Iteration 10/25 | Loss: 0.00141983
Iteration 11/25 | Loss: 0.00142259
Iteration 12/25 | Loss: 0.00140856
Iteration 13/25 | Loss: 0.00139548
Iteration 14/25 | Loss: 0.00138287
Iteration 15/25 | Loss: 0.00137942
Iteration 16/25 | Loss: 0.00137849
Iteration 17/25 | Loss: 0.00137820
Iteration 18/25 | Loss: 0.00137813
Iteration 19/25 | Loss: 0.00137812
Iteration 20/25 | Loss: 0.00137811
Iteration 21/25 | Loss: 0.00137810
Iteration 22/25 | Loss: 0.00137809
Iteration 23/25 | Loss: 0.00137809
Iteration 24/25 | Loss: 0.00137809
Iteration 25/25 | Loss: 0.00137809

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.34280610
Iteration 2/25 | Loss: 0.00099206
Iteration 3/25 | Loss: 0.00099203
Iteration 4/25 | Loss: 0.00099203
Iteration 5/25 | Loss: 0.00099203
Iteration 6/25 | Loss: 0.00099203
Iteration 7/25 | Loss: 0.00099203
Iteration 8/25 | Loss: 0.00099203
Iteration 9/25 | Loss: 0.00099203
Iteration 10/25 | Loss: 0.00099203
Iteration 11/25 | Loss: 0.00099203
Iteration 12/25 | Loss: 0.00099203
Iteration 13/25 | Loss: 0.00099203
Iteration 14/25 | Loss: 0.00099203
Iteration 15/25 | Loss: 0.00099203
Iteration 16/25 | Loss: 0.00099203
Iteration 17/25 | Loss: 0.00099203
Iteration 18/25 | Loss: 0.00099203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009920262964442372, 0.0009920262964442372, 0.0009920262964442372, 0.0009920262964442372, 0.0009920262964442372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009920262964442372

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099203
Iteration 2/1000 | Loss: 0.00035210
Iteration 3/1000 | Loss: 0.00034122
Iteration 4/1000 | Loss: 0.00023143
Iteration 5/1000 | Loss: 0.00022485
Iteration 6/1000 | Loss: 0.00013407
Iteration 7/1000 | Loss: 0.00009751
Iteration 8/1000 | Loss: 0.00008576
Iteration 9/1000 | Loss: 0.00007796
Iteration 10/1000 | Loss: 0.00011122
Iteration 11/1000 | Loss: 0.00034334
Iteration 12/1000 | Loss: 0.00014155
Iteration 13/1000 | Loss: 0.00012321
Iteration 14/1000 | Loss: 0.00010378
Iteration 15/1000 | Loss: 0.00009477
Iteration 16/1000 | Loss: 0.00015119
Iteration 17/1000 | Loss: 0.00014914
Iteration 18/1000 | Loss: 0.00008926
Iteration 19/1000 | Loss: 0.00008204
Iteration 20/1000 | Loss: 0.00006989
Iteration 21/1000 | Loss: 0.00006049
Iteration 22/1000 | Loss: 0.00005530
Iteration 23/1000 | Loss: 0.00005098
Iteration 24/1000 | Loss: 0.00004662
Iteration 25/1000 | Loss: 0.00004337
Iteration 26/1000 | Loss: 0.00004089
Iteration 27/1000 | Loss: 0.00003901
Iteration 28/1000 | Loss: 0.00007155
Iteration 29/1000 | Loss: 0.00005948
Iteration 30/1000 | Loss: 0.00006722
Iteration 31/1000 | Loss: 0.00006115
Iteration 32/1000 | Loss: 0.00006555
Iteration 33/1000 | Loss: 0.00005087
Iteration 34/1000 | Loss: 0.00006277
Iteration 35/1000 | Loss: 0.00005325
Iteration 36/1000 | Loss: 0.00004655
Iteration 37/1000 | Loss: 0.00003512
Iteration 38/1000 | Loss: 0.00004956
Iteration 39/1000 | Loss: 0.00008434
Iteration 40/1000 | Loss: 0.00004257
Iteration 41/1000 | Loss: 0.00010212
Iteration 42/1000 | Loss: 0.00006996
Iteration 43/1000 | Loss: 0.00007785
Iteration 44/1000 | Loss: 0.00005981
Iteration 45/1000 | Loss: 0.00003742
Iteration 46/1000 | Loss: 0.00003418
Iteration 47/1000 | Loss: 0.00003335
Iteration 48/1000 | Loss: 0.00004194
Iteration 49/1000 | Loss: 0.00003759
Iteration 50/1000 | Loss: 0.00003368
Iteration 51/1000 | Loss: 0.00003863
Iteration 52/1000 | Loss: 0.00003492
Iteration 53/1000 | Loss: 0.00003075
Iteration 54/1000 | Loss: 0.00005995
Iteration 55/1000 | Loss: 0.00015261
Iteration 56/1000 | Loss: 0.00013255
Iteration 57/1000 | Loss: 0.00011918
Iteration 58/1000 | Loss: 0.00009500
Iteration 59/1000 | Loss: 0.00006539
Iteration 60/1000 | Loss: 0.00006840
Iteration 61/1000 | Loss: 0.00005650
Iteration 62/1000 | Loss: 0.00005216
Iteration 63/1000 | Loss: 0.00006548
Iteration 64/1000 | Loss: 0.00005562
Iteration 65/1000 | Loss: 0.00005315
Iteration 66/1000 | Loss: 0.00004687
Iteration 67/1000 | Loss: 0.00004936
Iteration 68/1000 | Loss: 0.00005859
Iteration 69/1000 | Loss: 0.00004443
Iteration 70/1000 | Loss: 0.00003567
Iteration 71/1000 | Loss: 0.00003388
Iteration 72/1000 | Loss: 0.00004003
Iteration 73/1000 | Loss: 0.00003783
Iteration 74/1000 | Loss: 0.00003348
Iteration 75/1000 | Loss: 0.00003781
Iteration 76/1000 | Loss: 0.00003481
Iteration 77/1000 | Loss: 0.00003239
Iteration 78/1000 | Loss: 0.00003602
Iteration 79/1000 | Loss: 0.00003333
Iteration 80/1000 | Loss: 0.00003141
Iteration 81/1000 | Loss: 0.00006399
Iteration 82/1000 | Loss: 0.00005775
Iteration 83/1000 | Loss: 0.00003925
Iteration 84/1000 | Loss: 0.00006966
Iteration 85/1000 | Loss: 0.00005294
Iteration 86/1000 | Loss: 0.00005141
Iteration 87/1000 | Loss: 0.00005900
Iteration 88/1000 | Loss: 0.00003433
Iteration 89/1000 | Loss: 0.00003203
Iteration 90/1000 | Loss: 0.00003816
Iteration 91/1000 | Loss: 0.00006732
Iteration 92/1000 | Loss: 0.00005247
Iteration 93/1000 | Loss: 0.00005788
Iteration 94/1000 | Loss: 0.00003521
Iteration 95/1000 | Loss: 0.00006264
Iteration 96/1000 | Loss: 0.00005653
Iteration 97/1000 | Loss: 0.00003163
Iteration 98/1000 | Loss: 0.00003075
Iteration 99/1000 | Loss: 0.00005975
Iteration 100/1000 | Loss: 0.00020214
Iteration 101/1000 | Loss: 0.00012917
Iteration 102/1000 | Loss: 0.00006610
Iteration 103/1000 | Loss: 0.00005951
Iteration 104/1000 | Loss: 0.00004244
Iteration 105/1000 | Loss: 0.00003720
Iteration 106/1000 | Loss: 0.00003235
Iteration 107/1000 | Loss: 0.00003157
Iteration 108/1000 | Loss: 0.00003434
Iteration 109/1000 | Loss: 0.00003115
Iteration 110/1000 | Loss: 0.00008892
Iteration 111/1000 | Loss: 0.00027281
Iteration 112/1000 | Loss: 0.00012827
Iteration 113/1000 | Loss: 0.00004212
Iteration 114/1000 | Loss: 0.00003616
Iteration 115/1000 | Loss: 0.00003360
Iteration 116/1000 | Loss: 0.00003175
Iteration 117/1000 | Loss: 0.00003068
Iteration 118/1000 | Loss: 0.00014356
Iteration 119/1000 | Loss: 0.00006379
Iteration 120/1000 | Loss: 0.00012847
Iteration 121/1000 | Loss: 0.00005225
Iteration 122/1000 | Loss: 0.00015041
Iteration 123/1000 | Loss: 0.00005139
Iteration 124/1000 | Loss: 0.00005328
Iteration 125/1000 | Loss: 0.00004137
Iteration 126/1000 | Loss: 0.00002922
Iteration 127/1000 | Loss: 0.00014860
Iteration 128/1000 | Loss: 0.00010188
Iteration 129/1000 | Loss: 0.00008379
Iteration 130/1000 | Loss: 0.00002996
Iteration 131/1000 | Loss: 0.00002833
Iteration 132/1000 | Loss: 0.00002794
Iteration 133/1000 | Loss: 0.00002765
Iteration 134/1000 | Loss: 0.00002744
Iteration 135/1000 | Loss: 0.00002740
Iteration 136/1000 | Loss: 0.00002738
Iteration 137/1000 | Loss: 0.00002732
Iteration 138/1000 | Loss: 0.00002730
Iteration 139/1000 | Loss: 0.00002729
Iteration 140/1000 | Loss: 0.00002726
Iteration 141/1000 | Loss: 0.00002719
Iteration 142/1000 | Loss: 0.00002717
Iteration 143/1000 | Loss: 0.00002714
Iteration 144/1000 | Loss: 0.00002714
Iteration 145/1000 | Loss: 0.00002713
Iteration 146/1000 | Loss: 0.00002712
Iteration 147/1000 | Loss: 0.00002711
Iteration 148/1000 | Loss: 0.00011709
Iteration 149/1000 | Loss: 0.00007775
Iteration 150/1000 | Loss: 0.00011235
Iteration 151/1000 | Loss: 0.00010226
Iteration 152/1000 | Loss: 0.00010615
Iteration 153/1000 | Loss: 0.00009206
Iteration 154/1000 | Loss: 0.00010666
Iteration 155/1000 | Loss: 0.00009056
Iteration 156/1000 | Loss: 0.00006420
Iteration 157/1000 | Loss: 0.00005065
Iteration 158/1000 | Loss: 0.00008517
Iteration 159/1000 | Loss: 0.00009352
Iteration 160/1000 | Loss: 0.00005184
Iteration 161/1000 | Loss: 0.00004153
Iteration 162/1000 | Loss: 0.00006112
Iteration 163/1000 | Loss: 0.00005451
Iteration 164/1000 | Loss: 0.00005876
Iteration 165/1000 | Loss: 0.00005582
Iteration 166/1000 | Loss: 0.00003447
Iteration 167/1000 | Loss: 0.00003636
Iteration 168/1000 | Loss: 0.00003377
Iteration 169/1000 | Loss: 0.00003769
Iteration 170/1000 | Loss: 0.00004996
Iteration 171/1000 | Loss: 0.00003590
Iteration 172/1000 | Loss: 0.00003784
Iteration 173/1000 | Loss: 0.00005075
Iteration 174/1000 | Loss: 0.00003788
Iteration 175/1000 | Loss: 0.00003581
Iteration 176/1000 | Loss: 0.00005164
Iteration 177/1000 | Loss: 0.00003884
Iteration 178/1000 | Loss: 0.00003643
Iteration 179/1000 | Loss: 0.00005041
Iteration 180/1000 | Loss: 0.00003085
Iteration 181/1000 | Loss: 0.00002862
Iteration 182/1000 | Loss: 0.00002753
Iteration 183/1000 | Loss: 0.00002707
Iteration 184/1000 | Loss: 0.00002682
Iteration 185/1000 | Loss: 0.00002670
Iteration 186/1000 | Loss: 0.00002670
Iteration 187/1000 | Loss: 0.00002668
Iteration 188/1000 | Loss: 0.00002667
Iteration 189/1000 | Loss: 0.00002651
Iteration 190/1000 | Loss: 0.00002647
Iteration 191/1000 | Loss: 0.00002645
Iteration 192/1000 | Loss: 0.00002643
Iteration 193/1000 | Loss: 0.00002641
Iteration 194/1000 | Loss: 0.00002640
Iteration 195/1000 | Loss: 0.00002640
Iteration 196/1000 | Loss: 0.00002639
Iteration 197/1000 | Loss: 0.00002639
Iteration 198/1000 | Loss: 0.00002639
Iteration 199/1000 | Loss: 0.00002639
Iteration 200/1000 | Loss: 0.00002638
Iteration 201/1000 | Loss: 0.00002638
Iteration 202/1000 | Loss: 0.00002638
Iteration 203/1000 | Loss: 0.00002638
Iteration 204/1000 | Loss: 0.00002637
Iteration 205/1000 | Loss: 0.00002637
Iteration 206/1000 | Loss: 0.00002637
Iteration 207/1000 | Loss: 0.00002637
Iteration 208/1000 | Loss: 0.00002637
Iteration 209/1000 | Loss: 0.00002636
Iteration 210/1000 | Loss: 0.00002635
Iteration 211/1000 | Loss: 0.00002635
Iteration 212/1000 | Loss: 0.00002635
Iteration 213/1000 | Loss: 0.00002635
Iteration 214/1000 | Loss: 0.00002634
Iteration 215/1000 | Loss: 0.00002634
Iteration 216/1000 | Loss: 0.00002634
Iteration 217/1000 | Loss: 0.00002634
Iteration 218/1000 | Loss: 0.00002634
Iteration 219/1000 | Loss: 0.00002634
Iteration 220/1000 | Loss: 0.00002634
Iteration 221/1000 | Loss: 0.00002633
Iteration 222/1000 | Loss: 0.00002633
Iteration 223/1000 | Loss: 0.00002633
Iteration 224/1000 | Loss: 0.00002633
Iteration 225/1000 | Loss: 0.00002633
Iteration 226/1000 | Loss: 0.00002633
Iteration 227/1000 | Loss: 0.00002633
Iteration 228/1000 | Loss: 0.00002633
Iteration 229/1000 | Loss: 0.00002632
Iteration 230/1000 | Loss: 0.00002632
Iteration 231/1000 | Loss: 0.00002632
Iteration 232/1000 | Loss: 0.00002632
Iteration 233/1000 | Loss: 0.00002632
Iteration 234/1000 | Loss: 0.00002632
Iteration 235/1000 | Loss: 0.00002632
Iteration 236/1000 | Loss: 0.00002632
Iteration 237/1000 | Loss: 0.00002632
Iteration 238/1000 | Loss: 0.00002632
Iteration 239/1000 | Loss: 0.00002632
Iteration 240/1000 | Loss: 0.00002632
Iteration 241/1000 | Loss: 0.00002632
Iteration 242/1000 | Loss: 0.00002632
Iteration 243/1000 | Loss: 0.00002632
Iteration 244/1000 | Loss: 0.00002632
Iteration 245/1000 | Loss: 0.00002632
Iteration 246/1000 | Loss: 0.00002632
Iteration 247/1000 | Loss: 0.00002631
Iteration 248/1000 | Loss: 0.00002631
Iteration 249/1000 | Loss: 0.00002631
Iteration 250/1000 | Loss: 0.00002631
Iteration 251/1000 | Loss: 0.00002631
Iteration 252/1000 | Loss: 0.00002631
Iteration 253/1000 | Loss: 0.00002631
Iteration 254/1000 | Loss: 0.00002631
Iteration 255/1000 | Loss: 0.00002631
Iteration 256/1000 | Loss: 0.00002631
Iteration 257/1000 | Loss: 0.00002631
Iteration 258/1000 | Loss: 0.00002631
Iteration 259/1000 | Loss: 0.00002631
Iteration 260/1000 | Loss: 0.00002631
Iteration 261/1000 | Loss: 0.00002631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [2.6314448405173607e-05, 2.6314448405173607e-05, 2.6314448405173607e-05, 2.6314448405173607e-05, 2.6314448405173607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6314448405173607e-05

Optimization complete. Final v2v error: 4.18616247177124 mm

Highest mean error: 6.4623236656188965 mm for frame 103

Lowest mean error: 3.285306215286255 mm for frame 74

Saving results

Total time: 296.3763313293457
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_claudia_posed_024/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_claudia_posed_024/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00499392
Iteration 2/25 | Loss: 0.00168239
Iteration 3/25 | Loss: 0.00149399
Iteration 4/25 | Loss: 0.00145237
Iteration 5/25 | Loss: 0.00145516
Iteration 6/25 | Loss: 0.00143328
Iteration 7/25 | Loss: 0.00142383
Iteration 8/25 | Loss: 0.00140630
Iteration 9/25 | Loss: 0.00140564
Iteration 10/25 | Loss: 0.00140463
Iteration 11/25 | Loss: 0.00139671
Iteration 12/25 | Loss: 0.00138759
Iteration 13/25 | Loss: 0.00138180
Iteration 14/25 | Loss: 0.00138050
Iteration 15/25 | Loss: 0.00138010
Iteration 16/25 | Loss: 0.00137997
Iteration 17/25 | Loss: 0.00137983
Iteration 18/25 | Loss: 0.00138412
Iteration 19/25 | Loss: 0.00137823
Iteration 20/25 | Loss: 0.00137728
Iteration 21/25 | Loss: 0.00137702
Iteration 22/25 | Loss: 0.00137686
Iteration 23/25 | Loss: 0.00137672
Iteration 24/25 | Loss: 0.00137655
Iteration 25/25 | Loss: 0.00137641

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42947721
Iteration 2/25 | Loss: 0.00307980
Iteration 3/25 | Loss: 0.00307980
Iteration 4/25 | Loss: 0.00307980
Iteration 5/25 | Loss: 0.00307980
Iteration 6/25 | Loss: 0.00307980
Iteration 7/25 | Loss: 0.00307980
Iteration 8/25 | Loss: 0.00307980
Iteration 9/25 | Loss: 0.00307980
Iteration 10/25 | Loss: 0.00307980
Iteration 11/25 | Loss: 0.00307980
Iteration 12/25 | Loss: 0.00307980
Iteration 13/25 | Loss: 0.00307980
Iteration 14/25 | Loss: 0.00307980
Iteration 15/25 | Loss: 0.00307980
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0030797997023910284, 0.0030797997023910284, 0.0030797997023910284, 0.0030797997023910284, 0.0030797997023910284]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030797997023910284

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00307980
Iteration 2/1000 | Loss: 0.00026218
Iteration 3/1000 | Loss: 0.00014993
Iteration 4/1000 | Loss: 0.00011695
Iteration 5/1000 | Loss: 0.00026888
Iteration 6/1000 | Loss: 0.00009880
Iteration 7/1000 | Loss: 0.00027603
Iteration 8/1000 | Loss: 0.00009244
Iteration 9/1000 | Loss: 0.00009873
Iteration 10/1000 | Loss: 0.00017121
Iteration 11/1000 | Loss: 0.00032069
Iteration 12/1000 | Loss: 0.00026627
Iteration 13/1000 | Loss: 0.00023116
Iteration 14/1000 | Loss: 0.00008647
Iteration 15/1000 | Loss: 0.00008075
Iteration 16/1000 | Loss: 0.00007958
Iteration 17/1000 | Loss: 0.00007706
Iteration 18/1000 | Loss: 0.00007559
Iteration 19/1000 | Loss: 0.00007423
Iteration 20/1000 | Loss: 0.00025752
Iteration 21/1000 | Loss: 0.00007603
Iteration 22/1000 | Loss: 0.00007169
Iteration 23/1000 | Loss: 0.00023307
Iteration 24/1000 | Loss: 0.00081874
Iteration 25/1000 | Loss: 0.00044421
Iteration 26/1000 | Loss: 0.00008864
Iteration 27/1000 | Loss: 0.00008040
Iteration 28/1000 | Loss: 0.00007264
Iteration 29/1000 | Loss: 0.00006989
Iteration 30/1000 | Loss: 0.00061863
Iteration 31/1000 | Loss: 0.00069113
Iteration 32/1000 | Loss: 0.00010172
Iteration 33/1000 | Loss: 0.00009031
Iteration 34/1000 | Loss: 0.00008243
Iteration 35/1000 | Loss: 0.00007692
Iteration 36/1000 | Loss: 0.00008415
Iteration 37/1000 | Loss: 0.00007226
Iteration 38/1000 | Loss: 0.00006995
Iteration 39/1000 | Loss: 0.00006852
Iteration 40/1000 | Loss: 0.00006715
Iteration 41/1000 | Loss: 0.00006628
Iteration 42/1000 | Loss: 0.00006557
Iteration 43/1000 | Loss: 0.00006477
Iteration 44/1000 | Loss: 0.00006408
Iteration 45/1000 | Loss: 0.00006324
Iteration 46/1000 | Loss: 0.00030019
Iteration 47/1000 | Loss: 0.00006829
Iteration 48/1000 | Loss: 0.00006558
Iteration 49/1000 | Loss: 0.00006473
Iteration 50/1000 | Loss: 0.00006363
Iteration 51/1000 | Loss: 0.00006258
Iteration 52/1000 | Loss: 0.00006183
Iteration 53/1000 | Loss: 0.00006105
Iteration 54/1000 | Loss: 0.00006076
Iteration 55/1000 | Loss: 0.00006045
Iteration 56/1000 | Loss: 0.00006018
Iteration 57/1000 | Loss: 0.00005998
Iteration 58/1000 | Loss: 0.00029598
Iteration 59/1000 | Loss: 0.00006581
Iteration 60/1000 | Loss: 0.00006284
Iteration 61/1000 | Loss: 0.00006216
Iteration 62/1000 | Loss: 0.00006132
Iteration 63/1000 | Loss: 0.00006033
Iteration 64/1000 | Loss: 0.00006118
Iteration 65/1000 | Loss: 0.00005938
Iteration 66/1000 | Loss: 0.00005909
Iteration 67/1000 | Loss: 0.00005893
Iteration 68/1000 | Loss: 0.00005874
Iteration 69/1000 | Loss: 0.00005872
Iteration 70/1000 | Loss: 0.00005853
Iteration 71/1000 | Loss: 0.00005850
Iteration 72/1000 | Loss: 0.00005850
Iteration 73/1000 | Loss: 0.00005848
Iteration 74/1000 | Loss: 0.00005848
Iteration 75/1000 | Loss: 0.00005847
Iteration 76/1000 | Loss: 0.00005845
Iteration 77/1000 | Loss: 0.00005844
Iteration 78/1000 | Loss: 0.00005844
Iteration 79/1000 | Loss: 0.00005839
Iteration 80/1000 | Loss: 0.00005839
Iteration 81/1000 | Loss: 0.00005837
Iteration 82/1000 | Loss: 0.00005836
Iteration 83/1000 | Loss: 0.00005834
Iteration 84/1000 | Loss: 0.00005834
Iteration 85/1000 | Loss: 0.00005833
Iteration 86/1000 | Loss: 0.00005832
Iteration 87/1000 | Loss: 0.00005829
Iteration 88/1000 | Loss: 0.00005823
Iteration 89/1000 | Loss: 0.00005822
Iteration 90/1000 | Loss: 0.00005822
Iteration 91/1000 | Loss: 0.00005821
Iteration 92/1000 | Loss: 0.00005821
Iteration 93/1000 | Loss: 0.00005820
Iteration 94/1000 | Loss: 0.00005820
Iteration 95/1000 | Loss: 0.00005819
Iteration 96/1000 | Loss: 0.00005819
Iteration 97/1000 | Loss: 0.00005818
Iteration 98/1000 | Loss: 0.00005818
Iteration 99/1000 | Loss: 0.00005817
Iteration 100/1000 | Loss: 0.00005814
Iteration 101/1000 | Loss: 0.00005814
Iteration 102/1000 | Loss: 0.00005813
Iteration 103/1000 | Loss: 0.00005812
Iteration 104/1000 | Loss: 0.00005812
Iteration 105/1000 | Loss: 0.00005812
Iteration 106/1000 | Loss: 0.00005812
Iteration 107/1000 | Loss: 0.00005811
Iteration 108/1000 | Loss: 0.00005809
Iteration 109/1000 | Loss: 0.00005808
Iteration 110/1000 | Loss: 0.00005806
Iteration 111/1000 | Loss: 0.00005806
Iteration 112/1000 | Loss: 0.00005806
Iteration 113/1000 | Loss: 0.00005805
Iteration 114/1000 | Loss: 0.00005805
Iteration 115/1000 | Loss: 0.00005805
Iteration 116/1000 | Loss: 0.00005804
Iteration 117/1000 | Loss: 0.00005803
Iteration 118/1000 | Loss: 0.00005803
Iteration 119/1000 | Loss: 0.00005800
Iteration 120/1000 | Loss: 0.00005800
Iteration 121/1000 | Loss: 0.00005798
Iteration 122/1000 | Loss: 0.00005798
Iteration 123/1000 | Loss: 0.00005797
Iteration 124/1000 | Loss: 0.00005797
Iteration 125/1000 | Loss: 0.00005795
Iteration 126/1000 | Loss: 0.00005795
Iteration 127/1000 | Loss: 0.00005794
Iteration 128/1000 | Loss: 0.00005794
Iteration 129/1000 | Loss: 0.00005793
Iteration 130/1000 | Loss: 0.00005793
Iteration 131/1000 | Loss: 0.00005793
Iteration 132/1000 | Loss: 0.00005793
Iteration 133/1000 | Loss: 0.00005793
Iteration 134/1000 | Loss: 0.00005793
Iteration 135/1000 | Loss: 0.00005792
Iteration 136/1000 | Loss: 0.00005792
Iteration 137/1000 | Loss: 0.00005792
Iteration 138/1000 | Loss: 0.00005791
Iteration 139/1000 | Loss: 0.00005791
Iteration 140/1000 | Loss: 0.00005791
Iteration 141/1000 | Loss: 0.00005791
Iteration 142/1000 | Loss: 0.00005790
Iteration 143/1000 | Loss: 0.00005790
Iteration 144/1000 | Loss: 0.00005790
Iteration 145/1000 | Loss: 0.00005790
Iteration 146/1000 | Loss: 0.00005789
Iteration 147/1000 | Loss: 0.00005789
Iteration 148/1000 | Loss: 0.00005789
Iteration 149/1000 | Loss: 0.00005788
Iteration 150/1000 | Loss: 0.00005788
Iteration 151/1000 | Loss: 0.00005788
Iteration 152/1000 | Loss: 0.00005788
Iteration 153/1000 | Loss: 0.00005788
Iteration 154/1000 | Loss: 0.00005788
Iteration 155/1000 | Loss: 0.00005788
Iteration 156/1000 | Loss: 0.00005788
Iteration 157/1000 | Loss: 0.00005788
Iteration 158/1000 | Loss: 0.00005788
Iteration 159/1000 | Loss: 0.00005787
Iteration 160/1000 | Loss: 0.00005787
Iteration 161/1000 | Loss: 0.00005787
Iteration 162/1000 | Loss: 0.00005787
Iteration 163/1000 | Loss: 0.00005787
Iteration 164/1000 | Loss: 0.00005787
Iteration 165/1000 | Loss: 0.00005787
Iteration 166/1000 | Loss: 0.00005787
Iteration 167/1000 | Loss: 0.00005787
Iteration 168/1000 | Loss: 0.00005787
Iteration 169/1000 | Loss: 0.00005787
Iteration 170/1000 | Loss: 0.00005787
Iteration 171/1000 | Loss: 0.00005786
Iteration 172/1000 | Loss: 0.00005786
Iteration 173/1000 | Loss: 0.00005786
Iteration 174/1000 | Loss: 0.00005786
Iteration 175/1000 | Loss: 0.00005786
Iteration 176/1000 | Loss: 0.00005786
Iteration 177/1000 | Loss: 0.00005786
Iteration 178/1000 | Loss: 0.00005786
Iteration 179/1000 | Loss: 0.00005786
Iteration 180/1000 | Loss: 0.00005786
Iteration 181/1000 | Loss: 0.00005786
Iteration 182/1000 | Loss: 0.00005786
Iteration 183/1000 | Loss: 0.00005786
Iteration 184/1000 | Loss: 0.00005786
Iteration 185/1000 | Loss: 0.00005786
Iteration 186/1000 | Loss: 0.00005786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [5.785762186860666e-05, 5.785762186860666e-05, 5.785762186860666e-05, 5.785762186860666e-05, 5.785762186860666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.785762186860666e-05

Optimization complete. Final v2v error: 4.59566068649292 mm

Highest mean error: 11.538966178894043 mm for frame 108

Lowest mean error: 2.9104788303375244 mm for frame 77

Saving results

Total time: 159.77350449562073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00875283
Iteration 2/25 | Loss: 0.00134775
Iteration 3/25 | Loss: 0.00099026
Iteration 4/25 | Loss: 0.00093194
Iteration 5/25 | Loss: 0.00091756
Iteration 6/25 | Loss: 0.00091295
Iteration 7/25 | Loss: 0.00091131
Iteration 8/25 | Loss: 0.00091601
Iteration 9/25 | Loss: 0.00090877
Iteration 10/25 | Loss: 0.00090878
Iteration 11/25 | Loss: 0.00089909
Iteration 12/25 | Loss: 0.00089712
Iteration 13/25 | Loss: 0.00090016
Iteration 14/25 | Loss: 0.00089916
Iteration 15/25 | Loss: 0.00089763
Iteration 16/25 | Loss: 0.00089581
Iteration 17/25 | Loss: 0.00089495
Iteration 18/25 | Loss: 0.00089326
Iteration 19/25 | Loss: 0.00089329
Iteration 20/25 | Loss: 0.00089325
Iteration 21/25 | Loss: 0.00089250
Iteration 22/25 | Loss: 0.00089291
Iteration 23/25 | Loss: 0.00089265
Iteration 24/25 | Loss: 0.00089322
Iteration 25/25 | Loss: 0.00089275

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29519475
Iteration 2/25 | Loss: 0.00066905
Iteration 3/25 | Loss: 0.00066905
Iteration 4/25 | Loss: 0.00066905
Iteration 5/25 | Loss: 0.00066905
Iteration 6/25 | Loss: 0.00066905
Iteration 7/25 | Loss: 0.00066905
Iteration 8/25 | Loss: 0.00066905
Iteration 9/25 | Loss: 0.00066905
Iteration 10/25 | Loss: 0.00066905
Iteration 11/25 | Loss: 0.00066905
Iteration 12/25 | Loss: 0.00066905
Iteration 13/25 | Loss: 0.00066905
Iteration 14/25 | Loss: 0.00066905
Iteration 15/25 | Loss: 0.00066905
Iteration 16/25 | Loss: 0.00066905
Iteration 17/25 | Loss: 0.00066905
Iteration 18/25 | Loss: 0.00066905
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000669052533339709, 0.000669052533339709, 0.000669052533339709, 0.000669052533339709, 0.000669052533339709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000669052533339709

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066905
Iteration 2/1000 | Loss: 0.00003493
Iteration 3/1000 | Loss: 0.00003744
Iteration 4/1000 | Loss: 0.00002498
Iteration 5/1000 | Loss: 0.00004187
Iteration 6/1000 | Loss: 0.00002044
Iteration 7/1000 | Loss: 0.00003767
Iteration 8/1000 | Loss: 0.00003574
Iteration 9/1000 | Loss: 0.00002642
Iteration 10/1000 | Loss: 0.00004282
Iteration 11/1000 | Loss: 0.00003711
Iteration 12/1000 | Loss: 0.00003681
Iteration 13/1000 | Loss: 0.00003679
Iteration 14/1000 | Loss: 0.00002556
Iteration 15/1000 | Loss: 0.00004735
Iteration 16/1000 | Loss: 0.00003020
Iteration 17/1000 | Loss: 0.00003380
Iteration 18/1000 | Loss: 0.00003788
Iteration 19/1000 | Loss: 0.00003431
Iteration 20/1000 | Loss: 0.00003021
Iteration 21/1000 | Loss: 0.00003321
Iteration 22/1000 | Loss: 0.00005404
Iteration 23/1000 | Loss: 0.00002039
Iteration 24/1000 | Loss: 0.00003175
Iteration 25/1000 | Loss: 0.00003485
Iteration 26/1000 | Loss: 0.00003821
Iteration 27/1000 | Loss: 0.00003511
Iteration 28/1000 | Loss: 0.00004016
Iteration 29/1000 | Loss: 0.00003968
Iteration 30/1000 | Loss: 0.00002063
Iteration 31/1000 | Loss: 0.00001782
Iteration 32/1000 | Loss: 0.00001706
Iteration 33/1000 | Loss: 0.00001657
Iteration 34/1000 | Loss: 0.00001650
Iteration 35/1000 | Loss: 0.00001649
Iteration 36/1000 | Loss: 0.00001644
Iteration 37/1000 | Loss: 0.00001642
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001638
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001633
Iteration 43/1000 | Loss: 0.00001633
Iteration 44/1000 | Loss: 0.00001632
Iteration 45/1000 | Loss: 0.00001631
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001630
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001627
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001626
Iteration 55/1000 | Loss: 0.00001625
Iteration 56/1000 | Loss: 0.00001625
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001621
Iteration 60/1000 | Loss: 0.00001621
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001620
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001619
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001618
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001618
Iteration 72/1000 | Loss: 0.00001618
Iteration 73/1000 | Loss: 0.00001618
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001617
Iteration 76/1000 | Loss: 0.00001617
Iteration 77/1000 | Loss: 0.00001617
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001617
Iteration 80/1000 | Loss: 0.00001617
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001615
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001615
Iteration 90/1000 | Loss: 0.00001615
Iteration 91/1000 | Loss: 0.00001615
Iteration 92/1000 | Loss: 0.00001615
Iteration 93/1000 | Loss: 0.00001615
Iteration 94/1000 | Loss: 0.00001615
Iteration 95/1000 | Loss: 0.00001615
Iteration 96/1000 | Loss: 0.00001615
Iteration 97/1000 | Loss: 0.00001615
Iteration 98/1000 | Loss: 0.00001615
Iteration 99/1000 | Loss: 0.00001615
Iteration 100/1000 | Loss: 0.00001614
Iteration 101/1000 | Loss: 0.00001614
Iteration 102/1000 | Loss: 0.00001614
Iteration 103/1000 | Loss: 0.00001614
Iteration 104/1000 | Loss: 0.00001614
Iteration 105/1000 | Loss: 0.00001614
Iteration 106/1000 | Loss: 0.00001614
Iteration 107/1000 | Loss: 0.00001614
Iteration 108/1000 | Loss: 0.00001614
Iteration 109/1000 | Loss: 0.00001614
Iteration 110/1000 | Loss: 0.00001614
Iteration 111/1000 | Loss: 0.00001614
Iteration 112/1000 | Loss: 0.00001614
Iteration 113/1000 | Loss: 0.00001614
Iteration 114/1000 | Loss: 0.00001614
Iteration 115/1000 | Loss: 0.00001614
Iteration 116/1000 | Loss: 0.00001614
Iteration 117/1000 | Loss: 0.00001614
Iteration 118/1000 | Loss: 0.00001614
Iteration 119/1000 | Loss: 0.00001614
Iteration 120/1000 | Loss: 0.00001614
Iteration 121/1000 | Loss: 0.00001614
Iteration 122/1000 | Loss: 0.00001614
Iteration 123/1000 | Loss: 0.00001614
Iteration 124/1000 | Loss: 0.00001614
Iteration 125/1000 | Loss: 0.00001614
Iteration 126/1000 | Loss: 0.00001614
Iteration 127/1000 | Loss: 0.00001614
Iteration 128/1000 | Loss: 0.00001614
Iteration 129/1000 | Loss: 0.00001614
Iteration 130/1000 | Loss: 0.00001614
Iteration 131/1000 | Loss: 0.00001614
Iteration 132/1000 | Loss: 0.00001614
Iteration 133/1000 | Loss: 0.00001614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.614014217921067e-05, 1.614014217921067e-05, 1.614014217921067e-05, 1.614014217921067e-05, 1.614014217921067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.614014217921067e-05

Optimization complete. Final v2v error: 3.466332197189331 mm

Highest mean error: 5.08384895324707 mm for frame 66

Lowest mean error: 3.0479729175567627 mm for frame 191

Saving results

Total time: 104.62137722969055
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01100897
Iteration 2/25 | Loss: 0.00277247
Iteration 3/25 | Loss: 0.00210964
Iteration 4/25 | Loss: 0.00186918
Iteration 5/25 | Loss: 0.00168810
Iteration 6/25 | Loss: 0.00153973
Iteration 7/25 | Loss: 0.00138373
Iteration 8/25 | Loss: 0.00125470
Iteration 9/25 | Loss: 0.00120791
Iteration 10/25 | Loss: 0.00117047
Iteration 11/25 | Loss: 0.00116348
Iteration 12/25 | Loss: 0.00115471
Iteration 13/25 | Loss: 0.00114216
Iteration 14/25 | Loss: 0.00113970
Iteration 15/25 | Loss: 0.00113309
Iteration 16/25 | Loss: 0.00113110
Iteration 17/25 | Loss: 0.00112540
Iteration 18/25 | Loss: 0.00112327
Iteration 19/25 | Loss: 0.00112145
Iteration 20/25 | Loss: 0.00112113
Iteration 21/25 | Loss: 0.00112128
Iteration 22/25 | Loss: 0.00112102
Iteration 23/25 | Loss: 0.00112138
Iteration 24/25 | Loss: 0.00112103
Iteration 25/25 | Loss: 0.00112148

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41768742
Iteration 2/25 | Loss: 0.00339198
Iteration 3/25 | Loss: 0.00223662
Iteration 4/25 | Loss: 0.00223643
Iteration 5/25 | Loss: 0.00223642
Iteration 6/25 | Loss: 0.00223642
Iteration 7/25 | Loss: 0.00223642
Iteration 8/25 | Loss: 0.00223642
Iteration 9/25 | Loss: 0.00223642
Iteration 10/25 | Loss: 0.00223642
Iteration 11/25 | Loss: 0.00223642
Iteration 12/25 | Loss: 0.00223642
Iteration 13/25 | Loss: 0.00223642
Iteration 14/25 | Loss: 0.00223642
Iteration 15/25 | Loss: 0.00223642
Iteration 16/25 | Loss: 0.00223642
Iteration 17/25 | Loss: 0.00223642
Iteration 18/25 | Loss: 0.00223642
Iteration 19/25 | Loss: 0.00223642
Iteration 20/25 | Loss: 0.00223642
Iteration 21/25 | Loss: 0.00223642
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0022364214528352022, 0.0022364214528352022, 0.0022364214528352022, 0.0022364214528352022, 0.0022364214528352022]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022364214528352022

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223642
Iteration 2/1000 | Loss: 0.00096227
Iteration 3/1000 | Loss: 0.00078356
Iteration 4/1000 | Loss: 0.00044671
Iteration 5/1000 | Loss: 0.00027115
Iteration 6/1000 | Loss: 0.00022734
Iteration 7/1000 | Loss: 0.00096754
Iteration 8/1000 | Loss: 0.00038891
Iteration 9/1000 | Loss: 0.00014335
Iteration 10/1000 | Loss: 0.00051200
Iteration 11/1000 | Loss: 0.00067526
Iteration 12/1000 | Loss: 0.00014963
Iteration 13/1000 | Loss: 0.00012118
Iteration 14/1000 | Loss: 0.00011576
Iteration 15/1000 | Loss: 0.00044235
Iteration 16/1000 | Loss: 0.00053681
Iteration 17/1000 | Loss: 0.00039607
Iteration 18/1000 | Loss: 0.00017612
Iteration 19/1000 | Loss: 0.00010782
Iteration 20/1000 | Loss: 0.00041026
Iteration 21/1000 | Loss: 0.00051845
Iteration 22/1000 | Loss: 0.00059613
Iteration 23/1000 | Loss: 0.00047464
Iteration 24/1000 | Loss: 0.00025309
Iteration 25/1000 | Loss: 0.00065374
Iteration 26/1000 | Loss: 0.00124589
Iteration 27/1000 | Loss: 0.00022979
Iteration 28/1000 | Loss: 0.00012691
Iteration 29/1000 | Loss: 0.00025807
Iteration 30/1000 | Loss: 0.00009871
Iteration 31/1000 | Loss: 0.00008844
Iteration 32/1000 | Loss: 0.00018795
Iteration 33/1000 | Loss: 0.00008516
Iteration 34/1000 | Loss: 0.00008066
Iteration 35/1000 | Loss: 0.00022128
Iteration 36/1000 | Loss: 0.00029925
Iteration 37/1000 | Loss: 0.00042211
Iteration 38/1000 | Loss: 0.00027417
Iteration 39/1000 | Loss: 0.00007905
Iteration 40/1000 | Loss: 0.00017668
Iteration 41/1000 | Loss: 0.00010999
Iteration 42/1000 | Loss: 0.00009076
Iteration 43/1000 | Loss: 0.00007435
Iteration 44/1000 | Loss: 0.00007235
Iteration 45/1000 | Loss: 0.00007138
Iteration 46/1000 | Loss: 0.00007085
Iteration 47/1000 | Loss: 0.00009182
Iteration 48/1000 | Loss: 0.00008558
Iteration 49/1000 | Loss: 0.00009088
Iteration 50/1000 | Loss: 0.00007712
Iteration 51/1000 | Loss: 0.00007435
Iteration 52/1000 | Loss: 0.00007258
Iteration 53/1000 | Loss: 0.00027437
Iteration 54/1000 | Loss: 0.00007119
Iteration 55/1000 | Loss: 0.00007049
Iteration 56/1000 | Loss: 0.00007022
Iteration 57/1000 | Loss: 0.00006998
Iteration 58/1000 | Loss: 0.00006976
Iteration 59/1000 | Loss: 0.00006973
Iteration 60/1000 | Loss: 0.00006969
Iteration 61/1000 | Loss: 0.00006964
Iteration 62/1000 | Loss: 0.00006964
Iteration 63/1000 | Loss: 0.00006964
Iteration 64/1000 | Loss: 0.00006964
Iteration 65/1000 | Loss: 0.00006964
Iteration 66/1000 | Loss: 0.00006964
Iteration 67/1000 | Loss: 0.00006964
Iteration 68/1000 | Loss: 0.00006961
Iteration 69/1000 | Loss: 0.00006959
Iteration 70/1000 | Loss: 0.00006959
Iteration 71/1000 | Loss: 0.00006959
Iteration 72/1000 | Loss: 0.00006959
Iteration 73/1000 | Loss: 0.00006959
Iteration 74/1000 | Loss: 0.00006959
Iteration 75/1000 | Loss: 0.00006959
Iteration 76/1000 | Loss: 0.00006959
Iteration 77/1000 | Loss: 0.00006958
Iteration 78/1000 | Loss: 0.00006958
Iteration 79/1000 | Loss: 0.00006958
Iteration 80/1000 | Loss: 0.00006958
Iteration 81/1000 | Loss: 0.00006958
Iteration 82/1000 | Loss: 0.00006958
Iteration 83/1000 | Loss: 0.00006957
Iteration 84/1000 | Loss: 0.00006957
Iteration 85/1000 | Loss: 0.00006957
Iteration 86/1000 | Loss: 0.00006957
Iteration 87/1000 | Loss: 0.00006957
Iteration 88/1000 | Loss: 0.00006957
Iteration 89/1000 | Loss: 0.00006957
Iteration 90/1000 | Loss: 0.00006957
Iteration 91/1000 | Loss: 0.00006957
Iteration 92/1000 | Loss: 0.00006957
Iteration 93/1000 | Loss: 0.00006957
Iteration 94/1000 | Loss: 0.00006956
Iteration 95/1000 | Loss: 0.00006956
Iteration 96/1000 | Loss: 0.00006956
Iteration 97/1000 | Loss: 0.00006956
Iteration 98/1000 | Loss: 0.00006956
Iteration 99/1000 | Loss: 0.00006956
Iteration 100/1000 | Loss: 0.00006956
Iteration 101/1000 | Loss: 0.00006956
Iteration 102/1000 | Loss: 0.00006956
Iteration 103/1000 | Loss: 0.00006956
Iteration 104/1000 | Loss: 0.00006956
Iteration 105/1000 | Loss: 0.00006956
Iteration 106/1000 | Loss: 0.00006956
Iteration 107/1000 | Loss: 0.00006956
Iteration 108/1000 | Loss: 0.00006955
Iteration 109/1000 | Loss: 0.00006955
Iteration 110/1000 | Loss: 0.00006955
Iteration 111/1000 | Loss: 0.00006955
Iteration 112/1000 | Loss: 0.00006955
Iteration 113/1000 | Loss: 0.00006955
Iteration 114/1000 | Loss: 0.00006955
Iteration 115/1000 | Loss: 0.00006955
Iteration 116/1000 | Loss: 0.00006955
Iteration 117/1000 | Loss: 0.00006955
Iteration 118/1000 | Loss: 0.00006955
Iteration 119/1000 | Loss: 0.00006955
Iteration 120/1000 | Loss: 0.00006955
Iteration 121/1000 | Loss: 0.00006955
Iteration 122/1000 | Loss: 0.00006955
Iteration 123/1000 | Loss: 0.00006955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [6.954974378459156e-05, 6.954974378459156e-05, 6.954974378459156e-05, 6.954974378459156e-05, 6.954974378459156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.954974378459156e-05

Optimization complete. Final v2v error: 5.178760051727295 mm

Highest mean error: 13.76183032989502 mm for frame 24

Lowest mean error: 3.920581579208374 mm for frame 187

Saving results

Total time: 137.0076847076416
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373931
Iteration 2/25 | Loss: 0.00099093
Iteration 3/25 | Loss: 0.00090511
Iteration 4/25 | Loss: 0.00089319
Iteration 5/25 | Loss: 0.00088753
Iteration 6/25 | Loss: 0.00088599
Iteration 7/25 | Loss: 0.00088574
Iteration 8/25 | Loss: 0.00088574
Iteration 9/25 | Loss: 0.00088574
Iteration 10/25 | Loss: 0.00088574
Iteration 11/25 | Loss: 0.00088574
Iteration 12/25 | Loss: 0.00088574
Iteration 13/25 | Loss: 0.00088574
Iteration 14/25 | Loss: 0.00088574
Iteration 15/25 | Loss: 0.00088574
Iteration 16/25 | Loss: 0.00088574
Iteration 17/25 | Loss: 0.00088574
Iteration 18/25 | Loss: 0.00088574
Iteration 19/25 | Loss: 0.00088574
Iteration 20/25 | Loss: 0.00088574
Iteration 21/25 | Loss: 0.00088574
Iteration 22/25 | Loss: 0.00088574
Iteration 23/25 | Loss: 0.00088574
Iteration 24/25 | Loss: 0.00088574
Iteration 25/25 | Loss: 0.00088574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31298745
Iteration 2/25 | Loss: 0.00073721
Iteration 3/25 | Loss: 0.00073721
Iteration 4/25 | Loss: 0.00073721
Iteration 5/25 | Loss: 0.00073721
Iteration 6/25 | Loss: 0.00073721
Iteration 7/25 | Loss: 0.00073721
Iteration 8/25 | Loss: 0.00073721
Iteration 9/25 | Loss: 0.00073721
Iteration 10/25 | Loss: 0.00073721
Iteration 11/25 | Loss: 0.00073721
Iteration 12/25 | Loss: 0.00073721
Iteration 13/25 | Loss: 0.00073721
Iteration 14/25 | Loss: 0.00073721
Iteration 15/25 | Loss: 0.00073721
Iteration 16/25 | Loss: 0.00073721
Iteration 17/25 | Loss: 0.00073721
Iteration 18/25 | Loss: 0.00073721
Iteration 19/25 | Loss: 0.00073721
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007372088148258626, 0.0007372088148258626, 0.0007372088148258626, 0.0007372088148258626, 0.0007372088148258626]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007372088148258626

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073721
Iteration 2/1000 | Loss: 0.00002564
Iteration 3/1000 | Loss: 0.00001842
Iteration 4/1000 | Loss: 0.00001651
Iteration 5/1000 | Loss: 0.00001562
Iteration 6/1000 | Loss: 0.00001514
Iteration 7/1000 | Loss: 0.00001498
Iteration 8/1000 | Loss: 0.00001489
Iteration 9/1000 | Loss: 0.00001487
Iteration 10/1000 | Loss: 0.00001486
Iteration 11/1000 | Loss: 0.00001480
Iteration 12/1000 | Loss: 0.00001479
Iteration 13/1000 | Loss: 0.00001478
Iteration 14/1000 | Loss: 0.00001478
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001477
Iteration 17/1000 | Loss: 0.00001477
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001476
Iteration 20/1000 | Loss: 0.00001476
Iteration 21/1000 | Loss: 0.00001476
Iteration 22/1000 | Loss: 0.00001476
Iteration 23/1000 | Loss: 0.00001476
Iteration 24/1000 | Loss: 0.00001476
Iteration 25/1000 | Loss: 0.00001476
Iteration 26/1000 | Loss: 0.00001475
Iteration 27/1000 | Loss: 0.00001474
Iteration 28/1000 | Loss: 0.00001474
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001471
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001470
Iteration 37/1000 | Loss: 0.00001469
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001468
Iteration 40/1000 | Loss: 0.00001466
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001466
Iteration 43/1000 | Loss: 0.00001465
Iteration 44/1000 | Loss: 0.00001465
Iteration 45/1000 | Loss: 0.00001465
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001465
Iteration 51/1000 | Loss: 0.00001465
Iteration 52/1000 | Loss: 0.00001465
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001465
Iteration 55/1000 | Loss: 0.00001464
Iteration 56/1000 | Loss: 0.00001464
Iteration 57/1000 | Loss: 0.00001464
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001462
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001459
Iteration 78/1000 | Loss: 0.00001459
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00001457
Iteration 94/1000 | Loss: 0.00001457
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001457
Iteration 97/1000 | Loss: 0.00001457
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001455
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001455
Iteration 123/1000 | Loss: 0.00001455
Iteration 124/1000 | Loss: 0.00001454
Iteration 125/1000 | Loss: 0.00001454
Iteration 126/1000 | Loss: 0.00001454
Iteration 127/1000 | Loss: 0.00001454
Iteration 128/1000 | Loss: 0.00001454
Iteration 129/1000 | Loss: 0.00001454
Iteration 130/1000 | Loss: 0.00001454
Iteration 131/1000 | Loss: 0.00001454
Iteration 132/1000 | Loss: 0.00001454
Iteration 133/1000 | Loss: 0.00001454
Iteration 134/1000 | Loss: 0.00001454
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001454
Iteration 138/1000 | Loss: 0.00001454
Iteration 139/1000 | Loss: 0.00001454
Iteration 140/1000 | Loss: 0.00001454
Iteration 141/1000 | Loss: 0.00001454
Iteration 142/1000 | Loss: 0.00001454
Iteration 143/1000 | Loss: 0.00001454
Iteration 144/1000 | Loss: 0.00001454
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.453996173950145e-05, 1.453996173950145e-05, 1.453996173950145e-05, 1.453996173950145e-05, 1.453996173950145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.453996173950145e-05

Optimization complete. Final v2v error: 3.2968523502349854 mm

Highest mean error: 3.5773110389709473 mm for frame 141

Lowest mean error: 3.0600903034210205 mm for frame 0

Saving results

Total time: 29.679680585861206
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00663352
Iteration 2/25 | Loss: 0.00102451
Iteration 3/25 | Loss: 0.00090939
Iteration 4/25 | Loss: 0.00089766
Iteration 5/25 | Loss: 0.00089109
Iteration 6/25 | Loss: 0.00088944
Iteration 7/25 | Loss: 0.00088927
Iteration 8/25 | Loss: 0.00088927
Iteration 9/25 | Loss: 0.00088927
Iteration 10/25 | Loss: 0.00088927
Iteration 11/25 | Loss: 0.00088927
Iteration 12/25 | Loss: 0.00088927
Iteration 13/25 | Loss: 0.00088927
Iteration 14/25 | Loss: 0.00088927
Iteration 15/25 | Loss: 0.00088927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008892730693332851, 0.0008892730693332851, 0.0008892730693332851, 0.0008892730693332851, 0.0008892730693332851]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008892730693332851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33215964
Iteration 2/25 | Loss: 0.00068868
Iteration 3/25 | Loss: 0.00068868
Iteration 4/25 | Loss: 0.00068868
Iteration 5/25 | Loss: 0.00068868
Iteration 6/25 | Loss: 0.00068868
Iteration 7/25 | Loss: 0.00068868
Iteration 8/25 | Loss: 0.00068868
Iteration 9/25 | Loss: 0.00068868
Iteration 10/25 | Loss: 0.00068868
Iteration 11/25 | Loss: 0.00068868
Iteration 12/25 | Loss: 0.00068868
Iteration 13/25 | Loss: 0.00068868
Iteration 14/25 | Loss: 0.00068868
Iteration 15/25 | Loss: 0.00068868
Iteration 16/25 | Loss: 0.00068868
Iteration 17/25 | Loss: 0.00068868
Iteration 18/25 | Loss: 0.00068868
Iteration 19/25 | Loss: 0.00068868
Iteration 20/25 | Loss: 0.00068868
Iteration 21/25 | Loss: 0.00068868
Iteration 22/25 | Loss: 0.00068868
Iteration 23/25 | Loss: 0.00068868
Iteration 24/25 | Loss: 0.00068868
Iteration 25/25 | Loss: 0.00068868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068868
Iteration 2/1000 | Loss: 0.00002399
Iteration 3/1000 | Loss: 0.00001807
Iteration 4/1000 | Loss: 0.00001597
Iteration 5/1000 | Loss: 0.00001522
Iteration 6/1000 | Loss: 0.00001478
Iteration 7/1000 | Loss: 0.00001448
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001427
Iteration 10/1000 | Loss: 0.00001426
Iteration 11/1000 | Loss: 0.00001426
Iteration 12/1000 | Loss: 0.00001425
Iteration 13/1000 | Loss: 0.00001424
Iteration 14/1000 | Loss: 0.00001422
Iteration 15/1000 | Loss: 0.00001421
Iteration 16/1000 | Loss: 0.00001421
Iteration 17/1000 | Loss: 0.00001419
Iteration 18/1000 | Loss: 0.00001417
Iteration 19/1000 | Loss: 0.00001417
Iteration 20/1000 | Loss: 0.00001416
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001416
Iteration 23/1000 | Loss: 0.00001416
Iteration 24/1000 | Loss: 0.00001416
Iteration 25/1000 | Loss: 0.00001416
Iteration 26/1000 | Loss: 0.00001416
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001416
Iteration 30/1000 | Loss: 0.00001415
Iteration 31/1000 | Loss: 0.00001413
Iteration 32/1000 | Loss: 0.00001412
Iteration 33/1000 | Loss: 0.00001411
Iteration 34/1000 | Loss: 0.00001411
Iteration 35/1000 | Loss: 0.00001410
Iteration 36/1000 | Loss: 0.00001410
Iteration 37/1000 | Loss: 0.00001409
Iteration 38/1000 | Loss: 0.00001409
Iteration 39/1000 | Loss: 0.00001409
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001408
Iteration 42/1000 | Loss: 0.00001408
Iteration 43/1000 | Loss: 0.00001408
Iteration 44/1000 | Loss: 0.00001407
Iteration 45/1000 | Loss: 0.00001405
Iteration 46/1000 | Loss: 0.00001405
Iteration 47/1000 | Loss: 0.00001405
Iteration 48/1000 | Loss: 0.00001405
Iteration 49/1000 | Loss: 0.00001405
Iteration 50/1000 | Loss: 0.00001405
Iteration 51/1000 | Loss: 0.00001405
Iteration 52/1000 | Loss: 0.00001405
Iteration 53/1000 | Loss: 0.00001404
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001403
Iteration 56/1000 | Loss: 0.00001403
Iteration 57/1000 | Loss: 0.00001403
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001402
Iteration 64/1000 | Loss: 0.00001402
Iteration 65/1000 | Loss: 0.00001402
Iteration 66/1000 | Loss: 0.00001402
Iteration 67/1000 | Loss: 0.00001402
Iteration 68/1000 | Loss: 0.00001402
Iteration 69/1000 | Loss: 0.00001402
Iteration 70/1000 | Loss: 0.00001402
Iteration 71/1000 | Loss: 0.00001402
Iteration 72/1000 | Loss: 0.00001401
Iteration 73/1000 | Loss: 0.00001401
Iteration 74/1000 | Loss: 0.00001401
Iteration 75/1000 | Loss: 0.00001401
Iteration 76/1000 | Loss: 0.00001401
Iteration 77/1000 | Loss: 0.00001401
Iteration 78/1000 | Loss: 0.00001401
Iteration 79/1000 | Loss: 0.00001401
Iteration 80/1000 | Loss: 0.00001401
Iteration 81/1000 | Loss: 0.00001401
Iteration 82/1000 | Loss: 0.00001400
Iteration 83/1000 | Loss: 0.00001400
Iteration 84/1000 | Loss: 0.00001400
Iteration 85/1000 | Loss: 0.00001400
Iteration 86/1000 | Loss: 0.00001399
Iteration 87/1000 | Loss: 0.00001399
Iteration 88/1000 | Loss: 0.00001399
Iteration 89/1000 | Loss: 0.00001399
Iteration 90/1000 | Loss: 0.00001399
Iteration 91/1000 | Loss: 0.00001399
Iteration 92/1000 | Loss: 0.00001399
Iteration 93/1000 | Loss: 0.00001399
Iteration 94/1000 | Loss: 0.00001398
Iteration 95/1000 | Loss: 0.00001398
Iteration 96/1000 | Loss: 0.00001398
Iteration 97/1000 | Loss: 0.00001398
Iteration 98/1000 | Loss: 0.00001398
Iteration 99/1000 | Loss: 0.00001398
Iteration 100/1000 | Loss: 0.00001398
Iteration 101/1000 | Loss: 0.00001398
Iteration 102/1000 | Loss: 0.00001398
Iteration 103/1000 | Loss: 0.00001398
Iteration 104/1000 | Loss: 0.00001398
Iteration 105/1000 | Loss: 0.00001398
Iteration 106/1000 | Loss: 0.00001398
Iteration 107/1000 | Loss: 0.00001398
Iteration 108/1000 | Loss: 0.00001398
Iteration 109/1000 | Loss: 0.00001398
Iteration 110/1000 | Loss: 0.00001398
Iteration 111/1000 | Loss: 0.00001397
Iteration 112/1000 | Loss: 0.00001397
Iteration 113/1000 | Loss: 0.00001397
Iteration 114/1000 | Loss: 0.00001397
Iteration 115/1000 | Loss: 0.00001397
Iteration 116/1000 | Loss: 0.00001397
Iteration 117/1000 | Loss: 0.00001397
Iteration 118/1000 | Loss: 0.00001397
Iteration 119/1000 | Loss: 0.00001397
Iteration 120/1000 | Loss: 0.00001397
Iteration 121/1000 | Loss: 0.00001397
Iteration 122/1000 | Loss: 0.00001397
Iteration 123/1000 | Loss: 0.00001396
Iteration 124/1000 | Loss: 0.00001396
Iteration 125/1000 | Loss: 0.00001396
Iteration 126/1000 | Loss: 0.00001396
Iteration 127/1000 | Loss: 0.00001396
Iteration 128/1000 | Loss: 0.00001396
Iteration 129/1000 | Loss: 0.00001396
Iteration 130/1000 | Loss: 0.00001396
Iteration 131/1000 | Loss: 0.00001396
Iteration 132/1000 | Loss: 0.00001396
Iteration 133/1000 | Loss: 0.00001396
Iteration 134/1000 | Loss: 0.00001396
Iteration 135/1000 | Loss: 0.00001396
Iteration 136/1000 | Loss: 0.00001396
Iteration 137/1000 | Loss: 0.00001395
Iteration 138/1000 | Loss: 0.00001395
Iteration 139/1000 | Loss: 0.00001395
Iteration 140/1000 | Loss: 0.00001395
Iteration 141/1000 | Loss: 0.00001395
Iteration 142/1000 | Loss: 0.00001395
Iteration 143/1000 | Loss: 0.00001395
Iteration 144/1000 | Loss: 0.00001395
Iteration 145/1000 | Loss: 0.00001395
Iteration 146/1000 | Loss: 0.00001395
Iteration 147/1000 | Loss: 0.00001395
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001395
Iteration 151/1000 | Loss: 0.00001395
Iteration 152/1000 | Loss: 0.00001395
Iteration 153/1000 | Loss: 0.00001395
Iteration 154/1000 | Loss: 0.00001395
Iteration 155/1000 | Loss: 0.00001395
Iteration 156/1000 | Loss: 0.00001395
Iteration 157/1000 | Loss: 0.00001395
Iteration 158/1000 | Loss: 0.00001395
Iteration 159/1000 | Loss: 0.00001395
Iteration 160/1000 | Loss: 0.00001395
Iteration 161/1000 | Loss: 0.00001395
Iteration 162/1000 | Loss: 0.00001395
Iteration 163/1000 | Loss: 0.00001395
Iteration 164/1000 | Loss: 0.00001395
Iteration 165/1000 | Loss: 0.00001395
Iteration 166/1000 | Loss: 0.00001395
Iteration 167/1000 | Loss: 0.00001395
Iteration 168/1000 | Loss: 0.00001395
Iteration 169/1000 | Loss: 0.00001395
Iteration 170/1000 | Loss: 0.00001395
Iteration 171/1000 | Loss: 0.00001395
Iteration 172/1000 | Loss: 0.00001395
Iteration 173/1000 | Loss: 0.00001395
Iteration 174/1000 | Loss: 0.00001395
Iteration 175/1000 | Loss: 0.00001395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.3950200809631497e-05, 1.3950200809631497e-05, 1.3950200809631497e-05, 1.3950200809631497e-05, 1.3950200809631497e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3950200809631497e-05

Optimization complete. Final v2v error: 3.194875717163086 mm

Highest mean error: 3.5332980155944824 mm for frame 31

Lowest mean error: 2.812899589538574 mm for frame 113

Saving results

Total time: 30.690213441848755
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460260
Iteration 2/25 | Loss: 0.00111229
Iteration 3/25 | Loss: 0.00100745
Iteration 4/25 | Loss: 0.00099243
Iteration 5/25 | Loss: 0.00098810
Iteration 6/25 | Loss: 0.00098736
Iteration 7/25 | Loss: 0.00098722
Iteration 8/25 | Loss: 0.00098722
Iteration 9/25 | Loss: 0.00098722
Iteration 10/25 | Loss: 0.00098722
Iteration 11/25 | Loss: 0.00098722
Iteration 12/25 | Loss: 0.00098722
Iteration 13/25 | Loss: 0.00098722
Iteration 14/25 | Loss: 0.00098722
Iteration 15/25 | Loss: 0.00098722
Iteration 16/25 | Loss: 0.00098722
Iteration 17/25 | Loss: 0.00098722
Iteration 18/25 | Loss: 0.00098722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000987223582342267, 0.000987223582342267, 0.000987223582342267, 0.000987223582342267, 0.000987223582342267]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000987223582342267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26739287
Iteration 2/25 | Loss: 0.00067049
Iteration 3/25 | Loss: 0.00067049
Iteration 4/25 | Loss: 0.00067049
Iteration 5/25 | Loss: 0.00067049
Iteration 6/25 | Loss: 0.00067049
Iteration 7/25 | Loss: 0.00067049
Iteration 8/25 | Loss: 0.00067049
Iteration 9/25 | Loss: 0.00067049
Iteration 10/25 | Loss: 0.00067049
Iteration 11/25 | Loss: 0.00067049
Iteration 12/25 | Loss: 0.00067049
Iteration 13/25 | Loss: 0.00067049
Iteration 14/25 | Loss: 0.00067049
Iteration 15/25 | Loss: 0.00067049
Iteration 16/25 | Loss: 0.00067049
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006704852567054331, 0.0006704852567054331, 0.0006704852567054331, 0.0006704852567054331, 0.0006704852567054331]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006704852567054331

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067049
Iteration 2/1000 | Loss: 0.00005051
Iteration 3/1000 | Loss: 0.00004234
Iteration 4/1000 | Loss: 0.00003878
Iteration 5/1000 | Loss: 0.00003679
Iteration 6/1000 | Loss: 0.00003561
Iteration 7/1000 | Loss: 0.00003450
Iteration 8/1000 | Loss: 0.00003407
Iteration 9/1000 | Loss: 0.00003376
Iteration 10/1000 | Loss: 0.00003354
Iteration 11/1000 | Loss: 0.00003340
Iteration 12/1000 | Loss: 0.00003335
Iteration 13/1000 | Loss: 0.00003335
Iteration 14/1000 | Loss: 0.00003333
Iteration 15/1000 | Loss: 0.00003333
Iteration 16/1000 | Loss: 0.00003332
Iteration 17/1000 | Loss: 0.00003332
Iteration 18/1000 | Loss: 0.00003332
Iteration 19/1000 | Loss: 0.00003332
Iteration 20/1000 | Loss: 0.00003332
Iteration 21/1000 | Loss: 0.00003332
Iteration 22/1000 | Loss: 0.00003331
Iteration 23/1000 | Loss: 0.00003331
Iteration 24/1000 | Loss: 0.00003331
Iteration 25/1000 | Loss: 0.00003330
Iteration 26/1000 | Loss: 0.00003330
Iteration 27/1000 | Loss: 0.00003330
Iteration 28/1000 | Loss: 0.00003330
Iteration 29/1000 | Loss: 0.00003330
Iteration 30/1000 | Loss: 0.00003330
Iteration 31/1000 | Loss: 0.00003330
Iteration 32/1000 | Loss: 0.00003329
Iteration 33/1000 | Loss: 0.00003329
Iteration 34/1000 | Loss: 0.00003329
Iteration 35/1000 | Loss: 0.00003329
Iteration 36/1000 | Loss: 0.00003329
Iteration 37/1000 | Loss: 0.00003329
Iteration 38/1000 | Loss: 0.00003329
Iteration 39/1000 | Loss: 0.00003329
Iteration 40/1000 | Loss: 0.00003329
Iteration 41/1000 | Loss: 0.00003329
Iteration 42/1000 | Loss: 0.00003329
Iteration 43/1000 | Loss: 0.00003329
Iteration 44/1000 | Loss: 0.00003329
Iteration 45/1000 | Loss: 0.00003329
Iteration 46/1000 | Loss: 0.00003329
Iteration 47/1000 | Loss: 0.00003329
Iteration 48/1000 | Loss: 0.00003329
Iteration 49/1000 | Loss: 0.00003329
Iteration 50/1000 | Loss: 0.00003329
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [3.329190076328814e-05, 3.329190076328814e-05, 3.329190076328814e-05, 3.329190076328814e-05, 3.329190076328814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.329190076328814e-05

Optimization complete. Final v2v error: 4.796972274780273 mm

Highest mean error: 5.2130255699157715 mm for frame 37

Lowest mean error: 4.317512035369873 mm for frame 17

Saving results

Total time: 27.760798931121826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01081887
Iteration 2/25 | Loss: 0.00220068
Iteration 3/25 | Loss: 0.00158254
Iteration 4/25 | Loss: 0.00143032
Iteration 5/25 | Loss: 0.00150023
Iteration 6/25 | Loss: 0.00147901
Iteration 7/25 | Loss: 0.00137571
Iteration 8/25 | Loss: 0.00133207
Iteration 9/25 | Loss: 0.00130720
Iteration 10/25 | Loss: 0.00126759
Iteration 11/25 | Loss: 0.00124833
Iteration 12/25 | Loss: 0.00123737
Iteration 13/25 | Loss: 0.00120995
Iteration 14/25 | Loss: 0.00119859
Iteration 15/25 | Loss: 0.00118712
Iteration 16/25 | Loss: 0.00117441
Iteration 17/25 | Loss: 0.00116435
Iteration 18/25 | Loss: 0.00116017
Iteration 19/25 | Loss: 0.00115487
Iteration 20/25 | Loss: 0.00114755
Iteration 21/25 | Loss: 0.00113835
Iteration 22/25 | Loss: 0.00113187
Iteration 23/25 | Loss: 0.00113013
Iteration 24/25 | Loss: 0.00113215
Iteration 25/25 | Loss: 0.00112850

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20322347
Iteration 2/25 | Loss: 0.00175271
Iteration 3/25 | Loss: 0.00175269
Iteration 4/25 | Loss: 0.00175269
Iteration 5/25 | Loss: 0.00175269
Iteration 6/25 | Loss: 0.00175269
Iteration 7/25 | Loss: 0.00175269
Iteration 8/25 | Loss: 0.00175269
Iteration 9/25 | Loss: 0.00175269
Iteration 10/25 | Loss: 0.00175269
Iteration 11/25 | Loss: 0.00175269
Iteration 12/25 | Loss: 0.00175269
Iteration 13/25 | Loss: 0.00175269
Iteration 14/25 | Loss: 0.00175269
Iteration 15/25 | Loss: 0.00175269
Iteration 16/25 | Loss: 0.00175269
Iteration 17/25 | Loss: 0.00175269
Iteration 18/25 | Loss: 0.00175269
Iteration 19/25 | Loss: 0.00175269
Iteration 20/25 | Loss: 0.00175269
Iteration 21/25 | Loss: 0.00175269
Iteration 22/25 | Loss: 0.00175269
Iteration 23/25 | Loss: 0.00175269
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0017526919255033135, 0.0017526919255033135, 0.0017526919255033135, 0.0017526919255033135, 0.0017526919255033135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017526919255033135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00175269
Iteration 2/1000 | Loss: 0.00045721
Iteration 3/1000 | Loss: 0.00053646
Iteration 4/1000 | Loss: 0.00034547
Iteration 5/1000 | Loss: 0.00042299
Iteration 6/1000 | Loss: 0.00045987
Iteration 7/1000 | Loss: 0.00054660
Iteration 8/1000 | Loss: 0.00026955
Iteration 9/1000 | Loss: 0.00032545
Iteration 10/1000 | Loss: 0.00020317
Iteration 11/1000 | Loss: 0.00044562
Iteration 12/1000 | Loss: 0.00037650
Iteration 13/1000 | Loss: 0.00016685
Iteration 14/1000 | Loss: 0.00041609
Iteration 15/1000 | Loss: 0.00027222
Iteration 16/1000 | Loss: 0.00027162
Iteration 17/1000 | Loss: 0.00022217
Iteration 18/1000 | Loss: 0.00034352
Iteration 19/1000 | Loss: 0.00026710
Iteration 20/1000 | Loss: 0.00034558
Iteration 21/1000 | Loss: 0.00028260
Iteration 22/1000 | Loss: 0.00032549
Iteration 23/1000 | Loss: 0.00028961
Iteration 24/1000 | Loss: 0.00029959
Iteration 25/1000 | Loss: 0.00038886
Iteration 26/1000 | Loss: 0.00027307
Iteration 27/1000 | Loss: 0.00032628
Iteration 28/1000 | Loss: 0.00032755
Iteration 29/1000 | Loss: 0.00027962
Iteration 30/1000 | Loss: 0.00025850
Iteration 31/1000 | Loss: 0.00022799
Iteration 32/1000 | Loss: 0.00019703
Iteration 33/1000 | Loss: 0.00028226
Iteration 34/1000 | Loss: 0.00033172
Iteration 35/1000 | Loss: 0.00032021
Iteration 36/1000 | Loss: 0.00032372
Iteration 37/1000 | Loss: 0.00036476
Iteration 38/1000 | Loss: 0.00026973
Iteration 39/1000 | Loss: 0.00021960
Iteration 40/1000 | Loss: 0.00026363
Iteration 41/1000 | Loss: 0.00033101
Iteration 42/1000 | Loss: 0.00029928
Iteration 43/1000 | Loss: 0.00028870
Iteration 44/1000 | Loss: 0.00028093
Iteration 45/1000 | Loss: 0.00024997
Iteration 46/1000 | Loss: 0.00022025
Iteration 47/1000 | Loss: 0.00031066
Iteration 48/1000 | Loss: 0.00024990
Iteration 49/1000 | Loss: 0.00034786
Iteration 50/1000 | Loss: 0.00019905
Iteration 51/1000 | Loss: 0.00021244
Iteration 52/1000 | Loss: 0.00015174
Iteration 53/1000 | Loss: 0.00017907
Iteration 54/1000 | Loss: 0.00026288
Iteration 55/1000 | Loss: 0.00018092
Iteration 56/1000 | Loss: 0.00018973
Iteration 57/1000 | Loss: 0.00019487
Iteration 58/1000 | Loss: 0.00026613
Iteration 59/1000 | Loss: 0.00016412
Iteration 60/1000 | Loss: 0.00010094
Iteration 61/1000 | Loss: 0.00010763
Iteration 62/1000 | Loss: 0.00009066
Iteration 63/1000 | Loss: 0.00008924
Iteration 64/1000 | Loss: 0.00009075
Iteration 65/1000 | Loss: 0.00009946
Iteration 66/1000 | Loss: 0.00009040
Iteration 67/1000 | Loss: 0.00009359
Iteration 68/1000 | Loss: 0.00010955
Iteration 69/1000 | Loss: 0.00055467
Iteration 70/1000 | Loss: 0.00016678
Iteration 71/1000 | Loss: 0.00042288
Iteration 72/1000 | Loss: 0.00014627
Iteration 73/1000 | Loss: 0.00009801
Iteration 74/1000 | Loss: 0.00008583
Iteration 75/1000 | Loss: 0.00042356
Iteration 76/1000 | Loss: 0.00025346
Iteration 77/1000 | Loss: 0.00009419
Iteration 78/1000 | Loss: 0.00043860
Iteration 79/1000 | Loss: 0.00043436
Iteration 80/1000 | Loss: 0.00017702
Iteration 81/1000 | Loss: 0.00022178
Iteration 82/1000 | Loss: 0.00022069
Iteration 83/1000 | Loss: 0.00008302
Iteration 84/1000 | Loss: 0.00007857
Iteration 85/1000 | Loss: 0.00007654
Iteration 86/1000 | Loss: 0.00007546
Iteration 87/1000 | Loss: 0.00021983
Iteration 88/1000 | Loss: 0.00017852
Iteration 89/1000 | Loss: 0.00018284
Iteration 90/1000 | Loss: 0.00040902
Iteration 91/1000 | Loss: 0.00104188
Iteration 92/1000 | Loss: 0.00048208
Iteration 93/1000 | Loss: 0.00016001
Iteration 94/1000 | Loss: 0.00037850
Iteration 95/1000 | Loss: 0.00009612
Iteration 96/1000 | Loss: 0.00008236
Iteration 97/1000 | Loss: 0.00007600
Iteration 98/1000 | Loss: 0.00007150
Iteration 99/1000 | Loss: 0.00006813
Iteration 100/1000 | Loss: 0.00006659
Iteration 101/1000 | Loss: 0.00006564
Iteration 102/1000 | Loss: 0.00006523
Iteration 103/1000 | Loss: 0.00006484
Iteration 104/1000 | Loss: 0.00036572
Iteration 105/1000 | Loss: 0.00017222
Iteration 106/1000 | Loss: 0.00013815
Iteration 107/1000 | Loss: 0.00006743
Iteration 108/1000 | Loss: 0.00006523
Iteration 109/1000 | Loss: 0.00006384
Iteration 110/1000 | Loss: 0.00006288
Iteration 111/1000 | Loss: 0.00006240
Iteration 112/1000 | Loss: 0.00006215
Iteration 113/1000 | Loss: 0.00006213
Iteration 114/1000 | Loss: 0.00006205
Iteration 115/1000 | Loss: 0.00006201
Iteration 116/1000 | Loss: 0.00006201
Iteration 117/1000 | Loss: 0.00006200
Iteration 118/1000 | Loss: 0.00006200
Iteration 119/1000 | Loss: 0.00006200
Iteration 120/1000 | Loss: 0.00006200
Iteration 121/1000 | Loss: 0.00006199
Iteration 122/1000 | Loss: 0.00006199
Iteration 123/1000 | Loss: 0.00006199
Iteration 124/1000 | Loss: 0.00006198
Iteration 125/1000 | Loss: 0.00006196
Iteration 126/1000 | Loss: 0.00006195
Iteration 127/1000 | Loss: 0.00006194
Iteration 128/1000 | Loss: 0.00006194
Iteration 129/1000 | Loss: 0.00006193
Iteration 130/1000 | Loss: 0.00006193
Iteration 131/1000 | Loss: 0.00006191
Iteration 132/1000 | Loss: 0.00006191
Iteration 133/1000 | Loss: 0.00006190
Iteration 134/1000 | Loss: 0.00006190
Iteration 135/1000 | Loss: 0.00006190
Iteration 136/1000 | Loss: 0.00006189
Iteration 137/1000 | Loss: 0.00006189
Iteration 138/1000 | Loss: 0.00006189
Iteration 139/1000 | Loss: 0.00006189
Iteration 140/1000 | Loss: 0.00006189
Iteration 141/1000 | Loss: 0.00006188
Iteration 142/1000 | Loss: 0.00006187
Iteration 143/1000 | Loss: 0.00006187
Iteration 144/1000 | Loss: 0.00006184
Iteration 145/1000 | Loss: 0.00006184
Iteration 146/1000 | Loss: 0.00006181
Iteration 147/1000 | Loss: 0.00006181
Iteration 148/1000 | Loss: 0.00006181
Iteration 149/1000 | Loss: 0.00006181
Iteration 150/1000 | Loss: 0.00006181
Iteration 151/1000 | Loss: 0.00006181
Iteration 152/1000 | Loss: 0.00006180
Iteration 153/1000 | Loss: 0.00006180
Iteration 154/1000 | Loss: 0.00006180
Iteration 155/1000 | Loss: 0.00006180
Iteration 156/1000 | Loss: 0.00006180
Iteration 157/1000 | Loss: 0.00006180
Iteration 158/1000 | Loss: 0.00006180
Iteration 159/1000 | Loss: 0.00006180
Iteration 160/1000 | Loss: 0.00006180
Iteration 161/1000 | Loss: 0.00006180
Iteration 162/1000 | Loss: 0.00006180
Iteration 163/1000 | Loss: 0.00006180
Iteration 164/1000 | Loss: 0.00006179
Iteration 165/1000 | Loss: 0.00006179
Iteration 166/1000 | Loss: 0.00006179
Iteration 167/1000 | Loss: 0.00006179
Iteration 168/1000 | Loss: 0.00006178
Iteration 169/1000 | Loss: 0.00006178
Iteration 170/1000 | Loss: 0.00006178
Iteration 171/1000 | Loss: 0.00006178
Iteration 172/1000 | Loss: 0.00006178
Iteration 173/1000 | Loss: 0.00006178
Iteration 174/1000 | Loss: 0.00006177
Iteration 175/1000 | Loss: 0.00006177
Iteration 176/1000 | Loss: 0.00006177
Iteration 177/1000 | Loss: 0.00006177
Iteration 178/1000 | Loss: 0.00006177
Iteration 179/1000 | Loss: 0.00006177
Iteration 180/1000 | Loss: 0.00006177
Iteration 181/1000 | Loss: 0.00006177
Iteration 182/1000 | Loss: 0.00006177
Iteration 183/1000 | Loss: 0.00006177
Iteration 184/1000 | Loss: 0.00006177
Iteration 185/1000 | Loss: 0.00006176
Iteration 186/1000 | Loss: 0.00006176
Iteration 187/1000 | Loss: 0.00006176
Iteration 188/1000 | Loss: 0.00006176
Iteration 189/1000 | Loss: 0.00006176
Iteration 190/1000 | Loss: 0.00006175
Iteration 191/1000 | Loss: 0.00006175
Iteration 192/1000 | Loss: 0.00006175
Iteration 193/1000 | Loss: 0.00006175
Iteration 194/1000 | Loss: 0.00006175
Iteration 195/1000 | Loss: 0.00006175
Iteration 196/1000 | Loss: 0.00006175
Iteration 197/1000 | Loss: 0.00006175
Iteration 198/1000 | Loss: 0.00006175
Iteration 199/1000 | Loss: 0.00006175
Iteration 200/1000 | Loss: 0.00006174
Iteration 201/1000 | Loss: 0.00006174
Iteration 202/1000 | Loss: 0.00006174
Iteration 203/1000 | Loss: 0.00006174
Iteration 204/1000 | Loss: 0.00006174
Iteration 205/1000 | Loss: 0.00006174
Iteration 206/1000 | Loss: 0.00006174
Iteration 207/1000 | Loss: 0.00006174
Iteration 208/1000 | Loss: 0.00006174
Iteration 209/1000 | Loss: 0.00006174
Iteration 210/1000 | Loss: 0.00006174
Iteration 211/1000 | Loss: 0.00006174
Iteration 212/1000 | Loss: 0.00006174
Iteration 213/1000 | Loss: 0.00006174
Iteration 214/1000 | Loss: 0.00006173
Iteration 215/1000 | Loss: 0.00006173
Iteration 216/1000 | Loss: 0.00006173
Iteration 217/1000 | Loss: 0.00006173
Iteration 218/1000 | Loss: 0.00006173
Iteration 219/1000 | Loss: 0.00006173
Iteration 220/1000 | Loss: 0.00006173
Iteration 221/1000 | Loss: 0.00006173
Iteration 222/1000 | Loss: 0.00006173
Iteration 223/1000 | Loss: 0.00006173
Iteration 224/1000 | Loss: 0.00006173
Iteration 225/1000 | Loss: 0.00006172
Iteration 226/1000 | Loss: 0.00006172
Iteration 227/1000 | Loss: 0.00006172
Iteration 228/1000 | Loss: 0.00006172
Iteration 229/1000 | Loss: 0.00006172
Iteration 230/1000 | Loss: 0.00006172
Iteration 231/1000 | Loss: 0.00006172
Iteration 232/1000 | Loss: 0.00006172
Iteration 233/1000 | Loss: 0.00006171
Iteration 234/1000 | Loss: 0.00006171
Iteration 235/1000 | Loss: 0.00006171
Iteration 236/1000 | Loss: 0.00006171
Iteration 237/1000 | Loss: 0.00006171
Iteration 238/1000 | Loss: 0.00006171
Iteration 239/1000 | Loss: 0.00006171
Iteration 240/1000 | Loss: 0.00006171
Iteration 241/1000 | Loss: 0.00006171
Iteration 242/1000 | Loss: 0.00006171
Iteration 243/1000 | Loss: 0.00006171
Iteration 244/1000 | Loss: 0.00006171
Iteration 245/1000 | Loss: 0.00006171
Iteration 246/1000 | Loss: 0.00006171
Iteration 247/1000 | Loss: 0.00006171
Iteration 248/1000 | Loss: 0.00006171
Iteration 249/1000 | Loss: 0.00006171
Iteration 250/1000 | Loss: 0.00006171
Iteration 251/1000 | Loss: 0.00006171
Iteration 252/1000 | Loss: 0.00006171
Iteration 253/1000 | Loss: 0.00006171
Iteration 254/1000 | Loss: 0.00006171
Iteration 255/1000 | Loss: 0.00006171
Iteration 256/1000 | Loss: 0.00006171
Iteration 257/1000 | Loss: 0.00006171
Iteration 258/1000 | Loss: 0.00006171
Iteration 259/1000 | Loss: 0.00006171
Iteration 260/1000 | Loss: 0.00006171
Iteration 261/1000 | Loss: 0.00006171
Iteration 262/1000 | Loss: 0.00006171
Iteration 263/1000 | Loss: 0.00006171
Iteration 264/1000 | Loss: 0.00006171
Iteration 265/1000 | Loss: 0.00006171
Iteration 266/1000 | Loss: 0.00006171
Iteration 267/1000 | Loss: 0.00006171
Iteration 268/1000 | Loss: 0.00006171
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [6.170954293338582e-05, 6.170954293338582e-05, 6.170954293338582e-05, 6.170954293338582e-05, 6.170954293338582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.170954293338582e-05

Optimization complete. Final v2v error: 4.404355049133301 mm

Highest mean error: 12.279358863830566 mm for frame 77

Lowest mean error: 3.3066632747650146 mm for frame 203

Saving results

Total time: 242.61734676361084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01001050
Iteration 2/25 | Loss: 0.00148540
Iteration 3/25 | Loss: 0.00108919
Iteration 4/25 | Loss: 0.00104875
Iteration 5/25 | Loss: 0.00103927
Iteration 6/25 | Loss: 0.00103704
Iteration 7/25 | Loss: 0.00103670
Iteration 8/25 | Loss: 0.00103670
Iteration 9/25 | Loss: 0.00103670
Iteration 10/25 | Loss: 0.00103670
Iteration 11/25 | Loss: 0.00103670
Iteration 12/25 | Loss: 0.00103670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010367018403485417, 0.0010367018403485417, 0.0010367018403485417, 0.0010367018403485417, 0.0010367018403485417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010367018403485417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.22356391
Iteration 2/25 | Loss: 0.00051638
Iteration 3/25 | Loss: 0.00051635
Iteration 4/25 | Loss: 0.00051635
Iteration 5/25 | Loss: 0.00051635
Iteration 6/25 | Loss: 0.00051635
Iteration 7/25 | Loss: 0.00051635
Iteration 8/25 | Loss: 0.00051635
Iteration 9/25 | Loss: 0.00051635
Iteration 10/25 | Loss: 0.00051635
Iteration 11/25 | Loss: 0.00051635
Iteration 12/25 | Loss: 0.00051635
Iteration 13/25 | Loss: 0.00051635
Iteration 14/25 | Loss: 0.00051635
Iteration 15/25 | Loss: 0.00051635
Iteration 16/25 | Loss: 0.00051635
Iteration 17/25 | Loss: 0.00051635
Iteration 18/25 | Loss: 0.00051635
Iteration 19/25 | Loss: 0.00051635
Iteration 20/25 | Loss: 0.00051635
Iteration 21/25 | Loss: 0.00051635
Iteration 22/25 | Loss: 0.00051635
Iteration 23/25 | Loss: 0.00051635
Iteration 24/25 | Loss: 0.00051635
Iteration 25/25 | Loss: 0.00051635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005163521855138242, 0.0005163521855138242, 0.0005163521855138242, 0.0005163521855138242, 0.0005163521855138242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005163521855138242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051635
Iteration 2/1000 | Loss: 0.00006744
Iteration 3/1000 | Loss: 0.00005109
Iteration 4/1000 | Loss: 0.00004335
Iteration 5/1000 | Loss: 0.00004117
Iteration 6/1000 | Loss: 0.00003967
Iteration 7/1000 | Loss: 0.00003882
Iteration 8/1000 | Loss: 0.00003814
Iteration 9/1000 | Loss: 0.00003766
Iteration 10/1000 | Loss: 0.00003719
Iteration 11/1000 | Loss: 0.00003691
Iteration 12/1000 | Loss: 0.00003663
Iteration 13/1000 | Loss: 0.00003642
Iteration 14/1000 | Loss: 0.00003623
Iteration 15/1000 | Loss: 0.00003607
Iteration 16/1000 | Loss: 0.00003603
Iteration 17/1000 | Loss: 0.00003602
Iteration 18/1000 | Loss: 0.00003599
Iteration 19/1000 | Loss: 0.00003584
Iteration 20/1000 | Loss: 0.00003577
Iteration 21/1000 | Loss: 0.00003577
Iteration 22/1000 | Loss: 0.00003574
Iteration 23/1000 | Loss: 0.00003573
Iteration 24/1000 | Loss: 0.00003571
Iteration 25/1000 | Loss: 0.00003570
Iteration 26/1000 | Loss: 0.00003570
Iteration 27/1000 | Loss: 0.00003569
Iteration 28/1000 | Loss: 0.00003569
Iteration 29/1000 | Loss: 0.00003568
Iteration 30/1000 | Loss: 0.00003568
Iteration 31/1000 | Loss: 0.00003568
Iteration 32/1000 | Loss: 0.00003567
Iteration 33/1000 | Loss: 0.00003567
Iteration 34/1000 | Loss: 0.00003563
Iteration 35/1000 | Loss: 0.00003563
Iteration 36/1000 | Loss: 0.00003563
Iteration 37/1000 | Loss: 0.00003562
Iteration 38/1000 | Loss: 0.00003561
Iteration 39/1000 | Loss: 0.00003558
Iteration 40/1000 | Loss: 0.00003558
Iteration 41/1000 | Loss: 0.00003557
Iteration 42/1000 | Loss: 0.00003557
Iteration 43/1000 | Loss: 0.00003556
Iteration 44/1000 | Loss: 0.00003556
Iteration 45/1000 | Loss: 0.00003556
Iteration 46/1000 | Loss: 0.00003553
Iteration 47/1000 | Loss: 0.00003552
Iteration 48/1000 | Loss: 0.00003552
Iteration 49/1000 | Loss: 0.00003552
Iteration 50/1000 | Loss: 0.00003551
Iteration 51/1000 | Loss: 0.00003550
Iteration 52/1000 | Loss: 0.00003549
Iteration 53/1000 | Loss: 0.00003549
Iteration 54/1000 | Loss: 0.00003547
Iteration 55/1000 | Loss: 0.00003547
Iteration 56/1000 | Loss: 0.00003546
Iteration 57/1000 | Loss: 0.00003546
Iteration 58/1000 | Loss: 0.00003545
Iteration 59/1000 | Loss: 0.00003545
Iteration 60/1000 | Loss: 0.00003544
Iteration 61/1000 | Loss: 0.00003544
Iteration 62/1000 | Loss: 0.00003544
Iteration 63/1000 | Loss: 0.00003543
Iteration 64/1000 | Loss: 0.00003543
Iteration 65/1000 | Loss: 0.00003542
Iteration 66/1000 | Loss: 0.00003542
Iteration 67/1000 | Loss: 0.00003542
Iteration 68/1000 | Loss: 0.00003541
Iteration 69/1000 | Loss: 0.00003541
Iteration 70/1000 | Loss: 0.00003540
Iteration 71/1000 | Loss: 0.00003540
Iteration 72/1000 | Loss: 0.00003538
Iteration 73/1000 | Loss: 0.00003538
Iteration 74/1000 | Loss: 0.00003537
Iteration 75/1000 | Loss: 0.00003537
Iteration 76/1000 | Loss: 0.00003537
Iteration 77/1000 | Loss: 0.00003536
Iteration 78/1000 | Loss: 0.00003536
Iteration 79/1000 | Loss: 0.00003536
Iteration 80/1000 | Loss: 0.00003536
Iteration 81/1000 | Loss: 0.00003536
Iteration 82/1000 | Loss: 0.00003536
Iteration 83/1000 | Loss: 0.00003536
Iteration 84/1000 | Loss: 0.00003536
Iteration 85/1000 | Loss: 0.00003536
Iteration 86/1000 | Loss: 0.00003536
Iteration 87/1000 | Loss: 0.00003535
Iteration 88/1000 | Loss: 0.00003535
Iteration 89/1000 | Loss: 0.00003532
Iteration 90/1000 | Loss: 0.00003532
Iteration 91/1000 | Loss: 0.00003532
Iteration 92/1000 | Loss: 0.00003531
Iteration 93/1000 | Loss: 0.00003531
Iteration 94/1000 | Loss: 0.00003531
Iteration 95/1000 | Loss: 0.00003530
Iteration 96/1000 | Loss: 0.00003530
Iteration 97/1000 | Loss: 0.00003530
Iteration 98/1000 | Loss: 0.00003530
Iteration 99/1000 | Loss: 0.00003530
Iteration 100/1000 | Loss: 0.00003530
Iteration 101/1000 | Loss: 0.00003529
Iteration 102/1000 | Loss: 0.00003529
Iteration 103/1000 | Loss: 0.00003529
Iteration 104/1000 | Loss: 0.00003529
Iteration 105/1000 | Loss: 0.00003529
Iteration 106/1000 | Loss: 0.00003529
Iteration 107/1000 | Loss: 0.00003529
Iteration 108/1000 | Loss: 0.00003529
Iteration 109/1000 | Loss: 0.00003529
Iteration 110/1000 | Loss: 0.00003529
Iteration 111/1000 | Loss: 0.00003529
Iteration 112/1000 | Loss: 0.00003529
Iteration 113/1000 | Loss: 0.00003528
Iteration 114/1000 | Loss: 0.00003528
Iteration 115/1000 | Loss: 0.00003528
Iteration 116/1000 | Loss: 0.00003528
Iteration 117/1000 | Loss: 0.00003528
Iteration 118/1000 | Loss: 0.00003528
Iteration 119/1000 | Loss: 0.00003528
Iteration 120/1000 | Loss: 0.00003528
Iteration 121/1000 | Loss: 0.00003528
Iteration 122/1000 | Loss: 0.00003528
Iteration 123/1000 | Loss: 0.00003528
Iteration 124/1000 | Loss: 0.00003527
Iteration 125/1000 | Loss: 0.00003527
Iteration 126/1000 | Loss: 0.00003527
Iteration 127/1000 | Loss: 0.00003527
Iteration 128/1000 | Loss: 0.00003526
Iteration 129/1000 | Loss: 0.00003526
Iteration 130/1000 | Loss: 0.00003525
Iteration 131/1000 | Loss: 0.00003524
Iteration 132/1000 | Loss: 0.00003524
Iteration 133/1000 | Loss: 0.00003524
Iteration 134/1000 | Loss: 0.00003524
Iteration 135/1000 | Loss: 0.00003523
Iteration 136/1000 | Loss: 0.00003521
Iteration 137/1000 | Loss: 0.00003521
Iteration 138/1000 | Loss: 0.00003521
Iteration 139/1000 | Loss: 0.00003521
Iteration 140/1000 | Loss: 0.00003521
Iteration 141/1000 | Loss: 0.00003521
Iteration 142/1000 | Loss: 0.00003520
Iteration 143/1000 | Loss: 0.00003520
Iteration 144/1000 | Loss: 0.00003520
Iteration 145/1000 | Loss: 0.00003520
Iteration 146/1000 | Loss: 0.00003520
Iteration 147/1000 | Loss: 0.00003520
Iteration 148/1000 | Loss: 0.00003520
Iteration 149/1000 | Loss: 0.00003520
Iteration 150/1000 | Loss: 0.00003520
Iteration 151/1000 | Loss: 0.00003520
Iteration 152/1000 | Loss: 0.00003520
Iteration 153/1000 | Loss: 0.00003520
Iteration 154/1000 | Loss: 0.00003520
Iteration 155/1000 | Loss: 0.00003520
Iteration 156/1000 | Loss: 0.00003520
Iteration 157/1000 | Loss: 0.00003519
Iteration 158/1000 | Loss: 0.00003519
Iteration 159/1000 | Loss: 0.00003519
Iteration 160/1000 | Loss: 0.00003519
Iteration 161/1000 | Loss: 0.00003519
Iteration 162/1000 | Loss: 0.00003519
Iteration 163/1000 | Loss: 0.00003519
Iteration 164/1000 | Loss: 0.00003519
Iteration 165/1000 | Loss: 0.00003518
Iteration 166/1000 | Loss: 0.00003518
Iteration 167/1000 | Loss: 0.00003518
Iteration 168/1000 | Loss: 0.00003518
Iteration 169/1000 | Loss: 0.00003517
Iteration 170/1000 | Loss: 0.00003517
Iteration 171/1000 | Loss: 0.00003517
Iteration 172/1000 | Loss: 0.00003517
Iteration 173/1000 | Loss: 0.00003516
Iteration 174/1000 | Loss: 0.00003516
Iteration 175/1000 | Loss: 0.00003516
Iteration 176/1000 | Loss: 0.00003516
Iteration 177/1000 | Loss: 0.00003516
Iteration 178/1000 | Loss: 0.00003516
Iteration 179/1000 | Loss: 0.00003516
Iteration 180/1000 | Loss: 0.00003516
Iteration 181/1000 | Loss: 0.00003516
Iteration 182/1000 | Loss: 0.00003516
Iteration 183/1000 | Loss: 0.00003516
Iteration 184/1000 | Loss: 0.00003516
Iteration 185/1000 | Loss: 0.00003516
Iteration 186/1000 | Loss: 0.00003516
Iteration 187/1000 | Loss: 0.00003516
Iteration 188/1000 | Loss: 0.00003516
Iteration 189/1000 | Loss: 0.00003515
Iteration 190/1000 | Loss: 0.00003515
Iteration 191/1000 | Loss: 0.00003515
Iteration 192/1000 | Loss: 0.00003515
Iteration 193/1000 | Loss: 0.00003515
Iteration 194/1000 | Loss: 0.00003514
Iteration 195/1000 | Loss: 0.00003514
Iteration 196/1000 | Loss: 0.00003514
Iteration 197/1000 | Loss: 0.00003514
Iteration 198/1000 | Loss: 0.00003514
Iteration 199/1000 | Loss: 0.00003514
Iteration 200/1000 | Loss: 0.00003514
Iteration 201/1000 | Loss: 0.00003514
Iteration 202/1000 | Loss: 0.00003514
Iteration 203/1000 | Loss: 0.00003513
Iteration 204/1000 | Loss: 0.00003513
Iteration 205/1000 | Loss: 0.00003513
Iteration 206/1000 | Loss: 0.00003513
Iteration 207/1000 | Loss: 0.00003513
Iteration 208/1000 | Loss: 0.00003512
Iteration 209/1000 | Loss: 0.00003512
Iteration 210/1000 | Loss: 0.00003512
Iteration 211/1000 | Loss: 0.00003512
Iteration 212/1000 | Loss: 0.00003512
Iteration 213/1000 | Loss: 0.00003512
Iteration 214/1000 | Loss: 0.00003512
Iteration 215/1000 | Loss: 0.00003512
Iteration 216/1000 | Loss: 0.00003511
Iteration 217/1000 | Loss: 0.00003511
Iteration 218/1000 | Loss: 0.00003511
Iteration 219/1000 | Loss: 0.00003511
Iteration 220/1000 | Loss: 0.00003511
Iteration 221/1000 | Loss: 0.00003511
Iteration 222/1000 | Loss: 0.00003510
Iteration 223/1000 | Loss: 0.00003510
Iteration 224/1000 | Loss: 0.00003510
Iteration 225/1000 | Loss: 0.00003510
Iteration 226/1000 | Loss: 0.00003510
Iteration 227/1000 | Loss: 0.00003510
Iteration 228/1000 | Loss: 0.00003509
Iteration 229/1000 | Loss: 0.00003509
Iteration 230/1000 | Loss: 0.00003509
Iteration 231/1000 | Loss: 0.00003509
Iteration 232/1000 | Loss: 0.00003509
Iteration 233/1000 | Loss: 0.00003509
Iteration 234/1000 | Loss: 0.00003509
Iteration 235/1000 | Loss: 0.00003509
Iteration 236/1000 | Loss: 0.00003508
Iteration 237/1000 | Loss: 0.00003508
Iteration 238/1000 | Loss: 0.00003508
Iteration 239/1000 | Loss: 0.00003508
Iteration 240/1000 | Loss: 0.00003508
Iteration 241/1000 | Loss: 0.00003508
Iteration 242/1000 | Loss: 0.00003507
Iteration 243/1000 | Loss: 0.00003507
Iteration 244/1000 | Loss: 0.00003507
Iteration 245/1000 | Loss: 0.00003507
Iteration 246/1000 | Loss: 0.00003507
Iteration 247/1000 | Loss: 0.00003507
Iteration 248/1000 | Loss: 0.00003507
Iteration 249/1000 | Loss: 0.00003506
Iteration 250/1000 | Loss: 0.00003506
Iteration 251/1000 | Loss: 0.00003506
Iteration 252/1000 | Loss: 0.00003506
Iteration 253/1000 | Loss: 0.00003506
Iteration 254/1000 | Loss: 0.00003506
Iteration 255/1000 | Loss: 0.00003506
Iteration 256/1000 | Loss: 0.00003506
Iteration 257/1000 | Loss: 0.00003506
Iteration 258/1000 | Loss: 0.00003505
Iteration 259/1000 | Loss: 0.00003505
Iteration 260/1000 | Loss: 0.00003505
Iteration 261/1000 | Loss: 0.00003505
Iteration 262/1000 | Loss: 0.00003505
Iteration 263/1000 | Loss: 0.00003505
Iteration 264/1000 | Loss: 0.00003504
Iteration 265/1000 | Loss: 0.00003504
Iteration 266/1000 | Loss: 0.00003504
Iteration 267/1000 | Loss: 0.00003504
Iteration 268/1000 | Loss: 0.00003504
Iteration 269/1000 | Loss: 0.00003504
Iteration 270/1000 | Loss: 0.00003504
Iteration 271/1000 | Loss: 0.00003504
Iteration 272/1000 | Loss: 0.00003503
Iteration 273/1000 | Loss: 0.00003503
Iteration 274/1000 | Loss: 0.00003503
Iteration 275/1000 | Loss: 0.00003503
Iteration 276/1000 | Loss: 0.00003503
Iteration 277/1000 | Loss: 0.00003502
Iteration 278/1000 | Loss: 0.00003502
Iteration 279/1000 | Loss: 0.00003502
Iteration 280/1000 | Loss: 0.00003502
Iteration 281/1000 | Loss: 0.00003502
Iteration 282/1000 | Loss: 0.00003502
Iteration 283/1000 | Loss: 0.00003502
Iteration 284/1000 | Loss: 0.00003501
Iteration 285/1000 | Loss: 0.00003501
Iteration 286/1000 | Loss: 0.00003501
Iteration 287/1000 | Loss: 0.00003501
Iteration 288/1000 | Loss: 0.00003501
Iteration 289/1000 | Loss: 0.00003501
Iteration 290/1000 | Loss: 0.00003501
Iteration 291/1000 | Loss: 0.00003501
Iteration 292/1000 | Loss: 0.00003501
Iteration 293/1000 | Loss: 0.00003501
Iteration 294/1000 | Loss: 0.00003500
Iteration 295/1000 | Loss: 0.00003500
Iteration 296/1000 | Loss: 0.00003500
Iteration 297/1000 | Loss: 0.00003500
Iteration 298/1000 | Loss: 0.00003500
Iteration 299/1000 | Loss: 0.00003500
Iteration 300/1000 | Loss: 0.00003500
Iteration 301/1000 | Loss: 0.00003500
Iteration 302/1000 | Loss: 0.00003500
Iteration 303/1000 | Loss: 0.00003499
Iteration 304/1000 | Loss: 0.00003499
Iteration 305/1000 | Loss: 0.00003499
Iteration 306/1000 | Loss: 0.00003499
Iteration 307/1000 | Loss: 0.00003499
Iteration 308/1000 | Loss: 0.00003499
Iteration 309/1000 | Loss: 0.00003499
Iteration 310/1000 | Loss: 0.00003499
Iteration 311/1000 | Loss: 0.00003499
Iteration 312/1000 | Loss: 0.00003499
Iteration 313/1000 | Loss: 0.00003499
Iteration 314/1000 | Loss: 0.00003499
Iteration 315/1000 | Loss: 0.00003498
Iteration 316/1000 | Loss: 0.00003498
Iteration 317/1000 | Loss: 0.00003498
Iteration 318/1000 | Loss: 0.00003498
Iteration 319/1000 | Loss: 0.00003498
Iteration 320/1000 | Loss: 0.00003498
Iteration 321/1000 | Loss: 0.00003498
Iteration 322/1000 | Loss: 0.00003498
Iteration 323/1000 | Loss: 0.00003498
Iteration 324/1000 | Loss: 0.00003497
Iteration 325/1000 | Loss: 0.00003497
Iteration 326/1000 | Loss: 0.00003497
Iteration 327/1000 | Loss: 0.00003497
Iteration 328/1000 | Loss: 0.00003497
Iteration 329/1000 | Loss: 0.00003497
Iteration 330/1000 | Loss: 0.00003497
Iteration 331/1000 | Loss: 0.00003496
Iteration 332/1000 | Loss: 0.00003496
Iteration 333/1000 | Loss: 0.00003496
Iteration 334/1000 | Loss: 0.00003496
Iteration 335/1000 | Loss: 0.00003496
Iteration 336/1000 | Loss: 0.00003496
Iteration 337/1000 | Loss: 0.00003496
Iteration 338/1000 | Loss: 0.00003496
Iteration 339/1000 | Loss: 0.00003496
Iteration 340/1000 | Loss: 0.00003496
Iteration 341/1000 | Loss: 0.00003496
Iteration 342/1000 | Loss: 0.00003496
Iteration 343/1000 | Loss: 0.00003496
Iteration 344/1000 | Loss: 0.00003496
Iteration 345/1000 | Loss: 0.00003496
Iteration 346/1000 | Loss: 0.00003495
Iteration 347/1000 | Loss: 0.00003495
Iteration 348/1000 | Loss: 0.00003495
Iteration 349/1000 | Loss: 0.00003495
Iteration 350/1000 | Loss: 0.00003495
Iteration 351/1000 | Loss: 0.00003495
Iteration 352/1000 | Loss: 0.00003495
Iteration 353/1000 | Loss: 0.00003495
Iteration 354/1000 | Loss: 0.00003495
Iteration 355/1000 | Loss: 0.00003495
Iteration 356/1000 | Loss: 0.00003495
Iteration 357/1000 | Loss: 0.00003495
Iteration 358/1000 | Loss: 0.00003495
Iteration 359/1000 | Loss: 0.00003494
Iteration 360/1000 | Loss: 0.00003494
Iteration 361/1000 | Loss: 0.00003494
Iteration 362/1000 | Loss: 0.00003494
Iteration 363/1000 | Loss: 0.00003494
Iteration 364/1000 | Loss: 0.00003494
Iteration 365/1000 | Loss: 0.00003494
Iteration 366/1000 | Loss: 0.00003494
Iteration 367/1000 | Loss: 0.00003493
Iteration 368/1000 | Loss: 0.00003493
Iteration 369/1000 | Loss: 0.00003493
Iteration 370/1000 | Loss: 0.00003493
Iteration 371/1000 | Loss: 0.00003493
Iteration 372/1000 | Loss: 0.00003493
Iteration 373/1000 | Loss: 0.00003493
Iteration 374/1000 | Loss: 0.00003493
Iteration 375/1000 | Loss: 0.00003493
Iteration 376/1000 | Loss: 0.00003493
Iteration 377/1000 | Loss: 0.00003493
Iteration 378/1000 | Loss: 0.00003493
Iteration 379/1000 | Loss: 0.00003493
Iteration 380/1000 | Loss: 0.00003493
Iteration 381/1000 | Loss: 0.00003493
Iteration 382/1000 | Loss: 0.00003493
Iteration 383/1000 | Loss: 0.00003493
Iteration 384/1000 | Loss: 0.00003493
Iteration 385/1000 | Loss: 0.00003493
Iteration 386/1000 | Loss: 0.00003493
Iteration 387/1000 | Loss: 0.00003493
Iteration 388/1000 | Loss: 0.00003493
Iteration 389/1000 | Loss: 0.00003493
Iteration 390/1000 | Loss: 0.00003493
Iteration 391/1000 | Loss: 0.00003493
Iteration 392/1000 | Loss: 0.00003493
Iteration 393/1000 | Loss: 0.00003493
Iteration 394/1000 | Loss: 0.00003493
Iteration 395/1000 | Loss: 0.00003493
Iteration 396/1000 | Loss: 0.00003493
Iteration 397/1000 | Loss: 0.00003493
Iteration 398/1000 | Loss: 0.00003493
Iteration 399/1000 | Loss: 0.00003493
Iteration 400/1000 | Loss: 0.00003493
Iteration 401/1000 | Loss: 0.00003493
Iteration 402/1000 | Loss: 0.00003493
Iteration 403/1000 | Loss: 0.00003493
Iteration 404/1000 | Loss: 0.00003493
Iteration 405/1000 | Loss: 0.00003493
Iteration 406/1000 | Loss: 0.00003493
Iteration 407/1000 | Loss: 0.00003493
Iteration 408/1000 | Loss: 0.00003493
Iteration 409/1000 | Loss: 0.00003493
Iteration 410/1000 | Loss: 0.00003493
Iteration 411/1000 | Loss: 0.00003493
Iteration 412/1000 | Loss: 0.00003493
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 412. Stopping optimization.
Last 5 losses: [3.4932378184748814e-05, 3.4932378184748814e-05, 3.4932378184748814e-05, 3.4932378184748814e-05, 3.4932378184748814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4932378184748814e-05

Optimization complete. Final v2v error: 4.763958930969238 mm

Highest mean error: 6.347427845001221 mm for frame 122

Lowest mean error: 3.737722396850586 mm for frame 233

Saving results

Total time: 70.72863531112671
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_nl_5786/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_nl_5786/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871928
Iteration 2/25 | Loss: 0.00122054
Iteration 3/25 | Loss: 0.00105277
Iteration 4/25 | Loss: 0.00102497
Iteration 5/25 | Loss: 0.00101799
Iteration 6/25 | Loss: 0.00101581
Iteration 7/25 | Loss: 0.00101914
Iteration 8/25 | Loss: 0.00101573
Iteration 9/25 | Loss: 0.00101773
Iteration 10/25 | Loss: 0.00101826
Iteration 11/25 | Loss: 0.00101692
Iteration 12/25 | Loss: 0.00101367
Iteration 13/25 | Loss: 0.00101433
Iteration 14/25 | Loss: 0.00101391
Iteration 15/25 | Loss: 0.00101407
Iteration 16/25 | Loss: 0.00101346
Iteration 17/25 | Loss: 0.00101398
Iteration 18/25 | Loss: 0.00101326
Iteration 19/25 | Loss: 0.00101430
Iteration 20/25 | Loss: 0.00101206
Iteration 21/25 | Loss: 0.00101073
Iteration 22/25 | Loss: 0.00101048
Iteration 23/25 | Loss: 0.00101036
Iteration 24/25 | Loss: 0.00101032
Iteration 25/25 | Loss: 0.00101032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30677772
Iteration 2/25 | Loss: 0.00206610
Iteration 3/25 | Loss: 0.00206609
Iteration 4/25 | Loss: 0.00206609
Iteration 5/25 | Loss: 0.00206609
Iteration 6/25 | Loss: 0.00206609
Iteration 7/25 | Loss: 0.00206609
Iteration 8/25 | Loss: 0.00206609
Iteration 9/25 | Loss: 0.00206609
Iteration 10/25 | Loss: 0.00206609
Iteration 11/25 | Loss: 0.00206609
Iteration 12/25 | Loss: 0.00206609
Iteration 13/25 | Loss: 0.00206609
Iteration 14/25 | Loss: 0.00206609
Iteration 15/25 | Loss: 0.00206609
Iteration 16/25 | Loss: 0.00206609
Iteration 17/25 | Loss: 0.00206609
Iteration 18/25 | Loss: 0.00206609
Iteration 19/25 | Loss: 0.00206609
Iteration 20/25 | Loss: 0.00206609
Iteration 21/25 | Loss: 0.00206609
Iteration 22/25 | Loss: 0.00206609
Iteration 23/25 | Loss: 0.00206609
Iteration 24/25 | Loss: 0.00206609
Iteration 25/25 | Loss: 0.00206609

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206609
Iteration 2/1000 | Loss: 0.00018181
Iteration 3/1000 | Loss: 0.00012105
Iteration 4/1000 | Loss: 0.00010211
Iteration 5/1000 | Loss: 0.00009298
Iteration 6/1000 | Loss: 0.00008671
Iteration 7/1000 | Loss: 0.00029081
Iteration 8/1000 | Loss: 0.00008117
Iteration 9/1000 | Loss: 0.00007590
Iteration 10/1000 | Loss: 0.00007183
Iteration 11/1000 | Loss: 0.00037403
Iteration 12/1000 | Loss: 0.00007014
Iteration 13/1000 | Loss: 0.00006613
Iteration 14/1000 | Loss: 0.00006413
Iteration 15/1000 | Loss: 0.00006249
Iteration 16/1000 | Loss: 0.00115952
Iteration 17/1000 | Loss: 0.00069460
Iteration 18/1000 | Loss: 0.00007239
Iteration 19/1000 | Loss: 0.00266094
Iteration 20/1000 | Loss: 0.00143910
Iteration 21/1000 | Loss: 0.00040764
Iteration 22/1000 | Loss: 0.00015771
Iteration 23/1000 | Loss: 0.00005342
Iteration 24/1000 | Loss: 0.00004941
Iteration 25/1000 | Loss: 0.00003887
Iteration 26/1000 | Loss: 0.00004184
Iteration 27/1000 | Loss: 0.00003205
Iteration 28/1000 | Loss: 0.00002855
Iteration 29/1000 | Loss: 0.00055206
Iteration 30/1000 | Loss: 0.00002983
Iteration 31/1000 | Loss: 0.00002561
Iteration 32/1000 | Loss: 0.00002410
Iteration 33/1000 | Loss: 0.00002271
Iteration 34/1000 | Loss: 0.00002171
Iteration 35/1000 | Loss: 0.00002089
Iteration 36/1000 | Loss: 0.00002036
Iteration 37/1000 | Loss: 0.00001992
Iteration 38/1000 | Loss: 0.00001960
Iteration 39/1000 | Loss: 0.00070955
Iteration 40/1000 | Loss: 0.00011523
Iteration 41/1000 | Loss: 0.00003172
Iteration 42/1000 | Loss: 0.00002467
Iteration 43/1000 | Loss: 0.00002035
Iteration 44/1000 | Loss: 0.00001949
Iteration 45/1000 | Loss: 0.00001923
Iteration 46/1000 | Loss: 0.00001921
Iteration 47/1000 | Loss: 0.00001919
Iteration 48/1000 | Loss: 0.00001918
Iteration 49/1000 | Loss: 0.00001917
Iteration 50/1000 | Loss: 0.00001916
Iteration 51/1000 | Loss: 0.00001914
Iteration 52/1000 | Loss: 0.00001908
Iteration 53/1000 | Loss: 0.00001904
Iteration 54/1000 | Loss: 0.00001904
Iteration 55/1000 | Loss: 0.00001904
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001903
Iteration 59/1000 | Loss: 0.00001902
Iteration 60/1000 | Loss: 0.00001900
Iteration 61/1000 | Loss: 0.00001899
Iteration 62/1000 | Loss: 0.00001898
Iteration 63/1000 | Loss: 0.00001898
Iteration 64/1000 | Loss: 0.00001897
Iteration 65/1000 | Loss: 0.00001897
Iteration 66/1000 | Loss: 0.00001896
Iteration 67/1000 | Loss: 0.00001896
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001895
Iteration 71/1000 | Loss: 0.00001894
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001893
Iteration 75/1000 | Loss: 0.00001893
Iteration 76/1000 | Loss: 0.00001893
Iteration 77/1000 | Loss: 0.00001892
Iteration 78/1000 | Loss: 0.00001890
Iteration 79/1000 | Loss: 0.00001889
Iteration 80/1000 | Loss: 0.00001888
Iteration 81/1000 | Loss: 0.00001888
Iteration 82/1000 | Loss: 0.00001888
Iteration 83/1000 | Loss: 0.00001887
Iteration 84/1000 | Loss: 0.00001887
Iteration 85/1000 | Loss: 0.00001887
Iteration 86/1000 | Loss: 0.00001886
Iteration 87/1000 | Loss: 0.00001886
Iteration 88/1000 | Loss: 0.00001885
Iteration 89/1000 | Loss: 0.00001885
Iteration 90/1000 | Loss: 0.00001884
Iteration 91/1000 | Loss: 0.00001884
Iteration 92/1000 | Loss: 0.00001884
Iteration 93/1000 | Loss: 0.00001884
Iteration 94/1000 | Loss: 0.00001884
Iteration 95/1000 | Loss: 0.00001884
Iteration 96/1000 | Loss: 0.00001884
Iteration 97/1000 | Loss: 0.00001883
Iteration 98/1000 | Loss: 0.00001883
Iteration 99/1000 | Loss: 0.00001883
Iteration 100/1000 | Loss: 0.00001883
Iteration 101/1000 | Loss: 0.00001882
Iteration 102/1000 | Loss: 0.00001882
Iteration 103/1000 | Loss: 0.00001882
Iteration 104/1000 | Loss: 0.00001882
Iteration 105/1000 | Loss: 0.00001882
Iteration 106/1000 | Loss: 0.00001882
Iteration 107/1000 | Loss: 0.00001881
Iteration 108/1000 | Loss: 0.00001881
Iteration 109/1000 | Loss: 0.00001881
Iteration 110/1000 | Loss: 0.00001881
Iteration 111/1000 | Loss: 0.00001881
Iteration 112/1000 | Loss: 0.00001880
Iteration 113/1000 | Loss: 0.00001880
Iteration 114/1000 | Loss: 0.00001880
Iteration 115/1000 | Loss: 0.00001880
Iteration 116/1000 | Loss: 0.00001880
Iteration 117/1000 | Loss: 0.00001880
Iteration 118/1000 | Loss: 0.00001880
Iteration 119/1000 | Loss: 0.00001879
Iteration 120/1000 | Loss: 0.00001879
Iteration 121/1000 | Loss: 0.00001879
Iteration 122/1000 | Loss: 0.00001879
Iteration 123/1000 | Loss: 0.00001879
Iteration 124/1000 | Loss: 0.00001879
Iteration 125/1000 | Loss: 0.00001879
Iteration 126/1000 | Loss: 0.00001879
Iteration 127/1000 | Loss: 0.00001879
Iteration 128/1000 | Loss: 0.00001879
Iteration 129/1000 | Loss: 0.00001878
Iteration 130/1000 | Loss: 0.00001878
Iteration 131/1000 | Loss: 0.00001878
Iteration 132/1000 | Loss: 0.00001878
Iteration 133/1000 | Loss: 0.00001878
Iteration 134/1000 | Loss: 0.00001878
Iteration 135/1000 | Loss: 0.00001878
Iteration 136/1000 | Loss: 0.00001878
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.8784947314998135e-05, 1.8784947314998135e-05, 1.8784947314998135e-05, 1.8784947314998135e-05, 1.8784947314998135e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8784947314998135e-05

Optimization complete. Final v2v error: 3.342743396759033 mm

Highest mean error: 13.518763542175293 mm for frame 81

Lowest mean error: 2.753814220428467 mm for frame 16

Saving results

Total time: 119.6326994895935
