Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=217, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12152-12207
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00374622
Iteration 2/25 | Loss: 0.00116245
Iteration 3/25 | Loss: 0.00108585
Iteration 4/25 | Loss: 0.00107751
Iteration 5/25 | Loss: 0.00107516
Iteration 6/25 | Loss: 0.00107459
Iteration 7/25 | Loss: 0.00107459
Iteration 8/25 | Loss: 0.00107459
Iteration 9/25 | Loss: 0.00107459
Iteration 10/25 | Loss: 0.00107459
Iteration 11/25 | Loss: 0.00107459
Iteration 12/25 | Loss: 0.00107459
Iteration 13/25 | Loss: 0.00107459
Iteration 14/25 | Loss: 0.00107459
Iteration 15/25 | Loss: 0.00107459
Iteration 16/25 | Loss: 0.00107459
Iteration 17/25 | Loss: 0.00107459
Iteration 18/25 | Loss: 0.00107459
Iteration 19/25 | Loss: 0.00107459
Iteration 20/25 | Loss: 0.00107459
Iteration 21/25 | Loss: 0.00107459
Iteration 22/25 | Loss: 0.00107459
Iteration 23/25 | Loss: 0.00107459
Iteration 24/25 | Loss: 0.00107459
Iteration 25/25 | Loss: 0.00107459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001074590370990336, 0.001074590370990336, 0.001074590370990336, 0.001074590370990336, 0.001074590370990336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001074590370990336

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65168476
Iteration 2/25 | Loss: 0.00087662
Iteration 3/25 | Loss: 0.00087662
Iteration 4/25 | Loss: 0.00087662
Iteration 5/25 | Loss: 0.00087662
Iteration 6/25 | Loss: 0.00087662
Iteration 7/25 | Loss: 0.00087662
Iteration 8/25 | Loss: 0.00087662
Iteration 9/25 | Loss: 0.00087662
Iteration 10/25 | Loss: 0.00087662
Iteration 11/25 | Loss: 0.00087662
Iteration 12/25 | Loss: 0.00087662
Iteration 13/25 | Loss: 0.00087662
Iteration 14/25 | Loss: 0.00087662
Iteration 15/25 | Loss: 0.00087662
Iteration 16/25 | Loss: 0.00087662
Iteration 17/25 | Loss: 0.00087662
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008766183746047318, 0.0008766183746047318, 0.0008766183746047318, 0.0008766183746047318, 0.0008766183746047318]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008766183746047318

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087662
Iteration 2/1000 | Loss: 0.00001982
Iteration 3/1000 | Loss: 0.00001232
Iteration 4/1000 | Loss: 0.00001074
Iteration 5/1000 | Loss: 0.00001011
Iteration 6/1000 | Loss: 0.00000976
Iteration 7/1000 | Loss: 0.00000936
Iteration 8/1000 | Loss: 0.00000932
Iteration 9/1000 | Loss: 0.00000928
Iteration 10/1000 | Loss: 0.00000913
Iteration 11/1000 | Loss: 0.00000910
Iteration 12/1000 | Loss: 0.00000903
Iteration 13/1000 | Loss: 0.00000891
Iteration 14/1000 | Loss: 0.00000882
Iteration 15/1000 | Loss: 0.00000882
Iteration 16/1000 | Loss: 0.00000882
Iteration 17/1000 | Loss: 0.00000881
Iteration 18/1000 | Loss: 0.00000880
Iteration 19/1000 | Loss: 0.00000877
Iteration 20/1000 | Loss: 0.00000876
Iteration 21/1000 | Loss: 0.00000872
Iteration 22/1000 | Loss: 0.00000869
Iteration 23/1000 | Loss: 0.00000869
Iteration 24/1000 | Loss: 0.00000868
Iteration 25/1000 | Loss: 0.00000867
Iteration 26/1000 | Loss: 0.00000865
Iteration 27/1000 | Loss: 0.00000864
Iteration 28/1000 | Loss: 0.00000862
Iteration 29/1000 | Loss: 0.00000860
Iteration 30/1000 | Loss: 0.00000860
Iteration 31/1000 | Loss: 0.00000860
Iteration 32/1000 | Loss: 0.00000858
Iteration 33/1000 | Loss: 0.00000857
Iteration 34/1000 | Loss: 0.00000857
Iteration 35/1000 | Loss: 0.00000857
Iteration 36/1000 | Loss: 0.00000857
Iteration 37/1000 | Loss: 0.00000856
Iteration 38/1000 | Loss: 0.00000855
Iteration 39/1000 | Loss: 0.00000855
Iteration 40/1000 | Loss: 0.00000854
Iteration 41/1000 | Loss: 0.00000854
Iteration 42/1000 | Loss: 0.00000854
Iteration 43/1000 | Loss: 0.00000854
Iteration 44/1000 | Loss: 0.00000854
Iteration 45/1000 | Loss: 0.00000854
Iteration 46/1000 | Loss: 0.00000854
Iteration 47/1000 | Loss: 0.00000854
Iteration 48/1000 | Loss: 0.00000854
Iteration 49/1000 | Loss: 0.00000854
Iteration 50/1000 | Loss: 0.00000853
Iteration 51/1000 | Loss: 0.00000853
Iteration 52/1000 | Loss: 0.00000853
Iteration 53/1000 | Loss: 0.00000853
Iteration 54/1000 | Loss: 0.00000853
Iteration 55/1000 | Loss: 0.00000853
Iteration 56/1000 | Loss: 0.00000851
Iteration 57/1000 | Loss: 0.00000851
Iteration 58/1000 | Loss: 0.00000851
Iteration 59/1000 | Loss: 0.00000850
Iteration 60/1000 | Loss: 0.00000850
Iteration 61/1000 | Loss: 0.00000850
Iteration 62/1000 | Loss: 0.00000850
Iteration 63/1000 | Loss: 0.00000849
Iteration 64/1000 | Loss: 0.00000849
Iteration 65/1000 | Loss: 0.00000848
Iteration 66/1000 | Loss: 0.00000848
Iteration 67/1000 | Loss: 0.00000848
Iteration 68/1000 | Loss: 0.00000847
Iteration 69/1000 | Loss: 0.00000847
Iteration 70/1000 | Loss: 0.00000847
Iteration 71/1000 | Loss: 0.00000847
Iteration 72/1000 | Loss: 0.00000847
Iteration 73/1000 | Loss: 0.00000846
Iteration 74/1000 | Loss: 0.00000846
Iteration 75/1000 | Loss: 0.00000845
Iteration 76/1000 | Loss: 0.00000845
Iteration 77/1000 | Loss: 0.00000845
Iteration 78/1000 | Loss: 0.00000845
Iteration 79/1000 | Loss: 0.00000845
Iteration 80/1000 | Loss: 0.00000844
Iteration 81/1000 | Loss: 0.00000844
Iteration 82/1000 | Loss: 0.00000844
Iteration 83/1000 | Loss: 0.00000844
Iteration 84/1000 | Loss: 0.00000844
Iteration 85/1000 | Loss: 0.00000844
Iteration 86/1000 | Loss: 0.00000844
Iteration 87/1000 | Loss: 0.00000844
Iteration 88/1000 | Loss: 0.00000844
Iteration 89/1000 | Loss: 0.00000844
Iteration 90/1000 | Loss: 0.00000844
Iteration 91/1000 | Loss: 0.00000844
Iteration 92/1000 | Loss: 0.00000843
Iteration 93/1000 | Loss: 0.00000843
Iteration 94/1000 | Loss: 0.00000843
Iteration 95/1000 | Loss: 0.00000843
Iteration 96/1000 | Loss: 0.00000842
Iteration 97/1000 | Loss: 0.00000842
Iteration 98/1000 | Loss: 0.00000842
Iteration 99/1000 | Loss: 0.00000842
Iteration 100/1000 | Loss: 0.00000842
Iteration 101/1000 | Loss: 0.00000842
Iteration 102/1000 | Loss: 0.00000841
Iteration 103/1000 | Loss: 0.00000841
Iteration 104/1000 | Loss: 0.00000841
Iteration 105/1000 | Loss: 0.00000841
Iteration 106/1000 | Loss: 0.00000840
Iteration 107/1000 | Loss: 0.00000840
Iteration 108/1000 | Loss: 0.00000840
Iteration 109/1000 | Loss: 0.00000840
Iteration 110/1000 | Loss: 0.00000839
Iteration 111/1000 | Loss: 0.00000839
Iteration 112/1000 | Loss: 0.00000839
Iteration 113/1000 | Loss: 0.00000839
Iteration 114/1000 | Loss: 0.00000839
Iteration 115/1000 | Loss: 0.00000839
Iteration 116/1000 | Loss: 0.00000838
Iteration 117/1000 | Loss: 0.00000838
Iteration 118/1000 | Loss: 0.00000838
Iteration 119/1000 | Loss: 0.00000838
Iteration 120/1000 | Loss: 0.00000838
Iteration 121/1000 | Loss: 0.00000838
Iteration 122/1000 | Loss: 0.00000838
Iteration 123/1000 | Loss: 0.00000838
Iteration 124/1000 | Loss: 0.00000838
Iteration 125/1000 | Loss: 0.00000837
Iteration 126/1000 | Loss: 0.00000837
Iteration 127/1000 | Loss: 0.00000837
Iteration 128/1000 | Loss: 0.00000837
Iteration 129/1000 | Loss: 0.00000837
Iteration 130/1000 | Loss: 0.00000837
Iteration 131/1000 | Loss: 0.00000837
Iteration 132/1000 | Loss: 0.00000836
Iteration 133/1000 | Loss: 0.00000836
Iteration 134/1000 | Loss: 0.00000836
Iteration 135/1000 | Loss: 0.00000836
Iteration 136/1000 | Loss: 0.00000836
Iteration 137/1000 | Loss: 0.00000836
Iteration 138/1000 | Loss: 0.00000836
Iteration 139/1000 | Loss: 0.00000835
Iteration 140/1000 | Loss: 0.00000835
Iteration 141/1000 | Loss: 0.00000835
Iteration 142/1000 | Loss: 0.00000835
Iteration 143/1000 | Loss: 0.00000835
Iteration 144/1000 | Loss: 0.00000835
Iteration 145/1000 | Loss: 0.00000835
Iteration 146/1000 | Loss: 0.00000835
Iteration 147/1000 | Loss: 0.00000835
Iteration 148/1000 | Loss: 0.00000835
Iteration 149/1000 | Loss: 0.00000835
Iteration 150/1000 | Loss: 0.00000834
Iteration 151/1000 | Loss: 0.00000834
Iteration 152/1000 | Loss: 0.00000834
Iteration 153/1000 | Loss: 0.00000834
Iteration 154/1000 | Loss: 0.00000834
Iteration 155/1000 | Loss: 0.00000834
Iteration 156/1000 | Loss: 0.00000834
Iteration 157/1000 | Loss: 0.00000833
Iteration 158/1000 | Loss: 0.00000833
Iteration 159/1000 | Loss: 0.00000833
Iteration 160/1000 | Loss: 0.00000833
Iteration 161/1000 | Loss: 0.00000833
Iteration 162/1000 | Loss: 0.00000833
Iteration 163/1000 | Loss: 0.00000832
Iteration 164/1000 | Loss: 0.00000832
Iteration 165/1000 | Loss: 0.00000832
Iteration 166/1000 | Loss: 0.00000832
Iteration 167/1000 | Loss: 0.00000832
Iteration 168/1000 | Loss: 0.00000831
Iteration 169/1000 | Loss: 0.00000831
Iteration 170/1000 | Loss: 0.00000831
Iteration 171/1000 | Loss: 0.00000831
Iteration 172/1000 | Loss: 0.00000830
Iteration 173/1000 | Loss: 0.00000830
Iteration 174/1000 | Loss: 0.00000830
Iteration 175/1000 | Loss: 0.00000830
Iteration 176/1000 | Loss: 0.00000830
Iteration 177/1000 | Loss: 0.00000830
Iteration 178/1000 | Loss: 0.00000830
Iteration 179/1000 | Loss: 0.00000830
Iteration 180/1000 | Loss: 0.00000830
Iteration 181/1000 | Loss: 0.00000830
Iteration 182/1000 | Loss: 0.00000830
Iteration 183/1000 | Loss: 0.00000830
Iteration 184/1000 | Loss: 0.00000830
Iteration 185/1000 | Loss: 0.00000830
Iteration 186/1000 | Loss: 0.00000830
Iteration 187/1000 | Loss: 0.00000830
Iteration 188/1000 | Loss: 0.00000830
Iteration 189/1000 | Loss: 0.00000830
Iteration 190/1000 | Loss: 0.00000830
Iteration 191/1000 | Loss: 0.00000829
Iteration 192/1000 | Loss: 0.00000829
Iteration 193/1000 | Loss: 0.00000829
Iteration 194/1000 | Loss: 0.00000828
Iteration 195/1000 | Loss: 0.00000828
Iteration 196/1000 | Loss: 0.00000828
Iteration 197/1000 | Loss: 0.00000828
Iteration 198/1000 | Loss: 0.00000828
Iteration 199/1000 | Loss: 0.00000828
Iteration 200/1000 | Loss: 0.00000828
Iteration 201/1000 | Loss: 0.00000828
Iteration 202/1000 | Loss: 0.00000828
Iteration 203/1000 | Loss: 0.00000828
Iteration 204/1000 | Loss: 0.00000827
Iteration 205/1000 | Loss: 0.00000827
Iteration 206/1000 | Loss: 0.00000827
Iteration 207/1000 | Loss: 0.00000826
Iteration 208/1000 | Loss: 0.00000826
Iteration 209/1000 | Loss: 0.00000826
Iteration 210/1000 | Loss: 0.00000826
Iteration 211/1000 | Loss: 0.00000826
Iteration 212/1000 | Loss: 0.00000826
Iteration 213/1000 | Loss: 0.00000826
Iteration 214/1000 | Loss: 0.00000826
Iteration 215/1000 | Loss: 0.00000826
Iteration 216/1000 | Loss: 0.00000826
Iteration 217/1000 | Loss: 0.00000826
Iteration 218/1000 | Loss: 0.00000826
Iteration 219/1000 | Loss: 0.00000826
Iteration 220/1000 | Loss: 0.00000826
Iteration 221/1000 | Loss: 0.00000825
Iteration 222/1000 | Loss: 0.00000825
Iteration 223/1000 | Loss: 0.00000825
Iteration 224/1000 | Loss: 0.00000825
Iteration 225/1000 | Loss: 0.00000825
Iteration 226/1000 | Loss: 0.00000825
Iteration 227/1000 | Loss: 0.00000825
Iteration 228/1000 | Loss: 0.00000824
Iteration 229/1000 | Loss: 0.00000824
Iteration 230/1000 | Loss: 0.00000824
Iteration 231/1000 | Loss: 0.00000824
Iteration 232/1000 | Loss: 0.00000824
Iteration 233/1000 | Loss: 0.00000824
Iteration 234/1000 | Loss: 0.00000824
Iteration 235/1000 | Loss: 0.00000824
Iteration 236/1000 | Loss: 0.00000824
Iteration 237/1000 | Loss: 0.00000824
Iteration 238/1000 | Loss: 0.00000824
Iteration 239/1000 | Loss: 0.00000824
Iteration 240/1000 | Loss: 0.00000824
Iteration 241/1000 | Loss: 0.00000824
Iteration 242/1000 | Loss: 0.00000824
Iteration 243/1000 | Loss: 0.00000824
Iteration 244/1000 | Loss: 0.00000824
Iteration 245/1000 | Loss: 0.00000824
Iteration 246/1000 | Loss: 0.00000824
Iteration 247/1000 | Loss: 0.00000824
Iteration 248/1000 | Loss: 0.00000824
Iteration 249/1000 | Loss: 0.00000824
Iteration 250/1000 | Loss: 0.00000824
Iteration 251/1000 | Loss: 0.00000824
Iteration 252/1000 | Loss: 0.00000824
Iteration 253/1000 | Loss: 0.00000824
Iteration 254/1000 | Loss: 0.00000824
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [8.237871952587739e-06, 8.237871952587739e-06, 8.237871952587739e-06, 8.237871952587739e-06, 8.237871952587739e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.237871952587739e-06

Optimization complete. Final v2v error: 2.4615478515625 mm

Highest mean error: 2.928107261657715 mm for frame 76

Lowest mean error: 2.3603017330169678 mm for frame 57

Saving results

Total time: 43.67254090309143
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01052439
Iteration 2/25 | Loss: 0.00312754
Iteration 3/25 | Loss: 0.00206772
Iteration 4/25 | Loss: 0.00194325
Iteration 5/25 | Loss: 0.00181609
Iteration 6/25 | Loss: 0.00182472
Iteration 7/25 | Loss: 0.00181476
Iteration 8/25 | Loss: 0.00169546
Iteration 9/25 | Loss: 0.00161596
Iteration 10/25 | Loss: 0.00158698
Iteration 11/25 | Loss: 0.00153925
Iteration 12/25 | Loss: 0.00152378
Iteration 13/25 | Loss: 0.00155279
Iteration 14/25 | Loss: 0.00152821
Iteration 15/25 | Loss: 0.00152052
Iteration 16/25 | Loss: 0.00149897
Iteration 17/25 | Loss: 0.00149329
Iteration 18/25 | Loss: 0.00147813
Iteration 19/25 | Loss: 0.00146952
Iteration 20/25 | Loss: 0.00147597
Iteration 21/25 | Loss: 0.00147722
Iteration 22/25 | Loss: 0.00145820
Iteration 23/25 | Loss: 0.00144656
Iteration 24/25 | Loss: 0.00144789
Iteration 25/25 | Loss: 0.00144454

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.08763850
Iteration 2/25 | Loss: 0.00225811
Iteration 3/25 | Loss: 0.00222917
Iteration 4/25 | Loss: 0.00222916
Iteration 5/25 | Loss: 0.00221994
Iteration 6/25 | Loss: 0.00221994
Iteration 7/25 | Loss: 0.00221994
Iteration 8/25 | Loss: 0.00221994
Iteration 9/25 | Loss: 0.00221994
Iteration 10/25 | Loss: 0.00221994
Iteration 11/25 | Loss: 0.00221994
Iteration 12/25 | Loss: 0.00221994
Iteration 13/25 | Loss: 0.00221994
Iteration 14/25 | Loss: 0.00221994
Iteration 15/25 | Loss: 0.00221994
Iteration 16/25 | Loss: 0.00221994
Iteration 17/25 | Loss: 0.00221994
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0022199361119419336, 0.0022199361119419336, 0.0022199361119419336, 0.0022199361119419336, 0.0022199361119419336]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0022199361119419336

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00221994
Iteration 2/1000 | Loss: 0.00078914
Iteration 3/1000 | Loss: 0.00049438
Iteration 4/1000 | Loss: 0.00019572
Iteration 5/1000 | Loss: 0.00025761
Iteration 6/1000 | Loss: 0.00026171
Iteration 7/1000 | Loss: 0.00013533
Iteration 8/1000 | Loss: 0.00037262
Iteration 9/1000 | Loss: 0.00070822
Iteration 10/1000 | Loss: 0.00048071
Iteration 11/1000 | Loss: 0.00019659
Iteration 12/1000 | Loss: 0.00073015
Iteration 13/1000 | Loss: 0.00015843
Iteration 14/1000 | Loss: 0.00013087
Iteration 15/1000 | Loss: 0.00014083
Iteration 16/1000 | Loss: 0.00028533
Iteration 17/1000 | Loss: 0.00062083
Iteration 18/1000 | Loss: 0.00016099
Iteration 19/1000 | Loss: 0.00013965
Iteration 20/1000 | Loss: 0.00012364
Iteration 21/1000 | Loss: 0.00011785
Iteration 22/1000 | Loss: 0.00055939
Iteration 23/1000 | Loss: 0.00033972
Iteration 24/1000 | Loss: 0.00025698
Iteration 25/1000 | Loss: 0.00013140
Iteration 26/1000 | Loss: 0.00019964
Iteration 27/1000 | Loss: 0.00019902
Iteration 28/1000 | Loss: 0.00038585
Iteration 29/1000 | Loss: 0.00013365
Iteration 30/1000 | Loss: 0.00024332
Iteration 31/1000 | Loss: 0.00011244
Iteration 32/1000 | Loss: 0.00015262
Iteration 33/1000 | Loss: 0.00015265
Iteration 34/1000 | Loss: 0.00013346
Iteration 35/1000 | Loss: 0.00015397
Iteration 36/1000 | Loss: 0.00013773
Iteration 37/1000 | Loss: 0.00016701
Iteration 38/1000 | Loss: 0.00017779
Iteration 39/1000 | Loss: 0.00010859
Iteration 40/1000 | Loss: 0.00015099
Iteration 41/1000 | Loss: 0.00051079
Iteration 42/1000 | Loss: 0.00024832
Iteration 43/1000 | Loss: 0.00014707
Iteration 44/1000 | Loss: 0.00011101
Iteration 45/1000 | Loss: 0.00017892
Iteration 46/1000 | Loss: 0.00014264
Iteration 47/1000 | Loss: 0.00015862
Iteration 48/1000 | Loss: 0.00010504
Iteration 49/1000 | Loss: 0.00012739
Iteration 50/1000 | Loss: 0.00014780
Iteration 51/1000 | Loss: 0.00011145
Iteration 52/1000 | Loss: 0.00010279
Iteration 53/1000 | Loss: 0.00012306
Iteration 54/1000 | Loss: 0.00009886
Iteration 55/1000 | Loss: 0.00009679
Iteration 56/1000 | Loss: 0.00011349
Iteration 57/1000 | Loss: 0.00012920
Iteration 58/1000 | Loss: 0.00011060
Iteration 59/1000 | Loss: 0.00009549
Iteration 60/1000 | Loss: 0.00012581
Iteration 61/1000 | Loss: 0.00024729
Iteration 62/1000 | Loss: 0.00012722
Iteration 63/1000 | Loss: 0.00009509
Iteration 64/1000 | Loss: 0.00009457
Iteration 65/1000 | Loss: 0.00009423
Iteration 66/1000 | Loss: 0.00009394
Iteration 67/1000 | Loss: 0.00013465
Iteration 68/1000 | Loss: 0.00011778
Iteration 69/1000 | Loss: 0.00012631
Iteration 70/1000 | Loss: 0.00011104
Iteration 71/1000 | Loss: 0.00009511
Iteration 72/1000 | Loss: 0.00009358
Iteration 73/1000 | Loss: 0.00009353
Iteration 74/1000 | Loss: 0.00009353
Iteration 75/1000 | Loss: 0.00009531
Iteration 76/1000 | Loss: 0.00009349
Iteration 77/1000 | Loss: 0.00009349
Iteration 78/1000 | Loss: 0.00009349
Iteration 79/1000 | Loss: 0.00009348
Iteration 80/1000 | Loss: 0.00009348
Iteration 81/1000 | Loss: 0.00009348
Iteration 82/1000 | Loss: 0.00009348
Iteration 83/1000 | Loss: 0.00009348
Iteration 84/1000 | Loss: 0.00009348
Iteration 85/1000 | Loss: 0.00009348
Iteration 86/1000 | Loss: 0.00009348
Iteration 87/1000 | Loss: 0.00009348
Iteration 88/1000 | Loss: 0.00009348
Iteration 89/1000 | Loss: 0.00016039
Iteration 90/1000 | Loss: 0.00009723
Iteration 91/1000 | Loss: 0.00010061
Iteration 92/1000 | Loss: 0.00009334
Iteration 93/1000 | Loss: 0.00009326
Iteration 94/1000 | Loss: 0.00012408
Iteration 95/1000 | Loss: 0.00019348
Iteration 96/1000 | Loss: 0.00010503
Iteration 97/1000 | Loss: 0.00013775
Iteration 98/1000 | Loss: 0.00011715
Iteration 99/1000 | Loss: 0.00011294
Iteration 100/1000 | Loss: 0.00013314
Iteration 101/1000 | Loss: 0.00011382
Iteration 102/1000 | Loss: 0.00009735
Iteration 103/1000 | Loss: 0.00015851
Iteration 104/1000 | Loss: 0.00010881
Iteration 105/1000 | Loss: 0.00012287
Iteration 106/1000 | Loss: 0.00011538
Iteration 107/1000 | Loss: 0.00012246
Iteration 108/1000 | Loss: 0.00011399
Iteration 109/1000 | Loss: 0.00012699
Iteration 110/1000 | Loss: 0.00011218
Iteration 111/1000 | Loss: 0.00012424
Iteration 112/1000 | Loss: 0.00012880
Iteration 113/1000 | Loss: 0.00012430
Iteration 114/1000 | Loss: 0.00011886
Iteration 115/1000 | Loss: 0.00012231
Iteration 116/1000 | Loss: 0.00009772
Iteration 117/1000 | Loss: 0.00013280
Iteration 118/1000 | Loss: 0.00009410
Iteration 119/1000 | Loss: 0.00009772
Iteration 120/1000 | Loss: 0.00011059
Iteration 121/1000 | Loss: 0.00009324
Iteration 122/1000 | Loss: 0.00009304
Iteration 123/1000 | Loss: 0.00009303
Iteration 124/1000 | Loss: 0.00009302
Iteration 125/1000 | Loss: 0.00009302
Iteration 126/1000 | Loss: 0.00012049
Iteration 127/1000 | Loss: 0.00009294
Iteration 128/1000 | Loss: 0.00009270
Iteration 129/1000 | Loss: 0.00009269
Iteration 130/1000 | Loss: 0.00009269
Iteration 131/1000 | Loss: 0.00009269
Iteration 132/1000 | Loss: 0.00009268
Iteration 133/1000 | Loss: 0.00009268
Iteration 134/1000 | Loss: 0.00009268
Iteration 135/1000 | Loss: 0.00009268
Iteration 136/1000 | Loss: 0.00009268
Iteration 137/1000 | Loss: 0.00009267
Iteration 138/1000 | Loss: 0.00009267
Iteration 139/1000 | Loss: 0.00009267
Iteration 140/1000 | Loss: 0.00009267
Iteration 141/1000 | Loss: 0.00009267
Iteration 142/1000 | Loss: 0.00009267
Iteration 143/1000 | Loss: 0.00009267
Iteration 144/1000 | Loss: 0.00009267
Iteration 145/1000 | Loss: 0.00009266
Iteration 146/1000 | Loss: 0.00009266
Iteration 147/1000 | Loss: 0.00009265
Iteration 148/1000 | Loss: 0.00009265
Iteration 149/1000 | Loss: 0.00009265
Iteration 150/1000 | Loss: 0.00009265
Iteration 151/1000 | Loss: 0.00009264
Iteration 152/1000 | Loss: 0.00009264
Iteration 153/1000 | Loss: 0.00009264
Iteration 154/1000 | Loss: 0.00009263
Iteration 155/1000 | Loss: 0.00009263
Iteration 156/1000 | Loss: 0.00009263
Iteration 157/1000 | Loss: 0.00009263
Iteration 158/1000 | Loss: 0.00009263
Iteration 159/1000 | Loss: 0.00009263
Iteration 160/1000 | Loss: 0.00009262
Iteration 161/1000 | Loss: 0.00009262
Iteration 162/1000 | Loss: 0.00009262
Iteration 163/1000 | Loss: 0.00009262
Iteration 164/1000 | Loss: 0.00009262
Iteration 165/1000 | Loss: 0.00009262
Iteration 166/1000 | Loss: 0.00009262
Iteration 167/1000 | Loss: 0.00009262
Iteration 168/1000 | Loss: 0.00009262
Iteration 169/1000 | Loss: 0.00009262
Iteration 170/1000 | Loss: 0.00009262
Iteration 171/1000 | Loss: 0.00009262
Iteration 172/1000 | Loss: 0.00009262
Iteration 173/1000 | Loss: 0.00009262
Iteration 174/1000 | Loss: 0.00009262
Iteration 175/1000 | Loss: 0.00009262
Iteration 176/1000 | Loss: 0.00009262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [9.261635568691418e-05, 9.261635568691418e-05, 9.261635568691418e-05, 9.261635568691418e-05, 9.261635568691418e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.261635568691418e-05

Optimization complete. Final v2v error: 5.599743366241455 mm

Highest mean error: 11.337362289428711 mm for frame 114

Lowest mean error: 3.5366010665893555 mm for frame 3

Saving results

Total time: 205.10708379745483
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00626210
Iteration 2/25 | Loss: 0.00117168
Iteration 3/25 | Loss: 0.00108731
Iteration 4/25 | Loss: 0.00107671
Iteration 5/25 | Loss: 0.00107289
Iteration 6/25 | Loss: 0.00107211
Iteration 7/25 | Loss: 0.00107211
Iteration 8/25 | Loss: 0.00107211
Iteration 9/25 | Loss: 0.00107211
Iteration 10/25 | Loss: 0.00107211
Iteration 11/25 | Loss: 0.00107211
Iteration 12/25 | Loss: 0.00107211
Iteration 13/25 | Loss: 0.00107211
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.00107210548594594, 0.00107210548594594, 0.00107210548594594, 0.00107210548594594, 0.00107210548594594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00107210548594594

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39362872
Iteration 2/25 | Loss: 0.00083206
Iteration 3/25 | Loss: 0.00083206
Iteration 4/25 | Loss: 0.00083206
Iteration 5/25 | Loss: 0.00083206
Iteration 6/25 | Loss: 0.00083206
Iteration 7/25 | Loss: 0.00083206
Iteration 8/25 | Loss: 0.00083206
Iteration 9/25 | Loss: 0.00083206
Iteration 10/25 | Loss: 0.00083206
Iteration 11/25 | Loss: 0.00083206
Iteration 12/25 | Loss: 0.00083206
Iteration 13/25 | Loss: 0.00083206
Iteration 14/25 | Loss: 0.00083206
Iteration 15/25 | Loss: 0.00083206
Iteration 16/25 | Loss: 0.00083206
Iteration 17/25 | Loss: 0.00083206
Iteration 18/25 | Loss: 0.00083206
Iteration 19/25 | Loss: 0.00083206
Iteration 20/25 | Loss: 0.00083206
Iteration 21/25 | Loss: 0.00083206
Iteration 22/25 | Loss: 0.00083206
Iteration 23/25 | Loss: 0.00083206
Iteration 24/25 | Loss: 0.00083206
Iteration 25/25 | Loss: 0.00083206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083206
Iteration 2/1000 | Loss: 0.00002185
Iteration 3/1000 | Loss: 0.00001336
Iteration 4/1000 | Loss: 0.00001150
Iteration 5/1000 | Loss: 0.00001052
Iteration 6/1000 | Loss: 0.00001009
Iteration 7/1000 | Loss: 0.00000976
Iteration 8/1000 | Loss: 0.00000947
Iteration 9/1000 | Loss: 0.00000934
Iteration 10/1000 | Loss: 0.00000925
Iteration 11/1000 | Loss: 0.00000923
Iteration 12/1000 | Loss: 0.00000912
Iteration 13/1000 | Loss: 0.00000898
Iteration 14/1000 | Loss: 0.00000893
Iteration 15/1000 | Loss: 0.00000893
Iteration 16/1000 | Loss: 0.00000892
Iteration 17/1000 | Loss: 0.00000891
Iteration 18/1000 | Loss: 0.00000891
Iteration 19/1000 | Loss: 0.00000879
Iteration 20/1000 | Loss: 0.00000876
Iteration 21/1000 | Loss: 0.00000876
Iteration 22/1000 | Loss: 0.00000876
Iteration 23/1000 | Loss: 0.00000876
Iteration 24/1000 | Loss: 0.00000876
Iteration 25/1000 | Loss: 0.00000876
Iteration 26/1000 | Loss: 0.00000875
Iteration 27/1000 | Loss: 0.00000873
Iteration 28/1000 | Loss: 0.00000872
Iteration 29/1000 | Loss: 0.00000872
Iteration 30/1000 | Loss: 0.00000871
Iteration 31/1000 | Loss: 0.00000871
Iteration 32/1000 | Loss: 0.00000869
Iteration 33/1000 | Loss: 0.00000869
Iteration 34/1000 | Loss: 0.00000869
Iteration 35/1000 | Loss: 0.00000868
Iteration 36/1000 | Loss: 0.00000868
Iteration 37/1000 | Loss: 0.00000868
Iteration 38/1000 | Loss: 0.00000868
Iteration 39/1000 | Loss: 0.00000867
Iteration 40/1000 | Loss: 0.00000867
Iteration 41/1000 | Loss: 0.00000867
Iteration 42/1000 | Loss: 0.00000867
Iteration 43/1000 | Loss: 0.00000867
Iteration 44/1000 | Loss: 0.00000867
Iteration 45/1000 | Loss: 0.00000866
Iteration 46/1000 | Loss: 0.00000865
Iteration 47/1000 | Loss: 0.00000865
Iteration 48/1000 | Loss: 0.00000864
Iteration 49/1000 | Loss: 0.00000864
Iteration 50/1000 | Loss: 0.00000863
Iteration 51/1000 | Loss: 0.00000863
Iteration 52/1000 | Loss: 0.00000863
Iteration 53/1000 | Loss: 0.00000861
Iteration 54/1000 | Loss: 0.00000861
Iteration 55/1000 | Loss: 0.00000861
Iteration 56/1000 | Loss: 0.00000860
Iteration 57/1000 | Loss: 0.00000859
Iteration 58/1000 | Loss: 0.00000858
Iteration 59/1000 | Loss: 0.00000857
Iteration 60/1000 | Loss: 0.00000857
Iteration 61/1000 | Loss: 0.00000856
Iteration 62/1000 | Loss: 0.00000855
Iteration 63/1000 | Loss: 0.00000855
Iteration 64/1000 | Loss: 0.00000855
Iteration 65/1000 | Loss: 0.00000855
Iteration 66/1000 | Loss: 0.00000855
Iteration 67/1000 | Loss: 0.00000855
Iteration 68/1000 | Loss: 0.00000855
Iteration 69/1000 | Loss: 0.00000854
Iteration 70/1000 | Loss: 0.00000854
Iteration 71/1000 | Loss: 0.00000854
Iteration 72/1000 | Loss: 0.00000854
Iteration 73/1000 | Loss: 0.00000854
Iteration 74/1000 | Loss: 0.00000854
Iteration 75/1000 | Loss: 0.00000854
Iteration 76/1000 | Loss: 0.00000853
Iteration 77/1000 | Loss: 0.00000853
Iteration 78/1000 | Loss: 0.00000853
Iteration 79/1000 | Loss: 0.00000853
Iteration 80/1000 | Loss: 0.00000853
Iteration 81/1000 | Loss: 0.00000853
Iteration 82/1000 | Loss: 0.00000853
Iteration 83/1000 | Loss: 0.00000853
Iteration 84/1000 | Loss: 0.00000853
Iteration 85/1000 | Loss: 0.00000853
Iteration 86/1000 | Loss: 0.00000853
Iteration 87/1000 | Loss: 0.00000853
Iteration 88/1000 | Loss: 0.00000853
Iteration 89/1000 | Loss: 0.00000853
Iteration 90/1000 | Loss: 0.00000853
Iteration 91/1000 | Loss: 0.00000853
Iteration 92/1000 | Loss: 0.00000853
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [8.529916158295237e-06, 8.529916158295237e-06, 8.529916158295237e-06, 8.529916158295237e-06, 8.529916158295237e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.529916158295237e-06

Optimization complete. Final v2v error: 2.5111093521118164 mm

Highest mean error: 3.0943617820739746 mm for frame 80

Lowest mean error: 2.3383231163024902 mm for frame 140

Saving results

Total time: 31.395958423614502
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00878191
Iteration 2/25 | Loss: 0.00118434
Iteration 3/25 | Loss: 0.00110366
Iteration 4/25 | Loss: 0.00109346
Iteration 5/25 | Loss: 0.00109068
Iteration 6/25 | Loss: 0.00109037
Iteration 7/25 | Loss: 0.00109037
Iteration 8/25 | Loss: 0.00109037
Iteration 9/25 | Loss: 0.00109037
Iteration 10/25 | Loss: 0.00109037
Iteration 11/25 | Loss: 0.00109037
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010903694201260805, 0.0010903694201260805, 0.0010903694201260805, 0.0010903694201260805, 0.0010903694201260805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010903694201260805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.92502189
Iteration 2/25 | Loss: 0.00088939
Iteration 3/25 | Loss: 0.00088938
Iteration 4/25 | Loss: 0.00088938
Iteration 5/25 | Loss: 0.00088938
Iteration 6/25 | Loss: 0.00088938
Iteration 7/25 | Loss: 0.00088938
Iteration 8/25 | Loss: 0.00088938
Iteration 9/25 | Loss: 0.00088938
Iteration 10/25 | Loss: 0.00088938
Iteration 11/25 | Loss: 0.00088938
Iteration 12/25 | Loss: 0.00088938
Iteration 13/25 | Loss: 0.00088938
Iteration 14/25 | Loss: 0.00088938
Iteration 15/25 | Loss: 0.00088938
Iteration 16/25 | Loss: 0.00088938
Iteration 17/25 | Loss: 0.00088938
Iteration 18/25 | Loss: 0.00088938
Iteration 19/25 | Loss: 0.00088938
Iteration 20/25 | Loss: 0.00088938
Iteration 21/25 | Loss: 0.00088938
Iteration 22/25 | Loss: 0.00088938
Iteration 23/25 | Loss: 0.00088938
Iteration 24/25 | Loss: 0.00088938
Iteration 25/25 | Loss: 0.00088938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088938
Iteration 2/1000 | Loss: 0.00002830
Iteration 3/1000 | Loss: 0.00001677
Iteration 4/1000 | Loss: 0.00001263
Iteration 5/1000 | Loss: 0.00001160
Iteration 6/1000 | Loss: 0.00001090
Iteration 7/1000 | Loss: 0.00001045
Iteration 8/1000 | Loss: 0.00001004
Iteration 9/1000 | Loss: 0.00000985
Iteration 10/1000 | Loss: 0.00000973
Iteration 11/1000 | Loss: 0.00000963
Iteration 12/1000 | Loss: 0.00000963
Iteration 13/1000 | Loss: 0.00000949
Iteration 14/1000 | Loss: 0.00000949
Iteration 15/1000 | Loss: 0.00000946
Iteration 16/1000 | Loss: 0.00000945
Iteration 17/1000 | Loss: 0.00000944
Iteration 18/1000 | Loss: 0.00000944
Iteration 19/1000 | Loss: 0.00000943
Iteration 20/1000 | Loss: 0.00000942
Iteration 21/1000 | Loss: 0.00000940
Iteration 22/1000 | Loss: 0.00000940
Iteration 23/1000 | Loss: 0.00000939
Iteration 24/1000 | Loss: 0.00000937
Iteration 25/1000 | Loss: 0.00000937
Iteration 26/1000 | Loss: 0.00000934
Iteration 27/1000 | Loss: 0.00000932
Iteration 28/1000 | Loss: 0.00000930
Iteration 29/1000 | Loss: 0.00000929
Iteration 30/1000 | Loss: 0.00000929
Iteration 31/1000 | Loss: 0.00000924
Iteration 32/1000 | Loss: 0.00000924
Iteration 33/1000 | Loss: 0.00000924
Iteration 34/1000 | Loss: 0.00000924
Iteration 35/1000 | Loss: 0.00000924
Iteration 36/1000 | Loss: 0.00000924
Iteration 37/1000 | Loss: 0.00000920
Iteration 38/1000 | Loss: 0.00000920
Iteration 39/1000 | Loss: 0.00000920
Iteration 40/1000 | Loss: 0.00000920
Iteration 41/1000 | Loss: 0.00000920
Iteration 42/1000 | Loss: 0.00000920
Iteration 43/1000 | Loss: 0.00000918
Iteration 44/1000 | Loss: 0.00000918
Iteration 45/1000 | Loss: 0.00000916
Iteration 46/1000 | Loss: 0.00000916
Iteration 47/1000 | Loss: 0.00000915
Iteration 48/1000 | Loss: 0.00000915
Iteration 49/1000 | Loss: 0.00000914
Iteration 50/1000 | Loss: 0.00000914
Iteration 51/1000 | Loss: 0.00000914
Iteration 52/1000 | Loss: 0.00000913
Iteration 53/1000 | Loss: 0.00000913
Iteration 54/1000 | Loss: 0.00000912
Iteration 55/1000 | Loss: 0.00000912
Iteration 56/1000 | Loss: 0.00000912
Iteration 57/1000 | Loss: 0.00000911
Iteration 58/1000 | Loss: 0.00000911
Iteration 59/1000 | Loss: 0.00000911
Iteration 60/1000 | Loss: 0.00000911
Iteration 61/1000 | Loss: 0.00000910
Iteration 62/1000 | Loss: 0.00000910
Iteration 63/1000 | Loss: 0.00000910
Iteration 64/1000 | Loss: 0.00000910
Iteration 65/1000 | Loss: 0.00000909
Iteration 66/1000 | Loss: 0.00000909
Iteration 67/1000 | Loss: 0.00000909
Iteration 68/1000 | Loss: 0.00000909
Iteration 69/1000 | Loss: 0.00000909
Iteration 70/1000 | Loss: 0.00000908
Iteration 71/1000 | Loss: 0.00000908
Iteration 72/1000 | Loss: 0.00000908
Iteration 73/1000 | Loss: 0.00000908
Iteration 74/1000 | Loss: 0.00000907
Iteration 75/1000 | Loss: 0.00000907
Iteration 76/1000 | Loss: 0.00000907
Iteration 77/1000 | Loss: 0.00000907
Iteration 78/1000 | Loss: 0.00000907
Iteration 79/1000 | Loss: 0.00000907
Iteration 80/1000 | Loss: 0.00000906
Iteration 81/1000 | Loss: 0.00000906
Iteration 82/1000 | Loss: 0.00000906
Iteration 83/1000 | Loss: 0.00000906
Iteration 84/1000 | Loss: 0.00000905
Iteration 85/1000 | Loss: 0.00000905
Iteration 86/1000 | Loss: 0.00000905
Iteration 87/1000 | Loss: 0.00000905
Iteration 88/1000 | Loss: 0.00000904
Iteration 89/1000 | Loss: 0.00000904
Iteration 90/1000 | Loss: 0.00000904
Iteration 91/1000 | Loss: 0.00000904
Iteration 92/1000 | Loss: 0.00000904
Iteration 93/1000 | Loss: 0.00000904
Iteration 94/1000 | Loss: 0.00000904
Iteration 95/1000 | Loss: 0.00000904
Iteration 96/1000 | Loss: 0.00000904
Iteration 97/1000 | Loss: 0.00000904
Iteration 98/1000 | Loss: 0.00000903
Iteration 99/1000 | Loss: 0.00000903
Iteration 100/1000 | Loss: 0.00000903
Iteration 101/1000 | Loss: 0.00000903
Iteration 102/1000 | Loss: 0.00000902
Iteration 103/1000 | Loss: 0.00000902
Iteration 104/1000 | Loss: 0.00000901
Iteration 105/1000 | Loss: 0.00000901
Iteration 106/1000 | Loss: 0.00000901
Iteration 107/1000 | Loss: 0.00000900
Iteration 108/1000 | Loss: 0.00000899
Iteration 109/1000 | Loss: 0.00000899
Iteration 110/1000 | Loss: 0.00000899
Iteration 111/1000 | Loss: 0.00000899
Iteration 112/1000 | Loss: 0.00000898
Iteration 113/1000 | Loss: 0.00000898
Iteration 114/1000 | Loss: 0.00000898
Iteration 115/1000 | Loss: 0.00000897
Iteration 116/1000 | Loss: 0.00000897
Iteration 117/1000 | Loss: 0.00000897
Iteration 118/1000 | Loss: 0.00000896
Iteration 119/1000 | Loss: 0.00000896
Iteration 120/1000 | Loss: 0.00000894
Iteration 121/1000 | Loss: 0.00000894
Iteration 122/1000 | Loss: 0.00000894
Iteration 123/1000 | Loss: 0.00000894
Iteration 124/1000 | Loss: 0.00000894
Iteration 125/1000 | Loss: 0.00000894
Iteration 126/1000 | Loss: 0.00000894
Iteration 127/1000 | Loss: 0.00000894
Iteration 128/1000 | Loss: 0.00000894
Iteration 129/1000 | Loss: 0.00000893
Iteration 130/1000 | Loss: 0.00000893
Iteration 131/1000 | Loss: 0.00000893
Iteration 132/1000 | Loss: 0.00000892
Iteration 133/1000 | Loss: 0.00000892
Iteration 134/1000 | Loss: 0.00000891
Iteration 135/1000 | Loss: 0.00000891
Iteration 136/1000 | Loss: 0.00000891
Iteration 137/1000 | Loss: 0.00000891
Iteration 138/1000 | Loss: 0.00000891
Iteration 139/1000 | Loss: 0.00000891
Iteration 140/1000 | Loss: 0.00000891
Iteration 141/1000 | Loss: 0.00000891
Iteration 142/1000 | Loss: 0.00000891
Iteration 143/1000 | Loss: 0.00000890
Iteration 144/1000 | Loss: 0.00000890
Iteration 145/1000 | Loss: 0.00000890
Iteration 146/1000 | Loss: 0.00000890
Iteration 147/1000 | Loss: 0.00000890
Iteration 148/1000 | Loss: 0.00000890
Iteration 149/1000 | Loss: 0.00000890
Iteration 150/1000 | Loss: 0.00000890
Iteration 151/1000 | Loss: 0.00000890
Iteration 152/1000 | Loss: 0.00000890
Iteration 153/1000 | Loss: 0.00000889
Iteration 154/1000 | Loss: 0.00000889
Iteration 155/1000 | Loss: 0.00000889
Iteration 156/1000 | Loss: 0.00000889
Iteration 157/1000 | Loss: 0.00000888
Iteration 158/1000 | Loss: 0.00000888
Iteration 159/1000 | Loss: 0.00000888
Iteration 160/1000 | Loss: 0.00000888
Iteration 161/1000 | Loss: 0.00000887
Iteration 162/1000 | Loss: 0.00000887
Iteration 163/1000 | Loss: 0.00000887
Iteration 164/1000 | Loss: 0.00000887
Iteration 165/1000 | Loss: 0.00000887
Iteration 166/1000 | Loss: 0.00000887
Iteration 167/1000 | Loss: 0.00000886
Iteration 168/1000 | Loss: 0.00000886
Iteration 169/1000 | Loss: 0.00000886
Iteration 170/1000 | Loss: 0.00000886
Iteration 171/1000 | Loss: 0.00000886
Iteration 172/1000 | Loss: 0.00000886
Iteration 173/1000 | Loss: 0.00000885
Iteration 174/1000 | Loss: 0.00000885
Iteration 175/1000 | Loss: 0.00000885
Iteration 176/1000 | Loss: 0.00000884
Iteration 177/1000 | Loss: 0.00000884
Iteration 178/1000 | Loss: 0.00000884
Iteration 179/1000 | Loss: 0.00000883
Iteration 180/1000 | Loss: 0.00000883
Iteration 181/1000 | Loss: 0.00000883
Iteration 182/1000 | Loss: 0.00000883
Iteration 183/1000 | Loss: 0.00000883
Iteration 184/1000 | Loss: 0.00000883
Iteration 185/1000 | Loss: 0.00000882
Iteration 186/1000 | Loss: 0.00000882
Iteration 187/1000 | Loss: 0.00000882
Iteration 188/1000 | Loss: 0.00000882
Iteration 189/1000 | Loss: 0.00000882
Iteration 190/1000 | Loss: 0.00000882
Iteration 191/1000 | Loss: 0.00000882
Iteration 192/1000 | Loss: 0.00000882
Iteration 193/1000 | Loss: 0.00000882
Iteration 194/1000 | Loss: 0.00000882
Iteration 195/1000 | Loss: 0.00000882
Iteration 196/1000 | Loss: 0.00000881
Iteration 197/1000 | Loss: 0.00000881
Iteration 198/1000 | Loss: 0.00000881
Iteration 199/1000 | Loss: 0.00000881
Iteration 200/1000 | Loss: 0.00000881
Iteration 201/1000 | Loss: 0.00000881
Iteration 202/1000 | Loss: 0.00000881
Iteration 203/1000 | Loss: 0.00000881
Iteration 204/1000 | Loss: 0.00000881
Iteration 205/1000 | Loss: 0.00000881
Iteration 206/1000 | Loss: 0.00000881
Iteration 207/1000 | Loss: 0.00000881
Iteration 208/1000 | Loss: 0.00000881
Iteration 209/1000 | Loss: 0.00000881
Iteration 210/1000 | Loss: 0.00000881
Iteration 211/1000 | Loss: 0.00000881
Iteration 212/1000 | Loss: 0.00000881
Iteration 213/1000 | Loss: 0.00000881
Iteration 214/1000 | Loss: 0.00000881
Iteration 215/1000 | Loss: 0.00000881
Iteration 216/1000 | Loss: 0.00000881
Iteration 217/1000 | Loss: 0.00000881
Iteration 218/1000 | Loss: 0.00000881
Iteration 219/1000 | Loss: 0.00000881
Iteration 220/1000 | Loss: 0.00000881
Iteration 221/1000 | Loss: 0.00000881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [8.807505764707457e-06, 8.807505764707457e-06, 8.807505764707457e-06, 8.807505764707457e-06, 8.807505764707457e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.807505764707457e-06

Optimization complete. Final v2v error: 2.5604636669158936 mm

Highest mean error: 3.0423693656921387 mm for frame 117

Lowest mean error: 2.3640170097351074 mm for frame 88

Saving results

Total time: 41.65082287788391
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817323
Iteration 2/25 | Loss: 0.00118449
Iteration 3/25 | Loss: 0.00107585
Iteration 4/25 | Loss: 0.00106607
Iteration 5/25 | Loss: 0.00106408
Iteration 6/25 | Loss: 0.00106408
Iteration 7/25 | Loss: 0.00106408
Iteration 8/25 | Loss: 0.00106408
Iteration 9/25 | Loss: 0.00106408
Iteration 10/25 | Loss: 0.00106408
Iteration 11/25 | Loss: 0.00106408
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010640849359333515, 0.0010640849359333515, 0.0010640849359333515, 0.0010640849359333515, 0.0010640849359333515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010640849359333515

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34623778
Iteration 2/25 | Loss: 0.00075507
Iteration 3/25 | Loss: 0.00075506
Iteration 4/25 | Loss: 0.00075506
Iteration 5/25 | Loss: 0.00075506
Iteration 6/25 | Loss: 0.00075506
Iteration 7/25 | Loss: 0.00075506
Iteration 8/25 | Loss: 0.00075506
Iteration 9/25 | Loss: 0.00075506
Iteration 10/25 | Loss: 0.00075506
Iteration 11/25 | Loss: 0.00075506
Iteration 12/25 | Loss: 0.00075506
Iteration 13/25 | Loss: 0.00075506
Iteration 14/25 | Loss: 0.00075506
Iteration 15/25 | Loss: 0.00075506
Iteration 16/25 | Loss: 0.00075506
Iteration 17/25 | Loss: 0.00075506
Iteration 18/25 | Loss: 0.00075506
Iteration 19/25 | Loss: 0.00075506
Iteration 20/25 | Loss: 0.00075506
Iteration 21/25 | Loss: 0.00075506
Iteration 22/25 | Loss: 0.00075506
Iteration 23/25 | Loss: 0.00075506
Iteration 24/25 | Loss: 0.00075506
Iteration 25/25 | Loss: 0.00075506

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075506
Iteration 2/1000 | Loss: 0.00001759
Iteration 3/1000 | Loss: 0.00001278
Iteration 4/1000 | Loss: 0.00001170
Iteration 5/1000 | Loss: 0.00001090
Iteration 6/1000 | Loss: 0.00001046
Iteration 7/1000 | Loss: 0.00001017
Iteration 8/1000 | Loss: 0.00000989
Iteration 9/1000 | Loss: 0.00000961
Iteration 10/1000 | Loss: 0.00000948
Iteration 11/1000 | Loss: 0.00000947
Iteration 12/1000 | Loss: 0.00000945
Iteration 13/1000 | Loss: 0.00000944
Iteration 14/1000 | Loss: 0.00000942
Iteration 15/1000 | Loss: 0.00000941
Iteration 16/1000 | Loss: 0.00000941
Iteration 17/1000 | Loss: 0.00000941
Iteration 18/1000 | Loss: 0.00000941
Iteration 19/1000 | Loss: 0.00000940
Iteration 20/1000 | Loss: 0.00000940
Iteration 21/1000 | Loss: 0.00000940
Iteration 22/1000 | Loss: 0.00000937
Iteration 23/1000 | Loss: 0.00000936
Iteration 24/1000 | Loss: 0.00000935
Iteration 25/1000 | Loss: 0.00000935
Iteration 26/1000 | Loss: 0.00000935
Iteration 27/1000 | Loss: 0.00000935
Iteration 28/1000 | Loss: 0.00000934
Iteration 29/1000 | Loss: 0.00000934
Iteration 30/1000 | Loss: 0.00000933
Iteration 31/1000 | Loss: 0.00000933
Iteration 32/1000 | Loss: 0.00000932
Iteration 33/1000 | Loss: 0.00000931
Iteration 34/1000 | Loss: 0.00000931
Iteration 35/1000 | Loss: 0.00000930
Iteration 36/1000 | Loss: 0.00000930
Iteration 37/1000 | Loss: 0.00000930
Iteration 38/1000 | Loss: 0.00000930
Iteration 39/1000 | Loss: 0.00000929
Iteration 40/1000 | Loss: 0.00000927
Iteration 41/1000 | Loss: 0.00000927
Iteration 42/1000 | Loss: 0.00000925
Iteration 43/1000 | Loss: 0.00000925
Iteration 44/1000 | Loss: 0.00000921
Iteration 45/1000 | Loss: 0.00000921
Iteration 46/1000 | Loss: 0.00000921
Iteration 47/1000 | Loss: 0.00000920
Iteration 48/1000 | Loss: 0.00000920
Iteration 49/1000 | Loss: 0.00000920
Iteration 50/1000 | Loss: 0.00000920
Iteration 51/1000 | Loss: 0.00000920
Iteration 52/1000 | Loss: 0.00000920
Iteration 53/1000 | Loss: 0.00000916
Iteration 54/1000 | Loss: 0.00000916
Iteration 55/1000 | Loss: 0.00000915
Iteration 56/1000 | Loss: 0.00000915
Iteration 57/1000 | Loss: 0.00000914
Iteration 58/1000 | Loss: 0.00000913
Iteration 59/1000 | Loss: 0.00000913
Iteration 60/1000 | Loss: 0.00000912
Iteration 61/1000 | Loss: 0.00000908
Iteration 62/1000 | Loss: 0.00000907
Iteration 63/1000 | Loss: 0.00000906
Iteration 64/1000 | Loss: 0.00000903
Iteration 65/1000 | Loss: 0.00000902
Iteration 66/1000 | Loss: 0.00000902
Iteration 67/1000 | Loss: 0.00000902
Iteration 68/1000 | Loss: 0.00000902
Iteration 69/1000 | Loss: 0.00000902
Iteration 70/1000 | Loss: 0.00000902
Iteration 71/1000 | Loss: 0.00000902
Iteration 72/1000 | Loss: 0.00000902
Iteration 73/1000 | Loss: 0.00000902
Iteration 74/1000 | Loss: 0.00000901
Iteration 75/1000 | Loss: 0.00000901
Iteration 76/1000 | Loss: 0.00000901
Iteration 77/1000 | Loss: 0.00000901
Iteration 78/1000 | Loss: 0.00000900
Iteration 79/1000 | Loss: 0.00000900
Iteration 80/1000 | Loss: 0.00000899
Iteration 81/1000 | Loss: 0.00000899
Iteration 82/1000 | Loss: 0.00000898
Iteration 83/1000 | Loss: 0.00000898
Iteration 84/1000 | Loss: 0.00000897
Iteration 85/1000 | Loss: 0.00000897
Iteration 86/1000 | Loss: 0.00000896
Iteration 87/1000 | Loss: 0.00000896
Iteration 88/1000 | Loss: 0.00000896
Iteration 89/1000 | Loss: 0.00000895
Iteration 90/1000 | Loss: 0.00000894
Iteration 91/1000 | Loss: 0.00000894
Iteration 92/1000 | Loss: 0.00000894
Iteration 93/1000 | Loss: 0.00000893
Iteration 94/1000 | Loss: 0.00000893
Iteration 95/1000 | Loss: 0.00000893
Iteration 96/1000 | Loss: 0.00000891
Iteration 97/1000 | Loss: 0.00000891
Iteration 98/1000 | Loss: 0.00000891
Iteration 99/1000 | Loss: 0.00000891
Iteration 100/1000 | Loss: 0.00000891
Iteration 101/1000 | Loss: 0.00000891
Iteration 102/1000 | Loss: 0.00000891
Iteration 103/1000 | Loss: 0.00000890
Iteration 104/1000 | Loss: 0.00000890
Iteration 105/1000 | Loss: 0.00000890
Iteration 106/1000 | Loss: 0.00000889
Iteration 107/1000 | Loss: 0.00000889
Iteration 108/1000 | Loss: 0.00000888
Iteration 109/1000 | Loss: 0.00000888
Iteration 110/1000 | Loss: 0.00000888
Iteration 111/1000 | Loss: 0.00000887
Iteration 112/1000 | Loss: 0.00000887
Iteration 113/1000 | Loss: 0.00000887
Iteration 114/1000 | Loss: 0.00000887
Iteration 115/1000 | Loss: 0.00000886
Iteration 116/1000 | Loss: 0.00000886
Iteration 117/1000 | Loss: 0.00000886
Iteration 118/1000 | Loss: 0.00000885
Iteration 119/1000 | Loss: 0.00000884
Iteration 120/1000 | Loss: 0.00000884
Iteration 121/1000 | Loss: 0.00000884
Iteration 122/1000 | Loss: 0.00000884
Iteration 123/1000 | Loss: 0.00000884
Iteration 124/1000 | Loss: 0.00000884
Iteration 125/1000 | Loss: 0.00000884
Iteration 126/1000 | Loss: 0.00000884
Iteration 127/1000 | Loss: 0.00000884
Iteration 128/1000 | Loss: 0.00000884
Iteration 129/1000 | Loss: 0.00000884
Iteration 130/1000 | Loss: 0.00000884
Iteration 131/1000 | Loss: 0.00000883
Iteration 132/1000 | Loss: 0.00000883
Iteration 133/1000 | Loss: 0.00000883
Iteration 134/1000 | Loss: 0.00000883
Iteration 135/1000 | Loss: 0.00000883
Iteration 136/1000 | Loss: 0.00000883
Iteration 137/1000 | Loss: 0.00000882
Iteration 138/1000 | Loss: 0.00000882
Iteration 139/1000 | Loss: 0.00000882
Iteration 140/1000 | Loss: 0.00000882
Iteration 141/1000 | Loss: 0.00000882
Iteration 142/1000 | Loss: 0.00000881
Iteration 143/1000 | Loss: 0.00000881
Iteration 144/1000 | Loss: 0.00000881
Iteration 145/1000 | Loss: 0.00000881
Iteration 146/1000 | Loss: 0.00000881
Iteration 147/1000 | Loss: 0.00000880
Iteration 148/1000 | Loss: 0.00000880
Iteration 149/1000 | Loss: 0.00000880
Iteration 150/1000 | Loss: 0.00000878
Iteration 151/1000 | Loss: 0.00000878
Iteration 152/1000 | Loss: 0.00000878
Iteration 153/1000 | Loss: 0.00000877
Iteration 154/1000 | Loss: 0.00000877
Iteration 155/1000 | Loss: 0.00000877
Iteration 156/1000 | Loss: 0.00000877
Iteration 157/1000 | Loss: 0.00000876
Iteration 158/1000 | Loss: 0.00000875
Iteration 159/1000 | Loss: 0.00000875
Iteration 160/1000 | Loss: 0.00000875
Iteration 161/1000 | Loss: 0.00000874
Iteration 162/1000 | Loss: 0.00000874
Iteration 163/1000 | Loss: 0.00000873
Iteration 164/1000 | Loss: 0.00000873
Iteration 165/1000 | Loss: 0.00000873
Iteration 166/1000 | Loss: 0.00000873
Iteration 167/1000 | Loss: 0.00000873
Iteration 168/1000 | Loss: 0.00000872
Iteration 169/1000 | Loss: 0.00000872
Iteration 170/1000 | Loss: 0.00000872
Iteration 171/1000 | Loss: 0.00000872
Iteration 172/1000 | Loss: 0.00000871
Iteration 173/1000 | Loss: 0.00000871
Iteration 174/1000 | Loss: 0.00000871
Iteration 175/1000 | Loss: 0.00000871
Iteration 176/1000 | Loss: 0.00000870
Iteration 177/1000 | Loss: 0.00000870
Iteration 178/1000 | Loss: 0.00000870
Iteration 179/1000 | Loss: 0.00000870
Iteration 180/1000 | Loss: 0.00000870
Iteration 181/1000 | Loss: 0.00000870
Iteration 182/1000 | Loss: 0.00000870
Iteration 183/1000 | Loss: 0.00000870
Iteration 184/1000 | Loss: 0.00000870
Iteration 185/1000 | Loss: 0.00000870
Iteration 186/1000 | Loss: 0.00000870
Iteration 187/1000 | Loss: 0.00000870
Iteration 188/1000 | Loss: 0.00000870
Iteration 189/1000 | Loss: 0.00000869
Iteration 190/1000 | Loss: 0.00000869
Iteration 191/1000 | Loss: 0.00000869
Iteration 192/1000 | Loss: 0.00000869
Iteration 193/1000 | Loss: 0.00000869
Iteration 194/1000 | Loss: 0.00000869
Iteration 195/1000 | Loss: 0.00000868
Iteration 196/1000 | Loss: 0.00000868
Iteration 197/1000 | Loss: 0.00000868
Iteration 198/1000 | Loss: 0.00000868
Iteration 199/1000 | Loss: 0.00000868
Iteration 200/1000 | Loss: 0.00000868
Iteration 201/1000 | Loss: 0.00000868
Iteration 202/1000 | Loss: 0.00000868
Iteration 203/1000 | Loss: 0.00000867
Iteration 204/1000 | Loss: 0.00000867
Iteration 205/1000 | Loss: 0.00000867
Iteration 206/1000 | Loss: 0.00000867
Iteration 207/1000 | Loss: 0.00000867
Iteration 208/1000 | Loss: 0.00000867
Iteration 209/1000 | Loss: 0.00000867
Iteration 210/1000 | Loss: 0.00000867
Iteration 211/1000 | Loss: 0.00000867
Iteration 212/1000 | Loss: 0.00000867
Iteration 213/1000 | Loss: 0.00000867
Iteration 214/1000 | Loss: 0.00000867
Iteration 215/1000 | Loss: 0.00000866
Iteration 216/1000 | Loss: 0.00000866
Iteration 217/1000 | Loss: 0.00000866
Iteration 218/1000 | Loss: 0.00000866
Iteration 219/1000 | Loss: 0.00000866
Iteration 220/1000 | Loss: 0.00000866
Iteration 221/1000 | Loss: 0.00000866
Iteration 222/1000 | Loss: 0.00000866
Iteration 223/1000 | Loss: 0.00000866
Iteration 224/1000 | Loss: 0.00000866
Iteration 225/1000 | Loss: 0.00000866
Iteration 226/1000 | Loss: 0.00000866
Iteration 227/1000 | Loss: 0.00000866
Iteration 228/1000 | Loss: 0.00000866
Iteration 229/1000 | Loss: 0.00000865
Iteration 230/1000 | Loss: 0.00000865
Iteration 231/1000 | Loss: 0.00000865
Iteration 232/1000 | Loss: 0.00000865
Iteration 233/1000 | Loss: 0.00000865
Iteration 234/1000 | Loss: 0.00000865
Iteration 235/1000 | Loss: 0.00000865
Iteration 236/1000 | Loss: 0.00000865
Iteration 237/1000 | Loss: 0.00000865
Iteration 238/1000 | Loss: 0.00000865
Iteration 239/1000 | Loss: 0.00000865
Iteration 240/1000 | Loss: 0.00000865
Iteration 241/1000 | Loss: 0.00000865
Iteration 242/1000 | Loss: 0.00000865
Iteration 243/1000 | Loss: 0.00000865
Iteration 244/1000 | Loss: 0.00000865
Iteration 245/1000 | Loss: 0.00000865
Iteration 246/1000 | Loss: 0.00000865
Iteration 247/1000 | Loss: 0.00000865
Iteration 248/1000 | Loss: 0.00000865
Iteration 249/1000 | Loss: 0.00000865
Iteration 250/1000 | Loss: 0.00000865
Iteration 251/1000 | Loss: 0.00000865
Iteration 252/1000 | Loss: 0.00000865
Iteration 253/1000 | Loss: 0.00000865
Iteration 254/1000 | Loss: 0.00000865
Iteration 255/1000 | Loss: 0.00000865
Iteration 256/1000 | Loss: 0.00000865
Iteration 257/1000 | Loss: 0.00000865
Iteration 258/1000 | Loss: 0.00000865
Iteration 259/1000 | Loss: 0.00000865
Iteration 260/1000 | Loss: 0.00000865
Iteration 261/1000 | Loss: 0.00000865
Iteration 262/1000 | Loss: 0.00000865
Iteration 263/1000 | Loss: 0.00000865
Iteration 264/1000 | Loss: 0.00000865
Iteration 265/1000 | Loss: 0.00000865
Iteration 266/1000 | Loss: 0.00000865
Iteration 267/1000 | Loss: 0.00000865
Iteration 268/1000 | Loss: 0.00000865
Iteration 269/1000 | Loss: 0.00000865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [8.6469617599505e-06, 8.6469617599505e-06, 8.6469617599505e-06, 8.6469617599505e-06, 8.6469617599505e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.6469617599505e-06

Optimization complete. Final v2v error: 2.5182907581329346 mm

Highest mean error: 2.6839795112609863 mm for frame 32

Lowest mean error: 2.3694865703582764 mm for frame 234

Saving results

Total time: 50.96740412712097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00554570
Iteration 2/25 | Loss: 0.00141272
Iteration 3/25 | Loss: 0.00121931
Iteration 4/25 | Loss: 0.00120812
Iteration 5/25 | Loss: 0.00120562
Iteration 6/25 | Loss: 0.00120530
Iteration 7/25 | Loss: 0.00120530
Iteration 8/25 | Loss: 0.00120530
Iteration 9/25 | Loss: 0.00120530
Iteration 10/25 | Loss: 0.00120530
Iteration 11/25 | Loss: 0.00120530
Iteration 12/25 | Loss: 0.00120530
Iteration 13/25 | Loss: 0.00120530
Iteration 14/25 | Loss: 0.00120530
Iteration 15/25 | Loss: 0.00120530
Iteration 16/25 | Loss: 0.00120530
Iteration 17/25 | Loss: 0.00120530
Iteration 18/25 | Loss: 0.00120530
Iteration 19/25 | Loss: 0.00120530
Iteration 20/25 | Loss: 0.00120530
Iteration 21/25 | Loss: 0.00120530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012053000973537564, 0.0012053000973537564, 0.0012053000973537564, 0.0012053000973537564, 0.0012053000973537564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012053000973537564

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.42838526
Iteration 2/25 | Loss: 0.00073585
Iteration 3/25 | Loss: 0.00073585
Iteration 4/25 | Loss: 0.00073585
Iteration 5/25 | Loss: 0.00073585
Iteration 6/25 | Loss: 0.00073585
Iteration 7/25 | Loss: 0.00073585
Iteration 8/25 | Loss: 0.00073585
Iteration 9/25 | Loss: 0.00073585
Iteration 10/25 | Loss: 0.00073585
Iteration 11/25 | Loss: 0.00073585
Iteration 12/25 | Loss: 0.00073585
Iteration 13/25 | Loss: 0.00073585
Iteration 14/25 | Loss: 0.00073585
Iteration 15/25 | Loss: 0.00073585
Iteration 16/25 | Loss: 0.00073585
Iteration 17/25 | Loss: 0.00073585
Iteration 18/25 | Loss: 0.00073585
Iteration 19/25 | Loss: 0.00073585
Iteration 20/25 | Loss: 0.00073585
Iteration 21/25 | Loss: 0.00073585
Iteration 22/25 | Loss: 0.00073585
Iteration 23/25 | Loss: 0.00073585
Iteration 24/25 | Loss: 0.00073585
Iteration 25/25 | Loss: 0.00073585

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073585
Iteration 2/1000 | Loss: 0.00004444
Iteration 3/1000 | Loss: 0.00002565
Iteration 4/1000 | Loss: 0.00002129
Iteration 5/1000 | Loss: 0.00002011
Iteration 6/1000 | Loss: 0.00001936
Iteration 7/1000 | Loss: 0.00001881
Iteration 8/1000 | Loss: 0.00001843
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001789
Iteration 11/1000 | Loss: 0.00001788
Iteration 12/1000 | Loss: 0.00001787
Iteration 13/1000 | Loss: 0.00001771
Iteration 14/1000 | Loss: 0.00001760
Iteration 15/1000 | Loss: 0.00001759
Iteration 16/1000 | Loss: 0.00001757
Iteration 17/1000 | Loss: 0.00001755
Iteration 18/1000 | Loss: 0.00001754
Iteration 19/1000 | Loss: 0.00001749
Iteration 20/1000 | Loss: 0.00001745
Iteration 21/1000 | Loss: 0.00001742
Iteration 22/1000 | Loss: 0.00001741
Iteration 23/1000 | Loss: 0.00001741
Iteration 24/1000 | Loss: 0.00001741
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001739
Iteration 28/1000 | Loss: 0.00001737
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001737
Iteration 33/1000 | Loss: 0.00001737
Iteration 34/1000 | Loss: 0.00001737
Iteration 35/1000 | Loss: 0.00001736
Iteration 36/1000 | Loss: 0.00001735
Iteration 37/1000 | Loss: 0.00001735
Iteration 38/1000 | Loss: 0.00001735
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001734
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001734
Iteration 44/1000 | Loss: 0.00001734
Iteration 45/1000 | Loss: 0.00001733
Iteration 46/1000 | Loss: 0.00001733
Iteration 47/1000 | Loss: 0.00001733
Iteration 48/1000 | Loss: 0.00001732
Iteration 49/1000 | Loss: 0.00001732
Iteration 50/1000 | Loss: 0.00001732
Iteration 51/1000 | Loss: 0.00001732
Iteration 52/1000 | Loss: 0.00001731
Iteration 53/1000 | Loss: 0.00001731
Iteration 54/1000 | Loss: 0.00001731
Iteration 55/1000 | Loss: 0.00001730
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001728
Iteration 61/1000 | Loss: 0.00001727
Iteration 62/1000 | Loss: 0.00001727
Iteration 63/1000 | Loss: 0.00001726
Iteration 64/1000 | Loss: 0.00001726
Iteration 65/1000 | Loss: 0.00001726
Iteration 66/1000 | Loss: 0.00001725
Iteration 67/1000 | Loss: 0.00001725
Iteration 68/1000 | Loss: 0.00001725
Iteration 69/1000 | Loss: 0.00001725
Iteration 70/1000 | Loss: 0.00001724
Iteration 71/1000 | Loss: 0.00001724
Iteration 72/1000 | Loss: 0.00001723
Iteration 73/1000 | Loss: 0.00001723
Iteration 74/1000 | Loss: 0.00001723
Iteration 75/1000 | Loss: 0.00001723
Iteration 76/1000 | Loss: 0.00001723
Iteration 77/1000 | Loss: 0.00001723
Iteration 78/1000 | Loss: 0.00001723
Iteration 79/1000 | Loss: 0.00001722
Iteration 80/1000 | Loss: 0.00001722
Iteration 81/1000 | Loss: 0.00001722
Iteration 82/1000 | Loss: 0.00001722
Iteration 83/1000 | Loss: 0.00001722
Iteration 84/1000 | Loss: 0.00001722
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001721
Iteration 87/1000 | Loss: 0.00001721
Iteration 88/1000 | Loss: 0.00001720
Iteration 89/1000 | Loss: 0.00001720
Iteration 90/1000 | Loss: 0.00001720
Iteration 91/1000 | Loss: 0.00001720
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001719
Iteration 98/1000 | Loss: 0.00001719
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001718
Iteration 103/1000 | Loss: 0.00001718
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001717
Iteration 108/1000 | Loss: 0.00001717
Iteration 109/1000 | Loss: 0.00001717
Iteration 110/1000 | Loss: 0.00001717
Iteration 111/1000 | Loss: 0.00001717
Iteration 112/1000 | Loss: 0.00001717
Iteration 113/1000 | Loss: 0.00001716
Iteration 114/1000 | Loss: 0.00001716
Iteration 115/1000 | Loss: 0.00001716
Iteration 116/1000 | Loss: 0.00001716
Iteration 117/1000 | Loss: 0.00001716
Iteration 118/1000 | Loss: 0.00001716
Iteration 119/1000 | Loss: 0.00001715
Iteration 120/1000 | Loss: 0.00001715
Iteration 121/1000 | Loss: 0.00001715
Iteration 122/1000 | Loss: 0.00001715
Iteration 123/1000 | Loss: 0.00001715
Iteration 124/1000 | Loss: 0.00001715
Iteration 125/1000 | Loss: 0.00001715
Iteration 126/1000 | Loss: 0.00001715
Iteration 127/1000 | Loss: 0.00001715
Iteration 128/1000 | Loss: 0.00001715
Iteration 129/1000 | Loss: 0.00001715
Iteration 130/1000 | Loss: 0.00001715
Iteration 131/1000 | Loss: 0.00001715
Iteration 132/1000 | Loss: 0.00001715
Iteration 133/1000 | Loss: 0.00001715
Iteration 134/1000 | Loss: 0.00001715
Iteration 135/1000 | Loss: 0.00001715
Iteration 136/1000 | Loss: 0.00001715
Iteration 137/1000 | Loss: 0.00001715
Iteration 138/1000 | Loss: 0.00001715
Iteration 139/1000 | Loss: 0.00001715
Iteration 140/1000 | Loss: 0.00001715
Iteration 141/1000 | Loss: 0.00001715
Iteration 142/1000 | Loss: 0.00001715
Iteration 143/1000 | Loss: 0.00001715
Iteration 144/1000 | Loss: 0.00001715
Iteration 145/1000 | Loss: 0.00001715
Iteration 146/1000 | Loss: 0.00001715
Iteration 147/1000 | Loss: 0.00001715
Iteration 148/1000 | Loss: 0.00001715
Iteration 149/1000 | Loss: 0.00001715
Iteration 150/1000 | Loss: 0.00001715
Iteration 151/1000 | Loss: 0.00001715
Iteration 152/1000 | Loss: 0.00001715
Iteration 153/1000 | Loss: 0.00001715
Iteration 154/1000 | Loss: 0.00001715
Iteration 155/1000 | Loss: 0.00001715
Iteration 156/1000 | Loss: 0.00001715
Iteration 157/1000 | Loss: 0.00001715
Iteration 158/1000 | Loss: 0.00001715
Iteration 159/1000 | Loss: 0.00001715
Iteration 160/1000 | Loss: 0.00001715
Iteration 161/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [1.7146870959550142e-05, 1.7146870959550142e-05, 1.7146870959550142e-05, 1.7146870959550142e-05, 1.7146870959550142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7146870959550142e-05

Optimization complete. Final v2v error: 3.4720358848571777 mm

Highest mean error: 5.2199273109436035 mm for frame 194

Lowest mean error: 2.971276044845581 mm for frame 147

Saving results

Total time: 42.53324317932129
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401060
Iteration 2/25 | Loss: 0.00120736
Iteration 3/25 | Loss: 0.00109845
Iteration 4/25 | Loss: 0.00108523
Iteration 5/25 | Loss: 0.00108224
Iteration 6/25 | Loss: 0.00108133
Iteration 7/25 | Loss: 0.00108130
Iteration 8/25 | Loss: 0.00108130
Iteration 9/25 | Loss: 0.00108130
Iteration 10/25 | Loss: 0.00108130
Iteration 11/25 | Loss: 0.00108130
Iteration 12/25 | Loss: 0.00108130
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010813029948621988, 0.0010813029948621988, 0.0010813029948621988, 0.0010813029948621988, 0.0010813029948621988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010813029948621988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33657598
Iteration 2/25 | Loss: 0.00076171
Iteration 3/25 | Loss: 0.00076170
Iteration 4/25 | Loss: 0.00076170
Iteration 5/25 | Loss: 0.00076170
Iteration 6/25 | Loss: 0.00076170
Iteration 7/25 | Loss: 0.00076170
Iteration 8/25 | Loss: 0.00076170
Iteration 9/25 | Loss: 0.00076170
Iteration 10/25 | Loss: 0.00076170
Iteration 11/25 | Loss: 0.00076170
Iteration 12/25 | Loss: 0.00076170
Iteration 13/25 | Loss: 0.00076170
Iteration 14/25 | Loss: 0.00076170
Iteration 15/25 | Loss: 0.00076170
Iteration 16/25 | Loss: 0.00076170
Iteration 17/25 | Loss: 0.00076170
Iteration 18/25 | Loss: 0.00076170
Iteration 19/25 | Loss: 0.00076170
Iteration 20/25 | Loss: 0.00076170
Iteration 21/25 | Loss: 0.00076170
Iteration 22/25 | Loss: 0.00076170
Iteration 23/25 | Loss: 0.00076170
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000761699746362865, 0.000761699746362865, 0.000761699746362865, 0.000761699746362865, 0.000761699746362865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000761699746362865

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076170
Iteration 2/1000 | Loss: 0.00002516
Iteration 3/1000 | Loss: 0.00001787
Iteration 4/1000 | Loss: 0.00001565
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001350
Iteration 8/1000 | Loss: 0.00001315
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001271
Iteration 11/1000 | Loss: 0.00001252
Iteration 12/1000 | Loss: 0.00001251
Iteration 13/1000 | Loss: 0.00001247
Iteration 14/1000 | Loss: 0.00001247
Iteration 15/1000 | Loss: 0.00001247
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001246
Iteration 18/1000 | Loss: 0.00001244
Iteration 19/1000 | Loss: 0.00001243
Iteration 20/1000 | Loss: 0.00001242
Iteration 21/1000 | Loss: 0.00001239
Iteration 22/1000 | Loss: 0.00001236
Iteration 23/1000 | Loss: 0.00001236
Iteration 24/1000 | Loss: 0.00001235
Iteration 25/1000 | Loss: 0.00001235
Iteration 26/1000 | Loss: 0.00001234
Iteration 27/1000 | Loss: 0.00001234
Iteration 28/1000 | Loss: 0.00001233
Iteration 29/1000 | Loss: 0.00001233
Iteration 30/1000 | Loss: 0.00001233
Iteration 31/1000 | Loss: 0.00001232
Iteration 32/1000 | Loss: 0.00001232
Iteration 33/1000 | Loss: 0.00001232
Iteration 34/1000 | Loss: 0.00001232
Iteration 35/1000 | Loss: 0.00001232
Iteration 36/1000 | Loss: 0.00001232
Iteration 37/1000 | Loss: 0.00001231
Iteration 38/1000 | Loss: 0.00001231
Iteration 39/1000 | Loss: 0.00001231
Iteration 40/1000 | Loss: 0.00001231
Iteration 41/1000 | Loss: 0.00001230
Iteration 42/1000 | Loss: 0.00001230
Iteration 43/1000 | Loss: 0.00001230
Iteration 44/1000 | Loss: 0.00001229
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001229
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001226
Iteration 50/1000 | Loss: 0.00001224
Iteration 51/1000 | Loss: 0.00001223
Iteration 52/1000 | Loss: 0.00001223
Iteration 53/1000 | Loss: 0.00001223
Iteration 54/1000 | Loss: 0.00001223
Iteration 55/1000 | Loss: 0.00001222
Iteration 56/1000 | Loss: 0.00001222
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001222
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001221
Iteration 62/1000 | Loss: 0.00001221
Iteration 63/1000 | Loss: 0.00001220
Iteration 64/1000 | Loss: 0.00001220
Iteration 65/1000 | Loss: 0.00001220
Iteration 66/1000 | Loss: 0.00001220
Iteration 67/1000 | Loss: 0.00001220
Iteration 68/1000 | Loss: 0.00001220
Iteration 69/1000 | Loss: 0.00001220
Iteration 70/1000 | Loss: 0.00001219
Iteration 71/1000 | Loss: 0.00001219
Iteration 72/1000 | Loss: 0.00001219
Iteration 73/1000 | Loss: 0.00001219
Iteration 74/1000 | Loss: 0.00001218
Iteration 75/1000 | Loss: 0.00001218
Iteration 76/1000 | Loss: 0.00001218
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001217
Iteration 84/1000 | Loss: 0.00001217
Iteration 85/1000 | Loss: 0.00001217
Iteration 86/1000 | Loss: 0.00001216
Iteration 87/1000 | Loss: 0.00001216
Iteration 88/1000 | Loss: 0.00001216
Iteration 89/1000 | Loss: 0.00001216
Iteration 90/1000 | Loss: 0.00001216
Iteration 91/1000 | Loss: 0.00001216
Iteration 92/1000 | Loss: 0.00001215
Iteration 93/1000 | Loss: 0.00001215
Iteration 94/1000 | Loss: 0.00001215
Iteration 95/1000 | Loss: 0.00001215
Iteration 96/1000 | Loss: 0.00001214
Iteration 97/1000 | Loss: 0.00001214
Iteration 98/1000 | Loss: 0.00001214
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001213
Iteration 102/1000 | Loss: 0.00001213
Iteration 103/1000 | Loss: 0.00001213
Iteration 104/1000 | Loss: 0.00001213
Iteration 105/1000 | Loss: 0.00001213
Iteration 106/1000 | Loss: 0.00001213
Iteration 107/1000 | Loss: 0.00001213
Iteration 108/1000 | Loss: 0.00001212
Iteration 109/1000 | Loss: 0.00001212
Iteration 110/1000 | Loss: 0.00001212
Iteration 111/1000 | Loss: 0.00001212
Iteration 112/1000 | Loss: 0.00001212
Iteration 113/1000 | Loss: 0.00001211
Iteration 114/1000 | Loss: 0.00001211
Iteration 115/1000 | Loss: 0.00001211
Iteration 116/1000 | Loss: 0.00001211
Iteration 117/1000 | Loss: 0.00001211
Iteration 118/1000 | Loss: 0.00001211
Iteration 119/1000 | Loss: 0.00001210
Iteration 120/1000 | Loss: 0.00001210
Iteration 121/1000 | Loss: 0.00001210
Iteration 122/1000 | Loss: 0.00001209
Iteration 123/1000 | Loss: 0.00001209
Iteration 124/1000 | Loss: 0.00001209
Iteration 125/1000 | Loss: 0.00001209
Iteration 126/1000 | Loss: 0.00001209
Iteration 127/1000 | Loss: 0.00001208
Iteration 128/1000 | Loss: 0.00001208
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001208
Iteration 132/1000 | Loss: 0.00001207
Iteration 133/1000 | Loss: 0.00001207
Iteration 134/1000 | Loss: 0.00001207
Iteration 135/1000 | Loss: 0.00001206
Iteration 136/1000 | Loss: 0.00001206
Iteration 137/1000 | Loss: 0.00001206
Iteration 138/1000 | Loss: 0.00001206
Iteration 139/1000 | Loss: 0.00001206
Iteration 140/1000 | Loss: 0.00001206
Iteration 141/1000 | Loss: 0.00001205
Iteration 142/1000 | Loss: 0.00001205
Iteration 143/1000 | Loss: 0.00001205
Iteration 144/1000 | Loss: 0.00001205
Iteration 145/1000 | Loss: 0.00001205
Iteration 146/1000 | Loss: 0.00001204
Iteration 147/1000 | Loss: 0.00001204
Iteration 148/1000 | Loss: 0.00001204
Iteration 149/1000 | Loss: 0.00001204
Iteration 150/1000 | Loss: 0.00001204
Iteration 151/1000 | Loss: 0.00001204
Iteration 152/1000 | Loss: 0.00001204
Iteration 153/1000 | Loss: 0.00001203
Iteration 154/1000 | Loss: 0.00001203
Iteration 155/1000 | Loss: 0.00001203
Iteration 156/1000 | Loss: 0.00001202
Iteration 157/1000 | Loss: 0.00001202
Iteration 158/1000 | Loss: 0.00001202
Iteration 159/1000 | Loss: 0.00001201
Iteration 160/1000 | Loss: 0.00001201
Iteration 161/1000 | Loss: 0.00001201
Iteration 162/1000 | Loss: 0.00001201
Iteration 163/1000 | Loss: 0.00001201
Iteration 164/1000 | Loss: 0.00001201
Iteration 165/1000 | Loss: 0.00001201
Iteration 166/1000 | Loss: 0.00001200
Iteration 167/1000 | Loss: 0.00001200
Iteration 168/1000 | Loss: 0.00001200
Iteration 169/1000 | Loss: 0.00001200
Iteration 170/1000 | Loss: 0.00001200
Iteration 171/1000 | Loss: 0.00001200
Iteration 172/1000 | Loss: 0.00001199
Iteration 173/1000 | Loss: 0.00001199
Iteration 174/1000 | Loss: 0.00001199
Iteration 175/1000 | Loss: 0.00001199
Iteration 176/1000 | Loss: 0.00001199
Iteration 177/1000 | Loss: 0.00001199
Iteration 178/1000 | Loss: 0.00001199
Iteration 179/1000 | Loss: 0.00001199
Iteration 180/1000 | Loss: 0.00001199
Iteration 181/1000 | Loss: 0.00001199
Iteration 182/1000 | Loss: 0.00001199
Iteration 183/1000 | Loss: 0.00001199
Iteration 184/1000 | Loss: 0.00001198
Iteration 185/1000 | Loss: 0.00001198
Iteration 186/1000 | Loss: 0.00001198
Iteration 187/1000 | Loss: 0.00001198
Iteration 188/1000 | Loss: 0.00001198
Iteration 189/1000 | Loss: 0.00001197
Iteration 190/1000 | Loss: 0.00001197
Iteration 191/1000 | Loss: 0.00001197
Iteration 192/1000 | Loss: 0.00001197
Iteration 193/1000 | Loss: 0.00001196
Iteration 194/1000 | Loss: 0.00001196
Iteration 195/1000 | Loss: 0.00001196
Iteration 196/1000 | Loss: 0.00001196
Iteration 197/1000 | Loss: 0.00001196
Iteration 198/1000 | Loss: 0.00001196
Iteration 199/1000 | Loss: 0.00001195
Iteration 200/1000 | Loss: 0.00001195
Iteration 201/1000 | Loss: 0.00001195
Iteration 202/1000 | Loss: 0.00001195
Iteration 203/1000 | Loss: 0.00001195
Iteration 204/1000 | Loss: 0.00001195
Iteration 205/1000 | Loss: 0.00001195
Iteration 206/1000 | Loss: 0.00001194
Iteration 207/1000 | Loss: 0.00001194
Iteration 208/1000 | Loss: 0.00001194
Iteration 209/1000 | Loss: 0.00001194
Iteration 210/1000 | Loss: 0.00001194
Iteration 211/1000 | Loss: 0.00001194
Iteration 212/1000 | Loss: 0.00001194
Iteration 213/1000 | Loss: 0.00001194
Iteration 214/1000 | Loss: 0.00001194
Iteration 215/1000 | Loss: 0.00001193
Iteration 216/1000 | Loss: 0.00001193
Iteration 217/1000 | Loss: 0.00001193
Iteration 218/1000 | Loss: 0.00001193
Iteration 219/1000 | Loss: 0.00001193
Iteration 220/1000 | Loss: 0.00001193
Iteration 221/1000 | Loss: 0.00001193
Iteration 222/1000 | Loss: 0.00001193
Iteration 223/1000 | Loss: 0.00001193
Iteration 224/1000 | Loss: 0.00001193
Iteration 225/1000 | Loss: 0.00001192
Iteration 226/1000 | Loss: 0.00001192
Iteration 227/1000 | Loss: 0.00001192
Iteration 228/1000 | Loss: 0.00001192
Iteration 229/1000 | Loss: 0.00001192
Iteration 230/1000 | Loss: 0.00001192
Iteration 231/1000 | Loss: 0.00001192
Iteration 232/1000 | Loss: 0.00001192
Iteration 233/1000 | Loss: 0.00001192
Iteration 234/1000 | Loss: 0.00001192
Iteration 235/1000 | Loss: 0.00001192
Iteration 236/1000 | Loss: 0.00001192
Iteration 237/1000 | Loss: 0.00001192
Iteration 238/1000 | Loss: 0.00001192
Iteration 239/1000 | Loss: 0.00001192
Iteration 240/1000 | Loss: 0.00001191
Iteration 241/1000 | Loss: 0.00001191
Iteration 242/1000 | Loss: 0.00001191
Iteration 243/1000 | Loss: 0.00001191
Iteration 244/1000 | Loss: 0.00001191
Iteration 245/1000 | Loss: 0.00001191
Iteration 246/1000 | Loss: 0.00001191
Iteration 247/1000 | Loss: 0.00001191
Iteration 248/1000 | Loss: 0.00001191
Iteration 249/1000 | Loss: 0.00001191
Iteration 250/1000 | Loss: 0.00001191
Iteration 251/1000 | Loss: 0.00001191
Iteration 252/1000 | Loss: 0.00001191
Iteration 253/1000 | Loss: 0.00001191
Iteration 254/1000 | Loss: 0.00001191
Iteration 255/1000 | Loss: 0.00001191
Iteration 256/1000 | Loss: 0.00001191
Iteration 257/1000 | Loss: 0.00001191
Iteration 258/1000 | Loss: 0.00001190
Iteration 259/1000 | Loss: 0.00001190
Iteration 260/1000 | Loss: 0.00001190
Iteration 261/1000 | Loss: 0.00001190
Iteration 262/1000 | Loss: 0.00001190
Iteration 263/1000 | Loss: 0.00001190
Iteration 264/1000 | Loss: 0.00001190
Iteration 265/1000 | Loss: 0.00001190
Iteration 266/1000 | Loss: 0.00001189
Iteration 267/1000 | Loss: 0.00001189
Iteration 268/1000 | Loss: 0.00001189
Iteration 269/1000 | Loss: 0.00001189
Iteration 270/1000 | Loss: 0.00001189
Iteration 271/1000 | Loss: 0.00001189
Iteration 272/1000 | Loss: 0.00001189
Iteration 273/1000 | Loss: 0.00001189
Iteration 274/1000 | Loss: 0.00001189
Iteration 275/1000 | Loss: 0.00001189
Iteration 276/1000 | Loss: 0.00001189
Iteration 277/1000 | Loss: 0.00001189
Iteration 278/1000 | Loss: 0.00001189
Iteration 279/1000 | Loss: 0.00001189
Iteration 280/1000 | Loss: 0.00001189
Iteration 281/1000 | Loss: 0.00001189
Iteration 282/1000 | Loss: 0.00001189
Iteration 283/1000 | Loss: 0.00001189
Iteration 284/1000 | Loss: 0.00001189
Iteration 285/1000 | Loss: 0.00001189
Iteration 286/1000 | Loss: 0.00001189
Iteration 287/1000 | Loss: 0.00001189
Iteration 288/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 288. Stopping optimization.
Last 5 losses: [1.1889062079717405e-05, 1.1889062079717405e-05, 1.1889062079717405e-05, 1.1889062079717405e-05, 1.1889062079717405e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1889062079717405e-05

Optimization complete. Final v2v error: 2.8828399181365967 mm

Highest mean error: 3.3199899196624756 mm for frame 87

Lowest mean error: 2.553358554840088 mm for frame 12

Saving results

Total time: 46.10089421272278
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034176
Iteration 2/25 | Loss: 0.00156115
Iteration 3/25 | Loss: 0.00118916
Iteration 4/25 | Loss: 0.00115127
Iteration 5/25 | Loss: 0.00113278
Iteration 6/25 | Loss: 0.00113112
Iteration 7/25 | Loss: 0.00111122
Iteration 8/25 | Loss: 0.00110965
Iteration 9/25 | Loss: 0.00110674
Iteration 10/25 | Loss: 0.00110670
Iteration 11/25 | Loss: 0.00110670
Iteration 12/25 | Loss: 0.00110669
Iteration 13/25 | Loss: 0.00110669
Iteration 14/25 | Loss: 0.00110669
Iteration 15/25 | Loss: 0.00110669
Iteration 16/25 | Loss: 0.00110669
Iteration 17/25 | Loss: 0.00110669
Iteration 18/25 | Loss: 0.00110669
Iteration 19/25 | Loss: 0.00110669
Iteration 20/25 | Loss: 0.00110669
Iteration 21/25 | Loss: 0.00110669
Iteration 22/25 | Loss: 0.00110669
Iteration 23/25 | Loss: 0.00110669
Iteration 24/25 | Loss: 0.00110669
Iteration 25/25 | Loss: 0.00110669

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.37206984
Iteration 2/25 | Loss: 0.00087115
Iteration 3/25 | Loss: 0.00087114
Iteration 4/25 | Loss: 0.00087114
Iteration 5/25 | Loss: 0.00087114
Iteration 6/25 | Loss: 0.00087114
Iteration 7/25 | Loss: 0.00087114
Iteration 8/25 | Loss: 0.00087114
Iteration 9/25 | Loss: 0.00087114
Iteration 10/25 | Loss: 0.00087114
Iteration 11/25 | Loss: 0.00087114
Iteration 12/25 | Loss: 0.00087114
Iteration 13/25 | Loss: 0.00087114
Iteration 14/25 | Loss: 0.00087114
Iteration 15/25 | Loss: 0.00087114
Iteration 16/25 | Loss: 0.00087114
Iteration 17/25 | Loss: 0.00087114
Iteration 18/25 | Loss: 0.00087114
Iteration 19/25 | Loss: 0.00087114
Iteration 20/25 | Loss: 0.00087114
Iteration 21/25 | Loss: 0.00087114
Iteration 22/25 | Loss: 0.00087114
Iteration 23/25 | Loss: 0.00087114
Iteration 24/25 | Loss: 0.00087114
Iteration 25/25 | Loss: 0.00087114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087114
Iteration 2/1000 | Loss: 0.00002472
Iteration 3/1000 | Loss: 0.00001742
Iteration 4/1000 | Loss: 0.00001521
Iteration 5/1000 | Loss: 0.00001438
Iteration 6/1000 | Loss: 0.00001380
Iteration 7/1000 | Loss: 0.00001334
Iteration 8/1000 | Loss: 0.00001309
Iteration 9/1000 | Loss: 0.00001273
Iteration 10/1000 | Loss: 0.00001248
Iteration 11/1000 | Loss: 0.00001237
Iteration 12/1000 | Loss: 0.00001224
Iteration 13/1000 | Loss: 0.00001210
Iteration 14/1000 | Loss: 0.00001207
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001196
Iteration 18/1000 | Loss: 0.00001196
Iteration 19/1000 | Loss: 0.00001196
Iteration 20/1000 | Loss: 0.00001195
Iteration 21/1000 | Loss: 0.00001192
Iteration 22/1000 | Loss: 0.00001192
Iteration 23/1000 | Loss: 0.00001192
Iteration 24/1000 | Loss: 0.00001191
Iteration 25/1000 | Loss: 0.00001191
Iteration 26/1000 | Loss: 0.00001191
Iteration 27/1000 | Loss: 0.00001191
Iteration 28/1000 | Loss: 0.00001191
Iteration 29/1000 | Loss: 0.00001191
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001191
Iteration 32/1000 | Loss: 0.00001191
Iteration 33/1000 | Loss: 0.00001191
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001191
Iteration 38/1000 | Loss: 0.00001191
Iteration 39/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 39. Stopping optimization.
Last 5 losses: [1.1912685295101255e-05, 1.1912685295101255e-05, 1.1912685295101255e-05, 1.1912685295101255e-05, 1.1912685295101255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1912685295101255e-05

Optimization complete. Final v2v error: 2.9560728073120117 mm

Highest mean error: 3.4843599796295166 mm for frame 181

Lowest mean error: 2.7484419345855713 mm for frame 42

Saving results

Total time: 40.21881175041199
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00384830
Iteration 2/25 | Loss: 0.00122815
Iteration 3/25 | Loss: 0.00110223
Iteration 4/25 | Loss: 0.00108227
Iteration 5/25 | Loss: 0.00107580
Iteration 6/25 | Loss: 0.00107419
Iteration 7/25 | Loss: 0.00107414
Iteration 8/25 | Loss: 0.00107414
Iteration 9/25 | Loss: 0.00107414
Iteration 10/25 | Loss: 0.00107414
Iteration 11/25 | Loss: 0.00107414
Iteration 12/25 | Loss: 0.00107414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001074144383892417, 0.001074144383892417, 0.001074144383892417, 0.001074144383892417, 0.001074144383892417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001074144383892417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33467722
Iteration 2/25 | Loss: 0.00097556
Iteration 3/25 | Loss: 0.00097556
Iteration 4/25 | Loss: 0.00097556
Iteration 5/25 | Loss: 0.00097556
Iteration 6/25 | Loss: 0.00097556
Iteration 7/25 | Loss: 0.00097556
Iteration 8/25 | Loss: 0.00097556
Iteration 9/25 | Loss: 0.00097555
Iteration 10/25 | Loss: 0.00097555
Iteration 11/25 | Loss: 0.00097555
Iteration 12/25 | Loss: 0.00097555
Iteration 13/25 | Loss: 0.00097555
Iteration 14/25 | Loss: 0.00097555
Iteration 15/25 | Loss: 0.00097555
Iteration 16/25 | Loss: 0.00097555
Iteration 17/25 | Loss: 0.00097555
Iteration 18/25 | Loss: 0.00097555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000975554168689996, 0.000975554168689996, 0.000975554168689996, 0.000975554168689996, 0.000975554168689996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000975554168689996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097555
Iteration 2/1000 | Loss: 0.00004354
Iteration 3/1000 | Loss: 0.00002977
Iteration 4/1000 | Loss: 0.00002173
Iteration 5/1000 | Loss: 0.00001887
Iteration 6/1000 | Loss: 0.00001719
Iteration 7/1000 | Loss: 0.00001591
Iteration 8/1000 | Loss: 0.00001519
Iteration 9/1000 | Loss: 0.00001474
Iteration 10/1000 | Loss: 0.00001437
Iteration 11/1000 | Loss: 0.00001412
Iteration 12/1000 | Loss: 0.00001391
Iteration 13/1000 | Loss: 0.00001382
Iteration 14/1000 | Loss: 0.00001377
Iteration 15/1000 | Loss: 0.00001366
Iteration 16/1000 | Loss: 0.00001360
Iteration 17/1000 | Loss: 0.00001352
Iteration 18/1000 | Loss: 0.00001351
Iteration 19/1000 | Loss: 0.00001349
Iteration 20/1000 | Loss: 0.00001348
Iteration 21/1000 | Loss: 0.00001348
Iteration 22/1000 | Loss: 0.00001347
Iteration 23/1000 | Loss: 0.00001347
Iteration 24/1000 | Loss: 0.00001346
Iteration 25/1000 | Loss: 0.00001344
Iteration 26/1000 | Loss: 0.00001343
Iteration 27/1000 | Loss: 0.00001342
Iteration 28/1000 | Loss: 0.00001341
Iteration 29/1000 | Loss: 0.00001341
Iteration 30/1000 | Loss: 0.00001340
Iteration 31/1000 | Loss: 0.00001340
Iteration 32/1000 | Loss: 0.00001340
Iteration 33/1000 | Loss: 0.00001335
Iteration 34/1000 | Loss: 0.00001332
Iteration 35/1000 | Loss: 0.00001332
Iteration 36/1000 | Loss: 0.00001331
Iteration 37/1000 | Loss: 0.00001331
Iteration 38/1000 | Loss: 0.00001330
Iteration 39/1000 | Loss: 0.00001330
Iteration 40/1000 | Loss: 0.00001330
Iteration 41/1000 | Loss: 0.00001330
Iteration 42/1000 | Loss: 0.00001330
Iteration 43/1000 | Loss: 0.00001330
Iteration 44/1000 | Loss: 0.00001329
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001329
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001328
Iteration 49/1000 | Loss: 0.00001328
Iteration 50/1000 | Loss: 0.00001328
Iteration 51/1000 | Loss: 0.00001327
Iteration 52/1000 | Loss: 0.00001327
Iteration 53/1000 | Loss: 0.00001327
Iteration 54/1000 | Loss: 0.00001326
Iteration 55/1000 | Loss: 0.00001326
Iteration 56/1000 | Loss: 0.00001326
Iteration 57/1000 | Loss: 0.00001326
Iteration 58/1000 | Loss: 0.00001326
Iteration 59/1000 | Loss: 0.00001326
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001326
Iteration 62/1000 | Loss: 0.00001326
Iteration 63/1000 | Loss: 0.00001325
Iteration 64/1000 | Loss: 0.00001325
Iteration 65/1000 | Loss: 0.00001325
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001324
Iteration 69/1000 | Loss: 0.00001324
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001322
Iteration 80/1000 | Loss: 0.00001322
Iteration 81/1000 | Loss: 0.00001322
Iteration 82/1000 | Loss: 0.00001321
Iteration 83/1000 | Loss: 0.00001321
Iteration 84/1000 | Loss: 0.00001321
Iteration 85/1000 | Loss: 0.00001321
Iteration 86/1000 | Loss: 0.00001320
Iteration 87/1000 | Loss: 0.00001320
Iteration 88/1000 | Loss: 0.00001320
Iteration 89/1000 | Loss: 0.00001319
Iteration 90/1000 | Loss: 0.00001319
Iteration 91/1000 | Loss: 0.00001319
Iteration 92/1000 | Loss: 0.00001319
Iteration 93/1000 | Loss: 0.00001318
Iteration 94/1000 | Loss: 0.00001318
Iteration 95/1000 | Loss: 0.00001318
Iteration 96/1000 | Loss: 0.00001318
Iteration 97/1000 | Loss: 0.00001318
Iteration 98/1000 | Loss: 0.00001317
Iteration 99/1000 | Loss: 0.00001317
Iteration 100/1000 | Loss: 0.00001317
Iteration 101/1000 | Loss: 0.00001317
Iteration 102/1000 | Loss: 0.00001317
Iteration 103/1000 | Loss: 0.00001317
Iteration 104/1000 | Loss: 0.00001317
Iteration 105/1000 | Loss: 0.00001316
Iteration 106/1000 | Loss: 0.00001316
Iteration 107/1000 | Loss: 0.00001316
Iteration 108/1000 | Loss: 0.00001316
Iteration 109/1000 | Loss: 0.00001316
Iteration 110/1000 | Loss: 0.00001316
Iteration 111/1000 | Loss: 0.00001316
Iteration 112/1000 | Loss: 0.00001315
Iteration 113/1000 | Loss: 0.00001315
Iteration 114/1000 | Loss: 0.00001315
Iteration 115/1000 | Loss: 0.00001315
Iteration 116/1000 | Loss: 0.00001315
Iteration 117/1000 | Loss: 0.00001315
Iteration 118/1000 | Loss: 0.00001314
Iteration 119/1000 | Loss: 0.00001314
Iteration 120/1000 | Loss: 0.00001314
Iteration 121/1000 | Loss: 0.00001314
Iteration 122/1000 | Loss: 0.00001313
Iteration 123/1000 | Loss: 0.00001313
Iteration 124/1000 | Loss: 0.00001312
Iteration 125/1000 | Loss: 0.00001312
Iteration 126/1000 | Loss: 0.00001312
Iteration 127/1000 | Loss: 0.00001312
Iteration 128/1000 | Loss: 0.00001311
Iteration 129/1000 | Loss: 0.00001311
Iteration 130/1000 | Loss: 0.00001311
Iteration 131/1000 | Loss: 0.00001311
Iteration 132/1000 | Loss: 0.00001311
Iteration 133/1000 | Loss: 0.00001311
Iteration 134/1000 | Loss: 0.00001311
Iteration 135/1000 | Loss: 0.00001311
Iteration 136/1000 | Loss: 0.00001311
Iteration 137/1000 | Loss: 0.00001311
Iteration 138/1000 | Loss: 0.00001311
Iteration 139/1000 | Loss: 0.00001311
Iteration 140/1000 | Loss: 0.00001311
Iteration 141/1000 | Loss: 0.00001310
Iteration 142/1000 | Loss: 0.00001310
Iteration 143/1000 | Loss: 0.00001310
Iteration 144/1000 | Loss: 0.00001310
Iteration 145/1000 | Loss: 0.00001310
Iteration 146/1000 | Loss: 0.00001310
Iteration 147/1000 | Loss: 0.00001309
Iteration 148/1000 | Loss: 0.00001309
Iteration 149/1000 | Loss: 0.00001309
Iteration 150/1000 | Loss: 0.00001309
Iteration 151/1000 | Loss: 0.00001309
Iteration 152/1000 | Loss: 0.00001309
Iteration 153/1000 | Loss: 0.00001309
Iteration 154/1000 | Loss: 0.00001309
Iteration 155/1000 | Loss: 0.00001309
Iteration 156/1000 | Loss: 0.00001309
Iteration 157/1000 | Loss: 0.00001309
Iteration 158/1000 | Loss: 0.00001309
Iteration 159/1000 | Loss: 0.00001309
Iteration 160/1000 | Loss: 0.00001309
Iteration 161/1000 | Loss: 0.00001309
Iteration 162/1000 | Loss: 0.00001309
Iteration 163/1000 | Loss: 0.00001308
Iteration 164/1000 | Loss: 0.00001308
Iteration 165/1000 | Loss: 0.00001308
Iteration 166/1000 | Loss: 0.00001308
Iteration 167/1000 | Loss: 0.00001308
Iteration 168/1000 | Loss: 0.00001308
Iteration 169/1000 | Loss: 0.00001308
Iteration 170/1000 | Loss: 0.00001308
Iteration 171/1000 | Loss: 0.00001308
Iteration 172/1000 | Loss: 0.00001308
Iteration 173/1000 | Loss: 0.00001308
Iteration 174/1000 | Loss: 0.00001307
Iteration 175/1000 | Loss: 0.00001307
Iteration 176/1000 | Loss: 0.00001307
Iteration 177/1000 | Loss: 0.00001307
Iteration 178/1000 | Loss: 0.00001307
Iteration 179/1000 | Loss: 0.00001307
Iteration 180/1000 | Loss: 0.00001307
Iteration 181/1000 | Loss: 0.00001307
Iteration 182/1000 | Loss: 0.00001307
Iteration 183/1000 | Loss: 0.00001307
Iteration 184/1000 | Loss: 0.00001307
Iteration 185/1000 | Loss: 0.00001307
Iteration 186/1000 | Loss: 0.00001306
Iteration 187/1000 | Loss: 0.00001306
Iteration 188/1000 | Loss: 0.00001306
Iteration 189/1000 | Loss: 0.00001306
Iteration 190/1000 | Loss: 0.00001306
Iteration 191/1000 | Loss: 0.00001306
Iteration 192/1000 | Loss: 0.00001305
Iteration 193/1000 | Loss: 0.00001305
Iteration 194/1000 | Loss: 0.00001305
Iteration 195/1000 | Loss: 0.00001305
Iteration 196/1000 | Loss: 0.00001305
Iteration 197/1000 | Loss: 0.00001305
Iteration 198/1000 | Loss: 0.00001304
Iteration 199/1000 | Loss: 0.00001304
Iteration 200/1000 | Loss: 0.00001304
Iteration 201/1000 | Loss: 0.00001304
Iteration 202/1000 | Loss: 0.00001304
Iteration 203/1000 | Loss: 0.00001304
Iteration 204/1000 | Loss: 0.00001304
Iteration 205/1000 | Loss: 0.00001303
Iteration 206/1000 | Loss: 0.00001303
Iteration 207/1000 | Loss: 0.00001303
Iteration 208/1000 | Loss: 0.00001303
Iteration 209/1000 | Loss: 0.00001303
Iteration 210/1000 | Loss: 0.00001303
Iteration 211/1000 | Loss: 0.00001303
Iteration 212/1000 | Loss: 0.00001303
Iteration 213/1000 | Loss: 0.00001303
Iteration 214/1000 | Loss: 0.00001303
Iteration 215/1000 | Loss: 0.00001303
Iteration 216/1000 | Loss: 0.00001303
Iteration 217/1000 | Loss: 0.00001303
Iteration 218/1000 | Loss: 0.00001302
Iteration 219/1000 | Loss: 0.00001302
Iteration 220/1000 | Loss: 0.00001302
Iteration 221/1000 | Loss: 0.00001302
Iteration 222/1000 | Loss: 0.00001302
Iteration 223/1000 | Loss: 0.00001302
Iteration 224/1000 | Loss: 0.00001302
Iteration 225/1000 | Loss: 0.00001302
Iteration 226/1000 | Loss: 0.00001302
Iteration 227/1000 | Loss: 0.00001302
Iteration 228/1000 | Loss: 0.00001301
Iteration 229/1000 | Loss: 0.00001301
Iteration 230/1000 | Loss: 0.00001301
Iteration 231/1000 | Loss: 0.00001301
Iteration 232/1000 | Loss: 0.00001301
Iteration 233/1000 | Loss: 0.00001301
Iteration 234/1000 | Loss: 0.00001301
Iteration 235/1000 | Loss: 0.00001301
Iteration 236/1000 | Loss: 0.00001300
Iteration 237/1000 | Loss: 0.00001300
Iteration 238/1000 | Loss: 0.00001300
Iteration 239/1000 | Loss: 0.00001300
Iteration 240/1000 | Loss: 0.00001300
Iteration 241/1000 | Loss: 0.00001300
Iteration 242/1000 | Loss: 0.00001300
Iteration 243/1000 | Loss: 0.00001300
Iteration 244/1000 | Loss: 0.00001300
Iteration 245/1000 | Loss: 0.00001300
Iteration 246/1000 | Loss: 0.00001300
Iteration 247/1000 | Loss: 0.00001300
Iteration 248/1000 | Loss: 0.00001300
Iteration 249/1000 | Loss: 0.00001300
Iteration 250/1000 | Loss: 0.00001300
Iteration 251/1000 | Loss: 0.00001300
Iteration 252/1000 | Loss: 0.00001300
Iteration 253/1000 | Loss: 0.00001300
Iteration 254/1000 | Loss: 0.00001300
Iteration 255/1000 | Loss: 0.00001300
Iteration 256/1000 | Loss: 0.00001300
Iteration 257/1000 | Loss: 0.00001300
Iteration 258/1000 | Loss: 0.00001300
Iteration 259/1000 | Loss: 0.00001299
Iteration 260/1000 | Loss: 0.00001299
Iteration 261/1000 | Loss: 0.00001299
Iteration 262/1000 | Loss: 0.00001299
Iteration 263/1000 | Loss: 0.00001299
Iteration 264/1000 | Loss: 0.00001299
Iteration 265/1000 | Loss: 0.00001299
Iteration 266/1000 | Loss: 0.00001299
Iteration 267/1000 | Loss: 0.00001299
Iteration 268/1000 | Loss: 0.00001299
Iteration 269/1000 | Loss: 0.00001299
Iteration 270/1000 | Loss: 0.00001299
Iteration 271/1000 | Loss: 0.00001299
Iteration 272/1000 | Loss: 0.00001299
Iteration 273/1000 | Loss: 0.00001299
Iteration 274/1000 | Loss: 0.00001299
Iteration 275/1000 | Loss: 0.00001299
Iteration 276/1000 | Loss: 0.00001299
Iteration 277/1000 | Loss: 0.00001299
Iteration 278/1000 | Loss: 0.00001299
Iteration 279/1000 | Loss: 0.00001299
Iteration 280/1000 | Loss: 0.00001299
Iteration 281/1000 | Loss: 0.00001298
Iteration 282/1000 | Loss: 0.00001298
Iteration 283/1000 | Loss: 0.00001298
Iteration 284/1000 | Loss: 0.00001298
Iteration 285/1000 | Loss: 0.00001298
Iteration 286/1000 | Loss: 0.00001298
Iteration 287/1000 | Loss: 0.00001298
Iteration 288/1000 | Loss: 0.00001298
Iteration 289/1000 | Loss: 0.00001298
Iteration 290/1000 | Loss: 0.00001298
Iteration 291/1000 | Loss: 0.00001298
Iteration 292/1000 | Loss: 0.00001298
Iteration 293/1000 | Loss: 0.00001298
Iteration 294/1000 | Loss: 0.00001298
Iteration 295/1000 | Loss: 0.00001297
Iteration 296/1000 | Loss: 0.00001297
Iteration 297/1000 | Loss: 0.00001297
Iteration 298/1000 | Loss: 0.00001297
Iteration 299/1000 | Loss: 0.00001297
Iteration 300/1000 | Loss: 0.00001297
Iteration 301/1000 | Loss: 0.00001297
Iteration 302/1000 | Loss: 0.00001297
Iteration 303/1000 | Loss: 0.00001297
Iteration 304/1000 | Loss: 0.00001297
Iteration 305/1000 | Loss: 0.00001297
Iteration 306/1000 | Loss: 0.00001297
Iteration 307/1000 | Loss: 0.00001297
Iteration 308/1000 | Loss: 0.00001297
Iteration 309/1000 | Loss: 0.00001296
Iteration 310/1000 | Loss: 0.00001296
Iteration 311/1000 | Loss: 0.00001296
Iteration 312/1000 | Loss: 0.00001296
Iteration 313/1000 | Loss: 0.00001296
Iteration 314/1000 | Loss: 0.00001296
Iteration 315/1000 | Loss: 0.00001296
Iteration 316/1000 | Loss: 0.00001296
Iteration 317/1000 | Loss: 0.00001296
Iteration 318/1000 | Loss: 0.00001296
Iteration 319/1000 | Loss: 0.00001296
Iteration 320/1000 | Loss: 0.00001296
Iteration 321/1000 | Loss: 0.00001296
Iteration 322/1000 | Loss: 0.00001296
Iteration 323/1000 | Loss: 0.00001296
Iteration 324/1000 | Loss: 0.00001296
Iteration 325/1000 | Loss: 0.00001296
Iteration 326/1000 | Loss: 0.00001296
Iteration 327/1000 | Loss: 0.00001296
Iteration 328/1000 | Loss: 0.00001296
Iteration 329/1000 | Loss: 0.00001296
Iteration 330/1000 | Loss: 0.00001296
Iteration 331/1000 | Loss: 0.00001296
Iteration 332/1000 | Loss: 0.00001296
Iteration 333/1000 | Loss: 0.00001296
Iteration 334/1000 | Loss: 0.00001296
Iteration 335/1000 | Loss: 0.00001296
Iteration 336/1000 | Loss: 0.00001296
Iteration 337/1000 | Loss: 0.00001296
Iteration 338/1000 | Loss: 0.00001296
Iteration 339/1000 | Loss: 0.00001296
Iteration 340/1000 | Loss: 0.00001296
Iteration 341/1000 | Loss: 0.00001296
Iteration 342/1000 | Loss: 0.00001296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 342. Stopping optimization.
Last 5 losses: [1.296196296607377e-05, 1.296196296607377e-05, 1.296196296607377e-05, 1.296196296607377e-05, 1.296196296607377e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.296196296607377e-05

Optimization complete. Final v2v error: 3.035489559173584 mm

Highest mean error: 3.8973543643951416 mm for frame 24

Lowest mean error: 2.5193443298339844 mm for frame 11

Saving results

Total time: 51.881671667099
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768545
Iteration 2/25 | Loss: 0.00118362
Iteration 3/25 | Loss: 0.00108220
Iteration 4/25 | Loss: 0.00107332
Iteration 5/25 | Loss: 0.00107136
Iteration 6/25 | Loss: 0.00107136
Iteration 7/25 | Loss: 0.00107136
Iteration 8/25 | Loss: 0.00107136
Iteration 9/25 | Loss: 0.00107136
Iteration 10/25 | Loss: 0.00107136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010713565861806273, 0.0010713565861806273, 0.0010713565861806273, 0.0010713565861806273, 0.0010713565861806273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010713565861806273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35269082
Iteration 2/25 | Loss: 0.00076230
Iteration 3/25 | Loss: 0.00076230
Iteration 4/25 | Loss: 0.00076230
Iteration 5/25 | Loss: 0.00076229
Iteration 6/25 | Loss: 0.00076229
Iteration 7/25 | Loss: 0.00076229
Iteration 8/25 | Loss: 0.00076229
Iteration 9/25 | Loss: 0.00076229
Iteration 10/25 | Loss: 0.00076229
Iteration 11/25 | Loss: 0.00076229
Iteration 12/25 | Loss: 0.00076229
Iteration 13/25 | Loss: 0.00076229
Iteration 14/25 | Loss: 0.00076229
Iteration 15/25 | Loss: 0.00076229
Iteration 16/25 | Loss: 0.00076229
Iteration 17/25 | Loss: 0.00076229
Iteration 18/25 | Loss: 0.00076229
Iteration 19/25 | Loss: 0.00076229
Iteration 20/25 | Loss: 0.00076229
Iteration 21/25 | Loss: 0.00076229
Iteration 22/25 | Loss: 0.00076229
Iteration 23/25 | Loss: 0.00076229
Iteration 24/25 | Loss: 0.00076229
Iteration 25/25 | Loss: 0.00076229

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076229
Iteration 2/1000 | Loss: 0.00001966
Iteration 3/1000 | Loss: 0.00001422
Iteration 4/1000 | Loss: 0.00001270
Iteration 5/1000 | Loss: 0.00001180
Iteration 6/1000 | Loss: 0.00001118
Iteration 7/1000 | Loss: 0.00001083
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001013
Iteration 10/1000 | Loss: 0.00001007
Iteration 11/1000 | Loss: 0.00001001
Iteration 12/1000 | Loss: 0.00000996
Iteration 13/1000 | Loss: 0.00000989
Iteration 14/1000 | Loss: 0.00000988
Iteration 15/1000 | Loss: 0.00000986
Iteration 16/1000 | Loss: 0.00000985
Iteration 17/1000 | Loss: 0.00000985
Iteration 18/1000 | Loss: 0.00000984
Iteration 19/1000 | Loss: 0.00000982
Iteration 20/1000 | Loss: 0.00000979
Iteration 21/1000 | Loss: 0.00000978
Iteration 22/1000 | Loss: 0.00000978
Iteration 23/1000 | Loss: 0.00000977
Iteration 24/1000 | Loss: 0.00000977
Iteration 25/1000 | Loss: 0.00000976
Iteration 26/1000 | Loss: 0.00000976
Iteration 27/1000 | Loss: 0.00000975
Iteration 28/1000 | Loss: 0.00000973
Iteration 29/1000 | Loss: 0.00000972
Iteration 30/1000 | Loss: 0.00000972
Iteration 31/1000 | Loss: 0.00000971
Iteration 32/1000 | Loss: 0.00000970
Iteration 33/1000 | Loss: 0.00000969
Iteration 34/1000 | Loss: 0.00000968
Iteration 35/1000 | Loss: 0.00000967
Iteration 36/1000 | Loss: 0.00000967
Iteration 37/1000 | Loss: 0.00000967
Iteration 38/1000 | Loss: 0.00000966
Iteration 39/1000 | Loss: 0.00000966
Iteration 40/1000 | Loss: 0.00000966
Iteration 41/1000 | Loss: 0.00000965
Iteration 42/1000 | Loss: 0.00000962
Iteration 43/1000 | Loss: 0.00000961
Iteration 44/1000 | Loss: 0.00000959
Iteration 45/1000 | Loss: 0.00000958
Iteration 46/1000 | Loss: 0.00000957
Iteration 47/1000 | Loss: 0.00000957
Iteration 48/1000 | Loss: 0.00000957
Iteration 49/1000 | Loss: 0.00000956
Iteration 50/1000 | Loss: 0.00000955
Iteration 51/1000 | Loss: 0.00000955
Iteration 52/1000 | Loss: 0.00000951
Iteration 53/1000 | Loss: 0.00000951
Iteration 54/1000 | Loss: 0.00000950
Iteration 55/1000 | Loss: 0.00000950
Iteration 56/1000 | Loss: 0.00000949
Iteration 57/1000 | Loss: 0.00000949
Iteration 58/1000 | Loss: 0.00000946
Iteration 59/1000 | Loss: 0.00000946
Iteration 60/1000 | Loss: 0.00000946
Iteration 61/1000 | Loss: 0.00000946
Iteration 62/1000 | Loss: 0.00000946
Iteration 63/1000 | Loss: 0.00000946
Iteration 64/1000 | Loss: 0.00000946
Iteration 65/1000 | Loss: 0.00000945
Iteration 66/1000 | Loss: 0.00000945
Iteration 67/1000 | Loss: 0.00000945
Iteration 68/1000 | Loss: 0.00000945
Iteration 69/1000 | Loss: 0.00000943
Iteration 70/1000 | Loss: 0.00000943
Iteration 71/1000 | Loss: 0.00000943
Iteration 72/1000 | Loss: 0.00000943
Iteration 73/1000 | Loss: 0.00000943
Iteration 74/1000 | Loss: 0.00000942
Iteration 75/1000 | Loss: 0.00000942
Iteration 76/1000 | Loss: 0.00000942
Iteration 77/1000 | Loss: 0.00000942
Iteration 78/1000 | Loss: 0.00000942
Iteration 79/1000 | Loss: 0.00000941
Iteration 80/1000 | Loss: 0.00000941
Iteration 81/1000 | Loss: 0.00000939
Iteration 82/1000 | Loss: 0.00000939
Iteration 83/1000 | Loss: 0.00000938
Iteration 84/1000 | Loss: 0.00000938
Iteration 85/1000 | Loss: 0.00000937
Iteration 86/1000 | Loss: 0.00000937
Iteration 87/1000 | Loss: 0.00000937
Iteration 88/1000 | Loss: 0.00000937
Iteration 89/1000 | Loss: 0.00000936
Iteration 90/1000 | Loss: 0.00000936
Iteration 91/1000 | Loss: 0.00000936
Iteration 92/1000 | Loss: 0.00000936
Iteration 93/1000 | Loss: 0.00000936
Iteration 94/1000 | Loss: 0.00000936
Iteration 95/1000 | Loss: 0.00000936
Iteration 96/1000 | Loss: 0.00000936
Iteration 97/1000 | Loss: 0.00000935
Iteration 98/1000 | Loss: 0.00000935
Iteration 99/1000 | Loss: 0.00000934
Iteration 100/1000 | Loss: 0.00000934
Iteration 101/1000 | Loss: 0.00000933
Iteration 102/1000 | Loss: 0.00000933
Iteration 103/1000 | Loss: 0.00000933
Iteration 104/1000 | Loss: 0.00000933
Iteration 105/1000 | Loss: 0.00000933
Iteration 106/1000 | Loss: 0.00000932
Iteration 107/1000 | Loss: 0.00000932
Iteration 108/1000 | Loss: 0.00000932
Iteration 109/1000 | Loss: 0.00000931
Iteration 110/1000 | Loss: 0.00000931
Iteration 111/1000 | Loss: 0.00000930
Iteration 112/1000 | Loss: 0.00000930
Iteration 113/1000 | Loss: 0.00000930
Iteration 114/1000 | Loss: 0.00000930
Iteration 115/1000 | Loss: 0.00000930
Iteration 116/1000 | Loss: 0.00000929
Iteration 117/1000 | Loss: 0.00000929
Iteration 118/1000 | Loss: 0.00000929
Iteration 119/1000 | Loss: 0.00000929
Iteration 120/1000 | Loss: 0.00000929
Iteration 121/1000 | Loss: 0.00000929
Iteration 122/1000 | Loss: 0.00000928
Iteration 123/1000 | Loss: 0.00000928
Iteration 124/1000 | Loss: 0.00000928
Iteration 125/1000 | Loss: 0.00000928
Iteration 126/1000 | Loss: 0.00000927
Iteration 127/1000 | Loss: 0.00000927
Iteration 128/1000 | Loss: 0.00000927
Iteration 129/1000 | Loss: 0.00000927
Iteration 130/1000 | Loss: 0.00000927
Iteration 131/1000 | Loss: 0.00000927
Iteration 132/1000 | Loss: 0.00000927
Iteration 133/1000 | Loss: 0.00000926
Iteration 134/1000 | Loss: 0.00000926
Iteration 135/1000 | Loss: 0.00000926
Iteration 136/1000 | Loss: 0.00000926
Iteration 137/1000 | Loss: 0.00000925
Iteration 138/1000 | Loss: 0.00000925
Iteration 139/1000 | Loss: 0.00000924
Iteration 140/1000 | Loss: 0.00000924
Iteration 141/1000 | Loss: 0.00000924
Iteration 142/1000 | Loss: 0.00000923
Iteration 143/1000 | Loss: 0.00000923
Iteration 144/1000 | Loss: 0.00000923
Iteration 145/1000 | Loss: 0.00000923
Iteration 146/1000 | Loss: 0.00000922
Iteration 147/1000 | Loss: 0.00000922
Iteration 148/1000 | Loss: 0.00000922
Iteration 149/1000 | Loss: 0.00000921
Iteration 150/1000 | Loss: 0.00000921
Iteration 151/1000 | Loss: 0.00000921
Iteration 152/1000 | Loss: 0.00000920
Iteration 153/1000 | Loss: 0.00000920
Iteration 154/1000 | Loss: 0.00000920
Iteration 155/1000 | Loss: 0.00000919
Iteration 156/1000 | Loss: 0.00000919
Iteration 157/1000 | Loss: 0.00000919
Iteration 158/1000 | Loss: 0.00000918
Iteration 159/1000 | Loss: 0.00000918
Iteration 160/1000 | Loss: 0.00000918
Iteration 161/1000 | Loss: 0.00000918
Iteration 162/1000 | Loss: 0.00000918
Iteration 163/1000 | Loss: 0.00000918
Iteration 164/1000 | Loss: 0.00000918
Iteration 165/1000 | Loss: 0.00000917
Iteration 166/1000 | Loss: 0.00000917
Iteration 167/1000 | Loss: 0.00000917
Iteration 168/1000 | Loss: 0.00000917
Iteration 169/1000 | Loss: 0.00000917
Iteration 170/1000 | Loss: 0.00000917
Iteration 171/1000 | Loss: 0.00000917
Iteration 172/1000 | Loss: 0.00000917
Iteration 173/1000 | Loss: 0.00000917
Iteration 174/1000 | Loss: 0.00000917
Iteration 175/1000 | Loss: 0.00000916
Iteration 176/1000 | Loss: 0.00000916
Iteration 177/1000 | Loss: 0.00000916
Iteration 178/1000 | Loss: 0.00000916
Iteration 179/1000 | Loss: 0.00000916
Iteration 180/1000 | Loss: 0.00000916
Iteration 181/1000 | Loss: 0.00000916
Iteration 182/1000 | Loss: 0.00000916
Iteration 183/1000 | Loss: 0.00000916
Iteration 184/1000 | Loss: 0.00000916
Iteration 185/1000 | Loss: 0.00000916
Iteration 186/1000 | Loss: 0.00000916
Iteration 187/1000 | Loss: 0.00000916
Iteration 188/1000 | Loss: 0.00000916
Iteration 189/1000 | Loss: 0.00000915
Iteration 190/1000 | Loss: 0.00000915
Iteration 191/1000 | Loss: 0.00000915
Iteration 192/1000 | Loss: 0.00000915
Iteration 193/1000 | Loss: 0.00000915
Iteration 194/1000 | Loss: 0.00000915
Iteration 195/1000 | Loss: 0.00000915
Iteration 196/1000 | Loss: 0.00000915
Iteration 197/1000 | Loss: 0.00000915
Iteration 198/1000 | Loss: 0.00000915
Iteration 199/1000 | Loss: 0.00000915
Iteration 200/1000 | Loss: 0.00000915
Iteration 201/1000 | Loss: 0.00000915
Iteration 202/1000 | Loss: 0.00000915
Iteration 203/1000 | Loss: 0.00000914
Iteration 204/1000 | Loss: 0.00000914
Iteration 205/1000 | Loss: 0.00000914
Iteration 206/1000 | Loss: 0.00000914
Iteration 207/1000 | Loss: 0.00000914
Iteration 208/1000 | Loss: 0.00000914
Iteration 209/1000 | Loss: 0.00000914
Iteration 210/1000 | Loss: 0.00000914
Iteration 211/1000 | Loss: 0.00000914
Iteration 212/1000 | Loss: 0.00000914
Iteration 213/1000 | Loss: 0.00000914
Iteration 214/1000 | Loss: 0.00000914
Iteration 215/1000 | Loss: 0.00000914
Iteration 216/1000 | Loss: 0.00000914
Iteration 217/1000 | Loss: 0.00000914
Iteration 218/1000 | Loss: 0.00000914
Iteration 219/1000 | Loss: 0.00000914
Iteration 220/1000 | Loss: 0.00000914
Iteration 221/1000 | Loss: 0.00000914
Iteration 222/1000 | Loss: 0.00000914
Iteration 223/1000 | Loss: 0.00000914
Iteration 224/1000 | Loss: 0.00000914
Iteration 225/1000 | Loss: 0.00000914
Iteration 226/1000 | Loss: 0.00000914
Iteration 227/1000 | Loss: 0.00000914
Iteration 228/1000 | Loss: 0.00000914
Iteration 229/1000 | Loss: 0.00000914
Iteration 230/1000 | Loss: 0.00000914
Iteration 231/1000 | Loss: 0.00000914
Iteration 232/1000 | Loss: 0.00000914
Iteration 233/1000 | Loss: 0.00000914
Iteration 234/1000 | Loss: 0.00000914
Iteration 235/1000 | Loss: 0.00000914
Iteration 236/1000 | Loss: 0.00000914
Iteration 237/1000 | Loss: 0.00000914
Iteration 238/1000 | Loss: 0.00000914
Iteration 239/1000 | Loss: 0.00000914
Iteration 240/1000 | Loss: 0.00000914
Iteration 241/1000 | Loss: 0.00000914
Iteration 242/1000 | Loss: 0.00000914
Iteration 243/1000 | Loss: 0.00000914
Iteration 244/1000 | Loss: 0.00000914
Iteration 245/1000 | Loss: 0.00000914
Iteration 246/1000 | Loss: 0.00000914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [9.13762141863117e-06, 9.13762141863117e-06, 9.13762141863117e-06, 9.13762141863117e-06, 9.13762141863117e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.13762141863117e-06

Optimization complete. Final v2v error: 2.58406138420105 mm

Highest mean error: 2.7781825065612793 mm for frame 140

Lowest mean error: 2.424445390701294 mm for frame 13

Saving results

Total time: 48.70859241485596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00489055
Iteration 2/25 | Loss: 0.00136960
Iteration 3/25 | Loss: 0.00122291
Iteration 4/25 | Loss: 0.00120684
Iteration 5/25 | Loss: 0.00120323
Iteration 6/25 | Loss: 0.00120266
Iteration 7/25 | Loss: 0.00120266
Iteration 8/25 | Loss: 0.00120266
Iteration 9/25 | Loss: 0.00120266
Iteration 10/25 | Loss: 0.00120266
Iteration 11/25 | Loss: 0.00120266
Iteration 12/25 | Loss: 0.00120266
Iteration 13/25 | Loss: 0.00120266
Iteration 14/25 | Loss: 0.00120266
Iteration 15/25 | Loss: 0.00120266
Iteration 16/25 | Loss: 0.00120266
Iteration 17/25 | Loss: 0.00120266
Iteration 18/25 | Loss: 0.00120266
Iteration 19/25 | Loss: 0.00120266
Iteration 20/25 | Loss: 0.00120266
Iteration 21/25 | Loss: 0.00120266
Iteration 22/25 | Loss: 0.00120266
Iteration 23/25 | Loss: 0.00120266
Iteration 24/25 | Loss: 0.00120266
Iteration 25/25 | Loss: 0.00120266

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35888982
Iteration 2/25 | Loss: 0.00087466
Iteration 3/25 | Loss: 0.00087465
Iteration 4/25 | Loss: 0.00087465
Iteration 5/25 | Loss: 0.00087465
Iteration 6/25 | Loss: 0.00087465
Iteration 7/25 | Loss: 0.00087465
Iteration 8/25 | Loss: 0.00087465
Iteration 9/25 | Loss: 0.00087465
Iteration 10/25 | Loss: 0.00087465
Iteration 11/25 | Loss: 0.00087465
Iteration 12/25 | Loss: 0.00087465
Iteration 13/25 | Loss: 0.00087465
Iteration 14/25 | Loss: 0.00087465
Iteration 15/25 | Loss: 0.00087465
Iteration 16/25 | Loss: 0.00087465
Iteration 17/25 | Loss: 0.00087465
Iteration 18/25 | Loss: 0.00087465
Iteration 19/25 | Loss: 0.00087465
Iteration 20/25 | Loss: 0.00087465
Iteration 21/25 | Loss: 0.00087465
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008746469975449145, 0.0008746469975449145, 0.0008746469975449145, 0.0008746469975449145, 0.0008746469975449145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008746469975449145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087465
Iteration 2/1000 | Loss: 0.00004230
Iteration 3/1000 | Loss: 0.00002585
Iteration 4/1000 | Loss: 0.00002354
Iteration 5/1000 | Loss: 0.00002220
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002079
Iteration 8/1000 | Loss: 0.00002036
Iteration 9/1000 | Loss: 0.00002013
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001992
Iteration 12/1000 | Loss: 0.00001989
Iteration 13/1000 | Loss: 0.00001988
Iteration 14/1000 | Loss: 0.00001988
Iteration 15/1000 | Loss: 0.00001975
Iteration 16/1000 | Loss: 0.00001971
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001969
Iteration 19/1000 | Loss: 0.00001963
Iteration 20/1000 | Loss: 0.00001963
Iteration 21/1000 | Loss: 0.00001960
Iteration 22/1000 | Loss: 0.00001959
Iteration 23/1000 | Loss: 0.00001958
Iteration 24/1000 | Loss: 0.00001958
Iteration 25/1000 | Loss: 0.00001957
Iteration 26/1000 | Loss: 0.00001956
Iteration 27/1000 | Loss: 0.00001955
Iteration 28/1000 | Loss: 0.00001955
Iteration 29/1000 | Loss: 0.00001955
Iteration 30/1000 | Loss: 0.00001955
Iteration 31/1000 | Loss: 0.00001955
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001953
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001951
Iteration 37/1000 | Loss: 0.00001951
Iteration 38/1000 | Loss: 0.00001950
Iteration 39/1000 | Loss: 0.00001949
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001948
Iteration 42/1000 | Loss: 0.00001948
Iteration 43/1000 | Loss: 0.00001947
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001946
Iteration 46/1000 | Loss: 0.00001946
Iteration 47/1000 | Loss: 0.00001946
Iteration 48/1000 | Loss: 0.00001946
Iteration 49/1000 | Loss: 0.00001946
Iteration 50/1000 | Loss: 0.00001945
Iteration 51/1000 | Loss: 0.00001945
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001944
Iteration 57/1000 | Loss: 0.00001943
Iteration 58/1000 | Loss: 0.00001943
Iteration 59/1000 | Loss: 0.00001943
Iteration 60/1000 | Loss: 0.00001943
Iteration 61/1000 | Loss: 0.00001943
Iteration 62/1000 | Loss: 0.00001943
Iteration 63/1000 | Loss: 0.00001942
Iteration 64/1000 | Loss: 0.00001942
Iteration 65/1000 | Loss: 0.00001941
Iteration 66/1000 | Loss: 0.00001941
Iteration 67/1000 | Loss: 0.00001941
Iteration 68/1000 | Loss: 0.00001940
Iteration 69/1000 | Loss: 0.00001940
Iteration 70/1000 | Loss: 0.00001939
Iteration 71/1000 | Loss: 0.00001939
Iteration 72/1000 | Loss: 0.00001939
Iteration 73/1000 | Loss: 0.00001939
Iteration 74/1000 | Loss: 0.00001938
Iteration 75/1000 | Loss: 0.00001938
Iteration 76/1000 | Loss: 0.00001938
Iteration 77/1000 | Loss: 0.00001938
Iteration 78/1000 | Loss: 0.00001938
Iteration 79/1000 | Loss: 0.00001938
Iteration 80/1000 | Loss: 0.00001937
Iteration 81/1000 | Loss: 0.00001937
Iteration 82/1000 | Loss: 0.00001936
Iteration 83/1000 | Loss: 0.00001936
Iteration 84/1000 | Loss: 0.00001936
Iteration 85/1000 | Loss: 0.00001935
Iteration 86/1000 | Loss: 0.00001935
Iteration 87/1000 | Loss: 0.00001935
Iteration 88/1000 | Loss: 0.00001935
Iteration 89/1000 | Loss: 0.00001935
Iteration 90/1000 | Loss: 0.00001935
Iteration 91/1000 | Loss: 0.00001934
Iteration 92/1000 | Loss: 0.00001934
Iteration 93/1000 | Loss: 0.00001934
Iteration 94/1000 | Loss: 0.00001934
Iteration 95/1000 | Loss: 0.00001933
Iteration 96/1000 | Loss: 0.00001933
Iteration 97/1000 | Loss: 0.00001933
Iteration 98/1000 | Loss: 0.00001933
Iteration 99/1000 | Loss: 0.00001933
Iteration 100/1000 | Loss: 0.00001933
Iteration 101/1000 | Loss: 0.00001933
Iteration 102/1000 | Loss: 0.00001932
Iteration 103/1000 | Loss: 0.00001932
Iteration 104/1000 | Loss: 0.00001932
Iteration 105/1000 | Loss: 0.00001932
Iteration 106/1000 | Loss: 0.00001932
Iteration 107/1000 | Loss: 0.00001932
Iteration 108/1000 | Loss: 0.00001932
Iteration 109/1000 | Loss: 0.00001932
Iteration 110/1000 | Loss: 0.00001932
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001931
Iteration 114/1000 | Loss: 0.00001931
Iteration 115/1000 | Loss: 0.00001931
Iteration 116/1000 | Loss: 0.00001931
Iteration 117/1000 | Loss: 0.00001930
Iteration 118/1000 | Loss: 0.00001930
Iteration 119/1000 | Loss: 0.00001930
Iteration 120/1000 | Loss: 0.00001930
Iteration 121/1000 | Loss: 0.00001930
Iteration 122/1000 | Loss: 0.00001930
Iteration 123/1000 | Loss: 0.00001930
Iteration 124/1000 | Loss: 0.00001929
Iteration 125/1000 | Loss: 0.00001929
Iteration 126/1000 | Loss: 0.00001929
Iteration 127/1000 | Loss: 0.00001929
Iteration 128/1000 | Loss: 0.00001929
Iteration 129/1000 | Loss: 0.00001929
Iteration 130/1000 | Loss: 0.00001929
Iteration 131/1000 | Loss: 0.00001929
Iteration 132/1000 | Loss: 0.00001929
Iteration 133/1000 | Loss: 0.00001928
Iteration 134/1000 | Loss: 0.00001928
Iteration 135/1000 | Loss: 0.00001928
Iteration 136/1000 | Loss: 0.00001928
Iteration 137/1000 | Loss: 0.00001928
Iteration 138/1000 | Loss: 0.00001928
Iteration 139/1000 | Loss: 0.00001928
Iteration 140/1000 | Loss: 0.00001928
Iteration 141/1000 | Loss: 0.00001927
Iteration 142/1000 | Loss: 0.00001927
Iteration 143/1000 | Loss: 0.00001927
Iteration 144/1000 | Loss: 0.00001927
Iteration 145/1000 | Loss: 0.00001926
Iteration 146/1000 | Loss: 0.00001926
Iteration 147/1000 | Loss: 0.00001926
Iteration 148/1000 | Loss: 0.00001925
Iteration 149/1000 | Loss: 0.00001925
Iteration 150/1000 | Loss: 0.00001925
Iteration 151/1000 | Loss: 0.00001925
Iteration 152/1000 | Loss: 0.00001925
Iteration 153/1000 | Loss: 0.00001925
Iteration 154/1000 | Loss: 0.00001925
Iteration 155/1000 | Loss: 0.00001925
Iteration 156/1000 | Loss: 0.00001924
Iteration 157/1000 | Loss: 0.00001924
Iteration 158/1000 | Loss: 0.00001924
Iteration 159/1000 | Loss: 0.00001924
Iteration 160/1000 | Loss: 0.00001924
Iteration 161/1000 | Loss: 0.00001924
Iteration 162/1000 | Loss: 0.00001924
Iteration 163/1000 | Loss: 0.00001924
Iteration 164/1000 | Loss: 0.00001924
Iteration 165/1000 | Loss: 0.00001924
Iteration 166/1000 | Loss: 0.00001924
Iteration 167/1000 | Loss: 0.00001923
Iteration 168/1000 | Loss: 0.00001923
Iteration 169/1000 | Loss: 0.00001923
Iteration 170/1000 | Loss: 0.00001923
Iteration 171/1000 | Loss: 0.00001923
Iteration 172/1000 | Loss: 0.00001923
Iteration 173/1000 | Loss: 0.00001923
Iteration 174/1000 | Loss: 0.00001923
Iteration 175/1000 | Loss: 0.00001923
Iteration 176/1000 | Loss: 0.00001923
Iteration 177/1000 | Loss: 0.00001923
Iteration 178/1000 | Loss: 0.00001922
Iteration 179/1000 | Loss: 0.00001922
Iteration 180/1000 | Loss: 0.00001922
Iteration 181/1000 | Loss: 0.00001922
Iteration 182/1000 | Loss: 0.00001922
Iteration 183/1000 | Loss: 0.00001922
Iteration 184/1000 | Loss: 0.00001922
Iteration 185/1000 | Loss: 0.00001922
Iteration 186/1000 | Loss: 0.00001921
Iteration 187/1000 | Loss: 0.00001921
Iteration 188/1000 | Loss: 0.00001921
Iteration 189/1000 | Loss: 0.00001921
Iteration 190/1000 | Loss: 0.00001921
Iteration 191/1000 | Loss: 0.00001921
Iteration 192/1000 | Loss: 0.00001921
Iteration 193/1000 | Loss: 0.00001921
Iteration 194/1000 | Loss: 0.00001921
Iteration 195/1000 | Loss: 0.00001921
Iteration 196/1000 | Loss: 0.00001921
Iteration 197/1000 | Loss: 0.00001921
Iteration 198/1000 | Loss: 0.00001920
Iteration 199/1000 | Loss: 0.00001920
Iteration 200/1000 | Loss: 0.00001920
Iteration 201/1000 | Loss: 0.00001920
Iteration 202/1000 | Loss: 0.00001920
Iteration 203/1000 | Loss: 0.00001920
Iteration 204/1000 | Loss: 0.00001920
Iteration 205/1000 | Loss: 0.00001920
Iteration 206/1000 | Loss: 0.00001919
Iteration 207/1000 | Loss: 0.00001919
Iteration 208/1000 | Loss: 0.00001919
Iteration 209/1000 | Loss: 0.00001919
Iteration 210/1000 | Loss: 0.00001919
Iteration 211/1000 | Loss: 0.00001919
Iteration 212/1000 | Loss: 0.00001919
Iteration 213/1000 | Loss: 0.00001918
Iteration 214/1000 | Loss: 0.00001918
Iteration 215/1000 | Loss: 0.00001918
Iteration 216/1000 | Loss: 0.00001918
Iteration 217/1000 | Loss: 0.00001918
Iteration 218/1000 | Loss: 0.00001918
Iteration 219/1000 | Loss: 0.00001918
Iteration 220/1000 | Loss: 0.00001918
Iteration 221/1000 | Loss: 0.00001918
Iteration 222/1000 | Loss: 0.00001918
Iteration 223/1000 | Loss: 0.00001918
Iteration 224/1000 | Loss: 0.00001918
Iteration 225/1000 | Loss: 0.00001918
Iteration 226/1000 | Loss: 0.00001918
Iteration 227/1000 | Loss: 0.00001918
Iteration 228/1000 | Loss: 0.00001917
Iteration 229/1000 | Loss: 0.00001917
Iteration 230/1000 | Loss: 0.00001917
Iteration 231/1000 | Loss: 0.00001917
Iteration 232/1000 | Loss: 0.00001917
Iteration 233/1000 | Loss: 0.00001917
Iteration 234/1000 | Loss: 0.00001917
Iteration 235/1000 | Loss: 0.00001917
Iteration 236/1000 | Loss: 0.00001917
Iteration 237/1000 | Loss: 0.00001917
Iteration 238/1000 | Loss: 0.00001917
Iteration 239/1000 | Loss: 0.00001917
Iteration 240/1000 | Loss: 0.00001917
Iteration 241/1000 | Loss: 0.00001917
Iteration 242/1000 | Loss: 0.00001917
Iteration 243/1000 | Loss: 0.00001917
Iteration 244/1000 | Loss: 0.00001917
Iteration 245/1000 | Loss: 0.00001917
Iteration 246/1000 | Loss: 0.00001917
Iteration 247/1000 | Loss: 0.00001916
Iteration 248/1000 | Loss: 0.00001916
Iteration 249/1000 | Loss: 0.00001916
Iteration 250/1000 | Loss: 0.00001916
Iteration 251/1000 | Loss: 0.00001916
Iteration 252/1000 | Loss: 0.00001916
Iteration 253/1000 | Loss: 0.00001916
Iteration 254/1000 | Loss: 0.00001916
Iteration 255/1000 | Loss: 0.00001916
Iteration 256/1000 | Loss: 0.00001915
Iteration 257/1000 | Loss: 0.00001915
Iteration 258/1000 | Loss: 0.00001915
Iteration 259/1000 | Loss: 0.00001915
Iteration 260/1000 | Loss: 0.00001915
Iteration 261/1000 | Loss: 0.00001915
Iteration 262/1000 | Loss: 0.00001915
Iteration 263/1000 | Loss: 0.00001915
Iteration 264/1000 | Loss: 0.00001915
Iteration 265/1000 | Loss: 0.00001915
Iteration 266/1000 | Loss: 0.00001915
Iteration 267/1000 | Loss: 0.00001915
Iteration 268/1000 | Loss: 0.00001915
Iteration 269/1000 | Loss: 0.00001915
Iteration 270/1000 | Loss: 0.00001915
Iteration 271/1000 | Loss: 0.00001915
Iteration 272/1000 | Loss: 0.00001915
Iteration 273/1000 | Loss: 0.00001914
Iteration 274/1000 | Loss: 0.00001914
Iteration 275/1000 | Loss: 0.00001914
Iteration 276/1000 | Loss: 0.00001914
Iteration 277/1000 | Loss: 0.00001914
Iteration 278/1000 | Loss: 0.00001914
Iteration 279/1000 | Loss: 0.00001914
Iteration 280/1000 | Loss: 0.00001914
Iteration 281/1000 | Loss: 0.00001914
Iteration 282/1000 | Loss: 0.00001914
Iteration 283/1000 | Loss: 0.00001914
Iteration 284/1000 | Loss: 0.00001914
Iteration 285/1000 | Loss: 0.00001914
Iteration 286/1000 | Loss: 0.00001913
Iteration 287/1000 | Loss: 0.00001913
Iteration 288/1000 | Loss: 0.00001913
Iteration 289/1000 | Loss: 0.00001913
Iteration 290/1000 | Loss: 0.00001913
Iteration 291/1000 | Loss: 0.00001913
Iteration 292/1000 | Loss: 0.00001913
Iteration 293/1000 | Loss: 0.00001913
Iteration 294/1000 | Loss: 0.00001913
Iteration 295/1000 | Loss: 0.00001913
Iteration 296/1000 | Loss: 0.00001913
Iteration 297/1000 | Loss: 0.00001913
Iteration 298/1000 | Loss: 0.00001913
Iteration 299/1000 | Loss: 0.00001913
Iteration 300/1000 | Loss: 0.00001913
Iteration 301/1000 | Loss: 0.00001912
Iteration 302/1000 | Loss: 0.00001912
Iteration 303/1000 | Loss: 0.00001912
Iteration 304/1000 | Loss: 0.00001912
Iteration 305/1000 | Loss: 0.00001912
Iteration 306/1000 | Loss: 0.00001912
Iteration 307/1000 | Loss: 0.00001912
Iteration 308/1000 | Loss: 0.00001912
Iteration 309/1000 | Loss: 0.00001912
Iteration 310/1000 | Loss: 0.00001912
Iteration 311/1000 | Loss: 0.00001912
Iteration 312/1000 | Loss: 0.00001912
Iteration 313/1000 | Loss: 0.00001912
Iteration 314/1000 | Loss: 0.00001912
Iteration 315/1000 | Loss: 0.00001912
Iteration 316/1000 | Loss: 0.00001912
Iteration 317/1000 | Loss: 0.00001912
Iteration 318/1000 | Loss: 0.00001912
Iteration 319/1000 | Loss: 0.00001912
Iteration 320/1000 | Loss: 0.00001911
Iteration 321/1000 | Loss: 0.00001911
Iteration 322/1000 | Loss: 0.00001911
Iteration 323/1000 | Loss: 0.00001911
Iteration 324/1000 | Loss: 0.00001911
Iteration 325/1000 | Loss: 0.00001911
Iteration 326/1000 | Loss: 0.00001911
Iteration 327/1000 | Loss: 0.00001911
Iteration 328/1000 | Loss: 0.00001911
Iteration 329/1000 | Loss: 0.00001911
Iteration 330/1000 | Loss: 0.00001911
Iteration 331/1000 | Loss: 0.00001911
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 331. Stopping optimization.
Last 5 losses: [1.9113349480903707e-05, 1.9113349480903707e-05, 1.9113349480903707e-05, 1.9113349480903707e-05, 1.9113349480903707e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9113349480903707e-05

Optimization complete. Final v2v error: 3.5477473735809326 mm

Highest mean error: 4.163183212280273 mm for frame 132

Lowest mean error: 3.023470401763916 mm for frame 1

Saving results

Total time: 48.403520584106445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00910053
Iteration 2/25 | Loss: 0.00127132
Iteration 3/25 | Loss: 0.00115683
Iteration 4/25 | Loss: 0.00113568
Iteration 5/25 | Loss: 0.00112837
Iteration 6/25 | Loss: 0.00112663
Iteration 7/25 | Loss: 0.00112658
Iteration 8/25 | Loss: 0.00112658
Iteration 9/25 | Loss: 0.00112658
Iteration 10/25 | Loss: 0.00112658
Iteration 11/25 | Loss: 0.00112658
Iteration 12/25 | Loss: 0.00112658
Iteration 13/25 | Loss: 0.00112658
Iteration 14/25 | Loss: 0.00112658
Iteration 15/25 | Loss: 0.00112658
Iteration 16/25 | Loss: 0.00112658
Iteration 17/25 | Loss: 0.00112658
Iteration 18/25 | Loss: 0.00112658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011265825014561415, 0.0011265825014561415, 0.0011265825014561415, 0.0011265825014561415, 0.0011265825014561415]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011265825014561415

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34928870
Iteration 2/25 | Loss: 0.00085207
Iteration 3/25 | Loss: 0.00085206
Iteration 4/25 | Loss: 0.00085206
Iteration 5/25 | Loss: 0.00085206
Iteration 6/25 | Loss: 0.00085206
Iteration 7/25 | Loss: 0.00085206
Iteration 8/25 | Loss: 0.00085206
Iteration 9/25 | Loss: 0.00085206
Iteration 10/25 | Loss: 0.00085206
Iteration 11/25 | Loss: 0.00085206
Iteration 12/25 | Loss: 0.00085206
Iteration 13/25 | Loss: 0.00085206
Iteration 14/25 | Loss: 0.00085206
Iteration 15/25 | Loss: 0.00085206
Iteration 16/25 | Loss: 0.00085206
Iteration 17/25 | Loss: 0.00085206
Iteration 18/25 | Loss: 0.00085206
Iteration 19/25 | Loss: 0.00085206
Iteration 20/25 | Loss: 0.00085206
Iteration 21/25 | Loss: 0.00085206
Iteration 22/25 | Loss: 0.00085206
Iteration 23/25 | Loss: 0.00085206
Iteration 24/25 | Loss: 0.00085206
Iteration 25/25 | Loss: 0.00085206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085206
Iteration 2/1000 | Loss: 0.00003558
Iteration 3/1000 | Loss: 0.00002442
Iteration 4/1000 | Loss: 0.00002106
Iteration 5/1000 | Loss: 0.00001989
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001858
Iteration 8/1000 | Loss: 0.00001813
Iteration 9/1000 | Loss: 0.00001783
Iteration 10/1000 | Loss: 0.00001779
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001751
Iteration 13/1000 | Loss: 0.00001746
Iteration 14/1000 | Loss: 0.00001730
Iteration 15/1000 | Loss: 0.00001727
Iteration 16/1000 | Loss: 0.00001726
Iteration 17/1000 | Loss: 0.00001723
Iteration 18/1000 | Loss: 0.00001721
Iteration 19/1000 | Loss: 0.00001720
Iteration 20/1000 | Loss: 0.00001720
Iteration 21/1000 | Loss: 0.00001719
Iteration 22/1000 | Loss: 0.00001719
Iteration 23/1000 | Loss: 0.00001716
Iteration 24/1000 | Loss: 0.00001715
Iteration 25/1000 | Loss: 0.00001715
Iteration 26/1000 | Loss: 0.00001713
Iteration 27/1000 | Loss: 0.00001712
Iteration 28/1000 | Loss: 0.00001709
Iteration 29/1000 | Loss: 0.00001709
Iteration 30/1000 | Loss: 0.00001709
Iteration 31/1000 | Loss: 0.00001707
Iteration 32/1000 | Loss: 0.00001706
Iteration 33/1000 | Loss: 0.00001705
Iteration 34/1000 | Loss: 0.00001705
Iteration 35/1000 | Loss: 0.00001704
Iteration 36/1000 | Loss: 0.00001704
Iteration 37/1000 | Loss: 0.00001703
Iteration 38/1000 | Loss: 0.00001702
Iteration 39/1000 | Loss: 0.00001701
Iteration 40/1000 | Loss: 0.00001700
Iteration 41/1000 | Loss: 0.00001700
Iteration 42/1000 | Loss: 0.00001700
Iteration 43/1000 | Loss: 0.00001697
Iteration 44/1000 | Loss: 0.00001697
Iteration 45/1000 | Loss: 0.00001697
Iteration 46/1000 | Loss: 0.00001697
Iteration 47/1000 | Loss: 0.00001697
Iteration 48/1000 | Loss: 0.00001697
Iteration 49/1000 | Loss: 0.00001696
Iteration 50/1000 | Loss: 0.00001696
Iteration 51/1000 | Loss: 0.00001696
Iteration 52/1000 | Loss: 0.00001696
Iteration 53/1000 | Loss: 0.00001696
Iteration 54/1000 | Loss: 0.00001696
Iteration 55/1000 | Loss: 0.00001696
Iteration 56/1000 | Loss: 0.00001696
Iteration 57/1000 | Loss: 0.00001696
Iteration 58/1000 | Loss: 0.00001695
Iteration 59/1000 | Loss: 0.00001695
Iteration 60/1000 | Loss: 0.00001694
Iteration 61/1000 | Loss: 0.00001693
Iteration 62/1000 | Loss: 0.00001693
Iteration 63/1000 | Loss: 0.00001693
Iteration 64/1000 | Loss: 0.00001693
Iteration 65/1000 | Loss: 0.00001692
Iteration 66/1000 | Loss: 0.00001692
Iteration 67/1000 | Loss: 0.00001691
Iteration 68/1000 | Loss: 0.00001691
Iteration 69/1000 | Loss: 0.00001691
Iteration 70/1000 | Loss: 0.00001691
Iteration 71/1000 | Loss: 0.00001690
Iteration 72/1000 | Loss: 0.00001690
Iteration 73/1000 | Loss: 0.00001690
Iteration 74/1000 | Loss: 0.00001689
Iteration 75/1000 | Loss: 0.00001689
Iteration 76/1000 | Loss: 0.00001689
Iteration 77/1000 | Loss: 0.00001688
Iteration 78/1000 | Loss: 0.00001688
Iteration 79/1000 | Loss: 0.00001688
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001687
Iteration 82/1000 | Loss: 0.00001686
Iteration 83/1000 | Loss: 0.00001686
Iteration 84/1000 | Loss: 0.00001686
Iteration 85/1000 | Loss: 0.00001685
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001685
Iteration 88/1000 | Loss: 0.00001685
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00001683
Iteration 93/1000 | Loss: 0.00001683
Iteration 94/1000 | Loss: 0.00001683
Iteration 95/1000 | Loss: 0.00001683
Iteration 96/1000 | Loss: 0.00001682
Iteration 97/1000 | Loss: 0.00001682
Iteration 98/1000 | Loss: 0.00001682
Iteration 99/1000 | Loss: 0.00001681
Iteration 100/1000 | Loss: 0.00001681
Iteration 101/1000 | Loss: 0.00001681
Iteration 102/1000 | Loss: 0.00001680
Iteration 103/1000 | Loss: 0.00001680
Iteration 104/1000 | Loss: 0.00001680
Iteration 105/1000 | Loss: 0.00001680
Iteration 106/1000 | Loss: 0.00001680
Iteration 107/1000 | Loss: 0.00001679
Iteration 108/1000 | Loss: 0.00001679
Iteration 109/1000 | Loss: 0.00001679
Iteration 110/1000 | Loss: 0.00001679
Iteration 111/1000 | Loss: 0.00001679
Iteration 112/1000 | Loss: 0.00001679
Iteration 113/1000 | Loss: 0.00001679
Iteration 114/1000 | Loss: 0.00001679
Iteration 115/1000 | Loss: 0.00001679
Iteration 116/1000 | Loss: 0.00001679
Iteration 117/1000 | Loss: 0.00001679
Iteration 118/1000 | Loss: 0.00001679
Iteration 119/1000 | Loss: 0.00001679
Iteration 120/1000 | Loss: 0.00001679
Iteration 121/1000 | Loss: 0.00001679
Iteration 122/1000 | Loss: 0.00001679
Iteration 123/1000 | Loss: 0.00001679
Iteration 124/1000 | Loss: 0.00001679
Iteration 125/1000 | Loss: 0.00001679
Iteration 126/1000 | Loss: 0.00001678
Iteration 127/1000 | Loss: 0.00001678
Iteration 128/1000 | Loss: 0.00001677
Iteration 129/1000 | Loss: 0.00001677
Iteration 130/1000 | Loss: 0.00001677
Iteration 131/1000 | Loss: 0.00001677
Iteration 132/1000 | Loss: 0.00001677
Iteration 133/1000 | Loss: 0.00001677
Iteration 134/1000 | Loss: 0.00001676
Iteration 135/1000 | Loss: 0.00001676
Iteration 136/1000 | Loss: 0.00001676
Iteration 137/1000 | Loss: 0.00001676
Iteration 138/1000 | Loss: 0.00001676
Iteration 139/1000 | Loss: 0.00001675
Iteration 140/1000 | Loss: 0.00001675
Iteration 141/1000 | Loss: 0.00001675
Iteration 142/1000 | Loss: 0.00001675
Iteration 143/1000 | Loss: 0.00001675
Iteration 144/1000 | Loss: 0.00001675
Iteration 145/1000 | Loss: 0.00001675
Iteration 146/1000 | Loss: 0.00001675
Iteration 147/1000 | Loss: 0.00001675
Iteration 148/1000 | Loss: 0.00001675
Iteration 149/1000 | Loss: 0.00001674
Iteration 150/1000 | Loss: 0.00001674
Iteration 151/1000 | Loss: 0.00001674
Iteration 152/1000 | Loss: 0.00001674
Iteration 153/1000 | Loss: 0.00001673
Iteration 154/1000 | Loss: 0.00001673
Iteration 155/1000 | Loss: 0.00001673
Iteration 156/1000 | Loss: 0.00001673
Iteration 157/1000 | Loss: 0.00001673
Iteration 158/1000 | Loss: 0.00001673
Iteration 159/1000 | Loss: 0.00001672
Iteration 160/1000 | Loss: 0.00001672
Iteration 161/1000 | Loss: 0.00001672
Iteration 162/1000 | Loss: 0.00001672
Iteration 163/1000 | Loss: 0.00001672
Iteration 164/1000 | Loss: 0.00001672
Iteration 165/1000 | Loss: 0.00001672
Iteration 166/1000 | Loss: 0.00001672
Iteration 167/1000 | Loss: 0.00001671
Iteration 168/1000 | Loss: 0.00001671
Iteration 169/1000 | Loss: 0.00001671
Iteration 170/1000 | Loss: 0.00001671
Iteration 171/1000 | Loss: 0.00001671
Iteration 172/1000 | Loss: 0.00001671
Iteration 173/1000 | Loss: 0.00001671
Iteration 174/1000 | Loss: 0.00001671
Iteration 175/1000 | Loss: 0.00001671
Iteration 176/1000 | Loss: 0.00001671
Iteration 177/1000 | Loss: 0.00001671
Iteration 178/1000 | Loss: 0.00001670
Iteration 179/1000 | Loss: 0.00001670
Iteration 180/1000 | Loss: 0.00001670
Iteration 181/1000 | Loss: 0.00001670
Iteration 182/1000 | Loss: 0.00001670
Iteration 183/1000 | Loss: 0.00001670
Iteration 184/1000 | Loss: 0.00001670
Iteration 185/1000 | Loss: 0.00001670
Iteration 186/1000 | Loss: 0.00001670
Iteration 187/1000 | Loss: 0.00001670
Iteration 188/1000 | Loss: 0.00001670
Iteration 189/1000 | Loss: 0.00001670
Iteration 190/1000 | Loss: 0.00001670
Iteration 191/1000 | Loss: 0.00001670
Iteration 192/1000 | Loss: 0.00001670
Iteration 193/1000 | Loss: 0.00001670
Iteration 194/1000 | Loss: 0.00001670
Iteration 195/1000 | Loss: 0.00001670
Iteration 196/1000 | Loss: 0.00001670
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [1.6701551430742256e-05, 1.6701551430742256e-05, 1.6701551430742256e-05, 1.6701551430742256e-05, 1.6701551430742256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6701551430742256e-05

Optimization complete. Final v2v error: 3.3877134323120117 mm

Highest mean error: 5.378125190734863 mm for frame 69

Lowest mean error: 2.89260196685791 mm for frame 95

Saving results

Total time: 42.16911864280701
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00747379
Iteration 2/25 | Loss: 0.00127183
Iteration 3/25 | Loss: 0.00107424
Iteration 4/25 | Loss: 0.00105260
Iteration 5/25 | Loss: 0.00104843
Iteration 6/25 | Loss: 0.00104792
Iteration 7/25 | Loss: 0.00104792
Iteration 8/25 | Loss: 0.00104792
Iteration 9/25 | Loss: 0.00104792
Iteration 10/25 | Loss: 0.00104792
Iteration 11/25 | Loss: 0.00104792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010479198535904288, 0.0010479198535904288, 0.0010479198535904288, 0.0010479198535904288, 0.0010479198535904288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010479198535904288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31191039
Iteration 2/25 | Loss: 0.00062087
Iteration 3/25 | Loss: 0.00062084
Iteration 4/25 | Loss: 0.00062084
Iteration 5/25 | Loss: 0.00062084
Iteration 6/25 | Loss: 0.00062084
Iteration 7/25 | Loss: 0.00062084
Iteration 8/25 | Loss: 0.00062084
Iteration 9/25 | Loss: 0.00062084
Iteration 10/25 | Loss: 0.00062084
Iteration 11/25 | Loss: 0.00062084
Iteration 12/25 | Loss: 0.00062084
Iteration 13/25 | Loss: 0.00062084
Iteration 14/25 | Loss: 0.00062084
Iteration 15/25 | Loss: 0.00062084
Iteration 16/25 | Loss: 0.00062084
Iteration 17/25 | Loss: 0.00062084
Iteration 18/25 | Loss: 0.00062084
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000620838429313153, 0.000620838429313153, 0.000620838429313153, 0.000620838429313153, 0.000620838429313153]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000620838429313153

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062084
Iteration 2/1000 | Loss: 0.00002661
Iteration 3/1000 | Loss: 0.00002066
Iteration 4/1000 | Loss: 0.00001799
Iteration 5/1000 | Loss: 0.00001686
Iteration 6/1000 | Loss: 0.00001584
Iteration 7/1000 | Loss: 0.00001512
Iteration 8/1000 | Loss: 0.00001460
Iteration 9/1000 | Loss: 0.00001419
Iteration 10/1000 | Loss: 0.00001385
Iteration 11/1000 | Loss: 0.00001363
Iteration 12/1000 | Loss: 0.00001337
Iteration 13/1000 | Loss: 0.00001320
Iteration 14/1000 | Loss: 0.00001299
Iteration 15/1000 | Loss: 0.00001275
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001249
Iteration 20/1000 | Loss: 0.00001249
Iteration 21/1000 | Loss: 0.00001248
Iteration 22/1000 | Loss: 0.00001247
Iteration 23/1000 | Loss: 0.00001247
Iteration 24/1000 | Loss: 0.00001247
Iteration 25/1000 | Loss: 0.00001247
Iteration 26/1000 | Loss: 0.00001245
Iteration 27/1000 | Loss: 0.00001245
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001245
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001245
Iteration 35/1000 | Loss: 0.00001244
Iteration 36/1000 | Loss: 0.00001244
Iteration 37/1000 | Loss: 0.00001244
Iteration 38/1000 | Loss: 0.00001244
Iteration 39/1000 | Loss: 0.00001244
Iteration 40/1000 | Loss: 0.00001244
Iteration 41/1000 | Loss: 0.00001243
Iteration 42/1000 | Loss: 0.00001242
Iteration 43/1000 | Loss: 0.00001242
Iteration 44/1000 | Loss: 0.00001242
Iteration 45/1000 | Loss: 0.00001242
Iteration 46/1000 | Loss: 0.00001241
Iteration 47/1000 | Loss: 0.00001241
Iteration 48/1000 | Loss: 0.00001241
Iteration 49/1000 | Loss: 0.00001241
Iteration 50/1000 | Loss: 0.00001240
Iteration 51/1000 | Loss: 0.00001240
Iteration 52/1000 | Loss: 0.00001240
Iteration 53/1000 | Loss: 0.00001240
Iteration 54/1000 | Loss: 0.00001240
Iteration 55/1000 | Loss: 0.00001240
Iteration 56/1000 | Loss: 0.00001240
Iteration 57/1000 | Loss: 0.00001240
Iteration 58/1000 | Loss: 0.00001240
Iteration 59/1000 | Loss: 0.00001239
Iteration 60/1000 | Loss: 0.00001239
Iteration 61/1000 | Loss: 0.00001238
Iteration 62/1000 | Loss: 0.00001238
Iteration 63/1000 | Loss: 0.00001237
Iteration 64/1000 | Loss: 0.00001237
Iteration 65/1000 | Loss: 0.00001236
Iteration 66/1000 | Loss: 0.00001236
Iteration 67/1000 | Loss: 0.00001236
Iteration 68/1000 | Loss: 0.00001235
Iteration 69/1000 | Loss: 0.00001235
Iteration 70/1000 | Loss: 0.00001234
Iteration 71/1000 | Loss: 0.00001233
Iteration 72/1000 | Loss: 0.00001233
Iteration 73/1000 | Loss: 0.00001233
Iteration 74/1000 | Loss: 0.00001232
Iteration 75/1000 | Loss: 0.00001232
Iteration 76/1000 | Loss: 0.00001231
Iteration 77/1000 | Loss: 0.00001231
Iteration 78/1000 | Loss: 0.00001231
Iteration 79/1000 | Loss: 0.00001231
Iteration 80/1000 | Loss: 0.00001231
Iteration 81/1000 | Loss: 0.00001231
Iteration 82/1000 | Loss: 0.00001231
Iteration 83/1000 | Loss: 0.00001230
Iteration 84/1000 | Loss: 0.00001230
Iteration 85/1000 | Loss: 0.00001230
Iteration 86/1000 | Loss: 0.00001229
Iteration 87/1000 | Loss: 0.00001229
Iteration 88/1000 | Loss: 0.00001229
Iteration 89/1000 | Loss: 0.00001229
Iteration 90/1000 | Loss: 0.00001228
Iteration 91/1000 | Loss: 0.00001228
Iteration 92/1000 | Loss: 0.00001228
Iteration 93/1000 | Loss: 0.00001228
Iteration 94/1000 | Loss: 0.00001228
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001226
Iteration 98/1000 | Loss: 0.00001226
Iteration 99/1000 | Loss: 0.00001226
Iteration 100/1000 | Loss: 0.00001224
Iteration 101/1000 | Loss: 0.00001224
Iteration 102/1000 | Loss: 0.00001224
Iteration 103/1000 | Loss: 0.00001224
Iteration 104/1000 | Loss: 0.00001223
Iteration 105/1000 | Loss: 0.00001223
Iteration 106/1000 | Loss: 0.00001223
Iteration 107/1000 | Loss: 0.00001223
Iteration 108/1000 | Loss: 0.00001223
Iteration 109/1000 | Loss: 0.00001223
Iteration 110/1000 | Loss: 0.00001223
Iteration 111/1000 | Loss: 0.00001223
Iteration 112/1000 | Loss: 0.00001223
Iteration 113/1000 | Loss: 0.00001222
Iteration 114/1000 | Loss: 0.00001222
Iteration 115/1000 | Loss: 0.00001221
Iteration 116/1000 | Loss: 0.00001221
Iteration 117/1000 | Loss: 0.00001221
Iteration 118/1000 | Loss: 0.00001221
Iteration 119/1000 | Loss: 0.00001221
Iteration 120/1000 | Loss: 0.00001220
Iteration 121/1000 | Loss: 0.00001220
Iteration 122/1000 | Loss: 0.00001220
Iteration 123/1000 | Loss: 0.00001220
Iteration 124/1000 | Loss: 0.00001220
Iteration 125/1000 | Loss: 0.00001220
Iteration 126/1000 | Loss: 0.00001220
Iteration 127/1000 | Loss: 0.00001220
Iteration 128/1000 | Loss: 0.00001220
Iteration 129/1000 | Loss: 0.00001220
Iteration 130/1000 | Loss: 0.00001219
Iteration 131/1000 | Loss: 0.00001219
Iteration 132/1000 | Loss: 0.00001219
Iteration 133/1000 | Loss: 0.00001219
Iteration 134/1000 | Loss: 0.00001219
Iteration 135/1000 | Loss: 0.00001218
Iteration 136/1000 | Loss: 0.00001218
Iteration 137/1000 | Loss: 0.00001218
Iteration 138/1000 | Loss: 0.00001218
Iteration 139/1000 | Loss: 0.00001218
Iteration 140/1000 | Loss: 0.00001218
Iteration 141/1000 | Loss: 0.00001218
Iteration 142/1000 | Loss: 0.00001218
Iteration 143/1000 | Loss: 0.00001218
Iteration 144/1000 | Loss: 0.00001218
Iteration 145/1000 | Loss: 0.00001217
Iteration 146/1000 | Loss: 0.00001217
Iteration 147/1000 | Loss: 0.00001217
Iteration 148/1000 | Loss: 0.00001217
Iteration 149/1000 | Loss: 0.00001217
Iteration 150/1000 | Loss: 0.00001217
Iteration 151/1000 | Loss: 0.00001217
Iteration 152/1000 | Loss: 0.00001217
Iteration 153/1000 | Loss: 0.00001217
Iteration 154/1000 | Loss: 0.00001217
Iteration 155/1000 | Loss: 0.00001217
Iteration 156/1000 | Loss: 0.00001217
Iteration 157/1000 | Loss: 0.00001217
Iteration 158/1000 | Loss: 0.00001217
Iteration 159/1000 | Loss: 0.00001217
Iteration 160/1000 | Loss: 0.00001217
Iteration 161/1000 | Loss: 0.00001217
Iteration 162/1000 | Loss: 0.00001217
Iteration 163/1000 | Loss: 0.00001217
Iteration 164/1000 | Loss: 0.00001216
Iteration 165/1000 | Loss: 0.00001216
Iteration 166/1000 | Loss: 0.00001216
Iteration 167/1000 | Loss: 0.00001216
Iteration 168/1000 | Loss: 0.00001216
Iteration 169/1000 | Loss: 0.00001216
Iteration 170/1000 | Loss: 0.00001216
Iteration 171/1000 | Loss: 0.00001216
Iteration 172/1000 | Loss: 0.00001215
Iteration 173/1000 | Loss: 0.00001215
Iteration 174/1000 | Loss: 0.00001215
Iteration 175/1000 | Loss: 0.00001215
Iteration 176/1000 | Loss: 0.00001215
Iteration 177/1000 | Loss: 0.00001215
Iteration 178/1000 | Loss: 0.00001215
Iteration 179/1000 | Loss: 0.00001215
Iteration 180/1000 | Loss: 0.00001215
Iteration 181/1000 | Loss: 0.00001215
Iteration 182/1000 | Loss: 0.00001215
Iteration 183/1000 | Loss: 0.00001215
Iteration 184/1000 | Loss: 0.00001215
Iteration 185/1000 | Loss: 0.00001215
Iteration 186/1000 | Loss: 0.00001215
Iteration 187/1000 | Loss: 0.00001215
Iteration 188/1000 | Loss: 0.00001215
Iteration 189/1000 | Loss: 0.00001215
Iteration 190/1000 | Loss: 0.00001215
Iteration 191/1000 | Loss: 0.00001215
Iteration 192/1000 | Loss: 0.00001215
Iteration 193/1000 | Loss: 0.00001215
Iteration 194/1000 | Loss: 0.00001215
Iteration 195/1000 | Loss: 0.00001215
Iteration 196/1000 | Loss: 0.00001215
Iteration 197/1000 | Loss: 0.00001215
Iteration 198/1000 | Loss: 0.00001215
Iteration 199/1000 | Loss: 0.00001215
Iteration 200/1000 | Loss: 0.00001215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.2154158866906073e-05, 1.2154158866906073e-05, 1.2154158866906073e-05, 1.2154158866906073e-05, 1.2154158866906073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2154158866906073e-05

Optimization complete. Final v2v error: 2.9766955375671387 mm

Highest mean error: 3.359973669052124 mm for frame 146

Lowest mean error: 2.5571670532226562 mm for frame 2

Saving results

Total time: 50.3578679561615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404265
Iteration 2/25 | Loss: 0.00124886
Iteration 3/25 | Loss: 0.00113036
Iteration 4/25 | Loss: 0.00111287
Iteration 5/25 | Loss: 0.00110758
Iteration 6/25 | Loss: 0.00110592
Iteration 7/25 | Loss: 0.00110546
Iteration 8/25 | Loss: 0.00110546
Iteration 9/25 | Loss: 0.00110546
Iteration 10/25 | Loss: 0.00110546
Iteration 11/25 | Loss: 0.00110546
Iteration 12/25 | Loss: 0.00110546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011054615024477243, 0.0011054615024477243, 0.0011054615024477243, 0.0011054615024477243, 0.0011054615024477243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011054615024477243

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32099712
Iteration 2/25 | Loss: 0.00100815
Iteration 3/25 | Loss: 0.00100812
Iteration 4/25 | Loss: 0.00100812
Iteration 5/25 | Loss: 0.00100812
Iteration 6/25 | Loss: 0.00100812
Iteration 7/25 | Loss: 0.00100812
Iteration 8/25 | Loss: 0.00100812
Iteration 9/25 | Loss: 0.00100812
Iteration 10/25 | Loss: 0.00100812
Iteration 11/25 | Loss: 0.00100812
Iteration 12/25 | Loss: 0.00100812
Iteration 13/25 | Loss: 0.00100812
Iteration 14/25 | Loss: 0.00100812
Iteration 15/25 | Loss: 0.00100812
Iteration 16/25 | Loss: 0.00100812
Iteration 17/25 | Loss: 0.00100812
Iteration 18/25 | Loss: 0.00100812
Iteration 19/25 | Loss: 0.00100812
Iteration 20/25 | Loss: 0.00100812
Iteration 21/25 | Loss: 0.00100812
Iteration 22/25 | Loss: 0.00100812
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010081176878884435, 0.0010081176878884435, 0.0010081176878884435, 0.0010081176878884435, 0.0010081176878884435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010081176878884435

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100812
Iteration 2/1000 | Loss: 0.00004090
Iteration 3/1000 | Loss: 0.00002689
Iteration 4/1000 | Loss: 0.00002156
Iteration 5/1000 | Loss: 0.00001953
Iteration 6/1000 | Loss: 0.00001814
Iteration 7/1000 | Loss: 0.00001715
Iteration 8/1000 | Loss: 0.00001656
Iteration 9/1000 | Loss: 0.00001610
Iteration 10/1000 | Loss: 0.00001577
Iteration 11/1000 | Loss: 0.00001556
Iteration 12/1000 | Loss: 0.00001532
Iteration 13/1000 | Loss: 0.00001524
Iteration 14/1000 | Loss: 0.00001516
Iteration 15/1000 | Loss: 0.00001516
Iteration 16/1000 | Loss: 0.00001515
Iteration 17/1000 | Loss: 0.00001515
Iteration 18/1000 | Loss: 0.00001515
Iteration 19/1000 | Loss: 0.00001512
Iteration 20/1000 | Loss: 0.00001509
Iteration 21/1000 | Loss: 0.00001508
Iteration 22/1000 | Loss: 0.00001507
Iteration 23/1000 | Loss: 0.00001507
Iteration 24/1000 | Loss: 0.00001507
Iteration 25/1000 | Loss: 0.00001506
Iteration 26/1000 | Loss: 0.00001506
Iteration 27/1000 | Loss: 0.00001506
Iteration 28/1000 | Loss: 0.00001505
Iteration 29/1000 | Loss: 0.00001505
Iteration 30/1000 | Loss: 0.00001504
Iteration 31/1000 | Loss: 0.00001504
Iteration 32/1000 | Loss: 0.00001503
Iteration 33/1000 | Loss: 0.00001502
Iteration 34/1000 | Loss: 0.00001502
Iteration 35/1000 | Loss: 0.00001501
Iteration 36/1000 | Loss: 0.00001501
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001500
Iteration 39/1000 | Loss: 0.00001500
Iteration 40/1000 | Loss: 0.00001499
Iteration 41/1000 | Loss: 0.00001497
Iteration 42/1000 | Loss: 0.00001495
Iteration 43/1000 | Loss: 0.00001494
Iteration 44/1000 | Loss: 0.00001493
Iteration 45/1000 | Loss: 0.00001493
Iteration 46/1000 | Loss: 0.00001492
Iteration 47/1000 | Loss: 0.00001492
Iteration 48/1000 | Loss: 0.00001492
Iteration 49/1000 | Loss: 0.00001492
Iteration 50/1000 | Loss: 0.00001491
Iteration 51/1000 | Loss: 0.00001491
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001490
Iteration 55/1000 | Loss: 0.00001490
Iteration 56/1000 | Loss: 0.00001490
Iteration 57/1000 | Loss: 0.00001488
Iteration 58/1000 | Loss: 0.00001487
Iteration 59/1000 | Loss: 0.00001487
Iteration 60/1000 | Loss: 0.00001486
Iteration 61/1000 | Loss: 0.00001486
Iteration 62/1000 | Loss: 0.00001485
Iteration 63/1000 | Loss: 0.00001484
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001484
Iteration 66/1000 | Loss: 0.00001484
Iteration 67/1000 | Loss: 0.00001483
Iteration 68/1000 | Loss: 0.00001483
Iteration 69/1000 | Loss: 0.00001482
Iteration 70/1000 | Loss: 0.00001482
Iteration 71/1000 | Loss: 0.00001482
Iteration 72/1000 | Loss: 0.00001482
Iteration 73/1000 | Loss: 0.00001481
Iteration 74/1000 | Loss: 0.00001481
Iteration 75/1000 | Loss: 0.00001480
Iteration 76/1000 | Loss: 0.00001480
Iteration 77/1000 | Loss: 0.00001480
Iteration 78/1000 | Loss: 0.00001480
Iteration 79/1000 | Loss: 0.00001480
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001477
Iteration 94/1000 | Loss: 0.00001476
Iteration 95/1000 | Loss: 0.00001476
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001475
Iteration 98/1000 | Loss: 0.00001475
Iteration 99/1000 | Loss: 0.00001475
Iteration 100/1000 | Loss: 0.00001475
Iteration 101/1000 | Loss: 0.00001475
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001474
Iteration 104/1000 | Loss: 0.00001474
Iteration 105/1000 | Loss: 0.00001474
Iteration 106/1000 | Loss: 0.00001474
Iteration 107/1000 | Loss: 0.00001474
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001474
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001474
Iteration 116/1000 | Loss: 0.00001473
Iteration 117/1000 | Loss: 0.00001473
Iteration 118/1000 | Loss: 0.00001473
Iteration 119/1000 | Loss: 0.00001473
Iteration 120/1000 | Loss: 0.00001473
Iteration 121/1000 | Loss: 0.00001473
Iteration 122/1000 | Loss: 0.00001473
Iteration 123/1000 | Loss: 0.00001473
Iteration 124/1000 | Loss: 0.00001473
Iteration 125/1000 | Loss: 0.00001472
Iteration 126/1000 | Loss: 0.00001472
Iteration 127/1000 | Loss: 0.00001472
Iteration 128/1000 | Loss: 0.00001472
Iteration 129/1000 | Loss: 0.00001471
Iteration 130/1000 | Loss: 0.00001471
Iteration 131/1000 | Loss: 0.00001471
Iteration 132/1000 | Loss: 0.00001470
Iteration 133/1000 | Loss: 0.00001470
Iteration 134/1000 | Loss: 0.00001470
Iteration 135/1000 | Loss: 0.00001470
Iteration 136/1000 | Loss: 0.00001469
Iteration 137/1000 | Loss: 0.00001469
Iteration 138/1000 | Loss: 0.00001469
Iteration 139/1000 | Loss: 0.00001469
Iteration 140/1000 | Loss: 0.00001469
Iteration 141/1000 | Loss: 0.00001469
Iteration 142/1000 | Loss: 0.00001469
Iteration 143/1000 | Loss: 0.00001469
Iteration 144/1000 | Loss: 0.00001468
Iteration 145/1000 | Loss: 0.00001468
Iteration 146/1000 | Loss: 0.00001468
Iteration 147/1000 | Loss: 0.00001468
Iteration 148/1000 | Loss: 0.00001468
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001467
Iteration 151/1000 | Loss: 0.00001467
Iteration 152/1000 | Loss: 0.00001467
Iteration 153/1000 | Loss: 0.00001466
Iteration 154/1000 | Loss: 0.00001466
Iteration 155/1000 | Loss: 0.00001466
Iteration 156/1000 | Loss: 0.00001466
Iteration 157/1000 | Loss: 0.00001466
Iteration 158/1000 | Loss: 0.00001466
Iteration 159/1000 | Loss: 0.00001466
Iteration 160/1000 | Loss: 0.00001466
Iteration 161/1000 | Loss: 0.00001466
Iteration 162/1000 | Loss: 0.00001466
Iteration 163/1000 | Loss: 0.00001465
Iteration 164/1000 | Loss: 0.00001465
Iteration 165/1000 | Loss: 0.00001465
Iteration 166/1000 | Loss: 0.00001465
Iteration 167/1000 | Loss: 0.00001465
Iteration 168/1000 | Loss: 0.00001465
Iteration 169/1000 | Loss: 0.00001465
Iteration 170/1000 | Loss: 0.00001465
Iteration 171/1000 | Loss: 0.00001465
Iteration 172/1000 | Loss: 0.00001465
Iteration 173/1000 | Loss: 0.00001465
Iteration 174/1000 | Loss: 0.00001465
Iteration 175/1000 | Loss: 0.00001465
Iteration 176/1000 | Loss: 0.00001464
Iteration 177/1000 | Loss: 0.00001464
Iteration 178/1000 | Loss: 0.00001464
Iteration 179/1000 | Loss: 0.00001464
Iteration 180/1000 | Loss: 0.00001464
Iteration 181/1000 | Loss: 0.00001464
Iteration 182/1000 | Loss: 0.00001464
Iteration 183/1000 | Loss: 0.00001464
Iteration 184/1000 | Loss: 0.00001463
Iteration 185/1000 | Loss: 0.00001463
Iteration 186/1000 | Loss: 0.00001463
Iteration 187/1000 | Loss: 0.00001463
Iteration 188/1000 | Loss: 0.00001463
Iteration 189/1000 | Loss: 0.00001463
Iteration 190/1000 | Loss: 0.00001463
Iteration 191/1000 | Loss: 0.00001463
Iteration 192/1000 | Loss: 0.00001463
Iteration 193/1000 | Loss: 0.00001463
Iteration 194/1000 | Loss: 0.00001463
Iteration 195/1000 | Loss: 0.00001463
Iteration 196/1000 | Loss: 0.00001463
Iteration 197/1000 | Loss: 0.00001463
Iteration 198/1000 | Loss: 0.00001463
Iteration 199/1000 | Loss: 0.00001462
Iteration 200/1000 | Loss: 0.00001462
Iteration 201/1000 | Loss: 0.00001462
Iteration 202/1000 | Loss: 0.00001462
Iteration 203/1000 | Loss: 0.00001462
Iteration 204/1000 | Loss: 0.00001462
Iteration 205/1000 | Loss: 0.00001462
Iteration 206/1000 | Loss: 0.00001462
Iteration 207/1000 | Loss: 0.00001462
Iteration 208/1000 | Loss: 0.00001462
Iteration 209/1000 | Loss: 0.00001462
Iteration 210/1000 | Loss: 0.00001461
Iteration 211/1000 | Loss: 0.00001461
Iteration 212/1000 | Loss: 0.00001461
Iteration 213/1000 | Loss: 0.00001461
Iteration 214/1000 | Loss: 0.00001461
Iteration 215/1000 | Loss: 0.00001461
Iteration 216/1000 | Loss: 0.00001461
Iteration 217/1000 | Loss: 0.00001460
Iteration 218/1000 | Loss: 0.00001460
Iteration 219/1000 | Loss: 0.00001460
Iteration 220/1000 | Loss: 0.00001460
Iteration 221/1000 | Loss: 0.00001460
Iteration 222/1000 | Loss: 0.00001460
Iteration 223/1000 | Loss: 0.00001460
Iteration 224/1000 | Loss: 0.00001460
Iteration 225/1000 | Loss: 0.00001459
Iteration 226/1000 | Loss: 0.00001459
Iteration 227/1000 | Loss: 0.00001459
Iteration 228/1000 | Loss: 0.00001459
Iteration 229/1000 | Loss: 0.00001459
Iteration 230/1000 | Loss: 0.00001459
Iteration 231/1000 | Loss: 0.00001459
Iteration 232/1000 | Loss: 0.00001459
Iteration 233/1000 | Loss: 0.00001458
Iteration 234/1000 | Loss: 0.00001458
Iteration 235/1000 | Loss: 0.00001458
Iteration 236/1000 | Loss: 0.00001457
Iteration 237/1000 | Loss: 0.00001457
Iteration 238/1000 | Loss: 0.00001457
Iteration 239/1000 | Loss: 0.00001457
Iteration 240/1000 | Loss: 0.00001456
Iteration 241/1000 | Loss: 0.00001456
Iteration 242/1000 | Loss: 0.00001456
Iteration 243/1000 | Loss: 0.00001456
Iteration 244/1000 | Loss: 0.00001456
Iteration 245/1000 | Loss: 0.00001456
Iteration 246/1000 | Loss: 0.00001455
Iteration 247/1000 | Loss: 0.00001455
Iteration 248/1000 | Loss: 0.00001455
Iteration 249/1000 | Loss: 0.00001455
Iteration 250/1000 | Loss: 0.00001455
Iteration 251/1000 | Loss: 0.00001455
Iteration 252/1000 | Loss: 0.00001455
Iteration 253/1000 | Loss: 0.00001455
Iteration 254/1000 | Loss: 0.00001455
Iteration 255/1000 | Loss: 0.00001455
Iteration 256/1000 | Loss: 0.00001455
Iteration 257/1000 | Loss: 0.00001455
Iteration 258/1000 | Loss: 0.00001455
Iteration 259/1000 | Loss: 0.00001454
Iteration 260/1000 | Loss: 0.00001454
Iteration 261/1000 | Loss: 0.00001454
Iteration 262/1000 | Loss: 0.00001454
Iteration 263/1000 | Loss: 0.00001454
Iteration 264/1000 | Loss: 0.00001453
Iteration 265/1000 | Loss: 0.00001453
Iteration 266/1000 | Loss: 0.00001453
Iteration 267/1000 | Loss: 0.00001453
Iteration 268/1000 | Loss: 0.00001453
Iteration 269/1000 | Loss: 0.00001453
Iteration 270/1000 | Loss: 0.00001453
Iteration 271/1000 | Loss: 0.00001453
Iteration 272/1000 | Loss: 0.00001453
Iteration 273/1000 | Loss: 0.00001453
Iteration 274/1000 | Loss: 0.00001453
Iteration 275/1000 | Loss: 0.00001453
Iteration 276/1000 | Loss: 0.00001452
Iteration 277/1000 | Loss: 0.00001452
Iteration 278/1000 | Loss: 0.00001452
Iteration 279/1000 | Loss: 0.00001452
Iteration 280/1000 | Loss: 0.00001452
Iteration 281/1000 | Loss: 0.00001452
Iteration 282/1000 | Loss: 0.00001452
Iteration 283/1000 | Loss: 0.00001452
Iteration 284/1000 | Loss: 0.00001452
Iteration 285/1000 | Loss: 0.00001452
Iteration 286/1000 | Loss: 0.00001451
Iteration 287/1000 | Loss: 0.00001451
Iteration 288/1000 | Loss: 0.00001451
Iteration 289/1000 | Loss: 0.00001451
Iteration 290/1000 | Loss: 0.00001451
Iteration 291/1000 | Loss: 0.00001451
Iteration 292/1000 | Loss: 0.00001451
Iteration 293/1000 | Loss: 0.00001451
Iteration 294/1000 | Loss: 0.00001451
Iteration 295/1000 | Loss: 0.00001451
Iteration 296/1000 | Loss: 0.00001451
Iteration 297/1000 | Loss: 0.00001451
Iteration 298/1000 | Loss: 0.00001451
Iteration 299/1000 | Loss: 0.00001451
Iteration 300/1000 | Loss: 0.00001451
Iteration 301/1000 | Loss: 0.00001451
Iteration 302/1000 | Loss: 0.00001451
Iteration 303/1000 | Loss: 0.00001451
Iteration 304/1000 | Loss: 0.00001451
Iteration 305/1000 | Loss: 0.00001451
Iteration 306/1000 | Loss: 0.00001451
Iteration 307/1000 | Loss: 0.00001451
Iteration 308/1000 | Loss: 0.00001451
Iteration 309/1000 | Loss: 0.00001451
Iteration 310/1000 | Loss: 0.00001451
Iteration 311/1000 | Loss: 0.00001451
Iteration 312/1000 | Loss: 0.00001451
Iteration 313/1000 | Loss: 0.00001451
Iteration 314/1000 | Loss: 0.00001451
Iteration 315/1000 | Loss: 0.00001451
Iteration 316/1000 | Loss: 0.00001451
Iteration 317/1000 | Loss: 0.00001451
Iteration 318/1000 | Loss: 0.00001451
Iteration 319/1000 | Loss: 0.00001451
Iteration 320/1000 | Loss: 0.00001451
Iteration 321/1000 | Loss: 0.00001451
Iteration 322/1000 | Loss: 0.00001451
Iteration 323/1000 | Loss: 0.00001451
Iteration 324/1000 | Loss: 0.00001451
Iteration 325/1000 | Loss: 0.00001451
Iteration 326/1000 | Loss: 0.00001451
Iteration 327/1000 | Loss: 0.00001451
Iteration 328/1000 | Loss: 0.00001451
Iteration 329/1000 | Loss: 0.00001451
Iteration 330/1000 | Loss: 0.00001451
Iteration 331/1000 | Loss: 0.00001451
Iteration 332/1000 | Loss: 0.00001451
Iteration 333/1000 | Loss: 0.00001451
Iteration 334/1000 | Loss: 0.00001451
Iteration 335/1000 | Loss: 0.00001451
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 335. Stopping optimization.
Last 5 losses: [1.4506203115161043e-05, 1.4506203115161043e-05, 1.4506203115161043e-05, 1.4506203115161043e-05, 1.4506203115161043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4506203115161043e-05

Optimization complete. Final v2v error: 3.0496957302093506 mm

Highest mean error: 5.08136510848999 mm for frame 87

Lowest mean error: 2.410148859024048 mm for frame 22

Saving results

Total time: 52.51946687698364
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00718230
Iteration 2/25 | Loss: 0.00120540
Iteration 3/25 | Loss: 0.00112497
Iteration 4/25 | Loss: 0.00111556
Iteration 5/25 | Loss: 0.00111346
Iteration 6/25 | Loss: 0.00111346
Iteration 7/25 | Loss: 0.00111346
Iteration 8/25 | Loss: 0.00111346
Iteration 9/25 | Loss: 0.00111346
Iteration 10/25 | Loss: 0.00111346
Iteration 11/25 | Loss: 0.00111346
Iteration 12/25 | Loss: 0.00111346
Iteration 13/25 | Loss: 0.00111346
Iteration 14/25 | Loss: 0.00111346
Iteration 15/25 | Loss: 0.00111346
Iteration 16/25 | Loss: 0.00111346
Iteration 17/25 | Loss: 0.00111346
Iteration 18/25 | Loss: 0.00111346
Iteration 19/25 | Loss: 0.00111346
Iteration 20/25 | Loss: 0.00111346
Iteration 21/25 | Loss: 0.00111346
Iteration 22/25 | Loss: 0.00111346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011134595843032002, 0.0011134595843032002, 0.0011134595843032002, 0.0011134595843032002, 0.0011134595843032002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011134595843032002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47890675
Iteration 2/25 | Loss: 0.00080987
Iteration 3/25 | Loss: 0.00080986
Iteration 4/25 | Loss: 0.00080986
Iteration 5/25 | Loss: 0.00080986
Iteration 6/25 | Loss: 0.00080986
Iteration 7/25 | Loss: 0.00080986
Iteration 8/25 | Loss: 0.00080986
Iteration 9/25 | Loss: 0.00080986
Iteration 10/25 | Loss: 0.00080986
Iteration 11/25 | Loss: 0.00080986
Iteration 12/25 | Loss: 0.00080986
Iteration 13/25 | Loss: 0.00080986
Iteration 14/25 | Loss: 0.00080986
Iteration 15/25 | Loss: 0.00080986
Iteration 16/25 | Loss: 0.00080986
Iteration 17/25 | Loss: 0.00080986
Iteration 18/25 | Loss: 0.00080986
Iteration 19/25 | Loss: 0.00080986
Iteration 20/25 | Loss: 0.00080986
Iteration 21/25 | Loss: 0.00080986
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008098602993413806, 0.0008098602993413806, 0.0008098602993413806, 0.0008098602993413806, 0.0008098602993413806]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008098602993413806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080986
Iteration 2/1000 | Loss: 0.00002202
Iteration 3/1000 | Loss: 0.00001431
Iteration 4/1000 | Loss: 0.00001303
Iteration 5/1000 | Loss: 0.00001225
Iteration 6/1000 | Loss: 0.00001181
Iteration 7/1000 | Loss: 0.00001143
Iteration 8/1000 | Loss: 0.00001124
Iteration 9/1000 | Loss: 0.00001097
Iteration 10/1000 | Loss: 0.00001076
Iteration 11/1000 | Loss: 0.00001068
Iteration 12/1000 | Loss: 0.00001068
Iteration 13/1000 | Loss: 0.00001064
Iteration 14/1000 | Loss: 0.00001062
Iteration 15/1000 | Loss: 0.00001059
Iteration 16/1000 | Loss: 0.00001047
Iteration 17/1000 | Loss: 0.00001040
Iteration 18/1000 | Loss: 0.00001035
Iteration 19/1000 | Loss: 0.00001034
Iteration 20/1000 | Loss: 0.00001034
Iteration 21/1000 | Loss: 0.00001033
Iteration 22/1000 | Loss: 0.00001032
Iteration 23/1000 | Loss: 0.00001031
Iteration 24/1000 | Loss: 0.00001030
Iteration 25/1000 | Loss: 0.00001029
Iteration 26/1000 | Loss: 0.00001029
Iteration 27/1000 | Loss: 0.00001028
Iteration 28/1000 | Loss: 0.00001028
Iteration 29/1000 | Loss: 0.00001027
Iteration 30/1000 | Loss: 0.00001026
Iteration 31/1000 | Loss: 0.00001025
Iteration 32/1000 | Loss: 0.00001024
Iteration 33/1000 | Loss: 0.00001022
Iteration 34/1000 | Loss: 0.00001021
Iteration 35/1000 | Loss: 0.00001020
Iteration 36/1000 | Loss: 0.00001019
Iteration 37/1000 | Loss: 0.00001018
Iteration 38/1000 | Loss: 0.00001017
Iteration 39/1000 | Loss: 0.00001017
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001015
Iteration 42/1000 | Loss: 0.00001014
Iteration 43/1000 | Loss: 0.00001014
Iteration 44/1000 | Loss: 0.00001013
Iteration 45/1000 | Loss: 0.00001013
Iteration 46/1000 | Loss: 0.00001013
Iteration 47/1000 | Loss: 0.00001012
Iteration 48/1000 | Loss: 0.00001012
Iteration 49/1000 | Loss: 0.00001012
Iteration 50/1000 | Loss: 0.00001011
Iteration 51/1000 | Loss: 0.00001011
Iteration 52/1000 | Loss: 0.00001010
Iteration 53/1000 | Loss: 0.00001010
Iteration 54/1000 | Loss: 0.00001009
Iteration 55/1000 | Loss: 0.00001009
Iteration 56/1000 | Loss: 0.00001009
Iteration 57/1000 | Loss: 0.00001005
Iteration 58/1000 | Loss: 0.00001005
Iteration 59/1000 | Loss: 0.00001003
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001001
Iteration 64/1000 | Loss: 0.00001001
Iteration 65/1000 | Loss: 0.00001000
Iteration 66/1000 | Loss: 0.00001000
Iteration 67/1000 | Loss: 0.00001000
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000999
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000998
Iteration 72/1000 | Loss: 0.00000998
Iteration 73/1000 | Loss: 0.00000997
Iteration 74/1000 | Loss: 0.00000997
Iteration 75/1000 | Loss: 0.00000997
Iteration 76/1000 | Loss: 0.00000997
Iteration 77/1000 | Loss: 0.00000996
Iteration 78/1000 | Loss: 0.00000996
Iteration 79/1000 | Loss: 0.00000995
Iteration 80/1000 | Loss: 0.00000995
Iteration 81/1000 | Loss: 0.00000995
Iteration 82/1000 | Loss: 0.00000994
Iteration 83/1000 | Loss: 0.00000993
Iteration 84/1000 | Loss: 0.00000993
Iteration 85/1000 | Loss: 0.00000993
Iteration 86/1000 | Loss: 0.00000992
Iteration 87/1000 | Loss: 0.00000992
Iteration 88/1000 | Loss: 0.00000992
Iteration 89/1000 | Loss: 0.00000992
Iteration 90/1000 | Loss: 0.00000992
Iteration 91/1000 | Loss: 0.00000991
Iteration 92/1000 | Loss: 0.00000991
Iteration 93/1000 | Loss: 0.00000990
Iteration 94/1000 | Loss: 0.00000989
Iteration 95/1000 | Loss: 0.00000989
Iteration 96/1000 | Loss: 0.00000988
Iteration 97/1000 | Loss: 0.00000988
Iteration 98/1000 | Loss: 0.00000988
Iteration 99/1000 | Loss: 0.00000987
Iteration 100/1000 | Loss: 0.00000987
Iteration 101/1000 | Loss: 0.00000986
Iteration 102/1000 | Loss: 0.00000986
Iteration 103/1000 | Loss: 0.00000986
Iteration 104/1000 | Loss: 0.00000986
Iteration 105/1000 | Loss: 0.00000986
Iteration 106/1000 | Loss: 0.00000985
Iteration 107/1000 | Loss: 0.00000985
Iteration 108/1000 | Loss: 0.00000985
Iteration 109/1000 | Loss: 0.00000985
Iteration 110/1000 | Loss: 0.00000985
Iteration 111/1000 | Loss: 0.00000985
Iteration 112/1000 | Loss: 0.00000985
Iteration 113/1000 | Loss: 0.00000985
Iteration 114/1000 | Loss: 0.00000985
Iteration 115/1000 | Loss: 0.00000985
Iteration 116/1000 | Loss: 0.00000985
Iteration 117/1000 | Loss: 0.00000985
Iteration 118/1000 | Loss: 0.00000984
Iteration 119/1000 | Loss: 0.00000984
Iteration 120/1000 | Loss: 0.00000984
Iteration 121/1000 | Loss: 0.00000984
Iteration 122/1000 | Loss: 0.00000983
Iteration 123/1000 | Loss: 0.00000983
Iteration 124/1000 | Loss: 0.00000983
Iteration 125/1000 | Loss: 0.00000983
Iteration 126/1000 | Loss: 0.00000982
Iteration 127/1000 | Loss: 0.00000982
Iteration 128/1000 | Loss: 0.00000982
Iteration 129/1000 | Loss: 0.00000982
Iteration 130/1000 | Loss: 0.00000982
Iteration 131/1000 | Loss: 0.00000982
Iteration 132/1000 | Loss: 0.00000982
Iteration 133/1000 | Loss: 0.00000982
Iteration 134/1000 | Loss: 0.00000981
Iteration 135/1000 | Loss: 0.00000981
Iteration 136/1000 | Loss: 0.00000981
Iteration 137/1000 | Loss: 0.00000981
Iteration 138/1000 | Loss: 0.00000980
Iteration 139/1000 | Loss: 0.00000980
Iteration 140/1000 | Loss: 0.00000980
Iteration 141/1000 | Loss: 0.00000980
Iteration 142/1000 | Loss: 0.00000980
Iteration 143/1000 | Loss: 0.00000980
Iteration 144/1000 | Loss: 0.00000980
Iteration 145/1000 | Loss: 0.00000980
Iteration 146/1000 | Loss: 0.00000980
Iteration 147/1000 | Loss: 0.00000979
Iteration 148/1000 | Loss: 0.00000979
Iteration 149/1000 | Loss: 0.00000979
Iteration 150/1000 | Loss: 0.00000979
Iteration 151/1000 | Loss: 0.00000979
Iteration 152/1000 | Loss: 0.00000979
Iteration 153/1000 | Loss: 0.00000979
Iteration 154/1000 | Loss: 0.00000979
Iteration 155/1000 | Loss: 0.00000979
Iteration 156/1000 | Loss: 0.00000979
Iteration 157/1000 | Loss: 0.00000979
Iteration 158/1000 | Loss: 0.00000979
Iteration 159/1000 | Loss: 0.00000979
Iteration 160/1000 | Loss: 0.00000979
Iteration 161/1000 | Loss: 0.00000979
Iteration 162/1000 | Loss: 0.00000978
Iteration 163/1000 | Loss: 0.00000978
Iteration 164/1000 | Loss: 0.00000978
Iteration 165/1000 | Loss: 0.00000978
Iteration 166/1000 | Loss: 0.00000978
Iteration 167/1000 | Loss: 0.00000978
Iteration 168/1000 | Loss: 0.00000978
Iteration 169/1000 | Loss: 0.00000978
Iteration 170/1000 | Loss: 0.00000978
Iteration 171/1000 | Loss: 0.00000978
Iteration 172/1000 | Loss: 0.00000978
Iteration 173/1000 | Loss: 0.00000978
Iteration 174/1000 | Loss: 0.00000978
Iteration 175/1000 | Loss: 0.00000978
Iteration 176/1000 | Loss: 0.00000978
Iteration 177/1000 | Loss: 0.00000978
Iteration 178/1000 | Loss: 0.00000978
Iteration 179/1000 | Loss: 0.00000978
Iteration 180/1000 | Loss: 0.00000978
Iteration 181/1000 | Loss: 0.00000978
Iteration 182/1000 | Loss: 0.00000978
Iteration 183/1000 | Loss: 0.00000978
Iteration 184/1000 | Loss: 0.00000978
Iteration 185/1000 | Loss: 0.00000978
Iteration 186/1000 | Loss: 0.00000978
Iteration 187/1000 | Loss: 0.00000978
Iteration 188/1000 | Loss: 0.00000978
Iteration 189/1000 | Loss: 0.00000978
Iteration 190/1000 | Loss: 0.00000978
Iteration 191/1000 | Loss: 0.00000978
Iteration 192/1000 | Loss: 0.00000978
Iteration 193/1000 | Loss: 0.00000978
Iteration 194/1000 | Loss: 0.00000978
Iteration 195/1000 | Loss: 0.00000978
Iteration 196/1000 | Loss: 0.00000978
Iteration 197/1000 | Loss: 0.00000978
Iteration 198/1000 | Loss: 0.00000978
Iteration 199/1000 | Loss: 0.00000978
Iteration 200/1000 | Loss: 0.00000978
Iteration 201/1000 | Loss: 0.00000978
Iteration 202/1000 | Loss: 0.00000978
Iteration 203/1000 | Loss: 0.00000978
Iteration 204/1000 | Loss: 0.00000978
Iteration 205/1000 | Loss: 0.00000978
Iteration 206/1000 | Loss: 0.00000978
Iteration 207/1000 | Loss: 0.00000978
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [9.784196663531475e-06, 9.784196663531475e-06, 9.784196663531475e-06, 9.784196663531475e-06, 9.784196663531475e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.784196663531475e-06

Optimization complete. Final v2v error: 2.6830201148986816 mm

Highest mean error: 3.011075019836426 mm for frame 21

Lowest mean error: 2.4465878009796143 mm for frame 46

Saving results

Total time: 41.216052293777466
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00731210
Iteration 2/25 | Loss: 0.00128627
Iteration 3/25 | Loss: 0.00122038
Iteration 4/25 | Loss: 0.00121394
Iteration 5/25 | Loss: 0.00121181
Iteration 6/25 | Loss: 0.00121163
Iteration 7/25 | Loss: 0.00121163
Iteration 8/25 | Loss: 0.00121163
Iteration 9/25 | Loss: 0.00121163
Iteration 10/25 | Loss: 0.00121163
Iteration 11/25 | Loss: 0.00121163
Iteration 12/25 | Loss: 0.00121163
Iteration 13/25 | Loss: 0.00121163
Iteration 14/25 | Loss: 0.00121163
Iteration 15/25 | Loss: 0.00121163
Iteration 16/25 | Loss: 0.00121163
Iteration 17/25 | Loss: 0.00121163
Iteration 18/25 | Loss: 0.00121163
Iteration 19/25 | Loss: 0.00121163
Iteration 20/25 | Loss: 0.00121163
Iteration 21/25 | Loss: 0.00121163
Iteration 22/25 | Loss: 0.00121163
Iteration 23/25 | Loss: 0.00121163
Iteration 24/25 | Loss: 0.00121163
Iteration 25/25 | Loss: 0.00121163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17295849
Iteration 2/25 | Loss: 0.00081448
Iteration 3/25 | Loss: 0.00081447
Iteration 4/25 | Loss: 0.00081446
Iteration 5/25 | Loss: 0.00081446
Iteration 6/25 | Loss: 0.00081446
Iteration 7/25 | Loss: 0.00081446
Iteration 8/25 | Loss: 0.00081446
Iteration 9/25 | Loss: 0.00081446
Iteration 10/25 | Loss: 0.00081446
Iteration 11/25 | Loss: 0.00081446
Iteration 12/25 | Loss: 0.00081446
Iteration 13/25 | Loss: 0.00081446
Iteration 14/25 | Loss: 0.00081446
Iteration 15/25 | Loss: 0.00081446
Iteration 16/25 | Loss: 0.00081446
Iteration 17/25 | Loss: 0.00081446
Iteration 18/25 | Loss: 0.00081446
Iteration 19/25 | Loss: 0.00081446
Iteration 20/25 | Loss: 0.00081446
Iteration 21/25 | Loss: 0.00081446
Iteration 22/25 | Loss: 0.00081446
Iteration 23/25 | Loss: 0.00081446
Iteration 24/25 | Loss: 0.00081446
Iteration 25/25 | Loss: 0.00081446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081446
Iteration 2/1000 | Loss: 0.00003747
Iteration 3/1000 | Loss: 0.00002311
Iteration 4/1000 | Loss: 0.00002071
Iteration 5/1000 | Loss: 0.00001969
Iteration 6/1000 | Loss: 0.00001920
Iteration 7/1000 | Loss: 0.00001892
Iteration 8/1000 | Loss: 0.00001864
Iteration 9/1000 | Loss: 0.00001828
Iteration 10/1000 | Loss: 0.00001798
Iteration 11/1000 | Loss: 0.00001783
Iteration 12/1000 | Loss: 0.00001762
Iteration 13/1000 | Loss: 0.00001757
Iteration 14/1000 | Loss: 0.00001752
Iteration 15/1000 | Loss: 0.00001747
Iteration 16/1000 | Loss: 0.00001743
Iteration 17/1000 | Loss: 0.00001741
Iteration 18/1000 | Loss: 0.00001726
Iteration 19/1000 | Loss: 0.00001724
Iteration 20/1000 | Loss: 0.00001723
Iteration 21/1000 | Loss: 0.00001723
Iteration 22/1000 | Loss: 0.00001722
Iteration 23/1000 | Loss: 0.00001721
Iteration 24/1000 | Loss: 0.00001717
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00001716
Iteration 27/1000 | Loss: 0.00001715
Iteration 28/1000 | Loss: 0.00001715
Iteration 29/1000 | Loss: 0.00001714
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001712
Iteration 34/1000 | Loss: 0.00001712
Iteration 35/1000 | Loss: 0.00001712
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001711
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001710
Iteration 40/1000 | Loss: 0.00001710
Iteration 41/1000 | Loss: 0.00001710
Iteration 42/1000 | Loss: 0.00001710
Iteration 43/1000 | Loss: 0.00001709
Iteration 44/1000 | Loss: 0.00001709
Iteration 45/1000 | Loss: 0.00001709
Iteration 46/1000 | Loss: 0.00001709
Iteration 47/1000 | Loss: 0.00001709
Iteration 48/1000 | Loss: 0.00001709
Iteration 49/1000 | Loss: 0.00001709
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001709
Iteration 55/1000 | Loss: 0.00001707
Iteration 56/1000 | Loss: 0.00001707
Iteration 57/1000 | Loss: 0.00001707
Iteration 58/1000 | Loss: 0.00001706
Iteration 59/1000 | Loss: 0.00001706
Iteration 60/1000 | Loss: 0.00001705
Iteration 61/1000 | Loss: 0.00001705
Iteration 62/1000 | Loss: 0.00001702
Iteration 63/1000 | Loss: 0.00001702
Iteration 64/1000 | Loss: 0.00001701
Iteration 65/1000 | Loss: 0.00001701
Iteration 66/1000 | Loss: 0.00001701
Iteration 67/1000 | Loss: 0.00001700
Iteration 68/1000 | Loss: 0.00001700
Iteration 69/1000 | Loss: 0.00001699
Iteration 70/1000 | Loss: 0.00001699
Iteration 71/1000 | Loss: 0.00001698
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001695
Iteration 74/1000 | Loss: 0.00001694
Iteration 75/1000 | Loss: 0.00001694
Iteration 76/1000 | Loss: 0.00001693
Iteration 77/1000 | Loss: 0.00001693
Iteration 78/1000 | Loss: 0.00001693
Iteration 79/1000 | Loss: 0.00001693
Iteration 80/1000 | Loss: 0.00001692
Iteration 81/1000 | Loss: 0.00001692
Iteration 82/1000 | Loss: 0.00001692
Iteration 83/1000 | Loss: 0.00001692
Iteration 84/1000 | Loss: 0.00001692
Iteration 85/1000 | Loss: 0.00001692
Iteration 86/1000 | Loss: 0.00001691
Iteration 87/1000 | Loss: 0.00001691
Iteration 88/1000 | Loss: 0.00001691
Iteration 89/1000 | Loss: 0.00001691
Iteration 90/1000 | Loss: 0.00001691
Iteration 91/1000 | Loss: 0.00001691
Iteration 92/1000 | Loss: 0.00001691
Iteration 93/1000 | Loss: 0.00001691
Iteration 94/1000 | Loss: 0.00001690
Iteration 95/1000 | Loss: 0.00001690
Iteration 96/1000 | Loss: 0.00001690
Iteration 97/1000 | Loss: 0.00001690
Iteration 98/1000 | Loss: 0.00001690
Iteration 99/1000 | Loss: 0.00001690
Iteration 100/1000 | Loss: 0.00001690
Iteration 101/1000 | Loss: 0.00001690
Iteration 102/1000 | Loss: 0.00001690
Iteration 103/1000 | Loss: 0.00001690
Iteration 104/1000 | Loss: 0.00001690
Iteration 105/1000 | Loss: 0.00001690
Iteration 106/1000 | Loss: 0.00001690
Iteration 107/1000 | Loss: 0.00001690
Iteration 108/1000 | Loss: 0.00001689
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001689
Iteration 111/1000 | Loss: 0.00001689
Iteration 112/1000 | Loss: 0.00001689
Iteration 113/1000 | Loss: 0.00001689
Iteration 114/1000 | Loss: 0.00001689
Iteration 115/1000 | Loss: 0.00001688
Iteration 116/1000 | Loss: 0.00001688
Iteration 117/1000 | Loss: 0.00001688
Iteration 118/1000 | Loss: 0.00001688
Iteration 119/1000 | Loss: 0.00001688
Iteration 120/1000 | Loss: 0.00001687
Iteration 121/1000 | Loss: 0.00001687
Iteration 122/1000 | Loss: 0.00001687
Iteration 123/1000 | Loss: 0.00001687
Iteration 124/1000 | Loss: 0.00001687
Iteration 125/1000 | Loss: 0.00001687
Iteration 126/1000 | Loss: 0.00001687
Iteration 127/1000 | Loss: 0.00001687
Iteration 128/1000 | Loss: 0.00001687
Iteration 129/1000 | Loss: 0.00001686
Iteration 130/1000 | Loss: 0.00001686
Iteration 131/1000 | Loss: 0.00001686
Iteration 132/1000 | Loss: 0.00001686
Iteration 133/1000 | Loss: 0.00001686
Iteration 134/1000 | Loss: 0.00001686
Iteration 135/1000 | Loss: 0.00001686
Iteration 136/1000 | Loss: 0.00001686
Iteration 137/1000 | Loss: 0.00001686
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001685
Iteration 141/1000 | Loss: 0.00001685
Iteration 142/1000 | Loss: 0.00001685
Iteration 143/1000 | Loss: 0.00001685
Iteration 144/1000 | Loss: 0.00001684
Iteration 145/1000 | Loss: 0.00001684
Iteration 146/1000 | Loss: 0.00001684
Iteration 147/1000 | Loss: 0.00001684
Iteration 148/1000 | Loss: 0.00001684
Iteration 149/1000 | Loss: 0.00001684
Iteration 150/1000 | Loss: 0.00001684
Iteration 151/1000 | Loss: 0.00001684
Iteration 152/1000 | Loss: 0.00001684
Iteration 153/1000 | Loss: 0.00001684
Iteration 154/1000 | Loss: 0.00001684
Iteration 155/1000 | Loss: 0.00001684
Iteration 156/1000 | Loss: 0.00001684
Iteration 157/1000 | Loss: 0.00001684
Iteration 158/1000 | Loss: 0.00001684
Iteration 159/1000 | Loss: 0.00001684
Iteration 160/1000 | Loss: 0.00001684
Iteration 161/1000 | Loss: 0.00001684
Iteration 162/1000 | Loss: 0.00001684
Iteration 163/1000 | Loss: 0.00001684
Iteration 164/1000 | Loss: 0.00001684
Iteration 165/1000 | Loss: 0.00001684
Iteration 166/1000 | Loss: 0.00001684
Iteration 167/1000 | Loss: 0.00001684
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Iteration 170/1000 | Loss: 0.00001684
Iteration 171/1000 | Loss: 0.00001684
Iteration 172/1000 | Loss: 0.00001684
Iteration 173/1000 | Loss: 0.00001684
Iteration 174/1000 | Loss: 0.00001684
Iteration 175/1000 | Loss: 0.00001684
Iteration 176/1000 | Loss: 0.00001684
Iteration 177/1000 | Loss: 0.00001684
Iteration 178/1000 | Loss: 0.00001684
Iteration 179/1000 | Loss: 0.00001684
Iteration 180/1000 | Loss: 0.00001684
Iteration 181/1000 | Loss: 0.00001684
Iteration 182/1000 | Loss: 0.00001684
Iteration 183/1000 | Loss: 0.00001684
Iteration 184/1000 | Loss: 0.00001684
Iteration 185/1000 | Loss: 0.00001684
Iteration 186/1000 | Loss: 0.00001684
Iteration 187/1000 | Loss: 0.00001684
Iteration 188/1000 | Loss: 0.00001684
Iteration 189/1000 | Loss: 0.00001684
Iteration 190/1000 | Loss: 0.00001684
Iteration 191/1000 | Loss: 0.00001684
Iteration 192/1000 | Loss: 0.00001684
Iteration 193/1000 | Loss: 0.00001684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.68407987075625e-05, 1.68407987075625e-05, 1.68407987075625e-05, 1.68407987075625e-05, 1.68407987075625e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.68407987075625e-05

Optimization complete. Final v2v error: 3.417421340942383 mm

Highest mean error: 3.601956605911255 mm for frame 10

Lowest mean error: 3.146848201751709 mm for frame 131

Saving results

Total time: 40.244601011276245
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017321
Iteration 2/25 | Loss: 0.00422295
Iteration 3/25 | Loss: 0.00256360
Iteration 4/25 | Loss: 0.00209072
Iteration 5/25 | Loss: 0.00195521
Iteration 6/25 | Loss: 0.00184887
Iteration 7/25 | Loss: 0.00162404
Iteration 8/25 | Loss: 0.00145321
Iteration 9/25 | Loss: 0.00133116
Iteration 10/25 | Loss: 0.00132549
Iteration 11/25 | Loss: 0.00126507
Iteration 12/25 | Loss: 0.00122111
Iteration 13/25 | Loss: 0.00121097
Iteration 14/25 | Loss: 0.00120432
Iteration 15/25 | Loss: 0.00120106
Iteration 16/25 | Loss: 0.00120199
Iteration 17/25 | Loss: 0.00119751
Iteration 18/25 | Loss: 0.00119616
Iteration 19/25 | Loss: 0.00119572
Iteration 20/25 | Loss: 0.00119546
Iteration 21/25 | Loss: 0.00119525
Iteration 22/25 | Loss: 0.00119516
Iteration 23/25 | Loss: 0.00119515
Iteration 24/25 | Loss: 0.00119515
Iteration 25/25 | Loss: 0.00119514

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36714208
Iteration 2/25 | Loss: 0.00096810
Iteration 3/25 | Loss: 0.00096810
Iteration 4/25 | Loss: 0.00096810
Iteration 5/25 | Loss: 0.00096810
Iteration 6/25 | Loss: 0.00096810
Iteration 7/25 | Loss: 0.00096810
Iteration 8/25 | Loss: 0.00096810
Iteration 9/25 | Loss: 0.00096810
Iteration 10/25 | Loss: 0.00096810
Iteration 11/25 | Loss: 0.00096810
Iteration 12/25 | Loss: 0.00096810
Iteration 13/25 | Loss: 0.00096810
Iteration 14/25 | Loss: 0.00096810
Iteration 15/25 | Loss: 0.00096810
Iteration 16/25 | Loss: 0.00096810
Iteration 17/25 | Loss: 0.00096810
Iteration 18/25 | Loss: 0.00096810
Iteration 19/25 | Loss: 0.00096810
Iteration 20/25 | Loss: 0.00096810
Iteration 21/25 | Loss: 0.00096810
Iteration 22/25 | Loss: 0.00096810
Iteration 23/25 | Loss: 0.00096810
Iteration 24/25 | Loss: 0.00096810
Iteration 25/25 | Loss: 0.00096810

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096810
Iteration 2/1000 | Loss: 0.00007429
Iteration 3/1000 | Loss: 0.00005909
Iteration 4/1000 | Loss: 0.00005292
Iteration 5/1000 | Loss: 0.00004986
Iteration 6/1000 | Loss: 0.00004799
Iteration 7/1000 | Loss: 0.00004673
Iteration 8/1000 | Loss: 0.00004555
Iteration 9/1000 | Loss: 0.00004452
Iteration 10/1000 | Loss: 0.00004388
Iteration 11/1000 | Loss: 0.00472203
Iteration 12/1000 | Loss: 0.00802412
Iteration 13/1000 | Loss: 0.00112709
Iteration 14/1000 | Loss: 0.00124718
Iteration 15/1000 | Loss: 0.00019619
Iteration 16/1000 | Loss: 0.00016188
Iteration 17/1000 | Loss: 0.00010739
Iteration 18/1000 | Loss: 0.00031454
Iteration 19/1000 | Loss: 0.00005282
Iteration 20/1000 | Loss: 0.00004159
Iteration 21/1000 | Loss: 0.00003396
Iteration 22/1000 | Loss: 0.00003034
Iteration 23/1000 | Loss: 0.00002771
Iteration 24/1000 | Loss: 0.00002531
Iteration 25/1000 | Loss: 0.00002332
Iteration 26/1000 | Loss: 0.00002168
Iteration 27/1000 | Loss: 0.00002028
Iteration 28/1000 | Loss: 0.00001893
Iteration 29/1000 | Loss: 0.00001748
Iteration 30/1000 | Loss: 0.00001650
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001602
Iteration 33/1000 | Loss: 0.00001572
Iteration 34/1000 | Loss: 0.00001544
Iteration 35/1000 | Loss: 0.00001518
Iteration 36/1000 | Loss: 0.00001506
Iteration 37/1000 | Loss: 0.00001504
Iteration 38/1000 | Loss: 0.00001503
Iteration 39/1000 | Loss: 0.00001495
Iteration 40/1000 | Loss: 0.00001491
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001489
Iteration 43/1000 | Loss: 0.00001488
Iteration 44/1000 | Loss: 0.00001487
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001485
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001484
Iteration 51/1000 | Loss: 0.00001484
Iteration 52/1000 | Loss: 0.00001483
Iteration 53/1000 | Loss: 0.00001483
Iteration 54/1000 | Loss: 0.00001483
Iteration 55/1000 | Loss: 0.00001483
Iteration 56/1000 | Loss: 0.00001483
Iteration 57/1000 | Loss: 0.00001483
Iteration 58/1000 | Loss: 0.00001482
Iteration 59/1000 | Loss: 0.00001482
Iteration 60/1000 | Loss: 0.00001482
Iteration 61/1000 | Loss: 0.00001482
Iteration 62/1000 | Loss: 0.00001481
Iteration 63/1000 | Loss: 0.00001481
Iteration 64/1000 | Loss: 0.00001481
Iteration 65/1000 | Loss: 0.00001481
Iteration 66/1000 | Loss: 0.00001481
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001480
Iteration 69/1000 | Loss: 0.00001480
Iteration 70/1000 | Loss: 0.00001480
Iteration 71/1000 | Loss: 0.00001480
Iteration 72/1000 | Loss: 0.00001480
Iteration 73/1000 | Loss: 0.00001480
Iteration 74/1000 | Loss: 0.00001479
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001479
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001479
Iteration 79/1000 | Loss: 0.00001479
Iteration 80/1000 | Loss: 0.00001479
Iteration 81/1000 | Loss: 0.00001479
Iteration 82/1000 | Loss: 0.00001479
Iteration 83/1000 | Loss: 0.00001479
Iteration 84/1000 | Loss: 0.00001478
Iteration 85/1000 | Loss: 0.00001478
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001478
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001478
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001477
Iteration 98/1000 | Loss: 0.00001477
Iteration 99/1000 | Loss: 0.00001477
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001475
Iteration 103/1000 | Loss: 0.00001475
Iteration 104/1000 | Loss: 0.00001475
Iteration 105/1000 | Loss: 0.00001475
Iteration 106/1000 | Loss: 0.00001475
Iteration 107/1000 | Loss: 0.00001475
Iteration 108/1000 | Loss: 0.00001475
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001474
Iteration 111/1000 | Loss: 0.00001474
Iteration 112/1000 | Loss: 0.00001474
Iteration 113/1000 | Loss: 0.00001473
Iteration 114/1000 | Loss: 0.00001473
Iteration 115/1000 | Loss: 0.00001473
Iteration 116/1000 | Loss: 0.00001473
Iteration 117/1000 | Loss: 0.00001473
Iteration 118/1000 | Loss: 0.00001472
Iteration 119/1000 | Loss: 0.00001472
Iteration 120/1000 | Loss: 0.00001472
Iteration 121/1000 | Loss: 0.00001471
Iteration 122/1000 | Loss: 0.00001471
Iteration 123/1000 | Loss: 0.00001471
Iteration 124/1000 | Loss: 0.00001471
Iteration 125/1000 | Loss: 0.00001471
Iteration 126/1000 | Loss: 0.00001470
Iteration 127/1000 | Loss: 0.00001470
Iteration 128/1000 | Loss: 0.00001470
Iteration 129/1000 | Loss: 0.00001470
Iteration 130/1000 | Loss: 0.00001470
Iteration 131/1000 | Loss: 0.00001469
Iteration 132/1000 | Loss: 0.00001469
Iteration 133/1000 | Loss: 0.00001469
Iteration 134/1000 | Loss: 0.00001469
Iteration 135/1000 | Loss: 0.00001469
Iteration 136/1000 | Loss: 0.00001469
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001468
Iteration 143/1000 | Loss: 0.00001468
Iteration 144/1000 | Loss: 0.00001468
Iteration 145/1000 | Loss: 0.00001468
Iteration 146/1000 | Loss: 0.00001468
Iteration 147/1000 | Loss: 0.00001468
Iteration 148/1000 | Loss: 0.00001468
Iteration 149/1000 | Loss: 0.00001467
Iteration 150/1000 | Loss: 0.00001467
Iteration 151/1000 | Loss: 0.00001467
Iteration 152/1000 | Loss: 0.00001467
Iteration 153/1000 | Loss: 0.00001467
Iteration 154/1000 | Loss: 0.00001467
Iteration 155/1000 | Loss: 0.00001467
Iteration 156/1000 | Loss: 0.00001467
Iteration 157/1000 | Loss: 0.00001467
Iteration 158/1000 | Loss: 0.00001467
Iteration 159/1000 | Loss: 0.00001467
Iteration 160/1000 | Loss: 0.00001467
Iteration 161/1000 | Loss: 0.00001467
Iteration 162/1000 | Loss: 0.00001467
Iteration 163/1000 | Loss: 0.00001467
Iteration 164/1000 | Loss: 0.00001467
Iteration 165/1000 | Loss: 0.00001467
Iteration 166/1000 | Loss: 0.00001467
Iteration 167/1000 | Loss: 0.00001467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.4674269550596364e-05, 1.4674269550596364e-05, 1.4674269550596364e-05, 1.4674269550596364e-05, 1.4674269550596364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4674269550596364e-05

Optimization complete. Final v2v error: 3.270893096923828 mm

Highest mean error: 4.632100582122803 mm for frame 32

Lowest mean error: 3.0173473358154297 mm for frame 239

Saving results

Total time: 111.72823238372803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987896
Iteration 2/25 | Loss: 0.00229556
Iteration 3/25 | Loss: 0.00187413
Iteration 4/25 | Loss: 0.00172916
Iteration 5/25 | Loss: 0.00148211
Iteration 6/25 | Loss: 0.00144525
Iteration 7/25 | Loss: 0.00136678
Iteration 8/25 | Loss: 0.00128764
Iteration 9/25 | Loss: 0.00125945
Iteration 10/25 | Loss: 0.00124913
Iteration 11/25 | Loss: 0.00124465
Iteration 12/25 | Loss: 0.00123234
Iteration 13/25 | Loss: 0.00122625
Iteration 14/25 | Loss: 0.00122274
Iteration 15/25 | Loss: 0.00122280
Iteration 16/25 | Loss: 0.00122021
Iteration 17/25 | Loss: 0.00121672
Iteration 18/25 | Loss: 0.00121540
Iteration 19/25 | Loss: 0.00121350
Iteration 20/25 | Loss: 0.00121409
Iteration 21/25 | Loss: 0.00121487
Iteration 22/25 | Loss: 0.00121343
Iteration 23/25 | Loss: 0.00121184
Iteration 24/25 | Loss: 0.00121095
Iteration 25/25 | Loss: 0.00121249

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33806801
Iteration 2/25 | Loss: 0.00080377
Iteration 3/25 | Loss: 0.00080377
Iteration 4/25 | Loss: 0.00080376
Iteration 5/25 | Loss: 0.00080376
Iteration 6/25 | Loss: 0.00080376
Iteration 7/25 | Loss: 0.00080376
Iteration 8/25 | Loss: 0.00080376
Iteration 9/25 | Loss: 0.00080376
Iteration 10/25 | Loss: 0.00080376
Iteration 11/25 | Loss: 0.00080376
Iteration 12/25 | Loss: 0.00080376
Iteration 13/25 | Loss: 0.00080376
Iteration 14/25 | Loss: 0.00080376
Iteration 15/25 | Loss: 0.00080376
Iteration 16/25 | Loss: 0.00080376
Iteration 17/25 | Loss: 0.00080376
Iteration 18/25 | Loss: 0.00080376
Iteration 19/25 | Loss: 0.00080376
Iteration 20/25 | Loss: 0.00080376
Iteration 21/25 | Loss: 0.00080376
Iteration 22/25 | Loss: 0.00080376
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008037618244998157, 0.0008037618244998157, 0.0008037618244998157, 0.0008037618244998157, 0.0008037618244998157]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008037618244998157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080376
Iteration 2/1000 | Loss: 0.00013769
Iteration 3/1000 | Loss: 0.00007194
Iteration 4/1000 | Loss: 0.00004085
Iteration 5/1000 | Loss: 0.00004033
Iteration 6/1000 | Loss: 0.00002921
Iteration 7/1000 | Loss: 0.00004160
Iteration 8/1000 | Loss: 0.00003967
Iteration 9/1000 | Loss: 0.00003713
Iteration 10/1000 | Loss: 0.00003994
Iteration 11/1000 | Loss: 0.00004809
Iteration 12/1000 | Loss: 0.00003299
Iteration 13/1000 | Loss: 0.00002945
Iteration 14/1000 | Loss: 0.00002944
Iteration 15/1000 | Loss: 0.00003138
Iteration 16/1000 | Loss: 0.00003316
Iteration 17/1000 | Loss: 0.00003014
Iteration 18/1000 | Loss: 0.00015653
Iteration 19/1000 | Loss: 0.00003521
Iteration 20/1000 | Loss: 0.00003360
Iteration 21/1000 | Loss: 0.00006085
Iteration 22/1000 | Loss: 0.00003997
Iteration 23/1000 | Loss: 0.00003477
Iteration 24/1000 | Loss: 0.00003631
Iteration 25/1000 | Loss: 0.00003645
Iteration 26/1000 | Loss: 0.00002970
Iteration 27/1000 | Loss: 0.00003219
Iteration 28/1000 | Loss: 0.00003321
Iteration 29/1000 | Loss: 0.00003787
Iteration 30/1000 | Loss: 0.00003470
Iteration 31/1000 | Loss: 0.00002933
Iteration 32/1000 | Loss: 0.00002417
Iteration 33/1000 | Loss: 0.00002738
Iteration 34/1000 | Loss: 0.00003216
Iteration 35/1000 | Loss: 0.00002634
Iteration 36/1000 | Loss: 0.00002216
Iteration 37/1000 | Loss: 0.00003539
Iteration 38/1000 | Loss: 0.00002841
Iteration 39/1000 | Loss: 0.00003420
Iteration 40/1000 | Loss: 0.00002808
Iteration 41/1000 | Loss: 0.00002842
Iteration 42/1000 | Loss: 0.00002865
Iteration 43/1000 | Loss: 0.00002546
Iteration 44/1000 | Loss: 0.00003065
Iteration 45/1000 | Loss: 0.00003238
Iteration 46/1000 | Loss: 0.00002583
Iteration 47/1000 | Loss: 0.00003837
Iteration 48/1000 | Loss: 0.00002781
Iteration 49/1000 | Loss: 0.00002360
Iteration 50/1000 | Loss: 0.00004619
Iteration 51/1000 | Loss: 0.00003152
Iteration 52/1000 | Loss: 0.00002400
Iteration 53/1000 | Loss: 0.00002438
Iteration 54/1000 | Loss: 0.00002339
Iteration 55/1000 | Loss: 0.00002093
Iteration 56/1000 | Loss: 0.00002203
Iteration 57/1000 | Loss: 0.00002096
Iteration 58/1000 | Loss: 0.00002566
Iteration 59/1000 | Loss: 0.00003138
Iteration 60/1000 | Loss: 0.00002565
Iteration 61/1000 | Loss: 0.00002099
Iteration 62/1000 | Loss: 0.00002094
Iteration 63/1000 | Loss: 0.00003617
Iteration 64/1000 | Loss: 0.00003171
Iteration 65/1000 | Loss: 0.00002190
Iteration 66/1000 | Loss: 0.00003329
Iteration 67/1000 | Loss: 0.00002475
Iteration 68/1000 | Loss: 0.00003551
Iteration 69/1000 | Loss: 0.00003172
Iteration 70/1000 | Loss: 0.00003045
Iteration 71/1000 | Loss: 0.00002922
Iteration 72/1000 | Loss: 0.00003818
Iteration 73/1000 | Loss: 0.00003423
Iteration 74/1000 | Loss: 0.00003029
Iteration 75/1000 | Loss: 0.00003340
Iteration 76/1000 | Loss: 0.00003382
Iteration 77/1000 | Loss: 0.00003385
Iteration 78/1000 | Loss: 0.00002958
Iteration 79/1000 | Loss: 0.00002774
Iteration 80/1000 | Loss: 0.00003018
Iteration 81/1000 | Loss: 0.00002731
Iteration 82/1000 | Loss: 0.00003115
Iteration 83/1000 | Loss: 0.00002585
Iteration 84/1000 | Loss: 0.00004737
Iteration 85/1000 | Loss: 0.00004588
Iteration 86/1000 | Loss: 0.00003139
Iteration 87/1000 | Loss: 0.00003360
Iteration 88/1000 | Loss: 0.00003331
Iteration 89/1000 | Loss: 0.00003150
Iteration 90/1000 | Loss: 0.00004078
Iteration 91/1000 | Loss: 0.00002915
Iteration 92/1000 | Loss: 0.00002892
Iteration 93/1000 | Loss: 0.00002313
Iteration 94/1000 | Loss: 0.00002684
Iteration 95/1000 | Loss: 0.00002657
Iteration 96/1000 | Loss: 0.00002195
Iteration 97/1000 | Loss: 0.00002528
Iteration 98/1000 | Loss: 0.00002501
Iteration 99/1000 | Loss: 0.00003056
Iteration 100/1000 | Loss: 0.00003962
Iteration 101/1000 | Loss: 0.00003325
Iteration 102/1000 | Loss: 0.00002584
Iteration 103/1000 | Loss: 0.00003021
Iteration 104/1000 | Loss: 0.00002540
Iteration 105/1000 | Loss: 0.00003275
Iteration 106/1000 | Loss: 0.00002361
Iteration 107/1000 | Loss: 0.00002560
Iteration 108/1000 | Loss: 0.00002230
Iteration 109/1000 | Loss: 0.00002595
Iteration 110/1000 | Loss: 0.00003643
Iteration 111/1000 | Loss: 0.00003270
Iteration 112/1000 | Loss: 0.00002412
Iteration 113/1000 | Loss: 0.00003314
Iteration 114/1000 | Loss: 0.00002426
Iteration 115/1000 | Loss: 0.00003221
Iteration 116/1000 | Loss: 0.00002383
Iteration 117/1000 | Loss: 0.00002879
Iteration 118/1000 | Loss: 0.00002856
Iteration 119/1000 | Loss: 0.00002473
Iteration 120/1000 | Loss: 0.00002620
Iteration 121/1000 | Loss: 0.00002903
Iteration 122/1000 | Loss: 0.00003363
Iteration 123/1000 | Loss: 0.00002537
Iteration 124/1000 | Loss: 0.00003122
Iteration 125/1000 | Loss: 0.00002413
Iteration 126/1000 | Loss: 0.00002382
Iteration 127/1000 | Loss: 0.00003929
Iteration 128/1000 | Loss: 0.00002650
Iteration 129/1000 | Loss: 0.00003865
Iteration 130/1000 | Loss: 0.00004255
Iteration 131/1000 | Loss: 0.00003377
Iteration 132/1000 | Loss: 0.00002894
Iteration 133/1000 | Loss: 0.00002757
Iteration 134/1000 | Loss: 0.00002870
Iteration 135/1000 | Loss: 0.00002850
Iteration 136/1000 | Loss: 0.00002892
Iteration 137/1000 | Loss: 0.00002504
Iteration 138/1000 | Loss: 0.00003392
Iteration 139/1000 | Loss: 0.00002757
Iteration 140/1000 | Loss: 0.00003348
Iteration 141/1000 | Loss: 0.00002798
Iteration 142/1000 | Loss: 0.00002347
Iteration 143/1000 | Loss: 0.00002243
Iteration 144/1000 | Loss: 0.00002183
Iteration 145/1000 | Loss: 0.00002085
Iteration 146/1000 | Loss: 0.00002026
Iteration 147/1000 | Loss: 0.00001980
Iteration 148/1000 | Loss: 0.00001948
Iteration 149/1000 | Loss: 0.00001909
Iteration 150/1000 | Loss: 0.00001875
Iteration 151/1000 | Loss: 0.00001859
Iteration 152/1000 | Loss: 0.00001856
Iteration 153/1000 | Loss: 0.00001856
Iteration 154/1000 | Loss: 0.00001855
Iteration 155/1000 | Loss: 0.00001855
Iteration 156/1000 | Loss: 0.00001854
Iteration 157/1000 | Loss: 0.00001854
Iteration 158/1000 | Loss: 0.00001854
Iteration 159/1000 | Loss: 0.00001853
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001850
Iteration 162/1000 | Loss: 0.00001850
Iteration 163/1000 | Loss: 0.00001849
Iteration 164/1000 | Loss: 0.00001847
Iteration 165/1000 | Loss: 0.00001844
Iteration 166/1000 | Loss: 0.00001844
Iteration 167/1000 | Loss: 0.00001844
Iteration 168/1000 | Loss: 0.00001844
Iteration 169/1000 | Loss: 0.00001844
Iteration 170/1000 | Loss: 0.00001844
Iteration 171/1000 | Loss: 0.00001844
Iteration 172/1000 | Loss: 0.00001844
Iteration 173/1000 | Loss: 0.00001844
Iteration 174/1000 | Loss: 0.00001843
Iteration 175/1000 | Loss: 0.00001843
Iteration 176/1000 | Loss: 0.00001843
Iteration 177/1000 | Loss: 0.00001841
Iteration 178/1000 | Loss: 0.00001841
Iteration 179/1000 | Loss: 0.00001841
Iteration 180/1000 | Loss: 0.00001840
Iteration 181/1000 | Loss: 0.00001840
Iteration 182/1000 | Loss: 0.00001840
Iteration 183/1000 | Loss: 0.00001840
Iteration 184/1000 | Loss: 0.00001839
Iteration 185/1000 | Loss: 0.00001836
Iteration 186/1000 | Loss: 0.00001836
Iteration 187/1000 | Loss: 0.00001832
Iteration 188/1000 | Loss: 0.00001831
Iteration 189/1000 | Loss: 0.00001827
Iteration 190/1000 | Loss: 0.00001824
Iteration 191/1000 | Loss: 0.00001822
Iteration 192/1000 | Loss: 0.00001822
Iteration 193/1000 | Loss: 0.00001819
Iteration 194/1000 | Loss: 0.00001818
Iteration 195/1000 | Loss: 0.00001818
Iteration 196/1000 | Loss: 0.00001818
Iteration 197/1000 | Loss: 0.00001818
Iteration 198/1000 | Loss: 0.00001818
Iteration 199/1000 | Loss: 0.00001818
Iteration 200/1000 | Loss: 0.00001818
Iteration 201/1000 | Loss: 0.00001818
Iteration 202/1000 | Loss: 0.00001818
Iteration 203/1000 | Loss: 0.00001818
Iteration 204/1000 | Loss: 0.00001818
Iteration 205/1000 | Loss: 0.00001818
Iteration 206/1000 | Loss: 0.00001817
Iteration 207/1000 | Loss: 0.00001817
Iteration 208/1000 | Loss: 0.00001816
Iteration 209/1000 | Loss: 0.00001816
Iteration 210/1000 | Loss: 0.00001816
Iteration 211/1000 | Loss: 0.00001816
Iteration 212/1000 | Loss: 0.00001816
Iteration 213/1000 | Loss: 0.00001816
Iteration 214/1000 | Loss: 0.00001816
Iteration 215/1000 | Loss: 0.00001816
Iteration 216/1000 | Loss: 0.00001815
Iteration 217/1000 | Loss: 0.00001815
Iteration 218/1000 | Loss: 0.00001815
Iteration 219/1000 | Loss: 0.00001815
Iteration 220/1000 | Loss: 0.00001815
Iteration 221/1000 | Loss: 0.00001815
Iteration 222/1000 | Loss: 0.00001815
Iteration 223/1000 | Loss: 0.00001815
Iteration 224/1000 | Loss: 0.00001815
Iteration 225/1000 | Loss: 0.00001815
Iteration 226/1000 | Loss: 0.00001815
Iteration 227/1000 | Loss: 0.00001814
Iteration 228/1000 | Loss: 0.00001814
Iteration 229/1000 | Loss: 0.00001814
Iteration 230/1000 | Loss: 0.00001814
Iteration 231/1000 | Loss: 0.00001814
Iteration 232/1000 | Loss: 0.00001813
Iteration 233/1000 | Loss: 0.00001813
Iteration 234/1000 | Loss: 0.00001813
Iteration 235/1000 | Loss: 0.00001813
Iteration 236/1000 | Loss: 0.00001813
Iteration 237/1000 | Loss: 0.00001813
Iteration 238/1000 | Loss: 0.00001813
Iteration 239/1000 | Loss: 0.00001813
Iteration 240/1000 | Loss: 0.00001812
Iteration 241/1000 | Loss: 0.00001812
Iteration 242/1000 | Loss: 0.00001812
Iteration 243/1000 | Loss: 0.00001812
Iteration 244/1000 | Loss: 0.00001812
Iteration 245/1000 | Loss: 0.00001812
Iteration 246/1000 | Loss: 0.00001812
Iteration 247/1000 | Loss: 0.00001812
Iteration 248/1000 | Loss: 0.00001812
Iteration 249/1000 | Loss: 0.00001812
Iteration 250/1000 | Loss: 0.00001811
Iteration 251/1000 | Loss: 0.00001811
Iteration 252/1000 | Loss: 0.00001811
Iteration 253/1000 | Loss: 0.00001811
Iteration 254/1000 | Loss: 0.00001811
Iteration 255/1000 | Loss: 0.00001811
Iteration 256/1000 | Loss: 0.00001811
Iteration 257/1000 | Loss: 0.00001811
Iteration 258/1000 | Loss: 0.00001811
Iteration 259/1000 | Loss: 0.00001811
Iteration 260/1000 | Loss: 0.00001811
Iteration 261/1000 | Loss: 0.00001811
Iteration 262/1000 | Loss: 0.00001811
Iteration 263/1000 | Loss: 0.00001811
Iteration 264/1000 | Loss: 0.00001811
Iteration 265/1000 | Loss: 0.00001811
Iteration 266/1000 | Loss: 0.00001811
Iteration 267/1000 | Loss: 0.00001811
Iteration 268/1000 | Loss: 0.00001811
Iteration 269/1000 | Loss: 0.00001811
Iteration 270/1000 | Loss: 0.00001811
Iteration 271/1000 | Loss: 0.00001811
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 271. Stopping optimization.
Last 5 losses: [1.8112883481080644e-05, 1.8112883481080644e-05, 1.8112883481080644e-05, 1.8112883481080644e-05, 1.8112883481080644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8112883481080644e-05

Optimization complete. Final v2v error: 3.466554641723633 mm

Highest mean error: 7.809227466583252 mm for frame 58

Lowest mean error: 3.220083475112915 mm for frame 24

Saving results

Total time: 283.3993926048279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821237
Iteration 2/25 | Loss: 0.00148490
Iteration 3/25 | Loss: 0.00126785
Iteration 4/25 | Loss: 0.00124927
Iteration 5/25 | Loss: 0.00124454
Iteration 6/25 | Loss: 0.00124378
Iteration 7/25 | Loss: 0.00124378
Iteration 8/25 | Loss: 0.00124378
Iteration 9/25 | Loss: 0.00124378
Iteration 10/25 | Loss: 0.00124378
Iteration 11/25 | Loss: 0.00124378
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001243778271600604, 0.001243778271600604, 0.001243778271600604, 0.001243778271600604, 0.001243778271600604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001243778271600604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12388420
Iteration 2/25 | Loss: 0.00094315
Iteration 3/25 | Loss: 0.00094312
Iteration 4/25 | Loss: 0.00094312
Iteration 5/25 | Loss: 0.00094312
Iteration 6/25 | Loss: 0.00094312
Iteration 7/25 | Loss: 0.00094312
Iteration 8/25 | Loss: 0.00094312
Iteration 9/25 | Loss: 0.00094312
Iteration 10/25 | Loss: 0.00094312
Iteration 11/25 | Loss: 0.00094312
Iteration 12/25 | Loss: 0.00094312
Iteration 13/25 | Loss: 0.00094312
Iteration 14/25 | Loss: 0.00094312
Iteration 15/25 | Loss: 0.00094312
Iteration 16/25 | Loss: 0.00094312
Iteration 17/25 | Loss: 0.00094312
Iteration 18/25 | Loss: 0.00094312
Iteration 19/25 | Loss: 0.00094312
Iteration 20/25 | Loss: 0.00094312
Iteration 21/25 | Loss: 0.00094312
Iteration 22/25 | Loss: 0.00094312
Iteration 23/25 | Loss: 0.00094312
Iteration 24/25 | Loss: 0.00094312
Iteration 25/25 | Loss: 0.00094312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094312
Iteration 2/1000 | Loss: 0.00004298
Iteration 3/1000 | Loss: 0.00003224
Iteration 4/1000 | Loss: 0.00002581
Iteration 5/1000 | Loss: 0.00002449
Iteration 6/1000 | Loss: 0.00002364
Iteration 7/1000 | Loss: 0.00002302
Iteration 8/1000 | Loss: 0.00002247
Iteration 9/1000 | Loss: 0.00002215
Iteration 10/1000 | Loss: 0.00002191
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002158
Iteration 13/1000 | Loss: 0.00002156
Iteration 14/1000 | Loss: 0.00002147
Iteration 15/1000 | Loss: 0.00002139
Iteration 16/1000 | Loss: 0.00002131
Iteration 17/1000 | Loss: 0.00002131
Iteration 18/1000 | Loss: 0.00002127
Iteration 19/1000 | Loss: 0.00002125
Iteration 20/1000 | Loss: 0.00002125
Iteration 21/1000 | Loss: 0.00002125
Iteration 22/1000 | Loss: 0.00002125
Iteration 23/1000 | Loss: 0.00002124
Iteration 24/1000 | Loss: 0.00002124
Iteration 25/1000 | Loss: 0.00002120
Iteration 26/1000 | Loss: 0.00002120
Iteration 27/1000 | Loss: 0.00002120
Iteration 28/1000 | Loss: 0.00002119
Iteration 29/1000 | Loss: 0.00002119
Iteration 30/1000 | Loss: 0.00002119
Iteration 31/1000 | Loss: 0.00002119
Iteration 32/1000 | Loss: 0.00002118
Iteration 33/1000 | Loss: 0.00002118
Iteration 34/1000 | Loss: 0.00002117
Iteration 35/1000 | Loss: 0.00002117
Iteration 36/1000 | Loss: 0.00002117
Iteration 37/1000 | Loss: 0.00002117
Iteration 38/1000 | Loss: 0.00002117
Iteration 39/1000 | Loss: 0.00002117
Iteration 40/1000 | Loss: 0.00002117
Iteration 41/1000 | Loss: 0.00002117
Iteration 42/1000 | Loss: 0.00002116
Iteration 43/1000 | Loss: 0.00002116
Iteration 44/1000 | Loss: 0.00002116
Iteration 45/1000 | Loss: 0.00002116
Iteration 46/1000 | Loss: 0.00002116
Iteration 47/1000 | Loss: 0.00002116
Iteration 48/1000 | Loss: 0.00002115
Iteration 49/1000 | Loss: 0.00002115
Iteration 50/1000 | Loss: 0.00002115
Iteration 51/1000 | Loss: 0.00002115
Iteration 52/1000 | Loss: 0.00002114
Iteration 53/1000 | Loss: 0.00002114
Iteration 54/1000 | Loss: 0.00002114
Iteration 55/1000 | Loss: 0.00002114
Iteration 56/1000 | Loss: 0.00002114
Iteration 57/1000 | Loss: 0.00002114
Iteration 58/1000 | Loss: 0.00002114
Iteration 59/1000 | Loss: 0.00002114
Iteration 60/1000 | Loss: 0.00002113
Iteration 61/1000 | Loss: 0.00002113
Iteration 62/1000 | Loss: 0.00002113
Iteration 63/1000 | Loss: 0.00002113
Iteration 64/1000 | Loss: 0.00002113
Iteration 65/1000 | Loss: 0.00002113
Iteration 66/1000 | Loss: 0.00002113
Iteration 67/1000 | Loss: 0.00002113
Iteration 68/1000 | Loss: 0.00002112
Iteration 69/1000 | Loss: 0.00002112
Iteration 70/1000 | Loss: 0.00002112
Iteration 71/1000 | Loss: 0.00002111
Iteration 72/1000 | Loss: 0.00002111
Iteration 73/1000 | Loss: 0.00002111
Iteration 74/1000 | Loss: 0.00002110
Iteration 75/1000 | Loss: 0.00002110
Iteration 76/1000 | Loss: 0.00002110
Iteration 77/1000 | Loss: 0.00002110
Iteration 78/1000 | Loss: 0.00002110
Iteration 79/1000 | Loss: 0.00002109
Iteration 80/1000 | Loss: 0.00002108
Iteration 81/1000 | Loss: 0.00002108
Iteration 82/1000 | Loss: 0.00002108
Iteration 83/1000 | Loss: 0.00002107
Iteration 84/1000 | Loss: 0.00002107
Iteration 85/1000 | Loss: 0.00002107
Iteration 86/1000 | Loss: 0.00002107
Iteration 87/1000 | Loss: 0.00002106
Iteration 88/1000 | Loss: 0.00002106
Iteration 89/1000 | Loss: 0.00002106
Iteration 90/1000 | Loss: 0.00002106
Iteration 91/1000 | Loss: 0.00002106
Iteration 92/1000 | Loss: 0.00002106
Iteration 93/1000 | Loss: 0.00002106
Iteration 94/1000 | Loss: 0.00002106
Iteration 95/1000 | Loss: 0.00002106
Iteration 96/1000 | Loss: 0.00002106
Iteration 97/1000 | Loss: 0.00002105
Iteration 98/1000 | Loss: 0.00002105
Iteration 99/1000 | Loss: 0.00002105
Iteration 100/1000 | Loss: 0.00002104
Iteration 101/1000 | Loss: 0.00002104
Iteration 102/1000 | Loss: 0.00002104
Iteration 103/1000 | Loss: 0.00002104
Iteration 104/1000 | Loss: 0.00002103
Iteration 105/1000 | Loss: 0.00002103
Iteration 106/1000 | Loss: 0.00002103
Iteration 107/1000 | Loss: 0.00002103
Iteration 108/1000 | Loss: 0.00002102
Iteration 109/1000 | Loss: 0.00002102
Iteration 110/1000 | Loss: 0.00002102
Iteration 111/1000 | Loss: 0.00002102
Iteration 112/1000 | Loss: 0.00002101
Iteration 113/1000 | Loss: 0.00002101
Iteration 114/1000 | Loss: 0.00002101
Iteration 115/1000 | Loss: 0.00002100
Iteration 116/1000 | Loss: 0.00002100
Iteration 117/1000 | Loss: 0.00002100
Iteration 118/1000 | Loss: 0.00002100
Iteration 119/1000 | Loss: 0.00002099
Iteration 120/1000 | Loss: 0.00002099
Iteration 121/1000 | Loss: 0.00002099
Iteration 122/1000 | Loss: 0.00002099
Iteration 123/1000 | Loss: 0.00002099
Iteration 124/1000 | Loss: 0.00002099
Iteration 125/1000 | Loss: 0.00002098
Iteration 126/1000 | Loss: 0.00002098
Iteration 127/1000 | Loss: 0.00002098
Iteration 128/1000 | Loss: 0.00002098
Iteration 129/1000 | Loss: 0.00002098
Iteration 130/1000 | Loss: 0.00002098
Iteration 131/1000 | Loss: 0.00002097
Iteration 132/1000 | Loss: 0.00002097
Iteration 133/1000 | Loss: 0.00002097
Iteration 134/1000 | Loss: 0.00002097
Iteration 135/1000 | Loss: 0.00002097
Iteration 136/1000 | Loss: 0.00002097
Iteration 137/1000 | Loss: 0.00002096
Iteration 138/1000 | Loss: 0.00002096
Iteration 139/1000 | Loss: 0.00002096
Iteration 140/1000 | Loss: 0.00002096
Iteration 141/1000 | Loss: 0.00002096
Iteration 142/1000 | Loss: 0.00002096
Iteration 143/1000 | Loss: 0.00002096
Iteration 144/1000 | Loss: 0.00002096
Iteration 145/1000 | Loss: 0.00002096
Iteration 146/1000 | Loss: 0.00002096
Iteration 147/1000 | Loss: 0.00002096
Iteration 148/1000 | Loss: 0.00002096
Iteration 149/1000 | Loss: 0.00002095
Iteration 150/1000 | Loss: 0.00002095
Iteration 151/1000 | Loss: 0.00002095
Iteration 152/1000 | Loss: 0.00002095
Iteration 153/1000 | Loss: 0.00002094
Iteration 154/1000 | Loss: 0.00002094
Iteration 155/1000 | Loss: 0.00002094
Iteration 156/1000 | Loss: 0.00002094
Iteration 157/1000 | Loss: 0.00002094
Iteration 158/1000 | Loss: 0.00002094
Iteration 159/1000 | Loss: 0.00002094
Iteration 160/1000 | Loss: 0.00002094
Iteration 161/1000 | Loss: 0.00002094
Iteration 162/1000 | Loss: 0.00002094
Iteration 163/1000 | Loss: 0.00002094
Iteration 164/1000 | Loss: 0.00002094
Iteration 165/1000 | Loss: 0.00002094
Iteration 166/1000 | Loss: 0.00002094
Iteration 167/1000 | Loss: 0.00002093
Iteration 168/1000 | Loss: 0.00002093
Iteration 169/1000 | Loss: 0.00002093
Iteration 170/1000 | Loss: 0.00002093
Iteration 171/1000 | Loss: 0.00002093
Iteration 172/1000 | Loss: 0.00002093
Iteration 173/1000 | Loss: 0.00002093
Iteration 174/1000 | Loss: 0.00002092
Iteration 175/1000 | Loss: 0.00002092
Iteration 176/1000 | Loss: 0.00002092
Iteration 177/1000 | Loss: 0.00002092
Iteration 178/1000 | Loss: 0.00002091
Iteration 179/1000 | Loss: 0.00002091
Iteration 180/1000 | Loss: 0.00002091
Iteration 181/1000 | Loss: 0.00002091
Iteration 182/1000 | Loss: 0.00002090
Iteration 183/1000 | Loss: 0.00002090
Iteration 184/1000 | Loss: 0.00002090
Iteration 185/1000 | Loss: 0.00002090
Iteration 186/1000 | Loss: 0.00002090
Iteration 187/1000 | Loss: 0.00002089
Iteration 188/1000 | Loss: 0.00002089
Iteration 189/1000 | Loss: 0.00002089
Iteration 190/1000 | Loss: 0.00002089
Iteration 191/1000 | Loss: 0.00002088
Iteration 192/1000 | Loss: 0.00002088
Iteration 193/1000 | Loss: 0.00002088
Iteration 194/1000 | Loss: 0.00002088
Iteration 195/1000 | Loss: 0.00002087
Iteration 196/1000 | Loss: 0.00002087
Iteration 197/1000 | Loss: 0.00002087
Iteration 198/1000 | Loss: 0.00002087
Iteration 199/1000 | Loss: 0.00002087
Iteration 200/1000 | Loss: 0.00002087
Iteration 201/1000 | Loss: 0.00002087
Iteration 202/1000 | Loss: 0.00002086
Iteration 203/1000 | Loss: 0.00002086
Iteration 204/1000 | Loss: 0.00002086
Iteration 205/1000 | Loss: 0.00002086
Iteration 206/1000 | Loss: 0.00002086
Iteration 207/1000 | Loss: 0.00002085
Iteration 208/1000 | Loss: 0.00002085
Iteration 209/1000 | Loss: 0.00002085
Iteration 210/1000 | Loss: 0.00002085
Iteration 211/1000 | Loss: 0.00002085
Iteration 212/1000 | Loss: 0.00002085
Iteration 213/1000 | Loss: 0.00002085
Iteration 214/1000 | Loss: 0.00002085
Iteration 215/1000 | Loss: 0.00002084
Iteration 216/1000 | Loss: 0.00002084
Iteration 217/1000 | Loss: 0.00002084
Iteration 218/1000 | Loss: 0.00002084
Iteration 219/1000 | Loss: 0.00002084
Iteration 220/1000 | Loss: 0.00002084
Iteration 221/1000 | Loss: 0.00002084
Iteration 222/1000 | Loss: 0.00002084
Iteration 223/1000 | Loss: 0.00002084
Iteration 224/1000 | Loss: 0.00002084
Iteration 225/1000 | Loss: 0.00002084
Iteration 226/1000 | Loss: 0.00002083
Iteration 227/1000 | Loss: 0.00002083
Iteration 228/1000 | Loss: 0.00002083
Iteration 229/1000 | Loss: 0.00002083
Iteration 230/1000 | Loss: 0.00002083
Iteration 231/1000 | Loss: 0.00002083
Iteration 232/1000 | Loss: 0.00002083
Iteration 233/1000 | Loss: 0.00002083
Iteration 234/1000 | Loss: 0.00002083
Iteration 235/1000 | Loss: 0.00002083
Iteration 236/1000 | Loss: 0.00002083
Iteration 237/1000 | Loss: 0.00002083
Iteration 238/1000 | Loss: 0.00002083
Iteration 239/1000 | Loss: 0.00002083
Iteration 240/1000 | Loss: 0.00002083
Iteration 241/1000 | Loss: 0.00002083
Iteration 242/1000 | Loss: 0.00002083
Iteration 243/1000 | Loss: 0.00002083
Iteration 244/1000 | Loss: 0.00002083
Iteration 245/1000 | Loss: 0.00002083
Iteration 246/1000 | Loss: 0.00002083
Iteration 247/1000 | Loss: 0.00002083
Iteration 248/1000 | Loss: 0.00002083
Iteration 249/1000 | Loss: 0.00002083
Iteration 250/1000 | Loss: 0.00002083
Iteration 251/1000 | Loss: 0.00002083
Iteration 252/1000 | Loss: 0.00002083
Iteration 253/1000 | Loss: 0.00002083
Iteration 254/1000 | Loss: 0.00002083
Iteration 255/1000 | Loss: 0.00002083
Iteration 256/1000 | Loss: 0.00002083
Iteration 257/1000 | Loss: 0.00002083
Iteration 258/1000 | Loss: 0.00002083
Iteration 259/1000 | Loss: 0.00002083
Iteration 260/1000 | Loss: 0.00002083
Iteration 261/1000 | Loss: 0.00002083
Iteration 262/1000 | Loss: 0.00002083
Iteration 263/1000 | Loss: 0.00002083
Iteration 264/1000 | Loss: 0.00002083
Iteration 265/1000 | Loss: 0.00002083
Iteration 266/1000 | Loss: 0.00002083
Iteration 267/1000 | Loss: 0.00002083
Iteration 268/1000 | Loss: 0.00002083
Iteration 269/1000 | Loss: 0.00002083
Iteration 270/1000 | Loss: 0.00002083
Iteration 271/1000 | Loss: 0.00002083
Iteration 272/1000 | Loss: 0.00002083
Iteration 273/1000 | Loss: 0.00002083
Iteration 274/1000 | Loss: 0.00002083
Iteration 275/1000 | Loss: 0.00002083
Iteration 276/1000 | Loss: 0.00002083
Iteration 277/1000 | Loss: 0.00002083
Iteration 278/1000 | Loss: 0.00002083
Iteration 279/1000 | Loss: 0.00002083
Iteration 280/1000 | Loss: 0.00002083
Iteration 281/1000 | Loss: 0.00002083
Iteration 282/1000 | Loss: 0.00002083
Iteration 283/1000 | Loss: 0.00002083
Iteration 284/1000 | Loss: 0.00002083
Iteration 285/1000 | Loss: 0.00002083
Iteration 286/1000 | Loss: 0.00002083
Iteration 287/1000 | Loss: 0.00002083
Iteration 288/1000 | Loss: 0.00002083
Iteration 289/1000 | Loss: 0.00002083
Iteration 290/1000 | Loss: 0.00002083
Iteration 291/1000 | Loss: 0.00002083
Iteration 292/1000 | Loss: 0.00002083
Iteration 293/1000 | Loss: 0.00002083
Iteration 294/1000 | Loss: 0.00002083
Iteration 295/1000 | Loss: 0.00002083
Iteration 296/1000 | Loss: 0.00002083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 296. Stopping optimization.
Last 5 losses: [2.0832228983636014e-05, 2.0832228983636014e-05, 2.0832228983636014e-05, 2.0832228983636014e-05, 2.0832228983636014e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0832228983636014e-05

Optimization complete. Final v2v error: 3.6445980072021484 mm

Highest mean error: 4.4507737159729 mm for frame 61

Lowest mean error: 3.100506067276001 mm for frame 24

Saving results

Total time: 47.63275218009949
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013670
Iteration 2/25 | Loss: 0.00198657
Iteration 3/25 | Loss: 0.00244578
Iteration 4/25 | Loss: 0.00140282
Iteration 5/25 | Loss: 0.00129614
Iteration 6/25 | Loss: 0.00114074
Iteration 7/25 | Loss: 0.00113216
Iteration 8/25 | Loss: 0.00112963
Iteration 9/25 | Loss: 0.00113192
Iteration 10/25 | Loss: 0.00112845
Iteration 11/25 | Loss: 0.00112196
Iteration 12/25 | Loss: 0.00111967
Iteration 13/25 | Loss: 0.00111832
Iteration 14/25 | Loss: 0.00111768
Iteration 15/25 | Loss: 0.00111729
Iteration 16/25 | Loss: 0.00111600
Iteration 17/25 | Loss: 0.00111571
Iteration 18/25 | Loss: 0.00111561
Iteration 19/25 | Loss: 0.00111560
Iteration 20/25 | Loss: 0.00111560
Iteration 21/25 | Loss: 0.00111559
Iteration 22/25 | Loss: 0.00111559
Iteration 23/25 | Loss: 0.00111558
Iteration 24/25 | Loss: 0.00111557
Iteration 25/25 | Loss: 0.00111557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45580840
Iteration 2/25 | Loss: 0.00091338
Iteration 3/25 | Loss: 0.00091338
Iteration 4/25 | Loss: 0.00091338
Iteration 5/25 | Loss: 0.00091338
Iteration 6/25 | Loss: 0.00091338
Iteration 7/25 | Loss: 0.00091338
Iteration 8/25 | Loss: 0.00091337
Iteration 9/25 | Loss: 0.00091337
Iteration 10/25 | Loss: 0.00091337
Iteration 11/25 | Loss: 0.00091337
Iteration 12/25 | Loss: 0.00091337
Iteration 13/25 | Loss: 0.00091337
Iteration 14/25 | Loss: 0.00091337
Iteration 15/25 | Loss: 0.00091337
Iteration 16/25 | Loss: 0.00091337
Iteration 17/25 | Loss: 0.00091337
Iteration 18/25 | Loss: 0.00091337
Iteration 19/25 | Loss: 0.00091337
Iteration 20/25 | Loss: 0.00091337
Iteration 21/25 | Loss: 0.00091337
Iteration 22/25 | Loss: 0.00091337
Iteration 23/25 | Loss: 0.00091337
Iteration 24/25 | Loss: 0.00091337
Iteration 25/25 | Loss: 0.00091337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091337
Iteration 2/1000 | Loss: 0.00002798
Iteration 3/1000 | Loss: 0.00001855
Iteration 4/1000 | Loss: 0.00001630
Iteration 5/1000 | Loss: 0.00001478
Iteration 6/1000 | Loss: 0.00001395
Iteration 7/1000 | Loss: 0.00001332
Iteration 8/1000 | Loss: 0.00001295
Iteration 9/1000 | Loss: 0.00001272
Iteration 10/1000 | Loss: 0.00001252
Iteration 11/1000 | Loss: 0.00001243
Iteration 12/1000 | Loss: 0.00001242
Iteration 13/1000 | Loss: 0.00001239
Iteration 14/1000 | Loss: 0.00001239
Iteration 15/1000 | Loss: 0.00001237
Iteration 16/1000 | Loss: 0.00001236
Iteration 17/1000 | Loss: 0.00001235
Iteration 18/1000 | Loss: 0.00001235
Iteration 19/1000 | Loss: 0.00001234
Iteration 20/1000 | Loss: 0.00001234
Iteration 21/1000 | Loss: 0.00001227
Iteration 22/1000 | Loss: 0.00001225
Iteration 23/1000 | Loss: 0.00001224
Iteration 24/1000 | Loss: 0.00001224
Iteration 25/1000 | Loss: 0.00001223
Iteration 26/1000 | Loss: 0.00001223
Iteration 27/1000 | Loss: 0.00001219
Iteration 28/1000 | Loss: 0.00001219
Iteration 29/1000 | Loss: 0.00001217
Iteration 30/1000 | Loss: 0.00001217
Iteration 31/1000 | Loss: 0.00001217
Iteration 32/1000 | Loss: 0.00001217
Iteration 33/1000 | Loss: 0.00001217
Iteration 34/1000 | Loss: 0.00001217
Iteration 35/1000 | Loss: 0.00001216
Iteration 36/1000 | Loss: 0.00001216
Iteration 37/1000 | Loss: 0.00001216
Iteration 38/1000 | Loss: 0.00001216
Iteration 39/1000 | Loss: 0.00001216
Iteration 40/1000 | Loss: 0.00001216
Iteration 41/1000 | Loss: 0.00001216
Iteration 42/1000 | Loss: 0.00001216
Iteration 43/1000 | Loss: 0.00001216
Iteration 44/1000 | Loss: 0.00001216
Iteration 45/1000 | Loss: 0.00001216
Iteration 46/1000 | Loss: 0.00001215
Iteration 47/1000 | Loss: 0.00001215
Iteration 48/1000 | Loss: 0.00001215
Iteration 49/1000 | Loss: 0.00001214
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001214
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001214
Iteration 56/1000 | Loss: 0.00001213
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001211
Iteration 62/1000 | Loss: 0.00001211
Iteration 63/1000 | Loss: 0.00001211
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001211
Iteration 68/1000 | Loss: 0.00001211
Iteration 69/1000 | Loss: 0.00001211
Iteration 70/1000 | Loss: 0.00001211
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001209
Iteration 76/1000 | Loss: 0.00001208
Iteration 77/1000 | Loss: 0.00001208
Iteration 78/1000 | Loss: 0.00001208
Iteration 79/1000 | Loss: 0.00001208
Iteration 80/1000 | Loss: 0.00001208
Iteration 81/1000 | Loss: 0.00001208
Iteration 82/1000 | Loss: 0.00001208
Iteration 83/1000 | Loss: 0.00001207
Iteration 84/1000 | Loss: 0.00001207
Iteration 85/1000 | Loss: 0.00001207
Iteration 86/1000 | Loss: 0.00001207
Iteration 87/1000 | Loss: 0.00001207
Iteration 88/1000 | Loss: 0.00001206
Iteration 89/1000 | Loss: 0.00001206
Iteration 90/1000 | Loss: 0.00001206
Iteration 91/1000 | Loss: 0.00001206
Iteration 92/1000 | Loss: 0.00001206
Iteration 93/1000 | Loss: 0.00001205
Iteration 94/1000 | Loss: 0.00001205
Iteration 95/1000 | Loss: 0.00001205
Iteration 96/1000 | Loss: 0.00001205
Iteration 97/1000 | Loss: 0.00001205
Iteration 98/1000 | Loss: 0.00001205
Iteration 99/1000 | Loss: 0.00001205
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001204
Iteration 103/1000 | Loss: 0.00001204
Iteration 104/1000 | Loss: 0.00001204
Iteration 105/1000 | Loss: 0.00001204
Iteration 106/1000 | Loss: 0.00001204
Iteration 107/1000 | Loss: 0.00001204
Iteration 108/1000 | Loss: 0.00001204
Iteration 109/1000 | Loss: 0.00001203
Iteration 110/1000 | Loss: 0.00001203
Iteration 111/1000 | Loss: 0.00001203
Iteration 112/1000 | Loss: 0.00001203
Iteration 113/1000 | Loss: 0.00001203
Iteration 114/1000 | Loss: 0.00001203
Iteration 115/1000 | Loss: 0.00001203
Iteration 116/1000 | Loss: 0.00001203
Iteration 117/1000 | Loss: 0.00001203
Iteration 118/1000 | Loss: 0.00001202
Iteration 119/1000 | Loss: 0.00001202
Iteration 120/1000 | Loss: 0.00001202
Iteration 121/1000 | Loss: 0.00001202
Iteration 122/1000 | Loss: 0.00001201
Iteration 123/1000 | Loss: 0.00001201
Iteration 124/1000 | Loss: 0.00001201
Iteration 125/1000 | Loss: 0.00001201
Iteration 126/1000 | Loss: 0.00001201
Iteration 127/1000 | Loss: 0.00001201
Iteration 128/1000 | Loss: 0.00001201
Iteration 129/1000 | Loss: 0.00001201
Iteration 130/1000 | Loss: 0.00001201
Iteration 131/1000 | Loss: 0.00001200
Iteration 132/1000 | Loss: 0.00001200
Iteration 133/1000 | Loss: 0.00001200
Iteration 134/1000 | Loss: 0.00001200
Iteration 135/1000 | Loss: 0.00001200
Iteration 136/1000 | Loss: 0.00001199
Iteration 137/1000 | Loss: 0.00001199
Iteration 138/1000 | Loss: 0.00001199
Iteration 139/1000 | Loss: 0.00001199
Iteration 140/1000 | Loss: 0.00001199
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001198
Iteration 145/1000 | Loss: 0.00001198
Iteration 146/1000 | Loss: 0.00001198
Iteration 147/1000 | Loss: 0.00001198
Iteration 148/1000 | Loss: 0.00001197
Iteration 149/1000 | Loss: 0.00001197
Iteration 150/1000 | Loss: 0.00001197
Iteration 151/1000 | Loss: 0.00001197
Iteration 152/1000 | Loss: 0.00001197
Iteration 153/1000 | Loss: 0.00001196
Iteration 154/1000 | Loss: 0.00001196
Iteration 155/1000 | Loss: 0.00001196
Iteration 156/1000 | Loss: 0.00001195
Iteration 157/1000 | Loss: 0.00001195
Iteration 158/1000 | Loss: 0.00001195
Iteration 159/1000 | Loss: 0.00001195
Iteration 160/1000 | Loss: 0.00001194
Iteration 161/1000 | Loss: 0.00001194
Iteration 162/1000 | Loss: 0.00001194
Iteration 163/1000 | Loss: 0.00001194
Iteration 164/1000 | Loss: 0.00001194
Iteration 165/1000 | Loss: 0.00001193
Iteration 166/1000 | Loss: 0.00001193
Iteration 167/1000 | Loss: 0.00001193
Iteration 168/1000 | Loss: 0.00001192
Iteration 169/1000 | Loss: 0.00001192
Iteration 170/1000 | Loss: 0.00001192
Iteration 171/1000 | Loss: 0.00001192
Iteration 172/1000 | Loss: 0.00001192
Iteration 173/1000 | Loss: 0.00001191
Iteration 174/1000 | Loss: 0.00001191
Iteration 175/1000 | Loss: 0.00001191
Iteration 176/1000 | Loss: 0.00001191
Iteration 177/1000 | Loss: 0.00001191
Iteration 178/1000 | Loss: 0.00001191
Iteration 179/1000 | Loss: 0.00001190
Iteration 180/1000 | Loss: 0.00001190
Iteration 181/1000 | Loss: 0.00001190
Iteration 182/1000 | Loss: 0.00001190
Iteration 183/1000 | Loss: 0.00001190
Iteration 184/1000 | Loss: 0.00001190
Iteration 185/1000 | Loss: 0.00001190
Iteration 186/1000 | Loss: 0.00001189
Iteration 187/1000 | Loss: 0.00001189
Iteration 188/1000 | Loss: 0.00001189
Iteration 189/1000 | Loss: 0.00001189
Iteration 190/1000 | Loss: 0.00001189
Iteration 191/1000 | Loss: 0.00001189
Iteration 192/1000 | Loss: 0.00001189
Iteration 193/1000 | Loss: 0.00001189
Iteration 194/1000 | Loss: 0.00001189
Iteration 195/1000 | Loss: 0.00001189
Iteration 196/1000 | Loss: 0.00001189
Iteration 197/1000 | Loss: 0.00001189
Iteration 198/1000 | Loss: 0.00001189
Iteration 199/1000 | Loss: 0.00001189
Iteration 200/1000 | Loss: 0.00001189
Iteration 201/1000 | Loss: 0.00001189
Iteration 202/1000 | Loss: 0.00001189
Iteration 203/1000 | Loss: 0.00001189
Iteration 204/1000 | Loss: 0.00001189
Iteration 205/1000 | Loss: 0.00001189
Iteration 206/1000 | Loss: 0.00001189
Iteration 207/1000 | Loss: 0.00001189
Iteration 208/1000 | Loss: 0.00001189
Iteration 209/1000 | Loss: 0.00001189
Iteration 210/1000 | Loss: 0.00001189
Iteration 211/1000 | Loss: 0.00001189
Iteration 212/1000 | Loss: 0.00001189
Iteration 213/1000 | Loss: 0.00001189
Iteration 214/1000 | Loss: 0.00001189
Iteration 215/1000 | Loss: 0.00001189
Iteration 216/1000 | Loss: 0.00001189
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 216. Stopping optimization.
Last 5 losses: [1.189227077702526e-05, 1.189227077702526e-05, 1.189227077702526e-05, 1.189227077702526e-05, 1.189227077702526e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.189227077702526e-05

Optimization complete. Final v2v error: 2.8887977600097656 mm

Highest mean error: 3.577913522720337 mm for frame 88

Lowest mean error: 2.5193400382995605 mm for frame 125

Saving results

Total time: 63.829875230789185
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991584
Iteration 2/25 | Loss: 0.00323434
Iteration 3/25 | Loss: 0.00242225
Iteration 4/25 | Loss: 0.00198149
Iteration 5/25 | Loss: 0.00210687
Iteration 6/25 | Loss: 0.00185140
Iteration 7/25 | Loss: 0.00170189
Iteration 8/25 | Loss: 0.00162304
Iteration 9/25 | Loss: 0.00159305
Iteration 10/25 | Loss: 0.00157686
Iteration 11/25 | Loss: 0.00158112
Iteration 12/25 | Loss: 0.00157082
Iteration 13/25 | Loss: 0.00156800
Iteration 14/25 | Loss: 0.00156366
Iteration 15/25 | Loss: 0.00156773
Iteration 16/25 | Loss: 0.00156833
Iteration 17/25 | Loss: 0.00156700
Iteration 18/25 | Loss: 0.00156229
Iteration 19/25 | Loss: 0.00155843
Iteration 20/25 | Loss: 0.00155667
Iteration 21/25 | Loss: 0.00155641
Iteration 22/25 | Loss: 0.00155212
Iteration 23/25 | Loss: 0.00155076
Iteration 24/25 | Loss: 0.00154987
Iteration 25/25 | Loss: 0.00154909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36548281
Iteration 2/25 | Loss: 0.00592482
Iteration 3/25 | Loss: 0.00480801
Iteration 4/25 | Loss: 0.00480801
Iteration 5/25 | Loss: 0.00480801
Iteration 6/25 | Loss: 0.00480801
Iteration 7/25 | Loss: 0.00480801
Iteration 8/25 | Loss: 0.00480801
Iteration 9/25 | Loss: 0.00480801
Iteration 10/25 | Loss: 0.00480801
Iteration 11/25 | Loss: 0.00480801
Iteration 12/25 | Loss: 0.00480801
Iteration 13/25 | Loss: 0.00480801
Iteration 14/25 | Loss: 0.00480801
Iteration 15/25 | Loss: 0.00480801
Iteration 16/25 | Loss: 0.00480801
Iteration 17/25 | Loss: 0.00480801
Iteration 18/25 | Loss: 0.00480801
Iteration 19/25 | Loss: 0.00480801
Iteration 20/25 | Loss: 0.00480801
Iteration 21/25 | Loss: 0.00480801
Iteration 22/25 | Loss: 0.00480801
Iteration 23/25 | Loss: 0.00480801
Iteration 24/25 | Loss: 0.00480801
Iteration 25/25 | Loss: 0.00480801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00480801
Iteration 2/1000 | Loss: 0.00092067
Iteration 3/1000 | Loss: 0.00083663
Iteration 4/1000 | Loss: 0.00279249
Iteration 5/1000 | Loss: 0.00371054
Iteration 6/1000 | Loss: 0.00033369
Iteration 7/1000 | Loss: 0.00268411
Iteration 8/1000 | Loss: 0.00123203
Iteration 9/1000 | Loss: 0.00389714
Iteration 10/1000 | Loss: 0.00026383
Iteration 11/1000 | Loss: 0.00121855
Iteration 12/1000 | Loss: 0.00048944
Iteration 13/1000 | Loss: 0.00062882
Iteration 14/1000 | Loss: 0.00026111
Iteration 15/1000 | Loss: 0.00013645
Iteration 16/1000 | Loss: 0.00137543
Iteration 17/1000 | Loss: 0.00310990
Iteration 18/1000 | Loss: 0.00550378
Iteration 19/1000 | Loss: 0.00076840
Iteration 20/1000 | Loss: 0.00017606
Iteration 21/1000 | Loss: 0.00013960
Iteration 22/1000 | Loss: 0.00012077
Iteration 23/1000 | Loss: 0.00046414
Iteration 24/1000 | Loss: 0.00159336
Iteration 25/1000 | Loss: 0.00010533
Iteration 26/1000 | Loss: 0.00009616
Iteration 27/1000 | Loss: 0.00043650
Iteration 28/1000 | Loss: 0.00045394
Iteration 29/1000 | Loss: 0.00118193
Iteration 30/1000 | Loss: 0.00078094
Iteration 31/1000 | Loss: 0.00048931
Iteration 32/1000 | Loss: 0.00153058
Iteration 33/1000 | Loss: 0.00269013
Iteration 34/1000 | Loss: 0.00091621
Iteration 35/1000 | Loss: 0.00164938
Iteration 36/1000 | Loss: 0.00142023
Iteration 37/1000 | Loss: 0.00033795
Iteration 38/1000 | Loss: 0.00048949
Iteration 39/1000 | Loss: 0.00254535
Iteration 40/1000 | Loss: 0.00075352
Iteration 41/1000 | Loss: 0.00039039
Iteration 42/1000 | Loss: 0.00062136
Iteration 43/1000 | Loss: 0.00042876
Iteration 44/1000 | Loss: 0.00070157
Iteration 45/1000 | Loss: 0.00175010
Iteration 46/1000 | Loss: 0.00063467
Iteration 47/1000 | Loss: 0.00007976
Iteration 48/1000 | Loss: 0.00028712
Iteration 49/1000 | Loss: 0.00031431
Iteration 50/1000 | Loss: 0.00006531
Iteration 51/1000 | Loss: 0.00005760
Iteration 52/1000 | Loss: 0.00025881
Iteration 53/1000 | Loss: 0.00054394
Iteration 54/1000 | Loss: 0.00015999
Iteration 55/1000 | Loss: 0.00005387
Iteration 56/1000 | Loss: 0.00004867
Iteration 57/1000 | Loss: 0.00083327
Iteration 58/1000 | Loss: 0.00062312
Iteration 59/1000 | Loss: 0.00053059
Iteration 60/1000 | Loss: 0.00271720
Iteration 61/1000 | Loss: 0.00050567
Iteration 62/1000 | Loss: 0.00028504
Iteration 63/1000 | Loss: 0.00025964
Iteration 64/1000 | Loss: 0.00026152
Iteration 65/1000 | Loss: 0.00020581
Iteration 66/1000 | Loss: 0.00021917
Iteration 67/1000 | Loss: 0.00009187
Iteration 68/1000 | Loss: 0.00028957
Iteration 69/1000 | Loss: 0.00014526
Iteration 70/1000 | Loss: 0.00011484
Iteration 71/1000 | Loss: 0.00035640
Iteration 72/1000 | Loss: 0.00009766
Iteration 73/1000 | Loss: 0.00004550
Iteration 74/1000 | Loss: 0.00004309
Iteration 75/1000 | Loss: 0.00025829
Iteration 76/1000 | Loss: 0.00033780
Iteration 77/1000 | Loss: 0.00010052
Iteration 78/1000 | Loss: 0.00014168
Iteration 79/1000 | Loss: 0.00132189
Iteration 80/1000 | Loss: 0.00024196
Iteration 81/1000 | Loss: 0.00004352
Iteration 82/1000 | Loss: 0.00015491
Iteration 83/1000 | Loss: 0.00023419
Iteration 84/1000 | Loss: 0.00049633
Iteration 85/1000 | Loss: 0.00018579
Iteration 86/1000 | Loss: 0.00018125
Iteration 87/1000 | Loss: 0.00016394
Iteration 88/1000 | Loss: 0.00012897
Iteration 89/1000 | Loss: 0.00014648
Iteration 90/1000 | Loss: 0.00012562
Iteration 91/1000 | Loss: 0.00014747
Iteration 92/1000 | Loss: 0.00019332
Iteration 93/1000 | Loss: 0.00019486
Iteration 94/1000 | Loss: 0.00017991
Iteration 95/1000 | Loss: 0.00019350
Iteration 96/1000 | Loss: 0.00020770
Iteration 97/1000 | Loss: 0.00004686
Iteration 98/1000 | Loss: 0.00004364
Iteration 99/1000 | Loss: 0.00004202
Iteration 100/1000 | Loss: 0.00004118
Iteration 101/1000 | Loss: 0.00004029
Iteration 102/1000 | Loss: 0.00032232
Iteration 103/1000 | Loss: 0.00005040
Iteration 104/1000 | Loss: 0.00026871
Iteration 105/1000 | Loss: 0.00070498
Iteration 106/1000 | Loss: 0.00045929
Iteration 107/1000 | Loss: 0.00004905
Iteration 108/1000 | Loss: 0.00005511
Iteration 109/1000 | Loss: 0.00004823
Iteration 110/1000 | Loss: 0.00003959
Iteration 111/1000 | Loss: 0.00003721
Iteration 112/1000 | Loss: 0.00003565
Iteration 113/1000 | Loss: 0.00003445
Iteration 114/1000 | Loss: 0.00003384
Iteration 115/1000 | Loss: 0.00003337
Iteration 116/1000 | Loss: 0.00003301
Iteration 117/1000 | Loss: 0.00003288
Iteration 118/1000 | Loss: 0.00003287
Iteration 119/1000 | Loss: 0.00003277
Iteration 120/1000 | Loss: 0.00003276
Iteration 121/1000 | Loss: 0.00003273
Iteration 122/1000 | Loss: 0.00003272
Iteration 123/1000 | Loss: 0.00024848
Iteration 124/1000 | Loss: 0.00027253
Iteration 125/1000 | Loss: 0.00003517
Iteration 126/1000 | Loss: 0.00003163
Iteration 127/1000 | Loss: 0.00003032
Iteration 128/1000 | Loss: 0.00002948
Iteration 129/1000 | Loss: 0.00002880
Iteration 130/1000 | Loss: 0.00002842
Iteration 131/1000 | Loss: 0.00002819
Iteration 132/1000 | Loss: 0.00002818
Iteration 133/1000 | Loss: 0.00002795
Iteration 134/1000 | Loss: 0.00002789
Iteration 135/1000 | Loss: 0.00002786
Iteration 136/1000 | Loss: 0.00002779
Iteration 137/1000 | Loss: 0.00002759
Iteration 138/1000 | Loss: 0.00002753
Iteration 139/1000 | Loss: 0.00002748
Iteration 140/1000 | Loss: 0.00002747
Iteration 141/1000 | Loss: 0.00002747
Iteration 142/1000 | Loss: 0.00002747
Iteration 143/1000 | Loss: 0.00002746
Iteration 144/1000 | Loss: 0.00002746
Iteration 145/1000 | Loss: 0.00002746
Iteration 146/1000 | Loss: 0.00002746
Iteration 147/1000 | Loss: 0.00002746
Iteration 148/1000 | Loss: 0.00002745
Iteration 149/1000 | Loss: 0.00002745
Iteration 150/1000 | Loss: 0.00002745
Iteration 151/1000 | Loss: 0.00002744
Iteration 152/1000 | Loss: 0.00002744
Iteration 153/1000 | Loss: 0.00002743
Iteration 154/1000 | Loss: 0.00002743
Iteration 155/1000 | Loss: 0.00002743
Iteration 156/1000 | Loss: 0.00002742
Iteration 157/1000 | Loss: 0.00002741
Iteration 158/1000 | Loss: 0.00002741
Iteration 159/1000 | Loss: 0.00002737
Iteration 160/1000 | Loss: 0.00002736
Iteration 161/1000 | Loss: 0.00002735
Iteration 162/1000 | Loss: 0.00002733
Iteration 163/1000 | Loss: 0.00002733
Iteration 164/1000 | Loss: 0.00002732
Iteration 165/1000 | Loss: 0.00002731
Iteration 166/1000 | Loss: 0.00002731
Iteration 167/1000 | Loss: 0.00002730
Iteration 168/1000 | Loss: 0.00014318
Iteration 169/1000 | Loss: 0.00054866
Iteration 170/1000 | Loss: 0.00033591
Iteration 171/1000 | Loss: 0.00039679
Iteration 172/1000 | Loss: 0.00002732
Iteration 173/1000 | Loss: 0.00002605
Iteration 174/1000 | Loss: 0.00002473
Iteration 175/1000 | Loss: 0.00002392
Iteration 176/1000 | Loss: 0.00002354
Iteration 177/1000 | Loss: 0.00002343
Iteration 178/1000 | Loss: 0.00002338
Iteration 179/1000 | Loss: 0.00002322
Iteration 180/1000 | Loss: 0.00002320
Iteration 181/1000 | Loss: 0.00002313
Iteration 182/1000 | Loss: 0.00002309
Iteration 183/1000 | Loss: 0.00002308
Iteration 184/1000 | Loss: 0.00002306
Iteration 185/1000 | Loss: 0.00002302
Iteration 186/1000 | Loss: 0.00002302
Iteration 187/1000 | Loss: 0.00002300
Iteration 188/1000 | Loss: 0.00002299
Iteration 189/1000 | Loss: 0.00002299
Iteration 190/1000 | Loss: 0.00002299
Iteration 191/1000 | Loss: 0.00002298
Iteration 192/1000 | Loss: 0.00002297
Iteration 193/1000 | Loss: 0.00002294
Iteration 194/1000 | Loss: 0.00002291
Iteration 195/1000 | Loss: 0.00002290
Iteration 196/1000 | Loss: 0.00002289
Iteration 197/1000 | Loss: 0.00002289
Iteration 198/1000 | Loss: 0.00002288
Iteration 199/1000 | Loss: 0.00002283
Iteration 200/1000 | Loss: 0.00002283
Iteration 201/1000 | Loss: 0.00002282
Iteration 202/1000 | Loss: 0.00002281
Iteration 203/1000 | Loss: 0.00002281
Iteration 204/1000 | Loss: 0.00002281
Iteration 205/1000 | Loss: 0.00002280
Iteration 206/1000 | Loss: 0.00002280
Iteration 207/1000 | Loss: 0.00002280
Iteration 208/1000 | Loss: 0.00002280
Iteration 209/1000 | Loss: 0.00002279
Iteration 210/1000 | Loss: 0.00002279
Iteration 211/1000 | Loss: 0.00002279
Iteration 212/1000 | Loss: 0.00002279
Iteration 213/1000 | Loss: 0.00002278
Iteration 214/1000 | Loss: 0.00002278
Iteration 215/1000 | Loss: 0.00002277
Iteration 216/1000 | Loss: 0.00002277
Iteration 217/1000 | Loss: 0.00002277
Iteration 218/1000 | Loss: 0.00002277
Iteration 219/1000 | Loss: 0.00002276
Iteration 220/1000 | Loss: 0.00002276
Iteration 221/1000 | Loss: 0.00002276
Iteration 222/1000 | Loss: 0.00002276
Iteration 223/1000 | Loss: 0.00002276
Iteration 224/1000 | Loss: 0.00002276
Iteration 225/1000 | Loss: 0.00002276
Iteration 226/1000 | Loss: 0.00002276
Iteration 227/1000 | Loss: 0.00002275
Iteration 228/1000 | Loss: 0.00002275
Iteration 229/1000 | Loss: 0.00002275
Iteration 230/1000 | Loss: 0.00002275
Iteration 231/1000 | Loss: 0.00002275
Iteration 232/1000 | Loss: 0.00002275
Iteration 233/1000 | Loss: 0.00002274
Iteration 234/1000 | Loss: 0.00002274
Iteration 235/1000 | Loss: 0.00002274
Iteration 236/1000 | Loss: 0.00002274
Iteration 237/1000 | Loss: 0.00002274
Iteration 238/1000 | Loss: 0.00002273
Iteration 239/1000 | Loss: 0.00002273
Iteration 240/1000 | Loss: 0.00002273
Iteration 241/1000 | Loss: 0.00002273
Iteration 242/1000 | Loss: 0.00002272
Iteration 243/1000 | Loss: 0.00002272
Iteration 244/1000 | Loss: 0.00002272
Iteration 245/1000 | Loss: 0.00002272
Iteration 246/1000 | Loss: 0.00002272
Iteration 247/1000 | Loss: 0.00002272
Iteration 248/1000 | Loss: 0.00002272
Iteration 249/1000 | Loss: 0.00002272
Iteration 250/1000 | Loss: 0.00002272
Iteration 251/1000 | Loss: 0.00002272
Iteration 252/1000 | Loss: 0.00002272
Iteration 253/1000 | Loss: 0.00002272
Iteration 254/1000 | Loss: 0.00002271
Iteration 255/1000 | Loss: 0.00002271
Iteration 256/1000 | Loss: 0.00002271
Iteration 257/1000 | Loss: 0.00002271
Iteration 258/1000 | Loss: 0.00002271
Iteration 259/1000 | Loss: 0.00002271
Iteration 260/1000 | Loss: 0.00002271
Iteration 261/1000 | Loss: 0.00002271
Iteration 262/1000 | Loss: 0.00002271
Iteration 263/1000 | Loss: 0.00002271
Iteration 264/1000 | Loss: 0.00002271
Iteration 265/1000 | Loss: 0.00002271
Iteration 266/1000 | Loss: 0.00002271
Iteration 267/1000 | Loss: 0.00002271
Iteration 268/1000 | Loss: 0.00002271
Iteration 269/1000 | Loss: 0.00002271
Iteration 270/1000 | Loss: 0.00002271
Iteration 271/1000 | Loss: 0.00002271
Iteration 272/1000 | Loss: 0.00002271
Iteration 273/1000 | Loss: 0.00002271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 273. Stopping optimization.
Last 5 losses: [2.2713944417773746e-05, 2.2713944417773746e-05, 2.2713944417773746e-05, 2.2713944417773746e-05, 2.2713944417773746e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2713944417773746e-05

Optimization complete. Final v2v error: 3.565855026245117 mm

Highest mean error: 11.422795295715332 mm for frame 63

Lowest mean error: 2.751622438430786 mm for frame 200

Saving results

Total time: 278.22170186042786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853886
Iteration 2/25 | Loss: 0.00118781
Iteration 3/25 | Loss: 0.00109604
Iteration 4/25 | Loss: 0.00108566
Iteration 5/25 | Loss: 0.00108244
Iteration 6/25 | Loss: 0.00108167
Iteration 7/25 | Loss: 0.00108167
Iteration 8/25 | Loss: 0.00108167
Iteration 9/25 | Loss: 0.00108167
Iteration 10/25 | Loss: 0.00108167
Iteration 11/25 | Loss: 0.00108167
Iteration 12/25 | Loss: 0.00108167
Iteration 13/25 | Loss: 0.00108167
Iteration 14/25 | Loss: 0.00108167
Iteration 15/25 | Loss: 0.00108167
Iteration 16/25 | Loss: 0.00108167
Iteration 17/25 | Loss: 0.00108167
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001081671449355781, 0.001081671449355781, 0.001081671449355781, 0.001081671449355781, 0.001081671449355781]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001081671449355781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99861670
Iteration 2/25 | Loss: 0.00087979
Iteration 3/25 | Loss: 0.00087979
Iteration 4/25 | Loss: 0.00087979
Iteration 5/25 | Loss: 0.00087979
Iteration 6/25 | Loss: 0.00087979
Iteration 7/25 | Loss: 0.00087979
Iteration 8/25 | Loss: 0.00087979
Iteration 9/25 | Loss: 0.00087979
Iteration 10/25 | Loss: 0.00087979
Iteration 11/25 | Loss: 0.00087979
Iteration 12/25 | Loss: 0.00087978
Iteration 13/25 | Loss: 0.00087978
Iteration 14/25 | Loss: 0.00087978
Iteration 15/25 | Loss: 0.00087978
Iteration 16/25 | Loss: 0.00087978
Iteration 17/25 | Loss: 0.00087978
Iteration 18/25 | Loss: 0.00087978
Iteration 19/25 | Loss: 0.00087978
Iteration 20/25 | Loss: 0.00087978
Iteration 21/25 | Loss: 0.00087978
Iteration 22/25 | Loss: 0.00087978
Iteration 23/25 | Loss: 0.00087978
Iteration 24/25 | Loss: 0.00087978
Iteration 25/25 | Loss: 0.00087978

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087978
Iteration 2/1000 | Loss: 0.00002515
Iteration 3/1000 | Loss: 0.00001621
Iteration 4/1000 | Loss: 0.00001276
Iteration 5/1000 | Loss: 0.00001196
Iteration 6/1000 | Loss: 0.00001139
Iteration 7/1000 | Loss: 0.00001104
Iteration 8/1000 | Loss: 0.00001063
Iteration 9/1000 | Loss: 0.00001050
Iteration 10/1000 | Loss: 0.00001044
Iteration 11/1000 | Loss: 0.00001025
Iteration 12/1000 | Loss: 0.00001021
Iteration 13/1000 | Loss: 0.00001009
Iteration 14/1000 | Loss: 0.00001006
Iteration 15/1000 | Loss: 0.00001005
Iteration 16/1000 | Loss: 0.00001004
Iteration 17/1000 | Loss: 0.00001000
Iteration 18/1000 | Loss: 0.00000997
Iteration 19/1000 | Loss: 0.00000990
Iteration 20/1000 | Loss: 0.00000989
Iteration 21/1000 | Loss: 0.00000986
Iteration 22/1000 | Loss: 0.00000985
Iteration 23/1000 | Loss: 0.00000984
Iteration 24/1000 | Loss: 0.00000984
Iteration 25/1000 | Loss: 0.00000984
Iteration 26/1000 | Loss: 0.00000984
Iteration 27/1000 | Loss: 0.00000984
Iteration 28/1000 | Loss: 0.00000984
Iteration 29/1000 | Loss: 0.00000984
Iteration 30/1000 | Loss: 0.00000984
Iteration 31/1000 | Loss: 0.00000984
Iteration 32/1000 | Loss: 0.00000983
Iteration 33/1000 | Loss: 0.00000982
Iteration 34/1000 | Loss: 0.00000982
Iteration 35/1000 | Loss: 0.00000981
Iteration 36/1000 | Loss: 0.00000981
Iteration 37/1000 | Loss: 0.00000979
Iteration 38/1000 | Loss: 0.00000979
Iteration 39/1000 | Loss: 0.00000979
Iteration 40/1000 | Loss: 0.00000979
Iteration 41/1000 | Loss: 0.00000979
Iteration 42/1000 | Loss: 0.00000979
Iteration 43/1000 | Loss: 0.00000979
Iteration 44/1000 | Loss: 0.00000979
Iteration 45/1000 | Loss: 0.00000978
Iteration 46/1000 | Loss: 0.00000978
Iteration 47/1000 | Loss: 0.00000978
Iteration 48/1000 | Loss: 0.00000975
Iteration 49/1000 | Loss: 0.00000975
Iteration 50/1000 | Loss: 0.00000975
Iteration 51/1000 | Loss: 0.00000975
Iteration 52/1000 | Loss: 0.00000974
Iteration 53/1000 | Loss: 0.00000972
Iteration 54/1000 | Loss: 0.00000971
Iteration 55/1000 | Loss: 0.00000971
Iteration 56/1000 | Loss: 0.00000971
Iteration 57/1000 | Loss: 0.00000971
Iteration 58/1000 | Loss: 0.00000971
Iteration 59/1000 | Loss: 0.00000971
Iteration 60/1000 | Loss: 0.00000971
Iteration 61/1000 | Loss: 0.00000971
Iteration 62/1000 | Loss: 0.00000970
Iteration 63/1000 | Loss: 0.00000970
Iteration 64/1000 | Loss: 0.00000969
Iteration 65/1000 | Loss: 0.00000969
Iteration 66/1000 | Loss: 0.00000969
Iteration 67/1000 | Loss: 0.00000968
Iteration 68/1000 | Loss: 0.00000968
Iteration 69/1000 | Loss: 0.00000968
Iteration 70/1000 | Loss: 0.00000968
Iteration 71/1000 | Loss: 0.00000968
Iteration 72/1000 | Loss: 0.00000968
Iteration 73/1000 | Loss: 0.00000968
Iteration 74/1000 | Loss: 0.00000967
Iteration 75/1000 | Loss: 0.00000967
Iteration 76/1000 | Loss: 0.00000967
Iteration 77/1000 | Loss: 0.00000967
Iteration 78/1000 | Loss: 0.00000967
Iteration 79/1000 | Loss: 0.00000967
Iteration 80/1000 | Loss: 0.00000967
Iteration 81/1000 | Loss: 0.00000967
Iteration 82/1000 | Loss: 0.00000967
Iteration 83/1000 | Loss: 0.00000967
Iteration 84/1000 | Loss: 0.00000967
Iteration 85/1000 | Loss: 0.00000967
Iteration 86/1000 | Loss: 0.00000967
Iteration 87/1000 | Loss: 0.00000967
Iteration 88/1000 | Loss: 0.00000967
Iteration 89/1000 | Loss: 0.00000966
Iteration 90/1000 | Loss: 0.00000966
Iteration 91/1000 | Loss: 0.00000966
Iteration 92/1000 | Loss: 0.00000966
Iteration 93/1000 | Loss: 0.00000966
Iteration 94/1000 | Loss: 0.00000966
Iteration 95/1000 | Loss: 0.00000965
Iteration 96/1000 | Loss: 0.00000965
Iteration 97/1000 | Loss: 0.00000965
Iteration 98/1000 | Loss: 0.00000965
Iteration 99/1000 | Loss: 0.00000965
Iteration 100/1000 | Loss: 0.00000965
Iteration 101/1000 | Loss: 0.00000965
Iteration 102/1000 | Loss: 0.00000965
Iteration 103/1000 | Loss: 0.00000965
Iteration 104/1000 | Loss: 0.00000965
Iteration 105/1000 | Loss: 0.00000964
Iteration 106/1000 | Loss: 0.00000964
Iteration 107/1000 | Loss: 0.00000964
Iteration 108/1000 | Loss: 0.00000964
Iteration 109/1000 | Loss: 0.00000964
Iteration 110/1000 | Loss: 0.00000964
Iteration 111/1000 | Loss: 0.00000964
Iteration 112/1000 | Loss: 0.00000963
Iteration 113/1000 | Loss: 0.00000963
Iteration 114/1000 | Loss: 0.00000963
Iteration 115/1000 | Loss: 0.00000963
Iteration 116/1000 | Loss: 0.00000963
Iteration 117/1000 | Loss: 0.00000962
Iteration 118/1000 | Loss: 0.00000962
Iteration 119/1000 | Loss: 0.00000962
Iteration 120/1000 | Loss: 0.00000962
Iteration 121/1000 | Loss: 0.00000962
Iteration 122/1000 | Loss: 0.00000962
Iteration 123/1000 | Loss: 0.00000962
Iteration 124/1000 | Loss: 0.00000962
Iteration 125/1000 | Loss: 0.00000961
Iteration 126/1000 | Loss: 0.00000961
Iteration 127/1000 | Loss: 0.00000961
Iteration 128/1000 | Loss: 0.00000961
Iteration 129/1000 | Loss: 0.00000960
Iteration 130/1000 | Loss: 0.00000960
Iteration 131/1000 | Loss: 0.00000960
Iteration 132/1000 | Loss: 0.00000960
Iteration 133/1000 | Loss: 0.00000960
Iteration 134/1000 | Loss: 0.00000960
Iteration 135/1000 | Loss: 0.00000960
Iteration 136/1000 | Loss: 0.00000959
Iteration 137/1000 | Loss: 0.00000959
Iteration 138/1000 | Loss: 0.00000959
Iteration 139/1000 | Loss: 0.00000959
Iteration 140/1000 | Loss: 0.00000959
Iteration 141/1000 | Loss: 0.00000959
Iteration 142/1000 | Loss: 0.00000959
Iteration 143/1000 | Loss: 0.00000959
Iteration 144/1000 | Loss: 0.00000959
Iteration 145/1000 | Loss: 0.00000959
Iteration 146/1000 | Loss: 0.00000958
Iteration 147/1000 | Loss: 0.00000958
Iteration 148/1000 | Loss: 0.00000958
Iteration 149/1000 | Loss: 0.00000958
Iteration 150/1000 | Loss: 0.00000957
Iteration 151/1000 | Loss: 0.00000957
Iteration 152/1000 | Loss: 0.00000957
Iteration 153/1000 | Loss: 0.00000956
Iteration 154/1000 | Loss: 0.00000956
Iteration 155/1000 | Loss: 0.00000956
Iteration 156/1000 | Loss: 0.00000956
Iteration 157/1000 | Loss: 0.00000955
Iteration 158/1000 | Loss: 0.00000955
Iteration 159/1000 | Loss: 0.00000955
Iteration 160/1000 | Loss: 0.00000955
Iteration 161/1000 | Loss: 0.00000955
Iteration 162/1000 | Loss: 0.00000955
Iteration 163/1000 | Loss: 0.00000955
Iteration 164/1000 | Loss: 0.00000955
Iteration 165/1000 | Loss: 0.00000955
Iteration 166/1000 | Loss: 0.00000955
Iteration 167/1000 | Loss: 0.00000954
Iteration 168/1000 | Loss: 0.00000954
Iteration 169/1000 | Loss: 0.00000954
Iteration 170/1000 | Loss: 0.00000954
Iteration 171/1000 | Loss: 0.00000954
Iteration 172/1000 | Loss: 0.00000954
Iteration 173/1000 | Loss: 0.00000954
Iteration 174/1000 | Loss: 0.00000954
Iteration 175/1000 | Loss: 0.00000954
Iteration 176/1000 | Loss: 0.00000954
Iteration 177/1000 | Loss: 0.00000954
Iteration 178/1000 | Loss: 0.00000954
Iteration 179/1000 | Loss: 0.00000953
Iteration 180/1000 | Loss: 0.00000953
Iteration 181/1000 | Loss: 0.00000953
Iteration 182/1000 | Loss: 0.00000953
Iteration 183/1000 | Loss: 0.00000953
Iteration 184/1000 | Loss: 0.00000953
Iteration 185/1000 | Loss: 0.00000953
Iteration 186/1000 | Loss: 0.00000953
Iteration 187/1000 | Loss: 0.00000953
Iteration 188/1000 | Loss: 0.00000953
Iteration 189/1000 | Loss: 0.00000953
Iteration 190/1000 | Loss: 0.00000953
Iteration 191/1000 | Loss: 0.00000953
Iteration 192/1000 | Loss: 0.00000953
Iteration 193/1000 | Loss: 0.00000953
Iteration 194/1000 | Loss: 0.00000953
Iteration 195/1000 | Loss: 0.00000953
Iteration 196/1000 | Loss: 0.00000953
Iteration 197/1000 | Loss: 0.00000953
Iteration 198/1000 | Loss: 0.00000953
Iteration 199/1000 | Loss: 0.00000953
Iteration 200/1000 | Loss: 0.00000953
Iteration 201/1000 | Loss: 0.00000953
Iteration 202/1000 | Loss: 0.00000953
Iteration 203/1000 | Loss: 0.00000953
Iteration 204/1000 | Loss: 0.00000953
Iteration 205/1000 | Loss: 0.00000953
Iteration 206/1000 | Loss: 0.00000953
Iteration 207/1000 | Loss: 0.00000953
Iteration 208/1000 | Loss: 0.00000953
Iteration 209/1000 | Loss: 0.00000953
Iteration 210/1000 | Loss: 0.00000953
Iteration 211/1000 | Loss: 0.00000953
Iteration 212/1000 | Loss: 0.00000953
Iteration 213/1000 | Loss: 0.00000953
Iteration 214/1000 | Loss: 0.00000953
Iteration 215/1000 | Loss: 0.00000953
Iteration 216/1000 | Loss: 0.00000953
Iteration 217/1000 | Loss: 0.00000953
Iteration 218/1000 | Loss: 0.00000953
Iteration 219/1000 | Loss: 0.00000953
Iteration 220/1000 | Loss: 0.00000953
Iteration 221/1000 | Loss: 0.00000953
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [9.530714123684447e-06, 9.530714123684447e-06, 9.530714123684447e-06, 9.530714123684447e-06, 9.530714123684447e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.530714123684447e-06

Optimization complete. Final v2v error: 2.610234022140503 mm

Highest mean error: 3.289992094039917 mm for frame 52

Lowest mean error: 2.3720591068267822 mm for frame 101

Saving results

Total time: 40.45461416244507
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_010/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_010/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00621441
Iteration 2/25 | Loss: 0.00148432
Iteration 3/25 | Loss: 0.00123222
Iteration 4/25 | Loss: 0.00119034
Iteration 5/25 | Loss: 0.00116160
Iteration 6/25 | Loss: 0.00117526
Iteration 7/25 | Loss: 0.00117531
Iteration 8/25 | Loss: 0.00118527
Iteration 9/25 | Loss: 0.00115676
Iteration 10/25 | Loss: 0.00114524
Iteration 11/25 | Loss: 0.00114119
Iteration 12/25 | Loss: 0.00113037
Iteration 13/25 | Loss: 0.00113155
Iteration 14/25 | Loss: 0.00112591
Iteration 15/25 | Loss: 0.00113112
Iteration 16/25 | Loss: 0.00113514
Iteration 17/25 | Loss: 0.00112578
Iteration 18/25 | Loss: 0.00112168
Iteration 19/25 | Loss: 0.00112568
Iteration 20/25 | Loss: 0.00112170
Iteration 21/25 | Loss: 0.00112173
Iteration 22/25 | Loss: 0.00112004
Iteration 23/25 | Loss: 0.00112099
Iteration 24/25 | Loss: 0.00112122
Iteration 25/25 | Loss: 0.00112160

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71874750
Iteration 2/25 | Loss: 0.00111135
Iteration 3/25 | Loss: 0.00111133
Iteration 4/25 | Loss: 0.00111133
Iteration 5/25 | Loss: 0.00111133
Iteration 6/25 | Loss: 0.00111133
Iteration 7/25 | Loss: 0.00111133
Iteration 8/25 | Loss: 0.00111133
Iteration 9/25 | Loss: 0.00111132
Iteration 10/25 | Loss: 0.00111132
Iteration 11/25 | Loss: 0.00111132
Iteration 12/25 | Loss: 0.00111132
Iteration 13/25 | Loss: 0.00111132
Iteration 14/25 | Loss: 0.00111132
Iteration 15/25 | Loss: 0.00111132
Iteration 16/25 | Loss: 0.00111132
Iteration 17/25 | Loss: 0.00111132
Iteration 18/25 | Loss: 0.00111132
Iteration 19/25 | Loss: 0.00111132
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011113244108855724, 0.0011113244108855724, 0.0011113244108855724, 0.0011113244108855724, 0.0011113244108855724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011113244108855724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111132
Iteration 2/1000 | Loss: 0.00004816
Iteration 3/1000 | Loss: 0.00003125
Iteration 4/1000 | Loss: 0.00008774
Iteration 5/1000 | Loss: 0.00005781
Iteration 6/1000 | Loss: 0.00003186
Iteration 7/1000 | Loss: 0.00008301
Iteration 8/1000 | Loss: 0.00002799
Iteration 9/1000 | Loss: 0.00004010
Iteration 10/1000 | Loss: 0.00002078
Iteration 11/1000 | Loss: 0.00004930
Iteration 12/1000 | Loss: 0.00020197
Iteration 13/1000 | Loss: 0.00022401
Iteration 14/1000 | Loss: 0.00003288
Iteration 15/1000 | Loss: 0.00002645
Iteration 16/1000 | Loss: 0.00002967
Iteration 17/1000 | Loss: 0.00002428
Iteration 18/1000 | Loss: 0.00002895
Iteration 19/1000 | Loss: 0.00002897
Iteration 20/1000 | Loss: 0.00003739
Iteration 21/1000 | Loss: 0.00003088
Iteration 22/1000 | Loss: 0.00002656
Iteration 23/1000 | Loss: 0.00004135
Iteration 24/1000 | Loss: 0.00002707
Iteration 25/1000 | Loss: 0.00003413
Iteration 26/1000 | Loss: 0.00002832
Iteration 27/1000 | Loss: 0.00003493
Iteration 28/1000 | Loss: 0.00003018
Iteration 29/1000 | Loss: 0.00003084
Iteration 30/1000 | Loss: 0.00002702
Iteration 31/1000 | Loss: 0.00002746
Iteration 32/1000 | Loss: 0.00002616
Iteration 33/1000 | Loss: 0.00002689
Iteration 34/1000 | Loss: 0.00002598
Iteration 35/1000 | Loss: 0.00003314
Iteration 36/1000 | Loss: 0.00002776
Iteration 37/1000 | Loss: 0.00002773
Iteration 38/1000 | Loss: 0.00002928
Iteration 39/1000 | Loss: 0.00002027
Iteration 40/1000 | Loss: 0.00002545
Iteration 41/1000 | Loss: 0.00002242
Iteration 42/1000 | Loss: 0.00002203
Iteration 43/1000 | Loss: 0.00021535
Iteration 44/1000 | Loss: 0.00011261
Iteration 45/1000 | Loss: 0.00002497
Iteration 46/1000 | Loss: 0.00002255
Iteration 47/1000 | Loss: 0.00007757
Iteration 48/1000 | Loss: 0.00002119
Iteration 49/1000 | Loss: 0.00001945
Iteration 50/1000 | Loss: 0.00001911
Iteration 51/1000 | Loss: 0.00004828
Iteration 52/1000 | Loss: 0.00002402
Iteration 53/1000 | Loss: 0.00003018
Iteration 54/1000 | Loss: 0.00003407
Iteration 55/1000 | Loss: 0.00002330
Iteration 56/1000 | Loss: 0.00002023
Iteration 57/1000 | Loss: 0.00001992
Iteration 58/1000 | Loss: 0.00001960
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001889
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001814
Iteration 64/1000 | Loss: 0.00001808
Iteration 65/1000 | Loss: 0.00001801
Iteration 66/1000 | Loss: 0.00001795
Iteration 67/1000 | Loss: 0.00001778
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001776
Iteration 70/1000 | Loss: 0.00001775
Iteration 71/1000 | Loss: 0.00001775
Iteration 72/1000 | Loss: 0.00001775
Iteration 73/1000 | Loss: 0.00001774
Iteration 74/1000 | Loss: 0.00001774
Iteration 75/1000 | Loss: 0.00001774
Iteration 76/1000 | Loss: 0.00001774
Iteration 77/1000 | Loss: 0.00001774
Iteration 78/1000 | Loss: 0.00001774
Iteration 79/1000 | Loss: 0.00001773
Iteration 80/1000 | Loss: 0.00001773
Iteration 81/1000 | Loss: 0.00001773
Iteration 82/1000 | Loss: 0.00001773
Iteration 83/1000 | Loss: 0.00001773
Iteration 84/1000 | Loss: 0.00001773
Iteration 85/1000 | Loss: 0.00001773
Iteration 86/1000 | Loss: 0.00001773
Iteration 87/1000 | Loss: 0.00001773
Iteration 88/1000 | Loss: 0.00001773
Iteration 89/1000 | Loss: 0.00001773
Iteration 90/1000 | Loss: 0.00001772
Iteration 91/1000 | Loss: 0.00001772
Iteration 92/1000 | Loss: 0.00001772
Iteration 93/1000 | Loss: 0.00001770
Iteration 94/1000 | Loss: 0.00001769
Iteration 95/1000 | Loss: 0.00001769
Iteration 96/1000 | Loss: 0.00001769
Iteration 97/1000 | Loss: 0.00001767
Iteration 98/1000 | Loss: 0.00001766
Iteration 99/1000 | Loss: 0.00001766
Iteration 100/1000 | Loss: 0.00001766
Iteration 101/1000 | Loss: 0.00001766
Iteration 102/1000 | Loss: 0.00001765
Iteration 103/1000 | Loss: 0.00001765
Iteration 104/1000 | Loss: 0.00001765
Iteration 105/1000 | Loss: 0.00001765
Iteration 106/1000 | Loss: 0.00001764
Iteration 107/1000 | Loss: 0.00001764
Iteration 108/1000 | Loss: 0.00001764
Iteration 109/1000 | Loss: 0.00001764
Iteration 110/1000 | Loss: 0.00001764
Iteration 111/1000 | Loss: 0.00001764
Iteration 112/1000 | Loss: 0.00001763
Iteration 113/1000 | Loss: 0.00001763
Iteration 114/1000 | Loss: 0.00001762
Iteration 115/1000 | Loss: 0.00001762
Iteration 116/1000 | Loss: 0.00001762
Iteration 117/1000 | Loss: 0.00001762
Iteration 118/1000 | Loss: 0.00001761
Iteration 119/1000 | Loss: 0.00001761
Iteration 120/1000 | Loss: 0.00001760
Iteration 121/1000 | Loss: 0.00001759
Iteration 122/1000 | Loss: 0.00001759
Iteration 123/1000 | Loss: 0.00001759
Iteration 124/1000 | Loss: 0.00001759
Iteration 125/1000 | Loss: 0.00001759
Iteration 126/1000 | Loss: 0.00001758
Iteration 127/1000 | Loss: 0.00001758
Iteration 128/1000 | Loss: 0.00001758
Iteration 129/1000 | Loss: 0.00001758
Iteration 130/1000 | Loss: 0.00001758
Iteration 131/1000 | Loss: 0.00001758
Iteration 132/1000 | Loss: 0.00001758
Iteration 133/1000 | Loss: 0.00001758
Iteration 134/1000 | Loss: 0.00001758
Iteration 135/1000 | Loss: 0.00001757
Iteration 136/1000 | Loss: 0.00001756
Iteration 137/1000 | Loss: 0.00001756
Iteration 138/1000 | Loss: 0.00001756
Iteration 139/1000 | Loss: 0.00001756
Iteration 140/1000 | Loss: 0.00001756
Iteration 141/1000 | Loss: 0.00001756
Iteration 142/1000 | Loss: 0.00001756
Iteration 143/1000 | Loss: 0.00001755
Iteration 144/1000 | Loss: 0.00001755
Iteration 145/1000 | Loss: 0.00001755
Iteration 146/1000 | Loss: 0.00001755
Iteration 147/1000 | Loss: 0.00001753
Iteration 148/1000 | Loss: 0.00001753
Iteration 149/1000 | Loss: 0.00001752
Iteration 150/1000 | Loss: 0.00001751
Iteration 151/1000 | Loss: 0.00001751
Iteration 152/1000 | Loss: 0.00001750
Iteration 153/1000 | Loss: 0.00001749
Iteration 154/1000 | Loss: 0.00001749
Iteration 155/1000 | Loss: 0.00001748
Iteration 156/1000 | Loss: 0.00001748
Iteration 157/1000 | Loss: 0.00001747
Iteration 158/1000 | Loss: 0.00001747
Iteration 159/1000 | Loss: 0.00001747
Iteration 160/1000 | Loss: 0.00001746
Iteration 161/1000 | Loss: 0.00001746
Iteration 162/1000 | Loss: 0.00001745
Iteration 163/1000 | Loss: 0.00001744
Iteration 164/1000 | Loss: 0.00001744
Iteration 165/1000 | Loss: 0.00001743
Iteration 166/1000 | Loss: 0.00001743
Iteration 167/1000 | Loss: 0.00001743
Iteration 168/1000 | Loss: 0.00001743
Iteration 169/1000 | Loss: 0.00001741
Iteration 170/1000 | Loss: 0.00001740
Iteration 171/1000 | Loss: 0.00001739
Iteration 172/1000 | Loss: 0.00001739
Iteration 173/1000 | Loss: 0.00001738
Iteration 174/1000 | Loss: 0.00001738
Iteration 175/1000 | Loss: 0.00001738
Iteration 176/1000 | Loss: 0.00001737
Iteration 177/1000 | Loss: 0.00001737
Iteration 178/1000 | Loss: 0.00001736
Iteration 179/1000 | Loss: 0.00001736
Iteration 180/1000 | Loss: 0.00001736
Iteration 181/1000 | Loss: 0.00001735
Iteration 182/1000 | Loss: 0.00001735
Iteration 183/1000 | Loss: 0.00001734
Iteration 184/1000 | Loss: 0.00001734
Iteration 185/1000 | Loss: 0.00001734
Iteration 186/1000 | Loss: 0.00001733
Iteration 187/1000 | Loss: 0.00001733
Iteration 188/1000 | Loss: 0.00001732
Iteration 189/1000 | Loss: 0.00001732
Iteration 190/1000 | Loss: 0.00001731
Iteration 191/1000 | Loss: 0.00001731
Iteration 192/1000 | Loss: 0.00001731
Iteration 193/1000 | Loss: 0.00001730
Iteration 194/1000 | Loss: 0.00001730
Iteration 195/1000 | Loss: 0.00001730
Iteration 196/1000 | Loss: 0.00001730
Iteration 197/1000 | Loss: 0.00001729
Iteration 198/1000 | Loss: 0.00001729
Iteration 199/1000 | Loss: 0.00001729
Iteration 200/1000 | Loss: 0.00001729
Iteration 201/1000 | Loss: 0.00001728
Iteration 202/1000 | Loss: 0.00001728
Iteration 203/1000 | Loss: 0.00001728
Iteration 204/1000 | Loss: 0.00001728
Iteration 205/1000 | Loss: 0.00001727
Iteration 206/1000 | Loss: 0.00001727
Iteration 207/1000 | Loss: 0.00001727
Iteration 208/1000 | Loss: 0.00016934
Iteration 209/1000 | Loss: 0.00002206
Iteration 210/1000 | Loss: 0.00008391
Iteration 211/1000 | Loss: 0.00001919
Iteration 212/1000 | Loss: 0.00006803
Iteration 213/1000 | Loss: 0.00001851
Iteration 214/1000 | Loss: 0.00001802
Iteration 215/1000 | Loss: 0.00001774
Iteration 216/1000 | Loss: 0.00002357
Iteration 217/1000 | Loss: 0.00002200
Iteration 218/1000 | Loss: 0.00002349
Iteration 219/1000 | Loss: 0.00002215
Iteration 220/1000 | Loss: 0.00002406
Iteration 221/1000 | Loss: 0.00002406
Iteration 222/1000 | Loss: 0.00006638
Iteration 223/1000 | Loss: 0.00001856
Iteration 224/1000 | Loss: 0.00003799
Iteration 225/1000 | Loss: 0.00001782
Iteration 226/1000 | Loss: 0.00001739
Iteration 227/1000 | Loss: 0.00001713
Iteration 228/1000 | Loss: 0.00001709
Iteration 229/1000 | Loss: 0.00001697
Iteration 230/1000 | Loss: 0.00001693
Iteration 231/1000 | Loss: 0.00001692
Iteration 232/1000 | Loss: 0.00001691
Iteration 233/1000 | Loss: 0.00001691
Iteration 234/1000 | Loss: 0.00001691
Iteration 235/1000 | Loss: 0.00001690
Iteration 236/1000 | Loss: 0.00001690
Iteration 237/1000 | Loss: 0.00001689
Iteration 238/1000 | Loss: 0.00001689
Iteration 239/1000 | Loss: 0.00001689
Iteration 240/1000 | Loss: 0.00001687
Iteration 241/1000 | Loss: 0.00001685
Iteration 242/1000 | Loss: 0.00001684
Iteration 243/1000 | Loss: 0.00001684
Iteration 244/1000 | Loss: 0.00001683
Iteration 245/1000 | Loss: 0.00001675
Iteration 246/1000 | Loss: 0.00001672
Iteration 247/1000 | Loss: 0.00001672
Iteration 248/1000 | Loss: 0.00001667
Iteration 249/1000 | Loss: 0.00001664
Iteration 250/1000 | Loss: 0.00001664
Iteration 251/1000 | Loss: 0.00001663
Iteration 252/1000 | Loss: 0.00001661
Iteration 253/1000 | Loss: 0.00001661
Iteration 254/1000 | Loss: 0.00001661
Iteration 255/1000 | Loss: 0.00001660
Iteration 256/1000 | Loss: 0.00001660
Iteration 257/1000 | Loss: 0.00001660
Iteration 258/1000 | Loss: 0.00001659
Iteration 259/1000 | Loss: 0.00001659
Iteration 260/1000 | Loss: 0.00001658
Iteration 261/1000 | Loss: 0.00001658
Iteration 262/1000 | Loss: 0.00001657
Iteration 263/1000 | Loss: 0.00001657
Iteration 264/1000 | Loss: 0.00001656
Iteration 265/1000 | Loss: 0.00001656
Iteration 266/1000 | Loss: 0.00001655
Iteration 267/1000 | Loss: 0.00001655
Iteration 268/1000 | Loss: 0.00001655
Iteration 269/1000 | Loss: 0.00001654
Iteration 270/1000 | Loss: 0.00001654
Iteration 271/1000 | Loss: 0.00001654
Iteration 272/1000 | Loss: 0.00001653
Iteration 273/1000 | Loss: 0.00001653
Iteration 274/1000 | Loss: 0.00001653
Iteration 275/1000 | Loss: 0.00001653
Iteration 276/1000 | Loss: 0.00001653
Iteration 277/1000 | Loss: 0.00001653
Iteration 278/1000 | Loss: 0.00001653
Iteration 279/1000 | Loss: 0.00001653
Iteration 280/1000 | Loss: 0.00001653
Iteration 281/1000 | Loss: 0.00001652
Iteration 282/1000 | Loss: 0.00001652
Iteration 283/1000 | Loss: 0.00001652
Iteration 284/1000 | Loss: 0.00001652
Iteration 285/1000 | Loss: 0.00001651
Iteration 286/1000 | Loss: 0.00001650
Iteration 287/1000 | Loss: 0.00001650
Iteration 288/1000 | Loss: 0.00001650
Iteration 289/1000 | Loss: 0.00001650
Iteration 290/1000 | Loss: 0.00001649
Iteration 291/1000 | Loss: 0.00001649
Iteration 292/1000 | Loss: 0.00001649
Iteration 293/1000 | Loss: 0.00001648
Iteration 294/1000 | Loss: 0.00001648
Iteration 295/1000 | Loss: 0.00001648
Iteration 296/1000 | Loss: 0.00001648
Iteration 297/1000 | Loss: 0.00001648
Iteration 298/1000 | Loss: 0.00001647
Iteration 299/1000 | Loss: 0.00001647
Iteration 300/1000 | Loss: 0.00001647
Iteration 301/1000 | Loss: 0.00001647
Iteration 302/1000 | Loss: 0.00001647
Iteration 303/1000 | Loss: 0.00001647
Iteration 304/1000 | Loss: 0.00001646
Iteration 305/1000 | Loss: 0.00001646
Iteration 306/1000 | Loss: 0.00001646
Iteration 307/1000 | Loss: 0.00001646
Iteration 308/1000 | Loss: 0.00001646
Iteration 309/1000 | Loss: 0.00001646
Iteration 310/1000 | Loss: 0.00001646
Iteration 311/1000 | Loss: 0.00001646
Iteration 312/1000 | Loss: 0.00001646
Iteration 313/1000 | Loss: 0.00001646
Iteration 314/1000 | Loss: 0.00001646
Iteration 315/1000 | Loss: 0.00001646
Iteration 316/1000 | Loss: 0.00001645
Iteration 317/1000 | Loss: 0.00001645
Iteration 318/1000 | Loss: 0.00001645
Iteration 319/1000 | Loss: 0.00001645
Iteration 320/1000 | Loss: 0.00001645
Iteration 321/1000 | Loss: 0.00001645
Iteration 322/1000 | Loss: 0.00001645
Iteration 323/1000 | Loss: 0.00001645
Iteration 324/1000 | Loss: 0.00001645
Iteration 325/1000 | Loss: 0.00001645
Iteration 326/1000 | Loss: 0.00001645
Iteration 327/1000 | Loss: 0.00001645
Iteration 328/1000 | Loss: 0.00001645
Iteration 329/1000 | Loss: 0.00001645
Iteration 330/1000 | Loss: 0.00001645
Iteration 331/1000 | Loss: 0.00001645
Iteration 332/1000 | Loss: 0.00001644
Iteration 333/1000 | Loss: 0.00001644
Iteration 334/1000 | Loss: 0.00001644
Iteration 335/1000 | Loss: 0.00001644
Iteration 336/1000 | Loss: 0.00001644
Iteration 337/1000 | Loss: 0.00001644
Iteration 338/1000 | Loss: 0.00001644
Iteration 339/1000 | Loss: 0.00001644
Iteration 340/1000 | Loss: 0.00001644
Iteration 341/1000 | Loss: 0.00001644
Iteration 342/1000 | Loss: 0.00001644
Iteration 343/1000 | Loss: 0.00001644
Iteration 344/1000 | Loss: 0.00001643
Iteration 345/1000 | Loss: 0.00001643
Iteration 346/1000 | Loss: 0.00001643
Iteration 347/1000 | Loss: 0.00001643
Iteration 348/1000 | Loss: 0.00001643
Iteration 349/1000 | Loss: 0.00001643
Iteration 350/1000 | Loss: 0.00001643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 350. Stopping optimization.
Last 5 losses: [1.6434672943432815e-05, 1.6434672943432815e-05, 1.6434672943432815e-05, 1.6434672943432815e-05, 1.6434672943432815e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6434672943432815e-05

Optimization complete. Final v2v error: 3.3739891052246094 mm

Highest mean error: 4.599680423736572 mm for frame 64

Lowest mean error: 2.5168492794036865 mm for frame 181

Saving results

Total time: 216.51369214057922
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00928358
Iteration 2/25 | Loss: 0.00159902
Iteration 3/25 | Loss: 0.00130872
Iteration 4/25 | Loss: 0.00126439
Iteration 5/25 | Loss: 0.00125537
Iteration 6/25 | Loss: 0.00126904
Iteration 7/25 | Loss: 0.00120837
Iteration 8/25 | Loss: 0.00118250
Iteration 9/25 | Loss: 0.00117950
Iteration 10/25 | Loss: 0.00117886
Iteration 11/25 | Loss: 0.00117869
Iteration 12/25 | Loss: 0.00117866
Iteration 13/25 | Loss: 0.00117865
Iteration 14/25 | Loss: 0.00117865
Iteration 15/25 | Loss: 0.00117865
Iteration 16/25 | Loss: 0.00117865
Iteration 17/25 | Loss: 0.00117865
Iteration 18/25 | Loss: 0.00117865
Iteration 19/25 | Loss: 0.00117865
Iteration 20/25 | Loss: 0.00117865
Iteration 21/25 | Loss: 0.00117865
Iteration 22/25 | Loss: 0.00117865
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001178649952635169, 0.001178649952635169, 0.001178649952635169, 0.001178649952635169, 0.001178649952635169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001178649952635169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.03432167
Iteration 2/25 | Loss: 0.00089146
Iteration 3/25 | Loss: 0.00089145
Iteration 4/25 | Loss: 0.00089145
Iteration 5/25 | Loss: 0.00089145
Iteration 6/25 | Loss: 0.00089145
Iteration 7/25 | Loss: 0.00089145
Iteration 8/25 | Loss: 0.00089145
Iteration 9/25 | Loss: 0.00089145
Iteration 10/25 | Loss: 0.00089145
Iteration 11/25 | Loss: 0.00089145
Iteration 12/25 | Loss: 0.00089145
Iteration 13/25 | Loss: 0.00089145
Iteration 14/25 | Loss: 0.00089145
Iteration 15/25 | Loss: 0.00089145
Iteration 16/25 | Loss: 0.00089145
Iteration 17/25 | Loss: 0.00089145
Iteration 18/25 | Loss: 0.00089145
Iteration 19/25 | Loss: 0.00089145
Iteration 20/25 | Loss: 0.00089145
Iteration 21/25 | Loss: 0.00089145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008914475329220295, 0.0008914475329220295, 0.0008914475329220295, 0.0008914475329220295, 0.0008914475329220295]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008914475329220295

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089145
Iteration 2/1000 | Loss: 0.00004350
Iteration 3/1000 | Loss: 0.00003631
Iteration 4/1000 | Loss: 0.00003256
Iteration 5/1000 | Loss: 0.00003049
Iteration 6/1000 | Loss: 0.00002948
Iteration 7/1000 | Loss: 0.00002882
Iteration 8/1000 | Loss: 0.00002836
Iteration 9/1000 | Loss: 0.00002812
Iteration 10/1000 | Loss: 0.00002807
Iteration 11/1000 | Loss: 0.00002786
Iteration 12/1000 | Loss: 0.00002778
Iteration 13/1000 | Loss: 0.00002775
Iteration 14/1000 | Loss: 0.00002775
Iteration 15/1000 | Loss: 0.00002775
Iteration 16/1000 | Loss: 0.00002775
Iteration 17/1000 | Loss: 0.00002775
Iteration 18/1000 | Loss: 0.00002775
Iteration 19/1000 | Loss: 0.00002774
Iteration 20/1000 | Loss: 0.00002774
Iteration 21/1000 | Loss: 0.00002774
Iteration 22/1000 | Loss: 0.00002774
Iteration 23/1000 | Loss: 0.00002774
Iteration 24/1000 | Loss: 0.00002774
Iteration 25/1000 | Loss: 0.00002774
Iteration 26/1000 | Loss: 0.00002774
Iteration 27/1000 | Loss: 0.00002773
Iteration 28/1000 | Loss: 0.00002773
Iteration 29/1000 | Loss: 0.00002773
Iteration 30/1000 | Loss: 0.00002773
Iteration 31/1000 | Loss: 0.00002773
Iteration 32/1000 | Loss: 0.00002773
Iteration 33/1000 | Loss: 0.00002773
Iteration 34/1000 | Loss: 0.00002773
Iteration 35/1000 | Loss: 0.00002772
Iteration 36/1000 | Loss: 0.00002772
Iteration 37/1000 | Loss: 0.00002772
Iteration 38/1000 | Loss: 0.00002772
Iteration 39/1000 | Loss: 0.00002772
Iteration 40/1000 | Loss: 0.00002772
Iteration 41/1000 | Loss: 0.00002772
Iteration 42/1000 | Loss: 0.00002771
Iteration 43/1000 | Loss: 0.00002771
Iteration 44/1000 | Loss: 0.00002771
Iteration 45/1000 | Loss: 0.00002771
Iteration 46/1000 | Loss: 0.00002771
Iteration 47/1000 | Loss: 0.00002771
Iteration 48/1000 | Loss: 0.00002771
Iteration 49/1000 | Loss: 0.00002771
Iteration 50/1000 | Loss: 0.00002771
Iteration 51/1000 | Loss: 0.00002771
Iteration 52/1000 | Loss: 0.00002771
Iteration 53/1000 | Loss: 0.00002770
Iteration 54/1000 | Loss: 0.00002770
Iteration 55/1000 | Loss: 0.00002770
Iteration 56/1000 | Loss: 0.00002770
Iteration 57/1000 | Loss: 0.00002770
Iteration 58/1000 | Loss: 0.00002770
Iteration 59/1000 | Loss: 0.00002770
Iteration 60/1000 | Loss: 0.00002770
Iteration 61/1000 | Loss: 0.00002769
Iteration 62/1000 | Loss: 0.00002769
Iteration 63/1000 | Loss: 0.00002769
Iteration 64/1000 | Loss: 0.00002769
Iteration 65/1000 | Loss: 0.00002769
Iteration 66/1000 | Loss: 0.00002769
Iteration 67/1000 | Loss: 0.00002769
Iteration 68/1000 | Loss: 0.00002769
Iteration 69/1000 | Loss: 0.00002769
Iteration 70/1000 | Loss: 0.00002769
Iteration 71/1000 | Loss: 0.00002769
Iteration 72/1000 | Loss: 0.00002769
Iteration 73/1000 | Loss: 0.00002769
Iteration 74/1000 | Loss: 0.00002768
Iteration 75/1000 | Loss: 0.00002768
Iteration 76/1000 | Loss: 0.00002768
Iteration 77/1000 | Loss: 0.00002768
Iteration 78/1000 | Loss: 0.00002768
Iteration 79/1000 | Loss: 0.00002768
Iteration 80/1000 | Loss: 0.00002768
Iteration 81/1000 | Loss: 0.00002768
Iteration 82/1000 | Loss: 0.00002768
Iteration 83/1000 | Loss: 0.00002768
Iteration 84/1000 | Loss: 0.00002767
Iteration 85/1000 | Loss: 0.00002767
Iteration 86/1000 | Loss: 0.00002767
Iteration 87/1000 | Loss: 0.00002767
Iteration 88/1000 | Loss: 0.00002767
Iteration 89/1000 | Loss: 0.00002767
Iteration 90/1000 | Loss: 0.00002766
Iteration 91/1000 | Loss: 0.00002766
Iteration 92/1000 | Loss: 0.00002766
Iteration 93/1000 | Loss: 0.00002766
Iteration 94/1000 | Loss: 0.00002766
Iteration 95/1000 | Loss: 0.00002766
Iteration 96/1000 | Loss: 0.00002766
Iteration 97/1000 | Loss: 0.00002766
Iteration 98/1000 | Loss: 0.00002766
Iteration 99/1000 | Loss: 0.00002766
Iteration 100/1000 | Loss: 0.00002766
Iteration 101/1000 | Loss: 0.00002766
Iteration 102/1000 | Loss: 0.00002766
Iteration 103/1000 | Loss: 0.00002766
Iteration 104/1000 | Loss: 0.00002766
Iteration 105/1000 | Loss: 0.00002766
Iteration 106/1000 | Loss: 0.00002766
Iteration 107/1000 | Loss: 0.00002766
Iteration 108/1000 | Loss: 0.00002766
Iteration 109/1000 | Loss: 0.00002765
Iteration 110/1000 | Loss: 0.00002765
Iteration 111/1000 | Loss: 0.00002765
Iteration 112/1000 | Loss: 0.00002765
Iteration 113/1000 | Loss: 0.00002765
Iteration 114/1000 | Loss: 0.00002765
Iteration 115/1000 | Loss: 0.00002765
Iteration 116/1000 | Loss: 0.00002765
Iteration 117/1000 | Loss: 0.00002765
Iteration 118/1000 | Loss: 0.00002765
Iteration 119/1000 | Loss: 0.00002765
Iteration 120/1000 | Loss: 0.00002765
Iteration 121/1000 | Loss: 0.00002765
Iteration 122/1000 | Loss: 0.00002765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.765194403764326e-05, 2.765194403764326e-05, 2.765194403764326e-05, 2.765194403764326e-05, 2.765194403764326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.765194403764326e-05

Optimization complete. Final v2v error: 4.462774276733398 mm

Highest mean error: 4.753870010375977 mm for frame 80

Lowest mean error: 4.146038055419922 mm for frame 32

Saving results

Total time: 41.73993420600891
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425309
Iteration 2/25 | Loss: 0.00111682
Iteration 3/25 | Loss: 0.00098760
Iteration 4/25 | Loss: 0.00096798
Iteration 5/25 | Loss: 0.00096321
Iteration 6/25 | Loss: 0.00096195
Iteration 7/25 | Loss: 0.00096157
Iteration 8/25 | Loss: 0.00096148
Iteration 9/25 | Loss: 0.00096148
Iteration 10/25 | Loss: 0.00096148
Iteration 11/25 | Loss: 0.00096148
Iteration 12/25 | Loss: 0.00096148
Iteration 13/25 | Loss: 0.00096148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009614778682589531, 0.0009614778682589531, 0.0009614778682589531, 0.0009614778682589531, 0.0009614778682589531]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009614778682589531

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54139566
Iteration 2/25 | Loss: 0.00096020
Iteration 3/25 | Loss: 0.00096020
Iteration 4/25 | Loss: 0.00096020
Iteration 5/25 | Loss: 0.00096020
Iteration 6/25 | Loss: 0.00096019
Iteration 7/25 | Loss: 0.00096019
Iteration 8/25 | Loss: 0.00096019
Iteration 9/25 | Loss: 0.00096019
Iteration 10/25 | Loss: 0.00096019
Iteration 11/25 | Loss: 0.00096019
Iteration 12/25 | Loss: 0.00096019
Iteration 13/25 | Loss: 0.00096019
Iteration 14/25 | Loss: 0.00096019
Iteration 15/25 | Loss: 0.00096019
Iteration 16/25 | Loss: 0.00096019
Iteration 17/25 | Loss: 0.00096019
Iteration 18/25 | Loss: 0.00096019
Iteration 19/25 | Loss: 0.00096019
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009601935744285583, 0.0009601935744285583, 0.0009601935744285583, 0.0009601935744285583, 0.0009601935744285583]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009601935744285583

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096019
Iteration 2/1000 | Loss: 0.00003387
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00001730
Iteration 5/1000 | Loss: 0.00001518
Iteration 6/1000 | Loss: 0.00001440
Iteration 7/1000 | Loss: 0.00001384
Iteration 8/1000 | Loss: 0.00001350
Iteration 9/1000 | Loss: 0.00001326
Iteration 10/1000 | Loss: 0.00001326
Iteration 11/1000 | Loss: 0.00001324
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001313
Iteration 14/1000 | Loss: 0.00001307
Iteration 15/1000 | Loss: 0.00001306
Iteration 16/1000 | Loss: 0.00001305
Iteration 17/1000 | Loss: 0.00001304
Iteration 18/1000 | Loss: 0.00001302
Iteration 19/1000 | Loss: 0.00001300
Iteration 20/1000 | Loss: 0.00001299
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001294
Iteration 24/1000 | Loss: 0.00001290
Iteration 25/1000 | Loss: 0.00001290
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001288
Iteration 28/1000 | Loss: 0.00001287
Iteration 29/1000 | Loss: 0.00001287
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001281
Iteration 32/1000 | Loss: 0.00001281
Iteration 33/1000 | Loss: 0.00001280
Iteration 34/1000 | Loss: 0.00001277
Iteration 35/1000 | Loss: 0.00001277
Iteration 36/1000 | Loss: 0.00001277
Iteration 37/1000 | Loss: 0.00001277
Iteration 38/1000 | Loss: 0.00001276
Iteration 39/1000 | Loss: 0.00001276
Iteration 40/1000 | Loss: 0.00001275
Iteration 41/1000 | Loss: 0.00001275
Iteration 42/1000 | Loss: 0.00001275
Iteration 43/1000 | Loss: 0.00001275
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001274
Iteration 46/1000 | Loss: 0.00001274
Iteration 47/1000 | Loss: 0.00001274
Iteration 48/1000 | Loss: 0.00001274
Iteration 49/1000 | Loss: 0.00001274
Iteration 50/1000 | Loss: 0.00001274
Iteration 51/1000 | Loss: 0.00001273
Iteration 52/1000 | Loss: 0.00001273
Iteration 53/1000 | Loss: 0.00001273
Iteration 54/1000 | Loss: 0.00001273
Iteration 55/1000 | Loss: 0.00001272
Iteration 56/1000 | Loss: 0.00001272
Iteration 57/1000 | Loss: 0.00001272
Iteration 58/1000 | Loss: 0.00001272
Iteration 59/1000 | Loss: 0.00001272
Iteration 60/1000 | Loss: 0.00001272
Iteration 61/1000 | Loss: 0.00001272
Iteration 62/1000 | Loss: 0.00001272
Iteration 63/1000 | Loss: 0.00001272
Iteration 64/1000 | Loss: 0.00001272
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001272
Iteration 67/1000 | Loss: 0.00001272
Iteration 68/1000 | Loss: 0.00001271
Iteration 69/1000 | Loss: 0.00001271
Iteration 70/1000 | Loss: 0.00001271
Iteration 71/1000 | Loss: 0.00001271
Iteration 72/1000 | Loss: 0.00001271
Iteration 73/1000 | Loss: 0.00001271
Iteration 74/1000 | Loss: 0.00001271
Iteration 75/1000 | Loss: 0.00001271
Iteration 76/1000 | Loss: 0.00001271
Iteration 77/1000 | Loss: 0.00001271
Iteration 78/1000 | Loss: 0.00001271
Iteration 79/1000 | Loss: 0.00001271
Iteration 80/1000 | Loss: 0.00001270
Iteration 81/1000 | Loss: 0.00001270
Iteration 82/1000 | Loss: 0.00001270
Iteration 83/1000 | Loss: 0.00001270
Iteration 84/1000 | Loss: 0.00001270
Iteration 85/1000 | Loss: 0.00001270
Iteration 86/1000 | Loss: 0.00001269
Iteration 87/1000 | Loss: 0.00001269
Iteration 88/1000 | Loss: 0.00001269
Iteration 89/1000 | Loss: 0.00001269
Iteration 90/1000 | Loss: 0.00001269
Iteration 91/1000 | Loss: 0.00001268
Iteration 92/1000 | Loss: 0.00001268
Iteration 93/1000 | Loss: 0.00001268
Iteration 94/1000 | Loss: 0.00001268
Iteration 95/1000 | Loss: 0.00001268
Iteration 96/1000 | Loss: 0.00001268
Iteration 97/1000 | Loss: 0.00001268
Iteration 98/1000 | Loss: 0.00001268
Iteration 99/1000 | Loss: 0.00001268
Iteration 100/1000 | Loss: 0.00001267
Iteration 101/1000 | Loss: 0.00001267
Iteration 102/1000 | Loss: 0.00001267
Iteration 103/1000 | Loss: 0.00001267
Iteration 104/1000 | Loss: 0.00001267
Iteration 105/1000 | Loss: 0.00001266
Iteration 106/1000 | Loss: 0.00001266
Iteration 107/1000 | Loss: 0.00001266
Iteration 108/1000 | Loss: 0.00001266
Iteration 109/1000 | Loss: 0.00001266
Iteration 110/1000 | Loss: 0.00001266
Iteration 111/1000 | Loss: 0.00001266
Iteration 112/1000 | Loss: 0.00001266
Iteration 113/1000 | Loss: 0.00001266
Iteration 114/1000 | Loss: 0.00001266
Iteration 115/1000 | Loss: 0.00001266
Iteration 116/1000 | Loss: 0.00001265
Iteration 117/1000 | Loss: 0.00001265
Iteration 118/1000 | Loss: 0.00001265
Iteration 119/1000 | Loss: 0.00001265
Iteration 120/1000 | Loss: 0.00001265
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001265
Iteration 123/1000 | Loss: 0.00001264
Iteration 124/1000 | Loss: 0.00001264
Iteration 125/1000 | Loss: 0.00001264
Iteration 126/1000 | Loss: 0.00001264
Iteration 127/1000 | Loss: 0.00001264
Iteration 128/1000 | Loss: 0.00001264
Iteration 129/1000 | Loss: 0.00001264
Iteration 130/1000 | Loss: 0.00001264
Iteration 131/1000 | Loss: 0.00001264
Iteration 132/1000 | Loss: 0.00001264
Iteration 133/1000 | Loss: 0.00001264
Iteration 134/1000 | Loss: 0.00001263
Iteration 135/1000 | Loss: 0.00001263
Iteration 136/1000 | Loss: 0.00001263
Iteration 137/1000 | Loss: 0.00001263
Iteration 138/1000 | Loss: 0.00001263
Iteration 139/1000 | Loss: 0.00001263
Iteration 140/1000 | Loss: 0.00001263
Iteration 141/1000 | Loss: 0.00001263
Iteration 142/1000 | Loss: 0.00001263
Iteration 143/1000 | Loss: 0.00001263
Iteration 144/1000 | Loss: 0.00001263
Iteration 145/1000 | Loss: 0.00001263
Iteration 146/1000 | Loss: 0.00001263
Iteration 147/1000 | Loss: 0.00001263
Iteration 148/1000 | Loss: 0.00001263
Iteration 149/1000 | Loss: 0.00001263
Iteration 150/1000 | Loss: 0.00001263
Iteration 151/1000 | Loss: 0.00001263
Iteration 152/1000 | Loss: 0.00001263
Iteration 153/1000 | Loss: 0.00001263
Iteration 154/1000 | Loss: 0.00001263
Iteration 155/1000 | Loss: 0.00001262
Iteration 156/1000 | Loss: 0.00001262
Iteration 157/1000 | Loss: 0.00001262
Iteration 158/1000 | Loss: 0.00001262
Iteration 159/1000 | Loss: 0.00001262
Iteration 160/1000 | Loss: 0.00001262
Iteration 161/1000 | Loss: 0.00001262
Iteration 162/1000 | Loss: 0.00001262
Iteration 163/1000 | Loss: 0.00001262
Iteration 164/1000 | Loss: 0.00001262
Iteration 165/1000 | Loss: 0.00001262
Iteration 166/1000 | Loss: 0.00001262
Iteration 167/1000 | Loss: 0.00001262
Iteration 168/1000 | Loss: 0.00001261
Iteration 169/1000 | Loss: 0.00001261
Iteration 170/1000 | Loss: 0.00001261
Iteration 171/1000 | Loss: 0.00001261
Iteration 172/1000 | Loss: 0.00001261
Iteration 173/1000 | Loss: 0.00001261
Iteration 174/1000 | Loss: 0.00001261
Iteration 175/1000 | Loss: 0.00001261
Iteration 176/1000 | Loss: 0.00001261
Iteration 177/1000 | Loss: 0.00001261
Iteration 178/1000 | Loss: 0.00001261
Iteration 179/1000 | Loss: 0.00001261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.2613199942279607e-05, 1.2613199942279607e-05, 1.2613199942279607e-05, 1.2613199942279607e-05, 1.2613199942279607e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2613199942279607e-05

Optimization complete. Final v2v error: 3.05700421333313 mm

Highest mean error: 4.021683692932129 mm for frame 55

Lowest mean error: 2.7454047203063965 mm for frame 32

Saving results

Total time: 39.47334027290344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00874307
Iteration 2/25 | Loss: 0.00122621
Iteration 3/25 | Loss: 0.00099776
Iteration 4/25 | Loss: 0.00097404
Iteration 5/25 | Loss: 0.00096764
Iteration 6/25 | Loss: 0.00096566
Iteration 7/25 | Loss: 0.00096533
Iteration 8/25 | Loss: 0.00096533
Iteration 9/25 | Loss: 0.00096533
Iteration 10/25 | Loss: 0.00096533
Iteration 11/25 | Loss: 0.00096533
Iteration 12/25 | Loss: 0.00096533
Iteration 13/25 | Loss: 0.00096533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009653278393670917, 0.0009653278393670917, 0.0009653278393670917, 0.0009653278393670917, 0.0009653278393670917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009653278393670917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52199888
Iteration 2/25 | Loss: 0.00090908
Iteration 3/25 | Loss: 0.00090908
Iteration 4/25 | Loss: 0.00090908
Iteration 5/25 | Loss: 0.00090908
Iteration 6/25 | Loss: 0.00090908
Iteration 7/25 | Loss: 0.00090908
Iteration 8/25 | Loss: 0.00090908
Iteration 9/25 | Loss: 0.00090908
Iteration 10/25 | Loss: 0.00090908
Iteration 11/25 | Loss: 0.00090907
Iteration 12/25 | Loss: 0.00090907
Iteration 13/25 | Loss: 0.00090907
Iteration 14/25 | Loss: 0.00090907
Iteration 15/25 | Loss: 0.00090907
Iteration 16/25 | Loss: 0.00090907
Iteration 17/25 | Loss: 0.00090907
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009090748499147594, 0.0009090748499147594, 0.0009090748499147594, 0.0009090748499147594, 0.0009090748499147594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009090748499147594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090907
Iteration 2/1000 | Loss: 0.00002602
Iteration 3/1000 | Loss: 0.00002055
Iteration 4/1000 | Loss: 0.00001690
Iteration 5/1000 | Loss: 0.00001606
Iteration 6/1000 | Loss: 0.00001562
Iteration 7/1000 | Loss: 0.00001527
Iteration 8/1000 | Loss: 0.00001514
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001511
Iteration 11/1000 | Loss: 0.00001510
Iteration 12/1000 | Loss: 0.00001508
Iteration 13/1000 | Loss: 0.00001505
Iteration 14/1000 | Loss: 0.00001499
Iteration 15/1000 | Loss: 0.00001497
Iteration 16/1000 | Loss: 0.00001496
Iteration 17/1000 | Loss: 0.00001496
Iteration 18/1000 | Loss: 0.00001496
Iteration 19/1000 | Loss: 0.00001495
Iteration 20/1000 | Loss: 0.00001493
Iteration 21/1000 | Loss: 0.00001492
Iteration 22/1000 | Loss: 0.00001489
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001485
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001484
Iteration 27/1000 | Loss: 0.00001481
Iteration 28/1000 | Loss: 0.00001480
Iteration 29/1000 | Loss: 0.00001480
Iteration 30/1000 | Loss: 0.00001480
Iteration 31/1000 | Loss: 0.00001479
Iteration 32/1000 | Loss: 0.00001479
Iteration 33/1000 | Loss: 0.00001478
Iteration 34/1000 | Loss: 0.00001478
Iteration 35/1000 | Loss: 0.00001477
Iteration 36/1000 | Loss: 0.00001477
Iteration 37/1000 | Loss: 0.00001476
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001475
Iteration 40/1000 | Loss: 0.00001475
Iteration 41/1000 | Loss: 0.00001474
Iteration 42/1000 | Loss: 0.00001474
Iteration 43/1000 | Loss: 0.00001474
Iteration 44/1000 | Loss: 0.00001473
Iteration 45/1000 | Loss: 0.00001472
Iteration 46/1000 | Loss: 0.00001472
Iteration 47/1000 | Loss: 0.00001472
Iteration 48/1000 | Loss: 0.00001471
Iteration 49/1000 | Loss: 0.00001471
Iteration 50/1000 | Loss: 0.00001471
Iteration 51/1000 | Loss: 0.00001471
Iteration 52/1000 | Loss: 0.00001471
Iteration 53/1000 | Loss: 0.00001471
Iteration 54/1000 | Loss: 0.00001470
Iteration 55/1000 | Loss: 0.00001470
Iteration 56/1000 | Loss: 0.00001470
Iteration 57/1000 | Loss: 0.00001469
Iteration 58/1000 | Loss: 0.00001467
Iteration 59/1000 | Loss: 0.00001467
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001465
Iteration 63/1000 | Loss: 0.00001465
Iteration 64/1000 | Loss: 0.00001465
Iteration 65/1000 | Loss: 0.00001465
Iteration 66/1000 | Loss: 0.00001464
Iteration 67/1000 | Loss: 0.00001464
Iteration 68/1000 | Loss: 0.00001464
Iteration 69/1000 | Loss: 0.00001463
Iteration 70/1000 | Loss: 0.00001463
Iteration 71/1000 | Loss: 0.00001463
Iteration 72/1000 | Loss: 0.00001463
Iteration 73/1000 | Loss: 0.00001463
Iteration 74/1000 | Loss: 0.00001463
Iteration 75/1000 | Loss: 0.00001463
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001462
Iteration 78/1000 | Loss: 0.00001462
Iteration 79/1000 | Loss: 0.00001462
Iteration 80/1000 | Loss: 0.00001462
Iteration 81/1000 | Loss: 0.00001462
Iteration 82/1000 | Loss: 0.00001462
Iteration 83/1000 | Loss: 0.00001462
Iteration 84/1000 | Loss: 0.00001462
Iteration 85/1000 | Loss: 0.00001461
Iteration 86/1000 | Loss: 0.00001461
Iteration 87/1000 | Loss: 0.00001461
Iteration 88/1000 | Loss: 0.00001461
Iteration 89/1000 | Loss: 0.00001461
Iteration 90/1000 | Loss: 0.00001461
Iteration 91/1000 | Loss: 0.00001460
Iteration 92/1000 | Loss: 0.00001460
Iteration 93/1000 | Loss: 0.00001460
Iteration 94/1000 | Loss: 0.00001460
Iteration 95/1000 | Loss: 0.00001460
Iteration 96/1000 | Loss: 0.00001459
Iteration 97/1000 | Loss: 0.00001459
Iteration 98/1000 | Loss: 0.00001459
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001458
Iteration 102/1000 | Loss: 0.00001458
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001457
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001455
Iteration 115/1000 | Loss: 0.00001455
Iteration 116/1000 | Loss: 0.00001455
Iteration 117/1000 | Loss: 0.00001455
Iteration 118/1000 | Loss: 0.00001455
Iteration 119/1000 | Loss: 0.00001455
Iteration 120/1000 | Loss: 0.00001455
Iteration 121/1000 | Loss: 0.00001455
Iteration 122/1000 | Loss: 0.00001455
Iteration 123/1000 | Loss: 0.00001455
Iteration 124/1000 | Loss: 0.00001455
Iteration 125/1000 | Loss: 0.00001455
Iteration 126/1000 | Loss: 0.00001455
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001455
Iteration 134/1000 | Loss: 0.00001455
Iteration 135/1000 | Loss: 0.00001455
Iteration 136/1000 | Loss: 0.00001455
Iteration 137/1000 | Loss: 0.00001455
Iteration 138/1000 | Loss: 0.00001455
Iteration 139/1000 | Loss: 0.00001455
Iteration 140/1000 | Loss: 0.00001455
Iteration 141/1000 | Loss: 0.00001455
Iteration 142/1000 | Loss: 0.00001455
Iteration 143/1000 | Loss: 0.00001455
Iteration 144/1000 | Loss: 0.00001455
Iteration 145/1000 | Loss: 0.00001455
Iteration 146/1000 | Loss: 0.00001455
Iteration 147/1000 | Loss: 0.00001455
Iteration 148/1000 | Loss: 0.00001455
Iteration 149/1000 | Loss: 0.00001455
Iteration 150/1000 | Loss: 0.00001455
Iteration 151/1000 | Loss: 0.00001455
Iteration 152/1000 | Loss: 0.00001455
Iteration 153/1000 | Loss: 0.00001455
Iteration 154/1000 | Loss: 0.00001455
Iteration 155/1000 | Loss: 0.00001455
Iteration 156/1000 | Loss: 0.00001455
Iteration 157/1000 | Loss: 0.00001455
Iteration 158/1000 | Loss: 0.00001455
Iteration 159/1000 | Loss: 0.00001455
Iteration 160/1000 | Loss: 0.00001455
Iteration 161/1000 | Loss: 0.00001455
Iteration 162/1000 | Loss: 0.00001455
Iteration 163/1000 | Loss: 0.00001455
Iteration 164/1000 | Loss: 0.00001455
Iteration 165/1000 | Loss: 0.00001455
Iteration 166/1000 | Loss: 0.00001455
Iteration 167/1000 | Loss: 0.00001455
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.454828725400148e-05, 1.454828725400148e-05, 1.454828725400148e-05, 1.454828725400148e-05, 1.454828725400148e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.454828725400148e-05

Optimization complete. Final v2v error: 3.2803850173950195 mm

Highest mean error: 3.5093066692352295 mm for frame 122

Lowest mean error: 2.989574909210205 mm for frame 0

Saving results

Total time: 34.88083219528198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00765506
Iteration 2/25 | Loss: 0.00148357
Iteration 3/25 | Loss: 0.00121707
Iteration 4/25 | Loss: 0.00116122
Iteration 5/25 | Loss: 0.00114325
Iteration 6/25 | Loss: 0.00113851
Iteration 7/25 | Loss: 0.00113648
Iteration 8/25 | Loss: 0.00113587
Iteration 9/25 | Loss: 0.00113587
Iteration 10/25 | Loss: 0.00113587
Iteration 11/25 | Loss: 0.00113587
Iteration 12/25 | Loss: 0.00113587
Iteration 13/25 | Loss: 0.00113587
Iteration 14/25 | Loss: 0.00113587
Iteration 15/25 | Loss: 0.00113587
Iteration 16/25 | Loss: 0.00113587
Iteration 17/25 | Loss: 0.00113587
Iteration 18/25 | Loss: 0.00113587
Iteration 19/25 | Loss: 0.00113587
Iteration 20/25 | Loss: 0.00113587
Iteration 21/25 | Loss: 0.00113587
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011358722113072872, 0.0011358722113072872, 0.0011358722113072872, 0.0011358722113072872, 0.0011358722113072872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011358722113072872

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54378998
Iteration 2/25 | Loss: 0.00140500
Iteration 3/25 | Loss: 0.00140499
Iteration 4/25 | Loss: 0.00140499
Iteration 5/25 | Loss: 0.00140499
Iteration 6/25 | Loss: 0.00140499
Iteration 7/25 | Loss: 0.00140499
Iteration 8/25 | Loss: 0.00140499
Iteration 9/25 | Loss: 0.00140499
Iteration 10/25 | Loss: 0.00140499
Iteration 11/25 | Loss: 0.00140499
Iteration 12/25 | Loss: 0.00140499
Iteration 13/25 | Loss: 0.00140499
Iteration 14/25 | Loss: 0.00140499
Iteration 15/25 | Loss: 0.00140499
Iteration 16/25 | Loss: 0.00140499
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014049919554963708, 0.0014049919554963708, 0.0014049919554963708, 0.0014049919554963708, 0.0014049919554963708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014049919554963708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140499
Iteration 2/1000 | Loss: 0.00167893
Iteration 3/1000 | Loss: 0.00218528
Iteration 4/1000 | Loss: 0.00017312
Iteration 5/1000 | Loss: 0.00135498
Iteration 6/1000 | Loss: 0.00012678
Iteration 7/1000 | Loss: 0.00008075
Iteration 8/1000 | Loss: 0.00037937
Iteration 9/1000 | Loss: 0.00005796
Iteration 10/1000 | Loss: 0.00108752
Iteration 11/1000 | Loss: 0.00043518
Iteration 12/1000 | Loss: 0.00077610
Iteration 13/1000 | Loss: 0.00034702
Iteration 14/1000 | Loss: 0.00094497
Iteration 15/1000 | Loss: 0.00071082
Iteration 16/1000 | Loss: 0.00034461
Iteration 17/1000 | Loss: 0.00110292
Iteration 18/1000 | Loss: 0.00007144
Iteration 19/1000 | Loss: 0.00062358
Iteration 20/1000 | Loss: 0.00080376
Iteration 21/1000 | Loss: 0.00004542
Iteration 22/1000 | Loss: 0.00038974
Iteration 23/1000 | Loss: 0.00051770
Iteration 24/1000 | Loss: 0.00025686
Iteration 25/1000 | Loss: 0.00066151
Iteration 26/1000 | Loss: 0.00046035
Iteration 27/1000 | Loss: 0.00007685
Iteration 28/1000 | Loss: 0.00045355
Iteration 29/1000 | Loss: 0.00016832
Iteration 30/1000 | Loss: 0.00046570
Iteration 31/1000 | Loss: 0.00039930
Iteration 32/1000 | Loss: 0.00020743
Iteration 33/1000 | Loss: 0.00011022
Iteration 34/1000 | Loss: 0.00021294
Iteration 35/1000 | Loss: 0.00019036
Iteration 36/1000 | Loss: 0.00015124
Iteration 37/1000 | Loss: 0.00047976
Iteration 38/1000 | Loss: 0.00011734
Iteration 39/1000 | Loss: 0.00051152
Iteration 40/1000 | Loss: 0.00014595
Iteration 41/1000 | Loss: 0.00097531
Iteration 42/1000 | Loss: 0.00024287
Iteration 43/1000 | Loss: 0.00004010
Iteration 44/1000 | Loss: 0.00066121
Iteration 45/1000 | Loss: 0.00024682
Iteration 46/1000 | Loss: 0.00004784
Iteration 47/1000 | Loss: 0.00003975
Iteration 48/1000 | Loss: 0.00003493
Iteration 49/1000 | Loss: 0.00003255
Iteration 50/1000 | Loss: 0.00002896
Iteration 51/1000 | Loss: 0.00002686
Iteration 52/1000 | Loss: 0.00002581
Iteration 53/1000 | Loss: 0.00002534
Iteration 54/1000 | Loss: 0.00002501
Iteration 55/1000 | Loss: 0.00002475
Iteration 56/1000 | Loss: 0.00002448
Iteration 57/1000 | Loss: 0.00002427
Iteration 58/1000 | Loss: 0.00002422
Iteration 59/1000 | Loss: 0.00002421
Iteration 60/1000 | Loss: 0.00002421
Iteration 61/1000 | Loss: 0.00002421
Iteration 62/1000 | Loss: 0.00002420
Iteration 63/1000 | Loss: 0.00002419
Iteration 64/1000 | Loss: 0.00002418
Iteration 65/1000 | Loss: 0.00002418
Iteration 66/1000 | Loss: 0.00002417
Iteration 67/1000 | Loss: 0.00002417
Iteration 68/1000 | Loss: 0.00002417
Iteration 69/1000 | Loss: 0.00002416
Iteration 70/1000 | Loss: 0.00002416
Iteration 71/1000 | Loss: 0.00002416
Iteration 72/1000 | Loss: 0.00002414
Iteration 73/1000 | Loss: 0.00002413
Iteration 74/1000 | Loss: 0.00002409
Iteration 75/1000 | Loss: 0.00002409
Iteration 76/1000 | Loss: 0.00002407
Iteration 77/1000 | Loss: 0.00002406
Iteration 78/1000 | Loss: 0.00002406
Iteration 79/1000 | Loss: 0.00002406
Iteration 80/1000 | Loss: 0.00002406
Iteration 81/1000 | Loss: 0.00002406
Iteration 82/1000 | Loss: 0.00002405
Iteration 83/1000 | Loss: 0.00002405
Iteration 84/1000 | Loss: 0.00002405
Iteration 85/1000 | Loss: 0.00002405
Iteration 86/1000 | Loss: 0.00002404
Iteration 87/1000 | Loss: 0.00002403
Iteration 88/1000 | Loss: 0.00002403
Iteration 89/1000 | Loss: 0.00002403
Iteration 90/1000 | Loss: 0.00002403
Iteration 91/1000 | Loss: 0.00002403
Iteration 92/1000 | Loss: 0.00002403
Iteration 93/1000 | Loss: 0.00002402
Iteration 94/1000 | Loss: 0.00002402
Iteration 95/1000 | Loss: 0.00002402
Iteration 96/1000 | Loss: 0.00002402
Iteration 97/1000 | Loss: 0.00002401
Iteration 98/1000 | Loss: 0.00002401
Iteration 99/1000 | Loss: 0.00002401
Iteration 100/1000 | Loss: 0.00002401
Iteration 101/1000 | Loss: 0.00002401
Iteration 102/1000 | Loss: 0.00002400
Iteration 103/1000 | Loss: 0.00002400
Iteration 104/1000 | Loss: 0.00002400
Iteration 105/1000 | Loss: 0.00002400
Iteration 106/1000 | Loss: 0.00002400
Iteration 107/1000 | Loss: 0.00002400
Iteration 108/1000 | Loss: 0.00002400
Iteration 109/1000 | Loss: 0.00002400
Iteration 110/1000 | Loss: 0.00002400
Iteration 111/1000 | Loss: 0.00002400
Iteration 112/1000 | Loss: 0.00002400
Iteration 113/1000 | Loss: 0.00002400
Iteration 114/1000 | Loss: 0.00002400
Iteration 115/1000 | Loss: 0.00002400
Iteration 116/1000 | Loss: 0.00002400
Iteration 117/1000 | Loss: 0.00002400
Iteration 118/1000 | Loss: 0.00002399
Iteration 119/1000 | Loss: 0.00002399
Iteration 120/1000 | Loss: 0.00002399
Iteration 121/1000 | Loss: 0.00002399
Iteration 122/1000 | Loss: 0.00002398
Iteration 123/1000 | Loss: 0.00002398
Iteration 124/1000 | Loss: 0.00002398
Iteration 125/1000 | Loss: 0.00002398
Iteration 126/1000 | Loss: 0.00002398
Iteration 127/1000 | Loss: 0.00002398
Iteration 128/1000 | Loss: 0.00002398
Iteration 129/1000 | Loss: 0.00002397
Iteration 130/1000 | Loss: 0.00002397
Iteration 131/1000 | Loss: 0.00002397
Iteration 132/1000 | Loss: 0.00002397
Iteration 133/1000 | Loss: 0.00002396
Iteration 134/1000 | Loss: 0.00002396
Iteration 135/1000 | Loss: 0.00002396
Iteration 136/1000 | Loss: 0.00002395
Iteration 137/1000 | Loss: 0.00002395
Iteration 138/1000 | Loss: 0.00002395
Iteration 139/1000 | Loss: 0.00002395
Iteration 140/1000 | Loss: 0.00002395
Iteration 141/1000 | Loss: 0.00002395
Iteration 142/1000 | Loss: 0.00002395
Iteration 143/1000 | Loss: 0.00002395
Iteration 144/1000 | Loss: 0.00002395
Iteration 145/1000 | Loss: 0.00002395
Iteration 146/1000 | Loss: 0.00002395
Iteration 147/1000 | Loss: 0.00002395
Iteration 148/1000 | Loss: 0.00002395
Iteration 149/1000 | Loss: 0.00002395
Iteration 150/1000 | Loss: 0.00002395
Iteration 151/1000 | Loss: 0.00002395
Iteration 152/1000 | Loss: 0.00002395
Iteration 153/1000 | Loss: 0.00002395
Iteration 154/1000 | Loss: 0.00002395
Iteration 155/1000 | Loss: 0.00002395
Iteration 156/1000 | Loss: 0.00002395
Iteration 157/1000 | Loss: 0.00002395
Iteration 158/1000 | Loss: 0.00002395
Iteration 159/1000 | Loss: 0.00002395
Iteration 160/1000 | Loss: 0.00002395
Iteration 161/1000 | Loss: 0.00002395
Iteration 162/1000 | Loss: 0.00002395
Iteration 163/1000 | Loss: 0.00002395
Iteration 164/1000 | Loss: 0.00002395
Iteration 165/1000 | Loss: 0.00002395
Iteration 166/1000 | Loss: 0.00002395
Iteration 167/1000 | Loss: 0.00002395
Iteration 168/1000 | Loss: 0.00002395
Iteration 169/1000 | Loss: 0.00002395
Iteration 170/1000 | Loss: 0.00002395
Iteration 171/1000 | Loss: 0.00002395
Iteration 172/1000 | Loss: 0.00002395
Iteration 173/1000 | Loss: 0.00002395
Iteration 174/1000 | Loss: 0.00002395
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [2.3946740839164704e-05, 2.3946740839164704e-05, 2.3946740839164704e-05, 2.3946740839164704e-05, 2.3946740839164704e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3946740839164704e-05

Optimization complete. Final v2v error: 4.116215705871582 mm

Highest mean error: 5.151797294616699 mm for frame 137

Lowest mean error: 3.7078492641448975 mm for frame 214

Saving results

Total time: 114.72291827201843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828840
Iteration 2/25 | Loss: 0.00167813
Iteration 3/25 | Loss: 0.00140815
Iteration 4/25 | Loss: 0.00134932
Iteration 5/25 | Loss: 0.00133530
Iteration 6/25 | Loss: 0.00133230
Iteration 7/25 | Loss: 0.00133177
Iteration 8/25 | Loss: 0.00133177
Iteration 9/25 | Loss: 0.00133169
Iteration 10/25 | Loss: 0.00133165
Iteration 11/25 | Loss: 0.00133165
Iteration 12/25 | Loss: 0.00133165
Iteration 13/25 | Loss: 0.00133165
Iteration 14/25 | Loss: 0.00133165
Iteration 15/25 | Loss: 0.00133165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001331652281805873, 0.001331652281805873, 0.001331652281805873, 0.001331652281805873, 0.001331652281805873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001331652281805873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.95667619
Iteration 2/25 | Loss: 0.00086635
Iteration 3/25 | Loss: 0.00086628
Iteration 4/25 | Loss: 0.00086628
Iteration 5/25 | Loss: 0.00086628
Iteration 6/25 | Loss: 0.00086628
Iteration 7/25 | Loss: 0.00086628
Iteration 8/25 | Loss: 0.00086628
Iteration 9/25 | Loss: 0.00086628
Iteration 10/25 | Loss: 0.00086627
Iteration 11/25 | Loss: 0.00086627
Iteration 12/25 | Loss: 0.00086627
Iteration 13/25 | Loss: 0.00086627
Iteration 14/25 | Loss: 0.00086627
Iteration 15/25 | Loss: 0.00086627
Iteration 16/25 | Loss: 0.00086627
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000866274640429765, 0.000866274640429765, 0.000866274640429765, 0.000866274640429765, 0.000866274640429765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000866274640429765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086627
Iteration 2/1000 | Loss: 0.00010308
Iteration 3/1000 | Loss: 0.00008235
Iteration 4/1000 | Loss: 0.00007566
Iteration 5/1000 | Loss: 0.00007116
Iteration 6/1000 | Loss: 0.00006702
Iteration 7/1000 | Loss: 0.00006582
Iteration 8/1000 | Loss: 0.00006424
Iteration 9/1000 | Loss: 0.00006356
Iteration 10/1000 | Loss: 0.00006298
Iteration 11/1000 | Loss: 0.00006242
Iteration 12/1000 | Loss: 0.00006199
Iteration 13/1000 | Loss: 0.00006154
Iteration 14/1000 | Loss: 0.00006119
Iteration 15/1000 | Loss: 0.00006090
Iteration 16/1000 | Loss: 0.00006062
Iteration 17/1000 | Loss: 0.00006031
Iteration 18/1000 | Loss: 0.00006007
Iteration 19/1000 | Loss: 0.00005993
Iteration 20/1000 | Loss: 0.00005982
Iteration 21/1000 | Loss: 0.00005964
Iteration 22/1000 | Loss: 0.00005955
Iteration 23/1000 | Loss: 0.00005954
Iteration 24/1000 | Loss: 0.00005949
Iteration 25/1000 | Loss: 0.00005949
Iteration 26/1000 | Loss: 0.00005948
Iteration 27/1000 | Loss: 0.00005948
Iteration 28/1000 | Loss: 0.00005948
Iteration 29/1000 | Loss: 0.00005947
Iteration 30/1000 | Loss: 0.00005947
Iteration 31/1000 | Loss: 0.00005947
Iteration 32/1000 | Loss: 0.00005946
Iteration 33/1000 | Loss: 0.00005945
Iteration 34/1000 | Loss: 0.00005945
Iteration 35/1000 | Loss: 0.00005945
Iteration 36/1000 | Loss: 0.00005944
Iteration 37/1000 | Loss: 0.00005944
Iteration 38/1000 | Loss: 0.00005944
Iteration 39/1000 | Loss: 0.00005944
Iteration 40/1000 | Loss: 0.00005944
Iteration 41/1000 | Loss: 0.00005944
Iteration 42/1000 | Loss: 0.00005944
Iteration 43/1000 | Loss: 0.00005944
Iteration 44/1000 | Loss: 0.00005944
Iteration 45/1000 | Loss: 0.00005944
Iteration 46/1000 | Loss: 0.00005944
Iteration 47/1000 | Loss: 0.00005943
Iteration 48/1000 | Loss: 0.00005943
Iteration 49/1000 | Loss: 0.00005943
Iteration 50/1000 | Loss: 0.00005943
Iteration 51/1000 | Loss: 0.00005942
Iteration 52/1000 | Loss: 0.00005942
Iteration 53/1000 | Loss: 0.00005942
Iteration 54/1000 | Loss: 0.00005942
Iteration 55/1000 | Loss: 0.00005941
Iteration 56/1000 | Loss: 0.00005941
Iteration 57/1000 | Loss: 0.00005940
Iteration 58/1000 | Loss: 0.00005940
Iteration 59/1000 | Loss: 0.00005940
Iteration 60/1000 | Loss: 0.00005940
Iteration 61/1000 | Loss: 0.00005940
Iteration 62/1000 | Loss: 0.00005940
Iteration 63/1000 | Loss: 0.00005940
Iteration 64/1000 | Loss: 0.00005939
Iteration 65/1000 | Loss: 0.00005939
Iteration 66/1000 | Loss: 0.00005939
Iteration 67/1000 | Loss: 0.00005939
Iteration 68/1000 | Loss: 0.00005939
Iteration 69/1000 | Loss: 0.00005939
Iteration 70/1000 | Loss: 0.00005939
Iteration 71/1000 | Loss: 0.00005939
Iteration 72/1000 | Loss: 0.00005939
Iteration 73/1000 | Loss: 0.00005939
Iteration 74/1000 | Loss: 0.00005939
Iteration 75/1000 | Loss: 0.00005938
Iteration 76/1000 | Loss: 0.00005938
Iteration 77/1000 | Loss: 0.00005938
Iteration 78/1000 | Loss: 0.00005938
Iteration 79/1000 | Loss: 0.00005937
Iteration 80/1000 | Loss: 0.00005936
Iteration 81/1000 | Loss: 0.00005936
Iteration 82/1000 | Loss: 0.00005935
Iteration 83/1000 | Loss: 0.00005934
Iteration 84/1000 | Loss: 0.00005934
Iteration 85/1000 | Loss: 0.00005934
Iteration 86/1000 | Loss: 0.00005933
Iteration 87/1000 | Loss: 0.00005933
Iteration 88/1000 | Loss: 0.00005933
Iteration 89/1000 | Loss: 0.00005932
Iteration 90/1000 | Loss: 0.00005932
Iteration 91/1000 | Loss: 0.00005930
Iteration 92/1000 | Loss: 0.00005930
Iteration 93/1000 | Loss: 0.00005930
Iteration 94/1000 | Loss: 0.00005930
Iteration 95/1000 | Loss: 0.00005930
Iteration 96/1000 | Loss: 0.00005930
Iteration 97/1000 | Loss: 0.00005930
Iteration 98/1000 | Loss: 0.00005930
Iteration 99/1000 | Loss: 0.00005930
Iteration 100/1000 | Loss: 0.00005930
Iteration 101/1000 | Loss: 0.00005929
Iteration 102/1000 | Loss: 0.00005929
Iteration 103/1000 | Loss: 0.00005926
Iteration 104/1000 | Loss: 0.00005924
Iteration 105/1000 | Loss: 0.00005917
Iteration 106/1000 | Loss: 0.00005917
Iteration 107/1000 | Loss: 0.00005917
Iteration 108/1000 | Loss: 0.00005915
Iteration 109/1000 | Loss: 0.00005915
Iteration 110/1000 | Loss: 0.00005915
Iteration 111/1000 | Loss: 0.00005913
Iteration 112/1000 | Loss: 0.00005913
Iteration 113/1000 | Loss: 0.00005913
Iteration 114/1000 | Loss: 0.00005913
Iteration 115/1000 | Loss: 0.00005913
Iteration 116/1000 | Loss: 0.00005913
Iteration 117/1000 | Loss: 0.00005913
Iteration 118/1000 | Loss: 0.00005913
Iteration 119/1000 | Loss: 0.00005913
Iteration 120/1000 | Loss: 0.00005913
Iteration 121/1000 | Loss: 0.00005913
Iteration 122/1000 | Loss: 0.00005913
Iteration 123/1000 | Loss: 0.00005913
Iteration 124/1000 | Loss: 0.00005913
Iteration 125/1000 | Loss: 0.00005913
Iteration 126/1000 | Loss: 0.00005913
Iteration 127/1000 | Loss: 0.00005913
Iteration 128/1000 | Loss: 0.00005913
Iteration 129/1000 | Loss: 0.00005913
Iteration 130/1000 | Loss: 0.00005913
Iteration 131/1000 | Loss: 0.00005913
Iteration 132/1000 | Loss: 0.00005913
Iteration 133/1000 | Loss: 0.00005913
Iteration 134/1000 | Loss: 0.00005913
Iteration 135/1000 | Loss: 0.00005913
Iteration 136/1000 | Loss: 0.00005913
Iteration 137/1000 | Loss: 0.00005913
Iteration 138/1000 | Loss: 0.00005913
Iteration 139/1000 | Loss: 0.00005913
Iteration 140/1000 | Loss: 0.00005913
Iteration 141/1000 | Loss: 0.00005913
Iteration 142/1000 | Loss: 0.00005913
Iteration 143/1000 | Loss: 0.00005913
Iteration 144/1000 | Loss: 0.00005913
Iteration 145/1000 | Loss: 0.00005913
Iteration 146/1000 | Loss: 0.00005913
Iteration 147/1000 | Loss: 0.00005913
Iteration 148/1000 | Loss: 0.00005913
Iteration 149/1000 | Loss: 0.00005913
Iteration 150/1000 | Loss: 0.00005913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [5.912685446674004e-05, 5.912685446674004e-05, 5.912685446674004e-05, 5.912685446674004e-05, 5.912685446674004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.912685446674004e-05

Optimization complete. Final v2v error: 6.308858871459961 mm

Highest mean error: 8.092021942138672 mm for frame 120

Lowest mean error: 5.439899921417236 mm for frame 23

Saving results

Total time: 59.81986737251282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911953
Iteration 2/25 | Loss: 0.00133643
Iteration 3/25 | Loss: 0.00104991
Iteration 4/25 | Loss: 0.00099539
Iteration 5/25 | Loss: 0.00098856
Iteration 6/25 | Loss: 0.00098451
Iteration 7/25 | Loss: 0.00098116
Iteration 8/25 | Loss: 0.00098067
Iteration 9/25 | Loss: 0.00098054
Iteration 10/25 | Loss: 0.00098107
Iteration 11/25 | Loss: 0.00097967
Iteration 12/25 | Loss: 0.00097981
Iteration 13/25 | Loss: 0.00097928
Iteration 14/25 | Loss: 0.00097924
Iteration 15/25 | Loss: 0.00097989
Iteration 16/25 | Loss: 0.00097943
Iteration 17/25 | Loss: 0.00097924
Iteration 18/25 | Loss: 0.00097924
Iteration 19/25 | Loss: 0.00097923
Iteration 20/25 | Loss: 0.00097923
Iteration 21/25 | Loss: 0.00097923
Iteration 22/25 | Loss: 0.00097923
Iteration 23/25 | Loss: 0.00097923
Iteration 24/25 | Loss: 0.00097923
Iteration 25/25 | Loss: 0.00097923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.89928818
Iteration 2/25 | Loss: 0.00093991
Iteration 3/25 | Loss: 0.00093991
Iteration 4/25 | Loss: 0.00093991
Iteration 5/25 | Loss: 0.00093991
Iteration 6/25 | Loss: 0.00093991
Iteration 7/25 | Loss: 0.00093991
Iteration 8/25 | Loss: 0.00093991
Iteration 9/25 | Loss: 0.00093991
Iteration 10/25 | Loss: 0.00093991
Iteration 11/25 | Loss: 0.00093991
Iteration 12/25 | Loss: 0.00093991
Iteration 13/25 | Loss: 0.00093991
Iteration 14/25 | Loss: 0.00093991
Iteration 15/25 | Loss: 0.00093991
Iteration 16/25 | Loss: 0.00093991
Iteration 17/25 | Loss: 0.00093991
Iteration 18/25 | Loss: 0.00093991
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009399106493219733, 0.0009399106493219733, 0.0009399106493219733, 0.0009399106493219733, 0.0009399106493219733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009399106493219733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093991
Iteration 2/1000 | Loss: 0.00003013
Iteration 3/1000 | Loss: 0.00002742
Iteration 4/1000 | Loss: 0.00002248
Iteration 5/1000 | Loss: 0.00002019
Iteration 6/1000 | Loss: 0.00002002
Iteration 7/1000 | Loss: 0.00003347
Iteration 8/1000 | Loss: 0.00001911
Iteration 9/1000 | Loss: 0.00001883
Iteration 10/1000 | Loss: 0.00002938
Iteration 11/1000 | Loss: 0.00002334
Iteration 12/1000 | Loss: 0.00002943
Iteration 13/1000 | Loss: 0.00002194
Iteration 14/1000 | Loss: 0.00002331
Iteration 15/1000 | Loss: 0.00001937
Iteration 16/1000 | Loss: 0.00002157
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00004498
Iteration 19/1000 | Loss: 0.00001842
Iteration 20/1000 | Loss: 0.00001841
Iteration 21/1000 | Loss: 0.00001841
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001841
Iteration 24/1000 | Loss: 0.00001841
Iteration 25/1000 | Loss: 0.00001841
Iteration 26/1000 | Loss: 0.00001999
Iteration 27/1000 | Loss: 0.00001876
Iteration 28/1000 | Loss: 0.00001839
Iteration 29/1000 | Loss: 0.00001838
Iteration 30/1000 | Loss: 0.00001838
Iteration 31/1000 | Loss: 0.00001838
Iteration 32/1000 | Loss: 0.00001838
Iteration 33/1000 | Loss: 0.00001838
Iteration 34/1000 | Loss: 0.00001838
Iteration 35/1000 | Loss: 0.00001838
Iteration 36/1000 | Loss: 0.00001838
Iteration 37/1000 | Loss: 0.00001838
Iteration 38/1000 | Loss: 0.00001838
Iteration 39/1000 | Loss: 0.00001838
Iteration 40/1000 | Loss: 0.00001838
Iteration 41/1000 | Loss: 0.00001838
Iteration 42/1000 | Loss: 0.00001838
Iteration 43/1000 | Loss: 0.00001838
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 43. Stopping optimization.
Last 5 losses: [1.8379178072791547e-05, 1.8379178072791547e-05, 1.8379178072791547e-05, 1.8379178072791547e-05, 1.8379178072791547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8379178072791547e-05

Optimization complete. Final v2v error: 3.678178310394287 mm

Highest mean error: 10.041474342346191 mm for frame 15

Lowest mean error: 3.1094820499420166 mm for frame 93

Saving results

Total time: 58.612300157547
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00924466
Iteration 2/25 | Loss: 0.00118727
Iteration 3/25 | Loss: 0.00104233
Iteration 4/25 | Loss: 0.00101989
Iteration 5/25 | Loss: 0.00101249
Iteration 6/25 | Loss: 0.00101039
Iteration 7/25 | Loss: 0.00101016
Iteration 8/25 | Loss: 0.00101016
Iteration 9/25 | Loss: 0.00101016
Iteration 10/25 | Loss: 0.00101016
Iteration 11/25 | Loss: 0.00101016
Iteration 12/25 | Loss: 0.00101016
Iteration 13/25 | Loss: 0.00101016
Iteration 14/25 | Loss: 0.00101016
Iteration 15/25 | Loss: 0.00101016
Iteration 16/25 | Loss: 0.00101016
Iteration 17/25 | Loss: 0.00101016
Iteration 18/25 | Loss: 0.00101016
Iteration 19/25 | Loss: 0.00101016
Iteration 20/25 | Loss: 0.00101016
Iteration 21/25 | Loss: 0.00101016
Iteration 22/25 | Loss: 0.00101016
Iteration 23/25 | Loss: 0.00101016
Iteration 24/25 | Loss: 0.00101016
Iteration 25/25 | Loss: 0.00101016

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51808453
Iteration 2/25 | Loss: 0.00094145
Iteration 3/25 | Loss: 0.00094143
Iteration 4/25 | Loss: 0.00094143
Iteration 5/25 | Loss: 0.00094143
Iteration 6/25 | Loss: 0.00094143
Iteration 7/25 | Loss: 0.00094142
Iteration 8/25 | Loss: 0.00094142
Iteration 9/25 | Loss: 0.00094142
Iteration 10/25 | Loss: 0.00094142
Iteration 11/25 | Loss: 0.00094142
Iteration 12/25 | Loss: 0.00094142
Iteration 13/25 | Loss: 0.00094142
Iteration 14/25 | Loss: 0.00094142
Iteration 15/25 | Loss: 0.00094142
Iteration 16/25 | Loss: 0.00094142
Iteration 17/25 | Loss: 0.00094142
Iteration 18/25 | Loss: 0.00094142
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009414240485057235, 0.0009414240485057235, 0.0009414240485057235, 0.0009414240485057235, 0.0009414240485057235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009414240485057235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094142
Iteration 2/1000 | Loss: 0.00003084
Iteration 3/1000 | Loss: 0.00002373
Iteration 4/1000 | Loss: 0.00001944
Iteration 5/1000 | Loss: 0.00001718
Iteration 6/1000 | Loss: 0.00001645
Iteration 7/1000 | Loss: 0.00001587
Iteration 8/1000 | Loss: 0.00001546
Iteration 9/1000 | Loss: 0.00001510
Iteration 10/1000 | Loss: 0.00001501
Iteration 11/1000 | Loss: 0.00001496
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001495
Iteration 14/1000 | Loss: 0.00001495
Iteration 15/1000 | Loss: 0.00001493
Iteration 16/1000 | Loss: 0.00001490
Iteration 17/1000 | Loss: 0.00001485
Iteration 18/1000 | Loss: 0.00001482
Iteration 19/1000 | Loss: 0.00001482
Iteration 20/1000 | Loss: 0.00001481
Iteration 21/1000 | Loss: 0.00001481
Iteration 22/1000 | Loss: 0.00001479
Iteration 23/1000 | Loss: 0.00001478
Iteration 24/1000 | Loss: 0.00001478
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001477
Iteration 27/1000 | Loss: 0.00001471
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001469
Iteration 30/1000 | Loss: 0.00001467
Iteration 31/1000 | Loss: 0.00001467
Iteration 32/1000 | Loss: 0.00001467
Iteration 33/1000 | Loss: 0.00001467
Iteration 34/1000 | Loss: 0.00001467
Iteration 35/1000 | Loss: 0.00001467
Iteration 36/1000 | Loss: 0.00001467
Iteration 37/1000 | Loss: 0.00001466
Iteration 38/1000 | Loss: 0.00001466
Iteration 39/1000 | Loss: 0.00001466
Iteration 40/1000 | Loss: 0.00001466
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001466
Iteration 43/1000 | Loss: 0.00001466
Iteration 44/1000 | Loss: 0.00001466
Iteration 45/1000 | Loss: 0.00001466
Iteration 46/1000 | Loss: 0.00001466
Iteration 47/1000 | Loss: 0.00001466
Iteration 48/1000 | Loss: 0.00001466
Iteration 49/1000 | Loss: 0.00001466
Iteration 50/1000 | Loss: 0.00001466
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001466
Iteration 53/1000 | Loss: 0.00001466
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001466
Iteration 56/1000 | Loss: 0.00001466
Iteration 57/1000 | Loss: 0.00001466
Iteration 58/1000 | Loss: 0.00001466
Iteration 59/1000 | Loss: 0.00001466
Iteration 60/1000 | Loss: 0.00001466
Iteration 61/1000 | Loss: 0.00001466
Iteration 62/1000 | Loss: 0.00001466
Iteration 63/1000 | Loss: 0.00001466
Iteration 64/1000 | Loss: 0.00001466
Iteration 65/1000 | Loss: 0.00001466
Iteration 66/1000 | Loss: 0.00001466
Iteration 67/1000 | Loss: 0.00001466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 67. Stopping optimization.
Last 5 losses: [1.4656242456112523e-05, 1.4656242456112523e-05, 1.4656242456112523e-05, 1.4656242456112523e-05, 1.4656242456112523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4656242456112523e-05

Optimization complete. Final v2v error: 3.3103504180908203 mm

Highest mean error: 3.5287649631500244 mm for frame 29

Lowest mean error: 3.0697708129882812 mm for frame 150

Saving results

Total time: 30.245046854019165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145356
Iteration 2/25 | Loss: 0.00166016
Iteration 3/25 | Loss: 0.00124956
Iteration 4/25 | Loss: 0.00130506
Iteration 5/25 | Loss: 0.00116960
Iteration 6/25 | Loss: 0.00109754
Iteration 7/25 | Loss: 0.00110245
Iteration 8/25 | Loss: 0.00106771
Iteration 9/25 | Loss: 0.00105977
Iteration 10/25 | Loss: 0.00105775
Iteration 11/25 | Loss: 0.00105680
Iteration 12/25 | Loss: 0.00105775
Iteration 13/25 | Loss: 0.00105744
Iteration 14/25 | Loss: 0.00105547
Iteration 15/25 | Loss: 0.00105431
Iteration 16/25 | Loss: 0.00105380
Iteration 17/25 | Loss: 0.00108558
Iteration 18/25 | Loss: 0.00105550
Iteration 19/25 | Loss: 0.00105484
Iteration 20/25 | Loss: 0.00105462
Iteration 21/25 | Loss: 0.00105444
Iteration 22/25 | Loss: 0.00105545
Iteration 23/25 | Loss: 0.00105423
Iteration 24/25 | Loss: 0.00107793
Iteration 25/25 | Loss: 0.00104464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.66559315
Iteration 2/25 | Loss: 0.00095960
Iteration 3/25 | Loss: 0.00095960
Iteration 4/25 | Loss: 0.00095960
Iteration 5/25 | Loss: 0.00095960
Iteration 6/25 | Loss: 0.00095960
Iteration 7/25 | Loss: 0.00095960
Iteration 8/25 | Loss: 0.00095960
Iteration 9/25 | Loss: 0.00095960
Iteration 10/25 | Loss: 0.00095960
Iteration 11/25 | Loss: 0.00095960
Iteration 12/25 | Loss: 0.00095960
Iteration 13/25 | Loss: 0.00095960
Iteration 14/25 | Loss: 0.00095960
Iteration 15/25 | Loss: 0.00095960
Iteration 16/25 | Loss: 0.00095960
Iteration 17/25 | Loss: 0.00095960
Iteration 18/25 | Loss: 0.00095960
Iteration 19/25 | Loss: 0.00095960
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009595957235433161, 0.0009595957235433161, 0.0009595957235433161, 0.0009595957235433161, 0.0009595957235433161]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009595957235433161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095960
Iteration 2/1000 | Loss: 0.00003244
Iteration 3/1000 | Loss: 0.00002292
Iteration 4/1000 | Loss: 0.00001975
Iteration 5/1000 | Loss: 0.00001849
Iteration 6/1000 | Loss: 0.00001778
Iteration 7/1000 | Loss: 0.00001732
Iteration 8/1000 | Loss: 0.00001700
Iteration 9/1000 | Loss: 0.00001676
Iteration 10/1000 | Loss: 0.00001669
Iteration 11/1000 | Loss: 0.00001664
Iteration 12/1000 | Loss: 0.00001659
Iteration 13/1000 | Loss: 0.00001653
Iteration 14/1000 | Loss: 0.00001652
Iteration 15/1000 | Loss: 0.00001651
Iteration 16/1000 | Loss: 0.00001650
Iteration 17/1000 | Loss: 0.00001645
Iteration 18/1000 | Loss: 0.00001643
Iteration 19/1000 | Loss: 0.00001641
Iteration 20/1000 | Loss: 0.00001640
Iteration 21/1000 | Loss: 0.00001640
Iteration 22/1000 | Loss: 0.00001639
Iteration 23/1000 | Loss: 0.00001636
Iteration 24/1000 | Loss: 0.00001636
Iteration 25/1000 | Loss: 0.00001635
Iteration 26/1000 | Loss: 0.00001635
Iteration 27/1000 | Loss: 0.00001635
Iteration 28/1000 | Loss: 0.00001635
Iteration 29/1000 | Loss: 0.00001635
Iteration 30/1000 | Loss: 0.00001635
Iteration 31/1000 | Loss: 0.00001634
Iteration 32/1000 | Loss: 0.00001634
Iteration 33/1000 | Loss: 0.00001633
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001632
Iteration 37/1000 | Loss: 0.00001632
Iteration 38/1000 | Loss: 0.00001632
Iteration 39/1000 | Loss: 0.00001631
Iteration 40/1000 | Loss: 0.00001631
Iteration 41/1000 | Loss: 0.00001631
Iteration 42/1000 | Loss: 0.00001631
Iteration 43/1000 | Loss: 0.00001631
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001628
Iteration 55/1000 | Loss: 0.00001628
Iteration 56/1000 | Loss: 0.00001628
Iteration 57/1000 | Loss: 0.00001628
Iteration 58/1000 | Loss: 0.00001628
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001628
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001627
Iteration 68/1000 | Loss: 0.00001627
Iteration 69/1000 | Loss: 0.00001627
Iteration 70/1000 | Loss: 0.00001626
Iteration 71/1000 | Loss: 0.00001626
Iteration 72/1000 | Loss: 0.00001626
Iteration 73/1000 | Loss: 0.00001626
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001626
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.626270022825338e-05, 1.626270022825338e-05, 1.626270022825338e-05, 1.626270022825338e-05, 1.626270022825338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.626270022825338e-05

Optimization complete. Final v2v error: 3.4123995304107666 mm

Highest mean error: 4.07696533203125 mm for frame 0

Lowest mean error: 3.072265863418579 mm for frame 69

Saving results

Total time: 68.17925977706909
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01095828
Iteration 2/25 | Loss: 0.00172194
Iteration 3/25 | Loss: 0.00136065
Iteration 4/25 | Loss: 0.00130668
Iteration 5/25 | Loss: 0.00128885
Iteration 6/25 | Loss: 0.00125466
Iteration 7/25 | Loss: 0.00124861
Iteration 8/25 | Loss: 0.00124772
Iteration 9/25 | Loss: 0.00124201
Iteration 10/25 | Loss: 0.00123838
Iteration 11/25 | Loss: 0.00123488
Iteration 12/25 | Loss: 0.00123579
Iteration 13/25 | Loss: 0.00123158
Iteration 14/25 | Loss: 0.00123548
Iteration 15/25 | Loss: 0.00122947
Iteration 16/25 | Loss: 0.00122337
Iteration 17/25 | Loss: 0.00122015
Iteration 18/25 | Loss: 0.00121959
Iteration 19/25 | Loss: 0.00121932
Iteration 20/25 | Loss: 0.00121920
Iteration 21/25 | Loss: 0.00121919
Iteration 22/25 | Loss: 0.00121919
Iteration 23/25 | Loss: 0.00121919
Iteration 24/25 | Loss: 0.00121919
Iteration 25/25 | Loss: 0.00121919

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89208651
Iteration 2/25 | Loss: 0.00198091
Iteration 3/25 | Loss: 0.00198091
Iteration 4/25 | Loss: 0.00198091
Iteration 5/25 | Loss: 0.00198091
Iteration 6/25 | Loss: 0.00198091
Iteration 7/25 | Loss: 0.00198091
Iteration 8/25 | Loss: 0.00198091
Iteration 9/25 | Loss: 0.00198091
Iteration 10/25 | Loss: 0.00198091
Iteration 11/25 | Loss: 0.00198091
Iteration 12/25 | Loss: 0.00198091
Iteration 13/25 | Loss: 0.00198091
Iteration 14/25 | Loss: 0.00198091
Iteration 15/25 | Loss: 0.00198091
Iteration 16/25 | Loss: 0.00198091
Iteration 17/25 | Loss: 0.00198091
Iteration 18/25 | Loss: 0.00198091
Iteration 19/25 | Loss: 0.00198091
Iteration 20/25 | Loss: 0.00198091
Iteration 21/25 | Loss: 0.00198091
Iteration 22/25 | Loss: 0.00198091
Iteration 23/25 | Loss: 0.00198091
Iteration 24/25 | Loss: 0.00198091
Iteration 25/25 | Loss: 0.00198091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198091
Iteration 2/1000 | Loss: 0.00037542
Iteration 3/1000 | Loss: 0.00040314
Iteration 4/1000 | Loss: 0.00016748
Iteration 5/1000 | Loss: 0.00141332
Iteration 6/1000 | Loss: 0.00155036
Iteration 7/1000 | Loss: 0.00067348
Iteration 8/1000 | Loss: 0.00190876
Iteration 9/1000 | Loss: 0.00078077
Iteration 10/1000 | Loss: 0.00058094
Iteration 11/1000 | Loss: 0.00057991
Iteration 12/1000 | Loss: 0.00072076
Iteration 13/1000 | Loss: 0.00116910
Iteration 14/1000 | Loss: 0.00082681
Iteration 15/1000 | Loss: 0.00017166
Iteration 16/1000 | Loss: 0.00012584
Iteration 17/1000 | Loss: 0.00015420
Iteration 18/1000 | Loss: 0.00095580
Iteration 19/1000 | Loss: 0.00035648
Iteration 20/1000 | Loss: 0.00050986
Iteration 21/1000 | Loss: 0.00009760
Iteration 22/1000 | Loss: 0.00017477
Iteration 23/1000 | Loss: 0.00056082
Iteration 24/1000 | Loss: 0.00009104
Iteration 25/1000 | Loss: 0.00008359
Iteration 26/1000 | Loss: 0.00100736
Iteration 27/1000 | Loss: 0.00263939
Iteration 28/1000 | Loss: 0.00197874
Iteration 29/1000 | Loss: 0.00167947
Iteration 30/1000 | Loss: 0.00172906
Iteration 31/1000 | Loss: 0.00040555
Iteration 32/1000 | Loss: 0.00009855
Iteration 33/1000 | Loss: 0.00008129
Iteration 34/1000 | Loss: 0.00007570
Iteration 35/1000 | Loss: 0.00176622
Iteration 36/1000 | Loss: 0.00017128
Iteration 37/1000 | Loss: 0.00008063
Iteration 38/1000 | Loss: 0.00067269
Iteration 39/1000 | Loss: 0.00286742
Iteration 40/1000 | Loss: 0.00211156
Iteration 41/1000 | Loss: 0.00011899
Iteration 42/1000 | Loss: 0.00009652
Iteration 43/1000 | Loss: 0.00007105
Iteration 44/1000 | Loss: 0.00006300
Iteration 45/1000 | Loss: 0.00053698
Iteration 46/1000 | Loss: 0.00062549
Iteration 47/1000 | Loss: 0.00006808
Iteration 48/1000 | Loss: 0.00005619
Iteration 49/1000 | Loss: 0.00005147
Iteration 50/1000 | Loss: 0.00004727
Iteration 51/1000 | Loss: 0.00004471
Iteration 52/1000 | Loss: 0.00004321
Iteration 53/1000 | Loss: 0.00004186
Iteration 54/1000 | Loss: 0.00003974
Iteration 55/1000 | Loss: 0.00003808
Iteration 56/1000 | Loss: 0.00003705
Iteration 57/1000 | Loss: 0.00003643
Iteration 58/1000 | Loss: 0.00003611
Iteration 59/1000 | Loss: 0.00003591
Iteration 60/1000 | Loss: 0.00003588
Iteration 61/1000 | Loss: 0.00003587
Iteration 62/1000 | Loss: 0.00003584
Iteration 63/1000 | Loss: 0.00003570
Iteration 64/1000 | Loss: 0.00003569
Iteration 65/1000 | Loss: 0.00003568
Iteration 66/1000 | Loss: 0.00003568
Iteration 67/1000 | Loss: 0.00003567
Iteration 68/1000 | Loss: 0.00003561
Iteration 69/1000 | Loss: 0.00003559
Iteration 70/1000 | Loss: 0.00003558
Iteration 71/1000 | Loss: 0.00003557
Iteration 72/1000 | Loss: 0.00003557
Iteration 73/1000 | Loss: 0.00003557
Iteration 74/1000 | Loss: 0.00003556
Iteration 75/1000 | Loss: 0.00003556
Iteration 76/1000 | Loss: 0.00003555
Iteration 77/1000 | Loss: 0.00003555
Iteration 78/1000 | Loss: 0.00003555
Iteration 79/1000 | Loss: 0.00003555
Iteration 80/1000 | Loss: 0.00003553
Iteration 81/1000 | Loss: 0.00003553
Iteration 82/1000 | Loss: 0.00003553
Iteration 83/1000 | Loss: 0.00003553
Iteration 84/1000 | Loss: 0.00003552
Iteration 85/1000 | Loss: 0.00003552
Iteration 86/1000 | Loss: 0.00003552
Iteration 87/1000 | Loss: 0.00003551
Iteration 88/1000 | Loss: 0.00003551
Iteration 89/1000 | Loss: 0.00003551
Iteration 90/1000 | Loss: 0.00003550
Iteration 91/1000 | Loss: 0.00003550
Iteration 92/1000 | Loss: 0.00003550
Iteration 93/1000 | Loss: 0.00003550
Iteration 94/1000 | Loss: 0.00003550
Iteration 95/1000 | Loss: 0.00003549
Iteration 96/1000 | Loss: 0.00003549
Iteration 97/1000 | Loss: 0.00003549
Iteration 98/1000 | Loss: 0.00003549
Iteration 99/1000 | Loss: 0.00003549
Iteration 100/1000 | Loss: 0.00003549
Iteration 101/1000 | Loss: 0.00003549
Iteration 102/1000 | Loss: 0.00003549
Iteration 103/1000 | Loss: 0.00003549
Iteration 104/1000 | Loss: 0.00003549
Iteration 105/1000 | Loss: 0.00003549
Iteration 106/1000 | Loss: 0.00003549
Iteration 107/1000 | Loss: 0.00003549
Iteration 108/1000 | Loss: 0.00003549
Iteration 109/1000 | Loss: 0.00003548
Iteration 110/1000 | Loss: 0.00003548
Iteration 111/1000 | Loss: 0.00003548
Iteration 112/1000 | Loss: 0.00003548
Iteration 113/1000 | Loss: 0.00003548
Iteration 114/1000 | Loss: 0.00003548
Iteration 115/1000 | Loss: 0.00003548
Iteration 116/1000 | Loss: 0.00003548
Iteration 117/1000 | Loss: 0.00003547
Iteration 118/1000 | Loss: 0.00003547
Iteration 119/1000 | Loss: 0.00003547
Iteration 120/1000 | Loss: 0.00003547
Iteration 121/1000 | Loss: 0.00003547
Iteration 122/1000 | Loss: 0.00003547
Iteration 123/1000 | Loss: 0.00003547
Iteration 124/1000 | Loss: 0.00003547
Iteration 125/1000 | Loss: 0.00003547
Iteration 126/1000 | Loss: 0.00003547
Iteration 127/1000 | Loss: 0.00003547
Iteration 128/1000 | Loss: 0.00003547
Iteration 129/1000 | Loss: 0.00003547
Iteration 130/1000 | Loss: 0.00003547
Iteration 131/1000 | Loss: 0.00003547
Iteration 132/1000 | Loss: 0.00003547
Iteration 133/1000 | Loss: 0.00003547
Iteration 134/1000 | Loss: 0.00003547
Iteration 135/1000 | Loss: 0.00003547
Iteration 136/1000 | Loss: 0.00003547
Iteration 137/1000 | Loss: 0.00003547
Iteration 138/1000 | Loss: 0.00003547
Iteration 139/1000 | Loss: 0.00003547
Iteration 140/1000 | Loss: 0.00003547
Iteration 141/1000 | Loss: 0.00003547
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [3.547087908373214e-05, 3.547087908373214e-05, 3.547087908373214e-05, 3.547087908373214e-05, 3.547087908373214e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.547087908373214e-05

Optimization complete. Final v2v error: 4.724328517913818 mm

Highest mean error: 21.016748428344727 mm for frame 155

Lowest mean error: 3.98866868019104 mm for frame 239

Saving results

Total time: 147.92202186584473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471851
Iteration 2/25 | Loss: 0.00131333
Iteration 3/25 | Loss: 0.00110529
Iteration 4/25 | Loss: 0.00107170
Iteration 5/25 | Loss: 0.00106558
Iteration 6/25 | Loss: 0.00106359
Iteration 7/25 | Loss: 0.00106355
Iteration 8/25 | Loss: 0.00106355
Iteration 9/25 | Loss: 0.00106355
Iteration 10/25 | Loss: 0.00106355
Iteration 11/25 | Loss: 0.00106355
Iteration 12/25 | Loss: 0.00106355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001063551171682775, 0.001063551171682775, 0.001063551171682775, 0.001063551171682775, 0.001063551171682775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001063551171682775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.93006247
Iteration 2/25 | Loss: 0.00095656
Iteration 3/25 | Loss: 0.00095656
Iteration 4/25 | Loss: 0.00095656
Iteration 5/25 | Loss: 0.00095656
Iteration 6/25 | Loss: 0.00095656
Iteration 7/25 | Loss: 0.00095656
Iteration 8/25 | Loss: 0.00095656
Iteration 9/25 | Loss: 0.00095656
Iteration 10/25 | Loss: 0.00095656
Iteration 11/25 | Loss: 0.00095656
Iteration 12/25 | Loss: 0.00095656
Iteration 13/25 | Loss: 0.00095656
Iteration 14/25 | Loss: 0.00095656
Iteration 15/25 | Loss: 0.00095656
Iteration 16/25 | Loss: 0.00095656
Iteration 17/25 | Loss: 0.00095656
Iteration 18/25 | Loss: 0.00095656
Iteration 19/25 | Loss: 0.00095656
Iteration 20/25 | Loss: 0.00095656
Iteration 21/25 | Loss: 0.00095656
Iteration 22/25 | Loss: 0.00095656
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009565572254359722, 0.0009565572254359722, 0.0009565572254359722, 0.0009565572254359722, 0.0009565572254359722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009565572254359722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095656
Iteration 2/1000 | Loss: 0.00004291
Iteration 3/1000 | Loss: 0.00002768
Iteration 4/1000 | Loss: 0.00002457
Iteration 5/1000 | Loss: 0.00002343
Iteration 6/1000 | Loss: 0.00002272
Iteration 7/1000 | Loss: 0.00002223
Iteration 8/1000 | Loss: 0.00002179
Iteration 9/1000 | Loss: 0.00002147
Iteration 10/1000 | Loss: 0.00002108
Iteration 11/1000 | Loss: 0.00002087
Iteration 12/1000 | Loss: 0.00002070
Iteration 13/1000 | Loss: 0.00002060
Iteration 14/1000 | Loss: 0.00002054
Iteration 15/1000 | Loss: 0.00002053
Iteration 16/1000 | Loss: 0.00002052
Iteration 17/1000 | Loss: 0.00002052
Iteration 18/1000 | Loss: 0.00002052
Iteration 19/1000 | Loss: 0.00002051
Iteration 20/1000 | Loss: 0.00002051
Iteration 21/1000 | Loss: 0.00002050
Iteration 22/1000 | Loss: 0.00002049
Iteration 23/1000 | Loss: 0.00002049
Iteration 24/1000 | Loss: 0.00002049
Iteration 25/1000 | Loss: 0.00002049
Iteration 26/1000 | Loss: 0.00002049
Iteration 27/1000 | Loss: 0.00002049
Iteration 28/1000 | Loss: 0.00002048
Iteration 29/1000 | Loss: 0.00002048
Iteration 30/1000 | Loss: 0.00002042
Iteration 31/1000 | Loss: 0.00002042
Iteration 32/1000 | Loss: 0.00002041
Iteration 33/1000 | Loss: 0.00002041
Iteration 34/1000 | Loss: 0.00002040
Iteration 35/1000 | Loss: 0.00002040
Iteration 36/1000 | Loss: 0.00002039
Iteration 37/1000 | Loss: 0.00002039
Iteration 38/1000 | Loss: 0.00002036
Iteration 39/1000 | Loss: 0.00002036
Iteration 40/1000 | Loss: 0.00002036
Iteration 41/1000 | Loss: 0.00002033
Iteration 42/1000 | Loss: 0.00002031
Iteration 43/1000 | Loss: 0.00002031
Iteration 44/1000 | Loss: 0.00002031
Iteration 45/1000 | Loss: 0.00002030
Iteration 46/1000 | Loss: 0.00002030
Iteration 47/1000 | Loss: 0.00002030
Iteration 48/1000 | Loss: 0.00002027
Iteration 49/1000 | Loss: 0.00002025
Iteration 50/1000 | Loss: 0.00002025
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002021
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002020
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002020
Iteration 60/1000 | Loss: 0.00002020
Iteration 61/1000 | Loss: 0.00002019
Iteration 62/1000 | Loss: 0.00002019
Iteration 63/1000 | Loss: 0.00002019
Iteration 64/1000 | Loss: 0.00002019
Iteration 65/1000 | Loss: 0.00002019
Iteration 66/1000 | Loss: 0.00002018
Iteration 67/1000 | Loss: 0.00002018
Iteration 68/1000 | Loss: 0.00002018
Iteration 69/1000 | Loss: 0.00002018
Iteration 70/1000 | Loss: 0.00002018
Iteration 71/1000 | Loss: 0.00002018
Iteration 72/1000 | Loss: 0.00002017
Iteration 73/1000 | Loss: 0.00002017
Iteration 74/1000 | Loss: 0.00002017
Iteration 75/1000 | Loss: 0.00002017
Iteration 76/1000 | Loss: 0.00002017
Iteration 77/1000 | Loss: 0.00002016
Iteration 78/1000 | Loss: 0.00002016
Iteration 79/1000 | Loss: 0.00002016
Iteration 80/1000 | Loss: 0.00002016
Iteration 81/1000 | Loss: 0.00002016
Iteration 82/1000 | Loss: 0.00002016
Iteration 83/1000 | Loss: 0.00002016
Iteration 84/1000 | Loss: 0.00002015
Iteration 85/1000 | Loss: 0.00002015
Iteration 86/1000 | Loss: 0.00002015
Iteration 87/1000 | Loss: 0.00002015
Iteration 88/1000 | Loss: 0.00002015
Iteration 89/1000 | Loss: 0.00002015
Iteration 90/1000 | Loss: 0.00002015
Iteration 91/1000 | Loss: 0.00002015
Iteration 92/1000 | Loss: 0.00002015
Iteration 93/1000 | Loss: 0.00002015
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002015
Iteration 96/1000 | Loss: 0.00002015
Iteration 97/1000 | Loss: 0.00002015
Iteration 98/1000 | Loss: 0.00002014
Iteration 99/1000 | Loss: 0.00002014
Iteration 100/1000 | Loss: 0.00002014
Iteration 101/1000 | Loss: 0.00002014
Iteration 102/1000 | Loss: 0.00002014
Iteration 103/1000 | Loss: 0.00002014
Iteration 104/1000 | Loss: 0.00002013
Iteration 105/1000 | Loss: 0.00002013
Iteration 106/1000 | Loss: 0.00002013
Iteration 107/1000 | Loss: 0.00002013
Iteration 108/1000 | Loss: 0.00002013
Iteration 109/1000 | Loss: 0.00002012
Iteration 110/1000 | Loss: 0.00002012
Iteration 111/1000 | Loss: 0.00002012
Iteration 112/1000 | Loss: 0.00002012
Iteration 113/1000 | Loss: 0.00002012
Iteration 114/1000 | Loss: 0.00002012
Iteration 115/1000 | Loss: 0.00002011
Iteration 116/1000 | Loss: 0.00002011
Iteration 117/1000 | Loss: 0.00002011
Iteration 118/1000 | Loss: 0.00002010
Iteration 119/1000 | Loss: 0.00002010
Iteration 120/1000 | Loss: 0.00002010
Iteration 121/1000 | Loss: 0.00002010
Iteration 122/1000 | Loss: 0.00002010
Iteration 123/1000 | Loss: 0.00002010
Iteration 124/1000 | Loss: 0.00002010
Iteration 125/1000 | Loss: 0.00002010
Iteration 126/1000 | Loss: 0.00002009
Iteration 127/1000 | Loss: 0.00002009
Iteration 128/1000 | Loss: 0.00002009
Iteration 129/1000 | Loss: 0.00002009
Iteration 130/1000 | Loss: 0.00002009
Iteration 131/1000 | Loss: 0.00002009
Iteration 132/1000 | Loss: 0.00002009
Iteration 133/1000 | Loss: 0.00002009
Iteration 134/1000 | Loss: 0.00002009
Iteration 135/1000 | Loss: 0.00002009
Iteration 136/1000 | Loss: 0.00002009
Iteration 137/1000 | Loss: 0.00002008
Iteration 138/1000 | Loss: 0.00002008
Iteration 139/1000 | Loss: 0.00002008
Iteration 140/1000 | Loss: 0.00002008
Iteration 141/1000 | Loss: 0.00002007
Iteration 142/1000 | Loss: 0.00002007
Iteration 143/1000 | Loss: 0.00002007
Iteration 144/1000 | Loss: 0.00002007
Iteration 145/1000 | Loss: 0.00002007
Iteration 146/1000 | Loss: 0.00002007
Iteration 147/1000 | Loss: 0.00002007
Iteration 148/1000 | Loss: 0.00002007
Iteration 149/1000 | Loss: 0.00002007
Iteration 150/1000 | Loss: 0.00002007
Iteration 151/1000 | Loss: 0.00002007
Iteration 152/1000 | Loss: 0.00002007
Iteration 153/1000 | Loss: 0.00002007
Iteration 154/1000 | Loss: 0.00002006
Iteration 155/1000 | Loss: 0.00002006
Iteration 156/1000 | Loss: 0.00002006
Iteration 157/1000 | Loss: 0.00002005
Iteration 158/1000 | Loss: 0.00002005
Iteration 159/1000 | Loss: 0.00002005
Iteration 160/1000 | Loss: 0.00002005
Iteration 161/1000 | Loss: 0.00002005
Iteration 162/1000 | Loss: 0.00002004
Iteration 163/1000 | Loss: 0.00002004
Iteration 164/1000 | Loss: 0.00002004
Iteration 165/1000 | Loss: 0.00002004
Iteration 166/1000 | Loss: 0.00002004
Iteration 167/1000 | Loss: 0.00002003
Iteration 168/1000 | Loss: 0.00002003
Iteration 169/1000 | Loss: 0.00002003
Iteration 170/1000 | Loss: 0.00002003
Iteration 171/1000 | Loss: 0.00002003
Iteration 172/1000 | Loss: 0.00002003
Iteration 173/1000 | Loss: 0.00002003
Iteration 174/1000 | Loss: 0.00002003
Iteration 175/1000 | Loss: 0.00002003
Iteration 176/1000 | Loss: 0.00002003
Iteration 177/1000 | Loss: 0.00002002
Iteration 178/1000 | Loss: 0.00002002
Iteration 179/1000 | Loss: 0.00002002
Iteration 180/1000 | Loss: 0.00002002
Iteration 181/1000 | Loss: 0.00002002
Iteration 182/1000 | Loss: 0.00002002
Iteration 183/1000 | Loss: 0.00002002
Iteration 184/1000 | Loss: 0.00002001
Iteration 185/1000 | Loss: 0.00002001
Iteration 186/1000 | Loss: 0.00002001
Iteration 187/1000 | Loss: 0.00002001
Iteration 188/1000 | Loss: 0.00002001
Iteration 189/1000 | Loss: 0.00002001
Iteration 190/1000 | Loss: 0.00002001
Iteration 191/1000 | Loss: 0.00002000
Iteration 192/1000 | Loss: 0.00002000
Iteration 193/1000 | Loss: 0.00002000
Iteration 194/1000 | Loss: 0.00002000
Iteration 195/1000 | Loss: 0.00002000
Iteration 196/1000 | Loss: 0.00002000
Iteration 197/1000 | Loss: 0.00002000
Iteration 198/1000 | Loss: 0.00001999
Iteration 199/1000 | Loss: 0.00001999
Iteration 200/1000 | Loss: 0.00001999
Iteration 201/1000 | Loss: 0.00001999
Iteration 202/1000 | Loss: 0.00001999
Iteration 203/1000 | Loss: 0.00001998
Iteration 204/1000 | Loss: 0.00001998
Iteration 205/1000 | Loss: 0.00001998
Iteration 206/1000 | Loss: 0.00001998
Iteration 207/1000 | Loss: 0.00001998
Iteration 208/1000 | Loss: 0.00001998
Iteration 209/1000 | Loss: 0.00001997
Iteration 210/1000 | Loss: 0.00001997
Iteration 211/1000 | Loss: 0.00001997
Iteration 212/1000 | Loss: 0.00001997
Iteration 213/1000 | Loss: 0.00001997
Iteration 214/1000 | Loss: 0.00001997
Iteration 215/1000 | Loss: 0.00001997
Iteration 216/1000 | Loss: 0.00001997
Iteration 217/1000 | Loss: 0.00001997
Iteration 218/1000 | Loss: 0.00001997
Iteration 219/1000 | Loss: 0.00001997
Iteration 220/1000 | Loss: 0.00001997
Iteration 221/1000 | Loss: 0.00001996
Iteration 222/1000 | Loss: 0.00001996
Iteration 223/1000 | Loss: 0.00001996
Iteration 224/1000 | Loss: 0.00001996
Iteration 225/1000 | Loss: 0.00001996
Iteration 226/1000 | Loss: 0.00001996
Iteration 227/1000 | Loss: 0.00001996
Iteration 228/1000 | Loss: 0.00001996
Iteration 229/1000 | Loss: 0.00001996
Iteration 230/1000 | Loss: 0.00001996
Iteration 231/1000 | Loss: 0.00001996
Iteration 232/1000 | Loss: 0.00001996
Iteration 233/1000 | Loss: 0.00001996
Iteration 234/1000 | Loss: 0.00001996
Iteration 235/1000 | Loss: 0.00001996
Iteration 236/1000 | Loss: 0.00001996
Iteration 237/1000 | Loss: 0.00001996
Iteration 238/1000 | Loss: 0.00001996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.9961469661211595e-05, 1.9961469661211595e-05, 1.9961469661211595e-05, 1.9961469661211595e-05, 1.9961469661211595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9961469661211595e-05

Optimization complete. Final v2v error: 3.8978941440582275 mm

Highest mean error: 4.125600814819336 mm for frame 180

Lowest mean error: 3.7046337127685547 mm for frame 199

Saving results

Total time: 53.92083120346069
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849516
Iteration 2/25 | Loss: 0.00125363
Iteration 3/25 | Loss: 0.00105221
Iteration 4/25 | Loss: 0.00102197
Iteration 5/25 | Loss: 0.00100627
Iteration 6/25 | Loss: 0.00100374
Iteration 7/25 | Loss: 0.00100716
Iteration 8/25 | Loss: 0.00100450
Iteration 9/25 | Loss: 0.00100171
Iteration 10/25 | Loss: 0.00100046
Iteration 11/25 | Loss: 0.00100004
Iteration 12/25 | Loss: 0.00099985
Iteration 13/25 | Loss: 0.00099975
Iteration 14/25 | Loss: 0.00099973
Iteration 15/25 | Loss: 0.00099973
Iteration 16/25 | Loss: 0.00099973
Iteration 17/25 | Loss: 0.00099973
Iteration 18/25 | Loss: 0.00099972
Iteration 19/25 | Loss: 0.00099972
Iteration 20/25 | Loss: 0.00099972
Iteration 21/25 | Loss: 0.00099972
Iteration 22/25 | Loss: 0.00099972
Iteration 23/25 | Loss: 0.00099972
Iteration 24/25 | Loss: 0.00099972
Iteration 25/25 | Loss: 0.00099972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.58529329
Iteration 2/25 | Loss: 0.00089279
Iteration 3/25 | Loss: 0.00089278
Iteration 4/25 | Loss: 0.00089278
Iteration 5/25 | Loss: 0.00089278
Iteration 6/25 | Loss: 0.00089278
Iteration 7/25 | Loss: 0.00089278
Iteration 8/25 | Loss: 0.00089278
Iteration 9/25 | Loss: 0.00089278
Iteration 10/25 | Loss: 0.00089278
Iteration 11/25 | Loss: 0.00089278
Iteration 12/25 | Loss: 0.00089278
Iteration 13/25 | Loss: 0.00089278
Iteration 14/25 | Loss: 0.00089278
Iteration 15/25 | Loss: 0.00089278
Iteration 16/25 | Loss: 0.00089278
Iteration 17/25 | Loss: 0.00089278
Iteration 18/25 | Loss: 0.00089278
Iteration 19/25 | Loss: 0.00089278
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.000892776355613023, 0.000892776355613023, 0.000892776355613023, 0.000892776355613023, 0.000892776355613023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000892776355613023

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089278
Iteration 2/1000 | Loss: 0.00002906
Iteration 3/1000 | Loss: 0.00002046
Iteration 4/1000 | Loss: 0.00001849
Iteration 5/1000 | Loss: 0.00001774
Iteration 6/1000 | Loss: 0.00001721
Iteration 7/1000 | Loss: 0.00001694
Iteration 8/1000 | Loss: 0.00001669
Iteration 9/1000 | Loss: 0.00001661
Iteration 10/1000 | Loss: 0.00001656
Iteration 11/1000 | Loss: 0.00001655
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001654
Iteration 14/1000 | Loss: 0.00001654
Iteration 15/1000 | Loss: 0.00001654
Iteration 16/1000 | Loss: 0.00001654
Iteration 17/1000 | Loss: 0.00001653
Iteration 18/1000 | Loss: 0.00001653
Iteration 19/1000 | Loss: 0.00001651
Iteration 20/1000 | Loss: 0.00001650
Iteration 21/1000 | Loss: 0.00001649
Iteration 22/1000 | Loss: 0.00001648
Iteration 23/1000 | Loss: 0.00001648
Iteration 24/1000 | Loss: 0.00001648
Iteration 25/1000 | Loss: 0.00001647
Iteration 26/1000 | Loss: 0.00001645
Iteration 27/1000 | Loss: 0.00001645
Iteration 28/1000 | Loss: 0.00001644
Iteration 29/1000 | Loss: 0.00001644
Iteration 30/1000 | Loss: 0.00001644
Iteration 31/1000 | Loss: 0.00001643
Iteration 32/1000 | Loss: 0.00001643
Iteration 33/1000 | Loss: 0.00001642
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001642
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001641
Iteration 38/1000 | Loss: 0.00001641
Iteration 39/1000 | Loss: 0.00001641
Iteration 40/1000 | Loss: 0.00001641
Iteration 41/1000 | Loss: 0.00001641
Iteration 42/1000 | Loss: 0.00001641
Iteration 43/1000 | Loss: 0.00001641
Iteration 44/1000 | Loss: 0.00001641
Iteration 45/1000 | Loss: 0.00001640
Iteration 46/1000 | Loss: 0.00001640
Iteration 47/1000 | Loss: 0.00001640
Iteration 48/1000 | Loss: 0.00001640
Iteration 49/1000 | Loss: 0.00001639
Iteration 50/1000 | Loss: 0.00001639
Iteration 51/1000 | Loss: 0.00001638
Iteration 52/1000 | Loss: 0.00001638
Iteration 53/1000 | Loss: 0.00001638
Iteration 54/1000 | Loss: 0.00001638
Iteration 55/1000 | Loss: 0.00001638
Iteration 56/1000 | Loss: 0.00001638
Iteration 57/1000 | Loss: 0.00001638
Iteration 58/1000 | Loss: 0.00001638
Iteration 59/1000 | Loss: 0.00001638
Iteration 60/1000 | Loss: 0.00001638
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001637
Iteration 64/1000 | Loss: 0.00001637
Iteration 65/1000 | Loss: 0.00001636
Iteration 66/1000 | Loss: 0.00001636
Iteration 67/1000 | Loss: 0.00001636
Iteration 68/1000 | Loss: 0.00001636
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001636
Iteration 71/1000 | Loss: 0.00001636
Iteration 72/1000 | Loss: 0.00001635
Iteration 73/1000 | Loss: 0.00001635
Iteration 74/1000 | Loss: 0.00001635
Iteration 75/1000 | Loss: 0.00001635
Iteration 76/1000 | Loss: 0.00001635
Iteration 77/1000 | Loss: 0.00001635
Iteration 78/1000 | Loss: 0.00001635
Iteration 79/1000 | Loss: 0.00001635
Iteration 80/1000 | Loss: 0.00001635
Iteration 81/1000 | Loss: 0.00001635
Iteration 82/1000 | Loss: 0.00001634
Iteration 83/1000 | Loss: 0.00001634
Iteration 84/1000 | Loss: 0.00001634
Iteration 85/1000 | Loss: 0.00001634
Iteration 86/1000 | Loss: 0.00001634
Iteration 87/1000 | Loss: 0.00001634
Iteration 88/1000 | Loss: 0.00001633
Iteration 89/1000 | Loss: 0.00001633
Iteration 90/1000 | Loss: 0.00001633
Iteration 91/1000 | Loss: 0.00001633
Iteration 92/1000 | Loss: 0.00001633
Iteration 93/1000 | Loss: 0.00001633
Iteration 94/1000 | Loss: 0.00001633
Iteration 95/1000 | Loss: 0.00001633
Iteration 96/1000 | Loss: 0.00001632
Iteration 97/1000 | Loss: 0.00001632
Iteration 98/1000 | Loss: 0.00001632
Iteration 99/1000 | Loss: 0.00001632
Iteration 100/1000 | Loss: 0.00001632
Iteration 101/1000 | Loss: 0.00001632
Iteration 102/1000 | Loss: 0.00001632
Iteration 103/1000 | Loss: 0.00001632
Iteration 104/1000 | Loss: 0.00001632
Iteration 105/1000 | Loss: 0.00001632
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001631
Iteration 111/1000 | Loss: 0.00001630
Iteration 112/1000 | Loss: 0.00001630
Iteration 113/1000 | Loss: 0.00001630
Iteration 114/1000 | Loss: 0.00001630
Iteration 115/1000 | Loss: 0.00001630
Iteration 116/1000 | Loss: 0.00001629
Iteration 117/1000 | Loss: 0.00001629
Iteration 118/1000 | Loss: 0.00001629
Iteration 119/1000 | Loss: 0.00001629
Iteration 120/1000 | Loss: 0.00001629
Iteration 121/1000 | Loss: 0.00001629
Iteration 122/1000 | Loss: 0.00001629
Iteration 123/1000 | Loss: 0.00001629
Iteration 124/1000 | Loss: 0.00001629
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.6294830857077613e-05, 1.6294830857077613e-05, 1.6294830857077613e-05, 1.6294830857077613e-05, 1.6294830857077613e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6294830857077613e-05

Optimization complete. Final v2v error: 3.45434308052063 mm

Highest mean error: 3.8183071613311768 mm for frame 187

Lowest mean error: 3.067458152770996 mm for frame 162

Saving results

Total time: 48.02068734169006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00682944
Iteration 2/25 | Loss: 0.00158330
Iteration 3/25 | Loss: 0.00119589
Iteration 4/25 | Loss: 0.00116256
Iteration 5/25 | Loss: 0.00115400
Iteration 6/25 | Loss: 0.00115161
Iteration 7/25 | Loss: 0.00115149
Iteration 8/25 | Loss: 0.00115149
Iteration 9/25 | Loss: 0.00115149
Iteration 10/25 | Loss: 0.00115149
Iteration 11/25 | Loss: 0.00115149
Iteration 12/25 | Loss: 0.00115149
Iteration 13/25 | Loss: 0.00115149
Iteration 14/25 | Loss: 0.00115149
Iteration 15/25 | Loss: 0.00115149
Iteration 16/25 | Loss: 0.00115149
Iteration 17/25 | Loss: 0.00115149
Iteration 18/25 | Loss: 0.00115149
Iteration 19/25 | Loss: 0.00115149
Iteration 20/25 | Loss: 0.00115149
Iteration 21/25 | Loss: 0.00115149
Iteration 22/25 | Loss: 0.00115149
Iteration 23/25 | Loss: 0.00115149
Iteration 24/25 | Loss: 0.00115149
Iteration 25/25 | Loss: 0.00115149

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52251303
Iteration 2/25 | Loss: 0.00123204
Iteration 3/25 | Loss: 0.00123201
Iteration 4/25 | Loss: 0.00123200
Iteration 5/25 | Loss: 0.00123200
Iteration 6/25 | Loss: 0.00123200
Iteration 7/25 | Loss: 0.00123200
Iteration 8/25 | Loss: 0.00123200
Iteration 9/25 | Loss: 0.00123200
Iteration 10/25 | Loss: 0.00123200
Iteration 11/25 | Loss: 0.00123200
Iteration 12/25 | Loss: 0.00123200
Iteration 13/25 | Loss: 0.00123200
Iteration 14/25 | Loss: 0.00123200
Iteration 15/25 | Loss: 0.00123200
Iteration 16/25 | Loss: 0.00123200
Iteration 17/25 | Loss: 0.00123200
Iteration 18/25 | Loss: 0.00123200
Iteration 19/25 | Loss: 0.00123200
Iteration 20/25 | Loss: 0.00123200
Iteration 21/25 | Loss: 0.00123200
Iteration 22/25 | Loss: 0.00123200
Iteration 23/25 | Loss: 0.00123200
Iteration 24/25 | Loss: 0.00123200
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012320027453824878, 0.0012320027453824878, 0.0012320027453824878, 0.0012320027453824878, 0.0012320027453824878]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012320027453824878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123200
Iteration 2/1000 | Loss: 0.00004494
Iteration 3/1000 | Loss: 0.00003605
Iteration 4/1000 | Loss: 0.00003273
Iteration 5/1000 | Loss: 0.00003160
Iteration 6/1000 | Loss: 0.00003110
Iteration 7/1000 | Loss: 0.00003063
Iteration 8/1000 | Loss: 0.00003006
Iteration 9/1000 | Loss: 0.00002962
Iteration 10/1000 | Loss: 0.00002936
Iteration 11/1000 | Loss: 0.00002921
Iteration 12/1000 | Loss: 0.00002906
Iteration 13/1000 | Loss: 0.00002905
Iteration 14/1000 | Loss: 0.00002902
Iteration 15/1000 | Loss: 0.00002902
Iteration 16/1000 | Loss: 0.00002901
Iteration 17/1000 | Loss: 0.00002900
Iteration 18/1000 | Loss: 0.00002899
Iteration 19/1000 | Loss: 0.00002899
Iteration 20/1000 | Loss: 0.00002898
Iteration 21/1000 | Loss: 0.00002896
Iteration 22/1000 | Loss: 0.00002895
Iteration 23/1000 | Loss: 0.00002894
Iteration 24/1000 | Loss: 0.00002894
Iteration 25/1000 | Loss: 0.00002894
Iteration 26/1000 | Loss: 0.00002894
Iteration 27/1000 | Loss: 0.00002894
Iteration 28/1000 | Loss: 0.00002894
Iteration 29/1000 | Loss: 0.00002894
Iteration 30/1000 | Loss: 0.00002894
Iteration 31/1000 | Loss: 0.00002894
Iteration 32/1000 | Loss: 0.00002893
Iteration 33/1000 | Loss: 0.00002893
Iteration 34/1000 | Loss: 0.00002893
Iteration 35/1000 | Loss: 0.00002892
Iteration 36/1000 | Loss: 0.00002892
Iteration 37/1000 | Loss: 0.00002891
Iteration 38/1000 | Loss: 0.00002891
Iteration 39/1000 | Loss: 0.00002890
Iteration 40/1000 | Loss: 0.00002890
Iteration 41/1000 | Loss: 0.00002890
Iteration 42/1000 | Loss: 0.00002890
Iteration 43/1000 | Loss: 0.00002890
Iteration 44/1000 | Loss: 0.00002890
Iteration 45/1000 | Loss: 0.00002888
Iteration 46/1000 | Loss: 0.00002888
Iteration 47/1000 | Loss: 0.00002887
Iteration 48/1000 | Loss: 0.00002887
Iteration 49/1000 | Loss: 0.00002886
Iteration 50/1000 | Loss: 0.00002884
Iteration 51/1000 | Loss: 0.00002884
Iteration 52/1000 | Loss: 0.00002884
Iteration 53/1000 | Loss: 0.00002884
Iteration 54/1000 | Loss: 0.00002881
Iteration 55/1000 | Loss: 0.00002881
Iteration 56/1000 | Loss: 0.00002880
Iteration 57/1000 | Loss: 0.00002880
Iteration 58/1000 | Loss: 0.00002879
Iteration 59/1000 | Loss: 0.00002879
Iteration 60/1000 | Loss: 0.00002879
Iteration 61/1000 | Loss: 0.00002878
Iteration 62/1000 | Loss: 0.00002878
Iteration 63/1000 | Loss: 0.00002878
Iteration 64/1000 | Loss: 0.00002877
Iteration 65/1000 | Loss: 0.00002876
Iteration 66/1000 | Loss: 0.00002875
Iteration 67/1000 | Loss: 0.00002875
Iteration 68/1000 | Loss: 0.00002874
Iteration 69/1000 | Loss: 0.00002873
Iteration 70/1000 | Loss: 0.00002873
Iteration 71/1000 | Loss: 0.00002872
Iteration 72/1000 | Loss: 0.00002871
Iteration 73/1000 | Loss: 0.00002871
Iteration 74/1000 | Loss: 0.00002870
Iteration 75/1000 | Loss: 0.00002870
Iteration 76/1000 | Loss: 0.00002869
Iteration 77/1000 | Loss: 0.00002869
Iteration 78/1000 | Loss: 0.00002869
Iteration 79/1000 | Loss: 0.00002869
Iteration 80/1000 | Loss: 0.00002868
Iteration 81/1000 | Loss: 0.00002868
Iteration 82/1000 | Loss: 0.00002868
Iteration 83/1000 | Loss: 0.00002868
Iteration 84/1000 | Loss: 0.00002867
Iteration 85/1000 | Loss: 0.00002867
Iteration 86/1000 | Loss: 0.00002867
Iteration 87/1000 | Loss: 0.00002866
Iteration 88/1000 | Loss: 0.00002866
Iteration 89/1000 | Loss: 0.00002866
Iteration 90/1000 | Loss: 0.00002866
Iteration 91/1000 | Loss: 0.00002866
Iteration 92/1000 | Loss: 0.00002865
Iteration 93/1000 | Loss: 0.00002865
Iteration 94/1000 | Loss: 0.00002865
Iteration 95/1000 | Loss: 0.00002865
Iteration 96/1000 | Loss: 0.00002865
Iteration 97/1000 | Loss: 0.00002864
Iteration 98/1000 | Loss: 0.00002864
Iteration 99/1000 | Loss: 0.00002864
Iteration 100/1000 | Loss: 0.00002864
Iteration 101/1000 | Loss: 0.00002864
Iteration 102/1000 | Loss: 0.00002864
Iteration 103/1000 | Loss: 0.00002864
Iteration 104/1000 | Loss: 0.00002864
Iteration 105/1000 | Loss: 0.00002864
Iteration 106/1000 | Loss: 0.00002864
Iteration 107/1000 | Loss: 0.00002864
Iteration 108/1000 | Loss: 0.00002864
Iteration 109/1000 | Loss: 0.00002864
Iteration 110/1000 | Loss: 0.00002863
Iteration 111/1000 | Loss: 0.00002863
Iteration 112/1000 | Loss: 0.00002863
Iteration 113/1000 | Loss: 0.00002863
Iteration 114/1000 | Loss: 0.00002863
Iteration 115/1000 | Loss: 0.00002862
Iteration 116/1000 | Loss: 0.00002862
Iteration 117/1000 | Loss: 0.00002862
Iteration 118/1000 | Loss: 0.00002861
Iteration 119/1000 | Loss: 0.00002861
Iteration 120/1000 | Loss: 0.00002861
Iteration 121/1000 | Loss: 0.00002861
Iteration 122/1000 | Loss: 0.00002860
Iteration 123/1000 | Loss: 0.00002860
Iteration 124/1000 | Loss: 0.00002860
Iteration 125/1000 | Loss: 0.00002859
Iteration 126/1000 | Loss: 0.00002859
Iteration 127/1000 | Loss: 0.00002858
Iteration 128/1000 | Loss: 0.00002858
Iteration 129/1000 | Loss: 0.00002857
Iteration 130/1000 | Loss: 0.00002857
Iteration 131/1000 | Loss: 0.00002857
Iteration 132/1000 | Loss: 0.00002857
Iteration 133/1000 | Loss: 0.00002856
Iteration 134/1000 | Loss: 0.00002856
Iteration 135/1000 | Loss: 0.00002856
Iteration 136/1000 | Loss: 0.00002856
Iteration 137/1000 | Loss: 0.00002856
Iteration 138/1000 | Loss: 0.00002856
Iteration 139/1000 | Loss: 0.00002856
Iteration 140/1000 | Loss: 0.00002856
Iteration 141/1000 | Loss: 0.00002856
Iteration 142/1000 | Loss: 0.00002855
Iteration 143/1000 | Loss: 0.00002855
Iteration 144/1000 | Loss: 0.00002855
Iteration 145/1000 | Loss: 0.00002855
Iteration 146/1000 | Loss: 0.00002855
Iteration 147/1000 | Loss: 0.00002855
Iteration 148/1000 | Loss: 0.00002855
Iteration 149/1000 | Loss: 0.00002855
Iteration 150/1000 | Loss: 0.00002855
Iteration 151/1000 | Loss: 0.00002855
Iteration 152/1000 | Loss: 0.00002854
Iteration 153/1000 | Loss: 0.00002854
Iteration 154/1000 | Loss: 0.00002854
Iteration 155/1000 | Loss: 0.00002854
Iteration 156/1000 | Loss: 0.00002854
Iteration 157/1000 | Loss: 0.00002854
Iteration 158/1000 | Loss: 0.00002854
Iteration 159/1000 | Loss: 0.00002854
Iteration 160/1000 | Loss: 0.00002854
Iteration 161/1000 | Loss: 0.00002854
Iteration 162/1000 | Loss: 0.00002854
Iteration 163/1000 | Loss: 0.00002854
Iteration 164/1000 | Loss: 0.00002854
Iteration 165/1000 | Loss: 0.00002854
Iteration 166/1000 | Loss: 0.00002854
Iteration 167/1000 | Loss: 0.00002854
Iteration 168/1000 | Loss: 0.00002854
Iteration 169/1000 | Loss: 0.00002854
Iteration 170/1000 | Loss: 0.00002854
Iteration 171/1000 | Loss: 0.00002854
Iteration 172/1000 | Loss: 0.00002854
Iteration 173/1000 | Loss: 0.00002854
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [2.8544052838697098e-05, 2.8544052838697098e-05, 2.8544052838697098e-05, 2.8544052838697098e-05, 2.8544052838697098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8544052838697098e-05

Optimization complete. Final v2v error: 4.585912704467773 mm

Highest mean error: 5.145543575286865 mm for frame 189

Lowest mean error: 4.0415472984313965 mm for frame 109

Saving results

Total time: 46.160531997680664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01099262
Iteration 2/25 | Loss: 0.00306720
Iteration 3/25 | Loss: 0.00212960
Iteration 4/25 | Loss: 0.00161153
Iteration 5/25 | Loss: 0.00165419
Iteration 6/25 | Loss: 0.00152020
Iteration 7/25 | Loss: 0.00134754
Iteration 8/25 | Loss: 0.00126012
Iteration 9/25 | Loss: 0.00120738
Iteration 10/25 | Loss: 0.00118943
Iteration 11/25 | Loss: 0.00117808
Iteration 12/25 | Loss: 0.00116477
Iteration 13/25 | Loss: 0.00116635
Iteration 14/25 | Loss: 0.00116313
Iteration 15/25 | Loss: 0.00114813
Iteration 16/25 | Loss: 0.00114363
Iteration 17/25 | Loss: 0.00114173
Iteration 18/25 | Loss: 0.00114424
Iteration 19/25 | Loss: 0.00114184
Iteration 20/25 | Loss: 0.00113620
Iteration 21/25 | Loss: 0.00113468
Iteration 22/25 | Loss: 0.00113747
Iteration 23/25 | Loss: 0.00113232
Iteration 24/25 | Loss: 0.00113110
Iteration 25/25 | Loss: 0.00113419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45311987
Iteration 2/25 | Loss: 0.00412362
Iteration 3/25 | Loss: 0.00171866
Iteration 4/25 | Loss: 0.00171853
Iteration 5/25 | Loss: 0.00171853
Iteration 6/25 | Loss: 0.00171853
Iteration 7/25 | Loss: 0.00171853
Iteration 8/25 | Loss: 0.00171853
Iteration 9/25 | Loss: 0.00171853
Iteration 10/25 | Loss: 0.00171853
Iteration 11/25 | Loss: 0.00171853
Iteration 12/25 | Loss: 0.00171853
Iteration 13/25 | Loss: 0.00171853
Iteration 14/25 | Loss: 0.00171853
Iteration 15/25 | Loss: 0.00171853
Iteration 16/25 | Loss: 0.00171853
Iteration 17/25 | Loss: 0.00171853
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001718526124022901, 0.001718526124022901, 0.001718526124022901, 0.001718526124022901, 0.001718526124022901]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001718526124022901

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00171853
Iteration 2/1000 | Loss: 0.00176337
Iteration 3/1000 | Loss: 0.00012225
Iteration 4/1000 | Loss: 0.00012579
Iteration 5/1000 | Loss: 0.00009056
Iteration 6/1000 | Loss: 0.00007486
Iteration 7/1000 | Loss: 0.00070923
Iteration 8/1000 | Loss: 0.00039096
Iteration 9/1000 | Loss: 0.00042415
Iteration 10/1000 | Loss: 0.00045900
Iteration 11/1000 | Loss: 0.00469021
Iteration 12/1000 | Loss: 0.00654450
Iteration 13/1000 | Loss: 0.00070297
Iteration 14/1000 | Loss: 0.00023962
Iteration 15/1000 | Loss: 0.00018885
Iteration 16/1000 | Loss: 0.00013121
Iteration 17/1000 | Loss: 0.00010819
Iteration 18/1000 | Loss: 0.00008651
Iteration 19/1000 | Loss: 0.00056347
Iteration 20/1000 | Loss: 0.00046488
Iteration 21/1000 | Loss: 0.00034315
Iteration 22/1000 | Loss: 0.00071744
Iteration 23/1000 | Loss: 0.00062741
Iteration 24/1000 | Loss: 0.00034593
Iteration 25/1000 | Loss: 0.00074096
Iteration 26/1000 | Loss: 0.00074430
Iteration 27/1000 | Loss: 0.00006885
Iteration 28/1000 | Loss: 0.00012807
Iteration 29/1000 | Loss: 0.00027842
Iteration 30/1000 | Loss: 0.00129392
Iteration 31/1000 | Loss: 0.00030648
Iteration 32/1000 | Loss: 0.00005464
Iteration 33/1000 | Loss: 0.00020708
Iteration 34/1000 | Loss: 0.00022542
Iteration 35/1000 | Loss: 0.00032559
Iteration 36/1000 | Loss: 0.00004781
Iteration 37/1000 | Loss: 0.00034390
Iteration 38/1000 | Loss: 0.00046099
Iteration 39/1000 | Loss: 0.00037206
Iteration 40/1000 | Loss: 0.00056801
Iteration 41/1000 | Loss: 0.00004941
Iteration 42/1000 | Loss: 0.00004263
Iteration 43/1000 | Loss: 0.00004084
Iteration 44/1000 | Loss: 0.00003924
Iteration 45/1000 | Loss: 0.00003778
Iteration 46/1000 | Loss: 0.00003682
Iteration 47/1000 | Loss: 0.00003634
Iteration 48/1000 | Loss: 0.00003594
Iteration 49/1000 | Loss: 0.00003571
Iteration 50/1000 | Loss: 0.00003568
Iteration 51/1000 | Loss: 0.00003565
Iteration 52/1000 | Loss: 0.00003565
Iteration 53/1000 | Loss: 0.00003564
Iteration 54/1000 | Loss: 0.00003564
Iteration 55/1000 | Loss: 0.00003564
Iteration 56/1000 | Loss: 0.00003560
Iteration 57/1000 | Loss: 0.00003559
Iteration 58/1000 | Loss: 0.00003559
Iteration 59/1000 | Loss: 0.00003553
Iteration 60/1000 | Loss: 0.00003551
Iteration 61/1000 | Loss: 0.00003551
Iteration 62/1000 | Loss: 0.00003551
Iteration 63/1000 | Loss: 0.00003551
Iteration 64/1000 | Loss: 0.00003550
Iteration 65/1000 | Loss: 0.00003550
Iteration 66/1000 | Loss: 0.00003550
Iteration 67/1000 | Loss: 0.00003550
Iteration 68/1000 | Loss: 0.00003550
Iteration 69/1000 | Loss: 0.00003550
Iteration 70/1000 | Loss: 0.00003550
Iteration 71/1000 | Loss: 0.00003550
Iteration 72/1000 | Loss: 0.00003550
Iteration 73/1000 | Loss: 0.00003550
Iteration 74/1000 | Loss: 0.00003550
Iteration 75/1000 | Loss: 0.00003549
Iteration 76/1000 | Loss: 0.00003549
Iteration 77/1000 | Loss: 0.00003549
Iteration 78/1000 | Loss: 0.00003549
Iteration 79/1000 | Loss: 0.00003549
Iteration 80/1000 | Loss: 0.00003549
Iteration 81/1000 | Loss: 0.00003549
Iteration 82/1000 | Loss: 0.00003548
Iteration 83/1000 | Loss: 0.00003548
Iteration 84/1000 | Loss: 0.00003548
Iteration 85/1000 | Loss: 0.00003540
Iteration 86/1000 | Loss: 0.00003540
Iteration 87/1000 | Loss: 0.00003540
Iteration 88/1000 | Loss: 0.00003538
Iteration 89/1000 | Loss: 0.00003537
Iteration 90/1000 | Loss: 0.00003537
Iteration 91/1000 | Loss: 0.00003537
Iteration 92/1000 | Loss: 0.00003537
Iteration 93/1000 | Loss: 0.00003536
Iteration 94/1000 | Loss: 0.00003536
Iteration 95/1000 | Loss: 0.00003536
Iteration 96/1000 | Loss: 0.00003536
Iteration 97/1000 | Loss: 0.00003536
Iteration 98/1000 | Loss: 0.00003535
Iteration 99/1000 | Loss: 0.00003535
Iteration 100/1000 | Loss: 0.00003535
Iteration 101/1000 | Loss: 0.00003534
Iteration 102/1000 | Loss: 0.00003534
Iteration 103/1000 | Loss: 0.00003534
Iteration 104/1000 | Loss: 0.00003534
Iteration 105/1000 | Loss: 0.00003533
Iteration 106/1000 | Loss: 0.00003533
Iteration 107/1000 | Loss: 0.00003533
Iteration 108/1000 | Loss: 0.00003532
Iteration 109/1000 | Loss: 0.00003532
Iteration 110/1000 | Loss: 0.00003532
Iteration 111/1000 | Loss: 0.00003532
Iteration 112/1000 | Loss: 0.00003531
Iteration 113/1000 | Loss: 0.00003531
Iteration 114/1000 | Loss: 0.00003531
Iteration 115/1000 | Loss: 0.00003530
Iteration 116/1000 | Loss: 0.00003530
Iteration 117/1000 | Loss: 0.00003530
Iteration 118/1000 | Loss: 0.00003530
Iteration 119/1000 | Loss: 0.00003530
Iteration 120/1000 | Loss: 0.00003529
Iteration 121/1000 | Loss: 0.00003529
Iteration 122/1000 | Loss: 0.00003529
Iteration 123/1000 | Loss: 0.00003529
Iteration 124/1000 | Loss: 0.00003529
Iteration 125/1000 | Loss: 0.00003528
Iteration 126/1000 | Loss: 0.00003528
Iteration 127/1000 | Loss: 0.00003528
Iteration 128/1000 | Loss: 0.00003528
Iteration 129/1000 | Loss: 0.00003528
Iteration 130/1000 | Loss: 0.00003528
Iteration 131/1000 | Loss: 0.00003528
Iteration 132/1000 | Loss: 0.00003528
Iteration 133/1000 | Loss: 0.00003527
Iteration 134/1000 | Loss: 0.00003527
Iteration 135/1000 | Loss: 0.00003527
Iteration 136/1000 | Loss: 0.00003527
Iteration 137/1000 | Loss: 0.00003527
Iteration 138/1000 | Loss: 0.00003527
Iteration 139/1000 | Loss: 0.00003527
Iteration 140/1000 | Loss: 0.00003527
Iteration 141/1000 | Loss: 0.00003526
Iteration 142/1000 | Loss: 0.00003526
Iteration 143/1000 | Loss: 0.00003526
Iteration 144/1000 | Loss: 0.00003526
Iteration 145/1000 | Loss: 0.00003526
Iteration 146/1000 | Loss: 0.00003526
Iteration 147/1000 | Loss: 0.00003526
Iteration 148/1000 | Loss: 0.00003526
Iteration 149/1000 | Loss: 0.00003526
Iteration 150/1000 | Loss: 0.00003526
Iteration 151/1000 | Loss: 0.00003526
Iteration 152/1000 | Loss: 0.00003526
Iteration 153/1000 | Loss: 0.00003526
Iteration 154/1000 | Loss: 0.00003526
Iteration 155/1000 | Loss: 0.00003526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [3.5260025470051914e-05, 3.5260025470051914e-05, 3.5260025470051914e-05, 3.5260025470051914e-05, 3.5260025470051914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5260025470051914e-05

Optimization complete. Final v2v error: 5.075628280639648 mm

Highest mean error: 11.115333557128906 mm for frame 180

Lowest mean error: 4.520313739776611 mm for frame 113

Saving results

Total time: 145.10983419418335
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00890644
Iteration 2/25 | Loss: 0.00132713
Iteration 3/25 | Loss: 0.00107299
Iteration 4/25 | Loss: 0.00102050
Iteration 5/25 | Loss: 0.00100724
Iteration 6/25 | Loss: 0.00100324
Iteration 7/25 | Loss: 0.00100180
Iteration 8/25 | Loss: 0.00100150
Iteration 9/25 | Loss: 0.00100150
Iteration 10/25 | Loss: 0.00100150
Iteration 11/25 | Loss: 0.00100150
Iteration 12/25 | Loss: 0.00100150
Iteration 13/25 | Loss: 0.00100150
Iteration 14/25 | Loss: 0.00100150
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010015018051490188, 0.0010015018051490188, 0.0010015018051490188, 0.0010015018051490188, 0.0010015018051490188]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010015018051490188

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56211686
Iteration 2/25 | Loss: 0.00107712
Iteration 3/25 | Loss: 0.00107712
Iteration 4/25 | Loss: 0.00107712
Iteration 5/25 | Loss: 0.00107711
Iteration 6/25 | Loss: 0.00107711
Iteration 7/25 | Loss: 0.00107711
Iteration 8/25 | Loss: 0.00107711
Iteration 9/25 | Loss: 0.00107711
Iteration 10/25 | Loss: 0.00107711
Iteration 11/25 | Loss: 0.00107711
Iteration 12/25 | Loss: 0.00107711
Iteration 13/25 | Loss: 0.00107711
Iteration 14/25 | Loss: 0.00107711
Iteration 15/25 | Loss: 0.00107711
Iteration 16/25 | Loss: 0.00107711
Iteration 17/25 | Loss: 0.00107711
Iteration 18/25 | Loss: 0.00107711
Iteration 19/25 | Loss: 0.00107711
Iteration 20/25 | Loss: 0.00107711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010771122761070728, 0.0010771122761070728, 0.0010771122761070728, 0.0010771122761070728, 0.0010771122761070728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010771122761070728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107711
Iteration 2/1000 | Loss: 0.00004921
Iteration 3/1000 | Loss: 0.00002941
Iteration 4/1000 | Loss: 0.00002310
Iteration 5/1000 | Loss: 0.00002016
Iteration 6/1000 | Loss: 0.00001868
Iteration 7/1000 | Loss: 0.00001788
Iteration 8/1000 | Loss: 0.00001722
Iteration 9/1000 | Loss: 0.00001671
Iteration 10/1000 | Loss: 0.00001628
Iteration 11/1000 | Loss: 0.00001602
Iteration 12/1000 | Loss: 0.00001584
Iteration 13/1000 | Loss: 0.00001565
Iteration 14/1000 | Loss: 0.00001559
Iteration 15/1000 | Loss: 0.00001557
Iteration 16/1000 | Loss: 0.00001553
Iteration 17/1000 | Loss: 0.00001545
Iteration 18/1000 | Loss: 0.00001544
Iteration 19/1000 | Loss: 0.00001542
Iteration 20/1000 | Loss: 0.00001541
Iteration 21/1000 | Loss: 0.00001539
Iteration 22/1000 | Loss: 0.00001537
Iteration 23/1000 | Loss: 0.00001536
Iteration 24/1000 | Loss: 0.00001535
Iteration 25/1000 | Loss: 0.00001535
Iteration 26/1000 | Loss: 0.00001534
Iteration 27/1000 | Loss: 0.00001534
Iteration 28/1000 | Loss: 0.00001533
Iteration 29/1000 | Loss: 0.00001533
Iteration 30/1000 | Loss: 0.00001533
Iteration 31/1000 | Loss: 0.00001532
Iteration 32/1000 | Loss: 0.00001532
Iteration 33/1000 | Loss: 0.00001532
Iteration 34/1000 | Loss: 0.00001531
Iteration 35/1000 | Loss: 0.00001531
Iteration 36/1000 | Loss: 0.00001530
Iteration 37/1000 | Loss: 0.00001530
Iteration 38/1000 | Loss: 0.00001530
Iteration 39/1000 | Loss: 0.00001529
Iteration 40/1000 | Loss: 0.00001529
Iteration 41/1000 | Loss: 0.00001528
Iteration 42/1000 | Loss: 0.00001528
Iteration 43/1000 | Loss: 0.00001528
Iteration 44/1000 | Loss: 0.00001527
Iteration 45/1000 | Loss: 0.00001527
Iteration 46/1000 | Loss: 0.00001527
Iteration 47/1000 | Loss: 0.00001527
Iteration 48/1000 | Loss: 0.00001527
Iteration 49/1000 | Loss: 0.00001527
Iteration 50/1000 | Loss: 0.00001527
Iteration 51/1000 | Loss: 0.00001527
Iteration 52/1000 | Loss: 0.00001526
Iteration 53/1000 | Loss: 0.00001526
Iteration 54/1000 | Loss: 0.00001526
Iteration 55/1000 | Loss: 0.00001526
Iteration 56/1000 | Loss: 0.00001526
Iteration 57/1000 | Loss: 0.00001526
Iteration 58/1000 | Loss: 0.00001526
Iteration 59/1000 | Loss: 0.00001526
Iteration 60/1000 | Loss: 0.00001526
Iteration 61/1000 | Loss: 0.00001526
Iteration 62/1000 | Loss: 0.00001526
Iteration 63/1000 | Loss: 0.00001526
Iteration 64/1000 | Loss: 0.00001526
Iteration 65/1000 | Loss: 0.00001526
Iteration 66/1000 | Loss: 0.00001525
Iteration 67/1000 | Loss: 0.00001525
Iteration 68/1000 | Loss: 0.00001524
Iteration 69/1000 | Loss: 0.00001524
Iteration 70/1000 | Loss: 0.00001524
Iteration 71/1000 | Loss: 0.00001524
Iteration 72/1000 | Loss: 0.00001524
Iteration 73/1000 | Loss: 0.00001524
Iteration 74/1000 | Loss: 0.00001524
Iteration 75/1000 | Loss: 0.00001524
Iteration 76/1000 | Loss: 0.00001523
Iteration 77/1000 | Loss: 0.00001523
Iteration 78/1000 | Loss: 0.00001523
Iteration 79/1000 | Loss: 0.00001523
Iteration 80/1000 | Loss: 0.00001523
Iteration 81/1000 | Loss: 0.00001522
Iteration 82/1000 | Loss: 0.00001522
Iteration 83/1000 | Loss: 0.00001522
Iteration 84/1000 | Loss: 0.00001521
Iteration 85/1000 | Loss: 0.00001521
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00001521
Iteration 88/1000 | Loss: 0.00001521
Iteration 89/1000 | Loss: 0.00001521
Iteration 90/1000 | Loss: 0.00001520
Iteration 91/1000 | Loss: 0.00001520
Iteration 92/1000 | Loss: 0.00001520
Iteration 93/1000 | Loss: 0.00001520
Iteration 94/1000 | Loss: 0.00001519
Iteration 95/1000 | Loss: 0.00001519
Iteration 96/1000 | Loss: 0.00001519
Iteration 97/1000 | Loss: 0.00001519
Iteration 98/1000 | Loss: 0.00001519
Iteration 99/1000 | Loss: 0.00001519
Iteration 100/1000 | Loss: 0.00001518
Iteration 101/1000 | Loss: 0.00001518
Iteration 102/1000 | Loss: 0.00001518
Iteration 103/1000 | Loss: 0.00001518
Iteration 104/1000 | Loss: 0.00001517
Iteration 105/1000 | Loss: 0.00001517
Iteration 106/1000 | Loss: 0.00001517
Iteration 107/1000 | Loss: 0.00001517
Iteration 108/1000 | Loss: 0.00001517
Iteration 109/1000 | Loss: 0.00001517
Iteration 110/1000 | Loss: 0.00001517
Iteration 111/1000 | Loss: 0.00001517
Iteration 112/1000 | Loss: 0.00001517
Iteration 113/1000 | Loss: 0.00001517
Iteration 114/1000 | Loss: 0.00001516
Iteration 115/1000 | Loss: 0.00001516
Iteration 116/1000 | Loss: 0.00001516
Iteration 117/1000 | Loss: 0.00001516
Iteration 118/1000 | Loss: 0.00001516
Iteration 119/1000 | Loss: 0.00001516
Iteration 120/1000 | Loss: 0.00001516
Iteration 121/1000 | Loss: 0.00001516
Iteration 122/1000 | Loss: 0.00001516
Iteration 123/1000 | Loss: 0.00001516
Iteration 124/1000 | Loss: 0.00001516
Iteration 125/1000 | Loss: 0.00001516
Iteration 126/1000 | Loss: 0.00001516
Iteration 127/1000 | Loss: 0.00001516
Iteration 128/1000 | Loss: 0.00001516
Iteration 129/1000 | Loss: 0.00001516
Iteration 130/1000 | Loss: 0.00001516
Iteration 131/1000 | Loss: 0.00001516
Iteration 132/1000 | Loss: 0.00001516
Iteration 133/1000 | Loss: 0.00001516
Iteration 134/1000 | Loss: 0.00001516
Iteration 135/1000 | Loss: 0.00001516
Iteration 136/1000 | Loss: 0.00001516
Iteration 137/1000 | Loss: 0.00001516
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001516
Iteration 144/1000 | Loss: 0.00001516
Iteration 145/1000 | Loss: 0.00001516
Iteration 146/1000 | Loss: 0.00001516
Iteration 147/1000 | Loss: 0.00001516
Iteration 148/1000 | Loss: 0.00001516
Iteration 149/1000 | Loss: 0.00001516
Iteration 150/1000 | Loss: 0.00001516
Iteration 151/1000 | Loss: 0.00001516
Iteration 152/1000 | Loss: 0.00001516
Iteration 153/1000 | Loss: 0.00001516
Iteration 154/1000 | Loss: 0.00001516
Iteration 155/1000 | Loss: 0.00001516
Iteration 156/1000 | Loss: 0.00001516
Iteration 157/1000 | Loss: 0.00001516
Iteration 158/1000 | Loss: 0.00001516
Iteration 159/1000 | Loss: 0.00001516
Iteration 160/1000 | Loss: 0.00001516
Iteration 161/1000 | Loss: 0.00001516
Iteration 162/1000 | Loss: 0.00001516
Iteration 163/1000 | Loss: 0.00001516
Iteration 164/1000 | Loss: 0.00001516
Iteration 165/1000 | Loss: 0.00001516
Iteration 166/1000 | Loss: 0.00001516
Iteration 167/1000 | Loss: 0.00001516
Iteration 168/1000 | Loss: 0.00001516
Iteration 169/1000 | Loss: 0.00001516
Iteration 170/1000 | Loss: 0.00001516
Iteration 171/1000 | Loss: 0.00001516
Iteration 172/1000 | Loss: 0.00001516
Iteration 173/1000 | Loss: 0.00001516
Iteration 174/1000 | Loss: 0.00001516
Iteration 175/1000 | Loss: 0.00001516
Iteration 176/1000 | Loss: 0.00001516
Iteration 177/1000 | Loss: 0.00001516
Iteration 178/1000 | Loss: 0.00001516
Iteration 179/1000 | Loss: 0.00001516
Iteration 180/1000 | Loss: 0.00001516
Iteration 181/1000 | Loss: 0.00001516
Iteration 182/1000 | Loss: 0.00001516
Iteration 183/1000 | Loss: 0.00001516
Iteration 184/1000 | Loss: 0.00001516
Iteration 185/1000 | Loss: 0.00001516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.5164443539106287e-05, 1.5164443539106287e-05, 1.5164443539106287e-05, 1.5164443539106287e-05, 1.5164443539106287e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5164443539106287e-05

Optimization complete. Final v2v error: 3.35742449760437 mm

Highest mean error: 3.8785972595214844 mm for frame 48

Lowest mean error: 3.129063844680786 mm for frame 65

Saving results

Total time: 41.17172050476074
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00988383
Iteration 2/25 | Loss: 0.00197309
Iteration 3/25 | Loss: 0.00156704
Iteration 4/25 | Loss: 0.00140479
Iteration 5/25 | Loss: 0.00134204
Iteration 6/25 | Loss: 0.00133427
Iteration 7/25 | Loss: 0.00128101
Iteration 8/25 | Loss: 0.00125276
Iteration 9/25 | Loss: 0.00121355
Iteration 10/25 | Loss: 0.00119343
Iteration 11/25 | Loss: 0.00119750
Iteration 12/25 | Loss: 0.00118437
Iteration 13/25 | Loss: 0.00117407
Iteration 14/25 | Loss: 0.00117091
Iteration 15/25 | Loss: 0.00116511
Iteration 16/25 | Loss: 0.00116374
Iteration 17/25 | Loss: 0.00116282
Iteration 18/25 | Loss: 0.00116061
Iteration 19/25 | Loss: 0.00116716
Iteration 20/25 | Loss: 0.00116030
Iteration 21/25 | Loss: 0.00115727
Iteration 22/25 | Loss: 0.00115683
Iteration 23/25 | Loss: 0.00115668
Iteration 24/25 | Loss: 0.00115655
Iteration 25/25 | Loss: 0.00115640

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65604889
Iteration 2/25 | Loss: 0.00152065
Iteration 3/25 | Loss: 0.00152063
Iteration 4/25 | Loss: 0.00152063
Iteration 5/25 | Loss: 0.00152063
Iteration 6/25 | Loss: 0.00152063
Iteration 7/25 | Loss: 0.00152063
Iteration 8/25 | Loss: 0.00152063
Iteration 9/25 | Loss: 0.00152063
Iteration 10/25 | Loss: 0.00152063
Iteration 11/25 | Loss: 0.00152063
Iteration 12/25 | Loss: 0.00152063
Iteration 13/25 | Loss: 0.00152063
Iteration 14/25 | Loss: 0.00152063
Iteration 15/25 | Loss: 0.00152063
Iteration 16/25 | Loss: 0.00152063
Iteration 17/25 | Loss: 0.00152063
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00152063206769526, 0.00152063206769526, 0.00152063206769526, 0.00152063206769526, 0.00152063206769526]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00152063206769526

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152063
Iteration 2/1000 | Loss: 0.00014397
Iteration 3/1000 | Loss: 0.00010838
Iteration 4/1000 | Loss: 0.00008295
Iteration 5/1000 | Loss: 0.00006773
Iteration 6/1000 | Loss: 0.00077826
Iteration 7/1000 | Loss: 0.00076327
Iteration 8/1000 | Loss: 0.00071696
Iteration 9/1000 | Loss: 0.00070503
Iteration 10/1000 | Loss: 0.00168057
Iteration 11/1000 | Loss: 0.00057240
Iteration 12/1000 | Loss: 0.00143326
Iteration 13/1000 | Loss: 0.00138106
Iteration 14/1000 | Loss: 0.00086052
Iteration 15/1000 | Loss: 0.00067084
Iteration 16/1000 | Loss: 0.00015857
Iteration 17/1000 | Loss: 0.00035911
Iteration 18/1000 | Loss: 0.00049303
Iteration 19/1000 | Loss: 0.00036921
Iteration 20/1000 | Loss: 0.00015127
Iteration 21/1000 | Loss: 0.00020239
Iteration 22/1000 | Loss: 0.00035149
Iteration 23/1000 | Loss: 0.00058646
Iteration 24/1000 | Loss: 0.00070901
Iteration 25/1000 | Loss: 0.00029841
Iteration 26/1000 | Loss: 0.00036561
Iteration 27/1000 | Loss: 0.00025654
Iteration 28/1000 | Loss: 0.00058506
Iteration 29/1000 | Loss: 0.00012184
Iteration 30/1000 | Loss: 0.00020244
Iteration 31/1000 | Loss: 0.00061099
Iteration 32/1000 | Loss: 0.00039797
Iteration 33/1000 | Loss: 0.00048462
Iteration 34/1000 | Loss: 0.00021828
Iteration 35/1000 | Loss: 0.00066242
Iteration 36/1000 | Loss: 0.00045810
Iteration 37/1000 | Loss: 0.00039574
Iteration 38/1000 | Loss: 0.00093265
Iteration 39/1000 | Loss: 0.00025720
Iteration 40/1000 | Loss: 0.00004143
Iteration 41/1000 | Loss: 0.00003513
Iteration 42/1000 | Loss: 0.00003303
Iteration 43/1000 | Loss: 0.00003167
Iteration 44/1000 | Loss: 0.00018925
Iteration 45/1000 | Loss: 0.00050865
Iteration 46/1000 | Loss: 0.00072576
Iteration 47/1000 | Loss: 0.00023845
Iteration 48/1000 | Loss: 0.00046347
Iteration 49/1000 | Loss: 0.00020072
Iteration 50/1000 | Loss: 0.00041713
Iteration 51/1000 | Loss: 0.00013988
Iteration 52/1000 | Loss: 0.00052456
Iteration 53/1000 | Loss: 0.00063633
Iteration 54/1000 | Loss: 0.00015388
Iteration 55/1000 | Loss: 0.00052929
Iteration 56/1000 | Loss: 0.00036090
Iteration 57/1000 | Loss: 0.00028813
Iteration 58/1000 | Loss: 0.00058752
Iteration 59/1000 | Loss: 0.00023112
Iteration 60/1000 | Loss: 0.00071725
Iteration 61/1000 | Loss: 0.00023395
Iteration 62/1000 | Loss: 0.00010599
Iteration 63/1000 | Loss: 0.00035379
Iteration 64/1000 | Loss: 0.00040008
Iteration 65/1000 | Loss: 0.00056804
Iteration 66/1000 | Loss: 0.00005225
Iteration 67/1000 | Loss: 0.00010601
Iteration 68/1000 | Loss: 0.00046707
Iteration 69/1000 | Loss: 0.00059817
Iteration 70/1000 | Loss: 0.00041812
Iteration 71/1000 | Loss: 0.00039987
Iteration 72/1000 | Loss: 0.00058762
Iteration 73/1000 | Loss: 0.00007675
Iteration 74/1000 | Loss: 0.00035787
Iteration 75/1000 | Loss: 0.00036729
Iteration 76/1000 | Loss: 0.00021203
Iteration 77/1000 | Loss: 0.00033906
Iteration 78/1000 | Loss: 0.00021699
Iteration 79/1000 | Loss: 0.00024212
Iteration 80/1000 | Loss: 0.00014652
Iteration 81/1000 | Loss: 0.00017843
Iteration 82/1000 | Loss: 0.00016142
Iteration 83/1000 | Loss: 0.00047060
Iteration 84/1000 | Loss: 0.00070835
Iteration 85/1000 | Loss: 0.00148317
Iteration 86/1000 | Loss: 0.00055915
Iteration 87/1000 | Loss: 0.00057672
Iteration 88/1000 | Loss: 0.00059286
Iteration 89/1000 | Loss: 0.00031616
Iteration 90/1000 | Loss: 0.00062114
Iteration 91/1000 | Loss: 0.00029950
Iteration 92/1000 | Loss: 0.00052145
Iteration 93/1000 | Loss: 0.00016403
Iteration 94/1000 | Loss: 0.00057836
Iteration 95/1000 | Loss: 0.00050888
Iteration 96/1000 | Loss: 0.00049588
Iteration 97/1000 | Loss: 0.00018767
Iteration 98/1000 | Loss: 0.00018924
Iteration 99/1000 | Loss: 0.00070463
Iteration 100/1000 | Loss: 0.00057397
Iteration 101/1000 | Loss: 0.00041893
Iteration 102/1000 | Loss: 0.00021621
Iteration 103/1000 | Loss: 0.00105899
Iteration 104/1000 | Loss: 0.00021976
Iteration 105/1000 | Loss: 0.00005110
Iteration 106/1000 | Loss: 0.00003440
Iteration 107/1000 | Loss: 0.00003056
Iteration 108/1000 | Loss: 0.00002929
Iteration 109/1000 | Loss: 0.00121782
Iteration 110/1000 | Loss: 0.00044345
Iteration 111/1000 | Loss: 0.00076857
Iteration 112/1000 | Loss: 0.00010988
Iteration 113/1000 | Loss: 0.00040055
Iteration 114/1000 | Loss: 0.00086800
Iteration 115/1000 | Loss: 0.00006384
Iteration 116/1000 | Loss: 0.00003830
Iteration 117/1000 | Loss: 0.00003553
Iteration 118/1000 | Loss: 0.00039943
Iteration 119/1000 | Loss: 0.00003434
Iteration 120/1000 | Loss: 0.00003070
Iteration 121/1000 | Loss: 0.00002851
Iteration 122/1000 | Loss: 0.00002773
Iteration 123/1000 | Loss: 0.00002728
Iteration 124/1000 | Loss: 0.00002701
Iteration 125/1000 | Loss: 0.00002660
Iteration 126/1000 | Loss: 0.00002614
Iteration 127/1000 | Loss: 0.00002569
Iteration 128/1000 | Loss: 0.00101367
Iteration 129/1000 | Loss: 0.00004734
Iteration 130/1000 | Loss: 0.00139956
Iteration 131/1000 | Loss: 0.00003866
Iteration 132/1000 | Loss: 0.00002793
Iteration 133/1000 | Loss: 0.00002591
Iteration 134/1000 | Loss: 0.00002536
Iteration 135/1000 | Loss: 0.00002490
Iteration 136/1000 | Loss: 0.00002486
Iteration 137/1000 | Loss: 0.00002469
Iteration 138/1000 | Loss: 0.00002469
Iteration 139/1000 | Loss: 0.00002466
Iteration 140/1000 | Loss: 0.00002461
Iteration 141/1000 | Loss: 0.00002460
Iteration 142/1000 | Loss: 0.00002459
Iteration 143/1000 | Loss: 0.00002457
Iteration 144/1000 | Loss: 0.00002456
Iteration 145/1000 | Loss: 0.00002448
Iteration 146/1000 | Loss: 0.00002446
Iteration 147/1000 | Loss: 0.00002444
Iteration 148/1000 | Loss: 0.00002442
Iteration 149/1000 | Loss: 0.00002442
Iteration 150/1000 | Loss: 0.00002435
Iteration 151/1000 | Loss: 0.00037091
Iteration 152/1000 | Loss: 0.00003826
Iteration 153/1000 | Loss: 0.00002413
Iteration 154/1000 | Loss: 0.00002218
Iteration 155/1000 | Loss: 0.00002164
Iteration 156/1000 | Loss: 0.00002132
Iteration 157/1000 | Loss: 0.00002106
Iteration 158/1000 | Loss: 0.00002093
Iteration 159/1000 | Loss: 0.00002090
Iteration 160/1000 | Loss: 0.00002089
Iteration 161/1000 | Loss: 0.00002089
Iteration 162/1000 | Loss: 0.00002089
Iteration 163/1000 | Loss: 0.00002088
Iteration 164/1000 | Loss: 0.00002088
Iteration 165/1000 | Loss: 0.00002087
Iteration 166/1000 | Loss: 0.00002083
Iteration 167/1000 | Loss: 0.00002081
Iteration 168/1000 | Loss: 0.00002076
Iteration 169/1000 | Loss: 0.00002076
Iteration 170/1000 | Loss: 0.00002075
Iteration 171/1000 | Loss: 0.00002075
Iteration 172/1000 | Loss: 0.00002074
Iteration 173/1000 | Loss: 0.00002074
Iteration 174/1000 | Loss: 0.00002074
Iteration 175/1000 | Loss: 0.00002073
Iteration 176/1000 | Loss: 0.00002073
Iteration 177/1000 | Loss: 0.00002073
Iteration 178/1000 | Loss: 0.00002071
Iteration 179/1000 | Loss: 0.00002071
Iteration 180/1000 | Loss: 0.00002071
Iteration 181/1000 | Loss: 0.00002071
Iteration 182/1000 | Loss: 0.00002070
Iteration 183/1000 | Loss: 0.00002070
Iteration 184/1000 | Loss: 0.00002070
Iteration 185/1000 | Loss: 0.00002070
Iteration 186/1000 | Loss: 0.00002070
Iteration 187/1000 | Loss: 0.00002070
Iteration 188/1000 | Loss: 0.00002068
Iteration 189/1000 | Loss: 0.00002068
Iteration 190/1000 | Loss: 0.00002068
Iteration 191/1000 | Loss: 0.00002067
Iteration 192/1000 | Loss: 0.00002066
Iteration 193/1000 | Loss: 0.00002066
Iteration 194/1000 | Loss: 0.00002065
Iteration 195/1000 | Loss: 0.00002065
Iteration 196/1000 | Loss: 0.00002065
Iteration 197/1000 | Loss: 0.00002064
Iteration 198/1000 | Loss: 0.00002064
Iteration 199/1000 | Loss: 0.00002064
Iteration 200/1000 | Loss: 0.00002064
Iteration 201/1000 | Loss: 0.00002063
Iteration 202/1000 | Loss: 0.00002063
Iteration 203/1000 | Loss: 0.00002063
Iteration 204/1000 | Loss: 0.00002063
Iteration 205/1000 | Loss: 0.00002062
Iteration 206/1000 | Loss: 0.00002062
Iteration 207/1000 | Loss: 0.00002062
Iteration 208/1000 | Loss: 0.00002062
Iteration 209/1000 | Loss: 0.00002062
Iteration 210/1000 | Loss: 0.00002062
Iteration 211/1000 | Loss: 0.00002061
Iteration 212/1000 | Loss: 0.00002061
Iteration 213/1000 | Loss: 0.00002061
Iteration 214/1000 | Loss: 0.00002061
Iteration 215/1000 | Loss: 0.00002061
Iteration 216/1000 | Loss: 0.00002060
Iteration 217/1000 | Loss: 0.00002060
Iteration 218/1000 | Loss: 0.00002060
Iteration 219/1000 | Loss: 0.00002060
Iteration 220/1000 | Loss: 0.00002060
Iteration 221/1000 | Loss: 0.00002060
Iteration 222/1000 | Loss: 0.00002060
Iteration 223/1000 | Loss: 0.00002060
Iteration 224/1000 | Loss: 0.00002059
Iteration 225/1000 | Loss: 0.00002059
Iteration 226/1000 | Loss: 0.00002059
Iteration 227/1000 | Loss: 0.00002059
Iteration 228/1000 | Loss: 0.00002059
Iteration 229/1000 | Loss: 0.00002059
Iteration 230/1000 | Loss: 0.00002059
Iteration 231/1000 | Loss: 0.00002059
Iteration 232/1000 | Loss: 0.00002059
Iteration 233/1000 | Loss: 0.00002059
Iteration 234/1000 | Loss: 0.00002059
Iteration 235/1000 | Loss: 0.00002059
Iteration 236/1000 | Loss: 0.00002059
Iteration 237/1000 | Loss: 0.00002059
Iteration 238/1000 | Loss: 0.00002059
Iteration 239/1000 | Loss: 0.00002059
Iteration 240/1000 | Loss: 0.00002059
Iteration 241/1000 | Loss: 0.00002059
Iteration 242/1000 | Loss: 0.00002059
Iteration 243/1000 | Loss: 0.00002059
Iteration 244/1000 | Loss: 0.00002058
Iteration 245/1000 | Loss: 0.00002058
Iteration 246/1000 | Loss: 0.00002058
Iteration 247/1000 | Loss: 0.00002058
Iteration 248/1000 | Loss: 0.00002058
Iteration 249/1000 | Loss: 0.00002058
Iteration 250/1000 | Loss: 0.00002058
Iteration 251/1000 | Loss: 0.00002058
Iteration 252/1000 | Loss: 0.00002058
Iteration 253/1000 | Loss: 0.00002058
Iteration 254/1000 | Loss: 0.00002058
Iteration 255/1000 | Loss: 0.00002058
Iteration 256/1000 | Loss: 0.00002058
Iteration 257/1000 | Loss: 0.00002058
Iteration 258/1000 | Loss: 0.00002058
Iteration 259/1000 | Loss: 0.00002058
Iteration 260/1000 | Loss: 0.00002058
Iteration 261/1000 | Loss: 0.00002058
Iteration 262/1000 | Loss: 0.00002058
Iteration 263/1000 | Loss: 0.00002058
Iteration 264/1000 | Loss: 0.00002058
Iteration 265/1000 | Loss: 0.00002057
Iteration 266/1000 | Loss: 0.00002057
Iteration 267/1000 | Loss: 0.00002057
Iteration 268/1000 | Loss: 0.00002057
Iteration 269/1000 | Loss: 0.00002057
Iteration 270/1000 | Loss: 0.00002057
Iteration 271/1000 | Loss: 0.00002057
Iteration 272/1000 | Loss: 0.00002057
Iteration 273/1000 | Loss: 0.00002057
Iteration 274/1000 | Loss: 0.00002057
Iteration 275/1000 | Loss: 0.00002057
Iteration 276/1000 | Loss: 0.00002057
Iteration 277/1000 | Loss: 0.00002057
Iteration 278/1000 | Loss: 0.00002057
Iteration 279/1000 | Loss: 0.00002057
Iteration 280/1000 | Loss: 0.00002057
Iteration 281/1000 | Loss: 0.00002056
Iteration 282/1000 | Loss: 0.00002056
Iteration 283/1000 | Loss: 0.00002056
Iteration 284/1000 | Loss: 0.00002056
Iteration 285/1000 | Loss: 0.00002056
Iteration 286/1000 | Loss: 0.00002056
Iteration 287/1000 | Loss: 0.00002056
Iteration 288/1000 | Loss: 0.00002056
Iteration 289/1000 | Loss: 0.00002056
Iteration 290/1000 | Loss: 0.00002056
Iteration 291/1000 | Loss: 0.00002056
Iteration 292/1000 | Loss: 0.00002056
Iteration 293/1000 | Loss: 0.00002056
Iteration 294/1000 | Loss: 0.00002056
Iteration 295/1000 | Loss: 0.00002056
Iteration 296/1000 | Loss: 0.00002056
Iteration 297/1000 | Loss: 0.00002056
Iteration 298/1000 | Loss: 0.00002056
Iteration 299/1000 | Loss: 0.00002056
Iteration 300/1000 | Loss: 0.00002055
Iteration 301/1000 | Loss: 0.00002055
Iteration 302/1000 | Loss: 0.00002055
Iteration 303/1000 | Loss: 0.00002055
Iteration 304/1000 | Loss: 0.00002055
Iteration 305/1000 | Loss: 0.00002055
Iteration 306/1000 | Loss: 0.00002055
Iteration 307/1000 | Loss: 0.00002055
Iteration 308/1000 | Loss: 0.00002055
Iteration 309/1000 | Loss: 0.00002055
Iteration 310/1000 | Loss: 0.00002055
Iteration 311/1000 | Loss: 0.00002055
Iteration 312/1000 | Loss: 0.00002055
Iteration 313/1000 | Loss: 0.00002055
Iteration 314/1000 | Loss: 0.00002055
Iteration 315/1000 | Loss: 0.00002055
Iteration 316/1000 | Loss: 0.00002055
Iteration 317/1000 | Loss: 0.00002055
Iteration 318/1000 | Loss: 0.00002055
Iteration 319/1000 | Loss: 0.00002055
Iteration 320/1000 | Loss: 0.00002055
Iteration 321/1000 | Loss: 0.00002055
Iteration 322/1000 | Loss: 0.00002055
Iteration 323/1000 | Loss: 0.00002055
Iteration 324/1000 | Loss: 0.00002055
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 324. Stopping optimization.
Last 5 losses: [2.0545976440189406e-05, 2.0545976440189406e-05, 2.0545976440189406e-05, 2.0545976440189406e-05, 2.0545976440189406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0545976440189406e-05

Optimization complete. Final v2v error: 3.888474702835083 mm

Highest mean error: 4.395172119140625 mm for frame 76

Lowest mean error: 2.9958672523498535 mm for frame 137

Saving results

Total time: 268.01500391960144
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841387
Iteration 2/25 | Loss: 0.00151502
Iteration 3/25 | Loss: 0.00113493
Iteration 4/25 | Loss: 0.00110335
Iteration 5/25 | Loss: 0.00108719
Iteration 6/25 | Loss: 0.00108586
Iteration 7/25 | Loss: 0.00108586
Iteration 8/25 | Loss: 0.00108586
Iteration 9/25 | Loss: 0.00108586
Iteration 10/25 | Loss: 0.00108586
Iteration 11/25 | Loss: 0.00108586
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001085860189050436, 0.001085860189050436, 0.001085860189050436, 0.001085860189050436, 0.001085860189050436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001085860189050436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32701850
Iteration 2/25 | Loss: 0.00095276
Iteration 3/25 | Loss: 0.00095273
Iteration 4/25 | Loss: 0.00095273
Iteration 5/25 | Loss: 0.00095273
Iteration 6/25 | Loss: 0.00095273
Iteration 7/25 | Loss: 0.00095273
Iteration 8/25 | Loss: 0.00095273
Iteration 9/25 | Loss: 0.00095273
Iteration 10/25 | Loss: 0.00095272
Iteration 11/25 | Loss: 0.00095272
Iteration 12/25 | Loss: 0.00095272
Iteration 13/25 | Loss: 0.00095272
Iteration 14/25 | Loss: 0.00095272
Iteration 15/25 | Loss: 0.00095272
Iteration 16/25 | Loss: 0.00095272
Iteration 17/25 | Loss: 0.00095272
Iteration 18/25 | Loss: 0.00095272
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009527249494567513, 0.0009527249494567513, 0.0009527249494567513, 0.0009527249494567513, 0.0009527249494567513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009527249494567513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095273
Iteration 2/1000 | Loss: 0.00003938
Iteration 3/1000 | Loss: 0.00002872
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002378
Iteration 6/1000 | Loss: 0.00002307
Iteration 7/1000 | Loss: 0.00002264
Iteration 8/1000 | Loss: 0.00002234
Iteration 9/1000 | Loss: 0.00002203
Iteration 10/1000 | Loss: 0.00002183
Iteration 11/1000 | Loss: 0.00002182
Iteration 12/1000 | Loss: 0.00002174
Iteration 13/1000 | Loss: 0.00002169
Iteration 14/1000 | Loss: 0.00002162
Iteration 15/1000 | Loss: 0.00002162
Iteration 16/1000 | Loss: 0.00002161
Iteration 17/1000 | Loss: 0.00002157
Iteration 18/1000 | Loss: 0.00002156
Iteration 19/1000 | Loss: 0.00002156
Iteration 20/1000 | Loss: 0.00002156
Iteration 21/1000 | Loss: 0.00002155
Iteration 22/1000 | Loss: 0.00002152
Iteration 23/1000 | Loss: 0.00002148
Iteration 24/1000 | Loss: 0.00002146
Iteration 25/1000 | Loss: 0.00002146
Iteration 26/1000 | Loss: 0.00002145
Iteration 27/1000 | Loss: 0.00002144
Iteration 28/1000 | Loss: 0.00002144
Iteration 29/1000 | Loss: 0.00002144
Iteration 30/1000 | Loss: 0.00002143
Iteration 31/1000 | Loss: 0.00002143
Iteration 32/1000 | Loss: 0.00002143
Iteration 33/1000 | Loss: 0.00002142
Iteration 34/1000 | Loss: 0.00002142
Iteration 35/1000 | Loss: 0.00002142
Iteration 36/1000 | Loss: 0.00002142
Iteration 37/1000 | Loss: 0.00002142
Iteration 38/1000 | Loss: 0.00002141
Iteration 39/1000 | Loss: 0.00002141
Iteration 40/1000 | Loss: 0.00002141
Iteration 41/1000 | Loss: 0.00002141
Iteration 42/1000 | Loss: 0.00002141
Iteration 43/1000 | Loss: 0.00002141
Iteration 44/1000 | Loss: 0.00002141
Iteration 45/1000 | Loss: 0.00002141
Iteration 46/1000 | Loss: 0.00002141
Iteration 47/1000 | Loss: 0.00002140
Iteration 48/1000 | Loss: 0.00002140
Iteration 49/1000 | Loss: 0.00002140
Iteration 50/1000 | Loss: 0.00002140
Iteration 51/1000 | Loss: 0.00002140
Iteration 52/1000 | Loss: 0.00002140
Iteration 53/1000 | Loss: 0.00002140
Iteration 54/1000 | Loss: 0.00002139
Iteration 55/1000 | Loss: 0.00002139
Iteration 56/1000 | Loss: 0.00002139
Iteration 57/1000 | Loss: 0.00002139
Iteration 58/1000 | Loss: 0.00002138
Iteration 59/1000 | Loss: 0.00002138
Iteration 60/1000 | Loss: 0.00002138
Iteration 61/1000 | Loss: 0.00002138
Iteration 62/1000 | Loss: 0.00002138
Iteration 63/1000 | Loss: 0.00002137
Iteration 64/1000 | Loss: 0.00002137
Iteration 65/1000 | Loss: 0.00002137
Iteration 66/1000 | Loss: 0.00002136
Iteration 67/1000 | Loss: 0.00002136
Iteration 68/1000 | Loss: 0.00002136
Iteration 69/1000 | Loss: 0.00002135
Iteration 70/1000 | Loss: 0.00002135
Iteration 71/1000 | Loss: 0.00002135
Iteration 72/1000 | Loss: 0.00002134
Iteration 73/1000 | Loss: 0.00002134
Iteration 74/1000 | Loss: 0.00002134
Iteration 75/1000 | Loss: 0.00002134
Iteration 76/1000 | Loss: 0.00002133
Iteration 77/1000 | Loss: 0.00002133
Iteration 78/1000 | Loss: 0.00002133
Iteration 79/1000 | Loss: 0.00002132
Iteration 80/1000 | Loss: 0.00002132
Iteration 81/1000 | Loss: 0.00002132
Iteration 82/1000 | Loss: 0.00002132
Iteration 83/1000 | Loss: 0.00002132
Iteration 84/1000 | Loss: 0.00002132
Iteration 85/1000 | Loss: 0.00002132
Iteration 86/1000 | Loss: 0.00002132
Iteration 87/1000 | Loss: 0.00002132
Iteration 88/1000 | Loss: 0.00002131
Iteration 89/1000 | Loss: 0.00002131
Iteration 90/1000 | Loss: 0.00002131
Iteration 91/1000 | Loss: 0.00002131
Iteration 92/1000 | Loss: 0.00002131
Iteration 93/1000 | Loss: 0.00002131
Iteration 94/1000 | Loss: 0.00002131
Iteration 95/1000 | Loss: 0.00002131
Iteration 96/1000 | Loss: 0.00002131
Iteration 97/1000 | Loss: 0.00002131
Iteration 98/1000 | Loss: 0.00002131
Iteration 99/1000 | Loss: 0.00002131
Iteration 100/1000 | Loss: 0.00002131
Iteration 101/1000 | Loss: 0.00002131
Iteration 102/1000 | Loss: 0.00002131
Iteration 103/1000 | Loss: 0.00002131
Iteration 104/1000 | Loss: 0.00002131
Iteration 105/1000 | Loss: 0.00002131
Iteration 106/1000 | Loss: 0.00002131
Iteration 107/1000 | Loss: 0.00002131
Iteration 108/1000 | Loss: 0.00002131
Iteration 109/1000 | Loss: 0.00002131
Iteration 110/1000 | Loss: 0.00002131
Iteration 111/1000 | Loss: 0.00002131
Iteration 112/1000 | Loss: 0.00002131
Iteration 113/1000 | Loss: 0.00002131
Iteration 114/1000 | Loss: 0.00002131
Iteration 115/1000 | Loss: 0.00002131
Iteration 116/1000 | Loss: 0.00002131
Iteration 117/1000 | Loss: 0.00002131
Iteration 118/1000 | Loss: 0.00002131
Iteration 119/1000 | Loss: 0.00002131
Iteration 120/1000 | Loss: 0.00002131
Iteration 121/1000 | Loss: 0.00002131
Iteration 122/1000 | Loss: 0.00002131
Iteration 123/1000 | Loss: 0.00002131
Iteration 124/1000 | Loss: 0.00002131
Iteration 125/1000 | Loss: 0.00002131
Iteration 126/1000 | Loss: 0.00002131
Iteration 127/1000 | Loss: 0.00002131
Iteration 128/1000 | Loss: 0.00002131
Iteration 129/1000 | Loss: 0.00002131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.131326800736133e-05, 2.131326800736133e-05, 2.131326800736133e-05, 2.131326800736133e-05, 2.131326800736133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.131326800736133e-05

Optimization complete. Final v2v error: 3.947125196456909 mm

Highest mean error: 5.122357368469238 mm for frame 222

Lowest mean error: 3.2887516021728516 mm for frame 17

Saving results

Total time: 38.8316764831543
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121432
Iteration 2/25 | Loss: 0.00219053
Iteration 3/25 | Loss: 0.00158685
Iteration 4/25 | Loss: 0.00121923
Iteration 5/25 | Loss: 0.00114787
Iteration 6/25 | Loss: 0.00111816
Iteration 7/25 | Loss: 0.00111722
Iteration 8/25 | Loss: 0.00111232
Iteration 9/25 | Loss: 0.00111084
Iteration 10/25 | Loss: 0.00111017
Iteration 11/25 | Loss: 0.00111000
Iteration 12/25 | Loss: 0.00110998
Iteration 13/25 | Loss: 0.00110998
Iteration 14/25 | Loss: 0.00110998
Iteration 15/25 | Loss: 0.00110998
Iteration 16/25 | Loss: 0.00110997
Iteration 17/25 | Loss: 0.00110997
Iteration 18/25 | Loss: 0.00110997
Iteration 19/25 | Loss: 0.00110997
Iteration 20/25 | Loss: 0.00110997
Iteration 21/25 | Loss: 0.00110997
Iteration 22/25 | Loss: 0.00110997
Iteration 23/25 | Loss: 0.00110997
Iteration 24/25 | Loss: 0.00110997
Iteration 25/25 | Loss: 0.00110997

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64632463
Iteration 2/25 | Loss: 0.00102512
Iteration 3/25 | Loss: 0.00102512
Iteration 4/25 | Loss: 0.00102512
Iteration 5/25 | Loss: 0.00102512
Iteration 6/25 | Loss: 0.00102511
Iteration 7/25 | Loss: 0.00102511
Iteration 8/25 | Loss: 0.00102511
Iteration 9/25 | Loss: 0.00102511
Iteration 10/25 | Loss: 0.00102511
Iteration 11/25 | Loss: 0.00102511
Iteration 12/25 | Loss: 0.00102511
Iteration 13/25 | Loss: 0.00102511
Iteration 14/25 | Loss: 0.00102511
Iteration 15/25 | Loss: 0.00102511
Iteration 16/25 | Loss: 0.00102511
Iteration 17/25 | Loss: 0.00102511
Iteration 18/25 | Loss: 0.00102511
Iteration 19/25 | Loss: 0.00102511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010251140920445323, 0.0010251140920445323, 0.0010251140920445323, 0.0010251140920445323, 0.0010251140920445323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010251140920445323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102511
Iteration 2/1000 | Loss: 0.00005005
Iteration 3/1000 | Loss: 0.00003827
Iteration 4/1000 | Loss: 0.00003103
Iteration 5/1000 | Loss: 0.00002844
Iteration 6/1000 | Loss: 0.00002745
Iteration 7/1000 | Loss: 0.00002678
Iteration 8/1000 | Loss: 0.00002611
Iteration 9/1000 | Loss: 0.00002559
Iteration 10/1000 | Loss: 0.00002533
Iteration 11/1000 | Loss: 0.00002531
Iteration 12/1000 | Loss: 0.00002529
Iteration 13/1000 | Loss: 0.00002525
Iteration 14/1000 | Loss: 0.00002523
Iteration 15/1000 | Loss: 0.00002522
Iteration 16/1000 | Loss: 0.00002517
Iteration 17/1000 | Loss: 0.00002515
Iteration 18/1000 | Loss: 0.00002515
Iteration 19/1000 | Loss: 0.00002509
Iteration 20/1000 | Loss: 0.00002508
Iteration 21/1000 | Loss: 0.00002508
Iteration 22/1000 | Loss: 0.00002508
Iteration 23/1000 | Loss: 0.00002507
Iteration 24/1000 | Loss: 0.00002505
Iteration 25/1000 | Loss: 0.00002505
Iteration 26/1000 | Loss: 0.00002504
Iteration 27/1000 | Loss: 0.00002504
Iteration 28/1000 | Loss: 0.00002504
Iteration 29/1000 | Loss: 0.00002504
Iteration 30/1000 | Loss: 0.00002504
Iteration 31/1000 | Loss: 0.00002503
Iteration 32/1000 | Loss: 0.00002503
Iteration 33/1000 | Loss: 0.00002503
Iteration 34/1000 | Loss: 0.00002502
Iteration 35/1000 | Loss: 0.00002502
Iteration 36/1000 | Loss: 0.00002502
Iteration 37/1000 | Loss: 0.00002502
Iteration 38/1000 | Loss: 0.00002502
Iteration 39/1000 | Loss: 0.00002502
Iteration 40/1000 | Loss: 0.00002501
Iteration 41/1000 | Loss: 0.00002501
Iteration 42/1000 | Loss: 0.00002501
Iteration 43/1000 | Loss: 0.00002501
Iteration 44/1000 | Loss: 0.00002501
Iteration 45/1000 | Loss: 0.00002500
Iteration 46/1000 | Loss: 0.00002500
Iteration 47/1000 | Loss: 0.00002500
Iteration 48/1000 | Loss: 0.00002500
Iteration 49/1000 | Loss: 0.00002500
Iteration 50/1000 | Loss: 0.00002500
Iteration 51/1000 | Loss: 0.00002500
Iteration 52/1000 | Loss: 0.00002500
Iteration 53/1000 | Loss: 0.00002500
Iteration 54/1000 | Loss: 0.00002500
Iteration 55/1000 | Loss: 0.00002500
Iteration 56/1000 | Loss: 0.00002500
Iteration 57/1000 | Loss: 0.00002500
Iteration 58/1000 | Loss: 0.00002500
Iteration 59/1000 | Loss: 0.00002499
Iteration 60/1000 | Loss: 0.00002499
Iteration 61/1000 | Loss: 0.00002499
Iteration 62/1000 | Loss: 0.00002499
Iteration 63/1000 | Loss: 0.00002499
Iteration 64/1000 | Loss: 0.00002499
Iteration 65/1000 | Loss: 0.00002498
Iteration 66/1000 | Loss: 0.00002498
Iteration 67/1000 | Loss: 0.00002498
Iteration 68/1000 | Loss: 0.00002498
Iteration 69/1000 | Loss: 0.00002498
Iteration 70/1000 | Loss: 0.00002498
Iteration 71/1000 | Loss: 0.00002497
Iteration 72/1000 | Loss: 0.00002497
Iteration 73/1000 | Loss: 0.00002497
Iteration 74/1000 | Loss: 0.00002497
Iteration 75/1000 | Loss: 0.00002497
Iteration 76/1000 | Loss: 0.00002496
Iteration 77/1000 | Loss: 0.00002496
Iteration 78/1000 | Loss: 0.00002496
Iteration 79/1000 | Loss: 0.00002496
Iteration 80/1000 | Loss: 0.00002496
Iteration 81/1000 | Loss: 0.00002496
Iteration 82/1000 | Loss: 0.00002496
Iteration 83/1000 | Loss: 0.00002496
Iteration 84/1000 | Loss: 0.00002495
Iteration 85/1000 | Loss: 0.00002495
Iteration 86/1000 | Loss: 0.00002495
Iteration 87/1000 | Loss: 0.00002495
Iteration 88/1000 | Loss: 0.00002495
Iteration 89/1000 | Loss: 0.00002495
Iteration 90/1000 | Loss: 0.00002495
Iteration 91/1000 | Loss: 0.00002495
Iteration 92/1000 | Loss: 0.00002495
Iteration 93/1000 | Loss: 0.00002495
Iteration 94/1000 | Loss: 0.00002495
Iteration 95/1000 | Loss: 0.00002495
Iteration 96/1000 | Loss: 0.00002495
Iteration 97/1000 | Loss: 0.00002495
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 97. Stopping optimization.
Last 5 losses: [2.4953413230832666e-05, 2.4953413230832666e-05, 2.4953413230832666e-05, 2.4953413230832666e-05, 2.4953413230832666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4953413230832666e-05

Optimization complete. Final v2v error: 4.21410608291626 mm

Highest mean error: 4.546811103820801 mm for frame 170

Lowest mean error: 3.6905012130737305 mm for frame 13

Saving results

Total time: 44.88010263442993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531730
Iteration 2/25 | Loss: 0.00124379
Iteration 3/25 | Loss: 0.00107975
Iteration 4/25 | Loss: 0.00098845
Iteration 5/25 | Loss: 0.00097835
Iteration 6/25 | Loss: 0.00097414
Iteration 7/25 | Loss: 0.00097275
Iteration 8/25 | Loss: 0.00097210
Iteration 9/25 | Loss: 0.00097186
Iteration 10/25 | Loss: 0.00097162
Iteration 11/25 | Loss: 0.00097142
Iteration 12/25 | Loss: 0.00097137
Iteration 13/25 | Loss: 0.00097137
Iteration 14/25 | Loss: 0.00097137
Iteration 15/25 | Loss: 0.00097137
Iteration 16/25 | Loss: 0.00097137
Iteration 17/25 | Loss: 0.00097137
Iteration 18/25 | Loss: 0.00097137
Iteration 19/25 | Loss: 0.00097137
Iteration 20/25 | Loss: 0.00097137
Iteration 21/25 | Loss: 0.00097137
Iteration 22/25 | Loss: 0.00097137
Iteration 23/25 | Loss: 0.00097136
Iteration 24/25 | Loss: 0.00097136
Iteration 25/25 | Loss: 0.00097136

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.08142567
Iteration 2/25 | Loss: 0.00099083
Iteration 3/25 | Loss: 0.00099080
Iteration 4/25 | Loss: 0.00099080
Iteration 5/25 | Loss: 0.00099080
Iteration 6/25 | Loss: 0.00099080
Iteration 7/25 | Loss: 0.00099080
Iteration 8/25 | Loss: 0.00099080
Iteration 9/25 | Loss: 0.00099080
Iteration 10/25 | Loss: 0.00099080
Iteration 11/25 | Loss: 0.00099080
Iteration 12/25 | Loss: 0.00099080
Iteration 13/25 | Loss: 0.00099080
Iteration 14/25 | Loss: 0.00099080
Iteration 15/25 | Loss: 0.00099080
Iteration 16/25 | Loss: 0.00099080
Iteration 17/25 | Loss: 0.00099080
Iteration 18/25 | Loss: 0.00099080
Iteration 19/25 | Loss: 0.00099080
Iteration 20/25 | Loss: 0.00099080
Iteration 21/25 | Loss: 0.00099080
Iteration 22/25 | Loss: 0.00099080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009907953208312392, 0.0009907953208312392, 0.0009907953208312392, 0.0009907953208312392, 0.0009907953208312392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009907953208312392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099080
Iteration 2/1000 | Loss: 0.00003136
Iteration 3/1000 | Loss: 0.00002233
Iteration 4/1000 | Loss: 0.00001920
Iteration 5/1000 | Loss: 0.00001829
Iteration 6/1000 | Loss: 0.00001783
Iteration 7/1000 | Loss: 0.00001742
Iteration 8/1000 | Loss: 0.00001722
Iteration 9/1000 | Loss: 0.00001720
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001697
Iteration 12/1000 | Loss: 0.00001694
Iteration 13/1000 | Loss: 0.00001683
Iteration 14/1000 | Loss: 0.00001677
Iteration 15/1000 | Loss: 0.00001669
Iteration 16/1000 | Loss: 0.00001663
Iteration 17/1000 | Loss: 0.00001662
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001652
Iteration 21/1000 | Loss: 0.00001652
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001651
Iteration 24/1000 | Loss: 0.00001646
Iteration 25/1000 | Loss: 0.00001646
Iteration 26/1000 | Loss: 0.00001645
Iteration 27/1000 | Loss: 0.00001642
Iteration 28/1000 | Loss: 0.00001642
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001642
Iteration 31/1000 | Loss: 0.00001642
Iteration 32/1000 | Loss: 0.00001642
Iteration 33/1000 | Loss: 0.00001642
Iteration 34/1000 | Loss: 0.00001642
Iteration 35/1000 | Loss: 0.00001641
Iteration 36/1000 | Loss: 0.00001641
Iteration 37/1000 | Loss: 0.00001640
Iteration 38/1000 | Loss: 0.00001640
Iteration 39/1000 | Loss: 0.00001639
Iteration 40/1000 | Loss: 0.00001639
Iteration 41/1000 | Loss: 0.00001639
Iteration 42/1000 | Loss: 0.00001638
Iteration 43/1000 | Loss: 0.00001638
Iteration 44/1000 | Loss: 0.00001638
Iteration 45/1000 | Loss: 0.00001637
Iteration 46/1000 | Loss: 0.00001637
Iteration 47/1000 | Loss: 0.00001637
Iteration 48/1000 | Loss: 0.00001637
Iteration 49/1000 | Loss: 0.00001636
Iteration 50/1000 | Loss: 0.00001635
Iteration 51/1000 | Loss: 0.00001635
Iteration 52/1000 | Loss: 0.00001635
Iteration 53/1000 | Loss: 0.00001635
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001634
Iteration 57/1000 | Loss: 0.00001634
Iteration 58/1000 | Loss: 0.00001633
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001632
Iteration 62/1000 | Loss: 0.00001632
Iteration 63/1000 | Loss: 0.00001632
Iteration 64/1000 | Loss: 0.00001632
Iteration 65/1000 | Loss: 0.00001632
Iteration 66/1000 | Loss: 0.00001632
Iteration 67/1000 | Loss: 0.00001632
Iteration 68/1000 | Loss: 0.00001632
Iteration 69/1000 | Loss: 0.00001632
Iteration 70/1000 | Loss: 0.00001631
Iteration 71/1000 | Loss: 0.00001631
Iteration 72/1000 | Loss: 0.00001631
Iteration 73/1000 | Loss: 0.00001631
Iteration 74/1000 | Loss: 0.00001631
Iteration 75/1000 | Loss: 0.00001631
Iteration 76/1000 | Loss: 0.00001631
Iteration 77/1000 | Loss: 0.00001630
Iteration 78/1000 | Loss: 0.00001630
Iteration 79/1000 | Loss: 0.00001630
Iteration 80/1000 | Loss: 0.00001630
Iteration 81/1000 | Loss: 0.00001630
Iteration 82/1000 | Loss: 0.00001629
Iteration 83/1000 | Loss: 0.00001629
Iteration 84/1000 | Loss: 0.00001629
Iteration 85/1000 | Loss: 0.00001629
Iteration 86/1000 | Loss: 0.00001629
Iteration 87/1000 | Loss: 0.00001629
Iteration 88/1000 | Loss: 0.00001628
Iteration 89/1000 | Loss: 0.00001628
Iteration 90/1000 | Loss: 0.00001628
Iteration 91/1000 | Loss: 0.00001628
Iteration 92/1000 | Loss: 0.00001628
Iteration 93/1000 | Loss: 0.00001627
Iteration 94/1000 | Loss: 0.00001627
Iteration 95/1000 | Loss: 0.00001627
Iteration 96/1000 | Loss: 0.00001627
Iteration 97/1000 | Loss: 0.00001627
Iteration 98/1000 | Loss: 0.00001627
Iteration 99/1000 | Loss: 0.00001627
Iteration 100/1000 | Loss: 0.00001627
Iteration 101/1000 | Loss: 0.00001627
Iteration 102/1000 | Loss: 0.00001627
Iteration 103/1000 | Loss: 0.00001627
Iteration 104/1000 | Loss: 0.00001627
Iteration 105/1000 | Loss: 0.00001627
Iteration 106/1000 | Loss: 0.00001627
Iteration 107/1000 | Loss: 0.00001627
Iteration 108/1000 | Loss: 0.00001627
Iteration 109/1000 | Loss: 0.00001627
Iteration 110/1000 | Loss: 0.00001627
Iteration 111/1000 | Loss: 0.00001627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.6271787899313495e-05, 1.6271787899313495e-05, 1.6271787899313495e-05, 1.6271787899313495e-05, 1.6271787899313495e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6271787899313495e-05

Optimization complete. Final v2v error: 3.477267026901245 mm

Highest mean error: 10.31136703491211 mm for frame 228

Lowest mean error: 3.008544445037842 mm for frame 145

Saving results

Total time: 51.794971227645874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422836
Iteration 2/25 | Loss: 0.00113790
Iteration 3/25 | Loss: 0.00103451
Iteration 4/25 | Loss: 0.00101588
Iteration 5/25 | Loss: 0.00101216
Iteration 6/25 | Loss: 0.00101128
Iteration 7/25 | Loss: 0.00101128
Iteration 8/25 | Loss: 0.00101128
Iteration 9/25 | Loss: 0.00101128
Iteration 10/25 | Loss: 0.00101128
Iteration 11/25 | Loss: 0.00101128
Iteration 12/25 | Loss: 0.00101128
Iteration 13/25 | Loss: 0.00101128
Iteration 14/25 | Loss: 0.00101128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010112817399203777, 0.0010112817399203777, 0.0010112817399203777, 0.0010112817399203777, 0.0010112817399203777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010112817399203777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.51382899
Iteration 2/25 | Loss: 0.00086850
Iteration 3/25 | Loss: 0.00086849
Iteration 4/25 | Loss: 0.00086849
Iteration 5/25 | Loss: 0.00086849
Iteration 6/25 | Loss: 0.00086849
Iteration 7/25 | Loss: 0.00086849
Iteration 8/25 | Loss: 0.00086849
Iteration 9/25 | Loss: 0.00086849
Iteration 10/25 | Loss: 0.00086849
Iteration 11/25 | Loss: 0.00086849
Iteration 12/25 | Loss: 0.00086849
Iteration 13/25 | Loss: 0.00086849
Iteration 14/25 | Loss: 0.00086849
Iteration 15/25 | Loss: 0.00086849
Iteration 16/25 | Loss: 0.00086849
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008684926433488727, 0.0008684926433488727, 0.0008684926433488727, 0.0008684926433488727, 0.0008684926433488727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008684926433488727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086849
Iteration 2/1000 | Loss: 0.00003154
Iteration 3/1000 | Loss: 0.00002411
Iteration 4/1000 | Loss: 0.00002232
Iteration 5/1000 | Loss: 0.00002180
Iteration 6/1000 | Loss: 0.00002125
Iteration 7/1000 | Loss: 0.00002087
Iteration 8/1000 | Loss: 0.00002061
Iteration 9/1000 | Loss: 0.00002044
Iteration 10/1000 | Loss: 0.00002044
Iteration 11/1000 | Loss: 0.00002041
Iteration 12/1000 | Loss: 0.00002040
Iteration 13/1000 | Loss: 0.00002040
Iteration 14/1000 | Loss: 0.00002040
Iteration 15/1000 | Loss: 0.00002039
Iteration 16/1000 | Loss: 0.00002036
Iteration 17/1000 | Loss: 0.00002036
Iteration 18/1000 | Loss: 0.00002036
Iteration 19/1000 | Loss: 0.00002035
Iteration 20/1000 | Loss: 0.00002032
Iteration 21/1000 | Loss: 0.00002030
Iteration 22/1000 | Loss: 0.00002029
Iteration 23/1000 | Loss: 0.00002029
Iteration 24/1000 | Loss: 0.00002028
Iteration 25/1000 | Loss: 0.00002027
Iteration 26/1000 | Loss: 0.00002026
Iteration 27/1000 | Loss: 0.00002026
Iteration 28/1000 | Loss: 0.00002026
Iteration 29/1000 | Loss: 0.00002026
Iteration 30/1000 | Loss: 0.00002025
Iteration 31/1000 | Loss: 0.00002025
Iteration 32/1000 | Loss: 0.00002025
Iteration 33/1000 | Loss: 0.00002025
Iteration 34/1000 | Loss: 0.00002024
Iteration 35/1000 | Loss: 0.00002024
Iteration 36/1000 | Loss: 0.00002024
Iteration 37/1000 | Loss: 0.00002023
Iteration 38/1000 | Loss: 0.00002023
Iteration 39/1000 | Loss: 0.00002023
Iteration 40/1000 | Loss: 0.00002023
Iteration 41/1000 | Loss: 0.00002023
Iteration 42/1000 | Loss: 0.00002023
Iteration 43/1000 | Loss: 0.00002023
Iteration 44/1000 | Loss: 0.00002022
Iteration 45/1000 | Loss: 0.00002022
Iteration 46/1000 | Loss: 0.00002022
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002022
Iteration 50/1000 | Loss: 0.00002022
Iteration 51/1000 | Loss: 0.00002021
Iteration 52/1000 | Loss: 0.00002021
Iteration 53/1000 | Loss: 0.00002021
Iteration 54/1000 | Loss: 0.00002021
Iteration 55/1000 | Loss: 0.00002021
Iteration 56/1000 | Loss: 0.00002020
Iteration 57/1000 | Loss: 0.00002020
Iteration 58/1000 | Loss: 0.00002020
Iteration 59/1000 | Loss: 0.00002019
Iteration 60/1000 | Loss: 0.00002019
Iteration 61/1000 | Loss: 0.00002019
Iteration 62/1000 | Loss: 0.00002019
Iteration 63/1000 | Loss: 0.00002019
Iteration 64/1000 | Loss: 0.00002019
Iteration 65/1000 | Loss: 0.00002019
Iteration 66/1000 | Loss: 0.00002019
Iteration 67/1000 | Loss: 0.00002019
Iteration 68/1000 | Loss: 0.00002019
Iteration 69/1000 | Loss: 0.00002019
Iteration 70/1000 | Loss: 0.00002018
Iteration 71/1000 | Loss: 0.00002018
Iteration 72/1000 | Loss: 0.00002018
Iteration 73/1000 | Loss: 0.00002018
Iteration 74/1000 | Loss: 0.00002018
Iteration 75/1000 | Loss: 0.00002018
Iteration 76/1000 | Loss: 0.00002018
Iteration 77/1000 | Loss: 0.00002017
Iteration 78/1000 | Loss: 0.00002017
Iteration 79/1000 | Loss: 0.00002017
Iteration 80/1000 | Loss: 0.00002017
Iteration 81/1000 | Loss: 0.00002017
Iteration 82/1000 | Loss: 0.00002016
Iteration 83/1000 | Loss: 0.00002016
Iteration 84/1000 | Loss: 0.00002016
Iteration 85/1000 | Loss: 0.00002016
Iteration 86/1000 | Loss: 0.00002016
Iteration 87/1000 | Loss: 0.00002016
Iteration 88/1000 | Loss: 0.00002016
Iteration 89/1000 | Loss: 0.00002016
Iteration 90/1000 | Loss: 0.00002015
Iteration 91/1000 | Loss: 0.00002015
Iteration 92/1000 | Loss: 0.00002015
Iteration 93/1000 | Loss: 0.00002015
Iteration 94/1000 | Loss: 0.00002015
Iteration 95/1000 | Loss: 0.00002015
Iteration 96/1000 | Loss: 0.00002015
Iteration 97/1000 | Loss: 0.00002015
Iteration 98/1000 | Loss: 0.00002015
Iteration 99/1000 | Loss: 0.00002015
Iteration 100/1000 | Loss: 0.00002015
Iteration 101/1000 | Loss: 0.00002015
Iteration 102/1000 | Loss: 0.00002015
Iteration 103/1000 | Loss: 0.00002015
Iteration 104/1000 | Loss: 0.00002015
Iteration 105/1000 | Loss: 0.00002015
Iteration 106/1000 | Loss: 0.00002015
Iteration 107/1000 | Loss: 0.00002015
Iteration 108/1000 | Loss: 0.00002015
Iteration 109/1000 | Loss: 0.00002015
Iteration 110/1000 | Loss: 0.00002015
Iteration 111/1000 | Loss: 0.00002015
Iteration 112/1000 | Loss: 0.00002015
Iteration 113/1000 | Loss: 0.00002015
Iteration 114/1000 | Loss: 0.00002015
Iteration 115/1000 | Loss: 0.00002015
Iteration 116/1000 | Loss: 0.00002015
Iteration 117/1000 | Loss: 0.00002015
Iteration 118/1000 | Loss: 0.00002015
Iteration 119/1000 | Loss: 0.00002015
Iteration 120/1000 | Loss: 0.00002015
Iteration 121/1000 | Loss: 0.00002015
Iteration 122/1000 | Loss: 0.00002015
Iteration 123/1000 | Loss: 0.00002015
Iteration 124/1000 | Loss: 0.00002015
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.0145960661466233e-05, 2.0145960661466233e-05, 2.0145960661466233e-05, 2.0145960661466233e-05, 2.0145960661466233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0145960661466233e-05

Optimization complete. Final v2v error: 3.92519211769104 mm

Highest mean error: 4.257181167602539 mm for frame 98

Lowest mean error: 3.5812344551086426 mm for frame 0

Saving results

Total time: 33.339452266693115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00368084
Iteration 2/25 | Loss: 0.00109179
Iteration 3/25 | Loss: 0.00100870
Iteration 4/25 | Loss: 0.00098858
Iteration 5/25 | Loss: 0.00098387
Iteration 6/25 | Loss: 0.00098253
Iteration 7/25 | Loss: 0.00098245
Iteration 8/25 | Loss: 0.00098245
Iteration 9/25 | Loss: 0.00098245
Iteration 10/25 | Loss: 0.00098245
Iteration 11/25 | Loss: 0.00098245
Iteration 12/25 | Loss: 0.00098245
Iteration 13/25 | Loss: 0.00098245
Iteration 14/25 | Loss: 0.00098245
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0009824464796110988, 0.0009824464796110988, 0.0009824464796110988, 0.0009824464796110988, 0.0009824464796110988]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009824464796110988

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51891136
Iteration 2/25 | Loss: 0.00104668
Iteration 3/25 | Loss: 0.00104667
Iteration 4/25 | Loss: 0.00104667
Iteration 5/25 | Loss: 0.00104667
Iteration 6/25 | Loss: 0.00104667
Iteration 7/25 | Loss: 0.00104667
Iteration 8/25 | Loss: 0.00104667
Iteration 9/25 | Loss: 0.00104667
Iteration 10/25 | Loss: 0.00104667
Iteration 11/25 | Loss: 0.00104667
Iteration 12/25 | Loss: 0.00104667
Iteration 13/25 | Loss: 0.00104667
Iteration 14/25 | Loss: 0.00104667
Iteration 15/25 | Loss: 0.00104667
Iteration 16/25 | Loss: 0.00104667
Iteration 17/25 | Loss: 0.00104667
Iteration 18/25 | Loss: 0.00104667
Iteration 19/25 | Loss: 0.00104667
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010466724634170532, 0.0010466724634170532, 0.0010466724634170532, 0.0010466724634170532, 0.0010466724634170532]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010466724634170532

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104667
Iteration 2/1000 | Loss: 0.00003106
Iteration 3/1000 | Loss: 0.00002080
Iteration 4/1000 | Loss: 0.00001738
Iteration 5/1000 | Loss: 0.00001635
Iteration 6/1000 | Loss: 0.00001579
Iteration 7/1000 | Loss: 0.00001547
Iteration 8/1000 | Loss: 0.00001547
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001546
Iteration 11/1000 | Loss: 0.00001545
Iteration 12/1000 | Loss: 0.00001544
Iteration 13/1000 | Loss: 0.00001532
Iteration 14/1000 | Loss: 0.00001526
Iteration 15/1000 | Loss: 0.00001518
Iteration 16/1000 | Loss: 0.00001518
Iteration 17/1000 | Loss: 0.00001504
Iteration 18/1000 | Loss: 0.00001502
Iteration 19/1000 | Loss: 0.00001498
Iteration 20/1000 | Loss: 0.00001496
Iteration 21/1000 | Loss: 0.00001494
Iteration 22/1000 | Loss: 0.00001488
Iteration 23/1000 | Loss: 0.00001488
Iteration 24/1000 | Loss: 0.00001488
Iteration 25/1000 | Loss: 0.00001487
Iteration 26/1000 | Loss: 0.00001486
Iteration 27/1000 | Loss: 0.00001486
Iteration 28/1000 | Loss: 0.00001485
Iteration 29/1000 | Loss: 0.00001485
Iteration 30/1000 | Loss: 0.00001483
Iteration 31/1000 | Loss: 0.00001483
Iteration 32/1000 | Loss: 0.00001483
Iteration 33/1000 | Loss: 0.00001483
Iteration 34/1000 | Loss: 0.00001483
Iteration 35/1000 | Loss: 0.00001483
Iteration 36/1000 | Loss: 0.00001483
Iteration 37/1000 | Loss: 0.00001482
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001480
Iteration 41/1000 | Loss: 0.00001480
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001479
Iteration 44/1000 | Loss: 0.00001479
Iteration 45/1000 | Loss: 0.00001478
Iteration 46/1000 | Loss: 0.00001478
Iteration 47/1000 | Loss: 0.00001478
Iteration 48/1000 | Loss: 0.00001478
Iteration 49/1000 | Loss: 0.00001478
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001478
Iteration 52/1000 | Loss: 0.00001477
Iteration 53/1000 | Loss: 0.00001476
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001476
Iteration 59/1000 | Loss: 0.00001476
Iteration 60/1000 | Loss: 0.00001476
Iteration 61/1000 | Loss: 0.00001475
Iteration 62/1000 | Loss: 0.00001475
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001474
Iteration 65/1000 | Loss: 0.00001473
Iteration 66/1000 | Loss: 0.00001473
Iteration 67/1000 | Loss: 0.00001473
Iteration 68/1000 | Loss: 0.00001472
Iteration 69/1000 | Loss: 0.00001472
Iteration 70/1000 | Loss: 0.00001472
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001472
Iteration 73/1000 | Loss: 0.00001472
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001472
Iteration 76/1000 | Loss: 0.00001472
Iteration 77/1000 | Loss: 0.00001472
Iteration 78/1000 | Loss: 0.00001472
Iteration 79/1000 | Loss: 0.00001472
Iteration 80/1000 | Loss: 0.00001472
Iteration 81/1000 | Loss: 0.00001471
Iteration 82/1000 | Loss: 0.00001471
Iteration 83/1000 | Loss: 0.00001471
Iteration 84/1000 | Loss: 0.00001470
Iteration 85/1000 | Loss: 0.00001470
Iteration 86/1000 | Loss: 0.00001470
Iteration 87/1000 | Loss: 0.00001470
Iteration 88/1000 | Loss: 0.00001470
Iteration 89/1000 | Loss: 0.00001470
Iteration 90/1000 | Loss: 0.00001470
Iteration 91/1000 | Loss: 0.00001470
Iteration 92/1000 | Loss: 0.00001470
Iteration 93/1000 | Loss: 0.00001470
Iteration 94/1000 | Loss: 0.00001470
Iteration 95/1000 | Loss: 0.00001470
Iteration 96/1000 | Loss: 0.00001470
Iteration 97/1000 | Loss: 0.00001470
Iteration 98/1000 | Loss: 0.00001469
Iteration 99/1000 | Loss: 0.00001469
Iteration 100/1000 | Loss: 0.00001469
Iteration 101/1000 | Loss: 0.00001469
Iteration 102/1000 | Loss: 0.00001469
Iteration 103/1000 | Loss: 0.00001469
Iteration 104/1000 | Loss: 0.00001469
Iteration 105/1000 | Loss: 0.00001469
Iteration 106/1000 | Loss: 0.00001469
Iteration 107/1000 | Loss: 0.00001469
Iteration 108/1000 | Loss: 0.00001469
Iteration 109/1000 | Loss: 0.00001468
Iteration 110/1000 | Loss: 0.00001468
Iteration 111/1000 | Loss: 0.00001468
Iteration 112/1000 | Loss: 0.00001468
Iteration 113/1000 | Loss: 0.00001468
Iteration 114/1000 | Loss: 0.00001468
Iteration 115/1000 | Loss: 0.00001468
Iteration 116/1000 | Loss: 0.00001468
Iteration 117/1000 | Loss: 0.00001468
Iteration 118/1000 | Loss: 0.00001468
Iteration 119/1000 | Loss: 0.00001468
Iteration 120/1000 | Loss: 0.00001468
Iteration 121/1000 | Loss: 0.00001468
Iteration 122/1000 | Loss: 0.00001468
Iteration 123/1000 | Loss: 0.00001468
Iteration 124/1000 | Loss: 0.00001468
Iteration 125/1000 | Loss: 0.00001468
Iteration 126/1000 | Loss: 0.00001468
Iteration 127/1000 | Loss: 0.00001468
Iteration 128/1000 | Loss: 0.00001468
Iteration 129/1000 | Loss: 0.00001468
Iteration 130/1000 | Loss: 0.00001468
Iteration 131/1000 | Loss: 0.00001468
Iteration 132/1000 | Loss: 0.00001468
Iteration 133/1000 | Loss: 0.00001468
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001468
Iteration 137/1000 | Loss: 0.00001468
Iteration 138/1000 | Loss: 0.00001468
Iteration 139/1000 | Loss: 0.00001468
Iteration 140/1000 | Loss: 0.00001468
Iteration 141/1000 | Loss: 0.00001468
Iteration 142/1000 | Loss: 0.00001468
Iteration 143/1000 | Loss: 0.00001468
Iteration 144/1000 | Loss: 0.00001468
Iteration 145/1000 | Loss: 0.00001468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [1.4683062545373105e-05, 1.4683062545373105e-05, 1.4683062545373105e-05, 1.4683062545373105e-05, 1.4683062545373105e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4683062545373105e-05

Optimization complete. Final v2v error: 3.348865032196045 mm

Highest mean error: 3.9643733501434326 mm for frame 9

Lowest mean error: 2.9801697731018066 mm for frame 110

Saving results

Total time: 32.54596447944641
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074129
Iteration 2/25 | Loss: 0.00157832
Iteration 3/25 | Loss: 0.00121523
Iteration 4/25 | Loss: 0.00109573
Iteration 5/25 | Loss: 0.00107391
Iteration 6/25 | Loss: 0.00106802
Iteration 7/25 | Loss: 0.00107002
Iteration 8/25 | Loss: 0.00107025
Iteration 9/25 | Loss: 0.00108014
Iteration 10/25 | Loss: 0.00106156
Iteration 11/25 | Loss: 0.00105808
Iteration 12/25 | Loss: 0.00106125
Iteration 13/25 | Loss: 0.00105820
Iteration 14/25 | Loss: 0.00105857
Iteration 15/25 | Loss: 0.00105701
Iteration 16/25 | Loss: 0.00105447
Iteration 17/25 | Loss: 0.00105245
Iteration 18/25 | Loss: 0.00105188
Iteration 19/25 | Loss: 0.00105174
Iteration 20/25 | Loss: 0.00105157
Iteration 21/25 | Loss: 0.00105276
Iteration 22/25 | Loss: 0.00105071
Iteration 23/25 | Loss: 0.00105008
Iteration 24/25 | Loss: 0.00104989
Iteration 25/25 | Loss: 0.00104986

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63953567
Iteration 2/25 | Loss: 0.00098931
Iteration 3/25 | Loss: 0.00098930
Iteration 4/25 | Loss: 0.00098930
Iteration 5/25 | Loss: 0.00098930
Iteration 6/25 | Loss: 0.00098930
Iteration 7/25 | Loss: 0.00098930
Iteration 8/25 | Loss: 0.00098930
Iteration 9/25 | Loss: 0.00098930
Iteration 10/25 | Loss: 0.00098930
Iteration 11/25 | Loss: 0.00098930
Iteration 12/25 | Loss: 0.00098930
Iteration 13/25 | Loss: 0.00098930
Iteration 14/25 | Loss: 0.00098930
Iteration 15/25 | Loss: 0.00098930
Iteration 16/25 | Loss: 0.00098930
Iteration 17/25 | Loss: 0.00098930
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009892989182844758, 0.0009892989182844758, 0.0009892989182844758, 0.0009892989182844758, 0.0009892989182844758]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009892989182844758

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098930
Iteration 2/1000 | Loss: 0.00003755
Iteration 3/1000 | Loss: 0.00002542
Iteration 4/1000 | Loss: 0.00002216
Iteration 5/1000 | Loss: 0.00002106
Iteration 6/1000 | Loss: 0.00002046
Iteration 7/1000 | Loss: 0.00002002
Iteration 8/1000 | Loss: 0.00001963
Iteration 9/1000 | Loss: 0.00001929
Iteration 10/1000 | Loss: 0.00001901
Iteration 11/1000 | Loss: 0.00001901
Iteration 12/1000 | Loss: 0.00001888
Iteration 13/1000 | Loss: 0.00001887
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001884
Iteration 16/1000 | Loss: 0.00001883
Iteration 17/1000 | Loss: 0.00001877
Iteration 18/1000 | Loss: 0.00001873
Iteration 19/1000 | Loss: 0.00001865
Iteration 20/1000 | Loss: 0.00001861
Iteration 21/1000 | Loss: 0.00001860
Iteration 22/1000 | Loss: 0.00001860
Iteration 23/1000 | Loss: 0.00001857
Iteration 24/1000 | Loss: 0.00001856
Iteration 25/1000 | Loss: 0.00001856
Iteration 26/1000 | Loss: 0.00001855
Iteration 27/1000 | Loss: 0.00001855
Iteration 28/1000 | Loss: 0.00001855
Iteration 29/1000 | Loss: 0.00001852
Iteration 30/1000 | Loss: 0.00001851
Iteration 31/1000 | Loss: 0.00001851
Iteration 32/1000 | Loss: 0.00001851
Iteration 33/1000 | Loss: 0.00001850
Iteration 34/1000 | Loss: 0.00001849
Iteration 35/1000 | Loss: 0.00001849
Iteration 36/1000 | Loss: 0.00001849
Iteration 37/1000 | Loss: 0.00001849
Iteration 38/1000 | Loss: 0.00001848
Iteration 39/1000 | Loss: 0.00001848
Iteration 40/1000 | Loss: 0.00001848
Iteration 41/1000 | Loss: 0.00001848
Iteration 42/1000 | Loss: 0.00001848
Iteration 43/1000 | Loss: 0.00001848
Iteration 44/1000 | Loss: 0.00001848
Iteration 45/1000 | Loss: 0.00001847
Iteration 46/1000 | Loss: 0.00001847
Iteration 47/1000 | Loss: 0.00001847
Iteration 48/1000 | Loss: 0.00001847
Iteration 49/1000 | Loss: 0.00001846
Iteration 50/1000 | Loss: 0.00001846
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001845
Iteration 53/1000 | Loss: 0.00001845
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001845
Iteration 56/1000 | Loss: 0.00001845
Iteration 57/1000 | Loss: 0.00001844
Iteration 58/1000 | Loss: 0.00001844
Iteration 59/1000 | Loss: 0.00001844
Iteration 60/1000 | Loss: 0.00001844
Iteration 61/1000 | Loss: 0.00001844
Iteration 62/1000 | Loss: 0.00001844
Iteration 63/1000 | Loss: 0.00001844
Iteration 64/1000 | Loss: 0.00001844
Iteration 65/1000 | Loss: 0.00001844
Iteration 66/1000 | Loss: 0.00001843
Iteration 67/1000 | Loss: 0.00001843
Iteration 68/1000 | Loss: 0.00001843
Iteration 69/1000 | Loss: 0.00001843
Iteration 70/1000 | Loss: 0.00001843
Iteration 71/1000 | Loss: 0.00001843
Iteration 72/1000 | Loss: 0.00001843
Iteration 73/1000 | Loss: 0.00001843
Iteration 74/1000 | Loss: 0.00001843
Iteration 75/1000 | Loss: 0.00001843
Iteration 76/1000 | Loss: 0.00001843
Iteration 77/1000 | Loss: 0.00001843
Iteration 78/1000 | Loss: 0.00001843
Iteration 79/1000 | Loss: 0.00001843
Iteration 80/1000 | Loss: 0.00001843
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001843
Iteration 84/1000 | Loss: 0.00001843
Iteration 85/1000 | Loss: 0.00001843
Iteration 86/1000 | Loss: 0.00001843
Iteration 87/1000 | Loss: 0.00001843
Iteration 88/1000 | Loss: 0.00001843
Iteration 89/1000 | Loss: 0.00001843
Iteration 90/1000 | Loss: 0.00001843
Iteration 91/1000 | Loss: 0.00001843
Iteration 92/1000 | Loss: 0.00001843
Iteration 93/1000 | Loss: 0.00001843
Iteration 94/1000 | Loss: 0.00001843
Iteration 95/1000 | Loss: 0.00001843
Iteration 96/1000 | Loss: 0.00001843
Iteration 97/1000 | Loss: 0.00001843
Iteration 98/1000 | Loss: 0.00001843
Iteration 99/1000 | Loss: 0.00001843
Iteration 100/1000 | Loss: 0.00001843
Iteration 101/1000 | Loss: 0.00001843
Iteration 102/1000 | Loss: 0.00001843
Iteration 103/1000 | Loss: 0.00001843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.842896017478779e-05, 1.842896017478779e-05, 1.842896017478779e-05, 1.842896017478779e-05, 1.842896017478779e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.842896017478779e-05

Optimization complete. Final v2v error: 3.6992619037628174 mm

Highest mean error: 4.066090106964111 mm for frame 86

Lowest mean error: 3.3197174072265625 mm for frame 28

Saving results

Total time: 76.77912425994873
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373735
Iteration 2/25 | Loss: 0.00123647
Iteration 3/25 | Loss: 0.00105910
Iteration 4/25 | Loss: 0.00102276
Iteration 5/25 | Loss: 0.00101160
Iteration 6/25 | Loss: 0.00100858
Iteration 7/25 | Loss: 0.00100787
Iteration 8/25 | Loss: 0.00100787
Iteration 9/25 | Loss: 0.00100787
Iteration 10/25 | Loss: 0.00100787
Iteration 11/25 | Loss: 0.00100787
Iteration 12/25 | Loss: 0.00100787
Iteration 13/25 | Loss: 0.00100787
Iteration 14/25 | Loss: 0.00100787
Iteration 15/25 | Loss: 0.00100787
Iteration 16/25 | Loss: 0.00100787
Iteration 17/25 | Loss: 0.00100787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010078696068376303, 0.0010078696068376303, 0.0010078696068376303, 0.0010078696068376303, 0.0010078696068376303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010078696068376303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50913775
Iteration 2/25 | Loss: 0.00098590
Iteration 3/25 | Loss: 0.00098589
Iteration 4/25 | Loss: 0.00098589
Iteration 5/25 | Loss: 0.00098589
Iteration 6/25 | Loss: 0.00098589
Iteration 7/25 | Loss: 0.00098589
Iteration 8/25 | Loss: 0.00098588
Iteration 9/25 | Loss: 0.00098588
Iteration 10/25 | Loss: 0.00098588
Iteration 11/25 | Loss: 0.00098588
Iteration 12/25 | Loss: 0.00098588
Iteration 13/25 | Loss: 0.00098588
Iteration 14/25 | Loss: 0.00098588
Iteration 15/25 | Loss: 0.00098588
Iteration 16/25 | Loss: 0.00098588
Iteration 17/25 | Loss: 0.00098588
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009858841076493263, 0.0009858841076493263, 0.0009858841076493263, 0.0009858841076493263, 0.0009858841076493263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009858841076493263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098588
Iteration 2/1000 | Loss: 0.00003572
Iteration 3/1000 | Loss: 0.00002421
Iteration 4/1000 | Loss: 0.00002053
Iteration 5/1000 | Loss: 0.00001911
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001805
Iteration 8/1000 | Loss: 0.00001761
Iteration 9/1000 | Loss: 0.00001734
Iteration 10/1000 | Loss: 0.00001726
Iteration 11/1000 | Loss: 0.00001719
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001717
Iteration 14/1000 | Loss: 0.00001715
Iteration 15/1000 | Loss: 0.00001715
Iteration 16/1000 | Loss: 0.00001714
Iteration 17/1000 | Loss: 0.00001713
Iteration 18/1000 | Loss: 0.00001713
Iteration 19/1000 | Loss: 0.00001713
Iteration 20/1000 | Loss: 0.00001712
Iteration 21/1000 | Loss: 0.00001712
Iteration 22/1000 | Loss: 0.00001711
Iteration 23/1000 | Loss: 0.00001709
Iteration 24/1000 | Loss: 0.00001704
Iteration 25/1000 | Loss: 0.00001703
Iteration 26/1000 | Loss: 0.00001702
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001698
Iteration 29/1000 | Loss: 0.00001698
Iteration 30/1000 | Loss: 0.00001696
Iteration 31/1000 | Loss: 0.00001696
Iteration 32/1000 | Loss: 0.00001696
Iteration 33/1000 | Loss: 0.00001695
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00001694
Iteration 37/1000 | Loss: 0.00001693
Iteration 38/1000 | Loss: 0.00001693
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001690
Iteration 45/1000 | Loss: 0.00001690
Iteration 46/1000 | Loss: 0.00001690
Iteration 47/1000 | Loss: 0.00001689
Iteration 48/1000 | Loss: 0.00001689
Iteration 49/1000 | Loss: 0.00001689
Iteration 50/1000 | Loss: 0.00001688
Iteration 51/1000 | Loss: 0.00001688
Iteration 52/1000 | Loss: 0.00001688
Iteration 53/1000 | Loss: 0.00001687
Iteration 54/1000 | Loss: 0.00001687
Iteration 55/1000 | Loss: 0.00001687
Iteration 56/1000 | Loss: 0.00001687
Iteration 57/1000 | Loss: 0.00001686
Iteration 58/1000 | Loss: 0.00001686
Iteration 59/1000 | Loss: 0.00001686
Iteration 60/1000 | Loss: 0.00001686
Iteration 61/1000 | Loss: 0.00001685
Iteration 62/1000 | Loss: 0.00001685
Iteration 63/1000 | Loss: 0.00001685
Iteration 64/1000 | Loss: 0.00001685
Iteration 65/1000 | Loss: 0.00001685
Iteration 66/1000 | Loss: 0.00001685
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001684
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001684
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001684
Iteration 76/1000 | Loss: 0.00001684
Iteration 77/1000 | Loss: 0.00001684
Iteration 78/1000 | Loss: 0.00001684
Iteration 79/1000 | Loss: 0.00001684
Iteration 80/1000 | Loss: 0.00001684
Iteration 81/1000 | Loss: 0.00001684
Iteration 82/1000 | Loss: 0.00001684
Iteration 83/1000 | Loss: 0.00001684
Iteration 84/1000 | Loss: 0.00001684
Iteration 85/1000 | Loss: 0.00001684
Iteration 86/1000 | Loss: 0.00001684
Iteration 87/1000 | Loss: 0.00001684
Iteration 88/1000 | Loss: 0.00001684
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001684
Iteration 92/1000 | Loss: 0.00001684
Iteration 93/1000 | Loss: 0.00001684
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.6835143469506875e-05, 1.6835143469506875e-05, 1.6835143469506875e-05, 1.6835143469506875e-05, 1.6835143469506875e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6835143469506875e-05

Optimization complete. Final v2v error: 3.541806936264038 mm

Highest mean error: 3.8267264366149902 mm for frame 8

Lowest mean error: 3.307454824447632 mm for frame 68

Saving results

Total time: 37.107821464538574
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498826
Iteration 2/25 | Loss: 0.00148539
Iteration 3/25 | Loss: 0.00117373
Iteration 4/25 | Loss: 0.00113183
Iteration 5/25 | Loss: 0.00112083
Iteration 6/25 | Loss: 0.00111791
Iteration 7/25 | Loss: 0.00111690
Iteration 8/25 | Loss: 0.00111674
Iteration 9/25 | Loss: 0.00111674
Iteration 10/25 | Loss: 0.00111674
Iteration 11/25 | Loss: 0.00111674
Iteration 12/25 | Loss: 0.00111674
Iteration 13/25 | Loss: 0.00111674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001116743660531938, 0.001116743660531938, 0.001116743660531938, 0.001116743660531938, 0.001116743660531938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001116743660531938

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26359808
Iteration 2/25 | Loss: 0.00076149
Iteration 3/25 | Loss: 0.00076149
Iteration 4/25 | Loss: 0.00076149
Iteration 5/25 | Loss: 0.00076149
Iteration 6/25 | Loss: 0.00076149
Iteration 7/25 | Loss: 0.00076149
Iteration 8/25 | Loss: 0.00076149
Iteration 9/25 | Loss: 0.00076149
Iteration 10/25 | Loss: 0.00076149
Iteration 11/25 | Loss: 0.00076149
Iteration 12/25 | Loss: 0.00076149
Iteration 13/25 | Loss: 0.00076149
Iteration 14/25 | Loss: 0.00076149
Iteration 15/25 | Loss: 0.00076149
Iteration 16/25 | Loss: 0.00076149
Iteration 17/25 | Loss: 0.00076149
Iteration 18/25 | Loss: 0.00076149
Iteration 19/25 | Loss: 0.00076149
Iteration 20/25 | Loss: 0.00076149
Iteration 21/25 | Loss: 0.00076149
Iteration 22/25 | Loss: 0.00076149
Iteration 23/25 | Loss: 0.00076149
Iteration 24/25 | Loss: 0.00076149
Iteration 25/25 | Loss: 0.00076149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076149
Iteration 2/1000 | Loss: 0.00005962
Iteration 3/1000 | Loss: 0.00004717
Iteration 4/1000 | Loss: 0.00004153
Iteration 5/1000 | Loss: 0.00003882
Iteration 6/1000 | Loss: 0.00003726
Iteration 7/1000 | Loss: 0.00003646
Iteration 8/1000 | Loss: 0.00003579
Iteration 9/1000 | Loss: 0.00003536
Iteration 10/1000 | Loss: 0.00003496
Iteration 11/1000 | Loss: 0.00003459
Iteration 12/1000 | Loss: 0.00003430
Iteration 13/1000 | Loss: 0.00003408
Iteration 14/1000 | Loss: 0.00003388
Iteration 15/1000 | Loss: 0.00003377
Iteration 16/1000 | Loss: 0.00003377
Iteration 17/1000 | Loss: 0.00003376
Iteration 18/1000 | Loss: 0.00003375
Iteration 19/1000 | Loss: 0.00003374
Iteration 20/1000 | Loss: 0.00003373
Iteration 21/1000 | Loss: 0.00003371
Iteration 22/1000 | Loss: 0.00003371
Iteration 23/1000 | Loss: 0.00003370
Iteration 24/1000 | Loss: 0.00003370
Iteration 25/1000 | Loss: 0.00003370
Iteration 26/1000 | Loss: 0.00003369
Iteration 27/1000 | Loss: 0.00003369
Iteration 28/1000 | Loss: 0.00003369
Iteration 29/1000 | Loss: 0.00003369
Iteration 30/1000 | Loss: 0.00003369
Iteration 31/1000 | Loss: 0.00003369
Iteration 32/1000 | Loss: 0.00003369
Iteration 33/1000 | Loss: 0.00003368
Iteration 34/1000 | Loss: 0.00003366
Iteration 35/1000 | Loss: 0.00003366
Iteration 36/1000 | Loss: 0.00003366
Iteration 37/1000 | Loss: 0.00003366
Iteration 38/1000 | Loss: 0.00003366
Iteration 39/1000 | Loss: 0.00003366
Iteration 40/1000 | Loss: 0.00003366
Iteration 41/1000 | Loss: 0.00003366
Iteration 42/1000 | Loss: 0.00003366
Iteration 43/1000 | Loss: 0.00003366
Iteration 44/1000 | Loss: 0.00003366
Iteration 45/1000 | Loss: 0.00003365
Iteration 46/1000 | Loss: 0.00003365
Iteration 47/1000 | Loss: 0.00003365
Iteration 48/1000 | Loss: 0.00003365
Iteration 49/1000 | Loss: 0.00003364
Iteration 50/1000 | Loss: 0.00003364
Iteration 51/1000 | Loss: 0.00003364
Iteration 52/1000 | Loss: 0.00003363
Iteration 53/1000 | Loss: 0.00003363
Iteration 54/1000 | Loss: 0.00003362
Iteration 55/1000 | Loss: 0.00003362
Iteration 56/1000 | Loss: 0.00003361
Iteration 57/1000 | Loss: 0.00003361
Iteration 58/1000 | Loss: 0.00003361
Iteration 59/1000 | Loss: 0.00003361
Iteration 60/1000 | Loss: 0.00003361
Iteration 61/1000 | Loss: 0.00003361
Iteration 62/1000 | Loss: 0.00003361
Iteration 63/1000 | Loss: 0.00003361
Iteration 64/1000 | Loss: 0.00003361
Iteration 65/1000 | Loss: 0.00003360
Iteration 66/1000 | Loss: 0.00003360
Iteration 67/1000 | Loss: 0.00003360
Iteration 68/1000 | Loss: 0.00003360
Iteration 69/1000 | Loss: 0.00003359
Iteration 70/1000 | Loss: 0.00003359
Iteration 71/1000 | Loss: 0.00003359
Iteration 72/1000 | Loss: 0.00003359
Iteration 73/1000 | Loss: 0.00003358
Iteration 74/1000 | Loss: 0.00003358
Iteration 75/1000 | Loss: 0.00003357
Iteration 76/1000 | Loss: 0.00003357
Iteration 77/1000 | Loss: 0.00003357
Iteration 78/1000 | Loss: 0.00003357
Iteration 79/1000 | Loss: 0.00003357
Iteration 80/1000 | Loss: 0.00003357
Iteration 81/1000 | Loss: 0.00003357
Iteration 82/1000 | Loss: 0.00003357
Iteration 83/1000 | Loss: 0.00003356
Iteration 84/1000 | Loss: 0.00003356
Iteration 85/1000 | Loss: 0.00003356
Iteration 86/1000 | Loss: 0.00003356
Iteration 87/1000 | Loss: 0.00003356
Iteration 88/1000 | Loss: 0.00003356
Iteration 89/1000 | Loss: 0.00003356
Iteration 90/1000 | Loss: 0.00003356
Iteration 91/1000 | Loss: 0.00003356
Iteration 92/1000 | Loss: 0.00003356
Iteration 93/1000 | Loss: 0.00003355
Iteration 94/1000 | Loss: 0.00003355
Iteration 95/1000 | Loss: 0.00003355
Iteration 96/1000 | Loss: 0.00003355
Iteration 97/1000 | Loss: 0.00003355
Iteration 98/1000 | Loss: 0.00003355
Iteration 99/1000 | Loss: 0.00003355
Iteration 100/1000 | Loss: 0.00003355
Iteration 101/1000 | Loss: 0.00003355
Iteration 102/1000 | Loss: 0.00003355
Iteration 103/1000 | Loss: 0.00003354
Iteration 104/1000 | Loss: 0.00003354
Iteration 105/1000 | Loss: 0.00003354
Iteration 106/1000 | Loss: 0.00003354
Iteration 107/1000 | Loss: 0.00003354
Iteration 108/1000 | Loss: 0.00003353
Iteration 109/1000 | Loss: 0.00003353
Iteration 110/1000 | Loss: 0.00003353
Iteration 111/1000 | Loss: 0.00003353
Iteration 112/1000 | Loss: 0.00003353
Iteration 113/1000 | Loss: 0.00003353
Iteration 114/1000 | Loss: 0.00003353
Iteration 115/1000 | Loss: 0.00003353
Iteration 116/1000 | Loss: 0.00003353
Iteration 117/1000 | Loss: 0.00003353
Iteration 118/1000 | Loss: 0.00003353
Iteration 119/1000 | Loss: 0.00003353
Iteration 120/1000 | Loss: 0.00003353
Iteration 121/1000 | Loss: 0.00003353
Iteration 122/1000 | Loss: 0.00003352
Iteration 123/1000 | Loss: 0.00003352
Iteration 124/1000 | Loss: 0.00003352
Iteration 125/1000 | Loss: 0.00003352
Iteration 126/1000 | Loss: 0.00003352
Iteration 127/1000 | Loss: 0.00003352
Iteration 128/1000 | Loss: 0.00003352
Iteration 129/1000 | Loss: 0.00003352
Iteration 130/1000 | Loss: 0.00003352
Iteration 131/1000 | Loss: 0.00003352
Iteration 132/1000 | Loss: 0.00003352
Iteration 133/1000 | Loss: 0.00003351
Iteration 134/1000 | Loss: 0.00003351
Iteration 135/1000 | Loss: 0.00003351
Iteration 136/1000 | Loss: 0.00003351
Iteration 137/1000 | Loss: 0.00003351
Iteration 138/1000 | Loss: 0.00003351
Iteration 139/1000 | Loss: 0.00003350
Iteration 140/1000 | Loss: 0.00003350
Iteration 141/1000 | Loss: 0.00003350
Iteration 142/1000 | Loss: 0.00003350
Iteration 143/1000 | Loss: 0.00003350
Iteration 144/1000 | Loss: 0.00003350
Iteration 145/1000 | Loss: 0.00003350
Iteration 146/1000 | Loss: 0.00003350
Iteration 147/1000 | Loss: 0.00003350
Iteration 148/1000 | Loss: 0.00003350
Iteration 149/1000 | Loss: 0.00003350
Iteration 150/1000 | Loss: 0.00003350
Iteration 151/1000 | Loss: 0.00003349
Iteration 152/1000 | Loss: 0.00003349
Iteration 153/1000 | Loss: 0.00003349
Iteration 154/1000 | Loss: 0.00003349
Iteration 155/1000 | Loss: 0.00003349
Iteration 156/1000 | Loss: 0.00003349
Iteration 157/1000 | Loss: 0.00003349
Iteration 158/1000 | Loss: 0.00003348
Iteration 159/1000 | Loss: 0.00003348
Iteration 160/1000 | Loss: 0.00003348
Iteration 161/1000 | Loss: 0.00003348
Iteration 162/1000 | Loss: 0.00003348
Iteration 163/1000 | Loss: 0.00003348
Iteration 164/1000 | Loss: 0.00003347
Iteration 165/1000 | Loss: 0.00003347
Iteration 166/1000 | Loss: 0.00003347
Iteration 167/1000 | Loss: 0.00003347
Iteration 168/1000 | Loss: 0.00003347
Iteration 169/1000 | Loss: 0.00003347
Iteration 170/1000 | Loss: 0.00003347
Iteration 171/1000 | Loss: 0.00003346
Iteration 172/1000 | Loss: 0.00003346
Iteration 173/1000 | Loss: 0.00003346
Iteration 174/1000 | Loss: 0.00003345
Iteration 175/1000 | Loss: 0.00003345
Iteration 176/1000 | Loss: 0.00003345
Iteration 177/1000 | Loss: 0.00003345
Iteration 178/1000 | Loss: 0.00003345
Iteration 179/1000 | Loss: 0.00003345
Iteration 180/1000 | Loss: 0.00003345
Iteration 181/1000 | Loss: 0.00003344
Iteration 182/1000 | Loss: 0.00003344
Iteration 183/1000 | Loss: 0.00003344
Iteration 184/1000 | Loss: 0.00003344
Iteration 185/1000 | Loss: 0.00003344
Iteration 186/1000 | Loss: 0.00003344
Iteration 187/1000 | Loss: 0.00003344
Iteration 188/1000 | Loss: 0.00003344
Iteration 189/1000 | Loss: 0.00003344
Iteration 190/1000 | Loss: 0.00003344
Iteration 191/1000 | Loss: 0.00003344
Iteration 192/1000 | Loss: 0.00003344
Iteration 193/1000 | Loss: 0.00003343
Iteration 194/1000 | Loss: 0.00003343
Iteration 195/1000 | Loss: 0.00003343
Iteration 196/1000 | Loss: 0.00003343
Iteration 197/1000 | Loss: 0.00003343
Iteration 198/1000 | Loss: 0.00003343
Iteration 199/1000 | Loss: 0.00003343
Iteration 200/1000 | Loss: 0.00003343
Iteration 201/1000 | Loss: 0.00003343
Iteration 202/1000 | Loss: 0.00003343
Iteration 203/1000 | Loss: 0.00003343
Iteration 204/1000 | Loss: 0.00003343
Iteration 205/1000 | Loss: 0.00003343
Iteration 206/1000 | Loss: 0.00003342
Iteration 207/1000 | Loss: 0.00003342
Iteration 208/1000 | Loss: 0.00003342
Iteration 209/1000 | Loss: 0.00003342
Iteration 210/1000 | Loss: 0.00003342
Iteration 211/1000 | Loss: 0.00003341
Iteration 212/1000 | Loss: 0.00003341
Iteration 213/1000 | Loss: 0.00003341
Iteration 214/1000 | Loss: 0.00003341
Iteration 215/1000 | Loss: 0.00003341
Iteration 216/1000 | Loss: 0.00003341
Iteration 217/1000 | Loss: 0.00003341
Iteration 218/1000 | Loss: 0.00003341
Iteration 219/1000 | Loss: 0.00003341
Iteration 220/1000 | Loss: 0.00003341
Iteration 221/1000 | Loss: 0.00003340
Iteration 222/1000 | Loss: 0.00003340
Iteration 223/1000 | Loss: 0.00003340
Iteration 224/1000 | Loss: 0.00003340
Iteration 225/1000 | Loss: 0.00003340
Iteration 226/1000 | Loss: 0.00003340
Iteration 227/1000 | Loss: 0.00003340
Iteration 228/1000 | Loss: 0.00003340
Iteration 229/1000 | Loss: 0.00003339
Iteration 230/1000 | Loss: 0.00003339
Iteration 231/1000 | Loss: 0.00003339
Iteration 232/1000 | Loss: 0.00003339
Iteration 233/1000 | Loss: 0.00003339
Iteration 234/1000 | Loss: 0.00003339
Iteration 235/1000 | Loss: 0.00003339
Iteration 236/1000 | Loss: 0.00003339
Iteration 237/1000 | Loss: 0.00003339
Iteration 238/1000 | Loss: 0.00003339
Iteration 239/1000 | Loss: 0.00003338
Iteration 240/1000 | Loss: 0.00003338
Iteration 241/1000 | Loss: 0.00003338
Iteration 242/1000 | Loss: 0.00003338
Iteration 243/1000 | Loss: 0.00003337
Iteration 244/1000 | Loss: 0.00003337
Iteration 245/1000 | Loss: 0.00003337
Iteration 246/1000 | Loss: 0.00003337
Iteration 247/1000 | Loss: 0.00003337
Iteration 248/1000 | Loss: 0.00003337
Iteration 249/1000 | Loss: 0.00003337
Iteration 250/1000 | Loss: 0.00003337
Iteration 251/1000 | Loss: 0.00003337
Iteration 252/1000 | Loss: 0.00003337
Iteration 253/1000 | Loss: 0.00003337
Iteration 254/1000 | Loss: 0.00003337
Iteration 255/1000 | Loss: 0.00003337
Iteration 256/1000 | Loss: 0.00003337
Iteration 257/1000 | Loss: 0.00003337
Iteration 258/1000 | Loss: 0.00003337
Iteration 259/1000 | Loss: 0.00003337
Iteration 260/1000 | Loss: 0.00003337
Iteration 261/1000 | Loss: 0.00003337
Iteration 262/1000 | Loss: 0.00003337
Iteration 263/1000 | Loss: 0.00003337
Iteration 264/1000 | Loss: 0.00003337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [3.33679563482292e-05, 3.33679563482292e-05, 3.33679563482292e-05, 3.33679563482292e-05, 3.33679563482292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.33679563482292e-05

Optimization complete. Final v2v error: 4.63311243057251 mm

Highest mean error: 6.26401424407959 mm for frame 75

Lowest mean error: 3.845001220703125 mm for frame 35

Saving results

Total time: 47.947447061538696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00540845
Iteration 2/25 | Loss: 0.00135765
Iteration 3/25 | Loss: 0.00106531
Iteration 4/25 | Loss: 0.00104297
Iteration 5/25 | Loss: 0.00103696
Iteration 6/25 | Loss: 0.00103512
Iteration 7/25 | Loss: 0.00103468
Iteration 8/25 | Loss: 0.00103468
Iteration 9/25 | Loss: 0.00103468
Iteration 10/25 | Loss: 0.00103468
Iteration 11/25 | Loss: 0.00103468
Iteration 12/25 | Loss: 0.00103468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010346773779019713, 0.0010346773779019713, 0.0010346773779019713, 0.0010346773779019713, 0.0010346773779019713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010346773779019713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54008019
Iteration 2/25 | Loss: 0.00097160
Iteration 3/25 | Loss: 0.00097159
Iteration 4/25 | Loss: 0.00097159
Iteration 5/25 | Loss: 0.00097159
Iteration 6/25 | Loss: 0.00097159
Iteration 7/25 | Loss: 0.00097159
Iteration 8/25 | Loss: 0.00097159
Iteration 9/25 | Loss: 0.00097159
Iteration 10/25 | Loss: 0.00097159
Iteration 11/25 | Loss: 0.00097159
Iteration 12/25 | Loss: 0.00097159
Iteration 13/25 | Loss: 0.00097159
Iteration 14/25 | Loss: 0.00097159
Iteration 15/25 | Loss: 0.00097159
Iteration 16/25 | Loss: 0.00097159
Iteration 17/25 | Loss: 0.00097159
Iteration 18/25 | Loss: 0.00097159
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000971588771790266, 0.000971588771790266, 0.000971588771790266, 0.000971588771790266, 0.000971588771790266]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000971588771790266

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097159
Iteration 2/1000 | Loss: 0.00005339
Iteration 3/1000 | Loss: 0.00003794
Iteration 4/1000 | Loss: 0.00003317
Iteration 5/1000 | Loss: 0.00003105
Iteration 6/1000 | Loss: 0.00002985
Iteration 7/1000 | Loss: 0.00002918
Iteration 8/1000 | Loss: 0.00002866
Iteration 9/1000 | Loss: 0.00002830
Iteration 10/1000 | Loss: 0.00002808
Iteration 11/1000 | Loss: 0.00002798
Iteration 12/1000 | Loss: 0.00002792
Iteration 13/1000 | Loss: 0.00002780
Iteration 14/1000 | Loss: 0.00002774
Iteration 15/1000 | Loss: 0.00002774
Iteration 16/1000 | Loss: 0.00002771
Iteration 17/1000 | Loss: 0.00002771
Iteration 18/1000 | Loss: 0.00002771
Iteration 19/1000 | Loss: 0.00002770
Iteration 20/1000 | Loss: 0.00002770
Iteration 21/1000 | Loss: 0.00002769
Iteration 22/1000 | Loss: 0.00002769
Iteration 23/1000 | Loss: 0.00002769
Iteration 24/1000 | Loss: 0.00002767
Iteration 25/1000 | Loss: 0.00002765
Iteration 26/1000 | Loss: 0.00002765
Iteration 27/1000 | Loss: 0.00002764
Iteration 28/1000 | Loss: 0.00002764
Iteration 29/1000 | Loss: 0.00002764
Iteration 30/1000 | Loss: 0.00002763
Iteration 31/1000 | Loss: 0.00002762
Iteration 32/1000 | Loss: 0.00002762
Iteration 33/1000 | Loss: 0.00002762
Iteration 34/1000 | Loss: 0.00002761
Iteration 35/1000 | Loss: 0.00002761
Iteration 36/1000 | Loss: 0.00002761
Iteration 37/1000 | Loss: 0.00002760
Iteration 38/1000 | Loss: 0.00002760
Iteration 39/1000 | Loss: 0.00002760
Iteration 40/1000 | Loss: 0.00002759
Iteration 41/1000 | Loss: 0.00002759
Iteration 42/1000 | Loss: 0.00002758
Iteration 43/1000 | Loss: 0.00002758
Iteration 44/1000 | Loss: 0.00002758
Iteration 45/1000 | Loss: 0.00002757
Iteration 46/1000 | Loss: 0.00002757
Iteration 47/1000 | Loss: 0.00002757
Iteration 48/1000 | Loss: 0.00002756
Iteration 49/1000 | Loss: 0.00002755
Iteration 50/1000 | Loss: 0.00002755
Iteration 51/1000 | Loss: 0.00002755
Iteration 52/1000 | Loss: 0.00002755
Iteration 53/1000 | Loss: 0.00002754
Iteration 54/1000 | Loss: 0.00002754
Iteration 55/1000 | Loss: 0.00002754
Iteration 56/1000 | Loss: 0.00002754
Iteration 57/1000 | Loss: 0.00002753
Iteration 58/1000 | Loss: 0.00002753
Iteration 59/1000 | Loss: 0.00002753
Iteration 60/1000 | Loss: 0.00002753
Iteration 61/1000 | Loss: 0.00002752
Iteration 62/1000 | Loss: 0.00002752
Iteration 63/1000 | Loss: 0.00002752
Iteration 64/1000 | Loss: 0.00002752
Iteration 65/1000 | Loss: 0.00002752
Iteration 66/1000 | Loss: 0.00002752
Iteration 67/1000 | Loss: 0.00002752
Iteration 68/1000 | Loss: 0.00002751
Iteration 69/1000 | Loss: 0.00002751
Iteration 70/1000 | Loss: 0.00002751
Iteration 71/1000 | Loss: 0.00002751
Iteration 72/1000 | Loss: 0.00002751
Iteration 73/1000 | Loss: 0.00002751
Iteration 74/1000 | Loss: 0.00002750
Iteration 75/1000 | Loss: 0.00002750
Iteration 76/1000 | Loss: 0.00002750
Iteration 77/1000 | Loss: 0.00002750
Iteration 78/1000 | Loss: 0.00002750
Iteration 79/1000 | Loss: 0.00002750
Iteration 80/1000 | Loss: 0.00002750
Iteration 81/1000 | Loss: 0.00002750
Iteration 82/1000 | Loss: 0.00002749
Iteration 83/1000 | Loss: 0.00002749
Iteration 84/1000 | Loss: 0.00002749
Iteration 85/1000 | Loss: 0.00002749
Iteration 86/1000 | Loss: 0.00002749
Iteration 87/1000 | Loss: 0.00002749
Iteration 88/1000 | Loss: 0.00002749
Iteration 89/1000 | Loss: 0.00002749
Iteration 90/1000 | Loss: 0.00002749
Iteration 91/1000 | Loss: 0.00002749
Iteration 92/1000 | Loss: 0.00002749
Iteration 93/1000 | Loss: 0.00002749
Iteration 94/1000 | Loss: 0.00002748
Iteration 95/1000 | Loss: 0.00002748
Iteration 96/1000 | Loss: 0.00002748
Iteration 97/1000 | Loss: 0.00002748
Iteration 98/1000 | Loss: 0.00002747
Iteration 99/1000 | Loss: 0.00002747
Iteration 100/1000 | Loss: 0.00002747
Iteration 101/1000 | Loss: 0.00002747
Iteration 102/1000 | Loss: 0.00002747
Iteration 103/1000 | Loss: 0.00002747
Iteration 104/1000 | Loss: 0.00002747
Iteration 105/1000 | Loss: 0.00002747
Iteration 106/1000 | Loss: 0.00002747
Iteration 107/1000 | Loss: 0.00002747
Iteration 108/1000 | Loss: 0.00002746
Iteration 109/1000 | Loss: 0.00002746
Iteration 110/1000 | Loss: 0.00002746
Iteration 111/1000 | Loss: 0.00002746
Iteration 112/1000 | Loss: 0.00002746
Iteration 113/1000 | Loss: 0.00002745
Iteration 114/1000 | Loss: 0.00002745
Iteration 115/1000 | Loss: 0.00002745
Iteration 116/1000 | Loss: 0.00002745
Iteration 117/1000 | Loss: 0.00002745
Iteration 118/1000 | Loss: 0.00002745
Iteration 119/1000 | Loss: 0.00002745
Iteration 120/1000 | Loss: 0.00002745
Iteration 121/1000 | Loss: 0.00002745
Iteration 122/1000 | Loss: 0.00002745
Iteration 123/1000 | Loss: 0.00002745
Iteration 124/1000 | Loss: 0.00002745
Iteration 125/1000 | Loss: 0.00002745
Iteration 126/1000 | Loss: 0.00002745
Iteration 127/1000 | Loss: 0.00002744
Iteration 128/1000 | Loss: 0.00002744
Iteration 129/1000 | Loss: 0.00002744
Iteration 130/1000 | Loss: 0.00002744
Iteration 131/1000 | Loss: 0.00002744
Iteration 132/1000 | Loss: 0.00002744
Iteration 133/1000 | Loss: 0.00002744
Iteration 134/1000 | Loss: 0.00002744
Iteration 135/1000 | Loss: 0.00002744
Iteration 136/1000 | Loss: 0.00002744
Iteration 137/1000 | Loss: 0.00002744
Iteration 138/1000 | Loss: 0.00002744
Iteration 139/1000 | Loss: 0.00002743
Iteration 140/1000 | Loss: 0.00002743
Iteration 141/1000 | Loss: 0.00002743
Iteration 142/1000 | Loss: 0.00002743
Iteration 143/1000 | Loss: 0.00002743
Iteration 144/1000 | Loss: 0.00002743
Iteration 145/1000 | Loss: 0.00002743
Iteration 146/1000 | Loss: 0.00002743
Iteration 147/1000 | Loss: 0.00002743
Iteration 148/1000 | Loss: 0.00002743
Iteration 149/1000 | Loss: 0.00002743
Iteration 150/1000 | Loss: 0.00002743
Iteration 151/1000 | Loss: 0.00002743
Iteration 152/1000 | Loss: 0.00002743
Iteration 153/1000 | Loss: 0.00002743
Iteration 154/1000 | Loss: 0.00002743
Iteration 155/1000 | Loss: 0.00002743
Iteration 156/1000 | Loss: 0.00002743
Iteration 157/1000 | Loss: 0.00002743
Iteration 158/1000 | Loss: 0.00002743
Iteration 159/1000 | Loss: 0.00002743
Iteration 160/1000 | Loss: 0.00002743
Iteration 161/1000 | Loss: 0.00002743
Iteration 162/1000 | Loss: 0.00002743
Iteration 163/1000 | Loss: 0.00002743
Iteration 164/1000 | Loss: 0.00002743
Iteration 165/1000 | Loss: 0.00002743
Iteration 166/1000 | Loss: 0.00002743
Iteration 167/1000 | Loss: 0.00002743
Iteration 168/1000 | Loss: 0.00002743
Iteration 169/1000 | Loss: 0.00002743
Iteration 170/1000 | Loss: 0.00002743
Iteration 171/1000 | Loss: 0.00002743
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [2.742964352364652e-05, 2.742964352364652e-05, 2.742964352364652e-05, 2.742964352364652e-05, 2.742964352364652e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.742964352364652e-05

Optimization complete. Final v2v error: 4.217151165008545 mm

Highest mean error: 4.838176250457764 mm for frame 174

Lowest mean error: 3.1362810134887695 mm for frame 193

Saving results

Total time: 41.12204098701477
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_35_us_2506/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_35_us_2506/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01188328
Iteration 2/25 | Loss: 0.00209173
Iteration 3/25 | Loss: 0.00196467
Iteration 4/25 | Loss: 0.00148377
Iteration 5/25 | Loss: 0.00126442
Iteration 6/25 | Loss: 0.00122393
Iteration 7/25 | Loss: 0.00121019
Iteration 8/25 | Loss: 0.00119886
Iteration 9/25 | Loss: 0.00119423
Iteration 10/25 | Loss: 0.00119178
Iteration 11/25 | Loss: 0.00119196
Iteration 12/25 | Loss: 0.00118930
Iteration 13/25 | Loss: 0.00118823
Iteration 14/25 | Loss: 0.00118797
Iteration 15/25 | Loss: 0.00118795
Iteration 16/25 | Loss: 0.00118795
Iteration 17/25 | Loss: 0.00118795
Iteration 18/25 | Loss: 0.00118794
Iteration 19/25 | Loss: 0.00118794
Iteration 20/25 | Loss: 0.00118794
Iteration 21/25 | Loss: 0.00118794
Iteration 22/25 | Loss: 0.00118794
Iteration 23/25 | Loss: 0.00118794
Iteration 24/25 | Loss: 0.00118794
Iteration 25/25 | Loss: 0.00118794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97558933
Iteration 2/25 | Loss: 0.00061880
Iteration 3/25 | Loss: 0.00061879
Iteration 4/25 | Loss: 0.00061879
Iteration 5/25 | Loss: 0.00061879
Iteration 6/25 | Loss: 0.00061879
Iteration 7/25 | Loss: 0.00061879
Iteration 8/25 | Loss: 0.00061879
Iteration 9/25 | Loss: 0.00061879
Iteration 10/25 | Loss: 0.00061879
Iteration 11/25 | Loss: 0.00061879
Iteration 12/25 | Loss: 0.00061879
Iteration 13/25 | Loss: 0.00061879
Iteration 14/25 | Loss: 0.00061879
Iteration 15/25 | Loss: 0.00061879
Iteration 16/25 | Loss: 0.00061879
Iteration 17/25 | Loss: 0.00061879
Iteration 18/25 | Loss: 0.00061879
Iteration 19/25 | Loss: 0.00061879
Iteration 20/25 | Loss: 0.00061879
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006187920807860792, 0.0006187920807860792, 0.0006187920807860792, 0.0006187920807860792, 0.0006187920807860792]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006187920807860792

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061879
Iteration 2/1000 | Loss: 0.00006217
Iteration 3/1000 | Loss: 0.00005058
Iteration 4/1000 | Loss: 0.00004587
Iteration 5/1000 | Loss: 0.00004374
Iteration 6/1000 | Loss: 0.00004261
Iteration 7/1000 | Loss: 0.00004167
Iteration 8/1000 | Loss: 0.00004093
Iteration 9/1000 | Loss: 0.00004054
Iteration 10/1000 | Loss: 0.00004026
Iteration 11/1000 | Loss: 0.00003998
Iteration 12/1000 | Loss: 0.00003983
Iteration 13/1000 | Loss: 0.00003974
Iteration 14/1000 | Loss: 0.00003968
Iteration 15/1000 | Loss: 0.00003968
Iteration 16/1000 | Loss: 0.00003967
Iteration 17/1000 | Loss: 0.00003962
Iteration 18/1000 | Loss: 0.00003962
Iteration 19/1000 | Loss: 0.00003962
Iteration 20/1000 | Loss: 0.00003961
Iteration 21/1000 | Loss: 0.00003960
Iteration 22/1000 | Loss: 0.00003959
Iteration 23/1000 | Loss: 0.00003959
Iteration 24/1000 | Loss: 0.00003958
Iteration 25/1000 | Loss: 0.00003958
Iteration 26/1000 | Loss: 0.00003958
Iteration 27/1000 | Loss: 0.00003957
Iteration 28/1000 | Loss: 0.00003957
Iteration 29/1000 | Loss: 0.00003956
Iteration 30/1000 | Loss: 0.00003956
Iteration 31/1000 | Loss: 0.00003955
Iteration 32/1000 | Loss: 0.00003954
Iteration 33/1000 | Loss: 0.00003954
Iteration 34/1000 | Loss: 0.00003953
Iteration 35/1000 | Loss: 0.00003952
Iteration 36/1000 | Loss: 0.00003952
Iteration 37/1000 | Loss: 0.00003952
Iteration 38/1000 | Loss: 0.00003951
Iteration 39/1000 | Loss: 0.00003951
Iteration 40/1000 | Loss: 0.00003951
Iteration 41/1000 | Loss: 0.00003950
Iteration 42/1000 | Loss: 0.00003950
Iteration 43/1000 | Loss: 0.00003949
Iteration 44/1000 | Loss: 0.00003948
Iteration 45/1000 | Loss: 0.00003948
Iteration 46/1000 | Loss: 0.00003948
Iteration 47/1000 | Loss: 0.00003947
Iteration 48/1000 | Loss: 0.00003947
Iteration 49/1000 | Loss: 0.00003947
Iteration 50/1000 | Loss: 0.00003947
Iteration 51/1000 | Loss: 0.00003946
Iteration 52/1000 | Loss: 0.00003946
Iteration 53/1000 | Loss: 0.00003945
Iteration 54/1000 | Loss: 0.00003945
Iteration 55/1000 | Loss: 0.00003945
Iteration 56/1000 | Loss: 0.00003945
Iteration 57/1000 | Loss: 0.00003945
Iteration 58/1000 | Loss: 0.00003945
Iteration 59/1000 | Loss: 0.00003945
Iteration 60/1000 | Loss: 0.00003945
Iteration 61/1000 | Loss: 0.00003944
Iteration 62/1000 | Loss: 0.00003944
Iteration 63/1000 | Loss: 0.00003944
Iteration 64/1000 | Loss: 0.00003944
Iteration 65/1000 | Loss: 0.00003943
Iteration 66/1000 | Loss: 0.00003943
Iteration 67/1000 | Loss: 0.00003943
Iteration 68/1000 | Loss: 0.00003942
Iteration 69/1000 | Loss: 0.00003942
Iteration 70/1000 | Loss: 0.00003942
Iteration 71/1000 | Loss: 0.00003942
Iteration 72/1000 | Loss: 0.00003942
Iteration 73/1000 | Loss: 0.00003942
Iteration 74/1000 | Loss: 0.00003942
Iteration 75/1000 | Loss: 0.00003942
Iteration 76/1000 | Loss: 0.00003941
Iteration 77/1000 | Loss: 0.00003941
Iteration 78/1000 | Loss: 0.00003941
Iteration 79/1000 | Loss: 0.00003941
Iteration 80/1000 | Loss: 0.00003940
Iteration 81/1000 | Loss: 0.00003940
Iteration 82/1000 | Loss: 0.00003940
Iteration 83/1000 | Loss: 0.00003940
Iteration 84/1000 | Loss: 0.00003939
Iteration 85/1000 | Loss: 0.00003939
Iteration 86/1000 | Loss: 0.00003939
Iteration 87/1000 | Loss: 0.00003939
Iteration 88/1000 | Loss: 0.00003939
Iteration 89/1000 | Loss: 0.00003939
Iteration 90/1000 | Loss: 0.00003938
Iteration 91/1000 | Loss: 0.00003938
Iteration 92/1000 | Loss: 0.00003938
Iteration 93/1000 | Loss: 0.00003938
Iteration 94/1000 | Loss: 0.00003938
Iteration 95/1000 | Loss: 0.00003938
Iteration 96/1000 | Loss: 0.00003938
Iteration 97/1000 | Loss: 0.00003937
Iteration 98/1000 | Loss: 0.00003937
Iteration 99/1000 | Loss: 0.00003937
Iteration 100/1000 | Loss: 0.00003937
Iteration 101/1000 | Loss: 0.00003937
Iteration 102/1000 | Loss: 0.00003937
Iteration 103/1000 | Loss: 0.00003937
Iteration 104/1000 | Loss: 0.00003937
Iteration 105/1000 | Loss: 0.00003937
Iteration 106/1000 | Loss: 0.00003936
Iteration 107/1000 | Loss: 0.00003936
Iteration 108/1000 | Loss: 0.00003936
Iteration 109/1000 | Loss: 0.00003936
Iteration 110/1000 | Loss: 0.00003936
Iteration 111/1000 | Loss: 0.00003936
Iteration 112/1000 | Loss: 0.00003936
Iteration 113/1000 | Loss: 0.00003936
Iteration 114/1000 | Loss: 0.00003936
Iteration 115/1000 | Loss: 0.00003936
Iteration 116/1000 | Loss: 0.00003936
Iteration 117/1000 | Loss: 0.00003935
Iteration 118/1000 | Loss: 0.00003935
Iteration 119/1000 | Loss: 0.00003935
Iteration 120/1000 | Loss: 0.00003935
Iteration 121/1000 | Loss: 0.00003935
Iteration 122/1000 | Loss: 0.00003935
Iteration 123/1000 | Loss: 0.00003935
Iteration 124/1000 | Loss: 0.00003935
Iteration 125/1000 | Loss: 0.00003934
Iteration 126/1000 | Loss: 0.00003934
Iteration 127/1000 | Loss: 0.00003934
Iteration 128/1000 | Loss: 0.00003934
Iteration 129/1000 | Loss: 0.00003934
Iteration 130/1000 | Loss: 0.00003934
Iteration 131/1000 | Loss: 0.00003934
Iteration 132/1000 | Loss: 0.00003933
Iteration 133/1000 | Loss: 0.00003933
Iteration 134/1000 | Loss: 0.00003933
Iteration 135/1000 | Loss: 0.00003933
Iteration 136/1000 | Loss: 0.00003933
Iteration 137/1000 | Loss: 0.00003933
Iteration 138/1000 | Loss: 0.00003933
Iteration 139/1000 | Loss: 0.00003933
Iteration 140/1000 | Loss: 0.00003933
Iteration 141/1000 | Loss: 0.00003933
Iteration 142/1000 | Loss: 0.00003933
Iteration 143/1000 | Loss: 0.00003933
Iteration 144/1000 | Loss: 0.00003932
Iteration 145/1000 | Loss: 0.00003932
Iteration 146/1000 | Loss: 0.00003932
Iteration 147/1000 | Loss: 0.00003932
Iteration 148/1000 | Loss: 0.00003932
Iteration 149/1000 | Loss: 0.00003932
Iteration 150/1000 | Loss: 0.00003932
Iteration 151/1000 | Loss: 0.00003932
Iteration 152/1000 | Loss: 0.00003931
Iteration 153/1000 | Loss: 0.00003931
Iteration 154/1000 | Loss: 0.00003931
Iteration 155/1000 | Loss: 0.00003931
Iteration 156/1000 | Loss: 0.00003931
Iteration 157/1000 | Loss: 0.00003931
Iteration 158/1000 | Loss: 0.00003931
Iteration 159/1000 | Loss: 0.00003930
Iteration 160/1000 | Loss: 0.00003930
Iteration 161/1000 | Loss: 0.00003930
Iteration 162/1000 | Loss: 0.00003930
Iteration 163/1000 | Loss: 0.00003930
Iteration 164/1000 | Loss: 0.00003930
Iteration 165/1000 | Loss: 0.00003930
Iteration 166/1000 | Loss: 0.00003930
Iteration 167/1000 | Loss: 0.00003930
Iteration 168/1000 | Loss: 0.00003930
Iteration 169/1000 | Loss: 0.00003930
Iteration 170/1000 | Loss: 0.00003930
Iteration 171/1000 | Loss: 0.00003929
Iteration 172/1000 | Loss: 0.00003929
Iteration 173/1000 | Loss: 0.00003929
Iteration 174/1000 | Loss: 0.00003929
Iteration 175/1000 | Loss: 0.00003929
Iteration 176/1000 | Loss: 0.00003929
Iteration 177/1000 | Loss: 0.00003928
Iteration 178/1000 | Loss: 0.00003928
Iteration 179/1000 | Loss: 0.00003928
Iteration 180/1000 | Loss: 0.00003928
Iteration 181/1000 | Loss: 0.00003928
Iteration 182/1000 | Loss: 0.00003927
Iteration 183/1000 | Loss: 0.00003927
Iteration 184/1000 | Loss: 0.00003927
Iteration 185/1000 | Loss: 0.00003927
Iteration 186/1000 | Loss: 0.00003927
Iteration 187/1000 | Loss: 0.00003927
Iteration 188/1000 | Loss: 0.00003927
Iteration 189/1000 | Loss: 0.00003927
Iteration 190/1000 | Loss: 0.00003927
Iteration 191/1000 | Loss: 0.00003926
Iteration 192/1000 | Loss: 0.00003926
Iteration 193/1000 | Loss: 0.00003926
Iteration 194/1000 | Loss: 0.00003926
Iteration 195/1000 | Loss: 0.00003926
Iteration 196/1000 | Loss: 0.00003926
Iteration 197/1000 | Loss: 0.00003926
Iteration 198/1000 | Loss: 0.00003926
Iteration 199/1000 | Loss: 0.00003926
Iteration 200/1000 | Loss: 0.00003926
Iteration 201/1000 | Loss: 0.00003926
Iteration 202/1000 | Loss: 0.00003925
Iteration 203/1000 | Loss: 0.00003925
Iteration 204/1000 | Loss: 0.00003925
Iteration 205/1000 | Loss: 0.00003925
Iteration 206/1000 | Loss: 0.00003925
Iteration 207/1000 | Loss: 0.00003925
Iteration 208/1000 | Loss: 0.00003925
Iteration 209/1000 | Loss: 0.00003925
Iteration 210/1000 | Loss: 0.00003925
Iteration 211/1000 | Loss: 0.00003925
Iteration 212/1000 | Loss: 0.00003925
Iteration 213/1000 | Loss: 0.00003925
Iteration 214/1000 | Loss: 0.00003925
Iteration 215/1000 | Loss: 0.00003925
Iteration 216/1000 | Loss: 0.00003925
Iteration 217/1000 | Loss: 0.00003925
Iteration 218/1000 | Loss: 0.00003925
Iteration 219/1000 | Loss: 0.00003925
Iteration 220/1000 | Loss: 0.00003925
Iteration 221/1000 | Loss: 0.00003925
Iteration 222/1000 | Loss: 0.00003925
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [3.9251874113688245e-05, 3.9251874113688245e-05, 3.9251874113688245e-05, 3.9251874113688245e-05, 3.9251874113688245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9251874113688245e-05

Optimization complete. Final v2v error: 5.251317501068115 mm

Highest mean error: 5.947675704956055 mm for frame 119

Lowest mean error: 4.478517532348633 mm for frame 43

Saving results

Total time: 60.093584060668945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850811
Iteration 2/25 | Loss: 0.00087304
Iteration 3/25 | Loss: 0.00070154
Iteration 4/25 | Loss: 0.00067363
Iteration 5/25 | Loss: 0.00066697
Iteration 6/25 | Loss: 0.00066550
Iteration 7/25 | Loss: 0.00066538
Iteration 8/25 | Loss: 0.00066538
Iteration 9/25 | Loss: 0.00066538
Iteration 10/25 | Loss: 0.00066538
Iteration 11/25 | Loss: 0.00066538
Iteration 12/25 | Loss: 0.00066538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0006653819000348449, 0.0006653819000348449, 0.0006653819000348449, 0.0006653819000348449, 0.0006653819000348449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006653819000348449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40562189
Iteration 2/25 | Loss: 0.00010375
Iteration 3/25 | Loss: 0.00010373
Iteration 4/25 | Loss: 0.00010373
Iteration 5/25 | Loss: 0.00010373
Iteration 6/25 | Loss: 0.00010373
Iteration 7/25 | Loss: 0.00010373
Iteration 8/25 | Loss: 0.00010373
Iteration 9/25 | Loss: 0.00010373
Iteration 10/25 | Loss: 0.00010373
Iteration 11/25 | Loss: 0.00010373
Iteration 12/25 | Loss: 0.00010373
Iteration 13/25 | Loss: 0.00010373
Iteration 14/25 | Loss: 0.00010373
Iteration 15/25 | Loss: 0.00010373
Iteration 16/25 | Loss: 0.00010373
Iteration 17/25 | Loss: 0.00010373
Iteration 18/25 | Loss: 0.00010373
Iteration 19/25 | Loss: 0.00010373
Iteration 20/25 | Loss: 0.00010373
Iteration 21/25 | Loss: 0.00010373
Iteration 22/25 | Loss: 0.00010373
Iteration 23/25 | Loss: 0.00010373
Iteration 24/25 | Loss: 0.00010373
Iteration 25/25 | Loss: 0.00010373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00010373
Iteration 2/1000 | Loss: 0.00004132
Iteration 3/1000 | Loss: 0.00002785
Iteration 4/1000 | Loss: 0.00002328
Iteration 5/1000 | Loss: 0.00002159
Iteration 6/1000 | Loss: 0.00002037
Iteration 7/1000 | Loss: 0.00001923
Iteration 8/1000 | Loss: 0.00001877
Iteration 9/1000 | Loss: 0.00001844
Iteration 10/1000 | Loss: 0.00001820
Iteration 11/1000 | Loss: 0.00001801
Iteration 12/1000 | Loss: 0.00001782
Iteration 13/1000 | Loss: 0.00001775
Iteration 14/1000 | Loss: 0.00001774
Iteration 15/1000 | Loss: 0.00001770
Iteration 16/1000 | Loss: 0.00001770
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001769
Iteration 19/1000 | Loss: 0.00001769
Iteration 20/1000 | Loss: 0.00001767
Iteration 21/1000 | Loss: 0.00001766
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001765
Iteration 24/1000 | Loss: 0.00001765
Iteration 25/1000 | Loss: 0.00001764
Iteration 26/1000 | Loss: 0.00001764
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001763
Iteration 29/1000 | Loss: 0.00001761
Iteration 30/1000 | Loss: 0.00001761
Iteration 31/1000 | Loss: 0.00001761
Iteration 32/1000 | Loss: 0.00001761
Iteration 33/1000 | Loss: 0.00001760
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001760
Iteration 36/1000 | Loss: 0.00001759
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001758
Iteration 41/1000 | Loss: 0.00001757
Iteration 42/1000 | Loss: 0.00001757
Iteration 43/1000 | Loss: 0.00001757
Iteration 44/1000 | Loss: 0.00001757
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001756
Iteration 47/1000 | Loss: 0.00001756
Iteration 48/1000 | Loss: 0.00001756
Iteration 49/1000 | Loss: 0.00001755
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001754
Iteration 52/1000 | Loss: 0.00001754
Iteration 53/1000 | Loss: 0.00001753
Iteration 54/1000 | Loss: 0.00001753
Iteration 55/1000 | Loss: 0.00001753
Iteration 56/1000 | Loss: 0.00001753
Iteration 57/1000 | Loss: 0.00001753
Iteration 58/1000 | Loss: 0.00001752
Iteration 59/1000 | Loss: 0.00001752
Iteration 60/1000 | Loss: 0.00001752
Iteration 61/1000 | Loss: 0.00001752
Iteration 62/1000 | Loss: 0.00001752
Iteration 63/1000 | Loss: 0.00001752
Iteration 64/1000 | Loss: 0.00001752
Iteration 65/1000 | Loss: 0.00001752
Iteration 66/1000 | Loss: 0.00001752
Iteration 67/1000 | Loss: 0.00001749
Iteration 68/1000 | Loss: 0.00001749
Iteration 69/1000 | Loss: 0.00001749
Iteration 70/1000 | Loss: 0.00001748
Iteration 71/1000 | Loss: 0.00001748
Iteration 72/1000 | Loss: 0.00001748
Iteration 73/1000 | Loss: 0.00001747
Iteration 74/1000 | Loss: 0.00001747
Iteration 75/1000 | Loss: 0.00001746
Iteration 76/1000 | Loss: 0.00001746
Iteration 77/1000 | Loss: 0.00001746
Iteration 78/1000 | Loss: 0.00001745
Iteration 79/1000 | Loss: 0.00001745
Iteration 80/1000 | Loss: 0.00001745
Iteration 81/1000 | Loss: 0.00001745
Iteration 82/1000 | Loss: 0.00001744
Iteration 83/1000 | Loss: 0.00001744
Iteration 84/1000 | Loss: 0.00001744
Iteration 85/1000 | Loss: 0.00001743
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001742
Iteration 88/1000 | Loss: 0.00001742
Iteration 89/1000 | Loss: 0.00001742
Iteration 90/1000 | Loss: 0.00001742
Iteration 91/1000 | Loss: 0.00001742
Iteration 92/1000 | Loss: 0.00001741
Iteration 93/1000 | Loss: 0.00001741
Iteration 94/1000 | Loss: 0.00001741
Iteration 95/1000 | Loss: 0.00001741
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001740
Iteration 98/1000 | Loss: 0.00001740
Iteration 99/1000 | Loss: 0.00001740
Iteration 100/1000 | Loss: 0.00001740
Iteration 101/1000 | Loss: 0.00001740
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001739
Iteration 106/1000 | Loss: 0.00001739
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00001739
Iteration 111/1000 | Loss: 0.00001739
Iteration 112/1000 | Loss: 0.00001739
Iteration 113/1000 | Loss: 0.00001739
Iteration 114/1000 | Loss: 0.00001739
Iteration 115/1000 | Loss: 0.00001738
Iteration 116/1000 | Loss: 0.00001738
Iteration 117/1000 | Loss: 0.00001738
Iteration 118/1000 | Loss: 0.00001738
Iteration 119/1000 | Loss: 0.00001738
Iteration 120/1000 | Loss: 0.00001738
Iteration 121/1000 | Loss: 0.00001738
Iteration 122/1000 | Loss: 0.00001738
Iteration 123/1000 | Loss: 0.00001737
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001737
Iteration 130/1000 | Loss: 0.00001737
Iteration 131/1000 | Loss: 0.00001736
Iteration 132/1000 | Loss: 0.00001736
Iteration 133/1000 | Loss: 0.00001736
Iteration 134/1000 | Loss: 0.00001736
Iteration 135/1000 | Loss: 0.00001736
Iteration 136/1000 | Loss: 0.00001735
Iteration 137/1000 | Loss: 0.00001735
Iteration 138/1000 | Loss: 0.00001735
Iteration 139/1000 | Loss: 0.00001735
Iteration 140/1000 | Loss: 0.00001735
Iteration 141/1000 | Loss: 0.00001735
Iteration 142/1000 | Loss: 0.00001735
Iteration 143/1000 | Loss: 0.00001735
Iteration 144/1000 | Loss: 0.00001735
Iteration 145/1000 | Loss: 0.00001735
Iteration 146/1000 | Loss: 0.00001735
Iteration 147/1000 | Loss: 0.00001735
Iteration 148/1000 | Loss: 0.00001734
Iteration 149/1000 | Loss: 0.00001734
Iteration 150/1000 | Loss: 0.00001734
Iteration 151/1000 | Loss: 0.00001734
Iteration 152/1000 | Loss: 0.00001734
Iteration 153/1000 | Loss: 0.00001734
Iteration 154/1000 | Loss: 0.00001734
Iteration 155/1000 | Loss: 0.00001734
Iteration 156/1000 | Loss: 0.00001734
Iteration 157/1000 | Loss: 0.00001734
Iteration 158/1000 | Loss: 0.00001734
Iteration 159/1000 | Loss: 0.00001734
Iteration 160/1000 | Loss: 0.00001733
Iteration 161/1000 | Loss: 0.00001733
Iteration 162/1000 | Loss: 0.00001733
Iteration 163/1000 | Loss: 0.00001733
Iteration 164/1000 | Loss: 0.00001733
Iteration 165/1000 | Loss: 0.00001733
Iteration 166/1000 | Loss: 0.00001733
Iteration 167/1000 | Loss: 0.00001733
Iteration 168/1000 | Loss: 0.00001733
Iteration 169/1000 | Loss: 0.00001733
Iteration 170/1000 | Loss: 0.00001733
Iteration 171/1000 | Loss: 0.00001733
Iteration 172/1000 | Loss: 0.00001733
Iteration 173/1000 | Loss: 0.00001733
Iteration 174/1000 | Loss: 0.00001733
Iteration 175/1000 | Loss: 0.00001733
Iteration 176/1000 | Loss: 0.00001733
Iteration 177/1000 | Loss: 0.00001732
Iteration 178/1000 | Loss: 0.00001732
Iteration 179/1000 | Loss: 0.00001732
Iteration 180/1000 | Loss: 0.00001732
Iteration 181/1000 | Loss: 0.00001732
Iteration 182/1000 | Loss: 0.00001732
Iteration 183/1000 | Loss: 0.00001732
Iteration 184/1000 | Loss: 0.00001732
Iteration 185/1000 | Loss: 0.00001732
Iteration 186/1000 | Loss: 0.00001732
Iteration 187/1000 | Loss: 0.00001732
Iteration 188/1000 | Loss: 0.00001732
Iteration 189/1000 | Loss: 0.00001732
Iteration 190/1000 | Loss: 0.00001732
Iteration 191/1000 | Loss: 0.00001732
Iteration 192/1000 | Loss: 0.00001732
Iteration 193/1000 | Loss: 0.00001732
Iteration 194/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.7323114661849104e-05, 1.7323114661849104e-05, 1.7323114661849104e-05, 1.7323114661849104e-05, 1.7323114661849104e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7323114661849104e-05

Optimization complete. Final v2v error: 3.531585931777954 mm

Highest mean error: 3.7542548179626465 mm for frame 74

Lowest mean error: 3.174379587173462 mm for frame 1

Saving results

Total time: 41.65990710258484
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031945
Iteration 2/25 | Loss: 0.00224325
Iteration 3/25 | Loss: 0.00173760
Iteration 4/25 | Loss: 0.00160482
Iteration 5/25 | Loss: 0.00137706
Iteration 6/25 | Loss: 0.00143698
Iteration 7/25 | Loss: 0.00108882
Iteration 8/25 | Loss: 0.00089510
Iteration 9/25 | Loss: 0.00084014
Iteration 10/25 | Loss: 0.00082475
Iteration 11/25 | Loss: 0.00081196
Iteration 12/25 | Loss: 0.00080327
Iteration 13/25 | Loss: 0.00080054
Iteration 14/25 | Loss: 0.00079769
Iteration 15/25 | Loss: 0.00079283
Iteration 16/25 | Loss: 0.00079010
Iteration 17/25 | Loss: 0.00078843
Iteration 18/25 | Loss: 0.00078476
Iteration 19/25 | Loss: 0.00078200
Iteration 20/25 | Loss: 0.00078131
Iteration 21/25 | Loss: 0.00078115
Iteration 22/25 | Loss: 0.00078109
Iteration 23/25 | Loss: 0.00078109
Iteration 24/25 | Loss: 0.00078109
Iteration 25/25 | Loss: 0.00078109

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39935565
Iteration 2/25 | Loss: 0.00013923
Iteration 3/25 | Loss: 0.00013922
Iteration 4/25 | Loss: 0.00013922
Iteration 5/25 | Loss: 0.00013922
Iteration 6/25 | Loss: 0.00013922
Iteration 7/25 | Loss: 0.00013922
Iteration 8/25 | Loss: 0.00013922
Iteration 9/25 | Loss: 0.00013922
Iteration 10/25 | Loss: 0.00013922
Iteration 11/25 | Loss: 0.00013922
Iteration 12/25 | Loss: 0.00013922
Iteration 13/25 | Loss: 0.00013922
Iteration 14/25 | Loss: 0.00013922
Iteration 15/25 | Loss: 0.00013922
Iteration 16/25 | Loss: 0.00013922
Iteration 17/25 | Loss: 0.00013922
Iteration 18/25 | Loss: 0.00013922
Iteration 19/25 | Loss: 0.00013922
Iteration 20/25 | Loss: 0.00013922
Iteration 21/25 | Loss: 0.00013922
Iteration 22/25 | Loss: 0.00013922
Iteration 23/25 | Loss: 0.00013922
Iteration 24/25 | Loss: 0.00013922
Iteration 25/25 | Loss: 0.00013922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00013922
Iteration 2/1000 | Loss: 0.00004059
Iteration 3/1000 | Loss: 0.00002647
Iteration 4/1000 | Loss: 0.00002479
Iteration 5/1000 | Loss: 0.00002333
Iteration 6/1000 | Loss: 0.00002243
Iteration 7/1000 | Loss: 0.00002182
Iteration 8/1000 | Loss: 0.00002138
Iteration 9/1000 | Loss: 0.00002108
Iteration 10/1000 | Loss: 0.00002106
Iteration 11/1000 | Loss: 0.00002090
Iteration 12/1000 | Loss: 0.00002077
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002065
Iteration 15/1000 | Loss: 0.00002064
Iteration 16/1000 | Loss: 0.00002064
Iteration 17/1000 | Loss: 0.00002064
Iteration 18/1000 | Loss: 0.00002064
Iteration 19/1000 | Loss: 0.00002064
Iteration 20/1000 | Loss: 0.00002064
Iteration 21/1000 | Loss: 0.00002063
Iteration 22/1000 | Loss: 0.00002062
Iteration 23/1000 | Loss: 0.00002061
Iteration 24/1000 | Loss: 0.00002060
Iteration 25/1000 | Loss: 0.00002059
Iteration 26/1000 | Loss: 0.00002058
Iteration 27/1000 | Loss: 0.00002058
Iteration 28/1000 | Loss: 0.00002057
Iteration 29/1000 | Loss: 0.00002057
Iteration 30/1000 | Loss: 0.00002057
Iteration 31/1000 | Loss: 0.00002056
Iteration 32/1000 | Loss: 0.00002056
Iteration 33/1000 | Loss: 0.00002055
Iteration 34/1000 | Loss: 0.00002054
Iteration 35/1000 | Loss: 0.00002053
Iteration 36/1000 | Loss: 0.00002053
Iteration 37/1000 | Loss: 0.00002053
Iteration 38/1000 | Loss: 0.00002052
Iteration 39/1000 | Loss: 0.00002052
Iteration 40/1000 | Loss: 0.00002052
Iteration 41/1000 | Loss: 0.00002052
Iteration 42/1000 | Loss: 0.00002052
Iteration 43/1000 | Loss: 0.00002051
Iteration 44/1000 | Loss: 0.00002051
Iteration 45/1000 | Loss: 0.00002050
Iteration 46/1000 | Loss: 0.00002050
Iteration 47/1000 | Loss: 0.00002050
Iteration 48/1000 | Loss: 0.00002049
Iteration 49/1000 | Loss: 0.00002049
Iteration 50/1000 | Loss: 0.00002049
Iteration 51/1000 | Loss: 0.00002049
Iteration 52/1000 | Loss: 0.00002048
Iteration 53/1000 | Loss: 0.00002048
Iteration 54/1000 | Loss: 0.00002047
Iteration 55/1000 | Loss: 0.00002047
Iteration 56/1000 | Loss: 0.00002047
Iteration 57/1000 | Loss: 0.00002047
Iteration 58/1000 | Loss: 0.00002047
Iteration 59/1000 | Loss: 0.00002047
Iteration 60/1000 | Loss: 0.00002047
Iteration 61/1000 | Loss: 0.00002047
Iteration 62/1000 | Loss: 0.00002047
Iteration 63/1000 | Loss: 0.00002047
Iteration 64/1000 | Loss: 0.00002047
Iteration 65/1000 | Loss: 0.00002047
Iteration 66/1000 | Loss: 0.00002047
Iteration 67/1000 | Loss: 0.00002047
Iteration 68/1000 | Loss: 0.00002047
Iteration 69/1000 | Loss: 0.00002047
Iteration 70/1000 | Loss: 0.00002047
Iteration 71/1000 | Loss: 0.00002047
Iteration 72/1000 | Loss: 0.00002047
Iteration 73/1000 | Loss: 0.00002047
Iteration 74/1000 | Loss: 0.00002047
Iteration 75/1000 | Loss: 0.00002047
Iteration 76/1000 | Loss: 0.00002047
Iteration 77/1000 | Loss: 0.00002047
Iteration 78/1000 | Loss: 0.00002047
Iteration 79/1000 | Loss: 0.00002047
Iteration 80/1000 | Loss: 0.00002047
Iteration 81/1000 | Loss: 0.00002047
Iteration 82/1000 | Loss: 0.00002047
Iteration 83/1000 | Loss: 0.00002047
Iteration 84/1000 | Loss: 0.00002047
Iteration 85/1000 | Loss: 0.00002046
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002046
Iteration 90/1000 | Loss: 0.00002046
Iteration 91/1000 | Loss: 0.00002046
Iteration 92/1000 | Loss: 0.00002046
Iteration 93/1000 | Loss: 0.00002046
Iteration 94/1000 | Loss: 0.00002046
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.0464784029172733e-05, 2.0464784029172733e-05, 2.0464784029172733e-05, 2.0464784029172733e-05, 2.0464784029172733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0464784029172733e-05

Optimization complete. Final v2v error: 3.811415910720825 mm

Highest mean error: 4.1060638427734375 mm for frame 149

Lowest mean error: 3.631859064102173 mm for frame 50

Saving results

Total time: 68.96381902694702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840594
Iteration 2/25 | Loss: 0.00079691
Iteration 3/25 | Loss: 0.00065849
Iteration 4/25 | Loss: 0.00064148
Iteration 5/25 | Loss: 0.00063651
Iteration 6/25 | Loss: 0.00063546
Iteration 7/25 | Loss: 0.00063546
Iteration 8/25 | Loss: 0.00063546
Iteration 9/25 | Loss: 0.00063546
Iteration 10/25 | Loss: 0.00063541
Iteration 11/25 | Loss: 0.00063541
Iteration 12/25 | Loss: 0.00063541
Iteration 13/25 | Loss: 0.00063541
Iteration 14/25 | Loss: 0.00063541
Iteration 15/25 | Loss: 0.00063541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006354129291139543, 0.0006354129291139543, 0.0006354129291139543, 0.0006354129291139543, 0.0006354129291139543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006354129291139543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40433061
Iteration 2/25 | Loss: 0.00015776
Iteration 3/25 | Loss: 0.00015775
Iteration 4/25 | Loss: 0.00015775
Iteration 5/25 | Loss: 0.00015775
Iteration 6/25 | Loss: 0.00015775
Iteration 7/25 | Loss: 0.00015775
Iteration 8/25 | Loss: 0.00015775
Iteration 9/25 | Loss: 0.00015775
Iteration 10/25 | Loss: 0.00015775
Iteration 11/25 | Loss: 0.00015775
Iteration 12/25 | Loss: 0.00015775
Iteration 13/25 | Loss: 0.00015775
Iteration 14/25 | Loss: 0.00015775
Iteration 15/25 | Loss: 0.00015775
Iteration 16/25 | Loss: 0.00015775
Iteration 17/25 | Loss: 0.00015775
Iteration 18/25 | Loss: 0.00015775
Iteration 19/25 | Loss: 0.00015775
Iteration 20/25 | Loss: 0.00015775
Iteration 21/25 | Loss: 0.00015775
Iteration 22/25 | Loss: 0.00015775
Iteration 23/25 | Loss: 0.00015775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00015775091014802456, 0.00015775091014802456, 0.00015775091014802456, 0.00015775091014802456, 0.00015775091014802456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00015775091014802456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015775
Iteration 2/1000 | Loss: 0.00002658
Iteration 3/1000 | Loss: 0.00001836
Iteration 4/1000 | Loss: 0.00001634
Iteration 5/1000 | Loss: 0.00001534
Iteration 6/1000 | Loss: 0.00001470
Iteration 7/1000 | Loss: 0.00001412
Iteration 8/1000 | Loss: 0.00001379
Iteration 9/1000 | Loss: 0.00001358
Iteration 10/1000 | Loss: 0.00001352
Iteration 11/1000 | Loss: 0.00001344
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001343
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001338
Iteration 16/1000 | Loss: 0.00001337
Iteration 17/1000 | Loss: 0.00001319
Iteration 18/1000 | Loss: 0.00001317
Iteration 19/1000 | Loss: 0.00001314
Iteration 20/1000 | Loss: 0.00001311
Iteration 21/1000 | Loss: 0.00001311
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001307
Iteration 26/1000 | Loss: 0.00001306
Iteration 27/1000 | Loss: 0.00001306
Iteration 28/1000 | Loss: 0.00001306
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001300
Iteration 31/1000 | Loss: 0.00001299
Iteration 32/1000 | Loss: 0.00001298
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001294
Iteration 35/1000 | Loss: 0.00001294
Iteration 36/1000 | Loss: 0.00001293
Iteration 37/1000 | Loss: 0.00001293
Iteration 38/1000 | Loss: 0.00001292
Iteration 39/1000 | Loss: 0.00001292
Iteration 40/1000 | Loss: 0.00001291
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001289
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001289
Iteration 48/1000 | Loss: 0.00001289
Iteration 49/1000 | Loss: 0.00001289
Iteration 50/1000 | Loss: 0.00001289
Iteration 51/1000 | Loss: 0.00001289
Iteration 52/1000 | Loss: 0.00001289
Iteration 53/1000 | Loss: 0.00001288
Iteration 54/1000 | Loss: 0.00001288
Iteration 55/1000 | Loss: 0.00001286
Iteration 56/1000 | Loss: 0.00001285
Iteration 57/1000 | Loss: 0.00001285
Iteration 58/1000 | Loss: 0.00001284
Iteration 59/1000 | Loss: 0.00001284
Iteration 60/1000 | Loss: 0.00001284
Iteration 61/1000 | Loss: 0.00001284
Iteration 62/1000 | Loss: 0.00001284
Iteration 63/1000 | Loss: 0.00001284
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001279
Iteration 67/1000 | Loss: 0.00001279
Iteration 68/1000 | Loss: 0.00001279
Iteration 69/1000 | Loss: 0.00001279
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001279
Iteration 72/1000 | Loss: 0.00001279
Iteration 73/1000 | Loss: 0.00001279
Iteration 74/1000 | Loss: 0.00001279
Iteration 75/1000 | Loss: 0.00001278
Iteration 76/1000 | Loss: 0.00001278
Iteration 77/1000 | Loss: 0.00001278
Iteration 78/1000 | Loss: 0.00001277
Iteration 79/1000 | Loss: 0.00001277
Iteration 80/1000 | Loss: 0.00001277
Iteration 81/1000 | Loss: 0.00001277
Iteration 82/1000 | Loss: 0.00001276
Iteration 83/1000 | Loss: 0.00001276
Iteration 84/1000 | Loss: 0.00001276
Iteration 85/1000 | Loss: 0.00001276
Iteration 86/1000 | Loss: 0.00001276
Iteration 87/1000 | Loss: 0.00001276
Iteration 88/1000 | Loss: 0.00001276
Iteration 89/1000 | Loss: 0.00001276
Iteration 90/1000 | Loss: 0.00001276
Iteration 91/1000 | Loss: 0.00001276
Iteration 92/1000 | Loss: 0.00001276
Iteration 93/1000 | Loss: 0.00001276
Iteration 94/1000 | Loss: 0.00001276
Iteration 95/1000 | Loss: 0.00001276
Iteration 96/1000 | Loss: 0.00001275
Iteration 97/1000 | Loss: 0.00001275
Iteration 98/1000 | Loss: 0.00001275
Iteration 99/1000 | Loss: 0.00001275
Iteration 100/1000 | Loss: 0.00001275
Iteration 101/1000 | Loss: 0.00001275
Iteration 102/1000 | Loss: 0.00001275
Iteration 103/1000 | Loss: 0.00001275
Iteration 104/1000 | Loss: 0.00001275
Iteration 105/1000 | Loss: 0.00001275
Iteration 106/1000 | Loss: 0.00001275
Iteration 107/1000 | Loss: 0.00001275
Iteration 108/1000 | Loss: 0.00001275
Iteration 109/1000 | Loss: 0.00001275
Iteration 110/1000 | Loss: 0.00001275
Iteration 111/1000 | Loss: 0.00001275
Iteration 112/1000 | Loss: 0.00001274
Iteration 113/1000 | Loss: 0.00001274
Iteration 114/1000 | Loss: 0.00001274
Iteration 115/1000 | Loss: 0.00001274
Iteration 116/1000 | Loss: 0.00001274
Iteration 117/1000 | Loss: 0.00001274
Iteration 118/1000 | Loss: 0.00001274
Iteration 119/1000 | Loss: 0.00001274
Iteration 120/1000 | Loss: 0.00001274
Iteration 121/1000 | Loss: 0.00001274
Iteration 122/1000 | Loss: 0.00001274
Iteration 123/1000 | Loss: 0.00001274
Iteration 124/1000 | Loss: 0.00001274
Iteration 125/1000 | Loss: 0.00001274
Iteration 126/1000 | Loss: 0.00001273
Iteration 127/1000 | Loss: 0.00001273
Iteration 128/1000 | Loss: 0.00001273
Iteration 129/1000 | Loss: 0.00001273
Iteration 130/1000 | Loss: 0.00001273
Iteration 131/1000 | Loss: 0.00001273
Iteration 132/1000 | Loss: 0.00001273
Iteration 133/1000 | Loss: 0.00001273
Iteration 134/1000 | Loss: 0.00001273
Iteration 135/1000 | Loss: 0.00001272
Iteration 136/1000 | Loss: 0.00001272
Iteration 137/1000 | Loss: 0.00001272
Iteration 138/1000 | Loss: 0.00001272
Iteration 139/1000 | Loss: 0.00001272
Iteration 140/1000 | Loss: 0.00001272
Iteration 141/1000 | Loss: 0.00001271
Iteration 142/1000 | Loss: 0.00001271
Iteration 143/1000 | Loss: 0.00001271
Iteration 144/1000 | Loss: 0.00001271
Iteration 145/1000 | Loss: 0.00001271
Iteration 146/1000 | Loss: 0.00001271
Iteration 147/1000 | Loss: 0.00001271
Iteration 148/1000 | Loss: 0.00001271
Iteration 149/1000 | Loss: 0.00001271
Iteration 150/1000 | Loss: 0.00001271
Iteration 151/1000 | Loss: 0.00001271
Iteration 152/1000 | Loss: 0.00001271
Iteration 153/1000 | Loss: 0.00001270
Iteration 154/1000 | Loss: 0.00001270
Iteration 155/1000 | Loss: 0.00001270
Iteration 156/1000 | Loss: 0.00001270
Iteration 157/1000 | Loss: 0.00001270
Iteration 158/1000 | Loss: 0.00001270
Iteration 159/1000 | Loss: 0.00001270
Iteration 160/1000 | Loss: 0.00001270
Iteration 161/1000 | Loss: 0.00001269
Iteration 162/1000 | Loss: 0.00001269
Iteration 163/1000 | Loss: 0.00001269
Iteration 164/1000 | Loss: 0.00001269
Iteration 165/1000 | Loss: 0.00001269
Iteration 166/1000 | Loss: 0.00001269
Iteration 167/1000 | Loss: 0.00001269
Iteration 168/1000 | Loss: 0.00001269
Iteration 169/1000 | Loss: 0.00001268
Iteration 170/1000 | Loss: 0.00001268
Iteration 171/1000 | Loss: 0.00001268
Iteration 172/1000 | Loss: 0.00001268
Iteration 173/1000 | Loss: 0.00001268
Iteration 174/1000 | Loss: 0.00001268
Iteration 175/1000 | Loss: 0.00001267
Iteration 176/1000 | Loss: 0.00001267
Iteration 177/1000 | Loss: 0.00001267
Iteration 178/1000 | Loss: 0.00001267
Iteration 179/1000 | Loss: 0.00001267
Iteration 180/1000 | Loss: 0.00001267
Iteration 181/1000 | Loss: 0.00001267
Iteration 182/1000 | Loss: 0.00001267
Iteration 183/1000 | Loss: 0.00001267
Iteration 184/1000 | Loss: 0.00001267
Iteration 185/1000 | Loss: 0.00001267
Iteration 186/1000 | Loss: 0.00001267
Iteration 187/1000 | Loss: 0.00001267
Iteration 188/1000 | Loss: 0.00001267
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.2672402590396814e-05, 1.2672402590396814e-05, 1.2672402590396814e-05, 1.2672402590396814e-05, 1.2672402590396814e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2672402590396814e-05

Optimization complete. Final v2v error: 3.0159294605255127 mm

Highest mean error: 3.3643710613250732 mm for frame 78

Lowest mean error: 2.689932346343994 mm for frame 203

Saving results

Total time: 42.60051107406616
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386096
Iteration 2/25 | Loss: 0.00092449
Iteration 3/25 | Loss: 0.00079166
Iteration 4/25 | Loss: 0.00077339
Iteration 5/25 | Loss: 0.00076733
Iteration 6/25 | Loss: 0.00076617
Iteration 7/25 | Loss: 0.00076617
Iteration 8/25 | Loss: 0.00076617
Iteration 9/25 | Loss: 0.00076617
Iteration 10/25 | Loss: 0.00076617
Iteration 11/25 | Loss: 0.00076617
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007661691051907837, 0.0007661691051907837, 0.0007661691051907837, 0.0007661691051907837, 0.0007661691051907837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007661691051907837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35885489
Iteration 2/25 | Loss: 0.00016878
Iteration 3/25 | Loss: 0.00016878
Iteration 4/25 | Loss: 0.00016878
Iteration 5/25 | Loss: 0.00016878
Iteration 6/25 | Loss: 0.00016878
Iteration 7/25 | Loss: 0.00016878
Iteration 8/25 | Loss: 0.00016878
Iteration 9/25 | Loss: 0.00016878
Iteration 10/25 | Loss: 0.00016878
Iteration 11/25 | Loss: 0.00016878
Iteration 12/25 | Loss: 0.00016878
Iteration 13/25 | Loss: 0.00016878
Iteration 14/25 | Loss: 0.00016878
Iteration 15/25 | Loss: 0.00016878
Iteration 16/25 | Loss: 0.00016878
Iteration 17/25 | Loss: 0.00016878
Iteration 18/25 | Loss: 0.00016878
Iteration 19/25 | Loss: 0.00016878
Iteration 20/25 | Loss: 0.00016878
Iteration 21/25 | Loss: 0.00016878
Iteration 22/25 | Loss: 0.00016878
Iteration 23/25 | Loss: 0.00016878
Iteration 24/25 | Loss: 0.00016878
Iteration 25/25 | Loss: 0.00016878

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00016878
Iteration 2/1000 | Loss: 0.00006998
Iteration 3/1000 | Loss: 0.00005243
Iteration 4/1000 | Loss: 0.00004690
Iteration 5/1000 | Loss: 0.00004417
Iteration 6/1000 | Loss: 0.00004253
Iteration 7/1000 | Loss: 0.00004103
Iteration 8/1000 | Loss: 0.00004014
Iteration 9/1000 | Loss: 0.00003959
Iteration 10/1000 | Loss: 0.00003904
Iteration 11/1000 | Loss: 0.00003862
Iteration 12/1000 | Loss: 0.00003819
Iteration 13/1000 | Loss: 0.00003776
Iteration 14/1000 | Loss: 0.00003746
Iteration 15/1000 | Loss: 0.00003729
Iteration 16/1000 | Loss: 0.00003719
Iteration 17/1000 | Loss: 0.00003704
Iteration 18/1000 | Loss: 0.00003703
Iteration 19/1000 | Loss: 0.00003699
Iteration 20/1000 | Loss: 0.00003697
Iteration 21/1000 | Loss: 0.00003692
Iteration 22/1000 | Loss: 0.00003690
Iteration 23/1000 | Loss: 0.00003689
Iteration 24/1000 | Loss: 0.00003689
Iteration 25/1000 | Loss: 0.00003689
Iteration 26/1000 | Loss: 0.00003688
Iteration 27/1000 | Loss: 0.00003688
Iteration 28/1000 | Loss: 0.00003688
Iteration 29/1000 | Loss: 0.00003687
Iteration 30/1000 | Loss: 0.00003687
Iteration 31/1000 | Loss: 0.00003687
Iteration 32/1000 | Loss: 0.00003686
Iteration 33/1000 | Loss: 0.00003686
Iteration 34/1000 | Loss: 0.00003686
Iteration 35/1000 | Loss: 0.00003686
Iteration 36/1000 | Loss: 0.00003685
Iteration 37/1000 | Loss: 0.00003685
Iteration 38/1000 | Loss: 0.00003685
Iteration 39/1000 | Loss: 0.00003685
Iteration 40/1000 | Loss: 0.00003684
Iteration 41/1000 | Loss: 0.00003684
Iteration 42/1000 | Loss: 0.00003684
Iteration 43/1000 | Loss: 0.00003683
Iteration 44/1000 | Loss: 0.00003683
Iteration 45/1000 | Loss: 0.00003683
Iteration 46/1000 | Loss: 0.00003682
Iteration 47/1000 | Loss: 0.00003682
Iteration 48/1000 | Loss: 0.00003682
Iteration 49/1000 | Loss: 0.00003680
Iteration 50/1000 | Loss: 0.00003680
Iteration 51/1000 | Loss: 0.00003680
Iteration 52/1000 | Loss: 0.00003680
Iteration 53/1000 | Loss: 0.00003680
Iteration 54/1000 | Loss: 0.00003680
Iteration 55/1000 | Loss: 0.00003680
Iteration 56/1000 | Loss: 0.00003680
Iteration 57/1000 | Loss: 0.00003680
Iteration 58/1000 | Loss: 0.00003679
Iteration 59/1000 | Loss: 0.00003679
Iteration 60/1000 | Loss: 0.00003678
Iteration 61/1000 | Loss: 0.00003678
Iteration 62/1000 | Loss: 0.00003677
Iteration 63/1000 | Loss: 0.00003677
Iteration 64/1000 | Loss: 0.00003677
Iteration 65/1000 | Loss: 0.00003676
Iteration 66/1000 | Loss: 0.00003676
Iteration 67/1000 | Loss: 0.00003675
Iteration 68/1000 | Loss: 0.00003675
Iteration 69/1000 | Loss: 0.00003674
Iteration 70/1000 | Loss: 0.00003674
Iteration 71/1000 | Loss: 0.00003674
Iteration 72/1000 | Loss: 0.00003673
Iteration 73/1000 | Loss: 0.00003673
Iteration 74/1000 | Loss: 0.00003672
Iteration 75/1000 | Loss: 0.00003672
Iteration 76/1000 | Loss: 0.00003671
Iteration 77/1000 | Loss: 0.00003671
Iteration 78/1000 | Loss: 0.00003671
Iteration 79/1000 | Loss: 0.00003670
Iteration 80/1000 | Loss: 0.00003670
Iteration 81/1000 | Loss: 0.00003669
Iteration 82/1000 | Loss: 0.00003669
Iteration 83/1000 | Loss: 0.00003669
Iteration 84/1000 | Loss: 0.00003669
Iteration 85/1000 | Loss: 0.00003669
Iteration 86/1000 | Loss: 0.00003669
Iteration 87/1000 | Loss: 0.00003669
Iteration 88/1000 | Loss: 0.00003669
Iteration 89/1000 | Loss: 0.00003668
Iteration 90/1000 | Loss: 0.00003668
Iteration 91/1000 | Loss: 0.00003668
Iteration 92/1000 | Loss: 0.00003668
Iteration 93/1000 | Loss: 0.00003668
Iteration 94/1000 | Loss: 0.00003668
Iteration 95/1000 | Loss: 0.00003668
Iteration 96/1000 | Loss: 0.00003668
Iteration 97/1000 | Loss: 0.00003668
Iteration 98/1000 | Loss: 0.00003668
Iteration 99/1000 | Loss: 0.00003668
Iteration 100/1000 | Loss: 0.00003667
Iteration 101/1000 | Loss: 0.00003667
Iteration 102/1000 | Loss: 0.00003667
Iteration 103/1000 | Loss: 0.00003667
Iteration 104/1000 | Loss: 0.00003667
Iteration 105/1000 | Loss: 0.00003667
Iteration 106/1000 | Loss: 0.00003667
Iteration 107/1000 | Loss: 0.00003667
Iteration 108/1000 | Loss: 0.00003666
Iteration 109/1000 | Loss: 0.00003666
Iteration 110/1000 | Loss: 0.00003666
Iteration 111/1000 | Loss: 0.00003666
Iteration 112/1000 | Loss: 0.00003666
Iteration 113/1000 | Loss: 0.00003666
Iteration 114/1000 | Loss: 0.00003666
Iteration 115/1000 | Loss: 0.00003666
Iteration 116/1000 | Loss: 0.00003666
Iteration 117/1000 | Loss: 0.00003666
Iteration 118/1000 | Loss: 0.00003665
Iteration 119/1000 | Loss: 0.00003665
Iteration 120/1000 | Loss: 0.00003665
Iteration 121/1000 | Loss: 0.00003665
Iteration 122/1000 | Loss: 0.00003665
Iteration 123/1000 | Loss: 0.00003664
Iteration 124/1000 | Loss: 0.00003664
Iteration 125/1000 | Loss: 0.00003664
Iteration 126/1000 | Loss: 0.00003664
Iteration 127/1000 | Loss: 0.00003664
Iteration 128/1000 | Loss: 0.00003664
Iteration 129/1000 | Loss: 0.00003664
Iteration 130/1000 | Loss: 0.00003663
Iteration 131/1000 | Loss: 0.00003663
Iteration 132/1000 | Loss: 0.00003663
Iteration 133/1000 | Loss: 0.00003663
Iteration 134/1000 | Loss: 0.00003663
Iteration 135/1000 | Loss: 0.00003663
Iteration 136/1000 | Loss: 0.00003663
Iteration 137/1000 | Loss: 0.00003663
Iteration 138/1000 | Loss: 0.00003663
Iteration 139/1000 | Loss: 0.00003663
Iteration 140/1000 | Loss: 0.00003663
Iteration 141/1000 | Loss: 0.00003662
Iteration 142/1000 | Loss: 0.00003662
Iteration 143/1000 | Loss: 0.00003662
Iteration 144/1000 | Loss: 0.00003662
Iteration 145/1000 | Loss: 0.00003662
Iteration 146/1000 | Loss: 0.00003662
Iteration 147/1000 | Loss: 0.00003662
Iteration 148/1000 | Loss: 0.00003662
Iteration 149/1000 | Loss: 0.00003662
Iteration 150/1000 | Loss: 0.00003662
Iteration 151/1000 | Loss: 0.00003662
Iteration 152/1000 | Loss: 0.00003662
Iteration 153/1000 | Loss: 0.00003662
Iteration 154/1000 | Loss: 0.00003662
Iteration 155/1000 | Loss: 0.00003662
Iteration 156/1000 | Loss: 0.00003662
Iteration 157/1000 | Loss: 0.00003662
Iteration 158/1000 | Loss: 0.00003662
Iteration 159/1000 | Loss: 0.00003662
Iteration 160/1000 | Loss: 0.00003662
Iteration 161/1000 | Loss: 0.00003662
Iteration 162/1000 | Loss: 0.00003662
Iteration 163/1000 | Loss: 0.00003662
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [3.6624871427193284e-05, 3.6624871427193284e-05, 3.6624871427193284e-05, 3.6624871427193284e-05, 3.6624871427193284e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.6624871427193284e-05

Optimization complete. Final v2v error: 4.9715776443481445 mm

Highest mean error: 5.190280914306641 mm for frame 87

Lowest mean error: 4.796501636505127 mm for frame 209

Saving results

Total time: 50.01816272735596
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851032
Iteration 2/25 | Loss: 0.00136952
Iteration 3/25 | Loss: 0.00077950
Iteration 4/25 | Loss: 0.00072485
Iteration 5/25 | Loss: 0.00068360
Iteration 6/25 | Loss: 0.00067592
Iteration 7/25 | Loss: 0.00066628
Iteration 8/25 | Loss: 0.00066044
Iteration 9/25 | Loss: 0.00065938
Iteration 10/25 | Loss: 0.00065361
Iteration 11/25 | Loss: 0.00064796
Iteration 12/25 | Loss: 0.00064596
Iteration 13/25 | Loss: 0.00064860
Iteration 14/25 | Loss: 0.00064492
Iteration 15/25 | Loss: 0.00064341
Iteration 16/25 | Loss: 0.00064219
Iteration 17/25 | Loss: 0.00064182
Iteration 18/25 | Loss: 0.00064172
Iteration 19/25 | Loss: 0.00064172
Iteration 20/25 | Loss: 0.00064172
Iteration 21/25 | Loss: 0.00064172
Iteration 22/25 | Loss: 0.00064172
Iteration 23/25 | Loss: 0.00064172
Iteration 24/25 | Loss: 0.00064171
Iteration 25/25 | Loss: 0.00064171

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93960071
Iteration 2/25 | Loss: 0.00015066
Iteration 3/25 | Loss: 0.00015064
Iteration 4/25 | Loss: 0.00015064
Iteration 5/25 | Loss: 0.00015064
Iteration 6/25 | Loss: 0.00015064
Iteration 7/25 | Loss: 0.00015064
Iteration 8/25 | Loss: 0.00015064
Iteration 9/25 | Loss: 0.00015064
Iteration 10/25 | Loss: 0.00015064
Iteration 11/25 | Loss: 0.00015064
Iteration 12/25 | Loss: 0.00015064
Iteration 13/25 | Loss: 0.00015064
Iteration 14/25 | Loss: 0.00015064
Iteration 15/25 | Loss: 0.00015064
Iteration 16/25 | Loss: 0.00015064
Iteration 17/25 | Loss: 0.00015064
Iteration 18/25 | Loss: 0.00015064
Iteration 19/25 | Loss: 0.00015064
Iteration 20/25 | Loss: 0.00015064
Iteration 21/25 | Loss: 0.00015064
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0001506417611381039, 0.0001506417611381039, 0.0001506417611381039, 0.0001506417611381039, 0.0001506417611381039]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001506417611381039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00015064
Iteration 2/1000 | Loss: 0.00002474
Iteration 3/1000 | Loss: 0.00001688
Iteration 4/1000 | Loss: 0.00001529
Iteration 5/1000 | Loss: 0.00001479
Iteration 6/1000 | Loss: 0.00001445
Iteration 7/1000 | Loss: 0.00001414
Iteration 8/1000 | Loss: 0.00001394
Iteration 9/1000 | Loss: 0.00001389
Iteration 10/1000 | Loss: 0.00001380
Iteration 11/1000 | Loss: 0.00001379
Iteration 12/1000 | Loss: 0.00001372
Iteration 13/1000 | Loss: 0.00001372
Iteration 14/1000 | Loss: 0.00001372
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001372
Iteration 17/1000 | Loss: 0.00001372
Iteration 18/1000 | Loss: 0.00001371
Iteration 19/1000 | Loss: 0.00001371
Iteration 20/1000 | Loss: 0.00001371
Iteration 21/1000 | Loss: 0.00001370
Iteration 22/1000 | Loss: 0.00001369
Iteration 23/1000 | Loss: 0.00001368
Iteration 24/1000 | Loss: 0.00001368
Iteration 25/1000 | Loss: 0.00001368
Iteration 26/1000 | Loss: 0.00001368
Iteration 27/1000 | Loss: 0.00001367
Iteration 28/1000 | Loss: 0.00001367
Iteration 29/1000 | Loss: 0.00001367
Iteration 30/1000 | Loss: 0.00001366
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001365
Iteration 33/1000 | Loss: 0.00001365
Iteration 34/1000 | Loss: 0.00001365
Iteration 35/1000 | Loss: 0.00001365
Iteration 36/1000 | Loss: 0.00001364
Iteration 37/1000 | Loss: 0.00001364
Iteration 38/1000 | Loss: 0.00001364
Iteration 39/1000 | Loss: 0.00001364
Iteration 40/1000 | Loss: 0.00001364
Iteration 41/1000 | Loss: 0.00001363
Iteration 42/1000 | Loss: 0.00001363
Iteration 43/1000 | Loss: 0.00001362
Iteration 44/1000 | Loss: 0.00001362
Iteration 45/1000 | Loss: 0.00001361
Iteration 46/1000 | Loss: 0.00001361
Iteration 47/1000 | Loss: 0.00001361
Iteration 48/1000 | Loss: 0.00001361
Iteration 49/1000 | Loss: 0.00001361
Iteration 50/1000 | Loss: 0.00001360
Iteration 51/1000 | Loss: 0.00001360
Iteration 52/1000 | Loss: 0.00001360
Iteration 53/1000 | Loss: 0.00001360
Iteration 54/1000 | Loss: 0.00001360
Iteration 55/1000 | Loss: 0.00001359
Iteration 56/1000 | Loss: 0.00001359
Iteration 57/1000 | Loss: 0.00001358
Iteration 58/1000 | Loss: 0.00001357
Iteration 59/1000 | Loss: 0.00001357
Iteration 60/1000 | Loss: 0.00001357
Iteration 61/1000 | Loss: 0.00001356
Iteration 62/1000 | Loss: 0.00001356
Iteration 63/1000 | Loss: 0.00001356
Iteration 64/1000 | Loss: 0.00001355
Iteration 65/1000 | Loss: 0.00001355
Iteration 66/1000 | Loss: 0.00001354
Iteration 67/1000 | Loss: 0.00001354
Iteration 68/1000 | Loss: 0.00001354
Iteration 69/1000 | Loss: 0.00001354
Iteration 70/1000 | Loss: 0.00001354
Iteration 71/1000 | Loss: 0.00001354
Iteration 72/1000 | Loss: 0.00001354
Iteration 73/1000 | Loss: 0.00001354
Iteration 74/1000 | Loss: 0.00001354
Iteration 75/1000 | Loss: 0.00001354
Iteration 76/1000 | Loss: 0.00001354
Iteration 77/1000 | Loss: 0.00001353
Iteration 78/1000 | Loss: 0.00001353
Iteration 79/1000 | Loss: 0.00001353
Iteration 80/1000 | Loss: 0.00001351
Iteration 81/1000 | Loss: 0.00001351
Iteration 82/1000 | Loss: 0.00001351
Iteration 83/1000 | Loss: 0.00001350
Iteration 84/1000 | Loss: 0.00001350
Iteration 85/1000 | Loss: 0.00001350
Iteration 86/1000 | Loss: 0.00001349
Iteration 87/1000 | Loss: 0.00001349
Iteration 88/1000 | Loss: 0.00001349
Iteration 89/1000 | Loss: 0.00001349
Iteration 90/1000 | Loss: 0.00001348
Iteration 91/1000 | Loss: 0.00001348
Iteration 92/1000 | Loss: 0.00001348
Iteration 93/1000 | Loss: 0.00001348
Iteration 94/1000 | Loss: 0.00001348
Iteration 95/1000 | Loss: 0.00001348
Iteration 96/1000 | Loss: 0.00001348
Iteration 97/1000 | Loss: 0.00001348
Iteration 98/1000 | Loss: 0.00001347
Iteration 99/1000 | Loss: 0.00001347
Iteration 100/1000 | Loss: 0.00001347
Iteration 101/1000 | Loss: 0.00001347
Iteration 102/1000 | Loss: 0.00001347
Iteration 103/1000 | Loss: 0.00001347
Iteration 104/1000 | Loss: 0.00001346
Iteration 105/1000 | Loss: 0.00001346
Iteration 106/1000 | Loss: 0.00001346
Iteration 107/1000 | Loss: 0.00001346
Iteration 108/1000 | Loss: 0.00001346
Iteration 109/1000 | Loss: 0.00001345
Iteration 110/1000 | Loss: 0.00001345
Iteration 111/1000 | Loss: 0.00001345
Iteration 112/1000 | Loss: 0.00001345
Iteration 113/1000 | Loss: 0.00001345
Iteration 114/1000 | Loss: 0.00001345
Iteration 115/1000 | Loss: 0.00001345
Iteration 116/1000 | Loss: 0.00001344
Iteration 117/1000 | Loss: 0.00001344
Iteration 118/1000 | Loss: 0.00001344
Iteration 119/1000 | Loss: 0.00001344
Iteration 120/1000 | Loss: 0.00001344
Iteration 121/1000 | Loss: 0.00001344
Iteration 122/1000 | Loss: 0.00001344
Iteration 123/1000 | Loss: 0.00001343
Iteration 124/1000 | Loss: 0.00001343
Iteration 125/1000 | Loss: 0.00001343
Iteration 126/1000 | Loss: 0.00001343
Iteration 127/1000 | Loss: 0.00001342
Iteration 128/1000 | Loss: 0.00001342
Iteration 129/1000 | Loss: 0.00001342
Iteration 130/1000 | Loss: 0.00001342
Iteration 131/1000 | Loss: 0.00001341
Iteration 132/1000 | Loss: 0.00001341
Iteration 133/1000 | Loss: 0.00001341
Iteration 134/1000 | Loss: 0.00001341
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001341
Iteration 137/1000 | Loss: 0.00001341
Iteration 138/1000 | Loss: 0.00001340
Iteration 139/1000 | Loss: 0.00001340
Iteration 140/1000 | Loss: 0.00001340
Iteration 141/1000 | Loss: 0.00001340
Iteration 142/1000 | Loss: 0.00001340
Iteration 143/1000 | Loss: 0.00001340
Iteration 144/1000 | Loss: 0.00001339
Iteration 145/1000 | Loss: 0.00001339
Iteration 146/1000 | Loss: 0.00001339
Iteration 147/1000 | Loss: 0.00001339
Iteration 148/1000 | Loss: 0.00001339
Iteration 149/1000 | Loss: 0.00001339
Iteration 150/1000 | Loss: 0.00001339
Iteration 151/1000 | Loss: 0.00001339
Iteration 152/1000 | Loss: 0.00001339
Iteration 153/1000 | Loss: 0.00001339
Iteration 154/1000 | Loss: 0.00001339
Iteration 155/1000 | Loss: 0.00001339
Iteration 156/1000 | Loss: 0.00001339
Iteration 157/1000 | Loss: 0.00001339
Iteration 158/1000 | Loss: 0.00001338
Iteration 159/1000 | Loss: 0.00001338
Iteration 160/1000 | Loss: 0.00001338
Iteration 161/1000 | Loss: 0.00001338
Iteration 162/1000 | Loss: 0.00001338
Iteration 163/1000 | Loss: 0.00001338
Iteration 164/1000 | Loss: 0.00001338
Iteration 165/1000 | Loss: 0.00001338
Iteration 166/1000 | Loss: 0.00001338
Iteration 167/1000 | Loss: 0.00001338
Iteration 168/1000 | Loss: 0.00001338
Iteration 169/1000 | Loss: 0.00001338
Iteration 170/1000 | Loss: 0.00001337
Iteration 171/1000 | Loss: 0.00001337
Iteration 172/1000 | Loss: 0.00001337
Iteration 173/1000 | Loss: 0.00001337
Iteration 174/1000 | Loss: 0.00001337
Iteration 175/1000 | Loss: 0.00001337
Iteration 176/1000 | Loss: 0.00001337
Iteration 177/1000 | Loss: 0.00001337
Iteration 178/1000 | Loss: 0.00001337
Iteration 179/1000 | Loss: 0.00001337
Iteration 180/1000 | Loss: 0.00001337
Iteration 181/1000 | Loss: 0.00001337
Iteration 182/1000 | Loss: 0.00001337
Iteration 183/1000 | Loss: 0.00001337
Iteration 184/1000 | Loss: 0.00001337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.3370481610763818e-05, 1.3370481610763818e-05, 1.3370481610763818e-05, 1.3370481610763818e-05, 1.3370481610763818e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3370481610763818e-05

Optimization complete. Final v2v error: 3.081944227218628 mm

Highest mean error: 3.568183660507202 mm for frame 224

Lowest mean error: 2.6392905712127686 mm for frame 203

Saving results

Total time: 65.7293107509613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00346450
Iteration 2/25 | Loss: 0.00086383
Iteration 3/25 | Loss: 0.00069927
Iteration 4/25 | Loss: 0.00067828
Iteration 5/25 | Loss: 0.00067429
Iteration 6/25 | Loss: 0.00067350
Iteration 7/25 | Loss: 0.00067350
Iteration 8/25 | Loss: 0.00067350
Iteration 9/25 | Loss: 0.00067350
Iteration 10/25 | Loss: 0.00067350
Iteration 11/25 | Loss: 0.00067350
Iteration 12/25 | Loss: 0.00067350
Iteration 13/25 | Loss: 0.00067350
Iteration 14/25 | Loss: 0.00067350
Iteration 15/25 | Loss: 0.00067350
Iteration 16/25 | Loss: 0.00067350
Iteration 17/25 | Loss: 0.00067350
Iteration 18/25 | Loss: 0.00067350
Iteration 19/25 | Loss: 0.00067350
Iteration 20/25 | Loss: 0.00067350
Iteration 21/25 | Loss: 0.00067350
Iteration 22/25 | Loss: 0.00067350
Iteration 23/25 | Loss: 0.00067350
Iteration 24/25 | Loss: 0.00067350
Iteration 25/25 | Loss: 0.00067350

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43373144
Iteration 2/25 | Loss: 0.00019728
Iteration 3/25 | Loss: 0.00019728
Iteration 4/25 | Loss: 0.00019728
Iteration 5/25 | Loss: 0.00019728
Iteration 6/25 | Loss: 0.00019728
Iteration 7/25 | Loss: 0.00019728
Iteration 8/25 | Loss: 0.00019728
Iteration 9/25 | Loss: 0.00019727
Iteration 10/25 | Loss: 0.00019727
Iteration 11/25 | Loss: 0.00019727
Iteration 12/25 | Loss: 0.00019727
Iteration 13/25 | Loss: 0.00019727
Iteration 14/25 | Loss: 0.00019727
Iteration 15/25 | Loss: 0.00019727
Iteration 16/25 | Loss: 0.00019727
Iteration 17/25 | Loss: 0.00019727
Iteration 18/25 | Loss: 0.00019727
Iteration 19/25 | Loss: 0.00019727
Iteration 20/25 | Loss: 0.00019727
Iteration 21/25 | Loss: 0.00019727
Iteration 22/25 | Loss: 0.00019727
Iteration 23/25 | Loss: 0.00019727
Iteration 24/25 | Loss: 0.00019727
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00019727463950403035, 0.00019727463950403035, 0.00019727463950403035, 0.00019727463950403035, 0.00019727463950403035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00019727463950403035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00019727
Iteration 2/1000 | Loss: 0.00004072
Iteration 3/1000 | Loss: 0.00002322
Iteration 4/1000 | Loss: 0.00002093
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001873
Iteration 8/1000 | Loss: 0.00001820
Iteration 9/1000 | Loss: 0.00001778
Iteration 10/1000 | Loss: 0.00001748
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001731
Iteration 13/1000 | Loss: 0.00001726
Iteration 14/1000 | Loss: 0.00001721
Iteration 15/1000 | Loss: 0.00001718
Iteration 16/1000 | Loss: 0.00001706
Iteration 17/1000 | Loss: 0.00001705
Iteration 18/1000 | Loss: 0.00001704
Iteration 19/1000 | Loss: 0.00001704
Iteration 20/1000 | Loss: 0.00001703
Iteration 21/1000 | Loss: 0.00001702
Iteration 22/1000 | Loss: 0.00001702
Iteration 23/1000 | Loss: 0.00001701
Iteration 24/1000 | Loss: 0.00001701
Iteration 25/1000 | Loss: 0.00001701
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001699
Iteration 32/1000 | Loss: 0.00001699
Iteration 33/1000 | Loss: 0.00001698
Iteration 34/1000 | Loss: 0.00001698
Iteration 35/1000 | Loss: 0.00001698
Iteration 36/1000 | Loss: 0.00001697
Iteration 37/1000 | Loss: 0.00001697
Iteration 38/1000 | Loss: 0.00001697
Iteration 39/1000 | Loss: 0.00001697
Iteration 40/1000 | Loss: 0.00001696
Iteration 41/1000 | Loss: 0.00001696
Iteration 42/1000 | Loss: 0.00001696
Iteration 43/1000 | Loss: 0.00001696
Iteration 44/1000 | Loss: 0.00001695
Iteration 45/1000 | Loss: 0.00001695
Iteration 46/1000 | Loss: 0.00001695
Iteration 47/1000 | Loss: 0.00001695
Iteration 48/1000 | Loss: 0.00001695
Iteration 49/1000 | Loss: 0.00001695
Iteration 50/1000 | Loss: 0.00001695
Iteration 51/1000 | Loss: 0.00001695
Iteration 52/1000 | Loss: 0.00001695
Iteration 53/1000 | Loss: 0.00001695
Iteration 54/1000 | Loss: 0.00001695
Iteration 55/1000 | Loss: 0.00001695
Iteration 56/1000 | Loss: 0.00001695
Iteration 57/1000 | Loss: 0.00001695
Iteration 58/1000 | Loss: 0.00001694
Iteration 59/1000 | Loss: 0.00001694
Iteration 60/1000 | Loss: 0.00001694
Iteration 61/1000 | Loss: 0.00001694
Iteration 62/1000 | Loss: 0.00001693
Iteration 63/1000 | Loss: 0.00001693
Iteration 64/1000 | Loss: 0.00001692
Iteration 65/1000 | Loss: 0.00001692
Iteration 66/1000 | Loss: 0.00001691
Iteration 67/1000 | Loss: 0.00001691
Iteration 68/1000 | Loss: 0.00001691
Iteration 69/1000 | Loss: 0.00001691
Iteration 70/1000 | Loss: 0.00001691
Iteration 71/1000 | Loss: 0.00001691
Iteration 72/1000 | Loss: 0.00001691
Iteration 73/1000 | Loss: 0.00001689
Iteration 74/1000 | Loss: 0.00001689
Iteration 75/1000 | Loss: 0.00001689
Iteration 76/1000 | Loss: 0.00001688
Iteration 77/1000 | Loss: 0.00001688
Iteration 78/1000 | Loss: 0.00001688
Iteration 79/1000 | Loss: 0.00001687
Iteration 80/1000 | Loss: 0.00001687
Iteration 81/1000 | Loss: 0.00001686
Iteration 82/1000 | Loss: 0.00001686
Iteration 83/1000 | Loss: 0.00001686
Iteration 84/1000 | Loss: 0.00001685
Iteration 85/1000 | Loss: 0.00001685
Iteration 86/1000 | Loss: 0.00001685
Iteration 87/1000 | Loss: 0.00001685
Iteration 88/1000 | Loss: 0.00001684
Iteration 89/1000 | Loss: 0.00001684
Iteration 90/1000 | Loss: 0.00001684
Iteration 91/1000 | Loss: 0.00001683
Iteration 92/1000 | Loss: 0.00001683
Iteration 93/1000 | Loss: 0.00001683
Iteration 94/1000 | Loss: 0.00001683
Iteration 95/1000 | Loss: 0.00001683
Iteration 96/1000 | Loss: 0.00001683
Iteration 97/1000 | Loss: 0.00001683
Iteration 98/1000 | Loss: 0.00001683
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001682
Iteration 101/1000 | Loss: 0.00001682
Iteration 102/1000 | Loss: 0.00001682
Iteration 103/1000 | Loss: 0.00001682
Iteration 104/1000 | Loss: 0.00001681
Iteration 105/1000 | Loss: 0.00001681
Iteration 106/1000 | Loss: 0.00001681
Iteration 107/1000 | Loss: 0.00001681
Iteration 108/1000 | Loss: 0.00001681
Iteration 109/1000 | Loss: 0.00001681
Iteration 110/1000 | Loss: 0.00001681
Iteration 111/1000 | Loss: 0.00001681
Iteration 112/1000 | Loss: 0.00001680
Iteration 113/1000 | Loss: 0.00001680
Iteration 114/1000 | Loss: 0.00001680
Iteration 115/1000 | Loss: 0.00001680
Iteration 116/1000 | Loss: 0.00001680
Iteration 117/1000 | Loss: 0.00001680
Iteration 118/1000 | Loss: 0.00001680
Iteration 119/1000 | Loss: 0.00001680
Iteration 120/1000 | Loss: 0.00001679
Iteration 121/1000 | Loss: 0.00001679
Iteration 122/1000 | Loss: 0.00001679
Iteration 123/1000 | Loss: 0.00001679
Iteration 124/1000 | Loss: 0.00001679
Iteration 125/1000 | Loss: 0.00001679
Iteration 126/1000 | Loss: 0.00001679
Iteration 127/1000 | Loss: 0.00001679
Iteration 128/1000 | Loss: 0.00001679
Iteration 129/1000 | Loss: 0.00001679
Iteration 130/1000 | Loss: 0.00001679
Iteration 131/1000 | Loss: 0.00001679
Iteration 132/1000 | Loss: 0.00001679
Iteration 133/1000 | Loss: 0.00001679
Iteration 134/1000 | Loss: 0.00001679
Iteration 135/1000 | Loss: 0.00001679
Iteration 136/1000 | Loss: 0.00001679
Iteration 137/1000 | Loss: 0.00001679
Iteration 138/1000 | Loss: 0.00001679
Iteration 139/1000 | Loss: 0.00001679
Iteration 140/1000 | Loss: 0.00001679
Iteration 141/1000 | Loss: 0.00001679
Iteration 142/1000 | Loss: 0.00001679
Iteration 143/1000 | Loss: 0.00001679
Iteration 144/1000 | Loss: 0.00001679
Iteration 145/1000 | Loss: 0.00001679
Iteration 146/1000 | Loss: 0.00001679
Iteration 147/1000 | Loss: 0.00001679
Iteration 148/1000 | Loss: 0.00001679
Iteration 149/1000 | Loss: 0.00001679
Iteration 150/1000 | Loss: 0.00001679
Iteration 151/1000 | Loss: 0.00001679
Iteration 152/1000 | Loss: 0.00001679
Iteration 153/1000 | Loss: 0.00001679
Iteration 154/1000 | Loss: 0.00001679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.678824264672585e-05, 1.678824264672585e-05, 1.678824264672585e-05, 1.678824264672585e-05, 1.678824264672585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.678824264672585e-05

Optimization complete. Final v2v error: 3.483924150466919 mm

Highest mean error: 3.8555874824523926 mm for frame 52

Lowest mean error: 3.090810537338257 mm for frame 126

Saving results

Total time: 38.04117703437805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047034
Iteration 2/25 | Loss: 0.00256393
Iteration 3/25 | Loss: 0.00173335
Iteration 4/25 | Loss: 0.00146320
Iteration 5/25 | Loss: 0.00150181
Iteration 6/25 | Loss: 0.00124519
Iteration 7/25 | Loss: 0.00115257
Iteration 8/25 | Loss: 0.00109508
Iteration 9/25 | Loss: 0.00102957
Iteration 10/25 | Loss: 0.00099049
Iteration 11/25 | Loss: 0.00096086
Iteration 12/25 | Loss: 0.00094113
Iteration 13/25 | Loss: 0.00091716
Iteration 14/25 | Loss: 0.00090849
Iteration 15/25 | Loss: 0.00090468
Iteration 16/25 | Loss: 0.00089618
Iteration 17/25 | Loss: 0.00091044
Iteration 18/25 | Loss: 0.00091121
Iteration 19/25 | Loss: 0.00088960
Iteration 20/25 | Loss: 0.00088388
Iteration 21/25 | Loss: 0.00087733
Iteration 22/25 | Loss: 0.00087454
Iteration 23/25 | Loss: 0.00087678
Iteration 24/25 | Loss: 0.00087407
Iteration 25/25 | Loss: 0.00087520

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40545464
Iteration 2/25 | Loss: 0.00260067
Iteration 3/25 | Loss: 0.00182275
Iteration 4/25 | Loss: 0.00182275
Iteration 5/25 | Loss: 0.00182275
Iteration 6/25 | Loss: 0.00182275
Iteration 7/25 | Loss: 0.00182275
Iteration 8/25 | Loss: 0.00182275
Iteration 9/25 | Loss: 0.00182275
Iteration 10/25 | Loss: 0.00182275
Iteration 11/25 | Loss: 0.00182275
Iteration 12/25 | Loss: 0.00182275
Iteration 13/25 | Loss: 0.00182275
Iteration 14/25 | Loss: 0.00182275
Iteration 15/25 | Loss: 0.00182275
Iteration 16/25 | Loss: 0.00182275
Iteration 17/25 | Loss: 0.00182275
Iteration 18/25 | Loss: 0.00182275
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0018227454274892807, 0.0018227454274892807, 0.0018227454274892807, 0.0018227454274892807, 0.0018227454274892807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018227454274892807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00182275
Iteration 2/1000 | Loss: 0.00163260
Iteration 3/1000 | Loss: 0.00070305
Iteration 4/1000 | Loss: 0.00027481
Iteration 5/1000 | Loss: 0.00046046
Iteration 6/1000 | Loss: 0.00029624
Iteration 7/1000 | Loss: 0.00139353
Iteration 8/1000 | Loss: 0.00233035
Iteration 9/1000 | Loss: 0.00026263
Iteration 10/1000 | Loss: 0.00092767
Iteration 11/1000 | Loss: 0.00110606
Iteration 12/1000 | Loss: 0.00053710
Iteration 13/1000 | Loss: 0.00058594
Iteration 14/1000 | Loss: 0.00294658
Iteration 15/1000 | Loss: 0.00222753
Iteration 16/1000 | Loss: 0.00041990
Iteration 17/1000 | Loss: 0.00031117
Iteration 18/1000 | Loss: 0.00078129
Iteration 19/1000 | Loss: 0.00095279
Iteration 20/1000 | Loss: 0.00041220
Iteration 21/1000 | Loss: 0.00051351
Iteration 22/1000 | Loss: 0.00043681
Iteration 23/1000 | Loss: 0.00062260
Iteration 24/1000 | Loss: 0.00011335
Iteration 25/1000 | Loss: 0.00009231
Iteration 26/1000 | Loss: 0.00060134
Iteration 27/1000 | Loss: 0.00028555
Iteration 28/1000 | Loss: 0.00040552
Iteration 29/1000 | Loss: 0.00030348
Iteration 30/1000 | Loss: 0.00026153
Iteration 31/1000 | Loss: 0.00042017
Iteration 32/1000 | Loss: 0.00068741
Iteration 33/1000 | Loss: 0.00014014
Iteration 34/1000 | Loss: 0.00010410
Iteration 35/1000 | Loss: 0.00014591
Iteration 36/1000 | Loss: 0.00034289
Iteration 37/1000 | Loss: 0.00031831
Iteration 38/1000 | Loss: 0.00114038
Iteration 39/1000 | Loss: 0.00028880
Iteration 40/1000 | Loss: 0.00098766
Iteration 41/1000 | Loss: 0.00087457
Iteration 42/1000 | Loss: 0.00108859
Iteration 43/1000 | Loss: 0.00021567
Iteration 44/1000 | Loss: 0.00012287
Iteration 45/1000 | Loss: 0.00070133
Iteration 46/1000 | Loss: 0.00011967
Iteration 47/1000 | Loss: 0.00007574
Iteration 48/1000 | Loss: 0.00007662
Iteration 49/1000 | Loss: 0.00017405
Iteration 50/1000 | Loss: 0.00039755
Iteration 51/1000 | Loss: 0.00029777
Iteration 52/1000 | Loss: 0.00022355
Iteration 53/1000 | Loss: 0.00020683
Iteration 54/1000 | Loss: 0.00023375
Iteration 55/1000 | Loss: 0.00019903
Iteration 56/1000 | Loss: 0.00022937
Iteration 57/1000 | Loss: 0.00007485
Iteration 58/1000 | Loss: 0.00005920
Iteration 59/1000 | Loss: 0.00005288
Iteration 60/1000 | Loss: 0.00068467
Iteration 61/1000 | Loss: 0.00054326
Iteration 62/1000 | Loss: 0.00005454
Iteration 63/1000 | Loss: 0.00004869
Iteration 64/1000 | Loss: 0.00050864
Iteration 65/1000 | Loss: 0.00028015
Iteration 66/1000 | Loss: 0.00010432
Iteration 67/1000 | Loss: 0.00007822
Iteration 68/1000 | Loss: 0.00008493
Iteration 69/1000 | Loss: 0.00049607
Iteration 70/1000 | Loss: 0.00005249
Iteration 71/1000 | Loss: 0.00005498
Iteration 72/1000 | Loss: 0.00006193
Iteration 73/1000 | Loss: 0.00006547
Iteration 74/1000 | Loss: 0.00006511
Iteration 75/1000 | Loss: 0.00004914
Iteration 76/1000 | Loss: 0.00004671
Iteration 77/1000 | Loss: 0.00004471
Iteration 78/1000 | Loss: 0.00005418
Iteration 79/1000 | Loss: 0.00006323
Iteration 80/1000 | Loss: 0.00007199
Iteration 81/1000 | Loss: 0.00005193
Iteration 82/1000 | Loss: 0.00005341
Iteration 83/1000 | Loss: 0.00006143
Iteration 84/1000 | Loss: 0.00006571
Iteration 85/1000 | Loss: 0.00006973
Iteration 86/1000 | Loss: 0.00007466
Iteration 87/1000 | Loss: 0.00007090
Iteration 88/1000 | Loss: 0.00007375
Iteration 89/1000 | Loss: 0.00007264
Iteration 90/1000 | Loss: 0.00007154
Iteration 91/1000 | Loss: 0.00007386
Iteration 92/1000 | Loss: 0.00008954
Iteration 93/1000 | Loss: 0.00006320
Iteration 94/1000 | Loss: 0.00007399
Iteration 95/1000 | Loss: 0.00011964
Iteration 96/1000 | Loss: 0.00007593
Iteration 97/1000 | Loss: 0.00011568
Iteration 98/1000 | Loss: 0.00007568
Iteration 99/1000 | Loss: 0.00007219
Iteration 100/1000 | Loss: 0.00007341
Iteration 101/1000 | Loss: 0.00006991
Iteration 102/1000 | Loss: 0.00007593
Iteration 103/1000 | Loss: 0.00006700
Iteration 104/1000 | Loss: 0.00007480
Iteration 105/1000 | Loss: 0.00006437
Iteration 106/1000 | Loss: 0.00007795
Iteration 107/1000 | Loss: 0.00005936
Iteration 108/1000 | Loss: 0.00046197
Iteration 109/1000 | Loss: 0.00012790
Iteration 110/1000 | Loss: 0.00007751
Iteration 111/1000 | Loss: 0.00035399
Iteration 112/1000 | Loss: 0.00026794
Iteration 113/1000 | Loss: 0.00018140
Iteration 114/1000 | Loss: 0.00013039
Iteration 115/1000 | Loss: 0.00017730
Iteration 116/1000 | Loss: 0.00014142
Iteration 117/1000 | Loss: 0.00011792
Iteration 118/1000 | Loss: 0.00006398
Iteration 119/1000 | Loss: 0.00016639
Iteration 120/1000 | Loss: 0.00010133
Iteration 121/1000 | Loss: 0.00011310
Iteration 122/1000 | Loss: 0.00020873
Iteration 123/1000 | Loss: 0.00066213
Iteration 124/1000 | Loss: 0.00019603
Iteration 125/1000 | Loss: 0.00026000
Iteration 126/1000 | Loss: 0.00065873
Iteration 127/1000 | Loss: 0.00042509
Iteration 128/1000 | Loss: 0.00004809
Iteration 129/1000 | Loss: 0.00005278
Iteration 130/1000 | Loss: 0.00035776
Iteration 131/1000 | Loss: 0.00025632
Iteration 132/1000 | Loss: 0.00029667
Iteration 133/1000 | Loss: 0.00034086
Iteration 134/1000 | Loss: 0.00008681
Iteration 135/1000 | Loss: 0.00006195
Iteration 136/1000 | Loss: 0.00003515
Iteration 137/1000 | Loss: 0.00018951
Iteration 138/1000 | Loss: 0.00019307
Iteration 139/1000 | Loss: 0.00028399
Iteration 140/1000 | Loss: 0.00033860
Iteration 141/1000 | Loss: 0.00030408
Iteration 142/1000 | Loss: 0.00059887
Iteration 143/1000 | Loss: 0.00121270
Iteration 144/1000 | Loss: 0.00008209
Iteration 145/1000 | Loss: 0.00054338
Iteration 146/1000 | Loss: 0.00042254
Iteration 147/1000 | Loss: 0.00017250
Iteration 148/1000 | Loss: 0.00030560
Iteration 149/1000 | Loss: 0.00030101
Iteration 150/1000 | Loss: 0.00024127
Iteration 151/1000 | Loss: 0.00037738
Iteration 152/1000 | Loss: 0.00068236
Iteration 153/1000 | Loss: 0.00020353
Iteration 154/1000 | Loss: 0.00005707
Iteration 155/1000 | Loss: 0.00019282
Iteration 156/1000 | Loss: 0.00007382
Iteration 157/1000 | Loss: 0.00014842
Iteration 158/1000 | Loss: 0.00008878
Iteration 159/1000 | Loss: 0.00016529
Iteration 160/1000 | Loss: 0.00009663
Iteration 161/1000 | Loss: 0.00007967
Iteration 162/1000 | Loss: 0.00013573
Iteration 163/1000 | Loss: 0.00034176
Iteration 164/1000 | Loss: 0.00051770
Iteration 165/1000 | Loss: 0.00062197
Iteration 166/1000 | Loss: 0.00020576
Iteration 167/1000 | Loss: 0.00032490
Iteration 168/1000 | Loss: 0.00031715
Iteration 169/1000 | Loss: 0.00019694
Iteration 170/1000 | Loss: 0.00036346
Iteration 171/1000 | Loss: 0.00041196
Iteration 172/1000 | Loss: 0.00085254
Iteration 173/1000 | Loss: 0.00014541
Iteration 174/1000 | Loss: 0.00005578
Iteration 175/1000 | Loss: 0.00096160
Iteration 176/1000 | Loss: 0.00005551
Iteration 177/1000 | Loss: 0.00005649
Iteration 178/1000 | Loss: 0.00031359
Iteration 179/1000 | Loss: 0.00004595
Iteration 180/1000 | Loss: 0.00012721
Iteration 181/1000 | Loss: 0.00009144
Iteration 182/1000 | Loss: 0.00004787
Iteration 183/1000 | Loss: 0.00004782
Iteration 184/1000 | Loss: 0.00007953
Iteration 185/1000 | Loss: 0.00004945
Iteration 186/1000 | Loss: 0.00005760
Iteration 187/1000 | Loss: 0.00005223
Iteration 188/1000 | Loss: 0.00005412
Iteration 189/1000 | Loss: 0.00005206
Iteration 190/1000 | Loss: 0.00005860
Iteration 191/1000 | Loss: 0.00004917
Iteration 192/1000 | Loss: 0.00005000
Iteration 193/1000 | Loss: 0.00005312
Iteration 194/1000 | Loss: 0.00022733
Iteration 195/1000 | Loss: 0.00005361
Iteration 196/1000 | Loss: 0.00004123
Iteration 197/1000 | Loss: 0.00004884
Iteration 198/1000 | Loss: 0.00005237
Iteration 199/1000 | Loss: 0.00005542
Iteration 200/1000 | Loss: 0.00005247
Iteration 201/1000 | Loss: 0.00005515
Iteration 202/1000 | Loss: 0.00004884
Iteration 203/1000 | Loss: 0.00005499
Iteration 204/1000 | Loss: 0.00005465
Iteration 205/1000 | Loss: 0.00005671
Iteration 206/1000 | Loss: 0.00004895
Iteration 207/1000 | Loss: 0.00005704
Iteration 208/1000 | Loss: 0.00003453
Iteration 209/1000 | Loss: 0.00004585
Iteration 210/1000 | Loss: 0.00005512
Iteration 211/1000 | Loss: 0.00005074
Iteration 212/1000 | Loss: 0.00007294
Iteration 213/1000 | Loss: 0.00006909
Iteration 214/1000 | Loss: 0.00007198
Iteration 215/1000 | Loss: 0.00003976
Iteration 216/1000 | Loss: 0.00003746
Iteration 217/1000 | Loss: 0.00004513
Iteration 218/1000 | Loss: 0.00004899
Iteration 219/1000 | Loss: 0.00006141
Iteration 220/1000 | Loss: 0.00005406
Iteration 221/1000 | Loss: 0.00007282
Iteration 222/1000 | Loss: 0.00004783
Iteration 223/1000 | Loss: 0.00004178
Iteration 224/1000 | Loss: 0.00004100
Iteration 225/1000 | Loss: 0.00004244
Iteration 226/1000 | Loss: 0.00005225
Iteration 227/1000 | Loss: 0.00005153
Iteration 228/1000 | Loss: 0.00005539
Iteration 229/1000 | Loss: 0.00006975
Iteration 230/1000 | Loss: 0.00005846
Iteration 231/1000 | Loss: 0.00005859
Iteration 232/1000 | Loss: 0.00006389
Iteration 233/1000 | Loss: 0.00003338
Iteration 234/1000 | Loss: 0.00005928
Iteration 235/1000 | Loss: 0.00005002
Iteration 236/1000 | Loss: 0.00004115
Iteration 237/1000 | Loss: 0.00003478
Iteration 238/1000 | Loss: 0.00005957
Iteration 239/1000 | Loss: 0.00005095
Iteration 240/1000 | Loss: 0.00005062
Iteration 241/1000 | Loss: 0.00004808
Iteration 242/1000 | Loss: 0.00004996
Iteration 243/1000 | Loss: 0.00006764
Iteration 244/1000 | Loss: 0.00006878
Iteration 245/1000 | Loss: 0.00005473
Iteration 246/1000 | Loss: 0.00004696
Iteration 247/1000 | Loss: 0.00005430
Iteration 248/1000 | Loss: 0.00005450
Iteration 249/1000 | Loss: 0.00005567
Iteration 250/1000 | Loss: 0.00004854
Iteration 251/1000 | Loss: 0.00005427
Iteration 252/1000 | Loss: 0.00004865
Iteration 253/1000 | Loss: 0.00007375
Iteration 254/1000 | Loss: 0.00004016
Iteration 255/1000 | Loss: 0.00004603
Iteration 256/1000 | Loss: 0.00004343
Iteration 257/1000 | Loss: 0.00003564
Iteration 258/1000 | Loss: 0.00005526
Iteration 259/1000 | Loss: 0.00005430
Iteration 260/1000 | Loss: 0.00003461
Iteration 261/1000 | Loss: 0.00011814
Iteration 262/1000 | Loss: 0.00005632
Iteration 263/1000 | Loss: 0.00006114
Iteration 264/1000 | Loss: 0.00003267
Iteration 265/1000 | Loss: 0.00004053
Iteration 266/1000 | Loss: 0.00004601
Iteration 267/1000 | Loss: 0.00005765
Iteration 268/1000 | Loss: 0.00004705
Iteration 269/1000 | Loss: 0.00004403
Iteration 270/1000 | Loss: 0.00004626
Iteration 271/1000 | Loss: 0.00005420
Iteration 272/1000 | Loss: 0.00007254
Iteration 273/1000 | Loss: 0.00004635
Iteration 274/1000 | Loss: 0.00006307
Iteration 275/1000 | Loss: 0.00005475
Iteration 276/1000 | Loss: 0.00005384
Iteration 277/1000 | Loss: 0.00005281
Iteration 278/1000 | Loss: 0.00005683
Iteration 279/1000 | Loss: 0.00005587
Iteration 280/1000 | Loss: 0.00003819
Iteration 281/1000 | Loss: 0.00006354
Iteration 282/1000 | Loss: 0.00005236
Iteration 283/1000 | Loss: 0.00005510
Iteration 284/1000 | Loss: 0.00005143
Iteration 285/1000 | Loss: 0.00004744
Iteration 286/1000 | Loss: 0.00004861
Iteration 287/1000 | Loss: 0.00007472
Iteration 288/1000 | Loss: 0.00008563
Iteration 289/1000 | Loss: 0.00005119
Iteration 290/1000 | Loss: 0.00004706
Iteration 291/1000 | Loss: 0.00005300
Iteration 292/1000 | Loss: 0.00005361
Iteration 293/1000 | Loss: 0.00005201
Iteration 294/1000 | Loss: 0.00004794
Iteration 295/1000 | Loss: 0.00004998
Iteration 296/1000 | Loss: 0.00007371
Iteration 297/1000 | Loss: 0.00009771
Iteration 298/1000 | Loss: 0.00004915
Iteration 299/1000 | Loss: 0.00005409
Iteration 300/1000 | Loss: 0.00005043
Iteration 301/1000 | Loss: 0.00005125
Iteration 302/1000 | Loss: 0.00004920
Iteration 303/1000 | Loss: 0.00005984
Iteration 304/1000 | Loss: 0.00005024
Iteration 305/1000 | Loss: 0.00005993
Iteration 306/1000 | Loss: 0.00004878
Iteration 307/1000 | Loss: 0.00005445
Iteration 308/1000 | Loss: 0.00004809
Iteration 309/1000 | Loss: 0.00005362
Iteration 310/1000 | Loss: 0.00008522
Iteration 311/1000 | Loss: 0.00004261
Iteration 312/1000 | Loss: 0.00015240
Iteration 313/1000 | Loss: 0.00003384
Iteration 314/1000 | Loss: 0.00002871
Iteration 315/1000 | Loss: 0.00005656
Iteration 316/1000 | Loss: 0.00003191
Iteration 317/1000 | Loss: 0.00002561
Iteration 318/1000 | Loss: 0.00008367
Iteration 319/1000 | Loss: 0.00002399
Iteration 320/1000 | Loss: 0.00002345
Iteration 321/1000 | Loss: 0.00002311
Iteration 322/1000 | Loss: 0.00003267
Iteration 323/1000 | Loss: 0.00002556
Iteration 324/1000 | Loss: 0.00002270
Iteration 325/1000 | Loss: 0.00002269
Iteration 326/1000 | Loss: 0.00040696
Iteration 327/1000 | Loss: 0.00007284
Iteration 328/1000 | Loss: 0.00002901
Iteration 329/1000 | Loss: 0.00003370
Iteration 330/1000 | Loss: 0.00002140
Iteration 331/1000 | Loss: 0.00003696
Iteration 332/1000 | Loss: 0.00003390
Iteration 333/1000 | Loss: 0.00002030
Iteration 334/1000 | Loss: 0.00003997
Iteration 335/1000 | Loss: 0.00003052
Iteration 336/1000 | Loss: 0.00001975
Iteration 337/1000 | Loss: 0.00002529
Iteration 338/1000 | Loss: 0.00002455
Iteration 339/1000 | Loss: 0.00001966
Iteration 340/1000 | Loss: 0.00002081
Iteration 341/1000 | Loss: 0.00001955
Iteration 342/1000 | Loss: 0.00001955
Iteration 343/1000 | Loss: 0.00001969
Iteration 344/1000 | Loss: 0.00001968
Iteration 345/1000 | Loss: 0.00001975
Iteration 346/1000 | Loss: 0.00001952
Iteration 347/1000 | Loss: 0.00001951
Iteration 348/1000 | Loss: 0.00001951
Iteration 349/1000 | Loss: 0.00001951
Iteration 350/1000 | Loss: 0.00001951
Iteration 351/1000 | Loss: 0.00002722
Iteration 352/1000 | Loss: 0.00001986
Iteration 353/1000 | Loss: 0.00001948
Iteration 354/1000 | Loss: 0.00001947
Iteration 355/1000 | Loss: 0.00001947
Iteration 356/1000 | Loss: 0.00001947
Iteration 357/1000 | Loss: 0.00001947
Iteration 358/1000 | Loss: 0.00001947
Iteration 359/1000 | Loss: 0.00001947
Iteration 360/1000 | Loss: 0.00001947
Iteration 361/1000 | Loss: 0.00001947
Iteration 362/1000 | Loss: 0.00002005
Iteration 363/1000 | Loss: 0.00001983
Iteration 364/1000 | Loss: 0.00002233
Iteration 365/1000 | Loss: 0.00001975
Iteration 366/1000 | Loss: 0.00001943
Iteration 367/1000 | Loss: 0.00001942
Iteration 368/1000 | Loss: 0.00001942
Iteration 369/1000 | Loss: 0.00001942
Iteration 370/1000 | Loss: 0.00001942
Iteration 371/1000 | Loss: 0.00001942
Iteration 372/1000 | Loss: 0.00001942
Iteration 373/1000 | Loss: 0.00001942
Iteration 374/1000 | Loss: 0.00001942
Iteration 375/1000 | Loss: 0.00001942
Iteration 376/1000 | Loss: 0.00001942
Iteration 377/1000 | Loss: 0.00001942
Iteration 378/1000 | Loss: 0.00001942
Iteration 379/1000 | Loss: 0.00001942
Iteration 380/1000 | Loss: 0.00001942
Iteration 381/1000 | Loss: 0.00001942
Iteration 382/1000 | Loss: 0.00001942
Iteration 383/1000 | Loss: 0.00001942
Iteration 384/1000 | Loss: 0.00001942
Iteration 385/1000 | Loss: 0.00001942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 385. Stopping optimization.
Last 5 losses: [1.9417422663536854e-05, 1.9417422663536854e-05, 1.9417422663536854e-05, 1.9417422663536854e-05, 1.9417422663536854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9417422663536854e-05

Optimization complete. Final v2v error: 3.758453369140625 mm

Highest mean error: 5.020443916320801 mm for frame 211

Lowest mean error: 3.4603869915008545 mm for frame 220

Saving results

Total time: 623.900468826294
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_25_us_0523/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_25_us_0523/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008553
Iteration 2/25 | Loss: 0.01008553
Iteration 3/25 | Loss: 0.00450769
Iteration 4/25 | Loss: 0.00252908
Iteration 5/25 | Loss: 0.00212983
Iteration 6/25 | Loss: 0.00195921
Iteration 7/25 | Loss: 0.00185336
Iteration 8/25 | Loss: 0.00178422
Iteration 9/25 | Loss: 0.00169518
Iteration 10/25 | Loss: 0.00163739
Iteration 11/25 | Loss: 0.00159127
Iteration 12/25 | Loss: 0.00156787
Iteration 13/25 | Loss: 0.00155053
Iteration 14/25 | Loss: 0.00152986
Iteration 15/25 | Loss: 0.00152546
Iteration 16/25 | Loss: 0.00152642
Iteration 17/25 | Loss: 0.00152098
Iteration 18/25 | Loss: 0.00151822
Iteration 19/25 | Loss: 0.00151724
Iteration 20/25 | Loss: 0.00151697
Iteration 21/25 | Loss: 0.00151685
Iteration 22/25 | Loss: 0.00151682
Iteration 23/25 | Loss: 0.00151682
Iteration 24/25 | Loss: 0.00151681
Iteration 25/25 | Loss: 0.00151681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36246264
Iteration 2/25 | Loss: 0.00471347
Iteration 3/25 | Loss: 0.00471347
Iteration 4/25 | Loss: 0.00471347
Iteration 5/25 | Loss: 0.00471347
Iteration 6/25 | Loss: 0.00471347
Iteration 7/25 | Loss: 0.00471346
Iteration 8/25 | Loss: 0.00471346
Iteration 9/25 | Loss: 0.00471346
Iteration 10/25 | Loss: 0.00471346
Iteration 11/25 | Loss: 0.00471346
Iteration 12/25 | Loss: 0.00471346
Iteration 13/25 | Loss: 0.00471346
Iteration 14/25 | Loss: 0.00471346
Iteration 15/25 | Loss: 0.00471346
Iteration 16/25 | Loss: 0.00471346
Iteration 17/25 | Loss: 0.00471346
Iteration 18/25 | Loss: 0.00471346
Iteration 19/25 | Loss: 0.00471346
Iteration 20/25 | Loss: 0.00471346
Iteration 21/25 | Loss: 0.00471346
Iteration 22/25 | Loss: 0.00471346
Iteration 23/25 | Loss: 0.00471346
Iteration 24/25 | Loss: 0.00471346
Iteration 25/25 | Loss: 0.00471346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0047134640626609325, 0.0047134640626609325, 0.0047134640626609325, 0.0047134640626609325, 0.0047134640626609325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0047134640626609325

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00471346
Iteration 2/1000 | Loss: 0.00207378
Iteration 3/1000 | Loss: 0.00178118
Iteration 4/1000 | Loss: 0.00104830
Iteration 5/1000 | Loss: 0.00308659
Iteration 6/1000 | Loss: 0.00234659
Iteration 7/1000 | Loss: 0.00148240
Iteration 8/1000 | Loss: 0.00255076
Iteration 9/1000 | Loss: 0.00183121
Iteration 10/1000 | Loss: 0.00117986
Iteration 11/1000 | Loss: 0.00139574
Iteration 12/1000 | Loss: 0.00145596
Iteration 13/1000 | Loss: 0.00143252
Iteration 14/1000 | Loss: 0.00096430
Iteration 15/1000 | Loss: 0.00094109
Iteration 16/1000 | Loss: 0.00207146
Iteration 17/1000 | Loss: 0.00630749
Iteration 18/1000 | Loss: 0.01054578
Iteration 19/1000 | Loss: 0.00777133
Iteration 20/1000 | Loss: 0.00142669
Iteration 21/1000 | Loss: 0.00130752
Iteration 22/1000 | Loss: 0.00081002
Iteration 23/1000 | Loss: 0.00095447
Iteration 24/1000 | Loss: 0.00100250
Iteration 25/1000 | Loss: 0.00057511
Iteration 26/1000 | Loss: 0.00021754
Iteration 27/1000 | Loss: 0.00020994
Iteration 28/1000 | Loss: 0.00050393
Iteration 29/1000 | Loss: 0.00031362
Iteration 30/1000 | Loss: 0.00040791
Iteration 31/1000 | Loss: 0.00032099
Iteration 32/1000 | Loss: 0.00033781
Iteration 33/1000 | Loss: 0.00048813
Iteration 34/1000 | Loss: 0.00008421
Iteration 35/1000 | Loss: 0.00006517
Iteration 36/1000 | Loss: 0.00005474
Iteration 37/1000 | Loss: 0.00004759
Iteration 38/1000 | Loss: 0.00034502
Iteration 39/1000 | Loss: 0.00017217
Iteration 40/1000 | Loss: 0.00012407
Iteration 41/1000 | Loss: 0.00016857
Iteration 42/1000 | Loss: 0.00008875
Iteration 43/1000 | Loss: 0.00008837
Iteration 44/1000 | Loss: 0.00008917
Iteration 45/1000 | Loss: 0.00007369
Iteration 46/1000 | Loss: 0.00009844
Iteration 47/1000 | Loss: 0.00010117
Iteration 48/1000 | Loss: 0.00012812
Iteration 49/1000 | Loss: 0.00007137
Iteration 50/1000 | Loss: 0.00012150
Iteration 51/1000 | Loss: 0.00026677
Iteration 52/1000 | Loss: 0.00004376
Iteration 53/1000 | Loss: 0.00004458
Iteration 54/1000 | Loss: 0.00003099
Iteration 55/1000 | Loss: 0.00002948
Iteration 56/1000 | Loss: 0.00002873
Iteration 57/1000 | Loss: 0.00002808
Iteration 58/1000 | Loss: 0.00002764
Iteration 59/1000 | Loss: 0.00016828
Iteration 60/1000 | Loss: 0.00003255
Iteration 61/1000 | Loss: 0.00002916
Iteration 62/1000 | Loss: 0.00002603
Iteration 63/1000 | Loss: 0.00002551
Iteration 64/1000 | Loss: 0.00002529
Iteration 65/1000 | Loss: 0.00002524
Iteration 66/1000 | Loss: 0.00002523
Iteration 67/1000 | Loss: 0.00002522
Iteration 68/1000 | Loss: 0.00002512
Iteration 69/1000 | Loss: 0.00002511
Iteration 70/1000 | Loss: 0.00002510
Iteration 71/1000 | Loss: 0.00002510
Iteration 72/1000 | Loss: 0.00002509
Iteration 73/1000 | Loss: 0.00002509
Iteration 74/1000 | Loss: 0.00002508
Iteration 75/1000 | Loss: 0.00002508
Iteration 76/1000 | Loss: 0.00002507
Iteration 77/1000 | Loss: 0.00002507
Iteration 78/1000 | Loss: 0.00002506
Iteration 79/1000 | Loss: 0.00002506
Iteration 80/1000 | Loss: 0.00002505
Iteration 81/1000 | Loss: 0.00002505
Iteration 82/1000 | Loss: 0.00002505
Iteration 83/1000 | Loss: 0.00002505
Iteration 84/1000 | Loss: 0.00002504
Iteration 85/1000 | Loss: 0.00002502
Iteration 86/1000 | Loss: 0.00002501
Iteration 87/1000 | Loss: 0.00002501
Iteration 88/1000 | Loss: 0.00002500
Iteration 89/1000 | Loss: 0.00002500
Iteration 90/1000 | Loss: 0.00002500
Iteration 91/1000 | Loss: 0.00002500
Iteration 92/1000 | Loss: 0.00002500
Iteration 93/1000 | Loss: 0.00002500
Iteration 94/1000 | Loss: 0.00002500
Iteration 95/1000 | Loss: 0.00002500
Iteration 96/1000 | Loss: 0.00002500
Iteration 97/1000 | Loss: 0.00002500
Iteration 98/1000 | Loss: 0.00002499
Iteration 99/1000 | Loss: 0.00002498
Iteration 100/1000 | Loss: 0.00002498
Iteration 101/1000 | Loss: 0.00002496
Iteration 102/1000 | Loss: 0.00002495
Iteration 103/1000 | Loss: 0.00002495
Iteration 104/1000 | Loss: 0.00002495
Iteration 105/1000 | Loss: 0.00002494
Iteration 106/1000 | Loss: 0.00002494
Iteration 107/1000 | Loss: 0.00029676
Iteration 108/1000 | Loss: 0.00021505
Iteration 109/1000 | Loss: 0.00067435
Iteration 110/1000 | Loss: 0.00101710
Iteration 111/1000 | Loss: 0.00006317
Iteration 112/1000 | Loss: 0.00003180
Iteration 113/1000 | Loss: 0.00002614
Iteration 114/1000 | Loss: 0.00002549
Iteration 115/1000 | Loss: 0.00002519
Iteration 116/1000 | Loss: 0.00002510
Iteration 117/1000 | Loss: 0.00002509
Iteration 118/1000 | Loss: 0.00002507
Iteration 119/1000 | Loss: 0.00002507
Iteration 120/1000 | Loss: 0.00002507
Iteration 121/1000 | Loss: 0.00002506
Iteration 122/1000 | Loss: 0.00002506
Iteration 123/1000 | Loss: 0.00002506
Iteration 124/1000 | Loss: 0.00002506
Iteration 125/1000 | Loss: 0.00002506
Iteration 126/1000 | Loss: 0.00002506
Iteration 127/1000 | Loss: 0.00002506
Iteration 128/1000 | Loss: 0.00002506
Iteration 129/1000 | Loss: 0.00002506
Iteration 130/1000 | Loss: 0.00002506
Iteration 131/1000 | Loss: 0.00002506
Iteration 132/1000 | Loss: 0.00002506
Iteration 133/1000 | Loss: 0.00002505
Iteration 134/1000 | Loss: 0.00002505
Iteration 135/1000 | Loss: 0.00002505
Iteration 136/1000 | Loss: 0.00002505
Iteration 137/1000 | Loss: 0.00002505
Iteration 138/1000 | Loss: 0.00002503
Iteration 139/1000 | Loss: 0.00002503
Iteration 140/1000 | Loss: 0.00002503
Iteration 141/1000 | Loss: 0.00002503
Iteration 142/1000 | Loss: 0.00002502
Iteration 143/1000 | Loss: 0.00002502
Iteration 144/1000 | Loss: 0.00002502
Iteration 145/1000 | Loss: 0.00002501
Iteration 146/1000 | Loss: 0.00002500
Iteration 147/1000 | Loss: 0.00002500
Iteration 148/1000 | Loss: 0.00002499
Iteration 149/1000 | Loss: 0.00002499
Iteration 150/1000 | Loss: 0.00002499
Iteration 151/1000 | Loss: 0.00002498
Iteration 152/1000 | Loss: 0.00002498
Iteration 153/1000 | Loss: 0.00002497
Iteration 154/1000 | Loss: 0.00002497
Iteration 155/1000 | Loss: 0.00002497
Iteration 156/1000 | Loss: 0.00002497
Iteration 157/1000 | Loss: 0.00002497
Iteration 158/1000 | Loss: 0.00002497
Iteration 159/1000 | Loss: 0.00002497
Iteration 160/1000 | Loss: 0.00002497
Iteration 161/1000 | Loss: 0.00002496
Iteration 162/1000 | Loss: 0.00002496
Iteration 163/1000 | Loss: 0.00002496
Iteration 164/1000 | Loss: 0.00002496
Iteration 165/1000 | Loss: 0.00002496
Iteration 166/1000 | Loss: 0.00002496
Iteration 167/1000 | Loss: 0.00002495
Iteration 168/1000 | Loss: 0.00002495
Iteration 169/1000 | Loss: 0.00002495
Iteration 170/1000 | Loss: 0.00002495
Iteration 171/1000 | Loss: 0.00002495
Iteration 172/1000 | Loss: 0.00002495
Iteration 173/1000 | Loss: 0.00002495
Iteration 174/1000 | Loss: 0.00002494
Iteration 175/1000 | Loss: 0.00002494
Iteration 176/1000 | Loss: 0.00002494
Iteration 177/1000 | Loss: 0.00002493
Iteration 178/1000 | Loss: 0.00002493
Iteration 179/1000 | Loss: 0.00002492
Iteration 180/1000 | Loss: 0.00002492
Iteration 181/1000 | Loss: 0.00002492
Iteration 182/1000 | Loss: 0.00002492
Iteration 183/1000 | Loss: 0.00002492
Iteration 184/1000 | Loss: 0.00002491
Iteration 185/1000 | Loss: 0.00002491
Iteration 186/1000 | Loss: 0.00002490
Iteration 187/1000 | Loss: 0.00002490
Iteration 188/1000 | Loss: 0.00002490
Iteration 189/1000 | Loss: 0.00002490
Iteration 190/1000 | Loss: 0.00002490
Iteration 191/1000 | Loss: 0.00002490
Iteration 192/1000 | Loss: 0.00002490
Iteration 193/1000 | Loss: 0.00002490
Iteration 194/1000 | Loss: 0.00002490
Iteration 195/1000 | Loss: 0.00002490
Iteration 196/1000 | Loss: 0.00002490
Iteration 197/1000 | Loss: 0.00002490
Iteration 198/1000 | Loss: 0.00002490
Iteration 199/1000 | Loss: 0.00002490
Iteration 200/1000 | Loss: 0.00002489
Iteration 201/1000 | Loss: 0.00002489
Iteration 202/1000 | Loss: 0.00002489
Iteration 203/1000 | Loss: 0.00002489
Iteration 204/1000 | Loss: 0.00002489
Iteration 205/1000 | Loss: 0.00002489
Iteration 206/1000 | Loss: 0.00002489
Iteration 207/1000 | Loss: 0.00002489
Iteration 208/1000 | Loss: 0.00002489
Iteration 209/1000 | Loss: 0.00002489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.489490361767821e-05, 2.489490361767821e-05, 2.489490361767821e-05, 2.489490361767821e-05, 2.489490361767821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.489490361767821e-05

Optimization complete. Final v2v error: 4.041258335113525 mm

Highest mean error: 13.492452621459961 mm for frame 217

Lowest mean error: 3.6530447006225586 mm for frame 0

Saving results

Total time: 172.95024824142456
