Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=65, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3640-3695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00979724
Iteration 2/25 | Loss: 0.00158642
Iteration 3/25 | Loss: 0.00120575
Iteration 4/25 | Loss: 0.00115037
Iteration 5/25 | Loss: 0.00114137
Iteration 6/25 | Loss: 0.00113918
Iteration 7/25 | Loss: 0.00113860
Iteration 8/25 | Loss: 0.00113860
Iteration 9/25 | Loss: 0.00113860
Iteration 10/25 | Loss: 0.00113860
Iteration 11/25 | Loss: 0.00113860
Iteration 12/25 | Loss: 0.00113860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011386023834347725, 0.0011386023834347725, 0.0011386023834347725, 0.0011386023834347725, 0.0011386023834347725]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011386023834347725

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.58610672
Iteration 2/25 | Loss: 0.00072509
Iteration 3/25 | Loss: 0.00072509
Iteration 4/25 | Loss: 0.00072509
Iteration 5/25 | Loss: 0.00072509
Iteration 6/25 | Loss: 0.00072509
Iteration 7/25 | Loss: 0.00072509
Iteration 8/25 | Loss: 0.00072509
Iteration 9/25 | Loss: 0.00072509
Iteration 10/25 | Loss: 0.00072509
Iteration 11/25 | Loss: 0.00072509
Iteration 12/25 | Loss: 0.00072509
Iteration 13/25 | Loss: 0.00072509
Iteration 14/25 | Loss: 0.00072509
Iteration 15/25 | Loss: 0.00072509
Iteration 16/25 | Loss: 0.00072509
Iteration 17/25 | Loss: 0.00072509
Iteration 18/25 | Loss: 0.00072509
Iteration 19/25 | Loss: 0.00072509
Iteration 20/25 | Loss: 0.00072509
Iteration 21/25 | Loss: 0.00072509
Iteration 22/25 | Loss: 0.00072509
Iteration 23/25 | Loss: 0.00072509
Iteration 24/25 | Loss: 0.00072509
Iteration 25/25 | Loss: 0.00072509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072509
Iteration 2/1000 | Loss: 0.00009481
Iteration 3/1000 | Loss: 0.00005843
Iteration 4/1000 | Loss: 0.00005052
Iteration 5/1000 | Loss: 0.00004493
Iteration 6/1000 | Loss: 0.00004172
Iteration 7/1000 | Loss: 0.00004027
Iteration 8/1000 | Loss: 0.00003971
Iteration 9/1000 | Loss: 0.00003941
Iteration 10/1000 | Loss: 0.00003908
Iteration 11/1000 | Loss: 0.00003882
Iteration 12/1000 | Loss: 0.00003865
Iteration 13/1000 | Loss: 0.00003862
Iteration 14/1000 | Loss: 0.00003862
Iteration 15/1000 | Loss: 0.00003860
Iteration 16/1000 | Loss: 0.00003859
Iteration 17/1000 | Loss: 0.00003856
Iteration 18/1000 | Loss: 0.00003856
Iteration 19/1000 | Loss: 0.00003856
Iteration 20/1000 | Loss: 0.00003855
Iteration 21/1000 | Loss: 0.00003854
Iteration 22/1000 | Loss: 0.00003854
Iteration 23/1000 | Loss: 0.00003854
Iteration 24/1000 | Loss: 0.00003853
Iteration 25/1000 | Loss: 0.00003853
Iteration 26/1000 | Loss: 0.00003853
Iteration 27/1000 | Loss: 0.00003853
Iteration 28/1000 | Loss: 0.00003853
Iteration 29/1000 | Loss: 0.00003853
Iteration 30/1000 | Loss: 0.00003853
Iteration 31/1000 | Loss: 0.00003853
Iteration 32/1000 | Loss: 0.00003852
Iteration 33/1000 | Loss: 0.00003852
Iteration 34/1000 | Loss: 0.00003851
Iteration 35/1000 | Loss: 0.00003851
Iteration 36/1000 | Loss: 0.00003850
Iteration 37/1000 | Loss: 0.00003850
Iteration 38/1000 | Loss: 0.00003850
Iteration 39/1000 | Loss: 0.00003850
Iteration 40/1000 | Loss: 0.00003850
Iteration 41/1000 | Loss: 0.00003850
Iteration 42/1000 | Loss: 0.00003850
Iteration 43/1000 | Loss: 0.00003850
Iteration 44/1000 | Loss: 0.00003850
Iteration 45/1000 | Loss: 0.00003850
Iteration 46/1000 | Loss: 0.00003849
Iteration 47/1000 | Loss: 0.00003849
Iteration 48/1000 | Loss: 0.00003849
Iteration 49/1000 | Loss: 0.00003849
Iteration 50/1000 | Loss: 0.00003849
Iteration 51/1000 | Loss: 0.00003847
Iteration 52/1000 | Loss: 0.00003846
Iteration 53/1000 | Loss: 0.00003846
Iteration 54/1000 | Loss: 0.00003845
Iteration 55/1000 | Loss: 0.00003845
Iteration 56/1000 | Loss: 0.00003845
Iteration 57/1000 | Loss: 0.00003844
Iteration 58/1000 | Loss: 0.00003844
Iteration 59/1000 | Loss: 0.00003843
Iteration 60/1000 | Loss: 0.00003842
Iteration 61/1000 | Loss: 0.00003842
Iteration 62/1000 | Loss: 0.00003842
Iteration 63/1000 | Loss: 0.00003842
Iteration 64/1000 | Loss: 0.00003842
Iteration 65/1000 | Loss: 0.00003842
Iteration 66/1000 | Loss: 0.00003842
Iteration 67/1000 | Loss: 0.00003842
Iteration 68/1000 | Loss: 0.00003842
Iteration 69/1000 | Loss: 0.00003842
Iteration 70/1000 | Loss: 0.00003842
Iteration 71/1000 | Loss: 0.00003842
Iteration 72/1000 | Loss: 0.00003842
Iteration 73/1000 | Loss: 0.00003842
Iteration 74/1000 | Loss: 0.00003842
Iteration 75/1000 | Loss: 0.00003842
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [3.841520447167568e-05, 3.841520447167568e-05, 3.841520447167568e-05, 3.841520447167568e-05, 3.841520447167568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.841520447167568e-05

Optimization complete. Final v2v error: 5.31097412109375 mm

Highest mean error: 5.579675674438477 mm for frame 22

Lowest mean error: 4.903345584869385 mm for frame 42

Saving results

Total time: 33.013731479644775
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00227971
Iteration 2/25 | Loss: 0.00122160
Iteration 3/25 | Loss: 0.00112556
Iteration 4/25 | Loss: 0.00109889
Iteration 5/25 | Loss: 0.00109094
Iteration 6/25 | Loss: 0.00108893
Iteration 7/25 | Loss: 0.00108864
Iteration 8/25 | Loss: 0.00108864
Iteration 9/25 | Loss: 0.00108864
Iteration 10/25 | Loss: 0.00108864
Iteration 11/25 | Loss: 0.00108864
Iteration 12/25 | Loss: 0.00108864
Iteration 13/25 | Loss: 0.00108864
Iteration 14/25 | Loss: 0.00108864
Iteration 15/25 | Loss: 0.00108864
Iteration 16/25 | Loss: 0.00108864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010886411182582378, 0.0010886411182582378, 0.0010886411182582378, 0.0010886411182582378, 0.0010886411182582378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010886411182582378

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43918335
Iteration 2/25 | Loss: 0.00045887
Iteration 3/25 | Loss: 0.00045887
Iteration 4/25 | Loss: 0.00045887
Iteration 5/25 | Loss: 0.00045887
Iteration 6/25 | Loss: 0.00045887
Iteration 7/25 | Loss: 0.00045887
Iteration 8/25 | Loss: 0.00045887
Iteration 9/25 | Loss: 0.00045887
Iteration 10/25 | Loss: 0.00045887
Iteration 11/25 | Loss: 0.00045887
Iteration 12/25 | Loss: 0.00045887
Iteration 13/25 | Loss: 0.00045887
Iteration 14/25 | Loss: 0.00045887
Iteration 15/25 | Loss: 0.00045887
Iteration 16/25 | Loss: 0.00045887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004588653100654483, 0.0004588653100654483, 0.0004588653100654483, 0.0004588653100654483, 0.0004588653100654483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004588653100654483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045887
Iteration 2/1000 | Loss: 0.00005819
Iteration 3/1000 | Loss: 0.00003222
Iteration 4/1000 | Loss: 0.00002674
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002155
Iteration 7/1000 | Loss: 0.00002075
Iteration 8/1000 | Loss: 0.00002022
Iteration 9/1000 | Loss: 0.00001992
Iteration 10/1000 | Loss: 0.00001964
Iteration 11/1000 | Loss: 0.00001959
Iteration 12/1000 | Loss: 0.00001959
Iteration 13/1000 | Loss: 0.00001938
Iteration 14/1000 | Loss: 0.00001937
Iteration 15/1000 | Loss: 0.00001932
Iteration 16/1000 | Loss: 0.00001929
Iteration 17/1000 | Loss: 0.00001927
Iteration 18/1000 | Loss: 0.00001925
Iteration 19/1000 | Loss: 0.00001924
Iteration 20/1000 | Loss: 0.00001924
Iteration 21/1000 | Loss: 0.00001923
Iteration 22/1000 | Loss: 0.00001923
Iteration 23/1000 | Loss: 0.00001920
Iteration 24/1000 | Loss: 0.00001920
Iteration 25/1000 | Loss: 0.00001920
Iteration 26/1000 | Loss: 0.00001920
Iteration 27/1000 | Loss: 0.00001920
Iteration 28/1000 | Loss: 0.00001920
Iteration 29/1000 | Loss: 0.00001919
Iteration 30/1000 | Loss: 0.00001919
Iteration 31/1000 | Loss: 0.00001915
Iteration 32/1000 | Loss: 0.00001913
Iteration 33/1000 | Loss: 0.00001913
Iteration 34/1000 | Loss: 0.00001912
Iteration 35/1000 | Loss: 0.00001908
Iteration 36/1000 | Loss: 0.00001907
Iteration 37/1000 | Loss: 0.00001907
Iteration 38/1000 | Loss: 0.00001907
Iteration 39/1000 | Loss: 0.00001906
Iteration 40/1000 | Loss: 0.00001902
Iteration 41/1000 | Loss: 0.00001901
Iteration 42/1000 | Loss: 0.00001901
Iteration 43/1000 | Loss: 0.00001901
Iteration 44/1000 | Loss: 0.00001901
Iteration 45/1000 | Loss: 0.00001900
Iteration 46/1000 | Loss: 0.00001900
Iteration 47/1000 | Loss: 0.00001900
Iteration 48/1000 | Loss: 0.00001899
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001899
Iteration 51/1000 | Loss: 0.00001899
Iteration 52/1000 | Loss: 0.00001899
Iteration 53/1000 | Loss: 0.00001899
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00001898
Iteration 56/1000 | Loss: 0.00001898
Iteration 57/1000 | Loss: 0.00001898
Iteration 58/1000 | Loss: 0.00001898
Iteration 59/1000 | Loss: 0.00001898
Iteration 60/1000 | Loss: 0.00001897
Iteration 61/1000 | Loss: 0.00001897
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001895
Iteration 65/1000 | Loss: 0.00001895
Iteration 66/1000 | Loss: 0.00001895
Iteration 67/1000 | Loss: 0.00001895
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001895
Iteration 71/1000 | Loss: 0.00001895
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001893
Iteration 75/1000 | Loss: 0.00001893
Iteration 76/1000 | Loss: 0.00001892
Iteration 77/1000 | Loss: 0.00001891
Iteration 78/1000 | Loss: 0.00001891
Iteration 79/1000 | Loss: 0.00001891
Iteration 80/1000 | Loss: 0.00001890
Iteration 81/1000 | Loss: 0.00001890
Iteration 82/1000 | Loss: 0.00001890
Iteration 83/1000 | Loss: 0.00001890
Iteration 84/1000 | Loss: 0.00001890
Iteration 85/1000 | Loss: 0.00001890
Iteration 86/1000 | Loss: 0.00001890
Iteration 87/1000 | Loss: 0.00001890
Iteration 88/1000 | Loss: 0.00001890
Iteration 89/1000 | Loss: 0.00001890
Iteration 90/1000 | Loss: 0.00001890
Iteration 91/1000 | Loss: 0.00001890
Iteration 92/1000 | Loss: 0.00001889
Iteration 93/1000 | Loss: 0.00001887
Iteration 94/1000 | Loss: 0.00001887
Iteration 95/1000 | Loss: 0.00001887
Iteration 96/1000 | Loss: 0.00001887
Iteration 97/1000 | Loss: 0.00001887
Iteration 98/1000 | Loss: 0.00001887
Iteration 99/1000 | Loss: 0.00001887
Iteration 100/1000 | Loss: 0.00001887
Iteration 101/1000 | Loss: 0.00001887
Iteration 102/1000 | Loss: 0.00001887
Iteration 103/1000 | Loss: 0.00001885
Iteration 104/1000 | Loss: 0.00001885
Iteration 105/1000 | Loss: 0.00001884
Iteration 106/1000 | Loss: 0.00001884
Iteration 107/1000 | Loss: 0.00001884
Iteration 108/1000 | Loss: 0.00001884
Iteration 109/1000 | Loss: 0.00001884
Iteration 110/1000 | Loss: 0.00001883
Iteration 111/1000 | Loss: 0.00001883
Iteration 112/1000 | Loss: 0.00001883
Iteration 113/1000 | Loss: 0.00001883
Iteration 114/1000 | Loss: 0.00001883
Iteration 115/1000 | Loss: 0.00001882
Iteration 116/1000 | Loss: 0.00001882
Iteration 117/1000 | Loss: 0.00001882
Iteration 118/1000 | Loss: 0.00001881
Iteration 119/1000 | Loss: 0.00001881
Iteration 120/1000 | Loss: 0.00001880
Iteration 121/1000 | Loss: 0.00001879
Iteration 122/1000 | Loss: 0.00001879
Iteration 123/1000 | Loss: 0.00001879
Iteration 124/1000 | Loss: 0.00001879
Iteration 125/1000 | Loss: 0.00001879
Iteration 126/1000 | Loss: 0.00001879
Iteration 127/1000 | Loss: 0.00001879
Iteration 128/1000 | Loss: 0.00001879
Iteration 129/1000 | Loss: 0.00001879
Iteration 130/1000 | Loss: 0.00001879
Iteration 131/1000 | Loss: 0.00001878
Iteration 132/1000 | Loss: 0.00001878
Iteration 133/1000 | Loss: 0.00001878
Iteration 134/1000 | Loss: 0.00001878
Iteration 135/1000 | Loss: 0.00001878
Iteration 136/1000 | Loss: 0.00001878
Iteration 137/1000 | Loss: 0.00001877
Iteration 138/1000 | Loss: 0.00001877
Iteration 139/1000 | Loss: 0.00001877
Iteration 140/1000 | Loss: 0.00001877
Iteration 141/1000 | Loss: 0.00001877
Iteration 142/1000 | Loss: 0.00001877
Iteration 143/1000 | Loss: 0.00001877
Iteration 144/1000 | Loss: 0.00001877
Iteration 145/1000 | Loss: 0.00001877
Iteration 146/1000 | Loss: 0.00001877
Iteration 147/1000 | Loss: 0.00001876
Iteration 148/1000 | Loss: 0.00001876
Iteration 149/1000 | Loss: 0.00001876
Iteration 150/1000 | Loss: 0.00001876
Iteration 151/1000 | Loss: 0.00001876
Iteration 152/1000 | Loss: 0.00001876
Iteration 153/1000 | Loss: 0.00001876
Iteration 154/1000 | Loss: 0.00001876
Iteration 155/1000 | Loss: 0.00001875
Iteration 156/1000 | Loss: 0.00001875
Iteration 157/1000 | Loss: 0.00001875
Iteration 158/1000 | Loss: 0.00001875
Iteration 159/1000 | Loss: 0.00001874
Iteration 160/1000 | Loss: 0.00001874
Iteration 161/1000 | Loss: 0.00001874
Iteration 162/1000 | Loss: 0.00001874
Iteration 163/1000 | Loss: 0.00001874
Iteration 164/1000 | Loss: 0.00001874
Iteration 165/1000 | Loss: 0.00001874
Iteration 166/1000 | Loss: 0.00001874
Iteration 167/1000 | Loss: 0.00001873
Iteration 168/1000 | Loss: 0.00001873
Iteration 169/1000 | Loss: 0.00001873
Iteration 170/1000 | Loss: 0.00001873
Iteration 171/1000 | Loss: 0.00001873
Iteration 172/1000 | Loss: 0.00001873
Iteration 173/1000 | Loss: 0.00001873
Iteration 174/1000 | Loss: 0.00001873
Iteration 175/1000 | Loss: 0.00001873
Iteration 176/1000 | Loss: 0.00001873
Iteration 177/1000 | Loss: 0.00001873
Iteration 178/1000 | Loss: 0.00001873
Iteration 179/1000 | Loss: 0.00001873
Iteration 180/1000 | Loss: 0.00001872
Iteration 181/1000 | Loss: 0.00001872
Iteration 182/1000 | Loss: 0.00001872
Iteration 183/1000 | Loss: 0.00001872
Iteration 184/1000 | Loss: 0.00001872
Iteration 185/1000 | Loss: 0.00001872
Iteration 186/1000 | Loss: 0.00001872
Iteration 187/1000 | Loss: 0.00001872
Iteration 188/1000 | Loss: 0.00001872
Iteration 189/1000 | Loss: 0.00001872
Iteration 190/1000 | Loss: 0.00001872
Iteration 191/1000 | Loss: 0.00001871
Iteration 192/1000 | Loss: 0.00001871
Iteration 193/1000 | Loss: 0.00001871
Iteration 194/1000 | Loss: 0.00001871
Iteration 195/1000 | Loss: 0.00001871
Iteration 196/1000 | Loss: 0.00001871
Iteration 197/1000 | Loss: 0.00001871
Iteration 198/1000 | Loss: 0.00001870
Iteration 199/1000 | Loss: 0.00001870
Iteration 200/1000 | Loss: 0.00001870
Iteration 201/1000 | Loss: 0.00001870
Iteration 202/1000 | Loss: 0.00001870
Iteration 203/1000 | Loss: 0.00001870
Iteration 204/1000 | Loss: 0.00001870
Iteration 205/1000 | Loss: 0.00001870
Iteration 206/1000 | Loss: 0.00001870
Iteration 207/1000 | Loss: 0.00001870
Iteration 208/1000 | Loss: 0.00001869
Iteration 209/1000 | Loss: 0.00001869
Iteration 210/1000 | Loss: 0.00001869
Iteration 211/1000 | Loss: 0.00001869
Iteration 212/1000 | Loss: 0.00001869
Iteration 213/1000 | Loss: 0.00001869
Iteration 214/1000 | Loss: 0.00001869
Iteration 215/1000 | Loss: 0.00001869
Iteration 216/1000 | Loss: 0.00001869
Iteration 217/1000 | Loss: 0.00001869
Iteration 218/1000 | Loss: 0.00001868
Iteration 219/1000 | Loss: 0.00001868
Iteration 220/1000 | Loss: 0.00001868
Iteration 221/1000 | Loss: 0.00001868
Iteration 222/1000 | Loss: 0.00001868
Iteration 223/1000 | Loss: 0.00001868
Iteration 224/1000 | Loss: 0.00001868
Iteration 225/1000 | Loss: 0.00001868
Iteration 226/1000 | Loss: 0.00001868
Iteration 227/1000 | Loss: 0.00001868
Iteration 228/1000 | Loss: 0.00001868
Iteration 229/1000 | Loss: 0.00001868
Iteration 230/1000 | Loss: 0.00001868
Iteration 231/1000 | Loss: 0.00001868
Iteration 232/1000 | Loss: 0.00001868
Iteration 233/1000 | Loss: 0.00001867
Iteration 234/1000 | Loss: 0.00001867
Iteration 235/1000 | Loss: 0.00001867
Iteration 236/1000 | Loss: 0.00001867
Iteration 237/1000 | Loss: 0.00001867
Iteration 238/1000 | Loss: 0.00001867
Iteration 239/1000 | Loss: 0.00001867
Iteration 240/1000 | Loss: 0.00001867
Iteration 241/1000 | Loss: 0.00001866
Iteration 242/1000 | Loss: 0.00001866
Iteration 243/1000 | Loss: 0.00001866
Iteration 244/1000 | Loss: 0.00001866
Iteration 245/1000 | Loss: 0.00001866
Iteration 246/1000 | Loss: 0.00001865
Iteration 247/1000 | Loss: 0.00001865
Iteration 248/1000 | Loss: 0.00001865
Iteration 249/1000 | Loss: 0.00001865
Iteration 250/1000 | Loss: 0.00001865
Iteration 251/1000 | Loss: 0.00001865
Iteration 252/1000 | Loss: 0.00001865
Iteration 253/1000 | Loss: 0.00001865
Iteration 254/1000 | Loss: 0.00001865
Iteration 255/1000 | Loss: 0.00001865
Iteration 256/1000 | Loss: 0.00001865
Iteration 257/1000 | Loss: 0.00001864
Iteration 258/1000 | Loss: 0.00001864
Iteration 259/1000 | Loss: 0.00001864
Iteration 260/1000 | Loss: 0.00001863
Iteration 261/1000 | Loss: 0.00001863
Iteration 262/1000 | Loss: 0.00001863
Iteration 263/1000 | Loss: 0.00001863
Iteration 264/1000 | Loss: 0.00001862
Iteration 265/1000 | Loss: 0.00001862
Iteration 266/1000 | Loss: 0.00001862
Iteration 267/1000 | Loss: 0.00001862
Iteration 268/1000 | Loss: 0.00001862
Iteration 269/1000 | Loss: 0.00001862
Iteration 270/1000 | Loss: 0.00001862
Iteration 271/1000 | Loss: 0.00001862
Iteration 272/1000 | Loss: 0.00001862
Iteration 273/1000 | Loss: 0.00001861
Iteration 274/1000 | Loss: 0.00001861
Iteration 275/1000 | Loss: 0.00001861
Iteration 276/1000 | Loss: 0.00001861
Iteration 277/1000 | Loss: 0.00001860
Iteration 278/1000 | Loss: 0.00001860
Iteration 279/1000 | Loss: 0.00001860
Iteration 280/1000 | Loss: 0.00001860
Iteration 281/1000 | Loss: 0.00001860
Iteration 282/1000 | Loss: 0.00001860
Iteration 283/1000 | Loss: 0.00001860
Iteration 284/1000 | Loss: 0.00001860
Iteration 285/1000 | Loss: 0.00001860
Iteration 286/1000 | Loss: 0.00001860
Iteration 287/1000 | Loss: 0.00001860
Iteration 288/1000 | Loss: 0.00001860
Iteration 289/1000 | Loss: 0.00001859
Iteration 290/1000 | Loss: 0.00001858
Iteration 291/1000 | Loss: 0.00001858
Iteration 292/1000 | Loss: 0.00001858
Iteration 293/1000 | Loss: 0.00001858
Iteration 294/1000 | Loss: 0.00001858
Iteration 295/1000 | Loss: 0.00001858
Iteration 296/1000 | Loss: 0.00001858
Iteration 297/1000 | Loss: 0.00001858
Iteration 298/1000 | Loss: 0.00001857
Iteration 299/1000 | Loss: 0.00001857
Iteration 300/1000 | Loss: 0.00001857
Iteration 301/1000 | Loss: 0.00001857
Iteration 302/1000 | Loss: 0.00001857
Iteration 303/1000 | Loss: 0.00001857
Iteration 304/1000 | Loss: 0.00001857
Iteration 305/1000 | Loss: 0.00001856
Iteration 306/1000 | Loss: 0.00001856
Iteration 307/1000 | Loss: 0.00001856
Iteration 308/1000 | Loss: 0.00001856
Iteration 309/1000 | Loss: 0.00001856
Iteration 310/1000 | Loss: 0.00001856
Iteration 311/1000 | Loss: 0.00001856
Iteration 312/1000 | Loss: 0.00001856
Iteration 313/1000 | Loss: 0.00001855
Iteration 314/1000 | Loss: 0.00001855
Iteration 315/1000 | Loss: 0.00001855
Iteration 316/1000 | Loss: 0.00001855
Iteration 317/1000 | Loss: 0.00001855
Iteration 318/1000 | Loss: 0.00001855
Iteration 319/1000 | Loss: 0.00001855
Iteration 320/1000 | Loss: 0.00001855
Iteration 321/1000 | Loss: 0.00001855
Iteration 322/1000 | Loss: 0.00001854
Iteration 323/1000 | Loss: 0.00001854
Iteration 324/1000 | Loss: 0.00001854
Iteration 325/1000 | Loss: 0.00001854
Iteration 326/1000 | Loss: 0.00001854
Iteration 327/1000 | Loss: 0.00001854
Iteration 328/1000 | Loss: 0.00001854
Iteration 329/1000 | Loss: 0.00001854
Iteration 330/1000 | Loss: 0.00001854
Iteration 331/1000 | Loss: 0.00001854
Iteration 332/1000 | Loss: 0.00001854
Iteration 333/1000 | Loss: 0.00001854
Iteration 334/1000 | Loss: 0.00001854
Iteration 335/1000 | Loss: 0.00001854
Iteration 336/1000 | Loss: 0.00001854
Iteration 337/1000 | Loss: 0.00001853
Iteration 338/1000 | Loss: 0.00001853
Iteration 339/1000 | Loss: 0.00001853
Iteration 340/1000 | Loss: 0.00001853
Iteration 341/1000 | Loss: 0.00001853
Iteration 342/1000 | Loss: 0.00001853
Iteration 343/1000 | Loss: 0.00001853
Iteration 344/1000 | Loss: 0.00001853
Iteration 345/1000 | Loss: 0.00001853
Iteration 346/1000 | Loss: 0.00001853
Iteration 347/1000 | Loss: 0.00001853
Iteration 348/1000 | Loss: 0.00001853
Iteration 349/1000 | Loss: 0.00001853
Iteration 350/1000 | Loss: 0.00001853
Iteration 351/1000 | Loss: 0.00001853
Iteration 352/1000 | Loss: 0.00001853
Iteration 353/1000 | Loss: 0.00001853
Iteration 354/1000 | Loss: 0.00001852
Iteration 355/1000 | Loss: 0.00001852
Iteration 356/1000 | Loss: 0.00001852
Iteration 357/1000 | Loss: 0.00001852
Iteration 358/1000 | Loss: 0.00001852
Iteration 359/1000 | Loss: 0.00001852
Iteration 360/1000 | Loss: 0.00001852
Iteration 361/1000 | Loss: 0.00001852
Iteration 362/1000 | Loss: 0.00001852
Iteration 363/1000 | Loss: 0.00001852
Iteration 364/1000 | Loss: 0.00001852
Iteration 365/1000 | Loss: 0.00001852
Iteration 366/1000 | Loss: 0.00001852
Iteration 367/1000 | Loss: 0.00001852
Iteration 368/1000 | Loss: 0.00001852
Iteration 369/1000 | Loss: 0.00001852
Iteration 370/1000 | Loss: 0.00001851
Iteration 371/1000 | Loss: 0.00001851
Iteration 372/1000 | Loss: 0.00001851
Iteration 373/1000 | Loss: 0.00001851
Iteration 374/1000 | Loss: 0.00001851
Iteration 375/1000 | Loss: 0.00001851
Iteration 376/1000 | Loss: 0.00001851
Iteration 377/1000 | Loss: 0.00001851
Iteration 378/1000 | Loss: 0.00001851
Iteration 379/1000 | Loss: 0.00001851
Iteration 380/1000 | Loss: 0.00001851
Iteration 381/1000 | Loss: 0.00001851
Iteration 382/1000 | Loss: 0.00001851
Iteration 383/1000 | Loss: 0.00001851
Iteration 384/1000 | Loss: 0.00001851
Iteration 385/1000 | Loss: 0.00001851
Iteration 386/1000 | Loss: 0.00001851
Iteration 387/1000 | Loss: 0.00001851
Iteration 388/1000 | Loss: 0.00001851
Iteration 389/1000 | Loss: 0.00001851
Iteration 390/1000 | Loss: 0.00001851
Iteration 391/1000 | Loss: 0.00001851
Iteration 392/1000 | Loss: 0.00001851
Iteration 393/1000 | Loss: 0.00001851
Iteration 394/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 394. Stopping optimization.
Last 5 losses: [1.8512415408622473e-05, 1.8512415408622473e-05, 1.8512415408622473e-05, 1.8512415408622473e-05, 1.8512415408622473e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8512415408622473e-05

Optimization complete. Final v2v error: 3.804267168045044 mm

Highest mean error: 4.280675411224365 mm for frame 3

Lowest mean error: 3.6041629314422607 mm for frame 177

Saving results

Total time: 54.169002056121826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911017
Iteration 2/25 | Loss: 0.00117464
Iteration 3/25 | Loss: 0.00105057
Iteration 4/25 | Loss: 0.00103427
Iteration 5/25 | Loss: 0.00102916
Iteration 6/25 | Loss: 0.00102747
Iteration 7/25 | Loss: 0.00102747
Iteration 8/25 | Loss: 0.00102747
Iteration 9/25 | Loss: 0.00102747
Iteration 10/25 | Loss: 0.00102747
Iteration 11/25 | Loss: 0.00102747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010274742962792516, 0.0010274742962792516, 0.0010274742962792516, 0.0010274742962792516, 0.0010274742962792516]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010274742962792516

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43635941
Iteration 2/25 | Loss: 0.00055872
Iteration 3/25 | Loss: 0.00055872
Iteration 4/25 | Loss: 0.00055872
Iteration 5/25 | Loss: 0.00055872
Iteration 6/25 | Loss: 0.00055872
Iteration 7/25 | Loss: 0.00055872
Iteration 8/25 | Loss: 0.00055872
Iteration 9/25 | Loss: 0.00055872
Iteration 10/25 | Loss: 0.00055872
Iteration 11/25 | Loss: 0.00055872
Iteration 12/25 | Loss: 0.00055872
Iteration 13/25 | Loss: 0.00055872
Iteration 14/25 | Loss: 0.00055872
Iteration 15/25 | Loss: 0.00055872
Iteration 16/25 | Loss: 0.00055872
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000558717641979456, 0.000558717641979456, 0.000558717641979456, 0.000558717641979456, 0.000558717641979456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000558717641979456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055872
Iteration 2/1000 | Loss: 0.00005741
Iteration 3/1000 | Loss: 0.00003766
Iteration 4/1000 | Loss: 0.00003095
Iteration 5/1000 | Loss: 0.00002753
Iteration 6/1000 | Loss: 0.00002604
Iteration 7/1000 | Loss: 0.00002544
Iteration 8/1000 | Loss: 0.00002507
Iteration 9/1000 | Loss: 0.00002488
Iteration 10/1000 | Loss: 0.00002474
Iteration 11/1000 | Loss: 0.00002469
Iteration 12/1000 | Loss: 0.00002469
Iteration 13/1000 | Loss: 0.00002469
Iteration 14/1000 | Loss: 0.00002467
Iteration 15/1000 | Loss: 0.00002460
Iteration 16/1000 | Loss: 0.00002460
Iteration 17/1000 | Loss: 0.00002459
Iteration 18/1000 | Loss: 0.00002459
Iteration 19/1000 | Loss: 0.00002458
Iteration 20/1000 | Loss: 0.00002458
Iteration 21/1000 | Loss: 0.00002457
Iteration 22/1000 | Loss: 0.00002457
Iteration 23/1000 | Loss: 0.00002457
Iteration 24/1000 | Loss: 0.00002456
Iteration 25/1000 | Loss: 0.00002456
Iteration 26/1000 | Loss: 0.00002455
Iteration 27/1000 | Loss: 0.00002455
Iteration 28/1000 | Loss: 0.00002454
Iteration 29/1000 | Loss: 0.00002454
Iteration 30/1000 | Loss: 0.00002454
Iteration 31/1000 | Loss: 0.00002454
Iteration 32/1000 | Loss: 0.00002454
Iteration 33/1000 | Loss: 0.00002453
Iteration 34/1000 | Loss: 0.00002453
Iteration 35/1000 | Loss: 0.00002453
Iteration 36/1000 | Loss: 0.00002453
Iteration 37/1000 | Loss: 0.00002452
Iteration 38/1000 | Loss: 0.00002452
Iteration 39/1000 | Loss: 0.00002452
Iteration 40/1000 | Loss: 0.00002452
Iteration 41/1000 | Loss: 0.00002452
Iteration 42/1000 | Loss: 0.00002452
Iteration 43/1000 | Loss: 0.00002451
Iteration 44/1000 | Loss: 0.00002451
Iteration 45/1000 | Loss: 0.00002451
Iteration 46/1000 | Loss: 0.00002451
Iteration 47/1000 | Loss: 0.00002451
Iteration 48/1000 | Loss: 0.00002451
Iteration 49/1000 | Loss: 0.00002451
Iteration 50/1000 | Loss: 0.00002451
Iteration 51/1000 | Loss: 0.00002451
Iteration 52/1000 | Loss: 0.00002451
Iteration 53/1000 | Loss: 0.00002451
Iteration 54/1000 | Loss: 0.00002450
Iteration 55/1000 | Loss: 0.00002450
Iteration 56/1000 | Loss: 0.00002450
Iteration 57/1000 | Loss: 0.00002450
Iteration 58/1000 | Loss: 0.00002450
Iteration 59/1000 | Loss: 0.00002450
Iteration 60/1000 | Loss: 0.00002450
Iteration 61/1000 | Loss: 0.00002450
Iteration 62/1000 | Loss: 0.00002450
Iteration 63/1000 | Loss: 0.00002450
Iteration 64/1000 | Loss: 0.00002450
Iteration 65/1000 | Loss: 0.00002450
Iteration 66/1000 | Loss: 0.00002450
Iteration 67/1000 | Loss: 0.00002450
Iteration 68/1000 | Loss: 0.00002450
Iteration 69/1000 | Loss: 0.00002450
Iteration 70/1000 | Loss: 0.00002450
Iteration 71/1000 | Loss: 0.00002450
Iteration 72/1000 | Loss: 0.00002450
Iteration 73/1000 | Loss: 0.00002450
Iteration 74/1000 | Loss: 0.00002450
Iteration 75/1000 | Loss: 0.00002450
Iteration 76/1000 | Loss: 0.00002450
Iteration 77/1000 | Loss: 0.00002450
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [2.4503386157448404e-05, 2.4503386157448404e-05, 2.4503386157448404e-05, 2.4503386157448404e-05, 2.4503386157448404e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4503386157448404e-05

Optimization complete. Final v2v error: 4.236273765563965 mm

Highest mean error: 4.7462992668151855 mm for frame 100

Lowest mean error: 3.868215322494507 mm for frame 170

Saving results

Total time: 29.328146934509277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407651
Iteration 2/25 | Loss: 0.00118285
Iteration 3/25 | Loss: 0.00103708
Iteration 4/25 | Loss: 0.00102418
Iteration 5/25 | Loss: 0.00102008
Iteration 6/25 | Loss: 0.00101897
Iteration 7/25 | Loss: 0.00101874
Iteration 8/25 | Loss: 0.00101874
Iteration 9/25 | Loss: 0.00101874
Iteration 10/25 | Loss: 0.00101874
Iteration 11/25 | Loss: 0.00101874
Iteration 12/25 | Loss: 0.00101874
Iteration 13/25 | Loss: 0.00101874
Iteration 14/25 | Loss: 0.00101874
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010187412844970822, 0.0010187412844970822, 0.0010187412844970822, 0.0010187412844970822, 0.0010187412844970822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010187412844970822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52125919
Iteration 2/25 | Loss: 0.00051751
Iteration 3/25 | Loss: 0.00051751
Iteration 4/25 | Loss: 0.00051751
Iteration 5/25 | Loss: 0.00051751
Iteration 6/25 | Loss: 0.00051751
Iteration 7/25 | Loss: 0.00051751
Iteration 8/25 | Loss: 0.00051751
Iteration 9/25 | Loss: 0.00051751
Iteration 10/25 | Loss: 0.00051751
Iteration 11/25 | Loss: 0.00051751
Iteration 12/25 | Loss: 0.00051751
Iteration 13/25 | Loss: 0.00051751
Iteration 14/25 | Loss: 0.00051751
Iteration 15/25 | Loss: 0.00051751
Iteration 16/25 | Loss: 0.00051751
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005175090045668185, 0.0005175090045668185, 0.0005175090045668185, 0.0005175090045668185, 0.0005175090045668185]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005175090045668185

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051751
Iteration 2/1000 | Loss: 0.00008522
Iteration 3/1000 | Loss: 0.00004386
Iteration 4/1000 | Loss: 0.00003384
Iteration 5/1000 | Loss: 0.00002926
Iteration 6/1000 | Loss: 0.00002604
Iteration 7/1000 | Loss: 0.00002389
Iteration 8/1000 | Loss: 0.00002284
Iteration 9/1000 | Loss: 0.00002250
Iteration 10/1000 | Loss: 0.00002230
Iteration 11/1000 | Loss: 0.00002213
Iteration 12/1000 | Loss: 0.00002192
Iteration 13/1000 | Loss: 0.00002176
Iteration 14/1000 | Loss: 0.00002175
Iteration 15/1000 | Loss: 0.00002175
Iteration 16/1000 | Loss: 0.00002172
Iteration 17/1000 | Loss: 0.00002172
Iteration 18/1000 | Loss: 0.00002171
Iteration 19/1000 | Loss: 0.00002171
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002169
Iteration 22/1000 | Loss: 0.00002166
Iteration 23/1000 | Loss: 0.00002165
Iteration 24/1000 | Loss: 0.00002163
Iteration 25/1000 | Loss: 0.00002163
Iteration 26/1000 | Loss: 0.00002163
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002163
Iteration 29/1000 | Loss: 0.00002163
Iteration 30/1000 | Loss: 0.00002163
Iteration 31/1000 | Loss: 0.00002163
Iteration 32/1000 | Loss: 0.00002162
Iteration 33/1000 | Loss: 0.00002162
Iteration 34/1000 | Loss: 0.00002162
Iteration 35/1000 | Loss: 0.00002161
Iteration 36/1000 | Loss: 0.00002161
Iteration 37/1000 | Loss: 0.00002161
Iteration 38/1000 | Loss: 0.00002161
Iteration 39/1000 | Loss: 0.00002160
Iteration 40/1000 | Loss: 0.00002160
Iteration 41/1000 | Loss: 0.00002160
Iteration 42/1000 | Loss: 0.00002160
Iteration 43/1000 | Loss: 0.00002160
Iteration 44/1000 | Loss: 0.00002160
Iteration 45/1000 | Loss: 0.00002159
Iteration 46/1000 | Loss: 0.00002159
Iteration 47/1000 | Loss: 0.00002159
Iteration 48/1000 | Loss: 0.00002159
Iteration 49/1000 | Loss: 0.00002159
Iteration 50/1000 | Loss: 0.00002159
Iteration 51/1000 | Loss: 0.00002159
Iteration 52/1000 | Loss: 0.00002159
Iteration 53/1000 | Loss: 0.00002159
Iteration 54/1000 | Loss: 0.00002159
Iteration 55/1000 | Loss: 0.00002159
Iteration 56/1000 | Loss: 0.00002159
Iteration 57/1000 | Loss: 0.00002159
Iteration 58/1000 | Loss: 0.00002159
Iteration 59/1000 | Loss: 0.00002159
Iteration 60/1000 | Loss: 0.00002159
Iteration 61/1000 | Loss: 0.00002158
Iteration 62/1000 | Loss: 0.00002158
Iteration 63/1000 | Loss: 0.00002158
Iteration 64/1000 | Loss: 0.00002158
Iteration 65/1000 | Loss: 0.00002158
Iteration 66/1000 | Loss: 0.00002158
Iteration 67/1000 | Loss: 0.00002158
Iteration 68/1000 | Loss: 0.00002158
Iteration 69/1000 | Loss: 0.00002158
Iteration 70/1000 | Loss: 0.00002158
Iteration 71/1000 | Loss: 0.00002158
Iteration 72/1000 | Loss: 0.00002158
Iteration 73/1000 | Loss: 0.00002158
Iteration 74/1000 | Loss: 0.00002158
Iteration 75/1000 | Loss: 0.00002158
Iteration 76/1000 | Loss: 0.00002158
Iteration 77/1000 | Loss: 0.00002158
Iteration 78/1000 | Loss: 0.00002158
Iteration 79/1000 | Loss: 0.00002158
Iteration 80/1000 | Loss: 0.00002158
Iteration 81/1000 | Loss: 0.00002158
Iteration 82/1000 | Loss: 0.00002158
Iteration 83/1000 | Loss: 0.00002158
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [2.1580424800049514e-05, 2.1580424800049514e-05, 2.1580424800049514e-05, 2.1580424800049514e-05, 2.1580424800049514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1580424800049514e-05

Optimization complete. Final v2v error: 3.9662766456604004 mm

Highest mean error: 4.474372863769531 mm for frame 61

Lowest mean error: 3.623237371444702 mm for frame 34

Saving results

Total time: 32.047362089157104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00865063
Iteration 2/25 | Loss: 0.00113135
Iteration 3/25 | Loss: 0.00101653
Iteration 4/25 | Loss: 0.00100461
Iteration 5/25 | Loss: 0.00100198
Iteration 6/25 | Loss: 0.00100062
Iteration 7/25 | Loss: 0.00100058
Iteration 8/25 | Loss: 0.00100058
Iteration 9/25 | Loss: 0.00100058
Iteration 10/25 | Loss: 0.00100058
Iteration 11/25 | Loss: 0.00100058
Iteration 12/25 | Loss: 0.00100058
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001000577467493713, 0.001000577467493713, 0.001000577467493713, 0.001000577467493713, 0.001000577467493713]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001000577467493713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42456257
Iteration 2/25 | Loss: 0.00050984
Iteration 3/25 | Loss: 0.00050984
Iteration 4/25 | Loss: 0.00050984
Iteration 5/25 | Loss: 0.00050983
Iteration 6/25 | Loss: 0.00050983
Iteration 7/25 | Loss: 0.00050983
Iteration 8/25 | Loss: 0.00050983
Iteration 9/25 | Loss: 0.00050983
Iteration 10/25 | Loss: 0.00050983
Iteration 11/25 | Loss: 0.00050983
Iteration 12/25 | Loss: 0.00050983
Iteration 13/25 | Loss: 0.00050983
Iteration 14/25 | Loss: 0.00050983
Iteration 15/25 | Loss: 0.00050983
Iteration 16/25 | Loss: 0.00050983
Iteration 17/25 | Loss: 0.00050983
Iteration 18/25 | Loss: 0.00050983
Iteration 19/25 | Loss: 0.00050983
Iteration 20/25 | Loss: 0.00050983
Iteration 21/25 | Loss: 0.00050983
Iteration 22/25 | Loss: 0.00050983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0005098339752294123, 0.0005098339752294123, 0.0005098339752294123, 0.0005098339752294123, 0.0005098339752294123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005098339752294123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050983
Iteration 2/1000 | Loss: 0.00006310
Iteration 3/1000 | Loss: 0.00003743
Iteration 4/1000 | Loss: 0.00002955
Iteration 5/1000 | Loss: 0.00002549
Iteration 6/1000 | Loss: 0.00002321
Iteration 7/1000 | Loss: 0.00002218
Iteration 8/1000 | Loss: 0.00002173
Iteration 9/1000 | Loss: 0.00002147
Iteration 10/1000 | Loss: 0.00002146
Iteration 11/1000 | Loss: 0.00002126
Iteration 12/1000 | Loss: 0.00002111
Iteration 13/1000 | Loss: 0.00002107
Iteration 14/1000 | Loss: 0.00002099
Iteration 15/1000 | Loss: 0.00002098
Iteration 16/1000 | Loss: 0.00002097
Iteration 17/1000 | Loss: 0.00002097
Iteration 18/1000 | Loss: 0.00002095
Iteration 19/1000 | Loss: 0.00002095
Iteration 20/1000 | Loss: 0.00002095
Iteration 21/1000 | Loss: 0.00002093
Iteration 22/1000 | Loss: 0.00002093
Iteration 23/1000 | Loss: 0.00002093
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002092
Iteration 27/1000 | Loss: 0.00002092
Iteration 28/1000 | Loss: 0.00002092
Iteration 29/1000 | Loss: 0.00002092
Iteration 30/1000 | Loss: 0.00002092
Iteration 31/1000 | Loss: 0.00002091
Iteration 32/1000 | Loss: 0.00002091
Iteration 33/1000 | Loss: 0.00002091
Iteration 34/1000 | Loss: 0.00002090
Iteration 35/1000 | Loss: 0.00002090
Iteration 36/1000 | Loss: 0.00002089
Iteration 37/1000 | Loss: 0.00002089
Iteration 38/1000 | Loss: 0.00002089
Iteration 39/1000 | Loss: 0.00002089
Iteration 40/1000 | Loss: 0.00002089
Iteration 41/1000 | Loss: 0.00002089
Iteration 42/1000 | Loss: 0.00002089
Iteration 43/1000 | Loss: 0.00002088
Iteration 44/1000 | Loss: 0.00002088
Iteration 45/1000 | Loss: 0.00002088
Iteration 46/1000 | Loss: 0.00002088
Iteration 47/1000 | Loss: 0.00002088
Iteration 48/1000 | Loss: 0.00002088
Iteration 49/1000 | Loss: 0.00002088
Iteration 50/1000 | Loss: 0.00002088
Iteration 51/1000 | Loss: 0.00002087
Iteration 52/1000 | Loss: 0.00002087
Iteration 53/1000 | Loss: 0.00002087
Iteration 54/1000 | Loss: 0.00002087
Iteration 55/1000 | Loss: 0.00002087
Iteration 56/1000 | Loss: 0.00002087
Iteration 57/1000 | Loss: 0.00002087
Iteration 58/1000 | Loss: 0.00002087
Iteration 59/1000 | Loss: 0.00002087
Iteration 60/1000 | Loss: 0.00002087
Iteration 61/1000 | Loss: 0.00002087
Iteration 62/1000 | Loss: 0.00002087
Iteration 63/1000 | Loss: 0.00002087
Iteration 64/1000 | Loss: 0.00002086
Iteration 65/1000 | Loss: 0.00002086
Iteration 66/1000 | Loss: 0.00002086
Iteration 67/1000 | Loss: 0.00002086
Iteration 68/1000 | Loss: 0.00002086
Iteration 69/1000 | Loss: 0.00002086
Iteration 70/1000 | Loss: 0.00002086
Iteration 71/1000 | Loss: 0.00002086
Iteration 72/1000 | Loss: 0.00002086
Iteration 73/1000 | Loss: 0.00002086
Iteration 74/1000 | Loss: 0.00002086
Iteration 75/1000 | Loss: 0.00002086
Iteration 76/1000 | Loss: 0.00002085
Iteration 77/1000 | Loss: 0.00002085
Iteration 78/1000 | Loss: 0.00002085
Iteration 79/1000 | Loss: 0.00002085
Iteration 80/1000 | Loss: 0.00002085
Iteration 81/1000 | Loss: 0.00002085
Iteration 82/1000 | Loss: 0.00002085
Iteration 83/1000 | Loss: 0.00002085
Iteration 84/1000 | Loss: 0.00002085
Iteration 85/1000 | Loss: 0.00002085
Iteration 86/1000 | Loss: 0.00002085
Iteration 87/1000 | Loss: 0.00002085
Iteration 88/1000 | Loss: 0.00002084
Iteration 89/1000 | Loss: 0.00002084
Iteration 90/1000 | Loss: 0.00002084
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002084
Iteration 93/1000 | Loss: 0.00002084
Iteration 94/1000 | Loss: 0.00002084
Iteration 95/1000 | Loss: 0.00002084
Iteration 96/1000 | Loss: 0.00002084
Iteration 97/1000 | Loss: 0.00002084
Iteration 98/1000 | Loss: 0.00002084
Iteration 99/1000 | Loss: 0.00002084
Iteration 100/1000 | Loss: 0.00002084
Iteration 101/1000 | Loss: 0.00002084
Iteration 102/1000 | Loss: 0.00002084
Iteration 103/1000 | Loss: 0.00002084
Iteration 104/1000 | Loss: 0.00002084
Iteration 105/1000 | Loss: 0.00002083
Iteration 106/1000 | Loss: 0.00002083
Iteration 107/1000 | Loss: 0.00002083
Iteration 108/1000 | Loss: 0.00002083
Iteration 109/1000 | Loss: 0.00002083
Iteration 110/1000 | Loss: 0.00002083
Iteration 111/1000 | Loss: 0.00002083
Iteration 112/1000 | Loss: 0.00002083
Iteration 113/1000 | Loss: 0.00002082
Iteration 114/1000 | Loss: 0.00002082
Iteration 115/1000 | Loss: 0.00002082
Iteration 116/1000 | Loss: 0.00002082
Iteration 117/1000 | Loss: 0.00002082
Iteration 118/1000 | Loss: 0.00002082
Iteration 119/1000 | Loss: 0.00002082
Iteration 120/1000 | Loss: 0.00002082
Iteration 121/1000 | Loss: 0.00002082
Iteration 122/1000 | Loss: 0.00002082
Iteration 123/1000 | Loss: 0.00002082
Iteration 124/1000 | Loss: 0.00002082
Iteration 125/1000 | Loss: 0.00002082
Iteration 126/1000 | Loss: 0.00002081
Iteration 127/1000 | Loss: 0.00002081
Iteration 128/1000 | Loss: 0.00002081
Iteration 129/1000 | Loss: 0.00002081
Iteration 130/1000 | Loss: 0.00002081
Iteration 131/1000 | Loss: 0.00002081
Iteration 132/1000 | Loss: 0.00002081
Iteration 133/1000 | Loss: 0.00002081
Iteration 134/1000 | Loss: 0.00002081
Iteration 135/1000 | Loss: 0.00002080
Iteration 136/1000 | Loss: 0.00002080
Iteration 137/1000 | Loss: 0.00002080
Iteration 138/1000 | Loss: 0.00002080
Iteration 139/1000 | Loss: 0.00002080
Iteration 140/1000 | Loss: 0.00002080
Iteration 141/1000 | Loss: 0.00002080
Iteration 142/1000 | Loss: 0.00002080
Iteration 143/1000 | Loss: 0.00002080
Iteration 144/1000 | Loss: 0.00002079
Iteration 145/1000 | Loss: 0.00002079
Iteration 146/1000 | Loss: 0.00002079
Iteration 147/1000 | Loss: 0.00002079
Iteration 148/1000 | Loss: 0.00002079
Iteration 149/1000 | Loss: 0.00002079
Iteration 150/1000 | Loss: 0.00002079
Iteration 151/1000 | Loss: 0.00002079
Iteration 152/1000 | Loss: 0.00002079
Iteration 153/1000 | Loss: 0.00002079
Iteration 154/1000 | Loss: 0.00002079
Iteration 155/1000 | Loss: 0.00002079
Iteration 156/1000 | Loss: 0.00002079
Iteration 157/1000 | Loss: 0.00002079
Iteration 158/1000 | Loss: 0.00002079
Iteration 159/1000 | Loss: 0.00002079
Iteration 160/1000 | Loss: 0.00002079
Iteration 161/1000 | Loss: 0.00002079
Iteration 162/1000 | Loss: 0.00002079
Iteration 163/1000 | Loss: 0.00002079
Iteration 164/1000 | Loss: 0.00002079
Iteration 165/1000 | Loss: 0.00002079
Iteration 166/1000 | Loss: 0.00002079
Iteration 167/1000 | Loss: 0.00002079
Iteration 168/1000 | Loss: 0.00002079
Iteration 169/1000 | Loss: 0.00002079
Iteration 170/1000 | Loss: 0.00002079
Iteration 171/1000 | Loss: 0.00002079
Iteration 172/1000 | Loss: 0.00002079
Iteration 173/1000 | Loss: 0.00002079
Iteration 174/1000 | Loss: 0.00002079
Iteration 175/1000 | Loss: 0.00002079
Iteration 176/1000 | Loss: 0.00002079
Iteration 177/1000 | Loss: 0.00002079
Iteration 178/1000 | Loss: 0.00002079
Iteration 179/1000 | Loss: 0.00002079
Iteration 180/1000 | Loss: 0.00002079
Iteration 181/1000 | Loss: 0.00002079
Iteration 182/1000 | Loss: 0.00002079
Iteration 183/1000 | Loss: 0.00002079
Iteration 184/1000 | Loss: 0.00002079
Iteration 185/1000 | Loss: 0.00002079
Iteration 186/1000 | Loss: 0.00002079
Iteration 187/1000 | Loss: 0.00002079
Iteration 188/1000 | Loss: 0.00002079
Iteration 189/1000 | Loss: 0.00002079
Iteration 190/1000 | Loss: 0.00002079
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.0785109882126562e-05, 2.0785109882126562e-05, 2.0785109882126562e-05, 2.0785109882126562e-05, 2.0785109882126562e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0785109882126562e-05

Optimization complete. Final v2v error: 3.9199957847595215 mm

Highest mean error: 4.155102729797363 mm for frame 33

Lowest mean error: 3.7853429317474365 mm for frame 155

Saving results

Total time: 36.20563769340515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884888
Iteration 2/25 | Loss: 0.00120452
Iteration 3/25 | Loss: 0.00105106
Iteration 4/25 | Loss: 0.00102942
Iteration 5/25 | Loss: 0.00102566
Iteration 6/25 | Loss: 0.00102412
Iteration 7/25 | Loss: 0.00102396
Iteration 8/25 | Loss: 0.00102396
Iteration 9/25 | Loss: 0.00102396
Iteration 10/25 | Loss: 0.00102396
Iteration 11/25 | Loss: 0.00102396
Iteration 12/25 | Loss: 0.00102396
Iteration 13/25 | Loss: 0.00102396
Iteration 14/25 | Loss: 0.00102396
Iteration 15/25 | Loss: 0.00102396
Iteration 16/25 | Loss: 0.00102396
Iteration 17/25 | Loss: 0.00102396
Iteration 18/25 | Loss: 0.00102396
Iteration 19/25 | Loss: 0.00102396
Iteration 20/25 | Loss: 0.00102396
Iteration 21/25 | Loss: 0.00102396
Iteration 22/25 | Loss: 0.00102396
Iteration 23/25 | Loss: 0.00102396
Iteration 24/25 | Loss: 0.00102396
Iteration 25/25 | Loss: 0.00102396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36794472
Iteration 2/25 | Loss: 0.00058854
Iteration 3/25 | Loss: 0.00058853
Iteration 4/25 | Loss: 0.00058853
Iteration 5/25 | Loss: 0.00058853
Iteration 6/25 | Loss: 0.00058853
Iteration 7/25 | Loss: 0.00058853
Iteration 8/25 | Loss: 0.00058853
Iteration 9/25 | Loss: 0.00058853
Iteration 10/25 | Loss: 0.00058853
Iteration 11/25 | Loss: 0.00058853
Iteration 12/25 | Loss: 0.00058853
Iteration 13/25 | Loss: 0.00058853
Iteration 14/25 | Loss: 0.00058853
Iteration 15/25 | Loss: 0.00058853
Iteration 16/25 | Loss: 0.00058853
Iteration 17/25 | Loss: 0.00058853
Iteration 18/25 | Loss: 0.00058853
Iteration 19/25 | Loss: 0.00058853
Iteration 20/25 | Loss: 0.00058853
Iteration 21/25 | Loss: 0.00058853
Iteration 22/25 | Loss: 0.00058853
Iteration 23/25 | Loss: 0.00058853
Iteration 24/25 | Loss: 0.00058853
Iteration 25/25 | Loss: 0.00058853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058853
Iteration 2/1000 | Loss: 0.00005707
Iteration 3/1000 | Loss: 0.00003310
Iteration 4/1000 | Loss: 0.00002649
Iteration 5/1000 | Loss: 0.00002379
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00002100
Iteration 8/1000 | Loss: 0.00002065
Iteration 9/1000 | Loss: 0.00002045
Iteration 10/1000 | Loss: 0.00002022
Iteration 11/1000 | Loss: 0.00002012
Iteration 12/1000 | Loss: 0.00002003
Iteration 13/1000 | Loss: 0.00002003
Iteration 14/1000 | Loss: 0.00002003
Iteration 15/1000 | Loss: 0.00002001
Iteration 16/1000 | Loss: 0.00002000
Iteration 17/1000 | Loss: 0.00002000
Iteration 18/1000 | Loss: 0.00002000
Iteration 19/1000 | Loss: 0.00001999
Iteration 20/1000 | Loss: 0.00001998
Iteration 21/1000 | Loss: 0.00001998
Iteration 22/1000 | Loss: 0.00001998
Iteration 23/1000 | Loss: 0.00001998
Iteration 24/1000 | Loss: 0.00001998
Iteration 25/1000 | Loss: 0.00001998
Iteration 26/1000 | Loss: 0.00001998
Iteration 27/1000 | Loss: 0.00001998
Iteration 28/1000 | Loss: 0.00001997
Iteration 29/1000 | Loss: 0.00001997
Iteration 30/1000 | Loss: 0.00001997
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001996
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001995
Iteration 36/1000 | Loss: 0.00001995
Iteration 37/1000 | Loss: 0.00001995
Iteration 38/1000 | Loss: 0.00001995
Iteration 39/1000 | Loss: 0.00001994
Iteration 40/1000 | Loss: 0.00001994
Iteration 41/1000 | Loss: 0.00001994
Iteration 42/1000 | Loss: 0.00001994
Iteration 43/1000 | Loss: 0.00001994
Iteration 44/1000 | Loss: 0.00001994
Iteration 45/1000 | Loss: 0.00001993
Iteration 46/1000 | Loss: 0.00001993
Iteration 47/1000 | Loss: 0.00001993
Iteration 48/1000 | Loss: 0.00001993
Iteration 49/1000 | Loss: 0.00001993
Iteration 50/1000 | Loss: 0.00001993
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001993
Iteration 53/1000 | Loss: 0.00001993
Iteration 54/1000 | Loss: 0.00001993
Iteration 55/1000 | Loss: 0.00001993
Iteration 56/1000 | Loss: 0.00001993
Iteration 57/1000 | Loss: 0.00001993
Iteration 58/1000 | Loss: 0.00001992
Iteration 59/1000 | Loss: 0.00001992
Iteration 60/1000 | Loss: 0.00001992
Iteration 61/1000 | Loss: 0.00001992
Iteration 62/1000 | Loss: 0.00001992
Iteration 63/1000 | Loss: 0.00001992
Iteration 64/1000 | Loss: 0.00001992
Iteration 65/1000 | Loss: 0.00001992
Iteration 66/1000 | Loss: 0.00001992
Iteration 67/1000 | Loss: 0.00001992
Iteration 68/1000 | Loss: 0.00001992
Iteration 69/1000 | Loss: 0.00001992
Iteration 70/1000 | Loss: 0.00001992
Iteration 71/1000 | Loss: 0.00001992
Iteration 72/1000 | Loss: 0.00001992
Iteration 73/1000 | Loss: 0.00001992
Iteration 74/1000 | Loss: 0.00001991
Iteration 75/1000 | Loss: 0.00001991
Iteration 76/1000 | Loss: 0.00001991
Iteration 77/1000 | Loss: 0.00001991
Iteration 78/1000 | Loss: 0.00001991
Iteration 79/1000 | Loss: 0.00001991
Iteration 80/1000 | Loss: 0.00001991
Iteration 81/1000 | Loss: 0.00001991
Iteration 82/1000 | Loss: 0.00001991
Iteration 83/1000 | Loss: 0.00001991
Iteration 84/1000 | Loss: 0.00001991
Iteration 85/1000 | Loss: 0.00001991
Iteration 86/1000 | Loss: 0.00001991
Iteration 87/1000 | Loss: 0.00001991
Iteration 88/1000 | Loss: 0.00001991
Iteration 89/1000 | Loss: 0.00001991
Iteration 90/1000 | Loss: 0.00001991
Iteration 91/1000 | Loss: 0.00001991
Iteration 92/1000 | Loss: 0.00001991
Iteration 93/1000 | Loss: 0.00001991
Iteration 94/1000 | Loss: 0.00001991
Iteration 95/1000 | Loss: 0.00001991
Iteration 96/1000 | Loss: 0.00001991
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 96. Stopping optimization.
Last 5 losses: [1.990742384805344e-05, 1.990742384805344e-05, 1.990742384805344e-05, 1.990742384805344e-05, 1.990742384805344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.990742384805344e-05

Optimization complete. Final v2v error: 3.784341335296631 mm

Highest mean error: 4.540197372436523 mm for frame 85

Lowest mean error: 3.4935262203216553 mm for frame 0

Saving results

Total time: 30.373181343078613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444946
Iteration 2/25 | Loss: 0.00112625
Iteration 3/25 | Loss: 0.00104858
Iteration 4/25 | Loss: 0.00103456
Iteration 5/25 | Loss: 0.00103052
Iteration 6/25 | Loss: 0.00102853
Iteration 7/25 | Loss: 0.00102828
Iteration 8/25 | Loss: 0.00102828
Iteration 9/25 | Loss: 0.00102828
Iteration 10/25 | Loss: 0.00102828
Iteration 11/25 | Loss: 0.00102828
Iteration 12/25 | Loss: 0.00102828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001028281170874834, 0.001028281170874834, 0.001028281170874834, 0.001028281170874834, 0.001028281170874834]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001028281170874834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.07736921
Iteration 2/25 | Loss: 0.00055647
Iteration 3/25 | Loss: 0.00055647
Iteration 4/25 | Loss: 0.00055647
Iteration 5/25 | Loss: 0.00055646
Iteration 6/25 | Loss: 0.00055646
Iteration 7/25 | Loss: 0.00055646
Iteration 8/25 | Loss: 0.00055646
Iteration 9/25 | Loss: 0.00055646
Iteration 10/25 | Loss: 0.00055646
Iteration 11/25 | Loss: 0.00055646
Iteration 12/25 | Loss: 0.00055646
Iteration 13/25 | Loss: 0.00055646
Iteration 14/25 | Loss: 0.00055646
Iteration 15/25 | Loss: 0.00055646
Iteration 16/25 | Loss: 0.00055646
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005564632592722774, 0.0005564632592722774, 0.0005564632592722774, 0.0005564632592722774, 0.0005564632592722774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005564632592722774

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055646
Iteration 2/1000 | Loss: 0.00005401
Iteration 3/1000 | Loss: 0.00002986
Iteration 4/1000 | Loss: 0.00002437
Iteration 5/1000 | Loss: 0.00002241
Iteration 6/1000 | Loss: 0.00002137
Iteration 7/1000 | Loss: 0.00002093
Iteration 8/1000 | Loss: 0.00002080
Iteration 9/1000 | Loss: 0.00002076
Iteration 10/1000 | Loss: 0.00002076
Iteration 11/1000 | Loss: 0.00002056
Iteration 12/1000 | Loss: 0.00002055
Iteration 13/1000 | Loss: 0.00002043
Iteration 14/1000 | Loss: 0.00002037
Iteration 15/1000 | Loss: 0.00002037
Iteration 16/1000 | Loss: 0.00002037
Iteration 17/1000 | Loss: 0.00002037
Iteration 18/1000 | Loss: 0.00002037
Iteration 19/1000 | Loss: 0.00002037
Iteration 20/1000 | Loss: 0.00002037
Iteration 21/1000 | Loss: 0.00002037
Iteration 22/1000 | Loss: 0.00002036
Iteration 23/1000 | Loss: 0.00002036
Iteration 24/1000 | Loss: 0.00002036
Iteration 25/1000 | Loss: 0.00002036
Iteration 26/1000 | Loss: 0.00002035
Iteration 27/1000 | Loss: 0.00002034
Iteration 28/1000 | Loss: 0.00002033
Iteration 29/1000 | Loss: 0.00002033
Iteration 30/1000 | Loss: 0.00002032
Iteration 31/1000 | Loss: 0.00002032
Iteration 32/1000 | Loss: 0.00002032
Iteration 33/1000 | Loss: 0.00002032
Iteration 34/1000 | Loss: 0.00002032
Iteration 35/1000 | Loss: 0.00002032
Iteration 36/1000 | Loss: 0.00002031
Iteration 37/1000 | Loss: 0.00002031
Iteration 38/1000 | Loss: 0.00002031
Iteration 39/1000 | Loss: 0.00002030
Iteration 40/1000 | Loss: 0.00002030
Iteration 41/1000 | Loss: 0.00002030
Iteration 42/1000 | Loss: 0.00002030
Iteration 43/1000 | Loss: 0.00002030
Iteration 44/1000 | Loss: 0.00002030
Iteration 45/1000 | Loss: 0.00002030
Iteration 46/1000 | Loss: 0.00002029
Iteration 47/1000 | Loss: 0.00002029
Iteration 48/1000 | Loss: 0.00002029
Iteration 49/1000 | Loss: 0.00002029
Iteration 50/1000 | Loss: 0.00002028
Iteration 51/1000 | Loss: 0.00002028
Iteration 52/1000 | Loss: 0.00002028
Iteration 53/1000 | Loss: 0.00002027
Iteration 54/1000 | Loss: 0.00002027
Iteration 55/1000 | Loss: 0.00002027
Iteration 56/1000 | Loss: 0.00002027
Iteration 57/1000 | Loss: 0.00002027
Iteration 58/1000 | Loss: 0.00002027
Iteration 59/1000 | Loss: 0.00002027
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002027
Iteration 62/1000 | Loss: 0.00002027
Iteration 63/1000 | Loss: 0.00002027
Iteration 64/1000 | Loss: 0.00002027
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00002027
Iteration 67/1000 | Loss: 0.00002027
Iteration 68/1000 | Loss: 0.00002027
Iteration 69/1000 | Loss: 0.00002027
Iteration 70/1000 | Loss: 0.00002027
Iteration 71/1000 | Loss: 0.00002026
Iteration 72/1000 | Loss: 0.00002026
Iteration 73/1000 | Loss: 0.00002026
Iteration 74/1000 | Loss: 0.00002026
Iteration 75/1000 | Loss: 0.00002026
Iteration 76/1000 | Loss: 0.00002026
Iteration 77/1000 | Loss: 0.00002026
Iteration 78/1000 | Loss: 0.00002026
Iteration 79/1000 | Loss: 0.00002026
Iteration 80/1000 | Loss: 0.00002026
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002026
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002026
Iteration 87/1000 | Loss: 0.00002026
Iteration 88/1000 | Loss: 0.00002026
Iteration 89/1000 | Loss: 0.00002026
Iteration 90/1000 | Loss: 0.00002026
Iteration 91/1000 | Loss: 0.00002025
Iteration 92/1000 | Loss: 0.00002025
Iteration 93/1000 | Loss: 0.00002025
Iteration 94/1000 | Loss: 0.00002025
Iteration 95/1000 | Loss: 0.00002025
Iteration 96/1000 | Loss: 0.00002025
Iteration 97/1000 | Loss: 0.00002025
Iteration 98/1000 | Loss: 0.00002025
Iteration 99/1000 | Loss: 0.00002025
Iteration 100/1000 | Loss: 0.00002025
Iteration 101/1000 | Loss: 0.00002025
Iteration 102/1000 | Loss: 0.00002025
Iteration 103/1000 | Loss: 0.00002025
Iteration 104/1000 | Loss: 0.00002025
Iteration 105/1000 | Loss: 0.00002025
Iteration 106/1000 | Loss: 0.00002025
Iteration 107/1000 | Loss: 0.00002025
Iteration 108/1000 | Loss: 0.00002025
Iteration 109/1000 | Loss: 0.00002025
Iteration 110/1000 | Loss: 0.00002025
Iteration 111/1000 | Loss: 0.00002025
Iteration 112/1000 | Loss: 0.00002025
Iteration 113/1000 | Loss: 0.00002025
Iteration 114/1000 | Loss: 0.00002025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [2.0245875930413604e-05, 2.0245875930413604e-05, 2.0245875930413604e-05, 2.0245875930413604e-05, 2.0245875930413604e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0245875930413604e-05

Optimization complete. Final v2v error: 3.849536895751953 mm

Highest mean error: 4.009586334228516 mm for frame 15

Lowest mean error: 3.5420658588409424 mm for frame 120

Saving results

Total time: 30.649364948272705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00645327
Iteration 2/25 | Loss: 0.00140866
Iteration 3/25 | Loss: 0.00114821
Iteration 4/25 | Loss: 0.00108208
Iteration 5/25 | Loss: 0.00106861
Iteration 6/25 | Loss: 0.00107034
Iteration 7/25 | Loss: 0.00106659
Iteration 8/25 | Loss: 0.00106188
Iteration 9/25 | Loss: 0.00105930
Iteration 10/25 | Loss: 0.00105828
Iteration 11/25 | Loss: 0.00105811
Iteration 12/25 | Loss: 0.00105757
Iteration 13/25 | Loss: 0.00105733
Iteration 14/25 | Loss: 0.00105729
Iteration 15/25 | Loss: 0.00105729
Iteration 16/25 | Loss: 0.00105729
Iteration 17/25 | Loss: 0.00105729
Iteration 18/25 | Loss: 0.00105728
Iteration 19/25 | Loss: 0.00105728
Iteration 20/25 | Loss: 0.00105728
Iteration 21/25 | Loss: 0.00105728
Iteration 22/25 | Loss: 0.00105728
Iteration 23/25 | Loss: 0.00105728
Iteration 24/25 | Loss: 0.00105728
Iteration 25/25 | Loss: 0.00105728

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.69243145
Iteration 2/25 | Loss: 0.00062131
Iteration 3/25 | Loss: 0.00062131
Iteration 4/25 | Loss: 0.00062131
Iteration 5/25 | Loss: 0.00062131
Iteration 6/25 | Loss: 0.00062131
Iteration 7/25 | Loss: 0.00062131
Iteration 8/25 | Loss: 0.00062131
Iteration 9/25 | Loss: 0.00062131
Iteration 10/25 | Loss: 0.00062131
Iteration 11/25 | Loss: 0.00062131
Iteration 12/25 | Loss: 0.00062131
Iteration 13/25 | Loss: 0.00062131
Iteration 14/25 | Loss: 0.00062131
Iteration 15/25 | Loss: 0.00062131
Iteration 16/25 | Loss: 0.00062131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0006213110173121095, 0.0006213110173121095, 0.0006213110173121095, 0.0006213110173121095, 0.0006213110173121095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006213110173121095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062131
Iteration 2/1000 | Loss: 0.00006218
Iteration 3/1000 | Loss: 0.00004327
Iteration 4/1000 | Loss: 0.00003560
Iteration 5/1000 | Loss: 0.00003155
Iteration 6/1000 | Loss: 0.00002940
Iteration 7/1000 | Loss: 0.00002880
Iteration 8/1000 | Loss: 0.00002825
Iteration 9/1000 | Loss: 0.00002788
Iteration 10/1000 | Loss: 0.00002764
Iteration 11/1000 | Loss: 0.00002745
Iteration 12/1000 | Loss: 0.00002735
Iteration 13/1000 | Loss: 0.00002733
Iteration 14/1000 | Loss: 0.00002733
Iteration 15/1000 | Loss: 0.00002732
Iteration 16/1000 | Loss: 0.00002732
Iteration 17/1000 | Loss: 0.00002731
Iteration 18/1000 | Loss: 0.00002731
Iteration 19/1000 | Loss: 0.00002730
Iteration 20/1000 | Loss: 0.00002730
Iteration 21/1000 | Loss: 0.00002730
Iteration 22/1000 | Loss: 0.00002729
Iteration 23/1000 | Loss: 0.00002729
Iteration 24/1000 | Loss: 0.00002728
Iteration 25/1000 | Loss: 0.00002728
Iteration 26/1000 | Loss: 0.00002728
Iteration 27/1000 | Loss: 0.00002728
Iteration 28/1000 | Loss: 0.00002727
Iteration 29/1000 | Loss: 0.00002727
Iteration 30/1000 | Loss: 0.00002727
Iteration 31/1000 | Loss: 0.00002727
Iteration 32/1000 | Loss: 0.00002726
Iteration 33/1000 | Loss: 0.00002726
Iteration 34/1000 | Loss: 0.00002726
Iteration 35/1000 | Loss: 0.00002725
Iteration 36/1000 | Loss: 0.00002725
Iteration 37/1000 | Loss: 0.00002725
Iteration 38/1000 | Loss: 0.00002724
Iteration 39/1000 | Loss: 0.00002724
Iteration 40/1000 | Loss: 0.00002724
Iteration 41/1000 | Loss: 0.00002724
Iteration 42/1000 | Loss: 0.00002723
Iteration 43/1000 | Loss: 0.00002723
Iteration 44/1000 | Loss: 0.00002723
Iteration 45/1000 | Loss: 0.00002723
Iteration 46/1000 | Loss: 0.00002722
Iteration 47/1000 | Loss: 0.00002722
Iteration 48/1000 | Loss: 0.00002722
Iteration 49/1000 | Loss: 0.00002722
Iteration 50/1000 | Loss: 0.00002721
Iteration 51/1000 | Loss: 0.00002721
Iteration 52/1000 | Loss: 0.00002721
Iteration 53/1000 | Loss: 0.00002721
Iteration 54/1000 | Loss: 0.00002721
Iteration 55/1000 | Loss: 0.00002720
Iteration 56/1000 | Loss: 0.00002720
Iteration 57/1000 | Loss: 0.00002720
Iteration 58/1000 | Loss: 0.00002720
Iteration 59/1000 | Loss: 0.00002720
Iteration 60/1000 | Loss: 0.00002720
Iteration 61/1000 | Loss: 0.00002720
Iteration 62/1000 | Loss: 0.00002720
Iteration 63/1000 | Loss: 0.00002720
Iteration 64/1000 | Loss: 0.00002719
Iteration 65/1000 | Loss: 0.00002719
Iteration 66/1000 | Loss: 0.00002719
Iteration 67/1000 | Loss: 0.00002719
Iteration 68/1000 | Loss: 0.00002719
Iteration 69/1000 | Loss: 0.00002719
Iteration 70/1000 | Loss: 0.00002719
Iteration 71/1000 | Loss: 0.00002718
Iteration 72/1000 | Loss: 0.00002718
Iteration 73/1000 | Loss: 0.00002718
Iteration 74/1000 | Loss: 0.00002718
Iteration 75/1000 | Loss: 0.00002718
Iteration 76/1000 | Loss: 0.00002718
Iteration 77/1000 | Loss: 0.00002718
Iteration 78/1000 | Loss: 0.00002717
Iteration 79/1000 | Loss: 0.00002717
Iteration 80/1000 | Loss: 0.00002717
Iteration 81/1000 | Loss: 0.00002717
Iteration 82/1000 | Loss: 0.00002717
Iteration 83/1000 | Loss: 0.00002716
Iteration 84/1000 | Loss: 0.00002716
Iteration 85/1000 | Loss: 0.00002716
Iteration 86/1000 | Loss: 0.00002716
Iteration 87/1000 | Loss: 0.00002716
Iteration 88/1000 | Loss: 0.00002716
Iteration 89/1000 | Loss: 0.00002716
Iteration 90/1000 | Loss: 0.00002716
Iteration 91/1000 | Loss: 0.00002716
Iteration 92/1000 | Loss: 0.00002715
Iteration 93/1000 | Loss: 0.00002715
Iteration 94/1000 | Loss: 0.00002715
Iteration 95/1000 | Loss: 0.00002715
Iteration 96/1000 | Loss: 0.00002715
Iteration 97/1000 | Loss: 0.00002715
Iteration 98/1000 | Loss: 0.00002715
Iteration 99/1000 | Loss: 0.00002715
Iteration 100/1000 | Loss: 0.00002715
Iteration 101/1000 | Loss: 0.00002715
Iteration 102/1000 | Loss: 0.00002714
Iteration 103/1000 | Loss: 0.00002714
Iteration 104/1000 | Loss: 0.00002714
Iteration 105/1000 | Loss: 0.00002714
Iteration 106/1000 | Loss: 0.00002714
Iteration 107/1000 | Loss: 0.00002714
Iteration 108/1000 | Loss: 0.00002714
Iteration 109/1000 | Loss: 0.00002714
Iteration 110/1000 | Loss: 0.00002714
Iteration 111/1000 | Loss: 0.00002714
Iteration 112/1000 | Loss: 0.00002713
Iteration 113/1000 | Loss: 0.00002713
Iteration 114/1000 | Loss: 0.00002713
Iteration 115/1000 | Loss: 0.00002713
Iteration 116/1000 | Loss: 0.00002713
Iteration 117/1000 | Loss: 0.00002713
Iteration 118/1000 | Loss: 0.00002713
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.7133903131471016e-05, 2.7133903131471016e-05, 2.7133903131471016e-05, 2.7133903131471016e-05, 2.7133903131471016e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7133903131471016e-05

Optimization complete. Final v2v error: 4.501021862030029 mm

Highest mean error: 4.960671901702881 mm for frame 56

Lowest mean error: 4.0377397537231445 mm for frame 114

Saving results

Total time: 52.037060499191284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856926
Iteration 2/25 | Loss: 0.00135134
Iteration 3/25 | Loss: 0.00108585
Iteration 4/25 | Loss: 0.00105142
Iteration 5/25 | Loss: 0.00104554
Iteration 6/25 | Loss: 0.00104421
Iteration 7/25 | Loss: 0.00104421
Iteration 8/25 | Loss: 0.00104421
Iteration 9/25 | Loss: 0.00104421
Iteration 10/25 | Loss: 0.00104421
Iteration 11/25 | Loss: 0.00104421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010442135389894247, 0.0010442135389894247, 0.0010442135389894247, 0.0010442135389894247, 0.0010442135389894247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010442135389894247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41909575
Iteration 2/25 | Loss: 0.00049441
Iteration 3/25 | Loss: 0.00049441
Iteration 4/25 | Loss: 0.00049441
Iteration 5/25 | Loss: 0.00049441
Iteration 6/25 | Loss: 0.00049440
Iteration 7/25 | Loss: 0.00049440
Iteration 8/25 | Loss: 0.00049440
Iteration 9/25 | Loss: 0.00049440
Iteration 10/25 | Loss: 0.00049440
Iteration 11/25 | Loss: 0.00049440
Iteration 12/25 | Loss: 0.00049440
Iteration 13/25 | Loss: 0.00049440
Iteration 14/25 | Loss: 0.00049440
Iteration 15/25 | Loss: 0.00049440
Iteration 16/25 | Loss: 0.00049440
Iteration 17/25 | Loss: 0.00049440
Iteration 18/25 | Loss: 0.00049440
Iteration 19/25 | Loss: 0.00049440
Iteration 20/25 | Loss: 0.00049440
Iteration 21/25 | Loss: 0.00049440
Iteration 22/25 | Loss: 0.00049440
Iteration 23/25 | Loss: 0.00049440
Iteration 24/25 | Loss: 0.00049440
Iteration 25/25 | Loss: 0.00049440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0004944041138514876, 0.0004944041138514876, 0.0004944041138514876, 0.0004944041138514876, 0.0004944041138514876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004944041138514876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049440
Iteration 2/1000 | Loss: 0.00006049
Iteration 3/1000 | Loss: 0.00003808
Iteration 4/1000 | Loss: 0.00003047
Iteration 5/1000 | Loss: 0.00002739
Iteration 6/1000 | Loss: 0.00002579
Iteration 7/1000 | Loss: 0.00002509
Iteration 8/1000 | Loss: 0.00002472
Iteration 9/1000 | Loss: 0.00002445
Iteration 10/1000 | Loss: 0.00002421
Iteration 11/1000 | Loss: 0.00002416
Iteration 12/1000 | Loss: 0.00002416
Iteration 13/1000 | Loss: 0.00002413
Iteration 14/1000 | Loss: 0.00002412
Iteration 15/1000 | Loss: 0.00002412
Iteration 16/1000 | Loss: 0.00002411
Iteration 17/1000 | Loss: 0.00002410
Iteration 18/1000 | Loss: 0.00002410
Iteration 19/1000 | Loss: 0.00002410
Iteration 20/1000 | Loss: 0.00002410
Iteration 21/1000 | Loss: 0.00002408
Iteration 22/1000 | Loss: 0.00002408
Iteration 23/1000 | Loss: 0.00002408
Iteration 24/1000 | Loss: 0.00002408
Iteration 25/1000 | Loss: 0.00002407
Iteration 26/1000 | Loss: 0.00002407
Iteration 27/1000 | Loss: 0.00002406
Iteration 28/1000 | Loss: 0.00002405
Iteration 29/1000 | Loss: 0.00002405
Iteration 30/1000 | Loss: 0.00002405
Iteration 31/1000 | Loss: 0.00002404
Iteration 32/1000 | Loss: 0.00002404
Iteration 33/1000 | Loss: 0.00002403
Iteration 34/1000 | Loss: 0.00002403
Iteration 35/1000 | Loss: 0.00002402
Iteration 36/1000 | Loss: 0.00002402
Iteration 37/1000 | Loss: 0.00002402
Iteration 38/1000 | Loss: 0.00002402
Iteration 39/1000 | Loss: 0.00002402
Iteration 40/1000 | Loss: 0.00002402
Iteration 41/1000 | Loss: 0.00002401
Iteration 42/1000 | Loss: 0.00002401
Iteration 43/1000 | Loss: 0.00002401
Iteration 44/1000 | Loss: 0.00002401
Iteration 45/1000 | Loss: 0.00002401
Iteration 46/1000 | Loss: 0.00002401
Iteration 47/1000 | Loss: 0.00002401
Iteration 48/1000 | Loss: 0.00002400
Iteration 49/1000 | Loss: 0.00002400
Iteration 50/1000 | Loss: 0.00002400
Iteration 51/1000 | Loss: 0.00002400
Iteration 52/1000 | Loss: 0.00002400
Iteration 53/1000 | Loss: 0.00002400
Iteration 54/1000 | Loss: 0.00002400
Iteration 55/1000 | Loss: 0.00002400
Iteration 56/1000 | Loss: 0.00002399
Iteration 57/1000 | Loss: 0.00002399
Iteration 58/1000 | Loss: 0.00002399
Iteration 59/1000 | Loss: 0.00002399
Iteration 60/1000 | Loss: 0.00002399
Iteration 61/1000 | Loss: 0.00002399
Iteration 62/1000 | Loss: 0.00002399
Iteration 63/1000 | Loss: 0.00002399
Iteration 64/1000 | Loss: 0.00002399
Iteration 65/1000 | Loss: 0.00002398
Iteration 66/1000 | Loss: 0.00002398
Iteration 67/1000 | Loss: 0.00002398
Iteration 68/1000 | Loss: 0.00002398
Iteration 69/1000 | Loss: 0.00002398
Iteration 70/1000 | Loss: 0.00002398
Iteration 71/1000 | Loss: 0.00002398
Iteration 72/1000 | Loss: 0.00002397
Iteration 73/1000 | Loss: 0.00002397
Iteration 74/1000 | Loss: 0.00002397
Iteration 75/1000 | Loss: 0.00002397
Iteration 76/1000 | Loss: 0.00002396
Iteration 77/1000 | Loss: 0.00002396
Iteration 78/1000 | Loss: 0.00002395
Iteration 79/1000 | Loss: 0.00002395
Iteration 80/1000 | Loss: 0.00002395
Iteration 81/1000 | Loss: 0.00002395
Iteration 82/1000 | Loss: 0.00002395
Iteration 83/1000 | Loss: 0.00002395
Iteration 84/1000 | Loss: 0.00002394
Iteration 85/1000 | Loss: 0.00002394
Iteration 86/1000 | Loss: 0.00002394
Iteration 87/1000 | Loss: 0.00002394
Iteration 88/1000 | Loss: 0.00002393
Iteration 89/1000 | Loss: 0.00002393
Iteration 90/1000 | Loss: 0.00002393
Iteration 91/1000 | Loss: 0.00002393
Iteration 92/1000 | Loss: 0.00002393
Iteration 93/1000 | Loss: 0.00002393
Iteration 94/1000 | Loss: 0.00002393
Iteration 95/1000 | Loss: 0.00002393
Iteration 96/1000 | Loss: 0.00002392
Iteration 97/1000 | Loss: 0.00002392
Iteration 98/1000 | Loss: 0.00002392
Iteration 99/1000 | Loss: 0.00002392
Iteration 100/1000 | Loss: 0.00002392
Iteration 101/1000 | Loss: 0.00002392
Iteration 102/1000 | Loss: 0.00002392
Iteration 103/1000 | Loss: 0.00002392
Iteration 104/1000 | Loss: 0.00002392
Iteration 105/1000 | Loss: 0.00002392
Iteration 106/1000 | Loss: 0.00002392
Iteration 107/1000 | Loss: 0.00002392
Iteration 108/1000 | Loss: 0.00002392
Iteration 109/1000 | Loss: 0.00002392
Iteration 110/1000 | Loss: 0.00002392
Iteration 111/1000 | Loss: 0.00002392
Iteration 112/1000 | Loss: 0.00002392
Iteration 113/1000 | Loss: 0.00002391
Iteration 114/1000 | Loss: 0.00002391
Iteration 115/1000 | Loss: 0.00002391
Iteration 116/1000 | Loss: 0.00002391
Iteration 117/1000 | Loss: 0.00002391
Iteration 118/1000 | Loss: 0.00002391
Iteration 119/1000 | Loss: 0.00002391
Iteration 120/1000 | Loss: 0.00002391
Iteration 121/1000 | Loss: 0.00002391
Iteration 122/1000 | Loss: 0.00002391
Iteration 123/1000 | Loss: 0.00002391
Iteration 124/1000 | Loss: 0.00002391
Iteration 125/1000 | Loss: 0.00002391
Iteration 126/1000 | Loss: 0.00002391
Iteration 127/1000 | Loss: 0.00002391
Iteration 128/1000 | Loss: 0.00002391
Iteration 129/1000 | Loss: 0.00002391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.390810004726518e-05, 2.390810004726518e-05, 2.390810004726518e-05, 2.390810004726518e-05, 2.390810004726518e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.390810004726518e-05

Optimization complete. Final v2v error: 4.191754341125488 mm

Highest mean error: 4.474855422973633 mm for frame 64

Lowest mean error: 3.5355138778686523 mm for frame 1

Saving results

Total time: 36.56643843650818
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_36_nl_2015/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_36_nl_2015/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854998
Iteration 2/25 | Loss: 0.00131442
Iteration 3/25 | Loss: 0.00114657
Iteration 4/25 | Loss: 0.00111471
Iteration 5/25 | Loss: 0.00110934
Iteration 6/25 | Loss: 0.00110801
Iteration 7/25 | Loss: 0.00110801
Iteration 8/25 | Loss: 0.00110801
Iteration 9/25 | Loss: 0.00110801
Iteration 10/25 | Loss: 0.00110801
Iteration 11/25 | Loss: 0.00110801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011080073891207576, 0.0011080073891207576, 0.0011080073891207576, 0.0011080073891207576, 0.0011080073891207576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011080073891207576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41565537
Iteration 2/25 | Loss: 0.00055977
Iteration 3/25 | Loss: 0.00055974
Iteration 4/25 | Loss: 0.00055974
Iteration 5/25 | Loss: 0.00055974
Iteration 6/25 | Loss: 0.00055974
Iteration 7/25 | Loss: 0.00055974
Iteration 8/25 | Loss: 0.00055974
Iteration 9/25 | Loss: 0.00055974
Iteration 10/25 | Loss: 0.00055974
Iteration 11/25 | Loss: 0.00055974
Iteration 12/25 | Loss: 0.00055974
Iteration 13/25 | Loss: 0.00055974
Iteration 14/25 | Loss: 0.00055974
Iteration 15/25 | Loss: 0.00055974
Iteration 16/25 | Loss: 0.00055974
Iteration 17/25 | Loss: 0.00055974
Iteration 18/25 | Loss: 0.00055974
Iteration 19/25 | Loss: 0.00055974
Iteration 20/25 | Loss: 0.00055974
Iteration 21/25 | Loss: 0.00055974
Iteration 22/25 | Loss: 0.00055974
Iteration 23/25 | Loss: 0.00055974
Iteration 24/25 | Loss: 0.00055974
Iteration 25/25 | Loss: 0.00055974
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.000559738022275269, 0.000559738022275269, 0.000559738022275269, 0.000559738022275269, 0.000559738022275269]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000559738022275269

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00055974
Iteration 2/1000 | Loss: 0.00006398
Iteration 3/1000 | Loss: 0.00004229
Iteration 4/1000 | Loss: 0.00003524
Iteration 5/1000 | Loss: 0.00003147
Iteration 6/1000 | Loss: 0.00003017
Iteration 7/1000 | Loss: 0.00002940
Iteration 8/1000 | Loss: 0.00002899
Iteration 9/1000 | Loss: 0.00002866
Iteration 10/1000 | Loss: 0.00002839
Iteration 11/1000 | Loss: 0.00002826
Iteration 12/1000 | Loss: 0.00002820
Iteration 13/1000 | Loss: 0.00002819
Iteration 14/1000 | Loss: 0.00002818
Iteration 15/1000 | Loss: 0.00002818
Iteration 16/1000 | Loss: 0.00002817
Iteration 17/1000 | Loss: 0.00002816
Iteration 18/1000 | Loss: 0.00002815
Iteration 19/1000 | Loss: 0.00002815
Iteration 20/1000 | Loss: 0.00002815
Iteration 21/1000 | Loss: 0.00002815
Iteration 22/1000 | Loss: 0.00002815
Iteration 23/1000 | Loss: 0.00002815
Iteration 24/1000 | Loss: 0.00002814
Iteration 25/1000 | Loss: 0.00002814
Iteration 26/1000 | Loss: 0.00002814
Iteration 27/1000 | Loss: 0.00002814
Iteration 28/1000 | Loss: 0.00002814
Iteration 29/1000 | Loss: 0.00002814
Iteration 30/1000 | Loss: 0.00002814
Iteration 31/1000 | Loss: 0.00002813
Iteration 32/1000 | Loss: 0.00002813
Iteration 33/1000 | Loss: 0.00002813
Iteration 34/1000 | Loss: 0.00002813
Iteration 35/1000 | Loss: 0.00002813
Iteration 36/1000 | Loss: 0.00002813
Iteration 37/1000 | Loss: 0.00002813
Iteration 38/1000 | Loss: 0.00002813
Iteration 39/1000 | Loss: 0.00002813
Iteration 40/1000 | Loss: 0.00002812
Iteration 41/1000 | Loss: 0.00002812
Iteration 42/1000 | Loss: 0.00002812
Iteration 43/1000 | Loss: 0.00002810
Iteration 44/1000 | Loss: 0.00002810
Iteration 45/1000 | Loss: 0.00002810
Iteration 46/1000 | Loss: 0.00002810
Iteration 47/1000 | Loss: 0.00002810
Iteration 48/1000 | Loss: 0.00002809
Iteration 49/1000 | Loss: 0.00002809
Iteration 50/1000 | Loss: 0.00002809
Iteration 51/1000 | Loss: 0.00002809
Iteration 52/1000 | Loss: 0.00002809
Iteration 53/1000 | Loss: 0.00002809
Iteration 54/1000 | Loss: 0.00002809
Iteration 55/1000 | Loss: 0.00002809
Iteration 56/1000 | Loss: 0.00002809
Iteration 57/1000 | Loss: 0.00002809
Iteration 58/1000 | Loss: 0.00002808
Iteration 59/1000 | Loss: 0.00002808
Iteration 60/1000 | Loss: 0.00002807
Iteration 61/1000 | Loss: 0.00002807
Iteration 62/1000 | Loss: 0.00002807
Iteration 63/1000 | Loss: 0.00002807
Iteration 64/1000 | Loss: 0.00002807
Iteration 65/1000 | Loss: 0.00002807
Iteration 66/1000 | Loss: 0.00002807
Iteration 67/1000 | Loss: 0.00002807
Iteration 68/1000 | Loss: 0.00002807
Iteration 69/1000 | Loss: 0.00002807
Iteration 70/1000 | Loss: 0.00002807
Iteration 71/1000 | Loss: 0.00002807
Iteration 72/1000 | Loss: 0.00002807
Iteration 73/1000 | Loss: 0.00002807
Iteration 74/1000 | Loss: 0.00002806
Iteration 75/1000 | Loss: 0.00002806
Iteration 76/1000 | Loss: 0.00002806
Iteration 77/1000 | Loss: 0.00002806
Iteration 78/1000 | Loss: 0.00002806
Iteration 79/1000 | Loss: 0.00002806
Iteration 80/1000 | Loss: 0.00002806
Iteration 81/1000 | Loss: 0.00002806
Iteration 82/1000 | Loss: 0.00002806
Iteration 83/1000 | Loss: 0.00002806
Iteration 84/1000 | Loss: 0.00002806
Iteration 85/1000 | Loss: 0.00002806
Iteration 86/1000 | Loss: 0.00002806
Iteration 87/1000 | Loss: 0.00002806
Iteration 88/1000 | Loss: 0.00002806
Iteration 89/1000 | Loss: 0.00002806
Iteration 90/1000 | Loss: 0.00002806
Iteration 91/1000 | Loss: 0.00002806
Iteration 92/1000 | Loss: 0.00002806
Iteration 93/1000 | Loss: 0.00002806
Iteration 94/1000 | Loss: 0.00002806
Iteration 95/1000 | Loss: 0.00002806
Iteration 96/1000 | Loss: 0.00002806
Iteration 97/1000 | Loss: 0.00002806
Iteration 98/1000 | Loss: 0.00002806
Iteration 99/1000 | Loss: 0.00002806
Iteration 100/1000 | Loss: 0.00002806
Iteration 101/1000 | Loss: 0.00002806
Iteration 102/1000 | Loss: 0.00002806
Iteration 103/1000 | Loss: 0.00002806
Iteration 104/1000 | Loss: 0.00002806
Iteration 105/1000 | Loss: 0.00002806
Iteration 106/1000 | Loss: 0.00002806
Iteration 107/1000 | Loss: 0.00002806
Iteration 108/1000 | Loss: 0.00002806
Iteration 109/1000 | Loss: 0.00002806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [2.8055275834049098e-05, 2.8055275834049098e-05, 2.8055275834049098e-05, 2.8055275834049098e-05, 2.8055275834049098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8055275834049098e-05

Optimization complete. Final v2v error: 4.568653583526611 mm

Highest mean error: 4.832746505737305 mm for frame 156

Lowest mean error: 4.234751224517822 mm for frame 0

Saving results

Total time: 33.6977219581604
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01130776
Iteration 2/25 | Loss: 0.00280232
Iteration 3/25 | Loss: 0.00215363
Iteration 4/25 | Loss: 0.00225392
Iteration 5/25 | Loss: 0.00218411
Iteration 6/25 | Loss: 0.00193894
Iteration 7/25 | Loss: 0.00189482
Iteration 8/25 | Loss: 0.00184346
Iteration 9/25 | Loss: 0.00181486
Iteration 10/25 | Loss: 0.00181228
Iteration 11/25 | Loss: 0.00180621
Iteration 12/25 | Loss: 0.00180829
Iteration 13/25 | Loss: 0.00179954
Iteration 14/25 | Loss: 0.00180079
Iteration 15/25 | Loss: 0.00176322
Iteration 16/25 | Loss: 0.00176448
Iteration 17/25 | Loss: 0.00176363
Iteration 18/25 | Loss: 0.00176641
Iteration 19/25 | Loss: 0.00175660
Iteration 20/25 | Loss: 0.00175576
Iteration 21/25 | Loss: 0.00175897
Iteration 22/25 | Loss: 0.00175816
Iteration 23/25 | Loss: 0.00175606
Iteration 24/25 | Loss: 0.00175580
Iteration 25/25 | Loss: 0.00175596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55927730
Iteration 2/25 | Loss: 0.00385745
Iteration 3/25 | Loss: 0.00385744
Iteration 4/25 | Loss: 0.00385744
Iteration 5/25 | Loss: 0.00385744
Iteration 6/25 | Loss: 0.00385744
Iteration 7/25 | Loss: 0.00385744
Iteration 8/25 | Loss: 0.00385744
Iteration 9/25 | Loss: 0.00385744
Iteration 10/25 | Loss: 0.00385744
Iteration 11/25 | Loss: 0.00385744
Iteration 12/25 | Loss: 0.00385744
Iteration 13/25 | Loss: 0.00385744
Iteration 14/25 | Loss: 0.00385744
Iteration 15/25 | Loss: 0.00385744
Iteration 16/25 | Loss: 0.00385744
Iteration 17/25 | Loss: 0.00385744
Iteration 18/25 | Loss: 0.00385744
Iteration 19/25 | Loss: 0.00385744
Iteration 20/25 | Loss: 0.00385744
Iteration 21/25 | Loss: 0.00385744
Iteration 22/25 | Loss: 0.00385744
Iteration 23/25 | Loss: 0.00385744
Iteration 24/25 | Loss: 0.00385744
Iteration 25/25 | Loss: 0.00385744

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00385744
Iteration 2/1000 | Loss: 0.00025119
Iteration 3/1000 | Loss: 0.00017838
Iteration 4/1000 | Loss: 0.00061547
Iteration 5/1000 | Loss: 0.00025975
Iteration 6/1000 | Loss: 0.00046456
Iteration 7/1000 | Loss: 0.00011886
Iteration 8/1000 | Loss: 0.00149217
Iteration 9/1000 | Loss: 0.00205819
Iteration 10/1000 | Loss: 0.00019193
Iteration 11/1000 | Loss: 0.00012929
Iteration 12/1000 | Loss: 0.00041984
Iteration 13/1000 | Loss: 0.00011644
Iteration 14/1000 | Loss: 0.00175576
Iteration 15/1000 | Loss: 0.00014685
Iteration 16/1000 | Loss: 0.00246052
Iteration 17/1000 | Loss: 0.00186642
Iteration 18/1000 | Loss: 0.00017145
Iteration 19/1000 | Loss: 0.00011459
Iteration 20/1000 | Loss: 0.00125191
Iteration 21/1000 | Loss: 0.00057917
Iteration 22/1000 | Loss: 0.00008785
Iteration 23/1000 | Loss: 0.00008267
Iteration 24/1000 | Loss: 0.00073673
Iteration 25/1000 | Loss: 0.00084044
Iteration 26/1000 | Loss: 0.00034879
Iteration 27/1000 | Loss: 0.00009767
Iteration 28/1000 | Loss: 0.00007533
Iteration 29/1000 | Loss: 0.00007988
Iteration 30/1000 | Loss: 0.00091662
Iteration 31/1000 | Loss: 0.00067705
Iteration 32/1000 | Loss: 0.00198291
Iteration 33/1000 | Loss: 0.00055426
Iteration 34/1000 | Loss: 0.00009730
Iteration 35/1000 | Loss: 0.00042953
Iteration 36/1000 | Loss: 0.00099943
Iteration 37/1000 | Loss: 0.00116171
Iteration 38/1000 | Loss: 0.00012504
Iteration 39/1000 | Loss: 0.00090797
Iteration 40/1000 | Loss: 0.00008998
Iteration 41/1000 | Loss: 0.00067887
Iteration 42/1000 | Loss: 0.00028947
Iteration 43/1000 | Loss: 0.00073166
Iteration 44/1000 | Loss: 0.00060360
Iteration 45/1000 | Loss: 0.00048083
Iteration 46/1000 | Loss: 0.00006939
Iteration 47/1000 | Loss: 0.00055964
Iteration 48/1000 | Loss: 0.00033280
Iteration 49/1000 | Loss: 0.00020895
Iteration 50/1000 | Loss: 0.00065179
Iteration 51/1000 | Loss: 0.00074881
Iteration 52/1000 | Loss: 0.00025554
Iteration 53/1000 | Loss: 0.00019406
Iteration 54/1000 | Loss: 0.00078003
Iteration 55/1000 | Loss: 0.00049185
Iteration 56/1000 | Loss: 0.00021596
Iteration 57/1000 | Loss: 0.00007517
Iteration 58/1000 | Loss: 0.00006878
Iteration 59/1000 | Loss: 0.00005689
Iteration 60/1000 | Loss: 0.00054881
Iteration 61/1000 | Loss: 0.00008248
Iteration 62/1000 | Loss: 0.00005196
Iteration 63/1000 | Loss: 0.00005356
Iteration 64/1000 | Loss: 0.00004914
Iteration 65/1000 | Loss: 0.00005524
Iteration 66/1000 | Loss: 0.00004670
Iteration 67/1000 | Loss: 0.00005031
Iteration 68/1000 | Loss: 0.00005345
Iteration 69/1000 | Loss: 0.00005445
Iteration 70/1000 | Loss: 0.00005348
Iteration 71/1000 | Loss: 0.00005367
Iteration 72/1000 | Loss: 0.00005249
Iteration 73/1000 | Loss: 0.00005323
Iteration 74/1000 | Loss: 0.00005219
Iteration 75/1000 | Loss: 0.00074956
Iteration 76/1000 | Loss: 0.00007865
Iteration 77/1000 | Loss: 0.00006607
Iteration 78/1000 | Loss: 0.00005677
Iteration 79/1000 | Loss: 0.00006263
Iteration 80/1000 | Loss: 0.00006549
Iteration 81/1000 | Loss: 0.00006141
Iteration 82/1000 | Loss: 0.00064581
Iteration 83/1000 | Loss: 0.00009549
Iteration 84/1000 | Loss: 0.00007207
Iteration 85/1000 | Loss: 0.00007422
Iteration 86/1000 | Loss: 0.00005840
Iteration 87/1000 | Loss: 0.00005794
Iteration 88/1000 | Loss: 0.00006495
Iteration 89/1000 | Loss: 0.00005484
Iteration 90/1000 | Loss: 0.00005137
Iteration 91/1000 | Loss: 0.00005410
Iteration 92/1000 | Loss: 0.00005037
Iteration 93/1000 | Loss: 0.00004485
Iteration 94/1000 | Loss: 0.00005064
Iteration 95/1000 | Loss: 0.00005122
Iteration 96/1000 | Loss: 0.00006341
Iteration 97/1000 | Loss: 0.00005089
Iteration 98/1000 | Loss: 0.00006392
Iteration 99/1000 | Loss: 0.00004951
Iteration 100/1000 | Loss: 0.00005111
Iteration 101/1000 | Loss: 0.00004844
Iteration 102/1000 | Loss: 0.00004788
Iteration 103/1000 | Loss: 0.00004650
Iteration 104/1000 | Loss: 0.00004028
Iteration 105/1000 | Loss: 0.00003798
Iteration 106/1000 | Loss: 0.00003667
Iteration 107/1000 | Loss: 0.00003575
Iteration 108/1000 | Loss: 0.00003503
Iteration 109/1000 | Loss: 0.00003464
Iteration 110/1000 | Loss: 0.00004677
Iteration 111/1000 | Loss: 0.00004098
Iteration 112/1000 | Loss: 0.00004831
Iteration 113/1000 | Loss: 0.00004125
Iteration 114/1000 | Loss: 0.00004938
Iteration 115/1000 | Loss: 0.00038530
Iteration 116/1000 | Loss: 0.00021252
Iteration 117/1000 | Loss: 0.00003504
Iteration 118/1000 | Loss: 0.00003431
Iteration 119/1000 | Loss: 0.00029176
Iteration 120/1000 | Loss: 0.00019225
Iteration 121/1000 | Loss: 0.00026966
Iteration 122/1000 | Loss: 0.00010105
Iteration 123/1000 | Loss: 0.00025109
Iteration 124/1000 | Loss: 0.00006890
Iteration 125/1000 | Loss: 0.00022637
Iteration 126/1000 | Loss: 0.00008490
Iteration 127/1000 | Loss: 0.00068905
Iteration 128/1000 | Loss: 0.00014579
Iteration 129/1000 | Loss: 0.00005077
Iteration 130/1000 | Loss: 0.00003809
Iteration 131/1000 | Loss: 0.00003468
Iteration 132/1000 | Loss: 0.00034275
Iteration 133/1000 | Loss: 0.00018997
Iteration 134/1000 | Loss: 0.00003348
Iteration 135/1000 | Loss: 0.00033989
Iteration 136/1000 | Loss: 0.00035251
Iteration 137/1000 | Loss: 0.00026567
Iteration 138/1000 | Loss: 0.00042460
Iteration 139/1000 | Loss: 0.00024126
Iteration 140/1000 | Loss: 0.00044480
Iteration 141/1000 | Loss: 0.00020513
Iteration 142/1000 | Loss: 0.00023421
Iteration 143/1000 | Loss: 0.00021029
Iteration 144/1000 | Loss: 0.00020707
Iteration 145/1000 | Loss: 0.00019655
Iteration 146/1000 | Loss: 0.00063319
Iteration 147/1000 | Loss: 0.00003995
Iteration 148/1000 | Loss: 0.00003553
Iteration 149/1000 | Loss: 0.00027123
Iteration 150/1000 | Loss: 0.00021622
Iteration 151/1000 | Loss: 0.00026343
Iteration 152/1000 | Loss: 0.00021666
Iteration 153/1000 | Loss: 0.00005202
Iteration 154/1000 | Loss: 0.00020851
Iteration 155/1000 | Loss: 0.00003664
Iteration 156/1000 | Loss: 0.00003377
Iteration 157/1000 | Loss: 0.00007702
Iteration 158/1000 | Loss: 0.00003353
Iteration 159/1000 | Loss: 0.00003141
Iteration 160/1000 | Loss: 0.00003062
Iteration 161/1000 | Loss: 0.00003021
Iteration 162/1000 | Loss: 0.00003009
Iteration 163/1000 | Loss: 0.00002984
Iteration 164/1000 | Loss: 0.00002968
Iteration 165/1000 | Loss: 0.00002964
Iteration 166/1000 | Loss: 0.00002954
Iteration 167/1000 | Loss: 0.00002943
Iteration 168/1000 | Loss: 0.00002937
Iteration 169/1000 | Loss: 0.00002933
Iteration 170/1000 | Loss: 0.00002929
Iteration 171/1000 | Loss: 0.00002928
Iteration 172/1000 | Loss: 0.00002926
Iteration 173/1000 | Loss: 0.00002926
Iteration 174/1000 | Loss: 0.00002926
Iteration 175/1000 | Loss: 0.00002926
Iteration 176/1000 | Loss: 0.00002925
Iteration 177/1000 | Loss: 0.00002925
Iteration 178/1000 | Loss: 0.00002925
Iteration 179/1000 | Loss: 0.00002925
Iteration 180/1000 | Loss: 0.00002925
Iteration 181/1000 | Loss: 0.00002925
Iteration 182/1000 | Loss: 0.00002924
Iteration 183/1000 | Loss: 0.00002924
Iteration 184/1000 | Loss: 0.00002924
Iteration 185/1000 | Loss: 0.00002924
Iteration 186/1000 | Loss: 0.00002924
Iteration 187/1000 | Loss: 0.00002924
Iteration 188/1000 | Loss: 0.00002924
Iteration 189/1000 | Loss: 0.00002924
Iteration 190/1000 | Loss: 0.00002923
Iteration 191/1000 | Loss: 0.00002923
Iteration 192/1000 | Loss: 0.00002923
Iteration 193/1000 | Loss: 0.00002923
Iteration 194/1000 | Loss: 0.00002923
Iteration 195/1000 | Loss: 0.00002923
Iteration 196/1000 | Loss: 0.00002923
Iteration 197/1000 | Loss: 0.00002923
Iteration 198/1000 | Loss: 0.00002923
Iteration 199/1000 | Loss: 0.00002923
Iteration 200/1000 | Loss: 0.00002923
Iteration 201/1000 | Loss: 0.00002923
Iteration 202/1000 | Loss: 0.00002923
Iteration 203/1000 | Loss: 0.00002923
Iteration 204/1000 | Loss: 0.00002923
Iteration 205/1000 | Loss: 0.00002923
Iteration 206/1000 | Loss: 0.00002923
Iteration 207/1000 | Loss: 0.00002923
Iteration 208/1000 | Loss: 0.00002923
Iteration 209/1000 | Loss: 0.00002923
Iteration 210/1000 | Loss: 0.00002923
Iteration 211/1000 | Loss: 0.00002923
Iteration 212/1000 | Loss: 0.00002923
Iteration 213/1000 | Loss: 0.00002923
Iteration 214/1000 | Loss: 0.00002923
Iteration 215/1000 | Loss: 0.00002923
Iteration 216/1000 | Loss: 0.00002923
Iteration 217/1000 | Loss: 0.00002923
Iteration 218/1000 | Loss: 0.00002923
Iteration 219/1000 | Loss: 0.00002923
Iteration 220/1000 | Loss: 0.00002923
Iteration 221/1000 | Loss: 0.00002923
Iteration 222/1000 | Loss: 0.00002923
Iteration 223/1000 | Loss: 0.00002923
Iteration 224/1000 | Loss: 0.00002923
Iteration 225/1000 | Loss: 0.00002923
Iteration 226/1000 | Loss: 0.00002923
Iteration 227/1000 | Loss: 0.00002923
Iteration 228/1000 | Loss: 0.00002923
Iteration 229/1000 | Loss: 0.00002923
Iteration 230/1000 | Loss: 0.00002923
Iteration 231/1000 | Loss: 0.00002923
Iteration 232/1000 | Loss: 0.00002923
Iteration 233/1000 | Loss: 0.00002923
Iteration 234/1000 | Loss: 0.00002923
Iteration 235/1000 | Loss: 0.00002923
Iteration 236/1000 | Loss: 0.00002923
Iteration 237/1000 | Loss: 0.00002923
Iteration 238/1000 | Loss: 0.00002923
Iteration 239/1000 | Loss: 0.00002923
Iteration 240/1000 | Loss: 0.00002923
Iteration 241/1000 | Loss: 0.00002923
Iteration 242/1000 | Loss: 0.00002923
Iteration 243/1000 | Loss: 0.00002923
Iteration 244/1000 | Loss: 0.00002923
Iteration 245/1000 | Loss: 0.00002923
Iteration 246/1000 | Loss: 0.00002923
Iteration 247/1000 | Loss: 0.00002923
Iteration 248/1000 | Loss: 0.00002923
Iteration 249/1000 | Loss: 0.00002923
Iteration 250/1000 | Loss: 0.00002923
Iteration 251/1000 | Loss: 0.00002923
Iteration 252/1000 | Loss: 0.00002923
Iteration 253/1000 | Loss: 0.00002923
Iteration 254/1000 | Loss: 0.00002923
Iteration 255/1000 | Loss: 0.00002923
Iteration 256/1000 | Loss: 0.00002923
Iteration 257/1000 | Loss: 0.00002923
Iteration 258/1000 | Loss: 0.00002923
Iteration 259/1000 | Loss: 0.00002923
Iteration 260/1000 | Loss: 0.00002923
Iteration 261/1000 | Loss: 0.00002923
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [2.9225240723462775e-05, 2.9225240723462775e-05, 2.9225240723462775e-05, 2.9225240723462775e-05, 2.9225240723462775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.9225240723462775e-05

Optimization complete. Final v2v error: 4.7518205642700195 mm

Highest mean error: 11.21249008178711 mm for frame 259

Lowest mean error: 4.0885796546936035 mm for frame 75

Saving results

Total time: 327.8541350364685
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975303
Iteration 2/25 | Loss: 0.00257909
Iteration 3/25 | Loss: 0.00215842
Iteration 4/25 | Loss: 0.00278215
Iteration 5/25 | Loss: 0.00205217
Iteration 6/25 | Loss: 0.00208455
Iteration 7/25 | Loss: 0.00194130
Iteration 8/25 | Loss: 0.00190686
Iteration 9/25 | Loss: 0.00190544
Iteration 10/25 | Loss: 0.00190534
Iteration 11/25 | Loss: 0.00190534
Iteration 12/25 | Loss: 0.00190534
Iteration 13/25 | Loss: 0.00190534
Iteration 14/25 | Loss: 0.00190534
Iteration 15/25 | Loss: 0.00190534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0019053354626521468, 0.0019053354626521468, 0.0019053354626521468, 0.0019053354626521468, 0.0019053354626521468]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019053354626521468

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29768813
Iteration 2/25 | Loss: 0.00393372
Iteration 3/25 | Loss: 0.00393372
Iteration 4/25 | Loss: 0.00393372
Iteration 5/25 | Loss: 0.00393372
Iteration 6/25 | Loss: 0.00393372
Iteration 7/25 | Loss: 0.00393371
Iteration 8/25 | Loss: 0.00393372
Iteration 9/25 | Loss: 0.00393372
Iteration 10/25 | Loss: 0.00393372
Iteration 11/25 | Loss: 0.00393372
Iteration 12/25 | Loss: 0.00393372
Iteration 13/25 | Loss: 0.00393371
Iteration 14/25 | Loss: 0.00393372
Iteration 15/25 | Loss: 0.00393372
Iteration 16/25 | Loss: 0.00393372
Iteration 17/25 | Loss: 0.00393372
Iteration 18/25 | Loss: 0.00393372
Iteration 19/25 | Loss: 0.00393372
Iteration 20/25 | Loss: 0.00393372
Iteration 21/25 | Loss: 0.00393372
Iteration 22/25 | Loss: 0.00393372
Iteration 23/25 | Loss: 0.00393372
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.003933715634047985, 0.003933715634047985, 0.003933715634047985, 0.003933715634047985, 0.003933715634047985]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003933715634047985

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00393372
Iteration 2/1000 | Loss: 0.00008223
Iteration 3/1000 | Loss: 0.00006691
Iteration 4/1000 | Loss: 0.00006062
Iteration 5/1000 | Loss: 0.00005721
Iteration 6/1000 | Loss: 0.00005580
Iteration 7/1000 | Loss: 0.00005473
Iteration 8/1000 | Loss: 0.00005426
Iteration 9/1000 | Loss: 0.00005390
Iteration 10/1000 | Loss: 0.00005373
Iteration 11/1000 | Loss: 0.00005348
Iteration 12/1000 | Loss: 0.00005336
Iteration 13/1000 | Loss: 0.00005336
Iteration 14/1000 | Loss: 0.00005336
Iteration 15/1000 | Loss: 0.00005336
Iteration 16/1000 | Loss: 0.00005336
Iteration 17/1000 | Loss: 0.00005336
Iteration 18/1000 | Loss: 0.00005336
Iteration 19/1000 | Loss: 0.00005336
Iteration 20/1000 | Loss: 0.00005335
Iteration 21/1000 | Loss: 0.00005335
Iteration 22/1000 | Loss: 0.00005334
Iteration 23/1000 | Loss: 0.00005333
Iteration 24/1000 | Loss: 0.00005325
Iteration 25/1000 | Loss: 0.00005325
Iteration 26/1000 | Loss: 0.00005323
Iteration 27/1000 | Loss: 0.00005322
Iteration 28/1000 | Loss: 0.00005322
Iteration 29/1000 | Loss: 0.00005322
Iteration 30/1000 | Loss: 0.00005321
Iteration 31/1000 | Loss: 0.00005321
Iteration 32/1000 | Loss: 0.00005321
Iteration 33/1000 | Loss: 0.00005321
Iteration 34/1000 | Loss: 0.00005320
Iteration 35/1000 | Loss: 0.00005320
Iteration 36/1000 | Loss: 0.00005319
Iteration 37/1000 | Loss: 0.00005319
Iteration 38/1000 | Loss: 0.00005319
Iteration 39/1000 | Loss: 0.00005319
Iteration 40/1000 | Loss: 0.00005318
Iteration 41/1000 | Loss: 0.00005318
Iteration 42/1000 | Loss: 0.00005318
Iteration 43/1000 | Loss: 0.00005318
Iteration 44/1000 | Loss: 0.00005318
Iteration 45/1000 | Loss: 0.00005317
Iteration 46/1000 | Loss: 0.00005317
Iteration 47/1000 | Loss: 0.00005317
Iteration 48/1000 | Loss: 0.00005316
Iteration 49/1000 | Loss: 0.00005316
Iteration 50/1000 | Loss: 0.00005316
Iteration 51/1000 | Loss: 0.00005315
Iteration 52/1000 | Loss: 0.00005315
Iteration 53/1000 | Loss: 0.00005315
Iteration 54/1000 | Loss: 0.00005314
Iteration 55/1000 | Loss: 0.00005314
Iteration 56/1000 | Loss: 0.00005314
Iteration 57/1000 | Loss: 0.00005314
Iteration 58/1000 | Loss: 0.00005313
Iteration 59/1000 | Loss: 0.00005312
Iteration 60/1000 | Loss: 0.00005311
Iteration 61/1000 | Loss: 0.00005311
Iteration 62/1000 | Loss: 0.00005310
Iteration 63/1000 | Loss: 0.00005310
Iteration 64/1000 | Loss: 0.00005309
Iteration 65/1000 | Loss: 0.00005309
Iteration 66/1000 | Loss: 0.00005309
Iteration 67/1000 | Loss: 0.00005309
Iteration 68/1000 | Loss: 0.00005308
Iteration 69/1000 | Loss: 0.00005308
Iteration 70/1000 | Loss: 0.00005308
Iteration 71/1000 | Loss: 0.00005307
Iteration 72/1000 | Loss: 0.00005307
Iteration 73/1000 | Loss: 0.00005307
Iteration 74/1000 | Loss: 0.00005306
Iteration 75/1000 | Loss: 0.00005305
Iteration 76/1000 | Loss: 0.00005305
Iteration 77/1000 | Loss: 0.00005305
Iteration 78/1000 | Loss: 0.00005305
Iteration 79/1000 | Loss: 0.00005305
Iteration 80/1000 | Loss: 0.00005305
Iteration 81/1000 | Loss: 0.00005304
Iteration 82/1000 | Loss: 0.00005304
Iteration 83/1000 | Loss: 0.00005304
Iteration 84/1000 | Loss: 0.00005304
Iteration 85/1000 | Loss: 0.00005304
Iteration 86/1000 | Loss: 0.00005304
Iteration 87/1000 | Loss: 0.00005304
Iteration 88/1000 | Loss: 0.00005304
Iteration 89/1000 | Loss: 0.00005303
Iteration 90/1000 | Loss: 0.00005303
Iteration 91/1000 | Loss: 0.00005302
Iteration 92/1000 | Loss: 0.00005302
Iteration 93/1000 | Loss: 0.00005302
Iteration 94/1000 | Loss: 0.00005302
Iteration 95/1000 | Loss: 0.00005302
Iteration 96/1000 | Loss: 0.00005302
Iteration 97/1000 | Loss: 0.00005302
Iteration 98/1000 | Loss: 0.00005302
Iteration 99/1000 | Loss: 0.00005302
Iteration 100/1000 | Loss: 0.00005302
Iteration 101/1000 | Loss: 0.00005302
Iteration 102/1000 | Loss: 0.00005301
Iteration 103/1000 | Loss: 0.00005301
Iteration 104/1000 | Loss: 0.00005301
Iteration 105/1000 | Loss: 0.00005301
Iteration 106/1000 | Loss: 0.00005300
Iteration 107/1000 | Loss: 0.00005300
Iteration 108/1000 | Loss: 0.00005300
Iteration 109/1000 | Loss: 0.00005300
Iteration 110/1000 | Loss: 0.00005300
Iteration 111/1000 | Loss: 0.00005300
Iteration 112/1000 | Loss: 0.00005300
Iteration 113/1000 | Loss: 0.00005300
Iteration 114/1000 | Loss: 0.00005299
Iteration 115/1000 | Loss: 0.00005299
Iteration 116/1000 | Loss: 0.00005299
Iteration 117/1000 | Loss: 0.00005299
Iteration 118/1000 | Loss: 0.00005299
Iteration 119/1000 | Loss: 0.00005299
Iteration 120/1000 | Loss: 0.00005299
Iteration 121/1000 | Loss: 0.00005299
Iteration 122/1000 | Loss: 0.00005299
Iteration 123/1000 | Loss: 0.00005299
Iteration 124/1000 | Loss: 0.00005299
Iteration 125/1000 | Loss: 0.00005299
Iteration 126/1000 | Loss: 0.00005298
Iteration 127/1000 | Loss: 0.00005298
Iteration 128/1000 | Loss: 0.00005298
Iteration 129/1000 | Loss: 0.00005298
Iteration 130/1000 | Loss: 0.00005298
Iteration 131/1000 | Loss: 0.00005298
Iteration 132/1000 | Loss: 0.00005298
Iteration 133/1000 | Loss: 0.00005298
Iteration 134/1000 | Loss: 0.00005298
Iteration 135/1000 | Loss: 0.00005298
Iteration 136/1000 | Loss: 0.00005297
Iteration 137/1000 | Loss: 0.00005297
Iteration 138/1000 | Loss: 0.00005297
Iteration 139/1000 | Loss: 0.00005297
Iteration 140/1000 | Loss: 0.00005297
Iteration 141/1000 | Loss: 0.00005297
Iteration 142/1000 | Loss: 0.00005297
Iteration 143/1000 | Loss: 0.00005297
Iteration 144/1000 | Loss: 0.00005297
Iteration 145/1000 | Loss: 0.00005297
Iteration 146/1000 | Loss: 0.00005297
Iteration 147/1000 | Loss: 0.00005296
Iteration 148/1000 | Loss: 0.00005296
Iteration 149/1000 | Loss: 0.00005296
Iteration 150/1000 | Loss: 0.00005296
Iteration 151/1000 | Loss: 0.00005296
Iteration 152/1000 | Loss: 0.00005296
Iteration 153/1000 | Loss: 0.00005296
Iteration 154/1000 | Loss: 0.00005296
Iteration 155/1000 | Loss: 0.00005296
Iteration 156/1000 | Loss: 0.00005296
Iteration 157/1000 | Loss: 0.00005296
Iteration 158/1000 | Loss: 0.00005296
Iteration 159/1000 | Loss: 0.00005296
Iteration 160/1000 | Loss: 0.00005296
Iteration 161/1000 | Loss: 0.00005296
Iteration 162/1000 | Loss: 0.00005296
Iteration 163/1000 | Loss: 0.00005296
Iteration 164/1000 | Loss: 0.00005296
Iteration 165/1000 | Loss: 0.00005296
Iteration 166/1000 | Loss: 0.00005296
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [5.2958494052290916e-05, 5.2958494052290916e-05, 5.2958494052290916e-05, 5.2958494052290916e-05, 5.2958494052290916e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.2958494052290916e-05

Optimization complete. Final v2v error: 6.341279029846191 mm

Highest mean error: 6.450536727905273 mm for frame 69

Lowest mean error: 6.1556878089904785 mm for frame 8

Saving results

Total time: 43.23357081413269
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887132
Iteration 2/25 | Loss: 0.00192076
Iteration 3/25 | Loss: 0.00182348
Iteration 4/25 | Loss: 0.00180819
Iteration 5/25 | Loss: 0.00180407
Iteration 6/25 | Loss: 0.00180365
Iteration 7/25 | Loss: 0.00180365
Iteration 8/25 | Loss: 0.00180365
Iteration 9/25 | Loss: 0.00180365
Iteration 10/25 | Loss: 0.00180365
Iteration 11/25 | Loss: 0.00180365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0018036471446976066, 0.0018036471446976066, 0.0018036471446976066, 0.0018036471446976066, 0.0018036471446976066]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018036471446976066

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57983327
Iteration 2/25 | Loss: 0.00267589
Iteration 3/25 | Loss: 0.00267578
Iteration 4/25 | Loss: 0.00267578
Iteration 5/25 | Loss: 0.00267578
Iteration 6/25 | Loss: 0.00267578
Iteration 7/25 | Loss: 0.00267578
Iteration 8/25 | Loss: 0.00267578
Iteration 9/25 | Loss: 0.00267578
Iteration 10/25 | Loss: 0.00267578
Iteration 11/25 | Loss: 0.00267578
Iteration 12/25 | Loss: 0.00267578
Iteration 13/25 | Loss: 0.00267578
Iteration 14/25 | Loss: 0.00267578
Iteration 15/25 | Loss: 0.00267578
Iteration 16/25 | Loss: 0.00267578
Iteration 17/25 | Loss: 0.00267578
Iteration 18/25 | Loss: 0.00267578
Iteration 19/25 | Loss: 0.00267578
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0026757779996842146, 0.0026757779996842146, 0.0026757779996842146, 0.0026757779996842146, 0.0026757779996842146]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026757779996842146

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267578
Iteration 2/1000 | Loss: 0.00006825
Iteration 3/1000 | Loss: 0.00004380
Iteration 4/1000 | Loss: 0.00003783
Iteration 5/1000 | Loss: 0.00003531
Iteration 6/1000 | Loss: 0.00003375
Iteration 7/1000 | Loss: 0.00003265
Iteration 8/1000 | Loss: 0.00003190
Iteration 9/1000 | Loss: 0.00003146
Iteration 10/1000 | Loss: 0.00003113
Iteration 11/1000 | Loss: 0.00003102
Iteration 12/1000 | Loss: 0.00003089
Iteration 13/1000 | Loss: 0.00003088
Iteration 14/1000 | Loss: 0.00003082
Iteration 15/1000 | Loss: 0.00003074
Iteration 16/1000 | Loss: 0.00003073
Iteration 17/1000 | Loss: 0.00003073
Iteration 18/1000 | Loss: 0.00003068
Iteration 19/1000 | Loss: 0.00003068
Iteration 20/1000 | Loss: 0.00003067
Iteration 21/1000 | Loss: 0.00003067
Iteration 22/1000 | Loss: 0.00003067
Iteration 23/1000 | Loss: 0.00003066
Iteration 24/1000 | Loss: 0.00003062
Iteration 25/1000 | Loss: 0.00003062
Iteration 26/1000 | Loss: 0.00003062
Iteration 27/1000 | Loss: 0.00003061
Iteration 28/1000 | Loss: 0.00003061
Iteration 29/1000 | Loss: 0.00003058
Iteration 30/1000 | Loss: 0.00003058
Iteration 31/1000 | Loss: 0.00003057
Iteration 32/1000 | Loss: 0.00003057
Iteration 33/1000 | Loss: 0.00003055
Iteration 34/1000 | Loss: 0.00003055
Iteration 35/1000 | Loss: 0.00003054
Iteration 36/1000 | Loss: 0.00003054
Iteration 37/1000 | Loss: 0.00003054
Iteration 38/1000 | Loss: 0.00003054
Iteration 39/1000 | Loss: 0.00003054
Iteration 40/1000 | Loss: 0.00003054
Iteration 41/1000 | Loss: 0.00003054
Iteration 42/1000 | Loss: 0.00003053
Iteration 43/1000 | Loss: 0.00003053
Iteration 44/1000 | Loss: 0.00003053
Iteration 45/1000 | Loss: 0.00003053
Iteration 46/1000 | Loss: 0.00003053
Iteration 47/1000 | Loss: 0.00003053
Iteration 48/1000 | Loss: 0.00003053
Iteration 49/1000 | Loss: 0.00003053
Iteration 50/1000 | Loss: 0.00003053
Iteration 51/1000 | Loss: 0.00003053
Iteration 52/1000 | Loss: 0.00003053
Iteration 53/1000 | Loss: 0.00003053
Iteration 54/1000 | Loss: 0.00003053
Iteration 55/1000 | Loss: 0.00003053
Iteration 56/1000 | Loss: 0.00003053
Iteration 57/1000 | Loss: 0.00003053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [3.053252657991834e-05, 3.053252657991834e-05, 3.053252657991834e-05, 3.053252657991834e-05, 3.053252657991834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.053252657991834e-05

Optimization complete. Final v2v error: 4.814541339874268 mm

Highest mean error: 5.281598091125488 mm for frame 17

Lowest mean error: 4.511161804199219 mm for frame 82

Saving results

Total time: 34.5526237487793
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00673103
Iteration 2/25 | Loss: 0.00187071
Iteration 3/25 | Loss: 0.00175295
Iteration 4/25 | Loss: 0.00174042
Iteration 5/25 | Loss: 0.00173758
Iteration 6/25 | Loss: 0.00173655
Iteration 7/25 | Loss: 0.00173655
Iteration 8/25 | Loss: 0.00173655
Iteration 9/25 | Loss: 0.00173655
Iteration 10/25 | Loss: 0.00173655
Iteration 11/25 | Loss: 0.00173655
Iteration 12/25 | Loss: 0.00173655
Iteration 13/25 | Loss: 0.00173655
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0017365483799949288, 0.0017365483799949288, 0.0017365483799949288, 0.0017365483799949288, 0.0017365483799949288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017365483799949288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.78337103
Iteration 2/25 | Loss: 0.00293255
Iteration 3/25 | Loss: 0.00293251
Iteration 4/25 | Loss: 0.00293251
Iteration 5/25 | Loss: 0.00293251
Iteration 6/25 | Loss: 0.00293251
Iteration 7/25 | Loss: 0.00293251
Iteration 8/25 | Loss: 0.00293251
Iteration 9/25 | Loss: 0.00293251
Iteration 10/25 | Loss: 0.00293251
Iteration 11/25 | Loss: 0.00293251
Iteration 12/25 | Loss: 0.00293251
Iteration 13/25 | Loss: 0.00293251
Iteration 14/25 | Loss: 0.00293251
Iteration 15/25 | Loss: 0.00293251
Iteration 16/25 | Loss: 0.00293251
Iteration 17/25 | Loss: 0.00293251
Iteration 18/25 | Loss: 0.00293251
Iteration 19/25 | Loss: 0.00293251
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0029325103387236595, 0.0029325103387236595, 0.0029325103387236595, 0.0029325103387236595, 0.0029325103387236595]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029325103387236595

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00293251
Iteration 2/1000 | Loss: 0.00009018
Iteration 3/1000 | Loss: 0.00005566
Iteration 4/1000 | Loss: 0.00004534
Iteration 5/1000 | Loss: 0.00004092
Iteration 6/1000 | Loss: 0.00003715
Iteration 7/1000 | Loss: 0.00003507
Iteration 8/1000 | Loss: 0.00003433
Iteration 9/1000 | Loss: 0.00003389
Iteration 10/1000 | Loss: 0.00003362
Iteration 11/1000 | Loss: 0.00003326
Iteration 12/1000 | Loss: 0.00003300
Iteration 13/1000 | Loss: 0.00003280
Iteration 14/1000 | Loss: 0.00003276
Iteration 15/1000 | Loss: 0.00003272
Iteration 16/1000 | Loss: 0.00003272
Iteration 17/1000 | Loss: 0.00003271
Iteration 18/1000 | Loss: 0.00003270
Iteration 19/1000 | Loss: 0.00003270
Iteration 20/1000 | Loss: 0.00003269
Iteration 21/1000 | Loss: 0.00003267
Iteration 22/1000 | Loss: 0.00003267
Iteration 23/1000 | Loss: 0.00003266
Iteration 24/1000 | Loss: 0.00003266
Iteration 25/1000 | Loss: 0.00003265
Iteration 26/1000 | Loss: 0.00003264
Iteration 27/1000 | Loss: 0.00003264
Iteration 28/1000 | Loss: 0.00003261
Iteration 29/1000 | Loss: 0.00003261
Iteration 30/1000 | Loss: 0.00003261
Iteration 31/1000 | Loss: 0.00003261
Iteration 32/1000 | Loss: 0.00003261
Iteration 33/1000 | Loss: 0.00003261
Iteration 34/1000 | Loss: 0.00003261
Iteration 35/1000 | Loss: 0.00003261
Iteration 36/1000 | Loss: 0.00003261
Iteration 37/1000 | Loss: 0.00003260
Iteration 38/1000 | Loss: 0.00003260
Iteration 39/1000 | Loss: 0.00003260
Iteration 40/1000 | Loss: 0.00003260
Iteration 41/1000 | Loss: 0.00003260
Iteration 42/1000 | Loss: 0.00003260
Iteration 43/1000 | Loss: 0.00003260
Iteration 44/1000 | Loss: 0.00003260
Iteration 45/1000 | Loss: 0.00003260
Iteration 46/1000 | Loss: 0.00003259
Iteration 47/1000 | Loss: 0.00003259
Iteration 48/1000 | Loss: 0.00003259
Iteration 49/1000 | Loss: 0.00003257
Iteration 50/1000 | Loss: 0.00003256
Iteration 51/1000 | Loss: 0.00003256
Iteration 52/1000 | Loss: 0.00003256
Iteration 53/1000 | Loss: 0.00003255
Iteration 54/1000 | Loss: 0.00003255
Iteration 55/1000 | Loss: 0.00003254
Iteration 56/1000 | Loss: 0.00003254
Iteration 57/1000 | Loss: 0.00003254
Iteration 58/1000 | Loss: 0.00003254
Iteration 59/1000 | Loss: 0.00003254
Iteration 60/1000 | Loss: 0.00003254
Iteration 61/1000 | Loss: 0.00003253
Iteration 62/1000 | Loss: 0.00003253
Iteration 63/1000 | Loss: 0.00003253
Iteration 64/1000 | Loss: 0.00003253
Iteration 65/1000 | Loss: 0.00003253
Iteration 66/1000 | Loss: 0.00003253
Iteration 67/1000 | Loss: 0.00003253
Iteration 68/1000 | Loss: 0.00003252
Iteration 69/1000 | Loss: 0.00003252
Iteration 70/1000 | Loss: 0.00003252
Iteration 71/1000 | Loss: 0.00003251
Iteration 72/1000 | Loss: 0.00003251
Iteration 73/1000 | Loss: 0.00003250
Iteration 74/1000 | Loss: 0.00003250
Iteration 75/1000 | Loss: 0.00003249
Iteration 76/1000 | Loss: 0.00003249
Iteration 77/1000 | Loss: 0.00003248
Iteration 78/1000 | Loss: 0.00003248
Iteration 79/1000 | Loss: 0.00003248
Iteration 80/1000 | Loss: 0.00003247
Iteration 81/1000 | Loss: 0.00003247
Iteration 82/1000 | Loss: 0.00003247
Iteration 83/1000 | Loss: 0.00003246
Iteration 84/1000 | Loss: 0.00003246
Iteration 85/1000 | Loss: 0.00003246
Iteration 86/1000 | Loss: 0.00003246
Iteration 87/1000 | Loss: 0.00003246
Iteration 88/1000 | Loss: 0.00003246
Iteration 89/1000 | Loss: 0.00003245
Iteration 90/1000 | Loss: 0.00003245
Iteration 91/1000 | Loss: 0.00003245
Iteration 92/1000 | Loss: 0.00003245
Iteration 93/1000 | Loss: 0.00003244
Iteration 94/1000 | Loss: 0.00003244
Iteration 95/1000 | Loss: 0.00003244
Iteration 96/1000 | Loss: 0.00003244
Iteration 97/1000 | Loss: 0.00003244
Iteration 98/1000 | Loss: 0.00003244
Iteration 99/1000 | Loss: 0.00003244
Iteration 100/1000 | Loss: 0.00003243
Iteration 101/1000 | Loss: 0.00003243
Iteration 102/1000 | Loss: 0.00003243
Iteration 103/1000 | Loss: 0.00003243
Iteration 104/1000 | Loss: 0.00003243
Iteration 105/1000 | Loss: 0.00003243
Iteration 106/1000 | Loss: 0.00003243
Iteration 107/1000 | Loss: 0.00003242
Iteration 108/1000 | Loss: 0.00003242
Iteration 109/1000 | Loss: 0.00003242
Iteration 110/1000 | Loss: 0.00003242
Iteration 111/1000 | Loss: 0.00003242
Iteration 112/1000 | Loss: 0.00003242
Iteration 113/1000 | Loss: 0.00003242
Iteration 114/1000 | Loss: 0.00003242
Iteration 115/1000 | Loss: 0.00003242
Iteration 116/1000 | Loss: 0.00003242
Iteration 117/1000 | Loss: 0.00003242
Iteration 118/1000 | Loss: 0.00003242
Iteration 119/1000 | Loss: 0.00003242
Iteration 120/1000 | Loss: 0.00003242
Iteration 121/1000 | Loss: 0.00003242
Iteration 122/1000 | Loss: 0.00003242
Iteration 123/1000 | Loss: 0.00003242
Iteration 124/1000 | Loss: 0.00003242
Iteration 125/1000 | Loss: 0.00003242
Iteration 126/1000 | Loss: 0.00003242
Iteration 127/1000 | Loss: 0.00003242
Iteration 128/1000 | Loss: 0.00003242
Iteration 129/1000 | Loss: 0.00003242
Iteration 130/1000 | Loss: 0.00003242
Iteration 131/1000 | Loss: 0.00003242
Iteration 132/1000 | Loss: 0.00003242
Iteration 133/1000 | Loss: 0.00003242
Iteration 134/1000 | Loss: 0.00003242
Iteration 135/1000 | Loss: 0.00003242
Iteration 136/1000 | Loss: 0.00003242
Iteration 137/1000 | Loss: 0.00003242
Iteration 138/1000 | Loss: 0.00003242
Iteration 139/1000 | Loss: 0.00003242
Iteration 140/1000 | Loss: 0.00003242
Iteration 141/1000 | Loss: 0.00003242
Iteration 142/1000 | Loss: 0.00003242
Iteration 143/1000 | Loss: 0.00003242
Iteration 144/1000 | Loss: 0.00003242
Iteration 145/1000 | Loss: 0.00003242
Iteration 146/1000 | Loss: 0.00003242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [3.24168213410303e-05, 3.24168213410303e-05, 3.24168213410303e-05, 3.24168213410303e-05, 3.24168213410303e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.24168213410303e-05

Optimization complete. Final v2v error: 5.004818916320801 mm

Highest mean error: 5.484076023101807 mm for frame 0

Lowest mean error: 4.833522796630859 mm for frame 10

Saving results

Total time: 35.73564648628235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00435908
Iteration 2/25 | Loss: 0.00185212
Iteration 3/25 | Loss: 0.00177280
Iteration 4/25 | Loss: 0.00175528
Iteration 5/25 | Loss: 0.00174935
Iteration 6/25 | Loss: 0.00174789
Iteration 7/25 | Loss: 0.00174789
Iteration 8/25 | Loss: 0.00174789
Iteration 9/25 | Loss: 0.00174789
Iteration 10/25 | Loss: 0.00174789
Iteration 11/25 | Loss: 0.00174789
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001747889444231987, 0.001747889444231987, 0.001747889444231987, 0.001747889444231987, 0.001747889444231987]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001747889444231987

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60378957
Iteration 2/25 | Loss: 0.00395913
Iteration 3/25 | Loss: 0.00395913
Iteration 4/25 | Loss: 0.00395913
Iteration 5/25 | Loss: 0.00395913
Iteration 6/25 | Loss: 0.00395913
Iteration 7/25 | Loss: 0.00395913
Iteration 8/25 | Loss: 0.00395913
Iteration 9/25 | Loss: 0.00395913
Iteration 10/25 | Loss: 0.00395913
Iteration 11/25 | Loss: 0.00395913
Iteration 12/25 | Loss: 0.00395913
Iteration 13/25 | Loss: 0.00395913
Iteration 14/25 | Loss: 0.00395913
Iteration 15/25 | Loss: 0.00395913
Iteration 16/25 | Loss: 0.00395913
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.003959127236157656, 0.003959127236157656, 0.003959127236157656, 0.003959127236157656, 0.003959127236157656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003959127236157656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00395913
Iteration 2/1000 | Loss: 0.00009534
Iteration 3/1000 | Loss: 0.00006156
Iteration 4/1000 | Loss: 0.00005057
Iteration 5/1000 | Loss: 0.00004487
Iteration 6/1000 | Loss: 0.00004208
Iteration 7/1000 | Loss: 0.00004010
Iteration 8/1000 | Loss: 0.00003892
Iteration 9/1000 | Loss: 0.00003770
Iteration 10/1000 | Loss: 0.00003689
Iteration 11/1000 | Loss: 0.00003636
Iteration 12/1000 | Loss: 0.00003583
Iteration 13/1000 | Loss: 0.00003548
Iteration 14/1000 | Loss: 0.00003527
Iteration 15/1000 | Loss: 0.00003510
Iteration 16/1000 | Loss: 0.00003495
Iteration 17/1000 | Loss: 0.00003492
Iteration 18/1000 | Loss: 0.00003483
Iteration 19/1000 | Loss: 0.00003480
Iteration 20/1000 | Loss: 0.00003476
Iteration 21/1000 | Loss: 0.00003476
Iteration 22/1000 | Loss: 0.00003475
Iteration 23/1000 | Loss: 0.00003474
Iteration 24/1000 | Loss: 0.00003473
Iteration 25/1000 | Loss: 0.00003473
Iteration 26/1000 | Loss: 0.00003471
Iteration 27/1000 | Loss: 0.00003470
Iteration 28/1000 | Loss: 0.00003469
Iteration 29/1000 | Loss: 0.00003469
Iteration 30/1000 | Loss: 0.00003466
Iteration 31/1000 | Loss: 0.00003462
Iteration 32/1000 | Loss: 0.00003459
Iteration 33/1000 | Loss: 0.00003459
Iteration 34/1000 | Loss: 0.00003458
Iteration 35/1000 | Loss: 0.00003454
Iteration 36/1000 | Loss: 0.00003454
Iteration 37/1000 | Loss: 0.00003454
Iteration 38/1000 | Loss: 0.00003452
Iteration 39/1000 | Loss: 0.00003452
Iteration 40/1000 | Loss: 0.00003451
Iteration 41/1000 | Loss: 0.00003451
Iteration 42/1000 | Loss: 0.00003451
Iteration 43/1000 | Loss: 0.00003450
Iteration 44/1000 | Loss: 0.00003450
Iteration 45/1000 | Loss: 0.00003449
Iteration 46/1000 | Loss: 0.00003449
Iteration 47/1000 | Loss: 0.00003448
Iteration 48/1000 | Loss: 0.00003448
Iteration 49/1000 | Loss: 0.00003448
Iteration 50/1000 | Loss: 0.00003447
Iteration 51/1000 | Loss: 0.00003447
Iteration 52/1000 | Loss: 0.00003447
Iteration 53/1000 | Loss: 0.00003446
Iteration 54/1000 | Loss: 0.00003446
Iteration 55/1000 | Loss: 0.00003446
Iteration 56/1000 | Loss: 0.00003446
Iteration 57/1000 | Loss: 0.00003446
Iteration 58/1000 | Loss: 0.00003445
Iteration 59/1000 | Loss: 0.00003445
Iteration 60/1000 | Loss: 0.00003445
Iteration 61/1000 | Loss: 0.00003445
Iteration 62/1000 | Loss: 0.00003444
Iteration 63/1000 | Loss: 0.00003444
Iteration 64/1000 | Loss: 0.00003444
Iteration 65/1000 | Loss: 0.00003444
Iteration 66/1000 | Loss: 0.00003443
Iteration 67/1000 | Loss: 0.00003443
Iteration 68/1000 | Loss: 0.00003443
Iteration 69/1000 | Loss: 0.00003443
Iteration 70/1000 | Loss: 0.00003442
Iteration 71/1000 | Loss: 0.00003442
Iteration 72/1000 | Loss: 0.00003442
Iteration 73/1000 | Loss: 0.00003441
Iteration 74/1000 | Loss: 0.00003441
Iteration 75/1000 | Loss: 0.00003441
Iteration 76/1000 | Loss: 0.00003441
Iteration 77/1000 | Loss: 0.00003441
Iteration 78/1000 | Loss: 0.00003440
Iteration 79/1000 | Loss: 0.00003440
Iteration 80/1000 | Loss: 0.00003440
Iteration 81/1000 | Loss: 0.00003440
Iteration 82/1000 | Loss: 0.00003440
Iteration 83/1000 | Loss: 0.00003439
Iteration 84/1000 | Loss: 0.00003439
Iteration 85/1000 | Loss: 0.00003438
Iteration 86/1000 | Loss: 0.00003438
Iteration 87/1000 | Loss: 0.00003438
Iteration 88/1000 | Loss: 0.00003438
Iteration 89/1000 | Loss: 0.00003437
Iteration 90/1000 | Loss: 0.00003437
Iteration 91/1000 | Loss: 0.00003437
Iteration 92/1000 | Loss: 0.00003437
Iteration 93/1000 | Loss: 0.00003437
Iteration 94/1000 | Loss: 0.00003437
Iteration 95/1000 | Loss: 0.00003437
Iteration 96/1000 | Loss: 0.00003437
Iteration 97/1000 | Loss: 0.00003437
Iteration 98/1000 | Loss: 0.00003436
Iteration 99/1000 | Loss: 0.00003436
Iteration 100/1000 | Loss: 0.00003436
Iteration 101/1000 | Loss: 0.00003436
Iteration 102/1000 | Loss: 0.00003435
Iteration 103/1000 | Loss: 0.00003435
Iteration 104/1000 | Loss: 0.00003435
Iteration 105/1000 | Loss: 0.00003435
Iteration 106/1000 | Loss: 0.00003434
Iteration 107/1000 | Loss: 0.00003434
Iteration 108/1000 | Loss: 0.00003434
Iteration 109/1000 | Loss: 0.00003434
Iteration 110/1000 | Loss: 0.00003434
Iteration 111/1000 | Loss: 0.00003434
Iteration 112/1000 | Loss: 0.00003434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 112. Stopping optimization.
Last 5 losses: [3.434224709053524e-05, 3.434224709053524e-05, 3.434224709053524e-05, 3.434224709053524e-05, 3.434224709053524e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.434224709053524e-05

Optimization complete. Final v2v error: 5.0595011711120605 mm

Highest mean error: 5.695917129516602 mm for frame 242

Lowest mean error: 4.462234973907471 mm for frame 51

Saving results

Total time: 49.46500372886658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01237860
Iteration 2/25 | Loss: 0.00377386
Iteration 3/25 | Loss: 0.00268075
Iteration 4/25 | Loss: 0.00221132
Iteration 5/25 | Loss: 0.00183250
Iteration 6/25 | Loss: 0.00162444
Iteration 7/25 | Loss: 0.00141902
Iteration 8/25 | Loss: 0.00139361
Iteration 9/25 | Loss: 0.00139008
Iteration 10/25 | Loss: 0.00137784
Iteration 11/25 | Loss: 0.00136265
Iteration 12/25 | Loss: 0.00136158
Iteration 13/25 | Loss: 0.00136060
Iteration 14/25 | Loss: 0.00136092
Iteration 15/25 | Loss: 0.00136062
Iteration 16/25 | Loss: 0.00136064
Iteration 17/25 | Loss: 0.00136007
Iteration 18/25 | Loss: 0.00135983
Iteration 19/25 | Loss: 0.00138564
Iteration 20/25 | Loss: 0.00136307
Iteration 21/25 | Loss: 0.00139389
Iteration 22/25 | Loss: 0.00136089
Iteration 23/25 | Loss: 0.00135977
Iteration 24/25 | Loss: 0.00138608
Iteration 25/25 | Loss: 0.00135916

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.81056392
Iteration 2/25 | Loss: 0.00269188
Iteration 3/25 | Loss: 0.00161714
Iteration 4/25 | Loss: 0.00161714
Iteration 5/25 | Loss: 0.00161714
Iteration 6/25 | Loss: 0.00161714
Iteration 7/25 | Loss: 0.00161714
Iteration 8/25 | Loss: 0.00161714
Iteration 9/25 | Loss: 0.00161714
Iteration 10/25 | Loss: 0.00161714
Iteration 11/25 | Loss: 0.00161714
Iteration 12/25 | Loss: 0.00161714
Iteration 13/25 | Loss: 0.00161714
Iteration 14/25 | Loss: 0.00161714
Iteration 15/25 | Loss: 0.00161714
Iteration 16/25 | Loss: 0.00161714
Iteration 17/25 | Loss: 0.00161714
Iteration 18/25 | Loss: 0.00161714
Iteration 19/25 | Loss: 0.00161714
Iteration 20/25 | Loss: 0.00161714
Iteration 21/25 | Loss: 0.00161714
Iteration 22/25 | Loss: 0.00161714
Iteration 23/25 | Loss: 0.00161714
Iteration 24/25 | Loss: 0.00161714
Iteration 25/25 | Loss: 0.00161714

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161714
Iteration 2/1000 | Loss: 0.00106664
Iteration 3/1000 | Loss: 0.00006789
Iteration 4/1000 | Loss: 0.00005530
Iteration 5/1000 | Loss: 0.00005280
Iteration 6/1000 | Loss: 0.00004930
Iteration 7/1000 | Loss: 0.00005203
Iteration 8/1000 | Loss: 0.00005038
Iteration 9/1000 | Loss: 0.00005116
Iteration 10/1000 | Loss: 0.00005414
Iteration 11/1000 | Loss: 0.00005233
Iteration 12/1000 | Loss: 0.00004972
Iteration 13/1000 | Loss: 0.00005089
Iteration 14/1000 | Loss: 0.00005416
Iteration 15/1000 | Loss: 0.00005344
Iteration 16/1000 | Loss: 0.00005402
Iteration 17/1000 | Loss: 0.00007844
Iteration 18/1000 | Loss: 0.00011323
Iteration 19/1000 | Loss: 0.00005194
Iteration 20/1000 | Loss: 0.00005260
Iteration 21/1000 | Loss: 0.00005254
Iteration 22/1000 | Loss: 0.00005308
Iteration 23/1000 | Loss: 0.00005321
Iteration 24/1000 | Loss: 0.00004759
Iteration 25/1000 | Loss: 0.00004962
Iteration 26/1000 | Loss: 0.00004912
Iteration 27/1000 | Loss: 0.00005207
Iteration 28/1000 | Loss: 0.00005042
Iteration 29/1000 | Loss: 0.00004901
Iteration 30/1000 | Loss: 0.00004929
Iteration 31/1000 | Loss: 0.00004932
Iteration 32/1000 | Loss: 0.00005296
Iteration 33/1000 | Loss: 0.00005365
Iteration 34/1000 | Loss: 0.00005433
Iteration 35/1000 | Loss: 0.00005171
Iteration 36/1000 | Loss: 0.00005251
Iteration 37/1000 | Loss: 0.00005207
Iteration 38/1000 | Loss: 0.00006495
Iteration 39/1000 | Loss: 0.00005229
Iteration 40/1000 | Loss: 0.00005167
Iteration 41/1000 | Loss: 0.00005226
Iteration 42/1000 | Loss: 0.00006376
Iteration 43/1000 | Loss: 0.00004629
Iteration 44/1000 | Loss: 0.00004424
Iteration 45/1000 | Loss: 0.00004401
Iteration 46/1000 | Loss: 0.00004385
Iteration 47/1000 | Loss: 0.00004385
Iteration 48/1000 | Loss: 0.00004385
Iteration 49/1000 | Loss: 0.00004384
Iteration 50/1000 | Loss: 0.00004384
Iteration 51/1000 | Loss: 0.00004384
Iteration 52/1000 | Loss: 0.00004384
Iteration 53/1000 | Loss: 0.00004383
Iteration 54/1000 | Loss: 0.00004383
Iteration 55/1000 | Loss: 0.00004382
Iteration 56/1000 | Loss: 0.00004382
Iteration 57/1000 | Loss: 0.00004382
Iteration 58/1000 | Loss: 0.00004382
Iteration 59/1000 | Loss: 0.00004381
Iteration 60/1000 | Loss: 0.00004380
Iteration 61/1000 | Loss: 0.00004380
Iteration 62/1000 | Loss: 0.00004380
Iteration 63/1000 | Loss: 0.00004379
Iteration 64/1000 | Loss: 0.00004379
Iteration 65/1000 | Loss: 0.00004379
Iteration 66/1000 | Loss: 0.00004379
Iteration 67/1000 | Loss: 0.00004378
Iteration 68/1000 | Loss: 0.00004378
Iteration 69/1000 | Loss: 0.00004377
Iteration 70/1000 | Loss: 0.00004377
Iteration 71/1000 | Loss: 0.00004377
Iteration 72/1000 | Loss: 0.00004376
Iteration 73/1000 | Loss: 0.00004376
Iteration 74/1000 | Loss: 0.00004376
Iteration 75/1000 | Loss: 0.00004376
Iteration 76/1000 | Loss: 0.00004376
Iteration 77/1000 | Loss: 0.00004376
Iteration 78/1000 | Loss: 0.00004376
Iteration 79/1000 | Loss: 0.00004376
Iteration 80/1000 | Loss: 0.00004375
Iteration 81/1000 | Loss: 0.00004375
Iteration 82/1000 | Loss: 0.00004375
Iteration 83/1000 | Loss: 0.00004375
Iteration 84/1000 | Loss: 0.00004375
Iteration 85/1000 | Loss: 0.00004375
Iteration 86/1000 | Loss: 0.00004375
Iteration 87/1000 | Loss: 0.00004375
Iteration 88/1000 | Loss: 0.00004375
Iteration 89/1000 | Loss: 0.00004375
Iteration 90/1000 | Loss: 0.00004375
Iteration 91/1000 | Loss: 0.00004375
Iteration 92/1000 | Loss: 0.00004374
Iteration 93/1000 | Loss: 0.00004374
Iteration 94/1000 | Loss: 0.00004374
Iteration 95/1000 | Loss: 0.00004373
Iteration 96/1000 | Loss: 0.00004373
Iteration 97/1000 | Loss: 0.00004373
Iteration 98/1000 | Loss: 0.00004373
Iteration 99/1000 | Loss: 0.00004373
Iteration 100/1000 | Loss: 0.00004372
Iteration 101/1000 | Loss: 0.00004371
Iteration 102/1000 | Loss: 0.00004371
Iteration 103/1000 | Loss: 0.00004371
Iteration 104/1000 | Loss: 0.00004371
Iteration 105/1000 | Loss: 0.00004371
Iteration 106/1000 | Loss: 0.00004371
Iteration 107/1000 | Loss: 0.00004371
Iteration 108/1000 | Loss: 0.00004371
Iteration 109/1000 | Loss: 0.00004371
Iteration 110/1000 | Loss: 0.00004371
Iteration 111/1000 | Loss: 0.00004371
Iteration 112/1000 | Loss: 0.00004371
Iteration 113/1000 | Loss: 0.00004371
Iteration 114/1000 | Loss: 0.00004371
Iteration 115/1000 | Loss: 0.00004370
Iteration 116/1000 | Loss: 0.00004370
Iteration 117/1000 | Loss: 0.00004370
Iteration 118/1000 | Loss: 0.00004370
Iteration 119/1000 | Loss: 0.00004370
Iteration 120/1000 | Loss: 0.00004370
Iteration 121/1000 | Loss: 0.00004370
Iteration 122/1000 | Loss: 0.00004370
Iteration 123/1000 | Loss: 0.00004370
Iteration 124/1000 | Loss: 0.00004370
Iteration 125/1000 | Loss: 0.00004370
Iteration 126/1000 | Loss: 0.00004369
Iteration 127/1000 | Loss: 0.00004369
Iteration 128/1000 | Loss: 0.00004369
Iteration 129/1000 | Loss: 0.00004369
Iteration 130/1000 | Loss: 0.00004369
Iteration 131/1000 | Loss: 0.00004369
Iteration 132/1000 | Loss: 0.00004369
Iteration 133/1000 | Loss: 0.00004369
Iteration 134/1000 | Loss: 0.00004369
Iteration 135/1000 | Loss: 0.00004369
Iteration 136/1000 | Loss: 0.00004369
Iteration 137/1000 | Loss: 0.00004368
Iteration 138/1000 | Loss: 0.00004368
Iteration 139/1000 | Loss: 0.00004368
Iteration 140/1000 | Loss: 0.00004368
Iteration 141/1000 | Loss: 0.00004367
Iteration 142/1000 | Loss: 0.00004367
Iteration 143/1000 | Loss: 0.00004367
Iteration 144/1000 | Loss: 0.00004367
Iteration 145/1000 | Loss: 0.00004366
Iteration 146/1000 | Loss: 0.00004366
Iteration 147/1000 | Loss: 0.00004366
Iteration 148/1000 | Loss: 0.00004366
Iteration 149/1000 | Loss: 0.00004366
Iteration 150/1000 | Loss: 0.00004366
Iteration 151/1000 | Loss: 0.00004366
Iteration 152/1000 | Loss: 0.00004366
Iteration 153/1000 | Loss: 0.00004366
Iteration 154/1000 | Loss: 0.00004366
Iteration 155/1000 | Loss: 0.00004365
Iteration 156/1000 | Loss: 0.00004365
Iteration 157/1000 | Loss: 0.00004365
Iteration 158/1000 | Loss: 0.00004365
Iteration 159/1000 | Loss: 0.00004365
Iteration 160/1000 | Loss: 0.00004365
Iteration 161/1000 | Loss: 0.00004365
Iteration 162/1000 | Loss: 0.00004365
Iteration 163/1000 | Loss: 0.00004365
Iteration 164/1000 | Loss: 0.00004365
Iteration 165/1000 | Loss: 0.00004365
Iteration 166/1000 | Loss: 0.00004365
Iteration 167/1000 | Loss: 0.00004365
Iteration 168/1000 | Loss: 0.00004365
Iteration 169/1000 | Loss: 0.00004365
Iteration 170/1000 | Loss: 0.00004365
Iteration 171/1000 | Loss: 0.00004365
Iteration 172/1000 | Loss: 0.00004365
Iteration 173/1000 | Loss: 0.00004365
Iteration 174/1000 | Loss: 0.00004365
Iteration 175/1000 | Loss: 0.00004364
Iteration 176/1000 | Loss: 0.00004364
Iteration 177/1000 | Loss: 0.00004364
Iteration 178/1000 | Loss: 0.00004364
Iteration 179/1000 | Loss: 0.00004364
Iteration 180/1000 | Loss: 0.00004364
Iteration 181/1000 | Loss: 0.00004364
Iteration 182/1000 | Loss: 0.00004364
Iteration 183/1000 | Loss: 0.00004364
Iteration 184/1000 | Loss: 0.00004364
Iteration 185/1000 | Loss: 0.00004364
Iteration 186/1000 | Loss: 0.00004364
Iteration 187/1000 | Loss: 0.00004364
Iteration 188/1000 | Loss: 0.00004364
Iteration 189/1000 | Loss: 0.00004364
Iteration 190/1000 | Loss: 0.00004364
Iteration 191/1000 | Loss: 0.00004364
Iteration 192/1000 | Loss: 0.00004364
Iteration 193/1000 | Loss: 0.00004364
Iteration 194/1000 | Loss: 0.00004364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [4.3640022340696305e-05, 4.3640022340696305e-05, 4.3640022340696305e-05, 4.3640022340696305e-05, 4.3640022340696305e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.3640022340696305e-05

Optimization complete. Final v2v error: 5.281078815460205 mm

Highest mean error: 11.929068565368652 mm for frame 50

Lowest mean error: 4.567790508270264 mm for frame 6

Saving results

Total time: 116.43497562408447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00472681
Iteration 2/25 | Loss: 0.00173598
Iteration 3/25 | Loss: 0.00166826
Iteration 4/25 | Loss: 0.00165763
Iteration 5/25 | Loss: 0.00165222
Iteration 6/25 | Loss: 0.00165172
Iteration 7/25 | Loss: 0.00165172
Iteration 8/25 | Loss: 0.00165172
Iteration 9/25 | Loss: 0.00165172
Iteration 10/25 | Loss: 0.00165172
Iteration 11/25 | Loss: 0.00165172
Iteration 12/25 | Loss: 0.00165172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0016517216572538018, 0.0016517216572538018, 0.0016517216572538018, 0.0016517216572538018, 0.0016517216572538018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016517216572538018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56994557
Iteration 2/25 | Loss: 0.00255073
Iteration 3/25 | Loss: 0.00255073
Iteration 4/25 | Loss: 0.00255073
Iteration 5/25 | Loss: 0.00255073
Iteration 6/25 | Loss: 0.00255073
Iteration 7/25 | Loss: 0.00255073
Iteration 8/25 | Loss: 0.00255073
Iteration 9/25 | Loss: 0.00255073
Iteration 10/25 | Loss: 0.00255073
Iteration 11/25 | Loss: 0.00255073
Iteration 12/25 | Loss: 0.00255073
Iteration 13/25 | Loss: 0.00255073
Iteration 14/25 | Loss: 0.00255073
Iteration 15/25 | Loss: 0.00255073
Iteration 16/25 | Loss: 0.00255073
Iteration 17/25 | Loss: 0.00255073
Iteration 18/25 | Loss: 0.00255073
Iteration 19/25 | Loss: 0.00255073
Iteration 20/25 | Loss: 0.00255073
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002550727454945445, 0.002550727454945445, 0.002550727454945445, 0.002550727454945445, 0.002550727454945445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002550727454945445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255073
Iteration 2/1000 | Loss: 0.00004324
Iteration 3/1000 | Loss: 0.00003536
Iteration 4/1000 | Loss: 0.00003258
Iteration 5/1000 | Loss: 0.00003140
Iteration 6/1000 | Loss: 0.00003047
Iteration 7/1000 | Loss: 0.00002998
Iteration 8/1000 | Loss: 0.00002946
Iteration 9/1000 | Loss: 0.00002916
Iteration 10/1000 | Loss: 0.00002895
Iteration 11/1000 | Loss: 0.00002888
Iteration 12/1000 | Loss: 0.00002880
Iteration 13/1000 | Loss: 0.00002879
Iteration 14/1000 | Loss: 0.00002878
Iteration 15/1000 | Loss: 0.00002872
Iteration 16/1000 | Loss: 0.00002872
Iteration 17/1000 | Loss: 0.00002868
Iteration 18/1000 | Loss: 0.00002866
Iteration 19/1000 | Loss: 0.00002865
Iteration 20/1000 | Loss: 0.00002865
Iteration 21/1000 | Loss: 0.00002864
Iteration 22/1000 | Loss: 0.00002864
Iteration 23/1000 | Loss: 0.00002862
Iteration 24/1000 | Loss: 0.00002862
Iteration 25/1000 | Loss: 0.00002862
Iteration 26/1000 | Loss: 0.00002862
Iteration 27/1000 | Loss: 0.00002862
Iteration 28/1000 | Loss: 0.00002862
Iteration 29/1000 | Loss: 0.00002861
Iteration 30/1000 | Loss: 0.00002861
Iteration 31/1000 | Loss: 0.00002861
Iteration 32/1000 | Loss: 0.00002861
Iteration 33/1000 | Loss: 0.00002861
Iteration 34/1000 | Loss: 0.00002860
Iteration 35/1000 | Loss: 0.00002859
Iteration 36/1000 | Loss: 0.00002859
Iteration 37/1000 | Loss: 0.00002858
Iteration 38/1000 | Loss: 0.00002858
Iteration 39/1000 | Loss: 0.00002858
Iteration 40/1000 | Loss: 0.00002858
Iteration 41/1000 | Loss: 0.00002857
Iteration 42/1000 | Loss: 0.00002856
Iteration 43/1000 | Loss: 0.00002855
Iteration 44/1000 | Loss: 0.00002855
Iteration 45/1000 | Loss: 0.00002854
Iteration 46/1000 | Loss: 0.00002854
Iteration 47/1000 | Loss: 0.00002854
Iteration 48/1000 | Loss: 0.00002854
Iteration 49/1000 | Loss: 0.00002854
Iteration 50/1000 | Loss: 0.00002854
Iteration 51/1000 | Loss: 0.00002854
Iteration 52/1000 | Loss: 0.00002854
Iteration 53/1000 | Loss: 0.00002854
Iteration 54/1000 | Loss: 0.00002854
Iteration 55/1000 | Loss: 0.00002853
Iteration 56/1000 | Loss: 0.00002853
Iteration 57/1000 | Loss: 0.00002853
Iteration 58/1000 | Loss: 0.00002852
Iteration 59/1000 | Loss: 0.00002852
Iteration 60/1000 | Loss: 0.00002852
Iteration 61/1000 | Loss: 0.00002852
Iteration 62/1000 | Loss: 0.00002852
Iteration 63/1000 | Loss: 0.00002851
Iteration 64/1000 | Loss: 0.00002851
Iteration 65/1000 | Loss: 0.00002851
Iteration 66/1000 | Loss: 0.00002851
Iteration 67/1000 | Loss: 0.00002851
Iteration 68/1000 | Loss: 0.00002851
Iteration 69/1000 | Loss: 0.00002851
Iteration 70/1000 | Loss: 0.00002851
Iteration 71/1000 | Loss: 0.00002851
Iteration 72/1000 | Loss: 0.00002851
Iteration 73/1000 | Loss: 0.00002850
Iteration 74/1000 | Loss: 0.00002850
Iteration 75/1000 | Loss: 0.00002850
Iteration 76/1000 | Loss: 0.00002850
Iteration 77/1000 | Loss: 0.00002850
Iteration 78/1000 | Loss: 0.00002850
Iteration 79/1000 | Loss: 0.00002850
Iteration 80/1000 | Loss: 0.00002850
Iteration 81/1000 | Loss: 0.00002850
Iteration 82/1000 | Loss: 0.00002849
Iteration 83/1000 | Loss: 0.00002849
Iteration 84/1000 | Loss: 0.00002849
Iteration 85/1000 | Loss: 0.00002849
Iteration 86/1000 | Loss: 0.00002849
Iteration 87/1000 | Loss: 0.00002849
Iteration 88/1000 | Loss: 0.00002849
Iteration 89/1000 | Loss: 0.00002849
Iteration 90/1000 | Loss: 0.00002849
Iteration 91/1000 | Loss: 0.00002848
Iteration 92/1000 | Loss: 0.00002848
Iteration 93/1000 | Loss: 0.00002848
Iteration 94/1000 | Loss: 0.00002848
Iteration 95/1000 | Loss: 0.00002848
Iteration 96/1000 | Loss: 0.00002848
Iteration 97/1000 | Loss: 0.00002848
Iteration 98/1000 | Loss: 0.00002848
Iteration 99/1000 | Loss: 0.00002848
Iteration 100/1000 | Loss: 0.00002848
Iteration 101/1000 | Loss: 0.00002848
Iteration 102/1000 | Loss: 0.00002848
Iteration 103/1000 | Loss: 0.00002848
Iteration 104/1000 | Loss: 0.00002848
Iteration 105/1000 | Loss: 0.00002848
Iteration 106/1000 | Loss: 0.00002848
Iteration 107/1000 | Loss: 0.00002848
Iteration 108/1000 | Loss: 0.00002848
Iteration 109/1000 | Loss: 0.00002848
Iteration 110/1000 | Loss: 0.00002848
Iteration 111/1000 | Loss: 0.00002848
Iteration 112/1000 | Loss: 0.00002848
Iteration 113/1000 | Loss: 0.00002848
Iteration 114/1000 | Loss: 0.00002848
Iteration 115/1000 | Loss: 0.00002848
Iteration 116/1000 | Loss: 0.00002848
Iteration 117/1000 | Loss: 0.00002848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.847874020517338e-05, 2.847874020517338e-05, 2.847874020517338e-05, 2.847874020517338e-05, 2.847874020517338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.847874020517338e-05

Optimization complete. Final v2v error: 4.692594051361084 mm

Highest mean error: 4.915953159332275 mm for frame 52

Lowest mean error: 4.469115257263184 mm for frame 97

Saving results

Total time: 33.9926643371582
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01200544
Iteration 2/25 | Loss: 0.00205552
Iteration 3/25 | Loss: 0.00202695
Iteration 4/25 | Loss: 0.00178203
Iteration 5/25 | Loss: 0.00178441
Iteration 6/25 | Loss: 0.00177389
Iteration 7/25 | Loss: 0.00175823
Iteration 8/25 | Loss: 0.00175251
Iteration 9/25 | Loss: 0.00174764
Iteration 10/25 | Loss: 0.00174312
Iteration 11/25 | Loss: 0.00174160
Iteration 12/25 | Loss: 0.00174120
Iteration 13/25 | Loss: 0.00174065
Iteration 14/25 | Loss: 0.00174046
Iteration 15/25 | Loss: 0.00174043
Iteration 16/25 | Loss: 0.00174042
Iteration 17/25 | Loss: 0.00174042
Iteration 18/25 | Loss: 0.00174042
Iteration 19/25 | Loss: 0.00174042
Iteration 20/25 | Loss: 0.00174041
Iteration 21/25 | Loss: 0.00174041
Iteration 22/25 | Loss: 0.00174038
Iteration 23/25 | Loss: 0.00174038
Iteration 24/25 | Loss: 0.00174038
Iteration 25/25 | Loss: 0.00174038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.26647425
Iteration 2/25 | Loss: 0.00289608
Iteration 3/25 | Loss: 0.00289608
Iteration 4/25 | Loss: 0.00289608
Iteration 5/25 | Loss: 0.00289608
Iteration 6/25 | Loss: 0.00289608
Iteration 7/25 | Loss: 0.00289608
Iteration 8/25 | Loss: 0.00289608
Iteration 9/25 | Loss: 0.00289608
Iteration 10/25 | Loss: 0.00289608
Iteration 11/25 | Loss: 0.00289608
Iteration 12/25 | Loss: 0.00289608
Iteration 13/25 | Loss: 0.00289608
Iteration 14/25 | Loss: 0.00289608
Iteration 15/25 | Loss: 0.00289608
Iteration 16/25 | Loss: 0.00289608
Iteration 17/25 | Loss: 0.00289608
Iteration 18/25 | Loss: 0.00289608
Iteration 19/25 | Loss: 0.00289608
Iteration 20/25 | Loss: 0.00289608
Iteration 21/25 | Loss: 0.00289608
Iteration 22/25 | Loss: 0.00289608
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0028960760682821274, 0.0028960760682821274, 0.0028960760682821274, 0.0028960760682821274, 0.0028960760682821274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028960760682821274

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00289608
Iteration 2/1000 | Loss: 0.00005277
Iteration 3/1000 | Loss: 0.00025649
Iteration 4/1000 | Loss: 0.00004024
Iteration 5/1000 | Loss: 0.00003517
Iteration 6/1000 | Loss: 0.00003235
Iteration 7/1000 | Loss: 0.00003075
Iteration 8/1000 | Loss: 0.00002992
Iteration 9/1000 | Loss: 0.00002951
Iteration 10/1000 | Loss: 0.00002918
Iteration 11/1000 | Loss: 0.00002891
Iteration 12/1000 | Loss: 0.00002876
Iteration 13/1000 | Loss: 0.00002859
Iteration 14/1000 | Loss: 0.00002856
Iteration 15/1000 | Loss: 0.00002852
Iteration 16/1000 | Loss: 0.00002851
Iteration 17/1000 | Loss: 0.00002850
Iteration 18/1000 | Loss: 0.00002845
Iteration 19/1000 | Loss: 0.00002843
Iteration 20/1000 | Loss: 0.00002843
Iteration 21/1000 | Loss: 0.00002843
Iteration 22/1000 | Loss: 0.00002843
Iteration 23/1000 | Loss: 0.00002843
Iteration 24/1000 | Loss: 0.00002843
Iteration 25/1000 | Loss: 0.00002842
Iteration 26/1000 | Loss: 0.00002842
Iteration 27/1000 | Loss: 0.00002842
Iteration 28/1000 | Loss: 0.00002842
Iteration 29/1000 | Loss: 0.00002842
Iteration 30/1000 | Loss: 0.00002841
Iteration 31/1000 | Loss: 0.00002841
Iteration 32/1000 | Loss: 0.00002840
Iteration 33/1000 | Loss: 0.00002839
Iteration 34/1000 | Loss: 0.00002839
Iteration 35/1000 | Loss: 0.00002839
Iteration 36/1000 | Loss: 0.00002838
Iteration 37/1000 | Loss: 0.00002838
Iteration 38/1000 | Loss: 0.00002837
Iteration 39/1000 | Loss: 0.00002837
Iteration 40/1000 | Loss: 0.00002836
Iteration 41/1000 | Loss: 0.00002836
Iteration 42/1000 | Loss: 0.00002835
Iteration 43/1000 | Loss: 0.00002835
Iteration 44/1000 | Loss: 0.00002835
Iteration 45/1000 | Loss: 0.00002835
Iteration 46/1000 | Loss: 0.00002835
Iteration 47/1000 | Loss: 0.00002835
Iteration 48/1000 | Loss: 0.00002834
Iteration 49/1000 | Loss: 0.00002834
Iteration 50/1000 | Loss: 0.00002834
Iteration 51/1000 | Loss: 0.00002833
Iteration 52/1000 | Loss: 0.00002833
Iteration 53/1000 | Loss: 0.00002833
Iteration 54/1000 | Loss: 0.00002833
Iteration 55/1000 | Loss: 0.00002833
Iteration 56/1000 | Loss: 0.00002833
Iteration 57/1000 | Loss: 0.00002833
Iteration 58/1000 | Loss: 0.00002832
Iteration 59/1000 | Loss: 0.00002832
Iteration 60/1000 | Loss: 0.00002832
Iteration 61/1000 | Loss: 0.00002832
Iteration 62/1000 | Loss: 0.00002832
Iteration 63/1000 | Loss: 0.00002832
Iteration 64/1000 | Loss: 0.00002831
Iteration 65/1000 | Loss: 0.00002831
Iteration 66/1000 | Loss: 0.00002831
Iteration 67/1000 | Loss: 0.00002831
Iteration 68/1000 | Loss: 0.00002831
Iteration 69/1000 | Loss: 0.00002831
Iteration 70/1000 | Loss: 0.00002830
Iteration 71/1000 | Loss: 0.00002830
Iteration 72/1000 | Loss: 0.00002830
Iteration 73/1000 | Loss: 0.00002830
Iteration 74/1000 | Loss: 0.00002830
Iteration 75/1000 | Loss: 0.00002830
Iteration 76/1000 | Loss: 0.00002830
Iteration 77/1000 | Loss: 0.00002830
Iteration 78/1000 | Loss: 0.00002829
Iteration 79/1000 | Loss: 0.00002829
Iteration 80/1000 | Loss: 0.00002829
Iteration 81/1000 | Loss: 0.00002829
Iteration 82/1000 | Loss: 0.00002829
Iteration 83/1000 | Loss: 0.00002829
Iteration 84/1000 | Loss: 0.00002828
Iteration 85/1000 | Loss: 0.00002828
Iteration 86/1000 | Loss: 0.00002828
Iteration 87/1000 | Loss: 0.00002828
Iteration 88/1000 | Loss: 0.00002828
Iteration 89/1000 | Loss: 0.00002828
Iteration 90/1000 | Loss: 0.00002827
Iteration 91/1000 | Loss: 0.00002827
Iteration 92/1000 | Loss: 0.00002827
Iteration 93/1000 | Loss: 0.00002827
Iteration 94/1000 | Loss: 0.00002827
Iteration 95/1000 | Loss: 0.00002827
Iteration 96/1000 | Loss: 0.00002826
Iteration 97/1000 | Loss: 0.00002826
Iteration 98/1000 | Loss: 0.00002826
Iteration 99/1000 | Loss: 0.00002826
Iteration 100/1000 | Loss: 0.00002826
Iteration 101/1000 | Loss: 0.00002825
Iteration 102/1000 | Loss: 0.00002825
Iteration 103/1000 | Loss: 0.00002825
Iteration 104/1000 | Loss: 0.00002825
Iteration 105/1000 | Loss: 0.00002825
Iteration 106/1000 | Loss: 0.00002825
Iteration 107/1000 | Loss: 0.00002825
Iteration 108/1000 | Loss: 0.00002825
Iteration 109/1000 | Loss: 0.00002825
Iteration 110/1000 | Loss: 0.00002825
Iteration 111/1000 | Loss: 0.00002825
Iteration 112/1000 | Loss: 0.00002825
Iteration 113/1000 | Loss: 0.00002825
Iteration 114/1000 | Loss: 0.00002824
Iteration 115/1000 | Loss: 0.00002824
Iteration 116/1000 | Loss: 0.00002824
Iteration 117/1000 | Loss: 0.00002824
Iteration 118/1000 | Loss: 0.00002824
Iteration 119/1000 | Loss: 0.00002824
Iteration 120/1000 | Loss: 0.00002824
Iteration 121/1000 | Loss: 0.00002824
Iteration 122/1000 | Loss: 0.00002824
Iteration 123/1000 | Loss: 0.00002824
Iteration 124/1000 | Loss: 0.00002824
Iteration 125/1000 | Loss: 0.00002824
Iteration 126/1000 | Loss: 0.00002824
Iteration 127/1000 | Loss: 0.00002823
Iteration 128/1000 | Loss: 0.00002823
Iteration 129/1000 | Loss: 0.00002823
Iteration 130/1000 | Loss: 0.00002823
Iteration 131/1000 | Loss: 0.00002823
Iteration 132/1000 | Loss: 0.00002823
Iteration 133/1000 | Loss: 0.00002823
Iteration 134/1000 | Loss: 0.00002823
Iteration 135/1000 | Loss: 0.00002823
Iteration 136/1000 | Loss: 0.00002823
Iteration 137/1000 | Loss: 0.00002823
Iteration 138/1000 | Loss: 0.00002823
Iteration 139/1000 | Loss: 0.00002823
Iteration 140/1000 | Loss: 0.00002823
Iteration 141/1000 | Loss: 0.00002823
Iteration 142/1000 | Loss: 0.00002823
Iteration 143/1000 | Loss: 0.00002823
Iteration 144/1000 | Loss: 0.00002823
Iteration 145/1000 | Loss: 0.00002822
Iteration 146/1000 | Loss: 0.00002822
Iteration 147/1000 | Loss: 0.00002822
Iteration 148/1000 | Loss: 0.00002822
Iteration 149/1000 | Loss: 0.00002822
Iteration 150/1000 | Loss: 0.00002822
Iteration 151/1000 | Loss: 0.00002822
Iteration 152/1000 | Loss: 0.00002822
Iteration 153/1000 | Loss: 0.00002822
Iteration 154/1000 | Loss: 0.00002822
Iteration 155/1000 | Loss: 0.00002822
Iteration 156/1000 | Loss: 0.00002822
Iteration 157/1000 | Loss: 0.00002822
Iteration 158/1000 | Loss: 0.00002822
Iteration 159/1000 | Loss: 0.00002822
Iteration 160/1000 | Loss: 0.00002822
Iteration 161/1000 | Loss: 0.00002822
Iteration 162/1000 | Loss: 0.00002822
Iteration 163/1000 | Loss: 0.00002821
Iteration 164/1000 | Loss: 0.00002821
Iteration 165/1000 | Loss: 0.00002821
Iteration 166/1000 | Loss: 0.00002821
Iteration 167/1000 | Loss: 0.00002821
Iteration 168/1000 | Loss: 0.00002821
Iteration 169/1000 | Loss: 0.00002821
Iteration 170/1000 | Loss: 0.00002821
Iteration 171/1000 | Loss: 0.00002821
Iteration 172/1000 | Loss: 0.00002821
Iteration 173/1000 | Loss: 0.00002821
Iteration 174/1000 | Loss: 0.00002821
Iteration 175/1000 | Loss: 0.00002821
Iteration 176/1000 | Loss: 0.00002821
Iteration 177/1000 | Loss: 0.00002821
Iteration 178/1000 | Loss: 0.00002821
Iteration 179/1000 | Loss: 0.00002821
Iteration 180/1000 | Loss: 0.00002821
Iteration 181/1000 | Loss: 0.00002821
Iteration 182/1000 | Loss: 0.00002821
Iteration 183/1000 | Loss: 0.00002821
Iteration 184/1000 | Loss: 0.00002821
Iteration 185/1000 | Loss: 0.00002821
Iteration 186/1000 | Loss: 0.00002821
Iteration 187/1000 | Loss: 0.00002821
Iteration 188/1000 | Loss: 0.00002821
Iteration 189/1000 | Loss: 0.00002821
Iteration 190/1000 | Loss: 0.00002821
Iteration 191/1000 | Loss: 0.00002821
Iteration 192/1000 | Loss: 0.00002821
Iteration 193/1000 | Loss: 0.00002821
Iteration 194/1000 | Loss: 0.00002821
Iteration 195/1000 | Loss: 0.00002821
Iteration 196/1000 | Loss: 0.00002821
Iteration 197/1000 | Loss: 0.00002821
Iteration 198/1000 | Loss: 0.00002821
Iteration 199/1000 | Loss: 0.00002821
Iteration 200/1000 | Loss: 0.00002821
Iteration 201/1000 | Loss: 0.00002821
Iteration 202/1000 | Loss: 0.00002821
Iteration 203/1000 | Loss: 0.00002821
Iteration 204/1000 | Loss: 0.00002821
Iteration 205/1000 | Loss: 0.00002821
Iteration 206/1000 | Loss: 0.00002821
Iteration 207/1000 | Loss: 0.00002821
Iteration 208/1000 | Loss: 0.00002821
Iteration 209/1000 | Loss: 0.00002821
Iteration 210/1000 | Loss: 0.00002821
Iteration 211/1000 | Loss: 0.00002821
Iteration 212/1000 | Loss: 0.00002821
Iteration 213/1000 | Loss: 0.00002821
Iteration 214/1000 | Loss: 0.00002821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [2.8211252356413752e-05, 2.8211252356413752e-05, 2.8211252356413752e-05, 2.8211252356413752e-05, 2.8211252356413752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8211252356413752e-05

Optimization complete. Final v2v error: 4.696028232574463 mm

Highest mean error: 5.164342403411865 mm for frame 8

Lowest mean error: 4.375524044036865 mm for frame 74

Saving results

Total time: 54.472161293029785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886603
Iteration 2/25 | Loss: 0.00216342
Iteration 3/25 | Loss: 0.00179431
Iteration 4/25 | Loss: 0.00171697
Iteration 5/25 | Loss: 0.00169581
Iteration 6/25 | Loss: 0.00170557
Iteration 7/25 | Loss: 0.00170991
Iteration 8/25 | Loss: 0.00170100
Iteration 9/25 | Loss: 0.00169603
Iteration 10/25 | Loss: 0.00168855
Iteration 11/25 | Loss: 0.00168895
Iteration 12/25 | Loss: 0.00168500
Iteration 13/25 | Loss: 0.00167850
Iteration 14/25 | Loss: 0.00167963
Iteration 15/25 | Loss: 0.00167897
Iteration 16/25 | Loss: 0.00167598
Iteration 17/25 | Loss: 0.00167370
Iteration 18/25 | Loss: 0.00167708
Iteration 19/25 | Loss: 0.00167820
Iteration 20/25 | Loss: 0.00167507
Iteration 21/25 | Loss: 0.00168076
Iteration 22/25 | Loss: 0.00167826
Iteration 23/25 | Loss: 0.00167658
Iteration 24/25 | Loss: 0.00167546
Iteration 25/25 | Loss: 0.00167359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.43987846
Iteration 2/25 | Loss: 0.00263411
Iteration 3/25 | Loss: 0.00263410
Iteration 4/25 | Loss: 0.00263410
Iteration 5/25 | Loss: 0.00263410
Iteration 6/25 | Loss: 0.00263410
Iteration 7/25 | Loss: 0.00263410
Iteration 8/25 | Loss: 0.00263410
Iteration 9/25 | Loss: 0.00263410
Iteration 10/25 | Loss: 0.00263410
Iteration 11/25 | Loss: 0.00263410
Iteration 12/25 | Loss: 0.00263410
Iteration 13/25 | Loss: 0.00263410
Iteration 14/25 | Loss: 0.00263410
Iteration 15/25 | Loss: 0.00263410
Iteration 16/25 | Loss: 0.00263410
Iteration 17/25 | Loss: 0.00263410
Iteration 18/25 | Loss: 0.00263410
Iteration 19/25 | Loss: 0.00263410
Iteration 20/25 | Loss: 0.00263410
Iteration 21/25 | Loss: 0.00263410
Iteration 22/25 | Loss: 0.00263410
Iteration 23/25 | Loss: 0.00263410
Iteration 24/25 | Loss: 0.00263410
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00263409991748631, 0.00263409991748631, 0.00263409991748631, 0.00263409991748631, 0.00263409991748631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00263409991748631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263410
Iteration 2/1000 | Loss: 0.00007454
Iteration 3/1000 | Loss: 0.00005190
Iteration 4/1000 | Loss: 0.00007136
Iteration 5/1000 | Loss: 0.00024788
Iteration 6/1000 | Loss: 0.00005812
Iteration 7/1000 | Loss: 0.00006415
Iteration 8/1000 | Loss: 0.00005031
Iteration 9/1000 | Loss: 0.00004976
Iteration 10/1000 | Loss: 0.00005543
Iteration 11/1000 | Loss: 0.00005838
Iteration 12/1000 | Loss: 0.00005429
Iteration 13/1000 | Loss: 0.00005211
Iteration 14/1000 | Loss: 0.00006866
Iteration 15/1000 | Loss: 0.00005129
Iteration 16/1000 | Loss: 0.00004989
Iteration 17/1000 | Loss: 0.00007111
Iteration 18/1000 | Loss: 0.00003653
Iteration 19/1000 | Loss: 0.00003396
Iteration 20/1000 | Loss: 0.00003380
Iteration 21/1000 | Loss: 0.00003263
Iteration 22/1000 | Loss: 0.00003231
Iteration 23/1000 | Loss: 0.00003260
Iteration 24/1000 | Loss: 0.00003186
Iteration 25/1000 | Loss: 0.00003185
Iteration 26/1000 | Loss: 0.00003183
Iteration 27/1000 | Loss: 0.00003183
Iteration 28/1000 | Loss: 0.00003182
Iteration 29/1000 | Loss: 0.00023100
Iteration 30/1000 | Loss: 0.00003706
Iteration 31/1000 | Loss: 0.00003607
Iteration 32/1000 | Loss: 0.00003391
Iteration 33/1000 | Loss: 0.00003426
Iteration 34/1000 | Loss: 0.00003285
Iteration 35/1000 | Loss: 0.00003237
Iteration 36/1000 | Loss: 0.00003200
Iteration 37/1000 | Loss: 0.00003181
Iteration 38/1000 | Loss: 0.00003172
Iteration 39/1000 | Loss: 0.00003159
Iteration 40/1000 | Loss: 0.00003157
Iteration 41/1000 | Loss: 0.00003155
Iteration 42/1000 | Loss: 0.00003155
Iteration 43/1000 | Loss: 0.00003154
Iteration 44/1000 | Loss: 0.00003153
Iteration 45/1000 | Loss: 0.00003220
Iteration 46/1000 | Loss: 0.00003148
Iteration 47/1000 | Loss: 0.00003203
Iteration 48/1000 | Loss: 0.00003138
Iteration 49/1000 | Loss: 0.00003138
Iteration 50/1000 | Loss: 0.00003137
Iteration 51/1000 | Loss: 0.00003137
Iteration 52/1000 | Loss: 0.00003137
Iteration 53/1000 | Loss: 0.00003137
Iteration 54/1000 | Loss: 0.00003136
Iteration 55/1000 | Loss: 0.00003140
Iteration 56/1000 | Loss: 0.00022287
Iteration 57/1000 | Loss: 0.00003755
Iteration 58/1000 | Loss: 0.00004184
Iteration 59/1000 | Loss: 0.00003144
Iteration 60/1000 | Loss: 0.00003095
Iteration 61/1000 | Loss: 0.00003071
Iteration 62/1000 | Loss: 0.00003066
Iteration 63/1000 | Loss: 0.00003192
Iteration 64/1000 | Loss: 0.00003055
Iteration 65/1000 | Loss: 0.00003054
Iteration 66/1000 | Loss: 0.00003054
Iteration 67/1000 | Loss: 0.00003054
Iteration 68/1000 | Loss: 0.00003054
Iteration 69/1000 | Loss: 0.00003053
Iteration 70/1000 | Loss: 0.00003193
Iteration 71/1000 | Loss: 0.00023658
Iteration 72/1000 | Loss: 0.00004420
Iteration 73/1000 | Loss: 0.00003615
Iteration 74/1000 | Loss: 0.00003408
Iteration 75/1000 | Loss: 0.00003619
Iteration 76/1000 | Loss: 0.00003239
Iteration 77/1000 | Loss: 0.00003170
Iteration 78/1000 | Loss: 0.00003168
Iteration 79/1000 | Loss: 0.00003112
Iteration 80/1000 | Loss: 0.00003112
Iteration 81/1000 | Loss: 0.00003112
Iteration 82/1000 | Loss: 0.00003112
Iteration 83/1000 | Loss: 0.00003112
Iteration 84/1000 | Loss: 0.00003112
Iteration 85/1000 | Loss: 0.00003112
Iteration 86/1000 | Loss: 0.00003111
Iteration 87/1000 | Loss: 0.00003111
Iteration 88/1000 | Loss: 0.00003129
Iteration 89/1000 | Loss: 0.00003105
Iteration 90/1000 | Loss: 0.00003105
Iteration 91/1000 | Loss: 0.00003105
Iteration 92/1000 | Loss: 0.00003105
Iteration 93/1000 | Loss: 0.00003104
Iteration 94/1000 | Loss: 0.00003104
Iteration 95/1000 | Loss: 0.00003104
Iteration 96/1000 | Loss: 0.00003104
Iteration 97/1000 | Loss: 0.00003104
Iteration 98/1000 | Loss: 0.00003104
Iteration 99/1000 | Loss: 0.00003104
Iteration 100/1000 | Loss: 0.00003104
Iteration 101/1000 | Loss: 0.00003104
Iteration 102/1000 | Loss: 0.00003104
Iteration 103/1000 | Loss: 0.00003104
Iteration 104/1000 | Loss: 0.00003104
Iteration 105/1000 | Loss: 0.00003104
Iteration 106/1000 | Loss: 0.00003104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [3.103926792391576e-05, 3.103926792391576e-05, 3.103926792391576e-05, 3.103926792391576e-05, 3.103926792391576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.103926792391576e-05

Optimization complete. Final v2v error: 4.690323829650879 mm

Highest mean error: 10.328248023986816 mm for frame 147

Lowest mean error: 4.078150272369385 mm for frame 204

Saving results

Total time: 140.1454141139984
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01128502
Iteration 2/25 | Loss: 0.00478041
Iteration 3/25 | Loss: 0.00304811
Iteration 4/25 | Loss: 0.00277877
Iteration 5/25 | Loss: 0.00265980
Iteration 6/25 | Loss: 0.00253801
Iteration 7/25 | Loss: 0.00250292
Iteration 8/25 | Loss: 0.00240358
Iteration 9/25 | Loss: 0.00237416
Iteration 10/25 | Loss: 0.00243897
Iteration 11/25 | Loss: 0.00239270
Iteration 12/25 | Loss: 0.00232328
Iteration 13/25 | Loss: 0.00225765
Iteration 14/25 | Loss: 0.00223798
Iteration 15/25 | Loss: 0.00222429
Iteration 16/25 | Loss: 0.00221992
Iteration 17/25 | Loss: 0.00221991
Iteration 18/25 | Loss: 0.00221442
Iteration 19/25 | Loss: 0.00222371
Iteration 20/25 | Loss: 0.00222046
Iteration 21/25 | Loss: 0.00221712
Iteration 22/25 | Loss: 0.00220874
Iteration 23/25 | Loss: 0.00220584
Iteration 24/25 | Loss: 0.00220478
Iteration 25/25 | Loss: 0.00220321

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56676173
Iteration 2/25 | Loss: 0.00811374
Iteration 3/25 | Loss: 0.00772790
Iteration 4/25 | Loss: 0.00772790
Iteration 5/25 | Loss: 0.00772790
Iteration 6/25 | Loss: 0.00772790
Iteration 7/25 | Loss: 0.00772790
Iteration 8/25 | Loss: 0.00772790
Iteration 9/25 | Loss: 0.00772790
Iteration 10/25 | Loss: 0.00772789
Iteration 11/25 | Loss: 0.00772790
Iteration 12/25 | Loss: 0.00772790
Iteration 13/25 | Loss: 0.00772790
Iteration 14/25 | Loss: 0.00772789
Iteration 15/25 | Loss: 0.00772790
Iteration 16/25 | Loss: 0.00772789
Iteration 17/25 | Loss: 0.00772790
Iteration 18/25 | Loss: 0.00772790
Iteration 19/25 | Loss: 0.00772790
Iteration 20/25 | Loss: 0.00772790
Iteration 21/25 | Loss: 0.00772790
Iteration 22/25 | Loss: 0.00772790
Iteration 23/25 | Loss: 0.00772790
Iteration 24/25 | Loss: 0.00772789
Iteration 25/25 | Loss: 0.00772790

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00772790
Iteration 2/1000 | Loss: 0.00161919
Iteration 3/1000 | Loss: 0.00102701
Iteration 4/1000 | Loss: 0.00108363
Iteration 5/1000 | Loss: 0.00106312
Iteration 6/1000 | Loss: 0.00904313
Iteration 7/1000 | Loss: 0.00183604
Iteration 8/1000 | Loss: 0.00210929
Iteration 9/1000 | Loss: 0.00238462
Iteration 10/1000 | Loss: 0.00202730
Iteration 11/1000 | Loss: 0.00266108
Iteration 12/1000 | Loss: 0.00074385
Iteration 13/1000 | Loss: 0.00069031
Iteration 14/1000 | Loss: 0.00043081
Iteration 15/1000 | Loss: 0.00071280
Iteration 16/1000 | Loss: 0.00150925
Iteration 17/1000 | Loss: 0.00071059
Iteration 18/1000 | Loss: 0.00066663
Iteration 19/1000 | Loss: 0.00054633
Iteration 20/1000 | Loss: 0.00039747
Iteration 21/1000 | Loss: 0.00057481
Iteration 22/1000 | Loss: 0.00053661
Iteration 23/1000 | Loss: 0.00179731
Iteration 24/1000 | Loss: 0.00292128
Iteration 25/1000 | Loss: 0.00183044
Iteration 26/1000 | Loss: 0.00096413
Iteration 27/1000 | Loss: 0.00050539
Iteration 28/1000 | Loss: 0.00038703
Iteration 29/1000 | Loss: 0.00036096
Iteration 30/1000 | Loss: 0.00034416
Iteration 31/1000 | Loss: 0.00033176
Iteration 32/1000 | Loss: 0.00079053
Iteration 33/1000 | Loss: 0.00032866
Iteration 34/1000 | Loss: 0.00037246
Iteration 35/1000 | Loss: 0.00031892
Iteration 36/1000 | Loss: 0.00031435
Iteration 37/1000 | Loss: 0.00031077
Iteration 38/1000 | Loss: 0.00030919
Iteration 39/1000 | Loss: 0.00064949
Iteration 40/1000 | Loss: 0.00031271
Iteration 41/1000 | Loss: 0.00030715
Iteration 42/1000 | Loss: 0.00030462
Iteration 43/1000 | Loss: 0.00080845
Iteration 44/1000 | Loss: 0.00031640
Iteration 45/1000 | Loss: 0.00046570
Iteration 46/1000 | Loss: 0.00033980
Iteration 47/1000 | Loss: 0.00030167
Iteration 48/1000 | Loss: 0.00029925
Iteration 49/1000 | Loss: 0.00029812
Iteration 50/1000 | Loss: 0.00029742
Iteration 51/1000 | Loss: 0.00029702
Iteration 52/1000 | Loss: 0.00029660
Iteration 53/1000 | Loss: 0.00029635
Iteration 54/1000 | Loss: 0.00029621
Iteration 55/1000 | Loss: 0.00029604
Iteration 56/1000 | Loss: 0.00029591
Iteration 57/1000 | Loss: 0.00029590
Iteration 58/1000 | Loss: 0.00029588
Iteration 59/1000 | Loss: 0.00029588
Iteration 60/1000 | Loss: 0.00029587
Iteration 61/1000 | Loss: 0.00029587
Iteration 62/1000 | Loss: 0.00029587
Iteration 63/1000 | Loss: 0.00029586
Iteration 64/1000 | Loss: 0.00029586
Iteration 65/1000 | Loss: 0.00029585
Iteration 66/1000 | Loss: 0.00029585
Iteration 67/1000 | Loss: 0.00029584
Iteration 68/1000 | Loss: 0.00029583
Iteration 69/1000 | Loss: 0.00029583
Iteration 70/1000 | Loss: 0.00029583
Iteration 71/1000 | Loss: 0.00029582
Iteration 72/1000 | Loss: 0.00029582
Iteration 73/1000 | Loss: 0.00029582
Iteration 74/1000 | Loss: 0.00029582
Iteration 75/1000 | Loss: 0.00029582
Iteration 76/1000 | Loss: 0.00029582
Iteration 77/1000 | Loss: 0.00029582
Iteration 78/1000 | Loss: 0.00029582
Iteration 79/1000 | Loss: 0.00029581
Iteration 80/1000 | Loss: 0.00029580
Iteration 81/1000 | Loss: 0.00029580
Iteration 82/1000 | Loss: 0.00029580
Iteration 83/1000 | Loss: 0.00029579
Iteration 84/1000 | Loss: 0.00029579
Iteration 85/1000 | Loss: 0.00029579
Iteration 86/1000 | Loss: 0.00029579
Iteration 87/1000 | Loss: 0.00029579
Iteration 88/1000 | Loss: 0.00029579
Iteration 89/1000 | Loss: 0.00029579
Iteration 90/1000 | Loss: 0.00029579
Iteration 91/1000 | Loss: 0.00029579
Iteration 92/1000 | Loss: 0.00029578
Iteration 93/1000 | Loss: 0.00029577
Iteration 94/1000 | Loss: 0.00029577
Iteration 95/1000 | Loss: 0.00029576
Iteration 96/1000 | Loss: 0.00029575
Iteration 97/1000 | Loss: 0.00029575
Iteration 98/1000 | Loss: 0.00029575
Iteration 99/1000 | Loss: 0.00029575
Iteration 100/1000 | Loss: 0.00029575
Iteration 101/1000 | Loss: 0.00029575
Iteration 102/1000 | Loss: 0.00029575
Iteration 103/1000 | Loss: 0.00029575
Iteration 104/1000 | Loss: 0.00029574
Iteration 105/1000 | Loss: 0.00029574
Iteration 106/1000 | Loss: 0.00029572
Iteration 107/1000 | Loss: 0.00029572
Iteration 108/1000 | Loss: 0.00029572
Iteration 109/1000 | Loss: 0.00029572
Iteration 110/1000 | Loss: 0.00029572
Iteration 111/1000 | Loss: 0.00029572
Iteration 112/1000 | Loss: 0.00029572
Iteration 113/1000 | Loss: 0.00029572
Iteration 114/1000 | Loss: 0.00029572
Iteration 115/1000 | Loss: 0.00029570
Iteration 116/1000 | Loss: 0.00029569
Iteration 117/1000 | Loss: 0.00029569
Iteration 118/1000 | Loss: 0.00029569
Iteration 119/1000 | Loss: 0.00029569
Iteration 120/1000 | Loss: 0.00029569
Iteration 121/1000 | Loss: 0.00029569
Iteration 122/1000 | Loss: 0.00029569
Iteration 123/1000 | Loss: 0.00029569
Iteration 124/1000 | Loss: 0.00029569
Iteration 125/1000 | Loss: 0.00029569
Iteration 126/1000 | Loss: 0.00029569
Iteration 127/1000 | Loss: 0.00029568
Iteration 128/1000 | Loss: 0.00029568
Iteration 129/1000 | Loss: 0.00029568
Iteration 130/1000 | Loss: 0.00029568
Iteration 131/1000 | Loss: 0.00029567
Iteration 132/1000 | Loss: 0.00029567
Iteration 133/1000 | Loss: 0.00029567
Iteration 134/1000 | Loss: 0.00029567
Iteration 135/1000 | Loss: 0.00029566
Iteration 136/1000 | Loss: 0.00029566
Iteration 137/1000 | Loss: 0.00029565
Iteration 138/1000 | Loss: 0.00029565
Iteration 139/1000 | Loss: 0.00029565
Iteration 140/1000 | Loss: 0.00029564
Iteration 141/1000 | Loss: 0.00029563
Iteration 142/1000 | Loss: 0.00029563
Iteration 143/1000 | Loss: 0.00029562
Iteration 144/1000 | Loss: 0.00029562
Iteration 145/1000 | Loss: 0.00029562
Iteration 146/1000 | Loss: 0.00029562
Iteration 147/1000 | Loss: 0.00029561
Iteration 148/1000 | Loss: 0.00029561
Iteration 149/1000 | Loss: 0.00029561
Iteration 150/1000 | Loss: 0.00029561
Iteration 151/1000 | Loss: 0.00029561
Iteration 152/1000 | Loss: 0.00029561
Iteration 153/1000 | Loss: 0.00029561
Iteration 154/1000 | Loss: 0.00029561
Iteration 155/1000 | Loss: 0.00029561
Iteration 156/1000 | Loss: 0.00029560
Iteration 157/1000 | Loss: 0.00029560
Iteration 158/1000 | Loss: 0.00029560
Iteration 159/1000 | Loss: 0.00029560
Iteration 160/1000 | Loss: 0.00029560
Iteration 161/1000 | Loss: 0.00029560
Iteration 162/1000 | Loss: 0.00029560
Iteration 163/1000 | Loss: 0.00029560
Iteration 164/1000 | Loss: 0.00029560
Iteration 165/1000 | Loss: 0.00029560
Iteration 166/1000 | Loss: 0.00029559
Iteration 167/1000 | Loss: 0.00029559
Iteration 168/1000 | Loss: 0.00029559
Iteration 169/1000 | Loss: 0.00029559
Iteration 170/1000 | Loss: 0.00029559
Iteration 171/1000 | Loss: 0.00029559
Iteration 172/1000 | Loss: 0.00029559
Iteration 173/1000 | Loss: 0.00029559
Iteration 174/1000 | Loss: 0.00029559
Iteration 175/1000 | Loss: 0.00029559
Iteration 176/1000 | Loss: 0.00029559
Iteration 177/1000 | Loss: 0.00029558
Iteration 178/1000 | Loss: 0.00029558
Iteration 179/1000 | Loss: 0.00029558
Iteration 180/1000 | Loss: 0.00029558
Iteration 181/1000 | Loss: 0.00029558
Iteration 182/1000 | Loss: 0.00029558
Iteration 183/1000 | Loss: 0.00029558
Iteration 184/1000 | Loss: 0.00029558
Iteration 185/1000 | Loss: 0.00029558
Iteration 186/1000 | Loss: 0.00029558
Iteration 187/1000 | Loss: 0.00029558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [0.0002955822565127164, 0.0002955822565127164, 0.0002955822565127164, 0.0002955822565127164, 0.0002955822565127164]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002955822565127164

Optimization complete. Final v2v error: 9.214641571044922 mm

Highest mean error: 15.07303237915039 mm for frame 37

Lowest mean error: 5.730404853820801 mm for frame 2

Saving results

Total time: 130.47986435890198
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968546
Iteration 2/25 | Loss: 0.00180587
Iteration 3/25 | Loss: 0.00173145
Iteration 4/25 | Loss: 0.00171102
Iteration 5/25 | Loss: 0.00170413
Iteration 6/25 | Loss: 0.00170350
Iteration 7/25 | Loss: 0.00170350
Iteration 8/25 | Loss: 0.00170350
Iteration 9/25 | Loss: 0.00170350
Iteration 10/25 | Loss: 0.00170350
Iteration 11/25 | Loss: 0.00170350
Iteration 12/25 | Loss: 0.00170350
Iteration 13/25 | Loss: 0.00170350
Iteration 14/25 | Loss: 0.00170350
Iteration 15/25 | Loss: 0.00170350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0017035047058016062, 0.0017035047058016062, 0.0017035047058016062, 0.0017035047058016062, 0.0017035047058016062]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017035047058016062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.29296446
Iteration 2/25 | Loss: 0.00266941
Iteration 3/25 | Loss: 0.00266938
Iteration 4/25 | Loss: 0.00266938
Iteration 5/25 | Loss: 0.00266937
Iteration 6/25 | Loss: 0.00266937
Iteration 7/25 | Loss: 0.00266937
Iteration 8/25 | Loss: 0.00266937
Iteration 9/25 | Loss: 0.00266937
Iteration 10/25 | Loss: 0.00266937
Iteration 11/25 | Loss: 0.00266937
Iteration 12/25 | Loss: 0.00266937
Iteration 13/25 | Loss: 0.00266937
Iteration 14/25 | Loss: 0.00266937
Iteration 15/25 | Loss: 0.00266937
Iteration 16/25 | Loss: 0.00266937
Iteration 17/25 | Loss: 0.00266937
Iteration 18/25 | Loss: 0.00266937
Iteration 19/25 | Loss: 0.00266937
Iteration 20/25 | Loss: 0.00266937
Iteration 21/25 | Loss: 0.00266937
Iteration 22/25 | Loss: 0.00266937
Iteration 23/25 | Loss: 0.00266937
Iteration 24/25 | Loss: 0.00266937
Iteration 25/25 | Loss: 0.00266937

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266937
Iteration 2/1000 | Loss: 0.00006292
Iteration 3/1000 | Loss: 0.00004583
Iteration 4/1000 | Loss: 0.00004069
Iteration 5/1000 | Loss: 0.00003803
Iteration 6/1000 | Loss: 0.00003664
Iteration 7/1000 | Loss: 0.00003546
Iteration 8/1000 | Loss: 0.00003449
Iteration 9/1000 | Loss: 0.00003385
Iteration 10/1000 | Loss: 0.00003354
Iteration 11/1000 | Loss: 0.00003338
Iteration 12/1000 | Loss: 0.00003337
Iteration 13/1000 | Loss: 0.00003327
Iteration 14/1000 | Loss: 0.00003320
Iteration 15/1000 | Loss: 0.00003310
Iteration 16/1000 | Loss: 0.00003308
Iteration 17/1000 | Loss: 0.00003308
Iteration 18/1000 | Loss: 0.00003308
Iteration 19/1000 | Loss: 0.00003300
Iteration 20/1000 | Loss: 0.00003300
Iteration 21/1000 | Loss: 0.00003299
Iteration 22/1000 | Loss: 0.00003299
Iteration 23/1000 | Loss: 0.00003296
Iteration 24/1000 | Loss: 0.00003296
Iteration 25/1000 | Loss: 0.00003295
Iteration 26/1000 | Loss: 0.00003295
Iteration 27/1000 | Loss: 0.00003295
Iteration 28/1000 | Loss: 0.00003295
Iteration 29/1000 | Loss: 0.00003294
Iteration 30/1000 | Loss: 0.00003290
Iteration 31/1000 | Loss: 0.00003289
Iteration 32/1000 | Loss: 0.00003288
Iteration 33/1000 | Loss: 0.00003287
Iteration 34/1000 | Loss: 0.00003287
Iteration 35/1000 | Loss: 0.00003287
Iteration 36/1000 | Loss: 0.00003287
Iteration 37/1000 | Loss: 0.00003287
Iteration 38/1000 | Loss: 0.00003287
Iteration 39/1000 | Loss: 0.00003286
Iteration 40/1000 | Loss: 0.00003286
Iteration 41/1000 | Loss: 0.00003286
Iteration 42/1000 | Loss: 0.00003286
Iteration 43/1000 | Loss: 0.00003286
Iteration 44/1000 | Loss: 0.00003286
Iteration 45/1000 | Loss: 0.00003286
Iteration 46/1000 | Loss: 0.00003285
Iteration 47/1000 | Loss: 0.00003285
Iteration 48/1000 | Loss: 0.00003285
Iteration 49/1000 | Loss: 0.00003285
Iteration 50/1000 | Loss: 0.00003285
Iteration 51/1000 | Loss: 0.00003285
Iteration 52/1000 | Loss: 0.00003284
Iteration 53/1000 | Loss: 0.00003284
Iteration 54/1000 | Loss: 0.00003284
Iteration 55/1000 | Loss: 0.00003284
Iteration 56/1000 | Loss: 0.00003284
Iteration 57/1000 | Loss: 0.00003284
Iteration 58/1000 | Loss: 0.00003284
Iteration 59/1000 | Loss: 0.00003283
Iteration 60/1000 | Loss: 0.00003283
Iteration 61/1000 | Loss: 0.00003283
Iteration 62/1000 | Loss: 0.00003283
Iteration 63/1000 | Loss: 0.00003283
Iteration 64/1000 | Loss: 0.00003283
Iteration 65/1000 | Loss: 0.00003283
Iteration 66/1000 | Loss: 0.00003283
Iteration 67/1000 | Loss: 0.00003283
Iteration 68/1000 | Loss: 0.00003283
Iteration 69/1000 | Loss: 0.00003283
Iteration 70/1000 | Loss: 0.00003283
Iteration 71/1000 | Loss: 0.00003283
Iteration 72/1000 | Loss: 0.00003283
Iteration 73/1000 | Loss: 0.00003283
Iteration 74/1000 | Loss: 0.00003283
Iteration 75/1000 | Loss: 0.00003283
Iteration 76/1000 | Loss: 0.00003283
Iteration 77/1000 | Loss: 0.00003283
Iteration 78/1000 | Loss: 0.00003283
Iteration 79/1000 | Loss: 0.00003283
Iteration 80/1000 | Loss: 0.00003283
Iteration 81/1000 | Loss: 0.00003283
Iteration 82/1000 | Loss: 0.00003283
Iteration 83/1000 | Loss: 0.00003283
Iteration 84/1000 | Loss: 0.00003283
Iteration 85/1000 | Loss: 0.00003283
Iteration 86/1000 | Loss: 0.00003283
Iteration 87/1000 | Loss: 0.00003283
Iteration 88/1000 | Loss: 0.00003283
Iteration 89/1000 | Loss: 0.00003283
Iteration 90/1000 | Loss: 0.00003283
Iteration 91/1000 | Loss: 0.00003283
Iteration 92/1000 | Loss: 0.00003283
Iteration 93/1000 | Loss: 0.00003283
Iteration 94/1000 | Loss: 0.00003283
Iteration 95/1000 | Loss: 0.00003283
Iteration 96/1000 | Loss: 0.00003283
Iteration 97/1000 | Loss: 0.00003283
Iteration 98/1000 | Loss: 0.00003283
Iteration 99/1000 | Loss: 0.00003283
Iteration 100/1000 | Loss: 0.00003283
Iteration 101/1000 | Loss: 0.00003283
Iteration 102/1000 | Loss: 0.00003283
Iteration 103/1000 | Loss: 0.00003283
Iteration 104/1000 | Loss: 0.00003283
Iteration 105/1000 | Loss: 0.00003283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [3.283065598225221e-05, 3.283065598225221e-05, 3.283065598225221e-05, 3.283065598225221e-05, 3.283065598225221e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.283065598225221e-05

Optimization complete. Final v2v error: 5.0013275146484375 mm

Highest mean error: 5.574204921722412 mm for frame 24

Lowest mean error: 4.691100597381592 mm for frame 40

Saving results

Total time: 33.77410817146301
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696364
Iteration 2/25 | Loss: 0.00174284
Iteration 3/25 | Loss: 0.00167570
Iteration 4/25 | Loss: 0.00165886
Iteration 5/25 | Loss: 0.00165555
Iteration 6/25 | Loss: 0.00165517
Iteration 7/25 | Loss: 0.00165517
Iteration 8/25 | Loss: 0.00165517
Iteration 9/25 | Loss: 0.00165517
Iteration 10/25 | Loss: 0.00165517
Iteration 11/25 | Loss: 0.00165517
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0016551729058846831, 0.0016551729058846831, 0.0016551729058846831, 0.0016551729058846831, 0.0016551729058846831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016551729058846831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.66680741
Iteration 2/25 | Loss: 0.00245865
Iteration 3/25 | Loss: 0.00245864
Iteration 4/25 | Loss: 0.00245864
Iteration 5/25 | Loss: 0.00245864
Iteration 6/25 | Loss: 0.00245864
Iteration 7/25 | Loss: 0.00245864
Iteration 8/25 | Loss: 0.00245864
Iteration 9/25 | Loss: 0.00245864
Iteration 10/25 | Loss: 0.00245864
Iteration 11/25 | Loss: 0.00245864
Iteration 12/25 | Loss: 0.00245864
Iteration 13/25 | Loss: 0.00245864
Iteration 14/25 | Loss: 0.00245864
Iteration 15/25 | Loss: 0.00245864
Iteration 16/25 | Loss: 0.00245864
Iteration 17/25 | Loss: 0.00245864
Iteration 18/25 | Loss: 0.00245864
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002458636648952961, 0.002458636648952961, 0.002458636648952961, 0.002458636648952961, 0.002458636648952961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002458636648952961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00245864
Iteration 2/1000 | Loss: 0.00005891
Iteration 3/1000 | Loss: 0.00003862
Iteration 4/1000 | Loss: 0.00003446
Iteration 5/1000 | Loss: 0.00003237
Iteration 6/1000 | Loss: 0.00003099
Iteration 7/1000 | Loss: 0.00003029
Iteration 8/1000 | Loss: 0.00002977
Iteration 9/1000 | Loss: 0.00002934
Iteration 10/1000 | Loss: 0.00002890
Iteration 11/1000 | Loss: 0.00002872
Iteration 12/1000 | Loss: 0.00002867
Iteration 13/1000 | Loss: 0.00002864
Iteration 14/1000 | Loss: 0.00002855
Iteration 15/1000 | Loss: 0.00002855
Iteration 16/1000 | Loss: 0.00002853
Iteration 17/1000 | Loss: 0.00002853
Iteration 18/1000 | Loss: 0.00002852
Iteration 19/1000 | Loss: 0.00002852
Iteration 20/1000 | Loss: 0.00002852
Iteration 21/1000 | Loss: 0.00002851
Iteration 22/1000 | Loss: 0.00002851
Iteration 23/1000 | Loss: 0.00002850
Iteration 24/1000 | Loss: 0.00002849
Iteration 25/1000 | Loss: 0.00002848
Iteration 26/1000 | Loss: 0.00002847
Iteration 27/1000 | Loss: 0.00002847
Iteration 28/1000 | Loss: 0.00002847
Iteration 29/1000 | Loss: 0.00002847
Iteration 30/1000 | Loss: 0.00002847
Iteration 31/1000 | Loss: 0.00002847
Iteration 32/1000 | Loss: 0.00002847
Iteration 33/1000 | Loss: 0.00002846
Iteration 34/1000 | Loss: 0.00002846
Iteration 35/1000 | Loss: 0.00002846
Iteration 36/1000 | Loss: 0.00002845
Iteration 37/1000 | Loss: 0.00002845
Iteration 38/1000 | Loss: 0.00002844
Iteration 39/1000 | Loss: 0.00002844
Iteration 40/1000 | Loss: 0.00002843
Iteration 41/1000 | Loss: 0.00002843
Iteration 42/1000 | Loss: 0.00002843
Iteration 43/1000 | Loss: 0.00002843
Iteration 44/1000 | Loss: 0.00002843
Iteration 45/1000 | Loss: 0.00002843
Iteration 46/1000 | Loss: 0.00002843
Iteration 47/1000 | Loss: 0.00002843
Iteration 48/1000 | Loss: 0.00002843
Iteration 49/1000 | Loss: 0.00002843
Iteration 50/1000 | Loss: 0.00002843
Iteration 51/1000 | Loss: 0.00002842
Iteration 52/1000 | Loss: 0.00002842
Iteration 53/1000 | Loss: 0.00002842
Iteration 54/1000 | Loss: 0.00002842
Iteration 55/1000 | Loss: 0.00002841
Iteration 56/1000 | Loss: 0.00002841
Iteration 57/1000 | Loss: 0.00002840
Iteration 58/1000 | Loss: 0.00002840
Iteration 59/1000 | Loss: 0.00002840
Iteration 60/1000 | Loss: 0.00002840
Iteration 61/1000 | Loss: 0.00002840
Iteration 62/1000 | Loss: 0.00002840
Iteration 63/1000 | Loss: 0.00002840
Iteration 64/1000 | Loss: 0.00002840
Iteration 65/1000 | Loss: 0.00002839
Iteration 66/1000 | Loss: 0.00002839
Iteration 67/1000 | Loss: 0.00002839
Iteration 68/1000 | Loss: 0.00002839
Iteration 69/1000 | Loss: 0.00002839
Iteration 70/1000 | Loss: 0.00002839
Iteration 71/1000 | Loss: 0.00002839
Iteration 72/1000 | Loss: 0.00002838
Iteration 73/1000 | Loss: 0.00002838
Iteration 74/1000 | Loss: 0.00002837
Iteration 75/1000 | Loss: 0.00002836
Iteration 76/1000 | Loss: 0.00002836
Iteration 77/1000 | Loss: 0.00002836
Iteration 78/1000 | Loss: 0.00002835
Iteration 79/1000 | Loss: 0.00002835
Iteration 80/1000 | Loss: 0.00002834
Iteration 81/1000 | Loss: 0.00002834
Iteration 82/1000 | Loss: 0.00002834
Iteration 83/1000 | Loss: 0.00002833
Iteration 84/1000 | Loss: 0.00002833
Iteration 85/1000 | Loss: 0.00002833
Iteration 86/1000 | Loss: 0.00002832
Iteration 87/1000 | Loss: 0.00002832
Iteration 88/1000 | Loss: 0.00002831
Iteration 89/1000 | Loss: 0.00002831
Iteration 90/1000 | Loss: 0.00002831
Iteration 91/1000 | Loss: 0.00002831
Iteration 92/1000 | Loss: 0.00002830
Iteration 93/1000 | Loss: 0.00002830
Iteration 94/1000 | Loss: 0.00002830
Iteration 95/1000 | Loss: 0.00002830
Iteration 96/1000 | Loss: 0.00002830
Iteration 97/1000 | Loss: 0.00002830
Iteration 98/1000 | Loss: 0.00002830
Iteration 99/1000 | Loss: 0.00002830
Iteration 100/1000 | Loss: 0.00002830
Iteration 101/1000 | Loss: 0.00002829
Iteration 102/1000 | Loss: 0.00002829
Iteration 103/1000 | Loss: 0.00002829
Iteration 104/1000 | Loss: 0.00002829
Iteration 105/1000 | Loss: 0.00002829
Iteration 106/1000 | Loss: 0.00002829
Iteration 107/1000 | Loss: 0.00002829
Iteration 108/1000 | Loss: 0.00002829
Iteration 109/1000 | Loss: 0.00002829
Iteration 110/1000 | Loss: 0.00002829
Iteration 111/1000 | Loss: 0.00002829
Iteration 112/1000 | Loss: 0.00002828
Iteration 113/1000 | Loss: 0.00002828
Iteration 114/1000 | Loss: 0.00002828
Iteration 115/1000 | Loss: 0.00002828
Iteration 116/1000 | Loss: 0.00002828
Iteration 117/1000 | Loss: 0.00002828
Iteration 118/1000 | Loss: 0.00002828
Iteration 119/1000 | Loss: 0.00002828
Iteration 120/1000 | Loss: 0.00002828
Iteration 121/1000 | Loss: 0.00002828
Iteration 122/1000 | Loss: 0.00002828
Iteration 123/1000 | Loss: 0.00002828
Iteration 124/1000 | Loss: 0.00002828
Iteration 125/1000 | Loss: 0.00002828
Iteration 126/1000 | Loss: 0.00002828
Iteration 127/1000 | Loss: 0.00002828
Iteration 128/1000 | Loss: 0.00002827
Iteration 129/1000 | Loss: 0.00002827
Iteration 130/1000 | Loss: 0.00002827
Iteration 131/1000 | Loss: 0.00002827
Iteration 132/1000 | Loss: 0.00002827
Iteration 133/1000 | Loss: 0.00002827
Iteration 134/1000 | Loss: 0.00002827
Iteration 135/1000 | Loss: 0.00002827
Iteration 136/1000 | Loss: 0.00002827
Iteration 137/1000 | Loss: 0.00002827
Iteration 138/1000 | Loss: 0.00002827
Iteration 139/1000 | Loss: 0.00002827
Iteration 140/1000 | Loss: 0.00002827
Iteration 141/1000 | Loss: 0.00002827
Iteration 142/1000 | Loss: 0.00002827
Iteration 143/1000 | Loss: 0.00002827
Iteration 144/1000 | Loss: 0.00002827
Iteration 145/1000 | Loss: 0.00002827
Iteration 146/1000 | Loss: 0.00002827
Iteration 147/1000 | Loss: 0.00002827
Iteration 148/1000 | Loss: 0.00002827
Iteration 149/1000 | Loss: 0.00002827
Iteration 150/1000 | Loss: 0.00002827
Iteration 151/1000 | Loss: 0.00002827
Iteration 152/1000 | Loss: 0.00002827
Iteration 153/1000 | Loss: 0.00002827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.8274158466956578e-05, 2.8274158466956578e-05, 2.8274158466956578e-05, 2.8274158466956578e-05, 2.8274158466956578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8274158466956578e-05

Optimization complete. Final v2v error: 4.643051624298096 mm

Highest mean error: 5.107375621795654 mm for frame 88

Lowest mean error: 4.345585346221924 mm for frame 31

Saving results

Total time: 32.921640157699585
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00545187
Iteration 2/25 | Loss: 0.00199288
Iteration 3/25 | Loss: 0.00181438
Iteration 4/25 | Loss: 0.00178684
Iteration 5/25 | Loss: 0.00177902
Iteration 6/25 | Loss: 0.00177656
Iteration 7/25 | Loss: 0.00177595
Iteration 8/25 | Loss: 0.00177595
Iteration 9/25 | Loss: 0.00177595
Iteration 10/25 | Loss: 0.00177595
Iteration 11/25 | Loss: 0.00177595
Iteration 12/25 | Loss: 0.00177595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017759540351107717, 0.0017759540351107717, 0.0017759540351107717, 0.0017759540351107717, 0.0017759540351107717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017759540351107717

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32908773
Iteration 2/25 | Loss: 0.00287430
Iteration 3/25 | Loss: 0.00287428
Iteration 4/25 | Loss: 0.00287428
Iteration 5/25 | Loss: 0.00287428
Iteration 6/25 | Loss: 0.00287428
Iteration 7/25 | Loss: 0.00287428
Iteration 8/25 | Loss: 0.00287428
Iteration 9/25 | Loss: 0.00287428
Iteration 10/25 | Loss: 0.00287428
Iteration 11/25 | Loss: 0.00287428
Iteration 12/25 | Loss: 0.00287428
Iteration 13/25 | Loss: 0.00287428
Iteration 14/25 | Loss: 0.00287428
Iteration 15/25 | Loss: 0.00287428
Iteration 16/25 | Loss: 0.00287428
Iteration 17/25 | Loss: 0.00287428
Iteration 18/25 | Loss: 0.00287428
Iteration 19/25 | Loss: 0.00287428
Iteration 20/25 | Loss: 0.00287428
Iteration 21/25 | Loss: 0.00287428
Iteration 22/25 | Loss: 0.00287428
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002874281257390976, 0.002874281257390976, 0.002874281257390976, 0.002874281257390976, 0.002874281257390976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002874281257390976

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00287428
Iteration 2/1000 | Loss: 0.00009955
Iteration 3/1000 | Loss: 0.00007125
Iteration 4/1000 | Loss: 0.00006032
Iteration 5/1000 | Loss: 0.00005558
Iteration 6/1000 | Loss: 0.00005280
Iteration 7/1000 | Loss: 0.00005098
Iteration 8/1000 | Loss: 0.00004941
Iteration 9/1000 | Loss: 0.00004805
Iteration 10/1000 | Loss: 0.00004716
Iteration 11/1000 | Loss: 0.00004654
Iteration 12/1000 | Loss: 0.00004604
Iteration 13/1000 | Loss: 0.00004565
Iteration 14/1000 | Loss: 0.00004537
Iteration 15/1000 | Loss: 0.00004537
Iteration 16/1000 | Loss: 0.00004518
Iteration 17/1000 | Loss: 0.00004518
Iteration 18/1000 | Loss: 0.00004509
Iteration 19/1000 | Loss: 0.00004504
Iteration 20/1000 | Loss: 0.00004500
Iteration 21/1000 | Loss: 0.00004499
Iteration 22/1000 | Loss: 0.00004489
Iteration 23/1000 | Loss: 0.00004486
Iteration 24/1000 | Loss: 0.00004485
Iteration 25/1000 | Loss: 0.00004485
Iteration 26/1000 | Loss: 0.00004484
Iteration 27/1000 | Loss: 0.00004484
Iteration 28/1000 | Loss: 0.00004479
Iteration 29/1000 | Loss: 0.00004479
Iteration 30/1000 | Loss: 0.00004479
Iteration 31/1000 | Loss: 0.00004478
Iteration 32/1000 | Loss: 0.00004478
Iteration 33/1000 | Loss: 0.00004474
Iteration 34/1000 | Loss: 0.00004472
Iteration 35/1000 | Loss: 0.00004472
Iteration 36/1000 | Loss: 0.00004471
Iteration 37/1000 | Loss: 0.00004471
Iteration 38/1000 | Loss: 0.00004467
Iteration 39/1000 | Loss: 0.00004466
Iteration 40/1000 | Loss: 0.00004465
Iteration 41/1000 | Loss: 0.00004465
Iteration 42/1000 | Loss: 0.00004465
Iteration 43/1000 | Loss: 0.00004464
Iteration 44/1000 | Loss: 0.00004464
Iteration 45/1000 | Loss: 0.00004464
Iteration 46/1000 | Loss: 0.00004464
Iteration 47/1000 | Loss: 0.00004464
Iteration 48/1000 | Loss: 0.00004464
Iteration 49/1000 | Loss: 0.00004464
Iteration 50/1000 | Loss: 0.00004463
Iteration 51/1000 | Loss: 0.00004463
Iteration 52/1000 | Loss: 0.00004461
Iteration 53/1000 | Loss: 0.00004460
Iteration 54/1000 | Loss: 0.00004460
Iteration 55/1000 | Loss: 0.00004460
Iteration 56/1000 | Loss: 0.00004460
Iteration 57/1000 | Loss: 0.00004459
Iteration 58/1000 | Loss: 0.00004456
Iteration 59/1000 | Loss: 0.00004455
Iteration 60/1000 | Loss: 0.00004455
Iteration 61/1000 | Loss: 0.00004454
Iteration 62/1000 | Loss: 0.00004454
Iteration 63/1000 | Loss: 0.00004454
Iteration 64/1000 | Loss: 0.00004453
Iteration 65/1000 | Loss: 0.00004453
Iteration 66/1000 | Loss: 0.00004453
Iteration 67/1000 | Loss: 0.00004453
Iteration 68/1000 | Loss: 0.00004453
Iteration 69/1000 | Loss: 0.00004452
Iteration 70/1000 | Loss: 0.00004452
Iteration 71/1000 | Loss: 0.00004451
Iteration 72/1000 | Loss: 0.00004451
Iteration 73/1000 | Loss: 0.00004451
Iteration 74/1000 | Loss: 0.00004450
Iteration 75/1000 | Loss: 0.00004450
Iteration 76/1000 | Loss: 0.00004450
Iteration 77/1000 | Loss: 0.00004450
Iteration 78/1000 | Loss: 0.00004450
Iteration 79/1000 | Loss: 0.00004450
Iteration 80/1000 | Loss: 0.00004450
Iteration 81/1000 | Loss: 0.00004450
Iteration 82/1000 | Loss: 0.00004450
Iteration 83/1000 | Loss: 0.00004450
Iteration 84/1000 | Loss: 0.00004449
Iteration 85/1000 | Loss: 0.00004449
Iteration 86/1000 | Loss: 0.00004449
Iteration 87/1000 | Loss: 0.00004449
Iteration 88/1000 | Loss: 0.00004449
Iteration 89/1000 | Loss: 0.00004449
Iteration 90/1000 | Loss: 0.00004449
Iteration 91/1000 | Loss: 0.00004449
Iteration 92/1000 | Loss: 0.00004449
Iteration 93/1000 | Loss: 0.00004448
Iteration 94/1000 | Loss: 0.00004448
Iteration 95/1000 | Loss: 0.00004448
Iteration 96/1000 | Loss: 0.00004447
Iteration 97/1000 | Loss: 0.00004447
Iteration 98/1000 | Loss: 0.00004447
Iteration 99/1000 | Loss: 0.00004447
Iteration 100/1000 | Loss: 0.00004446
Iteration 101/1000 | Loss: 0.00004446
Iteration 102/1000 | Loss: 0.00004446
Iteration 103/1000 | Loss: 0.00004446
Iteration 104/1000 | Loss: 0.00004445
Iteration 105/1000 | Loss: 0.00004445
Iteration 106/1000 | Loss: 0.00004445
Iteration 107/1000 | Loss: 0.00004445
Iteration 108/1000 | Loss: 0.00004444
Iteration 109/1000 | Loss: 0.00004444
Iteration 110/1000 | Loss: 0.00004444
Iteration 111/1000 | Loss: 0.00004444
Iteration 112/1000 | Loss: 0.00004443
Iteration 113/1000 | Loss: 0.00004443
Iteration 114/1000 | Loss: 0.00004443
Iteration 115/1000 | Loss: 0.00004443
Iteration 116/1000 | Loss: 0.00004443
Iteration 117/1000 | Loss: 0.00004442
Iteration 118/1000 | Loss: 0.00004442
Iteration 119/1000 | Loss: 0.00004442
Iteration 120/1000 | Loss: 0.00004442
Iteration 121/1000 | Loss: 0.00004442
Iteration 122/1000 | Loss: 0.00004442
Iteration 123/1000 | Loss: 0.00004442
Iteration 124/1000 | Loss: 0.00004442
Iteration 125/1000 | Loss: 0.00004442
Iteration 126/1000 | Loss: 0.00004441
Iteration 127/1000 | Loss: 0.00004441
Iteration 128/1000 | Loss: 0.00004441
Iteration 129/1000 | Loss: 0.00004441
Iteration 130/1000 | Loss: 0.00004441
Iteration 131/1000 | Loss: 0.00004441
Iteration 132/1000 | Loss: 0.00004441
Iteration 133/1000 | Loss: 0.00004441
Iteration 134/1000 | Loss: 0.00004440
Iteration 135/1000 | Loss: 0.00004440
Iteration 136/1000 | Loss: 0.00004440
Iteration 137/1000 | Loss: 0.00004440
Iteration 138/1000 | Loss: 0.00004440
Iteration 139/1000 | Loss: 0.00004440
Iteration 140/1000 | Loss: 0.00004440
Iteration 141/1000 | Loss: 0.00004440
Iteration 142/1000 | Loss: 0.00004440
Iteration 143/1000 | Loss: 0.00004440
Iteration 144/1000 | Loss: 0.00004440
Iteration 145/1000 | Loss: 0.00004440
Iteration 146/1000 | Loss: 0.00004439
Iteration 147/1000 | Loss: 0.00004439
Iteration 148/1000 | Loss: 0.00004439
Iteration 149/1000 | Loss: 0.00004439
Iteration 150/1000 | Loss: 0.00004439
Iteration 151/1000 | Loss: 0.00004439
Iteration 152/1000 | Loss: 0.00004439
Iteration 153/1000 | Loss: 0.00004439
Iteration 154/1000 | Loss: 0.00004438
Iteration 155/1000 | Loss: 0.00004438
Iteration 156/1000 | Loss: 0.00004438
Iteration 157/1000 | Loss: 0.00004438
Iteration 158/1000 | Loss: 0.00004438
Iteration 159/1000 | Loss: 0.00004438
Iteration 160/1000 | Loss: 0.00004438
Iteration 161/1000 | Loss: 0.00004438
Iteration 162/1000 | Loss: 0.00004438
Iteration 163/1000 | Loss: 0.00004438
Iteration 164/1000 | Loss: 0.00004437
Iteration 165/1000 | Loss: 0.00004437
Iteration 166/1000 | Loss: 0.00004437
Iteration 167/1000 | Loss: 0.00004437
Iteration 168/1000 | Loss: 0.00004437
Iteration 169/1000 | Loss: 0.00004437
Iteration 170/1000 | Loss: 0.00004437
Iteration 171/1000 | Loss: 0.00004436
Iteration 172/1000 | Loss: 0.00004436
Iteration 173/1000 | Loss: 0.00004436
Iteration 174/1000 | Loss: 0.00004436
Iteration 175/1000 | Loss: 0.00004436
Iteration 176/1000 | Loss: 0.00004436
Iteration 177/1000 | Loss: 0.00004436
Iteration 178/1000 | Loss: 0.00004436
Iteration 179/1000 | Loss: 0.00004436
Iteration 180/1000 | Loss: 0.00004436
Iteration 181/1000 | Loss: 0.00004436
Iteration 182/1000 | Loss: 0.00004436
Iteration 183/1000 | Loss: 0.00004436
Iteration 184/1000 | Loss: 0.00004436
Iteration 185/1000 | Loss: 0.00004436
Iteration 186/1000 | Loss: 0.00004436
Iteration 187/1000 | Loss: 0.00004435
Iteration 188/1000 | Loss: 0.00004435
Iteration 189/1000 | Loss: 0.00004435
Iteration 190/1000 | Loss: 0.00004435
Iteration 191/1000 | Loss: 0.00004435
Iteration 192/1000 | Loss: 0.00004435
Iteration 193/1000 | Loss: 0.00004435
Iteration 194/1000 | Loss: 0.00004435
Iteration 195/1000 | Loss: 0.00004435
Iteration 196/1000 | Loss: 0.00004435
Iteration 197/1000 | Loss: 0.00004435
Iteration 198/1000 | Loss: 0.00004435
Iteration 199/1000 | Loss: 0.00004434
Iteration 200/1000 | Loss: 0.00004434
Iteration 201/1000 | Loss: 0.00004434
Iteration 202/1000 | Loss: 0.00004434
Iteration 203/1000 | Loss: 0.00004434
Iteration 204/1000 | Loss: 0.00004434
Iteration 205/1000 | Loss: 0.00004434
Iteration 206/1000 | Loss: 0.00004434
Iteration 207/1000 | Loss: 0.00004434
Iteration 208/1000 | Loss: 0.00004434
Iteration 209/1000 | Loss: 0.00004434
Iteration 210/1000 | Loss: 0.00004434
Iteration 211/1000 | Loss: 0.00004434
Iteration 212/1000 | Loss: 0.00004434
Iteration 213/1000 | Loss: 0.00004434
Iteration 214/1000 | Loss: 0.00004434
Iteration 215/1000 | Loss: 0.00004434
Iteration 216/1000 | Loss: 0.00004434
Iteration 217/1000 | Loss: 0.00004434
Iteration 218/1000 | Loss: 0.00004434
Iteration 219/1000 | Loss: 0.00004434
Iteration 220/1000 | Loss: 0.00004434
Iteration 221/1000 | Loss: 0.00004434
Iteration 222/1000 | Loss: 0.00004434
Iteration 223/1000 | Loss: 0.00004434
Iteration 224/1000 | Loss: 0.00004433
Iteration 225/1000 | Loss: 0.00004433
Iteration 226/1000 | Loss: 0.00004433
Iteration 227/1000 | Loss: 0.00004433
Iteration 228/1000 | Loss: 0.00004433
Iteration 229/1000 | Loss: 0.00004433
Iteration 230/1000 | Loss: 0.00004433
Iteration 231/1000 | Loss: 0.00004433
Iteration 232/1000 | Loss: 0.00004433
Iteration 233/1000 | Loss: 0.00004433
Iteration 234/1000 | Loss: 0.00004433
Iteration 235/1000 | Loss: 0.00004433
Iteration 236/1000 | Loss: 0.00004433
Iteration 237/1000 | Loss: 0.00004433
Iteration 238/1000 | Loss: 0.00004433
Iteration 239/1000 | Loss: 0.00004433
Iteration 240/1000 | Loss: 0.00004433
Iteration 241/1000 | Loss: 0.00004433
Iteration 242/1000 | Loss: 0.00004433
Iteration 243/1000 | Loss: 0.00004433
Iteration 244/1000 | Loss: 0.00004433
Iteration 245/1000 | Loss: 0.00004433
Iteration 246/1000 | Loss: 0.00004433
Iteration 247/1000 | Loss: 0.00004433
Iteration 248/1000 | Loss: 0.00004433
Iteration 249/1000 | Loss: 0.00004433
Iteration 250/1000 | Loss: 0.00004433
Iteration 251/1000 | Loss: 0.00004433
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [4.432852438185364e-05, 4.432852438185364e-05, 4.432852438185364e-05, 4.432852438185364e-05, 4.432852438185364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.432852438185364e-05

Optimization complete. Final v2v error: 5.4858808517456055 mm

Highest mean error: 6.957205772399902 mm for frame 73

Lowest mean error: 4.637290000915527 mm for frame 36

Saving results

Total time: 50.40932106971741
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434894
Iteration 2/25 | Loss: 0.00185303
Iteration 3/25 | Loss: 0.00173614
Iteration 4/25 | Loss: 0.00170967
Iteration 5/25 | Loss: 0.00170433
Iteration 6/25 | Loss: 0.00170233
Iteration 7/25 | Loss: 0.00170175
Iteration 8/25 | Loss: 0.00170175
Iteration 9/25 | Loss: 0.00170175
Iteration 10/25 | Loss: 0.00170175
Iteration 11/25 | Loss: 0.00170175
Iteration 12/25 | Loss: 0.00170175
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017017530044540763, 0.0017017530044540763, 0.0017017530044540763, 0.0017017530044540763, 0.0017017530044540763]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017017530044540763

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67689002
Iteration 2/25 | Loss: 0.00395015
Iteration 3/25 | Loss: 0.00395015
Iteration 4/25 | Loss: 0.00395015
Iteration 5/25 | Loss: 0.00395015
Iteration 6/25 | Loss: 0.00395015
Iteration 7/25 | Loss: 0.00395015
Iteration 8/25 | Loss: 0.00395015
Iteration 9/25 | Loss: 0.00395015
Iteration 10/25 | Loss: 0.00395015
Iteration 11/25 | Loss: 0.00395015
Iteration 12/25 | Loss: 0.00395015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.003950145561248064, 0.003950145561248064, 0.003950145561248064, 0.003950145561248064, 0.003950145561248064]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003950145561248064

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00395015
Iteration 2/1000 | Loss: 0.00008172
Iteration 3/1000 | Loss: 0.00004696
Iteration 4/1000 | Loss: 0.00003809
Iteration 5/1000 | Loss: 0.00003427
Iteration 6/1000 | Loss: 0.00003232
Iteration 7/1000 | Loss: 0.00003083
Iteration 8/1000 | Loss: 0.00002966
Iteration 9/1000 | Loss: 0.00002886
Iteration 10/1000 | Loss: 0.00002829
Iteration 11/1000 | Loss: 0.00002795
Iteration 12/1000 | Loss: 0.00002763
Iteration 13/1000 | Loss: 0.00002741
Iteration 14/1000 | Loss: 0.00002727
Iteration 15/1000 | Loss: 0.00002705
Iteration 16/1000 | Loss: 0.00002690
Iteration 17/1000 | Loss: 0.00002683
Iteration 18/1000 | Loss: 0.00002680
Iteration 19/1000 | Loss: 0.00002679
Iteration 20/1000 | Loss: 0.00002678
Iteration 21/1000 | Loss: 0.00002677
Iteration 22/1000 | Loss: 0.00002674
Iteration 23/1000 | Loss: 0.00002666
Iteration 24/1000 | Loss: 0.00002659
Iteration 25/1000 | Loss: 0.00002652
Iteration 26/1000 | Loss: 0.00002647
Iteration 27/1000 | Loss: 0.00002647
Iteration 28/1000 | Loss: 0.00002646
Iteration 29/1000 | Loss: 0.00002646
Iteration 30/1000 | Loss: 0.00002646
Iteration 31/1000 | Loss: 0.00002646
Iteration 32/1000 | Loss: 0.00002643
Iteration 33/1000 | Loss: 0.00002643
Iteration 34/1000 | Loss: 0.00002642
Iteration 35/1000 | Loss: 0.00002638
Iteration 36/1000 | Loss: 0.00002638
Iteration 37/1000 | Loss: 0.00002638
Iteration 38/1000 | Loss: 0.00002638
Iteration 39/1000 | Loss: 0.00002638
Iteration 40/1000 | Loss: 0.00002638
Iteration 41/1000 | Loss: 0.00002638
Iteration 42/1000 | Loss: 0.00002637
Iteration 43/1000 | Loss: 0.00002637
Iteration 44/1000 | Loss: 0.00002637
Iteration 45/1000 | Loss: 0.00002637
Iteration 46/1000 | Loss: 0.00002637
Iteration 47/1000 | Loss: 0.00002636
Iteration 48/1000 | Loss: 0.00002636
Iteration 49/1000 | Loss: 0.00002636
Iteration 50/1000 | Loss: 0.00002636
Iteration 51/1000 | Loss: 0.00002636
Iteration 52/1000 | Loss: 0.00002635
Iteration 53/1000 | Loss: 0.00002635
Iteration 54/1000 | Loss: 0.00002635
Iteration 55/1000 | Loss: 0.00002634
Iteration 56/1000 | Loss: 0.00002634
Iteration 57/1000 | Loss: 0.00002634
Iteration 58/1000 | Loss: 0.00002634
Iteration 59/1000 | Loss: 0.00002633
Iteration 60/1000 | Loss: 0.00002633
Iteration 61/1000 | Loss: 0.00002633
Iteration 62/1000 | Loss: 0.00002632
Iteration 63/1000 | Loss: 0.00002632
Iteration 64/1000 | Loss: 0.00002631
Iteration 65/1000 | Loss: 0.00002631
Iteration 66/1000 | Loss: 0.00002631
Iteration 67/1000 | Loss: 0.00002630
Iteration 68/1000 | Loss: 0.00002630
Iteration 69/1000 | Loss: 0.00002630
Iteration 70/1000 | Loss: 0.00002629
Iteration 71/1000 | Loss: 0.00002629
Iteration 72/1000 | Loss: 0.00002628
Iteration 73/1000 | Loss: 0.00002628
Iteration 74/1000 | Loss: 0.00002628
Iteration 75/1000 | Loss: 0.00002628
Iteration 76/1000 | Loss: 0.00002627
Iteration 77/1000 | Loss: 0.00002627
Iteration 78/1000 | Loss: 0.00002627
Iteration 79/1000 | Loss: 0.00002627
Iteration 80/1000 | Loss: 0.00002626
Iteration 81/1000 | Loss: 0.00002626
Iteration 82/1000 | Loss: 0.00002626
Iteration 83/1000 | Loss: 0.00002626
Iteration 84/1000 | Loss: 0.00002625
Iteration 85/1000 | Loss: 0.00002625
Iteration 86/1000 | Loss: 0.00002625
Iteration 87/1000 | Loss: 0.00002625
Iteration 88/1000 | Loss: 0.00002625
Iteration 89/1000 | Loss: 0.00002625
Iteration 90/1000 | Loss: 0.00002625
Iteration 91/1000 | Loss: 0.00002624
Iteration 92/1000 | Loss: 0.00002624
Iteration 93/1000 | Loss: 0.00002624
Iteration 94/1000 | Loss: 0.00002624
Iteration 95/1000 | Loss: 0.00002624
Iteration 96/1000 | Loss: 0.00002624
Iteration 97/1000 | Loss: 0.00002624
Iteration 98/1000 | Loss: 0.00002624
Iteration 99/1000 | Loss: 0.00002623
Iteration 100/1000 | Loss: 0.00002623
Iteration 101/1000 | Loss: 0.00002623
Iteration 102/1000 | Loss: 0.00002623
Iteration 103/1000 | Loss: 0.00002623
Iteration 104/1000 | Loss: 0.00002623
Iteration 105/1000 | Loss: 0.00002623
Iteration 106/1000 | Loss: 0.00002623
Iteration 107/1000 | Loss: 0.00002622
Iteration 108/1000 | Loss: 0.00002622
Iteration 109/1000 | Loss: 0.00002622
Iteration 110/1000 | Loss: 0.00002622
Iteration 111/1000 | Loss: 0.00002622
Iteration 112/1000 | Loss: 0.00002622
Iteration 113/1000 | Loss: 0.00002622
Iteration 114/1000 | Loss: 0.00002622
Iteration 115/1000 | Loss: 0.00002622
Iteration 116/1000 | Loss: 0.00002622
Iteration 117/1000 | Loss: 0.00002622
Iteration 118/1000 | Loss: 0.00002622
Iteration 119/1000 | Loss: 0.00002621
Iteration 120/1000 | Loss: 0.00002621
Iteration 121/1000 | Loss: 0.00002621
Iteration 122/1000 | Loss: 0.00002621
Iteration 123/1000 | Loss: 0.00002621
Iteration 124/1000 | Loss: 0.00002621
Iteration 125/1000 | Loss: 0.00002621
Iteration 126/1000 | Loss: 0.00002621
Iteration 127/1000 | Loss: 0.00002621
Iteration 128/1000 | Loss: 0.00002621
Iteration 129/1000 | Loss: 0.00002621
Iteration 130/1000 | Loss: 0.00002620
Iteration 131/1000 | Loss: 0.00002620
Iteration 132/1000 | Loss: 0.00002620
Iteration 133/1000 | Loss: 0.00002620
Iteration 134/1000 | Loss: 0.00002620
Iteration 135/1000 | Loss: 0.00002620
Iteration 136/1000 | Loss: 0.00002620
Iteration 137/1000 | Loss: 0.00002620
Iteration 138/1000 | Loss: 0.00002620
Iteration 139/1000 | Loss: 0.00002620
Iteration 140/1000 | Loss: 0.00002620
Iteration 141/1000 | Loss: 0.00002620
Iteration 142/1000 | Loss: 0.00002619
Iteration 143/1000 | Loss: 0.00002619
Iteration 144/1000 | Loss: 0.00002619
Iteration 145/1000 | Loss: 0.00002619
Iteration 146/1000 | Loss: 0.00002619
Iteration 147/1000 | Loss: 0.00002619
Iteration 148/1000 | Loss: 0.00002619
Iteration 149/1000 | Loss: 0.00002619
Iteration 150/1000 | Loss: 0.00002619
Iteration 151/1000 | Loss: 0.00002619
Iteration 152/1000 | Loss: 0.00002619
Iteration 153/1000 | Loss: 0.00002618
Iteration 154/1000 | Loss: 0.00002618
Iteration 155/1000 | Loss: 0.00002618
Iteration 156/1000 | Loss: 0.00002618
Iteration 157/1000 | Loss: 0.00002618
Iteration 158/1000 | Loss: 0.00002618
Iteration 159/1000 | Loss: 0.00002617
Iteration 160/1000 | Loss: 0.00002617
Iteration 161/1000 | Loss: 0.00002617
Iteration 162/1000 | Loss: 0.00002617
Iteration 163/1000 | Loss: 0.00002617
Iteration 164/1000 | Loss: 0.00002617
Iteration 165/1000 | Loss: 0.00002617
Iteration 166/1000 | Loss: 0.00002617
Iteration 167/1000 | Loss: 0.00002617
Iteration 168/1000 | Loss: 0.00002617
Iteration 169/1000 | Loss: 0.00002617
Iteration 170/1000 | Loss: 0.00002617
Iteration 171/1000 | Loss: 0.00002616
Iteration 172/1000 | Loss: 0.00002616
Iteration 173/1000 | Loss: 0.00002616
Iteration 174/1000 | Loss: 0.00002616
Iteration 175/1000 | Loss: 0.00002616
Iteration 176/1000 | Loss: 0.00002616
Iteration 177/1000 | Loss: 0.00002616
Iteration 178/1000 | Loss: 0.00002616
Iteration 179/1000 | Loss: 0.00002616
Iteration 180/1000 | Loss: 0.00002616
Iteration 181/1000 | Loss: 0.00002616
Iteration 182/1000 | Loss: 0.00002616
Iteration 183/1000 | Loss: 0.00002616
Iteration 184/1000 | Loss: 0.00002616
Iteration 185/1000 | Loss: 0.00002616
Iteration 186/1000 | Loss: 0.00002616
Iteration 187/1000 | Loss: 0.00002616
Iteration 188/1000 | Loss: 0.00002616
Iteration 189/1000 | Loss: 0.00002615
Iteration 190/1000 | Loss: 0.00002615
Iteration 191/1000 | Loss: 0.00002615
Iteration 192/1000 | Loss: 0.00002615
Iteration 193/1000 | Loss: 0.00002615
Iteration 194/1000 | Loss: 0.00002615
Iteration 195/1000 | Loss: 0.00002615
Iteration 196/1000 | Loss: 0.00002615
Iteration 197/1000 | Loss: 0.00002615
Iteration 198/1000 | Loss: 0.00002615
Iteration 199/1000 | Loss: 0.00002615
Iteration 200/1000 | Loss: 0.00002615
Iteration 201/1000 | Loss: 0.00002615
Iteration 202/1000 | Loss: 0.00002615
Iteration 203/1000 | Loss: 0.00002615
Iteration 204/1000 | Loss: 0.00002615
Iteration 205/1000 | Loss: 0.00002615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.6147015887545422e-05, 2.6147015887545422e-05, 2.6147015887545422e-05, 2.6147015887545422e-05, 2.6147015887545422e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6147015887545422e-05

Optimization complete. Final v2v error: 4.502207279205322 mm

Highest mean error: 5.455533981323242 mm for frame 152

Lowest mean error: 4.068658351898193 mm for frame 108

Saving results

Total time: 51.830238580703735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01124875
Iteration 2/25 | Loss: 0.00297150
Iteration 3/25 | Loss: 0.00228624
Iteration 4/25 | Loss: 0.00211985
Iteration 5/25 | Loss: 0.00219306
Iteration 6/25 | Loss: 0.00212696
Iteration 7/25 | Loss: 0.00192618
Iteration 8/25 | Loss: 0.00186347
Iteration 9/25 | Loss: 0.00181529
Iteration 10/25 | Loss: 0.00179642
Iteration 11/25 | Loss: 0.00176386
Iteration 12/25 | Loss: 0.00175469
Iteration 13/25 | Loss: 0.00174039
Iteration 14/25 | Loss: 0.00172112
Iteration 15/25 | Loss: 0.00170274
Iteration 16/25 | Loss: 0.00170368
Iteration 17/25 | Loss: 0.00169657
Iteration 18/25 | Loss: 0.00169122
Iteration 19/25 | Loss: 0.00169018
Iteration 20/25 | Loss: 0.00169072
Iteration 21/25 | Loss: 0.00168931
Iteration 22/25 | Loss: 0.00168756
Iteration 23/25 | Loss: 0.00168820
Iteration 24/25 | Loss: 0.00168660
Iteration 25/25 | Loss: 0.00168667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66366899
Iteration 2/25 | Loss: 0.00517279
Iteration 3/25 | Loss: 0.00498607
Iteration 4/25 | Loss: 0.00498607
Iteration 5/25 | Loss: 0.00498607
Iteration 6/25 | Loss: 0.00498607
Iteration 7/25 | Loss: 0.00498607
Iteration 8/25 | Loss: 0.00498606
Iteration 9/25 | Loss: 0.00498606
Iteration 10/25 | Loss: 0.00498606
Iteration 11/25 | Loss: 0.00498606
Iteration 12/25 | Loss: 0.00498606
Iteration 13/25 | Loss: 0.00498606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.004986064974218607, 0.004986064974218607, 0.004986064974218607, 0.004986064974218607, 0.004986064974218607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004986064974218607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00498607
Iteration 2/1000 | Loss: 0.00096011
Iteration 3/1000 | Loss: 0.00092315
Iteration 4/1000 | Loss: 0.00165009
Iteration 5/1000 | Loss: 0.00227408
Iteration 6/1000 | Loss: 0.00172683
Iteration 7/1000 | Loss: 0.00166836
Iteration 8/1000 | Loss: 0.00116873
Iteration 9/1000 | Loss: 0.00267273
Iteration 10/1000 | Loss: 0.00138051
Iteration 11/1000 | Loss: 0.00255767
Iteration 12/1000 | Loss: 0.00212662
Iteration 13/1000 | Loss: 0.00135638
Iteration 14/1000 | Loss: 0.00103591
Iteration 15/1000 | Loss: 0.00266648
Iteration 16/1000 | Loss: 0.00287698
Iteration 17/1000 | Loss: 0.00148395
Iteration 18/1000 | Loss: 0.00257290
Iteration 19/1000 | Loss: 0.00144382
Iteration 20/1000 | Loss: 0.00279072
Iteration 21/1000 | Loss: 0.00254205
Iteration 22/1000 | Loss: 0.00292944
Iteration 23/1000 | Loss: 0.00455539
Iteration 24/1000 | Loss: 0.00162159
Iteration 25/1000 | Loss: 0.00231310
Iteration 26/1000 | Loss: 0.00189208
Iteration 27/1000 | Loss: 0.00372562
Iteration 28/1000 | Loss: 0.00267501
Iteration 29/1000 | Loss: 0.00170597
Iteration 30/1000 | Loss: 0.00044694
Iteration 31/1000 | Loss: 0.00081071
Iteration 32/1000 | Loss: 0.00056477
Iteration 33/1000 | Loss: 0.00037632
Iteration 34/1000 | Loss: 0.00146526
Iteration 35/1000 | Loss: 0.00094221
Iteration 36/1000 | Loss: 0.00120657
Iteration 37/1000 | Loss: 0.00150265
Iteration 38/1000 | Loss: 0.00060871
Iteration 39/1000 | Loss: 0.00141837
Iteration 40/1000 | Loss: 0.00069697
Iteration 41/1000 | Loss: 0.00027902
Iteration 42/1000 | Loss: 0.00069086
Iteration 43/1000 | Loss: 0.00068375
Iteration 44/1000 | Loss: 0.00074100
Iteration 45/1000 | Loss: 0.00089666
Iteration 46/1000 | Loss: 0.00027609
Iteration 47/1000 | Loss: 0.00076069
Iteration 48/1000 | Loss: 0.00158044
Iteration 49/1000 | Loss: 0.00039472
Iteration 50/1000 | Loss: 0.00050343
Iteration 51/1000 | Loss: 0.00094947
Iteration 52/1000 | Loss: 0.00021580
Iteration 53/1000 | Loss: 0.00077882
Iteration 54/1000 | Loss: 0.00090144
Iteration 55/1000 | Loss: 0.00045194
Iteration 56/1000 | Loss: 0.00064195
Iteration 57/1000 | Loss: 0.00037219
Iteration 58/1000 | Loss: 0.00120448
Iteration 59/1000 | Loss: 0.00077375
Iteration 60/1000 | Loss: 0.00168195
Iteration 61/1000 | Loss: 0.00036614
Iteration 62/1000 | Loss: 0.00071706
Iteration 63/1000 | Loss: 0.00103435
Iteration 64/1000 | Loss: 0.00035204
Iteration 65/1000 | Loss: 0.00053321
Iteration 66/1000 | Loss: 0.00015132
Iteration 67/1000 | Loss: 0.00010742
Iteration 68/1000 | Loss: 0.00064257
Iteration 69/1000 | Loss: 0.00066978
Iteration 70/1000 | Loss: 0.00086557
Iteration 71/1000 | Loss: 0.00079854
Iteration 72/1000 | Loss: 0.00099662
Iteration 73/1000 | Loss: 0.00074658
Iteration 74/1000 | Loss: 0.00086374
Iteration 75/1000 | Loss: 0.00060946
Iteration 76/1000 | Loss: 0.00073077
Iteration 77/1000 | Loss: 0.00076286
Iteration 78/1000 | Loss: 0.00058872
Iteration 79/1000 | Loss: 0.00030953
Iteration 80/1000 | Loss: 0.00057350
Iteration 81/1000 | Loss: 0.00036079
Iteration 82/1000 | Loss: 0.00022599
Iteration 83/1000 | Loss: 0.00008901
Iteration 84/1000 | Loss: 0.00008192
Iteration 85/1000 | Loss: 0.00006970
Iteration 86/1000 | Loss: 0.00064320
Iteration 87/1000 | Loss: 0.00052155
Iteration 88/1000 | Loss: 0.00011083
Iteration 89/1000 | Loss: 0.00050150
Iteration 90/1000 | Loss: 0.00009902
Iteration 91/1000 | Loss: 0.00042915
Iteration 92/1000 | Loss: 0.00010793
Iteration 93/1000 | Loss: 0.00040043
Iteration 94/1000 | Loss: 0.00022327
Iteration 95/1000 | Loss: 0.00096035
Iteration 96/1000 | Loss: 0.00030211
Iteration 97/1000 | Loss: 0.00021026
Iteration 98/1000 | Loss: 0.00016686
Iteration 99/1000 | Loss: 0.00019391
Iteration 100/1000 | Loss: 0.00016879
Iteration 101/1000 | Loss: 0.00025893
Iteration 102/1000 | Loss: 0.00054774
Iteration 103/1000 | Loss: 0.00013294
Iteration 104/1000 | Loss: 0.00074162
Iteration 105/1000 | Loss: 0.00030431
Iteration 106/1000 | Loss: 0.00065343
Iteration 107/1000 | Loss: 0.00008679
Iteration 108/1000 | Loss: 0.00006142
Iteration 109/1000 | Loss: 0.00006541
Iteration 110/1000 | Loss: 0.00016941
Iteration 111/1000 | Loss: 0.00007545
Iteration 112/1000 | Loss: 0.00008240
Iteration 113/1000 | Loss: 0.00054534
Iteration 114/1000 | Loss: 0.00051622
Iteration 115/1000 | Loss: 0.00010491
Iteration 116/1000 | Loss: 0.00009014
Iteration 117/1000 | Loss: 0.00008199
Iteration 118/1000 | Loss: 0.00007325
Iteration 119/1000 | Loss: 0.00007489
Iteration 120/1000 | Loss: 0.00006276
Iteration 121/1000 | Loss: 0.00007000
Iteration 122/1000 | Loss: 0.00006107
Iteration 123/1000 | Loss: 0.00006595
Iteration 124/1000 | Loss: 0.00006078
Iteration 125/1000 | Loss: 0.00007689
Iteration 126/1000 | Loss: 0.00012259
Iteration 127/1000 | Loss: 0.00014421
Iteration 128/1000 | Loss: 0.00043700
Iteration 129/1000 | Loss: 0.00019488
Iteration 130/1000 | Loss: 0.00036194
Iteration 131/1000 | Loss: 0.00025485
Iteration 132/1000 | Loss: 0.00018718
Iteration 133/1000 | Loss: 0.00026689
Iteration 134/1000 | Loss: 0.00021153
Iteration 135/1000 | Loss: 0.00006742
Iteration 136/1000 | Loss: 0.00005491
Iteration 137/1000 | Loss: 0.00005972
Iteration 138/1000 | Loss: 0.00005453
Iteration 139/1000 | Loss: 0.00005380
Iteration 140/1000 | Loss: 0.00005886
Iteration 141/1000 | Loss: 0.00007479
Iteration 142/1000 | Loss: 0.00006345
Iteration 143/1000 | Loss: 0.00006319
Iteration 144/1000 | Loss: 0.00022776
Iteration 145/1000 | Loss: 0.00013690
Iteration 146/1000 | Loss: 0.00012403
Iteration 147/1000 | Loss: 0.00010175
Iteration 148/1000 | Loss: 0.00011032
Iteration 149/1000 | Loss: 0.00008896
Iteration 150/1000 | Loss: 0.00017439
Iteration 151/1000 | Loss: 0.00017427
Iteration 152/1000 | Loss: 0.00022515
Iteration 153/1000 | Loss: 0.00021864
Iteration 154/1000 | Loss: 0.00016206
Iteration 155/1000 | Loss: 0.00034958
Iteration 156/1000 | Loss: 0.00027316
Iteration 157/1000 | Loss: 0.00026677
Iteration 158/1000 | Loss: 0.00007184
Iteration 159/1000 | Loss: 0.00015049
Iteration 160/1000 | Loss: 0.00014374
Iteration 161/1000 | Loss: 0.00015345
Iteration 162/1000 | Loss: 0.00020278
Iteration 163/1000 | Loss: 0.00018189
Iteration 164/1000 | Loss: 0.00022692
Iteration 165/1000 | Loss: 0.00009036
Iteration 166/1000 | Loss: 0.00007375
Iteration 167/1000 | Loss: 0.00011339
Iteration 168/1000 | Loss: 0.00022131
Iteration 169/1000 | Loss: 0.00018646
Iteration 170/1000 | Loss: 0.00024557
Iteration 171/1000 | Loss: 0.00021760
Iteration 172/1000 | Loss: 0.00028923
Iteration 173/1000 | Loss: 0.00018903
Iteration 174/1000 | Loss: 0.00016789
Iteration 175/1000 | Loss: 0.00014847
Iteration 176/1000 | Loss: 0.00008540
Iteration 177/1000 | Loss: 0.00007989
Iteration 178/1000 | Loss: 0.00016047
Iteration 179/1000 | Loss: 0.00032963
Iteration 180/1000 | Loss: 0.00012100
Iteration 181/1000 | Loss: 0.00013152
Iteration 182/1000 | Loss: 0.00008697
Iteration 183/1000 | Loss: 0.00008886
Iteration 184/1000 | Loss: 0.00010568
Iteration 185/1000 | Loss: 0.00008357
Iteration 186/1000 | Loss: 0.00017644
Iteration 187/1000 | Loss: 0.00026064
Iteration 188/1000 | Loss: 0.00016414
Iteration 189/1000 | Loss: 0.00027620
Iteration 190/1000 | Loss: 0.00016413
Iteration 191/1000 | Loss: 0.00051468
Iteration 192/1000 | Loss: 0.00010202
Iteration 193/1000 | Loss: 0.00012490
Iteration 194/1000 | Loss: 0.00022520
Iteration 195/1000 | Loss: 0.00026724
Iteration 196/1000 | Loss: 0.00017319
Iteration 197/1000 | Loss: 0.00025620
Iteration 198/1000 | Loss: 0.00025972
Iteration 199/1000 | Loss: 0.00018687
Iteration 200/1000 | Loss: 0.00028183
Iteration 201/1000 | Loss: 0.00017427
Iteration 202/1000 | Loss: 0.00016751
Iteration 203/1000 | Loss: 0.00036414
Iteration 204/1000 | Loss: 0.00077701
Iteration 205/1000 | Loss: 0.00047585
Iteration 206/1000 | Loss: 0.00019362
Iteration 207/1000 | Loss: 0.00012463
Iteration 208/1000 | Loss: 0.00015040
Iteration 209/1000 | Loss: 0.00015245
Iteration 210/1000 | Loss: 0.00038829
Iteration 211/1000 | Loss: 0.00022487
Iteration 212/1000 | Loss: 0.00017178
Iteration 213/1000 | Loss: 0.00016247
Iteration 214/1000 | Loss: 0.00020530
Iteration 215/1000 | Loss: 0.00025649
Iteration 216/1000 | Loss: 0.00019348
Iteration 217/1000 | Loss: 0.00005188
Iteration 218/1000 | Loss: 0.00006389
Iteration 219/1000 | Loss: 0.00006091
Iteration 220/1000 | Loss: 0.00005897
Iteration 221/1000 | Loss: 0.00005931
Iteration 222/1000 | Loss: 0.00007168
Iteration 223/1000 | Loss: 0.00006418
Iteration 224/1000 | Loss: 0.00006028
Iteration 225/1000 | Loss: 0.00005938
Iteration 226/1000 | Loss: 0.00006941
Iteration 227/1000 | Loss: 0.00005323
Iteration 228/1000 | Loss: 0.00005130
Iteration 229/1000 | Loss: 0.00006155
Iteration 230/1000 | Loss: 0.00006108
Iteration 231/1000 | Loss: 0.00006009
Iteration 232/1000 | Loss: 0.00006067
Iteration 233/1000 | Loss: 0.00028905
Iteration 234/1000 | Loss: 0.00008411
Iteration 235/1000 | Loss: 0.00008355
Iteration 236/1000 | Loss: 0.00009841
Iteration 237/1000 | Loss: 0.00005690
Iteration 238/1000 | Loss: 0.00014933
Iteration 239/1000 | Loss: 0.00033102
Iteration 240/1000 | Loss: 0.00017828
Iteration 241/1000 | Loss: 0.00022011
Iteration 242/1000 | Loss: 0.00019683
Iteration 243/1000 | Loss: 0.00022141
Iteration 244/1000 | Loss: 0.00032491
Iteration 245/1000 | Loss: 0.00021455
Iteration 246/1000 | Loss: 0.00024722
Iteration 247/1000 | Loss: 0.00053530
Iteration 248/1000 | Loss: 0.00020736
Iteration 249/1000 | Loss: 0.00015597
Iteration 250/1000 | Loss: 0.00024061
Iteration 251/1000 | Loss: 0.00006268
Iteration 252/1000 | Loss: 0.00005053
Iteration 253/1000 | Loss: 0.00009712
Iteration 254/1000 | Loss: 0.00004389
Iteration 255/1000 | Loss: 0.00004079
Iteration 256/1000 | Loss: 0.00004485
Iteration 257/1000 | Loss: 0.00003917
Iteration 258/1000 | Loss: 0.00033282
Iteration 259/1000 | Loss: 0.00036966
Iteration 260/1000 | Loss: 0.00015048
Iteration 261/1000 | Loss: 0.00016660
Iteration 262/1000 | Loss: 0.00020987
Iteration 263/1000 | Loss: 0.00022979
Iteration 264/1000 | Loss: 0.00014725
Iteration 265/1000 | Loss: 0.00021673
Iteration 266/1000 | Loss: 0.00028659
Iteration 267/1000 | Loss: 0.00023962
Iteration 268/1000 | Loss: 0.00006998
Iteration 269/1000 | Loss: 0.00048220
Iteration 270/1000 | Loss: 0.00025337
Iteration 271/1000 | Loss: 0.00006608
Iteration 272/1000 | Loss: 0.00009254
Iteration 273/1000 | Loss: 0.00004731
Iteration 274/1000 | Loss: 0.00028807
Iteration 275/1000 | Loss: 0.00005451
Iteration 276/1000 | Loss: 0.00003898
Iteration 277/1000 | Loss: 0.00004537
Iteration 278/1000 | Loss: 0.00006202
Iteration 279/1000 | Loss: 0.00003752
Iteration 280/1000 | Loss: 0.00003799
Iteration 281/1000 | Loss: 0.00003652
Iteration 282/1000 | Loss: 0.00003495
Iteration 283/1000 | Loss: 0.00003450
Iteration 284/1000 | Loss: 0.00003441
Iteration 285/1000 | Loss: 0.00003423
Iteration 286/1000 | Loss: 0.00005692
Iteration 287/1000 | Loss: 0.00003532
Iteration 288/1000 | Loss: 0.00003429
Iteration 289/1000 | Loss: 0.00003462
Iteration 290/1000 | Loss: 0.00003553
Iteration 291/1000 | Loss: 0.00003553
Iteration 292/1000 | Loss: 0.00021599
Iteration 293/1000 | Loss: 0.00004030
Iteration 294/1000 | Loss: 0.00003998
Iteration 295/1000 | Loss: 0.00003716
Iteration 296/1000 | Loss: 0.00003748
Iteration 297/1000 | Loss: 0.00003658
Iteration 298/1000 | Loss: 0.00003605
Iteration 299/1000 | Loss: 0.00003726
Iteration 300/1000 | Loss: 0.00003632
Iteration 301/1000 | Loss: 0.00003723
Iteration 302/1000 | Loss: 0.00003679
Iteration 303/1000 | Loss: 0.00003540
Iteration 304/1000 | Loss: 0.00003539
Iteration 305/1000 | Loss: 0.00003503
Iteration 306/1000 | Loss: 0.00005384
Iteration 307/1000 | Loss: 0.00003613
Iteration 308/1000 | Loss: 0.00003814
Iteration 309/1000 | Loss: 0.00003641
Iteration 310/1000 | Loss: 0.00003925
Iteration 311/1000 | Loss: 0.00004889
Iteration 312/1000 | Loss: 0.00003753
Iteration 313/1000 | Loss: 0.00003666
Iteration 314/1000 | Loss: 0.00003525
Iteration 315/1000 | Loss: 0.00003663
Iteration 316/1000 | Loss: 0.00003904
Iteration 317/1000 | Loss: 0.00003646
Iteration 318/1000 | Loss: 0.00003873
Iteration 319/1000 | Loss: 0.00003626
Iteration 320/1000 | Loss: 0.00003914
Iteration 321/1000 | Loss: 0.00003672
Iteration 322/1000 | Loss: 0.00003918
Iteration 323/1000 | Loss: 0.00003678
Iteration 324/1000 | Loss: 0.00003919
Iteration 325/1000 | Loss: 0.00003749
Iteration 326/1000 | Loss: 0.00003986
Iteration 327/1000 | Loss: 0.00003737
Iteration 328/1000 | Loss: 0.00004117
Iteration 329/1000 | Loss: 0.00003738
Iteration 330/1000 | Loss: 0.00004108
Iteration 331/1000 | Loss: 0.00003688
Iteration 332/1000 | Loss: 0.00003954
Iteration 333/1000 | Loss: 0.00003808
Iteration 334/1000 | Loss: 0.00003808
Iteration 335/1000 | Loss: 0.00003585
Iteration 336/1000 | Loss: 0.00003474
Iteration 337/1000 | Loss: 0.00003467
Iteration 338/1000 | Loss: 0.00003757
Iteration 339/1000 | Loss: 0.00004091
Iteration 340/1000 | Loss: 0.00003711
Iteration 341/1000 | Loss: 0.00003518
Iteration 342/1000 | Loss: 0.00003467
Iteration 343/1000 | Loss: 0.00003742
Iteration 344/1000 | Loss: 0.00004013
Iteration 345/1000 | Loss: 0.00003751
Iteration 346/1000 | Loss: 0.00004041
Iteration 347/1000 | Loss: 0.00003703
Iteration 348/1000 | Loss: 0.00003914
Iteration 349/1000 | Loss: 0.00003597
Iteration 350/1000 | Loss: 0.00003496
Iteration 351/1000 | Loss: 0.00003801
Iteration 352/1000 | Loss: 0.00003504
Iteration 353/1000 | Loss: 0.00003434
Iteration 354/1000 | Loss: 0.00003434
Iteration 355/1000 | Loss: 0.00003434
Iteration 356/1000 | Loss: 0.00003434
Iteration 357/1000 | Loss: 0.00003434
Iteration 358/1000 | Loss: 0.00003434
Iteration 359/1000 | Loss: 0.00003604
Iteration 360/1000 | Loss: 0.00003473
Iteration 361/1000 | Loss: 0.00003669
Iteration 362/1000 | Loss: 0.00003474
Iteration 363/1000 | Loss: 0.00003432
Iteration 364/1000 | Loss: 0.00003432
Iteration 365/1000 | Loss: 0.00003432
Iteration 366/1000 | Loss: 0.00003432
Iteration 367/1000 | Loss: 0.00003432
Iteration 368/1000 | Loss: 0.00003432
Iteration 369/1000 | Loss: 0.00003432
Iteration 370/1000 | Loss: 0.00003432
Iteration 371/1000 | Loss: 0.00003432
Iteration 372/1000 | Loss: 0.00003431
Iteration 373/1000 | Loss: 0.00003431
Iteration 374/1000 | Loss: 0.00003580
Iteration 375/1000 | Loss: 0.00003460
Iteration 376/1000 | Loss: 0.00003563
Iteration 377/1000 | Loss: 0.00003454
Iteration 378/1000 | Loss: 0.00003720
Iteration 379/1000 | Loss: 0.00003673
Iteration 380/1000 | Loss: 0.00004048
Iteration 381/1000 | Loss: 0.00003597
Iteration 382/1000 | Loss: 0.00003847
Iteration 383/1000 | Loss: 0.00003866
Iteration 384/1000 | Loss: 0.00003670
Iteration 385/1000 | Loss: 0.00003618
Iteration 386/1000 | Loss: 0.00003739
Iteration 387/1000 | Loss: 0.00003584
Iteration 388/1000 | Loss: 0.00003492
Iteration 389/1000 | Loss: 0.00003602
Iteration 390/1000 | Loss: 0.00005320
Iteration 391/1000 | Loss: 0.00004042
Iteration 392/1000 | Loss: 0.00003648
Iteration 393/1000 | Loss: 0.00003433
Iteration 394/1000 | Loss: 0.00004144
Iteration 395/1000 | Loss: 0.00003686
Iteration 396/1000 | Loss: 0.00003544
Iteration 397/1000 | Loss: 0.00004010
Iteration 398/1000 | Loss: 0.00003610
Iteration 399/1000 | Loss: 0.00004038
Iteration 400/1000 | Loss: 0.00003670
Iteration 401/1000 | Loss: 0.00004489
Iteration 402/1000 | Loss: 0.00003602
Iteration 403/1000 | Loss: 0.00004153
Iteration 404/1000 | Loss: 0.00003629
Iteration 405/1000 | Loss: 0.00003678
Iteration 406/1000 | Loss: 0.00003883
Iteration 407/1000 | Loss: 0.00003928
Iteration 408/1000 | Loss: 0.00004367
Iteration 409/1000 | Loss: 0.00003774
Iteration 410/1000 | Loss: 0.00003432
Iteration 411/1000 | Loss: 0.00003429
Iteration 412/1000 | Loss: 0.00003429
Iteration 413/1000 | Loss: 0.00003429
Iteration 414/1000 | Loss: 0.00003428
Iteration 415/1000 | Loss: 0.00003428
Iteration 416/1000 | Loss: 0.00003428
Iteration 417/1000 | Loss: 0.00003428
Iteration 418/1000 | Loss: 0.00003428
Iteration 419/1000 | Loss: 0.00003428
Iteration 420/1000 | Loss: 0.00003427
Iteration 421/1000 | Loss: 0.00003427
Iteration 422/1000 | Loss: 0.00003427
Iteration 423/1000 | Loss: 0.00003426
Iteration 424/1000 | Loss: 0.00003426
Iteration 425/1000 | Loss: 0.00003426
Iteration 426/1000 | Loss: 0.00003426
Iteration 427/1000 | Loss: 0.00003426
Iteration 428/1000 | Loss: 0.00003426
Iteration 429/1000 | Loss: 0.00003878
Iteration 430/1000 | Loss: 0.00004855
Iteration 431/1000 | Loss: 0.00003528
Iteration 432/1000 | Loss: 0.00003777
Iteration 433/1000 | Loss: 0.00003478
Iteration 434/1000 | Loss: 0.00003703
Iteration 435/1000 | Loss: 0.00003917
Iteration 436/1000 | Loss: 0.00003950
Iteration 437/1000 | Loss: 0.00003705
Iteration 438/1000 | Loss: 0.00003698
Iteration 439/1000 | Loss: 0.00003656
Iteration 440/1000 | Loss: 0.00003965
Iteration 441/1000 | Loss: 0.00003645
Iteration 442/1000 | Loss: 0.00004274
Iteration 443/1000 | Loss: 0.00003674
Iteration 444/1000 | Loss: 0.00003696
Iteration 445/1000 | Loss: 0.00003523
Iteration 446/1000 | Loss: 0.00003432
Iteration 447/1000 | Loss: 0.00003430
Iteration 448/1000 | Loss: 0.00003430
Iteration 449/1000 | Loss: 0.00003430
Iteration 450/1000 | Loss: 0.00003428
Iteration 451/1000 | Loss: 0.00003428
Iteration 452/1000 | Loss: 0.00003428
Iteration 453/1000 | Loss: 0.00003428
Iteration 454/1000 | Loss: 0.00003880
Iteration 455/1000 | Loss: 0.00003933
Iteration 456/1000 | Loss: 0.00003812
Iteration 457/1000 | Loss: 0.00009021
Iteration 458/1000 | Loss: 0.00003698
Iteration 459/1000 | Loss: 0.00003623
Iteration 460/1000 | Loss: 0.00003614
Iteration 461/1000 | Loss: 0.00003472
Iteration 462/1000 | Loss: 0.00003595
Iteration 463/1000 | Loss: 0.00003433
Iteration 464/1000 | Loss: 0.00003430
Iteration 465/1000 | Loss: 0.00003430
Iteration 466/1000 | Loss: 0.00003430
Iteration 467/1000 | Loss: 0.00003430
Iteration 468/1000 | Loss: 0.00003430
Iteration 469/1000 | Loss: 0.00003430
Iteration 470/1000 | Loss: 0.00003430
Iteration 471/1000 | Loss: 0.00003429
Iteration 472/1000 | Loss: 0.00003429
Iteration 473/1000 | Loss: 0.00003429
Iteration 474/1000 | Loss: 0.00003429
Iteration 475/1000 | Loss: 0.00003429
Iteration 476/1000 | Loss: 0.00003429
Iteration 477/1000 | Loss: 0.00003429
Iteration 478/1000 | Loss: 0.00003429
Iteration 479/1000 | Loss: 0.00003553
Iteration 480/1000 | Loss: 0.00003475
Iteration 481/1000 | Loss: 0.00003427
Iteration 482/1000 | Loss: 0.00003427
Iteration 483/1000 | Loss: 0.00003427
Iteration 484/1000 | Loss: 0.00003427
Iteration 485/1000 | Loss: 0.00003427
Iteration 486/1000 | Loss: 0.00003426
Iteration 487/1000 | Loss: 0.00003426
Iteration 488/1000 | Loss: 0.00003426
Iteration 489/1000 | Loss: 0.00003426
Iteration 490/1000 | Loss: 0.00003426
Iteration 491/1000 | Loss: 0.00003426
Iteration 492/1000 | Loss: 0.00003426
Iteration 493/1000 | Loss: 0.00003426
Iteration 494/1000 | Loss: 0.00003426
Iteration 495/1000 | Loss: 0.00003426
Iteration 496/1000 | Loss: 0.00003426
Iteration 497/1000 | Loss: 0.00003426
Iteration 498/1000 | Loss: 0.00003426
Iteration 499/1000 | Loss: 0.00003426
Iteration 500/1000 | Loss: 0.00003426
Iteration 501/1000 | Loss: 0.00003426
Iteration 502/1000 | Loss: 0.00003550
Iteration 503/1000 | Loss: 0.00003482
Iteration 504/1000 | Loss: 0.00003436
Iteration 505/1000 | Loss: 0.00003430
Iteration 506/1000 | Loss: 0.00003425
Iteration 507/1000 | Loss: 0.00003425
Iteration 508/1000 | Loss: 0.00003425
Iteration 509/1000 | Loss: 0.00003425
Iteration 510/1000 | Loss: 0.00003425
Iteration 511/1000 | Loss: 0.00003425
Iteration 512/1000 | Loss: 0.00003425
Iteration 513/1000 | Loss: 0.00003425
Iteration 514/1000 | Loss: 0.00003425
Iteration 515/1000 | Loss: 0.00003424
Iteration 516/1000 | Loss: 0.00003424
Iteration 517/1000 | Loss: 0.00003424
Iteration 518/1000 | Loss: 0.00003424
Iteration 519/1000 | Loss: 0.00003423
Iteration 520/1000 | Loss: 0.00003423
Iteration 521/1000 | Loss: 0.00003423
Iteration 522/1000 | Loss: 0.00003423
Iteration 523/1000 | Loss: 0.00003423
Iteration 524/1000 | Loss: 0.00003545
Iteration 525/1000 | Loss: 0.00003545
Iteration 526/1000 | Loss: 0.00003486
Iteration 527/1000 | Loss: 0.00003486
Iteration 528/1000 | Loss: 0.00004047
Iteration 529/1000 | Loss: 0.00003536
Iteration 530/1000 | Loss: 0.00004077
Iteration 531/1000 | Loss: 0.00003562
Iteration 532/1000 | Loss: 0.00003889
Iteration 533/1000 | Loss: 0.00003631
Iteration 534/1000 | Loss: 0.00003806
Iteration 535/1000 | Loss: 0.00003473
Iteration 536/1000 | Loss: 0.00004236
Iteration 537/1000 | Loss: 0.00003575
Iteration 538/1000 | Loss: 0.00004225
Iteration 539/1000 | Loss: 0.00004225
Iteration 540/1000 | Loss: 0.00003593
Iteration 541/1000 | Loss: 0.00004073
Iteration 542/1000 | Loss: 0.00003620
Iteration 543/1000 | Loss: 0.00003620
Iteration 544/1000 | Loss: 0.00003553
Iteration 545/1000 | Loss: 0.00004395
Iteration 546/1000 | Loss: 0.00004062
Iteration 547/1000 | Loss: 0.00003940
Iteration 548/1000 | Loss: 0.00003538
Iteration 549/1000 | Loss: 0.00004570
Iteration 550/1000 | Loss: 0.00003510
Iteration 551/1000 | Loss: 0.00003573
Iteration 552/1000 | Loss: 0.00003521
Iteration 553/1000 | Loss: 0.00004590
Iteration 554/1000 | Loss: 0.00003840
Iteration 555/1000 | Loss: 0.00003613
Iteration 556/1000 | Loss: 0.00003467
Iteration 557/1000 | Loss: 0.00003431
Iteration 558/1000 | Loss: 0.00003970
Iteration 559/1000 | Loss: 0.00003565
Iteration 560/1000 | Loss: 0.00003590
Iteration 561/1000 | Loss: 0.00006879
Iteration 562/1000 | Loss: 0.00003732
Iteration 563/1000 | Loss: 0.00003427
Iteration 564/1000 | Loss: 0.00003425
Iteration 565/1000 | Loss: 0.00003424
Iteration 566/1000 | Loss: 0.00004073
Iteration 567/1000 | Loss: 0.00003474
Iteration 568/1000 | Loss: 0.00003608
Iteration 569/1000 | Loss: 0.00003512
Iteration 570/1000 | Loss: 0.00006287
Iteration 571/1000 | Loss: 0.00004246
Iteration 572/1000 | Loss: 0.00007263
Iteration 573/1000 | Loss: 0.00004432
Iteration 574/1000 | Loss: 0.00004560
Iteration 575/1000 | Loss: 0.00003565
Iteration 576/1000 | Loss: 0.00003523
Iteration 577/1000 | Loss: 0.00004465
Iteration 578/1000 | Loss: 0.00003619
Iteration 579/1000 | Loss: 0.00004136
Iteration 580/1000 | Loss: 0.00003580
Iteration 581/1000 | Loss: 0.00004045
Iteration 582/1000 | Loss: 0.00003438
Iteration 583/1000 | Loss: 0.00003438
Iteration 584/1000 | Loss: 0.00003437
Iteration 585/1000 | Loss: 0.00003434
Iteration 586/1000 | Loss: 0.00003433
Iteration 587/1000 | Loss: 0.00003433
Iteration 588/1000 | Loss: 0.00003433
Iteration 589/1000 | Loss: 0.00003432
Iteration 590/1000 | Loss: 0.00003432
Iteration 591/1000 | Loss: 0.00003432
Iteration 592/1000 | Loss: 0.00003431
Iteration 593/1000 | Loss: 0.00003431
Iteration 594/1000 | Loss: 0.00003430
Iteration 595/1000 | Loss: 0.00003589
Iteration 596/1000 | Loss: 0.00003596
Iteration 597/1000 | Loss: 0.00003547
Iteration 598/1000 | Loss: 0.00003452
Iteration 599/1000 | Loss: 0.00003523
Iteration 600/1000 | Loss: 0.00003562
Iteration 601/1000 | Loss: 0.00003538
Iteration 602/1000 | Loss: 0.00003450
Iteration 603/1000 | Loss: 0.00003638
Iteration 604/1000 | Loss: 0.00003551
Iteration 605/1000 | Loss: 0.00003568
Iteration 606/1000 | Loss: 0.00003568
Iteration 607/1000 | Loss: 0.00003545
Iteration 608/1000 | Loss: 0.00003457
Iteration 609/1000 | Loss: 0.00005018
Iteration 610/1000 | Loss: 0.00005018
Iteration 611/1000 | Loss: 0.00004376
Iteration 612/1000 | Loss: 0.00004482
Iteration 613/1000 | Loss: 0.00003831
Iteration 614/1000 | Loss: 0.00003538
Iteration 615/1000 | Loss: 0.00003610
Iteration 616/1000 | Loss: 0.00003563
Iteration 617/1000 | Loss: 0.00003611
Iteration 618/1000 | Loss: 0.00003561
Iteration 619/1000 | Loss: 0.00003645
Iteration 620/1000 | Loss: 0.00003555
Iteration 621/1000 | Loss: 0.00003656
Iteration 622/1000 | Loss: 0.00003623
Iteration 623/1000 | Loss: 0.00003672
Iteration 624/1000 | Loss: 0.00004450
Iteration 625/1000 | Loss: 0.00003721
Iteration 626/1000 | Loss: 0.00003649
Iteration 627/1000 | Loss: 0.00003688
Iteration 628/1000 | Loss: 0.00003752
Iteration 629/1000 | Loss: 0.00003680
Iteration 630/1000 | Loss: 0.00003556
Iteration 631/1000 | Loss: 0.00003652
Iteration 632/1000 | Loss: 0.00003738
Iteration 633/1000 | Loss: 0.00003731
Iteration 634/1000 | Loss: 0.00003720
Iteration 635/1000 | Loss: 0.00003643
Iteration 636/1000 | Loss: 0.00003782
Iteration 637/1000 | Loss: 0.00003647
Iteration 638/1000 | Loss: 0.00003547
Iteration 639/1000 | Loss: 0.00003987
Iteration 640/1000 | Loss: 0.00004259
Iteration 641/1000 | Loss: 0.00003633
Iteration 642/1000 | Loss: 0.00004089
Iteration 643/1000 | Loss: 0.00003791
Iteration 644/1000 | Loss: 0.00003438
Iteration 645/1000 | Loss: 0.00003434
Iteration 646/1000 | Loss: 0.00003428
Iteration 647/1000 | Loss: 0.00003428
Iteration 648/1000 | Loss: 0.00003428
Iteration 649/1000 | Loss: 0.00003428
Iteration 650/1000 | Loss: 0.00003428
Iteration 651/1000 | Loss: 0.00003428
Iteration 652/1000 | Loss: 0.00003428
Iteration 653/1000 | Loss: 0.00003428
Iteration 654/1000 | Loss: 0.00003428
Iteration 655/1000 | Loss: 0.00003428
Iteration 656/1000 | Loss: 0.00003428
Iteration 657/1000 | Loss: 0.00003428
Iteration 658/1000 | Loss: 0.00003428
Iteration 659/1000 | Loss: 0.00003427
Iteration 660/1000 | Loss: 0.00003427
Iteration 661/1000 | Loss: 0.00003555
Iteration 662/1000 | Loss: 0.00003629
Iteration 663/1000 | Loss: 0.00003637
Iteration 664/1000 | Loss: 0.00003498
Iteration 665/1000 | Loss: 0.00003574
Iteration 666/1000 | Loss: 0.00003662
Iteration 667/1000 | Loss: 0.00003474
Iteration 668/1000 | Loss: 0.00003474
Iteration 669/1000 | Loss: 0.00003585
Iteration 670/1000 | Loss: 0.00004496
Iteration 671/1000 | Loss: 0.00003766
Iteration 672/1000 | Loss: 0.00003624
Iteration 673/1000 | Loss: 0.00003586
Iteration 674/1000 | Loss: 0.00003580
Iteration 675/1000 | Loss: 0.00003580
Iteration 676/1000 | Loss: 0.00003627
Iteration 677/1000 | Loss: 0.00003546
Iteration 678/1000 | Loss: 0.00003597
Iteration 679/1000 | Loss: 0.00003679
Iteration 680/1000 | Loss: 0.00003673
Iteration 681/1000 | Loss: 0.00004601
Iteration 682/1000 | Loss: 0.00003886
Iteration 683/1000 | Loss: 0.00003648
Iteration 684/1000 | Loss: 0.00003620
Iteration 685/1000 | Loss: 0.00003620
Iteration 686/1000 | Loss: 0.00004161
Iteration 687/1000 | Loss: 0.00003618
Iteration 688/1000 | Loss: 0.00003621
Iteration 689/1000 | Loss: 0.00003585
Iteration 690/1000 | Loss: 0.00003659
Iteration 691/1000 | Loss: 0.00003733
Iteration 692/1000 | Loss: 0.00003628
Iteration 693/1000 | Loss: 0.00003524
Iteration 694/1000 | Loss: 0.00003750
Iteration 695/1000 | Loss: 0.00003678
Iteration 696/1000 | Loss: 0.00003686
Iteration 697/1000 | Loss: 0.00003515
Iteration 698/1000 | Loss: 0.00003860
Iteration 699/1000 | Loss: 0.00004449
Iteration 700/1000 | Loss: 0.00003785
Iteration 701/1000 | Loss: 0.00004559
Iteration 702/1000 | Loss: 0.00003812
Iteration 703/1000 | Loss: 0.00003756
Iteration 704/1000 | Loss: 0.00003766
Iteration 705/1000 | Loss: 0.00003712
Iteration 706/1000 | Loss: 0.00003767
Iteration 707/1000 | Loss: 0.00003695
Iteration 708/1000 | Loss: 0.00003694
Iteration 709/1000 | Loss: 0.00003694
Iteration 710/1000 | Loss: 0.00004175
Iteration 711/1000 | Loss: 0.00003526
Iteration 712/1000 | Loss: 0.00004357
Iteration 713/1000 | Loss: 0.00003798
Iteration 714/1000 | Loss: 0.00003564
Iteration 715/1000 | Loss: 0.00003715
Iteration 716/1000 | Loss: 0.00003506
Iteration 717/1000 | Loss: 0.00003457
Iteration 718/1000 | Loss: 0.00003424
Iteration 719/1000 | Loss: 0.00003709
Iteration 720/1000 | Loss: 0.00003534
Iteration 721/1000 | Loss: 0.00003588
Iteration 722/1000 | Loss: 0.00003430
Iteration 723/1000 | Loss: 0.00003526
Iteration 724/1000 | Loss: 0.00003981
Iteration 725/1000 | Loss: 0.00003668
Iteration 726/1000 | Loss: 0.00004589
Iteration 727/1000 | Loss: 0.00003633
Iteration 728/1000 | Loss: 0.00003539
Iteration 729/1000 | Loss: 0.00003650
Iteration 730/1000 | Loss: 0.00003450
Iteration 731/1000 | Loss: 0.00003661
Iteration 732/1000 | Loss: 0.00003551
Iteration 733/1000 | Loss: 0.00003571
Iteration 734/1000 | Loss: 0.00003485
Iteration 735/1000 | Loss: 0.00003418
Iteration 736/1000 | Loss: 0.00003532
Iteration 737/1000 | Loss: 0.00003667
Iteration 738/1000 | Loss: 0.00004405
Iteration 739/1000 | Loss: 0.00003455
Iteration 740/1000 | Loss: 0.00004631
Iteration 741/1000 | Loss: 0.00003540
Iteration 742/1000 | Loss: 0.00003568
Iteration 743/1000 | Loss: 0.00004017
Iteration 744/1000 | Loss: 0.00003886
Iteration 745/1000 | Loss: 0.00003628
Iteration 746/1000 | Loss: 0.00003808
Iteration 747/1000 | Loss: 0.00003808
Iteration 748/1000 | Loss: 0.00004407
Iteration 749/1000 | Loss: 0.00003884
Iteration 750/1000 | Loss: 0.00003467
Iteration 751/1000 | Loss: 0.00003887
Iteration 752/1000 | Loss: 0.00003597
Iteration 753/1000 | Loss: 0.00003981
Iteration 754/1000 | Loss: 0.00003556
Iteration 755/1000 | Loss: 0.00003715
Iteration 756/1000 | Loss: 0.00003824
Iteration 757/1000 | Loss: 0.00003562
Iteration 758/1000 | Loss: 0.00004006
Iteration 759/1000 | Loss: 0.00003436
Iteration 760/1000 | Loss: 0.00004582
Iteration 761/1000 | Loss: 0.00004300
Iteration 762/1000 | Loss: 0.00003713
Iteration 763/1000 | Loss: 0.00003384
Iteration 764/1000 | Loss: 0.00005478
Iteration 765/1000 | Loss: 0.00003977
Iteration 766/1000 | Loss: 0.00004305
Iteration 767/1000 | Loss: 0.00003905
Iteration 768/1000 | Loss: 0.00003954
Iteration 769/1000 | Loss: 0.00003518
Iteration 770/1000 | Loss: 0.00003768
Iteration 771/1000 | Loss: 0.00003500
Iteration 772/1000 | Loss: 0.00003968
Iteration 773/1000 | Loss: 0.00003902
Iteration 774/1000 | Loss: 0.00003461
Iteration 775/1000 | Loss: 0.00003354
Iteration 776/1000 | Loss: 0.00003354
Iteration 777/1000 | Loss: 0.00003354
Iteration 778/1000 | Loss: 0.00003354
Iteration 779/1000 | Loss: 0.00003354
Iteration 780/1000 | Loss: 0.00003353
Iteration 781/1000 | Loss: 0.00003353
Iteration 782/1000 | Loss: 0.00003351
Iteration 783/1000 | Loss: 0.00003351
Iteration 784/1000 | Loss: 0.00003351
Iteration 785/1000 | Loss: 0.00003351
Iteration 786/1000 | Loss: 0.00003350
Iteration 787/1000 | Loss: 0.00003349
Iteration 788/1000 | Loss: 0.00003453
Iteration 789/1000 | Loss: 0.00003452
Iteration 790/1000 | Loss: 0.00003345
Iteration 791/1000 | Loss: 0.00003345
Iteration 792/1000 | Loss: 0.00003344
Iteration 793/1000 | Loss: 0.00003343
Iteration 794/1000 | Loss: 0.00003343
Iteration 795/1000 | Loss: 0.00003343
Iteration 796/1000 | Loss: 0.00003449
Iteration 797/1000 | Loss: 0.00003449
Iteration 798/1000 | Loss: 0.00003449
Iteration 799/1000 | Loss: 0.00003448
Iteration 800/1000 | Loss: 0.00003447
Iteration 801/1000 | Loss: 0.00003447
Iteration 802/1000 | Loss: 0.00003447
Iteration 803/1000 | Loss: 0.00003340
Iteration 804/1000 | Loss: 0.00003340
Iteration 805/1000 | Loss: 0.00003340
Iteration 806/1000 | Loss: 0.00003340
Iteration 807/1000 | Loss: 0.00003340
Iteration 808/1000 | Loss: 0.00003340
Iteration 809/1000 | Loss: 0.00003340
Iteration 810/1000 | Loss: 0.00003340
Iteration 811/1000 | Loss: 0.00003339
Iteration 812/1000 | Loss: 0.00003339
Iteration 813/1000 | Loss: 0.00003339
Iteration 814/1000 | Loss: 0.00003339
Iteration 815/1000 | Loss: 0.00003339
Iteration 816/1000 | Loss: 0.00003339
Iteration 817/1000 | Loss: 0.00003338
Iteration 818/1000 | Loss: 0.00003338
Iteration 819/1000 | Loss: 0.00003338
Iteration 820/1000 | Loss: 0.00003338
Iteration 821/1000 | Loss: 0.00017093
Iteration 822/1000 | Loss: 0.00012680
Iteration 823/1000 | Loss: 0.00003337
Iteration 824/1000 | Loss: 0.00003337
Iteration 825/1000 | Loss: 0.00003337
Iteration 826/1000 | Loss: 0.00003337
Iteration 827/1000 | Loss: 0.00003337
Iteration 828/1000 | Loss: 0.00003337
Iteration 829/1000 | Loss: 0.00003337
Iteration 830/1000 | Loss: 0.00003337
Iteration 831/1000 | Loss: 0.00003337
Iteration 832/1000 | Loss: 0.00003337
Iteration 833/1000 | Loss: 0.00003337
Iteration 834/1000 | Loss: 0.00003337
Iteration 835/1000 | Loss: 0.00003336
Iteration 836/1000 | Loss: 0.00003336
Iteration 837/1000 | Loss: 0.00003336
Iteration 838/1000 | Loss: 0.00003336
Iteration 839/1000 | Loss: 0.00003336
Iteration 840/1000 | Loss: 0.00003336
Iteration 841/1000 | Loss: 0.00003336
Iteration 842/1000 | Loss: 0.00003336
Iteration 843/1000 | Loss: 0.00003336
Iteration 844/1000 | Loss: 0.00003336
Iteration 845/1000 | Loss: 0.00003335
Iteration 846/1000 | Loss: 0.00003335
Iteration 847/1000 | Loss: 0.00003335
Iteration 848/1000 | Loss: 0.00003335
Iteration 849/1000 | Loss: 0.00003335
Iteration 850/1000 | Loss: 0.00003335
Iteration 851/1000 | Loss: 0.00003335
Iteration 852/1000 | Loss: 0.00003334
Iteration 853/1000 | Loss: 0.00003334
Iteration 854/1000 | Loss: 0.00003334
Iteration 855/1000 | Loss: 0.00003334
Iteration 856/1000 | Loss: 0.00003334
Iteration 857/1000 | Loss: 0.00003334
Iteration 858/1000 | Loss: 0.00003334
Iteration 859/1000 | Loss: 0.00003334
Iteration 860/1000 | Loss: 0.00003334
Iteration 861/1000 | Loss: 0.00003334
Iteration 862/1000 | Loss: 0.00003334
Iteration 863/1000 | Loss: 0.00003333
Iteration 864/1000 | Loss: 0.00003333
Iteration 865/1000 | Loss: 0.00003333
Iteration 866/1000 | Loss: 0.00003333
Iteration 867/1000 | Loss: 0.00003333
Iteration 868/1000 | Loss: 0.00003333
Iteration 869/1000 | Loss: 0.00003333
Iteration 870/1000 | Loss: 0.00003333
Iteration 871/1000 | Loss: 0.00003333
Iteration 872/1000 | Loss: 0.00003333
Iteration 873/1000 | Loss: 0.00003333
Iteration 874/1000 | Loss: 0.00003333
Iteration 875/1000 | Loss: 0.00003333
Iteration 876/1000 | Loss: 0.00003333
Iteration 877/1000 | Loss: 0.00003333
Iteration 878/1000 | Loss: 0.00003333
Iteration 879/1000 | Loss: 0.00003333
Iteration 880/1000 | Loss: 0.00003333
Iteration 881/1000 | Loss: 0.00003333
Iteration 882/1000 | Loss: 0.00003333
Iteration 883/1000 | Loss: 0.00003333
Iteration 884/1000 | Loss: 0.00003333
Iteration 885/1000 | Loss: 0.00003333
Iteration 886/1000 | Loss: 0.00003333
Iteration 887/1000 | Loss: 0.00003333
Iteration 888/1000 | Loss: 0.00003333
Iteration 889/1000 | Loss: 0.00003333
Iteration 890/1000 | Loss: 0.00003333
Iteration 891/1000 | Loss: 0.00003333
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 891. Stopping optimization.
Last 5 losses: [3.333210770506412e-05, 3.333210770506412e-05, 3.333210770506412e-05, 3.333210770506412e-05, 3.333210770506412e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.333210770506412e-05

Optimization complete. Final v2v error: 4.54520845413208 mm

Highest mean error: 13.50658130645752 mm for frame 14

Lowest mean error: 3.545598268508911 mm for frame 174

Saving results

Total time: 1036.2077634334564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00974686
Iteration 2/25 | Loss: 0.00258634
Iteration 3/25 | Loss: 0.00186582
Iteration 4/25 | Loss: 0.00179776
Iteration 5/25 | Loss: 0.00175884
Iteration 6/25 | Loss: 0.00171124
Iteration 7/25 | Loss: 0.00170068
Iteration 8/25 | Loss: 0.00169609
Iteration 9/25 | Loss: 0.00169517
Iteration 10/25 | Loss: 0.00169211
Iteration 11/25 | Loss: 0.00169034
Iteration 12/25 | Loss: 0.00169226
Iteration 13/25 | Loss: 0.00169378
Iteration 14/25 | Loss: 0.00169347
Iteration 15/25 | Loss: 0.00169236
Iteration 16/25 | Loss: 0.00169261
Iteration 17/25 | Loss: 0.00169163
Iteration 18/25 | Loss: 0.00168955
Iteration 19/25 | Loss: 0.00169014
Iteration 20/25 | Loss: 0.00169207
Iteration 21/25 | Loss: 0.00169127
Iteration 22/25 | Loss: 0.00169001
Iteration 23/25 | Loss: 0.00169118
Iteration 24/25 | Loss: 0.00169206
Iteration 25/25 | Loss: 0.00169077

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65924692
Iteration 2/25 | Loss: 0.00268824
Iteration 3/25 | Loss: 0.00268824
Iteration 4/25 | Loss: 0.00268824
Iteration 5/25 | Loss: 0.00268824
Iteration 6/25 | Loss: 0.00268824
Iteration 7/25 | Loss: 0.00268824
Iteration 8/25 | Loss: 0.00268824
Iteration 9/25 | Loss: 0.00268824
Iteration 10/25 | Loss: 0.00268824
Iteration 11/25 | Loss: 0.00268824
Iteration 12/25 | Loss: 0.00268824
Iteration 13/25 | Loss: 0.00268824
Iteration 14/25 | Loss: 0.00268824
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0026882393285632133, 0.0026882393285632133, 0.0026882393285632133, 0.0026882393285632133, 0.0026882393285632133]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026882393285632133

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268824
Iteration 2/1000 | Loss: 0.00009222
Iteration 3/1000 | Loss: 0.00007733
Iteration 4/1000 | Loss: 0.00008087
Iteration 5/1000 | Loss: 0.00006616
Iteration 6/1000 | Loss: 0.00006834
Iteration 7/1000 | Loss: 0.00006408
Iteration 8/1000 | Loss: 0.00007140
Iteration 9/1000 | Loss: 0.00009832
Iteration 10/1000 | Loss: 0.00006618
Iteration 11/1000 | Loss: 0.00005681
Iteration 12/1000 | Loss: 0.00005232
Iteration 13/1000 | Loss: 0.00007459
Iteration 14/1000 | Loss: 0.00006259
Iteration 15/1000 | Loss: 0.00007698
Iteration 16/1000 | Loss: 0.00007345
Iteration 17/1000 | Loss: 0.00005197
Iteration 18/1000 | Loss: 0.00006969
Iteration 19/1000 | Loss: 0.00008003
Iteration 20/1000 | Loss: 0.00007525
Iteration 21/1000 | Loss: 0.00005976
Iteration 22/1000 | Loss: 0.00006219
Iteration 23/1000 | Loss: 0.00005994
Iteration 24/1000 | Loss: 0.00008101
Iteration 25/1000 | Loss: 0.00007788
Iteration 26/1000 | Loss: 0.00008174
Iteration 27/1000 | Loss: 0.00007135
Iteration 28/1000 | Loss: 0.00008275
Iteration 29/1000 | Loss: 0.00006679
Iteration 30/1000 | Loss: 0.00007905
Iteration 31/1000 | Loss: 0.00005594
Iteration 32/1000 | Loss: 0.00005311
Iteration 33/1000 | Loss: 0.00005532
Iteration 34/1000 | Loss: 0.00007600
Iteration 35/1000 | Loss: 0.00008037
Iteration 36/1000 | Loss: 0.00004736
Iteration 37/1000 | Loss: 0.00004213
Iteration 38/1000 | Loss: 0.00003972
Iteration 39/1000 | Loss: 0.00003858
Iteration 40/1000 | Loss: 0.00009085
Iteration 41/1000 | Loss: 0.00004157
Iteration 42/1000 | Loss: 0.00003689
Iteration 43/1000 | Loss: 0.00003551
Iteration 44/1000 | Loss: 0.00003481
Iteration 45/1000 | Loss: 0.00003432
Iteration 46/1000 | Loss: 0.00003401
Iteration 47/1000 | Loss: 0.00003376
Iteration 48/1000 | Loss: 0.00003361
Iteration 49/1000 | Loss: 0.00003353
Iteration 50/1000 | Loss: 0.00003348
Iteration 51/1000 | Loss: 0.00003332
Iteration 52/1000 | Loss: 0.00003329
Iteration 53/1000 | Loss: 0.00003328
Iteration 54/1000 | Loss: 0.00003328
Iteration 55/1000 | Loss: 0.00003326
Iteration 56/1000 | Loss: 0.00003325
Iteration 57/1000 | Loss: 0.00003324
Iteration 58/1000 | Loss: 0.00003315
Iteration 59/1000 | Loss: 0.00003312
Iteration 60/1000 | Loss: 0.00003311
Iteration 61/1000 | Loss: 0.00003310
Iteration 62/1000 | Loss: 0.00003310
Iteration 63/1000 | Loss: 0.00003310
Iteration 64/1000 | Loss: 0.00003309
Iteration 65/1000 | Loss: 0.00003309
Iteration 66/1000 | Loss: 0.00003309
Iteration 67/1000 | Loss: 0.00003309
Iteration 68/1000 | Loss: 0.00003309
Iteration 69/1000 | Loss: 0.00003308
Iteration 70/1000 | Loss: 0.00003308
Iteration 71/1000 | Loss: 0.00003308
Iteration 72/1000 | Loss: 0.00003308
Iteration 73/1000 | Loss: 0.00003308
Iteration 74/1000 | Loss: 0.00003308
Iteration 75/1000 | Loss: 0.00003307
Iteration 76/1000 | Loss: 0.00003307
Iteration 77/1000 | Loss: 0.00003307
Iteration 78/1000 | Loss: 0.00003306
Iteration 79/1000 | Loss: 0.00003306
Iteration 80/1000 | Loss: 0.00003306
Iteration 81/1000 | Loss: 0.00003306
Iteration 82/1000 | Loss: 0.00003306
Iteration 83/1000 | Loss: 0.00003305
Iteration 84/1000 | Loss: 0.00003305
Iteration 85/1000 | Loss: 0.00003304
Iteration 86/1000 | Loss: 0.00003304
Iteration 87/1000 | Loss: 0.00003304
Iteration 88/1000 | Loss: 0.00003304
Iteration 89/1000 | Loss: 0.00003304
Iteration 90/1000 | Loss: 0.00003304
Iteration 91/1000 | Loss: 0.00003304
Iteration 92/1000 | Loss: 0.00003304
Iteration 93/1000 | Loss: 0.00003304
Iteration 94/1000 | Loss: 0.00003304
Iteration 95/1000 | Loss: 0.00003304
Iteration 96/1000 | Loss: 0.00003304
Iteration 97/1000 | Loss: 0.00003304
Iteration 98/1000 | Loss: 0.00003303
Iteration 99/1000 | Loss: 0.00003303
Iteration 100/1000 | Loss: 0.00003303
Iteration 101/1000 | Loss: 0.00003303
Iteration 102/1000 | Loss: 0.00003303
Iteration 103/1000 | Loss: 0.00003303
Iteration 104/1000 | Loss: 0.00003303
Iteration 105/1000 | Loss: 0.00003303
Iteration 106/1000 | Loss: 0.00003303
Iteration 107/1000 | Loss: 0.00003303
Iteration 108/1000 | Loss: 0.00003303
Iteration 109/1000 | Loss: 0.00003303
Iteration 110/1000 | Loss: 0.00003303
Iteration 111/1000 | Loss: 0.00003303
Iteration 112/1000 | Loss: 0.00003302
Iteration 113/1000 | Loss: 0.00003302
Iteration 114/1000 | Loss: 0.00003302
Iteration 115/1000 | Loss: 0.00003302
Iteration 116/1000 | Loss: 0.00003302
Iteration 117/1000 | Loss: 0.00003302
Iteration 118/1000 | Loss: 0.00003302
Iteration 119/1000 | Loss: 0.00003302
Iteration 120/1000 | Loss: 0.00003302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [3.302244294900447e-05, 3.302244294900447e-05, 3.302244294900447e-05, 3.302244294900447e-05, 3.302244294900447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.302244294900447e-05

Optimization complete. Final v2v error: 4.986039161682129 mm

Highest mean error: 6.178483963012695 mm for frame 193

Lowest mean error: 4.291882514953613 mm for frame 82

Saving results

Total time: 135.94076681137085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01005648
Iteration 2/25 | Loss: 0.00186016
Iteration 3/25 | Loss: 0.00175629
Iteration 4/25 | Loss: 0.00173076
Iteration 5/25 | Loss: 0.00172338
Iteration 6/25 | Loss: 0.00172145
Iteration 7/25 | Loss: 0.00172117
Iteration 8/25 | Loss: 0.00172117
Iteration 9/25 | Loss: 0.00172117
Iteration 10/25 | Loss: 0.00172117
Iteration 11/25 | Loss: 0.00172117
Iteration 12/25 | Loss: 0.00172117
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017211722442880273, 0.0017211722442880273, 0.0017211722442880273, 0.0017211722442880273, 0.0017211722442880273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017211722442880273

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.78137279
Iteration 2/25 | Loss: 0.00275217
Iteration 3/25 | Loss: 0.00275215
Iteration 4/25 | Loss: 0.00275215
Iteration 5/25 | Loss: 0.00275215
Iteration 6/25 | Loss: 0.00275215
Iteration 7/25 | Loss: 0.00275215
Iteration 8/25 | Loss: 0.00275215
Iteration 9/25 | Loss: 0.00275215
Iteration 10/25 | Loss: 0.00275215
Iteration 11/25 | Loss: 0.00275215
Iteration 12/25 | Loss: 0.00275215
Iteration 13/25 | Loss: 0.00275215
Iteration 14/25 | Loss: 0.00275215
Iteration 15/25 | Loss: 0.00275215
Iteration 16/25 | Loss: 0.00275215
Iteration 17/25 | Loss: 0.00275215
Iteration 18/25 | Loss: 0.00275215
Iteration 19/25 | Loss: 0.00275215
Iteration 20/25 | Loss: 0.00275215
Iteration 21/25 | Loss: 0.00275215
Iteration 22/25 | Loss: 0.00275215
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002752146916463971, 0.002752146916463971, 0.002752146916463971, 0.002752146916463971, 0.002752146916463971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002752146916463971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00275215
Iteration 2/1000 | Loss: 0.00007102
Iteration 3/1000 | Loss: 0.00004418
Iteration 4/1000 | Loss: 0.00003889
Iteration 5/1000 | Loss: 0.00003671
Iteration 6/1000 | Loss: 0.00003507
Iteration 7/1000 | Loss: 0.00003428
Iteration 8/1000 | Loss: 0.00003350
Iteration 9/1000 | Loss: 0.00003305
Iteration 10/1000 | Loss: 0.00003256
Iteration 11/1000 | Loss: 0.00003224
Iteration 12/1000 | Loss: 0.00003209
Iteration 13/1000 | Loss: 0.00003205
Iteration 14/1000 | Loss: 0.00003204
Iteration 15/1000 | Loss: 0.00003200
Iteration 16/1000 | Loss: 0.00003199
Iteration 17/1000 | Loss: 0.00003195
Iteration 18/1000 | Loss: 0.00003191
Iteration 19/1000 | Loss: 0.00003191
Iteration 20/1000 | Loss: 0.00003190
Iteration 21/1000 | Loss: 0.00003190
Iteration 22/1000 | Loss: 0.00003188
Iteration 23/1000 | Loss: 0.00003187
Iteration 24/1000 | Loss: 0.00003187
Iteration 25/1000 | Loss: 0.00003186
Iteration 26/1000 | Loss: 0.00003186
Iteration 27/1000 | Loss: 0.00003186
Iteration 28/1000 | Loss: 0.00003185
Iteration 29/1000 | Loss: 0.00003182
Iteration 30/1000 | Loss: 0.00003182
Iteration 31/1000 | Loss: 0.00003181
Iteration 32/1000 | Loss: 0.00003181
Iteration 33/1000 | Loss: 0.00003177
Iteration 34/1000 | Loss: 0.00003171
Iteration 35/1000 | Loss: 0.00003171
Iteration 36/1000 | Loss: 0.00003171
Iteration 37/1000 | Loss: 0.00003170
Iteration 38/1000 | Loss: 0.00003170
Iteration 39/1000 | Loss: 0.00003170
Iteration 40/1000 | Loss: 0.00003169
Iteration 41/1000 | Loss: 0.00003169
Iteration 42/1000 | Loss: 0.00003169
Iteration 43/1000 | Loss: 0.00003168
Iteration 44/1000 | Loss: 0.00003168
Iteration 45/1000 | Loss: 0.00003168
Iteration 46/1000 | Loss: 0.00003167
Iteration 47/1000 | Loss: 0.00003167
Iteration 48/1000 | Loss: 0.00003167
Iteration 49/1000 | Loss: 0.00003166
Iteration 50/1000 | Loss: 0.00003166
Iteration 51/1000 | Loss: 0.00003165
Iteration 52/1000 | Loss: 0.00003165
Iteration 53/1000 | Loss: 0.00003164
Iteration 54/1000 | Loss: 0.00003164
Iteration 55/1000 | Loss: 0.00003164
Iteration 56/1000 | Loss: 0.00003163
Iteration 57/1000 | Loss: 0.00003163
Iteration 58/1000 | Loss: 0.00003162
Iteration 59/1000 | Loss: 0.00003162
Iteration 60/1000 | Loss: 0.00003162
Iteration 61/1000 | Loss: 0.00003162
Iteration 62/1000 | Loss: 0.00003162
Iteration 63/1000 | Loss: 0.00003162
Iteration 64/1000 | Loss: 0.00003161
Iteration 65/1000 | Loss: 0.00003161
Iteration 66/1000 | Loss: 0.00003161
Iteration 67/1000 | Loss: 0.00003160
Iteration 68/1000 | Loss: 0.00003160
Iteration 69/1000 | Loss: 0.00003160
Iteration 70/1000 | Loss: 0.00003160
Iteration 71/1000 | Loss: 0.00003159
Iteration 72/1000 | Loss: 0.00003159
Iteration 73/1000 | Loss: 0.00003159
Iteration 74/1000 | Loss: 0.00003158
Iteration 75/1000 | Loss: 0.00003158
Iteration 76/1000 | Loss: 0.00003158
Iteration 77/1000 | Loss: 0.00003158
Iteration 78/1000 | Loss: 0.00003158
Iteration 79/1000 | Loss: 0.00003158
Iteration 80/1000 | Loss: 0.00003158
Iteration 81/1000 | Loss: 0.00003158
Iteration 82/1000 | Loss: 0.00003157
Iteration 83/1000 | Loss: 0.00003157
Iteration 84/1000 | Loss: 0.00003157
Iteration 85/1000 | Loss: 0.00003156
Iteration 86/1000 | Loss: 0.00003156
Iteration 87/1000 | Loss: 0.00003156
Iteration 88/1000 | Loss: 0.00003156
Iteration 89/1000 | Loss: 0.00003156
Iteration 90/1000 | Loss: 0.00003156
Iteration 91/1000 | Loss: 0.00003156
Iteration 92/1000 | Loss: 0.00003156
Iteration 93/1000 | Loss: 0.00003156
Iteration 94/1000 | Loss: 0.00003156
Iteration 95/1000 | Loss: 0.00003156
Iteration 96/1000 | Loss: 0.00003156
Iteration 97/1000 | Loss: 0.00003156
Iteration 98/1000 | Loss: 0.00003156
Iteration 99/1000 | Loss: 0.00003156
Iteration 100/1000 | Loss: 0.00003155
Iteration 101/1000 | Loss: 0.00003155
Iteration 102/1000 | Loss: 0.00003155
Iteration 103/1000 | Loss: 0.00003155
Iteration 104/1000 | Loss: 0.00003155
Iteration 105/1000 | Loss: 0.00003155
Iteration 106/1000 | Loss: 0.00003155
Iteration 107/1000 | Loss: 0.00003155
Iteration 108/1000 | Loss: 0.00003155
Iteration 109/1000 | Loss: 0.00003155
Iteration 110/1000 | Loss: 0.00003155
Iteration 111/1000 | Loss: 0.00003155
Iteration 112/1000 | Loss: 0.00003155
Iteration 113/1000 | Loss: 0.00003155
Iteration 114/1000 | Loss: 0.00003155
Iteration 115/1000 | Loss: 0.00003154
Iteration 116/1000 | Loss: 0.00003154
Iteration 117/1000 | Loss: 0.00003154
Iteration 118/1000 | Loss: 0.00003154
Iteration 119/1000 | Loss: 0.00003154
Iteration 120/1000 | Loss: 0.00003154
Iteration 121/1000 | Loss: 0.00003154
Iteration 122/1000 | Loss: 0.00003154
Iteration 123/1000 | Loss: 0.00003154
Iteration 124/1000 | Loss: 0.00003154
Iteration 125/1000 | Loss: 0.00003154
Iteration 126/1000 | Loss: 0.00003154
Iteration 127/1000 | Loss: 0.00003154
Iteration 128/1000 | Loss: 0.00003154
Iteration 129/1000 | Loss: 0.00003154
Iteration 130/1000 | Loss: 0.00003154
Iteration 131/1000 | Loss: 0.00003154
Iteration 132/1000 | Loss: 0.00003154
Iteration 133/1000 | Loss: 0.00003154
Iteration 134/1000 | Loss: 0.00003154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [3.1543524528387934e-05, 3.1543524528387934e-05, 3.1543524528387934e-05, 3.1543524528387934e-05, 3.1543524528387934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1543524528387934e-05

Optimization complete. Final v2v error: 4.8616790771484375 mm

Highest mean error: 5.211140155792236 mm for frame 37

Lowest mean error: 4.688370227813721 mm for frame 16

Saving results

Total time: 35.711655616760254
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053439
Iteration 2/25 | Loss: 0.00223068
Iteration 3/25 | Loss: 0.00183631
Iteration 4/25 | Loss: 0.00179624
Iteration 5/25 | Loss: 0.00179184
Iteration 6/25 | Loss: 0.00179123
Iteration 7/25 | Loss: 0.00179123
Iteration 8/25 | Loss: 0.00179123
Iteration 9/25 | Loss: 0.00179123
Iteration 10/25 | Loss: 0.00179123
Iteration 11/25 | Loss: 0.00179123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0017912256298586726, 0.0017912256298586726, 0.0017912256298586726, 0.0017912256298586726, 0.0017912256298586726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017912256298586726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.61256218
Iteration 2/25 | Loss: 0.00359713
Iteration 3/25 | Loss: 0.00359710
Iteration 4/25 | Loss: 0.00359709
Iteration 5/25 | Loss: 0.00359709
Iteration 6/25 | Loss: 0.00359709
Iteration 7/25 | Loss: 0.00359709
Iteration 8/25 | Loss: 0.00359709
Iteration 9/25 | Loss: 0.00359709
Iteration 10/25 | Loss: 0.00359709
Iteration 11/25 | Loss: 0.00359709
Iteration 12/25 | Loss: 0.00359709
Iteration 13/25 | Loss: 0.00359709
Iteration 14/25 | Loss: 0.00359709
Iteration 15/25 | Loss: 0.00359709
Iteration 16/25 | Loss: 0.00359709
Iteration 17/25 | Loss: 0.00359709
Iteration 18/25 | Loss: 0.00359709
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0035970937460660934, 0.0035970937460660934, 0.0035970937460660934, 0.0035970937460660934, 0.0035970937460660934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035970937460660934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00359709
Iteration 2/1000 | Loss: 0.00006423
Iteration 3/1000 | Loss: 0.00004821
Iteration 4/1000 | Loss: 0.00004137
Iteration 5/1000 | Loss: 0.00003818
Iteration 6/1000 | Loss: 0.00003629
Iteration 7/1000 | Loss: 0.00003503
Iteration 8/1000 | Loss: 0.00003445
Iteration 9/1000 | Loss: 0.00003351
Iteration 10/1000 | Loss: 0.00003274
Iteration 11/1000 | Loss: 0.00003228
Iteration 12/1000 | Loss: 0.00003180
Iteration 13/1000 | Loss: 0.00003154
Iteration 14/1000 | Loss: 0.00003125
Iteration 15/1000 | Loss: 0.00003102
Iteration 16/1000 | Loss: 0.00003080
Iteration 17/1000 | Loss: 0.00003058
Iteration 18/1000 | Loss: 0.00003043
Iteration 19/1000 | Loss: 0.00003036
Iteration 20/1000 | Loss: 0.00003029
Iteration 21/1000 | Loss: 0.00003028
Iteration 22/1000 | Loss: 0.00003015
Iteration 23/1000 | Loss: 0.00003013
Iteration 24/1000 | Loss: 0.00003012
Iteration 25/1000 | Loss: 0.00003011
Iteration 26/1000 | Loss: 0.00003011
Iteration 27/1000 | Loss: 0.00003007
Iteration 28/1000 | Loss: 0.00003006
Iteration 29/1000 | Loss: 0.00003006
Iteration 30/1000 | Loss: 0.00003006
Iteration 31/1000 | Loss: 0.00003006
Iteration 32/1000 | Loss: 0.00003005
Iteration 33/1000 | Loss: 0.00003005
Iteration 34/1000 | Loss: 0.00003005
Iteration 35/1000 | Loss: 0.00003005
Iteration 36/1000 | Loss: 0.00003005
Iteration 37/1000 | Loss: 0.00003005
Iteration 38/1000 | Loss: 0.00003005
Iteration 39/1000 | Loss: 0.00003005
Iteration 40/1000 | Loss: 0.00003005
Iteration 41/1000 | Loss: 0.00003005
Iteration 42/1000 | Loss: 0.00003005
Iteration 43/1000 | Loss: 0.00003004
Iteration 44/1000 | Loss: 0.00003004
Iteration 45/1000 | Loss: 0.00003004
Iteration 46/1000 | Loss: 0.00003004
Iteration 47/1000 | Loss: 0.00003004
Iteration 48/1000 | Loss: 0.00003004
Iteration 49/1000 | Loss: 0.00003004
Iteration 50/1000 | Loss: 0.00003004
Iteration 51/1000 | Loss: 0.00003003
Iteration 52/1000 | Loss: 0.00003003
Iteration 53/1000 | Loss: 0.00003003
Iteration 54/1000 | Loss: 0.00003003
Iteration 55/1000 | Loss: 0.00003003
Iteration 56/1000 | Loss: 0.00003003
Iteration 57/1000 | Loss: 0.00003003
Iteration 58/1000 | Loss: 0.00003003
Iteration 59/1000 | Loss: 0.00003003
Iteration 60/1000 | Loss: 0.00003003
Iteration 61/1000 | Loss: 0.00003002
Iteration 62/1000 | Loss: 0.00003002
Iteration 63/1000 | Loss: 0.00003002
Iteration 64/1000 | Loss: 0.00003002
Iteration 65/1000 | Loss: 0.00003002
Iteration 66/1000 | Loss: 0.00003002
Iteration 67/1000 | Loss: 0.00003002
Iteration 68/1000 | Loss: 0.00003002
Iteration 69/1000 | Loss: 0.00003002
Iteration 70/1000 | Loss: 0.00003002
Iteration 71/1000 | Loss: 0.00003001
Iteration 72/1000 | Loss: 0.00003001
Iteration 73/1000 | Loss: 0.00003001
Iteration 74/1000 | Loss: 0.00003001
Iteration 75/1000 | Loss: 0.00003001
Iteration 76/1000 | Loss: 0.00003001
Iteration 77/1000 | Loss: 0.00003001
Iteration 78/1000 | Loss: 0.00003001
Iteration 79/1000 | Loss: 0.00003001
Iteration 80/1000 | Loss: 0.00003001
Iteration 81/1000 | Loss: 0.00003001
Iteration 82/1000 | Loss: 0.00003001
Iteration 83/1000 | Loss: 0.00003001
Iteration 84/1000 | Loss: 0.00003001
Iteration 85/1000 | Loss: 0.00003001
Iteration 86/1000 | Loss: 0.00003001
Iteration 87/1000 | Loss: 0.00003001
Iteration 88/1000 | Loss: 0.00003001
Iteration 89/1000 | Loss: 0.00003001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [3.0011055059731007e-05, 3.0011055059731007e-05, 3.0011055059731007e-05, 3.0011055059731007e-05, 3.0011055059731007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0011055059731007e-05

Optimization complete. Final v2v error: 4.767958164215088 mm

Highest mean error: 4.978953838348389 mm for frame 0

Lowest mean error: 4.5792646408081055 mm for frame 103

Saving results

Total time: 39.61702752113342
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966052
Iteration 2/25 | Loss: 0.00205789
Iteration 3/25 | Loss: 0.00173089
Iteration 4/25 | Loss: 0.00168973
Iteration 5/25 | Loss: 0.00168738
Iteration 6/25 | Loss: 0.00167656
Iteration 7/25 | Loss: 0.00167347
Iteration 8/25 | Loss: 0.00166554
Iteration 9/25 | Loss: 0.00165810
Iteration 10/25 | Loss: 0.00165552
Iteration 11/25 | Loss: 0.00166356
Iteration 12/25 | Loss: 0.00165887
Iteration 13/25 | Loss: 0.00165461
Iteration 14/25 | Loss: 0.00164961
Iteration 15/25 | Loss: 0.00164937
Iteration 16/25 | Loss: 0.00164987
Iteration 17/25 | Loss: 0.00164762
Iteration 18/25 | Loss: 0.00164559
Iteration 19/25 | Loss: 0.00164515
Iteration 20/25 | Loss: 0.00164489
Iteration 21/25 | Loss: 0.00164474
Iteration 22/25 | Loss: 0.00164474
Iteration 23/25 | Loss: 0.00164474
Iteration 24/25 | Loss: 0.00164474
Iteration 25/25 | Loss: 0.00164474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83899581
Iteration 2/25 | Loss: 0.00301178
Iteration 3/25 | Loss: 0.00301178
Iteration 4/25 | Loss: 0.00301178
Iteration 5/25 | Loss: 0.00301178
Iteration 6/25 | Loss: 0.00301178
Iteration 7/25 | Loss: 0.00301177
Iteration 8/25 | Loss: 0.00301177
Iteration 9/25 | Loss: 0.00301177
Iteration 10/25 | Loss: 0.00301177
Iteration 11/25 | Loss: 0.00301177
Iteration 12/25 | Loss: 0.00301177
Iteration 13/25 | Loss: 0.00301177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.003011774504557252, 0.003011774504557252, 0.003011774504557252, 0.003011774504557252, 0.003011774504557252]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003011774504557252

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00301177
Iteration 2/1000 | Loss: 0.00011974
Iteration 3/1000 | Loss: 0.00008241
Iteration 4/1000 | Loss: 0.00006963
Iteration 5/1000 | Loss: 0.00006270
Iteration 6/1000 | Loss: 0.00005867
Iteration 7/1000 | Loss: 0.00031927
Iteration 8/1000 | Loss: 0.00106998
Iteration 9/1000 | Loss: 0.00041272
Iteration 10/1000 | Loss: 0.00006540
Iteration 11/1000 | Loss: 0.00005185
Iteration 12/1000 | Loss: 0.00004540
Iteration 13/1000 | Loss: 0.00004137
Iteration 14/1000 | Loss: 0.00003880
Iteration 15/1000 | Loss: 0.00003755
Iteration 16/1000 | Loss: 0.00003706
Iteration 17/1000 | Loss: 0.00003668
Iteration 18/1000 | Loss: 0.00003639
Iteration 19/1000 | Loss: 0.00003615
Iteration 20/1000 | Loss: 0.00003593
Iteration 21/1000 | Loss: 0.00003585
Iteration 22/1000 | Loss: 0.00003574
Iteration 23/1000 | Loss: 0.00003560
Iteration 24/1000 | Loss: 0.00003557
Iteration 25/1000 | Loss: 0.00003554
Iteration 26/1000 | Loss: 0.00003554
Iteration 27/1000 | Loss: 0.00003553
Iteration 28/1000 | Loss: 0.00003553
Iteration 29/1000 | Loss: 0.00003552
Iteration 30/1000 | Loss: 0.00003552
Iteration 31/1000 | Loss: 0.00003551
Iteration 32/1000 | Loss: 0.00003551
Iteration 33/1000 | Loss: 0.00003550
Iteration 34/1000 | Loss: 0.00003549
Iteration 35/1000 | Loss: 0.00003549
Iteration 36/1000 | Loss: 0.00003548
Iteration 37/1000 | Loss: 0.00003547
Iteration 38/1000 | Loss: 0.00003547
Iteration 39/1000 | Loss: 0.00003547
Iteration 40/1000 | Loss: 0.00003547
Iteration 41/1000 | Loss: 0.00003547
Iteration 42/1000 | Loss: 0.00003546
Iteration 43/1000 | Loss: 0.00003546
Iteration 44/1000 | Loss: 0.00003546
Iteration 45/1000 | Loss: 0.00003545
Iteration 46/1000 | Loss: 0.00003545
Iteration 47/1000 | Loss: 0.00003545
Iteration 48/1000 | Loss: 0.00003545
Iteration 49/1000 | Loss: 0.00003544
Iteration 50/1000 | Loss: 0.00003544
Iteration 51/1000 | Loss: 0.00003544
Iteration 52/1000 | Loss: 0.00003544
Iteration 53/1000 | Loss: 0.00003544
Iteration 54/1000 | Loss: 0.00003544
Iteration 55/1000 | Loss: 0.00003544
Iteration 56/1000 | Loss: 0.00003543
Iteration 57/1000 | Loss: 0.00003543
Iteration 58/1000 | Loss: 0.00003543
Iteration 59/1000 | Loss: 0.00003543
Iteration 60/1000 | Loss: 0.00003543
Iteration 61/1000 | Loss: 0.00003543
Iteration 62/1000 | Loss: 0.00003543
Iteration 63/1000 | Loss: 0.00003543
Iteration 64/1000 | Loss: 0.00003543
Iteration 65/1000 | Loss: 0.00003543
Iteration 66/1000 | Loss: 0.00003542
Iteration 67/1000 | Loss: 0.00003542
Iteration 68/1000 | Loss: 0.00003542
Iteration 69/1000 | Loss: 0.00003542
Iteration 70/1000 | Loss: 0.00003542
Iteration 71/1000 | Loss: 0.00003542
Iteration 72/1000 | Loss: 0.00003542
Iteration 73/1000 | Loss: 0.00003541
Iteration 74/1000 | Loss: 0.00003541
Iteration 75/1000 | Loss: 0.00003541
Iteration 76/1000 | Loss: 0.00003541
Iteration 77/1000 | Loss: 0.00003541
Iteration 78/1000 | Loss: 0.00003541
Iteration 79/1000 | Loss: 0.00003541
Iteration 80/1000 | Loss: 0.00003540
Iteration 81/1000 | Loss: 0.00003540
Iteration 82/1000 | Loss: 0.00003540
Iteration 83/1000 | Loss: 0.00003540
Iteration 84/1000 | Loss: 0.00003540
Iteration 85/1000 | Loss: 0.00003540
Iteration 86/1000 | Loss: 0.00003540
Iteration 87/1000 | Loss: 0.00003540
Iteration 88/1000 | Loss: 0.00003540
Iteration 89/1000 | Loss: 0.00003540
Iteration 90/1000 | Loss: 0.00003540
Iteration 91/1000 | Loss: 0.00003540
Iteration 92/1000 | Loss: 0.00003540
Iteration 93/1000 | Loss: 0.00003540
Iteration 94/1000 | Loss: 0.00003539
Iteration 95/1000 | Loss: 0.00003539
Iteration 96/1000 | Loss: 0.00003539
Iteration 97/1000 | Loss: 0.00003539
Iteration 98/1000 | Loss: 0.00003539
Iteration 99/1000 | Loss: 0.00003539
Iteration 100/1000 | Loss: 0.00003539
Iteration 101/1000 | Loss: 0.00003539
Iteration 102/1000 | Loss: 0.00003539
Iteration 103/1000 | Loss: 0.00003539
Iteration 104/1000 | Loss: 0.00003539
Iteration 105/1000 | Loss: 0.00003539
Iteration 106/1000 | Loss: 0.00003539
Iteration 107/1000 | Loss: 0.00003539
Iteration 108/1000 | Loss: 0.00003539
Iteration 109/1000 | Loss: 0.00003539
Iteration 110/1000 | Loss: 0.00003539
Iteration 111/1000 | Loss: 0.00003539
Iteration 112/1000 | Loss: 0.00003539
Iteration 113/1000 | Loss: 0.00003539
Iteration 114/1000 | Loss: 0.00003539
Iteration 115/1000 | Loss: 0.00003539
Iteration 116/1000 | Loss: 0.00003539
Iteration 117/1000 | Loss: 0.00003539
Iteration 118/1000 | Loss: 0.00003539
Iteration 119/1000 | Loss: 0.00003539
Iteration 120/1000 | Loss: 0.00003539
Iteration 121/1000 | Loss: 0.00003539
Iteration 122/1000 | Loss: 0.00003539
Iteration 123/1000 | Loss: 0.00003539
Iteration 124/1000 | Loss: 0.00003539
Iteration 125/1000 | Loss: 0.00003539
Iteration 126/1000 | Loss: 0.00003539
Iteration 127/1000 | Loss: 0.00003539
Iteration 128/1000 | Loss: 0.00003539
Iteration 129/1000 | Loss: 0.00003539
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [3.538895180099644e-05, 3.538895180099644e-05, 3.538895180099644e-05, 3.538895180099644e-05, 3.538895180099644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.538895180099644e-05

Optimization complete. Final v2v error: 5.04561185836792 mm

Highest mean error: 12.572381973266602 mm for frame 173

Lowest mean error: 4.33479118347168 mm for frame 126

Saving results

Total time: 83.29143977165222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01192184
Iteration 2/25 | Loss: 0.01192184
Iteration 3/25 | Loss: 0.01192184
Iteration 4/25 | Loss: 0.01192184
Iteration 5/25 | Loss: 0.01192184
Iteration 6/25 | Loss: 0.01192184
Iteration 7/25 | Loss: 0.01192184
Iteration 8/25 | Loss: 0.01192184
Iteration 9/25 | Loss: 0.01192184
Iteration 10/25 | Loss: 0.01192184
Iteration 11/25 | Loss: 0.01192183
Iteration 12/25 | Loss: 0.01192183
Iteration 13/25 | Loss: 0.01192183
Iteration 14/25 | Loss: 0.01192183
Iteration 15/25 | Loss: 0.01192183
Iteration 16/25 | Loss: 0.01192183
Iteration 17/25 | Loss: 0.01192183
Iteration 18/25 | Loss: 0.01192183
Iteration 19/25 | Loss: 0.01192183
Iteration 20/25 | Loss: 0.01192183
Iteration 21/25 | Loss: 0.01192183
Iteration 22/25 | Loss: 0.01192183
Iteration 23/25 | Loss: 0.01192183
Iteration 24/25 | Loss: 0.01192183
Iteration 25/25 | Loss: 0.01192183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71098876
Iteration 2/25 | Loss: 0.11697427
Iteration 3/25 | Loss: 0.11689337
Iteration 4/25 | Loss: 0.11680727
Iteration 5/25 | Loss: 0.11680724
Iteration 6/25 | Loss: 0.11680724
Iteration 7/25 | Loss: 0.11680723
Iteration 8/25 | Loss: 0.11680723
Iteration 9/25 | Loss: 0.11680723
Iteration 10/25 | Loss: 0.11680723
Iteration 11/25 | Loss: 0.11680723
Iteration 12/25 | Loss: 0.11680723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.1168072298169136, 0.1168072298169136, 0.1168072298169136, 0.1168072298169136, 0.1168072298169136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.1168072298169136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.11680723
Iteration 2/1000 | Loss: 0.00171004
Iteration 3/1000 | Loss: 0.00101562
Iteration 4/1000 | Loss: 0.00052101
Iteration 5/1000 | Loss: 0.00044121
Iteration 6/1000 | Loss: 0.00019234
Iteration 7/1000 | Loss: 0.00129786
Iteration 8/1000 | Loss: 0.00009040
Iteration 9/1000 | Loss: 0.00036973
Iteration 10/1000 | Loss: 0.00039669
Iteration 11/1000 | Loss: 0.00033107
Iteration 12/1000 | Loss: 0.00026993
Iteration 13/1000 | Loss: 0.00006037
Iteration 14/1000 | Loss: 0.00023338
Iteration 15/1000 | Loss: 0.00052920
Iteration 16/1000 | Loss: 0.00035610
Iteration 17/1000 | Loss: 0.00007646
Iteration 18/1000 | Loss: 0.00009009
Iteration 19/1000 | Loss: 0.00064748
Iteration 20/1000 | Loss: 0.00121755
Iteration 21/1000 | Loss: 0.00014825
Iteration 22/1000 | Loss: 0.00011963
Iteration 23/1000 | Loss: 0.00005187
Iteration 24/1000 | Loss: 0.00012067
Iteration 25/1000 | Loss: 0.00006880
Iteration 26/1000 | Loss: 0.00021637
Iteration 27/1000 | Loss: 0.00009088
Iteration 28/1000 | Loss: 0.00004847
Iteration 29/1000 | Loss: 0.00005378
Iteration 30/1000 | Loss: 0.00003268
Iteration 31/1000 | Loss: 0.00003670
Iteration 32/1000 | Loss: 0.00011295
Iteration 33/1000 | Loss: 0.00003228
Iteration 34/1000 | Loss: 0.00014246
Iteration 35/1000 | Loss: 0.00008043
Iteration 36/1000 | Loss: 0.00003257
Iteration 37/1000 | Loss: 0.00003401
Iteration 38/1000 | Loss: 0.00006337
Iteration 39/1000 | Loss: 0.00005261
Iteration 40/1000 | Loss: 0.00003290
Iteration 41/1000 | Loss: 0.00003225
Iteration 42/1000 | Loss: 0.00004150
Iteration 43/1000 | Loss: 0.00004508
Iteration 44/1000 | Loss: 0.00002552
Iteration 45/1000 | Loss: 0.00003847
Iteration 46/1000 | Loss: 0.00002446
Iteration 47/1000 | Loss: 0.00004836
Iteration 48/1000 | Loss: 0.00002273
Iteration 49/1000 | Loss: 0.00004804
Iteration 50/1000 | Loss: 0.00002853
Iteration 51/1000 | Loss: 0.00002210
Iteration 52/1000 | Loss: 0.00003974
Iteration 53/1000 | Loss: 0.00003070
Iteration 54/1000 | Loss: 0.00003917
Iteration 55/1000 | Loss: 0.00002224
Iteration 56/1000 | Loss: 0.00003381
Iteration 57/1000 | Loss: 0.00002266
Iteration 58/1000 | Loss: 0.00002264
Iteration 59/1000 | Loss: 0.00002650
Iteration 60/1000 | Loss: 0.00002223
Iteration 61/1000 | Loss: 0.00002182
Iteration 62/1000 | Loss: 0.00002155
Iteration 63/1000 | Loss: 0.00002150
Iteration 64/1000 | Loss: 0.00002147
Iteration 65/1000 | Loss: 0.00003137
Iteration 66/1000 | Loss: 0.00002799
Iteration 67/1000 | Loss: 0.00002799
Iteration 68/1000 | Loss: 0.00008508
Iteration 69/1000 | Loss: 0.00002483
Iteration 70/1000 | Loss: 0.00004571
Iteration 71/1000 | Loss: 0.00003665
Iteration 72/1000 | Loss: 0.00002620
Iteration 73/1000 | Loss: 0.00002496
Iteration 74/1000 | Loss: 0.00005054
Iteration 75/1000 | Loss: 0.00002176
Iteration 76/1000 | Loss: 0.00004437
Iteration 77/1000 | Loss: 0.00003507
Iteration 78/1000 | Loss: 0.00003025
Iteration 79/1000 | Loss: 0.00002439
Iteration 80/1000 | Loss: 0.00002180
Iteration 81/1000 | Loss: 0.00002131
Iteration 82/1000 | Loss: 0.00002131
Iteration 83/1000 | Loss: 0.00002131
Iteration 84/1000 | Loss: 0.00002130
Iteration 85/1000 | Loss: 0.00002130
Iteration 86/1000 | Loss: 0.00002130
Iteration 87/1000 | Loss: 0.00002129
Iteration 88/1000 | Loss: 0.00002129
Iteration 89/1000 | Loss: 0.00002129
Iteration 90/1000 | Loss: 0.00002129
Iteration 91/1000 | Loss: 0.00002129
Iteration 92/1000 | Loss: 0.00002129
Iteration 93/1000 | Loss: 0.00002129
Iteration 94/1000 | Loss: 0.00002128
Iteration 95/1000 | Loss: 0.00002128
Iteration 96/1000 | Loss: 0.00002128
Iteration 97/1000 | Loss: 0.00002128
Iteration 98/1000 | Loss: 0.00002128
Iteration 99/1000 | Loss: 0.00002217
Iteration 100/1000 | Loss: 0.00002128
Iteration 101/1000 | Loss: 0.00002128
Iteration 102/1000 | Loss: 0.00002128
Iteration 103/1000 | Loss: 0.00002128
Iteration 104/1000 | Loss: 0.00002128
Iteration 105/1000 | Loss: 0.00002128
Iteration 106/1000 | Loss: 0.00002128
Iteration 107/1000 | Loss: 0.00002128
Iteration 108/1000 | Loss: 0.00002128
Iteration 109/1000 | Loss: 0.00002128
Iteration 110/1000 | Loss: 0.00002128
Iteration 111/1000 | Loss: 0.00002128
Iteration 112/1000 | Loss: 0.00002128
Iteration 113/1000 | Loss: 0.00002128
Iteration 114/1000 | Loss: 0.00002128
Iteration 115/1000 | Loss: 0.00002128
Iteration 116/1000 | Loss: 0.00002128
Iteration 117/1000 | Loss: 0.00002128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [2.1276935513014905e-05, 2.1276935513014905e-05, 2.1276935513014905e-05, 2.1276935513014905e-05, 2.1276935513014905e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1276935513014905e-05

Optimization complete. Final v2v error: 3.7779600620269775 mm

Highest mean error: 11.710062980651855 mm for frame 139

Lowest mean error: 3.2903501987457275 mm for frame 48

Saving results

Total time: 125.46800661087036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00544069
Iteration 2/25 | Loss: 0.00184405
Iteration 3/25 | Loss: 0.00178473
Iteration 4/25 | Loss: 0.00176512
Iteration 5/25 | Loss: 0.00176086
Iteration 6/25 | Loss: 0.00175977
Iteration 7/25 | Loss: 0.00175977
Iteration 8/25 | Loss: 0.00175977
Iteration 9/25 | Loss: 0.00175977
Iteration 10/25 | Loss: 0.00175977
Iteration 11/25 | Loss: 0.00175977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001759769394993782, 0.001759769394993782, 0.001759769394993782, 0.001759769394993782, 0.001759769394993782]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001759769394993782

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55085826
Iteration 2/25 | Loss: 0.00278796
Iteration 3/25 | Loss: 0.00278796
Iteration 4/25 | Loss: 0.00278796
Iteration 5/25 | Loss: 0.00278796
Iteration 6/25 | Loss: 0.00278796
Iteration 7/25 | Loss: 0.00278796
Iteration 8/25 | Loss: 0.00278796
Iteration 9/25 | Loss: 0.00278796
Iteration 10/25 | Loss: 0.00278796
Iteration 11/25 | Loss: 0.00278796
Iteration 12/25 | Loss: 0.00278796
Iteration 13/25 | Loss: 0.00278796
Iteration 14/25 | Loss: 0.00278796
Iteration 15/25 | Loss: 0.00278796
Iteration 16/25 | Loss: 0.00278796
Iteration 17/25 | Loss: 0.00278796
Iteration 18/25 | Loss: 0.00278796
Iteration 19/25 | Loss: 0.00278796
Iteration 20/25 | Loss: 0.00278796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00278795906342566, 0.00278795906342566, 0.00278795906342566, 0.00278795906342566, 0.00278795906342566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00278795906342566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278796
Iteration 2/1000 | Loss: 0.00008341
Iteration 3/1000 | Loss: 0.00005846
Iteration 4/1000 | Loss: 0.00005295
Iteration 5/1000 | Loss: 0.00005033
Iteration 6/1000 | Loss: 0.00004883
Iteration 7/1000 | Loss: 0.00004740
Iteration 8/1000 | Loss: 0.00004640
Iteration 9/1000 | Loss: 0.00004598
Iteration 10/1000 | Loss: 0.00004561
Iteration 11/1000 | Loss: 0.00004550
Iteration 12/1000 | Loss: 0.00004549
Iteration 13/1000 | Loss: 0.00004547
Iteration 14/1000 | Loss: 0.00004547
Iteration 15/1000 | Loss: 0.00004542
Iteration 16/1000 | Loss: 0.00004542
Iteration 17/1000 | Loss: 0.00004542
Iteration 18/1000 | Loss: 0.00004539
Iteration 19/1000 | Loss: 0.00004537
Iteration 20/1000 | Loss: 0.00004536
Iteration 21/1000 | Loss: 0.00004535
Iteration 22/1000 | Loss: 0.00004534
Iteration 23/1000 | Loss: 0.00004534
Iteration 24/1000 | Loss: 0.00004534
Iteration 25/1000 | Loss: 0.00004533
Iteration 26/1000 | Loss: 0.00004533
Iteration 27/1000 | Loss: 0.00004533
Iteration 28/1000 | Loss: 0.00004532
Iteration 29/1000 | Loss: 0.00004531
Iteration 30/1000 | Loss: 0.00004531
Iteration 31/1000 | Loss: 0.00004530
Iteration 32/1000 | Loss: 0.00004528
Iteration 33/1000 | Loss: 0.00004526
Iteration 34/1000 | Loss: 0.00004526
Iteration 35/1000 | Loss: 0.00004525
Iteration 36/1000 | Loss: 0.00004525
Iteration 37/1000 | Loss: 0.00004525
Iteration 38/1000 | Loss: 0.00004524
Iteration 39/1000 | Loss: 0.00004523
Iteration 40/1000 | Loss: 0.00004523
Iteration 41/1000 | Loss: 0.00004522
Iteration 42/1000 | Loss: 0.00004522
Iteration 43/1000 | Loss: 0.00004521
Iteration 44/1000 | Loss: 0.00004521
Iteration 45/1000 | Loss: 0.00004521
Iteration 46/1000 | Loss: 0.00004521
Iteration 47/1000 | Loss: 0.00004521
Iteration 48/1000 | Loss: 0.00004521
Iteration 49/1000 | Loss: 0.00004521
Iteration 50/1000 | Loss: 0.00004521
Iteration 51/1000 | Loss: 0.00004520
Iteration 52/1000 | Loss: 0.00004520
Iteration 53/1000 | Loss: 0.00004520
Iteration 54/1000 | Loss: 0.00004520
Iteration 55/1000 | Loss: 0.00004519
Iteration 56/1000 | Loss: 0.00004519
Iteration 57/1000 | Loss: 0.00004519
Iteration 58/1000 | Loss: 0.00004518
Iteration 59/1000 | Loss: 0.00004518
Iteration 60/1000 | Loss: 0.00004518
Iteration 61/1000 | Loss: 0.00004518
Iteration 62/1000 | Loss: 0.00004518
Iteration 63/1000 | Loss: 0.00004518
Iteration 64/1000 | Loss: 0.00004518
Iteration 65/1000 | Loss: 0.00004518
Iteration 66/1000 | Loss: 0.00004517
Iteration 67/1000 | Loss: 0.00004517
Iteration 68/1000 | Loss: 0.00004517
Iteration 69/1000 | Loss: 0.00004517
Iteration 70/1000 | Loss: 0.00004517
Iteration 71/1000 | Loss: 0.00004517
Iteration 72/1000 | Loss: 0.00004516
Iteration 73/1000 | Loss: 0.00004516
Iteration 74/1000 | Loss: 0.00004516
Iteration 75/1000 | Loss: 0.00004516
Iteration 76/1000 | Loss: 0.00004516
Iteration 77/1000 | Loss: 0.00004516
Iteration 78/1000 | Loss: 0.00004516
Iteration 79/1000 | Loss: 0.00004516
Iteration 80/1000 | Loss: 0.00004516
Iteration 81/1000 | Loss: 0.00004515
Iteration 82/1000 | Loss: 0.00004515
Iteration 83/1000 | Loss: 0.00004515
Iteration 84/1000 | Loss: 0.00004515
Iteration 85/1000 | Loss: 0.00004515
Iteration 86/1000 | Loss: 0.00004515
Iteration 87/1000 | Loss: 0.00004515
Iteration 88/1000 | Loss: 0.00004515
Iteration 89/1000 | Loss: 0.00004514
Iteration 90/1000 | Loss: 0.00004514
Iteration 91/1000 | Loss: 0.00004514
Iteration 92/1000 | Loss: 0.00004514
Iteration 93/1000 | Loss: 0.00004514
Iteration 94/1000 | Loss: 0.00004514
Iteration 95/1000 | Loss: 0.00004514
Iteration 96/1000 | Loss: 0.00004514
Iteration 97/1000 | Loss: 0.00004514
Iteration 98/1000 | Loss: 0.00004514
Iteration 99/1000 | Loss: 0.00004514
Iteration 100/1000 | Loss: 0.00004514
Iteration 101/1000 | Loss: 0.00004514
Iteration 102/1000 | Loss: 0.00004514
Iteration 103/1000 | Loss: 0.00004514
Iteration 104/1000 | Loss: 0.00004514
Iteration 105/1000 | Loss: 0.00004513
Iteration 106/1000 | Loss: 0.00004513
Iteration 107/1000 | Loss: 0.00004513
Iteration 108/1000 | Loss: 0.00004513
Iteration 109/1000 | Loss: 0.00004513
Iteration 110/1000 | Loss: 0.00004513
Iteration 111/1000 | Loss: 0.00004513
Iteration 112/1000 | Loss: 0.00004513
Iteration 113/1000 | Loss: 0.00004513
Iteration 114/1000 | Loss: 0.00004513
Iteration 115/1000 | Loss: 0.00004513
Iteration 116/1000 | Loss: 0.00004513
Iteration 117/1000 | Loss: 0.00004513
Iteration 118/1000 | Loss: 0.00004513
Iteration 119/1000 | Loss: 0.00004513
Iteration 120/1000 | Loss: 0.00004513
Iteration 121/1000 | Loss: 0.00004513
Iteration 122/1000 | Loss: 0.00004513
Iteration 123/1000 | Loss: 0.00004513
Iteration 124/1000 | Loss: 0.00004513
Iteration 125/1000 | Loss: 0.00004513
Iteration 126/1000 | Loss: 0.00004513
Iteration 127/1000 | Loss: 0.00004513
Iteration 128/1000 | Loss: 0.00004513
Iteration 129/1000 | Loss: 0.00004513
Iteration 130/1000 | Loss: 0.00004513
Iteration 131/1000 | Loss: 0.00004513
Iteration 132/1000 | Loss: 0.00004513
Iteration 133/1000 | Loss: 0.00004513
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [4.512542000156827e-05, 4.512542000156827e-05, 4.512542000156827e-05, 4.512542000156827e-05, 4.512542000156827e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.512542000156827e-05

Optimization complete. Final v2v error: 5.6458964347839355 mm

Highest mean error: 5.972785472869873 mm for frame 135

Lowest mean error: 5.274576663970947 mm for frame 44

Saving results

Total time: 32.969019412994385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615002
Iteration 2/25 | Loss: 0.00183487
Iteration 3/25 | Loss: 0.00175860
Iteration 4/25 | Loss: 0.00174371
Iteration 5/25 | Loss: 0.00173800
Iteration 6/25 | Loss: 0.00173596
Iteration 7/25 | Loss: 0.00173561
Iteration 8/25 | Loss: 0.00173561
Iteration 9/25 | Loss: 0.00173561
Iteration 10/25 | Loss: 0.00173561
Iteration 11/25 | Loss: 0.00173561
Iteration 12/25 | Loss: 0.00173561
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017356107709929347, 0.0017356107709929347, 0.0017356107709929347, 0.0017356107709929347, 0.0017356107709929347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017356107709929347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.50558567
Iteration 2/25 | Loss: 0.00273992
Iteration 3/25 | Loss: 0.00273990
Iteration 4/25 | Loss: 0.00273990
Iteration 5/25 | Loss: 0.00273990
Iteration 6/25 | Loss: 0.00273990
Iteration 7/25 | Loss: 0.00273990
Iteration 8/25 | Loss: 0.00273990
Iteration 9/25 | Loss: 0.00273990
Iteration 10/25 | Loss: 0.00273990
Iteration 11/25 | Loss: 0.00273990
Iteration 12/25 | Loss: 0.00273990
Iteration 13/25 | Loss: 0.00273990
Iteration 14/25 | Loss: 0.00273990
Iteration 15/25 | Loss: 0.00273990
Iteration 16/25 | Loss: 0.00273990
Iteration 17/25 | Loss: 0.00273990
Iteration 18/25 | Loss: 0.00273990
Iteration 19/25 | Loss: 0.00273990
Iteration 20/25 | Loss: 0.00273990
Iteration 21/25 | Loss: 0.00273990
Iteration 22/25 | Loss: 0.00273990
Iteration 23/25 | Loss: 0.00273990
Iteration 24/25 | Loss: 0.00273990
Iteration 25/25 | Loss: 0.00273990

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273990
Iteration 2/1000 | Loss: 0.00004154
Iteration 3/1000 | Loss: 0.00003361
Iteration 4/1000 | Loss: 0.00003171
Iteration 5/1000 | Loss: 0.00003057
Iteration 6/1000 | Loss: 0.00003001
Iteration 7/1000 | Loss: 0.00002924
Iteration 8/1000 | Loss: 0.00002874
Iteration 9/1000 | Loss: 0.00002843
Iteration 10/1000 | Loss: 0.00002830
Iteration 11/1000 | Loss: 0.00002823
Iteration 12/1000 | Loss: 0.00002818
Iteration 13/1000 | Loss: 0.00002815
Iteration 14/1000 | Loss: 0.00002814
Iteration 15/1000 | Loss: 0.00002813
Iteration 16/1000 | Loss: 0.00002809
Iteration 17/1000 | Loss: 0.00002808
Iteration 18/1000 | Loss: 0.00002806
Iteration 19/1000 | Loss: 0.00002804
Iteration 20/1000 | Loss: 0.00002803
Iteration 21/1000 | Loss: 0.00002803
Iteration 22/1000 | Loss: 0.00002790
Iteration 23/1000 | Loss: 0.00002790
Iteration 24/1000 | Loss: 0.00002788
Iteration 25/1000 | Loss: 0.00002788
Iteration 26/1000 | Loss: 0.00002787
Iteration 27/1000 | Loss: 0.00002787
Iteration 28/1000 | Loss: 0.00002787
Iteration 29/1000 | Loss: 0.00002784
Iteration 30/1000 | Loss: 0.00002783
Iteration 31/1000 | Loss: 0.00002783
Iteration 32/1000 | Loss: 0.00002781
Iteration 33/1000 | Loss: 0.00002780
Iteration 34/1000 | Loss: 0.00002780
Iteration 35/1000 | Loss: 0.00002780
Iteration 36/1000 | Loss: 0.00002780
Iteration 37/1000 | Loss: 0.00002780
Iteration 38/1000 | Loss: 0.00002779
Iteration 39/1000 | Loss: 0.00002778
Iteration 40/1000 | Loss: 0.00002778
Iteration 41/1000 | Loss: 0.00002778
Iteration 42/1000 | Loss: 0.00002778
Iteration 43/1000 | Loss: 0.00002778
Iteration 44/1000 | Loss: 0.00002778
Iteration 45/1000 | Loss: 0.00002777
Iteration 46/1000 | Loss: 0.00002777
Iteration 47/1000 | Loss: 0.00002777
Iteration 48/1000 | Loss: 0.00002777
Iteration 49/1000 | Loss: 0.00002776
Iteration 50/1000 | Loss: 0.00002776
Iteration 51/1000 | Loss: 0.00002776
Iteration 52/1000 | Loss: 0.00002776
Iteration 53/1000 | Loss: 0.00002775
Iteration 54/1000 | Loss: 0.00002775
Iteration 55/1000 | Loss: 0.00002775
Iteration 56/1000 | Loss: 0.00002775
Iteration 57/1000 | Loss: 0.00002774
Iteration 58/1000 | Loss: 0.00002774
Iteration 59/1000 | Loss: 0.00002774
Iteration 60/1000 | Loss: 0.00002774
Iteration 61/1000 | Loss: 0.00002774
Iteration 62/1000 | Loss: 0.00002774
Iteration 63/1000 | Loss: 0.00002774
Iteration 64/1000 | Loss: 0.00002774
Iteration 65/1000 | Loss: 0.00002774
Iteration 66/1000 | Loss: 0.00002774
Iteration 67/1000 | Loss: 0.00002774
Iteration 68/1000 | Loss: 0.00002774
Iteration 69/1000 | Loss: 0.00002774
Iteration 70/1000 | Loss: 0.00002774
Iteration 71/1000 | Loss: 0.00002774
Iteration 72/1000 | Loss: 0.00002774
Iteration 73/1000 | Loss: 0.00002774
Iteration 74/1000 | Loss: 0.00002774
Iteration 75/1000 | Loss: 0.00002774
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.7741723897634074e-05, 2.7741723897634074e-05, 2.7741723897634074e-05, 2.7741723897634074e-05, 2.7741723897634074e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7741723897634074e-05

Optimization complete. Final v2v error: 4.591652870178223 mm

Highest mean error: 5.0825581550598145 mm for frame 43

Lowest mean error: 4.115623474121094 mm for frame 72

Saving results

Total time: 34.39408540725708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053044
Iteration 2/25 | Loss: 0.00217566
Iteration 3/25 | Loss: 0.00179489
Iteration 4/25 | Loss: 0.00177051
Iteration 5/25 | Loss: 0.00176046
Iteration 6/25 | Loss: 0.00175794
Iteration 7/25 | Loss: 0.00175790
Iteration 8/25 | Loss: 0.00175790
Iteration 9/25 | Loss: 0.00175790
Iteration 10/25 | Loss: 0.00175790
Iteration 11/25 | Loss: 0.00175790
Iteration 12/25 | Loss: 0.00175790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0017578961560502648, 0.0017578961560502648, 0.0017578961560502648, 0.0017578961560502648, 0.0017578961560502648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017578961560502648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29855931
Iteration 2/25 | Loss: 0.00282953
Iteration 3/25 | Loss: 0.00282953
Iteration 4/25 | Loss: 0.00282953
Iteration 5/25 | Loss: 0.00282953
Iteration 6/25 | Loss: 0.00282953
Iteration 7/25 | Loss: 0.00282952
Iteration 8/25 | Loss: 0.00282952
Iteration 9/25 | Loss: 0.00282952
Iteration 10/25 | Loss: 0.00282952
Iteration 11/25 | Loss: 0.00282952
Iteration 12/25 | Loss: 0.00282952
Iteration 13/25 | Loss: 0.00282952
Iteration 14/25 | Loss: 0.00282952
Iteration 15/25 | Loss: 0.00282952
Iteration 16/25 | Loss: 0.00282952
Iteration 17/25 | Loss: 0.00282952
Iteration 18/25 | Loss: 0.00282952
Iteration 19/25 | Loss: 0.00282952
Iteration 20/25 | Loss: 0.00282952
Iteration 21/25 | Loss: 0.00282952
Iteration 22/25 | Loss: 0.00282952
Iteration 23/25 | Loss: 0.00282952
Iteration 24/25 | Loss: 0.00282952
Iteration 25/25 | Loss: 0.00282952

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00282952
Iteration 2/1000 | Loss: 0.00006762
Iteration 3/1000 | Loss: 0.00004915
Iteration 4/1000 | Loss: 0.00004358
Iteration 5/1000 | Loss: 0.00004083
Iteration 6/1000 | Loss: 0.00003930
Iteration 7/1000 | Loss: 0.00003835
Iteration 8/1000 | Loss: 0.00003731
Iteration 9/1000 | Loss: 0.00003658
Iteration 10/1000 | Loss: 0.00003605
Iteration 11/1000 | Loss: 0.00003564
Iteration 12/1000 | Loss: 0.00003533
Iteration 13/1000 | Loss: 0.00003506
Iteration 14/1000 | Loss: 0.00003489
Iteration 15/1000 | Loss: 0.00003487
Iteration 16/1000 | Loss: 0.00003472
Iteration 17/1000 | Loss: 0.00003470
Iteration 18/1000 | Loss: 0.00003462
Iteration 19/1000 | Loss: 0.00003459
Iteration 20/1000 | Loss: 0.00003458
Iteration 21/1000 | Loss: 0.00003453
Iteration 22/1000 | Loss: 0.00003450
Iteration 23/1000 | Loss: 0.00003447
Iteration 24/1000 | Loss: 0.00003446
Iteration 25/1000 | Loss: 0.00003443
Iteration 26/1000 | Loss: 0.00003442
Iteration 27/1000 | Loss: 0.00003442
Iteration 28/1000 | Loss: 0.00003441
Iteration 29/1000 | Loss: 0.00003441
Iteration 30/1000 | Loss: 0.00003440
Iteration 31/1000 | Loss: 0.00003440
Iteration 32/1000 | Loss: 0.00003440
Iteration 33/1000 | Loss: 0.00003440
Iteration 34/1000 | Loss: 0.00003440
Iteration 35/1000 | Loss: 0.00003440
Iteration 36/1000 | Loss: 0.00003439
Iteration 37/1000 | Loss: 0.00003439
Iteration 38/1000 | Loss: 0.00003439
Iteration 39/1000 | Loss: 0.00003439
Iteration 40/1000 | Loss: 0.00003439
Iteration 41/1000 | Loss: 0.00003438
Iteration 42/1000 | Loss: 0.00003438
Iteration 43/1000 | Loss: 0.00003438
Iteration 44/1000 | Loss: 0.00003438
Iteration 45/1000 | Loss: 0.00003438
Iteration 46/1000 | Loss: 0.00003438
Iteration 47/1000 | Loss: 0.00003438
Iteration 48/1000 | Loss: 0.00003438
Iteration 49/1000 | Loss: 0.00003437
Iteration 50/1000 | Loss: 0.00003437
Iteration 51/1000 | Loss: 0.00003437
Iteration 52/1000 | Loss: 0.00003437
Iteration 53/1000 | Loss: 0.00003437
Iteration 54/1000 | Loss: 0.00003437
Iteration 55/1000 | Loss: 0.00003437
Iteration 56/1000 | Loss: 0.00003437
Iteration 57/1000 | Loss: 0.00003437
Iteration 58/1000 | Loss: 0.00003436
Iteration 59/1000 | Loss: 0.00003436
Iteration 60/1000 | Loss: 0.00003436
Iteration 61/1000 | Loss: 0.00003436
Iteration 62/1000 | Loss: 0.00003436
Iteration 63/1000 | Loss: 0.00003436
Iteration 64/1000 | Loss: 0.00003436
Iteration 65/1000 | Loss: 0.00003436
Iteration 66/1000 | Loss: 0.00003435
Iteration 67/1000 | Loss: 0.00003435
Iteration 68/1000 | Loss: 0.00003435
Iteration 69/1000 | Loss: 0.00003435
Iteration 70/1000 | Loss: 0.00003435
Iteration 71/1000 | Loss: 0.00003435
Iteration 72/1000 | Loss: 0.00003434
Iteration 73/1000 | Loss: 0.00003434
Iteration 74/1000 | Loss: 0.00003434
Iteration 75/1000 | Loss: 0.00003434
Iteration 76/1000 | Loss: 0.00003434
Iteration 77/1000 | Loss: 0.00003434
Iteration 78/1000 | Loss: 0.00003434
Iteration 79/1000 | Loss: 0.00003434
Iteration 80/1000 | Loss: 0.00003434
Iteration 81/1000 | Loss: 0.00003434
Iteration 82/1000 | Loss: 0.00003433
Iteration 83/1000 | Loss: 0.00003433
Iteration 84/1000 | Loss: 0.00003433
Iteration 85/1000 | Loss: 0.00003433
Iteration 86/1000 | Loss: 0.00003433
Iteration 87/1000 | Loss: 0.00003433
Iteration 88/1000 | Loss: 0.00003433
Iteration 89/1000 | Loss: 0.00003433
Iteration 90/1000 | Loss: 0.00003433
Iteration 91/1000 | Loss: 0.00003433
Iteration 92/1000 | Loss: 0.00003433
Iteration 93/1000 | Loss: 0.00003433
Iteration 94/1000 | Loss: 0.00003433
Iteration 95/1000 | Loss: 0.00003433
Iteration 96/1000 | Loss: 0.00003432
Iteration 97/1000 | Loss: 0.00003432
Iteration 98/1000 | Loss: 0.00003432
Iteration 99/1000 | Loss: 0.00003432
Iteration 100/1000 | Loss: 0.00003432
Iteration 101/1000 | Loss: 0.00003432
Iteration 102/1000 | Loss: 0.00003432
Iteration 103/1000 | Loss: 0.00003432
Iteration 104/1000 | Loss: 0.00003432
Iteration 105/1000 | Loss: 0.00003431
Iteration 106/1000 | Loss: 0.00003431
Iteration 107/1000 | Loss: 0.00003431
Iteration 108/1000 | Loss: 0.00003431
Iteration 109/1000 | Loss: 0.00003431
Iteration 110/1000 | Loss: 0.00003431
Iteration 111/1000 | Loss: 0.00003431
Iteration 112/1000 | Loss: 0.00003431
Iteration 113/1000 | Loss: 0.00003431
Iteration 114/1000 | Loss: 0.00003431
Iteration 115/1000 | Loss: 0.00003431
Iteration 116/1000 | Loss: 0.00003431
Iteration 117/1000 | Loss: 0.00003431
Iteration 118/1000 | Loss: 0.00003430
Iteration 119/1000 | Loss: 0.00003430
Iteration 120/1000 | Loss: 0.00003430
Iteration 121/1000 | Loss: 0.00003430
Iteration 122/1000 | Loss: 0.00003430
Iteration 123/1000 | Loss: 0.00003430
Iteration 124/1000 | Loss: 0.00003430
Iteration 125/1000 | Loss: 0.00003430
Iteration 126/1000 | Loss: 0.00003430
Iteration 127/1000 | Loss: 0.00003429
Iteration 128/1000 | Loss: 0.00003429
Iteration 129/1000 | Loss: 0.00003429
Iteration 130/1000 | Loss: 0.00003429
Iteration 131/1000 | Loss: 0.00003429
Iteration 132/1000 | Loss: 0.00003429
Iteration 133/1000 | Loss: 0.00003429
Iteration 134/1000 | Loss: 0.00003429
Iteration 135/1000 | Loss: 0.00003429
Iteration 136/1000 | Loss: 0.00003429
Iteration 137/1000 | Loss: 0.00003429
Iteration 138/1000 | Loss: 0.00003429
Iteration 139/1000 | Loss: 0.00003429
Iteration 140/1000 | Loss: 0.00003429
Iteration 141/1000 | Loss: 0.00003429
Iteration 142/1000 | Loss: 0.00003428
Iteration 143/1000 | Loss: 0.00003428
Iteration 144/1000 | Loss: 0.00003428
Iteration 145/1000 | Loss: 0.00003428
Iteration 146/1000 | Loss: 0.00003428
Iteration 147/1000 | Loss: 0.00003428
Iteration 148/1000 | Loss: 0.00003428
Iteration 149/1000 | Loss: 0.00003428
Iteration 150/1000 | Loss: 0.00003428
Iteration 151/1000 | Loss: 0.00003428
Iteration 152/1000 | Loss: 0.00003428
Iteration 153/1000 | Loss: 0.00003428
Iteration 154/1000 | Loss: 0.00003427
Iteration 155/1000 | Loss: 0.00003427
Iteration 156/1000 | Loss: 0.00003427
Iteration 157/1000 | Loss: 0.00003427
Iteration 158/1000 | Loss: 0.00003427
Iteration 159/1000 | Loss: 0.00003427
Iteration 160/1000 | Loss: 0.00003427
Iteration 161/1000 | Loss: 0.00003427
Iteration 162/1000 | Loss: 0.00003427
Iteration 163/1000 | Loss: 0.00003427
Iteration 164/1000 | Loss: 0.00003427
Iteration 165/1000 | Loss: 0.00003427
Iteration 166/1000 | Loss: 0.00003427
Iteration 167/1000 | Loss: 0.00003427
Iteration 168/1000 | Loss: 0.00003427
Iteration 169/1000 | Loss: 0.00003427
Iteration 170/1000 | Loss: 0.00003427
Iteration 171/1000 | Loss: 0.00003426
Iteration 172/1000 | Loss: 0.00003426
Iteration 173/1000 | Loss: 0.00003426
Iteration 174/1000 | Loss: 0.00003426
Iteration 175/1000 | Loss: 0.00003426
Iteration 176/1000 | Loss: 0.00003426
Iteration 177/1000 | Loss: 0.00003426
Iteration 178/1000 | Loss: 0.00003426
Iteration 179/1000 | Loss: 0.00003426
Iteration 180/1000 | Loss: 0.00003426
Iteration 181/1000 | Loss: 0.00003426
Iteration 182/1000 | Loss: 0.00003426
Iteration 183/1000 | Loss: 0.00003426
Iteration 184/1000 | Loss: 0.00003426
Iteration 185/1000 | Loss: 0.00003426
Iteration 186/1000 | Loss: 0.00003426
Iteration 187/1000 | Loss: 0.00003426
Iteration 188/1000 | Loss: 0.00003426
Iteration 189/1000 | Loss: 0.00003426
Iteration 190/1000 | Loss: 0.00003426
Iteration 191/1000 | Loss: 0.00003426
Iteration 192/1000 | Loss: 0.00003426
Iteration 193/1000 | Loss: 0.00003425
Iteration 194/1000 | Loss: 0.00003425
Iteration 195/1000 | Loss: 0.00003425
Iteration 196/1000 | Loss: 0.00003425
Iteration 197/1000 | Loss: 0.00003425
Iteration 198/1000 | Loss: 0.00003425
Iteration 199/1000 | Loss: 0.00003425
Iteration 200/1000 | Loss: 0.00003425
Iteration 201/1000 | Loss: 0.00003425
Iteration 202/1000 | Loss: 0.00003425
Iteration 203/1000 | Loss: 0.00003425
Iteration 204/1000 | Loss: 0.00003425
Iteration 205/1000 | Loss: 0.00003425
Iteration 206/1000 | Loss: 0.00003425
Iteration 207/1000 | Loss: 0.00003425
Iteration 208/1000 | Loss: 0.00003425
Iteration 209/1000 | Loss: 0.00003424
Iteration 210/1000 | Loss: 0.00003424
Iteration 211/1000 | Loss: 0.00003424
Iteration 212/1000 | Loss: 0.00003424
Iteration 213/1000 | Loss: 0.00003424
Iteration 214/1000 | Loss: 0.00003424
Iteration 215/1000 | Loss: 0.00003424
Iteration 216/1000 | Loss: 0.00003424
Iteration 217/1000 | Loss: 0.00003424
Iteration 218/1000 | Loss: 0.00003424
Iteration 219/1000 | Loss: 0.00003424
Iteration 220/1000 | Loss: 0.00003424
Iteration 221/1000 | Loss: 0.00003424
Iteration 222/1000 | Loss: 0.00003424
Iteration 223/1000 | Loss: 0.00003424
Iteration 224/1000 | Loss: 0.00003424
Iteration 225/1000 | Loss: 0.00003424
Iteration 226/1000 | Loss: 0.00003424
Iteration 227/1000 | Loss: 0.00003424
Iteration 228/1000 | Loss: 0.00003424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 228. Stopping optimization.
Last 5 losses: [3.4242017136421055e-05, 3.4242017136421055e-05, 3.4242017136421055e-05, 3.4242017136421055e-05, 3.4242017136421055e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.4242017136421055e-05

Optimization complete. Final v2v error: 4.928056716918945 mm

Highest mean error: 6.480210304260254 mm for frame 133

Lowest mean error: 4.126339912414551 mm for frame 29

Saving results

Total time: 45.0857994556427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00583717
Iteration 2/25 | Loss: 0.00191981
Iteration 3/25 | Loss: 0.00170797
Iteration 4/25 | Loss: 0.00168956
Iteration 5/25 | Loss: 0.00168429
Iteration 6/25 | Loss: 0.00168254
Iteration 7/25 | Loss: 0.00168194
Iteration 8/25 | Loss: 0.00168194
Iteration 9/25 | Loss: 0.00168194
Iteration 10/25 | Loss: 0.00168194
Iteration 11/25 | Loss: 0.00168194
Iteration 12/25 | Loss: 0.00168194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001681942492723465, 0.001681942492723465, 0.001681942492723465, 0.001681942492723465, 0.001681942492723465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001681942492723465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61641836
Iteration 2/25 | Loss: 0.00258388
Iteration 3/25 | Loss: 0.00258388
Iteration 4/25 | Loss: 0.00258388
Iteration 5/25 | Loss: 0.00258388
Iteration 6/25 | Loss: 0.00258388
Iteration 7/25 | Loss: 0.00258388
Iteration 8/25 | Loss: 0.00258388
Iteration 9/25 | Loss: 0.00258388
Iteration 10/25 | Loss: 0.00258388
Iteration 11/25 | Loss: 0.00258388
Iteration 12/25 | Loss: 0.00258388
Iteration 13/25 | Loss: 0.00258388
Iteration 14/25 | Loss: 0.00258388
Iteration 15/25 | Loss: 0.00258388
Iteration 16/25 | Loss: 0.00258388
Iteration 17/25 | Loss: 0.00258388
Iteration 18/25 | Loss: 0.00258388
Iteration 19/25 | Loss: 0.00258388
Iteration 20/25 | Loss: 0.00258388
Iteration 21/25 | Loss: 0.00258388
Iteration 22/25 | Loss: 0.00258388
Iteration 23/25 | Loss: 0.00258388
Iteration 24/25 | Loss: 0.00258388
Iteration 25/25 | Loss: 0.00258388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258388
Iteration 2/1000 | Loss: 0.00006207
Iteration 3/1000 | Loss: 0.00004390
Iteration 4/1000 | Loss: 0.00003944
Iteration 5/1000 | Loss: 0.00003689
Iteration 6/1000 | Loss: 0.00003554
Iteration 7/1000 | Loss: 0.00003445
Iteration 8/1000 | Loss: 0.00003388
Iteration 9/1000 | Loss: 0.00003341
Iteration 10/1000 | Loss: 0.00003298
Iteration 11/1000 | Loss: 0.00003268
Iteration 12/1000 | Loss: 0.00003244
Iteration 13/1000 | Loss: 0.00003225
Iteration 14/1000 | Loss: 0.00003222
Iteration 15/1000 | Loss: 0.00003222
Iteration 16/1000 | Loss: 0.00003220
Iteration 17/1000 | Loss: 0.00003220
Iteration 18/1000 | Loss: 0.00003219
Iteration 19/1000 | Loss: 0.00003218
Iteration 20/1000 | Loss: 0.00003217
Iteration 21/1000 | Loss: 0.00003213
Iteration 22/1000 | Loss: 0.00003210
Iteration 23/1000 | Loss: 0.00003210
Iteration 24/1000 | Loss: 0.00003209
Iteration 25/1000 | Loss: 0.00003209
Iteration 26/1000 | Loss: 0.00003208
Iteration 27/1000 | Loss: 0.00003207
Iteration 28/1000 | Loss: 0.00003207
Iteration 29/1000 | Loss: 0.00003206
Iteration 30/1000 | Loss: 0.00003206
Iteration 31/1000 | Loss: 0.00003205
Iteration 32/1000 | Loss: 0.00003205
Iteration 33/1000 | Loss: 0.00003204
Iteration 34/1000 | Loss: 0.00003201
Iteration 35/1000 | Loss: 0.00003198
Iteration 36/1000 | Loss: 0.00003197
Iteration 37/1000 | Loss: 0.00003197
Iteration 38/1000 | Loss: 0.00003197
Iteration 39/1000 | Loss: 0.00003196
Iteration 40/1000 | Loss: 0.00003196
Iteration 41/1000 | Loss: 0.00003196
Iteration 42/1000 | Loss: 0.00003194
Iteration 43/1000 | Loss: 0.00003192
Iteration 44/1000 | Loss: 0.00003190
Iteration 45/1000 | Loss: 0.00003190
Iteration 46/1000 | Loss: 0.00003189
Iteration 47/1000 | Loss: 0.00003189
Iteration 48/1000 | Loss: 0.00003189
Iteration 49/1000 | Loss: 0.00003185
Iteration 50/1000 | Loss: 0.00003185
Iteration 51/1000 | Loss: 0.00003185
Iteration 52/1000 | Loss: 0.00003184
Iteration 53/1000 | Loss: 0.00003183
Iteration 54/1000 | Loss: 0.00003182
Iteration 55/1000 | Loss: 0.00003182
Iteration 56/1000 | Loss: 0.00003182
Iteration 57/1000 | Loss: 0.00003181
Iteration 58/1000 | Loss: 0.00003181
Iteration 59/1000 | Loss: 0.00003181
Iteration 60/1000 | Loss: 0.00003180
Iteration 61/1000 | Loss: 0.00003180
Iteration 62/1000 | Loss: 0.00003180
Iteration 63/1000 | Loss: 0.00003180
Iteration 64/1000 | Loss: 0.00003180
Iteration 65/1000 | Loss: 0.00003180
Iteration 66/1000 | Loss: 0.00003179
Iteration 67/1000 | Loss: 0.00003179
Iteration 68/1000 | Loss: 0.00003179
Iteration 69/1000 | Loss: 0.00003179
Iteration 70/1000 | Loss: 0.00003179
Iteration 71/1000 | Loss: 0.00003179
Iteration 72/1000 | Loss: 0.00003179
Iteration 73/1000 | Loss: 0.00003179
Iteration 74/1000 | Loss: 0.00003179
Iteration 75/1000 | Loss: 0.00003178
Iteration 76/1000 | Loss: 0.00003178
Iteration 77/1000 | Loss: 0.00003178
Iteration 78/1000 | Loss: 0.00003178
Iteration 79/1000 | Loss: 0.00003177
Iteration 80/1000 | Loss: 0.00003177
Iteration 81/1000 | Loss: 0.00003177
Iteration 82/1000 | Loss: 0.00003177
Iteration 83/1000 | Loss: 0.00003177
Iteration 84/1000 | Loss: 0.00003177
Iteration 85/1000 | Loss: 0.00003177
Iteration 86/1000 | Loss: 0.00003176
Iteration 87/1000 | Loss: 0.00003176
Iteration 88/1000 | Loss: 0.00003176
Iteration 89/1000 | Loss: 0.00003176
Iteration 90/1000 | Loss: 0.00003176
Iteration 91/1000 | Loss: 0.00003176
Iteration 92/1000 | Loss: 0.00003176
Iteration 93/1000 | Loss: 0.00003176
Iteration 94/1000 | Loss: 0.00003176
Iteration 95/1000 | Loss: 0.00003175
Iteration 96/1000 | Loss: 0.00003175
Iteration 97/1000 | Loss: 0.00003175
Iteration 98/1000 | Loss: 0.00003175
Iteration 99/1000 | Loss: 0.00003175
Iteration 100/1000 | Loss: 0.00003175
Iteration 101/1000 | Loss: 0.00003175
Iteration 102/1000 | Loss: 0.00003174
Iteration 103/1000 | Loss: 0.00003174
Iteration 104/1000 | Loss: 0.00003174
Iteration 105/1000 | Loss: 0.00003174
Iteration 106/1000 | Loss: 0.00003174
Iteration 107/1000 | Loss: 0.00003174
Iteration 108/1000 | Loss: 0.00003174
Iteration 109/1000 | Loss: 0.00003174
Iteration 110/1000 | Loss: 0.00003174
Iteration 111/1000 | Loss: 0.00003173
Iteration 112/1000 | Loss: 0.00003173
Iteration 113/1000 | Loss: 0.00003173
Iteration 114/1000 | Loss: 0.00003173
Iteration 115/1000 | Loss: 0.00003173
Iteration 116/1000 | Loss: 0.00003173
Iteration 117/1000 | Loss: 0.00003173
Iteration 118/1000 | Loss: 0.00003173
Iteration 119/1000 | Loss: 0.00003173
Iteration 120/1000 | Loss: 0.00003173
Iteration 121/1000 | Loss: 0.00003172
Iteration 122/1000 | Loss: 0.00003172
Iteration 123/1000 | Loss: 0.00003172
Iteration 124/1000 | Loss: 0.00003172
Iteration 125/1000 | Loss: 0.00003172
Iteration 126/1000 | Loss: 0.00003172
Iteration 127/1000 | Loss: 0.00003172
Iteration 128/1000 | Loss: 0.00003172
Iteration 129/1000 | Loss: 0.00003172
Iteration 130/1000 | Loss: 0.00003172
Iteration 131/1000 | Loss: 0.00003171
Iteration 132/1000 | Loss: 0.00003171
Iteration 133/1000 | Loss: 0.00003171
Iteration 134/1000 | Loss: 0.00003171
Iteration 135/1000 | Loss: 0.00003171
Iteration 136/1000 | Loss: 0.00003171
Iteration 137/1000 | Loss: 0.00003171
Iteration 138/1000 | Loss: 0.00003171
Iteration 139/1000 | Loss: 0.00003170
Iteration 140/1000 | Loss: 0.00003170
Iteration 141/1000 | Loss: 0.00003170
Iteration 142/1000 | Loss: 0.00003170
Iteration 143/1000 | Loss: 0.00003170
Iteration 144/1000 | Loss: 0.00003170
Iteration 145/1000 | Loss: 0.00003170
Iteration 146/1000 | Loss: 0.00003170
Iteration 147/1000 | Loss: 0.00003169
Iteration 148/1000 | Loss: 0.00003169
Iteration 149/1000 | Loss: 0.00003169
Iteration 150/1000 | Loss: 0.00003169
Iteration 151/1000 | Loss: 0.00003169
Iteration 152/1000 | Loss: 0.00003169
Iteration 153/1000 | Loss: 0.00003169
Iteration 154/1000 | Loss: 0.00003169
Iteration 155/1000 | Loss: 0.00003169
Iteration 156/1000 | Loss: 0.00003169
Iteration 157/1000 | Loss: 0.00003169
Iteration 158/1000 | Loss: 0.00003169
Iteration 159/1000 | Loss: 0.00003169
Iteration 160/1000 | Loss: 0.00003169
Iteration 161/1000 | Loss: 0.00003169
Iteration 162/1000 | Loss: 0.00003168
Iteration 163/1000 | Loss: 0.00003168
Iteration 164/1000 | Loss: 0.00003168
Iteration 165/1000 | Loss: 0.00003168
Iteration 166/1000 | Loss: 0.00003168
Iteration 167/1000 | Loss: 0.00003168
Iteration 168/1000 | Loss: 0.00003168
Iteration 169/1000 | Loss: 0.00003168
Iteration 170/1000 | Loss: 0.00003168
Iteration 171/1000 | Loss: 0.00003168
Iteration 172/1000 | Loss: 0.00003168
Iteration 173/1000 | Loss: 0.00003168
Iteration 174/1000 | Loss: 0.00003168
Iteration 175/1000 | Loss: 0.00003167
Iteration 176/1000 | Loss: 0.00003167
Iteration 177/1000 | Loss: 0.00003167
Iteration 178/1000 | Loss: 0.00003167
Iteration 179/1000 | Loss: 0.00003167
Iteration 180/1000 | Loss: 0.00003167
Iteration 181/1000 | Loss: 0.00003167
Iteration 182/1000 | Loss: 0.00003167
Iteration 183/1000 | Loss: 0.00003167
Iteration 184/1000 | Loss: 0.00003167
Iteration 185/1000 | Loss: 0.00003167
Iteration 186/1000 | Loss: 0.00003167
Iteration 187/1000 | Loss: 0.00003167
Iteration 188/1000 | Loss: 0.00003166
Iteration 189/1000 | Loss: 0.00003166
Iteration 190/1000 | Loss: 0.00003166
Iteration 191/1000 | Loss: 0.00003166
Iteration 192/1000 | Loss: 0.00003166
Iteration 193/1000 | Loss: 0.00003166
Iteration 194/1000 | Loss: 0.00003166
Iteration 195/1000 | Loss: 0.00003166
Iteration 196/1000 | Loss: 0.00003166
Iteration 197/1000 | Loss: 0.00003166
Iteration 198/1000 | Loss: 0.00003166
Iteration 199/1000 | Loss: 0.00003166
Iteration 200/1000 | Loss: 0.00003166
Iteration 201/1000 | Loss: 0.00003165
Iteration 202/1000 | Loss: 0.00003165
Iteration 203/1000 | Loss: 0.00003165
Iteration 204/1000 | Loss: 0.00003165
Iteration 205/1000 | Loss: 0.00003165
Iteration 206/1000 | Loss: 0.00003165
Iteration 207/1000 | Loss: 0.00003165
Iteration 208/1000 | Loss: 0.00003165
Iteration 209/1000 | Loss: 0.00003165
Iteration 210/1000 | Loss: 0.00003165
Iteration 211/1000 | Loss: 0.00003165
Iteration 212/1000 | Loss: 0.00003165
Iteration 213/1000 | Loss: 0.00003165
Iteration 214/1000 | Loss: 0.00003165
Iteration 215/1000 | Loss: 0.00003165
Iteration 216/1000 | Loss: 0.00003165
Iteration 217/1000 | Loss: 0.00003165
Iteration 218/1000 | Loss: 0.00003165
Iteration 219/1000 | Loss: 0.00003165
Iteration 220/1000 | Loss: 0.00003165
Iteration 221/1000 | Loss: 0.00003164
Iteration 222/1000 | Loss: 0.00003164
Iteration 223/1000 | Loss: 0.00003164
Iteration 224/1000 | Loss: 0.00003164
Iteration 225/1000 | Loss: 0.00003164
Iteration 226/1000 | Loss: 0.00003164
Iteration 227/1000 | Loss: 0.00003164
Iteration 228/1000 | Loss: 0.00003164
Iteration 229/1000 | Loss: 0.00003164
Iteration 230/1000 | Loss: 0.00003164
Iteration 231/1000 | Loss: 0.00003164
Iteration 232/1000 | Loss: 0.00003164
Iteration 233/1000 | Loss: 0.00003164
Iteration 234/1000 | Loss: 0.00003164
Iteration 235/1000 | Loss: 0.00003164
Iteration 236/1000 | Loss: 0.00003164
Iteration 237/1000 | Loss: 0.00003164
Iteration 238/1000 | Loss: 0.00003164
Iteration 239/1000 | Loss: 0.00003164
Iteration 240/1000 | Loss: 0.00003164
Iteration 241/1000 | Loss: 0.00003164
Iteration 242/1000 | Loss: 0.00003164
Iteration 243/1000 | Loss: 0.00003164
Iteration 244/1000 | Loss: 0.00003164
Iteration 245/1000 | Loss: 0.00003164
Iteration 246/1000 | Loss: 0.00003164
Iteration 247/1000 | Loss: 0.00003164
Iteration 248/1000 | Loss: 0.00003164
Iteration 249/1000 | Loss: 0.00003164
Iteration 250/1000 | Loss: 0.00003164
Iteration 251/1000 | Loss: 0.00003164
Iteration 252/1000 | Loss: 0.00003164
Iteration 253/1000 | Loss: 0.00003164
Iteration 254/1000 | Loss: 0.00003164
Iteration 255/1000 | Loss: 0.00003164
Iteration 256/1000 | Loss: 0.00003164
Iteration 257/1000 | Loss: 0.00003164
Iteration 258/1000 | Loss: 0.00003164
Iteration 259/1000 | Loss: 0.00003164
Iteration 260/1000 | Loss: 0.00003164
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [3.164132795063779e-05, 3.164132795063779e-05, 3.164132795063779e-05, 3.164132795063779e-05, 3.164132795063779e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.164132795063779e-05

Optimization complete. Final v2v error: 4.749276161193848 mm

Highest mean error: 6.33793830871582 mm for frame 63

Lowest mean error: 4.2085371017456055 mm for frame 26

Saving results

Total time: 44.75082063674927
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_45_us_0001/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_45_us_0001/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886050
Iteration 2/25 | Loss: 0.00206873
Iteration 3/25 | Loss: 0.00178446
Iteration 4/25 | Loss: 0.00175675
Iteration 5/25 | Loss: 0.00174992
Iteration 6/25 | Loss: 0.00174866
Iteration 7/25 | Loss: 0.00174866
Iteration 8/25 | Loss: 0.00174866
Iteration 9/25 | Loss: 0.00174866
Iteration 10/25 | Loss: 0.00174866
Iteration 11/25 | Loss: 0.00174866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0017486624419689178, 0.0017486624419689178, 0.0017486624419689178, 0.0017486624419689178, 0.0017486624419689178]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017486624419689178

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.28690529
Iteration 2/25 | Loss: 0.00316325
Iteration 3/25 | Loss: 0.00316325
Iteration 4/25 | Loss: 0.00316325
Iteration 5/25 | Loss: 0.00316325
Iteration 6/25 | Loss: 0.00316325
Iteration 7/25 | Loss: 0.00316325
Iteration 8/25 | Loss: 0.00316325
Iteration 9/25 | Loss: 0.00316325
Iteration 10/25 | Loss: 0.00316325
Iteration 11/25 | Loss: 0.00316325
Iteration 12/25 | Loss: 0.00316325
Iteration 13/25 | Loss: 0.00316325
Iteration 14/25 | Loss: 0.00316325
Iteration 15/25 | Loss: 0.00316325
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0031632480677217245, 0.0031632480677217245, 0.0031632480677217245, 0.0031632480677217245, 0.0031632480677217245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0031632480677217245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00316325
Iteration 2/1000 | Loss: 0.00006391
Iteration 3/1000 | Loss: 0.00004450
Iteration 4/1000 | Loss: 0.00004008
Iteration 5/1000 | Loss: 0.00003688
Iteration 6/1000 | Loss: 0.00003504
Iteration 7/1000 | Loss: 0.00003383
Iteration 8/1000 | Loss: 0.00003296
Iteration 9/1000 | Loss: 0.00003235
Iteration 10/1000 | Loss: 0.00003190
Iteration 11/1000 | Loss: 0.00003158
Iteration 12/1000 | Loss: 0.00003128
Iteration 13/1000 | Loss: 0.00003106
Iteration 14/1000 | Loss: 0.00003098
Iteration 15/1000 | Loss: 0.00003097
Iteration 16/1000 | Loss: 0.00003096
Iteration 17/1000 | Loss: 0.00003091
Iteration 18/1000 | Loss: 0.00003089
Iteration 19/1000 | Loss: 0.00003088
Iteration 20/1000 | Loss: 0.00003088
Iteration 21/1000 | Loss: 0.00003083
Iteration 22/1000 | Loss: 0.00003082
Iteration 23/1000 | Loss: 0.00003080
Iteration 24/1000 | Loss: 0.00003079
Iteration 25/1000 | Loss: 0.00003078
Iteration 26/1000 | Loss: 0.00003078
Iteration 27/1000 | Loss: 0.00003078
Iteration 28/1000 | Loss: 0.00003077
Iteration 29/1000 | Loss: 0.00003076
Iteration 30/1000 | Loss: 0.00003075
Iteration 31/1000 | Loss: 0.00003074
Iteration 32/1000 | Loss: 0.00003071
Iteration 33/1000 | Loss: 0.00003071
Iteration 34/1000 | Loss: 0.00003070
Iteration 35/1000 | Loss: 0.00003070
Iteration 36/1000 | Loss: 0.00003069
Iteration 37/1000 | Loss: 0.00003069
Iteration 38/1000 | Loss: 0.00003069
Iteration 39/1000 | Loss: 0.00003068
Iteration 40/1000 | Loss: 0.00003067
Iteration 41/1000 | Loss: 0.00003067
Iteration 42/1000 | Loss: 0.00003066
Iteration 43/1000 | Loss: 0.00003066
Iteration 44/1000 | Loss: 0.00003066
Iteration 45/1000 | Loss: 0.00003065
Iteration 46/1000 | Loss: 0.00003065
Iteration 47/1000 | Loss: 0.00003064
Iteration 48/1000 | Loss: 0.00003064
Iteration 49/1000 | Loss: 0.00003064
Iteration 50/1000 | Loss: 0.00003063
Iteration 51/1000 | Loss: 0.00003063
Iteration 52/1000 | Loss: 0.00003063
Iteration 53/1000 | Loss: 0.00003062
Iteration 54/1000 | Loss: 0.00003062
Iteration 55/1000 | Loss: 0.00003062
Iteration 56/1000 | Loss: 0.00003062
Iteration 57/1000 | Loss: 0.00003061
Iteration 58/1000 | Loss: 0.00003061
Iteration 59/1000 | Loss: 0.00003060
Iteration 60/1000 | Loss: 0.00003060
Iteration 61/1000 | Loss: 0.00003060
Iteration 62/1000 | Loss: 0.00003060
Iteration 63/1000 | Loss: 0.00003060
Iteration 64/1000 | Loss: 0.00003060
Iteration 65/1000 | Loss: 0.00003060
Iteration 66/1000 | Loss: 0.00003059
Iteration 67/1000 | Loss: 0.00003059
Iteration 68/1000 | Loss: 0.00003058
Iteration 69/1000 | Loss: 0.00003058
Iteration 70/1000 | Loss: 0.00003058
Iteration 71/1000 | Loss: 0.00003057
Iteration 72/1000 | Loss: 0.00003057
Iteration 73/1000 | Loss: 0.00003057
Iteration 74/1000 | Loss: 0.00003056
Iteration 75/1000 | Loss: 0.00003056
Iteration 76/1000 | Loss: 0.00003056
Iteration 77/1000 | Loss: 0.00003056
Iteration 78/1000 | Loss: 0.00003056
Iteration 79/1000 | Loss: 0.00003055
Iteration 80/1000 | Loss: 0.00003055
Iteration 81/1000 | Loss: 0.00003054
Iteration 82/1000 | Loss: 0.00003054
Iteration 83/1000 | Loss: 0.00003054
Iteration 84/1000 | Loss: 0.00003054
Iteration 85/1000 | Loss: 0.00003054
Iteration 86/1000 | Loss: 0.00003054
Iteration 87/1000 | Loss: 0.00003054
Iteration 88/1000 | Loss: 0.00003054
Iteration 89/1000 | Loss: 0.00003054
Iteration 90/1000 | Loss: 0.00003053
Iteration 91/1000 | Loss: 0.00003053
Iteration 92/1000 | Loss: 0.00003053
Iteration 93/1000 | Loss: 0.00003053
Iteration 94/1000 | Loss: 0.00003053
Iteration 95/1000 | Loss: 0.00003053
Iteration 96/1000 | Loss: 0.00003053
Iteration 97/1000 | Loss: 0.00003053
Iteration 98/1000 | Loss: 0.00003053
Iteration 99/1000 | Loss: 0.00003053
Iteration 100/1000 | Loss: 0.00003052
Iteration 101/1000 | Loss: 0.00003052
Iteration 102/1000 | Loss: 0.00003052
Iteration 103/1000 | Loss: 0.00003052
Iteration 104/1000 | Loss: 0.00003052
Iteration 105/1000 | Loss: 0.00003052
Iteration 106/1000 | Loss: 0.00003052
Iteration 107/1000 | Loss: 0.00003052
Iteration 108/1000 | Loss: 0.00003052
Iteration 109/1000 | Loss: 0.00003052
Iteration 110/1000 | Loss: 0.00003052
Iteration 111/1000 | Loss: 0.00003051
Iteration 112/1000 | Loss: 0.00003051
Iteration 113/1000 | Loss: 0.00003051
Iteration 114/1000 | Loss: 0.00003051
Iteration 115/1000 | Loss: 0.00003051
Iteration 116/1000 | Loss: 0.00003051
Iteration 117/1000 | Loss: 0.00003051
Iteration 118/1000 | Loss: 0.00003050
Iteration 119/1000 | Loss: 0.00003050
Iteration 120/1000 | Loss: 0.00003050
Iteration 121/1000 | Loss: 0.00003050
Iteration 122/1000 | Loss: 0.00003050
Iteration 123/1000 | Loss: 0.00003050
Iteration 124/1000 | Loss: 0.00003050
Iteration 125/1000 | Loss: 0.00003050
Iteration 126/1000 | Loss: 0.00003050
Iteration 127/1000 | Loss: 0.00003050
Iteration 128/1000 | Loss: 0.00003050
Iteration 129/1000 | Loss: 0.00003050
Iteration 130/1000 | Loss: 0.00003050
Iteration 131/1000 | Loss: 0.00003050
Iteration 132/1000 | Loss: 0.00003050
Iteration 133/1000 | Loss: 0.00003050
Iteration 134/1000 | Loss: 0.00003049
Iteration 135/1000 | Loss: 0.00003049
Iteration 136/1000 | Loss: 0.00003049
Iteration 137/1000 | Loss: 0.00003049
Iteration 138/1000 | Loss: 0.00003049
Iteration 139/1000 | Loss: 0.00003049
Iteration 140/1000 | Loss: 0.00003049
Iteration 141/1000 | Loss: 0.00003049
Iteration 142/1000 | Loss: 0.00003049
Iteration 143/1000 | Loss: 0.00003049
Iteration 144/1000 | Loss: 0.00003049
Iteration 145/1000 | Loss: 0.00003049
Iteration 146/1000 | Loss: 0.00003049
Iteration 147/1000 | Loss: 0.00003049
Iteration 148/1000 | Loss: 0.00003048
Iteration 149/1000 | Loss: 0.00003048
Iteration 150/1000 | Loss: 0.00003048
Iteration 151/1000 | Loss: 0.00003048
Iteration 152/1000 | Loss: 0.00003048
Iteration 153/1000 | Loss: 0.00003048
Iteration 154/1000 | Loss: 0.00003048
Iteration 155/1000 | Loss: 0.00003048
Iteration 156/1000 | Loss: 0.00003048
Iteration 157/1000 | Loss: 0.00003048
Iteration 158/1000 | Loss: 0.00003048
Iteration 159/1000 | Loss: 0.00003048
Iteration 160/1000 | Loss: 0.00003048
Iteration 161/1000 | Loss: 0.00003047
Iteration 162/1000 | Loss: 0.00003047
Iteration 163/1000 | Loss: 0.00003047
Iteration 164/1000 | Loss: 0.00003047
Iteration 165/1000 | Loss: 0.00003047
Iteration 166/1000 | Loss: 0.00003047
Iteration 167/1000 | Loss: 0.00003047
Iteration 168/1000 | Loss: 0.00003047
Iteration 169/1000 | Loss: 0.00003047
Iteration 170/1000 | Loss: 0.00003047
Iteration 171/1000 | Loss: 0.00003047
Iteration 172/1000 | Loss: 0.00003047
Iteration 173/1000 | Loss: 0.00003047
Iteration 174/1000 | Loss: 0.00003047
Iteration 175/1000 | Loss: 0.00003047
Iteration 176/1000 | Loss: 0.00003047
Iteration 177/1000 | Loss: 0.00003047
Iteration 178/1000 | Loss: 0.00003047
Iteration 179/1000 | Loss: 0.00003047
Iteration 180/1000 | Loss: 0.00003047
Iteration 181/1000 | Loss: 0.00003047
Iteration 182/1000 | Loss: 0.00003047
Iteration 183/1000 | Loss: 0.00003047
Iteration 184/1000 | Loss: 0.00003047
Iteration 185/1000 | Loss: 0.00003047
Iteration 186/1000 | Loss: 0.00003047
Iteration 187/1000 | Loss: 0.00003047
Iteration 188/1000 | Loss: 0.00003047
Iteration 189/1000 | Loss: 0.00003047
Iteration 190/1000 | Loss: 0.00003047
Iteration 191/1000 | Loss: 0.00003047
Iteration 192/1000 | Loss: 0.00003047
Iteration 193/1000 | Loss: 0.00003047
Iteration 194/1000 | Loss: 0.00003047
Iteration 195/1000 | Loss: 0.00003047
Iteration 196/1000 | Loss: 0.00003047
Iteration 197/1000 | Loss: 0.00003047
Iteration 198/1000 | Loss: 0.00003047
Iteration 199/1000 | Loss: 0.00003047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [3.0470280762529e-05, 3.0470280762529e-05, 3.0470280762529e-05, 3.0470280762529e-05, 3.0470280762529e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0470280762529e-05

Optimization complete. Final v2v error: 4.852800369262695 mm

Highest mean error: 5.653626441955566 mm for frame 169

Lowest mean error: 4.301383018493652 mm for frame 7

Saving results

Total time: 43.58655285835266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432420
Iteration 2/25 | Loss: 0.00147852
Iteration 3/25 | Loss: 0.00138424
Iteration 4/25 | Loss: 0.00137288
Iteration 5/25 | Loss: 0.00136950
Iteration 6/25 | Loss: 0.00136926
Iteration 7/25 | Loss: 0.00136926
Iteration 8/25 | Loss: 0.00136926
Iteration 9/25 | Loss: 0.00136926
Iteration 10/25 | Loss: 0.00136926
Iteration 11/25 | Loss: 0.00136926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013692574575543404, 0.0013692574575543404, 0.0013692574575543404, 0.0013692574575543404, 0.0013692574575543404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013692574575543404

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06676781
Iteration 2/25 | Loss: 0.00572700
Iteration 3/25 | Loss: 0.00572700
Iteration 4/25 | Loss: 0.00572700
Iteration 5/25 | Loss: 0.00572700
Iteration 6/25 | Loss: 0.00572700
Iteration 7/25 | Loss: 0.00572700
Iteration 8/25 | Loss: 0.00572700
Iteration 9/25 | Loss: 0.00572700
Iteration 10/25 | Loss: 0.00572700
Iteration 11/25 | Loss: 0.00572700
Iteration 12/25 | Loss: 0.00572700
Iteration 13/25 | Loss: 0.00572700
Iteration 14/25 | Loss: 0.00572700
Iteration 15/25 | Loss: 0.00572700
Iteration 16/25 | Loss: 0.00572700
Iteration 17/25 | Loss: 0.00572700
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.005726999137550592, 0.005726999137550592, 0.005726999137550592, 0.005726999137550592, 0.005726999137550592]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005726999137550592

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00572700
Iteration 2/1000 | Loss: 0.00005403
Iteration 3/1000 | Loss: 0.00003186
Iteration 4/1000 | Loss: 0.00002342
Iteration 5/1000 | Loss: 0.00002140
Iteration 6/1000 | Loss: 0.00002023
Iteration 7/1000 | Loss: 0.00001911
Iteration 8/1000 | Loss: 0.00001831
Iteration 9/1000 | Loss: 0.00001781
Iteration 10/1000 | Loss: 0.00001740
Iteration 11/1000 | Loss: 0.00001685
Iteration 12/1000 | Loss: 0.00001651
Iteration 13/1000 | Loss: 0.00001622
Iteration 14/1000 | Loss: 0.00001595
Iteration 15/1000 | Loss: 0.00001575
Iteration 16/1000 | Loss: 0.00001554
Iteration 17/1000 | Loss: 0.00001552
Iteration 18/1000 | Loss: 0.00001548
Iteration 19/1000 | Loss: 0.00001540
Iteration 20/1000 | Loss: 0.00001527
Iteration 21/1000 | Loss: 0.00001518
Iteration 22/1000 | Loss: 0.00001514
Iteration 23/1000 | Loss: 0.00001514
Iteration 24/1000 | Loss: 0.00001513
Iteration 25/1000 | Loss: 0.00001511
Iteration 26/1000 | Loss: 0.00001511
Iteration 27/1000 | Loss: 0.00001507
Iteration 28/1000 | Loss: 0.00001506
Iteration 29/1000 | Loss: 0.00001501
Iteration 30/1000 | Loss: 0.00001500
Iteration 31/1000 | Loss: 0.00001499
Iteration 32/1000 | Loss: 0.00001498
Iteration 33/1000 | Loss: 0.00001498
Iteration 34/1000 | Loss: 0.00001497
Iteration 35/1000 | Loss: 0.00001497
Iteration 36/1000 | Loss: 0.00001496
Iteration 37/1000 | Loss: 0.00001495
Iteration 38/1000 | Loss: 0.00001494
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001491
Iteration 42/1000 | Loss: 0.00001489
Iteration 43/1000 | Loss: 0.00001479
Iteration 44/1000 | Loss: 0.00001477
Iteration 45/1000 | Loss: 0.00001474
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001462
Iteration 52/1000 | Loss: 0.00001461
Iteration 53/1000 | Loss: 0.00001461
Iteration 54/1000 | Loss: 0.00001460
Iteration 55/1000 | Loss: 0.00001459
Iteration 56/1000 | Loss: 0.00001458
Iteration 57/1000 | Loss: 0.00001457
Iteration 58/1000 | Loss: 0.00001456
Iteration 59/1000 | Loss: 0.00001456
Iteration 60/1000 | Loss: 0.00001455
Iteration 61/1000 | Loss: 0.00001454
Iteration 62/1000 | Loss: 0.00001453
Iteration 63/1000 | Loss: 0.00001453
Iteration 64/1000 | Loss: 0.00001452
Iteration 65/1000 | Loss: 0.00001452
Iteration 66/1000 | Loss: 0.00001452
Iteration 67/1000 | Loss: 0.00001451
Iteration 68/1000 | Loss: 0.00001451
Iteration 69/1000 | Loss: 0.00001451
Iteration 70/1000 | Loss: 0.00001451
Iteration 71/1000 | Loss: 0.00001450
Iteration 72/1000 | Loss: 0.00001450
Iteration 73/1000 | Loss: 0.00001450
Iteration 74/1000 | Loss: 0.00001450
Iteration 75/1000 | Loss: 0.00001449
Iteration 76/1000 | Loss: 0.00001449
Iteration 77/1000 | Loss: 0.00001449
Iteration 78/1000 | Loss: 0.00001449
Iteration 79/1000 | Loss: 0.00001449
Iteration 80/1000 | Loss: 0.00001449
Iteration 81/1000 | Loss: 0.00001449
Iteration 82/1000 | Loss: 0.00001448
Iteration 83/1000 | Loss: 0.00001448
Iteration 84/1000 | Loss: 0.00001448
Iteration 85/1000 | Loss: 0.00001448
Iteration 86/1000 | Loss: 0.00001448
Iteration 87/1000 | Loss: 0.00001447
Iteration 88/1000 | Loss: 0.00001447
Iteration 89/1000 | Loss: 0.00001447
Iteration 90/1000 | Loss: 0.00001446
Iteration 91/1000 | Loss: 0.00001446
Iteration 92/1000 | Loss: 0.00001446
Iteration 93/1000 | Loss: 0.00001446
Iteration 94/1000 | Loss: 0.00001445
Iteration 95/1000 | Loss: 0.00001445
Iteration 96/1000 | Loss: 0.00001445
Iteration 97/1000 | Loss: 0.00001445
Iteration 98/1000 | Loss: 0.00001444
Iteration 99/1000 | Loss: 0.00001444
Iteration 100/1000 | Loss: 0.00001443
Iteration 101/1000 | Loss: 0.00001443
Iteration 102/1000 | Loss: 0.00001443
Iteration 103/1000 | Loss: 0.00001443
Iteration 104/1000 | Loss: 0.00001442
Iteration 105/1000 | Loss: 0.00001442
Iteration 106/1000 | Loss: 0.00001442
Iteration 107/1000 | Loss: 0.00001442
Iteration 108/1000 | Loss: 0.00001442
Iteration 109/1000 | Loss: 0.00001442
Iteration 110/1000 | Loss: 0.00001441
Iteration 111/1000 | Loss: 0.00001441
Iteration 112/1000 | Loss: 0.00001441
Iteration 113/1000 | Loss: 0.00001441
Iteration 114/1000 | Loss: 0.00001441
Iteration 115/1000 | Loss: 0.00001441
Iteration 116/1000 | Loss: 0.00001441
Iteration 117/1000 | Loss: 0.00001440
Iteration 118/1000 | Loss: 0.00001440
Iteration 119/1000 | Loss: 0.00001440
Iteration 120/1000 | Loss: 0.00001440
Iteration 121/1000 | Loss: 0.00001440
Iteration 122/1000 | Loss: 0.00001440
Iteration 123/1000 | Loss: 0.00001440
Iteration 124/1000 | Loss: 0.00001440
Iteration 125/1000 | Loss: 0.00001440
Iteration 126/1000 | Loss: 0.00001440
Iteration 127/1000 | Loss: 0.00001440
Iteration 128/1000 | Loss: 0.00001440
Iteration 129/1000 | Loss: 0.00001440
Iteration 130/1000 | Loss: 0.00001440
Iteration 131/1000 | Loss: 0.00001440
Iteration 132/1000 | Loss: 0.00001440
Iteration 133/1000 | Loss: 0.00001440
Iteration 134/1000 | Loss: 0.00001440
Iteration 135/1000 | Loss: 0.00001440
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.4397995073522907e-05, 1.4397995073522907e-05, 1.4397995073522907e-05, 1.4397995073522907e-05, 1.4397995073522907e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4397995073522907e-05

Optimization complete. Final v2v error: 3.258256673812866 mm

Highest mean error: 3.927906036376953 mm for frame 11

Lowest mean error: 2.7641024589538574 mm for frame 184

Saving results

Total time: 52.650187253952026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967343
Iteration 2/25 | Loss: 0.00335180
Iteration 3/25 | Loss: 0.00220613
Iteration 4/25 | Loss: 0.00196451
Iteration 5/25 | Loss: 0.00183028
Iteration 6/25 | Loss: 0.00181857
Iteration 7/25 | Loss: 0.00180208
Iteration 8/25 | Loss: 0.00169645
Iteration 9/25 | Loss: 0.00162695
Iteration 10/25 | Loss: 0.00160736
Iteration 11/25 | Loss: 0.00158523
Iteration 12/25 | Loss: 0.00156293
Iteration 13/25 | Loss: 0.00155208
Iteration 14/25 | Loss: 0.00153453
Iteration 15/25 | Loss: 0.00153532
Iteration 16/25 | Loss: 0.00152122
Iteration 17/25 | Loss: 0.00152011
Iteration 18/25 | Loss: 0.00150732
Iteration 19/25 | Loss: 0.00150402
Iteration 20/25 | Loss: 0.00149793
Iteration 21/25 | Loss: 0.00149835
Iteration 22/25 | Loss: 0.00149859
Iteration 23/25 | Loss: 0.00149935
Iteration 24/25 | Loss: 0.00149792
Iteration 25/25 | Loss: 0.00149616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07592189
Iteration 2/25 | Loss: 0.00625130
Iteration 3/25 | Loss: 0.00619747
Iteration 4/25 | Loss: 0.00619747
Iteration 5/25 | Loss: 0.00619746
Iteration 6/25 | Loss: 0.00619746
Iteration 7/25 | Loss: 0.00619746
Iteration 8/25 | Loss: 0.00619746
Iteration 9/25 | Loss: 0.00619746
Iteration 10/25 | Loss: 0.00619746
Iteration 11/25 | Loss: 0.00619746
Iteration 12/25 | Loss: 0.00619746
Iteration 13/25 | Loss: 0.00619746
Iteration 14/25 | Loss: 0.00619746
Iteration 15/25 | Loss: 0.00619746
Iteration 16/25 | Loss: 0.00619746
Iteration 17/25 | Loss: 0.00619746
Iteration 18/25 | Loss: 0.00619746
Iteration 19/25 | Loss: 0.00619746
Iteration 20/25 | Loss: 0.00619746
Iteration 21/25 | Loss: 0.00619746
Iteration 22/25 | Loss: 0.00619746
Iteration 23/25 | Loss: 0.00619746
Iteration 24/25 | Loss: 0.00619746
Iteration 25/25 | Loss: 0.00619746

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00619746
Iteration 2/1000 | Loss: 0.00088799
Iteration 3/1000 | Loss: 0.00178615
Iteration 4/1000 | Loss: 0.00036408
Iteration 5/1000 | Loss: 0.00054485
Iteration 6/1000 | Loss: 0.00046687
Iteration 7/1000 | Loss: 0.00049300
Iteration 8/1000 | Loss: 0.00085054
Iteration 9/1000 | Loss: 0.00036510
Iteration 10/1000 | Loss: 0.00044769
Iteration 11/1000 | Loss: 0.00048234
Iteration 12/1000 | Loss: 0.00017032
Iteration 13/1000 | Loss: 0.00017188
Iteration 14/1000 | Loss: 0.00052210
Iteration 15/1000 | Loss: 0.00014204
Iteration 16/1000 | Loss: 0.00015027
Iteration 17/1000 | Loss: 0.00036404
Iteration 18/1000 | Loss: 0.00009610
Iteration 19/1000 | Loss: 0.00052619
Iteration 20/1000 | Loss: 0.00008966
Iteration 21/1000 | Loss: 0.00010245
Iteration 22/1000 | Loss: 0.00021737
Iteration 23/1000 | Loss: 0.00011005
Iteration 24/1000 | Loss: 0.00017183
Iteration 25/1000 | Loss: 0.00010614
Iteration 26/1000 | Loss: 0.00011918
Iteration 27/1000 | Loss: 0.00007340
Iteration 28/1000 | Loss: 0.00037499
Iteration 29/1000 | Loss: 0.00023377
Iteration 30/1000 | Loss: 0.00009075
Iteration 31/1000 | Loss: 0.00009002
Iteration 32/1000 | Loss: 0.00035508
Iteration 33/1000 | Loss: 0.00010323
Iteration 34/1000 | Loss: 0.00037003
Iteration 35/1000 | Loss: 0.00008605
Iteration 36/1000 | Loss: 0.00008275
Iteration 37/1000 | Loss: 0.00050781
Iteration 38/1000 | Loss: 0.00021427
Iteration 39/1000 | Loss: 0.00007691
Iteration 40/1000 | Loss: 0.00008110
Iteration 41/1000 | Loss: 0.00022661
Iteration 42/1000 | Loss: 0.00067181
Iteration 43/1000 | Loss: 0.00077859
Iteration 44/1000 | Loss: 0.00047476
Iteration 45/1000 | Loss: 0.00025473
Iteration 46/1000 | Loss: 0.00007628
Iteration 47/1000 | Loss: 0.00010679
Iteration 48/1000 | Loss: 0.00006253
Iteration 49/1000 | Loss: 0.00011700
Iteration 50/1000 | Loss: 0.00003849
Iteration 51/1000 | Loss: 0.00005010
Iteration 52/1000 | Loss: 0.00003344
Iteration 53/1000 | Loss: 0.00003425
Iteration 54/1000 | Loss: 0.00003532
Iteration 55/1000 | Loss: 0.00003026
Iteration 56/1000 | Loss: 0.00004887
Iteration 57/1000 | Loss: 0.00003899
Iteration 58/1000 | Loss: 0.00005480
Iteration 59/1000 | Loss: 0.00003076
Iteration 60/1000 | Loss: 0.00004961
Iteration 61/1000 | Loss: 0.00002731
Iteration 62/1000 | Loss: 0.00002486
Iteration 63/1000 | Loss: 0.00002457
Iteration 64/1000 | Loss: 0.00027877
Iteration 65/1000 | Loss: 0.00018902
Iteration 66/1000 | Loss: 0.00002480
Iteration 67/1000 | Loss: 0.00021986
Iteration 68/1000 | Loss: 0.00020309
Iteration 69/1000 | Loss: 0.00033784
Iteration 70/1000 | Loss: 0.00037133
Iteration 71/1000 | Loss: 0.00002927
Iteration 72/1000 | Loss: 0.00002505
Iteration 73/1000 | Loss: 0.00026390
Iteration 74/1000 | Loss: 0.00010833
Iteration 75/1000 | Loss: 0.00003279
Iteration 76/1000 | Loss: 0.00004115
Iteration 77/1000 | Loss: 0.00003203
Iteration 78/1000 | Loss: 0.00003081
Iteration 79/1000 | Loss: 0.00003509
Iteration 80/1000 | Loss: 0.00002661
Iteration 81/1000 | Loss: 0.00002825
Iteration 82/1000 | Loss: 0.00003335
Iteration 83/1000 | Loss: 0.00003216
Iteration 84/1000 | Loss: 0.00002366
Iteration 85/1000 | Loss: 0.00002266
Iteration 86/1000 | Loss: 0.00002411
Iteration 87/1000 | Loss: 0.00002871
Iteration 88/1000 | Loss: 0.00002326
Iteration 89/1000 | Loss: 0.00002287
Iteration 90/1000 | Loss: 0.00002233
Iteration 91/1000 | Loss: 0.00002233
Iteration 92/1000 | Loss: 0.00002232
Iteration 93/1000 | Loss: 0.00002232
Iteration 94/1000 | Loss: 0.00002232
Iteration 95/1000 | Loss: 0.00002232
Iteration 96/1000 | Loss: 0.00002232
Iteration 97/1000 | Loss: 0.00002231
Iteration 98/1000 | Loss: 0.00002231
Iteration 99/1000 | Loss: 0.00002231
Iteration 100/1000 | Loss: 0.00002230
Iteration 101/1000 | Loss: 0.00002230
Iteration 102/1000 | Loss: 0.00002229
Iteration 103/1000 | Loss: 0.00015869
Iteration 104/1000 | Loss: 0.00005508
Iteration 105/1000 | Loss: 0.00002397
Iteration 106/1000 | Loss: 0.00015899
Iteration 107/1000 | Loss: 0.00005977
Iteration 108/1000 | Loss: 0.00002894
Iteration 109/1000 | Loss: 0.00016961
Iteration 110/1000 | Loss: 0.00007238
Iteration 111/1000 | Loss: 0.00002806
Iteration 112/1000 | Loss: 0.00019400
Iteration 113/1000 | Loss: 0.00010067
Iteration 114/1000 | Loss: 0.00002617
Iteration 115/1000 | Loss: 0.00013803
Iteration 116/1000 | Loss: 0.00009309
Iteration 117/1000 | Loss: 0.00008518
Iteration 118/1000 | Loss: 0.00006357
Iteration 119/1000 | Loss: 0.00002575
Iteration 120/1000 | Loss: 0.00004157
Iteration 121/1000 | Loss: 0.00002438
Iteration 122/1000 | Loss: 0.00002438
Iteration 123/1000 | Loss: 0.00002790
Iteration 124/1000 | Loss: 0.00002383
Iteration 125/1000 | Loss: 0.00002782
Iteration 126/1000 | Loss: 0.00002439
Iteration 127/1000 | Loss: 0.00002179
Iteration 128/1000 | Loss: 0.00002179
Iteration 129/1000 | Loss: 0.00002179
Iteration 130/1000 | Loss: 0.00002179
Iteration 131/1000 | Loss: 0.00002179
Iteration 132/1000 | Loss: 0.00002179
Iteration 133/1000 | Loss: 0.00002179
Iteration 134/1000 | Loss: 0.00002179
Iteration 135/1000 | Loss: 0.00002179
Iteration 136/1000 | Loss: 0.00002179
Iteration 137/1000 | Loss: 0.00002179
Iteration 138/1000 | Loss: 0.00002179
Iteration 139/1000 | Loss: 0.00002178
Iteration 140/1000 | Loss: 0.00002178
Iteration 141/1000 | Loss: 0.00002178
Iteration 142/1000 | Loss: 0.00002178
Iteration 143/1000 | Loss: 0.00002178
Iteration 144/1000 | Loss: 0.00002178
Iteration 145/1000 | Loss: 0.00002178
Iteration 146/1000 | Loss: 0.00002178
Iteration 147/1000 | Loss: 0.00002178
Iteration 148/1000 | Loss: 0.00002178
Iteration 149/1000 | Loss: 0.00002177
Iteration 150/1000 | Loss: 0.00002177
Iteration 151/1000 | Loss: 0.00002177
Iteration 152/1000 | Loss: 0.00002177
Iteration 153/1000 | Loss: 0.00002176
Iteration 154/1000 | Loss: 0.00002176
Iteration 155/1000 | Loss: 0.00002175
Iteration 156/1000 | Loss: 0.00002175
Iteration 157/1000 | Loss: 0.00002174
Iteration 158/1000 | Loss: 0.00002173
Iteration 159/1000 | Loss: 0.00002173
Iteration 160/1000 | Loss: 0.00002264
Iteration 161/1000 | Loss: 0.00002488
Iteration 162/1000 | Loss: 0.00002179
Iteration 163/1000 | Loss: 0.00002170
Iteration 164/1000 | Loss: 0.00002170
Iteration 165/1000 | Loss: 0.00002170
Iteration 166/1000 | Loss: 0.00002170
Iteration 167/1000 | Loss: 0.00002170
Iteration 168/1000 | Loss: 0.00002169
Iteration 169/1000 | Loss: 0.00002169
Iteration 170/1000 | Loss: 0.00002169
Iteration 171/1000 | Loss: 0.00002169
Iteration 172/1000 | Loss: 0.00002169
Iteration 173/1000 | Loss: 0.00002169
Iteration 174/1000 | Loss: 0.00002169
Iteration 175/1000 | Loss: 0.00002169
Iteration 176/1000 | Loss: 0.00002169
Iteration 177/1000 | Loss: 0.00002169
Iteration 178/1000 | Loss: 0.00002168
Iteration 179/1000 | Loss: 0.00002168
Iteration 180/1000 | Loss: 0.00002168
Iteration 181/1000 | Loss: 0.00002168
Iteration 182/1000 | Loss: 0.00002168
Iteration 183/1000 | Loss: 0.00002168
Iteration 184/1000 | Loss: 0.00002168
Iteration 185/1000 | Loss: 0.00002271
Iteration 186/1000 | Loss: 0.00002174
Iteration 187/1000 | Loss: 0.00002168
Iteration 188/1000 | Loss: 0.00002167
Iteration 189/1000 | Loss: 0.00002167
Iteration 190/1000 | Loss: 0.00002167
Iteration 191/1000 | Loss: 0.00002167
Iteration 192/1000 | Loss: 0.00002167
Iteration 193/1000 | Loss: 0.00002167
Iteration 194/1000 | Loss: 0.00002167
Iteration 195/1000 | Loss: 0.00002167
Iteration 196/1000 | Loss: 0.00002166
Iteration 197/1000 | Loss: 0.00002171
Iteration 198/1000 | Loss: 0.00003091
Iteration 199/1000 | Loss: 0.00002277
Iteration 200/1000 | Loss: 0.00002166
Iteration 201/1000 | Loss: 0.00002166
Iteration 202/1000 | Loss: 0.00002165
Iteration 203/1000 | Loss: 0.00002165
Iteration 204/1000 | Loss: 0.00002165
Iteration 205/1000 | Loss: 0.00002164
Iteration 206/1000 | Loss: 0.00002164
Iteration 207/1000 | Loss: 0.00002174
Iteration 208/1000 | Loss: 0.00002161
Iteration 209/1000 | Loss: 0.00002161
Iteration 210/1000 | Loss: 0.00002161
Iteration 211/1000 | Loss: 0.00002161
Iteration 212/1000 | Loss: 0.00002161
Iteration 213/1000 | Loss: 0.00002161
Iteration 214/1000 | Loss: 0.00002161
Iteration 215/1000 | Loss: 0.00002161
Iteration 216/1000 | Loss: 0.00002161
Iteration 217/1000 | Loss: 0.00002161
Iteration 218/1000 | Loss: 0.00002161
Iteration 219/1000 | Loss: 0.00002161
Iteration 220/1000 | Loss: 0.00002161
Iteration 221/1000 | Loss: 0.00002161
Iteration 222/1000 | Loss: 0.00002161
Iteration 223/1000 | Loss: 0.00002161
Iteration 224/1000 | Loss: 0.00002161
Iteration 225/1000 | Loss: 0.00002161
Iteration 226/1000 | Loss: 0.00002161
Iteration 227/1000 | Loss: 0.00002161
Iteration 228/1000 | Loss: 0.00002161
Iteration 229/1000 | Loss: 0.00002161
Iteration 230/1000 | Loss: 0.00002161
Iteration 231/1000 | Loss: 0.00002161
Iteration 232/1000 | Loss: 0.00002161
Iteration 233/1000 | Loss: 0.00002161
Iteration 234/1000 | Loss: 0.00002161
Iteration 235/1000 | Loss: 0.00002161
Iteration 236/1000 | Loss: 0.00002161
Iteration 237/1000 | Loss: 0.00002161
Iteration 238/1000 | Loss: 0.00002161
Iteration 239/1000 | Loss: 0.00002161
Iteration 240/1000 | Loss: 0.00002161
Iteration 241/1000 | Loss: 0.00002161
Iteration 242/1000 | Loss: 0.00002161
Iteration 243/1000 | Loss: 0.00002161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.160824260499794e-05, 2.160824260499794e-05, 2.160824260499794e-05, 2.160824260499794e-05, 2.160824260499794e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.160824260499794e-05

Optimization complete. Final v2v error: 3.0563783645629883 mm

Highest mean error: 11.695427894592285 mm for frame 15

Lowest mean error: 2.4802801609039307 mm for frame 151

Saving results

Total time: 240.7486937046051
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00965612
Iteration 2/25 | Loss: 0.00321162
Iteration 3/25 | Loss: 0.00227640
Iteration 4/25 | Loss: 0.00213725
Iteration 5/25 | Loss: 0.00203474
Iteration 6/25 | Loss: 0.00201910
Iteration 7/25 | Loss: 0.00204916
Iteration 8/25 | Loss: 0.00209702
Iteration 9/25 | Loss: 0.00204656
Iteration 10/25 | Loss: 0.00190136
Iteration 11/25 | Loss: 0.00184258
Iteration 12/25 | Loss: 0.00182257
Iteration 13/25 | Loss: 0.00182185
Iteration 14/25 | Loss: 0.00180173
Iteration 15/25 | Loss: 0.00177406
Iteration 16/25 | Loss: 0.00175743
Iteration 17/25 | Loss: 0.00173985
Iteration 18/25 | Loss: 0.00172325
Iteration 19/25 | Loss: 0.00172490
Iteration 20/25 | Loss: 0.00171968
Iteration 21/25 | Loss: 0.00171580
Iteration 22/25 | Loss: 0.00171133
Iteration 23/25 | Loss: 0.00170970
Iteration 24/25 | Loss: 0.00170983
Iteration 25/25 | Loss: 0.00170911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05348158
Iteration 2/25 | Loss: 0.00692243
Iteration 3/25 | Loss: 0.00670446
Iteration 4/25 | Loss: 0.00670446
Iteration 5/25 | Loss: 0.00670445
Iteration 6/25 | Loss: 0.00670445
Iteration 7/25 | Loss: 0.00670445
Iteration 8/25 | Loss: 0.00670445
Iteration 9/25 | Loss: 0.00670445
Iteration 10/25 | Loss: 0.00670445
Iteration 11/25 | Loss: 0.00670445
Iteration 12/25 | Loss: 0.00670445
Iteration 13/25 | Loss: 0.00670445
Iteration 14/25 | Loss: 0.00670445
Iteration 15/25 | Loss: 0.00670445
Iteration 16/25 | Loss: 0.00670445
Iteration 17/25 | Loss: 0.00670445
Iteration 18/25 | Loss: 0.00670445
Iteration 19/25 | Loss: 0.00670445
Iteration 20/25 | Loss: 0.00670445
Iteration 21/25 | Loss: 0.00670445
Iteration 22/25 | Loss: 0.00670445
Iteration 23/25 | Loss: 0.00670445
Iteration 24/25 | Loss: 0.00670445
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.006704452447593212, 0.006704452447593212, 0.006704452447593212, 0.006704452447593212, 0.006704452447593212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006704452447593212

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00670445
Iteration 2/1000 | Loss: 0.00175238
Iteration 3/1000 | Loss: 0.00243287
Iteration 4/1000 | Loss: 0.00138654
Iteration 5/1000 | Loss: 0.00030625
Iteration 6/1000 | Loss: 0.00034070
Iteration 7/1000 | Loss: 0.00080615
Iteration 8/1000 | Loss: 0.00029990
Iteration 9/1000 | Loss: 0.00076896
Iteration 10/1000 | Loss: 0.00024996
Iteration 11/1000 | Loss: 0.00026267
Iteration 12/1000 | Loss: 0.00027430
Iteration 13/1000 | Loss: 0.00022159
Iteration 14/1000 | Loss: 0.00030477
Iteration 15/1000 | Loss: 0.00052521
Iteration 16/1000 | Loss: 0.00022616
Iteration 17/1000 | Loss: 0.00049464
Iteration 18/1000 | Loss: 0.00029652
Iteration 19/1000 | Loss: 0.00086892
Iteration 20/1000 | Loss: 0.00068169
Iteration 21/1000 | Loss: 0.00123241
Iteration 22/1000 | Loss: 0.00032017
Iteration 23/1000 | Loss: 0.00035811
Iteration 24/1000 | Loss: 0.00022083
Iteration 25/1000 | Loss: 0.00099813
Iteration 26/1000 | Loss: 0.00153164
Iteration 27/1000 | Loss: 0.00114554
Iteration 28/1000 | Loss: 0.00318210
Iteration 29/1000 | Loss: 0.00590059
Iteration 30/1000 | Loss: 0.00680645
Iteration 31/1000 | Loss: 0.00515654
Iteration 32/1000 | Loss: 0.00256058
Iteration 33/1000 | Loss: 0.00056346
Iteration 34/1000 | Loss: 0.00192656
Iteration 35/1000 | Loss: 0.00231347
Iteration 36/1000 | Loss: 0.00036441
Iteration 37/1000 | Loss: 0.00024722
Iteration 38/1000 | Loss: 0.00059917
Iteration 39/1000 | Loss: 0.00042446
Iteration 40/1000 | Loss: 0.00028533
Iteration 41/1000 | Loss: 0.00034569
Iteration 42/1000 | Loss: 0.00032513
Iteration 43/1000 | Loss: 0.00042104
Iteration 44/1000 | Loss: 0.00019386
Iteration 45/1000 | Loss: 0.00022682
Iteration 46/1000 | Loss: 0.00014774
Iteration 47/1000 | Loss: 0.00108081
Iteration 48/1000 | Loss: 0.00030275
Iteration 49/1000 | Loss: 0.00060087
Iteration 50/1000 | Loss: 0.00059717
Iteration 51/1000 | Loss: 0.00055999
Iteration 52/1000 | Loss: 0.00020472
Iteration 53/1000 | Loss: 0.00051457
Iteration 54/1000 | Loss: 0.00043193
Iteration 55/1000 | Loss: 0.00014580
Iteration 56/1000 | Loss: 0.00042152
Iteration 57/1000 | Loss: 0.00013996
Iteration 58/1000 | Loss: 0.00028697
Iteration 59/1000 | Loss: 0.00066899
Iteration 60/1000 | Loss: 0.00162143
Iteration 61/1000 | Loss: 0.00299898
Iteration 62/1000 | Loss: 0.00169666
Iteration 63/1000 | Loss: 0.00054639
Iteration 64/1000 | Loss: 0.00019704
Iteration 65/1000 | Loss: 0.00013938
Iteration 66/1000 | Loss: 0.00015650
Iteration 67/1000 | Loss: 0.00013408
Iteration 68/1000 | Loss: 0.00016160
Iteration 69/1000 | Loss: 0.00024641
Iteration 70/1000 | Loss: 0.00008883
Iteration 71/1000 | Loss: 0.00008485
Iteration 72/1000 | Loss: 0.00008055
Iteration 73/1000 | Loss: 0.00007794
Iteration 74/1000 | Loss: 0.00007599
Iteration 75/1000 | Loss: 0.00014812
Iteration 76/1000 | Loss: 0.00007383
Iteration 77/1000 | Loss: 0.00029801
Iteration 78/1000 | Loss: 0.00088483
Iteration 79/1000 | Loss: 0.00060277
Iteration 80/1000 | Loss: 0.00073322
Iteration 81/1000 | Loss: 0.00027712
Iteration 82/1000 | Loss: 0.00008083
Iteration 83/1000 | Loss: 0.00022115
Iteration 84/1000 | Loss: 0.00007381
Iteration 85/1000 | Loss: 0.00006694
Iteration 86/1000 | Loss: 0.00029613
Iteration 87/1000 | Loss: 0.00021630
Iteration 88/1000 | Loss: 0.00012868
Iteration 89/1000 | Loss: 0.00007251
Iteration 90/1000 | Loss: 0.00006436
Iteration 91/1000 | Loss: 0.00006131
Iteration 92/1000 | Loss: 0.00005920
Iteration 93/1000 | Loss: 0.00005771
Iteration 94/1000 | Loss: 0.00011507
Iteration 95/1000 | Loss: 0.00005680
Iteration 96/1000 | Loss: 0.00005553
Iteration 97/1000 | Loss: 0.00005509
Iteration 98/1000 | Loss: 0.00005474
Iteration 99/1000 | Loss: 0.00005443
Iteration 100/1000 | Loss: 0.00029296
Iteration 101/1000 | Loss: 0.00063087
Iteration 102/1000 | Loss: 0.00043483
Iteration 103/1000 | Loss: 0.00013517
Iteration 104/1000 | Loss: 0.00007251
Iteration 105/1000 | Loss: 0.00006543
Iteration 106/1000 | Loss: 0.00005895
Iteration 107/1000 | Loss: 0.00005582
Iteration 108/1000 | Loss: 0.00005166
Iteration 109/1000 | Loss: 0.00004894
Iteration 110/1000 | Loss: 0.00004678
Iteration 111/1000 | Loss: 0.00004567
Iteration 112/1000 | Loss: 0.00004518
Iteration 113/1000 | Loss: 0.00004469
Iteration 114/1000 | Loss: 0.00004427
Iteration 115/1000 | Loss: 0.00004396
Iteration 116/1000 | Loss: 0.00004395
Iteration 117/1000 | Loss: 0.00004375
Iteration 118/1000 | Loss: 0.00004354
Iteration 119/1000 | Loss: 0.00004335
Iteration 120/1000 | Loss: 0.00004328
Iteration 121/1000 | Loss: 0.00004324
Iteration 122/1000 | Loss: 0.00004323
Iteration 123/1000 | Loss: 0.00004323
Iteration 124/1000 | Loss: 0.00004322
Iteration 125/1000 | Loss: 0.00004322
Iteration 126/1000 | Loss: 0.00004321
Iteration 127/1000 | Loss: 0.00004321
Iteration 128/1000 | Loss: 0.00004320
Iteration 129/1000 | Loss: 0.00004320
Iteration 130/1000 | Loss: 0.00004319
Iteration 131/1000 | Loss: 0.00004319
Iteration 132/1000 | Loss: 0.00004318
Iteration 133/1000 | Loss: 0.00004315
Iteration 134/1000 | Loss: 0.00004313
Iteration 135/1000 | Loss: 0.00004311
Iteration 136/1000 | Loss: 0.00004311
Iteration 137/1000 | Loss: 0.00004311
Iteration 138/1000 | Loss: 0.00004310
Iteration 139/1000 | Loss: 0.00004308
Iteration 140/1000 | Loss: 0.00004307
Iteration 141/1000 | Loss: 0.00004306
Iteration 142/1000 | Loss: 0.00004304
Iteration 143/1000 | Loss: 0.00004303
Iteration 144/1000 | Loss: 0.00004302
Iteration 145/1000 | Loss: 0.00004302
Iteration 146/1000 | Loss: 0.00004302
Iteration 147/1000 | Loss: 0.00004301
Iteration 148/1000 | Loss: 0.00004289
Iteration 149/1000 | Loss: 0.00004282
Iteration 150/1000 | Loss: 0.00004280
Iteration 151/1000 | Loss: 0.00004277
Iteration 152/1000 | Loss: 0.00004275
Iteration 153/1000 | Loss: 0.00004267
Iteration 154/1000 | Loss: 0.00004260
Iteration 155/1000 | Loss: 0.00004260
Iteration 156/1000 | Loss: 0.00004260
Iteration 157/1000 | Loss: 0.00004260
Iteration 158/1000 | Loss: 0.00004260
Iteration 159/1000 | Loss: 0.00004255
Iteration 160/1000 | Loss: 0.00004245
Iteration 161/1000 | Loss: 0.00004244
Iteration 162/1000 | Loss: 0.00004236
Iteration 163/1000 | Loss: 0.00004236
Iteration 164/1000 | Loss: 0.00004235
Iteration 165/1000 | Loss: 0.00004235
Iteration 166/1000 | Loss: 0.00004235
Iteration 167/1000 | Loss: 0.00004233
Iteration 168/1000 | Loss: 0.00004233
Iteration 169/1000 | Loss: 0.00004233
Iteration 170/1000 | Loss: 0.00004231
Iteration 171/1000 | Loss: 0.00004230
Iteration 172/1000 | Loss: 0.00004230
Iteration 173/1000 | Loss: 0.00004230
Iteration 174/1000 | Loss: 0.00004230
Iteration 175/1000 | Loss: 0.00004230
Iteration 176/1000 | Loss: 0.00004230
Iteration 177/1000 | Loss: 0.00004229
Iteration 178/1000 | Loss: 0.00004227
Iteration 179/1000 | Loss: 0.00004227
Iteration 180/1000 | Loss: 0.00004226
Iteration 181/1000 | Loss: 0.00004225
Iteration 182/1000 | Loss: 0.00004224
Iteration 183/1000 | Loss: 0.00004224
Iteration 184/1000 | Loss: 0.00004224
Iteration 185/1000 | Loss: 0.00004223
Iteration 186/1000 | Loss: 0.00004221
Iteration 187/1000 | Loss: 0.00004221
Iteration 188/1000 | Loss: 0.00004221
Iteration 189/1000 | Loss: 0.00004217
Iteration 190/1000 | Loss: 0.00004216
Iteration 191/1000 | Loss: 0.00004216
Iteration 192/1000 | Loss: 0.00004216
Iteration 193/1000 | Loss: 0.00004216
Iteration 194/1000 | Loss: 0.00004216
Iteration 195/1000 | Loss: 0.00004216
Iteration 196/1000 | Loss: 0.00004216
Iteration 197/1000 | Loss: 0.00004216
Iteration 198/1000 | Loss: 0.00004215
Iteration 199/1000 | Loss: 0.00004215
Iteration 200/1000 | Loss: 0.00004215
Iteration 201/1000 | Loss: 0.00004215
Iteration 202/1000 | Loss: 0.00004215
Iteration 203/1000 | Loss: 0.00004215
Iteration 204/1000 | Loss: 0.00004215
Iteration 205/1000 | Loss: 0.00004215
Iteration 206/1000 | Loss: 0.00004215
Iteration 207/1000 | Loss: 0.00004215
Iteration 208/1000 | Loss: 0.00004214
Iteration 209/1000 | Loss: 0.00004214
Iteration 210/1000 | Loss: 0.00004213
Iteration 211/1000 | Loss: 0.00004213
Iteration 212/1000 | Loss: 0.00004213
Iteration 213/1000 | Loss: 0.00004213
Iteration 214/1000 | Loss: 0.00004213
Iteration 215/1000 | Loss: 0.00004213
Iteration 216/1000 | Loss: 0.00004212
Iteration 217/1000 | Loss: 0.00004212
Iteration 218/1000 | Loss: 0.00004212
Iteration 219/1000 | Loss: 0.00004211
Iteration 220/1000 | Loss: 0.00004211
Iteration 221/1000 | Loss: 0.00004211
Iteration 222/1000 | Loss: 0.00004211
Iteration 223/1000 | Loss: 0.00004210
Iteration 224/1000 | Loss: 0.00004210
Iteration 225/1000 | Loss: 0.00004209
Iteration 226/1000 | Loss: 0.00004209
Iteration 227/1000 | Loss: 0.00004208
Iteration 228/1000 | Loss: 0.00004207
Iteration 229/1000 | Loss: 0.00004205
Iteration 230/1000 | Loss: 0.00004204
Iteration 231/1000 | Loss: 0.00004204
Iteration 232/1000 | Loss: 0.00004204
Iteration 233/1000 | Loss: 0.00004204
Iteration 234/1000 | Loss: 0.00004204
Iteration 235/1000 | Loss: 0.00004204
Iteration 236/1000 | Loss: 0.00004203
Iteration 237/1000 | Loss: 0.00004200
Iteration 238/1000 | Loss: 0.00004200
Iteration 239/1000 | Loss: 0.00004200
Iteration 240/1000 | Loss: 0.00004199
Iteration 241/1000 | Loss: 0.00004199
Iteration 242/1000 | Loss: 0.00004199
Iteration 243/1000 | Loss: 0.00004198
Iteration 244/1000 | Loss: 0.00004198
Iteration 245/1000 | Loss: 0.00004198
Iteration 246/1000 | Loss: 0.00004198
Iteration 247/1000 | Loss: 0.00004197
Iteration 248/1000 | Loss: 0.00004197
Iteration 249/1000 | Loss: 0.00004197
Iteration 250/1000 | Loss: 0.00004197
Iteration 251/1000 | Loss: 0.00004197
Iteration 252/1000 | Loss: 0.00004197
Iteration 253/1000 | Loss: 0.00004196
Iteration 254/1000 | Loss: 0.00004196
Iteration 255/1000 | Loss: 0.00004196
Iteration 256/1000 | Loss: 0.00004195
Iteration 257/1000 | Loss: 0.00004195
Iteration 258/1000 | Loss: 0.00004195
Iteration 259/1000 | Loss: 0.00004195
Iteration 260/1000 | Loss: 0.00004195
Iteration 261/1000 | Loss: 0.00004195
Iteration 262/1000 | Loss: 0.00004194
Iteration 263/1000 | Loss: 0.00004194
Iteration 264/1000 | Loss: 0.00004194
Iteration 265/1000 | Loss: 0.00004194
Iteration 266/1000 | Loss: 0.00004194
Iteration 267/1000 | Loss: 0.00004193
Iteration 268/1000 | Loss: 0.00004193
Iteration 269/1000 | Loss: 0.00004193
Iteration 270/1000 | Loss: 0.00004193
Iteration 271/1000 | Loss: 0.00004193
Iteration 272/1000 | Loss: 0.00004193
Iteration 273/1000 | Loss: 0.00004193
Iteration 274/1000 | Loss: 0.00004193
Iteration 275/1000 | Loss: 0.00004193
Iteration 276/1000 | Loss: 0.00004192
Iteration 277/1000 | Loss: 0.00004192
Iteration 278/1000 | Loss: 0.00004192
Iteration 279/1000 | Loss: 0.00004192
Iteration 280/1000 | Loss: 0.00004192
Iteration 281/1000 | Loss: 0.00004192
Iteration 282/1000 | Loss: 0.00004192
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 282. Stopping optimization.
Last 5 losses: [4.192480264464393e-05, 4.192480264464393e-05, 4.192480264464393e-05, 4.192480264464393e-05, 4.192480264464393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.192480264464393e-05

Optimization complete. Final v2v error: 3.9878580570220947 mm

Highest mean error: 11.59266185760498 mm for frame 37

Lowest mean error: 3.0830070972442627 mm for frame 0

Saving results

Total time: 229.5215425491333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00592216
Iteration 2/25 | Loss: 0.00142054
Iteration 3/25 | Loss: 0.00136352
Iteration 4/25 | Loss: 0.00135338
Iteration 5/25 | Loss: 0.00135035
Iteration 6/25 | Loss: 0.00134996
Iteration 7/25 | Loss: 0.00134996
Iteration 8/25 | Loss: 0.00134996
Iteration 9/25 | Loss: 0.00134996
Iteration 10/25 | Loss: 0.00134996
Iteration 11/25 | Loss: 0.00134996
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013499590568244457, 0.0013499590568244457, 0.0013499590568244457, 0.0013499590568244457, 0.0013499590568244457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013499590568244457

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.85334444
Iteration 2/25 | Loss: 0.00476810
Iteration 3/25 | Loss: 0.00476810
Iteration 4/25 | Loss: 0.00476810
Iteration 5/25 | Loss: 0.00476810
Iteration 6/25 | Loss: 0.00476810
Iteration 7/25 | Loss: 0.00476810
Iteration 8/25 | Loss: 0.00476810
Iteration 9/25 | Loss: 0.00476810
Iteration 10/25 | Loss: 0.00476810
Iteration 11/25 | Loss: 0.00476810
Iteration 12/25 | Loss: 0.00476810
Iteration 13/25 | Loss: 0.00476810
Iteration 14/25 | Loss: 0.00476810
Iteration 15/25 | Loss: 0.00476810
Iteration 16/25 | Loss: 0.00476810
Iteration 17/25 | Loss: 0.00476810
Iteration 18/25 | Loss: 0.00476810
Iteration 19/25 | Loss: 0.00476810
Iteration 20/25 | Loss: 0.00476810
Iteration 21/25 | Loss: 0.00476810
Iteration 22/25 | Loss: 0.00476810
Iteration 23/25 | Loss: 0.00476810
Iteration 24/25 | Loss: 0.00476810
Iteration 25/25 | Loss: 0.00476810

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00476810
Iteration 2/1000 | Loss: 0.00003417
Iteration 3/1000 | Loss: 0.00002177
Iteration 4/1000 | Loss: 0.00001865
Iteration 5/1000 | Loss: 0.00001739
Iteration 6/1000 | Loss: 0.00001626
Iteration 7/1000 | Loss: 0.00001521
Iteration 8/1000 | Loss: 0.00001478
Iteration 9/1000 | Loss: 0.00001435
Iteration 10/1000 | Loss: 0.00001393
Iteration 11/1000 | Loss: 0.00001362
Iteration 12/1000 | Loss: 0.00001333
Iteration 13/1000 | Loss: 0.00001302
Iteration 14/1000 | Loss: 0.00001284
Iteration 15/1000 | Loss: 0.00001278
Iteration 16/1000 | Loss: 0.00001278
Iteration 17/1000 | Loss: 0.00001270
Iteration 18/1000 | Loss: 0.00001262
Iteration 19/1000 | Loss: 0.00001252
Iteration 20/1000 | Loss: 0.00001252
Iteration 21/1000 | Loss: 0.00001244
Iteration 22/1000 | Loss: 0.00001244
Iteration 23/1000 | Loss: 0.00001243
Iteration 24/1000 | Loss: 0.00001241
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001240
Iteration 27/1000 | Loss: 0.00001240
Iteration 28/1000 | Loss: 0.00001240
Iteration 29/1000 | Loss: 0.00001239
Iteration 30/1000 | Loss: 0.00001239
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001238
Iteration 33/1000 | Loss: 0.00001238
Iteration 34/1000 | Loss: 0.00001236
Iteration 35/1000 | Loss: 0.00001236
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001233
Iteration 38/1000 | Loss: 0.00001230
Iteration 39/1000 | Loss: 0.00001229
Iteration 40/1000 | Loss: 0.00001228
Iteration 41/1000 | Loss: 0.00001227
Iteration 42/1000 | Loss: 0.00001227
Iteration 43/1000 | Loss: 0.00001227
Iteration 44/1000 | Loss: 0.00001227
Iteration 45/1000 | Loss: 0.00001226
Iteration 46/1000 | Loss: 0.00001226
Iteration 47/1000 | Loss: 0.00001226
Iteration 48/1000 | Loss: 0.00001226
Iteration 49/1000 | Loss: 0.00001225
Iteration 50/1000 | Loss: 0.00001225
Iteration 51/1000 | Loss: 0.00001225
Iteration 52/1000 | Loss: 0.00001225
Iteration 53/1000 | Loss: 0.00001225
Iteration 54/1000 | Loss: 0.00001225
Iteration 55/1000 | Loss: 0.00001224
Iteration 56/1000 | Loss: 0.00001224
Iteration 57/1000 | Loss: 0.00001223
Iteration 58/1000 | Loss: 0.00001223
Iteration 59/1000 | Loss: 0.00001223
Iteration 60/1000 | Loss: 0.00001223
Iteration 61/1000 | Loss: 0.00001222
Iteration 62/1000 | Loss: 0.00001222
Iteration 63/1000 | Loss: 0.00001222
Iteration 64/1000 | Loss: 0.00001222
Iteration 65/1000 | Loss: 0.00001222
Iteration 66/1000 | Loss: 0.00001222
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001219
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001218
Iteration 78/1000 | Loss: 0.00001218
Iteration 79/1000 | Loss: 0.00001217
Iteration 80/1000 | Loss: 0.00001217
Iteration 81/1000 | Loss: 0.00001217
Iteration 82/1000 | Loss: 0.00001217
Iteration 83/1000 | Loss: 0.00001217
Iteration 84/1000 | Loss: 0.00001217
Iteration 85/1000 | Loss: 0.00001217
Iteration 86/1000 | Loss: 0.00001217
Iteration 87/1000 | Loss: 0.00001216
Iteration 88/1000 | Loss: 0.00001216
Iteration 89/1000 | Loss: 0.00001216
Iteration 90/1000 | Loss: 0.00001216
Iteration 91/1000 | Loss: 0.00001216
Iteration 92/1000 | Loss: 0.00001216
Iteration 93/1000 | Loss: 0.00001216
Iteration 94/1000 | Loss: 0.00001216
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001215
Iteration 98/1000 | Loss: 0.00001215
Iteration 99/1000 | Loss: 0.00001214
Iteration 100/1000 | Loss: 0.00001214
Iteration 101/1000 | Loss: 0.00001214
Iteration 102/1000 | Loss: 0.00001214
Iteration 103/1000 | Loss: 0.00001214
Iteration 104/1000 | Loss: 0.00001213
Iteration 105/1000 | Loss: 0.00001213
Iteration 106/1000 | Loss: 0.00001213
Iteration 107/1000 | Loss: 0.00001212
Iteration 108/1000 | Loss: 0.00001212
Iteration 109/1000 | Loss: 0.00001212
Iteration 110/1000 | Loss: 0.00001212
Iteration 111/1000 | Loss: 0.00001212
Iteration 112/1000 | Loss: 0.00001212
Iteration 113/1000 | Loss: 0.00001212
Iteration 114/1000 | Loss: 0.00001212
Iteration 115/1000 | Loss: 0.00001212
Iteration 116/1000 | Loss: 0.00001212
Iteration 117/1000 | Loss: 0.00001212
Iteration 118/1000 | Loss: 0.00001210
Iteration 119/1000 | Loss: 0.00001210
Iteration 120/1000 | Loss: 0.00001210
Iteration 121/1000 | Loss: 0.00001210
Iteration 122/1000 | Loss: 0.00001210
Iteration 123/1000 | Loss: 0.00001210
Iteration 124/1000 | Loss: 0.00001209
Iteration 125/1000 | Loss: 0.00001209
Iteration 126/1000 | Loss: 0.00001209
Iteration 127/1000 | Loss: 0.00001209
Iteration 128/1000 | Loss: 0.00001208
Iteration 129/1000 | Loss: 0.00001208
Iteration 130/1000 | Loss: 0.00001208
Iteration 131/1000 | Loss: 0.00001207
Iteration 132/1000 | Loss: 0.00001207
Iteration 133/1000 | Loss: 0.00001207
Iteration 134/1000 | Loss: 0.00001207
Iteration 135/1000 | Loss: 0.00001207
Iteration 136/1000 | Loss: 0.00001207
Iteration 137/1000 | Loss: 0.00001207
Iteration 138/1000 | Loss: 0.00001207
Iteration 139/1000 | Loss: 0.00001207
Iteration 140/1000 | Loss: 0.00001207
Iteration 141/1000 | Loss: 0.00001207
Iteration 142/1000 | Loss: 0.00001207
Iteration 143/1000 | Loss: 0.00001206
Iteration 144/1000 | Loss: 0.00001206
Iteration 145/1000 | Loss: 0.00001206
Iteration 146/1000 | Loss: 0.00001206
Iteration 147/1000 | Loss: 0.00001206
Iteration 148/1000 | Loss: 0.00001206
Iteration 149/1000 | Loss: 0.00001206
Iteration 150/1000 | Loss: 0.00001206
Iteration 151/1000 | Loss: 0.00001206
Iteration 152/1000 | Loss: 0.00001206
Iteration 153/1000 | Loss: 0.00001206
Iteration 154/1000 | Loss: 0.00001206
Iteration 155/1000 | Loss: 0.00001206
Iteration 156/1000 | Loss: 0.00001206
Iteration 157/1000 | Loss: 0.00001206
Iteration 158/1000 | Loss: 0.00001206
Iteration 159/1000 | Loss: 0.00001206
Iteration 160/1000 | Loss: 0.00001206
Iteration 161/1000 | Loss: 0.00001206
Iteration 162/1000 | Loss: 0.00001206
Iteration 163/1000 | Loss: 0.00001206
Iteration 164/1000 | Loss: 0.00001206
Iteration 165/1000 | Loss: 0.00001206
Iteration 166/1000 | Loss: 0.00001206
Iteration 167/1000 | Loss: 0.00001206
Iteration 168/1000 | Loss: 0.00001206
Iteration 169/1000 | Loss: 0.00001206
Iteration 170/1000 | Loss: 0.00001206
Iteration 171/1000 | Loss: 0.00001206
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.2064058864780236e-05, 1.2064058864780236e-05, 1.2064058864780236e-05, 1.2064058864780236e-05, 1.2064058864780236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2064058864780236e-05

Optimization complete. Final v2v error: 3.1195240020751953 mm

Highest mean error: 3.2886803150177 mm for frame 43

Lowest mean error: 2.8893938064575195 mm for frame 0

Saving results

Total time: 40.93061876296997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011101
Iteration 2/25 | Loss: 0.00845073
Iteration 3/25 | Loss: 0.00244114
Iteration 4/25 | Loss: 0.00224914
Iteration 5/25 | Loss: 0.00216912
Iteration 6/25 | Loss: 0.00215030
Iteration 7/25 | Loss: 0.00213441
Iteration 8/25 | Loss: 0.00212802
Iteration 9/25 | Loss: 0.00212556
Iteration 10/25 | Loss: 0.00212440
Iteration 11/25 | Loss: 0.00212404
Iteration 12/25 | Loss: 0.00212394
Iteration 13/25 | Loss: 0.00212394
Iteration 14/25 | Loss: 0.00212394
Iteration 15/25 | Loss: 0.00212394
Iteration 16/25 | Loss: 0.00212394
Iteration 17/25 | Loss: 0.00212394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002123943530023098, 0.002123943530023098, 0.002123943530023098, 0.002123943530023098, 0.002123943530023098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002123943530023098

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.07758451
Iteration 2/25 | Loss: 0.00852231
Iteration 3/25 | Loss: 0.00852231
Iteration 4/25 | Loss: 0.00852231
Iteration 5/25 | Loss: 0.00852231
Iteration 6/25 | Loss: 0.00852231
Iteration 7/25 | Loss: 0.00852231
Iteration 8/25 | Loss: 0.00852231
Iteration 9/25 | Loss: 0.00852231
Iteration 10/25 | Loss: 0.00852231
Iteration 11/25 | Loss: 0.00852231
Iteration 12/25 | Loss: 0.00852231
Iteration 13/25 | Loss: 0.00852230
Iteration 14/25 | Loss: 0.00852230
Iteration 15/25 | Loss: 0.00852230
Iteration 16/25 | Loss: 0.00852230
Iteration 17/25 | Loss: 0.00852230
Iteration 18/25 | Loss: 0.00852230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.008522304706275463, 0.008522304706275463, 0.008522304706275463, 0.008522304706275463, 0.008522304706275463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008522304706275463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00852230
Iteration 2/1000 | Loss: 0.00072204
Iteration 3/1000 | Loss: 0.00052619
Iteration 4/1000 | Loss: 0.00045857
Iteration 5/1000 | Loss: 0.00041838
Iteration 6/1000 | Loss: 0.00038505
Iteration 7/1000 | Loss: 0.00036340
Iteration 8/1000 | Loss: 0.00034363
Iteration 9/1000 | Loss: 0.00032670
Iteration 10/1000 | Loss: 0.00741263
Iteration 11/1000 | Loss: 0.01174466
Iteration 12/1000 | Loss: 0.00044159
Iteration 13/1000 | Loss: 0.00024884
Iteration 14/1000 | Loss: 0.00016766
Iteration 15/1000 | Loss: 0.00012423
Iteration 16/1000 | Loss: 0.00008713
Iteration 17/1000 | Loss: 0.00006539
Iteration 18/1000 | Loss: 0.00004785
Iteration 19/1000 | Loss: 0.00003911
Iteration 20/1000 | Loss: 0.00003426
Iteration 21/1000 | Loss: 0.00003102
Iteration 22/1000 | Loss: 0.00002773
Iteration 23/1000 | Loss: 0.00002475
Iteration 24/1000 | Loss: 0.00002277
Iteration 25/1000 | Loss: 0.00002118
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001906
Iteration 28/1000 | Loss: 0.00001834
Iteration 29/1000 | Loss: 0.00001784
Iteration 30/1000 | Loss: 0.00001743
Iteration 31/1000 | Loss: 0.00001740
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001707
Iteration 34/1000 | Loss: 0.00001692
Iteration 35/1000 | Loss: 0.00001682
Iteration 36/1000 | Loss: 0.00001678
Iteration 37/1000 | Loss: 0.00001677
Iteration 38/1000 | Loss: 0.00001676
Iteration 39/1000 | Loss: 0.00001675
Iteration 40/1000 | Loss: 0.00001675
Iteration 41/1000 | Loss: 0.00001672
Iteration 42/1000 | Loss: 0.00001672
Iteration 43/1000 | Loss: 0.00001672
Iteration 44/1000 | Loss: 0.00001672
Iteration 45/1000 | Loss: 0.00001672
Iteration 46/1000 | Loss: 0.00001672
Iteration 47/1000 | Loss: 0.00001671
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001670
Iteration 52/1000 | Loss: 0.00001670
Iteration 53/1000 | Loss: 0.00001669
Iteration 54/1000 | Loss: 0.00001669
Iteration 55/1000 | Loss: 0.00001668
Iteration 56/1000 | Loss: 0.00001668
Iteration 57/1000 | Loss: 0.00001668
Iteration 58/1000 | Loss: 0.00001668
Iteration 59/1000 | Loss: 0.00001668
Iteration 60/1000 | Loss: 0.00001668
Iteration 61/1000 | Loss: 0.00001668
Iteration 62/1000 | Loss: 0.00001667
Iteration 63/1000 | Loss: 0.00001667
Iteration 64/1000 | Loss: 0.00001667
Iteration 65/1000 | Loss: 0.00001667
Iteration 66/1000 | Loss: 0.00001667
Iteration 67/1000 | Loss: 0.00001667
Iteration 68/1000 | Loss: 0.00001667
Iteration 69/1000 | Loss: 0.00001667
Iteration 70/1000 | Loss: 0.00001667
Iteration 71/1000 | Loss: 0.00001667
Iteration 72/1000 | Loss: 0.00001667
Iteration 73/1000 | Loss: 0.00001666
Iteration 74/1000 | Loss: 0.00001666
Iteration 75/1000 | Loss: 0.00001666
Iteration 76/1000 | Loss: 0.00001665
Iteration 77/1000 | Loss: 0.00001665
Iteration 78/1000 | Loss: 0.00001665
Iteration 79/1000 | Loss: 0.00001665
Iteration 80/1000 | Loss: 0.00001664
Iteration 81/1000 | Loss: 0.00001664
Iteration 82/1000 | Loss: 0.00001664
Iteration 83/1000 | Loss: 0.00001664
Iteration 84/1000 | Loss: 0.00001664
Iteration 85/1000 | Loss: 0.00001664
Iteration 86/1000 | Loss: 0.00001663
Iteration 87/1000 | Loss: 0.00001663
Iteration 88/1000 | Loss: 0.00001663
Iteration 89/1000 | Loss: 0.00001662
Iteration 90/1000 | Loss: 0.00001662
Iteration 91/1000 | Loss: 0.00001662
Iteration 92/1000 | Loss: 0.00001662
Iteration 93/1000 | Loss: 0.00001661
Iteration 94/1000 | Loss: 0.00001661
Iteration 95/1000 | Loss: 0.00001661
Iteration 96/1000 | Loss: 0.00001661
Iteration 97/1000 | Loss: 0.00001661
Iteration 98/1000 | Loss: 0.00001661
Iteration 99/1000 | Loss: 0.00001661
Iteration 100/1000 | Loss: 0.00001661
Iteration 101/1000 | Loss: 0.00001661
Iteration 102/1000 | Loss: 0.00001661
Iteration 103/1000 | Loss: 0.00001661
Iteration 104/1000 | Loss: 0.00001660
Iteration 105/1000 | Loss: 0.00001660
Iteration 106/1000 | Loss: 0.00001660
Iteration 107/1000 | Loss: 0.00001660
Iteration 108/1000 | Loss: 0.00001660
Iteration 109/1000 | Loss: 0.00001660
Iteration 110/1000 | Loss: 0.00001660
Iteration 111/1000 | Loss: 0.00001660
Iteration 112/1000 | Loss: 0.00001660
Iteration 113/1000 | Loss: 0.00001660
Iteration 114/1000 | Loss: 0.00001660
Iteration 115/1000 | Loss: 0.00001660
Iteration 116/1000 | Loss: 0.00001659
Iteration 117/1000 | Loss: 0.00001659
Iteration 118/1000 | Loss: 0.00001659
Iteration 119/1000 | Loss: 0.00001659
Iteration 120/1000 | Loss: 0.00001659
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001659
Iteration 123/1000 | Loss: 0.00001659
Iteration 124/1000 | Loss: 0.00001659
Iteration 125/1000 | Loss: 0.00001658
Iteration 126/1000 | Loss: 0.00001658
Iteration 127/1000 | Loss: 0.00001658
Iteration 128/1000 | Loss: 0.00001658
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001658
Iteration 134/1000 | Loss: 0.00001658
Iteration 135/1000 | Loss: 0.00001658
Iteration 136/1000 | Loss: 0.00001658
Iteration 137/1000 | Loss: 0.00001658
Iteration 138/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.6582573152845725e-05, 1.6582573152845725e-05, 1.6582573152845725e-05, 1.6582573152845725e-05, 1.6582573152845725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6582573152845725e-05

Optimization complete. Final v2v error: 3.58174204826355 mm

Highest mean error: 3.814645290374756 mm for frame 96

Lowest mean error: 3.2219839096069336 mm for frame 2

Saving results

Total time: 69.70188069343567
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016270
Iteration 2/25 | Loss: 0.00241788
Iteration 3/25 | Loss: 0.00309731
Iteration 4/25 | Loss: 0.00248463
Iteration 5/25 | Loss: 0.00188051
Iteration 6/25 | Loss: 0.00173739
Iteration 7/25 | Loss: 0.00158576
Iteration 8/25 | Loss: 0.00160845
Iteration 9/25 | Loss: 0.00154527
Iteration 10/25 | Loss: 0.00149707
Iteration 11/25 | Loss: 0.00143744
Iteration 12/25 | Loss: 0.00143183
Iteration 13/25 | Loss: 0.00145147
Iteration 14/25 | Loss: 0.00142790
Iteration 15/25 | Loss: 0.00141963
Iteration 16/25 | Loss: 0.00141791
Iteration 17/25 | Loss: 0.00141700
Iteration 18/25 | Loss: 0.00141664
Iteration 19/25 | Loss: 0.00141639
Iteration 20/25 | Loss: 0.00144031
Iteration 21/25 | Loss: 0.00141189
Iteration 22/25 | Loss: 0.00141071
Iteration 23/25 | Loss: 0.00141055
Iteration 24/25 | Loss: 0.00141054
Iteration 25/25 | Loss: 0.00141054

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16280174
Iteration 2/25 | Loss: 0.00454830
Iteration 3/25 | Loss: 0.00454830
Iteration 4/25 | Loss: 0.00454830
Iteration 5/25 | Loss: 0.00454830
Iteration 6/25 | Loss: 0.00454829
Iteration 7/25 | Loss: 0.00454829
Iteration 8/25 | Loss: 0.00454829
Iteration 9/25 | Loss: 0.00454829
Iteration 10/25 | Loss: 0.00454829
Iteration 11/25 | Loss: 0.00454829
Iteration 12/25 | Loss: 0.00454829
Iteration 13/25 | Loss: 0.00454829
Iteration 14/25 | Loss: 0.00454829
Iteration 15/25 | Loss: 0.00454829
Iteration 16/25 | Loss: 0.00454829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004548293072730303, 0.004548293072730303, 0.004548293072730303, 0.004548293072730303, 0.004548293072730303]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004548293072730303

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00454829
Iteration 2/1000 | Loss: 0.00005451
Iteration 3/1000 | Loss: 0.00044824
Iteration 4/1000 | Loss: 0.00054624
Iteration 5/1000 | Loss: 0.00002900
Iteration 6/1000 | Loss: 0.00002715
Iteration 7/1000 | Loss: 0.00002553
Iteration 8/1000 | Loss: 0.00002440
Iteration 9/1000 | Loss: 0.00002368
Iteration 10/1000 | Loss: 0.00002286
Iteration 11/1000 | Loss: 0.00002247
Iteration 12/1000 | Loss: 0.00002205
Iteration 13/1000 | Loss: 0.00002168
Iteration 14/1000 | Loss: 0.00002141
Iteration 15/1000 | Loss: 0.00002122
Iteration 16/1000 | Loss: 0.00002114
Iteration 17/1000 | Loss: 0.00002110
Iteration 18/1000 | Loss: 0.00002105
Iteration 19/1000 | Loss: 0.00002104
Iteration 20/1000 | Loss: 0.00002098
Iteration 21/1000 | Loss: 0.00002094
Iteration 22/1000 | Loss: 0.00002093
Iteration 23/1000 | Loss: 0.00002093
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00002092
Iteration 26/1000 | Loss: 0.00002086
Iteration 27/1000 | Loss: 0.00002083
Iteration 28/1000 | Loss: 0.00002083
Iteration 29/1000 | Loss: 0.00002083
Iteration 30/1000 | Loss: 0.00002082
Iteration 31/1000 | Loss: 0.00002082
Iteration 32/1000 | Loss: 0.00002081
Iteration 33/1000 | Loss: 0.00002081
Iteration 34/1000 | Loss: 0.00002080
Iteration 35/1000 | Loss: 0.00002079
Iteration 36/1000 | Loss: 0.00002078
Iteration 37/1000 | Loss: 0.00002078
Iteration 38/1000 | Loss: 0.00002078
Iteration 39/1000 | Loss: 0.00002077
Iteration 40/1000 | Loss: 0.00002077
Iteration 41/1000 | Loss: 0.00002076
Iteration 42/1000 | Loss: 0.00002076
Iteration 43/1000 | Loss: 0.00002075
Iteration 44/1000 | Loss: 0.00002075
Iteration 45/1000 | Loss: 0.00002074
Iteration 46/1000 | Loss: 0.00002074
Iteration 47/1000 | Loss: 0.00002074
Iteration 48/1000 | Loss: 0.00002074
Iteration 49/1000 | Loss: 0.00002074
Iteration 50/1000 | Loss: 0.00002074
Iteration 51/1000 | Loss: 0.00002073
Iteration 52/1000 | Loss: 0.00002073
Iteration 53/1000 | Loss: 0.00002073
Iteration 54/1000 | Loss: 0.00002073
Iteration 55/1000 | Loss: 0.00002073
Iteration 56/1000 | Loss: 0.00002073
Iteration 57/1000 | Loss: 0.00002073
Iteration 58/1000 | Loss: 0.00002073
Iteration 59/1000 | Loss: 0.00002073
Iteration 60/1000 | Loss: 0.00002073
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002073
Iteration 63/1000 | Loss: 0.00002073
Iteration 64/1000 | Loss: 0.00002073
Iteration 65/1000 | Loss: 0.00002073
Iteration 66/1000 | Loss: 0.00002073
Iteration 67/1000 | Loss: 0.00002073
Iteration 68/1000 | Loss: 0.00002073
Iteration 69/1000 | Loss: 0.00002073
Iteration 70/1000 | Loss: 0.00002073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 70. Stopping optimization.
Last 5 losses: [2.0731407857965678e-05, 2.0731407857965678e-05, 2.0731407857965678e-05, 2.0731407857965678e-05, 2.0731407857965678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0731407857965678e-05

Optimization complete. Final v2v error: 3.9057846069335938 mm

Highest mean error: 5.169052600860596 mm for frame 108

Lowest mean error: 3.3649754524230957 mm for frame 89

Saving results

Total time: 75.79809546470642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799311
Iteration 2/25 | Loss: 0.00144210
Iteration 3/25 | Loss: 0.00136937
Iteration 4/25 | Loss: 0.00136271
Iteration 5/25 | Loss: 0.00136172
Iteration 6/25 | Loss: 0.00136161
Iteration 7/25 | Loss: 0.00136161
Iteration 8/25 | Loss: 0.00136161
Iteration 9/25 | Loss: 0.00136161
Iteration 10/25 | Loss: 0.00136161
Iteration 11/25 | Loss: 0.00136161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013616124633699656, 0.0013616124633699656, 0.0013616124633699656, 0.0013616124633699656, 0.0013616124633699656]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013616124633699656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17552328
Iteration 2/25 | Loss: 0.00504396
Iteration 3/25 | Loss: 0.00504396
Iteration 4/25 | Loss: 0.00504395
Iteration 5/25 | Loss: 0.00504395
Iteration 6/25 | Loss: 0.00504395
Iteration 7/25 | Loss: 0.00504395
Iteration 8/25 | Loss: 0.00504395
Iteration 9/25 | Loss: 0.00504395
Iteration 10/25 | Loss: 0.00504395
Iteration 11/25 | Loss: 0.00504395
Iteration 12/25 | Loss: 0.00504395
Iteration 13/25 | Loss: 0.00504395
Iteration 14/25 | Loss: 0.00504395
Iteration 15/25 | Loss: 0.00504395
Iteration 16/25 | Loss: 0.00504395
Iteration 17/25 | Loss: 0.00504395
Iteration 18/25 | Loss: 0.00504395
Iteration 19/25 | Loss: 0.00504395
Iteration 20/25 | Loss: 0.00504395
Iteration 21/25 | Loss: 0.00504395
Iteration 22/25 | Loss: 0.00504395
Iteration 23/25 | Loss: 0.00504395
Iteration 24/25 | Loss: 0.00504395
Iteration 25/25 | Loss: 0.00504395

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00504395
Iteration 2/1000 | Loss: 0.00004201
Iteration 3/1000 | Loss: 0.00002576
Iteration 4/1000 | Loss: 0.00002236
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00001971
Iteration 7/1000 | Loss: 0.00001891
Iteration 8/1000 | Loss: 0.00001811
Iteration 9/1000 | Loss: 0.00001764
Iteration 10/1000 | Loss: 0.00001721
Iteration 11/1000 | Loss: 0.00001682
Iteration 12/1000 | Loss: 0.00001655
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001611
Iteration 15/1000 | Loss: 0.00001604
Iteration 16/1000 | Loss: 0.00001586
Iteration 17/1000 | Loss: 0.00001580
Iteration 18/1000 | Loss: 0.00001572
Iteration 19/1000 | Loss: 0.00001569
Iteration 20/1000 | Loss: 0.00001568
Iteration 21/1000 | Loss: 0.00001567
Iteration 22/1000 | Loss: 0.00001566
Iteration 23/1000 | Loss: 0.00001566
Iteration 24/1000 | Loss: 0.00001564
Iteration 25/1000 | Loss: 0.00001563
Iteration 26/1000 | Loss: 0.00001563
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001556
Iteration 29/1000 | Loss: 0.00001548
Iteration 30/1000 | Loss: 0.00001546
Iteration 31/1000 | Loss: 0.00001545
Iteration 32/1000 | Loss: 0.00001543
Iteration 33/1000 | Loss: 0.00001542
Iteration 34/1000 | Loss: 0.00001541
Iteration 35/1000 | Loss: 0.00001540
Iteration 36/1000 | Loss: 0.00001538
Iteration 37/1000 | Loss: 0.00001532
Iteration 38/1000 | Loss: 0.00001527
Iteration 39/1000 | Loss: 0.00001522
Iteration 40/1000 | Loss: 0.00001514
Iteration 41/1000 | Loss: 0.00001510
Iteration 42/1000 | Loss: 0.00001510
Iteration 43/1000 | Loss: 0.00001509
Iteration 44/1000 | Loss: 0.00001508
Iteration 45/1000 | Loss: 0.00001507
Iteration 46/1000 | Loss: 0.00001506
Iteration 47/1000 | Loss: 0.00001505
Iteration 48/1000 | Loss: 0.00001502
Iteration 49/1000 | Loss: 0.00001502
Iteration 50/1000 | Loss: 0.00001500
Iteration 51/1000 | Loss: 0.00001500
Iteration 52/1000 | Loss: 0.00001500
Iteration 53/1000 | Loss: 0.00001500
Iteration 54/1000 | Loss: 0.00001500
Iteration 55/1000 | Loss: 0.00001500
Iteration 56/1000 | Loss: 0.00001499
Iteration 57/1000 | Loss: 0.00001499
Iteration 58/1000 | Loss: 0.00001499
Iteration 59/1000 | Loss: 0.00001496
Iteration 60/1000 | Loss: 0.00001496
Iteration 61/1000 | Loss: 0.00001496
Iteration 62/1000 | Loss: 0.00001496
Iteration 63/1000 | Loss: 0.00001496
Iteration 64/1000 | Loss: 0.00001496
Iteration 65/1000 | Loss: 0.00001496
Iteration 66/1000 | Loss: 0.00001495
Iteration 67/1000 | Loss: 0.00001495
Iteration 68/1000 | Loss: 0.00001495
Iteration 69/1000 | Loss: 0.00001495
Iteration 70/1000 | Loss: 0.00001495
Iteration 71/1000 | Loss: 0.00001495
Iteration 72/1000 | Loss: 0.00001495
Iteration 73/1000 | Loss: 0.00001494
Iteration 74/1000 | Loss: 0.00001494
Iteration 75/1000 | Loss: 0.00001494
Iteration 76/1000 | Loss: 0.00001493
Iteration 77/1000 | Loss: 0.00001493
Iteration 78/1000 | Loss: 0.00001493
Iteration 79/1000 | Loss: 0.00001493
Iteration 80/1000 | Loss: 0.00001492
Iteration 81/1000 | Loss: 0.00001492
Iteration 82/1000 | Loss: 0.00001492
Iteration 83/1000 | Loss: 0.00001492
Iteration 84/1000 | Loss: 0.00001491
Iteration 85/1000 | Loss: 0.00001491
Iteration 86/1000 | Loss: 0.00001491
Iteration 87/1000 | Loss: 0.00001491
Iteration 88/1000 | Loss: 0.00001491
Iteration 89/1000 | Loss: 0.00001491
Iteration 90/1000 | Loss: 0.00001491
Iteration 91/1000 | Loss: 0.00001491
Iteration 92/1000 | Loss: 0.00001491
Iteration 93/1000 | Loss: 0.00001491
Iteration 94/1000 | Loss: 0.00001491
Iteration 95/1000 | Loss: 0.00001491
Iteration 96/1000 | Loss: 0.00001491
Iteration 97/1000 | Loss: 0.00001491
Iteration 98/1000 | Loss: 0.00001491
Iteration 99/1000 | Loss: 0.00001491
Iteration 100/1000 | Loss: 0.00001491
Iteration 101/1000 | Loss: 0.00001491
Iteration 102/1000 | Loss: 0.00001491
Iteration 103/1000 | Loss: 0.00001491
Iteration 104/1000 | Loss: 0.00001491
Iteration 105/1000 | Loss: 0.00001491
Iteration 106/1000 | Loss: 0.00001491
Iteration 107/1000 | Loss: 0.00001491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.4905694115441293e-05, 1.4905694115441293e-05, 1.4905694115441293e-05, 1.4905694115441293e-05, 1.4905694115441293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4905694115441293e-05

Optimization complete. Final v2v error: 3.303237199783325 mm

Highest mean error: 3.800379991531372 mm for frame 113

Lowest mean error: 2.6858484745025635 mm for frame 2

Saving results

Total time: 41.558525800704956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403685
Iteration 2/25 | Loss: 0.00142056
Iteration 3/25 | Loss: 0.00135987
Iteration 4/25 | Loss: 0.00135139
Iteration 5/25 | Loss: 0.00134919
Iteration 6/25 | Loss: 0.00134884
Iteration 7/25 | Loss: 0.00134884
Iteration 8/25 | Loss: 0.00134884
Iteration 9/25 | Loss: 0.00134884
Iteration 10/25 | Loss: 0.00134884
Iteration 11/25 | Loss: 0.00134884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013488441472873092, 0.0013488441472873092, 0.0013488441472873092, 0.0013488441472873092, 0.0013488441472873092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013488441472873092

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09183550
Iteration 2/25 | Loss: 0.00504514
Iteration 3/25 | Loss: 0.00504512
Iteration 4/25 | Loss: 0.00504512
Iteration 5/25 | Loss: 0.00504512
Iteration 6/25 | Loss: 0.00504512
Iteration 7/25 | Loss: 0.00504512
Iteration 8/25 | Loss: 0.00504512
Iteration 9/25 | Loss: 0.00504511
Iteration 10/25 | Loss: 0.00504511
Iteration 11/25 | Loss: 0.00504511
Iteration 12/25 | Loss: 0.00504511
Iteration 13/25 | Loss: 0.00504511
Iteration 14/25 | Loss: 0.00504511
Iteration 15/25 | Loss: 0.00504511
Iteration 16/25 | Loss: 0.00504511
Iteration 17/25 | Loss: 0.00504511
Iteration 18/25 | Loss: 0.00504511
Iteration 19/25 | Loss: 0.00504511
Iteration 20/25 | Loss: 0.00504511
Iteration 21/25 | Loss: 0.00504511
Iteration 22/25 | Loss: 0.00504511
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.005045113153755665, 0.005045113153755665, 0.005045113153755665, 0.005045113153755665, 0.005045113153755665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005045113153755665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00504511
Iteration 2/1000 | Loss: 0.00004207
Iteration 3/1000 | Loss: 0.00002073
Iteration 4/1000 | Loss: 0.00001753
Iteration 5/1000 | Loss: 0.00001579
Iteration 6/1000 | Loss: 0.00001514
Iteration 7/1000 | Loss: 0.00001414
Iteration 8/1000 | Loss: 0.00001359
Iteration 9/1000 | Loss: 0.00001304
Iteration 10/1000 | Loss: 0.00001257
Iteration 11/1000 | Loss: 0.00001221
Iteration 12/1000 | Loss: 0.00001193
Iteration 13/1000 | Loss: 0.00001164
Iteration 14/1000 | Loss: 0.00001139
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001125
Iteration 17/1000 | Loss: 0.00001114
Iteration 18/1000 | Loss: 0.00001114
Iteration 19/1000 | Loss: 0.00001105
Iteration 20/1000 | Loss: 0.00001094
Iteration 21/1000 | Loss: 0.00001094
Iteration 22/1000 | Loss: 0.00001092
Iteration 23/1000 | Loss: 0.00001084
Iteration 24/1000 | Loss: 0.00001074
Iteration 25/1000 | Loss: 0.00001067
Iteration 26/1000 | Loss: 0.00001056
Iteration 27/1000 | Loss: 0.00001053
Iteration 28/1000 | Loss: 0.00001050
Iteration 29/1000 | Loss: 0.00001048
Iteration 30/1000 | Loss: 0.00001048
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001040
Iteration 33/1000 | Loss: 0.00001040
Iteration 34/1000 | Loss: 0.00001037
Iteration 35/1000 | Loss: 0.00001036
Iteration 36/1000 | Loss: 0.00001035
Iteration 37/1000 | Loss: 0.00001034
Iteration 38/1000 | Loss: 0.00001034
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001033
Iteration 41/1000 | Loss: 0.00001033
Iteration 42/1000 | Loss: 0.00001032
Iteration 43/1000 | Loss: 0.00001032
Iteration 44/1000 | Loss: 0.00001031
Iteration 45/1000 | Loss: 0.00001031
Iteration 46/1000 | Loss: 0.00001030
Iteration 47/1000 | Loss: 0.00001030
Iteration 48/1000 | Loss: 0.00001030
Iteration 49/1000 | Loss: 0.00001030
Iteration 50/1000 | Loss: 0.00001029
Iteration 51/1000 | Loss: 0.00001029
Iteration 52/1000 | Loss: 0.00001029
Iteration 53/1000 | Loss: 0.00001029
Iteration 54/1000 | Loss: 0.00001029
Iteration 55/1000 | Loss: 0.00001029
Iteration 56/1000 | Loss: 0.00001029
Iteration 57/1000 | Loss: 0.00001029
Iteration 58/1000 | Loss: 0.00001029
Iteration 59/1000 | Loss: 0.00001029
Iteration 60/1000 | Loss: 0.00001028
Iteration 61/1000 | Loss: 0.00001027
Iteration 62/1000 | Loss: 0.00001027
Iteration 63/1000 | Loss: 0.00001026
Iteration 64/1000 | Loss: 0.00001026
Iteration 65/1000 | Loss: 0.00001025
Iteration 66/1000 | Loss: 0.00001025
Iteration 67/1000 | Loss: 0.00001025
Iteration 68/1000 | Loss: 0.00001024
Iteration 69/1000 | Loss: 0.00001024
Iteration 70/1000 | Loss: 0.00001024
Iteration 71/1000 | Loss: 0.00001023
Iteration 72/1000 | Loss: 0.00001023
Iteration 73/1000 | Loss: 0.00001023
Iteration 74/1000 | Loss: 0.00001023
Iteration 75/1000 | Loss: 0.00001023
Iteration 76/1000 | Loss: 0.00001022
Iteration 77/1000 | Loss: 0.00001022
Iteration 78/1000 | Loss: 0.00001022
Iteration 79/1000 | Loss: 0.00001022
Iteration 80/1000 | Loss: 0.00001022
Iteration 81/1000 | Loss: 0.00001022
Iteration 82/1000 | Loss: 0.00001021
Iteration 83/1000 | Loss: 0.00001021
Iteration 84/1000 | Loss: 0.00001021
Iteration 85/1000 | Loss: 0.00001021
Iteration 86/1000 | Loss: 0.00001021
Iteration 87/1000 | Loss: 0.00001021
Iteration 88/1000 | Loss: 0.00001020
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001020
Iteration 92/1000 | Loss: 0.00001020
Iteration 93/1000 | Loss: 0.00001020
Iteration 94/1000 | Loss: 0.00001020
Iteration 95/1000 | Loss: 0.00001019
Iteration 96/1000 | Loss: 0.00001019
Iteration 97/1000 | Loss: 0.00001019
Iteration 98/1000 | Loss: 0.00001019
Iteration 99/1000 | Loss: 0.00001019
Iteration 100/1000 | Loss: 0.00001018
Iteration 101/1000 | Loss: 0.00001018
Iteration 102/1000 | Loss: 0.00001018
Iteration 103/1000 | Loss: 0.00001017
Iteration 104/1000 | Loss: 0.00001017
Iteration 105/1000 | Loss: 0.00001017
Iteration 106/1000 | Loss: 0.00001017
Iteration 107/1000 | Loss: 0.00001017
Iteration 108/1000 | Loss: 0.00001017
Iteration 109/1000 | Loss: 0.00001017
Iteration 110/1000 | Loss: 0.00001017
Iteration 111/1000 | Loss: 0.00001016
Iteration 112/1000 | Loss: 0.00001016
Iteration 113/1000 | Loss: 0.00001016
Iteration 114/1000 | Loss: 0.00001016
Iteration 115/1000 | Loss: 0.00001016
Iteration 116/1000 | Loss: 0.00001016
Iteration 117/1000 | Loss: 0.00001016
Iteration 118/1000 | Loss: 0.00001015
Iteration 119/1000 | Loss: 0.00001015
Iteration 120/1000 | Loss: 0.00001015
Iteration 121/1000 | Loss: 0.00001015
Iteration 122/1000 | Loss: 0.00001015
Iteration 123/1000 | Loss: 0.00001015
Iteration 124/1000 | Loss: 0.00001015
Iteration 125/1000 | Loss: 0.00001015
Iteration 126/1000 | Loss: 0.00001015
Iteration 127/1000 | Loss: 0.00001015
Iteration 128/1000 | Loss: 0.00001015
Iteration 129/1000 | Loss: 0.00001014
Iteration 130/1000 | Loss: 0.00001014
Iteration 131/1000 | Loss: 0.00001014
Iteration 132/1000 | Loss: 0.00001014
Iteration 133/1000 | Loss: 0.00001014
Iteration 134/1000 | Loss: 0.00001014
Iteration 135/1000 | Loss: 0.00001014
Iteration 136/1000 | Loss: 0.00001014
Iteration 137/1000 | Loss: 0.00001014
Iteration 138/1000 | Loss: 0.00001014
Iteration 139/1000 | Loss: 0.00001014
Iteration 140/1000 | Loss: 0.00001014
Iteration 141/1000 | Loss: 0.00001014
Iteration 142/1000 | Loss: 0.00001014
Iteration 143/1000 | Loss: 0.00001014
Iteration 144/1000 | Loss: 0.00001014
Iteration 145/1000 | Loss: 0.00001014
Iteration 146/1000 | Loss: 0.00001014
Iteration 147/1000 | Loss: 0.00001014
Iteration 148/1000 | Loss: 0.00001014
Iteration 149/1000 | Loss: 0.00001014
Iteration 150/1000 | Loss: 0.00001014
Iteration 151/1000 | Loss: 0.00001014
Iteration 152/1000 | Loss: 0.00001014
Iteration 153/1000 | Loss: 0.00001014
Iteration 154/1000 | Loss: 0.00001014
Iteration 155/1000 | Loss: 0.00001014
Iteration 156/1000 | Loss: 0.00001014
Iteration 157/1000 | Loss: 0.00001014
Iteration 158/1000 | Loss: 0.00001014
Iteration 159/1000 | Loss: 0.00001014
Iteration 160/1000 | Loss: 0.00001014
Iteration 161/1000 | Loss: 0.00001014
Iteration 162/1000 | Loss: 0.00001014
Iteration 163/1000 | Loss: 0.00001014
Iteration 164/1000 | Loss: 0.00001014
Iteration 165/1000 | Loss: 0.00001014
Iteration 166/1000 | Loss: 0.00001014
Iteration 167/1000 | Loss: 0.00001014
Iteration 168/1000 | Loss: 0.00001014
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.0144653970201034e-05, 1.0144653970201034e-05, 1.0144653970201034e-05, 1.0144653970201034e-05, 1.0144653970201034e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0144653970201034e-05

Optimization complete. Final v2v error: 2.8042285442352295 mm

Highest mean error: 3.01802659034729 mm for frame 85

Lowest mean error: 2.5656700134277344 mm for frame 48

Saving results

Total time: 44.77956509590149
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931106
Iteration 2/25 | Loss: 0.00250152
Iteration 3/25 | Loss: 0.00180854
Iteration 4/25 | Loss: 0.00167985
Iteration 5/25 | Loss: 0.00164034
Iteration 6/25 | Loss: 0.00162262
Iteration 7/25 | Loss: 0.00156472
Iteration 8/25 | Loss: 0.00157022
Iteration 9/25 | Loss: 0.00155846
Iteration 10/25 | Loss: 0.00154133
Iteration 11/25 | Loss: 0.00155341
Iteration 12/25 | Loss: 0.00154482
Iteration 13/25 | Loss: 0.00152751
Iteration 14/25 | Loss: 0.00152984
Iteration 15/25 | Loss: 0.00152241
Iteration 16/25 | Loss: 0.00152174
Iteration 17/25 | Loss: 0.00152650
Iteration 18/25 | Loss: 0.00151863
Iteration 19/25 | Loss: 0.00151701
Iteration 20/25 | Loss: 0.00151716
Iteration 21/25 | Loss: 0.00151635
Iteration 22/25 | Loss: 0.00151880
Iteration 23/25 | Loss: 0.00151738
Iteration 24/25 | Loss: 0.00151494
Iteration 25/25 | Loss: 0.00151421

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63015234
Iteration 2/25 | Loss: 0.00425046
Iteration 3/25 | Loss: 0.00425044
Iteration 4/25 | Loss: 0.00425044
Iteration 5/25 | Loss: 0.00425044
Iteration 6/25 | Loss: 0.00425043
Iteration 7/25 | Loss: 0.00425043
Iteration 8/25 | Loss: 0.00425043
Iteration 9/25 | Loss: 0.00425043
Iteration 10/25 | Loss: 0.00425043
Iteration 11/25 | Loss: 0.00425043
Iteration 12/25 | Loss: 0.00425043
Iteration 13/25 | Loss: 0.00425043
Iteration 14/25 | Loss: 0.00425043
Iteration 15/25 | Loss: 0.00425043
Iteration 16/25 | Loss: 0.00425043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004250432830303907, 0.004250432830303907, 0.004250432830303907, 0.004250432830303907, 0.004250432830303907]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004250432830303907

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00425043
Iteration 2/1000 | Loss: 0.00006174
Iteration 3/1000 | Loss: 0.00004569
Iteration 4/1000 | Loss: 0.00004178
Iteration 5/1000 | Loss: 0.00003784
Iteration 6/1000 | Loss: 0.00003542
Iteration 7/1000 | Loss: 0.00003367
Iteration 8/1000 | Loss: 0.00003274
Iteration 9/1000 | Loss: 0.00012114
Iteration 10/1000 | Loss: 0.00003304
Iteration 11/1000 | Loss: 0.00003089
Iteration 12/1000 | Loss: 0.00003054
Iteration 13/1000 | Loss: 0.00003002
Iteration 14/1000 | Loss: 0.00002943
Iteration 15/1000 | Loss: 0.00009736
Iteration 16/1000 | Loss: 0.00107964
Iteration 17/1000 | Loss: 0.00010297
Iteration 18/1000 | Loss: 0.00017712
Iteration 19/1000 | Loss: 0.00014684
Iteration 20/1000 | Loss: 0.00004947
Iteration 21/1000 | Loss: 0.00007478
Iteration 22/1000 | Loss: 0.00002935
Iteration 23/1000 | Loss: 0.00012226
Iteration 24/1000 | Loss: 0.00002811
Iteration 25/1000 | Loss: 0.00006445
Iteration 26/1000 | Loss: 0.00002705
Iteration 27/1000 | Loss: 0.00015604
Iteration 28/1000 | Loss: 0.00003339
Iteration 29/1000 | Loss: 0.00002530
Iteration 30/1000 | Loss: 0.00002466
Iteration 31/1000 | Loss: 0.00002419
Iteration 32/1000 | Loss: 0.00007530
Iteration 33/1000 | Loss: 0.00012260
Iteration 34/1000 | Loss: 0.00003759
Iteration 35/1000 | Loss: 0.00002414
Iteration 36/1000 | Loss: 0.00004366
Iteration 37/1000 | Loss: 0.00002403
Iteration 38/1000 | Loss: 0.00013402
Iteration 39/1000 | Loss: 0.00002413
Iteration 40/1000 | Loss: 0.00002380
Iteration 41/1000 | Loss: 0.00002377
Iteration 42/1000 | Loss: 0.00002374
Iteration 43/1000 | Loss: 0.00002373
Iteration 44/1000 | Loss: 0.00002371
Iteration 45/1000 | Loss: 0.00002371
Iteration 46/1000 | Loss: 0.00002371
Iteration 47/1000 | Loss: 0.00002371
Iteration 48/1000 | Loss: 0.00002371
Iteration 49/1000 | Loss: 0.00002371
Iteration 50/1000 | Loss: 0.00002371
Iteration 51/1000 | Loss: 0.00002371
Iteration 52/1000 | Loss: 0.00002371
Iteration 53/1000 | Loss: 0.00002371
Iteration 54/1000 | Loss: 0.00002371
Iteration 55/1000 | Loss: 0.00002371
Iteration 56/1000 | Loss: 0.00002371
Iteration 57/1000 | Loss: 0.00002370
Iteration 58/1000 | Loss: 0.00002370
Iteration 59/1000 | Loss: 0.00002370
Iteration 60/1000 | Loss: 0.00002370
Iteration 61/1000 | Loss: 0.00002370
Iteration 62/1000 | Loss: 0.00002370
Iteration 63/1000 | Loss: 0.00002369
Iteration 64/1000 | Loss: 0.00002369
Iteration 65/1000 | Loss: 0.00002369
Iteration 66/1000 | Loss: 0.00002369
Iteration 67/1000 | Loss: 0.00002369
Iteration 68/1000 | Loss: 0.00002369
Iteration 69/1000 | Loss: 0.00002369
Iteration 70/1000 | Loss: 0.00002368
Iteration 71/1000 | Loss: 0.00002368
Iteration 72/1000 | Loss: 0.00002368
Iteration 73/1000 | Loss: 0.00002367
Iteration 74/1000 | Loss: 0.00002367
Iteration 75/1000 | Loss: 0.00002367
Iteration 76/1000 | Loss: 0.00002367
Iteration 77/1000 | Loss: 0.00002367
Iteration 78/1000 | Loss: 0.00002367
Iteration 79/1000 | Loss: 0.00002367
Iteration 80/1000 | Loss: 0.00002367
Iteration 81/1000 | Loss: 0.00002366
Iteration 82/1000 | Loss: 0.00002366
Iteration 83/1000 | Loss: 0.00002366
Iteration 84/1000 | Loss: 0.00002366
Iteration 85/1000 | Loss: 0.00002365
Iteration 86/1000 | Loss: 0.00002365
Iteration 87/1000 | Loss: 0.00002365
Iteration 88/1000 | Loss: 0.00002365
Iteration 89/1000 | Loss: 0.00002365
Iteration 90/1000 | Loss: 0.00002364
Iteration 91/1000 | Loss: 0.00002364
Iteration 92/1000 | Loss: 0.00002364
Iteration 93/1000 | Loss: 0.00002364
Iteration 94/1000 | Loss: 0.00002364
Iteration 95/1000 | Loss: 0.00002364
Iteration 96/1000 | Loss: 0.00002364
Iteration 97/1000 | Loss: 0.00002364
Iteration 98/1000 | Loss: 0.00002363
Iteration 99/1000 | Loss: 0.00002363
Iteration 100/1000 | Loss: 0.00002363
Iteration 101/1000 | Loss: 0.00002363
Iteration 102/1000 | Loss: 0.00002362
Iteration 103/1000 | Loss: 0.00002362
Iteration 104/1000 | Loss: 0.00002362
Iteration 105/1000 | Loss: 0.00002362
Iteration 106/1000 | Loss: 0.00002362
Iteration 107/1000 | Loss: 0.00002362
Iteration 108/1000 | Loss: 0.00002362
Iteration 109/1000 | Loss: 0.00002362
Iteration 110/1000 | Loss: 0.00002362
Iteration 111/1000 | Loss: 0.00002362
Iteration 112/1000 | Loss: 0.00002361
Iteration 113/1000 | Loss: 0.00002361
Iteration 114/1000 | Loss: 0.00002361
Iteration 115/1000 | Loss: 0.00002361
Iteration 116/1000 | Loss: 0.00002361
Iteration 117/1000 | Loss: 0.00002361
Iteration 118/1000 | Loss: 0.00002361
Iteration 119/1000 | Loss: 0.00002361
Iteration 120/1000 | Loss: 0.00002361
Iteration 121/1000 | Loss: 0.00002361
Iteration 122/1000 | Loss: 0.00002361
Iteration 123/1000 | Loss: 0.00002361
Iteration 124/1000 | Loss: 0.00002361
Iteration 125/1000 | Loss: 0.00002360
Iteration 126/1000 | Loss: 0.00002360
Iteration 127/1000 | Loss: 0.00002359
Iteration 128/1000 | Loss: 0.00002359
Iteration 129/1000 | Loss: 0.00002359
Iteration 130/1000 | Loss: 0.00002359
Iteration 131/1000 | Loss: 0.00002359
Iteration 132/1000 | Loss: 0.00002359
Iteration 133/1000 | Loss: 0.00002359
Iteration 134/1000 | Loss: 0.00002359
Iteration 135/1000 | Loss: 0.00002359
Iteration 136/1000 | Loss: 0.00002359
Iteration 137/1000 | Loss: 0.00002359
Iteration 138/1000 | Loss: 0.00002359
Iteration 139/1000 | Loss: 0.00002358
Iteration 140/1000 | Loss: 0.00002358
Iteration 141/1000 | Loss: 0.00002358
Iteration 142/1000 | Loss: 0.00002358
Iteration 143/1000 | Loss: 0.00002358
Iteration 144/1000 | Loss: 0.00002358
Iteration 145/1000 | Loss: 0.00002358
Iteration 146/1000 | Loss: 0.00002358
Iteration 147/1000 | Loss: 0.00002358
Iteration 148/1000 | Loss: 0.00002358
Iteration 149/1000 | Loss: 0.00002358
Iteration 150/1000 | Loss: 0.00002358
Iteration 151/1000 | Loss: 0.00002358
Iteration 152/1000 | Loss: 0.00002358
Iteration 153/1000 | Loss: 0.00002358
Iteration 154/1000 | Loss: 0.00002358
Iteration 155/1000 | Loss: 0.00002358
Iteration 156/1000 | Loss: 0.00002358
Iteration 157/1000 | Loss: 0.00002358
Iteration 158/1000 | Loss: 0.00002358
Iteration 159/1000 | Loss: 0.00002358
Iteration 160/1000 | Loss: 0.00002358
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.3575505110784434e-05, 2.3575505110784434e-05, 2.3575505110784434e-05, 2.3575505110784434e-05, 2.3575505110784434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3575505110784434e-05

Optimization complete. Final v2v error: 3.9810104370117188 mm

Highest mean error: 5.372413635253906 mm for frame 11

Lowest mean error: 3.7200376987457275 mm for frame 147

Saving results

Total time: 121.8951964378357
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447132
Iteration 2/25 | Loss: 0.00150408
Iteration 3/25 | Loss: 0.00143146
Iteration 4/25 | Loss: 0.00141396
Iteration 5/25 | Loss: 0.00140970
Iteration 6/25 | Loss: 0.00140876
Iteration 7/25 | Loss: 0.00140876
Iteration 8/25 | Loss: 0.00140876
Iteration 9/25 | Loss: 0.00140876
Iteration 10/25 | Loss: 0.00140876
Iteration 11/25 | Loss: 0.00140876
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014087582239881158, 0.0014087582239881158, 0.0014087582239881158, 0.0014087582239881158, 0.0014087582239881158]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014087582239881158

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11054134
Iteration 2/25 | Loss: 0.00482644
Iteration 3/25 | Loss: 0.00482644
Iteration 4/25 | Loss: 0.00482644
Iteration 5/25 | Loss: 0.00482644
Iteration 6/25 | Loss: 0.00482644
Iteration 7/25 | Loss: 0.00482644
Iteration 8/25 | Loss: 0.00482644
Iteration 9/25 | Loss: 0.00482644
Iteration 10/25 | Loss: 0.00482644
Iteration 11/25 | Loss: 0.00482644
Iteration 12/25 | Loss: 0.00482644
Iteration 13/25 | Loss: 0.00482644
Iteration 14/25 | Loss: 0.00482644
Iteration 15/25 | Loss: 0.00482644
Iteration 16/25 | Loss: 0.00482644
Iteration 17/25 | Loss: 0.00482644
Iteration 18/25 | Loss: 0.00482644
Iteration 19/25 | Loss: 0.00482644
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004826436750590801, 0.004826436750590801, 0.004826436750590801, 0.004826436750590801, 0.004826436750590801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004826436750590801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00482644
Iteration 2/1000 | Loss: 0.00004239
Iteration 3/1000 | Loss: 0.00002975
Iteration 4/1000 | Loss: 0.00002725
Iteration 5/1000 | Loss: 0.00002576
Iteration 6/1000 | Loss: 0.00002436
Iteration 7/1000 | Loss: 0.00002317
Iteration 8/1000 | Loss: 0.00002256
Iteration 9/1000 | Loss: 0.00002191
Iteration 10/1000 | Loss: 0.00002132
Iteration 11/1000 | Loss: 0.00002085
Iteration 12/1000 | Loss: 0.00002051
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00001987
Iteration 15/1000 | Loss: 0.00001982
Iteration 16/1000 | Loss: 0.00001981
Iteration 17/1000 | Loss: 0.00001980
Iteration 18/1000 | Loss: 0.00001979
Iteration 19/1000 | Loss: 0.00001979
Iteration 20/1000 | Loss: 0.00001979
Iteration 21/1000 | Loss: 0.00001971
Iteration 22/1000 | Loss: 0.00001970
Iteration 23/1000 | Loss: 0.00001966
Iteration 24/1000 | Loss: 0.00001961
Iteration 25/1000 | Loss: 0.00001960
Iteration 26/1000 | Loss: 0.00001958
Iteration 27/1000 | Loss: 0.00001957
Iteration 28/1000 | Loss: 0.00001951
Iteration 29/1000 | Loss: 0.00001949
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001938
Iteration 32/1000 | Loss: 0.00001938
Iteration 33/1000 | Loss: 0.00001937
Iteration 34/1000 | Loss: 0.00001937
Iteration 35/1000 | Loss: 0.00001937
Iteration 36/1000 | Loss: 0.00001936
Iteration 37/1000 | Loss: 0.00001934
Iteration 38/1000 | Loss: 0.00001934
Iteration 39/1000 | Loss: 0.00001933
Iteration 40/1000 | Loss: 0.00001932
Iteration 41/1000 | Loss: 0.00001932
Iteration 42/1000 | Loss: 0.00001932
Iteration 43/1000 | Loss: 0.00001932
Iteration 44/1000 | Loss: 0.00001932
Iteration 45/1000 | Loss: 0.00001932
Iteration 46/1000 | Loss: 0.00001931
Iteration 47/1000 | Loss: 0.00001931
Iteration 48/1000 | Loss: 0.00001930
Iteration 49/1000 | Loss: 0.00001930
Iteration 50/1000 | Loss: 0.00001929
Iteration 51/1000 | Loss: 0.00001929
Iteration 52/1000 | Loss: 0.00001928
Iteration 53/1000 | Loss: 0.00001928
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001928
Iteration 56/1000 | Loss: 0.00001927
Iteration 57/1000 | Loss: 0.00001927
Iteration 58/1000 | Loss: 0.00001927
Iteration 59/1000 | Loss: 0.00001927
Iteration 60/1000 | Loss: 0.00001927
Iteration 61/1000 | Loss: 0.00001927
Iteration 62/1000 | Loss: 0.00001927
Iteration 63/1000 | Loss: 0.00001927
Iteration 64/1000 | Loss: 0.00001926
Iteration 65/1000 | Loss: 0.00001926
Iteration 66/1000 | Loss: 0.00001926
Iteration 67/1000 | Loss: 0.00001926
Iteration 68/1000 | Loss: 0.00001926
Iteration 69/1000 | Loss: 0.00001926
Iteration 70/1000 | Loss: 0.00001926
Iteration 71/1000 | Loss: 0.00001925
Iteration 72/1000 | Loss: 0.00001925
Iteration 73/1000 | Loss: 0.00001925
Iteration 74/1000 | Loss: 0.00001924
Iteration 75/1000 | Loss: 0.00001924
Iteration 76/1000 | Loss: 0.00001924
Iteration 77/1000 | Loss: 0.00001924
Iteration 78/1000 | Loss: 0.00001924
Iteration 79/1000 | Loss: 0.00001924
Iteration 80/1000 | Loss: 0.00001924
Iteration 81/1000 | Loss: 0.00001924
Iteration 82/1000 | Loss: 0.00001924
Iteration 83/1000 | Loss: 0.00001924
Iteration 84/1000 | Loss: 0.00001924
Iteration 85/1000 | Loss: 0.00001924
Iteration 86/1000 | Loss: 0.00001924
Iteration 87/1000 | Loss: 0.00001924
Iteration 88/1000 | Loss: 0.00001924
Iteration 89/1000 | Loss: 0.00001923
Iteration 90/1000 | Loss: 0.00001923
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001923
Iteration 93/1000 | Loss: 0.00001923
Iteration 94/1000 | Loss: 0.00001923
Iteration 95/1000 | Loss: 0.00001923
Iteration 96/1000 | Loss: 0.00001923
Iteration 97/1000 | Loss: 0.00001923
Iteration 98/1000 | Loss: 0.00001923
Iteration 99/1000 | Loss: 0.00001923
Iteration 100/1000 | Loss: 0.00001923
Iteration 101/1000 | Loss: 0.00001923
Iteration 102/1000 | Loss: 0.00001923
Iteration 103/1000 | Loss: 0.00001922
Iteration 104/1000 | Loss: 0.00001922
Iteration 105/1000 | Loss: 0.00001922
Iteration 106/1000 | Loss: 0.00001922
Iteration 107/1000 | Loss: 0.00001922
Iteration 108/1000 | Loss: 0.00001922
Iteration 109/1000 | Loss: 0.00001922
Iteration 110/1000 | Loss: 0.00001922
Iteration 111/1000 | Loss: 0.00001922
Iteration 112/1000 | Loss: 0.00001922
Iteration 113/1000 | Loss: 0.00001922
Iteration 114/1000 | Loss: 0.00001922
Iteration 115/1000 | Loss: 0.00001922
Iteration 116/1000 | Loss: 0.00001922
Iteration 117/1000 | Loss: 0.00001922
Iteration 118/1000 | Loss: 0.00001922
Iteration 119/1000 | Loss: 0.00001922
Iteration 120/1000 | Loss: 0.00001922
Iteration 121/1000 | Loss: 0.00001922
Iteration 122/1000 | Loss: 0.00001922
Iteration 123/1000 | Loss: 0.00001922
Iteration 124/1000 | Loss: 0.00001922
Iteration 125/1000 | Loss: 0.00001922
Iteration 126/1000 | Loss: 0.00001922
Iteration 127/1000 | Loss: 0.00001922
Iteration 128/1000 | Loss: 0.00001922
Iteration 129/1000 | Loss: 0.00001922
Iteration 130/1000 | Loss: 0.00001922
Iteration 131/1000 | Loss: 0.00001922
Iteration 132/1000 | Loss: 0.00001922
Iteration 133/1000 | Loss: 0.00001922
Iteration 134/1000 | Loss: 0.00001922
Iteration 135/1000 | Loss: 0.00001922
Iteration 136/1000 | Loss: 0.00001922
Iteration 137/1000 | Loss: 0.00001922
Iteration 138/1000 | Loss: 0.00001922
Iteration 139/1000 | Loss: 0.00001922
Iteration 140/1000 | Loss: 0.00001922
Iteration 141/1000 | Loss: 0.00001922
Iteration 142/1000 | Loss: 0.00001922
Iteration 143/1000 | Loss: 0.00001922
Iteration 144/1000 | Loss: 0.00001922
Iteration 145/1000 | Loss: 0.00001922
Iteration 146/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.9217817680328153e-05, 1.9217817680328153e-05, 1.9217817680328153e-05, 1.9217817680328153e-05, 1.9217817680328153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9217817680328153e-05

Optimization complete. Final v2v error: 3.8893964290618896 mm

Highest mean error: 4.120063781738281 mm for frame 80

Lowest mean error: 3.650954484939575 mm for frame 133

Saving results

Total time: 38.37095618247986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019885
Iteration 2/25 | Loss: 0.00216204
Iteration 3/25 | Loss: 0.00167546
Iteration 4/25 | Loss: 0.00159408
Iteration 5/25 | Loss: 0.00161094
Iteration 6/25 | Loss: 0.00152123
Iteration 7/25 | Loss: 0.00146167
Iteration 8/25 | Loss: 0.00142351
Iteration 9/25 | Loss: 0.00141736
Iteration 10/25 | Loss: 0.00141535
Iteration 11/25 | Loss: 0.00141501
Iteration 12/25 | Loss: 0.00141501
Iteration 13/25 | Loss: 0.00141501
Iteration 14/25 | Loss: 0.00141501
Iteration 15/25 | Loss: 0.00141501
Iteration 16/25 | Loss: 0.00141501
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014150092611089349, 0.0014150092611089349, 0.0014150092611089349, 0.0014150092611089349, 0.0014150092611089349]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014150092611089349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.04536712
Iteration 2/25 | Loss: 0.00511153
Iteration 3/25 | Loss: 0.00511153
Iteration 4/25 | Loss: 0.00511153
Iteration 5/25 | Loss: 0.00511153
Iteration 6/25 | Loss: 0.00511153
Iteration 7/25 | Loss: 0.00511153
Iteration 8/25 | Loss: 0.00511153
Iteration 9/25 | Loss: 0.00511153
Iteration 10/25 | Loss: 0.00511153
Iteration 11/25 | Loss: 0.00511153
Iteration 12/25 | Loss: 0.00511153
Iteration 13/25 | Loss: 0.00511153
Iteration 14/25 | Loss: 0.00511153
Iteration 15/25 | Loss: 0.00511153
Iteration 16/25 | Loss: 0.00511153
Iteration 17/25 | Loss: 0.00511153
Iteration 18/25 | Loss: 0.00511153
Iteration 19/25 | Loss: 0.00511153
Iteration 20/25 | Loss: 0.00511153
Iteration 21/25 | Loss: 0.00511153
Iteration 22/25 | Loss: 0.00511153
Iteration 23/25 | Loss: 0.00511153
Iteration 24/25 | Loss: 0.00511153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00511152995750308, 0.00511152995750308, 0.00511152995750308, 0.00511152995750308, 0.00511152995750308]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00511152995750308

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00511153
Iteration 2/1000 | Loss: 0.00004078
Iteration 3/1000 | Loss: 0.00002813
Iteration 4/1000 | Loss: 0.00002568
Iteration 5/1000 | Loss: 0.00002368
Iteration 6/1000 | Loss: 0.00002278
Iteration 7/1000 | Loss: 0.00002213
Iteration 8/1000 | Loss: 0.00002168
Iteration 9/1000 | Loss: 0.00002151
Iteration 10/1000 | Loss: 0.00002120
Iteration 11/1000 | Loss: 0.00002085
Iteration 12/1000 | Loss: 0.00002044
Iteration 13/1000 | Loss: 0.00002015
Iteration 14/1000 | Loss: 0.00001995
Iteration 15/1000 | Loss: 0.00001983
Iteration 16/1000 | Loss: 0.00001969
Iteration 17/1000 | Loss: 0.00001968
Iteration 18/1000 | Loss: 0.00001955
Iteration 19/1000 | Loss: 0.00001954
Iteration 20/1000 | Loss: 0.00001950
Iteration 21/1000 | Loss: 0.00001950
Iteration 22/1000 | Loss: 0.00001950
Iteration 23/1000 | Loss: 0.00001950
Iteration 24/1000 | Loss: 0.00001950
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00001949
Iteration 27/1000 | Loss: 0.00001949
Iteration 28/1000 | Loss: 0.00001949
Iteration 29/1000 | Loss: 0.00001949
Iteration 30/1000 | Loss: 0.00001949
Iteration 31/1000 | Loss: 0.00001949
Iteration 32/1000 | Loss: 0.00001949
Iteration 33/1000 | Loss: 0.00001949
Iteration 34/1000 | Loss: 0.00001949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 34. Stopping optimization.
Last 5 losses: [1.9490822523948736e-05, 1.9490822523948736e-05, 1.9490822523948736e-05, 1.9490822523948736e-05, 1.9490822523948736e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9490822523948736e-05

Optimization complete. Final v2v error: 3.7715466022491455 mm

Highest mean error: 4.047132968902588 mm for frame 11

Lowest mean error: 3.242368459701538 mm for frame 182

Saving results

Total time: 40.36120247840881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01010113
Iteration 2/25 | Loss: 0.00214407
Iteration 3/25 | Loss: 0.00158883
Iteration 4/25 | Loss: 0.00150175
Iteration 5/25 | Loss: 0.00149975
Iteration 6/25 | Loss: 0.00147947
Iteration 7/25 | Loss: 0.00145759
Iteration 8/25 | Loss: 0.00143437
Iteration 9/25 | Loss: 0.00142938
Iteration 10/25 | Loss: 0.00142680
Iteration 11/25 | Loss: 0.00141803
Iteration 12/25 | Loss: 0.00141400
Iteration 13/25 | Loss: 0.00141122
Iteration 14/25 | Loss: 0.00141133
Iteration 15/25 | Loss: 0.00141199
Iteration 16/25 | Loss: 0.00140925
Iteration 17/25 | Loss: 0.00141497
Iteration 18/25 | Loss: 0.00140976
Iteration 19/25 | Loss: 0.00140599
Iteration 20/25 | Loss: 0.00140953
Iteration 21/25 | Loss: 0.00140858
Iteration 22/25 | Loss: 0.00140754
Iteration 23/25 | Loss: 0.00140767
Iteration 24/25 | Loss: 0.00140749
Iteration 25/25 | Loss: 0.00140814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09152377
Iteration 2/25 | Loss: 0.00544379
Iteration 3/25 | Loss: 0.00533313
Iteration 4/25 | Loss: 0.00533287
Iteration 5/25 | Loss: 0.00532740
Iteration 6/25 | Loss: 0.00532739
Iteration 7/25 | Loss: 0.00532739
Iteration 8/25 | Loss: 0.00532739
Iteration 9/25 | Loss: 0.00532739
Iteration 10/25 | Loss: 0.00532739
Iteration 11/25 | Loss: 0.00532738
Iteration 12/25 | Loss: 0.00532738
Iteration 13/25 | Loss: 0.00532738
Iteration 14/25 | Loss: 0.00532738
Iteration 15/25 | Loss: 0.00532738
Iteration 16/25 | Loss: 0.00532738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005327384453266859, 0.005327384453266859, 0.005327384453266859, 0.005327384453266859, 0.005327384453266859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005327384453266859

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00532738
Iteration 2/1000 | Loss: 0.00050884
Iteration 3/1000 | Loss: 0.00069981
Iteration 4/1000 | Loss: 0.00023271
Iteration 5/1000 | Loss: 0.00036721
Iteration 6/1000 | Loss: 0.00055355
Iteration 7/1000 | Loss: 0.00009761
Iteration 8/1000 | Loss: 0.00016296
Iteration 9/1000 | Loss: 0.00013838
Iteration 10/1000 | Loss: 0.00015428
Iteration 11/1000 | Loss: 0.00018578
Iteration 12/1000 | Loss: 0.00014343
Iteration 13/1000 | Loss: 0.00025227
Iteration 14/1000 | Loss: 0.00049269
Iteration 15/1000 | Loss: 0.00008939
Iteration 16/1000 | Loss: 0.00051877
Iteration 17/1000 | Loss: 0.00006331
Iteration 18/1000 | Loss: 0.00051387
Iteration 19/1000 | Loss: 0.00100478
Iteration 20/1000 | Loss: 0.00006791
Iteration 21/1000 | Loss: 0.00017799
Iteration 22/1000 | Loss: 0.00108586
Iteration 23/1000 | Loss: 0.00031726
Iteration 24/1000 | Loss: 0.00017077
Iteration 25/1000 | Loss: 0.00021399
Iteration 26/1000 | Loss: 0.00011791
Iteration 27/1000 | Loss: 0.00005630
Iteration 28/1000 | Loss: 0.00022984
Iteration 29/1000 | Loss: 0.00011470
Iteration 30/1000 | Loss: 0.00008772
Iteration 31/1000 | Loss: 0.00015420
Iteration 32/1000 | Loss: 0.00012092
Iteration 33/1000 | Loss: 0.00010148
Iteration 34/1000 | Loss: 0.00035028
Iteration 35/1000 | Loss: 0.00014695
Iteration 36/1000 | Loss: 0.00013832
Iteration 37/1000 | Loss: 0.00015472
Iteration 38/1000 | Loss: 0.00010652
Iteration 39/1000 | Loss: 0.00005815
Iteration 40/1000 | Loss: 0.00005069
Iteration 41/1000 | Loss: 0.00004953
Iteration 42/1000 | Loss: 0.00003634
Iteration 43/1000 | Loss: 0.00020922
Iteration 44/1000 | Loss: 0.00021092
Iteration 45/1000 | Loss: 0.00012107
Iteration 46/1000 | Loss: 0.00013794
Iteration 47/1000 | Loss: 0.00009538
Iteration 48/1000 | Loss: 0.00005850
Iteration 49/1000 | Loss: 0.00003613
Iteration 50/1000 | Loss: 0.00003482
Iteration 51/1000 | Loss: 0.00013676
Iteration 52/1000 | Loss: 0.00010798
Iteration 53/1000 | Loss: 0.00018536
Iteration 54/1000 | Loss: 0.00010657
Iteration 55/1000 | Loss: 0.00014001
Iteration 56/1000 | Loss: 0.00009533
Iteration 57/1000 | Loss: 0.00013439
Iteration 58/1000 | Loss: 0.00071621
Iteration 59/1000 | Loss: 0.00026626
Iteration 60/1000 | Loss: 0.00012736
Iteration 61/1000 | Loss: 0.00044260
Iteration 62/1000 | Loss: 0.00026527
Iteration 63/1000 | Loss: 0.00038321
Iteration 64/1000 | Loss: 0.00006526
Iteration 65/1000 | Loss: 0.00009953
Iteration 66/1000 | Loss: 0.00004627
Iteration 67/1000 | Loss: 0.00004019
Iteration 68/1000 | Loss: 0.00004794
Iteration 69/1000 | Loss: 0.00006430
Iteration 70/1000 | Loss: 0.00003275
Iteration 71/1000 | Loss: 0.00003183
Iteration 72/1000 | Loss: 0.00003101
Iteration 73/1000 | Loss: 0.00044379
Iteration 74/1000 | Loss: 0.00038938
Iteration 75/1000 | Loss: 0.00018460
Iteration 76/1000 | Loss: 0.00022284
Iteration 77/1000 | Loss: 0.00003548
Iteration 78/1000 | Loss: 0.00003013
Iteration 79/1000 | Loss: 0.00003174
Iteration 80/1000 | Loss: 0.00003219
Iteration 81/1000 | Loss: 0.00003596
Iteration 82/1000 | Loss: 0.00003453
Iteration 83/1000 | Loss: 0.00003298
Iteration 84/1000 | Loss: 0.00003587
Iteration 85/1000 | Loss: 0.00002901
Iteration 86/1000 | Loss: 0.00002567
Iteration 87/1000 | Loss: 0.00002566
Iteration 88/1000 | Loss: 0.00002563
Iteration 89/1000 | Loss: 0.00002563
Iteration 90/1000 | Loss: 0.00002562
Iteration 91/1000 | Loss: 0.00002557
Iteration 92/1000 | Loss: 0.00009265
Iteration 93/1000 | Loss: 0.00009307
Iteration 94/1000 | Loss: 0.00003048
Iteration 95/1000 | Loss: 0.00002904
Iteration 96/1000 | Loss: 0.00002546
Iteration 97/1000 | Loss: 0.00004022
Iteration 98/1000 | Loss: 0.00002767
Iteration 99/1000 | Loss: 0.00002504
Iteration 100/1000 | Loss: 0.00002497
Iteration 101/1000 | Loss: 0.00002497
Iteration 102/1000 | Loss: 0.00002495
Iteration 103/1000 | Loss: 0.00002495
Iteration 104/1000 | Loss: 0.00002495
Iteration 105/1000 | Loss: 0.00002495
Iteration 106/1000 | Loss: 0.00002494
Iteration 107/1000 | Loss: 0.00002494
Iteration 108/1000 | Loss: 0.00002492
Iteration 109/1000 | Loss: 0.00003093
Iteration 110/1000 | Loss: 0.00002477
Iteration 111/1000 | Loss: 0.00002477
Iteration 112/1000 | Loss: 0.00002477
Iteration 113/1000 | Loss: 0.00002477
Iteration 114/1000 | Loss: 0.00002476
Iteration 115/1000 | Loss: 0.00002472
Iteration 116/1000 | Loss: 0.00002471
Iteration 117/1000 | Loss: 0.00002471
Iteration 118/1000 | Loss: 0.00002470
Iteration 119/1000 | Loss: 0.00002467
Iteration 120/1000 | Loss: 0.00002467
Iteration 121/1000 | Loss: 0.00002466
Iteration 122/1000 | Loss: 0.00002463
Iteration 123/1000 | Loss: 0.00002460
Iteration 124/1000 | Loss: 0.00002459
Iteration 125/1000 | Loss: 0.00012986
Iteration 126/1000 | Loss: 0.00014109
Iteration 127/1000 | Loss: 0.00014108
Iteration 128/1000 | Loss: 0.00014765
Iteration 129/1000 | Loss: 0.00002887
Iteration 130/1000 | Loss: 0.00003459
Iteration 131/1000 | Loss: 0.00003419
Iteration 132/1000 | Loss: 0.00002504
Iteration 133/1000 | Loss: 0.00002463
Iteration 134/1000 | Loss: 0.00002503
Iteration 135/1000 | Loss: 0.00002451
Iteration 136/1000 | Loss: 0.00002459
Iteration 137/1000 | Loss: 0.00002442
Iteration 138/1000 | Loss: 0.00002442
Iteration 139/1000 | Loss: 0.00002442
Iteration 140/1000 | Loss: 0.00002442
Iteration 141/1000 | Loss: 0.00002442
Iteration 142/1000 | Loss: 0.00002442
Iteration 143/1000 | Loss: 0.00002442
Iteration 144/1000 | Loss: 0.00002442
Iteration 145/1000 | Loss: 0.00002441
Iteration 146/1000 | Loss: 0.00002441
Iteration 147/1000 | Loss: 0.00002441
Iteration 148/1000 | Loss: 0.00002441
Iteration 149/1000 | Loss: 0.00002441
Iteration 150/1000 | Loss: 0.00002440
Iteration 151/1000 | Loss: 0.00002440
Iteration 152/1000 | Loss: 0.00002438
Iteration 153/1000 | Loss: 0.00012798
Iteration 154/1000 | Loss: 0.00009263
Iteration 155/1000 | Loss: 0.00019149
Iteration 156/1000 | Loss: 0.00009794
Iteration 157/1000 | Loss: 0.00011285
Iteration 158/1000 | Loss: 0.00003421
Iteration 159/1000 | Loss: 0.00002759
Iteration 160/1000 | Loss: 0.00002744
Iteration 161/1000 | Loss: 0.00002544
Iteration 162/1000 | Loss: 0.00017903
Iteration 163/1000 | Loss: 0.00012486
Iteration 164/1000 | Loss: 0.00003252
Iteration 165/1000 | Loss: 0.00003082
Iteration 166/1000 | Loss: 0.00003056
Iteration 167/1000 | Loss: 0.00016864
Iteration 168/1000 | Loss: 0.00015954
Iteration 169/1000 | Loss: 0.00031565
Iteration 170/1000 | Loss: 0.00018401
Iteration 171/1000 | Loss: 0.00003805
Iteration 172/1000 | Loss: 0.00003270
Iteration 173/1000 | Loss: 0.00012035
Iteration 174/1000 | Loss: 0.00049358
Iteration 175/1000 | Loss: 0.00027693
Iteration 176/1000 | Loss: 0.00011428
Iteration 177/1000 | Loss: 0.00030669
Iteration 178/1000 | Loss: 0.00004389
Iteration 179/1000 | Loss: 0.00005502
Iteration 180/1000 | Loss: 0.00015625
Iteration 181/1000 | Loss: 0.00030076
Iteration 182/1000 | Loss: 0.00037829
Iteration 183/1000 | Loss: 0.00024114
Iteration 184/1000 | Loss: 0.00011056
Iteration 185/1000 | Loss: 0.00009084
Iteration 186/1000 | Loss: 0.00032728
Iteration 187/1000 | Loss: 0.00010759
Iteration 188/1000 | Loss: 0.00003075
Iteration 189/1000 | Loss: 0.00002538
Iteration 190/1000 | Loss: 0.00002475
Iteration 191/1000 | Loss: 0.00002236
Iteration 192/1000 | Loss: 0.00006519
Iteration 193/1000 | Loss: 0.00002636
Iteration 194/1000 | Loss: 0.00003594
Iteration 195/1000 | Loss: 0.00003486
Iteration 196/1000 | Loss: 0.00011999
Iteration 197/1000 | Loss: 0.00011899
Iteration 198/1000 | Loss: 0.00014857
Iteration 199/1000 | Loss: 0.00014495
Iteration 200/1000 | Loss: 0.00002954
Iteration 201/1000 | Loss: 0.00013443
Iteration 202/1000 | Loss: 0.00003729
Iteration 203/1000 | Loss: 0.00008487
Iteration 204/1000 | Loss: 0.00002903
Iteration 205/1000 | Loss: 0.00002561
Iteration 206/1000 | Loss: 0.00002996
Iteration 207/1000 | Loss: 0.00003612
Iteration 208/1000 | Loss: 0.00003507
Iteration 209/1000 | Loss: 0.00002225
Iteration 210/1000 | Loss: 0.00015957
Iteration 211/1000 | Loss: 0.00025217
Iteration 212/1000 | Loss: 0.00005340
Iteration 213/1000 | Loss: 0.00002291
Iteration 214/1000 | Loss: 0.00002832
Iteration 215/1000 | Loss: 0.00013637
Iteration 216/1000 | Loss: 0.00027244
Iteration 217/1000 | Loss: 0.00002191
Iteration 218/1000 | Loss: 0.00014837
Iteration 219/1000 | Loss: 0.00002720
Iteration 220/1000 | Loss: 0.00002449
Iteration 221/1000 | Loss: 0.00008308
Iteration 222/1000 | Loss: 0.00002228
Iteration 223/1000 | Loss: 0.00003047
Iteration 224/1000 | Loss: 0.00002590
Iteration 225/1000 | Loss: 0.00002071
Iteration 226/1000 | Loss: 0.00002049
Iteration 227/1000 | Loss: 0.00002034
Iteration 228/1000 | Loss: 0.00002034
Iteration 229/1000 | Loss: 0.00002032
Iteration 230/1000 | Loss: 0.00002031
Iteration 231/1000 | Loss: 0.00002030
Iteration 232/1000 | Loss: 0.00002030
Iteration 233/1000 | Loss: 0.00002029
Iteration 234/1000 | Loss: 0.00002029
Iteration 235/1000 | Loss: 0.00002028
Iteration 236/1000 | Loss: 0.00002028
Iteration 237/1000 | Loss: 0.00002027
Iteration 238/1000 | Loss: 0.00003566
Iteration 239/1000 | Loss: 0.00002458
Iteration 240/1000 | Loss: 0.00002023
Iteration 241/1000 | Loss: 0.00002023
Iteration 242/1000 | Loss: 0.00002023
Iteration 243/1000 | Loss: 0.00002022
Iteration 244/1000 | Loss: 0.00002022
Iteration 245/1000 | Loss: 0.00003029
Iteration 246/1000 | Loss: 0.00002676
Iteration 247/1000 | Loss: 0.00002368
Iteration 248/1000 | Loss: 0.00002136
Iteration 249/1000 | Loss: 0.00002081
Iteration 250/1000 | Loss: 0.00002098
Iteration 251/1000 | Loss: 0.00002024
Iteration 252/1000 | Loss: 0.00002108
Iteration 253/1000 | Loss: 0.00002017
Iteration 254/1000 | Loss: 0.00002014
Iteration 255/1000 | Loss: 0.00002013
Iteration 256/1000 | Loss: 0.00002013
Iteration 257/1000 | Loss: 0.00002013
Iteration 258/1000 | Loss: 0.00002013
Iteration 259/1000 | Loss: 0.00002013
Iteration 260/1000 | Loss: 0.00002013
Iteration 261/1000 | Loss: 0.00002012
Iteration 262/1000 | Loss: 0.00002012
Iteration 263/1000 | Loss: 0.00002012
Iteration 264/1000 | Loss: 0.00002012
Iteration 265/1000 | Loss: 0.00002012
Iteration 266/1000 | Loss: 0.00002012
Iteration 267/1000 | Loss: 0.00002012
Iteration 268/1000 | Loss: 0.00002012
Iteration 269/1000 | Loss: 0.00002012
Iteration 270/1000 | Loss: 0.00002012
Iteration 271/1000 | Loss: 0.00002012
Iteration 272/1000 | Loss: 0.00002012
Iteration 273/1000 | Loss: 0.00002012
Iteration 274/1000 | Loss: 0.00002012
Iteration 275/1000 | Loss: 0.00002012
Iteration 276/1000 | Loss: 0.00002012
Iteration 277/1000 | Loss: 0.00002012
Iteration 278/1000 | Loss: 0.00002012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [2.012404365814291e-05, 2.012404365814291e-05, 2.012404365814291e-05, 2.012404365814291e-05, 2.012404365814291e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.012404365814291e-05

Optimization complete. Final v2v error: 3.009573459625244 mm

Highest mean error: 12.817659378051758 mm for frame 3

Lowest mean error: 2.579306125640869 mm for frame 86

Saving results

Total time: 363.7031147480011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826877
Iteration 2/25 | Loss: 0.00148279
Iteration 3/25 | Loss: 0.00135789
Iteration 4/25 | Loss: 0.00134015
Iteration 5/25 | Loss: 0.00133362
Iteration 6/25 | Loss: 0.00133180
Iteration 7/25 | Loss: 0.00133169
Iteration 8/25 | Loss: 0.00133169
Iteration 9/25 | Loss: 0.00133169
Iteration 10/25 | Loss: 0.00133169
Iteration 11/25 | Loss: 0.00133169
Iteration 12/25 | Loss: 0.00133169
Iteration 13/25 | Loss: 0.00133169
Iteration 14/25 | Loss: 0.00133169
Iteration 15/25 | Loss: 0.00133169
Iteration 16/25 | Loss: 0.00133169
Iteration 17/25 | Loss: 0.00133169
Iteration 18/25 | Loss: 0.00133169
Iteration 19/25 | Loss: 0.00133169
Iteration 20/25 | Loss: 0.00133169
Iteration 21/25 | Loss: 0.00133169
Iteration 22/25 | Loss: 0.00133169
Iteration 23/25 | Loss: 0.00133169
Iteration 24/25 | Loss: 0.00133169
Iteration 25/25 | Loss: 0.00133169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23700702
Iteration 2/25 | Loss: 0.00671695
Iteration 3/25 | Loss: 0.00671695
Iteration 4/25 | Loss: 0.00671694
Iteration 5/25 | Loss: 0.00671694
Iteration 6/25 | Loss: 0.00671694
Iteration 7/25 | Loss: 0.00671694
Iteration 8/25 | Loss: 0.00671694
Iteration 9/25 | Loss: 0.00671694
Iteration 10/25 | Loss: 0.00671694
Iteration 11/25 | Loss: 0.00671694
Iteration 12/25 | Loss: 0.00671694
Iteration 13/25 | Loss: 0.00671694
Iteration 14/25 | Loss: 0.00671694
Iteration 15/25 | Loss: 0.00671694
Iteration 16/25 | Loss: 0.00671694
Iteration 17/25 | Loss: 0.00671694
Iteration 18/25 | Loss: 0.00671694
Iteration 19/25 | Loss: 0.00671694
Iteration 20/25 | Loss: 0.00671694
Iteration 21/25 | Loss: 0.00671694
Iteration 22/25 | Loss: 0.00671694
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00671694241464138, 0.00671694241464138, 0.00671694241464138, 0.00671694241464138, 0.00671694241464138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00671694241464138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00671694
Iteration 2/1000 | Loss: 0.00006976
Iteration 3/1000 | Loss: 0.00004179
Iteration 4/1000 | Loss: 0.00003245
Iteration 5/1000 | Loss: 0.00002865
Iteration 6/1000 | Loss: 0.00002655
Iteration 7/1000 | Loss: 0.00002518
Iteration 8/1000 | Loss: 0.00002434
Iteration 9/1000 | Loss: 0.00002376
Iteration 10/1000 | Loss: 0.00002335
Iteration 11/1000 | Loss: 0.00002308
Iteration 12/1000 | Loss: 0.00002280
Iteration 13/1000 | Loss: 0.00002248
Iteration 14/1000 | Loss: 0.00002228
Iteration 15/1000 | Loss: 0.00002220
Iteration 16/1000 | Loss: 0.00002216
Iteration 17/1000 | Loss: 0.00002199
Iteration 18/1000 | Loss: 0.00002189
Iteration 19/1000 | Loss: 0.00002185
Iteration 20/1000 | Loss: 0.00002184
Iteration 21/1000 | Loss: 0.00002184
Iteration 22/1000 | Loss: 0.00002182
Iteration 23/1000 | Loss: 0.00002182
Iteration 24/1000 | Loss: 0.00002181
Iteration 25/1000 | Loss: 0.00002181
Iteration 26/1000 | Loss: 0.00002179
Iteration 27/1000 | Loss: 0.00002174
Iteration 28/1000 | Loss: 0.00002171
Iteration 29/1000 | Loss: 0.00002171
Iteration 30/1000 | Loss: 0.00002168
Iteration 31/1000 | Loss: 0.00002168
Iteration 32/1000 | Loss: 0.00002167
Iteration 33/1000 | Loss: 0.00002166
Iteration 34/1000 | Loss: 0.00002166
Iteration 35/1000 | Loss: 0.00002166
Iteration 36/1000 | Loss: 0.00002165
Iteration 37/1000 | Loss: 0.00002165
Iteration 38/1000 | Loss: 0.00002165
Iteration 39/1000 | Loss: 0.00002164
Iteration 40/1000 | Loss: 0.00002164
Iteration 41/1000 | Loss: 0.00002164
Iteration 42/1000 | Loss: 0.00002163
Iteration 43/1000 | Loss: 0.00002163
Iteration 44/1000 | Loss: 0.00002163
Iteration 45/1000 | Loss: 0.00002162
Iteration 46/1000 | Loss: 0.00002162
Iteration 47/1000 | Loss: 0.00002162
Iteration 48/1000 | Loss: 0.00002161
Iteration 49/1000 | Loss: 0.00002161
Iteration 50/1000 | Loss: 0.00002160
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002160
Iteration 54/1000 | Loss: 0.00002159
Iteration 55/1000 | Loss: 0.00002159
Iteration 56/1000 | Loss: 0.00002159
Iteration 57/1000 | Loss: 0.00002158
Iteration 58/1000 | Loss: 0.00002158
Iteration 59/1000 | Loss: 0.00002158
Iteration 60/1000 | Loss: 0.00002158
Iteration 61/1000 | Loss: 0.00002157
Iteration 62/1000 | Loss: 0.00002157
Iteration 63/1000 | Loss: 0.00002157
Iteration 64/1000 | Loss: 0.00002156
Iteration 65/1000 | Loss: 0.00002155
Iteration 66/1000 | Loss: 0.00002155
Iteration 67/1000 | Loss: 0.00002155
Iteration 68/1000 | Loss: 0.00002154
Iteration 69/1000 | Loss: 0.00002154
Iteration 70/1000 | Loss: 0.00002154
Iteration 71/1000 | Loss: 0.00002154
Iteration 72/1000 | Loss: 0.00002154
Iteration 73/1000 | Loss: 0.00002153
Iteration 74/1000 | Loss: 0.00002153
Iteration 75/1000 | Loss: 0.00002153
Iteration 76/1000 | Loss: 0.00002153
Iteration 77/1000 | Loss: 0.00002153
Iteration 78/1000 | Loss: 0.00002153
Iteration 79/1000 | Loss: 0.00002153
Iteration 80/1000 | Loss: 0.00002152
Iteration 81/1000 | Loss: 0.00002152
Iteration 82/1000 | Loss: 0.00002151
Iteration 83/1000 | Loss: 0.00002150
Iteration 84/1000 | Loss: 0.00002150
Iteration 85/1000 | Loss: 0.00002150
Iteration 86/1000 | Loss: 0.00002150
Iteration 87/1000 | Loss: 0.00002150
Iteration 88/1000 | Loss: 0.00002150
Iteration 89/1000 | Loss: 0.00002149
Iteration 90/1000 | Loss: 0.00002149
Iteration 91/1000 | Loss: 0.00002149
Iteration 92/1000 | Loss: 0.00002148
Iteration 93/1000 | Loss: 0.00002148
Iteration 94/1000 | Loss: 0.00002148
Iteration 95/1000 | Loss: 0.00002147
Iteration 96/1000 | Loss: 0.00002147
Iteration 97/1000 | Loss: 0.00002147
Iteration 98/1000 | Loss: 0.00002146
Iteration 99/1000 | Loss: 0.00002146
Iteration 100/1000 | Loss: 0.00002146
Iteration 101/1000 | Loss: 0.00002145
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002145
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002144
Iteration 106/1000 | Loss: 0.00002144
Iteration 107/1000 | Loss: 0.00002144
Iteration 108/1000 | Loss: 0.00002144
Iteration 109/1000 | Loss: 0.00002143
Iteration 110/1000 | Loss: 0.00002143
Iteration 111/1000 | Loss: 0.00002143
Iteration 112/1000 | Loss: 0.00002143
Iteration 113/1000 | Loss: 0.00002142
Iteration 114/1000 | Loss: 0.00002142
Iteration 115/1000 | Loss: 0.00002142
Iteration 116/1000 | Loss: 0.00002142
Iteration 117/1000 | Loss: 0.00002141
Iteration 118/1000 | Loss: 0.00002141
Iteration 119/1000 | Loss: 0.00002141
Iteration 120/1000 | Loss: 0.00002141
Iteration 121/1000 | Loss: 0.00002140
Iteration 122/1000 | Loss: 0.00002140
Iteration 123/1000 | Loss: 0.00002140
Iteration 124/1000 | Loss: 0.00002140
Iteration 125/1000 | Loss: 0.00002140
Iteration 126/1000 | Loss: 0.00002140
Iteration 127/1000 | Loss: 0.00002140
Iteration 128/1000 | Loss: 0.00002140
Iteration 129/1000 | Loss: 0.00002139
Iteration 130/1000 | Loss: 0.00002139
Iteration 131/1000 | Loss: 0.00002139
Iteration 132/1000 | Loss: 0.00002139
Iteration 133/1000 | Loss: 0.00002139
Iteration 134/1000 | Loss: 0.00002139
Iteration 135/1000 | Loss: 0.00002139
Iteration 136/1000 | Loss: 0.00002138
Iteration 137/1000 | Loss: 0.00002138
Iteration 138/1000 | Loss: 0.00002138
Iteration 139/1000 | Loss: 0.00002138
Iteration 140/1000 | Loss: 0.00002137
Iteration 141/1000 | Loss: 0.00002137
Iteration 142/1000 | Loss: 0.00002137
Iteration 143/1000 | Loss: 0.00002137
Iteration 144/1000 | Loss: 0.00002137
Iteration 145/1000 | Loss: 0.00002136
Iteration 146/1000 | Loss: 0.00002136
Iteration 147/1000 | Loss: 0.00002136
Iteration 148/1000 | Loss: 0.00002136
Iteration 149/1000 | Loss: 0.00002136
Iteration 150/1000 | Loss: 0.00002136
Iteration 151/1000 | Loss: 0.00002136
Iteration 152/1000 | Loss: 0.00002136
Iteration 153/1000 | Loss: 0.00002136
Iteration 154/1000 | Loss: 0.00002136
Iteration 155/1000 | Loss: 0.00002136
Iteration 156/1000 | Loss: 0.00002136
Iteration 157/1000 | Loss: 0.00002135
Iteration 158/1000 | Loss: 0.00002135
Iteration 159/1000 | Loss: 0.00002135
Iteration 160/1000 | Loss: 0.00002135
Iteration 161/1000 | Loss: 0.00002135
Iteration 162/1000 | Loss: 0.00002135
Iteration 163/1000 | Loss: 0.00002135
Iteration 164/1000 | Loss: 0.00002135
Iteration 165/1000 | Loss: 0.00002135
Iteration 166/1000 | Loss: 0.00002135
Iteration 167/1000 | Loss: 0.00002135
Iteration 168/1000 | Loss: 0.00002135
Iteration 169/1000 | Loss: 0.00002134
Iteration 170/1000 | Loss: 0.00002134
Iteration 171/1000 | Loss: 0.00002134
Iteration 172/1000 | Loss: 0.00002134
Iteration 173/1000 | Loss: 0.00002134
Iteration 174/1000 | Loss: 0.00002134
Iteration 175/1000 | Loss: 0.00002133
Iteration 176/1000 | Loss: 0.00002133
Iteration 177/1000 | Loss: 0.00002133
Iteration 178/1000 | Loss: 0.00002133
Iteration 179/1000 | Loss: 0.00002133
Iteration 180/1000 | Loss: 0.00002133
Iteration 181/1000 | Loss: 0.00002132
Iteration 182/1000 | Loss: 0.00002132
Iteration 183/1000 | Loss: 0.00002132
Iteration 184/1000 | Loss: 0.00002132
Iteration 185/1000 | Loss: 0.00002132
Iteration 186/1000 | Loss: 0.00002132
Iteration 187/1000 | Loss: 0.00002132
Iteration 188/1000 | Loss: 0.00002132
Iteration 189/1000 | Loss: 0.00002132
Iteration 190/1000 | Loss: 0.00002131
Iteration 191/1000 | Loss: 0.00002131
Iteration 192/1000 | Loss: 0.00002131
Iteration 193/1000 | Loss: 0.00002131
Iteration 194/1000 | Loss: 0.00002131
Iteration 195/1000 | Loss: 0.00002131
Iteration 196/1000 | Loss: 0.00002131
Iteration 197/1000 | Loss: 0.00002130
Iteration 198/1000 | Loss: 0.00002130
Iteration 199/1000 | Loss: 0.00002130
Iteration 200/1000 | Loss: 0.00002130
Iteration 201/1000 | Loss: 0.00002130
Iteration 202/1000 | Loss: 0.00002130
Iteration 203/1000 | Loss: 0.00002129
Iteration 204/1000 | Loss: 0.00002129
Iteration 205/1000 | Loss: 0.00002129
Iteration 206/1000 | Loss: 0.00002129
Iteration 207/1000 | Loss: 0.00002129
Iteration 208/1000 | Loss: 0.00002129
Iteration 209/1000 | Loss: 0.00002129
Iteration 210/1000 | Loss: 0.00002129
Iteration 211/1000 | Loss: 0.00002129
Iteration 212/1000 | Loss: 0.00002129
Iteration 213/1000 | Loss: 0.00002129
Iteration 214/1000 | Loss: 0.00002128
Iteration 215/1000 | Loss: 0.00002128
Iteration 216/1000 | Loss: 0.00002128
Iteration 217/1000 | Loss: 0.00002128
Iteration 218/1000 | Loss: 0.00002128
Iteration 219/1000 | Loss: 0.00002128
Iteration 220/1000 | Loss: 0.00002128
Iteration 221/1000 | Loss: 0.00002128
Iteration 222/1000 | Loss: 0.00002128
Iteration 223/1000 | Loss: 0.00002128
Iteration 224/1000 | Loss: 0.00002128
Iteration 225/1000 | Loss: 0.00002128
Iteration 226/1000 | Loss: 0.00002128
Iteration 227/1000 | Loss: 0.00002128
Iteration 228/1000 | Loss: 0.00002128
Iteration 229/1000 | Loss: 0.00002128
Iteration 230/1000 | Loss: 0.00002128
Iteration 231/1000 | Loss: 0.00002128
Iteration 232/1000 | Loss: 0.00002127
Iteration 233/1000 | Loss: 0.00002127
Iteration 234/1000 | Loss: 0.00002127
Iteration 235/1000 | Loss: 0.00002127
Iteration 236/1000 | Loss: 0.00002127
Iteration 237/1000 | Loss: 0.00002127
Iteration 238/1000 | Loss: 0.00002127
Iteration 239/1000 | Loss: 0.00002127
Iteration 240/1000 | Loss: 0.00002126
Iteration 241/1000 | Loss: 0.00002126
Iteration 242/1000 | Loss: 0.00002126
Iteration 243/1000 | Loss: 0.00002126
Iteration 244/1000 | Loss: 0.00002126
Iteration 245/1000 | Loss: 0.00002126
Iteration 246/1000 | Loss: 0.00002126
Iteration 247/1000 | Loss: 0.00002126
Iteration 248/1000 | Loss: 0.00002126
Iteration 249/1000 | Loss: 0.00002126
Iteration 250/1000 | Loss: 0.00002125
Iteration 251/1000 | Loss: 0.00002125
Iteration 252/1000 | Loss: 0.00002125
Iteration 253/1000 | Loss: 0.00002125
Iteration 254/1000 | Loss: 0.00002125
Iteration 255/1000 | Loss: 0.00002125
Iteration 256/1000 | Loss: 0.00002125
Iteration 257/1000 | Loss: 0.00002125
Iteration 258/1000 | Loss: 0.00002125
Iteration 259/1000 | Loss: 0.00002125
Iteration 260/1000 | Loss: 0.00002125
Iteration 261/1000 | Loss: 0.00002125
Iteration 262/1000 | Loss: 0.00002125
Iteration 263/1000 | Loss: 0.00002125
Iteration 264/1000 | Loss: 0.00002125
Iteration 265/1000 | Loss: 0.00002125
Iteration 266/1000 | Loss: 0.00002125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 266. Stopping optimization.
Last 5 losses: [2.125058017554693e-05, 2.125058017554693e-05, 2.125058017554693e-05, 2.125058017554693e-05, 2.125058017554693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.125058017554693e-05

Optimization complete. Final v2v error: 3.875969648361206 mm

Highest mean error: 5.591605186462402 mm for frame 99

Lowest mean error: 2.6673762798309326 mm for frame 1

Saving results

Total time: 52.28625535964966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00380229
Iteration 2/25 | Loss: 0.00138864
Iteration 3/25 | Loss: 0.00133079
Iteration 4/25 | Loss: 0.00132394
Iteration 5/25 | Loss: 0.00132235
Iteration 6/25 | Loss: 0.00132231
Iteration 7/25 | Loss: 0.00132231
Iteration 8/25 | Loss: 0.00132231
Iteration 9/25 | Loss: 0.00132231
Iteration 10/25 | Loss: 0.00132231
Iteration 11/25 | Loss: 0.00132231
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013223120477050543, 0.0013223120477050543, 0.0013223120477050543, 0.0013223120477050543, 0.0013223120477050543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013223120477050543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42158234
Iteration 2/25 | Loss: 0.00452331
Iteration 3/25 | Loss: 0.00452331
Iteration 4/25 | Loss: 0.00452331
Iteration 5/25 | Loss: 0.00452330
Iteration 6/25 | Loss: 0.00452330
Iteration 7/25 | Loss: 0.00452330
Iteration 8/25 | Loss: 0.00452330
Iteration 9/25 | Loss: 0.00452330
Iteration 10/25 | Loss: 0.00452330
Iteration 11/25 | Loss: 0.00452330
Iteration 12/25 | Loss: 0.00452330
Iteration 13/25 | Loss: 0.00452330
Iteration 14/25 | Loss: 0.00452330
Iteration 15/25 | Loss: 0.00452330
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.004523302894085646, 0.004523302894085646, 0.004523302894085646, 0.004523302894085646, 0.004523302894085646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004523302894085646

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00452330
Iteration 2/1000 | Loss: 0.00003839
Iteration 3/1000 | Loss: 0.00001850
Iteration 4/1000 | Loss: 0.00001554
Iteration 5/1000 | Loss: 0.00001426
Iteration 6/1000 | Loss: 0.00001348
Iteration 7/1000 | Loss: 0.00001219
Iteration 8/1000 | Loss: 0.00001159
Iteration 9/1000 | Loss: 0.00001109
Iteration 10/1000 | Loss: 0.00001079
Iteration 11/1000 | Loss: 0.00001054
Iteration 12/1000 | Loss: 0.00001030
Iteration 13/1000 | Loss: 0.00001027
Iteration 14/1000 | Loss: 0.00001024
Iteration 15/1000 | Loss: 0.00001011
Iteration 16/1000 | Loss: 0.00001004
Iteration 17/1000 | Loss: 0.00000995
Iteration 18/1000 | Loss: 0.00000994
Iteration 19/1000 | Loss: 0.00000994
Iteration 20/1000 | Loss: 0.00000993
Iteration 21/1000 | Loss: 0.00000993
Iteration 22/1000 | Loss: 0.00000993
Iteration 23/1000 | Loss: 0.00000993
Iteration 24/1000 | Loss: 0.00000992
Iteration 25/1000 | Loss: 0.00000984
Iteration 26/1000 | Loss: 0.00000981
Iteration 27/1000 | Loss: 0.00000980
Iteration 28/1000 | Loss: 0.00000976
Iteration 29/1000 | Loss: 0.00000974
Iteration 30/1000 | Loss: 0.00000974
Iteration 31/1000 | Loss: 0.00000973
Iteration 32/1000 | Loss: 0.00000973
Iteration 33/1000 | Loss: 0.00000972
Iteration 34/1000 | Loss: 0.00000971
Iteration 35/1000 | Loss: 0.00000971
Iteration 36/1000 | Loss: 0.00000971
Iteration 37/1000 | Loss: 0.00000971
Iteration 38/1000 | Loss: 0.00000971
Iteration 39/1000 | Loss: 0.00000970
Iteration 40/1000 | Loss: 0.00000970
Iteration 41/1000 | Loss: 0.00000969
Iteration 42/1000 | Loss: 0.00000969
Iteration 43/1000 | Loss: 0.00000969
Iteration 44/1000 | Loss: 0.00000968
Iteration 45/1000 | Loss: 0.00000968
Iteration 46/1000 | Loss: 0.00000968
Iteration 47/1000 | Loss: 0.00000968
Iteration 48/1000 | Loss: 0.00000967
Iteration 49/1000 | Loss: 0.00000967
Iteration 50/1000 | Loss: 0.00000967
Iteration 51/1000 | Loss: 0.00000966
Iteration 52/1000 | Loss: 0.00000966
Iteration 53/1000 | Loss: 0.00000966
Iteration 54/1000 | Loss: 0.00000966
Iteration 55/1000 | Loss: 0.00000965
Iteration 56/1000 | Loss: 0.00000965
Iteration 57/1000 | Loss: 0.00000965
Iteration 58/1000 | Loss: 0.00000964
Iteration 59/1000 | Loss: 0.00000964
Iteration 60/1000 | Loss: 0.00000964
Iteration 61/1000 | Loss: 0.00000963
Iteration 62/1000 | Loss: 0.00000963
Iteration 63/1000 | Loss: 0.00000963
Iteration 64/1000 | Loss: 0.00000963
Iteration 65/1000 | Loss: 0.00000962
Iteration 66/1000 | Loss: 0.00000962
Iteration 67/1000 | Loss: 0.00000961
Iteration 68/1000 | Loss: 0.00000961
Iteration 69/1000 | Loss: 0.00000961
Iteration 70/1000 | Loss: 0.00000961
Iteration 71/1000 | Loss: 0.00000961
Iteration 72/1000 | Loss: 0.00000961
Iteration 73/1000 | Loss: 0.00000961
Iteration 74/1000 | Loss: 0.00000961
Iteration 75/1000 | Loss: 0.00000960
Iteration 76/1000 | Loss: 0.00000960
Iteration 77/1000 | Loss: 0.00000960
Iteration 78/1000 | Loss: 0.00000960
Iteration 79/1000 | Loss: 0.00000960
Iteration 80/1000 | Loss: 0.00000959
Iteration 81/1000 | Loss: 0.00000959
Iteration 82/1000 | Loss: 0.00000959
Iteration 83/1000 | Loss: 0.00000958
Iteration 84/1000 | Loss: 0.00000958
Iteration 85/1000 | Loss: 0.00000958
Iteration 86/1000 | Loss: 0.00000958
Iteration 87/1000 | Loss: 0.00000958
Iteration 88/1000 | Loss: 0.00000957
Iteration 89/1000 | Loss: 0.00000957
Iteration 90/1000 | Loss: 0.00000957
Iteration 91/1000 | Loss: 0.00000956
Iteration 92/1000 | Loss: 0.00000956
Iteration 93/1000 | Loss: 0.00000956
Iteration 94/1000 | Loss: 0.00000955
Iteration 95/1000 | Loss: 0.00000954
Iteration 96/1000 | Loss: 0.00000954
Iteration 97/1000 | Loss: 0.00000953
Iteration 98/1000 | Loss: 0.00000953
Iteration 99/1000 | Loss: 0.00000953
Iteration 100/1000 | Loss: 0.00000953
Iteration 101/1000 | Loss: 0.00000952
Iteration 102/1000 | Loss: 0.00000952
Iteration 103/1000 | Loss: 0.00000951
Iteration 104/1000 | Loss: 0.00000951
Iteration 105/1000 | Loss: 0.00000951
Iteration 106/1000 | Loss: 0.00000950
Iteration 107/1000 | Loss: 0.00000950
Iteration 108/1000 | Loss: 0.00000950
Iteration 109/1000 | Loss: 0.00000949
Iteration 110/1000 | Loss: 0.00000949
Iteration 111/1000 | Loss: 0.00000949
Iteration 112/1000 | Loss: 0.00000948
Iteration 113/1000 | Loss: 0.00000948
Iteration 114/1000 | Loss: 0.00000948
Iteration 115/1000 | Loss: 0.00000948
Iteration 116/1000 | Loss: 0.00000948
Iteration 117/1000 | Loss: 0.00000948
Iteration 118/1000 | Loss: 0.00000948
Iteration 119/1000 | Loss: 0.00000947
Iteration 120/1000 | Loss: 0.00000947
Iteration 121/1000 | Loss: 0.00000947
Iteration 122/1000 | Loss: 0.00000947
Iteration 123/1000 | Loss: 0.00000947
Iteration 124/1000 | Loss: 0.00000947
Iteration 125/1000 | Loss: 0.00000947
Iteration 126/1000 | Loss: 0.00000947
Iteration 127/1000 | Loss: 0.00000947
Iteration 128/1000 | Loss: 0.00000946
Iteration 129/1000 | Loss: 0.00000946
Iteration 130/1000 | Loss: 0.00000946
Iteration 131/1000 | Loss: 0.00000945
Iteration 132/1000 | Loss: 0.00000945
Iteration 133/1000 | Loss: 0.00000945
Iteration 134/1000 | Loss: 0.00000944
Iteration 135/1000 | Loss: 0.00000944
Iteration 136/1000 | Loss: 0.00000944
Iteration 137/1000 | Loss: 0.00000943
Iteration 138/1000 | Loss: 0.00000943
Iteration 139/1000 | Loss: 0.00000943
Iteration 140/1000 | Loss: 0.00000943
Iteration 141/1000 | Loss: 0.00000943
Iteration 142/1000 | Loss: 0.00000942
Iteration 143/1000 | Loss: 0.00000942
Iteration 144/1000 | Loss: 0.00000942
Iteration 145/1000 | Loss: 0.00000942
Iteration 146/1000 | Loss: 0.00000942
Iteration 147/1000 | Loss: 0.00000942
Iteration 148/1000 | Loss: 0.00000942
Iteration 149/1000 | Loss: 0.00000942
Iteration 150/1000 | Loss: 0.00000942
Iteration 151/1000 | Loss: 0.00000942
Iteration 152/1000 | Loss: 0.00000942
Iteration 153/1000 | Loss: 0.00000941
Iteration 154/1000 | Loss: 0.00000941
Iteration 155/1000 | Loss: 0.00000941
Iteration 156/1000 | Loss: 0.00000941
Iteration 157/1000 | Loss: 0.00000941
Iteration 158/1000 | Loss: 0.00000941
Iteration 159/1000 | Loss: 0.00000941
Iteration 160/1000 | Loss: 0.00000941
Iteration 161/1000 | Loss: 0.00000940
Iteration 162/1000 | Loss: 0.00000940
Iteration 163/1000 | Loss: 0.00000940
Iteration 164/1000 | Loss: 0.00000940
Iteration 165/1000 | Loss: 0.00000940
Iteration 166/1000 | Loss: 0.00000940
Iteration 167/1000 | Loss: 0.00000940
Iteration 168/1000 | Loss: 0.00000940
Iteration 169/1000 | Loss: 0.00000940
Iteration 170/1000 | Loss: 0.00000940
Iteration 171/1000 | Loss: 0.00000940
Iteration 172/1000 | Loss: 0.00000940
Iteration 173/1000 | Loss: 0.00000940
Iteration 174/1000 | Loss: 0.00000940
Iteration 175/1000 | Loss: 0.00000940
Iteration 176/1000 | Loss: 0.00000940
Iteration 177/1000 | Loss: 0.00000940
Iteration 178/1000 | Loss: 0.00000940
Iteration 179/1000 | Loss: 0.00000940
Iteration 180/1000 | Loss: 0.00000940
Iteration 181/1000 | Loss: 0.00000940
Iteration 182/1000 | Loss: 0.00000940
Iteration 183/1000 | Loss: 0.00000940
Iteration 184/1000 | Loss: 0.00000940
Iteration 185/1000 | Loss: 0.00000940
Iteration 186/1000 | Loss: 0.00000940
Iteration 187/1000 | Loss: 0.00000940
Iteration 188/1000 | Loss: 0.00000940
Iteration 189/1000 | Loss: 0.00000940
Iteration 190/1000 | Loss: 0.00000940
Iteration 191/1000 | Loss: 0.00000940
Iteration 192/1000 | Loss: 0.00000940
Iteration 193/1000 | Loss: 0.00000940
Iteration 194/1000 | Loss: 0.00000940
Iteration 195/1000 | Loss: 0.00000940
Iteration 196/1000 | Loss: 0.00000940
Iteration 197/1000 | Loss: 0.00000940
Iteration 198/1000 | Loss: 0.00000940
Iteration 199/1000 | Loss: 0.00000940
Iteration 200/1000 | Loss: 0.00000940
Iteration 201/1000 | Loss: 0.00000940
Iteration 202/1000 | Loss: 0.00000940
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [9.39665096666431e-06, 9.39665096666431e-06, 9.39665096666431e-06, 9.39665096666431e-06, 9.39665096666431e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.39665096666431e-06

Optimization complete. Final v2v error: 2.7310266494750977 mm

Highest mean error: 2.960195541381836 mm for frame 2

Lowest mean error: 2.618656635284424 mm for frame 160

Saving results

Total time: 40.82071399688721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500061
Iteration 2/25 | Loss: 0.00148717
Iteration 3/25 | Loss: 0.00139885
Iteration 4/25 | Loss: 0.00138691
Iteration 5/25 | Loss: 0.00138442
Iteration 6/25 | Loss: 0.00138377
Iteration 7/25 | Loss: 0.00138377
Iteration 8/25 | Loss: 0.00138377
Iteration 9/25 | Loss: 0.00138377
Iteration 10/25 | Loss: 0.00138377
Iteration 11/25 | Loss: 0.00138377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013837721198797226, 0.0013837721198797226, 0.0013837721198797226, 0.0013837721198797226, 0.0013837721198797226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013837721198797226

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22222376
Iteration 2/25 | Loss: 0.00403435
Iteration 3/25 | Loss: 0.00403435
Iteration 4/25 | Loss: 0.00403434
Iteration 5/25 | Loss: 0.00403434
Iteration 6/25 | Loss: 0.00403434
Iteration 7/25 | Loss: 0.00403434
Iteration 8/25 | Loss: 0.00403434
Iteration 9/25 | Loss: 0.00403434
Iteration 10/25 | Loss: 0.00403434
Iteration 11/25 | Loss: 0.00403434
Iteration 12/25 | Loss: 0.00403434
Iteration 13/25 | Loss: 0.00403434
Iteration 14/25 | Loss: 0.00403434
Iteration 15/25 | Loss: 0.00403434
Iteration 16/25 | Loss: 0.00403434
Iteration 17/25 | Loss: 0.00403434
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004034340847283602, 0.004034340847283602, 0.004034340847283602, 0.004034340847283602, 0.004034340847283602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004034340847283602

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00403434
Iteration 2/1000 | Loss: 0.00005728
Iteration 3/1000 | Loss: 0.00003533
Iteration 4/1000 | Loss: 0.00002623
Iteration 5/1000 | Loss: 0.00002343
Iteration 6/1000 | Loss: 0.00002199
Iteration 7/1000 | Loss: 0.00002090
Iteration 8/1000 | Loss: 0.00002011
Iteration 9/1000 | Loss: 0.00001921
Iteration 10/1000 | Loss: 0.00001865
Iteration 11/1000 | Loss: 0.00001836
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001773
Iteration 14/1000 | Loss: 0.00001754
Iteration 15/1000 | Loss: 0.00001734
Iteration 16/1000 | Loss: 0.00001732
Iteration 17/1000 | Loss: 0.00001732
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001730
Iteration 20/1000 | Loss: 0.00001730
Iteration 21/1000 | Loss: 0.00001729
Iteration 22/1000 | Loss: 0.00001728
Iteration 23/1000 | Loss: 0.00001728
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001717
Iteration 26/1000 | Loss: 0.00001717
Iteration 27/1000 | Loss: 0.00001716
Iteration 28/1000 | Loss: 0.00001716
Iteration 29/1000 | Loss: 0.00001715
Iteration 30/1000 | Loss: 0.00001714
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001710
Iteration 33/1000 | Loss: 0.00001706
Iteration 34/1000 | Loss: 0.00001706
Iteration 35/1000 | Loss: 0.00001705
Iteration 36/1000 | Loss: 0.00001704
Iteration 37/1000 | Loss: 0.00001704
Iteration 38/1000 | Loss: 0.00001704
Iteration 39/1000 | Loss: 0.00001703
Iteration 40/1000 | Loss: 0.00001702
Iteration 41/1000 | Loss: 0.00001702
Iteration 42/1000 | Loss: 0.00001701
Iteration 43/1000 | Loss: 0.00001693
Iteration 44/1000 | Loss: 0.00001693
Iteration 45/1000 | Loss: 0.00001693
Iteration 46/1000 | Loss: 0.00001692
Iteration 47/1000 | Loss: 0.00001692
Iteration 48/1000 | Loss: 0.00001691
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001690
Iteration 51/1000 | Loss: 0.00001690
Iteration 52/1000 | Loss: 0.00001689
Iteration 53/1000 | Loss: 0.00001689
Iteration 54/1000 | Loss: 0.00001688
Iteration 55/1000 | Loss: 0.00001688
Iteration 56/1000 | Loss: 0.00001688
Iteration 57/1000 | Loss: 0.00001688
Iteration 58/1000 | Loss: 0.00001688
Iteration 59/1000 | Loss: 0.00001688
Iteration 60/1000 | Loss: 0.00001688
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001688
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001685
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001684
Iteration 70/1000 | Loss: 0.00001684
Iteration 71/1000 | Loss: 0.00001683
Iteration 72/1000 | Loss: 0.00001683
Iteration 73/1000 | Loss: 0.00001683
Iteration 74/1000 | Loss: 0.00001683
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001683
Iteration 77/1000 | Loss: 0.00001682
Iteration 78/1000 | Loss: 0.00001682
Iteration 79/1000 | Loss: 0.00001682
Iteration 80/1000 | Loss: 0.00001682
Iteration 81/1000 | Loss: 0.00001682
Iteration 82/1000 | Loss: 0.00001682
Iteration 83/1000 | Loss: 0.00001682
Iteration 84/1000 | Loss: 0.00001681
Iteration 85/1000 | Loss: 0.00001681
Iteration 86/1000 | Loss: 0.00001681
Iteration 87/1000 | Loss: 0.00001681
Iteration 88/1000 | Loss: 0.00001681
Iteration 89/1000 | Loss: 0.00001681
Iteration 90/1000 | Loss: 0.00001681
Iteration 91/1000 | Loss: 0.00001681
Iteration 92/1000 | Loss: 0.00001680
Iteration 93/1000 | Loss: 0.00001680
Iteration 94/1000 | Loss: 0.00001680
Iteration 95/1000 | Loss: 0.00001680
Iteration 96/1000 | Loss: 0.00001680
Iteration 97/1000 | Loss: 0.00001679
Iteration 98/1000 | Loss: 0.00001679
Iteration 99/1000 | Loss: 0.00001679
Iteration 100/1000 | Loss: 0.00001679
Iteration 101/1000 | Loss: 0.00001679
Iteration 102/1000 | Loss: 0.00001678
Iteration 103/1000 | Loss: 0.00001678
Iteration 104/1000 | Loss: 0.00001678
Iteration 105/1000 | Loss: 0.00001678
Iteration 106/1000 | Loss: 0.00001678
Iteration 107/1000 | Loss: 0.00001678
Iteration 108/1000 | Loss: 0.00001678
Iteration 109/1000 | Loss: 0.00001678
Iteration 110/1000 | Loss: 0.00001678
Iteration 111/1000 | Loss: 0.00001677
Iteration 112/1000 | Loss: 0.00001677
Iteration 113/1000 | Loss: 0.00001677
Iteration 114/1000 | Loss: 0.00001677
Iteration 115/1000 | Loss: 0.00001677
Iteration 116/1000 | Loss: 0.00001676
Iteration 117/1000 | Loss: 0.00001676
Iteration 118/1000 | Loss: 0.00001676
Iteration 119/1000 | Loss: 0.00001675
Iteration 120/1000 | Loss: 0.00001675
Iteration 121/1000 | Loss: 0.00001675
Iteration 122/1000 | Loss: 0.00001674
Iteration 123/1000 | Loss: 0.00001674
Iteration 124/1000 | Loss: 0.00001674
Iteration 125/1000 | Loss: 0.00001674
Iteration 126/1000 | Loss: 0.00001673
Iteration 127/1000 | Loss: 0.00001673
Iteration 128/1000 | Loss: 0.00001673
Iteration 129/1000 | Loss: 0.00001673
Iteration 130/1000 | Loss: 0.00001673
Iteration 131/1000 | Loss: 0.00001673
Iteration 132/1000 | Loss: 0.00001672
Iteration 133/1000 | Loss: 0.00001672
Iteration 134/1000 | Loss: 0.00001672
Iteration 135/1000 | Loss: 0.00001672
Iteration 136/1000 | Loss: 0.00001672
Iteration 137/1000 | Loss: 0.00001672
Iteration 138/1000 | Loss: 0.00001672
Iteration 139/1000 | Loss: 0.00001672
Iteration 140/1000 | Loss: 0.00001672
Iteration 141/1000 | Loss: 0.00001672
Iteration 142/1000 | Loss: 0.00001671
Iteration 143/1000 | Loss: 0.00001671
Iteration 144/1000 | Loss: 0.00001671
Iteration 145/1000 | Loss: 0.00001670
Iteration 146/1000 | Loss: 0.00001670
Iteration 147/1000 | Loss: 0.00001670
Iteration 148/1000 | Loss: 0.00001670
Iteration 149/1000 | Loss: 0.00001670
Iteration 150/1000 | Loss: 0.00001670
Iteration 151/1000 | Loss: 0.00001670
Iteration 152/1000 | Loss: 0.00001670
Iteration 153/1000 | Loss: 0.00001670
Iteration 154/1000 | Loss: 0.00001670
Iteration 155/1000 | Loss: 0.00001670
Iteration 156/1000 | Loss: 0.00001670
Iteration 157/1000 | Loss: 0.00001669
Iteration 158/1000 | Loss: 0.00001669
Iteration 159/1000 | Loss: 0.00001669
Iteration 160/1000 | Loss: 0.00001668
Iteration 161/1000 | Loss: 0.00001668
Iteration 162/1000 | Loss: 0.00001668
Iteration 163/1000 | Loss: 0.00001668
Iteration 164/1000 | Loss: 0.00001667
Iteration 165/1000 | Loss: 0.00001667
Iteration 166/1000 | Loss: 0.00001667
Iteration 167/1000 | Loss: 0.00001667
Iteration 168/1000 | Loss: 0.00001667
Iteration 169/1000 | Loss: 0.00001666
Iteration 170/1000 | Loss: 0.00001666
Iteration 171/1000 | Loss: 0.00001666
Iteration 172/1000 | Loss: 0.00001666
Iteration 173/1000 | Loss: 0.00001666
Iteration 174/1000 | Loss: 0.00001666
Iteration 175/1000 | Loss: 0.00001666
Iteration 176/1000 | Loss: 0.00001666
Iteration 177/1000 | Loss: 0.00001666
Iteration 178/1000 | Loss: 0.00001666
Iteration 179/1000 | Loss: 0.00001665
Iteration 180/1000 | Loss: 0.00001665
Iteration 181/1000 | Loss: 0.00001665
Iteration 182/1000 | Loss: 0.00001665
Iteration 183/1000 | Loss: 0.00001665
Iteration 184/1000 | Loss: 0.00001665
Iteration 185/1000 | Loss: 0.00001665
Iteration 186/1000 | Loss: 0.00001665
Iteration 187/1000 | Loss: 0.00001664
Iteration 188/1000 | Loss: 0.00001664
Iteration 189/1000 | Loss: 0.00001664
Iteration 190/1000 | Loss: 0.00001664
Iteration 191/1000 | Loss: 0.00001664
Iteration 192/1000 | Loss: 0.00001664
Iteration 193/1000 | Loss: 0.00001664
Iteration 194/1000 | Loss: 0.00001664
Iteration 195/1000 | Loss: 0.00001663
Iteration 196/1000 | Loss: 0.00001663
Iteration 197/1000 | Loss: 0.00001663
Iteration 198/1000 | Loss: 0.00001663
Iteration 199/1000 | Loss: 0.00001663
Iteration 200/1000 | Loss: 0.00001663
Iteration 201/1000 | Loss: 0.00001663
Iteration 202/1000 | Loss: 0.00001663
Iteration 203/1000 | Loss: 0.00001663
Iteration 204/1000 | Loss: 0.00001663
Iteration 205/1000 | Loss: 0.00001663
Iteration 206/1000 | Loss: 0.00001663
Iteration 207/1000 | Loss: 0.00001663
Iteration 208/1000 | Loss: 0.00001662
Iteration 209/1000 | Loss: 0.00001662
Iteration 210/1000 | Loss: 0.00001662
Iteration 211/1000 | Loss: 0.00001662
Iteration 212/1000 | Loss: 0.00001662
Iteration 213/1000 | Loss: 0.00001662
Iteration 214/1000 | Loss: 0.00001662
Iteration 215/1000 | Loss: 0.00001662
Iteration 216/1000 | Loss: 0.00001662
Iteration 217/1000 | Loss: 0.00001661
Iteration 218/1000 | Loss: 0.00001661
Iteration 219/1000 | Loss: 0.00001661
Iteration 220/1000 | Loss: 0.00001661
Iteration 221/1000 | Loss: 0.00001661
Iteration 222/1000 | Loss: 0.00001661
Iteration 223/1000 | Loss: 0.00001661
Iteration 224/1000 | Loss: 0.00001660
Iteration 225/1000 | Loss: 0.00001660
Iteration 226/1000 | Loss: 0.00001660
Iteration 227/1000 | Loss: 0.00001660
Iteration 228/1000 | Loss: 0.00001660
Iteration 229/1000 | Loss: 0.00001660
Iteration 230/1000 | Loss: 0.00001660
Iteration 231/1000 | Loss: 0.00001660
Iteration 232/1000 | Loss: 0.00001660
Iteration 233/1000 | Loss: 0.00001660
Iteration 234/1000 | Loss: 0.00001660
Iteration 235/1000 | Loss: 0.00001660
Iteration 236/1000 | Loss: 0.00001660
Iteration 237/1000 | Loss: 0.00001660
Iteration 238/1000 | Loss: 0.00001659
Iteration 239/1000 | Loss: 0.00001659
Iteration 240/1000 | Loss: 0.00001659
Iteration 241/1000 | Loss: 0.00001659
Iteration 242/1000 | Loss: 0.00001659
Iteration 243/1000 | Loss: 0.00001659
Iteration 244/1000 | Loss: 0.00001659
Iteration 245/1000 | Loss: 0.00001659
Iteration 246/1000 | Loss: 0.00001659
Iteration 247/1000 | Loss: 0.00001659
Iteration 248/1000 | Loss: 0.00001659
Iteration 249/1000 | Loss: 0.00001659
Iteration 250/1000 | Loss: 0.00001659
Iteration 251/1000 | Loss: 0.00001659
Iteration 252/1000 | Loss: 0.00001658
Iteration 253/1000 | Loss: 0.00001658
Iteration 254/1000 | Loss: 0.00001658
Iteration 255/1000 | Loss: 0.00001658
Iteration 256/1000 | Loss: 0.00001658
Iteration 257/1000 | Loss: 0.00001658
Iteration 258/1000 | Loss: 0.00001658
Iteration 259/1000 | Loss: 0.00001658
Iteration 260/1000 | Loss: 0.00001658
Iteration 261/1000 | Loss: 0.00001658
Iteration 262/1000 | Loss: 0.00001658
Iteration 263/1000 | Loss: 0.00001658
Iteration 264/1000 | Loss: 0.00001658
Iteration 265/1000 | Loss: 0.00001658
Iteration 266/1000 | Loss: 0.00001658
Iteration 267/1000 | Loss: 0.00001658
Iteration 268/1000 | Loss: 0.00001658
Iteration 269/1000 | Loss: 0.00001658
Iteration 270/1000 | Loss: 0.00001658
Iteration 271/1000 | Loss: 0.00001658
Iteration 272/1000 | Loss: 0.00001658
Iteration 273/1000 | Loss: 0.00001658
Iteration 274/1000 | Loss: 0.00001658
Iteration 275/1000 | Loss: 0.00001658
Iteration 276/1000 | Loss: 0.00001658
Iteration 277/1000 | Loss: 0.00001658
Iteration 278/1000 | Loss: 0.00001658
Iteration 279/1000 | Loss: 0.00001658
Iteration 280/1000 | Loss: 0.00001658
Iteration 281/1000 | Loss: 0.00001658
Iteration 282/1000 | Loss: 0.00001658
Iteration 283/1000 | Loss: 0.00001658
Iteration 284/1000 | Loss: 0.00001658
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 284. Stopping optimization.
Last 5 losses: [1.6576530470047146e-05, 1.6576530470047146e-05, 1.6576530470047146e-05, 1.6576530470047146e-05, 1.6576530470047146e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6576530470047146e-05

Optimization complete. Final v2v error: 3.202103853225708 mm

Highest mean error: 5.258069038391113 mm for frame 59

Lowest mean error: 2.5549683570861816 mm for frame 83

Saving results

Total time: 48.715749979019165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00962540
Iteration 2/25 | Loss: 0.00320184
Iteration 3/25 | Loss: 0.00230897
Iteration 4/25 | Loss: 0.00209011
Iteration 5/25 | Loss: 0.00199035
Iteration 6/25 | Loss: 0.00191633
Iteration 7/25 | Loss: 0.00194872
Iteration 8/25 | Loss: 0.00196887
Iteration 9/25 | Loss: 0.00179625
Iteration 10/25 | Loss: 0.00172400
Iteration 11/25 | Loss: 0.00170076
Iteration 12/25 | Loss: 0.00166873
Iteration 13/25 | Loss: 0.00165298
Iteration 14/25 | Loss: 0.00163672
Iteration 15/25 | Loss: 0.00162771
Iteration 16/25 | Loss: 0.00162727
Iteration 17/25 | Loss: 0.00162757
Iteration 18/25 | Loss: 0.00161909
Iteration 19/25 | Loss: 0.00161764
Iteration 20/25 | Loss: 0.00161835
Iteration 21/25 | Loss: 0.00161672
Iteration 22/25 | Loss: 0.00161620
Iteration 23/25 | Loss: 0.00161619
Iteration 24/25 | Loss: 0.00161618
Iteration 25/25 | Loss: 0.00161618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06988132
Iteration 2/25 | Loss: 0.00851549
Iteration 3/25 | Loss: 0.00802952
Iteration 4/25 | Loss: 0.00802952
Iteration 5/25 | Loss: 0.00802952
Iteration 6/25 | Loss: 0.00802952
Iteration 7/25 | Loss: 0.00802952
Iteration 8/25 | Loss: 0.00802952
Iteration 9/25 | Loss: 0.00802952
Iteration 10/25 | Loss: 0.00802952
Iteration 11/25 | Loss: 0.00802952
Iteration 12/25 | Loss: 0.00802952
Iteration 13/25 | Loss: 0.00802952
Iteration 14/25 | Loss: 0.00802952
Iteration 15/25 | Loss: 0.00802952
Iteration 16/25 | Loss: 0.00802952
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.008029516786336899, 0.008029516786336899, 0.008029516786336899, 0.008029516786336899, 0.008029516786336899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008029516786336899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00802952
Iteration 2/1000 | Loss: 0.00110137
Iteration 3/1000 | Loss: 0.00552473
Iteration 4/1000 | Loss: 0.00123799
Iteration 5/1000 | Loss: 0.00058610
Iteration 6/1000 | Loss: 0.00029826
Iteration 7/1000 | Loss: 0.00066799
Iteration 8/1000 | Loss: 0.00043878
Iteration 9/1000 | Loss: 0.00039764
Iteration 10/1000 | Loss: 0.00083511
Iteration 11/1000 | Loss: 0.00096668
Iteration 12/1000 | Loss: 0.00079907
Iteration 13/1000 | Loss: 0.00117841
Iteration 14/1000 | Loss: 0.00200448
Iteration 15/1000 | Loss: 0.00014541
Iteration 16/1000 | Loss: 0.00014365
Iteration 17/1000 | Loss: 0.00020314
Iteration 18/1000 | Loss: 0.00018261
Iteration 19/1000 | Loss: 0.00012686
Iteration 20/1000 | Loss: 0.00026884
Iteration 21/1000 | Loss: 0.00032863
Iteration 22/1000 | Loss: 0.00012381
Iteration 23/1000 | Loss: 0.00016633
Iteration 24/1000 | Loss: 0.00013924
Iteration 25/1000 | Loss: 0.00014638
Iteration 26/1000 | Loss: 0.00012013
Iteration 27/1000 | Loss: 0.00020868
Iteration 28/1000 | Loss: 0.00010967
Iteration 29/1000 | Loss: 0.00010641
Iteration 30/1000 | Loss: 0.00011719
Iteration 31/1000 | Loss: 0.00105901
Iteration 32/1000 | Loss: 0.00014336
Iteration 33/1000 | Loss: 0.00011294
Iteration 34/1000 | Loss: 0.00009994
Iteration 35/1000 | Loss: 0.00014649
Iteration 36/1000 | Loss: 0.00011046
Iteration 37/1000 | Loss: 0.00020656
Iteration 38/1000 | Loss: 0.00164112
Iteration 39/1000 | Loss: 0.00168464
Iteration 40/1000 | Loss: 0.00154178
Iteration 41/1000 | Loss: 0.00199224
Iteration 42/1000 | Loss: 0.00155523
Iteration 43/1000 | Loss: 0.00236245
Iteration 44/1000 | Loss: 0.00247814
Iteration 45/1000 | Loss: 0.00286552
Iteration 46/1000 | Loss: 0.00153769
Iteration 47/1000 | Loss: 0.00088905
Iteration 48/1000 | Loss: 0.00027970
Iteration 49/1000 | Loss: 0.00024142
Iteration 50/1000 | Loss: 0.00071613
Iteration 51/1000 | Loss: 0.00139852
Iteration 52/1000 | Loss: 0.00054564
Iteration 53/1000 | Loss: 0.00037414
Iteration 54/1000 | Loss: 0.00040846
Iteration 55/1000 | Loss: 0.00022684
Iteration 56/1000 | Loss: 0.00010064
Iteration 57/1000 | Loss: 0.00008200
Iteration 58/1000 | Loss: 0.00006957
Iteration 59/1000 | Loss: 0.00045663
Iteration 60/1000 | Loss: 0.00003666
Iteration 61/1000 | Loss: 0.00007892
Iteration 62/1000 | Loss: 0.00003531
Iteration 63/1000 | Loss: 0.00002965
Iteration 64/1000 | Loss: 0.00002987
Iteration 65/1000 | Loss: 0.00003519
Iteration 66/1000 | Loss: 0.00013505
Iteration 67/1000 | Loss: 0.00002817
Iteration 68/1000 | Loss: 0.00005128
Iteration 69/1000 | Loss: 0.00012469
Iteration 70/1000 | Loss: 0.00004337
Iteration 71/1000 | Loss: 0.00002437
Iteration 72/1000 | Loss: 0.00003165
Iteration 73/1000 | Loss: 0.00029153
Iteration 74/1000 | Loss: 0.00036783
Iteration 75/1000 | Loss: 0.00003147
Iteration 76/1000 | Loss: 0.00003244
Iteration 77/1000 | Loss: 0.00047595
Iteration 78/1000 | Loss: 0.00025142
Iteration 79/1000 | Loss: 0.00003116
Iteration 80/1000 | Loss: 0.00020101
Iteration 81/1000 | Loss: 0.00012752
Iteration 82/1000 | Loss: 0.00029549
Iteration 83/1000 | Loss: 0.00010838
Iteration 84/1000 | Loss: 0.00013529
Iteration 85/1000 | Loss: 0.00003527
Iteration 86/1000 | Loss: 0.00002570
Iteration 87/1000 | Loss: 0.00005518
Iteration 88/1000 | Loss: 0.00002773
Iteration 89/1000 | Loss: 0.00002222
Iteration 90/1000 | Loss: 0.00002309
Iteration 91/1000 | Loss: 0.00002142
Iteration 92/1000 | Loss: 0.00002840
Iteration 93/1000 | Loss: 0.00002179
Iteration 94/1000 | Loss: 0.00001697
Iteration 95/1000 | Loss: 0.00001794
Iteration 96/1000 | Loss: 0.00002440
Iteration 97/1000 | Loss: 0.00001674
Iteration 98/1000 | Loss: 0.00001535
Iteration 99/1000 | Loss: 0.00015551
Iteration 100/1000 | Loss: 0.00002589
Iteration 101/1000 | Loss: 0.00002445
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001821
Iteration 104/1000 | Loss: 0.00002713
Iteration 105/1000 | Loss: 0.00001677
Iteration 106/1000 | Loss: 0.00001443
Iteration 107/1000 | Loss: 0.00002852
Iteration 108/1000 | Loss: 0.00002951
Iteration 109/1000 | Loss: 0.00002782
Iteration 110/1000 | Loss: 0.00001453
Iteration 111/1000 | Loss: 0.00001636
Iteration 112/1000 | Loss: 0.00001404
Iteration 113/1000 | Loss: 0.00001400
Iteration 114/1000 | Loss: 0.00001398
Iteration 115/1000 | Loss: 0.00001413
Iteration 116/1000 | Loss: 0.00001413
Iteration 117/1000 | Loss: 0.00001430
Iteration 118/1000 | Loss: 0.00001628
Iteration 119/1000 | Loss: 0.00001409
Iteration 120/1000 | Loss: 0.00001409
Iteration 121/1000 | Loss: 0.00001511
Iteration 122/1000 | Loss: 0.00001398
Iteration 123/1000 | Loss: 0.00001451
Iteration 124/1000 | Loss: 0.00001389
Iteration 125/1000 | Loss: 0.00001389
Iteration 126/1000 | Loss: 0.00001389
Iteration 127/1000 | Loss: 0.00001389
Iteration 128/1000 | Loss: 0.00001389
Iteration 129/1000 | Loss: 0.00001389
Iteration 130/1000 | Loss: 0.00001389
Iteration 131/1000 | Loss: 0.00001389
Iteration 132/1000 | Loss: 0.00001389
Iteration 133/1000 | Loss: 0.00001389
Iteration 134/1000 | Loss: 0.00001388
Iteration 135/1000 | Loss: 0.00001388
Iteration 136/1000 | Loss: 0.00001388
Iteration 137/1000 | Loss: 0.00001388
Iteration 138/1000 | Loss: 0.00001388
Iteration 139/1000 | Loss: 0.00001388
Iteration 140/1000 | Loss: 0.00001388
Iteration 141/1000 | Loss: 0.00001388
Iteration 142/1000 | Loss: 0.00001388
Iteration 143/1000 | Loss: 0.00001388
Iteration 144/1000 | Loss: 0.00001388
Iteration 145/1000 | Loss: 0.00001387
Iteration 146/1000 | Loss: 0.00001387
Iteration 147/1000 | Loss: 0.00001387
Iteration 148/1000 | Loss: 0.00001387
Iteration 149/1000 | Loss: 0.00001387
Iteration 150/1000 | Loss: 0.00001387
Iteration 151/1000 | Loss: 0.00001387
Iteration 152/1000 | Loss: 0.00001387
Iteration 153/1000 | Loss: 0.00001387
Iteration 154/1000 | Loss: 0.00001387
Iteration 155/1000 | Loss: 0.00001387
Iteration 156/1000 | Loss: 0.00001387
Iteration 157/1000 | Loss: 0.00001387
Iteration 158/1000 | Loss: 0.00001387
Iteration 159/1000 | Loss: 0.00001387
Iteration 160/1000 | Loss: 0.00001387
Iteration 161/1000 | Loss: 0.00001386
Iteration 162/1000 | Loss: 0.00001386
Iteration 163/1000 | Loss: 0.00001386
Iteration 164/1000 | Loss: 0.00001386
Iteration 165/1000 | Loss: 0.00001386
Iteration 166/1000 | Loss: 0.00001386
Iteration 167/1000 | Loss: 0.00001386
Iteration 168/1000 | Loss: 0.00001386
Iteration 169/1000 | Loss: 0.00001386
Iteration 170/1000 | Loss: 0.00001829
Iteration 171/1000 | Loss: 0.00001680
Iteration 172/1000 | Loss: 0.00001407
Iteration 173/1000 | Loss: 0.00001407
Iteration 174/1000 | Loss: 0.00001384
Iteration 175/1000 | Loss: 0.00001384
Iteration 176/1000 | Loss: 0.00001384
Iteration 177/1000 | Loss: 0.00001384
Iteration 178/1000 | Loss: 0.00001384
Iteration 179/1000 | Loss: 0.00001384
Iteration 180/1000 | Loss: 0.00001384
Iteration 181/1000 | Loss: 0.00001384
Iteration 182/1000 | Loss: 0.00001384
Iteration 183/1000 | Loss: 0.00001384
Iteration 184/1000 | Loss: 0.00001384
Iteration 185/1000 | Loss: 0.00001384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.3835419849783648e-05, 1.3835419849783648e-05, 1.3835419849783648e-05, 1.3835419849783648e-05, 1.3835419849783648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3835419849783648e-05

Optimization complete. Final v2v error: 3.0958950519561768 mm

Highest mean error: 10.535226821899414 mm for frame 195

Lowest mean error: 2.622620105743408 mm for frame 117

Saving results

Total time: 232.7555592060089
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00383921
Iteration 2/25 | Loss: 0.00141828
Iteration 3/25 | Loss: 0.00134545
Iteration 4/25 | Loss: 0.00133572
Iteration 5/25 | Loss: 0.00133476
Iteration 6/25 | Loss: 0.00133476
Iteration 7/25 | Loss: 0.00133476
Iteration 8/25 | Loss: 0.00133476
Iteration 9/25 | Loss: 0.00133476
Iteration 10/25 | Loss: 0.00133476
Iteration 11/25 | Loss: 0.00133476
Iteration 12/25 | Loss: 0.00133476
Iteration 13/25 | Loss: 0.00133476
Iteration 14/25 | Loss: 0.00133476
Iteration 15/25 | Loss: 0.00133476
Iteration 16/25 | Loss: 0.00133476
Iteration 17/25 | Loss: 0.00133476
Iteration 18/25 | Loss: 0.00133476
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00133475789334625, 0.00133475789334625, 0.00133475789334625, 0.00133475789334625, 0.00133475789334625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00133475789334625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39987552
Iteration 2/25 | Loss: 0.00505086
Iteration 3/25 | Loss: 0.00505083
Iteration 4/25 | Loss: 0.00505083
Iteration 5/25 | Loss: 0.00505083
Iteration 6/25 | Loss: 0.00505083
Iteration 7/25 | Loss: 0.00505083
Iteration 8/25 | Loss: 0.00505082
Iteration 9/25 | Loss: 0.00505082
Iteration 10/25 | Loss: 0.00505082
Iteration 11/25 | Loss: 0.00505082
Iteration 12/25 | Loss: 0.00505082
Iteration 13/25 | Loss: 0.00505082
Iteration 14/25 | Loss: 0.00505082
Iteration 15/25 | Loss: 0.00505082
Iteration 16/25 | Loss: 0.00505082
Iteration 17/25 | Loss: 0.00505082
Iteration 18/25 | Loss: 0.00505082
Iteration 19/25 | Loss: 0.00505082
Iteration 20/25 | Loss: 0.00505082
Iteration 21/25 | Loss: 0.00505082
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00505082355812192, 0.00505082355812192, 0.00505082355812192, 0.00505082355812192, 0.00505082355812192]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00505082355812192

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00505082
Iteration 2/1000 | Loss: 0.00004346
Iteration 3/1000 | Loss: 0.00002458
Iteration 4/1000 | Loss: 0.00001811
Iteration 5/1000 | Loss: 0.00001649
Iteration 6/1000 | Loss: 0.00001520
Iteration 7/1000 | Loss: 0.00001439
Iteration 8/1000 | Loss: 0.00001354
Iteration 9/1000 | Loss: 0.00001302
Iteration 10/1000 | Loss: 0.00001247
Iteration 11/1000 | Loss: 0.00001204
Iteration 12/1000 | Loss: 0.00001183
Iteration 13/1000 | Loss: 0.00001156
Iteration 14/1000 | Loss: 0.00001142
Iteration 15/1000 | Loss: 0.00001138
Iteration 16/1000 | Loss: 0.00001137
Iteration 17/1000 | Loss: 0.00001118
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001092
Iteration 20/1000 | Loss: 0.00001073
Iteration 21/1000 | Loss: 0.00001065
Iteration 22/1000 | Loss: 0.00001064
Iteration 23/1000 | Loss: 0.00001063
Iteration 24/1000 | Loss: 0.00001062
Iteration 25/1000 | Loss: 0.00001060
Iteration 26/1000 | Loss: 0.00001053
Iteration 27/1000 | Loss: 0.00001050
Iteration 28/1000 | Loss: 0.00001044
Iteration 29/1000 | Loss: 0.00001038
Iteration 30/1000 | Loss: 0.00001032
Iteration 31/1000 | Loss: 0.00001029
Iteration 32/1000 | Loss: 0.00001028
Iteration 33/1000 | Loss: 0.00001028
Iteration 34/1000 | Loss: 0.00001027
Iteration 35/1000 | Loss: 0.00001027
Iteration 36/1000 | Loss: 0.00001027
Iteration 37/1000 | Loss: 0.00001026
Iteration 38/1000 | Loss: 0.00001021
Iteration 39/1000 | Loss: 0.00001021
Iteration 40/1000 | Loss: 0.00001019
Iteration 41/1000 | Loss: 0.00001019
Iteration 42/1000 | Loss: 0.00001019
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001014
Iteration 45/1000 | Loss: 0.00001013
Iteration 46/1000 | Loss: 0.00001013
Iteration 47/1000 | Loss: 0.00001013
Iteration 48/1000 | Loss: 0.00001012
Iteration 49/1000 | Loss: 0.00001011
Iteration 50/1000 | Loss: 0.00001011
Iteration 51/1000 | Loss: 0.00001010
Iteration 52/1000 | Loss: 0.00001010
Iteration 53/1000 | Loss: 0.00001009
Iteration 54/1000 | Loss: 0.00001009
Iteration 55/1000 | Loss: 0.00001008
Iteration 56/1000 | Loss: 0.00001008
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001008
Iteration 59/1000 | Loss: 0.00001007
Iteration 60/1000 | Loss: 0.00001007
Iteration 61/1000 | Loss: 0.00001007
Iteration 62/1000 | Loss: 0.00001006
Iteration 63/1000 | Loss: 0.00001004
Iteration 64/1000 | Loss: 0.00001004
Iteration 65/1000 | Loss: 0.00001004
Iteration 66/1000 | Loss: 0.00001004
Iteration 67/1000 | Loss: 0.00001003
Iteration 68/1000 | Loss: 0.00001003
Iteration 69/1000 | Loss: 0.00001003
Iteration 70/1000 | Loss: 0.00001003
Iteration 71/1000 | Loss: 0.00001003
Iteration 72/1000 | Loss: 0.00001003
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001003
Iteration 75/1000 | Loss: 0.00001002
Iteration 76/1000 | Loss: 0.00001001
Iteration 77/1000 | Loss: 0.00001000
Iteration 78/1000 | Loss: 0.00001000
Iteration 79/1000 | Loss: 0.00001000
Iteration 80/1000 | Loss: 0.00001000
Iteration 81/1000 | Loss: 0.00001000
Iteration 82/1000 | Loss: 0.00001000
Iteration 83/1000 | Loss: 0.00001000
Iteration 84/1000 | Loss: 0.00001000
Iteration 85/1000 | Loss: 0.00001000
Iteration 86/1000 | Loss: 0.00001000
Iteration 87/1000 | Loss: 0.00001000
Iteration 88/1000 | Loss: 0.00001000
Iteration 89/1000 | Loss: 0.00001000
Iteration 90/1000 | Loss: 0.00001000
Iteration 91/1000 | Loss: 0.00001000
Iteration 92/1000 | Loss: 0.00001000
Iteration 93/1000 | Loss: 0.00000999
Iteration 94/1000 | Loss: 0.00000999
Iteration 95/1000 | Loss: 0.00000999
Iteration 96/1000 | Loss: 0.00000999
Iteration 97/1000 | Loss: 0.00000999
Iteration 98/1000 | Loss: 0.00000999
Iteration 99/1000 | Loss: 0.00000999
Iteration 100/1000 | Loss: 0.00000999
Iteration 101/1000 | Loss: 0.00000999
Iteration 102/1000 | Loss: 0.00000999
Iteration 103/1000 | Loss: 0.00000998
Iteration 104/1000 | Loss: 0.00000998
Iteration 105/1000 | Loss: 0.00000998
Iteration 106/1000 | Loss: 0.00000997
Iteration 107/1000 | Loss: 0.00000997
Iteration 108/1000 | Loss: 0.00000997
Iteration 109/1000 | Loss: 0.00000997
Iteration 110/1000 | Loss: 0.00000997
Iteration 111/1000 | Loss: 0.00000997
Iteration 112/1000 | Loss: 0.00000996
Iteration 113/1000 | Loss: 0.00000996
Iteration 114/1000 | Loss: 0.00000996
Iteration 115/1000 | Loss: 0.00000996
Iteration 116/1000 | Loss: 0.00000996
Iteration 117/1000 | Loss: 0.00000996
Iteration 118/1000 | Loss: 0.00000996
Iteration 119/1000 | Loss: 0.00000995
Iteration 120/1000 | Loss: 0.00000995
Iteration 121/1000 | Loss: 0.00000995
Iteration 122/1000 | Loss: 0.00000995
Iteration 123/1000 | Loss: 0.00000995
Iteration 124/1000 | Loss: 0.00000995
Iteration 125/1000 | Loss: 0.00000995
Iteration 126/1000 | Loss: 0.00000995
Iteration 127/1000 | Loss: 0.00000995
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000995
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000994
Iteration 135/1000 | Loss: 0.00000994
Iteration 136/1000 | Loss: 0.00000994
Iteration 137/1000 | Loss: 0.00000994
Iteration 138/1000 | Loss: 0.00000994
Iteration 139/1000 | Loss: 0.00000994
Iteration 140/1000 | Loss: 0.00000994
Iteration 141/1000 | Loss: 0.00000994
Iteration 142/1000 | Loss: 0.00000994
Iteration 143/1000 | Loss: 0.00000994
Iteration 144/1000 | Loss: 0.00000994
Iteration 145/1000 | Loss: 0.00000994
Iteration 146/1000 | Loss: 0.00000994
Iteration 147/1000 | Loss: 0.00000994
Iteration 148/1000 | Loss: 0.00000994
Iteration 149/1000 | Loss: 0.00000994
Iteration 150/1000 | Loss: 0.00000994
Iteration 151/1000 | Loss: 0.00000993
Iteration 152/1000 | Loss: 0.00000993
Iteration 153/1000 | Loss: 0.00000993
Iteration 154/1000 | Loss: 0.00000993
Iteration 155/1000 | Loss: 0.00000993
Iteration 156/1000 | Loss: 0.00000993
Iteration 157/1000 | Loss: 0.00000993
Iteration 158/1000 | Loss: 0.00000993
Iteration 159/1000 | Loss: 0.00000993
Iteration 160/1000 | Loss: 0.00000993
Iteration 161/1000 | Loss: 0.00000993
Iteration 162/1000 | Loss: 0.00000993
Iteration 163/1000 | Loss: 0.00000993
Iteration 164/1000 | Loss: 0.00000993
Iteration 165/1000 | Loss: 0.00000993
Iteration 166/1000 | Loss: 0.00000993
Iteration 167/1000 | Loss: 0.00000993
Iteration 168/1000 | Loss: 0.00000993
Iteration 169/1000 | Loss: 0.00000993
Iteration 170/1000 | Loss: 0.00000993
Iteration 171/1000 | Loss: 0.00000993
Iteration 172/1000 | Loss: 0.00000993
Iteration 173/1000 | Loss: 0.00000993
Iteration 174/1000 | Loss: 0.00000993
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [9.926927305059507e-06, 9.926927305059507e-06, 9.926927305059507e-06, 9.926927305059507e-06, 9.926927305059507e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.926927305059507e-06

Optimization complete. Final v2v error: 2.7572076320648193 mm

Highest mean error: 3.1384074687957764 mm for frame 34

Lowest mean error: 2.469452381134033 mm for frame 135

Saving results

Total time: 54.2488534450531
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841934
Iteration 2/25 | Loss: 0.00139630
Iteration 3/25 | Loss: 0.00135152
Iteration 4/25 | Loss: 0.00134664
Iteration 5/25 | Loss: 0.00134549
Iteration 6/25 | Loss: 0.00134549
Iteration 7/25 | Loss: 0.00134549
Iteration 8/25 | Loss: 0.00134549
Iteration 9/25 | Loss: 0.00134549
Iteration 10/25 | Loss: 0.00134549
Iteration 11/25 | Loss: 0.00134549
Iteration 12/25 | Loss: 0.00134549
Iteration 13/25 | Loss: 0.00134549
Iteration 14/25 | Loss: 0.00134549
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013454897562041879, 0.0013454897562041879, 0.0013454897562041879, 0.0013454897562041879, 0.0013454897562041879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013454897562041879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.22300100
Iteration 2/25 | Loss: 0.00443829
Iteration 3/25 | Loss: 0.00443828
Iteration 4/25 | Loss: 0.00443828
Iteration 5/25 | Loss: 0.00443828
Iteration 6/25 | Loss: 0.00443828
Iteration 7/25 | Loss: 0.00443827
Iteration 8/25 | Loss: 0.00443827
Iteration 9/25 | Loss: 0.00443827
Iteration 10/25 | Loss: 0.00443827
Iteration 11/25 | Loss: 0.00443827
Iteration 12/25 | Loss: 0.00443827
Iteration 13/25 | Loss: 0.00443827
Iteration 14/25 | Loss: 0.00443827
Iteration 15/25 | Loss: 0.00443827
Iteration 16/25 | Loss: 0.00443827
Iteration 17/25 | Loss: 0.00443827
Iteration 18/25 | Loss: 0.00443827
Iteration 19/25 | Loss: 0.00443827
Iteration 20/25 | Loss: 0.00443827
Iteration 21/25 | Loss: 0.00443827
Iteration 22/25 | Loss: 0.00443827
Iteration 23/25 | Loss: 0.00443827
Iteration 24/25 | Loss: 0.00443827
Iteration 25/25 | Loss: 0.00443827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00443827
Iteration 2/1000 | Loss: 0.00002966
Iteration 3/1000 | Loss: 0.00002183
Iteration 4/1000 | Loss: 0.00001941
Iteration 5/1000 | Loss: 0.00001796
Iteration 6/1000 | Loss: 0.00001701
Iteration 7/1000 | Loss: 0.00001621
Iteration 8/1000 | Loss: 0.00001561
Iteration 9/1000 | Loss: 0.00001514
Iteration 10/1000 | Loss: 0.00001502
Iteration 11/1000 | Loss: 0.00001479
Iteration 12/1000 | Loss: 0.00001450
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001416
Iteration 15/1000 | Loss: 0.00001404
Iteration 16/1000 | Loss: 0.00001382
Iteration 17/1000 | Loss: 0.00001366
Iteration 18/1000 | Loss: 0.00001364
Iteration 19/1000 | Loss: 0.00001361
Iteration 20/1000 | Loss: 0.00001360
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001358
Iteration 23/1000 | Loss: 0.00001358
Iteration 24/1000 | Loss: 0.00001358
Iteration 25/1000 | Loss: 0.00001358
Iteration 26/1000 | Loss: 0.00001358
Iteration 27/1000 | Loss: 0.00001358
Iteration 28/1000 | Loss: 0.00001358
Iteration 29/1000 | Loss: 0.00001358
Iteration 30/1000 | Loss: 0.00001357
Iteration 31/1000 | Loss: 0.00001357
Iteration 32/1000 | Loss: 0.00001356
Iteration 33/1000 | Loss: 0.00001354
Iteration 34/1000 | Loss: 0.00001354
Iteration 35/1000 | Loss: 0.00001353
Iteration 36/1000 | Loss: 0.00001353
Iteration 37/1000 | Loss: 0.00001353
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001350
Iteration 40/1000 | Loss: 0.00001349
Iteration 41/1000 | Loss: 0.00001349
Iteration 42/1000 | Loss: 0.00001348
Iteration 43/1000 | Loss: 0.00001348
Iteration 44/1000 | Loss: 0.00001348
Iteration 45/1000 | Loss: 0.00001342
Iteration 46/1000 | Loss: 0.00001341
Iteration 47/1000 | Loss: 0.00001339
Iteration 48/1000 | Loss: 0.00001339
Iteration 49/1000 | Loss: 0.00001339
Iteration 50/1000 | Loss: 0.00001339
Iteration 51/1000 | Loss: 0.00001339
Iteration 52/1000 | Loss: 0.00001338
Iteration 53/1000 | Loss: 0.00001338
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001337
Iteration 56/1000 | Loss: 0.00001337
Iteration 57/1000 | Loss: 0.00001337
Iteration 58/1000 | Loss: 0.00001337
Iteration 59/1000 | Loss: 0.00001337
Iteration 60/1000 | Loss: 0.00001336
Iteration 61/1000 | Loss: 0.00001336
Iteration 62/1000 | Loss: 0.00001336
Iteration 63/1000 | Loss: 0.00001336
Iteration 64/1000 | Loss: 0.00001335
Iteration 65/1000 | Loss: 0.00001335
Iteration 66/1000 | Loss: 0.00001335
Iteration 67/1000 | Loss: 0.00001334
Iteration 68/1000 | Loss: 0.00001334
Iteration 69/1000 | Loss: 0.00001334
Iteration 70/1000 | Loss: 0.00001333
Iteration 71/1000 | Loss: 0.00001333
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001330
Iteration 82/1000 | Loss: 0.00001329
Iteration 83/1000 | Loss: 0.00001328
Iteration 84/1000 | Loss: 0.00001328
Iteration 85/1000 | Loss: 0.00001328
Iteration 86/1000 | Loss: 0.00001328
Iteration 87/1000 | Loss: 0.00001327
Iteration 88/1000 | Loss: 0.00001327
Iteration 89/1000 | Loss: 0.00001327
Iteration 90/1000 | Loss: 0.00001327
Iteration 91/1000 | Loss: 0.00001327
Iteration 92/1000 | Loss: 0.00001326
Iteration 93/1000 | Loss: 0.00001326
Iteration 94/1000 | Loss: 0.00001326
Iteration 95/1000 | Loss: 0.00001326
Iteration 96/1000 | Loss: 0.00001326
Iteration 97/1000 | Loss: 0.00001326
Iteration 98/1000 | Loss: 0.00001325
Iteration 99/1000 | Loss: 0.00001325
Iteration 100/1000 | Loss: 0.00001325
Iteration 101/1000 | Loss: 0.00001325
Iteration 102/1000 | Loss: 0.00001325
Iteration 103/1000 | Loss: 0.00001325
Iteration 104/1000 | Loss: 0.00001325
Iteration 105/1000 | Loss: 0.00001325
Iteration 106/1000 | Loss: 0.00001325
Iteration 107/1000 | Loss: 0.00001325
Iteration 108/1000 | Loss: 0.00001325
Iteration 109/1000 | Loss: 0.00001325
Iteration 110/1000 | Loss: 0.00001325
Iteration 111/1000 | Loss: 0.00001325
Iteration 112/1000 | Loss: 0.00001325
Iteration 113/1000 | Loss: 0.00001325
Iteration 114/1000 | Loss: 0.00001325
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001325
Iteration 117/1000 | Loss: 0.00001325
Iteration 118/1000 | Loss: 0.00001325
Iteration 119/1000 | Loss: 0.00001325
Iteration 120/1000 | Loss: 0.00001325
Iteration 121/1000 | Loss: 0.00001325
Iteration 122/1000 | Loss: 0.00001325
Iteration 123/1000 | Loss: 0.00001325
Iteration 124/1000 | Loss: 0.00001325
Iteration 125/1000 | Loss: 0.00001325
Iteration 126/1000 | Loss: 0.00001325
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.324615914199967e-05, 1.324615914199967e-05, 1.324615914199967e-05, 1.324615914199967e-05, 1.324615914199967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.324615914199967e-05

Optimization complete. Final v2v error: 3.2060916423797607 mm

Highest mean error: 3.5593998432159424 mm for frame 47

Lowest mean error: 2.7809362411499023 mm for frame 166

Saving results

Total time: 38.3220636844635
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00369872
Iteration 2/25 | Loss: 0.00143513
Iteration 3/25 | Loss: 0.00135900
Iteration 4/25 | Loss: 0.00134855
Iteration 5/25 | Loss: 0.00134575
Iteration 6/25 | Loss: 0.00134575
Iteration 7/25 | Loss: 0.00134575
Iteration 8/25 | Loss: 0.00134575
Iteration 9/25 | Loss: 0.00134575
Iteration 10/25 | Loss: 0.00134575
Iteration 11/25 | Loss: 0.00134575
Iteration 12/25 | Loss: 0.00134575
Iteration 13/25 | Loss: 0.00134575
Iteration 14/25 | Loss: 0.00134575
Iteration 15/25 | Loss: 0.00134575
Iteration 16/25 | Loss: 0.00134575
Iteration 17/25 | Loss: 0.00134575
Iteration 18/25 | Loss: 0.00134575
Iteration 19/25 | Loss: 0.00134575
Iteration 20/25 | Loss: 0.00134575
Iteration 21/25 | Loss: 0.00134575
Iteration 22/25 | Loss: 0.00134575
Iteration 23/25 | Loss: 0.00134575
Iteration 24/25 | Loss: 0.00134575
Iteration 25/25 | Loss: 0.00134575

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16377759
Iteration 2/25 | Loss: 0.00523193
Iteration 3/25 | Loss: 0.00523192
Iteration 4/25 | Loss: 0.00523192
Iteration 5/25 | Loss: 0.00523192
Iteration 6/25 | Loss: 0.00523192
Iteration 7/25 | Loss: 0.00523192
Iteration 8/25 | Loss: 0.00523192
Iteration 9/25 | Loss: 0.00523192
Iteration 10/25 | Loss: 0.00523192
Iteration 11/25 | Loss: 0.00523192
Iteration 12/25 | Loss: 0.00523192
Iteration 13/25 | Loss: 0.00523192
Iteration 14/25 | Loss: 0.00523192
Iteration 15/25 | Loss: 0.00523192
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005231920629739761, 0.005231920629739761, 0.005231920629739761, 0.005231920629739761, 0.005231920629739761]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005231920629739761

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00523192
Iteration 2/1000 | Loss: 0.00003265
Iteration 3/1000 | Loss: 0.00002141
Iteration 4/1000 | Loss: 0.00001886
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00001697
Iteration 7/1000 | Loss: 0.00001627
Iteration 8/1000 | Loss: 0.00001598
Iteration 9/1000 | Loss: 0.00001546
Iteration 10/1000 | Loss: 0.00001507
Iteration 11/1000 | Loss: 0.00001482
Iteration 12/1000 | Loss: 0.00001453
Iteration 13/1000 | Loss: 0.00001428
Iteration 14/1000 | Loss: 0.00001398
Iteration 15/1000 | Loss: 0.00001372
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001355
Iteration 18/1000 | Loss: 0.00001345
Iteration 19/1000 | Loss: 0.00001344
Iteration 20/1000 | Loss: 0.00001329
Iteration 21/1000 | Loss: 0.00001324
Iteration 22/1000 | Loss: 0.00001320
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001292
Iteration 26/1000 | Loss: 0.00001291
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001285
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001279
Iteration 31/1000 | Loss: 0.00001279
Iteration 32/1000 | Loss: 0.00001279
Iteration 33/1000 | Loss: 0.00001279
Iteration 34/1000 | Loss: 0.00001278
Iteration 35/1000 | Loss: 0.00001278
Iteration 36/1000 | Loss: 0.00001277
Iteration 37/1000 | Loss: 0.00001273
Iteration 38/1000 | Loss: 0.00001273
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001273
Iteration 42/1000 | Loss: 0.00001273
Iteration 43/1000 | Loss: 0.00001273
Iteration 44/1000 | Loss: 0.00001273
Iteration 45/1000 | Loss: 0.00001272
Iteration 46/1000 | Loss: 0.00001272
Iteration 47/1000 | Loss: 0.00001271
Iteration 48/1000 | Loss: 0.00001271
Iteration 49/1000 | Loss: 0.00001271
Iteration 50/1000 | Loss: 0.00001271
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001268
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001268
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001267
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001264
Iteration 59/1000 | Loss: 0.00001264
Iteration 60/1000 | Loss: 0.00001264
Iteration 61/1000 | Loss: 0.00001263
Iteration 62/1000 | Loss: 0.00001263
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001263
Iteration 65/1000 | Loss: 0.00001262
Iteration 66/1000 | Loss: 0.00001262
Iteration 67/1000 | Loss: 0.00001262
Iteration 68/1000 | Loss: 0.00001262
Iteration 69/1000 | Loss: 0.00001262
Iteration 70/1000 | Loss: 0.00001262
Iteration 71/1000 | Loss: 0.00001262
Iteration 72/1000 | Loss: 0.00001262
Iteration 73/1000 | Loss: 0.00001261
Iteration 74/1000 | Loss: 0.00001261
Iteration 75/1000 | Loss: 0.00001261
Iteration 76/1000 | Loss: 0.00001260
Iteration 77/1000 | Loss: 0.00001260
Iteration 78/1000 | Loss: 0.00001260
Iteration 79/1000 | Loss: 0.00001259
Iteration 80/1000 | Loss: 0.00001259
Iteration 81/1000 | Loss: 0.00001259
Iteration 82/1000 | Loss: 0.00001259
Iteration 83/1000 | Loss: 0.00001259
Iteration 84/1000 | Loss: 0.00001259
Iteration 85/1000 | Loss: 0.00001259
Iteration 86/1000 | Loss: 0.00001259
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001258
Iteration 90/1000 | Loss: 0.00001258
Iteration 91/1000 | Loss: 0.00001258
Iteration 92/1000 | Loss: 0.00001258
Iteration 93/1000 | Loss: 0.00001258
Iteration 94/1000 | Loss: 0.00001258
Iteration 95/1000 | Loss: 0.00001258
Iteration 96/1000 | Loss: 0.00001257
Iteration 97/1000 | Loss: 0.00001257
Iteration 98/1000 | Loss: 0.00001257
Iteration 99/1000 | Loss: 0.00001257
Iteration 100/1000 | Loss: 0.00001257
Iteration 101/1000 | Loss: 0.00001257
Iteration 102/1000 | Loss: 0.00001257
Iteration 103/1000 | Loss: 0.00001257
Iteration 104/1000 | Loss: 0.00001257
Iteration 105/1000 | Loss: 0.00001257
Iteration 106/1000 | Loss: 0.00001256
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001256
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001256
Iteration 116/1000 | Loss: 0.00001256
Iteration 117/1000 | Loss: 0.00001256
Iteration 118/1000 | Loss: 0.00001256
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001256
Iteration 121/1000 | Loss: 0.00001256
Iteration 122/1000 | Loss: 0.00001256
Iteration 123/1000 | Loss: 0.00001256
Iteration 124/1000 | Loss: 0.00001256
Iteration 125/1000 | Loss: 0.00001256
Iteration 126/1000 | Loss: 0.00001256
Iteration 127/1000 | Loss: 0.00001256
Iteration 128/1000 | Loss: 0.00001256
Iteration 129/1000 | Loss: 0.00001256
Iteration 130/1000 | Loss: 0.00001256
Iteration 131/1000 | Loss: 0.00001256
Iteration 132/1000 | Loss: 0.00001256
Iteration 133/1000 | Loss: 0.00001256
Iteration 134/1000 | Loss: 0.00001256
Iteration 135/1000 | Loss: 0.00001256
Iteration 136/1000 | Loss: 0.00001256
Iteration 137/1000 | Loss: 0.00001256
Iteration 138/1000 | Loss: 0.00001256
Iteration 139/1000 | Loss: 0.00001256
Iteration 140/1000 | Loss: 0.00001256
Iteration 141/1000 | Loss: 0.00001256
Iteration 142/1000 | Loss: 0.00001256
Iteration 143/1000 | Loss: 0.00001256
Iteration 144/1000 | Loss: 0.00001256
Iteration 145/1000 | Loss: 0.00001256
Iteration 146/1000 | Loss: 0.00001256
Iteration 147/1000 | Loss: 0.00001256
Iteration 148/1000 | Loss: 0.00001256
Iteration 149/1000 | Loss: 0.00001256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.2556391993712168e-05, 1.2556391993712168e-05, 1.2556391993712168e-05, 1.2556391993712168e-05, 1.2556391993712168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2556391993712168e-05

Optimization complete. Final v2v error: 3.062455177307129 mm

Highest mean error: 3.7462716102600098 mm for frame 48

Lowest mean error: 2.3671762943267822 mm for frame 9

Saving results

Total time: 43.79890298843384
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809197
Iteration 2/25 | Loss: 0.00145246
Iteration 3/25 | Loss: 0.00135779
Iteration 4/25 | Loss: 0.00134268
Iteration 5/25 | Loss: 0.00133872
Iteration 6/25 | Loss: 0.00133806
Iteration 7/25 | Loss: 0.00133806
Iteration 8/25 | Loss: 0.00133806
Iteration 9/25 | Loss: 0.00133806
Iteration 10/25 | Loss: 0.00133806
Iteration 11/25 | Loss: 0.00133806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013380595482885838, 0.0013380595482885838, 0.0013380595482885838, 0.0013380595482885838, 0.0013380595482885838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013380595482885838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12574148
Iteration 2/25 | Loss: 0.00454628
Iteration 3/25 | Loss: 0.00454628
Iteration 4/25 | Loss: 0.00454628
Iteration 5/25 | Loss: 0.00454628
Iteration 6/25 | Loss: 0.00454628
Iteration 7/25 | Loss: 0.00454628
Iteration 8/25 | Loss: 0.00454628
Iteration 9/25 | Loss: 0.00454628
Iteration 10/25 | Loss: 0.00454628
Iteration 11/25 | Loss: 0.00454628
Iteration 12/25 | Loss: 0.00454628
Iteration 13/25 | Loss: 0.00454628
Iteration 14/25 | Loss: 0.00454628
Iteration 15/25 | Loss: 0.00454628
Iteration 16/25 | Loss: 0.00454628
Iteration 17/25 | Loss: 0.00454628
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004546275362372398, 0.004546275362372398, 0.004546275362372398, 0.004546275362372398, 0.004546275362372398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004546275362372398

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00454628
Iteration 2/1000 | Loss: 0.00004387
Iteration 3/1000 | Loss: 0.00002748
Iteration 4/1000 | Loss: 0.00002202
Iteration 5/1000 | Loss: 0.00001892
Iteration 6/1000 | Loss: 0.00001754
Iteration 7/1000 | Loss: 0.00001665
Iteration 8/1000 | Loss: 0.00001568
Iteration 9/1000 | Loss: 0.00001509
Iteration 10/1000 | Loss: 0.00001463
Iteration 11/1000 | Loss: 0.00001420
Iteration 12/1000 | Loss: 0.00001389
Iteration 13/1000 | Loss: 0.00001354
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001301
Iteration 17/1000 | Loss: 0.00001286
Iteration 18/1000 | Loss: 0.00001280
Iteration 19/1000 | Loss: 0.00001279
Iteration 20/1000 | Loss: 0.00001271
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001259
Iteration 24/1000 | Loss: 0.00001258
Iteration 25/1000 | Loss: 0.00001257
Iteration 26/1000 | Loss: 0.00001257
Iteration 27/1000 | Loss: 0.00001257
Iteration 28/1000 | Loss: 0.00001257
Iteration 29/1000 | Loss: 0.00001256
Iteration 30/1000 | Loss: 0.00001256
Iteration 31/1000 | Loss: 0.00001255
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001254
Iteration 34/1000 | Loss: 0.00001254
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001250
Iteration 41/1000 | Loss: 0.00001250
Iteration 42/1000 | Loss: 0.00001249
Iteration 43/1000 | Loss: 0.00001249
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001247
Iteration 48/1000 | Loss: 0.00001247
Iteration 49/1000 | Loss: 0.00001247
Iteration 50/1000 | Loss: 0.00001246
Iteration 51/1000 | Loss: 0.00001246
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001246
Iteration 54/1000 | Loss: 0.00001246
Iteration 55/1000 | Loss: 0.00001245
Iteration 56/1000 | Loss: 0.00001244
Iteration 57/1000 | Loss: 0.00001244
Iteration 58/1000 | Loss: 0.00001244
Iteration 59/1000 | Loss: 0.00001243
Iteration 60/1000 | Loss: 0.00001243
Iteration 61/1000 | Loss: 0.00001242
Iteration 62/1000 | Loss: 0.00001242
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001240
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001238
Iteration 76/1000 | Loss: 0.00001237
Iteration 77/1000 | Loss: 0.00001237
Iteration 78/1000 | Loss: 0.00001236
Iteration 79/1000 | Loss: 0.00001236
Iteration 80/1000 | Loss: 0.00001236
Iteration 81/1000 | Loss: 0.00001236
Iteration 82/1000 | Loss: 0.00001235
Iteration 83/1000 | Loss: 0.00001235
Iteration 84/1000 | Loss: 0.00001235
Iteration 85/1000 | Loss: 0.00001235
Iteration 86/1000 | Loss: 0.00001235
Iteration 87/1000 | Loss: 0.00001234
Iteration 88/1000 | Loss: 0.00001234
Iteration 89/1000 | Loss: 0.00001233
Iteration 90/1000 | Loss: 0.00001233
Iteration 91/1000 | Loss: 0.00001233
Iteration 92/1000 | Loss: 0.00001232
Iteration 93/1000 | Loss: 0.00001232
Iteration 94/1000 | Loss: 0.00001232
Iteration 95/1000 | Loss: 0.00001232
Iteration 96/1000 | Loss: 0.00001232
Iteration 97/1000 | Loss: 0.00001232
Iteration 98/1000 | Loss: 0.00001232
Iteration 99/1000 | Loss: 0.00001232
Iteration 100/1000 | Loss: 0.00001232
Iteration 101/1000 | Loss: 0.00001232
Iteration 102/1000 | Loss: 0.00001231
Iteration 103/1000 | Loss: 0.00001231
Iteration 104/1000 | Loss: 0.00001230
Iteration 105/1000 | Loss: 0.00001230
Iteration 106/1000 | Loss: 0.00001229
Iteration 107/1000 | Loss: 0.00001229
Iteration 108/1000 | Loss: 0.00001229
Iteration 109/1000 | Loss: 0.00001228
Iteration 110/1000 | Loss: 0.00001228
Iteration 111/1000 | Loss: 0.00001228
Iteration 112/1000 | Loss: 0.00001228
Iteration 113/1000 | Loss: 0.00001227
Iteration 114/1000 | Loss: 0.00001227
Iteration 115/1000 | Loss: 0.00001227
Iteration 116/1000 | Loss: 0.00001227
Iteration 117/1000 | Loss: 0.00001227
Iteration 118/1000 | Loss: 0.00001226
Iteration 119/1000 | Loss: 0.00001226
Iteration 120/1000 | Loss: 0.00001226
Iteration 121/1000 | Loss: 0.00001226
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001225
Iteration 127/1000 | Loss: 0.00001225
Iteration 128/1000 | Loss: 0.00001225
Iteration 129/1000 | Loss: 0.00001225
Iteration 130/1000 | Loss: 0.00001224
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001224
Iteration 134/1000 | Loss: 0.00001224
Iteration 135/1000 | Loss: 0.00001224
Iteration 136/1000 | Loss: 0.00001224
Iteration 137/1000 | Loss: 0.00001224
Iteration 138/1000 | Loss: 0.00001224
Iteration 139/1000 | Loss: 0.00001224
Iteration 140/1000 | Loss: 0.00001223
Iteration 141/1000 | Loss: 0.00001223
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001223
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001223
Iteration 147/1000 | Loss: 0.00001223
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001223
Iteration 151/1000 | Loss: 0.00001222
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001222
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001222
Iteration 163/1000 | Loss: 0.00001222
Iteration 164/1000 | Loss: 0.00001222
Iteration 165/1000 | Loss: 0.00001222
Iteration 166/1000 | Loss: 0.00001222
Iteration 167/1000 | Loss: 0.00001222
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001221
Iteration 174/1000 | Loss: 0.00001221
Iteration 175/1000 | Loss: 0.00001221
Iteration 176/1000 | Loss: 0.00001221
Iteration 177/1000 | Loss: 0.00001221
Iteration 178/1000 | Loss: 0.00001221
Iteration 179/1000 | Loss: 0.00001221
Iteration 180/1000 | Loss: 0.00001221
Iteration 181/1000 | Loss: 0.00001221
Iteration 182/1000 | Loss: 0.00001221
Iteration 183/1000 | Loss: 0.00001221
Iteration 184/1000 | Loss: 0.00001221
Iteration 185/1000 | Loss: 0.00001221
Iteration 186/1000 | Loss: 0.00001221
Iteration 187/1000 | Loss: 0.00001221
Iteration 188/1000 | Loss: 0.00001221
Iteration 189/1000 | Loss: 0.00001221
Iteration 190/1000 | Loss: 0.00001221
Iteration 191/1000 | Loss: 0.00001221
Iteration 192/1000 | Loss: 0.00001221
Iteration 193/1000 | Loss: 0.00001221
Iteration 194/1000 | Loss: 0.00001221
Iteration 195/1000 | Loss: 0.00001221
Iteration 196/1000 | Loss: 0.00001221
Iteration 197/1000 | Loss: 0.00001221
Iteration 198/1000 | Loss: 0.00001221
Iteration 199/1000 | Loss: 0.00001221
Iteration 200/1000 | Loss: 0.00001221
Iteration 201/1000 | Loss: 0.00001221
Iteration 202/1000 | Loss: 0.00001221
Iteration 203/1000 | Loss: 0.00001221
Iteration 204/1000 | Loss: 0.00001221
Iteration 205/1000 | Loss: 0.00001221
Iteration 206/1000 | Loss: 0.00001221
Iteration 207/1000 | Loss: 0.00001221
Iteration 208/1000 | Loss: 0.00001221
Iteration 209/1000 | Loss: 0.00001221
Iteration 210/1000 | Loss: 0.00001221
Iteration 211/1000 | Loss: 0.00001221
Iteration 212/1000 | Loss: 0.00001221
Iteration 213/1000 | Loss: 0.00001221
Iteration 214/1000 | Loss: 0.00001221
Iteration 215/1000 | Loss: 0.00001221
Iteration 216/1000 | Loss: 0.00001221
Iteration 217/1000 | Loss: 0.00001221
Iteration 218/1000 | Loss: 0.00001221
Iteration 219/1000 | Loss: 0.00001221
Iteration 220/1000 | Loss: 0.00001221
Iteration 221/1000 | Loss: 0.00001221
Iteration 222/1000 | Loss: 0.00001221
Iteration 223/1000 | Loss: 0.00001221
Iteration 224/1000 | Loss: 0.00001221
Iteration 225/1000 | Loss: 0.00001221
Iteration 226/1000 | Loss: 0.00001221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.2212624824314844e-05, 1.2212624824314844e-05, 1.2212624824314844e-05, 1.2212624824314844e-05, 1.2212624824314844e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2212624824314844e-05

Optimization complete. Final v2v error: 3.042144298553467 mm

Highest mean error: 4.020569324493408 mm for frame 86

Lowest mean error: 2.6637649536132812 mm for frame 24

Saving results

Total time: 45.176756620407104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_25_it_4663/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_25_it_4663/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01020065
Iteration 2/25 | Loss: 0.00275268
Iteration 3/25 | Loss: 0.00230673
Iteration 4/25 | Loss: 0.00212489
Iteration 5/25 | Loss: 0.00203347
Iteration 6/25 | Loss: 0.00185994
Iteration 7/25 | Loss: 0.00158787
Iteration 8/25 | Loss: 0.00145963
Iteration 9/25 | Loss: 0.00142733
Iteration 10/25 | Loss: 0.00142849
Iteration 11/25 | Loss: 0.00141329
Iteration 12/25 | Loss: 0.00141308
Iteration 13/25 | Loss: 0.00140648
Iteration 14/25 | Loss: 0.00140543
Iteration 15/25 | Loss: 0.00140513
Iteration 16/25 | Loss: 0.00140504
Iteration 17/25 | Loss: 0.00140504
Iteration 18/25 | Loss: 0.00140503
Iteration 19/25 | Loss: 0.00140503
Iteration 20/25 | Loss: 0.00140503
Iteration 21/25 | Loss: 0.00140502
Iteration 22/25 | Loss: 0.00140502
Iteration 23/25 | Loss: 0.00140502
Iteration 24/25 | Loss: 0.00140502
Iteration 25/25 | Loss: 0.00140502

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13540637
Iteration 2/25 | Loss: 0.00407765
Iteration 3/25 | Loss: 0.00407765
Iteration 4/25 | Loss: 0.00407765
Iteration 5/25 | Loss: 0.00407765
Iteration 6/25 | Loss: 0.00407765
Iteration 7/25 | Loss: 0.00407765
Iteration 8/25 | Loss: 0.00407765
Iteration 9/25 | Loss: 0.00407765
Iteration 10/25 | Loss: 0.00407765
Iteration 11/25 | Loss: 0.00407765
Iteration 12/25 | Loss: 0.00407765
Iteration 13/25 | Loss: 0.00407765
Iteration 14/25 | Loss: 0.00407765
Iteration 15/25 | Loss: 0.00407765
Iteration 16/25 | Loss: 0.00407765
Iteration 17/25 | Loss: 0.00407765
Iteration 18/25 | Loss: 0.00407765
Iteration 19/25 | Loss: 0.00407765
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.004077645018696785, 0.004077645018696785, 0.004077645018696785, 0.004077645018696785, 0.004077645018696785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004077645018696785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00407765
Iteration 2/1000 | Loss: 0.00004271
Iteration 3/1000 | Loss: 0.00003125
Iteration 4/1000 | Loss: 0.00002833
Iteration 5/1000 | Loss: 0.00002695
Iteration 6/1000 | Loss: 0.00002560
Iteration 7/1000 | Loss: 0.00002459
Iteration 8/1000 | Loss: 0.00002383
Iteration 9/1000 | Loss: 0.00002335
Iteration 10/1000 | Loss: 0.00002281
Iteration 11/1000 | Loss: 0.00002244
Iteration 12/1000 | Loss: 0.00002207
Iteration 13/1000 | Loss: 0.00002182
Iteration 14/1000 | Loss: 0.00002170
Iteration 15/1000 | Loss: 0.00002152
Iteration 16/1000 | Loss: 0.00002147
Iteration 17/1000 | Loss: 0.00002147
Iteration 18/1000 | Loss: 0.00002146
Iteration 19/1000 | Loss: 0.00002143
Iteration 20/1000 | Loss: 0.00002142
Iteration 21/1000 | Loss: 0.00002139
Iteration 22/1000 | Loss: 0.00002138
Iteration 23/1000 | Loss: 0.00002138
Iteration 24/1000 | Loss: 0.00002137
Iteration 25/1000 | Loss: 0.00002133
Iteration 26/1000 | Loss: 0.00002131
Iteration 27/1000 | Loss: 0.00002131
Iteration 28/1000 | Loss: 0.00002129
Iteration 29/1000 | Loss: 0.00002129
Iteration 30/1000 | Loss: 0.00002128
Iteration 31/1000 | Loss: 0.00002128
Iteration 32/1000 | Loss: 0.00002128
Iteration 33/1000 | Loss: 0.00002128
Iteration 34/1000 | Loss: 0.00002128
Iteration 35/1000 | Loss: 0.00002128
Iteration 36/1000 | Loss: 0.00002128
Iteration 37/1000 | Loss: 0.00002128
Iteration 38/1000 | Loss: 0.00002128
Iteration 39/1000 | Loss: 0.00002127
Iteration 40/1000 | Loss: 0.00002127
Iteration 41/1000 | Loss: 0.00002127
Iteration 42/1000 | Loss: 0.00002126
Iteration 43/1000 | Loss: 0.00002126
Iteration 44/1000 | Loss: 0.00002126
Iteration 45/1000 | Loss: 0.00002125
Iteration 46/1000 | Loss: 0.00002125
Iteration 47/1000 | Loss: 0.00002124
Iteration 48/1000 | Loss: 0.00002124
Iteration 49/1000 | Loss: 0.00002123
Iteration 50/1000 | Loss: 0.00002123
Iteration 51/1000 | Loss: 0.00002123
Iteration 52/1000 | Loss: 0.00002123
Iteration 53/1000 | Loss: 0.00002123
Iteration 54/1000 | Loss: 0.00002123
Iteration 55/1000 | Loss: 0.00002122
Iteration 56/1000 | Loss: 0.00002122
Iteration 57/1000 | Loss: 0.00002122
Iteration 58/1000 | Loss: 0.00002122
Iteration 59/1000 | Loss: 0.00002122
Iteration 60/1000 | Loss: 0.00002122
Iteration 61/1000 | Loss: 0.00002122
Iteration 62/1000 | Loss: 0.00002122
Iteration 63/1000 | Loss: 0.00002121
Iteration 64/1000 | Loss: 0.00002121
Iteration 65/1000 | Loss: 0.00002121
Iteration 66/1000 | Loss: 0.00002121
Iteration 67/1000 | Loss: 0.00002121
Iteration 68/1000 | Loss: 0.00002121
Iteration 69/1000 | Loss: 0.00002121
Iteration 70/1000 | Loss: 0.00002121
Iteration 71/1000 | Loss: 0.00002120
Iteration 72/1000 | Loss: 0.00002120
Iteration 73/1000 | Loss: 0.00002120
Iteration 74/1000 | Loss: 0.00002120
Iteration 75/1000 | Loss: 0.00002120
Iteration 76/1000 | Loss: 0.00002119
Iteration 77/1000 | Loss: 0.00002119
Iteration 78/1000 | Loss: 0.00002119
Iteration 79/1000 | Loss: 0.00002119
Iteration 80/1000 | Loss: 0.00002119
Iteration 81/1000 | Loss: 0.00002118
Iteration 82/1000 | Loss: 0.00002118
Iteration 83/1000 | Loss: 0.00002118
Iteration 84/1000 | Loss: 0.00002117
Iteration 85/1000 | Loss: 0.00002117
Iteration 86/1000 | Loss: 0.00002117
Iteration 87/1000 | Loss: 0.00002117
Iteration 88/1000 | Loss: 0.00002117
Iteration 89/1000 | Loss: 0.00002117
Iteration 90/1000 | Loss: 0.00002116
Iteration 91/1000 | Loss: 0.00002116
Iteration 92/1000 | Loss: 0.00002116
Iteration 93/1000 | Loss: 0.00002116
Iteration 94/1000 | Loss: 0.00002116
Iteration 95/1000 | Loss: 0.00002116
Iteration 96/1000 | Loss: 0.00002116
Iteration 97/1000 | Loss: 0.00002115
Iteration 98/1000 | Loss: 0.00002115
Iteration 99/1000 | Loss: 0.00002115
Iteration 100/1000 | Loss: 0.00002115
Iteration 101/1000 | Loss: 0.00002115
Iteration 102/1000 | Loss: 0.00002115
Iteration 103/1000 | Loss: 0.00002115
Iteration 104/1000 | Loss: 0.00002115
Iteration 105/1000 | Loss: 0.00002115
Iteration 106/1000 | Loss: 0.00002115
Iteration 107/1000 | Loss: 0.00002115
Iteration 108/1000 | Loss: 0.00002115
Iteration 109/1000 | Loss: 0.00002115
Iteration 110/1000 | Loss: 0.00002115
Iteration 111/1000 | Loss: 0.00002115
Iteration 112/1000 | Loss: 0.00002115
Iteration 113/1000 | Loss: 0.00002115
Iteration 114/1000 | Loss: 0.00002115
Iteration 115/1000 | Loss: 0.00002115
Iteration 116/1000 | Loss: 0.00002115
Iteration 117/1000 | Loss: 0.00002115
Iteration 118/1000 | Loss: 0.00002115
Iteration 119/1000 | Loss: 0.00002115
Iteration 120/1000 | Loss: 0.00002115
Iteration 121/1000 | Loss: 0.00002115
Iteration 122/1000 | Loss: 0.00002115
Iteration 123/1000 | Loss: 0.00002115
Iteration 124/1000 | Loss: 0.00002115
Iteration 125/1000 | Loss: 0.00002115
Iteration 126/1000 | Loss: 0.00002115
Iteration 127/1000 | Loss: 0.00002115
Iteration 128/1000 | Loss: 0.00002115
Iteration 129/1000 | Loss: 0.00002115
Iteration 130/1000 | Loss: 0.00002115
Iteration 131/1000 | Loss: 0.00002115
Iteration 132/1000 | Loss: 0.00002115
Iteration 133/1000 | Loss: 0.00002115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.1153693523956463e-05, 2.1153693523956463e-05, 2.1153693523956463e-05, 2.1153693523956463e-05, 2.1153693523956463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1153693523956463e-05

Optimization complete. Final v2v error: 4.036163806915283 mm

Highest mean error: 4.3477630615234375 mm for frame 145

Lowest mean error: 3.382209539413452 mm for frame 3

Saving results

Total time: 57.11889719963074
