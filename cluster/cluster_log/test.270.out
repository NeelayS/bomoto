Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=270, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 15120-15175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00570971
Iteration 2/25 | Loss: 0.00122500
Iteration 3/25 | Loss: 0.00106069
Iteration 4/25 | Loss: 0.00102720
Iteration 5/25 | Loss: 0.00102048
Iteration 6/25 | Loss: 0.00101868
Iteration 7/25 | Loss: 0.00101862
Iteration 8/25 | Loss: 0.00101862
Iteration 9/25 | Loss: 0.00101862
Iteration 10/25 | Loss: 0.00101862
Iteration 11/25 | Loss: 0.00101862
Iteration 12/25 | Loss: 0.00101862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001018615672364831, 0.001018615672364831, 0.001018615672364831, 0.001018615672364831, 0.001018615672364831]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001018615672364831

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.44314337
Iteration 2/25 | Loss: 0.00235246
Iteration 3/25 | Loss: 0.00235246
Iteration 4/25 | Loss: 0.00235245
Iteration 5/25 | Loss: 0.00235245
Iteration 6/25 | Loss: 0.00235245
Iteration 7/25 | Loss: 0.00235245
Iteration 8/25 | Loss: 0.00235245
Iteration 9/25 | Loss: 0.00235245
Iteration 10/25 | Loss: 0.00235245
Iteration 11/25 | Loss: 0.00235245
Iteration 12/25 | Loss: 0.00235245
Iteration 13/25 | Loss: 0.00235245
Iteration 14/25 | Loss: 0.00235245
Iteration 15/25 | Loss: 0.00235245
Iteration 16/25 | Loss: 0.00235245
Iteration 17/25 | Loss: 0.00235245
Iteration 18/25 | Loss: 0.00235245
Iteration 19/25 | Loss: 0.00235245
Iteration 20/25 | Loss: 0.00235245
Iteration 21/25 | Loss: 0.00235245
Iteration 22/25 | Loss: 0.00235245
Iteration 23/25 | Loss: 0.00235245
Iteration 24/25 | Loss: 0.00235245
Iteration 25/25 | Loss: 0.00235245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00235245
Iteration 2/1000 | Loss: 0.00004811
Iteration 3/1000 | Loss: 0.00003136
Iteration 4/1000 | Loss: 0.00002494
Iteration 5/1000 | Loss: 0.00002344
Iteration 6/1000 | Loss: 0.00002235
Iteration 7/1000 | Loss: 0.00002154
Iteration 8/1000 | Loss: 0.00002105
Iteration 9/1000 | Loss: 0.00002061
Iteration 10/1000 | Loss: 0.00002036
Iteration 11/1000 | Loss: 0.00002024
Iteration 12/1000 | Loss: 0.00002007
Iteration 13/1000 | Loss: 0.00001994
Iteration 14/1000 | Loss: 0.00001987
Iteration 15/1000 | Loss: 0.00001987
Iteration 16/1000 | Loss: 0.00001985
Iteration 17/1000 | Loss: 0.00001984
Iteration 18/1000 | Loss: 0.00001984
Iteration 19/1000 | Loss: 0.00001984
Iteration 20/1000 | Loss: 0.00001983
Iteration 21/1000 | Loss: 0.00001983
Iteration 22/1000 | Loss: 0.00001983
Iteration 23/1000 | Loss: 0.00001983
Iteration 24/1000 | Loss: 0.00001983
Iteration 25/1000 | Loss: 0.00001982
Iteration 26/1000 | Loss: 0.00001982
Iteration 27/1000 | Loss: 0.00001982
Iteration 28/1000 | Loss: 0.00001982
Iteration 29/1000 | Loss: 0.00001981
Iteration 30/1000 | Loss: 0.00001981
Iteration 31/1000 | Loss: 0.00001981
Iteration 32/1000 | Loss: 0.00001981
Iteration 33/1000 | Loss: 0.00001981
Iteration 34/1000 | Loss: 0.00001981
Iteration 35/1000 | Loss: 0.00001980
Iteration 36/1000 | Loss: 0.00001980
Iteration 37/1000 | Loss: 0.00001980
Iteration 38/1000 | Loss: 0.00001980
Iteration 39/1000 | Loss: 0.00001979
Iteration 40/1000 | Loss: 0.00001979
Iteration 41/1000 | Loss: 0.00001979
Iteration 42/1000 | Loss: 0.00001979
Iteration 43/1000 | Loss: 0.00001979
Iteration 44/1000 | Loss: 0.00001979
Iteration 45/1000 | Loss: 0.00001979
Iteration 46/1000 | Loss: 0.00001979
Iteration 47/1000 | Loss: 0.00001979
Iteration 48/1000 | Loss: 0.00001979
Iteration 49/1000 | Loss: 0.00001979
Iteration 50/1000 | Loss: 0.00001978
Iteration 51/1000 | Loss: 0.00001978
Iteration 52/1000 | Loss: 0.00001978
Iteration 53/1000 | Loss: 0.00001978
Iteration 54/1000 | Loss: 0.00001978
Iteration 55/1000 | Loss: 0.00001978
Iteration 56/1000 | Loss: 0.00001978
Iteration 57/1000 | Loss: 0.00001978
Iteration 58/1000 | Loss: 0.00001978
Iteration 59/1000 | Loss: 0.00001977
Iteration 60/1000 | Loss: 0.00001977
Iteration 61/1000 | Loss: 0.00001977
Iteration 62/1000 | Loss: 0.00001977
Iteration 63/1000 | Loss: 0.00001977
Iteration 64/1000 | Loss: 0.00001977
Iteration 65/1000 | Loss: 0.00001977
Iteration 66/1000 | Loss: 0.00001977
Iteration 67/1000 | Loss: 0.00001977
Iteration 68/1000 | Loss: 0.00001977
Iteration 69/1000 | Loss: 0.00001977
Iteration 70/1000 | Loss: 0.00001977
Iteration 71/1000 | Loss: 0.00001977
Iteration 72/1000 | Loss: 0.00001977
Iteration 73/1000 | Loss: 0.00001977
Iteration 74/1000 | Loss: 0.00001977
Iteration 75/1000 | Loss: 0.00001977
Iteration 76/1000 | Loss: 0.00001977
Iteration 77/1000 | Loss: 0.00001977
Iteration 78/1000 | Loss: 0.00001977
Iteration 79/1000 | Loss: 0.00001977
Iteration 80/1000 | Loss: 0.00001977
Iteration 81/1000 | Loss: 0.00001977
Iteration 82/1000 | Loss: 0.00001977
Iteration 83/1000 | Loss: 0.00001977
Iteration 84/1000 | Loss: 0.00001977
Iteration 85/1000 | Loss: 0.00001977
Iteration 86/1000 | Loss: 0.00001977
Iteration 87/1000 | Loss: 0.00001977
Iteration 88/1000 | Loss: 0.00001977
Iteration 89/1000 | Loss: 0.00001977
Iteration 90/1000 | Loss: 0.00001977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 90. Stopping optimization.
Last 5 losses: [1.976915700652171e-05, 1.976915700652171e-05, 1.976915700652171e-05, 1.976915700652171e-05, 1.976915700652171e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.976915700652171e-05

Optimization complete. Final v2v error: 3.8137927055358887 mm

Highest mean error: 4.162398338317871 mm for frame 179

Lowest mean error: 3.527693510055542 mm for frame 66

Saving results

Total time: 38.8491108417511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945201
Iteration 2/25 | Loss: 0.00111930
Iteration 3/25 | Loss: 0.00100233
Iteration 4/25 | Loss: 0.00096904
Iteration 5/25 | Loss: 0.00096022
Iteration 6/25 | Loss: 0.00095851
Iteration 7/25 | Loss: 0.00095821
Iteration 8/25 | Loss: 0.00095821
Iteration 9/25 | Loss: 0.00095821
Iteration 10/25 | Loss: 0.00095821
Iteration 11/25 | Loss: 0.00095821
Iteration 12/25 | Loss: 0.00095821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.000958206772338599, 0.000958206772338599, 0.000958206772338599, 0.000958206772338599, 0.000958206772338599]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000958206772338599

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.74164677
Iteration 2/25 | Loss: 0.00194446
Iteration 3/25 | Loss: 0.00194445
Iteration 4/25 | Loss: 0.00194445
Iteration 5/25 | Loss: 0.00194445
Iteration 6/25 | Loss: 0.00194445
Iteration 7/25 | Loss: 0.00194445
Iteration 8/25 | Loss: 0.00194445
Iteration 9/25 | Loss: 0.00194445
Iteration 10/25 | Loss: 0.00194445
Iteration 11/25 | Loss: 0.00194445
Iteration 12/25 | Loss: 0.00194445
Iteration 13/25 | Loss: 0.00194445
Iteration 14/25 | Loss: 0.00194445
Iteration 15/25 | Loss: 0.00194445
Iteration 16/25 | Loss: 0.00194445
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001944450894370675, 0.001944450894370675, 0.001944450894370675, 0.001944450894370675, 0.001944450894370675]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001944450894370675

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194445
Iteration 2/1000 | Loss: 0.00003455
Iteration 3/1000 | Loss: 0.00002512
Iteration 4/1000 | Loss: 0.00002358
Iteration 5/1000 | Loss: 0.00002234
Iteration 6/1000 | Loss: 0.00002159
Iteration 7/1000 | Loss: 0.00002100
Iteration 8/1000 | Loss: 0.00002080
Iteration 9/1000 | Loss: 0.00002074
Iteration 10/1000 | Loss: 0.00002070
Iteration 11/1000 | Loss: 0.00002069
Iteration 12/1000 | Loss: 0.00002067
Iteration 13/1000 | Loss: 0.00002065
Iteration 14/1000 | Loss: 0.00002064
Iteration 15/1000 | Loss: 0.00002063
Iteration 16/1000 | Loss: 0.00002062
Iteration 17/1000 | Loss: 0.00002061
Iteration 18/1000 | Loss: 0.00002061
Iteration 19/1000 | Loss: 0.00002060
Iteration 20/1000 | Loss: 0.00002060
Iteration 21/1000 | Loss: 0.00002060
Iteration 22/1000 | Loss: 0.00002058
Iteration 23/1000 | Loss: 0.00002058
Iteration 24/1000 | Loss: 0.00002058
Iteration 25/1000 | Loss: 0.00002058
Iteration 26/1000 | Loss: 0.00002057
Iteration 27/1000 | Loss: 0.00002057
Iteration 28/1000 | Loss: 0.00002056
Iteration 29/1000 | Loss: 0.00002055
Iteration 30/1000 | Loss: 0.00002055
Iteration 31/1000 | Loss: 0.00002054
Iteration 32/1000 | Loss: 0.00002053
Iteration 33/1000 | Loss: 0.00002053
Iteration 34/1000 | Loss: 0.00002053
Iteration 35/1000 | Loss: 0.00002052
Iteration 36/1000 | Loss: 0.00002052
Iteration 37/1000 | Loss: 0.00002052
Iteration 38/1000 | Loss: 0.00002051
Iteration 39/1000 | Loss: 0.00002051
Iteration 40/1000 | Loss: 0.00002050
Iteration 41/1000 | Loss: 0.00002049
Iteration 42/1000 | Loss: 0.00002049
Iteration 43/1000 | Loss: 0.00002049
Iteration 44/1000 | Loss: 0.00002049
Iteration 45/1000 | Loss: 0.00002049
Iteration 46/1000 | Loss: 0.00002048
Iteration 47/1000 | Loss: 0.00002048
Iteration 48/1000 | Loss: 0.00002048
Iteration 49/1000 | Loss: 0.00002048
Iteration 50/1000 | Loss: 0.00002048
Iteration 51/1000 | Loss: 0.00002048
Iteration 52/1000 | Loss: 0.00002048
Iteration 53/1000 | Loss: 0.00002048
Iteration 54/1000 | Loss: 0.00002048
Iteration 55/1000 | Loss: 0.00002048
Iteration 56/1000 | Loss: 0.00002048
Iteration 57/1000 | Loss: 0.00002048
Iteration 58/1000 | Loss: 0.00002048
Iteration 59/1000 | Loss: 0.00002048
Iteration 60/1000 | Loss: 0.00002048
Iteration 61/1000 | Loss: 0.00002048
Iteration 62/1000 | Loss: 0.00002046
Iteration 63/1000 | Loss: 0.00002046
Iteration 64/1000 | Loss: 0.00002046
Iteration 65/1000 | Loss: 0.00002046
Iteration 66/1000 | Loss: 0.00002045
Iteration 67/1000 | Loss: 0.00002045
Iteration 68/1000 | Loss: 0.00002045
Iteration 69/1000 | Loss: 0.00002045
Iteration 70/1000 | Loss: 0.00002045
Iteration 71/1000 | Loss: 0.00002045
Iteration 72/1000 | Loss: 0.00002045
Iteration 73/1000 | Loss: 0.00002045
Iteration 74/1000 | Loss: 0.00002045
Iteration 75/1000 | Loss: 0.00002045
Iteration 76/1000 | Loss: 0.00002044
Iteration 77/1000 | Loss: 0.00002044
Iteration 78/1000 | Loss: 0.00002043
Iteration 79/1000 | Loss: 0.00002043
Iteration 80/1000 | Loss: 0.00002043
Iteration 81/1000 | Loss: 0.00002043
Iteration 82/1000 | Loss: 0.00002043
Iteration 83/1000 | Loss: 0.00002043
Iteration 84/1000 | Loss: 0.00002042
Iteration 85/1000 | Loss: 0.00002042
Iteration 86/1000 | Loss: 0.00002042
Iteration 87/1000 | Loss: 0.00002042
Iteration 88/1000 | Loss: 0.00002041
Iteration 89/1000 | Loss: 0.00002041
Iteration 90/1000 | Loss: 0.00002041
Iteration 91/1000 | Loss: 0.00002041
Iteration 92/1000 | Loss: 0.00002041
Iteration 93/1000 | Loss: 0.00002040
Iteration 94/1000 | Loss: 0.00002040
Iteration 95/1000 | Loss: 0.00002040
Iteration 96/1000 | Loss: 0.00002040
Iteration 97/1000 | Loss: 0.00002040
Iteration 98/1000 | Loss: 0.00002040
Iteration 99/1000 | Loss: 0.00002040
Iteration 100/1000 | Loss: 0.00002040
Iteration 101/1000 | Loss: 0.00002040
Iteration 102/1000 | Loss: 0.00002040
Iteration 103/1000 | Loss: 0.00002040
Iteration 104/1000 | Loss: 0.00002040
Iteration 105/1000 | Loss: 0.00002040
Iteration 106/1000 | Loss: 0.00002040
Iteration 107/1000 | Loss: 0.00002040
Iteration 108/1000 | Loss: 0.00002039
Iteration 109/1000 | Loss: 0.00002039
Iteration 110/1000 | Loss: 0.00002039
Iteration 111/1000 | Loss: 0.00002039
Iteration 112/1000 | Loss: 0.00002038
Iteration 113/1000 | Loss: 0.00002038
Iteration 114/1000 | Loss: 0.00002038
Iteration 115/1000 | Loss: 0.00002037
Iteration 116/1000 | Loss: 0.00002037
Iteration 117/1000 | Loss: 0.00002037
Iteration 118/1000 | Loss: 0.00002037
Iteration 119/1000 | Loss: 0.00002036
Iteration 120/1000 | Loss: 0.00002036
Iteration 121/1000 | Loss: 0.00002035
Iteration 122/1000 | Loss: 0.00002034
Iteration 123/1000 | Loss: 0.00002034
Iteration 124/1000 | Loss: 0.00002034
Iteration 125/1000 | Loss: 0.00002034
Iteration 126/1000 | Loss: 0.00002033
Iteration 127/1000 | Loss: 0.00002033
Iteration 128/1000 | Loss: 0.00002033
Iteration 129/1000 | Loss: 0.00002033
Iteration 130/1000 | Loss: 0.00002033
Iteration 131/1000 | Loss: 0.00002033
Iteration 132/1000 | Loss: 0.00002032
Iteration 133/1000 | Loss: 0.00002032
Iteration 134/1000 | Loss: 0.00002032
Iteration 135/1000 | Loss: 0.00002032
Iteration 136/1000 | Loss: 0.00002032
Iteration 137/1000 | Loss: 0.00002032
Iteration 138/1000 | Loss: 0.00002032
Iteration 139/1000 | Loss: 0.00002031
Iteration 140/1000 | Loss: 0.00002031
Iteration 141/1000 | Loss: 0.00002031
Iteration 142/1000 | Loss: 0.00002031
Iteration 143/1000 | Loss: 0.00002031
Iteration 144/1000 | Loss: 0.00002030
Iteration 145/1000 | Loss: 0.00002030
Iteration 146/1000 | Loss: 0.00002030
Iteration 147/1000 | Loss: 0.00002030
Iteration 148/1000 | Loss: 0.00002030
Iteration 149/1000 | Loss: 0.00002030
Iteration 150/1000 | Loss: 0.00002030
Iteration 151/1000 | Loss: 0.00002030
Iteration 152/1000 | Loss: 0.00002030
Iteration 153/1000 | Loss: 0.00002030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.029797906288877e-05, 2.029797906288877e-05, 2.029797906288877e-05, 2.029797906288877e-05, 2.029797906288877e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.029797906288877e-05

Optimization complete. Final v2v error: 3.876582384109497 mm

Highest mean error: 4.365759372711182 mm for frame 180

Lowest mean error: 3.512397289276123 mm for frame 63

Saving results

Total time: 34.0344352722168
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01140376
Iteration 2/25 | Loss: 0.00160269
Iteration 3/25 | Loss: 0.00142433
Iteration 4/25 | Loss: 0.00101391
Iteration 5/25 | Loss: 0.00101370
Iteration 6/25 | Loss: 0.00099555
Iteration 7/25 | Loss: 0.00094241
Iteration 8/25 | Loss: 0.00094149
Iteration 9/25 | Loss: 0.00091846
Iteration 10/25 | Loss: 0.00089665
Iteration 11/25 | Loss: 0.00089931
Iteration 12/25 | Loss: 0.00088523
Iteration 13/25 | Loss: 0.00088388
Iteration 14/25 | Loss: 0.00087611
Iteration 15/25 | Loss: 0.00087564
Iteration 16/25 | Loss: 0.00087503
Iteration 17/25 | Loss: 0.00087474
Iteration 18/25 | Loss: 0.00087442
Iteration 19/25 | Loss: 0.00087548
Iteration 20/25 | Loss: 0.00087447
Iteration 21/25 | Loss: 0.00087504
Iteration 22/25 | Loss: 0.00087471
Iteration 23/25 | Loss: 0.00087505
Iteration 24/25 | Loss: 0.00087466
Iteration 25/25 | Loss: 0.00087484

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.82325220
Iteration 2/25 | Loss: 0.00205638
Iteration 3/25 | Loss: 0.00205638
Iteration 4/25 | Loss: 0.00205638
Iteration 5/25 | Loss: 0.00205637
Iteration 6/25 | Loss: 0.00205637
Iteration 7/25 | Loss: 0.00205637
Iteration 8/25 | Loss: 0.00205637
Iteration 9/25 | Loss: 0.00205637
Iteration 10/25 | Loss: 0.00205637
Iteration 11/25 | Loss: 0.00205637
Iteration 12/25 | Loss: 0.00205637
Iteration 13/25 | Loss: 0.00205637
Iteration 14/25 | Loss: 0.00205637
Iteration 15/25 | Loss: 0.00205637
Iteration 16/25 | Loss: 0.00205637
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002056373283267021, 0.002056373283267021, 0.002056373283267021, 0.002056373283267021, 0.002056373283267021]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002056373283267021

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00205637
Iteration 2/1000 | Loss: 0.00003160
Iteration 3/1000 | Loss: 0.00011226
Iteration 4/1000 | Loss: 0.00002059
Iteration 5/1000 | Loss: 0.00001976
Iteration 6/1000 | Loss: 0.00001909
Iteration 7/1000 | Loss: 0.00014463
Iteration 8/1000 | Loss: 0.00029784
Iteration 9/1000 | Loss: 0.00067539
Iteration 10/1000 | Loss: 0.00002115
Iteration 11/1000 | Loss: 0.00001822
Iteration 12/1000 | Loss: 0.00001789
Iteration 13/1000 | Loss: 0.00001775
Iteration 14/1000 | Loss: 0.00001770
Iteration 15/1000 | Loss: 0.00001769
Iteration 16/1000 | Loss: 0.00001768
Iteration 17/1000 | Loss: 0.00001766
Iteration 18/1000 | Loss: 0.00001763
Iteration 19/1000 | Loss: 0.00001763
Iteration 20/1000 | Loss: 0.00001762
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001761
Iteration 23/1000 | Loss: 0.00001760
Iteration 24/1000 | Loss: 0.00001760
Iteration 25/1000 | Loss: 0.00001760
Iteration 26/1000 | Loss: 0.00014645
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001758
Iteration 29/1000 | Loss: 0.00011653
Iteration 30/1000 | Loss: 0.00001766
Iteration 31/1000 | Loss: 0.00001756
Iteration 32/1000 | Loss: 0.00001756
Iteration 33/1000 | Loss: 0.00001755
Iteration 34/1000 | Loss: 0.00001755
Iteration 35/1000 | Loss: 0.00001755
Iteration 36/1000 | Loss: 0.00001754
Iteration 37/1000 | Loss: 0.00001754
Iteration 38/1000 | Loss: 0.00001752
Iteration 39/1000 | Loss: 0.00001752
Iteration 40/1000 | Loss: 0.00001748
Iteration 41/1000 | Loss: 0.00001748
Iteration 42/1000 | Loss: 0.00001748
Iteration 43/1000 | Loss: 0.00001748
Iteration 44/1000 | Loss: 0.00001748
Iteration 45/1000 | Loss: 0.00001748
Iteration 46/1000 | Loss: 0.00001748
Iteration 47/1000 | Loss: 0.00001747
Iteration 48/1000 | Loss: 0.00001747
Iteration 49/1000 | Loss: 0.00001747
Iteration 50/1000 | Loss: 0.00001747
Iteration 51/1000 | Loss: 0.00001747
Iteration 52/1000 | Loss: 0.00001747
Iteration 53/1000 | Loss: 0.00001747
Iteration 54/1000 | Loss: 0.00001746
Iteration 55/1000 | Loss: 0.00001746
Iteration 56/1000 | Loss: 0.00001746
Iteration 57/1000 | Loss: 0.00025404
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001748
Iteration 60/1000 | Loss: 0.00001745
Iteration 61/1000 | Loss: 0.00001744
Iteration 62/1000 | Loss: 0.00001744
Iteration 63/1000 | Loss: 0.00001743
Iteration 64/1000 | Loss: 0.00001743
Iteration 65/1000 | Loss: 0.00001742
Iteration 66/1000 | Loss: 0.00001742
Iteration 67/1000 | Loss: 0.00001742
Iteration 68/1000 | Loss: 0.00001741
Iteration 69/1000 | Loss: 0.00009177
Iteration 70/1000 | Loss: 0.00003279
Iteration 71/1000 | Loss: 0.00001816
Iteration 72/1000 | Loss: 0.00006407
Iteration 73/1000 | Loss: 0.00001964
Iteration 74/1000 | Loss: 0.00002249
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00019715
Iteration 77/1000 | Loss: 0.00008472
Iteration 78/1000 | Loss: 0.00005432
Iteration 79/1000 | Loss: 0.00001766
Iteration 80/1000 | Loss: 0.00001740
Iteration 81/1000 | Loss: 0.00001736
Iteration 82/1000 | Loss: 0.00001736
Iteration 83/1000 | Loss: 0.00001735
Iteration 84/1000 | Loss: 0.00001735
Iteration 85/1000 | Loss: 0.00001735
Iteration 86/1000 | Loss: 0.00001735
Iteration 87/1000 | Loss: 0.00001734
Iteration 88/1000 | Loss: 0.00001734
Iteration 89/1000 | Loss: 0.00001734
Iteration 90/1000 | Loss: 0.00001734
Iteration 91/1000 | Loss: 0.00001734
Iteration 92/1000 | Loss: 0.00001734
Iteration 93/1000 | Loss: 0.00001734
Iteration 94/1000 | Loss: 0.00001734
Iteration 95/1000 | Loss: 0.00001733
Iteration 96/1000 | Loss: 0.00001733
Iteration 97/1000 | Loss: 0.00001733
Iteration 98/1000 | Loss: 0.00001733
Iteration 99/1000 | Loss: 0.00001733
Iteration 100/1000 | Loss: 0.00001733
Iteration 101/1000 | Loss: 0.00001733
Iteration 102/1000 | Loss: 0.00001733
Iteration 103/1000 | Loss: 0.00001733
Iteration 104/1000 | Loss: 0.00001733
Iteration 105/1000 | Loss: 0.00001732
Iteration 106/1000 | Loss: 0.00001732
Iteration 107/1000 | Loss: 0.00001732
Iteration 108/1000 | Loss: 0.00001732
Iteration 109/1000 | Loss: 0.00001732
Iteration 110/1000 | Loss: 0.00001732
Iteration 111/1000 | Loss: 0.00001732
Iteration 112/1000 | Loss: 0.00001732
Iteration 113/1000 | Loss: 0.00001732
Iteration 114/1000 | Loss: 0.00001732
Iteration 115/1000 | Loss: 0.00001732
Iteration 116/1000 | Loss: 0.00001732
Iteration 117/1000 | Loss: 0.00001732
Iteration 118/1000 | Loss: 0.00001732
Iteration 119/1000 | Loss: 0.00001732
Iteration 120/1000 | Loss: 0.00001732
Iteration 121/1000 | Loss: 0.00001732
Iteration 122/1000 | Loss: 0.00001732
Iteration 123/1000 | Loss: 0.00001732
Iteration 124/1000 | Loss: 0.00001732
Iteration 125/1000 | Loss: 0.00001732
Iteration 126/1000 | Loss: 0.00001732
Iteration 127/1000 | Loss: 0.00001732
Iteration 128/1000 | Loss: 0.00001732
Iteration 129/1000 | Loss: 0.00001732
Iteration 130/1000 | Loss: 0.00001732
Iteration 131/1000 | Loss: 0.00001732
Iteration 132/1000 | Loss: 0.00001732
Iteration 133/1000 | Loss: 0.00001732
Iteration 134/1000 | Loss: 0.00001732
Iteration 135/1000 | Loss: 0.00001732
Iteration 136/1000 | Loss: 0.00001732
Iteration 137/1000 | Loss: 0.00001732
Iteration 138/1000 | Loss: 0.00001732
Iteration 139/1000 | Loss: 0.00001732
Iteration 140/1000 | Loss: 0.00001732
Iteration 141/1000 | Loss: 0.00001732
Iteration 142/1000 | Loss: 0.00001732
Iteration 143/1000 | Loss: 0.00001732
Iteration 144/1000 | Loss: 0.00001732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 144. Stopping optimization.
Last 5 losses: [1.732312739477493e-05, 1.732312739477493e-05, 1.732312739477493e-05, 1.732312739477493e-05, 1.732312739477493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.732312739477493e-05

Optimization complete. Final v2v error: 3.500805616378784 mm

Highest mean error: 9.652586936950684 mm for frame 98

Lowest mean error: 3.185992956161499 mm for frame 128

Saving results

Total time: 97.59059166908264
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014013
Iteration 2/25 | Loss: 0.00146180
Iteration 3/25 | Loss: 0.00123622
Iteration 4/25 | Loss: 0.00106365
Iteration 5/25 | Loss: 0.00105632
Iteration 6/25 | Loss: 0.00103141
Iteration 7/25 | Loss: 0.00102251
Iteration 8/25 | Loss: 0.00103084
Iteration 9/25 | Loss: 0.00101869
Iteration 10/25 | Loss: 0.00101861
Iteration 11/25 | Loss: 0.00101861
Iteration 12/25 | Loss: 0.00101861
Iteration 13/25 | Loss: 0.00101861
Iteration 14/25 | Loss: 0.00101861
Iteration 15/25 | Loss: 0.00101861
Iteration 16/25 | Loss: 0.00101861
Iteration 17/25 | Loss: 0.00101861
Iteration 18/25 | Loss: 0.00101861
Iteration 19/25 | Loss: 0.00101861
Iteration 20/25 | Loss: 0.00101861
Iteration 21/25 | Loss: 0.00101861
Iteration 22/25 | Loss: 0.00101861
Iteration 23/25 | Loss: 0.00101861
Iteration 24/25 | Loss: 0.00101861
Iteration 25/25 | Loss: 0.00101861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72167861
Iteration 2/25 | Loss: 0.00260660
Iteration 3/25 | Loss: 0.00255353
Iteration 4/25 | Loss: 0.00255353
Iteration 5/25 | Loss: 0.00255353
Iteration 6/25 | Loss: 0.00255353
Iteration 7/25 | Loss: 0.00255353
Iteration 8/25 | Loss: 0.00255353
Iteration 9/25 | Loss: 0.00255353
Iteration 10/25 | Loss: 0.00255353
Iteration 11/25 | Loss: 0.00255353
Iteration 12/25 | Loss: 0.00255353
Iteration 13/25 | Loss: 0.00255353
Iteration 14/25 | Loss: 0.00255353
Iteration 15/25 | Loss: 0.00255353
Iteration 16/25 | Loss: 0.00255353
Iteration 17/25 | Loss: 0.00255353
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0025535321328788996, 0.0025535321328788996, 0.0025535321328788996, 0.0025535321328788996, 0.0025535321328788996]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025535321328788996

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00255353
Iteration 2/1000 | Loss: 0.00005044
Iteration 3/1000 | Loss: 0.00003766
Iteration 4/1000 | Loss: 0.00003042
Iteration 5/1000 | Loss: 0.00002795
Iteration 6/1000 | Loss: 0.00002646
Iteration 7/1000 | Loss: 0.00002537
Iteration 8/1000 | Loss: 0.00002448
Iteration 9/1000 | Loss: 0.00002405
Iteration 10/1000 | Loss: 0.00002374
Iteration 11/1000 | Loss: 0.00002352
Iteration 12/1000 | Loss: 0.00002330
Iteration 13/1000 | Loss: 0.00002330
Iteration 14/1000 | Loss: 0.00002322
Iteration 15/1000 | Loss: 0.00002311
Iteration 16/1000 | Loss: 0.00002310
Iteration 17/1000 | Loss: 0.00002308
Iteration 18/1000 | Loss: 0.00002306
Iteration 19/1000 | Loss: 0.00002305
Iteration 20/1000 | Loss: 0.00002304
Iteration 21/1000 | Loss: 0.00002300
Iteration 22/1000 | Loss: 0.00002298
Iteration 23/1000 | Loss: 0.00002298
Iteration 24/1000 | Loss: 0.00002297
Iteration 25/1000 | Loss: 0.00002296
Iteration 26/1000 | Loss: 0.00002296
Iteration 27/1000 | Loss: 0.00002295
Iteration 28/1000 | Loss: 0.00002294
Iteration 29/1000 | Loss: 0.00002294
Iteration 30/1000 | Loss: 0.00002294
Iteration 31/1000 | Loss: 0.00002294
Iteration 32/1000 | Loss: 0.00002294
Iteration 33/1000 | Loss: 0.00002294
Iteration 34/1000 | Loss: 0.00002293
Iteration 35/1000 | Loss: 0.00002292
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002291
Iteration 38/1000 | Loss: 0.00002291
Iteration 39/1000 | Loss: 0.00002290
Iteration 40/1000 | Loss: 0.00002290
Iteration 41/1000 | Loss: 0.00002290
Iteration 42/1000 | Loss: 0.00002290
Iteration 43/1000 | Loss: 0.00002289
Iteration 44/1000 | Loss: 0.00002289
Iteration 45/1000 | Loss: 0.00002289
Iteration 46/1000 | Loss: 0.00002289
Iteration 47/1000 | Loss: 0.00002289
Iteration 48/1000 | Loss: 0.00002288
Iteration 49/1000 | Loss: 0.00002288
Iteration 50/1000 | Loss: 0.00002288
Iteration 51/1000 | Loss: 0.00002288
Iteration 52/1000 | Loss: 0.00002288
Iteration 53/1000 | Loss: 0.00002288
Iteration 54/1000 | Loss: 0.00002287
Iteration 55/1000 | Loss: 0.00002287
Iteration 56/1000 | Loss: 0.00002287
Iteration 57/1000 | Loss: 0.00002287
Iteration 58/1000 | Loss: 0.00002287
Iteration 59/1000 | Loss: 0.00002287
Iteration 60/1000 | Loss: 0.00002287
Iteration 61/1000 | Loss: 0.00002287
Iteration 62/1000 | Loss: 0.00002287
Iteration 63/1000 | Loss: 0.00002286
Iteration 64/1000 | Loss: 0.00002286
Iteration 65/1000 | Loss: 0.00002286
Iteration 66/1000 | Loss: 0.00002286
Iteration 67/1000 | Loss: 0.00002286
Iteration 68/1000 | Loss: 0.00002285
Iteration 69/1000 | Loss: 0.00002285
Iteration 70/1000 | Loss: 0.00002285
Iteration 71/1000 | Loss: 0.00002284
Iteration 72/1000 | Loss: 0.00002284
Iteration 73/1000 | Loss: 0.00002284
Iteration 74/1000 | Loss: 0.00002284
Iteration 75/1000 | Loss: 0.00002283
Iteration 76/1000 | Loss: 0.00002283
Iteration 77/1000 | Loss: 0.00002283
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002281
Iteration 81/1000 | Loss: 0.00002281
Iteration 82/1000 | Loss: 0.00002280
Iteration 83/1000 | Loss: 0.00002280
Iteration 84/1000 | Loss: 0.00002279
Iteration 85/1000 | Loss: 0.00002279
Iteration 86/1000 | Loss: 0.00002279
Iteration 87/1000 | Loss: 0.00002279
Iteration 88/1000 | Loss: 0.00002277
Iteration 89/1000 | Loss: 0.00002277
Iteration 90/1000 | Loss: 0.00002277
Iteration 91/1000 | Loss: 0.00002276
Iteration 92/1000 | Loss: 0.00002276
Iteration 93/1000 | Loss: 0.00002276
Iteration 94/1000 | Loss: 0.00002276
Iteration 95/1000 | Loss: 0.00002276
Iteration 96/1000 | Loss: 0.00002276
Iteration 97/1000 | Loss: 0.00002276
Iteration 98/1000 | Loss: 0.00002276
Iteration 99/1000 | Loss: 0.00002275
Iteration 100/1000 | Loss: 0.00002275
Iteration 101/1000 | Loss: 0.00002275
Iteration 102/1000 | Loss: 0.00002275
Iteration 103/1000 | Loss: 0.00002275
Iteration 104/1000 | Loss: 0.00002275
Iteration 105/1000 | Loss: 0.00002275
Iteration 106/1000 | Loss: 0.00002275
Iteration 107/1000 | Loss: 0.00002275
Iteration 108/1000 | Loss: 0.00002275
Iteration 109/1000 | Loss: 0.00002274
Iteration 110/1000 | Loss: 0.00002274
Iteration 111/1000 | Loss: 0.00002274
Iteration 112/1000 | Loss: 0.00002274
Iteration 113/1000 | Loss: 0.00002274
Iteration 114/1000 | Loss: 0.00002274
Iteration 115/1000 | Loss: 0.00002274
Iteration 116/1000 | Loss: 0.00002274
Iteration 117/1000 | Loss: 0.00002274
Iteration 118/1000 | Loss: 0.00002273
Iteration 119/1000 | Loss: 0.00002273
Iteration 120/1000 | Loss: 0.00002273
Iteration 121/1000 | Loss: 0.00002273
Iteration 122/1000 | Loss: 0.00002273
Iteration 123/1000 | Loss: 0.00002273
Iteration 124/1000 | Loss: 0.00002273
Iteration 125/1000 | Loss: 0.00002273
Iteration 126/1000 | Loss: 0.00002273
Iteration 127/1000 | Loss: 0.00002273
Iteration 128/1000 | Loss: 0.00002273
Iteration 129/1000 | Loss: 0.00002273
Iteration 130/1000 | Loss: 0.00002273
Iteration 131/1000 | Loss: 0.00002272
Iteration 132/1000 | Loss: 0.00002272
Iteration 133/1000 | Loss: 0.00002272
Iteration 134/1000 | Loss: 0.00002272
Iteration 135/1000 | Loss: 0.00002272
Iteration 136/1000 | Loss: 0.00002272
Iteration 137/1000 | Loss: 0.00002272
Iteration 138/1000 | Loss: 0.00002272
Iteration 139/1000 | Loss: 0.00002271
Iteration 140/1000 | Loss: 0.00002271
Iteration 141/1000 | Loss: 0.00002271
Iteration 142/1000 | Loss: 0.00002271
Iteration 143/1000 | Loss: 0.00002271
Iteration 144/1000 | Loss: 0.00002271
Iteration 145/1000 | Loss: 0.00002271
Iteration 146/1000 | Loss: 0.00002271
Iteration 147/1000 | Loss: 0.00002271
Iteration 148/1000 | Loss: 0.00002271
Iteration 149/1000 | Loss: 0.00002271
Iteration 150/1000 | Loss: 0.00002271
Iteration 151/1000 | Loss: 0.00002271
Iteration 152/1000 | Loss: 0.00002271
Iteration 153/1000 | Loss: 0.00002271
Iteration 154/1000 | Loss: 0.00002271
Iteration 155/1000 | Loss: 0.00002271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.27147920668358e-05, 2.27147920668358e-05, 2.27147920668358e-05, 2.27147920668358e-05, 2.27147920668358e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.27147920668358e-05

Optimization complete. Final v2v error: 4.027971267700195 mm

Highest mean error: 5.1245012283325195 mm for frame 36

Lowest mean error: 3.469956636428833 mm for frame 122

Saving results

Total time: 47.96755266189575
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_30_nl_5289/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_30_nl_5289/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00537967
Iteration 2/25 | Loss: 0.00119117
Iteration 3/25 | Loss: 0.00099577
Iteration 4/25 | Loss: 0.00097200
Iteration 5/25 | Loss: 0.00096314
Iteration 6/25 | Loss: 0.00096203
Iteration 7/25 | Loss: 0.00096191
Iteration 8/25 | Loss: 0.00096191
Iteration 9/25 | Loss: 0.00096191
Iteration 10/25 | Loss: 0.00096191
Iteration 11/25 | Loss: 0.00096191
Iteration 12/25 | Loss: 0.00096191
Iteration 13/25 | Loss: 0.00096191
Iteration 14/25 | Loss: 0.00096191
Iteration 15/25 | Loss: 0.00096191
Iteration 16/25 | Loss: 0.00096191
Iteration 17/25 | Loss: 0.00096191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009619139600545168, 0.0009619139600545168, 0.0009619139600545168, 0.0009619139600545168, 0.0009619139600545168]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009619139600545168

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54195118
Iteration 2/25 | Loss: 0.00192418
Iteration 3/25 | Loss: 0.00192415
Iteration 4/25 | Loss: 0.00192415
Iteration 5/25 | Loss: 0.00192415
Iteration 6/25 | Loss: 0.00192415
Iteration 7/25 | Loss: 0.00192414
Iteration 8/25 | Loss: 0.00192414
Iteration 9/25 | Loss: 0.00192414
Iteration 10/25 | Loss: 0.00192414
Iteration 11/25 | Loss: 0.00192414
Iteration 12/25 | Loss: 0.00192414
Iteration 13/25 | Loss: 0.00192414
Iteration 14/25 | Loss: 0.00192414
Iteration 15/25 | Loss: 0.00192414
Iteration 16/25 | Loss: 0.00192414
Iteration 17/25 | Loss: 0.00192414
Iteration 18/25 | Loss: 0.00192414
Iteration 19/25 | Loss: 0.00192414
Iteration 20/25 | Loss: 0.00192414
Iteration 21/25 | Loss: 0.00192414
Iteration 22/25 | Loss: 0.00192414
Iteration 23/25 | Loss: 0.00192414
Iteration 24/25 | Loss: 0.00192414
Iteration 25/25 | Loss: 0.00192414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00192414
Iteration 2/1000 | Loss: 0.00007640
Iteration 3/1000 | Loss: 0.00005332
Iteration 4/1000 | Loss: 0.00003901
Iteration 5/1000 | Loss: 0.00003435
Iteration 6/1000 | Loss: 0.00003163
Iteration 7/1000 | Loss: 0.00003036
Iteration 8/1000 | Loss: 0.00002911
Iteration 9/1000 | Loss: 0.00002818
Iteration 10/1000 | Loss: 0.00002741
Iteration 11/1000 | Loss: 0.00002685
Iteration 12/1000 | Loss: 0.00002645
Iteration 13/1000 | Loss: 0.00002610
Iteration 14/1000 | Loss: 0.00002586
Iteration 15/1000 | Loss: 0.00002581
Iteration 16/1000 | Loss: 0.00002562
Iteration 17/1000 | Loss: 0.00002559
Iteration 18/1000 | Loss: 0.00002555
Iteration 19/1000 | Loss: 0.00002554
Iteration 20/1000 | Loss: 0.00002552
Iteration 21/1000 | Loss: 0.00002544
Iteration 22/1000 | Loss: 0.00002543
Iteration 23/1000 | Loss: 0.00002540
Iteration 24/1000 | Loss: 0.00002537
Iteration 25/1000 | Loss: 0.00002537
Iteration 26/1000 | Loss: 0.00002536
Iteration 27/1000 | Loss: 0.00002536
Iteration 28/1000 | Loss: 0.00002536
Iteration 29/1000 | Loss: 0.00002535
Iteration 30/1000 | Loss: 0.00002534
Iteration 31/1000 | Loss: 0.00002534
Iteration 32/1000 | Loss: 0.00002534
Iteration 33/1000 | Loss: 0.00002533
Iteration 34/1000 | Loss: 0.00002533
Iteration 35/1000 | Loss: 0.00002533
Iteration 36/1000 | Loss: 0.00002533
Iteration 37/1000 | Loss: 0.00002533
Iteration 38/1000 | Loss: 0.00002532
Iteration 39/1000 | Loss: 0.00002531
Iteration 40/1000 | Loss: 0.00002531
Iteration 41/1000 | Loss: 0.00002530
Iteration 42/1000 | Loss: 0.00002530
Iteration 43/1000 | Loss: 0.00002530
Iteration 44/1000 | Loss: 0.00002530
Iteration 45/1000 | Loss: 0.00002529
Iteration 46/1000 | Loss: 0.00002529
Iteration 47/1000 | Loss: 0.00002529
Iteration 48/1000 | Loss: 0.00002528
Iteration 49/1000 | Loss: 0.00002528
Iteration 50/1000 | Loss: 0.00002528
Iteration 51/1000 | Loss: 0.00002528
Iteration 52/1000 | Loss: 0.00002527
Iteration 53/1000 | Loss: 0.00002527
Iteration 54/1000 | Loss: 0.00002527
Iteration 55/1000 | Loss: 0.00002527
Iteration 56/1000 | Loss: 0.00002527
Iteration 57/1000 | Loss: 0.00002527
Iteration 58/1000 | Loss: 0.00002527
Iteration 59/1000 | Loss: 0.00002527
Iteration 60/1000 | Loss: 0.00002526
Iteration 61/1000 | Loss: 0.00002526
Iteration 62/1000 | Loss: 0.00002526
Iteration 63/1000 | Loss: 0.00002525
Iteration 64/1000 | Loss: 0.00002525
Iteration 65/1000 | Loss: 0.00002525
Iteration 66/1000 | Loss: 0.00002525
Iteration 67/1000 | Loss: 0.00002524
Iteration 68/1000 | Loss: 0.00002524
Iteration 69/1000 | Loss: 0.00002524
Iteration 70/1000 | Loss: 0.00002524
Iteration 71/1000 | Loss: 0.00002524
Iteration 72/1000 | Loss: 0.00002523
Iteration 73/1000 | Loss: 0.00002523
Iteration 74/1000 | Loss: 0.00002523
Iteration 75/1000 | Loss: 0.00002523
Iteration 76/1000 | Loss: 0.00002523
Iteration 77/1000 | Loss: 0.00002523
Iteration 78/1000 | Loss: 0.00002523
Iteration 79/1000 | Loss: 0.00002523
Iteration 80/1000 | Loss: 0.00002523
Iteration 81/1000 | Loss: 0.00002523
Iteration 82/1000 | Loss: 0.00002523
Iteration 83/1000 | Loss: 0.00002522
Iteration 84/1000 | Loss: 0.00002522
Iteration 85/1000 | Loss: 0.00002522
Iteration 86/1000 | Loss: 0.00002522
Iteration 87/1000 | Loss: 0.00002522
Iteration 88/1000 | Loss: 0.00002522
Iteration 89/1000 | Loss: 0.00002521
Iteration 90/1000 | Loss: 0.00002521
Iteration 91/1000 | Loss: 0.00002521
Iteration 92/1000 | Loss: 0.00002521
Iteration 93/1000 | Loss: 0.00002521
Iteration 94/1000 | Loss: 0.00002521
Iteration 95/1000 | Loss: 0.00002521
Iteration 96/1000 | Loss: 0.00002520
Iteration 97/1000 | Loss: 0.00002520
Iteration 98/1000 | Loss: 0.00002520
Iteration 99/1000 | Loss: 0.00002520
Iteration 100/1000 | Loss: 0.00002520
Iteration 101/1000 | Loss: 0.00002520
Iteration 102/1000 | Loss: 0.00002520
Iteration 103/1000 | Loss: 0.00002520
Iteration 104/1000 | Loss: 0.00002520
Iteration 105/1000 | Loss: 0.00002519
Iteration 106/1000 | Loss: 0.00002519
Iteration 107/1000 | Loss: 0.00002519
Iteration 108/1000 | Loss: 0.00002519
Iteration 109/1000 | Loss: 0.00002519
Iteration 110/1000 | Loss: 0.00002519
Iteration 111/1000 | Loss: 0.00002519
Iteration 112/1000 | Loss: 0.00002519
Iteration 113/1000 | Loss: 0.00002519
Iteration 114/1000 | Loss: 0.00002519
Iteration 115/1000 | Loss: 0.00002519
Iteration 116/1000 | Loss: 0.00002519
Iteration 117/1000 | Loss: 0.00002519
Iteration 118/1000 | Loss: 0.00002518
Iteration 119/1000 | Loss: 0.00002518
Iteration 120/1000 | Loss: 0.00002518
Iteration 121/1000 | Loss: 0.00002518
Iteration 122/1000 | Loss: 0.00002518
Iteration 123/1000 | Loss: 0.00002518
Iteration 124/1000 | Loss: 0.00002518
Iteration 125/1000 | Loss: 0.00002518
Iteration 126/1000 | Loss: 0.00002518
Iteration 127/1000 | Loss: 0.00002518
Iteration 128/1000 | Loss: 0.00002518
Iteration 129/1000 | Loss: 0.00002517
Iteration 130/1000 | Loss: 0.00002517
Iteration 131/1000 | Loss: 0.00002517
Iteration 132/1000 | Loss: 0.00002517
Iteration 133/1000 | Loss: 0.00002517
Iteration 134/1000 | Loss: 0.00002516
Iteration 135/1000 | Loss: 0.00002516
Iteration 136/1000 | Loss: 0.00002516
Iteration 137/1000 | Loss: 0.00002516
Iteration 138/1000 | Loss: 0.00002516
Iteration 139/1000 | Loss: 0.00002516
Iteration 140/1000 | Loss: 0.00002516
Iteration 141/1000 | Loss: 0.00002516
Iteration 142/1000 | Loss: 0.00002516
Iteration 143/1000 | Loss: 0.00002516
Iteration 144/1000 | Loss: 0.00002515
Iteration 145/1000 | Loss: 0.00002515
Iteration 146/1000 | Loss: 0.00002515
Iteration 147/1000 | Loss: 0.00002515
Iteration 148/1000 | Loss: 0.00002515
Iteration 149/1000 | Loss: 0.00002515
Iteration 150/1000 | Loss: 0.00002515
Iteration 151/1000 | Loss: 0.00002515
Iteration 152/1000 | Loss: 0.00002515
Iteration 153/1000 | Loss: 0.00002514
Iteration 154/1000 | Loss: 0.00002514
Iteration 155/1000 | Loss: 0.00002514
Iteration 156/1000 | Loss: 0.00002514
Iteration 157/1000 | Loss: 0.00002514
Iteration 158/1000 | Loss: 0.00002514
Iteration 159/1000 | Loss: 0.00002514
Iteration 160/1000 | Loss: 0.00002514
Iteration 161/1000 | Loss: 0.00002514
Iteration 162/1000 | Loss: 0.00002514
Iteration 163/1000 | Loss: 0.00002514
Iteration 164/1000 | Loss: 0.00002514
Iteration 165/1000 | Loss: 0.00002514
Iteration 166/1000 | Loss: 0.00002514
Iteration 167/1000 | Loss: 0.00002514
Iteration 168/1000 | Loss: 0.00002514
Iteration 169/1000 | Loss: 0.00002514
Iteration 170/1000 | Loss: 0.00002514
Iteration 171/1000 | Loss: 0.00002514
Iteration 172/1000 | Loss: 0.00002514
Iteration 173/1000 | Loss: 0.00002514
Iteration 174/1000 | Loss: 0.00002514
Iteration 175/1000 | Loss: 0.00002514
Iteration 176/1000 | Loss: 0.00002514
Iteration 177/1000 | Loss: 0.00002514
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.514203697501216e-05, 2.514203697501216e-05, 2.514203697501216e-05, 2.514203697501216e-05, 2.514203697501216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.514203697501216e-05

Optimization complete. Final v2v error: 4.1652350425720215 mm

Highest mean error: 5.931430816650391 mm for frame 60

Lowest mean error: 3.4569454193115234 mm for frame 82

Saving results

Total time: 44.27269673347473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416992
Iteration 2/25 | Loss: 0.00127524
Iteration 3/25 | Loss: 0.00093927
Iteration 4/25 | Loss: 0.00088515
Iteration 5/25 | Loss: 0.00088031
Iteration 6/25 | Loss: 0.00087893
Iteration 7/25 | Loss: 0.00087868
Iteration 8/25 | Loss: 0.00087868
Iteration 9/25 | Loss: 0.00087868
Iteration 10/25 | Loss: 0.00087868
Iteration 11/25 | Loss: 0.00087868
Iteration 12/25 | Loss: 0.00087868
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008786844555288553, 0.0008786844555288553, 0.0008786844555288553, 0.0008786844555288553, 0.0008786844555288553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008786844555288553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25957596
Iteration 2/25 | Loss: 0.00088168
Iteration 3/25 | Loss: 0.00088168
Iteration 4/25 | Loss: 0.00088168
Iteration 5/25 | Loss: 0.00088168
Iteration 6/25 | Loss: 0.00088168
Iteration 7/25 | Loss: 0.00088168
Iteration 8/25 | Loss: 0.00088168
Iteration 9/25 | Loss: 0.00088168
Iteration 10/25 | Loss: 0.00088168
Iteration 11/25 | Loss: 0.00088168
Iteration 12/25 | Loss: 0.00088168
Iteration 13/25 | Loss: 0.00088168
Iteration 14/25 | Loss: 0.00088168
Iteration 15/25 | Loss: 0.00088168
Iteration 16/25 | Loss: 0.00088168
Iteration 17/25 | Loss: 0.00088168
Iteration 18/25 | Loss: 0.00088168
Iteration 19/25 | Loss: 0.00088168
Iteration 20/25 | Loss: 0.00088168
Iteration 21/25 | Loss: 0.00088168
Iteration 22/25 | Loss: 0.00088168
Iteration 23/25 | Loss: 0.00088168
Iteration 24/25 | Loss: 0.00088168
Iteration 25/25 | Loss: 0.00088168

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088168
Iteration 2/1000 | Loss: 0.00005302
Iteration 3/1000 | Loss: 0.00002982
Iteration 4/1000 | Loss: 0.00001566
Iteration 5/1000 | Loss: 0.00001356
Iteration 6/1000 | Loss: 0.00001252
Iteration 7/1000 | Loss: 0.00001163
Iteration 8/1000 | Loss: 0.00001111
Iteration 9/1000 | Loss: 0.00001074
Iteration 10/1000 | Loss: 0.00001041
Iteration 11/1000 | Loss: 0.00001024
Iteration 12/1000 | Loss: 0.00001008
Iteration 13/1000 | Loss: 0.00001007
Iteration 14/1000 | Loss: 0.00001004
Iteration 15/1000 | Loss: 0.00001003
Iteration 16/1000 | Loss: 0.00000998
Iteration 17/1000 | Loss: 0.00000996
Iteration 18/1000 | Loss: 0.00000995
Iteration 19/1000 | Loss: 0.00000994
Iteration 20/1000 | Loss: 0.00000992
Iteration 21/1000 | Loss: 0.00000991
Iteration 22/1000 | Loss: 0.00000989
Iteration 23/1000 | Loss: 0.00000988
Iteration 24/1000 | Loss: 0.00000985
Iteration 25/1000 | Loss: 0.00000985
Iteration 26/1000 | Loss: 0.00000983
Iteration 27/1000 | Loss: 0.00000983
Iteration 28/1000 | Loss: 0.00000982
Iteration 29/1000 | Loss: 0.00000979
Iteration 30/1000 | Loss: 0.00000976
Iteration 31/1000 | Loss: 0.00000975
Iteration 32/1000 | Loss: 0.00000973
Iteration 33/1000 | Loss: 0.00000972
Iteration 34/1000 | Loss: 0.00000967
Iteration 35/1000 | Loss: 0.00000965
Iteration 36/1000 | Loss: 0.00000964
Iteration 37/1000 | Loss: 0.00000963
Iteration 38/1000 | Loss: 0.00000962
Iteration 39/1000 | Loss: 0.00000961
Iteration 40/1000 | Loss: 0.00000961
Iteration 41/1000 | Loss: 0.00000960
Iteration 42/1000 | Loss: 0.00000960
Iteration 43/1000 | Loss: 0.00000960
Iteration 44/1000 | Loss: 0.00000960
Iteration 45/1000 | Loss: 0.00000959
Iteration 46/1000 | Loss: 0.00000959
Iteration 47/1000 | Loss: 0.00000959
Iteration 48/1000 | Loss: 0.00000959
Iteration 49/1000 | Loss: 0.00000958
Iteration 50/1000 | Loss: 0.00000958
Iteration 51/1000 | Loss: 0.00000958
Iteration 52/1000 | Loss: 0.00000958
Iteration 53/1000 | Loss: 0.00000957
Iteration 54/1000 | Loss: 0.00000957
Iteration 55/1000 | Loss: 0.00000957
Iteration 56/1000 | Loss: 0.00000957
Iteration 57/1000 | Loss: 0.00000957
Iteration 58/1000 | Loss: 0.00000957
Iteration 59/1000 | Loss: 0.00000957
Iteration 60/1000 | Loss: 0.00000956
Iteration 61/1000 | Loss: 0.00000956
Iteration 62/1000 | Loss: 0.00000956
Iteration 63/1000 | Loss: 0.00000956
Iteration 64/1000 | Loss: 0.00000956
Iteration 65/1000 | Loss: 0.00000956
Iteration 66/1000 | Loss: 0.00000955
Iteration 67/1000 | Loss: 0.00000955
Iteration 68/1000 | Loss: 0.00000955
Iteration 69/1000 | Loss: 0.00000955
Iteration 70/1000 | Loss: 0.00000954
Iteration 71/1000 | Loss: 0.00000954
Iteration 72/1000 | Loss: 0.00000954
Iteration 73/1000 | Loss: 0.00000954
Iteration 74/1000 | Loss: 0.00000954
Iteration 75/1000 | Loss: 0.00000953
Iteration 76/1000 | Loss: 0.00000953
Iteration 77/1000 | Loss: 0.00000953
Iteration 78/1000 | Loss: 0.00000953
Iteration 79/1000 | Loss: 0.00000953
Iteration 80/1000 | Loss: 0.00000952
Iteration 81/1000 | Loss: 0.00000952
Iteration 82/1000 | Loss: 0.00000952
Iteration 83/1000 | Loss: 0.00000952
Iteration 84/1000 | Loss: 0.00000951
Iteration 85/1000 | Loss: 0.00000951
Iteration 86/1000 | Loss: 0.00000951
Iteration 87/1000 | Loss: 0.00000951
Iteration 88/1000 | Loss: 0.00000950
Iteration 89/1000 | Loss: 0.00000949
Iteration 90/1000 | Loss: 0.00000949
Iteration 91/1000 | Loss: 0.00000949
Iteration 92/1000 | Loss: 0.00000948
Iteration 93/1000 | Loss: 0.00000948
Iteration 94/1000 | Loss: 0.00000948
Iteration 95/1000 | Loss: 0.00000948
Iteration 96/1000 | Loss: 0.00000948
Iteration 97/1000 | Loss: 0.00000948
Iteration 98/1000 | Loss: 0.00000948
Iteration 99/1000 | Loss: 0.00000948
Iteration 100/1000 | Loss: 0.00000948
Iteration 101/1000 | Loss: 0.00000947
Iteration 102/1000 | Loss: 0.00000947
Iteration 103/1000 | Loss: 0.00000947
Iteration 104/1000 | Loss: 0.00000947
Iteration 105/1000 | Loss: 0.00000946
Iteration 106/1000 | Loss: 0.00000946
Iteration 107/1000 | Loss: 0.00000946
Iteration 108/1000 | Loss: 0.00000946
Iteration 109/1000 | Loss: 0.00000946
Iteration 110/1000 | Loss: 0.00000946
Iteration 111/1000 | Loss: 0.00000945
Iteration 112/1000 | Loss: 0.00000945
Iteration 113/1000 | Loss: 0.00000945
Iteration 114/1000 | Loss: 0.00000945
Iteration 115/1000 | Loss: 0.00000944
Iteration 116/1000 | Loss: 0.00000944
Iteration 117/1000 | Loss: 0.00000944
Iteration 118/1000 | Loss: 0.00000944
Iteration 119/1000 | Loss: 0.00000944
Iteration 120/1000 | Loss: 0.00000944
Iteration 121/1000 | Loss: 0.00000944
Iteration 122/1000 | Loss: 0.00000944
Iteration 123/1000 | Loss: 0.00000943
Iteration 124/1000 | Loss: 0.00000943
Iteration 125/1000 | Loss: 0.00000943
Iteration 126/1000 | Loss: 0.00000943
Iteration 127/1000 | Loss: 0.00000943
Iteration 128/1000 | Loss: 0.00000942
Iteration 129/1000 | Loss: 0.00000942
Iteration 130/1000 | Loss: 0.00000942
Iteration 131/1000 | Loss: 0.00000942
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000942
Iteration 134/1000 | Loss: 0.00000942
Iteration 135/1000 | Loss: 0.00000942
Iteration 136/1000 | Loss: 0.00000942
Iteration 137/1000 | Loss: 0.00000942
Iteration 138/1000 | Loss: 0.00000942
Iteration 139/1000 | Loss: 0.00000942
Iteration 140/1000 | Loss: 0.00000942
Iteration 141/1000 | Loss: 0.00000941
Iteration 142/1000 | Loss: 0.00000941
Iteration 143/1000 | Loss: 0.00000941
Iteration 144/1000 | Loss: 0.00000941
Iteration 145/1000 | Loss: 0.00000940
Iteration 146/1000 | Loss: 0.00000940
Iteration 147/1000 | Loss: 0.00000940
Iteration 148/1000 | Loss: 0.00000940
Iteration 149/1000 | Loss: 0.00000940
Iteration 150/1000 | Loss: 0.00000940
Iteration 151/1000 | Loss: 0.00000939
Iteration 152/1000 | Loss: 0.00000939
Iteration 153/1000 | Loss: 0.00000939
Iteration 154/1000 | Loss: 0.00000939
Iteration 155/1000 | Loss: 0.00000939
Iteration 156/1000 | Loss: 0.00000939
Iteration 157/1000 | Loss: 0.00000939
Iteration 158/1000 | Loss: 0.00000939
Iteration 159/1000 | Loss: 0.00000939
Iteration 160/1000 | Loss: 0.00000939
Iteration 161/1000 | Loss: 0.00000939
Iteration 162/1000 | Loss: 0.00000939
Iteration 163/1000 | Loss: 0.00000939
Iteration 164/1000 | Loss: 0.00000938
Iteration 165/1000 | Loss: 0.00000938
Iteration 166/1000 | Loss: 0.00000938
Iteration 167/1000 | Loss: 0.00000938
Iteration 168/1000 | Loss: 0.00000938
Iteration 169/1000 | Loss: 0.00000938
Iteration 170/1000 | Loss: 0.00000938
Iteration 171/1000 | Loss: 0.00000938
Iteration 172/1000 | Loss: 0.00000937
Iteration 173/1000 | Loss: 0.00000937
Iteration 174/1000 | Loss: 0.00000937
Iteration 175/1000 | Loss: 0.00000937
Iteration 176/1000 | Loss: 0.00000937
Iteration 177/1000 | Loss: 0.00000937
Iteration 178/1000 | Loss: 0.00000937
Iteration 179/1000 | Loss: 0.00000937
Iteration 180/1000 | Loss: 0.00000937
Iteration 181/1000 | Loss: 0.00000937
Iteration 182/1000 | Loss: 0.00000937
Iteration 183/1000 | Loss: 0.00000937
Iteration 184/1000 | Loss: 0.00000937
Iteration 185/1000 | Loss: 0.00000937
Iteration 186/1000 | Loss: 0.00000937
Iteration 187/1000 | Loss: 0.00000937
Iteration 188/1000 | Loss: 0.00000937
Iteration 189/1000 | Loss: 0.00000937
Iteration 190/1000 | Loss: 0.00000937
Iteration 191/1000 | Loss: 0.00000937
Iteration 192/1000 | Loss: 0.00000936
Iteration 193/1000 | Loss: 0.00000936
Iteration 194/1000 | Loss: 0.00000936
Iteration 195/1000 | Loss: 0.00000936
Iteration 196/1000 | Loss: 0.00000936
Iteration 197/1000 | Loss: 0.00000936
Iteration 198/1000 | Loss: 0.00000936
Iteration 199/1000 | Loss: 0.00000936
Iteration 200/1000 | Loss: 0.00000936
Iteration 201/1000 | Loss: 0.00000936
Iteration 202/1000 | Loss: 0.00000936
Iteration 203/1000 | Loss: 0.00000936
Iteration 204/1000 | Loss: 0.00000935
Iteration 205/1000 | Loss: 0.00000935
Iteration 206/1000 | Loss: 0.00000935
Iteration 207/1000 | Loss: 0.00000935
Iteration 208/1000 | Loss: 0.00000935
Iteration 209/1000 | Loss: 0.00000935
Iteration 210/1000 | Loss: 0.00000935
Iteration 211/1000 | Loss: 0.00000935
Iteration 212/1000 | Loss: 0.00000935
Iteration 213/1000 | Loss: 0.00000935
Iteration 214/1000 | Loss: 0.00000934
Iteration 215/1000 | Loss: 0.00000934
Iteration 216/1000 | Loss: 0.00000934
Iteration 217/1000 | Loss: 0.00000934
Iteration 218/1000 | Loss: 0.00000934
Iteration 219/1000 | Loss: 0.00000934
Iteration 220/1000 | Loss: 0.00000934
Iteration 221/1000 | Loss: 0.00000934
Iteration 222/1000 | Loss: 0.00000933
Iteration 223/1000 | Loss: 0.00000933
Iteration 224/1000 | Loss: 0.00000933
Iteration 225/1000 | Loss: 0.00000933
Iteration 226/1000 | Loss: 0.00000933
Iteration 227/1000 | Loss: 0.00000933
Iteration 228/1000 | Loss: 0.00000933
Iteration 229/1000 | Loss: 0.00000933
Iteration 230/1000 | Loss: 0.00000933
Iteration 231/1000 | Loss: 0.00000933
Iteration 232/1000 | Loss: 0.00000933
Iteration 233/1000 | Loss: 0.00000933
Iteration 234/1000 | Loss: 0.00000933
Iteration 235/1000 | Loss: 0.00000933
Iteration 236/1000 | Loss: 0.00000933
Iteration 237/1000 | Loss: 0.00000933
Iteration 238/1000 | Loss: 0.00000933
Iteration 239/1000 | Loss: 0.00000933
Iteration 240/1000 | Loss: 0.00000933
Iteration 241/1000 | Loss: 0.00000933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 241. Stopping optimization.
Last 5 losses: [9.332084118796047e-06, 9.332084118796047e-06, 9.332084118796047e-06, 9.332084118796047e-06, 9.332084118796047e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.332084118796047e-06

Optimization complete. Final v2v error: 2.6398403644561768 mm

Highest mean error: 3.061948299407959 mm for frame 73

Lowest mean error: 2.3714942932128906 mm for frame 90

Saving results

Total time: 44.91037082672119
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465231
Iteration 2/25 | Loss: 0.00114986
Iteration 3/25 | Loss: 0.00095600
Iteration 4/25 | Loss: 0.00093750
Iteration 5/25 | Loss: 0.00093418
Iteration 6/25 | Loss: 0.00093343
Iteration 7/25 | Loss: 0.00093343
Iteration 8/25 | Loss: 0.00093343
Iteration 9/25 | Loss: 0.00093343
Iteration 10/25 | Loss: 0.00093343
Iteration 11/25 | Loss: 0.00093343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009334253845736384, 0.0009334253845736384, 0.0009334253845736384, 0.0009334253845736384, 0.0009334253845736384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009334253845736384

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30843103
Iteration 2/25 | Loss: 0.00071892
Iteration 3/25 | Loss: 0.00071892
Iteration 4/25 | Loss: 0.00071892
Iteration 5/25 | Loss: 0.00071892
Iteration 6/25 | Loss: 0.00071892
Iteration 7/25 | Loss: 0.00071892
Iteration 8/25 | Loss: 0.00071892
Iteration 9/25 | Loss: 0.00071892
Iteration 10/25 | Loss: 0.00071892
Iteration 11/25 | Loss: 0.00071892
Iteration 12/25 | Loss: 0.00071892
Iteration 13/25 | Loss: 0.00071892
Iteration 14/25 | Loss: 0.00071892
Iteration 15/25 | Loss: 0.00071892
Iteration 16/25 | Loss: 0.00071892
Iteration 17/25 | Loss: 0.00071892
Iteration 18/25 | Loss: 0.00071892
Iteration 19/25 | Loss: 0.00071892
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007189179887063801, 0.0007189179887063801, 0.0007189179887063801, 0.0007189179887063801, 0.0007189179887063801]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007189179887063801

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071892
Iteration 2/1000 | Loss: 0.00003991
Iteration 3/1000 | Loss: 0.00002979
Iteration 4/1000 | Loss: 0.00002745
Iteration 5/1000 | Loss: 0.00002612
Iteration 6/1000 | Loss: 0.00002530
Iteration 7/1000 | Loss: 0.00002474
Iteration 8/1000 | Loss: 0.00002430
Iteration 9/1000 | Loss: 0.00002393
Iteration 10/1000 | Loss: 0.00002367
Iteration 11/1000 | Loss: 0.00002350
Iteration 12/1000 | Loss: 0.00002348
Iteration 13/1000 | Loss: 0.00002336
Iteration 14/1000 | Loss: 0.00002326
Iteration 15/1000 | Loss: 0.00002316
Iteration 16/1000 | Loss: 0.00002312
Iteration 17/1000 | Loss: 0.00002310
Iteration 18/1000 | Loss: 0.00002309
Iteration 19/1000 | Loss: 0.00002308
Iteration 20/1000 | Loss: 0.00002304
Iteration 21/1000 | Loss: 0.00002304
Iteration 22/1000 | Loss: 0.00002299
Iteration 23/1000 | Loss: 0.00002298
Iteration 24/1000 | Loss: 0.00002294
Iteration 25/1000 | Loss: 0.00002291
Iteration 26/1000 | Loss: 0.00002291
Iteration 27/1000 | Loss: 0.00002291
Iteration 28/1000 | Loss: 0.00002290
Iteration 29/1000 | Loss: 0.00002290
Iteration 30/1000 | Loss: 0.00002290
Iteration 31/1000 | Loss: 0.00002290
Iteration 32/1000 | Loss: 0.00002290
Iteration 33/1000 | Loss: 0.00002290
Iteration 34/1000 | Loss: 0.00002288
Iteration 35/1000 | Loss: 0.00002287
Iteration 36/1000 | Loss: 0.00002287
Iteration 37/1000 | Loss: 0.00002287
Iteration 38/1000 | Loss: 0.00002286
Iteration 39/1000 | Loss: 0.00002286
Iteration 40/1000 | Loss: 0.00002286
Iteration 41/1000 | Loss: 0.00002285
Iteration 42/1000 | Loss: 0.00002285
Iteration 43/1000 | Loss: 0.00002285
Iteration 44/1000 | Loss: 0.00002285
Iteration 45/1000 | Loss: 0.00002285
Iteration 46/1000 | Loss: 0.00002284
Iteration 47/1000 | Loss: 0.00002284
Iteration 48/1000 | Loss: 0.00002283
Iteration 49/1000 | Loss: 0.00002283
Iteration 50/1000 | Loss: 0.00002283
Iteration 51/1000 | Loss: 0.00002282
Iteration 52/1000 | Loss: 0.00002282
Iteration 53/1000 | Loss: 0.00002281
Iteration 54/1000 | Loss: 0.00002281
Iteration 55/1000 | Loss: 0.00002281
Iteration 56/1000 | Loss: 0.00002280
Iteration 57/1000 | Loss: 0.00002280
Iteration 58/1000 | Loss: 0.00002280
Iteration 59/1000 | Loss: 0.00002278
Iteration 60/1000 | Loss: 0.00002277
Iteration 61/1000 | Loss: 0.00002277
Iteration 62/1000 | Loss: 0.00002277
Iteration 63/1000 | Loss: 0.00002276
Iteration 64/1000 | Loss: 0.00002276
Iteration 65/1000 | Loss: 0.00002275
Iteration 66/1000 | Loss: 0.00002275
Iteration 67/1000 | Loss: 0.00002274
Iteration 68/1000 | Loss: 0.00002274
Iteration 69/1000 | Loss: 0.00002273
Iteration 70/1000 | Loss: 0.00002273
Iteration 71/1000 | Loss: 0.00002271
Iteration 72/1000 | Loss: 0.00002271
Iteration 73/1000 | Loss: 0.00002271
Iteration 74/1000 | Loss: 0.00002271
Iteration 75/1000 | Loss: 0.00002271
Iteration 76/1000 | Loss: 0.00002271
Iteration 77/1000 | Loss: 0.00002271
Iteration 78/1000 | Loss: 0.00002271
Iteration 79/1000 | Loss: 0.00002271
Iteration 80/1000 | Loss: 0.00002271
Iteration 81/1000 | Loss: 0.00002271
Iteration 82/1000 | Loss: 0.00002270
Iteration 83/1000 | Loss: 0.00002270
Iteration 84/1000 | Loss: 0.00002269
Iteration 85/1000 | Loss: 0.00002269
Iteration 86/1000 | Loss: 0.00002269
Iteration 87/1000 | Loss: 0.00002268
Iteration 88/1000 | Loss: 0.00002268
Iteration 89/1000 | Loss: 0.00002268
Iteration 90/1000 | Loss: 0.00002267
Iteration 91/1000 | Loss: 0.00002267
Iteration 92/1000 | Loss: 0.00002267
Iteration 93/1000 | Loss: 0.00002267
Iteration 94/1000 | Loss: 0.00002267
Iteration 95/1000 | Loss: 0.00002267
Iteration 96/1000 | Loss: 0.00002267
Iteration 97/1000 | Loss: 0.00002266
Iteration 98/1000 | Loss: 0.00002266
Iteration 99/1000 | Loss: 0.00002265
Iteration 100/1000 | Loss: 0.00002265
Iteration 101/1000 | Loss: 0.00002265
Iteration 102/1000 | Loss: 0.00002265
Iteration 103/1000 | Loss: 0.00002265
Iteration 104/1000 | Loss: 0.00002265
Iteration 105/1000 | Loss: 0.00002264
Iteration 106/1000 | Loss: 0.00002264
Iteration 107/1000 | Loss: 0.00002264
Iteration 108/1000 | Loss: 0.00002264
Iteration 109/1000 | Loss: 0.00002264
Iteration 110/1000 | Loss: 0.00002263
Iteration 111/1000 | Loss: 0.00002263
Iteration 112/1000 | Loss: 0.00002263
Iteration 113/1000 | Loss: 0.00002263
Iteration 114/1000 | Loss: 0.00002263
Iteration 115/1000 | Loss: 0.00002262
Iteration 116/1000 | Loss: 0.00002262
Iteration 117/1000 | Loss: 0.00002262
Iteration 118/1000 | Loss: 0.00002262
Iteration 119/1000 | Loss: 0.00002262
Iteration 120/1000 | Loss: 0.00002262
Iteration 121/1000 | Loss: 0.00002262
Iteration 122/1000 | Loss: 0.00002262
Iteration 123/1000 | Loss: 0.00002262
Iteration 124/1000 | Loss: 0.00002262
Iteration 125/1000 | Loss: 0.00002262
Iteration 126/1000 | Loss: 0.00002262
Iteration 127/1000 | Loss: 0.00002262
Iteration 128/1000 | Loss: 0.00002262
Iteration 129/1000 | Loss: 0.00002262
Iteration 130/1000 | Loss: 0.00002262
Iteration 131/1000 | Loss: 0.00002261
Iteration 132/1000 | Loss: 0.00002261
Iteration 133/1000 | Loss: 0.00002261
Iteration 134/1000 | Loss: 0.00002261
Iteration 135/1000 | Loss: 0.00002261
Iteration 136/1000 | Loss: 0.00002261
Iteration 137/1000 | Loss: 0.00002261
Iteration 138/1000 | Loss: 0.00002261
Iteration 139/1000 | Loss: 0.00002261
Iteration 140/1000 | Loss: 0.00002261
Iteration 141/1000 | Loss: 0.00002260
Iteration 142/1000 | Loss: 0.00002260
Iteration 143/1000 | Loss: 0.00002260
Iteration 144/1000 | Loss: 0.00002260
Iteration 145/1000 | Loss: 0.00002260
Iteration 146/1000 | Loss: 0.00002259
Iteration 147/1000 | Loss: 0.00002259
Iteration 148/1000 | Loss: 0.00002259
Iteration 149/1000 | Loss: 0.00002259
Iteration 150/1000 | Loss: 0.00002259
Iteration 151/1000 | Loss: 0.00002259
Iteration 152/1000 | Loss: 0.00002259
Iteration 153/1000 | Loss: 0.00002259
Iteration 154/1000 | Loss: 0.00002259
Iteration 155/1000 | Loss: 0.00002259
Iteration 156/1000 | Loss: 0.00002259
Iteration 157/1000 | Loss: 0.00002259
Iteration 158/1000 | Loss: 0.00002259
Iteration 159/1000 | Loss: 0.00002259
Iteration 160/1000 | Loss: 0.00002259
Iteration 161/1000 | Loss: 0.00002258
Iteration 162/1000 | Loss: 0.00002258
Iteration 163/1000 | Loss: 0.00002258
Iteration 164/1000 | Loss: 0.00002258
Iteration 165/1000 | Loss: 0.00002258
Iteration 166/1000 | Loss: 0.00002257
Iteration 167/1000 | Loss: 0.00002257
Iteration 168/1000 | Loss: 0.00002257
Iteration 169/1000 | Loss: 0.00002257
Iteration 170/1000 | Loss: 0.00002257
Iteration 171/1000 | Loss: 0.00002257
Iteration 172/1000 | Loss: 0.00002257
Iteration 173/1000 | Loss: 0.00002257
Iteration 174/1000 | Loss: 0.00002257
Iteration 175/1000 | Loss: 0.00002256
Iteration 176/1000 | Loss: 0.00002256
Iteration 177/1000 | Loss: 0.00002256
Iteration 178/1000 | Loss: 0.00002256
Iteration 179/1000 | Loss: 0.00002256
Iteration 180/1000 | Loss: 0.00002256
Iteration 181/1000 | Loss: 0.00002256
Iteration 182/1000 | Loss: 0.00002256
Iteration 183/1000 | Loss: 0.00002256
Iteration 184/1000 | Loss: 0.00002256
Iteration 185/1000 | Loss: 0.00002256
Iteration 186/1000 | Loss: 0.00002256
Iteration 187/1000 | Loss: 0.00002256
Iteration 188/1000 | Loss: 0.00002256
Iteration 189/1000 | Loss: 0.00002256
Iteration 190/1000 | Loss: 0.00002256
Iteration 191/1000 | Loss: 0.00002256
Iteration 192/1000 | Loss: 0.00002256
Iteration 193/1000 | Loss: 0.00002256
Iteration 194/1000 | Loss: 0.00002256
Iteration 195/1000 | Loss: 0.00002256
Iteration 196/1000 | Loss: 0.00002256
Iteration 197/1000 | Loss: 0.00002256
Iteration 198/1000 | Loss: 0.00002256
Iteration 199/1000 | Loss: 0.00002256
Iteration 200/1000 | Loss: 0.00002256
Iteration 201/1000 | Loss: 0.00002256
Iteration 202/1000 | Loss: 0.00002256
Iteration 203/1000 | Loss: 0.00002256
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [2.2562422600458376e-05, 2.2562422600458376e-05, 2.2562422600458376e-05, 2.2562422600458376e-05, 2.2562422600458376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2562422600458376e-05

Optimization complete. Final v2v error: 3.974055290222168 mm

Highest mean error: 5.099710941314697 mm for frame 88

Lowest mean error: 2.8234317302703857 mm for frame 6

Saving results

Total time: 49.599424600601196
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00381719
Iteration 2/25 | Loss: 0.00094352
Iteration 3/25 | Loss: 0.00085076
Iteration 4/25 | Loss: 0.00083731
Iteration 5/25 | Loss: 0.00083319
Iteration 6/25 | Loss: 0.00083151
Iteration 7/25 | Loss: 0.00083151
Iteration 8/25 | Loss: 0.00083151
Iteration 9/25 | Loss: 0.00083151
Iteration 10/25 | Loss: 0.00083151
Iteration 11/25 | Loss: 0.00083151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008315065060742199, 0.0008315065060742199, 0.0008315065060742199, 0.0008315065060742199, 0.0008315065060742199]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008315065060742199

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29339063
Iteration 2/25 | Loss: 0.00083011
Iteration 3/25 | Loss: 0.00083011
Iteration 4/25 | Loss: 0.00083011
Iteration 5/25 | Loss: 0.00083011
Iteration 6/25 | Loss: 0.00083011
Iteration 7/25 | Loss: 0.00083011
Iteration 8/25 | Loss: 0.00083011
Iteration 9/25 | Loss: 0.00083011
Iteration 10/25 | Loss: 0.00083011
Iteration 11/25 | Loss: 0.00083011
Iteration 12/25 | Loss: 0.00083011
Iteration 13/25 | Loss: 0.00083011
Iteration 14/25 | Loss: 0.00083011
Iteration 15/25 | Loss: 0.00083011
Iteration 16/25 | Loss: 0.00083011
Iteration 17/25 | Loss: 0.00083011
Iteration 18/25 | Loss: 0.00083011
Iteration 19/25 | Loss: 0.00083011
Iteration 20/25 | Loss: 0.00083011
Iteration 21/25 | Loss: 0.00083011
Iteration 22/25 | Loss: 0.00083011
Iteration 23/25 | Loss: 0.00083011
Iteration 24/25 | Loss: 0.00083011
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0008301079506054521, 0.0008301079506054521, 0.0008301079506054521, 0.0008301079506054521, 0.0008301079506054521]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008301079506054521

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083011
Iteration 2/1000 | Loss: 0.00003174
Iteration 3/1000 | Loss: 0.00001442
Iteration 4/1000 | Loss: 0.00001248
Iteration 5/1000 | Loss: 0.00001129
Iteration 6/1000 | Loss: 0.00001071
Iteration 7/1000 | Loss: 0.00001016
Iteration 8/1000 | Loss: 0.00000995
Iteration 9/1000 | Loss: 0.00000974
Iteration 10/1000 | Loss: 0.00000960
Iteration 11/1000 | Loss: 0.00000959
Iteration 12/1000 | Loss: 0.00000959
Iteration 13/1000 | Loss: 0.00000958
Iteration 14/1000 | Loss: 0.00000955
Iteration 15/1000 | Loss: 0.00000951
Iteration 16/1000 | Loss: 0.00000951
Iteration 17/1000 | Loss: 0.00000950
Iteration 18/1000 | Loss: 0.00000950
Iteration 19/1000 | Loss: 0.00000943
Iteration 20/1000 | Loss: 0.00000943
Iteration 21/1000 | Loss: 0.00000942
Iteration 22/1000 | Loss: 0.00000942
Iteration 23/1000 | Loss: 0.00000941
Iteration 24/1000 | Loss: 0.00000941
Iteration 25/1000 | Loss: 0.00000941
Iteration 26/1000 | Loss: 0.00000939
Iteration 27/1000 | Loss: 0.00000939
Iteration 28/1000 | Loss: 0.00000938
Iteration 29/1000 | Loss: 0.00000938
Iteration 30/1000 | Loss: 0.00000938
Iteration 31/1000 | Loss: 0.00000937
Iteration 32/1000 | Loss: 0.00000937
Iteration 33/1000 | Loss: 0.00000936
Iteration 34/1000 | Loss: 0.00000936
Iteration 35/1000 | Loss: 0.00000935
Iteration 36/1000 | Loss: 0.00000935
Iteration 37/1000 | Loss: 0.00000934
Iteration 38/1000 | Loss: 0.00000934
Iteration 39/1000 | Loss: 0.00000932
Iteration 40/1000 | Loss: 0.00000931
Iteration 41/1000 | Loss: 0.00000930
Iteration 42/1000 | Loss: 0.00000929
Iteration 43/1000 | Loss: 0.00000929
Iteration 44/1000 | Loss: 0.00000928
Iteration 45/1000 | Loss: 0.00000928
Iteration 46/1000 | Loss: 0.00000928
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000927
Iteration 49/1000 | Loss: 0.00000926
Iteration 50/1000 | Loss: 0.00000925
Iteration 51/1000 | Loss: 0.00000925
Iteration 52/1000 | Loss: 0.00000925
Iteration 53/1000 | Loss: 0.00000925
Iteration 54/1000 | Loss: 0.00000924
Iteration 55/1000 | Loss: 0.00000924
Iteration 56/1000 | Loss: 0.00000924
Iteration 57/1000 | Loss: 0.00000923
Iteration 58/1000 | Loss: 0.00000923
Iteration 59/1000 | Loss: 0.00000923
Iteration 60/1000 | Loss: 0.00000923
Iteration 61/1000 | Loss: 0.00000923
Iteration 62/1000 | Loss: 0.00000923
Iteration 63/1000 | Loss: 0.00000922
Iteration 64/1000 | Loss: 0.00000922
Iteration 65/1000 | Loss: 0.00000922
Iteration 66/1000 | Loss: 0.00000922
Iteration 67/1000 | Loss: 0.00000922
Iteration 68/1000 | Loss: 0.00000922
Iteration 69/1000 | Loss: 0.00000922
Iteration 70/1000 | Loss: 0.00000921
Iteration 71/1000 | Loss: 0.00000921
Iteration 72/1000 | Loss: 0.00000920
Iteration 73/1000 | Loss: 0.00000920
Iteration 74/1000 | Loss: 0.00000919
Iteration 75/1000 | Loss: 0.00000919
Iteration 76/1000 | Loss: 0.00000919
Iteration 77/1000 | Loss: 0.00000918
Iteration 78/1000 | Loss: 0.00000917
Iteration 79/1000 | Loss: 0.00000917
Iteration 80/1000 | Loss: 0.00000917
Iteration 81/1000 | Loss: 0.00000917
Iteration 82/1000 | Loss: 0.00000916
Iteration 83/1000 | Loss: 0.00000916
Iteration 84/1000 | Loss: 0.00000916
Iteration 85/1000 | Loss: 0.00000916
Iteration 86/1000 | Loss: 0.00000916
Iteration 87/1000 | Loss: 0.00000916
Iteration 88/1000 | Loss: 0.00000916
Iteration 89/1000 | Loss: 0.00000916
Iteration 90/1000 | Loss: 0.00000916
Iteration 91/1000 | Loss: 0.00000916
Iteration 92/1000 | Loss: 0.00000916
Iteration 93/1000 | Loss: 0.00000916
Iteration 94/1000 | Loss: 0.00000915
Iteration 95/1000 | Loss: 0.00000915
Iteration 96/1000 | Loss: 0.00000915
Iteration 97/1000 | Loss: 0.00000915
Iteration 98/1000 | Loss: 0.00000915
Iteration 99/1000 | Loss: 0.00000915
Iteration 100/1000 | Loss: 0.00000915
Iteration 101/1000 | Loss: 0.00000915
Iteration 102/1000 | Loss: 0.00000915
Iteration 103/1000 | Loss: 0.00000915
Iteration 104/1000 | Loss: 0.00000915
Iteration 105/1000 | Loss: 0.00000915
Iteration 106/1000 | Loss: 0.00000915
Iteration 107/1000 | Loss: 0.00000915
Iteration 108/1000 | Loss: 0.00000915
Iteration 109/1000 | Loss: 0.00000915
Iteration 110/1000 | Loss: 0.00000915
Iteration 111/1000 | Loss: 0.00000915
Iteration 112/1000 | Loss: 0.00000915
Iteration 113/1000 | Loss: 0.00000915
Iteration 114/1000 | Loss: 0.00000915
Iteration 115/1000 | Loss: 0.00000915
Iteration 116/1000 | Loss: 0.00000915
Iteration 117/1000 | Loss: 0.00000915
Iteration 118/1000 | Loss: 0.00000915
Iteration 119/1000 | Loss: 0.00000915
Iteration 120/1000 | Loss: 0.00000915
Iteration 121/1000 | Loss: 0.00000915
Iteration 122/1000 | Loss: 0.00000915
Iteration 123/1000 | Loss: 0.00000915
Iteration 124/1000 | Loss: 0.00000915
Iteration 125/1000 | Loss: 0.00000915
Iteration 126/1000 | Loss: 0.00000915
Iteration 127/1000 | Loss: 0.00000915
Iteration 128/1000 | Loss: 0.00000915
Iteration 129/1000 | Loss: 0.00000915
Iteration 130/1000 | Loss: 0.00000915
Iteration 131/1000 | Loss: 0.00000915
Iteration 132/1000 | Loss: 0.00000915
Iteration 133/1000 | Loss: 0.00000915
Iteration 134/1000 | Loss: 0.00000915
Iteration 135/1000 | Loss: 0.00000915
Iteration 136/1000 | Loss: 0.00000915
Iteration 137/1000 | Loss: 0.00000915
Iteration 138/1000 | Loss: 0.00000915
Iteration 139/1000 | Loss: 0.00000915
Iteration 140/1000 | Loss: 0.00000915
Iteration 141/1000 | Loss: 0.00000915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [9.15017426450504e-06, 9.15017426450504e-06, 9.15017426450504e-06, 9.15017426450504e-06, 9.15017426450504e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.15017426450504e-06

Optimization complete. Final v2v error: 2.597975015640259 mm

Highest mean error: 2.8083910942077637 mm for frame 67

Lowest mean error: 2.292501449584961 mm for frame 171

Saving results

Total time: 32.907443046569824
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00821446
Iteration 2/25 | Loss: 0.00253868
Iteration 3/25 | Loss: 0.00179661
Iteration 4/25 | Loss: 0.00153558
Iteration 5/25 | Loss: 0.00155573
Iteration 6/25 | Loss: 0.00147236
Iteration 7/25 | Loss: 0.00133466
Iteration 8/25 | Loss: 0.00128448
Iteration 9/25 | Loss: 0.00128874
Iteration 10/25 | Loss: 0.00130594
Iteration 11/25 | Loss: 0.00125303
Iteration 12/25 | Loss: 0.00111469
Iteration 13/25 | Loss: 0.00111245
Iteration 14/25 | Loss: 0.00108632
Iteration 15/25 | Loss: 0.00108435
Iteration 16/25 | Loss: 0.00106741
Iteration 17/25 | Loss: 0.00103637
Iteration 18/25 | Loss: 0.00102945
Iteration 19/25 | Loss: 0.00102832
Iteration 20/25 | Loss: 0.00102482
Iteration 21/25 | Loss: 0.00101760
Iteration 22/25 | Loss: 0.00102407
Iteration 23/25 | Loss: 0.00102574
Iteration 24/25 | Loss: 0.00102379
Iteration 25/25 | Loss: 0.00101744

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73636436
Iteration 2/25 | Loss: 0.00172173
Iteration 3/25 | Loss: 0.00153248
Iteration 4/25 | Loss: 0.00139876
Iteration 5/25 | Loss: 0.00139876
Iteration 6/25 | Loss: 0.00139876
Iteration 7/25 | Loss: 0.00139876
Iteration 8/25 | Loss: 0.00139876
Iteration 9/25 | Loss: 0.00139876
Iteration 10/25 | Loss: 0.00139876
Iteration 11/25 | Loss: 0.00139876
Iteration 12/25 | Loss: 0.00139876
Iteration 13/25 | Loss: 0.00139876
Iteration 14/25 | Loss: 0.00139876
Iteration 15/25 | Loss: 0.00139876
Iteration 16/25 | Loss: 0.00139876
Iteration 17/25 | Loss: 0.00139876
Iteration 18/25 | Loss: 0.00139876
Iteration 19/25 | Loss: 0.00139876
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001398761523887515, 0.001398761523887515, 0.001398761523887515, 0.001398761523887515, 0.001398761523887515]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001398761523887515

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139876
Iteration 2/1000 | Loss: 0.00026967
Iteration 3/1000 | Loss: 0.00019082
Iteration 4/1000 | Loss: 0.00149189
Iteration 5/1000 | Loss: 0.00671313
Iteration 6/1000 | Loss: 0.00691234
Iteration 7/1000 | Loss: 0.00272967
Iteration 8/1000 | Loss: 0.00226657
Iteration 9/1000 | Loss: 0.00359423
Iteration 10/1000 | Loss: 0.00264028
Iteration 11/1000 | Loss: 0.00227921
Iteration 12/1000 | Loss: 0.00019730
Iteration 13/1000 | Loss: 0.00013423
Iteration 14/1000 | Loss: 0.00013544
Iteration 15/1000 | Loss: 0.00079337
Iteration 16/1000 | Loss: 0.00014183
Iteration 17/1000 | Loss: 0.00007304
Iteration 18/1000 | Loss: 0.00038452
Iteration 19/1000 | Loss: 0.00009809
Iteration 20/1000 | Loss: 0.00012803
Iteration 21/1000 | Loss: 0.00024470
Iteration 22/1000 | Loss: 0.00018277
Iteration 23/1000 | Loss: 0.00009998
Iteration 24/1000 | Loss: 0.00019464
Iteration 25/1000 | Loss: 0.00610375
Iteration 26/1000 | Loss: 0.00181317
Iteration 27/1000 | Loss: 0.00011923
Iteration 28/1000 | Loss: 0.00010447
Iteration 29/1000 | Loss: 0.00034619
Iteration 30/1000 | Loss: 0.00007835
Iteration 31/1000 | Loss: 0.00016925
Iteration 32/1000 | Loss: 0.00015453
Iteration 33/1000 | Loss: 0.00004407
Iteration 34/1000 | Loss: 0.00003396
Iteration 35/1000 | Loss: 0.00024550
Iteration 36/1000 | Loss: 0.00002133
Iteration 37/1000 | Loss: 0.00002002
Iteration 38/1000 | Loss: 0.00006181
Iteration 39/1000 | Loss: 0.00007567
Iteration 40/1000 | Loss: 0.00008760
Iteration 41/1000 | Loss: 0.00001643
Iteration 42/1000 | Loss: 0.00006060
Iteration 43/1000 | Loss: 0.00009049
Iteration 44/1000 | Loss: 0.00001477
Iteration 45/1000 | Loss: 0.00001437
Iteration 46/1000 | Loss: 0.00011044
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00041322
Iteration 49/1000 | Loss: 0.00003303
Iteration 50/1000 | Loss: 0.00001366
Iteration 51/1000 | Loss: 0.00001342
Iteration 52/1000 | Loss: 0.00001327
Iteration 53/1000 | Loss: 0.00005158
Iteration 54/1000 | Loss: 0.00061400
Iteration 55/1000 | Loss: 0.00005473
Iteration 56/1000 | Loss: 0.00012188
Iteration 57/1000 | Loss: 0.00001705
Iteration 58/1000 | Loss: 0.00005086
Iteration 59/1000 | Loss: 0.00007027
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001319
Iteration 62/1000 | Loss: 0.00001318
Iteration 63/1000 | Loss: 0.00001318
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001317
Iteration 68/1000 | Loss: 0.00001317
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001316
Iteration 74/1000 | Loss: 0.00001316
Iteration 75/1000 | Loss: 0.00001314
Iteration 76/1000 | Loss: 0.00001314
Iteration 77/1000 | Loss: 0.00001314
Iteration 78/1000 | Loss: 0.00001314
Iteration 79/1000 | Loss: 0.00001314
Iteration 80/1000 | Loss: 0.00001314
Iteration 81/1000 | Loss: 0.00001314
Iteration 82/1000 | Loss: 0.00001314
Iteration 83/1000 | Loss: 0.00001314
Iteration 84/1000 | Loss: 0.00001314
Iteration 85/1000 | Loss: 0.00001314
Iteration 86/1000 | Loss: 0.00001314
Iteration 87/1000 | Loss: 0.00001314
Iteration 88/1000 | Loss: 0.00001314
Iteration 89/1000 | Loss: 0.00001313
Iteration 90/1000 | Loss: 0.00001313
Iteration 91/1000 | Loss: 0.00001313
Iteration 92/1000 | Loss: 0.00001313
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001313
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001313
Iteration 98/1000 | Loss: 0.00001313
Iteration 99/1000 | Loss: 0.00001313
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001312
Iteration 107/1000 | Loss: 0.00001311
Iteration 108/1000 | Loss: 0.00001311
Iteration 109/1000 | Loss: 0.00001311
Iteration 110/1000 | Loss: 0.00001311
Iteration 111/1000 | Loss: 0.00001311
Iteration 112/1000 | Loss: 0.00001311
Iteration 113/1000 | Loss: 0.00001311
Iteration 114/1000 | Loss: 0.00001311
Iteration 115/1000 | Loss: 0.00001311
Iteration 116/1000 | Loss: 0.00001311
Iteration 117/1000 | Loss: 0.00001311
Iteration 118/1000 | Loss: 0.00001311
Iteration 119/1000 | Loss: 0.00001310
Iteration 120/1000 | Loss: 0.00001310
Iteration 121/1000 | Loss: 0.00001310
Iteration 122/1000 | Loss: 0.00001310
Iteration 123/1000 | Loss: 0.00001310
Iteration 124/1000 | Loss: 0.00001310
Iteration 125/1000 | Loss: 0.00001310
Iteration 126/1000 | Loss: 0.00001310
Iteration 127/1000 | Loss: 0.00001310
Iteration 128/1000 | Loss: 0.00001310
Iteration 129/1000 | Loss: 0.00001310
Iteration 130/1000 | Loss: 0.00001310
Iteration 131/1000 | Loss: 0.00001310
Iteration 132/1000 | Loss: 0.00001309
Iteration 133/1000 | Loss: 0.00001309
Iteration 134/1000 | Loss: 0.00001309
Iteration 135/1000 | Loss: 0.00001309
Iteration 136/1000 | Loss: 0.00001309
Iteration 137/1000 | Loss: 0.00001309
Iteration 138/1000 | Loss: 0.00001309
Iteration 139/1000 | Loss: 0.00001309
Iteration 140/1000 | Loss: 0.00001309
Iteration 141/1000 | Loss: 0.00001309
Iteration 142/1000 | Loss: 0.00001309
Iteration 143/1000 | Loss: 0.00001309
Iteration 144/1000 | Loss: 0.00001309
Iteration 145/1000 | Loss: 0.00001309
Iteration 146/1000 | Loss: 0.00001309
Iteration 147/1000 | Loss: 0.00001309
Iteration 148/1000 | Loss: 0.00001309
Iteration 149/1000 | Loss: 0.00001309
Iteration 150/1000 | Loss: 0.00001309
Iteration 151/1000 | Loss: 0.00001309
Iteration 152/1000 | Loss: 0.00001309
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.3086564649711363e-05, 1.3086564649711363e-05, 1.3086564649711363e-05, 1.3086564649711363e-05, 1.3086564649711363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3086564649711363e-05

Optimization complete. Final v2v error: 3.0182712078094482 mm

Highest mean error: 3.245695114135742 mm for frame 44

Lowest mean error: 2.701974391937256 mm for frame 125

Saving results

Total time: 134.9176197052002
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00884257
Iteration 2/25 | Loss: 0.00130698
Iteration 3/25 | Loss: 0.00099654
Iteration 4/25 | Loss: 0.00095438
Iteration 5/25 | Loss: 0.00095050
Iteration 6/25 | Loss: 0.00094980
Iteration 7/25 | Loss: 0.00094980
Iteration 8/25 | Loss: 0.00094980
Iteration 9/25 | Loss: 0.00094980
Iteration 10/25 | Loss: 0.00094980
Iteration 11/25 | Loss: 0.00094980
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009498023428022861, 0.0009498023428022861, 0.0009498023428022861, 0.0009498023428022861, 0.0009498023428022861]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009498023428022861

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85632598
Iteration 2/25 | Loss: 0.00045263
Iteration 3/25 | Loss: 0.00045262
Iteration 4/25 | Loss: 0.00045262
Iteration 5/25 | Loss: 0.00045262
Iteration 6/25 | Loss: 0.00045262
Iteration 7/25 | Loss: 0.00045262
Iteration 8/25 | Loss: 0.00045262
Iteration 9/25 | Loss: 0.00045262
Iteration 10/25 | Loss: 0.00045262
Iteration 11/25 | Loss: 0.00045262
Iteration 12/25 | Loss: 0.00045262
Iteration 13/25 | Loss: 0.00045262
Iteration 14/25 | Loss: 0.00045262
Iteration 15/25 | Loss: 0.00045262
Iteration 16/25 | Loss: 0.00045262
Iteration 17/25 | Loss: 0.00045262
Iteration 18/25 | Loss: 0.00045262
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0004526152042672038, 0.0004526152042672038, 0.0004526152042672038, 0.0004526152042672038, 0.0004526152042672038]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004526152042672038

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045262
Iteration 2/1000 | Loss: 0.00004341
Iteration 3/1000 | Loss: 0.00003181
Iteration 4/1000 | Loss: 0.00002829
Iteration 5/1000 | Loss: 0.00002616
Iteration 6/1000 | Loss: 0.00002518
Iteration 7/1000 | Loss: 0.00002436
Iteration 8/1000 | Loss: 0.00002401
Iteration 9/1000 | Loss: 0.00002370
Iteration 10/1000 | Loss: 0.00002344
Iteration 11/1000 | Loss: 0.00002328
Iteration 12/1000 | Loss: 0.00002314
Iteration 13/1000 | Loss: 0.00002298
Iteration 14/1000 | Loss: 0.00002298
Iteration 15/1000 | Loss: 0.00002298
Iteration 16/1000 | Loss: 0.00002298
Iteration 17/1000 | Loss: 0.00002298
Iteration 18/1000 | Loss: 0.00002297
Iteration 19/1000 | Loss: 0.00002297
Iteration 20/1000 | Loss: 0.00002297
Iteration 21/1000 | Loss: 0.00002296
Iteration 22/1000 | Loss: 0.00002296
Iteration 23/1000 | Loss: 0.00002295
Iteration 24/1000 | Loss: 0.00002295
Iteration 25/1000 | Loss: 0.00002292
Iteration 26/1000 | Loss: 0.00002291
Iteration 27/1000 | Loss: 0.00002291
Iteration 28/1000 | Loss: 0.00002291
Iteration 29/1000 | Loss: 0.00002290
Iteration 30/1000 | Loss: 0.00002290
Iteration 31/1000 | Loss: 0.00002290
Iteration 32/1000 | Loss: 0.00002290
Iteration 33/1000 | Loss: 0.00002290
Iteration 34/1000 | Loss: 0.00002289
Iteration 35/1000 | Loss: 0.00002289
Iteration 36/1000 | Loss: 0.00002289
Iteration 37/1000 | Loss: 0.00002289
Iteration 38/1000 | Loss: 0.00002289
Iteration 39/1000 | Loss: 0.00002289
Iteration 40/1000 | Loss: 0.00002289
Iteration 41/1000 | Loss: 0.00002288
Iteration 42/1000 | Loss: 0.00002288
Iteration 43/1000 | Loss: 0.00002288
Iteration 44/1000 | Loss: 0.00002288
Iteration 45/1000 | Loss: 0.00002288
Iteration 46/1000 | Loss: 0.00002288
Iteration 47/1000 | Loss: 0.00002288
Iteration 48/1000 | Loss: 0.00002288
Iteration 49/1000 | Loss: 0.00002286
Iteration 50/1000 | Loss: 0.00002286
Iteration 51/1000 | Loss: 0.00002286
Iteration 52/1000 | Loss: 0.00002285
Iteration 53/1000 | Loss: 0.00002285
Iteration 54/1000 | Loss: 0.00002285
Iteration 55/1000 | Loss: 0.00002285
Iteration 56/1000 | Loss: 0.00002285
Iteration 57/1000 | Loss: 0.00002285
Iteration 58/1000 | Loss: 0.00002285
Iteration 59/1000 | Loss: 0.00002285
Iteration 60/1000 | Loss: 0.00002285
Iteration 61/1000 | Loss: 0.00002285
Iteration 62/1000 | Loss: 0.00002285
Iteration 63/1000 | Loss: 0.00002285
Iteration 64/1000 | Loss: 0.00002285
Iteration 65/1000 | Loss: 0.00002285
Iteration 66/1000 | Loss: 0.00002284
Iteration 67/1000 | Loss: 0.00002283
Iteration 68/1000 | Loss: 0.00002283
Iteration 69/1000 | Loss: 0.00002283
Iteration 70/1000 | Loss: 0.00002282
Iteration 71/1000 | Loss: 0.00002282
Iteration 72/1000 | Loss: 0.00002282
Iteration 73/1000 | Loss: 0.00002282
Iteration 74/1000 | Loss: 0.00002282
Iteration 75/1000 | Loss: 0.00002282
Iteration 76/1000 | Loss: 0.00002282
Iteration 77/1000 | Loss: 0.00002282
Iteration 78/1000 | Loss: 0.00002282
Iteration 79/1000 | Loss: 0.00002282
Iteration 80/1000 | Loss: 0.00002282
Iteration 81/1000 | Loss: 0.00002282
Iteration 82/1000 | Loss: 0.00002282
Iteration 83/1000 | Loss: 0.00002281
Iteration 84/1000 | Loss: 0.00002281
Iteration 85/1000 | Loss: 0.00002281
Iteration 86/1000 | Loss: 0.00002281
Iteration 87/1000 | Loss: 0.00002281
Iteration 88/1000 | Loss: 0.00002281
Iteration 89/1000 | Loss: 0.00002281
Iteration 90/1000 | Loss: 0.00002281
Iteration 91/1000 | Loss: 0.00002281
Iteration 92/1000 | Loss: 0.00002281
Iteration 93/1000 | Loss: 0.00002280
Iteration 94/1000 | Loss: 0.00002280
Iteration 95/1000 | Loss: 0.00002280
Iteration 96/1000 | Loss: 0.00002280
Iteration 97/1000 | Loss: 0.00002279
Iteration 98/1000 | Loss: 0.00002279
Iteration 99/1000 | Loss: 0.00002278
Iteration 100/1000 | Loss: 0.00002278
Iteration 101/1000 | Loss: 0.00002278
Iteration 102/1000 | Loss: 0.00002277
Iteration 103/1000 | Loss: 0.00002277
Iteration 104/1000 | Loss: 0.00002277
Iteration 105/1000 | Loss: 0.00002277
Iteration 106/1000 | Loss: 0.00002277
Iteration 107/1000 | Loss: 0.00002276
Iteration 108/1000 | Loss: 0.00002276
Iteration 109/1000 | Loss: 0.00002276
Iteration 110/1000 | Loss: 0.00002276
Iteration 111/1000 | Loss: 0.00002275
Iteration 112/1000 | Loss: 0.00002275
Iteration 113/1000 | Loss: 0.00002275
Iteration 114/1000 | Loss: 0.00002275
Iteration 115/1000 | Loss: 0.00002275
Iteration 116/1000 | Loss: 0.00002274
Iteration 117/1000 | Loss: 0.00002274
Iteration 118/1000 | Loss: 0.00002274
Iteration 119/1000 | Loss: 0.00002274
Iteration 120/1000 | Loss: 0.00002274
Iteration 121/1000 | Loss: 0.00002274
Iteration 122/1000 | Loss: 0.00002274
Iteration 123/1000 | Loss: 0.00002274
Iteration 124/1000 | Loss: 0.00002274
Iteration 125/1000 | Loss: 0.00002273
Iteration 126/1000 | Loss: 0.00002273
Iteration 127/1000 | Loss: 0.00002272
Iteration 128/1000 | Loss: 0.00002272
Iteration 129/1000 | Loss: 0.00002272
Iteration 130/1000 | Loss: 0.00002271
Iteration 131/1000 | Loss: 0.00002271
Iteration 132/1000 | Loss: 0.00002271
Iteration 133/1000 | Loss: 0.00002271
Iteration 134/1000 | Loss: 0.00002271
Iteration 135/1000 | Loss: 0.00002271
Iteration 136/1000 | Loss: 0.00002271
Iteration 137/1000 | Loss: 0.00002271
Iteration 138/1000 | Loss: 0.00002271
Iteration 139/1000 | Loss: 0.00002271
Iteration 140/1000 | Loss: 0.00002270
Iteration 141/1000 | Loss: 0.00002270
Iteration 142/1000 | Loss: 0.00002270
Iteration 143/1000 | Loss: 0.00002270
Iteration 144/1000 | Loss: 0.00002269
Iteration 145/1000 | Loss: 0.00002269
Iteration 146/1000 | Loss: 0.00002269
Iteration 147/1000 | Loss: 0.00002269
Iteration 148/1000 | Loss: 0.00002269
Iteration 149/1000 | Loss: 0.00002269
Iteration 150/1000 | Loss: 0.00002269
Iteration 151/1000 | Loss: 0.00002269
Iteration 152/1000 | Loss: 0.00002268
Iteration 153/1000 | Loss: 0.00002268
Iteration 154/1000 | Loss: 0.00002268
Iteration 155/1000 | Loss: 0.00002266
Iteration 156/1000 | Loss: 0.00002266
Iteration 157/1000 | Loss: 0.00002266
Iteration 158/1000 | Loss: 0.00002266
Iteration 159/1000 | Loss: 0.00002266
Iteration 160/1000 | Loss: 0.00002266
Iteration 161/1000 | Loss: 0.00002266
Iteration 162/1000 | Loss: 0.00002266
Iteration 163/1000 | Loss: 0.00002265
Iteration 164/1000 | Loss: 0.00002265
Iteration 165/1000 | Loss: 0.00002265
Iteration 166/1000 | Loss: 0.00002265
Iteration 167/1000 | Loss: 0.00002265
Iteration 168/1000 | Loss: 0.00002265
Iteration 169/1000 | Loss: 0.00002264
Iteration 170/1000 | Loss: 0.00002264
Iteration 171/1000 | Loss: 0.00002264
Iteration 172/1000 | Loss: 0.00002264
Iteration 173/1000 | Loss: 0.00002264
Iteration 174/1000 | Loss: 0.00002264
Iteration 175/1000 | Loss: 0.00002264
Iteration 176/1000 | Loss: 0.00002264
Iteration 177/1000 | Loss: 0.00002264
Iteration 178/1000 | Loss: 0.00002264
Iteration 179/1000 | Loss: 0.00002264
Iteration 180/1000 | Loss: 0.00002264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.264234899485018e-05, 2.264234899485018e-05, 2.264234899485018e-05, 2.264234899485018e-05, 2.264234899485018e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.264234899485018e-05

Optimization complete. Final v2v error: 3.973940372467041 mm

Highest mean error: 4.477252006530762 mm for frame 0

Lowest mean error: 3.8521170616149902 mm for frame 117

Saving results

Total time: 38.689566135406494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046492
Iteration 2/25 | Loss: 0.00327612
Iteration 3/25 | Loss: 0.00187981
Iteration 4/25 | Loss: 0.00164182
Iteration 5/25 | Loss: 0.00172627
Iteration 6/25 | Loss: 0.00148575
Iteration 7/25 | Loss: 0.00132798
Iteration 8/25 | Loss: 0.00118947
Iteration 9/25 | Loss: 0.00113081
Iteration 10/25 | Loss: 0.00112268
Iteration 11/25 | Loss: 0.00111342
Iteration 12/25 | Loss: 0.00109708
Iteration 13/25 | Loss: 0.00108471
Iteration 14/25 | Loss: 0.00107448
Iteration 15/25 | Loss: 0.00106611
Iteration 16/25 | Loss: 0.00106749
Iteration 17/25 | Loss: 0.00106355
Iteration 18/25 | Loss: 0.00105633
Iteration 19/25 | Loss: 0.00104823
Iteration 20/25 | Loss: 0.00104969
Iteration 21/25 | Loss: 0.00104985
Iteration 22/25 | Loss: 0.00104888
Iteration 23/25 | Loss: 0.00104911
Iteration 24/25 | Loss: 0.00104449
Iteration 25/25 | Loss: 0.00104534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28239191
Iteration 2/25 | Loss: 0.00282971
Iteration 3/25 | Loss: 0.00276262
Iteration 4/25 | Loss: 0.00276262
Iteration 5/25 | Loss: 0.00276261
Iteration 6/25 | Loss: 0.00276261
Iteration 7/25 | Loss: 0.00276261
Iteration 8/25 | Loss: 0.00276261
Iteration 9/25 | Loss: 0.00276261
Iteration 10/25 | Loss: 0.00276261
Iteration 11/25 | Loss: 0.00276261
Iteration 12/25 | Loss: 0.00276261
Iteration 13/25 | Loss: 0.00276261
Iteration 14/25 | Loss: 0.00276261
Iteration 15/25 | Loss: 0.00276261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0027626128867268562, 0.0027626128867268562, 0.0027626128867268562, 0.0027626128867268562, 0.0027626128867268562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0027626128867268562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00276261
Iteration 2/1000 | Loss: 0.00169839
Iteration 3/1000 | Loss: 0.00082638
Iteration 4/1000 | Loss: 0.00092847
Iteration 5/1000 | Loss: 0.00052801
Iteration 6/1000 | Loss: 0.00123827
Iteration 7/1000 | Loss: 0.00095973
Iteration 8/1000 | Loss: 0.00147444
Iteration 9/1000 | Loss: 0.00080079
Iteration 10/1000 | Loss: 0.00022032
Iteration 11/1000 | Loss: 0.00033749
Iteration 12/1000 | Loss: 0.00069757
Iteration 13/1000 | Loss: 0.00092954
Iteration 14/1000 | Loss: 0.00059015
Iteration 15/1000 | Loss: 0.00088104
Iteration 16/1000 | Loss: 0.00108477
Iteration 17/1000 | Loss: 0.00093132
Iteration 18/1000 | Loss: 0.00071149
Iteration 19/1000 | Loss: 0.00112422
Iteration 20/1000 | Loss: 0.00034322
Iteration 21/1000 | Loss: 0.00036417
Iteration 22/1000 | Loss: 0.00073934
Iteration 23/1000 | Loss: 0.00066880
Iteration 24/1000 | Loss: 0.00053114
Iteration 25/1000 | Loss: 0.00042838
Iteration 26/1000 | Loss: 0.00042891
Iteration 27/1000 | Loss: 0.00034432
Iteration 28/1000 | Loss: 0.00030475
Iteration 29/1000 | Loss: 0.00056861
Iteration 30/1000 | Loss: 0.00067294
Iteration 31/1000 | Loss: 0.00060689
Iteration 32/1000 | Loss: 0.00065879
Iteration 33/1000 | Loss: 0.00106573
Iteration 34/1000 | Loss: 0.00190640
Iteration 35/1000 | Loss: 0.00054973
Iteration 36/1000 | Loss: 0.00126129
Iteration 37/1000 | Loss: 0.00294275
Iteration 38/1000 | Loss: 0.00176835
Iteration 39/1000 | Loss: 0.00211516
Iteration 40/1000 | Loss: 0.00049273
Iteration 41/1000 | Loss: 0.00047347
Iteration 42/1000 | Loss: 0.00027405
Iteration 43/1000 | Loss: 0.00040576
Iteration 44/1000 | Loss: 0.00021957
Iteration 45/1000 | Loss: 0.00018943
Iteration 46/1000 | Loss: 0.00035780
Iteration 47/1000 | Loss: 0.00043557
Iteration 48/1000 | Loss: 0.00021361
Iteration 49/1000 | Loss: 0.00010510
Iteration 50/1000 | Loss: 0.00052934
Iteration 51/1000 | Loss: 0.00031064
Iteration 52/1000 | Loss: 0.00026269
Iteration 53/1000 | Loss: 0.00026364
Iteration 54/1000 | Loss: 0.00024688
Iteration 55/1000 | Loss: 0.00043549
Iteration 56/1000 | Loss: 0.00035593
Iteration 57/1000 | Loss: 0.00025906
Iteration 58/1000 | Loss: 0.00041270
Iteration 59/1000 | Loss: 0.00066639
Iteration 60/1000 | Loss: 0.00089944
Iteration 61/1000 | Loss: 0.00065118
Iteration 62/1000 | Loss: 0.00076376
Iteration 63/1000 | Loss: 0.00058035
Iteration 64/1000 | Loss: 0.00018705
Iteration 65/1000 | Loss: 0.00018366
Iteration 66/1000 | Loss: 0.00037526
Iteration 67/1000 | Loss: 0.00015606
Iteration 68/1000 | Loss: 0.00035412
Iteration 69/1000 | Loss: 0.00031861
Iteration 70/1000 | Loss: 0.00079137
Iteration 71/1000 | Loss: 0.00062685
Iteration 72/1000 | Loss: 0.00040347
Iteration 73/1000 | Loss: 0.00039838
Iteration 74/1000 | Loss: 0.00041262
Iteration 75/1000 | Loss: 0.00050300
Iteration 76/1000 | Loss: 0.00025873
Iteration 77/1000 | Loss: 0.00050488
Iteration 78/1000 | Loss: 0.00071889
Iteration 79/1000 | Loss: 0.00032667
Iteration 80/1000 | Loss: 0.00062870
Iteration 81/1000 | Loss: 0.00040151
Iteration 82/1000 | Loss: 0.00102146
Iteration 83/1000 | Loss: 0.00022937
Iteration 84/1000 | Loss: 0.00009089
Iteration 85/1000 | Loss: 0.00005710
Iteration 86/1000 | Loss: 0.00017029
Iteration 87/1000 | Loss: 0.00019091
Iteration 88/1000 | Loss: 0.00014563
Iteration 89/1000 | Loss: 0.00027834
Iteration 90/1000 | Loss: 0.00022204
Iteration 91/1000 | Loss: 0.00023963
Iteration 92/1000 | Loss: 0.00025179
Iteration 93/1000 | Loss: 0.00017070
Iteration 94/1000 | Loss: 0.00017346
Iteration 95/1000 | Loss: 0.00016149
Iteration 96/1000 | Loss: 0.00018623
Iteration 97/1000 | Loss: 0.00018246
Iteration 98/1000 | Loss: 0.00029458
Iteration 99/1000 | Loss: 0.00017698
Iteration 100/1000 | Loss: 0.00032498
Iteration 101/1000 | Loss: 0.00020966
Iteration 102/1000 | Loss: 0.00006334
Iteration 103/1000 | Loss: 0.00011943
Iteration 104/1000 | Loss: 0.00004845
Iteration 105/1000 | Loss: 0.00003791
Iteration 106/1000 | Loss: 0.00003534
Iteration 107/1000 | Loss: 0.00004281
Iteration 108/1000 | Loss: 0.00010360
Iteration 109/1000 | Loss: 0.00007144
Iteration 110/1000 | Loss: 0.00005480
Iteration 111/1000 | Loss: 0.00005438
Iteration 112/1000 | Loss: 0.00015396
Iteration 113/1000 | Loss: 0.00012881
Iteration 114/1000 | Loss: 0.00003899
Iteration 115/1000 | Loss: 0.00005859
Iteration 116/1000 | Loss: 0.00013104
Iteration 117/1000 | Loss: 0.00006130
Iteration 118/1000 | Loss: 0.00003683
Iteration 119/1000 | Loss: 0.00006001
Iteration 120/1000 | Loss: 0.00004677
Iteration 121/1000 | Loss: 0.00011159
Iteration 122/1000 | Loss: 0.00009804
Iteration 123/1000 | Loss: 0.00009550
Iteration 124/1000 | Loss: 0.00009687
Iteration 125/1000 | Loss: 0.00014266
Iteration 126/1000 | Loss: 0.00009512
Iteration 127/1000 | Loss: 0.00014061
Iteration 128/1000 | Loss: 0.00013844
Iteration 129/1000 | Loss: 0.00015033
Iteration 130/1000 | Loss: 0.00013375
Iteration 131/1000 | Loss: 0.00007871
Iteration 132/1000 | Loss: 0.00010844
Iteration 133/1000 | Loss: 0.00004874
Iteration 134/1000 | Loss: 0.00009579
Iteration 135/1000 | Loss: 0.00008646
Iteration 136/1000 | Loss: 0.00012239
Iteration 137/1000 | Loss: 0.00012492
Iteration 138/1000 | Loss: 0.00013487
Iteration 139/1000 | Loss: 0.00014520
Iteration 140/1000 | Loss: 0.00013326
Iteration 141/1000 | Loss: 0.00006455
Iteration 142/1000 | Loss: 0.00006344
Iteration 143/1000 | Loss: 0.00004026
Iteration 144/1000 | Loss: 0.00003879
Iteration 145/1000 | Loss: 0.00004446
Iteration 146/1000 | Loss: 0.00003426
Iteration 147/1000 | Loss: 0.00004263
Iteration 148/1000 | Loss: 0.00003539
Iteration 149/1000 | Loss: 0.00003610
Iteration 150/1000 | Loss: 0.00002730
Iteration 151/1000 | Loss: 0.00004003
Iteration 152/1000 | Loss: 0.00006050
Iteration 153/1000 | Loss: 0.00004380
Iteration 154/1000 | Loss: 0.00003928
Iteration 155/1000 | Loss: 0.00004373
Iteration 156/1000 | Loss: 0.00005141
Iteration 157/1000 | Loss: 0.00004571
Iteration 158/1000 | Loss: 0.00004217
Iteration 159/1000 | Loss: 0.00005283
Iteration 160/1000 | Loss: 0.00004456
Iteration 161/1000 | Loss: 0.00004083
Iteration 162/1000 | Loss: 0.00004067
Iteration 163/1000 | Loss: 0.00003668
Iteration 164/1000 | Loss: 0.00003799
Iteration 165/1000 | Loss: 0.00004093
Iteration 166/1000 | Loss: 0.00004695
Iteration 167/1000 | Loss: 0.00003444
Iteration 168/1000 | Loss: 0.00003433
Iteration 169/1000 | Loss: 0.00003508
Iteration 170/1000 | Loss: 0.00004744
Iteration 171/1000 | Loss: 0.00004217
Iteration 172/1000 | Loss: 0.00004815
Iteration 173/1000 | Loss: 0.00003621
Iteration 174/1000 | Loss: 0.00004165
Iteration 175/1000 | Loss: 0.00003265
Iteration 176/1000 | Loss: 0.00003849
Iteration 177/1000 | Loss: 0.00003516
Iteration 178/1000 | Loss: 0.00003834
Iteration 179/1000 | Loss: 0.00003919
Iteration 180/1000 | Loss: 0.00004298
Iteration 181/1000 | Loss: 0.00003818
Iteration 182/1000 | Loss: 0.00004920
Iteration 183/1000 | Loss: 0.00002183
Iteration 184/1000 | Loss: 0.00003813
Iteration 185/1000 | Loss: 0.00003623
Iteration 186/1000 | Loss: 0.00004083
Iteration 187/1000 | Loss: 0.00003615
Iteration 188/1000 | Loss: 0.00004169
Iteration 189/1000 | Loss: 0.00003692
Iteration 190/1000 | Loss: 0.00004124
Iteration 191/1000 | Loss: 0.00003616
Iteration 192/1000 | Loss: 0.00004184
Iteration 193/1000 | Loss: 0.00003549
Iteration 194/1000 | Loss: 0.00003828
Iteration 195/1000 | Loss: 0.00003435
Iteration 196/1000 | Loss: 0.00004526
Iteration 197/1000 | Loss: 0.00003708
Iteration 198/1000 | Loss: 0.00004343
Iteration 199/1000 | Loss: 0.00004092
Iteration 200/1000 | Loss: 0.00002180
Iteration 201/1000 | Loss: 0.00003209
Iteration 202/1000 | Loss: 0.00002902
Iteration 203/1000 | Loss: 0.00003800
Iteration 204/1000 | Loss: 0.00003151
Iteration 205/1000 | Loss: 0.00003413
Iteration 206/1000 | Loss: 0.00004616
Iteration 207/1000 | Loss: 0.00002970
Iteration 208/1000 | Loss: 0.00003542
Iteration 209/1000 | Loss: 0.00003496
Iteration 210/1000 | Loss: 0.00003966
Iteration 211/1000 | Loss: 0.00003251
Iteration 212/1000 | Loss: 0.00002862
Iteration 213/1000 | Loss: 0.00002538
Iteration 214/1000 | Loss: 0.00003083
Iteration 215/1000 | Loss: 0.00003206
Iteration 216/1000 | Loss: 0.00003355
Iteration 217/1000 | Loss: 0.00002649
Iteration 218/1000 | Loss: 0.00003343
Iteration 219/1000 | Loss: 0.00002881
Iteration 220/1000 | Loss: 0.00003180
Iteration 221/1000 | Loss: 0.00003631
Iteration 222/1000 | Loss: 0.00003453
Iteration 223/1000 | Loss: 0.00003181
Iteration 224/1000 | Loss: 0.00003375
Iteration 225/1000 | Loss: 0.00003312
Iteration 226/1000 | Loss: 0.00002951
Iteration 227/1000 | Loss: 0.00002764
Iteration 228/1000 | Loss: 0.00003530
Iteration 229/1000 | Loss: 0.00002215
Iteration 230/1000 | Loss: 0.00003203
Iteration 231/1000 | Loss: 0.00002998
Iteration 232/1000 | Loss: 0.00003894
Iteration 233/1000 | Loss: 0.00003036
Iteration 234/1000 | Loss: 0.00003445
Iteration 235/1000 | Loss: 0.00003744
Iteration 236/1000 | Loss: 0.00003090
Iteration 237/1000 | Loss: 0.00003275
Iteration 238/1000 | Loss: 0.00006727
Iteration 239/1000 | Loss: 0.00002922
Iteration 240/1000 | Loss: 0.00002894
Iteration 241/1000 | Loss: 0.00003381
Iteration 242/1000 | Loss: 0.00002882
Iteration 243/1000 | Loss: 0.00002738
Iteration 244/1000 | Loss: 0.00001589
Iteration 245/1000 | Loss: 0.00002176
Iteration 246/1000 | Loss: 0.00001397
Iteration 247/1000 | Loss: 0.00001337
Iteration 248/1000 | Loss: 0.00001305
Iteration 249/1000 | Loss: 0.00001858
Iteration 250/1000 | Loss: 0.00001466
Iteration 251/1000 | Loss: 0.00001737
Iteration 252/1000 | Loss: 0.00001272
Iteration 253/1000 | Loss: 0.00001271
Iteration 254/1000 | Loss: 0.00001270
Iteration 255/1000 | Loss: 0.00001270
Iteration 256/1000 | Loss: 0.00001270
Iteration 257/1000 | Loss: 0.00001270
Iteration 258/1000 | Loss: 0.00001270
Iteration 259/1000 | Loss: 0.00001270
Iteration 260/1000 | Loss: 0.00001270
Iteration 261/1000 | Loss: 0.00001270
Iteration 262/1000 | Loss: 0.00001270
Iteration 263/1000 | Loss: 0.00001270
Iteration 264/1000 | Loss: 0.00001270
Iteration 265/1000 | Loss: 0.00001269
Iteration 266/1000 | Loss: 0.00001269
Iteration 267/1000 | Loss: 0.00001269
Iteration 268/1000 | Loss: 0.00001269
Iteration 269/1000 | Loss: 0.00001269
Iteration 270/1000 | Loss: 0.00001269
Iteration 271/1000 | Loss: 0.00001269
Iteration 272/1000 | Loss: 0.00001269
Iteration 273/1000 | Loss: 0.00001267
Iteration 274/1000 | Loss: 0.00002209
Iteration 275/1000 | Loss: 0.00001400
Iteration 276/1000 | Loss: 0.00001257
Iteration 277/1000 | Loss: 0.00001250
Iteration 278/1000 | Loss: 0.00001248
Iteration 279/1000 | Loss: 0.00001247
Iteration 280/1000 | Loss: 0.00001247
Iteration 281/1000 | Loss: 0.00001246
Iteration 282/1000 | Loss: 0.00001245
Iteration 283/1000 | Loss: 0.00001239
Iteration 284/1000 | Loss: 0.00001239
Iteration 285/1000 | Loss: 0.00001239
Iteration 286/1000 | Loss: 0.00001239
Iteration 287/1000 | Loss: 0.00001239
Iteration 288/1000 | Loss: 0.00001239
Iteration 289/1000 | Loss: 0.00001239
Iteration 290/1000 | Loss: 0.00001239
Iteration 291/1000 | Loss: 0.00001239
Iteration 292/1000 | Loss: 0.00001238
Iteration 293/1000 | Loss: 0.00001238
Iteration 294/1000 | Loss: 0.00001238
Iteration 295/1000 | Loss: 0.00001238
Iteration 296/1000 | Loss: 0.00001237
Iteration 297/1000 | Loss: 0.00001234
Iteration 298/1000 | Loss: 0.00001231
Iteration 299/1000 | Loss: 0.00001230
Iteration 300/1000 | Loss: 0.00001230
Iteration 301/1000 | Loss: 0.00001230
Iteration 302/1000 | Loss: 0.00001229
Iteration 303/1000 | Loss: 0.00001229
Iteration 304/1000 | Loss: 0.00001229
Iteration 305/1000 | Loss: 0.00001229
Iteration 306/1000 | Loss: 0.00001229
Iteration 307/1000 | Loss: 0.00001228
Iteration 308/1000 | Loss: 0.00001228
Iteration 309/1000 | Loss: 0.00001228
Iteration 310/1000 | Loss: 0.00001228
Iteration 311/1000 | Loss: 0.00001227
Iteration 312/1000 | Loss: 0.00001227
Iteration 313/1000 | Loss: 0.00001227
Iteration 314/1000 | Loss: 0.00001227
Iteration 315/1000 | Loss: 0.00001227
Iteration 316/1000 | Loss: 0.00001227
Iteration 317/1000 | Loss: 0.00001227
Iteration 318/1000 | Loss: 0.00001227
Iteration 319/1000 | Loss: 0.00001227
Iteration 320/1000 | Loss: 0.00001227
Iteration 321/1000 | Loss: 0.00001226
Iteration 322/1000 | Loss: 0.00001226
Iteration 323/1000 | Loss: 0.00001226
Iteration 324/1000 | Loss: 0.00001226
Iteration 325/1000 | Loss: 0.00001226
Iteration 326/1000 | Loss: 0.00001226
Iteration 327/1000 | Loss: 0.00001225
Iteration 328/1000 | Loss: 0.00001225
Iteration 329/1000 | Loss: 0.00001225
Iteration 330/1000 | Loss: 0.00001225
Iteration 331/1000 | Loss: 0.00001224
Iteration 332/1000 | Loss: 0.00001224
Iteration 333/1000 | Loss: 0.00001224
Iteration 334/1000 | Loss: 0.00001224
Iteration 335/1000 | Loss: 0.00001223
Iteration 336/1000 | Loss: 0.00001223
Iteration 337/1000 | Loss: 0.00001223
Iteration 338/1000 | Loss: 0.00001222
Iteration 339/1000 | Loss: 0.00001222
Iteration 340/1000 | Loss: 0.00001222
Iteration 341/1000 | Loss: 0.00001222
Iteration 342/1000 | Loss: 0.00001221
Iteration 343/1000 | Loss: 0.00001221
Iteration 344/1000 | Loss: 0.00001221
Iteration 345/1000 | Loss: 0.00001221
Iteration 346/1000 | Loss: 0.00001221
Iteration 347/1000 | Loss: 0.00001221
Iteration 348/1000 | Loss: 0.00001221
Iteration 349/1000 | Loss: 0.00001221
Iteration 350/1000 | Loss: 0.00001221
Iteration 351/1000 | Loss: 0.00001221
Iteration 352/1000 | Loss: 0.00001220
Iteration 353/1000 | Loss: 0.00001220
Iteration 354/1000 | Loss: 0.00001220
Iteration 355/1000 | Loss: 0.00001220
Iteration 356/1000 | Loss: 0.00001220
Iteration 357/1000 | Loss: 0.00001220
Iteration 358/1000 | Loss: 0.00001220
Iteration 359/1000 | Loss: 0.00001220
Iteration 360/1000 | Loss: 0.00001220
Iteration 361/1000 | Loss: 0.00001219
Iteration 362/1000 | Loss: 0.00001219
Iteration 363/1000 | Loss: 0.00001219
Iteration 364/1000 | Loss: 0.00001219
Iteration 365/1000 | Loss: 0.00001219
Iteration 366/1000 | Loss: 0.00001219
Iteration 367/1000 | Loss: 0.00001219
Iteration 368/1000 | Loss: 0.00001219
Iteration 369/1000 | Loss: 0.00001219
Iteration 370/1000 | Loss: 0.00001218
Iteration 371/1000 | Loss: 0.00001218
Iteration 372/1000 | Loss: 0.00001218
Iteration 373/1000 | Loss: 0.00001218
Iteration 374/1000 | Loss: 0.00001218
Iteration 375/1000 | Loss: 0.00001218
Iteration 376/1000 | Loss: 0.00001218
Iteration 377/1000 | Loss: 0.00001218
Iteration 378/1000 | Loss: 0.00001217
Iteration 379/1000 | Loss: 0.00001217
Iteration 380/1000 | Loss: 0.00001217
Iteration 381/1000 | Loss: 0.00001217
Iteration 382/1000 | Loss: 0.00001217
Iteration 383/1000 | Loss: 0.00001217
Iteration 384/1000 | Loss: 0.00001217
Iteration 385/1000 | Loss: 0.00001217
Iteration 386/1000 | Loss: 0.00001217
Iteration 387/1000 | Loss: 0.00001217
Iteration 388/1000 | Loss: 0.00001216
Iteration 389/1000 | Loss: 0.00001216
Iteration 390/1000 | Loss: 0.00001216
Iteration 391/1000 | Loss: 0.00001216
Iteration 392/1000 | Loss: 0.00001216
Iteration 393/1000 | Loss: 0.00001216
Iteration 394/1000 | Loss: 0.00001216
Iteration 395/1000 | Loss: 0.00001216
Iteration 396/1000 | Loss: 0.00001216
Iteration 397/1000 | Loss: 0.00001216
Iteration 398/1000 | Loss: 0.00001216
Iteration 399/1000 | Loss: 0.00001216
Iteration 400/1000 | Loss: 0.00001216
Iteration 401/1000 | Loss: 0.00001216
Iteration 402/1000 | Loss: 0.00001216
Iteration 403/1000 | Loss: 0.00001216
Iteration 404/1000 | Loss: 0.00001216
Iteration 405/1000 | Loss: 0.00001216
Iteration 406/1000 | Loss: 0.00001216
Iteration 407/1000 | Loss: 0.00001216
Iteration 408/1000 | Loss: 0.00001216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 408. Stopping optimization.
Last 5 losses: [1.216123655467527e-05, 1.216123655467527e-05, 1.216123655467527e-05, 1.216123655467527e-05, 1.216123655467527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.216123655467527e-05

Optimization complete. Final v2v error: 2.4426610469818115 mm

Highest mean error: 20.158246994018555 mm for frame 54

Lowest mean error: 2.0393779277801514 mm for frame 19

Saving results

Total time: 473.27750539779663
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407856
Iteration 2/25 | Loss: 0.00098246
Iteration 3/25 | Loss: 0.00085432
Iteration 4/25 | Loss: 0.00084171
Iteration 5/25 | Loss: 0.00083713
Iteration 6/25 | Loss: 0.00083581
Iteration 7/25 | Loss: 0.00083567
Iteration 8/25 | Loss: 0.00083567
Iteration 9/25 | Loss: 0.00083567
Iteration 10/25 | Loss: 0.00083567
Iteration 11/25 | Loss: 0.00083567
Iteration 12/25 | Loss: 0.00083567
Iteration 13/25 | Loss: 0.00083567
Iteration 14/25 | Loss: 0.00083567
Iteration 15/25 | Loss: 0.00083567
Iteration 16/25 | Loss: 0.00083567
Iteration 17/25 | Loss: 0.00083567
Iteration 18/25 | Loss: 0.00083567
Iteration 19/25 | Loss: 0.00083567
Iteration 20/25 | Loss: 0.00083567
Iteration 21/25 | Loss: 0.00083567
Iteration 22/25 | Loss: 0.00083567
Iteration 23/25 | Loss: 0.00083567
Iteration 24/25 | Loss: 0.00083567
Iteration 25/25 | Loss: 0.00083567

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29191220
Iteration 2/25 | Loss: 0.00068041
Iteration 3/25 | Loss: 0.00068041
Iteration 4/25 | Loss: 0.00068041
Iteration 5/25 | Loss: 0.00068041
Iteration 6/25 | Loss: 0.00068041
Iteration 7/25 | Loss: 0.00068041
Iteration 8/25 | Loss: 0.00068041
Iteration 9/25 | Loss: 0.00068041
Iteration 10/25 | Loss: 0.00068041
Iteration 11/25 | Loss: 0.00068041
Iteration 12/25 | Loss: 0.00068041
Iteration 13/25 | Loss: 0.00068041
Iteration 14/25 | Loss: 0.00068041
Iteration 15/25 | Loss: 0.00068041
Iteration 16/25 | Loss: 0.00068041
Iteration 17/25 | Loss: 0.00068041
Iteration 18/25 | Loss: 0.00068041
Iteration 19/25 | Loss: 0.00068041
Iteration 20/25 | Loss: 0.00068041
Iteration 21/25 | Loss: 0.00068041
Iteration 22/25 | Loss: 0.00068041
Iteration 23/25 | Loss: 0.00068041
Iteration 24/25 | Loss: 0.00068041
Iteration 25/25 | Loss: 0.00068041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068041
Iteration 2/1000 | Loss: 0.00003310
Iteration 3/1000 | Loss: 0.00001542
Iteration 4/1000 | Loss: 0.00001330
Iteration 5/1000 | Loss: 0.00001256
Iteration 6/1000 | Loss: 0.00001208
Iteration 7/1000 | Loss: 0.00001155
Iteration 8/1000 | Loss: 0.00001119
Iteration 9/1000 | Loss: 0.00001084
Iteration 10/1000 | Loss: 0.00001082
Iteration 11/1000 | Loss: 0.00001050
Iteration 12/1000 | Loss: 0.00001048
Iteration 13/1000 | Loss: 0.00001047
Iteration 14/1000 | Loss: 0.00001032
Iteration 15/1000 | Loss: 0.00001026
Iteration 16/1000 | Loss: 0.00001025
Iteration 17/1000 | Loss: 0.00001023
Iteration 18/1000 | Loss: 0.00001021
Iteration 19/1000 | Loss: 0.00001021
Iteration 20/1000 | Loss: 0.00001020
Iteration 21/1000 | Loss: 0.00001020
Iteration 22/1000 | Loss: 0.00001019
Iteration 23/1000 | Loss: 0.00001018
Iteration 24/1000 | Loss: 0.00001018
Iteration 25/1000 | Loss: 0.00001018
Iteration 26/1000 | Loss: 0.00001015
Iteration 27/1000 | Loss: 0.00001015
Iteration 28/1000 | Loss: 0.00001013
Iteration 29/1000 | Loss: 0.00001013
Iteration 30/1000 | Loss: 0.00001013
Iteration 31/1000 | Loss: 0.00001012
Iteration 32/1000 | Loss: 0.00001012
Iteration 33/1000 | Loss: 0.00001012
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001012
Iteration 36/1000 | Loss: 0.00001012
Iteration 37/1000 | Loss: 0.00001011
Iteration 38/1000 | Loss: 0.00001010
Iteration 39/1000 | Loss: 0.00001010
Iteration 40/1000 | Loss: 0.00001010
Iteration 41/1000 | Loss: 0.00001010
Iteration 42/1000 | Loss: 0.00001010
Iteration 43/1000 | Loss: 0.00001010
Iteration 44/1000 | Loss: 0.00001010
Iteration 45/1000 | Loss: 0.00001009
Iteration 46/1000 | Loss: 0.00001009
Iteration 47/1000 | Loss: 0.00001009
Iteration 48/1000 | Loss: 0.00001009
Iteration 49/1000 | Loss: 0.00001009
Iteration 50/1000 | Loss: 0.00001009
Iteration 51/1000 | Loss: 0.00001009
Iteration 52/1000 | Loss: 0.00001009
Iteration 53/1000 | Loss: 0.00001009
Iteration 54/1000 | Loss: 0.00001009
Iteration 55/1000 | Loss: 0.00001009
Iteration 56/1000 | Loss: 0.00001009
Iteration 57/1000 | Loss: 0.00001008
Iteration 58/1000 | Loss: 0.00001008
Iteration 59/1000 | Loss: 0.00001008
Iteration 60/1000 | Loss: 0.00001008
Iteration 61/1000 | Loss: 0.00001008
Iteration 62/1000 | Loss: 0.00001008
Iteration 63/1000 | Loss: 0.00001007
Iteration 64/1000 | Loss: 0.00001007
Iteration 65/1000 | Loss: 0.00001007
Iteration 66/1000 | Loss: 0.00001007
Iteration 67/1000 | Loss: 0.00001007
Iteration 68/1000 | Loss: 0.00001007
Iteration 69/1000 | Loss: 0.00001007
Iteration 70/1000 | Loss: 0.00001007
Iteration 71/1000 | Loss: 0.00001007
Iteration 72/1000 | Loss: 0.00001007
Iteration 73/1000 | Loss: 0.00001007
Iteration 74/1000 | Loss: 0.00001007
Iteration 75/1000 | Loss: 0.00001007
Iteration 76/1000 | Loss: 0.00001007
Iteration 77/1000 | Loss: 0.00001007
Iteration 78/1000 | Loss: 0.00001007
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.006602087727515e-05, 1.006602087727515e-05, 1.006602087727515e-05, 1.006602087727515e-05, 1.006602087727515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.006602087727515e-05

Optimization complete. Final v2v error: 2.6569485664367676 mm

Highest mean error: 2.9618122577667236 mm for frame 40

Lowest mean error: 2.391101598739624 mm for frame 134

Saving results

Total time: 30.415258407592773
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795085
Iteration 2/25 | Loss: 0.00125079
Iteration 3/25 | Loss: 0.00089827
Iteration 4/25 | Loss: 0.00087304
Iteration 5/25 | Loss: 0.00085388
Iteration 6/25 | Loss: 0.00084946
Iteration 7/25 | Loss: 0.00084816
Iteration 8/25 | Loss: 0.00084797
Iteration 9/25 | Loss: 0.00084745
Iteration 10/25 | Loss: 0.00084753
Iteration 11/25 | Loss: 0.00084738
Iteration 12/25 | Loss: 0.00084738
Iteration 13/25 | Loss: 0.00084738
Iteration 14/25 | Loss: 0.00084738
Iteration 15/25 | Loss: 0.00084738
Iteration 16/25 | Loss: 0.00084738
Iteration 17/25 | Loss: 0.00084738
Iteration 18/25 | Loss: 0.00084738
Iteration 19/25 | Loss: 0.00084738
Iteration 20/25 | Loss: 0.00084738
Iteration 21/25 | Loss: 0.00084737
Iteration 22/25 | Loss: 0.00084737
Iteration 23/25 | Loss: 0.00084737
Iteration 24/25 | Loss: 0.00084737
Iteration 25/25 | Loss: 0.00084737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60921693
Iteration 2/25 | Loss: 0.00073102
Iteration 3/25 | Loss: 0.00073100
Iteration 4/25 | Loss: 0.00073100
Iteration 5/25 | Loss: 0.00073100
Iteration 6/25 | Loss: 0.00073100
Iteration 7/25 | Loss: 0.00073100
Iteration 8/25 | Loss: 0.00073100
Iteration 9/25 | Loss: 0.00073100
Iteration 10/25 | Loss: 0.00073100
Iteration 11/25 | Loss: 0.00073100
Iteration 12/25 | Loss: 0.00073100
Iteration 13/25 | Loss: 0.00073100
Iteration 14/25 | Loss: 0.00073100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007309977081604302, 0.0007309977081604302, 0.0007309977081604302, 0.0007309977081604302, 0.0007309977081604302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007309977081604302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073100
Iteration 2/1000 | Loss: 0.00002709
Iteration 3/1000 | Loss: 0.00002011
Iteration 4/1000 | Loss: 0.00001507
Iteration 5/1000 | Loss: 0.00001391
Iteration 6/1000 | Loss: 0.00001497
Iteration 7/1000 | Loss: 0.00001292
Iteration 8/1000 | Loss: 0.00001540
Iteration 9/1000 | Loss: 0.00001247
Iteration 10/1000 | Loss: 0.00001223
Iteration 11/1000 | Loss: 0.00001222
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001206
Iteration 14/1000 | Loss: 0.00001206
Iteration 15/1000 | Loss: 0.00001205
Iteration 16/1000 | Loss: 0.00001205
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001224
Iteration 20/1000 | Loss: 0.00001198
Iteration 21/1000 | Loss: 0.00001198
Iteration 22/1000 | Loss: 0.00001198
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001197
Iteration 27/1000 | Loss: 0.00001197
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001197
Iteration 31/1000 | Loss: 0.00001197
Iteration 32/1000 | Loss: 0.00001197
Iteration 33/1000 | Loss: 0.00001196
Iteration 34/1000 | Loss: 0.00001196
Iteration 35/1000 | Loss: 0.00001196
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001193
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001193
Iteration 41/1000 | Loss: 0.00001192
Iteration 42/1000 | Loss: 0.00001192
Iteration 43/1000 | Loss: 0.00001190
Iteration 44/1000 | Loss: 0.00001190
Iteration 45/1000 | Loss: 0.00001189
Iteration 46/1000 | Loss: 0.00001189
Iteration 47/1000 | Loss: 0.00001188
Iteration 48/1000 | Loss: 0.00001188
Iteration 49/1000 | Loss: 0.00001270
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001197
Iteration 52/1000 | Loss: 0.00001183
Iteration 53/1000 | Loss: 0.00001183
Iteration 54/1000 | Loss: 0.00001182
Iteration 55/1000 | Loss: 0.00001182
Iteration 56/1000 | Loss: 0.00001182
Iteration 57/1000 | Loss: 0.00001182
Iteration 58/1000 | Loss: 0.00001181
Iteration 59/1000 | Loss: 0.00001181
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001180
Iteration 68/1000 | Loss: 0.00001180
Iteration 69/1000 | Loss: 0.00001180
Iteration 70/1000 | Loss: 0.00001180
Iteration 71/1000 | Loss: 0.00001180
Iteration 72/1000 | Loss: 0.00001179
Iteration 73/1000 | Loss: 0.00001179
Iteration 74/1000 | Loss: 0.00001179
Iteration 75/1000 | Loss: 0.00001179
Iteration 76/1000 | Loss: 0.00001179
Iteration 77/1000 | Loss: 0.00001179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 77. Stopping optimization.
Last 5 losses: [1.1794712008850183e-05, 1.1794712008850183e-05, 1.1794712008850183e-05, 1.1794712008850183e-05, 1.1794712008850183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1794712008850183e-05

Optimization complete. Final v2v error: 2.8695967197418213 mm

Highest mean error: 3.225688934326172 mm for frame 98

Lowest mean error: 2.4758009910583496 mm for frame 37

Saving results

Total time: 46.86768364906311
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00873759
Iteration 2/25 | Loss: 0.00103304
Iteration 3/25 | Loss: 0.00087707
Iteration 4/25 | Loss: 0.00086180
Iteration 5/25 | Loss: 0.00085875
Iteration 6/25 | Loss: 0.00085805
Iteration 7/25 | Loss: 0.00085805
Iteration 8/25 | Loss: 0.00085805
Iteration 9/25 | Loss: 0.00085805
Iteration 10/25 | Loss: 0.00085805
Iteration 11/25 | Loss: 0.00085805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008580487919971347, 0.0008580487919971347, 0.0008580487919971347, 0.0008580487919971347, 0.0008580487919971347]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008580487919971347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38983393
Iteration 2/25 | Loss: 0.00065459
Iteration 3/25 | Loss: 0.00065459
Iteration 4/25 | Loss: 0.00065459
Iteration 5/25 | Loss: 0.00065459
Iteration 6/25 | Loss: 0.00065459
Iteration 7/25 | Loss: 0.00065459
Iteration 8/25 | Loss: 0.00065459
Iteration 9/25 | Loss: 0.00065459
Iteration 10/25 | Loss: 0.00065459
Iteration 11/25 | Loss: 0.00065459
Iteration 12/25 | Loss: 0.00065459
Iteration 13/25 | Loss: 0.00065459
Iteration 14/25 | Loss: 0.00065459
Iteration 15/25 | Loss: 0.00065459
Iteration 16/25 | Loss: 0.00065459
Iteration 17/25 | Loss: 0.00065459
Iteration 18/25 | Loss: 0.00065459
Iteration 19/25 | Loss: 0.00065459
Iteration 20/25 | Loss: 0.00065459
Iteration 21/25 | Loss: 0.00065459
Iteration 22/25 | Loss: 0.00065459
Iteration 23/25 | Loss: 0.00065459
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0006545857759192586, 0.0006545857759192586, 0.0006545857759192586, 0.0006545857759192586, 0.0006545857759192586]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006545857759192586

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065459
Iteration 2/1000 | Loss: 0.00003217
Iteration 3/1000 | Loss: 0.00001517
Iteration 4/1000 | Loss: 0.00001298
Iteration 5/1000 | Loss: 0.00001124
Iteration 6/1000 | Loss: 0.00001060
Iteration 7/1000 | Loss: 0.00001012
Iteration 8/1000 | Loss: 0.00001003
Iteration 9/1000 | Loss: 0.00000993
Iteration 10/1000 | Loss: 0.00000973
Iteration 11/1000 | Loss: 0.00000958
Iteration 12/1000 | Loss: 0.00000948
Iteration 13/1000 | Loss: 0.00000941
Iteration 14/1000 | Loss: 0.00000940
Iteration 15/1000 | Loss: 0.00000934
Iteration 16/1000 | Loss: 0.00000932
Iteration 17/1000 | Loss: 0.00000931
Iteration 18/1000 | Loss: 0.00000926
Iteration 19/1000 | Loss: 0.00000926
Iteration 20/1000 | Loss: 0.00000925
Iteration 21/1000 | Loss: 0.00000924
Iteration 22/1000 | Loss: 0.00000924
Iteration 23/1000 | Loss: 0.00000923
Iteration 24/1000 | Loss: 0.00000922
Iteration 25/1000 | Loss: 0.00000920
Iteration 26/1000 | Loss: 0.00000920
Iteration 27/1000 | Loss: 0.00000920
Iteration 28/1000 | Loss: 0.00000920
Iteration 29/1000 | Loss: 0.00000919
Iteration 30/1000 | Loss: 0.00000919
Iteration 31/1000 | Loss: 0.00000918
Iteration 32/1000 | Loss: 0.00000918
Iteration 33/1000 | Loss: 0.00000916
Iteration 34/1000 | Loss: 0.00000916
Iteration 35/1000 | Loss: 0.00000915
Iteration 36/1000 | Loss: 0.00000915
Iteration 37/1000 | Loss: 0.00000915
Iteration 38/1000 | Loss: 0.00000913
Iteration 39/1000 | Loss: 0.00000912
Iteration 40/1000 | Loss: 0.00000912
Iteration 41/1000 | Loss: 0.00000911
Iteration 42/1000 | Loss: 0.00000911
Iteration 43/1000 | Loss: 0.00000911
Iteration 44/1000 | Loss: 0.00000911
Iteration 45/1000 | Loss: 0.00000911
Iteration 46/1000 | Loss: 0.00000911
Iteration 47/1000 | Loss: 0.00000910
Iteration 48/1000 | Loss: 0.00000910
Iteration 49/1000 | Loss: 0.00000910
Iteration 50/1000 | Loss: 0.00000910
Iteration 51/1000 | Loss: 0.00000909
Iteration 52/1000 | Loss: 0.00000909
Iteration 53/1000 | Loss: 0.00000908
Iteration 54/1000 | Loss: 0.00000908
Iteration 55/1000 | Loss: 0.00000908
Iteration 56/1000 | Loss: 0.00000908
Iteration 57/1000 | Loss: 0.00000908
Iteration 58/1000 | Loss: 0.00000908
Iteration 59/1000 | Loss: 0.00000907
Iteration 60/1000 | Loss: 0.00000907
Iteration 61/1000 | Loss: 0.00000907
Iteration 62/1000 | Loss: 0.00000907
Iteration 63/1000 | Loss: 0.00000907
Iteration 64/1000 | Loss: 0.00000906
Iteration 65/1000 | Loss: 0.00000906
Iteration 66/1000 | Loss: 0.00000906
Iteration 67/1000 | Loss: 0.00000906
Iteration 68/1000 | Loss: 0.00000906
Iteration 69/1000 | Loss: 0.00000906
Iteration 70/1000 | Loss: 0.00000905
Iteration 71/1000 | Loss: 0.00000905
Iteration 72/1000 | Loss: 0.00000905
Iteration 73/1000 | Loss: 0.00000905
Iteration 74/1000 | Loss: 0.00000905
Iteration 75/1000 | Loss: 0.00000904
Iteration 76/1000 | Loss: 0.00000904
Iteration 77/1000 | Loss: 0.00000904
Iteration 78/1000 | Loss: 0.00000904
Iteration 79/1000 | Loss: 0.00000904
Iteration 80/1000 | Loss: 0.00000904
Iteration 81/1000 | Loss: 0.00000904
Iteration 82/1000 | Loss: 0.00000904
Iteration 83/1000 | Loss: 0.00000903
Iteration 84/1000 | Loss: 0.00000903
Iteration 85/1000 | Loss: 0.00000903
Iteration 86/1000 | Loss: 0.00000903
Iteration 87/1000 | Loss: 0.00000903
Iteration 88/1000 | Loss: 0.00000903
Iteration 89/1000 | Loss: 0.00000903
Iteration 90/1000 | Loss: 0.00000902
Iteration 91/1000 | Loss: 0.00000902
Iteration 92/1000 | Loss: 0.00000902
Iteration 93/1000 | Loss: 0.00000902
Iteration 94/1000 | Loss: 0.00000902
Iteration 95/1000 | Loss: 0.00000902
Iteration 96/1000 | Loss: 0.00000902
Iteration 97/1000 | Loss: 0.00000902
Iteration 98/1000 | Loss: 0.00000902
Iteration 99/1000 | Loss: 0.00000902
Iteration 100/1000 | Loss: 0.00000901
Iteration 101/1000 | Loss: 0.00000901
Iteration 102/1000 | Loss: 0.00000901
Iteration 103/1000 | Loss: 0.00000901
Iteration 104/1000 | Loss: 0.00000901
Iteration 105/1000 | Loss: 0.00000901
Iteration 106/1000 | Loss: 0.00000901
Iteration 107/1000 | Loss: 0.00000901
Iteration 108/1000 | Loss: 0.00000901
Iteration 109/1000 | Loss: 0.00000901
Iteration 110/1000 | Loss: 0.00000901
Iteration 111/1000 | Loss: 0.00000901
Iteration 112/1000 | Loss: 0.00000901
Iteration 113/1000 | Loss: 0.00000901
Iteration 114/1000 | Loss: 0.00000901
Iteration 115/1000 | Loss: 0.00000901
Iteration 116/1000 | Loss: 0.00000901
Iteration 117/1000 | Loss: 0.00000901
Iteration 118/1000 | Loss: 0.00000901
Iteration 119/1000 | Loss: 0.00000901
Iteration 120/1000 | Loss: 0.00000901
Iteration 121/1000 | Loss: 0.00000901
Iteration 122/1000 | Loss: 0.00000901
Iteration 123/1000 | Loss: 0.00000901
Iteration 124/1000 | Loss: 0.00000901
Iteration 125/1000 | Loss: 0.00000901
Iteration 126/1000 | Loss: 0.00000901
Iteration 127/1000 | Loss: 0.00000901
Iteration 128/1000 | Loss: 0.00000901
Iteration 129/1000 | Loss: 0.00000901
Iteration 130/1000 | Loss: 0.00000901
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [9.010636858874932e-06, 9.010636858874932e-06, 9.010636858874932e-06, 9.010636858874932e-06, 9.010636858874932e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.010636858874932e-06

Optimization complete. Final v2v error: 2.544290065765381 mm

Highest mean error: 3.3304805755615234 mm for frame 112

Lowest mean error: 2.261704921722412 mm for frame 175

Saving results

Total time: 34.91042947769165
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841725
Iteration 2/25 | Loss: 0.00151446
Iteration 3/25 | Loss: 0.00119593
Iteration 4/25 | Loss: 0.00114660
Iteration 5/25 | Loss: 0.00113490
Iteration 6/25 | Loss: 0.00112663
Iteration 7/25 | Loss: 0.00112374
Iteration 8/25 | Loss: 0.00112248
Iteration 9/25 | Loss: 0.00112184
Iteration 10/25 | Loss: 0.00112120
Iteration 11/25 | Loss: 0.00112069
Iteration 12/25 | Loss: 0.00112018
Iteration 13/25 | Loss: 0.00111951
Iteration 14/25 | Loss: 0.00111853
Iteration 15/25 | Loss: 0.00111782
Iteration 16/25 | Loss: 0.00111755
Iteration 17/25 | Loss: 0.00111753
Iteration 18/25 | Loss: 0.00111753
Iteration 19/25 | Loss: 0.00111753
Iteration 20/25 | Loss: 0.00111753
Iteration 21/25 | Loss: 0.00111753
Iteration 22/25 | Loss: 0.00111753
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011175264371559024, 0.0011175264371559024, 0.0011175264371559024, 0.0011175264371559024, 0.0011175264371559024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011175264371559024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38003016
Iteration 2/25 | Loss: 0.00248151
Iteration 3/25 | Loss: 0.00248151
Iteration 4/25 | Loss: 0.00248150
Iteration 5/25 | Loss: 0.00248150
Iteration 6/25 | Loss: 0.00248150
Iteration 7/25 | Loss: 0.00248150
Iteration 8/25 | Loss: 0.00248150
Iteration 9/25 | Loss: 0.00248150
Iteration 10/25 | Loss: 0.00248150
Iteration 11/25 | Loss: 0.00248150
Iteration 12/25 | Loss: 0.00248150
Iteration 13/25 | Loss: 0.00248150
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.002481502713635564, 0.002481502713635564, 0.002481502713635564, 0.002481502713635564, 0.002481502713635564]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002481502713635564

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00248150
Iteration 2/1000 | Loss: 0.00031537
Iteration 3/1000 | Loss: 0.00037736
Iteration 4/1000 | Loss: 0.00128933
Iteration 5/1000 | Loss: 0.00345722
Iteration 6/1000 | Loss: 0.00042861
Iteration 7/1000 | Loss: 0.00267808
Iteration 8/1000 | Loss: 0.00048745
Iteration 9/1000 | Loss: 0.00091853
Iteration 10/1000 | Loss: 0.00051783
Iteration 11/1000 | Loss: 0.00035847
Iteration 12/1000 | Loss: 0.00055253
Iteration 13/1000 | Loss: 0.00050265
Iteration 14/1000 | Loss: 0.00013870
Iteration 15/1000 | Loss: 0.00010695
Iteration 16/1000 | Loss: 0.00009442
Iteration 17/1000 | Loss: 0.00008549
Iteration 18/1000 | Loss: 0.00007987
Iteration 19/1000 | Loss: 0.00007629
Iteration 20/1000 | Loss: 0.00007429
Iteration 21/1000 | Loss: 0.00007220
Iteration 22/1000 | Loss: 0.00007054
Iteration 23/1000 | Loss: 0.00006872
Iteration 24/1000 | Loss: 0.00006751
Iteration 25/1000 | Loss: 0.00006650
Iteration 26/1000 | Loss: 0.00006557
Iteration 27/1000 | Loss: 0.00006468
Iteration 28/1000 | Loss: 0.00031932
Iteration 29/1000 | Loss: 0.00734040
Iteration 30/1000 | Loss: 0.00310738
Iteration 31/1000 | Loss: 0.00566473
Iteration 32/1000 | Loss: 0.00204848
Iteration 33/1000 | Loss: 0.00313759
Iteration 34/1000 | Loss: 0.00011277
Iteration 35/1000 | Loss: 0.00007761
Iteration 36/1000 | Loss: 0.00005663
Iteration 37/1000 | Loss: 0.00003928
Iteration 38/1000 | Loss: 0.00003062
Iteration 39/1000 | Loss: 0.00002542
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002040
Iteration 42/1000 | Loss: 0.00001892
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001694
Iteration 45/1000 | Loss: 0.00001619
Iteration 46/1000 | Loss: 0.00001550
Iteration 47/1000 | Loss: 0.00001503
Iteration 48/1000 | Loss: 0.00001468
Iteration 49/1000 | Loss: 0.00001439
Iteration 50/1000 | Loss: 0.00001419
Iteration 51/1000 | Loss: 0.00001409
Iteration 52/1000 | Loss: 0.00001406
Iteration 53/1000 | Loss: 0.00001405
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001404
Iteration 57/1000 | Loss: 0.00001404
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001403
Iteration 60/1000 | Loss: 0.00001402
Iteration 61/1000 | Loss: 0.00001402
Iteration 62/1000 | Loss: 0.00001402
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001399
Iteration 66/1000 | Loss: 0.00001399
Iteration 67/1000 | Loss: 0.00001399
Iteration 68/1000 | Loss: 0.00001399
Iteration 69/1000 | Loss: 0.00001399
Iteration 70/1000 | Loss: 0.00001399
Iteration 71/1000 | Loss: 0.00001399
Iteration 72/1000 | Loss: 0.00001396
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001394
Iteration 75/1000 | Loss: 0.00001394
Iteration 76/1000 | Loss: 0.00001394
Iteration 77/1000 | Loss: 0.00001394
Iteration 78/1000 | Loss: 0.00001393
Iteration 79/1000 | Loss: 0.00001393
Iteration 80/1000 | Loss: 0.00001392
Iteration 81/1000 | Loss: 0.00001392
Iteration 82/1000 | Loss: 0.00001392
Iteration 83/1000 | Loss: 0.00001392
Iteration 84/1000 | Loss: 0.00001392
Iteration 85/1000 | Loss: 0.00001392
Iteration 86/1000 | Loss: 0.00001391
Iteration 87/1000 | Loss: 0.00001391
Iteration 88/1000 | Loss: 0.00001391
Iteration 89/1000 | Loss: 0.00001391
Iteration 90/1000 | Loss: 0.00001390
Iteration 91/1000 | Loss: 0.00001390
Iteration 92/1000 | Loss: 0.00001390
Iteration 93/1000 | Loss: 0.00001389
Iteration 94/1000 | Loss: 0.00001389
Iteration 95/1000 | Loss: 0.00001389
Iteration 96/1000 | Loss: 0.00001389
Iteration 97/1000 | Loss: 0.00001389
Iteration 98/1000 | Loss: 0.00001389
Iteration 99/1000 | Loss: 0.00001389
Iteration 100/1000 | Loss: 0.00001389
Iteration 101/1000 | Loss: 0.00001389
Iteration 102/1000 | Loss: 0.00001388
Iteration 103/1000 | Loss: 0.00001388
Iteration 104/1000 | Loss: 0.00001388
Iteration 105/1000 | Loss: 0.00001388
Iteration 106/1000 | Loss: 0.00001388
Iteration 107/1000 | Loss: 0.00001388
Iteration 108/1000 | Loss: 0.00001388
Iteration 109/1000 | Loss: 0.00001388
Iteration 110/1000 | Loss: 0.00001387
Iteration 111/1000 | Loss: 0.00001387
Iteration 112/1000 | Loss: 0.00001387
Iteration 113/1000 | Loss: 0.00001386
Iteration 114/1000 | Loss: 0.00001386
Iteration 115/1000 | Loss: 0.00001386
Iteration 116/1000 | Loss: 0.00001385
Iteration 117/1000 | Loss: 0.00001385
Iteration 118/1000 | Loss: 0.00001385
Iteration 119/1000 | Loss: 0.00001385
Iteration 120/1000 | Loss: 0.00001385
Iteration 121/1000 | Loss: 0.00001385
Iteration 122/1000 | Loss: 0.00001385
Iteration 123/1000 | Loss: 0.00001385
Iteration 124/1000 | Loss: 0.00001385
Iteration 125/1000 | Loss: 0.00001385
Iteration 126/1000 | Loss: 0.00001385
Iteration 127/1000 | Loss: 0.00001385
Iteration 128/1000 | Loss: 0.00001385
Iteration 129/1000 | Loss: 0.00001385
Iteration 130/1000 | Loss: 0.00001385
Iteration 131/1000 | Loss: 0.00001385
Iteration 132/1000 | Loss: 0.00001384
Iteration 133/1000 | Loss: 0.00001384
Iteration 134/1000 | Loss: 0.00001384
Iteration 135/1000 | Loss: 0.00001383
Iteration 136/1000 | Loss: 0.00001383
Iteration 137/1000 | Loss: 0.00001383
Iteration 138/1000 | Loss: 0.00001382
Iteration 139/1000 | Loss: 0.00001382
Iteration 140/1000 | Loss: 0.00001382
Iteration 141/1000 | Loss: 0.00001382
Iteration 142/1000 | Loss: 0.00001382
Iteration 143/1000 | Loss: 0.00001382
Iteration 144/1000 | Loss: 0.00001382
Iteration 145/1000 | Loss: 0.00001382
Iteration 146/1000 | Loss: 0.00001382
Iteration 147/1000 | Loss: 0.00001381
Iteration 148/1000 | Loss: 0.00001381
Iteration 149/1000 | Loss: 0.00001381
Iteration 150/1000 | Loss: 0.00001381
Iteration 151/1000 | Loss: 0.00001381
Iteration 152/1000 | Loss: 0.00001381
Iteration 153/1000 | Loss: 0.00001380
Iteration 154/1000 | Loss: 0.00001380
Iteration 155/1000 | Loss: 0.00001380
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001379
Iteration 159/1000 | Loss: 0.00001379
Iteration 160/1000 | Loss: 0.00001379
Iteration 161/1000 | Loss: 0.00001379
Iteration 162/1000 | Loss: 0.00001379
Iteration 163/1000 | Loss: 0.00001379
Iteration 164/1000 | Loss: 0.00001379
Iteration 165/1000 | Loss: 0.00001379
Iteration 166/1000 | Loss: 0.00001379
Iteration 167/1000 | Loss: 0.00001379
Iteration 168/1000 | Loss: 0.00001379
Iteration 169/1000 | Loss: 0.00001378
Iteration 170/1000 | Loss: 0.00001378
Iteration 171/1000 | Loss: 0.00001378
Iteration 172/1000 | Loss: 0.00001378
Iteration 173/1000 | Loss: 0.00001378
Iteration 174/1000 | Loss: 0.00001377
Iteration 175/1000 | Loss: 0.00001377
Iteration 176/1000 | Loss: 0.00001377
Iteration 177/1000 | Loss: 0.00001377
Iteration 178/1000 | Loss: 0.00001377
Iteration 179/1000 | Loss: 0.00001377
Iteration 180/1000 | Loss: 0.00001377
Iteration 181/1000 | Loss: 0.00001377
Iteration 182/1000 | Loss: 0.00001377
Iteration 183/1000 | Loss: 0.00001377
Iteration 184/1000 | Loss: 0.00001377
Iteration 185/1000 | Loss: 0.00001377
Iteration 186/1000 | Loss: 0.00001376
Iteration 187/1000 | Loss: 0.00001376
Iteration 188/1000 | Loss: 0.00001376
Iteration 189/1000 | Loss: 0.00001376
Iteration 190/1000 | Loss: 0.00001375
Iteration 191/1000 | Loss: 0.00001375
Iteration 192/1000 | Loss: 0.00001374
Iteration 193/1000 | Loss: 0.00001374
Iteration 194/1000 | Loss: 0.00001374
Iteration 195/1000 | Loss: 0.00001374
Iteration 196/1000 | Loss: 0.00001374
Iteration 197/1000 | Loss: 0.00001374
Iteration 198/1000 | Loss: 0.00001374
Iteration 199/1000 | Loss: 0.00001374
Iteration 200/1000 | Loss: 0.00001374
Iteration 201/1000 | Loss: 0.00001373
Iteration 202/1000 | Loss: 0.00001373
Iteration 203/1000 | Loss: 0.00001373
Iteration 204/1000 | Loss: 0.00001373
Iteration 205/1000 | Loss: 0.00001373
Iteration 206/1000 | Loss: 0.00001372
Iteration 207/1000 | Loss: 0.00001372
Iteration 208/1000 | Loss: 0.00001372
Iteration 209/1000 | Loss: 0.00001372
Iteration 210/1000 | Loss: 0.00001372
Iteration 211/1000 | Loss: 0.00001372
Iteration 212/1000 | Loss: 0.00001372
Iteration 213/1000 | Loss: 0.00001372
Iteration 214/1000 | Loss: 0.00001372
Iteration 215/1000 | Loss: 0.00001372
Iteration 216/1000 | Loss: 0.00001372
Iteration 217/1000 | Loss: 0.00001371
Iteration 218/1000 | Loss: 0.00001371
Iteration 219/1000 | Loss: 0.00001371
Iteration 220/1000 | Loss: 0.00001371
Iteration 221/1000 | Loss: 0.00001371
Iteration 222/1000 | Loss: 0.00001371
Iteration 223/1000 | Loss: 0.00001371
Iteration 224/1000 | Loss: 0.00001371
Iteration 225/1000 | Loss: 0.00001370
Iteration 226/1000 | Loss: 0.00001370
Iteration 227/1000 | Loss: 0.00001370
Iteration 228/1000 | Loss: 0.00001370
Iteration 229/1000 | Loss: 0.00001370
Iteration 230/1000 | Loss: 0.00001370
Iteration 231/1000 | Loss: 0.00001370
Iteration 232/1000 | Loss: 0.00001370
Iteration 233/1000 | Loss: 0.00001370
Iteration 234/1000 | Loss: 0.00001370
Iteration 235/1000 | Loss: 0.00001370
Iteration 236/1000 | Loss: 0.00001370
Iteration 237/1000 | Loss: 0.00001370
Iteration 238/1000 | Loss: 0.00001370
Iteration 239/1000 | Loss: 0.00001370
Iteration 240/1000 | Loss: 0.00001370
Iteration 241/1000 | Loss: 0.00001369
Iteration 242/1000 | Loss: 0.00001369
Iteration 243/1000 | Loss: 0.00001369
Iteration 244/1000 | Loss: 0.00001369
Iteration 245/1000 | Loss: 0.00001369
Iteration 246/1000 | Loss: 0.00001369
Iteration 247/1000 | Loss: 0.00001369
Iteration 248/1000 | Loss: 0.00001369
Iteration 249/1000 | Loss: 0.00001369
Iteration 250/1000 | Loss: 0.00001369
Iteration 251/1000 | Loss: 0.00001369
Iteration 252/1000 | Loss: 0.00001369
Iteration 253/1000 | Loss: 0.00001369
Iteration 254/1000 | Loss: 0.00001369
Iteration 255/1000 | Loss: 0.00001369
Iteration 256/1000 | Loss: 0.00001369
Iteration 257/1000 | Loss: 0.00001369
Iteration 258/1000 | Loss: 0.00001369
Iteration 259/1000 | Loss: 0.00001368
Iteration 260/1000 | Loss: 0.00001368
Iteration 261/1000 | Loss: 0.00001368
Iteration 262/1000 | Loss: 0.00001368
Iteration 263/1000 | Loss: 0.00001368
Iteration 264/1000 | Loss: 0.00001368
Iteration 265/1000 | Loss: 0.00001368
Iteration 266/1000 | Loss: 0.00001368
Iteration 267/1000 | Loss: 0.00001368
Iteration 268/1000 | Loss: 0.00001368
Iteration 269/1000 | Loss: 0.00001368
Iteration 270/1000 | Loss: 0.00001368
Iteration 271/1000 | Loss: 0.00001368
Iteration 272/1000 | Loss: 0.00001368
Iteration 273/1000 | Loss: 0.00001368
Iteration 274/1000 | Loss: 0.00001368
Iteration 275/1000 | Loss: 0.00001368
Iteration 276/1000 | Loss: 0.00001368
Iteration 277/1000 | Loss: 0.00001368
Iteration 278/1000 | Loss: 0.00001368
Iteration 279/1000 | Loss: 0.00001368
Iteration 280/1000 | Loss: 0.00001368
Iteration 281/1000 | Loss: 0.00001368
Iteration 282/1000 | Loss: 0.00001368
Iteration 283/1000 | Loss: 0.00001368
Iteration 284/1000 | Loss: 0.00001368
Iteration 285/1000 | Loss: 0.00001368
Iteration 286/1000 | Loss: 0.00001368
Iteration 287/1000 | Loss: 0.00001368
Iteration 288/1000 | Loss: 0.00001368
Iteration 289/1000 | Loss: 0.00001368
Iteration 290/1000 | Loss: 0.00001368
Iteration 291/1000 | Loss: 0.00001368
Iteration 292/1000 | Loss: 0.00001368
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [1.368300127069233e-05, 1.368300127069233e-05, 1.368300127069233e-05, 1.368300127069233e-05, 1.368300127069233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.368300127069233e-05

Optimization complete. Final v2v error: 3.0832622051239014 mm

Highest mean error: 3.390380859375 mm for frame 94

Lowest mean error: 2.728889226913452 mm for frame 48

Saving results

Total time: 130.88143587112427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01134387
Iteration 2/25 | Loss: 0.00149087
Iteration 3/25 | Loss: 0.00103991
Iteration 4/25 | Loss: 0.00100469
Iteration 5/25 | Loss: 0.00099703
Iteration 6/25 | Loss: 0.00098789
Iteration 7/25 | Loss: 0.00096844
Iteration 8/25 | Loss: 0.00096622
Iteration 9/25 | Loss: 0.00097011
Iteration 10/25 | Loss: 0.00095564
Iteration 11/25 | Loss: 0.00096305
Iteration 12/25 | Loss: 0.00096088
Iteration 13/25 | Loss: 0.00096052
Iteration 14/25 | Loss: 0.00095934
Iteration 15/25 | Loss: 0.00096177
Iteration 16/25 | Loss: 0.00096040
Iteration 17/25 | Loss: 0.00095741
Iteration 18/25 | Loss: 0.00095935
Iteration 19/25 | Loss: 0.00095955
Iteration 20/25 | Loss: 0.00095487
Iteration 21/25 | Loss: 0.00095478
Iteration 22/25 | Loss: 0.00095324
Iteration 23/25 | Loss: 0.00095176
Iteration 24/25 | Loss: 0.00095166
Iteration 25/25 | Loss: 0.00095166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02573586
Iteration 2/25 | Loss: 0.00072604
Iteration 3/25 | Loss: 0.00072603
Iteration 4/25 | Loss: 0.00072603
Iteration 5/25 | Loss: 0.00072603
Iteration 6/25 | Loss: 0.00072603
Iteration 7/25 | Loss: 0.00072603
Iteration 8/25 | Loss: 0.00072603
Iteration 9/25 | Loss: 0.00072603
Iteration 10/25 | Loss: 0.00072603
Iteration 11/25 | Loss: 0.00072603
Iteration 12/25 | Loss: 0.00072603
Iteration 13/25 | Loss: 0.00072603
Iteration 14/25 | Loss: 0.00072603
Iteration 15/25 | Loss: 0.00072603
Iteration 16/25 | Loss: 0.00072603
Iteration 17/25 | Loss: 0.00072603
Iteration 18/25 | Loss: 0.00072603
Iteration 19/25 | Loss: 0.00072603
Iteration 20/25 | Loss: 0.00072603
Iteration 21/25 | Loss: 0.00072603
Iteration 22/25 | Loss: 0.00072603
Iteration 23/25 | Loss: 0.00072603
Iteration 24/25 | Loss: 0.00072603
Iteration 25/25 | Loss: 0.00072603

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072603
Iteration 2/1000 | Loss: 0.00006869
Iteration 3/1000 | Loss: 0.00003980
Iteration 4/1000 | Loss: 0.00003356
Iteration 5/1000 | Loss: 0.00003117
Iteration 6/1000 | Loss: 0.00002923
Iteration 7/1000 | Loss: 0.00002838
Iteration 8/1000 | Loss: 0.00004538
Iteration 9/1000 | Loss: 0.00019107
Iteration 10/1000 | Loss: 0.00003040
Iteration 11/1000 | Loss: 0.00002736
Iteration 12/1000 | Loss: 0.00002569
Iteration 13/1000 | Loss: 0.00004280
Iteration 14/1000 | Loss: 0.00002458
Iteration 15/1000 | Loss: 0.00002440
Iteration 16/1000 | Loss: 0.00002417
Iteration 17/1000 | Loss: 0.00002412
Iteration 18/1000 | Loss: 0.00002399
Iteration 19/1000 | Loss: 0.00002398
Iteration 20/1000 | Loss: 0.00002398
Iteration 21/1000 | Loss: 0.00002398
Iteration 22/1000 | Loss: 0.00002397
Iteration 23/1000 | Loss: 0.00002397
Iteration 24/1000 | Loss: 0.00002396
Iteration 25/1000 | Loss: 0.00002396
Iteration 26/1000 | Loss: 0.00002395
Iteration 27/1000 | Loss: 0.00002389
Iteration 28/1000 | Loss: 0.00002375
Iteration 29/1000 | Loss: 0.00002374
Iteration 30/1000 | Loss: 0.00002370
Iteration 31/1000 | Loss: 0.00002370
Iteration 32/1000 | Loss: 0.00002369
Iteration 33/1000 | Loss: 0.00002369
Iteration 34/1000 | Loss: 0.00002369
Iteration 35/1000 | Loss: 0.00002369
Iteration 36/1000 | Loss: 0.00002369
Iteration 37/1000 | Loss: 0.00002369
Iteration 38/1000 | Loss: 0.00002369
Iteration 39/1000 | Loss: 0.00002369
Iteration 40/1000 | Loss: 0.00002369
Iteration 41/1000 | Loss: 0.00002369
Iteration 42/1000 | Loss: 0.00002368
Iteration 43/1000 | Loss: 0.00002368
Iteration 44/1000 | Loss: 0.00002368
Iteration 45/1000 | Loss: 0.00002368
Iteration 46/1000 | Loss: 0.00002368
Iteration 47/1000 | Loss: 0.00002368
Iteration 48/1000 | Loss: 0.00003803
Iteration 49/1000 | Loss: 0.00002645
Iteration 50/1000 | Loss: 0.00002836
Iteration 51/1000 | Loss: 0.00002362
Iteration 52/1000 | Loss: 0.00002362
Iteration 53/1000 | Loss: 0.00002362
Iteration 54/1000 | Loss: 0.00002362
Iteration 55/1000 | Loss: 0.00002362
Iteration 56/1000 | Loss: 0.00002362
Iteration 57/1000 | Loss: 0.00002362
Iteration 58/1000 | Loss: 0.00002361
Iteration 59/1000 | Loss: 0.00002361
Iteration 60/1000 | Loss: 0.00002361
Iteration 61/1000 | Loss: 0.00002361
Iteration 62/1000 | Loss: 0.00002360
Iteration 63/1000 | Loss: 0.00002360
Iteration 64/1000 | Loss: 0.00002360
Iteration 65/1000 | Loss: 0.00002359
Iteration 66/1000 | Loss: 0.00002359
Iteration 67/1000 | Loss: 0.00002359
Iteration 68/1000 | Loss: 0.00002359
Iteration 69/1000 | Loss: 0.00002359
Iteration 70/1000 | Loss: 0.00002359
Iteration 71/1000 | Loss: 0.00002359
Iteration 72/1000 | Loss: 0.00002359
Iteration 73/1000 | Loss: 0.00002359
Iteration 74/1000 | Loss: 0.00002359
Iteration 75/1000 | Loss: 0.00002359
Iteration 76/1000 | Loss: 0.00002359
Iteration 77/1000 | Loss: 0.00002358
Iteration 78/1000 | Loss: 0.00002358
Iteration 79/1000 | Loss: 0.00002358
Iteration 80/1000 | Loss: 0.00002358
Iteration 81/1000 | Loss: 0.00002358
Iteration 82/1000 | Loss: 0.00002358
Iteration 83/1000 | Loss: 0.00002358
Iteration 84/1000 | Loss: 0.00002358
Iteration 85/1000 | Loss: 0.00002358
Iteration 86/1000 | Loss: 0.00002358
Iteration 87/1000 | Loss: 0.00002358
Iteration 88/1000 | Loss: 0.00002358
Iteration 89/1000 | Loss: 0.00002358
Iteration 90/1000 | Loss: 0.00002358
Iteration 91/1000 | Loss: 0.00002358
Iteration 92/1000 | Loss: 0.00002358
Iteration 93/1000 | Loss: 0.00002358
Iteration 94/1000 | Loss: 0.00002358
Iteration 95/1000 | Loss: 0.00002358
Iteration 96/1000 | Loss: 0.00002357
Iteration 97/1000 | Loss: 0.00002357
Iteration 98/1000 | Loss: 0.00002357
Iteration 99/1000 | Loss: 0.00002357
Iteration 100/1000 | Loss: 0.00002356
Iteration 101/1000 | Loss: 0.00002356
Iteration 102/1000 | Loss: 0.00002356
Iteration 103/1000 | Loss: 0.00002356
Iteration 104/1000 | Loss: 0.00002356
Iteration 105/1000 | Loss: 0.00002356
Iteration 106/1000 | Loss: 0.00002356
Iteration 107/1000 | Loss: 0.00002356
Iteration 108/1000 | Loss: 0.00002356
Iteration 109/1000 | Loss: 0.00002356
Iteration 110/1000 | Loss: 0.00002356
Iteration 111/1000 | Loss: 0.00002356
Iteration 112/1000 | Loss: 0.00002356
Iteration 113/1000 | Loss: 0.00002356
Iteration 114/1000 | Loss: 0.00002356
Iteration 115/1000 | Loss: 0.00002356
Iteration 116/1000 | Loss: 0.00002356
Iteration 117/1000 | Loss: 0.00002356
Iteration 118/1000 | Loss: 0.00002356
Iteration 119/1000 | Loss: 0.00002356
Iteration 120/1000 | Loss: 0.00002356
Iteration 121/1000 | Loss: 0.00002356
Iteration 122/1000 | Loss: 0.00002356
Iteration 123/1000 | Loss: 0.00002356
Iteration 124/1000 | Loss: 0.00002356
Iteration 125/1000 | Loss: 0.00002356
Iteration 126/1000 | Loss: 0.00002356
Iteration 127/1000 | Loss: 0.00002356
Iteration 128/1000 | Loss: 0.00002356
Iteration 129/1000 | Loss: 0.00002356
Iteration 130/1000 | Loss: 0.00002356
Iteration 131/1000 | Loss: 0.00002356
Iteration 132/1000 | Loss: 0.00002356
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [2.3557919121230952e-05, 2.3557919121230952e-05, 2.3557919121230952e-05, 2.3557919121230952e-05, 2.3557919121230952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3557919121230952e-05

Optimization complete. Final v2v error: 4.032355308532715 mm

Highest mean error: 4.71075439453125 mm for frame 19

Lowest mean error: 3.6415557861328125 mm for frame 0

Saving results

Total time: 80.40193557739258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077018
Iteration 2/25 | Loss: 0.00146580
Iteration 3/25 | Loss: 0.00112460
Iteration 4/25 | Loss: 0.00107442
Iteration 5/25 | Loss: 0.00106511
Iteration 6/25 | Loss: 0.00106176
Iteration 7/25 | Loss: 0.00106155
Iteration 8/25 | Loss: 0.00106155
Iteration 9/25 | Loss: 0.00106155
Iteration 10/25 | Loss: 0.00106155
Iteration 11/25 | Loss: 0.00106155
Iteration 12/25 | Loss: 0.00106155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010615530190989375, 0.0010615530190989375, 0.0010615530190989375, 0.0010615530190989375, 0.0010615530190989375]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010615530190989375

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.11376369
Iteration 2/25 | Loss: 0.00082406
Iteration 3/25 | Loss: 0.00082404
Iteration 4/25 | Loss: 0.00082404
Iteration 5/25 | Loss: 0.00082404
Iteration 6/25 | Loss: 0.00082404
Iteration 7/25 | Loss: 0.00082404
Iteration 8/25 | Loss: 0.00082404
Iteration 9/25 | Loss: 0.00082404
Iteration 10/25 | Loss: 0.00082404
Iteration 11/25 | Loss: 0.00082404
Iteration 12/25 | Loss: 0.00082404
Iteration 13/25 | Loss: 0.00082404
Iteration 14/25 | Loss: 0.00082404
Iteration 15/25 | Loss: 0.00082404
Iteration 16/25 | Loss: 0.00082404
Iteration 17/25 | Loss: 0.00082404
Iteration 18/25 | Loss: 0.00082404
Iteration 19/25 | Loss: 0.00082404
Iteration 20/25 | Loss: 0.00082404
Iteration 21/25 | Loss: 0.00082404
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000824039860162884, 0.000824039860162884, 0.000824039860162884, 0.000824039860162884, 0.000824039860162884]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000824039860162884

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082404
Iteration 2/1000 | Loss: 0.00006879
Iteration 3/1000 | Loss: 0.00004336
Iteration 4/1000 | Loss: 0.00003414
Iteration 5/1000 | Loss: 0.00003273
Iteration 6/1000 | Loss: 0.00003156
Iteration 7/1000 | Loss: 0.00003091
Iteration 8/1000 | Loss: 0.00003041
Iteration 9/1000 | Loss: 0.00002994
Iteration 10/1000 | Loss: 0.00002959
Iteration 11/1000 | Loss: 0.00002933
Iteration 12/1000 | Loss: 0.00002910
Iteration 13/1000 | Loss: 0.00002894
Iteration 14/1000 | Loss: 0.00002890
Iteration 15/1000 | Loss: 0.00002889
Iteration 16/1000 | Loss: 0.00002889
Iteration 17/1000 | Loss: 0.00002888
Iteration 18/1000 | Loss: 0.00002888
Iteration 19/1000 | Loss: 0.00002883
Iteration 20/1000 | Loss: 0.00002872
Iteration 21/1000 | Loss: 0.00002867
Iteration 22/1000 | Loss: 0.00002867
Iteration 23/1000 | Loss: 0.00002864
Iteration 24/1000 | Loss: 0.00002862
Iteration 25/1000 | Loss: 0.00002861
Iteration 26/1000 | Loss: 0.00002860
Iteration 27/1000 | Loss: 0.00002858
Iteration 28/1000 | Loss: 0.00002857
Iteration 29/1000 | Loss: 0.00002857
Iteration 30/1000 | Loss: 0.00002855
Iteration 31/1000 | Loss: 0.00002854
Iteration 32/1000 | Loss: 0.00002854
Iteration 33/1000 | Loss: 0.00002854
Iteration 34/1000 | Loss: 0.00002852
Iteration 35/1000 | Loss: 0.00002852
Iteration 36/1000 | Loss: 0.00002851
Iteration 37/1000 | Loss: 0.00002851
Iteration 38/1000 | Loss: 0.00002851
Iteration 39/1000 | Loss: 0.00002850
Iteration 40/1000 | Loss: 0.00002850
Iteration 41/1000 | Loss: 0.00002849
Iteration 42/1000 | Loss: 0.00002846
Iteration 43/1000 | Loss: 0.00002846
Iteration 44/1000 | Loss: 0.00002846
Iteration 45/1000 | Loss: 0.00002846
Iteration 46/1000 | Loss: 0.00002846
Iteration 47/1000 | Loss: 0.00002846
Iteration 48/1000 | Loss: 0.00002845
Iteration 49/1000 | Loss: 0.00002845
Iteration 50/1000 | Loss: 0.00002841
Iteration 51/1000 | Loss: 0.00002841
Iteration 52/1000 | Loss: 0.00002841
Iteration 53/1000 | Loss: 0.00002840
Iteration 54/1000 | Loss: 0.00002840
Iteration 55/1000 | Loss: 0.00002839
Iteration 56/1000 | Loss: 0.00002839
Iteration 57/1000 | Loss: 0.00002838
Iteration 58/1000 | Loss: 0.00002838
Iteration 59/1000 | Loss: 0.00002838
Iteration 60/1000 | Loss: 0.00002837
Iteration 61/1000 | Loss: 0.00002836
Iteration 62/1000 | Loss: 0.00002835
Iteration 63/1000 | Loss: 0.00002833
Iteration 64/1000 | Loss: 0.00002833
Iteration 65/1000 | Loss: 0.00002833
Iteration 66/1000 | Loss: 0.00002833
Iteration 67/1000 | Loss: 0.00002832
Iteration 68/1000 | Loss: 0.00002832
Iteration 69/1000 | Loss: 0.00002832
Iteration 70/1000 | Loss: 0.00002831
Iteration 71/1000 | Loss: 0.00002831
Iteration 72/1000 | Loss: 0.00002830
Iteration 73/1000 | Loss: 0.00002830
Iteration 74/1000 | Loss: 0.00002830
Iteration 75/1000 | Loss: 0.00002830
Iteration 76/1000 | Loss: 0.00002829
Iteration 77/1000 | Loss: 0.00002829
Iteration 78/1000 | Loss: 0.00002829
Iteration 79/1000 | Loss: 0.00002829
Iteration 80/1000 | Loss: 0.00002829
Iteration 81/1000 | Loss: 0.00002828
Iteration 82/1000 | Loss: 0.00002828
Iteration 83/1000 | Loss: 0.00002828
Iteration 84/1000 | Loss: 0.00002828
Iteration 85/1000 | Loss: 0.00002827
Iteration 86/1000 | Loss: 0.00002827
Iteration 87/1000 | Loss: 0.00002827
Iteration 88/1000 | Loss: 0.00002827
Iteration 89/1000 | Loss: 0.00002827
Iteration 90/1000 | Loss: 0.00002827
Iteration 91/1000 | Loss: 0.00002827
Iteration 92/1000 | Loss: 0.00002827
Iteration 93/1000 | Loss: 0.00002826
Iteration 94/1000 | Loss: 0.00002826
Iteration 95/1000 | Loss: 0.00002826
Iteration 96/1000 | Loss: 0.00002826
Iteration 97/1000 | Loss: 0.00002826
Iteration 98/1000 | Loss: 0.00002826
Iteration 99/1000 | Loss: 0.00002826
Iteration 100/1000 | Loss: 0.00002826
Iteration 101/1000 | Loss: 0.00002826
Iteration 102/1000 | Loss: 0.00002826
Iteration 103/1000 | Loss: 0.00002826
Iteration 104/1000 | Loss: 0.00002826
Iteration 105/1000 | Loss: 0.00002825
Iteration 106/1000 | Loss: 0.00002825
Iteration 107/1000 | Loss: 0.00002825
Iteration 108/1000 | Loss: 0.00002825
Iteration 109/1000 | Loss: 0.00002825
Iteration 110/1000 | Loss: 0.00002825
Iteration 111/1000 | Loss: 0.00002824
Iteration 112/1000 | Loss: 0.00002824
Iteration 113/1000 | Loss: 0.00002824
Iteration 114/1000 | Loss: 0.00002824
Iteration 115/1000 | Loss: 0.00002824
Iteration 116/1000 | Loss: 0.00002824
Iteration 117/1000 | Loss: 0.00002824
Iteration 118/1000 | Loss: 0.00002824
Iteration 119/1000 | Loss: 0.00002824
Iteration 120/1000 | Loss: 0.00002824
Iteration 121/1000 | Loss: 0.00002824
Iteration 122/1000 | Loss: 0.00002824
Iteration 123/1000 | Loss: 0.00002824
Iteration 124/1000 | Loss: 0.00002824
Iteration 125/1000 | Loss: 0.00002823
Iteration 126/1000 | Loss: 0.00002823
Iteration 127/1000 | Loss: 0.00002823
Iteration 128/1000 | Loss: 0.00002823
Iteration 129/1000 | Loss: 0.00002823
Iteration 130/1000 | Loss: 0.00002823
Iteration 131/1000 | Loss: 0.00002823
Iteration 132/1000 | Loss: 0.00002823
Iteration 133/1000 | Loss: 0.00002822
Iteration 134/1000 | Loss: 0.00002822
Iteration 135/1000 | Loss: 0.00002822
Iteration 136/1000 | Loss: 0.00002822
Iteration 137/1000 | Loss: 0.00002822
Iteration 138/1000 | Loss: 0.00002822
Iteration 139/1000 | Loss: 0.00002822
Iteration 140/1000 | Loss: 0.00002822
Iteration 141/1000 | Loss: 0.00002822
Iteration 142/1000 | Loss: 0.00002822
Iteration 143/1000 | Loss: 0.00002822
Iteration 144/1000 | Loss: 0.00002822
Iteration 145/1000 | Loss: 0.00002822
Iteration 146/1000 | Loss: 0.00002822
Iteration 147/1000 | Loss: 0.00002822
Iteration 148/1000 | Loss: 0.00002822
Iteration 149/1000 | Loss: 0.00002822
Iteration 150/1000 | Loss: 0.00002822
Iteration 151/1000 | Loss: 0.00002822
Iteration 152/1000 | Loss: 0.00002822
Iteration 153/1000 | Loss: 0.00002822
Iteration 154/1000 | Loss: 0.00002822
Iteration 155/1000 | Loss: 0.00002822
Iteration 156/1000 | Loss: 0.00002822
Iteration 157/1000 | Loss: 0.00002822
Iteration 158/1000 | Loss: 0.00002822
Iteration 159/1000 | Loss: 0.00002822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.8220427338965237e-05, 2.8220427338965237e-05, 2.8220427338965237e-05, 2.8220427338965237e-05, 2.8220427338965237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8220427338965237e-05

Optimization complete. Final v2v error: 4.448605537414551 mm

Highest mean error: 5.729414939880371 mm for frame 158

Lowest mean error: 3.4046530723571777 mm for frame 227

Saving results

Total time: 50.057947874069214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01073487
Iteration 2/25 | Loss: 0.00195039
Iteration 3/25 | Loss: 0.00135084
Iteration 4/25 | Loss: 0.00129330
Iteration 5/25 | Loss: 0.00121640
Iteration 6/25 | Loss: 0.00124083
Iteration 7/25 | Loss: 0.00125412
Iteration 8/25 | Loss: 0.00113243
Iteration 9/25 | Loss: 0.00114505
Iteration 10/25 | Loss: 0.00109680
Iteration 11/25 | Loss: 0.00106467
Iteration 12/25 | Loss: 0.00098725
Iteration 13/25 | Loss: 0.00103134
Iteration 14/25 | Loss: 0.00104086
Iteration 15/25 | Loss: 0.00100975
Iteration 16/25 | Loss: 0.00096191
Iteration 17/25 | Loss: 0.00095784
Iteration 18/25 | Loss: 0.00095365
Iteration 19/25 | Loss: 0.00094832
Iteration 20/25 | Loss: 0.00095212
Iteration 21/25 | Loss: 0.00093791
Iteration 22/25 | Loss: 0.00093625
Iteration 23/25 | Loss: 0.00093429
Iteration 24/25 | Loss: 0.00093046
Iteration 25/25 | Loss: 0.00093330

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59337270
Iteration 2/25 | Loss: 0.00133854
Iteration 3/25 | Loss: 0.00133854
Iteration 4/25 | Loss: 0.00133854
Iteration 5/25 | Loss: 0.00133854
Iteration 6/25 | Loss: 0.00133854
Iteration 7/25 | Loss: 0.00133854
Iteration 8/25 | Loss: 0.00133854
Iteration 9/25 | Loss: 0.00133854
Iteration 10/25 | Loss: 0.00133854
Iteration 11/25 | Loss: 0.00133854
Iteration 12/25 | Loss: 0.00133854
Iteration 13/25 | Loss: 0.00133854
Iteration 14/25 | Loss: 0.00133854
Iteration 15/25 | Loss: 0.00133854
Iteration 16/25 | Loss: 0.00133854
Iteration 17/25 | Loss: 0.00133854
Iteration 18/25 | Loss: 0.00133854
Iteration 19/25 | Loss: 0.00133854
Iteration 20/25 | Loss: 0.00133854
Iteration 21/25 | Loss: 0.00133854
Iteration 22/25 | Loss: 0.00133854
Iteration 23/25 | Loss: 0.00133854
Iteration 24/25 | Loss: 0.00133854
Iteration 25/25 | Loss: 0.00133854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133854
Iteration 2/1000 | Loss: 0.00029790
Iteration 3/1000 | Loss: 0.00030746
Iteration 4/1000 | Loss: 0.00018594
Iteration 5/1000 | Loss: 0.00054401
Iteration 6/1000 | Loss: 0.00031185
Iteration 7/1000 | Loss: 0.00061847
Iteration 8/1000 | Loss: 0.00100703
Iteration 9/1000 | Loss: 0.00139739
Iteration 10/1000 | Loss: 0.00068427
Iteration 11/1000 | Loss: 0.00048661
Iteration 12/1000 | Loss: 0.00020714
Iteration 13/1000 | Loss: 0.00020709
Iteration 14/1000 | Loss: 0.00049724
Iteration 15/1000 | Loss: 0.00047375
Iteration 16/1000 | Loss: 0.00025891
Iteration 17/1000 | Loss: 0.00084107
Iteration 18/1000 | Loss: 0.00055080
Iteration 19/1000 | Loss: 0.00010041
Iteration 20/1000 | Loss: 0.00094096
Iteration 21/1000 | Loss: 0.00090600
Iteration 22/1000 | Loss: 0.00065836
Iteration 23/1000 | Loss: 0.00086796
Iteration 24/1000 | Loss: 0.00057670
Iteration 25/1000 | Loss: 0.00037864
Iteration 26/1000 | Loss: 0.00092236
Iteration 27/1000 | Loss: 0.00047949
Iteration 28/1000 | Loss: 0.00036450
Iteration 29/1000 | Loss: 0.00094375
Iteration 30/1000 | Loss: 0.00062339
Iteration 31/1000 | Loss: 0.00051047
Iteration 32/1000 | Loss: 0.00097188
Iteration 33/1000 | Loss: 0.00084752
Iteration 34/1000 | Loss: 0.00046228
Iteration 35/1000 | Loss: 0.00028275
Iteration 36/1000 | Loss: 0.00040996
Iteration 37/1000 | Loss: 0.00056352
Iteration 38/1000 | Loss: 0.00038848
Iteration 39/1000 | Loss: 0.00074788
Iteration 40/1000 | Loss: 0.00032954
Iteration 41/1000 | Loss: 0.00021857
Iteration 42/1000 | Loss: 0.00029721
Iteration 43/1000 | Loss: 0.00016992
Iteration 44/1000 | Loss: 0.00023931
Iteration 45/1000 | Loss: 0.00035176
Iteration 46/1000 | Loss: 0.00029248
Iteration 47/1000 | Loss: 0.00024477
Iteration 48/1000 | Loss: 0.00068094
Iteration 49/1000 | Loss: 0.00074135
Iteration 50/1000 | Loss: 0.00038806
Iteration 51/1000 | Loss: 0.00033489
Iteration 52/1000 | Loss: 0.00019191
Iteration 53/1000 | Loss: 0.00019270
Iteration 54/1000 | Loss: 0.00020329
Iteration 55/1000 | Loss: 0.00030430
Iteration 56/1000 | Loss: 0.00024439
Iteration 57/1000 | Loss: 0.00022197
Iteration 58/1000 | Loss: 0.00044033
Iteration 59/1000 | Loss: 0.00039936
Iteration 60/1000 | Loss: 0.00038693
Iteration 61/1000 | Loss: 0.00087389
Iteration 62/1000 | Loss: 0.00064810
Iteration 63/1000 | Loss: 0.00056610
Iteration 64/1000 | Loss: 0.00068783
Iteration 65/1000 | Loss: 0.00057445
Iteration 66/1000 | Loss: 0.00024451
Iteration 67/1000 | Loss: 0.00022579
Iteration 68/1000 | Loss: 0.00020124
Iteration 69/1000 | Loss: 0.00028286
Iteration 70/1000 | Loss: 0.00025855
Iteration 71/1000 | Loss: 0.00024776
Iteration 72/1000 | Loss: 0.00084084
Iteration 73/1000 | Loss: 0.00052341
Iteration 74/1000 | Loss: 0.00071666
Iteration 75/1000 | Loss: 0.00024082
Iteration 76/1000 | Loss: 0.00021944
Iteration 77/1000 | Loss: 0.00035844
Iteration 78/1000 | Loss: 0.00017857
Iteration 79/1000 | Loss: 0.00062621
Iteration 80/1000 | Loss: 0.00030002
Iteration 81/1000 | Loss: 0.00021957
Iteration 82/1000 | Loss: 0.00050737
Iteration 83/1000 | Loss: 0.00028952
Iteration 84/1000 | Loss: 0.00030169
Iteration 85/1000 | Loss: 0.00066074
Iteration 86/1000 | Loss: 0.00028085
Iteration 87/1000 | Loss: 0.00003906
Iteration 88/1000 | Loss: 0.00003037
Iteration 89/1000 | Loss: 0.00003553
Iteration 90/1000 | Loss: 0.00009222
Iteration 91/1000 | Loss: 0.00064823
Iteration 92/1000 | Loss: 0.00028083
Iteration 93/1000 | Loss: 0.00041794
Iteration 94/1000 | Loss: 0.00005245
Iteration 95/1000 | Loss: 0.00002667
Iteration 96/1000 | Loss: 0.00002507
Iteration 97/1000 | Loss: 0.00003251
Iteration 98/1000 | Loss: 0.00002967
Iteration 99/1000 | Loss: 0.00002664
Iteration 100/1000 | Loss: 0.00002097
Iteration 101/1000 | Loss: 0.00003117
Iteration 102/1000 | Loss: 0.00002635
Iteration 103/1000 | Loss: 0.00003106
Iteration 104/1000 | Loss: 0.00003180
Iteration 105/1000 | Loss: 0.00003065
Iteration 106/1000 | Loss: 0.00002030
Iteration 107/1000 | Loss: 0.00002183
Iteration 108/1000 | Loss: 0.00002990
Iteration 109/1000 | Loss: 0.00002974
Iteration 110/1000 | Loss: 0.00003026
Iteration 111/1000 | Loss: 0.00001893
Iteration 112/1000 | Loss: 0.00002958
Iteration 113/1000 | Loss: 0.00001784
Iteration 114/1000 | Loss: 0.00003010
Iteration 115/1000 | Loss: 0.00003052
Iteration 116/1000 | Loss: 0.00002965
Iteration 117/1000 | Loss: 0.00003610
Iteration 118/1000 | Loss: 0.00003378
Iteration 119/1000 | Loss: 0.00002824
Iteration 120/1000 | Loss: 0.00002804
Iteration 121/1000 | Loss: 0.00003176
Iteration 122/1000 | Loss: 0.00001948
Iteration 123/1000 | Loss: 0.00001656
Iteration 124/1000 | Loss: 0.00002620
Iteration 125/1000 | Loss: 0.00003109
Iteration 126/1000 | Loss: 0.00003465
Iteration 127/1000 | Loss: 0.00001259
Iteration 128/1000 | Loss: 0.00001556
Iteration 129/1000 | Loss: 0.00003030
Iteration 130/1000 | Loss: 0.00003065
Iteration 131/1000 | Loss: 0.00002967
Iteration 132/1000 | Loss: 0.00003063
Iteration 133/1000 | Loss: 0.00002146
Iteration 134/1000 | Loss: 0.00003113
Iteration 135/1000 | Loss: 0.00003508
Iteration 136/1000 | Loss: 0.00001492
Iteration 137/1000 | Loss: 0.00003086
Iteration 138/1000 | Loss: 0.00001776
Iteration 139/1000 | Loss: 0.00004659
Iteration 140/1000 | Loss: 0.00003874
Iteration 141/1000 | Loss: 0.00003021
Iteration 142/1000 | Loss: 0.00002588
Iteration 143/1000 | Loss: 0.00004256
Iteration 144/1000 | Loss: 0.00002384
Iteration 145/1000 | Loss: 0.00003434
Iteration 146/1000 | Loss: 0.00003116
Iteration 147/1000 | Loss: 0.00003415
Iteration 148/1000 | Loss: 0.00003173
Iteration 149/1000 | Loss: 0.00003471
Iteration 150/1000 | Loss: 0.00002818
Iteration 151/1000 | Loss: 0.00003176
Iteration 152/1000 | Loss: 0.00003026
Iteration 153/1000 | Loss: 0.00003855
Iteration 154/1000 | Loss: 0.00003081
Iteration 155/1000 | Loss: 0.00003134
Iteration 156/1000 | Loss: 0.00002267
Iteration 157/1000 | Loss: 0.00002276
Iteration 158/1000 | Loss: 0.00002585
Iteration 159/1000 | Loss: 0.00003173
Iteration 160/1000 | Loss: 0.00002769
Iteration 161/1000 | Loss: 0.00002961
Iteration 162/1000 | Loss: 0.00002971
Iteration 163/1000 | Loss: 0.00003215
Iteration 164/1000 | Loss: 0.00002363
Iteration 165/1000 | Loss: 0.00002452
Iteration 166/1000 | Loss: 0.00004839
Iteration 167/1000 | Loss: 0.00004675
Iteration 168/1000 | Loss: 0.00003362
Iteration 169/1000 | Loss: 0.00002988
Iteration 170/1000 | Loss: 0.00002381
Iteration 171/1000 | Loss: 0.00002535
Iteration 172/1000 | Loss: 0.00001618
Iteration 173/1000 | Loss: 0.00001554
Iteration 174/1000 | Loss: 0.00001452
Iteration 175/1000 | Loss: 0.00001334
Iteration 176/1000 | Loss: 0.00001269
Iteration 177/1000 | Loss: 0.00001231
Iteration 178/1000 | Loss: 0.00001205
Iteration 179/1000 | Loss: 0.00001185
Iteration 180/1000 | Loss: 0.00001180
Iteration 181/1000 | Loss: 0.00001176
Iteration 182/1000 | Loss: 0.00001171
Iteration 183/1000 | Loss: 0.00001171
Iteration 184/1000 | Loss: 0.00001165
Iteration 185/1000 | Loss: 0.00001160
Iteration 186/1000 | Loss: 0.00001159
Iteration 187/1000 | Loss: 0.00001159
Iteration 188/1000 | Loss: 0.00001157
Iteration 189/1000 | Loss: 0.00001157
Iteration 190/1000 | Loss: 0.00001157
Iteration 191/1000 | Loss: 0.00001156
Iteration 192/1000 | Loss: 0.00001156
Iteration 193/1000 | Loss: 0.00001156
Iteration 194/1000 | Loss: 0.00001156
Iteration 195/1000 | Loss: 0.00001156
Iteration 196/1000 | Loss: 0.00001156
Iteration 197/1000 | Loss: 0.00001156
Iteration 198/1000 | Loss: 0.00001155
Iteration 199/1000 | Loss: 0.00001155
Iteration 200/1000 | Loss: 0.00001155
Iteration 201/1000 | Loss: 0.00001155
Iteration 202/1000 | Loss: 0.00001155
Iteration 203/1000 | Loss: 0.00001155
Iteration 204/1000 | Loss: 0.00001155
Iteration 205/1000 | Loss: 0.00001154
Iteration 206/1000 | Loss: 0.00001154
Iteration 207/1000 | Loss: 0.00001154
Iteration 208/1000 | Loss: 0.00001154
Iteration 209/1000 | Loss: 0.00001153
Iteration 210/1000 | Loss: 0.00001153
Iteration 211/1000 | Loss: 0.00001153
Iteration 212/1000 | Loss: 0.00001153
Iteration 213/1000 | Loss: 0.00001153
Iteration 214/1000 | Loss: 0.00001153
Iteration 215/1000 | Loss: 0.00001153
Iteration 216/1000 | Loss: 0.00001153
Iteration 217/1000 | Loss: 0.00001153
Iteration 218/1000 | Loss: 0.00001153
Iteration 219/1000 | Loss: 0.00001153
Iteration 220/1000 | Loss: 0.00001153
Iteration 221/1000 | Loss: 0.00001153
Iteration 222/1000 | Loss: 0.00001153
Iteration 223/1000 | Loss: 0.00001153
Iteration 224/1000 | Loss: 0.00001153
Iteration 225/1000 | Loss: 0.00001153
Iteration 226/1000 | Loss: 0.00001153
Iteration 227/1000 | Loss: 0.00001152
Iteration 228/1000 | Loss: 0.00001152
Iteration 229/1000 | Loss: 0.00001152
Iteration 230/1000 | Loss: 0.00001152
Iteration 231/1000 | Loss: 0.00001151
Iteration 232/1000 | Loss: 0.00001151
Iteration 233/1000 | Loss: 0.00001151
Iteration 234/1000 | Loss: 0.00001151
Iteration 235/1000 | Loss: 0.00001151
Iteration 236/1000 | Loss: 0.00001151
Iteration 237/1000 | Loss: 0.00001151
Iteration 238/1000 | Loss: 0.00001150
Iteration 239/1000 | Loss: 0.00001150
Iteration 240/1000 | Loss: 0.00001150
Iteration 241/1000 | Loss: 0.00001150
Iteration 242/1000 | Loss: 0.00001150
Iteration 243/1000 | Loss: 0.00001149
Iteration 244/1000 | Loss: 0.00001149
Iteration 245/1000 | Loss: 0.00001149
Iteration 246/1000 | Loss: 0.00001149
Iteration 247/1000 | Loss: 0.00001149
Iteration 248/1000 | Loss: 0.00001149
Iteration 249/1000 | Loss: 0.00001149
Iteration 250/1000 | Loss: 0.00001149
Iteration 251/1000 | Loss: 0.00001148
Iteration 252/1000 | Loss: 0.00001148
Iteration 253/1000 | Loss: 0.00001148
Iteration 254/1000 | Loss: 0.00001148
Iteration 255/1000 | Loss: 0.00001148
Iteration 256/1000 | Loss: 0.00001148
Iteration 257/1000 | Loss: 0.00001147
Iteration 258/1000 | Loss: 0.00001147
Iteration 259/1000 | Loss: 0.00001147
Iteration 260/1000 | Loss: 0.00001147
Iteration 261/1000 | Loss: 0.00001146
Iteration 262/1000 | Loss: 0.00001146
Iteration 263/1000 | Loss: 0.00001146
Iteration 264/1000 | Loss: 0.00001146
Iteration 265/1000 | Loss: 0.00001146
Iteration 266/1000 | Loss: 0.00001146
Iteration 267/1000 | Loss: 0.00001146
Iteration 268/1000 | Loss: 0.00001145
Iteration 269/1000 | Loss: 0.00001145
Iteration 270/1000 | Loss: 0.00001145
Iteration 271/1000 | Loss: 0.00001145
Iteration 272/1000 | Loss: 0.00001145
Iteration 273/1000 | Loss: 0.00001145
Iteration 274/1000 | Loss: 0.00001145
Iteration 275/1000 | Loss: 0.00001145
Iteration 276/1000 | Loss: 0.00001144
Iteration 277/1000 | Loss: 0.00001144
Iteration 278/1000 | Loss: 0.00001144
Iteration 279/1000 | Loss: 0.00001144
Iteration 280/1000 | Loss: 0.00001144
Iteration 281/1000 | Loss: 0.00001144
Iteration 282/1000 | Loss: 0.00001144
Iteration 283/1000 | Loss: 0.00001144
Iteration 284/1000 | Loss: 0.00001144
Iteration 285/1000 | Loss: 0.00001144
Iteration 286/1000 | Loss: 0.00001144
Iteration 287/1000 | Loss: 0.00001144
Iteration 288/1000 | Loss: 0.00001144
Iteration 289/1000 | Loss: 0.00001143
Iteration 290/1000 | Loss: 0.00001143
Iteration 291/1000 | Loss: 0.00001143
Iteration 292/1000 | Loss: 0.00001143
Iteration 293/1000 | Loss: 0.00001143
Iteration 294/1000 | Loss: 0.00001143
Iteration 295/1000 | Loss: 0.00001143
Iteration 296/1000 | Loss: 0.00001143
Iteration 297/1000 | Loss: 0.00001143
Iteration 298/1000 | Loss: 0.00001143
Iteration 299/1000 | Loss: 0.00001143
Iteration 300/1000 | Loss: 0.00001143
Iteration 301/1000 | Loss: 0.00001143
Iteration 302/1000 | Loss: 0.00001143
Iteration 303/1000 | Loss: 0.00001143
Iteration 304/1000 | Loss: 0.00001143
Iteration 305/1000 | Loss: 0.00001143
Iteration 306/1000 | Loss: 0.00001143
Iteration 307/1000 | Loss: 0.00001143
Iteration 308/1000 | Loss: 0.00001143
Iteration 309/1000 | Loss: 0.00001143
Iteration 310/1000 | Loss: 0.00001143
Iteration 311/1000 | Loss: 0.00001143
Iteration 312/1000 | Loss: 0.00001143
Iteration 313/1000 | Loss: 0.00001143
Iteration 314/1000 | Loss: 0.00001143
Iteration 315/1000 | Loss: 0.00001143
Iteration 316/1000 | Loss: 0.00001143
Iteration 317/1000 | Loss: 0.00001143
Iteration 318/1000 | Loss: 0.00001143
Iteration 319/1000 | Loss: 0.00001143
Iteration 320/1000 | Loss: 0.00001143
Iteration 321/1000 | Loss: 0.00001143
Iteration 322/1000 | Loss: 0.00001143
Iteration 323/1000 | Loss: 0.00001143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [1.142568089562701e-05, 1.142568089562701e-05, 1.142568089562701e-05, 1.142568089562701e-05, 1.142568089562701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.142568089562701e-05

Optimization complete. Final v2v error: 2.7591471672058105 mm

Highest mean error: 4.8337531089782715 mm for frame 94

Lowest mean error: 2.2859573364257812 mm for frame 51

Saving results

Total time: 316.4218339920044
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056171
Iteration 2/25 | Loss: 0.00162577
Iteration 3/25 | Loss: 0.00123730
Iteration 4/25 | Loss: 0.00112482
Iteration 5/25 | Loss: 0.00102088
Iteration 6/25 | Loss: 0.00096652
Iteration 7/25 | Loss: 0.00096572
Iteration 8/25 | Loss: 0.00098530
Iteration 9/25 | Loss: 0.00098692
Iteration 10/25 | Loss: 0.00098467
Iteration 11/25 | Loss: 0.00094795
Iteration 12/25 | Loss: 0.00094083
Iteration 13/25 | Loss: 0.00093811
Iteration 14/25 | Loss: 0.00093861
Iteration 15/25 | Loss: 0.00093881
Iteration 16/25 | Loss: 0.00093846
Iteration 17/25 | Loss: 0.00093801
Iteration 18/25 | Loss: 0.00093815
Iteration 19/25 | Loss: 0.00093858
Iteration 20/25 | Loss: 0.00093637
Iteration 21/25 | Loss: 0.00093759
Iteration 22/25 | Loss: 0.00093798
Iteration 23/25 | Loss: 0.00093762
Iteration 24/25 | Loss: 0.00093672
Iteration 25/25 | Loss: 0.00093823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52680922
Iteration 2/25 | Loss: 0.00103299
Iteration 3/25 | Loss: 0.00103299
Iteration 4/25 | Loss: 0.00103299
Iteration 5/25 | Loss: 0.00103299
Iteration 6/25 | Loss: 0.00103299
Iteration 7/25 | Loss: 0.00103299
Iteration 8/25 | Loss: 0.00103299
Iteration 9/25 | Loss: 0.00103299
Iteration 10/25 | Loss: 0.00103299
Iteration 11/25 | Loss: 0.00103299
Iteration 12/25 | Loss: 0.00103299
Iteration 13/25 | Loss: 0.00103299
Iteration 14/25 | Loss: 0.00103299
Iteration 15/25 | Loss: 0.00103299
Iteration 16/25 | Loss: 0.00103299
Iteration 17/25 | Loss: 0.00103299
Iteration 18/25 | Loss: 0.00103299
Iteration 19/25 | Loss: 0.00103299
Iteration 20/25 | Loss: 0.00103299
Iteration 21/25 | Loss: 0.00103299
Iteration 22/25 | Loss: 0.00103299
Iteration 23/25 | Loss: 0.00103299
Iteration 24/25 | Loss: 0.00103299
Iteration 25/25 | Loss: 0.00103299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103299
Iteration 2/1000 | Loss: 0.00019613
Iteration 3/1000 | Loss: 0.00010270
Iteration 4/1000 | Loss: 0.00004795
Iteration 5/1000 | Loss: 0.00003724
Iteration 6/1000 | Loss: 0.00002907
Iteration 7/1000 | Loss: 0.00002814
Iteration 8/1000 | Loss: 0.00003896
Iteration 9/1000 | Loss: 0.00002059
Iteration 10/1000 | Loss: 0.00002980
Iteration 11/1000 | Loss: 0.00003459
Iteration 12/1000 | Loss: 0.00003077
Iteration 13/1000 | Loss: 0.00003326
Iteration 14/1000 | Loss: 0.00002945
Iteration 15/1000 | Loss: 0.00002200
Iteration 16/1000 | Loss: 0.00002536
Iteration 17/1000 | Loss: 0.00003192
Iteration 18/1000 | Loss: 0.00003174
Iteration 19/1000 | Loss: 0.00002294
Iteration 20/1000 | Loss: 0.00002580
Iteration 21/1000 | Loss: 0.00003305
Iteration 22/1000 | Loss: 0.00001993
Iteration 23/1000 | Loss: 0.00002134
Iteration 24/1000 | Loss: 0.00002620
Iteration 25/1000 | Loss: 0.00002082
Iteration 26/1000 | Loss: 0.00002709
Iteration 27/1000 | Loss: 0.00002723
Iteration 28/1000 | Loss: 0.00002956
Iteration 29/1000 | Loss: 0.00001870
Iteration 30/1000 | Loss: 0.00001678
Iteration 31/1000 | Loss: 0.00003328
Iteration 32/1000 | Loss: 0.00002708
Iteration 33/1000 | Loss: 0.00003108
Iteration 34/1000 | Loss: 0.00001669
Iteration 35/1000 | Loss: 0.00003173
Iteration 36/1000 | Loss: 0.00002642
Iteration 37/1000 | Loss: 0.00001808
Iteration 38/1000 | Loss: 0.00003156
Iteration 39/1000 | Loss: 0.00002603
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001605
Iteration 42/1000 | Loss: 0.00001575
Iteration 43/1000 | Loss: 0.00001543
Iteration 44/1000 | Loss: 0.00001506
Iteration 45/1000 | Loss: 0.00001471
Iteration 46/1000 | Loss: 0.00001442
Iteration 47/1000 | Loss: 0.00001398
Iteration 48/1000 | Loss: 0.00001370
Iteration 49/1000 | Loss: 0.00001349
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001320
Iteration 52/1000 | Loss: 0.00001317
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001316
Iteration 56/1000 | Loss: 0.00001315
Iteration 57/1000 | Loss: 0.00001314
Iteration 58/1000 | Loss: 0.00001314
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001314
Iteration 61/1000 | Loss: 0.00001314
Iteration 62/1000 | Loss: 0.00001314
Iteration 63/1000 | Loss: 0.00001314
Iteration 64/1000 | Loss: 0.00001313
Iteration 65/1000 | Loss: 0.00001313
Iteration 66/1000 | Loss: 0.00001313
Iteration 67/1000 | Loss: 0.00001313
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001311
Iteration 71/1000 | Loss: 0.00001311
Iteration 72/1000 | Loss: 0.00001310
Iteration 73/1000 | Loss: 0.00001310
Iteration 74/1000 | Loss: 0.00001308
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001308
Iteration 81/1000 | Loss: 0.00001308
Iteration 82/1000 | Loss: 0.00001308
Iteration 83/1000 | Loss: 0.00001307
Iteration 84/1000 | Loss: 0.00001307
Iteration 85/1000 | Loss: 0.00001307
Iteration 86/1000 | Loss: 0.00001307
Iteration 87/1000 | Loss: 0.00001307
Iteration 88/1000 | Loss: 0.00001307
Iteration 89/1000 | Loss: 0.00001307
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001306
Iteration 93/1000 | Loss: 0.00001306
Iteration 94/1000 | Loss: 0.00001306
Iteration 95/1000 | Loss: 0.00001305
Iteration 96/1000 | Loss: 0.00001305
Iteration 97/1000 | Loss: 0.00001305
Iteration 98/1000 | Loss: 0.00001305
Iteration 99/1000 | Loss: 0.00001304
Iteration 100/1000 | Loss: 0.00001304
Iteration 101/1000 | Loss: 0.00001304
Iteration 102/1000 | Loss: 0.00001304
Iteration 103/1000 | Loss: 0.00001304
Iteration 104/1000 | Loss: 0.00001303
Iteration 105/1000 | Loss: 0.00001303
Iteration 106/1000 | Loss: 0.00001303
Iteration 107/1000 | Loss: 0.00001303
Iteration 108/1000 | Loss: 0.00001303
Iteration 109/1000 | Loss: 0.00001303
Iteration 110/1000 | Loss: 0.00001303
Iteration 111/1000 | Loss: 0.00001303
Iteration 112/1000 | Loss: 0.00001303
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001302
Iteration 117/1000 | Loss: 0.00001302
Iteration 118/1000 | Loss: 0.00001302
Iteration 119/1000 | Loss: 0.00001302
Iteration 120/1000 | Loss: 0.00001302
Iteration 121/1000 | Loss: 0.00001302
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001301
Iteration 126/1000 | Loss: 0.00001301
Iteration 127/1000 | Loss: 0.00001301
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001301
Iteration 130/1000 | Loss: 0.00001301
Iteration 131/1000 | Loss: 0.00001301
Iteration 132/1000 | Loss: 0.00001301
Iteration 133/1000 | Loss: 0.00001301
Iteration 134/1000 | Loss: 0.00001301
Iteration 135/1000 | Loss: 0.00001301
Iteration 136/1000 | Loss: 0.00001300
Iteration 137/1000 | Loss: 0.00001300
Iteration 138/1000 | Loss: 0.00001300
Iteration 139/1000 | Loss: 0.00001300
Iteration 140/1000 | Loss: 0.00001300
Iteration 141/1000 | Loss: 0.00001299
Iteration 142/1000 | Loss: 0.00001299
Iteration 143/1000 | Loss: 0.00001299
Iteration 144/1000 | Loss: 0.00001299
Iteration 145/1000 | Loss: 0.00001299
Iteration 146/1000 | Loss: 0.00001299
Iteration 147/1000 | Loss: 0.00001299
Iteration 148/1000 | Loss: 0.00001299
Iteration 149/1000 | Loss: 0.00001299
Iteration 150/1000 | Loss: 0.00001299
Iteration 151/1000 | Loss: 0.00001299
Iteration 152/1000 | Loss: 0.00001299
Iteration 153/1000 | Loss: 0.00001299
Iteration 154/1000 | Loss: 0.00001299
Iteration 155/1000 | Loss: 0.00001298
Iteration 156/1000 | Loss: 0.00001298
Iteration 157/1000 | Loss: 0.00001298
Iteration 158/1000 | Loss: 0.00001298
Iteration 159/1000 | Loss: 0.00001298
Iteration 160/1000 | Loss: 0.00001298
Iteration 161/1000 | Loss: 0.00001298
Iteration 162/1000 | Loss: 0.00001298
Iteration 163/1000 | Loss: 0.00001298
Iteration 164/1000 | Loss: 0.00001298
Iteration 165/1000 | Loss: 0.00001298
Iteration 166/1000 | Loss: 0.00001298
Iteration 167/1000 | Loss: 0.00001298
Iteration 168/1000 | Loss: 0.00001298
Iteration 169/1000 | Loss: 0.00001297
Iteration 170/1000 | Loss: 0.00001297
Iteration 171/1000 | Loss: 0.00001297
Iteration 172/1000 | Loss: 0.00001297
Iteration 173/1000 | Loss: 0.00001297
Iteration 174/1000 | Loss: 0.00001297
Iteration 175/1000 | Loss: 0.00001297
Iteration 176/1000 | Loss: 0.00001296
Iteration 177/1000 | Loss: 0.00001296
Iteration 178/1000 | Loss: 0.00001296
Iteration 179/1000 | Loss: 0.00001296
Iteration 180/1000 | Loss: 0.00001295
Iteration 181/1000 | Loss: 0.00001295
Iteration 182/1000 | Loss: 0.00001295
Iteration 183/1000 | Loss: 0.00001295
Iteration 184/1000 | Loss: 0.00001294
Iteration 185/1000 | Loss: 0.00001294
Iteration 186/1000 | Loss: 0.00001294
Iteration 187/1000 | Loss: 0.00001293
Iteration 188/1000 | Loss: 0.00001293
Iteration 189/1000 | Loss: 0.00001293
Iteration 190/1000 | Loss: 0.00001293
Iteration 191/1000 | Loss: 0.00001293
Iteration 192/1000 | Loss: 0.00001293
Iteration 193/1000 | Loss: 0.00001293
Iteration 194/1000 | Loss: 0.00001293
Iteration 195/1000 | Loss: 0.00001292
Iteration 196/1000 | Loss: 0.00001292
Iteration 197/1000 | Loss: 0.00001292
Iteration 198/1000 | Loss: 0.00001292
Iteration 199/1000 | Loss: 0.00001292
Iteration 200/1000 | Loss: 0.00001292
Iteration 201/1000 | Loss: 0.00001291
Iteration 202/1000 | Loss: 0.00001291
Iteration 203/1000 | Loss: 0.00001291
Iteration 204/1000 | Loss: 0.00001291
Iteration 205/1000 | Loss: 0.00001291
Iteration 206/1000 | Loss: 0.00001291
Iteration 207/1000 | Loss: 0.00001291
Iteration 208/1000 | Loss: 0.00001291
Iteration 209/1000 | Loss: 0.00001290
Iteration 210/1000 | Loss: 0.00001290
Iteration 211/1000 | Loss: 0.00001290
Iteration 212/1000 | Loss: 0.00001290
Iteration 213/1000 | Loss: 0.00001290
Iteration 214/1000 | Loss: 0.00001290
Iteration 215/1000 | Loss: 0.00001290
Iteration 216/1000 | Loss: 0.00001289
Iteration 217/1000 | Loss: 0.00001289
Iteration 218/1000 | Loss: 0.00001289
Iteration 219/1000 | Loss: 0.00001289
Iteration 220/1000 | Loss: 0.00001289
Iteration 221/1000 | Loss: 0.00001289
Iteration 222/1000 | Loss: 0.00001289
Iteration 223/1000 | Loss: 0.00001289
Iteration 224/1000 | Loss: 0.00001289
Iteration 225/1000 | Loss: 0.00001289
Iteration 226/1000 | Loss: 0.00001289
Iteration 227/1000 | Loss: 0.00001289
Iteration 228/1000 | Loss: 0.00001289
Iteration 229/1000 | Loss: 0.00001289
Iteration 230/1000 | Loss: 0.00001289
Iteration 231/1000 | Loss: 0.00001289
Iteration 232/1000 | Loss: 0.00001289
Iteration 233/1000 | Loss: 0.00001288
Iteration 234/1000 | Loss: 0.00001288
Iteration 235/1000 | Loss: 0.00001288
Iteration 236/1000 | Loss: 0.00001288
Iteration 237/1000 | Loss: 0.00001288
Iteration 238/1000 | Loss: 0.00001288
Iteration 239/1000 | Loss: 0.00001288
Iteration 240/1000 | Loss: 0.00001288
Iteration 241/1000 | Loss: 0.00001288
Iteration 242/1000 | Loss: 0.00001288
Iteration 243/1000 | Loss: 0.00001288
Iteration 244/1000 | Loss: 0.00001288
Iteration 245/1000 | Loss: 0.00001288
Iteration 246/1000 | Loss: 0.00001287
Iteration 247/1000 | Loss: 0.00001287
Iteration 248/1000 | Loss: 0.00001287
Iteration 249/1000 | Loss: 0.00001287
Iteration 250/1000 | Loss: 0.00001287
Iteration 251/1000 | Loss: 0.00001287
Iteration 252/1000 | Loss: 0.00001287
Iteration 253/1000 | Loss: 0.00001287
Iteration 254/1000 | Loss: 0.00001287
Iteration 255/1000 | Loss: 0.00001286
Iteration 256/1000 | Loss: 0.00001286
Iteration 257/1000 | Loss: 0.00001286
Iteration 258/1000 | Loss: 0.00001286
Iteration 259/1000 | Loss: 0.00001286
Iteration 260/1000 | Loss: 0.00001286
Iteration 261/1000 | Loss: 0.00001286
Iteration 262/1000 | Loss: 0.00001286
Iteration 263/1000 | Loss: 0.00001286
Iteration 264/1000 | Loss: 0.00001286
Iteration 265/1000 | Loss: 0.00001286
Iteration 266/1000 | Loss: 0.00001286
Iteration 267/1000 | Loss: 0.00001286
Iteration 268/1000 | Loss: 0.00001285
Iteration 269/1000 | Loss: 0.00001285
Iteration 270/1000 | Loss: 0.00001285
Iteration 271/1000 | Loss: 0.00001285
Iteration 272/1000 | Loss: 0.00001285
Iteration 273/1000 | Loss: 0.00001285
Iteration 274/1000 | Loss: 0.00001285
Iteration 275/1000 | Loss: 0.00001285
Iteration 276/1000 | Loss: 0.00001285
Iteration 277/1000 | Loss: 0.00001285
Iteration 278/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [1.2854155102104414e-05, 1.2854155102104414e-05, 1.2854155102104414e-05, 1.2854155102104414e-05, 1.2854155102104414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2854155102104414e-05

Optimization complete. Final v2v error: 2.942671537399292 mm

Highest mean error: 5.126821994781494 mm for frame 152

Lowest mean error: 2.4405360221862793 mm for frame 0

Saving results

Total time: 135.58006167411804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00484040
Iteration 2/25 | Loss: 0.00109504
Iteration 3/25 | Loss: 0.00090654
Iteration 4/25 | Loss: 0.00088335
Iteration 5/25 | Loss: 0.00087967
Iteration 6/25 | Loss: 0.00087946
Iteration 7/25 | Loss: 0.00087946
Iteration 8/25 | Loss: 0.00087946
Iteration 9/25 | Loss: 0.00087946
Iteration 10/25 | Loss: 0.00087946
Iteration 11/25 | Loss: 0.00087945
Iteration 12/25 | Loss: 0.00087945
Iteration 13/25 | Loss: 0.00087945
Iteration 14/25 | Loss: 0.00087945
Iteration 15/25 | Loss: 0.00087945
Iteration 16/25 | Loss: 0.00087945
Iteration 17/25 | Loss: 0.00087945
Iteration 18/25 | Loss: 0.00087945
Iteration 19/25 | Loss: 0.00087945
Iteration 20/25 | Loss: 0.00087945
Iteration 21/25 | Loss: 0.00087945
Iteration 22/25 | Loss: 0.00087945
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008794472087174654, 0.0008794472087174654, 0.0008794472087174654, 0.0008794472087174654, 0.0008794472087174654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008794472087174654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.64557743
Iteration 2/25 | Loss: 0.00052858
Iteration 3/25 | Loss: 0.00052858
Iteration 4/25 | Loss: 0.00052858
Iteration 5/25 | Loss: 0.00052858
Iteration 6/25 | Loss: 0.00052858
Iteration 7/25 | Loss: 0.00052858
Iteration 8/25 | Loss: 0.00052858
Iteration 9/25 | Loss: 0.00052858
Iteration 10/25 | Loss: 0.00052858
Iteration 11/25 | Loss: 0.00052858
Iteration 12/25 | Loss: 0.00052858
Iteration 13/25 | Loss: 0.00052858
Iteration 14/25 | Loss: 0.00052858
Iteration 15/25 | Loss: 0.00052858
Iteration 16/25 | Loss: 0.00052858
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005285810912027955, 0.0005285810912027955, 0.0005285810912027955, 0.0005285810912027955, 0.0005285810912027955]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005285810912027955

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052858
Iteration 2/1000 | Loss: 0.00003835
Iteration 3/1000 | Loss: 0.00002291
Iteration 4/1000 | Loss: 0.00001973
Iteration 5/1000 | Loss: 0.00001885
Iteration 6/1000 | Loss: 0.00001815
Iteration 7/1000 | Loss: 0.00001769
Iteration 8/1000 | Loss: 0.00001721
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001675
Iteration 11/1000 | Loss: 0.00001663
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001646
Iteration 14/1000 | Loss: 0.00001635
Iteration 15/1000 | Loss: 0.00001634
Iteration 16/1000 | Loss: 0.00001630
Iteration 17/1000 | Loss: 0.00001629
Iteration 18/1000 | Loss: 0.00001628
Iteration 19/1000 | Loss: 0.00001626
Iteration 20/1000 | Loss: 0.00001626
Iteration 21/1000 | Loss: 0.00001625
Iteration 22/1000 | Loss: 0.00001625
Iteration 23/1000 | Loss: 0.00001623
Iteration 24/1000 | Loss: 0.00001623
Iteration 25/1000 | Loss: 0.00001622
Iteration 26/1000 | Loss: 0.00001621
Iteration 27/1000 | Loss: 0.00001621
Iteration 28/1000 | Loss: 0.00001621
Iteration 29/1000 | Loss: 0.00001618
Iteration 30/1000 | Loss: 0.00001618
Iteration 31/1000 | Loss: 0.00001617
Iteration 32/1000 | Loss: 0.00001617
Iteration 33/1000 | Loss: 0.00001617
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001614
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001614
Iteration 39/1000 | Loss: 0.00001614
Iteration 40/1000 | Loss: 0.00001614
Iteration 41/1000 | Loss: 0.00001614
Iteration 42/1000 | Loss: 0.00001614
Iteration 43/1000 | Loss: 0.00001614
Iteration 44/1000 | Loss: 0.00001614
Iteration 45/1000 | Loss: 0.00001613
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00001612
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001608
Iteration 50/1000 | Loss: 0.00001608
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001605
Iteration 53/1000 | Loss: 0.00001602
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001600
Iteration 59/1000 | Loss: 0.00001600
Iteration 60/1000 | Loss: 0.00001600
Iteration 61/1000 | Loss: 0.00001600
Iteration 62/1000 | Loss: 0.00001599
Iteration 63/1000 | Loss: 0.00001599
Iteration 64/1000 | Loss: 0.00001599
Iteration 65/1000 | Loss: 0.00001598
Iteration 66/1000 | Loss: 0.00001598
Iteration 67/1000 | Loss: 0.00001598
Iteration 68/1000 | Loss: 0.00001598
Iteration 69/1000 | Loss: 0.00001597
Iteration 70/1000 | Loss: 0.00001597
Iteration 71/1000 | Loss: 0.00001597
Iteration 72/1000 | Loss: 0.00001596
Iteration 73/1000 | Loss: 0.00001596
Iteration 74/1000 | Loss: 0.00001596
Iteration 75/1000 | Loss: 0.00001596
Iteration 76/1000 | Loss: 0.00001596
Iteration 77/1000 | Loss: 0.00001595
Iteration 78/1000 | Loss: 0.00001595
Iteration 79/1000 | Loss: 0.00001595
Iteration 80/1000 | Loss: 0.00001595
Iteration 81/1000 | Loss: 0.00001595
Iteration 82/1000 | Loss: 0.00001595
Iteration 83/1000 | Loss: 0.00001595
Iteration 84/1000 | Loss: 0.00001595
Iteration 85/1000 | Loss: 0.00001595
Iteration 86/1000 | Loss: 0.00001595
Iteration 87/1000 | Loss: 0.00001594
Iteration 88/1000 | Loss: 0.00001594
Iteration 89/1000 | Loss: 0.00001594
Iteration 90/1000 | Loss: 0.00001594
Iteration 91/1000 | Loss: 0.00001594
Iteration 92/1000 | Loss: 0.00001594
Iteration 93/1000 | Loss: 0.00001594
Iteration 94/1000 | Loss: 0.00001594
Iteration 95/1000 | Loss: 0.00001594
Iteration 96/1000 | Loss: 0.00001594
Iteration 97/1000 | Loss: 0.00001594
Iteration 98/1000 | Loss: 0.00001594
Iteration 99/1000 | Loss: 0.00001594
Iteration 100/1000 | Loss: 0.00001594
Iteration 101/1000 | Loss: 0.00001593
Iteration 102/1000 | Loss: 0.00001593
Iteration 103/1000 | Loss: 0.00001593
Iteration 104/1000 | Loss: 0.00001593
Iteration 105/1000 | Loss: 0.00001593
Iteration 106/1000 | Loss: 0.00001593
Iteration 107/1000 | Loss: 0.00001593
Iteration 108/1000 | Loss: 0.00001593
Iteration 109/1000 | Loss: 0.00001593
Iteration 110/1000 | Loss: 0.00001593
Iteration 111/1000 | Loss: 0.00001593
Iteration 112/1000 | Loss: 0.00001592
Iteration 113/1000 | Loss: 0.00001592
Iteration 114/1000 | Loss: 0.00001592
Iteration 115/1000 | Loss: 0.00001592
Iteration 116/1000 | Loss: 0.00001592
Iteration 117/1000 | Loss: 0.00001592
Iteration 118/1000 | Loss: 0.00001592
Iteration 119/1000 | Loss: 0.00001592
Iteration 120/1000 | Loss: 0.00001592
Iteration 121/1000 | Loss: 0.00001592
Iteration 122/1000 | Loss: 0.00001592
Iteration 123/1000 | Loss: 0.00001591
Iteration 124/1000 | Loss: 0.00001591
Iteration 125/1000 | Loss: 0.00001591
Iteration 126/1000 | Loss: 0.00001591
Iteration 127/1000 | Loss: 0.00001591
Iteration 128/1000 | Loss: 0.00001591
Iteration 129/1000 | Loss: 0.00001591
Iteration 130/1000 | Loss: 0.00001591
Iteration 131/1000 | Loss: 0.00001591
Iteration 132/1000 | Loss: 0.00001591
Iteration 133/1000 | Loss: 0.00001591
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.5912324670352973e-05, 1.5912324670352973e-05, 1.5912324670352973e-05, 1.5912324670352973e-05, 1.5912324670352973e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5912324670352973e-05

Optimization complete. Final v2v error: 3.25559401512146 mm

Highest mean error: 4.004303455352783 mm for frame 39

Lowest mean error: 2.7278225421905518 mm for frame 155

Saving results

Total time: 37.27423644065857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01059124
Iteration 2/25 | Loss: 0.00189029
Iteration 3/25 | Loss: 0.00116330
Iteration 4/25 | Loss: 0.00119204
Iteration 5/25 | Loss: 0.00106708
Iteration 6/25 | Loss: 0.00103385
Iteration 7/25 | Loss: 0.00099275
Iteration 8/25 | Loss: 0.00098943
Iteration 9/25 | Loss: 0.00095034
Iteration 10/25 | Loss: 0.00093365
Iteration 11/25 | Loss: 0.00092077
Iteration 12/25 | Loss: 0.00092819
Iteration 13/25 | Loss: 0.00091820
Iteration 14/25 | Loss: 0.00090361
Iteration 15/25 | Loss: 0.00090385
Iteration 16/25 | Loss: 0.00090014
Iteration 17/25 | Loss: 0.00089036
Iteration 18/25 | Loss: 0.00088965
Iteration 19/25 | Loss: 0.00088954
Iteration 20/25 | Loss: 0.00088953
Iteration 21/25 | Loss: 0.00088953
Iteration 22/25 | Loss: 0.00088953
Iteration 23/25 | Loss: 0.00088953
Iteration 24/25 | Loss: 0.00088953
Iteration 25/25 | Loss: 0.00088951

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31760502
Iteration 2/25 | Loss: 0.00114633
Iteration 3/25 | Loss: 0.00114633
Iteration 4/25 | Loss: 0.00114633
Iteration 5/25 | Loss: 0.00114633
Iteration 6/25 | Loss: 0.00114633
Iteration 7/25 | Loss: 0.00114633
Iteration 8/25 | Loss: 0.00114633
Iteration 9/25 | Loss: 0.00114633
Iteration 10/25 | Loss: 0.00114633
Iteration 11/25 | Loss: 0.00114633
Iteration 12/25 | Loss: 0.00114633
Iteration 13/25 | Loss: 0.00114633
Iteration 14/25 | Loss: 0.00114633
Iteration 15/25 | Loss: 0.00114633
Iteration 16/25 | Loss: 0.00114633
Iteration 17/25 | Loss: 0.00114633
Iteration 18/25 | Loss: 0.00114633
Iteration 19/25 | Loss: 0.00114633
Iteration 20/25 | Loss: 0.00114633
Iteration 21/25 | Loss: 0.00114633
Iteration 22/25 | Loss: 0.00114633
Iteration 23/25 | Loss: 0.00114633
Iteration 24/25 | Loss: 0.00114633
Iteration 25/25 | Loss: 0.00114633

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114633
Iteration 2/1000 | Loss: 0.00008795
Iteration 3/1000 | Loss: 0.00005787
Iteration 4/1000 | Loss: 0.00004858
Iteration 5/1000 | Loss: 0.00016228
Iteration 6/1000 | Loss: 0.00008015
Iteration 7/1000 | Loss: 0.00004827
Iteration 8/1000 | Loss: 0.00008014
Iteration 9/1000 | Loss: 0.00003912
Iteration 10/1000 | Loss: 0.00003747
Iteration 11/1000 | Loss: 0.00003667
Iteration 12/1000 | Loss: 0.00092588
Iteration 13/1000 | Loss: 0.00065261
Iteration 14/1000 | Loss: 0.00005178
Iteration 15/1000 | Loss: 0.00003954
Iteration 16/1000 | Loss: 0.00003163
Iteration 17/1000 | Loss: 0.00014398
Iteration 18/1000 | Loss: 0.00002503
Iteration 19/1000 | Loss: 0.00002286
Iteration 20/1000 | Loss: 0.00002152
Iteration 21/1000 | Loss: 0.00002037
Iteration 22/1000 | Loss: 0.00016185
Iteration 23/1000 | Loss: 0.00001965
Iteration 24/1000 | Loss: 0.00001886
Iteration 25/1000 | Loss: 0.00001949
Iteration 26/1000 | Loss: 0.00013418
Iteration 27/1000 | Loss: 0.00001856
Iteration 28/1000 | Loss: 0.00001824
Iteration 29/1000 | Loss: 0.00001799
Iteration 30/1000 | Loss: 0.00014476
Iteration 31/1000 | Loss: 0.00003195
Iteration 32/1000 | Loss: 0.00005400
Iteration 33/1000 | Loss: 0.00001794
Iteration 34/1000 | Loss: 0.00005137
Iteration 35/1000 | Loss: 0.00001779
Iteration 36/1000 | Loss: 0.00009339
Iteration 37/1000 | Loss: 0.00001769
Iteration 38/1000 | Loss: 0.00001768
Iteration 39/1000 | Loss: 0.00001768
Iteration 40/1000 | Loss: 0.00001767
Iteration 41/1000 | Loss: 0.00001763
Iteration 42/1000 | Loss: 0.00001763
Iteration 43/1000 | Loss: 0.00001763
Iteration 44/1000 | Loss: 0.00001763
Iteration 45/1000 | Loss: 0.00001763
Iteration 46/1000 | Loss: 0.00001763
Iteration 47/1000 | Loss: 0.00001763
Iteration 48/1000 | Loss: 0.00001763
Iteration 49/1000 | Loss: 0.00001763
Iteration 50/1000 | Loss: 0.00001763
Iteration 51/1000 | Loss: 0.00001763
Iteration 52/1000 | Loss: 0.00001763
Iteration 53/1000 | Loss: 0.00001762
Iteration 54/1000 | Loss: 0.00001762
Iteration 55/1000 | Loss: 0.00001762
Iteration 56/1000 | Loss: 0.00001761
Iteration 57/1000 | Loss: 0.00001761
Iteration 58/1000 | Loss: 0.00001760
Iteration 59/1000 | Loss: 0.00001760
Iteration 60/1000 | Loss: 0.00001759
Iteration 61/1000 | Loss: 0.00001759
Iteration 62/1000 | Loss: 0.00001759
Iteration 63/1000 | Loss: 0.00001758
Iteration 64/1000 | Loss: 0.00001933
Iteration 65/1000 | Loss: 0.00001933
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001756
Iteration 68/1000 | Loss: 0.00001756
Iteration 69/1000 | Loss: 0.00001756
Iteration 70/1000 | Loss: 0.00001756
Iteration 71/1000 | Loss: 0.00001756
Iteration 72/1000 | Loss: 0.00001756
Iteration 73/1000 | Loss: 0.00001756
Iteration 74/1000 | Loss: 0.00001756
Iteration 75/1000 | Loss: 0.00001756
Iteration 76/1000 | Loss: 0.00001756
Iteration 77/1000 | Loss: 0.00001755
Iteration 78/1000 | Loss: 0.00001755
Iteration 79/1000 | Loss: 0.00001755
Iteration 80/1000 | Loss: 0.00001755
Iteration 81/1000 | Loss: 0.00001755
Iteration 82/1000 | Loss: 0.00001755
Iteration 83/1000 | Loss: 0.00001755
Iteration 84/1000 | Loss: 0.00001755
Iteration 85/1000 | Loss: 0.00001755
Iteration 86/1000 | Loss: 0.00001755
Iteration 87/1000 | Loss: 0.00001755
Iteration 88/1000 | Loss: 0.00001755
Iteration 89/1000 | Loss: 0.00001754
Iteration 90/1000 | Loss: 0.00001754
Iteration 91/1000 | Loss: 0.00001754
Iteration 92/1000 | Loss: 0.00001754
Iteration 93/1000 | Loss: 0.00001754
Iteration 94/1000 | Loss: 0.00001754
Iteration 95/1000 | Loss: 0.00001754
Iteration 96/1000 | Loss: 0.00001754
Iteration 97/1000 | Loss: 0.00001753
Iteration 98/1000 | Loss: 0.00001753
Iteration 99/1000 | Loss: 0.00001752
Iteration 100/1000 | Loss: 0.00001752
Iteration 101/1000 | Loss: 0.00001752
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Iteration 106/1000 | Loss: 0.00001750
Iteration 107/1000 | Loss: 0.00001750
Iteration 108/1000 | Loss: 0.00001750
Iteration 109/1000 | Loss: 0.00001750
Iteration 110/1000 | Loss: 0.00001749
Iteration 111/1000 | Loss: 0.00001749
Iteration 112/1000 | Loss: 0.00001749
Iteration 113/1000 | Loss: 0.00001749
Iteration 114/1000 | Loss: 0.00001749
Iteration 115/1000 | Loss: 0.00001749
Iteration 116/1000 | Loss: 0.00001749
Iteration 117/1000 | Loss: 0.00001749
Iteration 118/1000 | Loss: 0.00001749
Iteration 119/1000 | Loss: 0.00001749
Iteration 120/1000 | Loss: 0.00001748
Iteration 121/1000 | Loss: 0.00001748
Iteration 122/1000 | Loss: 0.00001748
Iteration 123/1000 | Loss: 0.00001748
Iteration 124/1000 | Loss: 0.00001748
Iteration 125/1000 | Loss: 0.00001748
Iteration 126/1000 | Loss: 0.00001748
Iteration 127/1000 | Loss: 0.00001748
Iteration 128/1000 | Loss: 0.00001748
Iteration 129/1000 | Loss: 0.00001748
Iteration 130/1000 | Loss: 0.00001748
Iteration 131/1000 | Loss: 0.00001748
Iteration 132/1000 | Loss: 0.00001748
Iteration 133/1000 | Loss: 0.00001748
Iteration 134/1000 | Loss: 0.00001748
Iteration 135/1000 | Loss: 0.00001748
Iteration 136/1000 | Loss: 0.00001748
Iteration 137/1000 | Loss: 0.00001748
Iteration 138/1000 | Loss: 0.00001748
Iteration 139/1000 | Loss: 0.00001748
Iteration 140/1000 | Loss: 0.00001748
Iteration 141/1000 | Loss: 0.00001748
Iteration 142/1000 | Loss: 0.00001748
Iteration 143/1000 | Loss: 0.00001748
Iteration 144/1000 | Loss: 0.00001748
Iteration 145/1000 | Loss: 0.00001748
Iteration 146/1000 | Loss: 0.00001748
Iteration 147/1000 | Loss: 0.00001748
Iteration 148/1000 | Loss: 0.00001748
Iteration 149/1000 | Loss: 0.00001748
Iteration 150/1000 | Loss: 0.00001748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.7475791537435725e-05, 1.7475791537435725e-05, 1.7475791537435725e-05, 1.7475791537435725e-05, 1.7475791537435725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7475791537435725e-05

Optimization complete. Final v2v error: 2.7060718536376953 mm

Highest mean error: 21.61674690246582 mm for frame 22

Lowest mean error: 2.1584835052490234 mm for frame 148

Saving results

Total time: 94.47604084014893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01129558
Iteration 2/25 | Loss: 0.01129558
Iteration 3/25 | Loss: 0.01129557
Iteration 4/25 | Loss: 0.01129557
Iteration 5/25 | Loss: 0.01129556
Iteration 6/25 | Loss: 0.00260400
Iteration 7/25 | Loss: 0.00196683
Iteration 8/25 | Loss: 0.00171093
Iteration 9/25 | Loss: 0.00156722
Iteration 10/25 | Loss: 0.00138487
Iteration 11/25 | Loss: 0.00134495
Iteration 12/25 | Loss: 0.00133672
Iteration 13/25 | Loss: 0.00133241
Iteration 14/25 | Loss: 0.00132942
Iteration 15/25 | Loss: 0.00132887
Iteration 16/25 | Loss: 0.00132855
Iteration 17/25 | Loss: 0.00132834
Iteration 18/25 | Loss: 0.00132822
Iteration 19/25 | Loss: 0.00132811
Iteration 20/25 | Loss: 0.00132803
Iteration 21/25 | Loss: 0.00132801
Iteration 22/25 | Loss: 0.00132801
Iteration 23/25 | Loss: 0.00132800
Iteration 24/25 | Loss: 0.00132800
Iteration 25/25 | Loss: 0.00132800

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17069054
Iteration 2/25 | Loss: 0.00218370
Iteration 3/25 | Loss: 0.00218370
Iteration 4/25 | Loss: 0.00218370
Iteration 5/25 | Loss: 0.00218370
Iteration 6/25 | Loss: 0.00218370
Iteration 7/25 | Loss: 0.00218370
Iteration 8/25 | Loss: 0.00218370
Iteration 9/25 | Loss: 0.00218370
Iteration 10/25 | Loss: 0.00218370
Iteration 11/25 | Loss: 0.00218370
Iteration 12/25 | Loss: 0.00218370
Iteration 13/25 | Loss: 0.00218370
Iteration 14/25 | Loss: 0.00218370
Iteration 15/25 | Loss: 0.00218370
Iteration 16/25 | Loss: 0.00218370
Iteration 17/25 | Loss: 0.00218370
Iteration 18/25 | Loss: 0.00218370
Iteration 19/25 | Loss: 0.00218370
Iteration 20/25 | Loss: 0.00218370
Iteration 21/25 | Loss: 0.00218370
Iteration 22/25 | Loss: 0.00218370
Iteration 23/25 | Loss: 0.00218370
Iteration 24/25 | Loss: 0.00218370
Iteration 25/25 | Loss: 0.00218370

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218370
Iteration 2/1000 | Loss: 0.00032309
Iteration 3/1000 | Loss: 0.00025490
Iteration 4/1000 | Loss: 0.00022380
Iteration 5/1000 | Loss: 0.00020433
Iteration 6/1000 | Loss: 0.00018761
Iteration 7/1000 | Loss: 0.00017838
Iteration 8/1000 | Loss: 0.00017064
Iteration 9/1000 | Loss: 0.00016481
Iteration 10/1000 | Loss: 0.00016515
Iteration 11/1000 | Loss: 0.00015767
Iteration 12/1000 | Loss: 0.00015938
Iteration 13/1000 | Loss: 0.00015609
Iteration 14/1000 | Loss: 0.00015389
Iteration 15/1000 | Loss: 0.00015251
Iteration 16/1000 | Loss: 0.00015138
Iteration 17/1000 | Loss: 0.00015030
Iteration 18/1000 | Loss: 0.00014937
Iteration 19/1000 | Loss: 0.00014861
Iteration 20/1000 | Loss: 0.00014821
Iteration 21/1000 | Loss: 0.00014776
Iteration 22/1000 | Loss: 0.00014751
Iteration 23/1000 | Loss: 0.00014727
Iteration 24/1000 | Loss: 0.00014704
Iteration 25/1000 | Loss: 0.00014699
Iteration 26/1000 | Loss: 0.00014981
Iteration 27/1000 | Loss: 0.00014734
Iteration 28/1000 | Loss: 0.00014693
Iteration 29/1000 | Loss: 0.00014666
Iteration 30/1000 | Loss: 0.00014641
Iteration 31/1000 | Loss: 0.00014640
Iteration 32/1000 | Loss: 0.00014640
Iteration 33/1000 | Loss: 0.00014640
Iteration 34/1000 | Loss: 0.00014639
Iteration 35/1000 | Loss: 0.00014639
Iteration 36/1000 | Loss: 0.00014638
Iteration 37/1000 | Loss: 0.00014629
Iteration 38/1000 | Loss: 0.00014622
Iteration 39/1000 | Loss: 0.00014620
Iteration 40/1000 | Loss: 0.00014615
Iteration 41/1000 | Loss: 0.00014621
Iteration 42/1000 | Loss: 0.00014620
Iteration 43/1000 | Loss: 0.00014604
Iteration 44/1000 | Loss: 0.00014604
Iteration 45/1000 | Loss: 0.00014603
Iteration 46/1000 | Loss: 0.00014603
Iteration 47/1000 | Loss: 0.00014603
Iteration 48/1000 | Loss: 0.00014603
Iteration 49/1000 | Loss: 0.00014603
Iteration 50/1000 | Loss: 0.00014603
Iteration 51/1000 | Loss: 0.00014603
Iteration 52/1000 | Loss: 0.00014603
Iteration 53/1000 | Loss: 0.00014603
Iteration 54/1000 | Loss: 0.00014603
Iteration 55/1000 | Loss: 0.00014602
Iteration 56/1000 | Loss: 0.00014602
Iteration 57/1000 | Loss: 0.00014602
Iteration 58/1000 | Loss: 0.00014602
Iteration 59/1000 | Loss: 0.00014602
Iteration 60/1000 | Loss: 0.00014602
Iteration 61/1000 | Loss: 0.00014602
Iteration 62/1000 | Loss: 0.00014602
Iteration 63/1000 | Loss: 0.00014602
Iteration 64/1000 | Loss: 0.00014602
Iteration 65/1000 | Loss: 0.00014601
Iteration 66/1000 | Loss: 0.00014600
Iteration 67/1000 | Loss: 0.00014600
Iteration 68/1000 | Loss: 0.00014600
Iteration 69/1000 | Loss: 0.00014604
Iteration 70/1000 | Loss: 0.00014595
Iteration 71/1000 | Loss: 0.00014595
Iteration 72/1000 | Loss: 0.00014584
Iteration 73/1000 | Loss: 0.00014583
Iteration 74/1000 | Loss: 0.00014583
Iteration 75/1000 | Loss: 0.00014583
Iteration 76/1000 | Loss: 0.00014582
Iteration 77/1000 | Loss: 0.00014582
Iteration 78/1000 | Loss: 0.00014581
Iteration 79/1000 | Loss: 0.00014581
Iteration 80/1000 | Loss: 0.00014581
Iteration 81/1000 | Loss: 0.00014580
Iteration 82/1000 | Loss: 0.00014636
Iteration 83/1000 | Loss: 0.00014594
Iteration 84/1000 | Loss: 0.00014578
Iteration 85/1000 | Loss: 0.00014578
Iteration 86/1000 | Loss: 0.00014578
Iteration 87/1000 | Loss: 0.00014578
Iteration 88/1000 | Loss: 0.00014578
Iteration 89/1000 | Loss: 0.00014578
Iteration 90/1000 | Loss: 0.00014578
Iteration 91/1000 | Loss: 0.00014578
Iteration 92/1000 | Loss: 0.00014578
Iteration 93/1000 | Loss: 0.00014578
Iteration 94/1000 | Loss: 0.00014578
Iteration 95/1000 | Loss: 0.00014578
Iteration 96/1000 | Loss: 0.00014622
Iteration 97/1000 | Loss: 0.00014591
Iteration 98/1000 | Loss: 0.00014578
Iteration 99/1000 | Loss: 0.00014613
Iteration 100/1000 | Loss: 0.00014584
Iteration 101/1000 | Loss: 0.00014584
Iteration 102/1000 | Loss: 0.00014584
Iteration 103/1000 | Loss: 0.00014584
Iteration 104/1000 | Loss: 0.00014584
Iteration 105/1000 | Loss: 0.00014584
Iteration 106/1000 | Loss: 0.00014584
Iteration 107/1000 | Loss: 0.00014584
Iteration 108/1000 | Loss: 0.00014584
Iteration 109/1000 | Loss: 0.00014584
Iteration 110/1000 | Loss: 0.00014584
Iteration 111/1000 | Loss: 0.00014584
Iteration 112/1000 | Loss: 0.00014584
Iteration 113/1000 | Loss: 0.00014584
Iteration 114/1000 | Loss: 0.00014584
Iteration 115/1000 | Loss: 0.00014584
Iteration 116/1000 | Loss: 0.00014584
Iteration 117/1000 | Loss: 0.00014584
Iteration 118/1000 | Loss: 0.00014584
Iteration 119/1000 | Loss: 0.00014584
Iteration 120/1000 | Loss: 0.00014584
Iteration 121/1000 | Loss: 0.00014584
Iteration 122/1000 | Loss: 0.00014584
Iteration 123/1000 | Loss: 0.00014584
Iteration 124/1000 | Loss: 0.00014584
Iteration 125/1000 | Loss: 0.00014584
Iteration 126/1000 | Loss: 0.00014584
Iteration 127/1000 | Loss: 0.00014584
Iteration 128/1000 | Loss: 0.00014584
Iteration 129/1000 | Loss: 0.00014584
Iteration 130/1000 | Loss: 0.00014584
Iteration 131/1000 | Loss: 0.00014584
Iteration 132/1000 | Loss: 0.00014584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [0.00014583877054974437, 0.00014583877054974437, 0.00014583877054974437, 0.00014583877054974437, 0.00014583877054974437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014583877054974437

Optimization complete. Final v2v error: 6.861269950866699 mm

Highest mean error: 11.754165649414062 mm for frame 93

Lowest mean error: 4.510772228240967 mm for frame 65

Saving results

Total time: 97.8915376663208
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041522
Iteration 2/25 | Loss: 0.00348053
Iteration 3/25 | Loss: 0.00180847
Iteration 4/25 | Loss: 0.00164443
Iteration 5/25 | Loss: 0.00156490
Iteration 6/25 | Loss: 0.00161559
Iteration 7/25 | Loss: 0.00142839
Iteration 8/25 | Loss: 0.00137575
Iteration 9/25 | Loss: 0.00133900
Iteration 10/25 | Loss: 0.00132977
Iteration 11/25 | Loss: 0.00130292
Iteration 12/25 | Loss: 0.00128371
Iteration 13/25 | Loss: 0.00126981
Iteration 14/25 | Loss: 0.00126040
Iteration 15/25 | Loss: 0.00126336
Iteration 16/25 | Loss: 0.00125590
Iteration 17/25 | Loss: 0.00125431
Iteration 18/25 | Loss: 0.00125181
Iteration 19/25 | Loss: 0.00125474
Iteration 20/25 | Loss: 0.00124893
Iteration 21/25 | Loss: 0.00124438
Iteration 22/25 | Loss: 0.00124399
Iteration 23/25 | Loss: 0.00124238
Iteration 24/25 | Loss: 0.00124418
Iteration 25/25 | Loss: 0.00124690

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27974498
Iteration 2/25 | Loss: 0.00462897
Iteration 3/25 | Loss: 0.00368852
Iteration 4/25 | Loss: 0.00368779
Iteration 5/25 | Loss: 0.00368779
Iteration 6/25 | Loss: 0.00368779
Iteration 7/25 | Loss: 0.00368779
Iteration 8/25 | Loss: 0.00368779
Iteration 9/25 | Loss: 0.00368779
Iteration 10/25 | Loss: 0.00368779
Iteration 11/25 | Loss: 0.00368779
Iteration 12/25 | Loss: 0.00368779
Iteration 13/25 | Loss: 0.00368779
Iteration 14/25 | Loss: 0.00368779
Iteration 15/25 | Loss: 0.00368779
Iteration 16/25 | Loss: 0.00368779
Iteration 17/25 | Loss: 0.00368779
Iteration 18/25 | Loss: 0.00368779
Iteration 19/25 | Loss: 0.00368779
Iteration 20/25 | Loss: 0.00368779
Iteration 21/25 | Loss: 0.00368779
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0036877866368740797, 0.0036877866368740797, 0.0036877866368740797, 0.0036877866368740797, 0.0036877866368740797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0036877866368740797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00368779
Iteration 2/1000 | Loss: 0.00175412
Iteration 3/1000 | Loss: 0.00102072
Iteration 4/1000 | Loss: 0.00104447
Iteration 5/1000 | Loss: 0.00185210
Iteration 6/1000 | Loss: 0.00079914
Iteration 7/1000 | Loss: 0.00264287
Iteration 8/1000 | Loss: 0.00070791
Iteration 9/1000 | Loss: 0.00144605
Iteration 10/1000 | Loss: 0.00193837
Iteration 11/1000 | Loss: 0.00067253
Iteration 12/1000 | Loss: 0.00176039
Iteration 13/1000 | Loss: 0.00073198
Iteration 14/1000 | Loss: 0.00155068
Iteration 15/1000 | Loss: 0.00151838
Iteration 16/1000 | Loss: 0.00133112
Iteration 17/1000 | Loss: 0.00045328
Iteration 18/1000 | Loss: 0.00070202
Iteration 19/1000 | Loss: 0.00059299
Iteration 20/1000 | Loss: 0.00051048
Iteration 21/1000 | Loss: 0.00034225
Iteration 22/1000 | Loss: 0.00064922
Iteration 23/1000 | Loss: 0.00042770
Iteration 24/1000 | Loss: 0.00254919
Iteration 25/1000 | Loss: 0.00042352
Iteration 26/1000 | Loss: 0.00243974
Iteration 27/1000 | Loss: 0.00116609
Iteration 28/1000 | Loss: 0.00112354
Iteration 29/1000 | Loss: 0.00161747
Iteration 30/1000 | Loss: 0.00258557
Iteration 31/1000 | Loss: 0.00148731
Iteration 32/1000 | Loss: 0.00113640
Iteration 33/1000 | Loss: 0.00105846
Iteration 34/1000 | Loss: 0.00087512
Iteration 35/1000 | Loss: 0.00198259
Iteration 36/1000 | Loss: 0.00097927
Iteration 37/1000 | Loss: 0.00071554
Iteration 38/1000 | Loss: 0.00111665
Iteration 39/1000 | Loss: 0.00112135
Iteration 40/1000 | Loss: 0.00068805
Iteration 41/1000 | Loss: 0.00039806
Iteration 42/1000 | Loss: 0.00038305
Iteration 43/1000 | Loss: 0.00051064
Iteration 44/1000 | Loss: 0.00039780
Iteration 45/1000 | Loss: 0.00043707
Iteration 46/1000 | Loss: 0.00150078
Iteration 47/1000 | Loss: 0.00057454
Iteration 48/1000 | Loss: 0.00092344
Iteration 49/1000 | Loss: 0.00080961
Iteration 50/1000 | Loss: 0.00098258
Iteration 51/1000 | Loss: 0.00029586
Iteration 52/1000 | Loss: 0.00041046
Iteration 53/1000 | Loss: 0.00021082
Iteration 54/1000 | Loss: 0.00033124
Iteration 55/1000 | Loss: 0.00027731
Iteration 56/1000 | Loss: 0.00011626
Iteration 57/1000 | Loss: 0.00092364
Iteration 58/1000 | Loss: 0.00051366
Iteration 59/1000 | Loss: 0.00080962
Iteration 60/1000 | Loss: 0.00032694
Iteration 61/1000 | Loss: 0.00205938
Iteration 62/1000 | Loss: 0.00354846
Iteration 63/1000 | Loss: 0.00113106
Iteration 64/1000 | Loss: 0.00016321
Iteration 65/1000 | Loss: 0.00011327
Iteration 66/1000 | Loss: 0.00048933
Iteration 67/1000 | Loss: 0.00055800
Iteration 68/1000 | Loss: 0.00041525
Iteration 69/1000 | Loss: 0.00043443
Iteration 70/1000 | Loss: 0.00118359
Iteration 71/1000 | Loss: 0.00051233
Iteration 72/1000 | Loss: 0.00061250
Iteration 73/1000 | Loss: 0.00057483
Iteration 74/1000 | Loss: 0.00010118
Iteration 75/1000 | Loss: 0.00038991
Iteration 76/1000 | Loss: 0.00041173
Iteration 77/1000 | Loss: 0.00019242
Iteration 78/1000 | Loss: 0.00100720
Iteration 79/1000 | Loss: 0.00045269
Iteration 80/1000 | Loss: 0.00017132
Iteration 81/1000 | Loss: 0.00096173
Iteration 82/1000 | Loss: 0.00067131
Iteration 83/1000 | Loss: 0.00044915
Iteration 84/1000 | Loss: 0.00042164
Iteration 85/1000 | Loss: 0.00007958
Iteration 86/1000 | Loss: 0.00011542
Iteration 87/1000 | Loss: 0.00058260
Iteration 88/1000 | Loss: 0.00021422
Iteration 89/1000 | Loss: 0.00047508
Iteration 90/1000 | Loss: 0.00019170
Iteration 91/1000 | Loss: 0.00009411
Iteration 92/1000 | Loss: 0.00010006
Iteration 93/1000 | Loss: 0.00006124
Iteration 94/1000 | Loss: 0.00008983
Iteration 95/1000 | Loss: 0.00006815
Iteration 96/1000 | Loss: 0.00045380
Iteration 97/1000 | Loss: 0.00027869
Iteration 98/1000 | Loss: 0.00033693
Iteration 99/1000 | Loss: 0.00026874
Iteration 100/1000 | Loss: 0.00026198
Iteration 101/1000 | Loss: 0.00025272
Iteration 102/1000 | Loss: 0.00060349
Iteration 103/1000 | Loss: 0.00031819
Iteration 104/1000 | Loss: 0.00031370
Iteration 105/1000 | Loss: 0.00020048
Iteration 106/1000 | Loss: 0.00015691
Iteration 107/1000 | Loss: 0.00022675
Iteration 108/1000 | Loss: 0.00027818
Iteration 109/1000 | Loss: 0.00024759
Iteration 110/1000 | Loss: 0.00041776
Iteration 111/1000 | Loss: 0.00028401
Iteration 112/1000 | Loss: 0.00044251
Iteration 113/1000 | Loss: 0.00049993
Iteration 114/1000 | Loss: 0.00041838
Iteration 115/1000 | Loss: 0.00045790
Iteration 116/1000 | Loss: 0.00034498
Iteration 117/1000 | Loss: 0.00021059
Iteration 118/1000 | Loss: 0.00045238
Iteration 119/1000 | Loss: 0.00056442
Iteration 120/1000 | Loss: 0.00060702
Iteration 121/1000 | Loss: 0.00018074
Iteration 122/1000 | Loss: 0.00007365
Iteration 123/1000 | Loss: 0.00011251
Iteration 124/1000 | Loss: 0.00014369
Iteration 125/1000 | Loss: 0.00014711
Iteration 126/1000 | Loss: 0.00017924
Iteration 127/1000 | Loss: 0.00013535
Iteration 128/1000 | Loss: 0.00012457
Iteration 129/1000 | Loss: 0.00010398
Iteration 130/1000 | Loss: 0.00008194
Iteration 131/1000 | Loss: 0.00006882
Iteration 132/1000 | Loss: 0.00008885
Iteration 133/1000 | Loss: 0.00008691
Iteration 134/1000 | Loss: 0.00028632
Iteration 135/1000 | Loss: 0.00014244
Iteration 136/1000 | Loss: 0.00005925
Iteration 137/1000 | Loss: 0.00014210
Iteration 138/1000 | Loss: 0.00006587
Iteration 139/1000 | Loss: 0.00019646
Iteration 140/1000 | Loss: 0.00004289
Iteration 141/1000 | Loss: 0.00005292
Iteration 142/1000 | Loss: 0.00012160
Iteration 143/1000 | Loss: 0.00004197
Iteration 144/1000 | Loss: 0.00004066
Iteration 145/1000 | Loss: 0.00010729
Iteration 146/1000 | Loss: 0.00004034
Iteration 147/1000 | Loss: 0.00003866
Iteration 148/1000 | Loss: 0.00005011
Iteration 149/1000 | Loss: 0.00010747
Iteration 150/1000 | Loss: 0.00004634
Iteration 151/1000 | Loss: 0.00004010
Iteration 152/1000 | Loss: 0.00004675
Iteration 153/1000 | Loss: 0.00003661
Iteration 154/1000 | Loss: 0.00004503
Iteration 155/1000 | Loss: 0.00004222
Iteration 156/1000 | Loss: 0.00004062
Iteration 157/1000 | Loss: 0.00003975
Iteration 158/1000 | Loss: 0.00009807
Iteration 159/1000 | Loss: 0.00016322
Iteration 160/1000 | Loss: 0.00009199
Iteration 161/1000 | Loss: 0.00032475
Iteration 162/1000 | Loss: 0.00003484
Iteration 163/1000 | Loss: 0.00009848
Iteration 164/1000 | Loss: 0.00003468
Iteration 165/1000 | Loss: 0.00003436
Iteration 166/1000 | Loss: 0.00003768
Iteration 167/1000 | Loss: 0.00004332
Iteration 168/1000 | Loss: 0.00003609
Iteration 169/1000 | Loss: 0.00003419
Iteration 170/1000 | Loss: 0.00003417
Iteration 171/1000 | Loss: 0.00003417
Iteration 172/1000 | Loss: 0.00003417
Iteration 173/1000 | Loss: 0.00003417
Iteration 174/1000 | Loss: 0.00003417
Iteration 175/1000 | Loss: 0.00003417
Iteration 176/1000 | Loss: 0.00003417
Iteration 177/1000 | Loss: 0.00003417
Iteration 178/1000 | Loss: 0.00003417
Iteration 179/1000 | Loss: 0.00003416
Iteration 180/1000 | Loss: 0.00003416
Iteration 181/1000 | Loss: 0.00003416
Iteration 182/1000 | Loss: 0.00003416
Iteration 183/1000 | Loss: 0.00003415
Iteration 184/1000 | Loss: 0.00003415
Iteration 185/1000 | Loss: 0.00003414
Iteration 186/1000 | Loss: 0.00003413
Iteration 187/1000 | Loss: 0.00003413
Iteration 188/1000 | Loss: 0.00003413
Iteration 189/1000 | Loss: 0.00003413
Iteration 190/1000 | Loss: 0.00003413
Iteration 191/1000 | Loss: 0.00003413
Iteration 192/1000 | Loss: 0.00003413
Iteration 193/1000 | Loss: 0.00003413
Iteration 194/1000 | Loss: 0.00003413
Iteration 195/1000 | Loss: 0.00003412
Iteration 196/1000 | Loss: 0.00003412
Iteration 197/1000 | Loss: 0.00003412
Iteration 198/1000 | Loss: 0.00003890
Iteration 199/1000 | Loss: 0.00004911
Iteration 200/1000 | Loss: 0.00003434
Iteration 201/1000 | Loss: 0.00003408
Iteration 202/1000 | Loss: 0.00003408
Iteration 203/1000 | Loss: 0.00003407
Iteration 204/1000 | Loss: 0.00003407
Iteration 205/1000 | Loss: 0.00003407
Iteration 206/1000 | Loss: 0.00003407
Iteration 207/1000 | Loss: 0.00003407
Iteration 208/1000 | Loss: 0.00003407
Iteration 209/1000 | Loss: 0.00003406
Iteration 210/1000 | Loss: 0.00003625
Iteration 211/1000 | Loss: 0.00003781
Iteration 212/1000 | Loss: 0.00003728
Iteration 213/1000 | Loss: 0.00003618
Iteration 214/1000 | Loss: 0.00003405
Iteration 215/1000 | Loss: 0.00003404
Iteration 216/1000 | Loss: 0.00003404
Iteration 217/1000 | Loss: 0.00003404
Iteration 218/1000 | Loss: 0.00003403
Iteration 219/1000 | Loss: 0.00003403
Iteration 220/1000 | Loss: 0.00003403
Iteration 221/1000 | Loss: 0.00003403
Iteration 222/1000 | Loss: 0.00003403
Iteration 223/1000 | Loss: 0.00003403
Iteration 224/1000 | Loss: 0.00003403
Iteration 225/1000 | Loss: 0.00003403
Iteration 226/1000 | Loss: 0.00003403
Iteration 227/1000 | Loss: 0.00003403
Iteration 228/1000 | Loss: 0.00003403
Iteration 229/1000 | Loss: 0.00003402
Iteration 230/1000 | Loss: 0.00003402
Iteration 231/1000 | Loss: 0.00003402
Iteration 232/1000 | Loss: 0.00003402
Iteration 233/1000 | Loss: 0.00003402
Iteration 234/1000 | Loss: 0.00003402
Iteration 235/1000 | Loss: 0.00003402
Iteration 236/1000 | Loss: 0.00003402
Iteration 237/1000 | Loss: 0.00003402
Iteration 238/1000 | Loss: 0.00003402
Iteration 239/1000 | Loss: 0.00003402
Iteration 240/1000 | Loss: 0.00003402
Iteration 241/1000 | Loss: 0.00003402
Iteration 242/1000 | Loss: 0.00003402
Iteration 243/1000 | Loss: 0.00003402
Iteration 244/1000 | Loss: 0.00003402
Iteration 245/1000 | Loss: 0.00003402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 245. Stopping optimization.
Last 5 losses: [3.40216611220967e-05, 3.40216611220967e-05, 3.40216611220967e-05, 3.40216611220967e-05, 3.40216611220967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.40216611220967e-05

Optimization complete. Final v2v error: 3.16707706451416 mm

Highest mean error: 12.839122772216797 mm for frame 141

Lowest mean error: 2.0673062801361084 mm for frame 16

Saving results

Total time: 340.97587418556213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834349
Iteration 2/25 | Loss: 0.00105507
Iteration 3/25 | Loss: 0.00090621
Iteration 4/25 | Loss: 0.00087854
Iteration 5/25 | Loss: 0.00087218
Iteration 6/25 | Loss: 0.00087057
Iteration 7/25 | Loss: 0.00087057
Iteration 8/25 | Loss: 0.00087057
Iteration 9/25 | Loss: 0.00087057
Iteration 10/25 | Loss: 0.00087057
Iteration 11/25 | Loss: 0.00087057
Iteration 12/25 | Loss: 0.00087057
Iteration 13/25 | Loss: 0.00087057
Iteration 14/25 | Loss: 0.00087057
Iteration 15/25 | Loss: 0.00087057
Iteration 16/25 | Loss: 0.00087057
Iteration 17/25 | Loss: 0.00087057
Iteration 18/25 | Loss: 0.00087057
Iteration 19/25 | Loss: 0.00087057
Iteration 20/25 | Loss: 0.00087057
Iteration 21/25 | Loss: 0.00087057
Iteration 22/25 | Loss: 0.00087057
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008705654181540012, 0.0008705654181540012, 0.0008705654181540012, 0.0008705654181540012, 0.0008705654181540012]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008705654181540012

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25847471
Iteration 2/25 | Loss: 0.00082974
Iteration 3/25 | Loss: 0.00082972
Iteration 4/25 | Loss: 0.00082972
Iteration 5/25 | Loss: 0.00082972
Iteration 6/25 | Loss: 0.00082972
Iteration 7/25 | Loss: 0.00082972
Iteration 8/25 | Loss: 0.00082972
Iteration 9/25 | Loss: 0.00082972
Iteration 10/25 | Loss: 0.00082972
Iteration 11/25 | Loss: 0.00082972
Iteration 12/25 | Loss: 0.00082972
Iteration 13/25 | Loss: 0.00082972
Iteration 14/25 | Loss: 0.00082972
Iteration 15/25 | Loss: 0.00082972
Iteration 16/25 | Loss: 0.00082972
Iteration 17/25 | Loss: 0.00082972
Iteration 18/25 | Loss: 0.00082972
Iteration 19/25 | Loss: 0.00082972
Iteration 20/25 | Loss: 0.00082972
Iteration 21/25 | Loss: 0.00082972
Iteration 22/25 | Loss: 0.00082972
Iteration 23/25 | Loss: 0.00082972
Iteration 24/25 | Loss: 0.00082972
Iteration 25/25 | Loss: 0.00082972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082972
Iteration 2/1000 | Loss: 0.00004534
Iteration 3/1000 | Loss: 0.00002515
Iteration 4/1000 | Loss: 0.00001876
Iteration 5/1000 | Loss: 0.00001617
Iteration 6/1000 | Loss: 0.00001512
Iteration 7/1000 | Loss: 0.00001440
Iteration 8/1000 | Loss: 0.00001379
Iteration 9/1000 | Loss: 0.00001339
Iteration 10/1000 | Loss: 0.00001309
Iteration 11/1000 | Loss: 0.00001291
Iteration 12/1000 | Loss: 0.00001284
Iteration 13/1000 | Loss: 0.00001283
Iteration 14/1000 | Loss: 0.00001282
Iteration 15/1000 | Loss: 0.00001281
Iteration 16/1000 | Loss: 0.00001273
Iteration 17/1000 | Loss: 0.00001273
Iteration 18/1000 | Loss: 0.00001270
Iteration 19/1000 | Loss: 0.00001269
Iteration 20/1000 | Loss: 0.00001267
Iteration 21/1000 | Loss: 0.00001267
Iteration 22/1000 | Loss: 0.00001266
Iteration 23/1000 | Loss: 0.00001266
Iteration 24/1000 | Loss: 0.00001265
Iteration 25/1000 | Loss: 0.00001259
Iteration 26/1000 | Loss: 0.00001256
Iteration 27/1000 | Loss: 0.00001254
Iteration 28/1000 | Loss: 0.00001254
Iteration 29/1000 | Loss: 0.00001254
Iteration 30/1000 | Loss: 0.00001254
Iteration 31/1000 | Loss: 0.00001254
Iteration 32/1000 | Loss: 0.00001254
Iteration 33/1000 | Loss: 0.00001253
Iteration 34/1000 | Loss: 0.00001253
Iteration 35/1000 | Loss: 0.00001253
Iteration 36/1000 | Loss: 0.00001253
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001252
Iteration 39/1000 | Loss: 0.00001252
Iteration 40/1000 | Loss: 0.00001251
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001250
Iteration 44/1000 | Loss: 0.00001250
Iteration 45/1000 | Loss: 0.00001249
Iteration 46/1000 | Loss: 0.00001249
Iteration 47/1000 | Loss: 0.00001246
Iteration 48/1000 | Loss: 0.00001245
Iteration 49/1000 | Loss: 0.00001245
Iteration 50/1000 | Loss: 0.00001245
Iteration 51/1000 | Loss: 0.00001245
Iteration 52/1000 | Loss: 0.00001244
Iteration 53/1000 | Loss: 0.00001244
Iteration 54/1000 | Loss: 0.00001243
Iteration 55/1000 | Loss: 0.00001243
Iteration 56/1000 | Loss: 0.00001243
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001243
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001242
Iteration 61/1000 | Loss: 0.00001241
Iteration 62/1000 | Loss: 0.00001241
Iteration 63/1000 | Loss: 0.00001241
Iteration 64/1000 | Loss: 0.00001241
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001240
Iteration 67/1000 | Loss: 0.00001240
Iteration 68/1000 | Loss: 0.00001240
Iteration 69/1000 | Loss: 0.00001240
Iteration 70/1000 | Loss: 0.00001240
Iteration 71/1000 | Loss: 0.00001240
Iteration 72/1000 | Loss: 0.00001239
Iteration 73/1000 | Loss: 0.00001239
Iteration 74/1000 | Loss: 0.00001239
Iteration 75/1000 | Loss: 0.00001239
Iteration 76/1000 | Loss: 0.00001239
Iteration 77/1000 | Loss: 0.00001238
Iteration 78/1000 | Loss: 0.00001238
Iteration 79/1000 | Loss: 0.00001238
Iteration 80/1000 | Loss: 0.00001238
Iteration 81/1000 | Loss: 0.00001238
Iteration 82/1000 | Loss: 0.00001238
Iteration 83/1000 | Loss: 0.00001238
Iteration 84/1000 | Loss: 0.00001238
Iteration 85/1000 | Loss: 0.00001237
Iteration 86/1000 | Loss: 0.00001237
Iteration 87/1000 | Loss: 0.00001237
Iteration 88/1000 | Loss: 0.00001236
Iteration 89/1000 | Loss: 0.00001236
Iteration 90/1000 | Loss: 0.00001236
Iteration 91/1000 | Loss: 0.00001236
Iteration 92/1000 | Loss: 0.00001236
Iteration 93/1000 | Loss: 0.00001236
Iteration 94/1000 | Loss: 0.00001236
Iteration 95/1000 | Loss: 0.00001235
Iteration 96/1000 | Loss: 0.00001235
Iteration 97/1000 | Loss: 0.00001235
Iteration 98/1000 | Loss: 0.00001234
Iteration 99/1000 | Loss: 0.00001234
Iteration 100/1000 | Loss: 0.00001234
Iteration 101/1000 | Loss: 0.00001234
Iteration 102/1000 | Loss: 0.00001234
Iteration 103/1000 | Loss: 0.00001234
Iteration 104/1000 | Loss: 0.00001234
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001233
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001233
Iteration 109/1000 | Loss: 0.00001232
Iteration 110/1000 | Loss: 0.00001232
Iteration 111/1000 | Loss: 0.00001232
Iteration 112/1000 | Loss: 0.00001231
Iteration 113/1000 | Loss: 0.00001231
Iteration 114/1000 | Loss: 0.00001231
Iteration 115/1000 | Loss: 0.00001231
Iteration 116/1000 | Loss: 0.00001231
Iteration 117/1000 | Loss: 0.00001231
Iteration 118/1000 | Loss: 0.00001231
Iteration 119/1000 | Loss: 0.00001231
Iteration 120/1000 | Loss: 0.00001230
Iteration 121/1000 | Loss: 0.00001230
Iteration 122/1000 | Loss: 0.00001230
Iteration 123/1000 | Loss: 0.00001230
Iteration 124/1000 | Loss: 0.00001230
Iteration 125/1000 | Loss: 0.00001230
Iteration 126/1000 | Loss: 0.00001229
Iteration 127/1000 | Loss: 0.00001229
Iteration 128/1000 | Loss: 0.00001229
Iteration 129/1000 | Loss: 0.00001229
Iteration 130/1000 | Loss: 0.00001229
Iteration 131/1000 | Loss: 0.00001229
Iteration 132/1000 | Loss: 0.00001229
Iteration 133/1000 | Loss: 0.00001229
Iteration 134/1000 | Loss: 0.00001229
Iteration 135/1000 | Loss: 0.00001229
Iteration 136/1000 | Loss: 0.00001229
Iteration 137/1000 | Loss: 0.00001229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.2293461622903123e-05, 1.2293461622903123e-05, 1.2293461622903123e-05, 1.2293461622903123e-05, 1.2293461622903123e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2293461622903123e-05

Optimization complete. Final v2v error: 2.9940898418426514 mm

Highest mean error: 3.2571158409118652 mm for frame 148

Lowest mean error: 2.392017364501953 mm for frame 0

Saving results

Total time: 38.44239115715027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817898
Iteration 2/25 | Loss: 0.00113456
Iteration 3/25 | Loss: 0.00095540
Iteration 4/25 | Loss: 0.00092600
Iteration 5/25 | Loss: 0.00091477
Iteration 6/25 | Loss: 0.00091223
Iteration 7/25 | Loss: 0.00091139
Iteration 8/25 | Loss: 0.00091132
Iteration 9/25 | Loss: 0.00091132
Iteration 10/25 | Loss: 0.00091132
Iteration 11/25 | Loss: 0.00091132
Iteration 12/25 | Loss: 0.00091132
Iteration 13/25 | Loss: 0.00091132
Iteration 14/25 | Loss: 0.00091132
Iteration 15/25 | Loss: 0.00091132
Iteration 16/25 | Loss: 0.00091132
Iteration 17/25 | Loss: 0.00091132
Iteration 18/25 | Loss: 0.00091132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009113209671340883, 0.0009113209671340883, 0.0009113209671340883, 0.0009113209671340883, 0.0009113209671340883]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009113209671340883

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25412655
Iteration 2/25 | Loss: 0.00099508
Iteration 3/25 | Loss: 0.00099508
Iteration 4/25 | Loss: 0.00099508
Iteration 5/25 | Loss: 0.00099508
Iteration 6/25 | Loss: 0.00099508
Iteration 7/25 | Loss: 0.00099508
Iteration 8/25 | Loss: 0.00099508
Iteration 9/25 | Loss: 0.00099508
Iteration 10/25 | Loss: 0.00099508
Iteration 11/25 | Loss: 0.00099508
Iteration 12/25 | Loss: 0.00099508
Iteration 13/25 | Loss: 0.00099508
Iteration 14/25 | Loss: 0.00099508
Iteration 15/25 | Loss: 0.00099508
Iteration 16/25 | Loss: 0.00099508
Iteration 17/25 | Loss: 0.00099508
Iteration 18/25 | Loss: 0.00099508
Iteration 19/25 | Loss: 0.00099508
Iteration 20/25 | Loss: 0.00099508
Iteration 21/25 | Loss: 0.00099508
Iteration 22/25 | Loss: 0.00099508
Iteration 23/25 | Loss: 0.00099508
Iteration 24/25 | Loss: 0.00099508
Iteration 25/25 | Loss: 0.00099508

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099508
Iteration 2/1000 | Loss: 0.00004606
Iteration 3/1000 | Loss: 0.00003288
Iteration 4/1000 | Loss: 0.00002496
Iteration 5/1000 | Loss: 0.00002288
Iteration 6/1000 | Loss: 0.00002182
Iteration 7/1000 | Loss: 0.00002101
Iteration 8/1000 | Loss: 0.00002058
Iteration 9/1000 | Loss: 0.00002014
Iteration 10/1000 | Loss: 0.00001970
Iteration 11/1000 | Loss: 0.00001953
Iteration 12/1000 | Loss: 0.00001948
Iteration 13/1000 | Loss: 0.00001935
Iteration 14/1000 | Loss: 0.00001931
Iteration 15/1000 | Loss: 0.00001927
Iteration 16/1000 | Loss: 0.00001925
Iteration 17/1000 | Loss: 0.00001915
Iteration 18/1000 | Loss: 0.00001914
Iteration 19/1000 | Loss: 0.00001914
Iteration 20/1000 | Loss: 0.00001913
Iteration 21/1000 | Loss: 0.00001913
Iteration 22/1000 | Loss: 0.00001912
Iteration 23/1000 | Loss: 0.00001912
Iteration 24/1000 | Loss: 0.00001912
Iteration 25/1000 | Loss: 0.00001911
Iteration 26/1000 | Loss: 0.00001910
Iteration 27/1000 | Loss: 0.00001907
Iteration 28/1000 | Loss: 0.00001906
Iteration 29/1000 | Loss: 0.00001905
Iteration 30/1000 | Loss: 0.00001905
Iteration 31/1000 | Loss: 0.00001905
Iteration 32/1000 | Loss: 0.00001902
Iteration 33/1000 | Loss: 0.00001893
Iteration 34/1000 | Loss: 0.00001891
Iteration 35/1000 | Loss: 0.00001891
Iteration 36/1000 | Loss: 0.00001890
Iteration 37/1000 | Loss: 0.00001889
Iteration 38/1000 | Loss: 0.00001889
Iteration 39/1000 | Loss: 0.00001888
Iteration 40/1000 | Loss: 0.00001888
Iteration 41/1000 | Loss: 0.00001881
Iteration 42/1000 | Loss: 0.00001881
Iteration 43/1000 | Loss: 0.00001881
Iteration 44/1000 | Loss: 0.00001881
Iteration 45/1000 | Loss: 0.00001880
Iteration 46/1000 | Loss: 0.00001880
Iteration 47/1000 | Loss: 0.00001880
Iteration 48/1000 | Loss: 0.00001879
Iteration 49/1000 | Loss: 0.00001879
Iteration 50/1000 | Loss: 0.00001878
Iteration 51/1000 | Loss: 0.00001878
Iteration 52/1000 | Loss: 0.00001878
Iteration 53/1000 | Loss: 0.00001877
Iteration 54/1000 | Loss: 0.00001877
Iteration 55/1000 | Loss: 0.00001876
Iteration 56/1000 | Loss: 0.00001876
Iteration 57/1000 | Loss: 0.00001876
Iteration 58/1000 | Loss: 0.00001876
Iteration 59/1000 | Loss: 0.00001876
Iteration 60/1000 | Loss: 0.00001876
Iteration 61/1000 | Loss: 0.00001876
Iteration 62/1000 | Loss: 0.00001876
Iteration 63/1000 | Loss: 0.00001876
Iteration 64/1000 | Loss: 0.00001876
Iteration 65/1000 | Loss: 0.00001876
Iteration 66/1000 | Loss: 0.00001875
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00001875
Iteration 69/1000 | Loss: 0.00001875
Iteration 70/1000 | Loss: 0.00001875
Iteration 71/1000 | Loss: 0.00001875
Iteration 72/1000 | Loss: 0.00001874
Iteration 73/1000 | Loss: 0.00001874
Iteration 74/1000 | Loss: 0.00001874
Iteration 75/1000 | Loss: 0.00001874
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001873
Iteration 78/1000 | Loss: 0.00001873
Iteration 79/1000 | Loss: 0.00001873
Iteration 80/1000 | Loss: 0.00001873
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001872
Iteration 83/1000 | Loss: 0.00001872
Iteration 84/1000 | Loss: 0.00001872
Iteration 85/1000 | Loss: 0.00001872
Iteration 86/1000 | Loss: 0.00001872
Iteration 87/1000 | Loss: 0.00001872
Iteration 88/1000 | Loss: 0.00001872
Iteration 89/1000 | Loss: 0.00001872
Iteration 90/1000 | Loss: 0.00001872
Iteration 91/1000 | Loss: 0.00001872
Iteration 92/1000 | Loss: 0.00001872
Iteration 93/1000 | Loss: 0.00001871
Iteration 94/1000 | Loss: 0.00001871
Iteration 95/1000 | Loss: 0.00001871
Iteration 96/1000 | Loss: 0.00001871
Iteration 97/1000 | Loss: 0.00001871
Iteration 98/1000 | Loss: 0.00001871
Iteration 99/1000 | Loss: 0.00001871
Iteration 100/1000 | Loss: 0.00001871
Iteration 101/1000 | Loss: 0.00001871
Iteration 102/1000 | Loss: 0.00001871
Iteration 103/1000 | Loss: 0.00001871
Iteration 104/1000 | Loss: 0.00001871
Iteration 105/1000 | Loss: 0.00001871
Iteration 106/1000 | Loss: 0.00001871
Iteration 107/1000 | Loss: 0.00001871
Iteration 108/1000 | Loss: 0.00001871
Iteration 109/1000 | Loss: 0.00001871
Iteration 110/1000 | Loss: 0.00001871
Iteration 111/1000 | Loss: 0.00001871
Iteration 112/1000 | Loss: 0.00001871
Iteration 113/1000 | Loss: 0.00001871
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.870501000666991e-05, 1.870501000666991e-05, 1.870501000666991e-05, 1.870501000666991e-05, 1.870501000666991e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.870501000666991e-05

Optimization complete. Final v2v error: 3.393536329269409 mm

Highest mean error: 3.7616727352142334 mm for frame 87

Lowest mean error: 2.4856765270233154 mm for frame 2

Saving results

Total time: 39.37280535697937
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871663
Iteration 2/25 | Loss: 0.00118984
Iteration 3/25 | Loss: 0.00098176
Iteration 4/25 | Loss: 0.00095510
Iteration 5/25 | Loss: 0.00094993
Iteration 6/25 | Loss: 0.00094896
Iteration 7/25 | Loss: 0.00094881
Iteration 8/25 | Loss: 0.00094881
Iteration 9/25 | Loss: 0.00094881
Iteration 10/25 | Loss: 0.00094881
Iteration 11/25 | Loss: 0.00094881
Iteration 12/25 | Loss: 0.00094881
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009488114155828953, 0.0009488114155828953, 0.0009488114155828953, 0.0009488114155828953, 0.0009488114155828953]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009488114155828953

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15678203
Iteration 2/25 | Loss: 0.00065149
Iteration 3/25 | Loss: 0.00065145
Iteration 4/25 | Loss: 0.00065145
Iteration 5/25 | Loss: 0.00065145
Iteration 6/25 | Loss: 0.00065145
Iteration 7/25 | Loss: 0.00065145
Iteration 8/25 | Loss: 0.00065145
Iteration 9/25 | Loss: 0.00065145
Iteration 10/25 | Loss: 0.00065145
Iteration 11/25 | Loss: 0.00065145
Iteration 12/25 | Loss: 0.00065145
Iteration 13/25 | Loss: 0.00065145
Iteration 14/25 | Loss: 0.00065145
Iteration 15/25 | Loss: 0.00065145
Iteration 16/25 | Loss: 0.00065145
Iteration 17/25 | Loss: 0.00065145
Iteration 18/25 | Loss: 0.00065145
Iteration 19/25 | Loss: 0.00065145
Iteration 20/25 | Loss: 0.00065145
Iteration 21/25 | Loss: 0.00065145
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006514483247883618, 0.0006514483247883618, 0.0006514483247883618, 0.0006514483247883618, 0.0006514483247883618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006514483247883618

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065145
Iteration 2/1000 | Loss: 0.00004604
Iteration 3/1000 | Loss: 0.00003394
Iteration 4/1000 | Loss: 0.00002652
Iteration 5/1000 | Loss: 0.00002247
Iteration 6/1000 | Loss: 0.00002108
Iteration 7/1000 | Loss: 0.00002012
Iteration 8/1000 | Loss: 0.00001960
Iteration 9/1000 | Loss: 0.00001914
Iteration 10/1000 | Loss: 0.00001884
Iteration 11/1000 | Loss: 0.00001862
Iteration 12/1000 | Loss: 0.00001857
Iteration 13/1000 | Loss: 0.00001839
Iteration 14/1000 | Loss: 0.00001829
Iteration 15/1000 | Loss: 0.00001814
Iteration 16/1000 | Loss: 0.00001813
Iteration 17/1000 | Loss: 0.00001811
Iteration 18/1000 | Loss: 0.00001811
Iteration 19/1000 | Loss: 0.00001810
Iteration 20/1000 | Loss: 0.00001810
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00001804
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001803
Iteration 25/1000 | Loss: 0.00001802
Iteration 26/1000 | Loss: 0.00001800
Iteration 27/1000 | Loss: 0.00001800
Iteration 28/1000 | Loss: 0.00001800
Iteration 29/1000 | Loss: 0.00001800
Iteration 30/1000 | Loss: 0.00001800
Iteration 31/1000 | Loss: 0.00001800
Iteration 32/1000 | Loss: 0.00001799
Iteration 33/1000 | Loss: 0.00001799
Iteration 34/1000 | Loss: 0.00001799
Iteration 35/1000 | Loss: 0.00001799
Iteration 36/1000 | Loss: 0.00001799
Iteration 37/1000 | Loss: 0.00001799
Iteration 38/1000 | Loss: 0.00001799
Iteration 39/1000 | Loss: 0.00001799
Iteration 40/1000 | Loss: 0.00001799
Iteration 41/1000 | Loss: 0.00001798
Iteration 42/1000 | Loss: 0.00001798
Iteration 43/1000 | Loss: 0.00001798
Iteration 44/1000 | Loss: 0.00001797
Iteration 45/1000 | Loss: 0.00001797
Iteration 46/1000 | Loss: 0.00001796
Iteration 47/1000 | Loss: 0.00001796
Iteration 48/1000 | Loss: 0.00001795
Iteration 49/1000 | Loss: 0.00001794
Iteration 50/1000 | Loss: 0.00001794
Iteration 51/1000 | Loss: 0.00001794
Iteration 52/1000 | Loss: 0.00001794
Iteration 53/1000 | Loss: 0.00001794
Iteration 54/1000 | Loss: 0.00001794
Iteration 55/1000 | Loss: 0.00001794
Iteration 56/1000 | Loss: 0.00001793
Iteration 57/1000 | Loss: 0.00001793
Iteration 58/1000 | Loss: 0.00001793
Iteration 59/1000 | Loss: 0.00001793
Iteration 60/1000 | Loss: 0.00001792
Iteration 61/1000 | Loss: 0.00001792
Iteration 62/1000 | Loss: 0.00001792
Iteration 63/1000 | Loss: 0.00001792
Iteration 64/1000 | Loss: 0.00001792
Iteration 65/1000 | Loss: 0.00001792
Iteration 66/1000 | Loss: 0.00001791
Iteration 67/1000 | Loss: 0.00001791
Iteration 68/1000 | Loss: 0.00001791
Iteration 69/1000 | Loss: 0.00001791
Iteration 70/1000 | Loss: 0.00001791
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001791
Iteration 73/1000 | Loss: 0.00001791
Iteration 74/1000 | Loss: 0.00001791
Iteration 75/1000 | Loss: 0.00001791
Iteration 76/1000 | Loss: 0.00001791
Iteration 77/1000 | Loss: 0.00001791
Iteration 78/1000 | Loss: 0.00001791
Iteration 79/1000 | Loss: 0.00001790
Iteration 80/1000 | Loss: 0.00001790
Iteration 81/1000 | Loss: 0.00001790
Iteration 82/1000 | Loss: 0.00001790
Iteration 83/1000 | Loss: 0.00001790
Iteration 84/1000 | Loss: 0.00001790
Iteration 85/1000 | Loss: 0.00001789
Iteration 86/1000 | Loss: 0.00001789
Iteration 87/1000 | Loss: 0.00001789
Iteration 88/1000 | Loss: 0.00001789
Iteration 89/1000 | Loss: 0.00001789
Iteration 90/1000 | Loss: 0.00001789
Iteration 91/1000 | Loss: 0.00001789
Iteration 92/1000 | Loss: 0.00001788
Iteration 93/1000 | Loss: 0.00001788
Iteration 94/1000 | Loss: 0.00001787
Iteration 95/1000 | Loss: 0.00001787
Iteration 96/1000 | Loss: 0.00001787
Iteration 97/1000 | Loss: 0.00001787
Iteration 98/1000 | Loss: 0.00001787
Iteration 99/1000 | Loss: 0.00001787
Iteration 100/1000 | Loss: 0.00001786
Iteration 101/1000 | Loss: 0.00001786
Iteration 102/1000 | Loss: 0.00001786
Iteration 103/1000 | Loss: 0.00001786
Iteration 104/1000 | Loss: 0.00001786
Iteration 105/1000 | Loss: 0.00001785
Iteration 106/1000 | Loss: 0.00001785
Iteration 107/1000 | Loss: 0.00001784
Iteration 108/1000 | Loss: 0.00001784
Iteration 109/1000 | Loss: 0.00001784
Iteration 110/1000 | Loss: 0.00001784
Iteration 111/1000 | Loss: 0.00001784
Iteration 112/1000 | Loss: 0.00001784
Iteration 113/1000 | Loss: 0.00001784
Iteration 114/1000 | Loss: 0.00001784
Iteration 115/1000 | Loss: 0.00001784
Iteration 116/1000 | Loss: 0.00001784
Iteration 117/1000 | Loss: 0.00001783
Iteration 118/1000 | Loss: 0.00001783
Iteration 119/1000 | Loss: 0.00001783
Iteration 120/1000 | Loss: 0.00001783
Iteration 121/1000 | Loss: 0.00001783
Iteration 122/1000 | Loss: 0.00001783
Iteration 123/1000 | Loss: 0.00001783
Iteration 124/1000 | Loss: 0.00001783
Iteration 125/1000 | Loss: 0.00001783
Iteration 126/1000 | Loss: 0.00001783
Iteration 127/1000 | Loss: 0.00001783
Iteration 128/1000 | Loss: 0.00001783
Iteration 129/1000 | Loss: 0.00001782
Iteration 130/1000 | Loss: 0.00001782
Iteration 131/1000 | Loss: 0.00001782
Iteration 132/1000 | Loss: 0.00001782
Iteration 133/1000 | Loss: 0.00001782
Iteration 134/1000 | Loss: 0.00001782
Iteration 135/1000 | Loss: 0.00001782
Iteration 136/1000 | Loss: 0.00001782
Iteration 137/1000 | Loss: 0.00001782
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001782
Iteration 140/1000 | Loss: 0.00001782
Iteration 141/1000 | Loss: 0.00001782
Iteration 142/1000 | Loss: 0.00001782
Iteration 143/1000 | Loss: 0.00001782
Iteration 144/1000 | Loss: 0.00001782
Iteration 145/1000 | Loss: 0.00001782
Iteration 146/1000 | Loss: 0.00001782
Iteration 147/1000 | Loss: 0.00001782
Iteration 148/1000 | Loss: 0.00001782
Iteration 149/1000 | Loss: 0.00001782
Iteration 150/1000 | Loss: 0.00001782
Iteration 151/1000 | Loss: 0.00001782
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.7818185369833373e-05, 1.7818185369833373e-05, 1.7818185369833373e-05, 1.7818185369833373e-05, 1.7818185369833373e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7818185369833373e-05

Optimization complete. Final v2v error: 3.5413031578063965 mm

Highest mean error: 3.9932024478912354 mm for frame 121

Lowest mean error: 2.9423787593841553 mm for frame 111

Saving results

Total time: 39.15741586685181
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029305
Iteration 2/25 | Loss: 0.00396840
Iteration 3/25 | Loss: 0.00257174
Iteration 4/25 | Loss: 0.00182945
Iteration 5/25 | Loss: 0.00175066
Iteration 6/25 | Loss: 0.00154204
Iteration 7/25 | Loss: 0.00144225
Iteration 8/25 | Loss: 0.00138280
Iteration 9/25 | Loss: 0.00133557
Iteration 10/25 | Loss: 0.00130633
Iteration 11/25 | Loss: 0.00129346
Iteration 12/25 | Loss: 0.00128630
Iteration 13/25 | Loss: 0.00127651
Iteration 14/25 | Loss: 0.00126908
Iteration 15/25 | Loss: 0.00126217
Iteration 16/25 | Loss: 0.00126808
Iteration 17/25 | Loss: 0.00126908
Iteration 18/25 | Loss: 0.00126765
Iteration 19/25 | Loss: 0.00126920
Iteration 20/25 | Loss: 0.00126832
Iteration 21/25 | Loss: 0.00126848
Iteration 22/25 | Loss: 0.00126690
Iteration 23/25 | Loss: 0.00126433
Iteration 24/25 | Loss: 0.00126465
Iteration 25/25 | Loss: 0.00126019

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25526285
Iteration 2/25 | Loss: 0.00345276
Iteration 3/25 | Loss: 0.00345276
Iteration 4/25 | Loss: 0.00345276
Iteration 5/25 | Loss: 0.00345276
Iteration 6/25 | Loss: 0.00345276
Iteration 7/25 | Loss: 0.00345276
Iteration 8/25 | Loss: 0.00345276
Iteration 9/25 | Loss: 0.00345276
Iteration 10/25 | Loss: 0.00345276
Iteration 11/25 | Loss: 0.00345276
Iteration 12/25 | Loss: 0.00345276
Iteration 13/25 | Loss: 0.00345276
Iteration 14/25 | Loss: 0.00345276
Iteration 15/25 | Loss: 0.00345276
Iteration 16/25 | Loss: 0.00345276
Iteration 17/25 | Loss: 0.00345276
Iteration 18/25 | Loss: 0.00345276
Iteration 19/25 | Loss: 0.00345276
Iteration 20/25 | Loss: 0.00345276
Iteration 21/25 | Loss: 0.00345276
Iteration 22/25 | Loss: 0.00345276
Iteration 23/25 | Loss: 0.00345276
Iteration 24/25 | Loss: 0.00345276
Iteration 25/25 | Loss: 0.00345276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00345276
Iteration 2/1000 | Loss: 0.00055019
Iteration 3/1000 | Loss: 0.00032121
Iteration 4/1000 | Loss: 0.00026304
Iteration 5/1000 | Loss: 0.00023450
Iteration 6/1000 | Loss: 0.00030209
Iteration 7/1000 | Loss: 0.00026453
Iteration 8/1000 | Loss: 0.00023169
Iteration 9/1000 | Loss: 0.00018347
Iteration 10/1000 | Loss: 0.00020206
Iteration 11/1000 | Loss: 0.00016677
Iteration 12/1000 | Loss: 0.00017152
Iteration 13/1000 | Loss: 0.00023430
Iteration 14/1000 | Loss: 0.00015852
Iteration 15/1000 | Loss: 0.00054708
Iteration 16/1000 | Loss: 0.00080286
Iteration 17/1000 | Loss: 0.00063166
Iteration 18/1000 | Loss: 0.00046245
Iteration 19/1000 | Loss: 0.00023840
Iteration 20/1000 | Loss: 0.00067151
Iteration 21/1000 | Loss: 0.00039494
Iteration 22/1000 | Loss: 0.00017425
Iteration 23/1000 | Loss: 0.00022096
Iteration 24/1000 | Loss: 0.00023235
Iteration 25/1000 | Loss: 0.00019443
Iteration 26/1000 | Loss: 0.00018453
Iteration 27/1000 | Loss: 0.00019600
Iteration 28/1000 | Loss: 0.00025019
Iteration 29/1000 | Loss: 0.00037049
Iteration 30/1000 | Loss: 0.00055033
Iteration 31/1000 | Loss: 0.00019882
Iteration 32/1000 | Loss: 0.00025893
Iteration 33/1000 | Loss: 0.00022200
Iteration 34/1000 | Loss: 0.00045618
Iteration 35/1000 | Loss: 0.00054792
Iteration 36/1000 | Loss: 0.00022418
Iteration 37/1000 | Loss: 0.00016706
Iteration 38/1000 | Loss: 0.00023098
Iteration 39/1000 | Loss: 0.00023489
Iteration 40/1000 | Loss: 0.00023834
Iteration 41/1000 | Loss: 0.00037518
Iteration 42/1000 | Loss: 0.00059781
Iteration 43/1000 | Loss: 0.00040268
Iteration 44/1000 | Loss: 0.00029742
Iteration 45/1000 | Loss: 0.00021516
Iteration 46/1000 | Loss: 0.00038910
Iteration 47/1000 | Loss: 0.00031489
Iteration 48/1000 | Loss: 0.00034787
Iteration 49/1000 | Loss: 0.00034914
Iteration 50/1000 | Loss: 0.00036600
Iteration 51/1000 | Loss: 0.00038207
Iteration 52/1000 | Loss: 0.00035448
Iteration 53/1000 | Loss: 0.00040114
Iteration 54/1000 | Loss: 0.00014911
Iteration 55/1000 | Loss: 0.00025383
Iteration 56/1000 | Loss: 0.00031491
Iteration 57/1000 | Loss: 0.00033539
Iteration 58/1000 | Loss: 0.00027697
Iteration 59/1000 | Loss: 0.00026522
Iteration 60/1000 | Loss: 0.00034951
Iteration 61/1000 | Loss: 0.00034586
Iteration 62/1000 | Loss: 0.00043958
Iteration 63/1000 | Loss: 0.00025808
Iteration 64/1000 | Loss: 0.00027966
Iteration 65/1000 | Loss: 0.00014489
Iteration 66/1000 | Loss: 0.00023746
Iteration 67/1000 | Loss: 0.00022452
Iteration 68/1000 | Loss: 0.00023354
Iteration 69/1000 | Loss: 0.00022409
Iteration 70/1000 | Loss: 0.00040952
Iteration 71/1000 | Loss: 0.00030525
Iteration 72/1000 | Loss: 0.00027538
Iteration 73/1000 | Loss: 0.00027745
Iteration 74/1000 | Loss: 0.00014184
Iteration 75/1000 | Loss: 0.00020900
Iteration 76/1000 | Loss: 0.00021630
Iteration 77/1000 | Loss: 0.00014031
Iteration 78/1000 | Loss: 0.00022601
Iteration 79/1000 | Loss: 0.00014166
Iteration 80/1000 | Loss: 0.00020037
Iteration 81/1000 | Loss: 0.00013681
Iteration 82/1000 | Loss: 0.00032055
Iteration 83/1000 | Loss: 0.00029257
Iteration 84/1000 | Loss: 0.00023851
Iteration 85/1000 | Loss: 0.00015308
Iteration 86/1000 | Loss: 0.00013788
Iteration 87/1000 | Loss: 0.00013544
Iteration 88/1000 | Loss: 0.00013460
Iteration 89/1000 | Loss: 0.00013390
Iteration 90/1000 | Loss: 0.00013354
Iteration 91/1000 | Loss: 0.00013324
Iteration 92/1000 | Loss: 0.00013290
Iteration 93/1000 | Loss: 0.00013263
Iteration 94/1000 | Loss: 0.00013245
Iteration 95/1000 | Loss: 0.00013243
Iteration 96/1000 | Loss: 0.00013243
Iteration 97/1000 | Loss: 0.00013243
Iteration 98/1000 | Loss: 0.00013242
Iteration 99/1000 | Loss: 0.00013242
Iteration 100/1000 | Loss: 0.00013242
Iteration 101/1000 | Loss: 0.00013242
Iteration 102/1000 | Loss: 0.00013241
Iteration 103/1000 | Loss: 0.00013241
Iteration 104/1000 | Loss: 0.00013240
Iteration 105/1000 | Loss: 0.00013240
Iteration 106/1000 | Loss: 0.00013239
Iteration 107/1000 | Loss: 0.00013239
Iteration 108/1000 | Loss: 0.00013238
Iteration 109/1000 | Loss: 0.00013238
Iteration 110/1000 | Loss: 0.00013238
Iteration 111/1000 | Loss: 0.00013237
Iteration 112/1000 | Loss: 0.00013236
Iteration 113/1000 | Loss: 0.00013236
Iteration 114/1000 | Loss: 0.00013236
Iteration 115/1000 | Loss: 0.00013236
Iteration 116/1000 | Loss: 0.00013236
Iteration 117/1000 | Loss: 0.00013236
Iteration 118/1000 | Loss: 0.00013236
Iteration 119/1000 | Loss: 0.00013236
Iteration 120/1000 | Loss: 0.00013236
Iteration 121/1000 | Loss: 0.00013236
Iteration 122/1000 | Loss: 0.00013236
Iteration 123/1000 | Loss: 0.00013236
Iteration 124/1000 | Loss: 0.00013235
Iteration 125/1000 | Loss: 0.00013235
Iteration 126/1000 | Loss: 0.00013234
Iteration 127/1000 | Loss: 0.00013234
Iteration 128/1000 | Loss: 0.00013234
Iteration 129/1000 | Loss: 0.00013234
Iteration 130/1000 | Loss: 0.00013234
Iteration 131/1000 | Loss: 0.00013233
Iteration 132/1000 | Loss: 0.00013233
Iteration 133/1000 | Loss: 0.00013233
Iteration 134/1000 | Loss: 0.00013233
Iteration 135/1000 | Loss: 0.00013233
Iteration 136/1000 | Loss: 0.00013233
Iteration 137/1000 | Loss: 0.00013233
Iteration 138/1000 | Loss: 0.00013233
Iteration 139/1000 | Loss: 0.00013233
Iteration 140/1000 | Loss: 0.00013233
Iteration 141/1000 | Loss: 0.00013232
Iteration 142/1000 | Loss: 0.00013232
Iteration 143/1000 | Loss: 0.00013232
Iteration 144/1000 | Loss: 0.00013232
Iteration 145/1000 | Loss: 0.00013232
Iteration 146/1000 | Loss: 0.00013232
Iteration 147/1000 | Loss: 0.00013231
Iteration 148/1000 | Loss: 0.00013231
Iteration 149/1000 | Loss: 0.00013231
Iteration 150/1000 | Loss: 0.00013231
Iteration 151/1000 | Loss: 0.00013231
Iteration 152/1000 | Loss: 0.00013231
Iteration 153/1000 | Loss: 0.00013231
Iteration 154/1000 | Loss: 0.00013231
Iteration 155/1000 | Loss: 0.00013231
Iteration 156/1000 | Loss: 0.00013231
Iteration 157/1000 | Loss: 0.00013231
Iteration 158/1000 | Loss: 0.00013230
Iteration 159/1000 | Loss: 0.00013230
Iteration 160/1000 | Loss: 0.00013230
Iteration 161/1000 | Loss: 0.00013230
Iteration 162/1000 | Loss: 0.00013230
Iteration 163/1000 | Loss: 0.00013230
Iteration 164/1000 | Loss: 0.00013230
Iteration 165/1000 | Loss: 0.00013230
Iteration 166/1000 | Loss: 0.00013229
Iteration 167/1000 | Loss: 0.00013229
Iteration 168/1000 | Loss: 0.00013229
Iteration 169/1000 | Loss: 0.00013229
Iteration 170/1000 | Loss: 0.00013229
Iteration 171/1000 | Loss: 0.00013229
Iteration 172/1000 | Loss: 0.00013229
Iteration 173/1000 | Loss: 0.00013229
Iteration 174/1000 | Loss: 0.00013229
Iteration 175/1000 | Loss: 0.00013229
Iteration 176/1000 | Loss: 0.00013229
Iteration 177/1000 | Loss: 0.00013229
Iteration 178/1000 | Loss: 0.00013229
Iteration 179/1000 | Loss: 0.00013229
Iteration 180/1000 | Loss: 0.00013229
Iteration 181/1000 | Loss: 0.00013229
Iteration 182/1000 | Loss: 0.00013229
Iteration 183/1000 | Loss: 0.00013229
Iteration 184/1000 | Loss: 0.00013229
Iteration 185/1000 | Loss: 0.00013229
Iteration 186/1000 | Loss: 0.00013229
Iteration 187/1000 | Loss: 0.00013229
Iteration 188/1000 | Loss: 0.00013228
Iteration 189/1000 | Loss: 0.00013228
Iteration 190/1000 | Loss: 0.00013228
Iteration 191/1000 | Loss: 0.00013228
Iteration 192/1000 | Loss: 0.00013228
Iteration 193/1000 | Loss: 0.00013228
Iteration 194/1000 | Loss: 0.00013228
Iteration 195/1000 | Loss: 0.00013228
Iteration 196/1000 | Loss: 0.00013228
Iteration 197/1000 | Loss: 0.00013228
Iteration 198/1000 | Loss: 0.00013228
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 198. Stopping optimization.
Last 5 losses: [0.00013228422903921455, 0.00013228422903921455, 0.00013228422903921455, 0.00013228422903921455, 0.00013228422903921455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013228422903921455

Optimization complete. Final v2v error: 6.011199951171875 mm

Highest mean error: 12.065104484558105 mm for frame 11

Lowest mean error: 3.8629539012908936 mm for frame 47

Saving results

Total time: 208.11331868171692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756945
Iteration 2/25 | Loss: 0.00109120
Iteration 3/25 | Loss: 0.00091042
Iteration 4/25 | Loss: 0.00088852
Iteration 5/25 | Loss: 0.00088295
Iteration 6/25 | Loss: 0.00088172
Iteration 7/25 | Loss: 0.00088172
Iteration 8/25 | Loss: 0.00088172
Iteration 9/25 | Loss: 0.00088172
Iteration 10/25 | Loss: 0.00088172
Iteration 11/25 | Loss: 0.00088172
Iteration 12/25 | Loss: 0.00088172
Iteration 13/25 | Loss: 0.00088172
Iteration 14/25 | Loss: 0.00088172
Iteration 15/25 | Loss: 0.00088172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008817181806080043, 0.0008817181806080043, 0.0008817181806080043, 0.0008817181806080043, 0.0008817181806080043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008817181806080043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.93593216
Iteration 2/25 | Loss: 0.00072079
Iteration 3/25 | Loss: 0.00072068
Iteration 4/25 | Loss: 0.00072068
Iteration 5/25 | Loss: 0.00072068
Iteration 6/25 | Loss: 0.00072068
Iteration 7/25 | Loss: 0.00072068
Iteration 8/25 | Loss: 0.00072068
Iteration 9/25 | Loss: 0.00072068
Iteration 10/25 | Loss: 0.00072068
Iteration 11/25 | Loss: 0.00072068
Iteration 12/25 | Loss: 0.00072068
Iteration 13/25 | Loss: 0.00072068
Iteration 14/25 | Loss: 0.00072068
Iteration 15/25 | Loss: 0.00072068
Iteration 16/25 | Loss: 0.00072068
Iteration 17/25 | Loss: 0.00072068
Iteration 18/25 | Loss: 0.00072068
Iteration 19/25 | Loss: 0.00072068
Iteration 20/25 | Loss: 0.00072068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007206809241324663, 0.0007206809241324663, 0.0007206809241324663, 0.0007206809241324663, 0.0007206809241324663]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007206809241324663

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072068
Iteration 2/1000 | Loss: 0.00004247
Iteration 3/1000 | Loss: 0.00002416
Iteration 4/1000 | Loss: 0.00002121
Iteration 5/1000 | Loss: 0.00001944
Iteration 6/1000 | Loss: 0.00001845
Iteration 7/1000 | Loss: 0.00001794
Iteration 8/1000 | Loss: 0.00001752
Iteration 9/1000 | Loss: 0.00001721
Iteration 10/1000 | Loss: 0.00001694
Iteration 11/1000 | Loss: 0.00001668
Iteration 12/1000 | Loss: 0.00001647
Iteration 13/1000 | Loss: 0.00001643
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001626
Iteration 16/1000 | Loss: 0.00001623
Iteration 17/1000 | Loss: 0.00001620
Iteration 18/1000 | Loss: 0.00001620
Iteration 19/1000 | Loss: 0.00001619
Iteration 20/1000 | Loss: 0.00001618
Iteration 21/1000 | Loss: 0.00001618
Iteration 22/1000 | Loss: 0.00001617
Iteration 23/1000 | Loss: 0.00001617
Iteration 24/1000 | Loss: 0.00001617
Iteration 25/1000 | Loss: 0.00001617
Iteration 26/1000 | Loss: 0.00001617
Iteration 27/1000 | Loss: 0.00001617
Iteration 28/1000 | Loss: 0.00001617
Iteration 29/1000 | Loss: 0.00001617
Iteration 30/1000 | Loss: 0.00001617
Iteration 31/1000 | Loss: 0.00001616
Iteration 32/1000 | Loss: 0.00001616
Iteration 33/1000 | Loss: 0.00001616
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001615
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001614
Iteration 38/1000 | Loss: 0.00001613
Iteration 39/1000 | Loss: 0.00001613
Iteration 40/1000 | Loss: 0.00001612
Iteration 41/1000 | Loss: 0.00001612
Iteration 42/1000 | Loss: 0.00001612
Iteration 43/1000 | Loss: 0.00001612
Iteration 44/1000 | Loss: 0.00001612
Iteration 45/1000 | Loss: 0.00001612
Iteration 46/1000 | Loss: 0.00001612
Iteration 47/1000 | Loss: 0.00001611
Iteration 48/1000 | Loss: 0.00001611
Iteration 49/1000 | Loss: 0.00001611
Iteration 50/1000 | Loss: 0.00001611
Iteration 51/1000 | Loss: 0.00001611
Iteration 52/1000 | Loss: 0.00001611
Iteration 53/1000 | Loss: 0.00001610
Iteration 54/1000 | Loss: 0.00001610
Iteration 55/1000 | Loss: 0.00001610
Iteration 56/1000 | Loss: 0.00001610
Iteration 57/1000 | Loss: 0.00001609
Iteration 58/1000 | Loss: 0.00001608
Iteration 59/1000 | Loss: 0.00001608
Iteration 60/1000 | Loss: 0.00001608
Iteration 61/1000 | Loss: 0.00001608
Iteration 62/1000 | Loss: 0.00001608
Iteration 63/1000 | Loss: 0.00001608
Iteration 64/1000 | Loss: 0.00001608
Iteration 65/1000 | Loss: 0.00001608
Iteration 66/1000 | Loss: 0.00001608
Iteration 67/1000 | Loss: 0.00001607
Iteration 68/1000 | Loss: 0.00001607
Iteration 69/1000 | Loss: 0.00001607
Iteration 70/1000 | Loss: 0.00001607
Iteration 71/1000 | Loss: 0.00001607
Iteration 72/1000 | Loss: 0.00001607
Iteration 73/1000 | Loss: 0.00001607
Iteration 74/1000 | Loss: 0.00001607
Iteration 75/1000 | Loss: 0.00001607
Iteration 76/1000 | Loss: 0.00001607
Iteration 77/1000 | Loss: 0.00001607
Iteration 78/1000 | Loss: 0.00001607
Iteration 79/1000 | Loss: 0.00001606
Iteration 80/1000 | Loss: 0.00001606
Iteration 81/1000 | Loss: 0.00001605
Iteration 82/1000 | Loss: 0.00001605
Iteration 83/1000 | Loss: 0.00001604
Iteration 84/1000 | Loss: 0.00001604
Iteration 85/1000 | Loss: 0.00001604
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001602
Iteration 91/1000 | Loss: 0.00001602
Iteration 92/1000 | Loss: 0.00001602
Iteration 93/1000 | Loss: 0.00001602
Iteration 94/1000 | Loss: 0.00001601
Iteration 95/1000 | Loss: 0.00001601
Iteration 96/1000 | Loss: 0.00001601
Iteration 97/1000 | Loss: 0.00001600
Iteration 98/1000 | Loss: 0.00001600
Iteration 99/1000 | Loss: 0.00001600
Iteration 100/1000 | Loss: 0.00001599
Iteration 101/1000 | Loss: 0.00001599
Iteration 102/1000 | Loss: 0.00001598
Iteration 103/1000 | Loss: 0.00001598
Iteration 104/1000 | Loss: 0.00001598
Iteration 105/1000 | Loss: 0.00001598
Iteration 106/1000 | Loss: 0.00001597
Iteration 107/1000 | Loss: 0.00001597
Iteration 108/1000 | Loss: 0.00001597
Iteration 109/1000 | Loss: 0.00001597
Iteration 110/1000 | Loss: 0.00001596
Iteration 111/1000 | Loss: 0.00001596
Iteration 112/1000 | Loss: 0.00001596
Iteration 113/1000 | Loss: 0.00001595
Iteration 114/1000 | Loss: 0.00001595
Iteration 115/1000 | Loss: 0.00001595
Iteration 116/1000 | Loss: 0.00001594
Iteration 117/1000 | Loss: 0.00001594
Iteration 118/1000 | Loss: 0.00001594
Iteration 119/1000 | Loss: 0.00001594
Iteration 120/1000 | Loss: 0.00001594
Iteration 121/1000 | Loss: 0.00001594
Iteration 122/1000 | Loss: 0.00001593
Iteration 123/1000 | Loss: 0.00001593
Iteration 124/1000 | Loss: 0.00001593
Iteration 125/1000 | Loss: 0.00001593
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001592
Iteration 128/1000 | Loss: 0.00001592
Iteration 129/1000 | Loss: 0.00001592
Iteration 130/1000 | Loss: 0.00001591
Iteration 131/1000 | Loss: 0.00001591
Iteration 132/1000 | Loss: 0.00001591
Iteration 133/1000 | Loss: 0.00001591
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Iteration 136/1000 | Loss: 0.00001591
Iteration 137/1000 | Loss: 0.00001590
Iteration 138/1000 | Loss: 0.00001590
Iteration 139/1000 | Loss: 0.00001590
Iteration 140/1000 | Loss: 0.00001590
Iteration 141/1000 | Loss: 0.00001590
Iteration 142/1000 | Loss: 0.00001590
Iteration 143/1000 | Loss: 0.00001589
Iteration 144/1000 | Loss: 0.00001589
Iteration 145/1000 | Loss: 0.00001589
Iteration 146/1000 | Loss: 0.00001589
Iteration 147/1000 | Loss: 0.00001589
Iteration 148/1000 | Loss: 0.00001589
Iteration 149/1000 | Loss: 0.00001589
Iteration 150/1000 | Loss: 0.00001589
Iteration 151/1000 | Loss: 0.00001589
Iteration 152/1000 | Loss: 0.00001588
Iteration 153/1000 | Loss: 0.00001588
Iteration 154/1000 | Loss: 0.00001588
Iteration 155/1000 | Loss: 0.00001588
Iteration 156/1000 | Loss: 0.00001588
Iteration 157/1000 | Loss: 0.00001588
Iteration 158/1000 | Loss: 0.00001588
Iteration 159/1000 | Loss: 0.00001588
Iteration 160/1000 | Loss: 0.00001588
Iteration 161/1000 | Loss: 0.00001588
Iteration 162/1000 | Loss: 0.00001588
Iteration 163/1000 | Loss: 0.00001588
Iteration 164/1000 | Loss: 0.00001587
Iteration 165/1000 | Loss: 0.00001587
Iteration 166/1000 | Loss: 0.00001587
Iteration 167/1000 | Loss: 0.00001587
Iteration 168/1000 | Loss: 0.00001587
Iteration 169/1000 | Loss: 0.00001587
Iteration 170/1000 | Loss: 0.00001587
Iteration 171/1000 | Loss: 0.00001587
Iteration 172/1000 | Loss: 0.00001587
Iteration 173/1000 | Loss: 0.00001587
Iteration 174/1000 | Loss: 0.00001587
Iteration 175/1000 | Loss: 0.00001587
Iteration 176/1000 | Loss: 0.00001587
Iteration 177/1000 | Loss: 0.00001587
Iteration 178/1000 | Loss: 0.00001587
Iteration 179/1000 | Loss: 0.00001586
Iteration 180/1000 | Loss: 0.00001586
Iteration 181/1000 | Loss: 0.00001586
Iteration 182/1000 | Loss: 0.00001586
Iteration 183/1000 | Loss: 0.00001586
Iteration 184/1000 | Loss: 0.00001586
Iteration 185/1000 | Loss: 0.00001585
Iteration 186/1000 | Loss: 0.00001585
Iteration 187/1000 | Loss: 0.00001585
Iteration 188/1000 | Loss: 0.00001585
Iteration 189/1000 | Loss: 0.00001585
Iteration 190/1000 | Loss: 0.00001585
Iteration 191/1000 | Loss: 0.00001585
Iteration 192/1000 | Loss: 0.00001584
Iteration 193/1000 | Loss: 0.00001584
Iteration 194/1000 | Loss: 0.00001584
Iteration 195/1000 | Loss: 0.00001584
Iteration 196/1000 | Loss: 0.00001584
Iteration 197/1000 | Loss: 0.00001584
Iteration 198/1000 | Loss: 0.00001584
Iteration 199/1000 | Loss: 0.00001584
Iteration 200/1000 | Loss: 0.00001584
Iteration 201/1000 | Loss: 0.00001584
Iteration 202/1000 | Loss: 0.00001584
Iteration 203/1000 | Loss: 0.00001584
Iteration 204/1000 | Loss: 0.00001584
Iteration 205/1000 | Loss: 0.00001583
Iteration 206/1000 | Loss: 0.00001583
Iteration 207/1000 | Loss: 0.00001583
Iteration 208/1000 | Loss: 0.00001583
Iteration 209/1000 | Loss: 0.00001583
Iteration 210/1000 | Loss: 0.00001583
Iteration 211/1000 | Loss: 0.00001583
Iteration 212/1000 | Loss: 0.00001583
Iteration 213/1000 | Loss: 0.00001583
Iteration 214/1000 | Loss: 0.00001583
Iteration 215/1000 | Loss: 0.00001583
Iteration 216/1000 | Loss: 0.00001583
Iteration 217/1000 | Loss: 0.00001583
Iteration 218/1000 | Loss: 0.00001582
Iteration 219/1000 | Loss: 0.00001582
Iteration 220/1000 | Loss: 0.00001582
Iteration 221/1000 | Loss: 0.00001582
Iteration 222/1000 | Loss: 0.00001582
Iteration 223/1000 | Loss: 0.00001582
Iteration 224/1000 | Loss: 0.00001582
Iteration 225/1000 | Loss: 0.00001582
Iteration 226/1000 | Loss: 0.00001582
Iteration 227/1000 | Loss: 0.00001582
Iteration 228/1000 | Loss: 0.00001582
Iteration 229/1000 | Loss: 0.00001582
Iteration 230/1000 | Loss: 0.00001582
Iteration 231/1000 | Loss: 0.00001582
Iteration 232/1000 | Loss: 0.00001582
Iteration 233/1000 | Loss: 0.00001582
Iteration 234/1000 | Loss: 0.00001582
Iteration 235/1000 | Loss: 0.00001582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [1.5824154615984298e-05, 1.5824154615984298e-05, 1.5824154615984298e-05, 1.5824154615984298e-05, 1.5824154615984298e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5824154615984298e-05

Optimization complete. Final v2v error: 3.3009932041168213 mm

Highest mean error: 3.999788761138916 mm for frame 183

Lowest mean error: 2.4885215759277344 mm for frame 214

Saving results

Total time: 48.991779804229736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022278
Iteration 2/25 | Loss: 0.01022278
Iteration 3/25 | Loss: 0.01022277
Iteration 4/25 | Loss: 0.00285420
Iteration 5/25 | Loss: 0.00168193
Iteration 6/25 | Loss: 0.00138614
Iteration 7/25 | Loss: 0.00147855
Iteration 8/25 | Loss: 0.00140181
Iteration 9/25 | Loss: 0.00124580
Iteration 10/25 | Loss: 0.00115252
Iteration 11/25 | Loss: 0.00110986
Iteration 12/25 | Loss: 0.00111629
Iteration 13/25 | Loss: 0.00108288
Iteration 14/25 | Loss: 0.00104510
Iteration 15/25 | Loss: 0.00104347
Iteration 16/25 | Loss: 0.00102128
Iteration 17/25 | Loss: 0.00100162
Iteration 18/25 | Loss: 0.00098338
Iteration 19/25 | Loss: 0.00096725
Iteration 20/25 | Loss: 0.00097319
Iteration 21/25 | Loss: 0.00095607
Iteration 22/25 | Loss: 0.00096227
Iteration 23/25 | Loss: 0.00095471
Iteration 24/25 | Loss: 0.00094889
Iteration 25/25 | Loss: 0.00093530

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26132941
Iteration 2/25 | Loss: 0.00180099
Iteration 3/25 | Loss: 0.00180098
Iteration 4/25 | Loss: 0.00180098
Iteration 5/25 | Loss: 0.00180098
Iteration 6/25 | Loss: 0.00180098
Iteration 7/25 | Loss: 0.00180098
Iteration 8/25 | Loss: 0.00177126
Iteration 9/25 | Loss: 0.00177126
Iteration 10/25 | Loss: 0.00177126
Iteration 11/25 | Loss: 0.00177126
Iteration 12/25 | Loss: 0.00177126
Iteration 13/25 | Loss: 0.00177126
Iteration 14/25 | Loss: 0.00177126
Iteration 15/25 | Loss: 0.00177126
Iteration 16/25 | Loss: 0.00177126
Iteration 17/25 | Loss: 0.00177126
Iteration 18/25 | Loss: 0.00177126
Iteration 19/25 | Loss: 0.00177126
Iteration 20/25 | Loss: 0.00177126
Iteration 21/25 | Loss: 0.00177126
Iteration 22/25 | Loss: 0.00177126
Iteration 23/25 | Loss: 0.00177126
Iteration 24/25 | Loss: 0.00177126
Iteration 25/25 | Loss: 0.00177126

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00177126
Iteration 2/1000 | Loss: 0.00132795
Iteration 3/1000 | Loss: 0.00061418
Iteration 4/1000 | Loss: 0.00053063
Iteration 5/1000 | Loss: 0.00049602
Iteration 6/1000 | Loss: 0.00051437
Iteration 7/1000 | Loss: 0.00095373
Iteration 8/1000 | Loss: 0.00051389
Iteration 9/1000 | Loss: 0.00048650
Iteration 10/1000 | Loss: 0.00075578
Iteration 11/1000 | Loss: 0.00108914
Iteration 12/1000 | Loss: 0.00092693
Iteration 13/1000 | Loss: 0.00097440
Iteration 14/1000 | Loss: 0.00097281
Iteration 15/1000 | Loss: 0.00136237
Iteration 16/1000 | Loss: 0.00099321
Iteration 17/1000 | Loss: 0.00061907
Iteration 18/1000 | Loss: 0.00121170
Iteration 19/1000 | Loss: 0.00069089
Iteration 20/1000 | Loss: 0.00061032
Iteration 21/1000 | Loss: 0.00064981
Iteration 22/1000 | Loss: 0.00045842
Iteration 23/1000 | Loss: 0.00040326
Iteration 24/1000 | Loss: 0.00104282
Iteration 25/1000 | Loss: 0.00087120
Iteration 26/1000 | Loss: 0.00063701
Iteration 27/1000 | Loss: 0.00138768
Iteration 28/1000 | Loss: 0.00094996
Iteration 29/1000 | Loss: 0.00126481
Iteration 30/1000 | Loss: 0.00031570
Iteration 31/1000 | Loss: 0.00046914
Iteration 32/1000 | Loss: 0.00048941
Iteration 33/1000 | Loss: 0.00054716
Iteration 34/1000 | Loss: 0.00045864
Iteration 35/1000 | Loss: 0.00047038
Iteration 36/1000 | Loss: 0.00071794
Iteration 37/1000 | Loss: 0.00048804
Iteration 38/1000 | Loss: 0.00046603
Iteration 39/1000 | Loss: 0.00074923
Iteration 40/1000 | Loss: 0.00052612
Iteration 41/1000 | Loss: 0.00028072
Iteration 42/1000 | Loss: 0.00052211
Iteration 43/1000 | Loss: 0.00048925
Iteration 44/1000 | Loss: 0.00052312
Iteration 45/1000 | Loss: 0.00043666
Iteration 46/1000 | Loss: 0.00025288
Iteration 47/1000 | Loss: 0.00045322
Iteration 48/1000 | Loss: 0.00025518
Iteration 49/1000 | Loss: 0.00037936
Iteration 50/1000 | Loss: 0.00022920
Iteration 51/1000 | Loss: 0.00033584
Iteration 52/1000 | Loss: 0.00032050
Iteration 53/1000 | Loss: 0.00024586
Iteration 54/1000 | Loss: 0.00048142
Iteration 55/1000 | Loss: 0.00030795
Iteration 56/1000 | Loss: 0.00039553
Iteration 57/1000 | Loss: 0.00054089
Iteration 58/1000 | Loss: 0.00047012
Iteration 59/1000 | Loss: 0.00031871
Iteration 60/1000 | Loss: 0.00029013
Iteration 61/1000 | Loss: 0.00012778
Iteration 62/1000 | Loss: 0.00029382
Iteration 63/1000 | Loss: 0.00032197
Iteration 64/1000 | Loss: 0.00030866
Iteration 65/1000 | Loss: 0.00021369
Iteration 66/1000 | Loss: 0.00021257
Iteration 67/1000 | Loss: 0.00047650
Iteration 68/1000 | Loss: 0.00031138
Iteration 69/1000 | Loss: 0.00017197
Iteration 70/1000 | Loss: 0.00045760
Iteration 71/1000 | Loss: 0.00038618
Iteration 72/1000 | Loss: 0.00031421
Iteration 73/1000 | Loss: 0.00022816
Iteration 74/1000 | Loss: 0.00022999
Iteration 75/1000 | Loss: 0.00024651
Iteration 76/1000 | Loss: 0.00022019
Iteration 77/1000 | Loss: 0.00013946
Iteration 78/1000 | Loss: 0.00028409
Iteration 79/1000 | Loss: 0.00023553
Iteration 80/1000 | Loss: 0.00029306
Iteration 81/1000 | Loss: 0.00020304
Iteration 82/1000 | Loss: 0.00016212
Iteration 83/1000 | Loss: 0.00033083
Iteration 84/1000 | Loss: 0.00029024
Iteration 85/1000 | Loss: 0.00011320
Iteration 86/1000 | Loss: 0.00032750
Iteration 87/1000 | Loss: 0.00017896
Iteration 88/1000 | Loss: 0.00029506
Iteration 89/1000 | Loss: 0.00028673
Iteration 90/1000 | Loss: 0.00026042
Iteration 91/1000 | Loss: 0.00026542
Iteration 92/1000 | Loss: 0.00027683
Iteration 93/1000 | Loss: 0.00033726
Iteration 94/1000 | Loss: 0.00034025
Iteration 95/1000 | Loss: 0.00039793
Iteration 96/1000 | Loss: 0.00057080
Iteration 97/1000 | Loss: 0.00027881
Iteration 98/1000 | Loss: 0.00049531
Iteration 99/1000 | Loss: 0.00035604
Iteration 100/1000 | Loss: 0.00049575
Iteration 101/1000 | Loss: 0.00048406
Iteration 102/1000 | Loss: 0.00050289
Iteration 103/1000 | Loss: 0.00055573
Iteration 104/1000 | Loss: 0.00054157
Iteration 105/1000 | Loss: 0.00035237
Iteration 106/1000 | Loss: 0.00049800
Iteration 107/1000 | Loss: 0.00043404
Iteration 108/1000 | Loss: 0.00033589
Iteration 109/1000 | Loss: 0.00039498
Iteration 110/1000 | Loss: 0.00038389
Iteration 111/1000 | Loss: 0.00032848
Iteration 112/1000 | Loss: 0.00019175
Iteration 113/1000 | Loss: 0.00009651
Iteration 114/1000 | Loss: 0.00022875
Iteration 115/1000 | Loss: 0.00067641
Iteration 116/1000 | Loss: 0.00047835
Iteration 117/1000 | Loss: 0.00041624
Iteration 118/1000 | Loss: 0.00071732
Iteration 119/1000 | Loss: 0.00028806
Iteration 120/1000 | Loss: 0.00024159
Iteration 121/1000 | Loss: 0.00037206
Iteration 122/1000 | Loss: 0.00038113
Iteration 123/1000 | Loss: 0.00109234
Iteration 124/1000 | Loss: 0.00051907
Iteration 125/1000 | Loss: 0.00012648
Iteration 126/1000 | Loss: 0.00049706
Iteration 127/1000 | Loss: 0.00045380
Iteration 128/1000 | Loss: 0.00039603
Iteration 129/1000 | Loss: 0.00066698
Iteration 130/1000 | Loss: 0.00121870
Iteration 131/1000 | Loss: 0.00101009
Iteration 132/1000 | Loss: 0.00041939
Iteration 133/1000 | Loss: 0.00017914
Iteration 134/1000 | Loss: 0.00048672
Iteration 135/1000 | Loss: 0.00048573
Iteration 136/1000 | Loss: 0.00040127
Iteration 137/1000 | Loss: 0.00070761
Iteration 138/1000 | Loss: 0.00058114
Iteration 139/1000 | Loss: 0.00038205
Iteration 140/1000 | Loss: 0.00025943
Iteration 141/1000 | Loss: 0.00018512
Iteration 142/1000 | Loss: 0.00030715
Iteration 143/1000 | Loss: 0.00094732
Iteration 144/1000 | Loss: 0.00060872
Iteration 145/1000 | Loss: 0.00054236
Iteration 146/1000 | Loss: 0.00022206
Iteration 147/1000 | Loss: 0.00018269
Iteration 148/1000 | Loss: 0.00015953
Iteration 149/1000 | Loss: 0.00030430
Iteration 150/1000 | Loss: 0.00021243
Iteration 151/1000 | Loss: 0.00015512
Iteration 152/1000 | Loss: 0.00022080
Iteration 153/1000 | Loss: 0.00013736
Iteration 154/1000 | Loss: 0.00029217
Iteration 155/1000 | Loss: 0.00058415
Iteration 156/1000 | Loss: 0.00035234
Iteration 157/1000 | Loss: 0.00052250
Iteration 158/1000 | Loss: 0.00026011
Iteration 159/1000 | Loss: 0.00008838
Iteration 160/1000 | Loss: 0.00015378
Iteration 161/1000 | Loss: 0.00024940
Iteration 162/1000 | Loss: 0.00030348
Iteration 163/1000 | Loss: 0.00029182
Iteration 164/1000 | Loss: 0.00034473
Iteration 165/1000 | Loss: 0.00049865
Iteration 166/1000 | Loss: 0.00015439
Iteration 167/1000 | Loss: 0.00020284
Iteration 168/1000 | Loss: 0.00026139
Iteration 169/1000 | Loss: 0.00054431
Iteration 170/1000 | Loss: 0.00067544
Iteration 171/1000 | Loss: 0.00056758
Iteration 172/1000 | Loss: 0.00059392
Iteration 173/1000 | Loss: 0.00023429
Iteration 174/1000 | Loss: 0.00021946
Iteration 175/1000 | Loss: 0.00030332
Iteration 176/1000 | Loss: 0.00022230
Iteration 177/1000 | Loss: 0.00016961
Iteration 178/1000 | Loss: 0.00014900
Iteration 179/1000 | Loss: 0.00007093
Iteration 180/1000 | Loss: 0.00007010
Iteration 181/1000 | Loss: 0.00029438
Iteration 182/1000 | Loss: 0.00020298
Iteration 183/1000 | Loss: 0.00053674
Iteration 184/1000 | Loss: 0.00029523
Iteration 185/1000 | Loss: 0.00032193
Iteration 186/1000 | Loss: 0.00035295
Iteration 187/1000 | Loss: 0.00020601
Iteration 188/1000 | Loss: 0.00046083
Iteration 189/1000 | Loss: 0.00032004
Iteration 190/1000 | Loss: 0.00013902
Iteration 191/1000 | Loss: 0.00021676
Iteration 192/1000 | Loss: 0.00036135
Iteration 193/1000 | Loss: 0.00026219
Iteration 194/1000 | Loss: 0.00048428
Iteration 195/1000 | Loss: 0.00035277
Iteration 196/1000 | Loss: 0.00034653
Iteration 197/1000 | Loss: 0.00044906
Iteration 198/1000 | Loss: 0.00081388
Iteration 199/1000 | Loss: 0.00035478
Iteration 200/1000 | Loss: 0.00030114
Iteration 201/1000 | Loss: 0.00029867
Iteration 202/1000 | Loss: 0.00051073
Iteration 203/1000 | Loss: 0.00036459
Iteration 204/1000 | Loss: 0.00029921
Iteration 205/1000 | Loss: 0.00023217
Iteration 206/1000 | Loss: 0.00025307
Iteration 207/1000 | Loss: 0.00022421
Iteration 208/1000 | Loss: 0.00020788
Iteration 209/1000 | Loss: 0.00009791
Iteration 210/1000 | Loss: 0.00031640
Iteration 211/1000 | Loss: 0.00039013
Iteration 212/1000 | Loss: 0.00035671
Iteration 213/1000 | Loss: 0.00018311
Iteration 214/1000 | Loss: 0.00018336
Iteration 215/1000 | Loss: 0.00016387
Iteration 216/1000 | Loss: 0.00004419
Iteration 217/1000 | Loss: 0.00052306
Iteration 218/1000 | Loss: 0.00057849
Iteration 219/1000 | Loss: 0.00031215
Iteration 220/1000 | Loss: 0.00034261
Iteration 221/1000 | Loss: 0.00010693
Iteration 222/1000 | Loss: 0.00037073
Iteration 223/1000 | Loss: 0.00033604
Iteration 224/1000 | Loss: 0.00027226
Iteration 225/1000 | Loss: 0.00040500
Iteration 226/1000 | Loss: 0.00059617
Iteration 227/1000 | Loss: 0.00049448
Iteration 228/1000 | Loss: 0.00017379
Iteration 229/1000 | Loss: 0.00027687
Iteration 230/1000 | Loss: 0.00043754
Iteration 231/1000 | Loss: 0.00025096
Iteration 232/1000 | Loss: 0.00008329
Iteration 233/1000 | Loss: 0.00025491
Iteration 234/1000 | Loss: 0.00034461
Iteration 235/1000 | Loss: 0.00026718
Iteration 236/1000 | Loss: 0.00015972
Iteration 237/1000 | Loss: 0.00035703
Iteration 238/1000 | Loss: 0.00012552
Iteration 239/1000 | Loss: 0.00014837
Iteration 240/1000 | Loss: 0.00018088
Iteration 241/1000 | Loss: 0.00004980
Iteration 242/1000 | Loss: 0.00021141
Iteration 243/1000 | Loss: 0.00011909
Iteration 244/1000 | Loss: 0.00005403
Iteration 245/1000 | Loss: 0.00008348
Iteration 246/1000 | Loss: 0.00005840
Iteration 247/1000 | Loss: 0.00006230
Iteration 248/1000 | Loss: 0.00010047
Iteration 249/1000 | Loss: 0.00012374
Iteration 250/1000 | Loss: 0.00011344
Iteration 251/1000 | Loss: 0.00021643
Iteration 252/1000 | Loss: 0.00040365
Iteration 253/1000 | Loss: 0.00015511
Iteration 254/1000 | Loss: 0.00005123
Iteration 255/1000 | Loss: 0.00026542
Iteration 256/1000 | Loss: 0.00023223
Iteration 257/1000 | Loss: 0.00007510
Iteration 258/1000 | Loss: 0.00003949
Iteration 259/1000 | Loss: 0.00002784
Iteration 260/1000 | Loss: 0.00028506
Iteration 261/1000 | Loss: 0.00003504
Iteration 262/1000 | Loss: 0.00006202
Iteration 263/1000 | Loss: 0.00003661
Iteration 264/1000 | Loss: 0.00026995
Iteration 265/1000 | Loss: 0.00060473
Iteration 266/1000 | Loss: 0.00039270
Iteration 267/1000 | Loss: 0.00008575
Iteration 268/1000 | Loss: 0.00004046
Iteration 269/1000 | Loss: 0.00020939
Iteration 270/1000 | Loss: 0.00003501
Iteration 271/1000 | Loss: 0.00003648
Iteration 272/1000 | Loss: 0.00003171
Iteration 273/1000 | Loss: 0.00004591
Iteration 274/1000 | Loss: 0.00004253
Iteration 275/1000 | Loss: 0.00004011
Iteration 276/1000 | Loss: 0.00003509
Iteration 277/1000 | Loss: 0.00007403
Iteration 278/1000 | Loss: 0.00004531
Iteration 279/1000 | Loss: 0.00002463
Iteration 280/1000 | Loss: 0.00001856
Iteration 281/1000 | Loss: 0.00002598
Iteration 282/1000 | Loss: 0.00011341
Iteration 283/1000 | Loss: 0.00002216
Iteration 284/1000 | Loss: 0.00001872
Iteration 285/1000 | Loss: 0.00010457
Iteration 286/1000 | Loss: 0.00002421
Iteration 287/1000 | Loss: 0.00001904
Iteration 288/1000 | Loss: 0.00005673
Iteration 289/1000 | Loss: 0.00001879
Iteration 290/1000 | Loss: 0.00001490
Iteration 291/1000 | Loss: 0.00001437
Iteration 292/1000 | Loss: 0.00001373
Iteration 293/1000 | Loss: 0.00001331
Iteration 294/1000 | Loss: 0.00001309
Iteration 295/1000 | Loss: 0.00001308
Iteration 296/1000 | Loss: 0.00001288
Iteration 297/1000 | Loss: 0.00001284
Iteration 298/1000 | Loss: 0.00011452
Iteration 299/1000 | Loss: 0.00003590
Iteration 300/1000 | Loss: 0.00006954
Iteration 301/1000 | Loss: 0.00001632
Iteration 302/1000 | Loss: 0.00012218
Iteration 303/1000 | Loss: 0.00002750
Iteration 304/1000 | Loss: 0.00012108
Iteration 305/1000 | Loss: 0.00010792
Iteration 306/1000 | Loss: 0.00005990
Iteration 307/1000 | Loss: 0.00010669
Iteration 308/1000 | Loss: 0.00005916
Iteration 309/1000 | Loss: 0.00015536
Iteration 310/1000 | Loss: 0.00003775
Iteration 311/1000 | Loss: 0.00013658
Iteration 312/1000 | Loss: 0.00002723
Iteration 313/1000 | Loss: 0.00005624
Iteration 314/1000 | Loss: 0.00005139
Iteration 315/1000 | Loss: 0.00010104
Iteration 316/1000 | Loss: 0.00015501
Iteration 317/1000 | Loss: 0.00002208
Iteration 318/1000 | Loss: 0.00001619
Iteration 319/1000 | Loss: 0.00001428
Iteration 320/1000 | Loss: 0.00001299
Iteration 321/1000 | Loss: 0.00001252
Iteration 322/1000 | Loss: 0.00001221
Iteration 323/1000 | Loss: 0.00001203
Iteration 324/1000 | Loss: 0.00001190
Iteration 325/1000 | Loss: 0.00001182
Iteration 326/1000 | Loss: 0.00001181
Iteration 327/1000 | Loss: 0.00001177
Iteration 328/1000 | Loss: 0.00001176
Iteration 329/1000 | Loss: 0.00001175
Iteration 330/1000 | Loss: 0.00001175
Iteration 331/1000 | Loss: 0.00001173
Iteration 332/1000 | Loss: 0.00001172
Iteration 333/1000 | Loss: 0.00001172
Iteration 334/1000 | Loss: 0.00001172
Iteration 335/1000 | Loss: 0.00001170
Iteration 336/1000 | Loss: 0.00001170
Iteration 337/1000 | Loss: 0.00001170
Iteration 338/1000 | Loss: 0.00001169
Iteration 339/1000 | Loss: 0.00001169
Iteration 340/1000 | Loss: 0.00001169
Iteration 341/1000 | Loss: 0.00001169
Iteration 342/1000 | Loss: 0.00001169
Iteration 343/1000 | Loss: 0.00001169
Iteration 344/1000 | Loss: 0.00001167
Iteration 345/1000 | Loss: 0.00001167
Iteration 346/1000 | Loss: 0.00001167
Iteration 347/1000 | Loss: 0.00001167
Iteration 348/1000 | Loss: 0.00001166
Iteration 349/1000 | Loss: 0.00001166
Iteration 350/1000 | Loss: 0.00001166
Iteration 351/1000 | Loss: 0.00001166
Iteration 352/1000 | Loss: 0.00001166
Iteration 353/1000 | Loss: 0.00001166
Iteration 354/1000 | Loss: 0.00001166
Iteration 355/1000 | Loss: 0.00001166
Iteration 356/1000 | Loss: 0.00001166
Iteration 357/1000 | Loss: 0.00001166
Iteration 358/1000 | Loss: 0.00001166
Iteration 359/1000 | Loss: 0.00001164
Iteration 360/1000 | Loss: 0.00001164
Iteration 361/1000 | Loss: 0.00001163
Iteration 362/1000 | Loss: 0.00001163
Iteration 363/1000 | Loss: 0.00001163
Iteration 364/1000 | Loss: 0.00001163
Iteration 365/1000 | Loss: 0.00001162
Iteration 366/1000 | Loss: 0.00001162
Iteration 367/1000 | Loss: 0.00001162
Iteration 368/1000 | Loss: 0.00001162
Iteration 369/1000 | Loss: 0.00001162
Iteration 370/1000 | Loss: 0.00001162
Iteration 371/1000 | Loss: 0.00001162
Iteration 372/1000 | Loss: 0.00001162
Iteration 373/1000 | Loss: 0.00001162
Iteration 374/1000 | Loss: 0.00001162
Iteration 375/1000 | Loss: 0.00001162
Iteration 376/1000 | Loss: 0.00001162
Iteration 377/1000 | Loss: 0.00001162
Iteration 378/1000 | Loss: 0.00001162
Iteration 379/1000 | Loss: 0.00001162
Iteration 380/1000 | Loss: 0.00001162
Iteration 381/1000 | Loss: 0.00001161
Iteration 382/1000 | Loss: 0.00001161
Iteration 383/1000 | Loss: 0.00001161
Iteration 384/1000 | Loss: 0.00001161
Iteration 385/1000 | Loss: 0.00001161
Iteration 386/1000 | Loss: 0.00001161
Iteration 387/1000 | Loss: 0.00001161
Iteration 388/1000 | Loss: 0.00001161
Iteration 389/1000 | Loss: 0.00001160
Iteration 390/1000 | Loss: 0.00001160
Iteration 391/1000 | Loss: 0.00001160
Iteration 392/1000 | Loss: 0.00001160
Iteration 393/1000 | Loss: 0.00001160
Iteration 394/1000 | Loss: 0.00001160
Iteration 395/1000 | Loss: 0.00001160
Iteration 396/1000 | Loss: 0.00001160
Iteration 397/1000 | Loss: 0.00001160
Iteration 398/1000 | Loss: 0.00001160
Iteration 399/1000 | Loss: 0.00001160
Iteration 400/1000 | Loss: 0.00001160
Iteration 401/1000 | Loss: 0.00001160
Iteration 402/1000 | Loss: 0.00001160
Iteration 403/1000 | Loss: 0.00001160
Iteration 404/1000 | Loss: 0.00001160
Iteration 405/1000 | Loss: 0.00001160
Iteration 406/1000 | Loss: 0.00001160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 406. Stopping optimization.
Last 5 losses: [1.1599279787333217e-05, 1.1599279787333217e-05, 1.1599279787333217e-05, 1.1599279787333217e-05, 1.1599279787333217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1599279787333217e-05

Optimization complete. Final v2v error: 2.6319854259490967 mm

Highest mean error: 9.409486770629883 mm for frame 186

Lowest mean error: 2.2154622077941895 mm for frame 32

Saving results

Total time: 576.1878542900085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2386/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2386/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00428589
Iteration 2/25 | Loss: 0.00103611
Iteration 3/25 | Loss: 0.00091107
Iteration 4/25 | Loss: 0.00088541
Iteration 5/25 | Loss: 0.00087914
Iteration 6/25 | Loss: 0.00087747
Iteration 7/25 | Loss: 0.00087726
Iteration 8/25 | Loss: 0.00087726
Iteration 9/25 | Loss: 0.00087726
Iteration 10/25 | Loss: 0.00087726
Iteration 11/25 | Loss: 0.00087726
Iteration 12/25 | Loss: 0.00087726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008772605215199292, 0.0008772605215199292, 0.0008772605215199292, 0.0008772605215199292, 0.0008772605215199292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008772605215199292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86565983
Iteration 2/25 | Loss: 0.00079649
Iteration 3/25 | Loss: 0.00079649
Iteration 4/25 | Loss: 0.00079649
Iteration 5/25 | Loss: 0.00079649
Iteration 6/25 | Loss: 0.00079649
Iteration 7/25 | Loss: 0.00079649
Iteration 8/25 | Loss: 0.00079649
Iteration 9/25 | Loss: 0.00079648
Iteration 10/25 | Loss: 0.00079648
Iteration 11/25 | Loss: 0.00079648
Iteration 12/25 | Loss: 0.00079648
Iteration 13/25 | Loss: 0.00079648
Iteration 14/25 | Loss: 0.00079648
Iteration 15/25 | Loss: 0.00079648
Iteration 16/25 | Loss: 0.00079648
Iteration 17/25 | Loss: 0.00079648
Iteration 18/25 | Loss: 0.00079648
Iteration 19/25 | Loss: 0.00079648
Iteration 20/25 | Loss: 0.00079648
Iteration 21/25 | Loss: 0.00079648
Iteration 22/25 | Loss: 0.00079648
Iteration 23/25 | Loss: 0.00079648
Iteration 24/25 | Loss: 0.00079648
Iteration 25/25 | Loss: 0.00079648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079648
Iteration 2/1000 | Loss: 0.00003673
Iteration 3/1000 | Loss: 0.00002383
Iteration 4/1000 | Loss: 0.00002142
Iteration 5/1000 | Loss: 0.00001974
Iteration 6/1000 | Loss: 0.00001886
Iteration 7/1000 | Loss: 0.00001801
Iteration 8/1000 | Loss: 0.00001763
Iteration 9/1000 | Loss: 0.00001736
Iteration 10/1000 | Loss: 0.00001716
Iteration 11/1000 | Loss: 0.00001702
Iteration 12/1000 | Loss: 0.00001701
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001698
Iteration 15/1000 | Loss: 0.00001697
Iteration 16/1000 | Loss: 0.00001696
Iteration 17/1000 | Loss: 0.00001695
Iteration 18/1000 | Loss: 0.00001695
Iteration 19/1000 | Loss: 0.00001693
Iteration 20/1000 | Loss: 0.00001693
Iteration 21/1000 | Loss: 0.00001692
Iteration 22/1000 | Loss: 0.00001691
Iteration 23/1000 | Loss: 0.00001691
Iteration 24/1000 | Loss: 0.00001690
Iteration 25/1000 | Loss: 0.00001690
Iteration 26/1000 | Loss: 0.00001690
Iteration 27/1000 | Loss: 0.00001688
Iteration 28/1000 | Loss: 0.00001688
Iteration 29/1000 | Loss: 0.00001687
Iteration 30/1000 | Loss: 0.00001687
Iteration 31/1000 | Loss: 0.00001687
Iteration 32/1000 | Loss: 0.00001687
Iteration 33/1000 | Loss: 0.00001687
Iteration 34/1000 | Loss: 0.00001686
Iteration 35/1000 | Loss: 0.00001686
Iteration 36/1000 | Loss: 0.00001686
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001685
Iteration 39/1000 | Loss: 0.00001685
Iteration 40/1000 | Loss: 0.00001684
Iteration 41/1000 | Loss: 0.00001683
Iteration 42/1000 | Loss: 0.00001683
Iteration 43/1000 | Loss: 0.00001683
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001682
Iteration 48/1000 | Loss: 0.00001681
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001680
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001679
Iteration 56/1000 | Loss: 0.00001679
Iteration 57/1000 | Loss: 0.00001679
Iteration 58/1000 | Loss: 0.00001679
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001679
Iteration 61/1000 | Loss: 0.00001679
Iteration 62/1000 | Loss: 0.00001679
Iteration 63/1000 | Loss: 0.00001679
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001678
Iteration 66/1000 | Loss: 0.00001678
Iteration 67/1000 | Loss: 0.00001678
Iteration 68/1000 | Loss: 0.00001678
Iteration 69/1000 | Loss: 0.00001678
Iteration 70/1000 | Loss: 0.00001678
Iteration 71/1000 | Loss: 0.00001678
Iteration 72/1000 | Loss: 0.00001678
Iteration 73/1000 | Loss: 0.00001677
Iteration 74/1000 | Loss: 0.00001677
Iteration 75/1000 | Loss: 0.00001677
Iteration 76/1000 | Loss: 0.00001677
Iteration 77/1000 | Loss: 0.00001677
Iteration 78/1000 | Loss: 0.00001677
Iteration 79/1000 | Loss: 0.00001677
Iteration 80/1000 | Loss: 0.00001677
Iteration 81/1000 | Loss: 0.00001676
Iteration 82/1000 | Loss: 0.00001676
Iteration 83/1000 | Loss: 0.00001676
Iteration 84/1000 | Loss: 0.00001676
Iteration 85/1000 | Loss: 0.00001676
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001675
Iteration 88/1000 | Loss: 0.00001675
Iteration 89/1000 | Loss: 0.00001675
Iteration 90/1000 | Loss: 0.00001675
Iteration 91/1000 | Loss: 0.00001675
Iteration 92/1000 | Loss: 0.00001674
Iteration 93/1000 | Loss: 0.00001674
Iteration 94/1000 | Loss: 0.00001674
Iteration 95/1000 | Loss: 0.00001674
Iteration 96/1000 | Loss: 0.00001674
Iteration 97/1000 | Loss: 0.00001674
Iteration 98/1000 | Loss: 0.00001674
Iteration 99/1000 | Loss: 0.00001674
Iteration 100/1000 | Loss: 0.00001674
Iteration 101/1000 | Loss: 0.00001673
Iteration 102/1000 | Loss: 0.00001673
Iteration 103/1000 | Loss: 0.00001673
Iteration 104/1000 | Loss: 0.00001673
Iteration 105/1000 | Loss: 0.00001673
Iteration 106/1000 | Loss: 0.00001673
Iteration 107/1000 | Loss: 0.00001673
Iteration 108/1000 | Loss: 0.00001673
Iteration 109/1000 | Loss: 0.00001673
Iteration 110/1000 | Loss: 0.00001673
Iteration 111/1000 | Loss: 0.00001673
Iteration 112/1000 | Loss: 0.00001673
Iteration 113/1000 | Loss: 0.00001673
Iteration 114/1000 | Loss: 0.00001673
Iteration 115/1000 | Loss: 0.00001673
Iteration 116/1000 | Loss: 0.00001673
Iteration 117/1000 | Loss: 0.00001673
Iteration 118/1000 | Loss: 0.00001673
Iteration 119/1000 | Loss: 0.00001673
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001673
Iteration 122/1000 | Loss: 0.00001673
Iteration 123/1000 | Loss: 0.00001673
Iteration 124/1000 | Loss: 0.00001673
Iteration 125/1000 | Loss: 0.00001673
Iteration 126/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.672675716690719e-05, 1.672675716690719e-05, 1.672675716690719e-05, 1.672675716690719e-05, 1.672675716690719e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.672675716690719e-05

Optimization complete. Final v2v error: 3.3588526248931885 mm

Highest mean error: 3.828756332397461 mm for frame 138

Lowest mean error: 2.956165075302124 mm for frame 22

Saving results

Total time: 33.489941358566284
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737508
Iteration 2/25 | Loss: 0.00151046
Iteration 3/25 | Loss: 0.00093434
Iteration 4/25 | Loss: 0.00085583
Iteration 5/25 | Loss: 0.00083482
Iteration 6/25 | Loss: 0.00082957
Iteration 7/25 | Loss: 0.00082568
Iteration 8/25 | Loss: 0.00081741
Iteration 9/25 | Loss: 0.00080886
Iteration 10/25 | Loss: 0.00080234
Iteration 11/25 | Loss: 0.00080095
Iteration 12/25 | Loss: 0.00080031
Iteration 13/25 | Loss: 0.00079958
Iteration 14/25 | Loss: 0.00079986
Iteration 15/25 | Loss: 0.00080033
Iteration 16/25 | Loss: 0.00079939
Iteration 17/25 | Loss: 0.00079838
Iteration 18/25 | Loss: 0.00079833
Iteration 19/25 | Loss: 0.00079813
Iteration 20/25 | Loss: 0.00079812
Iteration 21/25 | Loss: 0.00079810
Iteration 22/25 | Loss: 0.00079810
Iteration 23/25 | Loss: 0.00079810
Iteration 24/25 | Loss: 0.00079810
Iteration 25/25 | Loss: 0.00079810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48261976
Iteration 2/25 | Loss: 0.00077715
Iteration 3/25 | Loss: 0.00077712
Iteration 4/25 | Loss: 0.00077712
Iteration 5/25 | Loss: 0.00077712
Iteration 6/25 | Loss: 0.00077712
Iteration 7/25 | Loss: 0.00077712
Iteration 8/25 | Loss: 0.00077712
Iteration 9/25 | Loss: 0.00077712
Iteration 10/25 | Loss: 0.00077712
Iteration 11/25 | Loss: 0.00077711
Iteration 12/25 | Loss: 0.00077711
Iteration 13/25 | Loss: 0.00077711
Iteration 14/25 | Loss: 0.00077711
Iteration 15/25 | Loss: 0.00077711
Iteration 16/25 | Loss: 0.00077711
Iteration 17/25 | Loss: 0.00077711
Iteration 18/25 | Loss: 0.00077711
Iteration 19/25 | Loss: 0.00077711
Iteration 20/25 | Loss: 0.00077711
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007771149976179004, 0.0007771149976179004, 0.0007771149976179004, 0.0007771149976179004, 0.0007771149976179004]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007771149976179004

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077711
Iteration 2/1000 | Loss: 0.00003111
Iteration 3/1000 | Loss: 0.00002560
Iteration 4/1000 | Loss: 0.00002180
Iteration 5/1000 | Loss: 0.00002251
Iteration 6/1000 | Loss: 0.00002057
Iteration 7/1000 | Loss: 0.00001961
Iteration 8/1000 | Loss: 0.00002105
Iteration 9/1000 | Loss: 0.00001888
Iteration 10/1000 | Loss: 0.00001866
Iteration 11/1000 | Loss: 0.00001838
Iteration 12/1000 | Loss: 0.00001824
Iteration 13/1000 | Loss: 0.00001820
Iteration 14/1000 | Loss: 0.00001805
Iteration 15/1000 | Loss: 0.00001809
Iteration 16/1000 | Loss: 0.00001797
Iteration 17/1000 | Loss: 0.00001797
Iteration 18/1000 | Loss: 0.00001796
Iteration 19/1000 | Loss: 0.00001796
Iteration 20/1000 | Loss: 0.00001796
Iteration 21/1000 | Loss: 0.00001794
Iteration 22/1000 | Loss: 0.00001791
Iteration 23/1000 | Loss: 0.00001790
Iteration 24/1000 | Loss: 0.00001790
Iteration 25/1000 | Loss: 0.00001790
Iteration 26/1000 | Loss: 0.00001789
Iteration 27/1000 | Loss: 0.00001789
Iteration 28/1000 | Loss: 0.00001789
Iteration 29/1000 | Loss: 0.00001789
Iteration 30/1000 | Loss: 0.00001789
Iteration 31/1000 | Loss: 0.00001789
Iteration 32/1000 | Loss: 0.00001789
Iteration 33/1000 | Loss: 0.00001789
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001788
Iteration 36/1000 | Loss: 0.00001788
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001788
Iteration 39/1000 | Loss: 0.00001787
Iteration 40/1000 | Loss: 0.00001787
Iteration 41/1000 | Loss: 0.00001787
Iteration 42/1000 | Loss: 0.00001787
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001787
Iteration 45/1000 | Loss: 0.00001787
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001787
Iteration 49/1000 | Loss: 0.00001787
Iteration 50/1000 | Loss: 0.00001787
Iteration 51/1000 | Loss: 0.00001786
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001785
Iteration 54/1000 | Loss: 0.00001834
Iteration 55/1000 | Loss: 0.00001783
Iteration 56/1000 | Loss: 0.00001782
Iteration 57/1000 | Loss: 0.00001782
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001781
Iteration 60/1000 | Loss: 0.00001781
Iteration 61/1000 | Loss: 0.00001781
Iteration 62/1000 | Loss: 0.00001781
Iteration 63/1000 | Loss: 0.00001780
Iteration 64/1000 | Loss: 0.00001780
Iteration 65/1000 | Loss: 0.00001780
Iteration 66/1000 | Loss: 0.00001780
Iteration 67/1000 | Loss: 0.00001780
Iteration 68/1000 | Loss: 0.00001780
Iteration 69/1000 | Loss: 0.00001780
Iteration 70/1000 | Loss: 0.00001780
Iteration 71/1000 | Loss: 0.00001780
Iteration 72/1000 | Loss: 0.00001780
Iteration 73/1000 | Loss: 0.00001780
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [1.7804381059249863e-05, 1.7804381059249863e-05, 1.7804381059249863e-05, 1.7804381059249863e-05, 1.7804381059249863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7804381059249863e-05

Optimization complete. Final v2v error: 3.4646809101104736 mm

Highest mean error: 4.013392448425293 mm for frame 122

Lowest mean error: 2.9008524417877197 mm for frame 16

Saving results

Total time: 65.996258020401
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949001
Iteration 2/25 | Loss: 0.00243538
Iteration 3/25 | Loss: 0.00161476
Iteration 4/25 | Loss: 0.00145974
Iteration 5/25 | Loss: 0.00123773
Iteration 6/25 | Loss: 0.00110557
Iteration 7/25 | Loss: 0.00106037
Iteration 8/25 | Loss: 0.00102729
Iteration 9/25 | Loss: 0.00100851
Iteration 10/25 | Loss: 0.00099487
Iteration 11/25 | Loss: 0.00098566
Iteration 12/25 | Loss: 0.00099557
Iteration 13/25 | Loss: 0.00097482
Iteration 14/25 | Loss: 0.00096156
Iteration 15/25 | Loss: 0.00097155
Iteration 16/25 | Loss: 0.00092061
Iteration 17/25 | Loss: 0.00088710
Iteration 18/25 | Loss: 0.00088662
Iteration 19/25 | Loss: 0.00084728
Iteration 20/25 | Loss: 0.00083999
Iteration 21/25 | Loss: 0.00084127
Iteration 22/25 | Loss: 0.00083685
Iteration 23/25 | Loss: 0.00083613
Iteration 24/25 | Loss: 0.00083598
Iteration 25/25 | Loss: 0.00083595

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50718248
Iteration 2/25 | Loss: 0.00077864
Iteration 3/25 | Loss: 0.00077863
Iteration 4/25 | Loss: 0.00077863
Iteration 5/25 | Loss: 0.00077863
Iteration 6/25 | Loss: 0.00077863
Iteration 7/25 | Loss: 0.00077863
Iteration 8/25 | Loss: 0.00077863
Iteration 9/25 | Loss: 0.00077863
Iteration 10/25 | Loss: 0.00077863
Iteration 11/25 | Loss: 0.00077863
Iteration 12/25 | Loss: 0.00077863
Iteration 13/25 | Loss: 0.00077863
Iteration 14/25 | Loss: 0.00077863
Iteration 15/25 | Loss: 0.00077863
Iteration 16/25 | Loss: 0.00077863
Iteration 17/25 | Loss: 0.00077863
Iteration 18/25 | Loss: 0.00077863
Iteration 19/25 | Loss: 0.00077863
Iteration 20/25 | Loss: 0.00077863
Iteration 21/25 | Loss: 0.00077863
Iteration 22/25 | Loss: 0.00077863
Iteration 23/25 | Loss: 0.00077863
Iteration 24/25 | Loss: 0.00077863
Iteration 25/25 | Loss: 0.00077863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077863
Iteration 2/1000 | Loss: 0.00004972
Iteration 3/1000 | Loss: 0.00003153
Iteration 4/1000 | Loss: 0.00002708
Iteration 5/1000 | Loss: 0.00002522
Iteration 6/1000 | Loss: 0.00002400
Iteration 7/1000 | Loss: 0.00002336
Iteration 8/1000 | Loss: 0.00015016
Iteration 9/1000 | Loss: 0.00039837
Iteration 10/1000 | Loss: 0.00002256
Iteration 11/1000 | Loss: 0.00002228
Iteration 12/1000 | Loss: 0.00002228
Iteration 13/1000 | Loss: 0.00002227
Iteration 14/1000 | Loss: 0.00002227
Iteration 15/1000 | Loss: 0.00011143
Iteration 16/1000 | Loss: 0.00002223
Iteration 17/1000 | Loss: 0.00002210
Iteration 18/1000 | Loss: 0.00002191
Iteration 19/1000 | Loss: 0.00002188
Iteration 20/1000 | Loss: 0.00002188
Iteration 21/1000 | Loss: 0.00002187
Iteration 22/1000 | Loss: 0.00002182
Iteration 23/1000 | Loss: 0.00002182
Iteration 24/1000 | Loss: 0.00002178
Iteration 25/1000 | Loss: 0.00002178
Iteration 26/1000 | Loss: 0.00002177
Iteration 27/1000 | Loss: 0.00002177
Iteration 28/1000 | Loss: 0.00002177
Iteration 29/1000 | Loss: 0.00002177
Iteration 30/1000 | Loss: 0.00002176
Iteration 31/1000 | Loss: 0.00002176
Iteration 32/1000 | Loss: 0.00002176
Iteration 33/1000 | Loss: 0.00002175
Iteration 34/1000 | Loss: 0.00002173
Iteration 35/1000 | Loss: 0.00002172
Iteration 36/1000 | Loss: 0.00002172
Iteration 37/1000 | Loss: 0.00002171
Iteration 38/1000 | Loss: 0.00002171
Iteration 39/1000 | Loss: 0.00002170
Iteration 40/1000 | Loss: 0.00002167
Iteration 41/1000 | Loss: 0.00002167
Iteration 42/1000 | Loss: 0.00002167
Iteration 43/1000 | Loss: 0.00002166
Iteration 44/1000 | Loss: 0.00002166
Iteration 45/1000 | Loss: 0.00002166
Iteration 46/1000 | Loss: 0.00002166
Iteration 47/1000 | Loss: 0.00002166
Iteration 48/1000 | Loss: 0.00002165
Iteration 49/1000 | Loss: 0.00002163
Iteration 50/1000 | Loss: 0.00002162
Iteration 51/1000 | Loss: 0.00002162
Iteration 52/1000 | Loss: 0.00002161
Iteration 53/1000 | Loss: 0.00002161
Iteration 54/1000 | Loss: 0.00002161
Iteration 55/1000 | Loss: 0.00002160
Iteration 56/1000 | Loss: 0.00002160
Iteration 57/1000 | Loss: 0.00002159
Iteration 58/1000 | Loss: 0.00002159
Iteration 59/1000 | Loss: 0.00002159
Iteration 60/1000 | Loss: 0.00002159
Iteration 61/1000 | Loss: 0.00002158
Iteration 62/1000 | Loss: 0.00002158
Iteration 63/1000 | Loss: 0.00002157
Iteration 64/1000 | Loss: 0.00002157
Iteration 65/1000 | Loss: 0.00002156
Iteration 66/1000 | Loss: 0.00002156
Iteration 67/1000 | Loss: 0.00002156
Iteration 68/1000 | Loss: 0.00002155
Iteration 69/1000 | Loss: 0.00002155
Iteration 70/1000 | Loss: 0.00002155
Iteration 71/1000 | Loss: 0.00002154
Iteration 72/1000 | Loss: 0.00002154
Iteration 73/1000 | Loss: 0.00002154
Iteration 74/1000 | Loss: 0.00002154
Iteration 75/1000 | Loss: 0.00002154
Iteration 76/1000 | Loss: 0.00002154
Iteration 77/1000 | Loss: 0.00002154
Iteration 78/1000 | Loss: 0.00002154
Iteration 79/1000 | Loss: 0.00002153
Iteration 80/1000 | Loss: 0.00002153
Iteration 81/1000 | Loss: 0.00002153
Iteration 82/1000 | Loss: 0.00002153
Iteration 83/1000 | Loss: 0.00002153
Iteration 84/1000 | Loss: 0.00002153
Iteration 85/1000 | Loss: 0.00002153
Iteration 86/1000 | Loss: 0.00002153
Iteration 87/1000 | Loss: 0.00002152
Iteration 88/1000 | Loss: 0.00002152
Iteration 89/1000 | Loss: 0.00002152
Iteration 90/1000 | Loss: 0.00002152
Iteration 91/1000 | Loss: 0.00002151
Iteration 92/1000 | Loss: 0.00002151
Iteration 93/1000 | Loss: 0.00002150
Iteration 94/1000 | Loss: 0.00002150
Iteration 95/1000 | Loss: 0.00002150
Iteration 96/1000 | Loss: 0.00002150
Iteration 97/1000 | Loss: 0.00002149
Iteration 98/1000 | Loss: 0.00002149
Iteration 99/1000 | Loss: 0.00002149
Iteration 100/1000 | Loss: 0.00002149
Iteration 101/1000 | Loss: 0.00002148
Iteration 102/1000 | Loss: 0.00002148
Iteration 103/1000 | Loss: 0.00002148
Iteration 104/1000 | Loss: 0.00002148
Iteration 105/1000 | Loss: 0.00002148
Iteration 106/1000 | Loss: 0.00002148
Iteration 107/1000 | Loss: 0.00002148
Iteration 108/1000 | Loss: 0.00002148
Iteration 109/1000 | Loss: 0.00002148
Iteration 110/1000 | Loss: 0.00002148
Iteration 111/1000 | Loss: 0.00002147
Iteration 112/1000 | Loss: 0.00002147
Iteration 113/1000 | Loss: 0.00002147
Iteration 114/1000 | Loss: 0.00002147
Iteration 115/1000 | Loss: 0.00002147
Iteration 116/1000 | Loss: 0.00002147
Iteration 117/1000 | Loss: 0.00002147
Iteration 118/1000 | Loss: 0.00002147
Iteration 119/1000 | Loss: 0.00002147
Iteration 120/1000 | Loss: 0.00002147
Iteration 121/1000 | Loss: 0.00002147
Iteration 122/1000 | Loss: 0.00002147
Iteration 123/1000 | Loss: 0.00002146
Iteration 124/1000 | Loss: 0.00002146
Iteration 125/1000 | Loss: 0.00002146
Iteration 126/1000 | Loss: 0.00002146
Iteration 127/1000 | Loss: 0.00002146
Iteration 128/1000 | Loss: 0.00002146
Iteration 129/1000 | Loss: 0.00002146
Iteration 130/1000 | Loss: 0.00002146
Iteration 131/1000 | Loss: 0.00002146
Iteration 132/1000 | Loss: 0.00002145
Iteration 133/1000 | Loss: 0.00002145
Iteration 134/1000 | Loss: 0.00002145
Iteration 135/1000 | Loss: 0.00002145
Iteration 136/1000 | Loss: 0.00002145
Iteration 137/1000 | Loss: 0.00002145
Iteration 138/1000 | Loss: 0.00002145
Iteration 139/1000 | Loss: 0.00002145
Iteration 140/1000 | Loss: 0.00002145
Iteration 141/1000 | Loss: 0.00002145
Iteration 142/1000 | Loss: 0.00002145
Iteration 143/1000 | Loss: 0.00002145
Iteration 144/1000 | Loss: 0.00002145
Iteration 145/1000 | Loss: 0.00002145
Iteration 146/1000 | Loss: 0.00002145
Iteration 147/1000 | Loss: 0.00002145
Iteration 148/1000 | Loss: 0.00002145
Iteration 149/1000 | Loss: 0.00002145
Iteration 150/1000 | Loss: 0.00002145
Iteration 151/1000 | Loss: 0.00002145
Iteration 152/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [2.1453193767229095e-05, 2.1453193767229095e-05, 2.1453193767229095e-05, 2.1453193767229095e-05, 2.1453193767229095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1453193767229095e-05

Optimization complete. Final v2v error: 3.9460654258728027 mm

Highest mean error: 4.310091972351074 mm for frame 148

Lowest mean error: 3.7603673934936523 mm for frame 74

Saving results

Total time: 86.14415836334229
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971876
Iteration 2/25 | Loss: 0.00388628
Iteration 3/25 | Loss: 0.00298265
Iteration 4/25 | Loss: 0.00199774
Iteration 5/25 | Loss: 0.00180392
Iteration 6/25 | Loss: 0.00183157
Iteration 7/25 | Loss: 0.00172147
Iteration 8/25 | Loss: 0.00177322
Iteration 9/25 | Loss: 0.00165046
Iteration 10/25 | Loss: 0.00155005
Iteration 11/25 | Loss: 0.00184832
Iteration 12/25 | Loss: 0.00133216
Iteration 13/25 | Loss: 0.00103676
Iteration 14/25 | Loss: 0.00100746
Iteration 15/25 | Loss: 0.00089242
Iteration 16/25 | Loss: 0.00086511
Iteration 17/25 | Loss: 0.00085556
Iteration 18/25 | Loss: 0.00085476
Iteration 19/25 | Loss: 0.00085458
Iteration 20/25 | Loss: 0.00085594
Iteration 21/25 | Loss: 0.00084831
Iteration 22/25 | Loss: 0.00084669
Iteration 23/25 | Loss: 0.00084657
Iteration 24/25 | Loss: 0.00084656
Iteration 25/25 | Loss: 0.00084656

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53193223
Iteration 2/25 | Loss: 0.00134999
Iteration 3/25 | Loss: 0.00083399
Iteration 4/25 | Loss: 0.00083399
Iteration 5/25 | Loss: 0.00083399
Iteration 6/25 | Loss: 0.00083399
Iteration 7/25 | Loss: 0.00083399
Iteration 8/25 | Loss: 0.00083399
Iteration 9/25 | Loss: 0.00083399
Iteration 10/25 | Loss: 0.00083399
Iteration 11/25 | Loss: 0.00083399
Iteration 12/25 | Loss: 0.00083399
Iteration 13/25 | Loss: 0.00083399
Iteration 14/25 | Loss: 0.00083399
Iteration 15/25 | Loss: 0.00083399
Iteration 16/25 | Loss: 0.00083399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008339929627254605, 0.0008339929627254605, 0.0008339929627254605, 0.0008339929627254605, 0.0008339929627254605]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008339929627254605

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083399
Iteration 2/1000 | Loss: 0.00004463
Iteration 3/1000 | Loss: 0.00003311
Iteration 4/1000 | Loss: 0.00093937
Iteration 5/1000 | Loss: 0.00002810
Iteration 6/1000 | Loss: 0.00002681
Iteration 7/1000 | Loss: 0.00002514
Iteration 8/1000 | Loss: 0.00002445
Iteration 9/1000 | Loss: 0.00002374
Iteration 10/1000 | Loss: 0.00073752
Iteration 11/1000 | Loss: 0.00100482
Iteration 12/1000 | Loss: 0.00059381
Iteration 13/1000 | Loss: 0.00005065
Iteration 14/1000 | Loss: 0.00003301
Iteration 15/1000 | Loss: 0.00002656
Iteration 16/1000 | Loss: 0.00058697
Iteration 17/1000 | Loss: 0.00042141
Iteration 18/1000 | Loss: 0.00025516
Iteration 19/1000 | Loss: 0.00005226
Iteration 20/1000 | Loss: 0.00003543
Iteration 21/1000 | Loss: 0.00002341
Iteration 22/1000 | Loss: 0.00002183
Iteration 23/1000 | Loss: 0.00002102
Iteration 24/1000 | Loss: 0.00002060
Iteration 25/1000 | Loss: 0.00038558
Iteration 26/1000 | Loss: 0.00149864
Iteration 27/1000 | Loss: 0.00207954
Iteration 28/1000 | Loss: 0.00151727
Iteration 29/1000 | Loss: 0.00169421
Iteration 30/1000 | Loss: 0.00008487
Iteration 31/1000 | Loss: 0.00005070
Iteration 32/1000 | Loss: 0.00002112
Iteration 33/1000 | Loss: 0.00009850
Iteration 34/1000 | Loss: 0.00018525
Iteration 35/1000 | Loss: 0.00017671
Iteration 36/1000 | Loss: 0.00007448
Iteration 37/1000 | Loss: 0.00002089
Iteration 38/1000 | Loss: 0.00034630
Iteration 39/1000 | Loss: 0.00022111
Iteration 40/1000 | Loss: 0.00002236
Iteration 41/1000 | Loss: 0.00031326
Iteration 42/1000 | Loss: 0.00016406
Iteration 43/1000 | Loss: 0.00025951
Iteration 44/1000 | Loss: 0.00007649
Iteration 45/1000 | Loss: 0.00050580
Iteration 46/1000 | Loss: 0.00024583
Iteration 47/1000 | Loss: 0.00013471
Iteration 48/1000 | Loss: 0.00017921
Iteration 49/1000 | Loss: 0.00021942
Iteration 50/1000 | Loss: 0.00019007
Iteration 51/1000 | Loss: 0.00023762
Iteration 52/1000 | Loss: 0.00034609
Iteration 53/1000 | Loss: 0.00002276
Iteration 54/1000 | Loss: 0.00001979
Iteration 55/1000 | Loss: 0.00001840
Iteration 56/1000 | Loss: 0.00001751
Iteration 57/1000 | Loss: 0.00001710
Iteration 58/1000 | Loss: 0.00001699
Iteration 59/1000 | Loss: 0.00001696
Iteration 60/1000 | Loss: 0.00001689
Iteration 61/1000 | Loss: 0.00001688
Iteration 62/1000 | Loss: 0.00001687
Iteration 63/1000 | Loss: 0.00001687
Iteration 64/1000 | Loss: 0.00001687
Iteration 65/1000 | Loss: 0.00001686
Iteration 66/1000 | Loss: 0.00001686
Iteration 67/1000 | Loss: 0.00001686
Iteration 68/1000 | Loss: 0.00001685
Iteration 69/1000 | Loss: 0.00001685
Iteration 70/1000 | Loss: 0.00001685
Iteration 71/1000 | Loss: 0.00001685
Iteration 72/1000 | Loss: 0.00001684
Iteration 73/1000 | Loss: 0.00001684
Iteration 74/1000 | Loss: 0.00001684
Iteration 75/1000 | Loss: 0.00001683
Iteration 76/1000 | Loss: 0.00001683
Iteration 77/1000 | Loss: 0.00001683
Iteration 78/1000 | Loss: 0.00001683
Iteration 79/1000 | Loss: 0.00001683
Iteration 80/1000 | Loss: 0.00001683
Iteration 81/1000 | Loss: 0.00001683
Iteration 82/1000 | Loss: 0.00001683
Iteration 83/1000 | Loss: 0.00001682
Iteration 84/1000 | Loss: 0.00001682
Iteration 85/1000 | Loss: 0.00001682
Iteration 86/1000 | Loss: 0.00001682
Iteration 87/1000 | Loss: 0.00001682
Iteration 88/1000 | Loss: 0.00001682
Iteration 89/1000 | Loss: 0.00001682
Iteration 90/1000 | Loss: 0.00001682
Iteration 91/1000 | Loss: 0.00001682
Iteration 92/1000 | Loss: 0.00001682
Iteration 93/1000 | Loss: 0.00001682
Iteration 94/1000 | Loss: 0.00001682
Iteration 95/1000 | Loss: 0.00001682
Iteration 96/1000 | Loss: 0.00001682
Iteration 97/1000 | Loss: 0.00001682
Iteration 98/1000 | Loss: 0.00001682
Iteration 99/1000 | Loss: 0.00001682
Iteration 100/1000 | Loss: 0.00001682
Iteration 101/1000 | Loss: 0.00001682
Iteration 102/1000 | Loss: 0.00001682
Iteration 103/1000 | Loss: 0.00001682
Iteration 104/1000 | Loss: 0.00001682
Iteration 105/1000 | Loss: 0.00001682
Iteration 106/1000 | Loss: 0.00001682
Iteration 107/1000 | Loss: 0.00001682
Iteration 108/1000 | Loss: 0.00001682
Iteration 109/1000 | Loss: 0.00001682
Iteration 110/1000 | Loss: 0.00001682
Iteration 111/1000 | Loss: 0.00001682
Iteration 112/1000 | Loss: 0.00001682
Iteration 113/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.6817702999105677e-05, 1.6817702999105677e-05, 1.6817702999105677e-05, 1.6817702999105677e-05, 1.6817702999105677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6817702999105677e-05

Optimization complete. Final v2v error: 3.4758386611938477 mm

Highest mean error: 4.4528632164001465 mm for frame 0

Lowest mean error: 3.436108112335205 mm for frame 149

Saving results

Total time: 130.18605780601501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013813
Iteration 2/25 | Loss: 0.01013813
Iteration 3/25 | Loss: 0.00222666
Iteration 4/25 | Loss: 0.00148250
Iteration 5/25 | Loss: 0.00127780
Iteration 6/25 | Loss: 0.00114570
Iteration 7/25 | Loss: 0.00107226
Iteration 8/25 | Loss: 0.00112804
Iteration 9/25 | Loss: 0.00100578
Iteration 10/25 | Loss: 0.00091150
Iteration 11/25 | Loss: 0.00089211
Iteration 12/25 | Loss: 0.00089429
Iteration 13/25 | Loss: 0.00083324
Iteration 14/25 | Loss: 0.00080546
Iteration 15/25 | Loss: 0.00079299
Iteration 16/25 | Loss: 0.00078702
Iteration 17/25 | Loss: 0.00076949
Iteration 18/25 | Loss: 0.00075604
Iteration 19/25 | Loss: 0.00076007
Iteration 20/25 | Loss: 0.00074674
Iteration 21/25 | Loss: 0.00074844
Iteration 22/25 | Loss: 0.00074543
Iteration 23/25 | Loss: 0.00074644
Iteration 24/25 | Loss: 0.00074133
Iteration 25/25 | Loss: 0.00074236

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60028541
Iteration 2/25 | Loss: 0.00142961
Iteration 3/25 | Loss: 0.00106063
Iteration 4/25 | Loss: 0.00105444
Iteration 5/25 | Loss: 0.00105444
Iteration 6/25 | Loss: 0.00105444
Iteration 7/25 | Loss: 0.00105444
Iteration 8/25 | Loss: 0.00105444
Iteration 9/25 | Loss: 0.00105444
Iteration 10/25 | Loss: 0.00105444
Iteration 11/25 | Loss: 0.00105444
Iteration 12/25 | Loss: 0.00105444
Iteration 13/25 | Loss: 0.00105444
Iteration 14/25 | Loss: 0.00105444
Iteration 15/25 | Loss: 0.00105444
Iteration 16/25 | Loss: 0.00105444
Iteration 17/25 | Loss: 0.00105444
Iteration 18/25 | Loss: 0.00105444
Iteration 19/25 | Loss: 0.00105444
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010544364340603352, 0.0010544364340603352, 0.0010544364340603352, 0.0010544364340603352, 0.0010544364340603352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010544364340603352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105444
Iteration 2/1000 | Loss: 0.00019632
Iteration 3/1000 | Loss: 0.00087604
Iteration 4/1000 | Loss: 0.00069301
Iteration 5/1000 | Loss: 0.00012611
Iteration 6/1000 | Loss: 0.00001666
Iteration 7/1000 | Loss: 0.00007013
Iteration 8/1000 | Loss: 0.00003623
Iteration 9/1000 | Loss: 0.00005377
Iteration 10/1000 | Loss: 0.00010795
Iteration 11/1000 | Loss: 0.00002042
Iteration 12/1000 | Loss: 0.00001302
Iteration 13/1000 | Loss: 0.00008355
Iteration 14/1000 | Loss: 0.00001369
Iteration 15/1000 | Loss: 0.00001236
Iteration 16/1000 | Loss: 0.00001206
Iteration 17/1000 | Loss: 0.00001178
Iteration 18/1000 | Loss: 0.00001172
Iteration 19/1000 | Loss: 0.00002516
Iteration 20/1000 | Loss: 0.00001369
Iteration 21/1000 | Loss: 0.00001164
Iteration 22/1000 | Loss: 0.00001163
Iteration 23/1000 | Loss: 0.00001163
Iteration 24/1000 | Loss: 0.00001160
Iteration 25/1000 | Loss: 0.00006780
Iteration 26/1000 | Loss: 0.00002806
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00004378
Iteration 29/1000 | Loss: 0.00004227
Iteration 30/1000 | Loss: 0.00002034
Iteration 31/1000 | Loss: 0.00009451
Iteration 32/1000 | Loss: 0.00001144
Iteration 33/1000 | Loss: 0.00001135
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001127
Iteration 38/1000 | Loss: 0.00001127
Iteration 39/1000 | Loss: 0.00001127
Iteration 40/1000 | Loss: 0.00001127
Iteration 41/1000 | Loss: 0.00001127
Iteration 42/1000 | Loss: 0.00001127
Iteration 43/1000 | Loss: 0.00001127
Iteration 44/1000 | Loss: 0.00001127
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001127
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001126
Iteration 49/1000 | Loss: 0.00001126
Iteration 50/1000 | Loss: 0.00001126
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001126
Iteration 54/1000 | Loss: 0.00001126
Iteration 55/1000 | Loss: 0.00001126
Iteration 56/1000 | Loss: 0.00001126
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001125
Iteration 59/1000 | Loss: 0.00001173
Iteration 60/1000 | Loss: 0.00001118
Iteration 61/1000 | Loss: 0.00001117
Iteration 62/1000 | Loss: 0.00001117
Iteration 63/1000 | Loss: 0.00001117
Iteration 64/1000 | Loss: 0.00001117
Iteration 65/1000 | Loss: 0.00001117
Iteration 66/1000 | Loss: 0.00001117
Iteration 67/1000 | Loss: 0.00001117
Iteration 68/1000 | Loss: 0.00001117
Iteration 69/1000 | Loss: 0.00001117
Iteration 70/1000 | Loss: 0.00001117
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001117
Iteration 73/1000 | Loss: 0.00001117
Iteration 74/1000 | Loss: 0.00001116
Iteration 75/1000 | Loss: 0.00003537
Iteration 76/1000 | Loss: 0.00001328
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001258
Iteration 79/1000 | Loss: 0.00001268
Iteration 80/1000 | Loss: 0.00001226
Iteration 81/1000 | Loss: 0.00001114
Iteration 82/1000 | Loss: 0.00001114
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001113
Iteration 88/1000 | Loss: 0.00001113
Iteration 89/1000 | Loss: 0.00001113
Iteration 90/1000 | Loss: 0.00001113
Iteration 91/1000 | Loss: 0.00001113
Iteration 92/1000 | Loss: 0.00001113
Iteration 93/1000 | Loss: 0.00001113
Iteration 94/1000 | Loss: 0.00001112
Iteration 95/1000 | Loss: 0.00001112
Iteration 96/1000 | Loss: 0.00001112
Iteration 97/1000 | Loss: 0.00001112
Iteration 98/1000 | Loss: 0.00001112
Iteration 99/1000 | Loss: 0.00001112
Iteration 100/1000 | Loss: 0.00001112
Iteration 101/1000 | Loss: 0.00001112
Iteration 102/1000 | Loss: 0.00001112
Iteration 103/1000 | Loss: 0.00001112
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001112
Iteration 106/1000 | Loss: 0.00001112
Iteration 107/1000 | Loss: 0.00001111
Iteration 108/1000 | Loss: 0.00001111
Iteration 109/1000 | Loss: 0.00001111
Iteration 110/1000 | Loss: 0.00001111
Iteration 111/1000 | Loss: 0.00001111
Iteration 112/1000 | Loss: 0.00001111
Iteration 113/1000 | Loss: 0.00001111
Iteration 114/1000 | Loss: 0.00001111
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001111
Iteration 117/1000 | Loss: 0.00001111
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001110
Iteration 120/1000 | Loss: 0.00001110
Iteration 121/1000 | Loss: 0.00001109
Iteration 122/1000 | Loss: 0.00001109
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001264
Iteration 125/1000 | Loss: 0.00001110
Iteration 126/1000 | Loss: 0.00001110
Iteration 127/1000 | Loss: 0.00001110
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001110
Iteration 130/1000 | Loss: 0.00001110
Iteration 131/1000 | Loss: 0.00001110
Iteration 132/1000 | Loss: 0.00001110
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00002160
Iteration 135/1000 | Loss: 0.00001155
Iteration 136/1000 | Loss: 0.00001117
Iteration 137/1000 | Loss: 0.00001117
Iteration 138/1000 | Loss: 0.00001116
Iteration 139/1000 | Loss: 0.00001116
Iteration 140/1000 | Loss: 0.00001116
Iteration 141/1000 | Loss: 0.00001116
Iteration 142/1000 | Loss: 0.00001115
Iteration 143/1000 | Loss: 0.00001124
Iteration 144/1000 | Loss: 0.00001112
Iteration 145/1000 | Loss: 0.00001112
Iteration 146/1000 | Loss: 0.00001112
Iteration 147/1000 | Loss: 0.00001112
Iteration 148/1000 | Loss: 0.00001250
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001106
Iteration 152/1000 | Loss: 0.00001106
Iteration 153/1000 | Loss: 0.00001106
Iteration 154/1000 | Loss: 0.00001106
Iteration 155/1000 | Loss: 0.00001105
Iteration 156/1000 | Loss: 0.00001105
Iteration 157/1000 | Loss: 0.00001105
Iteration 158/1000 | Loss: 0.00001105
Iteration 159/1000 | Loss: 0.00001105
Iteration 160/1000 | Loss: 0.00001105
Iteration 161/1000 | Loss: 0.00001105
Iteration 162/1000 | Loss: 0.00001105
Iteration 163/1000 | Loss: 0.00001105
Iteration 164/1000 | Loss: 0.00001105
Iteration 165/1000 | Loss: 0.00001105
Iteration 166/1000 | Loss: 0.00001104
Iteration 167/1000 | Loss: 0.00001104
Iteration 168/1000 | Loss: 0.00001104
Iteration 169/1000 | Loss: 0.00001104
Iteration 170/1000 | Loss: 0.00001104
Iteration 171/1000 | Loss: 0.00001104
Iteration 172/1000 | Loss: 0.00001104
Iteration 173/1000 | Loss: 0.00001104
Iteration 174/1000 | Loss: 0.00001104
Iteration 175/1000 | Loss: 0.00001104
Iteration 176/1000 | Loss: 0.00001104
Iteration 177/1000 | Loss: 0.00001103
Iteration 178/1000 | Loss: 0.00001103
Iteration 179/1000 | Loss: 0.00001103
Iteration 180/1000 | Loss: 0.00001102
Iteration 181/1000 | Loss: 0.00001102
Iteration 182/1000 | Loss: 0.00001102
Iteration 183/1000 | Loss: 0.00001102
Iteration 184/1000 | Loss: 0.00001102
Iteration 185/1000 | Loss: 0.00001102
Iteration 186/1000 | Loss: 0.00001101
Iteration 187/1000 | Loss: 0.00001101
Iteration 188/1000 | Loss: 0.00001101
Iteration 189/1000 | Loss: 0.00001101
Iteration 190/1000 | Loss: 0.00001101
Iteration 191/1000 | Loss: 0.00001101
Iteration 192/1000 | Loss: 0.00001101
Iteration 193/1000 | Loss: 0.00001101
Iteration 194/1000 | Loss: 0.00001101
Iteration 195/1000 | Loss: 0.00001101
Iteration 196/1000 | Loss: 0.00001101
Iteration 197/1000 | Loss: 0.00001101
Iteration 198/1000 | Loss: 0.00001101
Iteration 199/1000 | Loss: 0.00001100
Iteration 200/1000 | Loss: 0.00001100
Iteration 201/1000 | Loss: 0.00001100
Iteration 202/1000 | Loss: 0.00001100
Iteration 203/1000 | Loss: 0.00001100
Iteration 204/1000 | Loss: 0.00001100
Iteration 205/1000 | Loss: 0.00001100
Iteration 206/1000 | Loss: 0.00001100
Iteration 207/1000 | Loss: 0.00001100
Iteration 208/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.100384906749241e-05, 1.100384906749241e-05, 1.100384906749241e-05, 1.100384906749241e-05, 1.100384906749241e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.100384906749241e-05

Optimization complete. Final v2v error: 2.8139610290527344 mm

Highest mean error: 3.3322839736938477 mm for frame 226

Lowest mean error: 2.3259222507476807 mm for frame 67

Saving results

Total time: 119.75419640541077
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769029
Iteration 2/25 | Loss: 0.00157272
Iteration 3/25 | Loss: 0.00099604
Iteration 4/25 | Loss: 0.00089372
Iteration 5/25 | Loss: 0.00086180
Iteration 6/25 | Loss: 0.00084379
Iteration 7/25 | Loss: 0.00083177
Iteration 8/25 | Loss: 0.00082765
Iteration 9/25 | Loss: 0.00082611
Iteration 10/25 | Loss: 0.00082579
Iteration 11/25 | Loss: 0.00082566
Iteration 12/25 | Loss: 0.00082562
Iteration 13/25 | Loss: 0.00082562
Iteration 14/25 | Loss: 0.00082562
Iteration 15/25 | Loss: 0.00082562
Iteration 16/25 | Loss: 0.00082562
Iteration 17/25 | Loss: 0.00082562
Iteration 18/25 | Loss: 0.00082562
Iteration 19/25 | Loss: 0.00082562
Iteration 20/25 | Loss: 0.00082562
Iteration 21/25 | Loss: 0.00082562
Iteration 22/25 | Loss: 0.00082562
Iteration 23/25 | Loss: 0.00082561
Iteration 24/25 | Loss: 0.00082561
Iteration 25/25 | Loss: 0.00082561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.24237323
Iteration 2/25 | Loss: 0.00089999
Iteration 3/25 | Loss: 0.00089996
Iteration 4/25 | Loss: 0.00089996
Iteration 5/25 | Loss: 0.00089996
Iteration 6/25 | Loss: 0.00089996
Iteration 7/25 | Loss: 0.00089995
Iteration 8/25 | Loss: 0.00089995
Iteration 9/25 | Loss: 0.00089995
Iteration 10/25 | Loss: 0.00089995
Iteration 11/25 | Loss: 0.00089995
Iteration 12/25 | Loss: 0.00089995
Iteration 13/25 | Loss: 0.00089995
Iteration 14/25 | Loss: 0.00089995
Iteration 15/25 | Loss: 0.00089995
Iteration 16/25 | Loss: 0.00089995
Iteration 17/25 | Loss: 0.00089995
Iteration 18/25 | Loss: 0.00089995
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008999538258649409, 0.0008999538258649409, 0.0008999538258649409, 0.0008999538258649409, 0.0008999538258649409]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008999538258649409

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089995
Iteration 2/1000 | Loss: 0.00003199
Iteration 3/1000 | Loss: 0.00002289
Iteration 4/1000 | Loss: 0.00002066
Iteration 5/1000 | Loss: 0.00001951
Iteration 6/1000 | Loss: 0.00001890
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00001840
Iteration 9/1000 | Loss: 0.00001831
Iteration 10/1000 | Loss: 0.00001822
Iteration 11/1000 | Loss: 0.00001819
Iteration 12/1000 | Loss: 0.00001818
Iteration 13/1000 | Loss: 0.00001817
Iteration 14/1000 | Loss: 0.00001816
Iteration 15/1000 | Loss: 0.00001815
Iteration 16/1000 | Loss: 0.00001815
Iteration 17/1000 | Loss: 0.00001814
Iteration 18/1000 | Loss: 0.00001814
Iteration 19/1000 | Loss: 0.00001813
Iteration 20/1000 | Loss: 0.00001813
Iteration 21/1000 | Loss: 0.00001810
Iteration 22/1000 | Loss: 0.00001808
Iteration 23/1000 | Loss: 0.00001801
Iteration 24/1000 | Loss: 0.00001800
Iteration 25/1000 | Loss: 0.00001799
Iteration 26/1000 | Loss: 0.00001798
Iteration 27/1000 | Loss: 0.00001797
Iteration 28/1000 | Loss: 0.00001794
Iteration 29/1000 | Loss: 0.00001789
Iteration 30/1000 | Loss: 0.00001786
Iteration 31/1000 | Loss: 0.00001786
Iteration 32/1000 | Loss: 0.00001785
Iteration 33/1000 | Loss: 0.00001785
Iteration 34/1000 | Loss: 0.00001784
Iteration 35/1000 | Loss: 0.00001783
Iteration 36/1000 | Loss: 0.00001783
Iteration 37/1000 | Loss: 0.00001782
Iteration 38/1000 | Loss: 0.00001782
Iteration 39/1000 | Loss: 0.00001782
Iteration 40/1000 | Loss: 0.00001781
Iteration 41/1000 | Loss: 0.00001781
Iteration 42/1000 | Loss: 0.00001781
Iteration 43/1000 | Loss: 0.00001780
Iteration 44/1000 | Loss: 0.00001780
Iteration 45/1000 | Loss: 0.00001779
Iteration 46/1000 | Loss: 0.00001779
Iteration 47/1000 | Loss: 0.00001779
Iteration 48/1000 | Loss: 0.00001778
Iteration 49/1000 | Loss: 0.00001778
Iteration 50/1000 | Loss: 0.00001778
Iteration 51/1000 | Loss: 0.00001778
Iteration 52/1000 | Loss: 0.00001777
Iteration 53/1000 | Loss: 0.00001777
Iteration 54/1000 | Loss: 0.00001777
Iteration 55/1000 | Loss: 0.00001777
Iteration 56/1000 | Loss: 0.00001777
Iteration 57/1000 | Loss: 0.00001776
Iteration 58/1000 | Loss: 0.00001775
Iteration 59/1000 | Loss: 0.00001774
Iteration 60/1000 | Loss: 0.00001774
Iteration 61/1000 | Loss: 0.00001774
Iteration 62/1000 | Loss: 0.00001773
Iteration 63/1000 | Loss: 0.00001773
Iteration 64/1000 | Loss: 0.00001772
Iteration 65/1000 | Loss: 0.00001772
Iteration 66/1000 | Loss: 0.00001771
Iteration 67/1000 | Loss: 0.00001771
Iteration 68/1000 | Loss: 0.00001771
Iteration 69/1000 | Loss: 0.00001771
Iteration 70/1000 | Loss: 0.00001771
Iteration 71/1000 | Loss: 0.00001770
Iteration 72/1000 | Loss: 0.00001770
Iteration 73/1000 | Loss: 0.00001770
Iteration 74/1000 | Loss: 0.00001770
Iteration 75/1000 | Loss: 0.00001770
Iteration 76/1000 | Loss: 0.00001769
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001769
Iteration 79/1000 | Loss: 0.00001768
Iteration 80/1000 | Loss: 0.00001768
Iteration 81/1000 | Loss: 0.00001768
Iteration 82/1000 | Loss: 0.00001768
Iteration 83/1000 | Loss: 0.00001768
Iteration 84/1000 | Loss: 0.00001767
Iteration 85/1000 | Loss: 0.00001767
Iteration 86/1000 | Loss: 0.00001767
Iteration 87/1000 | Loss: 0.00001767
Iteration 88/1000 | Loss: 0.00001767
Iteration 89/1000 | Loss: 0.00001767
Iteration 90/1000 | Loss: 0.00001767
Iteration 91/1000 | Loss: 0.00001767
Iteration 92/1000 | Loss: 0.00001767
Iteration 93/1000 | Loss: 0.00001767
Iteration 94/1000 | Loss: 0.00001767
Iteration 95/1000 | Loss: 0.00001766
Iteration 96/1000 | Loss: 0.00001766
Iteration 97/1000 | Loss: 0.00001766
Iteration 98/1000 | Loss: 0.00001766
Iteration 99/1000 | Loss: 0.00001766
Iteration 100/1000 | Loss: 0.00001766
Iteration 101/1000 | Loss: 0.00001766
Iteration 102/1000 | Loss: 0.00001766
Iteration 103/1000 | Loss: 0.00001765
Iteration 104/1000 | Loss: 0.00001765
Iteration 105/1000 | Loss: 0.00001765
Iteration 106/1000 | Loss: 0.00001765
Iteration 107/1000 | Loss: 0.00001765
Iteration 108/1000 | Loss: 0.00001765
Iteration 109/1000 | Loss: 0.00001765
Iteration 110/1000 | Loss: 0.00001765
Iteration 111/1000 | Loss: 0.00001765
Iteration 112/1000 | Loss: 0.00001764
Iteration 113/1000 | Loss: 0.00001764
Iteration 114/1000 | Loss: 0.00001764
Iteration 115/1000 | Loss: 0.00001764
Iteration 116/1000 | Loss: 0.00001764
Iteration 117/1000 | Loss: 0.00001764
Iteration 118/1000 | Loss: 0.00001764
Iteration 119/1000 | Loss: 0.00001764
Iteration 120/1000 | Loss: 0.00001763
Iteration 121/1000 | Loss: 0.00001763
Iteration 122/1000 | Loss: 0.00001763
Iteration 123/1000 | Loss: 0.00001763
Iteration 124/1000 | Loss: 0.00001763
Iteration 125/1000 | Loss: 0.00001763
Iteration 126/1000 | Loss: 0.00001763
Iteration 127/1000 | Loss: 0.00001763
Iteration 128/1000 | Loss: 0.00001763
Iteration 129/1000 | Loss: 0.00001763
Iteration 130/1000 | Loss: 0.00001762
Iteration 131/1000 | Loss: 0.00001762
Iteration 132/1000 | Loss: 0.00001762
Iteration 133/1000 | Loss: 0.00001762
Iteration 134/1000 | Loss: 0.00001762
Iteration 135/1000 | Loss: 0.00001762
Iteration 136/1000 | Loss: 0.00001762
Iteration 137/1000 | Loss: 0.00001761
Iteration 138/1000 | Loss: 0.00001761
Iteration 139/1000 | Loss: 0.00001761
Iteration 140/1000 | Loss: 0.00001761
Iteration 141/1000 | Loss: 0.00001761
Iteration 142/1000 | Loss: 0.00001761
Iteration 143/1000 | Loss: 0.00001761
Iteration 144/1000 | Loss: 0.00001761
Iteration 145/1000 | Loss: 0.00001761
Iteration 146/1000 | Loss: 0.00001761
Iteration 147/1000 | Loss: 0.00001760
Iteration 148/1000 | Loss: 0.00001760
Iteration 149/1000 | Loss: 0.00001760
Iteration 150/1000 | Loss: 0.00001760
Iteration 151/1000 | Loss: 0.00001760
Iteration 152/1000 | Loss: 0.00001760
Iteration 153/1000 | Loss: 0.00001760
Iteration 154/1000 | Loss: 0.00001760
Iteration 155/1000 | Loss: 0.00001760
Iteration 156/1000 | Loss: 0.00001760
Iteration 157/1000 | Loss: 0.00001760
Iteration 158/1000 | Loss: 0.00001760
Iteration 159/1000 | Loss: 0.00001760
Iteration 160/1000 | Loss: 0.00001759
Iteration 161/1000 | Loss: 0.00001759
Iteration 162/1000 | Loss: 0.00001759
Iteration 163/1000 | Loss: 0.00001759
Iteration 164/1000 | Loss: 0.00001759
Iteration 165/1000 | Loss: 0.00001759
Iteration 166/1000 | Loss: 0.00001759
Iteration 167/1000 | Loss: 0.00001759
Iteration 168/1000 | Loss: 0.00001759
Iteration 169/1000 | Loss: 0.00001759
Iteration 170/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.7593592929188162e-05, 1.7593592929188162e-05, 1.7593592929188162e-05, 1.7593592929188162e-05, 1.7593592929188162e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7593592929188162e-05

Optimization complete. Final v2v error: 3.539242744445801 mm

Highest mean error: 3.9970502853393555 mm for frame 112

Lowest mean error: 3.2920665740966797 mm for frame 28

Saving results

Total time: 53.28933882713318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758869
Iteration 2/25 | Loss: 0.00097188
Iteration 3/25 | Loss: 0.00078304
Iteration 4/25 | Loss: 0.00074717
Iteration 5/25 | Loss: 0.00074234
Iteration 6/25 | Loss: 0.00074168
Iteration 7/25 | Loss: 0.00074168
Iteration 8/25 | Loss: 0.00074168
Iteration 9/25 | Loss: 0.00074168
Iteration 10/25 | Loss: 0.00074168
Iteration 11/25 | Loss: 0.00074168
Iteration 12/25 | Loss: 0.00074168
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007416770095005631, 0.0007416770095005631, 0.0007416770095005631, 0.0007416770095005631, 0.0007416770095005631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007416770095005631

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55888855
Iteration 2/25 | Loss: 0.00064306
Iteration 3/25 | Loss: 0.00064299
Iteration 4/25 | Loss: 0.00064299
Iteration 5/25 | Loss: 0.00064299
Iteration 6/25 | Loss: 0.00064299
Iteration 7/25 | Loss: 0.00064299
Iteration 8/25 | Loss: 0.00064299
Iteration 9/25 | Loss: 0.00064299
Iteration 10/25 | Loss: 0.00064299
Iteration 11/25 | Loss: 0.00064299
Iteration 12/25 | Loss: 0.00064299
Iteration 13/25 | Loss: 0.00064299
Iteration 14/25 | Loss: 0.00064299
Iteration 15/25 | Loss: 0.00064299
Iteration 16/25 | Loss: 0.00064299
Iteration 17/25 | Loss: 0.00064299
Iteration 18/25 | Loss: 0.00064299
Iteration 19/25 | Loss: 0.00064299
Iteration 20/25 | Loss: 0.00064299
Iteration 21/25 | Loss: 0.00064299
Iteration 22/25 | Loss: 0.00064299
Iteration 23/25 | Loss: 0.00064299
Iteration 24/25 | Loss: 0.00064299
Iteration 25/25 | Loss: 0.00064299

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064299
Iteration 2/1000 | Loss: 0.00003039
Iteration 3/1000 | Loss: 0.00002097
Iteration 4/1000 | Loss: 0.00001728
Iteration 5/1000 | Loss: 0.00001581
Iteration 6/1000 | Loss: 0.00001488
Iteration 7/1000 | Loss: 0.00001435
Iteration 8/1000 | Loss: 0.00001398
Iteration 9/1000 | Loss: 0.00001369
Iteration 10/1000 | Loss: 0.00001346
Iteration 11/1000 | Loss: 0.00001341
Iteration 12/1000 | Loss: 0.00001331
Iteration 13/1000 | Loss: 0.00001330
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001329
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00001326
Iteration 18/1000 | Loss: 0.00001316
Iteration 19/1000 | Loss: 0.00001308
Iteration 20/1000 | Loss: 0.00001305
Iteration 21/1000 | Loss: 0.00001304
Iteration 22/1000 | Loss: 0.00001302
Iteration 23/1000 | Loss: 0.00001302
Iteration 24/1000 | Loss: 0.00001302
Iteration 25/1000 | Loss: 0.00001301
Iteration 26/1000 | Loss: 0.00001301
Iteration 27/1000 | Loss: 0.00001301
Iteration 28/1000 | Loss: 0.00001300
Iteration 29/1000 | Loss: 0.00001300
Iteration 30/1000 | Loss: 0.00001299
Iteration 31/1000 | Loss: 0.00001297
Iteration 32/1000 | Loss: 0.00001297
Iteration 33/1000 | Loss: 0.00001297
Iteration 34/1000 | Loss: 0.00001297
Iteration 35/1000 | Loss: 0.00001297
Iteration 36/1000 | Loss: 0.00001297
Iteration 37/1000 | Loss: 0.00001296
Iteration 38/1000 | Loss: 0.00001296
Iteration 39/1000 | Loss: 0.00001296
Iteration 40/1000 | Loss: 0.00001295
Iteration 41/1000 | Loss: 0.00001295
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001294
Iteration 44/1000 | Loss: 0.00001294
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001292
Iteration 47/1000 | Loss: 0.00001292
Iteration 48/1000 | Loss: 0.00001291
Iteration 49/1000 | Loss: 0.00001291
Iteration 50/1000 | Loss: 0.00001290
Iteration 51/1000 | Loss: 0.00001290
Iteration 52/1000 | Loss: 0.00001290
Iteration 53/1000 | Loss: 0.00001290
Iteration 54/1000 | Loss: 0.00001290
Iteration 55/1000 | Loss: 0.00001289
Iteration 56/1000 | Loss: 0.00001289
Iteration 57/1000 | Loss: 0.00001289
Iteration 58/1000 | Loss: 0.00001289
Iteration 59/1000 | Loss: 0.00001289
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001288
Iteration 62/1000 | Loss: 0.00001288
Iteration 63/1000 | Loss: 0.00001288
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001287
Iteration 66/1000 | Loss: 0.00001287
Iteration 67/1000 | Loss: 0.00001287
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001287
Iteration 72/1000 | Loss: 0.00001287
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001286
Iteration 75/1000 | Loss: 0.00001286
Iteration 76/1000 | Loss: 0.00001286
Iteration 77/1000 | Loss: 0.00001286
Iteration 78/1000 | Loss: 0.00001286
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001285
Iteration 82/1000 | Loss: 0.00001285
Iteration 83/1000 | Loss: 0.00001284
Iteration 84/1000 | Loss: 0.00001284
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001283
Iteration 87/1000 | Loss: 0.00001283
Iteration 88/1000 | Loss: 0.00001283
Iteration 89/1000 | Loss: 0.00001283
Iteration 90/1000 | Loss: 0.00001282
Iteration 91/1000 | Loss: 0.00001282
Iteration 92/1000 | Loss: 0.00001282
Iteration 93/1000 | Loss: 0.00001282
Iteration 94/1000 | Loss: 0.00001282
Iteration 95/1000 | Loss: 0.00001282
Iteration 96/1000 | Loss: 0.00001281
Iteration 97/1000 | Loss: 0.00001281
Iteration 98/1000 | Loss: 0.00001281
Iteration 99/1000 | Loss: 0.00001281
Iteration 100/1000 | Loss: 0.00001280
Iteration 101/1000 | Loss: 0.00001280
Iteration 102/1000 | Loss: 0.00001280
Iteration 103/1000 | Loss: 0.00001280
Iteration 104/1000 | Loss: 0.00001280
Iteration 105/1000 | Loss: 0.00001280
Iteration 106/1000 | Loss: 0.00001280
Iteration 107/1000 | Loss: 0.00001280
Iteration 108/1000 | Loss: 0.00001280
Iteration 109/1000 | Loss: 0.00001279
Iteration 110/1000 | Loss: 0.00001279
Iteration 111/1000 | Loss: 0.00001279
Iteration 112/1000 | Loss: 0.00001279
Iteration 113/1000 | Loss: 0.00001279
Iteration 114/1000 | Loss: 0.00001279
Iteration 115/1000 | Loss: 0.00001279
Iteration 116/1000 | Loss: 0.00001279
Iteration 117/1000 | Loss: 0.00001279
Iteration 118/1000 | Loss: 0.00001279
Iteration 119/1000 | Loss: 0.00001279
Iteration 120/1000 | Loss: 0.00001279
Iteration 121/1000 | Loss: 0.00001279
Iteration 122/1000 | Loss: 0.00001279
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001279
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001279
Iteration 129/1000 | Loss: 0.00001279
Iteration 130/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.2792367670044769e-05, 1.2792367670044769e-05, 1.2792367670044769e-05, 1.2792367670044769e-05, 1.2792367670044769e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2792367670044769e-05

Optimization complete. Final v2v error: 3.077727794647217 mm

Highest mean error: 3.3016257286071777 mm for frame 72

Lowest mean error: 2.8403244018554688 mm for frame 154

Saving results

Total time: 42.29776477813721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01174538
Iteration 2/25 | Loss: 0.00222069
Iteration 3/25 | Loss: 0.00135508
Iteration 4/25 | Loss: 0.00113472
Iteration 5/25 | Loss: 0.00107596
Iteration 6/25 | Loss: 0.00103956
Iteration 7/25 | Loss: 0.00101961
Iteration 8/25 | Loss: 0.00100542
Iteration 9/25 | Loss: 0.00100437
Iteration 10/25 | Loss: 0.00100207
Iteration 11/25 | Loss: 0.00099755
Iteration 12/25 | Loss: 0.00099194
Iteration 13/25 | Loss: 0.00098877
Iteration 14/25 | Loss: 0.00098768
Iteration 15/25 | Loss: 0.00099075
Iteration 16/25 | Loss: 0.00098979
Iteration 17/25 | Loss: 0.00098811
Iteration 18/25 | Loss: 0.00098769
Iteration 19/25 | Loss: 0.00098727
Iteration 20/25 | Loss: 0.00098926
Iteration 21/25 | Loss: 0.00098806
Iteration 22/25 | Loss: 0.00099344
Iteration 23/25 | Loss: 0.00098521
Iteration 24/25 | Loss: 0.00098241
Iteration 25/25 | Loss: 0.00098177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91841996
Iteration 2/25 | Loss: 0.00082924
Iteration 3/25 | Loss: 0.00082924
Iteration 4/25 | Loss: 0.00082924
Iteration 5/25 | Loss: 0.00082924
Iteration 6/25 | Loss: 0.00082924
Iteration 7/25 | Loss: 0.00082924
Iteration 8/25 | Loss: 0.00082924
Iteration 9/25 | Loss: 0.00082924
Iteration 10/25 | Loss: 0.00082924
Iteration 11/25 | Loss: 0.00082924
Iteration 12/25 | Loss: 0.00082924
Iteration 13/25 | Loss: 0.00082924
Iteration 14/25 | Loss: 0.00082924
Iteration 15/25 | Loss: 0.00082924
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.000829238910228014, 0.000829238910228014, 0.000829238910228014, 0.000829238910228014, 0.000829238910228014]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000829238910228014

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082924
Iteration 2/1000 | Loss: 0.00007816
Iteration 3/1000 | Loss: 0.00005955
Iteration 4/1000 | Loss: 0.00036410
Iteration 5/1000 | Loss: 0.00036973
Iteration 6/1000 | Loss: 0.00006002
Iteration 7/1000 | Loss: 0.00005139
Iteration 8/1000 | Loss: 0.00004843
Iteration 9/1000 | Loss: 0.00004527
Iteration 10/1000 | Loss: 0.00004378
Iteration 11/1000 | Loss: 0.00004291
Iteration 12/1000 | Loss: 0.00004243
Iteration 13/1000 | Loss: 0.00004197
Iteration 14/1000 | Loss: 0.00004152
Iteration 15/1000 | Loss: 0.00004122
Iteration 16/1000 | Loss: 0.00004097
Iteration 17/1000 | Loss: 0.00004078
Iteration 18/1000 | Loss: 0.00004073
Iteration 19/1000 | Loss: 0.00004056
Iteration 20/1000 | Loss: 0.00004046
Iteration 21/1000 | Loss: 0.00004045
Iteration 22/1000 | Loss: 0.00004042
Iteration 23/1000 | Loss: 0.00004031
Iteration 24/1000 | Loss: 0.00004023
Iteration 25/1000 | Loss: 0.00004020
Iteration 26/1000 | Loss: 0.00004017
Iteration 27/1000 | Loss: 0.00004016
Iteration 28/1000 | Loss: 0.00004015
Iteration 29/1000 | Loss: 0.00004015
Iteration 30/1000 | Loss: 0.00004015
Iteration 31/1000 | Loss: 0.00004014
Iteration 32/1000 | Loss: 0.00004014
Iteration 33/1000 | Loss: 0.00004009
Iteration 34/1000 | Loss: 0.00003995
Iteration 35/1000 | Loss: 0.00003994
Iteration 36/1000 | Loss: 0.00003990
Iteration 37/1000 | Loss: 0.00003985
Iteration 38/1000 | Loss: 0.00003985
Iteration 39/1000 | Loss: 0.00003981
Iteration 40/1000 | Loss: 0.00003979
Iteration 41/1000 | Loss: 0.00003978
Iteration 42/1000 | Loss: 0.00003977
Iteration 43/1000 | Loss: 0.00003977
Iteration 44/1000 | Loss: 0.00003977
Iteration 45/1000 | Loss: 0.00003976
Iteration 46/1000 | Loss: 0.00003975
Iteration 47/1000 | Loss: 0.00003975
Iteration 48/1000 | Loss: 0.00003975
Iteration 49/1000 | Loss: 0.00003975
Iteration 50/1000 | Loss: 0.00003975
Iteration 51/1000 | Loss: 0.00003975
Iteration 52/1000 | Loss: 0.00003974
Iteration 53/1000 | Loss: 0.00003974
Iteration 54/1000 | Loss: 0.00003974
Iteration 55/1000 | Loss: 0.00003974
Iteration 56/1000 | Loss: 0.00003974
Iteration 57/1000 | Loss: 0.00003974
Iteration 58/1000 | Loss: 0.00003974
Iteration 59/1000 | Loss: 0.00003973
Iteration 60/1000 | Loss: 0.00003973
Iteration 61/1000 | Loss: 0.00003972
Iteration 62/1000 | Loss: 0.00003972
Iteration 63/1000 | Loss: 0.00003972
Iteration 64/1000 | Loss: 0.00003972
Iteration 65/1000 | Loss: 0.00003971
Iteration 66/1000 | Loss: 0.00003971
Iteration 67/1000 | Loss: 0.00003971
Iteration 68/1000 | Loss: 0.00003971
Iteration 69/1000 | Loss: 0.00003970
Iteration 70/1000 | Loss: 0.00003970
Iteration 71/1000 | Loss: 0.00003969
Iteration 72/1000 | Loss: 0.00003969
Iteration 73/1000 | Loss: 0.00003969
Iteration 74/1000 | Loss: 0.00003969
Iteration 75/1000 | Loss: 0.00003969
Iteration 76/1000 | Loss: 0.00003968
Iteration 77/1000 | Loss: 0.00003968
Iteration 78/1000 | Loss: 0.00003968
Iteration 79/1000 | Loss: 0.00003968
Iteration 80/1000 | Loss: 0.00003968
Iteration 81/1000 | Loss: 0.00003968
Iteration 82/1000 | Loss: 0.00003968
Iteration 83/1000 | Loss: 0.00003968
Iteration 84/1000 | Loss: 0.00003968
Iteration 85/1000 | Loss: 0.00003968
Iteration 86/1000 | Loss: 0.00003967
Iteration 87/1000 | Loss: 0.00003967
Iteration 88/1000 | Loss: 0.00003967
Iteration 89/1000 | Loss: 0.00003967
Iteration 90/1000 | Loss: 0.00003966
Iteration 91/1000 | Loss: 0.00003966
Iteration 92/1000 | Loss: 0.00003966
Iteration 93/1000 | Loss: 0.00003966
Iteration 94/1000 | Loss: 0.00003966
Iteration 95/1000 | Loss: 0.00003966
Iteration 96/1000 | Loss: 0.00003966
Iteration 97/1000 | Loss: 0.00003965
Iteration 98/1000 | Loss: 0.00003965
Iteration 99/1000 | Loss: 0.00003965
Iteration 100/1000 | Loss: 0.00003965
Iteration 101/1000 | Loss: 0.00003965
Iteration 102/1000 | Loss: 0.00003965
Iteration 103/1000 | Loss: 0.00003965
Iteration 104/1000 | Loss: 0.00003964
Iteration 105/1000 | Loss: 0.00003964
Iteration 106/1000 | Loss: 0.00003964
Iteration 107/1000 | Loss: 0.00003964
Iteration 108/1000 | Loss: 0.00003964
Iteration 109/1000 | Loss: 0.00003964
Iteration 110/1000 | Loss: 0.00003964
Iteration 111/1000 | Loss: 0.00003964
Iteration 112/1000 | Loss: 0.00003964
Iteration 113/1000 | Loss: 0.00003964
Iteration 114/1000 | Loss: 0.00003963
Iteration 115/1000 | Loss: 0.00003963
Iteration 116/1000 | Loss: 0.00003963
Iteration 117/1000 | Loss: 0.00003963
Iteration 118/1000 | Loss: 0.00003963
Iteration 119/1000 | Loss: 0.00003963
Iteration 120/1000 | Loss: 0.00003963
Iteration 121/1000 | Loss: 0.00003963
Iteration 122/1000 | Loss: 0.00003962
Iteration 123/1000 | Loss: 0.00003962
Iteration 124/1000 | Loss: 0.00003962
Iteration 125/1000 | Loss: 0.00003962
Iteration 126/1000 | Loss: 0.00003962
Iteration 127/1000 | Loss: 0.00003962
Iteration 128/1000 | Loss: 0.00003962
Iteration 129/1000 | Loss: 0.00003962
Iteration 130/1000 | Loss: 0.00003961
Iteration 131/1000 | Loss: 0.00003961
Iteration 132/1000 | Loss: 0.00003961
Iteration 133/1000 | Loss: 0.00003961
Iteration 134/1000 | Loss: 0.00003961
Iteration 135/1000 | Loss: 0.00003960
Iteration 136/1000 | Loss: 0.00003960
Iteration 137/1000 | Loss: 0.00003960
Iteration 138/1000 | Loss: 0.00003960
Iteration 139/1000 | Loss: 0.00003960
Iteration 140/1000 | Loss: 0.00003959
Iteration 141/1000 | Loss: 0.00003959
Iteration 142/1000 | Loss: 0.00003959
Iteration 143/1000 | Loss: 0.00003959
Iteration 144/1000 | Loss: 0.00003959
Iteration 145/1000 | Loss: 0.00003959
Iteration 146/1000 | Loss: 0.00003959
Iteration 147/1000 | Loss: 0.00003959
Iteration 148/1000 | Loss: 0.00003959
Iteration 149/1000 | Loss: 0.00003959
Iteration 150/1000 | Loss: 0.00003959
Iteration 151/1000 | Loss: 0.00003958
Iteration 152/1000 | Loss: 0.00003958
Iteration 153/1000 | Loss: 0.00003958
Iteration 154/1000 | Loss: 0.00003958
Iteration 155/1000 | Loss: 0.00003958
Iteration 156/1000 | Loss: 0.00003958
Iteration 157/1000 | Loss: 0.00003958
Iteration 158/1000 | Loss: 0.00003958
Iteration 159/1000 | Loss: 0.00003958
Iteration 160/1000 | Loss: 0.00003958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [3.9583712350577116e-05, 3.9583712350577116e-05, 3.9583712350577116e-05, 3.9583712350577116e-05, 3.9583712350577116e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.9583712350577116e-05

Optimization complete. Final v2v error: 4.981800556182861 mm

Highest mean error: 5.351980209350586 mm for frame 217

Lowest mean error: 4.351139068603516 mm for frame 126

Saving results

Total time: 103.79051899909973
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815251
Iteration 2/25 | Loss: 0.00101841
Iteration 3/25 | Loss: 0.00078079
Iteration 4/25 | Loss: 0.00074320
Iteration 5/25 | Loss: 0.00073662
Iteration 6/25 | Loss: 0.00073628
Iteration 7/25 | Loss: 0.00073628
Iteration 8/25 | Loss: 0.00073628
Iteration 9/25 | Loss: 0.00073628
Iteration 10/25 | Loss: 0.00073628
Iteration 11/25 | Loss: 0.00073628
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00073627958772704, 0.00073627958772704, 0.00073627958772704, 0.00073627958772704, 0.00073627958772704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00073627958772704

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56164885
Iteration 2/25 | Loss: 0.00090392
Iteration 3/25 | Loss: 0.00090392
Iteration 4/25 | Loss: 0.00090392
Iteration 5/25 | Loss: 0.00090391
Iteration 6/25 | Loss: 0.00090391
Iteration 7/25 | Loss: 0.00090391
Iteration 8/25 | Loss: 0.00090391
Iteration 9/25 | Loss: 0.00090391
Iteration 10/25 | Loss: 0.00090391
Iteration 11/25 | Loss: 0.00090391
Iteration 12/25 | Loss: 0.00090391
Iteration 13/25 | Loss: 0.00090391
Iteration 14/25 | Loss: 0.00090391
Iteration 15/25 | Loss: 0.00090391
Iteration 16/25 | Loss: 0.00090391
Iteration 17/25 | Loss: 0.00090391
Iteration 18/25 | Loss: 0.00090391
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009039129363372922, 0.0009039129363372922, 0.0009039129363372922, 0.0009039129363372922, 0.0009039129363372922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009039129363372922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090391
Iteration 2/1000 | Loss: 0.00002392
Iteration 3/1000 | Loss: 0.00001498
Iteration 4/1000 | Loss: 0.00001348
Iteration 5/1000 | Loss: 0.00001268
Iteration 6/1000 | Loss: 0.00001236
Iteration 7/1000 | Loss: 0.00001213
Iteration 8/1000 | Loss: 0.00001194
Iteration 9/1000 | Loss: 0.00001188
Iteration 10/1000 | Loss: 0.00001186
Iteration 11/1000 | Loss: 0.00001185
Iteration 12/1000 | Loss: 0.00001184
Iteration 13/1000 | Loss: 0.00001183
Iteration 14/1000 | Loss: 0.00001182
Iteration 15/1000 | Loss: 0.00001182
Iteration 16/1000 | Loss: 0.00001181
Iteration 17/1000 | Loss: 0.00001179
Iteration 18/1000 | Loss: 0.00001178
Iteration 19/1000 | Loss: 0.00001178
Iteration 20/1000 | Loss: 0.00001177
Iteration 21/1000 | Loss: 0.00001172
Iteration 22/1000 | Loss: 0.00001172
Iteration 23/1000 | Loss: 0.00001171
Iteration 24/1000 | Loss: 0.00001170
Iteration 25/1000 | Loss: 0.00001169
Iteration 26/1000 | Loss: 0.00001169
Iteration 27/1000 | Loss: 0.00001165
Iteration 28/1000 | Loss: 0.00001164
Iteration 29/1000 | Loss: 0.00001164
Iteration 30/1000 | Loss: 0.00001163
Iteration 31/1000 | Loss: 0.00001163
Iteration 32/1000 | Loss: 0.00001162
Iteration 33/1000 | Loss: 0.00001162
Iteration 34/1000 | Loss: 0.00001161
Iteration 35/1000 | Loss: 0.00001160
Iteration 36/1000 | Loss: 0.00001160
Iteration 37/1000 | Loss: 0.00001159
Iteration 38/1000 | Loss: 0.00001159
Iteration 39/1000 | Loss: 0.00001158
Iteration 40/1000 | Loss: 0.00001157
Iteration 41/1000 | Loss: 0.00001157
Iteration 42/1000 | Loss: 0.00001154
Iteration 43/1000 | Loss: 0.00001154
Iteration 44/1000 | Loss: 0.00001154
Iteration 45/1000 | Loss: 0.00001154
Iteration 46/1000 | Loss: 0.00001154
Iteration 47/1000 | Loss: 0.00001152
Iteration 48/1000 | Loss: 0.00001152
Iteration 49/1000 | Loss: 0.00001152
Iteration 50/1000 | Loss: 0.00001152
Iteration 51/1000 | Loss: 0.00001152
Iteration 52/1000 | Loss: 0.00001152
Iteration 53/1000 | Loss: 0.00001152
Iteration 54/1000 | Loss: 0.00001151
Iteration 55/1000 | Loss: 0.00001151
Iteration 56/1000 | Loss: 0.00001151
Iteration 57/1000 | Loss: 0.00001151
Iteration 58/1000 | Loss: 0.00001151
Iteration 59/1000 | Loss: 0.00001150
Iteration 60/1000 | Loss: 0.00001150
Iteration 61/1000 | Loss: 0.00001150
Iteration 62/1000 | Loss: 0.00001150
Iteration 63/1000 | Loss: 0.00001150
Iteration 64/1000 | Loss: 0.00001150
Iteration 65/1000 | Loss: 0.00001150
Iteration 66/1000 | Loss: 0.00001150
Iteration 67/1000 | Loss: 0.00001150
Iteration 68/1000 | Loss: 0.00001150
Iteration 69/1000 | Loss: 0.00001150
Iteration 70/1000 | Loss: 0.00001150
Iteration 71/1000 | Loss: 0.00001149
Iteration 72/1000 | Loss: 0.00001149
Iteration 73/1000 | Loss: 0.00001149
Iteration 74/1000 | Loss: 0.00001149
Iteration 75/1000 | Loss: 0.00001149
Iteration 76/1000 | Loss: 0.00001149
Iteration 77/1000 | Loss: 0.00001149
Iteration 78/1000 | Loss: 0.00001148
Iteration 79/1000 | Loss: 0.00001148
Iteration 80/1000 | Loss: 0.00001148
Iteration 81/1000 | Loss: 0.00001148
Iteration 82/1000 | Loss: 0.00001148
Iteration 83/1000 | Loss: 0.00001148
Iteration 84/1000 | Loss: 0.00001148
Iteration 85/1000 | Loss: 0.00001148
Iteration 86/1000 | Loss: 0.00001148
Iteration 87/1000 | Loss: 0.00001147
Iteration 88/1000 | Loss: 0.00001147
Iteration 89/1000 | Loss: 0.00001147
Iteration 90/1000 | Loss: 0.00001147
Iteration 91/1000 | Loss: 0.00001147
Iteration 92/1000 | Loss: 0.00001147
Iteration 93/1000 | Loss: 0.00001147
Iteration 94/1000 | Loss: 0.00001147
Iteration 95/1000 | Loss: 0.00001147
Iteration 96/1000 | Loss: 0.00001147
Iteration 97/1000 | Loss: 0.00001147
Iteration 98/1000 | Loss: 0.00001147
Iteration 99/1000 | Loss: 0.00001147
Iteration 100/1000 | Loss: 0.00001147
Iteration 101/1000 | Loss: 0.00001147
Iteration 102/1000 | Loss: 0.00001147
Iteration 103/1000 | Loss: 0.00001147
Iteration 104/1000 | Loss: 0.00001147
Iteration 105/1000 | Loss: 0.00001147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.1472960977698676e-05, 1.1472960977698676e-05, 1.1472960977698676e-05, 1.1472960977698676e-05, 1.1472960977698676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1472960977698676e-05

Optimization complete. Final v2v error: 2.8115017414093018 mm

Highest mean error: 2.9910049438476562 mm for frame 129

Lowest mean error: 2.5616135597229004 mm for frame 236

Saving results

Total time: 33.77094912528992
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00810590
Iteration 2/25 | Loss: 0.00122871
Iteration 3/25 | Loss: 0.00089465
Iteration 4/25 | Loss: 0.00084329
Iteration 5/25 | Loss: 0.00083125
Iteration 6/25 | Loss: 0.00082869
Iteration 7/25 | Loss: 0.00082851
Iteration 8/25 | Loss: 0.00082851
Iteration 9/25 | Loss: 0.00082851
Iteration 10/25 | Loss: 0.00082851
Iteration 11/25 | Loss: 0.00082851
Iteration 12/25 | Loss: 0.00082851
Iteration 13/25 | Loss: 0.00082851
Iteration 14/25 | Loss: 0.00082851
Iteration 15/25 | Loss: 0.00082851
Iteration 16/25 | Loss: 0.00082851
Iteration 17/25 | Loss: 0.00082851
Iteration 18/25 | Loss: 0.00082851
Iteration 19/25 | Loss: 0.00082851
Iteration 20/25 | Loss: 0.00082851
Iteration 21/25 | Loss: 0.00082851
Iteration 22/25 | Loss: 0.00082851
Iteration 23/25 | Loss: 0.00082851
Iteration 24/25 | Loss: 0.00082851
Iteration 25/25 | Loss: 0.00082851

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54812646
Iteration 2/25 | Loss: 0.00097232
Iteration 3/25 | Loss: 0.00097230
Iteration 4/25 | Loss: 0.00097230
Iteration 5/25 | Loss: 0.00097230
Iteration 6/25 | Loss: 0.00097230
Iteration 7/25 | Loss: 0.00097230
Iteration 8/25 | Loss: 0.00097230
Iteration 9/25 | Loss: 0.00097230
Iteration 10/25 | Loss: 0.00097230
Iteration 11/25 | Loss: 0.00097230
Iteration 12/25 | Loss: 0.00097230
Iteration 13/25 | Loss: 0.00097230
Iteration 14/25 | Loss: 0.00097230
Iteration 15/25 | Loss: 0.00097230
Iteration 16/25 | Loss: 0.00097230
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000972301117144525, 0.000972301117144525, 0.000972301117144525, 0.000972301117144525, 0.000972301117144525]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000972301117144525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097230
Iteration 2/1000 | Loss: 0.00003314
Iteration 3/1000 | Loss: 0.00002538
Iteration 4/1000 | Loss: 0.00002316
Iteration 5/1000 | Loss: 0.00002204
Iteration 6/1000 | Loss: 0.00002148
Iteration 7/1000 | Loss: 0.00002097
Iteration 8/1000 | Loss: 0.00002047
Iteration 9/1000 | Loss: 0.00002015
Iteration 10/1000 | Loss: 0.00002014
Iteration 11/1000 | Loss: 0.00002013
Iteration 12/1000 | Loss: 0.00002010
Iteration 13/1000 | Loss: 0.00001998
Iteration 14/1000 | Loss: 0.00001990
Iteration 15/1000 | Loss: 0.00001981
Iteration 16/1000 | Loss: 0.00001979
Iteration 17/1000 | Loss: 0.00001979
Iteration 18/1000 | Loss: 0.00001978
Iteration 19/1000 | Loss: 0.00001978
Iteration 20/1000 | Loss: 0.00001977
Iteration 21/1000 | Loss: 0.00001977
Iteration 22/1000 | Loss: 0.00001977
Iteration 23/1000 | Loss: 0.00001976
Iteration 24/1000 | Loss: 0.00001976
Iteration 25/1000 | Loss: 0.00001976
Iteration 26/1000 | Loss: 0.00001976
Iteration 27/1000 | Loss: 0.00001975
Iteration 28/1000 | Loss: 0.00001975
Iteration 29/1000 | Loss: 0.00001975
Iteration 30/1000 | Loss: 0.00001975
Iteration 31/1000 | Loss: 0.00001975
Iteration 32/1000 | Loss: 0.00001975
Iteration 33/1000 | Loss: 0.00001975
Iteration 34/1000 | Loss: 0.00001974
Iteration 35/1000 | Loss: 0.00001974
Iteration 36/1000 | Loss: 0.00001974
Iteration 37/1000 | Loss: 0.00001974
Iteration 38/1000 | Loss: 0.00001974
Iteration 39/1000 | Loss: 0.00001974
Iteration 40/1000 | Loss: 0.00001973
Iteration 41/1000 | Loss: 0.00001973
Iteration 42/1000 | Loss: 0.00001973
Iteration 43/1000 | Loss: 0.00001973
Iteration 44/1000 | Loss: 0.00001973
Iteration 45/1000 | Loss: 0.00001973
Iteration 46/1000 | Loss: 0.00001973
Iteration 47/1000 | Loss: 0.00001973
Iteration 48/1000 | Loss: 0.00001973
Iteration 49/1000 | Loss: 0.00001973
Iteration 50/1000 | Loss: 0.00001973
Iteration 51/1000 | Loss: 0.00001973
Iteration 52/1000 | Loss: 0.00001972
Iteration 53/1000 | Loss: 0.00001972
Iteration 54/1000 | Loss: 0.00001972
Iteration 55/1000 | Loss: 0.00001972
Iteration 56/1000 | Loss: 0.00001972
Iteration 57/1000 | Loss: 0.00001972
Iteration 58/1000 | Loss: 0.00001972
Iteration 59/1000 | Loss: 0.00001971
Iteration 60/1000 | Loss: 0.00001971
Iteration 61/1000 | Loss: 0.00001971
Iteration 62/1000 | Loss: 0.00001971
Iteration 63/1000 | Loss: 0.00001971
Iteration 64/1000 | Loss: 0.00001971
Iteration 65/1000 | Loss: 0.00001971
Iteration 66/1000 | Loss: 0.00001971
Iteration 67/1000 | Loss: 0.00001971
Iteration 68/1000 | Loss: 0.00001971
Iteration 69/1000 | Loss: 0.00001970
Iteration 70/1000 | Loss: 0.00001970
Iteration 71/1000 | Loss: 0.00001970
Iteration 72/1000 | Loss: 0.00001970
Iteration 73/1000 | Loss: 0.00001970
Iteration 74/1000 | Loss: 0.00001970
Iteration 75/1000 | Loss: 0.00001970
Iteration 76/1000 | Loss: 0.00001970
Iteration 77/1000 | Loss: 0.00001970
Iteration 78/1000 | Loss: 0.00001970
Iteration 79/1000 | Loss: 0.00001970
Iteration 80/1000 | Loss: 0.00001970
Iteration 81/1000 | Loss: 0.00001970
Iteration 82/1000 | Loss: 0.00001970
Iteration 83/1000 | Loss: 0.00001970
Iteration 84/1000 | Loss: 0.00001970
Iteration 85/1000 | Loss: 0.00001970
Iteration 86/1000 | Loss: 0.00001970
Iteration 87/1000 | Loss: 0.00001970
Iteration 88/1000 | Loss: 0.00001970
Iteration 89/1000 | Loss: 0.00001970
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [1.9696863091667183e-05, 1.9696863091667183e-05, 1.9696863091667183e-05, 1.9696863091667183e-05, 1.9696863091667183e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9696863091667183e-05

Optimization complete. Final v2v error: 3.6796011924743652 mm

Highest mean error: 3.8116631507873535 mm for frame 64

Lowest mean error: 3.3968377113342285 mm for frame 7

Saving results

Total time: 34.47520112991333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00388622
Iteration 2/25 | Loss: 0.00097096
Iteration 3/25 | Loss: 0.00081852
Iteration 4/25 | Loss: 0.00077944
Iteration 5/25 | Loss: 0.00076827
Iteration 6/25 | Loss: 0.00076557
Iteration 7/25 | Loss: 0.00076479
Iteration 8/25 | Loss: 0.00076465
Iteration 9/25 | Loss: 0.00076465
Iteration 10/25 | Loss: 0.00076465
Iteration 11/25 | Loss: 0.00076465
Iteration 12/25 | Loss: 0.00076465
Iteration 13/25 | Loss: 0.00076465
Iteration 14/25 | Loss: 0.00076465
Iteration 15/25 | Loss: 0.00076465
Iteration 16/25 | Loss: 0.00076465
Iteration 17/25 | Loss: 0.00076465
Iteration 18/25 | Loss: 0.00076465
Iteration 19/25 | Loss: 0.00076465
Iteration 20/25 | Loss: 0.00076465
Iteration 21/25 | Loss: 0.00076465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007646501180715859, 0.0007646501180715859, 0.0007646501180715859, 0.0007646501180715859, 0.0007646501180715859]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007646501180715859

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52779448
Iteration 2/25 | Loss: 0.00097807
Iteration 3/25 | Loss: 0.00097806
Iteration 4/25 | Loss: 0.00097806
Iteration 5/25 | Loss: 0.00097806
Iteration 6/25 | Loss: 0.00097806
Iteration 7/25 | Loss: 0.00097806
Iteration 8/25 | Loss: 0.00097806
Iteration 9/25 | Loss: 0.00097806
Iteration 10/25 | Loss: 0.00097806
Iteration 11/25 | Loss: 0.00097806
Iteration 12/25 | Loss: 0.00097806
Iteration 13/25 | Loss: 0.00097806
Iteration 14/25 | Loss: 0.00097806
Iteration 15/25 | Loss: 0.00097806
Iteration 16/25 | Loss: 0.00097806
Iteration 17/25 | Loss: 0.00097806
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009780620457604527, 0.0009780620457604527, 0.0009780620457604527, 0.0009780620457604527, 0.0009780620457604527]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009780620457604527

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097806
Iteration 2/1000 | Loss: 0.00004950
Iteration 3/1000 | Loss: 0.00003624
Iteration 4/1000 | Loss: 0.00002940
Iteration 5/1000 | Loss: 0.00002691
Iteration 6/1000 | Loss: 0.00002555
Iteration 7/1000 | Loss: 0.00002445
Iteration 8/1000 | Loss: 0.00002382
Iteration 9/1000 | Loss: 0.00002323
Iteration 10/1000 | Loss: 0.00002276
Iteration 11/1000 | Loss: 0.00002247
Iteration 12/1000 | Loss: 0.00002245
Iteration 13/1000 | Loss: 0.00002227
Iteration 14/1000 | Loss: 0.00002213
Iteration 15/1000 | Loss: 0.00002198
Iteration 16/1000 | Loss: 0.00002195
Iteration 17/1000 | Loss: 0.00002193
Iteration 18/1000 | Loss: 0.00002182
Iteration 19/1000 | Loss: 0.00002176
Iteration 20/1000 | Loss: 0.00002173
Iteration 21/1000 | Loss: 0.00002173
Iteration 22/1000 | Loss: 0.00002172
Iteration 23/1000 | Loss: 0.00002168
Iteration 24/1000 | Loss: 0.00002168
Iteration 25/1000 | Loss: 0.00002165
Iteration 26/1000 | Loss: 0.00002165
Iteration 27/1000 | Loss: 0.00002165
Iteration 28/1000 | Loss: 0.00002164
Iteration 29/1000 | Loss: 0.00002164
Iteration 30/1000 | Loss: 0.00002164
Iteration 31/1000 | Loss: 0.00002164
Iteration 32/1000 | Loss: 0.00002163
Iteration 33/1000 | Loss: 0.00002163
Iteration 34/1000 | Loss: 0.00002162
Iteration 35/1000 | Loss: 0.00002162
Iteration 36/1000 | Loss: 0.00002161
Iteration 37/1000 | Loss: 0.00002161
Iteration 38/1000 | Loss: 0.00002161
Iteration 39/1000 | Loss: 0.00002161
Iteration 40/1000 | Loss: 0.00002160
Iteration 41/1000 | Loss: 0.00002160
Iteration 42/1000 | Loss: 0.00002160
Iteration 43/1000 | Loss: 0.00002159
Iteration 44/1000 | Loss: 0.00002159
Iteration 45/1000 | Loss: 0.00002159
Iteration 46/1000 | Loss: 0.00002158
Iteration 47/1000 | Loss: 0.00002158
Iteration 48/1000 | Loss: 0.00002158
Iteration 49/1000 | Loss: 0.00002157
Iteration 50/1000 | Loss: 0.00002156
Iteration 51/1000 | Loss: 0.00002156
Iteration 52/1000 | Loss: 0.00002156
Iteration 53/1000 | Loss: 0.00002155
Iteration 54/1000 | Loss: 0.00002155
Iteration 55/1000 | Loss: 0.00002155
Iteration 56/1000 | Loss: 0.00002155
Iteration 57/1000 | Loss: 0.00002155
Iteration 58/1000 | Loss: 0.00002155
Iteration 59/1000 | Loss: 0.00002154
Iteration 60/1000 | Loss: 0.00002154
Iteration 61/1000 | Loss: 0.00002154
Iteration 62/1000 | Loss: 0.00002153
Iteration 63/1000 | Loss: 0.00002153
Iteration 64/1000 | Loss: 0.00002153
Iteration 65/1000 | Loss: 0.00002152
Iteration 66/1000 | Loss: 0.00002152
Iteration 67/1000 | Loss: 0.00002152
Iteration 68/1000 | Loss: 0.00002152
Iteration 69/1000 | Loss: 0.00002152
Iteration 70/1000 | Loss: 0.00002151
Iteration 71/1000 | Loss: 0.00002151
Iteration 72/1000 | Loss: 0.00002151
Iteration 73/1000 | Loss: 0.00002151
Iteration 74/1000 | Loss: 0.00002150
Iteration 75/1000 | Loss: 0.00002150
Iteration 76/1000 | Loss: 0.00002150
Iteration 77/1000 | Loss: 0.00002150
Iteration 78/1000 | Loss: 0.00002150
Iteration 79/1000 | Loss: 0.00002150
Iteration 80/1000 | Loss: 0.00002150
Iteration 81/1000 | Loss: 0.00002149
Iteration 82/1000 | Loss: 0.00002149
Iteration 83/1000 | Loss: 0.00002149
Iteration 84/1000 | Loss: 0.00002148
Iteration 85/1000 | Loss: 0.00002148
Iteration 86/1000 | Loss: 0.00002148
Iteration 87/1000 | Loss: 0.00002148
Iteration 88/1000 | Loss: 0.00002148
Iteration 89/1000 | Loss: 0.00002148
Iteration 90/1000 | Loss: 0.00002148
Iteration 91/1000 | Loss: 0.00002148
Iteration 92/1000 | Loss: 0.00002147
Iteration 93/1000 | Loss: 0.00002147
Iteration 94/1000 | Loss: 0.00002147
Iteration 95/1000 | Loss: 0.00002147
Iteration 96/1000 | Loss: 0.00002147
Iteration 97/1000 | Loss: 0.00002147
Iteration 98/1000 | Loss: 0.00002147
Iteration 99/1000 | Loss: 0.00002147
Iteration 100/1000 | Loss: 0.00002147
Iteration 101/1000 | Loss: 0.00002147
Iteration 102/1000 | Loss: 0.00002147
Iteration 103/1000 | Loss: 0.00002146
Iteration 104/1000 | Loss: 0.00002146
Iteration 105/1000 | Loss: 0.00002146
Iteration 106/1000 | Loss: 0.00002146
Iteration 107/1000 | Loss: 0.00002146
Iteration 108/1000 | Loss: 0.00002146
Iteration 109/1000 | Loss: 0.00002145
Iteration 110/1000 | Loss: 0.00002145
Iteration 111/1000 | Loss: 0.00002145
Iteration 112/1000 | Loss: 0.00002145
Iteration 113/1000 | Loss: 0.00002145
Iteration 114/1000 | Loss: 0.00002145
Iteration 115/1000 | Loss: 0.00002145
Iteration 116/1000 | Loss: 0.00002145
Iteration 117/1000 | Loss: 0.00002145
Iteration 118/1000 | Loss: 0.00002145
Iteration 119/1000 | Loss: 0.00002145
Iteration 120/1000 | Loss: 0.00002144
Iteration 121/1000 | Loss: 0.00002144
Iteration 122/1000 | Loss: 0.00002144
Iteration 123/1000 | Loss: 0.00002144
Iteration 124/1000 | Loss: 0.00002144
Iteration 125/1000 | Loss: 0.00002144
Iteration 126/1000 | Loss: 0.00002144
Iteration 127/1000 | Loss: 0.00002144
Iteration 128/1000 | Loss: 0.00002143
Iteration 129/1000 | Loss: 0.00002143
Iteration 130/1000 | Loss: 0.00002143
Iteration 131/1000 | Loss: 0.00002143
Iteration 132/1000 | Loss: 0.00002143
Iteration 133/1000 | Loss: 0.00002143
Iteration 134/1000 | Loss: 0.00002143
Iteration 135/1000 | Loss: 0.00002143
Iteration 136/1000 | Loss: 0.00002143
Iteration 137/1000 | Loss: 0.00002143
Iteration 138/1000 | Loss: 0.00002142
Iteration 139/1000 | Loss: 0.00002142
Iteration 140/1000 | Loss: 0.00002142
Iteration 141/1000 | Loss: 0.00002142
Iteration 142/1000 | Loss: 0.00002142
Iteration 143/1000 | Loss: 0.00002142
Iteration 144/1000 | Loss: 0.00002142
Iteration 145/1000 | Loss: 0.00002142
Iteration 146/1000 | Loss: 0.00002142
Iteration 147/1000 | Loss: 0.00002142
Iteration 148/1000 | Loss: 0.00002142
Iteration 149/1000 | Loss: 0.00002142
Iteration 150/1000 | Loss: 0.00002142
Iteration 151/1000 | Loss: 0.00002142
Iteration 152/1000 | Loss: 0.00002142
Iteration 153/1000 | Loss: 0.00002142
Iteration 154/1000 | Loss: 0.00002141
Iteration 155/1000 | Loss: 0.00002141
Iteration 156/1000 | Loss: 0.00002141
Iteration 157/1000 | Loss: 0.00002141
Iteration 158/1000 | Loss: 0.00002141
Iteration 159/1000 | Loss: 0.00002141
Iteration 160/1000 | Loss: 0.00002141
Iteration 161/1000 | Loss: 0.00002140
Iteration 162/1000 | Loss: 0.00002140
Iteration 163/1000 | Loss: 0.00002140
Iteration 164/1000 | Loss: 0.00002140
Iteration 165/1000 | Loss: 0.00002140
Iteration 166/1000 | Loss: 0.00002140
Iteration 167/1000 | Loss: 0.00002140
Iteration 168/1000 | Loss: 0.00002140
Iteration 169/1000 | Loss: 0.00002140
Iteration 170/1000 | Loss: 0.00002140
Iteration 171/1000 | Loss: 0.00002140
Iteration 172/1000 | Loss: 0.00002140
Iteration 173/1000 | Loss: 0.00002140
Iteration 174/1000 | Loss: 0.00002139
Iteration 175/1000 | Loss: 0.00002139
Iteration 176/1000 | Loss: 0.00002139
Iteration 177/1000 | Loss: 0.00002139
Iteration 178/1000 | Loss: 0.00002139
Iteration 179/1000 | Loss: 0.00002139
Iteration 180/1000 | Loss: 0.00002139
Iteration 181/1000 | Loss: 0.00002139
Iteration 182/1000 | Loss: 0.00002139
Iteration 183/1000 | Loss: 0.00002139
Iteration 184/1000 | Loss: 0.00002139
Iteration 185/1000 | Loss: 0.00002139
Iteration 186/1000 | Loss: 0.00002139
Iteration 187/1000 | Loss: 0.00002139
Iteration 188/1000 | Loss: 0.00002139
Iteration 189/1000 | Loss: 0.00002139
Iteration 190/1000 | Loss: 0.00002139
Iteration 191/1000 | Loss: 0.00002139
Iteration 192/1000 | Loss: 0.00002139
Iteration 193/1000 | Loss: 0.00002138
Iteration 194/1000 | Loss: 0.00002138
Iteration 195/1000 | Loss: 0.00002138
Iteration 196/1000 | Loss: 0.00002138
Iteration 197/1000 | Loss: 0.00002138
Iteration 198/1000 | Loss: 0.00002138
Iteration 199/1000 | Loss: 0.00002138
Iteration 200/1000 | Loss: 0.00002138
Iteration 201/1000 | Loss: 0.00002138
Iteration 202/1000 | Loss: 0.00002138
Iteration 203/1000 | Loss: 0.00002138
Iteration 204/1000 | Loss: 0.00002138
Iteration 205/1000 | Loss: 0.00002138
Iteration 206/1000 | Loss: 0.00002138
Iteration 207/1000 | Loss: 0.00002138
Iteration 208/1000 | Loss: 0.00002138
Iteration 209/1000 | Loss: 0.00002138
Iteration 210/1000 | Loss: 0.00002138
Iteration 211/1000 | Loss: 0.00002138
Iteration 212/1000 | Loss: 0.00002138
Iteration 213/1000 | Loss: 0.00002138
Iteration 214/1000 | Loss: 0.00002137
Iteration 215/1000 | Loss: 0.00002137
Iteration 216/1000 | Loss: 0.00002137
Iteration 217/1000 | Loss: 0.00002137
Iteration 218/1000 | Loss: 0.00002137
Iteration 219/1000 | Loss: 0.00002137
Iteration 220/1000 | Loss: 0.00002137
Iteration 221/1000 | Loss: 0.00002137
Iteration 222/1000 | Loss: 0.00002137
Iteration 223/1000 | Loss: 0.00002137
Iteration 224/1000 | Loss: 0.00002137
Iteration 225/1000 | Loss: 0.00002137
Iteration 226/1000 | Loss: 0.00002137
Iteration 227/1000 | Loss: 0.00002137
Iteration 228/1000 | Loss: 0.00002137
Iteration 229/1000 | Loss: 0.00002137
Iteration 230/1000 | Loss: 0.00002137
Iteration 231/1000 | Loss: 0.00002137
Iteration 232/1000 | Loss: 0.00002137
Iteration 233/1000 | Loss: 0.00002137
Iteration 234/1000 | Loss: 0.00002137
Iteration 235/1000 | Loss: 0.00002137
Iteration 236/1000 | Loss: 0.00002137
Iteration 237/1000 | Loss: 0.00002136
Iteration 238/1000 | Loss: 0.00002136
Iteration 239/1000 | Loss: 0.00002136
Iteration 240/1000 | Loss: 0.00002136
Iteration 241/1000 | Loss: 0.00002136
Iteration 242/1000 | Loss: 0.00002136
Iteration 243/1000 | Loss: 0.00002136
Iteration 244/1000 | Loss: 0.00002136
Iteration 245/1000 | Loss: 0.00002136
Iteration 246/1000 | Loss: 0.00002136
Iteration 247/1000 | Loss: 0.00002136
Iteration 248/1000 | Loss: 0.00002136
Iteration 249/1000 | Loss: 0.00002136
Iteration 250/1000 | Loss: 0.00002136
Iteration 251/1000 | Loss: 0.00002136
Iteration 252/1000 | Loss: 0.00002136
Iteration 253/1000 | Loss: 0.00002136
Iteration 254/1000 | Loss: 0.00002136
Iteration 255/1000 | Loss: 0.00002136
Iteration 256/1000 | Loss: 0.00002136
Iteration 257/1000 | Loss: 0.00002136
Iteration 258/1000 | Loss: 0.00002136
Iteration 259/1000 | Loss: 0.00002136
Iteration 260/1000 | Loss: 0.00002136
Iteration 261/1000 | Loss: 0.00002136
Iteration 262/1000 | Loss: 0.00002136
Iteration 263/1000 | Loss: 0.00002136
Iteration 264/1000 | Loss: 0.00002136
Iteration 265/1000 | Loss: 0.00002136
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [2.1357540390454233e-05, 2.1357540390454233e-05, 2.1357540390454233e-05, 2.1357540390454233e-05, 2.1357540390454233e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1357540390454233e-05

Optimization complete. Final v2v error: 3.8114073276519775 mm

Highest mean error: 4.266057968139648 mm for frame 79

Lowest mean error: 3.132765293121338 mm for frame 70

Saving results

Total time: 48.86911392211914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00365176
Iteration 2/25 | Loss: 0.00085033
Iteration 3/25 | Loss: 0.00077916
Iteration 4/25 | Loss: 0.00075047
Iteration 5/25 | Loss: 0.00074696
Iteration 6/25 | Loss: 0.00074567
Iteration 7/25 | Loss: 0.00074532
Iteration 8/25 | Loss: 0.00074532
Iteration 9/25 | Loss: 0.00074532
Iteration 10/25 | Loss: 0.00074532
Iteration 11/25 | Loss: 0.00074532
Iteration 12/25 | Loss: 0.00074532
Iteration 13/25 | Loss: 0.00074532
Iteration 14/25 | Loss: 0.00074532
Iteration 15/25 | Loss: 0.00074532
Iteration 16/25 | Loss: 0.00074532
Iteration 17/25 | Loss: 0.00074532
Iteration 18/25 | Loss: 0.00074532
Iteration 19/25 | Loss: 0.00074532
Iteration 20/25 | Loss: 0.00074532
Iteration 21/25 | Loss: 0.00074532
Iteration 22/25 | Loss: 0.00074532
Iteration 23/25 | Loss: 0.00074532
Iteration 24/25 | Loss: 0.00074532
Iteration 25/25 | Loss: 0.00074532

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56707692
Iteration 2/25 | Loss: 0.00089248
Iteration 3/25 | Loss: 0.00089248
Iteration 4/25 | Loss: 0.00089247
Iteration 5/25 | Loss: 0.00089247
Iteration 6/25 | Loss: 0.00089247
Iteration 7/25 | Loss: 0.00089247
Iteration 8/25 | Loss: 0.00089247
Iteration 9/25 | Loss: 0.00089247
Iteration 10/25 | Loss: 0.00089247
Iteration 11/25 | Loss: 0.00089247
Iteration 12/25 | Loss: 0.00089247
Iteration 13/25 | Loss: 0.00089247
Iteration 14/25 | Loss: 0.00089247
Iteration 15/25 | Loss: 0.00089247
Iteration 16/25 | Loss: 0.00089247
Iteration 17/25 | Loss: 0.00089247
Iteration 18/25 | Loss: 0.00089247
Iteration 19/25 | Loss: 0.00089247
Iteration 20/25 | Loss: 0.00089247
Iteration 21/25 | Loss: 0.00089247
Iteration 22/25 | Loss: 0.00089247
Iteration 23/25 | Loss: 0.00089247
Iteration 24/25 | Loss: 0.00089247
Iteration 25/25 | Loss: 0.00089247

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089247
Iteration 2/1000 | Loss: 0.00002615
Iteration 3/1000 | Loss: 0.00001985
Iteration 4/1000 | Loss: 0.00001909
Iteration 5/1000 | Loss: 0.00001850
Iteration 6/1000 | Loss: 0.00001821
Iteration 7/1000 | Loss: 0.00001790
Iteration 8/1000 | Loss: 0.00001771
Iteration 9/1000 | Loss: 0.00001770
Iteration 10/1000 | Loss: 0.00001770
Iteration 11/1000 | Loss: 0.00001763
Iteration 12/1000 | Loss: 0.00001750
Iteration 13/1000 | Loss: 0.00001746
Iteration 14/1000 | Loss: 0.00001742
Iteration 15/1000 | Loss: 0.00001738
Iteration 16/1000 | Loss: 0.00001736
Iteration 17/1000 | Loss: 0.00001735
Iteration 18/1000 | Loss: 0.00001734
Iteration 19/1000 | Loss: 0.00001734
Iteration 20/1000 | Loss: 0.00001734
Iteration 21/1000 | Loss: 0.00001734
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001734
Iteration 24/1000 | Loss: 0.00001734
Iteration 25/1000 | Loss: 0.00001734
Iteration 26/1000 | Loss: 0.00001734
Iteration 27/1000 | Loss: 0.00001733
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00001731
Iteration 31/1000 | Loss: 0.00001731
Iteration 32/1000 | Loss: 0.00001731
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001730
Iteration 35/1000 | Loss: 0.00001730
Iteration 36/1000 | Loss: 0.00001729
Iteration 37/1000 | Loss: 0.00001729
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001726
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001726
Iteration 46/1000 | Loss: 0.00001725
Iteration 47/1000 | Loss: 0.00001725
Iteration 48/1000 | Loss: 0.00001725
Iteration 49/1000 | Loss: 0.00001725
Iteration 50/1000 | Loss: 0.00001724
Iteration 51/1000 | Loss: 0.00001724
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001724
Iteration 54/1000 | Loss: 0.00001724
Iteration 55/1000 | Loss: 0.00001724
Iteration 56/1000 | Loss: 0.00001724
Iteration 57/1000 | Loss: 0.00001724
Iteration 58/1000 | Loss: 0.00001724
Iteration 59/1000 | Loss: 0.00001724
Iteration 60/1000 | Loss: 0.00001724
Iteration 61/1000 | Loss: 0.00001724
Iteration 62/1000 | Loss: 0.00001724
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001723
Iteration 68/1000 | Loss: 0.00001723
Iteration 69/1000 | Loss: 0.00001723
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001723
Iteration 72/1000 | Loss: 0.00001723
Iteration 73/1000 | Loss: 0.00001723
Iteration 74/1000 | Loss: 0.00001722
Iteration 75/1000 | Loss: 0.00001722
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001722
Iteration 78/1000 | Loss: 0.00001722
Iteration 79/1000 | Loss: 0.00001722
Iteration 80/1000 | Loss: 0.00001722
Iteration 81/1000 | Loss: 0.00001722
Iteration 82/1000 | Loss: 0.00001722
Iteration 83/1000 | Loss: 0.00001722
Iteration 84/1000 | Loss: 0.00001722
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001722
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001722
Iteration 92/1000 | Loss: 0.00001722
Iteration 93/1000 | Loss: 0.00001722
Iteration 94/1000 | Loss: 0.00001722
Iteration 95/1000 | Loss: 0.00001722
Iteration 96/1000 | Loss: 0.00001722
Iteration 97/1000 | Loss: 0.00001722
Iteration 98/1000 | Loss: 0.00001722
Iteration 99/1000 | Loss: 0.00001722
Iteration 100/1000 | Loss: 0.00001722
Iteration 101/1000 | Loss: 0.00001722
Iteration 102/1000 | Loss: 0.00001722
Iteration 103/1000 | Loss: 0.00001722
Iteration 104/1000 | Loss: 0.00001722
Iteration 105/1000 | Loss: 0.00001722
Iteration 106/1000 | Loss: 0.00001722
Iteration 107/1000 | Loss: 0.00001722
Iteration 108/1000 | Loss: 0.00001722
Iteration 109/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.72166164702503e-05, 1.72166164702503e-05, 1.72166164702503e-05, 1.72166164702503e-05, 1.72166164702503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.72166164702503e-05

Optimization complete. Final v2v error: 3.4747846126556396 mm

Highest mean error: 3.682988405227661 mm for frame 88

Lowest mean error: 3.3563244342803955 mm for frame 25

Saving results

Total time: 30.404787063598633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470536
Iteration 2/25 | Loss: 0.00112370
Iteration 3/25 | Loss: 0.00084358
Iteration 4/25 | Loss: 0.00079626
Iteration 5/25 | Loss: 0.00078796
Iteration 6/25 | Loss: 0.00078706
Iteration 7/25 | Loss: 0.00078706
Iteration 8/25 | Loss: 0.00078706
Iteration 9/25 | Loss: 0.00078706
Iteration 10/25 | Loss: 0.00078706
Iteration 11/25 | Loss: 0.00078706
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007870587287470698, 0.0007870587287470698, 0.0007870587287470698, 0.0007870587287470698, 0.0007870587287470698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007870587287470698

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56537700
Iteration 2/25 | Loss: 0.00082011
Iteration 3/25 | Loss: 0.00082010
Iteration 4/25 | Loss: 0.00082010
Iteration 5/25 | Loss: 0.00082010
Iteration 6/25 | Loss: 0.00082010
Iteration 7/25 | Loss: 0.00082010
Iteration 8/25 | Loss: 0.00082010
Iteration 9/25 | Loss: 0.00082010
Iteration 10/25 | Loss: 0.00082010
Iteration 11/25 | Loss: 0.00082010
Iteration 12/25 | Loss: 0.00082010
Iteration 13/25 | Loss: 0.00082010
Iteration 14/25 | Loss: 0.00082010
Iteration 15/25 | Loss: 0.00082010
Iteration 16/25 | Loss: 0.00082010
Iteration 17/25 | Loss: 0.00082010
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008200967567972839, 0.0008200967567972839, 0.0008200967567972839, 0.0008200967567972839, 0.0008200967567972839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008200967567972839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082010
Iteration 2/1000 | Loss: 0.00002909
Iteration 3/1000 | Loss: 0.00002053
Iteration 4/1000 | Loss: 0.00001893
Iteration 5/1000 | Loss: 0.00001802
Iteration 6/1000 | Loss: 0.00001737
Iteration 7/1000 | Loss: 0.00001690
Iteration 8/1000 | Loss: 0.00001656
Iteration 9/1000 | Loss: 0.00001626
Iteration 10/1000 | Loss: 0.00001614
Iteration 11/1000 | Loss: 0.00001608
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001599
Iteration 14/1000 | Loss: 0.00001588
Iteration 15/1000 | Loss: 0.00001588
Iteration 16/1000 | Loss: 0.00001583
Iteration 17/1000 | Loss: 0.00001582
Iteration 18/1000 | Loss: 0.00001582
Iteration 19/1000 | Loss: 0.00001579
Iteration 20/1000 | Loss: 0.00001576
Iteration 21/1000 | Loss: 0.00001576
Iteration 22/1000 | Loss: 0.00001573
Iteration 23/1000 | Loss: 0.00001573
Iteration 24/1000 | Loss: 0.00001571
Iteration 25/1000 | Loss: 0.00001571
Iteration 26/1000 | Loss: 0.00001571
Iteration 27/1000 | Loss: 0.00001570
Iteration 28/1000 | Loss: 0.00001570
Iteration 29/1000 | Loss: 0.00001569
Iteration 30/1000 | Loss: 0.00001569
Iteration 31/1000 | Loss: 0.00001568
Iteration 32/1000 | Loss: 0.00001568
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001567
Iteration 35/1000 | Loss: 0.00001566
Iteration 36/1000 | Loss: 0.00001566
Iteration 37/1000 | Loss: 0.00001565
Iteration 38/1000 | Loss: 0.00001565
Iteration 39/1000 | Loss: 0.00001565
Iteration 40/1000 | Loss: 0.00001564
Iteration 41/1000 | Loss: 0.00001564
Iteration 42/1000 | Loss: 0.00001564
Iteration 43/1000 | Loss: 0.00001564
Iteration 44/1000 | Loss: 0.00001564
Iteration 45/1000 | Loss: 0.00001563
Iteration 46/1000 | Loss: 0.00001562
Iteration 47/1000 | Loss: 0.00001562
Iteration 48/1000 | Loss: 0.00001562
Iteration 49/1000 | Loss: 0.00001561
Iteration 50/1000 | Loss: 0.00001561
Iteration 51/1000 | Loss: 0.00001560
Iteration 52/1000 | Loss: 0.00001560
Iteration 53/1000 | Loss: 0.00001560
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001560
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001559
Iteration 59/1000 | Loss: 0.00001558
Iteration 60/1000 | Loss: 0.00001558
Iteration 61/1000 | Loss: 0.00001558
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001556
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001555
Iteration 67/1000 | Loss: 0.00001555
Iteration 68/1000 | Loss: 0.00001555
Iteration 69/1000 | Loss: 0.00001554
Iteration 70/1000 | Loss: 0.00001554
Iteration 71/1000 | Loss: 0.00001554
Iteration 72/1000 | Loss: 0.00001554
Iteration 73/1000 | Loss: 0.00001553
Iteration 74/1000 | Loss: 0.00001553
Iteration 75/1000 | Loss: 0.00001553
Iteration 76/1000 | Loss: 0.00001552
Iteration 77/1000 | Loss: 0.00001552
Iteration 78/1000 | Loss: 0.00001552
Iteration 79/1000 | Loss: 0.00001551
Iteration 80/1000 | Loss: 0.00001551
Iteration 81/1000 | Loss: 0.00001551
Iteration 82/1000 | Loss: 0.00001550
Iteration 83/1000 | Loss: 0.00001550
Iteration 84/1000 | Loss: 0.00001550
Iteration 85/1000 | Loss: 0.00001550
Iteration 86/1000 | Loss: 0.00001549
Iteration 87/1000 | Loss: 0.00001549
Iteration 88/1000 | Loss: 0.00001549
Iteration 89/1000 | Loss: 0.00001549
Iteration 90/1000 | Loss: 0.00001549
Iteration 91/1000 | Loss: 0.00001549
Iteration 92/1000 | Loss: 0.00001549
Iteration 93/1000 | Loss: 0.00001548
Iteration 94/1000 | Loss: 0.00001548
Iteration 95/1000 | Loss: 0.00001548
Iteration 96/1000 | Loss: 0.00001548
Iteration 97/1000 | Loss: 0.00001548
Iteration 98/1000 | Loss: 0.00001548
Iteration 99/1000 | Loss: 0.00001548
Iteration 100/1000 | Loss: 0.00001547
Iteration 101/1000 | Loss: 0.00001547
Iteration 102/1000 | Loss: 0.00001547
Iteration 103/1000 | Loss: 0.00001547
Iteration 104/1000 | Loss: 0.00001546
Iteration 105/1000 | Loss: 0.00001546
Iteration 106/1000 | Loss: 0.00001546
Iteration 107/1000 | Loss: 0.00001546
Iteration 108/1000 | Loss: 0.00001546
Iteration 109/1000 | Loss: 0.00001546
Iteration 110/1000 | Loss: 0.00001546
Iteration 111/1000 | Loss: 0.00001546
Iteration 112/1000 | Loss: 0.00001546
Iteration 113/1000 | Loss: 0.00001546
Iteration 114/1000 | Loss: 0.00001545
Iteration 115/1000 | Loss: 0.00001545
Iteration 116/1000 | Loss: 0.00001545
Iteration 117/1000 | Loss: 0.00001545
Iteration 118/1000 | Loss: 0.00001545
Iteration 119/1000 | Loss: 0.00001545
Iteration 120/1000 | Loss: 0.00001545
Iteration 121/1000 | Loss: 0.00001545
Iteration 122/1000 | Loss: 0.00001545
Iteration 123/1000 | Loss: 0.00001545
Iteration 124/1000 | Loss: 0.00001544
Iteration 125/1000 | Loss: 0.00001544
Iteration 126/1000 | Loss: 0.00001544
Iteration 127/1000 | Loss: 0.00001544
Iteration 128/1000 | Loss: 0.00001544
Iteration 129/1000 | Loss: 0.00001544
Iteration 130/1000 | Loss: 0.00001544
Iteration 131/1000 | Loss: 0.00001544
Iteration 132/1000 | Loss: 0.00001544
Iteration 133/1000 | Loss: 0.00001544
Iteration 134/1000 | Loss: 0.00001544
Iteration 135/1000 | Loss: 0.00001544
Iteration 136/1000 | Loss: 0.00001544
Iteration 137/1000 | Loss: 0.00001544
Iteration 138/1000 | Loss: 0.00001544
Iteration 139/1000 | Loss: 0.00001543
Iteration 140/1000 | Loss: 0.00001543
Iteration 141/1000 | Loss: 0.00001543
Iteration 142/1000 | Loss: 0.00001543
Iteration 143/1000 | Loss: 0.00001543
Iteration 144/1000 | Loss: 0.00001543
Iteration 145/1000 | Loss: 0.00001543
Iteration 146/1000 | Loss: 0.00001543
Iteration 147/1000 | Loss: 0.00001543
Iteration 148/1000 | Loss: 0.00001543
Iteration 149/1000 | Loss: 0.00001542
Iteration 150/1000 | Loss: 0.00001542
Iteration 151/1000 | Loss: 0.00001542
Iteration 152/1000 | Loss: 0.00001542
Iteration 153/1000 | Loss: 0.00001542
Iteration 154/1000 | Loss: 0.00001542
Iteration 155/1000 | Loss: 0.00001542
Iteration 156/1000 | Loss: 0.00001542
Iteration 157/1000 | Loss: 0.00001542
Iteration 158/1000 | Loss: 0.00001542
Iteration 159/1000 | Loss: 0.00001542
Iteration 160/1000 | Loss: 0.00001542
Iteration 161/1000 | Loss: 0.00001542
Iteration 162/1000 | Loss: 0.00001542
Iteration 163/1000 | Loss: 0.00001542
Iteration 164/1000 | Loss: 0.00001542
Iteration 165/1000 | Loss: 0.00001542
Iteration 166/1000 | Loss: 0.00001542
Iteration 167/1000 | Loss: 0.00001542
Iteration 168/1000 | Loss: 0.00001541
Iteration 169/1000 | Loss: 0.00001541
Iteration 170/1000 | Loss: 0.00001541
Iteration 171/1000 | Loss: 0.00001541
Iteration 172/1000 | Loss: 0.00001541
Iteration 173/1000 | Loss: 0.00001541
Iteration 174/1000 | Loss: 0.00001541
Iteration 175/1000 | Loss: 0.00001541
Iteration 176/1000 | Loss: 0.00001541
Iteration 177/1000 | Loss: 0.00001541
Iteration 178/1000 | Loss: 0.00001541
Iteration 179/1000 | Loss: 0.00001541
Iteration 180/1000 | Loss: 0.00001541
Iteration 181/1000 | Loss: 0.00001541
Iteration 182/1000 | Loss: 0.00001541
Iteration 183/1000 | Loss: 0.00001541
Iteration 184/1000 | Loss: 0.00001541
Iteration 185/1000 | Loss: 0.00001540
Iteration 186/1000 | Loss: 0.00001540
Iteration 187/1000 | Loss: 0.00001540
Iteration 188/1000 | Loss: 0.00001540
Iteration 189/1000 | Loss: 0.00001540
Iteration 190/1000 | Loss: 0.00001540
Iteration 191/1000 | Loss: 0.00001540
Iteration 192/1000 | Loss: 0.00001540
Iteration 193/1000 | Loss: 0.00001540
Iteration 194/1000 | Loss: 0.00001540
Iteration 195/1000 | Loss: 0.00001540
Iteration 196/1000 | Loss: 0.00001540
Iteration 197/1000 | Loss: 0.00001540
Iteration 198/1000 | Loss: 0.00001540
Iteration 199/1000 | Loss: 0.00001540
Iteration 200/1000 | Loss: 0.00001540
Iteration 201/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.5399760741274804e-05, 1.5399760741274804e-05, 1.5399760741274804e-05, 1.5399760741274804e-05, 1.5399760741274804e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5399760741274804e-05

Optimization complete. Final v2v error: 3.2728846073150635 mm

Highest mean error: 3.5005085468292236 mm for frame 165

Lowest mean error: 2.9754860401153564 mm for frame 150

Saving results

Total time: 44.321826219558716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847225
Iteration 2/25 | Loss: 0.00122169
Iteration 3/25 | Loss: 0.00080138
Iteration 4/25 | Loss: 0.00073754
Iteration 5/25 | Loss: 0.00073028
Iteration 6/25 | Loss: 0.00072830
Iteration 7/25 | Loss: 0.00072758
Iteration 8/25 | Loss: 0.00072737
Iteration 9/25 | Loss: 0.00072737
Iteration 10/25 | Loss: 0.00072737
Iteration 11/25 | Loss: 0.00072737
Iteration 12/25 | Loss: 0.00072737
Iteration 13/25 | Loss: 0.00072737
Iteration 14/25 | Loss: 0.00072737
Iteration 15/25 | Loss: 0.00072737
Iteration 16/25 | Loss: 0.00072737
Iteration 17/25 | Loss: 0.00072737
Iteration 18/25 | Loss: 0.00072737
Iteration 19/25 | Loss: 0.00072737
Iteration 20/25 | Loss: 0.00072737
Iteration 21/25 | Loss: 0.00072737
Iteration 22/25 | Loss: 0.00072737
Iteration 23/25 | Loss: 0.00072737
Iteration 24/25 | Loss: 0.00072737
Iteration 25/25 | Loss: 0.00072737

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55392289
Iteration 2/25 | Loss: 0.00086264
Iteration 3/25 | Loss: 0.00086264
Iteration 4/25 | Loss: 0.00086264
Iteration 5/25 | Loss: 0.00086264
Iteration 6/25 | Loss: 0.00086264
Iteration 7/25 | Loss: 0.00086264
Iteration 8/25 | Loss: 0.00086264
Iteration 9/25 | Loss: 0.00086264
Iteration 10/25 | Loss: 0.00086264
Iteration 11/25 | Loss: 0.00086264
Iteration 12/25 | Loss: 0.00086264
Iteration 13/25 | Loss: 0.00086264
Iteration 14/25 | Loss: 0.00086264
Iteration 15/25 | Loss: 0.00086264
Iteration 16/25 | Loss: 0.00086264
Iteration 17/25 | Loss: 0.00086264
Iteration 18/25 | Loss: 0.00086264
Iteration 19/25 | Loss: 0.00086264
Iteration 20/25 | Loss: 0.00086264
Iteration 21/25 | Loss: 0.00086264
Iteration 22/25 | Loss: 0.00086264
Iteration 23/25 | Loss: 0.00086264
Iteration 24/25 | Loss: 0.00086264
Iteration 25/25 | Loss: 0.00086264

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086264
Iteration 2/1000 | Loss: 0.00002143
Iteration 3/1000 | Loss: 0.00001447
Iteration 4/1000 | Loss: 0.00001312
Iteration 5/1000 | Loss: 0.00001251
Iteration 6/1000 | Loss: 0.00001201
Iteration 7/1000 | Loss: 0.00001175
Iteration 8/1000 | Loss: 0.00001155
Iteration 9/1000 | Loss: 0.00001145
Iteration 10/1000 | Loss: 0.00001142
Iteration 11/1000 | Loss: 0.00001138
Iteration 12/1000 | Loss: 0.00001132
Iteration 13/1000 | Loss: 0.00001131
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001127
Iteration 16/1000 | Loss: 0.00001124
Iteration 17/1000 | Loss: 0.00001122
Iteration 18/1000 | Loss: 0.00001121
Iteration 19/1000 | Loss: 0.00001120
Iteration 20/1000 | Loss: 0.00001120
Iteration 21/1000 | Loss: 0.00001119
Iteration 22/1000 | Loss: 0.00001119
Iteration 23/1000 | Loss: 0.00001118
Iteration 24/1000 | Loss: 0.00001118
Iteration 25/1000 | Loss: 0.00001117
Iteration 26/1000 | Loss: 0.00001117
Iteration 27/1000 | Loss: 0.00001117
Iteration 28/1000 | Loss: 0.00001117
Iteration 29/1000 | Loss: 0.00001116
Iteration 30/1000 | Loss: 0.00001116
Iteration 31/1000 | Loss: 0.00001115
Iteration 32/1000 | Loss: 0.00001115
Iteration 33/1000 | Loss: 0.00001114
Iteration 34/1000 | Loss: 0.00001114
Iteration 35/1000 | Loss: 0.00001114
Iteration 36/1000 | Loss: 0.00001113
Iteration 37/1000 | Loss: 0.00001113
Iteration 38/1000 | Loss: 0.00001112
Iteration 39/1000 | Loss: 0.00001112
Iteration 40/1000 | Loss: 0.00001111
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001110
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001110
Iteration 46/1000 | Loss: 0.00001110
Iteration 47/1000 | Loss: 0.00001110
Iteration 48/1000 | Loss: 0.00001110
Iteration 49/1000 | Loss: 0.00001110
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001108
Iteration 53/1000 | Loss: 0.00001108
Iteration 54/1000 | Loss: 0.00001108
Iteration 55/1000 | Loss: 0.00001108
Iteration 56/1000 | Loss: 0.00001108
Iteration 57/1000 | Loss: 0.00001108
Iteration 58/1000 | Loss: 0.00001107
Iteration 59/1000 | Loss: 0.00001107
Iteration 60/1000 | Loss: 0.00001107
Iteration 61/1000 | Loss: 0.00001107
Iteration 62/1000 | Loss: 0.00001107
Iteration 63/1000 | Loss: 0.00001107
Iteration 64/1000 | Loss: 0.00001107
Iteration 65/1000 | Loss: 0.00001106
Iteration 66/1000 | Loss: 0.00001106
Iteration 67/1000 | Loss: 0.00001106
Iteration 68/1000 | Loss: 0.00001106
Iteration 69/1000 | Loss: 0.00001106
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001106
Iteration 72/1000 | Loss: 0.00001105
Iteration 73/1000 | Loss: 0.00001105
Iteration 74/1000 | Loss: 0.00001105
Iteration 75/1000 | Loss: 0.00001105
Iteration 76/1000 | Loss: 0.00001105
Iteration 77/1000 | Loss: 0.00001105
Iteration 78/1000 | Loss: 0.00001105
Iteration 79/1000 | Loss: 0.00001105
Iteration 80/1000 | Loss: 0.00001105
Iteration 81/1000 | Loss: 0.00001105
Iteration 82/1000 | Loss: 0.00001105
Iteration 83/1000 | Loss: 0.00001105
Iteration 84/1000 | Loss: 0.00001105
Iteration 85/1000 | Loss: 0.00001105
Iteration 86/1000 | Loss: 0.00001105
Iteration 87/1000 | Loss: 0.00001105
Iteration 88/1000 | Loss: 0.00001105
Iteration 89/1000 | Loss: 0.00001105
Iteration 90/1000 | Loss: 0.00001105
Iteration 91/1000 | Loss: 0.00001105
Iteration 92/1000 | Loss: 0.00001105
Iteration 93/1000 | Loss: 0.00001104
Iteration 94/1000 | Loss: 0.00001104
Iteration 95/1000 | Loss: 0.00001104
Iteration 96/1000 | Loss: 0.00001104
Iteration 97/1000 | Loss: 0.00001104
Iteration 98/1000 | Loss: 0.00001104
Iteration 99/1000 | Loss: 0.00001104
Iteration 100/1000 | Loss: 0.00001104
Iteration 101/1000 | Loss: 0.00001104
Iteration 102/1000 | Loss: 0.00001104
Iteration 103/1000 | Loss: 0.00001104
Iteration 104/1000 | Loss: 0.00001104
Iteration 105/1000 | Loss: 0.00001104
Iteration 106/1000 | Loss: 0.00001104
Iteration 107/1000 | Loss: 0.00001104
Iteration 108/1000 | Loss: 0.00001104
Iteration 109/1000 | Loss: 0.00001104
Iteration 110/1000 | Loss: 0.00001104
Iteration 111/1000 | Loss: 0.00001104
Iteration 112/1000 | Loss: 0.00001104
Iteration 113/1000 | Loss: 0.00001104
Iteration 114/1000 | Loss: 0.00001104
Iteration 115/1000 | Loss: 0.00001104
Iteration 116/1000 | Loss: 0.00001104
Iteration 117/1000 | Loss: 0.00001103
Iteration 118/1000 | Loss: 0.00001103
Iteration 119/1000 | Loss: 0.00001103
Iteration 120/1000 | Loss: 0.00001103
Iteration 121/1000 | Loss: 0.00001103
Iteration 122/1000 | Loss: 0.00001103
Iteration 123/1000 | Loss: 0.00001103
Iteration 124/1000 | Loss: 0.00001103
Iteration 125/1000 | Loss: 0.00001103
Iteration 126/1000 | Loss: 0.00001103
Iteration 127/1000 | Loss: 0.00001103
Iteration 128/1000 | Loss: 0.00001103
Iteration 129/1000 | Loss: 0.00001103
Iteration 130/1000 | Loss: 0.00001103
Iteration 131/1000 | Loss: 0.00001102
Iteration 132/1000 | Loss: 0.00001102
Iteration 133/1000 | Loss: 0.00001102
Iteration 134/1000 | Loss: 0.00001102
Iteration 135/1000 | Loss: 0.00001102
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001102
Iteration 138/1000 | Loss: 0.00001102
Iteration 139/1000 | Loss: 0.00001102
Iteration 140/1000 | Loss: 0.00001102
Iteration 141/1000 | Loss: 0.00001102
Iteration 142/1000 | Loss: 0.00001102
Iteration 143/1000 | Loss: 0.00001102
Iteration 144/1000 | Loss: 0.00001102
Iteration 145/1000 | Loss: 0.00001102
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001101
Iteration 150/1000 | Loss: 0.00001101
Iteration 151/1000 | Loss: 0.00001101
Iteration 152/1000 | Loss: 0.00001101
Iteration 153/1000 | Loss: 0.00001101
Iteration 154/1000 | Loss: 0.00001101
Iteration 155/1000 | Loss: 0.00001101
Iteration 156/1000 | Loss: 0.00001101
Iteration 157/1000 | Loss: 0.00001101
Iteration 158/1000 | Loss: 0.00001101
Iteration 159/1000 | Loss: 0.00001101
Iteration 160/1000 | Loss: 0.00001101
Iteration 161/1000 | Loss: 0.00001101
Iteration 162/1000 | Loss: 0.00001101
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001101
Iteration 168/1000 | Loss: 0.00001101
Iteration 169/1000 | Loss: 0.00001101
Iteration 170/1000 | Loss: 0.00001101
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001101
Iteration 174/1000 | Loss: 0.00001101
Iteration 175/1000 | Loss: 0.00001101
Iteration 176/1000 | Loss: 0.00001101
Iteration 177/1000 | Loss: 0.00001101
Iteration 178/1000 | Loss: 0.00001101
Iteration 179/1000 | Loss: 0.00001101
Iteration 180/1000 | Loss: 0.00001101
Iteration 181/1000 | Loss: 0.00001101
Iteration 182/1000 | Loss: 0.00001101
Iteration 183/1000 | Loss: 0.00001101
Iteration 184/1000 | Loss: 0.00001101
Iteration 185/1000 | Loss: 0.00001101
Iteration 186/1000 | Loss: 0.00001101
Iteration 187/1000 | Loss: 0.00001101
Iteration 188/1000 | Loss: 0.00001101
Iteration 189/1000 | Loss: 0.00001101
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.1006402928614989e-05, 1.1006402928614989e-05, 1.1006402928614989e-05, 1.1006402928614989e-05, 1.1006402928614989e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1006402928614989e-05

Optimization complete. Final v2v error: 2.798401355743408 mm

Highest mean error: 3.0739798545837402 mm for frame 56

Lowest mean error: 2.5913901329040527 mm for frame 4

Saving results

Total time: 38.767822265625
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01015189
Iteration 2/25 | Loss: 0.00153144
Iteration 3/25 | Loss: 0.00097016
Iteration 4/25 | Loss: 0.00087312
Iteration 5/25 | Loss: 0.00083996
Iteration 6/25 | Loss: 0.00083562
Iteration 7/25 | Loss: 0.00083516
Iteration 8/25 | Loss: 0.00083516
Iteration 9/25 | Loss: 0.00083516
Iteration 10/25 | Loss: 0.00083516
Iteration 11/25 | Loss: 0.00083516
Iteration 12/25 | Loss: 0.00083516
Iteration 13/25 | Loss: 0.00083516
Iteration 14/25 | Loss: 0.00083516
Iteration 15/25 | Loss: 0.00083516
Iteration 16/25 | Loss: 0.00083516
Iteration 17/25 | Loss: 0.00083516
Iteration 18/25 | Loss: 0.00083516
Iteration 19/25 | Loss: 0.00083516
Iteration 20/25 | Loss: 0.00083516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008351551950909197, 0.0008351551950909197, 0.0008351551950909197, 0.0008351551950909197, 0.0008351551950909197]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008351551950909197

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.17275977
Iteration 2/25 | Loss: 0.00093744
Iteration 3/25 | Loss: 0.00093743
Iteration 4/25 | Loss: 0.00093742
Iteration 5/25 | Loss: 0.00093742
Iteration 6/25 | Loss: 0.00093742
Iteration 7/25 | Loss: 0.00093742
Iteration 8/25 | Loss: 0.00093742
Iteration 9/25 | Loss: 0.00093742
Iteration 10/25 | Loss: 0.00093742
Iteration 11/25 | Loss: 0.00093742
Iteration 12/25 | Loss: 0.00093742
Iteration 13/25 | Loss: 0.00093742
Iteration 14/25 | Loss: 0.00093742
Iteration 15/25 | Loss: 0.00093742
Iteration 16/25 | Loss: 0.00093742
Iteration 17/25 | Loss: 0.00093742
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009374225046485662, 0.0009374225046485662, 0.0009374225046485662, 0.0009374225046485662, 0.0009374225046485662]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009374225046485662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093742
Iteration 2/1000 | Loss: 0.00002904
Iteration 3/1000 | Loss: 0.00002307
Iteration 4/1000 | Loss: 0.00002130
Iteration 5/1000 | Loss: 0.00001991
Iteration 6/1000 | Loss: 0.00001905
Iteration 7/1000 | Loss: 0.00001852
Iteration 8/1000 | Loss: 0.00001819
Iteration 9/1000 | Loss: 0.00001795
Iteration 10/1000 | Loss: 0.00001775
Iteration 11/1000 | Loss: 0.00001763
Iteration 12/1000 | Loss: 0.00001761
Iteration 13/1000 | Loss: 0.00001756
Iteration 14/1000 | Loss: 0.00001755
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001752
Iteration 17/1000 | Loss: 0.00001752
Iteration 18/1000 | Loss: 0.00001751
Iteration 19/1000 | Loss: 0.00001751
Iteration 20/1000 | Loss: 0.00001750
Iteration 21/1000 | Loss: 0.00001750
Iteration 22/1000 | Loss: 0.00001750
Iteration 23/1000 | Loss: 0.00001750
Iteration 24/1000 | Loss: 0.00001749
Iteration 25/1000 | Loss: 0.00001749
Iteration 26/1000 | Loss: 0.00001749
Iteration 27/1000 | Loss: 0.00001749
Iteration 28/1000 | Loss: 0.00001748
Iteration 29/1000 | Loss: 0.00001747
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001746
Iteration 32/1000 | Loss: 0.00001745
Iteration 33/1000 | Loss: 0.00001744
Iteration 34/1000 | Loss: 0.00001744
Iteration 35/1000 | Loss: 0.00001744
Iteration 36/1000 | Loss: 0.00001743
Iteration 37/1000 | Loss: 0.00001741
Iteration 38/1000 | Loss: 0.00001741
Iteration 39/1000 | Loss: 0.00001741
Iteration 40/1000 | Loss: 0.00001741
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001740
Iteration 44/1000 | Loss: 0.00001740
Iteration 45/1000 | Loss: 0.00001740
Iteration 46/1000 | Loss: 0.00001740
Iteration 47/1000 | Loss: 0.00001740
Iteration 48/1000 | Loss: 0.00001740
Iteration 49/1000 | Loss: 0.00001740
Iteration 50/1000 | Loss: 0.00001740
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00001739
Iteration 54/1000 | Loss: 0.00001739
Iteration 55/1000 | Loss: 0.00001739
Iteration 56/1000 | Loss: 0.00001739
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001738
Iteration 59/1000 | Loss: 0.00001738
Iteration 60/1000 | Loss: 0.00001737
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001737
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001736
Iteration 69/1000 | Loss: 0.00001736
Iteration 70/1000 | Loss: 0.00001736
Iteration 71/1000 | Loss: 0.00001736
Iteration 72/1000 | Loss: 0.00001736
Iteration 73/1000 | Loss: 0.00001735
Iteration 74/1000 | Loss: 0.00001735
Iteration 75/1000 | Loss: 0.00001735
Iteration 76/1000 | Loss: 0.00001735
Iteration 77/1000 | Loss: 0.00001735
Iteration 78/1000 | Loss: 0.00001735
Iteration 79/1000 | Loss: 0.00001735
Iteration 80/1000 | Loss: 0.00001735
Iteration 81/1000 | Loss: 0.00001735
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [1.735458499751985e-05, 1.735458499751985e-05, 1.735458499751985e-05, 1.735458499751985e-05, 1.735458499751985e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.735458499751985e-05

Optimization complete. Final v2v error: 3.512094736099243 mm

Highest mean error: 3.7936007976531982 mm for frame 31

Lowest mean error: 3.3617966175079346 mm for frame 84

Saving results

Total time: 31.708613395690918
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966099
Iteration 2/25 | Loss: 0.00167464
Iteration 3/25 | Loss: 0.00111728
Iteration 4/25 | Loss: 0.00104536
Iteration 5/25 | Loss: 0.00103451
Iteration 6/25 | Loss: 0.00103226
Iteration 7/25 | Loss: 0.00103215
Iteration 8/25 | Loss: 0.00103215
Iteration 9/25 | Loss: 0.00103215
Iteration 10/25 | Loss: 0.00103215
Iteration 11/25 | Loss: 0.00103215
Iteration 12/25 | Loss: 0.00103215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010321520967409015, 0.0010321520967409015, 0.0010321520967409015, 0.0010321520967409015, 0.0010321520967409015]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010321520967409015

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85384023
Iteration 2/25 | Loss: 0.00083301
Iteration 3/25 | Loss: 0.00083301
Iteration 4/25 | Loss: 0.00083301
Iteration 5/25 | Loss: 0.00083301
Iteration 6/25 | Loss: 0.00083301
Iteration 7/25 | Loss: 0.00083301
Iteration 8/25 | Loss: 0.00083301
Iteration 9/25 | Loss: 0.00083301
Iteration 10/25 | Loss: 0.00083300
Iteration 11/25 | Loss: 0.00083300
Iteration 12/25 | Loss: 0.00083300
Iteration 13/25 | Loss: 0.00083300
Iteration 14/25 | Loss: 0.00083300
Iteration 15/25 | Loss: 0.00083300
Iteration 16/25 | Loss: 0.00083300
Iteration 17/25 | Loss: 0.00083300
Iteration 18/25 | Loss: 0.00083300
Iteration 19/25 | Loss: 0.00083300
Iteration 20/25 | Loss: 0.00083300
Iteration 21/25 | Loss: 0.00083300
Iteration 22/25 | Loss: 0.00083300
Iteration 23/25 | Loss: 0.00083300
Iteration 24/25 | Loss: 0.00083300
Iteration 25/25 | Loss: 0.00083300

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083300
Iteration 2/1000 | Loss: 0.00006205
Iteration 3/1000 | Loss: 0.00004381
Iteration 4/1000 | Loss: 0.00003999
Iteration 5/1000 | Loss: 0.00003842
Iteration 6/1000 | Loss: 0.00003735
Iteration 7/1000 | Loss: 0.00003689
Iteration 8/1000 | Loss: 0.00003651
Iteration 9/1000 | Loss: 0.00003624
Iteration 10/1000 | Loss: 0.00003609
Iteration 11/1000 | Loss: 0.00003594
Iteration 12/1000 | Loss: 0.00003591
Iteration 13/1000 | Loss: 0.00003590
Iteration 14/1000 | Loss: 0.00003589
Iteration 15/1000 | Loss: 0.00003588
Iteration 16/1000 | Loss: 0.00003588
Iteration 17/1000 | Loss: 0.00003584
Iteration 18/1000 | Loss: 0.00003576
Iteration 19/1000 | Loss: 0.00003562
Iteration 20/1000 | Loss: 0.00003553
Iteration 21/1000 | Loss: 0.00003551
Iteration 22/1000 | Loss: 0.00003551
Iteration 23/1000 | Loss: 0.00003549
Iteration 24/1000 | Loss: 0.00003549
Iteration 25/1000 | Loss: 0.00003549
Iteration 26/1000 | Loss: 0.00003549
Iteration 27/1000 | Loss: 0.00003549
Iteration 28/1000 | Loss: 0.00003548
Iteration 29/1000 | Loss: 0.00003548
Iteration 30/1000 | Loss: 0.00003548
Iteration 31/1000 | Loss: 0.00003548
Iteration 32/1000 | Loss: 0.00003548
Iteration 33/1000 | Loss: 0.00003548
Iteration 34/1000 | Loss: 0.00003548
Iteration 35/1000 | Loss: 0.00003548
Iteration 36/1000 | Loss: 0.00003548
Iteration 37/1000 | Loss: 0.00003548
Iteration 38/1000 | Loss: 0.00003548
Iteration 39/1000 | Loss: 0.00003546
Iteration 40/1000 | Loss: 0.00003546
Iteration 41/1000 | Loss: 0.00003546
Iteration 42/1000 | Loss: 0.00003546
Iteration 43/1000 | Loss: 0.00003546
Iteration 44/1000 | Loss: 0.00003545
Iteration 45/1000 | Loss: 0.00003545
Iteration 46/1000 | Loss: 0.00003545
Iteration 47/1000 | Loss: 0.00003545
Iteration 48/1000 | Loss: 0.00003544
Iteration 49/1000 | Loss: 0.00003544
Iteration 50/1000 | Loss: 0.00003539
Iteration 51/1000 | Loss: 0.00003537
Iteration 52/1000 | Loss: 0.00003537
Iteration 53/1000 | Loss: 0.00003537
Iteration 54/1000 | Loss: 0.00003536
Iteration 55/1000 | Loss: 0.00003536
Iteration 56/1000 | Loss: 0.00003536
Iteration 57/1000 | Loss: 0.00003536
Iteration 58/1000 | Loss: 0.00003536
Iteration 59/1000 | Loss: 0.00003535
Iteration 60/1000 | Loss: 0.00003535
Iteration 61/1000 | Loss: 0.00003535
Iteration 62/1000 | Loss: 0.00003535
Iteration 63/1000 | Loss: 0.00003535
Iteration 64/1000 | Loss: 0.00003535
Iteration 65/1000 | Loss: 0.00003535
Iteration 66/1000 | Loss: 0.00003534
Iteration 67/1000 | Loss: 0.00003534
Iteration 68/1000 | Loss: 0.00003534
Iteration 69/1000 | Loss: 0.00003532
Iteration 70/1000 | Loss: 0.00003532
Iteration 71/1000 | Loss: 0.00003532
Iteration 72/1000 | Loss: 0.00003532
Iteration 73/1000 | Loss: 0.00003532
Iteration 74/1000 | Loss: 0.00003532
Iteration 75/1000 | Loss: 0.00003531
Iteration 76/1000 | Loss: 0.00003531
Iteration 77/1000 | Loss: 0.00003531
Iteration 78/1000 | Loss: 0.00003531
Iteration 79/1000 | Loss: 0.00003531
Iteration 80/1000 | Loss: 0.00003531
Iteration 81/1000 | Loss: 0.00003531
Iteration 82/1000 | Loss: 0.00003531
Iteration 83/1000 | Loss: 0.00003530
Iteration 84/1000 | Loss: 0.00003530
Iteration 85/1000 | Loss: 0.00003529
Iteration 86/1000 | Loss: 0.00003529
Iteration 87/1000 | Loss: 0.00003529
Iteration 88/1000 | Loss: 0.00003529
Iteration 89/1000 | Loss: 0.00003529
Iteration 90/1000 | Loss: 0.00003529
Iteration 91/1000 | Loss: 0.00003529
Iteration 92/1000 | Loss: 0.00003529
Iteration 93/1000 | Loss: 0.00003528
Iteration 94/1000 | Loss: 0.00003528
Iteration 95/1000 | Loss: 0.00003528
Iteration 96/1000 | Loss: 0.00003528
Iteration 97/1000 | Loss: 0.00003527
Iteration 98/1000 | Loss: 0.00003527
Iteration 99/1000 | Loss: 0.00003527
Iteration 100/1000 | Loss: 0.00003527
Iteration 101/1000 | Loss: 0.00003527
Iteration 102/1000 | Loss: 0.00003527
Iteration 103/1000 | Loss: 0.00003527
Iteration 104/1000 | Loss: 0.00003527
Iteration 105/1000 | Loss: 0.00003527
Iteration 106/1000 | Loss: 0.00003527
Iteration 107/1000 | Loss: 0.00003526
Iteration 108/1000 | Loss: 0.00003526
Iteration 109/1000 | Loss: 0.00003526
Iteration 110/1000 | Loss: 0.00003526
Iteration 111/1000 | Loss: 0.00003526
Iteration 112/1000 | Loss: 0.00003526
Iteration 113/1000 | Loss: 0.00003525
Iteration 114/1000 | Loss: 0.00003525
Iteration 115/1000 | Loss: 0.00003525
Iteration 116/1000 | Loss: 0.00003525
Iteration 117/1000 | Loss: 0.00003525
Iteration 118/1000 | Loss: 0.00003525
Iteration 119/1000 | Loss: 0.00003525
Iteration 120/1000 | Loss: 0.00003524
Iteration 121/1000 | Loss: 0.00003524
Iteration 122/1000 | Loss: 0.00003524
Iteration 123/1000 | Loss: 0.00003524
Iteration 124/1000 | Loss: 0.00003523
Iteration 125/1000 | Loss: 0.00003523
Iteration 126/1000 | Loss: 0.00003523
Iteration 127/1000 | Loss: 0.00003523
Iteration 128/1000 | Loss: 0.00003523
Iteration 129/1000 | Loss: 0.00003522
Iteration 130/1000 | Loss: 0.00003522
Iteration 131/1000 | Loss: 0.00003521
Iteration 132/1000 | Loss: 0.00003521
Iteration 133/1000 | Loss: 0.00003521
Iteration 134/1000 | Loss: 0.00003521
Iteration 135/1000 | Loss: 0.00003521
Iteration 136/1000 | Loss: 0.00003521
Iteration 137/1000 | Loss: 0.00003520
Iteration 138/1000 | Loss: 0.00003520
Iteration 139/1000 | Loss: 0.00003520
Iteration 140/1000 | Loss: 0.00003520
Iteration 141/1000 | Loss: 0.00003520
Iteration 142/1000 | Loss: 0.00003520
Iteration 143/1000 | Loss: 0.00003520
Iteration 144/1000 | Loss: 0.00003520
Iteration 145/1000 | Loss: 0.00003520
Iteration 146/1000 | Loss: 0.00003520
Iteration 147/1000 | Loss: 0.00003519
Iteration 148/1000 | Loss: 0.00003519
Iteration 149/1000 | Loss: 0.00003519
Iteration 150/1000 | Loss: 0.00003519
Iteration 151/1000 | Loss: 0.00003519
Iteration 152/1000 | Loss: 0.00003519
Iteration 153/1000 | Loss: 0.00003519
Iteration 154/1000 | Loss: 0.00003519
Iteration 155/1000 | Loss: 0.00003519
Iteration 156/1000 | Loss: 0.00003518
Iteration 157/1000 | Loss: 0.00003518
Iteration 158/1000 | Loss: 0.00003518
Iteration 159/1000 | Loss: 0.00003518
Iteration 160/1000 | Loss: 0.00003518
Iteration 161/1000 | Loss: 0.00003518
Iteration 162/1000 | Loss: 0.00003518
Iteration 163/1000 | Loss: 0.00003518
Iteration 164/1000 | Loss: 0.00003518
Iteration 165/1000 | Loss: 0.00003518
Iteration 166/1000 | Loss: 0.00003518
Iteration 167/1000 | Loss: 0.00003518
Iteration 168/1000 | Loss: 0.00003518
Iteration 169/1000 | Loss: 0.00003518
Iteration 170/1000 | Loss: 0.00003518
Iteration 171/1000 | Loss: 0.00003518
Iteration 172/1000 | Loss: 0.00003518
Iteration 173/1000 | Loss: 0.00003518
Iteration 174/1000 | Loss: 0.00003518
Iteration 175/1000 | Loss: 0.00003518
Iteration 176/1000 | Loss: 0.00003518
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [3.5182867577532306e-05, 3.5182867577532306e-05, 3.5182867577532306e-05, 3.5182867577532306e-05, 3.5182867577532306e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5182867577532306e-05

Optimization complete. Final v2v error: 4.9306206703186035 mm

Highest mean error: 5.3406758308410645 mm for frame 30

Lowest mean error: 4.513110160827637 mm for frame 0

Saving results

Total time: 42.36185264587402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00769282
Iteration 2/25 | Loss: 0.00210487
Iteration 3/25 | Loss: 0.00146064
Iteration 4/25 | Loss: 0.00136726
Iteration 5/25 | Loss: 0.00126142
Iteration 6/25 | Loss: 0.00118732
Iteration 7/25 | Loss: 0.00111570
Iteration 8/25 | Loss: 0.00109619
Iteration 9/25 | Loss: 0.00108742
Iteration 10/25 | Loss: 0.00108623
Iteration 11/25 | Loss: 0.00108559
Iteration 12/25 | Loss: 0.00108358
Iteration 13/25 | Loss: 0.00108218
Iteration 14/25 | Loss: 0.00108184
Iteration 15/25 | Loss: 0.00108184
Iteration 16/25 | Loss: 0.00108183
Iteration 17/25 | Loss: 0.00108183
Iteration 18/25 | Loss: 0.00108183
Iteration 19/25 | Loss: 0.00108183
Iteration 20/25 | Loss: 0.00108183
Iteration 21/25 | Loss: 0.00108183
Iteration 22/25 | Loss: 0.00108183
Iteration 23/25 | Loss: 0.00108183
Iteration 24/25 | Loss: 0.00108183
Iteration 25/25 | Loss: 0.00108183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.54489410
Iteration 2/25 | Loss: 0.00132779
Iteration 3/25 | Loss: 0.00132778
Iteration 4/25 | Loss: 0.00132778
Iteration 5/25 | Loss: 0.00132778
Iteration 6/25 | Loss: 0.00132778
Iteration 7/25 | Loss: 0.00132777
Iteration 8/25 | Loss: 0.00132777
Iteration 9/25 | Loss: 0.00132777
Iteration 10/25 | Loss: 0.00132777
Iteration 11/25 | Loss: 0.00132777
Iteration 12/25 | Loss: 0.00132777
Iteration 13/25 | Loss: 0.00132777
Iteration 14/25 | Loss: 0.00132777
Iteration 15/25 | Loss: 0.00132777
Iteration 16/25 | Loss: 0.00132777
Iteration 17/25 | Loss: 0.00132777
Iteration 18/25 | Loss: 0.00132777
Iteration 19/25 | Loss: 0.00132777
Iteration 20/25 | Loss: 0.00132777
Iteration 21/25 | Loss: 0.00132777
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0013277733232825994, 0.0013277733232825994, 0.0013277733232825994, 0.0013277733232825994, 0.0013277733232825994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013277733232825994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132777
Iteration 2/1000 | Loss: 0.00009172
Iteration 3/1000 | Loss: 0.00006655
Iteration 4/1000 | Loss: 0.00005960
Iteration 5/1000 | Loss: 0.00005686
Iteration 6/1000 | Loss: 0.00005515
Iteration 7/1000 | Loss: 0.00005405
Iteration 8/1000 | Loss: 0.00005332
Iteration 9/1000 | Loss: 0.00005261
Iteration 10/1000 | Loss: 0.00005212
Iteration 11/1000 | Loss: 0.00005176
Iteration 12/1000 | Loss: 0.00005151
Iteration 13/1000 | Loss: 0.00005142
Iteration 14/1000 | Loss: 0.00005136
Iteration 15/1000 | Loss: 0.00005130
Iteration 16/1000 | Loss: 0.00005122
Iteration 17/1000 | Loss: 0.00005121
Iteration 18/1000 | Loss: 0.00005120
Iteration 19/1000 | Loss: 0.00005120
Iteration 20/1000 | Loss: 0.00005119
Iteration 21/1000 | Loss: 0.00005116
Iteration 22/1000 | Loss: 0.00005111
Iteration 23/1000 | Loss: 0.00005110
Iteration 24/1000 | Loss: 0.00005110
Iteration 25/1000 | Loss: 0.00005109
Iteration 26/1000 | Loss: 0.00005108
Iteration 27/1000 | Loss: 0.00005108
Iteration 28/1000 | Loss: 0.00005107
Iteration 29/1000 | Loss: 0.00005107
Iteration 30/1000 | Loss: 0.00005107
Iteration 31/1000 | Loss: 0.00005105
Iteration 32/1000 | Loss: 0.00005105
Iteration 33/1000 | Loss: 0.00005105
Iteration 34/1000 | Loss: 0.00005105
Iteration 35/1000 | Loss: 0.00005105
Iteration 36/1000 | Loss: 0.00005105
Iteration 37/1000 | Loss: 0.00005105
Iteration 38/1000 | Loss: 0.00005105
Iteration 39/1000 | Loss: 0.00005105
Iteration 40/1000 | Loss: 0.00005105
Iteration 41/1000 | Loss: 0.00005105
Iteration 42/1000 | Loss: 0.00005104
Iteration 43/1000 | Loss: 0.00005104
Iteration 44/1000 | Loss: 0.00005104
Iteration 45/1000 | Loss: 0.00005103
Iteration 46/1000 | Loss: 0.00005103
Iteration 47/1000 | Loss: 0.00005103
Iteration 48/1000 | Loss: 0.00005103
Iteration 49/1000 | Loss: 0.00005103
Iteration 50/1000 | Loss: 0.00005102
Iteration 51/1000 | Loss: 0.00005102
Iteration 52/1000 | Loss: 0.00005102
Iteration 53/1000 | Loss: 0.00005102
Iteration 54/1000 | Loss: 0.00005102
Iteration 55/1000 | Loss: 0.00005101
Iteration 56/1000 | Loss: 0.00005101
Iteration 57/1000 | Loss: 0.00005101
Iteration 58/1000 | Loss: 0.00005101
Iteration 59/1000 | Loss: 0.00005101
Iteration 60/1000 | Loss: 0.00005100
Iteration 61/1000 | Loss: 0.00005100
Iteration 62/1000 | Loss: 0.00005100
Iteration 63/1000 | Loss: 0.00005100
Iteration 64/1000 | Loss: 0.00005100
Iteration 65/1000 | Loss: 0.00005099
Iteration 66/1000 | Loss: 0.00005099
Iteration 67/1000 | Loss: 0.00005099
Iteration 68/1000 | Loss: 0.00005099
Iteration 69/1000 | Loss: 0.00005099
Iteration 70/1000 | Loss: 0.00005099
Iteration 71/1000 | Loss: 0.00005099
Iteration 72/1000 | Loss: 0.00005098
Iteration 73/1000 | Loss: 0.00005098
Iteration 74/1000 | Loss: 0.00005098
Iteration 75/1000 | Loss: 0.00005098
Iteration 76/1000 | Loss: 0.00005098
Iteration 77/1000 | Loss: 0.00005098
Iteration 78/1000 | Loss: 0.00005098
Iteration 79/1000 | Loss: 0.00005097
Iteration 80/1000 | Loss: 0.00005097
Iteration 81/1000 | Loss: 0.00005097
Iteration 82/1000 | Loss: 0.00005097
Iteration 83/1000 | Loss: 0.00005096
Iteration 84/1000 | Loss: 0.00005096
Iteration 85/1000 | Loss: 0.00005096
Iteration 86/1000 | Loss: 0.00005096
Iteration 87/1000 | Loss: 0.00005096
Iteration 88/1000 | Loss: 0.00005096
Iteration 89/1000 | Loss: 0.00005096
Iteration 90/1000 | Loss: 0.00005096
Iteration 91/1000 | Loss: 0.00005095
Iteration 92/1000 | Loss: 0.00005095
Iteration 93/1000 | Loss: 0.00005095
Iteration 94/1000 | Loss: 0.00005095
Iteration 95/1000 | Loss: 0.00005095
Iteration 96/1000 | Loss: 0.00005095
Iteration 97/1000 | Loss: 0.00005095
Iteration 98/1000 | Loss: 0.00005095
Iteration 99/1000 | Loss: 0.00005095
Iteration 100/1000 | Loss: 0.00005095
Iteration 101/1000 | Loss: 0.00005095
Iteration 102/1000 | Loss: 0.00005095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [5.094958032714203e-05, 5.094958032714203e-05, 5.094958032714203e-05, 5.094958032714203e-05, 5.094958032714203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.094958032714203e-05

Optimization complete. Final v2v error: 5.674433708190918 mm

Highest mean error: 6.731358528137207 mm for frame 40

Lowest mean error: 4.836087226867676 mm for frame 218

Saving results

Total time: 60.110522985458374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01023348
Iteration 2/25 | Loss: 0.00213353
Iteration 3/25 | Loss: 0.00158660
Iteration 4/25 | Loss: 0.00136534
Iteration 5/25 | Loss: 0.00125664
Iteration 6/25 | Loss: 0.00113560
Iteration 7/25 | Loss: 0.00105877
Iteration 8/25 | Loss: 0.00097981
Iteration 9/25 | Loss: 0.00097991
Iteration 10/25 | Loss: 0.00091337
Iteration 11/25 | Loss: 0.00089731
Iteration 12/25 | Loss: 0.00091050
Iteration 13/25 | Loss: 0.00087092
Iteration 14/25 | Loss: 0.00085397
Iteration 15/25 | Loss: 0.00087929
Iteration 16/25 | Loss: 0.00083792
Iteration 17/25 | Loss: 0.00083064
Iteration 18/25 | Loss: 0.00083105
Iteration 19/25 | Loss: 0.00083009
Iteration 20/25 | Loss: 0.00083344
Iteration 21/25 | Loss: 0.00082831
Iteration 22/25 | Loss: 0.00083062
Iteration 23/25 | Loss: 0.00083055
Iteration 24/25 | Loss: 0.00082898
Iteration 25/25 | Loss: 0.00083085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62893999
Iteration 2/25 | Loss: 0.00172251
Iteration 3/25 | Loss: 0.00172250
Iteration 4/25 | Loss: 0.00172250
Iteration 5/25 | Loss: 0.00172250
Iteration 6/25 | Loss: 0.00172250
Iteration 7/25 | Loss: 0.00172250
Iteration 8/25 | Loss: 0.00172250
Iteration 9/25 | Loss: 0.00172250
Iteration 10/25 | Loss: 0.00172250
Iteration 11/25 | Loss: 0.00172250
Iteration 12/25 | Loss: 0.00172250
Iteration 13/25 | Loss: 0.00172250
Iteration 14/25 | Loss: 0.00172250
Iteration 15/25 | Loss: 0.00172250
Iteration 16/25 | Loss: 0.00172250
Iteration 17/25 | Loss: 0.00172250
Iteration 18/25 | Loss: 0.00172250
Iteration 19/25 | Loss: 0.00172250
Iteration 20/25 | Loss: 0.00172250
Iteration 21/25 | Loss: 0.00172250
Iteration 22/25 | Loss: 0.00172250
Iteration 23/25 | Loss: 0.00172250
Iteration 24/25 | Loss: 0.00172250
Iteration 25/25 | Loss: 0.00172250

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172250
Iteration 2/1000 | Loss: 0.00157205
Iteration 3/1000 | Loss: 0.00267978
Iteration 4/1000 | Loss: 0.00020460
Iteration 5/1000 | Loss: 0.00027002
Iteration 6/1000 | Loss: 0.00025087
Iteration 7/1000 | Loss: 0.00014827
Iteration 8/1000 | Loss: 0.00140102
Iteration 9/1000 | Loss: 0.00069818
Iteration 10/1000 | Loss: 0.00023157
Iteration 11/1000 | Loss: 0.00146490
Iteration 12/1000 | Loss: 0.00131262
Iteration 13/1000 | Loss: 0.00088807
Iteration 14/1000 | Loss: 0.00129838
Iteration 15/1000 | Loss: 0.00043625
Iteration 16/1000 | Loss: 0.00020755
Iteration 17/1000 | Loss: 0.00006595
Iteration 18/1000 | Loss: 0.00007630
Iteration 19/1000 | Loss: 0.00005452
Iteration 20/1000 | Loss: 0.00007687
Iteration 21/1000 | Loss: 0.00007370
Iteration 22/1000 | Loss: 0.00005353
Iteration 23/1000 | Loss: 0.00005259
Iteration 24/1000 | Loss: 0.00058393
Iteration 25/1000 | Loss: 0.00022690
Iteration 26/1000 | Loss: 0.00004831
Iteration 27/1000 | Loss: 0.00004655
Iteration 28/1000 | Loss: 0.00005477
Iteration 29/1000 | Loss: 0.00062482
Iteration 30/1000 | Loss: 0.00012628
Iteration 31/1000 | Loss: 0.00005885
Iteration 32/1000 | Loss: 0.00004081
Iteration 33/1000 | Loss: 0.00004432
Iteration 34/1000 | Loss: 0.00005361
Iteration 35/1000 | Loss: 0.00005498
Iteration 36/1000 | Loss: 0.00006116
Iteration 37/1000 | Loss: 0.00003736
Iteration 38/1000 | Loss: 0.00005632
Iteration 39/1000 | Loss: 0.00005179
Iteration 40/1000 | Loss: 0.00005392
Iteration 41/1000 | Loss: 0.00005480
Iteration 42/1000 | Loss: 0.00005562
Iteration 43/1000 | Loss: 0.00005235
Iteration 44/1000 | Loss: 0.00005789
Iteration 45/1000 | Loss: 0.00005321
Iteration 46/1000 | Loss: 0.00005930
Iteration 47/1000 | Loss: 0.00005998
Iteration 48/1000 | Loss: 0.00004508
Iteration 49/1000 | Loss: 0.00005901
Iteration 50/1000 | Loss: 0.00026959
Iteration 51/1000 | Loss: 0.00008140
Iteration 52/1000 | Loss: 0.00004357
Iteration 53/1000 | Loss: 0.00003425
Iteration 54/1000 | Loss: 0.00003156
Iteration 55/1000 | Loss: 0.00003025
Iteration 56/1000 | Loss: 0.00002948
Iteration 57/1000 | Loss: 0.00002880
Iteration 58/1000 | Loss: 0.00176713
Iteration 59/1000 | Loss: 0.00019752
Iteration 60/1000 | Loss: 0.00051009
Iteration 61/1000 | Loss: 0.00059321
Iteration 62/1000 | Loss: 0.00006138
Iteration 63/1000 | Loss: 0.00003418
Iteration 64/1000 | Loss: 0.00002733
Iteration 65/1000 | Loss: 0.00002409
Iteration 66/1000 | Loss: 0.00002161
Iteration 67/1000 | Loss: 0.00002037
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001891
Iteration 70/1000 | Loss: 0.00001851
Iteration 71/1000 | Loss: 0.00001826
Iteration 72/1000 | Loss: 0.00001799
Iteration 73/1000 | Loss: 0.00001789
Iteration 74/1000 | Loss: 0.00001781
Iteration 75/1000 | Loss: 0.00001774
Iteration 76/1000 | Loss: 0.00001769
Iteration 77/1000 | Loss: 0.00001769
Iteration 78/1000 | Loss: 0.00001768
Iteration 79/1000 | Loss: 0.00001764
Iteration 80/1000 | Loss: 0.00001756
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001751
Iteration 83/1000 | Loss: 0.00001750
Iteration 84/1000 | Loss: 0.00001750
Iteration 85/1000 | Loss: 0.00001749
Iteration 86/1000 | Loss: 0.00001749
Iteration 87/1000 | Loss: 0.00001749
Iteration 88/1000 | Loss: 0.00001749
Iteration 89/1000 | Loss: 0.00001748
Iteration 90/1000 | Loss: 0.00001748
Iteration 91/1000 | Loss: 0.00001748
Iteration 92/1000 | Loss: 0.00001748
Iteration 93/1000 | Loss: 0.00001747
Iteration 94/1000 | Loss: 0.00001747
Iteration 95/1000 | Loss: 0.00001746
Iteration 96/1000 | Loss: 0.00001746
Iteration 97/1000 | Loss: 0.00001746
Iteration 98/1000 | Loss: 0.00001746
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001745
Iteration 101/1000 | Loss: 0.00001745
Iteration 102/1000 | Loss: 0.00001744
Iteration 103/1000 | Loss: 0.00001744
Iteration 104/1000 | Loss: 0.00001744
Iteration 105/1000 | Loss: 0.00001744
Iteration 106/1000 | Loss: 0.00001744
Iteration 107/1000 | Loss: 0.00001744
Iteration 108/1000 | Loss: 0.00001744
Iteration 109/1000 | Loss: 0.00001744
Iteration 110/1000 | Loss: 0.00001743
Iteration 111/1000 | Loss: 0.00001743
Iteration 112/1000 | Loss: 0.00001743
Iteration 113/1000 | Loss: 0.00001743
Iteration 114/1000 | Loss: 0.00001743
Iteration 115/1000 | Loss: 0.00001743
Iteration 116/1000 | Loss: 0.00001743
Iteration 117/1000 | Loss: 0.00001743
Iteration 118/1000 | Loss: 0.00001743
Iteration 119/1000 | Loss: 0.00001743
Iteration 120/1000 | Loss: 0.00001743
Iteration 121/1000 | Loss: 0.00001743
Iteration 122/1000 | Loss: 0.00001743
Iteration 123/1000 | Loss: 0.00001743
Iteration 124/1000 | Loss: 0.00001743
Iteration 125/1000 | Loss: 0.00001743
Iteration 126/1000 | Loss: 0.00001743
Iteration 127/1000 | Loss: 0.00001743
Iteration 128/1000 | Loss: 0.00001742
Iteration 129/1000 | Loss: 0.00001742
Iteration 130/1000 | Loss: 0.00001742
Iteration 131/1000 | Loss: 0.00001742
Iteration 132/1000 | Loss: 0.00001742
Iteration 133/1000 | Loss: 0.00001742
Iteration 134/1000 | Loss: 0.00001742
Iteration 135/1000 | Loss: 0.00001742
Iteration 136/1000 | Loss: 0.00001742
Iteration 137/1000 | Loss: 0.00001742
Iteration 138/1000 | Loss: 0.00001742
Iteration 139/1000 | Loss: 0.00001741
Iteration 140/1000 | Loss: 0.00001741
Iteration 141/1000 | Loss: 0.00001741
Iteration 142/1000 | Loss: 0.00001741
Iteration 143/1000 | Loss: 0.00001741
Iteration 144/1000 | Loss: 0.00001740
Iteration 145/1000 | Loss: 0.00001740
Iteration 146/1000 | Loss: 0.00001740
Iteration 147/1000 | Loss: 0.00001740
Iteration 148/1000 | Loss: 0.00001740
Iteration 149/1000 | Loss: 0.00001740
Iteration 150/1000 | Loss: 0.00001740
Iteration 151/1000 | Loss: 0.00001740
Iteration 152/1000 | Loss: 0.00001740
Iteration 153/1000 | Loss: 0.00001740
Iteration 154/1000 | Loss: 0.00001740
Iteration 155/1000 | Loss: 0.00001740
Iteration 156/1000 | Loss: 0.00001740
Iteration 157/1000 | Loss: 0.00001740
Iteration 158/1000 | Loss: 0.00001740
Iteration 159/1000 | Loss: 0.00001740
Iteration 160/1000 | Loss: 0.00001740
Iteration 161/1000 | Loss: 0.00001739
Iteration 162/1000 | Loss: 0.00001739
Iteration 163/1000 | Loss: 0.00001739
Iteration 164/1000 | Loss: 0.00001739
Iteration 165/1000 | Loss: 0.00001739
Iteration 166/1000 | Loss: 0.00001739
Iteration 167/1000 | Loss: 0.00001739
Iteration 168/1000 | Loss: 0.00001739
Iteration 169/1000 | Loss: 0.00001739
Iteration 170/1000 | Loss: 0.00001739
Iteration 171/1000 | Loss: 0.00001739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.739401886879932e-05, 1.739401886879932e-05, 1.739401886879932e-05, 1.739401886879932e-05, 1.739401886879932e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.739401886879932e-05

Optimization complete. Final v2v error: 3.4975228309631348 mm

Highest mean error: 5.891052722930908 mm for frame 49

Lowest mean error: 3.2281627655029297 mm for frame 62

Saving results

Total time: 158.19100713729858
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861575
Iteration 2/25 | Loss: 0.00122627
Iteration 3/25 | Loss: 0.00088449
Iteration 4/25 | Loss: 0.00085282
Iteration 5/25 | Loss: 0.00085008
Iteration 6/25 | Loss: 0.00085008
Iteration 7/25 | Loss: 0.00085008
Iteration 8/25 | Loss: 0.00085008
Iteration 9/25 | Loss: 0.00085008
Iteration 10/25 | Loss: 0.00085008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0008500814437866211, 0.0008500814437866211, 0.0008500814437866211, 0.0008500814437866211, 0.0008500814437866211]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008500814437866211

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.12412202
Iteration 2/25 | Loss: 0.00072876
Iteration 3/25 | Loss: 0.00072876
Iteration 4/25 | Loss: 0.00072876
Iteration 5/25 | Loss: 0.00072876
Iteration 6/25 | Loss: 0.00072876
Iteration 7/25 | Loss: 0.00072876
Iteration 8/25 | Loss: 0.00072876
Iteration 9/25 | Loss: 0.00072876
Iteration 10/25 | Loss: 0.00072876
Iteration 11/25 | Loss: 0.00072875
Iteration 12/25 | Loss: 0.00072875
Iteration 13/25 | Loss: 0.00072876
Iteration 14/25 | Loss: 0.00072875
Iteration 15/25 | Loss: 0.00072875
Iteration 16/25 | Loss: 0.00072876
Iteration 17/25 | Loss: 0.00072875
Iteration 18/25 | Loss: 0.00072875
Iteration 19/25 | Loss: 0.00072875
Iteration 20/25 | Loss: 0.00072876
Iteration 21/25 | Loss: 0.00072875
Iteration 22/25 | Loss: 0.00072875
Iteration 23/25 | Loss: 0.00072875
Iteration 24/25 | Loss: 0.00072875
Iteration 25/25 | Loss: 0.00072875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072876
Iteration 2/1000 | Loss: 0.00003718
Iteration 3/1000 | Loss: 0.00003117
Iteration 4/1000 | Loss: 0.00002905
Iteration 5/1000 | Loss: 0.00002692
Iteration 6/1000 | Loss: 0.00002622
Iteration 7/1000 | Loss: 0.00002562
Iteration 8/1000 | Loss: 0.00002531
Iteration 9/1000 | Loss: 0.00002508
Iteration 10/1000 | Loss: 0.00002478
Iteration 11/1000 | Loss: 0.00002456
Iteration 12/1000 | Loss: 0.00002453
Iteration 13/1000 | Loss: 0.00002452
Iteration 14/1000 | Loss: 0.00002452
Iteration 15/1000 | Loss: 0.00002447
Iteration 16/1000 | Loss: 0.00002440
Iteration 17/1000 | Loss: 0.00002439
Iteration 18/1000 | Loss: 0.00002439
Iteration 19/1000 | Loss: 0.00002439
Iteration 20/1000 | Loss: 0.00002439
Iteration 21/1000 | Loss: 0.00002439
Iteration 22/1000 | Loss: 0.00002439
Iteration 23/1000 | Loss: 0.00002439
Iteration 24/1000 | Loss: 0.00002438
Iteration 25/1000 | Loss: 0.00002438
Iteration 26/1000 | Loss: 0.00002438
Iteration 27/1000 | Loss: 0.00002436
Iteration 28/1000 | Loss: 0.00002435
Iteration 29/1000 | Loss: 0.00002433
Iteration 30/1000 | Loss: 0.00002432
Iteration 31/1000 | Loss: 0.00002431
Iteration 32/1000 | Loss: 0.00002429
Iteration 33/1000 | Loss: 0.00002429
Iteration 34/1000 | Loss: 0.00002429
Iteration 35/1000 | Loss: 0.00002429
Iteration 36/1000 | Loss: 0.00002429
Iteration 37/1000 | Loss: 0.00002428
Iteration 38/1000 | Loss: 0.00002428
Iteration 39/1000 | Loss: 0.00002428
Iteration 40/1000 | Loss: 0.00002427
Iteration 41/1000 | Loss: 0.00002427
Iteration 42/1000 | Loss: 0.00002426
Iteration 43/1000 | Loss: 0.00002424
Iteration 44/1000 | Loss: 0.00002424
Iteration 45/1000 | Loss: 0.00002424
Iteration 46/1000 | Loss: 0.00002423
Iteration 47/1000 | Loss: 0.00002423
Iteration 48/1000 | Loss: 0.00002423
Iteration 49/1000 | Loss: 0.00002423
Iteration 50/1000 | Loss: 0.00002423
Iteration 51/1000 | Loss: 0.00002423
Iteration 52/1000 | Loss: 0.00002423
Iteration 53/1000 | Loss: 0.00002423
Iteration 54/1000 | Loss: 0.00002423
Iteration 55/1000 | Loss: 0.00002422
Iteration 56/1000 | Loss: 0.00002421
Iteration 57/1000 | Loss: 0.00002421
Iteration 58/1000 | Loss: 0.00002421
Iteration 59/1000 | Loss: 0.00002421
Iteration 60/1000 | Loss: 0.00002420
Iteration 61/1000 | Loss: 0.00002420
Iteration 62/1000 | Loss: 0.00002420
Iteration 63/1000 | Loss: 0.00002420
Iteration 64/1000 | Loss: 0.00002420
Iteration 65/1000 | Loss: 0.00002420
Iteration 66/1000 | Loss: 0.00002420
Iteration 67/1000 | Loss: 0.00002420
Iteration 68/1000 | Loss: 0.00002420
Iteration 69/1000 | Loss: 0.00002420
Iteration 70/1000 | Loss: 0.00002420
Iteration 71/1000 | Loss: 0.00002420
Iteration 72/1000 | Loss: 0.00002418
Iteration 73/1000 | Loss: 0.00002417
Iteration 74/1000 | Loss: 0.00002417
Iteration 75/1000 | Loss: 0.00002416
Iteration 76/1000 | Loss: 0.00002416
Iteration 77/1000 | Loss: 0.00002415
Iteration 78/1000 | Loss: 0.00002415
Iteration 79/1000 | Loss: 0.00002415
Iteration 80/1000 | Loss: 0.00002415
Iteration 81/1000 | Loss: 0.00002415
Iteration 82/1000 | Loss: 0.00002415
Iteration 83/1000 | Loss: 0.00002415
Iteration 84/1000 | Loss: 0.00002415
Iteration 85/1000 | Loss: 0.00002415
Iteration 86/1000 | Loss: 0.00002415
Iteration 87/1000 | Loss: 0.00002415
Iteration 88/1000 | Loss: 0.00002415
Iteration 89/1000 | Loss: 0.00002415
Iteration 90/1000 | Loss: 0.00002415
Iteration 91/1000 | Loss: 0.00002415
Iteration 92/1000 | Loss: 0.00002415
Iteration 93/1000 | Loss: 0.00002415
Iteration 94/1000 | Loss: 0.00002415
Iteration 95/1000 | Loss: 0.00002415
Iteration 96/1000 | Loss: 0.00002415
Iteration 97/1000 | Loss: 0.00002415
Iteration 98/1000 | Loss: 0.00002415
Iteration 99/1000 | Loss: 0.00002415
Iteration 100/1000 | Loss: 0.00002415
Iteration 101/1000 | Loss: 0.00002415
Iteration 102/1000 | Loss: 0.00002415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 102. Stopping optimization.
Last 5 losses: [2.414690243313089e-05, 2.414690243313089e-05, 2.414690243313089e-05, 2.414690243313089e-05, 2.414690243313089e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.414690243313089e-05

Optimization complete. Final v2v error: 4.072152614593506 mm

Highest mean error: 4.157310485839844 mm for frame 116

Lowest mean error: 4.0217976570129395 mm for frame 14

Saving results

Total time: 30.56516146659851
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00754721
Iteration 2/25 | Loss: 0.00088248
Iteration 3/25 | Loss: 0.00077522
Iteration 4/25 | Loss: 0.00074898
Iteration 5/25 | Loss: 0.00073990
Iteration 6/25 | Loss: 0.00073840
Iteration 7/25 | Loss: 0.00073788
Iteration 8/25 | Loss: 0.00073788
Iteration 9/25 | Loss: 0.00073788
Iteration 10/25 | Loss: 0.00073788
Iteration 11/25 | Loss: 0.00073788
Iteration 12/25 | Loss: 0.00073788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007378756999969482, 0.0007378756999969482, 0.0007378756999969482, 0.0007378756999969482, 0.0007378756999969482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007378756999969482

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57164550
Iteration 2/25 | Loss: 0.00080069
Iteration 3/25 | Loss: 0.00080069
Iteration 4/25 | Loss: 0.00080069
Iteration 5/25 | Loss: 0.00080069
Iteration 6/25 | Loss: 0.00080069
Iteration 7/25 | Loss: 0.00080069
Iteration 8/25 | Loss: 0.00080069
Iteration 9/25 | Loss: 0.00080069
Iteration 10/25 | Loss: 0.00080068
Iteration 11/25 | Loss: 0.00080068
Iteration 12/25 | Loss: 0.00080068
Iteration 13/25 | Loss: 0.00080068
Iteration 14/25 | Loss: 0.00080068
Iteration 15/25 | Loss: 0.00080068
Iteration 16/25 | Loss: 0.00080068
Iteration 17/25 | Loss: 0.00080068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008006845018826425, 0.0008006845018826425, 0.0008006845018826425, 0.0008006845018826425, 0.0008006845018826425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008006845018826425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080068
Iteration 2/1000 | Loss: 0.00002713
Iteration 3/1000 | Loss: 0.00001993
Iteration 4/1000 | Loss: 0.00001888
Iteration 5/1000 | Loss: 0.00001818
Iteration 6/1000 | Loss: 0.00001776
Iteration 7/1000 | Loss: 0.00001735
Iteration 8/1000 | Loss: 0.00001708
Iteration 9/1000 | Loss: 0.00001693
Iteration 10/1000 | Loss: 0.00001684
Iteration 11/1000 | Loss: 0.00001683
Iteration 12/1000 | Loss: 0.00001676
Iteration 13/1000 | Loss: 0.00001676
Iteration 14/1000 | Loss: 0.00001674
Iteration 15/1000 | Loss: 0.00001674
Iteration 16/1000 | Loss: 0.00001673
Iteration 17/1000 | Loss: 0.00001671
Iteration 18/1000 | Loss: 0.00001670
Iteration 19/1000 | Loss: 0.00001670
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001669
Iteration 22/1000 | Loss: 0.00001669
Iteration 23/1000 | Loss: 0.00001668
Iteration 24/1000 | Loss: 0.00001667
Iteration 25/1000 | Loss: 0.00001663
Iteration 26/1000 | Loss: 0.00001661
Iteration 27/1000 | Loss: 0.00001660
Iteration 28/1000 | Loss: 0.00001659
Iteration 29/1000 | Loss: 0.00001659
Iteration 30/1000 | Loss: 0.00001658
Iteration 31/1000 | Loss: 0.00001658
Iteration 32/1000 | Loss: 0.00001658
Iteration 33/1000 | Loss: 0.00001658
Iteration 34/1000 | Loss: 0.00001657
Iteration 35/1000 | Loss: 0.00001656
Iteration 36/1000 | Loss: 0.00001656
Iteration 37/1000 | Loss: 0.00001656
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001655
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001655
Iteration 42/1000 | Loss: 0.00001654
Iteration 43/1000 | Loss: 0.00001654
Iteration 44/1000 | Loss: 0.00001653
Iteration 45/1000 | Loss: 0.00001653
Iteration 46/1000 | Loss: 0.00001653
Iteration 47/1000 | Loss: 0.00001652
Iteration 48/1000 | Loss: 0.00001652
Iteration 49/1000 | Loss: 0.00001652
Iteration 50/1000 | Loss: 0.00001652
Iteration 51/1000 | Loss: 0.00001652
Iteration 52/1000 | Loss: 0.00001651
Iteration 53/1000 | Loss: 0.00001650
Iteration 54/1000 | Loss: 0.00001650
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001648
Iteration 58/1000 | Loss: 0.00001648
Iteration 59/1000 | Loss: 0.00001648
Iteration 60/1000 | Loss: 0.00001647
Iteration 61/1000 | Loss: 0.00001646
Iteration 62/1000 | Loss: 0.00001642
Iteration 63/1000 | Loss: 0.00001637
Iteration 64/1000 | Loss: 0.00001637
Iteration 65/1000 | Loss: 0.00001636
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001635
Iteration 68/1000 | Loss: 0.00001635
Iteration 69/1000 | Loss: 0.00001635
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001634
Iteration 72/1000 | Loss: 0.00001634
Iteration 73/1000 | Loss: 0.00001634
Iteration 74/1000 | Loss: 0.00001633
Iteration 75/1000 | Loss: 0.00001633
Iteration 76/1000 | Loss: 0.00001633
Iteration 77/1000 | Loss: 0.00001633
Iteration 78/1000 | Loss: 0.00001632
Iteration 79/1000 | Loss: 0.00001632
Iteration 80/1000 | Loss: 0.00001632
Iteration 81/1000 | Loss: 0.00001632
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001631
Iteration 86/1000 | Loss: 0.00001631
Iteration 87/1000 | Loss: 0.00001631
Iteration 88/1000 | Loss: 0.00001631
Iteration 89/1000 | Loss: 0.00001631
Iteration 90/1000 | Loss: 0.00001631
Iteration 91/1000 | Loss: 0.00001631
Iteration 92/1000 | Loss: 0.00001631
Iteration 93/1000 | Loss: 0.00001631
Iteration 94/1000 | Loss: 0.00001631
Iteration 95/1000 | Loss: 0.00001631
Iteration 96/1000 | Loss: 0.00001631
Iteration 97/1000 | Loss: 0.00001631
Iteration 98/1000 | Loss: 0.00001631
Iteration 99/1000 | Loss: 0.00001631
Iteration 100/1000 | Loss: 0.00001631
Iteration 101/1000 | Loss: 0.00001631
Iteration 102/1000 | Loss: 0.00001631
Iteration 103/1000 | Loss: 0.00001631
Iteration 104/1000 | Loss: 0.00001631
Iteration 105/1000 | Loss: 0.00001631
Iteration 106/1000 | Loss: 0.00001631
Iteration 107/1000 | Loss: 0.00001631
Iteration 108/1000 | Loss: 0.00001631
Iteration 109/1000 | Loss: 0.00001631
Iteration 110/1000 | Loss: 0.00001631
Iteration 111/1000 | Loss: 0.00001631
Iteration 112/1000 | Loss: 0.00001631
Iteration 113/1000 | Loss: 0.00001631
Iteration 114/1000 | Loss: 0.00001631
Iteration 115/1000 | Loss: 0.00001631
Iteration 116/1000 | Loss: 0.00001631
Iteration 117/1000 | Loss: 0.00001631
Iteration 118/1000 | Loss: 0.00001631
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.631253689993173e-05, 1.631253689993173e-05, 1.631253689993173e-05, 1.631253689993173e-05, 1.631253689993173e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.631253689993173e-05

Optimization complete. Final v2v error: 3.4118576049804688 mm

Highest mean error: 3.666256904602051 mm for frame 117

Lowest mean error: 3.317840099334717 mm for frame 142

Saving results

Total time: 34.071250915527344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00201400
Iteration 2/25 | Loss: 0.00089120
Iteration 3/25 | Loss: 0.00077667
Iteration 4/25 | Loss: 0.00073707
Iteration 5/25 | Loss: 0.00072785
Iteration 6/25 | Loss: 0.00072524
Iteration 7/25 | Loss: 0.00072416
Iteration 8/25 | Loss: 0.00072416
Iteration 9/25 | Loss: 0.00072412
Iteration 10/25 | Loss: 0.00072412
Iteration 11/25 | Loss: 0.00072412
Iteration 12/25 | Loss: 0.00072412
Iteration 13/25 | Loss: 0.00072412
Iteration 14/25 | Loss: 0.00072412
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007241223938763142, 0.0007241223938763142, 0.0007241223938763142, 0.0007241223938763142, 0.0007241223938763142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007241223938763142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57937753
Iteration 2/25 | Loss: 0.00110246
Iteration 3/25 | Loss: 0.00110246
Iteration 4/25 | Loss: 0.00110246
Iteration 5/25 | Loss: 0.00110246
Iteration 6/25 | Loss: 0.00110246
Iteration 7/25 | Loss: 0.00110246
Iteration 8/25 | Loss: 0.00110246
Iteration 9/25 | Loss: 0.00110246
Iteration 10/25 | Loss: 0.00110246
Iteration 11/25 | Loss: 0.00110246
Iteration 12/25 | Loss: 0.00110246
Iteration 13/25 | Loss: 0.00110246
Iteration 14/25 | Loss: 0.00110246
Iteration 15/25 | Loss: 0.00110246
Iteration 16/25 | Loss: 0.00110246
Iteration 17/25 | Loss: 0.00110246
Iteration 18/25 | Loss: 0.00110246
Iteration 19/25 | Loss: 0.00110246
Iteration 20/25 | Loss: 0.00110246
Iteration 21/25 | Loss: 0.00110246
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011024577543139458, 0.0011024577543139458, 0.0011024577543139458, 0.0011024577543139458, 0.0011024577543139458]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011024577543139458

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110246
Iteration 2/1000 | Loss: 0.00003871
Iteration 3/1000 | Loss: 0.00002499
Iteration 4/1000 | Loss: 0.00001987
Iteration 5/1000 | Loss: 0.00001844
Iteration 6/1000 | Loss: 0.00001745
Iteration 7/1000 | Loss: 0.00001651
Iteration 8/1000 | Loss: 0.00001601
Iteration 9/1000 | Loss: 0.00001569
Iteration 10/1000 | Loss: 0.00001557
Iteration 11/1000 | Loss: 0.00001557
Iteration 12/1000 | Loss: 0.00001535
Iteration 13/1000 | Loss: 0.00001522
Iteration 14/1000 | Loss: 0.00001521
Iteration 15/1000 | Loss: 0.00001513
Iteration 16/1000 | Loss: 0.00001512
Iteration 17/1000 | Loss: 0.00001512
Iteration 18/1000 | Loss: 0.00001512
Iteration 19/1000 | Loss: 0.00001511
Iteration 20/1000 | Loss: 0.00001511
Iteration 21/1000 | Loss: 0.00001511
Iteration 22/1000 | Loss: 0.00001510
Iteration 23/1000 | Loss: 0.00001510
Iteration 24/1000 | Loss: 0.00001509
Iteration 25/1000 | Loss: 0.00001508
Iteration 26/1000 | Loss: 0.00001503
Iteration 27/1000 | Loss: 0.00001503
Iteration 28/1000 | Loss: 0.00001498
Iteration 29/1000 | Loss: 0.00001494
Iteration 30/1000 | Loss: 0.00001486
Iteration 31/1000 | Loss: 0.00001486
Iteration 32/1000 | Loss: 0.00001485
Iteration 33/1000 | Loss: 0.00001485
Iteration 34/1000 | Loss: 0.00001481
Iteration 35/1000 | Loss: 0.00001480
Iteration 36/1000 | Loss: 0.00001480
Iteration 37/1000 | Loss: 0.00001480
Iteration 38/1000 | Loss: 0.00001479
Iteration 39/1000 | Loss: 0.00001479
Iteration 40/1000 | Loss: 0.00001479
Iteration 41/1000 | Loss: 0.00001478
Iteration 42/1000 | Loss: 0.00001478
Iteration 43/1000 | Loss: 0.00001478
Iteration 44/1000 | Loss: 0.00001478
Iteration 45/1000 | Loss: 0.00001477
Iteration 46/1000 | Loss: 0.00001477
Iteration 47/1000 | Loss: 0.00001477
Iteration 48/1000 | Loss: 0.00001477
Iteration 49/1000 | Loss: 0.00001476
Iteration 50/1000 | Loss: 0.00001476
Iteration 51/1000 | Loss: 0.00001475
Iteration 52/1000 | Loss: 0.00001475
Iteration 53/1000 | Loss: 0.00001475
Iteration 54/1000 | Loss: 0.00001474
Iteration 55/1000 | Loss: 0.00001474
Iteration 56/1000 | Loss: 0.00001474
Iteration 57/1000 | Loss: 0.00001473
Iteration 58/1000 | Loss: 0.00001473
Iteration 59/1000 | Loss: 0.00001472
Iteration 60/1000 | Loss: 0.00001472
Iteration 61/1000 | Loss: 0.00001472
Iteration 62/1000 | Loss: 0.00001471
Iteration 63/1000 | Loss: 0.00001471
Iteration 64/1000 | Loss: 0.00001471
Iteration 65/1000 | Loss: 0.00001471
Iteration 66/1000 | Loss: 0.00001471
Iteration 67/1000 | Loss: 0.00001470
Iteration 68/1000 | Loss: 0.00001470
Iteration 69/1000 | Loss: 0.00001470
Iteration 70/1000 | Loss: 0.00001470
Iteration 71/1000 | Loss: 0.00001470
Iteration 72/1000 | Loss: 0.00001470
Iteration 73/1000 | Loss: 0.00001470
Iteration 74/1000 | Loss: 0.00001470
Iteration 75/1000 | Loss: 0.00001470
Iteration 76/1000 | Loss: 0.00001470
Iteration 77/1000 | Loss: 0.00001469
Iteration 78/1000 | Loss: 0.00001469
Iteration 79/1000 | Loss: 0.00001469
Iteration 80/1000 | Loss: 0.00001469
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001468
Iteration 83/1000 | Loss: 0.00001468
Iteration 84/1000 | Loss: 0.00001467
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001467
Iteration 87/1000 | Loss: 0.00001466
Iteration 88/1000 | Loss: 0.00001466
Iteration 89/1000 | Loss: 0.00001466
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001466
Iteration 92/1000 | Loss: 0.00001466
Iteration 93/1000 | Loss: 0.00001466
Iteration 94/1000 | Loss: 0.00001466
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001466
Iteration 97/1000 | Loss: 0.00001466
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001466
Iteration 100/1000 | Loss: 0.00001466
Iteration 101/1000 | Loss: 0.00001465
Iteration 102/1000 | Loss: 0.00001465
Iteration 103/1000 | Loss: 0.00001465
Iteration 104/1000 | Loss: 0.00001464
Iteration 105/1000 | Loss: 0.00001464
Iteration 106/1000 | Loss: 0.00001464
Iteration 107/1000 | Loss: 0.00001464
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001463
Iteration 112/1000 | Loss: 0.00001463
Iteration 113/1000 | Loss: 0.00001463
Iteration 114/1000 | Loss: 0.00001463
Iteration 115/1000 | Loss: 0.00001463
Iteration 116/1000 | Loss: 0.00001463
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.4626890333602205e-05, 1.4626890333602205e-05, 1.4626890333602205e-05, 1.4626890333602205e-05, 1.4626890333602205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4626890333602205e-05

Optimization complete. Final v2v error: 3.2382752895355225 mm

Highest mean error: 3.4905498027801514 mm for frame 62

Lowest mean error: 3.0340206623077393 mm for frame 86

Saving results

Total time: 39.928674936294556
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024493
Iteration 2/25 | Loss: 0.00148586
Iteration 3/25 | Loss: 0.00098321
Iteration 4/25 | Loss: 0.00093684
Iteration 5/25 | Loss: 0.00091985
Iteration 6/25 | Loss: 0.00091567
Iteration 7/25 | Loss: 0.00091467
Iteration 8/25 | Loss: 0.00091467
Iteration 9/25 | Loss: 0.00091467
Iteration 10/25 | Loss: 0.00091467
Iteration 11/25 | Loss: 0.00091467
Iteration 12/25 | Loss: 0.00091467
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009146721567958593, 0.0009146721567958593, 0.0009146721567958593, 0.0009146721567958593, 0.0009146721567958593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009146721567958593

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97736132
Iteration 2/25 | Loss: 0.00057724
Iteration 3/25 | Loss: 0.00057723
Iteration 4/25 | Loss: 0.00057723
Iteration 5/25 | Loss: 0.00057723
Iteration 6/25 | Loss: 0.00057723
Iteration 7/25 | Loss: 0.00057723
Iteration 8/25 | Loss: 0.00057723
Iteration 9/25 | Loss: 0.00057723
Iteration 10/25 | Loss: 0.00057723
Iteration 11/25 | Loss: 0.00057723
Iteration 12/25 | Loss: 0.00057723
Iteration 13/25 | Loss: 0.00057723
Iteration 14/25 | Loss: 0.00057723
Iteration 15/25 | Loss: 0.00057723
Iteration 16/25 | Loss: 0.00057723
Iteration 17/25 | Loss: 0.00057723
Iteration 18/25 | Loss: 0.00057723
Iteration 19/25 | Loss: 0.00057723
Iteration 20/25 | Loss: 0.00057723
Iteration 21/25 | Loss: 0.00057723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005772316362708807, 0.0005772316362708807, 0.0005772316362708807, 0.0005772316362708807, 0.0005772316362708807]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005772316362708807

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057723
Iteration 2/1000 | Loss: 0.00006167
Iteration 3/1000 | Loss: 0.00003928
Iteration 4/1000 | Loss: 0.00003541
Iteration 5/1000 | Loss: 0.00003404
Iteration 6/1000 | Loss: 0.00003301
Iteration 7/1000 | Loss: 0.00003238
Iteration 8/1000 | Loss: 0.00003195
Iteration 9/1000 | Loss: 0.00003160
Iteration 10/1000 | Loss: 0.00003135
Iteration 11/1000 | Loss: 0.00003114
Iteration 12/1000 | Loss: 0.00003095
Iteration 13/1000 | Loss: 0.00003086
Iteration 14/1000 | Loss: 0.00003070
Iteration 15/1000 | Loss: 0.00003070
Iteration 16/1000 | Loss: 0.00003063
Iteration 17/1000 | Loss: 0.00003062
Iteration 18/1000 | Loss: 0.00003051
Iteration 19/1000 | Loss: 0.00003049
Iteration 20/1000 | Loss: 0.00003049
Iteration 21/1000 | Loss: 0.00003047
Iteration 22/1000 | Loss: 0.00003047
Iteration 23/1000 | Loss: 0.00003046
Iteration 24/1000 | Loss: 0.00003046
Iteration 25/1000 | Loss: 0.00003046
Iteration 26/1000 | Loss: 0.00003037
Iteration 27/1000 | Loss: 0.00003034
Iteration 28/1000 | Loss: 0.00003033
Iteration 29/1000 | Loss: 0.00003032
Iteration 30/1000 | Loss: 0.00003031
Iteration 31/1000 | Loss: 0.00003031
Iteration 32/1000 | Loss: 0.00003030
Iteration 33/1000 | Loss: 0.00003029
Iteration 34/1000 | Loss: 0.00003028
Iteration 35/1000 | Loss: 0.00003028
Iteration 36/1000 | Loss: 0.00003027
Iteration 37/1000 | Loss: 0.00003026
Iteration 38/1000 | Loss: 0.00003026
Iteration 39/1000 | Loss: 0.00003026
Iteration 40/1000 | Loss: 0.00003025
Iteration 41/1000 | Loss: 0.00003025
Iteration 42/1000 | Loss: 0.00003025
Iteration 43/1000 | Loss: 0.00003024
Iteration 44/1000 | Loss: 0.00003024
Iteration 45/1000 | Loss: 0.00003024
Iteration 46/1000 | Loss: 0.00003023
Iteration 47/1000 | Loss: 0.00003023
Iteration 48/1000 | Loss: 0.00003023
Iteration 49/1000 | Loss: 0.00003023
Iteration 50/1000 | Loss: 0.00003023
Iteration 51/1000 | Loss: 0.00003023
Iteration 52/1000 | Loss: 0.00003022
Iteration 53/1000 | Loss: 0.00003022
Iteration 54/1000 | Loss: 0.00003022
Iteration 55/1000 | Loss: 0.00003021
Iteration 56/1000 | Loss: 0.00003021
Iteration 57/1000 | Loss: 0.00003020
Iteration 58/1000 | Loss: 0.00003020
Iteration 59/1000 | Loss: 0.00003020
Iteration 60/1000 | Loss: 0.00003020
Iteration 61/1000 | Loss: 0.00003020
Iteration 62/1000 | Loss: 0.00003020
Iteration 63/1000 | Loss: 0.00003019
Iteration 64/1000 | Loss: 0.00003019
Iteration 65/1000 | Loss: 0.00003019
Iteration 66/1000 | Loss: 0.00003019
Iteration 67/1000 | Loss: 0.00003018
Iteration 68/1000 | Loss: 0.00003018
Iteration 69/1000 | Loss: 0.00003018
Iteration 70/1000 | Loss: 0.00003017
Iteration 71/1000 | Loss: 0.00003017
Iteration 72/1000 | Loss: 0.00003017
Iteration 73/1000 | Loss: 0.00003017
Iteration 74/1000 | Loss: 0.00003016
Iteration 75/1000 | Loss: 0.00003016
Iteration 76/1000 | Loss: 0.00003016
Iteration 77/1000 | Loss: 0.00003016
Iteration 78/1000 | Loss: 0.00003016
Iteration 79/1000 | Loss: 0.00003015
Iteration 80/1000 | Loss: 0.00003015
Iteration 81/1000 | Loss: 0.00003015
Iteration 82/1000 | Loss: 0.00003015
Iteration 83/1000 | Loss: 0.00003014
Iteration 84/1000 | Loss: 0.00003014
Iteration 85/1000 | Loss: 0.00003013
Iteration 86/1000 | Loss: 0.00003013
Iteration 87/1000 | Loss: 0.00003013
Iteration 88/1000 | Loss: 0.00003012
Iteration 89/1000 | Loss: 0.00003012
Iteration 90/1000 | Loss: 0.00003012
Iteration 91/1000 | Loss: 0.00003012
Iteration 92/1000 | Loss: 0.00003012
Iteration 93/1000 | Loss: 0.00003012
Iteration 94/1000 | Loss: 0.00003012
Iteration 95/1000 | Loss: 0.00003012
Iteration 96/1000 | Loss: 0.00003011
Iteration 97/1000 | Loss: 0.00003011
Iteration 98/1000 | Loss: 0.00003011
Iteration 99/1000 | Loss: 0.00003011
Iteration 100/1000 | Loss: 0.00003011
Iteration 101/1000 | Loss: 0.00003011
Iteration 102/1000 | Loss: 0.00003010
Iteration 103/1000 | Loss: 0.00003010
Iteration 104/1000 | Loss: 0.00003010
Iteration 105/1000 | Loss: 0.00003010
Iteration 106/1000 | Loss: 0.00003010
Iteration 107/1000 | Loss: 0.00003009
Iteration 108/1000 | Loss: 0.00003009
Iteration 109/1000 | Loss: 0.00003009
Iteration 110/1000 | Loss: 0.00003009
Iteration 111/1000 | Loss: 0.00003009
Iteration 112/1000 | Loss: 0.00003008
Iteration 113/1000 | Loss: 0.00003008
Iteration 114/1000 | Loss: 0.00003008
Iteration 115/1000 | Loss: 0.00003008
Iteration 116/1000 | Loss: 0.00003008
Iteration 117/1000 | Loss: 0.00003008
Iteration 118/1000 | Loss: 0.00003008
Iteration 119/1000 | Loss: 0.00003007
Iteration 120/1000 | Loss: 0.00003007
Iteration 121/1000 | Loss: 0.00003007
Iteration 122/1000 | Loss: 0.00003007
Iteration 123/1000 | Loss: 0.00003007
Iteration 124/1000 | Loss: 0.00003007
Iteration 125/1000 | Loss: 0.00003006
Iteration 126/1000 | Loss: 0.00003006
Iteration 127/1000 | Loss: 0.00003006
Iteration 128/1000 | Loss: 0.00003006
Iteration 129/1000 | Loss: 0.00003006
Iteration 130/1000 | Loss: 0.00003006
Iteration 131/1000 | Loss: 0.00003006
Iteration 132/1000 | Loss: 0.00003006
Iteration 133/1000 | Loss: 0.00003006
Iteration 134/1000 | Loss: 0.00003006
Iteration 135/1000 | Loss: 0.00003005
Iteration 136/1000 | Loss: 0.00003005
Iteration 137/1000 | Loss: 0.00003005
Iteration 138/1000 | Loss: 0.00003005
Iteration 139/1000 | Loss: 0.00003005
Iteration 140/1000 | Loss: 0.00003005
Iteration 141/1000 | Loss: 0.00003005
Iteration 142/1000 | Loss: 0.00003005
Iteration 143/1000 | Loss: 0.00003005
Iteration 144/1000 | Loss: 0.00003005
Iteration 145/1000 | Loss: 0.00003005
Iteration 146/1000 | Loss: 0.00003004
Iteration 147/1000 | Loss: 0.00003004
Iteration 148/1000 | Loss: 0.00003004
Iteration 149/1000 | Loss: 0.00003004
Iteration 150/1000 | Loss: 0.00003004
Iteration 151/1000 | Loss: 0.00003004
Iteration 152/1000 | Loss: 0.00003004
Iteration 153/1000 | Loss: 0.00003004
Iteration 154/1000 | Loss: 0.00003004
Iteration 155/1000 | Loss: 0.00003004
Iteration 156/1000 | Loss: 0.00003004
Iteration 157/1000 | Loss: 0.00003004
Iteration 158/1000 | Loss: 0.00003004
Iteration 159/1000 | Loss: 0.00003003
Iteration 160/1000 | Loss: 0.00003003
Iteration 161/1000 | Loss: 0.00003003
Iteration 162/1000 | Loss: 0.00003003
Iteration 163/1000 | Loss: 0.00003003
Iteration 164/1000 | Loss: 0.00003003
Iteration 165/1000 | Loss: 0.00003003
Iteration 166/1000 | Loss: 0.00003003
Iteration 167/1000 | Loss: 0.00003003
Iteration 168/1000 | Loss: 0.00003003
Iteration 169/1000 | Loss: 0.00003003
Iteration 170/1000 | Loss: 0.00003003
Iteration 171/1000 | Loss: 0.00003003
Iteration 172/1000 | Loss: 0.00003002
Iteration 173/1000 | Loss: 0.00003002
Iteration 174/1000 | Loss: 0.00003002
Iteration 175/1000 | Loss: 0.00003002
Iteration 176/1000 | Loss: 0.00003002
Iteration 177/1000 | Loss: 0.00003002
Iteration 178/1000 | Loss: 0.00003002
Iteration 179/1000 | Loss: 0.00003002
Iteration 180/1000 | Loss: 0.00003002
Iteration 181/1000 | Loss: 0.00003002
Iteration 182/1000 | Loss: 0.00003002
Iteration 183/1000 | Loss: 0.00003002
Iteration 184/1000 | Loss: 0.00003002
Iteration 185/1000 | Loss: 0.00003002
Iteration 186/1000 | Loss: 0.00003002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [3.002357334480621e-05, 3.002357334480621e-05, 3.002357334480621e-05, 3.002357334480621e-05, 3.002357334480621e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.002357334480621e-05

Optimization complete. Final v2v error: 4.44126033782959 mm

Highest mean error: 5.273655891418457 mm for frame 100

Lowest mean error: 3.4164886474609375 mm for frame 53

Saving results

Total time: 52.99892783164978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00173761
Iteration 2/25 | Loss: 0.00083953
Iteration 3/25 | Loss: 0.00075001
Iteration 4/25 | Loss: 0.00072684
Iteration 5/25 | Loss: 0.00071796
Iteration 6/25 | Loss: 0.00071591
Iteration 7/25 | Loss: 0.00071491
Iteration 8/25 | Loss: 0.00071491
Iteration 9/25 | Loss: 0.00071491
Iteration 10/25 | Loss: 0.00071491
Iteration 11/25 | Loss: 0.00071491
Iteration 12/25 | Loss: 0.00071491
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007149092270992696, 0.0007149092270992696, 0.0007149092270992696, 0.0007149092270992696, 0.0007149092270992696]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007149092270992696

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60235178
Iteration 2/25 | Loss: 0.00103593
Iteration 3/25 | Loss: 0.00103593
Iteration 4/25 | Loss: 0.00103593
Iteration 5/25 | Loss: 0.00103593
Iteration 6/25 | Loss: 0.00103593
Iteration 7/25 | Loss: 0.00103593
Iteration 8/25 | Loss: 0.00103593
Iteration 9/25 | Loss: 0.00103593
Iteration 10/25 | Loss: 0.00103593
Iteration 11/25 | Loss: 0.00103593
Iteration 12/25 | Loss: 0.00103593
Iteration 13/25 | Loss: 0.00103593
Iteration 14/25 | Loss: 0.00103593
Iteration 15/25 | Loss: 0.00103593
Iteration 16/25 | Loss: 0.00103593
Iteration 17/25 | Loss: 0.00103593
Iteration 18/25 | Loss: 0.00103593
Iteration 19/25 | Loss: 0.00103593
Iteration 20/25 | Loss: 0.00103593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010359252337366343, 0.0010359252337366343, 0.0010359252337366343, 0.0010359252337366343, 0.0010359252337366343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010359252337366343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103593
Iteration 2/1000 | Loss: 0.00003124
Iteration 3/1000 | Loss: 0.00001851
Iteration 4/1000 | Loss: 0.00001516
Iteration 5/1000 | Loss: 0.00001372
Iteration 6/1000 | Loss: 0.00001278
Iteration 7/1000 | Loss: 0.00001223
Iteration 8/1000 | Loss: 0.00001191
Iteration 9/1000 | Loss: 0.00001190
Iteration 10/1000 | Loss: 0.00001175
Iteration 11/1000 | Loss: 0.00001174
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001172
Iteration 14/1000 | Loss: 0.00001172
Iteration 15/1000 | Loss: 0.00001171
Iteration 16/1000 | Loss: 0.00001170
Iteration 17/1000 | Loss: 0.00001167
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001154
Iteration 20/1000 | Loss: 0.00001154
Iteration 21/1000 | Loss: 0.00001150
Iteration 22/1000 | Loss: 0.00001146
Iteration 23/1000 | Loss: 0.00001146
Iteration 24/1000 | Loss: 0.00001146
Iteration 25/1000 | Loss: 0.00001146
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001141
Iteration 28/1000 | Loss: 0.00001141
Iteration 29/1000 | Loss: 0.00001137
Iteration 30/1000 | Loss: 0.00001132
Iteration 31/1000 | Loss: 0.00001129
Iteration 32/1000 | Loss: 0.00001129
Iteration 33/1000 | Loss: 0.00001128
Iteration 34/1000 | Loss: 0.00001128
Iteration 35/1000 | Loss: 0.00001128
Iteration 36/1000 | Loss: 0.00001127
Iteration 37/1000 | Loss: 0.00001127
Iteration 38/1000 | Loss: 0.00001127
Iteration 39/1000 | Loss: 0.00001127
Iteration 40/1000 | Loss: 0.00001127
Iteration 41/1000 | Loss: 0.00001127
Iteration 42/1000 | Loss: 0.00001126
Iteration 43/1000 | Loss: 0.00001126
Iteration 44/1000 | Loss: 0.00001126
Iteration 45/1000 | Loss: 0.00001126
Iteration 46/1000 | Loss: 0.00001125
Iteration 47/1000 | Loss: 0.00001125
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001125
Iteration 51/1000 | Loss: 0.00001124
Iteration 52/1000 | Loss: 0.00001124
Iteration 53/1000 | Loss: 0.00001124
Iteration 54/1000 | Loss: 0.00001124
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001123
Iteration 63/1000 | Loss: 0.00001123
Iteration 64/1000 | Loss: 0.00001122
Iteration 65/1000 | Loss: 0.00001122
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001122
Iteration 68/1000 | Loss: 0.00001122
Iteration 69/1000 | Loss: 0.00001122
Iteration 70/1000 | Loss: 0.00001122
Iteration 71/1000 | Loss: 0.00001122
Iteration 72/1000 | Loss: 0.00001122
Iteration 73/1000 | Loss: 0.00001122
Iteration 74/1000 | Loss: 0.00001122
Iteration 75/1000 | Loss: 0.00001121
Iteration 76/1000 | Loss: 0.00001121
Iteration 77/1000 | Loss: 0.00001121
Iteration 78/1000 | Loss: 0.00001120
Iteration 79/1000 | Loss: 0.00001120
Iteration 80/1000 | Loss: 0.00001120
Iteration 81/1000 | Loss: 0.00001120
Iteration 82/1000 | Loss: 0.00001119
Iteration 83/1000 | Loss: 0.00001119
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001118
Iteration 88/1000 | Loss: 0.00001118
Iteration 89/1000 | Loss: 0.00001118
Iteration 90/1000 | Loss: 0.00001118
Iteration 91/1000 | Loss: 0.00001118
Iteration 92/1000 | Loss: 0.00001118
Iteration 93/1000 | Loss: 0.00001118
Iteration 94/1000 | Loss: 0.00001117
Iteration 95/1000 | Loss: 0.00001117
Iteration 96/1000 | Loss: 0.00001117
Iteration 97/1000 | Loss: 0.00001117
Iteration 98/1000 | Loss: 0.00001116
Iteration 99/1000 | Loss: 0.00001116
Iteration 100/1000 | Loss: 0.00001116
Iteration 101/1000 | Loss: 0.00001116
Iteration 102/1000 | Loss: 0.00001115
Iteration 103/1000 | Loss: 0.00001115
Iteration 104/1000 | Loss: 0.00001115
Iteration 105/1000 | Loss: 0.00001115
Iteration 106/1000 | Loss: 0.00001115
Iteration 107/1000 | Loss: 0.00001115
Iteration 108/1000 | Loss: 0.00001115
Iteration 109/1000 | Loss: 0.00001115
Iteration 110/1000 | Loss: 0.00001115
Iteration 111/1000 | Loss: 0.00001115
Iteration 112/1000 | Loss: 0.00001114
Iteration 113/1000 | Loss: 0.00001114
Iteration 114/1000 | Loss: 0.00001114
Iteration 115/1000 | Loss: 0.00001114
Iteration 116/1000 | Loss: 0.00001114
Iteration 117/1000 | Loss: 0.00001114
Iteration 118/1000 | Loss: 0.00001114
Iteration 119/1000 | Loss: 0.00001114
Iteration 120/1000 | Loss: 0.00001114
Iteration 121/1000 | Loss: 0.00001114
Iteration 122/1000 | Loss: 0.00001113
Iteration 123/1000 | Loss: 0.00001113
Iteration 124/1000 | Loss: 0.00001113
Iteration 125/1000 | Loss: 0.00001113
Iteration 126/1000 | Loss: 0.00001113
Iteration 127/1000 | Loss: 0.00001113
Iteration 128/1000 | Loss: 0.00001113
Iteration 129/1000 | Loss: 0.00001112
Iteration 130/1000 | Loss: 0.00001112
Iteration 131/1000 | Loss: 0.00001112
Iteration 132/1000 | Loss: 0.00001112
Iteration 133/1000 | Loss: 0.00001112
Iteration 134/1000 | Loss: 0.00001112
Iteration 135/1000 | Loss: 0.00001112
Iteration 136/1000 | Loss: 0.00001112
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.112407335313037e-05, 1.112407335313037e-05, 1.112407335313037e-05, 1.112407335313037e-05, 1.112407335313037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.112407335313037e-05

Optimization complete. Final v2v error: 2.889077663421631 mm

Highest mean error: 3.21809458732605 mm for frame 38

Lowest mean error: 2.697788715362549 mm for frame 166

Saving results

Total time: 41.52684164047241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432986
Iteration 2/25 | Loss: 0.00098542
Iteration 3/25 | Loss: 0.00083765
Iteration 4/25 | Loss: 0.00081174
Iteration 5/25 | Loss: 0.00080756
Iteration 6/25 | Loss: 0.00080638
Iteration 7/25 | Loss: 0.00080606
Iteration 8/25 | Loss: 0.00080605
Iteration 9/25 | Loss: 0.00080605
Iteration 10/25 | Loss: 0.00080605
Iteration 11/25 | Loss: 0.00080605
Iteration 12/25 | Loss: 0.00080605
Iteration 13/25 | Loss: 0.00080605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008060531690716743, 0.0008060531690716743, 0.0008060531690716743, 0.0008060531690716743, 0.0008060531690716743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008060531690716743

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.08589029
Iteration 2/25 | Loss: 0.00087299
Iteration 3/25 | Loss: 0.00087298
Iteration 4/25 | Loss: 0.00087298
Iteration 5/25 | Loss: 0.00087298
Iteration 6/25 | Loss: 0.00087298
Iteration 7/25 | Loss: 0.00087298
Iteration 8/25 | Loss: 0.00087298
Iteration 9/25 | Loss: 0.00087298
Iteration 10/25 | Loss: 0.00087298
Iteration 11/25 | Loss: 0.00087298
Iteration 12/25 | Loss: 0.00087298
Iteration 13/25 | Loss: 0.00087298
Iteration 14/25 | Loss: 0.00087298
Iteration 15/25 | Loss: 0.00087298
Iteration 16/25 | Loss: 0.00087298
Iteration 17/25 | Loss: 0.00087298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008729815599508584, 0.0008729815599508584, 0.0008729815599508584, 0.0008729815599508584, 0.0008729815599508584]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008729815599508584

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087298
Iteration 2/1000 | Loss: 0.00005010
Iteration 3/1000 | Loss: 0.00003300
Iteration 4/1000 | Loss: 0.00003064
Iteration 5/1000 | Loss: 0.00002923
Iteration 6/1000 | Loss: 0.00002823
Iteration 7/1000 | Loss: 0.00002754
Iteration 8/1000 | Loss: 0.00002708
Iteration 9/1000 | Loss: 0.00002680
Iteration 10/1000 | Loss: 0.00002655
Iteration 11/1000 | Loss: 0.00002640
Iteration 12/1000 | Loss: 0.00002636
Iteration 13/1000 | Loss: 0.00002635
Iteration 14/1000 | Loss: 0.00002633
Iteration 15/1000 | Loss: 0.00002629
Iteration 16/1000 | Loss: 0.00002624
Iteration 17/1000 | Loss: 0.00002621
Iteration 18/1000 | Loss: 0.00002621
Iteration 19/1000 | Loss: 0.00002619
Iteration 20/1000 | Loss: 0.00002618
Iteration 21/1000 | Loss: 0.00002618
Iteration 22/1000 | Loss: 0.00002616
Iteration 23/1000 | Loss: 0.00002616
Iteration 24/1000 | Loss: 0.00002615
Iteration 25/1000 | Loss: 0.00002614
Iteration 26/1000 | Loss: 0.00002614
Iteration 27/1000 | Loss: 0.00002614
Iteration 28/1000 | Loss: 0.00002613
Iteration 29/1000 | Loss: 0.00002613
Iteration 30/1000 | Loss: 0.00002612
Iteration 31/1000 | Loss: 0.00002612
Iteration 32/1000 | Loss: 0.00002608
Iteration 33/1000 | Loss: 0.00002607
Iteration 34/1000 | Loss: 0.00002605
Iteration 35/1000 | Loss: 0.00002604
Iteration 36/1000 | Loss: 0.00002604
Iteration 37/1000 | Loss: 0.00002603
Iteration 38/1000 | Loss: 0.00002603
Iteration 39/1000 | Loss: 0.00002602
Iteration 40/1000 | Loss: 0.00002601
Iteration 41/1000 | Loss: 0.00002601
Iteration 42/1000 | Loss: 0.00002598
Iteration 43/1000 | Loss: 0.00002597
Iteration 44/1000 | Loss: 0.00002597
Iteration 45/1000 | Loss: 0.00002597
Iteration 46/1000 | Loss: 0.00002596
Iteration 47/1000 | Loss: 0.00002596
Iteration 48/1000 | Loss: 0.00002596
Iteration 49/1000 | Loss: 0.00002596
Iteration 50/1000 | Loss: 0.00002596
Iteration 51/1000 | Loss: 0.00002596
Iteration 52/1000 | Loss: 0.00002596
Iteration 53/1000 | Loss: 0.00002596
Iteration 54/1000 | Loss: 0.00002596
Iteration 55/1000 | Loss: 0.00002595
Iteration 56/1000 | Loss: 0.00002593
Iteration 57/1000 | Loss: 0.00002593
Iteration 58/1000 | Loss: 0.00002593
Iteration 59/1000 | Loss: 0.00002593
Iteration 60/1000 | Loss: 0.00002592
Iteration 61/1000 | Loss: 0.00002592
Iteration 62/1000 | Loss: 0.00002592
Iteration 63/1000 | Loss: 0.00002592
Iteration 64/1000 | Loss: 0.00002592
Iteration 65/1000 | Loss: 0.00002592
Iteration 66/1000 | Loss: 0.00002591
Iteration 67/1000 | Loss: 0.00002591
Iteration 68/1000 | Loss: 0.00002591
Iteration 69/1000 | Loss: 0.00002591
Iteration 70/1000 | Loss: 0.00002590
Iteration 71/1000 | Loss: 0.00002590
Iteration 72/1000 | Loss: 0.00002590
Iteration 73/1000 | Loss: 0.00002590
Iteration 74/1000 | Loss: 0.00002590
Iteration 75/1000 | Loss: 0.00002589
Iteration 76/1000 | Loss: 0.00002589
Iteration 77/1000 | Loss: 0.00002589
Iteration 78/1000 | Loss: 0.00002589
Iteration 79/1000 | Loss: 0.00002589
Iteration 80/1000 | Loss: 0.00002589
Iteration 81/1000 | Loss: 0.00002588
Iteration 82/1000 | Loss: 0.00002588
Iteration 83/1000 | Loss: 0.00002587
Iteration 84/1000 | Loss: 0.00002587
Iteration 85/1000 | Loss: 0.00002587
Iteration 86/1000 | Loss: 0.00002587
Iteration 87/1000 | Loss: 0.00002587
Iteration 88/1000 | Loss: 0.00002587
Iteration 89/1000 | Loss: 0.00002586
Iteration 90/1000 | Loss: 0.00002586
Iteration 91/1000 | Loss: 0.00002586
Iteration 92/1000 | Loss: 0.00002586
Iteration 93/1000 | Loss: 0.00002586
Iteration 94/1000 | Loss: 0.00002585
Iteration 95/1000 | Loss: 0.00002585
Iteration 96/1000 | Loss: 0.00002585
Iteration 97/1000 | Loss: 0.00002585
Iteration 98/1000 | Loss: 0.00002585
Iteration 99/1000 | Loss: 0.00002584
Iteration 100/1000 | Loss: 0.00002584
Iteration 101/1000 | Loss: 0.00002584
Iteration 102/1000 | Loss: 0.00002584
Iteration 103/1000 | Loss: 0.00002584
Iteration 104/1000 | Loss: 0.00002583
Iteration 105/1000 | Loss: 0.00002583
Iteration 106/1000 | Loss: 0.00002583
Iteration 107/1000 | Loss: 0.00002583
Iteration 108/1000 | Loss: 0.00002583
Iteration 109/1000 | Loss: 0.00002583
Iteration 110/1000 | Loss: 0.00002582
Iteration 111/1000 | Loss: 0.00002582
Iteration 112/1000 | Loss: 0.00002582
Iteration 113/1000 | Loss: 0.00002582
Iteration 114/1000 | Loss: 0.00002582
Iteration 115/1000 | Loss: 0.00002582
Iteration 116/1000 | Loss: 0.00002582
Iteration 117/1000 | Loss: 0.00002581
Iteration 118/1000 | Loss: 0.00002581
Iteration 119/1000 | Loss: 0.00002581
Iteration 120/1000 | Loss: 0.00002581
Iteration 121/1000 | Loss: 0.00002581
Iteration 122/1000 | Loss: 0.00002581
Iteration 123/1000 | Loss: 0.00002581
Iteration 124/1000 | Loss: 0.00002581
Iteration 125/1000 | Loss: 0.00002580
Iteration 126/1000 | Loss: 0.00002580
Iteration 127/1000 | Loss: 0.00002580
Iteration 128/1000 | Loss: 0.00002580
Iteration 129/1000 | Loss: 0.00002580
Iteration 130/1000 | Loss: 0.00002580
Iteration 131/1000 | Loss: 0.00002580
Iteration 132/1000 | Loss: 0.00002580
Iteration 133/1000 | Loss: 0.00002580
Iteration 134/1000 | Loss: 0.00002580
Iteration 135/1000 | Loss: 0.00002580
Iteration 136/1000 | Loss: 0.00002580
Iteration 137/1000 | Loss: 0.00002580
Iteration 138/1000 | Loss: 0.00002580
Iteration 139/1000 | Loss: 0.00002580
Iteration 140/1000 | Loss: 0.00002580
Iteration 141/1000 | Loss: 0.00002580
Iteration 142/1000 | Loss: 0.00002580
Iteration 143/1000 | Loss: 0.00002580
Iteration 144/1000 | Loss: 0.00002580
Iteration 145/1000 | Loss: 0.00002580
Iteration 146/1000 | Loss: 0.00002580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.580346517788712e-05, 2.580346517788712e-05, 2.580346517788712e-05, 2.580346517788712e-05, 2.580346517788712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.580346517788712e-05

Optimization complete. Final v2v error: 4.214960098266602 mm

Highest mean error: 4.553537845611572 mm for frame 101

Lowest mean error: 3.673708200454712 mm for frame 41

Saving results

Total time: 38.91904401779175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889805
Iteration 2/25 | Loss: 0.00173108
Iteration 3/25 | Loss: 0.00120783
Iteration 4/25 | Loss: 0.00105751
Iteration 5/25 | Loss: 0.00102101
Iteration 6/25 | Loss: 0.00102388
Iteration 7/25 | Loss: 0.00101274
Iteration 8/25 | Loss: 0.00101024
Iteration 9/25 | Loss: 0.00100520
Iteration 10/25 | Loss: 0.00100526
Iteration 11/25 | Loss: 0.00100349
Iteration 12/25 | Loss: 0.00100160
Iteration 13/25 | Loss: 0.00099876
Iteration 14/25 | Loss: 0.00099766
Iteration 15/25 | Loss: 0.00099750
Iteration 16/25 | Loss: 0.00099749
Iteration 17/25 | Loss: 0.00099749
Iteration 18/25 | Loss: 0.00099749
Iteration 19/25 | Loss: 0.00099749
Iteration 20/25 | Loss: 0.00099749
Iteration 21/25 | Loss: 0.00099749
Iteration 22/25 | Loss: 0.00099748
Iteration 23/25 | Loss: 0.00099748
Iteration 24/25 | Loss: 0.00099748
Iteration 25/25 | Loss: 0.00099748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39005542
Iteration 2/25 | Loss: 0.00081783
Iteration 3/25 | Loss: 0.00077101
Iteration 4/25 | Loss: 0.00077101
Iteration 5/25 | Loss: 0.00077100
Iteration 6/25 | Loss: 0.00077100
Iteration 7/25 | Loss: 0.00077100
Iteration 8/25 | Loss: 0.00077100
Iteration 9/25 | Loss: 0.00077100
Iteration 10/25 | Loss: 0.00077100
Iteration 11/25 | Loss: 0.00077100
Iteration 12/25 | Loss: 0.00077100
Iteration 13/25 | Loss: 0.00077100
Iteration 14/25 | Loss: 0.00077100
Iteration 15/25 | Loss: 0.00077100
Iteration 16/25 | Loss: 0.00077100
Iteration 17/25 | Loss: 0.00077100
Iteration 18/25 | Loss: 0.00077100
Iteration 19/25 | Loss: 0.00077100
Iteration 20/25 | Loss: 0.00077100
Iteration 21/25 | Loss: 0.00077100
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007710031932219863, 0.0007710031932219863, 0.0007710031932219863, 0.0007710031932219863, 0.0007710031932219863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007710031932219863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077100
Iteration 2/1000 | Loss: 0.00029896
Iteration 3/1000 | Loss: 0.00005306
Iteration 4/1000 | Loss: 0.00035292
Iteration 5/1000 | Loss: 0.00021192
Iteration 6/1000 | Loss: 0.00015579
Iteration 7/1000 | Loss: 0.00004445
Iteration 8/1000 | Loss: 0.00004286
Iteration 9/1000 | Loss: 0.00004154
Iteration 10/1000 | Loss: 0.00026236
Iteration 11/1000 | Loss: 0.00004062
Iteration 12/1000 | Loss: 0.00003980
Iteration 13/1000 | Loss: 0.00014385
Iteration 14/1000 | Loss: 0.00009154
Iteration 15/1000 | Loss: 0.00008224
Iteration 16/1000 | Loss: 0.00022496
Iteration 17/1000 | Loss: 0.00007602
Iteration 18/1000 | Loss: 0.00003888
Iteration 19/1000 | Loss: 0.00006010
Iteration 20/1000 | Loss: 0.00012980
Iteration 21/1000 | Loss: 0.00005712
Iteration 22/1000 | Loss: 0.00003681
Iteration 23/1000 | Loss: 0.00003648
Iteration 24/1000 | Loss: 0.00010040
Iteration 25/1000 | Loss: 0.00004689
Iteration 26/1000 | Loss: 0.00003625
Iteration 27/1000 | Loss: 0.00006178
Iteration 28/1000 | Loss: 0.00003618
Iteration 29/1000 | Loss: 0.00003615
Iteration 30/1000 | Loss: 0.00003614
Iteration 31/1000 | Loss: 0.00003614
Iteration 32/1000 | Loss: 0.00003614
Iteration 33/1000 | Loss: 0.00003614
Iteration 34/1000 | Loss: 0.00003613
Iteration 35/1000 | Loss: 0.00003613
Iteration 36/1000 | Loss: 0.00003613
Iteration 37/1000 | Loss: 0.00003613
Iteration 38/1000 | Loss: 0.00003613
Iteration 39/1000 | Loss: 0.00003613
Iteration 40/1000 | Loss: 0.00003613
Iteration 41/1000 | Loss: 0.00003613
Iteration 42/1000 | Loss: 0.00003613
Iteration 43/1000 | Loss: 0.00003613
Iteration 44/1000 | Loss: 0.00003613
Iteration 45/1000 | Loss: 0.00003613
Iteration 46/1000 | Loss: 0.00003612
Iteration 47/1000 | Loss: 0.00003612
Iteration 48/1000 | Loss: 0.00003612
Iteration 49/1000 | Loss: 0.00003612
Iteration 50/1000 | Loss: 0.00003612
Iteration 51/1000 | Loss: 0.00003612
Iteration 52/1000 | Loss: 0.00003612
Iteration 53/1000 | Loss: 0.00003612
Iteration 54/1000 | Loss: 0.00003611
Iteration 55/1000 | Loss: 0.00003611
Iteration 56/1000 | Loss: 0.00003611
Iteration 57/1000 | Loss: 0.00003611
Iteration 58/1000 | Loss: 0.00003611
Iteration 59/1000 | Loss: 0.00003611
Iteration 60/1000 | Loss: 0.00003611
Iteration 61/1000 | Loss: 0.00003611
Iteration 62/1000 | Loss: 0.00003611
Iteration 63/1000 | Loss: 0.00003611
Iteration 64/1000 | Loss: 0.00003610
Iteration 65/1000 | Loss: 0.00003610
Iteration 66/1000 | Loss: 0.00003610
Iteration 67/1000 | Loss: 0.00003609
Iteration 68/1000 | Loss: 0.00003609
Iteration 69/1000 | Loss: 0.00003608
Iteration 70/1000 | Loss: 0.00003608
Iteration 71/1000 | Loss: 0.00003608
Iteration 72/1000 | Loss: 0.00003607
Iteration 73/1000 | Loss: 0.00003607
Iteration 74/1000 | Loss: 0.00003607
Iteration 75/1000 | Loss: 0.00003606
Iteration 76/1000 | Loss: 0.00003606
Iteration 77/1000 | Loss: 0.00003606
Iteration 78/1000 | Loss: 0.00003605
Iteration 79/1000 | Loss: 0.00003605
Iteration 80/1000 | Loss: 0.00003605
Iteration 81/1000 | Loss: 0.00003604
Iteration 82/1000 | Loss: 0.00003604
Iteration 83/1000 | Loss: 0.00003603
Iteration 84/1000 | Loss: 0.00003603
Iteration 85/1000 | Loss: 0.00003603
Iteration 86/1000 | Loss: 0.00003603
Iteration 87/1000 | Loss: 0.00003602
Iteration 88/1000 | Loss: 0.00003602
Iteration 89/1000 | Loss: 0.00003601
Iteration 90/1000 | Loss: 0.00003601
Iteration 91/1000 | Loss: 0.00003601
Iteration 92/1000 | Loss: 0.00003601
Iteration 93/1000 | Loss: 0.00003600
Iteration 94/1000 | Loss: 0.00003600
Iteration 95/1000 | Loss: 0.00003600
Iteration 96/1000 | Loss: 0.00003600
Iteration 97/1000 | Loss: 0.00003600
Iteration 98/1000 | Loss: 0.00003600
Iteration 99/1000 | Loss: 0.00003600
Iteration 100/1000 | Loss: 0.00003600
Iteration 101/1000 | Loss: 0.00003600
Iteration 102/1000 | Loss: 0.00003600
Iteration 103/1000 | Loss: 0.00003600
Iteration 104/1000 | Loss: 0.00003600
Iteration 105/1000 | Loss: 0.00003600
Iteration 106/1000 | Loss: 0.00003600
Iteration 107/1000 | Loss: 0.00003600
Iteration 108/1000 | Loss: 0.00003600
Iteration 109/1000 | Loss: 0.00003599
Iteration 110/1000 | Loss: 0.00003599
Iteration 111/1000 | Loss: 0.00003599
Iteration 112/1000 | Loss: 0.00003599
Iteration 113/1000 | Loss: 0.00003599
Iteration 114/1000 | Loss: 0.00003599
Iteration 115/1000 | Loss: 0.00003599
Iteration 116/1000 | Loss: 0.00003599
Iteration 117/1000 | Loss: 0.00003599
Iteration 118/1000 | Loss: 0.00003599
Iteration 119/1000 | Loss: 0.00003599
Iteration 120/1000 | Loss: 0.00003599
Iteration 121/1000 | Loss: 0.00003599
Iteration 122/1000 | Loss: 0.00003599
Iteration 123/1000 | Loss: 0.00003599
Iteration 124/1000 | Loss: 0.00003599
Iteration 125/1000 | Loss: 0.00003599
Iteration 126/1000 | Loss: 0.00003599
Iteration 127/1000 | Loss: 0.00003599
Iteration 128/1000 | Loss: 0.00003599
Iteration 129/1000 | Loss: 0.00003599
Iteration 130/1000 | Loss: 0.00003599
Iteration 131/1000 | Loss: 0.00003599
Iteration 132/1000 | Loss: 0.00003599
Iteration 133/1000 | Loss: 0.00003599
Iteration 134/1000 | Loss: 0.00003599
Iteration 135/1000 | Loss: 0.00003599
Iteration 136/1000 | Loss: 0.00003599
Iteration 137/1000 | Loss: 0.00003599
Iteration 138/1000 | Loss: 0.00003599
Iteration 139/1000 | Loss: 0.00003599
Iteration 140/1000 | Loss: 0.00003599
Iteration 141/1000 | Loss: 0.00003599
Iteration 142/1000 | Loss: 0.00003599
Iteration 143/1000 | Loss: 0.00003599
Iteration 144/1000 | Loss: 0.00003599
Iteration 145/1000 | Loss: 0.00003599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [3.598796683945693e-05, 3.598796683945693e-05, 3.598796683945693e-05, 3.598796683945693e-05, 3.598796683945693e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.598796683945693e-05

Optimization complete. Final v2v error: 4.866326332092285 mm

Highest mean error: 5.873945236206055 mm for frame 223

Lowest mean error: 4.373743534088135 mm for frame 17

Saving results

Total time: 84.69060254096985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093723
Iteration 2/25 | Loss: 0.00262346
Iteration 3/25 | Loss: 0.00129483
Iteration 4/25 | Loss: 0.00095407
Iteration 5/25 | Loss: 0.00087867
Iteration 6/25 | Loss: 0.00087516
Iteration 7/25 | Loss: 0.00093823
Iteration 8/25 | Loss: 0.00090058
Iteration 9/25 | Loss: 0.00093086
Iteration 10/25 | Loss: 0.00090173
Iteration 11/25 | Loss: 0.00089473
Iteration 12/25 | Loss: 0.00089676
Iteration 13/25 | Loss: 0.00086664
Iteration 14/25 | Loss: 0.00083887
Iteration 15/25 | Loss: 0.00082759
Iteration 16/25 | Loss: 0.00082571
Iteration 17/25 | Loss: 0.00081655
Iteration 18/25 | Loss: 0.00082156
Iteration 19/25 | Loss: 0.00082020
Iteration 20/25 | Loss: 0.00081840
Iteration 21/25 | Loss: 0.00080190
Iteration 22/25 | Loss: 0.00080982
Iteration 23/25 | Loss: 0.00080380
Iteration 24/25 | Loss: 0.00080009
Iteration 25/25 | Loss: 0.00080265

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.97329283
Iteration 2/25 | Loss: 0.00171895
Iteration 3/25 | Loss: 0.00156885
Iteration 4/25 | Loss: 0.00156885
Iteration 5/25 | Loss: 0.00156885
Iteration 6/25 | Loss: 0.00156884
Iteration 7/25 | Loss: 0.00156884
Iteration 8/25 | Loss: 0.00156884
Iteration 9/25 | Loss: 0.00156884
Iteration 10/25 | Loss: 0.00156884
Iteration 11/25 | Loss: 0.00156884
Iteration 12/25 | Loss: 0.00156884
Iteration 13/25 | Loss: 0.00156884
Iteration 14/25 | Loss: 0.00156884
Iteration 15/25 | Loss: 0.00156884
Iteration 16/25 | Loss: 0.00156884
Iteration 17/25 | Loss: 0.00156884
Iteration 18/25 | Loss: 0.00156884
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015688433777540922, 0.0015688433777540922, 0.0015688433777540922, 0.0015688433777540922, 0.0015688433777540922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015688433777540922

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156884
Iteration 2/1000 | Loss: 0.00032589
Iteration 3/1000 | Loss: 0.00028213
Iteration 4/1000 | Loss: 0.00089903
Iteration 5/1000 | Loss: 0.00050344
Iteration 6/1000 | Loss: 0.00059722
Iteration 7/1000 | Loss: 0.00029848
Iteration 8/1000 | Loss: 0.00049780
Iteration 9/1000 | Loss: 0.00064231
Iteration 10/1000 | Loss: 0.00074298
Iteration 11/1000 | Loss: 0.00064937
Iteration 12/1000 | Loss: 0.00053492
Iteration 13/1000 | Loss: 0.00073157
Iteration 14/1000 | Loss: 0.00143164
Iteration 15/1000 | Loss: 0.00074548
Iteration 16/1000 | Loss: 0.00067034
Iteration 17/1000 | Loss: 0.00052816
Iteration 18/1000 | Loss: 0.00038192
Iteration 19/1000 | Loss: 0.00068666
Iteration 20/1000 | Loss: 0.00031206
Iteration 21/1000 | Loss: 0.00078891
Iteration 22/1000 | Loss: 0.00073511
Iteration 23/1000 | Loss: 0.00069773
Iteration 24/1000 | Loss: 0.00062979
Iteration 25/1000 | Loss: 0.00104774
Iteration 26/1000 | Loss: 0.00068846
Iteration 27/1000 | Loss: 0.00018954
Iteration 28/1000 | Loss: 0.00039118
Iteration 29/1000 | Loss: 0.00060715
Iteration 30/1000 | Loss: 0.00059134
Iteration 31/1000 | Loss: 0.00042066
Iteration 32/1000 | Loss: 0.00028048
Iteration 33/1000 | Loss: 0.00023103
Iteration 34/1000 | Loss: 0.00018202
Iteration 35/1000 | Loss: 0.00042248
Iteration 36/1000 | Loss: 0.00022311
Iteration 37/1000 | Loss: 0.00024731
Iteration 38/1000 | Loss: 0.00011141
Iteration 39/1000 | Loss: 0.00011257
Iteration 40/1000 | Loss: 0.00068785
Iteration 41/1000 | Loss: 0.00037060
Iteration 42/1000 | Loss: 0.00034501
Iteration 43/1000 | Loss: 0.00022753
Iteration 44/1000 | Loss: 0.00045746
Iteration 45/1000 | Loss: 0.00013903
Iteration 46/1000 | Loss: 0.00022737
Iteration 47/1000 | Loss: 0.00018491
Iteration 48/1000 | Loss: 0.00018044
Iteration 49/1000 | Loss: 0.00051203
Iteration 50/1000 | Loss: 0.00013115
Iteration 51/1000 | Loss: 0.00029529
Iteration 52/1000 | Loss: 0.00024230
Iteration 53/1000 | Loss: 0.00009381
Iteration 54/1000 | Loss: 0.00009055
Iteration 55/1000 | Loss: 0.00022425
Iteration 56/1000 | Loss: 0.00023321
Iteration 57/1000 | Loss: 0.00010030
Iteration 58/1000 | Loss: 0.00026598
Iteration 59/1000 | Loss: 0.00019947
Iteration 60/1000 | Loss: 0.00045889
Iteration 61/1000 | Loss: 0.00025133
Iteration 62/1000 | Loss: 0.00029446
Iteration 63/1000 | Loss: 0.00026793
Iteration 64/1000 | Loss: 0.00014765
Iteration 65/1000 | Loss: 0.00020855
Iteration 66/1000 | Loss: 0.00023396
Iteration 67/1000 | Loss: 0.00018299
Iteration 68/1000 | Loss: 0.00013494
Iteration 69/1000 | Loss: 0.00043461
Iteration 70/1000 | Loss: 0.00052431
Iteration 71/1000 | Loss: 0.00025336
Iteration 72/1000 | Loss: 0.00043854
Iteration 73/1000 | Loss: 0.00031442
Iteration 74/1000 | Loss: 0.00017031
Iteration 75/1000 | Loss: 0.00018305
Iteration 76/1000 | Loss: 0.00024882
Iteration 77/1000 | Loss: 0.00012038
Iteration 78/1000 | Loss: 0.00017678
Iteration 79/1000 | Loss: 0.00019923
Iteration 80/1000 | Loss: 0.00016789
Iteration 81/1000 | Loss: 0.00023917
Iteration 82/1000 | Loss: 0.00023228
Iteration 83/1000 | Loss: 0.00023808
Iteration 84/1000 | Loss: 0.00016195
Iteration 85/1000 | Loss: 0.00030524
Iteration 86/1000 | Loss: 0.00018057
Iteration 87/1000 | Loss: 0.00018927
Iteration 88/1000 | Loss: 0.00028909
Iteration 89/1000 | Loss: 0.00048694
Iteration 90/1000 | Loss: 0.00018502
Iteration 91/1000 | Loss: 0.00028669
Iteration 92/1000 | Loss: 0.00023549
Iteration 93/1000 | Loss: 0.00042534
Iteration 94/1000 | Loss: 0.00032418
Iteration 95/1000 | Loss: 0.00047221
Iteration 96/1000 | Loss: 0.00017193
Iteration 97/1000 | Loss: 0.00007872
Iteration 98/1000 | Loss: 0.00063314
Iteration 99/1000 | Loss: 0.00030471
Iteration 100/1000 | Loss: 0.00024440
Iteration 101/1000 | Loss: 0.00030541
Iteration 102/1000 | Loss: 0.00013495
Iteration 103/1000 | Loss: 0.00006734
Iteration 104/1000 | Loss: 0.00010432
Iteration 105/1000 | Loss: 0.00009261
Iteration 106/1000 | Loss: 0.00024600
Iteration 107/1000 | Loss: 0.00029100
Iteration 108/1000 | Loss: 0.00042333
Iteration 109/1000 | Loss: 0.00072628
Iteration 110/1000 | Loss: 0.00031089
Iteration 111/1000 | Loss: 0.00028207
Iteration 112/1000 | Loss: 0.00021068
Iteration 113/1000 | Loss: 0.00027319
Iteration 114/1000 | Loss: 0.00032839
Iteration 115/1000 | Loss: 0.00010335
Iteration 116/1000 | Loss: 0.00008873
Iteration 117/1000 | Loss: 0.00008509
Iteration 118/1000 | Loss: 0.00006374
Iteration 119/1000 | Loss: 0.00009608
Iteration 120/1000 | Loss: 0.00008489
Iteration 121/1000 | Loss: 0.00009993
Iteration 122/1000 | Loss: 0.00028260
Iteration 123/1000 | Loss: 0.00009438
Iteration 124/1000 | Loss: 0.00008909
Iteration 125/1000 | Loss: 0.00009302
Iteration 126/1000 | Loss: 0.00007918
Iteration 127/1000 | Loss: 0.00009987
Iteration 128/1000 | Loss: 0.00009523
Iteration 129/1000 | Loss: 0.00009370
Iteration 130/1000 | Loss: 0.00008683
Iteration 131/1000 | Loss: 0.00008581
Iteration 132/1000 | Loss: 0.00007505
Iteration 133/1000 | Loss: 0.00025099
Iteration 134/1000 | Loss: 0.00008577
Iteration 135/1000 | Loss: 0.00008160
Iteration 136/1000 | Loss: 0.00007783
Iteration 137/1000 | Loss: 0.00040845
Iteration 138/1000 | Loss: 0.00022608
Iteration 139/1000 | Loss: 0.00024494
Iteration 140/1000 | Loss: 0.00020071
Iteration 141/1000 | Loss: 0.00012053
Iteration 142/1000 | Loss: 0.00025221
Iteration 143/1000 | Loss: 0.00022173
Iteration 144/1000 | Loss: 0.00015667
Iteration 145/1000 | Loss: 0.00021772
Iteration 146/1000 | Loss: 0.00019589
Iteration 147/1000 | Loss: 0.00093859
Iteration 148/1000 | Loss: 0.00023719
Iteration 149/1000 | Loss: 0.00010635
Iteration 150/1000 | Loss: 0.00009387
Iteration 151/1000 | Loss: 0.00009208
Iteration 152/1000 | Loss: 0.00008262
Iteration 153/1000 | Loss: 0.00008702
Iteration 154/1000 | Loss: 0.00007552
Iteration 155/1000 | Loss: 0.00005671
Iteration 156/1000 | Loss: 0.00029858
Iteration 157/1000 | Loss: 0.00009859
Iteration 158/1000 | Loss: 0.00009134
Iteration 159/1000 | Loss: 0.00009621
Iteration 160/1000 | Loss: 0.00007256
Iteration 161/1000 | Loss: 0.00008003
Iteration 162/1000 | Loss: 0.00008314
Iteration 163/1000 | Loss: 0.00004696
Iteration 164/1000 | Loss: 0.00008543
Iteration 165/1000 | Loss: 0.00016342
Iteration 166/1000 | Loss: 0.00004286
Iteration 167/1000 | Loss: 0.00012651
Iteration 168/1000 | Loss: 0.00011760
Iteration 169/1000 | Loss: 0.00009181
Iteration 170/1000 | Loss: 0.00009454
Iteration 171/1000 | Loss: 0.00006377
Iteration 172/1000 | Loss: 0.00009967
Iteration 173/1000 | Loss: 0.00007904
Iteration 174/1000 | Loss: 0.00014005
Iteration 175/1000 | Loss: 0.00009071
Iteration 176/1000 | Loss: 0.00007062
Iteration 177/1000 | Loss: 0.00007156
Iteration 178/1000 | Loss: 0.00047767
Iteration 179/1000 | Loss: 0.00010398
Iteration 180/1000 | Loss: 0.00009538
Iteration 181/1000 | Loss: 0.00008001
Iteration 182/1000 | Loss: 0.00012503
Iteration 183/1000 | Loss: 0.00004080
Iteration 184/1000 | Loss: 0.00008039
Iteration 185/1000 | Loss: 0.00011156
Iteration 186/1000 | Loss: 0.00008546
Iteration 187/1000 | Loss: 0.00009081
Iteration 188/1000 | Loss: 0.00008657
Iteration 189/1000 | Loss: 0.00007182
Iteration 190/1000 | Loss: 0.00010065
Iteration 191/1000 | Loss: 0.00008524
Iteration 192/1000 | Loss: 0.00009093
Iteration 193/1000 | Loss: 0.00011355
Iteration 194/1000 | Loss: 0.00009933
Iteration 195/1000 | Loss: 0.00008840
Iteration 196/1000 | Loss: 0.00014370
Iteration 197/1000 | Loss: 0.00011310
Iteration 198/1000 | Loss: 0.00013450
Iteration 199/1000 | Loss: 0.00014716
Iteration 200/1000 | Loss: 0.00010448
Iteration 201/1000 | Loss: 0.00008787
Iteration 202/1000 | Loss: 0.00009402
Iteration 203/1000 | Loss: 0.00009003
Iteration 204/1000 | Loss: 0.00010956
Iteration 205/1000 | Loss: 0.00009775
Iteration 206/1000 | Loss: 0.00009584
Iteration 207/1000 | Loss: 0.00010026
Iteration 208/1000 | Loss: 0.00009482
Iteration 209/1000 | Loss: 0.00009470
Iteration 210/1000 | Loss: 0.00008650
Iteration 211/1000 | Loss: 0.00011191
Iteration 212/1000 | Loss: 0.00008961
Iteration 213/1000 | Loss: 0.00006933
Iteration 214/1000 | Loss: 0.00006882
Iteration 215/1000 | Loss: 0.00009064
Iteration 216/1000 | Loss: 0.00007523
Iteration 217/1000 | Loss: 0.00009844
Iteration 218/1000 | Loss: 0.00009545
Iteration 219/1000 | Loss: 0.00008642
Iteration 220/1000 | Loss: 0.00009135
Iteration 221/1000 | Loss: 0.00008935
Iteration 222/1000 | Loss: 0.00011400
Iteration 223/1000 | Loss: 0.00009685
Iteration 224/1000 | Loss: 0.00010665
Iteration 225/1000 | Loss: 0.00009203
Iteration 226/1000 | Loss: 0.00009309
Iteration 227/1000 | Loss: 0.00008437
Iteration 228/1000 | Loss: 0.00011249
Iteration 229/1000 | Loss: 0.00008081
Iteration 230/1000 | Loss: 0.00010655
Iteration 231/1000 | Loss: 0.00010680
Iteration 232/1000 | Loss: 0.00010263
Iteration 233/1000 | Loss: 0.00008021
Iteration 234/1000 | Loss: 0.00009226
Iteration 235/1000 | Loss: 0.00007480
Iteration 236/1000 | Loss: 0.00008933
Iteration 237/1000 | Loss: 0.00008778
Iteration 238/1000 | Loss: 0.00009362
Iteration 239/1000 | Loss: 0.00008050
Iteration 240/1000 | Loss: 0.00009016
Iteration 241/1000 | Loss: 0.00005605
Iteration 242/1000 | Loss: 0.00014549
Iteration 243/1000 | Loss: 0.00002983
Iteration 244/1000 | Loss: 0.00009372
Iteration 245/1000 | Loss: 0.00006707
Iteration 246/1000 | Loss: 0.00006485
Iteration 247/1000 | Loss: 0.00002382
Iteration 248/1000 | Loss: 0.00001964
Iteration 249/1000 | Loss: 0.00001784
Iteration 250/1000 | Loss: 0.00001599
Iteration 251/1000 | Loss: 0.00001539
Iteration 252/1000 | Loss: 0.00001503
Iteration 253/1000 | Loss: 0.00001487
Iteration 254/1000 | Loss: 0.00001486
Iteration 255/1000 | Loss: 0.00001483
Iteration 256/1000 | Loss: 0.00001482
Iteration 257/1000 | Loss: 0.00001481
Iteration 258/1000 | Loss: 0.00001481
Iteration 259/1000 | Loss: 0.00001479
Iteration 260/1000 | Loss: 0.00001478
Iteration 261/1000 | Loss: 0.00001472
Iteration 262/1000 | Loss: 0.00001466
Iteration 263/1000 | Loss: 0.00001466
Iteration 264/1000 | Loss: 0.00001465
Iteration 265/1000 | Loss: 0.00001461
Iteration 266/1000 | Loss: 0.00001460
Iteration 267/1000 | Loss: 0.00001454
Iteration 268/1000 | Loss: 0.00001453
Iteration 269/1000 | Loss: 0.00001451
Iteration 270/1000 | Loss: 0.00001451
Iteration 271/1000 | Loss: 0.00001450
Iteration 272/1000 | Loss: 0.00001449
Iteration 273/1000 | Loss: 0.00001448
Iteration 274/1000 | Loss: 0.00001448
Iteration 275/1000 | Loss: 0.00001447
Iteration 276/1000 | Loss: 0.00001446
Iteration 277/1000 | Loss: 0.00001445
Iteration 278/1000 | Loss: 0.00001445
Iteration 279/1000 | Loss: 0.00001444
Iteration 280/1000 | Loss: 0.00001444
Iteration 281/1000 | Loss: 0.00001440
Iteration 282/1000 | Loss: 0.00001440
Iteration 283/1000 | Loss: 0.00001440
Iteration 284/1000 | Loss: 0.00001440
Iteration 285/1000 | Loss: 0.00001440
Iteration 286/1000 | Loss: 0.00001440
Iteration 287/1000 | Loss: 0.00001439
Iteration 288/1000 | Loss: 0.00001439
Iteration 289/1000 | Loss: 0.00001436
Iteration 290/1000 | Loss: 0.00001436
Iteration 291/1000 | Loss: 0.00001436
Iteration 292/1000 | Loss: 0.00001436
Iteration 293/1000 | Loss: 0.00001436
Iteration 294/1000 | Loss: 0.00001435
Iteration 295/1000 | Loss: 0.00001435
Iteration 296/1000 | Loss: 0.00001435
Iteration 297/1000 | Loss: 0.00001435
Iteration 298/1000 | Loss: 0.00001435
Iteration 299/1000 | Loss: 0.00001435
Iteration 300/1000 | Loss: 0.00001435
Iteration 301/1000 | Loss: 0.00001435
Iteration 302/1000 | Loss: 0.00001434
Iteration 303/1000 | Loss: 0.00001434
Iteration 304/1000 | Loss: 0.00001433
Iteration 305/1000 | Loss: 0.00001433
Iteration 306/1000 | Loss: 0.00001433
Iteration 307/1000 | Loss: 0.00001433
Iteration 308/1000 | Loss: 0.00001433
Iteration 309/1000 | Loss: 0.00001433
Iteration 310/1000 | Loss: 0.00001433
Iteration 311/1000 | Loss: 0.00001432
Iteration 312/1000 | Loss: 0.00001432
Iteration 313/1000 | Loss: 0.00001432
Iteration 314/1000 | Loss: 0.00001432
Iteration 315/1000 | Loss: 0.00001432
Iteration 316/1000 | Loss: 0.00001432
Iteration 317/1000 | Loss: 0.00001432
Iteration 318/1000 | Loss: 0.00001432
Iteration 319/1000 | Loss: 0.00001432
Iteration 320/1000 | Loss: 0.00001432
Iteration 321/1000 | Loss: 0.00001431
Iteration 322/1000 | Loss: 0.00001431
Iteration 323/1000 | Loss: 0.00001431
Iteration 324/1000 | Loss: 0.00001431
Iteration 325/1000 | Loss: 0.00001430
Iteration 326/1000 | Loss: 0.00001430
Iteration 327/1000 | Loss: 0.00001430
Iteration 328/1000 | Loss: 0.00001430
Iteration 329/1000 | Loss: 0.00001430
Iteration 330/1000 | Loss: 0.00001430
Iteration 331/1000 | Loss: 0.00001430
Iteration 332/1000 | Loss: 0.00001429
Iteration 333/1000 | Loss: 0.00001429
Iteration 334/1000 | Loss: 0.00001429
Iteration 335/1000 | Loss: 0.00001429
Iteration 336/1000 | Loss: 0.00001429
Iteration 337/1000 | Loss: 0.00001429
Iteration 338/1000 | Loss: 0.00001429
Iteration 339/1000 | Loss: 0.00001429
Iteration 340/1000 | Loss: 0.00001429
Iteration 341/1000 | Loss: 0.00001429
Iteration 342/1000 | Loss: 0.00001429
Iteration 343/1000 | Loss: 0.00001428
Iteration 344/1000 | Loss: 0.00001428
Iteration 345/1000 | Loss: 0.00001428
Iteration 346/1000 | Loss: 0.00001428
Iteration 347/1000 | Loss: 0.00001428
Iteration 348/1000 | Loss: 0.00001428
Iteration 349/1000 | Loss: 0.00001428
Iteration 350/1000 | Loss: 0.00001428
Iteration 351/1000 | Loss: 0.00001428
Iteration 352/1000 | Loss: 0.00001428
Iteration 353/1000 | Loss: 0.00001428
Iteration 354/1000 | Loss: 0.00001428
Iteration 355/1000 | Loss: 0.00001428
Iteration 356/1000 | Loss: 0.00001428
Iteration 357/1000 | Loss: 0.00001428
Iteration 358/1000 | Loss: 0.00001427
Iteration 359/1000 | Loss: 0.00001427
Iteration 360/1000 | Loss: 0.00001427
Iteration 361/1000 | Loss: 0.00001427
Iteration 362/1000 | Loss: 0.00001427
Iteration 363/1000 | Loss: 0.00001427
Iteration 364/1000 | Loss: 0.00001427
Iteration 365/1000 | Loss: 0.00001427
Iteration 366/1000 | Loss: 0.00001427
Iteration 367/1000 | Loss: 0.00001427
Iteration 368/1000 | Loss: 0.00001427
Iteration 369/1000 | Loss: 0.00001427
Iteration 370/1000 | Loss: 0.00001427
Iteration 371/1000 | Loss: 0.00001426
Iteration 372/1000 | Loss: 0.00001426
Iteration 373/1000 | Loss: 0.00001426
Iteration 374/1000 | Loss: 0.00001426
Iteration 375/1000 | Loss: 0.00001426
Iteration 376/1000 | Loss: 0.00001426
Iteration 377/1000 | Loss: 0.00001426
Iteration 378/1000 | Loss: 0.00001426
Iteration 379/1000 | Loss: 0.00001426
Iteration 380/1000 | Loss: 0.00001426
Iteration 381/1000 | Loss: 0.00001426
Iteration 382/1000 | Loss: 0.00001426
Iteration 383/1000 | Loss: 0.00001426
Iteration 384/1000 | Loss: 0.00001426
Iteration 385/1000 | Loss: 0.00001426
Iteration 386/1000 | Loss: 0.00001426
Iteration 387/1000 | Loss: 0.00001426
Iteration 388/1000 | Loss: 0.00001425
Iteration 389/1000 | Loss: 0.00001425
Iteration 390/1000 | Loss: 0.00001425
Iteration 391/1000 | Loss: 0.00001425
Iteration 392/1000 | Loss: 0.00001425
Iteration 393/1000 | Loss: 0.00001425
Iteration 394/1000 | Loss: 0.00001425
Iteration 395/1000 | Loss: 0.00001425
Iteration 396/1000 | Loss: 0.00001425
Iteration 397/1000 | Loss: 0.00001425
Iteration 398/1000 | Loss: 0.00001425
Iteration 399/1000 | Loss: 0.00001425
Iteration 400/1000 | Loss: 0.00001425
Iteration 401/1000 | Loss: 0.00001425
Iteration 402/1000 | Loss: 0.00001425
Iteration 403/1000 | Loss: 0.00001425
Iteration 404/1000 | Loss: 0.00001425
Iteration 405/1000 | Loss: 0.00001425
Iteration 406/1000 | Loss: 0.00001425
Iteration 407/1000 | Loss: 0.00001425
Iteration 408/1000 | Loss: 0.00001425
Iteration 409/1000 | Loss: 0.00001425
Iteration 410/1000 | Loss: 0.00001425
Iteration 411/1000 | Loss: 0.00001424
Iteration 412/1000 | Loss: 0.00001424
Iteration 413/1000 | Loss: 0.00001424
Iteration 414/1000 | Loss: 0.00001424
Iteration 415/1000 | Loss: 0.00001424
Iteration 416/1000 | Loss: 0.00001424
Iteration 417/1000 | Loss: 0.00001424
Iteration 418/1000 | Loss: 0.00001424
Iteration 419/1000 | Loss: 0.00001424
Iteration 420/1000 | Loss: 0.00001424
Iteration 421/1000 | Loss: 0.00001424
Iteration 422/1000 | Loss: 0.00001424
Iteration 423/1000 | Loss: 0.00001424
Iteration 424/1000 | Loss: 0.00001424
Iteration 425/1000 | Loss: 0.00001424
Iteration 426/1000 | Loss: 0.00001424
Iteration 427/1000 | Loss: 0.00001424
Iteration 428/1000 | Loss: 0.00001424
Iteration 429/1000 | Loss: 0.00001424
Iteration 430/1000 | Loss: 0.00001424
Iteration 431/1000 | Loss: 0.00001423
Iteration 432/1000 | Loss: 0.00001423
Iteration 433/1000 | Loss: 0.00001423
Iteration 434/1000 | Loss: 0.00001423
Iteration 435/1000 | Loss: 0.00001423
Iteration 436/1000 | Loss: 0.00001423
Iteration 437/1000 | Loss: 0.00001423
Iteration 438/1000 | Loss: 0.00001423
Iteration 439/1000 | Loss: 0.00001423
Iteration 440/1000 | Loss: 0.00001423
Iteration 441/1000 | Loss: 0.00001423
Iteration 442/1000 | Loss: 0.00001423
Iteration 443/1000 | Loss: 0.00001422
Iteration 444/1000 | Loss: 0.00001422
Iteration 445/1000 | Loss: 0.00001422
Iteration 446/1000 | Loss: 0.00001422
Iteration 447/1000 | Loss: 0.00001422
Iteration 448/1000 | Loss: 0.00001422
Iteration 449/1000 | Loss: 0.00001422
Iteration 450/1000 | Loss: 0.00001422
Iteration 451/1000 | Loss: 0.00001422
Iteration 452/1000 | Loss: 0.00001422
Iteration 453/1000 | Loss: 0.00001422
Iteration 454/1000 | Loss: 0.00001422
Iteration 455/1000 | Loss: 0.00001422
Iteration 456/1000 | Loss: 0.00001422
Iteration 457/1000 | Loss: 0.00001422
Iteration 458/1000 | Loss: 0.00001422
Iteration 459/1000 | Loss: 0.00001422
Iteration 460/1000 | Loss: 0.00001422
Iteration 461/1000 | Loss: 0.00001422
Iteration 462/1000 | Loss: 0.00001422
Iteration 463/1000 | Loss: 0.00001422
Iteration 464/1000 | Loss: 0.00001421
Iteration 465/1000 | Loss: 0.00001421
Iteration 466/1000 | Loss: 0.00001421
Iteration 467/1000 | Loss: 0.00001421
Iteration 468/1000 | Loss: 0.00001421
Iteration 469/1000 | Loss: 0.00001421
Iteration 470/1000 | Loss: 0.00001421
Iteration 471/1000 | Loss: 0.00001421
Iteration 472/1000 | Loss: 0.00001421
Iteration 473/1000 | Loss: 0.00001421
Iteration 474/1000 | Loss: 0.00001421
Iteration 475/1000 | Loss: 0.00001421
Iteration 476/1000 | Loss: 0.00001421
Iteration 477/1000 | Loss: 0.00001421
Iteration 478/1000 | Loss: 0.00001421
Iteration 479/1000 | Loss: 0.00001421
Iteration 480/1000 | Loss: 0.00001421
Iteration 481/1000 | Loss: 0.00001421
Iteration 482/1000 | Loss: 0.00001421
Iteration 483/1000 | Loss: 0.00001421
Iteration 484/1000 | Loss: 0.00001421
Iteration 485/1000 | Loss: 0.00001421
Iteration 486/1000 | Loss: 0.00001421
Iteration 487/1000 | Loss: 0.00001421
Iteration 488/1000 | Loss: 0.00001421
Iteration 489/1000 | Loss: 0.00001421
Iteration 490/1000 | Loss: 0.00001421
Iteration 491/1000 | Loss: 0.00001421
Iteration 492/1000 | Loss: 0.00001421
Iteration 493/1000 | Loss: 0.00001421
Iteration 494/1000 | Loss: 0.00001421
Iteration 495/1000 | Loss: 0.00001421
Iteration 496/1000 | Loss: 0.00001421
Iteration 497/1000 | Loss: 0.00001421
Iteration 498/1000 | Loss: 0.00001421
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 498. Stopping optimization.
Last 5 losses: [1.4209926121111494e-05, 1.4209926121111494e-05, 1.4209926121111494e-05, 1.4209926121111494e-05, 1.4209926121111494e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4209926121111494e-05

Optimization complete. Final v2v error: 3.2124075889587402 mm

Highest mean error: 3.9167838096618652 mm for frame 113

Lowest mean error: 2.9891903400421143 mm for frame 40

Saving results

Total time: 429.1699323654175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_elias_posed_011/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_elias_posed_011/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796532
Iteration 2/25 | Loss: 0.00200895
Iteration 3/25 | Loss: 0.00120152
Iteration 4/25 | Loss: 0.00088738
Iteration 5/25 | Loss: 0.00086878
Iteration 6/25 | Loss: 0.00080594
Iteration 7/25 | Loss: 0.00081164
Iteration 8/25 | Loss: 0.00075066
Iteration 9/25 | Loss: 0.00072806
Iteration 10/25 | Loss: 0.00072076
Iteration 11/25 | Loss: 0.00071451
Iteration 12/25 | Loss: 0.00070804
Iteration 13/25 | Loss: 0.00070643
Iteration 14/25 | Loss: 0.00070510
Iteration 15/25 | Loss: 0.00070478
Iteration 16/25 | Loss: 0.00070477
Iteration 17/25 | Loss: 0.00070477
Iteration 18/25 | Loss: 0.00070477
Iteration 19/25 | Loss: 0.00070474
Iteration 20/25 | Loss: 0.00070474
Iteration 21/25 | Loss: 0.00070474
Iteration 22/25 | Loss: 0.00070474
Iteration 23/25 | Loss: 0.00070474
Iteration 24/25 | Loss: 0.00070473
Iteration 25/25 | Loss: 0.00070473

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.20784855
Iteration 2/25 | Loss: 0.00081604
Iteration 3/25 | Loss: 0.00081603
Iteration 4/25 | Loss: 0.00081603
Iteration 5/25 | Loss: 0.00081603
Iteration 6/25 | Loss: 0.00081603
Iteration 7/25 | Loss: 0.00081603
Iteration 8/25 | Loss: 0.00081603
Iteration 9/25 | Loss: 0.00080791
Iteration 10/25 | Loss: 0.00080791
Iteration 11/25 | Loss: 0.00080790
Iteration 12/25 | Loss: 0.00080790
Iteration 13/25 | Loss: 0.00080790
Iteration 14/25 | Loss: 0.00080790
Iteration 15/25 | Loss: 0.00080790
Iteration 16/25 | Loss: 0.00080790
Iteration 17/25 | Loss: 0.00080790
Iteration 18/25 | Loss: 0.00080790
Iteration 19/25 | Loss: 0.00080790
Iteration 20/25 | Loss: 0.00080790
Iteration 21/25 | Loss: 0.00080790
Iteration 22/25 | Loss: 0.00080790
Iteration 23/25 | Loss: 0.00080790
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008079031831584871, 0.0008079031831584871, 0.0008079031831584871, 0.0008079031831584871, 0.0008079031831584871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008079031831584871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080790
Iteration 2/1000 | Loss: 0.00002575
Iteration 3/1000 | Loss: 0.00009026
Iteration 4/1000 | Loss: 0.00006364
Iteration 5/1000 | Loss: 0.00011247
Iteration 6/1000 | Loss: 0.00016569
Iteration 7/1000 | Loss: 0.00001810
Iteration 8/1000 | Loss: 0.00001359
Iteration 9/1000 | Loss: 0.00001353
Iteration 10/1000 | Loss: 0.00001401
Iteration 11/1000 | Loss: 0.00009734
Iteration 12/1000 | Loss: 0.00002330
Iteration 13/1000 | Loss: 0.00002467
Iteration 14/1000 | Loss: 0.00001997
Iteration 15/1000 | Loss: 0.00001660
Iteration 16/1000 | Loss: 0.00001323
Iteration 17/1000 | Loss: 0.00003058
Iteration 18/1000 | Loss: 0.00001904
Iteration 19/1000 | Loss: 0.00001380
Iteration 20/1000 | Loss: 0.00001262
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001262
Iteration 23/1000 | Loss: 0.00001262
Iteration 24/1000 | Loss: 0.00001262
Iteration 25/1000 | Loss: 0.00001261
Iteration 26/1000 | Loss: 0.00001261
Iteration 27/1000 | Loss: 0.00001261
Iteration 28/1000 | Loss: 0.00001260
Iteration 29/1000 | Loss: 0.00001255
Iteration 30/1000 | Loss: 0.00001250
Iteration 31/1000 | Loss: 0.00004408
Iteration 32/1000 | Loss: 0.00001401
Iteration 33/1000 | Loss: 0.00018715
Iteration 34/1000 | Loss: 0.00003407
Iteration 35/1000 | Loss: 0.00009301
Iteration 36/1000 | Loss: 0.00025661
Iteration 37/1000 | Loss: 0.00001886
Iteration 38/1000 | Loss: 0.00003226
Iteration 39/1000 | Loss: 0.00001230
Iteration 40/1000 | Loss: 0.00001229
Iteration 41/1000 | Loss: 0.00001229
Iteration 42/1000 | Loss: 0.00001229
Iteration 43/1000 | Loss: 0.00001229
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001229
Iteration 46/1000 | Loss: 0.00001228
Iteration 47/1000 | Loss: 0.00001228
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001228
Iteration 50/1000 | Loss: 0.00001228
Iteration 51/1000 | Loss: 0.00001227
Iteration 52/1000 | Loss: 0.00001227
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001227
Iteration 55/1000 | Loss: 0.00001227
Iteration 56/1000 | Loss: 0.00001227
Iteration 57/1000 | Loss: 0.00001227
Iteration 58/1000 | Loss: 0.00001227
Iteration 59/1000 | Loss: 0.00001227
Iteration 60/1000 | Loss: 0.00001227
Iteration 61/1000 | Loss: 0.00001227
Iteration 62/1000 | Loss: 0.00001227
Iteration 63/1000 | Loss: 0.00001227
Iteration 64/1000 | Loss: 0.00001227
Iteration 65/1000 | Loss: 0.00001227
Iteration 66/1000 | Loss: 0.00001227
Iteration 67/1000 | Loss: 0.00001227
Iteration 68/1000 | Loss: 0.00001227
Iteration 69/1000 | Loss: 0.00001227
Iteration 70/1000 | Loss: 0.00001227
Iteration 71/1000 | Loss: 0.00001227
Iteration 72/1000 | Loss: 0.00001227
Iteration 73/1000 | Loss: 0.00001227
Iteration 74/1000 | Loss: 0.00001227
Iteration 75/1000 | Loss: 0.00001227
Iteration 76/1000 | Loss: 0.00001227
Iteration 77/1000 | Loss: 0.00001227
Iteration 78/1000 | Loss: 0.00001227
Iteration 79/1000 | Loss: 0.00001227
Iteration 80/1000 | Loss: 0.00001227
Iteration 81/1000 | Loss: 0.00001227
Iteration 82/1000 | Loss: 0.00001227
Iteration 83/1000 | Loss: 0.00001227
Iteration 84/1000 | Loss: 0.00001227
Iteration 85/1000 | Loss: 0.00001227
Iteration 86/1000 | Loss: 0.00001227
Iteration 87/1000 | Loss: 0.00001227
Iteration 88/1000 | Loss: 0.00001227
Iteration 89/1000 | Loss: 0.00001227
Iteration 90/1000 | Loss: 0.00001227
Iteration 91/1000 | Loss: 0.00001227
Iteration 92/1000 | Loss: 0.00001227
Iteration 93/1000 | Loss: 0.00001227
Iteration 94/1000 | Loss: 0.00001227
Iteration 95/1000 | Loss: 0.00001227
Iteration 96/1000 | Loss: 0.00001227
Iteration 97/1000 | Loss: 0.00001227
Iteration 98/1000 | Loss: 0.00001227
Iteration 99/1000 | Loss: 0.00001227
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.2265494660823606e-05, 1.2265494660823606e-05, 1.2265494660823606e-05, 1.2265494660823606e-05, 1.2265494660823606e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2265494660823606e-05

Optimization complete. Final v2v error: 2.9772703647613525 mm

Highest mean error: 3.230203151702881 mm for frame 61

Lowest mean error: 2.708211660385132 mm for frame 148

Saving results

Total time: 73.74235892295837
