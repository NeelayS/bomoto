Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=155, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 8680-8735
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889534
Iteration 2/25 | Loss: 0.00237643
Iteration 3/25 | Loss: 0.00170664
Iteration 4/25 | Loss: 0.00136293
Iteration 5/25 | Loss: 0.00133747
Iteration 6/25 | Loss: 0.00131832
Iteration 7/25 | Loss: 0.00130610
Iteration 8/25 | Loss: 0.00129380
Iteration 9/25 | Loss: 0.00127554
Iteration 10/25 | Loss: 0.00124731
Iteration 11/25 | Loss: 0.00124131
Iteration 12/25 | Loss: 0.00123726
Iteration 13/25 | Loss: 0.00122816
Iteration 14/25 | Loss: 0.00122541
Iteration 15/25 | Loss: 0.00122611
Iteration 16/25 | Loss: 0.00122612
Iteration 17/25 | Loss: 0.00122479
Iteration 18/25 | Loss: 0.00121793
Iteration 19/25 | Loss: 0.00121588
Iteration 20/25 | Loss: 0.00122018
Iteration 21/25 | Loss: 0.00122256
Iteration 22/25 | Loss: 0.00121901
Iteration 23/25 | Loss: 0.00122224
Iteration 24/25 | Loss: 0.00122190
Iteration 25/25 | Loss: 0.00122060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.25326538
Iteration 2/25 | Loss: 0.00337883
Iteration 3/25 | Loss: 0.00337883
Iteration 4/25 | Loss: 0.00337883
Iteration 5/25 | Loss: 0.00337883
Iteration 6/25 | Loss: 0.00337883
Iteration 7/25 | Loss: 0.00337883
Iteration 8/25 | Loss: 0.00337883
Iteration 9/25 | Loss: 0.00337883
Iteration 10/25 | Loss: 0.00337883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0033788273576647043, 0.0033788273576647043, 0.0033788273576647043, 0.0033788273576647043, 0.0033788273576647043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0033788273576647043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00337883
Iteration 2/1000 | Loss: 0.00078474
Iteration 3/1000 | Loss: 0.00097506
Iteration 4/1000 | Loss: 0.00086065
Iteration 5/1000 | Loss: 0.00066003
Iteration 6/1000 | Loss: 0.00017134
Iteration 7/1000 | Loss: 0.00019694
Iteration 8/1000 | Loss: 0.00044400
Iteration 9/1000 | Loss: 0.00035564
Iteration 10/1000 | Loss: 0.00092736
Iteration 11/1000 | Loss: 0.00031428
Iteration 12/1000 | Loss: 0.00007785
Iteration 13/1000 | Loss: 0.00026881
Iteration 14/1000 | Loss: 0.00022324
Iteration 15/1000 | Loss: 0.00007043
Iteration 16/1000 | Loss: 0.00005441
Iteration 17/1000 | Loss: 0.00004853
Iteration 18/1000 | Loss: 0.00015205
Iteration 19/1000 | Loss: 0.00005479
Iteration 20/1000 | Loss: 0.00004559
Iteration 21/1000 | Loss: 0.00005639
Iteration 22/1000 | Loss: 0.00005335
Iteration 23/1000 | Loss: 0.00004419
Iteration 24/1000 | Loss: 0.00004429
Iteration 25/1000 | Loss: 0.00003844
Iteration 26/1000 | Loss: 0.00004565
Iteration 27/1000 | Loss: 0.00024491
Iteration 28/1000 | Loss: 0.00006493
Iteration 29/1000 | Loss: 0.00006055
Iteration 30/1000 | Loss: 0.00005352
Iteration 31/1000 | Loss: 0.00004623
Iteration 32/1000 | Loss: 0.00004298
Iteration 33/1000 | Loss: 0.00005469
Iteration 34/1000 | Loss: 0.00003900
Iteration 35/1000 | Loss: 0.00003405
Iteration 36/1000 | Loss: 0.00003144
Iteration 37/1000 | Loss: 0.00003084
Iteration 38/1000 | Loss: 0.00003050
Iteration 39/1000 | Loss: 0.00003010
Iteration 40/1000 | Loss: 0.00003360
Iteration 41/1000 | Loss: 0.00002929
Iteration 42/1000 | Loss: 0.00002868
Iteration 43/1000 | Loss: 0.00002826
Iteration 44/1000 | Loss: 0.00002802
Iteration 45/1000 | Loss: 0.00002789
Iteration 46/1000 | Loss: 0.00002763
Iteration 47/1000 | Loss: 0.00002726
Iteration 48/1000 | Loss: 0.00003918
Iteration 49/1000 | Loss: 0.00003558
Iteration 50/1000 | Loss: 0.00005082
Iteration 51/1000 | Loss: 0.00003989
Iteration 52/1000 | Loss: 0.00003144
Iteration 53/1000 | Loss: 0.00002856
Iteration 54/1000 | Loss: 0.00002724
Iteration 55/1000 | Loss: 0.00002652
Iteration 56/1000 | Loss: 0.00002622
Iteration 57/1000 | Loss: 0.00002580
Iteration 58/1000 | Loss: 0.00002558
Iteration 59/1000 | Loss: 0.00002556
Iteration 60/1000 | Loss: 0.00002555
Iteration 61/1000 | Loss: 0.00002554
Iteration 62/1000 | Loss: 0.00002551
Iteration 63/1000 | Loss: 0.00002550
Iteration 64/1000 | Loss: 0.00002549
Iteration 65/1000 | Loss: 0.00002549
Iteration 66/1000 | Loss: 0.00002545
Iteration 67/1000 | Loss: 0.00002545
Iteration 68/1000 | Loss: 0.00002544
Iteration 69/1000 | Loss: 0.00002543
Iteration 70/1000 | Loss: 0.00002543
Iteration 71/1000 | Loss: 0.00002543
Iteration 72/1000 | Loss: 0.00002542
Iteration 73/1000 | Loss: 0.00002542
Iteration 74/1000 | Loss: 0.00002542
Iteration 75/1000 | Loss: 0.00002542
Iteration 76/1000 | Loss: 0.00002541
Iteration 77/1000 | Loss: 0.00002541
Iteration 78/1000 | Loss: 0.00002541
Iteration 79/1000 | Loss: 0.00002540
Iteration 80/1000 | Loss: 0.00002540
Iteration 81/1000 | Loss: 0.00002540
Iteration 82/1000 | Loss: 0.00002540
Iteration 83/1000 | Loss: 0.00002540
Iteration 84/1000 | Loss: 0.00002540
Iteration 85/1000 | Loss: 0.00002539
Iteration 86/1000 | Loss: 0.00002539
Iteration 87/1000 | Loss: 0.00002539
Iteration 88/1000 | Loss: 0.00002539
Iteration 89/1000 | Loss: 0.00002539
Iteration 90/1000 | Loss: 0.00002539
Iteration 91/1000 | Loss: 0.00002539
Iteration 92/1000 | Loss: 0.00002538
Iteration 93/1000 | Loss: 0.00002538
Iteration 94/1000 | Loss: 0.00002538
Iteration 95/1000 | Loss: 0.00002538
Iteration 96/1000 | Loss: 0.00002537
Iteration 97/1000 | Loss: 0.00002537
Iteration 98/1000 | Loss: 0.00002537
Iteration 99/1000 | Loss: 0.00002536
Iteration 100/1000 | Loss: 0.00002536
Iteration 101/1000 | Loss: 0.00002536
Iteration 102/1000 | Loss: 0.00002536
Iteration 103/1000 | Loss: 0.00002536
Iteration 104/1000 | Loss: 0.00002536
Iteration 105/1000 | Loss: 0.00002536
Iteration 106/1000 | Loss: 0.00002536
Iteration 107/1000 | Loss: 0.00002536
Iteration 108/1000 | Loss: 0.00002536
Iteration 109/1000 | Loss: 0.00002536
Iteration 110/1000 | Loss: 0.00002536
Iteration 111/1000 | Loss: 0.00002535
Iteration 112/1000 | Loss: 0.00002535
Iteration 113/1000 | Loss: 0.00002535
Iteration 114/1000 | Loss: 0.00002535
Iteration 115/1000 | Loss: 0.00002535
Iteration 116/1000 | Loss: 0.00002535
Iteration 117/1000 | Loss: 0.00002535
Iteration 118/1000 | Loss: 0.00002535
Iteration 119/1000 | Loss: 0.00002535
Iteration 120/1000 | Loss: 0.00002534
Iteration 121/1000 | Loss: 0.00002534
Iteration 122/1000 | Loss: 0.00002534
Iteration 123/1000 | Loss: 0.00002534
Iteration 124/1000 | Loss: 0.00002534
Iteration 125/1000 | Loss: 0.00002534
Iteration 126/1000 | Loss: 0.00002534
Iteration 127/1000 | Loss: 0.00002534
Iteration 128/1000 | Loss: 0.00002534
Iteration 129/1000 | Loss: 0.00002534
Iteration 130/1000 | Loss: 0.00002534
Iteration 131/1000 | Loss: 0.00002534
Iteration 132/1000 | Loss: 0.00002533
Iteration 133/1000 | Loss: 0.00002533
Iteration 134/1000 | Loss: 0.00002533
Iteration 135/1000 | Loss: 0.00002533
Iteration 136/1000 | Loss: 0.00002533
Iteration 137/1000 | Loss: 0.00002533
Iteration 138/1000 | Loss: 0.00002533
Iteration 139/1000 | Loss: 0.00002533
Iteration 140/1000 | Loss: 0.00002533
Iteration 141/1000 | Loss: 0.00002533
Iteration 142/1000 | Loss: 0.00002533
Iteration 143/1000 | Loss: 0.00002533
Iteration 144/1000 | Loss: 0.00002533
Iteration 145/1000 | Loss: 0.00002533
Iteration 146/1000 | Loss: 0.00002533
Iteration 147/1000 | Loss: 0.00002533
Iteration 148/1000 | Loss: 0.00002533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 148. Stopping optimization.
Last 5 losses: [2.5332634322694503e-05, 2.5332634322694503e-05, 2.5332634322694503e-05, 2.5332634322694503e-05, 2.5332634322694503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5332634322694503e-05

Optimization complete. Final v2v error: 3.497426986694336 mm

Highest mean error: 12.605069160461426 mm for frame 56

Lowest mean error: 2.4232919216156006 mm for frame 156

Saving results

Total time: 144.83411026000977
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075613
Iteration 2/25 | Loss: 0.00230989
Iteration 3/25 | Loss: 0.00203432
Iteration 4/25 | Loss: 0.00135651
Iteration 5/25 | Loss: 0.00126535
Iteration 6/25 | Loss: 0.00136323
Iteration 7/25 | Loss: 0.00123191
Iteration 8/25 | Loss: 0.00110553
Iteration 9/25 | Loss: 0.00105373
Iteration 10/25 | Loss: 0.00104130
Iteration 11/25 | Loss: 0.00103106
Iteration 12/25 | Loss: 0.00102720
Iteration 13/25 | Loss: 0.00102596
Iteration 14/25 | Loss: 0.00102741
Iteration 15/25 | Loss: 0.00102671
Iteration 16/25 | Loss: 0.00102747
Iteration 17/25 | Loss: 0.00102581
Iteration 18/25 | Loss: 0.00102646
Iteration 19/25 | Loss: 0.00102493
Iteration 20/25 | Loss: 0.00102545
Iteration 21/25 | Loss: 0.00102700
Iteration 22/25 | Loss: 0.00102669
Iteration 23/25 | Loss: 0.00102635
Iteration 24/25 | Loss: 0.00102483
Iteration 25/25 | Loss: 0.00102797

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34957421
Iteration 2/25 | Loss: 0.00091692
Iteration 3/25 | Loss: 0.00091692
Iteration 4/25 | Loss: 0.00091692
Iteration 5/25 | Loss: 0.00091692
Iteration 6/25 | Loss: 0.00091692
Iteration 7/25 | Loss: 0.00091692
Iteration 8/25 | Loss: 0.00091692
Iteration 9/25 | Loss: 0.00091692
Iteration 10/25 | Loss: 0.00091692
Iteration 11/25 | Loss: 0.00091692
Iteration 12/25 | Loss: 0.00091692
Iteration 13/25 | Loss: 0.00091692
Iteration 14/25 | Loss: 0.00091692
Iteration 15/25 | Loss: 0.00091692
Iteration 16/25 | Loss: 0.00091692
Iteration 17/25 | Loss: 0.00091692
Iteration 18/25 | Loss: 0.00091692
Iteration 19/25 | Loss: 0.00091692
Iteration 20/25 | Loss: 0.00091692
Iteration 21/25 | Loss: 0.00091692
Iteration 22/25 | Loss: 0.00091692
Iteration 23/25 | Loss: 0.00091692
Iteration 24/25 | Loss: 0.00091692
Iteration 25/25 | Loss: 0.00091692

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091692
Iteration 2/1000 | Loss: 0.00009202
Iteration 3/1000 | Loss: 0.00004650
Iteration 4/1000 | Loss: 0.00003613
Iteration 5/1000 | Loss: 0.00003240
Iteration 6/1000 | Loss: 0.00018211
Iteration 7/1000 | Loss: 0.00014016
Iteration 8/1000 | Loss: 0.00005003
Iteration 9/1000 | Loss: 0.00003681
Iteration 10/1000 | Loss: 0.00003010
Iteration 11/1000 | Loss: 0.00002494
Iteration 12/1000 | Loss: 0.00002127
Iteration 13/1000 | Loss: 0.00001982
Iteration 14/1000 | Loss: 0.00001899
Iteration 15/1000 | Loss: 0.00001847
Iteration 16/1000 | Loss: 0.00001810
Iteration 17/1000 | Loss: 0.00001767
Iteration 18/1000 | Loss: 0.00001744
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001725
Iteration 22/1000 | Loss: 0.00001725
Iteration 23/1000 | Loss: 0.00001715
Iteration 24/1000 | Loss: 0.00101205
Iteration 25/1000 | Loss: 0.00002776
Iteration 26/1000 | Loss: 0.00001826
Iteration 27/1000 | Loss: 0.00001631
Iteration 28/1000 | Loss: 0.00001556
Iteration 29/1000 | Loss: 0.00001505
Iteration 30/1000 | Loss: 0.00001473
Iteration 31/1000 | Loss: 0.00001455
Iteration 32/1000 | Loss: 0.00001450
Iteration 33/1000 | Loss: 0.00001450
Iteration 34/1000 | Loss: 0.00001449
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001447
Iteration 37/1000 | Loss: 0.00001440
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001433
Iteration 40/1000 | Loss: 0.00001432
Iteration 41/1000 | Loss: 0.00001432
Iteration 42/1000 | Loss: 0.00001427
Iteration 43/1000 | Loss: 0.00001426
Iteration 44/1000 | Loss: 0.00001426
Iteration 45/1000 | Loss: 0.00001425
Iteration 46/1000 | Loss: 0.00001424
Iteration 47/1000 | Loss: 0.00001422
Iteration 48/1000 | Loss: 0.00001422
Iteration 49/1000 | Loss: 0.00001421
Iteration 50/1000 | Loss: 0.00001421
Iteration 51/1000 | Loss: 0.00001421
Iteration 52/1000 | Loss: 0.00001421
Iteration 53/1000 | Loss: 0.00001420
Iteration 54/1000 | Loss: 0.00001420
Iteration 55/1000 | Loss: 0.00001420
Iteration 56/1000 | Loss: 0.00001419
Iteration 57/1000 | Loss: 0.00001419
Iteration 58/1000 | Loss: 0.00001418
Iteration 59/1000 | Loss: 0.00001418
Iteration 60/1000 | Loss: 0.00001418
Iteration 61/1000 | Loss: 0.00001418
Iteration 62/1000 | Loss: 0.00001418
Iteration 63/1000 | Loss: 0.00001417
Iteration 64/1000 | Loss: 0.00001417
Iteration 65/1000 | Loss: 0.00001417
Iteration 66/1000 | Loss: 0.00001417
Iteration 67/1000 | Loss: 0.00001417
Iteration 68/1000 | Loss: 0.00001417
Iteration 69/1000 | Loss: 0.00001417
Iteration 70/1000 | Loss: 0.00001416
Iteration 71/1000 | Loss: 0.00001416
Iteration 72/1000 | Loss: 0.00001416
Iteration 73/1000 | Loss: 0.00001416
Iteration 74/1000 | Loss: 0.00001416
Iteration 75/1000 | Loss: 0.00001416
Iteration 76/1000 | Loss: 0.00001415
Iteration 77/1000 | Loss: 0.00001415
Iteration 78/1000 | Loss: 0.00001415
Iteration 79/1000 | Loss: 0.00001415
Iteration 80/1000 | Loss: 0.00001414
Iteration 81/1000 | Loss: 0.00001414
Iteration 82/1000 | Loss: 0.00001414
Iteration 83/1000 | Loss: 0.00001414
Iteration 84/1000 | Loss: 0.00001414
Iteration 85/1000 | Loss: 0.00001414
Iteration 86/1000 | Loss: 0.00001414
Iteration 87/1000 | Loss: 0.00001413
Iteration 88/1000 | Loss: 0.00001413
Iteration 89/1000 | Loss: 0.00001413
Iteration 90/1000 | Loss: 0.00001413
Iteration 91/1000 | Loss: 0.00001413
Iteration 92/1000 | Loss: 0.00001412
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001412
Iteration 95/1000 | Loss: 0.00001412
Iteration 96/1000 | Loss: 0.00001412
Iteration 97/1000 | Loss: 0.00001412
Iteration 98/1000 | Loss: 0.00001412
Iteration 99/1000 | Loss: 0.00001412
Iteration 100/1000 | Loss: 0.00001412
Iteration 101/1000 | Loss: 0.00001412
Iteration 102/1000 | Loss: 0.00001412
Iteration 103/1000 | Loss: 0.00001412
Iteration 104/1000 | Loss: 0.00001412
Iteration 105/1000 | Loss: 0.00001412
Iteration 106/1000 | Loss: 0.00001411
Iteration 107/1000 | Loss: 0.00001411
Iteration 108/1000 | Loss: 0.00001411
Iteration 109/1000 | Loss: 0.00001410
Iteration 110/1000 | Loss: 0.00001410
Iteration 111/1000 | Loss: 0.00001410
Iteration 112/1000 | Loss: 0.00001410
Iteration 113/1000 | Loss: 0.00001410
Iteration 114/1000 | Loss: 0.00001410
Iteration 115/1000 | Loss: 0.00001410
Iteration 116/1000 | Loss: 0.00001410
Iteration 117/1000 | Loss: 0.00001410
Iteration 118/1000 | Loss: 0.00001410
Iteration 119/1000 | Loss: 0.00001410
Iteration 120/1000 | Loss: 0.00001410
Iteration 121/1000 | Loss: 0.00001410
Iteration 122/1000 | Loss: 0.00001410
Iteration 123/1000 | Loss: 0.00001410
Iteration 124/1000 | Loss: 0.00001409
Iteration 125/1000 | Loss: 0.00001409
Iteration 126/1000 | Loss: 0.00001409
Iteration 127/1000 | Loss: 0.00001409
Iteration 128/1000 | Loss: 0.00001409
Iteration 129/1000 | Loss: 0.00001409
Iteration 130/1000 | Loss: 0.00001409
Iteration 131/1000 | Loss: 0.00001409
Iteration 132/1000 | Loss: 0.00001409
Iteration 133/1000 | Loss: 0.00001409
Iteration 134/1000 | Loss: 0.00001409
Iteration 135/1000 | Loss: 0.00001409
Iteration 136/1000 | Loss: 0.00001409
Iteration 137/1000 | Loss: 0.00001409
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.4089240721659735e-05, 1.4089240721659735e-05, 1.4089240721659735e-05, 1.4089240721659735e-05, 1.4089240721659735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4089240721659735e-05

Optimization complete. Final v2v error: 3.0706264972686768 mm

Highest mean error: 5.059601306915283 mm for frame 66

Lowest mean error: 2.473111867904663 mm for frame 43

Saving results

Total time: 96.44961524009705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094888
Iteration 2/25 | Loss: 0.00128732
Iteration 3/25 | Loss: 0.00107809
Iteration 4/25 | Loss: 0.00097139
Iteration 5/25 | Loss: 0.00095475
Iteration 6/25 | Loss: 0.00095099
Iteration 7/25 | Loss: 0.00095013
Iteration 8/25 | Loss: 0.00095020
Iteration 9/25 | Loss: 0.00094893
Iteration 10/25 | Loss: 0.00094861
Iteration 11/25 | Loss: 0.00094848
Iteration 12/25 | Loss: 0.00094846
Iteration 13/25 | Loss: 0.00094846
Iteration 14/25 | Loss: 0.00094846
Iteration 15/25 | Loss: 0.00094846
Iteration 16/25 | Loss: 0.00094846
Iteration 17/25 | Loss: 0.00094846
Iteration 18/25 | Loss: 0.00094846
Iteration 19/25 | Loss: 0.00094846
Iteration 20/25 | Loss: 0.00094846
Iteration 21/25 | Loss: 0.00094846
Iteration 22/25 | Loss: 0.00094846
Iteration 23/25 | Loss: 0.00094846
Iteration 24/25 | Loss: 0.00094845
Iteration 25/25 | Loss: 0.00094845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.49957037
Iteration 2/25 | Loss: 0.00065391
Iteration 3/25 | Loss: 0.00065390
Iteration 4/25 | Loss: 0.00065390
Iteration 5/25 | Loss: 0.00065390
Iteration 6/25 | Loss: 0.00065390
Iteration 7/25 | Loss: 0.00065390
Iteration 8/25 | Loss: 0.00065390
Iteration 9/25 | Loss: 0.00065390
Iteration 10/25 | Loss: 0.00065389
Iteration 11/25 | Loss: 0.00065389
Iteration 12/25 | Loss: 0.00065389
Iteration 13/25 | Loss: 0.00065389
Iteration 14/25 | Loss: 0.00065389
Iteration 15/25 | Loss: 0.00065389
Iteration 16/25 | Loss: 0.00065389
Iteration 17/25 | Loss: 0.00065389
Iteration 18/25 | Loss: 0.00065389
Iteration 19/25 | Loss: 0.00065389
Iteration 20/25 | Loss: 0.00065389
Iteration 21/25 | Loss: 0.00065389
Iteration 22/25 | Loss: 0.00065389
Iteration 23/25 | Loss: 0.00065389
Iteration 24/25 | Loss: 0.00065389
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0006538942106999457, 0.0006538942106999457, 0.0006538942106999457, 0.0006538942106999457, 0.0006538942106999457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006538942106999457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065389
Iteration 2/1000 | Loss: 0.00002125
Iteration 3/1000 | Loss: 0.00001518
Iteration 4/1000 | Loss: 0.00001399
Iteration 5/1000 | Loss: 0.00001340
Iteration 6/1000 | Loss: 0.00001294
Iteration 7/1000 | Loss: 0.00001264
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001228
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001223
Iteration 12/1000 | Loss: 0.00001221
Iteration 13/1000 | Loss: 0.00001214
Iteration 14/1000 | Loss: 0.00001214
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001208
Iteration 17/1000 | Loss: 0.00001206
Iteration 18/1000 | Loss: 0.00001206
Iteration 19/1000 | Loss: 0.00001204
Iteration 20/1000 | Loss: 0.00001204
Iteration 21/1000 | Loss: 0.00001203
Iteration 22/1000 | Loss: 0.00001202
Iteration 23/1000 | Loss: 0.00001202
Iteration 24/1000 | Loss: 0.00001202
Iteration 25/1000 | Loss: 0.00001201
Iteration 26/1000 | Loss: 0.00001200
Iteration 27/1000 | Loss: 0.00001200
Iteration 28/1000 | Loss: 0.00001200
Iteration 29/1000 | Loss: 0.00001199
Iteration 30/1000 | Loss: 0.00001199
Iteration 31/1000 | Loss: 0.00001199
Iteration 32/1000 | Loss: 0.00001199
Iteration 33/1000 | Loss: 0.00001199
Iteration 34/1000 | Loss: 0.00001198
Iteration 35/1000 | Loss: 0.00001198
Iteration 36/1000 | Loss: 0.00001197
Iteration 37/1000 | Loss: 0.00001197
Iteration 38/1000 | Loss: 0.00001197
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001196
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001195
Iteration 46/1000 | Loss: 0.00001195
Iteration 47/1000 | Loss: 0.00001195
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001194
Iteration 51/1000 | Loss: 0.00001194
Iteration 52/1000 | Loss: 0.00001194
Iteration 53/1000 | Loss: 0.00001194
Iteration 54/1000 | Loss: 0.00001194
Iteration 55/1000 | Loss: 0.00001193
Iteration 56/1000 | Loss: 0.00001193
Iteration 57/1000 | Loss: 0.00001193
Iteration 58/1000 | Loss: 0.00001193
Iteration 59/1000 | Loss: 0.00001193
Iteration 60/1000 | Loss: 0.00001193
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001192
Iteration 67/1000 | Loss: 0.00001192
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00001191
Iteration 72/1000 | Loss: 0.00001190
Iteration 73/1000 | Loss: 0.00001190
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001189
Iteration 78/1000 | Loss: 0.00001189
Iteration 79/1000 | Loss: 0.00001189
Iteration 80/1000 | Loss: 0.00001189
Iteration 81/1000 | Loss: 0.00001189
Iteration 82/1000 | Loss: 0.00001189
Iteration 83/1000 | Loss: 0.00001188
Iteration 84/1000 | Loss: 0.00001188
Iteration 85/1000 | Loss: 0.00001188
Iteration 86/1000 | Loss: 0.00001187
Iteration 87/1000 | Loss: 0.00001187
Iteration 88/1000 | Loss: 0.00001187
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001186
Iteration 97/1000 | Loss: 0.00001186
Iteration 98/1000 | Loss: 0.00001186
Iteration 99/1000 | Loss: 0.00001185
Iteration 100/1000 | Loss: 0.00001185
Iteration 101/1000 | Loss: 0.00001185
Iteration 102/1000 | Loss: 0.00001185
Iteration 103/1000 | Loss: 0.00001185
Iteration 104/1000 | Loss: 0.00001185
Iteration 105/1000 | Loss: 0.00001184
Iteration 106/1000 | Loss: 0.00001184
Iteration 107/1000 | Loss: 0.00001184
Iteration 108/1000 | Loss: 0.00001184
Iteration 109/1000 | Loss: 0.00001184
Iteration 110/1000 | Loss: 0.00001184
Iteration 111/1000 | Loss: 0.00001184
Iteration 112/1000 | Loss: 0.00001184
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001184
Iteration 115/1000 | Loss: 0.00001184
Iteration 116/1000 | Loss: 0.00001184
Iteration 117/1000 | Loss: 0.00001184
Iteration 118/1000 | Loss: 0.00001184
Iteration 119/1000 | Loss: 0.00001184
Iteration 120/1000 | Loss: 0.00001184
Iteration 121/1000 | Loss: 0.00001184
Iteration 122/1000 | Loss: 0.00001184
Iteration 123/1000 | Loss: 0.00001184
Iteration 124/1000 | Loss: 0.00001184
Iteration 125/1000 | Loss: 0.00001184
Iteration 126/1000 | Loss: 0.00001184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.183543645311147e-05, 1.183543645311147e-05, 1.183543645311147e-05, 1.183543645311147e-05, 1.183543645311147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.183543645311147e-05

Optimization complete. Final v2v error: 2.8878695964813232 mm

Highest mean error: 3.4527747631073 mm for frame 222

Lowest mean error: 2.4110043048858643 mm for frame 46

Saving results

Total time: 50.392667293548584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00843091
Iteration 2/25 | Loss: 0.00145313
Iteration 3/25 | Loss: 0.00112312
Iteration 4/25 | Loss: 0.00107216
Iteration 5/25 | Loss: 0.00106766
Iteration 6/25 | Loss: 0.00106674
Iteration 7/25 | Loss: 0.00106674
Iteration 8/25 | Loss: 0.00106674
Iteration 9/25 | Loss: 0.00106674
Iteration 10/25 | Loss: 0.00106674
Iteration 11/25 | Loss: 0.00106674
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010667431633919477, 0.0010667431633919477, 0.0010667431633919477, 0.0010667431633919477, 0.0010667431633919477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010667431633919477

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32328451
Iteration 2/25 | Loss: 0.00078910
Iteration 3/25 | Loss: 0.00078905
Iteration 4/25 | Loss: 0.00078905
Iteration 5/25 | Loss: 0.00078905
Iteration 6/25 | Loss: 0.00078905
Iteration 7/25 | Loss: 0.00078904
Iteration 8/25 | Loss: 0.00078904
Iteration 9/25 | Loss: 0.00078904
Iteration 10/25 | Loss: 0.00078904
Iteration 11/25 | Loss: 0.00078904
Iteration 12/25 | Loss: 0.00078904
Iteration 13/25 | Loss: 0.00078904
Iteration 14/25 | Loss: 0.00078904
Iteration 15/25 | Loss: 0.00078904
Iteration 16/25 | Loss: 0.00078904
Iteration 17/25 | Loss: 0.00078904
Iteration 18/25 | Loss: 0.00078904
Iteration 19/25 | Loss: 0.00078904
Iteration 20/25 | Loss: 0.00078904
Iteration 21/25 | Loss: 0.00078904
Iteration 22/25 | Loss: 0.00078904
Iteration 23/25 | Loss: 0.00078904
Iteration 24/25 | Loss: 0.00078904
Iteration 25/25 | Loss: 0.00078904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078904
Iteration 2/1000 | Loss: 0.00004781
Iteration 3/1000 | Loss: 0.00003166
Iteration 4/1000 | Loss: 0.00002235
Iteration 5/1000 | Loss: 0.00002021
Iteration 6/1000 | Loss: 0.00001899
Iteration 7/1000 | Loss: 0.00001817
Iteration 8/1000 | Loss: 0.00001750
Iteration 9/1000 | Loss: 0.00001704
Iteration 10/1000 | Loss: 0.00001667
Iteration 11/1000 | Loss: 0.00001643
Iteration 12/1000 | Loss: 0.00001621
Iteration 13/1000 | Loss: 0.00001614
Iteration 14/1000 | Loss: 0.00001595
Iteration 15/1000 | Loss: 0.00001594
Iteration 16/1000 | Loss: 0.00001594
Iteration 17/1000 | Loss: 0.00001592
Iteration 18/1000 | Loss: 0.00001590
Iteration 19/1000 | Loss: 0.00001589
Iteration 20/1000 | Loss: 0.00001589
Iteration 21/1000 | Loss: 0.00001588
Iteration 22/1000 | Loss: 0.00001587
Iteration 23/1000 | Loss: 0.00001587
Iteration 24/1000 | Loss: 0.00001587
Iteration 25/1000 | Loss: 0.00001586
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001583
Iteration 28/1000 | Loss: 0.00001580
Iteration 29/1000 | Loss: 0.00001574
Iteration 30/1000 | Loss: 0.00001573
Iteration 31/1000 | Loss: 0.00001573
Iteration 32/1000 | Loss: 0.00001573
Iteration 33/1000 | Loss: 0.00001572
Iteration 34/1000 | Loss: 0.00001572
Iteration 35/1000 | Loss: 0.00001572
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001571
Iteration 40/1000 | Loss: 0.00001571
Iteration 41/1000 | Loss: 0.00001571
Iteration 42/1000 | Loss: 0.00001571
Iteration 43/1000 | Loss: 0.00001571
Iteration 44/1000 | Loss: 0.00001571
Iteration 45/1000 | Loss: 0.00001570
Iteration 46/1000 | Loss: 0.00001570
Iteration 47/1000 | Loss: 0.00001570
Iteration 48/1000 | Loss: 0.00001570
Iteration 49/1000 | Loss: 0.00001569
Iteration 50/1000 | Loss: 0.00001569
Iteration 51/1000 | Loss: 0.00001568
Iteration 52/1000 | Loss: 0.00001568
Iteration 53/1000 | Loss: 0.00001568
Iteration 54/1000 | Loss: 0.00001567
Iteration 55/1000 | Loss: 0.00001567
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001566
Iteration 59/1000 | Loss: 0.00001566
Iteration 60/1000 | Loss: 0.00001566
Iteration 61/1000 | Loss: 0.00001566
Iteration 62/1000 | Loss: 0.00001565
Iteration 63/1000 | Loss: 0.00001565
Iteration 64/1000 | Loss: 0.00001565
Iteration 65/1000 | Loss: 0.00001565
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001564
Iteration 68/1000 | Loss: 0.00001564
Iteration 69/1000 | Loss: 0.00001564
Iteration 70/1000 | Loss: 0.00001564
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00001564
Iteration 77/1000 | Loss: 0.00001564
Iteration 78/1000 | Loss: 0.00001564
Iteration 79/1000 | Loss: 0.00001564
Iteration 80/1000 | Loss: 0.00001564
Iteration 81/1000 | Loss: 0.00001564
Iteration 82/1000 | Loss: 0.00001564
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001564
Iteration 87/1000 | Loss: 0.00001564
Iteration 88/1000 | Loss: 0.00001564
Iteration 89/1000 | Loss: 0.00001564
Iteration 90/1000 | Loss: 0.00001564
Iteration 91/1000 | Loss: 0.00001564
Iteration 92/1000 | Loss: 0.00001564
Iteration 93/1000 | Loss: 0.00001564
Iteration 94/1000 | Loss: 0.00001564
Iteration 95/1000 | Loss: 0.00001564
Iteration 96/1000 | Loss: 0.00001564
Iteration 97/1000 | Loss: 0.00001564
Iteration 98/1000 | Loss: 0.00001564
Iteration 99/1000 | Loss: 0.00001564
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001564
Iteration 104/1000 | Loss: 0.00001564
Iteration 105/1000 | Loss: 0.00001564
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.5636212992831133e-05, 1.5636212992831133e-05, 1.5636212992831133e-05, 1.5636212992831133e-05, 1.5636212992831133e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5636212992831133e-05

Optimization complete. Final v2v error: 3.3730952739715576 mm

Highest mean error: 3.7390847206115723 mm for frame 29

Lowest mean error: 3.0164361000061035 mm for frame 0

Saving results

Total time: 35.482906103134155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887809
Iteration 2/25 | Loss: 0.00114480
Iteration 3/25 | Loss: 0.00095710
Iteration 4/25 | Loss: 0.00094218
Iteration 5/25 | Loss: 0.00093927
Iteration 6/25 | Loss: 0.00093848
Iteration 7/25 | Loss: 0.00093848
Iteration 8/25 | Loss: 0.00093848
Iteration 9/25 | Loss: 0.00093848
Iteration 10/25 | Loss: 0.00093848
Iteration 11/25 | Loss: 0.00093848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009384809527546167, 0.0009384809527546167, 0.0009384809527546167, 0.0009384809527546167, 0.0009384809527546167]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009384809527546167

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43140590
Iteration 2/25 | Loss: 0.00053207
Iteration 3/25 | Loss: 0.00053206
Iteration 4/25 | Loss: 0.00053206
Iteration 5/25 | Loss: 0.00053206
Iteration 6/25 | Loss: 0.00053206
Iteration 7/25 | Loss: 0.00053206
Iteration 8/25 | Loss: 0.00053206
Iteration 9/25 | Loss: 0.00053206
Iteration 10/25 | Loss: 0.00053206
Iteration 11/25 | Loss: 0.00053206
Iteration 12/25 | Loss: 0.00053206
Iteration 13/25 | Loss: 0.00053206
Iteration 14/25 | Loss: 0.00053206
Iteration 15/25 | Loss: 0.00053206
Iteration 16/25 | Loss: 0.00053206
Iteration 17/25 | Loss: 0.00053206
Iteration 18/25 | Loss: 0.00053206
Iteration 19/25 | Loss: 0.00053206
Iteration 20/25 | Loss: 0.00053206
Iteration 21/25 | Loss: 0.00053206
Iteration 22/25 | Loss: 0.00053206
Iteration 23/25 | Loss: 0.00053206
Iteration 24/25 | Loss: 0.00053206
Iteration 25/25 | Loss: 0.00053206

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00053206
Iteration 2/1000 | Loss: 0.00002623
Iteration 3/1000 | Loss: 0.00001378
Iteration 4/1000 | Loss: 0.00001184
Iteration 5/1000 | Loss: 0.00001072
Iteration 6/1000 | Loss: 0.00001021
Iteration 7/1000 | Loss: 0.00000991
Iteration 8/1000 | Loss: 0.00000964
Iteration 9/1000 | Loss: 0.00000949
Iteration 10/1000 | Loss: 0.00000947
Iteration 11/1000 | Loss: 0.00000932
Iteration 12/1000 | Loss: 0.00000927
Iteration 13/1000 | Loss: 0.00000924
Iteration 14/1000 | Loss: 0.00000923
Iteration 15/1000 | Loss: 0.00000922
Iteration 16/1000 | Loss: 0.00000920
Iteration 17/1000 | Loss: 0.00000919
Iteration 18/1000 | Loss: 0.00000918
Iteration 19/1000 | Loss: 0.00000917
Iteration 20/1000 | Loss: 0.00000917
Iteration 21/1000 | Loss: 0.00000917
Iteration 22/1000 | Loss: 0.00000916
Iteration 23/1000 | Loss: 0.00000916
Iteration 24/1000 | Loss: 0.00000915
Iteration 25/1000 | Loss: 0.00000914
Iteration 26/1000 | Loss: 0.00000914
Iteration 27/1000 | Loss: 0.00000913
Iteration 28/1000 | Loss: 0.00000913
Iteration 29/1000 | Loss: 0.00000912
Iteration 30/1000 | Loss: 0.00000911
Iteration 31/1000 | Loss: 0.00000911
Iteration 32/1000 | Loss: 0.00000910
Iteration 33/1000 | Loss: 0.00000910
Iteration 34/1000 | Loss: 0.00000909
Iteration 35/1000 | Loss: 0.00000909
Iteration 36/1000 | Loss: 0.00000909
Iteration 37/1000 | Loss: 0.00000908
Iteration 38/1000 | Loss: 0.00000905
Iteration 39/1000 | Loss: 0.00000905
Iteration 40/1000 | Loss: 0.00000904
Iteration 41/1000 | Loss: 0.00000903
Iteration 42/1000 | Loss: 0.00000902
Iteration 43/1000 | Loss: 0.00000902
Iteration 44/1000 | Loss: 0.00000902
Iteration 45/1000 | Loss: 0.00000902
Iteration 46/1000 | Loss: 0.00000902
Iteration 47/1000 | Loss: 0.00000901
Iteration 48/1000 | Loss: 0.00000901
Iteration 49/1000 | Loss: 0.00000901
Iteration 50/1000 | Loss: 0.00000900
Iteration 51/1000 | Loss: 0.00000900
Iteration 52/1000 | Loss: 0.00000900
Iteration 53/1000 | Loss: 0.00000900
Iteration 54/1000 | Loss: 0.00000900
Iteration 55/1000 | Loss: 0.00000900
Iteration 56/1000 | Loss: 0.00000899
Iteration 57/1000 | Loss: 0.00000899
Iteration 58/1000 | Loss: 0.00000899
Iteration 59/1000 | Loss: 0.00000899
Iteration 60/1000 | Loss: 0.00000899
Iteration 61/1000 | Loss: 0.00000899
Iteration 62/1000 | Loss: 0.00000898
Iteration 63/1000 | Loss: 0.00000898
Iteration 64/1000 | Loss: 0.00000898
Iteration 65/1000 | Loss: 0.00000897
Iteration 66/1000 | Loss: 0.00000897
Iteration 67/1000 | Loss: 0.00000896
Iteration 68/1000 | Loss: 0.00000896
Iteration 69/1000 | Loss: 0.00000896
Iteration 70/1000 | Loss: 0.00000895
Iteration 71/1000 | Loss: 0.00000895
Iteration 72/1000 | Loss: 0.00000895
Iteration 73/1000 | Loss: 0.00000895
Iteration 74/1000 | Loss: 0.00000895
Iteration 75/1000 | Loss: 0.00000895
Iteration 76/1000 | Loss: 0.00000895
Iteration 77/1000 | Loss: 0.00000895
Iteration 78/1000 | Loss: 0.00000895
Iteration 79/1000 | Loss: 0.00000895
Iteration 80/1000 | Loss: 0.00000895
Iteration 81/1000 | Loss: 0.00000895
Iteration 82/1000 | Loss: 0.00000894
Iteration 83/1000 | Loss: 0.00000893
Iteration 84/1000 | Loss: 0.00000893
Iteration 85/1000 | Loss: 0.00000893
Iteration 86/1000 | Loss: 0.00000892
Iteration 87/1000 | Loss: 0.00000892
Iteration 88/1000 | Loss: 0.00000892
Iteration 89/1000 | Loss: 0.00000892
Iteration 90/1000 | Loss: 0.00000892
Iteration 91/1000 | Loss: 0.00000892
Iteration 92/1000 | Loss: 0.00000892
Iteration 93/1000 | Loss: 0.00000892
Iteration 94/1000 | Loss: 0.00000892
Iteration 95/1000 | Loss: 0.00000892
Iteration 96/1000 | Loss: 0.00000891
Iteration 97/1000 | Loss: 0.00000891
Iteration 98/1000 | Loss: 0.00000891
Iteration 99/1000 | Loss: 0.00000890
Iteration 100/1000 | Loss: 0.00000890
Iteration 101/1000 | Loss: 0.00000890
Iteration 102/1000 | Loss: 0.00000890
Iteration 103/1000 | Loss: 0.00000889
Iteration 104/1000 | Loss: 0.00000889
Iteration 105/1000 | Loss: 0.00000889
Iteration 106/1000 | Loss: 0.00000889
Iteration 107/1000 | Loss: 0.00000889
Iteration 108/1000 | Loss: 0.00000889
Iteration 109/1000 | Loss: 0.00000889
Iteration 110/1000 | Loss: 0.00000889
Iteration 111/1000 | Loss: 0.00000889
Iteration 112/1000 | Loss: 0.00000889
Iteration 113/1000 | Loss: 0.00000888
Iteration 114/1000 | Loss: 0.00000888
Iteration 115/1000 | Loss: 0.00000888
Iteration 116/1000 | Loss: 0.00000888
Iteration 117/1000 | Loss: 0.00000888
Iteration 118/1000 | Loss: 0.00000888
Iteration 119/1000 | Loss: 0.00000888
Iteration 120/1000 | Loss: 0.00000888
Iteration 121/1000 | Loss: 0.00000888
Iteration 122/1000 | Loss: 0.00000888
Iteration 123/1000 | Loss: 0.00000888
Iteration 124/1000 | Loss: 0.00000888
Iteration 125/1000 | Loss: 0.00000888
Iteration 126/1000 | Loss: 0.00000888
Iteration 127/1000 | Loss: 0.00000888
Iteration 128/1000 | Loss: 0.00000888
Iteration 129/1000 | Loss: 0.00000888
Iteration 130/1000 | Loss: 0.00000888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [8.87691476236796e-06, 8.87691476236796e-06, 8.87691476236796e-06, 8.87691476236796e-06, 8.87691476236796e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.87691476236796e-06

Optimization complete. Final v2v error: 2.5193281173706055 mm

Highest mean error: 3.5756707191467285 mm for frame 113

Lowest mean error: 2.216444492340088 mm for frame 68

Saving results

Total time: 35.47520470619202
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970730
Iteration 2/25 | Loss: 0.00259012
Iteration 3/25 | Loss: 0.00182850
Iteration 4/25 | Loss: 0.00165243
Iteration 5/25 | Loss: 0.00164826
Iteration 6/25 | Loss: 0.00155904
Iteration 7/25 | Loss: 0.00143998
Iteration 8/25 | Loss: 0.00139321
Iteration 9/25 | Loss: 0.00138264
Iteration 10/25 | Loss: 0.00137605
Iteration 11/25 | Loss: 0.00136698
Iteration 12/25 | Loss: 0.00136183
Iteration 13/25 | Loss: 0.00135962
Iteration 14/25 | Loss: 0.00135801
Iteration 15/25 | Loss: 0.00135728
Iteration 16/25 | Loss: 0.00135705
Iteration 17/25 | Loss: 0.00135700
Iteration 18/25 | Loss: 0.00135694
Iteration 19/25 | Loss: 0.00135691
Iteration 20/25 | Loss: 0.00135691
Iteration 21/25 | Loss: 0.00135690
Iteration 22/25 | Loss: 0.00135690
Iteration 23/25 | Loss: 0.00135690
Iteration 24/25 | Loss: 0.00135690
Iteration 25/25 | Loss: 0.00135690

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29062402
Iteration 2/25 | Loss: 0.00286022
Iteration 3/25 | Loss: 0.00286022
Iteration 4/25 | Loss: 0.00286022
Iteration 5/25 | Loss: 0.00286022
Iteration 6/25 | Loss: 0.00286022
Iteration 7/25 | Loss: 0.00286022
Iteration 8/25 | Loss: 0.00286022
Iteration 9/25 | Loss: 0.00286022
Iteration 10/25 | Loss: 0.00286022
Iteration 11/25 | Loss: 0.00286022
Iteration 12/25 | Loss: 0.00286022
Iteration 13/25 | Loss: 0.00286022
Iteration 14/25 | Loss: 0.00286022
Iteration 15/25 | Loss: 0.00286022
Iteration 16/25 | Loss: 0.00286022
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0028602152597159147, 0.0028602152597159147, 0.0028602152597159147, 0.0028602152597159147, 0.0028602152597159147]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028602152597159147

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00286022
Iteration 2/1000 | Loss: 0.00035241
Iteration 3/1000 | Loss: 0.00026396
Iteration 4/1000 | Loss: 0.00022792
Iteration 5/1000 | Loss: 0.00021052
Iteration 6/1000 | Loss: 0.00019683
Iteration 7/1000 | Loss: 0.00019021
Iteration 8/1000 | Loss: 0.00018488
Iteration 9/1000 | Loss: 0.00018042
Iteration 10/1000 | Loss: 0.00017729
Iteration 11/1000 | Loss: 0.00017424
Iteration 12/1000 | Loss: 0.00017112
Iteration 13/1000 | Loss: 0.00032460
Iteration 14/1000 | Loss: 0.00071577
Iteration 15/1000 | Loss: 0.00163269
Iteration 16/1000 | Loss: 0.00481700
Iteration 17/1000 | Loss: 0.00161091
Iteration 18/1000 | Loss: 0.00065824
Iteration 19/1000 | Loss: 0.00033806
Iteration 20/1000 | Loss: 0.00021842
Iteration 21/1000 | Loss: 0.00017758
Iteration 22/1000 | Loss: 0.00013777
Iteration 23/1000 | Loss: 0.00011365
Iteration 24/1000 | Loss: 0.00009984
Iteration 25/1000 | Loss: 0.00009210
Iteration 26/1000 | Loss: 0.00008554
Iteration 27/1000 | Loss: 0.00008037
Iteration 28/1000 | Loss: 0.00007649
Iteration 29/1000 | Loss: 0.00007343
Iteration 30/1000 | Loss: 0.00007090
Iteration 31/1000 | Loss: 0.00006867
Iteration 32/1000 | Loss: 0.00006698
Iteration 33/1000 | Loss: 0.00006566
Iteration 34/1000 | Loss: 0.00006489
Iteration 35/1000 | Loss: 0.00006417
Iteration 36/1000 | Loss: 0.00006359
Iteration 37/1000 | Loss: 0.00006330
Iteration 38/1000 | Loss: 0.00006303
Iteration 39/1000 | Loss: 0.00006282
Iteration 40/1000 | Loss: 0.00006268
Iteration 41/1000 | Loss: 0.00006261
Iteration 42/1000 | Loss: 0.00006259
Iteration 43/1000 | Loss: 0.00006258
Iteration 44/1000 | Loss: 0.00006258
Iteration 45/1000 | Loss: 0.00006257
Iteration 46/1000 | Loss: 0.00006257
Iteration 47/1000 | Loss: 0.00006256
Iteration 48/1000 | Loss: 0.00006256
Iteration 49/1000 | Loss: 0.00006255
Iteration 50/1000 | Loss: 0.00006249
Iteration 51/1000 | Loss: 0.00006248
Iteration 52/1000 | Loss: 0.00006245
Iteration 53/1000 | Loss: 0.00006245
Iteration 54/1000 | Loss: 0.00006244
Iteration 55/1000 | Loss: 0.00006239
Iteration 56/1000 | Loss: 0.00006238
Iteration 57/1000 | Loss: 0.00006237
Iteration 58/1000 | Loss: 0.00006236
Iteration 59/1000 | Loss: 0.00006236
Iteration 60/1000 | Loss: 0.00006236
Iteration 61/1000 | Loss: 0.00006236
Iteration 62/1000 | Loss: 0.00006236
Iteration 63/1000 | Loss: 0.00006235
Iteration 64/1000 | Loss: 0.00006235
Iteration 65/1000 | Loss: 0.00006235
Iteration 66/1000 | Loss: 0.00006235
Iteration 67/1000 | Loss: 0.00006235
Iteration 68/1000 | Loss: 0.00006235
Iteration 69/1000 | Loss: 0.00006235
Iteration 70/1000 | Loss: 0.00006235
Iteration 71/1000 | Loss: 0.00006235
Iteration 72/1000 | Loss: 0.00006235
Iteration 73/1000 | Loss: 0.00006234
Iteration 74/1000 | Loss: 0.00006234
Iteration 75/1000 | Loss: 0.00006233
Iteration 76/1000 | Loss: 0.00006233
Iteration 77/1000 | Loss: 0.00006233
Iteration 78/1000 | Loss: 0.00006232
Iteration 79/1000 | Loss: 0.00006232
Iteration 80/1000 | Loss: 0.00006232
Iteration 81/1000 | Loss: 0.00006231
Iteration 82/1000 | Loss: 0.00006231
Iteration 83/1000 | Loss: 0.00006231
Iteration 84/1000 | Loss: 0.00006231
Iteration 85/1000 | Loss: 0.00006231
Iteration 86/1000 | Loss: 0.00006231
Iteration 87/1000 | Loss: 0.00006230
Iteration 88/1000 | Loss: 0.00006230
Iteration 89/1000 | Loss: 0.00006230
Iteration 90/1000 | Loss: 0.00006230
Iteration 91/1000 | Loss: 0.00006230
Iteration 92/1000 | Loss: 0.00006230
Iteration 93/1000 | Loss: 0.00006229
Iteration 94/1000 | Loss: 0.00006229
Iteration 95/1000 | Loss: 0.00006229
Iteration 96/1000 | Loss: 0.00006229
Iteration 97/1000 | Loss: 0.00006229
Iteration 98/1000 | Loss: 0.00006228
Iteration 99/1000 | Loss: 0.00006228
Iteration 100/1000 | Loss: 0.00006228
Iteration 101/1000 | Loss: 0.00006228
Iteration 102/1000 | Loss: 0.00006228
Iteration 103/1000 | Loss: 0.00006228
Iteration 104/1000 | Loss: 0.00006228
Iteration 105/1000 | Loss: 0.00006227
Iteration 106/1000 | Loss: 0.00006227
Iteration 107/1000 | Loss: 0.00006227
Iteration 108/1000 | Loss: 0.00006227
Iteration 109/1000 | Loss: 0.00006227
Iteration 110/1000 | Loss: 0.00006227
Iteration 111/1000 | Loss: 0.00006227
Iteration 112/1000 | Loss: 0.00006227
Iteration 113/1000 | Loss: 0.00006226
Iteration 114/1000 | Loss: 0.00006226
Iteration 115/1000 | Loss: 0.00006226
Iteration 116/1000 | Loss: 0.00006226
Iteration 117/1000 | Loss: 0.00006226
Iteration 118/1000 | Loss: 0.00006226
Iteration 119/1000 | Loss: 0.00006225
Iteration 120/1000 | Loss: 0.00006225
Iteration 121/1000 | Loss: 0.00006225
Iteration 122/1000 | Loss: 0.00006225
Iteration 123/1000 | Loss: 0.00006225
Iteration 124/1000 | Loss: 0.00006225
Iteration 125/1000 | Loss: 0.00006225
Iteration 126/1000 | Loss: 0.00006224
Iteration 127/1000 | Loss: 0.00006224
Iteration 128/1000 | Loss: 0.00006224
Iteration 129/1000 | Loss: 0.00006224
Iteration 130/1000 | Loss: 0.00006224
Iteration 131/1000 | Loss: 0.00006224
Iteration 132/1000 | Loss: 0.00006224
Iteration 133/1000 | Loss: 0.00006224
Iteration 134/1000 | Loss: 0.00006224
Iteration 135/1000 | Loss: 0.00006224
Iteration 136/1000 | Loss: 0.00006224
Iteration 137/1000 | Loss: 0.00006224
Iteration 138/1000 | Loss: 0.00006224
Iteration 139/1000 | Loss: 0.00006224
Iteration 140/1000 | Loss: 0.00006224
Iteration 141/1000 | Loss: 0.00006224
Iteration 142/1000 | Loss: 0.00006224
Iteration 143/1000 | Loss: 0.00006224
Iteration 144/1000 | Loss: 0.00006224
Iteration 145/1000 | Loss: 0.00006223
Iteration 146/1000 | Loss: 0.00006223
Iteration 147/1000 | Loss: 0.00006223
Iteration 148/1000 | Loss: 0.00006223
Iteration 149/1000 | Loss: 0.00006223
Iteration 150/1000 | Loss: 0.00006223
Iteration 151/1000 | Loss: 0.00006223
Iteration 152/1000 | Loss: 0.00006223
Iteration 153/1000 | Loss: 0.00006223
Iteration 154/1000 | Loss: 0.00006223
Iteration 155/1000 | Loss: 0.00006223
Iteration 156/1000 | Loss: 0.00006223
Iteration 157/1000 | Loss: 0.00006223
Iteration 158/1000 | Loss: 0.00006223
Iteration 159/1000 | Loss: 0.00006223
Iteration 160/1000 | Loss: 0.00006223
Iteration 161/1000 | Loss: 0.00006223
Iteration 162/1000 | Loss: 0.00006223
Iteration 163/1000 | Loss: 0.00006223
Iteration 164/1000 | Loss: 0.00006223
Iteration 165/1000 | Loss: 0.00006223
Iteration 166/1000 | Loss: 0.00006223
Iteration 167/1000 | Loss: 0.00006223
Iteration 168/1000 | Loss: 0.00006223
Iteration 169/1000 | Loss: 0.00006223
Iteration 170/1000 | Loss: 0.00006223
Iteration 171/1000 | Loss: 0.00006223
Iteration 172/1000 | Loss: 0.00006223
Iteration 173/1000 | Loss: 0.00006223
Iteration 174/1000 | Loss: 0.00006223
Iteration 175/1000 | Loss: 0.00006223
Iteration 176/1000 | Loss: 0.00006223
Iteration 177/1000 | Loss: 0.00006223
Iteration 178/1000 | Loss: 0.00006223
Iteration 179/1000 | Loss: 0.00006223
Iteration 180/1000 | Loss: 0.00006223
Iteration 181/1000 | Loss: 0.00006223
Iteration 182/1000 | Loss: 0.00006223
Iteration 183/1000 | Loss: 0.00006223
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [6.223186937859282e-05, 6.223186937859282e-05, 6.223186937859282e-05, 6.223186937859282e-05, 6.223186937859282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.223186937859282e-05

Optimization complete. Final v2v error: 4.305312633514404 mm

Highest mean error: 11.221985816955566 mm for frame 107

Lowest mean error: 2.6184513568878174 mm for frame 134

Saving results

Total time: 97.84160590171814
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00915487
Iteration 2/25 | Loss: 0.00170945
Iteration 3/25 | Loss: 0.00114031
Iteration 4/25 | Loss: 0.00111932
Iteration 5/25 | Loss: 0.00111261
Iteration 6/25 | Loss: 0.00110995
Iteration 7/25 | Loss: 0.00110930
Iteration 8/25 | Loss: 0.00110930
Iteration 9/25 | Loss: 0.00110930
Iteration 10/25 | Loss: 0.00110930
Iteration 11/25 | Loss: 0.00110930
Iteration 12/25 | Loss: 0.00110930
Iteration 13/25 | Loss: 0.00110930
Iteration 14/25 | Loss: 0.00110930
Iteration 15/25 | Loss: 0.00110930
Iteration 16/25 | Loss: 0.00110930
Iteration 17/25 | Loss: 0.00110930
Iteration 18/25 | Loss: 0.00110930
Iteration 19/25 | Loss: 0.00110930
Iteration 20/25 | Loss: 0.00110930
Iteration 21/25 | Loss: 0.00110930
Iteration 22/25 | Loss: 0.00110930
Iteration 23/25 | Loss: 0.00110930
Iteration 24/25 | Loss: 0.00110930
Iteration 25/25 | Loss: 0.00110930

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85042071
Iteration 2/25 | Loss: 0.00081560
Iteration 3/25 | Loss: 0.00081559
Iteration 4/25 | Loss: 0.00081559
Iteration 5/25 | Loss: 0.00081559
Iteration 6/25 | Loss: 0.00081559
Iteration 7/25 | Loss: 0.00081559
Iteration 8/25 | Loss: 0.00081559
Iteration 9/25 | Loss: 0.00081559
Iteration 10/25 | Loss: 0.00081559
Iteration 11/25 | Loss: 0.00081559
Iteration 12/25 | Loss: 0.00081559
Iteration 13/25 | Loss: 0.00081559
Iteration 14/25 | Loss: 0.00081559
Iteration 15/25 | Loss: 0.00081559
Iteration 16/25 | Loss: 0.00081559
Iteration 17/25 | Loss: 0.00081559
Iteration 18/25 | Loss: 0.00081559
Iteration 19/25 | Loss: 0.00081559
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008155908435583115, 0.0008155908435583115, 0.0008155908435583115, 0.0008155908435583115, 0.0008155908435583115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008155908435583115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081559
Iteration 2/1000 | Loss: 0.00007198
Iteration 3/1000 | Loss: 0.00004800
Iteration 4/1000 | Loss: 0.00004064
Iteration 5/1000 | Loss: 0.00003830
Iteration 6/1000 | Loss: 0.00003695
Iteration 7/1000 | Loss: 0.00003595
Iteration 8/1000 | Loss: 0.00003517
Iteration 9/1000 | Loss: 0.00003437
Iteration 10/1000 | Loss: 0.00003389
Iteration 11/1000 | Loss: 0.00003337
Iteration 12/1000 | Loss: 0.00003284
Iteration 13/1000 | Loss: 0.00003234
Iteration 14/1000 | Loss: 0.00003203
Iteration 15/1000 | Loss: 0.00003183
Iteration 16/1000 | Loss: 0.00003156
Iteration 17/1000 | Loss: 0.00003131
Iteration 18/1000 | Loss: 0.00003108
Iteration 19/1000 | Loss: 0.00003091
Iteration 20/1000 | Loss: 0.00003076
Iteration 21/1000 | Loss: 0.00003065
Iteration 22/1000 | Loss: 0.00003053
Iteration 23/1000 | Loss: 0.00003053
Iteration 24/1000 | Loss: 0.00003047
Iteration 25/1000 | Loss: 0.00003047
Iteration 26/1000 | Loss: 0.00003043
Iteration 27/1000 | Loss: 0.00003043
Iteration 28/1000 | Loss: 0.00003043
Iteration 29/1000 | Loss: 0.00003042
Iteration 30/1000 | Loss: 0.00003042
Iteration 31/1000 | Loss: 0.00003042
Iteration 32/1000 | Loss: 0.00003038
Iteration 33/1000 | Loss: 0.00003038
Iteration 34/1000 | Loss: 0.00003038
Iteration 35/1000 | Loss: 0.00003038
Iteration 36/1000 | Loss: 0.00003038
Iteration 37/1000 | Loss: 0.00003038
Iteration 38/1000 | Loss: 0.00003038
Iteration 39/1000 | Loss: 0.00003037
Iteration 40/1000 | Loss: 0.00003037
Iteration 41/1000 | Loss: 0.00003037
Iteration 42/1000 | Loss: 0.00003037
Iteration 43/1000 | Loss: 0.00003037
Iteration 44/1000 | Loss: 0.00003036
Iteration 45/1000 | Loss: 0.00003035
Iteration 46/1000 | Loss: 0.00003035
Iteration 47/1000 | Loss: 0.00003035
Iteration 48/1000 | Loss: 0.00003035
Iteration 49/1000 | Loss: 0.00003035
Iteration 50/1000 | Loss: 0.00003035
Iteration 51/1000 | Loss: 0.00003035
Iteration 52/1000 | Loss: 0.00003035
Iteration 53/1000 | Loss: 0.00003035
Iteration 54/1000 | Loss: 0.00003035
Iteration 55/1000 | Loss: 0.00003035
Iteration 56/1000 | Loss: 0.00003035
Iteration 57/1000 | Loss: 0.00003034
Iteration 58/1000 | Loss: 0.00003034
Iteration 59/1000 | Loss: 0.00003034
Iteration 60/1000 | Loss: 0.00003034
Iteration 61/1000 | Loss: 0.00003034
Iteration 62/1000 | Loss: 0.00003034
Iteration 63/1000 | Loss: 0.00003034
Iteration 64/1000 | Loss: 0.00003034
Iteration 65/1000 | Loss: 0.00003034
Iteration 66/1000 | Loss: 0.00003034
Iteration 67/1000 | Loss: 0.00003033
Iteration 68/1000 | Loss: 0.00003033
Iteration 69/1000 | Loss: 0.00003033
Iteration 70/1000 | Loss: 0.00003033
Iteration 71/1000 | Loss: 0.00003032
Iteration 72/1000 | Loss: 0.00003032
Iteration 73/1000 | Loss: 0.00003032
Iteration 74/1000 | Loss: 0.00003031
Iteration 75/1000 | Loss: 0.00003031
Iteration 76/1000 | Loss: 0.00003031
Iteration 77/1000 | Loss: 0.00003031
Iteration 78/1000 | Loss: 0.00003030
Iteration 79/1000 | Loss: 0.00003030
Iteration 80/1000 | Loss: 0.00003030
Iteration 81/1000 | Loss: 0.00003030
Iteration 82/1000 | Loss: 0.00003030
Iteration 83/1000 | Loss: 0.00003030
Iteration 84/1000 | Loss: 0.00003029
Iteration 85/1000 | Loss: 0.00003029
Iteration 86/1000 | Loss: 0.00003029
Iteration 87/1000 | Loss: 0.00003029
Iteration 88/1000 | Loss: 0.00003028
Iteration 89/1000 | Loss: 0.00003028
Iteration 90/1000 | Loss: 0.00003028
Iteration 91/1000 | Loss: 0.00003028
Iteration 92/1000 | Loss: 0.00003028
Iteration 93/1000 | Loss: 0.00003028
Iteration 94/1000 | Loss: 0.00003028
Iteration 95/1000 | Loss: 0.00003028
Iteration 96/1000 | Loss: 0.00003028
Iteration 97/1000 | Loss: 0.00003027
Iteration 98/1000 | Loss: 0.00003027
Iteration 99/1000 | Loss: 0.00003027
Iteration 100/1000 | Loss: 0.00003027
Iteration 101/1000 | Loss: 0.00003026
Iteration 102/1000 | Loss: 0.00003026
Iteration 103/1000 | Loss: 0.00003026
Iteration 104/1000 | Loss: 0.00003026
Iteration 105/1000 | Loss: 0.00003026
Iteration 106/1000 | Loss: 0.00003026
Iteration 107/1000 | Loss: 0.00003026
Iteration 108/1000 | Loss: 0.00003025
Iteration 109/1000 | Loss: 0.00003025
Iteration 110/1000 | Loss: 0.00003025
Iteration 111/1000 | Loss: 0.00003025
Iteration 112/1000 | Loss: 0.00003025
Iteration 113/1000 | Loss: 0.00003025
Iteration 114/1000 | Loss: 0.00003025
Iteration 115/1000 | Loss: 0.00003025
Iteration 116/1000 | Loss: 0.00003024
Iteration 117/1000 | Loss: 0.00003024
Iteration 118/1000 | Loss: 0.00003024
Iteration 119/1000 | Loss: 0.00003024
Iteration 120/1000 | Loss: 0.00003024
Iteration 121/1000 | Loss: 0.00003024
Iteration 122/1000 | Loss: 0.00003024
Iteration 123/1000 | Loss: 0.00003023
Iteration 124/1000 | Loss: 0.00003023
Iteration 125/1000 | Loss: 0.00003023
Iteration 126/1000 | Loss: 0.00003023
Iteration 127/1000 | Loss: 0.00003023
Iteration 128/1000 | Loss: 0.00003023
Iteration 129/1000 | Loss: 0.00003023
Iteration 130/1000 | Loss: 0.00003023
Iteration 131/1000 | Loss: 0.00003023
Iteration 132/1000 | Loss: 0.00003023
Iteration 133/1000 | Loss: 0.00003023
Iteration 134/1000 | Loss: 0.00003023
Iteration 135/1000 | Loss: 0.00003023
Iteration 136/1000 | Loss: 0.00003023
Iteration 137/1000 | Loss: 0.00003023
Iteration 138/1000 | Loss: 0.00003022
Iteration 139/1000 | Loss: 0.00003022
Iteration 140/1000 | Loss: 0.00003022
Iteration 141/1000 | Loss: 0.00003022
Iteration 142/1000 | Loss: 0.00003022
Iteration 143/1000 | Loss: 0.00003022
Iteration 144/1000 | Loss: 0.00003022
Iteration 145/1000 | Loss: 0.00003022
Iteration 146/1000 | Loss: 0.00003022
Iteration 147/1000 | Loss: 0.00003022
Iteration 148/1000 | Loss: 0.00003022
Iteration 149/1000 | Loss: 0.00003022
Iteration 150/1000 | Loss: 0.00003022
Iteration 151/1000 | Loss: 0.00003022
Iteration 152/1000 | Loss: 0.00003021
Iteration 153/1000 | Loss: 0.00003021
Iteration 154/1000 | Loss: 0.00003021
Iteration 155/1000 | Loss: 0.00003021
Iteration 156/1000 | Loss: 0.00003021
Iteration 157/1000 | Loss: 0.00003021
Iteration 158/1000 | Loss: 0.00003021
Iteration 159/1000 | Loss: 0.00003021
Iteration 160/1000 | Loss: 0.00003021
Iteration 161/1000 | Loss: 0.00003021
Iteration 162/1000 | Loss: 0.00003021
Iteration 163/1000 | Loss: 0.00003021
Iteration 164/1000 | Loss: 0.00003021
Iteration 165/1000 | Loss: 0.00003021
Iteration 166/1000 | Loss: 0.00003020
Iteration 167/1000 | Loss: 0.00003020
Iteration 168/1000 | Loss: 0.00003020
Iteration 169/1000 | Loss: 0.00003020
Iteration 170/1000 | Loss: 0.00003020
Iteration 171/1000 | Loss: 0.00003020
Iteration 172/1000 | Loss: 0.00003020
Iteration 173/1000 | Loss: 0.00003020
Iteration 174/1000 | Loss: 0.00003020
Iteration 175/1000 | Loss: 0.00003019
Iteration 176/1000 | Loss: 0.00003019
Iteration 177/1000 | Loss: 0.00003019
Iteration 178/1000 | Loss: 0.00003019
Iteration 179/1000 | Loss: 0.00003019
Iteration 180/1000 | Loss: 0.00003019
Iteration 181/1000 | Loss: 0.00003019
Iteration 182/1000 | Loss: 0.00003019
Iteration 183/1000 | Loss: 0.00003019
Iteration 184/1000 | Loss: 0.00003018
Iteration 185/1000 | Loss: 0.00003018
Iteration 186/1000 | Loss: 0.00003018
Iteration 187/1000 | Loss: 0.00003018
Iteration 188/1000 | Loss: 0.00003018
Iteration 189/1000 | Loss: 0.00003018
Iteration 190/1000 | Loss: 0.00003018
Iteration 191/1000 | Loss: 0.00003018
Iteration 192/1000 | Loss: 0.00003018
Iteration 193/1000 | Loss: 0.00003018
Iteration 194/1000 | Loss: 0.00003018
Iteration 195/1000 | Loss: 0.00003018
Iteration 196/1000 | Loss: 0.00003018
Iteration 197/1000 | Loss: 0.00003018
Iteration 198/1000 | Loss: 0.00003018
Iteration 199/1000 | Loss: 0.00003018
Iteration 200/1000 | Loss: 0.00003018
Iteration 201/1000 | Loss: 0.00003018
Iteration 202/1000 | Loss: 0.00003018
Iteration 203/1000 | Loss: 0.00003018
Iteration 204/1000 | Loss: 0.00003018
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [3.01764393952908e-05, 3.01764393952908e-05, 3.01764393952908e-05, 3.01764393952908e-05, 3.01764393952908e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.01764393952908e-05

Optimization complete. Final v2v error: 4.294798374176025 mm

Highest mean error: 5.347261905670166 mm for frame 25

Lowest mean error: 2.8504934310913086 mm for frame 131

Saving results

Total time: 54.62795948982239
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437854
Iteration 2/25 | Loss: 0.00107688
Iteration 3/25 | Loss: 0.00094181
Iteration 4/25 | Loss: 0.00092639
Iteration 5/25 | Loss: 0.00092191
Iteration 6/25 | Loss: 0.00092103
Iteration 7/25 | Loss: 0.00092103
Iteration 8/25 | Loss: 0.00092103
Iteration 9/25 | Loss: 0.00092103
Iteration 10/25 | Loss: 0.00092103
Iteration 11/25 | Loss: 0.00092103
Iteration 12/25 | Loss: 0.00092103
Iteration 13/25 | Loss: 0.00092103
Iteration 14/25 | Loss: 0.00092103
Iteration 15/25 | Loss: 0.00092103
Iteration 16/25 | Loss: 0.00092103
Iteration 17/25 | Loss: 0.00092103
Iteration 18/25 | Loss: 0.00092103
Iteration 19/25 | Loss: 0.00092103
Iteration 20/25 | Loss: 0.00092103
Iteration 21/25 | Loss: 0.00092103
Iteration 22/25 | Loss: 0.00092103
Iteration 23/25 | Loss: 0.00092103
Iteration 24/25 | Loss: 0.00092103
Iteration 25/25 | Loss: 0.00092103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.35256338
Iteration 2/25 | Loss: 0.00056568
Iteration 3/25 | Loss: 0.00056566
Iteration 4/25 | Loss: 0.00056566
Iteration 5/25 | Loss: 0.00056566
Iteration 6/25 | Loss: 0.00056566
Iteration 7/25 | Loss: 0.00056566
Iteration 8/25 | Loss: 0.00056566
Iteration 9/25 | Loss: 0.00056566
Iteration 10/25 | Loss: 0.00056566
Iteration 11/25 | Loss: 0.00056566
Iteration 12/25 | Loss: 0.00056566
Iteration 13/25 | Loss: 0.00056566
Iteration 14/25 | Loss: 0.00056566
Iteration 15/25 | Loss: 0.00056566
Iteration 16/25 | Loss: 0.00056566
Iteration 17/25 | Loss: 0.00056566
Iteration 18/25 | Loss: 0.00056566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005656565190292895, 0.0005656565190292895, 0.0005656565190292895, 0.0005656565190292895, 0.0005656565190292895]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005656565190292895

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056566
Iteration 2/1000 | Loss: 0.00001594
Iteration 3/1000 | Loss: 0.00001168
Iteration 4/1000 | Loss: 0.00001091
Iteration 5/1000 | Loss: 0.00001032
Iteration 6/1000 | Loss: 0.00001005
Iteration 7/1000 | Loss: 0.00001003
Iteration 8/1000 | Loss: 0.00000982
Iteration 9/1000 | Loss: 0.00000978
Iteration 10/1000 | Loss: 0.00000977
Iteration 11/1000 | Loss: 0.00000976
Iteration 12/1000 | Loss: 0.00000973
Iteration 13/1000 | Loss: 0.00000969
Iteration 14/1000 | Loss: 0.00000969
Iteration 15/1000 | Loss: 0.00000966
Iteration 16/1000 | Loss: 0.00000964
Iteration 17/1000 | Loss: 0.00000964
Iteration 18/1000 | Loss: 0.00000964
Iteration 19/1000 | Loss: 0.00000964
Iteration 20/1000 | Loss: 0.00000964
Iteration 21/1000 | Loss: 0.00000962
Iteration 22/1000 | Loss: 0.00000958
Iteration 23/1000 | Loss: 0.00000958
Iteration 24/1000 | Loss: 0.00000957
Iteration 25/1000 | Loss: 0.00000957
Iteration 26/1000 | Loss: 0.00000956
Iteration 27/1000 | Loss: 0.00000954
Iteration 28/1000 | Loss: 0.00000954
Iteration 29/1000 | Loss: 0.00000953
Iteration 30/1000 | Loss: 0.00000953
Iteration 31/1000 | Loss: 0.00000953
Iteration 32/1000 | Loss: 0.00000953
Iteration 33/1000 | Loss: 0.00000953
Iteration 34/1000 | Loss: 0.00000953
Iteration 35/1000 | Loss: 0.00000952
Iteration 36/1000 | Loss: 0.00000952
Iteration 37/1000 | Loss: 0.00000952
Iteration 38/1000 | Loss: 0.00000951
Iteration 39/1000 | Loss: 0.00000951
Iteration 40/1000 | Loss: 0.00000951
Iteration 41/1000 | Loss: 0.00000951
Iteration 42/1000 | Loss: 0.00000951
Iteration 43/1000 | Loss: 0.00000951
Iteration 44/1000 | Loss: 0.00000950
Iteration 45/1000 | Loss: 0.00000950
Iteration 46/1000 | Loss: 0.00000950
Iteration 47/1000 | Loss: 0.00000950
Iteration 48/1000 | Loss: 0.00000950
Iteration 49/1000 | Loss: 0.00000950
Iteration 50/1000 | Loss: 0.00000949
Iteration 51/1000 | Loss: 0.00000949
Iteration 52/1000 | Loss: 0.00000949
Iteration 53/1000 | Loss: 0.00000948
Iteration 54/1000 | Loss: 0.00000948
Iteration 55/1000 | Loss: 0.00000948
Iteration 56/1000 | Loss: 0.00000947
Iteration 57/1000 | Loss: 0.00000947
Iteration 58/1000 | Loss: 0.00000947
Iteration 59/1000 | Loss: 0.00000947
Iteration 60/1000 | Loss: 0.00000946
Iteration 61/1000 | Loss: 0.00000946
Iteration 62/1000 | Loss: 0.00000946
Iteration 63/1000 | Loss: 0.00000946
Iteration 64/1000 | Loss: 0.00000945
Iteration 65/1000 | Loss: 0.00000945
Iteration 66/1000 | Loss: 0.00000945
Iteration 67/1000 | Loss: 0.00000944
Iteration 68/1000 | Loss: 0.00000944
Iteration 69/1000 | Loss: 0.00000944
Iteration 70/1000 | Loss: 0.00000944
Iteration 71/1000 | Loss: 0.00000944
Iteration 72/1000 | Loss: 0.00000943
Iteration 73/1000 | Loss: 0.00000943
Iteration 74/1000 | Loss: 0.00000943
Iteration 75/1000 | Loss: 0.00000943
Iteration 76/1000 | Loss: 0.00000942
Iteration 77/1000 | Loss: 0.00000942
Iteration 78/1000 | Loss: 0.00000942
Iteration 79/1000 | Loss: 0.00000942
Iteration 80/1000 | Loss: 0.00000942
Iteration 81/1000 | Loss: 0.00000942
Iteration 82/1000 | Loss: 0.00000941
Iteration 83/1000 | Loss: 0.00000941
Iteration 84/1000 | Loss: 0.00000941
Iteration 85/1000 | Loss: 0.00000941
Iteration 86/1000 | Loss: 0.00000941
Iteration 87/1000 | Loss: 0.00000940
Iteration 88/1000 | Loss: 0.00000940
Iteration 89/1000 | Loss: 0.00000940
Iteration 90/1000 | Loss: 0.00000940
Iteration 91/1000 | Loss: 0.00000940
Iteration 92/1000 | Loss: 0.00000940
Iteration 93/1000 | Loss: 0.00000940
Iteration 94/1000 | Loss: 0.00000940
Iteration 95/1000 | Loss: 0.00000940
Iteration 96/1000 | Loss: 0.00000940
Iteration 97/1000 | Loss: 0.00000940
Iteration 98/1000 | Loss: 0.00000940
Iteration 99/1000 | Loss: 0.00000940
Iteration 100/1000 | Loss: 0.00000939
Iteration 101/1000 | Loss: 0.00000939
Iteration 102/1000 | Loss: 0.00000939
Iteration 103/1000 | Loss: 0.00000939
Iteration 104/1000 | Loss: 0.00000939
Iteration 105/1000 | Loss: 0.00000939
Iteration 106/1000 | Loss: 0.00000939
Iteration 107/1000 | Loss: 0.00000939
Iteration 108/1000 | Loss: 0.00000938
Iteration 109/1000 | Loss: 0.00000938
Iteration 110/1000 | Loss: 0.00000938
Iteration 111/1000 | Loss: 0.00000938
Iteration 112/1000 | Loss: 0.00000938
Iteration 113/1000 | Loss: 0.00000938
Iteration 114/1000 | Loss: 0.00000938
Iteration 115/1000 | Loss: 0.00000938
Iteration 116/1000 | Loss: 0.00000938
Iteration 117/1000 | Loss: 0.00000937
Iteration 118/1000 | Loss: 0.00000937
Iteration 119/1000 | Loss: 0.00000937
Iteration 120/1000 | Loss: 0.00000937
Iteration 121/1000 | Loss: 0.00000937
Iteration 122/1000 | Loss: 0.00000937
Iteration 123/1000 | Loss: 0.00000937
Iteration 124/1000 | Loss: 0.00000937
Iteration 125/1000 | Loss: 0.00000937
Iteration 126/1000 | Loss: 0.00000937
Iteration 127/1000 | Loss: 0.00000937
Iteration 128/1000 | Loss: 0.00000936
Iteration 129/1000 | Loss: 0.00000936
Iteration 130/1000 | Loss: 0.00000936
Iteration 131/1000 | Loss: 0.00000936
Iteration 132/1000 | Loss: 0.00000936
Iteration 133/1000 | Loss: 0.00000936
Iteration 134/1000 | Loss: 0.00000936
Iteration 135/1000 | Loss: 0.00000936
Iteration 136/1000 | Loss: 0.00000936
Iteration 137/1000 | Loss: 0.00000936
Iteration 138/1000 | Loss: 0.00000936
Iteration 139/1000 | Loss: 0.00000936
Iteration 140/1000 | Loss: 0.00000936
Iteration 141/1000 | Loss: 0.00000936
Iteration 142/1000 | Loss: 0.00000936
Iteration 143/1000 | Loss: 0.00000936
Iteration 144/1000 | Loss: 0.00000936
Iteration 145/1000 | Loss: 0.00000936
Iteration 146/1000 | Loss: 0.00000936
Iteration 147/1000 | Loss: 0.00000936
Iteration 148/1000 | Loss: 0.00000936
Iteration 149/1000 | Loss: 0.00000936
Iteration 150/1000 | Loss: 0.00000936
Iteration 151/1000 | Loss: 0.00000936
Iteration 152/1000 | Loss: 0.00000936
Iteration 153/1000 | Loss: 0.00000936
Iteration 154/1000 | Loss: 0.00000936
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [9.362548553326633e-06, 9.362548553326633e-06, 9.362548553326633e-06, 9.362548553326633e-06, 9.362548553326633e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.362548553326633e-06

Optimization complete. Final v2v error: 2.5954842567443848 mm

Highest mean error: 3.0363447666168213 mm for frame 87

Lowest mean error: 2.3444886207580566 mm for frame 60

Saving results

Total time: 31.07814073562622
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01068271
Iteration 2/25 | Loss: 0.00176674
Iteration 3/25 | Loss: 0.00156166
Iteration 4/25 | Loss: 0.00116591
Iteration 5/25 | Loss: 0.00111042
Iteration 6/25 | Loss: 0.00110856
Iteration 7/25 | Loss: 0.00110378
Iteration 8/25 | Loss: 0.00107075
Iteration 9/25 | Loss: 0.00103295
Iteration 10/25 | Loss: 0.00101688
Iteration 11/25 | Loss: 0.00104115
Iteration 12/25 | Loss: 0.00099859
Iteration 13/25 | Loss: 0.00099135
Iteration 14/25 | Loss: 0.00098867
Iteration 15/25 | Loss: 0.00098686
Iteration 16/25 | Loss: 0.00098460
Iteration 17/25 | Loss: 0.00098659
Iteration 18/25 | Loss: 0.00098530
Iteration 19/25 | Loss: 0.00098397
Iteration 20/25 | Loss: 0.00098178
Iteration 21/25 | Loss: 0.00097896
Iteration 22/25 | Loss: 0.00097810
Iteration 23/25 | Loss: 0.00097804
Iteration 24/25 | Loss: 0.00097851
Iteration 25/25 | Loss: 0.00097814

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34801650
Iteration 2/25 | Loss: 0.00082092
Iteration 3/25 | Loss: 0.00082092
Iteration 4/25 | Loss: 0.00082092
Iteration 5/25 | Loss: 0.00082092
Iteration 6/25 | Loss: 0.00082092
Iteration 7/25 | Loss: 0.00082091
Iteration 8/25 | Loss: 0.00082091
Iteration 9/25 | Loss: 0.00082091
Iteration 10/25 | Loss: 0.00082091
Iteration 11/25 | Loss: 0.00082091
Iteration 12/25 | Loss: 0.00082091
Iteration 13/25 | Loss: 0.00082091
Iteration 14/25 | Loss: 0.00082091
Iteration 15/25 | Loss: 0.00082091
Iteration 16/25 | Loss: 0.00082091
Iteration 17/25 | Loss: 0.00082091
Iteration 18/25 | Loss: 0.00082091
Iteration 19/25 | Loss: 0.00082091
Iteration 20/25 | Loss: 0.00082091
Iteration 21/25 | Loss: 0.00082091
Iteration 22/25 | Loss: 0.00082091
Iteration 23/25 | Loss: 0.00082091
Iteration 24/25 | Loss: 0.00082091
Iteration 25/25 | Loss: 0.00082091

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082091
Iteration 2/1000 | Loss: 0.00604195
Iteration 3/1000 | Loss: 0.00616612
Iteration 4/1000 | Loss: 0.00422646
Iteration 5/1000 | Loss: 0.00015152
Iteration 6/1000 | Loss: 0.00007720
Iteration 7/1000 | Loss: 0.00011941
Iteration 8/1000 | Loss: 0.00039934
Iteration 9/1000 | Loss: 0.00005455
Iteration 10/1000 | Loss: 0.00006165
Iteration 11/1000 | Loss: 0.00003902
Iteration 12/1000 | Loss: 0.00107681
Iteration 13/1000 | Loss: 0.00040596
Iteration 14/1000 | Loss: 0.00369258
Iteration 15/1000 | Loss: 0.00159657
Iteration 16/1000 | Loss: 0.00038696
Iteration 17/1000 | Loss: 0.00148954
Iteration 18/1000 | Loss: 0.00166086
Iteration 19/1000 | Loss: 0.00186806
Iteration 20/1000 | Loss: 0.00010809
Iteration 21/1000 | Loss: 0.00129159
Iteration 22/1000 | Loss: 0.00171156
Iteration 23/1000 | Loss: 0.00309267
Iteration 24/1000 | Loss: 0.00178553
Iteration 25/1000 | Loss: 0.00263180
Iteration 26/1000 | Loss: 0.00206715
Iteration 27/1000 | Loss: 0.00111914
Iteration 28/1000 | Loss: 0.00346524
Iteration 29/1000 | Loss: 0.00064470
Iteration 30/1000 | Loss: 0.00077754
Iteration 31/1000 | Loss: 0.00004365
Iteration 32/1000 | Loss: 0.00150520
Iteration 33/1000 | Loss: 0.00249337
Iteration 34/1000 | Loss: 0.00090731
Iteration 35/1000 | Loss: 0.00203503
Iteration 36/1000 | Loss: 0.00266847
Iteration 37/1000 | Loss: 0.00276818
Iteration 38/1000 | Loss: 0.00216810
Iteration 39/1000 | Loss: 0.00263306
Iteration 40/1000 | Loss: 0.00013432
Iteration 41/1000 | Loss: 0.00005963
Iteration 42/1000 | Loss: 0.00003764
Iteration 43/1000 | Loss: 0.00005685
Iteration 44/1000 | Loss: 0.00099412
Iteration 45/1000 | Loss: 0.00086309
Iteration 46/1000 | Loss: 0.00003261
Iteration 47/1000 | Loss: 0.00091485
Iteration 48/1000 | Loss: 0.00111056
Iteration 49/1000 | Loss: 0.00005266
Iteration 50/1000 | Loss: 0.00084239
Iteration 51/1000 | Loss: 0.00122960
Iteration 52/1000 | Loss: 0.00110218
Iteration 53/1000 | Loss: 0.00059253
Iteration 54/1000 | Loss: 0.00004315
Iteration 55/1000 | Loss: 0.00002673
Iteration 56/1000 | Loss: 0.00002295
Iteration 57/1000 | Loss: 0.00002159
Iteration 58/1000 | Loss: 0.00002055
Iteration 59/1000 | Loss: 0.00022314
Iteration 60/1000 | Loss: 0.00018346
Iteration 61/1000 | Loss: 0.00018649
Iteration 62/1000 | Loss: 0.00003030
Iteration 63/1000 | Loss: 0.00002320
Iteration 64/1000 | Loss: 0.00002139
Iteration 65/1000 | Loss: 0.00002493
Iteration 66/1000 | Loss: 0.00002004
Iteration 67/1000 | Loss: 0.00001866
Iteration 68/1000 | Loss: 0.00001799
Iteration 69/1000 | Loss: 0.00054624
Iteration 70/1000 | Loss: 0.00002265
Iteration 71/1000 | Loss: 0.00001735
Iteration 72/1000 | Loss: 0.00001583
Iteration 73/1000 | Loss: 0.00069046
Iteration 74/1000 | Loss: 0.00001544
Iteration 75/1000 | Loss: 0.00001328
Iteration 76/1000 | Loss: 0.00001211
Iteration 77/1000 | Loss: 0.00001122
Iteration 78/1000 | Loss: 0.00001060
Iteration 79/1000 | Loss: 0.00001027
Iteration 80/1000 | Loss: 0.00001007
Iteration 81/1000 | Loss: 0.00001006
Iteration 82/1000 | Loss: 0.00000995
Iteration 83/1000 | Loss: 0.00000993
Iteration 84/1000 | Loss: 0.00000991
Iteration 85/1000 | Loss: 0.00000988
Iteration 86/1000 | Loss: 0.00000987
Iteration 87/1000 | Loss: 0.00000986
Iteration 88/1000 | Loss: 0.00000986
Iteration 89/1000 | Loss: 0.00000980
Iteration 90/1000 | Loss: 0.00000977
Iteration 91/1000 | Loss: 0.00000974
Iteration 92/1000 | Loss: 0.00000973
Iteration 93/1000 | Loss: 0.00000973
Iteration 94/1000 | Loss: 0.00000972
Iteration 95/1000 | Loss: 0.00000971
Iteration 96/1000 | Loss: 0.00000971
Iteration 97/1000 | Loss: 0.00000971
Iteration 98/1000 | Loss: 0.00000970
Iteration 99/1000 | Loss: 0.00000970
Iteration 100/1000 | Loss: 0.00000968
Iteration 101/1000 | Loss: 0.00000964
Iteration 102/1000 | Loss: 0.00000963
Iteration 103/1000 | Loss: 0.00000963
Iteration 104/1000 | Loss: 0.00000961
Iteration 105/1000 | Loss: 0.00000961
Iteration 106/1000 | Loss: 0.00000961
Iteration 107/1000 | Loss: 0.00000961
Iteration 108/1000 | Loss: 0.00000961
Iteration 109/1000 | Loss: 0.00000961
Iteration 110/1000 | Loss: 0.00000960
Iteration 111/1000 | Loss: 0.00000960
Iteration 112/1000 | Loss: 0.00000960
Iteration 113/1000 | Loss: 0.00000960
Iteration 114/1000 | Loss: 0.00000960
Iteration 115/1000 | Loss: 0.00000960
Iteration 116/1000 | Loss: 0.00000960
Iteration 117/1000 | Loss: 0.00000960
Iteration 118/1000 | Loss: 0.00000960
Iteration 119/1000 | Loss: 0.00000960
Iteration 120/1000 | Loss: 0.00000960
Iteration 121/1000 | Loss: 0.00000960
Iteration 122/1000 | Loss: 0.00000959
Iteration 123/1000 | Loss: 0.00000959
Iteration 124/1000 | Loss: 0.00000959
Iteration 125/1000 | Loss: 0.00000959
Iteration 126/1000 | Loss: 0.00000959
Iteration 127/1000 | Loss: 0.00000959
Iteration 128/1000 | Loss: 0.00000959
Iteration 129/1000 | Loss: 0.00000958
Iteration 130/1000 | Loss: 0.00000958
Iteration 131/1000 | Loss: 0.00000957
Iteration 132/1000 | Loss: 0.00000957
Iteration 133/1000 | Loss: 0.00000957
Iteration 134/1000 | Loss: 0.00000957
Iteration 135/1000 | Loss: 0.00000957
Iteration 136/1000 | Loss: 0.00000957
Iteration 137/1000 | Loss: 0.00000957
Iteration 138/1000 | Loss: 0.00000957
Iteration 139/1000 | Loss: 0.00000956
Iteration 140/1000 | Loss: 0.00000956
Iteration 141/1000 | Loss: 0.00000956
Iteration 142/1000 | Loss: 0.00000956
Iteration 143/1000 | Loss: 0.00000956
Iteration 144/1000 | Loss: 0.00000956
Iteration 145/1000 | Loss: 0.00000956
Iteration 146/1000 | Loss: 0.00000956
Iteration 147/1000 | Loss: 0.00000956
Iteration 148/1000 | Loss: 0.00000956
Iteration 149/1000 | Loss: 0.00000956
Iteration 150/1000 | Loss: 0.00000956
Iteration 151/1000 | Loss: 0.00000956
Iteration 152/1000 | Loss: 0.00000955
Iteration 153/1000 | Loss: 0.00000955
Iteration 154/1000 | Loss: 0.00000955
Iteration 155/1000 | Loss: 0.00000955
Iteration 156/1000 | Loss: 0.00000955
Iteration 157/1000 | Loss: 0.00000955
Iteration 158/1000 | Loss: 0.00000955
Iteration 159/1000 | Loss: 0.00000955
Iteration 160/1000 | Loss: 0.00000955
Iteration 161/1000 | Loss: 0.00000955
Iteration 162/1000 | Loss: 0.00000955
Iteration 163/1000 | Loss: 0.00000955
Iteration 164/1000 | Loss: 0.00000955
Iteration 165/1000 | Loss: 0.00000955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [9.55176346906228e-06, 9.55176346906228e-06, 9.55176346906228e-06, 9.55176346906228e-06, 9.55176346906228e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.55176346906228e-06

Optimization complete. Final v2v error: 2.6392064094543457 mm

Highest mean error: 4.244229316711426 mm for frame 66

Lowest mean error: 2.316951036453247 mm for frame 16

Saving results

Total time: 169.03386425971985
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01039227
Iteration 2/25 | Loss: 0.00335332
Iteration 3/25 | Loss: 0.00239029
Iteration 4/25 | Loss: 0.00186268
Iteration 5/25 | Loss: 0.00190631
Iteration 6/25 | Loss: 0.00190870
Iteration 7/25 | Loss: 0.00180381
Iteration 8/25 | Loss: 0.00172519
Iteration 9/25 | Loss: 0.00156814
Iteration 10/25 | Loss: 0.00150897
Iteration 11/25 | Loss: 0.00148739
Iteration 12/25 | Loss: 0.00143995
Iteration 13/25 | Loss: 0.00142959
Iteration 14/25 | Loss: 0.00144408
Iteration 15/25 | Loss: 0.00143382
Iteration 16/25 | Loss: 0.00143002
Iteration 17/25 | Loss: 0.00142465
Iteration 18/25 | Loss: 0.00142202
Iteration 19/25 | Loss: 0.00141358
Iteration 20/25 | Loss: 0.00141219
Iteration 21/25 | Loss: 0.00140623
Iteration 22/25 | Loss: 0.00140130
Iteration 23/25 | Loss: 0.00139794
Iteration 24/25 | Loss: 0.00139720
Iteration 25/25 | Loss: 0.00139683

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36068106
Iteration 2/25 | Loss: 0.00427476
Iteration 3/25 | Loss: 0.00427476
Iteration 4/25 | Loss: 0.00427476
Iteration 5/25 | Loss: 0.00427476
Iteration 6/25 | Loss: 0.00427476
Iteration 7/25 | Loss: 0.00427476
Iteration 8/25 | Loss: 0.00427476
Iteration 9/25 | Loss: 0.00427476
Iteration 10/25 | Loss: 0.00427476
Iteration 11/25 | Loss: 0.00427476
Iteration 12/25 | Loss: 0.00427476
Iteration 13/25 | Loss: 0.00427476
Iteration 14/25 | Loss: 0.00427476
Iteration 15/25 | Loss: 0.00427476
Iteration 16/25 | Loss: 0.00427476
Iteration 17/25 | Loss: 0.00427476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0042747571133077145, 0.0042747571133077145, 0.0042747571133077145, 0.0042747571133077145, 0.0042747571133077145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0042747571133077145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00427476
Iteration 2/1000 | Loss: 0.00111603
Iteration 3/1000 | Loss: 0.00163323
Iteration 4/1000 | Loss: 0.00378583
Iteration 5/1000 | Loss: 0.00483491
Iteration 6/1000 | Loss: 0.00078973
Iteration 7/1000 | Loss: 0.00094212
Iteration 8/1000 | Loss: 0.00017257
Iteration 9/1000 | Loss: 0.00157697
Iteration 10/1000 | Loss: 0.00102815
Iteration 11/1000 | Loss: 0.00084592
Iteration 12/1000 | Loss: 0.00131798
Iteration 13/1000 | Loss: 0.00148779
Iteration 14/1000 | Loss: 0.00047376
Iteration 15/1000 | Loss: 0.00030181
Iteration 16/1000 | Loss: 0.00082104
Iteration 17/1000 | Loss: 0.00020130
Iteration 18/1000 | Loss: 0.00072312
Iteration 19/1000 | Loss: 0.00056884
Iteration 20/1000 | Loss: 0.00007899
Iteration 21/1000 | Loss: 0.00125720
Iteration 22/1000 | Loss: 0.00007094
Iteration 23/1000 | Loss: 0.00028590
Iteration 24/1000 | Loss: 0.00005820
Iteration 25/1000 | Loss: 0.00005387
Iteration 26/1000 | Loss: 0.00005011
Iteration 27/1000 | Loss: 0.00004749
Iteration 28/1000 | Loss: 0.00102482
Iteration 29/1000 | Loss: 0.00012101
Iteration 30/1000 | Loss: 0.00004658
Iteration 31/1000 | Loss: 0.00004366
Iteration 32/1000 | Loss: 0.00004239
Iteration 33/1000 | Loss: 0.00004126
Iteration 34/1000 | Loss: 0.00004031
Iteration 35/1000 | Loss: 0.00003976
Iteration 36/1000 | Loss: 0.00003909
Iteration 37/1000 | Loss: 0.00003861
Iteration 38/1000 | Loss: 0.00003821
Iteration 39/1000 | Loss: 0.00003784
Iteration 40/1000 | Loss: 0.00054460
Iteration 41/1000 | Loss: 0.00051972
Iteration 42/1000 | Loss: 0.00022636
Iteration 43/1000 | Loss: 0.00095714
Iteration 44/1000 | Loss: 0.00033490
Iteration 45/1000 | Loss: 0.00004108
Iteration 46/1000 | Loss: 0.00003816
Iteration 47/1000 | Loss: 0.00003657
Iteration 48/1000 | Loss: 0.00003561
Iteration 49/1000 | Loss: 0.00003488
Iteration 50/1000 | Loss: 0.00003445
Iteration 51/1000 | Loss: 0.00003423
Iteration 52/1000 | Loss: 0.00003400
Iteration 53/1000 | Loss: 0.00003376
Iteration 54/1000 | Loss: 0.00003372
Iteration 55/1000 | Loss: 0.00003368
Iteration 56/1000 | Loss: 0.00003367
Iteration 57/1000 | Loss: 0.00003367
Iteration 58/1000 | Loss: 0.00003366
Iteration 59/1000 | Loss: 0.00003366
Iteration 60/1000 | Loss: 0.00003366
Iteration 61/1000 | Loss: 0.00003365
Iteration 62/1000 | Loss: 0.00003364
Iteration 63/1000 | Loss: 0.00003361
Iteration 64/1000 | Loss: 0.00003361
Iteration 65/1000 | Loss: 0.00003360
Iteration 66/1000 | Loss: 0.00003359
Iteration 67/1000 | Loss: 0.00003356
Iteration 68/1000 | Loss: 0.00003356
Iteration 69/1000 | Loss: 0.00003356
Iteration 70/1000 | Loss: 0.00003355
Iteration 71/1000 | Loss: 0.00003355
Iteration 72/1000 | Loss: 0.00003353
Iteration 73/1000 | Loss: 0.00003353
Iteration 74/1000 | Loss: 0.00003352
Iteration 75/1000 | Loss: 0.00003352
Iteration 76/1000 | Loss: 0.00003350
Iteration 77/1000 | Loss: 0.00003349
Iteration 78/1000 | Loss: 0.00003349
Iteration 79/1000 | Loss: 0.00003349
Iteration 80/1000 | Loss: 0.00003349
Iteration 81/1000 | Loss: 0.00003349
Iteration 82/1000 | Loss: 0.00003348
Iteration 83/1000 | Loss: 0.00003348
Iteration 84/1000 | Loss: 0.00003348
Iteration 85/1000 | Loss: 0.00003347
Iteration 86/1000 | Loss: 0.00003347
Iteration 87/1000 | Loss: 0.00003347
Iteration 88/1000 | Loss: 0.00003347
Iteration 89/1000 | Loss: 0.00003347
Iteration 90/1000 | Loss: 0.00003346
Iteration 91/1000 | Loss: 0.00003346
Iteration 92/1000 | Loss: 0.00003345
Iteration 93/1000 | Loss: 0.00003345
Iteration 94/1000 | Loss: 0.00003345
Iteration 95/1000 | Loss: 0.00003344
Iteration 96/1000 | Loss: 0.00003344
Iteration 97/1000 | Loss: 0.00003344
Iteration 98/1000 | Loss: 0.00003344
Iteration 99/1000 | Loss: 0.00003343
Iteration 100/1000 | Loss: 0.00003343
Iteration 101/1000 | Loss: 0.00003343
Iteration 102/1000 | Loss: 0.00003342
Iteration 103/1000 | Loss: 0.00003342
Iteration 104/1000 | Loss: 0.00003342
Iteration 105/1000 | Loss: 0.00003342
Iteration 106/1000 | Loss: 0.00003342
Iteration 107/1000 | Loss: 0.00003342
Iteration 108/1000 | Loss: 0.00003341
Iteration 109/1000 | Loss: 0.00003341
Iteration 110/1000 | Loss: 0.00003341
Iteration 111/1000 | Loss: 0.00003341
Iteration 112/1000 | Loss: 0.00003341
Iteration 113/1000 | Loss: 0.00003341
Iteration 114/1000 | Loss: 0.00003341
Iteration 115/1000 | Loss: 0.00003341
Iteration 116/1000 | Loss: 0.00003341
Iteration 117/1000 | Loss: 0.00003341
Iteration 118/1000 | Loss: 0.00003341
Iteration 119/1000 | Loss: 0.00003340
Iteration 120/1000 | Loss: 0.00003340
Iteration 121/1000 | Loss: 0.00003340
Iteration 122/1000 | Loss: 0.00003340
Iteration 123/1000 | Loss: 0.00003340
Iteration 124/1000 | Loss: 0.00003340
Iteration 125/1000 | Loss: 0.00003340
Iteration 126/1000 | Loss: 0.00003340
Iteration 127/1000 | Loss: 0.00003340
Iteration 128/1000 | Loss: 0.00003340
Iteration 129/1000 | Loss: 0.00003340
Iteration 130/1000 | Loss: 0.00003340
Iteration 131/1000 | Loss: 0.00003339
Iteration 132/1000 | Loss: 0.00003339
Iteration 133/1000 | Loss: 0.00003339
Iteration 134/1000 | Loss: 0.00003339
Iteration 135/1000 | Loss: 0.00003339
Iteration 136/1000 | Loss: 0.00003339
Iteration 137/1000 | Loss: 0.00003339
Iteration 138/1000 | Loss: 0.00003339
Iteration 139/1000 | Loss: 0.00003338
Iteration 140/1000 | Loss: 0.00003338
Iteration 141/1000 | Loss: 0.00003338
Iteration 142/1000 | Loss: 0.00003338
Iteration 143/1000 | Loss: 0.00003338
Iteration 144/1000 | Loss: 0.00003338
Iteration 145/1000 | Loss: 0.00003338
Iteration 146/1000 | Loss: 0.00003338
Iteration 147/1000 | Loss: 0.00003337
Iteration 148/1000 | Loss: 0.00003337
Iteration 149/1000 | Loss: 0.00003337
Iteration 150/1000 | Loss: 0.00003337
Iteration 151/1000 | Loss: 0.00003337
Iteration 152/1000 | Loss: 0.00003337
Iteration 153/1000 | Loss: 0.00003337
Iteration 154/1000 | Loss: 0.00003337
Iteration 155/1000 | Loss: 0.00003337
Iteration 156/1000 | Loss: 0.00003337
Iteration 157/1000 | Loss: 0.00003337
Iteration 158/1000 | Loss: 0.00003337
Iteration 159/1000 | Loss: 0.00003337
Iteration 160/1000 | Loss: 0.00003337
Iteration 161/1000 | Loss: 0.00003337
Iteration 162/1000 | Loss: 0.00003337
Iteration 163/1000 | Loss: 0.00003337
Iteration 164/1000 | Loss: 0.00003337
Iteration 165/1000 | Loss: 0.00003337
Iteration 166/1000 | Loss: 0.00003337
Iteration 167/1000 | Loss: 0.00003337
Iteration 168/1000 | Loss: 0.00003337
Iteration 169/1000 | Loss: 0.00003337
Iteration 170/1000 | Loss: 0.00003337
Iteration 171/1000 | Loss: 0.00003337
Iteration 172/1000 | Loss: 0.00003337
Iteration 173/1000 | Loss: 0.00003337
Iteration 174/1000 | Loss: 0.00003337
Iteration 175/1000 | Loss: 0.00003337
Iteration 176/1000 | Loss: 0.00003337
Iteration 177/1000 | Loss: 0.00003337
Iteration 178/1000 | Loss: 0.00003337
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [3.3365384297212586e-05, 3.3365384297212586e-05, 3.3365384297212586e-05, 3.3365384297212586e-05, 3.3365384297212586e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.3365384297212586e-05

Optimization complete. Final v2v error: 3.7566285133361816 mm

Highest mean error: 13.039112091064453 mm for frame 89

Lowest mean error: 2.6060290336608887 mm for frame 196

Saving results

Total time: 138.37055826187134
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473241
Iteration 2/25 | Loss: 0.00127333
Iteration 3/25 | Loss: 0.00104280
Iteration 4/25 | Loss: 0.00101589
Iteration 5/25 | Loss: 0.00101346
Iteration 6/25 | Loss: 0.00101320
Iteration 7/25 | Loss: 0.00101320
Iteration 8/25 | Loss: 0.00101320
Iteration 9/25 | Loss: 0.00101320
Iteration 10/25 | Loss: 0.00101320
Iteration 11/25 | Loss: 0.00101320
Iteration 12/25 | Loss: 0.00101320
Iteration 13/25 | Loss: 0.00101320
Iteration 14/25 | Loss: 0.00101320
Iteration 15/25 | Loss: 0.00101320
Iteration 16/25 | Loss: 0.00101320
Iteration 17/25 | Loss: 0.00101320
Iteration 18/25 | Loss: 0.00101320
Iteration 19/25 | Loss: 0.00101320
Iteration 20/25 | Loss: 0.00101320
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010131957242265344, 0.0010131957242265344, 0.0010131957242265344, 0.0010131957242265344, 0.0010131957242265344]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010131957242265344

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33130348
Iteration 2/25 | Loss: 0.00058060
Iteration 3/25 | Loss: 0.00058059
Iteration 4/25 | Loss: 0.00058058
Iteration 5/25 | Loss: 0.00058058
Iteration 6/25 | Loss: 0.00058058
Iteration 7/25 | Loss: 0.00058058
Iteration 8/25 | Loss: 0.00058058
Iteration 9/25 | Loss: 0.00058058
Iteration 10/25 | Loss: 0.00058058
Iteration 11/25 | Loss: 0.00058058
Iteration 12/25 | Loss: 0.00058058
Iteration 13/25 | Loss: 0.00058058
Iteration 14/25 | Loss: 0.00058058
Iteration 15/25 | Loss: 0.00058058
Iteration 16/25 | Loss: 0.00058058
Iteration 17/25 | Loss: 0.00058058
Iteration 18/25 | Loss: 0.00058058
Iteration 19/25 | Loss: 0.00058058
Iteration 20/25 | Loss: 0.00058058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005805819528177381, 0.0005805819528177381, 0.0005805819528177381, 0.0005805819528177381, 0.0005805819528177381]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005805819528177381

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058058
Iteration 2/1000 | Loss: 0.00003091
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001674
Iteration 5/1000 | Loss: 0.00001601
Iteration 6/1000 | Loss: 0.00001560
Iteration 7/1000 | Loss: 0.00001535
Iteration 8/1000 | Loss: 0.00001508
Iteration 9/1000 | Loss: 0.00001508
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001479
Iteration 12/1000 | Loss: 0.00001469
Iteration 13/1000 | Loss: 0.00001462
Iteration 14/1000 | Loss: 0.00001457
Iteration 15/1000 | Loss: 0.00001455
Iteration 16/1000 | Loss: 0.00001455
Iteration 17/1000 | Loss: 0.00001454
Iteration 18/1000 | Loss: 0.00001453
Iteration 19/1000 | Loss: 0.00001453
Iteration 20/1000 | Loss: 0.00001451
Iteration 21/1000 | Loss: 0.00001450
Iteration 22/1000 | Loss: 0.00001449
Iteration 23/1000 | Loss: 0.00001448
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001445
Iteration 26/1000 | Loss: 0.00001441
Iteration 27/1000 | Loss: 0.00001438
Iteration 28/1000 | Loss: 0.00001438
Iteration 29/1000 | Loss: 0.00001438
Iteration 30/1000 | Loss: 0.00001436
Iteration 31/1000 | Loss: 0.00001436
Iteration 32/1000 | Loss: 0.00001436
Iteration 33/1000 | Loss: 0.00001435
Iteration 34/1000 | Loss: 0.00001434
Iteration 35/1000 | Loss: 0.00001433
Iteration 36/1000 | Loss: 0.00001433
Iteration 37/1000 | Loss: 0.00001432
Iteration 38/1000 | Loss: 0.00001432
Iteration 39/1000 | Loss: 0.00001431
Iteration 40/1000 | Loss: 0.00001431
Iteration 41/1000 | Loss: 0.00001430
Iteration 42/1000 | Loss: 0.00001430
Iteration 43/1000 | Loss: 0.00001429
Iteration 44/1000 | Loss: 0.00001429
Iteration 45/1000 | Loss: 0.00001429
Iteration 46/1000 | Loss: 0.00001429
Iteration 47/1000 | Loss: 0.00001429
Iteration 48/1000 | Loss: 0.00001429
Iteration 49/1000 | Loss: 0.00001429
Iteration 50/1000 | Loss: 0.00001429
Iteration 51/1000 | Loss: 0.00001428
Iteration 52/1000 | Loss: 0.00001428
Iteration 53/1000 | Loss: 0.00001428
Iteration 54/1000 | Loss: 0.00001428
Iteration 55/1000 | Loss: 0.00001427
Iteration 56/1000 | Loss: 0.00001427
Iteration 57/1000 | Loss: 0.00001426
Iteration 58/1000 | Loss: 0.00001426
Iteration 59/1000 | Loss: 0.00001426
Iteration 60/1000 | Loss: 0.00001426
Iteration 61/1000 | Loss: 0.00001426
Iteration 62/1000 | Loss: 0.00001426
Iteration 63/1000 | Loss: 0.00001426
Iteration 64/1000 | Loss: 0.00001426
Iteration 65/1000 | Loss: 0.00001425
Iteration 66/1000 | Loss: 0.00001425
Iteration 67/1000 | Loss: 0.00001425
Iteration 68/1000 | Loss: 0.00001424
Iteration 69/1000 | Loss: 0.00001424
Iteration 70/1000 | Loss: 0.00001424
Iteration 71/1000 | Loss: 0.00001424
Iteration 72/1000 | Loss: 0.00001424
Iteration 73/1000 | Loss: 0.00001423
Iteration 74/1000 | Loss: 0.00001423
Iteration 75/1000 | Loss: 0.00001423
Iteration 76/1000 | Loss: 0.00001423
Iteration 77/1000 | Loss: 0.00001422
Iteration 78/1000 | Loss: 0.00001422
Iteration 79/1000 | Loss: 0.00001422
Iteration 80/1000 | Loss: 0.00001422
Iteration 81/1000 | Loss: 0.00001422
Iteration 82/1000 | Loss: 0.00001421
Iteration 83/1000 | Loss: 0.00001421
Iteration 84/1000 | Loss: 0.00001421
Iteration 85/1000 | Loss: 0.00001421
Iteration 86/1000 | Loss: 0.00001421
Iteration 87/1000 | Loss: 0.00001420
Iteration 88/1000 | Loss: 0.00001420
Iteration 89/1000 | Loss: 0.00001420
Iteration 90/1000 | Loss: 0.00001420
Iteration 91/1000 | Loss: 0.00001420
Iteration 92/1000 | Loss: 0.00001419
Iteration 93/1000 | Loss: 0.00001419
Iteration 94/1000 | Loss: 0.00001419
Iteration 95/1000 | Loss: 0.00001419
Iteration 96/1000 | Loss: 0.00001419
Iteration 97/1000 | Loss: 0.00001418
Iteration 98/1000 | Loss: 0.00001418
Iteration 99/1000 | Loss: 0.00001418
Iteration 100/1000 | Loss: 0.00001417
Iteration 101/1000 | Loss: 0.00001417
Iteration 102/1000 | Loss: 0.00001417
Iteration 103/1000 | Loss: 0.00001417
Iteration 104/1000 | Loss: 0.00001417
Iteration 105/1000 | Loss: 0.00001417
Iteration 106/1000 | Loss: 0.00001417
Iteration 107/1000 | Loss: 0.00001416
Iteration 108/1000 | Loss: 0.00001416
Iteration 109/1000 | Loss: 0.00001416
Iteration 110/1000 | Loss: 0.00001416
Iteration 111/1000 | Loss: 0.00001416
Iteration 112/1000 | Loss: 0.00001416
Iteration 113/1000 | Loss: 0.00001416
Iteration 114/1000 | Loss: 0.00001416
Iteration 115/1000 | Loss: 0.00001416
Iteration 116/1000 | Loss: 0.00001416
Iteration 117/1000 | Loss: 0.00001416
Iteration 118/1000 | Loss: 0.00001415
Iteration 119/1000 | Loss: 0.00001415
Iteration 120/1000 | Loss: 0.00001415
Iteration 121/1000 | Loss: 0.00001415
Iteration 122/1000 | Loss: 0.00001415
Iteration 123/1000 | Loss: 0.00001415
Iteration 124/1000 | Loss: 0.00001415
Iteration 125/1000 | Loss: 0.00001415
Iteration 126/1000 | Loss: 0.00001415
Iteration 127/1000 | Loss: 0.00001415
Iteration 128/1000 | Loss: 0.00001414
Iteration 129/1000 | Loss: 0.00001414
Iteration 130/1000 | Loss: 0.00001414
Iteration 131/1000 | Loss: 0.00001414
Iteration 132/1000 | Loss: 0.00001414
Iteration 133/1000 | Loss: 0.00001414
Iteration 134/1000 | Loss: 0.00001414
Iteration 135/1000 | Loss: 0.00001414
Iteration 136/1000 | Loss: 0.00001414
Iteration 137/1000 | Loss: 0.00001414
Iteration 138/1000 | Loss: 0.00001414
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.414068083249731e-05, 1.414068083249731e-05, 1.414068083249731e-05, 1.414068083249731e-05, 1.414068083249731e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.414068083249731e-05

Optimization complete. Final v2v error: 3.126246213912964 mm

Highest mean error: 3.4493281841278076 mm for frame 191

Lowest mean error: 2.6619138717651367 mm for frame 239

Saving results

Total time: 39.51771879196167
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00649690
Iteration 2/25 | Loss: 0.00102807
Iteration 3/25 | Loss: 0.00094207
Iteration 4/25 | Loss: 0.00092903
Iteration 5/25 | Loss: 0.00092418
Iteration 6/25 | Loss: 0.00092294
Iteration 7/25 | Loss: 0.00092294
Iteration 8/25 | Loss: 0.00092294
Iteration 9/25 | Loss: 0.00092294
Iteration 10/25 | Loss: 0.00092294
Iteration 11/25 | Loss: 0.00092294
Iteration 12/25 | Loss: 0.00092294
Iteration 13/25 | Loss: 0.00092294
Iteration 14/25 | Loss: 0.00092294
Iteration 15/25 | Loss: 0.00092294
Iteration 16/25 | Loss: 0.00092294
Iteration 17/25 | Loss: 0.00092294
Iteration 18/25 | Loss: 0.00092294
Iteration 19/25 | Loss: 0.00092294
Iteration 20/25 | Loss: 0.00092294
Iteration 21/25 | Loss: 0.00092294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009229367133229971, 0.0009229367133229971, 0.0009229367133229971, 0.0009229367133229971, 0.0009229367133229971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009229367133229971

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.91824794
Iteration 2/25 | Loss: 0.00064914
Iteration 3/25 | Loss: 0.00064913
Iteration 4/25 | Loss: 0.00064913
Iteration 5/25 | Loss: 0.00064913
Iteration 6/25 | Loss: 0.00064913
Iteration 7/25 | Loss: 0.00064913
Iteration 8/25 | Loss: 0.00064913
Iteration 9/25 | Loss: 0.00064913
Iteration 10/25 | Loss: 0.00064913
Iteration 11/25 | Loss: 0.00064913
Iteration 12/25 | Loss: 0.00064913
Iteration 13/25 | Loss: 0.00064913
Iteration 14/25 | Loss: 0.00064913
Iteration 15/25 | Loss: 0.00064913
Iteration 16/25 | Loss: 0.00064913
Iteration 17/25 | Loss: 0.00064913
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006491316016763449, 0.0006491316016763449, 0.0006491316016763449, 0.0006491316016763449, 0.0006491316016763449]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006491316016763449

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064913
Iteration 2/1000 | Loss: 0.00001821
Iteration 3/1000 | Loss: 0.00001359
Iteration 4/1000 | Loss: 0.00001253
Iteration 5/1000 | Loss: 0.00001180
Iteration 6/1000 | Loss: 0.00001145
Iteration 7/1000 | Loss: 0.00001109
Iteration 8/1000 | Loss: 0.00001097
Iteration 9/1000 | Loss: 0.00001092
Iteration 10/1000 | Loss: 0.00001091
Iteration 11/1000 | Loss: 0.00001089
Iteration 12/1000 | Loss: 0.00001084
Iteration 13/1000 | Loss: 0.00001084
Iteration 14/1000 | Loss: 0.00001084
Iteration 15/1000 | Loss: 0.00001083
Iteration 16/1000 | Loss: 0.00001082
Iteration 17/1000 | Loss: 0.00001081
Iteration 18/1000 | Loss: 0.00001080
Iteration 19/1000 | Loss: 0.00001079
Iteration 20/1000 | Loss: 0.00001078
Iteration 21/1000 | Loss: 0.00001078
Iteration 22/1000 | Loss: 0.00001077
Iteration 23/1000 | Loss: 0.00001076
Iteration 24/1000 | Loss: 0.00001075
Iteration 25/1000 | Loss: 0.00001075
Iteration 26/1000 | Loss: 0.00001074
Iteration 27/1000 | Loss: 0.00001074
Iteration 28/1000 | Loss: 0.00001073
Iteration 29/1000 | Loss: 0.00001072
Iteration 30/1000 | Loss: 0.00001072
Iteration 31/1000 | Loss: 0.00001072
Iteration 32/1000 | Loss: 0.00001071
Iteration 33/1000 | Loss: 0.00001071
Iteration 34/1000 | Loss: 0.00001071
Iteration 35/1000 | Loss: 0.00001070
Iteration 36/1000 | Loss: 0.00001070
Iteration 37/1000 | Loss: 0.00001069
Iteration 38/1000 | Loss: 0.00001069
Iteration 39/1000 | Loss: 0.00001069
Iteration 40/1000 | Loss: 0.00001068
Iteration 41/1000 | Loss: 0.00001068
Iteration 42/1000 | Loss: 0.00001068
Iteration 43/1000 | Loss: 0.00001067
Iteration 44/1000 | Loss: 0.00001067
Iteration 45/1000 | Loss: 0.00001067
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001066
Iteration 49/1000 | Loss: 0.00001066
Iteration 50/1000 | Loss: 0.00001065
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001065
Iteration 54/1000 | Loss: 0.00001064
Iteration 55/1000 | Loss: 0.00001064
Iteration 56/1000 | Loss: 0.00001064
Iteration 57/1000 | Loss: 0.00001064
Iteration 58/1000 | Loss: 0.00001064
Iteration 59/1000 | Loss: 0.00001063
Iteration 60/1000 | Loss: 0.00001063
Iteration 61/1000 | Loss: 0.00001063
Iteration 62/1000 | Loss: 0.00001063
Iteration 63/1000 | Loss: 0.00001062
Iteration 64/1000 | Loss: 0.00001062
Iteration 65/1000 | Loss: 0.00001062
Iteration 66/1000 | Loss: 0.00001062
Iteration 67/1000 | Loss: 0.00001061
Iteration 68/1000 | Loss: 0.00001061
Iteration 69/1000 | Loss: 0.00001061
Iteration 70/1000 | Loss: 0.00001061
Iteration 71/1000 | Loss: 0.00001060
Iteration 72/1000 | Loss: 0.00001060
Iteration 73/1000 | Loss: 0.00001060
Iteration 74/1000 | Loss: 0.00001060
Iteration 75/1000 | Loss: 0.00001060
Iteration 76/1000 | Loss: 0.00001060
Iteration 77/1000 | Loss: 0.00001059
Iteration 78/1000 | Loss: 0.00001059
Iteration 79/1000 | Loss: 0.00001059
Iteration 80/1000 | Loss: 0.00001059
Iteration 81/1000 | Loss: 0.00001059
Iteration 82/1000 | Loss: 0.00001059
Iteration 83/1000 | Loss: 0.00001059
Iteration 84/1000 | Loss: 0.00001059
Iteration 85/1000 | Loss: 0.00001059
Iteration 86/1000 | Loss: 0.00001058
Iteration 87/1000 | Loss: 0.00001058
Iteration 88/1000 | Loss: 0.00001058
Iteration 89/1000 | Loss: 0.00001058
Iteration 90/1000 | Loss: 0.00001058
Iteration 91/1000 | Loss: 0.00001058
Iteration 92/1000 | Loss: 0.00001057
Iteration 93/1000 | Loss: 0.00001057
Iteration 94/1000 | Loss: 0.00001057
Iteration 95/1000 | Loss: 0.00001057
Iteration 96/1000 | Loss: 0.00001057
Iteration 97/1000 | Loss: 0.00001057
Iteration 98/1000 | Loss: 0.00001057
Iteration 99/1000 | Loss: 0.00001057
Iteration 100/1000 | Loss: 0.00001057
Iteration 101/1000 | Loss: 0.00001057
Iteration 102/1000 | Loss: 0.00001056
Iteration 103/1000 | Loss: 0.00001056
Iteration 104/1000 | Loss: 0.00001056
Iteration 105/1000 | Loss: 0.00001056
Iteration 106/1000 | Loss: 0.00001056
Iteration 107/1000 | Loss: 0.00001056
Iteration 108/1000 | Loss: 0.00001056
Iteration 109/1000 | Loss: 0.00001056
Iteration 110/1000 | Loss: 0.00001056
Iteration 111/1000 | Loss: 0.00001056
Iteration 112/1000 | Loss: 0.00001056
Iteration 113/1000 | Loss: 0.00001056
Iteration 114/1000 | Loss: 0.00001056
Iteration 115/1000 | Loss: 0.00001056
Iteration 116/1000 | Loss: 0.00001055
Iteration 117/1000 | Loss: 0.00001055
Iteration 118/1000 | Loss: 0.00001055
Iteration 119/1000 | Loss: 0.00001054
Iteration 120/1000 | Loss: 0.00001054
Iteration 121/1000 | Loss: 0.00001054
Iteration 122/1000 | Loss: 0.00001054
Iteration 123/1000 | Loss: 0.00001054
Iteration 124/1000 | Loss: 0.00001054
Iteration 125/1000 | Loss: 0.00001054
Iteration 126/1000 | Loss: 0.00001054
Iteration 127/1000 | Loss: 0.00001054
Iteration 128/1000 | Loss: 0.00001054
Iteration 129/1000 | Loss: 0.00001054
Iteration 130/1000 | Loss: 0.00001054
Iteration 131/1000 | Loss: 0.00001054
Iteration 132/1000 | Loss: 0.00001054
Iteration 133/1000 | Loss: 0.00001053
Iteration 134/1000 | Loss: 0.00001053
Iteration 135/1000 | Loss: 0.00001053
Iteration 136/1000 | Loss: 0.00001053
Iteration 137/1000 | Loss: 0.00001053
Iteration 138/1000 | Loss: 0.00001053
Iteration 139/1000 | Loss: 0.00001053
Iteration 140/1000 | Loss: 0.00001053
Iteration 141/1000 | Loss: 0.00001053
Iteration 142/1000 | Loss: 0.00001053
Iteration 143/1000 | Loss: 0.00001053
Iteration 144/1000 | Loss: 0.00001053
Iteration 145/1000 | Loss: 0.00001053
Iteration 146/1000 | Loss: 0.00001052
Iteration 147/1000 | Loss: 0.00001052
Iteration 148/1000 | Loss: 0.00001052
Iteration 149/1000 | Loss: 0.00001052
Iteration 150/1000 | Loss: 0.00001052
Iteration 151/1000 | Loss: 0.00001052
Iteration 152/1000 | Loss: 0.00001052
Iteration 153/1000 | Loss: 0.00001052
Iteration 154/1000 | Loss: 0.00001052
Iteration 155/1000 | Loss: 0.00001052
Iteration 156/1000 | Loss: 0.00001052
Iteration 157/1000 | Loss: 0.00001052
Iteration 158/1000 | Loss: 0.00001052
Iteration 159/1000 | Loss: 0.00001052
Iteration 160/1000 | Loss: 0.00001052
Iteration 161/1000 | Loss: 0.00001052
Iteration 162/1000 | Loss: 0.00001052
Iteration 163/1000 | Loss: 0.00001051
Iteration 164/1000 | Loss: 0.00001051
Iteration 165/1000 | Loss: 0.00001051
Iteration 166/1000 | Loss: 0.00001051
Iteration 167/1000 | Loss: 0.00001051
Iteration 168/1000 | Loss: 0.00001051
Iteration 169/1000 | Loss: 0.00001051
Iteration 170/1000 | Loss: 0.00001051
Iteration 171/1000 | Loss: 0.00001051
Iteration 172/1000 | Loss: 0.00001051
Iteration 173/1000 | Loss: 0.00001051
Iteration 174/1000 | Loss: 0.00001051
Iteration 175/1000 | Loss: 0.00001051
Iteration 176/1000 | Loss: 0.00001051
Iteration 177/1000 | Loss: 0.00001051
Iteration 178/1000 | Loss: 0.00001050
Iteration 179/1000 | Loss: 0.00001050
Iteration 180/1000 | Loss: 0.00001050
Iteration 181/1000 | Loss: 0.00001050
Iteration 182/1000 | Loss: 0.00001050
Iteration 183/1000 | Loss: 0.00001050
Iteration 184/1000 | Loss: 0.00001050
Iteration 185/1000 | Loss: 0.00001050
Iteration 186/1000 | Loss: 0.00001050
Iteration 187/1000 | Loss: 0.00001050
Iteration 188/1000 | Loss: 0.00001050
Iteration 189/1000 | Loss: 0.00001050
Iteration 190/1000 | Loss: 0.00001050
Iteration 191/1000 | Loss: 0.00001050
Iteration 192/1000 | Loss: 0.00001050
Iteration 193/1000 | Loss: 0.00001050
Iteration 194/1000 | Loss: 0.00001050
Iteration 195/1000 | Loss: 0.00001050
Iteration 196/1000 | Loss: 0.00001050
Iteration 197/1000 | Loss: 0.00001050
Iteration 198/1000 | Loss: 0.00001049
Iteration 199/1000 | Loss: 0.00001049
Iteration 200/1000 | Loss: 0.00001049
Iteration 201/1000 | Loss: 0.00001049
Iteration 202/1000 | Loss: 0.00001049
Iteration 203/1000 | Loss: 0.00001049
Iteration 204/1000 | Loss: 0.00001049
Iteration 205/1000 | Loss: 0.00001049
Iteration 206/1000 | Loss: 0.00001048
Iteration 207/1000 | Loss: 0.00001048
Iteration 208/1000 | Loss: 0.00001048
Iteration 209/1000 | Loss: 0.00001048
Iteration 210/1000 | Loss: 0.00001048
Iteration 211/1000 | Loss: 0.00001048
Iteration 212/1000 | Loss: 0.00001048
Iteration 213/1000 | Loss: 0.00001048
Iteration 214/1000 | Loss: 0.00001048
Iteration 215/1000 | Loss: 0.00001048
Iteration 216/1000 | Loss: 0.00001048
Iteration 217/1000 | Loss: 0.00001048
Iteration 218/1000 | Loss: 0.00001048
Iteration 219/1000 | Loss: 0.00001047
Iteration 220/1000 | Loss: 0.00001047
Iteration 221/1000 | Loss: 0.00001047
Iteration 222/1000 | Loss: 0.00001047
Iteration 223/1000 | Loss: 0.00001047
Iteration 224/1000 | Loss: 0.00001047
Iteration 225/1000 | Loss: 0.00001047
Iteration 226/1000 | Loss: 0.00001047
Iteration 227/1000 | Loss: 0.00001047
Iteration 228/1000 | Loss: 0.00001047
Iteration 229/1000 | Loss: 0.00001047
Iteration 230/1000 | Loss: 0.00001047
Iteration 231/1000 | Loss: 0.00001046
Iteration 232/1000 | Loss: 0.00001046
Iteration 233/1000 | Loss: 0.00001046
Iteration 234/1000 | Loss: 0.00001046
Iteration 235/1000 | Loss: 0.00001046
Iteration 236/1000 | Loss: 0.00001046
Iteration 237/1000 | Loss: 0.00001046
Iteration 238/1000 | Loss: 0.00001046
Iteration 239/1000 | Loss: 0.00001046
Iteration 240/1000 | Loss: 0.00001046
Iteration 241/1000 | Loss: 0.00001046
Iteration 242/1000 | Loss: 0.00001046
Iteration 243/1000 | Loss: 0.00001046
Iteration 244/1000 | Loss: 0.00001046
Iteration 245/1000 | Loss: 0.00001046
Iteration 246/1000 | Loss: 0.00001046
Iteration 247/1000 | Loss: 0.00001046
Iteration 248/1000 | Loss: 0.00001046
Iteration 249/1000 | Loss: 0.00001046
Iteration 250/1000 | Loss: 0.00001046
Iteration 251/1000 | Loss: 0.00001046
Iteration 252/1000 | Loss: 0.00001046
Iteration 253/1000 | Loss: 0.00001046
Iteration 254/1000 | Loss: 0.00001046
Iteration 255/1000 | Loss: 0.00001046
Iteration 256/1000 | Loss: 0.00001046
Iteration 257/1000 | Loss: 0.00001046
Iteration 258/1000 | Loss: 0.00001046
Iteration 259/1000 | Loss: 0.00001046
Iteration 260/1000 | Loss: 0.00001046
Iteration 261/1000 | Loss: 0.00001046
Iteration 262/1000 | Loss: 0.00001046
Iteration 263/1000 | Loss: 0.00001046
Iteration 264/1000 | Loss: 0.00001046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 264. Stopping optimization.
Last 5 losses: [1.0464778824825771e-05, 1.0464778824825771e-05, 1.0464778824825771e-05, 1.0464778824825771e-05, 1.0464778824825771e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0464778824825771e-05

Optimization complete. Final v2v error: 2.7716104984283447 mm

Highest mean error: 3.158745527267456 mm for frame 78

Lowest mean error: 2.4217424392700195 mm for frame 146

Saving results

Total time: 37.16793727874756
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050714
Iteration 2/25 | Loss: 0.00207442
Iteration 3/25 | Loss: 0.00135303
Iteration 4/25 | Loss: 0.00123245
Iteration 5/25 | Loss: 0.00114611
Iteration 6/25 | Loss: 0.00111629
Iteration 7/25 | Loss: 0.00107981
Iteration 8/25 | Loss: 0.00106911
Iteration 9/25 | Loss: 0.00106403
Iteration 10/25 | Loss: 0.00105442
Iteration 11/25 | Loss: 0.00104769
Iteration 12/25 | Loss: 0.00105034
Iteration 13/25 | Loss: 0.00104257
Iteration 14/25 | Loss: 0.00104462
Iteration 15/25 | Loss: 0.00103858
Iteration 16/25 | Loss: 0.00103435
Iteration 17/25 | Loss: 0.00103310
Iteration 18/25 | Loss: 0.00103190
Iteration 19/25 | Loss: 0.00103153
Iteration 20/25 | Loss: 0.00103140
Iteration 21/25 | Loss: 0.00103129
Iteration 22/25 | Loss: 0.00103118
Iteration 23/25 | Loss: 0.00103352
Iteration 24/25 | Loss: 0.00103294
Iteration 25/25 | Loss: 0.00103568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25649810
Iteration 2/25 | Loss: 0.00146740
Iteration 3/25 | Loss: 0.00092252
Iteration 4/25 | Loss: 0.00092252
Iteration 5/25 | Loss: 0.00092252
Iteration 6/25 | Loss: 0.00092252
Iteration 7/25 | Loss: 0.00092252
Iteration 8/25 | Loss: 0.00092252
Iteration 9/25 | Loss: 0.00092252
Iteration 10/25 | Loss: 0.00092252
Iteration 11/25 | Loss: 0.00092252
Iteration 12/25 | Loss: 0.00092252
Iteration 13/25 | Loss: 0.00092252
Iteration 14/25 | Loss: 0.00092252
Iteration 15/25 | Loss: 0.00092252
Iteration 16/25 | Loss: 0.00092252
Iteration 17/25 | Loss: 0.00092252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009225191897712648, 0.0009225191897712648, 0.0009225191897712648, 0.0009225191897712648, 0.0009225191897712648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009225191897712648

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092252
Iteration 2/1000 | Loss: 0.00061819
Iteration 3/1000 | Loss: 0.00012247
Iteration 4/1000 | Loss: 0.00006074
Iteration 5/1000 | Loss: 0.00003343
Iteration 6/1000 | Loss: 0.00002777
Iteration 7/1000 | Loss: 0.00006163
Iteration 8/1000 | Loss: 0.00009828
Iteration 9/1000 | Loss: 0.00016770
Iteration 10/1000 | Loss: 0.00024390
Iteration 11/1000 | Loss: 0.00022354
Iteration 12/1000 | Loss: 0.00002090
Iteration 13/1000 | Loss: 0.00010322
Iteration 14/1000 | Loss: 0.00012613
Iteration 15/1000 | Loss: 0.00032694
Iteration 16/1000 | Loss: 0.00005515
Iteration 17/1000 | Loss: 0.00001936
Iteration 18/1000 | Loss: 0.00001804
Iteration 19/1000 | Loss: 0.00010640
Iteration 20/1000 | Loss: 0.00023439
Iteration 21/1000 | Loss: 0.00015962
Iteration 22/1000 | Loss: 0.00007811
Iteration 23/1000 | Loss: 0.00012119
Iteration 24/1000 | Loss: 0.00020303
Iteration 25/1000 | Loss: 0.00006993
Iteration 26/1000 | Loss: 0.00015370
Iteration 27/1000 | Loss: 0.00011455
Iteration 28/1000 | Loss: 0.00006691
Iteration 29/1000 | Loss: 0.00003496
Iteration 30/1000 | Loss: 0.00005459
Iteration 31/1000 | Loss: 0.00008531
Iteration 32/1000 | Loss: 0.00011652
Iteration 33/1000 | Loss: 0.00018944
Iteration 34/1000 | Loss: 0.00023000
Iteration 35/1000 | Loss: 0.00005437
Iteration 36/1000 | Loss: 0.00010688
Iteration 37/1000 | Loss: 0.00003735
Iteration 38/1000 | Loss: 0.00001551
Iteration 39/1000 | Loss: 0.00001518
Iteration 40/1000 | Loss: 0.00013002
Iteration 41/1000 | Loss: 0.00001516
Iteration 42/1000 | Loss: 0.00001477
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001466
Iteration 46/1000 | Loss: 0.00001466
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001443
Iteration 51/1000 | Loss: 0.00012911
Iteration 52/1000 | Loss: 0.00031168
Iteration 53/1000 | Loss: 0.00010083
Iteration 54/1000 | Loss: 0.00004975
Iteration 55/1000 | Loss: 0.00008335
Iteration 56/1000 | Loss: 0.00001987
Iteration 57/1000 | Loss: 0.00009499
Iteration 58/1000 | Loss: 0.00019822
Iteration 59/1000 | Loss: 0.00005778
Iteration 60/1000 | Loss: 0.00001511
Iteration 61/1000 | Loss: 0.00013490
Iteration 62/1000 | Loss: 0.00001619
Iteration 63/1000 | Loss: 0.00010238
Iteration 64/1000 | Loss: 0.00006201
Iteration 65/1000 | Loss: 0.00001740
Iteration 66/1000 | Loss: 0.00005672
Iteration 67/1000 | Loss: 0.00001475
Iteration 68/1000 | Loss: 0.00001446
Iteration 69/1000 | Loss: 0.00001441
Iteration 70/1000 | Loss: 0.00006298
Iteration 71/1000 | Loss: 0.00015522
Iteration 72/1000 | Loss: 0.00028792
Iteration 73/1000 | Loss: 0.00013102
Iteration 74/1000 | Loss: 0.00005802
Iteration 75/1000 | Loss: 0.00031280
Iteration 76/1000 | Loss: 0.00072560
Iteration 77/1000 | Loss: 0.00010504
Iteration 78/1000 | Loss: 0.00004505
Iteration 79/1000 | Loss: 0.00005474
Iteration 80/1000 | Loss: 0.00010066
Iteration 81/1000 | Loss: 0.00005602
Iteration 82/1000 | Loss: 0.00006723
Iteration 83/1000 | Loss: 0.00014429
Iteration 84/1000 | Loss: 0.00047522
Iteration 85/1000 | Loss: 0.00007674
Iteration 86/1000 | Loss: 0.00011919
Iteration 87/1000 | Loss: 0.00014078
Iteration 88/1000 | Loss: 0.00012216
Iteration 89/1000 | Loss: 0.00023882
Iteration 90/1000 | Loss: 0.00010419
Iteration 91/1000 | Loss: 0.00016459
Iteration 92/1000 | Loss: 0.00015609
Iteration 93/1000 | Loss: 0.00045414
Iteration 94/1000 | Loss: 0.00017341
Iteration 95/1000 | Loss: 0.00011995
Iteration 96/1000 | Loss: 0.00014656
Iteration 97/1000 | Loss: 0.00014238
Iteration 98/1000 | Loss: 0.00012257
Iteration 99/1000 | Loss: 0.00019075
Iteration 100/1000 | Loss: 0.00024570
Iteration 101/1000 | Loss: 0.00067928
Iteration 102/1000 | Loss: 0.00004474
Iteration 103/1000 | Loss: 0.00002356
Iteration 104/1000 | Loss: 0.00009445
Iteration 105/1000 | Loss: 0.00001914
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001230
Iteration 109/1000 | Loss: 0.00001218
Iteration 110/1000 | Loss: 0.00001198
Iteration 111/1000 | Loss: 0.00001188
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001184
Iteration 114/1000 | Loss: 0.00001183
Iteration 115/1000 | Loss: 0.00001182
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001182
Iteration 118/1000 | Loss: 0.00001181
Iteration 119/1000 | Loss: 0.00001181
Iteration 120/1000 | Loss: 0.00001181
Iteration 121/1000 | Loss: 0.00001180
Iteration 122/1000 | Loss: 0.00001180
Iteration 123/1000 | Loss: 0.00001180
Iteration 124/1000 | Loss: 0.00001179
Iteration 125/1000 | Loss: 0.00001179
Iteration 126/1000 | Loss: 0.00001178
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001176
Iteration 131/1000 | Loss: 0.00001176
Iteration 132/1000 | Loss: 0.00001175
Iteration 133/1000 | Loss: 0.00001174
Iteration 134/1000 | Loss: 0.00001174
Iteration 135/1000 | Loss: 0.00001174
Iteration 136/1000 | Loss: 0.00001174
Iteration 137/1000 | Loss: 0.00001174
Iteration 138/1000 | Loss: 0.00001174
Iteration 139/1000 | Loss: 0.00001174
Iteration 140/1000 | Loss: 0.00001174
Iteration 141/1000 | Loss: 0.00001174
Iteration 142/1000 | Loss: 0.00001174
Iteration 143/1000 | Loss: 0.00001174
Iteration 144/1000 | Loss: 0.00001174
Iteration 145/1000 | Loss: 0.00001173
Iteration 146/1000 | Loss: 0.00001173
Iteration 147/1000 | Loss: 0.00001173
Iteration 148/1000 | Loss: 0.00001172
Iteration 149/1000 | Loss: 0.00001172
Iteration 150/1000 | Loss: 0.00001172
Iteration 151/1000 | Loss: 0.00001172
Iteration 152/1000 | Loss: 0.00001172
Iteration 153/1000 | Loss: 0.00001171
Iteration 154/1000 | Loss: 0.00001171
Iteration 155/1000 | Loss: 0.00001171
Iteration 156/1000 | Loss: 0.00001171
Iteration 157/1000 | Loss: 0.00001170
Iteration 158/1000 | Loss: 0.00001170
Iteration 159/1000 | Loss: 0.00001170
Iteration 160/1000 | Loss: 0.00001170
Iteration 161/1000 | Loss: 0.00001169
Iteration 162/1000 | Loss: 0.00001169
Iteration 163/1000 | Loss: 0.00001169
Iteration 164/1000 | Loss: 0.00001169
Iteration 165/1000 | Loss: 0.00001168
Iteration 166/1000 | Loss: 0.00001168
Iteration 167/1000 | Loss: 0.00001168
Iteration 168/1000 | Loss: 0.00001167
Iteration 169/1000 | Loss: 0.00001167
Iteration 170/1000 | Loss: 0.00001167
Iteration 171/1000 | Loss: 0.00001167
Iteration 172/1000 | Loss: 0.00001167
Iteration 173/1000 | Loss: 0.00001167
Iteration 174/1000 | Loss: 0.00001166
Iteration 175/1000 | Loss: 0.00001166
Iteration 176/1000 | Loss: 0.00001166
Iteration 177/1000 | Loss: 0.00001166
Iteration 178/1000 | Loss: 0.00001166
Iteration 179/1000 | Loss: 0.00001166
Iteration 180/1000 | Loss: 0.00001166
Iteration 181/1000 | Loss: 0.00001166
Iteration 182/1000 | Loss: 0.00001166
Iteration 183/1000 | Loss: 0.00001166
Iteration 184/1000 | Loss: 0.00001166
Iteration 185/1000 | Loss: 0.00001165
Iteration 186/1000 | Loss: 0.00001165
Iteration 187/1000 | Loss: 0.00001165
Iteration 188/1000 | Loss: 0.00001165
Iteration 189/1000 | Loss: 0.00001165
Iteration 190/1000 | Loss: 0.00001165
Iteration 191/1000 | Loss: 0.00001165
Iteration 192/1000 | Loss: 0.00001165
Iteration 193/1000 | Loss: 0.00001165
Iteration 194/1000 | Loss: 0.00001165
Iteration 195/1000 | Loss: 0.00001165
Iteration 196/1000 | Loss: 0.00001165
Iteration 197/1000 | Loss: 0.00001165
Iteration 198/1000 | Loss: 0.00001165
Iteration 199/1000 | Loss: 0.00001165
Iteration 200/1000 | Loss: 0.00001165
Iteration 201/1000 | Loss: 0.00001165
Iteration 202/1000 | Loss: 0.00001165
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.1649555744952522e-05, 1.1649555744952522e-05, 1.1649555744952522e-05, 1.1649555744952522e-05, 1.1649555744952522e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1649555744952522e-05

Optimization complete. Final v2v error: 2.8564648628234863 mm

Highest mean error: 3.858274221420288 mm for frame 179

Lowest mean error: 2.312189817428589 mm for frame 149

Saving results

Total time: 224.36243414878845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01091019
Iteration 2/25 | Loss: 0.00226237
Iteration 3/25 | Loss: 0.00171731
Iteration 4/25 | Loss: 0.00153937
Iteration 5/25 | Loss: 0.00122560
Iteration 6/25 | Loss: 0.00130494
Iteration 7/25 | Loss: 0.00128614
Iteration 8/25 | Loss: 0.00118308
Iteration 9/25 | Loss: 0.00112053
Iteration 10/25 | Loss: 0.00107410
Iteration 11/25 | Loss: 0.00110522
Iteration 12/25 | Loss: 0.00109923
Iteration 13/25 | Loss: 0.00107548
Iteration 14/25 | Loss: 0.00102185
Iteration 15/25 | Loss: 0.00102262
Iteration 16/25 | Loss: 0.00101415
Iteration 17/25 | Loss: 0.00100825
Iteration 18/25 | Loss: 0.00098730
Iteration 19/25 | Loss: 0.00097772
Iteration 20/25 | Loss: 0.00097279
Iteration 21/25 | Loss: 0.00097013
Iteration 22/25 | Loss: 0.00097783
Iteration 23/25 | Loss: 0.00097376
Iteration 24/25 | Loss: 0.00097175
Iteration 25/25 | Loss: 0.00097646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41728103
Iteration 2/25 | Loss: 0.00168924
Iteration 3/25 | Loss: 0.00132477
Iteration 4/25 | Loss: 0.00132477
Iteration 5/25 | Loss: 0.00132477
Iteration 6/25 | Loss: 0.00132477
Iteration 7/25 | Loss: 0.00132477
Iteration 8/25 | Loss: 0.00132477
Iteration 9/25 | Loss: 0.00132477
Iteration 10/25 | Loss: 0.00132477
Iteration 11/25 | Loss: 0.00132477
Iteration 12/25 | Loss: 0.00132477
Iteration 13/25 | Loss: 0.00132477
Iteration 14/25 | Loss: 0.00132477
Iteration 15/25 | Loss: 0.00132477
Iteration 16/25 | Loss: 0.00132477
Iteration 17/25 | Loss: 0.00132477
Iteration 18/25 | Loss: 0.00132477
Iteration 19/25 | Loss: 0.00132477
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0013247678289189935, 0.0013247678289189935, 0.0013247678289189935, 0.0013247678289189935, 0.0013247678289189935]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013247678289189935

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132477
Iteration 2/1000 | Loss: 0.00085143
Iteration 3/1000 | Loss: 0.00063730
Iteration 4/1000 | Loss: 0.00042588
Iteration 5/1000 | Loss: 0.00037178
Iteration 6/1000 | Loss: 0.00040203
Iteration 7/1000 | Loss: 0.00096516
Iteration 8/1000 | Loss: 0.00038119
Iteration 9/1000 | Loss: 0.00033557
Iteration 10/1000 | Loss: 0.00019701
Iteration 11/1000 | Loss: 0.00018850
Iteration 12/1000 | Loss: 0.00010151
Iteration 13/1000 | Loss: 0.00021393
Iteration 14/1000 | Loss: 0.00013719
Iteration 15/1000 | Loss: 0.00014000
Iteration 16/1000 | Loss: 0.00013803
Iteration 17/1000 | Loss: 0.00028732
Iteration 18/1000 | Loss: 0.00031830
Iteration 19/1000 | Loss: 0.00026376
Iteration 20/1000 | Loss: 0.00011048
Iteration 21/1000 | Loss: 0.00034114
Iteration 22/1000 | Loss: 0.00278092
Iteration 23/1000 | Loss: 0.00046740
Iteration 24/1000 | Loss: 0.00217935
Iteration 25/1000 | Loss: 0.00025855
Iteration 26/1000 | Loss: 0.00005709
Iteration 27/1000 | Loss: 0.00031851
Iteration 28/1000 | Loss: 0.00018608
Iteration 29/1000 | Loss: 0.00015962
Iteration 30/1000 | Loss: 0.00005216
Iteration 31/1000 | Loss: 0.00016961
Iteration 32/1000 | Loss: 0.00013830
Iteration 33/1000 | Loss: 0.00030329
Iteration 34/1000 | Loss: 0.00025887
Iteration 35/1000 | Loss: 0.00025400
Iteration 36/1000 | Loss: 0.00020236
Iteration 37/1000 | Loss: 0.00067691
Iteration 38/1000 | Loss: 0.00006069
Iteration 39/1000 | Loss: 0.00043478
Iteration 40/1000 | Loss: 0.00036335
Iteration 41/1000 | Loss: 0.00092758
Iteration 42/1000 | Loss: 0.00006783
Iteration 43/1000 | Loss: 0.00065734
Iteration 44/1000 | Loss: 0.00005064
Iteration 45/1000 | Loss: 0.00004968
Iteration 46/1000 | Loss: 0.00003529
Iteration 47/1000 | Loss: 0.00002455
Iteration 48/1000 | Loss: 0.00002885
Iteration 49/1000 | Loss: 0.00002026
Iteration 50/1000 | Loss: 0.00002434
Iteration 51/1000 | Loss: 0.00003564
Iteration 52/1000 | Loss: 0.00003426
Iteration 53/1000 | Loss: 0.00002318
Iteration 54/1000 | Loss: 0.00002984
Iteration 55/1000 | Loss: 0.00004389
Iteration 56/1000 | Loss: 0.00003129
Iteration 57/1000 | Loss: 0.00003330
Iteration 58/1000 | Loss: 0.00002272
Iteration 59/1000 | Loss: 0.00003254
Iteration 60/1000 | Loss: 0.00003512
Iteration 61/1000 | Loss: 0.00002896
Iteration 62/1000 | Loss: 0.00003510
Iteration 63/1000 | Loss: 0.00002895
Iteration 64/1000 | Loss: 0.00003631
Iteration 65/1000 | Loss: 0.00001853
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001489
Iteration 68/1000 | Loss: 0.00026198
Iteration 69/1000 | Loss: 0.00026196
Iteration 70/1000 | Loss: 0.00004723
Iteration 71/1000 | Loss: 0.00013398
Iteration 72/1000 | Loss: 0.00004538
Iteration 73/1000 | Loss: 0.00003999
Iteration 74/1000 | Loss: 0.00001711
Iteration 75/1000 | Loss: 0.00026495
Iteration 76/1000 | Loss: 0.00018650
Iteration 77/1000 | Loss: 0.00022242
Iteration 78/1000 | Loss: 0.00016943
Iteration 79/1000 | Loss: 0.00014619
Iteration 80/1000 | Loss: 0.00001766
Iteration 81/1000 | Loss: 0.00015294
Iteration 82/1000 | Loss: 0.00011947
Iteration 83/1000 | Loss: 0.00012084
Iteration 84/1000 | Loss: 0.00025093
Iteration 85/1000 | Loss: 0.00030270
Iteration 86/1000 | Loss: 0.00007989
Iteration 87/1000 | Loss: 0.00001862
Iteration 88/1000 | Loss: 0.00005965
Iteration 89/1000 | Loss: 0.00008276
Iteration 90/1000 | Loss: 0.00004965
Iteration 91/1000 | Loss: 0.00007148
Iteration 92/1000 | Loss: 0.00005268
Iteration 93/1000 | Loss: 0.00002285
Iteration 94/1000 | Loss: 0.00031932
Iteration 95/1000 | Loss: 0.00008788
Iteration 96/1000 | Loss: 0.00027613
Iteration 97/1000 | Loss: 0.00003730
Iteration 98/1000 | Loss: 0.00027901
Iteration 99/1000 | Loss: 0.00011411
Iteration 100/1000 | Loss: 0.00057121
Iteration 101/1000 | Loss: 0.00042814
Iteration 102/1000 | Loss: 0.00020421
Iteration 103/1000 | Loss: 0.00002542
Iteration 104/1000 | Loss: 0.00001578
Iteration 105/1000 | Loss: 0.00017067
Iteration 106/1000 | Loss: 0.00001688
Iteration 107/1000 | Loss: 0.00001452
Iteration 108/1000 | Loss: 0.00001341
Iteration 109/1000 | Loss: 0.00001264
Iteration 110/1000 | Loss: 0.00001189
Iteration 111/1000 | Loss: 0.00001172
Iteration 112/1000 | Loss: 0.00001160
Iteration 113/1000 | Loss: 0.00001148
Iteration 114/1000 | Loss: 0.00001144
Iteration 115/1000 | Loss: 0.00001137
Iteration 116/1000 | Loss: 0.00001133
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001129
Iteration 127/1000 | Loss: 0.00001129
Iteration 128/1000 | Loss: 0.00001129
Iteration 129/1000 | Loss: 0.00001129
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001128
Iteration 133/1000 | Loss: 0.00001127
Iteration 134/1000 | Loss: 0.00001126
Iteration 135/1000 | Loss: 0.00001126
Iteration 136/1000 | Loss: 0.00001126
Iteration 137/1000 | Loss: 0.00001126
Iteration 138/1000 | Loss: 0.00001126
Iteration 139/1000 | Loss: 0.00001126
Iteration 140/1000 | Loss: 0.00001126
Iteration 141/1000 | Loss: 0.00001126
Iteration 142/1000 | Loss: 0.00001126
Iteration 143/1000 | Loss: 0.00001126
Iteration 144/1000 | Loss: 0.00001125
Iteration 145/1000 | Loss: 0.00001125
Iteration 146/1000 | Loss: 0.00001125
Iteration 147/1000 | Loss: 0.00001125
Iteration 148/1000 | Loss: 0.00001124
Iteration 149/1000 | Loss: 0.00001124
Iteration 150/1000 | Loss: 0.00001123
Iteration 151/1000 | Loss: 0.00001123
Iteration 152/1000 | Loss: 0.00001123
Iteration 153/1000 | Loss: 0.00001122
Iteration 154/1000 | Loss: 0.00001122
Iteration 155/1000 | Loss: 0.00001122
Iteration 156/1000 | Loss: 0.00001122
Iteration 157/1000 | Loss: 0.00001122
Iteration 158/1000 | Loss: 0.00001121
Iteration 159/1000 | Loss: 0.00001121
Iteration 160/1000 | Loss: 0.00001121
Iteration 161/1000 | Loss: 0.00001120
Iteration 162/1000 | Loss: 0.00001120
Iteration 163/1000 | Loss: 0.00001120
Iteration 164/1000 | Loss: 0.00001120
Iteration 165/1000 | Loss: 0.00001120
Iteration 166/1000 | Loss: 0.00001119
Iteration 167/1000 | Loss: 0.00001119
Iteration 168/1000 | Loss: 0.00001119
Iteration 169/1000 | Loss: 0.00001119
Iteration 170/1000 | Loss: 0.00001119
Iteration 171/1000 | Loss: 0.00001119
Iteration 172/1000 | Loss: 0.00001119
Iteration 173/1000 | Loss: 0.00001119
Iteration 174/1000 | Loss: 0.00001119
Iteration 175/1000 | Loss: 0.00001119
Iteration 176/1000 | Loss: 0.00001119
Iteration 177/1000 | Loss: 0.00001119
Iteration 178/1000 | Loss: 0.00001119
Iteration 179/1000 | Loss: 0.00001119
Iteration 180/1000 | Loss: 0.00001119
Iteration 181/1000 | Loss: 0.00001119
Iteration 182/1000 | Loss: 0.00001119
Iteration 183/1000 | Loss: 0.00001119
Iteration 184/1000 | Loss: 0.00001119
Iteration 185/1000 | Loss: 0.00001119
Iteration 186/1000 | Loss: 0.00001119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.1192698366357945e-05, 1.1192698366357945e-05, 1.1192698366357945e-05, 1.1192698366357945e-05, 1.1192698366357945e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1192698366357945e-05

Optimization complete. Final v2v error: 2.7593166828155518 mm

Highest mean error: 7.945857048034668 mm for frame 104

Lowest mean error: 2.483306407928467 mm for frame 20

Saving results

Total time: 210.62697410583496
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00931592
Iteration 2/25 | Loss: 0.00107628
Iteration 3/25 | Loss: 0.00094778
Iteration 4/25 | Loss: 0.00093367
Iteration 5/25 | Loss: 0.00093065
Iteration 6/25 | Loss: 0.00092968
Iteration 7/25 | Loss: 0.00092968
Iteration 8/25 | Loss: 0.00092968
Iteration 9/25 | Loss: 0.00092968
Iteration 10/25 | Loss: 0.00092968
Iteration 11/25 | Loss: 0.00092968
Iteration 12/25 | Loss: 0.00092968
Iteration 13/25 | Loss: 0.00092968
Iteration 14/25 | Loss: 0.00092968
Iteration 15/25 | Loss: 0.00092968
Iteration 16/25 | Loss: 0.00092968
Iteration 17/25 | Loss: 0.00092968
Iteration 18/25 | Loss: 0.00092968
Iteration 19/25 | Loss: 0.00092968
Iteration 20/25 | Loss: 0.00092968
Iteration 21/25 | Loss: 0.00092968
Iteration 22/25 | Loss: 0.00092968
Iteration 23/25 | Loss: 0.00092968
Iteration 24/25 | Loss: 0.00092968
Iteration 25/25 | Loss: 0.00092968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.89884782
Iteration 2/25 | Loss: 0.00060097
Iteration 3/25 | Loss: 0.00060096
Iteration 4/25 | Loss: 0.00060096
Iteration 5/25 | Loss: 0.00060096
Iteration 6/25 | Loss: 0.00060096
Iteration 7/25 | Loss: 0.00060096
Iteration 8/25 | Loss: 0.00060096
Iteration 9/25 | Loss: 0.00060096
Iteration 10/25 | Loss: 0.00060096
Iteration 11/25 | Loss: 0.00060096
Iteration 12/25 | Loss: 0.00060096
Iteration 13/25 | Loss: 0.00060096
Iteration 14/25 | Loss: 0.00060096
Iteration 15/25 | Loss: 0.00060096
Iteration 16/25 | Loss: 0.00060096
Iteration 17/25 | Loss: 0.00060096
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006009584758430719, 0.0006009584758430719, 0.0006009584758430719, 0.0006009584758430719, 0.0006009584758430719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006009584758430719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060096
Iteration 2/1000 | Loss: 0.00002852
Iteration 3/1000 | Loss: 0.00001633
Iteration 4/1000 | Loss: 0.00001195
Iteration 5/1000 | Loss: 0.00001075
Iteration 6/1000 | Loss: 0.00000996
Iteration 7/1000 | Loss: 0.00000967
Iteration 8/1000 | Loss: 0.00000958
Iteration 9/1000 | Loss: 0.00000940
Iteration 10/1000 | Loss: 0.00000932
Iteration 11/1000 | Loss: 0.00000930
Iteration 12/1000 | Loss: 0.00000929
Iteration 13/1000 | Loss: 0.00000928
Iteration 14/1000 | Loss: 0.00000926
Iteration 15/1000 | Loss: 0.00000925
Iteration 16/1000 | Loss: 0.00000925
Iteration 17/1000 | Loss: 0.00000924
Iteration 18/1000 | Loss: 0.00000921
Iteration 19/1000 | Loss: 0.00000920
Iteration 20/1000 | Loss: 0.00000919
Iteration 21/1000 | Loss: 0.00000918
Iteration 22/1000 | Loss: 0.00000918
Iteration 23/1000 | Loss: 0.00000918
Iteration 24/1000 | Loss: 0.00000917
Iteration 25/1000 | Loss: 0.00000916
Iteration 26/1000 | Loss: 0.00000916
Iteration 27/1000 | Loss: 0.00000916
Iteration 28/1000 | Loss: 0.00000915
Iteration 29/1000 | Loss: 0.00000915
Iteration 30/1000 | Loss: 0.00000914
Iteration 31/1000 | Loss: 0.00000914
Iteration 32/1000 | Loss: 0.00000912
Iteration 33/1000 | Loss: 0.00000912
Iteration 34/1000 | Loss: 0.00000911
Iteration 35/1000 | Loss: 0.00000911
Iteration 36/1000 | Loss: 0.00000911
Iteration 37/1000 | Loss: 0.00000910
Iteration 38/1000 | Loss: 0.00000910
Iteration 39/1000 | Loss: 0.00000910
Iteration 40/1000 | Loss: 0.00000910
Iteration 41/1000 | Loss: 0.00000909
Iteration 42/1000 | Loss: 0.00000909
Iteration 43/1000 | Loss: 0.00000908
Iteration 44/1000 | Loss: 0.00000908
Iteration 45/1000 | Loss: 0.00000908
Iteration 46/1000 | Loss: 0.00000907
Iteration 47/1000 | Loss: 0.00000907
Iteration 48/1000 | Loss: 0.00000907
Iteration 49/1000 | Loss: 0.00000907
Iteration 50/1000 | Loss: 0.00000907
Iteration 51/1000 | Loss: 0.00000906
Iteration 52/1000 | Loss: 0.00000906
Iteration 53/1000 | Loss: 0.00000906
Iteration 54/1000 | Loss: 0.00000906
Iteration 55/1000 | Loss: 0.00000906
Iteration 56/1000 | Loss: 0.00000906
Iteration 57/1000 | Loss: 0.00000905
Iteration 58/1000 | Loss: 0.00000905
Iteration 59/1000 | Loss: 0.00000904
Iteration 60/1000 | Loss: 0.00000904
Iteration 61/1000 | Loss: 0.00000904
Iteration 62/1000 | Loss: 0.00000904
Iteration 63/1000 | Loss: 0.00000904
Iteration 64/1000 | Loss: 0.00000903
Iteration 65/1000 | Loss: 0.00000903
Iteration 66/1000 | Loss: 0.00000903
Iteration 67/1000 | Loss: 0.00000903
Iteration 68/1000 | Loss: 0.00000902
Iteration 69/1000 | Loss: 0.00000902
Iteration 70/1000 | Loss: 0.00000902
Iteration 71/1000 | Loss: 0.00000902
Iteration 72/1000 | Loss: 0.00000902
Iteration 73/1000 | Loss: 0.00000902
Iteration 74/1000 | Loss: 0.00000902
Iteration 75/1000 | Loss: 0.00000902
Iteration 76/1000 | Loss: 0.00000902
Iteration 77/1000 | Loss: 0.00000902
Iteration 78/1000 | Loss: 0.00000901
Iteration 79/1000 | Loss: 0.00000901
Iteration 80/1000 | Loss: 0.00000901
Iteration 81/1000 | Loss: 0.00000901
Iteration 82/1000 | Loss: 0.00000901
Iteration 83/1000 | Loss: 0.00000901
Iteration 84/1000 | Loss: 0.00000901
Iteration 85/1000 | Loss: 0.00000901
Iteration 86/1000 | Loss: 0.00000901
Iteration 87/1000 | Loss: 0.00000901
Iteration 88/1000 | Loss: 0.00000900
Iteration 89/1000 | Loss: 0.00000900
Iteration 90/1000 | Loss: 0.00000900
Iteration 91/1000 | Loss: 0.00000900
Iteration 92/1000 | Loss: 0.00000900
Iteration 93/1000 | Loss: 0.00000900
Iteration 94/1000 | Loss: 0.00000900
Iteration 95/1000 | Loss: 0.00000900
Iteration 96/1000 | Loss: 0.00000900
Iteration 97/1000 | Loss: 0.00000900
Iteration 98/1000 | Loss: 0.00000900
Iteration 99/1000 | Loss: 0.00000900
Iteration 100/1000 | Loss: 0.00000900
Iteration 101/1000 | Loss: 0.00000900
Iteration 102/1000 | Loss: 0.00000900
Iteration 103/1000 | Loss: 0.00000900
Iteration 104/1000 | Loss: 0.00000899
Iteration 105/1000 | Loss: 0.00000899
Iteration 106/1000 | Loss: 0.00000899
Iteration 107/1000 | Loss: 0.00000899
Iteration 108/1000 | Loss: 0.00000899
Iteration 109/1000 | Loss: 0.00000899
Iteration 110/1000 | Loss: 0.00000899
Iteration 111/1000 | Loss: 0.00000899
Iteration 112/1000 | Loss: 0.00000898
Iteration 113/1000 | Loss: 0.00000898
Iteration 114/1000 | Loss: 0.00000898
Iteration 115/1000 | Loss: 0.00000898
Iteration 116/1000 | Loss: 0.00000898
Iteration 117/1000 | Loss: 0.00000898
Iteration 118/1000 | Loss: 0.00000898
Iteration 119/1000 | Loss: 0.00000898
Iteration 120/1000 | Loss: 0.00000898
Iteration 121/1000 | Loss: 0.00000898
Iteration 122/1000 | Loss: 0.00000898
Iteration 123/1000 | Loss: 0.00000898
Iteration 124/1000 | Loss: 0.00000897
Iteration 125/1000 | Loss: 0.00000897
Iteration 126/1000 | Loss: 0.00000897
Iteration 127/1000 | Loss: 0.00000897
Iteration 128/1000 | Loss: 0.00000897
Iteration 129/1000 | Loss: 0.00000897
Iteration 130/1000 | Loss: 0.00000897
Iteration 131/1000 | Loss: 0.00000897
Iteration 132/1000 | Loss: 0.00000897
Iteration 133/1000 | Loss: 0.00000897
Iteration 134/1000 | Loss: 0.00000897
Iteration 135/1000 | Loss: 0.00000896
Iteration 136/1000 | Loss: 0.00000896
Iteration 137/1000 | Loss: 0.00000896
Iteration 138/1000 | Loss: 0.00000896
Iteration 139/1000 | Loss: 0.00000896
Iteration 140/1000 | Loss: 0.00000896
Iteration 141/1000 | Loss: 0.00000896
Iteration 142/1000 | Loss: 0.00000896
Iteration 143/1000 | Loss: 0.00000896
Iteration 144/1000 | Loss: 0.00000896
Iteration 145/1000 | Loss: 0.00000896
Iteration 146/1000 | Loss: 0.00000896
Iteration 147/1000 | Loss: 0.00000896
Iteration 148/1000 | Loss: 0.00000896
Iteration 149/1000 | Loss: 0.00000896
Iteration 150/1000 | Loss: 0.00000896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [8.964078006101772e-06, 8.964078006101772e-06, 8.964078006101772e-06, 8.964078006101772e-06, 8.964078006101772e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.964078006101772e-06

Optimization complete. Final v2v error: 2.5372390747070312 mm

Highest mean error: 2.799820899963379 mm for frame 117

Lowest mean error: 2.3659348487854004 mm for frame 25

Saving results

Total time: 30.998291492462158
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815318
Iteration 2/25 | Loss: 0.00141455
Iteration 3/25 | Loss: 0.00111142
Iteration 4/25 | Loss: 0.00107822
Iteration 5/25 | Loss: 0.00107552
Iteration 6/25 | Loss: 0.00107511
Iteration 7/25 | Loss: 0.00107511
Iteration 8/25 | Loss: 0.00107511
Iteration 9/25 | Loss: 0.00107511
Iteration 10/25 | Loss: 0.00107511
Iteration 11/25 | Loss: 0.00107511
Iteration 12/25 | Loss: 0.00107511
Iteration 13/25 | Loss: 0.00107511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010751067893579602, 0.0010751067893579602, 0.0010751067893579602, 0.0010751067893579602, 0.0010751067893579602]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010751067893579602

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30398548
Iteration 2/25 | Loss: 0.00052186
Iteration 3/25 | Loss: 0.00052183
Iteration 4/25 | Loss: 0.00052183
Iteration 5/25 | Loss: 0.00052183
Iteration 6/25 | Loss: 0.00052183
Iteration 7/25 | Loss: 0.00052183
Iteration 8/25 | Loss: 0.00052183
Iteration 9/25 | Loss: 0.00052183
Iteration 10/25 | Loss: 0.00052183
Iteration 11/25 | Loss: 0.00052183
Iteration 12/25 | Loss: 0.00052183
Iteration 13/25 | Loss: 0.00052183
Iteration 14/25 | Loss: 0.00052183
Iteration 15/25 | Loss: 0.00052183
Iteration 16/25 | Loss: 0.00052183
Iteration 17/25 | Loss: 0.00052183
Iteration 18/25 | Loss: 0.00052183
Iteration 19/25 | Loss: 0.00052183
Iteration 20/25 | Loss: 0.00052183
Iteration 21/25 | Loss: 0.00052183
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0005218313308432698, 0.0005218313308432698, 0.0005218313308432698, 0.0005218313308432698, 0.0005218313308432698]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005218313308432698

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052183
Iteration 2/1000 | Loss: 0.00003819
Iteration 3/1000 | Loss: 0.00002738
Iteration 4/1000 | Loss: 0.00002333
Iteration 5/1000 | Loss: 0.00002166
Iteration 6/1000 | Loss: 0.00002116
Iteration 7/1000 | Loss: 0.00002087
Iteration 8/1000 | Loss: 0.00002059
Iteration 9/1000 | Loss: 0.00002032
Iteration 10/1000 | Loss: 0.00002029
Iteration 11/1000 | Loss: 0.00002021
Iteration 12/1000 | Loss: 0.00001999
Iteration 13/1000 | Loss: 0.00001986
Iteration 14/1000 | Loss: 0.00001980
Iteration 15/1000 | Loss: 0.00001978
Iteration 16/1000 | Loss: 0.00001973
Iteration 17/1000 | Loss: 0.00001972
Iteration 18/1000 | Loss: 0.00001971
Iteration 19/1000 | Loss: 0.00001966
Iteration 20/1000 | Loss: 0.00001965
Iteration 21/1000 | Loss: 0.00001964
Iteration 22/1000 | Loss: 0.00001964
Iteration 23/1000 | Loss: 0.00001963
Iteration 24/1000 | Loss: 0.00001962
Iteration 25/1000 | Loss: 0.00001959
Iteration 26/1000 | Loss: 0.00001959
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001959
Iteration 29/1000 | Loss: 0.00001958
Iteration 30/1000 | Loss: 0.00001958
Iteration 31/1000 | Loss: 0.00001958
Iteration 32/1000 | Loss: 0.00001957
Iteration 33/1000 | Loss: 0.00001957
Iteration 34/1000 | Loss: 0.00001956
Iteration 35/1000 | Loss: 0.00001956
Iteration 36/1000 | Loss: 0.00001956
Iteration 37/1000 | Loss: 0.00001956
Iteration 38/1000 | Loss: 0.00001955
Iteration 39/1000 | Loss: 0.00001955
Iteration 40/1000 | Loss: 0.00001955
Iteration 41/1000 | Loss: 0.00001954
Iteration 42/1000 | Loss: 0.00001954
Iteration 43/1000 | Loss: 0.00001954
Iteration 44/1000 | Loss: 0.00001954
Iteration 45/1000 | Loss: 0.00001954
Iteration 46/1000 | Loss: 0.00001953
Iteration 47/1000 | Loss: 0.00001953
Iteration 48/1000 | Loss: 0.00001953
Iteration 49/1000 | Loss: 0.00001953
Iteration 50/1000 | Loss: 0.00001952
Iteration 51/1000 | Loss: 0.00001952
Iteration 52/1000 | Loss: 0.00001952
Iteration 53/1000 | Loss: 0.00001951
Iteration 54/1000 | Loss: 0.00001951
Iteration 55/1000 | Loss: 0.00001951
Iteration 56/1000 | Loss: 0.00001951
Iteration 57/1000 | Loss: 0.00001951
Iteration 58/1000 | Loss: 0.00001951
Iteration 59/1000 | Loss: 0.00001951
Iteration 60/1000 | Loss: 0.00001951
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001950
Iteration 63/1000 | Loss: 0.00001950
Iteration 64/1000 | Loss: 0.00001950
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00001949
Iteration 73/1000 | Loss: 0.00001949
Iteration 74/1000 | Loss: 0.00001949
Iteration 75/1000 | Loss: 0.00001949
Iteration 76/1000 | Loss: 0.00001949
Iteration 77/1000 | Loss: 0.00001949
Iteration 78/1000 | Loss: 0.00001949
Iteration 79/1000 | Loss: 0.00001949
Iteration 80/1000 | Loss: 0.00001949
Iteration 81/1000 | Loss: 0.00001949
Iteration 82/1000 | Loss: 0.00001949
Iteration 83/1000 | Loss: 0.00001949
Iteration 84/1000 | Loss: 0.00001948
Iteration 85/1000 | Loss: 0.00001948
Iteration 86/1000 | Loss: 0.00001948
Iteration 87/1000 | Loss: 0.00001948
Iteration 88/1000 | Loss: 0.00001948
Iteration 89/1000 | Loss: 0.00001948
Iteration 90/1000 | Loss: 0.00001948
Iteration 91/1000 | Loss: 0.00001948
Iteration 92/1000 | Loss: 0.00001948
Iteration 93/1000 | Loss: 0.00001948
Iteration 94/1000 | Loss: 0.00001948
Iteration 95/1000 | Loss: 0.00001948
Iteration 96/1000 | Loss: 0.00001947
Iteration 97/1000 | Loss: 0.00001947
Iteration 98/1000 | Loss: 0.00001947
Iteration 99/1000 | Loss: 0.00001947
Iteration 100/1000 | Loss: 0.00001947
Iteration 101/1000 | Loss: 0.00001947
Iteration 102/1000 | Loss: 0.00001947
Iteration 103/1000 | Loss: 0.00001947
Iteration 104/1000 | Loss: 0.00001947
Iteration 105/1000 | Loss: 0.00001947
Iteration 106/1000 | Loss: 0.00001947
Iteration 107/1000 | Loss: 0.00001947
Iteration 108/1000 | Loss: 0.00001947
Iteration 109/1000 | Loss: 0.00001947
Iteration 110/1000 | Loss: 0.00001947
Iteration 111/1000 | Loss: 0.00001946
Iteration 112/1000 | Loss: 0.00001946
Iteration 113/1000 | Loss: 0.00001946
Iteration 114/1000 | Loss: 0.00001946
Iteration 115/1000 | Loss: 0.00001946
Iteration 116/1000 | Loss: 0.00001946
Iteration 117/1000 | Loss: 0.00001946
Iteration 118/1000 | Loss: 0.00001946
Iteration 119/1000 | Loss: 0.00001946
Iteration 120/1000 | Loss: 0.00001945
Iteration 121/1000 | Loss: 0.00001945
Iteration 122/1000 | Loss: 0.00001945
Iteration 123/1000 | Loss: 0.00001945
Iteration 124/1000 | Loss: 0.00001945
Iteration 125/1000 | Loss: 0.00001945
Iteration 126/1000 | Loss: 0.00001945
Iteration 127/1000 | Loss: 0.00001945
Iteration 128/1000 | Loss: 0.00001945
Iteration 129/1000 | Loss: 0.00001944
Iteration 130/1000 | Loss: 0.00001944
Iteration 131/1000 | Loss: 0.00001944
Iteration 132/1000 | Loss: 0.00001944
Iteration 133/1000 | Loss: 0.00001944
Iteration 134/1000 | Loss: 0.00001943
Iteration 135/1000 | Loss: 0.00001943
Iteration 136/1000 | Loss: 0.00001943
Iteration 137/1000 | Loss: 0.00001943
Iteration 138/1000 | Loss: 0.00001943
Iteration 139/1000 | Loss: 0.00001943
Iteration 140/1000 | Loss: 0.00001943
Iteration 141/1000 | Loss: 0.00001943
Iteration 142/1000 | Loss: 0.00001942
Iteration 143/1000 | Loss: 0.00001942
Iteration 144/1000 | Loss: 0.00001942
Iteration 145/1000 | Loss: 0.00001942
Iteration 146/1000 | Loss: 0.00001942
Iteration 147/1000 | Loss: 0.00001942
Iteration 148/1000 | Loss: 0.00001942
Iteration 149/1000 | Loss: 0.00001942
Iteration 150/1000 | Loss: 0.00001942
Iteration 151/1000 | Loss: 0.00001942
Iteration 152/1000 | Loss: 0.00001942
Iteration 153/1000 | Loss: 0.00001942
Iteration 154/1000 | Loss: 0.00001942
Iteration 155/1000 | Loss: 0.00001942
Iteration 156/1000 | Loss: 0.00001942
Iteration 157/1000 | Loss: 0.00001942
Iteration 158/1000 | Loss: 0.00001942
Iteration 159/1000 | Loss: 0.00001942
Iteration 160/1000 | Loss: 0.00001942
Iteration 161/1000 | Loss: 0.00001942
Iteration 162/1000 | Loss: 0.00001942
Iteration 163/1000 | Loss: 0.00001942
Iteration 164/1000 | Loss: 0.00001942
Iteration 165/1000 | Loss: 0.00001942
Iteration 166/1000 | Loss: 0.00001942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.9422004697844386e-05, 1.9422004697844386e-05, 1.9422004697844386e-05, 1.9422004697844386e-05, 1.9422004697844386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9422004697844386e-05

Optimization complete. Final v2v error: 3.6303722858428955 mm

Highest mean error: 3.8340485095977783 mm for frame 75

Lowest mean error: 3.471071243286133 mm for frame 64

Saving results

Total time: 35.270456314086914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01126762
Iteration 2/25 | Loss: 0.01126762
Iteration 3/25 | Loss: 0.01126761
Iteration 4/25 | Loss: 0.01126761
Iteration 5/25 | Loss: 0.01126760
Iteration 6/25 | Loss: 0.00329192
Iteration 7/25 | Loss: 0.00198782
Iteration 8/25 | Loss: 0.00178391
Iteration 9/25 | Loss: 0.00165300
Iteration 10/25 | Loss: 0.00145707
Iteration 11/25 | Loss: 0.00141120
Iteration 12/25 | Loss: 0.00139839
Iteration 13/25 | Loss: 0.00138354
Iteration 14/25 | Loss: 0.00137868
Iteration 15/25 | Loss: 0.00137230
Iteration 16/25 | Loss: 0.00137138
Iteration 17/25 | Loss: 0.00137100
Iteration 18/25 | Loss: 0.00137080
Iteration 19/25 | Loss: 0.00137067
Iteration 20/25 | Loss: 0.00137057
Iteration 21/25 | Loss: 0.00137047
Iteration 22/25 | Loss: 0.00137043
Iteration 23/25 | Loss: 0.00137043
Iteration 24/25 | Loss: 0.00137043
Iteration 25/25 | Loss: 0.00137043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20083034
Iteration 2/25 | Loss: 0.00198724
Iteration 3/25 | Loss: 0.00198723
Iteration 4/25 | Loss: 0.00198723
Iteration 5/25 | Loss: 0.00198723
Iteration 6/25 | Loss: 0.00198723
Iteration 7/25 | Loss: 0.00198723
Iteration 8/25 | Loss: 0.00198723
Iteration 9/25 | Loss: 0.00198723
Iteration 10/25 | Loss: 0.00198723
Iteration 11/25 | Loss: 0.00198723
Iteration 12/25 | Loss: 0.00198723
Iteration 13/25 | Loss: 0.00198723
Iteration 14/25 | Loss: 0.00198723
Iteration 15/25 | Loss: 0.00198723
Iteration 16/25 | Loss: 0.00198723
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001987232593819499, 0.001987232593819499, 0.001987232593819499, 0.001987232593819499, 0.001987232593819499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001987232593819499

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198723
Iteration 2/1000 | Loss: 0.00028359
Iteration 3/1000 | Loss: 0.00022957
Iteration 4/1000 | Loss: 0.00020380
Iteration 5/1000 | Loss: 0.00018796
Iteration 6/1000 | Loss: 0.00017840
Iteration 7/1000 | Loss: 0.00017078
Iteration 8/1000 | Loss: 0.00016433
Iteration 9/1000 | Loss: 0.00016008
Iteration 10/1000 | Loss: 0.00015645
Iteration 11/1000 | Loss: 0.00015359
Iteration 12/1000 | Loss: 0.00015215
Iteration 13/1000 | Loss: 0.00015120
Iteration 14/1000 | Loss: 0.00015005
Iteration 15/1000 | Loss: 0.00014880
Iteration 16/1000 | Loss: 0.00014794
Iteration 17/1000 | Loss: 0.00014737
Iteration 18/1000 | Loss: 0.00014696
Iteration 19/1000 | Loss: 0.00014663
Iteration 20/1000 | Loss: 0.00014632
Iteration 21/1000 | Loss: 0.00014601
Iteration 22/1000 | Loss: 0.00014582
Iteration 23/1000 | Loss: 0.00014567
Iteration 24/1000 | Loss: 0.00014548
Iteration 25/1000 | Loss: 0.00014529
Iteration 26/1000 | Loss: 0.00014506
Iteration 27/1000 | Loss: 0.00014490
Iteration 28/1000 | Loss: 0.00014480
Iteration 29/1000 | Loss: 0.00014467
Iteration 30/1000 | Loss: 0.00014465
Iteration 31/1000 | Loss: 0.00014464
Iteration 32/1000 | Loss: 0.00014464
Iteration 33/1000 | Loss: 0.00014462
Iteration 34/1000 | Loss: 0.00014461
Iteration 35/1000 | Loss: 0.00014460
Iteration 36/1000 | Loss: 0.00014459
Iteration 37/1000 | Loss: 0.00014457
Iteration 38/1000 | Loss: 0.00014457
Iteration 39/1000 | Loss: 0.00014457
Iteration 40/1000 | Loss: 0.00014457
Iteration 41/1000 | Loss: 0.00014455
Iteration 42/1000 | Loss: 0.00014452
Iteration 43/1000 | Loss: 0.00014452
Iteration 44/1000 | Loss: 0.00014444
Iteration 45/1000 | Loss: 0.00014443
Iteration 46/1000 | Loss: 0.00014443
Iteration 47/1000 | Loss: 0.00014439
Iteration 48/1000 | Loss: 0.00014435
Iteration 49/1000 | Loss: 0.00014435
Iteration 50/1000 | Loss: 0.00014435
Iteration 51/1000 | Loss: 0.00014435
Iteration 52/1000 | Loss: 0.00014433
Iteration 53/1000 | Loss: 0.00014433
Iteration 54/1000 | Loss: 0.00014431
Iteration 55/1000 | Loss: 0.00014430
Iteration 56/1000 | Loss: 0.00014430
Iteration 57/1000 | Loss: 0.00014430
Iteration 58/1000 | Loss: 0.00014430
Iteration 59/1000 | Loss: 0.00014430
Iteration 60/1000 | Loss: 0.00014430
Iteration 61/1000 | Loss: 0.00014430
Iteration 62/1000 | Loss: 0.00014430
Iteration 63/1000 | Loss: 0.00014430
Iteration 64/1000 | Loss: 0.00014429
Iteration 65/1000 | Loss: 0.00014429
Iteration 66/1000 | Loss: 0.00014428
Iteration 67/1000 | Loss: 0.00014428
Iteration 68/1000 | Loss: 0.00014427
Iteration 69/1000 | Loss: 0.00014426
Iteration 70/1000 | Loss: 0.00014426
Iteration 71/1000 | Loss: 0.00014426
Iteration 72/1000 | Loss: 0.00014425
Iteration 73/1000 | Loss: 0.00014425
Iteration 74/1000 | Loss: 0.00014425
Iteration 75/1000 | Loss: 0.00014425
Iteration 76/1000 | Loss: 0.00014425
Iteration 77/1000 | Loss: 0.00014425
Iteration 78/1000 | Loss: 0.00014425
Iteration 79/1000 | Loss: 0.00014424
Iteration 80/1000 | Loss: 0.00014424
Iteration 81/1000 | Loss: 0.00014424
Iteration 82/1000 | Loss: 0.00014424
Iteration 83/1000 | Loss: 0.00014422
Iteration 84/1000 | Loss: 0.00014421
Iteration 85/1000 | Loss: 0.00014421
Iteration 86/1000 | Loss: 0.00014420
Iteration 87/1000 | Loss: 0.00014420
Iteration 88/1000 | Loss: 0.00014420
Iteration 89/1000 | Loss: 0.00014419
Iteration 90/1000 | Loss: 0.00014419
Iteration 91/1000 | Loss: 0.00014419
Iteration 92/1000 | Loss: 0.00014418
Iteration 93/1000 | Loss: 0.00014418
Iteration 94/1000 | Loss: 0.00014417
Iteration 95/1000 | Loss: 0.00014417
Iteration 96/1000 | Loss: 0.00014417
Iteration 97/1000 | Loss: 0.00014417
Iteration 98/1000 | Loss: 0.00014417
Iteration 99/1000 | Loss: 0.00014417
Iteration 100/1000 | Loss: 0.00014417
Iteration 101/1000 | Loss: 0.00014416
Iteration 102/1000 | Loss: 0.00014416
Iteration 103/1000 | Loss: 0.00014416
Iteration 104/1000 | Loss: 0.00014416
Iteration 105/1000 | Loss: 0.00014416
Iteration 106/1000 | Loss: 0.00014416
Iteration 107/1000 | Loss: 0.00014416
Iteration 108/1000 | Loss: 0.00014416
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [0.00014416046906262636, 0.00014416046906262636, 0.00014416046906262636, 0.00014416046906262636, 0.00014416046906262636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00014416046906262636

Optimization complete. Final v2v error: 6.694440841674805 mm

Highest mean error: 20.008838653564453 mm for frame 26

Lowest mean error: 4.2533063888549805 mm for frame 144

Saving results

Total time: 93.03069949150085
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462074
Iteration 2/25 | Loss: 0.00122538
Iteration 3/25 | Loss: 0.00100442
Iteration 4/25 | Loss: 0.00098242
Iteration 5/25 | Loss: 0.00097522
Iteration 6/25 | Loss: 0.00097270
Iteration 7/25 | Loss: 0.00097224
Iteration 8/25 | Loss: 0.00097224
Iteration 9/25 | Loss: 0.00097224
Iteration 10/25 | Loss: 0.00097224
Iteration 11/25 | Loss: 0.00097224
Iteration 12/25 | Loss: 0.00097224
Iteration 13/25 | Loss: 0.00097224
Iteration 14/25 | Loss: 0.00097224
Iteration 15/25 | Loss: 0.00097224
Iteration 16/25 | Loss: 0.00097224
Iteration 17/25 | Loss: 0.00097224
Iteration 18/25 | Loss: 0.00097224
Iteration 19/25 | Loss: 0.00097224
Iteration 20/25 | Loss: 0.00097224
Iteration 21/25 | Loss: 0.00097224
Iteration 22/25 | Loss: 0.00097224
Iteration 23/25 | Loss: 0.00097224
Iteration 24/25 | Loss: 0.00097224
Iteration 25/25 | Loss: 0.00097224

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34122276
Iteration 2/25 | Loss: 0.00076362
Iteration 3/25 | Loss: 0.00076362
Iteration 4/25 | Loss: 0.00076362
Iteration 5/25 | Loss: 0.00076362
Iteration 6/25 | Loss: 0.00076362
Iteration 7/25 | Loss: 0.00076362
Iteration 8/25 | Loss: 0.00076362
Iteration 9/25 | Loss: 0.00076362
Iteration 10/25 | Loss: 0.00076362
Iteration 11/25 | Loss: 0.00076362
Iteration 12/25 | Loss: 0.00076362
Iteration 13/25 | Loss: 0.00076362
Iteration 14/25 | Loss: 0.00076362
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007636197842657566, 0.0007636197842657566, 0.0007636197842657566, 0.0007636197842657566, 0.0007636197842657566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007636197842657566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076362
Iteration 2/1000 | Loss: 0.00002996
Iteration 3/1000 | Loss: 0.00002212
Iteration 4/1000 | Loss: 0.00002061
Iteration 5/1000 | Loss: 0.00001998
Iteration 6/1000 | Loss: 0.00001949
Iteration 7/1000 | Loss: 0.00001916
Iteration 8/1000 | Loss: 0.00001896
Iteration 9/1000 | Loss: 0.00001891
Iteration 10/1000 | Loss: 0.00001876
Iteration 11/1000 | Loss: 0.00001865
Iteration 12/1000 | Loss: 0.00001863
Iteration 13/1000 | Loss: 0.00001857
Iteration 14/1000 | Loss: 0.00001853
Iteration 15/1000 | Loss: 0.00001852
Iteration 16/1000 | Loss: 0.00001852
Iteration 17/1000 | Loss: 0.00001850
Iteration 18/1000 | Loss: 0.00001850
Iteration 19/1000 | Loss: 0.00001849
Iteration 20/1000 | Loss: 0.00001849
Iteration 21/1000 | Loss: 0.00001849
Iteration 22/1000 | Loss: 0.00001849
Iteration 23/1000 | Loss: 0.00001849
Iteration 24/1000 | Loss: 0.00001849
Iteration 25/1000 | Loss: 0.00001849
Iteration 26/1000 | Loss: 0.00001848
Iteration 27/1000 | Loss: 0.00001848
Iteration 28/1000 | Loss: 0.00001848
Iteration 29/1000 | Loss: 0.00001848
Iteration 30/1000 | Loss: 0.00001848
Iteration 31/1000 | Loss: 0.00001847
Iteration 32/1000 | Loss: 0.00001847
Iteration 33/1000 | Loss: 0.00001847
Iteration 34/1000 | Loss: 0.00001846
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00001845
Iteration 37/1000 | Loss: 0.00001845
Iteration 38/1000 | Loss: 0.00001845
Iteration 39/1000 | Loss: 0.00001845
Iteration 40/1000 | Loss: 0.00001843
Iteration 41/1000 | Loss: 0.00001843
Iteration 42/1000 | Loss: 0.00001842
Iteration 43/1000 | Loss: 0.00001842
Iteration 44/1000 | Loss: 0.00001842
Iteration 45/1000 | Loss: 0.00001842
Iteration 46/1000 | Loss: 0.00001842
Iteration 47/1000 | Loss: 0.00001841
Iteration 48/1000 | Loss: 0.00001841
Iteration 49/1000 | Loss: 0.00001841
Iteration 50/1000 | Loss: 0.00001841
Iteration 51/1000 | Loss: 0.00001841
Iteration 52/1000 | Loss: 0.00001841
Iteration 53/1000 | Loss: 0.00001840
Iteration 54/1000 | Loss: 0.00001840
Iteration 55/1000 | Loss: 0.00001840
Iteration 56/1000 | Loss: 0.00001840
Iteration 57/1000 | Loss: 0.00001840
Iteration 58/1000 | Loss: 0.00001840
Iteration 59/1000 | Loss: 0.00001840
Iteration 60/1000 | Loss: 0.00001839
Iteration 61/1000 | Loss: 0.00001839
Iteration 62/1000 | Loss: 0.00001838
Iteration 63/1000 | Loss: 0.00001838
Iteration 64/1000 | Loss: 0.00001838
Iteration 65/1000 | Loss: 0.00001838
Iteration 66/1000 | Loss: 0.00001837
Iteration 67/1000 | Loss: 0.00001837
Iteration 68/1000 | Loss: 0.00001836
Iteration 69/1000 | Loss: 0.00001836
Iteration 70/1000 | Loss: 0.00001836
Iteration 71/1000 | Loss: 0.00001835
Iteration 72/1000 | Loss: 0.00001835
Iteration 73/1000 | Loss: 0.00001834
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001834
Iteration 78/1000 | Loss: 0.00001833
Iteration 79/1000 | Loss: 0.00001833
Iteration 80/1000 | Loss: 0.00001832
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001832
Iteration 83/1000 | Loss: 0.00001832
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001831
Iteration 86/1000 | Loss: 0.00001831
Iteration 87/1000 | Loss: 0.00001831
Iteration 88/1000 | Loss: 0.00001831
Iteration 89/1000 | Loss: 0.00001831
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001829
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001828
Iteration 97/1000 | Loss: 0.00001828
Iteration 98/1000 | Loss: 0.00001828
Iteration 99/1000 | Loss: 0.00001828
Iteration 100/1000 | Loss: 0.00001828
Iteration 101/1000 | Loss: 0.00001827
Iteration 102/1000 | Loss: 0.00001827
Iteration 103/1000 | Loss: 0.00001827
Iteration 104/1000 | Loss: 0.00001827
Iteration 105/1000 | Loss: 0.00001826
Iteration 106/1000 | Loss: 0.00001826
Iteration 107/1000 | Loss: 0.00001826
Iteration 108/1000 | Loss: 0.00001826
Iteration 109/1000 | Loss: 0.00001826
Iteration 110/1000 | Loss: 0.00001826
Iteration 111/1000 | Loss: 0.00001826
Iteration 112/1000 | Loss: 0.00001826
Iteration 113/1000 | Loss: 0.00001825
Iteration 114/1000 | Loss: 0.00001825
Iteration 115/1000 | Loss: 0.00001825
Iteration 116/1000 | Loss: 0.00001825
Iteration 117/1000 | Loss: 0.00001825
Iteration 118/1000 | Loss: 0.00001825
Iteration 119/1000 | Loss: 0.00001825
Iteration 120/1000 | Loss: 0.00001824
Iteration 121/1000 | Loss: 0.00001824
Iteration 122/1000 | Loss: 0.00001824
Iteration 123/1000 | Loss: 0.00001823
Iteration 124/1000 | Loss: 0.00001823
Iteration 125/1000 | Loss: 0.00001823
Iteration 126/1000 | Loss: 0.00001823
Iteration 127/1000 | Loss: 0.00001823
Iteration 128/1000 | Loss: 0.00001823
Iteration 129/1000 | Loss: 0.00001823
Iteration 130/1000 | Loss: 0.00001823
Iteration 131/1000 | Loss: 0.00001822
Iteration 132/1000 | Loss: 0.00001822
Iteration 133/1000 | Loss: 0.00001822
Iteration 134/1000 | Loss: 0.00001822
Iteration 135/1000 | Loss: 0.00001822
Iteration 136/1000 | Loss: 0.00001822
Iteration 137/1000 | Loss: 0.00001822
Iteration 138/1000 | Loss: 0.00001822
Iteration 139/1000 | Loss: 0.00001822
Iteration 140/1000 | Loss: 0.00001822
Iteration 141/1000 | Loss: 0.00001821
Iteration 142/1000 | Loss: 0.00001821
Iteration 143/1000 | Loss: 0.00001821
Iteration 144/1000 | Loss: 0.00001821
Iteration 145/1000 | Loss: 0.00001821
Iteration 146/1000 | Loss: 0.00001820
Iteration 147/1000 | Loss: 0.00001820
Iteration 148/1000 | Loss: 0.00001820
Iteration 149/1000 | Loss: 0.00001819
Iteration 150/1000 | Loss: 0.00001819
Iteration 151/1000 | Loss: 0.00001819
Iteration 152/1000 | Loss: 0.00001819
Iteration 153/1000 | Loss: 0.00001819
Iteration 154/1000 | Loss: 0.00001819
Iteration 155/1000 | Loss: 0.00001819
Iteration 156/1000 | Loss: 0.00001819
Iteration 157/1000 | Loss: 0.00001818
Iteration 158/1000 | Loss: 0.00001818
Iteration 159/1000 | Loss: 0.00001818
Iteration 160/1000 | Loss: 0.00001818
Iteration 161/1000 | Loss: 0.00001818
Iteration 162/1000 | Loss: 0.00001818
Iteration 163/1000 | Loss: 0.00001818
Iteration 164/1000 | Loss: 0.00001818
Iteration 165/1000 | Loss: 0.00001818
Iteration 166/1000 | Loss: 0.00001818
Iteration 167/1000 | Loss: 0.00001818
Iteration 168/1000 | Loss: 0.00001818
Iteration 169/1000 | Loss: 0.00001818
Iteration 170/1000 | Loss: 0.00001818
Iteration 171/1000 | Loss: 0.00001818
Iteration 172/1000 | Loss: 0.00001818
Iteration 173/1000 | Loss: 0.00001818
Iteration 174/1000 | Loss: 0.00001818
Iteration 175/1000 | Loss: 0.00001818
Iteration 176/1000 | Loss: 0.00001818
Iteration 177/1000 | Loss: 0.00001818
Iteration 178/1000 | Loss: 0.00001818
Iteration 179/1000 | Loss: 0.00001818
Iteration 180/1000 | Loss: 0.00001818
Iteration 181/1000 | Loss: 0.00001818
Iteration 182/1000 | Loss: 0.00001818
Iteration 183/1000 | Loss: 0.00001818
Iteration 184/1000 | Loss: 0.00001818
Iteration 185/1000 | Loss: 0.00001818
Iteration 186/1000 | Loss: 0.00001818
Iteration 187/1000 | Loss: 0.00001818
Iteration 188/1000 | Loss: 0.00001818
Iteration 189/1000 | Loss: 0.00001818
Iteration 190/1000 | Loss: 0.00001818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [1.8181766790803522e-05, 1.8181766790803522e-05, 1.8181766790803522e-05, 1.8181766790803522e-05, 1.8181766790803522e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8181766790803522e-05

Optimization complete. Final v2v error: 3.344648838043213 mm

Highest mean error: 4.170258522033691 mm for frame 116

Lowest mean error: 2.641890525817871 mm for frame 204

Saving results

Total time: 44.285006284713745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058952
Iteration 2/25 | Loss: 0.00246372
Iteration 3/25 | Loss: 0.00244743
Iteration 4/25 | Loss: 0.00201730
Iteration 5/25 | Loss: 0.00184202
Iteration 6/25 | Loss: 0.00169916
Iteration 7/25 | Loss: 0.00151074
Iteration 8/25 | Loss: 0.00131269
Iteration 9/25 | Loss: 0.00127156
Iteration 10/25 | Loss: 0.00124004
Iteration 11/25 | Loss: 0.00120080
Iteration 12/25 | Loss: 0.00120183
Iteration 13/25 | Loss: 0.00119276
Iteration 14/25 | Loss: 0.00118348
Iteration 15/25 | Loss: 0.00115658
Iteration 16/25 | Loss: 0.00114150
Iteration 17/25 | Loss: 0.00113987
Iteration 18/25 | Loss: 0.00113444
Iteration 19/25 | Loss: 0.00112512
Iteration 20/25 | Loss: 0.00112733
Iteration 21/25 | Loss: 0.00112222
Iteration 22/25 | Loss: 0.00112149
Iteration 23/25 | Loss: 0.00112136
Iteration 24/25 | Loss: 0.00112124
Iteration 25/25 | Loss: 0.00112112

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35604644
Iteration 2/25 | Loss: 0.00202762
Iteration 3/25 | Loss: 0.00375004
Iteration 4/25 | Loss: 0.00476222
Iteration 5/25 | Loss: 0.00148636
Iteration 6/25 | Loss: 0.00115258
Iteration 7/25 | Loss: 0.00113910
Iteration 8/25 | Loss: 0.00113910
Iteration 9/25 | Loss: 0.00113909
Iteration 10/25 | Loss: 0.00113909
Iteration 11/25 | Loss: 0.00113909
Iteration 12/25 | Loss: 0.00113909
Iteration 13/25 | Loss: 0.00113909
Iteration 14/25 | Loss: 0.00113909
Iteration 15/25 | Loss: 0.00113909
Iteration 16/25 | Loss: 0.00113909
Iteration 17/25 | Loss: 0.00113909
Iteration 18/25 | Loss: 0.00113909
Iteration 19/25 | Loss: 0.00113909
Iteration 20/25 | Loss: 0.00113909
Iteration 21/25 | Loss: 0.00113909
Iteration 22/25 | Loss: 0.00113909
Iteration 23/25 | Loss: 0.00113909
Iteration 24/25 | Loss: 0.00113909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011390929576009512, 0.0011390929576009512, 0.0011390929576009512, 0.0011390929576009512, 0.0011390929576009512]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011390929576009512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113909
Iteration 2/1000 | Loss: 0.00093893
Iteration 3/1000 | Loss: 0.00021569
Iteration 4/1000 | Loss: 0.00063476
Iteration 5/1000 | Loss: 0.00033904
Iteration 6/1000 | Loss: 0.00045154
Iteration 7/1000 | Loss: 0.00041082
Iteration 8/1000 | Loss: 0.00147888
Iteration 9/1000 | Loss: 0.00313392
Iteration 10/1000 | Loss: 0.00253088
Iteration 11/1000 | Loss: 0.00243105
Iteration 12/1000 | Loss: 0.00262959
Iteration 13/1000 | Loss: 0.00073521
Iteration 14/1000 | Loss: 0.00034190
Iteration 15/1000 | Loss: 0.00036374
Iteration 16/1000 | Loss: 0.00035230
Iteration 17/1000 | Loss: 0.00009261
Iteration 18/1000 | Loss: 0.00009717
Iteration 19/1000 | Loss: 0.00043359
Iteration 20/1000 | Loss: 0.00114841
Iteration 21/1000 | Loss: 0.00016432
Iteration 22/1000 | Loss: 0.00105814
Iteration 23/1000 | Loss: 0.00312938
Iteration 24/1000 | Loss: 0.00263182
Iteration 25/1000 | Loss: 0.00039481
Iteration 26/1000 | Loss: 0.00220768
Iteration 27/1000 | Loss: 0.00011101
Iteration 28/1000 | Loss: 0.00012805
Iteration 29/1000 | Loss: 0.00008570
Iteration 30/1000 | Loss: 0.00029339
Iteration 31/1000 | Loss: 0.00115259
Iteration 32/1000 | Loss: 0.00012215
Iteration 33/1000 | Loss: 0.00008246
Iteration 34/1000 | Loss: 0.00008097
Iteration 35/1000 | Loss: 0.00021221
Iteration 36/1000 | Loss: 0.00041658
Iteration 37/1000 | Loss: 0.00015052
Iteration 38/1000 | Loss: 0.00008118
Iteration 39/1000 | Loss: 0.00020361
Iteration 40/1000 | Loss: 0.00019293
Iteration 41/1000 | Loss: 0.00007505
Iteration 42/1000 | Loss: 0.00007276
Iteration 43/1000 | Loss: 0.00016600
Iteration 44/1000 | Loss: 0.00007387
Iteration 45/1000 | Loss: 0.00028841
Iteration 46/1000 | Loss: 0.00007216
Iteration 47/1000 | Loss: 0.00020955
Iteration 48/1000 | Loss: 0.00008075
Iteration 49/1000 | Loss: 0.00117264
Iteration 50/1000 | Loss: 0.00159085
Iteration 51/1000 | Loss: 0.00297442
Iteration 52/1000 | Loss: 0.00190937
Iteration 53/1000 | Loss: 0.00054173
Iteration 54/1000 | Loss: 0.00011070
Iteration 55/1000 | Loss: 0.00042000
Iteration 56/1000 | Loss: 0.00006892
Iteration 57/1000 | Loss: 0.00004910
Iteration 58/1000 | Loss: 0.00008185
Iteration 59/1000 | Loss: 0.00008885
Iteration 60/1000 | Loss: 0.00036863
Iteration 61/1000 | Loss: 0.00003149
Iteration 62/1000 | Loss: 0.00002794
Iteration 63/1000 | Loss: 0.00004583
Iteration 64/1000 | Loss: 0.00006793
Iteration 65/1000 | Loss: 0.00003296
Iteration 66/1000 | Loss: 0.00003848
Iteration 67/1000 | Loss: 0.00003530
Iteration 68/1000 | Loss: 0.00003319
Iteration 69/1000 | Loss: 0.00002588
Iteration 70/1000 | Loss: 0.00002163
Iteration 71/1000 | Loss: 0.00008029
Iteration 72/1000 | Loss: 0.00002735
Iteration 73/1000 | Loss: 0.00033345
Iteration 74/1000 | Loss: 0.00002627
Iteration 75/1000 | Loss: 0.00002496
Iteration 76/1000 | Loss: 0.00002381
Iteration 77/1000 | Loss: 0.00002999
Iteration 78/1000 | Loss: 0.00002820
Iteration 79/1000 | Loss: 0.00002718
Iteration 80/1000 | Loss: 0.00002742
Iteration 81/1000 | Loss: 0.00002374
Iteration 82/1000 | Loss: 0.00002506
Iteration 83/1000 | Loss: 0.00003318
Iteration 84/1000 | Loss: 0.00002675
Iteration 85/1000 | Loss: 0.00003116
Iteration 86/1000 | Loss: 0.00003098
Iteration 87/1000 | Loss: 0.00002558
Iteration 88/1000 | Loss: 0.00002854
Iteration 89/1000 | Loss: 0.00002964
Iteration 90/1000 | Loss: 0.00002645
Iteration 91/1000 | Loss: 0.00003468
Iteration 92/1000 | Loss: 0.00002884
Iteration 93/1000 | Loss: 0.00003367
Iteration 94/1000 | Loss: 0.00003225
Iteration 95/1000 | Loss: 0.00003001
Iteration 96/1000 | Loss: 0.00006226
Iteration 97/1000 | Loss: 0.00004041
Iteration 98/1000 | Loss: 0.00002927
Iteration 99/1000 | Loss: 0.00003387
Iteration 100/1000 | Loss: 0.00003301
Iteration 101/1000 | Loss: 0.00018918
Iteration 102/1000 | Loss: 0.00003696
Iteration 103/1000 | Loss: 0.00003302
Iteration 104/1000 | Loss: 0.00003165
Iteration 105/1000 | Loss: 0.00003412
Iteration 106/1000 | Loss: 0.00013232
Iteration 107/1000 | Loss: 0.00024326
Iteration 108/1000 | Loss: 0.00005744
Iteration 109/1000 | Loss: 0.00002870
Iteration 110/1000 | Loss: 0.00008370
Iteration 111/1000 | Loss: 0.00002843
Iteration 112/1000 | Loss: 0.00003535
Iteration 113/1000 | Loss: 0.00003022
Iteration 114/1000 | Loss: 0.00003422
Iteration 115/1000 | Loss: 0.00003057
Iteration 116/1000 | Loss: 0.00003693
Iteration 117/1000 | Loss: 0.00004196
Iteration 118/1000 | Loss: 0.00023761
Iteration 119/1000 | Loss: 0.00014468
Iteration 120/1000 | Loss: 0.00043198
Iteration 121/1000 | Loss: 0.00002777
Iteration 122/1000 | Loss: 0.00004388
Iteration 123/1000 | Loss: 0.00003710
Iteration 124/1000 | Loss: 0.00003836
Iteration 125/1000 | Loss: 0.00008639
Iteration 126/1000 | Loss: 0.00003784
Iteration 127/1000 | Loss: 0.00002803
Iteration 128/1000 | Loss: 0.00003321
Iteration 129/1000 | Loss: 0.00003708
Iteration 130/1000 | Loss: 0.00013238
Iteration 131/1000 | Loss: 0.00004538
Iteration 132/1000 | Loss: 0.00003395
Iteration 133/1000 | Loss: 0.00004033
Iteration 134/1000 | Loss: 0.00003996
Iteration 135/1000 | Loss: 0.00005562
Iteration 136/1000 | Loss: 0.00004315
Iteration 137/1000 | Loss: 0.00001991
Iteration 138/1000 | Loss: 0.00001924
Iteration 139/1000 | Loss: 0.00004628
Iteration 140/1000 | Loss: 0.00001856
Iteration 141/1000 | Loss: 0.00001836
Iteration 142/1000 | Loss: 0.00001827
Iteration 143/1000 | Loss: 0.00005232
Iteration 144/1000 | Loss: 0.00011208
Iteration 145/1000 | Loss: 0.00001904
Iteration 146/1000 | Loss: 0.00001827
Iteration 147/1000 | Loss: 0.00001817
Iteration 148/1000 | Loss: 0.00001816
Iteration 149/1000 | Loss: 0.00001815
Iteration 150/1000 | Loss: 0.00001813
Iteration 151/1000 | Loss: 0.00001813
Iteration 152/1000 | Loss: 0.00001813
Iteration 153/1000 | Loss: 0.00001813
Iteration 154/1000 | Loss: 0.00001813
Iteration 155/1000 | Loss: 0.00001813
Iteration 156/1000 | Loss: 0.00001813
Iteration 157/1000 | Loss: 0.00001813
Iteration 158/1000 | Loss: 0.00001812
Iteration 159/1000 | Loss: 0.00001812
Iteration 160/1000 | Loss: 0.00001812
Iteration 161/1000 | Loss: 0.00001812
Iteration 162/1000 | Loss: 0.00001812
Iteration 163/1000 | Loss: 0.00001812
Iteration 164/1000 | Loss: 0.00001812
Iteration 165/1000 | Loss: 0.00001812
Iteration 166/1000 | Loss: 0.00001812
Iteration 167/1000 | Loss: 0.00001812
Iteration 168/1000 | Loss: 0.00001812
Iteration 169/1000 | Loss: 0.00001812
Iteration 170/1000 | Loss: 0.00001812
Iteration 171/1000 | Loss: 0.00001811
Iteration 172/1000 | Loss: 0.00001811
Iteration 173/1000 | Loss: 0.00001811
Iteration 174/1000 | Loss: 0.00001811
Iteration 175/1000 | Loss: 0.00001810
Iteration 176/1000 | Loss: 0.00001810
Iteration 177/1000 | Loss: 0.00001810
Iteration 178/1000 | Loss: 0.00001810
Iteration 179/1000 | Loss: 0.00001810
Iteration 180/1000 | Loss: 0.00001810
Iteration 181/1000 | Loss: 0.00001810
Iteration 182/1000 | Loss: 0.00001810
Iteration 183/1000 | Loss: 0.00001810
Iteration 184/1000 | Loss: 0.00001810
Iteration 185/1000 | Loss: 0.00001809
Iteration 186/1000 | Loss: 0.00001809
Iteration 187/1000 | Loss: 0.00001809
Iteration 188/1000 | Loss: 0.00001809
Iteration 189/1000 | Loss: 0.00001809
Iteration 190/1000 | Loss: 0.00001808
Iteration 191/1000 | Loss: 0.00001808
Iteration 192/1000 | Loss: 0.00001808
Iteration 193/1000 | Loss: 0.00001807
Iteration 194/1000 | Loss: 0.00001807
Iteration 195/1000 | Loss: 0.00001807
Iteration 196/1000 | Loss: 0.00001807
Iteration 197/1000 | Loss: 0.00001807
Iteration 198/1000 | Loss: 0.00001807
Iteration 199/1000 | Loss: 0.00001807
Iteration 200/1000 | Loss: 0.00001807
Iteration 201/1000 | Loss: 0.00001807
Iteration 202/1000 | Loss: 0.00001807
Iteration 203/1000 | Loss: 0.00001807
Iteration 204/1000 | Loss: 0.00001807
Iteration 205/1000 | Loss: 0.00001807
Iteration 206/1000 | Loss: 0.00001806
Iteration 207/1000 | Loss: 0.00001806
Iteration 208/1000 | Loss: 0.00001806
Iteration 209/1000 | Loss: 0.00001806
Iteration 210/1000 | Loss: 0.00001806
Iteration 211/1000 | Loss: 0.00001806
Iteration 212/1000 | Loss: 0.00001806
Iteration 213/1000 | Loss: 0.00001806
Iteration 214/1000 | Loss: 0.00001806
Iteration 215/1000 | Loss: 0.00001805
Iteration 216/1000 | Loss: 0.00001805
Iteration 217/1000 | Loss: 0.00001805
Iteration 218/1000 | Loss: 0.00001805
Iteration 219/1000 | Loss: 0.00001805
Iteration 220/1000 | Loss: 0.00001805
Iteration 221/1000 | Loss: 0.00001805
Iteration 222/1000 | Loss: 0.00001805
Iteration 223/1000 | Loss: 0.00001805
Iteration 224/1000 | Loss: 0.00001805
Iteration 225/1000 | Loss: 0.00001805
Iteration 226/1000 | Loss: 0.00001805
Iteration 227/1000 | Loss: 0.00001804
Iteration 228/1000 | Loss: 0.00001804
Iteration 229/1000 | Loss: 0.00001804
Iteration 230/1000 | Loss: 0.00001804
Iteration 231/1000 | Loss: 0.00001804
Iteration 232/1000 | Loss: 0.00001804
Iteration 233/1000 | Loss: 0.00001804
Iteration 234/1000 | Loss: 0.00001804
Iteration 235/1000 | Loss: 0.00006220
Iteration 236/1000 | Loss: 0.00001821
Iteration 237/1000 | Loss: 0.00001804
Iteration 238/1000 | Loss: 0.00001804
Iteration 239/1000 | Loss: 0.00001804
Iteration 240/1000 | Loss: 0.00001804
Iteration 241/1000 | Loss: 0.00001804
Iteration 242/1000 | Loss: 0.00001804
Iteration 243/1000 | Loss: 0.00001804
Iteration 244/1000 | Loss: 0.00001804
Iteration 245/1000 | Loss: 0.00001804
Iteration 246/1000 | Loss: 0.00001804
Iteration 247/1000 | Loss: 0.00001803
Iteration 248/1000 | Loss: 0.00001803
Iteration 249/1000 | Loss: 0.00001803
Iteration 250/1000 | Loss: 0.00001803
Iteration 251/1000 | Loss: 0.00001803
Iteration 252/1000 | Loss: 0.00001803
Iteration 253/1000 | Loss: 0.00001803
Iteration 254/1000 | Loss: 0.00001802
Iteration 255/1000 | Loss: 0.00001802
Iteration 256/1000 | Loss: 0.00001802
Iteration 257/1000 | Loss: 0.00001802
Iteration 258/1000 | Loss: 0.00001802
Iteration 259/1000 | Loss: 0.00001802
Iteration 260/1000 | Loss: 0.00001802
Iteration 261/1000 | Loss: 0.00001802
Iteration 262/1000 | Loss: 0.00001802
Iteration 263/1000 | Loss: 0.00001802
Iteration 264/1000 | Loss: 0.00001802
Iteration 265/1000 | Loss: 0.00001802
Iteration 266/1000 | Loss: 0.00001802
Iteration 267/1000 | Loss: 0.00001802
Iteration 268/1000 | Loss: 0.00001802
Iteration 269/1000 | Loss: 0.00001802
Iteration 270/1000 | Loss: 0.00001802
Iteration 271/1000 | Loss: 0.00001802
Iteration 272/1000 | Loss: 0.00001801
Iteration 273/1000 | Loss: 0.00001801
Iteration 274/1000 | Loss: 0.00001801
Iteration 275/1000 | Loss: 0.00001801
Iteration 276/1000 | Loss: 0.00001801
Iteration 277/1000 | Loss: 0.00001801
Iteration 278/1000 | Loss: 0.00001801
Iteration 279/1000 | Loss: 0.00001801
Iteration 280/1000 | Loss: 0.00001801
Iteration 281/1000 | Loss: 0.00001801
Iteration 282/1000 | Loss: 0.00001801
Iteration 283/1000 | Loss: 0.00001801
Iteration 284/1000 | Loss: 0.00001801
Iteration 285/1000 | Loss: 0.00001801
Iteration 286/1000 | Loss: 0.00001801
Iteration 287/1000 | Loss: 0.00001801
Iteration 288/1000 | Loss: 0.00001801
Iteration 289/1000 | Loss: 0.00001801
Iteration 290/1000 | Loss: 0.00001801
Iteration 291/1000 | Loss: 0.00001801
Iteration 292/1000 | Loss: 0.00001801
Iteration 293/1000 | Loss: 0.00001801
Iteration 294/1000 | Loss: 0.00001801
Iteration 295/1000 | Loss: 0.00001801
Iteration 296/1000 | Loss: 0.00001801
Iteration 297/1000 | Loss: 0.00001801
Iteration 298/1000 | Loss: 0.00001801
Iteration 299/1000 | Loss: 0.00001801
Iteration 300/1000 | Loss: 0.00001801
Iteration 301/1000 | Loss: 0.00001801
Iteration 302/1000 | Loss: 0.00001801
Iteration 303/1000 | Loss: 0.00001801
Iteration 304/1000 | Loss: 0.00001801
Iteration 305/1000 | Loss: 0.00001801
Iteration 306/1000 | Loss: 0.00001801
Iteration 307/1000 | Loss: 0.00001801
Iteration 308/1000 | Loss: 0.00001801
Iteration 309/1000 | Loss: 0.00001801
Iteration 310/1000 | Loss: 0.00001801
Iteration 311/1000 | Loss: 0.00001801
Iteration 312/1000 | Loss: 0.00001801
Iteration 313/1000 | Loss: 0.00001801
Iteration 314/1000 | Loss: 0.00001801
Iteration 315/1000 | Loss: 0.00001801
Iteration 316/1000 | Loss: 0.00001801
Iteration 317/1000 | Loss: 0.00001801
Iteration 318/1000 | Loss: 0.00001801
Iteration 319/1000 | Loss: 0.00001801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 319. Stopping optimization.
Last 5 losses: [1.8006765458267182e-05, 1.8006765458267182e-05, 1.8006765458267182e-05, 1.8006765458267182e-05, 1.8006765458267182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8006765458267182e-05

Optimization complete. Final v2v error: 3.334993362426758 mm

Highest mean error: 8.746319770812988 mm for frame 51

Lowest mean error: 2.8869640827178955 mm for frame 111

Saving results

Total time: 286.4342255592346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533634
Iteration 2/25 | Loss: 0.00122776
Iteration 3/25 | Loss: 0.00101120
Iteration 4/25 | Loss: 0.00098415
Iteration 5/25 | Loss: 0.00098025
Iteration 6/25 | Loss: 0.00097966
Iteration 7/25 | Loss: 0.00097966
Iteration 8/25 | Loss: 0.00097966
Iteration 9/25 | Loss: 0.00097966
Iteration 10/25 | Loss: 0.00097966
Iteration 11/25 | Loss: 0.00097966
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009796625236049294, 0.0009796625236049294, 0.0009796625236049294, 0.0009796625236049294, 0.0009796625236049294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009796625236049294

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31311584
Iteration 2/25 | Loss: 0.00041941
Iteration 3/25 | Loss: 0.00041940
Iteration 4/25 | Loss: 0.00041939
Iteration 5/25 | Loss: 0.00041939
Iteration 6/25 | Loss: 0.00041939
Iteration 7/25 | Loss: 0.00041939
Iteration 8/25 | Loss: 0.00041939
Iteration 9/25 | Loss: 0.00041939
Iteration 10/25 | Loss: 0.00041939
Iteration 11/25 | Loss: 0.00041939
Iteration 12/25 | Loss: 0.00041939
Iteration 13/25 | Loss: 0.00041939
Iteration 14/25 | Loss: 0.00041939
Iteration 15/25 | Loss: 0.00041939
Iteration 16/25 | Loss: 0.00041939
Iteration 17/25 | Loss: 0.00041939
Iteration 18/25 | Loss: 0.00041939
Iteration 19/25 | Loss: 0.00041939
Iteration 20/25 | Loss: 0.00041939
Iteration 21/25 | Loss: 0.00041939
Iteration 22/25 | Loss: 0.00041939
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.00041939233778975904, 0.00041939233778975904, 0.00041939233778975904, 0.00041939233778975904, 0.00041939233778975904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00041939233778975904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00041939
Iteration 2/1000 | Loss: 0.00003454
Iteration 3/1000 | Loss: 0.00002266
Iteration 4/1000 | Loss: 0.00001747
Iteration 5/1000 | Loss: 0.00001616
Iteration 6/1000 | Loss: 0.00001495
Iteration 7/1000 | Loss: 0.00001435
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001331
Iteration 10/1000 | Loss: 0.00001314
Iteration 11/1000 | Loss: 0.00001307
Iteration 12/1000 | Loss: 0.00001290
Iteration 13/1000 | Loss: 0.00001289
Iteration 14/1000 | Loss: 0.00001288
Iteration 15/1000 | Loss: 0.00001275
Iteration 16/1000 | Loss: 0.00001272
Iteration 17/1000 | Loss: 0.00001265
Iteration 18/1000 | Loss: 0.00001260
Iteration 19/1000 | Loss: 0.00001260
Iteration 20/1000 | Loss: 0.00001260
Iteration 21/1000 | Loss: 0.00001260
Iteration 22/1000 | Loss: 0.00001260
Iteration 23/1000 | Loss: 0.00001260
Iteration 24/1000 | Loss: 0.00001260
Iteration 25/1000 | Loss: 0.00001260
Iteration 26/1000 | Loss: 0.00001260
Iteration 27/1000 | Loss: 0.00001259
Iteration 28/1000 | Loss: 0.00001259
Iteration 29/1000 | Loss: 0.00001259
Iteration 30/1000 | Loss: 0.00001259
Iteration 31/1000 | Loss: 0.00001259
Iteration 32/1000 | Loss: 0.00001259
Iteration 33/1000 | Loss: 0.00001259
Iteration 34/1000 | Loss: 0.00001258
Iteration 35/1000 | Loss: 0.00001257
Iteration 36/1000 | Loss: 0.00001257
Iteration 37/1000 | Loss: 0.00001257
Iteration 38/1000 | Loss: 0.00001257
Iteration 39/1000 | Loss: 0.00001257
Iteration 40/1000 | Loss: 0.00001256
Iteration 41/1000 | Loss: 0.00001256
Iteration 42/1000 | Loss: 0.00001256
Iteration 43/1000 | Loss: 0.00001256
Iteration 44/1000 | Loss: 0.00001256
Iteration 45/1000 | Loss: 0.00001256
Iteration 46/1000 | Loss: 0.00001256
Iteration 47/1000 | Loss: 0.00001256
Iteration 48/1000 | Loss: 0.00001255
Iteration 49/1000 | Loss: 0.00001255
Iteration 50/1000 | Loss: 0.00001255
Iteration 51/1000 | Loss: 0.00001255
Iteration 52/1000 | Loss: 0.00001255
Iteration 53/1000 | Loss: 0.00001255
Iteration 54/1000 | Loss: 0.00001255
Iteration 55/1000 | Loss: 0.00001255
Iteration 56/1000 | Loss: 0.00001255
Iteration 57/1000 | Loss: 0.00001254
Iteration 58/1000 | Loss: 0.00001254
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001254
Iteration 61/1000 | Loss: 0.00001254
Iteration 62/1000 | Loss: 0.00001254
Iteration 63/1000 | Loss: 0.00001254
Iteration 64/1000 | Loss: 0.00001254
Iteration 65/1000 | Loss: 0.00001254
Iteration 66/1000 | Loss: 0.00001253
Iteration 67/1000 | Loss: 0.00001253
Iteration 68/1000 | Loss: 0.00001253
Iteration 69/1000 | Loss: 0.00001253
Iteration 70/1000 | Loss: 0.00001253
Iteration 71/1000 | Loss: 0.00001253
Iteration 72/1000 | Loss: 0.00001253
Iteration 73/1000 | Loss: 0.00001253
Iteration 74/1000 | Loss: 0.00001253
Iteration 75/1000 | Loss: 0.00001253
Iteration 76/1000 | Loss: 0.00001253
Iteration 77/1000 | Loss: 0.00001253
Iteration 78/1000 | Loss: 0.00001253
Iteration 79/1000 | Loss: 0.00001253
Iteration 80/1000 | Loss: 0.00001253
Iteration 81/1000 | Loss: 0.00001252
Iteration 82/1000 | Loss: 0.00001252
Iteration 83/1000 | Loss: 0.00001252
Iteration 84/1000 | Loss: 0.00001252
Iteration 85/1000 | Loss: 0.00001252
Iteration 86/1000 | Loss: 0.00001252
Iteration 87/1000 | Loss: 0.00001252
Iteration 88/1000 | Loss: 0.00001252
Iteration 89/1000 | Loss: 0.00001252
Iteration 90/1000 | Loss: 0.00001252
Iteration 91/1000 | Loss: 0.00001252
Iteration 92/1000 | Loss: 0.00001252
Iteration 93/1000 | Loss: 0.00001251
Iteration 94/1000 | Loss: 0.00001251
Iteration 95/1000 | Loss: 0.00001251
Iteration 96/1000 | Loss: 0.00001251
Iteration 97/1000 | Loss: 0.00001251
Iteration 98/1000 | Loss: 0.00001251
Iteration 99/1000 | Loss: 0.00001251
Iteration 100/1000 | Loss: 0.00001250
Iteration 101/1000 | Loss: 0.00001250
Iteration 102/1000 | Loss: 0.00001250
Iteration 103/1000 | Loss: 0.00001250
Iteration 104/1000 | Loss: 0.00001250
Iteration 105/1000 | Loss: 0.00001250
Iteration 106/1000 | Loss: 0.00001250
Iteration 107/1000 | Loss: 0.00001250
Iteration 108/1000 | Loss: 0.00001250
Iteration 109/1000 | Loss: 0.00001250
Iteration 110/1000 | Loss: 0.00001250
Iteration 111/1000 | Loss: 0.00001250
Iteration 112/1000 | Loss: 0.00001250
Iteration 113/1000 | Loss: 0.00001249
Iteration 114/1000 | Loss: 0.00001249
Iteration 115/1000 | Loss: 0.00001249
Iteration 116/1000 | Loss: 0.00001249
Iteration 117/1000 | Loss: 0.00001249
Iteration 118/1000 | Loss: 0.00001249
Iteration 119/1000 | Loss: 0.00001249
Iteration 120/1000 | Loss: 0.00001248
Iteration 121/1000 | Loss: 0.00001248
Iteration 122/1000 | Loss: 0.00001248
Iteration 123/1000 | Loss: 0.00001248
Iteration 124/1000 | Loss: 0.00001248
Iteration 125/1000 | Loss: 0.00001248
Iteration 126/1000 | Loss: 0.00001248
Iteration 127/1000 | Loss: 0.00001248
Iteration 128/1000 | Loss: 0.00001248
Iteration 129/1000 | Loss: 0.00001248
Iteration 130/1000 | Loss: 0.00001248
Iteration 131/1000 | Loss: 0.00001248
Iteration 132/1000 | Loss: 0.00001248
Iteration 133/1000 | Loss: 0.00001247
Iteration 134/1000 | Loss: 0.00001247
Iteration 135/1000 | Loss: 0.00001247
Iteration 136/1000 | Loss: 0.00001246
Iteration 137/1000 | Loss: 0.00001246
Iteration 138/1000 | Loss: 0.00001246
Iteration 139/1000 | Loss: 0.00001246
Iteration 140/1000 | Loss: 0.00001246
Iteration 141/1000 | Loss: 0.00001246
Iteration 142/1000 | Loss: 0.00001246
Iteration 143/1000 | Loss: 0.00001246
Iteration 144/1000 | Loss: 0.00001246
Iteration 145/1000 | Loss: 0.00001246
Iteration 146/1000 | Loss: 0.00001246
Iteration 147/1000 | Loss: 0.00001246
Iteration 148/1000 | Loss: 0.00001245
Iteration 149/1000 | Loss: 0.00001245
Iteration 150/1000 | Loss: 0.00001245
Iteration 151/1000 | Loss: 0.00001245
Iteration 152/1000 | Loss: 0.00001244
Iteration 153/1000 | Loss: 0.00001244
Iteration 154/1000 | Loss: 0.00001244
Iteration 155/1000 | Loss: 0.00001244
Iteration 156/1000 | Loss: 0.00001244
Iteration 157/1000 | Loss: 0.00001244
Iteration 158/1000 | Loss: 0.00001244
Iteration 159/1000 | Loss: 0.00001244
Iteration 160/1000 | Loss: 0.00001244
Iteration 161/1000 | Loss: 0.00001244
Iteration 162/1000 | Loss: 0.00001244
Iteration 163/1000 | Loss: 0.00001244
Iteration 164/1000 | Loss: 0.00001244
Iteration 165/1000 | Loss: 0.00001244
Iteration 166/1000 | Loss: 0.00001244
Iteration 167/1000 | Loss: 0.00001244
Iteration 168/1000 | Loss: 0.00001244
Iteration 169/1000 | Loss: 0.00001244
Iteration 170/1000 | Loss: 0.00001244
Iteration 171/1000 | Loss: 0.00001244
Iteration 172/1000 | Loss: 0.00001244
Iteration 173/1000 | Loss: 0.00001244
Iteration 174/1000 | Loss: 0.00001244
Iteration 175/1000 | Loss: 0.00001244
Iteration 176/1000 | Loss: 0.00001244
Iteration 177/1000 | Loss: 0.00001244
Iteration 178/1000 | Loss: 0.00001244
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.2438804333214648e-05, 1.2438804333214648e-05, 1.2438804333214648e-05, 1.2438804333214648e-05, 1.2438804333214648e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2438804333214648e-05

Optimization complete. Final v2v error: 3.0020949840545654 mm

Highest mean error: 3.193760871887207 mm for frame 0

Lowest mean error: 2.8553054332733154 mm for frame 136

Saving results

Total time: 36.0245475769043
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103970
Iteration 2/25 | Loss: 0.00312877
Iteration 3/25 | Loss: 0.00196774
Iteration 4/25 | Loss: 0.00179548
Iteration 5/25 | Loss: 0.00142454
Iteration 6/25 | Loss: 0.00131968
Iteration 7/25 | Loss: 0.00123259
Iteration 8/25 | Loss: 0.00119729
Iteration 9/25 | Loss: 0.00118544
Iteration 10/25 | Loss: 0.00115894
Iteration 11/25 | Loss: 0.00115184
Iteration 12/25 | Loss: 0.00115071
Iteration 13/25 | Loss: 0.00112770
Iteration 14/25 | Loss: 0.00113068
Iteration 15/25 | Loss: 0.00110591
Iteration 16/25 | Loss: 0.00110572
Iteration 17/25 | Loss: 0.00110100
Iteration 18/25 | Loss: 0.00110378
Iteration 19/25 | Loss: 0.00108682
Iteration 20/25 | Loss: 0.00108007
Iteration 21/25 | Loss: 0.00108526
Iteration 22/25 | Loss: 0.00107696
Iteration 23/25 | Loss: 0.00107486
Iteration 24/25 | Loss: 0.00107458
Iteration 25/25 | Loss: 0.00107446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.59005189
Iteration 2/25 | Loss: 0.00057139
Iteration 3/25 | Loss: 0.00057138
Iteration 4/25 | Loss: 0.00057138
Iteration 5/25 | Loss: 0.00057138
Iteration 6/25 | Loss: 0.00057138
Iteration 7/25 | Loss: 0.00057138
Iteration 8/25 | Loss: 0.00057138
Iteration 9/25 | Loss: 0.00057138
Iteration 10/25 | Loss: 0.00057138
Iteration 11/25 | Loss: 0.00057138
Iteration 12/25 | Loss: 0.00057138
Iteration 13/25 | Loss: 0.00057138
Iteration 14/25 | Loss: 0.00057138
Iteration 15/25 | Loss: 0.00057138
Iteration 16/25 | Loss: 0.00057138
Iteration 17/25 | Loss: 0.00057138
Iteration 18/25 | Loss: 0.00057138
Iteration 19/25 | Loss: 0.00057138
Iteration 20/25 | Loss: 0.00057138
Iteration 21/25 | Loss: 0.00057138
Iteration 22/25 | Loss: 0.00057138
Iteration 23/25 | Loss: 0.00057138
Iteration 24/25 | Loss: 0.00057138
Iteration 25/25 | Loss: 0.00057138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057138
Iteration 2/1000 | Loss: 0.00007659
Iteration 3/1000 | Loss: 0.00005782
Iteration 4/1000 | Loss: 0.00005206
Iteration 5/1000 | Loss: 0.00004890
Iteration 6/1000 | Loss: 0.00318837
Iteration 7/1000 | Loss: 0.00421390
Iteration 8/1000 | Loss: 0.00037004
Iteration 9/1000 | Loss: 0.00008786
Iteration 10/1000 | Loss: 0.00005289
Iteration 11/1000 | Loss: 0.00004743
Iteration 12/1000 | Loss: 0.00004572
Iteration 13/1000 | Loss: 0.00004497
Iteration 14/1000 | Loss: 0.00004455
Iteration 15/1000 | Loss: 0.00004416
Iteration 16/1000 | Loss: 0.00027510
Iteration 17/1000 | Loss: 0.00005169
Iteration 18/1000 | Loss: 0.00004610
Iteration 19/1000 | Loss: 0.00004428
Iteration 20/1000 | Loss: 0.00004205
Iteration 21/1000 | Loss: 0.00004110
Iteration 22/1000 | Loss: 0.00004063
Iteration 23/1000 | Loss: 0.00004029
Iteration 24/1000 | Loss: 0.00003993
Iteration 25/1000 | Loss: 0.00003976
Iteration 26/1000 | Loss: 0.00003961
Iteration 27/1000 | Loss: 0.00003953
Iteration 28/1000 | Loss: 0.00003939
Iteration 29/1000 | Loss: 0.00003936
Iteration 30/1000 | Loss: 0.00003932
Iteration 31/1000 | Loss: 0.00003918
Iteration 32/1000 | Loss: 0.00003912
Iteration 33/1000 | Loss: 0.00003909
Iteration 34/1000 | Loss: 0.00003909
Iteration 35/1000 | Loss: 0.00003908
Iteration 36/1000 | Loss: 0.00003903
Iteration 37/1000 | Loss: 0.00003894
Iteration 38/1000 | Loss: 0.00003877
Iteration 39/1000 | Loss: 0.00003863
Iteration 40/1000 | Loss: 0.00003859
Iteration 41/1000 | Loss: 0.00003859
Iteration 42/1000 | Loss: 0.00003854
Iteration 43/1000 | Loss: 0.00003854
Iteration 44/1000 | Loss: 0.00003854
Iteration 45/1000 | Loss: 0.00003854
Iteration 46/1000 | Loss: 0.00003854
Iteration 47/1000 | Loss: 0.00003853
Iteration 48/1000 | Loss: 0.00003852
Iteration 49/1000 | Loss: 0.00003852
Iteration 50/1000 | Loss: 0.00003852
Iteration 51/1000 | Loss: 0.00003851
Iteration 52/1000 | Loss: 0.00003851
Iteration 53/1000 | Loss: 0.00003851
Iteration 54/1000 | Loss: 0.00003851
Iteration 55/1000 | Loss: 0.00003851
Iteration 56/1000 | Loss: 0.00003851
Iteration 57/1000 | Loss: 0.00003851
Iteration 58/1000 | Loss: 0.00003851
Iteration 59/1000 | Loss: 0.00003851
Iteration 60/1000 | Loss: 0.00003851
Iteration 61/1000 | Loss: 0.00003851
Iteration 62/1000 | Loss: 0.00003851
Iteration 63/1000 | Loss: 0.00003851
Iteration 64/1000 | Loss: 0.00003851
Iteration 65/1000 | Loss: 0.00003849
Iteration 66/1000 | Loss: 0.00003849
Iteration 67/1000 | Loss: 0.00003849
Iteration 68/1000 | Loss: 0.00003845
Iteration 69/1000 | Loss: 0.00003845
Iteration 70/1000 | Loss: 0.00003845
Iteration 71/1000 | Loss: 0.00003845
Iteration 72/1000 | Loss: 0.00003845
Iteration 73/1000 | Loss: 0.00003845
Iteration 74/1000 | Loss: 0.00003845
Iteration 75/1000 | Loss: 0.00003844
Iteration 76/1000 | Loss: 0.00003844
Iteration 77/1000 | Loss: 0.00003844
Iteration 78/1000 | Loss: 0.00003844
Iteration 79/1000 | Loss: 0.00003843
Iteration 80/1000 | Loss: 0.00003843
Iteration 81/1000 | Loss: 0.00003843
Iteration 82/1000 | Loss: 0.00003843
Iteration 83/1000 | Loss: 0.00003843
Iteration 84/1000 | Loss: 0.00003843
Iteration 85/1000 | Loss: 0.00003843
Iteration 86/1000 | Loss: 0.00003843
Iteration 87/1000 | Loss: 0.00003843
Iteration 88/1000 | Loss: 0.00003843
Iteration 89/1000 | Loss: 0.00003843
Iteration 90/1000 | Loss: 0.00003843
Iteration 91/1000 | Loss: 0.00003843
Iteration 92/1000 | Loss: 0.00003842
Iteration 93/1000 | Loss: 0.00003842
Iteration 94/1000 | Loss: 0.00003842
Iteration 95/1000 | Loss: 0.00003842
Iteration 96/1000 | Loss: 0.00003841
Iteration 97/1000 | Loss: 0.00003841
Iteration 98/1000 | Loss: 0.00003841
Iteration 99/1000 | Loss: 0.00003841
Iteration 100/1000 | Loss: 0.00003841
Iteration 101/1000 | Loss: 0.00003841
Iteration 102/1000 | Loss: 0.00003841
Iteration 103/1000 | Loss: 0.00003841
Iteration 104/1000 | Loss: 0.00003841
Iteration 105/1000 | Loss: 0.00003841
Iteration 106/1000 | Loss: 0.00003841
Iteration 107/1000 | Loss: 0.00003840
Iteration 108/1000 | Loss: 0.00003840
Iteration 109/1000 | Loss: 0.00003840
Iteration 110/1000 | Loss: 0.00003840
Iteration 111/1000 | Loss: 0.00003840
Iteration 112/1000 | Loss: 0.00003839
Iteration 113/1000 | Loss: 0.00003839
Iteration 114/1000 | Loss: 0.00003839
Iteration 115/1000 | Loss: 0.00003839
Iteration 116/1000 | Loss: 0.00003838
Iteration 117/1000 | Loss: 0.00003838
Iteration 118/1000 | Loss: 0.00003838
Iteration 119/1000 | Loss: 0.00003838
Iteration 120/1000 | Loss: 0.00003838
Iteration 121/1000 | Loss: 0.00003837
Iteration 122/1000 | Loss: 0.00003837
Iteration 123/1000 | Loss: 0.00003837
Iteration 124/1000 | Loss: 0.00003837
Iteration 125/1000 | Loss: 0.00003837
Iteration 126/1000 | Loss: 0.00003837
Iteration 127/1000 | Loss: 0.00003836
Iteration 128/1000 | Loss: 0.00003836
Iteration 129/1000 | Loss: 0.00003836
Iteration 130/1000 | Loss: 0.00003836
Iteration 131/1000 | Loss: 0.00003836
Iteration 132/1000 | Loss: 0.00003836
Iteration 133/1000 | Loss: 0.00003836
Iteration 134/1000 | Loss: 0.00003836
Iteration 135/1000 | Loss: 0.00003836
Iteration 136/1000 | Loss: 0.00003835
Iteration 137/1000 | Loss: 0.00003835
Iteration 138/1000 | Loss: 0.00003835
Iteration 139/1000 | Loss: 0.00003835
Iteration 140/1000 | Loss: 0.00003835
Iteration 141/1000 | Loss: 0.00003835
Iteration 142/1000 | Loss: 0.00003835
Iteration 143/1000 | Loss: 0.00003835
Iteration 144/1000 | Loss: 0.00003835
Iteration 145/1000 | Loss: 0.00003835
Iteration 146/1000 | Loss: 0.00003835
Iteration 147/1000 | Loss: 0.00003835
Iteration 148/1000 | Loss: 0.00003835
Iteration 149/1000 | Loss: 0.00003835
Iteration 150/1000 | Loss: 0.00003835
Iteration 151/1000 | Loss: 0.00003834
Iteration 152/1000 | Loss: 0.00003834
Iteration 153/1000 | Loss: 0.00003834
Iteration 154/1000 | Loss: 0.00003834
Iteration 155/1000 | Loss: 0.00003834
Iteration 156/1000 | Loss: 0.00003833
Iteration 157/1000 | Loss: 0.00003833
Iteration 158/1000 | Loss: 0.00003833
Iteration 159/1000 | Loss: 0.00003833
Iteration 160/1000 | Loss: 0.00003833
Iteration 161/1000 | Loss: 0.00003833
Iteration 162/1000 | Loss: 0.00003833
Iteration 163/1000 | Loss: 0.00003833
Iteration 164/1000 | Loss: 0.00003833
Iteration 165/1000 | Loss: 0.00003833
Iteration 166/1000 | Loss: 0.00003832
Iteration 167/1000 | Loss: 0.00003832
Iteration 168/1000 | Loss: 0.00003832
Iteration 169/1000 | Loss: 0.00003832
Iteration 170/1000 | Loss: 0.00003832
Iteration 171/1000 | Loss: 0.00003832
Iteration 172/1000 | Loss: 0.00003832
Iteration 173/1000 | Loss: 0.00003832
Iteration 174/1000 | Loss: 0.00003832
Iteration 175/1000 | Loss: 0.00003832
Iteration 176/1000 | Loss: 0.00003832
Iteration 177/1000 | Loss: 0.00003832
Iteration 178/1000 | Loss: 0.00003832
Iteration 179/1000 | Loss: 0.00003832
Iteration 180/1000 | Loss: 0.00003832
Iteration 181/1000 | Loss: 0.00003831
Iteration 182/1000 | Loss: 0.00003831
Iteration 183/1000 | Loss: 0.00003831
Iteration 184/1000 | Loss: 0.00003831
Iteration 185/1000 | Loss: 0.00003831
Iteration 186/1000 | Loss: 0.00003831
Iteration 187/1000 | Loss: 0.00003831
Iteration 188/1000 | Loss: 0.00003831
Iteration 189/1000 | Loss: 0.00003830
Iteration 190/1000 | Loss: 0.00003830
Iteration 191/1000 | Loss: 0.00003830
Iteration 192/1000 | Loss: 0.00003830
Iteration 193/1000 | Loss: 0.00003830
Iteration 194/1000 | Loss: 0.00003830
Iteration 195/1000 | Loss: 0.00003830
Iteration 196/1000 | Loss: 0.00003830
Iteration 197/1000 | Loss: 0.00003830
Iteration 198/1000 | Loss: 0.00003830
Iteration 199/1000 | Loss: 0.00003830
Iteration 200/1000 | Loss: 0.00003830
Iteration 201/1000 | Loss: 0.00003830
Iteration 202/1000 | Loss: 0.00003830
Iteration 203/1000 | Loss: 0.00003830
Iteration 204/1000 | Loss: 0.00003829
Iteration 205/1000 | Loss: 0.00003829
Iteration 206/1000 | Loss: 0.00003829
Iteration 207/1000 | Loss: 0.00003829
Iteration 208/1000 | Loss: 0.00003829
Iteration 209/1000 | Loss: 0.00003829
Iteration 210/1000 | Loss: 0.00003829
Iteration 211/1000 | Loss: 0.00003829
Iteration 212/1000 | Loss: 0.00003829
Iteration 213/1000 | Loss: 0.00003829
Iteration 214/1000 | Loss: 0.00003829
Iteration 215/1000 | Loss: 0.00003829
Iteration 216/1000 | Loss: 0.00003829
Iteration 217/1000 | Loss: 0.00003829
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [3.8291345845209435e-05, 3.8291345845209435e-05, 3.8291345845209435e-05, 3.8291345845209435e-05, 3.8291345845209435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8291345845209435e-05

Optimization complete. Final v2v error: 4.51733922958374 mm

Highest mean error: 18.837602615356445 mm for frame 30

Lowest mean error: 3.983402729034424 mm for frame 93

Saving results

Total time: 101.60327529907227
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00861393
Iteration 2/25 | Loss: 0.00126009
Iteration 3/25 | Loss: 0.00104384
Iteration 4/25 | Loss: 0.00102536
Iteration 5/25 | Loss: 0.00102070
Iteration 6/25 | Loss: 0.00101443
Iteration 7/25 | Loss: 0.00101317
Iteration 8/25 | Loss: 0.00101252
Iteration 9/25 | Loss: 0.00101229
Iteration 10/25 | Loss: 0.00101219
Iteration 11/25 | Loss: 0.00101954
Iteration 12/25 | Loss: 0.00101119
Iteration 13/25 | Loss: 0.00101024
Iteration 14/25 | Loss: 0.00100996
Iteration 15/25 | Loss: 0.00100994
Iteration 16/25 | Loss: 0.00100993
Iteration 17/25 | Loss: 0.00100993
Iteration 18/25 | Loss: 0.00100993
Iteration 19/25 | Loss: 0.00100993
Iteration 20/25 | Loss: 0.00100993
Iteration 21/25 | Loss: 0.00100993
Iteration 22/25 | Loss: 0.00100993
Iteration 23/25 | Loss: 0.00100993
Iteration 24/25 | Loss: 0.00100992
Iteration 25/25 | Loss: 0.00100992

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.31618047
Iteration 2/25 | Loss: 0.00051148
Iteration 3/25 | Loss: 0.00051145
Iteration 4/25 | Loss: 0.00051145
Iteration 5/25 | Loss: 0.00051145
Iteration 6/25 | Loss: 0.00051144
Iteration 7/25 | Loss: 0.00051144
Iteration 8/25 | Loss: 0.00051144
Iteration 9/25 | Loss: 0.00051144
Iteration 10/25 | Loss: 0.00051144
Iteration 11/25 | Loss: 0.00051144
Iteration 12/25 | Loss: 0.00051144
Iteration 13/25 | Loss: 0.00051144
Iteration 14/25 | Loss: 0.00051144
Iteration 15/25 | Loss: 0.00051144
Iteration 16/25 | Loss: 0.00051144
Iteration 17/25 | Loss: 0.00051144
Iteration 18/25 | Loss: 0.00051144
Iteration 19/25 | Loss: 0.00051144
Iteration 20/25 | Loss: 0.00051144
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0005114438245072961, 0.0005114438245072961, 0.0005114438245072961, 0.0005114438245072961, 0.0005114438245072961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005114438245072961

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051144
Iteration 2/1000 | Loss: 0.00005994
Iteration 3/1000 | Loss: 0.00002642
Iteration 4/1000 | Loss: 0.00002155
Iteration 5/1000 | Loss: 0.00001987
Iteration 6/1000 | Loss: 0.00001895
Iteration 7/1000 | Loss: 0.00001857
Iteration 8/1000 | Loss: 0.00001824
Iteration 9/1000 | Loss: 0.00001801
Iteration 10/1000 | Loss: 0.00001786
Iteration 11/1000 | Loss: 0.00001786
Iteration 12/1000 | Loss: 0.00001785
Iteration 13/1000 | Loss: 0.00001780
Iteration 14/1000 | Loss: 0.00001766
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001752
Iteration 17/1000 | Loss: 0.00001751
Iteration 18/1000 | Loss: 0.00001750
Iteration 19/1000 | Loss: 0.00001748
Iteration 20/1000 | Loss: 0.00001747
Iteration 21/1000 | Loss: 0.00001745
Iteration 22/1000 | Loss: 0.00001745
Iteration 23/1000 | Loss: 0.00001745
Iteration 24/1000 | Loss: 0.00001744
Iteration 25/1000 | Loss: 0.00001744
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001740
Iteration 28/1000 | Loss: 0.00001735
Iteration 29/1000 | Loss: 0.00001735
Iteration 30/1000 | Loss: 0.00001734
Iteration 31/1000 | Loss: 0.00001734
Iteration 32/1000 | Loss: 0.00001734
Iteration 33/1000 | Loss: 0.00001733
Iteration 34/1000 | Loss: 0.00001733
Iteration 35/1000 | Loss: 0.00001733
Iteration 36/1000 | Loss: 0.00001732
Iteration 37/1000 | Loss: 0.00001732
Iteration 38/1000 | Loss: 0.00001732
Iteration 39/1000 | Loss: 0.00001732
Iteration 40/1000 | Loss: 0.00001731
Iteration 41/1000 | Loss: 0.00001731
Iteration 42/1000 | Loss: 0.00001731
Iteration 43/1000 | Loss: 0.00001731
Iteration 44/1000 | Loss: 0.00001731
Iteration 45/1000 | Loss: 0.00001730
Iteration 46/1000 | Loss: 0.00001730
Iteration 47/1000 | Loss: 0.00001730
Iteration 48/1000 | Loss: 0.00001729
Iteration 49/1000 | Loss: 0.00001729
Iteration 50/1000 | Loss: 0.00001729
Iteration 51/1000 | Loss: 0.00001729
Iteration 52/1000 | Loss: 0.00001729
Iteration 53/1000 | Loss: 0.00001729
Iteration 54/1000 | Loss: 0.00001729
Iteration 55/1000 | Loss: 0.00001729
Iteration 56/1000 | Loss: 0.00001729
Iteration 57/1000 | Loss: 0.00001728
Iteration 58/1000 | Loss: 0.00001728
Iteration 59/1000 | Loss: 0.00001728
Iteration 60/1000 | Loss: 0.00001728
Iteration 61/1000 | Loss: 0.00001728
Iteration 62/1000 | Loss: 0.00001727
Iteration 63/1000 | Loss: 0.00001727
Iteration 64/1000 | Loss: 0.00001727
Iteration 65/1000 | Loss: 0.00001727
Iteration 66/1000 | Loss: 0.00001727
Iteration 67/1000 | Loss: 0.00001726
Iteration 68/1000 | Loss: 0.00001726
Iteration 69/1000 | Loss: 0.00001726
Iteration 70/1000 | Loss: 0.00001726
Iteration 71/1000 | Loss: 0.00001726
Iteration 72/1000 | Loss: 0.00001726
Iteration 73/1000 | Loss: 0.00001726
Iteration 74/1000 | Loss: 0.00001726
Iteration 75/1000 | Loss: 0.00001726
Iteration 76/1000 | Loss: 0.00001726
Iteration 77/1000 | Loss: 0.00001726
Iteration 78/1000 | Loss: 0.00001726
Iteration 79/1000 | Loss: 0.00001726
Iteration 80/1000 | Loss: 0.00001726
Iteration 81/1000 | Loss: 0.00001726
Iteration 82/1000 | Loss: 0.00001726
Iteration 83/1000 | Loss: 0.00001726
Iteration 84/1000 | Loss: 0.00001726
Iteration 85/1000 | Loss: 0.00001726
Iteration 86/1000 | Loss: 0.00001726
Iteration 87/1000 | Loss: 0.00001726
Iteration 88/1000 | Loss: 0.00001726
Iteration 89/1000 | Loss: 0.00001726
Iteration 90/1000 | Loss: 0.00001726
Iteration 91/1000 | Loss: 0.00001726
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001726
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.7259591913898475e-05, 1.7259591913898475e-05, 1.7259591913898475e-05, 1.7259591913898475e-05, 1.7259591913898475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7259591913898475e-05

Optimization complete. Final v2v error: 3.332638740539551 mm

Highest mean error: 4.3730387687683105 mm for frame 96

Lowest mean error: 2.7541768550872803 mm for frame 74

Saving results

Total time: 48.415879249572754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00562693
Iteration 2/25 | Loss: 0.00172054
Iteration 3/25 | Loss: 0.00115441
Iteration 4/25 | Loss: 0.00112114
Iteration 5/25 | Loss: 0.00111318
Iteration 6/25 | Loss: 0.00111085
Iteration 7/25 | Loss: 0.00110971
Iteration 8/25 | Loss: 0.00111032
Iteration 9/25 | Loss: 0.00111402
Iteration 10/25 | Loss: 0.00111805
Iteration 11/25 | Loss: 0.00110787
Iteration 12/25 | Loss: 0.00109531
Iteration 13/25 | Loss: 0.00108437
Iteration 14/25 | Loss: 0.00108127
Iteration 15/25 | Loss: 0.00108067
Iteration 16/25 | Loss: 0.00108269
Iteration 17/25 | Loss: 0.00107989
Iteration 18/25 | Loss: 0.00107898
Iteration 19/25 | Loss: 0.00107886
Iteration 20/25 | Loss: 0.00107884
Iteration 21/25 | Loss: 0.00107884
Iteration 22/25 | Loss: 0.00107884
Iteration 23/25 | Loss: 0.00107884
Iteration 24/25 | Loss: 0.00107884
Iteration 25/25 | Loss: 0.00107884

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05698359
Iteration 2/25 | Loss: 0.00064274
Iteration 3/25 | Loss: 0.00064274
Iteration 4/25 | Loss: 0.00064274
Iteration 5/25 | Loss: 0.00064274
Iteration 6/25 | Loss: 0.00064274
Iteration 7/25 | Loss: 0.00064273
Iteration 8/25 | Loss: 0.00064273
Iteration 9/25 | Loss: 0.00064273
Iteration 10/25 | Loss: 0.00064273
Iteration 11/25 | Loss: 0.00064273
Iteration 12/25 | Loss: 0.00064273
Iteration 13/25 | Loss: 0.00064273
Iteration 14/25 | Loss: 0.00064273
Iteration 15/25 | Loss: 0.00064273
Iteration 16/25 | Loss: 0.00064273
Iteration 17/25 | Loss: 0.00064273
Iteration 18/25 | Loss: 0.00064273
Iteration 19/25 | Loss: 0.00064273
Iteration 20/25 | Loss: 0.00064273
Iteration 21/25 | Loss: 0.00064273
Iteration 22/25 | Loss: 0.00064273
Iteration 23/25 | Loss: 0.00064273
Iteration 24/25 | Loss: 0.00064273
Iteration 25/25 | Loss: 0.00064273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064273
Iteration 2/1000 | Loss: 0.00008440
Iteration 3/1000 | Loss: 0.00005041
Iteration 4/1000 | Loss: 0.00004523
Iteration 5/1000 | Loss: 0.00004285
Iteration 6/1000 | Loss: 0.00004185
Iteration 7/1000 | Loss: 0.00004071
Iteration 8/1000 | Loss: 0.00019849
Iteration 9/1000 | Loss: 0.00004264
Iteration 10/1000 | Loss: 0.00003996
Iteration 11/1000 | Loss: 0.00003806
Iteration 12/1000 | Loss: 0.00003761
Iteration 13/1000 | Loss: 0.00003720
Iteration 14/1000 | Loss: 0.00003686
Iteration 15/1000 | Loss: 0.00003633
Iteration 16/1000 | Loss: 0.00003595
Iteration 17/1000 | Loss: 0.00003567
Iteration 18/1000 | Loss: 0.00003540
Iteration 19/1000 | Loss: 0.00003520
Iteration 20/1000 | Loss: 0.00003499
Iteration 21/1000 | Loss: 0.00003479
Iteration 22/1000 | Loss: 0.00003463
Iteration 23/1000 | Loss: 0.00003457
Iteration 24/1000 | Loss: 0.00003453
Iteration 25/1000 | Loss: 0.00003453
Iteration 26/1000 | Loss: 0.00003450
Iteration 27/1000 | Loss: 0.00003448
Iteration 28/1000 | Loss: 0.00003448
Iteration 29/1000 | Loss: 0.00003448
Iteration 30/1000 | Loss: 0.00003448
Iteration 31/1000 | Loss: 0.00003448
Iteration 32/1000 | Loss: 0.00003448
Iteration 33/1000 | Loss: 0.00003448
Iteration 34/1000 | Loss: 0.00003447
Iteration 35/1000 | Loss: 0.00003447
Iteration 36/1000 | Loss: 0.00003447
Iteration 37/1000 | Loss: 0.00003446
Iteration 38/1000 | Loss: 0.00003445
Iteration 39/1000 | Loss: 0.00003445
Iteration 40/1000 | Loss: 0.00003445
Iteration 41/1000 | Loss: 0.00003445
Iteration 42/1000 | Loss: 0.00003445
Iteration 43/1000 | Loss: 0.00003443
Iteration 44/1000 | Loss: 0.00003442
Iteration 45/1000 | Loss: 0.00003442
Iteration 46/1000 | Loss: 0.00003442
Iteration 47/1000 | Loss: 0.00003441
Iteration 48/1000 | Loss: 0.00003440
Iteration 49/1000 | Loss: 0.00003439
Iteration 50/1000 | Loss: 0.00003439
Iteration 51/1000 | Loss: 0.00003439
Iteration 52/1000 | Loss: 0.00003439
Iteration 53/1000 | Loss: 0.00003439
Iteration 54/1000 | Loss: 0.00003439
Iteration 55/1000 | Loss: 0.00003439
Iteration 56/1000 | Loss: 0.00003439
Iteration 57/1000 | Loss: 0.00003439
Iteration 58/1000 | Loss: 0.00003438
Iteration 59/1000 | Loss: 0.00003438
Iteration 60/1000 | Loss: 0.00003438
Iteration 61/1000 | Loss: 0.00003438
Iteration 62/1000 | Loss: 0.00003437
Iteration 63/1000 | Loss: 0.00003437
Iteration 64/1000 | Loss: 0.00003437
Iteration 65/1000 | Loss: 0.00003437
Iteration 66/1000 | Loss: 0.00003437
Iteration 67/1000 | Loss: 0.00003437
Iteration 68/1000 | Loss: 0.00003437
Iteration 69/1000 | Loss: 0.00003437
Iteration 70/1000 | Loss: 0.00003437
Iteration 71/1000 | Loss: 0.00003437
Iteration 72/1000 | Loss: 0.00003436
Iteration 73/1000 | Loss: 0.00003436
Iteration 74/1000 | Loss: 0.00003436
Iteration 75/1000 | Loss: 0.00003436
Iteration 76/1000 | Loss: 0.00003436
Iteration 77/1000 | Loss: 0.00003436
Iteration 78/1000 | Loss: 0.00003436
Iteration 79/1000 | Loss: 0.00003436
Iteration 80/1000 | Loss: 0.00003436
Iteration 81/1000 | Loss: 0.00003436
Iteration 82/1000 | Loss: 0.00003435
Iteration 83/1000 | Loss: 0.00003435
Iteration 84/1000 | Loss: 0.00003435
Iteration 85/1000 | Loss: 0.00003435
Iteration 86/1000 | Loss: 0.00003435
Iteration 87/1000 | Loss: 0.00003434
Iteration 88/1000 | Loss: 0.00003434
Iteration 89/1000 | Loss: 0.00003434
Iteration 90/1000 | Loss: 0.00003434
Iteration 91/1000 | Loss: 0.00003434
Iteration 92/1000 | Loss: 0.00003433
Iteration 93/1000 | Loss: 0.00003433
Iteration 94/1000 | Loss: 0.00003433
Iteration 95/1000 | Loss: 0.00003433
Iteration 96/1000 | Loss: 0.00003433
Iteration 97/1000 | Loss: 0.00003433
Iteration 98/1000 | Loss: 0.00003433
Iteration 99/1000 | Loss: 0.00003431
Iteration 100/1000 | Loss: 0.00003431
Iteration 101/1000 | Loss: 0.00003430
Iteration 102/1000 | Loss: 0.00003430
Iteration 103/1000 | Loss: 0.00003430
Iteration 104/1000 | Loss: 0.00003430
Iteration 105/1000 | Loss: 0.00003430
Iteration 106/1000 | Loss: 0.00003430
Iteration 107/1000 | Loss: 0.00003430
Iteration 108/1000 | Loss: 0.00003430
Iteration 109/1000 | Loss: 0.00003430
Iteration 110/1000 | Loss: 0.00003430
Iteration 111/1000 | Loss: 0.00003429
Iteration 112/1000 | Loss: 0.00003429
Iteration 113/1000 | Loss: 0.00003429
Iteration 114/1000 | Loss: 0.00003429
Iteration 115/1000 | Loss: 0.00003429
Iteration 116/1000 | Loss: 0.00003428
Iteration 117/1000 | Loss: 0.00003428
Iteration 118/1000 | Loss: 0.00003428
Iteration 119/1000 | Loss: 0.00003428
Iteration 120/1000 | Loss: 0.00003428
Iteration 121/1000 | Loss: 0.00003428
Iteration 122/1000 | Loss: 0.00003428
Iteration 123/1000 | Loss: 0.00003428
Iteration 124/1000 | Loss: 0.00003428
Iteration 125/1000 | Loss: 0.00003428
Iteration 126/1000 | Loss: 0.00003427
Iteration 127/1000 | Loss: 0.00003427
Iteration 128/1000 | Loss: 0.00003427
Iteration 129/1000 | Loss: 0.00003427
Iteration 130/1000 | Loss: 0.00003427
Iteration 131/1000 | Loss: 0.00003427
Iteration 132/1000 | Loss: 0.00003426
Iteration 133/1000 | Loss: 0.00003426
Iteration 134/1000 | Loss: 0.00003426
Iteration 135/1000 | Loss: 0.00003426
Iteration 136/1000 | Loss: 0.00003426
Iteration 137/1000 | Loss: 0.00003426
Iteration 138/1000 | Loss: 0.00003425
Iteration 139/1000 | Loss: 0.00003425
Iteration 140/1000 | Loss: 0.00003425
Iteration 141/1000 | Loss: 0.00003425
Iteration 142/1000 | Loss: 0.00003425
Iteration 143/1000 | Loss: 0.00003424
Iteration 144/1000 | Loss: 0.00003424
Iteration 145/1000 | Loss: 0.00003424
Iteration 146/1000 | Loss: 0.00003424
Iteration 147/1000 | Loss: 0.00003424
Iteration 148/1000 | Loss: 0.00003424
Iteration 149/1000 | Loss: 0.00003423
Iteration 150/1000 | Loss: 0.00003423
Iteration 151/1000 | Loss: 0.00003423
Iteration 152/1000 | Loss: 0.00003423
Iteration 153/1000 | Loss: 0.00003422
Iteration 154/1000 | Loss: 0.00003422
Iteration 155/1000 | Loss: 0.00003422
Iteration 156/1000 | Loss: 0.00003422
Iteration 157/1000 | Loss: 0.00003422
Iteration 158/1000 | Loss: 0.00003422
Iteration 159/1000 | Loss: 0.00003422
Iteration 160/1000 | Loss: 0.00003422
Iteration 161/1000 | Loss: 0.00003422
Iteration 162/1000 | Loss: 0.00003421
Iteration 163/1000 | Loss: 0.00003421
Iteration 164/1000 | Loss: 0.00003421
Iteration 165/1000 | Loss: 0.00003421
Iteration 166/1000 | Loss: 0.00003421
Iteration 167/1000 | Loss: 0.00003421
Iteration 168/1000 | Loss: 0.00003421
Iteration 169/1000 | Loss: 0.00003421
Iteration 170/1000 | Loss: 0.00003421
Iteration 171/1000 | Loss: 0.00003421
Iteration 172/1000 | Loss: 0.00003420
Iteration 173/1000 | Loss: 0.00003420
Iteration 174/1000 | Loss: 0.00003420
Iteration 175/1000 | Loss: 0.00003420
Iteration 176/1000 | Loss: 0.00003420
Iteration 177/1000 | Loss: 0.00003420
Iteration 178/1000 | Loss: 0.00003420
Iteration 179/1000 | Loss: 0.00003420
Iteration 180/1000 | Loss: 0.00003420
Iteration 181/1000 | Loss: 0.00003420
Iteration 182/1000 | Loss: 0.00003420
Iteration 183/1000 | Loss: 0.00003420
Iteration 184/1000 | Loss: 0.00003420
Iteration 185/1000 | Loss: 0.00003420
Iteration 186/1000 | Loss: 0.00003420
Iteration 187/1000 | Loss: 0.00003420
Iteration 188/1000 | Loss: 0.00003420
Iteration 189/1000 | Loss: 0.00003419
Iteration 190/1000 | Loss: 0.00003419
Iteration 191/1000 | Loss: 0.00003419
Iteration 192/1000 | Loss: 0.00003419
Iteration 193/1000 | Loss: 0.00003419
Iteration 194/1000 | Loss: 0.00003419
Iteration 195/1000 | Loss: 0.00003419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [3.419282802497037e-05, 3.419282802497037e-05, 3.419282802497037e-05, 3.419282802497037e-05, 3.419282802497037e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.419282802497037e-05

Optimization complete. Final v2v error: 4.5252580642700195 mm

Highest mean error: 5.939689636230469 mm for frame 70

Lowest mean error: 3.3520171642303467 mm for frame 177

Saving results

Total time: 88.7766580581665
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00375666
Iteration 2/25 | Loss: 0.00099784
Iteration 3/25 | Loss: 0.00091025
Iteration 4/25 | Loss: 0.00090511
Iteration 5/25 | Loss: 0.00090381
Iteration 6/25 | Loss: 0.00090346
Iteration 7/25 | Loss: 0.00090346
Iteration 8/25 | Loss: 0.00090346
Iteration 9/25 | Loss: 0.00090346
Iteration 10/25 | Loss: 0.00090346
Iteration 11/25 | Loss: 0.00090346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000903463689610362, 0.000903463689610362, 0.000903463689610362, 0.000903463689610362, 0.000903463689610362]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000903463689610362

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63136053
Iteration 2/25 | Loss: 0.00060425
Iteration 3/25 | Loss: 0.00060425
Iteration 4/25 | Loss: 0.00060425
Iteration 5/25 | Loss: 0.00060425
Iteration 6/25 | Loss: 0.00060424
Iteration 7/25 | Loss: 0.00060424
Iteration 8/25 | Loss: 0.00060424
Iteration 9/25 | Loss: 0.00060424
Iteration 10/25 | Loss: 0.00060424
Iteration 11/25 | Loss: 0.00060424
Iteration 12/25 | Loss: 0.00060424
Iteration 13/25 | Loss: 0.00060424
Iteration 14/25 | Loss: 0.00060424
Iteration 15/25 | Loss: 0.00060424
Iteration 16/25 | Loss: 0.00060424
Iteration 17/25 | Loss: 0.00060424
Iteration 18/25 | Loss: 0.00060424
Iteration 19/25 | Loss: 0.00060424
Iteration 20/25 | Loss: 0.00060424
Iteration 21/25 | Loss: 0.00060424
Iteration 22/25 | Loss: 0.00060424
Iteration 23/25 | Loss: 0.00060424
Iteration 24/25 | Loss: 0.00060424
Iteration 25/25 | Loss: 0.00060424

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060424
Iteration 2/1000 | Loss: 0.00001867
Iteration 3/1000 | Loss: 0.00001083
Iteration 4/1000 | Loss: 0.00000935
Iteration 5/1000 | Loss: 0.00000886
Iteration 6/1000 | Loss: 0.00000851
Iteration 7/1000 | Loss: 0.00000848
Iteration 8/1000 | Loss: 0.00000823
Iteration 9/1000 | Loss: 0.00000814
Iteration 10/1000 | Loss: 0.00000807
Iteration 11/1000 | Loss: 0.00000804
Iteration 12/1000 | Loss: 0.00000804
Iteration 13/1000 | Loss: 0.00000804
Iteration 14/1000 | Loss: 0.00000804
Iteration 15/1000 | Loss: 0.00000804
Iteration 16/1000 | Loss: 0.00000804
Iteration 17/1000 | Loss: 0.00000804
Iteration 18/1000 | Loss: 0.00000804
Iteration 19/1000 | Loss: 0.00000803
Iteration 20/1000 | Loss: 0.00000803
Iteration 21/1000 | Loss: 0.00000801
Iteration 22/1000 | Loss: 0.00000796
Iteration 23/1000 | Loss: 0.00000796
Iteration 24/1000 | Loss: 0.00000796
Iteration 25/1000 | Loss: 0.00000789
Iteration 26/1000 | Loss: 0.00000789
Iteration 27/1000 | Loss: 0.00000789
Iteration 28/1000 | Loss: 0.00000786
Iteration 29/1000 | Loss: 0.00000785
Iteration 30/1000 | Loss: 0.00000785
Iteration 31/1000 | Loss: 0.00000784
Iteration 32/1000 | Loss: 0.00000784
Iteration 33/1000 | Loss: 0.00000784
Iteration 34/1000 | Loss: 0.00000783
Iteration 35/1000 | Loss: 0.00000783
Iteration 36/1000 | Loss: 0.00000783
Iteration 37/1000 | Loss: 0.00000782
Iteration 38/1000 | Loss: 0.00000782
Iteration 39/1000 | Loss: 0.00000781
Iteration 40/1000 | Loss: 0.00000781
Iteration 41/1000 | Loss: 0.00000781
Iteration 42/1000 | Loss: 0.00000781
Iteration 43/1000 | Loss: 0.00000781
Iteration 44/1000 | Loss: 0.00000780
Iteration 45/1000 | Loss: 0.00000780
Iteration 46/1000 | Loss: 0.00000780
Iteration 47/1000 | Loss: 0.00000780
Iteration 48/1000 | Loss: 0.00000779
Iteration 49/1000 | Loss: 0.00000779
Iteration 50/1000 | Loss: 0.00000779
Iteration 51/1000 | Loss: 0.00000778
Iteration 52/1000 | Loss: 0.00000777
Iteration 53/1000 | Loss: 0.00000777
Iteration 54/1000 | Loss: 0.00000776
Iteration 55/1000 | Loss: 0.00000776
Iteration 56/1000 | Loss: 0.00000775
Iteration 57/1000 | Loss: 0.00000774
Iteration 58/1000 | Loss: 0.00000774
Iteration 59/1000 | Loss: 0.00000773
Iteration 60/1000 | Loss: 0.00000773
Iteration 61/1000 | Loss: 0.00000773
Iteration 62/1000 | Loss: 0.00000773
Iteration 63/1000 | Loss: 0.00000772
Iteration 64/1000 | Loss: 0.00000772
Iteration 65/1000 | Loss: 0.00000772
Iteration 66/1000 | Loss: 0.00000772
Iteration 67/1000 | Loss: 0.00000772
Iteration 68/1000 | Loss: 0.00000772
Iteration 69/1000 | Loss: 0.00000772
Iteration 70/1000 | Loss: 0.00000772
Iteration 71/1000 | Loss: 0.00000772
Iteration 72/1000 | Loss: 0.00000771
Iteration 73/1000 | Loss: 0.00000771
Iteration 74/1000 | Loss: 0.00000770
Iteration 75/1000 | Loss: 0.00000770
Iteration 76/1000 | Loss: 0.00000770
Iteration 77/1000 | Loss: 0.00000770
Iteration 78/1000 | Loss: 0.00000770
Iteration 79/1000 | Loss: 0.00000769
Iteration 80/1000 | Loss: 0.00000769
Iteration 81/1000 | Loss: 0.00000769
Iteration 82/1000 | Loss: 0.00000769
Iteration 83/1000 | Loss: 0.00000769
Iteration 84/1000 | Loss: 0.00000769
Iteration 85/1000 | Loss: 0.00000769
Iteration 86/1000 | Loss: 0.00000769
Iteration 87/1000 | Loss: 0.00000769
Iteration 88/1000 | Loss: 0.00000769
Iteration 89/1000 | Loss: 0.00000769
Iteration 90/1000 | Loss: 0.00000769
Iteration 91/1000 | Loss: 0.00000768
Iteration 92/1000 | Loss: 0.00000768
Iteration 93/1000 | Loss: 0.00000767
Iteration 94/1000 | Loss: 0.00000767
Iteration 95/1000 | Loss: 0.00000766
Iteration 96/1000 | Loss: 0.00000766
Iteration 97/1000 | Loss: 0.00000766
Iteration 98/1000 | Loss: 0.00000766
Iteration 99/1000 | Loss: 0.00000766
Iteration 100/1000 | Loss: 0.00000766
Iteration 101/1000 | Loss: 0.00000766
Iteration 102/1000 | Loss: 0.00000766
Iteration 103/1000 | Loss: 0.00000766
Iteration 104/1000 | Loss: 0.00000766
Iteration 105/1000 | Loss: 0.00000765
Iteration 106/1000 | Loss: 0.00000765
Iteration 107/1000 | Loss: 0.00000765
Iteration 108/1000 | Loss: 0.00000765
Iteration 109/1000 | Loss: 0.00000765
Iteration 110/1000 | Loss: 0.00000765
Iteration 111/1000 | Loss: 0.00000765
Iteration 112/1000 | Loss: 0.00000765
Iteration 113/1000 | Loss: 0.00000764
Iteration 114/1000 | Loss: 0.00000764
Iteration 115/1000 | Loss: 0.00000764
Iteration 116/1000 | Loss: 0.00000764
Iteration 117/1000 | Loss: 0.00000764
Iteration 118/1000 | Loss: 0.00000764
Iteration 119/1000 | Loss: 0.00000764
Iteration 120/1000 | Loss: 0.00000764
Iteration 121/1000 | Loss: 0.00000764
Iteration 122/1000 | Loss: 0.00000764
Iteration 123/1000 | Loss: 0.00000764
Iteration 124/1000 | Loss: 0.00000764
Iteration 125/1000 | Loss: 0.00000764
Iteration 126/1000 | Loss: 0.00000764
Iteration 127/1000 | Loss: 0.00000764
Iteration 128/1000 | Loss: 0.00000764
Iteration 129/1000 | Loss: 0.00000763
Iteration 130/1000 | Loss: 0.00000763
Iteration 131/1000 | Loss: 0.00000763
Iteration 132/1000 | Loss: 0.00000763
Iteration 133/1000 | Loss: 0.00000763
Iteration 134/1000 | Loss: 0.00000763
Iteration 135/1000 | Loss: 0.00000763
Iteration 136/1000 | Loss: 0.00000763
Iteration 137/1000 | Loss: 0.00000763
Iteration 138/1000 | Loss: 0.00000763
Iteration 139/1000 | Loss: 0.00000763
Iteration 140/1000 | Loss: 0.00000763
Iteration 141/1000 | Loss: 0.00000763
Iteration 142/1000 | Loss: 0.00000763
Iteration 143/1000 | Loss: 0.00000763
Iteration 144/1000 | Loss: 0.00000763
Iteration 145/1000 | Loss: 0.00000763
Iteration 146/1000 | Loss: 0.00000763
Iteration 147/1000 | Loss: 0.00000763
Iteration 148/1000 | Loss: 0.00000763
Iteration 149/1000 | Loss: 0.00000763
Iteration 150/1000 | Loss: 0.00000763
Iteration 151/1000 | Loss: 0.00000763
Iteration 152/1000 | Loss: 0.00000763
Iteration 153/1000 | Loss: 0.00000763
Iteration 154/1000 | Loss: 0.00000763
Iteration 155/1000 | Loss: 0.00000763
Iteration 156/1000 | Loss: 0.00000763
Iteration 157/1000 | Loss: 0.00000763
Iteration 158/1000 | Loss: 0.00000763
Iteration 159/1000 | Loss: 0.00000763
Iteration 160/1000 | Loss: 0.00000763
Iteration 161/1000 | Loss: 0.00000763
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [7.633927452843636e-06, 7.633927452843636e-06, 7.633927452843636e-06, 7.633927452843636e-06, 7.633927452843636e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.633927452843636e-06

Optimization complete. Final v2v error: 2.347041130065918 mm

Highest mean error: 2.804651975631714 mm for frame 84

Lowest mean error: 2.19952130317688 mm for frame 102

Saving results

Total time: 32.25176477432251
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01064888
Iteration 2/25 | Loss: 0.00190685
Iteration 3/25 | Loss: 0.00144909
Iteration 4/25 | Loss: 0.00123136
Iteration 5/25 | Loss: 0.00110824
Iteration 6/25 | Loss: 0.00112401
Iteration 7/25 | Loss: 0.00110764
Iteration 8/25 | Loss: 0.00107794
Iteration 9/25 | Loss: 0.00104300
Iteration 10/25 | Loss: 0.00103108
Iteration 11/25 | Loss: 0.00102416
Iteration 12/25 | Loss: 0.00103302
Iteration 13/25 | Loss: 0.00104042
Iteration 14/25 | Loss: 0.00102488
Iteration 15/25 | Loss: 0.00100319
Iteration 16/25 | Loss: 0.00100516
Iteration 17/25 | Loss: 0.00100668
Iteration 18/25 | Loss: 0.00100052
Iteration 19/25 | Loss: 0.00099659
Iteration 20/25 | Loss: 0.00099405
Iteration 21/25 | Loss: 0.00099311
Iteration 22/25 | Loss: 0.00099415
Iteration 23/25 | Loss: 0.00098770
Iteration 24/25 | Loss: 0.00098502
Iteration 25/25 | Loss: 0.00098952

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36802530
Iteration 2/25 | Loss: 0.00171255
Iteration 3/25 | Loss: 0.00116703
Iteration 4/25 | Loss: 0.00116703
Iteration 5/25 | Loss: 0.00116703
Iteration 6/25 | Loss: 0.00116703
Iteration 7/25 | Loss: 0.00116703
Iteration 8/25 | Loss: 0.00116703
Iteration 9/25 | Loss: 0.00116703
Iteration 10/25 | Loss: 0.00116703
Iteration 11/25 | Loss: 0.00116703
Iteration 12/25 | Loss: 0.00116703
Iteration 13/25 | Loss: 0.00116703
Iteration 14/25 | Loss: 0.00116703
Iteration 15/25 | Loss: 0.00116703
Iteration 16/25 | Loss: 0.00116703
Iteration 17/25 | Loss: 0.00116703
Iteration 18/25 | Loss: 0.00116703
Iteration 19/25 | Loss: 0.00116703
Iteration 20/25 | Loss: 0.00116703
Iteration 21/25 | Loss: 0.00116703
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0011670284438878298, 0.0011670284438878298, 0.0011670284438878298, 0.0011670284438878298, 0.0011670284438878298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011670284438878298

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116703
Iteration 2/1000 | Loss: 0.00097921
Iteration 3/1000 | Loss: 0.00102991
Iteration 4/1000 | Loss: 0.00183589
Iteration 5/1000 | Loss: 0.00068332
Iteration 6/1000 | Loss: 0.00201685
Iteration 7/1000 | Loss: 0.00083765
Iteration 8/1000 | Loss: 0.00067071
Iteration 9/1000 | Loss: 0.00071532
Iteration 10/1000 | Loss: 0.00067559
Iteration 11/1000 | Loss: 0.00068646
Iteration 12/1000 | Loss: 0.00060807
Iteration 13/1000 | Loss: 0.00067656
Iteration 14/1000 | Loss: 0.00053638
Iteration 15/1000 | Loss: 0.00032637
Iteration 16/1000 | Loss: 0.00007567
Iteration 17/1000 | Loss: 0.00026799
Iteration 18/1000 | Loss: 0.00035814
Iteration 19/1000 | Loss: 0.00028945
Iteration 20/1000 | Loss: 0.00055510
Iteration 21/1000 | Loss: 0.00053450
Iteration 22/1000 | Loss: 0.00018458
Iteration 23/1000 | Loss: 0.00011867
Iteration 24/1000 | Loss: 0.00006736
Iteration 25/1000 | Loss: 0.00006161
Iteration 26/1000 | Loss: 0.00006255
Iteration 27/1000 | Loss: 0.00018945
Iteration 28/1000 | Loss: 0.00006292
Iteration 29/1000 | Loss: 0.00018788
Iteration 30/1000 | Loss: 0.00021512
Iteration 31/1000 | Loss: 0.00008696
Iteration 32/1000 | Loss: 0.00016265
Iteration 33/1000 | Loss: 0.00016057
Iteration 34/1000 | Loss: 0.00027891
Iteration 35/1000 | Loss: 0.00040606
Iteration 36/1000 | Loss: 0.00030973
Iteration 37/1000 | Loss: 0.00042974
Iteration 38/1000 | Loss: 0.00012248
Iteration 39/1000 | Loss: 0.00048479
Iteration 40/1000 | Loss: 0.00042125
Iteration 41/1000 | Loss: 0.00045216
Iteration 42/1000 | Loss: 0.00030594
Iteration 43/1000 | Loss: 0.00011640
Iteration 44/1000 | Loss: 0.00028398
Iteration 45/1000 | Loss: 0.00025495
Iteration 46/1000 | Loss: 0.00018462
Iteration 47/1000 | Loss: 0.00007244
Iteration 48/1000 | Loss: 0.00005961
Iteration 49/1000 | Loss: 0.00031498
Iteration 50/1000 | Loss: 0.00015517
Iteration 51/1000 | Loss: 0.00009233
Iteration 52/1000 | Loss: 0.00007170
Iteration 53/1000 | Loss: 0.00021967
Iteration 54/1000 | Loss: 0.00014572
Iteration 55/1000 | Loss: 0.00011233
Iteration 56/1000 | Loss: 0.00023559
Iteration 57/1000 | Loss: 0.00008183
Iteration 58/1000 | Loss: 0.00021472
Iteration 59/1000 | Loss: 0.00022732
Iteration 60/1000 | Loss: 0.00024913
Iteration 61/1000 | Loss: 0.00015546
Iteration 62/1000 | Loss: 0.00022827
Iteration 63/1000 | Loss: 0.00014577
Iteration 64/1000 | Loss: 0.00024253
Iteration 65/1000 | Loss: 0.00014351
Iteration 66/1000 | Loss: 0.00010518
Iteration 67/1000 | Loss: 0.00014288
Iteration 68/1000 | Loss: 0.00022057
Iteration 69/1000 | Loss: 0.00017772
Iteration 70/1000 | Loss: 0.00015201
Iteration 71/1000 | Loss: 0.00019929
Iteration 72/1000 | Loss: 0.00022230
Iteration 73/1000 | Loss: 0.00005538
Iteration 74/1000 | Loss: 0.00006917
Iteration 75/1000 | Loss: 0.00072151
Iteration 76/1000 | Loss: 0.00005326
Iteration 77/1000 | Loss: 0.00006911
Iteration 78/1000 | Loss: 0.00004415
Iteration 79/1000 | Loss: 0.00007085
Iteration 80/1000 | Loss: 0.00004157
Iteration 81/1000 | Loss: 0.00100167
Iteration 82/1000 | Loss: 0.00064571
Iteration 83/1000 | Loss: 0.00079438
Iteration 84/1000 | Loss: 0.00042292
Iteration 85/1000 | Loss: 0.00082920
Iteration 86/1000 | Loss: 0.00036369
Iteration 87/1000 | Loss: 0.00118964
Iteration 88/1000 | Loss: 0.00042159
Iteration 89/1000 | Loss: 0.00058403
Iteration 90/1000 | Loss: 0.00038803
Iteration 91/1000 | Loss: 0.00007119
Iteration 92/1000 | Loss: 0.00105440
Iteration 93/1000 | Loss: 0.00016253
Iteration 94/1000 | Loss: 0.00066767
Iteration 95/1000 | Loss: 0.00035406
Iteration 96/1000 | Loss: 0.00004318
Iteration 97/1000 | Loss: 0.00005928
Iteration 98/1000 | Loss: 0.00082097
Iteration 99/1000 | Loss: 0.00006499
Iteration 100/1000 | Loss: 0.00004970
Iteration 101/1000 | Loss: 0.00005003
Iteration 102/1000 | Loss: 0.00004771
Iteration 103/1000 | Loss: 0.00004173
Iteration 104/1000 | Loss: 0.00004464
Iteration 105/1000 | Loss: 0.00002470
Iteration 106/1000 | Loss: 0.00003654
Iteration 107/1000 | Loss: 0.00003389
Iteration 108/1000 | Loss: 0.00035013
Iteration 109/1000 | Loss: 0.00026623
Iteration 110/1000 | Loss: 0.00027449
Iteration 111/1000 | Loss: 0.00028639
Iteration 112/1000 | Loss: 0.00032747
Iteration 113/1000 | Loss: 0.00019734
Iteration 114/1000 | Loss: 0.00005298
Iteration 115/1000 | Loss: 0.00004825
Iteration 116/1000 | Loss: 0.00002877
Iteration 117/1000 | Loss: 0.00002746
Iteration 118/1000 | Loss: 0.00003662
Iteration 119/1000 | Loss: 0.00025560
Iteration 120/1000 | Loss: 0.00021006
Iteration 121/1000 | Loss: 0.00006565
Iteration 122/1000 | Loss: 0.00025882
Iteration 123/1000 | Loss: 0.00026585
Iteration 124/1000 | Loss: 0.00023116
Iteration 125/1000 | Loss: 0.00025694
Iteration 126/1000 | Loss: 0.00028433
Iteration 127/1000 | Loss: 0.00020509
Iteration 128/1000 | Loss: 0.00027162
Iteration 129/1000 | Loss: 0.00025586
Iteration 130/1000 | Loss: 0.00025917
Iteration 131/1000 | Loss: 0.00022896
Iteration 132/1000 | Loss: 0.00024263
Iteration 133/1000 | Loss: 0.00004483
Iteration 134/1000 | Loss: 0.00007938
Iteration 135/1000 | Loss: 0.00004868
Iteration 136/1000 | Loss: 0.00004002
Iteration 137/1000 | Loss: 0.00004049
Iteration 138/1000 | Loss: 0.00004009
Iteration 139/1000 | Loss: 0.00002242
Iteration 140/1000 | Loss: 0.00005214
Iteration 141/1000 | Loss: 0.00002424
Iteration 142/1000 | Loss: 0.00004590
Iteration 143/1000 | Loss: 0.00004437
Iteration 144/1000 | Loss: 0.00004613
Iteration 145/1000 | Loss: 0.00004153
Iteration 146/1000 | Loss: 0.00004445
Iteration 147/1000 | Loss: 0.00003299
Iteration 148/1000 | Loss: 0.00005190
Iteration 149/1000 | Loss: 0.00003566
Iteration 150/1000 | Loss: 0.00003638
Iteration 151/1000 | Loss: 0.00003508
Iteration 152/1000 | Loss: 0.00002353
Iteration 153/1000 | Loss: 0.00002217
Iteration 154/1000 | Loss: 0.00002477
Iteration 155/1000 | Loss: 0.00003452
Iteration 156/1000 | Loss: 0.00003407
Iteration 157/1000 | Loss: 0.00004242
Iteration 158/1000 | Loss: 0.00003972
Iteration 159/1000 | Loss: 0.00006368
Iteration 160/1000 | Loss: 0.00003010
Iteration 161/1000 | Loss: 0.00004021
Iteration 162/1000 | Loss: 0.00003995
Iteration 163/1000 | Loss: 0.00004328
Iteration 164/1000 | Loss: 0.00003144
Iteration 165/1000 | Loss: 0.00003120
Iteration 166/1000 | Loss: 0.00003074
Iteration 167/1000 | Loss: 0.00004219
Iteration 168/1000 | Loss: 0.00003344
Iteration 169/1000 | Loss: 0.00002883
Iteration 170/1000 | Loss: 0.00001987
Iteration 171/1000 | Loss: 0.00003093
Iteration 172/1000 | Loss: 0.00003337
Iteration 173/1000 | Loss: 0.00004377
Iteration 174/1000 | Loss: 0.00005331
Iteration 175/1000 | Loss: 0.00004109
Iteration 176/1000 | Loss: 0.00003349
Iteration 177/1000 | Loss: 0.00003316
Iteration 178/1000 | Loss: 0.00003379
Iteration 179/1000 | Loss: 0.00003253
Iteration 180/1000 | Loss: 0.00003817
Iteration 181/1000 | Loss: 0.00003292
Iteration 182/1000 | Loss: 0.00004759
Iteration 183/1000 | Loss: 0.00003613
Iteration 184/1000 | Loss: 0.00003664
Iteration 185/1000 | Loss: 0.00006434
Iteration 186/1000 | Loss: 0.00004044
Iteration 187/1000 | Loss: 0.00004930
Iteration 188/1000 | Loss: 0.00003304
Iteration 189/1000 | Loss: 0.00002925
Iteration 190/1000 | Loss: 0.00004603
Iteration 191/1000 | Loss: 0.00002855
Iteration 192/1000 | Loss: 0.00003114
Iteration 193/1000 | Loss: 0.00002840
Iteration 194/1000 | Loss: 0.00003794
Iteration 195/1000 | Loss: 0.00003515
Iteration 196/1000 | Loss: 0.00004377
Iteration 197/1000 | Loss: 0.00001785
Iteration 198/1000 | Loss: 0.00003536
Iteration 199/1000 | Loss: 0.00006126
Iteration 200/1000 | Loss: 0.00004102
Iteration 201/1000 | Loss: 0.00001843
Iteration 202/1000 | Loss: 0.00004705
Iteration 203/1000 | Loss: 0.00002903
Iteration 204/1000 | Loss: 0.00003199
Iteration 205/1000 | Loss: 0.00002630
Iteration 206/1000 | Loss: 0.00003821
Iteration 207/1000 | Loss: 0.00003151
Iteration 208/1000 | Loss: 0.00003379
Iteration 209/1000 | Loss: 0.00002872
Iteration 210/1000 | Loss: 0.00004693
Iteration 211/1000 | Loss: 0.00003763
Iteration 212/1000 | Loss: 0.00003679
Iteration 213/1000 | Loss: 0.00002919
Iteration 214/1000 | Loss: 0.00003612
Iteration 215/1000 | Loss: 0.00003102
Iteration 216/1000 | Loss: 0.00005222
Iteration 217/1000 | Loss: 0.00002978
Iteration 218/1000 | Loss: 0.00003425
Iteration 219/1000 | Loss: 0.00002998
Iteration 220/1000 | Loss: 0.00003466
Iteration 221/1000 | Loss: 0.00002377
Iteration 222/1000 | Loss: 0.00004972
Iteration 223/1000 | Loss: 0.00003407
Iteration 224/1000 | Loss: 0.00002947
Iteration 225/1000 | Loss: 0.00004306
Iteration 226/1000 | Loss: 0.00004772
Iteration 227/1000 | Loss: 0.00003277
Iteration 228/1000 | Loss: 0.00005750
Iteration 229/1000 | Loss: 0.00003618
Iteration 230/1000 | Loss: 0.00003296
Iteration 231/1000 | Loss: 0.00003753
Iteration 232/1000 | Loss: 0.00003134
Iteration 233/1000 | Loss: 0.00003209
Iteration 234/1000 | Loss: 0.00003155
Iteration 235/1000 | Loss: 0.00003163
Iteration 236/1000 | Loss: 0.00003055
Iteration 237/1000 | Loss: 0.00003711
Iteration 238/1000 | Loss: 0.00003234
Iteration 239/1000 | Loss: 0.00003692
Iteration 240/1000 | Loss: 0.00003103
Iteration 241/1000 | Loss: 0.00003427
Iteration 242/1000 | Loss: 0.00003119
Iteration 243/1000 | Loss: 0.00003138
Iteration 244/1000 | Loss: 0.00003686
Iteration 245/1000 | Loss: 0.00003120
Iteration 246/1000 | Loss: 0.00003276
Iteration 247/1000 | Loss: 0.00004998
Iteration 248/1000 | Loss: 0.00004278
Iteration 249/1000 | Loss: 0.00004610
Iteration 250/1000 | Loss: 0.00004322
Iteration 251/1000 | Loss: 0.00003556
Iteration 252/1000 | Loss: 0.00003576
Iteration 253/1000 | Loss: 0.00003641
Iteration 254/1000 | Loss: 0.00003141
Iteration 255/1000 | Loss: 0.00003056
Iteration 256/1000 | Loss: 0.00002843
Iteration 257/1000 | Loss: 0.00003339
Iteration 258/1000 | Loss: 0.00003164
Iteration 259/1000 | Loss: 0.00003038
Iteration 260/1000 | Loss: 0.00004041
Iteration 261/1000 | Loss: 0.00003243
Iteration 262/1000 | Loss: 0.00003069
Iteration 263/1000 | Loss: 0.00002018
Iteration 264/1000 | Loss: 0.00006769
Iteration 265/1000 | Loss: 0.00004723
Iteration 266/1000 | Loss: 0.00004199
Iteration 267/1000 | Loss: 0.00002942
Iteration 268/1000 | Loss: 0.00007635
Iteration 269/1000 | Loss: 0.00001711
Iteration 270/1000 | Loss: 0.00003371
Iteration 271/1000 | Loss: 0.00001372
Iteration 272/1000 | Loss: 0.00001772
Iteration 273/1000 | Loss: 0.00001189
Iteration 274/1000 | Loss: 0.00001219
Iteration 275/1000 | Loss: 0.00001104
Iteration 276/1000 | Loss: 0.00001088
Iteration 277/1000 | Loss: 0.00001074
Iteration 278/1000 | Loss: 0.00001070
Iteration 279/1000 | Loss: 0.00001055
Iteration 280/1000 | Loss: 0.00001051
Iteration 281/1000 | Loss: 0.00008065
Iteration 282/1000 | Loss: 0.00001048
Iteration 283/1000 | Loss: 0.00001023
Iteration 284/1000 | Loss: 0.00001019
Iteration 285/1000 | Loss: 0.00001019
Iteration 286/1000 | Loss: 0.00001019
Iteration 287/1000 | Loss: 0.00001019
Iteration 288/1000 | Loss: 0.00001016
Iteration 289/1000 | Loss: 0.00001016
Iteration 290/1000 | Loss: 0.00001016
Iteration 291/1000 | Loss: 0.00001016
Iteration 292/1000 | Loss: 0.00001016
Iteration 293/1000 | Loss: 0.00001016
Iteration 294/1000 | Loss: 0.00001015
Iteration 295/1000 | Loss: 0.00001015
Iteration 296/1000 | Loss: 0.00001015
Iteration 297/1000 | Loss: 0.00001015
Iteration 298/1000 | Loss: 0.00001015
Iteration 299/1000 | Loss: 0.00001015
Iteration 300/1000 | Loss: 0.00001014
Iteration 301/1000 | Loss: 0.00001014
Iteration 302/1000 | Loss: 0.00001013
Iteration 303/1000 | Loss: 0.00001013
Iteration 304/1000 | Loss: 0.00001012
Iteration 305/1000 | Loss: 0.00001012
Iteration 306/1000 | Loss: 0.00001012
Iteration 307/1000 | Loss: 0.00001011
Iteration 308/1000 | Loss: 0.00001011
Iteration 309/1000 | Loss: 0.00001010
Iteration 310/1000 | Loss: 0.00001009
Iteration 311/1000 | Loss: 0.00001009
Iteration 312/1000 | Loss: 0.00001008
Iteration 313/1000 | Loss: 0.00001008
Iteration 314/1000 | Loss: 0.00001007
Iteration 315/1000 | Loss: 0.00001007
Iteration 316/1000 | Loss: 0.00001007
Iteration 317/1000 | Loss: 0.00001007
Iteration 318/1000 | Loss: 0.00001007
Iteration 319/1000 | Loss: 0.00001007
Iteration 320/1000 | Loss: 0.00001007
Iteration 321/1000 | Loss: 0.00001007
Iteration 322/1000 | Loss: 0.00001007
Iteration 323/1000 | Loss: 0.00001007
Iteration 324/1000 | Loss: 0.00001006
Iteration 325/1000 | Loss: 0.00001006
Iteration 326/1000 | Loss: 0.00001006
Iteration 327/1000 | Loss: 0.00001006
Iteration 328/1000 | Loss: 0.00001006
Iteration 329/1000 | Loss: 0.00001006
Iteration 330/1000 | Loss: 0.00001006
Iteration 331/1000 | Loss: 0.00001006
Iteration 332/1000 | Loss: 0.00001006
Iteration 333/1000 | Loss: 0.00001006
Iteration 334/1000 | Loss: 0.00001006
Iteration 335/1000 | Loss: 0.00001006
Iteration 336/1000 | Loss: 0.00001006
Iteration 337/1000 | Loss: 0.00001006
Iteration 338/1000 | Loss: 0.00001006
Iteration 339/1000 | Loss: 0.00001006
Iteration 340/1000 | Loss: 0.00001006
Iteration 341/1000 | Loss: 0.00001005
Iteration 342/1000 | Loss: 0.00001005
Iteration 343/1000 | Loss: 0.00001005
Iteration 344/1000 | Loss: 0.00001005
Iteration 345/1000 | Loss: 0.00001005
Iteration 346/1000 | Loss: 0.00001005
Iteration 347/1000 | Loss: 0.00001005
Iteration 348/1000 | Loss: 0.00001005
Iteration 349/1000 | Loss: 0.00001005
Iteration 350/1000 | Loss: 0.00001005
Iteration 351/1000 | Loss: 0.00001005
Iteration 352/1000 | Loss: 0.00001004
Iteration 353/1000 | Loss: 0.00001004
Iteration 354/1000 | Loss: 0.00001003
Iteration 355/1000 | Loss: 0.00001003
Iteration 356/1000 | Loss: 0.00001003
Iteration 357/1000 | Loss: 0.00001003
Iteration 358/1000 | Loss: 0.00001003
Iteration 359/1000 | Loss: 0.00001003
Iteration 360/1000 | Loss: 0.00001003
Iteration 361/1000 | Loss: 0.00001002
Iteration 362/1000 | Loss: 0.00001002
Iteration 363/1000 | Loss: 0.00001002
Iteration 364/1000 | Loss: 0.00001002
Iteration 365/1000 | Loss: 0.00001002
Iteration 366/1000 | Loss: 0.00001002
Iteration 367/1000 | Loss: 0.00001002
Iteration 368/1000 | Loss: 0.00001002
Iteration 369/1000 | Loss: 0.00001002
Iteration 370/1000 | Loss: 0.00001002
Iteration 371/1000 | Loss: 0.00001002
Iteration 372/1000 | Loss: 0.00001002
Iteration 373/1000 | Loss: 0.00001002
Iteration 374/1000 | Loss: 0.00001002
Iteration 375/1000 | Loss: 0.00001002
Iteration 376/1000 | Loss: 0.00001002
Iteration 377/1000 | Loss: 0.00001002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 377. Stopping optimization.
Last 5 losses: [1.0020151421485934e-05, 1.0020151421485934e-05, 1.0020151421485934e-05, 1.0020151421485934e-05, 1.0020151421485934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0020151421485934e-05

Optimization complete. Final v2v error: 2.5620248317718506 mm

Highest mean error: 9.977465629577637 mm for frame 35

Lowest mean error: 2.1716501712799072 mm for frame 123

Saving results

Total time: 440.83789229393005
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775217
Iteration 2/25 | Loss: 0.00171362
Iteration 3/25 | Loss: 0.00133673
Iteration 4/25 | Loss: 0.00122224
Iteration 5/25 | Loss: 0.00114463
Iteration 6/25 | Loss: 0.00105153
Iteration 7/25 | Loss: 0.00102471
Iteration 8/25 | Loss: 0.00101730
Iteration 9/25 | Loss: 0.00101552
Iteration 10/25 | Loss: 0.00101365
Iteration 11/25 | Loss: 0.00101171
Iteration 12/25 | Loss: 0.00101096
Iteration 13/25 | Loss: 0.00101086
Iteration 14/25 | Loss: 0.00100913
Iteration 15/25 | Loss: 0.00100799
Iteration 16/25 | Loss: 0.00100718
Iteration 17/25 | Loss: 0.00100708
Iteration 18/25 | Loss: 0.00100705
Iteration 19/25 | Loss: 0.00100705
Iteration 20/25 | Loss: 0.00100705
Iteration 21/25 | Loss: 0.00100705
Iteration 22/25 | Loss: 0.00100697
Iteration 23/25 | Loss: 0.00100673
Iteration 24/25 | Loss: 0.00100583
Iteration 25/25 | Loss: 0.00100507

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30819428
Iteration 2/25 | Loss: 0.00145933
Iteration 3/25 | Loss: 0.00145932
Iteration 4/25 | Loss: 0.00145932
Iteration 5/25 | Loss: 0.00145932
Iteration 6/25 | Loss: 0.00145932
Iteration 7/25 | Loss: 0.00145932
Iteration 8/25 | Loss: 0.00145932
Iteration 9/25 | Loss: 0.00145932
Iteration 10/25 | Loss: 0.00145932
Iteration 11/25 | Loss: 0.00145932
Iteration 12/25 | Loss: 0.00145932
Iteration 13/25 | Loss: 0.00145932
Iteration 14/25 | Loss: 0.00145932
Iteration 15/25 | Loss: 0.00145932
Iteration 16/25 | Loss: 0.00145932
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014593187952414155, 0.0014593187952414155, 0.0014593187952414155, 0.0014593187952414155, 0.0014593187952414155]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014593187952414155

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145932
Iteration 2/1000 | Loss: 0.00022772
Iteration 3/1000 | Loss: 0.00012140
Iteration 4/1000 | Loss: 0.00006581
Iteration 5/1000 | Loss: 0.00003616
Iteration 6/1000 | Loss: 0.00002666
Iteration 7/1000 | Loss: 0.00002192
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001525
Iteration 10/1000 | Loss: 0.00001340
Iteration 11/1000 | Loss: 0.00001228
Iteration 12/1000 | Loss: 0.00001164
Iteration 13/1000 | Loss: 0.00001107
Iteration 14/1000 | Loss: 0.00001074
Iteration 15/1000 | Loss: 0.00001047
Iteration 16/1000 | Loss: 0.00001042
Iteration 17/1000 | Loss: 0.00001038
Iteration 18/1000 | Loss: 0.00001037
Iteration 19/1000 | Loss: 0.00001035
Iteration 20/1000 | Loss: 0.00001031
Iteration 21/1000 | Loss: 0.00001030
Iteration 22/1000 | Loss: 0.00001030
Iteration 23/1000 | Loss: 0.00001029
Iteration 24/1000 | Loss: 0.00001029
Iteration 25/1000 | Loss: 0.00001027
Iteration 26/1000 | Loss: 0.00001026
Iteration 27/1000 | Loss: 0.00001024
Iteration 28/1000 | Loss: 0.00001024
Iteration 29/1000 | Loss: 0.00001023
Iteration 30/1000 | Loss: 0.00001022
Iteration 31/1000 | Loss: 0.00001022
Iteration 32/1000 | Loss: 0.00001021
Iteration 33/1000 | Loss: 0.00001021
Iteration 34/1000 | Loss: 0.00001021
Iteration 35/1000 | Loss: 0.00001020
Iteration 36/1000 | Loss: 0.00001020
Iteration 37/1000 | Loss: 0.00001019
Iteration 38/1000 | Loss: 0.00001019
Iteration 39/1000 | Loss: 0.00001019
Iteration 40/1000 | Loss: 0.00001018
Iteration 41/1000 | Loss: 0.00001018
Iteration 42/1000 | Loss: 0.00001016
Iteration 43/1000 | Loss: 0.00001015
Iteration 44/1000 | Loss: 0.00001011
Iteration 45/1000 | Loss: 0.00001011
Iteration 46/1000 | Loss: 0.00001011
Iteration 47/1000 | Loss: 0.00001011
Iteration 48/1000 | Loss: 0.00001011
Iteration 49/1000 | Loss: 0.00001010
Iteration 50/1000 | Loss: 0.00001010
Iteration 51/1000 | Loss: 0.00001010
Iteration 52/1000 | Loss: 0.00001009
Iteration 53/1000 | Loss: 0.00001008
Iteration 54/1000 | Loss: 0.00001008
Iteration 55/1000 | Loss: 0.00001007
Iteration 56/1000 | Loss: 0.00001007
Iteration 57/1000 | Loss: 0.00001007
Iteration 58/1000 | Loss: 0.00001007
Iteration 59/1000 | Loss: 0.00001007
Iteration 60/1000 | Loss: 0.00001006
Iteration 61/1000 | Loss: 0.00001006
Iteration 62/1000 | Loss: 0.00001006
Iteration 63/1000 | Loss: 0.00001006
Iteration 64/1000 | Loss: 0.00001005
Iteration 65/1000 | Loss: 0.00001005
Iteration 66/1000 | Loss: 0.00001004
Iteration 67/1000 | Loss: 0.00001004
Iteration 68/1000 | Loss: 0.00001004
Iteration 69/1000 | Loss: 0.00001004
Iteration 70/1000 | Loss: 0.00001004
Iteration 71/1000 | Loss: 0.00001003
Iteration 72/1000 | Loss: 0.00001003
Iteration 73/1000 | Loss: 0.00001003
Iteration 74/1000 | Loss: 0.00001003
Iteration 75/1000 | Loss: 0.00001003
Iteration 76/1000 | Loss: 0.00001003
Iteration 77/1000 | Loss: 0.00001003
Iteration 78/1000 | Loss: 0.00001003
Iteration 79/1000 | Loss: 0.00001002
Iteration 80/1000 | Loss: 0.00001002
Iteration 81/1000 | Loss: 0.00001002
Iteration 82/1000 | Loss: 0.00001002
Iteration 83/1000 | Loss: 0.00001002
Iteration 84/1000 | Loss: 0.00001002
Iteration 85/1000 | Loss: 0.00001002
Iteration 86/1000 | Loss: 0.00001002
Iteration 87/1000 | Loss: 0.00001002
Iteration 88/1000 | Loss: 0.00001002
Iteration 89/1000 | Loss: 0.00001001
Iteration 90/1000 | Loss: 0.00001001
Iteration 91/1000 | Loss: 0.00001001
Iteration 92/1000 | Loss: 0.00001001
Iteration 93/1000 | Loss: 0.00001001
Iteration 94/1000 | Loss: 0.00001001
Iteration 95/1000 | Loss: 0.00001001
Iteration 96/1000 | Loss: 0.00001001
Iteration 97/1000 | Loss: 0.00001001
Iteration 98/1000 | Loss: 0.00001001
Iteration 99/1000 | Loss: 0.00001001
Iteration 100/1000 | Loss: 0.00001001
Iteration 101/1000 | Loss: 0.00001001
Iteration 102/1000 | Loss: 0.00001001
Iteration 103/1000 | Loss: 0.00001001
Iteration 104/1000 | Loss: 0.00001001
Iteration 105/1000 | Loss: 0.00001001
Iteration 106/1000 | Loss: 0.00001001
Iteration 107/1000 | Loss: 0.00001001
Iteration 108/1000 | Loss: 0.00001001
Iteration 109/1000 | Loss: 0.00001001
Iteration 110/1000 | Loss: 0.00001001
Iteration 111/1000 | Loss: 0.00001001
Iteration 112/1000 | Loss: 0.00001001
Iteration 113/1000 | Loss: 0.00001001
Iteration 114/1000 | Loss: 0.00001001
Iteration 115/1000 | Loss: 0.00001001
Iteration 116/1000 | Loss: 0.00001001
Iteration 117/1000 | Loss: 0.00001001
Iteration 118/1000 | Loss: 0.00001001
Iteration 119/1000 | Loss: 0.00001001
Iteration 120/1000 | Loss: 0.00001001
Iteration 121/1000 | Loss: 0.00001001
Iteration 122/1000 | Loss: 0.00001001
Iteration 123/1000 | Loss: 0.00001001
Iteration 124/1000 | Loss: 0.00001001
Iteration 125/1000 | Loss: 0.00001001
Iteration 126/1000 | Loss: 0.00001001
Iteration 127/1000 | Loss: 0.00001001
Iteration 128/1000 | Loss: 0.00001001
Iteration 129/1000 | Loss: 0.00001001
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.0008998287958093e-05, 1.0008998287958093e-05, 1.0008998287958093e-05, 1.0008998287958093e-05, 1.0008998287958093e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0008998287958093e-05

Optimization complete. Final v2v error: 2.7418718338012695 mm

Highest mean error: 3.3003761768341064 mm for frame 62

Lowest mean error: 2.6157472133636475 mm for frame 55

Saving results

Total time: 79.76758885383606
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882857
Iteration 2/25 | Loss: 0.00146745
Iteration 3/25 | Loss: 0.00109749
Iteration 4/25 | Loss: 0.00103708
Iteration 5/25 | Loss: 0.00102806
Iteration 6/25 | Loss: 0.00102649
Iteration 7/25 | Loss: 0.00102636
Iteration 8/25 | Loss: 0.00102636
Iteration 9/25 | Loss: 0.00102635
Iteration 10/25 | Loss: 0.00102635
Iteration 11/25 | Loss: 0.00102635
Iteration 12/25 | Loss: 0.00102635
Iteration 13/25 | Loss: 0.00102635
Iteration 14/25 | Loss: 0.00102635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010263466974720359, 0.0010263466974720359, 0.0010263466974720359, 0.0010263466974720359, 0.0010263466974720359]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010263466974720359

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.63311076
Iteration 2/25 | Loss: 0.00060096
Iteration 3/25 | Loss: 0.00060095
Iteration 4/25 | Loss: 0.00060095
Iteration 5/25 | Loss: 0.00060095
Iteration 6/25 | Loss: 0.00060095
Iteration 7/25 | Loss: 0.00060095
Iteration 8/25 | Loss: 0.00060095
Iteration 9/25 | Loss: 0.00060095
Iteration 10/25 | Loss: 0.00060095
Iteration 11/25 | Loss: 0.00060095
Iteration 12/25 | Loss: 0.00060095
Iteration 13/25 | Loss: 0.00060095
Iteration 14/25 | Loss: 0.00060095
Iteration 15/25 | Loss: 0.00060095
Iteration 16/25 | Loss: 0.00060095
Iteration 17/25 | Loss: 0.00060095
Iteration 18/25 | Loss: 0.00060095
Iteration 19/25 | Loss: 0.00060095
Iteration 20/25 | Loss: 0.00060095
Iteration 21/25 | Loss: 0.00060095
Iteration 22/25 | Loss: 0.00060095
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006009480566717684, 0.0006009480566717684, 0.0006009480566717684, 0.0006009480566717684, 0.0006009480566717684]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006009480566717684

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060095
Iteration 2/1000 | Loss: 0.00002893
Iteration 3/1000 | Loss: 0.00002101
Iteration 4/1000 | Loss: 0.00001919
Iteration 5/1000 | Loss: 0.00001798
Iteration 6/1000 | Loss: 0.00001732
Iteration 7/1000 | Loss: 0.00001691
Iteration 8/1000 | Loss: 0.00001665
Iteration 9/1000 | Loss: 0.00001641
Iteration 10/1000 | Loss: 0.00001627
Iteration 11/1000 | Loss: 0.00001622
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00001616
Iteration 14/1000 | Loss: 0.00001614
Iteration 15/1000 | Loss: 0.00001614
Iteration 16/1000 | Loss: 0.00001613
Iteration 17/1000 | Loss: 0.00001612
Iteration 18/1000 | Loss: 0.00001612
Iteration 19/1000 | Loss: 0.00001612
Iteration 20/1000 | Loss: 0.00001611
Iteration 21/1000 | Loss: 0.00001611
Iteration 22/1000 | Loss: 0.00001611
Iteration 23/1000 | Loss: 0.00001611
Iteration 24/1000 | Loss: 0.00001611
Iteration 25/1000 | Loss: 0.00001611
Iteration 26/1000 | Loss: 0.00001610
Iteration 27/1000 | Loss: 0.00001610
Iteration 28/1000 | Loss: 0.00001610
Iteration 29/1000 | Loss: 0.00001609
Iteration 30/1000 | Loss: 0.00001609
Iteration 31/1000 | Loss: 0.00001609
Iteration 32/1000 | Loss: 0.00001609
Iteration 33/1000 | Loss: 0.00001608
Iteration 34/1000 | Loss: 0.00001608
Iteration 35/1000 | Loss: 0.00001607
Iteration 36/1000 | Loss: 0.00001607
Iteration 37/1000 | Loss: 0.00001607
Iteration 38/1000 | Loss: 0.00001607
Iteration 39/1000 | Loss: 0.00001607
Iteration 40/1000 | Loss: 0.00001607
Iteration 41/1000 | Loss: 0.00001606
Iteration 42/1000 | Loss: 0.00001606
Iteration 43/1000 | Loss: 0.00001605
Iteration 44/1000 | Loss: 0.00001605
Iteration 45/1000 | Loss: 0.00001605
Iteration 46/1000 | Loss: 0.00001604
Iteration 47/1000 | Loss: 0.00001604
Iteration 48/1000 | Loss: 0.00001604
Iteration 49/1000 | Loss: 0.00001603
Iteration 50/1000 | Loss: 0.00001603
Iteration 51/1000 | Loss: 0.00001603
Iteration 52/1000 | Loss: 0.00001603
Iteration 53/1000 | Loss: 0.00001602
Iteration 54/1000 | Loss: 0.00001602
Iteration 55/1000 | Loss: 0.00001602
Iteration 56/1000 | Loss: 0.00001601
Iteration 57/1000 | Loss: 0.00001601
Iteration 58/1000 | Loss: 0.00001601
Iteration 59/1000 | Loss: 0.00001601
Iteration 60/1000 | Loss: 0.00001601
Iteration 61/1000 | Loss: 0.00001601
Iteration 62/1000 | Loss: 0.00001600
Iteration 63/1000 | Loss: 0.00001600
Iteration 64/1000 | Loss: 0.00001600
Iteration 65/1000 | Loss: 0.00001600
Iteration 66/1000 | Loss: 0.00001600
Iteration 67/1000 | Loss: 0.00001600
Iteration 68/1000 | Loss: 0.00001600
Iteration 69/1000 | Loss: 0.00001599
Iteration 70/1000 | Loss: 0.00001599
Iteration 71/1000 | Loss: 0.00001599
Iteration 72/1000 | Loss: 0.00001599
Iteration 73/1000 | Loss: 0.00001598
Iteration 74/1000 | Loss: 0.00001598
Iteration 75/1000 | Loss: 0.00001598
Iteration 76/1000 | Loss: 0.00001597
Iteration 77/1000 | Loss: 0.00001597
Iteration 78/1000 | Loss: 0.00001597
Iteration 79/1000 | Loss: 0.00001597
Iteration 80/1000 | Loss: 0.00001597
Iteration 81/1000 | Loss: 0.00001597
Iteration 82/1000 | Loss: 0.00001597
Iteration 83/1000 | Loss: 0.00001596
Iteration 84/1000 | Loss: 0.00001596
Iteration 85/1000 | Loss: 0.00001596
Iteration 86/1000 | Loss: 0.00001596
Iteration 87/1000 | Loss: 0.00001596
Iteration 88/1000 | Loss: 0.00001596
Iteration 89/1000 | Loss: 0.00001596
Iteration 90/1000 | Loss: 0.00001596
Iteration 91/1000 | Loss: 0.00001596
Iteration 92/1000 | Loss: 0.00001596
Iteration 93/1000 | Loss: 0.00001595
Iteration 94/1000 | Loss: 0.00001595
Iteration 95/1000 | Loss: 0.00001595
Iteration 96/1000 | Loss: 0.00001595
Iteration 97/1000 | Loss: 0.00001595
Iteration 98/1000 | Loss: 0.00001595
Iteration 99/1000 | Loss: 0.00001595
Iteration 100/1000 | Loss: 0.00001595
Iteration 101/1000 | Loss: 0.00001595
Iteration 102/1000 | Loss: 0.00001595
Iteration 103/1000 | Loss: 0.00001594
Iteration 104/1000 | Loss: 0.00001594
Iteration 105/1000 | Loss: 0.00001594
Iteration 106/1000 | Loss: 0.00001594
Iteration 107/1000 | Loss: 0.00001594
Iteration 108/1000 | Loss: 0.00001594
Iteration 109/1000 | Loss: 0.00001594
Iteration 110/1000 | Loss: 0.00001594
Iteration 111/1000 | Loss: 0.00001594
Iteration 112/1000 | Loss: 0.00001594
Iteration 113/1000 | Loss: 0.00001594
Iteration 114/1000 | Loss: 0.00001594
Iteration 115/1000 | Loss: 0.00001594
Iteration 116/1000 | Loss: 0.00001593
Iteration 117/1000 | Loss: 0.00001593
Iteration 118/1000 | Loss: 0.00001593
Iteration 119/1000 | Loss: 0.00001593
Iteration 120/1000 | Loss: 0.00001593
Iteration 121/1000 | Loss: 0.00001593
Iteration 122/1000 | Loss: 0.00001593
Iteration 123/1000 | Loss: 0.00001593
Iteration 124/1000 | Loss: 0.00001593
Iteration 125/1000 | Loss: 0.00001592
Iteration 126/1000 | Loss: 0.00001592
Iteration 127/1000 | Loss: 0.00001592
Iteration 128/1000 | Loss: 0.00001592
Iteration 129/1000 | Loss: 0.00001592
Iteration 130/1000 | Loss: 0.00001592
Iteration 131/1000 | Loss: 0.00001591
Iteration 132/1000 | Loss: 0.00001591
Iteration 133/1000 | Loss: 0.00001591
Iteration 134/1000 | Loss: 0.00001591
Iteration 135/1000 | Loss: 0.00001591
Iteration 136/1000 | Loss: 0.00001591
Iteration 137/1000 | Loss: 0.00001591
Iteration 138/1000 | Loss: 0.00001591
Iteration 139/1000 | Loss: 0.00001591
Iteration 140/1000 | Loss: 0.00001591
Iteration 141/1000 | Loss: 0.00001591
Iteration 142/1000 | Loss: 0.00001591
Iteration 143/1000 | Loss: 0.00001591
Iteration 144/1000 | Loss: 0.00001590
Iteration 145/1000 | Loss: 0.00001590
Iteration 146/1000 | Loss: 0.00001590
Iteration 147/1000 | Loss: 0.00001590
Iteration 148/1000 | Loss: 0.00001590
Iteration 149/1000 | Loss: 0.00001590
Iteration 150/1000 | Loss: 0.00001590
Iteration 151/1000 | Loss: 0.00001590
Iteration 152/1000 | Loss: 0.00001590
Iteration 153/1000 | Loss: 0.00001590
Iteration 154/1000 | Loss: 0.00001589
Iteration 155/1000 | Loss: 0.00001589
Iteration 156/1000 | Loss: 0.00001589
Iteration 157/1000 | Loss: 0.00001589
Iteration 158/1000 | Loss: 0.00001589
Iteration 159/1000 | Loss: 0.00001589
Iteration 160/1000 | Loss: 0.00001589
Iteration 161/1000 | Loss: 0.00001589
Iteration 162/1000 | Loss: 0.00001589
Iteration 163/1000 | Loss: 0.00001589
Iteration 164/1000 | Loss: 0.00001589
Iteration 165/1000 | Loss: 0.00001588
Iteration 166/1000 | Loss: 0.00001588
Iteration 167/1000 | Loss: 0.00001588
Iteration 168/1000 | Loss: 0.00001588
Iteration 169/1000 | Loss: 0.00001588
Iteration 170/1000 | Loss: 0.00001588
Iteration 171/1000 | Loss: 0.00001588
Iteration 172/1000 | Loss: 0.00001588
Iteration 173/1000 | Loss: 0.00001588
Iteration 174/1000 | Loss: 0.00001588
Iteration 175/1000 | Loss: 0.00001588
Iteration 176/1000 | Loss: 0.00001588
Iteration 177/1000 | Loss: 0.00001588
Iteration 178/1000 | Loss: 0.00001588
Iteration 179/1000 | Loss: 0.00001588
Iteration 180/1000 | Loss: 0.00001588
Iteration 181/1000 | Loss: 0.00001588
Iteration 182/1000 | Loss: 0.00001588
Iteration 183/1000 | Loss: 0.00001588
Iteration 184/1000 | Loss: 0.00001588
Iteration 185/1000 | Loss: 0.00001588
Iteration 186/1000 | Loss: 0.00001588
Iteration 187/1000 | Loss: 0.00001588
Iteration 188/1000 | Loss: 0.00001588
Iteration 189/1000 | Loss: 0.00001588
Iteration 190/1000 | Loss: 0.00001588
Iteration 191/1000 | Loss: 0.00001588
Iteration 192/1000 | Loss: 0.00001588
Iteration 193/1000 | Loss: 0.00001588
Iteration 194/1000 | Loss: 0.00001588
Iteration 195/1000 | Loss: 0.00001588
Iteration 196/1000 | Loss: 0.00001588
Iteration 197/1000 | Loss: 0.00001588
Iteration 198/1000 | Loss: 0.00001588
Iteration 199/1000 | Loss: 0.00001588
Iteration 200/1000 | Loss: 0.00001588
Iteration 201/1000 | Loss: 0.00001588
Iteration 202/1000 | Loss: 0.00001588
Iteration 203/1000 | Loss: 0.00001588
Iteration 204/1000 | Loss: 0.00001588
Iteration 205/1000 | Loss: 0.00001588
Iteration 206/1000 | Loss: 0.00001588
Iteration 207/1000 | Loss: 0.00001588
Iteration 208/1000 | Loss: 0.00001588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.5879595594014972e-05, 1.5879595594014972e-05, 1.5879595594014972e-05, 1.5879595594014972e-05, 1.5879595594014972e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5879595594014972e-05

Optimization complete. Final v2v error: 3.2475738525390625 mm

Highest mean error: 3.530982255935669 mm for frame 79

Lowest mean error: 2.8866074085235596 mm for frame 118

Saving results

Total time: 37.07470750808716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808587
Iteration 2/25 | Loss: 0.00122872
Iteration 3/25 | Loss: 0.00100905
Iteration 4/25 | Loss: 0.00098715
Iteration 5/25 | Loss: 0.00098270
Iteration 6/25 | Loss: 0.00098221
Iteration 7/25 | Loss: 0.00098221
Iteration 8/25 | Loss: 0.00098221
Iteration 9/25 | Loss: 0.00098221
Iteration 10/25 | Loss: 0.00098221
Iteration 11/25 | Loss: 0.00098221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009822137653827667, 0.0009822137653827667, 0.0009822137653827667, 0.0009822137653827667, 0.0009822137653827667]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009822137653827667

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28901577
Iteration 2/25 | Loss: 0.00064039
Iteration 3/25 | Loss: 0.00064035
Iteration 4/25 | Loss: 0.00064035
Iteration 5/25 | Loss: 0.00064035
Iteration 6/25 | Loss: 0.00064035
Iteration 7/25 | Loss: 0.00064035
Iteration 8/25 | Loss: 0.00064035
Iteration 9/25 | Loss: 0.00064035
Iteration 10/25 | Loss: 0.00064035
Iteration 11/25 | Loss: 0.00064035
Iteration 12/25 | Loss: 0.00064035
Iteration 13/25 | Loss: 0.00064035
Iteration 14/25 | Loss: 0.00064035
Iteration 15/25 | Loss: 0.00064035
Iteration 16/25 | Loss: 0.00064035
Iteration 17/25 | Loss: 0.00064035
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006403506849892437, 0.0006403506849892437, 0.0006403506849892437, 0.0006403506849892437, 0.0006403506849892437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006403506849892437

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064035
Iteration 2/1000 | Loss: 0.00002986
Iteration 3/1000 | Loss: 0.00001773
Iteration 4/1000 | Loss: 0.00001353
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001212
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001139
Iteration 9/1000 | Loss: 0.00001103
Iteration 10/1000 | Loss: 0.00001098
Iteration 11/1000 | Loss: 0.00001093
Iteration 12/1000 | Loss: 0.00001072
Iteration 13/1000 | Loss: 0.00001063
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001057
Iteration 16/1000 | Loss: 0.00001049
Iteration 17/1000 | Loss: 0.00001047
Iteration 18/1000 | Loss: 0.00001047
Iteration 19/1000 | Loss: 0.00001043
Iteration 20/1000 | Loss: 0.00001043
Iteration 21/1000 | Loss: 0.00001043
Iteration 22/1000 | Loss: 0.00001040
Iteration 23/1000 | Loss: 0.00001040
Iteration 24/1000 | Loss: 0.00001038
Iteration 25/1000 | Loss: 0.00001038
Iteration 26/1000 | Loss: 0.00001037
Iteration 27/1000 | Loss: 0.00001037
Iteration 28/1000 | Loss: 0.00001035
Iteration 29/1000 | Loss: 0.00001035
Iteration 30/1000 | Loss: 0.00001035
Iteration 31/1000 | Loss: 0.00001035
Iteration 32/1000 | Loss: 0.00001035
Iteration 33/1000 | Loss: 0.00001035
Iteration 34/1000 | Loss: 0.00001035
Iteration 35/1000 | Loss: 0.00001034
Iteration 36/1000 | Loss: 0.00001034
Iteration 37/1000 | Loss: 0.00001034
Iteration 38/1000 | Loss: 0.00001034
Iteration 39/1000 | Loss: 0.00001034
Iteration 40/1000 | Loss: 0.00001034
Iteration 41/1000 | Loss: 0.00001034
Iteration 42/1000 | Loss: 0.00001033
Iteration 43/1000 | Loss: 0.00001033
Iteration 44/1000 | Loss: 0.00001033
Iteration 45/1000 | Loss: 0.00001033
Iteration 46/1000 | Loss: 0.00001033
Iteration 47/1000 | Loss: 0.00001032
Iteration 48/1000 | Loss: 0.00001031
Iteration 49/1000 | Loss: 0.00001031
Iteration 50/1000 | Loss: 0.00001031
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001031
Iteration 54/1000 | Loss: 0.00001031
Iteration 55/1000 | Loss: 0.00001031
Iteration 56/1000 | Loss: 0.00001031
Iteration 57/1000 | Loss: 0.00001031
Iteration 58/1000 | Loss: 0.00001031
Iteration 59/1000 | Loss: 0.00001030
Iteration 60/1000 | Loss: 0.00001030
Iteration 61/1000 | Loss: 0.00001030
Iteration 62/1000 | Loss: 0.00001030
Iteration 63/1000 | Loss: 0.00001030
Iteration 64/1000 | Loss: 0.00001029
Iteration 65/1000 | Loss: 0.00001029
Iteration 66/1000 | Loss: 0.00001028
Iteration 67/1000 | Loss: 0.00001028
Iteration 68/1000 | Loss: 0.00001027
Iteration 69/1000 | Loss: 0.00001027
Iteration 70/1000 | Loss: 0.00001026
Iteration 71/1000 | Loss: 0.00001026
Iteration 72/1000 | Loss: 0.00001025
Iteration 73/1000 | Loss: 0.00001025
Iteration 74/1000 | Loss: 0.00001025
Iteration 75/1000 | Loss: 0.00001025
Iteration 76/1000 | Loss: 0.00001024
Iteration 77/1000 | Loss: 0.00001024
Iteration 78/1000 | Loss: 0.00001024
Iteration 79/1000 | Loss: 0.00001024
Iteration 80/1000 | Loss: 0.00001024
Iteration 81/1000 | Loss: 0.00001024
Iteration 82/1000 | Loss: 0.00001023
Iteration 83/1000 | Loss: 0.00001023
Iteration 84/1000 | Loss: 0.00001023
Iteration 85/1000 | Loss: 0.00001023
Iteration 86/1000 | Loss: 0.00001023
Iteration 87/1000 | Loss: 0.00001023
Iteration 88/1000 | Loss: 0.00001022
Iteration 89/1000 | Loss: 0.00001022
Iteration 90/1000 | Loss: 0.00001022
Iteration 91/1000 | Loss: 0.00001022
Iteration 92/1000 | Loss: 0.00001022
Iteration 93/1000 | Loss: 0.00001022
Iteration 94/1000 | Loss: 0.00001022
Iteration 95/1000 | Loss: 0.00001022
Iteration 96/1000 | Loss: 0.00001022
Iteration 97/1000 | Loss: 0.00001022
Iteration 98/1000 | Loss: 0.00001022
Iteration 99/1000 | Loss: 0.00001021
Iteration 100/1000 | Loss: 0.00001021
Iteration 101/1000 | Loss: 0.00001021
Iteration 102/1000 | Loss: 0.00001021
Iteration 103/1000 | Loss: 0.00001021
Iteration 104/1000 | Loss: 0.00001021
Iteration 105/1000 | Loss: 0.00001021
Iteration 106/1000 | Loss: 0.00001021
Iteration 107/1000 | Loss: 0.00001021
Iteration 108/1000 | Loss: 0.00001021
Iteration 109/1000 | Loss: 0.00001021
Iteration 110/1000 | Loss: 0.00001021
Iteration 111/1000 | Loss: 0.00001021
Iteration 112/1000 | Loss: 0.00001021
Iteration 113/1000 | Loss: 0.00001021
Iteration 114/1000 | Loss: 0.00001020
Iteration 115/1000 | Loss: 0.00001020
Iteration 116/1000 | Loss: 0.00001020
Iteration 117/1000 | Loss: 0.00001020
Iteration 118/1000 | Loss: 0.00001020
Iteration 119/1000 | Loss: 0.00001020
Iteration 120/1000 | Loss: 0.00001019
Iteration 121/1000 | Loss: 0.00001019
Iteration 122/1000 | Loss: 0.00001019
Iteration 123/1000 | Loss: 0.00001019
Iteration 124/1000 | Loss: 0.00001019
Iteration 125/1000 | Loss: 0.00001019
Iteration 126/1000 | Loss: 0.00001019
Iteration 127/1000 | Loss: 0.00001019
Iteration 128/1000 | Loss: 0.00001019
Iteration 129/1000 | Loss: 0.00001019
Iteration 130/1000 | Loss: 0.00001019
Iteration 131/1000 | Loss: 0.00001019
Iteration 132/1000 | Loss: 0.00001019
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001019
Iteration 135/1000 | Loss: 0.00001019
Iteration 136/1000 | Loss: 0.00001019
Iteration 137/1000 | Loss: 0.00001019
Iteration 138/1000 | Loss: 0.00001019
Iteration 139/1000 | Loss: 0.00001019
Iteration 140/1000 | Loss: 0.00001019
Iteration 141/1000 | Loss: 0.00001019
Iteration 142/1000 | Loss: 0.00001019
Iteration 143/1000 | Loss: 0.00001019
Iteration 144/1000 | Loss: 0.00001019
Iteration 145/1000 | Loss: 0.00001019
Iteration 146/1000 | Loss: 0.00001019
Iteration 147/1000 | Loss: 0.00001019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.0187130101257935e-05, 1.0187130101257935e-05, 1.0187130101257935e-05, 1.0187130101257935e-05, 1.0187130101257935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0187130101257935e-05

Optimization complete. Final v2v error: 2.7349069118499756 mm

Highest mean error: 3.037036180496216 mm for frame 141

Lowest mean error: 2.503859519958496 mm for frame 102

Saving results

Total time: 38.16044616699219
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030874
Iteration 2/25 | Loss: 0.01030874
Iteration 3/25 | Loss: 0.01030874
Iteration 4/25 | Loss: 0.01030874
Iteration 5/25 | Loss: 0.01030874
Iteration 6/25 | Loss: 0.00236223
Iteration 7/25 | Loss: 0.00195935
Iteration 8/25 | Loss: 0.00190272
Iteration 9/25 | Loss: 0.00184380
Iteration 10/25 | Loss: 0.00183654
Iteration 11/25 | Loss: 0.00183312
Iteration 12/25 | Loss: 0.00183147
Iteration 13/25 | Loss: 0.00183140
Iteration 14/25 | Loss: 0.00183140
Iteration 15/25 | Loss: 0.00183140
Iteration 16/25 | Loss: 0.00183140
Iteration 17/25 | Loss: 0.00183140
Iteration 18/25 | Loss: 0.00183140
Iteration 19/25 | Loss: 0.00183140
Iteration 20/25 | Loss: 0.00183140
Iteration 21/25 | Loss: 0.00183140
Iteration 22/25 | Loss: 0.00183140
Iteration 23/25 | Loss: 0.00183140
Iteration 24/25 | Loss: 0.00183140
Iteration 25/25 | Loss: 0.00183140

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30547559
Iteration 2/25 | Loss: 0.01511258
Iteration 3/25 | Loss: 0.00676186
Iteration 4/25 | Loss: 0.00677981
Iteration 5/25 | Loss: 0.00685341
Iteration 6/25 | Loss: 0.00678021
Iteration 7/25 | Loss: 0.00671388
Iteration 8/25 | Loss: 0.00671388
Iteration 9/25 | Loss: 0.00671387
Iteration 10/25 | Loss: 0.00671387
Iteration 11/25 | Loss: 0.00671387
Iteration 12/25 | Loss: 0.00671387
Iteration 13/25 | Loss: 0.00671387
Iteration 14/25 | Loss: 0.00671387
Iteration 15/25 | Loss: 0.00671387
Iteration 16/25 | Loss: 0.00671387
Iteration 17/25 | Loss: 0.00671387
Iteration 18/25 | Loss: 0.00671387
Iteration 19/25 | Loss: 0.00671387
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.006713872775435448, 0.006713872775435448, 0.006713872775435448, 0.006713872775435448, 0.006713872775435448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006713872775435448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00671387
Iteration 2/1000 | Loss: 0.00734033
Iteration 3/1000 | Loss: 0.00370213
Iteration 4/1000 | Loss: 0.00122839
Iteration 5/1000 | Loss: 0.00883783
Iteration 6/1000 | Loss: 0.00103188
Iteration 7/1000 | Loss: 0.00816606
Iteration 8/1000 | Loss: 0.00091329
Iteration 9/1000 | Loss: 0.00600605
Iteration 10/1000 | Loss: 0.00130814
Iteration 11/1000 | Loss: 0.00955634
Iteration 12/1000 | Loss: 0.00141094
Iteration 13/1000 | Loss: 0.00099005
Iteration 14/1000 | Loss: 0.00062999
Iteration 15/1000 | Loss: 0.00189680
Iteration 16/1000 | Loss: 0.00066401
Iteration 17/1000 | Loss: 0.00469516
Iteration 18/1000 | Loss: 0.00332252
Iteration 19/1000 | Loss: 0.00142216
Iteration 20/1000 | Loss: 0.00096036
Iteration 21/1000 | Loss: 0.00061473
Iteration 22/1000 | Loss: 0.00044773
Iteration 23/1000 | Loss: 0.00187623
Iteration 24/1000 | Loss: 0.01541956
Iteration 25/1000 | Loss: 0.01172467
Iteration 26/1000 | Loss: 0.00230490
Iteration 27/1000 | Loss: 0.00367597
Iteration 28/1000 | Loss: 0.00213145
Iteration 29/1000 | Loss: 0.00258278
Iteration 30/1000 | Loss: 0.00169353
Iteration 31/1000 | Loss: 0.00084888
Iteration 32/1000 | Loss: 0.00161784
Iteration 33/1000 | Loss: 0.00135645
Iteration 34/1000 | Loss: 0.00261273
Iteration 35/1000 | Loss: 0.00022874
Iteration 36/1000 | Loss: 0.00011095
Iteration 37/1000 | Loss: 0.00007444
Iteration 38/1000 | Loss: 0.00005689
Iteration 39/1000 | Loss: 0.00020271
Iteration 40/1000 | Loss: 0.00014043
Iteration 41/1000 | Loss: 0.00035553
Iteration 42/1000 | Loss: 0.00004160
Iteration 43/1000 | Loss: 0.00005055
Iteration 44/1000 | Loss: 0.00003058
Iteration 45/1000 | Loss: 0.00002610
Iteration 46/1000 | Loss: 0.00021701
Iteration 47/1000 | Loss: 0.00003541
Iteration 48/1000 | Loss: 0.00015245
Iteration 49/1000 | Loss: 0.00002182
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00012649
Iteration 52/1000 | Loss: 0.00001499
Iteration 53/1000 | Loss: 0.00001393
Iteration 54/1000 | Loss: 0.00001337
Iteration 55/1000 | Loss: 0.00001288
Iteration 56/1000 | Loss: 0.00001257
Iteration 57/1000 | Loss: 0.00001237
Iteration 58/1000 | Loss: 0.00001218
Iteration 59/1000 | Loss: 0.00001217
Iteration 60/1000 | Loss: 0.00001211
Iteration 61/1000 | Loss: 0.00001210
Iteration 62/1000 | Loss: 0.00001210
Iteration 63/1000 | Loss: 0.00001207
Iteration 64/1000 | Loss: 0.00001207
Iteration 65/1000 | Loss: 0.00001206
Iteration 66/1000 | Loss: 0.00001206
Iteration 67/1000 | Loss: 0.00001206
Iteration 68/1000 | Loss: 0.00001205
Iteration 69/1000 | Loss: 0.00001205
Iteration 70/1000 | Loss: 0.00001200
Iteration 71/1000 | Loss: 0.00001196
Iteration 72/1000 | Loss: 0.00001196
Iteration 73/1000 | Loss: 0.00001196
Iteration 74/1000 | Loss: 0.00001195
Iteration 75/1000 | Loss: 0.00001195
Iteration 76/1000 | Loss: 0.00001195
Iteration 77/1000 | Loss: 0.00001195
Iteration 78/1000 | Loss: 0.00001195
Iteration 79/1000 | Loss: 0.00001194
Iteration 80/1000 | Loss: 0.00001194
Iteration 81/1000 | Loss: 0.00001194
Iteration 82/1000 | Loss: 0.00001194
Iteration 83/1000 | Loss: 0.00001194
Iteration 84/1000 | Loss: 0.00001193
Iteration 85/1000 | Loss: 0.00001193
Iteration 86/1000 | Loss: 0.00001192
Iteration 87/1000 | Loss: 0.00001192
Iteration 88/1000 | Loss: 0.00001192
Iteration 89/1000 | Loss: 0.00001192
Iteration 90/1000 | Loss: 0.00001191
Iteration 91/1000 | Loss: 0.00001191
Iteration 92/1000 | Loss: 0.00001191
Iteration 93/1000 | Loss: 0.00001190
Iteration 94/1000 | Loss: 0.00001190
Iteration 95/1000 | Loss: 0.00001190
Iteration 96/1000 | Loss: 0.00001190
Iteration 97/1000 | Loss: 0.00001190
Iteration 98/1000 | Loss: 0.00001190
Iteration 99/1000 | Loss: 0.00001190
Iteration 100/1000 | Loss: 0.00001189
Iteration 101/1000 | Loss: 0.00001189
Iteration 102/1000 | Loss: 0.00001189
Iteration 103/1000 | Loss: 0.00001189
Iteration 104/1000 | Loss: 0.00001189
Iteration 105/1000 | Loss: 0.00001188
Iteration 106/1000 | Loss: 0.00001188
Iteration 107/1000 | Loss: 0.00001188
Iteration 108/1000 | Loss: 0.00001188
Iteration 109/1000 | Loss: 0.00001188
Iteration 110/1000 | Loss: 0.00001188
Iteration 111/1000 | Loss: 0.00001188
Iteration 112/1000 | Loss: 0.00001187
Iteration 113/1000 | Loss: 0.00001187
Iteration 114/1000 | Loss: 0.00001187
Iteration 115/1000 | Loss: 0.00001187
Iteration 116/1000 | Loss: 0.00001187
Iteration 117/1000 | Loss: 0.00001187
Iteration 118/1000 | Loss: 0.00001187
Iteration 119/1000 | Loss: 0.00001187
Iteration 120/1000 | Loss: 0.00001187
Iteration 121/1000 | Loss: 0.00001187
Iteration 122/1000 | Loss: 0.00001186
Iteration 123/1000 | Loss: 0.00001186
Iteration 124/1000 | Loss: 0.00001186
Iteration 125/1000 | Loss: 0.00001186
Iteration 126/1000 | Loss: 0.00001186
Iteration 127/1000 | Loss: 0.00001186
Iteration 128/1000 | Loss: 0.00001186
Iteration 129/1000 | Loss: 0.00001186
Iteration 130/1000 | Loss: 0.00001186
Iteration 131/1000 | Loss: 0.00001186
Iteration 132/1000 | Loss: 0.00001186
Iteration 133/1000 | Loss: 0.00001186
Iteration 134/1000 | Loss: 0.00001186
Iteration 135/1000 | Loss: 0.00001186
Iteration 136/1000 | Loss: 0.00001186
Iteration 137/1000 | Loss: 0.00001186
Iteration 138/1000 | Loss: 0.00001186
Iteration 139/1000 | Loss: 0.00001186
Iteration 140/1000 | Loss: 0.00001186
Iteration 141/1000 | Loss: 0.00001186
Iteration 142/1000 | Loss: 0.00001186
Iteration 143/1000 | Loss: 0.00001186
Iteration 144/1000 | Loss: 0.00001186
Iteration 145/1000 | Loss: 0.00001186
Iteration 146/1000 | Loss: 0.00001186
Iteration 147/1000 | Loss: 0.00001186
Iteration 148/1000 | Loss: 0.00001186
Iteration 149/1000 | Loss: 0.00001186
Iteration 150/1000 | Loss: 0.00001186
Iteration 151/1000 | Loss: 0.00001186
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.186393819807563e-05, 1.186393819807563e-05, 1.186393819807563e-05, 1.186393819807563e-05, 1.186393819807563e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.186393819807563e-05

Optimization complete. Final v2v error: 2.895672559738159 mm

Highest mean error: 3.360466957092285 mm for frame 159

Lowest mean error: 2.6962642669677734 mm for frame 179

Saving results

Total time: 122.88345241546631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01028983
Iteration 2/25 | Loss: 0.00136847
Iteration 3/25 | Loss: 0.00105022
Iteration 4/25 | Loss: 0.00099673
Iteration 5/25 | Loss: 0.00098758
Iteration 6/25 | Loss: 0.00098201
Iteration 7/25 | Loss: 0.00097749
Iteration 8/25 | Loss: 0.00098134
Iteration 9/25 | Loss: 0.00097642
Iteration 10/25 | Loss: 0.00097338
Iteration 11/25 | Loss: 0.00097243
Iteration 12/25 | Loss: 0.00097144
Iteration 13/25 | Loss: 0.00096944
Iteration 14/25 | Loss: 0.00096801
Iteration 15/25 | Loss: 0.00096618
Iteration 16/25 | Loss: 0.00096725
Iteration 17/25 | Loss: 0.00096690
Iteration 18/25 | Loss: 0.00096666
Iteration 19/25 | Loss: 0.00096788
Iteration 20/25 | Loss: 0.00096668
Iteration 21/25 | Loss: 0.00096655
Iteration 22/25 | Loss: 0.00096488
Iteration 23/25 | Loss: 0.00096294
Iteration 24/25 | Loss: 0.00096235
Iteration 25/25 | Loss: 0.00096192

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.83890414
Iteration 2/25 | Loss: 0.00083736
Iteration 3/25 | Loss: 0.00083728
Iteration 4/25 | Loss: 0.00083727
Iteration 5/25 | Loss: 0.00083727
Iteration 6/25 | Loss: 0.00083727
Iteration 7/25 | Loss: 0.00083727
Iteration 8/25 | Loss: 0.00083727
Iteration 9/25 | Loss: 0.00083727
Iteration 10/25 | Loss: 0.00083727
Iteration 11/25 | Loss: 0.00083727
Iteration 12/25 | Loss: 0.00083727
Iteration 13/25 | Loss: 0.00083727
Iteration 14/25 | Loss: 0.00083727
Iteration 15/25 | Loss: 0.00083727
Iteration 16/25 | Loss: 0.00083727
Iteration 17/25 | Loss: 0.00083727
Iteration 18/25 | Loss: 0.00083727
Iteration 19/25 | Loss: 0.00083727
Iteration 20/25 | Loss: 0.00083727
Iteration 21/25 | Loss: 0.00083727
Iteration 22/25 | Loss: 0.00083727
Iteration 23/25 | Loss: 0.00083727
Iteration 24/25 | Loss: 0.00083727
Iteration 25/25 | Loss: 0.00083727

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083727
Iteration 2/1000 | Loss: 0.00004376
Iteration 3/1000 | Loss: 0.00007129
Iteration 4/1000 | Loss: 0.00016515
Iteration 5/1000 | Loss: 0.00021402
Iteration 6/1000 | Loss: 0.00012041
Iteration 7/1000 | Loss: 0.00010861
Iteration 8/1000 | Loss: 0.00002174
Iteration 9/1000 | Loss: 0.00001736
Iteration 10/1000 | Loss: 0.00001537
Iteration 11/1000 | Loss: 0.00001462
Iteration 12/1000 | Loss: 0.00001417
Iteration 13/1000 | Loss: 0.00008770
Iteration 14/1000 | Loss: 0.00006900
Iteration 15/1000 | Loss: 0.00020007
Iteration 16/1000 | Loss: 0.00001882
Iteration 17/1000 | Loss: 0.00001450
Iteration 18/1000 | Loss: 0.00001367
Iteration 19/1000 | Loss: 0.00001285
Iteration 20/1000 | Loss: 0.00003118
Iteration 21/1000 | Loss: 0.00001235
Iteration 22/1000 | Loss: 0.00002283
Iteration 23/1000 | Loss: 0.00001216
Iteration 24/1000 | Loss: 0.00001208
Iteration 25/1000 | Loss: 0.00002033
Iteration 26/1000 | Loss: 0.00001308
Iteration 27/1000 | Loss: 0.00001177
Iteration 28/1000 | Loss: 0.00001169
Iteration 29/1000 | Loss: 0.00001169
Iteration 30/1000 | Loss: 0.00001168
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001166
Iteration 33/1000 | Loss: 0.00001163
Iteration 34/1000 | Loss: 0.00001163
Iteration 35/1000 | Loss: 0.00001163
Iteration 36/1000 | Loss: 0.00001162
Iteration 37/1000 | Loss: 0.00001162
Iteration 38/1000 | Loss: 0.00001175
Iteration 39/1000 | Loss: 0.00001161
Iteration 40/1000 | Loss: 0.00001162
Iteration 41/1000 | Loss: 0.00001162
Iteration 42/1000 | Loss: 0.00001161
Iteration 43/1000 | Loss: 0.00001159
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001158
Iteration 46/1000 | Loss: 0.00001158
Iteration 47/1000 | Loss: 0.00001156
Iteration 48/1000 | Loss: 0.00001156
Iteration 49/1000 | Loss: 0.00001155
Iteration 50/1000 | Loss: 0.00001154
Iteration 51/1000 | Loss: 0.00001153
Iteration 52/1000 | Loss: 0.00001153
Iteration 53/1000 | Loss: 0.00001153
Iteration 54/1000 | Loss: 0.00001152
Iteration 55/1000 | Loss: 0.00001152
Iteration 56/1000 | Loss: 0.00001152
Iteration 57/1000 | Loss: 0.00001149
Iteration 58/1000 | Loss: 0.00001148
Iteration 59/1000 | Loss: 0.00001148
Iteration 60/1000 | Loss: 0.00001147
Iteration 61/1000 | Loss: 0.00001145
Iteration 62/1000 | Loss: 0.00001144
Iteration 63/1000 | Loss: 0.00001144
Iteration 64/1000 | Loss: 0.00001143
Iteration 65/1000 | Loss: 0.00001143
Iteration 66/1000 | Loss: 0.00001142
Iteration 67/1000 | Loss: 0.00001142
Iteration 68/1000 | Loss: 0.00001142
Iteration 69/1000 | Loss: 0.00001141
Iteration 70/1000 | Loss: 0.00001141
Iteration 71/1000 | Loss: 0.00001140
Iteration 72/1000 | Loss: 0.00001140
Iteration 73/1000 | Loss: 0.00001140
Iteration 74/1000 | Loss: 0.00001139
Iteration 75/1000 | Loss: 0.00001139
Iteration 76/1000 | Loss: 0.00001139
Iteration 77/1000 | Loss: 0.00001138
Iteration 78/1000 | Loss: 0.00001138
Iteration 79/1000 | Loss: 0.00001138
Iteration 80/1000 | Loss: 0.00001138
Iteration 81/1000 | Loss: 0.00001137
Iteration 82/1000 | Loss: 0.00001137
Iteration 83/1000 | Loss: 0.00001137
Iteration 84/1000 | Loss: 0.00001136
Iteration 85/1000 | Loss: 0.00001136
Iteration 86/1000 | Loss: 0.00001136
Iteration 87/1000 | Loss: 0.00001136
Iteration 88/1000 | Loss: 0.00001136
Iteration 89/1000 | Loss: 0.00001136
Iteration 90/1000 | Loss: 0.00001136
Iteration 91/1000 | Loss: 0.00001136
Iteration 92/1000 | Loss: 0.00001136
Iteration 93/1000 | Loss: 0.00001136
Iteration 94/1000 | Loss: 0.00001136
Iteration 95/1000 | Loss: 0.00001136
Iteration 96/1000 | Loss: 0.00001136
Iteration 97/1000 | Loss: 0.00001136
Iteration 98/1000 | Loss: 0.00001135
Iteration 99/1000 | Loss: 0.00001135
Iteration 100/1000 | Loss: 0.00001135
Iteration 101/1000 | Loss: 0.00001135
Iteration 102/1000 | Loss: 0.00001135
Iteration 103/1000 | Loss: 0.00001135
Iteration 104/1000 | Loss: 0.00001135
Iteration 105/1000 | Loss: 0.00001135
Iteration 106/1000 | Loss: 0.00001135
Iteration 107/1000 | Loss: 0.00001135
Iteration 108/1000 | Loss: 0.00001135
Iteration 109/1000 | Loss: 0.00001135
Iteration 110/1000 | Loss: 0.00001134
Iteration 111/1000 | Loss: 0.00001134
Iteration 112/1000 | Loss: 0.00001134
Iteration 113/1000 | Loss: 0.00001134
Iteration 114/1000 | Loss: 0.00001134
Iteration 115/1000 | Loss: 0.00001134
Iteration 116/1000 | Loss: 0.00001134
Iteration 117/1000 | Loss: 0.00001134
Iteration 118/1000 | Loss: 0.00001133
Iteration 119/1000 | Loss: 0.00001133
Iteration 120/1000 | Loss: 0.00001133
Iteration 121/1000 | Loss: 0.00001133
Iteration 122/1000 | Loss: 0.00001133
Iteration 123/1000 | Loss: 0.00001133
Iteration 124/1000 | Loss: 0.00001133
Iteration 125/1000 | Loss: 0.00001133
Iteration 126/1000 | Loss: 0.00001133
Iteration 127/1000 | Loss: 0.00001133
Iteration 128/1000 | Loss: 0.00001133
Iteration 129/1000 | Loss: 0.00001133
Iteration 130/1000 | Loss: 0.00001133
Iteration 131/1000 | Loss: 0.00001133
Iteration 132/1000 | Loss: 0.00001133
Iteration 133/1000 | Loss: 0.00001132
Iteration 134/1000 | Loss: 0.00001132
Iteration 135/1000 | Loss: 0.00001132
Iteration 136/1000 | Loss: 0.00001132
Iteration 137/1000 | Loss: 0.00001132
Iteration 138/1000 | Loss: 0.00001132
Iteration 139/1000 | Loss: 0.00001132
Iteration 140/1000 | Loss: 0.00001132
Iteration 141/1000 | Loss: 0.00001132
Iteration 142/1000 | Loss: 0.00001132
Iteration 143/1000 | Loss: 0.00001132
Iteration 144/1000 | Loss: 0.00001132
Iteration 145/1000 | Loss: 0.00001132
Iteration 146/1000 | Loss: 0.00001132
Iteration 147/1000 | Loss: 0.00001132
Iteration 148/1000 | Loss: 0.00001132
Iteration 149/1000 | Loss: 0.00001131
Iteration 150/1000 | Loss: 0.00001131
Iteration 151/1000 | Loss: 0.00001131
Iteration 152/1000 | Loss: 0.00001131
Iteration 153/1000 | Loss: 0.00001131
Iteration 154/1000 | Loss: 0.00001131
Iteration 155/1000 | Loss: 0.00001131
Iteration 156/1000 | Loss: 0.00001131
Iteration 157/1000 | Loss: 0.00001131
Iteration 158/1000 | Loss: 0.00001131
Iteration 159/1000 | Loss: 0.00001131
Iteration 160/1000 | Loss: 0.00001131
Iteration 161/1000 | Loss: 0.00001131
Iteration 162/1000 | Loss: 0.00001131
Iteration 163/1000 | Loss: 0.00001131
Iteration 164/1000 | Loss: 0.00001131
Iteration 165/1000 | Loss: 0.00001131
Iteration 166/1000 | Loss: 0.00001131
Iteration 167/1000 | Loss: 0.00001131
Iteration 168/1000 | Loss: 0.00001131
Iteration 169/1000 | Loss: 0.00001131
Iteration 170/1000 | Loss: 0.00001131
Iteration 171/1000 | Loss: 0.00001131
Iteration 172/1000 | Loss: 0.00001131
Iteration 173/1000 | Loss: 0.00001131
Iteration 174/1000 | Loss: 0.00001131
Iteration 175/1000 | Loss: 0.00001131
Iteration 176/1000 | Loss: 0.00001131
Iteration 177/1000 | Loss: 0.00001131
Iteration 178/1000 | Loss: 0.00001131
Iteration 179/1000 | Loss: 0.00001131
Iteration 180/1000 | Loss: 0.00001131
Iteration 181/1000 | Loss: 0.00001131
Iteration 182/1000 | Loss: 0.00001131
Iteration 183/1000 | Loss: 0.00001131
Iteration 184/1000 | Loss: 0.00001131
Iteration 185/1000 | Loss: 0.00001131
Iteration 186/1000 | Loss: 0.00001131
Iteration 187/1000 | Loss: 0.00001131
Iteration 188/1000 | Loss: 0.00001131
Iteration 189/1000 | Loss: 0.00001131
Iteration 190/1000 | Loss: 0.00001131
Iteration 191/1000 | Loss: 0.00001131
Iteration 192/1000 | Loss: 0.00001131
Iteration 193/1000 | Loss: 0.00001131
Iteration 194/1000 | Loss: 0.00001131
Iteration 195/1000 | Loss: 0.00001131
Iteration 196/1000 | Loss: 0.00001131
Iteration 197/1000 | Loss: 0.00001131
Iteration 198/1000 | Loss: 0.00001131
Iteration 199/1000 | Loss: 0.00001131
Iteration 200/1000 | Loss: 0.00001131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.1307617569400463e-05, 1.1307617569400463e-05, 1.1307617569400463e-05, 1.1307617569400463e-05, 1.1307617569400463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1307617569400463e-05

Optimization complete. Final v2v error: 2.835975408554077 mm

Highest mean error: 3.7349417209625244 mm for frame 239

Lowest mean error: 2.4583659172058105 mm for frame 169

Saving results

Total time: 108.2085165977478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01129315
Iteration 2/25 | Loss: 0.00309085
Iteration 3/25 | Loss: 0.00233278
Iteration 4/25 | Loss: 0.00178686
Iteration 5/25 | Loss: 0.00182117
Iteration 6/25 | Loss: 0.00144072
Iteration 7/25 | Loss: 0.00131969
Iteration 8/25 | Loss: 0.00127093
Iteration 9/25 | Loss: 0.00118304
Iteration 10/25 | Loss: 0.00115692
Iteration 11/25 | Loss: 0.00114965
Iteration 12/25 | Loss: 0.00114674
Iteration 13/25 | Loss: 0.00127855
Iteration 14/25 | Loss: 0.00135031
Iteration 15/25 | Loss: 0.00111932
Iteration 16/25 | Loss: 0.00108416
Iteration 17/25 | Loss: 0.00106178
Iteration 18/25 | Loss: 0.00107696
Iteration 19/25 | Loss: 0.00106086
Iteration 20/25 | Loss: 0.00106072
Iteration 21/25 | Loss: 0.00106072
Iteration 22/25 | Loss: 0.00106072
Iteration 23/25 | Loss: 0.00106072
Iteration 24/25 | Loss: 0.00106072
Iteration 25/25 | Loss: 0.00106072

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44982433
Iteration 2/25 | Loss: 0.00165719
Iteration 3/25 | Loss: 0.00096301
Iteration 4/25 | Loss: 0.00096301
Iteration 5/25 | Loss: 0.00096301
Iteration 6/25 | Loss: 0.00096301
Iteration 7/25 | Loss: 0.00096301
Iteration 8/25 | Loss: 0.00096301
Iteration 9/25 | Loss: 0.00096301
Iteration 10/25 | Loss: 0.00096301
Iteration 11/25 | Loss: 0.00096301
Iteration 12/25 | Loss: 0.00096301
Iteration 13/25 | Loss: 0.00096301
Iteration 14/25 | Loss: 0.00096301
Iteration 15/25 | Loss: 0.00096301
Iteration 16/25 | Loss: 0.00096301
Iteration 17/25 | Loss: 0.00096301
Iteration 18/25 | Loss: 0.00096301
Iteration 19/25 | Loss: 0.00096301
Iteration 20/25 | Loss: 0.00096301
Iteration 21/25 | Loss: 0.00096301
Iteration 22/25 | Loss: 0.00096301
Iteration 23/25 | Loss: 0.00096301
Iteration 24/25 | Loss: 0.00096301
Iteration 25/25 | Loss: 0.00096301

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096301
Iteration 2/1000 | Loss: 0.01776183
Iteration 3/1000 | Loss: 0.02128204
Iteration 4/1000 | Loss: 0.01063152
Iteration 5/1000 | Loss: 0.00785867
Iteration 6/1000 | Loss: 0.00340514
Iteration 7/1000 | Loss: 0.00195302
Iteration 8/1000 | Loss: 0.00038621
Iteration 9/1000 | Loss: 0.00243760
Iteration 10/1000 | Loss: 0.00120569
Iteration 11/1000 | Loss: 0.00122179
Iteration 12/1000 | Loss: 0.00044321
Iteration 13/1000 | Loss: 0.00122497
Iteration 14/1000 | Loss: 0.00009747
Iteration 15/1000 | Loss: 0.00317208
Iteration 16/1000 | Loss: 0.00151796
Iteration 17/1000 | Loss: 0.00200541
Iteration 18/1000 | Loss: 0.00199589
Iteration 19/1000 | Loss: 0.00190714
Iteration 20/1000 | Loss: 0.00167467
Iteration 21/1000 | Loss: 0.00050257
Iteration 22/1000 | Loss: 0.00077654
Iteration 23/1000 | Loss: 0.00022300
Iteration 24/1000 | Loss: 0.00025143
Iteration 25/1000 | Loss: 0.00006936
Iteration 26/1000 | Loss: 0.00090252
Iteration 27/1000 | Loss: 0.00076703
Iteration 28/1000 | Loss: 0.00009833
Iteration 29/1000 | Loss: 0.00210665
Iteration 30/1000 | Loss: 0.00063734
Iteration 31/1000 | Loss: 0.00022014
Iteration 32/1000 | Loss: 0.00250502
Iteration 33/1000 | Loss: 0.00118464
Iteration 34/1000 | Loss: 0.00243589
Iteration 35/1000 | Loss: 0.00057957
Iteration 36/1000 | Loss: 0.00006130
Iteration 37/1000 | Loss: 0.00004983
Iteration 38/1000 | Loss: 0.00149408
Iteration 39/1000 | Loss: 0.00071980
Iteration 40/1000 | Loss: 0.00102440
Iteration 41/1000 | Loss: 0.00187608
Iteration 42/1000 | Loss: 0.00285863
Iteration 43/1000 | Loss: 0.00161244
Iteration 44/1000 | Loss: 0.00220469
Iteration 45/1000 | Loss: 0.00114300
Iteration 46/1000 | Loss: 0.00032613
Iteration 47/1000 | Loss: 0.00003780
Iteration 48/1000 | Loss: 0.00055406
Iteration 49/1000 | Loss: 0.00059154
Iteration 50/1000 | Loss: 0.00007034
Iteration 51/1000 | Loss: 0.00009734
Iteration 52/1000 | Loss: 0.00011033
Iteration 53/1000 | Loss: 0.00002433
Iteration 54/1000 | Loss: 0.00043713
Iteration 55/1000 | Loss: 0.00007413
Iteration 56/1000 | Loss: 0.00002366
Iteration 57/1000 | Loss: 0.00090922
Iteration 58/1000 | Loss: 0.00050394
Iteration 59/1000 | Loss: 0.00044719
Iteration 60/1000 | Loss: 0.00002964
Iteration 61/1000 | Loss: 0.00002306
Iteration 62/1000 | Loss: 0.00002341
Iteration 63/1000 | Loss: 0.00001596
Iteration 64/1000 | Loss: 0.00002544
Iteration 65/1000 | Loss: 0.00001359
Iteration 66/1000 | Loss: 0.00001295
Iteration 67/1000 | Loss: 0.00003039
Iteration 68/1000 | Loss: 0.00001236
Iteration 69/1000 | Loss: 0.00001204
Iteration 70/1000 | Loss: 0.00001191
Iteration 71/1000 | Loss: 0.00004324
Iteration 72/1000 | Loss: 0.00001201
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001134
Iteration 75/1000 | Loss: 0.00001131
Iteration 76/1000 | Loss: 0.00001131
Iteration 77/1000 | Loss: 0.00001130
Iteration 78/1000 | Loss: 0.00001130
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001124
Iteration 81/1000 | Loss: 0.00001121
Iteration 82/1000 | Loss: 0.00001121
Iteration 83/1000 | Loss: 0.00001120
Iteration 84/1000 | Loss: 0.00001119
Iteration 85/1000 | Loss: 0.00001119
Iteration 86/1000 | Loss: 0.00001118
Iteration 87/1000 | Loss: 0.00001117
Iteration 88/1000 | Loss: 0.00001116
Iteration 89/1000 | Loss: 0.00001115
Iteration 90/1000 | Loss: 0.00001115
Iteration 91/1000 | Loss: 0.00001115
Iteration 92/1000 | Loss: 0.00001115
Iteration 93/1000 | Loss: 0.00001115
Iteration 94/1000 | Loss: 0.00001115
Iteration 95/1000 | Loss: 0.00001115
Iteration 96/1000 | Loss: 0.00001115
Iteration 97/1000 | Loss: 0.00001114
Iteration 98/1000 | Loss: 0.00001114
Iteration 99/1000 | Loss: 0.00001114
Iteration 100/1000 | Loss: 0.00001114
Iteration 101/1000 | Loss: 0.00001114
Iteration 102/1000 | Loss: 0.00001114
Iteration 103/1000 | Loss: 0.00001114
Iteration 104/1000 | Loss: 0.00001114
Iteration 105/1000 | Loss: 0.00001114
Iteration 106/1000 | Loss: 0.00001113
Iteration 107/1000 | Loss: 0.00001113
Iteration 108/1000 | Loss: 0.00001113
Iteration 109/1000 | Loss: 0.00001113
Iteration 110/1000 | Loss: 0.00001113
Iteration 111/1000 | Loss: 0.00001113
Iteration 112/1000 | Loss: 0.00001113
Iteration 113/1000 | Loss: 0.00001113
Iteration 114/1000 | Loss: 0.00001113
Iteration 115/1000 | Loss: 0.00001113
Iteration 116/1000 | Loss: 0.00001112
Iteration 117/1000 | Loss: 0.00001112
Iteration 118/1000 | Loss: 0.00001112
Iteration 119/1000 | Loss: 0.00001112
Iteration 120/1000 | Loss: 0.00001112
Iteration 121/1000 | Loss: 0.00001112
Iteration 122/1000 | Loss: 0.00001112
Iteration 123/1000 | Loss: 0.00001112
Iteration 124/1000 | Loss: 0.00001112
Iteration 125/1000 | Loss: 0.00001112
Iteration 126/1000 | Loss: 0.00001112
Iteration 127/1000 | Loss: 0.00001111
Iteration 128/1000 | Loss: 0.00001111
Iteration 129/1000 | Loss: 0.00001111
Iteration 130/1000 | Loss: 0.00001111
Iteration 131/1000 | Loss: 0.00001111
Iteration 132/1000 | Loss: 0.00001111
Iteration 133/1000 | Loss: 0.00001111
Iteration 134/1000 | Loss: 0.00001111
Iteration 135/1000 | Loss: 0.00001111
Iteration 136/1000 | Loss: 0.00001111
Iteration 137/1000 | Loss: 0.00001111
Iteration 138/1000 | Loss: 0.00001111
Iteration 139/1000 | Loss: 0.00001111
Iteration 140/1000 | Loss: 0.00001110
Iteration 141/1000 | Loss: 0.00001110
Iteration 142/1000 | Loss: 0.00001110
Iteration 143/1000 | Loss: 0.00001109
Iteration 144/1000 | Loss: 0.00001109
Iteration 145/1000 | Loss: 0.00001109
Iteration 146/1000 | Loss: 0.00001109
Iteration 147/1000 | Loss: 0.00001108
Iteration 148/1000 | Loss: 0.00001108
Iteration 149/1000 | Loss: 0.00001108
Iteration 150/1000 | Loss: 0.00001108
Iteration 151/1000 | Loss: 0.00001108
Iteration 152/1000 | Loss: 0.00001108
Iteration 153/1000 | Loss: 0.00001108
Iteration 154/1000 | Loss: 0.00001108
Iteration 155/1000 | Loss: 0.00001108
Iteration 156/1000 | Loss: 0.00001107
Iteration 157/1000 | Loss: 0.00001107
Iteration 158/1000 | Loss: 0.00001107
Iteration 159/1000 | Loss: 0.00001106
Iteration 160/1000 | Loss: 0.00001106
Iteration 161/1000 | Loss: 0.00001106
Iteration 162/1000 | Loss: 0.00001106
Iteration 163/1000 | Loss: 0.00001106
Iteration 164/1000 | Loss: 0.00001106
Iteration 165/1000 | Loss: 0.00001106
Iteration 166/1000 | Loss: 0.00001105
Iteration 167/1000 | Loss: 0.00001105
Iteration 168/1000 | Loss: 0.00001105
Iteration 169/1000 | Loss: 0.00001105
Iteration 170/1000 | Loss: 0.00001105
Iteration 171/1000 | Loss: 0.00001105
Iteration 172/1000 | Loss: 0.00001105
Iteration 173/1000 | Loss: 0.00003385
Iteration 174/1000 | Loss: 0.00001112
Iteration 175/1000 | Loss: 0.00001105
Iteration 176/1000 | Loss: 0.00001104
Iteration 177/1000 | Loss: 0.00001104
Iteration 178/1000 | Loss: 0.00001104
Iteration 179/1000 | Loss: 0.00001104
Iteration 180/1000 | Loss: 0.00001104
Iteration 181/1000 | Loss: 0.00001104
Iteration 182/1000 | Loss: 0.00001104
Iteration 183/1000 | Loss: 0.00001104
Iteration 184/1000 | Loss: 0.00001104
Iteration 185/1000 | Loss: 0.00001104
Iteration 186/1000 | Loss: 0.00001104
Iteration 187/1000 | Loss: 0.00001104
Iteration 188/1000 | Loss: 0.00001104
Iteration 189/1000 | Loss: 0.00001104
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.1038005141017493e-05, 1.1038005141017493e-05, 1.1038005141017493e-05, 1.1038005141017493e-05, 1.1038005141017493e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1038005141017493e-05

Optimization complete. Final v2v error: 2.865633964538574 mm

Highest mean error: 3.6648178100585938 mm for frame 83

Lowest mean error: 2.6469192504882812 mm for frame 69

Saving results

Total time: 149.15344786643982
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862211
Iteration 2/25 | Loss: 0.00162099
Iteration 3/25 | Loss: 0.00111275
Iteration 4/25 | Loss: 0.00102473
Iteration 5/25 | Loss: 0.00101738
Iteration 6/25 | Loss: 0.00101712
Iteration 7/25 | Loss: 0.00101694
Iteration 8/25 | Loss: 0.00101694
Iteration 9/25 | Loss: 0.00101694
Iteration 10/25 | Loss: 0.00101694
Iteration 11/25 | Loss: 0.00101694
Iteration 12/25 | Loss: 0.00101694
Iteration 13/25 | Loss: 0.00101694
Iteration 14/25 | Loss: 0.00101694
Iteration 15/25 | Loss: 0.00101694
Iteration 16/25 | Loss: 0.00101694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010169412707909942, 0.0010169412707909942, 0.0010169412707909942, 0.0010169412707909942, 0.0010169412707909942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010169412707909942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30920899
Iteration 2/25 | Loss: 0.00049413
Iteration 3/25 | Loss: 0.00049413
Iteration 4/25 | Loss: 0.00049413
Iteration 5/25 | Loss: 0.00049413
Iteration 6/25 | Loss: 0.00049413
Iteration 7/25 | Loss: 0.00049413
Iteration 8/25 | Loss: 0.00049412
Iteration 9/25 | Loss: 0.00049412
Iteration 10/25 | Loss: 0.00049412
Iteration 11/25 | Loss: 0.00049412
Iteration 12/25 | Loss: 0.00049412
Iteration 13/25 | Loss: 0.00049412
Iteration 14/25 | Loss: 0.00049412
Iteration 15/25 | Loss: 0.00049412
Iteration 16/25 | Loss: 0.00049412
Iteration 17/25 | Loss: 0.00049412
Iteration 18/25 | Loss: 0.00049412
Iteration 19/25 | Loss: 0.00049412
Iteration 20/25 | Loss: 0.00049412
Iteration 21/25 | Loss: 0.00049412
Iteration 22/25 | Loss: 0.00049412
Iteration 23/25 | Loss: 0.00049412
Iteration 24/25 | Loss: 0.00049412
Iteration 25/25 | Loss: 0.00049412

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049412
Iteration 2/1000 | Loss: 0.00002522
Iteration 3/1000 | Loss: 0.00001975
Iteration 4/1000 | Loss: 0.00001813
Iteration 5/1000 | Loss: 0.00001673
Iteration 6/1000 | Loss: 0.00001611
Iteration 7/1000 | Loss: 0.00001554
Iteration 8/1000 | Loss: 0.00001525
Iteration 9/1000 | Loss: 0.00001506
Iteration 10/1000 | Loss: 0.00001503
Iteration 11/1000 | Loss: 0.00001499
Iteration 12/1000 | Loss: 0.00001498
Iteration 13/1000 | Loss: 0.00001497
Iteration 14/1000 | Loss: 0.00001496
Iteration 15/1000 | Loss: 0.00001485
Iteration 16/1000 | Loss: 0.00001482
Iteration 17/1000 | Loss: 0.00001481
Iteration 18/1000 | Loss: 0.00001480
Iteration 19/1000 | Loss: 0.00001480
Iteration 20/1000 | Loss: 0.00001480
Iteration 21/1000 | Loss: 0.00001480
Iteration 22/1000 | Loss: 0.00001477
Iteration 23/1000 | Loss: 0.00001477
Iteration 24/1000 | Loss: 0.00001477
Iteration 25/1000 | Loss: 0.00001477
Iteration 26/1000 | Loss: 0.00001476
Iteration 27/1000 | Loss: 0.00001476
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001472
Iteration 35/1000 | Loss: 0.00001472
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001469
Iteration 38/1000 | Loss: 0.00001469
Iteration 39/1000 | Loss: 0.00001469
Iteration 40/1000 | Loss: 0.00001468
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001468
Iteration 44/1000 | Loss: 0.00001467
Iteration 45/1000 | Loss: 0.00001466
Iteration 46/1000 | Loss: 0.00001466
Iteration 47/1000 | Loss: 0.00001466
Iteration 48/1000 | Loss: 0.00001465
Iteration 49/1000 | Loss: 0.00001465
Iteration 50/1000 | Loss: 0.00001465
Iteration 51/1000 | Loss: 0.00001465
Iteration 52/1000 | Loss: 0.00001465
Iteration 53/1000 | Loss: 0.00001465
Iteration 54/1000 | Loss: 0.00001464
Iteration 55/1000 | Loss: 0.00001463
Iteration 56/1000 | Loss: 0.00001463
Iteration 57/1000 | Loss: 0.00001463
Iteration 58/1000 | Loss: 0.00001462
Iteration 59/1000 | Loss: 0.00001462
Iteration 60/1000 | Loss: 0.00001461
Iteration 61/1000 | Loss: 0.00001461
Iteration 62/1000 | Loss: 0.00001461
Iteration 63/1000 | Loss: 0.00001460
Iteration 64/1000 | Loss: 0.00001460
Iteration 65/1000 | Loss: 0.00001460
Iteration 66/1000 | Loss: 0.00001460
Iteration 67/1000 | Loss: 0.00001460
Iteration 68/1000 | Loss: 0.00001460
Iteration 69/1000 | Loss: 0.00001459
Iteration 70/1000 | Loss: 0.00001459
Iteration 71/1000 | Loss: 0.00001459
Iteration 72/1000 | Loss: 0.00001459
Iteration 73/1000 | Loss: 0.00001459
Iteration 74/1000 | Loss: 0.00001459
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001458
Iteration 78/1000 | Loss: 0.00001458
Iteration 79/1000 | Loss: 0.00001458
Iteration 80/1000 | Loss: 0.00001458
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001457
Iteration 95/1000 | Loss: 0.00001457
Iteration 96/1000 | Loss: 0.00001457
Iteration 97/1000 | Loss: 0.00001457
Iteration 98/1000 | Loss: 0.00001457
Iteration 99/1000 | Loss: 0.00001457
Iteration 100/1000 | Loss: 0.00001457
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001456
Iteration 105/1000 | Loss: 0.00001456
Iteration 106/1000 | Loss: 0.00001456
Iteration 107/1000 | Loss: 0.00001456
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.4557561371475458e-05, 1.4557561371475458e-05, 1.4557561371475458e-05, 1.4557561371475458e-05, 1.4557561371475458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4557561371475458e-05

Optimization complete. Final v2v error: 3.106853723526001 mm

Highest mean error: 3.1604433059692383 mm for frame 170

Lowest mean error: 2.851508617401123 mm for frame 4

Saving results

Total time: 36.254849910736084
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00349195
Iteration 2/25 | Loss: 0.00112751
Iteration 3/25 | Loss: 0.00098918
Iteration 4/25 | Loss: 0.00096614
Iteration 5/25 | Loss: 0.00095924
Iteration 6/25 | Loss: 0.00095745
Iteration 7/25 | Loss: 0.00095721
Iteration 8/25 | Loss: 0.00095721
Iteration 9/25 | Loss: 0.00095721
Iteration 10/25 | Loss: 0.00095721
Iteration 11/25 | Loss: 0.00095721
Iteration 12/25 | Loss: 0.00095721
Iteration 13/25 | Loss: 0.00095721
Iteration 14/25 | Loss: 0.00095721
Iteration 15/25 | Loss: 0.00095721
Iteration 16/25 | Loss: 0.00095721
Iteration 17/25 | Loss: 0.00095721
Iteration 18/25 | Loss: 0.00095721
Iteration 19/25 | Loss: 0.00095721
Iteration 20/25 | Loss: 0.00095721
Iteration 21/25 | Loss: 0.00095721
Iteration 22/25 | Loss: 0.00095721
Iteration 23/25 | Loss: 0.00095721
Iteration 24/25 | Loss: 0.00095721
Iteration 25/25 | Loss: 0.00095721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39096403
Iteration 2/25 | Loss: 0.00091743
Iteration 3/25 | Loss: 0.00091743
Iteration 4/25 | Loss: 0.00091743
Iteration 5/25 | Loss: 0.00091743
Iteration 6/25 | Loss: 0.00091743
Iteration 7/25 | Loss: 0.00091743
Iteration 8/25 | Loss: 0.00091743
Iteration 9/25 | Loss: 0.00091743
Iteration 10/25 | Loss: 0.00091743
Iteration 11/25 | Loss: 0.00091743
Iteration 12/25 | Loss: 0.00091743
Iteration 13/25 | Loss: 0.00091743
Iteration 14/25 | Loss: 0.00091743
Iteration 15/25 | Loss: 0.00091743
Iteration 16/25 | Loss: 0.00091743
Iteration 17/25 | Loss: 0.00091743
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009174264268949628, 0.0009174264268949628, 0.0009174264268949628, 0.0009174264268949628, 0.0009174264268949628]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009174264268949628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091743
Iteration 2/1000 | Loss: 0.00004185
Iteration 3/1000 | Loss: 0.00002149
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001195
Iteration 6/1000 | Loss: 0.00001127
Iteration 7/1000 | Loss: 0.00001077
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001041
Iteration 10/1000 | Loss: 0.00001036
Iteration 11/1000 | Loss: 0.00001016
Iteration 12/1000 | Loss: 0.00001012
Iteration 13/1000 | Loss: 0.00001012
Iteration 14/1000 | Loss: 0.00001010
Iteration 15/1000 | Loss: 0.00001004
Iteration 16/1000 | Loss: 0.00001002
Iteration 17/1000 | Loss: 0.00000999
Iteration 18/1000 | Loss: 0.00000999
Iteration 19/1000 | Loss: 0.00000998
Iteration 20/1000 | Loss: 0.00000998
Iteration 21/1000 | Loss: 0.00000994
Iteration 22/1000 | Loss: 0.00000994
Iteration 23/1000 | Loss: 0.00000993
Iteration 24/1000 | Loss: 0.00000993
Iteration 25/1000 | Loss: 0.00000992
Iteration 26/1000 | Loss: 0.00000992
Iteration 27/1000 | Loss: 0.00000992
Iteration 28/1000 | Loss: 0.00000991
Iteration 29/1000 | Loss: 0.00000991
Iteration 30/1000 | Loss: 0.00000991
Iteration 31/1000 | Loss: 0.00000990
Iteration 32/1000 | Loss: 0.00000990
Iteration 33/1000 | Loss: 0.00000990
Iteration 34/1000 | Loss: 0.00000990
Iteration 35/1000 | Loss: 0.00000990
Iteration 36/1000 | Loss: 0.00000990
Iteration 37/1000 | Loss: 0.00000990
Iteration 38/1000 | Loss: 0.00000989
Iteration 39/1000 | Loss: 0.00000989
Iteration 40/1000 | Loss: 0.00000989
Iteration 41/1000 | Loss: 0.00000989
Iteration 42/1000 | Loss: 0.00000989
Iteration 43/1000 | Loss: 0.00000989
Iteration 44/1000 | Loss: 0.00000989
Iteration 45/1000 | Loss: 0.00000988
Iteration 46/1000 | Loss: 0.00000988
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000988
Iteration 49/1000 | Loss: 0.00000987
Iteration 50/1000 | Loss: 0.00000987
Iteration 51/1000 | Loss: 0.00000987
Iteration 52/1000 | Loss: 0.00000987
Iteration 53/1000 | Loss: 0.00000987
Iteration 54/1000 | Loss: 0.00000987
Iteration 55/1000 | Loss: 0.00000987
Iteration 56/1000 | Loss: 0.00000986
Iteration 57/1000 | Loss: 0.00000986
Iteration 58/1000 | Loss: 0.00000986
Iteration 59/1000 | Loss: 0.00000986
Iteration 60/1000 | Loss: 0.00000986
Iteration 61/1000 | Loss: 0.00000986
Iteration 62/1000 | Loss: 0.00000986
Iteration 63/1000 | Loss: 0.00000986
Iteration 64/1000 | Loss: 0.00000985
Iteration 65/1000 | Loss: 0.00000985
Iteration 66/1000 | Loss: 0.00000985
Iteration 67/1000 | Loss: 0.00000984
Iteration 68/1000 | Loss: 0.00000984
Iteration 69/1000 | Loss: 0.00000984
Iteration 70/1000 | Loss: 0.00000983
Iteration 71/1000 | Loss: 0.00000983
Iteration 72/1000 | Loss: 0.00000983
Iteration 73/1000 | Loss: 0.00000983
Iteration 74/1000 | Loss: 0.00000983
Iteration 75/1000 | Loss: 0.00000982
Iteration 76/1000 | Loss: 0.00000982
Iteration 77/1000 | Loss: 0.00000982
Iteration 78/1000 | Loss: 0.00000981
Iteration 79/1000 | Loss: 0.00000981
Iteration 80/1000 | Loss: 0.00000981
Iteration 81/1000 | Loss: 0.00000980
Iteration 82/1000 | Loss: 0.00000980
Iteration 83/1000 | Loss: 0.00000980
Iteration 84/1000 | Loss: 0.00000980
Iteration 85/1000 | Loss: 0.00000980
Iteration 86/1000 | Loss: 0.00000980
Iteration 87/1000 | Loss: 0.00000979
Iteration 88/1000 | Loss: 0.00000979
Iteration 89/1000 | Loss: 0.00000979
Iteration 90/1000 | Loss: 0.00000979
Iteration 91/1000 | Loss: 0.00000979
Iteration 92/1000 | Loss: 0.00000978
Iteration 93/1000 | Loss: 0.00000978
Iteration 94/1000 | Loss: 0.00000978
Iteration 95/1000 | Loss: 0.00000978
Iteration 96/1000 | Loss: 0.00000978
Iteration 97/1000 | Loss: 0.00000978
Iteration 98/1000 | Loss: 0.00000978
Iteration 99/1000 | Loss: 0.00000977
Iteration 100/1000 | Loss: 0.00000977
Iteration 101/1000 | Loss: 0.00000977
Iteration 102/1000 | Loss: 0.00000977
Iteration 103/1000 | Loss: 0.00000977
Iteration 104/1000 | Loss: 0.00000977
Iteration 105/1000 | Loss: 0.00000977
Iteration 106/1000 | Loss: 0.00000977
Iteration 107/1000 | Loss: 0.00000977
Iteration 108/1000 | Loss: 0.00000977
Iteration 109/1000 | Loss: 0.00000977
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000977
Iteration 114/1000 | Loss: 0.00000977
Iteration 115/1000 | Loss: 0.00000977
Iteration 116/1000 | Loss: 0.00000977
Iteration 117/1000 | Loss: 0.00000977
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000976
Iteration 120/1000 | Loss: 0.00000976
Iteration 121/1000 | Loss: 0.00000976
Iteration 122/1000 | Loss: 0.00000976
Iteration 123/1000 | Loss: 0.00000976
Iteration 124/1000 | Loss: 0.00000976
Iteration 125/1000 | Loss: 0.00000976
Iteration 126/1000 | Loss: 0.00000976
Iteration 127/1000 | Loss: 0.00000976
Iteration 128/1000 | Loss: 0.00000976
Iteration 129/1000 | Loss: 0.00000976
Iteration 130/1000 | Loss: 0.00000976
Iteration 131/1000 | Loss: 0.00000976
Iteration 132/1000 | Loss: 0.00000976
Iteration 133/1000 | Loss: 0.00000976
Iteration 134/1000 | Loss: 0.00000976
Iteration 135/1000 | Loss: 0.00000976
Iteration 136/1000 | Loss: 0.00000976
Iteration 137/1000 | Loss: 0.00000976
Iteration 138/1000 | Loss: 0.00000976
Iteration 139/1000 | Loss: 0.00000976
Iteration 140/1000 | Loss: 0.00000976
Iteration 141/1000 | Loss: 0.00000976
Iteration 142/1000 | Loss: 0.00000976
Iteration 143/1000 | Loss: 0.00000976
Iteration 144/1000 | Loss: 0.00000976
Iteration 145/1000 | Loss: 0.00000976
Iteration 146/1000 | Loss: 0.00000976
Iteration 147/1000 | Loss: 0.00000976
Iteration 148/1000 | Loss: 0.00000976
Iteration 149/1000 | Loss: 0.00000976
Iteration 150/1000 | Loss: 0.00000976
Iteration 151/1000 | Loss: 0.00000976
Iteration 152/1000 | Loss: 0.00000976
Iteration 153/1000 | Loss: 0.00000976
Iteration 154/1000 | Loss: 0.00000976
Iteration 155/1000 | Loss: 0.00000976
Iteration 156/1000 | Loss: 0.00000976
Iteration 157/1000 | Loss: 0.00000976
Iteration 158/1000 | Loss: 0.00000976
Iteration 159/1000 | Loss: 0.00000976
Iteration 160/1000 | Loss: 0.00000976
Iteration 161/1000 | Loss: 0.00000976
Iteration 162/1000 | Loss: 0.00000976
Iteration 163/1000 | Loss: 0.00000976
Iteration 164/1000 | Loss: 0.00000976
Iteration 165/1000 | Loss: 0.00000976
Iteration 166/1000 | Loss: 0.00000976
Iteration 167/1000 | Loss: 0.00000976
Iteration 168/1000 | Loss: 0.00000976
Iteration 169/1000 | Loss: 0.00000976
Iteration 170/1000 | Loss: 0.00000976
Iteration 171/1000 | Loss: 0.00000976
Iteration 172/1000 | Loss: 0.00000976
Iteration 173/1000 | Loss: 0.00000976
Iteration 174/1000 | Loss: 0.00000976
Iteration 175/1000 | Loss: 0.00000976
Iteration 176/1000 | Loss: 0.00000976
Iteration 177/1000 | Loss: 0.00000976
Iteration 178/1000 | Loss: 0.00000976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [9.755605788086541e-06, 9.755605788086541e-06, 9.755605788086541e-06, 9.755605788086541e-06, 9.755605788086541e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.755605788086541e-06

Optimization complete. Final v2v error: 2.68300461769104 mm

Highest mean error: 2.9009764194488525 mm for frame 128

Lowest mean error: 2.447354555130005 mm for frame 114

Saving results

Total time: 34.67295837402344
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043719
Iteration 2/25 | Loss: 0.00272295
Iteration 3/25 | Loss: 0.00166211
Iteration 4/25 | Loss: 0.00137967
Iteration 5/25 | Loss: 0.00155125
Iteration 6/25 | Loss: 0.00129705
Iteration 7/25 | Loss: 0.00118865
Iteration 8/25 | Loss: 0.00113315
Iteration 9/25 | Loss: 0.00104512
Iteration 10/25 | Loss: 0.00102057
Iteration 11/25 | Loss: 0.00099653
Iteration 12/25 | Loss: 0.00100355
Iteration 13/25 | Loss: 0.00099325
Iteration 14/25 | Loss: 0.00098493
Iteration 15/25 | Loss: 0.00097975
Iteration 16/25 | Loss: 0.00099747
Iteration 17/25 | Loss: 0.00099124
Iteration 18/25 | Loss: 0.00099163
Iteration 19/25 | Loss: 0.00098429
Iteration 20/25 | Loss: 0.00097391
Iteration 21/25 | Loss: 0.00097324
Iteration 22/25 | Loss: 0.00097600
Iteration 23/25 | Loss: 0.00097306
Iteration 24/25 | Loss: 0.00097426
Iteration 25/25 | Loss: 0.00097693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.44027984
Iteration 2/25 | Loss: 0.00103167
Iteration 3/25 | Loss: 0.00098196
Iteration 4/25 | Loss: 0.00098195
Iteration 5/25 | Loss: 0.00098195
Iteration 6/25 | Loss: 0.00098195
Iteration 7/25 | Loss: 0.00098195
Iteration 8/25 | Loss: 0.00098195
Iteration 9/25 | Loss: 0.00098195
Iteration 10/25 | Loss: 0.00098195
Iteration 11/25 | Loss: 0.00098195
Iteration 12/25 | Loss: 0.00098195
Iteration 13/25 | Loss: 0.00098195
Iteration 14/25 | Loss: 0.00098195
Iteration 15/25 | Loss: 0.00098195
Iteration 16/25 | Loss: 0.00098195
Iteration 17/25 | Loss: 0.00098195
Iteration 18/25 | Loss: 0.00098195
Iteration 19/25 | Loss: 0.00098195
Iteration 20/25 | Loss: 0.00098195
Iteration 21/25 | Loss: 0.00098195
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009819496190175414, 0.0009819496190175414, 0.0009819496190175414, 0.0009819496190175414, 0.0009819496190175414]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009819496190175414

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098195
Iteration 2/1000 | Loss: 0.00102682
Iteration 3/1000 | Loss: 0.00035248
Iteration 4/1000 | Loss: 0.00045453
Iteration 5/1000 | Loss: 0.00067388
Iteration 6/1000 | Loss: 0.00100722
Iteration 7/1000 | Loss: 0.00005395
Iteration 8/1000 | Loss: 0.00018049
Iteration 9/1000 | Loss: 0.00016495
Iteration 10/1000 | Loss: 0.00004710
Iteration 11/1000 | Loss: 0.00005446
Iteration 12/1000 | Loss: 0.00005933
Iteration 13/1000 | Loss: 0.00016307
Iteration 14/1000 | Loss: 0.00016325
Iteration 15/1000 | Loss: 0.00016353
Iteration 16/1000 | Loss: 0.00021073
Iteration 17/1000 | Loss: 0.00006097
Iteration 18/1000 | Loss: 0.00011619
Iteration 19/1000 | Loss: 0.00007453
Iteration 20/1000 | Loss: 0.00009563
Iteration 21/1000 | Loss: 0.00003374
Iteration 22/1000 | Loss: 0.00014425
Iteration 23/1000 | Loss: 0.00018005
Iteration 24/1000 | Loss: 0.00012176
Iteration 25/1000 | Loss: 0.00018650
Iteration 26/1000 | Loss: 0.00017405
Iteration 27/1000 | Loss: 0.00016260
Iteration 28/1000 | Loss: 0.00014537
Iteration 29/1000 | Loss: 0.00015432
Iteration 30/1000 | Loss: 0.00004894
Iteration 31/1000 | Loss: 0.00003388
Iteration 32/1000 | Loss: 0.00003245
Iteration 33/1000 | Loss: 0.00005756
Iteration 34/1000 | Loss: 0.00002607
Iteration 35/1000 | Loss: 0.00003149
Iteration 36/1000 | Loss: 0.00003304
Iteration 37/1000 | Loss: 0.00002368
Iteration 38/1000 | Loss: 0.00002630
Iteration 39/1000 | Loss: 0.00002075
Iteration 40/1000 | Loss: 0.00002733
Iteration 41/1000 | Loss: 0.00001991
Iteration 42/1000 | Loss: 0.00002782
Iteration 43/1000 | Loss: 0.00002015
Iteration 44/1000 | Loss: 0.00066610
Iteration 45/1000 | Loss: 0.00008954
Iteration 46/1000 | Loss: 0.00005012
Iteration 47/1000 | Loss: 0.00003054
Iteration 48/1000 | Loss: 0.00002546
Iteration 49/1000 | Loss: 0.00003978
Iteration 50/1000 | Loss: 0.00003108
Iteration 51/1000 | Loss: 0.00004111
Iteration 52/1000 | Loss: 0.00003761
Iteration 53/1000 | Loss: 0.00002214
Iteration 54/1000 | Loss: 0.00001933
Iteration 55/1000 | Loss: 0.00001830
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00002261
Iteration 58/1000 | Loss: 0.00001649
Iteration 59/1000 | Loss: 0.00003127
Iteration 60/1000 | Loss: 0.00001576
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001546
Iteration 64/1000 | Loss: 0.00001546
Iteration 65/1000 | Loss: 0.00001543
Iteration 66/1000 | Loss: 0.00001542
Iteration 67/1000 | Loss: 0.00001542
Iteration 68/1000 | Loss: 0.00001541
Iteration 69/1000 | Loss: 0.00002165
Iteration 70/1000 | Loss: 0.00001540
Iteration 71/1000 | Loss: 0.00001528
Iteration 72/1000 | Loss: 0.00001528
Iteration 73/1000 | Loss: 0.00001528
Iteration 74/1000 | Loss: 0.00001528
Iteration 75/1000 | Loss: 0.00001528
Iteration 76/1000 | Loss: 0.00001528
Iteration 77/1000 | Loss: 0.00001528
Iteration 78/1000 | Loss: 0.00001527
Iteration 79/1000 | Loss: 0.00001527
Iteration 80/1000 | Loss: 0.00001527
Iteration 81/1000 | Loss: 0.00001527
Iteration 82/1000 | Loss: 0.00001527
Iteration 83/1000 | Loss: 0.00001527
Iteration 84/1000 | Loss: 0.00001527
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001526
Iteration 87/1000 | Loss: 0.00001526
Iteration 88/1000 | Loss: 0.00001526
Iteration 89/1000 | Loss: 0.00001526
Iteration 90/1000 | Loss: 0.00001526
Iteration 91/1000 | Loss: 0.00001526
Iteration 92/1000 | Loss: 0.00001526
Iteration 93/1000 | Loss: 0.00001526
Iteration 94/1000 | Loss: 0.00001526
Iteration 95/1000 | Loss: 0.00001526
Iteration 96/1000 | Loss: 0.00001526
Iteration 97/1000 | Loss: 0.00001526
Iteration 98/1000 | Loss: 0.00001526
Iteration 99/1000 | Loss: 0.00001526
Iteration 100/1000 | Loss: 0.00001526
Iteration 101/1000 | Loss: 0.00001526
Iteration 102/1000 | Loss: 0.00001525
Iteration 103/1000 | Loss: 0.00001525
Iteration 104/1000 | Loss: 0.00001525
Iteration 105/1000 | Loss: 0.00001524
Iteration 106/1000 | Loss: 0.00001523
Iteration 107/1000 | Loss: 0.00001523
Iteration 108/1000 | Loss: 0.00001523
Iteration 109/1000 | Loss: 0.00001523
Iteration 110/1000 | Loss: 0.00001523
Iteration 111/1000 | Loss: 0.00001523
Iteration 112/1000 | Loss: 0.00001523
Iteration 113/1000 | Loss: 0.00001523
Iteration 114/1000 | Loss: 0.00001523
Iteration 115/1000 | Loss: 0.00001523
Iteration 116/1000 | Loss: 0.00001522
Iteration 117/1000 | Loss: 0.00001522
Iteration 118/1000 | Loss: 0.00001522
Iteration 119/1000 | Loss: 0.00001522
Iteration 120/1000 | Loss: 0.00001522
Iteration 121/1000 | Loss: 0.00001522
Iteration 122/1000 | Loss: 0.00001522
Iteration 123/1000 | Loss: 0.00001522
Iteration 124/1000 | Loss: 0.00001521
Iteration 125/1000 | Loss: 0.00001521
Iteration 126/1000 | Loss: 0.00001521
Iteration 127/1000 | Loss: 0.00001521
Iteration 128/1000 | Loss: 0.00001521
Iteration 129/1000 | Loss: 0.00002392
Iteration 130/1000 | Loss: 0.00001519
Iteration 131/1000 | Loss: 0.00001518
Iteration 132/1000 | Loss: 0.00001517
Iteration 133/1000 | Loss: 0.00001517
Iteration 134/1000 | Loss: 0.00001517
Iteration 135/1000 | Loss: 0.00001517
Iteration 136/1000 | Loss: 0.00001517
Iteration 137/1000 | Loss: 0.00001517
Iteration 138/1000 | Loss: 0.00001516
Iteration 139/1000 | Loss: 0.00001516
Iteration 140/1000 | Loss: 0.00001516
Iteration 141/1000 | Loss: 0.00001516
Iteration 142/1000 | Loss: 0.00001516
Iteration 143/1000 | Loss: 0.00001516
Iteration 144/1000 | Loss: 0.00001516
Iteration 145/1000 | Loss: 0.00001516
Iteration 146/1000 | Loss: 0.00001516
Iteration 147/1000 | Loss: 0.00001516
Iteration 148/1000 | Loss: 0.00001516
Iteration 149/1000 | Loss: 0.00001516
Iteration 150/1000 | Loss: 0.00001516
Iteration 151/1000 | Loss: 0.00001516
Iteration 152/1000 | Loss: 0.00001516
Iteration 153/1000 | Loss: 0.00001516
Iteration 154/1000 | Loss: 0.00001516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.5159205759118777e-05, 1.5159205759118777e-05, 1.5159205759118777e-05, 1.5159205759118777e-05, 1.5159205759118777e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5159205759118777e-05

Optimization complete. Final v2v error: 2.7452986240386963 mm

Highest mean error: 12.147749900817871 mm for frame 74

Lowest mean error: 2.1492838859558105 mm for frame 52

Saving results

Total time: 140.9446713924408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944771
Iteration 2/25 | Loss: 0.00437148
Iteration 3/25 | Loss: 0.00287488
Iteration 4/25 | Loss: 0.00248917
Iteration 5/25 | Loss: 0.00216046
Iteration 6/25 | Loss: 0.00188877
Iteration 7/25 | Loss: 0.00184157
Iteration 8/25 | Loss: 0.00182029
Iteration 9/25 | Loss: 0.00180917
Iteration 10/25 | Loss: 0.00180796
Iteration 11/25 | Loss: 0.00180509
Iteration 12/25 | Loss: 0.00179938
Iteration 13/25 | Loss: 0.00179645
Iteration 14/25 | Loss: 0.00179772
Iteration 15/25 | Loss: 0.00178161
Iteration 16/25 | Loss: 0.00177139
Iteration 17/25 | Loss: 0.00176064
Iteration 18/25 | Loss: 0.00174665
Iteration 19/25 | Loss: 0.00173343
Iteration 20/25 | Loss: 0.00172976
Iteration 21/25 | Loss: 0.00172833
Iteration 22/25 | Loss: 0.00173100
Iteration 23/25 | Loss: 0.00172551
Iteration 24/25 | Loss: 0.00172358
Iteration 25/25 | Loss: 0.00172793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31257057
Iteration 2/25 | Loss: 0.00858536
Iteration 3/25 | Loss: 0.00853352
Iteration 4/25 | Loss: 0.00853351
Iteration 5/25 | Loss: 0.00853351
Iteration 6/25 | Loss: 0.00853351
Iteration 7/25 | Loss: 0.00853351
Iteration 8/25 | Loss: 0.00853350
Iteration 9/25 | Loss: 0.00853350
Iteration 10/25 | Loss: 0.00853350
Iteration 11/25 | Loss: 0.00853350
Iteration 12/25 | Loss: 0.00853350
Iteration 13/25 | Loss: 0.00853350
Iteration 14/25 | Loss: 0.00853350
Iteration 15/25 | Loss: 0.00853350
Iteration 16/25 | Loss: 0.00853350
Iteration 17/25 | Loss: 0.00853350
Iteration 18/25 | Loss: 0.00853350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00853350106626749, 0.00853350106626749, 0.00853350106626749, 0.00853350106626749, 0.00853350106626749]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00853350106626749

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00853350
Iteration 2/1000 | Loss: 0.00071484
Iteration 3/1000 | Loss: 0.00054514
Iteration 4/1000 | Loss: 0.00044394
Iteration 5/1000 | Loss: 0.00040679
Iteration 6/1000 | Loss: 0.00039012
Iteration 7/1000 | Loss: 0.00037200
Iteration 8/1000 | Loss: 0.00035896
Iteration 9/1000 | Loss: 0.00035001
Iteration 10/1000 | Loss: 0.00034534
Iteration 11/1000 | Loss: 0.00034057
Iteration 12/1000 | Loss: 0.00033738
Iteration 13/1000 | Loss: 0.00033508
Iteration 14/1000 | Loss: 0.00283948
Iteration 15/1000 | Loss: 0.04274944
Iteration 16/1000 | Loss: 0.02463582
Iteration 17/1000 | Loss: 0.00165611
Iteration 18/1000 | Loss: 0.00159913
Iteration 19/1000 | Loss: 0.00180057
Iteration 20/1000 | Loss: 0.00093233
Iteration 21/1000 | Loss: 0.00086030
Iteration 22/1000 | Loss: 0.00066547
Iteration 23/1000 | Loss: 0.00036729
Iteration 24/1000 | Loss: 0.00030194
Iteration 25/1000 | Loss: 0.00020013
Iteration 26/1000 | Loss: 0.00018231
Iteration 27/1000 | Loss: 0.00033691
Iteration 28/1000 | Loss: 0.00050308
Iteration 29/1000 | Loss: 0.00019647
Iteration 30/1000 | Loss: 0.00055164
Iteration 31/1000 | Loss: 0.00034570
Iteration 32/1000 | Loss: 0.00046576
Iteration 33/1000 | Loss: 0.00005799
Iteration 34/1000 | Loss: 0.00004811
Iteration 35/1000 | Loss: 0.00014744
Iteration 36/1000 | Loss: 0.00005337
Iteration 37/1000 | Loss: 0.00004063
Iteration 38/1000 | Loss: 0.00003597
Iteration 39/1000 | Loss: 0.00019505
Iteration 40/1000 | Loss: 0.00003284
Iteration 41/1000 | Loss: 0.00003008
Iteration 42/1000 | Loss: 0.00002856
Iteration 43/1000 | Loss: 0.00003702
Iteration 44/1000 | Loss: 0.00002889
Iteration 45/1000 | Loss: 0.00002777
Iteration 46/1000 | Loss: 0.00002680
Iteration 47/1000 | Loss: 0.00002591
Iteration 48/1000 | Loss: 0.00002504
Iteration 49/1000 | Loss: 0.00002433
Iteration 50/1000 | Loss: 0.00002373
Iteration 51/1000 | Loss: 0.00002319
Iteration 52/1000 | Loss: 0.00002275
Iteration 53/1000 | Loss: 0.00002235
Iteration 54/1000 | Loss: 0.00002209
Iteration 55/1000 | Loss: 0.00002183
Iteration 56/1000 | Loss: 0.00002159
Iteration 57/1000 | Loss: 0.00002155
Iteration 58/1000 | Loss: 0.00002140
Iteration 59/1000 | Loss: 0.00002139
Iteration 60/1000 | Loss: 0.00002133
Iteration 61/1000 | Loss: 0.00002123
Iteration 62/1000 | Loss: 0.00002121
Iteration 63/1000 | Loss: 0.00002116
Iteration 64/1000 | Loss: 0.00002112
Iteration 65/1000 | Loss: 0.00002099
Iteration 66/1000 | Loss: 0.00002095
Iteration 67/1000 | Loss: 0.00002095
Iteration 68/1000 | Loss: 0.00002094
Iteration 69/1000 | Loss: 0.00002094
Iteration 70/1000 | Loss: 0.00002093
Iteration 71/1000 | Loss: 0.00002092
Iteration 72/1000 | Loss: 0.00002090
Iteration 73/1000 | Loss: 0.00006766
Iteration 74/1000 | Loss: 0.00003132
Iteration 75/1000 | Loss: 0.00129135
Iteration 76/1000 | Loss: 0.00048175
Iteration 77/1000 | Loss: 0.00003476
Iteration 78/1000 | Loss: 0.00002810
Iteration 79/1000 | Loss: 0.00002292
Iteration 80/1000 | Loss: 0.00001908
Iteration 81/1000 | Loss: 0.00003886
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001371
Iteration 84/1000 | Loss: 0.00002576
Iteration 85/1000 | Loss: 0.00005141
Iteration 86/1000 | Loss: 0.00002244
Iteration 87/1000 | Loss: 0.00001786
Iteration 88/1000 | Loss: 0.00001391
Iteration 89/1000 | Loss: 0.00002096
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001211
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001177
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001146
Iteration 97/1000 | Loss: 0.00001143
Iteration 98/1000 | Loss: 0.00001143
Iteration 99/1000 | Loss: 0.00001143
Iteration 100/1000 | Loss: 0.00001143
Iteration 101/1000 | Loss: 0.00001143
Iteration 102/1000 | Loss: 0.00001143
Iteration 103/1000 | Loss: 0.00001142
Iteration 104/1000 | Loss: 0.00001142
Iteration 105/1000 | Loss: 0.00001142
Iteration 106/1000 | Loss: 0.00001138
Iteration 107/1000 | Loss: 0.00001137
Iteration 108/1000 | Loss: 0.00001137
Iteration 109/1000 | Loss: 0.00001136
Iteration 110/1000 | Loss: 0.00001135
Iteration 111/1000 | Loss: 0.00001133
Iteration 112/1000 | Loss: 0.00001133
Iteration 113/1000 | Loss: 0.00001133
Iteration 114/1000 | Loss: 0.00001133
Iteration 115/1000 | Loss: 0.00001133
Iteration 116/1000 | Loss: 0.00001132
Iteration 117/1000 | Loss: 0.00001132
Iteration 118/1000 | Loss: 0.00001132
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001131
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001129
Iteration 125/1000 | Loss: 0.00001129
Iteration 126/1000 | Loss: 0.00001129
Iteration 127/1000 | Loss: 0.00001128
Iteration 128/1000 | Loss: 0.00001127
Iteration 129/1000 | Loss: 0.00001127
Iteration 130/1000 | Loss: 0.00001127
Iteration 131/1000 | Loss: 0.00001126
Iteration 132/1000 | Loss: 0.00001126
Iteration 133/1000 | Loss: 0.00001126
Iteration 134/1000 | Loss: 0.00001123
Iteration 135/1000 | Loss: 0.00001121
Iteration 136/1000 | Loss: 0.00001121
Iteration 137/1000 | Loss: 0.00001121
Iteration 138/1000 | Loss: 0.00001121
Iteration 139/1000 | Loss: 0.00001121
Iteration 140/1000 | Loss: 0.00001121
Iteration 141/1000 | Loss: 0.00001121
Iteration 142/1000 | Loss: 0.00001121
Iteration 143/1000 | Loss: 0.00001120
Iteration 144/1000 | Loss: 0.00001120
Iteration 145/1000 | Loss: 0.00001120
Iteration 146/1000 | Loss: 0.00001120
Iteration 147/1000 | Loss: 0.00001120
Iteration 148/1000 | Loss: 0.00001120
Iteration 149/1000 | Loss: 0.00001120
Iteration 150/1000 | Loss: 0.00001120
Iteration 151/1000 | Loss: 0.00001119
Iteration 152/1000 | Loss: 0.00001119
Iteration 153/1000 | Loss: 0.00001117
Iteration 154/1000 | Loss: 0.00001117
Iteration 155/1000 | Loss: 0.00001117
Iteration 156/1000 | Loss: 0.00001117
Iteration 157/1000 | Loss: 0.00001117
Iteration 158/1000 | Loss: 0.00001117
Iteration 159/1000 | Loss: 0.00001115
Iteration 160/1000 | Loss: 0.00001115
Iteration 161/1000 | Loss: 0.00001115
Iteration 162/1000 | Loss: 0.00001114
Iteration 163/1000 | Loss: 0.00001114
Iteration 164/1000 | Loss: 0.00001113
Iteration 165/1000 | Loss: 0.00001113
Iteration 166/1000 | Loss: 0.00001113
Iteration 167/1000 | Loss: 0.00001113
Iteration 168/1000 | Loss: 0.00001113
Iteration 169/1000 | Loss: 0.00001112
Iteration 170/1000 | Loss: 0.00001112
Iteration 171/1000 | Loss: 0.00001110
Iteration 172/1000 | Loss: 0.00001109
Iteration 173/1000 | Loss: 0.00001109
Iteration 174/1000 | Loss: 0.00001109
Iteration 175/1000 | Loss: 0.00001109
Iteration 176/1000 | Loss: 0.00001108
Iteration 177/1000 | Loss: 0.00001108
Iteration 178/1000 | Loss: 0.00001107
Iteration 179/1000 | Loss: 0.00001107
Iteration 180/1000 | Loss: 0.00001106
Iteration 181/1000 | Loss: 0.00001106
Iteration 182/1000 | Loss: 0.00001106
Iteration 183/1000 | Loss: 0.00001106
Iteration 184/1000 | Loss: 0.00001106
Iteration 185/1000 | Loss: 0.00001106
Iteration 186/1000 | Loss: 0.00001106
Iteration 187/1000 | Loss: 0.00001106
Iteration 188/1000 | Loss: 0.00001106
Iteration 189/1000 | Loss: 0.00001105
Iteration 190/1000 | Loss: 0.00001105
Iteration 191/1000 | Loss: 0.00001105
Iteration 192/1000 | Loss: 0.00001105
Iteration 193/1000 | Loss: 0.00001105
Iteration 194/1000 | Loss: 0.00001105
Iteration 195/1000 | Loss: 0.00001105
Iteration 196/1000 | Loss: 0.00001104
Iteration 197/1000 | Loss: 0.00001104
Iteration 198/1000 | Loss: 0.00001104
Iteration 199/1000 | Loss: 0.00001104
Iteration 200/1000 | Loss: 0.00001104
Iteration 201/1000 | Loss: 0.00001104
Iteration 202/1000 | Loss: 0.00001104
Iteration 203/1000 | Loss: 0.00001104
Iteration 204/1000 | Loss: 0.00001104
Iteration 205/1000 | Loss: 0.00001104
Iteration 206/1000 | Loss: 0.00001104
Iteration 207/1000 | Loss: 0.00001104
Iteration 208/1000 | Loss: 0.00001103
Iteration 209/1000 | Loss: 0.00001103
Iteration 210/1000 | Loss: 0.00001103
Iteration 211/1000 | Loss: 0.00001103
Iteration 212/1000 | Loss: 0.00001103
Iteration 213/1000 | Loss: 0.00001103
Iteration 214/1000 | Loss: 0.00001103
Iteration 215/1000 | Loss: 0.00001102
Iteration 216/1000 | Loss: 0.00001102
Iteration 217/1000 | Loss: 0.00001102
Iteration 218/1000 | Loss: 0.00001102
Iteration 219/1000 | Loss: 0.00001102
Iteration 220/1000 | Loss: 0.00001102
Iteration 221/1000 | Loss: 0.00001102
Iteration 222/1000 | Loss: 0.00001101
Iteration 223/1000 | Loss: 0.00001101
Iteration 224/1000 | Loss: 0.00001101
Iteration 225/1000 | Loss: 0.00001101
Iteration 226/1000 | Loss: 0.00001101
Iteration 227/1000 | Loss: 0.00001101
Iteration 228/1000 | Loss: 0.00001101
Iteration 229/1000 | Loss: 0.00001101
Iteration 230/1000 | Loss: 0.00001101
Iteration 231/1000 | Loss: 0.00001101
Iteration 232/1000 | Loss: 0.00001101
Iteration 233/1000 | Loss: 0.00001100
Iteration 234/1000 | Loss: 0.00001100
Iteration 235/1000 | Loss: 0.00001100
Iteration 236/1000 | Loss: 0.00001100
Iteration 237/1000 | Loss: 0.00001100
Iteration 238/1000 | Loss: 0.00001100
Iteration 239/1000 | Loss: 0.00001100
Iteration 240/1000 | Loss: 0.00001100
Iteration 241/1000 | Loss: 0.00001100
Iteration 242/1000 | Loss: 0.00001100
Iteration 243/1000 | Loss: 0.00001100
Iteration 244/1000 | Loss: 0.00001100
Iteration 245/1000 | Loss: 0.00001100
Iteration 246/1000 | Loss: 0.00001100
Iteration 247/1000 | Loss: 0.00001100
Iteration 248/1000 | Loss: 0.00001100
Iteration 249/1000 | Loss: 0.00001100
Iteration 250/1000 | Loss: 0.00001100
Iteration 251/1000 | Loss: 0.00001100
Iteration 252/1000 | Loss: 0.00001099
Iteration 253/1000 | Loss: 0.00001099
Iteration 254/1000 | Loss: 0.00001099
Iteration 255/1000 | Loss: 0.00001099
Iteration 256/1000 | Loss: 0.00001099
Iteration 257/1000 | Loss: 0.00001099
Iteration 258/1000 | Loss: 0.00001099
Iteration 259/1000 | Loss: 0.00001099
Iteration 260/1000 | Loss: 0.00001099
Iteration 261/1000 | Loss: 0.00001099
Iteration 262/1000 | Loss: 0.00001099
Iteration 263/1000 | Loss: 0.00001099
Iteration 264/1000 | Loss: 0.00001099
Iteration 265/1000 | Loss: 0.00001099
Iteration 266/1000 | Loss: 0.00001099
Iteration 267/1000 | Loss: 0.00001099
Iteration 268/1000 | Loss: 0.00001099
Iteration 269/1000 | Loss: 0.00001099
Iteration 270/1000 | Loss: 0.00001099
Iteration 271/1000 | Loss: 0.00001099
Iteration 272/1000 | Loss: 0.00001099
Iteration 273/1000 | Loss: 0.00001098
Iteration 274/1000 | Loss: 0.00001098
Iteration 275/1000 | Loss: 0.00001098
Iteration 276/1000 | Loss: 0.00001098
Iteration 277/1000 | Loss: 0.00001098
Iteration 278/1000 | Loss: 0.00001098
Iteration 279/1000 | Loss: 0.00001098
Iteration 280/1000 | Loss: 0.00001098
Iteration 281/1000 | Loss: 0.00001098
Iteration 282/1000 | Loss: 0.00001098
Iteration 283/1000 | Loss: 0.00001098
Iteration 284/1000 | Loss: 0.00001098
Iteration 285/1000 | Loss: 0.00001098
Iteration 286/1000 | Loss: 0.00001098
Iteration 287/1000 | Loss: 0.00001098
Iteration 288/1000 | Loss: 0.00001098
Iteration 289/1000 | Loss: 0.00001098
Iteration 290/1000 | Loss: 0.00001098
Iteration 291/1000 | Loss: 0.00001098
Iteration 292/1000 | Loss: 0.00001098
Iteration 293/1000 | Loss: 0.00001098
Iteration 294/1000 | Loss: 0.00001098
Iteration 295/1000 | Loss: 0.00001098
Iteration 296/1000 | Loss: 0.00001098
Iteration 297/1000 | Loss: 0.00001098
Iteration 298/1000 | Loss: 0.00001098
Iteration 299/1000 | Loss: 0.00001098
Iteration 300/1000 | Loss: 0.00001098
Iteration 301/1000 | Loss: 0.00001098
Iteration 302/1000 | Loss: 0.00001098
Iteration 303/1000 | Loss: 0.00001098
Iteration 304/1000 | Loss: 0.00001098
Iteration 305/1000 | Loss: 0.00001098
Iteration 306/1000 | Loss: 0.00001098
Iteration 307/1000 | Loss: 0.00001098
Iteration 308/1000 | Loss: 0.00001098
Iteration 309/1000 | Loss: 0.00001098
Iteration 310/1000 | Loss: 0.00001098
Iteration 311/1000 | Loss: 0.00001098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 311. Stopping optimization.
Last 5 losses: [1.0982137609971687e-05, 1.0982137609971687e-05, 1.0982137609971687e-05, 1.0982137609971687e-05, 1.0982137609971687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0982137609971687e-05

Optimization complete. Final v2v error: 2.5635063648223877 mm

Highest mean error: 11.216084480285645 mm for frame 21

Lowest mean error: 2.4090347290039062 mm for frame 1

Saving results

Total time: 182.08959126472473
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00606839
Iteration 2/25 | Loss: 0.00109813
Iteration 3/25 | Loss: 0.00099753
Iteration 4/25 | Loss: 0.00097722
Iteration 5/25 | Loss: 0.00097272
Iteration 6/25 | Loss: 0.00097234
Iteration 7/25 | Loss: 0.00097234
Iteration 8/25 | Loss: 0.00097234
Iteration 9/25 | Loss: 0.00097234
Iteration 10/25 | Loss: 0.00097234
Iteration 11/25 | Loss: 0.00097234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009723437833599746, 0.0009723437833599746, 0.0009723437833599746, 0.0009723437833599746, 0.0009723437833599746]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009723437833599746

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12754655
Iteration 2/25 | Loss: 0.00065794
Iteration 3/25 | Loss: 0.00065794
Iteration 4/25 | Loss: 0.00065794
Iteration 5/25 | Loss: 0.00065794
Iteration 6/25 | Loss: 0.00065794
Iteration 7/25 | Loss: 0.00065794
Iteration 8/25 | Loss: 0.00065794
Iteration 9/25 | Loss: 0.00065794
Iteration 10/25 | Loss: 0.00065794
Iteration 11/25 | Loss: 0.00065794
Iteration 12/25 | Loss: 0.00065794
Iteration 13/25 | Loss: 0.00065794
Iteration 14/25 | Loss: 0.00065794
Iteration 15/25 | Loss: 0.00065794
Iteration 16/25 | Loss: 0.00065794
Iteration 17/25 | Loss: 0.00065794
Iteration 18/25 | Loss: 0.00065794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006579377222806215, 0.0006579377222806215, 0.0006579377222806215, 0.0006579377222806215, 0.0006579377222806215]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006579377222806215

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065794
Iteration 2/1000 | Loss: 0.00002504
Iteration 3/1000 | Loss: 0.00001983
Iteration 4/1000 | Loss: 0.00001853
Iteration 5/1000 | Loss: 0.00001750
Iteration 6/1000 | Loss: 0.00001694
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001630
Iteration 9/1000 | Loss: 0.00001622
Iteration 10/1000 | Loss: 0.00001604
Iteration 11/1000 | Loss: 0.00001603
Iteration 12/1000 | Loss: 0.00001603
Iteration 13/1000 | Loss: 0.00001603
Iteration 14/1000 | Loss: 0.00001602
Iteration 15/1000 | Loss: 0.00001602
Iteration 16/1000 | Loss: 0.00001601
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001598
Iteration 19/1000 | Loss: 0.00001596
Iteration 20/1000 | Loss: 0.00001596
Iteration 21/1000 | Loss: 0.00001596
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00001595
Iteration 24/1000 | Loss: 0.00001593
Iteration 25/1000 | Loss: 0.00001592
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001591
Iteration 28/1000 | Loss: 0.00001591
Iteration 29/1000 | Loss: 0.00001591
Iteration 30/1000 | Loss: 0.00001590
Iteration 31/1000 | Loss: 0.00001590
Iteration 32/1000 | Loss: 0.00001589
Iteration 33/1000 | Loss: 0.00001589
Iteration 34/1000 | Loss: 0.00001588
Iteration 35/1000 | Loss: 0.00001588
Iteration 36/1000 | Loss: 0.00001588
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001587
Iteration 39/1000 | Loss: 0.00001587
Iteration 40/1000 | Loss: 0.00001587
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001585
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001585
Iteration 48/1000 | Loss: 0.00001585
Iteration 49/1000 | Loss: 0.00001585
Iteration 50/1000 | Loss: 0.00001585
Iteration 51/1000 | Loss: 0.00001585
Iteration 52/1000 | Loss: 0.00001584
Iteration 53/1000 | Loss: 0.00001584
Iteration 54/1000 | Loss: 0.00001584
Iteration 55/1000 | Loss: 0.00001584
Iteration 56/1000 | Loss: 0.00001584
Iteration 57/1000 | Loss: 0.00001584
Iteration 58/1000 | Loss: 0.00001584
Iteration 59/1000 | Loss: 0.00001584
Iteration 60/1000 | Loss: 0.00001584
Iteration 61/1000 | Loss: 0.00001584
Iteration 62/1000 | Loss: 0.00001583
Iteration 63/1000 | Loss: 0.00001583
Iteration 64/1000 | Loss: 0.00001583
Iteration 65/1000 | Loss: 0.00001583
Iteration 66/1000 | Loss: 0.00001583
Iteration 67/1000 | Loss: 0.00001583
Iteration 68/1000 | Loss: 0.00001583
Iteration 69/1000 | Loss: 0.00001583
Iteration 70/1000 | Loss: 0.00001583
Iteration 71/1000 | Loss: 0.00001583
Iteration 72/1000 | Loss: 0.00001583
Iteration 73/1000 | Loss: 0.00001583
Iteration 74/1000 | Loss: 0.00001583
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001583
Iteration 77/1000 | Loss: 0.00001583
Iteration 78/1000 | Loss: 0.00001583
Iteration 79/1000 | Loss: 0.00001582
Iteration 80/1000 | Loss: 0.00001582
Iteration 81/1000 | Loss: 0.00001582
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001582
Iteration 84/1000 | Loss: 0.00001582
Iteration 85/1000 | Loss: 0.00001582
Iteration 86/1000 | Loss: 0.00001582
Iteration 87/1000 | Loss: 0.00001582
Iteration 88/1000 | Loss: 0.00001582
Iteration 89/1000 | Loss: 0.00001581
Iteration 90/1000 | Loss: 0.00001581
Iteration 91/1000 | Loss: 0.00001581
Iteration 92/1000 | Loss: 0.00001581
Iteration 93/1000 | Loss: 0.00001581
Iteration 94/1000 | Loss: 0.00001581
Iteration 95/1000 | Loss: 0.00001581
Iteration 96/1000 | Loss: 0.00001581
Iteration 97/1000 | Loss: 0.00001581
Iteration 98/1000 | Loss: 0.00001581
Iteration 99/1000 | Loss: 0.00001581
Iteration 100/1000 | Loss: 0.00001581
Iteration 101/1000 | Loss: 0.00001581
Iteration 102/1000 | Loss: 0.00001581
Iteration 103/1000 | Loss: 0.00001581
Iteration 104/1000 | Loss: 0.00001581
Iteration 105/1000 | Loss: 0.00001581
Iteration 106/1000 | Loss: 0.00001581
Iteration 107/1000 | Loss: 0.00001581
Iteration 108/1000 | Loss: 0.00001581
Iteration 109/1000 | Loss: 0.00001581
Iteration 110/1000 | Loss: 0.00001581
Iteration 111/1000 | Loss: 0.00001581
Iteration 112/1000 | Loss: 0.00001581
Iteration 113/1000 | Loss: 0.00001581
Iteration 114/1000 | Loss: 0.00001581
Iteration 115/1000 | Loss: 0.00001581
Iteration 116/1000 | Loss: 0.00001581
Iteration 117/1000 | Loss: 0.00001581
Iteration 118/1000 | Loss: 0.00001581
Iteration 119/1000 | Loss: 0.00001581
Iteration 120/1000 | Loss: 0.00001581
Iteration 121/1000 | Loss: 0.00001581
Iteration 122/1000 | Loss: 0.00001581
Iteration 123/1000 | Loss: 0.00001581
Iteration 124/1000 | Loss: 0.00001581
Iteration 125/1000 | Loss: 0.00001581
Iteration 126/1000 | Loss: 0.00001581
Iteration 127/1000 | Loss: 0.00001581
Iteration 128/1000 | Loss: 0.00001581
Iteration 129/1000 | Loss: 0.00001581
Iteration 130/1000 | Loss: 0.00001581
Iteration 131/1000 | Loss: 0.00001581
Iteration 132/1000 | Loss: 0.00001581
Iteration 133/1000 | Loss: 0.00001581
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [1.5809388060006313e-05, 1.5809388060006313e-05, 1.5809388060006313e-05, 1.5809388060006313e-05, 1.5809388060006313e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5809388060006313e-05

Optimization complete. Final v2v error: 3.3465328216552734 mm

Highest mean error: 3.5487120151519775 mm for frame 94

Lowest mean error: 3.1492207050323486 mm for frame 77

Saving results

Total time: 28.75021505355835
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763249
Iteration 2/25 | Loss: 0.00181316
Iteration 3/25 | Loss: 0.00121142
Iteration 4/25 | Loss: 0.00105919
Iteration 5/25 | Loss: 0.00104239
Iteration 6/25 | Loss: 0.00100297
Iteration 7/25 | Loss: 0.00099795
Iteration 8/25 | Loss: 0.00099303
Iteration 9/25 | Loss: 0.00099132
Iteration 10/25 | Loss: 0.00099004
Iteration 11/25 | Loss: 0.00098662
Iteration 12/25 | Loss: 0.00098572
Iteration 13/25 | Loss: 0.00098554
Iteration 14/25 | Loss: 0.00098552
Iteration 15/25 | Loss: 0.00098552
Iteration 16/25 | Loss: 0.00098552
Iteration 17/25 | Loss: 0.00098552
Iteration 18/25 | Loss: 0.00098552
Iteration 19/25 | Loss: 0.00098552
Iteration 20/25 | Loss: 0.00098552
Iteration 21/25 | Loss: 0.00098552
Iteration 22/25 | Loss: 0.00098552
Iteration 23/25 | Loss: 0.00098552
Iteration 24/25 | Loss: 0.00098552
Iteration 25/25 | Loss: 0.00098552

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30569935
Iteration 2/25 | Loss: 0.00058548
Iteration 3/25 | Loss: 0.00058547
Iteration 4/25 | Loss: 0.00058547
Iteration 5/25 | Loss: 0.00058547
Iteration 6/25 | Loss: 0.00058547
Iteration 7/25 | Loss: 0.00058547
Iteration 8/25 | Loss: 0.00058547
Iteration 9/25 | Loss: 0.00058547
Iteration 10/25 | Loss: 0.00058547
Iteration 11/25 | Loss: 0.00058547
Iteration 12/25 | Loss: 0.00058547
Iteration 13/25 | Loss: 0.00058547
Iteration 14/25 | Loss: 0.00058547
Iteration 15/25 | Loss: 0.00058547
Iteration 16/25 | Loss: 0.00058547
Iteration 17/25 | Loss: 0.00058547
Iteration 18/25 | Loss: 0.00058547
Iteration 19/25 | Loss: 0.00058547
Iteration 20/25 | Loss: 0.00058547
Iteration 21/25 | Loss: 0.00058547
Iteration 22/25 | Loss: 0.00058547
Iteration 23/25 | Loss: 0.00058547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0005854662740603089, 0.0005854662740603089, 0.0005854662740603089, 0.0005854662740603089, 0.0005854662740603089]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005854662740603089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058547
Iteration 2/1000 | Loss: 0.00002483
Iteration 3/1000 | Loss: 0.00001738
Iteration 4/1000 | Loss: 0.00001476
Iteration 5/1000 | Loss: 0.00001410
Iteration 6/1000 | Loss: 0.00001352
Iteration 7/1000 | Loss: 0.00001316
Iteration 8/1000 | Loss: 0.00001313
Iteration 9/1000 | Loss: 0.00001287
Iteration 10/1000 | Loss: 0.00001280
Iteration 11/1000 | Loss: 0.00001266
Iteration 12/1000 | Loss: 0.00001265
Iteration 13/1000 | Loss: 0.00001264
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001257
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001256
Iteration 19/1000 | Loss: 0.00001254
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001254
Iteration 22/1000 | Loss: 0.00001254
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001253
Iteration 25/1000 | Loss: 0.00001253
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001252
Iteration 29/1000 | Loss: 0.00001252
Iteration 30/1000 | Loss: 0.00001252
Iteration 31/1000 | Loss: 0.00001252
Iteration 32/1000 | Loss: 0.00001252
Iteration 33/1000 | Loss: 0.00001252
Iteration 34/1000 | Loss: 0.00001252
Iteration 35/1000 | Loss: 0.00001252
Iteration 36/1000 | Loss: 0.00001252
Iteration 37/1000 | Loss: 0.00001252
Iteration 38/1000 | Loss: 0.00001251
Iteration 39/1000 | Loss: 0.00001251
Iteration 40/1000 | Loss: 0.00001251
Iteration 41/1000 | Loss: 0.00001251
Iteration 42/1000 | Loss: 0.00001251
Iteration 43/1000 | Loss: 0.00001251
Iteration 44/1000 | Loss: 0.00001251
Iteration 45/1000 | Loss: 0.00001251
Iteration 46/1000 | Loss: 0.00001251
Iteration 47/1000 | Loss: 0.00001250
Iteration 48/1000 | Loss: 0.00001250
Iteration 49/1000 | Loss: 0.00001250
Iteration 50/1000 | Loss: 0.00001250
Iteration 51/1000 | Loss: 0.00001250
Iteration 52/1000 | Loss: 0.00001250
Iteration 53/1000 | Loss: 0.00001250
Iteration 54/1000 | Loss: 0.00001248
Iteration 55/1000 | Loss: 0.00001247
Iteration 56/1000 | Loss: 0.00001247
Iteration 57/1000 | Loss: 0.00001247
Iteration 58/1000 | Loss: 0.00001246
Iteration 59/1000 | Loss: 0.00001246
Iteration 60/1000 | Loss: 0.00001246
Iteration 61/1000 | Loss: 0.00001246
Iteration 62/1000 | Loss: 0.00001245
Iteration 63/1000 | Loss: 0.00001245
Iteration 64/1000 | Loss: 0.00001245
Iteration 65/1000 | Loss: 0.00001245
Iteration 66/1000 | Loss: 0.00001244
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001244
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001242
Iteration 77/1000 | Loss: 0.00001242
Iteration 78/1000 | Loss: 0.00001242
Iteration 79/1000 | Loss: 0.00001241
Iteration 80/1000 | Loss: 0.00001241
Iteration 81/1000 | Loss: 0.00001241
Iteration 82/1000 | Loss: 0.00001241
Iteration 83/1000 | Loss: 0.00001240
Iteration 84/1000 | Loss: 0.00001240
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001240
Iteration 88/1000 | Loss: 0.00001240
Iteration 89/1000 | Loss: 0.00001240
Iteration 90/1000 | Loss: 0.00001240
Iteration 91/1000 | Loss: 0.00001240
Iteration 92/1000 | Loss: 0.00001240
Iteration 93/1000 | Loss: 0.00001240
Iteration 94/1000 | Loss: 0.00001240
Iteration 95/1000 | Loss: 0.00001240
Iteration 96/1000 | Loss: 0.00001240
Iteration 97/1000 | Loss: 0.00001240
Iteration 98/1000 | Loss: 0.00001239
Iteration 99/1000 | Loss: 0.00001239
Iteration 100/1000 | Loss: 0.00001239
Iteration 101/1000 | Loss: 0.00001239
Iteration 102/1000 | Loss: 0.00001239
Iteration 103/1000 | Loss: 0.00001239
Iteration 104/1000 | Loss: 0.00001239
Iteration 105/1000 | Loss: 0.00001238
Iteration 106/1000 | Loss: 0.00001238
Iteration 107/1000 | Loss: 0.00001238
Iteration 108/1000 | Loss: 0.00001238
Iteration 109/1000 | Loss: 0.00001238
Iteration 110/1000 | Loss: 0.00001238
Iteration 111/1000 | Loss: 0.00001238
Iteration 112/1000 | Loss: 0.00001238
Iteration 113/1000 | Loss: 0.00001238
Iteration 114/1000 | Loss: 0.00001238
Iteration 115/1000 | Loss: 0.00001238
Iteration 116/1000 | Loss: 0.00001238
Iteration 117/1000 | Loss: 0.00001237
Iteration 118/1000 | Loss: 0.00001237
Iteration 119/1000 | Loss: 0.00001237
Iteration 120/1000 | Loss: 0.00001237
Iteration 121/1000 | Loss: 0.00001237
Iteration 122/1000 | Loss: 0.00001237
Iteration 123/1000 | Loss: 0.00001237
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.2374837751849554e-05, 1.2374837751849554e-05, 1.2374837751849554e-05, 1.2374837751849554e-05, 1.2374837751849554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2374837751849554e-05

Optimization complete. Final v2v error: 2.965733528137207 mm

Highest mean error: 3.8710060119628906 mm for frame 234

Lowest mean error: 2.5903565883636475 mm for frame 192

Saving results

Total time: 51.431785345077515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882426
Iteration 2/25 | Loss: 0.00123313
Iteration 3/25 | Loss: 0.00104477
Iteration 4/25 | Loss: 0.00101728
Iteration 5/25 | Loss: 0.00100631
Iteration 6/25 | Loss: 0.00100332
Iteration 7/25 | Loss: 0.00100228
Iteration 8/25 | Loss: 0.00100213
Iteration 9/25 | Loss: 0.00100213
Iteration 10/25 | Loss: 0.00100213
Iteration 11/25 | Loss: 0.00100213
Iteration 12/25 | Loss: 0.00100213
Iteration 13/25 | Loss: 0.00100213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010021273046731949, 0.0010021273046731949, 0.0010021273046731949, 0.0010021273046731949, 0.0010021273046731949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010021273046731949

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48844850
Iteration 2/25 | Loss: 0.00078687
Iteration 3/25 | Loss: 0.00078687
Iteration 4/25 | Loss: 0.00078687
Iteration 5/25 | Loss: 0.00078687
Iteration 6/25 | Loss: 0.00078687
Iteration 7/25 | Loss: 0.00078687
Iteration 8/25 | Loss: 0.00078687
Iteration 9/25 | Loss: 0.00078687
Iteration 10/25 | Loss: 0.00078687
Iteration 11/25 | Loss: 0.00078687
Iteration 12/25 | Loss: 0.00078687
Iteration 13/25 | Loss: 0.00078687
Iteration 14/25 | Loss: 0.00078687
Iteration 15/25 | Loss: 0.00078687
Iteration 16/25 | Loss: 0.00078687
Iteration 17/25 | Loss: 0.00078687
Iteration 18/25 | Loss: 0.00078687
Iteration 19/25 | Loss: 0.00078687
Iteration 20/25 | Loss: 0.00078687
Iteration 21/25 | Loss: 0.00078687
Iteration 22/25 | Loss: 0.00078687
Iteration 23/25 | Loss: 0.00078687
Iteration 24/25 | Loss: 0.00078687
Iteration 25/25 | Loss: 0.00078687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078687
Iteration 2/1000 | Loss: 0.00005036
Iteration 3/1000 | Loss: 0.00003177
Iteration 4/1000 | Loss: 0.00002486
Iteration 5/1000 | Loss: 0.00002256
Iteration 6/1000 | Loss: 0.00002137
Iteration 7/1000 | Loss: 0.00002046
Iteration 8/1000 | Loss: 0.00002005
Iteration 9/1000 | Loss: 0.00001970
Iteration 10/1000 | Loss: 0.00001935
Iteration 11/1000 | Loss: 0.00001913
Iteration 12/1000 | Loss: 0.00001906
Iteration 13/1000 | Loss: 0.00001891
Iteration 14/1000 | Loss: 0.00001887
Iteration 15/1000 | Loss: 0.00001883
Iteration 16/1000 | Loss: 0.00001881
Iteration 17/1000 | Loss: 0.00001881
Iteration 18/1000 | Loss: 0.00001878
Iteration 19/1000 | Loss: 0.00001876
Iteration 20/1000 | Loss: 0.00001875
Iteration 21/1000 | Loss: 0.00001874
Iteration 22/1000 | Loss: 0.00001874
Iteration 23/1000 | Loss: 0.00001874
Iteration 24/1000 | Loss: 0.00001874
Iteration 25/1000 | Loss: 0.00001874
Iteration 26/1000 | Loss: 0.00001873
Iteration 27/1000 | Loss: 0.00001873
Iteration 28/1000 | Loss: 0.00001873
Iteration 29/1000 | Loss: 0.00001873
Iteration 30/1000 | Loss: 0.00001873
Iteration 31/1000 | Loss: 0.00001873
Iteration 32/1000 | Loss: 0.00001872
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001872
Iteration 36/1000 | Loss: 0.00001871
Iteration 37/1000 | Loss: 0.00001871
Iteration 38/1000 | Loss: 0.00001869
Iteration 39/1000 | Loss: 0.00001869
Iteration 40/1000 | Loss: 0.00001869
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001868
Iteration 45/1000 | Loss: 0.00001868
Iteration 46/1000 | Loss: 0.00001868
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001867
Iteration 50/1000 | Loss: 0.00001867
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001866
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001866
Iteration 55/1000 | Loss: 0.00001865
Iteration 56/1000 | Loss: 0.00001865
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001863
Iteration 67/1000 | Loss: 0.00001863
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001862
Iteration 74/1000 | Loss: 0.00001862
Iteration 75/1000 | Loss: 0.00001862
Iteration 76/1000 | Loss: 0.00001862
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001861
Iteration 81/1000 | Loss: 0.00001861
Iteration 82/1000 | Loss: 0.00001860
Iteration 83/1000 | Loss: 0.00001860
Iteration 84/1000 | Loss: 0.00001860
Iteration 85/1000 | Loss: 0.00001859
Iteration 86/1000 | Loss: 0.00001859
Iteration 87/1000 | Loss: 0.00001859
Iteration 88/1000 | Loss: 0.00001859
Iteration 89/1000 | Loss: 0.00001859
Iteration 90/1000 | Loss: 0.00001859
Iteration 91/1000 | Loss: 0.00001858
Iteration 92/1000 | Loss: 0.00001858
Iteration 93/1000 | Loss: 0.00001858
Iteration 94/1000 | Loss: 0.00001858
Iteration 95/1000 | Loss: 0.00001858
Iteration 96/1000 | Loss: 0.00001858
Iteration 97/1000 | Loss: 0.00001858
Iteration 98/1000 | Loss: 0.00001858
Iteration 99/1000 | Loss: 0.00001857
Iteration 100/1000 | Loss: 0.00001857
Iteration 101/1000 | Loss: 0.00001857
Iteration 102/1000 | Loss: 0.00001857
Iteration 103/1000 | Loss: 0.00001857
Iteration 104/1000 | Loss: 0.00001857
Iteration 105/1000 | Loss: 0.00001857
Iteration 106/1000 | Loss: 0.00001857
Iteration 107/1000 | Loss: 0.00001856
Iteration 108/1000 | Loss: 0.00001856
Iteration 109/1000 | Loss: 0.00001856
Iteration 110/1000 | Loss: 0.00001856
Iteration 111/1000 | Loss: 0.00001856
Iteration 112/1000 | Loss: 0.00001856
Iteration 113/1000 | Loss: 0.00001855
Iteration 114/1000 | Loss: 0.00001855
Iteration 115/1000 | Loss: 0.00001855
Iteration 116/1000 | Loss: 0.00001855
Iteration 117/1000 | Loss: 0.00001855
Iteration 118/1000 | Loss: 0.00001855
Iteration 119/1000 | Loss: 0.00001855
Iteration 120/1000 | Loss: 0.00001855
Iteration 121/1000 | Loss: 0.00001855
Iteration 122/1000 | Loss: 0.00001855
Iteration 123/1000 | Loss: 0.00001855
Iteration 124/1000 | Loss: 0.00001854
Iteration 125/1000 | Loss: 0.00001854
Iteration 126/1000 | Loss: 0.00001854
Iteration 127/1000 | Loss: 0.00001854
Iteration 128/1000 | Loss: 0.00001854
Iteration 129/1000 | Loss: 0.00001854
Iteration 130/1000 | Loss: 0.00001854
Iteration 131/1000 | Loss: 0.00001854
Iteration 132/1000 | Loss: 0.00001854
Iteration 133/1000 | Loss: 0.00001854
Iteration 134/1000 | Loss: 0.00001853
Iteration 135/1000 | Loss: 0.00001853
Iteration 136/1000 | Loss: 0.00001853
Iteration 137/1000 | Loss: 0.00001853
Iteration 138/1000 | Loss: 0.00001853
Iteration 139/1000 | Loss: 0.00001853
Iteration 140/1000 | Loss: 0.00001853
Iteration 141/1000 | Loss: 0.00001852
Iteration 142/1000 | Loss: 0.00001852
Iteration 143/1000 | Loss: 0.00001852
Iteration 144/1000 | Loss: 0.00001852
Iteration 145/1000 | Loss: 0.00001852
Iteration 146/1000 | Loss: 0.00001852
Iteration 147/1000 | Loss: 0.00001852
Iteration 148/1000 | Loss: 0.00001852
Iteration 149/1000 | Loss: 0.00001852
Iteration 150/1000 | Loss: 0.00001852
Iteration 151/1000 | Loss: 0.00001852
Iteration 152/1000 | Loss: 0.00001852
Iteration 153/1000 | Loss: 0.00001852
Iteration 154/1000 | Loss: 0.00001852
Iteration 155/1000 | Loss: 0.00001852
Iteration 156/1000 | Loss: 0.00001852
Iteration 157/1000 | Loss: 0.00001852
Iteration 158/1000 | Loss: 0.00001852
Iteration 159/1000 | Loss: 0.00001852
Iteration 160/1000 | Loss: 0.00001851
Iteration 161/1000 | Loss: 0.00001851
Iteration 162/1000 | Loss: 0.00001851
Iteration 163/1000 | Loss: 0.00001851
Iteration 164/1000 | Loss: 0.00001851
Iteration 165/1000 | Loss: 0.00001851
Iteration 166/1000 | Loss: 0.00001851
Iteration 167/1000 | Loss: 0.00001851
Iteration 168/1000 | Loss: 0.00001851
Iteration 169/1000 | Loss: 0.00001851
Iteration 170/1000 | Loss: 0.00001851
Iteration 171/1000 | Loss: 0.00001851
Iteration 172/1000 | Loss: 0.00001851
Iteration 173/1000 | Loss: 0.00001851
Iteration 174/1000 | Loss: 0.00001851
Iteration 175/1000 | Loss: 0.00001851
Iteration 176/1000 | Loss: 0.00001851
Iteration 177/1000 | Loss: 0.00001851
Iteration 178/1000 | Loss: 0.00001851
Iteration 179/1000 | Loss: 0.00001851
Iteration 180/1000 | Loss: 0.00001851
Iteration 181/1000 | Loss: 0.00001851
Iteration 182/1000 | Loss: 0.00001851
Iteration 183/1000 | Loss: 0.00001851
Iteration 184/1000 | Loss: 0.00001851
Iteration 185/1000 | Loss: 0.00001851
Iteration 186/1000 | Loss: 0.00001851
Iteration 187/1000 | Loss: 0.00001851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.8505537809687667e-05, 1.8505537809687667e-05, 1.8505537809687667e-05, 1.8505537809687667e-05, 1.8505537809687667e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8505537809687667e-05

Optimization complete. Final v2v error: 3.4959230422973633 mm

Highest mean error: 5.138786792755127 mm for frame 100

Lowest mean error: 2.306509017944336 mm for frame 15

Saving results

Total time: 41.72053503990173
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00506766
Iteration 2/25 | Loss: 0.00123256
Iteration 3/25 | Loss: 0.00103815
Iteration 4/25 | Loss: 0.00101986
Iteration 5/25 | Loss: 0.00101210
Iteration 6/25 | Loss: 0.00101028
Iteration 7/25 | Loss: 0.00101028
Iteration 8/25 | Loss: 0.00101028
Iteration 9/25 | Loss: 0.00101028
Iteration 10/25 | Loss: 0.00101028
Iteration 11/25 | Loss: 0.00101028
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001010275911539793, 0.001010275911539793, 0.001010275911539793, 0.001010275911539793, 0.001010275911539793]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001010275911539793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.61142558
Iteration 2/25 | Loss: 0.00068176
Iteration 3/25 | Loss: 0.00068176
Iteration 4/25 | Loss: 0.00068176
Iteration 5/25 | Loss: 0.00068176
Iteration 6/25 | Loss: 0.00068176
Iteration 7/25 | Loss: 0.00068175
Iteration 8/25 | Loss: 0.00068175
Iteration 9/25 | Loss: 0.00068175
Iteration 10/25 | Loss: 0.00068175
Iteration 11/25 | Loss: 0.00068175
Iteration 12/25 | Loss: 0.00068175
Iteration 13/25 | Loss: 0.00068175
Iteration 14/25 | Loss: 0.00068175
Iteration 15/25 | Loss: 0.00068175
Iteration 16/25 | Loss: 0.00068175
Iteration 17/25 | Loss: 0.00068175
Iteration 18/25 | Loss: 0.00068175
Iteration 19/25 | Loss: 0.00068175
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006817536195740104, 0.0006817536195740104, 0.0006817536195740104, 0.0006817536195740104, 0.0006817536195740104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006817536195740104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068175
Iteration 2/1000 | Loss: 0.00004072
Iteration 3/1000 | Loss: 0.00003072
Iteration 4/1000 | Loss: 0.00002806
Iteration 5/1000 | Loss: 0.00002632
Iteration 6/1000 | Loss: 0.00002566
Iteration 7/1000 | Loss: 0.00002497
Iteration 8/1000 | Loss: 0.00002452
Iteration 9/1000 | Loss: 0.00002422
Iteration 10/1000 | Loss: 0.00002393
Iteration 11/1000 | Loss: 0.00002373
Iteration 12/1000 | Loss: 0.00002355
Iteration 13/1000 | Loss: 0.00002330
Iteration 14/1000 | Loss: 0.00002309
Iteration 15/1000 | Loss: 0.00002307
Iteration 16/1000 | Loss: 0.00002289
Iteration 17/1000 | Loss: 0.00002278
Iteration 18/1000 | Loss: 0.00002262
Iteration 19/1000 | Loss: 0.00002251
Iteration 20/1000 | Loss: 0.00002244
Iteration 21/1000 | Loss: 0.00002224
Iteration 22/1000 | Loss: 0.00002206
Iteration 23/1000 | Loss: 0.00002189
Iteration 24/1000 | Loss: 0.00002182
Iteration 25/1000 | Loss: 0.00002182
Iteration 26/1000 | Loss: 0.00002171
Iteration 27/1000 | Loss: 0.00002167
Iteration 28/1000 | Loss: 0.00002164
Iteration 29/1000 | Loss: 0.00002164
Iteration 30/1000 | Loss: 0.00002163
Iteration 31/1000 | Loss: 0.00002163
Iteration 32/1000 | Loss: 0.00002162
Iteration 33/1000 | Loss: 0.00002162
Iteration 34/1000 | Loss: 0.00002161
Iteration 35/1000 | Loss: 0.00002161
Iteration 36/1000 | Loss: 0.00002161
Iteration 37/1000 | Loss: 0.00002161
Iteration 38/1000 | Loss: 0.00002161
Iteration 39/1000 | Loss: 0.00002161
Iteration 40/1000 | Loss: 0.00002161
Iteration 41/1000 | Loss: 0.00002161
Iteration 42/1000 | Loss: 0.00002161
Iteration 43/1000 | Loss: 0.00002161
Iteration 44/1000 | Loss: 0.00002161
Iteration 45/1000 | Loss: 0.00002160
Iteration 46/1000 | Loss: 0.00002160
Iteration 47/1000 | Loss: 0.00002159
Iteration 48/1000 | Loss: 0.00002158
Iteration 49/1000 | Loss: 0.00002157
Iteration 50/1000 | Loss: 0.00002156
Iteration 51/1000 | Loss: 0.00002155
Iteration 52/1000 | Loss: 0.00002155
Iteration 53/1000 | Loss: 0.00002152
Iteration 54/1000 | Loss: 0.00002152
Iteration 55/1000 | Loss: 0.00002151
Iteration 56/1000 | Loss: 0.00002150
Iteration 57/1000 | Loss: 0.00002150
Iteration 58/1000 | Loss: 0.00002150
Iteration 59/1000 | Loss: 0.00002149
Iteration 60/1000 | Loss: 0.00002148
Iteration 61/1000 | Loss: 0.00002146
Iteration 62/1000 | Loss: 0.00002146
Iteration 63/1000 | Loss: 0.00002146
Iteration 64/1000 | Loss: 0.00002146
Iteration 65/1000 | Loss: 0.00002146
Iteration 66/1000 | Loss: 0.00002145
Iteration 67/1000 | Loss: 0.00002145
Iteration 68/1000 | Loss: 0.00002145
Iteration 69/1000 | Loss: 0.00002145
Iteration 70/1000 | Loss: 0.00002144
Iteration 71/1000 | Loss: 0.00002144
Iteration 72/1000 | Loss: 0.00002144
Iteration 73/1000 | Loss: 0.00002144
Iteration 74/1000 | Loss: 0.00002143
Iteration 75/1000 | Loss: 0.00002143
Iteration 76/1000 | Loss: 0.00002143
Iteration 77/1000 | Loss: 0.00002142
Iteration 78/1000 | Loss: 0.00002142
Iteration 79/1000 | Loss: 0.00002142
Iteration 80/1000 | Loss: 0.00002142
Iteration 81/1000 | Loss: 0.00002142
Iteration 82/1000 | Loss: 0.00002142
Iteration 83/1000 | Loss: 0.00002141
Iteration 84/1000 | Loss: 0.00002141
Iteration 85/1000 | Loss: 0.00002141
Iteration 86/1000 | Loss: 0.00002141
Iteration 87/1000 | Loss: 0.00002141
Iteration 88/1000 | Loss: 0.00002141
Iteration 89/1000 | Loss: 0.00002140
Iteration 90/1000 | Loss: 0.00002140
Iteration 91/1000 | Loss: 0.00002140
Iteration 92/1000 | Loss: 0.00002140
Iteration 93/1000 | Loss: 0.00002140
Iteration 94/1000 | Loss: 0.00002140
Iteration 95/1000 | Loss: 0.00002140
Iteration 96/1000 | Loss: 0.00002140
Iteration 97/1000 | Loss: 0.00002140
Iteration 98/1000 | Loss: 0.00002140
Iteration 99/1000 | Loss: 0.00002139
Iteration 100/1000 | Loss: 0.00002139
Iteration 101/1000 | Loss: 0.00002139
Iteration 102/1000 | Loss: 0.00002139
Iteration 103/1000 | Loss: 0.00002139
Iteration 104/1000 | Loss: 0.00002139
Iteration 105/1000 | Loss: 0.00002139
Iteration 106/1000 | Loss: 0.00002139
Iteration 107/1000 | Loss: 0.00002139
Iteration 108/1000 | Loss: 0.00002139
Iteration 109/1000 | Loss: 0.00002139
Iteration 110/1000 | Loss: 0.00002139
Iteration 111/1000 | Loss: 0.00002139
Iteration 112/1000 | Loss: 0.00002138
Iteration 113/1000 | Loss: 0.00002138
Iteration 114/1000 | Loss: 0.00002138
Iteration 115/1000 | Loss: 0.00002138
Iteration 116/1000 | Loss: 0.00002138
Iteration 117/1000 | Loss: 0.00002138
Iteration 118/1000 | Loss: 0.00002138
Iteration 119/1000 | Loss: 0.00002138
Iteration 120/1000 | Loss: 0.00002138
Iteration 121/1000 | Loss: 0.00002138
Iteration 122/1000 | Loss: 0.00002138
Iteration 123/1000 | Loss: 0.00002138
Iteration 124/1000 | Loss: 0.00002138
Iteration 125/1000 | Loss: 0.00002137
Iteration 126/1000 | Loss: 0.00002137
Iteration 127/1000 | Loss: 0.00002137
Iteration 128/1000 | Loss: 0.00002137
Iteration 129/1000 | Loss: 0.00002137
Iteration 130/1000 | Loss: 0.00002137
Iteration 131/1000 | Loss: 0.00002137
Iteration 132/1000 | Loss: 0.00002137
Iteration 133/1000 | Loss: 0.00002137
Iteration 134/1000 | Loss: 0.00002137
Iteration 135/1000 | Loss: 0.00002137
Iteration 136/1000 | Loss: 0.00002137
Iteration 137/1000 | Loss: 0.00002137
Iteration 138/1000 | Loss: 0.00002137
Iteration 139/1000 | Loss: 0.00002137
Iteration 140/1000 | Loss: 0.00002137
Iteration 141/1000 | Loss: 0.00002137
Iteration 142/1000 | Loss: 0.00002137
Iteration 143/1000 | Loss: 0.00002137
Iteration 144/1000 | Loss: 0.00002137
Iteration 145/1000 | Loss: 0.00002136
Iteration 146/1000 | Loss: 0.00002136
Iteration 147/1000 | Loss: 0.00002136
Iteration 148/1000 | Loss: 0.00002136
Iteration 149/1000 | Loss: 0.00002136
Iteration 150/1000 | Loss: 0.00002136
Iteration 151/1000 | Loss: 0.00002136
Iteration 152/1000 | Loss: 0.00002136
Iteration 153/1000 | Loss: 0.00002136
Iteration 154/1000 | Loss: 0.00002136
Iteration 155/1000 | Loss: 0.00002136
Iteration 156/1000 | Loss: 0.00002136
Iteration 157/1000 | Loss: 0.00002136
Iteration 158/1000 | Loss: 0.00002136
Iteration 159/1000 | Loss: 0.00002136
Iteration 160/1000 | Loss: 0.00002136
Iteration 161/1000 | Loss: 0.00002136
Iteration 162/1000 | Loss: 0.00002135
Iteration 163/1000 | Loss: 0.00002135
Iteration 164/1000 | Loss: 0.00002135
Iteration 165/1000 | Loss: 0.00002135
Iteration 166/1000 | Loss: 0.00002135
Iteration 167/1000 | Loss: 0.00002135
Iteration 168/1000 | Loss: 0.00002135
Iteration 169/1000 | Loss: 0.00002135
Iteration 170/1000 | Loss: 0.00002135
Iteration 171/1000 | Loss: 0.00002135
Iteration 172/1000 | Loss: 0.00002135
Iteration 173/1000 | Loss: 0.00002135
Iteration 174/1000 | Loss: 0.00002135
Iteration 175/1000 | Loss: 0.00002135
Iteration 176/1000 | Loss: 0.00002135
Iteration 177/1000 | Loss: 0.00002135
Iteration 178/1000 | Loss: 0.00002135
Iteration 179/1000 | Loss: 0.00002135
Iteration 180/1000 | Loss: 0.00002134
Iteration 181/1000 | Loss: 0.00002134
Iteration 182/1000 | Loss: 0.00002134
Iteration 183/1000 | Loss: 0.00002134
Iteration 184/1000 | Loss: 0.00002134
Iteration 185/1000 | Loss: 0.00002134
Iteration 186/1000 | Loss: 0.00002134
Iteration 187/1000 | Loss: 0.00002134
Iteration 188/1000 | Loss: 0.00002134
Iteration 189/1000 | Loss: 0.00002134
Iteration 190/1000 | Loss: 0.00002134
Iteration 191/1000 | Loss: 0.00002134
Iteration 192/1000 | Loss: 0.00002134
Iteration 193/1000 | Loss: 0.00002134
Iteration 194/1000 | Loss: 0.00002134
Iteration 195/1000 | Loss: 0.00002133
Iteration 196/1000 | Loss: 0.00002133
Iteration 197/1000 | Loss: 0.00002133
Iteration 198/1000 | Loss: 0.00002133
Iteration 199/1000 | Loss: 0.00002133
Iteration 200/1000 | Loss: 0.00002133
Iteration 201/1000 | Loss: 0.00002133
Iteration 202/1000 | Loss: 0.00002133
Iteration 203/1000 | Loss: 0.00002133
Iteration 204/1000 | Loss: 0.00002133
Iteration 205/1000 | Loss: 0.00002133
Iteration 206/1000 | Loss: 0.00002133
Iteration 207/1000 | Loss: 0.00002133
Iteration 208/1000 | Loss: 0.00002133
Iteration 209/1000 | Loss: 0.00002133
Iteration 210/1000 | Loss: 0.00002133
Iteration 211/1000 | Loss: 0.00002133
Iteration 212/1000 | Loss: 0.00002132
Iteration 213/1000 | Loss: 0.00002132
Iteration 214/1000 | Loss: 0.00002132
Iteration 215/1000 | Loss: 0.00002132
Iteration 216/1000 | Loss: 0.00002132
Iteration 217/1000 | Loss: 0.00002132
Iteration 218/1000 | Loss: 0.00002132
Iteration 219/1000 | Loss: 0.00002132
Iteration 220/1000 | Loss: 0.00002132
Iteration 221/1000 | Loss: 0.00002132
Iteration 222/1000 | Loss: 0.00002132
Iteration 223/1000 | Loss: 0.00002132
Iteration 224/1000 | Loss: 0.00002132
Iteration 225/1000 | Loss: 0.00002132
Iteration 226/1000 | Loss: 0.00002132
Iteration 227/1000 | Loss: 0.00002132
Iteration 228/1000 | Loss: 0.00002132
Iteration 229/1000 | Loss: 0.00002132
Iteration 230/1000 | Loss: 0.00002132
Iteration 231/1000 | Loss: 0.00002132
Iteration 232/1000 | Loss: 0.00002132
Iteration 233/1000 | Loss: 0.00002132
Iteration 234/1000 | Loss: 0.00002132
Iteration 235/1000 | Loss: 0.00002132
Iteration 236/1000 | Loss: 0.00002132
Iteration 237/1000 | Loss: 0.00002132
Iteration 238/1000 | Loss: 0.00002132
Iteration 239/1000 | Loss: 0.00002132
Iteration 240/1000 | Loss: 0.00002132
Iteration 241/1000 | Loss: 0.00002132
Iteration 242/1000 | Loss: 0.00002132
Iteration 243/1000 | Loss: 0.00002132
Iteration 244/1000 | Loss: 0.00002132
Iteration 245/1000 | Loss: 0.00002132
Iteration 246/1000 | Loss: 0.00002132
Iteration 247/1000 | Loss: 0.00002132
Iteration 248/1000 | Loss: 0.00002132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 248. Stopping optimization.
Last 5 losses: [2.1324403860489838e-05, 2.1324403860489838e-05, 2.1324403860489838e-05, 2.1324403860489838e-05, 2.1324403860489838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1324403860489838e-05

Optimization complete. Final v2v error: 3.7481298446655273 mm

Highest mean error: 3.9174747467041016 mm for frame 240

Lowest mean error: 3.607583999633789 mm for frame 1

Saving results

Total time: 65.49016070365906
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437470
Iteration 2/25 | Loss: 0.00108549
Iteration 3/25 | Loss: 0.00096616
Iteration 4/25 | Loss: 0.00095179
Iteration 5/25 | Loss: 0.00094778
Iteration 6/25 | Loss: 0.00094648
Iteration 7/25 | Loss: 0.00094648
Iteration 8/25 | Loss: 0.00094646
Iteration 9/25 | Loss: 0.00094646
Iteration 10/25 | Loss: 0.00094646
Iteration 11/25 | Loss: 0.00094646
Iteration 12/25 | Loss: 0.00094646
Iteration 13/25 | Loss: 0.00094646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0009464629110880196, 0.0009464629110880196, 0.0009464629110880196, 0.0009464629110880196, 0.0009464629110880196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009464629110880196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30582631
Iteration 2/25 | Loss: 0.00075810
Iteration 3/25 | Loss: 0.00075810
Iteration 4/25 | Loss: 0.00075810
Iteration 5/25 | Loss: 0.00075810
Iteration 6/25 | Loss: 0.00075810
Iteration 7/25 | Loss: 0.00075810
Iteration 8/25 | Loss: 0.00075810
Iteration 9/25 | Loss: 0.00075810
Iteration 10/25 | Loss: 0.00075810
Iteration 11/25 | Loss: 0.00075810
Iteration 12/25 | Loss: 0.00075810
Iteration 13/25 | Loss: 0.00075810
Iteration 14/25 | Loss: 0.00075810
Iteration 15/25 | Loss: 0.00075810
Iteration 16/25 | Loss: 0.00075810
Iteration 17/25 | Loss: 0.00075810
Iteration 18/25 | Loss: 0.00075810
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000758101639803499, 0.000758101639803499, 0.000758101639803499, 0.000758101639803499, 0.000758101639803499]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000758101639803499

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075810
Iteration 2/1000 | Loss: 0.00002460
Iteration 3/1000 | Loss: 0.00001463
Iteration 4/1000 | Loss: 0.00001339
Iteration 5/1000 | Loss: 0.00001271
Iteration 6/1000 | Loss: 0.00001248
Iteration 7/1000 | Loss: 0.00001215
Iteration 8/1000 | Loss: 0.00001198
Iteration 9/1000 | Loss: 0.00001195
Iteration 10/1000 | Loss: 0.00001181
Iteration 11/1000 | Loss: 0.00001167
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001161
Iteration 14/1000 | Loss: 0.00001160
Iteration 15/1000 | Loss: 0.00001156
Iteration 16/1000 | Loss: 0.00001156
Iteration 17/1000 | Loss: 0.00001154
Iteration 18/1000 | Loss: 0.00001154
Iteration 19/1000 | Loss: 0.00001152
Iteration 20/1000 | Loss: 0.00001152
Iteration 21/1000 | Loss: 0.00001151
Iteration 22/1000 | Loss: 0.00001151
Iteration 23/1000 | Loss: 0.00001151
Iteration 24/1000 | Loss: 0.00001150
Iteration 25/1000 | Loss: 0.00001150
Iteration 26/1000 | Loss: 0.00001149
Iteration 27/1000 | Loss: 0.00001149
Iteration 28/1000 | Loss: 0.00001149
Iteration 29/1000 | Loss: 0.00001148
Iteration 30/1000 | Loss: 0.00001148
Iteration 31/1000 | Loss: 0.00001147
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001146
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001144
Iteration 40/1000 | Loss: 0.00001143
Iteration 41/1000 | Loss: 0.00001143
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001142
Iteration 44/1000 | Loss: 0.00001142
Iteration 45/1000 | Loss: 0.00001141
Iteration 46/1000 | Loss: 0.00001141
Iteration 47/1000 | Loss: 0.00001140
Iteration 48/1000 | Loss: 0.00001140
Iteration 49/1000 | Loss: 0.00001139
Iteration 50/1000 | Loss: 0.00001139
Iteration 51/1000 | Loss: 0.00001139
Iteration 52/1000 | Loss: 0.00001139
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001139
Iteration 55/1000 | Loss: 0.00001139
Iteration 56/1000 | Loss: 0.00001139
Iteration 57/1000 | Loss: 0.00001138
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001138
Iteration 61/1000 | Loss: 0.00001138
Iteration 62/1000 | Loss: 0.00001138
Iteration 63/1000 | Loss: 0.00001138
Iteration 64/1000 | Loss: 0.00001138
Iteration 65/1000 | Loss: 0.00001138
Iteration 66/1000 | Loss: 0.00001138
Iteration 67/1000 | Loss: 0.00001138
Iteration 68/1000 | Loss: 0.00001138
Iteration 69/1000 | Loss: 0.00001138
Iteration 70/1000 | Loss: 0.00001138
Iteration 71/1000 | Loss: 0.00001138
Iteration 72/1000 | Loss: 0.00001138
Iteration 73/1000 | Loss: 0.00001137
Iteration 74/1000 | Loss: 0.00001137
Iteration 75/1000 | Loss: 0.00001137
Iteration 76/1000 | Loss: 0.00001137
Iteration 77/1000 | Loss: 0.00001137
Iteration 78/1000 | Loss: 0.00001137
Iteration 79/1000 | Loss: 0.00001136
Iteration 80/1000 | Loss: 0.00001136
Iteration 81/1000 | Loss: 0.00001136
Iteration 82/1000 | Loss: 0.00001136
Iteration 83/1000 | Loss: 0.00001135
Iteration 84/1000 | Loss: 0.00001135
Iteration 85/1000 | Loss: 0.00001135
Iteration 86/1000 | Loss: 0.00001135
Iteration 87/1000 | Loss: 0.00001135
Iteration 88/1000 | Loss: 0.00001135
Iteration 89/1000 | Loss: 0.00001135
Iteration 90/1000 | Loss: 0.00001135
Iteration 91/1000 | Loss: 0.00001134
Iteration 92/1000 | Loss: 0.00001134
Iteration 93/1000 | Loss: 0.00001134
Iteration 94/1000 | Loss: 0.00001134
Iteration 95/1000 | Loss: 0.00001133
Iteration 96/1000 | Loss: 0.00001133
Iteration 97/1000 | Loss: 0.00001133
Iteration 98/1000 | Loss: 0.00001133
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001132
Iteration 101/1000 | Loss: 0.00001132
Iteration 102/1000 | Loss: 0.00001132
Iteration 103/1000 | Loss: 0.00001132
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001132
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001131
Iteration 108/1000 | Loss: 0.00001131
Iteration 109/1000 | Loss: 0.00001131
Iteration 110/1000 | Loss: 0.00001131
Iteration 111/1000 | Loss: 0.00001131
Iteration 112/1000 | Loss: 0.00001131
Iteration 113/1000 | Loss: 0.00001130
Iteration 114/1000 | Loss: 0.00001130
Iteration 115/1000 | Loss: 0.00001130
Iteration 116/1000 | Loss: 0.00001130
Iteration 117/1000 | Loss: 0.00001129
Iteration 118/1000 | Loss: 0.00001129
Iteration 119/1000 | Loss: 0.00001129
Iteration 120/1000 | Loss: 0.00001129
Iteration 121/1000 | Loss: 0.00001129
Iteration 122/1000 | Loss: 0.00001129
Iteration 123/1000 | Loss: 0.00001129
Iteration 124/1000 | Loss: 0.00001128
Iteration 125/1000 | Loss: 0.00001128
Iteration 126/1000 | Loss: 0.00001128
Iteration 127/1000 | Loss: 0.00001128
Iteration 128/1000 | Loss: 0.00001128
Iteration 129/1000 | Loss: 0.00001128
Iteration 130/1000 | Loss: 0.00001128
Iteration 131/1000 | Loss: 0.00001128
Iteration 132/1000 | Loss: 0.00001128
Iteration 133/1000 | Loss: 0.00001128
Iteration 134/1000 | Loss: 0.00001128
Iteration 135/1000 | Loss: 0.00001127
Iteration 136/1000 | Loss: 0.00001127
Iteration 137/1000 | Loss: 0.00001127
Iteration 138/1000 | Loss: 0.00001127
Iteration 139/1000 | Loss: 0.00001127
Iteration 140/1000 | Loss: 0.00001127
Iteration 141/1000 | Loss: 0.00001127
Iteration 142/1000 | Loss: 0.00001127
Iteration 143/1000 | Loss: 0.00001127
Iteration 144/1000 | Loss: 0.00001127
Iteration 145/1000 | Loss: 0.00001127
Iteration 146/1000 | Loss: 0.00001127
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001127
Iteration 149/1000 | Loss: 0.00001127
Iteration 150/1000 | Loss: 0.00001127
Iteration 151/1000 | Loss: 0.00001127
Iteration 152/1000 | Loss: 0.00001127
Iteration 153/1000 | Loss: 0.00001127
Iteration 154/1000 | Loss: 0.00001127
Iteration 155/1000 | Loss: 0.00001127
Iteration 156/1000 | Loss: 0.00001127
Iteration 157/1000 | Loss: 0.00001127
Iteration 158/1000 | Loss: 0.00001127
Iteration 159/1000 | Loss: 0.00001127
Iteration 160/1000 | Loss: 0.00001127
Iteration 161/1000 | Loss: 0.00001127
Iteration 162/1000 | Loss: 0.00001127
Iteration 163/1000 | Loss: 0.00001127
Iteration 164/1000 | Loss: 0.00001127
Iteration 165/1000 | Loss: 0.00001127
Iteration 166/1000 | Loss: 0.00001127
Iteration 167/1000 | Loss: 0.00001127
Iteration 168/1000 | Loss: 0.00001127
Iteration 169/1000 | Loss: 0.00001127
Iteration 170/1000 | Loss: 0.00001127
Iteration 171/1000 | Loss: 0.00001127
Iteration 172/1000 | Loss: 0.00001127
Iteration 173/1000 | Loss: 0.00001127
Iteration 174/1000 | Loss: 0.00001127
Iteration 175/1000 | Loss: 0.00001127
Iteration 176/1000 | Loss: 0.00001127
Iteration 177/1000 | Loss: 0.00001127
Iteration 178/1000 | Loss: 0.00001127
Iteration 179/1000 | Loss: 0.00001127
Iteration 180/1000 | Loss: 0.00001127
Iteration 181/1000 | Loss: 0.00001127
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.1272351912339218e-05, 1.1272351912339218e-05, 1.1272351912339218e-05, 1.1272351912339218e-05, 1.1272351912339218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1272351912339218e-05

Optimization complete. Final v2v error: 2.8272440433502197 mm

Highest mean error: 3.0977299213409424 mm for frame 171

Lowest mean error: 2.2110490798950195 mm for frame 10

Saving results

Total time: 39.774932861328125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00664911
Iteration 2/25 | Loss: 0.00132125
Iteration 3/25 | Loss: 0.00110205
Iteration 4/25 | Loss: 0.00106267
Iteration 5/25 | Loss: 0.00104519
Iteration 6/25 | Loss: 0.00104052
Iteration 7/25 | Loss: 0.00103924
Iteration 8/25 | Loss: 0.00103863
Iteration 9/25 | Loss: 0.00103830
Iteration 10/25 | Loss: 0.00103654
Iteration 11/25 | Loss: 0.00103965
Iteration 12/25 | Loss: 0.00103287
Iteration 13/25 | Loss: 0.00103184
Iteration 14/25 | Loss: 0.00103175
Iteration 15/25 | Loss: 0.00103175
Iteration 16/25 | Loss: 0.00103175
Iteration 17/25 | Loss: 0.00103174
Iteration 18/25 | Loss: 0.00103174
Iteration 19/25 | Loss: 0.00103174
Iteration 20/25 | Loss: 0.00103174
Iteration 21/25 | Loss: 0.00103174
Iteration 22/25 | Loss: 0.00103174
Iteration 23/25 | Loss: 0.00103174
Iteration 24/25 | Loss: 0.00103174
Iteration 25/25 | Loss: 0.00103174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.65232551
Iteration 2/25 | Loss: 0.00095966
Iteration 3/25 | Loss: 0.00095965
Iteration 4/25 | Loss: 0.00095965
Iteration 5/25 | Loss: 0.00095965
Iteration 6/25 | Loss: 0.00095965
Iteration 7/25 | Loss: 0.00095965
Iteration 8/25 | Loss: 0.00095965
Iteration 9/25 | Loss: 0.00095965
Iteration 10/25 | Loss: 0.00095965
Iteration 11/25 | Loss: 0.00095965
Iteration 12/25 | Loss: 0.00095965
Iteration 13/25 | Loss: 0.00095965
Iteration 14/25 | Loss: 0.00095965
Iteration 15/25 | Loss: 0.00095965
Iteration 16/25 | Loss: 0.00095965
Iteration 17/25 | Loss: 0.00095965
Iteration 18/25 | Loss: 0.00095965
Iteration 19/25 | Loss: 0.00095965
Iteration 20/25 | Loss: 0.00095965
Iteration 21/25 | Loss: 0.00095965
Iteration 22/25 | Loss: 0.00095965
Iteration 23/25 | Loss: 0.00095965
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009596484014764428, 0.0009596484014764428, 0.0009596484014764428, 0.0009596484014764428, 0.0009596484014764428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009596484014764428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095965
Iteration 2/1000 | Loss: 0.00005967
Iteration 3/1000 | Loss: 0.00003658
Iteration 4/1000 | Loss: 0.00026597
Iteration 5/1000 | Loss: 0.00002430
Iteration 6/1000 | Loss: 0.00016938
Iteration 7/1000 | Loss: 0.00003826
Iteration 8/1000 | Loss: 0.00002774
Iteration 9/1000 | Loss: 0.00002327
Iteration 10/1000 | Loss: 0.00013541
Iteration 11/1000 | Loss: 0.00002335
Iteration 12/1000 | Loss: 0.00002178
Iteration 13/1000 | Loss: 0.00002133
Iteration 14/1000 | Loss: 0.00002095
Iteration 15/1000 | Loss: 0.00002066
Iteration 16/1000 | Loss: 0.00002040
Iteration 17/1000 | Loss: 0.00002021
Iteration 18/1000 | Loss: 0.00002013
Iteration 19/1000 | Loss: 0.00002009
Iteration 20/1000 | Loss: 0.00002008
Iteration 21/1000 | Loss: 0.00002001
Iteration 22/1000 | Loss: 0.00001991
Iteration 23/1000 | Loss: 0.00001990
Iteration 24/1000 | Loss: 0.00001988
Iteration 25/1000 | Loss: 0.00001988
Iteration 26/1000 | Loss: 0.00001988
Iteration 27/1000 | Loss: 0.00001987
Iteration 28/1000 | Loss: 0.00001987
Iteration 29/1000 | Loss: 0.00001987
Iteration 30/1000 | Loss: 0.00001986
Iteration 31/1000 | Loss: 0.00001986
Iteration 32/1000 | Loss: 0.00001985
Iteration 33/1000 | Loss: 0.00001985
Iteration 34/1000 | Loss: 0.00001984
Iteration 35/1000 | Loss: 0.00001984
Iteration 36/1000 | Loss: 0.00001983
Iteration 37/1000 | Loss: 0.00001982
Iteration 38/1000 | Loss: 0.00001977
Iteration 39/1000 | Loss: 0.00001976
Iteration 40/1000 | Loss: 0.00001976
Iteration 41/1000 | Loss: 0.00001976
Iteration 42/1000 | Loss: 0.00001976
Iteration 43/1000 | Loss: 0.00001976
Iteration 44/1000 | Loss: 0.00001976
Iteration 45/1000 | Loss: 0.00001975
Iteration 46/1000 | Loss: 0.00001975
Iteration 47/1000 | Loss: 0.00001975
Iteration 48/1000 | Loss: 0.00001975
Iteration 49/1000 | Loss: 0.00001975
Iteration 50/1000 | Loss: 0.00001975
Iteration 51/1000 | Loss: 0.00001975
Iteration 52/1000 | Loss: 0.00001975
Iteration 53/1000 | Loss: 0.00001975
Iteration 54/1000 | Loss: 0.00001974
Iteration 55/1000 | Loss: 0.00001974
Iteration 56/1000 | Loss: 0.00001973
Iteration 57/1000 | Loss: 0.00001973
Iteration 58/1000 | Loss: 0.00001972
Iteration 59/1000 | Loss: 0.00001972
Iteration 60/1000 | Loss: 0.00001970
Iteration 61/1000 | Loss: 0.00001970
Iteration 62/1000 | Loss: 0.00001970
Iteration 63/1000 | Loss: 0.00001970
Iteration 64/1000 | Loss: 0.00001970
Iteration 65/1000 | Loss: 0.00001970
Iteration 66/1000 | Loss: 0.00001970
Iteration 67/1000 | Loss: 0.00001970
Iteration 68/1000 | Loss: 0.00001970
Iteration 69/1000 | Loss: 0.00001969
Iteration 70/1000 | Loss: 0.00001969
Iteration 71/1000 | Loss: 0.00001969
Iteration 72/1000 | Loss: 0.00001969
Iteration 73/1000 | Loss: 0.00001967
Iteration 74/1000 | Loss: 0.00001967
Iteration 75/1000 | Loss: 0.00001967
Iteration 76/1000 | Loss: 0.00001966
Iteration 77/1000 | Loss: 0.00001966
Iteration 78/1000 | Loss: 0.00001966
Iteration 79/1000 | Loss: 0.00001966
Iteration 80/1000 | Loss: 0.00001966
Iteration 81/1000 | Loss: 0.00001965
Iteration 82/1000 | Loss: 0.00001965
Iteration 83/1000 | Loss: 0.00001965
Iteration 84/1000 | Loss: 0.00001965
Iteration 85/1000 | Loss: 0.00001964
Iteration 86/1000 | Loss: 0.00001964
Iteration 87/1000 | Loss: 0.00001964
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001963
Iteration 90/1000 | Loss: 0.00001963
Iteration 91/1000 | Loss: 0.00001963
Iteration 92/1000 | Loss: 0.00001963
Iteration 93/1000 | Loss: 0.00001963
Iteration 94/1000 | Loss: 0.00001962
Iteration 95/1000 | Loss: 0.00001962
Iteration 96/1000 | Loss: 0.00001962
Iteration 97/1000 | Loss: 0.00001962
Iteration 98/1000 | Loss: 0.00001962
Iteration 99/1000 | Loss: 0.00001962
Iteration 100/1000 | Loss: 0.00001961
Iteration 101/1000 | Loss: 0.00001961
Iteration 102/1000 | Loss: 0.00001961
Iteration 103/1000 | Loss: 0.00001961
Iteration 104/1000 | Loss: 0.00001961
Iteration 105/1000 | Loss: 0.00001961
Iteration 106/1000 | Loss: 0.00001961
Iteration 107/1000 | Loss: 0.00001961
Iteration 108/1000 | Loss: 0.00001961
Iteration 109/1000 | Loss: 0.00001961
Iteration 110/1000 | Loss: 0.00001961
Iteration 111/1000 | Loss: 0.00001961
Iteration 112/1000 | Loss: 0.00001961
Iteration 113/1000 | Loss: 0.00001961
Iteration 114/1000 | Loss: 0.00001961
Iteration 115/1000 | Loss: 0.00001961
Iteration 116/1000 | Loss: 0.00001961
Iteration 117/1000 | Loss: 0.00001961
Iteration 118/1000 | Loss: 0.00001961
Iteration 119/1000 | Loss: 0.00001961
Iteration 120/1000 | Loss: 0.00001961
Iteration 121/1000 | Loss: 0.00001961
Iteration 122/1000 | Loss: 0.00001961
Iteration 123/1000 | Loss: 0.00001961
Iteration 124/1000 | Loss: 0.00001961
Iteration 125/1000 | Loss: 0.00001961
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.9608025468187407e-05, 1.9608025468187407e-05, 1.9608025468187407e-05, 1.9608025468187407e-05, 1.9608025468187407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9608025468187407e-05

Optimization complete. Final v2v error: 3.570777416229248 mm

Highest mean error: 4.938990592956543 mm for frame 50

Lowest mean error: 2.8050026893615723 mm for frame 20

Saving results

Total time: 59.737486362457275
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00909755
Iteration 2/25 | Loss: 0.00113643
Iteration 3/25 | Loss: 0.00099026
Iteration 4/25 | Loss: 0.00096773
Iteration 5/25 | Loss: 0.00096113
Iteration 6/25 | Loss: 0.00095929
Iteration 7/25 | Loss: 0.00095929
Iteration 8/25 | Loss: 0.00095929
Iteration 9/25 | Loss: 0.00095929
Iteration 10/25 | Loss: 0.00095929
Iteration 11/25 | Loss: 0.00095929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009592889109626412, 0.0009592889109626412, 0.0009592889109626412, 0.0009592889109626412, 0.0009592889109626412]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009592889109626412

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53270590
Iteration 2/25 | Loss: 0.00061198
Iteration 3/25 | Loss: 0.00061198
Iteration 4/25 | Loss: 0.00061198
Iteration 5/25 | Loss: 0.00061198
Iteration 6/25 | Loss: 0.00061198
Iteration 7/25 | Loss: 0.00061198
Iteration 8/25 | Loss: 0.00061198
Iteration 9/25 | Loss: 0.00061198
Iteration 10/25 | Loss: 0.00061198
Iteration 11/25 | Loss: 0.00061198
Iteration 12/25 | Loss: 0.00061198
Iteration 13/25 | Loss: 0.00061198
Iteration 14/25 | Loss: 0.00061198
Iteration 15/25 | Loss: 0.00061198
Iteration 16/25 | Loss: 0.00061198
Iteration 17/25 | Loss: 0.00061198
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006119775935076177, 0.0006119775935076177, 0.0006119775935076177, 0.0006119775935076177, 0.0006119775935076177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006119775935076177

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061198
Iteration 2/1000 | Loss: 0.00001919
Iteration 3/1000 | Loss: 0.00001257
Iteration 4/1000 | Loss: 0.00001139
Iteration 5/1000 | Loss: 0.00001100
Iteration 6/1000 | Loss: 0.00001070
Iteration 7/1000 | Loss: 0.00001062
Iteration 8/1000 | Loss: 0.00001047
Iteration 9/1000 | Loss: 0.00001043
Iteration 10/1000 | Loss: 0.00001040
Iteration 11/1000 | Loss: 0.00001033
Iteration 12/1000 | Loss: 0.00001028
Iteration 13/1000 | Loss: 0.00001027
Iteration 14/1000 | Loss: 0.00001026
Iteration 15/1000 | Loss: 0.00001024
Iteration 16/1000 | Loss: 0.00001023
Iteration 17/1000 | Loss: 0.00001022
Iteration 18/1000 | Loss: 0.00001020
Iteration 19/1000 | Loss: 0.00001020
Iteration 20/1000 | Loss: 0.00001020
Iteration 21/1000 | Loss: 0.00001020
Iteration 22/1000 | Loss: 0.00001020
Iteration 23/1000 | Loss: 0.00001020
Iteration 24/1000 | Loss: 0.00001020
Iteration 25/1000 | Loss: 0.00001020
Iteration 26/1000 | Loss: 0.00001020
Iteration 27/1000 | Loss: 0.00001020
Iteration 28/1000 | Loss: 0.00001020
Iteration 29/1000 | Loss: 0.00001019
Iteration 30/1000 | Loss: 0.00001018
Iteration 31/1000 | Loss: 0.00001018
Iteration 32/1000 | Loss: 0.00001017
Iteration 33/1000 | Loss: 0.00001017
Iteration 34/1000 | Loss: 0.00001017
Iteration 35/1000 | Loss: 0.00001016
Iteration 36/1000 | Loss: 0.00001016
Iteration 37/1000 | Loss: 0.00001016
Iteration 38/1000 | Loss: 0.00001016
Iteration 39/1000 | Loss: 0.00001016
Iteration 40/1000 | Loss: 0.00001016
Iteration 41/1000 | Loss: 0.00001016
Iteration 42/1000 | Loss: 0.00001016
Iteration 43/1000 | Loss: 0.00001016
Iteration 44/1000 | Loss: 0.00001016
Iteration 45/1000 | Loss: 0.00001016
Iteration 46/1000 | Loss: 0.00001016
Iteration 47/1000 | Loss: 0.00001016
Iteration 48/1000 | Loss: 0.00001015
Iteration 49/1000 | Loss: 0.00001015
Iteration 50/1000 | Loss: 0.00001015
Iteration 51/1000 | Loss: 0.00001014
Iteration 52/1000 | Loss: 0.00001014
Iteration 53/1000 | Loss: 0.00001014
Iteration 54/1000 | Loss: 0.00001014
Iteration 55/1000 | Loss: 0.00001014
Iteration 56/1000 | Loss: 0.00001013
Iteration 57/1000 | Loss: 0.00001013
Iteration 58/1000 | Loss: 0.00001013
Iteration 59/1000 | Loss: 0.00001013
Iteration 60/1000 | Loss: 0.00001013
Iteration 61/1000 | Loss: 0.00001013
Iteration 62/1000 | Loss: 0.00001013
Iteration 63/1000 | Loss: 0.00001013
Iteration 64/1000 | Loss: 0.00001013
Iteration 65/1000 | Loss: 0.00001012
Iteration 66/1000 | Loss: 0.00001012
Iteration 67/1000 | Loss: 0.00001011
Iteration 68/1000 | Loss: 0.00001011
Iteration 69/1000 | Loss: 0.00001011
Iteration 70/1000 | Loss: 0.00001011
Iteration 71/1000 | Loss: 0.00001011
Iteration 72/1000 | Loss: 0.00001010
Iteration 73/1000 | Loss: 0.00001010
Iteration 74/1000 | Loss: 0.00001010
Iteration 75/1000 | Loss: 0.00001010
Iteration 76/1000 | Loss: 0.00001009
Iteration 77/1000 | Loss: 0.00001009
Iteration 78/1000 | Loss: 0.00001009
Iteration 79/1000 | Loss: 0.00001008
Iteration 80/1000 | Loss: 0.00001008
Iteration 81/1000 | Loss: 0.00001008
Iteration 82/1000 | Loss: 0.00001007
Iteration 83/1000 | Loss: 0.00001007
Iteration 84/1000 | Loss: 0.00001006
Iteration 85/1000 | Loss: 0.00001006
Iteration 86/1000 | Loss: 0.00001006
Iteration 87/1000 | Loss: 0.00001006
Iteration 88/1000 | Loss: 0.00001006
Iteration 89/1000 | Loss: 0.00001005
Iteration 90/1000 | Loss: 0.00001005
Iteration 91/1000 | Loss: 0.00001005
Iteration 92/1000 | Loss: 0.00001005
Iteration 93/1000 | Loss: 0.00001005
Iteration 94/1000 | Loss: 0.00001005
Iteration 95/1000 | Loss: 0.00001005
Iteration 96/1000 | Loss: 0.00001005
Iteration 97/1000 | Loss: 0.00001005
Iteration 98/1000 | Loss: 0.00001005
Iteration 99/1000 | Loss: 0.00001004
Iteration 100/1000 | Loss: 0.00001004
Iteration 101/1000 | Loss: 0.00001004
Iteration 102/1000 | Loss: 0.00001003
Iteration 103/1000 | Loss: 0.00001003
Iteration 104/1000 | Loss: 0.00001003
Iteration 105/1000 | Loss: 0.00001003
Iteration 106/1000 | Loss: 0.00001003
Iteration 107/1000 | Loss: 0.00001003
Iteration 108/1000 | Loss: 0.00001003
Iteration 109/1000 | Loss: 0.00001003
Iteration 110/1000 | Loss: 0.00001003
Iteration 111/1000 | Loss: 0.00001003
Iteration 112/1000 | Loss: 0.00001003
Iteration 113/1000 | Loss: 0.00001003
Iteration 114/1000 | Loss: 0.00001003
Iteration 115/1000 | Loss: 0.00001003
Iteration 116/1000 | Loss: 0.00001003
Iteration 117/1000 | Loss: 0.00001003
Iteration 118/1000 | Loss: 0.00001003
Iteration 119/1000 | Loss: 0.00001003
Iteration 120/1000 | Loss: 0.00001003
Iteration 121/1000 | Loss: 0.00001003
Iteration 122/1000 | Loss: 0.00001003
Iteration 123/1000 | Loss: 0.00001003
Iteration 124/1000 | Loss: 0.00001003
Iteration 125/1000 | Loss: 0.00001003
Iteration 126/1000 | Loss: 0.00001003
Iteration 127/1000 | Loss: 0.00001003
Iteration 128/1000 | Loss: 0.00001003
Iteration 129/1000 | Loss: 0.00001003
Iteration 130/1000 | Loss: 0.00001003
Iteration 131/1000 | Loss: 0.00001003
Iteration 132/1000 | Loss: 0.00001003
Iteration 133/1000 | Loss: 0.00001003
Iteration 134/1000 | Loss: 0.00001003
Iteration 135/1000 | Loss: 0.00001003
Iteration 136/1000 | Loss: 0.00001003
Iteration 137/1000 | Loss: 0.00001003
Iteration 138/1000 | Loss: 0.00001003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [1.0034777005785145e-05, 1.0034777005785145e-05, 1.0034777005785145e-05, 1.0034777005785145e-05, 1.0034777005785145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0034777005785145e-05

Optimization complete. Final v2v error: 2.7292628288269043 mm

Highest mean error: 3.255333185195923 mm for frame 232

Lowest mean error: 2.43833327293396 mm for frame 61

Saving results

Total time: 32.76312232017517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00966426
Iteration 2/25 | Loss: 0.00120428
Iteration 3/25 | Loss: 0.00103044
Iteration 4/25 | Loss: 0.00099917
Iteration 5/25 | Loss: 0.00098648
Iteration 6/25 | Loss: 0.00098276
Iteration 7/25 | Loss: 0.00098216
Iteration 8/25 | Loss: 0.00098216
Iteration 9/25 | Loss: 0.00098216
Iteration 10/25 | Loss: 0.00098216
Iteration 11/25 | Loss: 0.00098216
Iteration 12/25 | Loss: 0.00098216
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009821566054597497, 0.0009821566054597497, 0.0009821566054597497, 0.0009821566054597497, 0.0009821566054597497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009821566054597497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35049808
Iteration 2/25 | Loss: 0.00064376
Iteration 3/25 | Loss: 0.00064375
Iteration 4/25 | Loss: 0.00064375
Iteration 5/25 | Loss: 0.00064375
Iteration 6/25 | Loss: 0.00064375
Iteration 7/25 | Loss: 0.00064375
Iteration 8/25 | Loss: 0.00064375
Iteration 9/25 | Loss: 0.00064375
Iteration 10/25 | Loss: 0.00064375
Iteration 11/25 | Loss: 0.00064375
Iteration 12/25 | Loss: 0.00064375
Iteration 13/25 | Loss: 0.00064375
Iteration 14/25 | Loss: 0.00064375
Iteration 15/25 | Loss: 0.00064375
Iteration 16/25 | Loss: 0.00064375
Iteration 17/25 | Loss: 0.00064375
Iteration 18/25 | Loss: 0.00064375
Iteration 19/25 | Loss: 0.00064375
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0006437511765398085, 0.0006437511765398085, 0.0006437511765398085, 0.0006437511765398085, 0.0006437511765398085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006437511765398085

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064375
Iteration 2/1000 | Loss: 0.00004107
Iteration 3/1000 | Loss: 0.00002664
Iteration 4/1000 | Loss: 0.00002353
Iteration 5/1000 | Loss: 0.00002207
Iteration 6/1000 | Loss: 0.00002098
Iteration 7/1000 | Loss: 0.00002030
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00001928
Iteration 10/1000 | Loss: 0.00001896
Iteration 11/1000 | Loss: 0.00001890
Iteration 12/1000 | Loss: 0.00001889
Iteration 13/1000 | Loss: 0.00001879
Iteration 14/1000 | Loss: 0.00001875
Iteration 15/1000 | Loss: 0.00001872
Iteration 16/1000 | Loss: 0.00001871
Iteration 17/1000 | Loss: 0.00001870
Iteration 18/1000 | Loss: 0.00001869
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00001858
Iteration 22/1000 | Loss: 0.00001855
Iteration 23/1000 | Loss: 0.00001852
Iteration 24/1000 | Loss: 0.00001852
Iteration 25/1000 | Loss: 0.00001852
Iteration 26/1000 | Loss: 0.00001852
Iteration 27/1000 | Loss: 0.00001852
Iteration 28/1000 | Loss: 0.00001851
Iteration 29/1000 | Loss: 0.00001851
Iteration 30/1000 | Loss: 0.00001851
Iteration 31/1000 | Loss: 0.00001851
Iteration 32/1000 | Loss: 0.00001851
Iteration 33/1000 | Loss: 0.00001851
Iteration 34/1000 | Loss: 0.00001851
Iteration 35/1000 | Loss: 0.00001851
Iteration 36/1000 | Loss: 0.00001851
Iteration 37/1000 | Loss: 0.00001851
Iteration 38/1000 | Loss: 0.00001851
Iteration 39/1000 | Loss: 0.00001850
Iteration 40/1000 | Loss: 0.00001850
Iteration 41/1000 | Loss: 0.00001850
Iteration 42/1000 | Loss: 0.00001850
Iteration 43/1000 | Loss: 0.00001850
Iteration 44/1000 | Loss: 0.00001850
Iteration 45/1000 | Loss: 0.00001850
Iteration 46/1000 | Loss: 0.00001850
Iteration 47/1000 | Loss: 0.00001850
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001850
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001849
Iteration 52/1000 | Loss: 0.00001849
Iteration 53/1000 | Loss: 0.00001849
Iteration 54/1000 | Loss: 0.00001849
Iteration 55/1000 | Loss: 0.00001849
Iteration 56/1000 | Loss: 0.00001849
Iteration 57/1000 | Loss: 0.00001849
Iteration 58/1000 | Loss: 0.00001849
Iteration 59/1000 | Loss: 0.00001849
Iteration 60/1000 | Loss: 0.00001848
Iteration 61/1000 | Loss: 0.00001848
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001847
Iteration 64/1000 | Loss: 0.00001847
Iteration 65/1000 | Loss: 0.00001847
Iteration 66/1000 | Loss: 0.00001846
Iteration 67/1000 | Loss: 0.00001846
Iteration 68/1000 | Loss: 0.00001846
Iteration 69/1000 | Loss: 0.00001846
Iteration 70/1000 | Loss: 0.00001846
Iteration 71/1000 | Loss: 0.00001846
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001846
Iteration 74/1000 | Loss: 0.00001846
Iteration 75/1000 | Loss: 0.00001846
Iteration 76/1000 | Loss: 0.00001846
Iteration 77/1000 | Loss: 0.00001845
Iteration 78/1000 | Loss: 0.00001845
Iteration 79/1000 | Loss: 0.00001845
Iteration 80/1000 | Loss: 0.00001845
Iteration 81/1000 | Loss: 0.00001845
Iteration 82/1000 | Loss: 0.00001845
Iteration 83/1000 | Loss: 0.00001845
Iteration 84/1000 | Loss: 0.00001844
Iteration 85/1000 | Loss: 0.00001844
Iteration 86/1000 | Loss: 0.00001844
Iteration 87/1000 | Loss: 0.00001844
Iteration 88/1000 | Loss: 0.00001844
Iteration 89/1000 | Loss: 0.00001844
Iteration 90/1000 | Loss: 0.00001843
Iteration 91/1000 | Loss: 0.00001843
Iteration 92/1000 | Loss: 0.00001843
Iteration 93/1000 | Loss: 0.00001843
Iteration 94/1000 | Loss: 0.00001843
Iteration 95/1000 | Loss: 0.00001843
Iteration 96/1000 | Loss: 0.00001843
Iteration 97/1000 | Loss: 0.00001842
Iteration 98/1000 | Loss: 0.00001842
Iteration 99/1000 | Loss: 0.00001842
Iteration 100/1000 | Loss: 0.00001842
Iteration 101/1000 | Loss: 0.00001842
Iteration 102/1000 | Loss: 0.00001842
Iteration 103/1000 | Loss: 0.00001842
Iteration 104/1000 | Loss: 0.00001842
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001841
Iteration 107/1000 | Loss: 0.00001841
Iteration 108/1000 | Loss: 0.00001841
Iteration 109/1000 | Loss: 0.00001841
Iteration 110/1000 | Loss: 0.00001841
Iteration 111/1000 | Loss: 0.00001841
Iteration 112/1000 | Loss: 0.00001841
Iteration 113/1000 | Loss: 0.00001841
Iteration 114/1000 | Loss: 0.00001841
Iteration 115/1000 | Loss: 0.00001840
Iteration 116/1000 | Loss: 0.00001840
Iteration 117/1000 | Loss: 0.00001840
Iteration 118/1000 | Loss: 0.00001840
Iteration 119/1000 | Loss: 0.00001840
Iteration 120/1000 | Loss: 0.00001840
Iteration 121/1000 | Loss: 0.00001840
Iteration 122/1000 | Loss: 0.00001840
Iteration 123/1000 | Loss: 0.00001840
Iteration 124/1000 | Loss: 0.00001839
Iteration 125/1000 | Loss: 0.00001839
Iteration 126/1000 | Loss: 0.00001839
Iteration 127/1000 | Loss: 0.00001839
Iteration 128/1000 | Loss: 0.00001839
Iteration 129/1000 | Loss: 0.00001839
Iteration 130/1000 | Loss: 0.00001839
Iteration 131/1000 | Loss: 0.00001839
Iteration 132/1000 | Loss: 0.00001839
Iteration 133/1000 | Loss: 0.00001839
Iteration 134/1000 | Loss: 0.00001839
Iteration 135/1000 | Loss: 0.00001839
Iteration 136/1000 | Loss: 0.00001839
Iteration 137/1000 | Loss: 0.00001839
Iteration 138/1000 | Loss: 0.00001839
Iteration 139/1000 | Loss: 0.00001839
Iteration 140/1000 | Loss: 0.00001839
Iteration 141/1000 | Loss: 0.00001839
Iteration 142/1000 | Loss: 0.00001839
Iteration 143/1000 | Loss: 0.00001839
Iteration 144/1000 | Loss: 0.00001839
Iteration 145/1000 | Loss: 0.00001839
Iteration 146/1000 | Loss: 0.00001839
Iteration 147/1000 | Loss: 0.00001839
Iteration 148/1000 | Loss: 0.00001839
Iteration 149/1000 | Loss: 0.00001839
Iteration 150/1000 | Loss: 0.00001839
Iteration 151/1000 | Loss: 0.00001839
Iteration 152/1000 | Loss: 0.00001839
Iteration 153/1000 | Loss: 0.00001839
Iteration 154/1000 | Loss: 0.00001839
Iteration 155/1000 | Loss: 0.00001839
Iteration 156/1000 | Loss: 0.00001839
Iteration 157/1000 | Loss: 0.00001839
Iteration 158/1000 | Loss: 0.00001839
Iteration 159/1000 | Loss: 0.00001839
Iteration 160/1000 | Loss: 0.00001839
Iteration 161/1000 | Loss: 0.00001839
Iteration 162/1000 | Loss: 0.00001839
Iteration 163/1000 | Loss: 0.00001839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.839045580709353e-05, 1.839045580709353e-05, 1.839045580709353e-05, 1.839045580709353e-05, 1.839045580709353e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.839045580709353e-05

Optimization complete. Final v2v error: 3.4792237281799316 mm

Highest mean error: 5.694802761077881 mm for frame 70

Lowest mean error: 2.813727855682373 mm for frame 138

Saving results

Total time: 35.657020568847656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00894815
Iteration 2/25 | Loss: 0.00161280
Iteration 3/25 | Loss: 0.00119260
Iteration 4/25 | Loss: 0.00114526
Iteration 5/25 | Loss: 0.00114162
Iteration 6/25 | Loss: 0.00113502
Iteration 7/25 | Loss: 0.00113220
Iteration 8/25 | Loss: 0.00112820
Iteration 9/25 | Loss: 0.00112362
Iteration 10/25 | Loss: 0.00111854
Iteration 11/25 | Loss: 0.00111766
Iteration 12/25 | Loss: 0.00111788
Iteration 13/25 | Loss: 0.00111541
Iteration 14/25 | Loss: 0.00111488
Iteration 15/25 | Loss: 0.00111470
Iteration 16/25 | Loss: 0.00111470
Iteration 17/25 | Loss: 0.00111470
Iteration 18/25 | Loss: 0.00111470
Iteration 19/25 | Loss: 0.00111470
Iteration 20/25 | Loss: 0.00111469
Iteration 21/25 | Loss: 0.00111469
Iteration 22/25 | Loss: 0.00111469
Iteration 23/25 | Loss: 0.00111469
Iteration 24/25 | Loss: 0.00111469
Iteration 25/25 | Loss: 0.00111469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25030541
Iteration 2/25 | Loss: 0.00024944
Iteration 3/25 | Loss: 0.00024944
Iteration 4/25 | Loss: 0.00024944
Iteration 5/25 | Loss: 0.00024944
Iteration 6/25 | Loss: 0.00024944
Iteration 7/25 | Loss: 0.00024944
Iteration 8/25 | Loss: 0.00024944
Iteration 9/25 | Loss: 0.00024944
Iteration 10/25 | Loss: 0.00024943
Iteration 11/25 | Loss: 0.00024943
Iteration 12/25 | Loss: 0.00024943
Iteration 13/25 | Loss: 0.00024943
Iteration 14/25 | Loss: 0.00024943
Iteration 15/25 | Loss: 0.00024943
Iteration 16/25 | Loss: 0.00024943
Iteration 17/25 | Loss: 0.00024943
Iteration 18/25 | Loss: 0.00024943
Iteration 19/25 | Loss: 0.00024943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0002494346408639103, 0.0002494346408639103, 0.0002494346408639103, 0.0002494346408639103, 0.0002494346408639103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0002494346408639103

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00024943
Iteration 2/1000 | Loss: 0.00003862
Iteration 3/1000 | Loss: 0.00002916
Iteration 4/1000 | Loss: 0.00002654
Iteration 5/1000 | Loss: 0.00002576
Iteration 6/1000 | Loss: 0.00002522
Iteration 7/1000 | Loss: 0.00002474
Iteration 8/1000 | Loss: 0.00002426
Iteration 9/1000 | Loss: 0.00002395
Iteration 10/1000 | Loss: 0.00002377
Iteration 11/1000 | Loss: 0.00002361
Iteration 12/1000 | Loss: 0.00002348
Iteration 13/1000 | Loss: 0.00002347
Iteration 14/1000 | Loss: 0.00002346
Iteration 15/1000 | Loss: 0.00002346
Iteration 16/1000 | Loss: 0.00002345
Iteration 17/1000 | Loss: 0.00002345
Iteration 18/1000 | Loss: 0.00002345
Iteration 19/1000 | Loss: 0.00002345
Iteration 20/1000 | Loss: 0.00002345
Iteration 21/1000 | Loss: 0.00002345
Iteration 22/1000 | Loss: 0.00002345
Iteration 23/1000 | Loss: 0.00002345
Iteration 24/1000 | Loss: 0.00002345
Iteration 25/1000 | Loss: 0.00002345
Iteration 26/1000 | Loss: 0.00002344
Iteration 27/1000 | Loss: 0.00002344
Iteration 28/1000 | Loss: 0.00002344
Iteration 29/1000 | Loss: 0.00002344
Iteration 30/1000 | Loss: 0.00002344
Iteration 31/1000 | Loss: 0.00002343
Iteration 32/1000 | Loss: 0.00002343
Iteration 33/1000 | Loss: 0.00002343
Iteration 34/1000 | Loss: 0.00002342
Iteration 35/1000 | Loss: 0.00002342
Iteration 36/1000 | Loss: 0.00002342
Iteration 37/1000 | Loss: 0.00002341
Iteration 38/1000 | Loss: 0.00002341
Iteration 39/1000 | Loss: 0.00002340
Iteration 40/1000 | Loss: 0.00002340
Iteration 41/1000 | Loss: 0.00002339
Iteration 42/1000 | Loss: 0.00002337
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002337
Iteration 45/1000 | Loss: 0.00002337
Iteration 46/1000 | Loss: 0.00002337
Iteration 47/1000 | Loss: 0.00002337
Iteration 48/1000 | Loss: 0.00002336
Iteration 49/1000 | Loss: 0.00002335
Iteration 50/1000 | Loss: 0.00002335
Iteration 51/1000 | Loss: 0.00002335
Iteration 52/1000 | Loss: 0.00002335
Iteration 53/1000 | Loss: 0.00002335
Iteration 54/1000 | Loss: 0.00002335
Iteration 55/1000 | Loss: 0.00002335
Iteration 56/1000 | Loss: 0.00002335
Iteration 57/1000 | Loss: 0.00002334
Iteration 58/1000 | Loss: 0.00002334
Iteration 59/1000 | Loss: 0.00002334
Iteration 60/1000 | Loss: 0.00002334
Iteration 61/1000 | Loss: 0.00002334
Iteration 62/1000 | Loss: 0.00002334
Iteration 63/1000 | Loss: 0.00002334
Iteration 64/1000 | Loss: 0.00002333
Iteration 65/1000 | Loss: 0.00002333
Iteration 66/1000 | Loss: 0.00002333
Iteration 67/1000 | Loss: 0.00002333
Iteration 68/1000 | Loss: 0.00002333
Iteration 69/1000 | Loss: 0.00002333
Iteration 70/1000 | Loss: 0.00002333
Iteration 71/1000 | Loss: 0.00002333
Iteration 72/1000 | Loss: 0.00002332
Iteration 73/1000 | Loss: 0.00002332
Iteration 74/1000 | Loss: 0.00002332
Iteration 75/1000 | Loss: 0.00002331
Iteration 76/1000 | Loss: 0.00002331
Iteration 77/1000 | Loss: 0.00002331
Iteration 78/1000 | Loss: 0.00002330
Iteration 79/1000 | Loss: 0.00002330
Iteration 80/1000 | Loss: 0.00002330
Iteration 81/1000 | Loss: 0.00002330
Iteration 82/1000 | Loss: 0.00002329
Iteration 83/1000 | Loss: 0.00002329
Iteration 84/1000 | Loss: 0.00002329
Iteration 85/1000 | Loss: 0.00002328
Iteration 86/1000 | Loss: 0.00002328
Iteration 87/1000 | Loss: 0.00002328
Iteration 88/1000 | Loss: 0.00002327
Iteration 89/1000 | Loss: 0.00002327
Iteration 90/1000 | Loss: 0.00002327
Iteration 91/1000 | Loss: 0.00002327
Iteration 92/1000 | Loss: 0.00002327
Iteration 93/1000 | Loss: 0.00002327
Iteration 94/1000 | Loss: 0.00002327
Iteration 95/1000 | Loss: 0.00002327
Iteration 96/1000 | Loss: 0.00002327
Iteration 97/1000 | Loss: 0.00002327
Iteration 98/1000 | Loss: 0.00002327
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [2.3268992663361132e-05, 2.3268992663361132e-05, 2.3268992663361132e-05, 2.3268992663361132e-05, 2.3268992663361132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3268992663361132e-05

Optimization complete. Final v2v error: 4.075855255126953 mm

Highest mean error: 4.706402778625488 mm for frame 145

Lowest mean error: 3.4588918685913086 mm for frame 1

Saving results

Total time: 56.36498785018921
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_cindy_posed_005/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_cindy_posed_005/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387360
Iteration 2/25 | Loss: 0.00100091
Iteration 3/25 | Loss: 0.00092480
Iteration 4/25 | Loss: 0.00091541
Iteration 5/25 | Loss: 0.00091220
Iteration 6/25 | Loss: 0.00091172
Iteration 7/25 | Loss: 0.00091172
Iteration 8/25 | Loss: 0.00091172
Iteration 9/25 | Loss: 0.00091172
Iteration 10/25 | Loss: 0.00091172
Iteration 11/25 | Loss: 0.00091172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009117239387705922, 0.0009117239387705922, 0.0009117239387705922, 0.0009117239387705922, 0.0009117239387705922]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009117239387705922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33355272
Iteration 2/25 | Loss: 0.00062349
Iteration 3/25 | Loss: 0.00062349
Iteration 4/25 | Loss: 0.00062349
Iteration 5/25 | Loss: 0.00062349
Iteration 6/25 | Loss: 0.00062349
Iteration 7/25 | Loss: 0.00062349
Iteration 8/25 | Loss: 0.00062349
Iteration 9/25 | Loss: 0.00062349
Iteration 10/25 | Loss: 0.00062349
Iteration 11/25 | Loss: 0.00062349
Iteration 12/25 | Loss: 0.00062349
Iteration 13/25 | Loss: 0.00062349
Iteration 14/25 | Loss: 0.00062349
Iteration 15/25 | Loss: 0.00062349
Iteration 16/25 | Loss: 0.00062349
Iteration 17/25 | Loss: 0.00062349
Iteration 18/25 | Loss: 0.00062349
Iteration 19/25 | Loss: 0.00062349
Iteration 20/25 | Loss: 0.00062349
Iteration 21/25 | Loss: 0.00062349
Iteration 22/25 | Loss: 0.00062349
Iteration 23/25 | Loss: 0.00062349
Iteration 24/25 | Loss: 0.00062349
Iteration 25/25 | Loss: 0.00062349
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006234897882677615, 0.0006234897882677615, 0.0006234897882677615, 0.0006234897882677615, 0.0006234897882677615]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006234897882677615

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062349
Iteration 2/1000 | Loss: 0.00001293
Iteration 3/1000 | Loss: 0.00000994
Iteration 4/1000 | Loss: 0.00000895
Iteration 5/1000 | Loss: 0.00000862
Iteration 6/1000 | Loss: 0.00000852
Iteration 7/1000 | Loss: 0.00000819
Iteration 8/1000 | Loss: 0.00000815
Iteration 9/1000 | Loss: 0.00000805
Iteration 10/1000 | Loss: 0.00000805
Iteration 11/1000 | Loss: 0.00000801
Iteration 12/1000 | Loss: 0.00000799
Iteration 13/1000 | Loss: 0.00000797
Iteration 14/1000 | Loss: 0.00000797
Iteration 15/1000 | Loss: 0.00000796
Iteration 16/1000 | Loss: 0.00000796
Iteration 17/1000 | Loss: 0.00000796
Iteration 18/1000 | Loss: 0.00000796
Iteration 19/1000 | Loss: 0.00000795
Iteration 20/1000 | Loss: 0.00000795
Iteration 21/1000 | Loss: 0.00000795
Iteration 22/1000 | Loss: 0.00000794
Iteration 23/1000 | Loss: 0.00000793
Iteration 24/1000 | Loss: 0.00000793
Iteration 25/1000 | Loss: 0.00000792
Iteration 26/1000 | Loss: 0.00000792
Iteration 27/1000 | Loss: 0.00000792
Iteration 28/1000 | Loss: 0.00000792
Iteration 29/1000 | Loss: 0.00000792
Iteration 30/1000 | Loss: 0.00000792
Iteration 31/1000 | Loss: 0.00000792
Iteration 32/1000 | Loss: 0.00000791
Iteration 33/1000 | Loss: 0.00000791
Iteration 34/1000 | Loss: 0.00000791
Iteration 35/1000 | Loss: 0.00000790
Iteration 36/1000 | Loss: 0.00000790
Iteration 37/1000 | Loss: 0.00000790
Iteration 38/1000 | Loss: 0.00000790
Iteration 39/1000 | Loss: 0.00000790
Iteration 40/1000 | Loss: 0.00000790
Iteration 41/1000 | Loss: 0.00000790
Iteration 42/1000 | Loss: 0.00000790
Iteration 43/1000 | Loss: 0.00000790
Iteration 44/1000 | Loss: 0.00000790
Iteration 45/1000 | Loss: 0.00000790
Iteration 46/1000 | Loss: 0.00000790
Iteration 47/1000 | Loss: 0.00000789
Iteration 48/1000 | Loss: 0.00000789
Iteration 49/1000 | Loss: 0.00000789
Iteration 50/1000 | Loss: 0.00000789
Iteration 51/1000 | Loss: 0.00000789
Iteration 52/1000 | Loss: 0.00000789
Iteration 53/1000 | Loss: 0.00000789
Iteration 54/1000 | Loss: 0.00000788
Iteration 55/1000 | Loss: 0.00000788
Iteration 56/1000 | Loss: 0.00000788
Iteration 57/1000 | Loss: 0.00000787
Iteration 58/1000 | Loss: 0.00000787
Iteration 59/1000 | Loss: 0.00000787
Iteration 60/1000 | Loss: 0.00000787
Iteration 61/1000 | Loss: 0.00000787
Iteration 62/1000 | Loss: 0.00000787
Iteration 63/1000 | Loss: 0.00000787
Iteration 64/1000 | Loss: 0.00000787
Iteration 65/1000 | Loss: 0.00000787
Iteration 66/1000 | Loss: 0.00000787
Iteration 67/1000 | Loss: 0.00000787
Iteration 68/1000 | Loss: 0.00000787
Iteration 69/1000 | Loss: 0.00000787
Iteration 70/1000 | Loss: 0.00000786
Iteration 71/1000 | Loss: 0.00000786
Iteration 72/1000 | Loss: 0.00000786
Iteration 73/1000 | Loss: 0.00000786
Iteration 74/1000 | Loss: 0.00000786
Iteration 75/1000 | Loss: 0.00000786
Iteration 76/1000 | Loss: 0.00000786
Iteration 77/1000 | Loss: 0.00000786
Iteration 78/1000 | Loss: 0.00000785
Iteration 79/1000 | Loss: 0.00000785
Iteration 80/1000 | Loss: 0.00000785
Iteration 81/1000 | Loss: 0.00000785
Iteration 82/1000 | Loss: 0.00000785
Iteration 83/1000 | Loss: 0.00000785
Iteration 84/1000 | Loss: 0.00000785
Iteration 85/1000 | Loss: 0.00000785
Iteration 86/1000 | Loss: 0.00000785
Iteration 87/1000 | Loss: 0.00000785
Iteration 88/1000 | Loss: 0.00000785
Iteration 89/1000 | Loss: 0.00000785
Iteration 90/1000 | Loss: 0.00000785
Iteration 91/1000 | Loss: 0.00000785
Iteration 92/1000 | Loss: 0.00000785
Iteration 93/1000 | Loss: 0.00000785
Iteration 94/1000 | Loss: 0.00000785
Iteration 95/1000 | Loss: 0.00000785
Iteration 96/1000 | Loss: 0.00000785
Iteration 97/1000 | Loss: 0.00000785
Iteration 98/1000 | Loss: 0.00000785
Iteration 99/1000 | Loss: 0.00000785
Iteration 100/1000 | Loss: 0.00000785
Iteration 101/1000 | Loss: 0.00000785
Iteration 102/1000 | Loss: 0.00000785
Iteration 103/1000 | Loss: 0.00000785
Iteration 104/1000 | Loss: 0.00000785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [7.84883104643086e-06, 7.84883104643086e-06, 7.84883104643086e-06, 7.84883104643086e-06, 7.84883104643086e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.84883104643086e-06

Optimization complete. Final v2v error: 2.3760461807250977 mm

Highest mean error: 2.5761969089508057 mm for frame 210

Lowest mean error: 2.3211376667022705 mm for frame 57

Saving results

Total time: 28.02603507041931
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00761965
Iteration 2/25 | Loss: 0.00176680
Iteration 3/25 | Loss: 0.00150078
Iteration 4/25 | Loss: 0.00146871
Iteration 5/25 | Loss: 0.00145933
Iteration 6/25 | Loss: 0.00144699
Iteration 7/25 | Loss: 0.00144138
Iteration 8/25 | Loss: 0.00143615
Iteration 9/25 | Loss: 0.00143077
Iteration 10/25 | Loss: 0.00142294
Iteration 11/25 | Loss: 0.00141972
Iteration 12/25 | Loss: 0.00141931
Iteration 13/25 | Loss: 0.00141930
Iteration 14/25 | Loss: 0.00141930
Iteration 15/25 | Loss: 0.00141930
Iteration 16/25 | Loss: 0.00141929
Iteration 17/25 | Loss: 0.00141929
Iteration 18/25 | Loss: 0.00141929
Iteration 19/25 | Loss: 0.00141929
Iteration 20/25 | Loss: 0.00141929
Iteration 21/25 | Loss: 0.00141929
Iteration 22/25 | Loss: 0.00141929
Iteration 23/25 | Loss: 0.00141929
Iteration 24/25 | Loss: 0.00141929
Iteration 25/25 | Loss: 0.00141929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.37176061
Iteration 2/25 | Loss: 0.00189246
Iteration 3/25 | Loss: 0.00189216
Iteration 4/25 | Loss: 0.00189216
Iteration 5/25 | Loss: 0.00189216
Iteration 6/25 | Loss: 0.00189216
Iteration 7/25 | Loss: 0.00189216
Iteration 8/25 | Loss: 0.00189216
Iteration 9/25 | Loss: 0.00189216
Iteration 10/25 | Loss: 0.00189216
Iteration 11/25 | Loss: 0.00189216
Iteration 12/25 | Loss: 0.00189216
Iteration 13/25 | Loss: 0.00189216
Iteration 14/25 | Loss: 0.00189216
Iteration 15/25 | Loss: 0.00189216
Iteration 16/25 | Loss: 0.00189216
Iteration 17/25 | Loss: 0.00189216
Iteration 18/25 | Loss: 0.00189216
Iteration 19/25 | Loss: 0.00189216
Iteration 20/25 | Loss: 0.00189216
Iteration 21/25 | Loss: 0.00189216
Iteration 22/25 | Loss: 0.00189216
Iteration 23/25 | Loss: 0.00189216
Iteration 24/25 | Loss: 0.00189216
Iteration 25/25 | Loss: 0.00189216

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189216
Iteration 2/1000 | Loss: 0.00006821
Iteration 3/1000 | Loss: 0.00005053
Iteration 4/1000 | Loss: 0.00004392
Iteration 5/1000 | Loss: 0.00004053
Iteration 6/1000 | Loss: 0.00003846
Iteration 7/1000 | Loss: 0.00003658
Iteration 8/1000 | Loss: 0.00003534
Iteration 9/1000 | Loss: 0.00003456
Iteration 10/1000 | Loss: 0.00003399
Iteration 11/1000 | Loss: 0.00003362
Iteration 12/1000 | Loss: 0.00003323
Iteration 13/1000 | Loss: 0.00003295
Iteration 14/1000 | Loss: 0.00003274
Iteration 15/1000 | Loss: 0.00003251
Iteration 16/1000 | Loss: 0.00003241
Iteration 17/1000 | Loss: 0.00003236
Iteration 18/1000 | Loss: 0.00003236
Iteration 19/1000 | Loss: 0.00003231
Iteration 20/1000 | Loss: 0.00003227
Iteration 21/1000 | Loss: 0.00003226
Iteration 22/1000 | Loss: 0.00003226
Iteration 23/1000 | Loss: 0.00003225
Iteration 24/1000 | Loss: 0.00003225
Iteration 25/1000 | Loss: 0.00003225
Iteration 26/1000 | Loss: 0.00003224
Iteration 27/1000 | Loss: 0.00003224
Iteration 28/1000 | Loss: 0.00003223
Iteration 29/1000 | Loss: 0.00003220
Iteration 30/1000 | Loss: 0.00003216
Iteration 31/1000 | Loss: 0.00003212
Iteration 32/1000 | Loss: 0.00003212
Iteration 33/1000 | Loss: 0.00003211
Iteration 34/1000 | Loss: 0.00003211
Iteration 35/1000 | Loss: 0.00003211
Iteration 36/1000 | Loss: 0.00003210
Iteration 37/1000 | Loss: 0.00003210
Iteration 38/1000 | Loss: 0.00003209
Iteration 39/1000 | Loss: 0.00003209
Iteration 40/1000 | Loss: 0.00003208
Iteration 41/1000 | Loss: 0.00003206
Iteration 42/1000 | Loss: 0.00003205
Iteration 43/1000 | Loss: 0.00003205
Iteration 44/1000 | Loss: 0.00003205
Iteration 45/1000 | Loss: 0.00003205
Iteration 46/1000 | Loss: 0.00003205
Iteration 47/1000 | Loss: 0.00003204
Iteration 48/1000 | Loss: 0.00003204
Iteration 49/1000 | Loss: 0.00003203
Iteration 50/1000 | Loss: 0.00003202
Iteration 51/1000 | Loss: 0.00003202
Iteration 52/1000 | Loss: 0.00003201
Iteration 53/1000 | Loss: 0.00003201
Iteration 54/1000 | Loss: 0.00003201
Iteration 55/1000 | Loss: 0.00003200
Iteration 56/1000 | Loss: 0.00003200
Iteration 57/1000 | Loss: 0.00003200
Iteration 58/1000 | Loss: 0.00003199
Iteration 59/1000 | Loss: 0.00003199
Iteration 60/1000 | Loss: 0.00003199
Iteration 61/1000 | Loss: 0.00003199
Iteration 62/1000 | Loss: 0.00003197
Iteration 63/1000 | Loss: 0.00003197
Iteration 64/1000 | Loss: 0.00003197
Iteration 65/1000 | Loss: 0.00003196
Iteration 66/1000 | Loss: 0.00003196
Iteration 67/1000 | Loss: 0.00003196
Iteration 68/1000 | Loss: 0.00003196
Iteration 69/1000 | Loss: 0.00003196
Iteration 70/1000 | Loss: 0.00003195
Iteration 71/1000 | Loss: 0.00003195
Iteration 72/1000 | Loss: 0.00003195
Iteration 73/1000 | Loss: 0.00003195
Iteration 74/1000 | Loss: 0.00003195
Iteration 75/1000 | Loss: 0.00003194
Iteration 76/1000 | Loss: 0.00003194
Iteration 77/1000 | Loss: 0.00003194
Iteration 78/1000 | Loss: 0.00003193
Iteration 79/1000 | Loss: 0.00003193
Iteration 80/1000 | Loss: 0.00003193
Iteration 81/1000 | Loss: 0.00003193
Iteration 82/1000 | Loss: 0.00003193
Iteration 83/1000 | Loss: 0.00003192
Iteration 84/1000 | Loss: 0.00003192
Iteration 85/1000 | Loss: 0.00003192
Iteration 86/1000 | Loss: 0.00003192
Iteration 87/1000 | Loss: 0.00003192
Iteration 88/1000 | Loss: 0.00003192
Iteration 89/1000 | Loss: 0.00003192
Iteration 90/1000 | Loss: 0.00003192
Iteration 91/1000 | Loss: 0.00003192
Iteration 92/1000 | Loss: 0.00003192
Iteration 93/1000 | Loss: 0.00003192
Iteration 94/1000 | Loss: 0.00003192
Iteration 95/1000 | Loss: 0.00003192
Iteration 96/1000 | Loss: 0.00003192
Iteration 97/1000 | Loss: 0.00003192
Iteration 98/1000 | Loss: 0.00003192
Iteration 99/1000 | Loss: 0.00003192
Iteration 100/1000 | Loss: 0.00003192
Iteration 101/1000 | Loss: 0.00003192
Iteration 102/1000 | Loss: 0.00003191
Iteration 103/1000 | Loss: 0.00003191
Iteration 104/1000 | Loss: 0.00003191
Iteration 105/1000 | Loss: 0.00003191
Iteration 106/1000 | Loss: 0.00003191
Iteration 107/1000 | Loss: 0.00003191
Iteration 108/1000 | Loss: 0.00003190
Iteration 109/1000 | Loss: 0.00003190
Iteration 110/1000 | Loss: 0.00003190
Iteration 111/1000 | Loss: 0.00003190
Iteration 112/1000 | Loss: 0.00003190
Iteration 113/1000 | Loss: 0.00003190
Iteration 114/1000 | Loss: 0.00003190
Iteration 115/1000 | Loss: 0.00003190
Iteration 116/1000 | Loss: 0.00003190
Iteration 117/1000 | Loss: 0.00003190
Iteration 118/1000 | Loss: 0.00003190
Iteration 119/1000 | Loss: 0.00003190
Iteration 120/1000 | Loss: 0.00003190
Iteration 121/1000 | Loss: 0.00003190
Iteration 122/1000 | Loss: 0.00003190
Iteration 123/1000 | Loss: 0.00003190
Iteration 124/1000 | Loss: 0.00003190
Iteration 125/1000 | Loss: 0.00003190
Iteration 126/1000 | Loss: 0.00003190
Iteration 127/1000 | Loss: 0.00003190
Iteration 128/1000 | Loss: 0.00003190
Iteration 129/1000 | Loss: 0.00003190
Iteration 130/1000 | Loss: 0.00003190
Iteration 131/1000 | Loss: 0.00003190
Iteration 132/1000 | Loss: 0.00003190
Iteration 133/1000 | Loss: 0.00003190
Iteration 134/1000 | Loss: 0.00003190
Iteration 135/1000 | Loss: 0.00003190
Iteration 136/1000 | Loss: 0.00003190
Iteration 137/1000 | Loss: 0.00003190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [3.189879498677328e-05, 3.189879498677328e-05, 3.189879498677328e-05, 3.189879498677328e-05, 3.189879498677328e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.189879498677328e-05

Optimization complete. Final v2v error: 4.830857753753662 mm

Highest mean error: 6.628054618835449 mm for frame 131

Lowest mean error: 4.100366115570068 mm for frame 170

Saving results

Total time: 63.62048864364624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919247
Iteration 2/25 | Loss: 0.00159532
Iteration 3/25 | Loss: 0.00145256
Iteration 4/25 | Loss: 0.00143237
Iteration 5/25 | Loss: 0.00142609
Iteration 6/25 | Loss: 0.00142481
Iteration 7/25 | Loss: 0.00142481
Iteration 8/25 | Loss: 0.00142481
Iteration 9/25 | Loss: 0.00142481
Iteration 10/25 | Loss: 0.00142481
Iteration 11/25 | Loss: 0.00142481
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014248136430978775, 0.0014248136430978775, 0.0014248136430978775, 0.0014248136430978775, 0.0014248136430978775]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014248136430978775

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48304391
Iteration 2/25 | Loss: 0.00157905
Iteration 3/25 | Loss: 0.00157900
Iteration 4/25 | Loss: 0.00157900
Iteration 5/25 | Loss: 0.00157900
Iteration 6/25 | Loss: 0.00157900
Iteration 7/25 | Loss: 0.00157900
Iteration 8/25 | Loss: 0.00157900
Iteration 9/25 | Loss: 0.00157900
Iteration 10/25 | Loss: 0.00157900
Iteration 11/25 | Loss: 0.00157900
Iteration 12/25 | Loss: 0.00157900
Iteration 13/25 | Loss: 0.00157900
Iteration 14/25 | Loss: 0.00157900
Iteration 15/25 | Loss: 0.00157900
Iteration 16/25 | Loss: 0.00157900
Iteration 17/25 | Loss: 0.00157900
Iteration 18/25 | Loss: 0.00157900
Iteration 19/25 | Loss: 0.00157900
Iteration 20/25 | Loss: 0.00157900
Iteration 21/25 | Loss: 0.00157900
Iteration 22/25 | Loss: 0.00157900
Iteration 23/25 | Loss: 0.00157900
Iteration 24/25 | Loss: 0.00157900
Iteration 25/25 | Loss: 0.00157900

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157900
Iteration 2/1000 | Loss: 0.00006880
Iteration 3/1000 | Loss: 0.00004563
Iteration 4/1000 | Loss: 0.00003764
Iteration 5/1000 | Loss: 0.00003444
Iteration 6/1000 | Loss: 0.00003268
Iteration 7/1000 | Loss: 0.00003157
Iteration 8/1000 | Loss: 0.00003090
Iteration 9/1000 | Loss: 0.00003039
Iteration 10/1000 | Loss: 0.00002994
Iteration 11/1000 | Loss: 0.00002957
Iteration 12/1000 | Loss: 0.00002940
Iteration 13/1000 | Loss: 0.00002919
Iteration 14/1000 | Loss: 0.00002916
Iteration 15/1000 | Loss: 0.00002911
Iteration 16/1000 | Loss: 0.00002901
Iteration 17/1000 | Loss: 0.00002893
Iteration 18/1000 | Loss: 0.00002891
Iteration 19/1000 | Loss: 0.00002890
Iteration 20/1000 | Loss: 0.00002890
Iteration 21/1000 | Loss: 0.00002890
Iteration 22/1000 | Loss: 0.00002890
Iteration 23/1000 | Loss: 0.00002889
Iteration 24/1000 | Loss: 0.00002889
Iteration 25/1000 | Loss: 0.00002889
Iteration 26/1000 | Loss: 0.00002889
Iteration 27/1000 | Loss: 0.00002888
Iteration 28/1000 | Loss: 0.00002888
Iteration 29/1000 | Loss: 0.00002887
Iteration 30/1000 | Loss: 0.00002887
Iteration 31/1000 | Loss: 0.00002887
Iteration 32/1000 | Loss: 0.00002887
Iteration 33/1000 | Loss: 0.00002886
Iteration 34/1000 | Loss: 0.00002885
Iteration 35/1000 | Loss: 0.00002885
Iteration 36/1000 | Loss: 0.00002885
Iteration 37/1000 | Loss: 0.00002884
Iteration 38/1000 | Loss: 0.00002884
Iteration 39/1000 | Loss: 0.00002883
Iteration 40/1000 | Loss: 0.00002880
Iteration 41/1000 | Loss: 0.00002880
Iteration 42/1000 | Loss: 0.00002880
Iteration 43/1000 | Loss: 0.00002880
Iteration 44/1000 | Loss: 0.00002879
Iteration 45/1000 | Loss: 0.00002879
Iteration 46/1000 | Loss: 0.00002879
Iteration 47/1000 | Loss: 0.00002878
Iteration 48/1000 | Loss: 0.00002878
Iteration 49/1000 | Loss: 0.00002877
Iteration 50/1000 | Loss: 0.00002877
Iteration 51/1000 | Loss: 0.00002876
Iteration 52/1000 | Loss: 0.00002876
Iteration 53/1000 | Loss: 0.00002876
Iteration 54/1000 | Loss: 0.00002875
Iteration 55/1000 | Loss: 0.00002875
Iteration 56/1000 | Loss: 0.00002874
Iteration 57/1000 | Loss: 0.00002873
Iteration 58/1000 | Loss: 0.00002873
Iteration 59/1000 | Loss: 0.00002873
Iteration 60/1000 | Loss: 0.00002872
Iteration 61/1000 | Loss: 0.00002872
Iteration 62/1000 | Loss: 0.00002872
Iteration 63/1000 | Loss: 0.00002872
Iteration 64/1000 | Loss: 0.00002872
Iteration 65/1000 | Loss: 0.00002871
Iteration 66/1000 | Loss: 0.00002871
Iteration 67/1000 | Loss: 0.00002871
Iteration 68/1000 | Loss: 0.00002871
Iteration 69/1000 | Loss: 0.00002871
Iteration 70/1000 | Loss: 0.00002871
Iteration 71/1000 | Loss: 0.00002871
Iteration 72/1000 | Loss: 0.00002871
Iteration 73/1000 | Loss: 0.00002870
Iteration 74/1000 | Loss: 0.00002870
Iteration 75/1000 | Loss: 0.00002870
Iteration 76/1000 | Loss: 0.00002869
Iteration 77/1000 | Loss: 0.00002869
Iteration 78/1000 | Loss: 0.00002869
Iteration 79/1000 | Loss: 0.00002869
Iteration 80/1000 | Loss: 0.00002869
Iteration 81/1000 | Loss: 0.00002869
Iteration 82/1000 | Loss: 0.00002869
Iteration 83/1000 | Loss: 0.00002868
Iteration 84/1000 | Loss: 0.00002868
Iteration 85/1000 | Loss: 0.00002868
Iteration 86/1000 | Loss: 0.00002868
Iteration 87/1000 | Loss: 0.00002868
Iteration 88/1000 | Loss: 0.00002867
Iteration 89/1000 | Loss: 0.00002867
Iteration 90/1000 | Loss: 0.00002867
Iteration 91/1000 | Loss: 0.00002867
Iteration 92/1000 | Loss: 0.00002867
Iteration 93/1000 | Loss: 0.00002866
Iteration 94/1000 | Loss: 0.00002866
Iteration 95/1000 | Loss: 0.00002866
Iteration 96/1000 | Loss: 0.00002866
Iteration 97/1000 | Loss: 0.00002866
Iteration 98/1000 | Loss: 0.00002866
Iteration 99/1000 | Loss: 0.00002866
Iteration 100/1000 | Loss: 0.00002865
Iteration 101/1000 | Loss: 0.00002865
Iteration 102/1000 | Loss: 0.00002865
Iteration 103/1000 | Loss: 0.00002865
Iteration 104/1000 | Loss: 0.00002865
Iteration 105/1000 | Loss: 0.00002865
Iteration 106/1000 | Loss: 0.00002865
Iteration 107/1000 | Loss: 0.00002865
Iteration 108/1000 | Loss: 0.00002865
Iteration 109/1000 | Loss: 0.00002864
Iteration 110/1000 | Loss: 0.00002864
Iteration 111/1000 | Loss: 0.00002864
Iteration 112/1000 | Loss: 0.00002864
Iteration 113/1000 | Loss: 0.00002864
Iteration 114/1000 | Loss: 0.00002863
Iteration 115/1000 | Loss: 0.00002863
Iteration 116/1000 | Loss: 0.00002863
Iteration 117/1000 | Loss: 0.00002863
Iteration 118/1000 | Loss: 0.00002862
Iteration 119/1000 | Loss: 0.00002862
Iteration 120/1000 | Loss: 0.00002862
Iteration 121/1000 | Loss: 0.00002862
Iteration 122/1000 | Loss: 0.00002862
Iteration 123/1000 | Loss: 0.00002862
Iteration 124/1000 | Loss: 0.00002862
Iteration 125/1000 | Loss: 0.00002862
Iteration 126/1000 | Loss: 0.00002862
Iteration 127/1000 | Loss: 0.00002862
Iteration 128/1000 | Loss: 0.00002862
Iteration 129/1000 | Loss: 0.00002862
Iteration 130/1000 | Loss: 0.00002862
Iteration 131/1000 | Loss: 0.00002862
Iteration 132/1000 | Loss: 0.00002862
Iteration 133/1000 | Loss: 0.00002862
Iteration 134/1000 | Loss: 0.00002862
Iteration 135/1000 | Loss: 0.00002862
Iteration 136/1000 | Loss: 0.00002862
Iteration 137/1000 | Loss: 0.00002862
Iteration 138/1000 | Loss: 0.00002862
Iteration 139/1000 | Loss: 0.00002862
Iteration 140/1000 | Loss: 0.00002862
Iteration 141/1000 | Loss: 0.00002862
Iteration 142/1000 | Loss: 0.00002862
Iteration 143/1000 | Loss: 0.00002862
Iteration 144/1000 | Loss: 0.00002862
Iteration 145/1000 | Loss: 0.00002862
Iteration 146/1000 | Loss: 0.00002862
Iteration 147/1000 | Loss: 0.00002862
Iteration 148/1000 | Loss: 0.00002862
Iteration 149/1000 | Loss: 0.00002862
Iteration 150/1000 | Loss: 0.00002862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.8621881938306615e-05, 2.8621881938306615e-05, 2.8621881938306615e-05, 2.8621881938306615e-05, 2.8621881938306615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8621881938306615e-05

Optimization complete. Final v2v error: 4.636373519897461 mm

Highest mean error: 5.852411270141602 mm for frame 116

Lowest mean error: 3.987239122390747 mm for frame 14

Saving results

Total time: 44.093013048172
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029912
Iteration 2/25 | Loss: 0.00279136
Iteration 3/25 | Loss: 0.00193902
Iteration 4/25 | Loss: 0.00175814
Iteration 5/25 | Loss: 0.00167263
Iteration 6/25 | Loss: 0.00159383
Iteration 7/25 | Loss: 0.00150718
Iteration 8/25 | Loss: 0.00145851
Iteration 9/25 | Loss: 0.00144175
Iteration 10/25 | Loss: 0.00143395
Iteration 11/25 | Loss: 0.00143974
Iteration 12/25 | Loss: 0.00142187
Iteration 13/25 | Loss: 0.00141046
Iteration 14/25 | Loss: 0.00141346
Iteration 15/25 | Loss: 0.00141458
Iteration 16/25 | Loss: 0.00141407
Iteration 17/25 | Loss: 0.00141088
Iteration 18/25 | Loss: 0.00140950
Iteration 19/25 | Loss: 0.00141038
Iteration 20/25 | Loss: 0.00141090
Iteration 21/25 | Loss: 0.00140996
Iteration 22/25 | Loss: 0.00140972
Iteration 23/25 | Loss: 0.00140971
Iteration 24/25 | Loss: 0.00140873
Iteration 25/25 | Loss: 0.00140794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.92245114
Iteration 2/25 | Loss: 0.00203285
Iteration 3/25 | Loss: 0.00202622
Iteration 4/25 | Loss: 0.00202622
Iteration 5/25 | Loss: 0.00202622
Iteration 6/25 | Loss: 0.00202622
Iteration 7/25 | Loss: 0.00202622
Iteration 8/25 | Loss: 0.00202622
Iteration 9/25 | Loss: 0.00202622
Iteration 10/25 | Loss: 0.00202622
Iteration 11/25 | Loss: 0.00202622
Iteration 12/25 | Loss: 0.00202622
Iteration 13/25 | Loss: 0.00202622
Iteration 14/25 | Loss: 0.00202622
Iteration 15/25 | Loss: 0.00202622
Iteration 16/25 | Loss: 0.00202622
Iteration 17/25 | Loss: 0.00202622
Iteration 18/25 | Loss: 0.00202622
Iteration 19/25 | Loss: 0.00202622
Iteration 20/25 | Loss: 0.00202622
Iteration 21/25 | Loss: 0.00202622
Iteration 22/25 | Loss: 0.00202622
Iteration 23/25 | Loss: 0.00202622
Iteration 24/25 | Loss: 0.00202622
Iteration 25/25 | Loss: 0.00202622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00202622
Iteration 2/1000 | Loss: 0.00029185
Iteration 3/1000 | Loss: 0.00013682
Iteration 4/1000 | Loss: 0.00009163
Iteration 5/1000 | Loss: 0.00005345
Iteration 6/1000 | Loss: 0.00004670
Iteration 7/1000 | Loss: 0.00004235
Iteration 8/1000 | Loss: 0.00003974
Iteration 9/1000 | Loss: 0.00003799
Iteration 10/1000 | Loss: 0.00003699
Iteration 11/1000 | Loss: 0.00021383
Iteration 12/1000 | Loss: 0.00004211
Iteration 13/1000 | Loss: 0.00003933
Iteration 14/1000 | Loss: 0.00003764
Iteration 15/1000 | Loss: 0.00003714
Iteration 16/1000 | Loss: 0.00003661
Iteration 17/1000 | Loss: 0.00003610
Iteration 18/1000 | Loss: 0.00003569
Iteration 19/1000 | Loss: 0.00003535
Iteration 20/1000 | Loss: 0.00003527
Iteration 21/1000 | Loss: 0.00003525
Iteration 22/1000 | Loss: 0.00043259
Iteration 23/1000 | Loss: 0.00026507
Iteration 24/1000 | Loss: 0.00004708
Iteration 25/1000 | Loss: 0.00021801
Iteration 26/1000 | Loss: 0.00004041
Iteration 27/1000 | Loss: 0.00018600
Iteration 28/1000 | Loss: 0.00021643
Iteration 29/1000 | Loss: 0.00012500
Iteration 30/1000 | Loss: 0.00010333
Iteration 31/1000 | Loss: 0.00004462
Iteration 32/1000 | Loss: 0.00004074
Iteration 33/1000 | Loss: 0.00018610
Iteration 34/1000 | Loss: 0.00011528
Iteration 35/1000 | Loss: 0.00003798
Iteration 36/1000 | Loss: 0.00003627
Iteration 37/1000 | Loss: 0.00003577
Iteration 38/1000 | Loss: 0.00003550
Iteration 39/1000 | Loss: 0.00003544
Iteration 40/1000 | Loss: 0.00003542
Iteration 41/1000 | Loss: 0.00003521
Iteration 42/1000 | Loss: 0.00020801
Iteration 43/1000 | Loss: 0.00003951
Iteration 44/1000 | Loss: 0.00003761
Iteration 45/1000 | Loss: 0.00003703
Iteration 46/1000 | Loss: 0.00003656
Iteration 47/1000 | Loss: 0.00003616
Iteration 48/1000 | Loss: 0.00003553
Iteration 49/1000 | Loss: 0.00003524
Iteration 50/1000 | Loss: 0.00003507
Iteration 51/1000 | Loss: 0.00003489
Iteration 52/1000 | Loss: 0.00003472
Iteration 53/1000 | Loss: 0.00022804
Iteration 54/1000 | Loss: 0.00013390
Iteration 55/1000 | Loss: 0.00028180
Iteration 56/1000 | Loss: 0.00017298
Iteration 57/1000 | Loss: 0.00020564
Iteration 58/1000 | Loss: 0.00004740
Iteration 59/1000 | Loss: 0.00004199
Iteration 60/1000 | Loss: 0.00003944
Iteration 61/1000 | Loss: 0.00003813
Iteration 62/1000 | Loss: 0.00003726
Iteration 63/1000 | Loss: 0.00003668
Iteration 64/1000 | Loss: 0.00003631
Iteration 65/1000 | Loss: 0.00003586
Iteration 66/1000 | Loss: 0.00004594
Iteration 67/1000 | Loss: 0.00003739
Iteration 68/1000 | Loss: 0.00003620
Iteration 69/1000 | Loss: 0.00003578
Iteration 70/1000 | Loss: 0.00003570
Iteration 71/1000 | Loss: 0.00003568
Iteration 72/1000 | Loss: 0.00003568
Iteration 73/1000 | Loss: 0.00003568
Iteration 74/1000 | Loss: 0.00003567
Iteration 75/1000 | Loss: 0.00003566
Iteration 76/1000 | Loss: 0.00003566
Iteration 77/1000 | Loss: 0.00003566
Iteration 78/1000 | Loss: 0.00003566
Iteration 79/1000 | Loss: 0.00003566
Iteration 80/1000 | Loss: 0.00003566
Iteration 81/1000 | Loss: 0.00003566
Iteration 82/1000 | Loss: 0.00003566
Iteration 83/1000 | Loss: 0.00003566
Iteration 84/1000 | Loss: 0.00003566
Iteration 85/1000 | Loss: 0.00003566
Iteration 86/1000 | Loss: 0.00003565
Iteration 87/1000 | Loss: 0.00003565
Iteration 88/1000 | Loss: 0.00003564
Iteration 89/1000 | Loss: 0.00003564
Iteration 90/1000 | Loss: 0.00003563
Iteration 91/1000 | Loss: 0.00003562
Iteration 92/1000 | Loss: 0.00003562
Iteration 93/1000 | Loss: 0.00003561
Iteration 94/1000 | Loss: 0.00003561
Iteration 95/1000 | Loss: 0.00003561
Iteration 96/1000 | Loss: 0.00003561
Iteration 97/1000 | Loss: 0.00003561
Iteration 98/1000 | Loss: 0.00003561
Iteration 99/1000 | Loss: 0.00003561
Iteration 100/1000 | Loss: 0.00003561
Iteration 101/1000 | Loss: 0.00003561
Iteration 102/1000 | Loss: 0.00003560
Iteration 103/1000 | Loss: 0.00003560
Iteration 104/1000 | Loss: 0.00003559
Iteration 105/1000 | Loss: 0.00003559
Iteration 106/1000 | Loss: 0.00003559
Iteration 107/1000 | Loss: 0.00003553
Iteration 108/1000 | Loss: 0.00003533
Iteration 109/1000 | Loss: 0.00003496
Iteration 110/1000 | Loss: 0.00003449
Iteration 111/1000 | Loss: 0.00003417
Iteration 112/1000 | Loss: 0.00003398
Iteration 113/1000 | Loss: 0.00003398
Iteration 114/1000 | Loss: 0.00003397
Iteration 115/1000 | Loss: 0.00003394
Iteration 116/1000 | Loss: 0.00003386
Iteration 117/1000 | Loss: 0.00003385
Iteration 118/1000 | Loss: 0.00003385
Iteration 119/1000 | Loss: 0.00003384
Iteration 120/1000 | Loss: 0.00003383
Iteration 121/1000 | Loss: 0.00003382
Iteration 122/1000 | Loss: 0.00003382
Iteration 123/1000 | Loss: 0.00003381
Iteration 124/1000 | Loss: 0.00003381
Iteration 125/1000 | Loss: 0.00003381
Iteration 126/1000 | Loss: 0.00003381
Iteration 127/1000 | Loss: 0.00003381
Iteration 128/1000 | Loss: 0.00003381
Iteration 129/1000 | Loss: 0.00003381
Iteration 130/1000 | Loss: 0.00003380
Iteration 131/1000 | Loss: 0.00003380
Iteration 132/1000 | Loss: 0.00003380
Iteration 133/1000 | Loss: 0.00003380
Iteration 134/1000 | Loss: 0.00003380
Iteration 135/1000 | Loss: 0.00003380
Iteration 136/1000 | Loss: 0.00003380
Iteration 137/1000 | Loss: 0.00003380
Iteration 138/1000 | Loss: 0.00003380
Iteration 139/1000 | Loss: 0.00003380
Iteration 140/1000 | Loss: 0.00003380
Iteration 141/1000 | Loss: 0.00003380
Iteration 142/1000 | Loss: 0.00003380
Iteration 143/1000 | Loss: 0.00003380
Iteration 144/1000 | Loss: 0.00003380
Iteration 145/1000 | Loss: 0.00003380
Iteration 146/1000 | Loss: 0.00003380
Iteration 147/1000 | Loss: 0.00003380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [3.379806366865523e-05, 3.379806366865523e-05, 3.379806366865523e-05, 3.379806366865523e-05, 3.379806366865523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.379806366865523e-05

Optimization complete. Final v2v error: 4.889355182647705 mm

Highest mean error: 10.863225936889648 mm for frame 200

Lowest mean error: 4.266233444213867 mm for frame 198

Saving results

Total time: 177.05870938301086
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109686
Iteration 2/25 | Loss: 0.00298464
Iteration 3/25 | Loss: 0.00240071
Iteration 4/25 | Loss: 0.00239152
Iteration 5/25 | Loss: 0.00218112
Iteration 6/25 | Loss: 0.00199095
Iteration 7/25 | Loss: 0.00185466
Iteration 8/25 | Loss: 0.00177464
Iteration 9/25 | Loss: 0.00177209
Iteration 10/25 | Loss: 0.00174025
Iteration 11/25 | Loss: 0.00170142
Iteration 12/25 | Loss: 0.00163764
Iteration 13/25 | Loss: 0.00161259
Iteration 14/25 | Loss: 0.00159025
Iteration 15/25 | Loss: 0.00158763
Iteration 16/25 | Loss: 0.00157454
Iteration 17/25 | Loss: 0.00157096
Iteration 18/25 | Loss: 0.00155186
Iteration 19/25 | Loss: 0.00154333
Iteration 20/25 | Loss: 0.00153414
Iteration 21/25 | Loss: 0.00151619
Iteration 22/25 | Loss: 0.00150473
Iteration 23/25 | Loss: 0.00150271
Iteration 24/25 | Loss: 0.00150771
Iteration 25/25 | Loss: 0.00150076

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.53967261
Iteration 2/25 | Loss: 0.00313273
Iteration 3/25 | Loss: 0.00313273
Iteration 4/25 | Loss: 0.00313273
Iteration 5/25 | Loss: 0.00313273
Iteration 6/25 | Loss: 0.00313273
Iteration 7/25 | Loss: 0.00313273
Iteration 8/25 | Loss: 0.00313273
Iteration 9/25 | Loss: 0.00313273
Iteration 10/25 | Loss: 0.00313273
Iteration 11/25 | Loss: 0.00313273
Iteration 12/25 | Loss: 0.00313273
Iteration 13/25 | Loss: 0.00313273
Iteration 14/25 | Loss: 0.00313273
Iteration 15/25 | Loss: 0.00313273
Iteration 16/25 | Loss: 0.00313273
Iteration 17/25 | Loss: 0.00313273
Iteration 18/25 | Loss: 0.00313273
Iteration 19/25 | Loss: 0.00313273
Iteration 20/25 | Loss: 0.00313273
Iteration 21/25 | Loss: 0.00313273
Iteration 22/25 | Loss: 0.00313273
Iteration 23/25 | Loss: 0.00313273
Iteration 24/25 | Loss: 0.00313273
Iteration 25/25 | Loss: 0.00313273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00313273
Iteration 2/1000 | Loss: 0.00063927
Iteration 3/1000 | Loss: 0.00227914
Iteration 4/1000 | Loss: 0.00020754
Iteration 5/1000 | Loss: 0.00013937
Iteration 6/1000 | Loss: 0.00010527
Iteration 7/1000 | Loss: 0.00007829
Iteration 8/1000 | Loss: 0.00006410
Iteration 9/1000 | Loss: 0.00020791
Iteration 10/1000 | Loss: 0.00005399
Iteration 11/1000 | Loss: 0.00004981
Iteration 12/1000 | Loss: 0.00004713
Iteration 13/1000 | Loss: 0.00004480
Iteration 14/1000 | Loss: 0.00004335
Iteration 15/1000 | Loss: 0.00032204
Iteration 16/1000 | Loss: 0.00005534
Iteration 17/1000 | Loss: 0.00004907
Iteration 18/1000 | Loss: 0.00004617
Iteration 19/1000 | Loss: 0.00005835
Iteration 20/1000 | Loss: 0.00004499
Iteration 21/1000 | Loss: 0.00004295
Iteration 22/1000 | Loss: 0.00004150
Iteration 23/1000 | Loss: 0.00004057
Iteration 24/1000 | Loss: 0.00004012
Iteration 25/1000 | Loss: 0.00003978
Iteration 26/1000 | Loss: 0.00003956
Iteration 27/1000 | Loss: 0.00003943
Iteration 28/1000 | Loss: 0.00003936
Iteration 29/1000 | Loss: 0.00003934
Iteration 30/1000 | Loss: 0.00025748
Iteration 31/1000 | Loss: 0.00004846
Iteration 32/1000 | Loss: 0.00004156
Iteration 33/1000 | Loss: 0.00003975
Iteration 34/1000 | Loss: 0.00003852
Iteration 35/1000 | Loss: 0.00003755
Iteration 36/1000 | Loss: 0.00003712
Iteration 37/1000 | Loss: 0.00003685
Iteration 38/1000 | Loss: 0.00003676
Iteration 39/1000 | Loss: 0.00003676
Iteration 40/1000 | Loss: 0.00003671
Iteration 41/1000 | Loss: 0.00003670
Iteration 42/1000 | Loss: 0.00003666
Iteration 43/1000 | Loss: 0.00003665
Iteration 44/1000 | Loss: 0.00003660
Iteration 45/1000 | Loss: 0.00003660
Iteration 46/1000 | Loss: 0.00003660
Iteration 47/1000 | Loss: 0.00003659
Iteration 48/1000 | Loss: 0.00003659
Iteration 49/1000 | Loss: 0.00003658
Iteration 50/1000 | Loss: 0.00003657
Iteration 51/1000 | Loss: 0.00003657
Iteration 52/1000 | Loss: 0.00003657
Iteration 53/1000 | Loss: 0.00003657
Iteration 54/1000 | Loss: 0.00003656
Iteration 55/1000 | Loss: 0.00003656
Iteration 56/1000 | Loss: 0.00003656
Iteration 57/1000 | Loss: 0.00003656
Iteration 58/1000 | Loss: 0.00003656
Iteration 59/1000 | Loss: 0.00003656
Iteration 60/1000 | Loss: 0.00003655
Iteration 61/1000 | Loss: 0.00003655
Iteration 62/1000 | Loss: 0.00003655
Iteration 63/1000 | Loss: 0.00003655
Iteration 64/1000 | Loss: 0.00003654
Iteration 65/1000 | Loss: 0.00003654
Iteration 66/1000 | Loss: 0.00003654
Iteration 67/1000 | Loss: 0.00003654
Iteration 68/1000 | Loss: 0.00003654
Iteration 69/1000 | Loss: 0.00003654
Iteration 70/1000 | Loss: 0.00003654
Iteration 71/1000 | Loss: 0.00003654
Iteration 72/1000 | Loss: 0.00003654
Iteration 73/1000 | Loss: 0.00003654
Iteration 74/1000 | Loss: 0.00003654
Iteration 75/1000 | Loss: 0.00003654
Iteration 76/1000 | Loss: 0.00003653
Iteration 77/1000 | Loss: 0.00003653
Iteration 78/1000 | Loss: 0.00003653
Iteration 79/1000 | Loss: 0.00003653
Iteration 80/1000 | Loss: 0.00003653
Iteration 81/1000 | Loss: 0.00003653
Iteration 82/1000 | Loss: 0.00003653
Iteration 83/1000 | Loss: 0.00003653
Iteration 84/1000 | Loss: 0.00003653
Iteration 85/1000 | Loss: 0.00003653
Iteration 86/1000 | Loss: 0.00003653
Iteration 87/1000 | Loss: 0.00003653
Iteration 88/1000 | Loss: 0.00003653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [3.652767190942541e-05, 3.652767190942541e-05, 3.652767190942541e-05, 3.652767190942541e-05, 3.652767190942541e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.652767190942541e-05

Optimization complete. Final v2v error: 4.784235000610352 mm

Highest mean error: 24.22229766845703 mm for frame 57

Lowest mean error: 4.211863040924072 mm for frame 163

Saving results

Total time: 116.51175880432129
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00629576
Iteration 2/25 | Loss: 0.00160117
Iteration 3/25 | Loss: 0.00137459
Iteration 4/25 | Loss: 0.00133403
Iteration 5/25 | Loss: 0.00132818
Iteration 6/25 | Loss: 0.00131442
Iteration 7/25 | Loss: 0.00131643
Iteration 8/25 | Loss: 0.00131279
Iteration 9/25 | Loss: 0.00131689
Iteration 10/25 | Loss: 0.00131088
Iteration 11/25 | Loss: 0.00131036
Iteration 12/25 | Loss: 0.00131018
Iteration 13/25 | Loss: 0.00131010
Iteration 14/25 | Loss: 0.00131010
Iteration 15/25 | Loss: 0.00131010
Iteration 16/25 | Loss: 0.00131009
Iteration 17/25 | Loss: 0.00131009
Iteration 18/25 | Loss: 0.00131009
Iteration 19/25 | Loss: 0.00131009
Iteration 20/25 | Loss: 0.00131009
Iteration 21/25 | Loss: 0.00131009
Iteration 22/25 | Loss: 0.00131009
Iteration 23/25 | Loss: 0.00131008
Iteration 24/25 | Loss: 0.00131008
Iteration 25/25 | Loss: 0.00131008

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.33626938
Iteration 2/25 | Loss: 0.00151005
Iteration 3/25 | Loss: 0.00148827
Iteration 4/25 | Loss: 0.00148827
Iteration 5/25 | Loss: 0.00148827
Iteration 6/25 | Loss: 0.00148827
Iteration 7/25 | Loss: 0.00148827
Iteration 8/25 | Loss: 0.00148827
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.0014882686082273722, 0.0014882686082273722, 0.0014882686082273722, 0.0014882686082273722, 0.0014882686082273722]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014882686082273722

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148827
Iteration 2/1000 | Loss: 0.00005161
Iteration 3/1000 | Loss: 0.00002951
Iteration 4/1000 | Loss: 0.00002753
Iteration 5/1000 | Loss: 0.00002588
Iteration 6/1000 | Loss: 0.00002514
Iteration 7/1000 | Loss: 0.00002474
Iteration 8/1000 | Loss: 0.00002439
Iteration 9/1000 | Loss: 0.00002418
Iteration 10/1000 | Loss: 0.00002399
Iteration 11/1000 | Loss: 0.00002412
Iteration 12/1000 | Loss: 0.00002392
Iteration 13/1000 | Loss: 0.00002392
Iteration 14/1000 | Loss: 0.00002391
Iteration 15/1000 | Loss: 0.00002386
Iteration 16/1000 | Loss: 0.00002384
Iteration 17/1000 | Loss: 0.00002378
Iteration 18/1000 | Loss: 0.00002378
Iteration 19/1000 | Loss: 0.00002377
Iteration 20/1000 | Loss: 0.00002373
Iteration 21/1000 | Loss: 0.00002382
Iteration 22/1000 | Loss: 0.00002489
Iteration 23/1000 | Loss: 0.00002362
Iteration 24/1000 | Loss: 0.00002362
Iteration 25/1000 | Loss: 0.00002362
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002362
Iteration 28/1000 | Loss: 0.00002362
Iteration 29/1000 | Loss: 0.00002362
Iteration 30/1000 | Loss: 0.00002362
Iteration 31/1000 | Loss: 0.00002362
Iteration 32/1000 | Loss: 0.00002362
Iteration 33/1000 | Loss: 0.00002362
Iteration 34/1000 | Loss: 0.00002362
Iteration 35/1000 | Loss: 0.00002362
Iteration 36/1000 | Loss: 0.00002361
Iteration 37/1000 | Loss: 0.00002361
Iteration 38/1000 | Loss: 0.00002361
Iteration 39/1000 | Loss: 0.00002361
Iteration 40/1000 | Loss: 0.00002361
Iteration 41/1000 | Loss: 0.00002361
Iteration 42/1000 | Loss: 0.00002361
Iteration 43/1000 | Loss: 0.00002361
Iteration 44/1000 | Loss: 0.00002361
Iteration 45/1000 | Loss: 0.00002361
Iteration 46/1000 | Loss: 0.00002360
Iteration 47/1000 | Loss: 0.00002360
Iteration 48/1000 | Loss: 0.00002360
Iteration 49/1000 | Loss: 0.00002360
Iteration 50/1000 | Loss: 0.00002402
Iteration 51/1000 | Loss: 0.00002359
Iteration 52/1000 | Loss: 0.00002359
Iteration 53/1000 | Loss: 0.00002359
Iteration 54/1000 | Loss: 0.00002359
Iteration 55/1000 | Loss: 0.00002359
Iteration 56/1000 | Loss: 0.00002359
Iteration 57/1000 | Loss: 0.00002359
Iteration 58/1000 | Loss: 0.00002359
Iteration 59/1000 | Loss: 0.00002370
Iteration 60/1000 | Loss: 0.00002370
Iteration 61/1000 | Loss: 0.00004220
Iteration 62/1000 | Loss: 0.00002721
Iteration 63/1000 | Loss: 0.00002356
Iteration 64/1000 | Loss: 0.00002832
Iteration 65/1000 | Loss: 0.00002355
Iteration 66/1000 | Loss: 0.00002355
Iteration 67/1000 | Loss: 0.00002354
Iteration 68/1000 | Loss: 0.00002354
Iteration 69/1000 | Loss: 0.00002354
Iteration 70/1000 | Loss: 0.00002354
Iteration 71/1000 | Loss: 0.00002354
Iteration 72/1000 | Loss: 0.00002354
Iteration 73/1000 | Loss: 0.00002354
Iteration 74/1000 | Loss: 0.00002354
Iteration 75/1000 | Loss: 0.00002354
Iteration 76/1000 | Loss: 0.00002354
Iteration 77/1000 | Loss: 0.00002354
Iteration 78/1000 | Loss: 0.00002354
Iteration 79/1000 | Loss: 0.00002354
Iteration 80/1000 | Loss: 0.00002353
Iteration 81/1000 | Loss: 0.00002353
Iteration 82/1000 | Loss: 0.00002353
Iteration 83/1000 | Loss: 0.00002353
Iteration 84/1000 | Loss: 0.00002352
Iteration 85/1000 | Loss: 0.00002352
Iteration 86/1000 | Loss: 0.00002352
Iteration 87/1000 | Loss: 0.00002352
Iteration 88/1000 | Loss: 0.00002352
Iteration 89/1000 | Loss: 0.00002352
Iteration 90/1000 | Loss: 0.00002352
Iteration 91/1000 | Loss: 0.00002352
Iteration 92/1000 | Loss: 0.00002352
Iteration 93/1000 | Loss: 0.00002352
Iteration 94/1000 | Loss: 0.00002352
Iteration 95/1000 | Loss: 0.00002352
Iteration 96/1000 | Loss: 0.00002352
Iteration 97/1000 | Loss: 0.00002351
Iteration 98/1000 | Loss: 0.00002351
Iteration 99/1000 | Loss: 0.00002351
Iteration 100/1000 | Loss: 0.00002351
Iteration 101/1000 | Loss: 0.00002351
Iteration 102/1000 | Loss: 0.00002351
Iteration 103/1000 | Loss: 0.00002351
Iteration 104/1000 | Loss: 0.00002351
Iteration 105/1000 | Loss: 0.00002351
Iteration 106/1000 | Loss: 0.00002351
Iteration 107/1000 | Loss: 0.00002351
Iteration 108/1000 | Loss: 0.00002351
Iteration 109/1000 | Loss: 0.00002351
Iteration 110/1000 | Loss: 0.00002351
Iteration 111/1000 | Loss: 0.00002351
Iteration 112/1000 | Loss: 0.00002351
Iteration 113/1000 | Loss: 0.00002351
Iteration 114/1000 | Loss: 0.00002350
Iteration 115/1000 | Loss: 0.00002350
Iteration 116/1000 | Loss: 0.00002350
Iteration 117/1000 | Loss: 0.00002350
Iteration 118/1000 | Loss: 0.00002350
Iteration 119/1000 | Loss: 0.00002350
Iteration 120/1000 | Loss: 0.00002350
Iteration 121/1000 | Loss: 0.00002350
Iteration 122/1000 | Loss: 0.00002350
Iteration 123/1000 | Loss: 0.00002350
Iteration 124/1000 | Loss: 0.00002350
Iteration 125/1000 | Loss: 0.00002350
Iteration 126/1000 | Loss: 0.00002350
Iteration 127/1000 | Loss: 0.00002350
Iteration 128/1000 | Loss: 0.00002350
Iteration 129/1000 | Loss: 0.00002350
Iteration 130/1000 | Loss: 0.00002350
Iteration 131/1000 | Loss: 0.00002350
Iteration 132/1000 | Loss: 0.00002350
Iteration 133/1000 | Loss: 0.00002350
Iteration 134/1000 | Loss: 0.00002350
Iteration 135/1000 | Loss: 0.00002350
Iteration 136/1000 | Loss: 0.00002350
Iteration 137/1000 | Loss: 0.00002350
Iteration 138/1000 | Loss: 0.00002350
Iteration 139/1000 | Loss: 0.00002350
Iteration 140/1000 | Loss: 0.00002350
Iteration 141/1000 | Loss: 0.00002350
Iteration 142/1000 | Loss: 0.00002350
Iteration 143/1000 | Loss: 0.00002350
Iteration 144/1000 | Loss: 0.00002350
Iteration 145/1000 | Loss: 0.00002350
Iteration 146/1000 | Loss: 0.00002350
Iteration 147/1000 | Loss: 0.00002350
Iteration 148/1000 | Loss: 0.00002350
Iteration 149/1000 | Loss: 0.00002350
Iteration 150/1000 | Loss: 0.00002350
Iteration 151/1000 | Loss: 0.00002350
Iteration 152/1000 | Loss: 0.00002350
Iteration 153/1000 | Loss: 0.00002350
Iteration 154/1000 | Loss: 0.00002350
Iteration 155/1000 | Loss: 0.00002350
Iteration 156/1000 | Loss: 0.00002350
Iteration 157/1000 | Loss: 0.00002350
Iteration 158/1000 | Loss: 0.00002350
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.3495294954045676e-05, 2.3495294954045676e-05, 2.3495294954045676e-05, 2.3495294954045676e-05, 2.3495294954045676e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3495294954045676e-05

Optimization complete. Final v2v error: 4.242709636688232 mm

Highest mean error: 4.6874542236328125 mm for frame 194

Lowest mean error: 3.7880475521087646 mm for frame 224

Saving results

Total time: 58.28734540939331
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01111633
Iteration 2/25 | Loss: 0.00225571
Iteration 3/25 | Loss: 0.00204345
Iteration 4/25 | Loss: 0.00190255
Iteration 5/25 | Loss: 0.00179136
Iteration 6/25 | Loss: 0.00189468
Iteration 7/25 | Loss: 0.00167410
Iteration 8/25 | Loss: 0.00156391
Iteration 9/25 | Loss: 0.00152976
Iteration 10/25 | Loss: 0.00145604
Iteration 11/25 | Loss: 0.00142688
Iteration 12/25 | Loss: 0.00141880
Iteration 13/25 | Loss: 0.00141228
Iteration 14/25 | Loss: 0.00141108
Iteration 15/25 | Loss: 0.00141068
Iteration 16/25 | Loss: 0.00141051
Iteration 17/25 | Loss: 0.00141036
Iteration 18/25 | Loss: 0.00141035
Iteration 19/25 | Loss: 0.00141034
Iteration 20/25 | Loss: 0.00141034
Iteration 21/25 | Loss: 0.00141034
Iteration 22/25 | Loss: 0.00141033
Iteration 23/25 | Loss: 0.00141033
Iteration 24/25 | Loss: 0.00141033
Iteration 25/25 | Loss: 0.00141033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91729820
Iteration 2/25 | Loss: 0.00191102
Iteration 3/25 | Loss: 0.00191101
Iteration 4/25 | Loss: 0.00191101
Iteration 5/25 | Loss: 0.00191101
Iteration 6/25 | Loss: 0.00191101
Iteration 7/25 | Loss: 0.00191101
Iteration 8/25 | Loss: 0.00191101
Iteration 9/25 | Loss: 0.00191101
Iteration 10/25 | Loss: 0.00191101
Iteration 11/25 | Loss: 0.00191101
Iteration 12/25 | Loss: 0.00191101
Iteration 13/25 | Loss: 0.00191101
Iteration 14/25 | Loss: 0.00191101
Iteration 15/25 | Loss: 0.00191101
Iteration 16/25 | Loss: 0.00191101
Iteration 17/25 | Loss: 0.00191101
Iteration 18/25 | Loss: 0.00191101
Iteration 19/25 | Loss: 0.00191101
Iteration 20/25 | Loss: 0.00191101
Iteration 21/25 | Loss: 0.00191101
Iteration 22/25 | Loss: 0.00191101
Iteration 23/25 | Loss: 0.00191101
Iteration 24/25 | Loss: 0.00191101
Iteration 25/25 | Loss: 0.00191101

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00191101
Iteration 2/1000 | Loss: 0.00012895
Iteration 3/1000 | Loss: 0.00013613
Iteration 4/1000 | Loss: 0.00018945
Iteration 5/1000 | Loss: 0.00055059
Iteration 6/1000 | Loss: 0.00011789
Iteration 7/1000 | Loss: 0.00056565
Iteration 8/1000 | Loss: 0.00009566
Iteration 9/1000 | Loss: 0.00005760
Iteration 10/1000 | Loss: 0.00004149
Iteration 11/1000 | Loss: 0.00003660
Iteration 12/1000 | Loss: 0.00003281
Iteration 13/1000 | Loss: 0.00003040
Iteration 14/1000 | Loss: 0.00002924
Iteration 15/1000 | Loss: 0.00002855
Iteration 16/1000 | Loss: 0.00002810
Iteration 17/1000 | Loss: 0.00002800
Iteration 18/1000 | Loss: 0.00002775
Iteration 19/1000 | Loss: 0.00002766
Iteration 20/1000 | Loss: 0.00002757
Iteration 21/1000 | Loss: 0.00002745
Iteration 22/1000 | Loss: 0.00002745
Iteration 23/1000 | Loss: 0.00002742
Iteration 24/1000 | Loss: 0.00002741
Iteration 25/1000 | Loss: 0.00002740
Iteration 26/1000 | Loss: 0.00002734
Iteration 27/1000 | Loss: 0.00002726
Iteration 28/1000 | Loss: 0.00002725
Iteration 29/1000 | Loss: 0.00002724
Iteration 30/1000 | Loss: 0.00002724
Iteration 31/1000 | Loss: 0.00002723
Iteration 32/1000 | Loss: 0.00002722
Iteration 33/1000 | Loss: 0.00002721
Iteration 34/1000 | Loss: 0.00002720
Iteration 35/1000 | Loss: 0.00002720
Iteration 36/1000 | Loss: 0.00002719
Iteration 37/1000 | Loss: 0.00002719
Iteration 38/1000 | Loss: 0.00002718
Iteration 39/1000 | Loss: 0.00002718
Iteration 40/1000 | Loss: 0.00002718
Iteration 41/1000 | Loss: 0.00002717
Iteration 42/1000 | Loss: 0.00002717
Iteration 43/1000 | Loss: 0.00002716
Iteration 44/1000 | Loss: 0.00002716
Iteration 45/1000 | Loss: 0.00002716
Iteration 46/1000 | Loss: 0.00002715
Iteration 47/1000 | Loss: 0.00002715
Iteration 48/1000 | Loss: 0.00002714
Iteration 49/1000 | Loss: 0.00002714
Iteration 50/1000 | Loss: 0.00002714
Iteration 51/1000 | Loss: 0.00002713
Iteration 52/1000 | Loss: 0.00002713
Iteration 53/1000 | Loss: 0.00002713
Iteration 54/1000 | Loss: 0.00002713
Iteration 55/1000 | Loss: 0.00002712
Iteration 56/1000 | Loss: 0.00002712
Iteration 57/1000 | Loss: 0.00002712
Iteration 58/1000 | Loss: 0.00002712
Iteration 59/1000 | Loss: 0.00002712
Iteration 60/1000 | Loss: 0.00002712
Iteration 61/1000 | Loss: 0.00002711
Iteration 62/1000 | Loss: 0.00002711
Iteration 63/1000 | Loss: 0.00002711
Iteration 64/1000 | Loss: 0.00002711
Iteration 65/1000 | Loss: 0.00002710
Iteration 66/1000 | Loss: 0.00002710
Iteration 67/1000 | Loss: 0.00002710
Iteration 68/1000 | Loss: 0.00002710
Iteration 69/1000 | Loss: 0.00002710
Iteration 70/1000 | Loss: 0.00002709
Iteration 71/1000 | Loss: 0.00002709
Iteration 72/1000 | Loss: 0.00002709
Iteration 73/1000 | Loss: 0.00002709
Iteration 74/1000 | Loss: 0.00002709
Iteration 75/1000 | Loss: 0.00002708
Iteration 76/1000 | Loss: 0.00002708
Iteration 77/1000 | Loss: 0.00002708
Iteration 78/1000 | Loss: 0.00002708
Iteration 79/1000 | Loss: 0.00002707
Iteration 80/1000 | Loss: 0.00002707
Iteration 81/1000 | Loss: 0.00002707
Iteration 82/1000 | Loss: 0.00002707
Iteration 83/1000 | Loss: 0.00002707
Iteration 84/1000 | Loss: 0.00002707
Iteration 85/1000 | Loss: 0.00002707
Iteration 86/1000 | Loss: 0.00002706
Iteration 87/1000 | Loss: 0.00002706
Iteration 88/1000 | Loss: 0.00002706
Iteration 89/1000 | Loss: 0.00002706
Iteration 90/1000 | Loss: 0.00002706
Iteration 91/1000 | Loss: 0.00002706
Iteration 92/1000 | Loss: 0.00002706
Iteration 93/1000 | Loss: 0.00002706
Iteration 94/1000 | Loss: 0.00002706
Iteration 95/1000 | Loss: 0.00002706
Iteration 96/1000 | Loss: 0.00002705
Iteration 97/1000 | Loss: 0.00002705
Iteration 98/1000 | Loss: 0.00002705
Iteration 99/1000 | Loss: 0.00002705
Iteration 100/1000 | Loss: 0.00002705
Iteration 101/1000 | Loss: 0.00002705
Iteration 102/1000 | Loss: 0.00002705
Iteration 103/1000 | Loss: 0.00002705
Iteration 104/1000 | Loss: 0.00002705
Iteration 105/1000 | Loss: 0.00002705
Iteration 106/1000 | Loss: 0.00002704
Iteration 107/1000 | Loss: 0.00002704
Iteration 108/1000 | Loss: 0.00002704
Iteration 109/1000 | Loss: 0.00002704
Iteration 110/1000 | Loss: 0.00002704
Iteration 111/1000 | Loss: 0.00002704
Iteration 112/1000 | Loss: 0.00002704
Iteration 113/1000 | Loss: 0.00002704
Iteration 114/1000 | Loss: 0.00002703
Iteration 115/1000 | Loss: 0.00002703
Iteration 116/1000 | Loss: 0.00002703
Iteration 117/1000 | Loss: 0.00002703
Iteration 118/1000 | Loss: 0.00002703
Iteration 119/1000 | Loss: 0.00002703
Iteration 120/1000 | Loss: 0.00002703
Iteration 121/1000 | Loss: 0.00002702
Iteration 122/1000 | Loss: 0.00002702
Iteration 123/1000 | Loss: 0.00002702
Iteration 124/1000 | Loss: 0.00002702
Iteration 125/1000 | Loss: 0.00002702
Iteration 126/1000 | Loss: 0.00002702
Iteration 127/1000 | Loss: 0.00002702
Iteration 128/1000 | Loss: 0.00002702
Iteration 129/1000 | Loss: 0.00002702
Iteration 130/1000 | Loss: 0.00002702
Iteration 131/1000 | Loss: 0.00002702
Iteration 132/1000 | Loss: 0.00002702
Iteration 133/1000 | Loss: 0.00002702
Iteration 134/1000 | Loss: 0.00002702
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.7018089895136654e-05, 2.7018089895136654e-05, 2.7018089895136654e-05, 2.7018089895136654e-05, 2.7018089895136654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7018089895136654e-05

Optimization complete. Final v2v error: 4.546173095703125 mm

Highest mean error: 5.218128681182861 mm for frame 64

Lowest mean error: 4.199212551116943 mm for frame 109

Saving results

Total time: 66.14777088165283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00958174
Iteration 2/25 | Loss: 0.00153863
Iteration 3/25 | Loss: 0.00140115
Iteration 4/25 | Loss: 0.00139031
Iteration 5/25 | Loss: 0.00138614
Iteration 6/25 | Loss: 0.00138554
Iteration 7/25 | Loss: 0.00138554
Iteration 8/25 | Loss: 0.00138554
Iteration 9/25 | Loss: 0.00138554
Iteration 10/25 | Loss: 0.00138554
Iteration 11/25 | Loss: 0.00138554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013855366269126534, 0.0013855366269126534, 0.0013855366269126534, 0.0013855366269126534, 0.0013855366269126534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013855366269126534

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52411664
Iteration 2/25 | Loss: 0.00178968
Iteration 3/25 | Loss: 0.00178967
Iteration 4/25 | Loss: 0.00178967
Iteration 5/25 | Loss: 0.00178967
Iteration 6/25 | Loss: 0.00178967
Iteration 7/25 | Loss: 0.00178967
Iteration 8/25 | Loss: 0.00178967
Iteration 9/25 | Loss: 0.00178967
Iteration 10/25 | Loss: 0.00178967
Iteration 11/25 | Loss: 0.00178967
Iteration 12/25 | Loss: 0.00178967
Iteration 13/25 | Loss: 0.00178967
Iteration 14/25 | Loss: 0.00178967
Iteration 15/25 | Loss: 0.00178967
Iteration 16/25 | Loss: 0.00178967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001789670204743743, 0.001789670204743743, 0.001789670204743743, 0.001789670204743743, 0.001789670204743743]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001789670204743743

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178967
Iteration 2/1000 | Loss: 0.00004833
Iteration 3/1000 | Loss: 0.00003870
Iteration 4/1000 | Loss: 0.00003595
Iteration 5/1000 | Loss: 0.00003403
Iteration 6/1000 | Loss: 0.00003302
Iteration 7/1000 | Loss: 0.00003225
Iteration 8/1000 | Loss: 0.00003179
Iteration 9/1000 | Loss: 0.00003146
Iteration 10/1000 | Loss: 0.00003132
Iteration 11/1000 | Loss: 0.00003121
Iteration 12/1000 | Loss: 0.00003120
Iteration 13/1000 | Loss: 0.00003119
Iteration 14/1000 | Loss: 0.00003117
Iteration 15/1000 | Loss: 0.00003116
Iteration 16/1000 | Loss: 0.00003116
Iteration 17/1000 | Loss: 0.00003113
Iteration 18/1000 | Loss: 0.00003112
Iteration 19/1000 | Loss: 0.00003108
Iteration 20/1000 | Loss: 0.00003106
Iteration 21/1000 | Loss: 0.00003103
Iteration 22/1000 | Loss: 0.00003103
Iteration 23/1000 | Loss: 0.00003101
Iteration 24/1000 | Loss: 0.00003101
Iteration 25/1000 | Loss: 0.00003100
Iteration 26/1000 | Loss: 0.00003100
Iteration 27/1000 | Loss: 0.00003099
Iteration 28/1000 | Loss: 0.00003099
Iteration 29/1000 | Loss: 0.00003096
Iteration 30/1000 | Loss: 0.00003096
Iteration 31/1000 | Loss: 0.00003094
Iteration 32/1000 | Loss: 0.00003093
Iteration 33/1000 | Loss: 0.00003092
Iteration 34/1000 | Loss: 0.00003092
Iteration 35/1000 | Loss: 0.00003091
Iteration 36/1000 | Loss: 0.00003091
Iteration 37/1000 | Loss: 0.00003091
Iteration 38/1000 | Loss: 0.00003090
Iteration 39/1000 | Loss: 0.00003090
Iteration 40/1000 | Loss: 0.00003088
Iteration 41/1000 | Loss: 0.00003088
Iteration 42/1000 | Loss: 0.00003087
Iteration 43/1000 | Loss: 0.00003087
Iteration 44/1000 | Loss: 0.00003087
Iteration 45/1000 | Loss: 0.00003086
Iteration 46/1000 | Loss: 0.00003086
Iteration 47/1000 | Loss: 0.00003085
Iteration 48/1000 | Loss: 0.00003085
Iteration 49/1000 | Loss: 0.00003084
Iteration 50/1000 | Loss: 0.00003084
Iteration 51/1000 | Loss: 0.00003084
Iteration 52/1000 | Loss: 0.00003084
Iteration 53/1000 | Loss: 0.00003084
Iteration 54/1000 | Loss: 0.00003083
Iteration 55/1000 | Loss: 0.00003083
Iteration 56/1000 | Loss: 0.00003083
Iteration 57/1000 | Loss: 0.00003083
Iteration 58/1000 | Loss: 0.00003083
Iteration 59/1000 | Loss: 0.00003083
Iteration 60/1000 | Loss: 0.00003083
Iteration 61/1000 | Loss: 0.00003083
Iteration 62/1000 | Loss: 0.00003083
Iteration 63/1000 | Loss: 0.00003083
Iteration 64/1000 | Loss: 0.00003083
Iteration 65/1000 | Loss: 0.00003083
Iteration 66/1000 | Loss: 0.00003083
Iteration 67/1000 | Loss: 0.00003083
Iteration 68/1000 | Loss: 0.00003083
Iteration 69/1000 | Loss: 0.00003083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [3.08289127133321e-05, 3.08289127133321e-05, 3.08289127133321e-05, 3.08289127133321e-05, 3.08289127133321e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.08289127133321e-05

Optimization complete. Final v2v error: 4.7676005363464355 mm

Highest mean error: 5.504766941070557 mm for frame 32

Lowest mean error: 4.340095520019531 mm for frame 198

Saving results

Total time: 34.481422424316406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811124
Iteration 2/25 | Loss: 0.00202064
Iteration 3/25 | Loss: 0.00159199
Iteration 4/25 | Loss: 0.00151849
Iteration 5/25 | Loss: 0.00150171
Iteration 6/25 | Loss: 0.00149585
Iteration 7/25 | Loss: 0.00151816
Iteration 8/25 | Loss: 0.00147589
Iteration 9/25 | Loss: 0.00147188
Iteration 10/25 | Loss: 0.00147140
Iteration 11/25 | Loss: 0.00147127
Iteration 12/25 | Loss: 0.00147125
Iteration 13/25 | Loss: 0.00147125
Iteration 14/25 | Loss: 0.00147125
Iteration 15/25 | Loss: 0.00147125
Iteration 16/25 | Loss: 0.00147125
Iteration 17/25 | Loss: 0.00147125
Iteration 18/25 | Loss: 0.00147125
Iteration 19/25 | Loss: 0.00147125
Iteration 20/25 | Loss: 0.00147124
Iteration 21/25 | Loss: 0.00147124
Iteration 22/25 | Loss: 0.00147124
Iteration 23/25 | Loss: 0.00147124
Iteration 24/25 | Loss: 0.00147124
Iteration 25/25 | Loss: 0.00147124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.50956440
Iteration 2/25 | Loss: 0.00225867
Iteration 3/25 | Loss: 0.00225811
Iteration 4/25 | Loss: 0.00225811
Iteration 5/25 | Loss: 0.00225810
Iteration 6/25 | Loss: 0.00225810
Iteration 7/25 | Loss: 0.00225810
Iteration 8/25 | Loss: 0.00225810
Iteration 9/25 | Loss: 0.00225810
Iteration 10/25 | Loss: 0.00225810
Iteration 11/25 | Loss: 0.00225810
Iteration 12/25 | Loss: 0.00225810
Iteration 13/25 | Loss: 0.00225810
Iteration 14/25 | Loss: 0.00225810
Iteration 15/25 | Loss: 0.00225810
Iteration 16/25 | Loss: 0.00225810
Iteration 17/25 | Loss: 0.00225810
Iteration 18/25 | Loss: 0.00225810
Iteration 19/25 | Loss: 0.00225810
Iteration 20/25 | Loss: 0.00225810
Iteration 21/25 | Loss: 0.00225810
Iteration 22/25 | Loss: 0.00225810
Iteration 23/25 | Loss: 0.00225810
Iteration 24/25 | Loss: 0.00225810
Iteration 25/25 | Loss: 0.00225810

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00225810
Iteration 2/1000 | Loss: 0.00012612
Iteration 3/1000 | Loss: 0.00007954
Iteration 4/1000 | Loss: 0.00006543
Iteration 5/1000 | Loss: 0.00005775
Iteration 6/1000 | Loss: 0.00095269
Iteration 7/1000 | Loss: 0.00012133
Iteration 8/1000 | Loss: 0.00006351
Iteration 9/1000 | Loss: 0.00005436
Iteration 10/1000 | Loss: 0.00015208
Iteration 11/1000 | Loss: 0.00005216
Iteration 12/1000 | Loss: 0.00004755
Iteration 13/1000 | Loss: 0.00004492
Iteration 14/1000 | Loss: 0.00004350
Iteration 15/1000 | Loss: 0.00004272
Iteration 16/1000 | Loss: 0.00004224
Iteration 17/1000 | Loss: 0.00004158
Iteration 18/1000 | Loss: 0.00004115
Iteration 19/1000 | Loss: 0.00004083
Iteration 20/1000 | Loss: 0.00004053
Iteration 21/1000 | Loss: 0.00004024
Iteration 22/1000 | Loss: 0.00003999
Iteration 23/1000 | Loss: 0.00003975
Iteration 24/1000 | Loss: 0.00007554
Iteration 25/1000 | Loss: 0.00008452
Iteration 26/1000 | Loss: 0.00007299
Iteration 27/1000 | Loss: 0.00004766
Iteration 28/1000 | Loss: 0.00005443
Iteration 29/1000 | Loss: 0.00007467
Iteration 30/1000 | Loss: 0.00008380
Iteration 31/1000 | Loss: 0.00004452
Iteration 32/1000 | Loss: 0.00004207
Iteration 33/1000 | Loss: 0.00004068
Iteration 34/1000 | Loss: 0.00003991
Iteration 35/1000 | Loss: 0.00003938
Iteration 36/1000 | Loss: 0.00003918
Iteration 37/1000 | Loss: 0.00003918
Iteration 38/1000 | Loss: 0.00003915
Iteration 39/1000 | Loss: 0.00003912
Iteration 40/1000 | Loss: 0.00003912
Iteration 41/1000 | Loss: 0.00003910
Iteration 42/1000 | Loss: 0.00003908
Iteration 43/1000 | Loss: 0.00003908
Iteration 44/1000 | Loss: 0.00003908
Iteration 45/1000 | Loss: 0.00003907
Iteration 46/1000 | Loss: 0.00003907
Iteration 47/1000 | Loss: 0.00003907
Iteration 48/1000 | Loss: 0.00003906
Iteration 49/1000 | Loss: 0.00003905
Iteration 50/1000 | Loss: 0.00003905
Iteration 51/1000 | Loss: 0.00003905
Iteration 52/1000 | Loss: 0.00003905
Iteration 53/1000 | Loss: 0.00003905
Iteration 54/1000 | Loss: 0.00003905
Iteration 55/1000 | Loss: 0.00003905
Iteration 56/1000 | Loss: 0.00003905
Iteration 57/1000 | Loss: 0.00003905
Iteration 58/1000 | Loss: 0.00003905
Iteration 59/1000 | Loss: 0.00003905
Iteration 60/1000 | Loss: 0.00003905
Iteration 61/1000 | Loss: 0.00003904
Iteration 62/1000 | Loss: 0.00003904
Iteration 63/1000 | Loss: 0.00003904
Iteration 64/1000 | Loss: 0.00003904
Iteration 65/1000 | Loss: 0.00003904
Iteration 66/1000 | Loss: 0.00003903
Iteration 67/1000 | Loss: 0.00003903
Iteration 68/1000 | Loss: 0.00003903
Iteration 69/1000 | Loss: 0.00003902
Iteration 70/1000 | Loss: 0.00003901
Iteration 71/1000 | Loss: 0.00003901
Iteration 72/1000 | Loss: 0.00003900
Iteration 73/1000 | Loss: 0.00003900
Iteration 74/1000 | Loss: 0.00003900
Iteration 75/1000 | Loss: 0.00003900
Iteration 76/1000 | Loss: 0.00003900
Iteration 77/1000 | Loss: 0.00003900
Iteration 78/1000 | Loss: 0.00003900
Iteration 79/1000 | Loss: 0.00003900
Iteration 80/1000 | Loss: 0.00003900
Iteration 81/1000 | Loss: 0.00003899
Iteration 82/1000 | Loss: 0.00003899
Iteration 83/1000 | Loss: 0.00003899
Iteration 84/1000 | Loss: 0.00003899
Iteration 85/1000 | Loss: 0.00003898
Iteration 86/1000 | Loss: 0.00003898
Iteration 87/1000 | Loss: 0.00003898
Iteration 88/1000 | Loss: 0.00003897
Iteration 89/1000 | Loss: 0.00003897
Iteration 90/1000 | Loss: 0.00003897
Iteration 91/1000 | Loss: 0.00003897
Iteration 92/1000 | Loss: 0.00003897
Iteration 93/1000 | Loss: 0.00003897
Iteration 94/1000 | Loss: 0.00003897
Iteration 95/1000 | Loss: 0.00003897
Iteration 96/1000 | Loss: 0.00003896
Iteration 97/1000 | Loss: 0.00003896
Iteration 98/1000 | Loss: 0.00003896
Iteration 99/1000 | Loss: 0.00003896
Iteration 100/1000 | Loss: 0.00003896
Iteration 101/1000 | Loss: 0.00003896
Iteration 102/1000 | Loss: 0.00003896
Iteration 103/1000 | Loss: 0.00003895
Iteration 104/1000 | Loss: 0.00003895
Iteration 105/1000 | Loss: 0.00003895
Iteration 106/1000 | Loss: 0.00003895
Iteration 107/1000 | Loss: 0.00003895
Iteration 108/1000 | Loss: 0.00003894
Iteration 109/1000 | Loss: 0.00003894
Iteration 110/1000 | Loss: 0.00003894
Iteration 111/1000 | Loss: 0.00003894
Iteration 112/1000 | Loss: 0.00003894
Iteration 113/1000 | Loss: 0.00003894
Iteration 114/1000 | Loss: 0.00003893
Iteration 115/1000 | Loss: 0.00003893
Iteration 116/1000 | Loss: 0.00003893
Iteration 117/1000 | Loss: 0.00003893
Iteration 118/1000 | Loss: 0.00003893
Iteration 119/1000 | Loss: 0.00003893
Iteration 120/1000 | Loss: 0.00003893
Iteration 121/1000 | Loss: 0.00003893
Iteration 122/1000 | Loss: 0.00003893
Iteration 123/1000 | Loss: 0.00003892
Iteration 124/1000 | Loss: 0.00003892
Iteration 125/1000 | Loss: 0.00003892
Iteration 126/1000 | Loss: 0.00003892
Iteration 127/1000 | Loss: 0.00003892
Iteration 128/1000 | Loss: 0.00003892
Iteration 129/1000 | Loss: 0.00003892
Iteration 130/1000 | Loss: 0.00003892
Iteration 131/1000 | Loss: 0.00003892
Iteration 132/1000 | Loss: 0.00003892
Iteration 133/1000 | Loss: 0.00003892
Iteration 134/1000 | Loss: 0.00003892
Iteration 135/1000 | Loss: 0.00003892
Iteration 136/1000 | Loss: 0.00003892
Iteration 137/1000 | Loss: 0.00003892
Iteration 138/1000 | Loss: 0.00003892
Iteration 139/1000 | Loss: 0.00003891
Iteration 140/1000 | Loss: 0.00003891
Iteration 141/1000 | Loss: 0.00003891
Iteration 142/1000 | Loss: 0.00003891
Iteration 143/1000 | Loss: 0.00003891
Iteration 144/1000 | Loss: 0.00003891
Iteration 145/1000 | Loss: 0.00003891
Iteration 146/1000 | Loss: 0.00003891
Iteration 147/1000 | Loss: 0.00003891
Iteration 148/1000 | Loss: 0.00003891
Iteration 149/1000 | Loss: 0.00003891
Iteration 150/1000 | Loss: 0.00003891
Iteration 151/1000 | Loss: 0.00003891
Iteration 152/1000 | Loss: 0.00003891
Iteration 153/1000 | Loss: 0.00003891
Iteration 154/1000 | Loss: 0.00003891
Iteration 155/1000 | Loss: 0.00003891
Iteration 156/1000 | Loss: 0.00003891
Iteration 157/1000 | Loss: 0.00003891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.8910842704353854e-05, 3.8910842704353854e-05, 3.8910842704353854e-05, 3.8910842704353854e-05, 3.8910842704353854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8910842704353854e-05

Optimization complete. Final v2v error: 5.344707012176514 mm

Highest mean error: 7.022199630737305 mm for frame 66

Lowest mean error: 4.526225566864014 mm for frame 0

Saving results

Total time: 78.4983172416687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00944091
Iteration 2/25 | Loss: 0.00155743
Iteration 3/25 | Loss: 0.00141055
Iteration 4/25 | Loss: 0.00139089
Iteration 5/25 | Loss: 0.00138390
Iteration 6/25 | Loss: 0.00138176
Iteration 7/25 | Loss: 0.00138140
Iteration 8/25 | Loss: 0.00138140
Iteration 9/25 | Loss: 0.00138140
Iteration 10/25 | Loss: 0.00138140
Iteration 11/25 | Loss: 0.00138140
Iteration 12/25 | Loss: 0.00138140
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013813982950523496, 0.0013813982950523496, 0.0013813982950523496, 0.0013813982950523496, 0.0013813982950523496]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013813982950523496

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.49319780
Iteration 2/25 | Loss: 0.00160978
Iteration 3/25 | Loss: 0.00160976
Iteration 4/25 | Loss: 0.00160976
Iteration 5/25 | Loss: 0.00160976
Iteration 6/25 | Loss: 0.00160976
Iteration 7/25 | Loss: 0.00160976
Iteration 8/25 | Loss: 0.00160976
Iteration 9/25 | Loss: 0.00160976
Iteration 10/25 | Loss: 0.00160976
Iteration 11/25 | Loss: 0.00160976
Iteration 12/25 | Loss: 0.00160976
Iteration 13/25 | Loss: 0.00160976
Iteration 14/25 | Loss: 0.00160976
Iteration 15/25 | Loss: 0.00160976
Iteration 16/25 | Loss: 0.00160976
Iteration 17/25 | Loss: 0.00160976
Iteration 18/25 | Loss: 0.00160976
Iteration 19/25 | Loss: 0.00160976
Iteration 20/25 | Loss: 0.00160976
Iteration 21/25 | Loss: 0.00160976
Iteration 22/25 | Loss: 0.00160976
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016097610350698233, 0.0016097610350698233, 0.0016097610350698233, 0.0016097610350698233, 0.0016097610350698233]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016097610350698233

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00160976
Iteration 2/1000 | Loss: 0.00004903
Iteration 3/1000 | Loss: 0.00003677
Iteration 4/1000 | Loss: 0.00003142
Iteration 5/1000 | Loss: 0.00002891
Iteration 6/1000 | Loss: 0.00002781
Iteration 7/1000 | Loss: 0.00002701
Iteration 8/1000 | Loss: 0.00002649
Iteration 9/1000 | Loss: 0.00002617
Iteration 10/1000 | Loss: 0.00002586
Iteration 11/1000 | Loss: 0.00002575
Iteration 12/1000 | Loss: 0.00002574
Iteration 13/1000 | Loss: 0.00002572
Iteration 14/1000 | Loss: 0.00002571
Iteration 15/1000 | Loss: 0.00002571
Iteration 16/1000 | Loss: 0.00002570
Iteration 17/1000 | Loss: 0.00002570
Iteration 18/1000 | Loss: 0.00002570
Iteration 19/1000 | Loss: 0.00002570
Iteration 20/1000 | Loss: 0.00002570
Iteration 21/1000 | Loss: 0.00002570
Iteration 22/1000 | Loss: 0.00002570
Iteration 23/1000 | Loss: 0.00002570
Iteration 24/1000 | Loss: 0.00002569
Iteration 25/1000 | Loss: 0.00002569
Iteration 26/1000 | Loss: 0.00002569
Iteration 27/1000 | Loss: 0.00002568
Iteration 28/1000 | Loss: 0.00002567
Iteration 29/1000 | Loss: 0.00002567
Iteration 30/1000 | Loss: 0.00002567
Iteration 31/1000 | Loss: 0.00002567
Iteration 32/1000 | Loss: 0.00002567
Iteration 33/1000 | Loss: 0.00002567
Iteration 34/1000 | Loss: 0.00002566
Iteration 35/1000 | Loss: 0.00002565
Iteration 36/1000 | Loss: 0.00002565
Iteration 37/1000 | Loss: 0.00002565
Iteration 38/1000 | Loss: 0.00002564
Iteration 39/1000 | Loss: 0.00002564
Iteration 40/1000 | Loss: 0.00002564
Iteration 41/1000 | Loss: 0.00002564
Iteration 42/1000 | Loss: 0.00002563
Iteration 43/1000 | Loss: 0.00002563
Iteration 44/1000 | Loss: 0.00002563
Iteration 45/1000 | Loss: 0.00002563
Iteration 46/1000 | Loss: 0.00002563
Iteration 47/1000 | Loss: 0.00002563
Iteration 48/1000 | Loss: 0.00002563
Iteration 49/1000 | Loss: 0.00002563
Iteration 50/1000 | Loss: 0.00002563
Iteration 51/1000 | Loss: 0.00002562
Iteration 52/1000 | Loss: 0.00002562
Iteration 53/1000 | Loss: 0.00002562
Iteration 54/1000 | Loss: 0.00002562
Iteration 55/1000 | Loss: 0.00002562
Iteration 56/1000 | Loss: 0.00002562
Iteration 57/1000 | Loss: 0.00002562
Iteration 58/1000 | Loss: 0.00002562
Iteration 59/1000 | Loss: 0.00002562
Iteration 60/1000 | Loss: 0.00002562
Iteration 61/1000 | Loss: 0.00002562
Iteration 62/1000 | Loss: 0.00002562
Iteration 63/1000 | Loss: 0.00002562
Iteration 64/1000 | Loss: 0.00002561
Iteration 65/1000 | Loss: 0.00002561
Iteration 66/1000 | Loss: 0.00002561
Iteration 67/1000 | Loss: 0.00002561
Iteration 68/1000 | Loss: 0.00002561
Iteration 69/1000 | Loss: 0.00002560
Iteration 70/1000 | Loss: 0.00002560
Iteration 71/1000 | Loss: 0.00002560
Iteration 72/1000 | Loss: 0.00002559
Iteration 73/1000 | Loss: 0.00002559
Iteration 74/1000 | Loss: 0.00002559
Iteration 75/1000 | Loss: 0.00002559
Iteration 76/1000 | Loss: 0.00002559
Iteration 77/1000 | Loss: 0.00002558
Iteration 78/1000 | Loss: 0.00002558
Iteration 79/1000 | Loss: 0.00002558
Iteration 80/1000 | Loss: 0.00002558
Iteration 81/1000 | Loss: 0.00002558
Iteration 82/1000 | Loss: 0.00002558
Iteration 83/1000 | Loss: 0.00002557
Iteration 84/1000 | Loss: 0.00002557
Iteration 85/1000 | Loss: 0.00002557
Iteration 86/1000 | Loss: 0.00002557
Iteration 87/1000 | Loss: 0.00002557
Iteration 88/1000 | Loss: 0.00002557
Iteration 89/1000 | Loss: 0.00002557
Iteration 90/1000 | Loss: 0.00002556
Iteration 91/1000 | Loss: 0.00002556
Iteration 92/1000 | Loss: 0.00002556
Iteration 93/1000 | Loss: 0.00002556
Iteration 94/1000 | Loss: 0.00002556
Iteration 95/1000 | Loss: 0.00002556
Iteration 96/1000 | Loss: 0.00002556
Iteration 97/1000 | Loss: 0.00002556
Iteration 98/1000 | Loss: 0.00002556
Iteration 99/1000 | Loss: 0.00002555
Iteration 100/1000 | Loss: 0.00002555
Iteration 101/1000 | Loss: 0.00002555
Iteration 102/1000 | Loss: 0.00002555
Iteration 103/1000 | Loss: 0.00002555
Iteration 104/1000 | Loss: 0.00002554
Iteration 105/1000 | Loss: 0.00002554
Iteration 106/1000 | Loss: 0.00002554
Iteration 107/1000 | Loss: 0.00002554
Iteration 108/1000 | Loss: 0.00002554
Iteration 109/1000 | Loss: 0.00002554
Iteration 110/1000 | Loss: 0.00002554
Iteration 111/1000 | Loss: 0.00002554
Iteration 112/1000 | Loss: 0.00002554
Iteration 113/1000 | Loss: 0.00002554
Iteration 114/1000 | Loss: 0.00002554
Iteration 115/1000 | Loss: 0.00002554
Iteration 116/1000 | Loss: 0.00002554
Iteration 117/1000 | Loss: 0.00002553
Iteration 118/1000 | Loss: 0.00002553
Iteration 119/1000 | Loss: 0.00002553
Iteration 120/1000 | Loss: 0.00002553
Iteration 121/1000 | Loss: 0.00002553
Iteration 122/1000 | Loss: 0.00002553
Iteration 123/1000 | Loss: 0.00002553
Iteration 124/1000 | Loss: 0.00002553
Iteration 125/1000 | Loss: 0.00002552
Iteration 126/1000 | Loss: 0.00002552
Iteration 127/1000 | Loss: 0.00002552
Iteration 128/1000 | Loss: 0.00002552
Iteration 129/1000 | Loss: 0.00002552
Iteration 130/1000 | Loss: 0.00002552
Iteration 131/1000 | Loss: 0.00002552
Iteration 132/1000 | Loss: 0.00002552
Iteration 133/1000 | Loss: 0.00002552
Iteration 134/1000 | Loss: 0.00002551
Iteration 135/1000 | Loss: 0.00002551
Iteration 136/1000 | Loss: 0.00002551
Iteration 137/1000 | Loss: 0.00002551
Iteration 138/1000 | Loss: 0.00002551
Iteration 139/1000 | Loss: 0.00002551
Iteration 140/1000 | Loss: 0.00002550
Iteration 141/1000 | Loss: 0.00002550
Iteration 142/1000 | Loss: 0.00002550
Iteration 143/1000 | Loss: 0.00002550
Iteration 144/1000 | Loss: 0.00002550
Iteration 145/1000 | Loss: 0.00002550
Iteration 146/1000 | Loss: 0.00002550
Iteration 147/1000 | Loss: 0.00002550
Iteration 148/1000 | Loss: 0.00002550
Iteration 149/1000 | Loss: 0.00002550
Iteration 150/1000 | Loss: 0.00002550
Iteration 151/1000 | Loss: 0.00002550
Iteration 152/1000 | Loss: 0.00002550
Iteration 153/1000 | Loss: 0.00002550
Iteration 154/1000 | Loss: 0.00002550
Iteration 155/1000 | Loss: 0.00002550
Iteration 156/1000 | Loss: 0.00002550
Iteration 157/1000 | Loss: 0.00002550
Iteration 158/1000 | Loss: 0.00002550
Iteration 159/1000 | Loss: 0.00002549
Iteration 160/1000 | Loss: 0.00002549
Iteration 161/1000 | Loss: 0.00002549
Iteration 162/1000 | Loss: 0.00002549
Iteration 163/1000 | Loss: 0.00002549
Iteration 164/1000 | Loss: 0.00002549
Iteration 165/1000 | Loss: 0.00002549
Iteration 166/1000 | Loss: 0.00002548
Iteration 167/1000 | Loss: 0.00002548
Iteration 168/1000 | Loss: 0.00002548
Iteration 169/1000 | Loss: 0.00002548
Iteration 170/1000 | Loss: 0.00002548
Iteration 171/1000 | Loss: 0.00002548
Iteration 172/1000 | Loss: 0.00002548
Iteration 173/1000 | Loss: 0.00002548
Iteration 174/1000 | Loss: 0.00002548
Iteration 175/1000 | Loss: 0.00002548
Iteration 176/1000 | Loss: 0.00002548
Iteration 177/1000 | Loss: 0.00002548
Iteration 178/1000 | Loss: 0.00002548
Iteration 179/1000 | Loss: 0.00002548
Iteration 180/1000 | Loss: 0.00002548
Iteration 181/1000 | Loss: 0.00002548
Iteration 182/1000 | Loss: 0.00002548
Iteration 183/1000 | Loss: 0.00002548
Iteration 184/1000 | Loss: 0.00002548
Iteration 185/1000 | Loss: 0.00002548
Iteration 186/1000 | Loss: 0.00002548
Iteration 187/1000 | Loss: 0.00002548
Iteration 188/1000 | Loss: 0.00002548
Iteration 189/1000 | Loss: 0.00002548
Iteration 190/1000 | Loss: 0.00002548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 190. Stopping optimization.
Last 5 losses: [2.548095289967023e-05, 2.548095289967023e-05, 2.548095289967023e-05, 2.548095289967023e-05, 2.548095289967023e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.548095289967023e-05

Optimization complete. Final v2v error: 4.385646820068359 mm

Highest mean error: 4.676483154296875 mm for frame 61

Lowest mean error: 4.0614752769470215 mm for frame 122

Saving results

Total time: 37.005040407180786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763477
Iteration 2/25 | Loss: 0.00143172
Iteration 3/25 | Loss: 0.00133156
Iteration 4/25 | Loss: 0.00131668
Iteration 5/25 | Loss: 0.00131130
Iteration 6/25 | Loss: 0.00130997
Iteration 7/25 | Loss: 0.00130958
Iteration 8/25 | Loss: 0.00130958
Iteration 9/25 | Loss: 0.00130958
Iteration 10/25 | Loss: 0.00130958
Iteration 11/25 | Loss: 0.00130958
Iteration 12/25 | Loss: 0.00130958
Iteration 13/25 | Loss: 0.00130958
Iteration 14/25 | Loss: 0.00130958
Iteration 15/25 | Loss: 0.00130958
Iteration 16/25 | Loss: 0.00130958
Iteration 17/25 | Loss: 0.00130958
Iteration 18/25 | Loss: 0.00130958
Iteration 19/25 | Loss: 0.00130958
Iteration 20/25 | Loss: 0.00130958
Iteration 21/25 | Loss: 0.00130958
Iteration 22/25 | Loss: 0.00130958
Iteration 23/25 | Loss: 0.00130958
Iteration 24/25 | Loss: 0.00130958
Iteration 25/25 | Loss: 0.00130958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57457709
Iteration 2/25 | Loss: 0.00143015
Iteration 3/25 | Loss: 0.00143015
Iteration 4/25 | Loss: 0.00143015
Iteration 5/25 | Loss: 0.00143015
Iteration 6/25 | Loss: 0.00143015
Iteration 7/25 | Loss: 0.00143015
Iteration 8/25 | Loss: 0.00143015
Iteration 9/25 | Loss: 0.00143015
Iteration 10/25 | Loss: 0.00143015
Iteration 11/25 | Loss: 0.00143015
Iteration 12/25 | Loss: 0.00143015
Iteration 13/25 | Loss: 0.00143015
Iteration 14/25 | Loss: 0.00143015
Iteration 15/25 | Loss: 0.00143015
Iteration 16/25 | Loss: 0.00143015
Iteration 17/25 | Loss: 0.00143015
Iteration 18/25 | Loss: 0.00143015
Iteration 19/25 | Loss: 0.00143015
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001430148258805275, 0.001430148258805275, 0.001430148258805275, 0.001430148258805275, 0.001430148258805275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001430148258805275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143015
Iteration 2/1000 | Loss: 0.00004677
Iteration 3/1000 | Loss: 0.00003059
Iteration 4/1000 | Loss: 0.00002734
Iteration 5/1000 | Loss: 0.00002632
Iteration 6/1000 | Loss: 0.00002550
Iteration 7/1000 | Loss: 0.00002505
Iteration 8/1000 | Loss: 0.00002477
Iteration 9/1000 | Loss: 0.00002455
Iteration 10/1000 | Loss: 0.00002444
Iteration 11/1000 | Loss: 0.00002434
Iteration 12/1000 | Loss: 0.00002429
Iteration 13/1000 | Loss: 0.00002429
Iteration 14/1000 | Loss: 0.00002428
Iteration 15/1000 | Loss: 0.00002428
Iteration 16/1000 | Loss: 0.00002426
Iteration 17/1000 | Loss: 0.00002426
Iteration 18/1000 | Loss: 0.00002426
Iteration 19/1000 | Loss: 0.00002426
Iteration 20/1000 | Loss: 0.00002426
Iteration 21/1000 | Loss: 0.00002426
Iteration 22/1000 | Loss: 0.00002426
Iteration 23/1000 | Loss: 0.00002426
Iteration 24/1000 | Loss: 0.00002425
Iteration 25/1000 | Loss: 0.00002425
Iteration 26/1000 | Loss: 0.00002425
Iteration 27/1000 | Loss: 0.00002425
Iteration 28/1000 | Loss: 0.00002425
Iteration 29/1000 | Loss: 0.00002424
Iteration 30/1000 | Loss: 0.00002423
Iteration 31/1000 | Loss: 0.00002422
Iteration 32/1000 | Loss: 0.00002420
Iteration 33/1000 | Loss: 0.00002417
Iteration 34/1000 | Loss: 0.00002417
Iteration 35/1000 | Loss: 0.00002417
Iteration 36/1000 | Loss: 0.00002416
Iteration 37/1000 | Loss: 0.00002415
Iteration 38/1000 | Loss: 0.00002415
Iteration 39/1000 | Loss: 0.00002414
Iteration 40/1000 | Loss: 0.00002414
Iteration 41/1000 | Loss: 0.00002414
Iteration 42/1000 | Loss: 0.00002413
Iteration 43/1000 | Loss: 0.00002413
Iteration 44/1000 | Loss: 0.00002413
Iteration 45/1000 | Loss: 0.00002412
Iteration 46/1000 | Loss: 0.00002412
Iteration 47/1000 | Loss: 0.00002411
Iteration 48/1000 | Loss: 0.00002411
Iteration 49/1000 | Loss: 0.00002411
Iteration 50/1000 | Loss: 0.00002411
Iteration 51/1000 | Loss: 0.00002410
Iteration 52/1000 | Loss: 0.00002410
Iteration 53/1000 | Loss: 0.00002410
Iteration 54/1000 | Loss: 0.00002410
Iteration 55/1000 | Loss: 0.00002409
Iteration 56/1000 | Loss: 0.00002409
Iteration 57/1000 | Loss: 0.00002409
Iteration 58/1000 | Loss: 0.00002409
Iteration 59/1000 | Loss: 0.00002409
Iteration 60/1000 | Loss: 0.00002409
Iteration 61/1000 | Loss: 0.00002409
Iteration 62/1000 | Loss: 0.00002409
Iteration 63/1000 | Loss: 0.00002408
Iteration 64/1000 | Loss: 0.00002408
Iteration 65/1000 | Loss: 0.00002408
Iteration 66/1000 | Loss: 0.00002408
Iteration 67/1000 | Loss: 0.00002408
Iteration 68/1000 | Loss: 0.00002408
Iteration 69/1000 | Loss: 0.00002408
Iteration 70/1000 | Loss: 0.00002408
Iteration 71/1000 | Loss: 0.00002408
Iteration 72/1000 | Loss: 0.00002408
Iteration 73/1000 | Loss: 0.00002408
Iteration 74/1000 | Loss: 0.00002408
Iteration 75/1000 | Loss: 0.00002408
Iteration 76/1000 | Loss: 0.00002408
Iteration 77/1000 | Loss: 0.00002407
Iteration 78/1000 | Loss: 0.00002407
Iteration 79/1000 | Loss: 0.00002407
Iteration 80/1000 | Loss: 0.00002407
Iteration 81/1000 | Loss: 0.00002407
Iteration 82/1000 | Loss: 0.00002407
Iteration 83/1000 | Loss: 0.00002407
Iteration 84/1000 | Loss: 0.00002407
Iteration 85/1000 | Loss: 0.00002407
Iteration 86/1000 | Loss: 0.00002407
Iteration 87/1000 | Loss: 0.00002407
Iteration 88/1000 | Loss: 0.00002407
Iteration 89/1000 | Loss: 0.00002407
Iteration 90/1000 | Loss: 0.00002407
Iteration 91/1000 | Loss: 0.00002407
Iteration 92/1000 | Loss: 0.00002407
Iteration 93/1000 | Loss: 0.00002406
Iteration 94/1000 | Loss: 0.00002406
Iteration 95/1000 | Loss: 0.00002406
Iteration 96/1000 | Loss: 0.00002406
Iteration 97/1000 | Loss: 0.00002406
Iteration 98/1000 | Loss: 0.00002406
Iteration 99/1000 | Loss: 0.00002406
Iteration 100/1000 | Loss: 0.00002406
Iteration 101/1000 | Loss: 0.00002406
Iteration 102/1000 | Loss: 0.00002406
Iteration 103/1000 | Loss: 0.00002406
Iteration 104/1000 | Loss: 0.00002406
Iteration 105/1000 | Loss: 0.00002405
Iteration 106/1000 | Loss: 0.00002405
Iteration 107/1000 | Loss: 0.00002405
Iteration 108/1000 | Loss: 0.00002405
Iteration 109/1000 | Loss: 0.00002405
Iteration 110/1000 | Loss: 0.00002405
Iteration 111/1000 | Loss: 0.00002405
Iteration 112/1000 | Loss: 0.00002405
Iteration 113/1000 | Loss: 0.00002405
Iteration 114/1000 | Loss: 0.00002405
Iteration 115/1000 | Loss: 0.00002405
Iteration 116/1000 | Loss: 0.00002405
Iteration 117/1000 | Loss: 0.00002405
Iteration 118/1000 | Loss: 0.00002405
Iteration 119/1000 | Loss: 0.00002405
Iteration 120/1000 | Loss: 0.00002405
Iteration 121/1000 | Loss: 0.00002405
Iteration 122/1000 | Loss: 0.00002405
Iteration 123/1000 | Loss: 0.00002405
Iteration 124/1000 | Loss: 0.00002404
Iteration 125/1000 | Loss: 0.00002404
Iteration 126/1000 | Loss: 0.00002404
Iteration 127/1000 | Loss: 0.00002404
Iteration 128/1000 | Loss: 0.00002404
Iteration 129/1000 | Loss: 0.00002404
Iteration 130/1000 | Loss: 0.00002404
Iteration 131/1000 | Loss: 0.00002404
Iteration 132/1000 | Loss: 0.00002404
Iteration 133/1000 | Loss: 0.00002404
Iteration 134/1000 | Loss: 0.00002404
Iteration 135/1000 | Loss: 0.00002404
Iteration 136/1000 | Loss: 0.00002404
Iteration 137/1000 | Loss: 0.00002404
Iteration 138/1000 | Loss: 0.00002404
Iteration 139/1000 | Loss: 0.00002404
Iteration 140/1000 | Loss: 0.00002404
Iteration 141/1000 | Loss: 0.00002404
Iteration 142/1000 | Loss: 0.00002403
Iteration 143/1000 | Loss: 0.00002403
Iteration 144/1000 | Loss: 0.00002403
Iteration 145/1000 | Loss: 0.00002403
Iteration 146/1000 | Loss: 0.00002403
Iteration 147/1000 | Loss: 0.00002403
Iteration 148/1000 | Loss: 0.00002403
Iteration 149/1000 | Loss: 0.00002403
Iteration 150/1000 | Loss: 0.00002403
Iteration 151/1000 | Loss: 0.00002403
Iteration 152/1000 | Loss: 0.00002403
Iteration 153/1000 | Loss: 0.00002403
Iteration 154/1000 | Loss: 0.00002403
Iteration 155/1000 | Loss: 0.00002403
Iteration 156/1000 | Loss: 0.00002403
Iteration 157/1000 | Loss: 0.00002403
Iteration 158/1000 | Loss: 0.00002403
Iteration 159/1000 | Loss: 0.00002403
Iteration 160/1000 | Loss: 0.00002403
Iteration 161/1000 | Loss: 0.00002403
Iteration 162/1000 | Loss: 0.00002403
Iteration 163/1000 | Loss: 0.00002402
Iteration 164/1000 | Loss: 0.00002402
Iteration 165/1000 | Loss: 0.00002402
Iteration 166/1000 | Loss: 0.00002402
Iteration 167/1000 | Loss: 0.00002402
Iteration 168/1000 | Loss: 0.00002402
Iteration 169/1000 | Loss: 0.00002402
Iteration 170/1000 | Loss: 0.00002402
Iteration 171/1000 | Loss: 0.00002402
Iteration 172/1000 | Loss: 0.00002402
Iteration 173/1000 | Loss: 0.00002402
Iteration 174/1000 | Loss: 0.00002402
Iteration 175/1000 | Loss: 0.00002402
Iteration 176/1000 | Loss: 0.00002402
Iteration 177/1000 | Loss: 0.00002402
Iteration 178/1000 | Loss: 0.00002402
Iteration 179/1000 | Loss: 0.00002402
Iteration 180/1000 | Loss: 0.00002402
Iteration 181/1000 | Loss: 0.00002402
Iteration 182/1000 | Loss: 0.00002402
Iteration 183/1000 | Loss: 0.00002402
Iteration 184/1000 | Loss: 0.00002402
Iteration 185/1000 | Loss: 0.00002402
Iteration 186/1000 | Loss: 0.00002402
Iteration 187/1000 | Loss: 0.00002402
Iteration 188/1000 | Loss: 0.00002402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [2.4017686882871203e-05, 2.4017686882871203e-05, 2.4017686882871203e-05, 2.4017686882871203e-05, 2.4017686882871203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4017686882871203e-05

Optimization complete. Final v2v error: 4.252533435821533 mm

Highest mean error: 4.5995564460754395 mm for frame 72

Lowest mean error: 3.927842617034912 mm for frame 29

Saving results

Total time: 36.32509636878967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_39_us_0647/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_39_us_0647/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01043857
Iteration 2/25 | Loss: 0.00288903
Iteration 3/25 | Loss: 0.00209000
Iteration 4/25 | Loss: 0.00199681
Iteration 5/25 | Loss: 0.00198867
Iteration 6/25 | Loss: 0.00190647
Iteration 7/25 | Loss: 0.00189558
Iteration 8/25 | Loss: 0.00188300
Iteration 9/25 | Loss: 0.00187858
Iteration 10/25 | Loss: 0.00187094
Iteration 11/25 | Loss: 0.00186998
Iteration 12/25 | Loss: 0.00186963
Iteration 13/25 | Loss: 0.00186930
Iteration 14/25 | Loss: 0.00186890
Iteration 15/25 | Loss: 0.00187349
Iteration 16/25 | Loss: 0.00187091
Iteration 17/25 | Loss: 0.00186873
Iteration 18/25 | Loss: 0.00186736
Iteration 19/25 | Loss: 0.00187390
Iteration 20/25 | Loss: 0.00186685
Iteration 21/25 | Loss: 0.00186674
Iteration 22/25 | Loss: 0.00186671
Iteration 23/25 | Loss: 0.00186663
Iteration 24/25 | Loss: 0.00186662
Iteration 25/25 | Loss: 0.00186662

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.56564450
Iteration 2/25 | Loss: 0.00716115
Iteration 3/25 | Loss: 0.00716115
Iteration 4/25 | Loss: 0.00716115
Iteration 5/25 | Loss: 0.00716115
Iteration 6/25 | Loss: 0.00716115
Iteration 7/25 | Loss: 0.00716115
Iteration 8/25 | Loss: 0.00716115
Iteration 9/25 | Loss: 0.00716115
Iteration 10/25 | Loss: 0.00716115
Iteration 11/25 | Loss: 0.00716115
Iteration 12/25 | Loss: 0.00716115
Iteration 13/25 | Loss: 0.00716115
Iteration 14/25 | Loss: 0.00716115
Iteration 15/25 | Loss: 0.00716115
Iteration 16/25 | Loss: 0.00716115
Iteration 17/25 | Loss: 0.00716115
Iteration 18/25 | Loss: 0.00716115
Iteration 19/25 | Loss: 0.00716115
Iteration 20/25 | Loss: 0.00716115
Iteration 21/25 | Loss: 0.00716115
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.007161145098507404, 0.007161145098507404, 0.007161145098507404, 0.007161145098507404, 0.007161145098507404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007161145098507404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00716115
Iteration 2/1000 | Loss: 0.00121373
Iteration 3/1000 | Loss: 0.00146611
Iteration 4/1000 | Loss: 0.00229014
Iteration 5/1000 | Loss: 0.00202848
Iteration 6/1000 | Loss: 0.01006337
Iteration 7/1000 | Loss: 0.00114482
Iteration 8/1000 | Loss: 0.00882471
Iteration 9/1000 | Loss: 0.00165069
Iteration 10/1000 | Loss: 0.00340227
Iteration 11/1000 | Loss: 0.01417608
Iteration 12/1000 | Loss: 0.00550788
Iteration 13/1000 | Loss: 0.00492516
Iteration 14/1000 | Loss: 0.00503845
Iteration 15/1000 | Loss: 0.00527438
Iteration 16/1000 | Loss: 0.00354330
Iteration 17/1000 | Loss: 0.00438839
Iteration 18/1000 | Loss: 0.00457034
Iteration 19/1000 | Loss: 0.00099248
Iteration 20/1000 | Loss: 0.00287795
Iteration 21/1000 | Loss: 0.00066161
Iteration 22/1000 | Loss: 0.00043966
Iteration 23/1000 | Loss: 0.00148861
Iteration 24/1000 | Loss: 0.00186169
Iteration 25/1000 | Loss: 0.00233572
Iteration 26/1000 | Loss: 0.00220215
Iteration 27/1000 | Loss: 0.00177079
Iteration 28/1000 | Loss: 0.00419040
Iteration 29/1000 | Loss: 0.00141781
Iteration 30/1000 | Loss: 0.00098954
Iteration 31/1000 | Loss: 0.00235896
Iteration 32/1000 | Loss: 0.00406988
Iteration 33/1000 | Loss: 0.00419832
Iteration 34/1000 | Loss: 0.00479999
Iteration 35/1000 | Loss: 0.00331416
Iteration 36/1000 | Loss: 0.00324102
Iteration 37/1000 | Loss: 0.00454265
Iteration 38/1000 | Loss: 0.00349633
Iteration 39/1000 | Loss: 0.00210196
Iteration 40/1000 | Loss: 0.00338301
Iteration 41/1000 | Loss: 0.00219979
Iteration 42/1000 | Loss: 0.00281641
Iteration 43/1000 | Loss: 0.00557994
Iteration 44/1000 | Loss: 0.00232889
Iteration 45/1000 | Loss: 0.00319750
Iteration 46/1000 | Loss: 0.00265944
Iteration 47/1000 | Loss: 0.00328105
Iteration 48/1000 | Loss: 0.00381798
Iteration 49/1000 | Loss: 0.00339555
Iteration 50/1000 | Loss: 0.00233788
Iteration 51/1000 | Loss: 0.00215823
Iteration 52/1000 | Loss: 0.00253443
Iteration 53/1000 | Loss: 0.00218166
Iteration 54/1000 | Loss: 0.00187160
Iteration 55/1000 | Loss: 0.00340541
Iteration 56/1000 | Loss: 0.00476592
Iteration 57/1000 | Loss: 0.00402995
Iteration 58/1000 | Loss: 0.00337999
Iteration 59/1000 | Loss: 0.00216754
Iteration 60/1000 | Loss: 0.00424937
Iteration 61/1000 | Loss: 0.00320104
Iteration 62/1000 | Loss: 0.00176591
Iteration 63/1000 | Loss: 0.00183063
Iteration 64/1000 | Loss: 0.00192254
Iteration 65/1000 | Loss: 0.00208300
Iteration 66/1000 | Loss: 0.00101845
Iteration 67/1000 | Loss: 0.00093729
Iteration 68/1000 | Loss: 0.00194197
Iteration 69/1000 | Loss: 0.00196244
Iteration 70/1000 | Loss: 0.00132587
Iteration 71/1000 | Loss: 0.00127913
Iteration 72/1000 | Loss: 0.00168896
Iteration 73/1000 | Loss: 0.00181271
Iteration 74/1000 | Loss: 0.00176703
Iteration 75/1000 | Loss: 0.00157831
Iteration 76/1000 | Loss: 0.00099662
Iteration 77/1000 | Loss: 0.00077512
Iteration 78/1000 | Loss: 0.00087411
Iteration 79/1000 | Loss: 0.00059786
Iteration 80/1000 | Loss: 0.00079924
Iteration 81/1000 | Loss: 0.00107406
Iteration 82/1000 | Loss: 0.00033085
Iteration 83/1000 | Loss: 0.00069114
Iteration 84/1000 | Loss: 0.00055047
Iteration 85/1000 | Loss: 0.00109884
Iteration 86/1000 | Loss: 0.00133634
Iteration 87/1000 | Loss: 0.00137427
Iteration 88/1000 | Loss: 0.00085648
Iteration 89/1000 | Loss: 0.00128808
Iteration 90/1000 | Loss: 0.00071453
Iteration 91/1000 | Loss: 0.00080510
Iteration 92/1000 | Loss: 0.00064864
Iteration 93/1000 | Loss: 0.00095551
Iteration 94/1000 | Loss: 0.00071906
Iteration 95/1000 | Loss: 0.00124152
Iteration 96/1000 | Loss: 0.00090975
Iteration 97/1000 | Loss: 0.00076021
Iteration 98/1000 | Loss: 0.00121751
Iteration 99/1000 | Loss: 0.00128053
Iteration 100/1000 | Loss: 0.00079327
Iteration 101/1000 | Loss: 0.00071063
Iteration 102/1000 | Loss: 0.00081988
Iteration 103/1000 | Loss: 0.00059569
Iteration 104/1000 | Loss: 0.00051151
Iteration 105/1000 | Loss: 0.00073930
Iteration 106/1000 | Loss: 0.00021443
Iteration 107/1000 | Loss: 0.00031591
Iteration 108/1000 | Loss: 0.00066080
Iteration 109/1000 | Loss: 0.00054946
Iteration 110/1000 | Loss: 0.00064987
Iteration 111/1000 | Loss: 0.00065996
Iteration 112/1000 | Loss: 0.00028601
Iteration 113/1000 | Loss: 0.00007400
Iteration 114/1000 | Loss: 0.00006170
Iteration 115/1000 | Loss: 0.00014678
Iteration 116/1000 | Loss: 0.00016325
Iteration 117/1000 | Loss: 0.00035563
Iteration 118/1000 | Loss: 0.00041723
Iteration 119/1000 | Loss: 0.00046062
Iteration 120/1000 | Loss: 0.00052899
Iteration 121/1000 | Loss: 0.00009028
Iteration 122/1000 | Loss: 0.00071339
Iteration 123/1000 | Loss: 0.00105869
Iteration 124/1000 | Loss: 0.00105171
Iteration 125/1000 | Loss: 0.00009247
Iteration 126/1000 | Loss: 0.00007971
Iteration 127/1000 | Loss: 0.00020563
Iteration 128/1000 | Loss: 0.00057354
Iteration 129/1000 | Loss: 0.00051491
Iteration 130/1000 | Loss: 0.00067860
Iteration 131/1000 | Loss: 0.00099209
Iteration 132/1000 | Loss: 0.00010665
Iteration 133/1000 | Loss: 0.00005956
Iteration 134/1000 | Loss: 0.00011889
Iteration 135/1000 | Loss: 0.00080962
Iteration 136/1000 | Loss: 0.00033283
Iteration 137/1000 | Loss: 0.00051018
Iteration 138/1000 | Loss: 0.00004768
Iteration 139/1000 | Loss: 0.00044509
Iteration 140/1000 | Loss: 0.00015781
Iteration 141/1000 | Loss: 0.00077576
Iteration 142/1000 | Loss: 0.00059454
Iteration 143/1000 | Loss: 0.00048554
Iteration 144/1000 | Loss: 0.00038847
Iteration 145/1000 | Loss: 0.00028848
Iteration 146/1000 | Loss: 0.00004604
Iteration 147/1000 | Loss: 0.00041614
Iteration 148/1000 | Loss: 0.00079441
Iteration 149/1000 | Loss: 0.00028731
Iteration 150/1000 | Loss: 0.00004433
Iteration 151/1000 | Loss: 0.00032034
Iteration 152/1000 | Loss: 0.00056931
Iteration 153/1000 | Loss: 0.00011965
Iteration 154/1000 | Loss: 0.00027352
Iteration 155/1000 | Loss: 0.00031576
Iteration 156/1000 | Loss: 0.00004143
Iteration 157/1000 | Loss: 0.00013828
Iteration 158/1000 | Loss: 0.00064723
Iteration 159/1000 | Loss: 0.00017172
Iteration 160/1000 | Loss: 0.00071366
Iteration 161/1000 | Loss: 0.00027535
Iteration 162/1000 | Loss: 0.00048059
Iteration 163/1000 | Loss: 0.00004046
Iteration 164/1000 | Loss: 0.00003954
Iteration 165/1000 | Loss: 0.00028897
Iteration 166/1000 | Loss: 0.00056355
Iteration 167/1000 | Loss: 0.00046394
Iteration 168/1000 | Loss: 0.00013884
Iteration 169/1000 | Loss: 0.00026183
Iteration 170/1000 | Loss: 0.00050568
Iteration 171/1000 | Loss: 0.00010940
Iteration 172/1000 | Loss: 0.00004244
Iteration 173/1000 | Loss: 0.00004846
Iteration 174/1000 | Loss: 0.00006312
Iteration 175/1000 | Loss: 0.00058112
Iteration 176/1000 | Loss: 0.00041935
Iteration 177/1000 | Loss: 0.00004083
Iteration 178/1000 | Loss: 0.00014000
Iteration 179/1000 | Loss: 0.00041553
Iteration 180/1000 | Loss: 0.00033491
Iteration 181/1000 | Loss: 0.00019157
Iteration 182/1000 | Loss: 0.00010093
Iteration 183/1000 | Loss: 0.00033403
Iteration 184/1000 | Loss: 0.00041359
Iteration 185/1000 | Loss: 0.00018080
Iteration 186/1000 | Loss: 0.00006399
Iteration 187/1000 | Loss: 0.00042429
Iteration 188/1000 | Loss: 0.00005060
Iteration 189/1000 | Loss: 0.00035511
Iteration 190/1000 | Loss: 0.00033587
Iteration 191/1000 | Loss: 0.00020698
Iteration 192/1000 | Loss: 0.00032775
Iteration 193/1000 | Loss: 0.00025607
Iteration 194/1000 | Loss: 0.00026652
Iteration 195/1000 | Loss: 0.00018014
Iteration 196/1000 | Loss: 0.00016043
Iteration 197/1000 | Loss: 0.00004400
Iteration 198/1000 | Loss: 0.00003869
Iteration 199/1000 | Loss: 0.00003703
Iteration 200/1000 | Loss: 0.00003659
Iteration 201/1000 | Loss: 0.00003622
Iteration 202/1000 | Loss: 0.00003573
Iteration 203/1000 | Loss: 0.00003549
Iteration 204/1000 | Loss: 0.00020856
Iteration 205/1000 | Loss: 0.00004958
Iteration 206/1000 | Loss: 0.00003873
Iteration 207/1000 | Loss: 0.00003624
Iteration 208/1000 | Loss: 0.00003440
Iteration 209/1000 | Loss: 0.00003328
Iteration 210/1000 | Loss: 0.00003274
Iteration 211/1000 | Loss: 0.00003252
Iteration 212/1000 | Loss: 0.00003249
Iteration 213/1000 | Loss: 0.00003245
Iteration 214/1000 | Loss: 0.00003243
Iteration 215/1000 | Loss: 0.00003241
Iteration 216/1000 | Loss: 0.00003240
Iteration 217/1000 | Loss: 0.00003240
Iteration 218/1000 | Loss: 0.00003239
Iteration 219/1000 | Loss: 0.00003238
Iteration 220/1000 | Loss: 0.00003233
Iteration 221/1000 | Loss: 0.00003233
Iteration 222/1000 | Loss: 0.00003226
Iteration 223/1000 | Loss: 0.00003225
Iteration 224/1000 | Loss: 0.00003222
Iteration 225/1000 | Loss: 0.00003222
Iteration 226/1000 | Loss: 0.00003222
Iteration 227/1000 | Loss: 0.00003222
Iteration 228/1000 | Loss: 0.00003222
Iteration 229/1000 | Loss: 0.00003222
Iteration 230/1000 | Loss: 0.00003222
Iteration 231/1000 | Loss: 0.00003222
Iteration 232/1000 | Loss: 0.00003221
Iteration 233/1000 | Loss: 0.00003221
Iteration 234/1000 | Loss: 0.00003221
Iteration 235/1000 | Loss: 0.00003220
Iteration 236/1000 | Loss: 0.00003219
Iteration 237/1000 | Loss: 0.00003219
Iteration 238/1000 | Loss: 0.00003219
Iteration 239/1000 | Loss: 0.00003219
Iteration 240/1000 | Loss: 0.00003218
Iteration 241/1000 | Loss: 0.00003218
Iteration 242/1000 | Loss: 0.00003217
Iteration 243/1000 | Loss: 0.00003217
Iteration 244/1000 | Loss: 0.00003217
Iteration 245/1000 | Loss: 0.00003217
Iteration 246/1000 | Loss: 0.00003217
Iteration 247/1000 | Loss: 0.00003216
Iteration 248/1000 | Loss: 0.00003216
Iteration 249/1000 | Loss: 0.00003216
Iteration 250/1000 | Loss: 0.00003216
Iteration 251/1000 | Loss: 0.00003216
Iteration 252/1000 | Loss: 0.00003216
Iteration 253/1000 | Loss: 0.00003216
Iteration 254/1000 | Loss: 0.00003216
Iteration 255/1000 | Loss: 0.00003216
Iteration 256/1000 | Loss: 0.00003216
Iteration 257/1000 | Loss: 0.00003216
Iteration 258/1000 | Loss: 0.00003216
Iteration 259/1000 | Loss: 0.00003215
Iteration 260/1000 | Loss: 0.00003215
Iteration 261/1000 | Loss: 0.00003215
Iteration 262/1000 | Loss: 0.00003215
Iteration 263/1000 | Loss: 0.00003215
Iteration 264/1000 | Loss: 0.00003215
Iteration 265/1000 | Loss: 0.00003215
Iteration 266/1000 | Loss: 0.00003215
Iteration 267/1000 | Loss: 0.00003215
Iteration 268/1000 | Loss: 0.00003215
Iteration 269/1000 | Loss: 0.00003215
Iteration 270/1000 | Loss: 0.00003215
Iteration 271/1000 | Loss: 0.00003214
Iteration 272/1000 | Loss: 0.00003214
Iteration 273/1000 | Loss: 0.00003214
Iteration 274/1000 | Loss: 0.00003214
Iteration 275/1000 | Loss: 0.00003214
Iteration 276/1000 | Loss: 0.00003214
Iteration 277/1000 | Loss: 0.00003214
Iteration 278/1000 | Loss: 0.00003214
Iteration 279/1000 | Loss: 0.00003214
Iteration 280/1000 | Loss: 0.00003214
Iteration 281/1000 | Loss: 0.00003214
Iteration 282/1000 | Loss: 0.00003214
Iteration 283/1000 | Loss: 0.00003214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [3.214009848306887e-05, 3.214009848306887e-05, 3.214009848306887e-05, 3.214009848306887e-05, 3.214009848306887e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.214009848306887e-05

Optimization complete. Final v2v error: 4.557622909545898 mm

Highest mean error: 14.15589427947998 mm for frame 1

Lowest mean error: 4.134861946105957 mm for frame 107

Saving results

Total time: 396.08523297309875
