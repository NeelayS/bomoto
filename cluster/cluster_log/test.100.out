Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=100, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 5600-5655
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00450576
Iteration 2/25 | Loss: 0.00136783
Iteration 3/25 | Loss: 0.00127693
Iteration 4/25 | Loss: 0.00125771
Iteration 5/25 | Loss: 0.00125362
Iteration 6/25 | Loss: 0.00125302
Iteration 7/25 | Loss: 0.00125302
Iteration 8/25 | Loss: 0.00125302
Iteration 9/25 | Loss: 0.00125302
Iteration 10/25 | Loss: 0.00125302
Iteration 11/25 | Loss: 0.00125302
Iteration 12/25 | Loss: 0.00125302
Iteration 13/25 | Loss: 0.00125302
Iteration 14/25 | Loss: 0.00125302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012530238600447774, 0.0012530238600447774, 0.0012530238600447774, 0.0012530238600447774, 0.0012530238600447774]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012530238600447774

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27504337
Iteration 2/25 | Loss: 0.00144112
Iteration 3/25 | Loss: 0.00144112
Iteration 4/25 | Loss: 0.00144112
Iteration 5/25 | Loss: 0.00144111
Iteration 6/25 | Loss: 0.00144111
Iteration 7/25 | Loss: 0.00144111
Iteration 8/25 | Loss: 0.00144111
Iteration 9/25 | Loss: 0.00144111
Iteration 10/25 | Loss: 0.00144111
Iteration 11/25 | Loss: 0.00144111
Iteration 12/25 | Loss: 0.00144111
Iteration 13/25 | Loss: 0.00144111
Iteration 14/25 | Loss: 0.00144111
Iteration 15/25 | Loss: 0.00144111
Iteration 16/25 | Loss: 0.00144111
Iteration 17/25 | Loss: 0.00144111
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014411128358915448, 0.0014411128358915448, 0.0014411128358915448, 0.0014411128358915448, 0.0014411128358915448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014411128358915448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144111
Iteration 2/1000 | Loss: 0.00004693
Iteration 3/1000 | Loss: 0.00002875
Iteration 4/1000 | Loss: 0.00002433
Iteration 5/1000 | Loss: 0.00002161
Iteration 6/1000 | Loss: 0.00002072
Iteration 7/1000 | Loss: 0.00001967
Iteration 8/1000 | Loss: 0.00001933
Iteration 9/1000 | Loss: 0.00001902
Iteration 10/1000 | Loss: 0.00001883
Iteration 11/1000 | Loss: 0.00001878
Iteration 12/1000 | Loss: 0.00001875
Iteration 13/1000 | Loss: 0.00001857
Iteration 14/1000 | Loss: 0.00001857
Iteration 15/1000 | Loss: 0.00001848
Iteration 16/1000 | Loss: 0.00001847
Iteration 17/1000 | Loss: 0.00001844
Iteration 18/1000 | Loss: 0.00001837
Iteration 19/1000 | Loss: 0.00001836
Iteration 20/1000 | Loss: 0.00001835
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001826
Iteration 23/1000 | Loss: 0.00001823
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001822
Iteration 26/1000 | Loss: 0.00001821
Iteration 27/1000 | Loss: 0.00001813
Iteration 28/1000 | Loss: 0.00001808
Iteration 29/1000 | Loss: 0.00001808
Iteration 30/1000 | Loss: 0.00001808
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001808
Iteration 33/1000 | Loss: 0.00001807
Iteration 34/1000 | Loss: 0.00001806
Iteration 35/1000 | Loss: 0.00001806
Iteration 36/1000 | Loss: 0.00001805
Iteration 37/1000 | Loss: 0.00001804
Iteration 38/1000 | Loss: 0.00001804
Iteration 39/1000 | Loss: 0.00001803
Iteration 40/1000 | Loss: 0.00001803
Iteration 41/1000 | Loss: 0.00001803
Iteration 42/1000 | Loss: 0.00001803
Iteration 43/1000 | Loss: 0.00001803
Iteration 44/1000 | Loss: 0.00001803
Iteration 45/1000 | Loss: 0.00001803
Iteration 46/1000 | Loss: 0.00001802
Iteration 47/1000 | Loss: 0.00001802
Iteration 48/1000 | Loss: 0.00001802
Iteration 49/1000 | Loss: 0.00001802
Iteration 50/1000 | Loss: 0.00001801
Iteration 51/1000 | Loss: 0.00001801
Iteration 52/1000 | Loss: 0.00001801
Iteration 53/1000 | Loss: 0.00001801
Iteration 54/1000 | Loss: 0.00001801
Iteration 55/1000 | Loss: 0.00001800
Iteration 56/1000 | Loss: 0.00001799
Iteration 57/1000 | Loss: 0.00001798
Iteration 58/1000 | Loss: 0.00001798
Iteration 59/1000 | Loss: 0.00001796
Iteration 60/1000 | Loss: 0.00001794
Iteration 61/1000 | Loss: 0.00001793
Iteration 62/1000 | Loss: 0.00001793
Iteration 63/1000 | Loss: 0.00001793
Iteration 64/1000 | Loss: 0.00001793
Iteration 65/1000 | Loss: 0.00001793
Iteration 66/1000 | Loss: 0.00001792
Iteration 67/1000 | Loss: 0.00001792
Iteration 68/1000 | Loss: 0.00001792
Iteration 69/1000 | Loss: 0.00001792
Iteration 70/1000 | Loss: 0.00001791
Iteration 71/1000 | Loss: 0.00001791
Iteration 72/1000 | Loss: 0.00001791
Iteration 73/1000 | Loss: 0.00001790
Iteration 74/1000 | Loss: 0.00001790
Iteration 75/1000 | Loss: 0.00001790
Iteration 76/1000 | Loss: 0.00001790
Iteration 77/1000 | Loss: 0.00001789
Iteration 78/1000 | Loss: 0.00001789
Iteration 79/1000 | Loss: 0.00001789
Iteration 80/1000 | Loss: 0.00001788
Iteration 81/1000 | Loss: 0.00001788
Iteration 82/1000 | Loss: 0.00001788
Iteration 83/1000 | Loss: 0.00001788
Iteration 84/1000 | Loss: 0.00001788
Iteration 85/1000 | Loss: 0.00001787
Iteration 86/1000 | Loss: 0.00001786
Iteration 87/1000 | Loss: 0.00001786
Iteration 88/1000 | Loss: 0.00001786
Iteration 89/1000 | Loss: 0.00001785
Iteration 90/1000 | Loss: 0.00001785
Iteration 91/1000 | Loss: 0.00001785
Iteration 92/1000 | Loss: 0.00001785
Iteration 93/1000 | Loss: 0.00001785
Iteration 94/1000 | Loss: 0.00001785
Iteration 95/1000 | Loss: 0.00001785
Iteration 96/1000 | Loss: 0.00001785
Iteration 97/1000 | Loss: 0.00001784
Iteration 98/1000 | Loss: 0.00001784
Iteration 99/1000 | Loss: 0.00001784
Iteration 100/1000 | Loss: 0.00001784
Iteration 101/1000 | Loss: 0.00001784
Iteration 102/1000 | Loss: 0.00001784
Iteration 103/1000 | Loss: 0.00001784
Iteration 104/1000 | Loss: 0.00001784
Iteration 105/1000 | Loss: 0.00001783
Iteration 106/1000 | Loss: 0.00001783
Iteration 107/1000 | Loss: 0.00001783
Iteration 108/1000 | Loss: 0.00001783
Iteration 109/1000 | Loss: 0.00001783
Iteration 110/1000 | Loss: 0.00001783
Iteration 111/1000 | Loss: 0.00001783
Iteration 112/1000 | Loss: 0.00001783
Iteration 113/1000 | Loss: 0.00001783
Iteration 114/1000 | Loss: 0.00001783
Iteration 115/1000 | Loss: 0.00001783
Iteration 116/1000 | Loss: 0.00001783
Iteration 117/1000 | Loss: 0.00001783
Iteration 118/1000 | Loss: 0.00001783
Iteration 119/1000 | Loss: 0.00001782
Iteration 120/1000 | Loss: 0.00001782
Iteration 121/1000 | Loss: 0.00001782
Iteration 122/1000 | Loss: 0.00001782
Iteration 123/1000 | Loss: 0.00001782
Iteration 124/1000 | Loss: 0.00001782
Iteration 125/1000 | Loss: 0.00001782
Iteration 126/1000 | Loss: 0.00001782
Iteration 127/1000 | Loss: 0.00001782
Iteration 128/1000 | Loss: 0.00001782
Iteration 129/1000 | Loss: 0.00001782
Iteration 130/1000 | Loss: 0.00001782
Iteration 131/1000 | Loss: 0.00001782
Iteration 132/1000 | Loss: 0.00001782
Iteration 133/1000 | Loss: 0.00001782
Iteration 134/1000 | Loss: 0.00001782
Iteration 135/1000 | Loss: 0.00001782
Iteration 136/1000 | Loss: 0.00001782
Iteration 137/1000 | Loss: 0.00001782
Iteration 138/1000 | Loss: 0.00001782
Iteration 139/1000 | Loss: 0.00001781
Iteration 140/1000 | Loss: 0.00001781
Iteration 141/1000 | Loss: 0.00001781
Iteration 142/1000 | Loss: 0.00001781
Iteration 143/1000 | Loss: 0.00001781
Iteration 144/1000 | Loss: 0.00001781
Iteration 145/1000 | Loss: 0.00001781
Iteration 146/1000 | Loss: 0.00001781
Iteration 147/1000 | Loss: 0.00001781
Iteration 148/1000 | Loss: 0.00001780
Iteration 149/1000 | Loss: 0.00001780
Iteration 150/1000 | Loss: 0.00001780
Iteration 151/1000 | Loss: 0.00001780
Iteration 152/1000 | Loss: 0.00001780
Iteration 153/1000 | Loss: 0.00001780
Iteration 154/1000 | Loss: 0.00001780
Iteration 155/1000 | Loss: 0.00001779
Iteration 156/1000 | Loss: 0.00001779
Iteration 157/1000 | Loss: 0.00001779
Iteration 158/1000 | Loss: 0.00001779
Iteration 159/1000 | Loss: 0.00001779
Iteration 160/1000 | Loss: 0.00001779
Iteration 161/1000 | Loss: 0.00001779
Iteration 162/1000 | Loss: 0.00001779
Iteration 163/1000 | Loss: 0.00001779
Iteration 164/1000 | Loss: 0.00001779
Iteration 165/1000 | Loss: 0.00001779
Iteration 166/1000 | Loss: 0.00001779
Iteration 167/1000 | Loss: 0.00001779
Iteration 168/1000 | Loss: 0.00001779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [1.779423837433569e-05, 1.779423837433569e-05, 1.779423837433569e-05, 1.779423837433569e-05, 1.779423837433569e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.779423837433569e-05

Optimization complete. Final v2v error: 3.619368553161621 mm

Highest mean error: 3.888000249862671 mm for frame 177

Lowest mean error: 3.444319009780884 mm for frame 16

Saving results

Total time: 40.83066368103027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00938043
Iteration 2/25 | Loss: 0.00140652
Iteration 3/25 | Loss: 0.00132465
Iteration 4/25 | Loss: 0.00130787
Iteration 5/25 | Loss: 0.00130222
Iteration 6/25 | Loss: 0.00130079
Iteration 7/25 | Loss: 0.00130067
Iteration 8/25 | Loss: 0.00130067
Iteration 9/25 | Loss: 0.00130067
Iteration 10/25 | Loss: 0.00130067
Iteration 11/25 | Loss: 0.00130067
Iteration 12/25 | Loss: 0.00130067
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013006726512685418, 0.0013006726512685418, 0.0013006726512685418, 0.0013006726512685418, 0.0013006726512685418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013006726512685418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23963058
Iteration 2/25 | Loss: 0.00185123
Iteration 3/25 | Loss: 0.00185123
Iteration 4/25 | Loss: 0.00185123
Iteration 5/25 | Loss: 0.00185123
Iteration 6/25 | Loss: 0.00185123
Iteration 7/25 | Loss: 0.00185123
Iteration 8/25 | Loss: 0.00185123
Iteration 9/25 | Loss: 0.00185123
Iteration 10/25 | Loss: 0.00185123
Iteration 11/25 | Loss: 0.00185123
Iteration 12/25 | Loss: 0.00185123
Iteration 13/25 | Loss: 0.00185123
Iteration 14/25 | Loss: 0.00185123
Iteration 15/25 | Loss: 0.00185123
Iteration 16/25 | Loss: 0.00185123
Iteration 17/25 | Loss: 0.00185123
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0018512258538976312, 0.0018512258538976312, 0.0018512258538976312, 0.0018512258538976312, 0.0018512258538976312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018512258538976312

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185123
Iteration 2/1000 | Loss: 0.00004978
Iteration 3/1000 | Loss: 0.00003064
Iteration 4/1000 | Loss: 0.00002475
Iteration 5/1000 | Loss: 0.00002214
Iteration 6/1000 | Loss: 0.00002152
Iteration 7/1000 | Loss: 0.00002108
Iteration 8/1000 | Loss: 0.00002067
Iteration 9/1000 | Loss: 0.00002030
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001991
Iteration 12/1000 | Loss: 0.00001974
Iteration 13/1000 | Loss: 0.00001956
Iteration 14/1000 | Loss: 0.00001952
Iteration 15/1000 | Loss: 0.00001951
Iteration 16/1000 | Loss: 0.00001951
Iteration 17/1000 | Loss: 0.00001950
Iteration 18/1000 | Loss: 0.00001950
Iteration 19/1000 | Loss: 0.00001948
Iteration 20/1000 | Loss: 0.00001947
Iteration 21/1000 | Loss: 0.00001947
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001947
Iteration 24/1000 | Loss: 0.00001947
Iteration 25/1000 | Loss: 0.00001947
Iteration 26/1000 | Loss: 0.00001947
Iteration 27/1000 | Loss: 0.00001947
Iteration 28/1000 | Loss: 0.00001947
Iteration 29/1000 | Loss: 0.00001947
Iteration 30/1000 | Loss: 0.00001947
Iteration 31/1000 | Loss: 0.00001947
Iteration 32/1000 | Loss: 0.00001947
Iteration 33/1000 | Loss: 0.00001947
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001947
Iteration 36/1000 | Loss: 0.00001947
Iteration 37/1000 | Loss: 0.00001947
Iteration 38/1000 | Loss: 0.00001947
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001947
Iteration 41/1000 | Loss: 0.00001947
Iteration 42/1000 | Loss: 0.00001947
Iteration 43/1000 | Loss: 0.00001947
Iteration 44/1000 | Loss: 0.00001947
Iteration 45/1000 | Loss: 0.00001947
Iteration 46/1000 | Loss: 0.00001947
Iteration 47/1000 | Loss: 0.00001947
Iteration 48/1000 | Loss: 0.00001947
Iteration 49/1000 | Loss: 0.00001947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 49. Stopping optimization.
Last 5 losses: [1.946769407368265e-05, 1.946769407368265e-05, 1.946769407368265e-05, 1.946769407368265e-05, 1.946769407368265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.946769407368265e-05

Optimization complete. Final v2v error: 3.7002217769622803 mm

Highest mean error: 3.8609132766723633 mm for frame 0

Lowest mean error: 3.3053693771362305 mm for frame 69

Saving results

Total time: 27.01309061050415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00898911
Iteration 2/25 | Loss: 0.00166138
Iteration 3/25 | Loss: 0.00135610
Iteration 4/25 | Loss: 0.00132830
Iteration 5/25 | Loss: 0.00132596
Iteration 6/25 | Loss: 0.00132566
Iteration 7/25 | Loss: 0.00132566
Iteration 8/25 | Loss: 0.00132566
Iteration 9/25 | Loss: 0.00132566
Iteration 10/25 | Loss: 0.00132566
Iteration 11/25 | Loss: 0.00132566
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013256613165140152, 0.0013256613165140152, 0.0013256613165140152, 0.0013256613165140152, 0.0013256613165140152]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013256613165140152

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.83773595
Iteration 2/25 | Loss: 0.00059736
Iteration 3/25 | Loss: 0.00059735
Iteration 4/25 | Loss: 0.00059735
Iteration 5/25 | Loss: 0.00059735
Iteration 6/25 | Loss: 0.00059735
Iteration 7/25 | Loss: 0.00059735
Iteration 8/25 | Loss: 0.00059735
Iteration 9/25 | Loss: 0.00059735
Iteration 10/25 | Loss: 0.00059735
Iteration 11/25 | Loss: 0.00059735
Iteration 12/25 | Loss: 0.00059735
Iteration 13/25 | Loss: 0.00059735
Iteration 14/25 | Loss: 0.00059735
Iteration 15/25 | Loss: 0.00059735
Iteration 16/25 | Loss: 0.00059735
Iteration 17/25 | Loss: 0.00059735
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005973461666144431, 0.0005973461666144431, 0.0005973461666144431, 0.0005973461666144431, 0.0005973461666144431]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005973461666144431

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059735
Iteration 2/1000 | Loss: 0.00004047
Iteration 3/1000 | Loss: 0.00002917
Iteration 4/1000 | Loss: 0.00002488
Iteration 5/1000 | Loss: 0.00002322
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00002142
Iteration 8/1000 | Loss: 0.00002093
Iteration 9/1000 | Loss: 0.00002073
Iteration 10/1000 | Loss: 0.00002051
Iteration 11/1000 | Loss: 0.00002041
Iteration 12/1000 | Loss: 0.00002036
Iteration 13/1000 | Loss: 0.00002033
Iteration 14/1000 | Loss: 0.00002033
Iteration 15/1000 | Loss: 0.00002033
Iteration 16/1000 | Loss: 0.00002030
Iteration 17/1000 | Loss: 0.00002030
Iteration 18/1000 | Loss: 0.00002030
Iteration 19/1000 | Loss: 0.00002030
Iteration 20/1000 | Loss: 0.00002030
Iteration 21/1000 | Loss: 0.00002030
Iteration 22/1000 | Loss: 0.00002030
Iteration 23/1000 | Loss: 0.00002029
Iteration 24/1000 | Loss: 0.00002029
Iteration 25/1000 | Loss: 0.00002029
Iteration 26/1000 | Loss: 0.00002029
Iteration 27/1000 | Loss: 0.00002029
Iteration 28/1000 | Loss: 0.00002029
Iteration 29/1000 | Loss: 0.00002029
Iteration 30/1000 | Loss: 0.00002027
Iteration 31/1000 | Loss: 0.00002027
Iteration 32/1000 | Loss: 0.00002027
Iteration 33/1000 | Loss: 0.00002027
Iteration 34/1000 | Loss: 0.00002026
Iteration 35/1000 | Loss: 0.00002025
Iteration 36/1000 | Loss: 0.00002023
Iteration 37/1000 | Loss: 0.00002023
Iteration 38/1000 | Loss: 0.00002022
Iteration 39/1000 | Loss: 0.00002022
Iteration 40/1000 | Loss: 0.00002022
Iteration 41/1000 | Loss: 0.00002022
Iteration 42/1000 | Loss: 0.00002022
Iteration 43/1000 | Loss: 0.00002022
Iteration 44/1000 | Loss: 0.00002022
Iteration 45/1000 | Loss: 0.00002022
Iteration 46/1000 | Loss: 0.00002022
Iteration 47/1000 | Loss: 0.00002022
Iteration 48/1000 | Loss: 0.00002022
Iteration 49/1000 | Loss: 0.00002022
Iteration 50/1000 | Loss: 0.00002022
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [2.022278385993559e-05, 2.022278385993559e-05, 2.022278385993559e-05, 2.022278385993559e-05, 2.022278385993559e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.022278385993559e-05

Optimization complete. Final v2v error: 3.84266996383667 mm

Highest mean error: 4.069009780883789 mm for frame 0

Lowest mean error: 3.792397975921631 mm for frame 34

Saving results

Total time: 26.32490849494934
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424657
Iteration 2/25 | Loss: 0.00138527
Iteration 3/25 | Loss: 0.00125278
Iteration 4/25 | Loss: 0.00124436
Iteration 5/25 | Loss: 0.00124332
Iteration 6/25 | Loss: 0.00124332
Iteration 7/25 | Loss: 0.00124332
Iteration 8/25 | Loss: 0.00124332
Iteration 9/25 | Loss: 0.00124332
Iteration 10/25 | Loss: 0.00124332
Iteration 11/25 | Loss: 0.00124332
Iteration 12/25 | Loss: 0.00124332
Iteration 13/25 | Loss: 0.00124332
Iteration 14/25 | Loss: 0.00124332
Iteration 15/25 | Loss: 0.00124332
Iteration 16/25 | Loss: 0.00124332
Iteration 17/25 | Loss: 0.00124332
Iteration 18/25 | Loss: 0.00124332
Iteration 19/25 | Loss: 0.00124332
Iteration 20/25 | Loss: 0.00124332
Iteration 21/25 | Loss: 0.00124332
Iteration 22/25 | Loss: 0.00124332
Iteration 23/25 | Loss: 0.00124332
Iteration 24/25 | Loss: 0.00124332
Iteration 25/25 | Loss: 0.00124332

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26600647
Iteration 2/25 | Loss: 0.00131042
Iteration 3/25 | Loss: 0.00131041
Iteration 4/25 | Loss: 0.00131041
Iteration 5/25 | Loss: 0.00131041
Iteration 6/25 | Loss: 0.00131041
Iteration 7/25 | Loss: 0.00131041
Iteration 8/25 | Loss: 0.00131041
Iteration 9/25 | Loss: 0.00131041
Iteration 10/25 | Loss: 0.00131041
Iteration 11/25 | Loss: 0.00131041
Iteration 12/25 | Loss: 0.00131041
Iteration 13/25 | Loss: 0.00131041
Iteration 14/25 | Loss: 0.00131041
Iteration 15/25 | Loss: 0.00131041
Iteration 16/25 | Loss: 0.00131041
Iteration 17/25 | Loss: 0.00131041
Iteration 18/25 | Loss: 0.00131041
Iteration 19/25 | Loss: 0.00131041
Iteration 20/25 | Loss: 0.00131041
Iteration 21/25 | Loss: 0.00131041
Iteration 22/25 | Loss: 0.00131041
Iteration 23/25 | Loss: 0.00131041
Iteration 24/25 | Loss: 0.00131041
Iteration 25/25 | Loss: 0.00131041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131041
Iteration 2/1000 | Loss: 0.00004311
Iteration 3/1000 | Loss: 0.00002264
Iteration 4/1000 | Loss: 0.00001917
Iteration 5/1000 | Loss: 0.00001825
Iteration 6/1000 | Loss: 0.00001756
Iteration 7/1000 | Loss: 0.00001699
Iteration 8/1000 | Loss: 0.00001670
Iteration 9/1000 | Loss: 0.00001650
Iteration 10/1000 | Loss: 0.00001647
Iteration 11/1000 | Loss: 0.00001641
Iteration 12/1000 | Loss: 0.00001632
Iteration 13/1000 | Loss: 0.00001632
Iteration 14/1000 | Loss: 0.00001632
Iteration 15/1000 | Loss: 0.00001630
Iteration 16/1000 | Loss: 0.00001627
Iteration 17/1000 | Loss: 0.00001624
Iteration 18/1000 | Loss: 0.00001619
Iteration 19/1000 | Loss: 0.00001603
Iteration 20/1000 | Loss: 0.00001598
Iteration 21/1000 | Loss: 0.00001596
Iteration 22/1000 | Loss: 0.00001595
Iteration 23/1000 | Loss: 0.00001594
Iteration 24/1000 | Loss: 0.00001593
Iteration 25/1000 | Loss: 0.00001593
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001592
Iteration 29/1000 | Loss: 0.00001592
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001591
Iteration 32/1000 | Loss: 0.00001591
Iteration 33/1000 | Loss: 0.00001591
Iteration 34/1000 | Loss: 0.00001590
Iteration 35/1000 | Loss: 0.00001590
Iteration 36/1000 | Loss: 0.00001590
Iteration 37/1000 | Loss: 0.00001588
Iteration 38/1000 | Loss: 0.00001588
Iteration 39/1000 | Loss: 0.00001587
Iteration 40/1000 | Loss: 0.00001586
Iteration 41/1000 | Loss: 0.00001586
Iteration 42/1000 | Loss: 0.00001586
Iteration 43/1000 | Loss: 0.00001585
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001584
Iteration 46/1000 | Loss: 0.00001584
Iteration 47/1000 | Loss: 0.00001584
Iteration 48/1000 | Loss: 0.00001584
Iteration 49/1000 | Loss: 0.00001584
Iteration 50/1000 | Loss: 0.00001583
Iteration 51/1000 | Loss: 0.00001582
Iteration 52/1000 | Loss: 0.00001581
Iteration 53/1000 | Loss: 0.00001581
Iteration 54/1000 | Loss: 0.00001579
Iteration 55/1000 | Loss: 0.00001579
Iteration 56/1000 | Loss: 0.00001579
Iteration 57/1000 | Loss: 0.00001579
Iteration 58/1000 | Loss: 0.00001579
Iteration 59/1000 | Loss: 0.00001578
Iteration 60/1000 | Loss: 0.00001578
Iteration 61/1000 | Loss: 0.00001578
Iteration 62/1000 | Loss: 0.00001578
Iteration 63/1000 | Loss: 0.00001576
Iteration 64/1000 | Loss: 0.00001576
Iteration 65/1000 | Loss: 0.00001575
Iteration 66/1000 | Loss: 0.00001575
Iteration 67/1000 | Loss: 0.00001575
Iteration 68/1000 | Loss: 0.00001575
Iteration 69/1000 | Loss: 0.00001574
Iteration 70/1000 | Loss: 0.00001574
Iteration 71/1000 | Loss: 0.00001574
Iteration 72/1000 | Loss: 0.00001573
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001572
Iteration 81/1000 | Loss: 0.00001572
Iteration 82/1000 | Loss: 0.00001572
Iteration 83/1000 | Loss: 0.00001572
Iteration 84/1000 | Loss: 0.00001572
Iteration 85/1000 | Loss: 0.00001572
Iteration 86/1000 | Loss: 0.00001572
Iteration 87/1000 | Loss: 0.00001571
Iteration 88/1000 | Loss: 0.00001571
Iteration 89/1000 | Loss: 0.00001571
Iteration 90/1000 | Loss: 0.00001571
Iteration 91/1000 | Loss: 0.00001571
Iteration 92/1000 | Loss: 0.00001570
Iteration 93/1000 | Loss: 0.00001570
Iteration 94/1000 | Loss: 0.00001570
Iteration 95/1000 | Loss: 0.00001570
Iteration 96/1000 | Loss: 0.00001570
Iteration 97/1000 | Loss: 0.00001570
Iteration 98/1000 | Loss: 0.00001570
Iteration 99/1000 | Loss: 0.00001570
Iteration 100/1000 | Loss: 0.00001569
Iteration 101/1000 | Loss: 0.00001569
Iteration 102/1000 | Loss: 0.00001569
Iteration 103/1000 | Loss: 0.00001569
Iteration 104/1000 | Loss: 0.00001569
Iteration 105/1000 | Loss: 0.00001569
Iteration 106/1000 | Loss: 0.00001569
Iteration 107/1000 | Loss: 0.00001569
Iteration 108/1000 | Loss: 0.00001569
Iteration 109/1000 | Loss: 0.00001569
Iteration 110/1000 | Loss: 0.00001569
Iteration 111/1000 | Loss: 0.00001569
Iteration 112/1000 | Loss: 0.00001569
Iteration 113/1000 | Loss: 0.00001569
Iteration 114/1000 | Loss: 0.00001569
Iteration 115/1000 | Loss: 0.00001569
Iteration 116/1000 | Loss: 0.00001568
Iteration 117/1000 | Loss: 0.00001568
Iteration 118/1000 | Loss: 0.00001568
Iteration 119/1000 | Loss: 0.00001568
Iteration 120/1000 | Loss: 0.00001568
Iteration 121/1000 | Loss: 0.00001568
Iteration 122/1000 | Loss: 0.00001568
Iteration 123/1000 | Loss: 0.00001568
Iteration 124/1000 | Loss: 0.00001568
Iteration 125/1000 | Loss: 0.00001568
Iteration 126/1000 | Loss: 0.00001568
Iteration 127/1000 | Loss: 0.00001568
Iteration 128/1000 | Loss: 0.00001568
Iteration 129/1000 | Loss: 0.00001568
Iteration 130/1000 | Loss: 0.00001567
Iteration 131/1000 | Loss: 0.00001567
Iteration 132/1000 | Loss: 0.00001567
Iteration 133/1000 | Loss: 0.00001567
Iteration 134/1000 | Loss: 0.00001567
Iteration 135/1000 | Loss: 0.00001567
Iteration 136/1000 | Loss: 0.00001567
Iteration 137/1000 | Loss: 0.00001567
Iteration 138/1000 | Loss: 0.00001567
Iteration 139/1000 | Loss: 0.00001567
Iteration 140/1000 | Loss: 0.00001567
Iteration 141/1000 | Loss: 0.00001567
Iteration 142/1000 | Loss: 0.00001567
Iteration 143/1000 | Loss: 0.00001567
Iteration 144/1000 | Loss: 0.00001567
Iteration 145/1000 | Loss: 0.00001567
Iteration 146/1000 | Loss: 0.00001566
Iteration 147/1000 | Loss: 0.00001566
Iteration 148/1000 | Loss: 0.00001566
Iteration 149/1000 | Loss: 0.00001566
Iteration 150/1000 | Loss: 0.00001566
Iteration 151/1000 | Loss: 0.00001566
Iteration 152/1000 | Loss: 0.00001566
Iteration 153/1000 | Loss: 0.00001566
Iteration 154/1000 | Loss: 0.00001566
Iteration 155/1000 | Loss: 0.00001566
Iteration 156/1000 | Loss: 0.00001566
Iteration 157/1000 | Loss: 0.00001566
Iteration 158/1000 | Loss: 0.00001566
Iteration 159/1000 | Loss: 0.00001566
Iteration 160/1000 | Loss: 0.00001566
Iteration 161/1000 | Loss: 0.00001566
Iteration 162/1000 | Loss: 0.00001566
Iteration 163/1000 | Loss: 0.00001566
Iteration 164/1000 | Loss: 0.00001566
Iteration 165/1000 | Loss: 0.00001565
Iteration 166/1000 | Loss: 0.00001565
Iteration 167/1000 | Loss: 0.00001565
Iteration 168/1000 | Loss: 0.00001565
Iteration 169/1000 | Loss: 0.00001565
Iteration 170/1000 | Loss: 0.00001565
Iteration 171/1000 | Loss: 0.00001565
Iteration 172/1000 | Loss: 0.00001565
Iteration 173/1000 | Loss: 0.00001565
Iteration 174/1000 | Loss: 0.00001565
Iteration 175/1000 | Loss: 0.00001565
Iteration 176/1000 | Loss: 0.00001565
Iteration 177/1000 | Loss: 0.00001565
Iteration 178/1000 | Loss: 0.00001565
Iteration 179/1000 | Loss: 0.00001565
Iteration 180/1000 | Loss: 0.00001565
Iteration 181/1000 | Loss: 0.00001565
Iteration 182/1000 | Loss: 0.00001565
Iteration 183/1000 | Loss: 0.00001565
Iteration 184/1000 | Loss: 0.00001565
Iteration 185/1000 | Loss: 0.00001565
Iteration 186/1000 | Loss: 0.00001565
Iteration 187/1000 | Loss: 0.00001565
Iteration 188/1000 | Loss: 0.00001565
Iteration 189/1000 | Loss: 0.00001565
Iteration 190/1000 | Loss: 0.00001565
Iteration 191/1000 | Loss: 0.00001565
Iteration 192/1000 | Loss: 0.00001565
Iteration 193/1000 | Loss: 0.00001565
Iteration 194/1000 | Loss: 0.00001565
Iteration 195/1000 | Loss: 0.00001565
Iteration 196/1000 | Loss: 0.00001565
Iteration 197/1000 | Loss: 0.00001565
Iteration 198/1000 | Loss: 0.00001565
Iteration 199/1000 | Loss: 0.00001565
Iteration 200/1000 | Loss: 0.00001565
Iteration 201/1000 | Loss: 0.00001565
Iteration 202/1000 | Loss: 0.00001565
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.5645726307411678e-05, 1.5645726307411678e-05, 1.5645726307411678e-05, 1.5645726307411678e-05, 1.5645726307411678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5645726307411678e-05

Optimization complete. Final v2v error: 3.2965023517608643 mm

Highest mean error: 4.165524482727051 mm for frame 61

Lowest mean error: 3.0472066402435303 mm for frame 3

Saving results

Total time: 35.28031587600708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01097686
Iteration 2/25 | Loss: 0.00233962
Iteration 3/25 | Loss: 0.00189322
Iteration 4/25 | Loss: 0.00184863
Iteration 5/25 | Loss: 0.00173326
Iteration 6/25 | Loss: 0.00160241
Iteration 7/25 | Loss: 0.00152192
Iteration 8/25 | Loss: 0.00148945
Iteration 9/25 | Loss: 0.00140790
Iteration 10/25 | Loss: 0.00137969
Iteration 11/25 | Loss: 0.00136310
Iteration 12/25 | Loss: 0.00139121
Iteration 13/25 | Loss: 0.00138007
Iteration 14/25 | Loss: 0.00136579
Iteration 15/25 | Loss: 0.00134168
Iteration 16/25 | Loss: 0.00133058
Iteration 17/25 | Loss: 0.00132164
Iteration 18/25 | Loss: 0.00131961
Iteration 19/25 | Loss: 0.00131791
Iteration 20/25 | Loss: 0.00131366
Iteration 21/25 | Loss: 0.00131771
Iteration 22/25 | Loss: 0.00131450
Iteration 23/25 | Loss: 0.00131123
Iteration 24/25 | Loss: 0.00131191
Iteration 25/25 | Loss: 0.00131302

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35302448
Iteration 2/25 | Loss: 0.00446729
Iteration 3/25 | Loss: 0.00240949
Iteration 4/25 | Loss: 0.00240949
Iteration 5/25 | Loss: 0.00240949
Iteration 6/25 | Loss: 0.00240949
Iteration 7/25 | Loss: 0.00240949
Iteration 8/25 | Loss: 0.00240949
Iteration 9/25 | Loss: 0.00240949
Iteration 10/25 | Loss: 0.00240949
Iteration 11/25 | Loss: 0.00240949
Iteration 12/25 | Loss: 0.00240949
Iteration 13/25 | Loss: 0.00240949
Iteration 14/25 | Loss: 0.00240949
Iteration 15/25 | Loss: 0.00240949
Iteration 16/25 | Loss: 0.00240949
Iteration 17/25 | Loss: 0.00240949
Iteration 18/25 | Loss: 0.00240949
Iteration 19/25 | Loss: 0.00240949
Iteration 20/25 | Loss: 0.00240949
Iteration 21/25 | Loss: 0.00240949
Iteration 22/25 | Loss: 0.00240949
Iteration 23/25 | Loss: 0.00240949
Iteration 24/25 | Loss: 0.00240949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.002409492153674364, 0.002409492153674364, 0.002409492153674364, 0.002409492153674364, 0.002409492153674364]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002409492153674364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00240949
Iteration 2/1000 | Loss: 0.00015552
Iteration 3/1000 | Loss: 0.00018535
Iteration 4/1000 | Loss: 0.00011769
Iteration 5/1000 | Loss: 0.00018293
Iteration 6/1000 | Loss: 0.00010775
Iteration 7/1000 | Loss: 0.00012620
Iteration 8/1000 | Loss: 0.00008337
Iteration 9/1000 | Loss: 0.00012609
Iteration 10/1000 | Loss: 0.00018678
Iteration 11/1000 | Loss: 0.00017384
Iteration 12/1000 | Loss: 0.00022135
Iteration 13/1000 | Loss: 0.00014817
Iteration 14/1000 | Loss: 0.00019711
Iteration 15/1000 | Loss: 0.00037487
Iteration 16/1000 | Loss: 0.00010125
Iteration 17/1000 | Loss: 0.00005724
Iteration 18/1000 | Loss: 0.00013572
Iteration 19/1000 | Loss: 0.00014698
Iteration 20/1000 | Loss: 0.00106420
Iteration 21/1000 | Loss: 0.00041936
Iteration 22/1000 | Loss: 0.00013249
Iteration 23/1000 | Loss: 0.00018636
Iteration 24/1000 | Loss: 0.00017428
Iteration 25/1000 | Loss: 0.00022038
Iteration 26/1000 | Loss: 0.00034423
Iteration 27/1000 | Loss: 0.00017005
Iteration 28/1000 | Loss: 0.00030242
Iteration 29/1000 | Loss: 0.00022754
Iteration 30/1000 | Loss: 0.00015380
Iteration 31/1000 | Loss: 0.00008866
Iteration 32/1000 | Loss: 0.00015758
Iteration 33/1000 | Loss: 0.00017433
Iteration 34/1000 | Loss: 0.00019257
Iteration 35/1000 | Loss: 0.00022800
Iteration 36/1000 | Loss: 0.00029742
Iteration 37/1000 | Loss: 0.00007566
Iteration 38/1000 | Loss: 0.00004923
Iteration 39/1000 | Loss: 0.00014435
Iteration 40/1000 | Loss: 0.00009253
Iteration 41/1000 | Loss: 0.00009488
Iteration 42/1000 | Loss: 0.00010137
Iteration 43/1000 | Loss: 0.00015393
Iteration 44/1000 | Loss: 0.00016456
Iteration 45/1000 | Loss: 0.00014732
Iteration 46/1000 | Loss: 0.00018478
Iteration 47/1000 | Loss: 0.00005529
Iteration 48/1000 | Loss: 0.00007607
Iteration 49/1000 | Loss: 0.00017405
Iteration 50/1000 | Loss: 0.00013588
Iteration 51/1000 | Loss: 0.00016835
Iteration 52/1000 | Loss: 0.00014266
Iteration 53/1000 | Loss: 0.00010075
Iteration 54/1000 | Loss: 0.00007232
Iteration 55/1000 | Loss: 0.00026171
Iteration 56/1000 | Loss: 0.00004452
Iteration 57/1000 | Loss: 0.00006892
Iteration 58/1000 | Loss: 0.00005209
Iteration 59/1000 | Loss: 0.00006228
Iteration 60/1000 | Loss: 0.00005748
Iteration 61/1000 | Loss: 0.00005616
Iteration 62/1000 | Loss: 0.00004546
Iteration 63/1000 | Loss: 0.00005524
Iteration 64/1000 | Loss: 0.00004734
Iteration 65/1000 | Loss: 0.00005129
Iteration 66/1000 | Loss: 0.00004172
Iteration 67/1000 | Loss: 0.00003384
Iteration 68/1000 | Loss: 0.00003304
Iteration 69/1000 | Loss: 0.00061502
Iteration 70/1000 | Loss: 0.00005946
Iteration 71/1000 | Loss: 0.00003694
Iteration 72/1000 | Loss: 0.00003177
Iteration 73/1000 | Loss: 0.00003205
Iteration 74/1000 | Loss: 0.00002916
Iteration 75/1000 | Loss: 0.00003588
Iteration 76/1000 | Loss: 0.00004166
Iteration 77/1000 | Loss: 0.00002751
Iteration 78/1000 | Loss: 0.00002864
Iteration 79/1000 | Loss: 0.00002709
Iteration 80/1000 | Loss: 0.00002703
Iteration 81/1000 | Loss: 0.00002696
Iteration 82/1000 | Loss: 0.00002694
Iteration 83/1000 | Loss: 0.00002693
Iteration 84/1000 | Loss: 0.00002692
Iteration 85/1000 | Loss: 0.00003116
Iteration 86/1000 | Loss: 0.00002680
Iteration 87/1000 | Loss: 0.00002674
Iteration 88/1000 | Loss: 0.00002674
Iteration 89/1000 | Loss: 0.00002674
Iteration 90/1000 | Loss: 0.00002673
Iteration 91/1000 | Loss: 0.00002673
Iteration 92/1000 | Loss: 0.00003000
Iteration 93/1000 | Loss: 0.00003179
Iteration 94/1000 | Loss: 0.00002698
Iteration 95/1000 | Loss: 0.00002666
Iteration 96/1000 | Loss: 0.00002665
Iteration 97/1000 | Loss: 0.00002665
Iteration 98/1000 | Loss: 0.00002664
Iteration 99/1000 | Loss: 0.00002664
Iteration 100/1000 | Loss: 0.00002659
Iteration 101/1000 | Loss: 0.00002658
Iteration 102/1000 | Loss: 0.00002657
Iteration 103/1000 | Loss: 0.00002657
Iteration 104/1000 | Loss: 0.00002657
Iteration 105/1000 | Loss: 0.00002656
Iteration 106/1000 | Loss: 0.00002656
Iteration 107/1000 | Loss: 0.00002656
Iteration 108/1000 | Loss: 0.00002656
Iteration 109/1000 | Loss: 0.00002656
Iteration 110/1000 | Loss: 0.00002656
Iteration 111/1000 | Loss: 0.00002656
Iteration 112/1000 | Loss: 0.00002656
Iteration 113/1000 | Loss: 0.00002656
Iteration 114/1000 | Loss: 0.00002656
Iteration 115/1000 | Loss: 0.00002656
Iteration 116/1000 | Loss: 0.00002654
Iteration 117/1000 | Loss: 0.00002654
Iteration 118/1000 | Loss: 0.00002654
Iteration 119/1000 | Loss: 0.00002654
Iteration 120/1000 | Loss: 0.00002654
Iteration 121/1000 | Loss: 0.00002654
Iteration 122/1000 | Loss: 0.00002654
Iteration 123/1000 | Loss: 0.00002653
Iteration 124/1000 | Loss: 0.00002653
Iteration 125/1000 | Loss: 0.00002653
Iteration 126/1000 | Loss: 0.00002653
Iteration 127/1000 | Loss: 0.00002653
Iteration 128/1000 | Loss: 0.00002651
Iteration 129/1000 | Loss: 0.00002920
Iteration 130/1000 | Loss: 0.00002652
Iteration 131/1000 | Loss: 0.00002651
Iteration 132/1000 | Loss: 0.00002650
Iteration 133/1000 | Loss: 0.00002650
Iteration 134/1000 | Loss: 0.00002650
Iteration 135/1000 | Loss: 0.00002650
Iteration 136/1000 | Loss: 0.00002650
Iteration 137/1000 | Loss: 0.00002662
Iteration 138/1000 | Loss: 0.00002649
Iteration 139/1000 | Loss: 0.00002649
Iteration 140/1000 | Loss: 0.00002649
Iteration 141/1000 | Loss: 0.00002649
Iteration 142/1000 | Loss: 0.00002649
Iteration 143/1000 | Loss: 0.00002649
Iteration 144/1000 | Loss: 0.00002648
Iteration 145/1000 | Loss: 0.00002648
Iteration 146/1000 | Loss: 0.00002647
Iteration 147/1000 | Loss: 0.00002693
Iteration 148/1000 | Loss: 0.00002654
Iteration 149/1000 | Loss: 0.00002647
Iteration 150/1000 | Loss: 0.00002647
Iteration 151/1000 | Loss: 0.00002646
Iteration 152/1000 | Loss: 0.00002646
Iteration 153/1000 | Loss: 0.00002646
Iteration 154/1000 | Loss: 0.00002646
Iteration 155/1000 | Loss: 0.00002646
Iteration 156/1000 | Loss: 0.00002645
Iteration 157/1000 | Loss: 0.00002645
Iteration 158/1000 | Loss: 0.00002650
Iteration 159/1000 | Loss: 0.00002645
Iteration 160/1000 | Loss: 0.00002645
Iteration 161/1000 | Loss: 0.00002645
Iteration 162/1000 | Loss: 0.00002645
Iteration 163/1000 | Loss: 0.00002645
Iteration 164/1000 | Loss: 0.00002645
Iteration 165/1000 | Loss: 0.00002644
Iteration 166/1000 | Loss: 0.00002644
Iteration 167/1000 | Loss: 0.00002644
Iteration 168/1000 | Loss: 0.00002644
Iteration 169/1000 | Loss: 0.00002644
Iteration 170/1000 | Loss: 0.00002643
Iteration 171/1000 | Loss: 0.00002643
Iteration 172/1000 | Loss: 0.00002643
Iteration 173/1000 | Loss: 0.00002642
Iteration 174/1000 | Loss: 0.00002642
Iteration 175/1000 | Loss: 0.00002642
Iteration 176/1000 | Loss: 0.00002642
Iteration 177/1000 | Loss: 0.00002642
Iteration 178/1000 | Loss: 0.00002642
Iteration 179/1000 | Loss: 0.00002642
Iteration 180/1000 | Loss: 0.00002642
Iteration 181/1000 | Loss: 0.00002642
Iteration 182/1000 | Loss: 0.00002642
Iteration 183/1000 | Loss: 0.00002642
Iteration 184/1000 | Loss: 0.00002642
Iteration 185/1000 | Loss: 0.00002642
Iteration 186/1000 | Loss: 0.00002642
Iteration 187/1000 | Loss: 0.00002642
Iteration 188/1000 | Loss: 0.00002641
Iteration 189/1000 | Loss: 0.00002641
Iteration 190/1000 | Loss: 0.00002641
Iteration 191/1000 | Loss: 0.00002640
Iteration 192/1000 | Loss: 0.00002640
Iteration 193/1000 | Loss: 0.00002640
Iteration 194/1000 | Loss: 0.00002640
Iteration 195/1000 | Loss: 0.00002640
Iteration 196/1000 | Loss: 0.00002640
Iteration 197/1000 | Loss: 0.00002640
Iteration 198/1000 | Loss: 0.00002640
Iteration 199/1000 | Loss: 0.00002640
Iteration 200/1000 | Loss: 0.00002640
Iteration 201/1000 | Loss: 0.00002640
Iteration 202/1000 | Loss: 0.00002640
Iteration 203/1000 | Loss: 0.00002640
Iteration 204/1000 | Loss: 0.00002640
Iteration 205/1000 | Loss: 0.00002640
Iteration 206/1000 | Loss: 0.00002640
Iteration 207/1000 | Loss: 0.00002640
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 207. Stopping optimization.
Last 5 losses: [2.639526610437315e-05, 2.639526610437315e-05, 2.639526610437315e-05, 2.639526610437315e-05, 2.639526610437315e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.639526610437315e-05

Optimization complete. Final v2v error: 3.6806843280792236 mm

Highest mean error: 12.458402633666992 mm for frame 94

Lowest mean error: 3.1039986610412598 mm for frame 37

Saving results

Total time: 173.47331595420837
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00446144
Iteration 2/25 | Loss: 0.00140204
Iteration 3/25 | Loss: 0.00129404
Iteration 4/25 | Loss: 0.00128830
Iteration 5/25 | Loss: 0.00128702
Iteration 6/25 | Loss: 0.00128684
Iteration 7/25 | Loss: 0.00128684
Iteration 8/25 | Loss: 0.00128684
Iteration 9/25 | Loss: 0.00128684
Iteration 10/25 | Loss: 0.00128684
Iteration 11/25 | Loss: 0.00128684
Iteration 12/25 | Loss: 0.00128684
Iteration 13/25 | Loss: 0.00128684
Iteration 14/25 | Loss: 0.00128684
Iteration 15/25 | Loss: 0.00128684
Iteration 16/25 | Loss: 0.00128684
Iteration 17/25 | Loss: 0.00128684
Iteration 18/25 | Loss: 0.00128684
Iteration 19/25 | Loss: 0.00128684
Iteration 20/25 | Loss: 0.00128684
Iteration 21/25 | Loss: 0.00128684
Iteration 22/25 | Loss: 0.00128684
Iteration 23/25 | Loss: 0.00128684
Iteration 24/25 | Loss: 0.00128684
Iteration 25/25 | Loss: 0.00128684

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24385417
Iteration 2/25 | Loss: 0.00140436
Iteration 3/25 | Loss: 0.00140436
Iteration 4/25 | Loss: 0.00140436
Iteration 5/25 | Loss: 0.00140436
Iteration 6/25 | Loss: 0.00140436
Iteration 7/25 | Loss: 0.00140436
Iteration 8/25 | Loss: 0.00140436
Iteration 9/25 | Loss: 0.00140436
Iteration 10/25 | Loss: 0.00140436
Iteration 11/25 | Loss: 0.00140436
Iteration 12/25 | Loss: 0.00140436
Iteration 13/25 | Loss: 0.00140436
Iteration 14/25 | Loss: 0.00140436
Iteration 15/25 | Loss: 0.00140436
Iteration 16/25 | Loss: 0.00140436
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014043611008673906, 0.0014043611008673906, 0.0014043611008673906, 0.0014043611008673906, 0.0014043611008673906]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014043611008673906

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140436
Iteration 2/1000 | Loss: 0.00003888
Iteration 3/1000 | Loss: 0.00002376
Iteration 4/1000 | Loss: 0.00002043
Iteration 5/1000 | Loss: 0.00001920
Iteration 6/1000 | Loss: 0.00001856
Iteration 7/1000 | Loss: 0.00001804
Iteration 8/1000 | Loss: 0.00001760
Iteration 9/1000 | Loss: 0.00001732
Iteration 10/1000 | Loss: 0.00001711
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001696
Iteration 13/1000 | Loss: 0.00001695
Iteration 14/1000 | Loss: 0.00001686
Iteration 15/1000 | Loss: 0.00001677
Iteration 16/1000 | Loss: 0.00001676
Iteration 17/1000 | Loss: 0.00001671
Iteration 18/1000 | Loss: 0.00001671
Iteration 19/1000 | Loss: 0.00001671
Iteration 20/1000 | Loss: 0.00001670
Iteration 21/1000 | Loss: 0.00001670
Iteration 22/1000 | Loss: 0.00001670
Iteration 23/1000 | Loss: 0.00001670
Iteration 24/1000 | Loss: 0.00001670
Iteration 25/1000 | Loss: 0.00001669
Iteration 26/1000 | Loss: 0.00001669
Iteration 27/1000 | Loss: 0.00001667
Iteration 28/1000 | Loss: 0.00001666
Iteration 29/1000 | Loss: 0.00001666
Iteration 30/1000 | Loss: 0.00001665
Iteration 31/1000 | Loss: 0.00001665
Iteration 32/1000 | Loss: 0.00001661
Iteration 33/1000 | Loss: 0.00001658
Iteration 34/1000 | Loss: 0.00001658
Iteration 35/1000 | Loss: 0.00001657
Iteration 36/1000 | Loss: 0.00001657
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001650
Iteration 39/1000 | Loss: 0.00001648
Iteration 40/1000 | Loss: 0.00001646
Iteration 41/1000 | Loss: 0.00001646
Iteration 42/1000 | Loss: 0.00001645
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001644
Iteration 46/1000 | Loss: 0.00001644
Iteration 47/1000 | Loss: 0.00001644
Iteration 48/1000 | Loss: 0.00001644
Iteration 49/1000 | Loss: 0.00001644
Iteration 50/1000 | Loss: 0.00001643
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001641
Iteration 53/1000 | Loss: 0.00001640
Iteration 54/1000 | Loss: 0.00001640
Iteration 55/1000 | Loss: 0.00001640
Iteration 56/1000 | Loss: 0.00001639
Iteration 57/1000 | Loss: 0.00001639
Iteration 58/1000 | Loss: 0.00001639
Iteration 59/1000 | Loss: 0.00001639
Iteration 60/1000 | Loss: 0.00001639
Iteration 61/1000 | Loss: 0.00001638
Iteration 62/1000 | Loss: 0.00001638
Iteration 63/1000 | Loss: 0.00001638
Iteration 64/1000 | Loss: 0.00001638
Iteration 65/1000 | Loss: 0.00001637
Iteration 66/1000 | Loss: 0.00001637
Iteration 67/1000 | Loss: 0.00001636
Iteration 68/1000 | Loss: 0.00001636
Iteration 69/1000 | Loss: 0.00001636
Iteration 70/1000 | Loss: 0.00001635
Iteration 71/1000 | Loss: 0.00001635
Iteration 72/1000 | Loss: 0.00001635
Iteration 73/1000 | Loss: 0.00001634
Iteration 74/1000 | Loss: 0.00001634
Iteration 75/1000 | Loss: 0.00001634
Iteration 76/1000 | Loss: 0.00001633
Iteration 77/1000 | Loss: 0.00001633
Iteration 78/1000 | Loss: 0.00001632
Iteration 79/1000 | Loss: 0.00001632
Iteration 80/1000 | Loss: 0.00001631
Iteration 81/1000 | Loss: 0.00001631
Iteration 82/1000 | Loss: 0.00001631
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001631
Iteration 85/1000 | Loss: 0.00001630
Iteration 86/1000 | Loss: 0.00001630
Iteration 87/1000 | Loss: 0.00001630
Iteration 88/1000 | Loss: 0.00001629
Iteration 89/1000 | Loss: 0.00001629
Iteration 90/1000 | Loss: 0.00001629
Iteration 91/1000 | Loss: 0.00001628
Iteration 92/1000 | Loss: 0.00001627
Iteration 93/1000 | Loss: 0.00001627
Iteration 94/1000 | Loss: 0.00001627
Iteration 95/1000 | Loss: 0.00001627
Iteration 96/1000 | Loss: 0.00001627
Iteration 97/1000 | Loss: 0.00001626
Iteration 98/1000 | Loss: 0.00001626
Iteration 99/1000 | Loss: 0.00001626
Iteration 100/1000 | Loss: 0.00001626
Iteration 101/1000 | Loss: 0.00001626
Iteration 102/1000 | Loss: 0.00001625
Iteration 103/1000 | Loss: 0.00001625
Iteration 104/1000 | Loss: 0.00001625
Iteration 105/1000 | Loss: 0.00001625
Iteration 106/1000 | Loss: 0.00001624
Iteration 107/1000 | Loss: 0.00001624
Iteration 108/1000 | Loss: 0.00001624
Iteration 109/1000 | Loss: 0.00001624
Iteration 110/1000 | Loss: 0.00001624
Iteration 111/1000 | Loss: 0.00001624
Iteration 112/1000 | Loss: 0.00001624
Iteration 113/1000 | Loss: 0.00001624
Iteration 114/1000 | Loss: 0.00001624
Iteration 115/1000 | Loss: 0.00001624
Iteration 116/1000 | Loss: 0.00001624
Iteration 117/1000 | Loss: 0.00001624
Iteration 118/1000 | Loss: 0.00001624
Iteration 119/1000 | Loss: 0.00001624
Iteration 120/1000 | Loss: 0.00001624
Iteration 121/1000 | Loss: 0.00001624
Iteration 122/1000 | Loss: 0.00001624
Iteration 123/1000 | Loss: 0.00001624
Iteration 124/1000 | Loss: 0.00001624
Iteration 125/1000 | Loss: 0.00001624
Iteration 126/1000 | Loss: 0.00001624
Iteration 127/1000 | Loss: 0.00001624
Iteration 128/1000 | Loss: 0.00001624
Iteration 129/1000 | Loss: 0.00001624
Iteration 130/1000 | Loss: 0.00001624
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.6239828255493194e-05, 1.6239828255493194e-05, 1.6239828255493194e-05, 1.6239828255493194e-05, 1.6239828255493194e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6239828255493194e-05

Optimization complete. Final v2v error: 3.354647636413574 mm

Highest mean error: 4.179983615875244 mm for frame 60

Lowest mean error: 3.030412435531616 mm for frame 206

Saving results

Total time: 41.54782772064209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00824856
Iteration 2/25 | Loss: 0.00154957
Iteration 3/25 | Loss: 0.00129507
Iteration 4/25 | Loss: 0.00127027
Iteration 5/25 | Loss: 0.00126529
Iteration 6/25 | Loss: 0.00126341
Iteration 7/25 | Loss: 0.00126291
Iteration 8/25 | Loss: 0.00126273
Iteration 9/25 | Loss: 0.00126263
Iteration 10/25 | Loss: 0.00126260
Iteration 11/25 | Loss: 0.00126260
Iteration 12/25 | Loss: 0.00126260
Iteration 13/25 | Loss: 0.00126260
Iteration 14/25 | Loss: 0.00126259
Iteration 15/25 | Loss: 0.00126259
Iteration 16/25 | Loss: 0.00126259
Iteration 17/25 | Loss: 0.00126259
Iteration 18/25 | Loss: 0.00126259
Iteration 19/25 | Loss: 0.00126259
Iteration 20/25 | Loss: 0.00126259
Iteration 21/25 | Loss: 0.00126259
Iteration 22/25 | Loss: 0.00126259
Iteration 23/25 | Loss: 0.00126258
Iteration 24/25 | Loss: 0.00126258
Iteration 25/25 | Loss: 0.00126258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58255959
Iteration 2/25 | Loss: 0.00126067
Iteration 3/25 | Loss: 0.00126065
Iteration 4/25 | Loss: 0.00126065
Iteration 5/25 | Loss: 0.00126065
Iteration 6/25 | Loss: 0.00126065
Iteration 7/25 | Loss: 0.00126065
Iteration 8/25 | Loss: 0.00126065
Iteration 9/25 | Loss: 0.00126065
Iteration 10/25 | Loss: 0.00126065
Iteration 11/25 | Loss: 0.00126065
Iteration 12/25 | Loss: 0.00126065
Iteration 13/25 | Loss: 0.00126065
Iteration 14/25 | Loss: 0.00126065
Iteration 15/25 | Loss: 0.00126065
Iteration 16/25 | Loss: 0.00126065
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001260649412870407, 0.001260649412870407, 0.001260649412870407, 0.001260649412870407, 0.001260649412870407]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001260649412870407

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126065
Iteration 2/1000 | Loss: 0.00003709
Iteration 3/1000 | Loss: 0.00002777
Iteration 4/1000 | Loss: 0.00003021
Iteration 5/1000 | Loss: 0.00003056
Iteration 6/1000 | Loss: 0.00002029
Iteration 7/1000 | Loss: 0.00002157
Iteration 8/1000 | Loss: 0.00002123
Iteration 9/1000 | Loss: 0.00002119
Iteration 10/1000 | Loss: 0.00001858
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001847
Iteration 13/1000 | Loss: 0.00001801
Iteration 14/1000 | Loss: 0.00001777
Iteration 15/1000 | Loss: 0.00001777
Iteration 16/1000 | Loss: 0.00001775
Iteration 17/1000 | Loss: 0.00001773
Iteration 18/1000 | Loss: 0.00001770
Iteration 19/1000 | Loss: 0.00001770
Iteration 20/1000 | Loss: 0.00001768
Iteration 21/1000 | Loss: 0.00001768
Iteration 22/1000 | Loss: 0.00001765
Iteration 23/1000 | Loss: 0.00001764
Iteration 24/1000 | Loss: 0.00001762
Iteration 25/1000 | Loss: 0.00001761
Iteration 26/1000 | Loss: 0.00001761
Iteration 27/1000 | Loss: 0.00001760
Iteration 28/1000 | Loss: 0.00001759
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001758
Iteration 31/1000 | Loss: 0.00001756
Iteration 32/1000 | Loss: 0.00002217
Iteration 33/1000 | Loss: 0.00002332
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001742
Iteration 36/1000 | Loss: 0.00001740
Iteration 37/1000 | Loss: 0.00001740
Iteration 38/1000 | Loss: 0.00001740
Iteration 39/1000 | Loss: 0.00001740
Iteration 40/1000 | Loss: 0.00001740
Iteration 41/1000 | Loss: 0.00001740
Iteration 42/1000 | Loss: 0.00001740
Iteration 43/1000 | Loss: 0.00001740
Iteration 44/1000 | Loss: 0.00001740
Iteration 45/1000 | Loss: 0.00001740
Iteration 46/1000 | Loss: 0.00001739
Iteration 47/1000 | Loss: 0.00001739
Iteration 48/1000 | Loss: 0.00001739
Iteration 49/1000 | Loss: 0.00001739
Iteration 50/1000 | Loss: 0.00001739
Iteration 51/1000 | Loss: 0.00001739
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00001739
Iteration 54/1000 | Loss: 0.00001739
Iteration 55/1000 | Loss: 0.00001739
Iteration 56/1000 | Loss: 0.00001739
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001738
Iteration 59/1000 | Loss: 0.00001738
Iteration 60/1000 | Loss: 0.00001738
Iteration 61/1000 | Loss: 0.00001738
Iteration 62/1000 | Loss: 0.00001738
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001737
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001736
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00002479
Iteration 71/1000 | Loss: 0.00001984
Iteration 72/1000 | Loss: 0.00002200
Iteration 73/1000 | Loss: 0.00001883
Iteration 74/1000 | Loss: 0.00001726
Iteration 75/1000 | Loss: 0.00001725
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001723
Iteration 78/1000 | Loss: 0.00001723
Iteration 79/1000 | Loss: 0.00001722
Iteration 80/1000 | Loss: 0.00001722
Iteration 81/1000 | Loss: 0.00001722
Iteration 82/1000 | Loss: 0.00001722
Iteration 83/1000 | Loss: 0.00001722
Iteration 84/1000 | Loss: 0.00001722
Iteration 85/1000 | Loss: 0.00001722
Iteration 86/1000 | Loss: 0.00001722
Iteration 87/1000 | Loss: 0.00001722
Iteration 88/1000 | Loss: 0.00001722
Iteration 89/1000 | Loss: 0.00001722
Iteration 90/1000 | Loss: 0.00001722
Iteration 91/1000 | Loss: 0.00001722
Iteration 92/1000 | Loss: 0.00001722
Iteration 93/1000 | Loss: 0.00001722
Iteration 94/1000 | Loss: 0.00001722
Iteration 95/1000 | Loss: 0.00001722
Iteration 96/1000 | Loss: 0.00001722
Iteration 97/1000 | Loss: 0.00001722
Iteration 98/1000 | Loss: 0.00001722
Iteration 99/1000 | Loss: 0.00001722
Iteration 100/1000 | Loss: 0.00001722
Iteration 101/1000 | Loss: 0.00001722
Iteration 102/1000 | Loss: 0.00001722
Iteration 103/1000 | Loss: 0.00001722
Iteration 104/1000 | Loss: 0.00001722
Iteration 105/1000 | Loss: 0.00001722
Iteration 106/1000 | Loss: 0.00001722
Iteration 107/1000 | Loss: 0.00001722
Iteration 108/1000 | Loss: 0.00001722
Iteration 109/1000 | Loss: 0.00001722
Iteration 110/1000 | Loss: 0.00001722
Iteration 111/1000 | Loss: 0.00001722
Iteration 112/1000 | Loss: 0.00001722
Iteration 113/1000 | Loss: 0.00001722
Iteration 114/1000 | Loss: 0.00001722
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001722
Iteration 117/1000 | Loss: 0.00001722
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 117. Stopping optimization.
Last 5 losses: [1.722295201034285e-05, 1.722295201034285e-05, 1.722295201034285e-05, 1.722295201034285e-05, 1.722295201034285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.722295201034285e-05

Optimization complete. Final v2v error: 3.5078158378601074 mm

Highest mean error: 3.8784258365631104 mm for frame 147

Lowest mean error: 3.2024447917938232 mm for frame 40

Saving results

Total time: 56.10759496688843
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00801781
Iteration 2/25 | Loss: 0.00135104
Iteration 3/25 | Loss: 0.00128031
Iteration 4/25 | Loss: 0.00127010
Iteration 5/25 | Loss: 0.00126615
Iteration 6/25 | Loss: 0.00126552
Iteration 7/25 | Loss: 0.00126552
Iteration 8/25 | Loss: 0.00126552
Iteration 9/25 | Loss: 0.00126552
Iteration 10/25 | Loss: 0.00126552
Iteration 11/25 | Loss: 0.00126552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012655237223953009, 0.0012655237223953009, 0.0012655237223953009, 0.0012655237223953009, 0.0012655237223953009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012655237223953009

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18647659
Iteration 2/25 | Loss: 0.00102915
Iteration 3/25 | Loss: 0.00102910
Iteration 4/25 | Loss: 0.00102909
Iteration 5/25 | Loss: 0.00102909
Iteration 6/25 | Loss: 0.00102909
Iteration 7/25 | Loss: 0.00102909
Iteration 8/25 | Loss: 0.00102909
Iteration 9/25 | Loss: 0.00102909
Iteration 10/25 | Loss: 0.00102909
Iteration 11/25 | Loss: 0.00102909
Iteration 12/25 | Loss: 0.00102909
Iteration 13/25 | Loss: 0.00102909
Iteration 14/25 | Loss: 0.00102909
Iteration 15/25 | Loss: 0.00102909
Iteration 16/25 | Loss: 0.00102909
Iteration 17/25 | Loss: 0.00102909
Iteration 18/25 | Loss: 0.00102909
Iteration 19/25 | Loss: 0.00102909
Iteration 20/25 | Loss: 0.00102909
Iteration 21/25 | Loss: 0.00102909
Iteration 22/25 | Loss: 0.00102909
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010290928184986115, 0.0010290928184986115, 0.0010290928184986115, 0.0010290928184986115, 0.0010290928184986115]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010290928184986115

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102909
Iteration 2/1000 | Loss: 0.00004916
Iteration 3/1000 | Loss: 0.00002694
Iteration 4/1000 | Loss: 0.00002068
Iteration 5/1000 | Loss: 0.00001938
Iteration 6/1000 | Loss: 0.00001863
Iteration 7/1000 | Loss: 0.00001804
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001744
Iteration 10/1000 | Loss: 0.00001730
Iteration 11/1000 | Loss: 0.00001718
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001714
Iteration 14/1000 | Loss: 0.00001713
Iteration 15/1000 | Loss: 0.00001713
Iteration 16/1000 | Loss: 0.00001713
Iteration 17/1000 | Loss: 0.00001712
Iteration 18/1000 | Loss: 0.00001710
Iteration 19/1000 | Loss: 0.00001709
Iteration 20/1000 | Loss: 0.00001709
Iteration 21/1000 | Loss: 0.00001708
Iteration 22/1000 | Loss: 0.00001707
Iteration 23/1000 | Loss: 0.00001706
Iteration 24/1000 | Loss: 0.00001706
Iteration 25/1000 | Loss: 0.00001705
Iteration 26/1000 | Loss: 0.00001705
Iteration 27/1000 | Loss: 0.00001704
Iteration 28/1000 | Loss: 0.00001704
Iteration 29/1000 | Loss: 0.00001704
Iteration 30/1000 | Loss: 0.00001703
Iteration 31/1000 | Loss: 0.00001703
Iteration 32/1000 | Loss: 0.00001701
Iteration 33/1000 | Loss: 0.00001700
Iteration 34/1000 | Loss: 0.00001697
Iteration 35/1000 | Loss: 0.00001696
Iteration 36/1000 | Loss: 0.00001695
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001691
Iteration 39/1000 | Loss: 0.00001691
Iteration 40/1000 | Loss: 0.00001691
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001690
Iteration 44/1000 | Loss: 0.00001688
Iteration 45/1000 | Loss: 0.00001688
Iteration 46/1000 | Loss: 0.00001687
Iteration 47/1000 | Loss: 0.00001687
Iteration 48/1000 | Loss: 0.00001687
Iteration 49/1000 | Loss: 0.00001687
Iteration 50/1000 | Loss: 0.00001687
Iteration 51/1000 | Loss: 0.00001686
Iteration 52/1000 | Loss: 0.00001686
Iteration 53/1000 | Loss: 0.00001686
Iteration 54/1000 | Loss: 0.00001685
Iteration 55/1000 | Loss: 0.00001685
Iteration 56/1000 | Loss: 0.00001685
Iteration 57/1000 | Loss: 0.00001684
Iteration 58/1000 | Loss: 0.00001684
Iteration 59/1000 | Loss: 0.00001684
Iteration 60/1000 | Loss: 0.00001684
Iteration 61/1000 | Loss: 0.00001684
Iteration 62/1000 | Loss: 0.00001683
Iteration 63/1000 | Loss: 0.00001683
Iteration 64/1000 | Loss: 0.00001683
Iteration 65/1000 | Loss: 0.00001683
Iteration 66/1000 | Loss: 0.00001683
Iteration 67/1000 | Loss: 0.00001683
Iteration 68/1000 | Loss: 0.00001683
Iteration 69/1000 | Loss: 0.00001683
Iteration 70/1000 | Loss: 0.00001683
Iteration 71/1000 | Loss: 0.00001683
Iteration 72/1000 | Loss: 0.00001682
Iteration 73/1000 | Loss: 0.00001682
Iteration 74/1000 | Loss: 0.00001682
Iteration 75/1000 | Loss: 0.00001682
Iteration 76/1000 | Loss: 0.00001682
Iteration 77/1000 | Loss: 0.00001682
Iteration 78/1000 | Loss: 0.00001682
Iteration 79/1000 | Loss: 0.00001681
Iteration 80/1000 | Loss: 0.00001681
Iteration 81/1000 | Loss: 0.00001681
Iteration 82/1000 | Loss: 0.00001681
Iteration 83/1000 | Loss: 0.00001681
Iteration 84/1000 | Loss: 0.00001680
Iteration 85/1000 | Loss: 0.00001680
Iteration 86/1000 | Loss: 0.00001680
Iteration 87/1000 | Loss: 0.00001680
Iteration 88/1000 | Loss: 0.00001680
Iteration 89/1000 | Loss: 0.00001680
Iteration 90/1000 | Loss: 0.00001679
Iteration 91/1000 | Loss: 0.00001679
Iteration 92/1000 | Loss: 0.00001679
Iteration 93/1000 | Loss: 0.00001679
Iteration 94/1000 | Loss: 0.00001679
Iteration 95/1000 | Loss: 0.00001679
Iteration 96/1000 | Loss: 0.00001679
Iteration 97/1000 | Loss: 0.00001679
Iteration 98/1000 | Loss: 0.00001679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.6793610484455712e-05, 1.6793610484455712e-05, 1.6793610484455712e-05, 1.6793610484455712e-05, 1.6793610484455712e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6793610484455712e-05

Optimization complete. Final v2v error: 3.511093854904175 mm

Highest mean error: 3.8926663398742676 mm for frame 12

Lowest mean error: 3.3161048889160156 mm for frame 51

Saving results

Total time: 35.242863178253174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00719481
Iteration 2/25 | Loss: 0.00180415
Iteration 3/25 | Loss: 0.00165589
Iteration 4/25 | Loss: 0.00134868
Iteration 5/25 | Loss: 0.00133706
Iteration 6/25 | Loss: 0.00133211
Iteration 7/25 | Loss: 0.00131178
Iteration 8/25 | Loss: 0.00130698
Iteration 9/25 | Loss: 0.00130624
Iteration 10/25 | Loss: 0.00130589
Iteration 11/25 | Loss: 0.00130578
Iteration 12/25 | Loss: 0.00130561
Iteration 13/25 | Loss: 0.00130492
Iteration 14/25 | Loss: 0.00130462
Iteration 15/25 | Loss: 0.00130613
Iteration 16/25 | Loss: 0.00130569
Iteration 17/25 | Loss: 0.00130472
Iteration 18/25 | Loss: 0.00130419
Iteration 19/25 | Loss: 0.00130354
Iteration 20/25 | Loss: 0.00130398
Iteration 21/25 | Loss: 0.00130340
Iteration 22/25 | Loss: 0.00130384
Iteration 23/25 | Loss: 0.00130360
Iteration 24/25 | Loss: 0.00130414
Iteration 25/25 | Loss: 0.00130363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.02245998
Iteration 2/25 | Loss: 0.00164073
Iteration 3/25 | Loss: 0.00164073
Iteration 4/25 | Loss: 0.00164072
Iteration 5/25 | Loss: 0.00164072
Iteration 6/25 | Loss: 0.00164072
Iteration 7/25 | Loss: 0.00164072
Iteration 8/25 | Loss: 0.00164072
Iteration 9/25 | Loss: 0.00164072
Iteration 10/25 | Loss: 0.00164072
Iteration 11/25 | Loss: 0.00164072
Iteration 12/25 | Loss: 0.00164072
Iteration 13/25 | Loss: 0.00164072
Iteration 14/25 | Loss: 0.00164072
Iteration 15/25 | Loss: 0.00164072
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0016407223884016275, 0.0016407223884016275, 0.0016407223884016275, 0.0016407223884016275, 0.0016407223884016275]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016407223884016275

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00164072
Iteration 2/1000 | Loss: 0.00030198
Iteration 3/1000 | Loss: 0.00004617
Iteration 4/1000 | Loss: 0.00003652
Iteration 5/1000 | Loss: 0.00003318
Iteration 6/1000 | Loss: 0.00003162
Iteration 7/1000 | Loss: 0.00003053
Iteration 8/1000 | Loss: 0.00003491
Iteration 9/1000 | Loss: 0.00002950
Iteration 10/1000 | Loss: 0.00002913
Iteration 11/1000 | Loss: 0.00002880
Iteration 12/1000 | Loss: 0.00002853
Iteration 13/1000 | Loss: 0.00002842
Iteration 14/1000 | Loss: 0.00002833
Iteration 15/1000 | Loss: 0.00002818
Iteration 16/1000 | Loss: 0.00002813
Iteration 17/1000 | Loss: 0.00002810
Iteration 18/1000 | Loss: 0.00002810
Iteration 19/1000 | Loss: 0.00002796
Iteration 20/1000 | Loss: 0.00002792
Iteration 21/1000 | Loss: 0.00002789
Iteration 22/1000 | Loss: 0.00002787
Iteration 23/1000 | Loss: 0.00002784
Iteration 24/1000 | Loss: 0.00002780
Iteration 25/1000 | Loss: 0.00002779
Iteration 26/1000 | Loss: 0.00002775
Iteration 27/1000 | Loss: 0.00002772
Iteration 28/1000 | Loss: 0.00002772
Iteration 29/1000 | Loss: 0.00002771
Iteration 30/1000 | Loss: 0.00002771
Iteration 31/1000 | Loss: 0.00002767
Iteration 32/1000 | Loss: 0.00002764
Iteration 33/1000 | Loss: 0.00002761
Iteration 34/1000 | Loss: 0.00002761
Iteration 35/1000 | Loss: 0.00002760
Iteration 36/1000 | Loss: 0.00002760
Iteration 37/1000 | Loss: 0.00002758
Iteration 38/1000 | Loss: 0.00002758
Iteration 39/1000 | Loss: 0.00002757
Iteration 40/1000 | Loss: 0.00002757
Iteration 41/1000 | Loss: 0.00002755
Iteration 42/1000 | Loss: 0.00002754
Iteration 43/1000 | Loss: 0.00002754
Iteration 44/1000 | Loss: 0.00002754
Iteration 45/1000 | Loss: 0.00002753
Iteration 46/1000 | Loss: 0.00002752
Iteration 47/1000 | Loss: 0.00002752
Iteration 48/1000 | Loss: 0.00002751
Iteration 49/1000 | Loss: 0.00002751
Iteration 50/1000 | Loss: 0.00002751
Iteration 51/1000 | Loss: 0.00002750
Iteration 52/1000 | Loss: 0.00002750
Iteration 53/1000 | Loss: 0.00002750
Iteration 54/1000 | Loss: 0.00002750
Iteration 55/1000 | Loss: 0.00002749
Iteration 56/1000 | Loss: 0.00002749
Iteration 57/1000 | Loss: 0.00002749
Iteration 58/1000 | Loss: 0.00002749
Iteration 59/1000 | Loss: 0.00002748
Iteration 60/1000 | Loss: 0.00002748
Iteration 61/1000 | Loss: 0.00002748
Iteration 62/1000 | Loss: 0.00002748
Iteration 63/1000 | Loss: 0.00002747
Iteration 64/1000 | Loss: 0.00002747
Iteration 65/1000 | Loss: 0.00002747
Iteration 66/1000 | Loss: 0.00002747
Iteration 67/1000 | Loss: 0.00002746
Iteration 68/1000 | Loss: 0.00002746
Iteration 69/1000 | Loss: 0.00002746
Iteration 70/1000 | Loss: 0.00002746
Iteration 71/1000 | Loss: 0.00002746
Iteration 72/1000 | Loss: 0.00002746
Iteration 73/1000 | Loss: 0.00002745
Iteration 74/1000 | Loss: 0.00002745
Iteration 75/1000 | Loss: 0.00002745
Iteration 76/1000 | Loss: 0.00002745
Iteration 77/1000 | Loss: 0.00002745
Iteration 78/1000 | Loss: 0.00002745
Iteration 79/1000 | Loss: 0.00002745
Iteration 80/1000 | Loss: 0.00002745
Iteration 81/1000 | Loss: 0.00002745
Iteration 82/1000 | Loss: 0.00002744
Iteration 83/1000 | Loss: 0.00002744
Iteration 84/1000 | Loss: 0.00002744
Iteration 85/1000 | Loss: 0.00002744
Iteration 86/1000 | Loss: 0.00002744
Iteration 87/1000 | Loss: 0.00002744
Iteration 88/1000 | Loss: 0.00002743
Iteration 89/1000 | Loss: 0.00002743
Iteration 90/1000 | Loss: 0.00002743
Iteration 91/1000 | Loss: 0.00002743
Iteration 92/1000 | Loss: 0.00002743
Iteration 93/1000 | Loss: 0.00002742
Iteration 94/1000 | Loss: 0.00002742
Iteration 95/1000 | Loss: 0.00002742
Iteration 96/1000 | Loss: 0.00002741
Iteration 97/1000 | Loss: 0.00002741
Iteration 98/1000 | Loss: 0.00002741
Iteration 99/1000 | Loss: 0.00002741
Iteration 100/1000 | Loss: 0.00002741
Iteration 101/1000 | Loss: 0.00002741
Iteration 102/1000 | Loss: 0.00002741
Iteration 103/1000 | Loss: 0.00002740
Iteration 104/1000 | Loss: 0.00002740
Iteration 105/1000 | Loss: 0.00002740
Iteration 106/1000 | Loss: 0.00002740
Iteration 107/1000 | Loss: 0.00002740
Iteration 108/1000 | Loss: 0.00002739
Iteration 109/1000 | Loss: 0.00002739
Iteration 110/1000 | Loss: 0.00002739
Iteration 111/1000 | Loss: 0.00002739
Iteration 112/1000 | Loss: 0.00002739
Iteration 113/1000 | Loss: 0.00002739
Iteration 114/1000 | Loss: 0.00002738
Iteration 115/1000 | Loss: 0.00002738
Iteration 116/1000 | Loss: 0.00002738
Iteration 117/1000 | Loss: 0.00002738
Iteration 118/1000 | Loss: 0.00002738
Iteration 119/1000 | Loss: 0.00002738
Iteration 120/1000 | Loss: 0.00002738
Iteration 121/1000 | Loss: 0.00002738
Iteration 122/1000 | Loss: 0.00002738
Iteration 123/1000 | Loss: 0.00002738
Iteration 124/1000 | Loss: 0.00002738
Iteration 125/1000 | Loss: 0.00002738
Iteration 126/1000 | Loss: 0.00002738
Iteration 127/1000 | Loss: 0.00002737
Iteration 128/1000 | Loss: 0.00002737
Iteration 129/1000 | Loss: 0.00002737
Iteration 130/1000 | Loss: 0.00002737
Iteration 131/1000 | Loss: 0.00002737
Iteration 132/1000 | Loss: 0.00002737
Iteration 133/1000 | Loss: 0.00002737
Iteration 134/1000 | Loss: 0.00002737
Iteration 135/1000 | Loss: 0.00002737
Iteration 136/1000 | Loss: 0.00002737
Iteration 137/1000 | Loss: 0.00002737
Iteration 138/1000 | Loss: 0.00002737
Iteration 139/1000 | Loss: 0.00002737
Iteration 140/1000 | Loss: 0.00002737
Iteration 141/1000 | Loss: 0.00002737
Iteration 142/1000 | Loss: 0.00002736
Iteration 143/1000 | Loss: 0.00002736
Iteration 144/1000 | Loss: 0.00002736
Iteration 145/1000 | Loss: 0.00002736
Iteration 146/1000 | Loss: 0.00002736
Iteration 147/1000 | Loss: 0.00002736
Iteration 148/1000 | Loss: 0.00002735
Iteration 149/1000 | Loss: 0.00002735
Iteration 150/1000 | Loss: 0.00002735
Iteration 151/1000 | Loss: 0.00002735
Iteration 152/1000 | Loss: 0.00002735
Iteration 153/1000 | Loss: 0.00002735
Iteration 154/1000 | Loss: 0.00002735
Iteration 155/1000 | Loss: 0.00002735
Iteration 156/1000 | Loss: 0.00002735
Iteration 157/1000 | Loss: 0.00002735
Iteration 158/1000 | Loss: 0.00002735
Iteration 159/1000 | Loss: 0.00002734
Iteration 160/1000 | Loss: 0.00002734
Iteration 161/1000 | Loss: 0.00002734
Iteration 162/1000 | Loss: 0.00002734
Iteration 163/1000 | Loss: 0.00002734
Iteration 164/1000 | Loss: 0.00002734
Iteration 165/1000 | Loss: 0.00002733
Iteration 166/1000 | Loss: 0.00002733
Iteration 167/1000 | Loss: 0.00002733
Iteration 168/1000 | Loss: 0.00002733
Iteration 169/1000 | Loss: 0.00002733
Iteration 170/1000 | Loss: 0.00002733
Iteration 171/1000 | Loss: 0.00002733
Iteration 172/1000 | Loss: 0.00002733
Iteration 173/1000 | Loss: 0.00002733
Iteration 174/1000 | Loss: 0.00002733
Iteration 175/1000 | Loss: 0.00002733
Iteration 176/1000 | Loss: 0.00002733
Iteration 177/1000 | Loss: 0.00002733
Iteration 178/1000 | Loss: 0.00002733
Iteration 179/1000 | Loss: 0.00002733
Iteration 180/1000 | Loss: 0.00002732
Iteration 181/1000 | Loss: 0.00002732
Iteration 182/1000 | Loss: 0.00002732
Iteration 183/1000 | Loss: 0.00002732
Iteration 184/1000 | Loss: 0.00002732
Iteration 185/1000 | Loss: 0.00002732
Iteration 186/1000 | Loss: 0.00002732
Iteration 187/1000 | Loss: 0.00002732
Iteration 188/1000 | Loss: 0.00002732
Iteration 189/1000 | Loss: 0.00002732
Iteration 190/1000 | Loss: 0.00002732
Iteration 191/1000 | Loss: 0.00002732
Iteration 192/1000 | Loss: 0.00002732
Iteration 193/1000 | Loss: 0.00002732
Iteration 194/1000 | Loss: 0.00002732
Iteration 195/1000 | Loss: 0.00002732
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.7317486456013285e-05, 2.7317486456013285e-05, 2.7317486456013285e-05, 2.7317486456013285e-05, 2.7317486456013285e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7317486456013285e-05

Optimization complete. Final v2v error: 4.194571495056152 mm

Highest mean error: 9.461469650268555 mm for frame 186

Lowest mean error: 3.2394771575927734 mm for frame 210

Saving results

Total time: 93.21001720428467
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851011
Iteration 2/25 | Loss: 0.00150540
Iteration 3/25 | Loss: 0.00132901
Iteration 4/25 | Loss: 0.00131695
Iteration 5/25 | Loss: 0.00131545
Iteration 6/25 | Loss: 0.00131545
Iteration 7/25 | Loss: 0.00131545
Iteration 8/25 | Loss: 0.00131545
Iteration 9/25 | Loss: 0.00131545
Iteration 10/25 | Loss: 0.00131545
Iteration 11/25 | Loss: 0.00131545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001315450295805931, 0.001315450295805931, 0.001315450295805931, 0.001315450295805931, 0.001315450295805931]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001315450295805931

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24779212
Iteration 2/25 | Loss: 0.00137757
Iteration 3/25 | Loss: 0.00137757
Iteration 4/25 | Loss: 0.00137757
Iteration 5/25 | Loss: 0.00137757
Iteration 6/25 | Loss: 0.00137757
Iteration 7/25 | Loss: 0.00137757
Iteration 8/25 | Loss: 0.00137757
Iteration 9/25 | Loss: 0.00137757
Iteration 10/25 | Loss: 0.00137757
Iteration 11/25 | Loss: 0.00137757
Iteration 12/25 | Loss: 0.00137757
Iteration 13/25 | Loss: 0.00137757
Iteration 14/25 | Loss: 0.00137757
Iteration 15/25 | Loss: 0.00137757
Iteration 16/25 | Loss: 0.00137757
Iteration 17/25 | Loss: 0.00137757
Iteration 18/25 | Loss: 0.00137757
Iteration 19/25 | Loss: 0.00137757
Iteration 20/25 | Loss: 0.00137757
Iteration 21/25 | Loss: 0.00137757
Iteration 22/25 | Loss: 0.00137757
Iteration 23/25 | Loss: 0.00137757
Iteration 24/25 | Loss: 0.00137757
Iteration 25/25 | Loss: 0.00137757

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137757
Iteration 2/1000 | Loss: 0.00003630
Iteration 3/1000 | Loss: 0.00002285
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001883
Iteration 6/1000 | Loss: 0.00001817
Iteration 7/1000 | Loss: 0.00001770
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00001726
Iteration 10/1000 | Loss: 0.00001716
Iteration 11/1000 | Loss: 0.00001711
Iteration 12/1000 | Loss: 0.00001711
Iteration 13/1000 | Loss: 0.00001704
Iteration 14/1000 | Loss: 0.00001698
Iteration 15/1000 | Loss: 0.00001693
Iteration 16/1000 | Loss: 0.00001686
Iteration 17/1000 | Loss: 0.00001684
Iteration 18/1000 | Loss: 0.00001683
Iteration 19/1000 | Loss: 0.00001683
Iteration 20/1000 | Loss: 0.00001681
Iteration 21/1000 | Loss: 0.00001681
Iteration 22/1000 | Loss: 0.00001680
Iteration 23/1000 | Loss: 0.00001680
Iteration 24/1000 | Loss: 0.00001679
Iteration 25/1000 | Loss: 0.00001679
Iteration 26/1000 | Loss: 0.00001679
Iteration 27/1000 | Loss: 0.00001678
Iteration 28/1000 | Loss: 0.00001677
Iteration 29/1000 | Loss: 0.00001676
Iteration 30/1000 | Loss: 0.00001676
Iteration 31/1000 | Loss: 0.00001675
Iteration 32/1000 | Loss: 0.00001674
Iteration 33/1000 | Loss: 0.00001674
Iteration 34/1000 | Loss: 0.00001674
Iteration 35/1000 | Loss: 0.00001673
Iteration 36/1000 | Loss: 0.00001672
Iteration 37/1000 | Loss: 0.00001671
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001670
Iteration 40/1000 | Loss: 0.00001670
Iteration 41/1000 | Loss: 0.00001670
Iteration 42/1000 | Loss: 0.00001669
Iteration 43/1000 | Loss: 0.00001669
Iteration 44/1000 | Loss: 0.00001669
Iteration 45/1000 | Loss: 0.00001669
Iteration 46/1000 | Loss: 0.00001668
Iteration 47/1000 | Loss: 0.00001668
Iteration 48/1000 | Loss: 0.00001667
Iteration 49/1000 | Loss: 0.00001666
Iteration 50/1000 | Loss: 0.00001664
Iteration 51/1000 | Loss: 0.00001664
Iteration 52/1000 | Loss: 0.00001664
Iteration 53/1000 | Loss: 0.00001663
Iteration 54/1000 | Loss: 0.00001663
Iteration 55/1000 | Loss: 0.00001663
Iteration 56/1000 | Loss: 0.00001663
Iteration 57/1000 | Loss: 0.00001663
Iteration 58/1000 | Loss: 0.00001663
Iteration 59/1000 | Loss: 0.00001663
Iteration 60/1000 | Loss: 0.00001663
Iteration 61/1000 | Loss: 0.00001663
Iteration 62/1000 | Loss: 0.00001663
Iteration 63/1000 | Loss: 0.00001661
Iteration 64/1000 | Loss: 0.00001661
Iteration 65/1000 | Loss: 0.00001660
Iteration 66/1000 | Loss: 0.00001659
Iteration 67/1000 | Loss: 0.00001659
Iteration 68/1000 | Loss: 0.00001659
Iteration 69/1000 | Loss: 0.00001658
Iteration 70/1000 | Loss: 0.00001658
Iteration 71/1000 | Loss: 0.00001658
Iteration 72/1000 | Loss: 0.00001657
Iteration 73/1000 | Loss: 0.00001657
Iteration 74/1000 | Loss: 0.00001656
Iteration 75/1000 | Loss: 0.00001656
Iteration 76/1000 | Loss: 0.00001655
Iteration 77/1000 | Loss: 0.00001655
Iteration 78/1000 | Loss: 0.00001655
Iteration 79/1000 | Loss: 0.00001654
Iteration 80/1000 | Loss: 0.00001654
Iteration 81/1000 | Loss: 0.00001653
Iteration 82/1000 | Loss: 0.00001653
Iteration 83/1000 | Loss: 0.00001653
Iteration 84/1000 | Loss: 0.00001653
Iteration 85/1000 | Loss: 0.00001653
Iteration 86/1000 | Loss: 0.00001652
Iteration 87/1000 | Loss: 0.00001652
Iteration 88/1000 | Loss: 0.00001652
Iteration 89/1000 | Loss: 0.00001652
Iteration 90/1000 | Loss: 0.00001652
Iteration 91/1000 | Loss: 0.00001651
Iteration 92/1000 | Loss: 0.00001651
Iteration 93/1000 | Loss: 0.00001651
Iteration 94/1000 | Loss: 0.00001650
Iteration 95/1000 | Loss: 0.00001650
Iteration 96/1000 | Loss: 0.00001650
Iteration 97/1000 | Loss: 0.00001650
Iteration 98/1000 | Loss: 0.00001649
Iteration 99/1000 | Loss: 0.00001649
Iteration 100/1000 | Loss: 0.00001649
Iteration 101/1000 | Loss: 0.00001649
Iteration 102/1000 | Loss: 0.00001649
Iteration 103/1000 | Loss: 0.00001649
Iteration 104/1000 | Loss: 0.00001649
Iteration 105/1000 | Loss: 0.00001649
Iteration 106/1000 | Loss: 0.00001649
Iteration 107/1000 | Loss: 0.00001649
Iteration 108/1000 | Loss: 0.00001649
Iteration 109/1000 | Loss: 0.00001649
Iteration 110/1000 | Loss: 0.00001649
Iteration 111/1000 | Loss: 0.00001649
Iteration 112/1000 | Loss: 0.00001649
Iteration 113/1000 | Loss: 0.00001649
Iteration 114/1000 | Loss: 0.00001649
Iteration 115/1000 | Loss: 0.00001649
Iteration 116/1000 | Loss: 0.00001649
Iteration 117/1000 | Loss: 0.00001649
Iteration 118/1000 | Loss: 0.00001649
Iteration 119/1000 | Loss: 0.00001649
Iteration 120/1000 | Loss: 0.00001649
Iteration 121/1000 | Loss: 0.00001649
Iteration 122/1000 | Loss: 0.00001649
Iteration 123/1000 | Loss: 0.00001649
Iteration 124/1000 | Loss: 0.00001649
Iteration 125/1000 | Loss: 0.00001649
Iteration 126/1000 | Loss: 0.00001649
Iteration 127/1000 | Loss: 0.00001649
Iteration 128/1000 | Loss: 0.00001649
Iteration 129/1000 | Loss: 0.00001649
Iteration 130/1000 | Loss: 0.00001649
Iteration 131/1000 | Loss: 0.00001649
Iteration 132/1000 | Loss: 0.00001649
Iteration 133/1000 | Loss: 0.00001649
Iteration 134/1000 | Loss: 0.00001649
Iteration 135/1000 | Loss: 0.00001649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.6490470443386585e-05, 1.6490470443386585e-05, 1.6490470443386585e-05, 1.6490470443386585e-05, 1.6490470443386585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6490470443386585e-05

Optimization complete. Final v2v error: 3.4030838012695312 mm

Highest mean error: 3.6882216930389404 mm for frame 164

Lowest mean error: 3.048564910888672 mm for frame 1

Saving results

Total time: 36.8120539188385
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860397
Iteration 2/25 | Loss: 0.00166204
Iteration 3/25 | Loss: 0.00145195
Iteration 4/25 | Loss: 0.00135068
Iteration 5/25 | Loss: 0.00134382
Iteration 6/25 | Loss: 0.00133823
Iteration 7/25 | Loss: 0.00133624
Iteration 8/25 | Loss: 0.00133546
Iteration 9/25 | Loss: 0.00133496
Iteration 10/25 | Loss: 0.00133482
Iteration 11/25 | Loss: 0.00133471
Iteration 12/25 | Loss: 0.00133466
Iteration 13/25 | Loss: 0.00133466
Iteration 14/25 | Loss: 0.00133466
Iteration 15/25 | Loss: 0.00133465
Iteration 16/25 | Loss: 0.00133465
Iteration 17/25 | Loss: 0.00133465
Iteration 18/25 | Loss: 0.00133465
Iteration 19/25 | Loss: 0.00133465
Iteration 20/25 | Loss: 0.00133465
Iteration 21/25 | Loss: 0.00133465
Iteration 22/25 | Loss: 0.00133464
Iteration 23/25 | Loss: 0.00133464
Iteration 24/25 | Loss: 0.00133464
Iteration 25/25 | Loss: 0.00133464

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.05070782
Iteration 2/25 | Loss: 0.00132791
Iteration 3/25 | Loss: 0.00132790
Iteration 4/25 | Loss: 0.00132790
Iteration 5/25 | Loss: 0.00132790
Iteration 6/25 | Loss: 0.00132790
Iteration 7/25 | Loss: 0.00132790
Iteration 8/25 | Loss: 0.00132790
Iteration 9/25 | Loss: 0.00132790
Iteration 10/25 | Loss: 0.00132790
Iteration 11/25 | Loss: 0.00132790
Iteration 12/25 | Loss: 0.00132790
Iteration 13/25 | Loss: 0.00132790
Iteration 14/25 | Loss: 0.00132790
Iteration 15/25 | Loss: 0.00132790
Iteration 16/25 | Loss: 0.00132790
Iteration 17/25 | Loss: 0.00132790
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013278978876769543, 0.0013278978876769543, 0.0013278978876769543, 0.0013278978876769543, 0.0013278978876769543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013278978876769543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132790
Iteration 2/1000 | Loss: 0.00003773
Iteration 3/1000 | Loss: 0.00002702
Iteration 4/1000 | Loss: 0.00002460
Iteration 5/1000 | Loss: 0.00002378
Iteration 6/1000 | Loss: 0.00002311
Iteration 7/1000 | Loss: 0.00002290
Iteration 8/1000 | Loss: 0.00002255
Iteration 9/1000 | Loss: 0.00002227
Iteration 10/1000 | Loss: 0.00002204
Iteration 11/1000 | Loss: 0.00002202
Iteration 12/1000 | Loss: 0.00002197
Iteration 13/1000 | Loss: 0.00002190
Iteration 14/1000 | Loss: 0.00002182
Iteration 15/1000 | Loss: 0.00002181
Iteration 16/1000 | Loss: 0.00002181
Iteration 17/1000 | Loss: 0.00002178
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002174
Iteration 20/1000 | Loss: 0.00002173
Iteration 21/1000 | Loss: 0.00002173
Iteration 22/1000 | Loss: 0.00002173
Iteration 23/1000 | Loss: 0.00002173
Iteration 24/1000 | Loss: 0.00002173
Iteration 25/1000 | Loss: 0.00002173
Iteration 26/1000 | Loss: 0.00002173
Iteration 27/1000 | Loss: 0.00002173
Iteration 28/1000 | Loss: 0.00002173
Iteration 29/1000 | Loss: 0.00002172
Iteration 30/1000 | Loss: 0.00002172
Iteration 31/1000 | Loss: 0.00002172
Iteration 32/1000 | Loss: 0.00002172
Iteration 33/1000 | Loss: 0.00002171
Iteration 34/1000 | Loss: 0.00002171
Iteration 35/1000 | Loss: 0.00002171
Iteration 36/1000 | Loss: 0.00002171
Iteration 37/1000 | Loss: 0.00002170
Iteration 38/1000 | Loss: 0.00002170
Iteration 39/1000 | Loss: 0.00002170
Iteration 40/1000 | Loss: 0.00002169
Iteration 41/1000 | Loss: 0.00002169
Iteration 42/1000 | Loss: 0.00002169
Iteration 43/1000 | Loss: 0.00002169
Iteration 44/1000 | Loss: 0.00002169
Iteration 45/1000 | Loss: 0.00002169
Iteration 46/1000 | Loss: 0.00002169
Iteration 47/1000 | Loss: 0.00002169
Iteration 48/1000 | Loss: 0.00002169
Iteration 49/1000 | Loss: 0.00002169
Iteration 50/1000 | Loss: 0.00002169
Iteration 51/1000 | Loss: 0.00002169
Iteration 52/1000 | Loss: 0.00002169
Iteration 53/1000 | Loss: 0.00002169
Iteration 54/1000 | Loss: 0.00002169
Iteration 55/1000 | Loss: 0.00002169
Iteration 56/1000 | Loss: 0.00002169
Iteration 57/1000 | Loss: 0.00002169
Iteration 58/1000 | Loss: 0.00002169
Iteration 59/1000 | Loss: 0.00002169
Iteration 60/1000 | Loss: 0.00002169
Iteration 61/1000 | Loss: 0.00002169
Iteration 62/1000 | Loss: 0.00002169
Iteration 63/1000 | Loss: 0.00002169
Iteration 64/1000 | Loss: 0.00002169
Iteration 65/1000 | Loss: 0.00002169
Iteration 66/1000 | Loss: 0.00002169
Iteration 67/1000 | Loss: 0.00002169
Iteration 68/1000 | Loss: 0.00002169
Iteration 69/1000 | Loss: 0.00002169
Iteration 70/1000 | Loss: 0.00002169
Iteration 71/1000 | Loss: 0.00002169
Iteration 72/1000 | Loss: 0.00002169
Iteration 73/1000 | Loss: 0.00002169
Iteration 74/1000 | Loss: 0.00002169
Iteration 75/1000 | Loss: 0.00002169
Iteration 76/1000 | Loss: 0.00002169
Iteration 77/1000 | Loss: 0.00002169
Iteration 78/1000 | Loss: 0.00002169
Iteration 79/1000 | Loss: 0.00002169
Iteration 80/1000 | Loss: 0.00002169
Iteration 81/1000 | Loss: 0.00002169
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [2.1686246327590197e-05, 2.1686246327590197e-05, 2.1686246327590197e-05, 2.1686246327590197e-05, 2.1686246327590197e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1686246327590197e-05

Optimization complete. Final v2v error: 3.994055986404419 mm

Highest mean error: 4.432237148284912 mm for frame 74

Lowest mean error: 3.4872560501098633 mm for frame 11

Saving results

Total time: 44.77565670013428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00425676
Iteration 2/25 | Loss: 0.00153656
Iteration 3/25 | Loss: 0.00129815
Iteration 4/25 | Loss: 0.00127760
Iteration 5/25 | Loss: 0.00127537
Iteration 6/25 | Loss: 0.00127494
Iteration 7/25 | Loss: 0.00127494
Iteration 8/25 | Loss: 0.00127494
Iteration 9/25 | Loss: 0.00127494
Iteration 10/25 | Loss: 0.00127494
Iteration 11/25 | Loss: 0.00127494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012749357847496867, 0.0012749357847496867, 0.0012749357847496867, 0.0012749357847496867, 0.0012749357847496867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012749357847496867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23967624
Iteration 2/25 | Loss: 0.00117440
Iteration 3/25 | Loss: 0.00117440
Iteration 4/25 | Loss: 0.00117440
Iteration 5/25 | Loss: 0.00117440
Iteration 6/25 | Loss: 0.00117440
Iteration 7/25 | Loss: 0.00117440
Iteration 8/25 | Loss: 0.00117440
Iteration 9/25 | Loss: 0.00117440
Iteration 10/25 | Loss: 0.00117440
Iteration 11/25 | Loss: 0.00117440
Iteration 12/25 | Loss: 0.00117440
Iteration 13/25 | Loss: 0.00117440
Iteration 14/25 | Loss: 0.00117440
Iteration 15/25 | Loss: 0.00117440
Iteration 16/25 | Loss: 0.00117440
Iteration 17/25 | Loss: 0.00117440
Iteration 18/25 | Loss: 0.00117440
Iteration 19/25 | Loss: 0.00117440
Iteration 20/25 | Loss: 0.00117440
Iteration 21/25 | Loss: 0.00117440
Iteration 22/25 | Loss: 0.00117440
Iteration 23/25 | Loss: 0.00117440
Iteration 24/25 | Loss: 0.00117440
Iteration 25/25 | Loss: 0.00117440

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117440
Iteration 2/1000 | Loss: 0.00003320
Iteration 3/1000 | Loss: 0.00002093
Iteration 4/1000 | Loss: 0.00001815
Iteration 5/1000 | Loss: 0.00001714
Iteration 6/1000 | Loss: 0.00001649
Iteration 7/1000 | Loss: 0.00001584
Iteration 8/1000 | Loss: 0.00001544
Iteration 9/1000 | Loss: 0.00001527
Iteration 10/1000 | Loss: 0.00001510
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001509
Iteration 13/1000 | Loss: 0.00001503
Iteration 14/1000 | Loss: 0.00001503
Iteration 15/1000 | Loss: 0.00001502
Iteration 16/1000 | Loss: 0.00001500
Iteration 17/1000 | Loss: 0.00001499
Iteration 18/1000 | Loss: 0.00001498
Iteration 19/1000 | Loss: 0.00001498
Iteration 20/1000 | Loss: 0.00001497
Iteration 21/1000 | Loss: 0.00001496
Iteration 22/1000 | Loss: 0.00001494
Iteration 23/1000 | Loss: 0.00001493
Iteration 24/1000 | Loss: 0.00001493
Iteration 25/1000 | Loss: 0.00001492
Iteration 26/1000 | Loss: 0.00001491
Iteration 27/1000 | Loss: 0.00001491
Iteration 28/1000 | Loss: 0.00001490
Iteration 29/1000 | Loss: 0.00001490
Iteration 30/1000 | Loss: 0.00001489
Iteration 31/1000 | Loss: 0.00001489
Iteration 32/1000 | Loss: 0.00001488
Iteration 33/1000 | Loss: 0.00001487
Iteration 34/1000 | Loss: 0.00001485
Iteration 35/1000 | Loss: 0.00001485
Iteration 36/1000 | Loss: 0.00001484
Iteration 37/1000 | Loss: 0.00001483
Iteration 38/1000 | Loss: 0.00001482
Iteration 39/1000 | Loss: 0.00001482
Iteration 40/1000 | Loss: 0.00001482
Iteration 41/1000 | Loss: 0.00001481
Iteration 42/1000 | Loss: 0.00001481
Iteration 43/1000 | Loss: 0.00001480
Iteration 44/1000 | Loss: 0.00001480
Iteration 45/1000 | Loss: 0.00001480
Iteration 46/1000 | Loss: 0.00001479
Iteration 47/1000 | Loss: 0.00001478
Iteration 48/1000 | Loss: 0.00001478
Iteration 49/1000 | Loss: 0.00001478
Iteration 50/1000 | Loss: 0.00001478
Iteration 51/1000 | Loss: 0.00001477
Iteration 52/1000 | Loss: 0.00001476
Iteration 53/1000 | Loss: 0.00001476
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001476
Iteration 58/1000 | Loss: 0.00001475
Iteration 59/1000 | Loss: 0.00001475
Iteration 60/1000 | Loss: 0.00001475
Iteration 61/1000 | Loss: 0.00001474
Iteration 62/1000 | Loss: 0.00001474
Iteration 63/1000 | Loss: 0.00001474
Iteration 64/1000 | Loss: 0.00001473
Iteration 65/1000 | Loss: 0.00001473
Iteration 66/1000 | Loss: 0.00001473
Iteration 67/1000 | Loss: 0.00001473
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001472
Iteration 70/1000 | Loss: 0.00001472
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001471
Iteration 73/1000 | Loss: 0.00001471
Iteration 74/1000 | Loss: 0.00001471
Iteration 75/1000 | Loss: 0.00001471
Iteration 76/1000 | Loss: 0.00001471
Iteration 77/1000 | Loss: 0.00001470
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001469
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001466
Iteration 91/1000 | Loss: 0.00001466
Iteration 92/1000 | Loss: 0.00001466
Iteration 93/1000 | Loss: 0.00001466
Iteration 94/1000 | Loss: 0.00001465
Iteration 95/1000 | Loss: 0.00001465
Iteration 96/1000 | Loss: 0.00001465
Iteration 97/1000 | Loss: 0.00001465
Iteration 98/1000 | Loss: 0.00001464
Iteration 99/1000 | Loss: 0.00001464
Iteration 100/1000 | Loss: 0.00001464
Iteration 101/1000 | Loss: 0.00001464
Iteration 102/1000 | Loss: 0.00001463
Iteration 103/1000 | Loss: 0.00001463
Iteration 104/1000 | Loss: 0.00001462
Iteration 105/1000 | Loss: 0.00001462
Iteration 106/1000 | Loss: 0.00001461
Iteration 107/1000 | Loss: 0.00001461
Iteration 108/1000 | Loss: 0.00001461
Iteration 109/1000 | Loss: 0.00001461
Iteration 110/1000 | Loss: 0.00001460
Iteration 111/1000 | Loss: 0.00001460
Iteration 112/1000 | Loss: 0.00001460
Iteration 113/1000 | Loss: 0.00001459
Iteration 114/1000 | Loss: 0.00001459
Iteration 115/1000 | Loss: 0.00001459
Iteration 116/1000 | Loss: 0.00001459
Iteration 117/1000 | Loss: 0.00001459
Iteration 118/1000 | Loss: 0.00001459
Iteration 119/1000 | Loss: 0.00001459
Iteration 120/1000 | Loss: 0.00001459
Iteration 121/1000 | Loss: 0.00001458
Iteration 122/1000 | Loss: 0.00001458
Iteration 123/1000 | Loss: 0.00001458
Iteration 124/1000 | Loss: 0.00001458
Iteration 125/1000 | Loss: 0.00001458
Iteration 126/1000 | Loss: 0.00001458
Iteration 127/1000 | Loss: 0.00001458
Iteration 128/1000 | Loss: 0.00001458
Iteration 129/1000 | Loss: 0.00001458
Iteration 130/1000 | Loss: 0.00001457
Iteration 131/1000 | Loss: 0.00001457
Iteration 132/1000 | Loss: 0.00001457
Iteration 133/1000 | Loss: 0.00001457
Iteration 134/1000 | Loss: 0.00001457
Iteration 135/1000 | Loss: 0.00001457
Iteration 136/1000 | Loss: 0.00001457
Iteration 137/1000 | Loss: 0.00001457
Iteration 138/1000 | Loss: 0.00001457
Iteration 139/1000 | Loss: 0.00001457
Iteration 140/1000 | Loss: 0.00001457
Iteration 141/1000 | Loss: 0.00001457
Iteration 142/1000 | Loss: 0.00001457
Iteration 143/1000 | Loss: 0.00001457
Iteration 144/1000 | Loss: 0.00001457
Iteration 145/1000 | Loss: 0.00001457
Iteration 146/1000 | Loss: 0.00001457
Iteration 147/1000 | Loss: 0.00001457
Iteration 148/1000 | Loss: 0.00001457
Iteration 149/1000 | Loss: 0.00001457
Iteration 150/1000 | Loss: 0.00001457
Iteration 151/1000 | Loss: 0.00001457
Iteration 152/1000 | Loss: 0.00001457
Iteration 153/1000 | Loss: 0.00001457
Iteration 154/1000 | Loss: 0.00001457
Iteration 155/1000 | Loss: 0.00001457
Iteration 156/1000 | Loss: 0.00001457
Iteration 157/1000 | Loss: 0.00001457
Iteration 158/1000 | Loss: 0.00001457
Iteration 159/1000 | Loss: 0.00001457
Iteration 160/1000 | Loss: 0.00001457
Iteration 161/1000 | Loss: 0.00001457
Iteration 162/1000 | Loss: 0.00001457
Iteration 163/1000 | Loss: 0.00001457
Iteration 164/1000 | Loss: 0.00001457
Iteration 165/1000 | Loss: 0.00001457
Iteration 166/1000 | Loss: 0.00001457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.4565915989805944e-05, 1.4565915989805944e-05, 1.4565915989805944e-05, 1.4565915989805944e-05, 1.4565915989805944e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4565915989805944e-05

Optimization complete. Final v2v error: 3.272068977355957 mm

Highest mean error: 3.536525249481201 mm for frame 115

Lowest mean error: 2.9596707820892334 mm for frame 3

Saving results

Total time: 33.679829120635986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857575
Iteration 2/25 | Loss: 0.00207259
Iteration 3/25 | Loss: 0.00164389
Iteration 4/25 | Loss: 0.00174053
Iteration 5/25 | Loss: 0.00153533
Iteration 6/25 | Loss: 0.00148708
Iteration 7/25 | Loss: 0.00145715
Iteration 8/25 | Loss: 0.00147232
Iteration 9/25 | Loss: 0.00149355
Iteration 10/25 | Loss: 0.00146325
Iteration 11/25 | Loss: 0.00145038
Iteration 12/25 | Loss: 0.00145223
Iteration 13/25 | Loss: 0.00143229
Iteration 14/25 | Loss: 0.00143045
Iteration 15/25 | Loss: 0.00143054
Iteration 16/25 | Loss: 0.00142994
Iteration 17/25 | Loss: 0.00143005
Iteration 18/25 | Loss: 0.00143057
Iteration 19/25 | Loss: 0.00143010
Iteration 20/25 | Loss: 0.00143336
Iteration 21/25 | Loss: 0.00143100
Iteration 22/25 | Loss: 0.00143170
Iteration 23/25 | Loss: 0.00142999
Iteration 24/25 | Loss: 0.00142841
Iteration 25/25 | Loss: 0.00142835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30481601
Iteration 2/25 | Loss: 0.00155867
Iteration 3/25 | Loss: 0.00155865
Iteration 4/25 | Loss: 0.00155865
Iteration 5/25 | Loss: 0.00155865
Iteration 6/25 | Loss: 0.00155865
Iteration 7/25 | Loss: 0.00155865
Iteration 8/25 | Loss: 0.00155865
Iteration 9/25 | Loss: 0.00155865
Iteration 10/25 | Loss: 0.00155865
Iteration 11/25 | Loss: 0.00155865
Iteration 12/25 | Loss: 0.00155865
Iteration 13/25 | Loss: 0.00155865
Iteration 14/25 | Loss: 0.00155865
Iteration 15/25 | Loss: 0.00155865
Iteration 16/25 | Loss: 0.00155865
Iteration 17/25 | Loss: 0.00155865
Iteration 18/25 | Loss: 0.00155865
Iteration 19/25 | Loss: 0.00155865
Iteration 20/25 | Loss: 0.00155865
Iteration 21/25 | Loss: 0.00155865
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015586473746225238, 0.0015586473746225238, 0.0015586473746225238, 0.0015586473746225238, 0.0015586473746225238]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015586473746225238

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155865
Iteration 2/1000 | Loss: 0.00046582
Iteration 3/1000 | Loss: 0.00072181
Iteration 4/1000 | Loss: 0.00105793
Iteration 5/1000 | Loss: 0.00072114
Iteration 6/1000 | Loss: 0.00049315
Iteration 7/1000 | Loss: 0.00015308
Iteration 8/1000 | Loss: 0.00006433
Iteration 9/1000 | Loss: 0.00008226
Iteration 10/1000 | Loss: 0.00005164
Iteration 11/1000 | Loss: 0.00010224
Iteration 12/1000 | Loss: 0.00015606
Iteration 13/1000 | Loss: 0.00005674
Iteration 14/1000 | Loss: 0.00011901
Iteration 15/1000 | Loss: 0.00021786
Iteration 16/1000 | Loss: 0.00004650
Iteration 17/1000 | Loss: 0.00008093
Iteration 18/1000 | Loss: 0.00013302
Iteration 19/1000 | Loss: 0.00017888
Iteration 20/1000 | Loss: 0.00016225
Iteration 21/1000 | Loss: 0.00018962
Iteration 22/1000 | Loss: 0.00005961
Iteration 23/1000 | Loss: 0.00003333
Iteration 24/1000 | Loss: 0.00003116
Iteration 25/1000 | Loss: 0.00018053
Iteration 26/1000 | Loss: 0.00007902
Iteration 27/1000 | Loss: 0.00003027
Iteration 28/1000 | Loss: 0.00018727
Iteration 29/1000 | Loss: 0.00010410
Iteration 30/1000 | Loss: 0.00004672
Iteration 31/1000 | Loss: 0.00009477
Iteration 32/1000 | Loss: 0.00009997
Iteration 33/1000 | Loss: 0.00010197
Iteration 34/1000 | Loss: 0.00013865
Iteration 35/1000 | Loss: 0.00011099
Iteration 36/1000 | Loss: 0.00010157
Iteration 37/1000 | Loss: 0.00010939
Iteration 38/1000 | Loss: 0.00006807
Iteration 39/1000 | Loss: 0.00018286
Iteration 40/1000 | Loss: 0.00009132
Iteration 41/1000 | Loss: 0.00006290
Iteration 42/1000 | Loss: 0.00005114
Iteration 43/1000 | Loss: 0.00005050
Iteration 44/1000 | Loss: 0.00006521
Iteration 45/1000 | Loss: 0.00020178
Iteration 46/1000 | Loss: 0.00044014
Iteration 47/1000 | Loss: 0.00039153
Iteration 48/1000 | Loss: 0.00015437
Iteration 49/1000 | Loss: 0.00052089
Iteration 50/1000 | Loss: 0.00005314
Iteration 51/1000 | Loss: 0.00021900
Iteration 52/1000 | Loss: 0.00029847
Iteration 53/1000 | Loss: 0.00004117
Iteration 54/1000 | Loss: 0.00009899
Iteration 55/1000 | Loss: 0.00003150
Iteration 56/1000 | Loss: 0.00003092
Iteration 57/1000 | Loss: 0.00003043
Iteration 58/1000 | Loss: 0.00023584
Iteration 59/1000 | Loss: 0.00004531
Iteration 60/1000 | Loss: 0.00003028
Iteration 61/1000 | Loss: 0.00022988
Iteration 62/1000 | Loss: 0.00005897
Iteration 63/1000 | Loss: 0.00003042
Iteration 64/1000 | Loss: 0.00017259
Iteration 65/1000 | Loss: 0.00008277
Iteration 66/1000 | Loss: 0.00006033
Iteration 67/1000 | Loss: 0.00003204
Iteration 68/1000 | Loss: 0.00002929
Iteration 69/1000 | Loss: 0.00002797
Iteration 70/1000 | Loss: 0.00002710
Iteration 71/1000 | Loss: 0.00002659
Iteration 72/1000 | Loss: 0.00002625
Iteration 73/1000 | Loss: 0.00002619
Iteration 74/1000 | Loss: 0.00002614
Iteration 75/1000 | Loss: 0.00002613
Iteration 76/1000 | Loss: 0.00002613
Iteration 77/1000 | Loss: 0.00002612
Iteration 78/1000 | Loss: 0.00002612
Iteration 79/1000 | Loss: 0.00002611
Iteration 80/1000 | Loss: 0.00002610
Iteration 81/1000 | Loss: 0.00002608
Iteration 82/1000 | Loss: 0.00002606
Iteration 83/1000 | Loss: 0.00002605
Iteration 84/1000 | Loss: 0.00002605
Iteration 85/1000 | Loss: 0.00002604
Iteration 86/1000 | Loss: 0.00002604
Iteration 87/1000 | Loss: 0.00002604
Iteration 88/1000 | Loss: 0.00002603
Iteration 89/1000 | Loss: 0.00002603
Iteration 90/1000 | Loss: 0.00002602
Iteration 91/1000 | Loss: 0.00002602
Iteration 92/1000 | Loss: 0.00002597
Iteration 93/1000 | Loss: 0.00002596
Iteration 94/1000 | Loss: 0.00002596
Iteration 95/1000 | Loss: 0.00002596
Iteration 96/1000 | Loss: 0.00002595
Iteration 97/1000 | Loss: 0.00002595
Iteration 98/1000 | Loss: 0.00002595
Iteration 99/1000 | Loss: 0.00002591
Iteration 100/1000 | Loss: 0.00002590
Iteration 101/1000 | Loss: 0.00002590
Iteration 102/1000 | Loss: 0.00002589
Iteration 103/1000 | Loss: 0.00002586
Iteration 104/1000 | Loss: 0.00002585
Iteration 105/1000 | Loss: 0.00002585
Iteration 106/1000 | Loss: 0.00002585
Iteration 107/1000 | Loss: 0.00002584
Iteration 108/1000 | Loss: 0.00002584
Iteration 109/1000 | Loss: 0.00002584
Iteration 110/1000 | Loss: 0.00002583
Iteration 111/1000 | Loss: 0.00002583
Iteration 112/1000 | Loss: 0.00002581
Iteration 113/1000 | Loss: 0.00002580
Iteration 114/1000 | Loss: 0.00002580
Iteration 115/1000 | Loss: 0.00002580
Iteration 116/1000 | Loss: 0.00002579
Iteration 117/1000 | Loss: 0.00002579
Iteration 118/1000 | Loss: 0.00002578
Iteration 119/1000 | Loss: 0.00002578
Iteration 120/1000 | Loss: 0.00002578
Iteration 121/1000 | Loss: 0.00002577
Iteration 122/1000 | Loss: 0.00002577
Iteration 123/1000 | Loss: 0.00002577
Iteration 124/1000 | Loss: 0.00002577
Iteration 125/1000 | Loss: 0.00002577
Iteration 126/1000 | Loss: 0.00002577
Iteration 127/1000 | Loss: 0.00002577
Iteration 128/1000 | Loss: 0.00002577
Iteration 129/1000 | Loss: 0.00002577
Iteration 130/1000 | Loss: 0.00002577
Iteration 131/1000 | Loss: 0.00002577
Iteration 132/1000 | Loss: 0.00002576
Iteration 133/1000 | Loss: 0.00002576
Iteration 134/1000 | Loss: 0.00002576
Iteration 135/1000 | Loss: 0.00002576
Iteration 136/1000 | Loss: 0.00002576
Iteration 137/1000 | Loss: 0.00002576
Iteration 138/1000 | Loss: 0.00002576
Iteration 139/1000 | Loss: 0.00002576
Iteration 140/1000 | Loss: 0.00002575
Iteration 141/1000 | Loss: 0.00002575
Iteration 142/1000 | Loss: 0.00002575
Iteration 143/1000 | Loss: 0.00002575
Iteration 144/1000 | Loss: 0.00002575
Iteration 145/1000 | Loss: 0.00002575
Iteration 146/1000 | Loss: 0.00002575
Iteration 147/1000 | Loss: 0.00002575
Iteration 148/1000 | Loss: 0.00002575
Iteration 149/1000 | Loss: 0.00002574
Iteration 150/1000 | Loss: 0.00002574
Iteration 151/1000 | Loss: 0.00002574
Iteration 152/1000 | Loss: 0.00002574
Iteration 153/1000 | Loss: 0.00002573
Iteration 154/1000 | Loss: 0.00002573
Iteration 155/1000 | Loss: 0.00002573
Iteration 156/1000 | Loss: 0.00002572
Iteration 157/1000 | Loss: 0.00002572
Iteration 158/1000 | Loss: 0.00002572
Iteration 159/1000 | Loss: 0.00002572
Iteration 160/1000 | Loss: 0.00002570
Iteration 161/1000 | Loss: 0.00002570
Iteration 162/1000 | Loss: 0.00002569
Iteration 163/1000 | Loss: 0.00002569
Iteration 164/1000 | Loss: 0.00002569
Iteration 165/1000 | Loss: 0.00002568
Iteration 166/1000 | Loss: 0.00002568
Iteration 167/1000 | Loss: 0.00002568
Iteration 168/1000 | Loss: 0.00002567
Iteration 169/1000 | Loss: 0.00002567
Iteration 170/1000 | Loss: 0.00002567
Iteration 171/1000 | Loss: 0.00002567
Iteration 172/1000 | Loss: 0.00002567
Iteration 173/1000 | Loss: 0.00002567
Iteration 174/1000 | Loss: 0.00002567
Iteration 175/1000 | Loss: 0.00002566
Iteration 176/1000 | Loss: 0.00002566
Iteration 177/1000 | Loss: 0.00002566
Iteration 178/1000 | Loss: 0.00002566
Iteration 179/1000 | Loss: 0.00002566
Iteration 180/1000 | Loss: 0.00002566
Iteration 181/1000 | Loss: 0.00002566
Iteration 182/1000 | Loss: 0.00002566
Iteration 183/1000 | Loss: 0.00002566
Iteration 184/1000 | Loss: 0.00002566
Iteration 185/1000 | Loss: 0.00002566
Iteration 186/1000 | Loss: 0.00002565
Iteration 187/1000 | Loss: 0.00002565
Iteration 188/1000 | Loss: 0.00002564
Iteration 189/1000 | Loss: 0.00002563
Iteration 190/1000 | Loss: 0.00002563
Iteration 191/1000 | Loss: 0.00002562
Iteration 192/1000 | Loss: 0.00002562
Iteration 193/1000 | Loss: 0.00002562
Iteration 194/1000 | Loss: 0.00002561
Iteration 195/1000 | Loss: 0.00002561
Iteration 196/1000 | Loss: 0.00002561
Iteration 197/1000 | Loss: 0.00002561
Iteration 198/1000 | Loss: 0.00002561
Iteration 199/1000 | Loss: 0.00002561
Iteration 200/1000 | Loss: 0.00002561
Iteration 201/1000 | Loss: 0.00002561
Iteration 202/1000 | Loss: 0.00002561
Iteration 203/1000 | Loss: 0.00002560
Iteration 204/1000 | Loss: 0.00002560
Iteration 205/1000 | Loss: 0.00002560
Iteration 206/1000 | Loss: 0.00002559
Iteration 207/1000 | Loss: 0.00002559
Iteration 208/1000 | Loss: 0.00002559
Iteration 209/1000 | Loss: 0.00002558
Iteration 210/1000 | Loss: 0.00002558
Iteration 211/1000 | Loss: 0.00002558
Iteration 212/1000 | Loss: 0.00002558
Iteration 213/1000 | Loss: 0.00002558
Iteration 214/1000 | Loss: 0.00002557
Iteration 215/1000 | Loss: 0.00002557
Iteration 216/1000 | Loss: 0.00002557
Iteration 217/1000 | Loss: 0.00002557
Iteration 218/1000 | Loss: 0.00002557
Iteration 219/1000 | Loss: 0.00002557
Iteration 220/1000 | Loss: 0.00002557
Iteration 221/1000 | Loss: 0.00002557
Iteration 222/1000 | Loss: 0.00002556
Iteration 223/1000 | Loss: 0.00002556
Iteration 224/1000 | Loss: 0.00002556
Iteration 225/1000 | Loss: 0.00002556
Iteration 226/1000 | Loss: 0.00002556
Iteration 227/1000 | Loss: 0.00002556
Iteration 228/1000 | Loss: 0.00002556
Iteration 229/1000 | Loss: 0.00002556
Iteration 230/1000 | Loss: 0.00002556
Iteration 231/1000 | Loss: 0.00002556
Iteration 232/1000 | Loss: 0.00002556
Iteration 233/1000 | Loss: 0.00002556
Iteration 234/1000 | Loss: 0.00002556
Iteration 235/1000 | Loss: 0.00002556
Iteration 236/1000 | Loss: 0.00002556
Iteration 237/1000 | Loss: 0.00002556
Iteration 238/1000 | Loss: 0.00002556
Iteration 239/1000 | Loss: 0.00002556
Iteration 240/1000 | Loss: 0.00002556
Iteration 241/1000 | Loss: 0.00002556
Iteration 242/1000 | Loss: 0.00002556
Iteration 243/1000 | Loss: 0.00002556
Iteration 244/1000 | Loss: 0.00002556
Iteration 245/1000 | Loss: 0.00002556
Iteration 246/1000 | Loss: 0.00002556
Iteration 247/1000 | Loss: 0.00002556
Iteration 248/1000 | Loss: 0.00002556
Iteration 249/1000 | Loss: 0.00002556
Iteration 250/1000 | Loss: 0.00002556
Iteration 251/1000 | Loss: 0.00002556
Iteration 252/1000 | Loss: 0.00002556
Iteration 253/1000 | Loss: 0.00002556
Iteration 254/1000 | Loss: 0.00002556
Iteration 255/1000 | Loss: 0.00002556
Iteration 256/1000 | Loss: 0.00002556
Iteration 257/1000 | Loss: 0.00002556
Iteration 258/1000 | Loss: 0.00002556
Iteration 259/1000 | Loss: 0.00002556
Iteration 260/1000 | Loss: 0.00002556
Iteration 261/1000 | Loss: 0.00002556
Iteration 262/1000 | Loss: 0.00002556
Iteration 263/1000 | Loss: 0.00002556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [2.5555729735060595e-05, 2.5555729735060595e-05, 2.5555729735060595e-05, 2.5555729735060595e-05, 2.5555729735060595e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5555729735060595e-05

Optimization complete. Final v2v error: 4.330228328704834 mm

Highest mean error: 4.909096717834473 mm for frame 211

Lowest mean error: 3.814049243927002 mm for frame 65

Saving results

Total time: 184.19844388961792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902601
Iteration 2/25 | Loss: 0.00140614
Iteration 3/25 | Loss: 0.00129848
Iteration 4/25 | Loss: 0.00128967
Iteration 5/25 | Loss: 0.00128767
Iteration 6/25 | Loss: 0.00128742
Iteration 7/25 | Loss: 0.00128742
Iteration 8/25 | Loss: 0.00128742
Iteration 9/25 | Loss: 0.00128742
Iteration 10/25 | Loss: 0.00128742
Iteration 11/25 | Loss: 0.00128742
Iteration 12/25 | Loss: 0.00128742
Iteration 13/25 | Loss: 0.00128742
Iteration 14/25 | Loss: 0.00128742
Iteration 15/25 | Loss: 0.00128742
Iteration 16/25 | Loss: 0.00128742
Iteration 17/25 | Loss: 0.00128742
Iteration 18/25 | Loss: 0.00128742
Iteration 19/25 | Loss: 0.00128742
Iteration 20/25 | Loss: 0.00128742
Iteration 21/25 | Loss: 0.00128742
Iteration 22/25 | Loss: 0.00128742
Iteration 23/25 | Loss: 0.00128742
Iteration 24/25 | Loss: 0.00128742
Iteration 25/25 | Loss: 0.00128742

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.06910610
Iteration 2/25 | Loss: 0.00130836
Iteration 3/25 | Loss: 0.00130835
Iteration 4/25 | Loss: 0.00130835
Iteration 5/25 | Loss: 0.00130835
Iteration 6/25 | Loss: 0.00130835
Iteration 7/25 | Loss: 0.00130835
Iteration 8/25 | Loss: 0.00130835
Iteration 9/25 | Loss: 0.00130835
Iteration 10/25 | Loss: 0.00130835
Iteration 11/25 | Loss: 0.00130835
Iteration 12/25 | Loss: 0.00130835
Iteration 13/25 | Loss: 0.00130835
Iteration 14/25 | Loss: 0.00130835
Iteration 15/25 | Loss: 0.00130835
Iteration 16/25 | Loss: 0.00130835
Iteration 17/25 | Loss: 0.00130835
Iteration 18/25 | Loss: 0.00130835
Iteration 19/25 | Loss: 0.00130835
Iteration 20/25 | Loss: 0.00130835
Iteration 21/25 | Loss: 0.00130835
Iteration 22/25 | Loss: 0.00130835
Iteration 23/25 | Loss: 0.00130835
Iteration 24/25 | Loss: 0.00130835
Iteration 25/25 | Loss: 0.00130835

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130835
Iteration 2/1000 | Loss: 0.00005420
Iteration 3/1000 | Loss: 0.00002566
Iteration 4/1000 | Loss: 0.00002206
Iteration 5/1000 | Loss: 0.00002066
Iteration 6/1000 | Loss: 0.00001987
Iteration 7/1000 | Loss: 0.00001929
Iteration 8/1000 | Loss: 0.00001896
Iteration 9/1000 | Loss: 0.00001882
Iteration 10/1000 | Loss: 0.00001870
Iteration 11/1000 | Loss: 0.00001868
Iteration 12/1000 | Loss: 0.00001865
Iteration 13/1000 | Loss: 0.00001864
Iteration 14/1000 | Loss: 0.00001864
Iteration 15/1000 | Loss: 0.00001863
Iteration 16/1000 | Loss: 0.00001863
Iteration 17/1000 | Loss: 0.00001863
Iteration 18/1000 | Loss: 0.00001862
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001862
Iteration 21/1000 | Loss: 0.00001861
Iteration 22/1000 | Loss: 0.00001861
Iteration 23/1000 | Loss: 0.00001860
Iteration 24/1000 | Loss: 0.00001860
Iteration 25/1000 | Loss: 0.00001859
Iteration 26/1000 | Loss: 0.00001859
Iteration 27/1000 | Loss: 0.00001859
Iteration 28/1000 | Loss: 0.00001859
Iteration 29/1000 | Loss: 0.00001859
Iteration 30/1000 | Loss: 0.00001859
Iteration 31/1000 | Loss: 0.00001859
Iteration 32/1000 | Loss: 0.00001859
Iteration 33/1000 | Loss: 0.00001858
Iteration 34/1000 | Loss: 0.00001858
Iteration 35/1000 | Loss: 0.00001856
Iteration 36/1000 | Loss: 0.00001856
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001855
Iteration 40/1000 | Loss: 0.00001855
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001855
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001853
Iteration 45/1000 | Loss: 0.00001852
Iteration 46/1000 | Loss: 0.00001851
Iteration 47/1000 | Loss: 0.00001851
Iteration 48/1000 | Loss: 0.00001850
Iteration 49/1000 | Loss: 0.00001849
Iteration 50/1000 | Loss: 0.00001849
Iteration 51/1000 | Loss: 0.00001848
Iteration 52/1000 | Loss: 0.00001848
Iteration 53/1000 | Loss: 0.00001847
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001844
Iteration 56/1000 | Loss: 0.00001843
Iteration 57/1000 | Loss: 0.00001843
Iteration 58/1000 | Loss: 0.00001843
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001843
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001843
Iteration 64/1000 | Loss: 0.00001843
Iteration 65/1000 | Loss: 0.00001843
Iteration 66/1000 | Loss: 0.00001842
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001839
Iteration 69/1000 | Loss: 0.00001839
Iteration 70/1000 | Loss: 0.00001839
Iteration 71/1000 | Loss: 0.00001839
Iteration 72/1000 | Loss: 0.00001839
Iteration 73/1000 | Loss: 0.00001839
Iteration 74/1000 | Loss: 0.00001839
Iteration 75/1000 | Loss: 0.00001838
Iteration 76/1000 | Loss: 0.00001838
Iteration 77/1000 | Loss: 0.00001837
Iteration 78/1000 | Loss: 0.00001837
Iteration 79/1000 | Loss: 0.00001836
Iteration 80/1000 | Loss: 0.00001836
Iteration 81/1000 | Loss: 0.00001836
Iteration 82/1000 | Loss: 0.00001836
Iteration 83/1000 | Loss: 0.00001836
Iteration 84/1000 | Loss: 0.00001836
Iteration 85/1000 | Loss: 0.00001836
Iteration 86/1000 | Loss: 0.00001836
Iteration 87/1000 | Loss: 0.00001836
Iteration 88/1000 | Loss: 0.00001836
Iteration 89/1000 | Loss: 0.00001836
Iteration 90/1000 | Loss: 0.00001835
Iteration 91/1000 | Loss: 0.00001835
Iteration 92/1000 | Loss: 0.00001835
Iteration 93/1000 | Loss: 0.00001835
Iteration 94/1000 | Loss: 0.00001835
Iteration 95/1000 | Loss: 0.00001835
Iteration 96/1000 | Loss: 0.00001835
Iteration 97/1000 | Loss: 0.00001835
Iteration 98/1000 | Loss: 0.00001835
Iteration 99/1000 | Loss: 0.00001834
Iteration 100/1000 | Loss: 0.00001834
Iteration 101/1000 | Loss: 0.00001834
Iteration 102/1000 | Loss: 0.00001834
Iteration 103/1000 | Loss: 0.00001833
Iteration 104/1000 | Loss: 0.00001833
Iteration 105/1000 | Loss: 0.00001833
Iteration 106/1000 | Loss: 0.00001833
Iteration 107/1000 | Loss: 0.00001833
Iteration 108/1000 | Loss: 0.00001833
Iteration 109/1000 | Loss: 0.00001833
Iteration 110/1000 | Loss: 0.00001832
Iteration 111/1000 | Loss: 0.00001832
Iteration 112/1000 | Loss: 0.00001832
Iteration 113/1000 | Loss: 0.00001832
Iteration 114/1000 | Loss: 0.00001832
Iteration 115/1000 | Loss: 0.00001832
Iteration 116/1000 | Loss: 0.00001832
Iteration 117/1000 | Loss: 0.00001832
Iteration 118/1000 | Loss: 0.00001831
Iteration 119/1000 | Loss: 0.00001831
Iteration 120/1000 | Loss: 0.00001831
Iteration 121/1000 | Loss: 0.00001831
Iteration 122/1000 | Loss: 0.00001831
Iteration 123/1000 | Loss: 0.00001831
Iteration 124/1000 | Loss: 0.00001831
Iteration 125/1000 | Loss: 0.00001831
Iteration 126/1000 | Loss: 0.00001831
Iteration 127/1000 | Loss: 0.00001830
Iteration 128/1000 | Loss: 0.00001830
Iteration 129/1000 | Loss: 0.00001830
Iteration 130/1000 | Loss: 0.00001830
Iteration 131/1000 | Loss: 0.00001830
Iteration 132/1000 | Loss: 0.00001830
Iteration 133/1000 | Loss: 0.00001829
Iteration 134/1000 | Loss: 0.00001829
Iteration 135/1000 | Loss: 0.00001829
Iteration 136/1000 | Loss: 0.00001829
Iteration 137/1000 | Loss: 0.00001829
Iteration 138/1000 | Loss: 0.00001828
Iteration 139/1000 | Loss: 0.00001828
Iteration 140/1000 | Loss: 0.00001828
Iteration 141/1000 | Loss: 0.00001828
Iteration 142/1000 | Loss: 0.00001828
Iteration 143/1000 | Loss: 0.00001828
Iteration 144/1000 | Loss: 0.00001828
Iteration 145/1000 | Loss: 0.00001828
Iteration 146/1000 | Loss: 0.00001827
Iteration 147/1000 | Loss: 0.00001827
Iteration 148/1000 | Loss: 0.00001827
Iteration 149/1000 | Loss: 0.00001827
Iteration 150/1000 | Loss: 0.00001827
Iteration 151/1000 | Loss: 0.00001827
Iteration 152/1000 | Loss: 0.00001827
Iteration 153/1000 | Loss: 0.00001827
Iteration 154/1000 | Loss: 0.00001827
Iteration 155/1000 | Loss: 0.00001827
Iteration 156/1000 | Loss: 0.00001827
Iteration 157/1000 | Loss: 0.00001827
Iteration 158/1000 | Loss: 0.00001827
Iteration 159/1000 | Loss: 0.00001827
Iteration 160/1000 | Loss: 0.00001827
Iteration 161/1000 | Loss: 0.00001827
Iteration 162/1000 | Loss: 0.00001827
Iteration 163/1000 | Loss: 0.00001827
Iteration 164/1000 | Loss: 0.00001827
Iteration 165/1000 | Loss: 0.00001827
Iteration 166/1000 | Loss: 0.00001827
Iteration 167/1000 | Loss: 0.00001827
Iteration 168/1000 | Loss: 0.00001827
Iteration 169/1000 | Loss: 0.00001827
Iteration 170/1000 | Loss: 0.00001827
Iteration 171/1000 | Loss: 0.00001827
Iteration 172/1000 | Loss: 0.00001827
Iteration 173/1000 | Loss: 0.00001827
Iteration 174/1000 | Loss: 0.00001827
Iteration 175/1000 | Loss: 0.00001827
Iteration 176/1000 | Loss: 0.00001827
Iteration 177/1000 | Loss: 0.00001827
Iteration 178/1000 | Loss: 0.00001827
Iteration 179/1000 | Loss: 0.00001827
Iteration 180/1000 | Loss: 0.00001827
Iteration 181/1000 | Loss: 0.00001827
Iteration 182/1000 | Loss: 0.00001827
Iteration 183/1000 | Loss: 0.00001827
Iteration 184/1000 | Loss: 0.00001827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.8269052816322073e-05, 1.8269052816322073e-05, 1.8269052816322073e-05, 1.8269052816322073e-05, 1.8269052816322073e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8269052816322073e-05

Optimization complete. Final v2v error: 3.59419322013855 mm

Highest mean error: 4.157590389251709 mm for frame 55

Lowest mean error: 3.188629388809204 mm for frame 26

Saving results

Total time: 33.57515597343445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01088803
Iteration 2/25 | Loss: 0.00146295
Iteration 3/25 | Loss: 0.00129829
Iteration 4/25 | Loss: 0.00128644
Iteration 5/25 | Loss: 0.00128376
Iteration 6/25 | Loss: 0.00128354
Iteration 7/25 | Loss: 0.00128354
Iteration 8/25 | Loss: 0.00128354
Iteration 9/25 | Loss: 0.00128354
Iteration 10/25 | Loss: 0.00128354
Iteration 11/25 | Loss: 0.00128354
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001283543067984283, 0.001283543067984283, 0.001283543067984283, 0.001283543067984283, 0.001283543067984283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001283543067984283

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23229825
Iteration 2/25 | Loss: 0.00136883
Iteration 3/25 | Loss: 0.00136883
Iteration 4/25 | Loss: 0.00136883
Iteration 5/25 | Loss: 0.00136883
Iteration 6/25 | Loss: 0.00136883
Iteration 7/25 | Loss: 0.00136883
Iteration 8/25 | Loss: 0.00136883
Iteration 9/25 | Loss: 0.00136883
Iteration 10/25 | Loss: 0.00136883
Iteration 11/25 | Loss: 0.00136883
Iteration 12/25 | Loss: 0.00136883
Iteration 13/25 | Loss: 0.00136883
Iteration 14/25 | Loss: 0.00136883
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013688283506780863, 0.0013688283506780863, 0.0013688283506780863, 0.0013688283506780863, 0.0013688283506780863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013688283506780863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136883
Iteration 2/1000 | Loss: 0.00003261
Iteration 3/1000 | Loss: 0.00002273
Iteration 4/1000 | Loss: 0.00001990
Iteration 5/1000 | Loss: 0.00001894
Iteration 6/1000 | Loss: 0.00001826
Iteration 7/1000 | Loss: 0.00001778
Iteration 8/1000 | Loss: 0.00001743
Iteration 9/1000 | Loss: 0.00001723
Iteration 10/1000 | Loss: 0.00001722
Iteration 11/1000 | Loss: 0.00001721
Iteration 12/1000 | Loss: 0.00001718
Iteration 13/1000 | Loss: 0.00001713
Iteration 14/1000 | Loss: 0.00001707
Iteration 15/1000 | Loss: 0.00001701
Iteration 16/1000 | Loss: 0.00001696
Iteration 17/1000 | Loss: 0.00001685
Iteration 18/1000 | Loss: 0.00001678
Iteration 19/1000 | Loss: 0.00001678
Iteration 20/1000 | Loss: 0.00001673
Iteration 21/1000 | Loss: 0.00001673
Iteration 22/1000 | Loss: 0.00001672
Iteration 23/1000 | Loss: 0.00001672
Iteration 24/1000 | Loss: 0.00001671
Iteration 25/1000 | Loss: 0.00001669
Iteration 26/1000 | Loss: 0.00001666
Iteration 27/1000 | Loss: 0.00001666
Iteration 28/1000 | Loss: 0.00001666
Iteration 29/1000 | Loss: 0.00001666
Iteration 30/1000 | Loss: 0.00001666
Iteration 31/1000 | Loss: 0.00001666
Iteration 32/1000 | Loss: 0.00001665
Iteration 33/1000 | Loss: 0.00001665
Iteration 34/1000 | Loss: 0.00001665
Iteration 35/1000 | Loss: 0.00001665
Iteration 36/1000 | Loss: 0.00001665
Iteration 37/1000 | Loss: 0.00001663
Iteration 38/1000 | Loss: 0.00001659
Iteration 39/1000 | Loss: 0.00001659
Iteration 40/1000 | Loss: 0.00001658
Iteration 41/1000 | Loss: 0.00001658
Iteration 42/1000 | Loss: 0.00001657
Iteration 43/1000 | Loss: 0.00001657
Iteration 44/1000 | Loss: 0.00001657
Iteration 45/1000 | Loss: 0.00001656
Iteration 46/1000 | Loss: 0.00001656
Iteration 47/1000 | Loss: 0.00001656
Iteration 48/1000 | Loss: 0.00001655
Iteration 49/1000 | Loss: 0.00001655
Iteration 50/1000 | Loss: 0.00001654
Iteration 51/1000 | Loss: 0.00001654
Iteration 52/1000 | Loss: 0.00001654
Iteration 53/1000 | Loss: 0.00001653
Iteration 54/1000 | Loss: 0.00001653
Iteration 55/1000 | Loss: 0.00001653
Iteration 56/1000 | Loss: 0.00001652
Iteration 57/1000 | Loss: 0.00001652
Iteration 58/1000 | Loss: 0.00001652
Iteration 59/1000 | Loss: 0.00001652
Iteration 60/1000 | Loss: 0.00001651
Iteration 61/1000 | Loss: 0.00001651
Iteration 62/1000 | Loss: 0.00001651
Iteration 63/1000 | Loss: 0.00001651
Iteration 64/1000 | Loss: 0.00001651
Iteration 65/1000 | Loss: 0.00001650
Iteration 66/1000 | Loss: 0.00001650
Iteration 67/1000 | Loss: 0.00001650
Iteration 68/1000 | Loss: 0.00001650
Iteration 69/1000 | Loss: 0.00001650
Iteration 70/1000 | Loss: 0.00001650
Iteration 71/1000 | Loss: 0.00001650
Iteration 72/1000 | Loss: 0.00001649
Iteration 73/1000 | Loss: 0.00001649
Iteration 74/1000 | Loss: 0.00001649
Iteration 75/1000 | Loss: 0.00001648
Iteration 76/1000 | Loss: 0.00001648
Iteration 77/1000 | Loss: 0.00001648
Iteration 78/1000 | Loss: 0.00001648
Iteration 79/1000 | Loss: 0.00001648
Iteration 80/1000 | Loss: 0.00001647
Iteration 81/1000 | Loss: 0.00001647
Iteration 82/1000 | Loss: 0.00001647
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001647
Iteration 85/1000 | Loss: 0.00001647
Iteration 86/1000 | Loss: 0.00001646
Iteration 87/1000 | Loss: 0.00001646
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001646
Iteration 91/1000 | Loss: 0.00001646
Iteration 92/1000 | Loss: 0.00001646
Iteration 93/1000 | Loss: 0.00001646
Iteration 94/1000 | Loss: 0.00001646
Iteration 95/1000 | Loss: 0.00001646
Iteration 96/1000 | Loss: 0.00001646
Iteration 97/1000 | Loss: 0.00001646
Iteration 98/1000 | Loss: 0.00001646
Iteration 99/1000 | Loss: 0.00001646
Iteration 100/1000 | Loss: 0.00001646
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.6462303392472677e-05, 1.6462303392472677e-05, 1.6462303392472677e-05, 1.6462303392472677e-05, 1.6462303392472677e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6462303392472677e-05

Optimization complete. Final v2v error: 3.4288690090179443 mm

Highest mean error: 3.7827186584472656 mm for frame 181

Lowest mean error: 3.2660634517669678 mm for frame 122

Saving results

Total time: 32.44252872467041
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01169614
Iteration 2/25 | Loss: 0.00217613
Iteration 3/25 | Loss: 0.00154552
Iteration 4/25 | Loss: 0.00147573
Iteration 5/25 | Loss: 0.00146440
Iteration 6/25 | Loss: 0.00147193
Iteration 7/25 | Loss: 0.00146491
Iteration 8/25 | Loss: 0.00147772
Iteration 9/25 | Loss: 0.00147022
Iteration 10/25 | Loss: 0.00147068
Iteration 11/25 | Loss: 0.00147042
Iteration 12/25 | Loss: 0.00146526
Iteration 13/25 | Loss: 0.00145563
Iteration 14/25 | Loss: 0.00146600
Iteration 15/25 | Loss: 0.00146796
Iteration 16/25 | Loss: 0.00146111
Iteration 17/25 | Loss: 0.00145200
Iteration 18/25 | Loss: 0.00144976
Iteration 19/25 | Loss: 0.00145663
Iteration 20/25 | Loss: 0.00145370
Iteration 21/25 | Loss: 0.00145301
Iteration 22/25 | Loss: 0.00144888
Iteration 23/25 | Loss: 0.00144908
Iteration 24/25 | Loss: 0.00145148
Iteration 25/25 | Loss: 0.00144941

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.99188405
Iteration 2/25 | Loss: 0.00162748
Iteration 3/25 | Loss: 0.00162747
Iteration 4/25 | Loss: 0.00162747
Iteration 5/25 | Loss: 0.00162747
Iteration 6/25 | Loss: 0.00162747
Iteration 7/25 | Loss: 0.00162747
Iteration 8/25 | Loss: 0.00162747
Iteration 9/25 | Loss: 0.00162747
Iteration 10/25 | Loss: 0.00162747
Iteration 11/25 | Loss: 0.00162747
Iteration 12/25 | Loss: 0.00162747
Iteration 13/25 | Loss: 0.00162747
Iteration 14/25 | Loss: 0.00162747
Iteration 15/25 | Loss: 0.00162747
Iteration 16/25 | Loss: 0.00162747
Iteration 17/25 | Loss: 0.00162747
Iteration 18/25 | Loss: 0.00162747
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016274707159027457, 0.0016274707159027457, 0.0016274707159027457, 0.0016274707159027457, 0.0016274707159027457]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016274707159027457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00162747
Iteration 2/1000 | Loss: 0.00034006
Iteration 3/1000 | Loss: 0.00058864
Iteration 4/1000 | Loss: 0.00031758
Iteration 5/1000 | Loss: 0.00033882
Iteration 6/1000 | Loss: 0.00051477
Iteration 7/1000 | Loss: 0.00062292
Iteration 8/1000 | Loss: 0.00059641
Iteration 9/1000 | Loss: 0.00066305
Iteration 10/1000 | Loss: 0.00052040
Iteration 11/1000 | Loss: 0.00035928
Iteration 12/1000 | Loss: 0.00053204
Iteration 13/1000 | Loss: 0.00065823
Iteration 14/1000 | Loss: 0.00040156
Iteration 15/1000 | Loss: 0.00041035
Iteration 16/1000 | Loss: 0.00034211
Iteration 17/1000 | Loss: 0.00014767
Iteration 18/1000 | Loss: 0.00048522
Iteration 19/1000 | Loss: 0.00040452
Iteration 20/1000 | Loss: 0.00044371
Iteration 21/1000 | Loss: 0.00039399
Iteration 22/1000 | Loss: 0.00046776
Iteration 23/1000 | Loss: 0.00055372
Iteration 24/1000 | Loss: 0.00058442
Iteration 25/1000 | Loss: 0.00048687
Iteration 26/1000 | Loss: 0.00062463
Iteration 27/1000 | Loss: 0.00058325
Iteration 28/1000 | Loss: 0.00040992
Iteration 29/1000 | Loss: 0.00082633
Iteration 30/1000 | Loss: 0.00106665
Iteration 31/1000 | Loss: 0.00079967
Iteration 32/1000 | Loss: 0.00078157
Iteration 33/1000 | Loss: 0.00058391
Iteration 34/1000 | Loss: 0.00076552
Iteration 35/1000 | Loss: 0.00087683
Iteration 36/1000 | Loss: 0.00081669
Iteration 37/1000 | Loss: 0.00071296
Iteration 38/1000 | Loss: 0.00046532
Iteration 39/1000 | Loss: 0.00076691
Iteration 40/1000 | Loss: 0.00049355
Iteration 41/1000 | Loss: 0.00046329
Iteration 42/1000 | Loss: 0.00031392
Iteration 43/1000 | Loss: 0.00049501
Iteration 44/1000 | Loss: 0.00044483
Iteration 45/1000 | Loss: 0.00067408
Iteration 46/1000 | Loss: 0.00043493
Iteration 47/1000 | Loss: 0.00032893
Iteration 48/1000 | Loss: 0.00033296
Iteration 49/1000 | Loss: 0.00067899
Iteration 50/1000 | Loss: 0.00037876
Iteration 51/1000 | Loss: 0.00045673
Iteration 52/1000 | Loss: 0.00050381
Iteration 53/1000 | Loss: 0.00047720
Iteration 54/1000 | Loss: 0.00050848
Iteration 55/1000 | Loss: 0.00060352
Iteration 56/1000 | Loss: 0.00041065
Iteration 57/1000 | Loss: 0.00038097
Iteration 58/1000 | Loss: 0.00059583
Iteration 59/1000 | Loss: 0.00062648
Iteration 60/1000 | Loss: 0.00049448
Iteration 61/1000 | Loss: 0.00044725
Iteration 62/1000 | Loss: 0.00053848
Iteration 63/1000 | Loss: 0.00032365
Iteration 64/1000 | Loss: 0.00040466
Iteration 65/1000 | Loss: 0.00026934
Iteration 66/1000 | Loss: 0.00012613
Iteration 67/1000 | Loss: 0.00033389
Iteration 68/1000 | Loss: 0.00033338
Iteration 69/1000 | Loss: 0.00025055
Iteration 70/1000 | Loss: 0.00018218
Iteration 71/1000 | Loss: 0.00018184
Iteration 72/1000 | Loss: 0.00034605
Iteration 73/1000 | Loss: 0.00027431
Iteration 74/1000 | Loss: 0.00027255
Iteration 75/1000 | Loss: 0.00024588
Iteration 76/1000 | Loss: 0.00031555
Iteration 77/1000 | Loss: 0.00030659
Iteration 78/1000 | Loss: 0.00019814
Iteration 79/1000 | Loss: 0.00030334
Iteration 80/1000 | Loss: 0.00026708
Iteration 81/1000 | Loss: 0.00032396
Iteration 82/1000 | Loss: 0.00012760
Iteration 83/1000 | Loss: 0.00008507
Iteration 84/1000 | Loss: 0.00018300
Iteration 85/1000 | Loss: 0.00017037
Iteration 86/1000 | Loss: 0.00025719
Iteration 87/1000 | Loss: 0.00027554
Iteration 88/1000 | Loss: 0.00012626
Iteration 89/1000 | Loss: 0.00013634
Iteration 90/1000 | Loss: 0.00015631
Iteration 91/1000 | Loss: 0.00016714
Iteration 92/1000 | Loss: 0.00022691
Iteration 93/1000 | Loss: 0.00017865
Iteration 94/1000 | Loss: 0.00012051
Iteration 95/1000 | Loss: 0.00009477
Iteration 96/1000 | Loss: 0.00013950
Iteration 97/1000 | Loss: 0.00028107
Iteration 98/1000 | Loss: 0.00027200
Iteration 99/1000 | Loss: 0.00017570
Iteration 100/1000 | Loss: 0.00017560
Iteration 101/1000 | Loss: 0.00024766
Iteration 102/1000 | Loss: 0.00024862
Iteration 103/1000 | Loss: 0.00016910
Iteration 104/1000 | Loss: 0.00018215
Iteration 105/1000 | Loss: 0.00026368
Iteration 106/1000 | Loss: 0.00026254
Iteration 107/1000 | Loss: 0.00013388
Iteration 108/1000 | Loss: 0.00033859
Iteration 109/1000 | Loss: 0.00022339
Iteration 110/1000 | Loss: 0.00025203
Iteration 111/1000 | Loss: 0.00028873
Iteration 112/1000 | Loss: 0.00027041
Iteration 113/1000 | Loss: 0.00035403
Iteration 114/1000 | Loss: 0.00028750
Iteration 115/1000 | Loss: 0.00028051
Iteration 116/1000 | Loss: 0.00028177
Iteration 117/1000 | Loss: 0.00027907
Iteration 118/1000 | Loss: 0.00028007
Iteration 119/1000 | Loss: 0.00021342
Iteration 120/1000 | Loss: 0.00024262
Iteration 121/1000 | Loss: 0.00027080
Iteration 122/1000 | Loss: 0.00021253
Iteration 123/1000 | Loss: 0.00019692
Iteration 124/1000 | Loss: 0.00013596
Iteration 125/1000 | Loss: 0.00007536
Iteration 126/1000 | Loss: 0.00019190
Iteration 127/1000 | Loss: 0.00015512
Iteration 128/1000 | Loss: 0.00015424
Iteration 129/1000 | Loss: 0.00017496
Iteration 130/1000 | Loss: 0.00027812
Iteration 131/1000 | Loss: 0.00014917
Iteration 132/1000 | Loss: 0.00022211
Iteration 133/1000 | Loss: 0.00015838
Iteration 134/1000 | Loss: 0.00028591
Iteration 135/1000 | Loss: 0.00019423
Iteration 136/1000 | Loss: 0.00024379
Iteration 137/1000 | Loss: 0.00026659
Iteration 138/1000 | Loss: 0.00023624
Iteration 139/1000 | Loss: 0.00010722
Iteration 140/1000 | Loss: 0.00013241
Iteration 141/1000 | Loss: 0.00006474
Iteration 142/1000 | Loss: 0.00011801
Iteration 143/1000 | Loss: 0.00008718
Iteration 144/1000 | Loss: 0.00018014
Iteration 145/1000 | Loss: 0.00033225
Iteration 146/1000 | Loss: 0.00038250
Iteration 147/1000 | Loss: 0.00035802
Iteration 148/1000 | Loss: 0.00030419
Iteration 149/1000 | Loss: 0.00014264
Iteration 150/1000 | Loss: 0.00025706
Iteration 151/1000 | Loss: 0.00017381
Iteration 152/1000 | Loss: 0.00019159
Iteration 153/1000 | Loss: 0.00025428
Iteration 154/1000 | Loss: 0.00025109
Iteration 155/1000 | Loss: 0.00030313
Iteration 156/1000 | Loss: 0.00026789
Iteration 157/1000 | Loss: 0.00029592
Iteration 158/1000 | Loss: 0.00027631
Iteration 159/1000 | Loss: 0.00027156
Iteration 160/1000 | Loss: 0.00027221
Iteration 161/1000 | Loss: 0.00030150
Iteration 162/1000 | Loss: 0.00032169
Iteration 163/1000 | Loss: 0.00029969
Iteration 164/1000 | Loss: 0.00029904
Iteration 165/1000 | Loss: 0.00034204
Iteration 166/1000 | Loss: 0.00036737
Iteration 167/1000 | Loss: 0.00035232
Iteration 168/1000 | Loss: 0.00031783
Iteration 169/1000 | Loss: 0.00021342
Iteration 170/1000 | Loss: 0.00030797
Iteration 171/1000 | Loss: 0.00038492
Iteration 172/1000 | Loss: 0.00032631
Iteration 173/1000 | Loss: 0.00027697
Iteration 174/1000 | Loss: 0.00032307
Iteration 175/1000 | Loss: 0.00028888
Iteration 176/1000 | Loss: 0.00032971
Iteration 177/1000 | Loss: 0.00025583
Iteration 178/1000 | Loss: 0.00029181
Iteration 179/1000 | Loss: 0.00029171
Iteration 180/1000 | Loss: 0.00022046
Iteration 181/1000 | Loss: 0.00029527
Iteration 182/1000 | Loss: 0.00025648
Iteration 183/1000 | Loss: 0.00021234
Iteration 184/1000 | Loss: 0.00024419
Iteration 185/1000 | Loss: 0.00019993
Iteration 186/1000 | Loss: 0.00011379
Iteration 187/1000 | Loss: 0.00017260
Iteration 188/1000 | Loss: 0.00011927
Iteration 189/1000 | Loss: 0.00015853
Iteration 190/1000 | Loss: 0.00032671
Iteration 191/1000 | Loss: 0.00008313
Iteration 192/1000 | Loss: 0.00013089
Iteration 193/1000 | Loss: 0.00014773
Iteration 194/1000 | Loss: 0.00015646
Iteration 195/1000 | Loss: 0.00012806
Iteration 196/1000 | Loss: 0.00008373
Iteration 197/1000 | Loss: 0.00005793
Iteration 198/1000 | Loss: 0.00052971
Iteration 199/1000 | Loss: 0.00008433
Iteration 200/1000 | Loss: 0.00067272
Iteration 201/1000 | Loss: 0.00067471
Iteration 202/1000 | Loss: 0.00048549
Iteration 203/1000 | Loss: 0.00019255
Iteration 204/1000 | Loss: 0.00020282
Iteration 205/1000 | Loss: 0.00030645
Iteration 206/1000 | Loss: 0.00017751
Iteration 207/1000 | Loss: 0.00019113
Iteration 208/1000 | Loss: 0.00012272
Iteration 209/1000 | Loss: 0.00020046
Iteration 210/1000 | Loss: 0.00025837
Iteration 211/1000 | Loss: 0.00015781
Iteration 212/1000 | Loss: 0.00013513
Iteration 213/1000 | Loss: 0.00008078
Iteration 214/1000 | Loss: 0.00012605
Iteration 215/1000 | Loss: 0.00011342
Iteration 216/1000 | Loss: 0.00021918
Iteration 217/1000 | Loss: 0.00027875
Iteration 218/1000 | Loss: 0.00016328
Iteration 219/1000 | Loss: 0.00018852
Iteration 220/1000 | Loss: 0.00011229
Iteration 221/1000 | Loss: 0.00012464
Iteration 222/1000 | Loss: 0.00011394
Iteration 223/1000 | Loss: 0.00013320
Iteration 224/1000 | Loss: 0.00020457
Iteration 225/1000 | Loss: 0.00026402
Iteration 226/1000 | Loss: 0.00015043
Iteration 227/1000 | Loss: 0.00014146
Iteration 228/1000 | Loss: 0.00009244
Iteration 229/1000 | Loss: 0.00011046
Iteration 230/1000 | Loss: 0.00012949
Iteration 231/1000 | Loss: 0.00016574
Iteration 232/1000 | Loss: 0.00012684
Iteration 233/1000 | Loss: 0.00012898
Iteration 234/1000 | Loss: 0.00016085
Iteration 235/1000 | Loss: 0.00021372
Iteration 236/1000 | Loss: 0.00022958
Iteration 237/1000 | Loss: 0.00018884
Iteration 238/1000 | Loss: 0.00021659
Iteration 239/1000 | Loss: 0.00020118
Iteration 240/1000 | Loss: 0.00022946
Iteration 241/1000 | Loss: 0.00017476
Iteration 242/1000 | Loss: 0.00033523
Iteration 243/1000 | Loss: 0.00026726
Iteration 244/1000 | Loss: 0.00026703
Iteration 245/1000 | Loss: 0.00019660
Iteration 246/1000 | Loss: 0.00021399
Iteration 247/1000 | Loss: 0.00021537
Iteration 248/1000 | Loss: 0.00025789
Iteration 249/1000 | Loss: 0.00026235
Iteration 250/1000 | Loss: 0.00032626
Iteration 251/1000 | Loss: 0.00019665
Iteration 252/1000 | Loss: 0.00020925
Iteration 253/1000 | Loss: 0.00018376
Iteration 254/1000 | Loss: 0.00017960
Iteration 255/1000 | Loss: 0.00022432
Iteration 256/1000 | Loss: 0.00015076
Iteration 257/1000 | Loss: 0.00027523
Iteration 258/1000 | Loss: 0.00028061
Iteration 259/1000 | Loss: 0.00013049
Iteration 260/1000 | Loss: 0.00021023
Iteration 261/1000 | Loss: 0.00026559
Iteration 262/1000 | Loss: 0.00021759
Iteration 263/1000 | Loss: 0.00011065
Iteration 264/1000 | Loss: 0.00018013
Iteration 265/1000 | Loss: 0.00020847
Iteration 266/1000 | Loss: 0.00017952
Iteration 267/1000 | Loss: 0.00018140
Iteration 268/1000 | Loss: 0.00023604
Iteration 269/1000 | Loss: 0.00013784
Iteration 270/1000 | Loss: 0.00020416
Iteration 271/1000 | Loss: 0.00020988
Iteration 272/1000 | Loss: 0.00013492
Iteration 273/1000 | Loss: 0.00023872
Iteration 274/1000 | Loss: 0.00057436
Iteration 275/1000 | Loss: 0.00041655
Iteration 276/1000 | Loss: 0.00024590
Iteration 277/1000 | Loss: 0.00045090
Iteration 278/1000 | Loss: 0.00041275
Iteration 279/1000 | Loss: 0.00040486
Iteration 280/1000 | Loss: 0.00020543
Iteration 281/1000 | Loss: 0.00048956
Iteration 282/1000 | Loss: 0.00016368
Iteration 283/1000 | Loss: 0.00015346
Iteration 284/1000 | Loss: 0.00015599
Iteration 285/1000 | Loss: 0.00021917
Iteration 286/1000 | Loss: 0.00022662
Iteration 287/1000 | Loss: 0.00021066
Iteration 288/1000 | Loss: 0.00031936
Iteration 289/1000 | Loss: 0.00061678
Iteration 290/1000 | Loss: 0.00028635
Iteration 291/1000 | Loss: 0.00043291
Iteration 292/1000 | Loss: 0.00052857
Iteration 293/1000 | Loss: 0.00024132
Iteration 294/1000 | Loss: 0.00017036
Iteration 295/1000 | Loss: 0.00036226
Iteration 296/1000 | Loss: 0.00043843
Iteration 297/1000 | Loss: 0.00038909
Iteration 298/1000 | Loss: 0.00054619
Iteration 299/1000 | Loss: 0.00014679
Iteration 300/1000 | Loss: 0.00012730
Iteration 301/1000 | Loss: 0.00044851
Iteration 302/1000 | Loss: 0.00034655
Iteration 303/1000 | Loss: 0.00006836
Iteration 304/1000 | Loss: 0.00013912
Iteration 305/1000 | Loss: 0.00007002
Iteration 306/1000 | Loss: 0.00015240
Iteration 307/1000 | Loss: 0.00006053
Iteration 308/1000 | Loss: 0.00009274
Iteration 309/1000 | Loss: 0.00005703
Iteration 310/1000 | Loss: 0.00005318
Iteration 311/1000 | Loss: 0.00038248
Iteration 312/1000 | Loss: 0.00051602
Iteration 313/1000 | Loss: 0.00016141
Iteration 314/1000 | Loss: 0.00012314
Iteration 315/1000 | Loss: 0.00009696
Iteration 316/1000 | Loss: 0.00005744
Iteration 317/1000 | Loss: 0.00012582
Iteration 318/1000 | Loss: 0.00006549
Iteration 319/1000 | Loss: 0.00013717
Iteration 320/1000 | Loss: 0.00013664
Iteration 321/1000 | Loss: 0.00026643
Iteration 322/1000 | Loss: 0.00015966
Iteration 323/1000 | Loss: 0.00005764
Iteration 324/1000 | Loss: 0.00020395
Iteration 325/1000 | Loss: 0.00005202
Iteration 326/1000 | Loss: 0.00004756
Iteration 327/1000 | Loss: 0.00004545
Iteration 328/1000 | Loss: 0.00010354
Iteration 329/1000 | Loss: 0.00004911
Iteration 330/1000 | Loss: 0.00004624
Iteration 331/1000 | Loss: 0.00004502
Iteration 332/1000 | Loss: 0.00004458
Iteration 333/1000 | Loss: 0.00004399
Iteration 334/1000 | Loss: 0.00004346
Iteration 335/1000 | Loss: 0.00007750
Iteration 336/1000 | Loss: 0.00004900
Iteration 337/1000 | Loss: 0.00004280
Iteration 338/1000 | Loss: 0.00004241
Iteration 339/1000 | Loss: 0.00004183
Iteration 340/1000 | Loss: 0.00004147
Iteration 341/1000 | Loss: 0.00004109
Iteration 342/1000 | Loss: 0.00004087
Iteration 343/1000 | Loss: 0.00004067
Iteration 344/1000 | Loss: 0.00004066
Iteration 345/1000 | Loss: 0.00061574
Iteration 346/1000 | Loss: 0.00041050
Iteration 347/1000 | Loss: 0.00005711
Iteration 348/1000 | Loss: 0.00004069
Iteration 349/1000 | Loss: 0.00004046
Iteration 350/1000 | Loss: 0.00004042
Iteration 351/1000 | Loss: 0.00004042
Iteration 352/1000 | Loss: 0.00004042
Iteration 353/1000 | Loss: 0.00004041
Iteration 354/1000 | Loss: 0.00004041
Iteration 355/1000 | Loss: 0.00004041
Iteration 356/1000 | Loss: 0.00004041
Iteration 357/1000 | Loss: 0.00004040
Iteration 358/1000 | Loss: 0.00004040
Iteration 359/1000 | Loss: 0.00004040
Iteration 360/1000 | Loss: 0.00004040
Iteration 361/1000 | Loss: 0.00004040
Iteration 362/1000 | Loss: 0.00004040
Iteration 363/1000 | Loss: 0.00004040
Iteration 364/1000 | Loss: 0.00004040
Iteration 365/1000 | Loss: 0.00004040
Iteration 366/1000 | Loss: 0.00004040
Iteration 367/1000 | Loss: 0.00004040
Iteration 368/1000 | Loss: 0.00004039
Iteration 369/1000 | Loss: 0.00004039
Iteration 370/1000 | Loss: 0.00004039
Iteration 371/1000 | Loss: 0.00004039
Iteration 372/1000 | Loss: 0.00004039
Iteration 373/1000 | Loss: 0.00004039
Iteration 374/1000 | Loss: 0.00004038
Iteration 375/1000 | Loss: 0.00063711
Iteration 376/1000 | Loss: 0.00028832
Iteration 377/1000 | Loss: 0.00004122
Iteration 378/1000 | Loss: 0.00004046
Iteration 379/1000 | Loss: 0.00004036
Iteration 380/1000 | Loss: 0.00004036
Iteration 381/1000 | Loss: 0.00062337
Iteration 382/1000 | Loss: 0.00005635
Iteration 383/1000 | Loss: 0.00004194
Iteration 384/1000 | Loss: 0.00004026
Iteration 385/1000 | Loss: 0.00003953
Iteration 386/1000 | Loss: 0.00003900
Iteration 387/1000 | Loss: 0.00003872
Iteration 388/1000 | Loss: 0.00003844
Iteration 389/1000 | Loss: 0.00003825
Iteration 390/1000 | Loss: 0.00003816
Iteration 391/1000 | Loss: 0.00003804
Iteration 392/1000 | Loss: 0.00003794
Iteration 393/1000 | Loss: 0.00003791
Iteration 394/1000 | Loss: 0.00003790
Iteration 395/1000 | Loss: 0.00003790
Iteration 396/1000 | Loss: 0.00003790
Iteration 397/1000 | Loss: 0.00003790
Iteration 398/1000 | Loss: 0.00003790
Iteration 399/1000 | Loss: 0.00003790
Iteration 400/1000 | Loss: 0.00003790
Iteration 401/1000 | Loss: 0.00003790
Iteration 402/1000 | Loss: 0.00003790
Iteration 403/1000 | Loss: 0.00003790
Iteration 404/1000 | Loss: 0.00003788
Iteration 405/1000 | Loss: 0.00003787
Iteration 406/1000 | Loss: 0.00003787
Iteration 407/1000 | Loss: 0.00003787
Iteration 408/1000 | Loss: 0.00003786
Iteration 409/1000 | Loss: 0.00003786
Iteration 410/1000 | Loss: 0.00003786
Iteration 411/1000 | Loss: 0.00003786
Iteration 412/1000 | Loss: 0.00003785
Iteration 413/1000 | Loss: 0.00003785
Iteration 414/1000 | Loss: 0.00003785
Iteration 415/1000 | Loss: 0.00003784
Iteration 416/1000 | Loss: 0.00003784
Iteration 417/1000 | Loss: 0.00003784
Iteration 418/1000 | Loss: 0.00003783
Iteration 419/1000 | Loss: 0.00003783
Iteration 420/1000 | Loss: 0.00003783
Iteration 421/1000 | Loss: 0.00003783
Iteration 422/1000 | Loss: 0.00003783
Iteration 423/1000 | Loss: 0.00003783
Iteration 424/1000 | Loss: 0.00003783
Iteration 425/1000 | Loss: 0.00003782
Iteration 426/1000 | Loss: 0.00003782
Iteration 427/1000 | Loss: 0.00003782
Iteration 428/1000 | Loss: 0.00003782
Iteration 429/1000 | Loss: 0.00003782
Iteration 430/1000 | Loss: 0.00003782
Iteration 431/1000 | Loss: 0.00003782
Iteration 432/1000 | Loss: 0.00003782
Iteration 433/1000 | Loss: 0.00003782
Iteration 434/1000 | Loss: 0.00003782
Iteration 435/1000 | Loss: 0.00003782
Iteration 436/1000 | Loss: 0.00003782
Iteration 437/1000 | Loss: 0.00003782
Iteration 438/1000 | Loss: 0.00003781
Iteration 439/1000 | Loss: 0.00003781
Iteration 440/1000 | Loss: 0.00003781
Iteration 441/1000 | Loss: 0.00003781
Iteration 442/1000 | Loss: 0.00003781
Iteration 443/1000 | Loss: 0.00003781
Iteration 444/1000 | Loss: 0.00003781
Iteration 445/1000 | Loss: 0.00003781
Iteration 446/1000 | Loss: 0.00003781
Iteration 447/1000 | Loss: 0.00003781
Iteration 448/1000 | Loss: 0.00003781
Iteration 449/1000 | Loss: 0.00003781
Iteration 450/1000 | Loss: 0.00003781
Iteration 451/1000 | Loss: 0.00003780
Iteration 452/1000 | Loss: 0.00003780
Iteration 453/1000 | Loss: 0.00003780
Iteration 454/1000 | Loss: 0.00003780
Iteration 455/1000 | Loss: 0.00003780
Iteration 456/1000 | Loss: 0.00003780
Iteration 457/1000 | Loss: 0.00003780
Iteration 458/1000 | Loss: 0.00003780
Iteration 459/1000 | Loss: 0.00003780
Iteration 460/1000 | Loss: 0.00003780
Iteration 461/1000 | Loss: 0.00003780
Iteration 462/1000 | Loss: 0.00003780
Iteration 463/1000 | Loss: 0.00003779
Iteration 464/1000 | Loss: 0.00003779
Iteration 465/1000 | Loss: 0.00003779
Iteration 466/1000 | Loss: 0.00003779
Iteration 467/1000 | Loss: 0.00003779
Iteration 468/1000 | Loss: 0.00003779
Iteration 469/1000 | Loss: 0.00003779
Iteration 470/1000 | Loss: 0.00003779
Iteration 471/1000 | Loss: 0.00003779
Iteration 472/1000 | Loss: 0.00003779
Iteration 473/1000 | Loss: 0.00003779
Iteration 474/1000 | Loss: 0.00003778
Iteration 475/1000 | Loss: 0.00003778
Iteration 476/1000 | Loss: 0.00003778
Iteration 477/1000 | Loss: 0.00003778
Iteration 478/1000 | Loss: 0.00003778
Iteration 479/1000 | Loss: 0.00003778
Iteration 480/1000 | Loss: 0.00003778
Iteration 481/1000 | Loss: 0.00003778
Iteration 482/1000 | Loss: 0.00003778
Iteration 483/1000 | Loss: 0.00003778
Iteration 484/1000 | Loss: 0.00003778
Iteration 485/1000 | Loss: 0.00003778
Iteration 486/1000 | Loss: 0.00003778
Iteration 487/1000 | Loss: 0.00003778
Iteration 488/1000 | Loss: 0.00003778
Iteration 489/1000 | Loss: 0.00003778
Iteration 490/1000 | Loss: 0.00003777
Iteration 491/1000 | Loss: 0.00003777
Iteration 492/1000 | Loss: 0.00003777
Iteration 493/1000 | Loss: 0.00003777
Iteration 494/1000 | Loss: 0.00003777
Iteration 495/1000 | Loss: 0.00003777
Iteration 496/1000 | Loss: 0.00003777
Iteration 497/1000 | Loss: 0.00003777
Iteration 498/1000 | Loss: 0.00003777
Iteration 499/1000 | Loss: 0.00003777
Iteration 500/1000 | Loss: 0.00003777
Iteration 501/1000 | Loss: 0.00003777
Iteration 502/1000 | Loss: 0.00003777
Iteration 503/1000 | Loss: 0.00003777
Iteration 504/1000 | Loss: 0.00003777
Iteration 505/1000 | Loss: 0.00003777
Iteration 506/1000 | Loss: 0.00003777
Iteration 507/1000 | Loss: 0.00003777
Iteration 508/1000 | Loss: 0.00003777
Iteration 509/1000 | Loss: 0.00003777
Iteration 510/1000 | Loss: 0.00003777
Iteration 511/1000 | Loss: 0.00003777
Iteration 512/1000 | Loss: 0.00003777
Iteration 513/1000 | Loss: 0.00003777
Iteration 514/1000 | Loss: 0.00003777
Iteration 515/1000 | Loss: 0.00003777
Iteration 516/1000 | Loss: 0.00003777
Iteration 517/1000 | Loss: 0.00003777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 517. Stopping optimization.
Last 5 losses: [3.7772657378809527e-05, 3.7772657378809527e-05, 3.7772657378809527e-05, 3.7772657378809527e-05, 3.7772657378809527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.7772657378809527e-05

Optimization complete. Final v2v error: 4.847933769226074 mm

Highest mean error: 14.115629196166992 mm for frame 152

Lowest mean error: 3.9219167232513428 mm for frame 35

Saving results

Total time: 591.4968004226685
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022676
Iteration 2/25 | Loss: 0.01022676
Iteration 3/25 | Loss: 0.01022676
Iteration 4/25 | Loss: 0.01022676
Iteration 5/25 | Loss: 0.01022676
Iteration 6/25 | Loss: 0.01022675
Iteration 7/25 | Loss: 0.01022675
Iteration 8/25 | Loss: 0.01022675
Iteration 9/25 | Loss: 0.01022675
Iteration 10/25 | Loss: 0.01022675
Iteration 11/25 | Loss: 0.01022675
Iteration 12/25 | Loss: 0.01022675
Iteration 13/25 | Loss: 0.01022675
Iteration 14/25 | Loss: 0.01022675
Iteration 15/25 | Loss: 0.01022674
Iteration 16/25 | Loss: 0.01022674
Iteration 17/25 | Loss: 0.01022674
Iteration 18/25 | Loss: 0.01022674
Iteration 19/25 | Loss: 0.01022674
Iteration 20/25 | Loss: 0.01022674
Iteration 21/25 | Loss: 0.01022674
Iteration 22/25 | Loss: 0.01022673
Iteration 23/25 | Loss: 0.01022673
Iteration 24/25 | Loss: 0.01022673
Iteration 25/25 | Loss: 0.01022673

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41777301
Iteration 2/25 | Loss: 0.18119447
Iteration 3/25 | Loss: 0.18116283
Iteration 4/25 | Loss: 0.18116283
Iteration 5/25 | Loss: 0.18116283
Iteration 6/25 | Loss: 0.18116283
Iteration 7/25 | Loss: 0.18116283
Iteration 8/25 | Loss: 0.18116283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 8. Stopping optimization.
Last 5 losses: [0.18116283416748047, 0.18116283416748047, 0.18116283416748047, 0.18116283416748047, 0.18116283416748047]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18116283416748047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18116283
Iteration 2/1000 | Loss: 0.00828903
Iteration 3/1000 | Loss: 0.00707030
Iteration 4/1000 | Loss: 0.00343236
Iteration 5/1000 | Loss: 0.00638881
Iteration 6/1000 | Loss: 0.00359372
Iteration 7/1000 | Loss: 0.00220930
Iteration 8/1000 | Loss: 0.00158205
Iteration 9/1000 | Loss: 0.00036464
Iteration 10/1000 | Loss: 0.00073908
Iteration 11/1000 | Loss: 0.00015210
Iteration 12/1000 | Loss: 0.00104994
Iteration 13/1000 | Loss: 0.00054954
Iteration 14/1000 | Loss: 0.00209455
Iteration 15/1000 | Loss: 0.00042310
Iteration 16/1000 | Loss: 0.00020198
Iteration 17/1000 | Loss: 0.00018010
Iteration 18/1000 | Loss: 0.00013718
Iteration 19/1000 | Loss: 0.00014925
Iteration 20/1000 | Loss: 0.00005990
Iteration 21/1000 | Loss: 0.00043588
Iteration 22/1000 | Loss: 0.00191656
Iteration 23/1000 | Loss: 0.00078029
Iteration 24/1000 | Loss: 0.00012716
Iteration 25/1000 | Loss: 0.00011121
Iteration 26/1000 | Loss: 0.00111367
Iteration 27/1000 | Loss: 0.00050229
Iteration 28/1000 | Loss: 0.00027509
Iteration 29/1000 | Loss: 0.00007570
Iteration 30/1000 | Loss: 0.00004969
Iteration 31/1000 | Loss: 0.00012723
Iteration 32/1000 | Loss: 0.00042519
Iteration 33/1000 | Loss: 0.00028898
Iteration 34/1000 | Loss: 0.00006151
Iteration 35/1000 | Loss: 0.00017317
Iteration 36/1000 | Loss: 0.00132458
Iteration 37/1000 | Loss: 0.00053718
Iteration 38/1000 | Loss: 0.00009663
Iteration 39/1000 | Loss: 0.00124302
Iteration 40/1000 | Loss: 0.00055073
Iteration 41/1000 | Loss: 0.00010520
Iteration 42/1000 | Loss: 0.00005116
Iteration 43/1000 | Loss: 0.00029356
Iteration 44/1000 | Loss: 0.00064478
Iteration 45/1000 | Loss: 0.00027757
Iteration 46/1000 | Loss: 0.00060057
Iteration 47/1000 | Loss: 0.00068058
Iteration 48/1000 | Loss: 0.00043580
Iteration 49/1000 | Loss: 0.00038373
Iteration 50/1000 | Loss: 0.00027573
Iteration 51/1000 | Loss: 0.00033856
Iteration 52/1000 | Loss: 0.00010592
Iteration 53/1000 | Loss: 0.00007133
Iteration 54/1000 | Loss: 0.00004997
Iteration 55/1000 | Loss: 0.00015706
Iteration 56/1000 | Loss: 0.00003611
Iteration 57/1000 | Loss: 0.00015984
Iteration 58/1000 | Loss: 0.00031159
Iteration 59/1000 | Loss: 0.00019716
Iteration 60/1000 | Loss: 0.00006116
Iteration 61/1000 | Loss: 0.00005911
Iteration 62/1000 | Loss: 0.00007881
Iteration 63/1000 | Loss: 0.00008669
Iteration 64/1000 | Loss: 0.00003315
Iteration 65/1000 | Loss: 0.00044059
Iteration 66/1000 | Loss: 0.00030696
Iteration 67/1000 | Loss: 0.00013551
Iteration 68/1000 | Loss: 0.00006496
Iteration 69/1000 | Loss: 0.00024590
Iteration 70/1000 | Loss: 0.00015392
Iteration 71/1000 | Loss: 0.00008363
Iteration 72/1000 | Loss: 0.00005947
Iteration 73/1000 | Loss: 0.00006636
Iteration 74/1000 | Loss: 0.00004036
Iteration 75/1000 | Loss: 0.00007154
Iteration 76/1000 | Loss: 0.00004463
Iteration 77/1000 | Loss: 0.00003406
Iteration 78/1000 | Loss: 0.00007689
Iteration 79/1000 | Loss: 0.00003283
Iteration 80/1000 | Loss: 0.00003157
Iteration 81/1000 | Loss: 0.00015313
Iteration 82/1000 | Loss: 0.00008656
Iteration 83/1000 | Loss: 0.00021498
Iteration 84/1000 | Loss: 0.00051969
Iteration 85/1000 | Loss: 0.00005170
Iteration 86/1000 | Loss: 0.00003854
Iteration 87/1000 | Loss: 0.00005936
Iteration 88/1000 | Loss: 0.00003715
Iteration 89/1000 | Loss: 0.00005671
Iteration 90/1000 | Loss: 0.00003372
Iteration 91/1000 | Loss: 0.00004054
Iteration 92/1000 | Loss: 0.00002972
Iteration 93/1000 | Loss: 0.00003678
Iteration 94/1000 | Loss: 0.00003497
Iteration 95/1000 | Loss: 0.00002808
Iteration 96/1000 | Loss: 0.00002792
Iteration 97/1000 | Loss: 0.00002790
Iteration 98/1000 | Loss: 0.00002790
Iteration 99/1000 | Loss: 0.00002772
Iteration 100/1000 | Loss: 0.00002753
Iteration 101/1000 | Loss: 0.00002748
Iteration 102/1000 | Loss: 0.00011354
Iteration 103/1000 | Loss: 0.00003063
Iteration 104/1000 | Loss: 0.00002845
Iteration 105/1000 | Loss: 0.00011009
Iteration 106/1000 | Loss: 0.00002738
Iteration 107/1000 | Loss: 0.00002724
Iteration 108/1000 | Loss: 0.00002714
Iteration 109/1000 | Loss: 0.00002711
Iteration 110/1000 | Loss: 0.00002710
Iteration 111/1000 | Loss: 0.00002710
Iteration 112/1000 | Loss: 0.00002708
Iteration 113/1000 | Loss: 0.00002708
Iteration 114/1000 | Loss: 0.00002708
Iteration 115/1000 | Loss: 0.00002708
Iteration 116/1000 | Loss: 0.00002707
Iteration 117/1000 | Loss: 0.00002706
Iteration 118/1000 | Loss: 0.00002705
Iteration 119/1000 | Loss: 0.00002703
Iteration 120/1000 | Loss: 0.00002703
Iteration 121/1000 | Loss: 0.00002703
Iteration 122/1000 | Loss: 0.00002703
Iteration 123/1000 | Loss: 0.00002703
Iteration 124/1000 | Loss: 0.00002702
Iteration 125/1000 | Loss: 0.00002702
Iteration 126/1000 | Loss: 0.00002702
Iteration 127/1000 | Loss: 0.00002702
Iteration 128/1000 | Loss: 0.00002702
Iteration 129/1000 | Loss: 0.00002702
Iteration 130/1000 | Loss: 0.00002702
Iteration 131/1000 | Loss: 0.00002702
Iteration 132/1000 | Loss: 0.00002702
Iteration 133/1000 | Loss: 0.00002701
Iteration 134/1000 | Loss: 0.00002701
Iteration 135/1000 | Loss: 0.00002701
Iteration 136/1000 | Loss: 0.00002701
Iteration 137/1000 | Loss: 0.00002701
Iteration 138/1000 | Loss: 0.00002701
Iteration 139/1000 | Loss: 0.00002701
Iteration 140/1000 | Loss: 0.00002701
Iteration 141/1000 | Loss: 0.00002701
Iteration 142/1000 | Loss: 0.00002701
Iteration 143/1000 | Loss: 0.00002701
Iteration 144/1000 | Loss: 0.00002701
Iteration 145/1000 | Loss: 0.00002701
Iteration 146/1000 | Loss: 0.00002701
Iteration 147/1000 | Loss: 0.00002701
Iteration 148/1000 | Loss: 0.00002701
Iteration 149/1000 | Loss: 0.00002701
Iteration 150/1000 | Loss: 0.00002701
Iteration 151/1000 | Loss: 0.00002701
Iteration 152/1000 | Loss: 0.00002701
Iteration 153/1000 | Loss: 0.00002701
Iteration 154/1000 | Loss: 0.00002701
Iteration 155/1000 | Loss: 0.00002701
Iteration 156/1000 | Loss: 0.00002701
Iteration 157/1000 | Loss: 0.00002701
Iteration 158/1000 | Loss: 0.00002701
Iteration 159/1000 | Loss: 0.00002701
Iteration 160/1000 | Loss: 0.00002701
Iteration 161/1000 | Loss: 0.00002701
Iteration 162/1000 | Loss: 0.00002701
Iteration 163/1000 | Loss: 0.00002701
Iteration 164/1000 | Loss: 0.00002701
Iteration 165/1000 | Loss: 0.00002701
Iteration 166/1000 | Loss: 0.00002701
Iteration 167/1000 | Loss: 0.00002701
Iteration 168/1000 | Loss: 0.00002701
Iteration 169/1000 | Loss: 0.00002701
Iteration 170/1000 | Loss: 0.00002701
Iteration 171/1000 | Loss: 0.00002701
Iteration 172/1000 | Loss: 0.00002701
Iteration 173/1000 | Loss: 0.00002701
Iteration 174/1000 | Loss: 0.00002701
Iteration 175/1000 | Loss: 0.00002701
Iteration 176/1000 | Loss: 0.00002701
Iteration 177/1000 | Loss: 0.00002701
Iteration 178/1000 | Loss: 0.00002701
Iteration 179/1000 | Loss: 0.00002701
Iteration 180/1000 | Loss: 0.00002701
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.7011272322852165e-05, 2.7011272322852165e-05, 2.7011272322852165e-05, 2.7011272322852165e-05, 2.7011272322852165e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7011272322852165e-05

Optimization complete. Final v2v error: 4.166995048522949 mm

Highest mean error: 11.56105899810791 mm for frame 233

Lowest mean error: 3.5688648223876953 mm for frame 8

Saving results

Total time: 177.417578458786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819990
Iteration 2/25 | Loss: 0.00169551
Iteration 3/25 | Loss: 0.00132421
Iteration 4/25 | Loss: 0.00128925
Iteration 5/25 | Loss: 0.00128485
Iteration 6/25 | Loss: 0.00128411
Iteration 7/25 | Loss: 0.00128411
Iteration 8/25 | Loss: 0.00128411
Iteration 9/25 | Loss: 0.00128411
Iteration 10/25 | Loss: 0.00128411
Iteration 11/25 | Loss: 0.00128411
Iteration 12/25 | Loss: 0.00128411
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012841066345572472, 0.0012841066345572472, 0.0012841066345572472, 0.0012841066345572472, 0.0012841066345572472]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012841066345572472

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21935415
Iteration 2/25 | Loss: 0.00094682
Iteration 3/25 | Loss: 0.00094681
Iteration 4/25 | Loss: 0.00094680
Iteration 5/25 | Loss: 0.00094680
Iteration 6/25 | Loss: 0.00094680
Iteration 7/25 | Loss: 0.00094680
Iteration 8/25 | Loss: 0.00094680
Iteration 9/25 | Loss: 0.00094680
Iteration 10/25 | Loss: 0.00094680
Iteration 11/25 | Loss: 0.00094680
Iteration 12/25 | Loss: 0.00094680
Iteration 13/25 | Loss: 0.00094680
Iteration 14/25 | Loss: 0.00094680
Iteration 15/25 | Loss: 0.00094680
Iteration 16/25 | Loss: 0.00094680
Iteration 17/25 | Loss: 0.00094680
Iteration 18/25 | Loss: 0.00094680
Iteration 19/25 | Loss: 0.00094680
Iteration 20/25 | Loss: 0.00094680
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009468026692047715, 0.0009468026692047715, 0.0009468026692047715, 0.0009468026692047715, 0.0009468026692047715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009468026692047715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094680
Iteration 2/1000 | Loss: 0.00006423
Iteration 3/1000 | Loss: 0.00002847
Iteration 4/1000 | Loss: 0.00002239
Iteration 5/1000 | Loss: 0.00001962
Iteration 6/1000 | Loss: 0.00001861
Iteration 7/1000 | Loss: 0.00001815
Iteration 8/1000 | Loss: 0.00001757
Iteration 9/1000 | Loss: 0.00001712
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001680
Iteration 12/1000 | Loss: 0.00001675
Iteration 13/1000 | Loss: 0.00001664
Iteration 14/1000 | Loss: 0.00001658
Iteration 15/1000 | Loss: 0.00001657
Iteration 16/1000 | Loss: 0.00001656
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001650
Iteration 19/1000 | Loss: 0.00001648
Iteration 20/1000 | Loss: 0.00001646
Iteration 21/1000 | Loss: 0.00001639
Iteration 22/1000 | Loss: 0.00001638
Iteration 23/1000 | Loss: 0.00001635
Iteration 24/1000 | Loss: 0.00001635
Iteration 25/1000 | Loss: 0.00001633
Iteration 26/1000 | Loss: 0.00001633
Iteration 27/1000 | Loss: 0.00001633
Iteration 28/1000 | Loss: 0.00001633
Iteration 29/1000 | Loss: 0.00001633
Iteration 30/1000 | Loss: 0.00001633
Iteration 31/1000 | Loss: 0.00001632
Iteration 32/1000 | Loss: 0.00001632
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001632
Iteration 35/1000 | Loss: 0.00001632
Iteration 36/1000 | Loss: 0.00001631
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001628
Iteration 39/1000 | Loss: 0.00001627
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001625
Iteration 43/1000 | Loss: 0.00001625
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001624
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001623
Iteration 48/1000 | Loss: 0.00001623
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001623
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001623
Iteration 56/1000 | Loss: 0.00001623
Iteration 57/1000 | Loss: 0.00001623
Iteration 58/1000 | Loss: 0.00001623
Iteration 59/1000 | Loss: 0.00001623
Iteration 60/1000 | Loss: 0.00001623
Iteration 61/1000 | Loss: 0.00001623
Iteration 62/1000 | Loss: 0.00001623
Iteration 63/1000 | Loss: 0.00001623
Iteration 64/1000 | Loss: 0.00001623
Iteration 65/1000 | Loss: 0.00001623
Iteration 66/1000 | Loss: 0.00001623
Iteration 67/1000 | Loss: 0.00001623
Iteration 68/1000 | Loss: 0.00001623
Iteration 69/1000 | Loss: 0.00001623
Iteration 70/1000 | Loss: 0.00001623
Iteration 71/1000 | Loss: 0.00001623
Iteration 72/1000 | Loss: 0.00001623
Iteration 73/1000 | Loss: 0.00001623
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001623
Iteration 83/1000 | Loss: 0.00001623
Iteration 84/1000 | Loss: 0.00001623
Iteration 85/1000 | Loss: 0.00001623
Iteration 86/1000 | Loss: 0.00001623
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001623
Iteration 99/1000 | Loss: 0.00001623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.6229878383455798e-05, 1.6229878383455798e-05, 1.6229878383455798e-05, 1.6229878383455798e-05, 1.6229878383455798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6229878383455798e-05

Optimization complete. Final v2v error: 3.375253915786743 mm

Highest mean error: 3.6964681148529053 mm for frame 111

Lowest mean error: 2.9409427642822266 mm for frame 129

Saving results

Total time: 31.341841459274292
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047105
Iteration 2/25 | Loss: 0.00278972
Iteration 3/25 | Loss: 0.00195825
Iteration 4/25 | Loss: 0.00174417
Iteration 5/25 | Loss: 0.00173698
Iteration 6/25 | Loss: 0.00154818
Iteration 7/25 | Loss: 0.00150125
Iteration 8/25 | Loss: 0.00143084
Iteration 9/25 | Loss: 0.00142458
Iteration 10/25 | Loss: 0.00140624
Iteration 11/25 | Loss: 0.00139753
Iteration 12/25 | Loss: 0.00139247
Iteration 13/25 | Loss: 0.00138468
Iteration 14/25 | Loss: 0.00138308
Iteration 15/25 | Loss: 0.00137534
Iteration 16/25 | Loss: 0.00136711
Iteration 17/25 | Loss: 0.00138045
Iteration 18/25 | Loss: 0.00136508
Iteration 19/25 | Loss: 0.00135799
Iteration 20/25 | Loss: 0.00136018
Iteration 21/25 | Loss: 0.00135621
Iteration 22/25 | Loss: 0.00135374
Iteration 23/25 | Loss: 0.00135342
Iteration 24/25 | Loss: 0.00135460
Iteration 25/25 | Loss: 0.00135311

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23276007
Iteration 2/25 | Loss: 0.00162010
Iteration 3/25 | Loss: 0.00152560
Iteration 4/25 | Loss: 0.00152560
Iteration 5/25 | Loss: 0.00152560
Iteration 6/25 | Loss: 0.00152560
Iteration 7/25 | Loss: 0.00152560
Iteration 8/25 | Loss: 0.00152560
Iteration 9/25 | Loss: 0.00152560
Iteration 10/25 | Loss: 0.00152560
Iteration 11/25 | Loss: 0.00152560
Iteration 12/25 | Loss: 0.00152560
Iteration 13/25 | Loss: 0.00152560
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015255995094776154, 0.0015255995094776154, 0.0015255995094776154, 0.0015255995094776154, 0.0015255995094776154]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015255995094776154

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152560
Iteration 2/1000 | Loss: 0.00033293
Iteration 3/1000 | Loss: 0.00010524
Iteration 4/1000 | Loss: 0.00006889
Iteration 5/1000 | Loss: 0.00059607
Iteration 6/1000 | Loss: 0.00026386
Iteration 7/1000 | Loss: 0.00006861
Iteration 8/1000 | Loss: 0.00042764
Iteration 9/1000 | Loss: 0.00055268
Iteration 10/1000 | Loss: 0.00057296
Iteration 11/1000 | Loss: 0.00042342
Iteration 12/1000 | Loss: 0.00038562
Iteration 13/1000 | Loss: 0.00009945
Iteration 14/1000 | Loss: 0.00005922
Iteration 15/1000 | Loss: 0.00008127
Iteration 16/1000 | Loss: 0.00025087
Iteration 17/1000 | Loss: 0.00004194
Iteration 18/1000 | Loss: 0.00005084
Iteration 19/1000 | Loss: 0.00003698
Iteration 20/1000 | Loss: 0.00004475
Iteration 21/1000 | Loss: 0.00023768
Iteration 22/1000 | Loss: 0.00018082
Iteration 23/1000 | Loss: 0.00058655
Iteration 24/1000 | Loss: 0.00015558
Iteration 25/1000 | Loss: 0.00004147
Iteration 26/1000 | Loss: 0.00003920
Iteration 27/1000 | Loss: 0.00003746
Iteration 28/1000 | Loss: 0.00004094
Iteration 29/1000 | Loss: 0.00002568
Iteration 30/1000 | Loss: 0.00002447
Iteration 31/1000 | Loss: 0.00003852
Iteration 32/1000 | Loss: 0.00003388
Iteration 33/1000 | Loss: 0.00015757
Iteration 34/1000 | Loss: 0.00003807
Iteration 35/1000 | Loss: 0.00002352
Iteration 36/1000 | Loss: 0.00003387
Iteration 37/1000 | Loss: 0.00002387
Iteration 38/1000 | Loss: 0.00002886
Iteration 39/1000 | Loss: 0.00002568
Iteration 40/1000 | Loss: 0.00002166
Iteration 41/1000 | Loss: 0.00002243
Iteration 42/1000 | Loss: 0.00002339
Iteration 43/1000 | Loss: 0.00002127
Iteration 44/1000 | Loss: 0.00002055
Iteration 45/1000 | Loss: 0.00002055
Iteration 46/1000 | Loss: 0.00002054
Iteration 47/1000 | Loss: 0.00002050
Iteration 48/1000 | Loss: 0.00002044
Iteration 49/1000 | Loss: 0.00002178
Iteration 50/1000 | Loss: 0.00002033
Iteration 51/1000 | Loss: 0.00002033
Iteration 52/1000 | Loss: 0.00002032
Iteration 53/1000 | Loss: 0.00002032
Iteration 54/1000 | Loss: 0.00002032
Iteration 55/1000 | Loss: 0.00002031
Iteration 56/1000 | Loss: 0.00002031
Iteration 57/1000 | Loss: 0.00002031
Iteration 58/1000 | Loss: 0.00002178
Iteration 59/1000 | Loss: 0.00002021
Iteration 60/1000 | Loss: 0.00002021
Iteration 61/1000 | Loss: 0.00002021
Iteration 62/1000 | Loss: 0.00002021
Iteration 63/1000 | Loss: 0.00002021
Iteration 64/1000 | Loss: 0.00002021
Iteration 65/1000 | Loss: 0.00002021
Iteration 66/1000 | Loss: 0.00002020
Iteration 67/1000 | Loss: 0.00002020
Iteration 68/1000 | Loss: 0.00002020
Iteration 69/1000 | Loss: 0.00002020
Iteration 70/1000 | Loss: 0.00002020
Iteration 71/1000 | Loss: 0.00002020
Iteration 72/1000 | Loss: 0.00002020
Iteration 73/1000 | Loss: 0.00002020
Iteration 74/1000 | Loss: 0.00002020
Iteration 75/1000 | Loss: 0.00002019
Iteration 76/1000 | Loss: 0.00002018
Iteration 77/1000 | Loss: 0.00002018
Iteration 78/1000 | Loss: 0.00002018
Iteration 79/1000 | Loss: 0.00002017
Iteration 80/1000 | Loss: 0.00002017
Iteration 81/1000 | Loss: 0.00002017
Iteration 82/1000 | Loss: 0.00002017
Iteration 83/1000 | Loss: 0.00002016
Iteration 84/1000 | Loss: 0.00002016
Iteration 85/1000 | Loss: 0.00002016
Iteration 86/1000 | Loss: 0.00003848
Iteration 87/1000 | Loss: 0.00002123
Iteration 88/1000 | Loss: 0.00002350
Iteration 89/1000 | Loss: 0.00002017
Iteration 90/1000 | Loss: 0.00002016
Iteration 91/1000 | Loss: 0.00002016
Iteration 92/1000 | Loss: 0.00002015
Iteration 93/1000 | Loss: 0.00002015
Iteration 94/1000 | Loss: 0.00002014
Iteration 95/1000 | Loss: 0.00002014
Iteration 96/1000 | Loss: 0.00002014
Iteration 97/1000 | Loss: 0.00002014
Iteration 98/1000 | Loss: 0.00002014
Iteration 99/1000 | Loss: 0.00002014
Iteration 100/1000 | Loss: 0.00002013
Iteration 101/1000 | Loss: 0.00002013
Iteration 102/1000 | Loss: 0.00002376
Iteration 103/1000 | Loss: 0.00002093
Iteration 104/1000 | Loss: 0.00002013
Iteration 105/1000 | Loss: 0.00002013
Iteration 106/1000 | Loss: 0.00002013
Iteration 107/1000 | Loss: 0.00002018
Iteration 108/1000 | Loss: 0.00002016
Iteration 109/1000 | Loss: 0.00002013
Iteration 110/1000 | Loss: 0.00002013
Iteration 111/1000 | Loss: 0.00002012
Iteration 112/1000 | Loss: 0.00002012
Iteration 113/1000 | Loss: 0.00002012
Iteration 114/1000 | Loss: 0.00002012
Iteration 115/1000 | Loss: 0.00002012
Iteration 116/1000 | Loss: 0.00002012
Iteration 117/1000 | Loss: 0.00002012
Iteration 118/1000 | Loss: 0.00002012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [2.0123768990742974e-05, 2.0123768990742974e-05, 2.0123768990742974e-05, 2.0123768990742974e-05, 2.0123768990742974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0123768990742974e-05

Optimization complete. Final v2v error: 3.6894609928131104 mm

Highest mean error: 10.542709350585938 mm for frame 239

Lowest mean error: 3.133653163909912 mm for frame 81

Saving results

Total time: 133.8710596561432
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01127147
Iteration 2/25 | Loss: 0.01127147
Iteration 3/25 | Loss: 0.01127147
Iteration 4/25 | Loss: 0.01127147
Iteration 5/25 | Loss: 0.01127147
Iteration 6/25 | Loss: 0.01127146
Iteration 7/25 | Loss: 0.01127146
Iteration 8/25 | Loss: 0.01127146
Iteration 9/25 | Loss: 0.01127146
Iteration 10/25 | Loss: 0.01127146
Iteration 11/25 | Loss: 0.01127146
Iteration 12/25 | Loss: 0.01127146
Iteration 13/25 | Loss: 0.01127146
Iteration 14/25 | Loss: 0.01127146
Iteration 15/25 | Loss: 0.01127145
Iteration 16/25 | Loss: 0.01127145
Iteration 17/25 | Loss: 0.01127145
Iteration 18/25 | Loss: 0.01127145
Iteration 19/25 | Loss: 0.01127145
Iteration 20/25 | Loss: 0.01127145
Iteration 21/25 | Loss: 0.01127145
Iteration 22/25 | Loss: 0.01127145
Iteration 23/25 | Loss: 0.01127145
Iteration 24/25 | Loss: 0.01127145
Iteration 25/25 | Loss: 0.01127145

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62684536
Iteration 2/25 | Loss: 0.08353207
Iteration 3/25 | Loss: 0.08344696
Iteration 4/25 | Loss: 0.08311823
Iteration 5/25 | Loss: 0.08311795
Iteration 6/25 | Loss: 0.08311795
Iteration 7/25 | Loss: 0.08311794
Iteration 8/25 | Loss: 0.08311794
Iteration 9/25 | Loss: 0.08311794
Iteration 10/25 | Loss: 0.08311794
Iteration 11/25 | Loss: 0.08311794
Iteration 12/25 | Loss: 0.08311793
Iteration 13/25 | Loss: 0.08311794
Iteration 14/25 | Loss: 0.08311794
Iteration 15/25 | Loss: 0.08311793
Iteration 16/25 | Loss: 0.08311793
Iteration 17/25 | Loss: 0.08311793
Iteration 18/25 | Loss: 0.08311793
Iteration 19/25 | Loss: 0.08311793
Iteration 20/25 | Loss: 0.08311793
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.08311793208122253, 0.08311793208122253, 0.08311793208122253, 0.08311793208122253, 0.08311793208122253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08311793208122253

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08311793
Iteration 2/1000 | Loss: 0.00400803
Iteration 3/1000 | Loss: 0.00131864
Iteration 4/1000 | Loss: 0.00139773
Iteration 5/1000 | Loss: 0.00057431
Iteration 6/1000 | Loss: 0.00147804
Iteration 7/1000 | Loss: 0.00022359
Iteration 8/1000 | Loss: 0.00033504
Iteration 9/1000 | Loss: 0.00005984
Iteration 10/1000 | Loss: 0.00014805
Iteration 11/1000 | Loss: 0.00009276
Iteration 12/1000 | Loss: 0.00007994
Iteration 13/1000 | Loss: 0.00003801
Iteration 14/1000 | Loss: 0.00003986
Iteration 15/1000 | Loss: 0.00002987
Iteration 16/1000 | Loss: 0.00004204
Iteration 17/1000 | Loss: 0.00006041
Iteration 18/1000 | Loss: 0.00002500
Iteration 19/1000 | Loss: 0.00005436
Iteration 20/1000 | Loss: 0.00005465
Iteration 21/1000 | Loss: 0.00009964
Iteration 22/1000 | Loss: 0.00005609
Iteration 23/1000 | Loss: 0.00002165
Iteration 24/1000 | Loss: 0.00002092
Iteration 25/1000 | Loss: 0.00005205
Iteration 26/1000 | Loss: 0.00004080
Iteration 27/1000 | Loss: 0.00002264
Iteration 28/1000 | Loss: 0.00002088
Iteration 29/1000 | Loss: 0.00002800
Iteration 30/1000 | Loss: 0.00001925
Iteration 31/1000 | Loss: 0.00002706
Iteration 32/1000 | Loss: 0.00006341
Iteration 33/1000 | Loss: 0.00009357
Iteration 34/1000 | Loss: 0.00003099
Iteration 35/1000 | Loss: 0.00003230
Iteration 36/1000 | Loss: 0.00001877
Iteration 37/1000 | Loss: 0.00004080
Iteration 38/1000 | Loss: 0.00001858
Iteration 39/1000 | Loss: 0.00005617
Iteration 40/1000 | Loss: 0.00002510
Iteration 41/1000 | Loss: 0.00005562
Iteration 42/1000 | Loss: 0.00001832
Iteration 43/1000 | Loss: 0.00002470
Iteration 44/1000 | Loss: 0.00008527
Iteration 45/1000 | Loss: 0.00002819
Iteration 46/1000 | Loss: 0.00002625
Iteration 47/1000 | Loss: 0.00001809
Iteration 48/1000 | Loss: 0.00001801
Iteration 49/1000 | Loss: 0.00001799
Iteration 50/1000 | Loss: 0.00001798
Iteration 51/1000 | Loss: 0.00001798
Iteration 52/1000 | Loss: 0.00001797
Iteration 53/1000 | Loss: 0.00001794
Iteration 54/1000 | Loss: 0.00017027
Iteration 55/1000 | Loss: 0.00001821
Iteration 56/1000 | Loss: 0.00004963
Iteration 57/1000 | Loss: 0.00003343
Iteration 58/1000 | Loss: 0.00003222
Iteration 59/1000 | Loss: 0.00001779
Iteration 60/1000 | Loss: 0.00001767
Iteration 61/1000 | Loss: 0.00001766
Iteration 62/1000 | Loss: 0.00001765
Iteration 63/1000 | Loss: 0.00001764
Iteration 64/1000 | Loss: 0.00001763
Iteration 65/1000 | Loss: 0.00001762
Iteration 66/1000 | Loss: 0.00001762
Iteration 67/1000 | Loss: 0.00001762
Iteration 68/1000 | Loss: 0.00001762
Iteration 69/1000 | Loss: 0.00001762
Iteration 70/1000 | Loss: 0.00001762
Iteration 71/1000 | Loss: 0.00001762
Iteration 72/1000 | Loss: 0.00001761
Iteration 73/1000 | Loss: 0.00001761
Iteration 74/1000 | Loss: 0.00001761
Iteration 75/1000 | Loss: 0.00001761
Iteration 76/1000 | Loss: 0.00001761
Iteration 77/1000 | Loss: 0.00001761
Iteration 78/1000 | Loss: 0.00001761
Iteration 79/1000 | Loss: 0.00001760
Iteration 80/1000 | Loss: 0.00001760
Iteration 81/1000 | Loss: 0.00001759
Iteration 82/1000 | Loss: 0.00001759
Iteration 83/1000 | Loss: 0.00001758
Iteration 84/1000 | Loss: 0.00003670
Iteration 85/1000 | Loss: 0.00002158
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001758
Iteration 88/1000 | Loss: 0.00001758
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Iteration 92/1000 | Loss: 0.00001758
Iteration 93/1000 | Loss: 0.00006850
Iteration 94/1000 | Loss: 0.00006850
Iteration 95/1000 | Loss: 0.00002000
Iteration 96/1000 | Loss: 0.00001755
Iteration 97/1000 | Loss: 0.00001755
Iteration 98/1000 | Loss: 0.00001755
Iteration 99/1000 | Loss: 0.00001755
Iteration 100/1000 | Loss: 0.00001755
Iteration 101/1000 | Loss: 0.00001754
Iteration 102/1000 | Loss: 0.00001754
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001754
Iteration 106/1000 | Loss: 0.00001754
Iteration 107/1000 | Loss: 0.00001754
Iteration 108/1000 | Loss: 0.00001754
Iteration 109/1000 | Loss: 0.00001754
Iteration 110/1000 | Loss: 0.00001754
Iteration 111/1000 | Loss: 0.00001754
Iteration 112/1000 | Loss: 0.00001754
Iteration 113/1000 | Loss: 0.00001754
Iteration 114/1000 | Loss: 0.00001754
Iteration 115/1000 | Loss: 0.00001754
Iteration 116/1000 | Loss: 0.00001754
Iteration 117/1000 | Loss: 0.00005381
Iteration 118/1000 | Loss: 0.00004605
Iteration 119/1000 | Loss: 0.00003172
Iteration 120/1000 | Loss: 0.00004820
Iteration 121/1000 | Loss: 0.00002545
Iteration 122/1000 | Loss: 0.00006609
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001758
Iteration 125/1000 | Loss: 0.00001753
Iteration 126/1000 | Loss: 0.00003489
Iteration 127/1000 | Loss: 0.00003515
Iteration 128/1000 | Loss: 0.00012561
Iteration 129/1000 | Loss: 0.00002300
Iteration 130/1000 | Loss: 0.00003235
Iteration 131/1000 | Loss: 0.00004697
Iteration 132/1000 | Loss: 0.00002734
Iteration 133/1000 | Loss: 0.00002374
Iteration 134/1000 | Loss: 0.00004988
Iteration 135/1000 | Loss: 0.00003590
Iteration 136/1000 | Loss: 0.00002869
Iteration 137/1000 | Loss: 0.00001757
Iteration 138/1000 | Loss: 0.00001756
Iteration 139/1000 | Loss: 0.00001756
Iteration 140/1000 | Loss: 0.00001756
Iteration 141/1000 | Loss: 0.00001756
Iteration 142/1000 | Loss: 0.00001756
Iteration 143/1000 | Loss: 0.00001756
Iteration 144/1000 | Loss: 0.00001756
Iteration 145/1000 | Loss: 0.00001756
Iteration 146/1000 | Loss: 0.00001756
Iteration 147/1000 | Loss: 0.00001754
Iteration 148/1000 | Loss: 0.00001754
Iteration 149/1000 | Loss: 0.00001753
Iteration 150/1000 | Loss: 0.00001753
Iteration 151/1000 | Loss: 0.00001753
Iteration 152/1000 | Loss: 0.00001753
Iteration 153/1000 | Loss: 0.00001752
Iteration 154/1000 | Loss: 0.00001752
Iteration 155/1000 | Loss: 0.00001752
Iteration 156/1000 | Loss: 0.00001752
Iteration 157/1000 | Loss: 0.00002456
Iteration 158/1000 | Loss: 0.00001766
Iteration 159/1000 | Loss: 0.00001756
Iteration 160/1000 | Loss: 0.00003319
Iteration 161/1000 | Loss: 0.00001921
Iteration 162/1000 | Loss: 0.00001873
Iteration 163/1000 | Loss: 0.00001873
Iteration 164/1000 | Loss: 0.00002197
Iteration 165/1000 | Loss: 0.00001796
Iteration 166/1000 | Loss: 0.00001761
Iteration 167/1000 | Loss: 0.00001753
Iteration 168/1000 | Loss: 0.00001753
Iteration 169/1000 | Loss: 0.00001752
Iteration 170/1000 | Loss: 0.00001752
Iteration 171/1000 | Loss: 0.00001752
Iteration 172/1000 | Loss: 0.00001752
Iteration 173/1000 | Loss: 0.00001752
Iteration 174/1000 | Loss: 0.00001752
Iteration 175/1000 | Loss: 0.00001752
Iteration 176/1000 | Loss: 0.00001752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.7522474081488326e-05, 1.7522474081488326e-05, 1.7522474081488326e-05, 1.7522474081488326e-05, 1.7522474081488326e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7522474081488326e-05

Optimization complete. Final v2v error: 3.5344693660736084 mm

Highest mean error: 9.493206024169922 mm for frame 158

Lowest mean error: 3.090972900390625 mm for frame 205

Saving results

Total time: 142.2175350189209
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01146430
Iteration 2/25 | Loss: 0.00182716
Iteration 3/25 | Loss: 0.00143805
Iteration 4/25 | Loss: 0.00141609
Iteration 5/25 | Loss: 0.00141231
Iteration 6/25 | Loss: 0.00141100
Iteration 7/25 | Loss: 0.00141100
Iteration 8/25 | Loss: 0.00141100
Iteration 9/25 | Loss: 0.00141100
Iteration 10/25 | Loss: 0.00141100
Iteration 11/25 | Loss: 0.00141100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014110006159171462, 0.0014110006159171462, 0.0014110006159171462, 0.0014110006159171462, 0.0014110006159171462]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014110006159171462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90470982
Iteration 2/25 | Loss: 0.00119447
Iteration 3/25 | Loss: 0.00119447
Iteration 4/25 | Loss: 0.00119447
Iteration 5/25 | Loss: 0.00119447
Iteration 6/25 | Loss: 0.00119447
Iteration 7/25 | Loss: 0.00119447
Iteration 8/25 | Loss: 0.00119447
Iteration 9/25 | Loss: 0.00119447
Iteration 10/25 | Loss: 0.00119447
Iteration 11/25 | Loss: 0.00119447
Iteration 12/25 | Loss: 0.00119447
Iteration 13/25 | Loss: 0.00119447
Iteration 14/25 | Loss: 0.00119447
Iteration 15/25 | Loss: 0.00119447
Iteration 16/25 | Loss: 0.00119447
Iteration 17/25 | Loss: 0.00119447
Iteration 18/25 | Loss: 0.00119447
Iteration 19/25 | Loss: 0.00119447
Iteration 20/25 | Loss: 0.00119447
Iteration 21/25 | Loss: 0.00119447
Iteration 22/25 | Loss: 0.00119447
Iteration 23/25 | Loss: 0.00119447
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011944693978875875, 0.0011944693978875875, 0.0011944693978875875, 0.0011944693978875875, 0.0011944693978875875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011944693978875875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119447
Iteration 2/1000 | Loss: 0.00006315
Iteration 3/1000 | Loss: 0.00004440
Iteration 4/1000 | Loss: 0.00003931
Iteration 5/1000 | Loss: 0.00003729
Iteration 6/1000 | Loss: 0.00003632
Iteration 7/1000 | Loss: 0.00003535
Iteration 8/1000 | Loss: 0.00003463
Iteration 9/1000 | Loss: 0.00003418
Iteration 10/1000 | Loss: 0.00003391
Iteration 11/1000 | Loss: 0.00003370
Iteration 12/1000 | Loss: 0.00003351
Iteration 13/1000 | Loss: 0.00003332
Iteration 14/1000 | Loss: 0.00003320
Iteration 15/1000 | Loss: 0.00003303
Iteration 16/1000 | Loss: 0.00003298
Iteration 17/1000 | Loss: 0.00003284
Iteration 18/1000 | Loss: 0.00003282
Iteration 19/1000 | Loss: 0.00003282
Iteration 20/1000 | Loss: 0.00003281
Iteration 21/1000 | Loss: 0.00003281
Iteration 22/1000 | Loss: 0.00003276
Iteration 23/1000 | Loss: 0.00003267
Iteration 24/1000 | Loss: 0.00003255
Iteration 25/1000 | Loss: 0.00003253
Iteration 26/1000 | Loss: 0.00003247
Iteration 27/1000 | Loss: 0.00003241
Iteration 28/1000 | Loss: 0.00003241
Iteration 29/1000 | Loss: 0.00003241
Iteration 30/1000 | Loss: 0.00003240
Iteration 31/1000 | Loss: 0.00003239
Iteration 32/1000 | Loss: 0.00003239
Iteration 33/1000 | Loss: 0.00003239
Iteration 34/1000 | Loss: 0.00003238
Iteration 35/1000 | Loss: 0.00003238
Iteration 36/1000 | Loss: 0.00003238
Iteration 37/1000 | Loss: 0.00003238
Iteration 38/1000 | Loss: 0.00003237
Iteration 39/1000 | Loss: 0.00003236
Iteration 40/1000 | Loss: 0.00003236
Iteration 41/1000 | Loss: 0.00003236
Iteration 42/1000 | Loss: 0.00003236
Iteration 43/1000 | Loss: 0.00003236
Iteration 44/1000 | Loss: 0.00003236
Iteration 45/1000 | Loss: 0.00003236
Iteration 46/1000 | Loss: 0.00003236
Iteration 47/1000 | Loss: 0.00003236
Iteration 48/1000 | Loss: 0.00003235
Iteration 49/1000 | Loss: 0.00003235
Iteration 50/1000 | Loss: 0.00003234
Iteration 51/1000 | Loss: 0.00003234
Iteration 52/1000 | Loss: 0.00003234
Iteration 53/1000 | Loss: 0.00003234
Iteration 54/1000 | Loss: 0.00003234
Iteration 55/1000 | Loss: 0.00003234
Iteration 56/1000 | Loss: 0.00003234
Iteration 57/1000 | Loss: 0.00003233
Iteration 58/1000 | Loss: 0.00003233
Iteration 59/1000 | Loss: 0.00003232
Iteration 60/1000 | Loss: 0.00003232
Iteration 61/1000 | Loss: 0.00003232
Iteration 62/1000 | Loss: 0.00003231
Iteration 63/1000 | Loss: 0.00003231
Iteration 64/1000 | Loss: 0.00003231
Iteration 65/1000 | Loss: 0.00003231
Iteration 66/1000 | Loss: 0.00003230
Iteration 67/1000 | Loss: 0.00003230
Iteration 68/1000 | Loss: 0.00003230
Iteration 69/1000 | Loss: 0.00003229
Iteration 70/1000 | Loss: 0.00003229
Iteration 71/1000 | Loss: 0.00003229
Iteration 72/1000 | Loss: 0.00003229
Iteration 73/1000 | Loss: 0.00003229
Iteration 74/1000 | Loss: 0.00003229
Iteration 75/1000 | Loss: 0.00003229
Iteration 76/1000 | Loss: 0.00003229
Iteration 77/1000 | Loss: 0.00003229
Iteration 78/1000 | Loss: 0.00003229
Iteration 79/1000 | Loss: 0.00003229
Iteration 80/1000 | Loss: 0.00003229
Iteration 81/1000 | Loss: 0.00003229
Iteration 82/1000 | Loss: 0.00003229
Iteration 83/1000 | Loss: 0.00003229
Iteration 84/1000 | Loss: 0.00003228
Iteration 85/1000 | Loss: 0.00003228
Iteration 86/1000 | Loss: 0.00003228
Iteration 87/1000 | Loss: 0.00003228
Iteration 88/1000 | Loss: 0.00003228
Iteration 89/1000 | Loss: 0.00003227
Iteration 90/1000 | Loss: 0.00003227
Iteration 91/1000 | Loss: 0.00003227
Iteration 92/1000 | Loss: 0.00003227
Iteration 93/1000 | Loss: 0.00003226
Iteration 94/1000 | Loss: 0.00003226
Iteration 95/1000 | Loss: 0.00003226
Iteration 96/1000 | Loss: 0.00003226
Iteration 97/1000 | Loss: 0.00003225
Iteration 98/1000 | Loss: 0.00003225
Iteration 99/1000 | Loss: 0.00003225
Iteration 100/1000 | Loss: 0.00003225
Iteration 101/1000 | Loss: 0.00003225
Iteration 102/1000 | Loss: 0.00003225
Iteration 103/1000 | Loss: 0.00003225
Iteration 104/1000 | Loss: 0.00003225
Iteration 105/1000 | Loss: 0.00003224
Iteration 106/1000 | Loss: 0.00003224
Iteration 107/1000 | Loss: 0.00003224
Iteration 108/1000 | Loss: 0.00003224
Iteration 109/1000 | Loss: 0.00003224
Iteration 110/1000 | Loss: 0.00003224
Iteration 111/1000 | Loss: 0.00003224
Iteration 112/1000 | Loss: 0.00003224
Iteration 113/1000 | Loss: 0.00003224
Iteration 114/1000 | Loss: 0.00003224
Iteration 115/1000 | Loss: 0.00003224
Iteration 116/1000 | Loss: 0.00003224
Iteration 117/1000 | Loss: 0.00003224
Iteration 118/1000 | Loss: 0.00003224
Iteration 119/1000 | Loss: 0.00003224
Iteration 120/1000 | Loss: 0.00003224
Iteration 121/1000 | Loss: 0.00003224
Iteration 122/1000 | Loss: 0.00003224
Iteration 123/1000 | Loss: 0.00003224
Iteration 124/1000 | Loss: 0.00003224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [3.223819294362329e-05, 3.223819294362329e-05, 3.223819294362329e-05, 3.223819294362329e-05, 3.223819294362329e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.223819294362329e-05

Optimization complete. Final v2v error: 4.603984832763672 mm

Highest mean error: 5.415464878082275 mm for frame 139

Lowest mean error: 4.091014385223389 mm for frame 3

Saving results

Total time: 44.96300435066223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053924
Iteration 2/25 | Loss: 0.00232287
Iteration 3/25 | Loss: 0.00173530
Iteration 4/25 | Loss: 0.00162318
Iteration 5/25 | Loss: 0.00156970
Iteration 6/25 | Loss: 0.00144683
Iteration 7/25 | Loss: 0.00135544
Iteration 8/25 | Loss: 0.00130677
Iteration 9/25 | Loss: 0.00128792
Iteration 10/25 | Loss: 0.00127700
Iteration 11/25 | Loss: 0.00126608
Iteration 12/25 | Loss: 0.00125772
Iteration 13/25 | Loss: 0.00125456
Iteration 14/25 | Loss: 0.00125447
Iteration 15/25 | Loss: 0.00124157
Iteration 16/25 | Loss: 0.00124171
Iteration 17/25 | Loss: 0.00123605
Iteration 18/25 | Loss: 0.00123223
Iteration 19/25 | Loss: 0.00122985
Iteration 20/25 | Loss: 0.00122931
Iteration 21/25 | Loss: 0.00122896
Iteration 22/25 | Loss: 0.00122871
Iteration 23/25 | Loss: 0.00122857
Iteration 24/25 | Loss: 0.00122831
Iteration 25/25 | Loss: 0.00122793

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29568458
Iteration 2/25 | Loss: 0.00169380
Iteration 3/25 | Loss: 0.00154665
Iteration 4/25 | Loss: 0.00154664
Iteration 5/25 | Loss: 0.00154664
Iteration 6/25 | Loss: 0.00154664
Iteration 7/25 | Loss: 0.00154664
Iteration 8/25 | Loss: 0.00154664
Iteration 9/25 | Loss: 0.00154664
Iteration 10/25 | Loss: 0.00154664
Iteration 11/25 | Loss: 0.00154664
Iteration 12/25 | Loss: 0.00154664
Iteration 13/25 | Loss: 0.00154664
Iteration 14/25 | Loss: 0.00154664
Iteration 15/25 | Loss: 0.00154664
Iteration 16/25 | Loss: 0.00154664
Iteration 17/25 | Loss: 0.00154664
Iteration 18/25 | Loss: 0.00154664
Iteration 19/25 | Loss: 0.00154664
Iteration 20/25 | Loss: 0.00154664
Iteration 21/25 | Loss: 0.00154664
Iteration 22/25 | Loss: 0.00154664
Iteration 23/25 | Loss: 0.00154664
Iteration 24/25 | Loss: 0.00154664
Iteration 25/25 | Loss: 0.00154664

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154664
Iteration 2/1000 | Loss: 0.00010233
Iteration 3/1000 | Loss: 0.00004060
Iteration 4/1000 | Loss: 0.00003486
Iteration 5/1000 | Loss: 0.00003224
Iteration 6/1000 | Loss: 0.00009158
Iteration 7/1000 | Loss: 0.00054565
Iteration 8/1000 | Loss: 0.00025248
Iteration 9/1000 | Loss: 0.00006613
Iteration 10/1000 | Loss: 0.00004180
Iteration 11/1000 | Loss: 0.00003209
Iteration 12/1000 | Loss: 0.00018286
Iteration 13/1000 | Loss: 0.00013150
Iteration 14/1000 | Loss: 0.00020727
Iteration 15/1000 | Loss: 0.00002926
Iteration 16/1000 | Loss: 0.00002526
Iteration 17/1000 | Loss: 0.00013002
Iteration 18/1000 | Loss: 0.00002308
Iteration 19/1000 | Loss: 0.00007011
Iteration 20/1000 | Loss: 0.00012620
Iteration 21/1000 | Loss: 0.00002424
Iteration 22/1000 | Loss: 0.00003027
Iteration 23/1000 | Loss: 0.00005906
Iteration 24/1000 | Loss: 0.00002128
Iteration 25/1000 | Loss: 0.00002115
Iteration 26/1000 | Loss: 0.00006156
Iteration 27/1000 | Loss: 0.00005743
Iteration 28/1000 | Loss: 0.00007258
Iteration 29/1000 | Loss: 0.00002063
Iteration 30/1000 | Loss: 0.00002043
Iteration 31/1000 | Loss: 0.00005688
Iteration 32/1000 | Loss: 0.00002057
Iteration 33/1000 | Loss: 0.00006340
Iteration 34/1000 | Loss: 0.00002014
Iteration 35/1000 | Loss: 0.00002012
Iteration 36/1000 | Loss: 0.00002006
Iteration 37/1000 | Loss: 0.00002006
Iteration 38/1000 | Loss: 0.00002006
Iteration 39/1000 | Loss: 0.00002005
Iteration 40/1000 | Loss: 0.00002005
Iteration 41/1000 | Loss: 0.00002005
Iteration 42/1000 | Loss: 0.00002005
Iteration 43/1000 | Loss: 0.00002005
Iteration 44/1000 | Loss: 0.00002004
Iteration 45/1000 | Loss: 0.00002004
Iteration 46/1000 | Loss: 0.00005562
Iteration 47/1000 | Loss: 0.00006234
Iteration 48/1000 | Loss: 0.00005038
Iteration 49/1000 | Loss: 0.00002576
Iteration 50/1000 | Loss: 0.00003864
Iteration 51/1000 | Loss: 0.00002018
Iteration 52/1000 | Loss: 0.00001991
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001988
Iteration 59/1000 | Loss: 0.00001988
Iteration 60/1000 | Loss: 0.00001988
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00003296
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001986
Iteration 69/1000 | Loss: 0.00001986
Iteration 70/1000 | Loss: 0.00001986
Iteration 71/1000 | Loss: 0.00001986
Iteration 72/1000 | Loss: 0.00001986
Iteration 73/1000 | Loss: 0.00001986
Iteration 74/1000 | Loss: 0.00001986
Iteration 75/1000 | Loss: 0.00001985
Iteration 76/1000 | Loss: 0.00001985
Iteration 77/1000 | Loss: 0.00001985
Iteration 78/1000 | Loss: 0.00001985
Iteration 79/1000 | Loss: 0.00001985
Iteration 80/1000 | Loss: 0.00001985
Iteration 81/1000 | Loss: 0.00001984
Iteration 82/1000 | Loss: 0.00001984
Iteration 83/1000 | Loss: 0.00001984
Iteration 84/1000 | Loss: 0.00001984
Iteration 85/1000 | Loss: 0.00001984
Iteration 86/1000 | Loss: 0.00001984
Iteration 87/1000 | Loss: 0.00001983
Iteration 88/1000 | Loss: 0.00001983
Iteration 89/1000 | Loss: 0.00001983
Iteration 90/1000 | Loss: 0.00001983
Iteration 91/1000 | Loss: 0.00001983
Iteration 92/1000 | Loss: 0.00001983
Iteration 93/1000 | Loss: 0.00001983
Iteration 94/1000 | Loss: 0.00001982
Iteration 95/1000 | Loss: 0.00001982
Iteration 96/1000 | Loss: 0.00001982
Iteration 97/1000 | Loss: 0.00001981
Iteration 98/1000 | Loss: 0.00001981
Iteration 99/1000 | Loss: 0.00001981
Iteration 100/1000 | Loss: 0.00001981
Iteration 101/1000 | Loss: 0.00001981
Iteration 102/1000 | Loss: 0.00001981
Iteration 103/1000 | Loss: 0.00001981
Iteration 104/1000 | Loss: 0.00001980
Iteration 105/1000 | Loss: 0.00001980
Iteration 106/1000 | Loss: 0.00001980
Iteration 107/1000 | Loss: 0.00001980
Iteration 108/1000 | Loss: 0.00001980
Iteration 109/1000 | Loss: 0.00001980
Iteration 110/1000 | Loss: 0.00001980
Iteration 111/1000 | Loss: 0.00001980
Iteration 112/1000 | Loss: 0.00004506
Iteration 113/1000 | Loss: 0.00004504
Iteration 114/1000 | Loss: 0.00004504
Iteration 115/1000 | Loss: 0.00023089
Iteration 116/1000 | Loss: 0.00003410
Iteration 117/1000 | Loss: 0.00006106
Iteration 118/1000 | Loss: 0.00002923
Iteration 119/1000 | Loss: 0.00004479
Iteration 120/1000 | Loss: 0.00006483
Iteration 121/1000 | Loss: 0.00007187
Iteration 122/1000 | Loss: 0.00003942
Iteration 123/1000 | Loss: 0.00006509
Iteration 124/1000 | Loss: 0.00006752
Iteration 125/1000 | Loss: 0.00004646
Iteration 126/1000 | Loss: 0.00002649
Iteration 127/1000 | Loss: 0.00004575
Iteration 128/1000 | Loss: 0.00002957
Iteration 129/1000 | Loss: 0.00015712
Iteration 130/1000 | Loss: 0.00003179
Iteration 131/1000 | Loss: 0.00004029
Iteration 132/1000 | Loss: 0.00005767
Iteration 133/1000 | Loss: 0.00004439
Iteration 134/1000 | Loss: 0.00002844
Iteration 135/1000 | Loss: 0.00004292
Iteration 136/1000 | Loss: 0.00006546
Iteration 137/1000 | Loss: 0.00002122
Iteration 138/1000 | Loss: 0.00002048
Iteration 139/1000 | Loss: 0.00003745
Iteration 140/1000 | Loss: 0.00002068
Iteration 141/1000 | Loss: 0.00002432
Iteration 142/1000 | Loss: 0.00001989
Iteration 143/1000 | Loss: 0.00001989
Iteration 144/1000 | Loss: 0.00001989
Iteration 145/1000 | Loss: 0.00001989
Iteration 146/1000 | Loss: 0.00001988
Iteration 147/1000 | Loss: 0.00001988
Iteration 148/1000 | Loss: 0.00001984
Iteration 149/1000 | Loss: 0.00001981
Iteration 150/1000 | Loss: 0.00002864
Iteration 151/1000 | Loss: 0.00001979
Iteration 152/1000 | Loss: 0.00001977
Iteration 153/1000 | Loss: 0.00001977
Iteration 154/1000 | Loss: 0.00001977
Iteration 155/1000 | Loss: 0.00001977
Iteration 156/1000 | Loss: 0.00001977
Iteration 157/1000 | Loss: 0.00001977
Iteration 158/1000 | Loss: 0.00001977
Iteration 159/1000 | Loss: 0.00001976
Iteration 160/1000 | Loss: 0.00001976
Iteration 161/1000 | Loss: 0.00001976
Iteration 162/1000 | Loss: 0.00004335
Iteration 163/1000 | Loss: 0.00003733
Iteration 164/1000 | Loss: 0.00010985
Iteration 165/1000 | Loss: 0.00004204
Iteration 166/1000 | Loss: 0.00002258
Iteration 167/1000 | Loss: 0.00001983
Iteration 168/1000 | Loss: 0.00001982
Iteration 169/1000 | Loss: 0.00001980
Iteration 170/1000 | Loss: 0.00001978
Iteration 171/1000 | Loss: 0.00001975
Iteration 172/1000 | Loss: 0.00001974
Iteration 173/1000 | Loss: 0.00001974
Iteration 174/1000 | Loss: 0.00001974
Iteration 175/1000 | Loss: 0.00001973
Iteration 176/1000 | Loss: 0.00001973
Iteration 177/1000 | Loss: 0.00001972
Iteration 178/1000 | Loss: 0.00001972
Iteration 179/1000 | Loss: 0.00001972
Iteration 180/1000 | Loss: 0.00001972
Iteration 181/1000 | Loss: 0.00001972
Iteration 182/1000 | Loss: 0.00001971
Iteration 183/1000 | Loss: 0.00001971
Iteration 184/1000 | Loss: 0.00001971
Iteration 185/1000 | Loss: 0.00001970
Iteration 186/1000 | Loss: 0.00001970
Iteration 187/1000 | Loss: 0.00001970
Iteration 188/1000 | Loss: 0.00001970
Iteration 189/1000 | Loss: 0.00001970
Iteration 190/1000 | Loss: 0.00001970
Iteration 191/1000 | Loss: 0.00001970
Iteration 192/1000 | Loss: 0.00001970
Iteration 193/1000 | Loss: 0.00001970
Iteration 194/1000 | Loss: 0.00001970
Iteration 195/1000 | Loss: 0.00001970
Iteration 196/1000 | Loss: 0.00001970
Iteration 197/1000 | Loss: 0.00001970
Iteration 198/1000 | Loss: 0.00001970
Iteration 199/1000 | Loss: 0.00001969
Iteration 200/1000 | Loss: 0.00001969
Iteration 201/1000 | Loss: 0.00001969
Iteration 202/1000 | Loss: 0.00001968
Iteration 203/1000 | Loss: 0.00001968
Iteration 204/1000 | Loss: 0.00001968
Iteration 205/1000 | Loss: 0.00001968
Iteration 206/1000 | Loss: 0.00001968
Iteration 207/1000 | Loss: 0.00001968
Iteration 208/1000 | Loss: 0.00001968
Iteration 209/1000 | Loss: 0.00001967
Iteration 210/1000 | Loss: 0.00001967
Iteration 211/1000 | Loss: 0.00001967
Iteration 212/1000 | Loss: 0.00001966
Iteration 213/1000 | Loss: 0.00001966
Iteration 214/1000 | Loss: 0.00001966
Iteration 215/1000 | Loss: 0.00001966
Iteration 216/1000 | Loss: 0.00001966
Iteration 217/1000 | Loss: 0.00001966
Iteration 218/1000 | Loss: 0.00001966
Iteration 219/1000 | Loss: 0.00001965
Iteration 220/1000 | Loss: 0.00001965
Iteration 221/1000 | Loss: 0.00001965
Iteration 222/1000 | Loss: 0.00001965
Iteration 223/1000 | Loss: 0.00001965
Iteration 224/1000 | Loss: 0.00001965
Iteration 225/1000 | Loss: 0.00001965
Iteration 226/1000 | Loss: 0.00001965
Iteration 227/1000 | Loss: 0.00001965
Iteration 228/1000 | Loss: 0.00001965
Iteration 229/1000 | Loss: 0.00001965
Iteration 230/1000 | Loss: 0.00001965
Iteration 231/1000 | Loss: 0.00001965
Iteration 232/1000 | Loss: 0.00001965
Iteration 233/1000 | Loss: 0.00001965
Iteration 234/1000 | Loss: 0.00001965
Iteration 235/1000 | Loss: 0.00001965
Iteration 236/1000 | Loss: 0.00001965
Iteration 237/1000 | Loss: 0.00001965
Iteration 238/1000 | Loss: 0.00001965
Iteration 239/1000 | Loss: 0.00001965
Iteration 240/1000 | Loss: 0.00001965
Iteration 241/1000 | Loss: 0.00001965
Iteration 242/1000 | Loss: 0.00001965
Iteration 243/1000 | Loss: 0.00001965
Iteration 244/1000 | Loss: 0.00001965
Iteration 245/1000 | Loss: 0.00001965
Iteration 246/1000 | Loss: 0.00001965
Iteration 247/1000 | Loss: 0.00001965
Iteration 248/1000 | Loss: 0.00001965
Iteration 249/1000 | Loss: 0.00001965
Iteration 250/1000 | Loss: 0.00001965
Iteration 251/1000 | Loss: 0.00001965
Iteration 252/1000 | Loss: 0.00001965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 252. Stopping optimization.
Last 5 losses: [1.9649667592602782e-05, 1.9649667592602782e-05, 1.9649667592602782e-05, 1.9649667592602782e-05, 1.9649667592602782e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9649667592602782e-05

Optimization complete. Final v2v error: 3.4288783073425293 mm

Highest mean error: 10.722230911254883 mm for frame 144

Lowest mean error: 2.7337162494659424 mm for frame 18

Saving results

Total time: 162.610209941864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467408
Iteration 2/25 | Loss: 0.00146659
Iteration 3/25 | Loss: 0.00133252
Iteration 4/25 | Loss: 0.00132729
Iteration 5/25 | Loss: 0.00132699
Iteration 6/25 | Loss: 0.00132699
Iteration 7/25 | Loss: 0.00132699
Iteration 8/25 | Loss: 0.00132699
Iteration 9/25 | Loss: 0.00132699
Iteration 10/25 | Loss: 0.00132699
Iteration 11/25 | Loss: 0.00132699
Iteration 12/25 | Loss: 0.00132699
Iteration 13/25 | Loss: 0.00132699
Iteration 14/25 | Loss: 0.00132699
Iteration 15/25 | Loss: 0.00132699
Iteration 16/25 | Loss: 0.00132699
Iteration 17/25 | Loss: 0.00132699
Iteration 18/25 | Loss: 0.00132699
Iteration 19/25 | Loss: 0.00132699
Iteration 20/25 | Loss: 0.00132699
Iteration 21/25 | Loss: 0.00132699
Iteration 22/25 | Loss: 0.00132699
Iteration 23/25 | Loss: 0.00132699
Iteration 24/25 | Loss: 0.00132699
Iteration 25/25 | Loss: 0.00132699

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23260605
Iteration 2/25 | Loss: 0.00107187
Iteration 3/25 | Loss: 0.00107187
Iteration 4/25 | Loss: 0.00107187
Iteration 5/25 | Loss: 0.00107187
Iteration 6/25 | Loss: 0.00107187
Iteration 7/25 | Loss: 0.00107187
Iteration 8/25 | Loss: 0.00107187
Iteration 9/25 | Loss: 0.00107186
Iteration 10/25 | Loss: 0.00107186
Iteration 11/25 | Loss: 0.00107186
Iteration 12/25 | Loss: 0.00107186
Iteration 13/25 | Loss: 0.00107186
Iteration 14/25 | Loss: 0.00107186
Iteration 15/25 | Loss: 0.00107186
Iteration 16/25 | Loss: 0.00107186
Iteration 17/25 | Loss: 0.00107186
Iteration 18/25 | Loss: 0.00107186
Iteration 19/25 | Loss: 0.00107186
Iteration 20/25 | Loss: 0.00107186
Iteration 21/25 | Loss: 0.00107186
Iteration 22/25 | Loss: 0.00107186
Iteration 23/25 | Loss: 0.00107186
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001071864739060402, 0.001071864739060402, 0.001071864739060402, 0.001071864739060402, 0.001071864739060402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001071864739060402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107186
Iteration 2/1000 | Loss: 0.00003869
Iteration 3/1000 | Loss: 0.00002603
Iteration 4/1000 | Loss: 0.00002114
Iteration 5/1000 | Loss: 0.00001984
Iteration 6/1000 | Loss: 0.00001923
Iteration 7/1000 | Loss: 0.00001879
Iteration 8/1000 | Loss: 0.00001841
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001807
Iteration 11/1000 | Loss: 0.00001801
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001794
Iteration 14/1000 | Loss: 0.00001785
Iteration 15/1000 | Loss: 0.00001773
Iteration 16/1000 | Loss: 0.00001767
Iteration 17/1000 | Loss: 0.00001761
Iteration 18/1000 | Loss: 0.00001759
Iteration 19/1000 | Loss: 0.00001759
Iteration 20/1000 | Loss: 0.00001758
Iteration 21/1000 | Loss: 0.00001758
Iteration 22/1000 | Loss: 0.00001758
Iteration 23/1000 | Loss: 0.00001757
Iteration 24/1000 | Loss: 0.00001757
Iteration 25/1000 | Loss: 0.00001756
Iteration 26/1000 | Loss: 0.00001756
Iteration 27/1000 | Loss: 0.00001755
Iteration 28/1000 | Loss: 0.00001755
Iteration 29/1000 | Loss: 0.00001755
Iteration 30/1000 | Loss: 0.00001755
Iteration 31/1000 | Loss: 0.00001755
Iteration 32/1000 | Loss: 0.00001754
Iteration 33/1000 | Loss: 0.00001754
Iteration 34/1000 | Loss: 0.00001751
Iteration 35/1000 | Loss: 0.00001751
Iteration 36/1000 | Loss: 0.00001751
Iteration 37/1000 | Loss: 0.00001750
Iteration 38/1000 | Loss: 0.00001750
Iteration 39/1000 | Loss: 0.00001749
Iteration 40/1000 | Loss: 0.00001748
Iteration 41/1000 | Loss: 0.00001747
Iteration 42/1000 | Loss: 0.00001747
Iteration 43/1000 | Loss: 0.00001747
Iteration 44/1000 | Loss: 0.00001746
Iteration 45/1000 | Loss: 0.00001745
Iteration 46/1000 | Loss: 0.00001744
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00001743
Iteration 49/1000 | Loss: 0.00001743
Iteration 50/1000 | Loss: 0.00001743
Iteration 51/1000 | Loss: 0.00001742
Iteration 52/1000 | Loss: 0.00001741
Iteration 53/1000 | Loss: 0.00001740
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001738
Iteration 56/1000 | Loss: 0.00001738
Iteration 57/1000 | Loss: 0.00001738
Iteration 58/1000 | Loss: 0.00001738
Iteration 59/1000 | Loss: 0.00001738
Iteration 60/1000 | Loss: 0.00001738
Iteration 61/1000 | Loss: 0.00001737
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001734
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001733
Iteration 69/1000 | Loss: 0.00001733
Iteration 70/1000 | Loss: 0.00001733
Iteration 71/1000 | Loss: 0.00001732
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001731
Iteration 75/1000 | Loss: 0.00001731
Iteration 76/1000 | Loss: 0.00001731
Iteration 77/1000 | Loss: 0.00001731
Iteration 78/1000 | Loss: 0.00001731
Iteration 79/1000 | Loss: 0.00001730
Iteration 80/1000 | Loss: 0.00001730
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001730
Iteration 85/1000 | Loss: 0.00001730
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001728
Iteration 93/1000 | Loss: 0.00001727
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001726
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001726
Iteration 99/1000 | Loss: 0.00001726
Iteration 100/1000 | Loss: 0.00001726
Iteration 101/1000 | Loss: 0.00001726
Iteration 102/1000 | Loss: 0.00001726
Iteration 103/1000 | Loss: 0.00001726
Iteration 104/1000 | Loss: 0.00001725
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001724
Iteration 110/1000 | Loss: 0.00001724
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001723
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001723
Iteration 115/1000 | Loss: 0.00001723
Iteration 116/1000 | Loss: 0.00001723
Iteration 117/1000 | Loss: 0.00001723
Iteration 118/1000 | Loss: 0.00001723
Iteration 119/1000 | Loss: 0.00001723
Iteration 120/1000 | Loss: 0.00001723
Iteration 121/1000 | Loss: 0.00001722
Iteration 122/1000 | Loss: 0.00001722
Iteration 123/1000 | Loss: 0.00001722
Iteration 124/1000 | Loss: 0.00001722
Iteration 125/1000 | Loss: 0.00001722
Iteration 126/1000 | Loss: 0.00001722
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001721
Iteration 131/1000 | Loss: 0.00001721
Iteration 132/1000 | Loss: 0.00001721
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001719
Iteration 138/1000 | Loss: 0.00001719
Iteration 139/1000 | Loss: 0.00001719
Iteration 140/1000 | Loss: 0.00001719
Iteration 141/1000 | Loss: 0.00001719
Iteration 142/1000 | Loss: 0.00001719
Iteration 143/1000 | Loss: 0.00001719
Iteration 144/1000 | Loss: 0.00001719
Iteration 145/1000 | Loss: 0.00001719
Iteration 146/1000 | Loss: 0.00001719
Iteration 147/1000 | Loss: 0.00001719
Iteration 148/1000 | Loss: 0.00001719
Iteration 149/1000 | Loss: 0.00001719
Iteration 150/1000 | Loss: 0.00001719
Iteration 151/1000 | Loss: 0.00001719
Iteration 152/1000 | Loss: 0.00001719
Iteration 153/1000 | Loss: 0.00001718
Iteration 154/1000 | Loss: 0.00001718
Iteration 155/1000 | Loss: 0.00001718
Iteration 156/1000 | Loss: 0.00001718
Iteration 157/1000 | Loss: 0.00001718
Iteration 158/1000 | Loss: 0.00001718
Iteration 159/1000 | Loss: 0.00001718
Iteration 160/1000 | Loss: 0.00001718
Iteration 161/1000 | Loss: 0.00001718
Iteration 162/1000 | Loss: 0.00001718
Iteration 163/1000 | Loss: 0.00001718
Iteration 164/1000 | Loss: 0.00001718
Iteration 165/1000 | Loss: 0.00001718
Iteration 166/1000 | Loss: 0.00001718
Iteration 167/1000 | Loss: 0.00001718
Iteration 168/1000 | Loss: 0.00001718
Iteration 169/1000 | Loss: 0.00001718
Iteration 170/1000 | Loss: 0.00001718
Iteration 171/1000 | Loss: 0.00001718
Iteration 172/1000 | Loss: 0.00001718
Iteration 173/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 173. Stopping optimization.
Last 5 losses: [1.717828490654938e-05, 1.717828490654938e-05, 1.717828490654938e-05, 1.717828490654938e-05, 1.717828490654938e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.717828490654938e-05

Optimization complete. Final v2v error: 3.4674580097198486 mm

Highest mean error: 3.7693371772766113 mm for frame 41

Lowest mean error: 3.2377777099609375 mm for frame 70

Saving results

Total time: 36.67305374145508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00758018
Iteration 2/25 | Loss: 0.00165931
Iteration 3/25 | Loss: 0.00147979
Iteration 4/25 | Loss: 0.00144300
Iteration 5/25 | Loss: 0.00143659
Iteration 6/25 | Loss: 0.00143474
Iteration 7/25 | Loss: 0.00143429
Iteration 8/25 | Loss: 0.00143429
Iteration 9/25 | Loss: 0.00143429
Iteration 10/25 | Loss: 0.00143429
Iteration 11/25 | Loss: 0.00143429
Iteration 12/25 | Loss: 0.00143429
Iteration 13/25 | Loss: 0.00143429
Iteration 14/25 | Loss: 0.00143429
Iteration 15/25 | Loss: 0.00143429
Iteration 16/25 | Loss: 0.00143429
Iteration 17/25 | Loss: 0.00143429
Iteration 18/25 | Loss: 0.00143429
Iteration 19/25 | Loss: 0.00143429
Iteration 20/25 | Loss: 0.00143429
Iteration 21/25 | Loss: 0.00143429
Iteration 22/25 | Loss: 0.00143429
Iteration 23/25 | Loss: 0.00143429
Iteration 24/25 | Loss: 0.00143429
Iteration 25/25 | Loss: 0.00143429

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19205618
Iteration 2/25 | Loss: 0.00118398
Iteration 3/25 | Loss: 0.00118398
Iteration 4/25 | Loss: 0.00118398
Iteration 5/25 | Loss: 0.00118398
Iteration 6/25 | Loss: 0.00118398
Iteration 7/25 | Loss: 0.00118398
Iteration 8/25 | Loss: 0.00118398
Iteration 9/25 | Loss: 0.00118398
Iteration 10/25 | Loss: 0.00118398
Iteration 11/25 | Loss: 0.00118398
Iteration 12/25 | Loss: 0.00118398
Iteration 13/25 | Loss: 0.00118398
Iteration 14/25 | Loss: 0.00118398
Iteration 15/25 | Loss: 0.00118398
Iteration 16/25 | Loss: 0.00118398
Iteration 17/25 | Loss: 0.00118398
Iteration 18/25 | Loss: 0.00118398
Iteration 19/25 | Loss: 0.00118398
Iteration 20/25 | Loss: 0.00118398
Iteration 21/25 | Loss: 0.00118398
Iteration 22/25 | Loss: 0.00118398
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011839752551168203, 0.0011839752551168203, 0.0011839752551168203, 0.0011839752551168203, 0.0011839752551168203]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011839752551168203

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118398
Iteration 2/1000 | Loss: 0.00010753
Iteration 3/1000 | Loss: 0.00005764
Iteration 4/1000 | Loss: 0.00004784
Iteration 5/1000 | Loss: 0.00004494
Iteration 6/1000 | Loss: 0.00004341
Iteration 7/1000 | Loss: 0.00004203
Iteration 8/1000 | Loss: 0.00004089
Iteration 9/1000 | Loss: 0.00003964
Iteration 10/1000 | Loss: 0.00003883
Iteration 11/1000 | Loss: 0.00003820
Iteration 12/1000 | Loss: 0.00003770
Iteration 13/1000 | Loss: 0.00003735
Iteration 14/1000 | Loss: 0.00003704
Iteration 15/1000 | Loss: 0.00003675
Iteration 16/1000 | Loss: 0.00003651
Iteration 17/1000 | Loss: 0.00003648
Iteration 18/1000 | Loss: 0.00003647
Iteration 19/1000 | Loss: 0.00003631
Iteration 20/1000 | Loss: 0.00003629
Iteration 21/1000 | Loss: 0.00003626
Iteration 22/1000 | Loss: 0.00003625
Iteration 23/1000 | Loss: 0.00003624
Iteration 24/1000 | Loss: 0.00003624
Iteration 25/1000 | Loss: 0.00003624
Iteration 26/1000 | Loss: 0.00003624
Iteration 27/1000 | Loss: 0.00003624
Iteration 28/1000 | Loss: 0.00003624
Iteration 29/1000 | Loss: 0.00003623
Iteration 30/1000 | Loss: 0.00003623
Iteration 31/1000 | Loss: 0.00003622
Iteration 32/1000 | Loss: 0.00003622
Iteration 33/1000 | Loss: 0.00003621
Iteration 34/1000 | Loss: 0.00003619
Iteration 35/1000 | Loss: 0.00003619
Iteration 36/1000 | Loss: 0.00003618
Iteration 37/1000 | Loss: 0.00003617
Iteration 38/1000 | Loss: 0.00003617
Iteration 39/1000 | Loss: 0.00003616
Iteration 40/1000 | Loss: 0.00003616
Iteration 41/1000 | Loss: 0.00003615
Iteration 42/1000 | Loss: 0.00003615
Iteration 43/1000 | Loss: 0.00003614
Iteration 44/1000 | Loss: 0.00003614
Iteration 45/1000 | Loss: 0.00003614
Iteration 46/1000 | Loss: 0.00003613
Iteration 47/1000 | Loss: 0.00003613
Iteration 48/1000 | Loss: 0.00003613
Iteration 49/1000 | Loss: 0.00003612
Iteration 50/1000 | Loss: 0.00003612
Iteration 51/1000 | Loss: 0.00003612
Iteration 52/1000 | Loss: 0.00003612
Iteration 53/1000 | Loss: 0.00003612
Iteration 54/1000 | Loss: 0.00003612
Iteration 55/1000 | Loss: 0.00003612
Iteration 56/1000 | Loss: 0.00003612
Iteration 57/1000 | Loss: 0.00003612
Iteration 58/1000 | Loss: 0.00003612
Iteration 59/1000 | Loss: 0.00003612
Iteration 60/1000 | Loss: 0.00003611
Iteration 61/1000 | Loss: 0.00003611
Iteration 62/1000 | Loss: 0.00003611
Iteration 63/1000 | Loss: 0.00003611
Iteration 64/1000 | Loss: 0.00003610
Iteration 65/1000 | Loss: 0.00003610
Iteration 66/1000 | Loss: 0.00003610
Iteration 67/1000 | Loss: 0.00003610
Iteration 68/1000 | Loss: 0.00003609
Iteration 69/1000 | Loss: 0.00003609
Iteration 70/1000 | Loss: 0.00003609
Iteration 71/1000 | Loss: 0.00003608
Iteration 72/1000 | Loss: 0.00003608
Iteration 73/1000 | Loss: 0.00003608
Iteration 74/1000 | Loss: 0.00003608
Iteration 75/1000 | Loss: 0.00003607
Iteration 76/1000 | Loss: 0.00003607
Iteration 77/1000 | Loss: 0.00003607
Iteration 78/1000 | Loss: 0.00003606
Iteration 79/1000 | Loss: 0.00003606
Iteration 80/1000 | Loss: 0.00003606
Iteration 81/1000 | Loss: 0.00003606
Iteration 82/1000 | Loss: 0.00003605
Iteration 83/1000 | Loss: 0.00003605
Iteration 84/1000 | Loss: 0.00003605
Iteration 85/1000 | Loss: 0.00003605
Iteration 86/1000 | Loss: 0.00003604
Iteration 87/1000 | Loss: 0.00003604
Iteration 88/1000 | Loss: 0.00003604
Iteration 89/1000 | Loss: 0.00003604
Iteration 90/1000 | Loss: 0.00003604
Iteration 91/1000 | Loss: 0.00003604
Iteration 92/1000 | Loss: 0.00003604
Iteration 93/1000 | Loss: 0.00003603
Iteration 94/1000 | Loss: 0.00003603
Iteration 95/1000 | Loss: 0.00003603
Iteration 96/1000 | Loss: 0.00003603
Iteration 97/1000 | Loss: 0.00003603
Iteration 98/1000 | Loss: 0.00003603
Iteration 99/1000 | Loss: 0.00003603
Iteration 100/1000 | Loss: 0.00003603
Iteration 101/1000 | Loss: 0.00003602
Iteration 102/1000 | Loss: 0.00003602
Iteration 103/1000 | Loss: 0.00003602
Iteration 104/1000 | Loss: 0.00003602
Iteration 105/1000 | Loss: 0.00003601
Iteration 106/1000 | Loss: 0.00003601
Iteration 107/1000 | Loss: 0.00003601
Iteration 108/1000 | Loss: 0.00003601
Iteration 109/1000 | Loss: 0.00003601
Iteration 110/1000 | Loss: 0.00003601
Iteration 111/1000 | Loss: 0.00003601
Iteration 112/1000 | Loss: 0.00003601
Iteration 113/1000 | Loss: 0.00003601
Iteration 114/1000 | Loss: 0.00003601
Iteration 115/1000 | Loss: 0.00003601
Iteration 116/1000 | Loss: 0.00003600
Iteration 117/1000 | Loss: 0.00003600
Iteration 118/1000 | Loss: 0.00003600
Iteration 119/1000 | Loss: 0.00003600
Iteration 120/1000 | Loss: 0.00003600
Iteration 121/1000 | Loss: 0.00003600
Iteration 122/1000 | Loss: 0.00003600
Iteration 123/1000 | Loss: 0.00003599
Iteration 124/1000 | Loss: 0.00003599
Iteration 125/1000 | Loss: 0.00003599
Iteration 126/1000 | Loss: 0.00003599
Iteration 127/1000 | Loss: 0.00003598
Iteration 128/1000 | Loss: 0.00003598
Iteration 129/1000 | Loss: 0.00003598
Iteration 130/1000 | Loss: 0.00003598
Iteration 131/1000 | Loss: 0.00003598
Iteration 132/1000 | Loss: 0.00003598
Iteration 133/1000 | Loss: 0.00003597
Iteration 134/1000 | Loss: 0.00003597
Iteration 135/1000 | Loss: 0.00003597
Iteration 136/1000 | Loss: 0.00003597
Iteration 137/1000 | Loss: 0.00003597
Iteration 138/1000 | Loss: 0.00003596
Iteration 139/1000 | Loss: 0.00003596
Iteration 140/1000 | Loss: 0.00003596
Iteration 141/1000 | Loss: 0.00003596
Iteration 142/1000 | Loss: 0.00003596
Iteration 143/1000 | Loss: 0.00003595
Iteration 144/1000 | Loss: 0.00003595
Iteration 145/1000 | Loss: 0.00003595
Iteration 146/1000 | Loss: 0.00003595
Iteration 147/1000 | Loss: 0.00003595
Iteration 148/1000 | Loss: 0.00003595
Iteration 149/1000 | Loss: 0.00003595
Iteration 150/1000 | Loss: 0.00003595
Iteration 151/1000 | Loss: 0.00003595
Iteration 152/1000 | Loss: 0.00003595
Iteration 153/1000 | Loss: 0.00003595
Iteration 154/1000 | Loss: 0.00003595
Iteration 155/1000 | Loss: 0.00003595
Iteration 156/1000 | Loss: 0.00003595
Iteration 157/1000 | Loss: 0.00003595
Iteration 158/1000 | Loss: 0.00003594
Iteration 159/1000 | Loss: 0.00003594
Iteration 160/1000 | Loss: 0.00003594
Iteration 161/1000 | Loss: 0.00003594
Iteration 162/1000 | Loss: 0.00003594
Iteration 163/1000 | Loss: 0.00003594
Iteration 164/1000 | Loss: 0.00003594
Iteration 165/1000 | Loss: 0.00003594
Iteration 166/1000 | Loss: 0.00003594
Iteration 167/1000 | Loss: 0.00003594
Iteration 168/1000 | Loss: 0.00003594
Iteration 169/1000 | Loss: 0.00003594
Iteration 170/1000 | Loss: 0.00003594
Iteration 171/1000 | Loss: 0.00003594
Iteration 172/1000 | Loss: 0.00003594
Iteration 173/1000 | Loss: 0.00003594
Iteration 174/1000 | Loss: 0.00003594
Iteration 175/1000 | Loss: 0.00003594
Iteration 176/1000 | Loss: 0.00003594
Iteration 177/1000 | Loss: 0.00003594
Iteration 178/1000 | Loss: 0.00003594
Iteration 179/1000 | Loss: 0.00003594
Iteration 180/1000 | Loss: 0.00003594
Iteration 181/1000 | Loss: 0.00003594
Iteration 182/1000 | Loss: 0.00003594
Iteration 183/1000 | Loss: 0.00003594
Iteration 184/1000 | Loss: 0.00003594
Iteration 185/1000 | Loss: 0.00003594
Iteration 186/1000 | Loss: 0.00003594
Iteration 187/1000 | Loss: 0.00003594
Iteration 188/1000 | Loss: 0.00003594
Iteration 189/1000 | Loss: 0.00003594
Iteration 190/1000 | Loss: 0.00003594
Iteration 191/1000 | Loss: 0.00003594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [3.594296867959201e-05, 3.594296867959201e-05, 3.594296867959201e-05, 3.594296867959201e-05, 3.594296867959201e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.594296867959201e-05

Optimization complete. Final v2v error: 4.962118625640869 mm

Highest mean error: 5.78031587600708 mm for frame 105

Lowest mean error: 3.9625298976898193 mm for frame 14

Saving results

Total time: 45.204238176345825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_42_us_1177/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_42_us_1177/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00411671
Iteration 2/25 | Loss: 0.00141979
Iteration 3/25 | Loss: 0.00125770
Iteration 4/25 | Loss: 0.00124440
Iteration 5/25 | Loss: 0.00124154
Iteration 6/25 | Loss: 0.00124069
Iteration 7/25 | Loss: 0.00124069
Iteration 8/25 | Loss: 0.00124069
Iteration 9/25 | Loss: 0.00124069
Iteration 10/25 | Loss: 0.00124069
Iteration 11/25 | Loss: 0.00124069
Iteration 12/25 | Loss: 0.00124069
Iteration 13/25 | Loss: 0.00124069
Iteration 14/25 | Loss: 0.00124069
Iteration 15/25 | Loss: 0.00124069
Iteration 16/25 | Loss: 0.00124069
Iteration 17/25 | Loss: 0.00124069
Iteration 18/25 | Loss: 0.00124069
Iteration 19/25 | Loss: 0.00124069
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012406876776367426, 0.0012406876776367426, 0.0012406876776367426, 0.0012406876776367426, 0.0012406876776367426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012406876776367426

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26256943
Iteration 2/25 | Loss: 0.00137687
Iteration 3/25 | Loss: 0.00137687
Iteration 4/25 | Loss: 0.00137687
Iteration 5/25 | Loss: 0.00137687
Iteration 6/25 | Loss: 0.00137687
Iteration 7/25 | Loss: 0.00137687
Iteration 8/25 | Loss: 0.00137687
Iteration 9/25 | Loss: 0.00137687
Iteration 10/25 | Loss: 0.00137687
Iteration 11/25 | Loss: 0.00137687
Iteration 12/25 | Loss: 0.00137687
Iteration 13/25 | Loss: 0.00137687
Iteration 14/25 | Loss: 0.00137687
Iteration 15/25 | Loss: 0.00137687
Iteration 16/25 | Loss: 0.00137687
Iteration 17/25 | Loss: 0.00137687
Iteration 18/25 | Loss: 0.00137687
Iteration 19/25 | Loss: 0.00137687
Iteration 20/25 | Loss: 0.00137687
Iteration 21/25 | Loss: 0.00137687
Iteration 22/25 | Loss: 0.00137687
Iteration 23/25 | Loss: 0.00137687
Iteration 24/25 | Loss: 0.00137687
Iteration 25/25 | Loss: 0.00137687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137687
Iteration 2/1000 | Loss: 0.00005193
Iteration 3/1000 | Loss: 0.00002853
Iteration 4/1000 | Loss: 0.00002251
Iteration 5/1000 | Loss: 0.00001991
Iteration 6/1000 | Loss: 0.00001906
Iteration 7/1000 | Loss: 0.00001842
Iteration 8/1000 | Loss: 0.00001785
Iteration 9/1000 | Loss: 0.00001753
Iteration 10/1000 | Loss: 0.00001728
Iteration 11/1000 | Loss: 0.00001725
Iteration 12/1000 | Loss: 0.00001712
Iteration 13/1000 | Loss: 0.00001692
Iteration 14/1000 | Loss: 0.00001675
Iteration 15/1000 | Loss: 0.00001656
Iteration 16/1000 | Loss: 0.00001652
Iteration 17/1000 | Loss: 0.00001650
Iteration 18/1000 | Loss: 0.00001642
Iteration 19/1000 | Loss: 0.00001639
Iteration 20/1000 | Loss: 0.00001639
Iteration 21/1000 | Loss: 0.00001639
Iteration 22/1000 | Loss: 0.00001639
Iteration 23/1000 | Loss: 0.00001638
Iteration 24/1000 | Loss: 0.00001637
Iteration 25/1000 | Loss: 0.00001637
Iteration 26/1000 | Loss: 0.00001637
Iteration 27/1000 | Loss: 0.00001637
Iteration 28/1000 | Loss: 0.00001636
Iteration 29/1000 | Loss: 0.00001636
Iteration 30/1000 | Loss: 0.00001636
Iteration 31/1000 | Loss: 0.00001636
Iteration 32/1000 | Loss: 0.00001635
Iteration 33/1000 | Loss: 0.00001635
Iteration 34/1000 | Loss: 0.00001634
Iteration 35/1000 | Loss: 0.00001634
Iteration 36/1000 | Loss: 0.00001633
Iteration 37/1000 | Loss: 0.00001633
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001632
Iteration 41/1000 | Loss: 0.00001632
Iteration 42/1000 | Loss: 0.00001631
Iteration 43/1000 | Loss: 0.00001631
Iteration 44/1000 | Loss: 0.00001631
Iteration 45/1000 | Loss: 0.00001631
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001630
Iteration 48/1000 | Loss: 0.00001630
Iteration 49/1000 | Loss: 0.00001630
Iteration 50/1000 | Loss: 0.00001630
Iteration 51/1000 | Loss: 0.00001630
Iteration 52/1000 | Loss: 0.00001630
Iteration 53/1000 | Loss: 0.00001630
Iteration 54/1000 | Loss: 0.00001630
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001630
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001630
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001627
Iteration 61/1000 | Loss: 0.00001627
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001626
Iteration 65/1000 | Loss: 0.00001626
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001624
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001622
Iteration 81/1000 | Loss: 0.00001622
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001621
Iteration 87/1000 | Loss: 0.00001621
Iteration 88/1000 | Loss: 0.00001621
Iteration 89/1000 | Loss: 0.00001621
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001620
Iteration 95/1000 | Loss: 0.00001620
Iteration 96/1000 | Loss: 0.00001620
Iteration 97/1000 | Loss: 0.00001620
Iteration 98/1000 | Loss: 0.00001619
Iteration 99/1000 | Loss: 0.00001619
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001619
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001619
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Iteration 110/1000 | Loss: 0.00001618
Iteration 111/1000 | Loss: 0.00001618
Iteration 112/1000 | Loss: 0.00001618
Iteration 113/1000 | Loss: 0.00001618
Iteration 114/1000 | Loss: 0.00001618
Iteration 115/1000 | Loss: 0.00001618
Iteration 116/1000 | Loss: 0.00001617
Iteration 117/1000 | Loss: 0.00001617
Iteration 118/1000 | Loss: 0.00001617
Iteration 119/1000 | Loss: 0.00001617
Iteration 120/1000 | Loss: 0.00001617
Iteration 121/1000 | Loss: 0.00001617
Iteration 122/1000 | Loss: 0.00001617
Iteration 123/1000 | Loss: 0.00001616
Iteration 124/1000 | Loss: 0.00001616
Iteration 125/1000 | Loss: 0.00001616
Iteration 126/1000 | Loss: 0.00001616
Iteration 127/1000 | Loss: 0.00001616
Iteration 128/1000 | Loss: 0.00001616
Iteration 129/1000 | Loss: 0.00001616
Iteration 130/1000 | Loss: 0.00001616
Iteration 131/1000 | Loss: 0.00001616
Iteration 132/1000 | Loss: 0.00001616
Iteration 133/1000 | Loss: 0.00001616
Iteration 134/1000 | Loss: 0.00001616
Iteration 135/1000 | Loss: 0.00001616
Iteration 136/1000 | Loss: 0.00001616
Iteration 137/1000 | Loss: 0.00001616
Iteration 138/1000 | Loss: 0.00001616
Iteration 139/1000 | Loss: 0.00001615
Iteration 140/1000 | Loss: 0.00001615
Iteration 141/1000 | Loss: 0.00001615
Iteration 142/1000 | Loss: 0.00001615
Iteration 143/1000 | Loss: 0.00001615
Iteration 144/1000 | Loss: 0.00001615
Iteration 145/1000 | Loss: 0.00001615
Iteration 146/1000 | Loss: 0.00001615
Iteration 147/1000 | Loss: 0.00001615
Iteration 148/1000 | Loss: 0.00001615
Iteration 149/1000 | Loss: 0.00001615
Iteration 150/1000 | Loss: 0.00001615
Iteration 151/1000 | Loss: 0.00001615
Iteration 152/1000 | Loss: 0.00001615
Iteration 153/1000 | Loss: 0.00001615
Iteration 154/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.6148795111803338e-05, 1.6148795111803338e-05, 1.6148795111803338e-05, 1.6148795111803338e-05, 1.6148795111803338e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6148795111803338e-05

Optimization complete. Final v2v error: 3.371917963027954 mm

Highest mean error: 4.756913185119629 mm for frame 39

Lowest mean error: 3.0309150218963623 mm for frame 63

Saving results

Total time: 39.869534969329834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00553584
Iteration 2/25 | Loss: 0.00166161
Iteration 3/25 | Loss: 0.00132435
Iteration 4/25 | Loss: 0.00128059
Iteration 5/25 | Loss: 0.00126786
Iteration 6/25 | Loss: 0.00126218
Iteration 7/25 | Loss: 0.00126151
Iteration 8/25 | Loss: 0.00126151
Iteration 9/25 | Loss: 0.00126151
Iteration 10/25 | Loss: 0.00126151
Iteration 11/25 | Loss: 0.00126151
Iteration 12/25 | Loss: 0.00126151
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012615143787115812, 0.0012615143787115812, 0.0012615143787115812, 0.0012615143787115812, 0.0012615143787115812]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012615143787115812

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15231836
Iteration 2/25 | Loss: 0.00086051
Iteration 3/25 | Loss: 0.00086049
Iteration 4/25 | Loss: 0.00086048
Iteration 5/25 | Loss: 0.00086048
Iteration 6/25 | Loss: 0.00086048
Iteration 7/25 | Loss: 0.00086048
Iteration 8/25 | Loss: 0.00086048
Iteration 9/25 | Loss: 0.00086048
Iteration 10/25 | Loss: 0.00086048
Iteration 11/25 | Loss: 0.00086048
Iteration 12/25 | Loss: 0.00086048
Iteration 13/25 | Loss: 0.00086048
Iteration 14/25 | Loss: 0.00086048
Iteration 15/25 | Loss: 0.00086048
Iteration 16/25 | Loss: 0.00086048
Iteration 17/25 | Loss: 0.00086048
Iteration 18/25 | Loss: 0.00086048
Iteration 19/25 | Loss: 0.00086048
Iteration 20/25 | Loss: 0.00086048
Iteration 21/25 | Loss: 0.00086048
Iteration 22/25 | Loss: 0.00086048
Iteration 23/25 | Loss: 0.00086048
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008604826289229095, 0.0008604826289229095, 0.0008604826289229095, 0.0008604826289229095, 0.0008604826289229095]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008604826289229095

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086048
Iteration 2/1000 | Loss: 0.00006638
Iteration 3/1000 | Loss: 0.00004759
Iteration 4/1000 | Loss: 0.00004174
Iteration 5/1000 | Loss: 0.00003856
Iteration 6/1000 | Loss: 0.00003630
Iteration 7/1000 | Loss: 0.00003518
Iteration 8/1000 | Loss: 0.00003454
Iteration 9/1000 | Loss: 0.00003405
Iteration 10/1000 | Loss: 0.00003369
Iteration 11/1000 | Loss: 0.00003310
Iteration 12/1000 | Loss: 0.00003277
Iteration 13/1000 | Loss: 0.00003259
Iteration 14/1000 | Loss: 0.00003246
Iteration 15/1000 | Loss: 0.00003238
Iteration 16/1000 | Loss: 0.00003227
Iteration 17/1000 | Loss: 0.00003225
Iteration 18/1000 | Loss: 0.00003219
Iteration 19/1000 | Loss: 0.00003208
Iteration 20/1000 | Loss: 0.00003205
Iteration 21/1000 | Loss: 0.00003204
Iteration 22/1000 | Loss: 0.00003204
Iteration 23/1000 | Loss: 0.00003204
Iteration 24/1000 | Loss: 0.00003204
Iteration 25/1000 | Loss: 0.00003204
Iteration 26/1000 | Loss: 0.00003204
Iteration 27/1000 | Loss: 0.00003204
Iteration 28/1000 | Loss: 0.00003202
Iteration 29/1000 | Loss: 0.00003202
Iteration 30/1000 | Loss: 0.00003202
Iteration 31/1000 | Loss: 0.00003202
Iteration 32/1000 | Loss: 0.00003202
Iteration 33/1000 | Loss: 0.00003202
Iteration 34/1000 | Loss: 0.00003202
Iteration 35/1000 | Loss: 0.00003202
Iteration 36/1000 | Loss: 0.00003201
Iteration 37/1000 | Loss: 0.00003201
Iteration 38/1000 | Loss: 0.00003201
Iteration 39/1000 | Loss: 0.00003201
Iteration 40/1000 | Loss: 0.00003201
Iteration 41/1000 | Loss: 0.00003201
Iteration 42/1000 | Loss: 0.00003201
Iteration 43/1000 | Loss: 0.00003201
Iteration 44/1000 | Loss: 0.00003201
Iteration 45/1000 | Loss: 0.00003200
Iteration 46/1000 | Loss: 0.00003200
Iteration 47/1000 | Loss: 0.00003199
Iteration 48/1000 | Loss: 0.00003199
Iteration 49/1000 | Loss: 0.00003199
Iteration 50/1000 | Loss: 0.00003199
Iteration 51/1000 | Loss: 0.00003198
Iteration 52/1000 | Loss: 0.00003198
Iteration 53/1000 | Loss: 0.00003198
Iteration 54/1000 | Loss: 0.00003197
Iteration 55/1000 | Loss: 0.00003197
Iteration 56/1000 | Loss: 0.00003196
Iteration 57/1000 | Loss: 0.00003196
Iteration 58/1000 | Loss: 0.00003196
Iteration 59/1000 | Loss: 0.00003196
Iteration 60/1000 | Loss: 0.00003195
Iteration 61/1000 | Loss: 0.00003195
Iteration 62/1000 | Loss: 0.00003195
Iteration 63/1000 | Loss: 0.00003195
Iteration 64/1000 | Loss: 0.00003194
Iteration 65/1000 | Loss: 0.00003194
Iteration 66/1000 | Loss: 0.00003194
Iteration 67/1000 | Loss: 0.00003194
Iteration 68/1000 | Loss: 0.00003194
Iteration 69/1000 | Loss: 0.00003194
Iteration 70/1000 | Loss: 0.00003194
Iteration 71/1000 | Loss: 0.00003194
Iteration 72/1000 | Loss: 0.00003194
Iteration 73/1000 | Loss: 0.00003194
Iteration 74/1000 | Loss: 0.00003194
Iteration 75/1000 | Loss: 0.00003193
Iteration 76/1000 | Loss: 0.00003193
Iteration 77/1000 | Loss: 0.00003193
Iteration 78/1000 | Loss: 0.00003193
Iteration 79/1000 | Loss: 0.00003193
Iteration 80/1000 | Loss: 0.00003192
Iteration 81/1000 | Loss: 0.00003192
Iteration 82/1000 | Loss: 0.00003192
Iteration 83/1000 | Loss: 0.00003192
Iteration 84/1000 | Loss: 0.00003192
Iteration 85/1000 | Loss: 0.00003192
Iteration 86/1000 | Loss: 0.00003192
Iteration 87/1000 | Loss: 0.00003192
Iteration 88/1000 | Loss: 0.00003192
Iteration 89/1000 | Loss: 0.00003192
Iteration 90/1000 | Loss: 0.00003192
Iteration 91/1000 | Loss: 0.00003191
Iteration 92/1000 | Loss: 0.00003191
Iteration 93/1000 | Loss: 0.00003191
Iteration 94/1000 | Loss: 0.00003190
Iteration 95/1000 | Loss: 0.00003190
Iteration 96/1000 | Loss: 0.00003190
Iteration 97/1000 | Loss: 0.00003190
Iteration 98/1000 | Loss: 0.00003190
Iteration 99/1000 | Loss: 0.00003190
Iteration 100/1000 | Loss: 0.00003190
Iteration 101/1000 | Loss: 0.00003189
Iteration 102/1000 | Loss: 0.00003189
Iteration 103/1000 | Loss: 0.00003189
Iteration 104/1000 | Loss: 0.00003189
Iteration 105/1000 | Loss: 0.00003189
Iteration 106/1000 | Loss: 0.00003189
Iteration 107/1000 | Loss: 0.00003189
Iteration 108/1000 | Loss: 0.00003189
Iteration 109/1000 | Loss: 0.00003189
Iteration 110/1000 | Loss: 0.00003189
Iteration 111/1000 | Loss: 0.00003189
Iteration 112/1000 | Loss: 0.00003189
Iteration 113/1000 | Loss: 0.00003189
Iteration 114/1000 | Loss: 0.00003189
Iteration 115/1000 | Loss: 0.00003189
Iteration 116/1000 | Loss: 0.00003188
Iteration 117/1000 | Loss: 0.00003188
Iteration 118/1000 | Loss: 0.00003188
Iteration 119/1000 | Loss: 0.00003188
Iteration 120/1000 | Loss: 0.00003188
Iteration 121/1000 | Loss: 0.00003188
Iteration 122/1000 | Loss: 0.00003187
Iteration 123/1000 | Loss: 0.00003187
Iteration 124/1000 | Loss: 0.00003187
Iteration 125/1000 | Loss: 0.00003186
Iteration 126/1000 | Loss: 0.00003186
Iteration 127/1000 | Loss: 0.00003186
Iteration 128/1000 | Loss: 0.00003186
Iteration 129/1000 | Loss: 0.00003186
Iteration 130/1000 | Loss: 0.00003186
Iteration 131/1000 | Loss: 0.00003186
Iteration 132/1000 | Loss: 0.00003186
Iteration 133/1000 | Loss: 0.00003185
Iteration 134/1000 | Loss: 0.00003185
Iteration 135/1000 | Loss: 0.00003185
Iteration 136/1000 | Loss: 0.00003185
Iteration 137/1000 | Loss: 0.00003185
Iteration 138/1000 | Loss: 0.00003185
Iteration 139/1000 | Loss: 0.00003185
Iteration 140/1000 | Loss: 0.00003184
Iteration 141/1000 | Loss: 0.00003184
Iteration 142/1000 | Loss: 0.00003184
Iteration 143/1000 | Loss: 0.00003184
Iteration 144/1000 | Loss: 0.00003184
Iteration 145/1000 | Loss: 0.00003183
Iteration 146/1000 | Loss: 0.00003183
Iteration 147/1000 | Loss: 0.00003183
Iteration 148/1000 | Loss: 0.00003183
Iteration 149/1000 | Loss: 0.00003183
Iteration 150/1000 | Loss: 0.00003183
Iteration 151/1000 | Loss: 0.00003183
Iteration 152/1000 | Loss: 0.00003183
Iteration 153/1000 | Loss: 0.00003183
Iteration 154/1000 | Loss: 0.00003183
Iteration 155/1000 | Loss: 0.00003183
Iteration 156/1000 | Loss: 0.00003183
Iteration 157/1000 | Loss: 0.00003183
Iteration 158/1000 | Loss: 0.00003183
Iteration 159/1000 | Loss: 0.00003183
Iteration 160/1000 | Loss: 0.00003182
Iteration 161/1000 | Loss: 0.00003182
Iteration 162/1000 | Loss: 0.00003182
Iteration 163/1000 | Loss: 0.00003182
Iteration 164/1000 | Loss: 0.00003182
Iteration 165/1000 | Loss: 0.00003182
Iteration 166/1000 | Loss: 0.00003182
Iteration 167/1000 | Loss: 0.00003182
Iteration 168/1000 | Loss: 0.00003182
Iteration 169/1000 | Loss: 0.00003182
Iteration 170/1000 | Loss: 0.00003182
Iteration 171/1000 | Loss: 0.00003182
Iteration 172/1000 | Loss: 0.00003182
Iteration 173/1000 | Loss: 0.00003181
Iteration 174/1000 | Loss: 0.00003181
Iteration 175/1000 | Loss: 0.00003181
Iteration 176/1000 | Loss: 0.00003181
Iteration 177/1000 | Loss: 0.00003181
Iteration 178/1000 | Loss: 0.00003181
Iteration 179/1000 | Loss: 0.00003181
Iteration 180/1000 | Loss: 0.00003181
Iteration 181/1000 | Loss: 0.00003181
Iteration 182/1000 | Loss: 0.00003181
Iteration 183/1000 | Loss: 0.00003180
Iteration 184/1000 | Loss: 0.00003180
Iteration 185/1000 | Loss: 0.00003180
Iteration 186/1000 | Loss: 0.00003180
Iteration 187/1000 | Loss: 0.00003180
Iteration 188/1000 | Loss: 0.00003180
Iteration 189/1000 | Loss: 0.00003180
Iteration 190/1000 | Loss: 0.00003180
Iteration 191/1000 | Loss: 0.00003180
Iteration 192/1000 | Loss: 0.00003180
Iteration 193/1000 | Loss: 0.00003180
Iteration 194/1000 | Loss: 0.00003180
Iteration 195/1000 | Loss: 0.00003179
Iteration 196/1000 | Loss: 0.00003179
Iteration 197/1000 | Loss: 0.00003179
Iteration 198/1000 | Loss: 0.00003179
Iteration 199/1000 | Loss: 0.00003179
Iteration 200/1000 | Loss: 0.00003179
Iteration 201/1000 | Loss: 0.00003179
Iteration 202/1000 | Loss: 0.00003179
Iteration 203/1000 | Loss: 0.00003179
Iteration 204/1000 | Loss: 0.00003179
Iteration 205/1000 | Loss: 0.00003179
Iteration 206/1000 | Loss: 0.00003179
Iteration 207/1000 | Loss: 0.00003179
Iteration 208/1000 | Loss: 0.00003179
Iteration 209/1000 | Loss: 0.00003179
Iteration 210/1000 | Loss: 0.00003179
Iteration 211/1000 | Loss: 0.00003178
Iteration 212/1000 | Loss: 0.00003178
Iteration 213/1000 | Loss: 0.00003178
Iteration 214/1000 | Loss: 0.00003178
Iteration 215/1000 | Loss: 0.00003178
Iteration 216/1000 | Loss: 0.00003178
Iteration 217/1000 | Loss: 0.00003178
Iteration 218/1000 | Loss: 0.00003178
Iteration 219/1000 | Loss: 0.00003178
Iteration 220/1000 | Loss: 0.00003178
Iteration 221/1000 | Loss: 0.00003178
Iteration 222/1000 | Loss: 0.00003178
Iteration 223/1000 | Loss: 0.00003178
Iteration 224/1000 | Loss: 0.00003178
Iteration 225/1000 | Loss: 0.00003178
Iteration 226/1000 | Loss: 0.00003178
Iteration 227/1000 | Loss: 0.00003178
Iteration 228/1000 | Loss: 0.00003178
Iteration 229/1000 | Loss: 0.00003178
Iteration 230/1000 | Loss: 0.00003178
Iteration 231/1000 | Loss: 0.00003178
Iteration 232/1000 | Loss: 0.00003178
Iteration 233/1000 | Loss: 0.00003178
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [3.177648250130005e-05, 3.177648250130005e-05, 3.177648250130005e-05, 3.177648250130005e-05, 3.177648250130005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.177648250130005e-05

Optimization complete. Final v2v error: 4.618762016296387 mm

Highest mean error: 7.210383415222168 mm for frame 79

Lowest mean error: 3.70896053314209 mm for frame 50

Saving results

Total time: 49.454933643341064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00892986
Iteration 2/25 | Loss: 0.00132237
Iteration 3/25 | Loss: 0.00122155
Iteration 4/25 | Loss: 0.00120897
Iteration 5/25 | Loss: 0.00120451
Iteration 6/25 | Loss: 0.00120283
Iteration 7/25 | Loss: 0.00120283
Iteration 8/25 | Loss: 0.00120283
Iteration 9/25 | Loss: 0.00120283
Iteration 10/25 | Loss: 0.00120283
Iteration 11/25 | Loss: 0.00120283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012028299970552325, 0.0012028299970552325, 0.0012028299970552325, 0.0012028299970552325, 0.0012028299970552325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012028299970552325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35364211
Iteration 2/25 | Loss: 0.00094772
Iteration 3/25 | Loss: 0.00094772
Iteration 4/25 | Loss: 0.00094772
Iteration 5/25 | Loss: 0.00094772
Iteration 6/25 | Loss: 0.00094772
Iteration 7/25 | Loss: 0.00094772
Iteration 8/25 | Loss: 0.00094772
Iteration 9/25 | Loss: 0.00094772
Iteration 10/25 | Loss: 0.00094772
Iteration 11/25 | Loss: 0.00094772
Iteration 12/25 | Loss: 0.00094772
Iteration 13/25 | Loss: 0.00094772
Iteration 14/25 | Loss: 0.00094772
Iteration 15/25 | Loss: 0.00094772
Iteration 16/25 | Loss: 0.00094772
Iteration 17/25 | Loss: 0.00094772
Iteration 18/25 | Loss: 0.00094772
Iteration 19/25 | Loss: 0.00094772
Iteration 20/25 | Loss: 0.00094772
Iteration 21/25 | Loss: 0.00094772
Iteration 22/25 | Loss: 0.00094772
Iteration 23/25 | Loss: 0.00094772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.000947717868257314, 0.000947717868257314, 0.000947717868257314, 0.000947717868257314, 0.000947717868257314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000947717868257314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094772
Iteration 2/1000 | Loss: 0.00004465
Iteration 3/1000 | Loss: 0.00003024
Iteration 4/1000 | Loss: 0.00002482
Iteration 5/1000 | Loss: 0.00002110
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001897
Iteration 8/1000 | Loss: 0.00001863
Iteration 9/1000 | Loss: 0.00001842
Iteration 10/1000 | Loss: 0.00001842
Iteration 11/1000 | Loss: 0.00001841
Iteration 12/1000 | Loss: 0.00001831
Iteration 13/1000 | Loss: 0.00001807
Iteration 14/1000 | Loss: 0.00001793
Iteration 15/1000 | Loss: 0.00001786
Iteration 16/1000 | Loss: 0.00001785
Iteration 17/1000 | Loss: 0.00001784
Iteration 18/1000 | Loss: 0.00001784
Iteration 19/1000 | Loss: 0.00001783
Iteration 20/1000 | Loss: 0.00001779
Iteration 21/1000 | Loss: 0.00001779
Iteration 22/1000 | Loss: 0.00001778
Iteration 23/1000 | Loss: 0.00001777
Iteration 24/1000 | Loss: 0.00001776
Iteration 25/1000 | Loss: 0.00001776
Iteration 26/1000 | Loss: 0.00001776
Iteration 27/1000 | Loss: 0.00001776
Iteration 28/1000 | Loss: 0.00001775
Iteration 29/1000 | Loss: 0.00001775
Iteration 30/1000 | Loss: 0.00001775
Iteration 31/1000 | Loss: 0.00001775
Iteration 32/1000 | Loss: 0.00001775
Iteration 33/1000 | Loss: 0.00001774
Iteration 34/1000 | Loss: 0.00001774
Iteration 35/1000 | Loss: 0.00001773
Iteration 36/1000 | Loss: 0.00001773
Iteration 37/1000 | Loss: 0.00001773
Iteration 38/1000 | Loss: 0.00001772
Iteration 39/1000 | Loss: 0.00001772
Iteration 40/1000 | Loss: 0.00001772
Iteration 41/1000 | Loss: 0.00001772
Iteration 42/1000 | Loss: 0.00001772
Iteration 43/1000 | Loss: 0.00001772
Iteration 44/1000 | Loss: 0.00001771
Iteration 45/1000 | Loss: 0.00001771
Iteration 46/1000 | Loss: 0.00001771
Iteration 47/1000 | Loss: 0.00001771
Iteration 48/1000 | Loss: 0.00001771
Iteration 49/1000 | Loss: 0.00001771
Iteration 50/1000 | Loss: 0.00001771
Iteration 51/1000 | Loss: 0.00001771
Iteration 52/1000 | Loss: 0.00001770
Iteration 53/1000 | Loss: 0.00001770
Iteration 54/1000 | Loss: 0.00001770
Iteration 55/1000 | Loss: 0.00001770
Iteration 56/1000 | Loss: 0.00001769
Iteration 57/1000 | Loss: 0.00001769
Iteration 58/1000 | Loss: 0.00001769
Iteration 59/1000 | Loss: 0.00001769
Iteration 60/1000 | Loss: 0.00001769
Iteration 61/1000 | Loss: 0.00001769
Iteration 62/1000 | Loss: 0.00001768
Iteration 63/1000 | Loss: 0.00001768
Iteration 64/1000 | Loss: 0.00001768
Iteration 65/1000 | Loss: 0.00001768
Iteration 66/1000 | Loss: 0.00001768
Iteration 67/1000 | Loss: 0.00001768
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001767
Iteration 70/1000 | Loss: 0.00001767
Iteration 71/1000 | Loss: 0.00001767
Iteration 72/1000 | Loss: 0.00001767
Iteration 73/1000 | Loss: 0.00001767
Iteration 74/1000 | Loss: 0.00001767
Iteration 75/1000 | Loss: 0.00001767
Iteration 76/1000 | Loss: 0.00001767
Iteration 77/1000 | Loss: 0.00001767
Iteration 78/1000 | Loss: 0.00001767
Iteration 79/1000 | Loss: 0.00001767
Iteration 80/1000 | Loss: 0.00001767
Iteration 81/1000 | Loss: 0.00001767
Iteration 82/1000 | Loss: 0.00001766
Iteration 83/1000 | Loss: 0.00001766
Iteration 84/1000 | Loss: 0.00001766
Iteration 85/1000 | Loss: 0.00001766
Iteration 86/1000 | Loss: 0.00001766
Iteration 87/1000 | Loss: 0.00001766
Iteration 88/1000 | Loss: 0.00001765
Iteration 89/1000 | Loss: 0.00001765
Iteration 90/1000 | Loss: 0.00001765
Iteration 91/1000 | Loss: 0.00001765
Iteration 92/1000 | Loss: 0.00001765
Iteration 93/1000 | Loss: 0.00001765
Iteration 94/1000 | Loss: 0.00001765
Iteration 95/1000 | Loss: 0.00001764
Iteration 96/1000 | Loss: 0.00001764
Iteration 97/1000 | Loss: 0.00001764
Iteration 98/1000 | Loss: 0.00001763
Iteration 99/1000 | Loss: 0.00001763
Iteration 100/1000 | Loss: 0.00001763
Iteration 101/1000 | Loss: 0.00001763
Iteration 102/1000 | Loss: 0.00001763
Iteration 103/1000 | Loss: 0.00001763
Iteration 104/1000 | Loss: 0.00001763
Iteration 105/1000 | Loss: 0.00001763
Iteration 106/1000 | Loss: 0.00001763
Iteration 107/1000 | Loss: 0.00001763
Iteration 108/1000 | Loss: 0.00001763
Iteration 109/1000 | Loss: 0.00001763
Iteration 110/1000 | Loss: 0.00001763
Iteration 111/1000 | Loss: 0.00001763
Iteration 112/1000 | Loss: 0.00001762
Iteration 113/1000 | Loss: 0.00001762
Iteration 114/1000 | Loss: 0.00001762
Iteration 115/1000 | Loss: 0.00001762
Iteration 116/1000 | Loss: 0.00001762
Iteration 117/1000 | Loss: 0.00001762
Iteration 118/1000 | Loss: 0.00001762
Iteration 119/1000 | Loss: 0.00001762
Iteration 120/1000 | Loss: 0.00001762
Iteration 121/1000 | Loss: 0.00001761
Iteration 122/1000 | Loss: 0.00001761
Iteration 123/1000 | Loss: 0.00001761
Iteration 124/1000 | Loss: 0.00001761
Iteration 125/1000 | Loss: 0.00001761
Iteration 126/1000 | Loss: 0.00001761
Iteration 127/1000 | Loss: 0.00001761
Iteration 128/1000 | Loss: 0.00001761
Iteration 129/1000 | Loss: 0.00001761
Iteration 130/1000 | Loss: 0.00001761
Iteration 131/1000 | Loss: 0.00001761
Iteration 132/1000 | Loss: 0.00001761
Iteration 133/1000 | Loss: 0.00001761
Iteration 134/1000 | Loss: 0.00001761
Iteration 135/1000 | Loss: 0.00001761
Iteration 136/1000 | Loss: 0.00001761
Iteration 137/1000 | Loss: 0.00001761
Iteration 138/1000 | Loss: 0.00001761
Iteration 139/1000 | Loss: 0.00001761
Iteration 140/1000 | Loss: 0.00001761
Iteration 141/1000 | Loss: 0.00001761
Iteration 142/1000 | Loss: 0.00001760
Iteration 143/1000 | Loss: 0.00001760
Iteration 144/1000 | Loss: 0.00001760
Iteration 145/1000 | Loss: 0.00001760
Iteration 146/1000 | Loss: 0.00001760
Iteration 147/1000 | Loss: 0.00001760
Iteration 148/1000 | Loss: 0.00001760
Iteration 149/1000 | Loss: 0.00001760
Iteration 150/1000 | Loss: 0.00001760
Iteration 151/1000 | Loss: 0.00001760
Iteration 152/1000 | Loss: 0.00001760
Iteration 153/1000 | Loss: 0.00001760
Iteration 154/1000 | Loss: 0.00001759
Iteration 155/1000 | Loss: 0.00001759
Iteration 156/1000 | Loss: 0.00001759
Iteration 157/1000 | Loss: 0.00001759
Iteration 158/1000 | Loss: 0.00001759
Iteration 159/1000 | Loss: 0.00001759
Iteration 160/1000 | Loss: 0.00001759
Iteration 161/1000 | Loss: 0.00001759
Iteration 162/1000 | Loss: 0.00001759
Iteration 163/1000 | Loss: 0.00001759
Iteration 164/1000 | Loss: 0.00001759
Iteration 165/1000 | Loss: 0.00001759
Iteration 166/1000 | Loss: 0.00001759
Iteration 167/1000 | Loss: 0.00001759
Iteration 168/1000 | Loss: 0.00001759
Iteration 169/1000 | Loss: 0.00001759
Iteration 170/1000 | Loss: 0.00001759
Iteration 171/1000 | Loss: 0.00001759
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.7594215023564175e-05, 1.7594215023564175e-05, 1.7594215023564175e-05, 1.7594215023564175e-05, 1.7594215023564175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7594215023564175e-05

Optimization complete. Final v2v error: 3.6262385845184326 mm

Highest mean error: 3.925447463989258 mm for frame 27

Lowest mean error: 3.3898983001708984 mm for frame 123

Saving results

Total time: 34.49751257896423
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00574159
Iteration 2/25 | Loss: 0.00152106
Iteration 3/25 | Loss: 0.00131323
Iteration 4/25 | Loss: 0.00129229
Iteration 5/25 | Loss: 0.00128531
Iteration 6/25 | Loss: 0.00128330
Iteration 7/25 | Loss: 0.00128274
Iteration 8/25 | Loss: 0.00128274
Iteration 9/25 | Loss: 0.00128274
Iteration 10/25 | Loss: 0.00128274
Iteration 11/25 | Loss: 0.00128274
Iteration 12/25 | Loss: 0.00128274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012827382888644934, 0.0012827382888644934, 0.0012827382888644934, 0.0012827382888644934, 0.0012827382888644934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012827382888644934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38784993
Iteration 2/25 | Loss: 0.00102157
Iteration 3/25 | Loss: 0.00102157
Iteration 4/25 | Loss: 0.00102157
Iteration 5/25 | Loss: 0.00102157
Iteration 6/25 | Loss: 0.00102157
Iteration 7/25 | Loss: 0.00102157
Iteration 8/25 | Loss: 0.00102157
Iteration 9/25 | Loss: 0.00102157
Iteration 10/25 | Loss: 0.00102157
Iteration 11/25 | Loss: 0.00102157
Iteration 12/25 | Loss: 0.00102157
Iteration 13/25 | Loss: 0.00102157
Iteration 14/25 | Loss: 0.00102157
Iteration 15/25 | Loss: 0.00102157
Iteration 16/25 | Loss: 0.00102157
Iteration 17/25 | Loss: 0.00102157
Iteration 18/25 | Loss: 0.00102157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0010215680813416839, 0.0010215680813416839, 0.0010215680813416839, 0.0010215680813416839, 0.0010215680813416839]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010215680813416839

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102157
Iteration 2/1000 | Loss: 0.00006844
Iteration 3/1000 | Loss: 0.00004729
Iteration 4/1000 | Loss: 0.00004051
Iteration 5/1000 | Loss: 0.00003672
Iteration 6/1000 | Loss: 0.00003524
Iteration 7/1000 | Loss: 0.00003445
Iteration 8/1000 | Loss: 0.00003382
Iteration 9/1000 | Loss: 0.00003343
Iteration 10/1000 | Loss: 0.00003306
Iteration 11/1000 | Loss: 0.00003278
Iteration 12/1000 | Loss: 0.00003257
Iteration 13/1000 | Loss: 0.00003252
Iteration 14/1000 | Loss: 0.00003246
Iteration 15/1000 | Loss: 0.00003240
Iteration 16/1000 | Loss: 0.00003236
Iteration 17/1000 | Loss: 0.00003235
Iteration 18/1000 | Loss: 0.00003233
Iteration 19/1000 | Loss: 0.00003229
Iteration 20/1000 | Loss: 0.00003227
Iteration 21/1000 | Loss: 0.00003227
Iteration 22/1000 | Loss: 0.00003225
Iteration 23/1000 | Loss: 0.00003225
Iteration 24/1000 | Loss: 0.00003224
Iteration 25/1000 | Loss: 0.00003223
Iteration 26/1000 | Loss: 0.00003223
Iteration 27/1000 | Loss: 0.00003222
Iteration 28/1000 | Loss: 0.00003222
Iteration 29/1000 | Loss: 0.00003222
Iteration 30/1000 | Loss: 0.00003221
Iteration 31/1000 | Loss: 0.00003221
Iteration 32/1000 | Loss: 0.00003221
Iteration 33/1000 | Loss: 0.00003220
Iteration 34/1000 | Loss: 0.00003220
Iteration 35/1000 | Loss: 0.00003220
Iteration 36/1000 | Loss: 0.00003220
Iteration 37/1000 | Loss: 0.00003220
Iteration 38/1000 | Loss: 0.00003219
Iteration 39/1000 | Loss: 0.00003219
Iteration 40/1000 | Loss: 0.00003219
Iteration 41/1000 | Loss: 0.00003219
Iteration 42/1000 | Loss: 0.00003219
Iteration 43/1000 | Loss: 0.00003219
Iteration 44/1000 | Loss: 0.00003219
Iteration 45/1000 | Loss: 0.00003218
Iteration 46/1000 | Loss: 0.00003218
Iteration 47/1000 | Loss: 0.00003218
Iteration 48/1000 | Loss: 0.00003218
Iteration 49/1000 | Loss: 0.00003218
Iteration 50/1000 | Loss: 0.00003217
Iteration 51/1000 | Loss: 0.00003217
Iteration 52/1000 | Loss: 0.00003217
Iteration 53/1000 | Loss: 0.00003217
Iteration 54/1000 | Loss: 0.00003217
Iteration 55/1000 | Loss: 0.00003217
Iteration 56/1000 | Loss: 0.00003217
Iteration 57/1000 | Loss: 0.00003217
Iteration 58/1000 | Loss: 0.00003217
Iteration 59/1000 | Loss: 0.00003217
Iteration 60/1000 | Loss: 0.00003217
Iteration 61/1000 | Loss: 0.00003217
Iteration 62/1000 | Loss: 0.00003216
Iteration 63/1000 | Loss: 0.00003216
Iteration 64/1000 | Loss: 0.00003216
Iteration 65/1000 | Loss: 0.00003216
Iteration 66/1000 | Loss: 0.00003216
Iteration 67/1000 | Loss: 0.00003216
Iteration 68/1000 | Loss: 0.00003216
Iteration 69/1000 | Loss: 0.00003216
Iteration 70/1000 | Loss: 0.00003216
Iteration 71/1000 | Loss: 0.00003216
Iteration 72/1000 | Loss: 0.00003216
Iteration 73/1000 | Loss: 0.00003216
Iteration 74/1000 | Loss: 0.00003216
Iteration 75/1000 | Loss: 0.00003216
Iteration 76/1000 | Loss: 0.00003216
Iteration 77/1000 | Loss: 0.00003216
Iteration 78/1000 | Loss: 0.00003216
Iteration 79/1000 | Loss: 0.00003215
Iteration 80/1000 | Loss: 0.00003215
Iteration 81/1000 | Loss: 0.00003215
Iteration 82/1000 | Loss: 0.00003215
Iteration 83/1000 | Loss: 0.00003215
Iteration 84/1000 | Loss: 0.00003215
Iteration 85/1000 | Loss: 0.00003215
Iteration 86/1000 | Loss: 0.00003215
Iteration 87/1000 | Loss: 0.00003215
Iteration 88/1000 | Loss: 0.00003215
Iteration 89/1000 | Loss: 0.00003215
Iteration 90/1000 | Loss: 0.00003215
Iteration 91/1000 | Loss: 0.00003215
Iteration 92/1000 | Loss: 0.00003215
Iteration 93/1000 | Loss: 0.00003215
Iteration 94/1000 | Loss: 0.00003215
Iteration 95/1000 | Loss: 0.00003215
Iteration 96/1000 | Loss: 0.00003215
Iteration 97/1000 | Loss: 0.00003215
Iteration 98/1000 | Loss: 0.00003215
Iteration 99/1000 | Loss: 0.00003215
Iteration 100/1000 | Loss: 0.00003215
Iteration 101/1000 | Loss: 0.00003215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [3.2152198400581256e-05, 3.2152198400581256e-05, 3.2152198400581256e-05, 3.2152198400581256e-05, 3.2152198400581256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2152198400581256e-05

Optimization complete. Final v2v error: 4.834359169006348 mm

Highest mean error: 5.518006324768066 mm for frame 147

Lowest mean error: 4.142401218414307 mm for frame 89

Saving results

Total time: 37.58588528633118
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00869856
Iteration 2/25 | Loss: 0.00133160
Iteration 3/25 | Loss: 0.00122897
Iteration 4/25 | Loss: 0.00121486
Iteration 5/25 | Loss: 0.00121009
Iteration 6/25 | Loss: 0.00120875
Iteration 7/25 | Loss: 0.00120875
Iteration 8/25 | Loss: 0.00120875
Iteration 9/25 | Loss: 0.00120875
Iteration 10/25 | Loss: 0.00120875
Iteration 11/25 | Loss: 0.00120875
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012087526265531778, 0.0012087526265531778, 0.0012087526265531778, 0.0012087526265531778, 0.0012087526265531778]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012087526265531778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35001266
Iteration 2/25 | Loss: 0.00094068
Iteration 3/25 | Loss: 0.00094068
Iteration 4/25 | Loss: 0.00094068
Iteration 5/25 | Loss: 0.00094068
Iteration 6/25 | Loss: 0.00094068
Iteration 7/25 | Loss: 0.00094068
Iteration 8/25 | Loss: 0.00094068
Iteration 9/25 | Loss: 0.00094068
Iteration 10/25 | Loss: 0.00094068
Iteration 11/25 | Loss: 0.00094068
Iteration 12/25 | Loss: 0.00094068
Iteration 13/25 | Loss: 0.00094068
Iteration 14/25 | Loss: 0.00094068
Iteration 15/25 | Loss: 0.00094068
Iteration 16/25 | Loss: 0.00094068
Iteration 17/25 | Loss: 0.00094068
Iteration 18/25 | Loss: 0.00094068
Iteration 19/25 | Loss: 0.00094068
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009406760800629854, 0.0009406760800629854, 0.0009406760800629854, 0.0009406760800629854, 0.0009406760800629854]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009406760800629854

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094068
Iteration 2/1000 | Loss: 0.00004101
Iteration 3/1000 | Loss: 0.00002912
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002192
Iteration 6/1000 | Loss: 0.00002084
Iteration 7/1000 | Loss: 0.00002040
Iteration 8/1000 | Loss: 0.00002018
Iteration 9/1000 | Loss: 0.00002012
Iteration 10/1000 | Loss: 0.00001985
Iteration 11/1000 | Loss: 0.00001970
Iteration 12/1000 | Loss: 0.00001968
Iteration 13/1000 | Loss: 0.00001966
Iteration 14/1000 | Loss: 0.00001962
Iteration 15/1000 | Loss: 0.00001961
Iteration 16/1000 | Loss: 0.00001960
Iteration 17/1000 | Loss: 0.00001959
Iteration 18/1000 | Loss: 0.00001958
Iteration 19/1000 | Loss: 0.00001958
Iteration 20/1000 | Loss: 0.00001957
Iteration 21/1000 | Loss: 0.00001956
Iteration 22/1000 | Loss: 0.00001953
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001951
Iteration 26/1000 | Loss: 0.00001951
Iteration 27/1000 | Loss: 0.00001950
Iteration 28/1000 | Loss: 0.00001946
Iteration 29/1000 | Loss: 0.00001945
Iteration 30/1000 | Loss: 0.00001945
Iteration 31/1000 | Loss: 0.00001944
Iteration 32/1000 | Loss: 0.00001943
Iteration 33/1000 | Loss: 0.00001942
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001941
Iteration 38/1000 | Loss: 0.00001941
Iteration 39/1000 | Loss: 0.00001941
Iteration 40/1000 | Loss: 0.00001941
Iteration 41/1000 | Loss: 0.00001940
Iteration 42/1000 | Loss: 0.00001940
Iteration 43/1000 | Loss: 0.00001940
Iteration 44/1000 | Loss: 0.00001940
Iteration 45/1000 | Loss: 0.00001940
Iteration 46/1000 | Loss: 0.00001940
Iteration 47/1000 | Loss: 0.00001939
Iteration 48/1000 | Loss: 0.00001939
Iteration 49/1000 | Loss: 0.00001939
Iteration 50/1000 | Loss: 0.00001939
Iteration 51/1000 | Loss: 0.00001939
Iteration 52/1000 | Loss: 0.00001939
Iteration 53/1000 | Loss: 0.00001938
Iteration 54/1000 | Loss: 0.00001938
Iteration 55/1000 | Loss: 0.00001938
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001937
Iteration 58/1000 | Loss: 0.00001937
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001936
Iteration 63/1000 | Loss: 0.00001935
Iteration 64/1000 | Loss: 0.00001935
Iteration 65/1000 | Loss: 0.00001935
Iteration 66/1000 | Loss: 0.00001935
Iteration 67/1000 | Loss: 0.00001935
Iteration 68/1000 | Loss: 0.00001935
Iteration 69/1000 | Loss: 0.00001934
Iteration 70/1000 | Loss: 0.00001934
Iteration 71/1000 | Loss: 0.00001934
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001934
Iteration 76/1000 | Loss: 0.00001933
Iteration 77/1000 | Loss: 0.00001933
Iteration 78/1000 | Loss: 0.00001933
Iteration 79/1000 | Loss: 0.00001933
Iteration 80/1000 | Loss: 0.00001932
Iteration 81/1000 | Loss: 0.00001932
Iteration 82/1000 | Loss: 0.00001932
Iteration 83/1000 | Loss: 0.00001932
Iteration 84/1000 | Loss: 0.00001932
Iteration 85/1000 | Loss: 0.00001932
Iteration 86/1000 | Loss: 0.00001931
Iteration 87/1000 | Loss: 0.00001931
Iteration 88/1000 | Loss: 0.00001931
Iteration 89/1000 | Loss: 0.00001931
Iteration 90/1000 | Loss: 0.00001931
Iteration 91/1000 | Loss: 0.00001930
Iteration 92/1000 | Loss: 0.00001930
Iteration 93/1000 | Loss: 0.00001930
Iteration 94/1000 | Loss: 0.00001930
Iteration 95/1000 | Loss: 0.00001929
Iteration 96/1000 | Loss: 0.00001929
Iteration 97/1000 | Loss: 0.00001929
Iteration 98/1000 | Loss: 0.00001929
Iteration 99/1000 | Loss: 0.00001929
Iteration 100/1000 | Loss: 0.00001929
Iteration 101/1000 | Loss: 0.00001929
Iteration 102/1000 | Loss: 0.00001929
Iteration 103/1000 | Loss: 0.00001928
Iteration 104/1000 | Loss: 0.00001928
Iteration 105/1000 | Loss: 0.00001928
Iteration 106/1000 | Loss: 0.00001928
Iteration 107/1000 | Loss: 0.00001928
Iteration 108/1000 | Loss: 0.00001928
Iteration 109/1000 | Loss: 0.00001928
Iteration 110/1000 | Loss: 0.00001928
Iteration 111/1000 | Loss: 0.00001928
Iteration 112/1000 | Loss: 0.00001927
Iteration 113/1000 | Loss: 0.00001927
Iteration 114/1000 | Loss: 0.00001927
Iteration 115/1000 | Loss: 0.00001927
Iteration 116/1000 | Loss: 0.00001927
Iteration 117/1000 | Loss: 0.00001927
Iteration 118/1000 | Loss: 0.00001927
Iteration 119/1000 | Loss: 0.00001927
Iteration 120/1000 | Loss: 0.00001927
Iteration 121/1000 | Loss: 0.00001927
Iteration 122/1000 | Loss: 0.00001927
Iteration 123/1000 | Loss: 0.00001927
Iteration 124/1000 | Loss: 0.00001927
Iteration 125/1000 | Loss: 0.00001927
Iteration 126/1000 | Loss: 0.00001927
Iteration 127/1000 | Loss: 0.00001927
Iteration 128/1000 | Loss: 0.00001927
Iteration 129/1000 | Loss: 0.00001927
Iteration 130/1000 | Loss: 0.00001927
Iteration 131/1000 | Loss: 0.00001927
Iteration 132/1000 | Loss: 0.00001927
Iteration 133/1000 | Loss: 0.00001927
Iteration 134/1000 | Loss: 0.00001927
Iteration 135/1000 | Loss: 0.00001927
Iteration 136/1000 | Loss: 0.00001927
Iteration 137/1000 | Loss: 0.00001927
Iteration 138/1000 | Loss: 0.00001927
Iteration 139/1000 | Loss: 0.00001927
Iteration 140/1000 | Loss: 0.00001927
Iteration 141/1000 | Loss: 0.00001927
Iteration 142/1000 | Loss: 0.00001927
Iteration 143/1000 | Loss: 0.00001927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.9267779862275347e-05, 1.9267779862275347e-05, 1.9267779862275347e-05, 1.9267779862275347e-05, 1.9267779862275347e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9267779862275347e-05

Optimization complete. Final v2v error: 3.8019776344299316 mm

Highest mean error: 4.038055419921875 mm for frame 104

Lowest mean error: 3.5010645389556885 mm for frame 168

Saving results

Total time: 32.78170847892761
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01122051
Iteration 2/25 | Loss: 0.00174714
Iteration 3/25 | Loss: 0.00141397
Iteration 4/25 | Loss: 0.00136431
Iteration 5/25 | Loss: 0.00134693
Iteration 6/25 | Loss: 0.00135128
Iteration 7/25 | Loss: 0.00132534
Iteration 8/25 | Loss: 0.00131722
Iteration 9/25 | Loss: 0.00131385
Iteration 10/25 | Loss: 0.00131336
Iteration 11/25 | Loss: 0.00131317
Iteration 12/25 | Loss: 0.00131317
Iteration 13/25 | Loss: 0.00131317
Iteration 14/25 | Loss: 0.00131317
Iteration 15/25 | Loss: 0.00131317
Iteration 16/25 | Loss: 0.00131317
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013131749583408237, 0.0013131749583408237, 0.0013131749583408237, 0.0013131749583408237, 0.0013131749583408237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013131749583408237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34897482
Iteration 2/25 | Loss: 0.00116435
Iteration 3/25 | Loss: 0.00116435
Iteration 4/25 | Loss: 0.00116435
Iteration 5/25 | Loss: 0.00116435
Iteration 6/25 | Loss: 0.00116435
Iteration 7/25 | Loss: 0.00116435
Iteration 8/25 | Loss: 0.00116435
Iteration 9/25 | Loss: 0.00116435
Iteration 10/25 | Loss: 0.00116435
Iteration 11/25 | Loss: 0.00116435
Iteration 12/25 | Loss: 0.00116435
Iteration 13/25 | Loss: 0.00116435
Iteration 14/25 | Loss: 0.00116435
Iteration 15/25 | Loss: 0.00116435
Iteration 16/25 | Loss: 0.00116435
Iteration 17/25 | Loss: 0.00116435
Iteration 18/25 | Loss: 0.00116435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011643458856269717, 0.0011643458856269717, 0.0011643458856269717, 0.0011643458856269717, 0.0011643458856269717]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011643458856269717

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116435
Iteration 2/1000 | Loss: 0.00006641
Iteration 3/1000 | Loss: 0.00004749
Iteration 4/1000 | Loss: 0.00003952
Iteration 5/1000 | Loss: 0.00003658
Iteration 6/1000 | Loss: 0.00003514
Iteration 7/1000 | Loss: 0.00003441
Iteration 8/1000 | Loss: 0.00003399
Iteration 9/1000 | Loss: 0.00003359
Iteration 10/1000 | Loss: 0.00003334
Iteration 11/1000 | Loss: 0.00003330
Iteration 12/1000 | Loss: 0.00003323
Iteration 13/1000 | Loss: 0.00003318
Iteration 14/1000 | Loss: 0.00003318
Iteration 15/1000 | Loss: 0.00003318
Iteration 16/1000 | Loss: 0.00003317
Iteration 17/1000 | Loss: 0.00003317
Iteration 18/1000 | Loss: 0.00003317
Iteration 19/1000 | Loss: 0.00003317
Iteration 20/1000 | Loss: 0.00003314
Iteration 21/1000 | Loss: 0.00003313
Iteration 22/1000 | Loss: 0.00003311
Iteration 23/1000 | Loss: 0.00003311
Iteration 24/1000 | Loss: 0.00003310
Iteration 25/1000 | Loss: 0.00003310
Iteration 26/1000 | Loss: 0.00003310
Iteration 27/1000 | Loss: 0.00003309
Iteration 28/1000 | Loss: 0.00003309
Iteration 29/1000 | Loss: 0.00003309
Iteration 30/1000 | Loss: 0.00003309
Iteration 31/1000 | Loss: 0.00003309
Iteration 32/1000 | Loss: 0.00003309
Iteration 33/1000 | Loss: 0.00003309
Iteration 34/1000 | Loss: 0.00003309
Iteration 35/1000 | Loss: 0.00003308
Iteration 36/1000 | Loss: 0.00003308
Iteration 37/1000 | Loss: 0.00003308
Iteration 38/1000 | Loss: 0.00003307
Iteration 39/1000 | Loss: 0.00003306
Iteration 40/1000 | Loss: 0.00003306
Iteration 41/1000 | Loss: 0.00003305
Iteration 42/1000 | Loss: 0.00003305
Iteration 43/1000 | Loss: 0.00003305
Iteration 44/1000 | Loss: 0.00003305
Iteration 45/1000 | Loss: 0.00003305
Iteration 46/1000 | Loss: 0.00003305
Iteration 47/1000 | Loss: 0.00003304
Iteration 48/1000 | Loss: 0.00003304
Iteration 49/1000 | Loss: 0.00003304
Iteration 50/1000 | Loss: 0.00003304
Iteration 51/1000 | Loss: 0.00003304
Iteration 52/1000 | Loss: 0.00003304
Iteration 53/1000 | Loss: 0.00003304
Iteration 54/1000 | Loss: 0.00003304
Iteration 55/1000 | Loss: 0.00003304
Iteration 56/1000 | Loss: 0.00003303
Iteration 57/1000 | Loss: 0.00003303
Iteration 58/1000 | Loss: 0.00003302
Iteration 59/1000 | Loss: 0.00003302
Iteration 60/1000 | Loss: 0.00003302
Iteration 61/1000 | Loss: 0.00003302
Iteration 62/1000 | Loss: 0.00003302
Iteration 63/1000 | Loss: 0.00003301
Iteration 64/1000 | Loss: 0.00003301
Iteration 65/1000 | Loss: 0.00003301
Iteration 66/1000 | Loss: 0.00003301
Iteration 67/1000 | Loss: 0.00003301
Iteration 68/1000 | Loss: 0.00003301
Iteration 69/1000 | Loss: 0.00003301
Iteration 70/1000 | Loss: 0.00003300
Iteration 71/1000 | Loss: 0.00003300
Iteration 72/1000 | Loss: 0.00003300
Iteration 73/1000 | Loss: 0.00003299
Iteration 74/1000 | Loss: 0.00003299
Iteration 75/1000 | Loss: 0.00003299
Iteration 76/1000 | Loss: 0.00003299
Iteration 77/1000 | Loss: 0.00003299
Iteration 78/1000 | Loss: 0.00003299
Iteration 79/1000 | Loss: 0.00003299
Iteration 80/1000 | Loss: 0.00003299
Iteration 81/1000 | Loss: 0.00003299
Iteration 82/1000 | Loss: 0.00003299
Iteration 83/1000 | Loss: 0.00003298
Iteration 84/1000 | Loss: 0.00003298
Iteration 85/1000 | Loss: 0.00003298
Iteration 86/1000 | Loss: 0.00003297
Iteration 87/1000 | Loss: 0.00003297
Iteration 88/1000 | Loss: 0.00003297
Iteration 89/1000 | Loss: 0.00003297
Iteration 90/1000 | Loss: 0.00003297
Iteration 91/1000 | Loss: 0.00003297
Iteration 92/1000 | Loss: 0.00003296
Iteration 93/1000 | Loss: 0.00003296
Iteration 94/1000 | Loss: 0.00003296
Iteration 95/1000 | Loss: 0.00003295
Iteration 96/1000 | Loss: 0.00003294
Iteration 97/1000 | Loss: 0.00003294
Iteration 98/1000 | Loss: 0.00003294
Iteration 99/1000 | Loss: 0.00003294
Iteration 100/1000 | Loss: 0.00003294
Iteration 101/1000 | Loss: 0.00003294
Iteration 102/1000 | Loss: 0.00003294
Iteration 103/1000 | Loss: 0.00003294
Iteration 104/1000 | Loss: 0.00003294
Iteration 105/1000 | Loss: 0.00003294
Iteration 106/1000 | Loss: 0.00003294
Iteration 107/1000 | Loss: 0.00003294
Iteration 108/1000 | Loss: 0.00003294
Iteration 109/1000 | Loss: 0.00003294
Iteration 110/1000 | Loss: 0.00003294
Iteration 111/1000 | Loss: 0.00003294
Iteration 112/1000 | Loss: 0.00003294
Iteration 113/1000 | Loss: 0.00003294
Iteration 114/1000 | Loss: 0.00003294
Iteration 115/1000 | Loss: 0.00003294
Iteration 116/1000 | Loss: 0.00003294
Iteration 117/1000 | Loss: 0.00003294
Iteration 118/1000 | Loss: 0.00003294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [3.2938976801233366e-05, 3.2938976801233366e-05, 3.2938976801233366e-05, 3.2938976801233366e-05, 3.2938976801233366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2938976801233366e-05

Optimization complete. Final v2v error: 5.029873371124268 mm

Highest mean error: 5.217187881469727 mm for frame 26

Lowest mean error: 4.860602378845215 mm for frame 115

Saving results

Total time: 38.37883949279785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00648621
Iteration 2/25 | Loss: 0.00149158
Iteration 3/25 | Loss: 0.00134129
Iteration 4/25 | Loss: 0.00132020
Iteration 5/25 | Loss: 0.00131588
Iteration 6/25 | Loss: 0.00131526
Iteration 7/25 | Loss: 0.00131526
Iteration 8/25 | Loss: 0.00131526
Iteration 9/25 | Loss: 0.00131526
Iteration 10/25 | Loss: 0.00131526
Iteration 11/25 | Loss: 0.00131526
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013152641477063298, 0.0013152641477063298, 0.0013152641477063298, 0.0013152641477063298, 0.0013152641477063298]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013152641477063298

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48978198
Iteration 2/25 | Loss: 0.00091680
Iteration 3/25 | Loss: 0.00091679
Iteration 4/25 | Loss: 0.00091679
Iteration 5/25 | Loss: 0.00091679
Iteration 6/25 | Loss: 0.00091679
Iteration 7/25 | Loss: 0.00091679
Iteration 8/25 | Loss: 0.00091679
Iteration 9/25 | Loss: 0.00091679
Iteration 10/25 | Loss: 0.00091679
Iteration 11/25 | Loss: 0.00091679
Iteration 12/25 | Loss: 0.00091679
Iteration 13/25 | Loss: 0.00091679
Iteration 14/25 | Loss: 0.00091679
Iteration 15/25 | Loss: 0.00091679
Iteration 16/25 | Loss: 0.00091679
Iteration 17/25 | Loss: 0.00091679
Iteration 18/25 | Loss: 0.00091679
Iteration 19/25 | Loss: 0.00091679
Iteration 20/25 | Loss: 0.00091679
Iteration 21/25 | Loss: 0.00091679
Iteration 22/25 | Loss: 0.00091679
Iteration 23/25 | Loss: 0.00091679
Iteration 24/25 | Loss: 0.00091679
Iteration 25/25 | Loss: 0.00091679

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091679
Iteration 2/1000 | Loss: 0.00007180
Iteration 3/1000 | Loss: 0.00004905
Iteration 4/1000 | Loss: 0.00004105
Iteration 5/1000 | Loss: 0.00003613
Iteration 6/1000 | Loss: 0.00003311
Iteration 7/1000 | Loss: 0.00003172
Iteration 8/1000 | Loss: 0.00003094
Iteration 9/1000 | Loss: 0.00003032
Iteration 10/1000 | Loss: 0.00002980
Iteration 11/1000 | Loss: 0.00002946
Iteration 12/1000 | Loss: 0.00002916
Iteration 13/1000 | Loss: 0.00002891
Iteration 14/1000 | Loss: 0.00002873
Iteration 15/1000 | Loss: 0.00002870
Iteration 16/1000 | Loss: 0.00002867
Iteration 17/1000 | Loss: 0.00002865
Iteration 18/1000 | Loss: 0.00002864
Iteration 19/1000 | Loss: 0.00002859
Iteration 20/1000 | Loss: 0.00002857
Iteration 21/1000 | Loss: 0.00002853
Iteration 22/1000 | Loss: 0.00002850
Iteration 23/1000 | Loss: 0.00002849
Iteration 24/1000 | Loss: 0.00002849
Iteration 25/1000 | Loss: 0.00002845
Iteration 26/1000 | Loss: 0.00002845
Iteration 27/1000 | Loss: 0.00002843
Iteration 28/1000 | Loss: 0.00002843
Iteration 29/1000 | Loss: 0.00002842
Iteration 30/1000 | Loss: 0.00002841
Iteration 31/1000 | Loss: 0.00002841
Iteration 32/1000 | Loss: 0.00002840
Iteration 33/1000 | Loss: 0.00002840
Iteration 34/1000 | Loss: 0.00002840
Iteration 35/1000 | Loss: 0.00002839
Iteration 36/1000 | Loss: 0.00002839
Iteration 37/1000 | Loss: 0.00002838
Iteration 38/1000 | Loss: 0.00002838
Iteration 39/1000 | Loss: 0.00002837
Iteration 40/1000 | Loss: 0.00002837
Iteration 41/1000 | Loss: 0.00002837
Iteration 42/1000 | Loss: 0.00002837
Iteration 43/1000 | Loss: 0.00002837
Iteration 44/1000 | Loss: 0.00002836
Iteration 45/1000 | Loss: 0.00002836
Iteration 46/1000 | Loss: 0.00002835
Iteration 47/1000 | Loss: 0.00002835
Iteration 48/1000 | Loss: 0.00002835
Iteration 49/1000 | Loss: 0.00002835
Iteration 50/1000 | Loss: 0.00002835
Iteration 51/1000 | Loss: 0.00002834
Iteration 52/1000 | Loss: 0.00002834
Iteration 53/1000 | Loss: 0.00002834
Iteration 54/1000 | Loss: 0.00002834
Iteration 55/1000 | Loss: 0.00002834
Iteration 56/1000 | Loss: 0.00002834
Iteration 57/1000 | Loss: 0.00002834
Iteration 58/1000 | Loss: 0.00002834
Iteration 59/1000 | Loss: 0.00002834
Iteration 60/1000 | Loss: 0.00002834
Iteration 61/1000 | Loss: 0.00002833
Iteration 62/1000 | Loss: 0.00002833
Iteration 63/1000 | Loss: 0.00002833
Iteration 64/1000 | Loss: 0.00002833
Iteration 65/1000 | Loss: 0.00002833
Iteration 66/1000 | Loss: 0.00002832
Iteration 67/1000 | Loss: 0.00002832
Iteration 68/1000 | Loss: 0.00002832
Iteration 69/1000 | Loss: 0.00002832
Iteration 70/1000 | Loss: 0.00002832
Iteration 71/1000 | Loss: 0.00002831
Iteration 72/1000 | Loss: 0.00002831
Iteration 73/1000 | Loss: 0.00002831
Iteration 74/1000 | Loss: 0.00002831
Iteration 75/1000 | Loss: 0.00002831
Iteration 76/1000 | Loss: 0.00002831
Iteration 77/1000 | Loss: 0.00002831
Iteration 78/1000 | Loss: 0.00002830
Iteration 79/1000 | Loss: 0.00002830
Iteration 80/1000 | Loss: 0.00002830
Iteration 81/1000 | Loss: 0.00002830
Iteration 82/1000 | Loss: 0.00002830
Iteration 83/1000 | Loss: 0.00002830
Iteration 84/1000 | Loss: 0.00002830
Iteration 85/1000 | Loss: 0.00002829
Iteration 86/1000 | Loss: 0.00002829
Iteration 87/1000 | Loss: 0.00002829
Iteration 88/1000 | Loss: 0.00002829
Iteration 89/1000 | Loss: 0.00002829
Iteration 90/1000 | Loss: 0.00002829
Iteration 91/1000 | Loss: 0.00002829
Iteration 92/1000 | Loss: 0.00002829
Iteration 93/1000 | Loss: 0.00002829
Iteration 94/1000 | Loss: 0.00002829
Iteration 95/1000 | Loss: 0.00002829
Iteration 96/1000 | Loss: 0.00002828
Iteration 97/1000 | Loss: 0.00002828
Iteration 98/1000 | Loss: 0.00002828
Iteration 99/1000 | Loss: 0.00002828
Iteration 100/1000 | Loss: 0.00002828
Iteration 101/1000 | Loss: 0.00002828
Iteration 102/1000 | Loss: 0.00002828
Iteration 103/1000 | Loss: 0.00002828
Iteration 104/1000 | Loss: 0.00002828
Iteration 105/1000 | Loss: 0.00002828
Iteration 106/1000 | Loss: 0.00002828
Iteration 107/1000 | Loss: 0.00002828
Iteration 108/1000 | Loss: 0.00002828
Iteration 109/1000 | Loss: 0.00002828
Iteration 110/1000 | Loss: 0.00002828
Iteration 111/1000 | Loss: 0.00002827
Iteration 112/1000 | Loss: 0.00002827
Iteration 113/1000 | Loss: 0.00002827
Iteration 114/1000 | Loss: 0.00002827
Iteration 115/1000 | Loss: 0.00002827
Iteration 116/1000 | Loss: 0.00002827
Iteration 117/1000 | Loss: 0.00002827
Iteration 118/1000 | Loss: 0.00002827
Iteration 119/1000 | Loss: 0.00002827
Iteration 120/1000 | Loss: 0.00002827
Iteration 121/1000 | Loss: 0.00002827
Iteration 122/1000 | Loss: 0.00002827
Iteration 123/1000 | Loss: 0.00002827
Iteration 124/1000 | Loss: 0.00002827
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [2.827364187396597e-05, 2.827364187396597e-05, 2.827364187396597e-05, 2.827364187396597e-05, 2.827364187396597e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.827364187396597e-05

Optimization complete. Final v2v error: 4.5366926193237305 mm

Highest mean error: 5.546177864074707 mm for frame 103

Lowest mean error: 3.622616767883301 mm for frame 222

Saving results

Total time: 43.087409019470215
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01105373
Iteration 2/25 | Loss: 0.00230040
Iteration 3/25 | Loss: 0.00181700
Iteration 4/25 | Loss: 0.00175108
Iteration 5/25 | Loss: 0.00176487
Iteration 6/25 | Loss: 0.00160593
Iteration 7/25 | Loss: 0.00159280
Iteration 8/25 | Loss: 0.00153048
Iteration 9/25 | Loss: 0.00151919
Iteration 10/25 | Loss: 0.00148400
Iteration 11/25 | Loss: 0.00144000
Iteration 12/25 | Loss: 0.00142108
Iteration 13/25 | Loss: 0.00141452
Iteration 14/25 | Loss: 0.00140839
Iteration 15/25 | Loss: 0.00139241
Iteration 16/25 | Loss: 0.00138568
Iteration 17/25 | Loss: 0.00138731
Iteration 18/25 | Loss: 0.00138106
Iteration 19/25 | Loss: 0.00137626
Iteration 20/25 | Loss: 0.00137674
Iteration 21/25 | Loss: 0.00137863
Iteration 22/25 | Loss: 0.00137660
Iteration 23/25 | Loss: 0.00137907
Iteration 24/25 | Loss: 0.00137740
Iteration 25/25 | Loss: 0.00137610

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36961627
Iteration 2/25 | Loss: 0.00307742
Iteration 3/25 | Loss: 0.00307742
Iteration 4/25 | Loss: 0.00307742
Iteration 5/25 | Loss: 0.00307742
Iteration 6/25 | Loss: 0.00307742
Iteration 7/25 | Loss: 0.00307742
Iteration 8/25 | Loss: 0.00307742
Iteration 9/25 | Loss: 0.00307742
Iteration 10/25 | Loss: 0.00307742
Iteration 11/25 | Loss: 0.00307742
Iteration 12/25 | Loss: 0.00307742
Iteration 13/25 | Loss: 0.00307742
Iteration 14/25 | Loss: 0.00307742
Iteration 15/25 | Loss: 0.00307742
Iteration 16/25 | Loss: 0.00307742
Iteration 17/25 | Loss: 0.00307742
Iteration 18/25 | Loss: 0.00307742
Iteration 19/25 | Loss: 0.00307742
Iteration 20/25 | Loss: 0.00307742
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0030774178449064493, 0.0030774178449064493, 0.0030774178449064493, 0.0030774178449064493, 0.0030774178449064493]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030774178449064493

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00307742
Iteration 2/1000 | Loss: 0.00098294
Iteration 3/1000 | Loss: 0.00076030
Iteration 4/1000 | Loss: 0.00080974
Iteration 5/1000 | Loss: 0.00040436
Iteration 6/1000 | Loss: 0.00048542
Iteration 7/1000 | Loss: 0.00043233
Iteration 8/1000 | Loss: 0.00038229
Iteration 9/1000 | Loss: 0.00044929
Iteration 10/1000 | Loss: 0.00079574
Iteration 11/1000 | Loss: 0.00042557
Iteration 12/1000 | Loss: 0.00055575
Iteration 13/1000 | Loss: 0.00029475
Iteration 14/1000 | Loss: 0.00024692
Iteration 15/1000 | Loss: 0.00021888
Iteration 16/1000 | Loss: 0.00081382
Iteration 17/1000 | Loss: 0.00053566
Iteration 18/1000 | Loss: 0.00390078
Iteration 19/1000 | Loss: 0.00865515
Iteration 20/1000 | Loss: 0.00915118
Iteration 21/1000 | Loss: 0.00177283
Iteration 22/1000 | Loss: 0.00344967
Iteration 23/1000 | Loss: 0.00103128
Iteration 24/1000 | Loss: 0.00102341
Iteration 25/1000 | Loss: 0.00134353
Iteration 26/1000 | Loss: 0.00044256
Iteration 27/1000 | Loss: 0.00057107
Iteration 28/1000 | Loss: 0.00034941
Iteration 29/1000 | Loss: 0.00066733
Iteration 30/1000 | Loss: 0.00039661
Iteration 31/1000 | Loss: 0.00057998
Iteration 32/1000 | Loss: 0.00010964
Iteration 33/1000 | Loss: 0.00018817
Iteration 34/1000 | Loss: 0.00050147
Iteration 35/1000 | Loss: 0.00074476
Iteration 36/1000 | Loss: 0.00029740
Iteration 37/1000 | Loss: 0.00015719
Iteration 38/1000 | Loss: 0.00010561
Iteration 39/1000 | Loss: 0.00026568
Iteration 40/1000 | Loss: 0.00016278
Iteration 41/1000 | Loss: 0.00019620
Iteration 42/1000 | Loss: 0.00005322
Iteration 43/1000 | Loss: 0.00023658
Iteration 44/1000 | Loss: 0.00030566
Iteration 45/1000 | Loss: 0.00028328
Iteration 46/1000 | Loss: 0.00030439
Iteration 47/1000 | Loss: 0.00030478
Iteration 48/1000 | Loss: 0.00015306
Iteration 49/1000 | Loss: 0.00003909
Iteration 50/1000 | Loss: 0.00003500
Iteration 51/1000 | Loss: 0.00003246
Iteration 52/1000 | Loss: 0.00003091
Iteration 53/1000 | Loss: 0.00016824
Iteration 54/1000 | Loss: 0.00031645
Iteration 55/1000 | Loss: 0.00037206
Iteration 56/1000 | Loss: 0.00032012
Iteration 57/1000 | Loss: 0.00033819
Iteration 58/1000 | Loss: 0.00003519
Iteration 59/1000 | Loss: 0.00003016
Iteration 60/1000 | Loss: 0.00026955
Iteration 61/1000 | Loss: 0.00019094
Iteration 62/1000 | Loss: 0.00033916
Iteration 63/1000 | Loss: 0.00012820
Iteration 64/1000 | Loss: 0.00003730
Iteration 65/1000 | Loss: 0.00003321
Iteration 66/1000 | Loss: 0.00025497
Iteration 67/1000 | Loss: 0.00036270
Iteration 68/1000 | Loss: 0.00040030
Iteration 69/1000 | Loss: 0.00017715
Iteration 70/1000 | Loss: 0.00054868
Iteration 71/1000 | Loss: 0.00020678
Iteration 72/1000 | Loss: 0.00022475
Iteration 73/1000 | Loss: 0.00017202
Iteration 74/1000 | Loss: 0.00003066
Iteration 75/1000 | Loss: 0.00006091
Iteration 76/1000 | Loss: 0.00027634
Iteration 77/1000 | Loss: 0.00038798
Iteration 78/1000 | Loss: 0.00038070
Iteration 79/1000 | Loss: 0.00005962
Iteration 80/1000 | Loss: 0.00013663
Iteration 81/1000 | Loss: 0.00003673
Iteration 82/1000 | Loss: 0.00002972
Iteration 83/1000 | Loss: 0.00002875
Iteration 84/1000 | Loss: 0.00074320
Iteration 85/1000 | Loss: 0.00033539
Iteration 86/1000 | Loss: 0.00017281
Iteration 87/1000 | Loss: 0.00067443
Iteration 88/1000 | Loss: 0.00032693
Iteration 89/1000 | Loss: 0.00061991
Iteration 90/1000 | Loss: 0.00032646
Iteration 91/1000 | Loss: 0.00058873
Iteration 92/1000 | Loss: 0.00010837
Iteration 93/1000 | Loss: 0.00024104
Iteration 94/1000 | Loss: 0.00016862
Iteration 95/1000 | Loss: 0.00017472
Iteration 96/1000 | Loss: 0.00008489
Iteration 97/1000 | Loss: 0.00003626
Iteration 98/1000 | Loss: 0.00003346
Iteration 99/1000 | Loss: 0.00003330
Iteration 100/1000 | Loss: 0.00003024
Iteration 101/1000 | Loss: 0.00026602
Iteration 102/1000 | Loss: 0.00021057
Iteration 103/1000 | Loss: 0.00023959
Iteration 104/1000 | Loss: 0.00045223
Iteration 105/1000 | Loss: 0.00014860
Iteration 106/1000 | Loss: 0.00008313
Iteration 107/1000 | Loss: 0.00003475
Iteration 108/1000 | Loss: 0.00003118
Iteration 109/1000 | Loss: 0.00003148
Iteration 110/1000 | Loss: 0.00002859
Iteration 111/1000 | Loss: 0.00002714
Iteration 112/1000 | Loss: 0.00002604
Iteration 113/1000 | Loss: 0.00002519
Iteration 114/1000 | Loss: 0.00002716
Iteration 115/1000 | Loss: 0.00002534
Iteration 116/1000 | Loss: 0.00002468
Iteration 117/1000 | Loss: 0.00002712
Iteration 118/1000 | Loss: 0.00002535
Iteration 119/1000 | Loss: 0.00002735
Iteration 120/1000 | Loss: 0.00002562
Iteration 121/1000 | Loss: 0.00002721
Iteration 122/1000 | Loss: 0.00002622
Iteration 123/1000 | Loss: 0.00002716
Iteration 124/1000 | Loss: 0.00002590
Iteration 125/1000 | Loss: 0.00002428
Iteration 126/1000 | Loss: 0.00002617
Iteration 127/1000 | Loss: 0.00002583
Iteration 128/1000 | Loss: 0.00002669
Iteration 129/1000 | Loss: 0.00002604
Iteration 130/1000 | Loss: 0.00002653
Iteration 131/1000 | Loss: 0.00002618
Iteration 132/1000 | Loss: 0.00002692
Iteration 133/1000 | Loss: 0.00002599
Iteration 134/1000 | Loss: 0.00002678
Iteration 135/1000 | Loss: 0.00002614
Iteration 136/1000 | Loss: 0.00002665
Iteration 137/1000 | Loss: 0.00002625
Iteration 138/1000 | Loss: 0.00002646
Iteration 139/1000 | Loss: 0.00002612
Iteration 140/1000 | Loss: 0.00002686
Iteration 141/1000 | Loss: 0.00002600
Iteration 142/1000 | Loss: 0.00002661
Iteration 143/1000 | Loss: 0.00002643
Iteration 144/1000 | Loss: 0.00002652
Iteration 145/1000 | Loss: 0.00002667
Iteration 146/1000 | Loss: 0.00002667
Iteration 147/1000 | Loss: 0.00002677
Iteration 148/1000 | Loss: 0.00002560
Iteration 149/1000 | Loss: 0.00002628
Iteration 150/1000 | Loss: 0.00002621
Iteration 151/1000 | Loss: 0.00002595
Iteration 152/1000 | Loss: 0.00002565
Iteration 153/1000 | Loss: 0.00002594
Iteration 154/1000 | Loss: 0.00002624
Iteration 155/1000 | Loss: 0.00002621
Iteration 156/1000 | Loss: 0.00002640
Iteration 157/1000 | Loss: 0.00002652
Iteration 158/1000 | Loss: 0.00002586
Iteration 159/1000 | Loss: 0.00002629
Iteration 160/1000 | Loss: 0.00002628
Iteration 161/1000 | Loss: 0.00002603
Iteration 162/1000 | Loss: 0.00002606
Iteration 163/1000 | Loss: 0.00002633
Iteration 164/1000 | Loss: 0.00002632
Iteration 165/1000 | Loss: 0.00002490
Iteration 166/1000 | Loss: 0.00002618
Iteration 167/1000 | Loss: 0.00002501
Iteration 168/1000 | Loss: 0.00002629
Iteration 169/1000 | Loss: 0.00002597
Iteration 170/1000 | Loss: 0.00002621
Iteration 171/1000 | Loss: 0.00002678
Iteration 172/1000 | Loss: 0.00002584
Iteration 173/1000 | Loss: 0.00002684
Iteration 174/1000 | Loss: 0.00002590
Iteration 175/1000 | Loss: 0.00002625
Iteration 176/1000 | Loss: 0.00002636
Iteration 177/1000 | Loss: 0.00002647
Iteration 178/1000 | Loss: 0.00002611
Iteration 179/1000 | Loss: 0.00002676
Iteration 180/1000 | Loss: 0.00002690
Iteration 181/1000 | Loss: 0.00002685
Iteration 182/1000 | Loss: 0.00002745
Iteration 183/1000 | Loss: 0.00002668
Iteration 184/1000 | Loss: 0.00002411
Iteration 185/1000 | Loss: 0.00002685
Iteration 186/1000 | Loss: 0.00002458
Iteration 187/1000 | Loss: 0.00002421
Iteration 188/1000 | Loss: 0.00002415
Iteration 189/1000 | Loss: 0.00002414
Iteration 190/1000 | Loss: 0.00002411
Iteration 191/1000 | Loss: 0.00002411
Iteration 192/1000 | Loss: 0.00002411
Iteration 193/1000 | Loss: 0.00002411
Iteration 194/1000 | Loss: 0.00002411
Iteration 195/1000 | Loss: 0.00002411
Iteration 196/1000 | Loss: 0.00002411
Iteration 197/1000 | Loss: 0.00002411
Iteration 198/1000 | Loss: 0.00002411
Iteration 199/1000 | Loss: 0.00002411
Iteration 200/1000 | Loss: 0.00002411
Iteration 201/1000 | Loss: 0.00002411
Iteration 202/1000 | Loss: 0.00002410
Iteration 203/1000 | Loss: 0.00002410
Iteration 204/1000 | Loss: 0.00002410
Iteration 205/1000 | Loss: 0.00002410
Iteration 206/1000 | Loss: 0.00002410
Iteration 207/1000 | Loss: 0.00002410
Iteration 208/1000 | Loss: 0.00002410
Iteration 209/1000 | Loss: 0.00002410
Iteration 210/1000 | Loss: 0.00002410
Iteration 211/1000 | Loss: 0.00002410
Iteration 212/1000 | Loss: 0.00002410
Iteration 213/1000 | Loss: 0.00002410
Iteration 214/1000 | Loss: 0.00002410
Iteration 215/1000 | Loss: 0.00002410
Iteration 216/1000 | Loss: 0.00002410
Iteration 217/1000 | Loss: 0.00002410
Iteration 218/1000 | Loss: 0.00002410
Iteration 219/1000 | Loss: 0.00002410
Iteration 220/1000 | Loss: 0.00002410
Iteration 221/1000 | Loss: 0.00002410
Iteration 222/1000 | Loss: 0.00002410
Iteration 223/1000 | Loss: 0.00002410
Iteration 224/1000 | Loss: 0.00002410
Iteration 225/1000 | Loss: 0.00002410
Iteration 226/1000 | Loss: 0.00002410
Iteration 227/1000 | Loss: 0.00002410
Iteration 228/1000 | Loss: 0.00002410
Iteration 229/1000 | Loss: 0.00002410
Iteration 230/1000 | Loss: 0.00002410
Iteration 231/1000 | Loss: 0.00002410
Iteration 232/1000 | Loss: 0.00002410
Iteration 233/1000 | Loss: 0.00002410
Iteration 234/1000 | Loss: 0.00002410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [2.409809167147614e-05, 2.409809167147614e-05, 2.409809167147614e-05, 2.409809167147614e-05, 2.409809167147614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.409809167147614e-05

Optimization complete. Final v2v error: 4.103960990905762 mm

Highest mean error: 10.859834671020508 mm for frame 107

Lowest mean error: 3.5961532592773438 mm for frame 5

Saving results

Total time: 309.7138874530792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01092779
Iteration 2/25 | Loss: 0.00319300
Iteration 3/25 | Loss: 0.00219549
Iteration 4/25 | Loss: 0.00211882
Iteration 5/25 | Loss: 0.00194792
Iteration 6/25 | Loss: 0.00177921
Iteration 7/25 | Loss: 0.00167524
Iteration 8/25 | Loss: 0.00161548
Iteration 9/25 | Loss: 0.00154936
Iteration 10/25 | Loss: 0.00149924
Iteration 11/25 | Loss: 0.00146479
Iteration 12/25 | Loss: 0.00145807
Iteration 13/25 | Loss: 0.00145012
Iteration 14/25 | Loss: 0.00144428
Iteration 15/25 | Loss: 0.00143457
Iteration 16/25 | Loss: 0.00142962
Iteration 17/25 | Loss: 0.00142102
Iteration 18/25 | Loss: 0.00141954
Iteration 19/25 | Loss: 0.00141884
Iteration 20/25 | Loss: 0.00141447
Iteration 21/25 | Loss: 0.00141132
Iteration 22/25 | Loss: 0.00141314
Iteration 23/25 | Loss: 0.00141801
Iteration 24/25 | Loss: 0.00140912
Iteration 25/25 | Loss: 0.00140767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32572389
Iteration 2/25 | Loss: 0.00394699
Iteration 3/25 | Loss: 0.00394688
Iteration 4/25 | Loss: 0.00394703
Iteration 5/25 | Loss: 0.00394157
Iteration 6/25 | Loss: 0.00394157
Iteration 7/25 | Loss: 0.00394157
Iteration 8/25 | Loss: 0.00394157
Iteration 9/25 | Loss: 0.00394157
Iteration 10/25 | Loss: 0.00394157
Iteration 11/25 | Loss: 0.00394157
Iteration 12/25 | Loss: 0.00394157
Iteration 13/25 | Loss: 0.00394157
Iteration 14/25 | Loss: 0.00394157
Iteration 15/25 | Loss: 0.00394157
Iteration 16/25 | Loss: 0.00394157
Iteration 17/25 | Loss: 0.00394157
Iteration 18/25 | Loss: 0.00394157
Iteration 19/25 | Loss: 0.00394157
Iteration 20/25 | Loss: 0.00394157
Iteration 21/25 | Loss: 0.00394157
Iteration 22/25 | Loss: 0.00394157
Iteration 23/25 | Loss: 0.00394157
Iteration 24/25 | Loss: 0.00394157
Iteration 25/25 | Loss: 0.00394157

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00394157
Iteration 2/1000 | Loss: 0.00071967
Iteration 3/1000 | Loss: 0.00106288
Iteration 4/1000 | Loss: 0.00247053
Iteration 5/1000 | Loss: 0.00144240
Iteration 6/1000 | Loss: 0.00095042
Iteration 7/1000 | Loss: 0.00120659
Iteration 8/1000 | Loss: 0.00092370
Iteration 9/1000 | Loss: 0.00065879
Iteration 10/1000 | Loss: 0.00027618
Iteration 11/1000 | Loss: 0.00129147
Iteration 12/1000 | Loss: 0.00097018
Iteration 13/1000 | Loss: 0.00081047
Iteration 14/1000 | Loss: 0.00083476
Iteration 15/1000 | Loss: 0.00115356
Iteration 16/1000 | Loss: 0.00136329
Iteration 17/1000 | Loss: 0.00045962
Iteration 18/1000 | Loss: 0.00082340
Iteration 19/1000 | Loss: 0.00062650
Iteration 20/1000 | Loss: 0.00252309
Iteration 21/1000 | Loss: 0.00182365
Iteration 22/1000 | Loss: 0.00268080
Iteration 23/1000 | Loss: 0.00091630
Iteration 24/1000 | Loss: 0.00049142
Iteration 25/1000 | Loss: 0.00052452
Iteration 26/1000 | Loss: 0.00089042
Iteration 27/1000 | Loss: 0.00099916
Iteration 28/1000 | Loss: 0.00037629
Iteration 29/1000 | Loss: 0.00075141
Iteration 30/1000 | Loss: 0.00022056
Iteration 31/1000 | Loss: 0.00020617
Iteration 32/1000 | Loss: 0.00064765
Iteration 33/1000 | Loss: 0.00193106
Iteration 34/1000 | Loss: 0.00053560
Iteration 35/1000 | Loss: 0.00133822
Iteration 36/1000 | Loss: 0.00219167
Iteration 37/1000 | Loss: 0.00081855
Iteration 38/1000 | Loss: 0.00023918
Iteration 39/1000 | Loss: 0.00118509
Iteration 40/1000 | Loss: 0.00030276
Iteration 41/1000 | Loss: 0.00090666
Iteration 42/1000 | Loss: 0.00039071
Iteration 43/1000 | Loss: 0.00053632
Iteration 44/1000 | Loss: 0.00033804
Iteration 45/1000 | Loss: 0.00158260
Iteration 46/1000 | Loss: 0.00142195
Iteration 47/1000 | Loss: 0.00065644
Iteration 48/1000 | Loss: 0.00082964
Iteration 49/1000 | Loss: 0.00053920
Iteration 50/1000 | Loss: 0.00107122
Iteration 51/1000 | Loss: 0.00243372
Iteration 52/1000 | Loss: 0.00043797
Iteration 53/1000 | Loss: 0.00029203
Iteration 54/1000 | Loss: 0.00029954
Iteration 55/1000 | Loss: 0.00057600
Iteration 56/1000 | Loss: 0.00012544
Iteration 57/1000 | Loss: 0.00070963
Iteration 58/1000 | Loss: 0.00142877
Iteration 59/1000 | Loss: 0.00051357
Iteration 60/1000 | Loss: 0.00062727
Iteration 61/1000 | Loss: 0.00087845
Iteration 62/1000 | Loss: 0.00025128
Iteration 63/1000 | Loss: 0.00063685
Iteration 64/1000 | Loss: 0.00056196
Iteration 65/1000 | Loss: 0.00046897
Iteration 66/1000 | Loss: 0.00011496
Iteration 67/1000 | Loss: 0.00021958
Iteration 68/1000 | Loss: 0.00024840
Iteration 69/1000 | Loss: 0.00009350
Iteration 70/1000 | Loss: 0.00060307
Iteration 71/1000 | Loss: 0.00010168
Iteration 72/1000 | Loss: 0.00113405
Iteration 73/1000 | Loss: 0.00130581
Iteration 74/1000 | Loss: 0.00038183
Iteration 75/1000 | Loss: 0.00018134
Iteration 76/1000 | Loss: 0.00013864
Iteration 77/1000 | Loss: 0.00012912
Iteration 78/1000 | Loss: 0.00012317
Iteration 79/1000 | Loss: 0.00007450
Iteration 80/1000 | Loss: 0.00071286
Iteration 81/1000 | Loss: 0.00101503
Iteration 82/1000 | Loss: 0.00058348
Iteration 83/1000 | Loss: 0.00034442
Iteration 84/1000 | Loss: 0.00030075
Iteration 85/1000 | Loss: 0.00040705
Iteration 86/1000 | Loss: 0.00023360
Iteration 87/1000 | Loss: 0.00007509
Iteration 88/1000 | Loss: 0.00052413
Iteration 89/1000 | Loss: 0.00055651
Iteration 90/1000 | Loss: 0.00046430
Iteration 91/1000 | Loss: 0.00052483
Iteration 92/1000 | Loss: 0.00021003
Iteration 93/1000 | Loss: 0.00020852
Iteration 94/1000 | Loss: 0.00023943
Iteration 95/1000 | Loss: 0.00053212
Iteration 96/1000 | Loss: 0.00041637
Iteration 97/1000 | Loss: 0.00027062
Iteration 98/1000 | Loss: 0.00020254
Iteration 99/1000 | Loss: 0.00005725
Iteration 100/1000 | Loss: 0.00005520
Iteration 101/1000 | Loss: 0.00010855
Iteration 102/1000 | Loss: 0.00012093
Iteration 103/1000 | Loss: 0.00004960
Iteration 104/1000 | Loss: 0.00009287
Iteration 105/1000 | Loss: 0.00008495
Iteration 106/1000 | Loss: 0.00039616
Iteration 107/1000 | Loss: 0.00012698
Iteration 108/1000 | Loss: 0.00030287
Iteration 109/1000 | Loss: 0.00011161
Iteration 110/1000 | Loss: 0.00022358
Iteration 111/1000 | Loss: 0.00011274
Iteration 112/1000 | Loss: 0.00019078
Iteration 113/1000 | Loss: 0.00006772
Iteration 114/1000 | Loss: 0.00006866
Iteration 115/1000 | Loss: 0.00004553
Iteration 116/1000 | Loss: 0.00005424
Iteration 117/1000 | Loss: 0.00004679
Iteration 118/1000 | Loss: 0.00005281
Iteration 119/1000 | Loss: 0.00004264
Iteration 120/1000 | Loss: 0.00037031
Iteration 121/1000 | Loss: 0.00005210
Iteration 122/1000 | Loss: 0.00005184
Iteration 123/1000 | Loss: 0.00004559
Iteration 124/1000 | Loss: 0.00004912
Iteration 125/1000 | Loss: 0.00004500
Iteration 126/1000 | Loss: 0.00004892
Iteration 127/1000 | Loss: 0.00004228
Iteration 128/1000 | Loss: 0.00004547
Iteration 129/1000 | Loss: 0.00004186
Iteration 130/1000 | Loss: 0.00032578
Iteration 131/1000 | Loss: 0.00005470
Iteration 132/1000 | Loss: 0.00006744
Iteration 133/1000 | Loss: 0.00004235
Iteration 134/1000 | Loss: 0.00004016
Iteration 135/1000 | Loss: 0.00044363
Iteration 136/1000 | Loss: 0.00013893
Iteration 137/1000 | Loss: 0.00005110
Iteration 138/1000 | Loss: 0.00004043
Iteration 139/1000 | Loss: 0.00005802
Iteration 140/1000 | Loss: 0.00004168
Iteration 141/1000 | Loss: 0.00003732
Iteration 142/1000 | Loss: 0.00003673
Iteration 143/1000 | Loss: 0.00003647
Iteration 144/1000 | Loss: 0.00005127
Iteration 145/1000 | Loss: 0.00003675
Iteration 146/1000 | Loss: 0.00003754
Iteration 147/1000 | Loss: 0.00003754
Iteration 148/1000 | Loss: 0.00005660
Iteration 149/1000 | Loss: 0.00003858
Iteration 150/1000 | Loss: 0.00003598
Iteration 151/1000 | Loss: 0.00003598
Iteration 152/1000 | Loss: 0.00003598
Iteration 153/1000 | Loss: 0.00003598
Iteration 154/1000 | Loss: 0.00003598
Iteration 155/1000 | Loss: 0.00003598
Iteration 156/1000 | Loss: 0.00003598
Iteration 157/1000 | Loss: 0.00003598
Iteration 158/1000 | Loss: 0.00003597
Iteration 159/1000 | Loss: 0.00003597
Iteration 160/1000 | Loss: 0.00003597
Iteration 161/1000 | Loss: 0.00003594
Iteration 162/1000 | Loss: 0.00003594
Iteration 163/1000 | Loss: 0.00003594
Iteration 164/1000 | Loss: 0.00003594
Iteration 165/1000 | Loss: 0.00003594
Iteration 166/1000 | Loss: 0.00003594
Iteration 167/1000 | Loss: 0.00003593
Iteration 168/1000 | Loss: 0.00003593
Iteration 169/1000 | Loss: 0.00003592
Iteration 170/1000 | Loss: 0.00003591
Iteration 171/1000 | Loss: 0.00003591
Iteration 172/1000 | Loss: 0.00003591
Iteration 173/1000 | Loss: 0.00003590
Iteration 174/1000 | Loss: 0.00003590
Iteration 175/1000 | Loss: 0.00003590
Iteration 176/1000 | Loss: 0.00003590
Iteration 177/1000 | Loss: 0.00003590
Iteration 178/1000 | Loss: 0.00003589
Iteration 179/1000 | Loss: 0.00003589
Iteration 180/1000 | Loss: 0.00003589
Iteration 181/1000 | Loss: 0.00003588
Iteration 182/1000 | Loss: 0.00003588
Iteration 183/1000 | Loss: 0.00003588
Iteration 184/1000 | Loss: 0.00003588
Iteration 185/1000 | Loss: 0.00003587
Iteration 186/1000 | Loss: 0.00003587
Iteration 187/1000 | Loss: 0.00003587
Iteration 188/1000 | Loss: 0.00003587
Iteration 189/1000 | Loss: 0.00003587
Iteration 190/1000 | Loss: 0.00003587
Iteration 191/1000 | Loss: 0.00003587
Iteration 192/1000 | Loss: 0.00003587
Iteration 193/1000 | Loss: 0.00003587
Iteration 194/1000 | Loss: 0.00003587
Iteration 195/1000 | Loss: 0.00003587
Iteration 196/1000 | Loss: 0.00003586
Iteration 197/1000 | Loss: 0.00003586
Iteration 198/1000 | Loss: 0.00003586
Iteration 199/1000 | Loss: 0.00003586
Iteration 200/1000 | Loss: 0.00003586
Iteration 201/1000 | Loss: 0.00004667
Iteration 202/1000 | Loss: 0.00004106
Iteration 203/1000 | Loss: 0.00003585
Iteration 204/1000 | Loss: 0.00003585
Iteration 205/1000 | Loss: 0.00003584
Iteration 206/1000 | Loss: 0.00003584
Iteration 207/1000 | Loss: 0.00003584
Iteration 208/1000 | Loss: 0.00003583
Iteration 209/1000 | Loss: 0.00003583
Iteration 210/1000 | Loss: 0.00003583
Iteration 211/1000 | Loss: 0.00003583
Iteration 212/1000 | Loss: 0.00003583
Iteration 213/1000 | Loss: 0.00003583
Iteration 214/1000 | Loss: 0.00003583
Iteration 215/1000 | Loss: 0.00003583
Iteration 216/1000 | Loss: 0.00003583
Iteration 217/1000 | Loss: 0.00003583
Iteration 218/1000 | Loss: 0.00003583
Iteration 219/1000 | Loss: 0.00003583
Iteration 220/1000 | Loss: 0.00003583
Iteration 221/1000 | Loss: 0.00003583
Iteration 222/1000 | Loss: 0.00003583
Iteration 223/1000 | Loss: 0.00003583
Iteration 224/1000 | Loss: 0.00003583
Iteration 225/1000 | Loss: 0.00003583
Iteration 226/1000 | Loss: 0.00003583
Iteration 227/1000 | Loss: 0.00003583
Iteration 228/1000 | Loss: 0.00003583
Iteration 229/1000 | Loss: 0.00003583
Iteration 230/1000 | Loss: 0.00003583
Iteration 231/1000 | Loss: 0.00003583
Iteration 232/1000 | Loss: 0.00003583
Iteration 233/1000 | Loss: 0.00003583
Iteration 234/1000 | Loss: 0.00003583
Iteration 235/1000 | Loss: 0.00003583
Iteration 236/1000 | Loss: 0.00003583
Iteration 237/1000 | Loss: 0.00003583
Iteration 238/1000 | Loss: 0.00003583
Iteration 239/1000 | Loss: 0.00003583
Iteration 240/1000 | Loss: 0.00003583
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 240. Stopping optimization.
Last 5 losses: [3.5825018130708486e-05, 3.5825018130708486e-05, 3.5825018130708486e-05, 3.5825018130708486e-05, 3.5825018130708486e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5825018130708486e-05

Optimization complete. Final v2v error: 4.45048713684082 mm

Highest mean error: 19.80786895751953 mm for frame 6

Lowest mean error: 3.485267400741577 mm for frame 199

Saving results

Total time: 302.9790711402893
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00972129
Iteration 2/25 | Loss: 0.00140221
Iteration 3/25 | Loss: 0.00128059
Iteration 4/25 | Loss: 0.00126810
Iteration 5/25 | Loss: 0.00126355
Iteration 6/25 | Loss: 0.00126213
Iteration 7/25 | Loss: 0.00126155
Iteration 8/25 | Loss: 0.00126155
Iteration 9/25 | Loss: 0.00126154
Iteration 10/25 | Loss: 0.00126154
Iteration 11/25 | Loss: 0.00126154
Iteration 12/25 | Loss: 0.00126154
Iteration 13/25 | Loss: 0.00126154
Iteration 14/25 | Loss: 0.00126154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012615392915904522, 0.0012615392915904522, 0.0012615392915904522, 0.0012615392915904522, 0.0012615392915904522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012615392915904522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.87657309
Iteration 2/25 | Loss: 0.00095687
Iteration 3/25 | Loss: 0.00095687
Iteration 4/25 | Loss: 0.00095687
Iteration 5/25 | Loss: 0.00095687
Iteration 6/25 | Loss: 0.00095687
Iteration 7/25 | Loss: 0.00095687
Iteration 8/25 | Loss: 0.00095687
Iteration 9/25 | Loss: 0.00095686
Iteration 10/25 | Loss: 0.00095686
Iteration 11/25 | Loss: 0.00095686
Iteration 12/25 | Loss: 0.00095686
Iteration 13/25 | Loss: 0.00095686
Iteration 14/25 | Loss: 0.00095686
Iteration 15/25 | Loss: 0.00095686
Iteration 16/25 | Loss: 0.00095686
Iteration 17/25 | Loss: 0.00095686
Iteration 18/25 | Loss: 0.00095686
Iteration 19/25 | Loss: 0.00095686
Iteration 20/25 | Loss: 0.00095686
Iteration 21/25 | Loss: 0.00095686
Iteration 22/25 | Loss: 0.00095686
Iteration 23/25 | Loss: 0.00095686
Iteration 24/25 | Loss: 0.00095686
Iteration 25/25 | Loss: 0.00095686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095686
Iteration 2/1000 | Loss: 0.00005094
Iteration 3/1000 | Loss: 0.00002876
Iteration 4/1000 | Loss: 0.00002418
Iteration 5/1000 | Loss: 0.00002151
Iteration 6/1000 | Loss: 0.00002030
Iteration 7/1000 | Loss: 0.00001975
Iteration 8/1000 | Loss: 0.00001935
Iteration 9/1000 | Loss: 0.00001921
Iteration 10/1000 | Loss: 0.00001902
Iteration 11/1000 | Loss: 0.00001901
Iteration 12/1000 | Loss: 0.00001889
Iteration 13/1000 | Loss: 0.00001873
Iteration 14/1000 | Loss: 0.00001872
Iteration 15/1000 | Loss: 0.00001871
Iteration 16/1000 | Loss: 0.00001870
Iteration 17/1000 | Loss: 0.00001869
Iteration 18/1000 | Loss: 0.00001863
Iteration 19/1000 | Loss: 0.00001862
Iteration 20/1000 | Loss: 0.00001862
Iteration 21/1000 | Loss: 0.00001861
Iteration 22/1000 | Loss: 0.00001860
Iteration 23/1000 | Loss: 0.00001860
Iteration 24/1000 | Loss: 0.00001860
Iteration 25/1000 | Loss: 0.00001859
Iteration 26/1000 | Loss: 0.00001858
Iteration 27/1000 | Loss: 0.00001858
Iteration 28/1000 | Loss: 0.00001856
Iteration 29/1000 | Loss: 0.00001856
Iteration 30/1000 | Loss: 0.00001855
Iteration 31/1000 | Loss: 0.00001855
Iteration 32/1000 | Loss: 0.00001855
Iteration 33/1000 | Loss: 0.00001855
Iteration 34/1000 | Loss: 0.00001855
Iteration 35/1000 | Loss: 0.00001855
Iteration 36/1000 | Loss: 0.00001855
Iteration 37/1000 | Loss: 0.00001855
Iteration 38/1000 | Loss: 0.00001855
Iteration 39/1000 | Loss: 0.00001855
Iteration 40/1000 | Loss: 0.00001855
Iteration 41/1000 | Loss: 0.00001854
Iteration 42/1000 | Loss: 0.00001854
Iteration 43/1000 | Loss: 0.00001854
Iteration 44/1000 | Loss: 0.00001854
Iteration 45/1000 | Loss: 0.00001854
Iteration 46/1000 | Loss: 0.00001853
Iteration 47/1000 | Loss: 0.00001853
Iteration 48/1000 | Loss: 0.00001852
Iteration 49/1000 | Loss: 0.00001852
Iteration 50/1000 | Loss: 0.00001852
Iteration 51/1000 | Loss: 0.00001852
Iteration 52/1000 | Loss: 0.00001852
Iteration 53/1000 | Loss: 0.00001852
Iteration 54/1000 | Loss: 0.00001852
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001852
Iteration 59/1000 | Loss: 0.00001852
Iteration 60/1000 | Loss: 0.00001851
Iteration 61/1000 | Loss: 0.00001851
Iteration 62/1000 | Loss: 0.00001851
Iteration 63/1000 | Loss: 0.00001851
Iteration 64/1000 | Loss: 0.00001851
Iteration 65/1000 | Loss: 0.00001851
Iteration 66/1000 | Loss: 0.00001851
Iteration 67/1000 | Loss: 0.00001851
Iteration 68/1000 | Loss: 0.00001851
Iteration 69/1000 | Loss: 0.00001851
Iteration 70/1000 | Loss: 0.00001851
Iteration 71/1000 | Loss: 0.00001851
Iteration 72/1000 | Loss: 0.00001851
Iteration 73/1000 | Loss: 0.00001851
Iteration 74/1000 | Loss: 0.00001851
Iteration 75/1000 | Loss: 0.00001851
Iteration 76/1000 | Loss: 0.00001851
Iteration 77/1000 | Loss: 0.00001851
Iteration 78/1000 | Loss: 0.00001851
Iteration 79/1000 | Loss: 0.00001850
Iteration 80/1000 | Loss: 0.00001850
Iteration 81/1000 | Loss: 0.00001850
Iteration 82/1000 | Loss: 0.00001850
Iteration 83/1000 | Loss: 0.00001850
Iteration 84/1000 | Loss: 0.00001850
Iteration 85/1000 | Loss: 0.00001850
Iteration 86/1000 | Loss: 0.00001850
Iteration 87/1000 | Loss: 0.00001850
Iteration 88/1000 | Loss: 0.00001850
Iteration 89/1000 | Loss: 0.00001850
Iteration 90/1000 | Loss: 0.00001850
Iteration 91/1000 | Loss: 0.00001850
Iteration 92/1000 | Loss: 0.00001850
Iteration 93/1000 | Loss: 0.00001850
Iteration 94/1000 | Loss: 0.00001850
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.8502551029087044e-05, 1.8502551029087044e-05, 1.8502551029087044e-05, 1.8502551029087044e-05, 1.8502551029087044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8502551029087044e-05

Optimization complete. Final v2v error: 3.7528154850006104 mm

Highest mean error: 4.034787178039551 mm for frame 64

Lowest mean error: 3.3922154903411865 mm for frame 0

Saving results

Total time: 31.755765199661255
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00740392
Iteration 2/25 | Loss: 0.00177753
Iteration 3/25 | Loss: 0.00142191
Iteration 4/25 | Loss: 0.00136492
Iteration 5/25 | Loss: 0.00136867
Iteration 6/25 | Loss: 0.00135364
Iteration 7/25 | Loss: 0.00134869
Iteration 8/25 | Loss: 0.00133127
Iteration 9/25 | Loss: 0.00132095
Iteration 10/25 | Loss: 0.00131735
Iteration 11/25 | Loss: 0.00131712
Iteration 12/25 | Loss: 0.00131712
Iteration 13/25 | Loss: 0.00131712
Iteration 14/25 | Loss: 0.00131712
Iteration 15/25 | Loss: 0.00131711
Iteration 16/25 | Loss: 0.00131711
Iteration 17/25 | Loss: 0.00131711
Iteration 18/25 | Loss: 0.00131711
Iteration 19/25 | Loss: 0.00131711
Iteration 20/25 | Loss: 0.00131711
Iteration 21/25 | Loss: 0.00131711
Iteration 22/25 | Loss: 0.00131711
Iteration 23/25 | Loss: 0.00131711
Iteration 24/25 | Loss: 0.00131711
Iteration 25/25 | Loss: 0.00131711

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.16631532
Iteration 2/25 | Loss: 0.00094760
Iteration 3/25 | Loss: 0.00094733
Iteration 4/25 | Loss: 0.00094733
Iteration 5/25 | Loss: 0.00094733
Iteration 6/25 | Loss: 0.00094733
Iteration 7/25 | Loss: 0.00094733
Iteration 8/25 | Loss: 0.00094733
Iteration 9/25 | Loss: 0.00094733
Iteration 10/25 | Loss: 0.00094733
Iteration 11/25 | Loss: 0.00094733
Iteration 12/25 | Loss: 0.00094733
Iteration 13/25 | Loss: 0.00094733
Iteration 14/25 | Loss: 0.00094733
Iteration 15/25 | Loss: 0.00094733
Iteration 16/25 | Loss: 0.00094733
Iteration 17/25 | Loss: 0.00094733
Iteration 18/25 | Loss: 0.00094733
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009473253157921135, 0.0009473253157921135, 0.0009473253157921135, 0.0009473253157921135, 0.0009473253157921135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009473253157921135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094733
Iteration 2/1000 | Loss: 0.00007449
Iteration 3/1000 | Loss: 0.00005118
Iteration 4/1000 | Loss: 0.00004378
Iteration 5/1000 | Loss: 0.00004004
Iteration 6/1000 | Loss: 0.00003717
Iteration 7/1000 | Loss: 0.00003540
Iteration 8/1000 | Loss: 0.00003444
Iteration 9/1000 | Loss: 0.00003373
Iteration 10/1000 | Loss: 0.00003300
Iteration 11/1000 | Loss: 0.00003237
Iteration 12/1000 | Loss: 0.00003195
Iteration 13/1000 | Loss: 0.00003159
Iteration 14/1000 | Loss: 0.00003127
Iteration 15/1000 | Loss: 0.00003102
Iteration 16/1000 | Loss: 0.00003078
Iteration 17/1000 | Loss: 0.00003062
Iteration 18/1000 | Loss: 0.00003046
Iteration 19/1000 | Loss: 0.00003046
Iteration 20/1000 | Loss: 0.00003045
Iteration 21/1000 | Loss: 0.00003045
Iteration 22/1000 | Loss: 0.00003039
Iteration 23/1000 | Loss: 0.00003039
Iteration 24/1000 | Loss: 0.00003039
Iteration 25/1000 | Loss: 0.00003038
Iteration 26/1000 | Loss: 0.00003038
Iteration 27/1000 | Loss: 0.00003037
Iteration 28/1000 | Loss: 0.00003037
Iteration 29/1000 | Loss: 0.00003035
Iteration 30/1000 | Loss: 0.00003034
Iteration 31/1000 | Loss: 0.00003034
Iteration 32/1000 | Loss: 0.00003034
Iteration 33/1000 | Loss: 0.00003033
Iteration 34/1000 | Loss: 0.00003033
Iteration 35/1000 | Loss: 0.00003033
Iteration 36/1000 | Loss: 0.00003032
Iteration 37/1000 | Loss: 0.00003032
Iteration 38/1000 | Loss: 0.00003030
Iteration 39/1000 | Loss: 0.00003030
Iteration 40/1000 | Loss: 0.00003029
Iteration 41/1000 | Loss: 0.00003029
Iteration 42/1000 | Loss: 0.00003029
Iteration 43/1000 | Loss: 0.00003029
Iteration 44/1000 | Loss: 0.00003028
Iteration 45/1000 | Loss: 0.00003028
Iteration 46/1000 | Loss: 0.00003028
Iteration 47/1000 | Loss: 0.00003027
Iteration 48/1000 | Loss: 0.00003025
Iteration 49/1000 | Loss: 0.00003025
Iteration 50/1000 | Loss: 0.00003025
Iteration 51/1000 | Loss: 0.00003025
Iteration 52/1000 | Loss: 0.00003025
Iteration 53/1000 | Loss: 0.00003025
Iteration 54/1000 | Loss: 0.00003025
Iteration 55/1000 | Loss: 0.00003024
Iteration 56/1000 | Loss: 0.00003024
Iteration 57/1000 | Loss: 0.00003024
Iteration 58/1000 | Loss: 0.00003023
Iteration 59/1000 | Loss: 0.00003023
Iteration 60/1000 | Loss: 0.00003022
Iteration 61/1000 | Loss: 0.00003022
Iteration 62/1000 | Loss: 0.00003022
Iteration 63/1000 | Loss: 0.00003022
Iteration 64/1000 | Loss: 0.00003022
Iteration 65/1000 | Loss: 0.00003021
Iteration 66/1000 | Loss: 0.00003021
Iteration 67/1000 | Loss: 0.00003021
Iteration 68/1000 | Loss: 0.00003021
Iteration 69/1000 | Loss: 0.00003021
Iteration 70/1000 | Loss: 0.00003021
Iteration 71/1000 | Loss: 0.00003021
Iteration 72/1000 | Loss: 0.00003021
Iteration 73/1000 | Loss: 0.00003021
Iteration 74/1000 | Loss: 0.00003021
Iteration 75/1000 | Loss: 0.00003021
Iteration 76/1000 | Loss: 0.00003021
Iteration 77/1000 | Loss: 0.00003021
Iteration 78/1000 | Loss: 0.00003021
Iteration 79/1000 | Loss: 0.00003021
Iteration 80/1000 | Loss: 0.00003021
Iteration 81/1000 | Loss: 0.00003021
Iteration 82/1000 | Loss: 0.00003021
Iteration 83/1000 | Loss: 0.00003021
Iteration 84/1000 | Loss: 0.00003021
Iteration 85/1000 | Loss: 0.00003021
Iteration 86/1000 | Loss: 0.00003021
Iteration 87/1000 | Loss: 0.00003021
Iteration 88/1000 | Loss: 0.00003021
Iteration 89/1000 | Loss: 0.00003021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [3.0207826057448983e-05, 3.0207826057448983e-05, 3.0207826057448983e-05, 3.0207826057448983e-05, 3.0207826057448983e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0207826057448983e-05

Optimization complete. Final v2v error: 4.627626419067383 mm

Highest mean error: 6.501450061798096 mm for frame 132

Lowest mean error: 3.783304214477539 mm for frame 172

Saving results

Total time: 57.31064057350159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00476981
Iteration 2/25 | Loss: 0.00147062
Iteration 3/25 | Loss: 0.00127382
Iteration 4/25 | Loss: 0.00125651
Iteration 5/25 | Loss: 0.00124725
Iteration 6/25 | Loss: 0.00124507
Iteration 7/25 | Loss: 0.00124507
Iteration 8/25 | Loss: 0.00124507
Iteration 9/25 | Loss: 0.00124507
Iteration 10/25 | Loss: 0.00124507
Iteration 11/25 | Loss: 0.00124507
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012450654758140445, 0.0012450654758140445, 0.0012450654758140445, 0.0012450654758140445, 0.0012450654758140445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012450654758140445

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32037818
Iteration 2/25 | Loss: 0.00098071
Iteration 3/25 | Loss: 0.00098071
Iteration 4/25 | Loss: 0.00098071
Iteration 5/25 | Loss: 0.00098071
Iteration 6/25 | Loss: 0.00098071
Iteration 7/25 | Loss: 0.00098071
Iteration 8/25 | Loss: 0.00098071
Iteration 9/25 | Loss: 0.00098071
Iteration 10/25 | Loss: 0.00098071
Iteration 11/25 | Loss: 0.00098071
Iteration 12/25 | Loss: 0.00098071
Iteration 13/25 | Loss: 0.00098071
Iteration 14/25 | Loss: 0.00098071
Iteration 15/25 | Loss: 0.00098071
Iteration 16/25 | Loss: 0.00098071
Iteration 17/25 | Loss: 0.00098071
Iteration 18/25 | Loss: 0.00098071
Iteration 19/25 | Loss: 0.00098071
Iteration 20/25 | Loss: 0.00098071
Iteration 21/25 | Loss: 0.00098071
Iteration 22/25 | Loss: 0.00098071
Iteration 23/25 | Loss: 0.00098071
Iteration 24/25 | Loss: 0.00098071
Iteration 25/25 | Loss: 0.00098071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098071
Iteration 2/1000 | Loss: 0.00004580
Iteration 3/1000 | Loss: 0.00003291
Iteration 4/1000 | Loss: 0.00002724
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00002464
Iteration 7/1000 | Loss: 0.00002413
Iteration 8/1000 | Loss: 0.00002382
Iteration 9/1000 | Loss: 0.00002355
Iteration 10/1000 | Loss: 0.00002332
Iteration 11/1000 | Loss: 0.00002326
Iteration 12/1000 | Loss: 0.00002317
Iteration 13/1000 | Loss: 0.00002314
Iteration 14/1000 | Loss: 0.00002313
Iteration 15/1000 | Loss: 0.00002312
Iteration 16/1000 | Loss: 0.00002311
Iteration 17/1000 | Loss: 0.00002308
Iteration 18/1000 | Loss: 0.00002301
Iteration 19/1000 | Loss: 0.00002301
Iteration 20/1000 | Loss: 0.00002300
Iteration 21/1000 | Loss: 0.00002300
Iteration 22/1000 | Loss: 0.00002299
Iteration 23/1000 | Loss: 0.00002298
Iteration 24/1000 | Loss: 0.00002298
Iteration 25/1000 | Loss: 0.00002298
Iteration 26/1000 | Loss: 0.00002298
Iteration 27/1000 | Loss: 0.00002297
Iteration 28/1000 | Loss: 0.00002297
Iteration 29/1000 | Loss: 0.00002297
Iteration 30/1000 | Loss: 0.00002297
Iteration 31/1000 | Loss: 0.00002297
Iteration 32/1000 | Loss: 0.00002297
Iteration 33/1000 | Loss: 0.00002296
Iteration 34/1000 | Loss: 0.00002296
Iteration 35/1000 | Loss: 0.00002296
Iteration 36/1000 | Loss: 0.00002296
Iteration 37/1000 | Loss: 0.00002296
Iteration 38/1000 | Loss: 0.00002296
Iteration 39/1000 | Loss: 0.00002296
Iteration 40/1000 | Loss: 0.00002296
Iteration 41/1000 | Loss: 0.00002296
Iteration 42/1000 | Loss: 0.00002296
Iteration 43/1000 | Loss: 0.00002296
Iteration 44/1000 | Loss: 0.00002295
Iteration 45/1000 | Loss: 0.00002295
Iteration 46/1000 | Loss: 0.00002295
Iteration 47/1000 | Loss: 0.00002295
Iteration 48/1000 | Loss: 0.00002295
Iteration 49/1000 | Loss: 0.00002295
Iteration 50/1000 | Loss: 0.00002295
Iteration 51/1000 | Loss: 0.00002295
Iteration 52/1000 | Loss: 0.00002295
Iteration 53/1000 | Loss: 0.00002295
Iteration 54/1000 | Loss: 0.00002295
Iteration 55/1000 | Loss: 0.00002295
Iteration 56/1000 | Loss: 0.00002295
Iteration 57/1000 | Loss: 0.00002295
Iteration 58/1000 | Loss: 0.00002295
Iteration 59/1000 | Loss: 0.00002295
Iteration 60/1000 | Loss: 0.00002295
Iteration 61/1000 | Loss: 0.00002294
Iteration 62/1000 | Loss: 0.00002294
Iteration 63/1000 | Loss: 0.00002294
Iteration 64/1000 | Loss: 0.00002294
Iteration 65/1000 | Loss: 0.00002294
Iteration 66/1000 | Loss: 0.00002294
Iteration 67/1000 | Loss: 0.00002294
Iteration 68/1000 | Loss: 0.00002294
Iteration 69/1000 | Loss: 0.00002294
Iteration 70/1000 | Loss: 0.00002294
Iteration 71/1000 | Loss: 0.00002294
Iteration 72/1000 | Loss: 0.00002294
Iteration 73/1000 | Loss: 0.00002294
Iteration 74/1000 | Loss: 0.00002294
Iteration 75/1000 | Loss: 0.00002294
Iteration 76/1000 | Loss: 0.00002294
Iteration 77/1000 | Loss: 0.00002294
Iteration 78/1000 | Loss: 0.00002294
Iteration 79/1000 | Loss: 0.00002294
Iteration 80/1000 | Loss: 0.00002294
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [2.2937860194360837e-05, 2.2937860194360837e-05, 2.2937860194360837e-05, 2.2937860194360837e-05, 2.2937860194360837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2937860194360837e-05

Optimization complete. Final v2v error: 4.218568325042725 mm

Highest mean error: 4.57076358795166 mm for frame 186

Lowest mean error: 3.7810463905334473 mm for frame 1

Saving results

Total time: 35.51504063606262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00922311
Iteration 2/25 | Loss: 0.00165634
Iteration 3/25 | Loss: 0.00132350
Iteration 4/25 | Loss: 0.00128371
Iteration 5/25 | Loss: 0.00127429
Iteration 6/25 | Loss: 0.00127215
Iteration 7/25 | Loss: 0.00127209
Iteration 8/25 | Loss: 0.00127209
Iteration 9/25 | Loss: 0.00127209
Iteration 10/25 | Loss: 0.00127209
Iteration 11/25 | Loss: 0.00127209
Iteration 12/25 | Loss: 0.00127209
Iteration 13/25 | Loss: 0.00127209
Iteration 14/25 | Loss: 0.00127209
Iteration 15/25 | Loss: 0.00127209
Iteration 16/25 | Loss: 0.00127209
Iteration 17/25 | Loss: 0.00127209
Iteration 18/25 | Loss: 0.00127209
Iteration 19/25 | Loss: 0.00127209
Iteration 20/25 | Loss: 0.00127209
Iteration 21/25 | Loss: 0.00127209
Iteration 22/25 | Loss: 0.00127209
Iteration 23/25 | Loss: 0.00127209
Iteration 24/25 | Loss: 0.00127209
Iteration 25/25 | Loss: 0.00127209

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35550225
Iteration 2/25 | Loss: 0.00099341
Iteration 3/25 | Loss: 0.00099341
Iteration 4/25 | Loss: 0.00099340
Iteration 5/25 | Loss: 0.00099340
Iteration 6/25 | Loss: 0.00099340
Iteration 7/25 | Loss: 0.00099340
Iteration 8/25 | Loss: 0.00099340
Iteration 9/25 | Loss: 0.00099340
Iteration 10/25 | Loss: 0.00099340
Iteration 11/25 | Loss: 0.00099340
Iteration 12/25 | Loss: 0.00099340
Iteration 13/25 | Loss: 0.00099340
Iteration 14/25 | Loss: 0.00099340
Iteration 15/25 | Loss: 0.00099340
Iteration 16/25 | Loss: 0.00099340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0009934022091329098, 0.0009934022091329098, 0.0009934022091329098, 0.0009934022091329098, 0.0009934022091329098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009934022091329098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099340
Iteration 2/1000 | Loss: 0.00004282
Iteration 3/1000 | Loss: 0.00003385
Iteration 4/1000 | Loss: 0.00002979
Iteration 5/1000 | Loss: 0.00002778
Iteration 6/1000 | Loss: 0.00002683
Iteration 7/1000 | Loss: 0.00002634
Iteration 8/1000 | Loss: 0.00002580
Iteration 9/1000 | Loss: 0.00002543
Iteration 10/1000 | Loss: 0.00002522
Iteration 11/1000 | Loss: 0.00002507
Iteration 12/1000 | Loss: 0.00002497
Iteration 13/1000 | Loss: 0.00002490
Iteration 14/1000 | Loss: 0.00002490
Iteration 15/1000 | Loss: 0.00002490
Iteration 16/1000 | Loss: 0.00002489
Iteration 17/1000 | Loss: 0.00002488
Iteration 18/1000 | Loss: 0.00002487
Iteration 19/1000 | Loss: 0.00002486
Iteration 20/1000 | Loss: 0.00002486
Iteration 21/1000 | Loss: 0.00002485
Iteration 22/1000 | Loss: 0.00002485
Iteration 23/1000 | Loss: 0.00002485
Iteration 24/1000 | Loss: 0.00002485
Iteration 25/1000 | Loss: 0.00002484
Iteration 26/1000 | Loss: 0.00002484
Iteration 27/1000 | Loss: 0.00002483
Iteration 28/1000 | Loss: 0.00002482
Iteration 29/1000 | Loss: 0.00002481
Iteration 30/1000 | Loss: 0.00002481
Iteration 31/1000 | Loss: 0.00002480
Iteration 32/1000 | Loss: 0.00002480
Iteration 33/1000 | Loss: 0.00002479
Iteration 34/1000 | Loss: 0.00002478
Iteration 35/1000 | Loss: 0.00002477
Iteration 36/1000 | Loss: 0.00002477
Iteration 37/1000 | Loss: 0.00002477
Iteration 38/1000 | Loss: 0.00002477
Iteration 39/1000 | Loss: 0.00002477
Iteration 40/1000 | Loss: 0.00002476
Iteration 41/1000 | Loss: 0.00002476
Iteration 42/1000 | Loss: 0.00002474
Iteration 43/1000 | Loss: 0.00002474
Iteration 44/1000 | Loss: 0.00002473
Iteration 45/1000 | Loss: 0.00002472
Iteration 46/1000 | Loss: 0.00002472
Iteration 47/1000 | Loss: 0.00002472
Iteration 48/1000 | Loss: 0.00002471
Iteration 49/1000 | Loss: 0.00002469
Iteration 50/1000 | Loss: 0.00002468
Iteration 51/1000 | Loss: 0.00002468
Iteration 52/1000 | Loss: 0.00002468
Iteration 53/1000 | Loss: 0.00002467
Iteration 54/1000 | Loss: 0.00002467
Iteration 55/1000 | Loss: 0.00002466
Iteration 56/1000 | Loss: 0.00002466
Iteration 57/1000 | Loss: 0.00002465
Iteration 58/1000 | Loss: 0.00002464
Iteration 59/1000 | Loss: 0.00002464
Iteration 60/1000 | Loss: 0.00002464
Iteration 61/1000 | Loss: 0.00002463
Iteration 62/1000 | Loss: 0.00002463
Iteration 63/1000 | Loss: 0.00002463
Iteration 64/1000 | Loss: 0.00002463
Iteration 65/1000 | Loss: 0.00002462
Iteration 66/1000 | Loss: 0.00002462
Iteration 67/1000 | Loss: 0.00002462
Iteration 68/1000 | Loss: 0.00002461
Iteration 69/1000 | Loss: 0.00002461
Iteration 70/1000 | Loss: 0.00002461
Iteration 71/1000 | Loss: 0.00002461
Iteration 72/1000 | Loss: 0.00002460
Iteration 73/1000 | Loss: 0.00002459
Iteration 74/1000 | Loss: 0.00002459
Iteration 75/1000 | Loss: 0.00002459
Iteration 76/1000 | Loss: 0.00002459
Iteration 77/1000 | Loss: 0.00002458
Iteration 78/1000 | Loss: 0.00002458
Iteration 79/1000 | Loss: 0.00002458
Iteration 80/1000 | Loss: 0.00002458
Iteration 81/1000 | Loss: 0.00002458
Iteration 82/1000 | Loss: 0.00002458
Iteration 83/1000 | Loss: 0.00002458
Iteration 84/1000 | Loss: 0.00002458
Iteration 85/1000 | Loss: 0.00002458
Iteration 86/1000 | Loss: 0.00002458
Iteration 87/1000 | Loss: 0.00002458
Iteration 88/1000 | Loss: 0.00002458
Iteration 89/1000 | Loss: 0.00002458
Iteration 90/1000 | Loss: 0.00002458
Iteration 91/1000 | Loss: 0.00002458
Iteration 92/1000 | Loss: 0.00002458
Iteration 93/1000 | Loss: 0.00002458
Iteration 94/1000 | Loss: 0.00002458
Iteration 95/1000 | Loss: 0.00002458
Iteration 96/1000 | Loss: 0.00002458
Iteration 97/1000 | Loss: 0.00002458
Iteration 98/1000 | Loss: 0.00002458
Iteration 99/1000 | Loss: 0.00002458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [2.457586015225388e-05, 2.457586015225388e-05, 2.457586015225388e-05, 2.457586015225388e-05, 2.457586015225388e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.457586015225388e-05

Optimization complete. Final v2v error: 4.3200154304504395 mm

Highest mean error: 4.524839878082275 mm for frame 167

Lowest mean error: 4.0010552406311035 mm for frame 266

Saving results

Total time: 37.96361780166626
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01075301
Iteration 2/25 | Loss: 0.00291611
Iteration 3/25 | Loss: 0.00201171
Iteration 4/25 | Loss: 0.00186829
Iteration 5/25 | Loss: 0.00182738
Iteration 6/25 | Loss: 0.00175100
Iteration 7/25 | Loss: 0.00174522
Iteration 8/25 | Loss: 0.00170444
Iteration 9/25 | Loss: 0.00180311
Iteration 10/25 | Loss: 0.00167133
Iteration 11/25 | Loss: 0.00161500
Iteration 12/25 | Loss: 0.00153870
Iteration 13/25 | Loss: 0.00151475
Iteration 14/25 | Loss: 0.00149665
Iteration 15/25 | Loss: 0.00149868
Iteration 16/25 | Loss: 0.00150123
Iteration 17/25 | Loss: 0.00148796
Iteration 18/25 | Loss: 0.00148647
Iteration 19/25 | Loss: 0.00148650
Iteration 20/25 | Loss: 0.00148933
Iteration 21/25 | Loss: 0.00148892
Iteration 22/25 | Loss: 0.00148155
Iteration 23/25 | Loss: 0.00148276
Iteration 24/25 | Loss: 0.00148611
Iteration 25/25 | Loss: 0.00148276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36558306
Iteration 2/25 | Loss: 0.00223869
Iteration 3/25 | Loss: 0.00223869
Iteration 4/25 | Loss: 0.00223869
Iteration 5/25 | Loss: 0.00223869
Iteration 6/25 | Loss: 0.00223868
Iteration 7/25 | Loss: 0.00223010
Iteration 8/25 | Loss: 0.00223010
Iteration 9/25 | Loss: 0.00223009
Iteration 10/25 | Loss: 0.00223009
Iteration 11/25 | Loss: 0.00223009
Iteration 12/25 | Loss: 0.00223009
Iteration 13/25 | Loss: 0.00223009
Iteration 14/25 | Loss: 0.00223009
Iteration 15/25 | Loss: 0.00223009
Iteration 16/25 | Loss: 0.00223009
Iteration 17/25 | Loss: 0.00223009
Iteration 18/25 | Loss: 0.00223009
Iteration 19/25 | Loss: 0.00223009
Iteration 20/25 | Loss: 0.00223009
Iteration 21/25 | Loss: 0.00223009
Iteration 22/25 | Loss: 0.00223009
Iteration 23/25 | Loss: 0.00223009
Iteration 24/25 | Loss: 0.00223009
Iteration 25/25 | Loss: 0.00223009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223009
Iteration 2/1000 | Loss: 0.00034176
Iteration 3/1000 | Loss: 0.00057827
Iteration 4/1000 | Loss: 0.00124624
Iteration 5/1000 | Loss: 0.00029328
Iteration 6/1000 | Loss: 0.00017826
Iteration 7/1000 | Loss: 0.00012528
Iteration 8/1000 | Loss: 0.00013163
Iteration 9/1000 | Loss: 0.00015345
Iteration 10/1000 | Loss: 0.00039218
Iteration 11/1000 | Loss: 0.00034660
Iteration 12/1000 | Loss: 0.00090923
Iteration 13/1000 | Loss: 0.00018180
Iteration 14/1000 | Loss: 0.00057205
Iteration 15/1000 | Loss: 0.00021983
Iteration 16/1000 | Loss: 0.00056605
Iteration 17/1000 | Loss: 0.00106540
Iteration 18/1000 | Loss: 0.00014109
Iteration 19/1000 | Loss: 0.00011015
Iteration 20/1000 | Loss: 0.00010240
Iteration 21/1000 | Loss: 0.00010789
Iteration 22/1000 | Loss: 0.00010053
Iteration 23/1000 | Loss: 0.00031181
Iteration 24/1000 | Loss: 0.00025415
Iteration 25/1000 | Loss: 0.00031705
Iteration 26/1000 | Loss: 0.00024689
Iteration 27/1000 | Loss: 0.00008800
Iteration 28/1000 | Loss: 0.00008659
Iteration 29/1000 | Loss: 0.00008537
Iteration 30/1000 | Loss: 0.00008372
Iteration 31/1000 | Loss: 0.00009919
Iteration 32/1000 | Loss: 0.00008492
Iteration 33/1000 | Loss: 0.00008291
Iteration 34/1000 | Loss: 0.00039007
Iteration 35/1000 | Loss: 0.00032062
Iteration 36/1000 | Loss: 0.00037536
Iteration 37/1000 | Loss: 0.00012226
Iteration 38/1000 | Loss: 0.00010148
Iteration 39/1000 | Loss: 0.00009377
Iteration 40/1000 | Loss: 0.00009973
Iteration 41/1000 | Loss: 0.00008672
Iteration 42/1000 | Loss: 0.00008405
Iteration 43/1000 | Loss: 0.00008238
Iteration 44/1000 | Loss: 0.00008140
Iteration 45/1000 | Loss: 0.00030706
Iteration 46/1000 | Loss: 0.00070608
Iteration 47/1000 | Loss: 0.00061807
Iteration 48/1000 | Loss: 0.00065957
Iteration 49/1000 | Loss: 0.00015825
Iteration 50/1000 | Loss: 0.00051362
Iteration 51/1000 | Loss: 0.00066091
Iteration 52/1000 | Loss: 0.00047905
Iteration 53/1000 | Loss: 0.00065006
Iteration 54/1000 | Loss: 0.00039884
Iteration 55/1000 | Loss: 0.00041158
Iteration 56/1000 | Loss: 0.00014076
Iteration 57/1000 | Loss: 0.00029693
Iteration 58/1000 | Loss: 0.00028413
Iteration 59/1000 | Loss: 0.00014478
Iteration 60/1000 | Loss: 0.00031294
Iteration 61/1000 | Loss: 0.00025325
Iteration 62/1000 | Loss: 0.00020365
Iteration 63/1000 | Loss: 0.00031903
Iteration 64/1000 | Loss: 0.00031596
Iteration 65/1000 | Loss: 0.00043489
Iteration 66/1000 | Loss: 0.00011259
Iteration 67/1000 | Loss: 0.00021446
Iteration 68/1000 | Loss: 0.00020168
Iteration 69/1000 | Loss: 0.00022512
Iteration 70/1000 | Loss: 0.00009931
Iteration 71/1000 | Loss: 0.00020621
Iteration 72/1000 | Loss: 0.00011219
Iteration 73/1000 | Loss: 0.00014613
Iteration 74/1000 | Loss: 0.00016603
Iteration 75/1000 | Loss: 0.00031831
Iteration 76/1000 | Loss: 0.00021774
Iteration 77/1000 | Loss: 0.00047407
Iteration 78/1000 | Loss: 0.00029468
Iteration 79/1000 | Loss: 0.00021140
Iteration 80/1000 | Loss: 0.00021387
Iteration 81/1000 | Loss: 0.00017838
Iteration 82/1000 | Loss: 0.00015280
Iteration 83/1000 | Loss: 0.00008878
Iteration 84/1000 | Loss: 0.00028893
Iteration 85/1000 | Loss: 0.00017244
Iteration 86/1000 | Loss: 0.00009273
Iteration 87/1000 | Loss: 0.00029334
Iteration 88/1000 | Loss: 0.00036311
Iteration 89/1000 | Loss: 0.00044896
Iteration 90/1000 | Loss: 0.00024248
Iteration 91/1000 | Loss: 0.00045595
Iteration 92/1000 | Loss: 0.00028577
Iteration 93/1000 | Loss: 0.00021468
Iteration 94/1000 | Loss: 0.00022314
Iteration 95/1000 | Loss: 0.00022121
Iteration 96/1000 | Loss: 0.00013708
Iteration 97/1000 | Loss: 0.00009855
Iteration 98/1000 | Loss: 0.00009009
Iteration 99/1000 | Loss: 0.00008578
Iteration 100/1000 | Loss: 0.00011922
Iteration 101/1000 | Loss: 0.00009533
Iteration 102/1000 | Loss: 0.00008832
Iteration 103/1000 | Loss: 0.00011628
Iteration 104/1000 | Loss: 0.00008553
Iteration 105/1000 | Loss: 0.00008130
Iteration 106/1000 | Loss: 0.00008562
Iteration 107/1000 | Loss: 0.00008117
Iteration 108/1000 | Loss: 0.00007916
Iteration 109/1000 | Loss: 0.00007845
Iteration 110/1000 | Loss: 0.00007802
Iteration 111/1000 | Loss: 0.00007772
Iteration 112/1000 | Loss: 0.00007756
Iteration 113/1000 | Loss: 0.00007739
Iteration 114/1000 | Loss: 0.00007738
Iteration 115/1000 | Loss: 0.00007728
Iteration 116/1000 | Loss: 0.00007726
Iteration 117/1000 | Loss: 0.00007726
Iteration 118/1000 | Loss: 0.00007724
Iteration 119/1000 | Loss: 0.00007724
Iteration 120/1000 | Loss: 0.00007723
Iteration 121/1000 | Loss: 0.00007723
Iteration 122/1000 | Loss: 0.00007722
Iteration 123/1000 | Loss: 0.00007721
Iteration 124/1000 | Loss: 0.00007721
Iteration 125/1000 | Loss: 0.00007721
Iteration 126/1000 | Loss: 0.00007721
Iteration 127/1000 | Loss: 0.00007721
Iteration 128/1000 | Loss: 0.00007720
Iteration 129/1000 | Loss: 0.00007720
Iteration 130/1000 | Loss: 0.00007720
Iteration 131/1000 | Loss: 0.00007720
Iteration 132/1000 | Loss: 0.00007720
Iteration 133/1000 | Loss: 0.00007719
Iteration 134/1000 | Loss: 0.00007718
Iteration 135/1000 | Loss: 0.00007717
Iteration 136/1000 | Loss: 0.00007717
Iteration 137/1000 | Loss: 0.00007717
Iteration 138/1000 | Loss: 0.00007716
Iteration 139/1000 | Loss: 0.00007716
Iteration 140/1000 | Loss: 0.00007716
Iteration 141/1000 | Loss: 0.00007715
Iteration 142/1000 | Loss: 0.00007715
Iteration 143/1000 | Loss: 0.00007715
Iteration 144/1000 | Loss: 0.00007715
Iteration 145/1000 | Loss: 0.00007715
Iteration 146/1000 | Loss: 0.00007715
Iteration 147/1000 | Loss: 0.00007714
Iteration 148/1000 | Loss: 0.00007714
Iteration 149/1000 | Loss: 0.00007714
Iteration 150/1000 | Loss: 0.00007714
Iteration 151/1000 | Loss: 0.00007714
Iteration 152/1000 | Loss: 0.00007713
Iteration 153/1000 | Loss: 0.00007713
Iteration 154/1000 | Loss: 0.00007713
Iteration 155/1000 | Loss: 0.00007713
Iteration 156/1000 | Loss: 0.00007713
Iteration 157/1000 | Loss: 0.00007713
Iteration 158/1000 | Loss: 0.00007712
Iteration 159/1000 | Loss: 0.00007712
Iteration 160/1000 | Loss: 0.00007712
Iteration 161/1000 | Loss: 0.00007712
Iteration 162/1000 | Loss: 0.00007712
Iteration 163/1000 | Loss: 0.00007712
Iteration 164/1000 | Loss: 0.00007712
Iteration 165/1000 | Loss: 0.00007712
Iteration 166/1000 | Loss: 0.00007712
Iteration 167/1000 | Loss: 0.00007712
Iteration 168/1000 | Loss: 0.00007712
Iteration 169/1000 | Loss: 0.00007712
Iteration 170/1000 | Loss: 0.00007711
Iteration 171/1000 | Loss: 0.00007711
Iteration 172/1000 | Loss: 0.00007711
Iteration 173/1000 | Loss: 0.00007711
Iteration 174/1000 | Loss: 0.00007711
Iteration 175/1000 | Loss: 0.00007711
Iteration 176/1000 | Loss: 0.00007711
Iteration 177/1000 | Loss: 0.00007711
Iteration 178/1000 | Loss: 0.00007710
Iteration 179/1000 | Loss: 0.00007710
Iteration 180/1000 | Loss: 0.00007710
Iteration 181/1000 | Loss: 0.00007710
Iteration 182/1000 | Loss: 0.00007710
Iteration 183/1000 | Loss: 0.00007710
Iteration 184/1000 | Loss: 0.00007710
Iteration 185/1000 | Loss: 0.00007710
Iteration 186/1000 | Loss: 0.00007710
Iteration 187/1000 | Loss: 0.00007710
Iteration 188/1000 | Loss: 0.00007710
Iteration 189/1000 | Loss: 0.00007710
Iteration 190/1000 | Loss: 0.00007710
Iteration 191/1000 | Loss: 0.00007710
Iteration 192/1000 | Loss: 0.00007709
Iteration 193/1000 | Loss: 0.00007709
Iteration 194/1000 | Loss: 0.00007709
Iteration 195/1000 | Loss: 0.00007709
Iteration 196/1000 | Loss: 0.00007709
Iteration 197/1000 | Loss: 0.00007709
Iteration 198/1000 | Loss: 0.00007709
Iteration 199/1000 | Loss: 0.00007709
Iteration 200/1000 | Loss: 0.00007709
Iteration 201/1000 | Loss: 0.00007709
Iteration 202/1000 | Loss: 0.00007709
Iteration 203/1000 | Loss: 0.00007709
Iteration 204/1000 | Loss: 0.00007709
Iteration 205/1000 | Loss: 0.00007709
Iteration 206/1000 | Loss: 0.00007709
Iteration 207/1000 | Loss: 0.00007709
Iteration 208/1000 | Loss: 0.00007709
Iteration 209/1000 | Loss: 0.00007709
Iteration 210/1000 | Loss: 0.00007709
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 210. Stopping optimization.
Last 5 losses: [7.708928023930639e-05, 7.708928023930639e-05, 7.708928023930639e-05, 7.708928023930639e-05, 7.708928023930639e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.708928023930639e-05

Optimization complete. Final v2v error: 5.624706745147705 mm

Highest mean error: 12.666824340820312 mm for frame 0

Lowest mean error: 3.810518741607666 mm for frame 38

Saving results

Total time: 209.88385939598083
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00524730
Iteration 2/25 | Loss: 0.00148431
Iteration 3/25 | Loss: 0.00137959
Iteration 4/25 | Loss: 0.00134597
Iteration 5/25 | Loss: 0.00133105
Iteration 6/25 | Loss: 0.00132834
Iteration 7/25 | Loss: 0.00132751
Iteration 8/25 | Loss: 0.00132751
Iteration 9/25 | Loss: 0.00132751
Iteration 10/25 | Loss: 0.00132751
Iteration 11/25 | Loss: 0.00132751
Iteration 12/25 | Loss: 0.00132751
Iteration 13/25 | Loss: 0.00132751
Iteration 14/25 | Loss: 0.00132751
Iteration 15/25 | Loss: 0.00132751
Iteration 16/25 | Loss: 0.00132751
Iteration 17/25 | Loss: 0.00132751
Iteration 18/25 | Loss: 0.00132751
Iteration 19/25 | Loss: 0.00132751
Iteration 20/25 | Loss: 0.00132751
Iteration 21/25 | Loss: 0.00132751
Iteration 22/25 | Loss: 0.00132751
Iteration 23/25 | Loss: 0.00132751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001327505218796432, 0.001327505218796432, 0.001327505218796432, 0.001327505218796432, 0.001327505218796432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001327505218796432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.28371382
Iteration 2/25 | Loss: 0.00095840
Iteration 3/25 | Loss: 0.00095839
Iteration 4/25 | Loss: 0.00095839
Iteration 5/25 | Loss: 0.00095839
Iteration 6/25 | Loss: 0.00095839
Iteration 7/25 | Loss: 0.00095839
Iteration 8/25 | Loss: 0.00095839
Iteration 9/25 | Loss: 0.00095838
Iteration 10/25 | Loss: 0.00095838
Iteration 11/25 | Loss: 0.00095838
Iteration 12/25 | Loss: 0.00095838
Iteration 13/25 | Loss: 0.00095838
Iteration 14/25 | Loss: 0.00095838
Iteration 15/25 | Loss: 0.00095838
Iteration 16/25 | Loss: 0.00095838
Iteration 17/25 | Loss: 0.00095838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009583845385350287, 0.0009583845385350287, 0.0009583845385350287, 0.0009583845385350287, 0.0009583845385350287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009583845385350287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095838
Iteration 2/1000 | Loss: 0.00007048
Iteration 3/1000 | Loss: 0.00004406
Iteration 4/1000 | Loss: 0.00003812
Iteration 5/1000 | Loss: 0.00003483
Iteration 6/1000 | Loss: 0.00003298
Iteration 7/1000 | Loss: 0.00003228
Iteration 8/1000 | Loss: 0.00003166
Iteration 9/1000 | Loss: 0.00003126
Iteration 10/1000 | Loss: 0.00003100
Iteration 11/1000 | Loss: 0.00003074
Iteration 12/1000 | Loss: 0.00003060
Iteration 13/1000 | Loss: 0.00003046
Iteration 14/1000 | Loss: 0.00003041
Iteration 15/1000 | Loss: 0.00003040
Iteration 16/1000 | Loss: 0.00003040
Iteration 17/1000 | Loss: 0.00003039
Iteration 18/1000 | Loss: 0.00003039
Iteration 19/1000 | Loss: 0.00003039
Iteration 20/1000 | Loss: 0.00003038
Iteration 21/1000 | Loss: 0.00003037
Iteration 22/1000 | Loss: 0.00003036
Iteration 23/1000 | Loss: 0.00003036
Iteration 24/1000 | Loss: 0.00003035
Iteration 25/1000 | Loss: 0.00003035
Iteration 26/1000 | Loss: 0.00003034
Iteration 27/1000 | Loss: 0.00003034
Iteration 28/1000 | Loss: 0.00003034
Iteration 29/1000 | Loss: 0.00003032
Iteration 30/1000 | Loss: 0.00003032
Iteration 31/1000 | Loss: 0.00003032
Iteration 32/1000 | Loss: 0.00003032
Iteration 33/1000 | Loss: 0.00003032
Iteration 34/1000 | Loss: 0.00003032
Iteration 35/1000 | Loss: 0.00003031
Iteration 36/1000 | Loss: 0.00003031
Iteration 37/1000 | Loss: 0.00003031
Iteration 38/1000 | Loss: 0.00003031
Iteration 39/1000 | Loss: 0.00003031
Iteration 40/1000 | Loss: 0.00003031
Iteration 41/1000 | Loss: 0.00003029
Iteration 42/1000 | Loss: 0.00003028
Iteration 43/1000 | Loss: 0.00003028
Iteration 44/1000 | Loss: 0.00003028
Iteration 45/1000 | Loss: 0.00003028
Iteration 46/1000 | Loss: 0.00003027
Iteration 47/1000 | Loss: 0.00003027
Iteration 48/1000 | Loss: 0.00003027
Iteration 49/1000 | Loss: 0.00003027
Iteration 50/1000 | Loss: 0.00003026
Iteration 51/1000 | Loss: 0.00003026
Iteration 52/1000 | Loss: 0.00003026
Iteration 53/1000 | Loss: 0.00003026
Iteration 54/1000 | Loss: 0.00003025
Iteration 55/1000 | Loss: 0.00003025
Iteration 56/1000 | Loss: 0.00003025
Iteration 57/1000 | Loss: 0.00003024
Iteration 58/1000 | Loss: 0.00003024
Iteration 59/1000 | Loss: 0.00003024
Iteration 60/1000 | Loss: 0.00003023
Iteration 61/1000 | Loss: 0.00003023
Iteration 62/1000 | Loss: 0.00003023
Iteration 63/1000 | Loss: 0.00003022
Iteration 64/1000 | Loss: 0.00003022
Iteration 65/1000 | Loss: 0.00003022
Iteration 66/1000 | Loss: 0.00003022
Iteration 67/1000 | Loss: 0.00003022
Iteration 68/1000 | Loss: 0.00003022
Iteration 69/1000 | Loss: 0.00003022
Iteration 70/1000 | Loss: 0.00003022
Iteration 71/1000 | Loss: 0.00003022
Iteration 72/1000 | Loss: 0.00003022
Iteration 73/1000 | Loss: 0.00003021
Iteration 74/1000 | Loss: 0.00003021
Iteration 75/1000 | Loss: 0.00003021
Iteration 76/1000 | Loss: 0.00003021
Iteration 77/1000 | Loss: 0.00003021
Iteration 78/1000 | Loss: 0.00003020
Iteration 79/1000 | Loss: 0.00003020
Iteration 80/1000 | Loss: 0.00003020
Iteration 81/1000 | Loss: 0.00003020
Iteration 82/1000 | Loss: 0.00003019
Iteration 83/1000 | Loss: 0.00003019
Iteration 84/1000 | Loss: 0.00003019
Iteration 85/1000 | Loss: 0.00003019
Iteration 86/1000 | Loss: 0.00003019
Iteration 87/1000 | Loss: 0.00003019
Iteration 88/1000 | Loss: 0.00003019
Iteration 89/1000 | Loss: 0.00003019
Iteration 90/1000 | Loss: 0.00003018
Iteration 91/1000 | Loss: 0.00003018
Iteration 92/1000 | Loss: 0.00003018
Iteration 93/1000 | Loss: 0.00003018
Iteration 94/1000 | Loss: 0.00003018
Iteration 95/1000 | Loss: 0.00003018
Iteration 96/1000 | Loss: 0.00003018
Iteration 97/1000 | Loss: 0.00003018
Iteration 98/1000 | Loss: 0.00003018
Iteration 99/1000 | Loss: 0.00003017
Iteration 100/1000 | Loss: 0.00003017
Iteration 101/1000 | Loss: 0.00003017
Iteration 102/1000 | Loss: 0.00003017
Iteration 103/1000 | Loss: 0.00003017
Iteration 104/1000 | Loss: 0.00003017
Iteration 105/1000 | Loss: 0.00003017
Iteration 106/1000 | Loss: 0.00003016
Iteration 107/1000 | Loss: 0.00003016
Iteration 108/1000 | Loss: 0.00003016
Iteration 109/1000 | Loss: 0.00003016
Iteration 110/1000 | Loss: 0.00003016
Iteration 111/1000 | Loss: 0.00003016
Iteration 112/1000 | Loss: 0.00003015
Iteration 113/1000 | Loss: 0.00003015
Iteration 114/1000 | Loss: 0.00003015
Iteration 115/1000 | Loss: 0.00003015
Iteration 116/1000 | Loss: 0.00003015
Iteration 117/1000 | Loss: 0.00003015
Iteration 118/1000 | Loss: 0.00003015
Iteration 119/1000 | Loss: 0.00003015
Iteration 120/1000 | Loss: 0.00003014
Iteration 121/1000 | Loss: 0.00003014
Iteration 122/1000 | Loss: 0.00003014
Iteration 123/1000 | Loss: 0.00003014
Iteration 124/1000 | Loss: 0.00003014
Iteration 125/1000 | Loss: 0.00003014
Iteration 126/1000 | Loss: 0.00003014
Iteration 127/1000 | Loss: 0.00003014
Iteration 128/1000 | Loss: 0.00003013
Iteration 129/1000 | Loss: 0.00003013
Iteration 130/1000 | Loss: 0.00003013
Iteration 131/1000 | Loss: 0.00003013
Iteration 132/1000 | Loss: 0.00003012
Iteration 133/1000 | Loss: 0.00003012
Iteration 134/1000 | Loss: 0.00003012
Iteration 135/1000 | Loss: 0.00003012
Iteration 136/1000 | Loss: 0.00003012
Iteration 137/1000 | Loss: 0.00003011
Iteration 138/1000 | Loss: 0.00003011
Iteration 139/1000 | Loss: 0.00003011
Iteration 140/1000 | Loss: 0.00003011
Iteration 141/1000 | Loss: 0.00003010
Iteration 142/1000 | Loss: 0.00003010
Iteration 143/1000 | Loss: 0.00003010
Iteration 144/1000 | Loss: 0.00003010
Iteration 145/1000 | Loss: 0.00003009
Iteration 146/1000 | Loss: 0.00003009
Iteration 147/1000 | Loss: 0.00003009
Iteration 148/1000 | Loss: 0.00003009
Iteration 149/1000 | Loss: 0.00003009
Iteration 150/1000 | Loss: 0.00003009
Iteration 151/1000 | Loss: 0.00003009
Iteration 152/1000 | Loss: 0.00003009
Iteration 153/1000 | Loss: 0.00003009
Iteration 154/1000 | Loss: 0.00003009
Iteration 155/1000 | Loss: 0.00003009
Iteration 156/1000 | Loss: 0.00003009
Iteration 157/1000 | Loss: 0.00003009
Iteration 158/1000 | Loss: 0.00003009
Iteration 159/1000 | Loss: 0.00003009
Iteration 160/1000 | Loss: 0.00003009
Iteration 161/1000 | Loss: 0.00003009
Iteration 162/1000 | Loss: 0.00003009
Iteration 163/1000 | Loss: 0.00003009
Iteration 164/1000 | Loss: 0.00003008
Iteration 165/1000 | Loss: 0.00003008
Iteration 166/1000 | Loss: 0.00003008
Iteration 167/1000 | Loss: 0.00003008
Iteration 168/1000 | Loss: 0.00003008
Iteration 169/1000 | Loss: 0.00003008
Iteration 170/1000 | Loss: 0.00003008
Iteration 171/1000 | Loss: 0.00003008
Iteration 172/1000 | Loss: 0.00003008
Iteration 173/1000 | Loss: 0.00003008
Iteration 174/1000 | Loss: 0.00003008
Iteration 175/1000 | Loss: 0.00003008
Iteration 176/1000 | Loss: 0.00003008
Iteration 177/1000 | Loss: 0.00003008
Iteration 178/1000 | Loss: 0.00003008
Iteration 179/1000 | Loss: 0.00003008
Iteration 180/1000 | Loss: 0.00003008
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [3.007733539561741e-05, 3.007733539561741e-05, 3.007733539561741e-05, 3.007733539561741e-05, 3.007733539561741e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.007733539561741e-05

Optimization complete. Final v2v error: 4.7491960525512695 mm

Highest mean error: 5.551114082336426 mm for frame 17

Lowest mean error: 4.069339752197266 mm for frame 4

Saving results

Total time: 39.680068254470825
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00577826
Iteration 2/25 | Loss: 0.00156949
Iteration 3/25 | Loss: 0.00133763
Iteration 4/25 | Loss: 0.00132590
Iteration 5/25 | Loss: 0.00132379
Iteration 6/25 | Loss: 0.00132379
Iteration 7/25 | Loss: 0.00132379
Iteration 8/25 | Loss: 0.00132379
Iteration 9/25 | Loss: 0.00132379
Iteration 10/25 | Loss: 0.00132379
Iteration 11/25 | Loss: 0.00132379
Iteration 12/25 | Loss: 0.00132379
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013237901730462909, 0.0013237901730462909, 0.0013237901730462909, 0.0013237901730462909, 0.0013237901730462909]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013237901730462909

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29971695
Iteration 2/25 | Loss: 0.00096966
Iteration 3/25 | Loss: 0.00096961
Iteration 4/25 | Loss: 0.00096961
Iteration 5/25 | Loss: 0.00096961
Iteration 6/25 | Loss: 0.00096960
Iteration 7/25 | Loss: 0.00096960
Iteration 8/25 | Loss: 0.00096960
Iteration 9/25 | Loss: 0.00096960
Iteration 10/25 | Loss: 0.00096960
Iteration 11/25 | Loss: 0.00096960
Iteration 12/25 | Loss: 0.00096960
Iteration 13/25 | Loss: 0.00096960
Iteration 14/25 | Loss: 0.00096960
Iteration 15/25 | Loss: 0.00096960
Iteration 16/25 | Loss: 0.00096960
Iteration 17/25 | Loss: 0.00096960
Iteration 18/25 | Loss: 0.00096960
Iteration 19/25 | Loss: 0.00096960
Iteration 20/25 | Loss: 0.00096960
Iteration 21/25 | Loss: 0.00096960
Iteration 22/25 | Loss: 0.00096960
Iteration 23/25 | Loss: 0.00096960
Iteration 24/25 | Loss: 0.00096960
Iteration 25/25 | Loss: 0.00096960

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096960
Iteration 2/1000 | Loss: 0.00008744
Iteration 3/1000 | Loss: 0.00005600
Iteration 4/1000 | Loss: 0.00004607
Iteration 5/1000 | Loss: 0.00003935
Iteration 6/1000 | Loss: 0.00003627
Iteration 7/1000 | Loss: 0.00003488
Iteration 8/1000 | Loss: 0.00003381
Iteration 9/1000 | Loss: 0.00003325
Iteration 10/1000 | Loss: 0.00003274
Iteration 11/1000 | Loss: 0.00003222
Iteration 12/1000 | Loss: 0.00003198
Iteration 13/1000 | Loss: 0.00003198
Iteration 14/1000 | Loss: 0.00003193
Iteration 15/1000 | Loss: 0.00003188
Iteration 16/1000 | Loss: 0.00003186
Iteration 17/1000 | Loss: 0.00003185
Iteration 18/1000 | Loss: 0.00003184
Iteration 19/1000 | Loss: 0.00003183
Iteration 20/1000 | Loss: 0.00003182
Iteration 21/1000 | Loss: 0.00003179
Iteration 22/1000 | Loss: 0.00003179
Iteration 23/1000 | Loss: 0.00003178
Iteration 24/1000 | Loss: 0.00003178
Iteration 25/1000 | Loss: 0.00003177
Iteration 26/1000 | Loss: 0.00003177
Iteration 27/1000 | Loss: 0.00003177
Iteration 28/1000 | Loss: 0.00003175
Iteration 29/1000 | Loss: 0.00003175
Iteration 30/1000 | Loss: 0.00003174
Iteration 31/1000 | Loss: 0.00003174
Iteration 32/1000 | Loss: 0.00003174
Iteration 33/1000 | Loss: 0.00003174
Iteration 34/1000 | Loss: 0.00003173
Iteration 35/1000 | Loss: 0.00003173
Iteration 36/1000 | Loss: 0.00003172
Iteration 37/1000 | Loss: 0.00003172
Iteration 38/1000 | Loss: 0.00003171
Iteration 39/1000 | Loss: 0.00003171
Iteration 40/1000 | Loss: 0.00003171
Iteration 41/1000 | Loss: 0.00003171
Iteration 42/1000 | Loss: 0.00003170
Iteration 43/1000 | Loss: 0.00003170
Iteration 44/1000 | Loss: 0.00003170
Iteration 45/1000 | Loss: 0.00003169
Iteration 46/1000 | Loss: 0.00003169
Iteration 47/1000 | Loss: 0.00003169
Iteration 48/1000 | Loss: 0.00003169
Iteration 49/1000 | Loss: 0.00003169
Iteration 50/1000 | Loss: 0.00003169
Iteration 51/1000 | Loss: 0.00003169
Iteration 52/1000 | Loss: 0.00003168
Iteration 53/1000 | Loss: 0.00003168
Iteration 54/1000 | Loss: 0.00003168
Iteration 55/1000 | Loss: 0.00003168
Iteration 56/1000 | Loss: 0.00003168
Iteration 57/1000 | Loss: 0.00003167
Iteration 58/1000 | Loss: 0.00003167
Iteration 59/1000 | Loss: 0.00003167
Iteration 60/1000 | Loss: 0.00003167
Iteration 61/1000 | Loss: 0.00003167
Iteration 62/1000 | Loss: 0.00003167
Iteration 63/1000 | Loss: 0.00003166
Iteration 64/1000 | Loss: 0.00003166
Iteration 65/1000 | Loss: 0.00003166
Iteration 66/1000 | Loss: 0.00003166
Iteration 67/1000 | Loss: 0.00003166
Iteration 68/1000 | Loss: 0.00003166
Iteration 69/1000 | Loss: 0.00003166
Iteration 70/1000 | Loss: 0.00003166
Iteration 71/1000 | Loss: 0.00003166
Iteration 72/1000 | Loss: 0.00003165
Iteration 73/1000 | Loss: 0.00003165
Iteration 74/1000 | Loss: 0.00003165
Iteration 75/1000 | Loss: 0.00003165
Iteration 76/1000 | Loss: 0.00003165
Iteration 77/1000 | Loss: 0.00003164
Iteration 78/1000 | Loss: 0.00003164
Iteration 79/1000 | Loss: 0.00003164
Iteration 80/1000 | Loss: 0.00003163
Iteration 81/1000 | Loss: 0.00003163
Iteration 82/1000 | Loss: 0.00003163
Iteration 83/1000 | Loss: 0.00003163
Iteration 84/1000 | Loss: 0.00003163
Iteration 85/1000 | Loss: 0.00003163
Iteration 86/1000 | Loss: 0.00003163
Iteration 87/1000 | Loss: 0.00003163
Iteration 88/1000 | Loss: 0.00003163
Iteration 89/1000 | Loss: 0.00003163
Iteration 90/1000 | Loss: 0.00003163
Iteration 91/1000 | Loss: 0.00003162
Iteration 92/1000 | Loss: 0.00003162
Iteration 93/1000 | Loss: 0.00003162
Iteration 94/1000 | Loss: 0.00003162
Iteration 95/1000 | Loss: 0.00003161
Iteration 96/1000 | Loss: 0.00003161
Iteration 97/1000 | Loss: 0.00003161
Iteration 98/1000 | Loss: 0.00003161
Iteration 99/1000 | Loss: 0.00003161
Iteration 100/1000 | Loss: 0.00003161
Iteration 101/1000 | Loss: 0.00003161
Iteration 102/1000 | Loss: 0.00003161
Iteration 103/1000 | Loss: 0.00003161
Iteration 104/1000 | Loss: 0.00003161
Iteration 105/1000 | Loss: 0.00003161
Iteration 106/1000 | Loss: 0.00003161
Iteration 107/1000 | Loss: 0.00003161
Iteration 108/1000 | Loss: 0.00003161
Iteration 109/1000 | Loss: 0.00003161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [3.161181302857585e-05, 3.161181302857585e-05, 3.161181302857585e-05, 3.161181302857585e-05, 3.161181302857585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.161181302857585e-05

Optimization complete. Final v2v error: 4.731263160705566 mm

Highest mean error: 5.45181131362915 mm for frame 152

Lowest mean error: 4.1514482498168945 mm for frame 9

Saving results

Total time: 32.52410864830017
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546616
Iteration 2/25 | Loss: 0.00153283
Iteration 3/25 | Loss: 0.00128643
Iteration 4/25 | Loss: 0.00124846
Iteration 5/25 | Loss: 0.00124231
Iteration 6/25 | Loss: 0.00123997
Iteration 7/25 | Loss: 0.00123933
Iteration 8/25 | Loss: 0.00123933
Iteration 9/25 | Loss: 0.00123933
Iteration 10/25 | Loss: 0.00123933
Iteration 11/25 | Loss: 0.00123933
Iteration 12/25 | Loss: 0.00123933
Iteration 13/25 | Loss: 0.00123933
Iteration 14/25 | Loss: 0.00123933
Iteration 15/25 | Loss: 0.00123933
Iteration 16/25 | Loss: 0.00123933
Iteration 17/25 | Loss: 0.00123933
Iteration 18/25 | Loss: 0.00123933
Iteration 19/25 | Loss: 0.00123933
Iteration 20/25 | Loss: 0.00123933
Iteration 21/25 | Loss: 0.00123933
Iteration 22/25 | Loss: 0.00123933
Iteration 23/25 | Loss: 0.00123933
Iteration 24/25 | Loss: 0.00123933
Iteration 25/25 | Loss: 0.00123933

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36669075
Iteration 2/25 | Loss: 0.00095584
Iteration 3/25 | Loss: 0.00095584
Iteration 4/25 | Loss: 0.00095584
Iteration 5/25 | Loss: 0.00095584
Iteration 6/25 | Loss: 0.00095584
Iteration 7/25 | Loss: 0.00095584
Iteration 8/25 | Loss: 0.00095584
Iteration 9/25 | Loss: 0.00095584
Iteration 10/25 | Loss: 0.00095584
Iteration 11/25 | Loss: 0.00095584
Iteration 12/25 | Loss: 0.00095584
Iteration 13/25 | Loss: 0.00095584
Iteration 14/25 | Loss: 0.00095584
Iteration 15/25 | Loss: 0.00095584
Iteration 16/25 | Loss: 0.00095584
Iteration 17/25 | Loss: 0.00095584
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009558354504406452, 0.0009558354504406452, 0.0009558354504406452, 0.0009558354504406452, 0.0009558354504406452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009558354504406452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095584
Iteration 2/1000 | Loss: 0.00006017
Iteration 3/1000 | Loss: 0.00004060
Iteration 4/1000 | Loss: 0.00003290
Iteration 5/1000 | Loss: 0.00002921
Iteration 6/1000 | Loss: 0.00002679
Iteration 7/1000 | Loss: 0.00002598
Iteration 8/1000 | Loss: 0.00002553
Iteration 9/1000 | Loss: 0.00002511
Iteration 10/1000 | Loss: 0.00002480
Iteration 11/1000 | Loss: 0.00002461
Iteration 12/1000 | Loss: 0.00002442
Iteration 13/1000 | Loss: 0.00002440
Iteration 14/1000 | Loss: 0.00002433
Iteration 15/1000 | Loss: 0.00002433
Iteration 16/1000 | Loss: 0.00002432
Iteration 17/1000 | Loss: 0.00002425
Iteration 18/1000 | Loss: 0.00002419
Iteration 19/1000 | Loss: 0.00002416
Iteration 20/1000 | Loss: 0.00002413
Iteration 21/1000 | Loss: 0.00002413
Iteration 22/1000 | Loss: 0.00002412
Iteration 23/1000 | Loss: 0.00002412
Iteration 24/1000 | Loss: 0.00002412
Iteration 25/1000 | Loss: 0.00002411
Iteration 26/1000 | Loss: 0.00002411
Iteration 27/1000 | Loss: 0.00002411
Iteration 28/1000 | Loss: 0.00002410
Iteration 29/1000 | Loss: 0.00002410
Iteration 30/1000 | Loss: 0.00002409
Iteration 31/1000 | Loss: 0.00002409
Iteration 32/1000 | Loss: 0.00002408
Iteration 33/1000 | Loss: 0.00002408
Iteration 34/1000 | Loss: 0.00002408
Iteration 35/1000 | Loss: 0.00002408
Iteration 36/1000 | Loss: 0.00002408
Iteration 37/1000 | Loss: 0.00002407
Iteration 38/1000 | Loss: 0.00002406
Iteration 39/1000 | Loss: 0.00002406
Iteration 40/1000 | Loss: 0.00002405
Iteration 41/1000 | Loss: 0.00002405
Iteration 42/1000 | Loss: 0.00002405
Iteration 43/1000 | Loss: 0.00002405
Iteration 44/1000 | Loss: 0.00002405
Iteration 45/1000 | Loss: 0.00002405
Iteration 46/1000 | Loss: 0.00002405
Iteration 47/1000 | Loss: 0.00002405
Iteration 48/1000 | Loss: 0.00002404
Iteration 49/1000 | Loss: 0.00002404
Iteration 50/1000 | Loss: 0.00002404
Iteration 51/1000 | Loss: 0.00002404
Iteration 52/1000 | Loss: 0.00002404
Iteration 53/1000 | Loss: 0.00002404
Iteration 54/1000 | Loss: 0.00002404
Iteration 55/1000 | Loss: 0.00002404
Iteration 56/1000 | Loss: 0.00002404
Iteration 57/1000 | Loss: 0.00002404
Iteration 58/1000 | Loss: 0.00002404
Iteration 59/1000 | Loss: 0.00002404
Iteration 60/1000 | Loss: 0.00002404
Iteration 61/1000 | Loss: 0.00002404
Iteration 62/1000 | Loss: 0.00002404
Iteration 63/1000 | Loss: 0.00002404
Iteration 64/1000 | Loss: 0.00002404
Iteration 65/1000 | Loss: 0.00002404
Iteration 66/1000 | Loss: 0.00002404
Iteration 67/1000 | Loss: 0.00002404
Iteration 68/1000 | Loss: 0.00002404
Iteration 69/1000 | Loss: 0.00002404
Iteration 70/1000 | Loss: 0.00002404
Iteration 71/1000 | Loss: 0.00002404
Iteration 72/1000 | Loss: 0.00002404
Iteration 73/1000 | Loss: 0.00002404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.404358019703068e-05, 2.404358019703068e-05, 2.404358019703068e-05, 2.404358019703068e-05, 2.404358019703068e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.404358019703068e-05

Optimization complete. Final v2v error: 4.134020805358887 mm

Highest mean error: 6.092188358306885 mm for frame 84

Lowest mean error: 3.5720932483673096 mm for frame 1

Saving results

Total time: 36.00880980491638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870843
Iteration 2/25 | Loss: 0.00184281
Iteration 3/25 | Loss: 0.00135712
Iteration 4/25 | Loss: 0.00130180
Iteration 5/25 | Loss: 0.00129447
Iteration 6/25 | Loss: 0.00129326
Iteration 7/25 | Loss: 0.00129302
Iteration 8/25 | Loss: 0.00129302
Iteration 9/25 | Loss: 0.00129302
Iteration 10/25 | Loss: 0.00129302
Iteration 11/25 | Loss: 0.00129302
Iteration 12/25 | Loss: 0.00129302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012930188095197082, 0.0012930188095197082, 0.0012930188095197082, 0.0012930188095197082, 0.0012930188095197082]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012930188095197082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37582695
Iteration 2/25 | Loss: 0.00085164
Iteration 3/25 | Loss: 0.00085164
Iteration 4/25 | Loss: 0.00085164
Iteration 5/25 | Loss: 0.00085164
Iteration 6/25 | Loss: 0.00085164
Iteration 7/25 | Loss: 0.00085164
Iteration 8/25 | Loss: 0.00085164
Iteration 9/25 | Loss: 0.00085164
Iteration 10/25 | Loss: 0.00085164
Iteration 11/25 | Loss: 0.00085164
Iteration 12/25 | Loss: 0.00085164
Iteration 13/25 | Loss: 0.00085164
Iteration 14/25 | Loss: 0.00085164
Iteration 15/25 | Loss: 0.00085164
Iteration 16/25 | Loss: 0.00085164
Iteration 17/25 | Loss: 0.00085164
Iteration 18/25 | Loss: 0.00085164
Iteration 19/25 | Loss: 0.00085164
Iteration 20/25 | Loss: 0.00085164
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0008516372763551772, 0.0008516372763551772, 0.0008516372763551772, 0.0008516372763551772, 0.0008516372763551772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008516372763551772

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085164
Iteration 2/1000 | Loss: 0.00006069
Iteration 3/1000 | Loss: 0.00004581
Iteration 4/1000 | Loss: 0.00003759
Iteration 5/1000 | Loss: 0.00003277
Iteration 6/1000 | Loss: 0.00003091
Iteration 7/1000 | Loss: 0.00003006
Iteration 8/1000 | Loss: 0.00002956
Iteration 9/1000 | Loss: 0.00002911
Iteration 10/1000 | Loss: 0.00002880
Iteration 11/1000 | Loss: 0.00002856
Iteration 12/1000 | Loss: 0.00002832
Iteration 13/1000 | Loss: 0.00002823
Iteration 14/1000 | Loss: 0.00002809
Iteration 15/1000 | Loss: 0.00002805
Iteration 16/1000 | Loss: 0.00002803
Iteration 17/1000 | Loss: 0.00002802
Iteration 18/1000 | Loss: 0.00002802
Iteration 19/1000 | Loss: 0.00002802
Iteration 20/1000 | Loss: 0.00002802
Iteration 21/1000 | Loss: 0.00002802
Iteration 22/1000 | Loss: 0.00002802
Iteration 23/1000 | Loss: 0.00002802
Iteration 24/1000 | Loss: 0.00002802
Iteration 25/1000 | Loss: 0.00002801
Iteration 26/1000 | Loss: 0.00002801
Iteration 27/1000 | Loss: 0.00002801
Iteration 28/1000 | Loss: 0.00002801
Iteration 29/1000 | Loss: 0.00002801
Iteration 30/1000 | Loss: 0.00002801
Iteration 31/1000 | Loss: 0.00002801
Iteration 32/1000 | Loss: 0.00002801
Iteration 33/1000 | Loss: 0.00002801
Iteration 34/1000 | Loss: 0.00002801
Iteration 35/1000 | Loss: 0.00002800
Iteration 36/1000 | Loss: 0.00002800
Iteration 37/1000 | Loss: 0.00002800
Iteration 38/1000 | Loss: 0.00002799
Iteration 39/1000 | Loss: 0.00002799
Iteration 40/1000 | Loss: 0.00002799
Iteration 41/1000 | Loss: 0.00002799
Iteration 42/1000 | Loss: 0.00002798
Iteration 43/1000 | Loss: 0.00002798
Iteration 44/1000 | Loss: 0.00002798
Iteration 45/1000 | Loss: 0.00002797
Iteration 46/1000 | Loss: 0.00002797
Iteration 47/1000 | Loss: 0.00002797
Iteration 48/1000 | Loss: 0.00002797
Iteration 49/1000 | Loss: 0.00002796
Iteration 50/1000 | Loss: 0.00002796
Iteration 51/1000 | Loss: 0.00002796
Iteration 52/1000 | Loss: 0.00002795
Iteration 53/1000 | Loss: 0.00002795
Iteration 54/1000 | Loss: 0.00002795
Iteration 55/1000 | Loss: 0.00002795
Iteration 56/1000 | Loss: 0.00002795
Iteration 57/1000 | Loss: 0.00002795
Iteration 58/1000 | Loss: 0.00002794
Iteration 59/1000 | Loss: 0.00002794
Iteration 60/1000 | Loss: 0.00002794
Iteration 61/1000 | Loss: 0.00002794
Iteration 62/1000 | Loss: 0.00002794
Iteration 63/1000 | Loss: 0.00002794
Iteration 64/1000 | Loss: 0.00002794
Iteration 65/1000 | Loss: 0.00002794
Iteration 66/1000 | Loss: 0.00002794
Iteration 67/1000 | Loss: 0.00002794
Iteration 68/1000 | Loss: 0.00002794
Iteration 69/1000 | Loss: 0.00002793
Iteration 70/1000 | Loss: 0.00002793
Iteration 71/1000 | Loss: 0.00002793
Iteration 72/1000 | Loss: 0.00002792
Iteration 73/1000 | Loss: 0.00002791
Iteration 74/1000 | Loss: 0.00002791
Iteration 75/1000 | Loss: 0.00002790
Iteration 76/1000 | Loss: 0.00002790
Iteration 77/1000 | Loss: 0.00002790
Iteration 78/1000 | Loss: 0.00002790
Iteration 79/1000 | Loss: 0.00002790
Iteration 80/1000 | Loss: 0.00002790
Iteration 81/1000 | Loss: 0.00002790
Iteration 82/1000 | Loss: 0.00002790
Iteration 83/1000 | Loss: 0.00002790
Iteration 84/1000 | Loss: 0.00002790
Iteration 85/1000 | Loss: 0.00002790
Iteration 86/1000 | Loss: 0.00002789
Iteration 87/1000 | Loss: 0.00002789
Iteration 88/1000 | Loss: 0.00002789
Iteration 89/1000 | Loss: 0.00002789
Iteration 90/1000 | Loss: 0.00002789
Iteration 91/1000 | Loss: 0.00002789
Iteration 92/1000 | Loss: 0.00002789
Iteration 93/1000 | Loss: 0.00002789
Iteration 94/1000 | Loss: 0.00002789
Iteration 95/1000 | Loss: 0.00002789
Iteration 96/1000 | Loss: 0.00002788
Iteration 97/1000 | Loss: 0.00002788
Iteration 98/1000 | Loss: 0.00002788
Iteration 99/1000 | Loss: 0.00002788
Iteration 100/1000 | Loss: 0.00002788
Iteration 101/1000 | Loss: 0.00002788
Iteration 102/1000 | Loss: 0.00002788
Iteration 103/1000 | Loss: 0.00002788
Iteration 104/1000 | Loss: 0.00002788
Iteration 105/1000 | Loss: 0.00002788
Iteration 106/1000 | Loss: 0.00002788
Iteration 107/1000 | Loss: 0.00002788
Iteration 108/1000 | Loss: 0.00002788
Iteration 109/1000 | Loss: 0.00002787
Iteration 110/1000 | Loss: 0.00002787
Iteration 111/1000 | Loss: 0.00002787
Iteration 112/1000 | Loss: 0.00002787
Iteration 113/1000 | Loss: 0.00002787
Iteration 114/1000 | Loss: 0.00002787
Iteration 115/1000 | Loss: 0.00002787
Iteration 116/1000 | Loss: 0.00002787
Iteration 117/1000 | Loss: 0.00002787
Iteration 118/1000 | Loss: 0.00002786
Iteration 119/1000 | Loss: 0.00002786
Iteration 120/1000 | Loss: 0.00002786
Iteration 121/1000 | Loss: 0.00002786
Iteration 122/1000 | Loss: 0.00002786
Iteration 123/1000 | Loss: 0.00002786
Iteration 124/1000 | Loss: 0.00002786
Iteration 125/1000 | Loss: 0.00002785
Iteration 126/1000 | Loss: 0.00002785
Iteration 127/1000 | Loss: 0.00002785
Iteration 128/1000 | Loss: 0.00002785
Iteration 129/1000 | Loss: 0.00002785
Iteration 130/1000 | Loss: 0.00002785
Iteration 131/1000 | Loss: 0.00002785
Iteration 132/1000 | Loss: 0.00002785
Iteration 133/1000 | Loss: 0.00002785
Iteration 134/1000 | Loss: 0.00002785
Iteration 135/1000 | Loss: 0.00002785
Iteration 136/1000 | Loss: 0.00002785
Iteration 137/1000 | Loss: 0.00002785
Iteration 138/1000 | Loss: 0.00002785
Iteration 139/1000 | Loss: 0.00002785
Iteration 140/1000 | Loss: 0.00002785
Iteration 141/1000 | Loss: 0.00002785
Iteration 142/1000 | Loss: 0.00002785
Iteration 143/1000 | Loss: 0.00002785
Iteration 144/1000 | Loss: 0.00002785
Iteration 145/1000 | Loss: 0.00002785
Iteration 146/1000 | Loss: 0.00002785
Iteration 147/1000 | Loss: 0.00002785
Iteration 148/1000 | Loss: 0.00002785
Iteration 149/1000 | Loss: 0.00002785
Iteration 150/1000 | Loss: 0.00002785
Iteration 151/1000 | Loss: 0.00002785
Iteration 152/1000 | Loss: 0.00002785
Iteration 153/1000 | Loss: 0.00002785
Iteration 154/1000 | Loss: 0.00002785
Iteration 155/1000 | Loss: 0.00002785
Iteration 156/1000 | Loss: 0.00002785
Iteration 157/1000 | Loss: 0.00002785
Iteration 158/1000 | Loss: 0.00002785
Iteration 159/1000 | Loss: 0.00002785
Iteration 160/1000 | Loss: 0.00002785
Iteration 161/1000 | Loss: 0.00002785
Iteration 162/1000 | Loss: 0.00002785
Iteration 163/1000 | Loss: 0.00002785
Iteration 164/1000 | Loss: 0.00002785
Iteration 165/1000 | Loss: 0.00002785
Iteration 166/1000 | Loss: 0.00002785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.7845757358591072e-05, 2.7845757358591072e-05, 2.7845757358591072e-05, 2.7845757358591072e-05, 2.7845757358591072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7845757358591072e-05

Optimization complete. Final v2v error: 4.583865642547607 mm

Highest mean error: 5.2179365158081055 mm for frame 138

Lowest mean error: 4.340324878692627 mm for frame 41

Saving results

Total time: 36.372339487075806
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00716924
Iteration 2/25 | Loss: 0.00134936
Iteration 3/25 | Loss: 0.00125507
Iteration 4/25 | Loss: 0.00124122
Iteration 5/25 | Loss: 0.00123586
Iteration 6/25 | Loss: 0.00123485
Iteration 7/25 | Loss: 0.00123485
Iteration 8/25 | Loss: 0.00123485
Iteration 9/25 | Loss: 0.00123485
Iteration 10/25 | Loss: 0.00123485
Iteration 11/25 | Loss: 0.00123485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012348545715212822, 0.0012348545715212822, 0.0012348545715212822, 0.0012348545715212822, 0.0012348545715212822]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012348545715212822

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.10493279
Iteration 2/25 | Loss: 0.00095776
Iteration 3/25 | Loss: 0.00095775
Iteration 4/25 | Loss: 0.00095775
Iteration 5/25 | Loss: 0.00095775
Iteration 6/25 | Loss: 0.00095775
Iteration 7/25 | Loss: 0.00095775
Iteration 8/25 | Loss: 0.00095775
Iteration 9/25 | Loss: 0.00095775
Iteration 10/25 | Loss: 0.00095775
Iteration 11/25 | Loss: 0.00095775
Iteration 12/25 | Loss: 0.00095775
Iteration 13/25 | Loss: 0.00095775
Iteration 14/25 | Loss: 0.00095775
Iteration 15/25 | Loss: 0.00095775
Iteration 16/25 | Loss: 0.00095775
Iteration 17/25 | Loss: 0.00095775
Iteration 18/25 | Loss: 0.00095775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00095774931833148, 0.00095774931833148, 0.00095774931833148, 0.00095774931833148, 0.00095774931833148]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00095774931833148

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095775
Iteration 2/1000 | Loss: 0.00005446
Iteration 3/1000 | Loss: 0.00003488
Iteration 4/1000 | Loss: 0.00002925
Iteration 5/1000 | Loss: 0.00002631
Iteration 6/1000 | Loss: 0.00002557
Iteration 7/1000 | Loss: 0.00002507
Iteration 8/1000 | Loss: 0.00002483
Iteration 9/1000 | Loss: 0.00002447
Iteration 10/1000 | Loss: 0.00002422
Iteration 11/1000 | Loss: 0.00002407
Iteration 12/1000 | Loss: 0.00002407
Iteration 13/1000 | Loss: 0.00002399
Iteration 14/1000 | Loss: 0.00002398
Iteration 15/1000 | Loss: 0.00002398
Iteration 16/1000 | Loss: 0.00002398
Iteration 17/1000 | Loss: 0.00002398
Iteration 18/1000 | Loss: 0.00002398
Iteration 19/1000 | Loss: 0.00002398
Iteration 20/1000 | Loss: 0.00002398
Iteration 21/1000 | Loss: 0.00002398
Iteration 22/1000 | Loss: 0.00002398
Iteration 23/1000 | Loss: 0.00002397
Iteration 24/1000 | Loss: 0.00002397
Iteration 25/1000 | Loss: 0.00002397
Iteration 26/1000 | Loss: 0.00002396
Iteration 27/1000 | Loss: 0.00002396
Iteration 28/1000 | Loss: 0.00002396
Iteration 29/1000 | Loss: 0.00002396
Iteration 30/1000 | Loss: 0.00002396
Iteration 31/1000 | Loss: 0.00002396
Iteration 32/1000 | Loss: 0.00002396
Iteration 33/1000 | Loss: 0.00002395
Iteration 34/1000 | Loss: 0.00002395
Iteration 35/1000 | Loss: 0.00002395
Iteration 36/1000 | Loss: 0.00002395
Iteration 37/1000 | Loss: 0.00002394
Iteration 38/1000 | Loss: 0.00002394
Iteration 39/1000 | Loss: 0.00002394
Iteration 40/1000 | Loss: 0.00002393
Iteration 41/1000 | Loss: 0.00002393
Iteration 42/1000 | Loss: 0.00002393
Iteration 43/1000 | Loss: 0.00002393
Iteration 44/1000 | Loss: 0.00002393
Iteration 45/1000 | Loss: 0.00002393
Iteration 46/1000 | Loss: 0.00002393
Iteration 47/1000 | Loss: 0.00002392
Iteration 48/1000 | Loss: 0.00002392
Iteration 49/1000 | Loss: 0.00002392
Iteration 50/1000 | Loss: 0.00002392
Iteration 51/1000 | Loss: 0.00002392
Iteration 52/1000 | Loss: 0.00002391
Iteration 53/1000 | Loss: 0.00002391
Iteration 54/1000 | Loss: 0.00002391
Iteration 55/1000 | Loss: 0.00002391
Iteration 56/1000 | Loss: 0.00002391
Iteration 57/1000 | Loss: 0.00002391
Iteration 58/1000 | Loss: 0.00002390
Iteration 59/1000 | Loss: 0.00002390
Iteration 60/1000 | Loss: 0.00002390
Iteration 61/1000 | Loss: 0.00002390
Iteration 62/1000 | Loss: 0.00002390
Iteration 63/1000 | Loss: 0.00002389
Iteration 64/1000 | Loss: 0.00002389
Iteration 65/1000 | Loss: 0.00002389
Iteration 66/1000 | Loss: 0.00002389
Iteration 67/1000 | Loss: 0.00002389
Iteration 68/1000 | Loss: 0.00002389
Iteration 69/1000 | Loss: 0.00002388
Iteration 70/1000 | Loss: 0.00002388
Iteration 71/1000 | Loss: 0.00002388
Iteration 72/1000 | Loss: 0.00002388
Iteration 73/1000 | Loss: 0.00002388
Iteration 74/1000 | Loss: 0.00002388
Iteration 75/1000 | Loss: 0.00002388
Iteration 76/1000 | Loss: 0.00002388
Iteration 77/1000 | Loss: 0.00002387
Iteration 78/1000 | Loss: 0.00002387
Iteration 79/1000 | Loss: 0.00002386
Iteration 80/1000 | Loss: 0.00002386
Iteration 81/1000 | Loss: 0.00002386
Iteration 82/1000 | Loss: 0.00002386
Iteration 83/1000 | Loss: 0.00002386
Iteration 84/1000 | Loss: 0.00002385
Iteration 85/1000 | Loss: 0.00002385
Iteration 86/1000 | Loss: 0.00002385
Iteration 87/1000 | Loss: 0.00002385
Iteration 88/1000 | Loss: 0.00002385
Iteration 89/1000 | Loss: 0.00002385
Iteration 90/1000 | Loss: 0.00002385
Iteration 91/1000 | Loss: 0.00002385
Iteration 92/1000 | Loss: 0.00002385
Iteration 93/1000 | Loss: 0.00002385
Iteration 94/1000 | Loss: 0.00002384
Iteration 95/1000 | Loss: 0.00002384
Iteration 96/1000 | Loss: 0.00002384
Iteration 97/1000 | Loss: 0.00002384
Iteration 98/1000 | Loss: 0.00002384
Iteration 99/1000 | Loss: 0.00002384
Iteration 100/1000 | Loss: 0.00002384
Iteration 101/1000 | Loss: 0.00002384
Iteration 102/1000 | Loss: 0.00002384
Iteration 103/1000 | Loss: 0.00002384
Iteration 104/1000 | Loss: 0.00002384
Iteration 105/1000 | Loss: 0.00002384
Iteration 106/1000 | Loss: 0.00002384
Iteration 107/1000 | Loss: 0.00002384
Iteration 108/1000 | Loss: 0.00002384
Iteration 109/1000 | Loss: 0.00002384
Iteration 110/1000 | Loss: 0.00002384
Iteration 111/1000 | Loss: 0.00002384
Iteration 112/1000 | Loss: 0.00002384
Iteration 113/1000 | Loss: 0.00002384
Iteration 114/1000 | Loss: 0.00002383
Iteration 115/1000 | Loss: 0.00002383
Iteration 116/1000 | Loss: 0.00002383
Iteration 117/1000 | Loss: 0.00002383
Iteration 118/1000 | Loss: 0.00002383
Iteration 119/1000 | Loss: 0.00002383
Iteration 120/1000 | Loss: 0.00002383
Iteration 121/1000 | Loss: 0.00002383
Iteration 122/1000 | Loss: 0.00002383
Iteration 123/1000 | Loss: 0.00002383
Iteration 124/1000 | Loss: 0.00002383
Iteration 125/1000 | Loss: 0.00002383
Iteration 126/1000 | Loss: 0.00002383
Iteration 127/1000 | Loss: 0.00002383
Iteration 128/1000 | Loss: 0.00002383
Iteration 129/1000 | Loss: 0.00002383
Iteration 130/1000 | Loss: 0.00002383
Iteration 131/1000 | Loss: 0.00002383
Iteration 132/1000 | Loss: 0.00002383
Iteration 133/1000 | Loss: 0.00002383
Iteration 134/1000 | Loss: 0.00002383
Iteration 135/1000 | Loss: 0.00002383
Iteration 136/1000 | Loss: 0.00002383
Iteration 137/1000 | Loss: 0.00002383
Iteration 138/1000 | Loss: 0.00002383
Iteration 139/1000 | Loss: 0.00002383
Iteration 140/1000 | Loss: 0.00002383
Iteration 141/1000 | Loss: 0.00002383
Iteration 142/1000 | Loss: 0.00002383
Iteration 143/1000 | Loss: 0.00002383
Iteration 144/1000 | Loss: 0.00002383
Iteration 145/1000 | Loss: 0.00002383
Iteration 146/1000 | Loss: 0.00002383
Iteration 147/1000 | Loss: 0.00002383
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [2.383064384048339e-05, 2.383064384048339e-05, 2.383064384048339e-05, 2.383064384048339e-05, 2.383064384048339e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.383064384048339e-05

Optimization complete. Final v2v error: 4.237860679626465 mm

Highest mean error: 4.690125942230225 mm for frame 77

Lowest mean error: 3.740889549255371 mm for frame 95

Saving results

Total time: 32.74081778526306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00456276
Iteration 2/25 | Loss: 0.00133306
Iteration 3/25 | Loss: 0.00123846
Iteration 4/25 | Loss: 0.00122479
Iteration 5/25 | Loss: 0.00121935
Iteration 6/25 | Loss: 0.00121859
Iteration 7/25 | Loss: 0.00121859
Iteration 8/25 | Loss: 0.00121859
Iteration 9/25 | Loss: 0.00121859
Iteration 10/25 | Loss: 0.00121859
Iteration 11/25 | Loss: 0.00121859
Iteration 12/25 | Loss: 0.00121859
Iteration 13/25 | Loss: 0.00121859
Iteration 14/25 | Loss: 0.00121859
Iteration 15/25 | Loss: 0.00121859
Iteration 16/25 | Loss: 0.00121859
Iteration 17/25 | Loss: 0.00121859
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012185935629531741, 0.0012185935629531741, 0.0012185935629531741, 0.0012185935629531741, 0.0012185935629531741]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012185935629531741

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35822022
Iteration 2/25 | Loss: 0.00099663
Iteration 3/25 | Loss: 0.00099663
Iteration 4/25 | Loss: 0.00099663
Iteration 5/25 | Loss: 0.00099662
Iteration 6/25 | Loss: 0.00099662
Iteration 7/25 | Loss: 0.00099662
Iteration 8/25 | Loss: 0.00099662
Iteration 9/25 | Loss: 0.00099662
Iteration 10/25 | Loss: 0.00099662
Iteration 11/25 | Loss: 0.00099662
Iteration 12/25 | Loss: 0.00099662
Iteration 13/25 | Loss: 0.00099662
Iteration 14/25 | Loss: 0.00099662
Iteration 15/25 | Loss: 0.00099662
Iteration 16/25 | Loss: 0.00099662
Iteration 17/25 | Loss: 0.00099662
Iteration 18/25 | Loss: 0.00099662
Iteration 19/25 | Loss: 0.00099662
Iteration 20/25 | Loss: 0.00099662
Iteration 21/25 | Loss: 0.00099662
Iteration 22/25 | Loss: 0.00099662
Iteration 23/25 | Loss: 0.00099662
Iteration 24/25 | Loss: 0.00099662
Iteration 25/25 | Loss: 0.00099662

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099662
Iteration 2/1000 | Loss: 0.00004104
Iteration 3/1000 | Loss: 0.00002895
Iteration 4/1000 | Loss: 0.00002482
Iteration 5/1000 | Loss: 0.00002369
Iteration 6/1000 | Loss: 0.00002298
Iteration 7/1000 | Loss: 0.00002244
Iteration 8/1000 | Loss: 0.00002207
Iteration 9/1000 | Loss: 0.00002189
Iteration 10/1000 | Loss: 0.00002171
Iteration 11/1000 | Loss: 0.00002163
Iteration 12/1000 | Loss: 0.00002152
Iteration 13/1000 | Loss: 0.00002148
Iteration 14/1000 | Loss: 0.00002148
Iteration 15/1000 | Loss: 0.00002147
Iteration 16/1000 | Loss: 0.00002146
Iteration 17/1000 | Loss: 0.00002146
Iteration 18/1000 | Loss: 0.00002146
Iteration 19/1000 | Loss: 0.00002145
Iteration 20/1000 | Loss: 0.00002139
Iteration 21/1000 | Loss: 0.00002137
Iteration 22/1000 | Loss: 0.00002136
Iteration 23/1000 | Loss: 0.00002135
Iteration 24/1000 | Loss: 0.00002135
Iteration 25/1000 | Loss: 0.00002135
Iteration 26/1000 | Loss: 0.00002133
Iteration 27/1000 | Loss: 0.00002132
Iteration 28/1000 | Loss: 0.00002132
Iteration 29/1000 | Loss: 0.00002131
Iteration 30/1000 | Loss: 0.00002131
Iteration 31/1000 | Loss: 0.00002131
Iteration 32/1000 | Loss: 0.00002130
Iteration 33/1000 | Loss: 0.00002130
Iteration 34/1000 | Loss: 0.00002130
Iteration 35/1000 | Loss: 0.00002130
Iteration 36/1000 | Loss: 0.00002130
Iteration 37/1000 | Loss: 0.00002129
Iteration 38/1000 | Loss: 0.00002129
Iteration 39/1000 | Loss: 0.00002129
Iteration 40/1000 | Loss: 0.00002129
Iteration 41/1000 | Loss: 0.00002129
Iteration 42/1000 | Loss: 0.00002129
Iteration 43/1000 | Loss: 0.00002129
Iteration 44/1000 | Loss: 0.00002129
Iteration 45/1000 | Loss: 0.00002129
Iteration 46/1000 | Loss: 0.00002128
Iteration 47/1000 | Loss: 0.00002128
Iteration 48/1000 | Loss: 0.00002128
Iteration 49/1000 | Loss: 0.00002128
Iteration 50/1000 | Loss: 0.00002128
Iteration 51/1000 | Loss: 0.00002128
Iteration 52/1000 | Loss: 0.00002128
Iteration 53/1000 | Loss: 0.00002128
Iteration 54/1000 | Loss: 0.00002127
Iteration 55/1000 | Loss: 0.00002127
Iteration 56/1000 | Loss: 0.00002127
Iteration 57/1000 | Loss: 0.00002127
Iteration 58/1000 | Loss: 0.00002127
Iteration 59/1000 | Loss: 0.00002127
Iteration 60/1000 | Loss: 0.00002127
Iteration 61/1000 | Loss: 0.00002127
Iteration 62/1000 | Loss: 0.00002127
Iteration 63/1000 | Loss: 0.00002127
Iteration 64/1000 | Loss: 0.00002127
Iteration 65/1000 | Loss: 0.00002126
Iteration 66/1000 | Loss: 0.00002126
Iteration 67/1000 | Loss: 0.00002126
Iteration 68/1000 | Loss: 0.00002126
Iteration 69/1000 | Loss: 0.00002126
Iteration 70/1000 | Loss: 0.00002126
Iteration 71/1000 | Loss: 0.00002126
Iteration 72/1000 | Loss: 0.00002126
Iteration 73/1000 | Loss: 0.00002126
Iteration 74/1000 | Loss: 0.00002126
Iteration 75/1000 | Loss: 0.00002126
Iteration 76/1000 | Loss: 0.00002126
Iteration 77/1000 | Loss: 0.00002126
Iteration 78/1000 | Loss: 0.00002126
Iteration 79/1000 | Loss: 0.00002126
Iteration 80/1000 | Loss: 0.00002126
Iteration 81/1000 | Loss: 0.00002126
Iteration 82/1000 | Loss: 0.00002125
Iteration 83/1000 | Loss: 0.00002125
Iteration 84/1000 | Loss: 0.00002125
Iteration 85/1000 | Loss: 0.00002125
Iteration 86/1000 | Loss: 0.00002125
Iteration 87/1000 | Loss: 0.00002125
Iteration 88/1000 | Loss: 0.00002125
Iteration 89/1000 | Loss: 0.00002125
Iteration 90/1000 | Loss: 0.00002125
Iteration 91/1000 | Loss: 0.00002125
Iteration 92/1000 | Loss: 0.00002125
Iteration 93/1000 | Loss: 0.00002125
Iteration 94/1000 | Loss: 0.00002125
Iteration 95/1000 | Loss: 0.00002125
Iteration 96/1000 | Loss: 0.00002125
Iteration 97/1000 | Loss: 0.00002125
Iteration 98/1000 | Loss: 0.00002125
Iteration 99/1000 | Loss: 0.00002125
Iteration 100/1000 | Loss: 0.00002125
Iteration 101/1000 | Loss: 0.00002125
Iteration 102/1000 | Loss: 0.00002124
Iteration 103/1000 | Loss: 0.00002124
Iteration 104/1000 | Loss: 0.00002124
Iteration 105/1000 | Loss: 0.00002124
Iteration 106/1000 | Loss: 0.00002124
Iteration 107/1000 | Loss: 0.00002124
Iteration 108/1000 | Loss: 0.00002124
Iteration 109/1000 | Loss: 0.00002124
Iteration 110/1000 | Loss: 0.00002124
Iteration 111/1000 | Loss: 0.00002124
Iteration 112/1000 | Loss: 0.00002124
Iteration 113/1000 | Loss: 0.00002124
Iteration 114/1000 | Loss: 0.00002124
Iteration 115/1000 | Loss: 0.00002124
Iteration 116/1000 | Loss: 0.00002124
Iteration 117/1000 | Loss: 0.00002124
Iteration 118/1000 | Loss: 0.00002124
Iteration 119/1000 | Loss: 0.00002124
Iteration 120/1000 | Loss: 0.00002124
Iteration 121/1000 | Loss: 0.00002124
Iteration 122/1000 | Loss: 0.00002124
Iteration 123/1000 | Loss: 0.00002124
Iteration 124/1000 | Loss: 0.00002124
Iteration 125/1000 | Loss: 0.00002124
Iteration 126/1000 | Loss: 0.00002124
Iteration 127/1000 | Loss: 0.00002124
Iteration 128/1000 | Loss: 0.00002124
Iteration 129/1000 | Loss: 0.00002124
Iteration 130/1000 | Loss: 0.00002124
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.1237172404653393e-05, 2.1237172404653393e-05, 2.1237172404653393e-05, 2.1237172404653393e-05, 2.1237172404653393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1237172404653393e-05

Optimization complete. Final v2v error: 4.054928779602051 mm

Highest mean error: 4.340550422668457 mm for frame 234

Lowest mean error: 3.730006456375122 mm for frame 165

Saving results

Total time: 35.719385385513306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479788
Iteration 2/25 | Loss: 0.00138877
Iteration 3/25 | Loss: 0.00129511
Iteration 4/25 | Loss: 0.00126953
Iteration 5/25 | Loss: 0.00126495
Iteration 6/25 | Loss: 0.00126415
Iteration 7/25 | Loss: 0.00126415
Iteration 8/25 | Loss: 0.00126415
Iteration 9/25 | Loss: 0.00126415
Iteration 10/25 | Loss: 0.00126415
Iteration 11/25 | Loss: 0.00126415
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00126414664555341, 0.00126414664555341, 0.00126414664555341, 0.00126414664555341, 0.00126414664555341]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00126414664555341

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50650096
Iteration 2/25 | Loss: 0.00099398
Iteration 3/25 | Loss: 0.00099398
Iteration 4/25 | Loss: 0.00099398
Iteration 5/25 | Loss: 0.00099398
Iteration 6/25 | Loss: 0.00099398
Iteration 7/25 | Loss: 0.00099398
Iteration 8/25 | Loss: 0.00099398
Iteration 9/25 | Loss: 0.00099398
Iteration 10/25 | Loss: 0.00099398
Iteration 11/25 | Loss: 0.00099397
Iteration 12/25 | Loss: 0.00099397
Iteration 13/25 | Loss: 0.00099397
Iteration 14/25 | Loss: 0.00099397
Iteration 15/25 | Loss: 0.00099397
Iteration 16/25 | Loss: 0.00099397
Iteration 17/25 | Loss: 0.00099397
Iteration 18/25 | Loss: 0.00099397
Iteration 19/25 | Loss: 0.00099397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009939748561009765, 0.0009939748561009765, 0.0009939748561009765, 0.0009939748561009765, 0.0009939748561009765]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009939748561009765

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099397
Iteration 2/1000 | Loss: 0.00005091
Iteration 3/1000 | Loss: 0.00003522
Iteration 4/1000 | Loss: 0.00003047
Iteration 5/1000 | Loss: 0.00002798
Iteration 6/1000 | Loss: 0.00002690
Iteration 7/1000 | Loss: 0.00002626
Iteration 8/1000 | Loss: 0.00002589
Iteration 9/1000 | Loss: 0.00002555
Iteration 10/1000 | Loss: 0.00002529
Iteration 11/1000 | Loss: 0.00002507
Iteration 12/1000 | Loss: 0.00002497
Iteration 13/1000 | Loss: 0.00002497
Iteration 14/1000 | Loss: 0.00002497
Iteration 15/1000 | Loss: 0.00002495
Iteration 16/1000 | Loss: 0.00002495
Iteration 17/1000 | Loss: 0.00002495
Iteration 18/1000 | Loss: 0.00002494
Iteration 19/1000 | Loss: 0.00002494
Iteration 20/1000 | Loss: 0.00002494
Iteration 21/1000 | Loss: 0.00002493
Iteration 22/1000 | Loss: 0.00002493
Iteration 23/1000 | Loss: 0.00002492
Iteration 24/1000 | Loss: 0.00002492
Iteration 25/1000 | Loss: 0.00002492
Iteration 26/1000 | Loss: 0.00002491
Iteration 27/1000 | Loss: 0.00002491
Iteration 28/1000 | Loss: 0.00002490
Iteration 29/1000 | Loss: 0.00002490
Iteration 30/1000 | Loss: 0.00002490
Iteration 31/1000 | Loss: 0.00002490
Iteration 32/1000 | Loss: 0.00002490
Iteration 33/1000 | Loss: 0.00002490
Iteration 34/1000 | Loss: 0.00002490
Iteration 35/1000 | Loss: 0.00002490
Iteration 36/1000 | Loss: 0.00002490
Iteration 37/1000 | Loss: 0.00002489
Iteration 38/1000 | Loss: 0.00002489
Iteration 39/1000 | Loss: 0.00002489
Iteration 40/1000 | Loss: 0.00002488
Iteration 41/1000 | Loss: 0.00002488
Iteration 42/1000 | Loss: 0.00002488
Iteration 43/1000 | Loss: 0.00002487
Iteration 44/1000 | Loss: 0.00002487
Iteration 45/1000 | Loss: 0.00002487
Iteration 46/1000 | Loss: 0.00002486
Iteration 47/1000 | Loss: 0.00002486
Iteration 48/1000 | Loss: 0.00002486
Iteration 49/1000 | Loss: 0.00002485
Iteration 50/1000 | Loss: 0.00002485
Iteration 51/1000 | Loss: 0.00002484
Iteration 52/1000 | Loss: 0.00002484
Iteration 53/1000 | Loss: 0.00002484
Iteration 54/1000 | Loss: 0.00002484
Iteration 55/1000 | Loss: 0.00002484
Iteration 56/1000 | Loss: 0.00002484
Iteration 57/1000 | Loss: 0.00002484
Iteration 58/1000 | Loss: 0.00002484
Iteration 59/1000 | Loss: 0.00002484
Iteration 60/1000 | Loss: 0.00002484
Iteration 61/1000 | Loss: 0.00002484
Iteration 62/1000 | Loss: 0.00002483
Iteration 63/1000 | Loss: 0.00002483
Iteration 64/1000 | Loss: 0.00002483
Iteration 65/1000 | Loss: 0.00002483
Iteration 66/1000 | Loss: 0.00002483
Iteration 67/1000 | Loss: 0.00002482
Iteration 68/1000 | Loss: 0.00002482
Iteration 69/1000 | Loss: 0.00002482
Iteration 70/1000 | Loss: 0.00002482
Iteration 71/1000 | Loss: 0.00002482
Iteration 72/1000 | Loss: 0.00002481
Iteration 73/1000 | Loss: 0.00002481
Iteration 74/1000 | Loss: 0.00002481
Iteration 75/1000 | Loss: 0.00002481
Iteration 76/1000 | Loss: 0.00002481
Iteration 77/1000 | Loss: 0.00002480
Iteration 78/1000 | Loss: 0.00002480
Iteration 79/1000 | Loss: 0.00002480
Iteration 80/1000 | Loss: 0.00002480
Iteration 81/1000 | Loss: 0.00002480
Iteration 82/1000 | Loss: 0.00002479
Iteration 83/1000 | Loss: 0.00002479
Iteration 84/1000 | Loss: 0.00002479
Iteration 85/1000 | Loss: 0.00002479
Iteration 86/1000 | Loss: 0.00002479
Iteration 87/1000 | Loss: 0.00002478
Iteration 88/1000 | Loss: 0.00002478
Iteration 89/1000 | Loss: 0.00002478
Iteration 90/1000 | Loss: 0.00002478
Iteration 91/1000 | Loss: 0.00002478
Iteration 92/1000 | Loss: 0.00002477
Iteration 93/1000 | Loss: 0.00002477
Iteration 94/1000 | Loss: 0.00002477
Iteration 95/1000 | Loss: 0.00002477
Iteration 96/1000 | Loss: 0.00002477
Iteration 97/1000 | Loss: 0.00002477
Iteration 98/1000 | Loss: 0.00002477
Iteration 99/1000 | Loss: 0.00002476
Iteration 100/1000 | Loss: 0.00002476
Iteration 101/1000 | Loss: 0.00002476
Iteration 102/1000 | Loss: 0.00002476
Iteration 103/1000 | Loss: 0.00002476
Iteration 104/1000 | Loss: 0.00002476
Iteration 105/1000 | Loss: 0.00002476
Iteration 106/1000 | Loss: 0.00002475
Iteration 107/1000 | Loss: 0.00002475
Iteration 108/1000 | Loss: 0.00002475
Iteration 109/1000 | Loss: 0.00002475
Iteration 110/1000 | Loss: 0.00002475
Iteration 111/1000 | Loss: 0.00002475
Iteration 112/1000 | Loss: 0.00002475
Iteration 113/1000 | Loss: 0.00002475
Iteration 114/1000 | Loss: 0.00002475
Iteration 115/1000 | Loss: 0.00002475
Iteration 116/1000 | Loss: 0.00002475
Iteration 117/1000 | Loss: 0.00002475
Iteration 118/1000 | Loss: 0.00002475
Iteration 119/1000 | Loss: 0.00002475
Iteration 120/1000 | Loss: 0.00002475
Iteration 121/1000 | Loss: 0.00002475
Iteration 122/1000 | Loss: 0.00002475
Iteration 123/1000 | Loss: 0.00002475
Iteration 124/1000 | Loss: 0.00002475
Iteration 125/1000 | Loss: 0.00002475
Iteration 126/1000 | Loss: 0.00002475
Iteration 127/1000 | Loss: 0.00002475
Iteration 128/1000 | Loss: 0.00002475
Iteration 129/1000 | Loss: 0.00002475
Iteration 130/1000 | Loss: 0.00002475
Iteration 131/1000 | Loss: 0.00002475
Iteration 132/1000 | Loss: 0.00002475
Iteration 133/1000 | Loss: 0.00002475
Iteration 134/1000 | Loss: 0.00002475
Iteration 135/1000 | Loss: 0.00002475
Iteration 136/1000 | Loss: 0.00002475
Iteration 137/1000 | Loss: 0.00002475
Iteration 138/1000 | Loss: 0.00002475
Iteration 139/1000 | Loss: 0.00002475
Iteration 140/1000 | Loss: 0.00002475
Iteration 141/1000 | Loss: 0.00002475
Iteration 142/1000 | Loss: 0.00002475
Iteration 143/1000 | Loss: 0.00002475
Iteration 144/1000 | Loss: 0.00002475
Iteration 145/1000 | Loss: 0.00002475
Iteration 146/1000 | Loss: 0.00002475
Iteration 147/1000 | Loss: 0.00002475
Iteration 148/1000 | Loss: 0.00002475
Iteration 149/1000 | Loss: 0.00002475
Iteration 150/1000 | Loss: 0.00002475
Iteration 151/1000 | Loss: 0.00002475
Iteration 152/1000 | Loss: 0.00002475
Iteration 153/1000 | Loss: 0.00002475
Iteration 154/1000 | Loss: 0.00002475
Iteration 155/1000 | Loss: 0.00002475
Iteration 156/1000 | Loss: 0.00002475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.4750303055043332e-05, 2.4750303055043332e-05, 2.4750303055043332e-05, 2.4750303055043332e-05, 2.4750303055043332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4750303055043332e-05

Optimization complete. Final v2v error: 4.3240461349487305 mm

Highest mean error: 4.667690753936768 mm for frame 144

Lowest mean error: 3.980958938598633 mm for frame 130

Saving results

Total time: 33.73908448219299
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01051097
Iteration 2/25 | Loss: 0.00290873
Iteration 3/25 | Loss: 0.00205033
Iteration 4/25 | Loss: 0.00163283
Iteration 5/25 | Loss: 0.00152199
Iteration 6/25 | Loss: 0.00139831
Iteration 7/25 | Loss: 0.00128373
Iteration 8/25 | Loss: 0.00124686
Iteration 9/25 | Loss: 0.00119018
Iteration 10/25 | Loss: 0.00117245
Iteration 11/25 | Loss: 0.00115792
Iteration 12/25 | Loss: 0.00115018
Iteration 13/25 | Loss: 0.00114757
Iteration 14/25 | Loss: 0.00114655
Iteration 15/25 | Loss: 0.00114925
Iteration 16/25 | Loss: 0.00114415
Iteration 17/25 | Loss: 0.00114288
Iteration 18/25 | Loss: 0.00114255
Iteration 19/25 | Loss: 0.00114249
Iteration 20/25 | Loss: 0.00114248
Iteration 21/25 | Loss: 0.00114246
Iteration 22/25 | Loss: 0.00114246
Iteration 23/25 | Loss: 0.00114245
Iteration 24/25 | Loss: 0.00114245
Iteration 25/25 | Loss: 0.00114245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28874922
Iteration 2/25 | Loss: 0.00097079
Iteration 3/25 | Loss: 0.00097079
Iteration 4/25 | Loss: 0.00097079
Iteration 5/25 | Loss: 0.00097079
Iteration 6/25 | Loss: 0.00097079
Iteration 7/25 | Loss: 0.00097079
Iteration 8/25 | Loss: 0.00097079
Iteration 9/25 | Loss: 0.00097079
Iteration 10/25 | Loss: 0.00097079
Iteration 11/25 | Loss: 0.00097079
Iteration 12/25 | Loss: 0.00097079
Iteration 13/25 | Loss: 0.00097079
Iteration 14/25 | Loss: 0.00097079
Iteration 15/25 | Loss: 0.00097079
Iteration 16/25 | Loss: 0.00097079
Iteration 17/25 | Loss: 0.00097079
Iteration 18/25 | Loss: 0.00097079
Iteration 19/25 | Loss: 0.00097079
Iteration 20/25 | Loss: 0.00097079
Iteration 21/25 | Loss: 0.00097079
Iteration 22/25 | Loss: 0.00097079
Iteration 23/25 | Loss: 0.00097079
Iteration 24/25 | Loss: 0.00097079
Iteration 25/25 | Loss: 0.00097079

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097079
Iteration 2/1000 | Loss: 0.00011961
Iteration 3/1000 | Loss: 0.00008390
Iteration 4/1000 | Loss: 0.00013011
Iteration 5/1000 | Loss: 0.00009567
Iteration 6/1000 | Loss: 0.00006499
Iteration 7/1000 | Loss: 0.00017080
Iteration 8/1000 | Loss: 0.00010840
Iteration 9/1000 | Loss: 0.00005982
Iteration 10/1000 | Loss: 0.00005804
Iteration 11/1000 | Loss: 0.00005678
Iteration 12/1000 | Loss: 0.00019976
Iteration 13/1000 | Loss: 0.00020811
Iteration 14/1000 | Loss: 0.00015077
Iteration 15/1000 | Loss: 0.00027756
Iteration 16/1000 | Loss: 0.00006673
Iteration 17/1000 | Loss: 0.00006651
Iteration 18/1000 | Loss: 0.00006253
Iteration 19/1000 | Loss: 0.00005245
Iteration 20/1000 | Loss: 0.00013420
Iteration 21/1000 | Loss: 0.00005187
Iteration 22/1000 | Loss: 0.00005080
Iteration 23/1000 | Loss: 0.00005033
Iteration 24/1000 | Loss: 0.00005006
Iteration 25/1000 | Loss: 0.00004999
Iteration 26/1000 | Loss: 0.00004998
Iteration 27/1000 | Loss: 0.00004992
Iteration 28/1000 | Loss: 0.00004992
Iteration 29/1000 | Loss: 0.00004990
Iteration 30/1000 | Loss: 0.00004988
Iteration 31/1000 | Loss: 0.00004987
Iteration 32/1000 | Loss: 0.00004982
Iteration 33/1000 | Loss: 0.00004978
Iteration 34/1000 | Loss: 0.00004977
Iteration 35/1000 | Loss: 0.00004977
Iteration 36/1000 | Loss: 0.00004976
Iteration 37/1000 | Loss: 0.00004975
Iteration 38/1000 | Loss: 0.00004975
Iteration 39/1000 | Loss: 0.00004975
Iteration 40/1000 | Loss: 0.00004975
Iteration 41/1000 | Loss: 0.00004975
Iteration 42/1000 | Loss: 0.00004975
Iteration 43/1000 | Loss: 0.00004975
Iteration 44/1000 | Loss: 0.00004975
Iteration 45/1000 | Loss: 0.00004975
Iteration 46/1000 | Loss: 0.00004974
Iteration 47/1000 | Loss: 0.00004974
Iteration 48/1000 | Loss: 0.00004973
Iteration 49/1000 | Loss: 0.00004973
Iteration 50/1000 | Loss: 0.00004973
Iteration 51/1000 | Loss: 0.00004973
Iteration 52/1000 | Loss: 0.00004972
Iteration 53/1000 | Loss: 0.00004972
Iteration 54/1000 | Loss: 0.00004971
Iteration 55/1000 | Loss: 0.00004970
Iteration 56/1000 | Loss: 0.00004970
Iteration 57/1000 | Loss: 0.00004970
Iteration 58/1000 | Loss: 0.00004970
Iteration 59/1000 | Loss: 0.00004969
Iteration 60/1000 | Loss: 0.00004969
Iteration 61/1000 | Loss: 0.00004969
Iteration 62/1000 | Loss: 0.00004969
Iteration 63/1000 | Loss: 0.00004968
Iteration 64/1000 | Loss: 0.00004968
Iteration 65/1000 | Loss: 0.00004968
Iteration 66/1000 | Loss: 0.00004968
Iteration 67/1000 | Loss: 0.00004967
Iteration 68/1000 | Loss: 0.00004967
Iteration 69/1000 | Loss: 0.00004967
Iteration 70/1000 | Loss: 0.00004967
Iteration 71/1000 | Loss: 0.00004966
Iteration 72/1000 | Loss: 0.00004966
Iteration 73/1000 | Loss: 0.00004966
Iteration 74/1000 | Loss: 0.00004966
Iteration 75/1000 | Loss: 0.00004966
Iteration 76/1000 | Loss: 0.00004966
Iteration 77/1000 | Loss: 0.00004965
Iteration 78/1000 | Loss: 0.00004965
Iteration 79/1000 | Loss: 0.00004965
Iteration 80/1000 | Loss: 0.00004965
Iteration 81/1000 | Loss: 0.00004964
Iteration 82/1000 | Loss: 0.00004964
Iteration 83/1000 | Loss: 0.00004964
Iteration 84/1000 | Loss: 0.00004964
Iteration 85/1000 | Loss: 0.00004964
Iteration 86/1000 | Loss: 0.00004964
Iteration 87/1000 | Loss: 0.00004964
Iteration 88/1000 | Loss: 0.00004964
Iteration 89/1000 | Loss: 0.00004963
Iteration 90/1000 | Loss: 0.00004963
Iteration 91/1000 | Loss: 0.00004963
Iteration 92/1000 | Loss: 0.00004963
Iteration 93/1000 | Loss: 0.00004963
Iteration 94/1000 | Loss: 0.00004963
Iteration 95/1000 | Loss: 0.00004962
Iteration 96/1000 | Loss: 0.00004962
Iteration 97/1000 | Loss: 0.00004962
Iteration 98/1000 | Loss: 0.00004962
Iteration 99/1000 | Loss: 0.00004962
Iteration 100/1000 | Loss: 0.00004962
Iteration 101/1000 | Loss: 0.00004961
Iteration 102/1000 | Loss: 0.00004961
Iteration 103/1000 | Loss: 0.00004961
Iteration 104/1000 | Loss: 0.00004960
Iteration 105/1000 | Loss: 0.00004960
Iteration 106/1000 | Loss: 0.00004960
Iteration 107/1000 | Loss: 0.00004960
Iteration 108/1000 | Loss: 0.00004959
Iteration 109/1000 | Loss: 0.00004959
Iteration 110/1000 | Loss: 0.00004958
Iteration 111/1000 | Loss: 0.00004958
Iteration 112/1000 | Loss: 0.00004958
Iteration 113/1000 | Loss: 0.00004958
Iteration 114/1000 | Loss: 0.00004958
Iteration 115/1000 | Loss: 0.00004958
Iteration 116/1000 | Loss: 0.00004958
Iteration 117/1000 | Loss: 0.00004958
Iteration 118/1000 | Loss: 0.00004958
Iteration 119/1000 | Loss: 0.00004958
Iteration 120/1000 | Loss: 0.00004958
Iteration 121/1000 | Loss: 0.00004958
Iteration 122/1000 | Loss: 0.00004957
Iteration 123/1000 | Loss: 0.00004957
Iteration 124/1000 | Loss: 0.00004957
Iteration 125/1000 | Loss: 0.00004957
Iteration 126/1000 | Loss: 0.00004957
Iteration 127/1000 | Loss: 0.00004957
Iteration 128/1000 | Loss: 0.00004957
Iteration 129/1000 | Loss: 0.00004956
Iteration 130/1000 | Loss: 0.00004956
Iteration 131/1000 | Loss: 0.00004956
Iteration 132/1000 | Loss: 0.00004956
Iteration 133/1000 | Loss: 0.00004956
Iteration 134/1000 | Loss: 0.00004956
Iteration 135/1000 | Loss: 0.00004955
Iteration 136/1000 | Loss: 0.00004955
Iteration 137/1000 | Loss: 0.00004955
Iteration 138/1000 | Loss: 0.00004955
Iteration 139/1000 | Loss: 0.00004955
Iteration 140/1000 | Loss: 0.00004955
Iteration 141/1000 | Loss: 0.00004955
Iteration 142/1000 | Loss: 0.00004955
Iteration 143/1000 | Loss: 0.00004955
Iteration 144/1000 | Loss: 0.00004955
Iteration 145/1000 | Loss: 0.00004954
Iteration 146/1000 | Loss: 0.00004954
Iteration 147/1000 | Loss: 0.00004954
Iteration 148/1000 | Loss: 0.00004954
Iteration 149/1000 | Loss: 0.00004954
Iteration 150/1000 | Loss: 0.00004954
Iteration 151/1000 | Loss: 0.00004954
Iteration 152/1000 | Loss: 0.00004954
Iteration 153/1000 | Loss: 0.00004953
Iteration 154/1000 | Loss: 0.00004953
Iteration 155/1000 | Loss: 0.00004953
Iteration 156/1000 | Loss: 0.00004953
Iteration 157/1000 | Loss: 0.00004953
Iteration 158/1000 | Loss: 0.00004953
Iteration 159/1000 | Loss: 0.00004953
Iteration 160/1000 | Loss: 0.00004953
Iteration 161/1000 | Loss: 0.00004953
Iteration 162/1000 | Loss: 0.00004953
Iteration 163/1000 | Loss: 0.00004953
Iteration 164/1000 | Loss: 0.00004953
Iteration 165/1000 | Loss: 0.00004953
Iteration 166/1000 | Loss: 0.00004953
Iteration 167/1000 | Loss: 0.00004953
Iteration 168/1000 | Loss: 0.00004953
Iteration 169/1000 | Loss: 0.00004952
Iteration 170/1000 | Loss: 0.00004952
Iteration 171/1000 | Loss: 0.00004952
Iteration 172/1000 | Loss: 0.00004952
Iteration 173/1000 | Loss: 0.00004952
Iteration 174/1000 | Loss: 0.00004952
Iteration 175/1000 | Loss: 0.00004952
Iteration 176/1000 | Loss: 0.00004952
Iteration 177/1000 | Loss: 0.00004952
Iteration 178/1000 | Loss: 0.00004952
Iteration 179/1000 | Loss: 0.00004952
Iteration 180/1000 | Loss: 0.00004952
Iteration 181/1000 | Loss: 0.00004952
Iteration 182/1000 | Loss: 0.00004951
Iteration 183/1000 | Loss: 0.00004951
Iteration 184/1000 | Loss: 0.00004951
Iteration 185/1000 | Loss: 0.00004951
Iteration 186/1000 | Loss: 0.00004951
Iteration 187/1000 | Loss: 0.00004951
Iteration 188/1000 | Loss: 0.00004951
Iteration 189/1000 | Loss: 0.00004951
Iteration 190/1000 | Loss: 0.00004951
Iteration 191/1000 | Loss: 0.00004951
Iteration 192/1000 | Loss: 0.00004951
Iteration 193/1000 | Loss: 0.00004951
Iteration 194/1000 | Loss: 0.00004951
Iteration 195/1000 | Loss: 0.00004951
Iteration 196/1000 | Loss: 0.00004951
Iteration 197/1000 | Loss: 0.00004951
Iteration 198/1000 | Loss: 0.00004951
Iteration 199/1000 | Loss: 0.00004951
Iteration 200/1000 | Loss: 0.00004951
Iteration 201/1000 | Loss: 0.00004951
Iteration 202/1000 | Loss: 0.00004951
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [4.951127266394906e-05, 4.951127266394906e-05, 4.951127266394906e-05, 4.951127266394906e-05, 4.951127266394906e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.951127266394906e-05

Optimization complete. Final v2v error: 5.9073991775512695 mm

Highest mean error: 12.284914016723633 mm for frame 172

Lowest mean error: 5.27133321762085 mm for frame 107

Saving results

Total time: 90.94235467910767
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973194
Iteration 2/25 | Loss: 0.00139634
Iteration 3/25 | Loss: 0.00128028
Iteration 4/25 | Loss: 0.00126563
Iteration 5/25 | Loss: 0.00126182
Iteration 6/25 | Loss: 0.00126154
Iteration 7/25 | Loss: 0.00126154
Iteration 8/25 | Loss: 0.00126154
Iteration 9/25 | Loss: 0.00126154
Iteration 10/25 | Loss: 0.00126154
Iteration 11/25 | Loss: 0.00126154
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012615439482033253, 0.0012615439482033253, 0.0012615439482033253, 0.0012615439482033253, 0.0012615439482033253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012615439482033253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.71117640
Iteration 2/25 | Loss: 0.00104488
Iteration 3/25 | Loss: 0.00104488
Iteration 4/25 | Loss: 0.00104488
Iteration 5/25 | Loss: 0.00104488
Iteration 6/25 | Loss: 0.00104488
Iteration 7/25 | Loss: 0.00104488
Iteration 8/25 | Loss: 0.00104488
Iteration 9/25 | Loss: 0.00104488
Iteration 10/25 | Loss: 0.00104488
Iteration 11/25 | Loss: 0.00104488
Iteration 12/25 | Loss: 0.00104488
Iteration 13/25 | Loss: 0.00104488
Iteration 14/25 | Loss: 0.00104488
Iteration 15/25 | Loss: 0.00104488
Iteration 16/25 | Loss: 0.00104488
Iteration 17/25 | Loss: 0.00104488
Iteration 18/25 | Loss: 0.00104488
Iteration 19/25 | Loss: 0.00104488
Iteration 20/25 | Loss: 0.00104488
Iteration 21/25 | Loss: 0.00104488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010448758257552981, 0.0010448758257552981, 0.0010448758257552981, 0.0010448758257552981, 0.0010448758257552981]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010448758257552981

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104488
Iteration 2/1000 | Loss: 0.00004121
Iteration 3/1000 | Loss: 0.00002949
Iteration 4/1000 | Loss: 0.00002470
Iteration 5/1000 | Loss: 0.00002246
Iteration 6/1000 | Loss: 0.00002187
Iteration 7/1000 | Loss: 0.00002142
Iteration 8/1000 | Loss: 0.00002107
Iteration 9/1000 | Loss: 0.00002101
Iteration 10/1000 | Loss: 0.00002094
Iteration 11/1000 | Loss: 0.00002075
Iteration 12/1000 | Loss: 0.00002067
Iteration 13/1000 | Loss: 0.00002066
Iteration 14/1000 | Loss: 0.00002066
Iteration 15/1000 | Loss: 0.00002065
Iteration 16/1000 | Loss: 0.00002065
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00002064
Iteration 19/1000 | Loss: 0.00002064
Iteration 20/1000 | Loss: 0.00002064
Iteration 21/1000 | Loss: 0.00002064
Iteration 22/1000 | Loss: 0.00002063
Iteration 23/1000 | Loss: 0.00002063
Iteration 24/1000 | Loss: 0.00002062
Iteration 25/1000 | Loss: 0.00002062
Iteration 26/1000 | Loss: 0.00002061
Iteration 27/1000 | Loss: 0.00002061
Iteration 28/1000 | Loss: 0.00002061
Iteration 29/1000 | Loss: 0.00002060
Iteration 30/1000 | Loss: 0.00002060
Iteration 31/1000 | Loss: 0.00002059
Iteration 32/1000 | Loss: 0.00002059
Iteration 33/1000 | Loss: 0.00002059
Iteration 34/1000 | Loss: 0.00002058
Iteration 35/1000 | Loss: 0.00002058
Iteration 36/1000 | Loss: 0.00002058
Iteration 37/1000 | Loss: 0.00002057
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002056
Iteration 42/1000 | Loss: 0.00002056
Iteration 43/1000 | Loss: 0.00002056
Iteration 44/1000 | Loss: 0.00002056
Iteration 45/1000 | Loss: 0.00002056
Iteration 46/1000 | Loss: 0.00002056
Iteration 47/1000 | Loss: 0.00002056
Iteration 48/1000 | Loss: 0.00002056
Iteration 49/1000 | Loss: 0.00002056
Iteration 50/1000 | Loss: 0.00002055
Iteration 51/1000 | Loss: 0.00002055
Iteration 52/1000 | Loss: 0.00002054
Iteration 53/1000 | Loss: 0.00002054
Iteration 54/1000 | Loss: 0.00002053
Iteration 55/1000 | Loss: 0.00002053
Iteration 56/1000 | Loss: 0.00002052
Iteration 57/1000 | Loss: 0.00002052
Iteration 58/1000 | Loss: 0.00002052
Iteration 59/1000 | Loss: 0.00002052
Iteration 60/1000 | Loss: 0.00002052
Iteration 61/1000 | Loss: 0.00002051
Iteration 62/1000 | Loss: 0.00002051
Iteration 63/1000 | Loss: 0.00002050
Iteration 64/1000 | Loss: 0.00002050
Iteration 65/1000 | Loss: 0.00002050
Iteration 66/1000 | Loss: 0.00002050
Iteration 67/1000 | Loss: 0.00002050
Iteration 68/1000 | Loss: 0.00002050
Iteration 69/1000 | Loss: 0.00002049
Iteration 70/1000 | Loss: 0.00002049
Iteration 71/1000 | Loss: 0.00002049
Iteration 72/1000 | Loss: 0.00002049
Iteration 73/1000 | Loss: 0.00002048
Iteration 74/1000 | Loss: 0.00002048
Iteration 75/1000 | Loss: 0.00002048
Iteration 76/1000 | Loss: 0.00002048
Iteration 77/1000 | Loss: 0.00002048
Iteration 78/1000 | Loss: 0.00002047
Iteration 79/1000 | Loss: 0.00002047
Iteration 80/1000 | Loss: 0.00002047
Iteration 81/1000 | Loss: 0.00002047
Iteration 82/1000 | Loss: 0.00002047
Iteration 83/1000 | Loss: 0.00002047
Iteration 84/1000 | Loss: 0.00002046
Iteration 85/1000 | Loss: 0.00002046
Iteration 86/1000 | Loss: 0.00002046
Iteration 87/1000 | Loss: 0.00002046
Iteration 88/1000 | Loss: 0.00002046
Iteration 89/1000 | Loss: 0.00002046
Iteration 90/1000 | Loss: 0.00002046
Iteration 91/1000 | Loss: 0.00002045
Iteration 92/1000 | Loss: 0.00002045
Iteration 93/1000 | Loss: 0.00002045
Iteration 94/1000 | Loss: 0.00002045
Iteration 95/1000 | Loss: 0.00002045
Iteration 96/1000 | Loss: 0.00002045
Iteration 97/1000 | Loss: 0.00002044
Iteration 98/1000 | Loss: 0.00002044
Iteration 99/1000 | Loss: 0.00002044
Iteration 100/1000 | Loss: 0.00002044
Iteration 101/1000 | Loss: 0.00002044
Iteration 102/1000 | Loss: 0.00002044
Iteration 103/1000 | Loss: 0.00002044
Iteration 104/1000 | Loss: 0.00002044
Iteration 105/1000 | Loss: 0.00002044
Iteration 106/1000 | Loss: 0.00002044
Iteration 107/1000 | Loss: 0.00002044
Iteration 108/1000 | Loss: 0.00002044
Iteration 109/1000 | Loss: 0.00002044
Iteration 110/1000 | Loss: 0.00002044
Iteration 111/1000 | Loss: 0.00002044
Iteration 112/1000 | Loss: 0.00002044
Iteration 113/1000 | Loss: 0.00002043
Iteration 114/1000 | Loss: 0.00002043
Iteration 115/1000 | Loss: 0.00002042
Iteration 116/1000 | Loss: 0.00002042
Iteration 117/1000 | Loss: 0.00002042
Iteration 118/1000 | Loss: 0.00002042
Iteration 119/1000 | Loss: 0.00002042
Iteration 120/1000 | Loss: 0.00002041
Iteration 121/1000 | Loss: 0.00002041
Iteration 122/1000 | Loss: 0.00002041
Iteration 123/1000 | Loss: 0.00002041
Iteration 124/1000 | Loss: 0.00002041
Iteration 125/1000 | Loss: 0.00002041
Iteration 126/1000 | Loss: 0.00002041
Iteration 127/1000 | Loss: 0.00002041
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.041405241470784e-05, 2.041405241470784e-05, 2.041405241470784e-05, 2.041405241470784e-05, 2.041405241470784e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.041405241470784e-05

Optimization complete. Final v2v error: 3.887129306793213 mm

Highest mean error: 4.077159404754639 mm for frame 149

Lowest mean error: 3.72175669670105 mm for frame 30

Saving results

Total time: 34.50772500038147
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022309
Iteration 2/25 | Loss: 0.00177587
Iteration 3/25 | Loss: 0.00154214
Iteration 4/25 | Loss: 0.00144121
Iteration 5/25 | Loss: 0.00141486
Iteration 6/25 | Loss: 0.00140897
Iteration 7/25 | Loss: 0.00140711
Iteration 8/25 | Loss: 0.00140646
Iteration 9/25 | Loss: 0.00140611
Iteration 10/25 | Loss: 0.00140592
Iteration 11/25 | Loss: 0.00140580
Iteration 12/25 | Loss: 0.00141287
Iteration 13/25 | Loss: 0.00140844
Iteration 14/25 | Loss: 0.00140648
Iteration 15/25 | Loss: 0.00140239
Iteration 16/25 | Loss: 0.00139915
Iteration 17/25 | Loss: 0.00139778
Iteration 18/25 | Loss: 0.00139717
Iteration 19/25 | Loss: 0.00139701
Iteration 20/25 | Loss: 0.00139701
Iteration 21/25 | Loss: 0.00139700
Iteration 22/25 | Loss: 0.00139700
Iteration 23/25 | Loss: 0.00139700
Iteration 24/25 | Loss: 0.00139700
Iteration 25/25 | Loss: 0.00139700

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31366277
Iteration 2/25 | Loss: 0.00112324
Iteration 3/25 | Loss: 0.00112323
Iteration 4/25 | Loss: 0.00112323
Iteration 5/25 | Loss: 0.00112323
Iteration 6/25 | Loss: 0.00112323
Iteration 7/25 | Loss: 0.00112323
Iteration 8/25 | Loss: 0.00112323
Iteration 9/25 | Loss: 0.00112323
Iteration 10/25 | Loss: 0.00112323
Iteration 11/25 | Loss: 0.00112323
Iteration 12/25 | Loss: 0.00112323
Iteration 13/25 | Loss: 0.00112323
Iteration 14/25 | Loss: 0.00112323
Iteration 15/25 | Loss: 0.00112323
Iteration 16/25 | Loss: 0.00112323
Iteration 17/25 | Loss: 0.00112323
Iteration 18/25 | Loss: 0.00112323
Iteration 19/25 | Loss: 0.00112323
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.001123229623772204, 0.001123229623772204, 0.001123229623772204, 0.001123229623772204, 0.001123229623772204]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001123229623772204

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112323
Iteration 2/1000 | Loss: 0.00012389
Iteration 3/1000 | Loss: 0.00024098
Iteration 4/1000 | Loss: 0.00007319
Iteration 5/1000 | Loss: 0.00005453
Iteration 6/1000 | Loss: 0.00004626
Iteration 7/1000 | Loss: 0.00071180
Iteration 8/1000 | Loss: 0.00107595
Iteration 9/1000 | Loss: 0.00072311
Iteration 10/1000 | Loss: 0.00007843
Iteration 11/1000 | Loss: 0.00005343
Iteration 12/1000 | Loss: 0.00004298
Iteration 13/1000 | Loss: 0.00003685
Iteration 14/1000 | Loss: 0.00003442
Iteration 15/1000 | Loss: 0.00003330
Iteration 16/1000 | Loss: 0.00003272
Iteration 17/1000 | Loss: 0.00003228
Iteration 18/1000 | Loss: 0.00003200
Iteration 19/1000 | Loss: 0.00003178
Iteration 20/1000 | Loss: 0.00003176
Iteration 21/1000 | Loss: 0.00003172
Iteration 22/1000 | Loss: 0.00003172
Iteration 23/1000 | Loss: 0.00003167
Iteration 24/1000 | Loss: 0.00003165
Iteration 25/1000 | Loss: 0.00003165
Iteration 26/1000 | Loss: 0.00003165
Iteration 27/1000 | Loss: 0.00003165
Iteration 28/1000 | Loss: 0.00003165
Iteration 29/1000 | Loss: 0.00003165
Iteration 30/1000 | Loss: 0.00003165
Iteration 31/1000 | Loss: 0.00003165
Iteration 32/1000 | Loss: 0.00003165
Iteration 33/1000 | Loss: 0.00003164
Iteration 34/1000 | Loss: 0.00003164
Iteration 35/1000 | Loss: 0.00003164
Iteration 36/1000 | Loss: 0.00003163
Iteration 37/1000 | Loss: 0.00003162
Iteration 38/1000 | Loss: 0.00003162
Iteration 39/1000 | Loss: 0.00003161
Iteration 40/1000 | Loss: 0.00003161
Iteration 41/1000 | Loss: 0.00003160
Iteration 42/1000 | Loss: 0.00003160
Iteration 43/1000 | Loss: 0.00003159
Iteration 44/1000 | Loss: 0.00003159
Iteration 45/1000 | Loss: 0.00003158
Iteration 46/1000 | Loss: 0.00003158
Iteration 47/1000 | Loss: 0.00003158
Iteration 48/1000 | Loss: 0.00003157
Iteration 49/1000 | Loss: 0.00003156
Iteration 50/1000 | Loss: 0.00003156
Iteration 51/1000 | Loss: 0.00003155
Iteration 52/1000 | Loss: 0.00003155
Iteration 53/1000 | Loss: 0.00003155
Iteration 54/1000 | Loss: 0.00003155
Iteration 55/1000 | Loss: 0.00003154
Iteration 56/1000 | Loss: 0.00003153
Iteration 57/1000 | Loss: 0.00003153
Iteration 58/1000 | Loss: 0.00003152
Iteration 59/1000 | Loss: 0.00003152
Iteration 60/1000 | Loss: 0.00003151
Iteration 61/1000 | Loss: 0.00003151
Iteration 62/1000 | Loss: 0.00003148
Iteration 63/1000 | Loss: 0.00003147
Iteration 64/1000 | Loss: 0.00003146
Iteration 65/1000 | Loss: 0.00003145
Iteration 66/1000 | Loss: 0.00003145
Iteration 67/1000 | Loss: 0.00003144
Iteration 68/1000 | Loss: 0.00003143
Iteration 69/1000 | Loss: 0.00003143
Iteration 70/1000 | Loss: 0.00003143
Iteration 71/1000 | Loss: 0.00003142
Iteration 72/1000 | Loss: 0.00003141
Iteration 73/1000 | Loss: 0.00003141
Iteration 74/1000 | Loss: 0.00003141
Iteration 75/1000 | Loss: 0.00003140
Iteration 76/1000 | Loss: 0.00003140
Iteration 77/1000 | Loss: 0.00003140
Iteration 78/1000 | Loss: 0.00003139
Iteration 79/1000 | Loss: 0.00003139
Iteration 80/1000 | Loss: 0.00003138
Iteration 81/1000 | Loss: 0.00003138
Iteration 82/1000 | Loss: 0.00003138
Iteration 83/1000 | Loss: 0.00003137
Iteration 84/1000 | Loss: 0.00003137
Iteration 85/1000 | Loss: 0.00003137
Iteration 86/1000 | Loss: 0.00003137
Iteration 87/1000 | Loss: 0.00003137
Iteration 88/1000 | Loss: 0.00003137
Iteration 89/1000 | Loss: 0.00003136
Iteration 90/1000 | Loss: 0.00003136
Iteration 91/1000 | Loss: 0.00003136
Iteration 92/1000 | Loss: 0.00003135
Iteration 93/1000 | Loss: 0.00003135
Iteration 94/1000 | Loss: 0.00003135
Iteration 95/1000 | Loss: 0.00003135
Iteration 96/1000 | Loss: 0.00003134
Iteration 97/1000 | Loss: 0.00003134
Iteration 98/1000 | Loss: 0.00003134
Iteration 99/1000 | Loss: 0.00003134
Iteration 100/1000 | Loss: 0.00003134
Iteration 101/1000 | Loss: 0.00003134
Iteration 102/1000 | Loss: 0.00003134
Iteration 103/1000 | Loss: 0.00003134
Iteration 104/1000 | Loss: 0.00003134
Iteration 105/1000 | Loss: 0.00003134
Iteration 106/1000 | Loss: 0.00003133
Iteration 107/1000 | Loss: 0.00003133
Iteration 108/1000 | Loss: 0.00003133
Iteration 109/1000 | Loss: 0.00003133
Iteration 110/1000 | Loss: 0.00003133
Iteration 111/1000 | Loss: 0.00003133
Iteration 112/1000 | Loss: 0.00003133
Iteration 113/1000 | Loss: 0.00003133
Iteration 114/1000 | Loss: 0.00003132
Iteration 115/1000 | Loss: 0.00003132
Iteration 116/1000 | Loss: 0.00003132
Iteration 117/1000 | Loss: 0.00003132
Iteration 118/1000 | Loss: 0.00003132
Iteration 119/1000 | Loss: 0.00003132
Iteration 120/1000 | Loss: 0.00003132
Iteration 121/1000 | Loss: 0.00003132
Iteration 122/1000 | Loss: 0.00003132
Iteration 123/1000 | Loss: 0.00003131
Iteration 124/1000 | Loss: 0.00003131
Iteration 125/1000 | Loss: 0.00003131
Iteration 126/1000 | Loss: 0.00003131
Iteration 127/1000 | Loss: 0.00003131
Iteration 128/1000 | Loss: 0.00003131
Iteration 129/1000 | Loss: 0.00003131
Iteration 130/1000 | Loss: 0.00003131
Iteration 131/1000 | Loss: 0.00003131
Iteration 132/1000 | Loss: 0.00003131
Iteration 133/1000 | Loss: 0.00003131
Iteration 134/1000 | Loss: 0.00003131
Iteration 135/1000 | Loss: 0.00003131
Iteration 136/1000 | Loss: 0.00003131
Iteration 137/1000 | Loss: 0.00003131
Iteration 138/1000 | Loss: 0.00003131
Iteration 139/1000 | Loss: 0.00003131
Iteration 140/1000 | Loss: 0.00003131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [3.1307979952543974e-05, 3.1307979952543974e-05, 3.1307979952543974e-05, 3.1307979952543974e-05, 3.1307979952543974e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1307979952543974e-05

Optimization complete. Final v2v error: 4.792720317840576 mm

Highest mean error: 5.951880931854248 mm for frame 135

Lowest mean error: 4.312865257263184 mm for frame 48

Saving results

Total time: 69.84272074699402
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00581365
Iteration 2/25 | Loss: 0.00146740
Iteration 3/25 | Loss: 0.00130147
Iteration 4/25 | Loss: 0.00127713
Iteration 5/25 | Loss: 0.00127071
Iteration 6/25 | Loss: 0.00126916
Iteration 7/25 | Loss: 0.00126871
Iteration 8/25 | Loss: 0.00126870
Iteration 9/25 | Loss: 0.00126870
Iteration 10/25 | Loss: 0.00126870
Iteration 11/25 | Loss: 0.00126870
Iteration 12/25 | Loss: 0.00126870
Iteration 13/25 | Loss: 0.00126870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012687016278505325, 0.0012687016278505325, 0.0012687016278505325, 0.0012687016278505325, 0.0012687016278505325]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012687016278505325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.41188049
Iteration 2/25 | Loss: 0.00098516
Iteration 3/25 | Loss: 0.00098514
Iteration 4/25 | Loss: 0.00098514
Iteration 5/25 | Loss: 0.00098514
Iteration 6/25 | Loss: 0.00098514
Iteration 7/25 | Loss: 0.00098514
Iteration 8/25 | Loss: 0.00098514
Iteration 9/25 | Loss: 0.00098514
Iteration 10/25 | Loss: 0.00098514
Iteration 11/25 | Loss: 0.00098514
Iteration 12/25 | Loss: 0.00098514
Iteration 13/25 | Loss: 0.00098514
Iteration 14/25 | Loss: 0.00098514
Iteration 15/25 | Loss: 0.00098514
Iteration 16/25 | Loss: 0.00098514
Iteration 17/25 | Loss: 0.00098514
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009851418435573578, 0.0009851418435573578, 0.0009851418435573578, 0.0009851418435573578, 0.0009851418435573578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009851418435573578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098514
Iteration 2/1000 | Loss: 0.00005916
Iteration 3/1000 | Loss: 0.00003928
Iteration 4/1000 | Loss: 0.00003376
Iteration 5/1000 | Loss: 0.00002988
Iteration 6/1000 | Loss: 0.00002817
Iteration 7/1000 | Loss: 0.00002735
Iteration 8/1000 | Loss: 0.00002687
Iteration 9/1000 | Loss: 0.00002662
Iteration 10/1000 | Loss: 0.00002638
Iteration 11/1000 | Loss: 0.00002619
Iteration 12/1000 | Loss: 0.00002617
Iteration 13/1000 | Loss: 0.00002602
Iteration 14/1000 | Loss: 0.00002600
Iteration 15/1000 | Loss: 0.00002600
Iteration 16/1000 | Loss: 0.00002596
Iteration 17/1000 | Loss: 0.00002595
Iteration 18/1000 | Loss: 0.00002594
Iteration 19/1000 | Loss: 0.00002594
Iteration 20/1000 | Loss: 0.00002592
Iteration 21/1000 | Loss: 0.00002592
Iteration 22/1000 | Loss: 0.00002592
Iteration 23/1000 | Loss: 0.00002591
Iteration 24/1000 | Loss: 0.00002591
Iteration 25/1000 | Loss: 0.00002590
Iteration 26/1000 | Loss: 0.00002589
Iteration 27/1000 | Loss: 0.00002588
Iteration 28/1000 | Loss: 0.00002587
Iteration 29/1000 | Loss: 0.00002586
Iteration 30/1000 | Loss: 0.00002584
Iteration 31/1000 | Loss: 0.00002584
Iteration 32/1000 | Loss: 0.00002584
Iteration 33/1000 | Loss: 0.00002584
Iteration 34/1000 | Loss: 0.00002584
Iteration 35/1000 | Loss: 0.00002584
Iteration 36/1000 | Loss: 0.00002584
Iteration 37/1000 | Loss: 0.00002584
Iteration 38/1000 | Loss: 0.00002584
Iteration 39/1000 | Loss: 0.00002583
Iteration 40/1000 | Loss: 0.00002583
Iteration 41/1000 | Loss: 0.00002583
Iteration 42/1000 | Loss: 0.00002583
Iteration 43/1000 | Loss: 0.00002582
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002582
Iteration 46/1000 | Loss: 0.00002581
Iteration 47/1000 | Loss: 0.00002581
Iteration 48/1000 | Loss: 0.00002581
Iteration 49/1000 | Loss: 0.00002581
Iteration 50/1000 | Loss: 0.00002581
Iteration 51/1000 | Loss: 0.00002581
Iteration 52/1000 | Loss: 0.00002581
Iteration 53/1000 | Loss: 0.00002581
Iteration 54/1000 | Loss: 0.00002581
Iteration 55/1000 | Loss: 0.00002581
Iteration 56/1000 | Loss: 0.00002581
Iteration 57/1000 | Loss: 0.00002580
Iteration 58/1000 | Loss: 0.00002580
Iteration 59/1000 | Loss: 0.00002580
Iteration 60/1000 | Loss: 0.00002580
Iteration 61/1000 | Loss: 0.00002580
Iteration 62/1000 | Loss: 0.00002580
Iteration 63/1000 | Loss: 0.00002580
Iteration 64/1000 | Loss: 0.00002580
Iteration 65/1000 | Loss: 0.00002580
Iteration 66/1000 | Loss: 0.00002580
Iteration 67/1000 | Loss: 0.00002580
Iteration 68/1000 | Loss: 0.00002580
Iteration 69/1000 | Loss: 0.00002580
Iteration 70/1000 | Loss: 0.00002580
Iteration 71/1000 | Loss: 0.00002580
Iteration 72/1000 | Loss: 0.00002580
Iteration 73/1000 | Loss: 0.00002580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 73. Stopping optimization.
Last 5 losses: [2.580390719231218e-05, 2.580390719231218e-05, 2.580390719231218e-05, 2.580390719231218e-05, 2.580390719231218e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.580390719231218e-05

Optimization complete. Final v2v error: 4.4054670333862305 mm

Highest mean error: 4.677894592285156 mm for frame 108

Lowest mean error: 4.026068210601807 mm for frame 93

Saving results

Total time: 31.000509023666382
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_41_us_1737/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_41_us_1737/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00991556
Iteration 2/25 | Loss: 0.00178548
Iteration 3/25 | Loss: 0.00145243
Iteration 4/25 | Loss: 0.00140799
Iteration 5/25 | Loss: 0.00140076
Iteration 6/25 | Loss: 0.00138449
Iteration 7/25 | Loss: 0.00137842
Iteration 8/25 | Loss: 0.00136908
Iteration 9/25 | Loss: 0.00136364
Iteration 10/25 | Loss: 0.00136147
Iteration 11/25 | Loss: 0.00136096
Iteration 12/25 | Loss: 0.00136084
Iteration 13/25 | Loss: 0.00136084
Iteration 14/25 | Loss: 0.00136084
Iteration 15/25 | Loss: 0.00136083
Iteration 16/25 | Loss: 0.00136083
Iteration 17/25 | Loss: 0.00136083
Iteration 18/25 | Loss: 0.00136083
Iteration 19/25 | Loss: 0.00136083
Iteration 20/25 | Loss: 0.00136083
Iteration 21/25 | Loss: 0.00136082
Iteration 22/25 | Loss: 0.00136082
Iteration 23/25 | Loss: 0.00136082
Iteration 24/25 | Loss: 0.00136082
Iteration 25/25 | Loss: 0.00136082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.32219505
Iteration 2/25 | Loss: 0.00107979
Iteration 3/25 | Loss: 0.00107978
Iteration 4/25 | Loss: 0.00107978
Iteration 5/25 | Loss: 0.00107978
Iteration 6/25 | Loss: 0.00107978
Iteration 7/25 | Loss: 0.00107978
Iteration 8/25 | Loss: 0.00107978
Iteration 9/25 | Loss: 0.00107978
Iteration 10/25 | Loss: 0.00107978
Iteration 11/25 | Loss: 0.00107978
Iteration 12/25 | Loss: 0.00107978
Iteration 13/25 | Loss: 0.00107978
Iteration 14/25 | Loss: 0.00107978
Iteration 15/25 | Loss: 0.00107978
Iteration 16/25 | Loss: 0.00107978
Iteration 17/25 | Loss: 0.00107978
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010797771392390132, 0.0010797771392390132, 0.0010797771392390132, 0.0010797771392390132, 0.0010797771392390132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010797771392390132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107978
Iteration 2/1000 | Loss: 0.00007511
Iteration 3/1000 | Loss: 0.00005778
Iteration 4/1000 | Loss: 0.00021985
Iteration 5/1000 | Loss: 0.00005721
Iteration 6/1000 | Loss: 0.00004737
Iteration 7/1000 | Loss: 0.00004335
Iteration 8/1000 | Loss: 0.00003986
Iteration 9/1000 | Loss: 0.00003829
Iteration 10/1000 | Loss: 0.00003757
Iteration 11/1000 | Loss: 0.00003691
Iteration 12/1000 | Loss: 0.00003651
Iteration 13/1000 | Loss: 0.00003634
Iteration 14/1000 | Loss: 0.00003625
Iteration 15/1000 | Loss: 0.00003619
Iteration 16/1000 | Loss: 0.00003619
Iteration 17/1000 | Loss: 0.00003607
Iteration 18/1000 | Loss: 0.00003606
Iteration 19/1000 | Loss: 0.00003596
Iteration 20/1000 | Loss: 0.00003595
Iteration 21/1000 | Loss: 0.00003595
Iteration 22/1000 | Loss: 0.00003594
Iteration 23/1000 | Loss: 0.00003594
Iteration 24/1000 | Loss: 0.00003593
Iteration 25/1000 | Loss: 0.00003592
Iteration 26/1000 | Loss: 0.00003592
Iteration 27/1000 | Loss: 0.00003589
Iteration 28/1000 | Loss: 0.00003588
Iteration 29/1000 | Loss: 0.00003588
Iteration 30/1000 | Loss: 0.00003588
Iteration 31/1000 | Loss: 0.00003588
Iteration 32/1000 | Loss: 0.00003587
Iteration 33/1000 | Loss: 0.00003587
Iteration 34/1000 | Loss: 0.00003587
Iteration 35/1000 | Loss: 0.00003587
Iteration 36/1000 | Loss: 0.00003587
Iteration 37/1000 | Loss: 0.00003587
Iteration 38/1000 | Loss: 0.00003587
Iteration 39/1000 | Loss: 0.00003587
Iteration 40/1000 | Loss: 0.00003587
Iteration 41/1000 | Loss: 0.00003586
Iteration 42/1000 | Loss: 0.00003586
Iteration 43/1000 | Loss: 0.00003586
Iteration 44/1000 | Loss: 0.00003586
Iteration 45/1000 | Loss: 0.00003586
Iteration 46/1000 | Loss: 0.00003585
Iteration 47/1000 | Loss: 0.00003585
Iteration 48/1000 | Loss: 0.00003585
Iteration 49/1000 | Loss: 0.00003585
Iteration 50/1000 | Loss: 0.00003585
Iteration 51/1000 | Loss: 0.00003585
Iteration 52/1000 | Loss: 0.00003585
Iteration 53/1000 | Loss: 0.00003585
Iteration 54/1000 | Loss: 0.00003584
Iteration 55/1000 | Loss: 0.00003584
Iteration 56/1000 | Loss: 0.00003584
Iteration 57/1000 | Loss: 0.00003584
Iteration 58/1000 | Loss: 0.00003584
Iteration 59/1000 | Loss: 0.00003584
Iteration 60/1000 | Loss: 0.00003583
Iteration 61/1000 | Loss: 0.00003583
Iteration 62/1000 | Loss: 0.00003583
Iteration 63/1000 | Loss: 0.00003583
Iteration 64/1000 | Loss: 0.00003583
Iteration 65/1000 | Loss: 0.00003583
Iteration 66/1000 | Loss: 0.00003582
Iteration 67/1000 | Loss: 0.00003582
Iteration 68/1000 | Loss: 0.00003582
Iteration 69/1000 | Loss: 0.00003582
Iteration 70/1000 | Loss: 0.00003582
Iteration 71/1000 | Loss: 0.00003582
Iteration 72/1000 | Loss: 0.00003582
Iteration 73/1000 | Loss: 0.00003581
Iteration 74/1000 | Loss: 0.00003581
Iteration 75/1000 | Loss: 0.00003581
Iteration 76/1000 | Loss: 0.00003581
Iteration 77/1000 | Loss: 0.00003581
Iteration 78/1000 | Loss: 0.00003581
Iteration 79/1000 | Loss: 0.00003581
Iteration 80/1000 | Loss: 0.00003581
Iteration 81/1000 | Loss: 0.00003581
Iteration 82/1000 | Loss: 0.00003581
Iteration 83/1000 | Loss: 0.00003581
Iteration 84/1000 | Loss: 0.00003581
Iteration 85/1000 | Loss: 0.00003580
Iteration 86/1000 | Loss: 0.00003580
Iteration 87/1000 | Loss: 0.00003580
Iteration 88/1000 | Loss: 0.00003580
Iteration 89/1000 | Loss: 0.00003580
Iteration 90/1000 | Loss: 0.00003580
Iteration 91/1000 | Loss: 0.00003580
Iteration 92/1000 | Loss: 0.00003580
Iteration 93/1000 | Loss: 0.00003580
Iteration 94/1000 | Loss: 0.00003580
Iteration 95/1000 | Loss: 0.00003580
Iteration 96/1000 | Loss: 0.00003580
Iteration 97/1000 | Loss: 0.00003580
Iteration 98/1000 | Loss: 0.00003580
Iteration 99/1000 | Loss: 0.00003580
Iteration 100/1000 | Loss: 0.00003580
Iteration 101/1000 | Loss: 0.00003580
Iteration 102/1000 | Loss: 0.00003580
Iteration 103/1000 | Loss: 0.00003580
Iteration 104/1000 | Loss: 0.00003580
Iteration 105/1000 | Loss: 0.00003580
Iteration 106/1000 | Loss: 0.00003580
Iteration 107/1000 | Loss: 0.00003580
Iteration 108/1000 | Loss: 0.00003580
Iteration 109/1000 | Loss: 0.00003580
Iteration 110/1000 | Loss: 0.00003580
Iteration 111/1000 | Loss: 0.00003580
Iteration 112/1000 | Loss: 0.00003580
Iteration 113/1000 | Loss: 0.00003580
Iteration 114/1000 | Loss: 0.00003580
Iteration 115/1000 | Loss: 0.00003580
Iteration 116/1000 | Loss: 0.00003580
Iteration 117/1000 | Loss: 0.00003580
Iteration 118/1000 | Loss: 0.00003580
Iteration 119/1000 | Loss: 0.00003580
Iteration 120/1000 | Loss: 0.00003580
Iteration 121/1000 | Loss: 0.00003580
Iteration 122/1000 | Loss: 0.00003580
Iteration 123/1000 | Loss: 0.00003580
Iteration 124/1000 | Loss: 0.00003580
Iteration 125/1000 | Loss: 0.00003580
Iteration 126/1000 | Loss: 0.00003580
Iteration 127/1000 | Loss: 0.00003580
Iteration 128/1000 | Loss: 0.00003580
Iteration 129/1000 | Loss: 0.00003580
Iteration 130/1000 | Loss: 0.00003580
Iteration 131/1000 | Loss: 0.00003580
Iteration 132/1000 | Loss: 0.00003580
Iteration 133/1000 | Loss: 0.00003580
Iteration 134/1000 | Loss: 0.00003580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [3.5796860174741596e-05, 3.5796860174741596e-05, 3.5796860174741596e-05, 3.5796860174741596e-05, 3.5796860174741596e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5796860174741596e-05

Optimization complete. Final v2v error: 5.076594352722168 mm

Highest mean error: 6.242743968963623 mm for frame 221

Lowest mean error: 4.642256736755371 mm for frame 101

Saving results

Total time: 55.13666844367981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01104998
Iteration 2/25 | Loss: 0.01104998
Iteration 3/25 | Loss: 0.00189477
Iteration 4/25 | Loss: 0.00150736
Iteration 5/25 | Loss: 0.00137799
Iteration 6/25 | Loss: 0.00115657
Iteration 7/25 | Loss: 0.00113872
Iteration 8/25 | Loss: 0.00108150
Iteration 9/25 | Loss: 0.00104121
Iteration 10/25 | Loss: 0.00102921
Iteration 11/25 | Loss: 0.00102580
Iteration 12/25 | Loss: 0.00101852
Iteration 13/25 | Loss: 0.00101393
Iteration 14/25 | Loss: 0.00101248
Iteration 15/25 | Loss: 0.00099809
Iteration 16/25 | Loss: 0.00099798
Iteration 17/25 | Loss: 0.00100297
Iteration 18/25 | Loss: 0.00100092
Iteration 19/25 | Loss: 0.00100666
Iteration 20/25 | Loss: 0.00101148
Iteration 21/25 | Loss: 0.00096918
Iteration 22/25 | Loss: 0.00097056
Iteration 23/25 | Loss: 0.00096475
Iteration 24/25 | Loss: 0.00095795
Iteration 25/25 | Loss: 0.00095835

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34719384
Iteration 2/25 | Loss: 0.00096401
Iteration 3/25 | Loss: 0.00084600
Iteration 4/25 | Loss: 0.00076506
Iteration 5/25 | Loss: 0.00076506
Iteration 6/25 | Loss: 0.00076506
Iteration 7/25 | Loss: 0.00076506
Iteration 8/25 | Loss: 0.00076506
Iteration 9/25 | Loss: 0.00076506
Iteration 10/25 | Loss: 0.00076506
Iteration 11/25 | Loss: 0.00076506
Iteration 12/25 | Loss: 0.00076506
Iteration 13/25 | Loss: 0.00076506
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007650608313269913, 0.0007650608313269913, 0.0007650608313269913, 0.0007650608313269913, 0.0007650608313269913]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007650608313269913

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076506
Iteration 2/1000 | Loss: 0.00048901
Iteration 3/1000 | Loss: 0.00089385
Iteration 4/1000 | Loss: 0.00020056
Iteration 5/1000 | Loss: 0.00027193
Iteration 6/1000 | Loss: 0.00014272
Iteration 7/1000 | Loss: 0.00013568
Iteration 8/1000 | Loss: 0.00015478
Iteration 9/1000 | Loss: 0.00020337
Iteration 10/1000 | Loss: 0.00018118
Iteration 11/1000 | Loss: 0.00017537
Iteration 12/1000 | Loss: 0.00008365
Iteration 13/1000 | Loss: 0.00014659
Iteration 14/1000 | Loss: 0.00009177
Iteration 15/1000 | Loss: 0.00013637
Iteration 16/1000 | Loss: 0.00018211
Iteration 17/1000 | Loss: 0.00016156
Iteration 18/1000 | Loss: 0.00018591
Iteration 19/1000 | Loss: 0.00034754
Iteration 20/1000 | Loss: 0.00016772
Iteration 21/1000 | Loss: 0.00019408
Iteration 22/1000 | Loss: 0.00018748
Iteration 23/1000 | Loss: 0.00056915
Iteration 24/1000 | Loss: 0.00007768
Iteration 25/1000 | Loss: 0.00008526
Iteration 26/1000 | Loss: 0.00014274
Iteration 27/1000 | Loss: 0.00008439
Iteration 28/1000 | Loss: 0.00018027
Iteration 29/1000 | Loss: 0.00017850
Iteration 30/1000 | Loss: 0.00042445
Iteration 31/1000 | Loss: 0.00081175
Iteration 32/1000 | Loss: 0.00045296
Iteration 33/1000 | Loss: 0.00086000
Iteration 34/1000 | Loss: 0.00057295
Iteration 35/1000 | Loss: 0.00018911
Iteration 36/1000 | Loss: 0.00018467
Iteration 37/1000 | Loss: 0.00017680
Iteration 38/1000 | Loss: 0.00016118
Iteration 39/1000 | Loss: 0.00010979
Iteration 40/1000 | Loss: 0.00011542
Iteration 41/1000 | Loss: 0.00010131
Iteration 42/1000 | Loss: 0.00009854
Iteration 43/1000 | Loss: 0.00015970
Iteration 44/1000 | Loss: 0.00011410
Iteration 45/1000 | Loss: 0.00016429
Iteration 46/1000 | Loss: 0.00021757
Iteration 47/1000 | Loss: 0.00013949
Iteration 48/1000 | Loss: 0.00028002
Iteration 49/1000 | Loss: 0.00014572
Iteration 50/1000 | Loss: 0.00011932
Iteration 51/1000 | Loss: 0.00018208
Iteration 52/1000 | Loss: 0.00014664
Iteration 53/1000 | Loss: 0.00013798
Iteration 54/1000 | Loss: 0.00020536
Iteration 55/1000 | Loss: 0.00015761
Iteration 56/1000 | Loss: 0.00018070
Iteration 57/1000 | Loss: 0.00014656
Iteration 58/1000 | Loss: 0.00017929
Iteration 59/1000 | Loss: 0.00022182
Iteration 60/1000 | Loss: 0.00016610
Iteration 61/1000 | Loss: 0.00038594
Iteration 62/1000 | Loss: 0.00039552
Iteration 63/1000 | Loss: 0.00024305
Iteration 64/1000 | Loss: 0.00012014
Iteration 65/1000 | Loss: 0.00021173
Iteration 66/1000 | Loss: 0.00013326
Iteration 67/1000 | Loss: 0.00025950
Iteration 68/1000 | Loss: 0.00019213
Iteration 69/1000 | Loss: 0.00005739
Iteration 70/1000 | Loss: 0.00024462
Iteration 71/1000 | Loss: 0.00045299
Iteration 72/1000 | Loss: 0.00038210
Iteration 73/1000 | Loss: 0.00037278
Iteration 74/1000 | Loss: 0.00024803
Iteration 75/1000 | Loss: 0.00006987
Iteration 76/1000 | Loss: 0.00010000
Iteration 77/1000 | Loss: 0.00005632
Iteration 78/1000 | Loss: 0.00006119
Iteration 79/1000 | Loss: 0.00016774
Iteration 80/1000 | Loss: 0.00020661
Iteration 81/1000 | Loss: 0.00006808
Iteration 82/1000 | Loss: 0.00004496
Iteration 83/1000 | Loss: 0.00004386
Iteration 84/1000 | Loss: 0.00050124
Iteration 85/1000 | Loss: 0.00004916
Iteration 86/1000 | Loss: 0.00004392
Iteration 87/1000 | Loss: 0.00004183
Iteration 88/1000 | Loss: 0.00004094
Iteration 89/1000 | Loss: 0.00003998
Iteration 90/1000 | Loss: 0.00003897
Iteration 91/1000 | Loss: 0.00003864
Iteration 92/1000 | Loss: 0.00003844
Iteration 93/1000 | Loss: 0.00003843
Iteration 94/1000 | Loss: 0.00003833
Iteration 95/1000 | Loss: 0.00016022
Iteration 96/1000 | Loss: 0.00016021
Iteration 97/1000 | Loss: 0.00046937
Iteration 98/1000 | Loss: 0.00013610
Iteration 99/1000 | Loss: 0.00015695
Iteration 100/1000 | Loss: 0.00024578
Iteration 101/1000 | Loss: 0.00027105
Iteration 102/1000 | Loss: 0.00014336
Iteration 103/1000 | Loss: 0.00015061
Iteration 104/1000 | Loss: 0.00003851
Iteration 105/1000 | Loss: 0.00003817
Iteration 106/1000 | Loss: 0.00003816
Iteration 107/1000 | Loss: 0.00003814
Iteration 108/1000 | Loss: 0.00003813
Iteration 109/1000 | Loss: 0.00015899
Iteration 110/1000 | Loss: 0.00027265
Iteration 111/1000 | Loss: 0.00036804
Iteration 112/1000 | Loss: 0.00014666
Iteration 113/1000 | Loss: 0.00013746
Iteration 114/1000 | Loss: 0.00003901
Iteration 115/1000 | Loss: 0.00003820
Iteration 116/1000 | Loss: 0.00003804
Iteration 117/1000 | Loss: 0.00003803
Iteration 118/1000 | Loss: 0.00003803
Iteration 119/1000 | Loss: 0.00003802
Iteration 120/1000 | Loss: 0.00003801
Iteration 121/1000 | Loss: 0.00003801
Iteration 122/1000 | Loss: 0.00003800
Iteration 123/1000 | Loss: 0.00003799
Iteration 124/1000 | Loss: 0.00003799
Iteration 125/1000 | Loss: 0.00003799
Iteration 126/1000 | Loss: 0.00003798
Iteration 127/1000 | Loss: 0.00003798
Iteration 128/1000 | Loss: 0.00003798
Iteration 129/1000 | Loss: 0.00003797
Iteration 130/1000 | Loss: 0.00003797
Iteration 131/1000 | Loss: 0.00003797
Iteration 132/1000 | Loss: 0.00003797
Iteration 133/1000 | Loss: 0.00003797
Iteration 134/1000 | Loss: 0.00003797
Iteration 135/1000 | Loss: 0.00003797
Iteration 136/1000 | Loss: 0.00003796
Iteration 137/1000 | Loss: 0.00003796
Iteration 138/1000 | Loss: 0.00003796
Iteration 139/1000 | Loss: 0.00003796
Iteration 140/1000 | Loss: 0.00003796
Iteration 141/1000 | Loss: 0.00003796
Iteration 142/1000 | Loss: 0.00003795
Iteration 143/1000 | Loss: 0.00003795
Iteration 144/1000 | Loss: 0.00003795
Iteration 145/1000 | Loss: 0.00003794
Iteration 146/1000 | Loss: 0.00003794
Iteration 147/1000 | Loss: 0.00003794
Iteration 148/1000 | Loss: 0.00003794
Iteration 149/1000 | Loss: 0.00016744
Iteration 150/1000 | Loss: 0.00019359
Iteration 151/1000 | Loss: 0.00016334
Iteration 152/1000 | Loss: 0.00018728
Iteration 153/1000 | Loss: 0.00022042
Iteration 154/1000 | Loss: 0.00019312
Iteration 155/1000 | Loss: 0.00020791
Iteration 156/1000 | Loss: 0.00020937
Iteration 157/1000 | Loss: 0.00028057
Iteration 158/1000 | Loss: 0.00005236
Iteration 159/1000 | Loss: 0.00004572
Iteration 160/1000 | Loss: 0.00099329
Iteration 161/1000 | Loss: 0.00047655
Iteration 162/1000 | Loss: 0.00007538
Iteration 163/1000 | Loss: 0.00010598
Iteration 164/1000 | Loss: 0.00015284
Iteration 165/1000 | Loss: 0.00006412
Iteration 166/1000 | Loss: 0.00022630
Iteration 167/1000 | Loss: 0.00018674
Iteration 168/1000 | Loss: 0.00019538
Iteration 169/1000 | Loss: 0.00005450
Iteration 170/1000 | Loss: 0.00026238
Iteration 171/1000 | Loss: 0.00014486
Iteration 172/1000 | Loss: 0.00003925
Iteration 173/1000 | Loss: 0.00014474
Iteration 174/1000 | Loss: 0.00009446
Iteration 175/1000 | Loss: 0.00020951
Iteration 176/1000 | Loss: 0.00010890
Iteration 177/1000 | Loss: 0.00020166
Iteration 178/1000 | Loss: 0.00013737
Iteration 179/1000 | Loss: 0.00004472
Iteration 180/1000 | Loss: 0.00004282
Iteration 181/1000 | Loss: 0.00004144
Iteration 182/1000 | Loss: 0.00004068
Iteration 183/1000 | Loss: 0.00006317
Iteration 184/1000 | Loss: 0.00004782
Iteration 185/1000 | Loss: 0.00006581
Iteration 186/1000 | Loss: 0.00004705
Iteration 187/1000 | Loss: 0.00006898
Iteration 188/1000 | Loss: 0.00004263
Iteration 189/1000 | Loss: 0.00019055
Iteration 190/1000 | Loss: 0.00006062
Iteration 191/1000 | Loss: 0.00005974
Iteration 192/1000 | Loss: 0.00005397
Iteration 193/1000 | Loss: 0.00005362
Iteration 194/1000 | Loss: 0.00005229
Iteration 195/1000 | Loss: 0.00005298
Iteration 196/1000 | Loss: 0.00004976
Iteration 197/1000 | Loss: 0.00005080
Iteration 198/1000 | Loss: 0.00004292
Iteration 199/1000 | Loss: 0.00003874
Iteration 200/1000 | Loss: 0.00012687
Iteration 201/1000 | Loss: 0.00005035
Iteration 202/1000 | Loss: 0.00007450
Iteration 203/1000 | Loss: 0.00006106
Iteration 204/1000 | Loss: 0.00004192
Iteration 205/1000 | Loss: 0.00004876
Iteration 206/1000 | Loss: 0.00004670
Iteration 207/1000 | Loss: 0.00008878
Iteration 208/1000 | Loss: 0.00005006
Iteration 209/1000 | Loss: 0.00007343
Iteration 210/1000 | Loss: 0.00005238
Iteration 211/1000 | Loss: 0.00006255
Iteration 212/1000 | Loss: 0.00009234
Iteration 213/1000 | Loss: 0.00009638
Iteration 214/1000 | Loss: 0.00004848
Iteration 215/1000 | Loss: 0.00010768
Iteration 216/1000 | Loss: 0.00005521
Iteration 217/1000 | Loss: 0.00005392
Iteration 218/1000 | Loss: 0.00005465
Iteration 219/1000 | Loss: 0.00004561
Iteration 220/1000 | Loss: 0.00004512
Iteration 221/1000 | Loss: 0.00005674
Iteration 222/1000 | Loss: 0.00046968
Iteration 223/1000 | Loss: 0.00007287
Iteration 224/1000 | Loss: 0.00006742
Iteration 225/1000 | Loss: 0.00005577
Iteration 226/1000 | Loss: 0.00006632
Iteration 227/1000 | Loss: 0.00006728
Iteration 228/1000 | Loss: 0.00005110
Iteration 229/1000 | Loss: 0.00005152
Iteration 230/1000 | Loss: 0.00005194
Iteration 231/1000 | Loss: 0.00004566
Iteration 232/1000 | Loss: 0.00004308
Iteration 233/1000 | Loss: 0.00005426
Iteration 234/1000 | Loss: 0.00005337
Iteration 235/1000 | Loss: 0.00005454
Iteration 236/1000 | Loss: 0.00005187
Iteration 237/1000 | Loss: 0.00005386
Iteration 238/1000 | Loss: 0.00033340
Iteration 239/1000 | Loss: 0.00009408
Iteration 240/1000 | Loss: 0.00038585
Iteration 241/1000 | Loss: 0.00030907
Iteration 242/1000 | Loss: 0.00005276
Iteration 243/1000 | Loss: 0.00005075
Iteration 244/1000 | Loss: 0.00004866
Iteration 245/1000 | Loss: 0.00015234
Iteration 246/1000 | Loss: 0.00005176
Iteration 247/1000 | Loss: 0.00005155
Iteration 248/1000 | Loss: 0.00005255
Iteration 249/1000 | Loss: 0.00005219
Iteration 250/1000 | Loss: 0.00005019
Iteration 251/1000 | Loss: 0.00005178
Iteration 252/1000 | Loss: 0.00005040
Iteration 253/1000 | Loss: 0.00029726
Iteration 254/1000 | Loss: 0.00010197
Iteration 255/1000 | Loss: 0.00005014
Iteration 256/1000 | Loss: 0.00004844
Iteration 257/1000 | Loss: 0.00005492
Iteration 258/1000 | Loss: 0.00005156
Iteration 259/1000 | Loss: 0.00004993
Iteration 260/1000 | Loss: 0.00005355
Iteration 261/1000 | Loss: 0.00005005
Iteration 262/1000 | Loss: 0.00005329
Iteration 263/1000 | Loss: 0.00004979
Iteration 264/1000 | Loss: 0.00005299
Iteration 265/1000 | Loss: 0.00004975
Iteration 266/1000 | Loss: 0.00004691
Iteration 267/1000 | Loss: 0.00010251
Iteration 268/1000 | Loss: 0.00006200
Iteration 269/1000 | Loss: 0.00004995
Iteration 270/1000 | Loss: 0.00004708
Iteration 271/1000 | Loss: 0.00004743
Iteration 272/1000 | Loss: 0.00004821
Iteration 273/1000 | Loss: 0.00005251
Iteration 274/1000 | Loss: 0.00004781
Iteration 275/1000 | Loss: 0.00005300
Iteration 276/1000 | Loss: 0.00005118
Iteration 277/1000 | Loss: 0.00005697
Iteration 278/1000 | Loss: 0.00007220
Iteration 279/1000 | Loss: 0.00005117
Iteration 280/1000 | Loss: 0.00005169
Iteration 281/1000 | Loss: 0.00005582
Iteration 282/1000 | Loss: 0.00005800
Iteration 283/1000 | Loss: 0.00004361
Iteration 284/1000 | Loss: 0.00005137
Iteration 285/1000 | Loss: 0.00006720
Iteration 286/1000 | Loss: 0.00005293
Iteration 287/1000 | Loss: 0.00006041
Iteration 288/1000 | Loss: 0.00004060
Iteration 289/1000 | Loss: 0.00006340
Iteration 290/1000 | Loss: 0.00019076
Iteration 291/1000 | Loss: 0.00003876
Iteration 292/1000 | Loss: 0.00003805
Iteration 293/1000 | Loss: 0.00003772
Iteration 294/1000 | Loss: 0.00003739
Iteration 295/1000 | Loss: 0.00003737
Iteration 296/1000 | Loss: 0.00003731
Iteration 297/1000 | Loss: 0.00019856
Iteration 298/1000 | Loss: 0.00003749
Iteration 299/1000 | Loss: 0.00003731
Iteration 300/1000 | Loss: 0.00003725
Iteration 301/1000 | Loss: 0.00003725
Iteration 302/1000 | Loss: 0.00003724
Iteration 303/1000 | Loss: 0.00011912
Iteration 304/1000 | Loss: 0.00006291
Iteration 305/1000 | Loss: 0.00005959
Iteration 306/1000 | Loss: 0.00003732
Iteration 307/1000 | Loss: 0.00003729
Iteration 308/1000 | Loss: 0.00003726
Iteration 309/1000 | Loss: 0.00003726
Iteration 310/1000 | Loss: 0.00003725
Iteration 311/1000 | Loss: 0.00003725
Iteration 312/1000 | Loss: 0.00003725
Iteration 313/1000 | Loss: 0.00003725
Iteration 314/1000 | Loss: 0.00003725
Iteration 315/1000 | Loss: 0.00003724
Iteration 316/1000 | Loss: 0.00003724
Iteration 317/1000 | Loss: 0.00003724
Iteration 318/1000 | Loss: 0.00003723
Iteration 319/1000 | Loss: 0.00003723
Iteration 320/1000 | Loss: 0.00003723
Iteration 321/1000 | Loss: 0.00003723
Iteration 322/1000 | Loss: 0.00003723
Iteration 323/1000 | Loss: 0.00003723
Iteration 324/1000 | Loss: 0.00003723
Iteration 325/1000 | Loss: 0.00003723
Iteration 326/1000 | Loss: 0.00003723
Iteration 327/1000 | Loss: 0.00003723
Iteration 328/1000 | Loss: 0.00003722
Iteration 329/1000 | Loss: 0.00003722
Iteration 330/1000 | Loss: 0.00003722
Iteration 331/1000 | Loss: 0.00003722
Iteration 332/1000 | Loss: 0.00003722
Iteration 333/1000 | Loss: 0.00003722
Iteration 334/1000 | Loss: 0.00003722
Iteration 335/1000 | Loss: 0.00003721
Iteration 336/1000 | Loss: 0.00003721
Iteration 337/1000 | Loss: 0.00003721
Iteration 338/1000 | Loss: 0.00003721
Iteration 339/1000 | Loss: 0.00003720
Iteration 340/1000 | Loss: 0.00003720
Iteration 341/1000 | Loss: 0.00003720
Iteration 342/1000 | Loss: 0.00003720
Iteration 343/1000 | Loss: 0.00003720
Iteration 344/1000 | Loss: 0.00003720
Iteration 345/1000 | Loss: 0.00003720
Iteration 346/1000 | Loss: 0.00003720
Iteration 347/1000 | Loss: 0.00003719
Iteration 348/1000 | Loss: 0.00003719
Iteration 349/1000 | Loss: 0.00003719
Iteration 350/1000 | Loss: 0.00003719
Iteration 351/1000 | Loss: 0.00003718
Iteration 352/1000 | Loss: 0.00003718
Iteration 353/1000 | Loss: 0.00003718
Iteration 354/1000 | Loss: 0.00003718
Iteration 355/1000 | Loss: 0.00003718
Iteration 356/1000 | Loss: 0.00003718
Iteration 357/1000 | Loss: 0.00003718
Iteration 358/1000 | Loss: 0.00003718
Iteration 359/1000 | Loss: 0.00003718
Iteration 360/1000 | Loss: 0.00003718
Iteration 361/1000 | Loss: 0.00003718
Iteration 362/1000 | Loss: 0.00003718
Iteration 363/1000 | Loss: 0.00003718
Iteration 364/1000 | Loss: 0.00003718
Iteration 365/1000 | Loss: 0.00003717
Iteration 366/1000 | Loss: 0.00003717
Iteration 367/1000 | Loss: 0.00003717
Iteration 368/1000 | Loss: 0.00003717
Iteration 369/1000 | Loss: 0.00003717
Iteration 370/1000 | Loss: 0.00003717
Iteration 371/1000 | Loss: 0.00003716
Iteration 372/1000 | Loss: 0.00003716
Iteration 373/1000 | Loss: 0.00003716
Iteration 374/1000 | Loss: 0.00003716
Iteration 375/1000 | Loss: 0.00003716
Iteration 376/1000 | Loss: 0.00003716
Iteration 377/1000 | Loss: 0.00003716
Iteration 378/1000 | Loss: 0.00003716
Iteration 379/1000 | Loss: 0.00003716
Iteration 380/1000 | Loss: 0.00003716
Iteration 381/1000 | Loss: 0.00003716
Iteration 382/1000 | Loss: 0.00003716
Iteration 383/1000 | Loss: 0.00003716
Iteration 384/1000 | Loss: 0.00003716
Iteration 385/1000 | Loss: 0.00003716
Iteration 386/1000 | Loss: 0.00003716
Iteration 387/1000 | Loss: 0.00003716
Iteration 388/1000 | Loss: 0.00003716
Iteration 389/1000 | Loss: 0.00003716
Iteration 390/1000 | Loss: 0.00003716
Iteration 391/1000 | Loss: 0.00003716
Iteration 392/1000 | Loss: 0.00003716
Iteration 393/1000 | Loss: 0.00003716
Iteration 394/1000 | Loss: 0.00003716
Iteration 395/1000 | Loss: 0.00003716
Iteration 396/1000 | Loss: 0.00003716
Iteration 397/1000 | Loss: 0.00003716
Iteration 398/1000 | Loss: 0.00003716
Iteration 399/1000 | Loss: 0.00003716
Iteration 400/1000 | Loss: 0.00003716
Iteration 401/1000 | Loss: 0.00003716
Iteration 402/1000 | Loss: 0.00003716
Iteration 403/1000 | Loss: 0.00003716
Iteration 404/1000 | Loss: 0.00003716
Iteration 405/1000 | Loss: 0.00003716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 405. Stopping optimization.
Last 5 losses: [3.715802085935138e-05, 3.715802085935138e-05, 3.715802085935138e-05, 3.715802085935138e-05, 3.715802085935138e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.715802085935138e-05

Optimization complete. Final v2v error: 4.399295330047607 mm

Highest mean error: 22.409299850463867 mm for frame 72

Lowest mean error: 3.6250030994415283 mm for frame 154

Saving results

Total time: 438.3715589046478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01120838
Iteration 2/25 | Loss: 0.00174023
Iteration 3/25 | Loss: 0.00106610
Iteration 4/25 | Loss: 0.00096817
Iteration 5/25 | Loss: 0.00091815
Iteration 6/25 | Loss: 0.00088000
Iteration 7/25 | Loss: 0.00086395
Iteration 8/25 | Loss: 0.00086672
Iteration 9/25 | Loss: 0.00085605
Iteration 10/25 | Loss: 0.00085458
Iteration 11/25 | Loss: 0.00085596
Iteration 12/25 | Loss: 0.00085507
Iteration 13/25 | Loss: 0.00085290
Iteration 14/25 | Loss: 0.00085206
Iteration 15/25 | Loss: 0.00085189
Iteration 16/25 | Loss: 0.00085186
Iteration 17/25 | Loss: 0.00085185
Iteration 18/25 | Loss: 0.00085184
Iteration 19/25 | Loss: 0.00085184
Iteration 20/25 | Loss: 0.00085182
Iteration 21/25 | Loss: 0.00085174
Iteration 22/25 | Loss: 0.00085163
Iteration 23/25 | Loss: 0.00085169
Iteration 24/25 | Loss: 0.00085168
Iteration 25/25 | Loss: 0.00085166

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52736366
Iteration 2/25 | Loss: 0.00043470
Iteration 3/25 | Loss: 0.00043470
Iteration 4/25 | Loss: 0.00043470
Iteration 5/25 | Loss: 0.00043470
Iteration 6/25 | Loss: 0.00043470
Iteration 7/25 | Loss: 0.00043470
Iteration 8/25 | Loss: 0.00043470
Iteration 9/25 | Loss: 0.00043470
Iteration 10/25 | Loss: 0.00043470
Iteration 11/25 | Loss: 0.00043470
Iteration 12/25 | Loss: 0.00043470
Iteration 13/25 | Loss: 0.00043470
Iteration 14/25 | Loss: 0.00043470
Iteration 15/25 | Loss: 0.00043470
Iteration 16/25 | Loss: 0.00043470
Iteration 17/25 | Loss: 0.00043470
Iteration 18/25 | Loss: 0.00043470
Iteration 19/25 | Loss: 0.00043470
Iteration 20/25 | Loss: 0.00043470
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00043469679076224566, 0.00043469679076224566, 0.00043469679076224566, 0.00043469679076224566, 0.00043469679076224566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00043469679076224566

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00043470
Iteration 2/1000 | Loss: 0.00020646
Iteration 3/1000 | Loss: 0.00003846
Iteration 4/1000 | Loss: 0.00003444
Iteration 5/1000 | Loss: 0.00003196
Iteration 6/1000 | Loss: 0.00003048
Iteration 7/1000 | Loss: 0.00002945
Iteration 8/1000 | Loss: 0.00002884
Iteration 9/1000 | Loss: 0.00002837
Iteration 10/1000 | Loss: 0.00002799
Iteration 11/1000 | Loss: 0.00002762
Iteration 12/1000 | Loss: 0.00002738
Iteration 13/1000 | Loss: 0.00002730
Iteration 14/1000 | Loss: 0.00002729
Iteration 15/1000 | Loss: 0.00002728
Iteration 16/1000 | Loss: 0.00002728
Iteration 17/1000 | Loss: 0.00002727
Iteration 18/1000 | Loss: 0.00002726
Iteration 19/1000 | Loss: 0.00002726
Iteration 20/1000 | Loss: 0.00002722
Iteration 21/1000 | Loss: 0.00002717
Iteration 22/1000 | Loss: 0.00002717
Iteration 23/1000 | Loss: 0.00002717
Iteration 24/1000 | Loss: 0.00002717
Iteration 25/1000 | Loss: 0.00002717
Iteration 26/1000 | Loss: 0.00002717
Iteration 27/1000 | Loss: 0.00002717
Iteration 28/1000 | Loss: 0.00002716
Iteration 29/1000 | Loss: 0.00002716
Iteration 30/1000 | Loss: 0.00002716
Iteration 31/1000 | Loss: 0.00002716
Iteration 32/1000 | Loss: 0.00002716
Iteration 33/1000 | Loss: 0.00002715
Iteration 34/1000 | Loss: 0.00002711
Iteration 35/1000 | Loss: 0.00002710
Iteration 36/1000 | Loss: 0.00002710
Iteration 37/1000 | Loss: 0.00002706
Iteration 38/1000 | Loss: 0.00002706
Iteration 39/1000 | Loss: 0.00002704
Iteration 40/1000 | Loss: 0.00002704
Iteration 41/1000 | Loss: 0.00002704
Iteration 42/1000 | Loss: 0.00002704
Iteration 43/1000 | Loss: 0.00002704
Iteration 44/1000 | Loss: 0.00002704
Iteration 45/1000 | Loss: 0.00002704
Iteration 46/1000 | Loss: 0.00002703
Iteration 47/1000 | Loss: 0.00002703
Iteration 48/1000 | Loss: 0.00002703
Iteration 49/1000 | Loss: 0.00002703
Iteration 50/1000 | Loss: 0.00002702
Iteration 51/1000 | Loss: 0.00002702
Iteration 52/1000 | Loss: 0.00002701
Iteration 53/1000 | Loss: 0.00002701
Iteration 54/1000 | Loss: 0.00002700
Iteration 55/1000 | Loss: 0.00002700
Iteration 56/1000 | Loss: 0.00002700
Iteration 57/1000 | Loss: 0.00002700
Iteration 58/1000 | Loss: 0.00002699
Iteration 59/1000 | Loss: 0.00002699
Iteration 60/1000 | Loss: 0.00002698
Iteration 61/1000 | Loss: 0.00002698
Iteration 62/1000 | Loss: 0.00002698
Iteration 63/1000 | Loss: 0.00002698
Iteration 64/1000 | Loss: 0.00002698
Iteration 65/1000 | Loss: 0.00002698
Iteration 66/1000 | Loss: 0.00002698
Iteration 67/1000 | Loss: 0.00002698
Iteration 68/1000 | Loss: 0.00002697
Iteration 69/1000 | Loss: 0.00002697
Iteration 70/1000 | Loss: 0.00002697
Iteration 71/1000 | Loss: 0.00002697
Iteration 72/1000 | Loss: 0.00002697
Iteration 73/1000 | Loss: 0.00002697
Iteration 74/1000 | Loss: 0.00002697
Iteration 75/1000 | Loss: 0.00002696
Iteration 76/1000 | Loss: 0.00002696
Iteration 77/1000 | Loss: 0.00002696
Iteration 78/1000 | Loss: 0.00002695
Iteration 79/1000 | Loss: 0.00002695
Iteration 80/1000 | Loss: 0.00002695
Iteration 81/1000 | Loss: 0.00002695
Iteration 82/1000 | Loss: 0.00002695
Iteration 83/1000 | Loss: 0.00002695
Iteration 84/1000 | Loss: 0.00002694
Iteration 85/1000 | Loss: 0.00002694
Iteration 86/1000 | Loss: 0.00002699
Iteration 87/1000 | Loss: 0.00002699
Iteration 88/1000 | Loss: 0.00002698
Iteration 89/1000 | Loss: 0.00002698
Iteration 90/1000 | Loss: 0.00002698
Iteration 91/1000 | Loss: 0.00002698
Iteration 92/1000 | Loss: 0.00002698
Iteration 93/1000 | Loss: 0.00002697
Iteration 94/1000 | Loss: 0.00002697
Iteration 95/1000 | Loss: 0.00002697
Iteration 96/1000 | Loss: 0.00002697
Iteration 97/1000 | Loss: 0.00002697
Iteration 98/1000 | Loss: 0.00002697
Iteration 99/1000 | Loss: 0.00002697
Iteration 100/1000 | Loss: 0.00002697
Iteration 101/1000 | Loss: 0.00002696
Iteration 102/1000 | Loss: 0.00002696
Iteration 103/1000 | Loss: 0.00002696
Iteration 104/1000 | Loss: 0.00002696
Iteration 105/1000 | Loss: 0.00002696
Iteration 106/1000 | Loss: 0.00002696
Iteration 107/1000 | Loss: 0.00002696
Iteration 108/1000 | Loss: 0.00002696
Iteration 109/1000 | Loss: 0.00002696
Iteration 110/1000 | Loss: 0.00002696
Iteration 111/1000 | Loss: 0.00002696
Iteration 112/1000 | Loss: 0.00002696
Iteration 113/1000 | Loss: 0.00002696
Iteration 114/1000 | Loss: 0.00002696
Iteration 115/1000 | Loss: 0.00002696
Iteration 116/1000 | Loss: 0.00002696
Iteration 117/1000 | Loss: 0.00002696
Iteration 118/1000 | Loss: 0.00002696
Iteration 119/1000 | Loss: 0.00002696
Iteration 120/1000 | Loss: 0.00002696
Iteration 121/1000 | Loss: 0.00002696
Iteration 122/1000 | Loss: 0.00002695
Iteration 123/1000 | Loss: 0.00002695
Iteration 124/1000 | Loss: 0.00002695
Iteration 125/1000 | Loss: 0.00002695
Iteration 126/1000 | Loss: 0.00002695
Iteration 127/1000 | Loss: 0.00002695
Iteration 128/1000 | Loss: 0.00002695
Iteration 129/1000 | Loss: 0.00002695
Iteration 130/1000 | Loss: 0.00002694
Iteration 131/1000 | Loss: 0.00002694
Iteration 132/1000 | Loss: 0.00002694
Iteration 133/1000 | Loss: 0.00002694
Iteration 134/1000 | Loss: 0.00002694
Iteration 135/1000 | Loss: 0.00002694
Iteration 136/1000 | Loss: 0.00002694
Iteration 137/1000 | Loss: 0.00002694
Iteration 138/1000 | Loss: 0.00002693
Iteration 139/1000 | Loss: 0.00002693
Iteration 140/1000 | Loss: 0.00002693
Iteration 141/1000 | Loss: 0.00002693
Iteration 142/1000 | Loss: 0.00002693
Iteration 143/1000 | Loss: 0.00002692
Iteration 144/1000 | Loss: 0.00002692
Iteration 145/1000 | Loss: 0.00002692
Iteration 146/1000 | Loss: 0.00002692
Iteration 147/1000 | Loss: 0.00002692
Iteration 148/1000 | Loss: 0.00002692
Iteration 149/1000 | Loss: 0.00002692
Iteration 150/1000 | Loss: 0.00002691
Iteration 151/1000 | Loss: 0.00002691
Iteration 152/1000 | Loss: 0.00002691
Iteration 153/1000 | Loss: 0.00002691
Iteration 154/1000 | Loss: 0.00002691
Iteration 155/1000 | Loss: 0.00002691
Iteration 156/1000 | Loss: 0.00002691
Iteration 157/1000 | Loss: 0.00002691
Iteration 158/1000 | Loss: 0.00002691
Iteration 159/1000 | Loss: 0.00002691
Iteration 160/1000 | Loss: 0.00002691
Iteration 161/1000 | Loss: 0.00002691
Iteration 162/1000 | Loss: 0.00002691
Iteration 163/1000 | Loss: 0.00002691
Iteration 164/1000 | Loss: 0.00002691
Iteration 165/1000 | Loss: 0.00002691
Iteration 166/1000 | Loss: 0.00002691
Iteration 167/1000 | Loss: 0.00002691
Iteration 168/1000 | Loss: 0.00002691
Iteration 169/1000 | Loss: 0.00002690
Iteration 170/1000 | Loss: 0.00002690
Iteration 171/1000 | Loss: 0.00002690
Iteration 172/1000 | Loss: 0.00002690
Iteration 173/1000 | Loss: 0.00002690
Iteration 174/1000 | Loss: 0.00002690
Iteration 175/1000 | Loss: 0.00002690
Iteration 176/1000 | Loss: 0.00002690
Iteration 177/1000 | Loss: 0.00002690
Iteration 178/1000 | Loss: 0.00002690
Iteration 179/1000 | Loss: 0.00002690
Iteration 180/1000 | Loss: 0.00002690
Iteration 181/1000 | Loss: 0.00002690
Iteration 182/1000 | Loss: 0.00002690
Iteration 183/1000 | Loss: 0.00002690
Iteration 184/1000 | Loss: 0.00002690
Iteration 185/1000 | Loss: 0.00002690
Iteration 186/1000 | Loss: 0.00002690
Iteration 187/1000 | Loss: 0.00002690
Iteration 188/1000 | Loss: 0.00002690
Iteration 189/1000 | Loss: 0.00002690
Iteration 190/1000 | Loss: 0.00002690
Iteration 191/1000 | Loss: 0.00002690
Iteration 192/1000 | Loss: 0.00002690
Iteration 193/1000 | Loss: 0.00002690
Iteration 194/1000 | Loss: 0.00002690
Iteration 195/1000 | Loss: 0.00002690
Iteration 196/1000 | Loss: 0.00002690
Iteration 197/1000 | Loss: 0.00002690
Iteration 198/1000 | Loss: 0.00002690
Iteration 199/1000 | Loss: 0.00002690
Iteration 200/1000 | Loss: 0.00002690
Iteration 201/1000 | Loss: 0.00002690
Iteration 202/1000 | Loss: 0.00002690
Iteration 203/1000 | Loss: 0.00002690
Iteration 204/1000 | Loss: 0.00002690
Iteration 205/1000 | Loss: 0.00002690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.6896808776655234e-05, 2.6896808776655234e-05, 2.6896808776655234e-05, 2.6896808776655234e-05, 2.6896808776655234e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6896808776655234e-05

Optimization complete. Final v2v error: 4.0735979080200195 mm

Highest mean error: 20.094749450683594 mm for frame 25

Lowest mean error: 3.4238083362579346 mm for frame 201

Saving results

Total time: 72.60840821266174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815842
Iteration 2/25 | Loss: 0.00098574
Iteration 3/25 | Loss: 0.00086945
Iteration 4/25 | Loss: 0.00084390
Iteration 5/25 | Loss: 0.00083320
Iteration 6/25 | Loss: 0.00083140
Iteration 7/25 | Loss: 0.00083078
Iteration 8/25 | Loss: 0.00083047
Iteration 9/25 | Loss: 0.00083047
Iteration 10/25 | Loss: 0.00083047
Iteration 11/25 | Loss: 0.00083047
Iteration 12/25 | Loss: 0.00083047
Iteration 13/25 | Loss: 0.00083047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.000830473261885345, 0.000830473261885345, 0.000830473261885345, 0.000830473261885345, 0.000830473261885345]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000830473261885345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27746797
Iteration 2/25 | Loss: 0.00037578
Iteration 3/25 | Loss: 0.00037577
Iteration 4/25 | Loss: 0.00037577
Iteration 5/25 | Loss: 0.00037577
Iteration 6/25 | Loss: 0.00037577
Iteration 7/25 | Loss: 0.00037577
Iteration 8/25 | Loss: 0.00037577
Iteration 9/25 | Loss: 0.00037577
Iteration 10/25 | Loss: 0.00037577
Iteration 11/25 | Loss: 0.00037577
Iteration 12/25 | Loss: 0.00037577
Iteration 13/25 | Loss: 0.00037577
Iteration 14/25 | Loss: 0.00037577
Iteration 15/25 | Loss: 0.00037577
Iteration 16/25 | Loss: 0.00037577
Iteration 17/25 | Loss: 0.00037577
Iteration 18/25 | Loss: 0.00037577
Iteration 19/25 | Loss: 0.00037577
Iteration 20/25 | Loss: 0.00037577
Iteration 21/25 | Loss: 0.00037577
Iteration 22/25 | Loss: 0.00037577
Iteration 23/25 | Loss: 0.00037577
Iteration 24/25 | Loss: 0.00037577
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.00037576997419819236, 0.00037576997419819236, 0.00037576997419819236, 0.00037576997419819236, 0.00037576997419819236]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00037576997419819236

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00037577
Iteration 2/1000 | Loss: 0.00004032
Iteration 3/1000 | Loss: 0.00002241
Iteration 4/1000 | Loss: 0.00001931
Iteration 5/1000 | Loss: 0.00001858
Iteration 6/1000 | Loss: 0.00001820
Iteration 7/1000 | Loss: 0.00001781
Iteration 8/1000 | Loss: 0.00001751
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001720
Iteration 11/1000 | Loss: 0.00001711
Iteration 12/1000 | Loss: 0.00001710
Iteration 13/1000 | Loss: 0.00001710
Iteration 14/1000 | Loss: 0.00001709
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001708
Iteration 17/1000 | Loss: 0.00001705
Iteration 18/1000 | Loss: 0.00001705
Iteration 19/1000 | Loss: 0.00001705
Iteration 20/1000 | Loss: 0.00001704
Iteration 21/1000 | Loss: 0.00001704
Iteration 22/1000 | Loss: 0.00001703
Iteration 23/1000 | Loss: 0.00001703
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001702
Iteration 26/1000 | Loss: 0.00001701
Iteration 27/1000 | Loss: 0.00001701
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001700
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001698
Iteration 32/1000 | Loss: 0.00001698
Iteration 33/1000 | Loss: 0.00001697
Iteration 34/1000 | Loss: 0.00001695
Iteration 35/1000 | Loss: 0.00001694
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001691
Iteration 40/1000 | Loss: 0.00001691
Iteration 41/1000 | Loss: 0.00001691
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001691
Iteration 45/1000 | Loss: 0.00001690
Iteration 46/1000 | Loss: 0.00001690
Iteration 47/1000 | Loss: 0.00001690
Iteration 48/1000 | Loss: 0.00001690
Iteration 49/1000 | Loss: 0.00001690
Iteration 50/1000 | Loss: 0.00001690
Iteration 51/1000 | Loss: 0.00001690
Iteration 52/1000 | Loss: 0.00001690
Iteration 53/1000 | Loss: 0.00001690
Iteration 54/1000 | Loss: 0.00001690
Iteration 55/1000 | Loss: 0.00001690
Iteration 56/1000 | Loss: 0.00001690
Iteration 57/1000 | Loss: 0.00001690
Iteration 58/1000 | Loss: 0.00001689
Iteration 59/1000 | Loss: 0.00001689
Iteration 60/1000 | Loss: 0.00001689
Iteration 61/1000 | Loss: 0.00001689
Iteration 62/1000 | Loss: 0.00001689
Iteration 63/1000 | Loss: 0.00001689
Iteration 64/1000 | Loss: 0.00001689
Iteration 65/1000 | Loss: 0.00001689
Iteration 66/1000 | Loss: 0.00001689
Iteration 67/1000 | Loss: 0.00001689
Iteration 68/1000 | Loss: 0.00001689
Iteration 69/1000 | Loss: 0.00001689
Iteration 70/1000 | Loss: 0.00001689
Iteration 71/1000 | Loss: 0.00001689
Iteration 72/1000 | Loss: 0.00001689
Iteration 73/1000 | Loss: 0.00001689
Iteration 74/1000 | Loss: 0.00001689
Iteration 75/1000 | Loss: 0.00001689
Iteration 76/1000 | Loss: 0.00001689
Iteration 77/1000 | Loss: 0.00001689
Iteration 78/1000 | Loss: 0.00001689
Iteration 79/1000 | Loss: 0.00001689
Iteration 80/1000 | Loss: 0.00001689
Iteration 81/1000 | Loss: 0.00001689
Iteration 82/1000 | Loss: 0.00001689
Iteration 83/1000 | Loss: 0.00001689
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.689139025984332e-05, 1.689139025984332e-05, 1.689139025984332e-05, 1.689139025984332e-05, 1.689139025984332e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.689139025984332e-05

Optimization complete. Final v2v error: 3.4810307025909424 mm

Highest mean error: 3.8831586837768555 mm for frame 91

Lowest mean error: 3.240144968032837 mm for frame 119

Saving results

Total time: 29.850964069366455
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066602
Iteration 2/25 | Loss: 0.01066602
Iteration 3/25 | Loss: 0.01066602
Iteration 4/25 | Loss: 0.01066602
Iteration 5/25 | Loss: 0.01066602
Iteration 6/25 | Loss: 0.01066602
Iteration 7/25 | Loss: 0.01066602
Iteration 8/25 | Loss: 0.01066601
Iteration 9/25 | Loss: 0.01066601
Iteration 10/25 | Loss: 0.01066601
Iteration 11/25 | Loss: 0.01066601
Iteration 12/25 | Loss: 0.01066601
Iteration 13/25 | Loss: 0.01066600
Iteration 14/25 | Loss: 0.01066600
Iteration 15/25 | Loss: 0.01066600
Iteration 16/25 | Loss: 0.01066600
Iteration 17/25 | Loss: 0.01066599
Iteration 18/25 | Loss: 0.01066599
Iteration 19/25 | Loss: 0.01066599
Iteration 20/25 | Loss: 0.01066598
Iteration 21/25 | Loss: 0.01066598
Iteration 22/25 | Loss: 0.01066598
Iteration 23/25 | Loss: 0.01066598
Iteration 24/25 | Loss: 0.01066598
Iteration 25/25 | Loss: 0.01066597

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50792718
Iteration 2/25 | Loss: 0.15235157
Iteration 3/25 | Loss: 0.15235050
Iteration 4/25 | Loss: 0.15235046
Iteration 5/25 | Loss: 0.15235044
Iteration 6/25 | Loss: 0.15235044
Iteration 7/25 | Loss: 0.15235043
Iteration 8/25 | Loss: 0.15235043
Iteration 9/25 | Loss: 0.15235043
Iteration 10/25 | Loss: 0.15235043
Iteration 11/25 | Loss: 0.15235043
Iteration 12/25 | Loss: 0.15235043
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.15235042572021484, 0.15235042572021484, 0.15235042572021484, 0.15235042572021484, 0.15235042572021484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.15235042572021484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15235043
Iteration 2/1000 | Loss: 0.00297203
Iteration 3/1000 | Loss: 0.00091651
Iteration 4/1000 | Loss: 0.00128973
Iteration 5/1000 | Loss: 0.00041524
Iteration 6/1000 | Loss: 0.00041191
Iteration 7/1000 | Loss: 0.00054739
Iteration 8/1000 | Loss: 0.00006035
Iteration 9/1000 | Loss: 0.00005216
Iteration 10/1000 | Loss: 0.00004395
Iteration 11/1000 | Loss: 0.00003953
Iteration 12/1000 | Loss: 0.00003536
Iteration 13/1000 | Loss: 0.00003309
Iteration 14/1000 | Loss: 0.00003127
Iteration 15/1000 | Loss: 0.00002912
Iteration 16/1000 | Loss: 0.00002723
Iteration 17/1000 | Loss: 0.00002599
Iteration 18/1000 | Loss: 0.00002520
Iteration 19/1000 | Loss: 0.00002462
Iteration 20/1000 | Loss: 0.00002428
Iteration 21/1000 | Loss: 0.00002410
Iteration 22/1000 | Loss: 0.00002409
Iteration 23/1000 | Loss: 0.00002402
Iteration 24/1000 | Loss: 0.00002398
Iteration 25/1000 | Loss: 0.00002395
Iteration 26/1000 | Loss: 0.00002395
Iteration 27/1000 | Loss: 0.00002395
Iteration 28/1000 | Loss: 0.00002395
Iteration 29/1000 | Loss: 0.00002395
Iteration 30/1000 | Loss: 0.00002395
Iteration 31/1000 | Loss: 0.00002395
Iteration 32/1000 | Loss: 0.00002395
Iteration 33/1000 | Loss: 0.00002394
Iteration 34/1000 | Loss: 0.00002394
Iteration 35/1000 | Loss: 0.00002394
Iteration 36/1000 | Loss: 0.00002394
Iteration 37/1000 | Loss: 0.00002394
Iteration 38/1000 | Loss: 0.00002394
Iteration 39/1000 | Loss: 0.00002394
Iteration 40/1000 | Loss: 0.00002394
Iteration 41/1000 | Loss: 0.00002393
Iteration 42/1000 | Loss: 0.00002389
Iteration 43/1000 | Loss: 0.00002387
Iteration 44/1000 | Loss: 0.00002386
Iteration 45/1000 | Loss: 0.00002386
Iteration 46/1000 | Loss: 0.00002386
Iteration 47/1000 | Loss: 0.00002386
Iteration 48/1000 | Loss: 0.00002385
Iteration 49/1000 | Loss: 0.00002385
Iteration 50/1000 | Loss: 0.00002385
Iteration 51/1000 | Loss: 0.00002385
Iteration 52/1000 | Loss: 0.00002385
Iteration 53/1000 | Loss: 0.00002385
Iteration 54/1000 | Loss: 0.00002384
Iteration 55/1000 | Loss: 0.00002384
Iteration 56/1000 | Loss: 0.00002384
Iteration 57/1000 | Loss: 0.00002384
Iteration 58/1000 | Loss: 0.00002384
Iteration 59/1000 | Loss: 0.00002384
Iteration 60/1000 | Loss: 0.00002384
Iteration 61/1000 | Loss: 0.00002384
Iteration 62/1000 | Loss: 0.00002384
Iteration 63/1000 | Loss: 0.00002384
Iteration 64/1000 | Loss: 0.00002384
Iteration 65/1000 | Loss: 0.00002384
Iteration 66/1000 | Loss: 0.00002384
Iteration 67/1000 | Loss: 0.00002384
Iteration 68/1000 | Loss: 0.00002384
Iteration 69/1000 | Loss: 0.00002384
Iteration 70/1000 | Loss: 0.00002384
Iteration 71/1000 | Loss: 0.00002384
Iteration 72/1000 | Loss: 0.00002384
Iteration 73/1000 | Loss: 0.00002384
Iteration 74/1000 | Loss: 0.00002384
Iteration 75/1000 | Loss: 0.00002384
Iteration 76/1000 | Loss: 0.00002384
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 76. Stopping optimization.
Last 5 losses: [2.3843318558647297e-05, 2.3843318558647297e-05, 2.3843318558647297e-05, 2.3843318558647297e-05, 2.3843318558647297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3843318558647297e-05

Optimization complete. Final v2v error: 4.102155685424805 mm

Highest mean error: 4.27921724319458 mm for frame 187

Lowest mean error: 3.727274179458618 mm for frame 1

Saving results

Total time: 46.334559202194214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907679
Iteration 2/25 | Loss: 0.00102723
Iteration 3/25 | Loss: 0.00085901
Iteration 4/25 | Loss: 0.00083411
Iteration 5/25 | Loss: 0.00082636
Iteration 6/25 | Loss: 0.00082417
Iteration 7/25 | Loss: 0.00082397
Iteration 8/25 | Loss: 0.00082397
Iteration 9/25 | Loss: 0.00082382
Iteration 10/25 | Loss: 0.00082382
Iteration 11/25 | Loss: 0.00082382
Iteration 12/25 | Loss: 0.00082382
Iteration 13/25 | Loss: 0.00082382
Iteration 14/25 | Loss: 0.00082382
Iteration 15/25 | Loss: 0.00082382
Iteration 16/25 | Loss: 0.00082382
Iteration 17/25 | Loss: 0.00082382
Iteration 18/25 | Loss: 0.00082382
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008238173322752118, 0.0008238173322752118, 0.0008238173322752118, 0.0008238173322752118, 0.0008238173322752118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008238173322752118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31825757
Iteration 2/25 | Loss: 0.00049171
Iteration 3/25 | Loss: 0.00049168
Iteration 4/25 | Loss: 0.00049168
Iteration 5/25 | Loss: 0.00049168
Iteration 6/25 | Loss: 0.00049168
Iteration 7/25 | Loss: 0.00049168
Iteration 8/25 | Loss: 0.00049168
Iteration 9/25 | Loss: 0.00049168
Iteration 10/25 | Loss: 0.00049168
Iteration 11/25 | Loss: 0.00049168
Iteration 12/25 | Loss: 0.00049168
Iteration 13/25 | Loss: 0.00049168
Iteration 14/25 | Loss: 0.00049168
Iteration 15/25 | Loss: 0.00049168
Iteration 16/25 | Loss: 0.00049168
Iteration 17/25 | Loss: 0.00049168
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0004916811594739556, 0.0004916811594739556, 0.0004916811594739556, 0.0004916811594739556, 0.0004916811594739556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004916811594739556

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049168
Iteration 2/1000 | Loss: 0.00004023
Iteration 3/1000 | Loss: 0.00002818
Iteration 4/1000 | Loss: 0.00002447
Iteration 5/1000 | Loss: 0.00002266
Iteration 6/1000 | Loss: 0.00002157
Iteration 7/1000 | Loss: 0.00002087
Iteration 8/1000 | Loss: 0.00002023
Iteration 9/1000 | Loss: 0.00001986
Iteration 10/1000 | Loss: 0.00001953
Iteration 11/1000 | Loss: 0.00001937
Iteration 12/1000 | Loss: 0.00001937
Iteration 13/1000 | Loss: 0.00001926
Iteration 14/1000 | Loss: 0.00001925
Iteration 15/1000 | Loss: 0.00001920
Iteration 16/1000 | Loss: 0.00001919
Iteration 17/1000 | Loss: 0.00001919
Iteration 18/1000 | Loss: 0.00001919
Iteration 19/1000 | Loss: 0.00001918
Iteration 20/1000 | Loss: 0.00001916
Iteration 21/1000 | Loss: 0.00001912
Iteration 22/1000 | Loss: 0.00001911
Iteration 23/1000 | Loss: 0.00001911
Iteration 24/1000 | Loss: 0.00001910
Iteration 25/1000 | Loss: 0.00001910
Iteration 26/1000 | Loss: 0.00001909
Iteration 27/1000 | Loss: 0.00001909
Iteration 28/1000 | Loss: 0.00001908
Iteration 29/1000 | Loss: 0.00001908
Iteration 30/1000 | Loss: 0.00001906
Iteration 31/1000 | Loss: 0.00001905
Iteration 32/1000 | Loss: 0.00001905
Iteration 33/1000 | Loss: 0.00001905
Iteration 34/1000 | Loss: 0.00001905
Iteration 35/1000 | Loss: 0.00001904
Iteration 36/1000 | Loss: 0.00001904
Iteration 37/1000 | Loss: 0.00001904
Iteration 38/1000 | Loss: 0.00001904
Iteration 39/1000 | Loss: 0.00001904
Iteration 40/1000 | Loss: 0.00001903
Iteration 41/1000 | Loss: 0.00001901
Iteration 42/1000 | Loss: 0.00001901
Iteration 43/1000 | Loss: 0.00001901
Iteration 44/1000 | Loss: 0.00001901
Iteration 45/1000 | Loss: 0.00001901
Iteration 46/1000 | Loss: 0.00001901
Iteration 47/1000 | Loss: 0.00001900
Iteration 48/1000 | Loss: 0.00001900
Iteration 49/1000 | Loss: 0.00001900
Iteration 50/1000 | Loss: 0.00001899
Iteration 51/1000 | Loss: 0.00001899
Iteration 52/1000 | Loss: 0.00001899
Iteration 53/1000 | Loss: 0.00001898
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00001898
Iteration 56/1000 | Loss: 0.00001897
Iteration 57/1000 | Loss: 0.00001897
Iteration 58/1000 | Loss: 0.00001897
Iteration 59/1000 | Loss: 0.00001897
Iteration 60/1000 | Loss: 0.00001897
Iteration 61/1000 | Loss: 0.00001897
Iteration 62/1000 | Loss: 0.00001896
Iteration 63/1000 | Loss: 0.00001896
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001895
Iteration 67/1000 | Loss: 0.00001895
Iteration 68/1000 | Loss: 0.00001895
Iteration 69/1000 | Loss: 0.00001895
Iteration 70/1000 | Loss: 0.00001895
Iteration 71/1000 | Loss: 0.00001894
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001893
Iteration 75/1000 | Loss: 0.00001893
Iteration 76/1000 | Loss: 0.00001893
Iteration 77/1000 | Loss: 0.00001893
Iteration 78/1000 | Loss: 0.00001892
Iteration 79/1000 | Loss: 0.00001892
Iteration 80/1000 | Loss: 0.00001892
Iteration 81/1000 | Loss: 0.00001892
Iteration 82/1000 | Loss: 0.00001892
Iteration 83/1000 | Loss: 0.00001892
Iteration 84/1000 | Loss: 0.00001892
Iteration 85/1000 | Loss: 0.00001892
Iteration 86/1000 | Loss: 0.00001892
Iteration 87/1000 | Loss: 0.00001891
Iteration 88/1000 | Loss: 0.00001891
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Iteration 101/1000 | Loss: 0.00001890
Iteration 102/1000 | Loss: 0.00001890
Iteration 103/1000 | Loss: 0.00001890
Iteration 104/1000 | Loss: 0.00001890
Iteration 105/1000 | Loss: 0.00001890
Iteration 106/1000 | Loss: 0.00001890
Iteration 107/1000 | Loss: 0.00001890
Iteration 108/1000 | Loss: 0.00001890
Iteration 109/1000 | Loss: 0.00001890
Iteration 110/1000 | Loss: 0.00001890
Iteration 111/1000 | Loss: 0.00001890
Iteration 112/1000 | Loss: 0.00001890
Iteration 113/1000 | Loss: 0.00001890
Iteration 114/1000 | Loss: 0.00001890
Iteration 115/1000 | Loss: 0.00001890
Iteration 116/1000 | Loss: 0.00001890
Iteration 117/1000 | Loss: 0.00001890
Iteration 118/1000 | Loss: 0.00001890
Iteration 119/1000 | Loss: 0.00001890
Iteration 120/1000 | Loss: 0.00001890
Iteration 121/1000 | Loss: 0.00001890
Iteration 122/1000 | Loss: 0.00001890
Iteration 123/1000 | Loss: 0.00001890
Iteration 124/1000 | Loss: 0.00001890
Iteration 125/1000 | Loss: 0.00001890
Iteration 126/1000 | Loss: 0.00001890
Iteration 127/1000 | Loss: 0.00001890
Iteration 128/1000 | Loss: 0.00001890
Iteration 129/1000 | Loss: 0.00001890
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.889565282908734e-05, 1.889565282908734e-05, 1.889565282908734e-05, 1.889565282908734e-05, 1.889565282908734e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.889565282908734e-05

Optimization complete. Final v2v error: 3.7433552742004395 mm

Highest mean error: 4.179113864898682 mm for frame 97

Lowest mean error: 3.1332433223724365 mm for frame 0

Saving results

Total time: 34.446900606155396
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_34_us_0405/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_34_us_0405/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00432627
Iteration 2/25 | Loss: 0.00112238
Iteration 3/25 | Loss: 0.00087483
Iteration 4/25 | Loss: 0.00083515
Iteration 5/25 | Loss: 0.00081981
Iteration 6/25 | Loss: 0.00081641
Iteration 7/25 | Loss: 0.00081546
Iteration 8/25 | Loss: 0.00081533
Iteration 9/25 | Loss: 0.00081533
Iteration 10/25 | Loss: 0.00081533
Iteration 11/25 | Loss: 0.00081533
Iteration 12/25 | Loss: 0.00081533
Iteration 13/25 | Loss: 0.00081533
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008153278613463044, 0.0008153278613463044, 0.0008153278613463044, 0.0008153278613463044, 0.0008153278613463044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008153278613463044

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36457908
Iteration 2/25 | Loss: 0.00039374
Iteration 3/25 | Loss: 0.00039374
Iteration 4/25 | Loss: 0.00039373
Iteration 5/25 | Loss: 0.00039373
Iteration 6/25 | Loss: 0.00039373
Iteration 7/25 | Loss: 0.00039373
Iteration 8/25 | Loss: 0.00039373
Iteration 9/25 | Loss: 0.00039373
Iteration 10/25 | Loss: 0.00039373
Iteration 11/25 | Loss: 0.00039373
Iteration 12/25 | Loss: 0.00039373
Iteration 13/25 | Loss: 0.00039373
Iteration 14/25 | Loss: 0.00039373
Iteration 15/25 | Loss: 0.00039373
Iteration 16/25 | Loss: 0.00039373
Iteration 17/25 | Loss: 0.00039373
Iteration 18/25 | Loss: 0.00039373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0003937323344871402, 0.0003937323344871402, 0.0003937323344871402, 0.0003937323344871402, 0.0003937323344871402]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0003937323344871402

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00039373
Iteration 2/1000 | Loss: 0.00003259
Iteration 3/1000 | Loss: 0.00002497
Iteration 4/1000 | Loss: 0.00002267
Iteration 5/1000 | Loss: 0.00002179
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00002073
Iteration 8/1000 | Loss: 0.00002042
Iteration 9/1000 | Loss: 0.00002018
Iteration 10/1000 | Loss: 0.00002012
Iteration 11/1000 | Loss: 0.00002008
Iteration 12/1000 | Loss: 0.00002005
Iteration 13/1000 | Loss: 0.00002005
Iteration 14/1000 | Loss: 0.00002002
Iteration 15/1000 | Loss: 0.00002001
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001998
Iteration 18/1000 | Loss: 0.00001997
Iteration 19/1000 | Loss: 0.00001995
Iteration 20/1000 | Loss: 0.00001995
Iteration 21/1000 | Loss: 0.00001994
Iteration 22/1000 | Loss: 0.00001994
Iteration 23/1000 | Loss: 0.00001994
Iteration 24/1000 | Loss: 0.00001992
Iteration 25/1000 | Loss: 0.00001992
Iteration 26/1000 | Loss: 0.00001991
Iteration 27/1000 | Loss: 0.00001991
Iteration 28/1000 | Loss: 0.00001991
Iteration 29/1000 | Loss: 0.00001991
Iteration 30/1000 | Loss: 0.00001991
Iteration 31/1000 | Loss: 0.00001991
Iteration 32/1000 | Loss: 0.00001991
Iteration 33/1000 | Loss: 0.00001991
Iteration 34/1000 | Loss: 0.00001991
Iteration 35/1000 | Loss: 0.00001991
Iteration 36/1000 | Loss: 0.00001991
Iteration 37/1000 | Loss: 0.00001990
Iteration 38/1000 | Loss: 0.00001990
Iteration 39/1000 | Loss: 0.00001990
Iteration 40/1000 | Loss: 0.00001990
Iteration 41/1000 | Loss: 0.00001988
Iteration 42/1000 | Loss: 0.00001987
Iteration 43/1000 | Loss: 0.00001987
Iteration 44/1000 | Loss: 0.00001987
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001987
Iteration 47/1000 | Loss: 0.00001987
Iteration 48/1000 | Loss: 0.00001987
Iteration 49/1000 | Loss: 0.00001987
Iteration 50/1000 | Loss: 0.00001987
Iteration 51/1000 | Loss: 0.00001986
Iteration 52/1000 | Loss: 0.00001986
Iteration 53/1000 | Loss: 0.00001986
Iteration 54/1000 | Loss: 0.00001986
Iteration 55/1000 | Loss: 0.00001986
Iteration 56/1000 | Loss: 0.00001985
Iteration 57/1000 | Loss: 0.00001985
Iteration 58/1000 | Loss: 0.00001985
Iteration 59/1000 | Loss: 0.00001985
Iteration 60/1000 | Loss: 0.00001984
Iteration 61/1000 | Loss: 0.00001984
Iteration 62/1000 | Loss: 0.00001984
Iteration 63/1000 | Loss: 0.00001984
Iteration 64/1000 | Loss: 0.00001984
Iteration 65/1000 | Loss: 0.00001984
Iteration 66/1000 | Loss: 0.00001984
Iteration 67/1000 | Loss: 0.00001983
Iteration 68/1000 | Loss: 0.00001983
Iteration 69/1000 | Loss: 0.00001983
Iteration 70/1000 | Loss: 0.00001983
Iteration 71/1000 | Loss: 0.00001983
Iteration 72/1000 | Loss: 0.00001983
Iteration 73/1000 | Loss: 0.00001983
Iteration 74/1000 | Loss: 0.00001983
Iteration 75/1000 | Loss: 0.00001983
Iteration 76/1000 | Loss: 0.00001983
Iteration 77/1000 | Loss: 0.00001983
Iteration 78/1000 | Loss: 0.00001983
Iteration 79/1000 | Loss: 0.00001982
Iteration 80/1000 | Loss: 0.00001982
Iteration 81/1000 | Loss: 0.00001982
Iteration 82/1000 | Loss: 0.00001982
Iteration 83/1000 | Loss: 0.00001982
Iteration 84/1000 | Loss: 0.00001981
Iteration 85/1000 | Loss: 0.00001981
Iteration 86/1000 | Loss: 0.00001981
Iteration 87/1000 | Loss: 0.00001981
Iteration 88/1000 | Loss: 0.00001981
Iteration 89/1000 | Loss: 0.00001981
Iteration 90/1000 | Loss: 0.00001981
Iteration 91/1000 | Loss: 0.00001981
Iteration 92/1000 | Loss: 0.00001980
Iteration 93/1000 | Loss: 0.00001980
Iteration 94/1000 | Loss: 0.00001980
Iteration 95/1000 | Loss: 0.00001980
Iteration 96/1000 | Loss: 0.00001980
Iteration 97/1000 | Loss: 0.00001979
Iteration 98/1000 | Loss: 0.00001979
Iteration 99/1000 | Loss: 0.00001979
Iteration 100/1000 | Loss: 0.00001979
Iteration 101/1000 | Loss: 0.00001979
Iteration 102/1000 | Loss: 0.00001979
Iteration 103/1000 | Loss: 0.00001978
Iteration 104/1000 | Loss: 0.00001978
Iteration 105/1000 | Loss: 0.00001978
Iteration 106/1000 | Loss: 0.00001978
Iteration 107/1000 | Loss: 0.00001978
Iteration 108/1000 | Loss: 0.00001978
Iteration 109/1000 | Loss: 0.00001978
Iteration 110/1000 | Loss: 0.00001978
Iteration 111/1000 | Loss: 0.00001978
Iteration 112/1000 | Loss: 0.00001978
Iteration 113/1000 | Loss: 0.00001978
Iteration 114/1000 | Loss: 0.00001978
Iteration 115/1000 | Loss: 0.00001978
Iteration 116/1000 | Loss: 0.00001978
Iteration 117/1000 | Loss: 0.00001978
Iteration 118/1000 | Loss: 0.00001978
Iteration 119/1000 | Loss: 0.00001977
Iteration 120/1000 | Loss: 0.00001977
Iteration 121/1000 | Loss: 0.00001977
Iteration 122/1000 | Loss: 0.00001977
Iteration 123/1000 | Loss: 0.00001977
Iteration 124/1000 | Loss: 0.00001977
Iteration 125/1000 | Loss: 0.00001977
Iteration 126/1000 | Loss: 0.00001977
Iteration 127/1000 | Loss: 0.00001977
Iteration 128/1000 | Loss: 0.00001977
Iteration 129/1000 | Loss: 0.00001977
Iteration 130/1000 | Loss: 0.00001977
Iteration 131/1000 | Loss: 0.00001977
Iteration 132/1000 | Loss: 0.00001977
Iteration 133/1000 | Loss: 0.00001977
Iteration 134/1000 | Loss: 0.00001976
Iteration 135/1000 | Loss: 0.00001976
Iteration 136/1000 | Loss: 0.00001976
Iteration 137/1000 | Loss: 0.00001976
Iteration 138/1000 | Loss: 0.00001976
Iteration 139/1000 | Loss: 0.00001976
Iteration 140/1000 | Loss: 0.00001976
Iteration 141/1000 | Loss: 0.00001976
Iteration 142/1000 | Loss: 0.00001976
Iteration 143/1000 | Loss: 0.00001976
Iteration 144/1000 | Loss: 0.00001976
Iteration 145/1000 | Loss: 0.00001976
Iteration 146/1000 | Loss: 0.00001976
Iteration 147/1000 | Loss: 0.00001976
Iteration 148/1000 | Loss: 0.00001976
Iteration 149/1000 | Loss: 0.00001976
Iteration 150/1000 | Loss: 0.00001976
Iteration 151/1000 | Loss: 0.00001976
Iteration 152/1000 | Loss: 0.00001976
Iteration 153/1000 | Loss: 0.00001976
Iteration 154/1000 | Loss: 0.00001976
Iteration 155/1000 | Loss: 0.00001976
Iteration 156/1000 | Loss: 0.00001976
Iteration 157/1000 | Loss: 0.00001976
Iteration 158/1000 | Loss: 0.00001976
Iteration 159/1000 | Loss: 0.00001976
Iteration 160/1000 | Loss: 0.00001976
Iteration 161/1000 | Loss: 0.00001976
Iteration 162/1000 | Loss: 0.00001976
Iteration 163/1000 | Loss: 0.00001976
Iteration 164/1000 | Loss: 0.00001976
Iteration 165/1000 | Loss: 0.00001976
Iteration 166/1000 | Loss: 0.00001976
Iteration 167/1000 | Loss: 0.00001976
Iteration 168/1000 | Loss: 0.00001976
Iteration 169/1000 | Loss: 0.00001976
Iteration 170/1000 | Loss: 0.00001976
Iteration 171/1000 | Loss: 0.00001976
Iteration 172/1000 | Loss: 0.00001976
Iteration 173/1000 | Loss: 0.00001976
Iteration 174/1000 | Loss: 0.00001976
Iteration 175/1000 | Loss: 0.00001976
Iteration 176/1000 | Loss: 0.00001976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.976183193619363e-05, 1.976183193619363e-05, 1.976183193619363e-05, 1.976183193619363e-05, 1.976183193619363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.976183193619363e-05

Optimization complete. Final v2v error: 3.7035892009735107 mm

Highest mean error: 4.433818817138672 mm for frame 80

Lowest mean error: 3.04705548286438 mm for frame 164

Saving results

Total time: 40.155317068099976
