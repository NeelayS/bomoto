Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=58, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 3248-3303
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01154022
Iteration 2/25 | Loss: 0.01154022
Iteration 3/25 | Loss: 0.01154022
Iteration 4/25 | Loss: 0.01154022
Iteration 5/25 | Loss: 0.01154022
Iteration 6/25 | Loss: 0.01154022
Iteration 7/25 | Loss: 0.01154021
Iteration 8/25 | Loss: 0.01154021
Iteration 9/25 | Loss: 0.01154021
Iteration 10/25 | Loss: 0.01154021
Iteration 11/25 | Loss: 0.01154021
Iteration 12/25 | Loss: 0.01154021
Iteration 13/25 | Loss: 0.01154021
Iteration 14/25 | Loss: 0.01154021
Iteration 15/25 | Loss: 0.01154021
Iteration 16/25 | Loss: 0.01154021
Iteration 17/25 | Loss: 0.01154021
Iteration 18/25 | Loss: 0.01154021
Iteration 19/25 | Loss: 0.01154021
Iteration 20/25 | Loss: 0.01154021
Iteration 21/25 | Loss: 0.01154021
Iteration 22/25 | Loss: 0.01154021
Iteration 23/25 | Loss: 0.01154021
Iteration 24/25 | Loss: 0.01154021
Iteration 25/25 | Loss: 0.01154021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01940727
Iteration 2/25 | Loss: 0.04711343
Iteration 3/25 | Loss: 0.04637568
Iteration 4/25 | Loss: 0.04581358
Iteration 5/25 | Loss: 0.04581358
Iteration 6/25 | Loss: 0.04581358
Iteration 7/25 | Loss: 0.04581357
Iteration 8/25 | Loss: 0.04581356
Iteration 9/25 | Loss: 0.04581356
Iteration 10/25 | Loss: 0.04581356
Iteration 11/25 | Loss: 0.04581356
Iteration 12/25 | Loss: 0.04581356
Iteration 13/25 | Loss: 0.04581356
Iteration 14/25 | Loss: 0.04581356
Iteration 15/25 | Loss: 0.04581356
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.045813560485839844, 0.045813560485839844, 0.045813560485839844, 0.045813560485839844, 0.045813560485839844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.045813560485839844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.04581356
Iteration 2/1000 | Loss: 0.00357044
Iteration 3/1000 | Loss: 0.00128980
Iteration 4/1000 | Loss: 0.00081636
Iteration 5/1000 | Loss: 0.00140706
Iteration 6/1000 | Loss: 0.00032349
Iteration 7/1000 | Loss: 0.00035049
Iteration 8/1000 | Loss: 0.00039937
Iteration 9/1000 | Loss: 0.00017358
Iteration 10/1000 | Loss: 0.00024114
Iteration 11/1000 | Loss: 0.00055788
Iteration 12/1000 | Loss: 0.00009205
Iteration 13/1000 | Loss: 0.00039062
Iteration 14/1000 | Loss: 0.00010408
Iteration 15/1000 | Loss: 0.00008078
Iteration 16/1000 | Loss: 0.00005330
Iteration 17/1000 | Loss: 0.00012727
Iteration 18/1000 | Loss: 0.00033839
Iteration 19/1000 | Loss: 0.00006093
Iteration 20/1000 | Loss: 0.00026830
Iteration 21/1000 | Loss: 0.00004517
Iteration 22/1000 | Loss: 0.00011973
Iteration 23/1000 | Loss: 0.00020419
Iteration 24/1000 | Loss: 0.00098383
Iteration 25/1000 | Loss: 0.00004509
Iteration 26/1000 | Loss: 0.00008226
Iteration 27/1000 | Loss: 0.00003997
Iteration 28/1000 | Loss: 0.00014103
Iteration 29/1000 | Loss: 0.00005545
Iteration 30/1000 | Loss: 0.00004578
Iteration 31/1000 | Loss: 0.00010049
Iteration 32/1000 | Loss: 0.00003979
Iteration 33/1000 | Loss: 0.00012074
Iteration 34/1000 | Loss: 0.00003530
Iteration 35/1000 | Loss: 0.00006154
Iteration 36/1000 | Loss: 0.00004978
Iteration 37/1000 | Loss: 0.00003381
Iteration 38/1000 | Loss: 0.00019201
Iteration 39/1000 | Loss: 0.00006442
Iteration 40/1000 | Loss: 0.00008160
Iteration 41/1000 | Loss: 0.00005976
Iteration 42/1000 | Loss: 0.00006195
Iteration 43/1000 | Loss: 0.00004960
Iteration 44/1000 | Loss: 0.00006982
Iteration 45/1000 | Loss: 0.00007365
Iteration 46/1000 | Loss: 0.00007223
Iteration 47/1000 | Loss: 0.00005925
Iteration 48/1000 | Loss: 0.00005354
Iteration 49/1000 | Loss: 0.00003642
Iteration 50/1000 | Loss: 0.00003554
Iteration 51/1000 | Loss: 0.00004598
Iteration 52/1000 | Loss: 0.00004284
Iteration 53/1000 | Loss: 0.00003348
Iteration 54/1000 | Loss: 0.00006215
Iteration 55/1000 | Loss: 0.00005678
Iteration 56/1000 | Loss: 0.00003076
Iteration 57/1000 | Loss: 0.00003906
Iteration 58/1000 | Loss: 0.00004167
Iteration 59/1000 | Loss: 0.00006195
Iteration 60/1000 | Loss: 0.00005074
Iteration 61/1000 | Loss: 0.00004313
Iteration 62/1000 | Loss: 0.00009031
Iteration 63/1000 | Loss: 0.00003168
Iteration 64/1000 | Loss: 0.00006013
Iteration 65/1000 | Loss: 0.00004455
Iteration 66/1000 | Loss: 0.00011375
Iteration 67/1000 | Loss: 0.00004508
Iteration 68/1000 | Loss: 0.00003154
Iteration 69/1000 | Loss: 0.00004135
Iteration 70/1000 | Loss: 0.00003038
Iteration 71/1000 | Loss: 0.00003066
Iteration 72/1000 | Loss: 0.00003037
Iteration 73/1000 | Loss: 0.00003034
Iteration 74/1000 | Loss: 0.00003034
Iteration 75/1000 | Loss: 0.00003033
Iteration 76/1000 | Loss: 0.00004912
Iteration 77/1000 | Loss: 0.00003033
Iteration 78/1000 | Loss: 0.00003032
Iteration 79/1000 | Loss: 0.00003032
Iteration 80/1000 | Loss: 0.00003031
Iteration 81/1000 | Loss: 0.00003031
Iteration 82/1000 | Loss: 0.00003031
Iteration 83/1000 | Loss: 0.00003031
Iteration 84/1000 | Loss: 0.00003030
Iteration 85/1000 | Loss: 0.00003030
Iteration 86/1000 | Loss: 0.00003029
Iteration 87/1000 | Loss: 0.00003029
Iteration 88/1000 | Loss: 0.00003029
Iteration 89/1000 | Loss: 0.00003029
Iteration 90/1000 | Loss: 0.00003029
Iteration 91/1000 | Loss: 0.00003028
Iteration 92/1000 | Loss: 0.00003028
Iteration 93/1000 | Loss: 0.00003028
Iteration 94/1000 | Loss: 0.00003027
Iteration 95/1000 | Loss: 0.00003027
Iteration 96/1000 | Loss: 0.00003027
Iteration 97/1000 | Loss: 0.00003026
Iteration 98/1000 | Loss: 0.00003026
Iteration 99/1000 | Loss: 0.00003026
Iteration 100/1000 | Loss: 0.00003025
Iteration 101/1000 | Loss: 0.00003024
Iteration 102/1000 | Loss: 0.00007700
Iteration 103/1000 | Loss: 0.00003407
Iteration 104/1000 | Loss: 0.00003072
Iteration 105/1000 | Loss: 0.00002954
Iteration 106/1000 | Loss: 0.00002954
Iteration 107/1000 | Loss: 0.00002953
Iteration 108/1000 | Loss: 0.00002953
Iteration 109/1000 | Loss: 0.00002953
Iteration 110/1000 | Loss: 0.00002953
Iteration 111/1000 | Loss: 0.00002952
Iteration 112/1000 | Loss: 0.00002951
Iteration 113/1000 | Loss: 0.00002950
Iteration 114/1000 | Loss: 0.00002950
Iteration 115/1000 | Loss: 0.00002950
Iteration 116/1000 | Loss: 0.00002950
Iteration 117/1000 | Loss: 0.00002949
Iteration 118/1000 | Loss: 0.00002949
Iteration 119/1000 | Loss: 0.00002949
Iteration 120/1000 | Loss: 0.00002949
Iteration 121/1000 | Loss: 0.00002948
Iteration 122/1000 | Loss: 0.00002948
Iteration 123/1000 | Loss: 0.00002948
Iteration 124/1000 | Loss: 0.00002947
Iteration 125/1000 | Loss: 0.00002947
Iteration 126/1000 | Loss: 0.00005532
Iteration 127/1000 | Loss: 0.00002951
Iteration 128/1000 | Loss: 0.00002979
Iteration 129/1000 | Loss: 0.00002946
Iteration 130/1000 | Loss: 0.00002943
Iteration 131/1000 | Loss: 0.00002943
Iteration 132/1000 | Loss: 0.00002943
Iteration 133/1000 | Loss: 0.00002943
Iteration 134/1000 | Loss: 0.00002943
Iteration 135/1000 | Loss: 0.00002943
Iteration 136/1000 | Loss: 0.00002943
Iteration 137/1000 | Loss: 0.00002943
Iteration 138/1000 | Loss: 0.00002943
Iteration 139/1000 | Loss: 0.00002943
Iteration 140/1000 | Loss: 0.00002943
Iteration 141/1000 | Loss: 0.00002942
Iteration 142/1000 | Loss: 0.00002942
Iteration 143/1000 | Loss: 0.00002942
Iteration 144/1000 | Loss: 0.00002942
Iteration 145/1000 | Loss: 0.00002942
Iteration 146/1000 | Loss: 0.00002942
Iteration 147/1000 | Loss: 0.00002973
Iteration 148/1000 | Loss: 0.00002973
Iteration 149/1000 | Loss: 0.00002973
Iteration 150/1000 | Loss: 0.00002973
Iteration 151/1000 | Loss: 0.00002942
Iteration 152/1000 | Loss: 0.00002941
Iteration 153/1000 | Loss: 0.00002941
Iteration 154/1000 | Loss: 0.00002941
Iteration 155/1000 | Loss: 0.00002941
Iteration 156/1000 | Loss: 0.00002941
Iteration 157/1000 | Loss: 0.00002941
Iteration 158/1000 | Loss: 0.00002941
Iteration 159/1000 | Loss: 0.00002941
Iteration 160/1000 | Loss: 0.00002941
Iteration 161/1000 | Loss: 0.00002941
Iteration 162/1000 | Loss: 0.00002941
Iteration 163/1000 | Loss: 0.00002941
Iteration 164/1000 | Loss: 0.00002941
Iteration 165/1000 | Loss: 0.00002941
Iteration 166/1000 | Loss: 0.00002941
Iteration 167/1000 | Loss: 0.00002941
Iteration 168/1000 | Loss: 0.00002941
Iteration 169/1000 | Loss: 0.00002941
Iteration 170/1000 | Loss: 0.00002941
Iteration 171/1000 | Loss: 0.00002941
Iteration 172/1000 | Loss: 0.00002941
Iteration 173/1000 | Loss: 0.00002941
Iteration 174/1000 | Loss: 0.00002941
Iteration 175/1000 | Loss: 0.00002941
Iteration 176/1000 | Loss: 0.00002941
Iteration 177/1000 | Loss: 0.00002941
Iteration 178/1000 | Loss: 0.00002941
Iteration 179/1000 | Loss: 0.00002941
Iteration 180/1000 | Loss: 0.00002941
Iteration 181/1000 | Loss: 0.00002941
Iteration 182/1000 | Loss: 0.00002941
Iteration 183/1000 | Loss: 0.00002941
Iteration 184/1000 | Loss: 0.00002941
Iteration 185/1000 | Loss: 0.00002941
Iteration 186/1000 | Loss: 0.00002941
Iteration 187/1000 | Loss: 0.00002941
Iteration 188/1000 | Loss: 0.00002941
Iteration 189/1000 | Loss: 0.00002941
Iteration 190/1000 | Loss: 0.00002941
Iteration 191/1000 | Loss: 0.00002941
Iteration 192/1000 | Loss: 0.00002941
Iteration 193/1000 | Loss: 0.00002941
Iteration 194/1000 | Loss: 0.00002941
Iteration 195/1000 | Loss: 0.00002941
Iteration 196/1000 | Loss: 0.00002941
Iteration 197/1000 | Loss: 0.00002941
Iteration 198/1000 | Loss: 0.00002941
Iteration 199/1000 | Loss: 0.00002941
Iteration 200/1000 | Loss: 0.00002941
Iteration 201/1000 | Loss: 0.00002941
Iteration 202/1000 | Loss: 0.00002941
Iteration 203/1000 | Loss: 0.00002941
Iteration 204/1000 | Loss: 0.00002941
Iteration 205/1000 | Loss: 0.00002941
Iteration 206/1000 | Loss: 0.00002941
Iteration 207/1000 | Loss: 0.00002941
Iteration 208/1000 | Loss: 0.00002941
Iteration 209/1000 | Loss: 0.00002941
Iteration 210/1000 | Loss: 0.00002941
Iteration 211/1000 | Loss: 0.00002941
Iteration 212/1000 | Loss: 0.00002941
Iteration 213/1000 | Loss: 0.00002941
Iteration 214/1000 | Loss: 0.00002941
Iteration 215/1000 | Loss: 0.00002941
Iteration 216/1000 | Loss: 0.00002941
Iteration 217/1000 | Loss: 0.00002941
Iteration 218/1000 | Loss: 0.00002941
Iteration 219/1000 | Loss: 0.00002941
Iteration 220/1000 | Loss: 0.00002941
Iteration 221/1000 | Loss: 0.00002941
Iteration 222/1000 | Loss: 0.00002941
Iteration 223/1000 | Loss: 0.00002941
Iteration 224/1000 | Loss: 0.00002941
Iteration 225/1000 | Loss: 0.00002941
Iteration 226/1000 | Loss: 0.00002941
Iteration 227/1000 | Loss: 0.00002941
Iteration 228/1000 | Loss: 0.00002941
Iteration 229/1000 | Loss: 0.00002941
Iteration 230/1000 | Loss: 0.00002941
Iteration 231/1000 | Loss: 0.00002941
Iteration 232/1000 | Loss: 0.00002941
Iteration 233/1000 | Loss: 0.00002941
Iteration 234/1000 | Loss: 0.00002941
Iteration 235/1000 | Loss: 0.00002941
Iteration 236/1000 | Loss: 0.00002941
Iteration 237/1000 | Loss: 0.00002941
Iteration 238/1000 | Loss: 0.00002941
Iteration 239/1000 | Loss: 0.00002941
Iteration 240/1000 | Loss: 0.00002941
Iteration 241/1000 | Loss: 0.00002941
Iteration 242/1000 | Loss: 0.00002941
Iteration 243/1000 | Loss: 0.00002941
Iteration 244/1000 | Loss: 0.00002941
Iteration 245/1000 | Loss: 0.00002941
Iteration 246/1000 | Loss: 0.00002941
Iteration 247/1000 | Loss: 0.00002941
Iteration 248/1000 | Loss: 0.00002941
Iteration 249/1000 | Loss: 0.00002941
Iteration 250/1000 | Loss: 0.00002941
Iteration 251/1000 | Loss: 0.00002941
Iteration 252/1000 | Loss: 0.00002941
Iteration 253/1000 | Loss: 0.00002941
Iteration 254/1000 | Loss: 0.00002941
Iteration 255/1000 | Loss: 0.00002941
Iteration 256/1000 | Loss: 0.00002941
Iteration 257/1000 | Loss: 0.00002941
Iteration 258/1000 | Loss: 0.00002941
Iteration 259/1000 | Loss: 0.00002941
Iteration 260/1000 | Loss: 0.00002941
Iteration 261/1000 | Loss: 0.00002941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 261. Stopping optimization.
Last 5 losses: [2.940935519291088e-05, 2.940935519291088e-05, 2.940935519291088e-05, 2.940935519291088e-05, 2.940935519291088e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.940935519291088e-05

Optimization complete. Final v2v error: 4.538616180419922 mm

Highest mean error: 10.997053146362305 mm for frame 169

Lowest mean error: 4.017068386077881 mm for frame 166

Saving results

Total time: 139.17662906646729
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_31_us_1593/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_31_us_1593/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103962
Iteration 2/25 | Loss: 0.00216156
Iteration 3/25 | Loss: 0.00137115
Iteration 4/25 | Loss: 0.00123130
Iteration 5/25 | Loss: 0.00115513
Iteration 6/25 | Loss: 0.00114417
Iteration 7/25 | Loss: 0.00114231
Iteration 8/25 | Loss: 0.00109217
Iteration 9/25 | Loss: 0.00106852
Iteration 10/25 | Loss: 0.00106048
Iteration 11/25 | Loss: 0.00105801
Iteration 12/25 | Loss: 0.00105997
Iteration 13/25 | Loss: 0.00105692
Iteration 14/25 | Loss: 0.00105531
Iteration 15/25 | Loss: 0.00105836
Iteration 16/25 | Loss: 0.00105644
Iteration 17/25 | Loss: 0.00105255
Iteration 18/25 | Loss: 0.00105095
Iteration 19/25 | Loss: 0.00105025
Iteration 20/25 | Loss: 0.00105010
Iteration 21/25 | Loss: 0.00104973
Iteration 22/25 | Loss: 0.00105014
Iteration 23/25 | Loss: 0.00104977
Iteration 24/25 | Loss: 0.00104961
Iteration 25/25 | Loss: 0.00104960

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50219202
Iteration 2/25 | Loss: 0.00217508
Iteration 3/25 | Loss: 0.00211674
Iteration 4/25 | Loss: 0.00211674
Iteration 5/25 | Loss: 0.00211674
Iteration 6/25 | Loss: 0.00211674
Iteration 7/25 | Loss: 0.00211674
Iteration 8/25 | Loss: 0.00211674
Iteration 9/25 | Loss: 0.00211674
Iteration 10/25 | Loss: 0.00211674
Iteration 11/25 | Loss: 0.00211674
Iteration 12/25 | Loss: 0.00211674
Iteration 13/25 | Loss: 0.00211674
Iteration 14/25 | Loss: 0.00211674
Iteration 15/25 | Loss: 0.00211674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0021167437080293894, 0.0021167437080293894, 0.0021167437080293894, 0.0021167437080293894, 0.0021167437080293894]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021167437080293894

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00211674
Iteration 2/1000 | Loss: 0.00225840
Iteration 3/1000 | Loss: 0.00057690
Iteration 4/1000 | Loss: 0.00141684
Iteration 5/1000 | Loss: 0.00026819
Iteration 6/1000 | Loss: 0.00025649
Iteration 7/1000 | Loss: 0.00009999
Iteration 8/1000 | Loss: 0.00007310
Iteration 9/1000 | Loss: 0.00008760
Iteration 10/1000 | Loss: 0.00004120
Iteration 11/1000 | Loss: 0.00004045
Iteration 12/1000 | Loss: 0.00021486
Iteration 13/1000 | Loss: 0.00037379
Iteration 14/1000 | Loss: 0.00009350
Iteration 15/1000 | Loss: 0.00007579
Iteration 16/1000 | Loss: 0.00003204
Iteration 17/1000 | Loss: 0.00006276
Iteration 18/1000 | Loss: 0.00004198
Iteration 19/1000 | Loss: 0.00002931
Iteration 20/1000 | Loss: 0.00002151
Iteration 21/1000 | Loss: 0.00002111
Iteration 22/1000 | Loss: 0.00002080
Iteration 23/1000 | Loss: 0.00002055
Iteration 24/1000 | Loss: 0.00003898
Iteration 25/1000 | Loss: 0.00002104
Iteration 26/1000 | Loss: 0.00002018
Iteration 27/1000 | Loss: 0.00002017
Iteration 28/1000 | Loss: 0.00002017
Iteration 29/1000 | Loss: 0.00002012
Iteration 30/1000 | Loss: 0.00002012
Iteration 31/1000 | Loss: 0.00002011
Iteration 32/1000 | Loss: 0.00002010
Iteration 33/1000 | Loss: 0.00002009
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002027
Iteration 36/1000 | Loss: 0.00002079
Iteration 37/1000 | Loss: 0.00001992
Iteration 38/1000 | Loss: 0.00001992
Iteration 39/1000 | Loss: 0.00001992
Iteration 40/1000 | Loss: 0.00001992
Iteration 41/1000 | Loss: 0.00001991
Iteration 42/1000 | Loss: 0.00002481
Iteration 43/1000 | Loss: 0.00002070
Iteration 44/1000 | Loss: 0.00001987
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001986
Iteration 47/1000 | Loss: 0.00001986
Iteration 48/1000 | Loss: 0.00001986
Iteration 49/1000 | Loss: 0.00001986
Iteration 50/1000 | Loss: 0.00001986
Iteration 51/1000 | Loss: 0.00001986
Iteration 52/1000 | Loss: 0.00001985
Iteration 53/1000 | Loss: 0.00001985
Iteration 54/1000 | Loss: 0.00001985
Iteration 55/1000 | Loss: 0.00001984
Iteration 56/1000 | Loss: 0.00001984
Iteration 57/1000 | Loss: 0.00001984
Iteration 58/1000 | Loss: 0.00001984
Iteration 59/1000 | Loss: 0.00001984
Iteration 60/1000 | Loss: 0.00001983
Iteration 61/1000 | Loss: 0.00001983
Iteration 62/1000 | Loss: 0.00001983
Iteration 63/1000 | Loss: 0.00001983
Iteration 64/1000 | Loss: 0.00001982
Iteration 65/1000 | Loss: 0.00001981
Iteration 66/1000 | Loss: 0.00001981
Iteration 67/1000 | Loss: 0.00002089
Iteration 68/1000 | Loss: 0.00001997
Iteration 69/1000 | Loss: 0.00001981
Iteration 70/1000 | Loss: 0.00001981
Iteration 71/1000 | Loss: 0.00001981
Iteration 72/1000 | Loss: 0.00001980
Iteration 73/1000 | Loss: 0.00001980
Iteration 74/1000 | Loss: 0.00001980
Iteration 75/1000 | Loss: 0.00001980
Iteration 76/1000 | Loss: 0.00001979
Iteration 77/1000 | Loss: 0.00001979
Iteration 78/1000 | Loss: 0.00001979
Iteration 79/1000 | Loss: 0.00001979
Iteration 80/1000 | Loss: 0.00001979
Iteration 81/1000 | Loss: 0.00001979
Iteration 82/1000 | Loss: 0.00001979
Iteration 83/1000 | Loss: 0.00001979
Iteration 84/1000 | Loss: 0.00001978
Iteration 85/1000 | Loss: 0.00001978
Iteration 86/1000 | Loss: 0.00001978
Iteration 87/1000 | Loss: 0.00001978
Iteration 88/1000 | Loss: 0.00001978
Iteration 89/1000 | Loss: 0.00001978
Iteration 90/1000 | Loss: 0.00001978
Iteration 91/1000 | Loss: 0.00001978
Iteration 92/1000 | Loss: 0.00001978
Iteration 93/1000 | Loss: 0.00001977
Iteration 94/1000 | Loss: 0.00001977
Iteration 95/1000 | Loss: 0.00001977
Iteration 96/1000 | Loss: 0.00001977
Iteration 97/1000 | Loss: 0.00001977
Iteration 98/1000 | Loss: 0.00001977
Iteration 99/1000 | Loss: 0.00001977
Iteration 100/1000 | Loss: 0.00001976
Iteration 101/1000 | Loss: 0.00001976
Iteration 102/1000 | Loss: 0.00001976
Iteration 103/1000 | Loss: 0.00001976
Iteration 104/1000 | Loss: 0.00002027
Iteration 105/1000 | Loss: 0.00002016
Iteration 106/1000 | Loss: 0.00001977
Iteration 107/1000 | Loss: 0.00001976
Iteration 108/1000 | Loss: 0.00001976
Iteration 109/1000 | Loss: 0.00001976
Iteration 110/1000 | Loss: 0.00001975
Iteration 111/1000 | Loss: 0.00001975
Iteration 112/1000 | Loss: 0.00001975
Iteration 113/1000 | Loss: 0.00001974
Iteration 114/1000 | Loss: 0.00001974
Iteration 115/1000 | Loss: 0.00001974
Iteration 116/1000 | Loss: 0.00001974
Iteration 117/1000 | Loss: 0.00001974
Iteration 118/1000 | Loss: 0.00001974
Iteration 119/1000 | Loss: 0.00001974
Iteration 120/1000 | Loss: 0.00001974
Iteration 121/1000 | Loss: 0.00001974
Iteration 122/1000 | Loss: 0.00001973
Iteration 123/1000 | Loss: 0.00001973
Iteration 124/1000 | Loss: 0.00001973
Iteration 125/1000 | Loss: 0.00001973
Iteration 126/1000 | Loss: 0.00001973
Iteration 127/1000 | Loss: 0.00001973
Iteration 128/1000 | Loss: 0.00001973
Iteration 129/1000 | Loss: 0.00001973
Iteration 130/1000 | Loss: 0.00001973
Iteration 131/1000 | Loss: 0.00001973
Iteration 132/1000 | Loss: 0.00001973
Iteration 133/1000 | Loss: 0.00001973
Iteration 134/1000 | Loss: 0.00001973
Iteration 135/1000 | Loss: 0.00001973
Iteration 136/1000 | Loss: 0.00001973
Iteration 137/1000 | Loss: 0.00001973
Iteration 138/1000 | Loss: 0.00001973
Iteration 139/1000 | Loss: 0.00001973
Iteration 140/1000 | Loss: 0.00001973
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.973126745724585e-05, 1.973126745724585e-05, 1.973126745724585e-05, 1.973126745724585e-05, 1.973126745724585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.973126745724585e-05

Optimization complete. Final v2v error: 3.760876417160034 mm

Highest mean error: 4.222528457641602 mm for frame 45

Lowest mean error: 3.400411605834961 mm for frame 210

Saving results

Total time: 103.10722732543945
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422928
Iteration 2/25 | Loss: 0.00124909
Iteration 3/25 | Loss: 0.00111041
Iteration 4/25 | Loss: 0.00109928
Iteration 5/25 | Loss: 0.00109621
Iteration 6/25 | Loss: 0.00109556
Iteration 7/25 | Loss: 0.00109556
Iteration 8/25 | Loss: 0.00109556
Iteration 9/25 | Loss: 0.00109556
Iteration 10/25 | Loss: 0.00109556
Iteration 11/25 | Loss: 0.00109556
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001095558749511838, 0.001095558749511838, 0.001095558749511838, 0.001095558749511838, 0.001095558749511838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001095558749511838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81209171
Iteration 2/25 | Loss: 0.00515675
Iteration 3/25 | Loss: 0.00515675
Iteration 4/25 | Loss: 0.00515674
Iteration 5/25 | Loss: 0.00515674
Iteration 6/25 | Loss: 0.00515674
Iteration 7/25 | Loss: 0.00515674
Iteration 8/25 | Loss: 0.00515674
Iteration 9/25 | Loss: 0.00515674
Iteration 10/25 | Loss: 0.00515674
Iteration 11/25 | Loss: 0.00515674
Iteration 12/25 | Loss: 0.00515674
Iteration 13/25 | Loss: 0.00515674
Iteration 14/25 | Loss: 0.00515674
Iteration 15/25 | Loss: 0.00515674
Iteration 16/25 | Loss: 0.00515674
Iteration 17/25 | Loss: 0.00515674
Iteration 18/25 | Loss: 0.00515674
Iteration 19/25 | Loss: 0.00515674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.005156742874532938, 0.005156742874532938, 0.005156742874532938, 0.005156742874532938, 0.005156742874532938]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005156742874532938

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00515674
Iteration 2/1000 | Loss: 0.00003929
Iteration 3/1000 | Loss: 0.00002747
Iteration 4/1000 | Loss: 0.00002501
Iteration 5/1000 | Loss: 0.00002339
Iteration 6/1000 | Loss: 0.00002238
Iteration 7/1000 | Loss: 0.00002169
Iteration 8/1000 | Loss: 0.00002132
Iteration 9/1000 | Loss: 0.00002112
Iteration 10/1000 | Loss: 0.00002111
Iteration 11/1000 | Loss: 0.00002102
Iteration 12/1000 | Loss: 0.00002097
Iteration 13/1000 | Loss: 0.00002096
Iteration 14/1000 | Loss: 0.00002095
Iteration 15/1000 | Loss: 0.00002091
Iteration 16/1000 | Loss: 0.00002090
Iteration 17/1000 | Loss: 0.00002090
Iteration 18/1000 | Loss: 0.00002088
Iteration 19/1000 | Loss: 0.00002087
Iteration 20/1000 | Loss: 0.00002085
Iteration 21/1000 | Loss: 0.00002081
Iteration 22/1000 | Loss: 0.00002079
Iteration 23/1000 | Loss: 0.00002074
Iteration 24/1000 | Loss: 0.00002073
Iteration 25/1000 | Loss: 0.00002069
Iteration 26/1000 | Loss: 0.00002068
Iteration 27/1000 | Loss: 0.00002065
Iteration 28/1000 | Loss: 0.00002065
Iteration 29/1000 | Loss: 0.00002064
Iteration 30/1000 | Loss: 0.00002064
Iteration 31/1000 | Loss: 0.00002064
Iteration 32/1000 | Loss: 0.00002064
Iteration 33/1000 | Loss: 0.00002063
Iteration 34/1000 | Loss: 0.00002063
Iteration 35/1000 | Loss: 0.00002063
Iteration 36/1000 | Loss: 0.00002063
Iteration 37/1000 | Loss: 0.00002062
Iteration 38/1000 | Loss: 0.00002062
Iteration 39/1000 | Loss: 0.00002062
Iteration 40/1000 | Loss: 0.00002062
Iteration 41/1000 | Loss: 0.00002061
Iteration 42/1000 | Loss: 0.00002061
Iteration 43/1000 | Loss: 0.00002061
Iteration 44/1000 | Loss: 0.00002061
Iteration 45/1000 | Loss: 0.00002061
Iteration 46/1000 | Loss: 0.00002061
Iteration 47/1000 | Loss: 0.00002061
Iteration 48/1000 | Loss: 0.00002061
Iteration 49/1000 | Loss: 0.00002061
Iteration 50/1000 | Loss: 0.00002060
Iteration 51/1000 | Loss: 0.00002060
Iteration 52/1000 | Loss: 0.00002060
Iteration 53/1000 | Loss: 0.00002060
Iteration 54/1000 | Loss: 0.00002060
Iteration 55/1000 | Loss: 0.00002060
Iteration 56/1000 | Loss: 0.00002059
Iteration 57/1000 | Loss: 0.00002059
Iteration 58/1000 | Loss: 0.00002059
Iteration 59/1000 | Loss: 0.00002059
Iteration 60/1000 | Loss: 0.00002059
Iteration 61/1000 | Loss: 0.00002058
Iteration 62/1000 | Loss: 0.00002058
Iteration 63/1000 | Loss: 0.00002058
Iteration 64/1000 | Loss: 0.00002058
Iteration 65/1000 | Loss: 0.00002057
Iteration 66/1000 | Loss: 0.00002057
Iteration 67/1000 | Loss: 0.00002057
Iteration 68/1000 | Loss: 0.00002057
Iteration 69/1000 | Loss: 0.00002057
Iteration 70/1000 | Loss: 0.00002057
Iteration 71/1000 | Loss: 0.00002057
Iteration 72/1000 | Loss: 0.00002057
Iteration 73/1000 | Loss: 0.00002056
Iteration 74/1000 | Loss: 0.00002056
Iteration 75/1000 | Loss: 0.00002056
Iteration 76/1000 | Loss: 0.00002056
Iteration 77/1000 | Loss: 0.00002056
Iteration 78/1000 | Loss: 0.00002056
Iteration 79/1000 | Loss: 0.00002056
Iteration 80/1000 | Loss: 0.00002055
Iteration 81/1000 | Loss: 0.00002055
Iteration 82/1000 | Loss: 0.00002055
Iteration 83/1000 | Loss: 0.00002055
Iteration 84/1000 | Loss: 0.00002055
Iteration 85/1000 | Loss: 0.00002055
Iteration 86/1000 | Loss: 0.00002055
Iteration 87/1000 | Loss: 0.00002055
Iteration 88/1000 | Loss: 0.00002055
Iteration 89/1000 | Loss: 0.00002055
Iteration 90/1000 | Loss: 0.00002055
Iteration 91/1000 | Loss: 0.00002055
Iteration 92/1000 | Loss: 0.00002055
Iteration 93/1000 | Loss: 0.00002055
Iteration 94/1000 | Loss: 0.00002055
Iteration 95/1000 | Loss: 0.00002054
Iteration 96/1000 | Loss: 0.00002054
Iteration 97/1000 | Loss: 0.00002054
Iteration 98/1000 | Loss: 0.00002054
Iteration 99/1000 | Loss: 0.00002054
Iteration 100/1000 | Loss: 0.00002054
Iteration 101/1000 | Loss: 0.00002054
Iteration 102/1000 | Loss: 0.00002054
Iteration 103/1000 | Loss: 0.00002054
Iteration 104/1000 | Loss: 0.00002054
Iteration 105/1000 | Loss: 0.00002054
Iteration 106/1000 | Loss: 0.00002054
Iteration 107/1000 | Loss: 0.00002053
Iteration 108/1000 | Loss: 0.00002053
Iteration 109/1000 | Loss: 0.00002053
Iteration 110/1000 | Loss: 0.00002053
Iteration 111/1000 | Loss: 0.00002053
Iteration 112/1000 | Loss: 0.00002053
Iteration 113/1000 | Loss: 0.00002053
Iteration 114/1000 | Loss: 0.00002053
Iteration 115/1000 | Loss: 0.00002053
Iteration 116/1000 | Loss: 0.00002053
Iteration 117/1000 | Loss: 0.00002053
Iteration 118/1000 | Loss: 0.00002053
Iteration 119/1000 | Loss: 0.00002053
Iteration 120/1000 | Loss: 0.00002053
Iteration 121/1000 | Loss: 0.00002053
Iteration 122/1000 | Loss: 0.00002053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [2.052957825071644e-05, 2.052957825071644e-05, 2.052957825071644e-05, 2.052957825071644e-05, 2.052957825071644e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.052957825071644e-05

Optimization complete. Final v2v error: 3.932643175125122 mm

Highest mean error: 4.532711505889893 mm for frame 73

Lowest mean error: 3.305734157562256 mm for frame 0

Saving results

Total time: 32.20336556434631
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00901227
Iteration 2/25 | Loss: 0.00142745
Iteration 3/25 | Loss: 0.00129746
Iteration 4/25 | Loss: 0.00125354
Iteration 5/25 | Loss: 0.00123728
Iteration 6/25 | Loss: 0.00123284
Iteration 7/25 | Loss: 0.00123099
Iteration 8/25 | Loss: 0.00123089
Iteration 9/25 | Loss: 0.00123089
Iteration 10/25 | Loss: 0.00123089
Iteration 11/25 | Loss: 0.00123089
Iteration 12/25 | Loss: 0.00123089
Iteration 13/25 | Loss: 0.00123089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012308937730267644, 0.0012308937730267644, 0.0012308937730267644, 0.0012308937730267644, 0.0012308937730267644]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012308937730267644

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.97166240
Iteration 2/25 | Loss: 0.00823117
Iteration 3/25 | Loss: 0.00823116
Iteration 4/25 | Loss: 0.00823116
Iteration 5/25 | Loss: 0.00823116
Iteration 6/25 | Loss: 0.00823116
Iteration 7/25 | Loss: 0.00823116
Iteration 8/25 | Loss: 0.00823116
Iteration 9/25 | Loss: 0.00823116
Iteration 10/25 | Loss: 0.00823116
Iteration 11/25 | Loss: 0.00823116
Iteration 12/25 | Loss: 0.00823116
Iteration 13/25 | Loss: 0.00823116
Iteration 14/25 | Loss: 0.00823116
Iteration 15/25 | Loss: 0.00823116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.008231161162257195, 0.008231161162257195, 0.008231161162257195, 0.008231161162257195, 0.008231161162257195]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.008231161162257195

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00823116
Iteration 2/1000 | Loss: 0.00007329
Iteration 3/1000 | Loss: 0.00004095
Iteration 4/1000 | Loss: 0.00003460
Iteration 5/1000 | Loss: 0.00002929
Iteration 6/1000 | Loss: 0.00002735
Iteration 7/1000 | Loss: 0.00002623
Iteration 8/1000 | Loss: 0.00002562
Iteration 9/1000 | Loss: 0.00002483
Iteration 10/1000 | Loss: 0.00002419
Iteration 11/1000 | Loss: 0.00002382
Iteration 12/1000 | Loss: 0.00002352
Iteration 13/1000 | Loss: 0.00002328
Iteration 14/1000 | Loss: 0.00002323
Iteration 15/1000 | Loss: 0.00002316
Iteration 16/1000 | Loss: 0.00002315
Iteration 17/1000 | Loss: 0.00002314
Iteration 18/1000 | Loss: 0.00002314
Iteration 19/1000 | Loss: 0.00002313
Iteration 20/1000 | Loss: 0.00002309
Iteration 21/1000 | Loss: 0.00002309
Iteration 22/1000 | Loss: 0.00002308
Iteration 23/1000 | Loss: 0.00002306
Iteration 24/1000 | Loss: 0.00002306
Iteration 25/1000 | Loss: 0.00002306
Iteration 26/1000 | Loss: 0.00002305
Iteration 27/1000 | Loss: 0.00002305
Iteration 28/1000 | Loss: 0.00002305
Iteration 29/1000 | Loss: 0.00002305
Iteration 30/1000 | Loss: 0.00002304
Iteration 31/1000 | Loss: 0.00002301
Iteration 32/1000 | Loss: 0.00002301
Iteration 33/1000 | Loss: 0.00002300
Iteration 34/1000 | Loss: 0.00002300
Iteration 35/1000 | Loss: 0.00002300
Iteration 36/1000 | Loss: 0.00002299
Iteration 37/1000 | Loss: 0.00002299
Iteration 38/1000 | Loss: 0.00002295
Iteration 39/1000 | Loss: 0.00002294
Iteration 40/1000 | Loss: 0.00002294
Iteration 41/1000 | Loss: 0.00002294
Iteration 42/1000 | Loss: 0.00002294
Iteration 43/1000 | Loss: 0.00002294
Iteration 44/1000 | Loss: 0.00002294
Iteration 45/1000 | Loss: 0.00002294
Iteration 46/1000 | Loss: 0.00002294
Iteration 47/1000 | Loss: 0.00002294
Iteration 48/1000 | Loss: 0.00002294
Iteration 49/1000 | Loss: 0.00002294
Iteration 50/1000 | Loss: 0.00002293
Iteration 51/1000 | Loss: 0.00002293
Iteration 52/1000 | Loss: 0.00002293
Iteration 53/1000 | Loss: 0.00002293
Iteration 54/1000 | Loss: 0.00002293
Iteration 55/1000 | Loss: 0.00002292
Iteration 56/1000 | Loss: 0.00002292
Iteration 57/1000 | Loss: 0.00002292
Iteration 58/1000 | Loss: 0.00002291
Iteration 59/1000 | Loss: 0.00002291
Iteration 60/1000 | Loss: 0.00002291
Iteration 61/1000 | Loss: 0.00002291
Iteration 62/1000 | Loss: 0.00002290
Iteration 63/1000 | Loss: 0.00002290
Iteration 64/1000 | Loss: 0.00002290
Iteration 65/1000 | Loss: 0.00002289
Iteration 66/1000 | Loss: 0.00002289
Iteration 67/1000 | Loss: 0.00002289
Iteration 68/1000 | Loss: 0.00002289
Iteration 69/1000 | Loss: 0.00002289
Iteration 70/1000 | Loss: 0.00002289
Iteration 71/1000 | Loss: 0.00002289
Iteration 72/1000 | Loss: 0.00002288
Iteration 73/1000 | Loss: 0.00002288
Iteration 74/1000 | Loss: 0.00002288
Iteration 75/1000 | Loss: 0.00002288
Iteration 76/1000 | Loss: 0.00002288
Iteration 77/1000 | Loss: 0.00002288
Iteration 78/1000 | Loss: 0.00002288
Iteration 79/1000 | Loss: 0.00002287
Iteration 80/1000 | Loss: 0.00002287
Iteration 81/1000 | Loss: 0.00002287
Iteration 82/1000 | Loss: 0.00002287
Iteration 83/1000 | Loss: 0.00002287
Iteration 84/1000 | Loss: 0.00002286
Iteration 85/1000 | Loss: 0.00002286
Iteration 86/1000 | Loss: 0.00002286
Iteration 87/1000 | Loss: 0.00002286
Iteration 88/1000 | Loss: 0.00002286
Iteration 89/1000 | Loss: 0.00002286
Iteration 90/1000 | Loss: 0.00002286
Iteration 91/1000 | Loss: 0.00002286
Iteration 92/1000 | Loss: 0.00002286
Iteration 93/1000 | Loss: 0.00002286
Iteration 94/1000 | Loss: 0.00002286
Iteration 95/1000 | Loss: 0.00002286
Iteration 96/1000 | Loss: 0.00002285
Iteration 97/1000 | Loss: 0.00002285
Iteration 98/1000 | Loss: 0.00002285
Iteration 99/1000 | Loss: 0.00002285
Iteration 100/1000 | Loss: 0.00002285
Iteration 101/1000 | Loss: 0.00002284
Iteration 102/1000 | Loss: 0.00002284
Iteration 103/1000 | Loss: 0.00002284
Iteration 104/1000 | Loss: 0.00002283
Iteration 105/1000 | Loss: 0.00002283
Iteration 106/1000 | Loss: 0.00002283
Iteration 107/1000 | Loss: 0.00002283
Iteration 108/1000 | Loss: 0.00002282
Iteration 109/1000 | Loss: 0.00002282
Iteration 110/1000 | Loss: 0.00002282
Iteration 111/1000 | Loss: 0.00002281
Iteration 112/1000 | Loss: 0.00002281
Iteration 113/1000 | Loss: 0.00002281
Iteration 114/1000 | Loss: 0.00002280
Iteration 115/1000 | Loss: 0.00002280
Iteration 116/1000 | Loss: 0.00002280
Iteration 117/1000 | Loss: 0.00002279
Iteration 118/1000 | Loss: 0.00002279
Iteration 119/1000 | Loss: 0.00002279
Iteration 120/1000 | Loss: 0.00002279
Iteration 121/1000 | Loss: 0.00002278
Iteration 122/1000 | Loss: 0.00002278
Iteration 123/1000 | Loss: 0.00002278
Iteration 124/1000 | Loss: 0.00002278
Iteration 125/1000 | Loss: 0.00002278
Iteration 126/1000 | Loss: 0.00002278
Iteration 127/1000 | Loss: 0.00002277
Iteration 128/1000 | Loss: 0.00002277
Iteration 129/1000 | Loss: 0.00002277
Iteration 130/1000 | Loss: 0.00002277
Iteration 131/1000 | Loss: 0.00002277
Iteration 132/1000 | Loss: 0.00002277
Iteration 133/1000 | Loss: 0.00002277
Iteration 134/1000 | Loss: 0.00002277
Iteration 135/1000 | Loss: 0.00002277
Iteration 136/1000 | Loss: 0.00002277
Iteration 137/1000 | Loss: 0.00002277
Iteration 138/1000 | Loss: 0.00002277
Iteration 139/1000 | Loss: 0.00002277
Iteration 140/1000 | Loss: 0.00002277
Iteration 141/1000 | Loss: 0.00002277
Iteration 142/1000 | Loss: 0.00002277
Iteration 143/1000 | Loss: 0.00002277
Iteration 144/1000 | Loss: 0.00002277
Iteration 145/1000 | Loss: 0.00002277
Iteration 146/1000 | Loss: 0.00002277
Iteration 147/1000 | Loss: 0.00002277
Iteration 148/1000 | Loss: 0.00002277
Iteration 149/1000 | Loss: 0.00002277
Iteration 150/1000 | Loss: 0.00002277
Iteration 151/1000 | Loss: 0.00002277
Iteration 152/1000 | Loss: 0.00002277
Iteration 153/1000 | Loss: 0.00002277
Iteration 154/1000 | Loss: 0.00002277
Iteration 155/1000 | Loss: 0.00002277
Iteration 156/1000 | Loss: 0.00002277
Iteration 157/1000 | Loss: 0.00002277
Iteration 158/1000 | Loss: 0.00002277
Iteration 159/1000 | Loss: 0.00002277
Iteration 160/1000 | Loss: 0.00002277
Iteration 161/1000 | Loss: 0.00002277
Iteration 162/1000 | Loss: 0.00002277
Iteration 163/1000 | Loss: 0.00002277
Iteration 164/1000 | Loss: 0.00002277
Iteration 165/1000 | Loss: 0.00002277
Iteration 166/1000 | Loss: 0.00002277
Iteration 167/1000 | Loss: 0.00002277
Iteration 168/1000 | Loss: 0.00002277
Iteration 169/1000 | Loss: 0.00002277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [2.2770864234189503e-05, 2.2770864234189503e-05, 2.2770864234189503e-05, 2.2770864234189503e-05, 2.2770864234189503e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2770864234189503e-05

Optimization complete. Final v2v error: 4.023550987243652 mm

Highest mean error: 5.429576396942139 mm for frame 127

Lowest mean error: 3.4177021980285645 mm for frame 45

Saving results

Total time: 39.981931924819946
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013299
Iteration 2/25 | Loss: 0.00203925
Iteration 3/25 | Loss: 0.00157169
Iteration 4/25 | Loss: 0.00146229
Iteration 5/25 | Loss: 0.00142427
Iteration 6/25 | Loss: 0.00139667
Iteration 7/25 | Loss: 0.00135363
Iteration 8/25 | Loss: 0.00132384
Iteration 9/25 | Loss: 0.00130885
Iteration 10/25 | Loss: 0.00130890
Iteration 11/25 | Loss: 0.00128391
Iteration 12/25 | Loss: 0.00127663
Iteration 13/25 | Loss: 0.00127372
Iteration 14/25 | Loss: 0.00127261
Iteration 15/25 | Loss: 0.00127921
Iteration 16/25 | Loss: 0.00126867
Iteration 17/25 | Loss: 0.00126583
Iteration 18/25 | Loss: 0.00126484
Iteration 19/25 | Loss: 0.00126461
Iteration 20/25 | Loss: 0.00126875
Iteration 21/25 | Loss: 0.00126875
Iteration 22/25 | Loss: 0.00126875
Iteration 23/25 | Loss: 0.00126872
Iteration 24/25 | Loss: 0.00126463
Iteration 25/25 | Loss: 0.00126866

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.43541503
Iteration 2/25 | Loss: 0.00420687
Iteration 3/25 | Loss: 0.00420687
Iteration 4/25 | Loss: 0.00420687
Iteration 5/25 | Loss: 0.00420687
Iteration 6/25 | Loss: 0.00420687
Iteration 7/25 | Loss: 0.00420687
Iteration 8/25 | Loss: 0.00420687
Iteration 9/25 | Loss: 0.00420687
Iteration 10/25 | Loss: 0.00420687
Iteration 11/25 | Loss: 0.00420687
Iteration 12/25 | Loss: 0.00420687
Iteration 13/25 | Loss: 0.00420687
Iteration 14/25 | Loss: 0.00420687
Iteration 15/25 | Loss: 0.00420687
Iteration 16/25 | Loss: 0.00420687
Iteration 17/25 | Loss: 0.00420687
Iteration 18/25 | Loss: 0.00420687
Iteration 19/25 | Loss: 0.00420687
Iteration 20/25 | Loss: 0.00420687
Iteration 21/25 | Loss: 0.00420687
Iteration 22/25 | Loss: 0.00420687
Iteration 23/25 | Loss: 0.00420687
Iteration 24/25 | Loss: 0.00420687
Iteration 25/25 | Loss: 0.00420687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00420687
Iteration 2/1000 | Loss: 0.00044237
Iteration 3/1000 | Loss: 0.00029076
Iteration 4/1000 | Loss: 0.00017903
Iteration 5/1000 | Loss: 0.00011875
Iteration 6/1000 | Loss: 0.00009664
Iteration 7/1000 | Loss: 0.00008699
Iteration 8/1000 | Loss: 0.00022999
Iteration 9/1000 | Loss: 0.00011732
Iteration 10/1000 | Loss: 0.00010783
Iteration 11/1000 | Loss: 0.00009656
Iteration 12/1000 | Loss: 0.00008969
Iteration 13/1000 | Loss: 0.00012081
Iteration 14/1000 | Loss: 0.00013262
Iteration 15/1000 | Loss: 0.00008091
Iteration 16/1000 | Loss: 0.00007482
Iteration 17/1000 | Loss: 0.00006769
Iteration 18/1000 | Loss: 0.00006282
Iteration 19/1000 | Loss: 0.00005891
Iteration 20/1000 | Loss: 0.00005536
Iteration 21/1000 | Loss: 0.00005280
Iteration 22/1000 | Loss: 0.00005801
Iteration 23/1000 | Loss: 0.00005410
Iteration 24/1000 | Loss: 0.00005230
Iteration 25/1000 | Loss: 0.00005100
Iteration 26/1000 | Loss: 0.00004982
Iteration 27/1000 | Loss: 0.00004857
Iteration 28/1000 | Loss: 0.00004769
Iteration 29/1000 | Loss: 0.00005547
Iteration 30/1000 | Loss: 0.00004963
Iteration 31/1000 | Loss: 0.00004795
Iteration 32/1000 | Loss: 0.00005168
Iteration 33/1000 | Loss: 0.00005229
Iteration 34/1000 | Loss: 0.00005427
Iteration 35/1000 | Loss: 0.00004961
Iteration 36/1000 | Loss: 0.00005373
Iteration 37/1000 | Loss: 0.00005721
Iteration 38/1000 | Loss: 0.00005210
Iteration 39/1000 | Loss: 0.00004588
Iteration 40/1000 | Loss: 0.00004459
Iteration 41/1000 | Loss: 0.00004341
Iteration 42/1000 | Loss: 0.00004295
Iteration 43/1000 | Loss: 0.00004246
Iteration 44/1000 | Loss: 0.00004229
Iteration 45/1000 | Loss: 0.00004227
Iteration 46/1000 | Loss: 0.00004227
Iteration 47/1000 | Loss: 0.00004227
Iteration 48/1000 | Loss: 0.00004226
Iteration 49/1000 | Loss: 0.00004226
Iteration 50/1000 | Loss: 0.00004222
Iteration 51/1000 | Loss: 0.00004218
Iteration 52/1000 | Loss: 0.00004217
Iteration 53/1000 | Loss: 0.00004212
Iteration 54/1000 | Loss: 0.00004206
Iteration 55/1000 | Loss: 0.00004203
Iteration 56/1000 | Loss: 0.00004202
Iteration 57/1000 | Loss: 0.00004202
Iteration 58/1000 | Loss: 0.00004201
Iteration 59/1000 | Loss: 0.00004200
Iteration 60/1000 | Loss: 0.00004198
Iteration 61/1000 | Loss: 0.00004197
Iteration 62/1000 | Loss: 0.00004194
Iteration 63/1000 | Loss: 0.00004194
Iteration 64/1000 | Loss: 0.00004194
Iteration 65/1000 | Loss: 0.00004193
Iteration 66/1000 | Loss: 0.00004193
Iteration 67/1000 | Loss: 0.00004193
Iteration 68/1000 | Loss: 0.00004193
Iteration 69/1000 | Loss: 0.00004193
Iteration 70/1000 | Loss: 0.00004193
Iteration 71/1000 | Loss: 0.00004192
Iteration 72/1000 | Loss: 0.00004192
Iteration 73/1000 | Loss: 0.00004192
Iteration 74/1000 | Loss: 0.00004191
Iteration 75/1000 | Loss: 0.00004191
Iteration 76/1000 | Loss: 0.00004191
Iteration 77/1000 | Loss: 0.00004190
Iteration 78/1000 | Loss: 0.00004190
Iteration 79/1000 | Loss: 0.00004190
Iteration 80/1000 | Loss: 0.00004190
Iteration 81/1000 | Loss: 0.00004190
Iteration 82/1000 | Loss: 0.00004190
Iteration 83/1000 | Loss: 0.00004190
Iteration 84/1000 | Loss: 0.00004190
Iteration 85/1000 | Loss: 0.00004190
Iteration 86/1000 | Loss: 0.00004189
Iteration 87/1000 | Loss: 0.00004189
Iteration 88/1000 | Loss: 0.00004189
Iteration 89/1000 | Loss: 0.00004189
Iteration 90/1000 | Loss: 0.00004188
Iteration 91/1000 | Loss: 0.00004188
Iteration 92/1000 | Loss: 0.00004188
Iteration 93/1000 | Loss: 0.00004188
Iteration 94/1000 | Loss: 0.00004188
Iteration 95/1000 | Loss: 0.00004188
Iteration 96/1000 | Loss: 0.00004188
Iteration 97/1000 | Loss: 0.00004188
Iteration 98/1000 | Loss: 0.00004188
Iteration 99/1000 | Loss: 0.00004187
Iteration 100/1000 | Loss: 0.00004187
Iteration 101/1000 | Loss: 0.00004187
Iteration 102/1000 | Loss: 0.00004187
Iteration 103/1000 | Loss: 0.00004187
Iteration 104/1000 | Loss: 0.00004187
Iteration 105/1000 | Loss: 0.00004187
Iteration 106/1000 | Loss: 0.00004187
Iteration 107/1000 | Loss: 0.00004186
Iteration 108/1000 | Loss: 0.00004186
Iteration 109/1000 | Loss: 0.00004186
Iteration 110/1000 | Loss: 0.00004186
Iteration 111/1000 | Loss: 0.00004186
Iteration 112/1000 | Loss: 0.00004186
Iteration 113/1000 | Loss: 0.00004186
Iteration 114/1000 | Loss: 0.00004186
Iteration 115/1000 | Loss: 0.00004185
Iteration 116/1000 | Loss: 0.00004185
Iteration 117/1000 | Loss: 0.00004185
Iteration 118/1000 | Loss: 0.00004185
Iteration 119/1000 | Loss: 0.00004185
Iteration 120/1000 | Loss: 0.00004185
Iteration 121/1000 | Loss: 0.00004185
Iteration 122/1000 | Loss: 0.00004185
Iteration 123/1000 | Loss: 0.00004185
Iteration 124/1000 | Loss: 0.00004185
Iteration 125/1000 | Loss: 0.00004185
Iteration 126/1000 | Loss: 0.00004185
Iteration 127/1000 | Loss: 0.00004185
Iteration 128/1000 | Loss: 0.00004185
Iteration 129/1000 | Loss: 0.00004184
Iteration 130/1000 | Loss: 0.00004184
Iteration 131/1000 | Loss: 0.00004184
Iteration 132/1000 | Loss: 0.00004184
Iteration 133/1000 | Loss: 0.00004184
Iteration 134/1000 | Loss: 0.00004184
Iteration 135/1000 | Loss: 0.00004184
Iteration 136/1000 | Loss: 0.00004184
Iteration 137/1000 | Loss: 0.00004184
Iteration 138/1000 | Loss: 0.00004184
Iteration 139/1000 | Loss: 0.00004184
Iteration 140/1000 | Loss: 0.00004184
Iteration 141/1000 | Loss: 0.00004184
Iteration 142/1000 | Loss: 0.00004184
Iteration 143/1000 | Loss: 0.00004184
Iteration 144/1000 | Loss: 0.00004184
Iteration 145/1000 | Loss: 0.00004183
Iteration 146/1000 | Loss: 0.00004183
Iteration 147/1000 | Loss: 0.00004183
Iteration 148/1000 | Loss: 0.00004183
Iteration 149/1000 | Loss: 0.00004183
Iteration 150/1000 | Loss: 0.00004183
Iteration 151/1000 | Loss: 0.00004183
Iteration 152/1000 | Loss: 0.00004183
Iteration 153/1000 | Loss: 0.00004183
Iteration 154/1000 | Loss: 0.00004183
Iteration 155/1000 | Loss: 0.00004183
Iteration 156/1000 | Loss: 0.00004183
Iteration 157/1000 | Loss: 0.00004183
Iteration 158/1000 | Loss: 0.00004183
Iteration 159/1000 | Loss: 0.00004183
Iteration 160/1000 | Loss: 0.00004183
Iteration 161/1000 | Loss: 0.00004183
Iteration 162/1000 | Loss: 0.00004183
Iteration 163/1000 | Loss: 0.00004182
Iteration 164/1000 | Loss: 0.00004182
Iteration 165/1000 | Loss: 0.00004182
Iteration 166/1000 | Loss: 0.00004182
Iteration 167/1000 | Loss: 0.00004182
Iteration 168/1000 | Loss: 0.00004182
Iteration 169/1000 | Loss: 0.00004181
Iteration 170/1000 | Loss: 0.00004181
Iteration 171/1000 | Loss: 0.00004181
Iteration 172/1000 | Loss: 0.00004181
Iteration 173/1000 | Loss: 0.00004181
Iteration 174/1000 | Loss: 0.00004181
Iteration 175/1000 | Loss: 0.00004181
Iteration 176/1000 | Loss: 0.00004181
Iteration 177/1000 | Loss: 0.00004181
Iteration 178/1000 | Loss: 0.00004181
Iteration 179/1000 | Loss: 0.00004181
Iteration 180/1000 | Loss: 0.00004181
Iteration 181/1000 | Loss: 0.00004180
Iteration 182/1000 | Loss: 0.00004180
Iteration 183/1000 | Loss: 0.00004180
Iteration 184/1000 | Loss: 0.00004180
Iteration 185/1000 | Loss: 0.00004180
Iteration 186/1000 | Loss: 0.00004180
Iteration 187/1000 | Loss: 0.00004180
Iteration 188/1000 | Loss: 0.00004180
Iteration 189/1000 | Loss: 0.00004180
Iteration 190/1000 | Loss: 0.00004180
Iteration 191/1000 | Loss: 0.00004180
Iteration 192/1000 | Loss: 0.00004180
Iteration 193/1000 | Loss: 0.00004180
Iteration 194/1000 | Loss: 0.00004180
Iteration 195/1000 | Loss: 0.00004180
Iteration 196/1000 | Loss: 0.00004180
Iteration 197/1000 | Loss: 0.00004180
Iteration 198/1000 | Loss: 0.00004180
Iteration 199/1000 | Loss: 0.00004180
Iteration 200/1000 | Loss: 0.00004180
Iteration 201/1000 | Loss: 0.00004180
Iteration 202/1000 | Loss: 0.00004180
Iteration 203/1000 | Loss: 0.00004180
Iteration 204/1000 | Loss: 0.00004180
Iteration 205/1000 | Loss: 0.00004180
Iteration 206/1000 | Loss: 0.00004180
Iteration 207/1000 | Loss: 0.00004180
Iteration 208/1000 | Loss: 0.00004180
Iteration 209/1000 | Loss: 0.00004180
Iteration 210/1000 | Loss: 0.00004180
Iteration 211/1000 | Loss: 0.00004180
Iteration 212/1000 | Loss: 0.00004180
Iteration 213/1000 | Loss: 0.00004180
Iteration 214/1000 | Loss: 0.00004180
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [4.180009273113683e-05, 4.180009273113683e-05, 4.180009273113683e-05, 4.180009273113683e-05, 4.180009273113683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.180009273113683e-05

Optimization complete. Final v2v error: 5.13710355758667 mm

Highest mean error: 9.280409812927246 mm for frame 119

Lowest mean error: 3.989698886871338 mm for frame 76

Saving results

Total time: 117.8846116065979
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864062
Iteration 2/25 | Loss: 0.00197049
Iteration 3/25 | Loss: 0.00142167
Iteration 4/25 | Loss: 0.00130907
Iteration 5/25 | Loss: 0.00135124
Iteration 6/25 | Loss: 0.00125461
Iteration 7/25 | Loss: 0.00122902
Iteration 8/25 | Loss: 0.00122028
Iteration 9/25 | Loss: 0.00121393
Iteration 10/25 | Loss: 0.00120628
Iteration 11/25 | Loss: 0.00120677
Iteration 12/25 | Loss: 0.00120557
Iteration 13/25 | Loss: 0.00120171
Iteration 14/25 | Loss: 0.00119936
Iteration 15/25 | Loss: 0.00120008
Iteration 16/25 | Loss: 0.00120088
Iteration 17/25 | Loss: 0.00120028
Iteration 18/25 | Loss: 0.00120081
Iteration 19/25 | Loss: 0.00120042
Iteration 20/25 | Loss: 0.00120080
Iteration 21/25 | Loss: 0.00120019
Iteration 22/25 | Loss: 0.00120021
Iteration 23/25 | Loss: 0.00119968
Iteration 24/25 | Loss: 0.00120038
Iteration 25/25 | Loss: 0.00120038

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.83218455
Iteration 2/25 | Loss: 0.00540776
Iteration 3/25 | Loss: 0.00540776
Iteration 4/25 | Loss: 0.00540776
Iteration 5/25 | Loss: 0.00540775
Iteration 6/25 | Loss: 0.00540775
Iteration 7/25 | Loss: 0.00540775
Iteration 8/25 | Loss: 0.00540775
Iteration 9/25 | Loss: 0.00540775
Iteration 10/25 | Loss: 0.00540775
Iteration 11/25 | Loss: 0.00540775
Iteration 12/25 | Loss: 0.00540775
Iteration 13/25 | Loss: 0.00540775
Iteration 14/25 | Loss: 0.00540775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.005407753866165876, 0.005407753866165876, 0.005407753866165876, 0.005407753866165876, 0.005407753866165876]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005407753866165876

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00540775
Iteration 2/1000 | Loss: 0.00004474
Iteration 3/1000 | Loss: 0.00003489
Iteration 4/1000 | Loss: 0.00004762
Iteration 5/1000 | Loss: 0.00002895
Iteration 6/1000 | Loss: 0.00003439
Iteration 7/1000 | Loss: 0.00003770
Iteration 8/1000 | Loss: 0.00002843
Iteration 9/1000 | Loss: 0.00004242
Iteration 10/1000 | Loss: 0.00003909
Iteration 11/1000 | Loss: 0.00004016
Iteration 12/1000 | Loss: 0.00005210
Iteration 13/1000 | Loss: 0.00004211
Iteration 14/1000 | Loss: 0.00003256
Iteration 15/1000 | Loss: 0.00004208
Iteration 16/1000 | Loss: 0.00003319
Iteration 17/1000 | Loss: 0.00003817
Iteration 18/1000 | Loss: 0.00003583
Iteration 19/1000 | Loss: 0.00002938
Iteration 20/1000 | Loss: 0.00005263
Iteration 21/1000 | Loss: 0.00002889
Iteration 22/1000 | Loss: 0.00002596
Iteration 23/1000 | Loss: 0.00002387
Iteration 24/1000 | Loss: 0.00002293
Iteration 25/1000 | Loss: 0.00002250
Iteration 26/1000 | Loss: 0.00002231
Iteration 27/1000 | Loss: 0.00002225
Iteration 28/1000 | Loss: 0.00002223
Iteration 29/1000 | Loss: 0.00002223
Iteration 30/1000 | Loss: 0.00002222
Iteration 31/1000 | Loss: 0.00002222
Iteration 32/1000 | Loss: 0.00002215
Iteration 33/1000 | Loss: 0.00002213
Iteration 34/1000 | Loss: 0.00002212
Iteration 35/1000 | Loss: 0.00002212
Iteration 36/1000 | Loss: 0.00002211
Iteration 37/1000 | Loss: 0.00002209
Iteration 38/1000 | Loss: 0.00002208
Iteration 39/1000 | Loss: 0.00002208
Iteration 40/1000 | Loss: 0.00002208
Iteration 41/1000 | Loss: 0.00002207
Iteration 42/1000 | Loss: 0.00002207
Iteration 43/1000 | Loss: 0.00002207
Iteration 44/1000 | Loss: 0.00002207
Iteration 45/1000 | Loss: 0.00002207
Iteration 46/1000 | Loss: 0.00002207
Iteration 47/1000 | Loss: 0.00002207
Iteration 48/1000 | Loss: 0.00002207
Iteration 49/1000 | Loss: 0.00002207
Iteration 50/1000 | Loss: 0.00002206
Iteration 51/1000 | Loss: 0.00002206
Iteration 52/1000 | Loss: 0.00002203
Iteration 53/1000 | Loss: 0.00002203
Iteration 54/1000 | Loss: 0.00002202
Iteration 55/1000 | Loss: 0.00002196
Iteration 56/1000 | Loss: 0.00002190
Iteration 57/1000 | Loss: 0.00002190
Iteration 58/1000 | Loss: 0.00002187
Iteration 59/1000 | Loss: 0.00002186
Iteration 60/1000 | Loss: 0.00002185
Iteration 61/1000 | Loss: 0.00002184
Iteration 62/1000 | Loss: 0.00002182
Iteration 63/1000 | Loss: 0.00002181
Iteration 64/1000 | Loss: 0.00002181
Iteration 65/1000 | Loss: 0.00002181
Iteration 66/1000 | Loss: 0.00002180
Iteration 67/1000 | Loss: 0.00002180
Iteration 68/1000 | Loss: 0.00002179
Iteration 69/1000 | Loss: 0.00002179
Iteration 70/1000 | Loss: 0.00002179
Iteration 71/1000 | Loss: 0.00002178
Iteration 72/1000 | Loss: 0.00002178
Iteration 73/1000 | Loss: 0.00002177
Iteration 74/1000 | Loss: 0.00002177
Iteration 75/1000 | Loss: 0.00002176
Iteration 76/1000 | Loss: 0.00002176
Iteration 77/1000 | Loss: 0.00002176
Iteration 78/1000 | Loss: 0.00002176
Iteration 79/1000 | Loss: 0.00002176
Iteration 80/1000 | Loss: 0.00002176
Iteration 81/1000 | Loss: 0.00002175
Iteration 82/1000 | Loss: 0.00002175
Iteration 83/1000 | Loss: 0.00002175
Iteration 84/1000 | Loss: 0.00002174
Iteration 85/1000 | Loss: 0.00002174
Iteration 86/1000 | Loss: 0.00002174
Iteration 87/1000 | Loss: 0.00002174
Iteration 88/1000 | Loss: 0.00002174
Iteration 89/1000 | Loss: 0.00002173
Iteration 90/1000 | Loss: 0.00002173
Iteration 91/1000 | Loss: 0.00002173
Iteration 92/1000 | Loss: 0.00002173
Iteration 93/1000 | Loss: 0.00002173
Iteration 94/1000 | Loss: 0.00002173
Iteration 95/1000 | Loss: 0.00002173
Iteration 96/1000 | Loss: 0.00002173
Iteration 97/1000 | Loss: 0.00002172
Iteration 98/1000 | Loss: 0.00002172
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002171
Iteration 101/1000 | Loss: 0.00002171
Iteration 102/1000 | Loss: 0.00002171
Iteration 103/1000 | Loss: 0.00002171
Iteration 104/1000 | Loss: 0.00002171
Iteration 105/1000 | Loss: 0.00002171
Iteration 106/1000 | Loss: 0.00002170
Iteration 107/1000 | Loss: 0.00002170
Iteration 108/1000 | Loss: 0.00002170
Iteration 109/1000 | Loss: 0.00002170
Iteration 110/1000 | Loss: 0.00002170
Iteration 111/1000 | Loss: 0.00002170
Iteration 112/1000 | Loss: 0.00002170
Iteration 113/1000 | Loss: 0.00002170
Iteration 114/1000 | Loss: 0.00002170
Iteration 115/1000 | Loss: 0.00002170
Iteration 116/1000 | Loss: 0.00002170
Iteration 117/1000 | Loss: 0.00002169
Iteration 118/1000 | Loss: 0.00002169
Iteration 119/1000 | Loss: 0.00002169
Iteration 120/1000 | Loss: 0.00002169
Iteration 121/1000 | Loss: 0.00002168
Iteration 122/1000 | Loss: 0.00002168
Iteration 123/1000 | Loss: 0.00002168
Iteration 124/1000 | Loss: 0.00002168
Iteration 125/1000 | Loss: 0.00002168
Iteration 126/1000 | Loss: 0.00002168
Iteration 127/1000 | Loss: 0.00002167
Iteration 128/1000 | Loss: 0.00002167
Iteration 129/1000 | Loss: 0.00002167
Iteration 130/1000 | Loss: 0.00002167
Iteration 131/1000 | Loss: 0.00002167
Iteration 132/1000 | Loss: 0.00002167
Iteration 133/1000 | Loss: 0.00002167
Iteration 134/1000 | Loss: 0.00002166
Iteration 135/1000 | Loss: 0.00002166
Iteration 136/1000 | Loss: 0.00002166
Iteration 137/1000 | Loss: 0.00002166
Iteration 138/1000 | Loss: 0.00002166
Iteration 139/1000 | Loss: 0.00002166
Iteration 140/1000 | Loss: 0.00002166
Iteration 141/1000 | Loss: 0.00002166
Iteration 142/1000 | Loss: 0.00002166
Iteration 143/1000 | Loss: 0.00002165
Iteration 144/1000 | Loss: 0.00002165
Iteration 145/1000 | Loss: 0.00002165
Iteration 146/1000 | Loss: 0.00002165
Iteration 147/1000 | Loss: 0.00002165
Iteration 148/1000 | Loss: 0.00002165
Iteration 149/1000 | Loss: 0.00002165
Iteration 150/1000 | Loss: 0.00002165
Iteration 151/1000 | Loss: 0.00002165
Iteration 152/1000 | Loss: 0.00002165
Iteration 153/1000 | Loss: 0.00002164
Iteration 154/1000 | Loss: 0.00002164
Iteration 155/1000 | Loss: 0.00002164
Iteration 156/1000 | Loss: 0.00002164
Iteration 157/1000 | Loss: 0.00002164
Iteration 158/1000 | Loss: 0.00002164
Iteration 159/1000 | Loss: 0.00002163
Iteration 160/1000 | Loss: 0.00002163
Iteration 161/1000 | Loss: 0.00002163
Iteration 162/1000 | Loss: 0.00002163
Iteration 163/1000 | Loss: 0.00002163
Iteration 164/1000 | Loss: 0.00002163
Iteration 165/1000 | Loss: 0.00002163
Iteration 166/1000 | Loss: 0.00002163
Iteration 167/1000 | Loss: 0.00002162
Iteration 168/1000 | Loss: 0.00002162
Iteration 169/1000 | Loss: 0.00002162
Iteration 170/1000 | Loss: 0.00002162
Iteration 171/1000 | Loss: 0.00002161
Iteration 172/1000 | Loss: 0.00002161
Iteration 173/1000 | Loss: 0.00002161
Iteration 174/1000 | Loss: 0.00002161
Iteration 175/1000 | Loss: 0.00002161
Iteration 176/1000 | Loss: 0.00002161
Iteration 177/1000 | Loss: 0.00002161
Iteration 178/1000 | Loss: 0.00002161
Iteration 179/1000 | Loss: 0.00002161
Iteration 180/1000 | Loss: 0.00002161
Iteration 181/1000 | Loss: 0.00002161
Iteration 182/1000 | Loss: 0.00002161
Iteration 183/1000 | Loss: 0.00002161
Iteration 184/1000 | Loss: 0.00002161
Iteration 185/1000 | Loss: 0.00002160
Iteration 186/1000 | Loss: 0.00002160
Iteration 187/1000 | Loss: 0.00002160
Iteration 188/1000 | Loss: 0.00002160
Iteration 189/1000 | Loss: 0.00002160
Iteration 190/1000 | Loss: 0.00002160
Iteration 191/1000 | Loss: 0.00002160
Iteration 192/1000 | Loss: 0.00002160
Iteration 193/1000 | Loss: 0.00002160
Iteration 194/1000 | Loss: 0.00002160
Iteration 195/1000 | Loss: 0.00002160
Iteration 196/1000 | Loss: 0.00002160
Iteration 197/1000 | Loss: 0.00002160
Iteration 198/1000 | Loss: 0.00002159
Iteration 199/1000 | Loss: 0.00002159
Iteration 200/1000 | Loss: 0.00002159
Iteration 201/1000 | Loss: 0.00002159
Iteration 202/1000 | Loss: 0.00002159
Iteration 203/1000 | Loss: 0.00002159
Iteration 204/1000 | Loss: 0.00002159
Iteration 205/1000 | Loss: 0.00002159
Iteration 206/1000 | Loss: 0.00002159
Iteration 207/1000 | Loss: 0.00002159
Iteration 208/1000 | Loss: 0.00002159
Iteration 209/1000 | Loss: 0.00002159
Iteration 210/1000 | Loss: 0.00002159
Iteration 211/1000 | Loss: 0.00002159
Iteration 212/1000 | Loss: 0.00002159
Iteration 213/1000 | Loss: 0.00002159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [2.158922143280506e-05, 2.158922143280506e-05, 2.158922143280506e-05, 2.158922143280506e-05, 2.158922143280506e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.158922143280506e-05

Optimization complete. Final v2v error: 3.955105781555176 mm

Highest mean error: 4.845097064971924 mm for frame 215

Lowest mean error: 3.4519128799438477 mm for frame 189

Saving results

Total time: 109.80340933799744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736594
Iteration 2/25 | Loss: 0.00127930
Iteration 3/25 | Loss: 0.00114396
Iteration 4/25 | Loss: 0.00112712
Iteration 5/25 | Loss: 0.00112079
Iteration 6/25 | Loss: 0.00111929
Iteration 7/25 | Loss: 0.00111889
Iteration 8/25 | Loss: 0.00111889
Iteration 9/25 | Loss: 0.00111889
Iteration 10/25 | Loss: 0.00111889
Iteration 11/25 | Loss: 0.00111889
Iteration 12/25 | Loss: 0.00111889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011188882635906339, 0.0011188882635906339, 0.0011188882635906339, 0.0011188882635906339, 0.0011188882635906339]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011188882635906339

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91927719
Iteration 2/25 | Loss: 0.00506746
Iteration 3/25 | Loss: 0.00506746
Iteration 4/25 | Loss: 0.00506746
Iteration 5/25 | Loss: 0.00506746
Iteration 6/25 | Loss: 0.00506746
Iteration 7/25 | Loss: 0.00506746
Iteration 8/25 | Loss: 0.00506746
Iteration 9/25 | Loss: 0.00506746
Iteration 10/25 | Loss: 0.00506746
Iteration 11/25 | Loss: 0.00506746
Iteration 12/25 | Loss: 0.00506746
Iteration 13/25 | Loss: 0.00506746
Iteration 14/25 | Loss: 0.00506746
Iteration 15/25 | Loss: 0.00506746
Iteration 16/25 | Loss: 0.00506746
Iteration 17/25 | Loss: 0.00506746
Iteration 18/25 | Loss: 0.00506746
Iteration 19/25 | Loss: 0.00506746
Iteration 20/25 | Loss: 0.00506746
Iteration 21/25 | Loss: 0.00506746
Iteration 22/25 | Loss: 0.00506746
Iteration 23/25 | Loss: 0.00506746
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.005067457910627127, 0.005067457910627127, 0.005067457910627127, 0.005067457910627127, 0.005067457910627127]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005067457910627127

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00506746
Iteration 2/1000 | Loss: 0.00003122
Iteration 3/1000 | Loss: 0.00002546
Iteration 4/1000 | Loss: 0.00002403
Iteration 5/1000 | Loss: 0.00002322
Iteration 6/1000 | Loss: 0.00002251
Iteration 7/1000 | Loss: 0.00002202
Iteration 8/1000 | Loss: 0.00002167
Iteration 9/1000 | Loss: 0.00002144
Iteration 10/1000 | Loss: 0.00002141
Iteration 11/1000 | Loss: 0.00002133
Iteration 12/1000 | Loss: 0.00002132
Iteration 13/1000 | Loss: 0.00002131
Iteration 14/1000 | Loss: 0.00002121
Iteration 15/1000 | Loss: 0.00002120
Iteration 16/1000 | Loss: 0.00002120
Iteration 17/1000 | Loss: 0.00002119
Iteration 18/1000 | Loss: 0.00002113
Iteration 19/1000 | Loss: 0.00002112
Iteration 20/1000 | Loss: 0.00002110
Iteration 21/1000 | Loss: 0.00002110
Iteration 22/1000 | Loss: 0.00002109
Iteration 23/1000 | Loss: 0.00002109
Iteration 24/1000 | Loss: 0.00002109
Iteration 25/1000 | Loss: 0.00002108
Iteration 26/1000 | Loss: 0.00002108
Iteration 27/1000 | Loss: 0.00002108
Iteration 28/1000 | Loss: 0.00002108
Iteration 29/1000 | Loss: 0.00002107
Iteration 30/1000 | Loss: 0.00002106
Iteration 31/1000 | Loss: 0.00002106
Iteration 32/1000 | Loss: 0.00002105
Iteration 33/1000 | Loss: 0.00002105
Iteration 34/1000 | Loss: 0.00002105
Iteration 35/1000 | Loss: 0.00002104
Iteration 36/1000 | Loss: 0.00002104
Iteration 37/1000 | Loss: 0.00002104
Iteration 38/1000 | Loss: 0.00002104
Iteration 39/1000 | Loss: 0.00002103
Iteration 40/1000 | Loss: 0.00002103
Iteration 41/1000 | Loss: 0.00002103
Iteration 42/1000 | Loss: 0.00002103
Iteration 43/1000 | Loss: 0.00002103
Iteration 44/1000 | Loss: 0.00002103
Iteration 45/1000 | Loss: 0.00002102
Iteration 46/1000 | Loss: 0.00002102
Iteration 47/1000 | Loss: 0.00002102
Iteration 48/1000 | Loss: 0.00002102
Iteration 49/1000 | Loss: 0.00002102
Iteration 50/1000 | Loss: 0.00002102
Iteration 51/1000 | Loss: 0.00002102
Iteration 52/1000 | Loss: 0.00002102
Iteration 53/1000 | Loss: 0.00002101
Iteration 54/1000 | Loss: 0.00002101
Iteration 55/1000 | Loss: 0.00002100
Iteration 56/1000 | Loss: 0.00002099
Iteration 57/1000 | Loss: 0.00002099
Iteration 58/1000 | Loss: 0.00002099
Iteration 59/1000 | Loss: 0.00002099
Iteration 60/1000 | Loss: 0.00002099
Iteration 61/1000 | Loss: 0.00002099
Iteration 62/1000 | Loss: 0.00002099
Iteration 63/1000 | Loss: 0.00002099
Iteration 64/1000 | Loss: 0.00002099
Iteration 65/1000 | Loss: 0.00002099
Iteration 66/1000 | Loss: 0.00002098
Iteration 67/1000 | Loss: 0.00002098
Iteration 68/1000 | Loss: 0.00002098
Iteration 69/1000 | Loss: 0.00002098
Iteration 70/1000 | Loss: 0.00002097
Iteration 71/1000 | Loss: 0.00002097
Iteration 72/1000 | Loss: 0.00002097
Iteration 73/1000 | Loss: 0.00002096
Iteration 74/1000 | Loss: 0.00002096
Iteration 75/1000 | Loss: 0.00002096
Iteration 76/1000 | Loss: 0.00002096
Iteration 77/1000 | Loss: 0.00002096
Iteration 78/1000 | Loss: 0.00002096
Iteration 79/1000 | Loss: 0.00002096
Iteration 80/1000 | Loss: 0.00002096
Iteration 81/1000 | Loss: 0.00002096
Iteration 82/1000 | Loss: 0.00002096
Iteration 83/1000 | Loss: 0.00002096
Iteration 84/1000 | Loss: 0.00002095
Iteration 85/1000 | Loss: 0.00002095
Iteration 86/1000 | Loss: 0.00002095
Iteration 87/1000 | Loss: 0.00002094
Iteration 88/1000 | Loss: 0.00002094
Iteration 89/1000 | Loss: 0.00002094
Iteration 90/1000 | Loss: 0.00002094
Iteration 91/1000 | Loss: 0.00002093
Iteration 92/1000 | Loss: 0.00002093
Iteration 93/1000 | Loss: 0.00002093
Iteration 94/1000 | Loss: 0.00002093
Iteration 95/1000 | Loss: 0.00002093
Iteration 96/1000 | Loss: 0.00002093
Iteration 97/1000 | Loss: 0.00002093
Iteration 98/1000 | Loss: 0.00002093
Iteration 99/1000 | Loss: 0.00002093
Iteration 100/1000 | Loss: 0.00002093
Iteration 101/1000 | Loss: 0.00002093
Iteration 102/1000 | Loss: 0.00002093
Iteration 103/1000 | Loss: 0.00002093
Iteration 104/1000 | Loss: 0.00002092
Iteration 105/1000 | Loss: 0.00002092
Iteration 106/1000 | Loss: 0.00002092
Iteration 107/1000 | Loss: 0.00002092
Iteration 108/1000 | Loss: 0.00002092
Iteration 109/1000 | Loss: 0.00002092
Iteration 110/1000 | Loss: 0.00002092
Iteration 111/1000 | Loss: 0.00002092
Iteration 112/1000 | Loss: 0.00002092
Iteration 113/1000 | Loss: 0.00002091
Iteration 114/1000 | Loss: 0.00002091
Iteration 115/1000 | Loss: 0.00002091
Iteration 116/1000 | Loss: 0.00002091
Iteration 117/1000 | Loss: 0.00002091
Iteration 118/1000 | Loss: 0.00002091
Iteration 119/1000 | Loss: 0.00002091
Iteration 120/1000 | Loss: 0.00002091
Iteration 121/1000 | Loss: 0.00002091
Iteration 122/1000 | Loss: 0.00002091
Iteration 123/1000 | Loss: 0.00002091
Iteration 124/1000 | Loss: 0.00002091
Iteration 125/1000 | Loss: 0.00002091
Iteration 126/1000 | Loss: 0.00002090
Iteration 127/1000 | Loss: 0.00002090
Iteration 128/1000 | Loss: 0.00002090
Iteration 129/1000 | Loss: 0.00002090
Iteration 130/1000 | Loss: 0.00002090
Iteration 131/1000 | Loss: 0.00002090
Iteration 132/1000 | Loss: 0.00002090
Iteration 133/1000 | Loss: 0.00002090
Iteration 134/1000 | Loss: 0.00002089
Iteration 135/1000 | Loss: 0.00002089
Iteration 136/1000 | Loss: 0.00002089
Iteration 137/1000 | Loss: 0.00002089
Iteration 138/1000 | Loss: 0.00002089
Iteration 139/1000 | Loss: 0.00002089
Iteration 140/1000 | Loss: 0.00002089
Iteration 141/1000 | Loss: 0.00002089
Iteration 142/1000 | Loss: 0.00002089
Iteration 143/1000 | Loss: 0.00002089
Iteration 144/1000 | Loss: 0.00002089
Iteration 145/1000 | Loss: 0.00002089
Iteration 146/1000 | Loss: 0.00002089
Iteration 147/1000 | Loss: 0.00002089
Iteration 148/1000 | Loss: 0.00002089
Iteration 149/1000 | Loss: 0.00002088
Iteration 150/1000 | Loss: 0.00002088
Iteration 151/1000 | Loss: 0.00002088
Iteration 152/1000 | Loss: 0.00002088
Iteration 153/1000 | Loss: 0.00002088
Iteration 154/1000 | Loss: 0.00002088
Iteration 155/1000 | Loss: 0.00002088
Iteration 156/1000 | Loss: 0.00002088
Iteration 157/1000 | Loss: 0.00002088
Iteration 158/1000 | Loss: 0.00002087
Iteration 159/1000 | Loss: 0.00002087
Iteration 160/1000 | Loss: 0.00002087
Iteration 161/1000 | Loss: 0.00002087
Iteration 162/1000 | Loss: 0.00002087
Iteration 163/1000 | Loss: 0.00002087
Iteration 164/1000 | Loss: 0.00002087
Iteration 165/1000 | Loss: 0.00002087
Iteration 166/1000 | Loss: 0.00002087
Iteration 167/1000 | Loss: 0.00002087
Iteration 168/1000 | Loss: 0.00002087
Iteration 169/1000 | Loss: 0.00002087
Iteration 170/1000 | Loss: 0.00002087
Iteration 171/1000 | Loss: 0.00002087
Iteration 172/1000 | Loss: 0.00002087
Iteration 173/1000 | Loss: 0.00002087
Iteration 174/1000 | Loss: 0.00002087
Iteration 175/1000 | Loss: 0.00002087
Iteration 176/1000 | Loss: 0.00002087
Iteration 177/1000 | Loss: 0.00002087
Iteration 178/1000 | Loss: 0.00002087
Iteration 179/1000 | Loss: 0.00002087
Iteration 180/1000 | Loss: 0.00002087
Iteration 181/1000 | Loss: 0.00002087
Iteration 182/1000 | Loss: 0.00002087
Iteration 183/1000 | Loss: 0.00002087
Iteration 184/1000 | Loss: 0.00002087
Iteration 185/1000 | Loss: 0.00002087
Iteration 186/1000 | Loss: 0.00002087
Iteration 187/1000 | Loss: 0.00002087
Iteration 188/1000 | Loss: 0.00002087
Iteration 189/1000 | Loss: 0.00002087
Iteration 190/1000 | Loss: 0.00002087
Iteration 191/1000 | Loss: 0.00002087
Iteration 192/1000 | Loss: 0.00002087
Iteration 193/1000 | Loss: 0.00002087
Iteration 194/1000 | Loss: 0.00002087
Iteration 195/1000 | Loss: 0.00002087
Iteration 196/1000 | Loss: 0.00002087
Iteration 197/1000 | Loss: 0.00002087
Iteration 198/1000 | Loss: 0.00002087
Iteration 199/1000 | Loss: 0.00002087
Iteration 200/1000 | Loss: 0.00002087
Iteration 201/1000 | Loss: 0.00002087
Iteration 202/1000 | Loss: 0.00002087
Iteration 203/1000 | Loss: 0.00002087
Iteration 204/1000 | Loss: 0.00002087
Iteration 205/1000 | Loss: 0.00002087
Iteration 206/1000 | Loss: 0.00002087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [2.0869545551249757e-05, 2.0869545551249757e-05, 2.0869545551249757e-05, 2.0869545551249757e-05, 2.0869545551249757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0869545551249757e-05

Optimization complete. Final v2v error: 3.9507644176483154 mm

Highest mean error: 4.745253562927246 mm for frame 72

Lowest mean error: 3.536008358001709 mm for frame 125

Saving results

Total time: 35.711023569107056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00904963
Iteration 2/25 | Loss: 0.00133126
Iteration 3/25 | Loss: 0.00121857
Iteration 4/25 | Loss: 0.00117435
Iteration 5/25 | Loss: 0.00116561
Iteration 6/25 | Loss: 0.00116328
Iteration 7/25 | Loss: 0.00116265
Iteration 8/25 | Loss: 0.00116265
Iteration 9/25 | Loss: 0.00116265
Iteration 10/25 | Loss: 0.00116265
Iteration 11/25 | Loss: 0.00116265
Iteration 12/25 | Loss: 0.00116265
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011626549530774355, 0.0011626549530774355, 0.0011626549530774355, 0.0011626549530774355, 0.0011626549530774355]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011626549530774355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.03159285
Iteration 2/25 | Loss: 0.00683288
Iteration 3/25 | Loss: 0.00683283
Iteration 4/25 | Loss: 0.00683283
Iteration 5/25 | Loss: 0.00683283
Iteration 6/25 | Loss: 0.00683283
Iteration 7/25 | Loss: 0.00683283
Iteration 8/25 | Loss: 0.00683283
Iteration 9/25 | Loss: 0.00683283
Iteration 10/25 | Loss: 0.00683283
Iteration 11/25 | Loss: 0.00683283
Iteration 12/25 | Loss: 0.00683283
Iteration 13/25 | Loss: 0.00683283
Iteration 14/25 | Loss: 0.00683283
Iteration 15/25 | Loss: 0.00683283
Iteration 16/25 | Loss: 0.00683283
Iteration 17/25 | Loss: 0.00683283
Iteration 18/25 | Loss: 0.00683283
Iteration 19/25 | Loss: 0.00683283
Iteration 20/25 | Loss: 0.00683283
Iteration 21/25 | Loss: 0.00683283
Iteration 22/25 | Loss: 0.00683283
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.006832826416939497, 0.006832826416939497, 0.006832826416939497, 0.006832826416939497, 0.006832826416939497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.006832826416939497

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00683283
Iteration 2/1000 | Loss: 0.00004580
Iteration 3/1000 | Loss: 0.00003058
Iteration 4/1000 | Loss: 0.00002636
Iteration 5/1000 | Loss: 0.00002471
Iteration 6/1000 | Loss: 0.00002347
Iteration 7/1000 | Loss: 0.00002296
Iteration 8/1000 | Loss: 0.00002226
Iteration 9/1000 | Loss: 0.00002163
Iteration 10/1000 | Loss: 0.00002129
Iteration 11/1000 | Loss: 0.00002100
Iteration 12/1000 | Loss: 0.00002082
Iteration 13/1000 | Loss: 0.00002074
Iteration 14/1000 | Loss: 0.00002073
Iteration 15/1000 | Loss: 0.00002073
Iteration 16/1000 | Loss: 0.00002072
Iteration 17/1000 | Loss: 0.00002072
Iteration 18/1000 | Loss: 0.00002068
Iteration 19/1000 | Loss: 0.00002064
Iteration 20/1000 | Loss: 0.00002063
Iteration 21/1000 | Loss: 0.00002063
Iteration 22/1000 | Loss: 0.00002062
Iteration 23/1000 | Loss: 0.00002061
Iteration 24/1000 | Loss: 0.00002061
Iteration 25/1000 | Loss: 0.00002060
Iteration 26/1000 | Loss: 0.00002060
Iteration 27/1000 | Loss: 0.00002059
Iteration 28/1000 | Loss: 0.00002059
Iteration 29/1000 | Loss: 0.00002059
Iteration 30/1000 | Loss: 0.00002058
Iteration 31/1000 | Loss: 0.00002058
Iteration 32/1000 | Loss: 0.00002058
Iteration 33/1000 | Loss: 0.00002056
Iteration 34/1000 | Loss: 0.00002055
Iteration 35/1000 | Loss: 0.00002055
Iteration 36/1000 | Loss: 0.00002055
Iteration 37/1000 | Loss: 0.00002055
Iteration 38/1000 | Loss: 0.00002055
Iteration 39/1000 | Loss: 0.00002055
Iteration 40/1000 | Loss: 0.00002055
Iteration 41/1000 | Loss: 0.00002055
Iteration 42/1000 | Loss: 0.00002055
Iteration 43/1000 | Loss: 0.00002054
Iteration 44/1000 | Loss: 0.00002054
Iteration 45/1000 | Loss: 0.00002054
Iteration 46/1000 | Loss: 0.00002053
Iteration 47/1000 | Loss: 0.00002053
Iteration 48/1000 | Loss: 0.00002053
Iteration 49/1000 | Loss: 0.00002053
Iteration 50/1000 | Loss: 0.00002053
Iteration 51/1000 | Loss: 0.00002052
Iteration 52/1000 | Loss: 0.00002052
Iteration 53/1000 | Loss: 0.00002052
Iteration 54/1000 | Loss: 0.00002052
Iteration 55/1000 | Loss: 0.00002051
Iteration 56/1000 | Loss: 0.00002051
Iteration 57/1000 | Loss: 0.00002051
Iteration 58/1000 | Loss: 0.00002051
Iteration 59/1000 | Loss: 0.00002051
Iteration 60/1000 | Loss: 0.00002050
Iteration 61/1000 | Loss: 0.00002050
Iteration 62/1000 | Loss: 0.00002050
Iteration 63/1000 | Loss: 0.00002050
Iteration 64/1000 | Loss: 0.00002049
Iteration 65/1000 | Loss: 0.00002049
Iteration 66/1000 | Loss: 0.00002049
Iteration 67/1000 | Loss: 0.00002049
Iteration 68/1000 | Loss: 0.00002049
Iteration 69/1000 | Loss: 0.00002049
Iteration 70/1000 | Loss: 0.00002049
Iteration 71/1000 | Loss: 0.00002049
Iteration 72/1000 | Loss: 0.00002049
Iteration 73/1000 | Loss: 0.00002049
Iteration 74/1000 | Loss: 0.00002049
Iteration 75/1000 | Loss: 0.00002049
Iteration 76/1000 | Loss: 0.00002048
Iteration 77/1000 | Loss: 0.00002048
Iteration 78/1000 | Loss: 0.00002048
Iteration 79/1000 | Loss: 0.00002048
Iteration 80/1000 | Loss: 0.00002048
Iteration 81/1000 | Loss: 0.00002048
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002048
Iteration 84/1000 | Loss: 0.00002048
Iteration 85/1000 | Loss: 0.00002048
Iteration 86/1000 | Loss: 0.00002048
Iteration 87/1000 | Loss: 0.00002048
Iteration 88/1000 | Loss: 0.00002048
Iteration 89/1000 | Loss: 0.00002048
Iteration 90/1000 | Loss: 0.00002048
Iteration 91/1000 | Loss: 0.00002048
Iteration 92/1000 | Loss: 0.00002048
Iteration 93/1000 | Loss: 0.00002048
Iteration 94/1000 | Loss: 0.00002048
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [2.0481758838286623e-05, 2.0481758838286623e-05, 2.0481758838286623e-05, 2.0481758838286623e-05, 2.0481758838286623e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0481758838286623e-05

Optimization complete. Final v2v error: 3.818132162094116 mm

Highest mean error: 4.174598693847656 mm for frame 28

Lowest mean error: 3.478273391723633 mm for frame 112

Saving results

Total time: 38.26693677902222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01192704
Iteration 2/25 | Loss: 0.00182197
Iteration 3/25 | Loss: 0.00132003
Iteration 4/25 | Loss: 0.00128131
Iteration 5/25 | Loss: 0.00127084
Iteration 6/25 | Loss: 0.00126799
Iteration 7/25 | Loss: 0.00126765
Iteration 8/25 | Loss: 0.00126765
Iteration 9/25 | Loss: 0.00126765
Iteration 10/25 | Loss: 0.00126765
Iteration 11/25 | Loss: 0.00126765
Iteration 12/25 | Loss: 0.00126765
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012676548212766647, 0.0012676548212766647, 0.0012676548212766647, 0.0012676548212766647, 0.0012676548212766647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012676548212766647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18143296
Iteration 2/25 | Loss: 0.00369926
Iteration 3/25 | Loss: 0.00369926
Iteration 4/25 | Loss: 0.00369926
Iteration 5/25 | Loss: 0.00369925
Iteration 6/25 | Loss: 0.00369925
Iteration 7/25 | Loss: 0.00369925
Iteration 8/25 | Loss: 0.00369925
Iteration 9/25 | Loss: 0.00369925
Iteration 10/25 | Loss: 0.00369925
Iteration 11/25 | Loss: 0.00369925
Iteration 12/25 | Loss: 0.00369925
Iteration 13/25 | Loss: 0.00369925
Iteration 14/25 | Loss: 0.00369925
Iteration 15/25 | Loss: 0.00369925
Iteration 16/25 | Loss: 0.00369925
Iteration 17/25 | Loss: 0.00369925
Iteration 18/25 | Loss: 0.00369925
Iteration 19/25 | Loss: 0.00369925
Iteration 20/25 | Loss: 0.00369925
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003699253313243389, 0.003699253313243389, 0.003699253313243389, 0.003699253313243389, 0.003699253313243389]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003699253313243389

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00369925
Iteration 2/1000 | Loss: 0.00005357
Iteration 3/1000 | Loss: 0.00004377
Iteration 4/1000 | Loss: 0.00004068
Iteration 5/1000 | Loss: 0.00003895
Iteration 6/1000 | Loss: 0.00003806
Iteration 7/1000 | Loss: 0.00003723
Iteration 8/1000 | Loss: 0.00003661
Iteration 9/1000 | Loss: 0.00003620
Iteration 10/1000 | Loss: 0.00003585
Iteration 11/1000 | Loss: 0.00003563
Iteration 12/1000 | Loss: 0.00003545
Iteration 13/1000 | Loss: 0.00003543
Iteration 14/1000 | Loss: 0.00003541
Iteration 15/1000 | Loss: 0.00003539
Iteration 16/1000 | Loss: 0.00003535
Iteration 17/1000 | Loss: 0.00003531
Iteration 18/1000 | Loss: 0.00003530
Iteration 19/1000 | Loss: 0.00003530
Iteration 20/1000 | Loss: 0.00003530
Iteration 21/1000 | Loss: 0.00003527
Iteration 22/1000 | Loss: 0.00003521
Iteration 23/1000 | Loss: 0.00003521
Iteration 24/1000 | Loss: 0.00003520
Iteration 25/1000 | Loss: 0.00003519
Iteration 26/1000 | Loss: 0.00003517
Iteration 27/1000 | Loss: 0.00003517
Iteration 28/1000 | Loss: 0.00003517
Iteration 29/1000 | Loss: 0.00003517
Iteration 30/1000 | Loss: 0.00003517
Iteration 31/1000 | Loss: 0.00003517
Iteration 32/1000 | Loss: 0.00003517
Iteration 33/1000 | Loss: 0.00003517
Iteration 34/1000 | Loss: 0.00003516
Iteration 35/1000 | Loss: 0.00003516
Iteration 36/1000 | Loss: 0.00003516
Iteration 37/1000 | Loss: 0.00003516
Iteration 38/1000 | Loss: 0.00003516
Iteration 39/1000 | Loss: 0.00003516
Iteration 40/1000 | Loss: 0.00003516
Iteration 41/1000 | Loss: 0.00003516
Iteration 42/1000 | Loss: 0.00003516
Iteration 43/1000 | Loss: 0.00003516
Iteration 44/1000 | Loss: 0.00003516
Iteration 45/1000 | Loss: 0.00003515
Iteration 46/1000 | Loss: 0.00003515
Iteration 47/1000 | Loss: 0.00003515
Iteration 48/1000 | Loss: 0.00003515
Iteration 49/1000 | Loss: 0.00003514
Iteration 50/1000 | Loss: 0.00003514
Iteration 51/1000 | Loss: 0.00003513
Iteration 52/1000 | Loss: 0.00003513
Iteration 53/1000 | Loss: 0.00003512
Iteration 54/1000 | Loss: 0.00003512
Iteration 55/1000 | Loss: 0.00003512
Iteration 56/1000 | Loss: 0.00003512
Iteration 57/1000 | Loss: 0.00003512
Iteration 58/1000 | Loss: 0.00003511
Iteration 59/1000 | Loss: 0.00003511
Iteration 60/1000 | Loss: 0.00003511
Iteration 61/1000 | Loss: 0.00003511
Iteration 62/1000 | Loss: 0.00003511
Iteration 63/1000 | Loss: 0.00003511
Iteration 64/1000 | Loss: 0.00003511
Iteration 65/1000 | Loss: 0.00003511
Iteration 66/1000 | Loss: 0.00003511
Iteration 67/1000 | Loss: 0.00003511
Iteration 68/1000 | Loss: 0.00003510
Iteration 69/1000 | Loss: 0.00003510
Iteration 70/1000 | Loss: 0.00003510
Iteration 71/1000 | Loss: 0.00003509
Iteration 72/1000 | Loss: 0.00003509
Iteration 73/1000 | Loss: 0.00003508
Iteration 74/1000 | Loss: 0.00003508
Iteration 75/1000 | Loss: 0.00003508
Iteration 76/1000 | Loss: 0.00003507
Iteration 77/1000 | Loss: 0.00003507
Iteration 78/1000 | Loss: 0.00003507
Iteration 79/1000 | Loss: 0.00003507
Iteration 80/1000 | Loss: 0.00003507
Iteration 81/1000 | Loss: 0.00003507
Iteration 82/1000 | Loss: 0.00003506
Iteration 83/1000 | Loss: 0.00003506
Iteration 84/1000 | Loss: 0.00003506
Iteration 85/1000 | Loss: 0.00003506
Iteration 86/1000 | Loss: 0.00003505
Iteration 87/1000 | Loss: 0.00003505
Iteration 88/1000 | Loss: 0.00003505
Iteration 89/1000 | Loss: 0.00003505
Iteration 90/1000 | Loss: 0.00003505
Iteration 91/1000 | Loss: 0.00003505
Iteration 92/1000 | Loss: 0.00003505
Iteration 93/1000 | Loss: 0.00003505
Iteration 94/1000 | Loss: 0.00003505
Iteration 95/1000 | Loss: 0.00003505
Iteration 96/1000 | Loss: 0.00003505
Iteration 97/1000 | Loss: 0.00003505
Iteration 98/1000 | Loss: 0.00003505
Iteration 99/1000 | Loss: 0.00003505
Iteration 100/1000 | Loss: 0.00003505
Iteration 101/1000 | Loss: 0.00003505
Iteration 102/1000 | Loss: 0.00003505
Iteration 103/1000 | Loss: 0.00003505
Iteration 104/1000 | Loss: 0.00003505
Iteration 105/1000 | Loss: 0.00003505
Iteration 106/1000 | Loss: 0.00003505
Iteration 107/1000 | Loss: 0.00003505
Iteration 108/1000 | Loss: 0.00003505
Iteration 109/1000 | Loss: 0.00003505
Iteration 110/1000 | Loss: 0.00003505
Iteration 111/1000 | Loss: 0.00003505
Iteration 112/1000 | Loss: 0.00003505
Iteration 113/1000 | Loss: 0.00003505
Iteration 114/1000 | Loss: 0.00003505
Iteration 115/1000 | Loss: 0.00003505
Iteration 116/1000 | Loss: 0.00003505
Iteration 117/1000 | Loss: 0.00003505
Iteration 118/1000 | Loss: 0.00003505
Iteration 119/1000 | Loss: 0.00003505
Iteration 120/1000 | Loss: 0.00003505
Iteration 121/1000 | Loss: 0.00003505
Iteration 122/1000 | Loss: 0.00003505
Iteration 123/1000 | Loss: 0.00003505
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [3.504532287479378e-05, 3.504532287479378e-05, 3.504532287479378e-05, 3.504532287479378e-05, 3.504532287479378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.504532287479378e-05

Optimization complete. Final v2v error: 4.930400848388672 mm

Highest mean error: 5.820431709289551 mm for frame 142

Lowest mean error: 4.310723781585693 mm for frame 112

Saving results

Total time: 37.68885684013367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533980
Iteration 2/25 | Loss: 0.00149213
Iteration 3/25 | Loss: 0.00122059
Iteration 4/25 | Loss: 0.00117916
Iteration 5/25 | Loss: 0.00116784
Iteration 6/25 | Loss: 0.00116450
Iteration 7/25 | Loss: 0.00116422
Iteration 8/25 | Loss: 0.00116422
Iteration 9/25 | Loss: 0.00116422
Iteration 10/25 | Loss: 0.00116422
Iteration 11/25 | Loss: 0.00116422
Iteration 12/25 | Loss: 0.00116422
Iteration 13/25 | Loss: 0.00116422
Iteration 14/25 | Loss: 0.00116422
Iteration 15/25 | Loss: 0.00116422
Iteration 16/25 | Loss: 0.00116422
Iteration 17/25 | Loss: 0.00116422
Iteration 18/25 | Loss: 0.00116422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011642173631116748, 0.0011642173631116748, 0.0011642173631116748, 0.0011642173631116748, 0.0011642173631116748]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011642173631116748

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.05686593
Iteration 2/25 | Loss: 0.00459117
Iteration 3/25 | Loss: 0.00459116
Iteration 4/25 | Loss: 0.00459116
Iteration 5/25 | Loss: 0.00459116
Iteration 6/25 | Loss: 0.00459116
Iteration 7/25 | Loss: 0.00459116
Iteration 8/25 | Loss: 0.00459116
Iteration 9/25 | Loss: 0.00459116
Iteration 10/25 | Loss: 0.00459116
Iteration 11/25 | Loss: 0.00459116
Iteration 12/25 | Loss: 0.00459116
Iteration 13/25 | Loss: 0.00459116
Iteration 14/25 | Loss: 0.00459116
Iteration 15/25 | Loss: 0.00459116
Iteration 16/25 | Loss: 0.00459116
Iteration 17/25 | Loss: 0.00459116
Iteration 18/25 | Loss: 0.00459116
Iteration 19/25 | Loss: 0.00459116
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0045911590568721294, 0.0045911590568721294, 0.0045911590568721294, 0.0045911590568721294, 0.0045911590568721294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0045911590568721294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00459116
Iteration 2/1000 | Loss: 0.00004282
Iteration 3/1000 | Loss: 0.00002907
Iteration 4/1000 | Loss: 0.00002725
Iteration 5/1000 | Loss: 0.00002644
Iteration 6/1000 | Loss: 0.00002544
Iteration 7/1000 | Loss: 0.00002477
Iteration 8/1000 | Loss: 0.00002404
Iteration 9/1000 | Loss: 0.00002369
Iteration 10/1000 | Loss: 0.00002345
Iteration 11/1000 | Loss: 0.00002335
Iteration 12/1000 | Loss: 0.00002324
Iteration 13/1000 | Loss: 0.00002319
Iteration 14/1000 | Loss: 0.00002313
Iteration 15/1000 | Loss: 0.00002313
Iteration 16/1000 | Loss: 0.00002312
Iteration 17/1000 | Loss: 0.00002310
Iteration 18/1000 | Loss: 0.00002310
Iteration 19/1000 | Loss: 0.00002303
Iteration 20/1000 | Loss: 0.00002303
Iteration 21/1000 | Loss: 0.00002303
Iteration 22/1000 | Loss: 0.00002303
Iteration 23/1000 | Loss: 0.00002303
Iteration 24/1000 | Loss: 0.00002302
Iteration 25/1000 | Loss: 0.00002302
Iteration 26/1000 | Loss: 0.00002301
Iteration 27/1000 | Loss: 0.00002301
Iteration 28/1000 | Loss: 0.00002301
Iteration 29/1000 | Loss: 0.00002301
Iteration 30/1000 | Loss: 0.00002300
Iteration 31/1000 | Loss: 0.00002300
Iteration 32/1000 | Loss: 0.00002300
Iteration 33/1000 | Loss: 0.00002299
Iteration 34/1000 | Loss: 0.00002299
Iteration 35/1000 | Loss: 0.00002299
Iteration 36/1000 | Loss: 0.00002298
Iteration 37/1000 | Loss: 0.00002297
Iteration 38/1000 | Loss: 0.00002296
Iteration 39/1000 | Loss: 0.00002296
Iteration 40/1000 | Loss: 0.00002296
Iteration 41/1000 | Loss: 0.00002295
Iteration 42/1000 | Loss: 0.00002295
Iteration 43/1000 | Loss: 0.00002295
Iteration 44/1000 | Loss: 0.00002295
Iteration 45/1000 | Loss: 0.00002294
Iteration 46/1000 | Loss: 0.00002294
Iteration 47/1000 | Loss: 0.00002294
Iteration 48/1000 | Loss: 0.00002293
Iteration 49/1000 | Loss: 0.00002293
Iteration 50/1000 | Loss: 0.00002292
Iteration 51/1000 | Loss: 0.00002292
Iteration 52/1000 | Loss: 0.00002291
Iteration 53/1000 | Loss: 0.00002291
Iteration 54/1000 | Loss: 0.00002291
Iteration 55/1000 | Loss: 0.00002291
Iteration 56/1000 | Loss: 0.00002291
Iteration 57/1000 | Loss: 0.00002291
Iteration 58/1000 | Loss: 0.00002291
Iteration 59/1000 | Loss: 0.00002291
Iteration 60/1000 | Loss: 0.00002291
Iteration 61/1000 | Loss: 0.00002291
Iteration 62/1000 | Loss: 0.00002291
Iteration 63/1000 | Loss: 0.00002290
Iteration 64/1000 | Loss: 0.00002290
Iteration 65/1000 | Loss: 0.00002290
Iteration 66/1000 | Loss: 0.00002290
Iteration 67/1000 | Loss: 0.00002290
Iteration 68/1000 | Loss: 0.00002289
Iteration 69/1000 | Loss: 0.00002289
Iteration 70/1000 | Loss: 0.00002289
Iteration 71/1000 | Loss: 0.00002289
Iteration 72/1000 | Loss: 0.00002289
Iteration 73/1000 | Loss: 0.00002288
Iteration 74/1000 | Loss: 0.00002287
Iteration 75/1000 | Loss: 0.00002286
Iteration 76/1000 | Loss: 0.00002286
Iteration 77/1000 | Loss: 0.00002286
Iteration 78/1000 | Loss: 0.00002286
Iteration 79/1000 | Loss: 0.00002285
Iteration 80/1000 | Loss: 0.00002285
Iteration 81/1000 | Loss: 0.00002285
Iteration 82/1000 | Loss: 0.00002285
Iteration 83/1000 | Loss: 0.00002285
Iteration 84/1000 | Loss: 0.00002285
Iteration 85/1000 | Loss: 0.00002285
Iteration 86/1000 | Loss: 0.00002285
Iteration 87/1000 | Loss: 0.00002285
Iteration 88/1000 | Loss: 0.00002284
Iteration 89/1000 | Loss: 0.00002284
Iteration 90/1000 | Loss: 0.00002284
Iteration 91/1000 | Loss: 0.00002283
Iteration 92/1000 | Loss: 0.00002282
Iteration 93/1000 | Loss: 0.00002282
Iteration 94/1000 | Loss: 0.00002281
Iteration 95/1000 | Loss: 0.00002281
Iteration 96/1000 | Loss: 0.00002281
Iteration 97/1000 | Loss: 0.00002281
Iteration 98/1000 | Loss: 0.00002280
Iteration 99/1000 | Loss: 0.00002280
Iteration 100/1000 | Loss: 0.00002280
Iteration 101/1000 | Loss: 0.00002280
Iteration 102/1000 | Loss: 0.00002280
Iteration 103/1000 | Loss: 0.00002279
Iteration 104/1000 | Loss: 0.00002279
Iteration 105/1000 | Loss: 0.00002279
Iteration 106/1000 | Loss: 0.00002278
Iteration 107/1000 | Loss: 0.00002278
Iteration 108/1000 | Loss: 0.00002278
Iteration 109/1000 | Loss: 0.00002277
Iteration 110/1000 | Loss: 0.00002277
Iteration 111/1000 | Loss: 0.00002277
Iteration 112/1000 | Loss: 0.00002277
Iteration 113/1000 | Loss: 0.00002277
Iteration 114/1000 | Loss: 0.00002277
Iteration 115/1000 | Loss: 0.00002277
Iteration 116/1000 | Loss: 0.00002277
Iteration 117/1000 | Loss: 0.00002277
Iteration 118/1000 | Loss: 0.00002277
Iteration 119/1000 | Loss: 0.00002277
Iteration 120/1000 | Loss: 0.00002277
Iteration 121/1000 | Loss: 0.00002277
Iteration 122/1000 | Loss: 0.00002277
Iteration 123/1000 | Loss: 0.00002277
Iteration 124/1000 | Loss: 0.00002277
Iteration 125/1000 | Loss: 0.00002276
Iteration 126/1000 | Loss: 0.00002276
Iteration 127/1000 | Loss: 0.00002276
Iteration 128/1000 | Loss: 0.00002275
Iteration 129/1000 | Loss: 0.00002275
Iteration 130/1000 | Loss: 0.00002275
Iteration 131/1000 | Loss: 0.00002275
Iteration 132/1000 | Loss: 0.00002275
Iteration 133/1000 | Loss: 0.00002275
Iteration 134/1000 | Loss: 0.00002275
Iteration 135/1000 | Loss: 0.00002275
Iteration 136/1000 | Loss: 0.00002275
Iteration 137/1000 | Loss: 0.00002274
Iteration 138/1000 | Loss: 0.00002274
Iteration 139/1000 | Loss: 0.00002274
Iteration 140/1000 | Loss: 0.00002274
Iteration 141/1000 | Loss: 0.00002274
Iteration 142/1000 | Loss: 0.00002274
Iteration 143/1000 | Loss: 0.00002274
Iteration 144/1000 | Loss: 0.00002274
Iteration 145/1000 | Loss: 0.00002274
Iteration 146/1000 | Loss: 0.00002274
Iteration 147/1000 | Loss: 0.00002274
Iteration 148/1000 | Loss: 0.00002273
Iteration 149/1000 | Loss: 0.00002273
Iteration 150/1000 | Loss: 0.00002273
Iteration 151/1000 | Loss: 0.00002273
Iteration 152/1000 | Loss: 0.00002273
Iteration 153/1000 | Loss: 0.00002273
Iteration 154/1000 | Loss: 0.00002273
Iteration 155/1000 | Loss: 0.00002273
Iteration 156/1000 | Loss: 0.00002273
Iteration 157/1000 | Loss: 0.00002273
Iteration 158/1000 | Loss: 0.00002273
Iteration 159/1000 | Loss: 0.00002273
Iteration 160/1000 | Loss: 0.00002272
Iteration 161/1000 | Loss: 0.00002272
Iteration 162/1000 | Loss: 0.00002272
Iteration 163/1000 | Loss: 0.00002272
Iteration 164/1000 | Loss: 0.00002271
Iteration 165/1000 | Loss: 0.00002271
Iteration 166/1000 | Loss: 0.00002271
Iteration 167/1000 | Loss: 0.00002271
Iteration 168/1000 | Loss: 0.00002271
Iteration 169/1000 | Loss: 0.00002271
Iteration 170/1000 | Loss: 0.00002271
Iteration 171/1000 | Loss: 0.00002271
Iteration 172/1000 | Loss: 0.00002270
Iteration 173/1000 | Loss: 0.00002270
Iteration 174/1000 | Loss: 0.00002270
Iteration 175/1000 | Loss: 0.00002270
Iteration 176/1000 | Loss: 0.00002270
Iteration 177/1000 | Loss: 0.00002270
Iteration 178/1000 | Loss: 0.00002269
Iteration 179/1000 | Loss: 0.00002269
Iteration 180/1000 | Loss: 0.00002269
Iteration 181/1000 | Loss: 0.00002269
Iteration 182/1000 | Loss: 0.00002269
Iteration 183/1000 | Loss: 0.00002269
Iteration 184/1000 | Loss: 0.00002268
Iteration 185/1000 | Loss: 0.00002268
Iteration 186/1000 | Loss: 0.00002268
Iteration 187/1000 | Loss: 0.00002268
Iteration 188/1000 | Loss: 0.00002268
Iteration 189/1000 | Loss: 0.00002268
Iteration 190/1000 | Loss: 0.00002268
Iteration 191/1000 | Loss: 0.00002267
Iteration 192/1000 | Loss: 0.00002267
Iteration 193/1000 | Loss: 0.00002267
Iteration 194/1000 | Loss: 0.00002267
Iteration 195/1000 | Loss: 0.00002267
Iteration 196/1000 | Loss: 0.00002267
Iteration 197/1000 | Loss: 0.00002267
Iteration 198/1000 | Loss: 0.00002267
Iteration 199/1000 | Loss: 0.00002267
Iteration 200/1000 | Loss: 0.00002267
Iteration 201/1000 | Loss: 0.00002267
Iteration 202/1000 | Loss: 0.00002267
Iteration 203/1000 | Loss: 0.00002267
Iteration 204/1000 | Loss: 0.00002266
Iteration 205/1000 | Loss: 0.00002266
Iteration 206/1000 | Loss: 0.00002266
Iteration 207/1000 | Loss: 0.00002266
Iteration 208/1000 | Loss: 0.00002266
Iteration 209/1000 | Loss: 0.00002266
Iteration 210/1000 | Loss: 0.00002266
Iteration 211/1000 | Loss: 0.00002266
Iteration 212/1000 | Loss: 0.00002266
Iteration 213/1000 | Loss: 0.00002266
Iteration 214/1000 | Loss: 0.00002266
Iteration 215/1000 | Loss: 0.00002266
Iteration 216/1000 | Loss: 0.00002265
Iteration 217/1000 | Loss: 0.00002265
Iteration 218/1000 | Loss: 0.00002265
Iteration 219/1000 | Loss: 0.00002265
Iteration 220/1000 | Loss: 0.00002265
Iteration 221/1000 | Loss: 0.00002265
Iteration 222/1000 | Loss: 0.00002264
Iteration 223/1000 | Loss: 0.00002264
Iteration 224/1000 | Loss: 0.00002264
Iteration 225/1000 | Loss: 0.00002264
Iteration 226/1000 | Loss: 0.00002264
Iteration 227/1000 | Loss: 0.00002264
Iteration 228/1000 | Loss: 0.00002264
Iteration 229/1000 | Loss: 0.00002264
Iteration 230/1000 | Loss: 0.00002264
Iteration 231/1000 | Loss: 0.00002264
Iteration 232/1000 | Loss: 0.00002264
Iteration 233/1000 | Loss: 0.00002264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.264163049403578e-05, 2.264163049403578e-05, 2.264163049403578e-05, 2.264163049403578e-05, 2.264163049403578e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.264163049403578e-05

Optimization complete. Final v2v error: 4.088088035583496 mm

Highest mean error: 4.366727352142334 mm for frame 236

Lowest mean error: 3.310262441635132 mm for frame 23

Saving results

Total time: 49.622554779052734
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945492
Iteration 2/25 | Loss: 0.00130912
Iteration 3/25 | Loss: 0.00117328
Iteration 4/25 | Loss: 0.00115614
Iteration 5/25 | Loss: 0.00115086
Iteration 6/25 | Loss: 0.00114963
Iteration 7/25 | Loss: 0.00114959
Iteration 8/25 | Loss: 0.00114959
Iteration 9/25 | Loss: 0.00114959
Iteration 10/25 | Loss: 0.00114959
Iteration 11/25 | Loss: 0.00114959
Iteration 12/25 | Loss: 0.00114959
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011495911749079823, 0.0011495911749079823, 0.0011495911749079823, 0.0011495911749079823, 0.0011495911749079823]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011495911749079823

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.77947509
Iteration 2/25 | Loss: 0.00496364
Iteration 3/25 | Loss: 0.00496361
Iteration 4/25 | Loss: 0.00496361
Iteration 5/25 | Loss: 0.00496361
Iteration 6/25 | Loss: 0.00496361
Iteration 7/25 | Loss: 0.00496361
Iteration 8/25 | Loss: 0.00496361
Iteration 9/25 | Loss: 0.00496361
Iteration 10/25 | Loss: 0.00496361
Iteration 11/25 | Loss: 0.00496361
Iteration 12/25 | Loss: 0.00496361
Iteration 13/25 | Loss: 0.00496361
Iteration 14/25 | Loss: 0.00496361
Iteration 15/25 | Loss: 0.00496361
Iteration 16/25 | Loss: 0.00496361
Iteration 17/25 | Loss: 0.00496361
Iteration 18/25 | Loss: 0.00496361
Iteration 19/25 | Loss: 0.00496361
Iteration 20/25 | Loss: 0.00496361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00496360519900918, 0.00496360519900918, 0.00496360519900918, 0.00496360519900918, 0.00496360519900918]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00496360519900918

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00496361
Iteration 2/1000 | Loss: 0.00002743
Iteration 3/1000 | Loss: 0.00002222
Iteration 4/1000 | Loss: 0.00001869
Iteration 5/1000 | Loss: 0.00001769
Iteration 6/1000 | Loss: 0.00001665
Iteration 7/1000 | Loss: 0.00001607
Iteration 8/1000 | Loss: 0.00001545
Iteration 9/1000 | Loss: 0.00001524
Iteration 10/1000 | Loss: 0.00001509
Iteration 11/1000 | Loss: 0.00001509
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001500
Iteration 15/1000 | Loss: 0.00001494
Iteration 16/1000 | Loss: 0.00001493
Iteration 17/1000 | Loss: 0.00001491
Iteration 18/1000 | Loss: 0.00001489
Iteration 19/1000 | Loss: 0.00001488
Iteration 20/1000 | Loss: 0.00001487
Iteration 21/1000 | Loss: 0.00001487
Iteration 22/1000 | Loss: 0.00001486
Iteration 23/1000 | Loss: 0.00001486
Iteration 24/1000 | Loss: 0.00001485
Iteration 25/1000 | Loss: 0.00001482
Iteration 26/1000 | Loss: 0.00001482
Iteration 27/1000 | Loss: 0.00001479
Iteration 28/1000 | Loss: 0.00001475
Iteration 29/1000 | Loss: 0.00001475
Iteration 30/1000 | Loss: 0.00001474
Iteration 31/1000 | Loss: 0.00001474
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001472
Iteration 35/1000 | Loss: 0.00001471
Iteration 36/1000 | Loss: 0.00001471
Iteration 37/1000 | Loss: 0.00001470
Iteration 38/1000 | Loss: 0.00001470
Iteration 39/1000 | Loss: 0.00001470
Iteration 40/1000 | Loss: 0.00001469
Iteration 41/1000 | Loss: 0.00001469
Iteration 42/1000 | Loss: 0.00001469
Iteration 43/1000 | Loss: 0.00001469
Iteration 44/1000 | Loss: 0.00001468
Iteration 45/1000 | Loss: 0.00001468
Iteration 46/1000 | Loss: 0.00001468
Iteration 47/1000 | Loss: 0.00001467
Iteration 48/1000 | Loss: 0.00001467
Iteration 49/1000 | Loss: 0.00001467
Iteration 50/1000 | Loss: 0.00001467
Iteration 51/1000 | Loss: 0.00001466
Iteration 52/1000 | Loss: 0.00001466
Iteration 53/1000 | Loss: 0.00001466
Iteration 54/1000 | Loss: 0.00001466
Iteration 55/1000 | Loss: 0.00001465
Iteration 56/1000 | Loss: 0.00001465
Iteration 57/1000 | Loss: 0.00001465
Iteration 58/1000 | Loss: 0.00001464
Iteration 59/1000 | Loss: 0.00001464
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001463
Iteration 62/1000 | Loss: 0.00001463
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001462
Iteration 66/1000 | Loss: 0.00001462
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001461
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001459
Iteration 76/1000 | Loss: 0.00001459
Iteration 77/1000 | Loss: 0.00001459
Iteration 78/1000 | Loss: 0.00001459
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001458
Iteration 90/1000 | Loss: 0.00001458
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001458
Iteration 102/1000 | Loss: 0.00001458
Iteration 103/1000 | Loss: 0.00001458
Iteration 104/1000 | Loss: 0.00001458
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001458
Iteration 109/1000 | Loss: 0.00001458
Iteration 110/1000 | Loss: 0.00001458
Iteration 111/1000 | Loss: 0.00001458
Iteration 112/1000 | Loss: 0.00001458
Iteration 113/1000 | Loss: 0.00001458
Iteration 114/1000 | Loss: 0.00001458
Iteration 115/1000 | Loss: 0.00001458
Iteration 116/1000 | Loss: 0.00001458
Iteration 117/1000 | Loss: 0.00001458
Iteration 118/1000 | Loss: 0.00001458
Iteration 119/1000 | Loss: 0.00001458
Iteration 120/1000 | Loss: 0.00001458
Iteration 121/1000 | Loss: 0.00001458
Iteration 122/1000 | Loss: 0.00001458
Iteration 123/1000 | Loss: 0.00001458
Iteration 124/1000 | Loss: 0.00001458
Iteration 125/1000 | Loss: 0.00001458
Iteration 126/1000 | Loss: 0.00001458
Iteration 127/1000 | Loss: 0.00001458
Iteration 128/1000 | Loss: 0.00001458
Iteration 129/1000 | Loss: 0.00001458
Iteration 130/1000 | Loss: 0.00001458
Iteration 131/1000 | Loss: 0.00001458
Iteration 132/1000 | Loss: 0.00001458
Iteration 133/1000 | Loss: 0.00001458
Iteration 134/1000 | Loss: 0.00001458
Iteration 135/1000 | Loss: 0.00001458
Iteration 136/1000 | Loss: 0.00001458
Iteration 137/1000 | Loss: 0.00001458
Iteration 138/1000 | Loss: 0.00001458
Iteration 139/1000 | Loss: 0.00001458
Iteration 140/1000 | Loss: 0.00001458
Iteration 141/1000 | Loss: 0.00001458
Iteration 142/1000 | Loss: 0.00001458
Iteration 143/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.4576140529243276e-05, 1.4576140529243276e-05, 1.4576140529243276e-05, 1.4576140529243276e-05, 1.4576140529243276e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4576140529243276e-05

Optimization complete. Final v2v error: 3.356353759765625 mm

Highest mean error: 3.6030666828155518 mm for frame 25

Lowest mean error: 3.169468641281128 mm for frame 132

Saving results

Total time: 31.805022478103638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882757
Iteration 2/25 | Loss: 0.00141678
Iteration 3/25 | Loss: 0.00120085
Iteration 4/25 | Loss: 0.00118241
Iteration 5/25 | Loss: 0.00117801
Iteration 6/25 | Loss: 0.00117718
Iteration 7/25 | Loss: 0.00117718
Iteration 8/25 | Loss: 0.00117718
Iteration 9/25 | Loss: 0.00117718
Iteration 10/25 | Loss: 0.00117718
Iteration 11/25 | Loss: 0.00117718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00117717613466084, 0.00117717613466084, 0.00117717613466084, 0.00117717613466084, 0.00117717613466084]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00117717613466084

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74466181
Iteration 2/25 | Loss: 0.00556804
Iteration 3/25 | Loss: 0.00556804
Iteration 4/25 | Loss: 0.00556803
Iteration 5/25 | Loss: 0.00556803
Iteration 6/25 | Loss: 0.00556803
Iteration 7/25 | Loss: 0.00556803
Iteration 8/25 | Loss: 0.00556803
Iteration 9/25 | Loss: 0.00556803
Iteration 10/25 | Loss: 0.00556803
Iteration 11/25 | Loss: 0.00556803
Iteration 12/25 | Loss: 0.00556803
Iteration 13/25 | Loss: 0.00556803
Iteration 14/25 | Loss: 0.00556803
Iteration 15/25 | Loss: 0.00556803
Iteration 16/25 | Loss: 0.00556803
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0055680302903056145, 0.0055680302903056145, 0.0055680302903056145, 0.0055680302903056145, 0.0055680302903056145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0055680302903056145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00556803
Iteration 2/1000 | Loss: 0.00003415
Iteration 3/1000 | Loss: 0.00002633
Iteration 4/1000 | Loss: 0.00002383
Iteration 5/1000 | Loss: 0.00002248
Iteration 6/1000 | Loss: 0.00002143
Iteration 7/1000 | Loss: 0.00002053
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001955
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001926
Iteration 12/1000 | Loss: 0.00001907
Iteration 13/1000 | Loss: 0.00001897
Iteration 14/1000 | Loss: 0.00001891
Iteration 15/1000 | Loss: 0.00001884
Iteration 16/1000 | Loss: 0.00001884
Iteration 17/1000 | Loss: 0.00001882
Iteration 18/1000 | Loss: 0.00001882
Iteration 19/1000 | Loss: 0.00001881
Iteration 20/1000 | Loss: 0.00001881
Iteration 21/1000 | Loss: 0.00001881
Iteration 22/1000 | Loss: 0.00001880
Iteration 23/1000 | Loss: 0.00001880
Iteration 24/1000 | Loss: 0.00001879
Iteration 25/1000 | Loss: 0.00001879
Iteration 26/1000 | Loss: 0.00001878
Iteration 27/1000 | Loss: 0.00001878
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001877
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001877
Iteration 32/1000 | Loss: 0.00001876
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001876
Iteration 35/1000 | Loss: 0.00001875
Iteration 36/1000 | Loss: 0.00001875
Iteration 37/1000 | Loss: 0.00001875
Iteration 38/1000 | Loss: 0.00001874
Iteration 39/1000 | Loss: 0.00001874
Iteration 40/1000 | Loss: 0.00001874
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001873
Iteration 43/1000 | Loss: 0.00001873
Iteration 44/1000 | Loss: 0.00001873
Iteration 45/1000 | Loss: 0.00001872
Iteration 46/1000 | Loss: 0.00001872
Iteration 47/1000 | Loss: 0.00001872
Iteration 48/1000 | Loss: 0.00001872
Iteration 49/1000 | Loss: 0.00001872
Iteration 50/1000 | Loss: 0.00001872
Iteration 51/1000 | Loss: 0.00001872
Iteration 52/1000 | Loss: 0.00001871
Iteration 53/1000 | Loss: 0.00001871
Iteration 54/1000 | Loss: 0.00001871
Iteration 55/1000 | Loss: 0.00001871
Iteration 56/1000 | Loss: 0.00001871
Iteration 57/1000 | Loss: 0.00001871
Iteration 58/1000 | Loss: 0.00001871
Iteration 59/1000 | Loss: 0.00001871
Iteration 60/1000 | Loss: 0.00001870
Iteration 61/1000 | Loss: 0.00001870
Iteration 62/1000 | Loss: 0.00001870
Iteration 63/1000 | Loss: 0.00001869
Iteration 64/1000 | Loss: 0.00001869
Iteration 65/1000 | Loss: 0.00001869
Iteration 66/1000 | Loss: 0.00001869
Iteration 67/1000 | Loss: 0.00001869
Iteration 68/1000 | Loss: 0.00001868
Iteration 69/1000 | Loss: 0.00001868
Iteration 70/1000 | Loss: 0.00001868
Iteration 71/1000 | Loss: 0.00001868
Iteration 72/1000 | Loss: 0.00001868
Iteration 73/1000 | Loss: 0.00001868
Iteration 74/1000 | Loss: 0.00001868
Iteration 75/1000 | Loss: 0.00001868
Iteration 76/1000 | Loss: 0.00001868
Iteration 77/1000 | Loss: 0.00001868
Iteration 78/1000 | Loss: 0.00001867
Iteration 79/1000 | Loss: 0.00001867
Iteration 80/1000 | Loss: 0.00001867
Iteration 81/1000 | Loss: 0.00001867
Iteration 82/1000 | Loss: 0.00001867
Iteration 83/1000 | Loss: 0.00001867
Iteration 84/1000 | Loss: 0.00001867
Iteration 85/1000 | Loss: 0.00001867
Iteration 86/1000 | Loss: 0.00001867
Iteration 87/1000 | Loss: 0.00001867
Iteration 88/1000 | Loss: 0.00001867
Iteration 89/1000 | Loss: 0.00001867
Iteration 90/1000 | Loss: 0.00001867
Iteration 91/1000 | Loss: 0.00001867
Iteration 92/1000 | Loss: 0.00001867
Iteration 93/1000 | Loss: 0.00001867
Iteration 94/1000 | Loss: 0.00001867
Iteration 95/1000 | Loss: 0.00001867
Iteration 96/1000 | Loss: 0.00001867
Iteration 97/1000 | Loss: 0.00001867
Iteration 98/1000 | Loss: 0.00001867
Iteration 99/1000 | Loss: 0.00001867
Iteration 100/1000 | Loss: 0.00001867
Iteration 101/1000 | Loss: 0.00001867
Iteration 102/1000 | Loss: 0.00001867
Iteration 103/1000 | Loss: 0.00001867
Iteration 104/1000 | Loss: 0.00001867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [1.8673146769287996e-05, 1.8673146769287996e-05, 1.8673146769287996e-05, 1.8673146769287996e-05, 1.8673146769287996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8673146769287996e-05

Optimization complete. Final v2v error: 3.6556341648101807 mm

Highest mean error: 4.830386638641357 mm for frame 202

Lowest mean error: 3.164694309234619 mm for frame 161

Saving results

Total time: 36.15975546836853
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036390
Iteration 2/25 | Loss: 0.00146187
Iteration 3/25 | Loss: 0.00129992
Iteration 4/25 | Loss: 0.00125673
Iteration 5/25 | Loss: 0.00124906
Iteration 6/25 | Loss: 0.00124807
Iteration 7/25 | Loss: 0.00124807
Iteration 8/25 | Loss: 0.00124807
Iteration 9/25 | Loss: 0.00124807
Iteration 10/25 | Loss: 0.00124807
Iteration 11/25 | Loss: 0.00124807
Iteration 12/25 | Loss: 0.00124807
Iteration 13/25 | Loss: 0.00124807
Iteration 14/25 | Loss: 0.00124807
Iteration 15/25 | Loss: 0.00124807
Iteration 16/25 | Loss: 0.00124807
Iteration 17/25 | Loss: 0.00124807
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012480695731937885, 0.0012480695731937885, 0.0012480695731937885, 0.0012480695731937885, 0.0012480695731937885]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012480695731937885

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00685930
Iteration 2/25 | Loss: 0.00706311
Iteration 3/25 | Loss: 0.00706311
Iteration 4/25 | Loss: 0.00706310
Iteration 5/25 | Loss: 0.00706310
Iteration 6/25 | Loss: 0.00706310
Iteration 7/25 | Loss: 0.00706310
Iteration 8/25 | Loss: 0.00706310
Iteration 9/25 | Loss: 0.00706310
Iteration 10/25 | Loss: 0.00706310
Iteration 11/25 | Loss: 0.00706310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.007063102908432484, 0.007063102908432484, 0.007063102908432484, 0.007063102908432484, 0.007063102908432484]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.007063102908432484

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00706310
Iteration 2/1000 | Loss: 0.00005388
Iteration 3/1000 | Loss: 0.00003235
Iteration 4/1000 | Loss: 0.00002797
Iteration 5/1000 | Loss: 0.00002578
Iteration 6/1000 | Loss: 0.00002478
Iteration 7/1000 | Loss: 0.00002367
Iteration 8/1000 | Loss: 0.00002302
Iteration 9/1000 | Loss: 0.00002255
Iteration 10/1000 | Loss: 0.00002226
Iteration 11/1000 | Loss: 0.00002209
Iteration 12/1000 | Loss: 0.00002205
Iteration 13/1000 | Loss: 0.00002204
Iteration 14/1000 | Loss: 0.00002204
Iteration 15/1000 | Loss: 0.00002203
Iteration 16/1000 | Loss: 0.00002202
Iteration 17/1000 | Loss: 0.00002202
Iteration 18/1000 | Loss: 0.00002202
Iteration 19/1000 | Loss: 0.00002199
Iteration 20/1000 | Loss: 0.00002197
Iteration 21/1000 | Loss: 0.00002196
Iteration 22/1000 | Loss: 0.00002195
Iteration 23/1000 | Loss: 0.00002185
Iteration 24/1000 | Loss: 0.00002176
Iteration 25/1000 | Loss: 0.00002163
Iteration 26/1000 | Loss: 0.00002162
Iteration 27/1000 | Loss: 0.00002156
Iteration 28/1000 | Loss: 0.00002151
Iteration 29/1000 | Loss: 0.00002150
Iteration 30/1000 | Loss: 0.00002144
Iteration 31/1000 | Loss: 0.00002142
Iteration 32/1000 | Loss: 0.00002139
Iteration 33/1000 | Loss: 0.00002138
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00002137
Iteration 36/1000 | Loss: 0.00002137
Iteration 37/1000 | Loss: 0.00002137
Iteration 38/1000 | Loss: 0.00002136
Iteration 39/1000 | Loss: 0.00002135
Iteration 40/1000 | Loss: 0.00002134
Iteration 41/1000 | Loss: 0.00002133
Iteration 42/1000 | Loss: 0.00002133
Iteration 43/1000 | Loss: 0.00002133
Iteration 44/1000 | Loss: 0.00002133
Iteration 45/1000 | Loss: 0.00002133
Iteration 46/1000 | Loss: 0.00002133
Iteration 47/1000 | Loss: 0.00002133
Iteration 48/1000 | Loss: 0.00002133
Iteration 49/1000 | Loss: 0.00002132
Iteration 50/1000 | Loss: 0.00002132
Iteration 51/1000 | Loss: 0.00002132
Iteration 52/1000 | Loss: 0.00002132
Iteration 53/1000 | Loss: 0.00002132
Iteration 54/1000 | Loss: 0.00002131
Iteration 55/1000 | Loss: 0.00002130
Iteration 56/1000 | Loss: 0.00002130
Iteration 57/1000 | Loss: 0.00002130
Iteration 58/1000 | Loss: 0.00002129
Iteration 59/1000 | Loss: 0.00002129
Iteration 60/1000 | Loss: 0.00002129
Iteration 61/1000 | Loss: 0.00002129
Iteration 62/1000 | Loss: 0.00002128
Iteration 63/1000 | Loss: 0.00002128
Iteration 64/1000 | Loss: 0.00002128
Iteration 65/1000 | Loss: 0.00002128
Iteration 66/1000 | Loss: 0.00002127
Iteration 67/1000 | Loss: 0.00002127
Iteration 68/1000 | Loss: 0.00002126
Iteration 69/1000 | Loss: 0.00002126
Iteration 70/1000 | Loss: 0.00002126
Iteration 71/1000 | Loss: 0.00002125
Iteration 72/1000 | Loss: 0.00002125
Iteration 73/1000 | Loss: 0.00002125
Iteration 74/1000 | Loss: 0.00002125
Iteration 75/1000 | Loss: 0.00002125
Iteration 76/1000 | Loss: 0.00002125
Iteration 77/1000 | Loss: 0.00002125
Iteration 78/1000 | Loss: 0.00002124
Iteration 79/1000 | Loss: 0.00002124
Iteration 80/1000 | Loss: 0.00002123
Iteration 81/1000 | Loss: 0.00002123
Iteration 82/1000 | Loss: 0.00002123
Iteration 83/1000 | Loss: 0.00002123
Iteration 84/1000 | Loss: 0.00002122
Iteration 85/1000 | Loss: 0.00002122
Iteration 86/1000 | Loss: 0.00002122
Iteration 87/1000 | Loss: 0.00002122
Iteration 88/1000 | Loss: 0.00002121
Iteration 89/1000 | Loss: 0.00002121
Iteration 90/1000 | Loss: 0.00002121
Iteration 91/1000 | Loss: 0.00002120
Iteration 92/1000 | Loss: 0.00002120
Iteration 93/1000 | Loss: 0.00002120
Iteration 94/1000 | Loss: 0.00002119
Iteration 95/1000 | Loss: 0.00002119
Iteration 96/1000 | Loss: 0.00002119
Iteration 97/1000 | Loss: 0.00002119
Iteration 98/1000 | Loss: 0.00002119
Iteration 99/1000 | Loss: 0.00002119
Iteration 100/1000 | Loss: 0.00002118
Iteration 101/1000 | Loss: 0.00002118
Iteration 102/1000 | Loss: 0.00002118
Iteration 103/1000 | Loss: 0.00002118
Iteration 104/1000 | Loss: 0.00002118
Iteration 105/1000 | Loss: 0.00002118
Iteration 106/1000 | Loss: 0.00002118
Iteration 107/1000 | Loss: 0.00002117
Iteration 108/1000 | Loss: 0.00002117
Iteration 109/1000 | Loss: 0.00002117
Iteration 110/1000 | Loss: 0.00002117
Iteration 111/1000 | Loss: 0.00002117
Iteration 112/1000 | Loss: 0.00002116
Iteration 113/1000 | Loss: 0.00002116
Iteration 114/1000 | Loss: 0.00002116
Iteration 115/1000 | Loss: 0.00002116
Iteration 116/1000 | Loss: 0.00002116
Iteration 117/1000 | Loss: 0.00002116
Iteration 118/1000 | Loss: 0.00002116
Iteration 119/1000 | Loss: 0.00002116
Iteration 120/1000 | Loss: 0.00002116
Iteration 121/1000 | Loss: 0.00002116
Iteration 122/1000 | Loss: 0.00002116
Iteration 123/1000 | Loss: 0.00002115
Iteration 124/1000 | Loss: 0.00002115
Iteration 125/1000 | Loss: 0.00002115
Iteration 126/1000 | Loss: 0.00002115
Iteration 127/1000 | Loss: 0.00002115
Iteration 128/1000 | Loss: 0.00002115
Iteration 129/1000 | Loss: 0.00002115
Iteration 130/1000 | Loss: 0.00002115
Iteration 131/1000 | Loss: 0.00002115
Iteration 132/1000 | Loss: 0.00002115
Iteration 133/1000 | Loss: 0.00002115
Iteration 134/1000 | Loss: 0.00002115
Iteration 135/1000 | Loss: 0.00002115
Iteration 136/1000 | Loss: 0.00002115
Iteration 137/1000 | Loss: 0.00002115
Iteration 138/1000 | Loss: 0.00002115
Iteration 139/1000 | Loss: 0.00002115
Iteration 140/1000 | Loss: 0.00002115
Iteration 141/1000 | Loss: 0.00002115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.1152154658921063e-05, 2.1152154658921063e-05, 2.1152154658921063e-05, 2.1152154658921063e-05, 2.1152154658921063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1152154658921063e-05

Optimization complete. Final v2v error: 3.935126543045044 mm

Highest mean error: 4.413643836975098 mm for frame 5

Lowest mean error: 3.658186197280884 mm for frame 92

Saving results

Total time: 39.16417169570923
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01153779
Iteration 2/25 | Loss: 0.01153779
Iteration 3/25 | Loss: 0.01153778
Iteration 4/25 | Loss: 0.01153778
Iteration 5/25 | Loss: 0.01153778
Iteration 6/25 | Loss: 0.01153777
Iteration 7/25 | Loss: 0.01153777
Iteration 8/25 | Loss: 0.01153777
Iteration 9/25 | Loss: 0.01153777
Iteration 10/25 | Loss: 0.01153777
Iteration 11/25 | Loss: 0.01153777
Iteration 12/25 | Loss: 0.01153777
Iteration 13/25 | Loss: 0.01153776
Iteration 14/25 | Loss: 0.01153776
Iteration 15/25 | Loss: 0.01153776
Iteration 16/25 | Loss: 0.01153776
Iteration 17/25 | Loss: 0.01153775
Iteration 18/25 | Loss: 0.01153775
Iteration 19/25 | Loss: 0.01153775
Iteration 20/25 | Loss: 0.01153775
Iteration 21/25 | Loss: 0.01153774
Iteration 22/25 | Loss: 0.01153774
Iteration 23/25 | Loss: 0.01153774
Iteration 24/25 | Loss: 0.01153773
Iteration 25/25 | Loss: 0.01153773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.98344529
Iteration 2/25 | Loss: 0.12694462
Iteration 3/25 | Loss: 0.12692980
Iteration 4/25 | Loss: 0.12684874
Iteration 5/25 | Loss: 0.12684874
Iteration 6/25 | Loss: 0.12684873
Iteration 7/25 | Loss: 0.12684871
Iteration 8/25 | Loss: 0.12684871
Iteration 9/25 | Loss: 0.12684870
Iteration 10/25 | Loss: 0.12684870
Iteration 11/25 | Loss: 0.12684870
Iteration 12/25 | Loss: 0.12684870
Iteration 13/25 | Loss: 0.12684870
Iteration 14/25 | Loss: 0.12684870
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.12684869766235352, 0.12684869766235352, 0.12684869766235352, 0.12684869766235352, 0.12684869766235352]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.12684869766235352

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.12684870
Iteration 2/1000 | Loss: 0.00301167
Iteration 3/1000 | Loss: 0.00096042
Iteration 4/1000 | Loss: 0.00065369
Iteration 5/1000 | Loss: 0.00134628
Iteration 6/1000 | Loss: 0.00012569
Iteration 7/1000 | Loss: 0.00153538
Iteration 8/1000 | Loss: 0.00871702
Iteration 9/1000 | Loss: 0.00064252
Iteration 10/1000 | Loss: 0.00153775
Iteration 11/1000 | Loss: 0.00041262
Iteration 12/1000 | Loss: 0.00120084
Iteration 13/1000 | Loss: 0.00062999
Iteration 14/1000 | Loss: 0.00088389
Iteration 15/1000 | Loss: 0.00029674
Iteration 16/1000 | Loss: 0.00010840
Iteration 17/1000 | Loss: 0.00008739
Iteration 18/1000 | Loss: 0.00004586
Iteration 19/1000 | Loss: 0.00018102
Iteration 20/1000 | Loss: 0.00003173
Iteration 21/1000 | Loss: 0.00005456
Iteration 22/1000 | Loss: 0.00002883
Iteration 23/1000 | Loss: 0.00017111
Iteration 24/1000 | Loss: 0.00002691
Iteration 25/1000 | Loss: 0.00004321
Iteration 26/1000 | Loss: 0.00006980
Iteration 27/1000 | Loss: 0.00002512
Iteration 28/1000 | Loss: 0.00004274
Iteration 29/1000 | Loss: 0.00003298
Iteration 30/1000 | Loss: 0.00030127
Iteration 31/1000 | Loss: 0.00003394
Iteration 32/1000 | Loss: 0.00004659
Iteration 33/1000 | Loss: 0.00003119
Iteration 34/1000 | Loss: 0.00002293
Iteration 35/1000 | Loss: 0.00002240
Iteration 36/1000 | Loss: 0.00004831
Iteration 37/1000 | Loss: 0.00002951
Iteration 38/1000 | Loss: 0.00002359
Iteration 39/1000 | Loss: 0.00002263
Iteration 40/1000 | Loss: 0.00002115
Iteration 41/1000 | Loss: 0.00002611
Iteration 42/1000 | Loss: 0.00002088
Iteration 43/1000 | Loss: 0.00002087
Iteration 44/1000 | Loss: 0.00002327
Iteration 45/1000 | Loss: 0.00002105
Iteration 46/1000 | Loss: 0.00002086
Iteration 47/1000 | Loss: 0.00002086
Iteration 48/1000 | Loss: 0.00002086
Iteration 49/1000 | Loss: 0.00002091
Iteration 50/1000 | Loss: 0.00002085
Iteration 51/1000 | Loss: 0.00002085
Iteration 52/1000 | Loss: 0.00002085
Iteration 53/1000 | Loss: 0.00002084
Iteration 54/1000 | Loss: 0.00002081
Iteration 55/1000 | Loss: 0.00002081
Iteration 56/1000 | Loss: 0.00002081
Iteration 57/1000 | Loss: 0.00002081
Iteration 58/1000 | Loss: 0.00002080
Iteration 59/1000 | Loss: 0.00002080
Iteration 60/1000 | Loss: 0.00002080
Iteration 61/1000 | Loss: 0.00002078
Iteration 62/1000 | Loss: 0.00002077
Iteration 63/1000 | Loss: 0.00002077
Iteration 64/1000 | Loss: 0.00002076
Iteration 65/1000 | Loss: 0.00002076
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002075
Iteration 68/1000 | Loss: 0.00002074
Iteration 69/1000 | Loss: 0.00002074
Iteration 70/1000 | Loss: 0.00002073
Iteration 71/1000 | Loss: 0.00002073
Iteration 72/1000 | Loss: 0.00002072
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002072
Iteration 75/1000 | Loss: 0.00002072
Iteration 76/1000 | Loss: 0.00002071
Iteration 77/1000 | Loss: 0.00002071
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002071
Iteration 83/1000 | Loss: 0.00002071
Iteration 84/1000 | Loss: 0.00002071
Iteration 85/1000 | Loss: 0.00002071
Iteration 86/1000 | Loss: 0.00002071
Iteration 87/1000 | Loss: 0.00002071
Iteration 88/1000 | Loss: 0.00002070
Iteration 89/1000 | Loss: 0.00002070
Iteration 90/1000 | Loss: 0.00002070
Iteration 91/1000 | Loss: 0.00002069
Iteration 92/1000 | Loss: 0.00002069
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002068
Iteration 97/1000 | Loss: 0.00002068
Iteration 98/1000 | Loss: 0.00002068
Iteration 99/1000 | Loss: 0.00002068
Iteration 100/1000 | Loss: 0.00002068
Iteration 101/1000 | Loss: 0.00002068
Iteration 102/1000 | Loss: 0.00002067
Iteration 103/1000 | Loss: 0.00002067
Iteration 104/1000 | Loss: 0.00002067
Iteration 105/1000 | Loss: 0.00002067
Iteration 106/1000 | Loss: 0.00002066
Iteration 107/1000 | Loss: 0.00002066
Iteration 108/1000 | Loss: 0.00002066
Iteration 109/1000 | Loss: 0.00002066
Iteration 110/1000 | Loss: 0.00002066
Iteration 111/1000 | Loss: 0.00002066
Iteration 112/1000 | Loss: 0.00002065
Iteration 113/1000 | Loss: 0.00002065
Iteration 114/1000 | Loss: 0.00002065
Iteration 115/1000 | Loss: 0.00002065
Iteration 116/1000 | Loss: 0.00002065
Iteration 117/1000 | Loss: 0.00002065
Iteration 118/1000 | Loss: 0.00002065
Iteration 119/1000 | Loss: 0.00002064
Iteration 120/1000 | Loss: 0.00002064
Iteration 121/1000 | Loss: 0.00002064
Iteration 122/1000 | Loss: 0.00002064
Iteration 123/1000 | Loss: 0.00002064
Iteration 124/1000 | Loss: 0.00002064
Iteration 125/1000 | Loss: 0.00002064
Iteration 126/1000 | Loss: 0.00002064
Iteration 127/1000 | Loss: 0.00002064
Iteration 128/1000 | Loss: 0.00002063
Iteration 129/1000 | Loss: 0.00002063
Iteration 130/1000 | Loss: 0.00002063
Iteration 131/1000 | Loss: 0.00002063
Iteration 132/1000 | Loss: 0.00002063
Iteration 133/1000 | Loss: 0.00002063
Iteration 134/1000 | Loss: 0.00002063
Iteration 135/1000 | Loss: 0.00002063
Iteration 136/1000 | Loss: 0.00002062
Iteration 137/1000 | Loss: 0.00002062
Iteration 138/1000 | Loss: 0.00002062
Iteration 139/1000 | Loss: 0.00002062
Iteration 140/1000 | Loss: 0.00002062
Iteration 141/1000 | Loss: 0.00002061
Iteration 142/1000 | Loss: 0.00002061
Iteration 143/1000 | Loss: 0.00002061
Iteration 144/1000 | Loss: 0.00002061
Iteration 145/1000 | Loss: 0.00002061
Iteration 146/1000 | Loss: 0.00002061
Iteration 147/1000 | Loss: 0.00002061
Iteration 148/1000 | Loss: 0.00002061
Iteration 149/1000 | Loss: 0.00002061
Iteration 150/1000 | Loss: 0.00002061
Iteration 151/1000 | Loss: 0.00002061
Iteration 152/1000 | Loss: 0.00002061
Iteration 153/1000 | Loss: 0.00002061
Iteration 154/1000 | Loss: 0.00002060
Iteration 155/1000 | Loss: 0.00002060
Iteration 156/1000 | Loss: 0.00002060
Iteration 157/1000 | Loss: 0.00002060
Iteration 158/1000 | Loss: 0.00002060
Iteration 159/1000 | Loss: 0.00002060
Iteration 160/1000 | Loss: 0.00002060
Iteration 161/1000 | Loss: 0.00002060
Iteration 162/1000 | Loss: 0.00002060
Iteration 163/1000 | Loss: 0.00002060
Iteration 164/1000 | Loss: 0.00002060
Iteration 165/1000 | Loss: 0.00002060
Iteration 166/1000 | Loss: 0.00002060
Iteration 167/1000 | Loss: 0.00002060
Iteration 168/1000 | Loss: 0.00002060
Iteration 169/1000 | Loss: 0.00002060
Iteration 170/1000 | Loss: 0.00002060
Iteration 171/1000 | Loss: 0.00002060
Iteration 172/1000 | Loss: 0.00002060
Iteration 173/1000 | Loss: 0.00002060
Iteration 174/1000 | Loss: 0.00002060
Iteration 175/1000 | Loss: 0.00002060
Iteration 176/1000 | Loss: 0.00002060
Iteration 177/1000 | Loss: 0.00002060
Iteration 178/1000 | Loss: 0.00002060
Iteration 179/1000 | Loss: 0.00002060
Iteration 180/1000 | Loss: 0.00002060
Iteration 181/1000 | Loss: 0.00002060
Iteration 182/1000 | Loss: 0.00002060
Iteration 183/1000 | Loss: 0.00002060
Iteration 184/1000 | Loss: 0.00002060
Iteration 185/1000 | Loss: 0.00002060
Iteration 186/1000 | Loss: 0.00002060
Iteration 187/1000 | Loss: 0.00002060
Iteration 188/1000 | Loss: 0.00002060
Iteration 189/1000 | Loss: 0.00002060
Iteration 190/1000 | Loss: 0.00002060
Iteration 191/1000 | Loss: 0.00002060
Iteration 192/1000 | Loss: 0.00002060
Iteration 193/1000 | Loss: 0.00002060
Iteration 194/1000 | Loss: 0.00002060
Iteration 195/1000 | Loss: 0.00002060
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [2.059728831227403e-05, 2.059728831227403e-05, 2.059728831227403e-05, 2.059728831227403e-05, 2.059728831227403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.059728831227403e-05

Optimization complete. Final v2v error: 3.909559965133667 mm

Highest mean error: 4.4655561447143555 mm for frame 3

Lowest mean error: 3.546050548553467 mm for frame 38

Saving results

Total time: 87.91320013999939
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00498110
Iteration 2/25 | Loss: 0.00124543
Iteration 3/25 | Loss: 0.00115554
Iteration 4/25 | Loss: 0.00114051
Iteration 5/25 | Loss: 0.00113543
Iteration 6/25 | Loss: 0.00113396
Iteration 7/25 | Loss: 0.00113373
Iteration 8/25 | Loss: 0.00113373
Iteration 9/25 | Loss: 0.00113373
Iteration 10/25 | Loss: 0.00113373
Iteration 11/25 | Loss: 0.00113373
Iteration 12/25 | Loss: 0.00113373
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011337302858009934, 0.0011337302858009934, 0.0011337302858009934, 0.0011337302858009934, 0.0011337302858009934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011337302858009934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91926122
Iteration 2/25 | Loss: 0.00496709
Iteration 3/25 | Loss: 0.00496708
Iteration 4/25 | Loss: 0.00496708
Iteration 5/25 | Loss: 0.00496708
Iteration 6/25 | Loss: 0.00496708
Iteration 7/25 | Loss: 0.00496708
Iteration 8/25 | Loss: 0.00496708
Iteration 9/25 | Loss: 0.00496708
Iteration 10/25 | Loss: 0.00496708
Iteration 11/25 | Loss: 0.00496708
Iteration 12/25 | Loss: 0.00496708
Iteration 13/25 | Loss: 0.00496708
Iteration 14/25 | Loss: 0.00496708
Iteration 15/25 | Loss: 0.00496708
Iteration 16/25 | Loss: 0.00496708
Iteration 17/25 | Loss: 0.00496708
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004967078100889921, 0.004967078100889921, 0.004967078100889921, 0.004967078100889921, 0.004967078100889921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004967078100889921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00496708
Iteration 2/1000 | Loss: 0.00002733
Iteration 3/1000 | Loss: 0.00002111
Iteration 4/1000 | Loss: 0.00001971
Iteration 5/1000 | Loss: 0.00001877
Iteration 6/1000 | Loss: 0.00001820
Iteration 7/1000 | Loss: 0.00001765
Iteration 8/1000 | Loss: 0.00001737
Iteration 9/1000 | Loss: 0.00001724
Iteration 10/1000 | Loss: 0.00001717
Iteration 11/1000 | Loss: 0.00001714
Iteration 12/1000 | Loss: 0.00001713
Iteration 13/1000 | Loss: 0.00001712
Iteration 14/1000 | Loss: 0.00001712
Iteration 15/1000 | Loss: 0.00001712
Iteration 16/1000 | Loss: 0.00001712
Iteration 17/1000 | Loss: 0.00001711
Iteration 18/1000 | Loss: 0.00001711
Iteration 19/1000 | Loss: 0.00001710
Iteration 20/1000 | Loss: 0.00001709
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00001709
Iteration 23/1000 | Loss: 0.00001708
Iteration 24/1000 | Loss: 0.00001707
Iteration 25/1000 | Loss: 0.00001706
Iteration 26/1000 | Loss: 0.00001704
Iteration 27/1000 | Loss: 0.00001704
Iteration 28/1000 | Loss: 0.00001703
Iteration 29/1000 | Loss: 0.00001703
Iteration 30/1000 | Loss: 0.00001702
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001702
Iteration 33/1000 | Loss: 0.00001701
Iteration 34/1000 | Loss: 0.00001701
Iteration 35/1000 | Loss: 0.00001701
Iteration 36/1000 | Loss: 0.00001701
Iteration 37/1000 | Loss: 0.00001700
Iteration 38/1000 | Loss: 0.00001700
Iteration 39/1000 | Loss: 0.00001699
Iteration 40/1000 | Loss: 0.00001699
Iteration 41/1000 | Loss: 0.00001699
Iteration 42/1000 | Loss: 0.00001699
Iteration 43/1000 | Loss: 0.00001699
Iteration 44/1000 | Loss: 0.00001699
Iteration 45/1000 | Loss: 0.00001699
Iteration 46/1000 | Loss: 0.00001699
Iteration 47/1000 | Loss: 0.00001699
Iteration 48/1000 | Loss: 0.00001698
Iteration 49/1000 | Loss: 0.00001698
Iteration 50/1000 | Loss: 0.00001698
Iteration 51/1000 | Loss: 0.00001697
Iteration 52/1000 | Loss: 0.00001697
Iteration 53/1000 | Loss: 0.00001697
Iteration 54/1000 | Loss: 0.00001696
Iteration 55/1000 | Loss: 0.00001696
Iteration 56/1000 | Loss: 0.00001696
Iteration 57/1000 | Loss: 0.00001696
Iteration 58/1000 | Loss: 0.00001696
Iteration 59/1000 | Loss: 0.00001695
Iteration 60/1000 | Loss: 0.00001695
Iteration 61/1000 | Loss: 0.00001695
Iteration 62/1000 | Loss: 0.00001694
Iteration 63/1000 | Loss: 0.00001694
Iteration 64/1000 | Loss: 0.00001694
Iteration 65/1000 | Loss: 0.00001694
Iteration 66/1000 | Loss: 0.00001694
Iteration 67/1000 | Loss: 0.00001694
Iteration 68/1000 | Loss: 0.00001694
Iteration 69/1000 | Loss: 0.00001694
Iteration 70/1000 | Loss: 0.00001694
Iteration 71/1000 | Loss: 0.00001693
Iteration 72/1000 | Loss: 0.00001693
Iteration 73/1000 | Loss: 0.00001693
Iteration 74/1000 | Loss: 0.00001692
Iteration 75/1000 | Loss: 0.00001692
Iteration 76/1000 | Loss: 0.00001692
Iteration 77/1000 | Loss: 0.00001692
Iteration 78/1000 | Loss: 0.00001691
Iteration 79/1000 | Loss: 0.00001691
Iteration 80/1000 | Loss: 0.00001691
Iteration 81/1000 | Loss: 0.00001690
Iteration 82/1000 | Loss: 0.00001690
Iteration 83/1000 | Loss: 0.00001689
Iteration 84/1000 | Loss: 0.00001689
Iteration 85/1000 | Loss: 0.00001689
Iteration 86/1000 | Loss: 0.00001689
Iteration 87/1000 | Loss: 0.00001688
Iteration 88/1000 | Loss: 0.00001688
Iteration 89/1000 | Loss: 0.00001688
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001688
Iteration 93/1000 | Loss: 0.00001687
Iteration 94/1000 | Loss: 0.00001687
Iteration 95/1000 | Loss: 0.00001687
Iteration 96/1000 | Loss: 0.00001687
Iteration 97/1000 | Loss: 0.00001687
Iteration 98/1000 | Loss: 0.00001687
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001687
Iteration 101/1000 | Loss: 0.00001687
Iteration 102/1000 | Loss: 0.00001687
Iteration 103/1000 | Loss: 0.00001687
Iteration 104/1000 | Loss: 0.00001687
Iteration 105/1000 | Loss: 0.00001687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.6873960703378543e-05, 1.6873960703378543e-05, 1.6873960703378543e-05, 1.6873960703378543e-05, 1.6873960703378543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6873960703378543e-05

Optimization complete. Final v2v error: 3.479750633239746 mm

Highest mean error: 3.9333133697509766 mm for frame 1

Lowest mean error: 3.07293438911438 mm for frame 77

Saving results

Total time: 29.25405979156494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01157078
Iteration 2/25 | Loss: 0.01157078
Iteration 3/25 | Loss: 0.00292822
Iteration 4/25 | Loss: 0.00231765
Iteration 5/25 | Loss: 0.00222706
Iteration 6/25 | Loss: 0.00217418
Iteration 7/25 | Loss: 0.00213538
Iteration 8/25 | Loss: 0.00211282
Iteration 9/25 | Loss: 0.00209245
Iteration 10/25 | Loss: 0.00208353
Iteration 11/25 | Loss: 0.00207904
Iteration 12/25 | Loss: 0.00207904
Iteration 13/25 | Loss: 0.00207904
Iteration 14/25 | Loss: 0.00207904
Iteration 15/25 | Loss: 0.00207904
Iteration 16/25 | Loss: 0.00207904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0020790384151041508, 0.0020790384151041508, 0.0020790384151041508, 0.0020790384151041508, 0.0020790384151041508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020790384151041508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62577581
Iteration 2/25 | Loss: 0.01839035
Iteration 3/25 | Loss: 0.01839033
Iteration 4/25 | Loss: 0.01839033
Iteration 5/25 | Loss: 0.01839033
Iteration 6/25 | Loss: 0.01839032
Iteration 7/25 | Loss: 0.01839032
Iteration 8/25 | Loss: 0.01839032
Iteration 9/25 | Loss: 0.01839032
Iteration 10/25 | Loss: 0.01839032
Iteration 11/25 | Loss: 0.01839032
Iteration 12/25 | Loss: 0.01839032
Iteration 13/25 | Loss: 0.01839032
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.018390322104096413, 0.018390322104096413, 0.018390322104096413, 0.018390322104096413, 0.018390322104096413]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.018390322104096413

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.01839032
Iteration 2/1000 | Loss: 0.00210365
Iteration 3/1000 | Loss: 0.00946084
Iteration 4/1000 | Loss: 0.00098854
Iteration 5/1000 | Loss: 0.00082276
Iteration 6/1000 | Loss: 0.00075451
Iteration 7/1000 | Loss: 0.00057425
Iteration 8/1000 | Loss: 0.00092690
Iteration 9/1000 | Loss: 0.00026685
Iteration 10/1000 | Loss: 0.00018801
Iteration 11/1000 | Loss: 0.00012123
Iteration 12/1000 | Loss: 0.00008929
Iteration 13/1000 | Loss: 0.00006362
Iteration 14/1000 | Loss: 0.00004821
Iteration 15/1000 | Loss: 0.00003862
Iteration 16/1000 | Loss: 0.00003320
Iteration 17/1000 | Loss: 0.00002895
Iteration 18/1000 | Loss: 0.00002579
Iteration 19/1000 | Loss: 0.00002327
Iteration 20/1000 | Loss: 0.00002152
Iteration 21/1000 | Loss: 0.00002048
Iteration 22/1000 | Loss: 0.00075583
Iteration 23/1000 | Loss: 0.00002574
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001807
Iteration 26/1000 | Loss: 0.00001666
Iteration 27/1000 | Loss: 0.00001583
Iteration 28/1000 | Loss: 0.00001532
Iteration 29/1000 | Loss: 0.00001511
Iteration 30/1000 | Loss: 0.00001498
Iteration 31/1000 | Loss: 0.00001494
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001492
Iteration 34/1000 | Loss: 0.00001491
Iteration 35/1000 | Loss: 0.00001488
Iteration 36/1000 | Loss: 0.00001479
Iteration 37/1000 | Loss: 0.00001477
Iteration 38/1000 | Loss: 0.00001474
Iteration 39/1000 | Loss: 0.00001474
Iteration 40/1000 | Loss: 0.00001473
Iteration 41/1000 | Loss: 0.00001468
Iteration 42/1000 | Loss: 0.00001468
Iteration 43/1000 | Loss: 0.00001467
Iteration 44/1000 | Loss: 0.00001466
Iteration 45/1000 | Loss: 0.00001466
Iteration 46/1000 | Loss: 0.00001466
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001463
Iteration 49/1000 | Loss: 0.00001463
Iteration 50/1000 | Loss: 0.00001462
Iteration 51/1000 | Loss: 0.00001462
Iteration 52/1000 | Loss: 0.00001462
Iteration 53/1000 | Loss: 0.00001462
Iteration 54/1000 | Loss: 0.00001461
Iteration 55/1000 | Loss: 0.00001461
Iteration 56/1000 | Loss: 0.00001460
Iteration 57/1000 | Loss: 0.00001460
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001459
Iteration 60/1000 | Loss: 0.00001459
Iteration 61/1000 | Loss: 0.00001459
Iteration 62/1000 | Loss: 0.00001459
Iteration 63/1000 | Loss: 0.00001459
Iteration 64/1000 | Loss: 0.00001459
Iteration 65/1000 | Loss: 0.00001459
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001458
Iteration 69/1000 | Loss: 0.00001457
Iteration 70/1000 | Loss: 0.00001457
Iteration 71/1000 | Loss: 0.00001457
Iteration 72/1000 | Loss: 0.00001457
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001456
Iteration 75/1000 | Loss: 0.00001456
Iteration 76/1000 | Loss: 0.00001456
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001455
Iteration 79/1000 | Loss: 0.00001455
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001454
Iteration 82/1000 | Loss: 0.00001454
Iteration 83/1000 | Loss: 0.00001453
Iteration 84/1000 | Loss: 0.00001453
Iteration 85/1000 | Loss: 0.00001453
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001452
Iteration 88/1000 | Loss: 0.00001452
Iteration 89/1000 | Loss: 0.00001452
Iteration 90/1000 | Loss: 0.00001452
Iteration 91/1000 | Loss: 0.00001452
Iteration 92/1000 | Loss: 0.00001450
Iteration 93/1000 | Loss: 0.00001449
Iteration 94/1000 | Loss: 0.00001449
Iteration 95/1000 | Loss: 0.00001449
Iteration 96/1000 | Loss: 0.00001449
Iteration 97/1000 | Loss: 0.00001449
Iteration 98/1000 | Loss: 0.00001449
Iteration 99/1000 | Loss: 0.00001449
Iteration 100/1000 | Loss: 0.00001449
Iteration 101/1000 | Loss: 0.00001449
Iteration 102/1000 | Loss: 0.00001449
Iteration 103/1000 | Loss: 0.00001449
Iteration 104/1000 | Loss: 0.00001449
Iteration 105/1000 | Loss: 0.00001448
Iteration 106/1000 | Loss: 0.00001448
Iteration 107/1000 | Loss: 0.00001448
Iteration 108/1000 | Loss: 0.00001448
Iteration 109/1000 | Loss: 0.00001448
Iteration 110/1000 | Loss: 0.00001448
Iteration 111/1000 | Loss: 0.00001448
Iteration 112/1000 | Loss: 0.00001447
Iteration 113/1000 | Loss: 0.00001447
Iteration 114/1000 | Loss: 0.00001447
Iteration 115/1000 | Loss: 0.00001447
Iteration 116/1000 | Loss: 0.00001446
Iteration 117/1000 | Loss: 0.00001446
Iteration 118/1000 | Loss: 0.00001446
Iteration 119/1000 | Loss: 0.00001446
Iteration 120/1000 | Loss: 0.00001445
Iteration 121/1000 | Loss: 0.00001445
Iteration 122/1000 | Loss: 0.00001445
Iteration 123/1000 | Loss: 0.00001445
Iteration 124/1000 | Loss: 0.00001445
Iteration 125/1000 | Loss: 0.00001445
Iteration 126/1000 | Loss: 0.00001445
Iteration 127/1000 | Loss: 0.00001445
Iteration 128/1000 | Loss: 0.00001445
Iteration 129/1000 | Loss: 0.00001444
Iteration 130/1000 | Loss: 0.00001444
Iteration 131/1000 | Loss: 0.00001444
Iteration 132/1000 | Loss: 0.00001444
Iteration 133/1000 | Loss: 0.00001444
Iteration 134/1000 | Loss: 0.00001444
Iteration 135/1000 | Loss: 0.00001444
Iteration 136/1000 | Loss: 0.00001444
Iteration 137/1000 | Loss: 0.00001444
Iteration 138/1000 | Loss: 0.00001444
Iteration 139/1000 | Loss: 0.00001443
Iteration 140/1000 | Loss: 0.00001443
Iteration 141/1000 | Loss: 0.00001443
Iteration 142/1000 | Loss: 0.00001443
Iteration 143/1000 | Loss: 0.00001443
Iteration 144/1000 | Loss: 0.00001443
Iteration 145/1000 | Loss: 0.00001443
Iteration 146/1000 | Loss: 0.00001443
Iteration 147/1000 | Loss: 0.00001443
Iteration 148/1000 | Loss: 0.00001443
Iteration 149/1000 | Loss: 0.00001443
Iteration 150/1000 | Loss: 0.00001443
Iteration 151/1000 | Loss: 0.00001443
Iteration 152/1000 | Loss: 0.00001443
Iteration 153/1000 | Loss: 0.00001443
Iteration 154/1000 | Loss: 0.00001443
Iteration 155/1000 | Loss: 0.00001443
Iteration 156/1000 | Loss: 0.00001443
Iteration 157/1000 | Loss: 0.00001443
Iteration 158/1000 | Loss: 0.00001443
Iteration 159/1000 | Loss: 0.00001443
Iteration 160/1000 | Loss: 0.00001443
Iteration 161/1000 | Loss: 0.00001443
Iteration 162/1000 | Loss: 0.00001443
Iteration 163/1000 | Loss: 0.00001443
Iteration 164/1000 | Loss: 0.00001443
Iteration 165/1000 | Loss: 0.00001443
Iteration 166/1000 | Loss: 0.00001443
Iteration 167/1000 | Loss: 0.00001443
Iteration 168/1000 | Loss: 0.00001443
Iteration 169/1000 | Loss: 0.00001443
Iteration 170/1000 | Loss: 0.00001443
Iteration 171/1000 | Loss: 0.00001443
Iteration 172/1000 | Loss: 0.00001443
Iteration 173/1000 | Loss: 0.00001443
Iteration 174/1000 | Loss: 0.00001443
Iteration 175/1000 | Loss: 0.00001443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.4432812349696178e-05, 1.4432812349696178e-05, 1.4432812349696178e-05, 1.4432812349696178e-05, 1.4432812349696178e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4432812349696178e-05

Optimization complete. Final v2v error: 3.2272112369537354 mm

Highest mean error: 4.218092441558838 mm for frame 52

Lowest mean error: 3.045799493789673 mm for frame 141

Saving results

Total time: 80.68712854385376
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00434672
Iteration 2/25 | Loss: 0.00130889
Iteration 3/25 | Loss: 0.00116147
Iteration 4/25 | Loss: 0.00114186
Iteration 5/25 | Loss: 0.00113591
Iteration 6/25 | Loss: 0.00113465
Iteration 7/25 | Loss: 0.00113436
Iteration 8/25 | Loss: 0.00113436
Iteration 9/25 | Loss: 0.00113436
Iteration 10/25 | Loss: 0.00113436
Iteration 11/25 | Loss: 0.00113436
Iteration 12/25 | Loss: 0.00113436
Iteration 13/25 | Loss: 0.00113436
Iteration 14/25 | Loss: 0.00113436
Iteration 15/25 | Loss: 0.00113436
Iteration 16/25 | Loss: 0.00113436
Iteration 17/25 | Loss: 0.00113436
Iteration 18/25 | Loss: 0.00113436
Iteration 19/25 | Loss: 0.00113436
Iteration 20/25 | Loss: 0.00113436
Iteration 21/25 | Loss: 0.00113436
Iteration 22/25 | Loss: 0.00113436
Iteration 23/25 | Loss: 0.00113436
Iteration 24/25 | Loss: 0.00113436
Iteration 25/25 | Loss: 0.00113436

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81266105
Iteration 2/25 | Loss: 0.00533337
Iteration 3/25 | Loss: 0.00533336
Iteration 4/25 | Loss: 0.00533336
Iteration 5/25 | Loss: 0.00533336
Iteration 6/25 | Loss: 0.00533336
Iteration 7/25 | Loss: 0.00533336
Iteration 8/25 | Loss: 0.00533336
Iteration 9/25 | Loss: 0.00533336
Iteration 10/25 | Loss: 0.00533336
Iteration 11/25 | Loss: 0.00533336
Iteration 12/25 | Loss: 0.00533336
Iteration 13/25 | Loss: 0.00533336
Iteration 14/25 | Loss: 0.00533336
Iteration 15/25 | Loss: 0.00533336
Iteration 16/25 | Loss: 0.00533336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.005333358887583017, 0.005333358887583017, 0.005333358887583017, 0.005333358887583017, 0.005333358887583017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005333358887583017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00533336
Iteration 2/1000 | Loss: 0.00002956
Iteration 3/1000 | Loss: 0.00002180
Iteration 4/1000 | Loss: 0.00001837
Iteration 5/1000 | Loss: 0.00001702
Iteration 6/1000 | Loss: 0.00001616
Iteration 7/1000 | Loss: 0.00001565
Iteration 8/1000 | Loss: 0.00001520
Iteration 9/1000 | Loss: 0.00001496
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001485
Iteration 12/1000 | Loss: 0.00001482
Iteration 13/1000 | Loss: 0.00001481
Iteration 14/1000 | Loss: 0.00001479
Iteration 15/1000 | Loss: 0.00001479
Iteration 16/1000 | Loss: 0.00001478
Iteration 17/1000 | Loss: 0.00001478
Iteration 18/1000 | Loss: 0.00001476
Iteration 19/1000 | Loss: 0.00001475
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001473
Iteration 22/1000 | Loss: 0.00001473
Iteration 23/1000 | Loss: 0.00001473
Iteration 24/1000 | Loss: 0.00001472
Iteration 25/1000 | Loss: 0.00001471
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001465
Iteration 30/1000 | Loss: 0.00001465
Iteration 31/1000 | Loss: 0.00001464
Iteration 32/1000 | Loss: 0.00001461
Iteration 33/1000 | Loss: 0.00001460
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001458
Iteration 37/1000 | Loss: 0.00001457
Iteration 38/1000 | Loss: 0.00001456
Iteration 39/1000 | Loss: 0.00001455
Iteration 40/1000 | Loss: 0.00001454
Iteration 41/1000 | Loss: 0.00001453
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001451
Iteration 44/1000 | Loss: 0.00001450
Iteration 45/1000 | Loss: 0.00001450
Iteration 46/1000 | Loss: 0.00001449
Iteration 47/1000 | Loss: 0.00001449
Iteration 48/1000 | Loss: 0.00001449
Iteration 49/1000 | Loss: 0.00001448
Iteration 50/1000 | Loss: 0.00001448
Iteration 51/1000 | Loss: 0.00001447
Iteration 52/1000 | Loss: 0.00001447
Iteration 53/1000 | Loss: 0.00001447
Iteration 54/1000 | Loss: 0.00001446
Iteration 55/1000 | Loss: 0.00001445
Iteration 56/1000 | Loss: 0.00001445
Iteration 57/1000 | Loss: 0.00001445
Iteration 58/1000 | Loss: 0.00001445
Iteration 59/1000 | Loss: 0.00001444
Iteration 60/1000 | Loss: 0.00001444
Iteration 61/1000 | Loss: 0.00001443
Iteration 62/1000 | Loss: 0.00001443
Iteration 63/1000 | Loss: 0.00001443
Iteration 64/1000 | Loss: 0.00001442
Iteration 65/1000 | Loss: 0.00001442
Iteration 66/1000 | Loss: 0.00001442
Iteration 67/1000 | Loss: 0.00001442
Iteration 68/1000 | Loss: 0.00001442
Iteration 69/1000 | Loss: 0.00001442
Iteration 70/1000 | Loss: 0.00001442
Iteration 71/1000 | Loss: 0.00001441
Iteration 72/1000 | Loss: 0.00001441
Iteration 73/1000 | Loss: 0.00001441
Iteration 74/1000 | Loss: 0.00001441
Iteration 75/1000 | Loss: 0.00001440
Iteration 76/1000 | Loss: 0.00001440
Iteration 77/1000 | Loss: 0.00001440
Iteration 78/1000 | Loss: 0.00001440
Iteration 79/1000 | Loss: 0.00001440
Iteration 80/1000 | Loss: 0.00001439
Iteration 81/1000 | Loss: 0.00001439
Iteration 82/1000 | Loss: 0.00001439
Iteration 83/1000 | Loss: 0.00001438
Iteration 84/1000 | Loss: 0.00001438
Iteration 85/1000 | Loss: 0.00001438
Iteration 86/1000 | Loss: 0.00001437
Iteration 87/1000 | Loss: 0.00001437
Iteration 88/1000 | Loss: 0.00001437
Iteration 89/1000 | Loss: 0.00001437
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001436
Iteration 93/1000 | Loss: 0.00001436
Iteration 94/1000 | Loss: 0.00001436
Iteration 95/1000 | Loss: 0.00001436
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Iteration 107/1000 | Loss: 0.00001434
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001433
Iteration 115/1000 | Loss: 0.00001433
Iteration 116/1000 | Loss: 0.00001433
Iteration 117/1000 | Loss: 0.00001433
Iteration 118/1000 | Loss: 0.00001433
Iteration 119/1000 | Loss: 0.00001433
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001432
Iteration 126/1000 | Loss: 0.00001432
Iteration 127/1000 | Loss: 0.00001431
Iteration 128/1000 | Loss: 0.00001431
Iteration 129/1000 | Loss: 0.00001431
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001431
Iteration 132/1000 | Loss: 0.00001430
Iteration 133/1000 | Loss: 0.00001430
Iteration 134/1000 | Loss: 0.00001430
Iteration 135/1000 | Loss: 0.00001430
Iteration 136/1000 | Loss: 0.00001430
Iteration 137/1000 | Loss: 0.00001430
Iteration 138/1000 | Loss: 0.00001430
Iteration 139/1000 | Loss: 0.00001430
Iteration 140/1000 | Loss: 0.00001430
Iteration 141/1000 | Loss: 0.00001430
Iteration 142/1000 | Loss: 0.00001430
Iteration 143/1000 | Loss: 0.00001430
Iteration 144/1000 | Loss: 0.00001429
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001429
Iteration 147/1000 | Loss: 0.00001429
Iteration 148/1000 | Loss: 0.00001429
Iteration 149/1000 | Loss: 0.00001429
Iteration 150/1000 | Loss: 0.00001429
Iteration 151/1000 | Loss: 0.00001429
Iteration 152/1000 | Loss: 0.00001429
Iteration 153/1000 | Loss: 0.00001429
Iteration 154/1000 | Loss: 0.00001429
Iteration 155/1000 | Loss: 0.00001429
Iteration 156/1000 | Loss: 0.00001429
Iteration 157/1000 | Loss: 0.00001429
Iteration 158/1000 | Loss: 0.00001429
Iteration 159/1000 | Loss: 0.00001429
Iteration 160/1000 | Loss: 0.00001429
Iteration 161/1000 | Loss: 0.00001429
Iteration 162/1000 | Loss: 0.00001429
Iteration 163/1000 | Loss: 0.00001429
Iteration 164/1000 | Loss: 0.00001429
Iteration 165/1000 | Loss: 0.00001429
Iteration 166/1000 | Loss: 0.00001429
Iteration 167/1000 | Loss: 0.00001429
Iteration 168/1000 | Loss: 0.00001429
Iteration 169/1000 | Loss: 0.00001429
Iteration 170/1000 | Loss: 0.00001429
Iteration 171/1000 | Loss: 0.00001429
Iteration 172/1000 | Loss: 0.00001429
Iteration 173/1000 | Loss: 0.00001429
Iteration 174/1000 | Loss: 0.00001429
Iteration 175/1000 | Loss: 0.00001429
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.4292393643700052e-05, 1.4292393643700052e-05, 1.4292393643700052e-05, 1.4292393643700052e-05, 1.4292393643700052e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4292393643700052e-05

Optimization complete. Final v2v error: 3.209702491760254 mm

Highest mean error: 3.761547803878784 mm for frame 123

Lowest mean error: 2.9531843662261963 mm for frame 11

Saving results

Total time: 37.04163336753845
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00880760
Iteration 2/25 | Loss: 0.00127528
Iteration 3/25 | Loss: 0.00115067
Iteration 4/25 | Loss: 0.00113454
Iteration 5/25 | Loss: 0.00112779
Iteration 6/25 | Loss: 0.00112655
Iteration 7/25 | Loss: 0.00112638
Iteration 8/25 | Loss: 0.00112638
Iteration 9/25 | Loss: 0.00112638
Iteration 10/25 | Loss: 0.00112638
Iteration 11/25 | Loss: 0.00112638
Iteration 12/25 | Loss: 0.00112638
Iteration 13/25 | Loss: 0.00112638
Iteration 14/25 | Loss: 0.00112638
Iteration 15/25 | Loss: 0.00112638
Iteration 16/25 | Loss: 0.00112638
Iteration 17/25 | Loss: 0.00112638
Iteration 18/25 | Loss: 0.00112638
Iteration 19/25 | Loss: 0.00112638
Iteration 20/25 | Loss: 0.00112638
Iteration 21/25 | Loss: 0.00112638
Iteration 22/25 | Loss: 0.00112638
Iteration 23/25 | Loss: 0.00112638
Iteration 24/25 | Loss: 0.00112638
Iteration 25/25 | Loss: 0.00112638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001126377726905048, 0.001126377726905048, 0.001126377726905048, 0.001126377726905048, 0.001126377726905048]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001126377726905048

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.93121278
Iteration 2/25 | Loss: 0.00517301
Iteration 3/25 | Loss: 0.00517300
Iteration 4/25 | Loss: 0.00517300
Iteration 5/25 | Loss: 0.00517300
Iteration 6/25 | Loss: 0.00517300
Iteration 7/25 | Loss: 0.00517300
Iteration 8/25 | Loss: 0.00517300
Iteration 9/25 | Loss: 0.00517300
Iteration 10/25 | Loss: 0.00517300
Iteration 11/25 | Loss: 0.00517300
Iteration 12/25 | Loss: 0.00517300
Iteration 13/25 | Loss: 0.00517300
Iteration 14/25 | Loss: 0.00517300
Iteration 15/25 | Loss: 0.00517300
Iteration 16/25 | Loss: 0.00517300
Iteration 17/25 | Loss: 0.00517300
Iteration 18/25 | Loss: 0.00517300
Iteration 19/25 | Loss: 0.00517300
Iteration 20/25 | Loss: 0.00517300
Iteration 21/25 | Loss: 0.00517300
Iteration 22/25 | Loss: 0.00517300
Iteration 23/25 | Loss: 0.00517300
Iteration 24/25 | Loss: 0.00517300
Iteration 25/25 | Loss: 0.00517300

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00517300
Iteration 2/1000 | Loss: 0.00003063
Iteration 3/1000 | Loss: 0.00002411
Iteration 4/1000 | Loss: 0.00002218
Iteration 5/1000 | Loss: 0.00002130
Iteration 6/1000 | Loss: 0.00002040
Iteration 7/1000 | Loss: 0.00001982
Iteration 8/1000 | Loss: 0.00001943
Iteration 9/1000 | Loss: 0.00001929
Iteration 10/1000 | Loss: 0.00001919
Iteration 11/1000 | Loss: 0.00001915
Iteration 12/1000 | Loss: 0.00001914
Iteration 13/1000 | Loss: 0.00001914
Iteration 14/1000 | Loss: 0.00001913
Iteration 15/1000 | Loss: 0.00001912
Iteration 16/1000 | Loss: 0.00001911
Iteration 17/1000 | Loss: 0.00001910
Iteration 18/1000 | Loss: 0.00001910
Iteration 19/1000 | Loss: 0.00001910
Iteration 20/1000 | Loss: 0.00001909
Iteration 21/1000 | Loss: 0.00001906
Iteration 22/1000 | Loss: 0.00001906
Iteration 23/1000 | Loss: 0.00001906
Iteration 24/1000 | Loss: 0.00001904
Iteration 25/1000 | Loss: 0.00001903
Iteration 26/1000 | Loss: 0.00001902
Iteration 27/1000 | Loss: 0.00001902
Iteration 28/1000 | Loss: 0.00001901
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00001899
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001899
Iteration 36/1000 | Loss: 0.00001899
Iteration 37/1000 | Loss: 0.00001899
Iteration 38/1000 | Loss: 0.00001899
Iteration 39/1000 | Loss: 0.00001898
Iteration 40/1000 | Loss: 0.00001898
Iteration 41/1000 | Loss: 0.00001897
Iteration 42/1000 | Loss: 0.00001897
Iteration 43/1000 | Loss: 0.00001897
Iteration 44/1000 | Loss: 0.00001897
Iteration 45/1000 | Loss: 0.00001897
Iteration 46/1000 | Loss: 0.00001897
Iteration 47/1000 | Loss: 0.00001897
Iteration 48/1000 | Loss: 0.00001897
Iteration 49/1000 | Loss: 0.00001896
Iteration 50/1000 | Loss: 0.00001896
Iteration 51/1000 | Loss: 0.00001896
Iteration 52/1000 | Loss: 0.00001896
Iteration 53/1000 | Loss: 0.00001896
Iteration 54/1000 | Loss: 0.00001896
Iteration 55/1000 | Loss: 0.00001896
Iteration 56/1000 | Loss: 0.00001896
Iteration 57/1000 | Loss: 0.00001896
Iteration 58/1000 | Loss: 0.00001896
Iteration 59/1000 | Loss: 0.00001896
Iteration 60/1000 | Loss: 0.00001895
Iteration 61/1000 | Loss: 0.00001895
Iteration 62/1000 | Loss: 0.00001895
Iteration 63/1000 | Loss: 0.00001895
Iteration 64/1000 | Loss: 0.00001894
Iteration 65/1000 | Loss: 0.00001894
Iteration 66/1000 | Loss: 0.00001894
Iteration 67/1000 | Loss: 0.00001894
Iteration 68/1000 | Loss: 0.00001894
Iteration 69/1000 | Loss: 0.00001894
Iteration 70/1000 | Loss: 0.00001894
Iteration 71/1000 | Loss: 0.00001894
Iteration 72/1000 | Loss: 0.00001894
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001893
Iteration 77/1000 | Loss: 0.00001893
Iteration 78/1000 | Loss: 0.00001893
Iteration 79/1000 | Loss: 0.00001893
Iteration 80/1000 | Loss: 0.00001893
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001893
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001892
Iteration 91/1000 | Loss: 0.00001892
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001890
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001890
Iteration 101/1000 | Loss: 0.00001890
Iteration 102/1000 | Loss: 0.00001890
Iteration 103/1000 | Loss: 0.00001890
Iteration 104/1000 | Loss: 0.00001890
Iteration 105/1000 | Loss: 0.00001890
Iteration 106/1000 | Loss: 0.00001890
Iteration 107/1000 | Loss: 0.00001890
Iteration 108/1000 | Loss: 0.00001890
Iteration 109/1000 | Loss: 0.00001890
Iteration 110/1000 | Loss: 0.00001889
Iteration 111/1000 | Loss: 0.00001889
Iteration 112/1000 | Loss: 0.00001889
Iteration 113/1000 | Loss: 0.00001889
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001889
Iteration 116/1000 | Loss: 0.00001889
Iteration 117/1000 | Loss: 0.00001889
Iteration 118/1000 | Loss: 0.00001889
Iteration 119/1000 | Loss: 0.00001889
Iteration 120/1000 | Loss: 0.00001889
Iteration 121/1000 | Loss: 0.00001889
Iteration 122/1000 | Loss: 0.00001889
Iteration 123/1000 | Loss: 0.00001889
Iteration 124/1000 | Loss: 0.00001889
Iteration 125/1000 | Loss: 0.00001889
Iteration 126/1000 | Loss: 0.00001889
Iteration 127/1000 | Loss: 0.00001889
Iteration 128/1000 | Loss: 0.00001889
Iteration 129/1000 | Loss: 0.00001889
Iteration 130/1000 | Loss: 0.00001889
Iteration 131/1000 | Loss: 0.00001889
Iteration 132/1000 | Loss: 0.00001889
Iteration 133/1000 | Loss: 0.00001889
Iteration 134/1000 | Loss: 0.00001889
Iteration 135/1000 | Loss: 0.00001889
Iteration 136/1000 | Loss: 0.00001889
Iteration 137/1000 | Loss: 0.00001889
Iteration 138/1000 | Loss: 0.00001889
Iteration 139/1000 | Loss: 0.00001889
Iteration 140/1000 | Loss: 0.00001889
Iteration 141/1000 | Loss: 0.00001889
Iteration 142/1000 | Loss: 0.00001889
Iteration 143/1000 | Loss: 0.00001889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.8889919374487363e-05, 1.8889919374487363e-05, 1.8889919374487363e-05, 1.8889919374487363e-05, 1.8889919374487363e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8889919374487363e-05

Optimization complete. Final v2v error: 3.7826669216156006 mm

Highest mean error: 4.4323859214782715 mm for frame 86

Lowest mean error: 3.4725377559661865 mm for frame 40

Saving results

Total time: 30.933821439743042
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01145947
Iteration 2/25 | Loss: 0.01145947
Iteration 3/25 | Loss: 0.01145947
Iteration 4/25 | Loss: 0.01145947
Iteration 5/25 | Loss: 0.01145947
Iteration 6/25 | Loss: 0.01145946
Iteration 7/25 | Loss: 0.01145946
Iteration 8/25 | Loss: 0.01145946
Iteration 9/25 | Loss: 0.01145946
Iteration 10/25 | Loss: 0.01145946
Iteration 11/25 | Loss: 0.01145946
Iteration 12/25 | Loss: 0.01145946
Iteration 13/25 | Loss: 0.01145946
Iteration 14/25 | Loss: 0.01145946
Iteration 15/25 | Loss: 0.01145946
Iteration 16/25 | Loss: 0.01145946
Iteration 17/25 | Loss: 0.01145946
Iteration 18/25 | Loss: 0.01145945
Iteration 19/25 | Loss: 0.01145945
Iteration 20/25 | Loss: 0.01145945
Iteration 21/25 | Loss: 0.01145945
Iteration 22/25 | Loss: 0.01145945
Iteration 23/25 | Loss: 0.01145945
Iteration 24/25 | Loss: 0.01145945
Iteration 25/25 | Loss: 0.01145945

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.40315366
Iteration 2/25 | Loss: 0.05968912
Iteration 3/25 | Loss: 0.05964346
Iteration 4/25 | Loss: 0.05964344
Iteration 5/25 | Loss: 0.05964344
Iteration 6/25 | Loss: 0.05964343
Iteration 7/25 | Loss: 0.05964343
Iteration 8/25 | Loss: 0.05964343
Iteration 9/25 | Loss: 0.05964343
Iteration 10/25 | Loss: 0.05964342
Iteration 11/25 | Loss: 0.05964342
Iteration 12/25 | Loss: 0.05964343
Iteration 13/25 | Loss: 0.05964343
Iteration 14/25 | Loss: 0.05964342
Iteration 15/25 | Loss: 0.05964342
Iteration 16/25 | Loss: 0.05964342
Iteration 17/25 | Loss: 0.05964342
Iteration 18/25 | Loss: 0.05964342
Iteration 19/25 | Loss: 0.05964342
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.05964341759681702, 0.05964341759681702, 0.05964341759681702, 0.05964341759681702, 0.05964341759681702]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.05964341759681702

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.05964342
Iteration 2/1000 | Loss: 0.01004214
Iteration 3/1000 | Loss: 0.00289802
Iteration 4/1000 | Loss: 0.00117284
Iteration 5/1000 | Loss: 0.00174220
Iteration 6/1000 | Loss: 0.00511559
Iteration 7/1000 | Loss: 0.00813091
Iteration 8/1000 | Loss: 0.00253811
Iteration 9/1000 | Loss: 0.00150411
Iteration 10/1000 | Loss: 0.00094457
Iteration 11/1000 | Loss: 0.00219398
Iteration 12/1000 | Loss: 0.00054218
Iteration 13/1000 | Loss: 0.00057173
Iteration 14/1000 | Loss: 0.00064420
Iteration 15/1000 | Loss: 0.00067239
Iteration 16/1000 | Loss: 0.00086990
Iteration 17/1000 | Loss: 0.00022458
Iteration 18/1000 | Loss: 0.00029942
Iteration 19/1000 | Loss: 0.00008990
Iteration 20/1000 | Loss: 0.00010598
Iteration 21/1000 | Loss: 0.00011688
Iteration 22/1000 | Loss: 0.00043256
Iteration 23/1000 | Loss: 0.00053713
Iteration 24/1000 | Loss: 0.00053148
Iteration 25/1000 | Loss: 0.00127781
Iteration 26/1000 | Loss: 0.00010994
Iteration 27/1000 | Loss: 0.00046607
Iteration 28/1000 | Loss: 0.00033981
Iteration 29/1000 | Loss: 0.00013836
Iteration 30/1000 | Loss: 0.00015629
Iteration 31/1000 | Loss: 0.00005461
Iteration 32/1000 | Loss: 0.00009530
Iteration 33/1000 | Loss: 0.00046636
Iteration 34/1000 | Loss: 0.00005836
Iteration 35/1000 | Loss: 0.00064243
Iteration 36/1000 | Loss: 0.00068169
Iteration 37/1000 | Loss: 0.00007902
Iteration 38/1000 | Loss: 0.00004469
Iteration 39/1000 | Loss: 0.00004351
Iteration 40/1000 | Loss: 0.00018022
Iteration 41/1000 | Loss: 0.00084029
Iteration 42/1000 | Loss: 0.00024053
Iteration 43/1000 | Loss: 0.00078171
Iteration 44/1000 | Loss: 0.00060840
Iteration 45/1000 | Loss: 0.00053826
Iteration 46/1000 | Loss: 0.00153844
Iteration 47/1000 | Loss: 0.00147667
Iteration 48/1000 | Loss: 0.00016578
Iteration 49/1000 | Loss: 0.00007609
Iteration 50/1000 | Loss: 0.00011406
Iteration 51/1000 | Loss: 0.00036377
Iteration 52/1000 | Loss: 0.00005462
Iteration 53/1000 | Loss: 0.00003872
Iteration 54/1000 | Loss: 0.00014537
Iteration 55/1000 | Loss: 0.00027189
Iteration 56/1000 | Loss: 0.00098162
Iteration 57/1000 | Loss: 0.00050817
Iteration 58/1000 | Loss: 0.00026254
Iteration 59/1000 | Loss: 0.00011187
Iteration 60/1000 | Loss: 0.00007946
Iteration 61/1000 | Loss: 0.00012928
Iteration 62/1000 | Loss: 0.00009605
Iteration 63/1000 | Loss: 0.00005542
Iteration 64/1000 | Loss: 0.00003961
Iteration 65/1000 | Loss: 0.00005700
Iteration 66/1000 | Loss: 0.00003745
Iteration 67/1000 | Loss: 0.00005206
Iteration 68/1000 | Loss: 0.00046207
Iteration 69/1000 | Loss: 0.00004107
Iteration 70/1000 | Loss: 0.00024184
Iteration 71/1000 | Loss: 0.00003531
Iteration 72/1000 | Loss: 0.00003703
Iteration 73/1000 | Loss: 0.00005339
Iteration 74/1000 | Loss: 0.00045963
Iteration 75/1000 | Loss: 0.00007397
Iteration 76/1000 | Loss: 0.00084534
Iteration 77/1000 | Loss: 0.00004264
Iteration 78/1000 | Loss: 0.00078386
Iteration 79/1000 | Loss: 0.00004773
Iteration 80/1000 | Loss: 0.00003470
Iteration 81/1000 | Loss: 0.00003593
Iteration 82/1000 | Loss: 0.00003375
Iteration 83/1000 | Loss: 0.00004057
Iteration 84/1000 | Loss: 0.00004056
Iteration 85/1000 | Loss: 0.00006050
Iteration 86/1000 | Loss: 0.00003316
Iteration 87/1000 | Loss: 0.00003287
Iteration 88/1000 | Loss: 0.00066300
Iteration 89/1000 | Loss: 0.00010287
Iteration 90/1000 | Loss: 0.00003384
Iteration 91/1000 | Loss: 0.00026481
Iteration 92/1000 | Loss: 0.00003842
Iteration 93/1000 | Loss: 0.00014278
Iteration 94/1000 | Loss: 0.00004289
Iteration 95/1000 | Loss: 0.00003401
Iteration 96/1000 | Loss: 0.00004035
Iteration 97/1000 | Loss: 0.00029341
Iteration 98/1000 | Loss: 0.00036454
Iteration 99/1000 | Loss: 0.00003511
Iteration 100/1000 | Loss: 0.00026315
Iteration 101/1000 | Loss: 0.00047540
Iteration 102/1000 | Loss: 0.00029488
Iteration 103/1000 | Loss: 0.00008707
Iteration 104/1000 | Loss: 0.00030065
Iteration 105/1000 | Loss: 0.00007507
Iteration 106/1000 | Loss: 0.00008262
Iteration 107/1000 | Loss: 0.00009150
Iteration 108/1000 | Loss: 0.00003928
Iteration 109/1000 | Loss: 0.00005796
Iteration 110/1000 | Loss: 0.00021429
Iteration 111/1000 | Loss: 0.00006742
Iteration 112/1000 | Loss: 0.00016191
Iteration 113/1000 | Loss: 0.00003536
Iteration 114/1000 | Loss: 0.00005884
Iteration 115/1000 | Loss: 0.00004322
Iteration 116/1000 | Loss: 0.00007799
Iteration 117/1000 | Loss: 0.00003676
Iteration 118/1000 | Loss: 0.00003285
Iteration 119/1000 | Loss: 0.00003282
Iteration 120/1000 | Loss: 0.00004083
Iteration 121/1000 | Loss: 0.00003747
Iteration 122/1000 | Loss: 0.00003278
Iteration 123/1000 | Loss: 0.00006171
Iteration 124/1000 | Loss: 0.00008658
Iteration 125/1000 | Loss: 0.00003952
Iteration 126/1000 | Loss: 0.00003566
Iteration 127/1000 | Loss: 0.00003153
Iteration 128/1000 | Loss: 0.00003115
Iteration 129/1000 | Loss: 0.00003115
Iteration 130/1000 | Loss: 0.00003115
Iteration 131/1000 | Loss: 0.00003115
Iteration 132/1000 | Loss: 0.00003115
Iteration 133/1000 | Loss: 0.00003115
Iteration 134/1000 | Loss: 0.00003115
Iteration 135/1000 | Loss: 0.00003115
Iteration 136/1000 | Loss: 0.00003115
Iteration 137/1000 | Loss: 0.00003115
Iteration 138/1000 | Loss: 0.00003114
Iteration 139/1000 | Loss: 0.00003114
Iteration 140/1000 | Loss: 0.00003114
Iteration 141/1000 | Loss: 0.00003114
Iteration 142/1000 | Loss: 0.00003114
Iteration 143/1000 | Loss: 0.00003114
Iteration 144/1000 | Loss: 0.00003113
Iteration 145/1000 | Loss: 0.00003113
Iteration 146/1000 | Loss: 0.00003122
Iteration 147/1000 | Loss: 0.00003107
Iteration 148/1000 | Loss: 0.00003106
Iteration 149/1000 | Loss: 0.00003106
Iteration 150/1000 | Loss: 0.00003106
Iteration 151/1000 | Loss: 0.00003106
Iteration 152/1000 | Loss: 0.00003105
Iteration 153/1000 | Loss: 0.00003102
Iteration 154/1000 | Loss: 0.00003102
Iteration 155/1000 | Loss: 0.00003102
Iteration 156/1000 | Loss: 0.00003102
Iteration 157/1000 | Loss: 0.00003101
Iteration 158/1000 | Loss: 0.00003101
Iteration 159/1000 | Loss: 0.00003101
Iteration 160/1000 | Loss: 0.00003123
Iteration 161/1000 | Loss: 0.00006091
Iteration 162/1000 | Loss: 0.00012339
Iteration 163/1000 | Loss: 0.00003706
Iteration 164/1000 | Loss: 0.00005379
Iteration 165/1000 | Loss: 0.00008206
Iteration 166/1000 | Loss: 0.00004001
Iteration 167/1000 | Loss: 0.00006511
Iteration 168/1000 | Loss: 0.00009960
Iteration 169/1000 | Loss: 0.00004252
Iteration 170/1000 | Loss: 0.00007066
Iteration 171/1000 | Loss: 0.00008828
Iteration 172/1000 | Loss: 0.00006371
Iteration 173/1000 | Loss: 0.00007323
Iteration 174/1000 | Loss: 0.00018424
Iteration 175/1000 | Loss: 0.00039577
Iteration 176/1000 | Loss: 0.00010490
Iteration 177/1000 | Loss: 0.00006777
Iteration 178/1000 | Loss: 0.00004063
Iteration 179/1000 | Loss: 0.00004009
Iteration 180/1000 | Loss: 0.00007803
Iteration 181/1000 | Loss: 0.00003754
Iteration 182/1000 | Loss: 0.00006143
Iteration 183/1000 | Loss: 0.00005468
Iteration 184/1000 | Loss: 0.00003289
Iteration 185/1000 | Loss: 0.00003110
Iteration 186/1000 | Loss: 0.00003110
Iteration 187/1000 | Loss: 0.00003539
Iteration 188/1000 | Loss: 0.00003233
Iteration 189/1000 | Loss: 0.00003100
Iteration 190/1000 | Loss: 0.00003100
Iteration 191/1000 | Loss: 0.00003099
Iteration 192/1000 | Loss: 0.00003099
Iteration 193/1000 | Loss: 0.00003099
Iteration 194/1000 | Loss: 0.00003099
Iteration 195/1000 | Loss: 0.00003099
Iteration 196/1000 | Loss: 0.00003099
Iteration 197/1000 | Loss: 0.00003099
Iteration 198/1000 | Loss: 0.00003099
Iteration 199/1000 | Loss: 0.00003099
Iteration 200/1000 | Loss: 0.00003099
Iteration 201/1000 | Loss: 0.00003099
Iteration 202/1000 | Loss: 0.00003099
Iteration 203/1000 | Loss: 0.00003099
Iteration 204/1000 | Loss: 0.00003099
Iteration 205/1000 | Loss: 0.00003226
Iteration 206/1000 | Loss: 0.00003995
Iteration 207/1000 | Loss: 0.00003095
Iteration 208/1000 | Loss: 0.00003092
Iteration 209/1000 | Loss: 0.00003092
Iteration 210/1000 | Loss: 0.00034971
Iteration 211/1000 | Loss: 0.00015962
Iteration 212/1000 | Loss: 0.00006390
Iteration 213/1000 | Loss: 0.00004714
Iteration 214/1000 | Loss: 0.00003100
Iteration 215/1000 | Loss: 0.00003092
Iteration 216/1000 | Loss: 0.00003092
Iteration 217/1000 | Loss: 0.00003091
Iteration 218/1000 | Loss: 0.00003090
Iteration 219/1000 | Loss: 0.00003089
Iteration 220/1000 | Loss: 0.00003089
Iteration 221/1000 | Loss: 0.00003089
Iteration 222/1000 | Loss: 0.00003088
Iteration 223/1000 | Loss: 0.00003088
Iteration 224/1000 | Loss: 0.00003088
Iteration 225/1000 | Loss: 0.00003088
Iteration 226/1000 | Loss: 0.00003087
Iteration 227/1000 | Loss: 0.00003087
Iteration 228/1000 | Loss: 0.00003086
Iteration 229/1000 | Loss: 0.00003086
Iteration 230/1000 | Loss: 0.00003085
Iteration 231/1000 | Loss: 0.00003084
Iteration 232/1000 | Loss: 0.00003084
Iteration 233/1000 | Loss: 0.00003084
Iteration 234/1000 | Loss: 0.00003084
Iteration 235/1000 | Loss: 0.00003083
Iteration 236/1000 | Loss: 0.00003082
Iteration 237/1000 | Loss: 0.00003081
Iteration 238/1000 | Loss: 0.00003079
Iteration 239/1000 | Loss: 0.00003078
Iteration 240/1000 | Loss: 0.00003077
Iteration 241/1000 | Loss: 0.00003076
Iteration 242/1000 | Loss: 0.00032639
Iteration 243/1000 | Loss: 0.00011612
Iteration 244/1000 | Loss: 0.00003319
Iteration 245/1000 | Loss: 0.00004389
Iteration 246/1000 | Loss: 0.00005166
Iteration 247/1000 | Loss: 0.00004620
Iteration 248/1000 | Loss: 0.00004745
Iteration 249/1000 | Loss: 0.00003265
Iteration 250/1000 | Loss: 0.00003433
Iteration 251/1000 | Loss: 0.00003412
Iteration 252/1000 | Loss: 0.00051938
Iteration 253/1000 | Loss: 0.00005745
Iteration 254/1000 | Loss: 0.00004463
Iteration 255/1000 | Loss: 0.00004810
Iteration 256/1000 | Loss: 0.00021010
Iteration 257/1000 | Loss: 0.00007886
Iteration 258/1000 | Loss: 0.00003259
Iteration 259/1000 | Loss: 0.00003024
Iteration 260/1000 | Loss: 0.00014305
Iteration 261/1000 | Loss: 0.00003458
Iteration 262/1000 | Loss: 0.00002975
Iteration 263/1000 | Loss: 0.00003185
Iteration 264/1000 | Loss: 0.00003378
Iteration 265/1000 | Loss: 0.00004821
Iteration 266/1000 | Loss: 0.00003517
Iteration 267/1000 | Loss: 0.00003204
Iteration 268/1000 | Loss: 0.00003608
Iteration 269/1000 | Loss: 0.00003029
Iteration 270/1000 | Loss: 0.00006517
Iteration 271/1000 | Loss: 0.00008743
Iteration 272/1000 | Loss: 0.00003900
Iteration 273/1000 | Loss: 0.00003634
Iteration 274/1000 | Loss: 0.00003437
Iteration 275/1000 | Loss: 0.00004454
Iteration 276/1000 | Loss: 0.00010256
Iteration 277/1000 | Loss: 0.00044198
Iteration 278/1000 | Loss: 0.00007038
Iteration 279/1000 | Loss: 0.00006542
Iteration 280/1000 | Loss: 0.00008979
Iteration 281/1000 | Loss: 0.00007972
Iteration 282/1000 | Loss: 0.00006003
Iteration 283/1000 | Loss: 0.00004787
Iteration 284/1000 | Loss: 0.00009625
Iteration 285/1000 | Loss: 0.00004563
Iteration 286/1000 | Loss: 0.00003779
Iteration 287/1000 | Loss: 0.00004772
Iteration 288/1000 | Loss: 0.00004103
Iteration 289/1000 | Loss: 0.00004618
Iteration 290/1000 | Loss: 0.00004486
Iteration 291/1000 | Loss: 0.00005233
Iteration 292/1000 | Loss: 0.00052999
Iteration 293/1000 | Loss: 0.00003644
Iteration 294/1000 | Loss: 0.00003948
Iteration 295/1000 | Loss: 0.00009574
Iteration 296/1000 | Loss: 0.00012899
Iteration 297/1000 | Loss: 0.00004956
Iteration 298/1000 | Loss: 0.00007475
Iteration 299/1000 | Loss: 0.00015043
Iteration 300/1000 | Loss: 0.00005242
Iteration 301/1000 | Loss: 0.00019796
Iteration 302/1000 | Loss: 0.00005534
Iteration 303/1000 | Loss: 0.00003745
Iteration 304/1000 | Loss: 0.00003804
Iteration 305/1000 | Loss: 0.00005116
Iteration 306/1000 | Loss: 0.00004658
Iteration 307/1000 | Loss: 0.00005473
Iteration 308/1000 | Loss: 0.00006303
Iteration 309/1000 | Loss: 0.00012868
Iteration 310/1000 | Loss: 0.00006634
Iteration 311/1000 | Loss: 0.00011919
Iteration 312/1000 | Loss: 0.00033461
Iteration 313/1000 | Loss: 0.00006277
Iteration 314/1000 | Loss: 0.00004287
Iteration 315/1000 | Loss: 0.00003717
Iteration 316/1000 | Loss: 0.00004072
Iteration 317/1000 | Loss: 0.00005200
Iteration 318/1000 | Loss: 0.00006300
Iteration 319/1000 | Loss: 0.00006263
Iteration 320/1000 | Loss: 0.00004890
Iteration 321/1000 | Loss: 0.00003851
Iteration 322/1000 | Loss: 0.00003950
Iteration 323/1000 | Loss: 0.00003969
Iteration 324/1000 | Loss: 0.00032156
Iteration 325/1000 | Loss: 0.00006212
Iteration 326/1000 | Loss: 0.00006316
Iteration 327/1000 | Loss: 0.00008388
Iteration 328/1000 | Loss: 0.00008835
Iteration 329/1000 | Loss: 0.00005757
Iteration 330/1000 | Loss: 0.00005440
Iteration 331/1000 | Loss: 0.00015390
Iteration 332/1000 | Loss: 0.00005839
Iteration 333/1000 | Loss: 0.00004102
Iteration 334/1000 | Loss: 0.00004538
Iteration 335/1000 | Loss: 0.00004213
Iteration 336/1000 | Loss: 0.00004185
Iteration 337/1000 | Loss: 0.00005799
Iteration 338/1000 | Loss: 0.00004103
Iteration 339/1000 | Loss: 0.00006837
Iteration 340/1000 | Loss: 0.00003998
Iteration 341/1000 | Loss: 0.00005202
Iteration 342/1000 | Loss: 0.00003859
Iteration 343/1000 | Loss: 0.00003936
Iteration 344/1000 | Loss: 0.00003734
Iteration 345/1000 | Loss: 0.00003852
Iteration 346/1000 | Loss: 0.00003852
Iteration 347/1000 | Loss: 0.00003857
Iteration 348/1000 | Loss: 0.00004055
Iteration 349/1000 | Loss: 0.00008196
Iteration 350/1000 | Loss: 0.00003849
Iteration 351/1000 | Loss: 0.00004642
Iteration 352/1000 | Loss: 0.00005791
Iteration 353/1000 | Loss: 0.00004504
Iteration 354/1000 | Loss: 0.00004310
Iteration 355/1000 | Loss: 0.00005207
Iteration 356/1000 | Loss: 0.00004041
Iteration 357/1000 | Loss: 0.00003660
Iteration 358/1000 | Loss: 0.00004918
Iteration 359/1000 | Loss: 0.00008023
Iteration 360/1000 | Loss: 0.00010348
Iteration 361/1000 | Loss: 0.00007485
Iteration 362/1000 | Loss: 0.00003713
Iteration 363/1000 | Loss: 0.00004164
Iteration 364/1000 | Loss: 0.00006187
Iteration 365/1000 | Loss: 0.00006438
Iteration 366/1000 | Loss: 0.00005079
Iteration 367/1000 | Loss: 0.00007745
Iteration 368/1000 | Loss: 0.00005201
Iteration 369/1000 | Loss: 0.00004648
Iteration 370/1000 | Loss: 0.00006680
Iteration 371/1000 | Loss: 0.00005039
Iteration 372/1000 | Loss: 0.00003738
Iteration 373/1000 | Loss: 0.00004397
Iteration 374/1000 | Loss: 0.00004297
Iteration 375/1000 | Loss: 0.00003917
Iteration 376/1000 | Loss: 0.00004447
Iteration 377/1000 | Loss: 0.00003852
Iteration 378/1000 | Loss: 0.00004104
Iteration 379/1000 | Loss: 0.00005338
Iteration 380/1000 | Loss: 0.00004185
Iteration 381/1000 | Loss: 0.00004572
Iteration 382/1000 | Loss: 0.00006431
Iteration 383/1000 | Loss: 0.00004000
Iteration 384/1000 | Loss: 0.00006163
Iteration 385/1000 | Loss: 0.00004143
Iteration 386/1000 | Loss: 0.00006378
Iteration 387/1000 | Loss: 0.00007026
Iteration 388/1000 | Loss: 0.00006942
Iteration 389/1000 | Loss: 0.00006260
Iteration 390/1000 | Loss: 0.00009052
Iteration 391/1000 | Loss: 0.00005090
Iteration 392/1000 | Loss: 0.00007407
Iteration 393/1000 | Loss: 0.00004990
Iteration 394/1000 | Loss: 0.00011668
Iteration 395/1000 | Loss: 0.00005188
Iteration 396/1000 | Loss: 0.00005054
Iteration 397/1000 | Loss: 0.00004328
Iteration 398/1000 | Loss: 0.00005087
Iteration 399/1000 | Loss: 0.00004430
Iteration 400/1000 | Loss: 0.00004727
Iteration 401/1000 | Loss: 0.00004559
Iteration 402/1000 | Loss: 0.00004778
Iteration 403/1000 | Loss: 0.00006971
Iteration 404/1000 | Loss: 0.00004649
Iteration 405/1000 | Loss: 0.00004478
Iteration 406/1000 | Loss: 0.00004741
Iteration 407/1000 | Loss: 0.00005473
Iteration 408/1000 | Loss: 0.00005427
Iteration 409/1000 | Loss: 0.00004095
Iteration 410/1000 | Loss: 0.00004878
Iteration 411/1000 | Loss: 0.00004505
Iteration 412/1000 | Loss: 0.00007571
Iteration 413/1000 | Loss: 0.00004427
Iteration 414/1000 | Loss: 0.00004762
Iteration 415/1000 | Loss: 0.00004137
Iteration 416/1000 | Loss: 0.00004768
Iteration 417/1000 | Loss: 0.00003965
Iteration 418/1000 | Loss: 0.00004589
Iteration 419/1000 | Loss: 0.00005818
Iteration 420/1000 | Loss: 0.00011562
Iteration 421/1000 | Loss: 0.00004065
Iteration 422/1000 | Loss: 0.00007690
Iteration 423/1000 | Loss: 0.00005905
Iteration 424/1000 | Loss: 0.00005527
Iteration 425/1000 | Loss: 0.00004774
Iteration 426/1000 | Loss: 0.00005212
Iteration 427/1000 | Loss: 0.00004527
Iteration 428/1000 | Loss: 0.00005017
Iteration 429/1000 | Loss: 0.00005055
Iteration 430/1000 | Loss: 0.00004987
Iteration 431/1000 | Loss: 0.00004609
Iteration 432/1000 | Loss: 0.00006486
Iteration 433/1000 | Loss: 0.00017582
Iteration 434/1000 | Loss: 0.00004751
Iteration 435/1000 | Loss: 0.00005040
Iteration 436/1000 | Loss: 0.00005117
Iteration 437/1000 | Loss: 0.00005632
Iteration 438/1000 | Loss: 0.00004298
Iteration 439/1000 | Loss: 0.00004123
Iteration 440/1000 | Loss: 0.00004505
Iteration 441/1000 | Loss: 0.00004505
Iteration 442/1000 | Loss: 0.00010797
Iteration 443/1000 | Loss: 0.00004715
Iteration 444/1000 | Loss: 0.00005296
Iteration 445/1000 | Loss: 0.00004735
Iteration 446/1000 | Loss: 0.00005073
Iteration 447/1000 | Loss: 0.00004498
Iteration 448/1000 | Loss: 0.00007749
Iteration 449/1000 | Loss: 0.00007371
Iteration 450/1000 | Loss: 0.00004925
Iteration 451/1000 | Loss: 0.00004984
Iteration 452/1000 | Loss: 0.00007630
Iteration 453/1000 | Loss: 0.00005838
Iteration 454/1000 | Loss: 0.00006230
Iteration 455/1000 | Loss: 0.00015155
Iteration 456/1000 | Loss: 0.00005845
Iteration 457/1000 | Loss: 0.00007742
Iteration 458/1000 | Loss: 0.00005352
Iteration 459/1000 | Loss: 0.00007962
Iteration 460/1000 | Loss: 0.00005353
Iteration 461/1000 | Loss: 0.00006182
Iteration 462/1000 | Loss: 0.00005184
Iteration 463/1000 | Loss: 0.00006035
Iteration 464/1000 | Loss: 0.00006034
Iteration 465/1000 | Loss: 0.00004962
Iteration 466/1000 | Loss: 0.00004697
Iteration 467/1000 | Loss: 0.00005885
Iteration 468/1000 | Loss: 0.00005035
Iteration 469/1000 | Loss: 0.00004763
Iteration 470/1000 | Loss: 0.00005793
Iteration 471/1000 | Loss: 0.00005541
Iteration 472/1000 | Loss: 0.00004666
Iteration 473/1000 | Loss: 0.00004687
Iteration 474/1000 | Loss: 0.00005257
Iteration 475/1000 | Loss: 0.00007190
Iteration 476/1000 | Loss: 0.00006273
Iteration 477/1000 | Loss: 0.00006580
Iteration 478/1000 | Loss: 0.00005467
Iteration 479/1000 | Loss: 0.00005685
Iteration 480/1000 | Loss: 0.00005417
Iteration 481/1000 | Loss: 0.00005744
Iteration 482/1000 | Loss: 0.00011652
Iteration 483/1000 | Loss: 0.00007413
Iteration 484/1000 | Loss: 0.00006748
Iteration 485/1000 | Loss: 0.00005677
Iteration 486/1000 | Loss: 0.00006807
Iteration 487/1000 | Loss: 0.00005756
Iteration 488/1000 | Loss: 0.00005693
Iteration 489/1000 | Loss: 0.00006551
Iteration 490/1000 | Loss: 0.00005773
Iteration 491/1000 | Loss: 0.00006135
Iteration 492/1000 | Loss: 0.00005645
Iteration 493/1000 | Loss: 0.00006651
Iteration 494/1000 | Loss: 0.00005443
Iteration 495/1000 | Loss: 0.00006427
Iteration 496/1000 | Loss: 0.00005115
Iteration 497/1000 | Loss: 0.00006665
Iteration 498/1000 | Loss: 0.00004953
Iteration 499/1000 | Loss: 0.00006817
Iteration 500/1000 | Loss: 0.00005644
Iteration 501/1000 | Loss: 0.00006643
Iteration 502/1000 | Loss: 0.00005422
Iteration 503/1000 | Loss: 0.00005983
Iteration 504/1000 | Loss: 0.00005212
Iteration 505/1000 | Loss: 0.00004008
Iteration 506/1000 | Loss: 0.00002910
Iteration 507/1000 | Loss: 0.00002844
Iteration 508/1000 | Loss: 0.00002818
Iteration 509/1000 | Loss: 0.00002814
Iteration 510/1000 | Loss: 0.00002814
Iteration 511/1000 | Loss: 0.00002812
Iteration 512/1000 | Loss: 0.00002812
Iteration 513/1000 | Loss: 0.00002811
Iteration 514/1000 | Loss: 0.00002811
Iteration 515/1000 | Loss: 0.00002811
Iteration 516/1000 | Loss: 0.00002811
Iteration 517/1000 | Loss: 0.00002811
Iteration 518/1000 | Loss: 0.00002810
Iteration 519/1000 | Loss: 0.00002810
Iteration 520/1000 | Loss: 0.00002810
Iteration 521/1000 | Loss: 0.00002810
Iteration 522/1000 | Loss: 0.00002945
Iteration 523/1000 | Loss: 0.00002810
Iteration 524/1000 | Loss: 0.00002809
Iteration 525/1000 | Loss: 0.00002809
Iteration 526/1000 | Loss: 0.00002809
Iteration 527/1000 | Loss: 0.00002809
Iteration 528/1000 | Loss: 0.00002809
Iteration 529/1000 | Loss: 0.00002809
Iteration 530/1000 | Loss: 0.00002808
Iteration 531/1000 | Loss: 0.00002808
Iteration 532/1000 | Loss: 0.00002808
Iteration 533/1000 | Loss: 0.00002808
Iteration 534/1000 | Loss: 0.00002808
Iteration 535/1000 | Loss: 0.00002808
Iteration 536/1000 | Loss: 0.00002808
Iteration 537/1000 | Loss: 0.00002808
Iteration 538/1000 | Loss: 0.00002808
Iteration 539/1000 | Loss: 0.00002808
Iteration 540/1000 | Loss: 0.00002808
Iteration 541/1000 | Loss: 0.00002808
Iteration 542/1000 | Loss: 0.00002808
Iteration 543/1000 | Loss: 0.00002808
Iteration 544/1000 | Loss: 0.00002808
Iteration 545/1000 | Loss: 0.00002808
Iteration 546/1000 | Loss: 0.00002808
Iteration 547/1000 | Loss: 0.00002808
Iteration 548/1000 | Loss: 0.00002808
Iteration 549/1000 | Loss: 0.00002808
Iteration 550/1000 | Loss: 0.00002808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 550. Stopping optimization.
Last 5 losses: [2.808217323035933e-05, 2.808217323035933e-05, 2.808217323035933e-05, 2.808217323035933e-05, 2.808217323035933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.808217323035933e-05

Optimization complete. Final v2v error: 3.7983250617980957 mm

Highest mean error: 18.346588134765625 mm for frame 92

Lowest mean error: 2.939058542251587 mm for frame 129

Saving results

Total time: 670.2471766471863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465237
Iteration 2/25 | Loss: 0.00147442
Iteration 3/25 | Loss: 0.00121198
Iteration 4/25 | Loss: 0.00114231
Iteration 5/25 | Loss: 0.00113391
Iteration 6/25 | Loss: 0.00113247
Iteration 7/25 | Loss: 0.00113167
Iteration 8/25 | Loss: 0.00113148
Iteration 9/25 | Loss: 0.00113148
Iteration 10/25 | Loss: 0.00113148
Iteration 11/25 | Loss: 0.00113148
Iteration 12/25 | Loss: 0.00113148
Iteration 13/25 | Loss: 0.00113148
Iteration 14/25 | Loss: 0.00113148
Iteration 15/25 | Loss: 0.00113148
Iteration 16/25 | Loss: 0.00113148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011314756702631712, 0.0011314756702631712, 0.0011314756702631712, 0.0011314756702631712, 0.0011314756702631712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011314756702631712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82880104
Iteration 2/25 | Loss: 0.00590326
Iteration 3/25 | Loss: 0.00590326
Iteration 4/25 | Loss: 0.00590326
Iteration 5/25 | Loss: 0.00590326
Iteration 6/25 | Loss: 0.00590326
Iteration 7/25 | Loss: 0.00590326
Iteration 8/25 | Loss: 0.00590326
Iteration 9/25 | Loss: 0.00590326
Iteration 10/25 | Loss: 0.00590326
Iteration 11/25 | Loss: 0.00590326
Iteration 12/25 | Loss: 0.00590326
Iteration 13/25 | Loss: 0.00590326
Iteration 14/25 | Loss: 0.00590326
Iteration 15/25 | Loss: 0.00590326
Iteration 16/25 | Loss: 0.00590326
Iteration 17/25 | Loss: 0.00590326
Iteration 18/25 | Loss: 0.00590326
Iteration 19/25 | Loss: 0.00590326
Iteration 20/25 | Loss: 0.00590326
Iteration 21/25 | Loss: 0.00590326
Iteration 22/25 | Loss: 0.00590326
Iteration 23/25 | Loss: 0.00590326
Iteration 24/25 | Loss: 0.00590326
Iteration 25/25 | Loss: 0.00590326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00590326
Iteration 2/1000 | Loss: 0.00004749
Iteration 3/1000 | Loss: 0.00002415
Iteration 4/1000 | Loss: 0.00001997
Iteration 5/1000 | Loss: 0.00001824
Iteration 6/1000 | Loss: 0.00001735
Iteration 7/1000 | Loss: 0.00001676
Iteration 8/1000 | Loss: 0.00001613
Iteration 9/1000 | Loss: 0.00001577
Iteration 10/1000 | Loss: 0.00001545
Iteration 11/1000 | Loss: 0.00001525
Iteration 12/1000 | Loss: 0.00001506
Iteration 13/1000 | Loss: 0.00001500
Iteration 14/1000 | Loss: 0.00001499
Iteration 15/1000 | Loss: 0.00001494
Iteration 16/1000 | Loss: 0.00001493
Iteration 17/1000 | Loss: 0.00001490
Iteration 18/1000 | Loss: 0.00001490
Iteration 19/1000 | Loss: 0.00001488
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001487
Iteration 22/1000 | Loss: 0.00001487
Iteration 23/1000 | Loss: 0.00001486
Iteration 24/1000 | Loss: 0.00001486
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001485
Iteration 28/1000 | Loss: 0.00001484
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001483
Iteration 31/1000 | Loss: 0.00001482
Iteration 32/1000 | Loss: 0.00001482
Iteration 33/1000 | Loss: 0.00001481
Iteration 34/1000 | Loss: 0.00001481
Iteration 35/1000 | Loss: 0.00001480
Iteration 36/1000 | Loss: 0.00001480
Iteration 37/1000 | Loss: 0.00001480
Iteration 38/1000 | Loss: 0.00001479
Iteration 39/1000 | Loss: 0.00001477
Iteration 40/1000 | Loss: 0.00001477
Iteration 41/1000 | Loss: 0.00001477
Iteration 42/1000 | Loss: 0.00001477
Iteration 43/1000 | Loss: 0.00001476
Iteration 44/1000 | Loss: 0.00001475
Iteration 45/1000 | Loss: 0.00001475
Iteration 46/1000 | Loss: 0.00001474
Iteration 47/1000 | Loss: 0.00001474
Iteration 48/1000 | Loss: 0.00001474
Iteration 49/1000 | Loss: 0.00001473
Iteration 50/1000 | Loss: 0.00001473
Iteration 51/1000 | Loss: 0.00001473
Iteration 52/1000 | Loss: 0.00001472
Iteration 53/1000 | Loss: 0.00001472
Iteration 54/1000 | Loss: 0.00001472
Iteration 55/1000 | Loss: 0.00001471
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001471
Iteration 59/1000 | Loss: 0.00001470
Iteration 60/1000 | Loss: 0.00001470
Iteration 61/1000 | Loss: 0.00001470
Iteration 62/1000 | Loss: 0.00001470
Iteration 63/1000 | Loss: 0.00001470
Iteration 64/1000 | Loss: 0.00001470
Iteration 65/1000 | Loss: 0.00001470
Iteration 66/1000 | Loss: 0.00001470
Iteration 67/1000 | Loss: 0.00001470
Iteration 68/1000 | Loss: 0.00001469
Iteration 69/1000 | Loss: 0.00001469
Iteration 70/1000 | Loss: 0.00001469
Iteration 71/1000 | Loss: 0.00001469
Iteration 72/1000 | Loss: 0.00001469
Iteration 73/1000 | Loss: 0.00001469
Iteration 74/1000 | Loss: 0.00001468
Iteration 75/1000 | Loss: 0.00001468
Iteration 76/1000 | Loss: 0.00001468
Iteration 77/1000 | Loss: 0.00001468
Iteration 78/1000 | Loss: 0.00001468
Iteration 79/1000 | Loss: 0.00001468
Iteration 80/1000 | Loss: 0.00001468
Iteration 81/1000 | Loss: 0.00001468
Iteration 82/1000 | Loss: 0.00001468
Iteration 83/1000 | Loss: 0.00001468
Iteration 84/1000 | Loss: 0.00001468
Iteration 85/1000 | Loss: 0.00001467
Iteration 86/1000 | Loss: 0.00001467
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001466
Iteration 93/1000 | Loss: 0.00001466
Iteration 94/1000 | Loss: 0.00001466
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001466
Iteration 97/1000 | Loss: 0.00001466
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001466
Iteration 100/1000 | Loss: 0.00001466
Iteration 101/1000 | Loss: 0.00001466
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001466
Iteration 105/1000 | Loss: 0.00001465
Iteration 106/1000 | Loss: 0.00001465
Iteration 107/1000 | Loss: 0.00001465
Iteration 108/1000 | Loss: 0.00001465
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001464
Iteration 112/1000 | Loss: 0.00001464
Iteration 113/1000 | Loss: 0.00001464
Iteration 114/1000 | Loss: 0.00001464
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001464
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001463
Iteration 124/1000 | Loss: 0.00001463
Iteration 125/1000 | Loss: 0.00001463
Iteration 126/1000 | Loss: 0.00001463
Iteration 127/1000 | Loss: 0.00001463
Iteration 128/1000 | Loss: 0.00001463
Iteration 129/1000 | Loss: 0.00001463
Iteration 130/1000 | Loss: 0.00001463
Iteration 131/1000 | Loss: 0.00001462
Iteration 132/1000 | Loss: 0.00001462
Iteration 133/1000 | Loss: 0.00001462
Iteration 134/1000 | Loss: 0.00001462
Iteration 135/1000 | Loss: 0.00001462
Iteration 136/1000 | Loss: 0.00001462
Iteration 137/1000 | Loss: 0.00001462
Iteration 138/1000 | Loss: 0.00001462
Iteration 139/1000 | Loss: 0.00001461
Iteration 140/1000 | Loss: 0.00001461
Iteration 141/1000 | Loss: 0.00001461
Iteration 142/1000 | Loss: 0.00001461
Iteration 143/1000 | Loss: 0.00001461
Iteration 144/1000 | Loss: 0.00001461
Iteration 145/1000 | Loss: 0.00001461
Iteration 146/1000 | Loss: 0.00001461
Iteration 147/1000 | Loss: 0.00001461
Iteration 148/1000 | Loss: 0.00001461
Iteration 149/1000 | Loss: 0.00001461
Iteration 150/1000 | Loss: 0.00001461
Iteration 151/1000 | Loss: 0.00001461
Iteration 152/1000 | Loss: 0.00001461
Iteration 153/1000 | Loss: 0.00001461
Iteration 154/1000 | Loss: 0.00001461
Iteration 155/1000 | Loss: 0.00001461
Iteration 156/1000 | Loss: 0.00001461
Iteration 157/1000 | Loss: 0.00001461
Iteration 158/1000 | Loss: 0.00001461
Iteration 159/1000 | Loss: 0.00001461
Iteration 160/1000 | Loss: 0.00001461
Iteration 161/1000 | Loss: 0.00001461
Iteration 162/1000 | Loss: 0.00001461
Iteration 163/1000 | Loss: 0.00001461
Iteration 164/1000 | Loss: 0.00001461
Iteration 165/1000 | Loss: 0.00001461
Iteration 166/1000 | Loss: 0.00001461
Iteration 167/1000 | Loss: 0.00001461
Iteration 168/1000 | Loss: 0.00001461
Iteration 169/1000 | Loss: 0.00001461
Iteration 170/1000 | Loss: 0.00001461
Iteration 171/1000 | Loss: 0.00001461
Iteration 172/1000 | Loss: 0.00001461
Iteration 173/1000 | Loss: 0.00001461
Iteration 174/1000 | Loss: 0.00001461
Iteration 175/1000 | Loss: 0.00001461
Iteration 176/1000 | Loss: 0.00001461
Iteration 177/1000 | Loss: 0.00001461
Iteration 178/1000 | Loss: 0.00001461
Iteration 179/1000 | Loss: 0.00001461
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001461
Iteration 187/1000 | Loss: 0.00001461
Iteration 188/1000 | Loss: 0.00001461
Iteration 189/1000 | Loss: 0.00001461
Iteration 190/1000 | Loss: 0.00001461
Iteration 191/1000 | Loss: 0.00001461
Iteration 192/1000 | Loss: 0.00001461
Iteration 193/1000 | Loss: 0.00001461
Iteration 194/1000 | Loss: 0.00001461
Iteration 195/1000 | Loss: 0.00001461
Iteration 196/1000 | Loss: 0.00001461
Iteration 197/1000 | Loss: 0.00001461
Iteration 198/1000 | Loss: 0.00001461
Iteration 199/1000 | Loss: 0.00001461
Iteration 200/1000 | Loss: 0.00001461
Iteration 201/1000 | Loss: 0.00001461
Iteration 202/1000 | Loss: 0.00001461
Iteration 203/1000 | Loss: 0.00001461
Iteration 204/1000 | Loss: 0.00001461
Iteration 205/1000 | Loss: 0.00001461
Iteration 206/1000 | Loss: 0.00001461
Iteration 207/1000 | Loss: 0.00001461
Iteration 208/1000 | Loss: 0.00001461
Iteration 209/1000 | Loss: 0.00001461
Iteration 210/1000 | Loss: 0.00001461
Iteration 211/1000 | Loss: 0.00001461
Iteration 212/1000 | Loss: 0.00001461
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.4606939657824114e-05, 1.4606939657824114e-05, 1.4606939657824114e-05, 1.4606939657824114e-05, 1.4606939657824114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4606939657824114e-05

Optimization complete. Final v2v error: 3.3478217124938965 mm

Highest mean error: 3.791896343231201 mm for frame 73

Lowest mean error: 2.910221576690674 mm for frame 80

Saving results

Total time: 39.58764338493347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919575
Iteration 2/25 | Loss: 0.00152459
Iteration 3/25 | Loss: 0.00126070
Iteration 4/25 | Loss: 0.00118325
Iteration 5/25 | Loss: 0.00116763
Iteration 6/25 | Loss: 0.00116237
Iteration 7/25 | Loss: 0.00116199
Iteration 8/25 | Loss: 0.00116036
Iteration 9/25 | Loss: 0.00114977
Iteration 10/25 | Loss: 0.00114755
Iteration 11/25 | Loss: 0.00114713
Iteration 12/25 | Loss: 0.00114709
Iteration 13/25 | Loss: 0.00114709
Iteration 14/25 | Loss: 0.00114709
Iteration 15/25 | Loss: 0.00114709
Iteration 16/25 | Loss: 0.00114709
Iteration 17/25 | Loss: 0.00114708
Iteration 18/25 | Loss: 0.00114708
Iteration 19/25 | Loss: 0.00114708
Iteration 20/25 | Loss: 0.00114708
Iteration 21/25 | Loss: 0.00114708
Iteration 22/25 | Loss: 0.00114707
Iteration 23/25 | Loss: 0.00114707
Iteration 24/25 | Loss: 0.00114707
Iteration 25/25 | Loss: 0.00114707

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.24846029
Iteration 2/25 | Loss: 0.00483767
Iteration 3/25 | Loss: 0.00483766
Iteration 4/25 | Loss: 0.00483766
Iteration 5/25 | Loss: 0.00483766
Iteration 6/25 | Loss: 0.00483766
Iteration 7/25 | Loss: 0.00483766
Iteration 8/25 | Loss: 0.00483766
Iteration 9/25 | Loss: 0.00483766
Iteration 10/25 | Loss: 0.00483766
Iteration 11/25 | Loss: 0.00483766
Iteration 12/25 | Loss: 0.00483766
Iteration 13/25 | Loss: 0.00483766
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.004837663844227791, 0.004837663844227791, 0.004837663844227791, 0.004837663844227791, 0.004837663844227791]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004837663844227791

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00483766
Iteration 2/1000 | Loss: 0.00003472
Iteration 3/1000 | Loss: 0.00002903
Iteration 4/1000 | Loss: 0.00002646
Iteration 5/1000 | Loss: 0.00002538
Iteration 6/1000 | Loss: 0.00028069
Iteration 7/1000 | Loss: 0.00018439
Iteration 8/1000 | Loss: 0.00016829
Iteration 9/1000 | Loss: 0.00002560
Iteration 10/1000 | Loss: 0.00002385
Iteration 11/1000 | Loss: 0.00002236
Iteration 12/1000 | Loss: 0.00002155
Iteration 13/1000 | Loss: 0.00002116
Iteration 14/1000 | Loss: 0.00002093
Iteration 15/1000 | Loss: 0.00002077
Iteration 16/1000 | Loss: 0.00002074
Iteration 17/1000 | Loss: 0.00002071
Iteration 18/1000 | Loss: 0.00002071
Iteration 19/1000 | Loss: 0.00002070
Iteration 20/1000 | Loss: 0.00002069
Iteration 21/1000 | Loss: 0.00002066
Iteration 22/1000 | Loss: 0.00002064
Iteration 23/1000 | Loss: 0.00002063
Iteration 24/1000 | Loss: 0.00002062
Iteration 25/1000 | Loss: 0.00002062
Iteration 26/1000 | Loss: 0.00002061
Iteration 27/1000 | Loss: 0.00002061
Iteration 28/1000 | Loss: 0.00002061
Iteration 29/1000 | Loss: 0.00002060
Iteration 30/1000 | Loss: 0.00002060
Iteration 31/1000 | Loss: 0.00002060
Iteration 32/1000 | Loss: 0.00002059
Iteration 33/1000 | Loss: 0.00002059
Iteration 34/1000 | Loss: 0.00002058
Iteration 35/1000 | Loss: 0.00002057
Iteration 36/1000 | Loss: 0.00002057
Iteration 37/1000 | Loss: 0.00002057
Iteration 38/1000 | Loss: 0.00002057
Iteration 39/1000 | Loss: 0.00002057
Iteration 40/1000 | Loss: 0.00002057
Iteration 41/1000 | Loss: 0.00002057
Iteration 42/1000 | Loss: 0.00002057
Iteration 43/1000 | Loss: 0.00002057
Iteration 44/1000 | Loss: 0.00002057
Iteration 45/1000 | Loss: 0.00002057
Iteration 46/1000 | Loss: 0.00002057
Iteration 47/1000 | Loss: 0.00002056
Iteration 48/1000 | Loss: 0.00002056
Iteration 49/1000 | Loss: 0.00002056
Iteration 50/1000 | Loss: 0.00002056
Iteration 51/1000 | Loss: 0.00002056
Iteration 52/1000 | Loss: 0.00002056
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 52. Stopping optimization.
Last 5 losses: [2.0564479200402275e-05, 2.0564479200402275e-05, 2.0564479200402275e-05, 2.0564479200402275e-05, 2.0564479200402275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0564479200402275e-05

Optimization complete. Final v2v error: 3.8648629188537598 mm

Highest mean error: 9.03750991821289 mm for frame 238

Lowest mean error: 3.2518718242645264 mm for frame 204

Saving results

Total time: 49.69742035865784
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00534966
Iteration 2/25 | Loss: 0.00128091
Iteration 3/25 | Loss: 0.00118404
Iteration 4/25 | Loss: 0.00116051
Iteration 5/25 | Loss: 0.00115215
Iteration 6/25 | Loss: 0.00115083
Iteration 7/25 | Loss: 0.00115083
Iteration 8/25 | Loss: 0.00115083
Iteration 9/25 | Loss: 0.00115083
Iteration 10/25 | Loss: 0.00115083
Iteration 11/25 | Loss: 0.00115083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011508323950693011, 0.0011508323950693011, 0.0011508323950693011, 0.0011508323950693011, 0.0011508323950693011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011508323950693011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96778476
Iteration 2/25 | Loss: 0.00547191
Iteration 3/25 | Loss: 0.00547189
Iteration 4/25 | Loss: 0.00547189
Iteration 5/25 | Loss: 0.00547189
Iteration 6/25 | Loss: 0.00547189
Iteration 7/25 | Loss: 0.00547189
Iteration 8/25 | Loss: 0.00547189
Iteration 9/25 | Loss: 0.00547189
Iteration 10/25 | Loss: 0.00547189
Iteration 11/25 | Loss: 0.00547189
Iteration 12/25 | Loss: 0.00547189
Iteration 13/25 | Loss: 0.00547189
Iteration 14/25 | Loss: 0.00547189
Iteration 15/25 | Loss: 0.00547189
Iteration 16/25 | Loss: 0.00547189
Iteration 17/25 | Loss: 0.00547189
Iteration 18/25 | Loss: 0.00547189
Iteration 19/25 | Loss: 0.00547189
Iteration 20/25 | Loss: 0.00547189
Iteration 21/25 | Loss: 0.00547189
Iteration 22/25 | Loss: 0.00547189
Iteration 23/25 | Loss: 0.00547189
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0054718866012990475, 0.0054718866012990475, 0.0054718866012990475, 0.0054718866012990475, 0.0054718866012990475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0054718866012990475

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00547189
Iteration 2/1000 | Loss: 0.00005348
Iteration 3/1000 | Loss: 0.00003896
Iteration 4/1000 | Loss: 0.00003350
Iteration 5/1000 | Loss: 0.00003118
Iteration 6/1000 | Loss: 0.00002976
Iteration 7/1000 | Loss: 0.00002841
Iteration 8/1000 | Loss: 0.00002728
Iteration 9/1000 | Loss: 0.00002659
Iteration 10/1000 | Loss: 0.00002624
Iteration 11/1000 | Loss: 0.00002601
Iteration 12/1000 | Loss: 0.00002578
Iteration 13/1000 | Loss: 0.00002564
Iteration 14/1000 | Loss: 0.00002554
Iteration 15/1000 | Loss: 0.00002551
Iteration 16/1000 | Loss: 0.00002551
Iteration 17/1000 | Loss: 0.00002550
Iteration 18/1000 | Loss: 0.00002550
Iteration 19/1000 | Loss: 0.00002550
Iteration 20/1000 | Loss: 0.00002548
Iteration 21/1000 | Loss: 0.00002548
Iteration 22/1000 | Loss: 0.00002547
Iteration 23/1000 | Loss: 0.00002547
Iteration 24/1000 | Loss: 0.00002546
Iteration 25/1000 | Loss: 0.00002546
Iteration 26/1000 | Loss: 0.00002546
Iteration 27/1000 | Loss: 0.00002545
Iteration 28/1000 | Loss: 0.00002545
Iteration 29/1000 | Loss: 0.00002545
Iteration 30/1000 | Loss: 0.00002544
Iteration 31/1000 | Loss: 0.00002544
Iteration 32/1000 | Loss: 0.00002543
Iteration 33/1000 | Loss: 0.00002542
Iteration 34/1000 | Loss: 0.00002542
Iteration 35/1000 | Loss: 0.00002542
Iteration 36/1000 | Loss: 0.00002542
Iteration 37/1000 | Loss: 0.00002541
Iteration 38/1000 | Loss: 0.00002541
Iteration 39/1000 | Loss: 0.00002541
Iteration 40/1000 | Loss: 0.00002540
Iteration 41/1000 | Loss: 0.00002540
Iteration 42/1000 | Loss: 0.00002539
Iteration 43/1000 | Loss: 0.00002538
Iteration 44/1000 | Loss: 0.00002538
Iteration 45/1000 | Loss: 0.00002538
Iteration 46/1000 | Loss: 0.00002538
Iteration 47/1000 | Loss: 0.00002537
Iteration 48/1000 | Loss: 0.00002537
Iteration 49/1000 | Loss: 0.00002537
Iteration 50/1000 | Loss: 0.00002537
Iteration 51/1000 | Loss: 0.00002537
Iteration 52/1000 | Loss: 0.00002536
Iteration 53/1000 | Loss: 0.00002536
Iteration 54/1000 | Loss: 0.00002536
Iteration 55/1000 | Loss: 0.00002536
Iteration 56/1000 | Loss: 0.00002536
Iteration 57/1000 | Loss: 0.00002536
Iteration 58/1000 | Loss: 0.00002536
Iteration 59/1000 | Loss: 0.00002535
Iteration 60/1000 | Loss: 0.00002535
Iteration 61/1000 | Loss: 0.00002535
Iteration 62/1000 | Loss: 0.00002535
Iteration 63/1000 | Loss: 0.00002535
Iteration 64/1000 | Loss: 0.00002535
Iteration 65/1000 | Loss: 0.00002534
Iteration 66/1000 | Loss: 0.00002534
Iteration 67/1000 | Loss: 0.00002534
Iteration 68/1000 | Loss: 0.00002534
Iteration 69/1000 | Loss: 0.00002534
Iteration 70/1000 | Loss: 0.00002534
Iteration 71/1000 | Loss: 0.00002534
Iteration 72/1000 | Loss: 0.00002534
Iteration 73/1000 | Loss: 0.00002534
Iteration 74/1000 | Loss: 0.00002534
Iteration 75/1000 | Loss: 0.00002534
Iteration 76/1000 | Loss: 0.00002534
Iteration 77/1000 | Loss: 0.00002534
Iteration 78/1000 | Loss: 0.00002534
Iteration 79/1000 | Loss: 0.00002534
Iteration 80/1000 | Loss: 0.00002534
Iteration 81/1000 | Loss: 0.00002534
Iteration 82/1000 | Loss: 0.00002534
Iteration 83/1000 | Loss: 0.00002534
Iteration 84/1000 | Loss: 0.00002534
Iteration 85/1000 | Loss: 0.00002534
Iteration 86/1000 | Loss: 0.00002534
Iteration 87/1000 | Loss: 0.00002534
Iteration 88/1000 | Loss: 0.00002534
Iteration 89/1000 | Loss: 0.00002534
Iteration 90/1000 | Loss: 0.00002534
Iteration 91/1000 | Loss: 0.00002534
Iteration 92/1000 | Loss: 0.00002534
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [2.5342182198073715e-05, 2.5342182198073715e-05, 2.5342182198073715e-05, 2.5342182198073715e-05, 2.5342182198073715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5342182198073715e-05

Optimization complete. Final v2v error: 4.4011054039001465 mm

Highest mean error: 5.10521936416626 mm for frame 215

Lowest mean error: 3.942631721496582 mm for frame 109

Saving results

Total time: 37.36655521392822
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00844067
Iteration 2/25 | Loss: 0.00140726
Iteration 3/25 | Loss: 0.00124765
Iteration 4/25 | Loss: 0.00121318
Iteration 5/25 | Loss: 0.00120517
Iteration 6/25 | Loss: 0.00120365
Iteration 7/25 | Loss: 0.00120365
Iteration 8/25 | Loss: 0.00120365
Iteration 9/25 | Loss: 0.00120365
Iteration 10/25 | Loss: 0.00120365
Iteration 11/25 | Loss: 0.00120365
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012036451371386647, 0.0012036451371386647, 0.0012036451371386647, 0.0012036451371386647, 0.0012036451371386647]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012036451371386647

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76652658
Iteration 2/25 | Loss: 0.00565024
Iteration 3/25 | Loss: 0.00565021
Iteration 4/25 | Loss: 0.00565021
Iteration 5/25 | Loss: 0.00565021
Iteration 6/25 | Loss: 0.00565021
Iteration 7/25 | Loss: 0.00565021
Iteration 8/25 | Loss: 0.00565021
Iteration 9/25 | Loss: 0.00565020
Iteration 10/25 | Loss: 0.00565020
Iteration 11/25 | Loss: 0.00565020
Iteration 12/25 | Loss: 0.00565020
Iteration 13/25 | Loss: 0.00565020
Iteration 14/25 | Loss: 0.00565020
Iteration 15/25 | Loss: 0.00565020
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005650204140692949, 0.005650204140692949, 0.005650204140692949, 0.005650204140692949, 0.005650204140692949]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005650204140692949

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00565020
Iteration 2/1000 | Loss: 0.00005293
Iteration 3/1000 | Loss: 0.00003719
Iteration 4/1000 | Loss: 0.00003126
Iteration 5/1000 | Loss: 0.00002911
Iteration 6/1000 | Loss: 0.00002773
Iteration 7/1000 | Loss: 0.00002668
Iteration 8/1000 | Loss: 0.00002592
Iteration 9/1000 | Loss: 0.00002553
Iteration 10/1000 | Loss: 0.00002545
Iteration 11/1000 | Loss: 0.00002534
Iteration 12/1000 | Loss: 0.00002527
Iteration 13/1000 | Loss: 0.00002517
Iteration 14/1000 | Loss: 0.00002503
Iteration 15/1000 | Loss: 0.00002492
Iteration 16/1000 | Loss: 0.00002486
Iteration 17/1000 | Loss: 0.00002466
Iteration 18/1000 | Loss: 0.00002453
Iteration 19/1000 | Loss: 0.00002441
Iteration 20/1000 | Loss: 0.00002440
Iteration 21/1000 | Loss: 0.00002439
Iteration 22/1000 | Loss: 0.00002438
Iteration 23/1000 | Loss: 0.00002435
Iteration 24/1000 | Loss: 0.00002435
Iteration 25/1000 | Loss: 0.00002435
Iteration 26/1000 | Loss: 0.00002435
Iteration 27/1000 | Loss: 0.00002434
Iteration 28/1000 | Loss: 0.00002434
Iteration 29/1000 | Loss: 0.00002434
Iteration 30/1000 | Loss: 0.00002434
Iteration 31/1000 | Loss: 0.00002434
Iteration 32/1000 | Loss: 0.00002434
Iteration 33/1000 | Loss: 0.00002434
Iteration 34/1000 | Loss: 0.00002434
Iteration 35/1000 | Loss: 0.00002434
Iteration 36/1000 | Loss: 0.00002434
Iteration 37/1000 | Loss: 0.00002434
Iteration 38/1000 | Loss: 0.00002433
Iteration 39/1000 | Loss: 0.00002432
Iteration 40/1000 | Loss: 0.00002431
Iteration 41/1000 | Loss: 0.00002431
Iteration 42/1000 | Loss: 0.00002430
Iteration 43/1000 | Loss: 0.00002429
Iteration 44/1000 | Loss: 0.00002429
Iteration 45/1000 | Loss: 0.00002429
Iteration 46/1000 | Loss: 0.00002428
Iteration 47/1000 | Loss: 0.00002426
Iteration 48/1000 | Loss: 0.00002426
Iteration 49/1000 | Loss: 0.00002425
Iteration 50/1000 | Loss: 0.00002425
Iteration 51/1000 | Loss: 0.00002425
Iteration 52/1000 | Loss: 0.00002424
Iteration 53/1000 | Loss: 0.00002424
Iteration 54/1000 | Loss: 0.00002423
Iteration 55/1000 | Loss: 0.00002423
Iteration 56/1000 | Loss: 0.00002423
Iteration 57/1000 | Loss: 0.00002423
Iteration 58/1000 | Loss: 0.00002422
Iteration 59/1000 | Loss: 0.00002422
Iteration 60/1000 | Loss: 0.00002421
Iteration 61/1000 | Loss: 0.00002421
Iteration 62/1000 | Loss: 0.00002421
Iteration 63/1000 | Loss: 0.00002421
Iteration 64/1000 | Loss: 0.00002420
Iteration 65/1000 | Loss: 0.00002420
Iteration 66/1000 | Loss: 0.00002420
Iteration 67/1000 | Loss: 0.00002420
Iteration 68/1000 | Loss: 0.00002420
Iteration 69/1000 | Loss: 0.00002419
Iteration 70/1000 | Loss: 0.00002419
Iteration 71/1000 | Loss: 0.00002419
Iteration 72/1000 | Loss: 0.00002419
Iteration 73/1000 | Loss: 0.00002419
Iteration 74/1000 | Loss: 0.00002419
Iteration 75/1000 | Loss: 0.00002419
Iteration 76/1000 | Loss: 0.00002418
Iteration 77/1000 | Loss: 0.00002418
Iteration 78/1000 | Loss: 0.00002418
Iteration 79/1000 | Loss: 0.00002418
Iteration 80/1000 | Loss: 0.00002418
Iteration 81/1000 | Loss: 0.00002418
Iteration 82/1000 | Loss: 0.00002418
Iteration 83/1000 | Loss: 0.00002418
Iteration 84/1000 | Loss: 0.00002417
Iteration 85/1000 | Loss: 0.00002417
Iteration 86/1000 | Loss: 0.00002417
Iteration 87/1000 | Loss: 0.00002417
Iteration 88/1000 | Loss: 0.00002417
Iteration 89/1000 | Loss: 0.00002417
Iteration 90/1000 | Loss: 0.00002417
Iteration 91/1000 | Loss: 0.00002416
Iteration 92/1000 | Loss: 0.00002416
Iteration 93/1000 | Loss: 0.00002416
Iteration 94/1000 | Loss: 0.00002416
Iteration 95/1000 | Loss: 0.00002416
Iteration 96/1000 | Loss: 0.00002416
Iteration 97/1000 | Loss: 0.00002415
Iteration 98/1000 | Loss: 0.00002415
Iteration 99/1000 | Loss: 0.00002415
Iteration 100/1000 | Loss: 0.00002415
Iteration 101/1000 | Loss: 0.00002415
Iteration 102/1000 | Loss: 0.00002415
Iteration 103/1000 | Loss: 0.00002414
Iteration 104/1000 | Loss: 0.00002414
Iteration 105/1000 | Loss: 0.00002414
Iteration 106/1000 | Loss: 0.00002414
Iteration 107/1000 | Loss: 0.00002414
Iteration 108/1000 | Loss: 0.00002414
Iteration 109/1000 | Loss: 0.00002414
Iteration 110/1000 | Loss: 0.00002414
Iteration 111/1000 | Loss: 0.00002414
Iteration 112/1000 | Loss: 0.00002414
Iteration 113/1000 | Loss: 0.00002414
Iteration 114/1000 | Loss: 0.00002414
Iteration 115/1000 | Loss: 0.00002414
Iteration 116/1000 | Loss: 0.00002414
Iteration 117/1000 | Loss: 0.00002413
Iteration 118/1000 | Loss: 0.00002413
Iteration 119/1000 | Loss: 0.00002413
Iteration 120/1000 | Loss: 0.00002413
Iteration 121/1000 | Loss: 0.00002413
Iteration 122/1000 | Loss: 0.00002413
Iteration 123/1000 | Loss: 0.00002413
Iteration 124/1000 | Loss: 0.00002412
Iteration 125/1000 | Loss: 0.00002412
Iteration 126/1000 | Loss: 0.00002412
Iteration 127/1000 | Loss: 0.00002412
Iteration 128/1000 | Loss: 0.00002412
Iteration 129/1000 | Loss: 0.00002412
Iteration 130/1000 | Loss: 0.00002412
Iteration 131/1000 | Loss: 0.00002412
Iteration 132/1000 | Loss: 0.00002411
Iteration 133/1000 | Loss: 0.00002411
Iteration 134/1000 | Loss: 0.00002411
Iteration 135/1000 | Loss: 0.00002411
Iteration 136/1000 | Loss: 0.00002411
Iteration 137/1000 | Loss: 0.00002411
Iteration 138/1000 | Loss: 0.00002411
Iteration 139/1000 | Loss: 0.00002411
Iteration 140/1000 | Loss: 0.00002411
Iteration 141/1000 | Loss: 0.00002411
Iteration 142/1000 | Loss: 0.00002411
Iteration 143/1000 | Loss: 0.00002410
Iteration 144/1000 | Loss: 0.00002410
Iteration 145/1000 | Loss: 0.00002410
Iteration 146/1000 | Loss: 0.00002410
Iteration 147/1000 | Loss: 0.00002410
Iteration 148/1000 | Loss: 0.00002410
Iteration 149/1000 | Loss: 0.00002410
Iteration 150/1000 | Loss: 0.00002410
Iteration 151/1000 | Loss: 0.00002410
Iteration 152/1000 | Loss: 0.00002410
Iteration 153/1000 | Loss: 0.00002410
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.410137494734954e-05, 2.410137494734954e-05, 2.410137494734954e-05, 2.410137494734954e-05, 2.410137494734954e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.410137494734954e-05

Optimization complete. Final v2v error: 4.315700531005859 mm

Highest mean error: 4.773474216461182 mm for frame 5

Lowest mean error: 4.001218795776367 mm for frame 116

Saving results

Total time: 45.14131546020508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413136
Iteration 2/25 | Loss: 0.00125022
Iteration 3/25 | Loss: 0.00113897
Iteration 4/25 | Loss: 0.00112785
Iteration 5/25 | Loss: 0.00112259
Iteration 6/25 | Loss: 0.00112103
Iteration 7/25 | Loss: 0.00112103
Iteration 8/25 | Loss: 0.00112103
Iteration 9/25 | Loss: 0.00112103
Iteration 10/25 | Loss: 0.00112103
Iteration 11/25 | Loss: 0.00112103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00112102588173002, 0.00112102588173002, 0.00112102588173002, 0.00112102588173002, 0.00112102588173002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00112102588173002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.96306372
Iteration 2/25 | Loss: 0.00571348
Iteration 3/25 | Loss: 0.00571347
Iteration 4/25 | Loss: 0.00571347
Iteration 5/25 | Loss: 0.00571347
Iteration 6/25 | Loss: 0.00571347
Iteration 7/25 | Loss: 0.00571347
Iteration 8/25 | Loss: 0.00571347
Iteration 9/25 | Loss: 0.00571347
Iteration 10/25 | Loss: 0.00571347
Iteration 11/25 | Loss: 0.00571347
Iteration 12/25 | Loss: 0.00571347
Iteration 13/25 | Loss: 0.00571347
Iteration 14/25 | Loss: 0.00571347
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.005713472608476877, 0.005713472608476877, 0.005713472608476877, 0.005713472608476877, 0.005713472608476877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005713472608476877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00571347
Iteration 2/1000 | Loss: 0.00003466
Iteration 3/1000 | Loss: 0.00002433
Iteration 4/1000 | Loss: 0.00002065
Iteration 5/1000 | Loss: 0.00001906
Iteration 6/1000 | Loss: 0.00001827
Iteration 7/1000 | Loss: 0.00001772
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001704
Iteration 10/1000 | Loss: 0.00001677
Iteration 11/1000 | Loss: 0.00001661
Iteration 12/1000 | Loss: 0.00001654
Iteration 13/1000 | Loss: 0.00001654
Iteration 14/1000 | Loss: 0.00001648
Iteration 15/1000 | Loss: 0.00001648
Iteration 16/1000 | Loss: 0.00001647
Iteration 17/1000 | Loss: 0.00001647
Iteration 18/1000 | Loss: 0.00001646
Iteration 19/1000 | Loss: 0.00001641
Iteration 20/1000 | Loss: 0.00001637
Iteration 21/1000 | Loss: 0.00001636
Iteration 22/1000 | Loss: 0.00001636
Iteration 23/1000 | Loss: 0.00001636
Iteration 24/1000 | Loss: 0.00001636
Iteration 25/1000 | Loss: 0.00001636
Iteration 26/1000 | Loss: 0.00001636
Iteration 27/1000 | Loss: 0.00001635
Iteration 28/1000 | Loss: 0.00001635
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001633
Iteration 31/1000 | Loss: 0.00001630
Iteration 32/1000 | Loss: 0.00001630
Iteration 33/1000 | Loss: 0.00001630
Iteration 34/1000 | Loss: 0.00001630
Iteration 35/1000 | Loss: 0.00001630
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001626
Iteration 38/1000 | Loss: 0.00001626
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001626
Iteration 42/1000 | Loss: 0.00001626
Iteration 43/1000 | Loss: 0.00001626
Iteration 44/1000 | Loss: 0.00001625
Iteration 45/1000 | Loss: 0.00001625
Iteration 46/1000 | Loss: 0.00001625
Iteration 47/1000 | Loss: 0.00001625
Iteration 48/1000 | Loss: 0.00001625
Iteration 49/1000 | Loss: 0.00001625
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001623
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001623
Iteration 56/1000 | Loss: 0.00001623
Iteration 57/1000 | Loss: 0.00001622
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001622
Iteration 62/1000 | Loss: 0.00001622
Iteration 63/1000 | Loss: 0.00001621
Iteration 64/1000 | Loss: 0.00001621
Iteration 65/1000 | Loss: 0.00001621
Iteration 66/1000 | Loss: 0.00001620
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001619
Iteration 69/1000 | Loss: 0.00001619
Iteration 70/1000 | Loss: 0.00001619
Iteration 71/1000 | Loss: 0.00001619
Iteration 72/1000 | Loss: 0.00001619
Iteration 73/1000 | Loss: 0.00001619
Iteration 74/1000 | Loss: 0.00001618
Iteration 75/1000 | Loss: 0.00001618
Iteration 76/1000 | Loss: 0.00001618
Iteration 77/1000 | Loss: 0.00001618
Iteration 78/1000 | Loss: 0.00001617
Iteration 79/1000 | Loss: 0.00001617
Iteration 80/1000 | Loss: 0.00001616
Iteration 81/1000 | Loss: 0.00001616
Iteration 82/1000 | Loss: 0.00001616
Iteration 83/1000 | Loss: 0.00001616
Iteration 84/1000 | Loss: 0.00001616
Iteration 85/1000 | Loss: 0.00001615
Iteration 86/1000 | Loss: 0.00001615
Iteration 87/1000 | Loss: 0.00001615
Iteration 88/1000 | Loss: 0.00001615
Iteration 89/1000 | Loss: 0.00001614
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001612
Iteration 97/1000 | Loss: 0.00001612
Iteration 98/1000 | Loss: 0.00001612
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001612
Iteration 102/1000 | Loss: 0.00001612
Iteration 103/1000 | Loss: 0.00001612
Iteration 104/1000 | Loss: 0.00001612
Iteration 105/1000 | Loss: 0.00001612
Iteration 106/1000 | Loss: 0.00001612
Iteration 107/1000 | Loss: 0.00001612
Iteration 108/1000 | Loss: 0.00001612
Iteration 109/1000 | Loss: 0.00001612
Iteration 110/1000 | Loss: 0.00001612
Iteration 111/1000 | Loss: 0.00001612
Iteration 112/1000 | Loss: 0.00001612
Iteration 113/1000 | Loss: 0.00001612
Iteration 114/1000 | Loss: 0.00001612
Iteration 115/1000 | Loss: 0.00001612
Iteration 116/1000 | Loss: 0.00001612
Iteration 117/1000 | Loss: 0.00001612
Iteration 118/1000 | Loss: 0.00001612
Iteration 119/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.611846892046742e-05, 1.611846892046742e-05, 1.611846892046742e-05, 1.611846892046742e-05, 1.611846892046742e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.611846892046742e-05

Optimization complete. Final v2v error: 3.5185370445251465 mm

Highest mean error: 3.9165070056915283 mm for frame 132

Lowest mean error: 3.2187907695770264 mm for frame 0

Saving results

Total time: 38.137791872024536
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516692
Iteration 2/25 | Loss: 0.00126481
Iteration 3/25 | Loss: 0.00115833
Iteration 4/25 | Loss: 0.00113815
Iteration 5/25 | Loss: 0.00112991
Iteration 6/25 | Loss: 0.00112827
Iteration 7/25 | Loss: 0.00112794
Iteration 8/25 | Loss: 0.00112794
Iteration 9/25 | Loss: 0.00112794
Iteration 10/25 | Loss: 0.00112794
Iteration 11/25 | Loss: 0.00112794
Iteration 12/25 | Loss: 0.00112794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011279447935521603, 0.0011279447935521603, 0.0011279447935521603, 0.0011279447935521603, 0.0011279447935521603]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011279447935521603

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.93872786
Iteration 2/25 | Loss: 0.00504669
Iteration 3/25 | Loss: 0.00504668
Iteration 4/25 | Loss: 0.00504668
Iteration 5/25 | Loss: 0.00504668
Iteration 6/25 | Loss: 0.00504668
Iteration 7/25 | Loss: 0.00504668
Iteration 8/25 | Loss: 0.00504668
Iteration 9/25 | Loss: 0.00504668
Iteration 10/25 | Loss: 0.00504668
Iteration 11/25 | Loss: 0.00504668
Iteration 12/25 | Loss: 0.00504668
Iteration 13/25 | Loss: 0.00504668
Iteration 14/25 | Loss: 0.00504668
Iteration 15/25 | Loss: 0.00504668
Iteration 16/25 | Loss: 0.00504668
Iteration 17/25 | Loss: 0.00504668
Iteration 18/25 | Loss: 0.00504668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.005046682897955179, 0.005046682897955179, 0.005046682897955179, 0.005046682897955179, 0.005046682897955179]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005046682897955179

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00504668
Iteration 2/1000 | Loss: 0.00004151
Iteration 3/1000 | Loss: 0.00002758
Iteration 4/1000 | Loss: 0.00002375
Iteration 5/1000 | Loss: 0.00002194
Iteration 6/1000 | Loss: 0.00002071
Iteration 7/1000 | Loss: 0.00001994
Iteration 8/1000 | Loss: 0.00001930
Iteration 9/1000 | Loss: 0.00001892
Iteration 10/1000 | Loss: 0.00001870
Iteration 11/1000 | Loss: 0.00001861
Iteration 12/1000 | Loss: 0.00001861
Iteration 13/1000 | Loss: 0.00001860
Iteration 14/1000 | Loss: 0.00001851
Iteration 15/1000 | Loss: 0.00001847
Iteration 16/1000 | Loss: 0.00001847
Iteration 17/1000 | Loss: 0.00001847
Iteration 18/1000 | Loss: 0.00001846
Iteration 19/1000 | Loss: 0.00001842
Iteration 20/1000 | Loss: 0.00001842
Iteration 21/1000 | Loss: 0.00001842
Iteration 22/1000 | Loss: 0.00001841
Iteration 23/1000 | Loss: 0.00001840
Iteration 24/1000 | Loss: 0.00001840
Iteration 25/1000 | Loss: 0.00001839
Iteration 26/1000 | Loss: 0.00001838
Iteration 27/1000 | Loss: 0.00001838
Iteration 28/1000 | Loss: 0.00001837
Iteration 29/1000 | Loss: 0.00001837
Iteration 30/1000 | Loss: 0.00001837
Iteration 31/1000 | Loss: 0.00001837
Iteration 32/1000 | Loss: 0.00001837
Iteration 33/1000 | Loss: 0.00001836
Iteration 34/1000 | Loss: 0.00001836
Iteration 35/1000 | Loss: 0.00001836
Iteration 36/1000 | Loss: 0.00001835
Iteration 37/1000 | Loss: 0.00001835
Iteration 38/1000 | Loss: 0.00001835
Iteration 39/1000 | Loss: 0.00001835
Iteration 40/1000 | Loss: 0.00001835
Iteration 41/1000 | Loss: 0.00001834
Iteration 42/1000 | Loss: 0.00001834
Iteration 43/1000 | Loss: 0.00001834
Iteration 44/1000 | Loss: 0.00001834
Iteration 45/1000 | Loss: 0.00001834
Iteration 46/1000 | Loss: 0.00001834
Iteration 47/1000 | Loss: 0.00001834
Iteration 48/1000 | Loss: 0.00001834
Iteration 49/1000 | Loss: 0.00001834
Iteration 50/1000 | Loss: 0.00001834
Iteration 51/1000 | Loss: 0.00001834
Iteration 52/1000 | Loss: 0.00001834
Iteration 53/1000 | Loss: 0.00001833
Iteration 54/1000 | Loss: 0.00001833
Iteration 55/1000 | Loss: 0.00001833
Iteration 56/1000 | Loss: 0.00001833
Iteration 57/1000 | Loss: 0.00001832
Iteration 58/1000 | Loss: 0.00001832
Iteration 59/1000 | Loss: 0.00001832
Iteration 60/1000 | Loss: 0.00001832
Iteration 61/1000 | Loss: 0.00001832
Iteration 62/1000 | Loss: 0.00001831
Iteration 63/1000 | Loss: 0.00001831
Iteration 64/1000 | Loss: 0.00001831
Iteration 65/1000 | Loss: 0.00001831
Iteration 66/1000 | Loss: 0.00001831
Iteration 67/1000 | Loss: 0.00001831
Iteration 68/1000 | Loss: 0.00001831
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001831
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001831
Iteration 76/1000 | Loss: 0.00001830
Iteration 77/1000 | Loss: 0.00001830
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00001830
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001829
Iteration 82/1000 | Loss: 0.00001829
Iteration 83/1000 | Loss: 0.00001829
Iteration 84/1000 | Loss: 0.00001829
Iteration 85/1000 | Loss: 0.00001829
Iteration 86/1000 | Loss: 0.00001829
Iteration 87/1000 | Loss: 0.00001829
Iteration 88/1000 | Loss: 0.00001829
Iteration 89/1000 | Loss: 0.00001828
Iteration 90/1000 | Loss: 0.00001828
Iteration 91/1000 | Loss: 0.00001828
Iteration 92/1000 | Loss: 0.00001828
Iteration 93/1000 | Loss: 0.00001828
Iteration 94/1000 | Loss: 0.00001828
Iteration 95/1000 | Loss: 0.00001828
Iteration 96/1000 | Loss: 0.00001827
Iteration 97/1000 | Loss: 0.00001827
Iteration 98/1000 | Loss: 0.00001827
Iteration 99/1000 | Loss: 0.00001827
Iteration 100/1000 | Loss: 0.00001826
Iteration 101/1000 | Loss: 0.00001825
Iteration 102/1000 | Loss: 0.00001825
Iteration 103/1000 | Loss: 0.00001825
Iteration 104/1000 | Loss: 0.00001825
Iteration 105/1000 | Loss: 0.00001825
Iteration 106/1000 | Loss: 0.00001825
Iteration 107/1000 | Loss: 0.00001825
Iteration 108/1000 | Loss: 0.00001825
Iteration 109/1000 | Loss: 0.00001824
Iteration 110/1000 | Loss: 0.00001824
Iteration 111/1000 | Loss: 0.00001824
Iteration 112/1000 | Loss: 0.00001824
Iteration 113/1000 | Loss: 0.00001824
Iteration 114/1000 | Loss: 0.00001823
Iteration 115/1000 | Loss: 0.00001823
Iteration 116/1000 | Loss: 0.00001823
Iteration 117/1000 | Loss: 0.00001823
Iteration 118/1000 | Loss: 0.00001823
Iteration 119/1000 | Loss: 0.00001823
Iteration 120/1000 | Loss: 0.00001823
Iteration 121/1000 | Loss: 0.00001823
Iteration 122/1000 | Loss: 0.00001823
Iteration 123/1000 | Loss: 0.00001822
Iteration 124/1000 | Loss: 0.00001822
Iteration 125/1000 | Loss: 0.00001822
Iteration 126/1000 | Loss: 0.00001822
Iteration 127/1000 | Loss: 0.00001822
Iteration 128/1000 | Loss: 0.00001822
Iteration 129/1000 | Loss: 0.00001822
Iteration 130/1000 | Loss: 0.00001821
Iteration 131/1000 | Loss: 0.00001821
Iteration 132/1000 | Loss: 0.00001821
Iteration 133/1000 | Loss: 0.00001821
Iteration 134/1000 | Loss: 0.00001821
Iteration 135/1000 | Loss: 0.00001821
Iteration 136/1000 | Loss: 0.00001821
Iteration 137/1000 | Loss: 0.00001821
Iteration 138/1000 | Loss: 0.00001820
Iteration 139/1000 | Loss: 0.00001820
Iteration 140/1000 | Loss: 0.00001820
Iteration 141/1000 | Loss: 0.00001820
Iteration 142/1000 | Loss: 0.00001820
Iteration 143/1000 | Loss: 0.00001820
Iteration 144/1000 | Loss: 0.00001820
Iteration 145/1000 | Loss: 0.00001820
Iteration 146/1000 | Loss: 0.00001820
Iteration 147/1000 | Loss: 0.00001819
Iteration 148/1000 | Loss: 0.00001819
Iteration 149/1000 | Loss: 0.00001819
Iteration 150/1000 | Loss: 0.00001819
Iteration 151/1000 | Loss: 0.00001819
Iteration 152/1000 | Loss: 0.00001819
Iteration 153/1000 | Loss: 0.00001819
Iteration 154/1000 | Loss: 0.00001819
Iteration 155/1000 | Loss: 0.00001819
Iteration 156/1000 | Loss: 0.00001819
Iteration 157/1000 | Loss: 0.00001819
Iteration 158/1000 | Loss: 0.00001819
Iteration 159/1000 | Loss: 0.00001819
Iteration 160/1000 | Loss: 0.00001819
Iteration 161/1000 | Loss: 0.00001819
Iteration 162/1000 | Loss: 0.00001819
Iteration 163/1000 | Loss: 0.00001819
Iteration 164/1000 | Loss: 0.00001819
Iteration 165/1000 | Loss: 0.00001818
Iteration 166/1000 | Loss: 0.00001818
Iteration 167/1000 | Loss: 0.00001818
Iteration 168/1000 | Loss: 0.00001818
Iteration 169/1000 | Loss: 0.00001818
Iteration 170/1000 | Loss: 0.00001818
Iteration 171/1000 | Loss: 0.00001818
Iteration 172/1000 | Loss: 0.00001818
Iteration 173/1000 | Loss: 0.00001818
Iteration 174/1000 | Loss: 0.00001818
Iteration 175/1000 | Loss: 0.00001818
Iteration 176/1000 | Loss: 0.00001818
Iteration 177/1000 | Loss: 0.00001818
Iteration 178/1000 | Loss: 0.00001818
Iteration 179/1000 | Loss: 0.00001818
Iteration 180/1000 | Loss: 0.00001818
Iteration 181/1000 | Loss: 0.00001817
Iteration 182/1000 | Loss: 0.00001817
Iteration 183/1000 | Loss: 0.00001817
Iteration 184/1000 | Loss: 0.00001817
Iteration 185/1000 | Loss: 0.00001817
Iteration 186/1000 | Loss: 0.00001817
Iteration 187/1000 | Loss: 0.00001817
Iteration 188/1000 | Loss: 0.00001817
Iteration 189/1000 | Loss: 0.00001817
Iteration 190/1000 | Loss: 0.00001817
Iteration 191/1000 | Loss: 0.00001817
Iteration 192/1000 | Loss: 0.00001817
Iteration 193/1000 | Loss: 0.00001817
Iteration 194/1000 | Loss: 0.00001817
Iteration 195/1000 | Loss: 0.00001817
Iteration 196/1000 | Loss: 0.00001817
Iteration 197/1000 | Loss: 0.00001817
Iteration 198/1000 | Loss: 0.00001817
Iteration 199/1000 | Loss: 0.00001817
Iteration 200/1000 | Loss: 0.00001817
Iteration 201/1000 | Loss: 0.00001817
Iteration 202/1000 | Loss: 0.00001817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.8171451301896013e-05, 1.8171451301896013e-05, 1.8171451301896013e-05, 1.8171451301896013e-05, 1.8171451301896013e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8171451301896013e-05

Optimization complete. Final v2v error: 3.6459898948669434 mm

Highest mean error: 4.261958599090576 mm for frame 37

Lowest mean error: 3.4114015102386475 mm for frame 2

Saving results

Total time: 35.20108985900879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00640728
Iteration 2/25 | Loss: 0.00161445
Iteration 3/25 | Loss: 0.00134209
Iteration 4/25 | Loss: 0.00129736
Iteration 5/25 | Loss: 0.00129142
Iteration 6/25 | Loss: 0.00129068
Iteration 7/25 | Loss: 0.00129068
Iteration 8/25 | Loss: 0.00129068
Iteration 9/25 | Loss: 0.00129068
Iteration 10/25 | Loss: 0.00129068
Iteration 11/25 | Loss: 0.00129068
Iteration 12/25 | Loss: 0.00129068
Iteration 13/25 | Loss: 0.00129068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012906825868412852, 0.0012906825868412852, 0.0012906825868412852, 0.0012906825868412852, 0.0012906825868412852]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012906825868412852

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39991593
Iteration 2/25 | Loss: 0.00560775
Iteration 3/25 | Loss: 0.00560775
Iteration 4/25 | Loss: 0.00560775
Iteration 5/25 | Loss: 0.00560775
Iteration 6/25 | Loss: 0.00560775
Iteration 7/25 | Loss: 0.00560775
Iteration 8/25 | Loss: 0.00560775
Iteration 9/25 | Loss: 0.00560775
Iteration 10/25 | Loss: 0.00560775
Iteration 11/25 | Loss: 0.00560775
Iteration 12/25 | Loss: 0.00560775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0056077465415000916, 0.0056077465415000916, 0.0056077465415000916, 0.0056077465415000916, 0.0056077465415000916]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0056077465415000916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00560775
Iteration 2/1000 | Loss: 0.00004252
Iteration 3/1000 | Loss: 0.00003076
Iteration 4/1000 | Loss: 0.00002771
Iteration 5/1000 | Loss: 0.00002584
Iteration 6/1000 | Loss: 0.00002478
Iteration 7/1000 | Loss: 0.00002399
Iteration 8/1000 | Loss: 0.00002343
Iteration 9/1000 | Loss: 0.00002310
Iteration 10/1000 | Loss: 0.00002285
Iteration 11/1000 | Loss: 0.00002263
Iteration 12/1000 | Loss: 0.00002243
Iteration 13/1000 | Loss: 0.00002241
Iteration 14/1000 | Loss: 0.00002234
Iteration 15/1000 | Loss: 0.00002228
Iteration 16/1000 | Loss: 0.00002228
Iteration 17/1000 | Loss: 0.00002226
Iteration 18/1000 | Loss: 0.00002226
Iteration 19/1000 | Loss: 0.00002225
Iteration 20/1000 | Loss: 0.00002223
Iteration 21/1000 | Loss: 0.00002223
Iteration 22/1000 | Loss: 0.00002223
Iteration 23/1000 | Loss: 0.00002223
Iteration 24/1000 | Loss: 0.00002223
Iteration 25/1000 | Loss: 0.00002222
Iteration 26/1000 | Loss: 0.00002222
Iteration 27/1000 | Loss: 0.00002222
Iteration 28/1000 | Loss: 0.00002221
Iteration 29/1000 | Loss: 0.00002221
Iteration 30/1000 | Loss: 0.00002221
Iteration 31/1000 | Loss: 0.00002221
Iteration 32/1000 | Loss: 0.00002221
Iteration 33/1000 | Loss: 0.00002221
Iteration 34/1000 | Loss: 0.00002221
Iteration 35/1000 | Loss: 0.00002221
Iteration 36/1000 | Loss: 0.00002221
Iteration 37/1000 | Loss: 0.00002220
Iteration 38/1000 | Loss: 0.00002220
Iteration 39/1000 | Loss: 0.00002220
Iteration 40/1000 | Loss: 0.00002220
Iteration 41/1000 | Loss: 0.00002220
Iteration 42/1000 | Loss: 0.00002219
Iteration 43/1000 | Loss: 0.00002219
Iteration 44/1000 | Loss: 0.00002219
Iteration 45/1000 | Loss: 0.00002219
Iteration 46/1000 | Loss: 0.00002219
Iteration 47/1000 | Loss: 0.00002219
Iteration 48/1000 | Loss: 0.00002219
Iteration 49/1000 | Loss: 0.00002219
Iteration 50/1000 | Loss: 0.00002219
Iteration 51/1000 | Loss: 0.00002219
Iteration 52/1000 | Loss: 0.00002218
Iteration 53/1000 | Loss: 0.00002218
Iteration 54/1000 | Loss: 0.00002218
Iteration 55/1000 | Loss: 0.00002218
Iteration 56/1000 | Loss: 0.00002218
Iteration 57/1000 | Loss: 0.00002218
Iteration 58/1000 | Loss: 0.00002218
Iteration 59/1000 | Loss: 0.00002218
Iteration 60/1000 | Loss: 0.00002218
Iteration 61/1000 | Loss: 0.00002218
Iteration 62/1000 | Loss: 0.00002218
Iteration 63/1000 | Loss: 0.00002218
Iteration 64/1000 | Loss: 0.00002218
Iteration 65/1000 | Loss: 0.00002217
Iteration 66/1000 | Loss: 0.00002217
Iteration 67/1000 | Loss: 0.00002217
Iteration 68/1000 | Loss: 0.00002217
Iteration 69/1000 | Loss: 0.00002216
Iteration 70/1000 | Loss: 0.00002216
Iteration 71/1000 | Loss: 0.00002216
Iteration 72/1000 | Loss: 0.00002216
Iteration 73/1000 | Loss: 0.00002216
Iteration 74/1000 | Loss: 0.00002216
Iteration 75/1000 | Loss: 0.00002216
Iteration 76/1000 | Loss: 0.00002216
Iteration 77/1000 | Loss: 0.00002216
Iteration 78/1000 | Loss: 0.00002216
Iteration 79/1000 | Loss: 0.00002215
Iteration 80/1000 | Loss: 0.00002215
Iteration 81/1000 | Loss: 0.00002215
Iteration 82/1000 | Loss: 0.00002215
Iteration 83/1000 | Loss: 0.00002215
Iteration 84/1000 | Loss: 0.00002215
Iteration 85/1000 | Loss: 0.00002214
Iteration 86/1000 | Loss: 0.00002214
Iteration 87/1000 | Loss: 0.00002214
Iteration 88/1000 | Loss: 0.00002214
Iteration 89/1000 | Loss: 0.00002214
Iteration 90/1000 | Loss: 0.00002214
Iteration 91/1000 | Loss: 0.00002214
Iteration 92/1000 | Loss: 0.00002214
Iteration 93/1000 | Loss: 0.00002214
Iteration 94/1000 | Loss: 0.00002213
Iteration 95/1000 | Loss: 0.00002213
Iteration 96/1000 | Loss: 0.00002213
Iteration 97/1000 | Loss: 0.00002213
Iteration 98/1000 | Loss: 0.00002213
Iteration 99/1000 | Loss: 0.00002213
Iteration 100/1000 | Loss: 0.00002213
Iteration 101/1000 | Loss: 0.00002213
Iteration 102/1000 | Loss: 0.00002213
Iteration 103/1000 | Loss: 0.00002213
Iteration 104/1000 | Loss: 0.00002213
Iteration 105/1000 | Loss: 0.00002213
Iteration 106/1000 | Loss: 0.00002213
Iteration 107/1000 | Loss: 0.00002213
Iteration 108/1000 | Loss: 0.00002213
Iteration 109/1000 | Loss: 0.00002213
Iteration 110/1000 | Loss: 0.00002213
Iteration 111/1000 | Loss: 0.00002213
Iteration 112/1000 | Loss: 0.00002213
Iteration 113/1000 | Loss: 0.00002213
Iteration 114/1000 | Loss: 0.00002213
Iteration 115/1000 | Loss: 0.00002213
Iteration 116/1000 | Loss: 0.00002213
Iteration 117/1000 | Loss: 0.00002213
Iteration 118/1000 | Loss: 0.00002213
Iteration 119/1000 | Loss: 0.00002213
Iteration 120/1000 | Loss: 0.00002213
Iteration 121/1000 | Loss: 0.00002213
Iteration 122/1000 | Loss: 0.00002213
Iteration 123/1000 | Loss: 0.00002213
Iteration 124/1000 | Loss: 0.00002213
Iteration 125/1000 | Loss: 0.00002213
Iteration 126/1000 | Loss: 0.00002213
Iteration 127/1000 | Loss: 0.00002213
Iteration 128/1000 | Loss: 0.00002213
Iteration 129/1000 | Loss: 0.00002213
Iteration 130/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.212722938566003e-05, 2.212722938566003e-05, 2.212722938566003e-05, 2.212722938566003e-05, 2.212722938566003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.212722938566003e-05

Optimization complete. Final v2v error: 4.064831256866455 mm

Highest mean error: 4.617055416107178 mm for frame 25

Lowest mean error: 3.4603466987609863 mm for frame 11

Saving results

Total time: 31.653218030929565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_32_us_1533/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_32_us_1533/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01074804
Iteration 2/25 | Loss: 0.00189273
Iteration 3/25 | Loss: 0.00141187
Iteration 4/25 | Loss: 0.00137454
Iteration 5/25 | Loss: 0.00136764
Iteration 6/25 | Loss: 0.00136615
Iteration 7/25 | Loss: 0.00136610
Iteration 8/25 | Loss: 0.00136610
Iteration 9/25 | Loss: 0.00136610
Iteration 10/25 | Loss: 0.00136610
Iteration 11/25 | Loss: 0.00136610
Iteration 12/25 | Loss: 0.00136610
Iteration 13/25 | Loss: 0.00136610
Iteration 14/25 | Loss: 0.00136610
Iteration 15/25 | Loss: 0.00136610
Iteration 16/25 | Loss: 0.00136610
Iteration 17/25 | Loss: 0.00136610
Iteration 18/25 | Loss: 0.00136610
Iteration 19/25 | Loss: 0.00136610
Iteration 20/25 | Loss: 0.00136610
Iteration 21/25 | Loss: 0.00136610
Iteration 22/25 | Loss: 0.00136610
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0013660952681675553, 0.0013660952681675553, 0.0013660952681675553, 0.0013660952681675553, 0.0013660952681675553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013660952681675553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.39662218
Iteration 2/25 | Loss: 0.00344796
Iteration 3/25 | Loss: 0.00344796
Iteration 4/25 | Loss: 0.00344796
Iteration 5/25 | Loss: 0.00344796
Iteration 6/25 | Loss: 0.00344796
Iteration 7/25 | Loss: 0.00344796
Iteration 8/25 | Loss: 0.00344796
Iteration 9/25 | Loss: 0.00344796
Iteration 10/25 | Loss: 0.00344796
Iteration 11/25 | Loss: 0.00344796
Iteration 12/25 | Loss: 0.00344796
Iteration 13/25 | Loss: 0.00344796
Iteration 14/25 | Loss: 0.00344796
Iteration 15/25 | Loss: 0.00344796
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003447960829362273, 0.003447960829362273, 0.003447960829362273, 0.003447960829362273, 0.003447960829362273]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003447960829362273

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00344796
Iteration 2/1000 | Loss: 0.00009082
Iteration 3/1000 | Loss: 0.00005879
Iteration 4/1000 | Loss: 0.00005294
Iteration 5/1000 | Loss: 0.00004977
Iteration 6/1000 | Loss: 0.00004795
Iteration 7/1000 | Loss: 0.00004648
Iteration 8/1000 | Loss: 0.00004552
Iteration 9/1000 | Loss: 0.00004447
Iteration 10/1000 | Loss: 0.00004351
Iteration 11/1000 | Loss: 0.00004290
Iteration 12/1000 | Loss: 0.00004238
Iteration 13/1000 | Loss: 0.00004202
Iteration 14/1000 | Loss: 0.00004175
Iteration 15/1000 | Loss: 0.00004156
Iteration 16/1000 | Loss: 0.00004143
Iteration 17/1000 | Loss: 0.00004126
Iteration 18/1000 | Loss: 0.00004124
Iteration 19/1000 | Loss: 0.00004108
Iteration 20/1000 | Loss: 0.00004100
Iteration 21/1000 | Loss: 0.00004081
Iteration 22/1000 | Loss: 0.00004077
Iteration 23/1000 | Loss: 0.00004069
Iteration 24/1000 | Loss: 0.00004069
Iteration 25/1000 | Loss: 0.00004069
Iteration 26/1000 | Loss: 0.00004066
Iteration 27/1000 | Loss: 0.00004066
Iteration 28/1000 | Loss: 0.00004066
Iteration 29/1000 | Loss: 0.00004066
Iteration 30/1000 | Loss: 0.00004066
Iteration 31/1000 | Loss: 0.00004065
Iteration 32/1000 | Loss: 0.00004065
Iteration 33/1000 | Loss: 0.00004065
Iteration 34/1000 | Loss: 0.00004065
Iteration 35/1000 | Loss: 0.00004065
Iteration 36/1000 | Loss: 0.00004065
Iteration 37/1000 | Loss: 0.00004065
Iteration 38/1000 | Loss: 0.00004065
Iteration 39/1000 | Loss: 0.00004065
Iteration 40/1000 | Loss: 0.00004064
Iteration 41/1000 | Loss: 0.00004064
Iteration 42/1000 | Loss: 0.00004063
Iteration 43/1000 | Loss: 0.00004063
Iteration 44/1000 | Loss: 0.00004063
Iteration 45/1000 | Loss: 0.00004063
Iteration 46/1000 | Loss: 0.00004063
Iteration 47/1000 | Loss: 0.00004063
Iteration 48/1000 | Loss: 0.00004063
Iteration 49/1000 | Loss: 0.00004063
Iteration 50/1000 | Loss: 0.00004063
Iteration 51/1000 | Loss: 0.00004063
Iteration 52/1000 | Loss: 0.00004063
Iteration 53/1000 | Loss: 0.00004063
Iteration 54/1000 | Loss: 0.00004062
Iteration 55/1000 | Loss: 0.00004062
Iteration 56/1000 | Loss: 0.00004061
Iteration 57/1000 | Loss: 0.00004060
Iteration 58/1000 | Loss: 0.00004060
Iteration 59/1000 | Loss: 0.00004060
Iteration 60/1000 | Loss: 0.00004060
Iteration 61/1000 | Loss: 0.00004060
Iteration 62/1000 | Loss: 0.00004059
Iteration 63/1000 | Loss: 0.00004059
Iteration 64/1000 | Loss: 0.00004059
Iteration 65/1000 | Loss: 0.00004059
Iteration 66/1000 | Loss: 0.00004059
Iteration 67/1000 | Loss: 0.00004059
Iteration 68/1000 | Loss: 0.00004059
Iteration 69/1000 | Loss: 0.00004059
Iteration 70/1000 | Loss: 0.00004059
Iteration 71/1000 | Loss: 0.00004058
Iteration 72/1000 | Loss: 0.00004058
Iteration 73/1000 | Loss: 0.00004058
Iteration 74/1000 | Loss: 0.00004058
Iteration 75/1000 | Loss: 0.00004058
Iteration 76/1000 | Loss: 0.00004058
Iteration 77/1000 | Loss: 0.00004058
Iteration 78/1000 | Loss: 0.00004058
Iteration 79/1000 | Loss: 0.00004058
Iteration 80/1000 | Loss: 0.00004058
Iteration 81/1000 | Loss: 0.00004057
Iteration 82/1000 | Loss: 0.00004057
Iteration 83/1000 | Loss: 0.00004056
Iteration 84/1000 | Loss: 0.00004056
Iteration 85/1000 | Loss: 0.00004056
Iteration 86/1000 | Loss: 0.00004056
Iteration 87/1000 | Loss: 0.00004056
Iteration 88/1000 | Loss: 0.00004056
Iteration 89/1000 | Loss: 0.00004056
Iteration 90/1000 | Loss: 0.00004056
Iteration 91/1000 | Loss: 0.00004056
Iteration 92/1000 | Loss: 0.00004056
Iteration 93/1000 | Loss: 0.00004055
Iteration 94/1000 | Loss: 0.00004055
Iteration 95/1000 | Loss: 0.00004055
Iteration 96/1000 | Loss: 0.00004055
Iteration 97/1000 | Loss: 0.00004054
Iteration 98/1000 | Loss: 0.00004054
Iteration 99/1000 | Loss: 0.00004054
Iteration 100/1000 | Loss: 0.00004054
Iteration 101/1000 | Loss: 0.00004054
Iteration 102/1000 | Loss: 0.00004053
Iteration 103/1000 | Loss: 0.00004053
Iteration 104/1000 | Loss: 0.00004053
Iteration 105/1000 | Loss: 0.00004053
Iteration 106/1000 | Loss: 0.00004053
Iteration 107/1000 | Loss: 0.00004053
Iteration 108/1000 | Loss: 0.00004052
Iteration 109/1000 | Loss: 0.00004052
Iteration 110/1000 | Loss: 0.00004052
Iteration 111/1000 | Loss: 0.00004052
Iteration 112/1000 | Loss: 0.00004052
Iteration 113/1000 | Loss: 0.00004051
Iteration 114/1000 | Loss: 0.00004051
Iteration 115/1000 | Loss: 0.00004050
Iteration 116/1000 | Loss: 0.00004050
Iteration 117/1000 | Loss: 0.00004050
Iteration 118/1000 | Loss: 0.00004049
Iteration 119/1000 | Loss: 0.00004049
Iteration 120/1000 | Loss: 0.00004048
Iteration 121/1000 | Loss: 0.00004048
Iteration 122/1000 | Loss: 0.00004048
Iteration 123/1000 | Loss: 0.00004047
Iteration 124/1000 | Loss: 0.00004047
Iteration 125/1000 | Loss: 0.00004046
Iteration 126/1000 | Loss: 0.00004046
Iteration 127/1000 | Loss: 0.00004046
Iteration 128/1000 | Loss: 0.00004046
Iteration 129/1000 | Loss: 0.00004045
Iteration 130/1000 | Loss: 0.00004045
Iteration 131/1000 | Loss: 0.00004045
Iteration 132/1000 | Loss: 0.00004045
Iteration 133/1000 | Loss: 0.00004045
Iteration 134/1000 | Loss: 0.00004044
Iteration 135/1000 | Loss: 0.00004044
Iteration 136/1000 | Loss: 0.00004044
Iteration 137/1000 | Loss: 0.00004044
Iteration 138/1000 | Loss: 0.00004043
Iteration 139/1000 | Loss: 0.00004041
Iteration 140/1000 | Loss: 0.00004040
Iteration 141/1000 | Loss: 0.00004039
Iteration 142/1000 | Loss: 0.00004039
Iteration 143/1000 | Loss: 0.00004038
Iteration 144/1000 | Loss: 0.00004038
Iteration 145/1000 | Loss: 0.00004038
Iteration 146/1000 | Loss: 0.00004038
Iteration 147/1000 | Loss: 0.00004037
Iteration 148/1000 | Loss: 0.00004037
Iteration 149/1000 | Loss: 0.00004037
Iteration 150/1000 | Loss: 0.00004037
Iteration 151/1000 | Loss: 0.00004036
Iteration 152/1000 | Loss: 0.00004036
Iteration 153/1000 | Loss: 0.00004036
Iteration 154/1000 | Loss: 0.00004036
Iteration 155/1000 | Loss: 0.00004036
Iteration 156/1000 | Loss: 0.00004036
Iteration 157/1000 | Loss: 0.00004036
Iteration 158/1000 | Loss: 0.00004036
Iteration 159/1000 | Loss: 0.00004036
Iteration 160/1000 | Loss: 0.00004036
Iteration 161/1000 | Loss: 0.00004036
Iteration 162/1000 | Loss: 0.00004035
Iteration 163/1000 | Loss: 0.00004035
Iteration 164/1000 | Loss: 0.00004035
Iteration 165/1000 | Loss: 0.00004035
Iteration 166/1000 | Loss: 0.00004035
Iteration 167/1000 | Loss: 0.00004035
Iteration 168/1000 | Loss: 0.00004035
Iteration 169/1000 | Loss: 0.00004035
Iteration 170/1000 | Loss: 0.00004035
Iteration 171/1000 | Loss: 0.00004035
Iteration 172/1000 | Loss: 0.00004034
Iteration 173/1000 | Loss: 0.00004034
Iteration 174/1000 | Loss: 0.00004034
Iteration 175/1000 | Loss: 0.00004034
Iteration 176/1000 | Loss: 0.00004034
Iteration 177/1000 | Loss: 0.00004034
Iteration 178/1000 | Loss: 0.00004034
Iteration 179/1000 | Loss: 0.00004034
Iteration 180/1000 | Loss: 0.00004034
Iteration 181/1000 | Loss: 0.00004034
Iteration 182/1000 | Loss: 0.00004034
Iteration 183/1000 | Loss: 0.00004034
Iteration 184/1000 | Loss: 0.00004034
Iteration 185/1000 | Loss: 0.00004034
Iteration 186/1000 | Loss: 0.00004034
Iteration 187/1000 | Loss: 0.00004034
Iteration 188/1000 | Loss: 0.00004034
Iteration 189/1000 | Loss: 0.00004034
Iteration 190/1000 | Loss: 0.00004034
Iteration 191/1000 | Loss: 0.00004034
Iteration 192/1000 | Loss: 0.00004034
Iteration 193/1000 | Loss: 0.00004034
Iteration 194/1000 | Loss: 0.00004034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [4.033911682199687e-05, 4.033911682199687e-05, 4.033911682199687e-05, 4.033911682199687e-05, 4.033911682199687e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.033911682199687e-05

Optimization complete. Final v2v error: 5.090641498565674 mm

Highest mean error: 5.997918605804443 mm for frame 123

Lowest mean error: 4.36611270904541 mm for frame 37

Saving results

Total time: 47.79612398147583
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996117
Iteration 2/25 | Loss: 0.00214008
Iteration 3/25 | Loss: 0.00166961
Iteration 4/25 | Loss: 0.00152940
Iteration 5/25 | Loss: 0.00153142
Iteration 6/25 | Loss: 0.00150215
Iteration 7/25 | Loss: 0.00146208
Iteration 8/25 | Loss: 0.00142698
Iteration 9/25 | Loss: 0.00142090
Iteration 10/25 | Loss: 0.00140400
Iteration 11/25 | Loss: 0.00141955
Iteration 12/25 | Loss: 0.00143157
Iteration 13/25 | Loss: 0.00141547
Iteration 14/25 | Loss: 0.00142171
Iteration 15/25 | Loss: 0.00140409
Iteration 16/25 | Loss: 0.00140519
Iteration 17/25 | Loss: 0.00140440
Iteration 18/25 | Loss: 0.00139261
Iteration 19/25 | Loss: 0.00139473
Iteration 20/25 | Loss: 0.00139070
Iteration 21/25 | Loss: 0.00138635
Iteration 22/25 | Loss: 0.00138730
Iteration 23/25 | Loss: 0.00138697
Iteration 24/25 | Loss: 0.00138800
Iteration 25/25 | Loss: 0.00139348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43032455
Iteration 2/25 | Loss: 0.00138999
Iteration 3/25 | Loss: 0.00138999
Iteration 4/25 | Loss: 0.00138999
Iteration 5/25 | Loss: 0.00138999
Iteration 6/25 | Loss: 0.00138999
Iteration 7/25 | Loss: 0.00138998
Iteration 8/25 | Loss: 0.00138998
Iteration 9/25 | Loss: 0.00138998
Iteration 10/25 | Loss: 0.00138998
Iteration 11/25 | Loss: 0.00138998
Iteration 12/25 | Loss: 0.00138998
Iteration 13/25 | Loss: 0.00138998
Iteration 14/25 | Loss: 0.00138998
Iteration 15/25 | Loss: 0.00138998
Iteration 16/25 | Loss: 0.00138998
Iteration 17/25 | Loss: 0.00138998
Iteration 18/25 | Loss: 0.00138998
Iteration 19/25 | Loss: 0.00138998
Iteration 20/25 | Loss: 0.00138998
Iteration 21/25 | Loss: 0.00138998
Iteration 22/25 | Loss: 0.00138998
Iteration 23/25 | Loss: 0.00138998
Iteration 24/25 | Loss: 0.00138998
Iteration 25/25 | Loss: 0.00138998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138998
Iteration 2/1000 | Loss: 0.00035577
Iteration 3/1000 | Loss: 0.00006812
Iteration 4/1000 | Loss: 0.00006562
Iteration 5/1000 | Loss: 0.00042442
Iteration 6/1000 | Loss: 0.00028488
Iteration 7/1000 | Loss: 0.00035528
Iteration 8/1000 | Loss: 0.00027969
Iteration 9/1000 | Loss: 0.00032738
Iteration 10/1000 | Loss: 0.00007204
Iteration 11/1000 | Loss: 0.00006723
Iteration 12/1000 | Loss: 0.00004326
Iteration 13/1000 | Loss: 0.00005099
Iteration 14/1000 | Loss: 0.00017357
Iteration 15/1000 | Loss: 0.00012057
Iteration 16/1000 | Loss: 0.00018660
Iteration 17/1000 | Loss: 0.00011855
Iteration 18/1000 | Loss: 0.00033603
Iteration 19/1000 | Loss: 0.00010014
Iteration 20/1000 | Loss: 0.00016032
Iteration 21/1000 | Loss: 0.00026547
Iteration 22/1000 | Loss: 0.00023680
Iteration 23/1000 | Loss: 0.00016385
Iteration 24/1000 | Loss: 0.00022462
Iteration 25/1000 | Loss: 0.00026777
Iteration 26/1000 | Loss: 0.00023770
Iteration 27/1000 | Loss: 0.00025209
Iteration 28/1000 | Loss: 0.00022852
Iteration 29/1000 | Loss: 0.00101768
Iteration 30/1000 | Loss: 0.00046375
Iteration 31/1000 | Loss: 0.00034340
Iteration 32/1000 | Loss: 0.00011050
Iteration 33/1000 | Loss: 0.00044430
Iteration 34/1000 | Loss: 0.00028879
Iteration 35/1000 | Loss: 0.00038767
Iteration 36/1000 | Loss: 0.00017858
Iteration 37/1000 | Loss: 0.00018576
Iteration 38/1000 | Loss: 0.00026857
Iteration 39/1000 | Loss: 0.00013190
Iteration 40/1000 | Loss: 0.00030895
Iteration 41/1000 | Loss: 0.00032894
Iteration 42/1000 | Loss: 0.00025332
Iteration 43/1000 | Loss: 0.00028820
Iteration 44/1000 | Loss: 0.00022109
Iteration 45/1000 | Loss: 0.00005523
Iteration 46/1000 | Loss: 0.00004151
Iteration 47/1000 | Loss: 0.00004854
Iteration 48/1000 | Loss: 0.00013805
Iteration 49/1000 | Loss: 0.00010040
Iteration 50/1000 | Loss: 0.00005302
Iteration 51/1000 | Loss: 0.00011383
Iteration 52/1000 | Loss: 0.00010975
Iteration 53/1000 | Loss: 0.00005643
Iteration 54/1000 | Loss: 0.00005828
Iteration 55/1000 | Loss: 0.00005217
Iteration 56/1000 | Loss: 0.00013719
Iteration 57/1000 | Loss: 0.00017214
Iteration 58/1000 | Loss: 0.00012014
Iteration 59/1000 | Loss: 0.00004388
Iteration 60/1000 | Loss: 0.00034349
Iteration 61/1000 | Loss: 0.00033071
Iteration 62/1000 | Loss: 0.00060000
Iteration 63/1000 | Loss: 0.00057175
Iteration 64/1000 | Loss: 0.00004548
Iteration 65/1000 | Loss: 0.00003370
Iteration 66/1000 | Loss: 0.00028466
Iteration 67/1000 | Loss: 0.00005015
Iteration 68/1000 | Loss: 0.00020781
Iteration 69/1000 | Loss: 0.00030081
Iteration 70/1000 | Loss: 0.00021828
Iteration 71/1000 | Loss: 0.00029459
Iteration 72/1000 | Loss: 0.00006839
Iteration 73/1000 | Loss: 0.00005803
Iteration 74/1000 | Loss: 0.00004707
Iteration 75/1000 | Loss: 0.00006797
Iteration 76/1000 | Loss: 0.00007968
Iteration 77/1000 | Loss: 0.00006780
Iteration 78/1000 | Loss: 0.00006570
Iteration 79/1000 | Loss: 0.00004315
Iteration 80/1000 | Loss: 0.00002715
Iteration 81/1000 | Loss: 0.00005007
Iteration 82/1000 | Loss: 0.00004679
Iteration 83/1000 | Loss: 0.00005140
Iteration 84/1000 | Loss: 0.00004689
Iteration 85/1000 | Loss: 0.00005027
Iteration 86/1000 | Loss: 0.00004455
Iteration 87/1000 | Loss: 0.00004995
Iteration 88/1000 | Loss: 0.00004492
Iteration 89/1000 | Loss: 0.00004945
Iteration 90/1000 | Loss: 0.00004407
Iteration 91/1000 | Loss: 0.00004907
Iteration 92/1000 | Loss: 0.00005322
Iteration 93/1000 | Loss: 0.00003914
Iteration 94/1000 | Loss: 0.00004273
Iteration 95/1000 | Loss: 0.00004960
Iteration 96/1000 | Loss: 0.00005130
Iteration 97/1000 | Loss: 0.00004960
Iteration 98/1000 | Loss: 0.00004300
Iteration 99/1000 | Loss: 0.00005820
Iteration 100/1000 | Loss: 0.00004706
Iteration 101/1000 | Loss: 0.00004707
Iteration 102/1000 | Loss: 0.00004535
Iteration 103/1000 | Loss: 0.00006550
Iteration 104/1000 | Loss: 0.00004128
Iteration 105/1000 | Loss: 0.00004650
Iteration 106/1000 | Loss: 0.00004640
Iteration 107/1000 | Loss: 0.00004257
Iteration 108/1000 | Loss: 0.00004261
Iteration 109/1000 | Loss: 0.00006406
Iteration 110/1000 | Loss: 0.00002894
Iteration 111/1000 | Loss: 0.00002537
Iteration 112/1000 | Loss: 0.00002350
Iteration 113/1000 | Loss: 0.00002296
Iteration 114/1000 | Loss: 0.00002264
Iteration 115/1000 | Loss: 0.00002245
Iteration 116/1000 | Loss: 0.00002239
Iteration 117/1000 | Loss: 0.00002239
Iteration 118/1000 | Loss: 0.00015482
Iteration 119/1000 | Loss: 0.00003117
Iteration 120/1000 | Loss: 0.00002453
Iteration 121/1000 | Loss: 0.00002339
Iteration 122/1000 | Loss: 0.00024105
Iteration 123/1000 | Loss: 0.00004768
Iteration 124/1000 | Loss: 0.00005099
Iteration 125/1000 | Loss: 0.00021739
Iteration 126/1000 | Loss: 0.00016873
Iteration 127/1000 | Loss: 0.00014071
Iteration 128/1000 | Loss: 0.00024545
Iteration 129/1000 | Loss: 0.00020635
Iteration 130/1000 | Loss: 0.00006163
Iteration 131/1000 | Loss: 0.00005800
Iteration 132/1000 | Loss: 0.00015334
Iteration 133/1000 | Loss: 0.00029231
Iteration 134/1000 | Loss: 0.00017395
Iteration 135/1000 | Loss: 0.00003179
Iteration 136/1000 | Loss: 0.00002706
Iteration 137/1000 | Loss: 0.00012452
Iteration 138/1000 | Loss: 0.00004257
Iteration 139/1000 | Loss: 0.00017326
Iteration 140/1000 | Loss: 0.00021648
Iteration 141/1000 | Loss: 0.00013879
Iteration 142/1000 | Loss: 0.00017607
Iteration 143/1000 | Loss: 0.00013946
Iteration 144/1000 | Loss: 0.00003359
Iteration 145/1000 | Loss: 0.00005639
Iteration 146/1000 | Loss: 0.00003739
Iteration 147/1000 | Loss: 0.00002542
Iteration 148/1000 | Loss: 0.00002448
Iteration 149/1000 | Loss: 0.00002399
Iteration 150/1000 | Loss: 0.00019919
Iteration 151/1000 | Loss: 0.00014016
Iteration 152/1000 | Loss: 0.00004722
Iteration 153/1000 | Loss: 0.00003777
Iteration 154/1000 | Loss: 0.00005069
Iteration 155/1000 | Loss: 0.00002715
Iteration 156/1000 | Loss: 0.00002497
Iteration 157/1000 | Loss: 0.00029330
Iteration 158/1000 | Loss: 0.00010761
Iteration 159/1000 | Loss: 0.00023163
Iteration 160/1000 | Loss: 0.00014182
Iteration 161/1000 | Loss: 0.00023941
Iteration 162/1000 | Loss: 0.00016954
Iteration 163/1000 | Loss: 0.00022714
Iteration 164/1000 | Loss: 0.00022688
Iteration 165/1000 | Loss: 0.00026858
Iteration 166/1000 | Loss: 0.00017495
Iteration 167/1000 | Loss: 0.00015828
Iteration 168/1000 | Loss: 0.00004350
Iteration 169/1000 | Loss: 0.00002586
Iteration 170/1000 | Loss: 0.00002940
Iteration 171/1000 | Loss: 0.00025304
Iteration 172/1000 | Loss: 0.00004795
Iteration 173/1000 | Loss: 0.00011617
Iteration 174/1000 | Loss: 0.00003120
Iteration 175/1000 | Loss: 0.00002504
Iteration 176/1000 | Loss: 0.00002323
Iteration 177/1000 | Loss: 0.00002274
Iteration 178/1000 | Loss: 0.00002196
Iteration 179/1000 | Loss: 0.00002138
Iteration 180/1000 | Loss: 0.00002092
Iteration 181/1000 | Loss: 0.00002063
Iteration 182/1000 | Loss: 0.00002050
Iteration 183/1000 | Loss: 0.00002044
Iteration 184/1000 | Loss: 0.00002044
Iteration 185/1000 | Loss: 0.00002042
Iteration 186/1000 | Loss: 0.00002041
Iteration 187/1000 | Loss: 0.00002041
Iteration 188/1000 | Loss: 0.00002037
Iteration 189/1000 | Loss: 0.00002037
Iteration 190/1000 | Loss: 0.00002036
Iteration 191/1000 | Loss: 0.00002036
Iteration 192/1000 | Loss: 0.00002035
Iteration 193/1000 | Loss: 0.00002035
Iteration 194/1000 | Loss: 0.00002035
Iteration 195/1000 | Loss: 0.00002035
Iteration 196/1000 | Loss: 0.00002034
Iteration 197/1000 | Loss: 0.00002034
Iteration 198/1000 | Loss: 0.00002034
Iteration 199/1000 | Loss: 0.00002034
Iteration 200/1000 | Loss: 0.00002034
Iteration 201/1000 | Loss: 0.00002034
Iteration 202/1000 | Loss: 0.00002034
Iteration 203/1000 | Loss: 0.00002033
Iteration 204/1000 | Loss: 0.00002033
Iteration 205/1000 | Loss: 0.00002033
Iteration 206/1000 | Loss: 0.00002033
Iteration 207/1000 | Loss: 0.00002033
Iteration 208/1000 | Loss: 0.00002033
Iteration 209/1000 | Loss: 0.00002033
Iteration 210/1000 | Loss: 0.00002033
Iteration 211/1000 | Loss: 0.00002033
Iteration 212/1000 | Loss: 0.00002033
Iteration 213/1000 | Loss: 0.00002033
Iteration 214/1000 | Loss: 0.00002033
Iteration 215/1000 | Loss: 0.00002032
Iteration 216/1000 | Loss: 0.00002032
Iteration 217/1000 | Loss: 0.00002032
Iteration 218/1000 | Loss: 0.00002032
Iteration 219/1000 | Loss: 0.00002032
Iteration 220/1000 | Loss: 0.00002032
Iteration 221/1000 | Loss: 0.00002032
Iteration 222/1000 | Loss: 0.00002032
Iteration 223/1000 | Loss: 0.00002032
Iteration 224/1000 | Loss: 0.00002032
Iteration 225/1000 | Loss: 0.00002031
Iteration 226/1000 | Loss: 0.00002031
Iteration 227/1000 | Loss: 0.00002031
Iteration 228/1000 | Loss: 0.00002031
Iteration 229/1000 | Loss: 0.00002030
Iteration 230/1000 | Loss: 0.00002030
Iteration 231/1000 | Loss: 0.00002030
Iteration 232/1000 | Loss: 0.00002030
Iteration 233/1000 | Loss: 0.00002030
Iteration 234/1000 | Loss: 0.00002030
Iteration 235/1000 | Loss: 0.00002030
Iteration 236/1000 | Loss: 0.00002030
Iteration 237/1000 | Loss: 0.00002030
Iteration 238/1000 | Loss: 0.00002030
Iteration 239/1000 | Loss: 0.00002030
Iteration 240/1000 | Loss: 0.00002030
Iteration 241/1000 | Loss: 0.00002030
Iteration 242/1000 | Loss: 0.00002030
Iteration 243/1000 | Loss: 0.00002030
Iteration 244/1000 | Loss: 0.00002030
Iteration 245/1000 | Loss: 0.00002030
Iteration 246/1000 | Loss: 0.00002030
Iteration 247/1000 | Loss: 0.00002030
Iteration 248/1000 | Loss: 0.00002030
Iteration 249/1000 | Loss: 0.00002030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 249. Stopping optimization.
Last 5 losses: [2.029531606240198e-05, 2.029531606240198e-05, 2.029531606240198e-05, 2.029531606240198e-05, 2.029531606240198e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.029531606240198e-05

Optimization complete. Final v2v error: 3.7822208404541016 mm

Highest mean error: 4.247194290161133 mm for frame 18

Lowest mean error: 3.2859857082366943 mm for frame 110

Saving results

Total time: 286.00895404815674
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989918
Iteration 2/25 | Loss: 0.00253898
Iteration 3/25 | Loss: 0.00185850
Iteration 4/25 | Loss: 0.00174920
Iteration 5/25 | Loss: 0.00165055
Iteration 6/25 | Loss: 0.00159365
Iteration 7/25 | Loss: 0.00157540
Iteration 8/25 | Loss: 0.00155345
Iteration 9/25 | Loss: 0.00151714
Iteration 10/25 | Loss: 0.00149406
Iteration 11/25 | Loss: 0.00147003
Iteration 12/25 | Loss: 0.00145237
Iteration 13/25 | Loss: 0.00145568
Iteration 14/25 | Loss: 0.00143300
Iteration 15/25 | Loss: 0.00141500
Iteration 16/25 | Loss: 0.00141540
Iteration 17/25 | Loss: 0.00140186
Iteration 18/25 | Loss: 0.00139079
Iteration 19/25 | Loss: 0.00138769
Iteration 20/25 | Loss: 0.00138425
Iteration 21/25 | Loss: 0.00138394
Iteration 22/25 | Loss: 0.00138382
Iteration 23/25 | Loss: 0.00138373
Iteration 24/25 | Loss: 0.00138370
Iteration 25/25 | Loss: 0.00138370

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12397361
Iteration 2/25 | Loss: 0.00116311
Iteration 3/25 | Loss: 0.00116311
Iteration 4/25 | Loss: 0.00116310
Iteration 5/25 | Loss: 0.00116310
Iteration 6/25 | Loss: 0.00116310
Iteration 7/25 | Loss: 0.00116310
Iteration 8/25 | Loss: 0.00116310
Iteration 9/25 | Loss: 0.00116310
Iteration 10/25 | Loss: 0.00116310
Iteration 11/25 | Loss: 0.00116310
Iteration 12/25 | Loss: 0.00116310
Iteration 13/25 | Loss: 0.00116310
Iteration 14/25 | Loss: 0.00116310
Iteration 15/25 | Loss: 0.00116310
Iteration 16/25 | Loss: 0.00116310
Iteration 17/25 | Loss: 0.00116310
Iteration 18/25 | Loss: 0.00116310
Iteration 19/25 | Loss: 0.00116310
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011631029192358255, 0.0011631029192358255, 0.0011631029192358255, 0.0011631029192358255, 0.0011631029192358255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011631029192358255

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116310
Iteration 2/1000 | Loss: 0.00020524
Iteration 3/1000 | Loss: 0.00011272
Iteration 4/1000 | Loss: 0.00010862
Iteration 5/1000 | Loss: 0.00027879
Iteration 6/1000 | Loss: 0.00009491
Iteration 7/1000 | Loss: 0.00006104
Iteration 8/1000 | Loss: 0.00010482
Iteration 9/1000 | Loss: 0.00023089
Iteration 10/1000 | Loss: 0.00006710
Iteration 11/1000 | Loss: 0.00012384
Iteration 12/1000 | Loss: 0.00005209
Iteration 13/1000 | Loss: 0.00004502
Iteration 14/1000 | Loss: 0.00004213
Iteration 15/1000 | Loss: 0.00005181
Iteration 16/1000 | Loss: 0.00004792
Iteration 17/1000 | Loss: 0.00032264
Iteration 18/1000 | Loss: 0.00005400
Iteration 19/1000 | Loss: 0.00007519
Iteration 20/1000 | Loss: 0.00022668
Iteration 21/1000 | Loss: 0.00007440
Iteration 22/1000 | Loss: 0.00003583
Iteration 23/1000 | Loss: 0.00004149
Iteration 24/1000 | Loss: 0.00003716
Iteration 25/1000 | Loss: 0.00004272
Iteration 26/1000 | Loss: 0.00004315
Iteration 27/1000 | Loss: 0.00003800
Iteration 28/1000 | Loss: 0.00004591
Iteration 29/1000 | Loss: 0.00003784
Iteration 30/1000 | Loss: 0.00004387
Iteration 31/1000 | Loss: 0.00004019
Iteration 32/1000 | Loss: 0.00004544
Iteration 33/1000 | Loss: 0.00004038
Iteration 34/1000 | Loss: 0.00004077
Iteration 35/1000 | Loss: 0.00004208
Iteration 36/1000 | Loss: 0.00004229
Iteration 37/1000 | Loss: 0.00025456
Iteration 38/1000 | Loss: 0.00005246
Iteration 39/1000 | Loss: 0.00004402
Iteration 40/1000 | Loss: 0.00004405
Iteration 41/1000 | Loss: 0.00004258
Iteration 42/1000 | Loss: 0.00009269
Iteration 43/1000 | Loss: 0.00004140
Iteration 44/1000 | Loss: 0.00003516
Iteration 45/1000 | Loss: 0.00002843
Iteration 46/1000 | Loss: 0.00004743
Iteration 47/1000 | Loss: 0.00004069
Iteration 48/1000 | Loss: 0.00004686
Iteration 49/1000 | Loss: 0.00020337
Iteration 50/1000 | Loss: 0.00004915
Iteration 51/1000 | Loss: 0.00004409
Iteration 52/1000 | Loss: 0.00005221
Iteration 53/1000 | Loss: 0.00004588
Iteration 54/1000 | Loss: 0.00003932
Iteration 55/1000 | Loss: 0.00004822
Iteration 56/1000 | Loss: 0.00003456
Iteration 57/1000 | Loss: 0.00014191
Iteration 58/1000 | Loss: 0.00006181
Iteration 59/1000 | Loss: 0.00003924
Iteration 60/1000 | Loss: 0.00008751
Iteration 61/1000 | Loss: 0.00004686
Iteration 62/1000 | Loss: 0.00004260
Iteration 63/1000 | Loss: 0.00005854
Iteration 64/1000 | Loss: 0.00011677
Iteration 65/1000 | Loss: 0.00003476
Iteration 66/1000 | Loss: 0.00003573
Iteration 67/1000 | Loss: 0.00004514
Iteration 68/1000 | Loss: 0.00009414
Iteration 69/1000 | Loss: 0.00006148
Iteration 70/1000 | Loss: 0.00003776
Iteration 71/1000 | Loss: 0.00004604
Iteration 72/1000 | Loss: 0.00003731
Iteration 73/1000 | Loss: 0.00004680
Iteration 74/1000 | Loss: 0.00003669
Iteration 75/1000 | Loss: 0.00004615
Iteration 76/1000 | Loss: 0.00003967
Iteration 77/1000 | Loss: 0.00034136
Iteration 78/1000 | Loss: 0.00003919
Iteration 79/1000 | Loss: 0.00004464
Iteration 80/1000 | Loss: 0.00005135
Iteration 81/1000 | Loss: 0.00004446
Iteration 82/1000 | Loss: 0.00002815
Iteration 83/1000 | Loss: 0.00003698
Iteration 84/1000 | Loss: 0.00011372
Iteration 85/1000 | Loss: 0.00014525
Iteration 86/1000 | Loss: 0.00003303
Iteration 87/1000 | Loss: 0.00004156
Iteration 88/1000 | Loss: 0.00003239
Iteration 89/1000 | Loss: 0.00004376
Iteration 90/1000 | Loss: 0.00004424
Iteration 91/1000 | Loss: 0.00005129
Iteration 92/1000 | Loss: 0.00004386
Iteration 93/1000 | Loss: 0.00005289
Iteration 94/1000 | Loss: 0.00015343
Iteration 95/1000 | Loss: 0.00007084
Iteration 96/1000 | Loss: 0.00002978
Iteration 97/1000 | Loss: 0.00007227
Iteration 98/1000 | Loss: 0.00005717
Iteration 99/1000 | Loss: 0.00004889
Iteration 100/1000 | Loss: 0.00005677
Iteration 101/1000 | Loss: 0.00005850
Iteration 102/1000 | Loss: 0.00004346
Iteration 103/1000 | Loss: 0.00005578
Iteration 104/1000 | Loss: 0.00004360
Iteration 105/1000 | Loss: 0.00013800
Iteration 106/1000 | Loss: 0.00002949
Iteration 107/1000 | Loss: 0.00018889
Iteration 108/1000 | Loss: 0.00069942
Iteration 109/1000 | Loss: 0.00003469
Iteration 110/1000 | Loss: 0.00004045
Iteration 111/1000 | Loss: 0.00012920
Iteration 112/1000 | Loss: 0.00018117
Iteration 113/1000 | Loss: 0.00002497
Iteration 114/1000 | Loss: 0.00002292
Iteration 115/1000 | Loss: 0.00002283
Iteration 116/1000 | Loss: 0.00002008
Iteration 117/1000 | Loss: 0.00001988
Iteration 118/1000 | Loss: 0.00001977
Iteration 119/1000 | Loss: 0.00001976
Iteration 120/1000 | Loss: 0.00001971
Iteration 121/1000 | Loss: 0.00001963
Iteration 122/1000 | Loss: 0.00001963
Iteration 123/1000 | Loss: 0.00001963
Iteration 124/1000 | Loss: 0.00001962
Iteration 125/1000 | Loss: 0.00001961
Iteration 126/1000 | Loss: 0.00001961
Iteration 127/1000 | Loss: 0.00001961
Iteration 128/1000 | Loss: 0.00001959
Iteration 129/1000 | Loss: 0.00001959
Iteration 130/1000 | Loss: 0.00001956
Iteration 131/1000 | Loss: 0.00001956
Iteration 132/1000 | Loss: 0.00001956
Iteration 133/1000 | Loss: 0.00001956
Iteration 134/1000 | Loss: 0.00001956
Iteration 135/1000 | Loss: 0.00001956
Iteration 136/1000 | Loss: 0.00001955
Iteration 137/1000 | Loss: 0.00001955
Iteration 138/1000 | Loss: 0.00001955
Iteration 139/1000 | Loss: 0.00001955
Iteration 140/1000 | Loss: 0.00001955
Iteration 141/1000 | Loss: 0.00001955
Iteration 142/1000 | Loss: 0.00001951
Iteration 143/1000 | Loss: 0.00001951
Iteration 144/1000 | Loss: 0.00001951
Iteration 145/1000 | Loss: 0.00001951
Iteration 146/1000 | Loss: 0.00001951
Iteration 147/1000 | Loss: 0.00001951
Iteration 148/1000 | Loss: 0.00001951
Iteration 149/1000 | Loss: 0.00001951
Iteration 150/1000 | Loss: 0.00001950
Iteration 151/1000 | Loss: 0.00001950
Iteration 152/1000 | Loss: 0.00001950
Iteration 153/1000 | Loss: 0.00001950
Iteration 154/1000 | Loss: 0.00001950
Iteration 155/1000 | Loss: 0.00001950
Iteration 156/1000 | Loss: 0.00001949
Iteration 157/1000 | Loss: 0.00001949
Iteration 158/1000 | Loss: 0.00001947
Iteration 159/1000 | Loss: 0.00001947
Iteration 160/1000 | Loss: 0.00001947
Iteration 161/1000 | Loss: 0.00001946
Iteration 162/1000 | Loss: 0.00001946
Iteration 163/1000 | Loss: 0.00001946
Iteration 164/1000 | Loss: 0.00001946
Iteration 165/1000 | Loss: 0.00001946
Iteration 166/1000 | Loss: 0.00012896
Iteration 167/1000 | Loss: 0.00001958
Iteration 168/1000 | Loss: 0.00001944
Iteration 169/1000 | Loss: 0.00001938
Iteration 170/1000 | Loss: 0.00001938
Iteration 171/1000 | Loss: 0.00001937
Iteration 172/1000 | Loss: 0.00001937
Iteration 173/1000 | Loss: 0.00001937
Iteration 174/1000 | Loss: 0.00001937
Iteration 175/1000 | Loss: 0.00001937
Iteration 176/1000 | Loss: 0.00001937
Iteration 177/1000 | Loss: 0.00001937
Iteration 178/1000 | Loss: 0.00001937
Iteration 179/1000 | Loss: 0.00001937
Iteration 180/1000 | Loss: 0.00001937
Iteration 181/1000 | Loss: 0.00001936
Iteration 182/1000 | Loss: 0.00001936
Iteration 183/1000 | Loss: 0.00001936
Iteration 184/1000 | Loss: 0.00001936
Iteration 185/1000 | Loss: 0.00001936
Iteration 186/1000 | Loss: 0.00001936
Iteration 187/1000 | Loss: 0.00001936
Iteration 188/1000 | Loss: 0.00001936
Iteration 189/1000 | Loss: 0.00001935
Iteration 190/1000 | Loss: 0.00001935
Iteration 191/1000 | Loss: 0.00001935
Iteration 192/1000 | Loss: 0.00001935
Iteration 193/1000 | Loss: 0.00001935
Iteration 194/1000 | Loss: 0.00001935
Iteration 195/1000 | Loss: 0.00001935
Iteration 196/1000 | Loss: 0.00001935
Iteration 197/1000 | Loss: 0.00001935
Iteration 198/1000 | Loss: 0.00001934
Iteration 199/1000 | Loss: 0.00001934
Iteration 200/1000 | Loss: 0.00001934
Iteration 201/1000 | Loss: 0.00001934
Iteration 202/1000 | Loss: 0.00001934
Iteration 203/1000 | Loss: 0.00001934
Iteration 204/1000 | Loss: 0.00001934
Iteration 205/1000 | Loss: 0.00001933
Iteration 206/1000 | Loss: 0.00001933
Iteration 207/1000 | Loss: 0.00001933
Iteration 208/1000 | Loss: 0.00001933
Iteration 209/1000 | Loss: 0.00001933
Iteration 210/1000 | Loss: 0.00001933
Iteration 211/1000 | Loss: 0.00001933
Iteration 212/1000 | Loss: 0.00001933
Iteration 213/1000 | Loss: 0.00001933
Iteration 214/1000 | Loss: 0.00001933
Iteration 215/1000 | Loss: 0.00001933
Iteration 216/1000 | Loss: 0.00001932
Iteration 217/1000 | Loss: 0.00001932
Iteration 218/1000 | Loss: 0.00001932
Iteration 219/1000 | Loss: 0.00001932
Iteration 220/1000 | Loss: 0.00001931
Iteration 221/1000 | Loss: 0.00001931
Iteration 222/1000 | Loss: 0.00001931
Iteration 223/1000 | Loss: 0.00001931
Iteration 224/1000 | Loss: 0.00001931
Iteration 225/1000 | Loss: 0.00001931
Iteration 226/1000 | Loss: 0.00001930
Iteration 227/1000 | Loss: 0.00001930
Iteration 228/1000 | Loss: 0.00001930
Iteration 229/1000 | Loss: 0.00001930
Iteration 230/1000 | Loss: 0.00001930
Iteration 231/1000 | Loss: 0.00001929
Iteration 232/1000 | Loss: 0.00001929
Iteration 233/1000 | Loss: 0.00001929
Iteration 234/1000 | Loss: 0.00001929
Iteration 235/1000 | Loss: 0.00001929
Iteration 236/1000 | Loss: 0.00001929
Iteration 237/1000 | Loss: 0.00001929
Iteration 238/1000 | Loss: 0.00001928
Iteration 239/1000 | Loss: 0.00001928
Iteration 240/1000 | Loss: 0.00001928
Iteration 241/1000 | Loss: 0.00001928
Iteration 242/1000 | Loss: 0.00001928
Iteration 243/1000 | Loss: 0.00001928
Iteration 244/1000 | Loss: 0.00001928
Iteration 245/1000 | Loss: 0.00001928
Iteration 246/1000 | Loss: 0.00001927
Iteration 247/1000 | Loss: 0.00001927
Iteration 248/1000 | Loss: 0.00001927
Iteration 249/1000 | Loss: 0.00001927
Iteration 250/1000 | Loss: 0.00001926
Iteration 251/1000 | Loss: 0.00001926
Iteration 252/1000 | Loss: 0.00001926
Iteration 253/1000 | Loss: 0.00001926
Iteration 254/1000 | Loss: 0.00001925
Iteration 255/1000 | Loss: 0.00001925
Iteration 256/1000 | Loss: 0.00001925
Iteration 257/1000 | Loss: 0.00001925
Iteration 258/1000 | Loss: 0.00001925
Iteration 259/1000 | Loss: 0.00001925
Iteration 260/1000 | Loss: 0.00001925
Iteration 261/1000 | Loss: 0.00001925
Iteration 262/1000 | Loss: 0.00001924
Iteration 263/1000 | Loss: 0.00001924
Iteration 264/1000 | Loss: 0.00001924
Iteration 265/1000 | Loss: 0.00001924
Iteration 266/1000 | Loss: 0.00001923
Iteration 267/1000 | Loss: 0.00001923
Iteration 268/1000 | Loss: 0.00001923
Iteration 269/1000 | Loss: 0.00001922
Iteration 270/1000 | Loss: 0.00001922
Iteration 271/1000 | Loss: 0.00001922
Iteration 272/1000 | Loss: 0.00001922
Iteration 273/1000 | Loss: 0.00001922
Iteration 274/1000 | Loss: 0.00001922
Iteration 275/1000 | Loss: 0.00001921
Iteration 276/1000 | Loss: 0.00001921
Iteration 277/1000 | Loss: 0.00001921
Iteration 278/1000 | Loss: 0.00001921
Iteration 279/1000 | Loss: 0.00001921
Iteration 280/1000 | Loss: 0.00001921
Iteration 281/1000 | Loss: 0.00001920
Iteration 282/1000 | Loss: 0.00001920
Iteration 283/1000 | Loss: 0.00001920
Iteration 284/1000 | Loss: 0.00001920
Iteration 285/1000 | Loss: 0.00001920
Iteration 286/1000 | Loss: 0.00001920
Iteration 287/1000 | Loss: 0.00001920
Iteration 288/1000 | Loss: 0.00001919
Iteration 289/1000 | Loss: 0.00001919
Iteration 290/1000 | Loss: 0.00001919
Iteration 291/1000 | Loss: 0.00001919
Iteration 292/1000 | Loss: 0.00001918
Iteration 293/1000 | Loss: 0.00001918
Iteration 294/1000 | Loss: 0.00001918
Iteration 295/1000 | Loss: 0.00001918
Iteration 296/1000 | Loss: 0.00001918
Iteration 297/1000 | Loss: 0.00001918
Iteration 298/1000 | Loss: 0.00001918
Iteration 299/1000 | Loss: 0.00001918
Iteration 300/1000 | Loss: 0.00001918
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 300. Stopping optimization.
Last 5 losses: [1.918110137921758e-05, 1.918110137921758e-05, 1.918110137921758e-05, 1.918110137921758e-05, 1.918110137921758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.918110137921758e-05

Optimization complete. Final v2v error: 3.674560070037842 mm

Highest mean error: 4.335675239562988 mm for frame 83

Lowest mean error: 3.365891695022583 mm for frame 14

Saving results

Total time: 209.9323661327362
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00439543
Iteration 2/25 | Loss: 0.00144077
Iteration 3/25 | Loss: 0.00135974
Iteration 4/25 | Loss: 0.00134340
Iteration 5/25 | Loss: 0.00133755
Iteration 6/25 | Loss: 0.00133654
Iteration 7/25 | Loss: 0.00133654
Iteration 8/25 | Loss: 0.00133654
Iteration 9/25 | Loss: 0.00133654
Iteration 10/25 | Loss: 0.00133654
Iteration 11/25 | Loss: 0.00133654
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013365412596613169, 0.0013365412596613169, 0.0013365412596613169, 0.0013365412596613169, 0.0013365412596613169]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013365412596613169

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81073844
Iteration 2/25 | Loss: 0.00101922
Iteration 3/25 | Loss: 0.00101921
Iteration 4/25 | Loss: 0.00101921
Iteration 5/25 | Loss: 0.00101921
Iteration 6/25 | Loss: 0.00101921
Iteration 7/25 | Loss: 0.00101921
Iteration 8/25 | Loss: 0.00101921
Iteration 9/25 | Loss: 0.00101921
Iteration 10/25 | Loss: 0.00101921
Iteration 11/25 | Loss: 0.00101921
Iteration 12/25 | Loss: 0.00101921
Iteration 13/25 | Loss: 0.00101921
Iteration 14/25 | Loss: 0.00101921
Iteration 15/25 | Loss: 0.00101921
Iteration 16/25 | Loss: 0.00101921
Iteration 17/25 | Loss: 0.00101921
Iteration 18/25 | Loss: 0.00101921
Iteration 19/25 | Loss: 0.00101921
Iteration 20/25 | Loss: 0.00101921
Iteration 21/25 | Loss: 0.00101921
Iteration 22/25 | Loss: 0.00101921
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010192084591835737, 0.0010192084591835737, 0.0010192084591835737, 0.0010192084591835737, 0.0010192084591835737]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010192084591835737

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101921
Iteration 2/1000 | Loss: 0.00003028
Iteration 3/1000 | Loss: 0.00002339
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00002131
Iteration 6/1000 | Loss: 0.00002089
Iteration 7/1000 | Loss: 0.00002061
Iteration 8/1000 | Loss: 0.00002057
Iteration 9/1000 | Loss: 0.00002033
Iteration 10/1000 | Loss: 0.00002011
Iteration 11/1000 | Loss: 0.00001995
Iteration 12/1000 | Loss: 0.00001991
Iteration 13/1000 | Loss: 0.00001982
Iteration 14/1000 | Loss: 0.00001972
Iteration 15/1000 | Loss: 0.00001968
Iteration 16/1000 | Loss: 0.00001964
Iteration 17/1000 | Loss: 0.00001963
Iteration 18/1000 | Loss: 0.00001956
Iteration 19/1000 | Loss: 0.00001956
Iteration 20/1000 | Loss: 0.00001955
Iteration 21/1000 | Loss: 0.00001954
Iteration 22/1000 | Loss: 0.00001953
Iteration 23/1000 | Loss: 0.00001952
Iteration 24/1000 | Loss: 0.00001952
Iteration 25/1000 | Loss: 0.00001952
Iteration 26/1000 | Loss: 0.00001951
Iteration 27/1000 | Loss: 0.00001951
Iteration 28/1000 | Loss: 0.00001951
Iteration 29/1000 | Loss: 0.00001951
Iteration 30/1000 | Loss: 0.00001950
Iteration 31/1000 | Loss: 0.00001950
Iteration 32/1000 | Loss: 0.00001950
Iteration 33/1000 | Loss: 0.00001949
Iteration 34/1000 | Loss: 0.00001949
Iteration 35/1000 | Loss: 0.00001948
Iteration 36/1000 | Loss: 0.00001948
Iteration 37/1000 | Loss: 0.00001948
Iteration 38/1000 | Loss: 0.00001948
Iteration 39/1000 | Loss: 0.00001947
Iteration 40/1000 | Loss: 0.00001947
Iteration 41/1000 | Loss: 0.00001947
Iteration 42/1000 | Loss: 0.00001946
Iteration 43/1000 | Loss: 0.00001946
Iteration 44/1000 | Loss: 0.00001946
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001944
Iteration 47/1000 | Loss: 0.00001944
Iteration 48/1000 | Loss: 0.00001943
Iteration 49/1000 | Loss: 0.00001943
Iteration 50/1000 | Loss: 0.00001943
Iteration 51/1000 | Loss: 0.00001943
Iteration 52/1000 | Loss: 0.00001942
Iteration 53/1000 | Loss: 0.00001942
Iteration 54/1000 | Loss: 0.00001941
Iteration 55/1000 | Loss: 0.00001941
Iteration 56/1000 | Loss: 0.00001941
Iteration 57/1000 | Loss: 0.00001940
Iteration 58/1000 | Loss: 0.00001940
Iteration 59/1000 | Loss: 0.00001940
Iteration 60/1000 | Loss: 0.00001940
Iteration 61/1000 | Loss: 0.00001939
Iteration 62/1000 | Loss: 0.00001939
Iteration 63/1000 | Loss: 0.00001939
Iteration 64/1000 | Loss: 0.00001939
Iteration 65/1000 | Loss: 0.00001938
Iteration 66/1000 | Loss: 0.00001938
Iteration 67/1000 | Loss: 0.00001938
Iteration 68/1000 | Loss: 0.00001937
Iteration 69/1000 | Loss: 0.00001937
Iteration 70/1000 | Loss: 0.00001937
Iteration 71/1000 | Loss: 0.00001936
Iteration 72/1000 | Loss: 0.00001936
Iteration 73/1000 | Loss: 0.00001936
Iteration 74/1000 | Loss: 0.00001936
Iteration 75/1000 | Loss: 0.00001936
Iteration 76/1000 | Loss: 0.00001936
Iteration 77/1000 | Loss: 0.00001936
Iteration 78/1000 | Loss: 0.00001935
Iteration 79/1000 | Loss: 0.00001935
Iteration 80/1000 | Loss: 0.00001935
Iteration 81/1000 | Loss: 0.00001934
Iteration 82/1000 | Loss: 0.00001934
Iteration 83/1000 | Loss: 0.00001933
Iteration 84/1000 | Loss: 0.00001933
Iteration 85/1000 | Loss: 0.00001933
Iteration 86/1000 | Loss: 0.00001933
Iteration 87/1000 | Loss: 0.00001932
Iteration 88/1000 | Loss: 0.00001932
Iteration 89/1000 | Loss: 0.00001932
Iteration 90/1000 | Loss: 0.00001932
Iteration 91/1000 | Loss: 0.00001932
Iteration 92/1000 | Loss: 0.00001931
Iteration 93/1000 | Loss: 0.00001931
Iteration 94/1000 | Loss: 0.00001931
Iteration 95/1000 | Loss: 0.00001931
Iteration 96/1000 | Loss: 0.00001931
Iteration 97/1000 | Loss: 0.00001931
Iteration 98/1000 | Loss: 0.00001931
Iteration 99/1000 | Loss: 0.00001931
Iteration 100/1000 | Loss: 0.00001931
Iteration 101/1000 | Loss: 0.00001931
Iteration 102/1000 | Loss: 0.00001931
Iteration 103/1000 | Loss: 0.00001930
Iteration 104/1000 | Loss: 0.00001930
Iteration 105/1000 | Loss: 0.00001930
Iteration 106/1000 | Loss: 0.00001929
Iteration 107/1000 | Loss: 0.00001929
Iteration 108/1000 | Loss: 0.00001929
Iteration 109/1000 | Loss: 0.00001929
Iteration 110/1000 | Loss: 0.00001929
Iteration 111/1000 | Loss: 0.00001929
Iteration 112/1000 | Loss: 0.00001928
Iteration 113/1000 | Loss: 0.00001928
Iteration 114/1000 | Loss: 0.00001928
Iteration 115/1000 | Loss: 0.00001928
Iteration 116/1000 | Loss: 0.00001928
Iteration 117/1000 | Loss: 0.00001928
Iteration 118/1000 | Loss: 0.00001927
Iteration 119/1000 | Loss: 0.00001927
Iteration 120/1000 | Loss: 0.00001927
Iteration 121/1000 | Loss: 0.00001927
Iteration 122/1000 | Loss: 0.00001927
Iteration 123/1000 | Loss: 0.00001926
Iteration 124/1000 | Loss: 0.00001926
Iteration 125/1000 | Loss: 0.00001925
Iteration 126/1000 | Loss: 0.00001925
Iteration 127/1000 | Loss: 0.00001925
Iteration 128/1000 | Loss: 0.00001925
Iteration 129/1000 | Loss: 0.00001925
Iteration 130/1000 | Loss: 0.00001925
Iteration 131/1000 | Loss: 0.00001925
Iteration 132/1000 | Loss: 0.00001924
Iteration 133/1000 | Loss: 0.00001924
Iteration 134/1000 | Loss: 0.00001924
Iteration 135/1000 | Loss: 0.00001924
Iteration 136/1000 | Loss: 0.00001924
Iteration 137/1000 | Loss: 0.00001923
Iteration 138/1000 | Loss: 0.00001923
Iteration 139/1000 | Loss: 0.00001923
Iteration 140/1000 | Loss: 0.00001923
Iteration 141/1000 | Loss: 0.00001923
Iteration 142/1000 | Loss: 0.00001923
Iteration 143/1000 | Loss: 0.00001923
Iteration 144/1000 | Loss: 0.00001922
Iteration 145/1000 | Loss: 0.00001922
Iteration 146/1000 | Loss: 0.00001922
Iteration 147/1000 | Loss: 0.00001922
Iteration 148/1000 | Loss: 0.00001922
Iteration 149/1000 | Loss: 0.00001922
Iteration 150/1000 | Loss: 0.00001922
Iteration 151/1000 | Loss: 0.00001922
Iteration 152/1000 | Loss: 0.00001922
Iteration 153/1000 | Loss: 0.00001922
Iteration 154/1000 | Loss: 0.00001922
Iteration 155/1000 | Loss: 0.00001922
Iteration 156/1000 | Loss: 0.00001922
Iteration 157/1000 | Loss: 0.00001922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.921639523061458e-05, 1.921639523061458e-05, 1.921639523061458e-05, 1.921639523061458e-05, 1.921639523061458e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.921639523061458e-05

Optimization complete. Final v2v error: 3.7052831649780273 mm

Highest mean error: 4.320254325866699 mm for frame 158

Lowest mean error: 3.5855209827423096 mm for frame 63

Saving results

Total time: 42.470853328704834
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058473
Iteration 2/25 | Loss: 0.00316879
Iteration 3/25 | Loss: 0.00236188
Iteration 4/25 | Loss: 0.00199742
Iteration 5/25 | Loss: 0.00205705
Iteration 6/25 | Loss: 0.00190300
Iteration 7/25 | Loss: 0.00189511
Iteration 8/25 | Loss: 0.00183393
Iteration 9/25 | Loss: 0.00176671
Iteration 10/25 | Loss: 0.00172752
Iteration 11/25 | Loss: 0.00165781
Iteration 12/25 | Loss: 0.00162962
Iteration 13/25 | Loss: 0.00162423
Iteration 14/25 | Loss: 0.00161941
Iteration 15/25 | Loss: 0.00161063
Iteration 16/25 | Loss: 0.00161702
Iteration 17/25 | Loss: 0.00161905
Iteration 18/25 | Loss: 0.00162165
Iteration 19/25 | Loss: 0.00161730
Iteration 20/25 | Loss: 0.00161287
Iteration 21/25 | Loss: 0.00158784
Iteration 22/25 | Loss: 0.00158503
Iteration 23/25 | Loss: 0.00158015
Iteration 24/25 | Loss: 0.00158062
Iteration 25/25 | Loss: 0.00158119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10173249
Iteration 2/25 | Loss: 0.00154005
Iteration 3/25 | Loss: 0.00122925
Iteration 4/25 | Loss: 0.00122925
Iteration 5/25 | Loss: 0.00122925
Iteration 6/25 | Loss: 0.00122924
Iteration 7/25 | Loss: 0.00122924
Iteration 8/25 | Loss: 0.00122924
Iteration 9/25 | Loss: 0.00122924
Iteration 10/25 | Loss: 0.00122924
Iteration 11/25 | Loss: 0.00122924
Iteration 12/25 | Loss: 0.00122924
Iteration 13/25 | Loss: 0.00122924
Iteration 14/25 | Loss: 0.00122924
Iteration 15/25 | Loss: 0.00122924
Iteration 16/25 | Loss: 0.00122924
Iteration 17/25 | Loss: 0.00122924
Iteration 18/25 | Loss: 0.00122924
Iteration 19/25 | Loss: 0.00122924
Iteration 20/25 | Loss: 0.00122924
Iteration 21/25 | Loss: 0.00122924
Iteration 22/25 | Loss: 0.00122924
Iteration 23/25 | Loss: 0.00122924
Iteration 24/25 | Loss: 0.00122924
Iteration 25/25 | Loss: 0.00122924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122924
Iteration 2/1000 | Loss: 0.00028349
Iteration 3/1000 | Loss: 0.00024509
Iteration 4/1000 | Loss: 0.00048113
Iteration 5/1000 | Loss: 0.00021165
Iteration 6/1000 | Loss: 0.00050593
Iteration 7/1000 | Loss: 0.00021198
Iteration 8/1000 | Loss: 0.00105468
Iteration 9/1000 | Loss: 0.00158560
Iteration 10/1000 | Loss: 0.00166567
Iteration 11/1000 | Loss: 0.00178796
Iteration 12/1000 | Loss: 0.00084988
Iteration 13/1000 | Loss: 0.00060439
Iteration 14/1000 | Loss: 0.00072562
Iteration 15/1000 | Loss: 0.00102260
Iteration 16/1000 | Loss: 0.00035987
Iteration 17/1000 | Loss: 0.00047021
Iteration 18/1000 | Loss: 0.00068317
Iteration 19/1000 | Loss: 0.00045916
Iteration 20/1000 | Loss: 0.00010883
Iteration 21/1000 | Loss: 0.00034276
Iteration 22/1000 | Loss: 0.00030912
Iteration 23/1000 | Loss: 0.00024586
Iteration 24/1000 | Loss: 0.00023508
Iteration 25/1000 | Loss: 0.00024427
Iteration 26/1000 | Loss: 0.00041011
Iteration 27/1000 | Loss: 0.00050307
Iteration 28/1000 | Loss: 0.00038665
Iteration 29/1000 | Loss: 0.00031510
Iteration 30/1000 | Loss: 0.00015281
Iteration 31/1000 | Loss: 0.00010138
Iteration 32/1000 | Loss: 0.00018848
Iteration 33/1000 | Loss: 0.00011919
Iteration 34/1000 | Loss: 0.00032940
Iteration 35/1000 | Loss: 0.00031088
Iteration 36/1000 | Loss: 0.00022851
Iteration 37/1000 | Loss: 0.00062250
Iteration 38/1000 | Loss: 0.00077358
Iteration 39/1000 | Loss: 0.00066737
Iteration 40/1000 | Loss: 0.00053818
Iteration 41/1000 | Loss: 0.00098013
Iteration 42/1000 | Loss: 0.00093593
Iteration 43/1000 | Loss: 0.00065384
Iteration 44/1000 | Loss: 0.00065072
Iteration 45/1000 | Loss: 0.00045167
Iteration 46/1000 | Loss: 0.00038304
Iteration 47/1000 | Loss: 0.00033544
Iteration 48/1000 | Loss: 0.00046570
Iteration 49/1000 | Loss: 0.00015282
Iteration 50/1000 | Loss: 0.00014242
Iteration 51/1000 | Loss: 0.00024212
Iteration 52/1000 | Loss: 0.00016952
Iteration 53/1000 | Loss: 0.00012253
Iteration 54/1000 | Loss: 0.00020019
Iteration 55/1000 | Loss: 0.00012984
Iteration 56/1000 | Loss: 0.00019912
Iteration 57/1000 | Loss: 0.00116663
Iteration 58/1000 | Loss: 0.00107429
Iteration 59/1000 | Loss: 0.00049268
Iteration 60/1000 | Loss: 0.00024370
Iteration 61/1000 | Loss: 0.00029088
Iteration 62/1000 | Loss: 0.00021119
Iteration 63/1000 | Loss: 0.00060034
Iteration 64/1000 | Loss: 0.00013202
Iteration 65/1000 | Loss: 0.00009456
Iteration 66/1000 | Loss: 0.00013906
Iteration 67/1000 | Loss: 0.00008688
Iteration 68/1000 | Loss: 0.00024032
Iteration 69/1000 | Loss: 0.00080014
Iteration 70/1000 | Loss: 0.00025422
Iteration 71/1000 | Loss: 0.00015556
Iteration 72/1000 | Loss: 0.00032334
Iteration 73/1000 | Loss: 0.00015727
Iteration 74/1000 | Loss: 0.00011953
Iteration 75/1000 | Loss: 0.00007988
Iteration 76/1000 | Loss: 0.00013230
Iteration 77/1000 | Loss: 0.00006867
Iteration 78/1000 | Loss: 0.00067970
Iteration 79/1000 | Loss: 0.00031445
Iteration 80/1000 | Loss: 0.00014262
Iteration 81/1000 | Loss: 0.00011376
Iteration 82/1000 | Loss: 0.00007470
Iteration 83/1000 | Loss: 0.00028265
Iteration 84/1000 | Loss: 0.00039678
Iteration 85/1000 | Loss: 0.00008320
Iteration 86/1000 | Loss: 0.00013064
Iteration 87/1000 | Loss: 0.00006485
Iteration 88/1000 | Loss: 0.00015254
Iteration 89/1000 | Loss: 0.00055111
Iteration 90/1000 | Loss: 0.00046855
Iteration 91/1000 | Loss: 0.00027082
Iteration 92/1000 | Loss: 0.00010824
Iteration 93/1000 | Loss: 0.00021379
Iteration 94/1000 | Loss: 0.00022690
Iteration 95/1000 | Loss: 0.00033911
Iteration 96/1000 | Loss: 0.00047144
Iteration 97/1000 | Loss: 0.00015047
Iteration 98/1000 | Loss: 0.00021399
Iteration 99/1000 | Loss: 0.00015137
Iteration 100/1000 | Loss: 0.00006858
Iteration 101/1000 | Loss: 0.00010189
Iteration 102/1000 | Loss: 0.00011221
Iteration 103/1000 | Loss: 0.00012898
Iteration 104/1000 | Loss: 0.00010275
Iteration 105/1000 | Loss: 0.00010415
Iteration 106/1000 | Loss: 0.00008605
Iteration 107/1000 | Loss: 0.00009080
Iteration 108/1000 | Loss: 0.00006830
Iteration 109/1000 | Loss: 0.00006964
Iteration 110/1000 | Loss: 0.00006046
Iteration 111/1000 | Loss: 0.00008967
Iteration 112/1000 | Loss: 0.00034210
Iteration 113/1000 | Loss: 0.00010330
Iteration 114/1000 | Loss: 0.00008200
Iteration 115/1000 | Loss: 0.00008484
Iteration 116/1000 | Loss: 0.00006646
Iteration 117/1000 | Loss: 0.00006047
Iteration 118/1000 | Loss: 0.00010617
Iteration 119/1000 | Loss: 0.00005544
Iteration 120/1000 | Loss: 0.00007251
Iteration 121/1000 | Loss: 0.00007581
Iteration 122/1000 | Loss: 0.00005111
Iteration 123/1000 | Loss: 0.00006391
Iteration 124/1000 | Loss: 0.00008431
Iteration 125/1000 | Loss: 0.00007180
Iteration 126/1000 | Loss: 0.00008696
Iteration 127/1000 | Loss: 0.00009662
Iteration 128/1000 | Loss: 0.00007628
Iteration 129/1000 | Loss: 0.00005780
Iteration 130/1000 | Loss: 0.00008485
Iteration 131/1000 | Loss: 0.00006347
Iteration 132/1000 | Loss: 0.00004892
Iteration 133/1000 | Loss: 0.00004815
Iteration 134/1000 | Loss: 0.00004756
Iteration 135/1000 | Loss: 0.00007766
Iteration 136/1000 | Loss: 0.00004687
Iteration 137/1000 | Loss: 0.00004655
Iteration 138/1000 | Loss: 0.00009580
Iteration 139/1000 | Loss: 0.00007204
Iteration 140/1000 | Loss: 0.00004615
Iteration 141/1000 | Loss: 0.00008209
Iteration 142/1000 | Loss: 0.00004576
Iteration 143/1000 | Loss: 0.00009661
Iteration 144/1000 | Loss: 0.00043327
Iteration 145/1000 | Loss: 0.00005955
Iteration 146/1000 | Loss: 0.00005206
Iteration 147/1000 | Loss: 0.00009441
Iteration 148/1000 | Loss: 0.00023335
Iteration 149/1000 | Loss: 0.00016521
Iteration 150/1000 | Loss: 0.00007497
Iteration 151/1000 | Loss: 0.00022447
Iteration 152/1000 | Loss: 0.00012877
Iteration 153/1000 | Loss: 0.00004673
Iteration 154/1000 | Loss: 0.00004900
Iteration 155/1000 | Loss: 0.00006316
Iteration 156/1000 | Loss: 0.00004316
Iteration 157/1000 | Loss: 0.00005786
Iteration 158/1000 | Loss: 0.00004272
Iteration 159/1000 | Loss: 0.00004229
Iteration 160/1000 | Loss: 0.00004208
Iteration 161/1000 | Loss: 0.00004182
Iteration 162/1000 | Loss: 0.00007891
Iteration 163/1000 | Loss: 0.00004433
Iteration 164/1000 | Loss: 0.00004385
Iteration 165/1000 | Loss: 0.00004147
Iteration 166/1000 | Loss: 0.00004147
Iteration 167/1000 | Loss: 0.00004142
Iteration 168/1000 | Loss: 0.00004142
Iteration 169/1000 | Loss: 0.00004142
Iteration 170/1000 | Loss: 0.00004142
Iteration 171/1000 | Loss: 0.00004142
Iteration 172/1000 | Loss: 0.00004141
Iteration 173/1000 | Loss: 0.00004139
Iteration 174/1000 | Loss: 0.00004139
Iteration 175/1000 | Loss: 0.00004139
Iteration 176/1000 | Loss: 0.00004139
Iteration 177/1000 | Loss: 0.00004139
Iteration 178/1000 | Loss: 0.00005955
Iteration 179/1000 | Loss: 0.00004137
Iteration 180/1000 | Loss: 0.00004137
Iteration 181/1000 | Loss: 0.00004137
Iteration 182/1000 | Loss: 0.00004137
Iteration 183/1000 | Loss: 0.00004137
Iteration 184/1000 | Loss: 0.00004137
Iteration 185/1000 | Loss: 0.00004137
Iteration 186/1000 | Loss: 0.00004136
Iteration 187/1000 | Loss: 0.00004136
Iteration 188/1000 | Loss: 0.00004136
Iteration 189/1000 | Loss: 0.00004136
Iteration 190/1000 | Loss: 0.00004136
Iteration 191/1000 | Loss: 0.00004135
Iteration 192/1000 | Loss: 0.00004135
Iteration 193/1000 | Loss: 0.00004135
Iteration 194/1000 | Loss: 0.00004135
Iteration 195/1000 | Loss: 0.00004135
Iteration 196/1000 | Loss: 0.00004135
Iteration 197/1000 | Loss: 0.00004135
Iteration 198/1000 | Loss: 0.00004135
Iteration 199/1000 | Loss: 0.00004135
Iteration 200/1000 | Loss: 0.00004135
Iteration 201/1000 | Loss: 0.00004135
Iteration 202/1000 | Loss: 0.00004135
Iteration 203/1000 | Loss: 0.00004134
Iteration 204/1000 | Loss: 0.00004134
Iteration 205/1000 | Loss: 0.00004134
Iteration 206/1000 | Loss: 0.00004134
Iteration 207/1000 | Loss: 0.00004134
Iteration 208/1000 | Loss: 0.00004134
Iteration 209/1000 | Loss: 0.00004134
Iteration 210/1000 | Loss: 0.00004134
Iteration 211/1000 | Loss: 0.00004134
Iteration 212/1000 | Loss: 0.00004134
Iteration 213/1000 | Loss: 0.00004134
Iteration 214/1000 | Loss: 0.00004134
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [4.134281334700063e-05, 4.134281334700063e-05, 4.134281334700063e-05, 4.134281334700063e-05, 4.134281334700063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.134281334700063e-05

Optimization complete. Final v2v error: 5.008325099945068 mm

Highest mean error: 13.528177261352539 mm for frame 56

Lowest mean error: 4.013628005981445 mm for frame 0

Saving results

Total time: 310.29864597320557
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00782422
Iteration 2/25 | Loss: 0.00159323
Iteration 3/25 | Loss: 0.00150492
Iteration 4/25 | Loss: 0.00149677
Iteration 5/25 | Loss: 0.00149284
Iteration 6/25 | Loss: 0.00149219
Iteration 7/25 | Loss: 0.00149220
Iteration 8/25 | Loss: 0.00149219
Iteration 9/25 | Loss: 0.00149219
Iteration 10/25 | Loss: 0.00149220
Iteration 11/25 | Loss: 0.00149220
Iteration 12/25 | Loss: 0.00149219
Iteration 13/25 | Loss: 0.00149219
Iteration 14/25 | Loss: 0.00149219
Iteration 15/25 | Loss: 0.00149219
Iteration 16/25 | Loss: 0.00149220
Iteration 17/25 | Loss: 0.00149219
Iteration 18/25 | Loss: 0.00149219
Iteration 19/25 | Loss: 0.00149219
Iteration 20/25 | Loss: 0.00149219
Iteration 21/25 | Loss: 0.00149220
Iteration 22/25 | Loss: 0.00149219
Iteration 23/25 | Loss: 0.00149220
Iteration 24/25 | Loss: 0.00149219
Iteration 25/25 | Loss: 0.00149219

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14852548
Iteration 2/25 | Loss: 0.00095392
Iteration 3/25 | Loss: 0.00095389
Iteration 4/25 | Loss: 0.00095389
Iteration 5/25 | Loss: 0.00095389
Iteration 6/25 | Loss: 0.00095388
Iteration 7/25 | Loss: 0.00095388
Iteration 8/25 | Loss: 0.00095388
Iteration 9/25 | Loss: 0.00095388
Iteration 10/25 | Loss: 0.00095388
Iteration 11/25 | Loss: 0.00095388
Iteration 12/25 | Loss: 0.00095388
Iteration 13/25 | Loss: 0.00095388
Iteration 14/25 | Loss: 0.00095388
Iteration 15/25 | Loss: 0.00095388
Iteration 16/25 | Loss: 0.00095388
Iteration 17/25 | Loss: 0.00095388
Iteration 18/25 | Loss: 0.00095388
Iteration 19/25 | Loss: 0.00095388
Iteration 20/25 | Loss: 0.00095388
Iteration 21/25 | Loss: 0.00095388
Iteration 22/25 | Loss: 0.00095388
Iteration 23/25 | Loss: 0.00095388
Iteration 24/25 | Loss: 0.00095388
Iteration 25/25 | Loss: 0.00095388

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095388
Iteration 2/1000 | Loss: 0.00005239
Iteration 3/1000 | Loss: 0.00003823
Iteration 4/1000 | Loss: 0.00003467
Iteration 5/1000 | Loss: 0.00003292
Iteration 6/1000 | Loss: 0.00003199
Iteration 7/1000 | Loss: 0.00003118
Iteration 8/1000 | Loss: 0.00003053
Iteration 9/1000 | Loss: 0.00003000
Iteration 10/1000 | Loss: 0.00002959
Iteration 11/1000 | Loss: 0.00002932
Iteration 12/1000 | Loss: 0.00002905
Iteration 13/1000 | Loss: 0.00002896
Iteration 14/1000 | Loss: 0.00002879
Iteration 15/1000 | Loss: 0.00002875
Iteration 16/1000 | Loss: 0.00002868
Iteration 17/1000 | Loss: 0.00002859
Iteration 18/1000 | Loss: 0.00002858
Iteration 19/1000 | Loss: 0.00002856
Iteration 20/1000 | Loss: 0.00002855
Iteration 21/1000 | Loss: 0.00002853
Iteration 22/1000 | Loss: 0.00002852
Iteration 23/1000 | Loss: 0.00002851
Iteration 24/1000 | Loss: 0.00002851
Iteration 25/1000 | Loss: 0.00002848
Iteration 26/1000 | Loss: 0.00002837
Iteration 27/1000 | Loss: 0.00002832
Iteration 28/1000 | Loss: 0.00002828
Iteration 29/1000 | Loss: 0.00002827
Iteration 30/1000 | Loss: 0.00002827
Iteration 31/1000 | Loss: 0.00002826
Iteration 32/1000 | Loss: 0.00002826
Iteration 33/1000 | Loss: 0.00002825
Iteration 34/1000 | Loss: 0.00002825
Iteration 35/1000 | Loss: 0.00002824
Iteration 36/1000 | Loss: 0.00002824
Iteration 37/1000 | Loss: 0.00002823
Iteration 38/1000 | Loss: 0.00002821
Iteration 39/1000 | Loss: 0.00002821
Iteration 40/1000 | Loss: 0.00002820
Iteration 41/1000 | Loss: 0.00002820
Iteration 42/1000 | Loss: 0.00002819
Iteration 43/1000 | Loss: 0.00002819
Iteration 44/1000 | Loss: 0.00002818
Iteration 45/1000 | Loss: 0.00002818
Iteration 46/1000 | Loss: 0.00002818
Iteration 47/1000 | Loss: 0.00002818
Iteration 48/1000 | Loss: 0.00002818
Iteration 49/1000 | Loss: 0.00002817
Iteration 50/1000 | Loss: 0.00002817
Iteration 51/1000 | Loss: 0.00002817
Iteration 52/1000 | Loss: 0.00002817
Iteration 53/1000 | Loss: 0.00002817
Iteration 54/1000 | Loss: 0.00002817
Iteration 55/1000 | Loss: 0.00002817
Iteration 56/1000 | Loss: 0.00002817
Iteration 57/1000 | Loss: 0.00002816
Iteration 58/1000 | Loss: 0.00002816
Iteration 59/1000 | Loss: 0.00002816
Iteration 60/1000 | Loss: 0.00002816
Iteration 61/1000 | Loss: 0.00002816
Iteration 62/1000 | Loss: 0.00002816
Iteration 63/1000 | Loss: 0.00002815
Iteration 64/1000 | Loss: 0.00002815
Iteration 65/1000 | Loss: 0.00002815
Iteration 66/1000 | Loss: 0.00002815
Iteration 67/1000 | Loss: 0.00002815
Iteration 68/1000 | Loss: 0.00002815
Iteration 69/1000 | Loss: 0.00002815
Iteration 70/1000 | Loss: 0.00002815
Iteration 71/1000 | Loss: 0.00002815
Iteration 72/1000 | Loss: 0.00002815
Iteration 73/1000 | Loss: 0.00002815
Iteration 74/1000 | Loss: 0.00002815
Iteration 75/1000 | Loss: 0.00002815
Iteration 76/1000 | Loss: 0.00002814
Iteration 77/1000 | Loss: 0.00002814
Iteration 78/1000 | Loss: 0.00002814
Iteration 79/1000 | Loss: 0.00002814
Iteration 80/1000 | Loss: 0.00002814
Iteration 81/1000 | Loss: 0.00002814
Iteration 82/1000 | Loss: 0.00002814
Iteration 83/1000 | Loss: 0.00002814
Iteration 84/1000 | Loss: 0.00002814
Iteration 85/1000 | Loss: 0.00002814
Iteration 86/1000 | Loss: 0.00002814
Iteration 87/1000 | Loss: 0.00002814
Iteration 88/1000 | Loss: 0.00002814
Iteration 89/1000 | Loss: 0.00002814
Iteration 90/1000 | Loss: 0.00002814
Iteration 91/1000 | Loss: 0.00002814
Iteration 92/1000 | Loss: 0.00002814
Iteration 93/1000 | Loss: 0.00002814
Iteration 94/1000 | Loss: 0.00002814
Iteration 95/1000 | Loss: 0.00002814
Iteration 96/1000 | Loss: 0.00002814
Iteration 97/1000 | Loss: 0.00002814
Iteration 98/1000 | Loss: 0.00002814
Iteration 99/1000 | Loss: 0.00002814
Iteration 100/1000 | Loss: 0.00002814
Iteration 101/1000 | Loss: 0.00002814
Iteration 102/1000 | Loss: 0.00002814
Iteration 103/1000 | Loss: 0.00002814
Iteration 104/1000 | Loss: 0.00002814
Iteration 105/1000 | Loss: 0.00002814
Iteration 106/1000 | Loss: 0.00002814
Iteration 107/1000 | Loss: 0.00002814
Iteration 108/1000 | Loss: 0.00002814
Iteration 109/1000 | Loss: 0.00002814
Iteration 110/1000 | Loss: 0.00002814
Iteration 111/1000 | Loss: 0.00002814
Iteration 112/1000 | Loss: 0.00002814
Iteration 113/1000 | Loss: 0.00002814
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [2.813945320667699e-05, 2.813945320667699e-05, 2.813945320667699e-05, 2.813945320667699e-05, 2.813945320667699e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.813945320667699e-05

Optimization complete. Final v2v error: 4.375592231750488 mm

Highest mean error: 4.531277179718018 mm for frame 108

Lowest mean error: 4.210677623748779 mm for frame 2

Saving results

Total time: 37.53233599662781
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024415
Iteration 2/25 | Loss: 0.00151235
Iteration 3/25 | Loss: 0.00136701
Iteration 4/25 | Loss: 0.00135240
Iteration 5/25 | Loss: 0.00134985
Iteration 6/25 | Loss: 0.00134933
Iteration 7/25 | Loss: 0.00134933
Iteration 8/25 | Loss: 0.00134933
Iteration 9/25 | Loss: 0.00134933
Iteration 10/25 | Loss: 0.00134933
Iteration 11/25 | Loss: 0.00134933
Iteration 12/25 | Loss: 0.00134933
Iteration 13/25 | Loss: 0.00134933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013493314618244767, 0.0013493314618244767, 0.0013493314618244767, 0.0013493314618244767, 0.0013493314618244767]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013493314618244767

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94438452
Iteration 2/25 | Loss: 0.00093144
Iteration 3/25 | Loss: 0.00093143
Iteration 4/25 | Loss: 0.00093143
Iteration 5/25 | Loss: 0.00093143
Iteration 6/25 | Loss: 0.00093143
Iteration 7/25 | Loss: 0.00093143
Iteration 8/25 | Loss: 0.00093143
Iteration 9/25 | Loss: 0.00093143
Iteration 10/25 | Loss: 0.00093143
Iteration 11/25 | Loss: 0.00093143
Iteration 12/25 | Loss: 0.00093143
Iteration 13/25 | Loss: 0.00093143
Iteration 14/25 | Loss: 0.00093143
Iteration 15/25 | Loss: 0.00093143
Iteration 16/25 | Loss: 0.00093143
Iteration 17/25 | Loss: 0.00093143
Iteration 18/25 | Loss: 0.00093143
Iteration 19/25 | Loss: 0.00093143
Iteration 20/25 | Loss: 0.00093143
Iteration 21/25 | Loss: 0.00093143
Iteration 22/25 | Loss: 0.00093143
Iteration 23/25 | Loss: 0.00093143
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0009314316557720304, 0.0009314316557720304, 0.0009314316557720304, 0.0009314316557720304, 0.0009314316557720304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009314316557720304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093143
Iteration 2/1000 | Loss: 0.00004830
Iteration 3/1000 | Loss: 0.00003267
Iteration 4/1000 | Loss: 0.00002433
Iteration 5/1000 | Loss: 0.00002232
Iteration 6/1000 | Loss: 0.00002117
Iteration 7/1000 | Loss: 0.00002054
Iteration 8/1000 | Loss: 0.00002008
Iteration 9/1000 | Loss: 0.00001977
Iteration 10/1000 | Loss: 0.00001949
Iteration 11/1000 | Loss: 0.00001933
Iteration 12/1000 | Loss: 0.00001916
Iteration 13/1000 | Loss: 0.00001902
Iteration 14/1000 | Loss: 0.00001901
Iteration 15/1000 | Loss: 0.00001901
Iteration 16/1000 | Loss: 0.00001901
Iteration 17/1000 | Loss: 0.00001901
Iteration 18/1000 | Loss: 0.00001895
Iteration 19/1000 | Loss: 0.00001892
Iteration 20/1000 | Loss: 0.00001890
Iteration 21/1000 | Loss: 0.00001889
Iteration 22/1000 | Loss: 0.00001885
Iteration 23/1000 | Loss: 0.00001883
Iteration 24/1000 | Loss: 0.00001883
Iteration 25/1000 | Loss: 0.00001882
Iteration 26/1000 | Loss: 0.00001881
Iteration 27/1000 | Loss: 0.00001881
Iteration 28/1000 | Loss: 0.00001878
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001876
Iteration 31/1000 | Loss: 0.00001875
Iteration 32/1000 | Loss: 0.00001875
Iteration 33/1000 | Loss: 0.00001875
Iteration 34/1000 | Loss: 0.00001874
Iteration 35/1000 | Loss: 0.00001874
Iteration 36/1000 | Loss: 0.00001873
Iteration 37/1000 | Loss: 0.00001872
Iteration 38/1000 | Loss: 0.00001869
Iteration 39/1000 | Loss: 0.00001868
Iteration 40/1000 | Loss: 0.00001868
Iteration 41/1000 | Loss: 0.00001868
Iteration 42/1000 | Loss: 0.00001868
Iteration 43/1000 | Loss: 0.00001868
Iteration 44/1000 | Loss: 0.00001867
Iteration 45/1000 | Loss: 0.00001867
Iteration 46/1000 | Loss: 0.00001867
Iteration 47/1000 | Loss: 0.00001867
Iteration 48/1000 | Loss: 0.00001867
Iteration 49/1000 | Loss: 0.00001866
Iteration 50/1000 | Loss: 0.00001866
Iteration 51/1000 | Loss: 0.00001866
Iteration 52/1000 | Loss: 0.00001865
Iteration 53/1000 | Loss: 0.00001865
Iteration 54/1000 | Loss: 0.00001865
Iteration 55/1000 | Loss: 0.00001865
Iteration 56/1000 | Loss: 0.00001865
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001864
Iteration 59/1000 | Loss: 0.00001864
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001864
Iteration 62/1000 | Loss: 0.00001864
Iteration 63/1000 | Loss: 0.00001864
Iteration 64/1000 | Loss: 0.00001864
Iteration 65/1000 | Loss: 0.00001864
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001864
Iteration 68/1000 | Loss: 0.00001864
Iteration 69/1000 | Loss: 0.00001864
Iteration 70/1000 | Loss: 0.00001864
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001863
Iteration 74/1000 | Loss: 0.00001863
Iteration 75/1000 | Loss: 0.00001863
Iteration 76/1000 | Loss: 0.00001863
Iteration 77/1000 | Loss: 0.00001863
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001862
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001862
Iteration 85/1000 | Loss: 0.00001861
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001861
Iteration 89/1000 | Loss: 0.00001861
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001860
Iteration 94/1000 | Loss: 0.00001860
Iteration 95/1000 | Loss: 0.00001860
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001860
Iteration 98/1000 | Loss: 0.00001860
Iteration 99/1000 | Loss: 0.00001860
Iteration 100/1000 | Loss: 0.00001860
Iteration 101/1000 | Loss: 0.00001860
Iteration 102/1000 | Loss: 0.00001860
Iteration 103/1000 | Loss: 0.00001860
Iteration 104/1000 | Loss: 0.00001860
Iteration 105/1000 | Loss: 0.00001860
Iteration 106/1000 | Loss: 0.00001860
Iteration 107/1000 | Loss: 0.00001860
Iteration 108/1000 | Loss: 0.00001860
Iteration 109/1000 | Loss: 0.00001860
Iteration 110/1000 | Loss: 0.00001860
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001860
Iteration 113/1000 | Loss: 0.00001860
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001860
Iteration 116/1000 | Loss: 0.00001860
Iteration 117/1000 | Loss: 0.00001860
Iteration 118/1000 | Loss: 0.00001860
Iteration 119/1000 | Loss: 0.00001860
Iteration 120/1000 | Loss: 0.00001860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [1.859921212599147e-05, 1.859921212599147e-05, 1.859921212599147e-05, 1.859921212599147e-05, 1.859921212599147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.859921212599147e-05

Optimization complete. Final v2v error: 3.6090457439422607 mm

Highest mean error: 4.020238399505615 mm for frame 50

Lowest mean error: 3.2750349044799805 mm for frame 14

Saving results

Total time: 33.28179049491882
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055544
Iteration 2/25 | Loss: 0.00424750
Iteration 3/25 | Loss: 0.00250977
Iteration 4/25 | Loss: 0.00220448
Iteration 5/25 | Loss: 0.00208723
Iteration 6/25 | Loss: 0.00199558
Iteration 7/25 | Loss: 0.00191317
Iteration 8/25 | Loss: 0.00183724
Iteration 9/25 | Loss: 0.00178413
Iteration 10/25 | Loss: 0.00173752
Iteration 11/25 | Loss: 0.00171709
Iteration 12/25 | Loss: 0.00171140
Iteration 13/25 | Loss: 0.00171546
Iteration 14/25 | Loss: 0.00170757
Iteration 15/25 | Loss: 0.00170554
Iteration 16/25 | Loss: 0.00170451
Iteration 17/25 | Loss: 0.00170742
Iteration 18/25 | Loss: 0.00170473
Iteration 19/25 | Loss: 0.00170064
Iteration 20/25 | Loss: 0.00169853
Iteration 21/25 | Loss: 0.00170053
Iteration 22/25 | Loss: 0.00169832
Iteration 23/25 | Loss: 0.00169833
Iteration 24/25 | Loss: 0.00169832
Iteration 25/25 | Loss: 0.00169829

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.55474311
Iteration 2/25 | Loss: 0.00299156
Iteration 3/25 | Loss: 0.00299156
Iteration 4/25 | Loss: 0.00228426
Iteration 5/25 | Loss: 0.00227186
Iteration 6/25 | Loss: 0.00227185
Iteration 7/25 | Loss: 0.00227185
Iteration 8/25 | Loss: 0.00227185
Iteration 9/25 | Loss: 0.00227185
Iteration 10/25 | Loss: 0.00227185
Iteration 11/25 | Loss: 0.00227185
Iteration 12/25 | Loss: 0.00227185
Iteration 13/25 | Loss: 0.00227185
Iteration 14/25 | Loss: 0.00227185
Iteration 15/25 | Loss: 0.00227185
Iteration 16/25 | Loss: 0.00227185
Iteration 17/25 | Loss: 0.00227185
Iteration 18/25 | Loss: 0.00227185
Iteration 19/25 | Loss: 0.00227185
Iteration 20/25 | Loss: 0.00227185
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.002271846402436495, 0.002271846402436495, 0.002271846402436495, 0.002271846402436495, 0.002271846402436495]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002271846402436495

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00227185
Iteration 2/1000 | Loss: 0.00034612
Iteration 3/1000 | Loss: 0.00039563
Iteration 4/1000 | Loss: 0.00109089
Iteration 5/1000 | Loss: 0.00179652
Iteration 6/1000 | Loss: 0.00082709
Iteration 7/1000 | Loss: 0.00142599
Iteration 8/1000 | Loss: 0.00255861
Iteration 9/1000 | Loss: 0.00031129
Iteration 10/1000 | Loss: 0.00051218
Iteration 11/1000 | Loss: 0.00055820
Iteration 12/1000 | Loss: 0.00028011
Iteration 13/1000 | Loss: 0.00018868
Iteration 14/1000 | Loss: 0.00019292
Iteration 15/1000 | Loss: 0.00017972
Iteration 16/1000 | Loss: 0.00016907
Iteration 17/1000 | Loss: 0.00043361
Iteration 18/1000 | Loss: 0.00015113
Iteration 19/1000 | Loss: 0.00019407
Iteration 20/1000 | Loss: 0.00013916
Iteration 21/1000 | Loss: 0.00030407
Iteration 22/1000 | Loss: 0.00431368
Iteration 23/1000 | Loss: 0.00225320
Iteration 24/1000 | Loss: 0.00037075
Iteration 25/1000 | Loss: 0.00039999
Iteration 26/1000 | Loss: 0.00023796
Iteration 27/1000 | Loss: 0.00089226
Iteration 28/1000 | Loss: 0.00148929
Iteration 29/1000 | Loss: 0.00085723
Iteration 30/1000 | Loss: 0.00017581
Iteration 31/1000 | Loss: 0.00017167
Iteration 32/1000 | Loss: 0.00010280
Iteration 33/1000 | Loss: 0.00012626
Iteration 34/1000 | Loss: 0.00004164
Iteration 35/1000 | Loss: 0.00051265
Iteration 36/1000 | Loss: 0.00008522
Iteration 37/1000 | Loss: 0.00021987
Iteration 38/1000 | Loss: 0.00004580
Iteration 39/1000 | Loss: 0.00011863
Iteration 40/1000 | Loss: 0.00007370
Iteration 41/1000 | Loss: 0.00005529
Iteration 42/1000 | Loss: 0.00002937
Iteration 43/1000 | Loss: 0.00009810
Iteration 44/1000 | Loss: 0.00009012
Iteration 45/1000 | Loss: 0.00006396
Iteration 46/1000 | Loss: 0.00005733
Iteration 47/1000 | Loss: 0.00019810
Iteration 48/1000 | Loss: 0.00003619
Iteration 49/1000 | Loss: 0.00002875
Iteration 50/1000 | Loss: 0.00003321
Iteration 51/1000 | Loss: 0.00005193
Iteration 52/1000 | Loss: 0.00005623
Iteration 53/1000 | Loss: 0.00005349
Iteration 54/1000 | Loss: 0.00004432
Iteration 55/1000 | Loss: 0.00005655
Iteration 56/1000 | Loss: 0.00028067
Iteration 57/1000 | Loss: 0.00004853
Iteration 58/1000 | Loss: 0.00004721
Iteration 59/1000 | Loss: 0.00002562
Iteration 60/1000 | Loss: 0.00003364
Iteration 61/1000 | Loss: 0.00003445
Iteration 62/1000 | Loss: 0.00002538
Iteration 63/1000 | Loss: 0.00004236
Iteration 64/1000 | Loss: 0.00002523
Iteration 65/1000 | Loss: 0.00003044
Iteration 66/1000 | Loss: 0.00002515
Iteration 67/1000 | Loss: 0.00002514
Iteration 68/1000 | Loss: 0.00002514
Iteration 69/1000 | Loss: 0.00002514
Iteration 70/1000 | Loss: 0.00002512
Iteration 71/1000 | Loss: 0.00002506
Iteration 72/1000 | Loss: 0.00002506
Iteration 73/1000 | Loss: 0.00002506
Iteration 74/1000 | Loss: 0.00002506
Iteration 75/1000 | Loss: 0.00002506
Iteration 76/1000 | Loss: 0.00002506
Iteration 77/1000 | Loss: 0.00002544
Iteration 78/1000 | Loss: 0.00002543
Iteration 79/1000 | Loss: 0.00007393
Iteration 80/1000 | Loss: 0.00013598
Iteration 81/1000 | Loss: 0.00004428
Iteration 82/1000 | Loss: 0.00004415
Iteration 83/1000 | Loss: 0.00002556
Iteration 84/1000 | Loss: 0.00003121
Iteration 85/1000 | Loss: 0.00004394
Iteration 86/1000 | Loss: 0.00002515
Iteration 87/1000 | Loss: 0.00002492
Iteration 88/1000 | Loss: 0.00002490
Iteration 89/1000 | Loss: 0.00002487
Iteration 90/1000 | Loss: 0.00002487
Iteration 91/1000 | Loss: 0.00002487
Iteration 92/1000 | Loss: 0.00002487
Iteration 93/1000 | Loss: 0.00002486
Iteration 94/1000 | Loss: 0.00002486
Iteration 95/1000 | Loss: 0.00002486
Iteration 96/1000 | Loss: 0.00002486
Iteration 97/1000 | Loss: 0.00002486
Iteration 98/1000 | Loss: 0.00002486
Iteration 99/1000 | Loss: 0.00002486
Iteration 100/1000 | Loss: 0.00002486
Iteration 101/1000 | Loss: 0.00002486
Iteration 102/1000 | Loss: 0.00002486
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002486
Iteration 105/1000 | Loss: 0.00002486
Iteration 106/1000 | Loss: 0.00002486
Iteration 107/1000 | Loss: 0.00002486
Iteration 108/1000 | Loss: 0.00002486
Iteration 109/1000 | Loss: 0.00002486
Iteration 110/1000 | Loss: 0.00002486
Iteration 111/1000 | Loss: 0.00002486
Iteration 112/1000 | Loss: 0.00002486
Iteration 113/1000 | Loss: 0.00002486
Iteration 114/1000 | Loss: 0.00002486
Iteration 115/1000 | Loss: 0.00002486
Iteration 116/1000 | Loss: 0.00002486
Iteration 117/1000 | Loss: 0.00002486
Iteration 118/1000 | Loss: 0.00002486
Iteration 119/1000 | Loss: 0.00002486
Iteration 120/1000 | Loss: 0.00002486
Iteration 121/1000 | Loss: 0.00002486
Iteration 122/1000 | Loss: 0.00002486
Iteration 123/1000 | Loss: 0.00002486
Iteration 124/1000 | Loss: 0.00002486
Iteration 125/1000 | Loss: 0.00002486
Iteration 126/1000 | Loss: 0.00002486
Iteration 127/1000 | Loss: 0.00002486
Iteration 128/1000 | Loss: 0.00002486
Iteration 129/1000 | Loss: 0.00002486
Iteration 130/1000 | Loss: 0.00002486
Iteration 131/1000 | Loss: 0.00002486
Iteration 132/1000 | Loss: 0.00002486
Iteration 133/1000 | Loss: 0.00002486
Iteration 134/1000 | Loss: 0.00002486
Iteration 135/1000 | Loss: 0.00002486
Iteration 136/1000 | Loss: 0.00002486
Iteration 137/1000 | Loss: 0.00002486
Iteration 138/1000 | Loss: 0.00002486
Iteration 139/1000 | Loss: 0.00002486
Iteration 140/1000 | Loss: 0.00002486
Iteration 141/1000 | Loss: 0.00002486
Iteration 142/1000 | Loss: 0.00002486
Iteration 143/1000 | Loss: 0.00002486
Iteration 144/1000 | Loss: 0.00002486
Iteration 145/1000 | Loss: 0.00002486
Iteration 146/1000 | Loss: 0.00002486
Iteration 147/1000 | Loss: 0.00002486
Iteration 148/1000 | Loss: 0.00002486
Iteration 149/1000 | Loss: 0.00002486
Iteration 150/1000 | Loss: 0.00002486
Iteration 151/1000 | Loss: 0.00002486
Iteration 152/1000 | Loss: 0.00002486
Iteration 153/1000 | Loss: 0.00002486
Iteration 154/1000 | Loss: 0.00002486
Iteration 155/1000 | Loss: 0.00002486
Iteration 156/1000 | Loss: 0.00002486
Iteration 157/1000 | Loss: 0.00002486
Iteration 158/1000 | Loss: 0.00002486
Iteration 159/1000 | Loss: 0.00002486
Iteration 160/1000 | Loss: 0.00002486
Iteration 161/1000 | Loss: 0.00002486
Iteration 162/1000 | Loss: 0.00002486
Iteration 163/1000 | Loss: 0.00002486
Iteration 164/1000 | Loss: 0.00002486
Iteration 165/1000 | Loss: 0.00002486
Iteration 166/1000 | Loss: 0.00002486
Iteration 167/1000 | Loss: 0.00002486
Iteration 168/1000 | Loss: 0.00002486
Iteration 169/1000 | Loss: 0.00002486
Iteration 170/1000 | Loss: 0.00002486
Iteration 171/1000 | Loss: 0.00002486
Iteration 172/1000 | Loss: 0.00002486
Iteration 173/1000 | Loss: 0.00002486
Iteration 174/1000 | Loss: 0.00002486
Iteration 175/1000 | Loss: 0.00002486
Iteration 176/1000 | Loss: 0.00002486
Iteration 177/1000 | Loss: 0.00002486
Iteration 178/1000 | Loss: 0.00002486
Iteration 179/1000 | Loss: 0.00002486
Iteration 180/1000 | Loss: 0.00002486
Iteration 181/1000 | Loss: 0.00002486
Iteration 182/1000 | Loss: 0.00002486
Iteration 183/1000 | Loss: 0.00002486
Iteration 184/1000 | Loss: 0.00002486
Iteration 185/1000 | Loss: 0.00002486
Iteration 186/1000 | Loss: 0.00002486
Iteration 187/1000 | Loss: 0.00002486
Iteration 188/1000 | Loss: 0.00002486
Iteration 189/1000 | Loss: 0.00002486
Iteration 190/1000 | Loss: 0.00002486
Iteration 191/1000 | Loss: 0.00002486
Iteration 192/1000 | Loss: 0.00002486
Iteration 193/1000 | Loss: 0.00002486
Iteration 194/1000 | Loss: 0.00002486
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [2.4858903998392634e-05, 2.4858903998392634e-05, 2.4858903998392634e-05, 2.4858903998392634e-05, 2.4858903998392634e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4858903998392634e-05

Optimization complete. Final v2v error: 4.232476234436035 mm

Highest mean error: 4.510982513427734 mm for frame 70

Lowest mean error: 3.986475944519043 mm for frame 123

Saving results

Total time: 147.25022101402283
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00763725
Iteration 2/25 | Loss: 0.00194416
Iteration 3/25 | Loss: 0.00157588
Iteration 4/25 | Loss: 0.00147681
Iteration 5/25 | Loss: 0.00143919
Iteration 6/25 | Loss: 0.00141072
Iteration 7/25 | Loss: 0.00140629
Iteration 8/25 | Loss: 0.00140567
Iteration 9/25 | Loss: 0.00140140
Iteration 10/25 | Loss: 0.00140095
Iteration 11/25 | Loss: 0.00140086
Iteration 12/25 | Loss: 0.00140086
Iteration 13/25 | Loss: 0.00140086
Iteration 14/25 | Loss: 0.00140086
Iteration 15/25 | Loss: 0.00140085
Iteration 16/25 | Loss: 0.00140085
Iteration 17/25 | Loss: 0.00140085
Iteration 18/25 | Loss: 0.00140085
Iteration 19/25 | Loss: 0.00140085
Iteration 20/25 | Loss: 0.00140085
Iteration 21/25 | Loss: 0.00140085
Iteration 22/25 | Loss: 0.00140085
Iteration 23/25 | Loss: 0.00140085
Iteration 24/25 | Loss: 0.00140085
Iteration 25/25 | Loss: 0.00140085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35836148
Iteration 2/25 | Loss: 0.00085369
Iteration 3/25 | Loss: 0.00085367
Iteration 4/25 | Loss: 0.00085367
Iteration 5/25 | Loss: 0.00085367
Iteration 6/25 | Loss: 0.00085367
Iteration 7/25 | Loss: 0.00085367
Iteration 8/25 | Loss: 0.00085367
Iteration 9/25 | Loss: 0.00085367
Iteration 10/25 | Loss: 0.00085367
Iteration 11/25 | Loss: 0.00085367
Iteration 12/25 | Loss: 0.00085367
Iteration 13/25 | Loss: 0.00085367
Iteration 14/25 | Loss: 0.00085367
Iteration 15/25 | Loss: 0.00085367
Iteration 16/25 | Loss: 0.00085367
Iteration 17/25 | Loss: 0.00085367
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008536704117432237, 0.0008536704117432237, 0.0008536704117432237, 0.0008536704117432237, 0.0008536704117432237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008536704117432237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085367
Iteration 2/1000 | Loss: 0.00004196
Iteration 3/1000 | Loss: 0.00002636
Iteration 4/1000 | Loss: 0.00002406
Iteration 5/1000 | Loss: 0.00002312
Iteration 6/1000 | Loss: 0.00002242
Iteration 7/1000 | Loss: 0.00002216
Iteration 8/1000 | Loss: 0.00002180
Iteration 9/1000 | Loss: 0.00002144
Iteration 10/1000 | Loss: 0.00002126
Iteration 11/1000 | Loss: 0.00002124
Iteration 12/1000 | Loss: 0.00002123
Iteration 13/1000 | Loss: 0.00002109
Iteration 14/1000 | Loss: 0.00002097
Iteration 15/1000 | Loss: 0.00002096
Iteration 16/1000 | Loss: 0.00002095
Iteration 17/1000 | Loss: 0.00002095
Iteration 18/1000 | Loss: 0.00002094
Iteration 19/1000 | Loss: 0.00002094
Iteration 20/1000 | Loss: 0.00002088
Iteration 21/1000 | Loss: 0.00002088
Iteration 22/1000 | Loss: 0.00002084
Iteration 23/1000 | Loss: 0.00002084
Iteration 24/1000 | Loss: 0.00002083
Iteration 25/1000 | Loss: 0.00002082
Iteration 26/1000 | Loss: 0.00002082
Iteration 27/1000 | Loss: 0.00002082
Iteration 28/1000 | Loss: 0.00002081
Iteration 29/1000 | Loss: 0.00002078
Iteration 30/1000 | Loss: 0.00002078
Iteration 31/1000 | Loss: 0.00002078
Iteration 32/1000 | Loss: 0.00002073
Iteration 33/1000 | Loss: 0.00002073
Iteration 34/1000 | Loss: 0.00002073
Iteration 35/1000 | Loss: 0.00002072
Iteration 36/1000 | Loss: 0.00002072
Iteration 37/1000 | Loss: 0.00002072
Iteration 38/1000 | Loss: 0.00002072
Iteration 39/1000 | Loss: 0.00002072
Iteration 40/1000 | Loss: 0.00002072
Iteration 41/1000 | Loss: 0.00002072
Iteration 42/1000 | Loss: 0.00002071
Iteration 43/1000 | Loss: 0.00002071
Iteration 44/1000 | Loss: 0.00002070
Iteration 45/1000 | Loss: 0.00002070
Iteration 46/1000 | Loss: 0.00002070
Iteration 47/1000 | Loss: 0.00002070
Iteration 48/1000 | Loss: 0.00002070
Iteration 49/1000 | Loss: 0.00002070
Iteration 50/1000 | Loss: 0.00002069
Iteration 51/1000 | Loss: 0.00002069
Iteration 52/1000 | Loss: 0.00002069
Iteration 53/1000 | Loss: 0.00002069
Iteration 54/1000 | Loss: 0.00002069
Iteration 55/1000 | Loss: 0.00002069
Iteration 56/1000 | Loss: 0.00002069
Iteration 57/1000 | Loss: 0.00002069
Iteration 58/1000 | Loss: 0.00002068
Iteration 59/1000 | Loss: 0.00002068
Iteration 60/1000 | Loss: 0.00002067
Iteration 61/1000 | Loss: 0.00002067
Iteration 62/1000 | Loss: 0.00002067
Iteration 63/1000 | Loss: 0.00002066
Iteration 64/1000 | Loss: 0.00002066
Iteration 65/1000 | Loss: 0.00002066
Iteration 66/1000 | Loss: 0.00002066
Iteration 67/1000 | Loss: 0.00002064
Iteration 68/1000 | Loss: 0.00002064
Iteration 69/1000 | Loss: 0.00002064
Iteration 70/1000 | Loss: 0.00002063
Iteration 71/1000 | Loss: 0.00002063
Iteration 72/1000 | Loss: 0.00002063
Iteration 73/1000 | Loss: 0.00002063
Iteration 74/1000 | Loss: 0.00002063
Iteration 75/1000 | Loss: 0.00002062
Iteration 76/1000 | Loss: 0.00002061
Iteration 77/1000 | Loss: 0.00002061
Iteration 78/1000 | Loss: 0.00002061
Iteration 79/1000 | Loss: 0.00002061
Iteration 80/1000 | Loss: 0.00002060
Iteration 81/1000 | Loss: 0.00002060
Iteration 82/1000 | Loss: 0.00002060
Iteration 83/1000 | Loss: 0.00002060
Iteration 84/1000 | Loss: 0.00002060
Iteration 85/1000 | Loss: 0.00002059
Iteration 86/1000 | Loss: 0.00002058
Iteration 87/1000 | Loss: 0.00002058
Iteration 88/1000 | Loss: 0.00002057
Iteration 89/1000 | Loss: 0.00002057
Iteration 90/1000 | Loss: 0.00002057
Iteration 91/1000 | Loss: 0.00002057
Iteration 92/1000 | Loss: 0.00002057
Iteration 93/1000 | Loss: 0.00002057
Iteration 94/1000 | Loss: 0.00002057
Iteration 95/1000 | Loss: 0.00002057
Iteration 96/1000 | Loss: 0.00002057
Iteration 97/1000 | Loss: 0.00002057
Iteration 98/1000 | Loss: 0.00002056
Iteration 99/1000 | Loss: 0.00002056
Iteration 100/1000 | Loss: 0.00002055
Iteration 101/1000 | Loss: 0.00002055
Iteration 102/1000 | Loss: 0.00002055
Iteration 103/1000 | Loss: 0.00002055
Iteration 104/1000 | Loss: 0.00002054
Iteration 105/1000 | Loss: 0.00002054
Iteration 106/1000 | Loss: 0.00002054
Iteration 107/1000 | Loss: 0.00002054
Iteration 108/1000 | Loss: 0.00002054
Iteration 109/1000 | Loss: 0.00002054
Iteration 110/1000 | Loss: 0.00002054
Iteration 111/1000 | Loss: 0.00002054
Iteration 112/1000 | Loss: 0.00002054
Iteration 113/1000 | Loss: 0.00002054
Iteration 114/1000 | Loss: 0.00002054
Iteration 115/1000 | Loss: 0.00002054
Iteration 116/1000 | Loss: 0.00002054
Iteration 117/1000 | Loss: 0.00002053
Iteration 118/1000 | Loss: 0.00002053
Iteration 119/1000 | Loss: 0.00002053
Iteration 120/1000 | Loss: 0.00002053
Iteration 121/1000 | Loss: 0.00002053
Iteration 122/1000 | Loss: 0.00002053
Iteration 123/1000 | Loss: 0.00002053
Iteration 124/1000 | Loss: 0.00002052
Iteration 125/1000 | Loss: 0.00002052
Iteration 126/1000 | Loss: 0.00002052
Iteration 127/1000 | Loss: 0.00002052
Iteration 128/1000 | Loss: 0.00002052
Iteration 129/1000 | Loss: 0.00002052
Iteration 130/1000 | Loss: 0.00002052
Iteration 131/1000 | Loss: 0.00002052
Iteration 132/1000 | Loss: 0.00002052
Iteration 133/1000 | Loss: 0.00002051
Iteration 134/1000 | Loss: 0.00002051
Iteration 135/1000 | Loss: 0.00002051
Iteration 136/1000 | Loss: 0.00002050
Iteration 137/1000 | Loss: 0.00002050
Iteration 138/1000 | Loss: 0.00002050
Iteration 139/1000 | Loss: 0.00002050
Iteration 140/1000 | Loss: 0.00002050
Iteration 141/1000 | Loss: 0.00002050
Iteration 142/1000 | Loss: 0.00002050
Iteration 143/1000 | Loss: 0.00002050
Iteration 144/1000 | Loss: 0.00002050
Iteration 145/1000 | Loss: 0.00002050
Iteration 146/1000 | Loss: 0.00002050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [2.0501487597357482e-05, 2.0501487597357482e-05, 2.0501487597357482e-05, 2.0501487597357482e-05, 2.0501487597357482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0501487597357482e-05

Optimization complete. Final v2v error: 3.776172637939453 mm

Highest mean error: 4.287806034088135 mm for frame 181

Lowest mean error: 3.582686424255371 mm for frame 239

Saving results

Total time: 54.019269704818726
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00598234
Iteration 2/25 | Loss: 0.00134260
Iteration 3/25 | Loss: 0.00128046
Iteration 4/25 | Loss: 0.00127160
Iteration 5/25 | Loss: 0.00126904
Iteration 6/25 | Loss: 0.00126886
Iteration 7/25 | Loss: 0.00126886
Iteration 8/25 | Loss: 0.00126886
Iteration 9/25 | Loss: 0.00126886
Iteration 10/25 | Loss: 0.00126886
Iteration 11/25 | Loss: 0.00126886
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001268858672119677, 0.001268858672119677, 0.001268858672119677, 0.001268858672119677, 0.001268858672119677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001268858672119677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.13208127
Iteration 2/25 | Loss: 0.00091090
Iteration 3/25 | Loss: 0.00091090
Iteration 4/25 | Loss: 0.00091089
Iteration 5/25 | Loss: 0.00091089
Iteration 6/25 | Loss: 0.00091089
Iteration 7/25 | Loss: 0.00091089
Iteration 8/25 | Loss: 0.00091089
Iteration 9/25 | Loss: 0.00091089
Iteration 10/25 | Loss: 0.00091089
Iteration 11/25 | Loss: 0.00091089
Iteration 12/25 | Loss: 0.00091089
Iteration 13/25 | Loss: 0.00091089
Iteration 14/25 | Loss: 0.00091089
Iteration 15/25 | Loss: 0.00091089
Iteration 16/25 | Loss: 0.00091089
Iteration 17/25 | Loss: 0.00091089
Iteration 18/25 | Loss: 0.00091089
Iteration 19/25 | Loss: 0.00091089
Iteration 20/25 | Loss: 0.00091089
Iteration 21/25 | Loss: 0.00091089
Iteration 22/25 | Loss: 0.00091089
Iteration 23/25 | Loss: 0.00091089
Iteration 24/25 | Loss: 0.00091089
Iteration 25/25 | Loss: 0.00091089

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091089
Iteration 2/1000 | Loss: 0.00002400
Iteration 3/1000 | Loss: 0.00001773
Iteration 4/1000 | Loss: 0.00001605
Iteration 5/1000 | Loss: 0.00001533
Iteration 6/1000 | Loss: 0.00001479
Iteration 7/1000 | Loss: 0.00001428
Iteration 8/1000 | Loss: 0.00001401
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001367
Iteration 11/1000 | Loss: 0.00001357
Iteration 12/1000 | Loss: 0.00001345
Iteration 13/1000 | Loss: 0.00001330
Iteration 14/1000 | Loss: 0.00001316
Iteration 15/1000 | Loss: 0.00001310
Iteration 16/1000 | Loss: 0.00001309
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001304
Iteration 19/1000 | Loss: 0.00001304
Iteration 20/1000 | Loss: 0.00001304
Iteration 21/1000 | Loss: 0.00001299
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001298
Iteration 24/1000 | Loss: 0.00001298
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001295
Iteration 27/1000 | Loss: 0.00001294
Iteration 28/1000 | Loss: 0.00001294
Iteration 29/1000 | Loss: 0.00001293
Iteration 30/1000 | Loss: 0.00001293
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001293
Iteration 33/1000 | Loss: 0.00001293
Iteration 34/1000 | Loss: 0.00001292
Iteration 35/1000 | Loss: 0.00001292
Iteration 36/1000 | Loss: 0.00001291
Iteration 37/1000 | Loss: 0.00001291
Iteration 38/1000 | Loss: 0.00001290
Iteration 39/1000 | Loss: 0.00001290
Iteration 40/1000 | Loss: 0.00001290
Iteration 41/1000 | Loss: 0.00001290
Iteration 42/1000 | Loss: 0.00001290
Iteration 43/1000 | Loss: 0.00001289
Iteration 44/1000 | Loss: 0.00001289
Iteration 45/1000 | Loss: 0.00001289
Iteration 46/1000 | Loss: 0.00001289
Iteration 47/1000 | Loss: 0.00001288
Iteration 48/1000 | Loss: 0.00001288
Iteration 49/1000 | Loss: 0.00001288
Iteration 50/1000 | Loss: 0.00001288
Iteration 51/1000 | Loss: 0.00001288
Iteration 52/1000 | Loss: 0.00001288
Iteration 53/1000 | Loss: 0.00001286
Iteration 54/1000 | Loss: 0.00001286
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001284
Iteration 57/1000 | Loss: 0.00001283
Iteration 58/1000 | Loss: 0.00001283
Iteration 59/1000 | Loss: 0.00001279
Iteration 60/1000 | Loss: 0.00001279
Iteration 61/1000 | Loss: 0.00001278
Iteration 62/1000 | Loss: 0.00001276
Iteration 63/1000 | Loss: 0.00001275
Iteration 64/1000 | Loss: 0.00001275
Iteration 65/1000 | Loss: 0.00001275
Iteration 66/1000 | Loss: 0.00001275
Iteration 67/1000 | Loss: 0.00001275
Iteration 68/1000 | Loss: 0.00001275
Iteration 69/1000 | Loss: 0.00001275
Iteration 70/1000 | Loss: 0.00001275
Iteration 71/1000 | Loss: 0.00001275
Iteration 72/1000 | Loss: 0.00001275
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001274
Iteration 75/1000 | Loss: 0.00001274
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001273
Iteration 79/1000 | Loss: 0.00001272
Iteration 80/1000 | Loss: 0.00001272
Iteration 81/1000 | Loss: 0.00001272
Iteration 82/1000 | Loss: 0.00001272
Iteration 83/1000 | Loss: 0.00001272
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001271
Iteration 86/1000 | Loss: 0.00001271
Iteration 87/1000 | Loss: 0.00001271
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001271
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001271
Iteration 95/1000 | Loss: 0.00001271
Iteration 96/1000 | Loss: 0.00001271
Iteration 97/1000 | Loss: 0.00001270
Iteration 98/1000 | Loss: 0.00001270
Iteration 99/1000 | Loss: 0.00001270
Iteration 100/1000 | Loss: 0.00001270
Iteration 101/1000 | Loss: 0.00001269
Iteration 102/1000 | Loss: 0.00001269
Iteration 103/1000 | Loss: 0.00001269
Iteration 104/1000 | Loss: 0.00001269
Iteration 105/1000 | Loss: 0.00001269
Iteration 106/1000 | Loss: 0.00001269
Iteration 107/1000 | Loss: 0.00001269
Iteration 108/1000 | Loss: 0.00001269
Iteration 109/1000 | Loss: 0.00001269
Iteration 110/1000 | Loss: 0.00001269
Iteration 111/1000 | Loss: 0.00001268
Iteration 112/1000 | Loss: 0.00001268
Iteration 113/1000 | Loss: 0.00001268
Iteration 114/1000 | Loss: 0.00001268
Iteration 115/1000 | Loss: 0.00001268
Iteration 116/1000 | Loss: 0.00001268
Iteration 117/1000 | Loss: 0.00001268
Iteration 118/1000 | Loss: 0.00001268
Iteration 119/1000 | Loss: 0.00001268
Iteration 120/1000 | Loss: 0.00001268
Iteration 121/1000 | Loss: 0.00001268
Iteration 122/1000 | Loss: 0.00001268
Iteration 123/1000 | Loss: 0.00001268
Iteration 124/1000 | Loss: 0.00001268
Iteration 125/1000 | Loss: 0.00001268
Iteration 126/1000 | Loss: 0.00001267
Iteration 127/1000 | Loss: 0.00001267
Iteration 128/1000 | Loss: 0.00001267
Iteration 129/1000 | Loss: 0.00001267
Iteration 130/1000 | Loss: 0.00001267
Iteration 131/1000 | Loss: 0.00001267
Iteration 132/1000 | Loss: 0.00001267
Iteration 133/1000 | Loss: 0.00001267
Iteration 134/1000 | Loss: 0.00001267
Iteration 135/1000 | Loss: 0.00001266
Iteration 136/1000 | Loss: 0.00001266
Iteration 137/1000 | Loss: 0.00001266
Iteration 138/1000 | Loss: 0.00001266
Iteration 139/1000 | Loss: 0.00001266
Iteration 140/1000 | Loss: 0.00001266
Iteration 141/1000 | Loss: 0.00001266
Iteration 142/1000 | Loss: 0.00001266
Iteration 143/1000 | Loss: 0.00001265
Iteration 144/1000 | Loss: 0.00001265
Iteration 145/1000 | Loss: 0.00001265
Iteration 146/1000 | Loss: 0.00001265
Iteration 147/1000 | Loss: 0.00001265
Iteration 148/1000 | Loss: 0.00001265
Iteration 149/1000 | Loss: 0.00001265
Iteration 150/1000 | Loss: 0.00001265
Iteration 151/1000 | Loss: 0.00001265
Iteration 152/1000 | Loss: 0.00001264
Iteration 153/1000 | Loss: 0.00001264
Iteration 154/1000 | Loss: 0.00001264
Iteration 155/1000 | Loss: 0.00001264
Iteration 156/1000 | Loss: 0.00001264
Iteration 157/1000 | Loss: 0.00001264
Iteration 158/1000 | Loss: 0.00001264
Iteration 159/1000 | Loss: 0.00001264
Iteration 160/1000 | Loss: 0.00001264
Iteration 161/1000 | Loss: 0.00001264
Iteration 162/1000 | Loss: 0.00001264
Iteration 163/1000 | Loss: 0.00001264
Iteration 164/1000 | Loss: 0.00001264
Iteration 165/1000 | Loss: 0.00001264
Iteration 166/1000 | Loss: 0.00001264
Iteration 167/1000 | Loss: 0.00001264
Iteration 168/1000 | Loss: 0.00001264
Iteration 169/1000 | Loss: 0.00001264
Iteration 170/1000 | Loss: 0.00001264
Iteration 171/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.2636965948331635e-05, 1.2636965948331635e-05, 1.2636965948331635e-05, 1.2636965948331635e-05, 1.2636965948331635e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2636965948331635e-05

Optimization complete. Final v2v error: 3.0410258769989014 mm

Highest mean error: 3.3551876544952393 mm for frame 60

Lowest mean error: 2.8686258792877197 mm for frame 119

Saving results

Total time: 37.5065336227417
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003449
Iteration 2/25 | Loss: 0.01003449
Iteration 3/25 | Loss: 0.00419118
Iteration 4/25 | Loss: 0.00282313
Iteration 5/25 | Loss: 0.00228081
Iteration 6/25 | Loss: 0.00216022
Iteration 7/25 | Loss: 0.00205122
Iteration 8/25 | Loss: 0.00176659
Iteration 9/25 | Loss: 0.00164554
Iteration 10/25 | Loss: 0.00154129
Iteration 11/25 | Loss: 0.00149294
Iteration 12/25 | Loss: 0.00146764
Iteration 13/25 | Loss: 0.00144756
Iteration 14/25 | Loss: 0.00142966
Iteration 15/25 | Loss: 0.00142883
Iteration 16/25 | Loss: 0.00142651
Iteration 17/25 | Loss: 0.00142255
Iteration 18/25 | Loss: 0.00142213
Iteration 19/25 | Loss: 0.00142148
Iteration 20/25 | Loss: 0.00142113
Iteration 21/25 | Loss: 0.00142088
Iteration 22/25 | Loss: 0.00142053
Iteration 23/25 | Loss: 0.00142025
Iteration 24/25 | Loss: 0.00142018
Iteration 25/25 | Loss: 0.00142018

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41412759
Iteration 2/25 | Loss: 0.00127704
Iteration 3/25 | Loss: 0.00120415
Iteration 4/25 | Loss: 0.00120414
Iteration 5/25 | Loss: 0.00120414
Iteration 6/25 | Loss: 0.00120414
Iteration 7/25 | Loss: 0.00120414
Iteration 8/25 | Loss: 0.00120414
Iteration 9/25 | Loss: 0.00120414
Iteration 10/25 | Loss: 0.00120413
Iteration 11/25 | Loss: 0.00120413
Iteration 12/25 | Loss: 0.00120413
Iteration 13/25 | Loss: 0.00120413
Iteration 14/25 | Loss: 0.00120413
Iteration 15/25 | Loss: 0.00120413
Iteration 16/25 | Loss: 0.00120413
Iteration 17/25 | Loss: 0.00120413
Iteration 18/25 | Loss: 0.00120413
Iteration 19/25 | Loss: 0.00120413
Iteration 20/25 | Loss: 0.00120413
Iteration 21/25 | Loss: 0.00120413
Iteration 22/25 | Loss: 0.00120413
Iteration 23/25 | Loss: 0.00120413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012041323352605104, 0.0012041323352605104, 0.0012041323352605104, 0.0012041323352605104, 0.0012041323352605104]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012041323352605104

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120413
Iteration 2/1000 | Loss: 0.00022381
Iteration 3/1000 | Loss: 0.00008575
Iteration 4/1000 | Loss: 0.00007723
Iteration 5/1000 | Loss: 0.00007223
Iteration 6/1000 | Loss: 0.00006962
Iteration 7/1000 | Loss: 0.00006761
Iteration 8/1000 | Loss: 0.00006602
Iteration 9/1000 | Loss: 0.00006415
Iteration 10/1000 | Loss: 0.00006318
Iteration 11/1000 | Loss: 0.00542954
Iteration 12/1000 | Loss: 0.00496243
Iteration 13/1000 | Loss: 0.00070096
Iteration 14/1000 | Loss: 0.00006862
Iteration 15/1000 | Loss: 0.00161860
Iteration 16/1000 | Loss: 0.00480397
Iteration 17/1000 | Loss: 0.00579926
Iteration 18/1000 | Loss: 0.00018143
Iteration 19/1000 | Loss: 0.00007071
Iteration 20/1000 | Loss: 0.00028198
Iteration 21/1000 | Loss: 0.00111984
Iteration 22/1000 | Loss: 0.00279464
Iteration 23/1000 | Loss: 0.00238920
Iteration 24/1000 | Loss: 0.00065465
Iteration 25/1000 | Loss: 0.00036381
Iteration 26/1000 | Loss: 0.00137424
Iteration 27/1000 | Loss: 0.00178376
Iteration 28/1000 | Loss: 0.00039477
Iteration 29/1000 | Loss: 0.00008334
Iteration 30/1000 | Loss: 0.00006551
Iteration 31/1000 | Loss: 0.00018026
Iteration 32/1000 | Loss: 0.00017668
Iteration 33/1000 | Loss: 0.00018073
Iteration 34/1000 | Loss: 0.00012219
Iteration 35/1000 | Loss: 0.00005544
Iteration 36/1000 | Loss: 0.00006107
Iteration 37/1000 | Loss: 0.00003635
Iteration 38/1000 | Loss: 0.00003315
Iteration 39/1000 | Loss: 0.00003082
Iteration 40/1000 | Loss: 0.00002872
Iteration 41/1000 | Loss: 0.00002709
Iteration 42/1000 | Loss: 0.00002569
Iteration 43/1000 | Loss: 0.00022088
Iteration 44/1000 | Loss: 0.00031602
Iteration 45/1000 | Loss: 0.00022001
Iteration 46/1000 | Loss: 0.00023968
Iteration 47/1000 | Loss: 0.00003772
Iteration 48/1000 | Loss: 0.00002940
Iteration 49/1000 | Loss: 0.00002701
Iteration 50/1000 | Loss: 0.00002489
Iteration 51/1000 | Loss: 0.00002377
Iteration 52/1000 | Loss: 0.00002254
Iteration 53/1000 | Loss: 0.00002129
Iteration 54/1000 | Loss: 0.00002046
Iteration 55/1000 | Loss: 0.00001972
Iteration 56/1000 | Loss: 0.00001939
Iteration 57/1000 | Loss: 0.00001913
Iteration 58/1000 | Loss: 0.00001895
Iteration 59/1000 | Loss: 0.00001876
Iteration 60/1000 | Loss: 0.00001864
Iteration 61/1000 | Loss: 0.00001852
Iteration 62/1000 | Loss: 0.00001848
Iteration 63/1000 | Loss: 0.00001843
Iteration 64/1000 | Loss: 0.00001842
Iteration 65/1000 | Loss: 0.00001842
Iteration 66/1000 | Loss: 0.00001836
Iteration 67/1000 | Loss: 0.00001836
Iteration 68/1000 | Loss: 0.00001836
Iteration 69/1000 | Loss: 0.00001835
Iteration 70/1000 | Loss: 0.00001835
Iteration 71/1000 | Loss: 0.00001835
Iteration 72/1000 | Loss: 0.00001835
Iteration 73/1000 | Loss: 0.00001834
Iteration 74/1000 | Loss: 0.00001834
Iteration 75/1000 | Loss: 0.00001834
Iteration 76/1000 | Loss: 0.00001834
Iteration 77/1000 | Loss: 0.00001833
Iteration 78/1000 | Loss: 0.00001833
Iteration 79/1000 | Loss: 0.00001833
Iteration 80/1000 | Loss: 0.00001833
Iteration 81/1000 | Loss: 0.00001832
Iteration 82/1000 | Loss: 0.00001832
Iteration 83/1000 | Loss: 0.00001831
Iteration 84/1000 | Loss: 0.00001831
Iteration 85/1000 | Loss: 0.00001830
Iteration 86/1000 | Loss: 0.00001830
Iteration 87/1000 | Loss: 0.00001830
Iteration 88/1000 | Loss: 0.00001830
Iteration 89/1000 | Loss: 0.00001830
Iteration 90/1000 | Loss: 0.00001830
Iteration 91/1000 | Loss: 0.00001830
Iteration 92/1000 | Loss: 0.00001829
Iteration 93/1000 | Loss: 0.00001829
Iteration 94/1000 | Loss: 0.00001829
Iteration 95/1000 | Loss: 0.00001829
Iteration 96/1000 | Loss: 0.00001829
Iteration 97/1000 | Loss: 0.00001829
Iteration 98/1000 | Loss: 0.00001829
Iteration 99/1000 | Loss: 0.00001829
Iteration 100/1000 | Loss: 0.00001829
Iteration 101/1000 | Loss: 0.00001829
Iteration 102/1000 | Loss: 0.00001828
Iteration 103/1000 | Loss: 0.00001828
Iteration 104/1000 | Loss: 0.00001828
Iteration 105/1000 | Loss: 0.00001828
Iteration 106/1000 | Loss: 0.00001828
Iteration 107/1000 | Loss: 0.00001828
Iteration 108/1000 | Loss: 0.00001828
Iteration 109/1000 | Loss: 0.00001828
Iteration 110/1000 | Loss: 0.00001828
Iteration 111/1000 | Loss: 0.00001828
Iteration 112/1000 | Loss: 0.00001828
Iteration 113/1000 | Loss: 0.00001828
Iteration 114/1000 | Loss: 0.00001828
Iteration 115/1000 | Loss: 0.00001828
Iteration 116/1000 | Loss: 0.00001828
Iteration 117/1000 | Loss: 0.00001828
Iteration 118/1000 | Loss: 0.00001828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.8281103621120565e-05, 1.8281103621120565e-05, 1.8281103621120565e-05, 1.8281103621120565e-05, 1.8281103621120565e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8281103621120565e-05

Optimization complete. Final v2v error: 3.64281964302063 mm

Highest mean error: 4.627015113830566 mm for frame 45

Lowest mean error: 3.4104135036468506 mm for frame 239

Saving results

Total time: 144.4117612838745
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387617
Iteration 2/25 | Loss: 0.00147800
Iteration 3/25 | Loss: 0.00132293
Iteration 4/25 | Loss: 0.00130038
Iteration 5/25 | Loss: 0.00129499
Iteration 6/25 | Loss: 0.00129485
Iteration 7/25 | Loss: 0.00129485
Iteration 8/25 | Loss: 0.00129485
Iteration 9/25 | Loss: 0.00129485
Iteration 10/25 | Loss: 0.00129485
Iteration 11/25 | Loss: 0.00129485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012948489747941494, 0.0012948489747941494, 0.0012948489747941494, 0.0012948489747941494, 0.0012948489747941494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012948489747941494

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37244678
Iteration 2/25 | Loss: 0.00081486
Iteration 3/25 | Loss: 0.00081486
Iteration 4/25 | Loss: 0.00081485
Iteration 5/25 | Loss: 0.00081485
Iteration 6/25 | Loss: 0.00081485
Iteration 7/25 | Loss: 0.00081485
Iteration 8/25 | Loss: 0.00081485
Iteration 9/25 | Loss: 0.00081485
Iteration 10/25 | Loss: 0.00081485
Iteration 11/25 | Loss: 0.00081485
Iteration 12/25 | Loss: 0.00081485
Iteration 13/25 | Loss: 0.00081485
Iteration 14/25 | Loss: 0.00081485
Iteration 15/25 | Loss: 0.00081485
Iteration 16/25 | Loss: 0.00081485
Iteration 17/25 | Loss: 0.00081485
Iteration 18/25 | Loss: 0.00081485
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008148520719259977, 0.0008148520719259977, 0.0008148520719259977, 0.0008148520719259977, 0.0008148520719259977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008148520719259977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081485
Iteration 2/1000 | Loss: 0.00003911
Iteration 3/1000 | Loss: 0.00002634
Iteration 4/1000 | Loss: 0.00002376
Iteration 5/1000 | Loss: 0.00002251
Iteration 6/1000 | Loss: 0.00002147
Iteration 7/1000 | Loss: 0.00002066
Iteration 8/1000 | Loss: 0.00002010
Iteration 9/1000 | Loss: 0.00001955
Iteration 10/1000 | Loss: 0.00001898
Iteration 11/1000 | Loss: 0.00001874
Iteration 12/1000 | Loss: 0.00001851
Iteration 13/1000 | Loss: 0.00001836
Iteration 14/1000 | Loss: 0.00001834
Iteration 15/1000 | Loss: 0.00001830
Iteration 16/1000 | Loss: 0.00001828
Iteration 17/1000 | Loss: 0.00001827
Iteration 18/1000 | Loss: 0.00001816
Iteration 19/1000 | Loss: 0.00001812
Iteration 20/1000 | Loss: 0.00001812
Iteration 21/1000 | Loss: 0.00001808
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001804
Iteration 25/1000 | Loss: 0.00001803
Iteration 26/1000 | Loss: 0.00001803
Iteration 27/1000 | Loss: 0.00001802
Iteration 28/1000 | Loss: 0.00001801
Iteration 29/1000 | Loss: 0.00001800
Iteration 30/1000 | Loss: 0.00001800
Iteration 31/1000 | Loss: 0.00001799
Iteration 32/1000 | Loss: 0.00001799
Iteration 33/1000 | Loss: 0.00001799
Iteration 34/1000 | Loss: 0.00001799
Iteration 35/1000 | Loss: 0.00001798
Iteration 36/1000 | Loss: 0.00001798
Iteration 37/1000 | Loss: 0.00001798
Iteration 38/1000 | Loss: 0.00001797
Iteration 39/1000 | Loss: 0.00001797
Iteration 40/1000 | Loss: 0.00001796
Iteration 41/1000 | Loss: 0.00001796
Iteration 42/1000 | Loss: 0.00001795
Iteration 43/1000 | Loss: 0.00001795
Iteration 44/1000 | Loss: 0.00001795
Iteration 45/1000 | Loss: 0.00001795
Iteration 46/1000 | Loss: 0.00001795
Iteration 47/1000 | Loss: 0.00001795
Iteration 48/1000 | Loss: 0.00001794
Iteration 49/1000 | Loss: 0.00001794
Iteration 50/1000 | Loss: 0.00001794
Iteration 51/1000 | Loss: 0.00001794
Iteration 52/1000 | Loss: 0.00001794
Iteration 53/1000 | Loss: 0.00001793
Iteration 54/1000 | Loss: 0.00001793
Iteration 55/1000 | Loss: 0.00001792
Iteration 56/1000 | Loss: 0.00001791
Iteration 57/1000 | Loss: 0.00001791
Iteration 58/1000 | Loss: 0.00001791
Iteration 59/1000 | Loss: 0.00001790
Iteration 60/1000 | Loss: 0.00001790
Iteration 61/1000 | Loss: 0.00001789
Iteration 62/1000 | Loss: 0.00001789
Iteration 63/1000 | Loss: 0.00001789
Iteration 64/1000 | Loss: 0.00001788
Iteration 65/1000 | Loss: 0.00001788
Iteration 66/1000 | Loss: 0.00001788
Iteration 67/1000 | Loss: 0.00001788
Iteration 68/1000 | Loss: 0.00001788
Iteration 69/1000 | Loss: 0.00001788
Iteration 70/1000 | Loss: 0.00001788
Iteration 71/1000 | Loss: 0.00001788
Iteration 72/1000 | Loss: 0.00001788
Iteration 73/1000 | Loss: 0.00001788
Iteration 74/1000 | Loss: 0.00001788
Iteration 75/1000 | Loss: 0.00001788
Iteration 76/1000 | Loss: 0.00001788
Iteration 77/1000 | Loss: 0.00001788
Iteration 78/1000 | Loss: 0.00001788
Iteration 79/1000 | Loss: 0.00001788
Iteration 80/1000 | Loss: 0.00001788
Iteration 81/1000 | Loss: 0.00001788
Iteration 82/1000 | Loss: 0.00001788
Iteration 83/1000 | Loss: 0.00001788
Iteration 84/1000 | Loss: 0.00001788
Iteration 85/1000 | Loss: 0.00001788
Iteration 86/1000 | Loss: 0.00001788
Iteration 87/1000 | Loss: 0.00001788
Iteration 88/1000 | Loss: 0.00001788
Iteration 89/1000 | Loss: 0.00001788
Iteration 90/1000 | Loss: 0.00001788
Iteration 91/1000 | Loss: 0.00001788
Iteration 92/1000 | Loss: 0.00001788
Iteration 93/1000 | Loss: 0.00001788
Iteration 94/1000 | Loss: 0.00001788
Iteration 95/1000 | Loss: 0.00001788
Iteration 96/1000 | Loss: 0.00001788
Iteration 97/1000 | Loss: 0.00001788
Iteration 98/1000 | Loss: 0.00001788
Iteration 99/1000 | Loss: 0.00001788
Iteration 100/1000 | Loss: 0.00001788
Iteration 101/1000 | Loss: 0.00001788
Iteration 102/1000 | Loss: 0.00001788
Iteration 103/1000 | Loss: 0.00001788
Iteration 104/1000 | Loss: 0.00001788
Iteration 105/1000 | Loss: 0.00001788
Iteration 106/1000 | Loss: 0.00001788
Iteration 107/1000 | Loss: 0.00001788
Iteration 108/1000 | Loss: 0.00001788
Iteration 109/1000 | Loss: 0.00001788
Iteration 110/1000 | Loss: 0.00001788
Iteration 111/1000 | Loss: 0.00001788
Iteration 112/1000 | Loss: 0.00001788
Iteration 113/1000 | Loss: 0.00001788
Iteration 114/1000 | Loss: 0.00001788
Iteration 115/1000 | Loss: 0.00001788
Iteration 116/1000 | Loss: 0.00001788
Iteration 117/1000 | Loss: 0.00001788
Iteration 118/1000 | Loss: 0.00001788
Iteration 119/1000 | Loss: 0.00001788
Iteration 120/1000 | Loss: 0.00001788
Iteration 121/1000 | Loss: 0.00001788
Iteration 122/1000 | Loss: 0.00001788
Iteration 123/1000 | Loss: 0.00001788
Iteration 124/1000 | Loss: 0.00001788
Iteration 125/1000 | Loss: 0.00001788
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 125. Stopping optimization.
Last 5 losses: [1.7882828615256585e-05, 1.7882828615256585e-05, 1.7882828615256585e-05, 1.7882828615256585e-05, 1.7882828615256585e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7882828615256585e-05

Optimization complete. Final v2v error: 3.5712428092956543 mm

Highest mean error: 4.084011077880859 mm for frame 196

Lowest mean error: 3.1520659923553467 mm for frame 130

Saving results

Total time: 38.770267963409424
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00465703
Iteration 2/25 | Loss: 0.00139705
Iteration 3/25 | Loss: 0.00131242
Iteration 4/25 | Loss: 0.00130161
Iteration 5/25 | Loss: 0.00129910
Iteration 6/25 | Loss: 0.00129870
Iteration 7/25 | Loss: 0.00129870
Iteration 8/25 | Loss: 0.00129870
Iteration 9/25 | Loss: 0.00129870
Iteration 10/25 | Loss: 0.00129870
Iteration 11/25 | Loss: 0.00129870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012986977817490697, 0.0012986977817490697, 0.0012986977817490697, 0.0012986977817490697, 0.0012986977817490697]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012986977817490697

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40045762
Iteration 2/25 | Loss: 0.00097906
Iteration 3/25 | Loss: 0.00097904
Iteration 4/25 | Loss: 0.00097904
Iteration 5/25 | Loss: 0.00097904
Iteration 6/25 | Loss: 0.00097904
Iteration 7/25 | Loss: 0.00097904
Iteration 8/25 | Loss: 0.00097904
Iteration 9/25 | Loss: 0.00097904
Iteration 10/25 | Loss: 0.00097904
Iteration 11/25 | Loss: 0.00097904
Iteration 12/25 | Loss: 0.00097904
Iteration 13/25 | Loss: 0.00097904
Iteration 14/25 | Loss: 0.00097904
Iteration 15/25 | Loss: 0.00097904
Iteration 16/25 | Loss: 0.00097904
Iteration 17/25 | Loss: 0.00097904
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009790376061573625, 0.0009790376061573625, 0.0009790376061573625, 0.0009790376061573625, 0.0009790376061573625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009790376061573625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097904
Iteration 2/1000 | Loss: 0.00003584
Iteration 3/1000 | Loss: 0.00002184
Iteration 4/1000 | Loss: 0.00001705
Iteration 5/1000 | Loss: 0.00001528
Iteration 6/1000 | Loss: 0.00001425
Iteration 7/1000 | Loss: 0.00001368
Iteration 8/1000 | Loss: 0.00001314
Iteration 9/1000 | Loss: 0.00001290
Iteration 10/1000 | Loss: 0.00001284
Iteration 11/1000 | Loss: 0.00001283
Iteration 12/1000 | Loss: 0.00001274
Iteration 13/1000 | Loss: 0.00001272
Iteration 14/1000 | Loss: 0.00001261
Iteration 15/1000 | Loss: 0.00001256
Iteration 16/1000 | Loss: 0.00001256
Iteration 17/1000 | Loss: 0.00001254
Iteration 18/1000 | Loss: 0.00001249
Iteration 19/1000 | Loss: 0.00001249
Iteration 20/1000 | Loss: 0.00001248
Iteration 21/1000 | Loss: 0.00001247
Iteration 22/1000 | Loss: 0.00001245
Iteration 23/1000 | Loss: 0.00001245
Iteration 24/1000 | Loss: 0.00001244
Iteration 25/1000 | Loss: 0.00001244
Iteration 26/1000 | Loss: 0.00001243
Iteration 27/1000 | Loss: 0.00001242
Iteration 28/1000 | Loss: 0.00001241
Iteration 29/1000 | Loss: 0.00001240
Iteration 30/1000 | Loss: 0.00001240
Iteration 31/1000 | Loss: 0.00001238
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001233
Iteration 34/1000 | Loss: 0.00001233
Iteration 35/1000 | Loss: 0.00001233
Iteration 36/1000 | Loss: 0.00001233
Iteration 37/1000 | Loss: 0.00001233
Iteration 38/1000 | Loss: 0.00001233
Iteration 39/1000 | Loss: 0.00001233
Iteration 40/1000 | Loss: 0.00001233
Iteration 41/1000 | Loss: 0.00001232
Iteration 42/1000 | Loss: 0.00001232
Iteration 43/1000 | Loss: 0.00001232
Iteration 44/1000 | Loss: 0.00001232
Iteration 45/1000 | Loss: 0.00001232
Iteration 46/1000 | Loss: 0.00001232
Iteration 47/1000 | Loss: 0.00001231
Iteration 48/1000 | Loss: 0.00001228
Iteration 49/1000 | Loss: 0.00001227
Iteration 50/1000 | Loss: 0.00001227
Iteration 51/1000 | Loss: 0.00001226
Iteration 52/1000 | Loss: 0.00001226
Iteration 53/1000 | Loss: 0.00001225
Iteration 54/1000 | Loss: 0.00001225
Iteration 55/1000 | Loss: 0.00001225
Iteration 56/1000 | Loss: 0.00001224
Iteration 57/1000 | Loss: 0.00001224
Iteration 58/1000 | Loss: 0.00001223
Iteration 59/1000 | Loss: 0.00001223
Iteration 60/1000 | Loss: 0.00001223
Iteration 61/1000 | Loss: 0.00001223
Iteration 62/1000 | Loss: 0.00001223
Iteration 63/1000 | Loss: 0.00001223
Iteration 64/1000 | Loss: 0.00001223
Iteration 65/1000 | Loss: 0.00001222
Iteration 66/1000 | Loss: 0.00001222
Iteration 67/1000 | Loss: 0.00001222
Iteration 68/1000 | Loss: 0.00001221
Iteration 69/1000 | Loss: 0.00001221
Iteration 70/1000 | Loss: 0.00001221
Iteration 71/1000 | Loss: 0.00001220
Iteration 72/1000 | Loss: 0.00001220
Iteration 73/1000 | Loss: 0.00001220
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001219
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001219
Iteration 80/1000 | Loss: 0.00001219
Iteration 81/1000 | Loss: 0.00001219
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001215
Iteration 85/1000 | Loss: 0.00001215
Iteration 86/1000 | Loss: 0.00001215
Iteration 87/1000 | Loss: 0.00001215
Iteration 88/1000 | Loss: 0.00001215
Iteration 89/1000 | Loss: 0.00001213
Iteration 90/1000 | Loss: 0.00001212
Iteration 91/1000 | Loss: 0.00001212
Iteration 92/1000 | Loss: 0.00001212
Iteration 93/1000 | Loss: 0.00001211
Iteration 94/1000 | Loss: 0.00001211
Iteration 95/1000 | Loss: 0.00001211
Iteration 96/1000 | Loss: 0.00001211
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001209
Iteration 100/1000 | Loss: 0.00001209
Iteration 101/1000 | Loss: 0.00001209
Iteration 102/1000 | Loss: 0.00001209
Iteration 103/1000 | Loss: 0.00001209
Iteration 104/1000 | Loss: 0.00001208
Iteration 105/1000 | Loss: 0.00001208
Iteration 106/1000 | Loss: 0.00001208
Iteration 107/1000 | Loss: 0.00001208
Iteration 108/1000 | Loss: 0.00001208
Iteration 109/1000 | Loss: 0.00001207
Iteration 110/1000 | Loss: 0.00001207
Iteration 111/1000 | Loss: 0.00001207
Iteration 112/1000 | Loss: 0.00001207
Iteration 113/1000 | Loss: 0.00001207
Iteration 114/1000 | Loss: 0.00001207
Iteration 115/1000 | Loss: 0.00001206
Iteration 116/1000 | Loss: 0.00001206
Iteration 117/1000 | Loss: 0.00001206
Iteration 118/1000 | Loss: 0.00001205
Iteration 119/1000 | Loss: 0.00001205
Iteration 120/1000 | Loss: 0.00001205
Iteration 121/1000 | Loss: 0.00001204
Iteration 122/1000 | Loss: 0.00001204
Iteration 123/1000 | Loss: 0.00001204
Iteration 124/1000 | Loss: 0.00001203
Iteration 125/1000 | Loss: 0.00001203
Iteration 126/1000 | Loss: 0.00001203
Iteration 127/1000 | Loss: 0.00001203
Iteration 128/1000 | Loss: 0.00001203
Iteration 129/1000 | Loss: 0.00001203
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001202
Iteration 136/1000 | Loss: 0.00001202
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001200
Iteration 140/1000 | Loss: 0.00001200
Iteration 141/1000 | Loss: 0.00001199
Iteration 142/1000 | Loss: 0.00001199
Iteration 143/1000 | Loss: 0.00001199
Iteration 144/1000 | Loss: 0.00001199
Iteration 145/1000 | Loss: 0.00001199
Iteration 146/1000 | Loss: 0.00001199
Iteration 147/1000 | Loss: 0.00001199
Iteration 148/1000 | Loss: 0.00001199
Iteration 149/1000 | Loss: 0.00001199
Iteration 150/1000 | Loss: 0.00001199
Iteration 151/1000 | Loss: 0.00001199
Iteration 152/1000 | Loss: 0.00001199
Iteration 153/1000 | Loss: 0.00001198
Iteration 154/1000 | Loss: 0.00001198
Iteration 155/1000 | Loss: 0.00001198
Iteration 156/1000 | Loss: 0.00001198
Iteration 157/1000 | Loss: 0.00001197
Iteration 158/1000 | Loss: 0.00001197
Iteration 159/1000 | Loss: 0.00001196
Iteration 160/1000 | Loss: 0.00001196
Iteration 161/1000 | Loss: 0.00001196
Iteration 162/1000 | Loss: 0.00001196
Iteration 163/1000 | Loss: 0.00001196
Iteration 164/1000 | Loss: 0.00001196
Iteration 165/1000 | Loss: 0.00001196
Iteration 166/1000 | Loss: 0.00001196
Iteration 167/1000 | Loss: 0.00001195
Iteration 168/1000 | Loss: 0.00001195
Iteration 169/1000 | Loss: 0.00001195
Iteration 170/1000 | Loss: 0.00001195
Iteration 171/1000 | Loss: 0.00001195
Iteration 172/1000 | Loss: 0.00001195
Iteration 173/1000 | Loss: 0.00001195
Iteration 174/1000 | Loss: 0.00001195
Iteration 175/1000 | Loss: 0.00001195
Iteration 176/1000 | Loss: 0.00001195
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001194
Iteration 179/1000 | Loss: 0.00001194
Iteration 180/1000 | Loss: 0.00001194
Iteration 181/1000 | Loss: 0.00001193
Iteration 182/1000 | Loss: 0.00001193
Iteration 183/1000 | Loss: 0.00001193
Iteration 184/1000 | Loss: 0.00001193
Iteration 185/1000 | Loss: 0.00001193
Iteration 186/1000 | Loss: 0.00001193
Iteration 187/1000 | Loss: 0.00001193
Iteration 188/1000 | Loss: 0.00001193
Iteration 189/1000 | Loss: 0.00001193
Iteration 190/1000 | Loss: 0.00001192
Iteration 191/1000 | Loss: 0.00001192
Iteration 192/1000 | Loss: 0.00001192
Iteration 193/1000 | Loss: 0.00001192
Iteration 194/1000 | Loss: 0.00001192
Iteration 195/1000 | Loss: 0.00001191
Iteration 196/1000 | Loss: 0.00001191
Iteration 197/1000 | Loss: 0.00001191
Iteration 198/1000 | Loss: 0.00001191
Iteration 199/1000 | Loss: 0.00001191
Iteration 200/1000 | Loss: 0.00001191
Iteration 201/1000 | Loss: 0.00001191
Iteration 202/1000 | Loss: 0.00001191
Iteration 203/1000 | Loss: 0.00001191
Iteration 204/1000 | Loss: 0.00001191
Iteration 205/1000 | Loss: 0.00001191
Iteration 206/1000 | Loss: 0.00001191
Iteration 207/1000 | Loss: 0.00001191
Iteration 208/1000 | Loss: 0.00001191
Iteration 209/1000 | Loss: 0.00001191
Iteration 210/1000 | Loss: 0.00001191
Iteration 211/1000 | Loss: 0.00001191
Iteration 212/1000 | Loss: 0.00001191
Iteration 213/1000 | Loss: 0.00001191
Iteration 214/1000 | Loss: 0.00001191
Iteration 215/1000 | Loss: 0.00001191
Iteration 216/1000 | Loss: 0.00001191
Iteration 217/1000 | Loss: 0.00001191
Iteration 218/1000 | Loss: 0.00001191
Iteration 219/1000 | Loss: 0.00001191
Iteration 220/1000 | Loss: 0.00001191
Iteration 221/1000 | Loss: 0.00001191
Iteration 222/1000 | Loss: 0.00001191
Iteration 223/1000 | Loss: 0.00001191
Iteration 224/1000 | Loss: 0.00001191
Iteration 225/1000 | Loss: 0.00001191
Iteration 226/1000 | Loss: 0.00001191
Iteration 227/1000 | Loss: 0.00001191
Iteration 228/1000 | Loss: 0.00001191
Iteration 229/1000 | Loss: 0.00001191
Iteration 230/1000 | Loss: 0.00001191
Iteration 231/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [1.1905585779459216e-05, 1.1905585779459216e-05, 1.1905585779459216e-05, 1.1905585779459216e-05, 1.1905585779459216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1905585779459216e-05

Optimization complete. Final v2v error: 2.9138975143432617 mm

Highest mean error: 3.2419655323028564 mm for frame 63

Lowest mean error: 2.790562868118286 mm for frame 97

Saving results

Total time: 38.71413516998291
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812463
Iteration 2/25 | Loss: 0.00226952
Iteration 3/25 | Loss: 0.00166326
Iteration 4/25 | Loss: 0.00159017
Iteration 5/25 | Loss: 0.00158387
Iteration 6/25 | Loss: 0.00158387
Iteration 7/25 | Loss: 0.00158387
Iteration 8/25 | Loss: 0.00158387
Iteration 9/25 | Loss: 0.00158387
Iteration 10/25 | Loss: 0.00158387
Iteration 11/25 | Loss: 0.00158387
Iteration 12/25 | Loss: 0.00158387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015838734107092023, 0.0015838734107092023, 0.0015838734107092023, 0.0015838734107092023, 0.0015838734107092023]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015838734107092023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34168231
Iteration 2/25 | Loss: 0.00133553
Iteration 3/25 | Loss: 0.00133550
Iteration 4/25 | Loss: 0.00133550
Iteration 5/25 | Loss: 0.00133550
Iteration 6/25 | Loss: 0.00133550
Iteration 7/25 | Loss: 0.00133550
Iteration 8/25 | Loss: 0.00133550
Iteration 9/25 | Loss: 0.00133550
Iteration 10/25 | Loss: 0.00133550
Iteration 11/25 | Loss: 0.00133550
Iteration 12/25 | Loss: 0.00133550
Iteration 13/25 | Loss: 0.00133550
Iteration 14/25 | Loss: 0.00133550
Iteration 15/25 | Loss: 0.00133550
Iteration 16/25 | Loss: 0.00133550
Iteration 17/25 | Loss: 0.00133550
Iteration 18/25 | Loss: 0.00133550
Iteration 19/25 | Loss: 0.00133550
Iteration 20/25 | Loss: 0.00133550
Iteration 21/25 | Loss: 0.00133550
Iteration 22/25 | Loss: 0.00133550
Iteration 23/25 | Loss: 0.00133550
Iteration 24/25 | Loss: 0.00133550
Iteration 25/25 | Loss: 0.00133550

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133550
Iteration 2/1000 | Loss: 0.00007339
Iteration 3/1000 | Loss: 0.00004384
Iteration 4/1000 | Loss: 0.00003999
Iteration 5/1000 | Loss: 0.00003810
Iteration 6/1000 | Loss: 0.00003680
Iteration 7/1000 | Loss: 0.00003639
Iteration 8/1000 | Loss: 0.00003600
Iteration 9/1000 | Loss: 0.00003581
Iteration 10/1000 | Loss: 0.00003578
Iteration 11/1000 | Loss: 0.00003576
Iteration 12/1000 | Loss: 0.00003569
Iteration 13/1000 | Loss: 0.00003568
Iteration 14/1000 | Loss: 0.00003559
Iteration 15/1000 | Loss: 0.00003550
Iteration 16/1000 | Loss: 0.00003547
Iteration 17/1000 | Loss: 0.00003544
Iteration 18/1000 | Loss: 0.00003536
Iteration 19/1000 | Loss: 0.00003534
Iteration 20/1000 | Loss: 0.00003533
Iteration 21/1000 | Loss: 0.00003532
Iteration 22/1000 | Loss: 0.00003531
Iteration 23/1000 | Loss: 0.00003530
Iteration 24/1000 | Loss: 0.00003529
Iteration 25/1000 | Loss: 0.00003525
Iteration 26/1000 | Loss: 0.00003525
Iteration 27/1000 | Loss: 0.00003524
Iteration 28/1000 | Loss: 0.00003523
Iteration 29/1000 | Loss: 0.00003523
Iteration 30/1000 | Loss: 0.00003523
Iteration 31/1000 | Loss: 0.00003522
Iteration 32/1000 | Loss: 0.00003522
Iteration 33/1000 | Loss: 0.00003522
Iteration 34/1000 | Loss: 0.00003522
Iteration 35/1000 | Loss: 0.00003522
Iteration 36/1000 | Loss: 0.00003522
Iteration 37/1000 | Loss: 0.00003522
Iteration 38/1000 | Loss: 0.00003522
Iteration 39/1000 | Loss: 0.00003522
Iteration 40/1000 | Loss: 0.00003522
Iteration 41/1000 | Loss: 0.00003521
Iteration 42/1000 | Loss: 0.00003521
Iteration 43/1000 | Loss: 0.00003521
Iteration 44/1000 | Loss: 0.00003521
Iteration 45/1000 | Loss: 0.00003521
Iteration 46/1000 | Loss: 0.00003520
Iteration 47/1000 | Loss: 0.00003520
Iteration 48/1000 | Loss: 0.00003520
Iteration 49/1000 | Loss: 0.00003520
Iteration 50/1000 | Loss: 0.00003520
Iteration 51/1000 | Loss: 0.00003519
Iteration 52/1000 | Loss: 0.00003519
Iteration 53/1000 | Loss: 0.00003519
Iteration 54/1000 | Loss: 0.00003519
Iteration 55/1000 | Loss: 0.00003519
Iteration 56/1000 | Loss: 0.00003519
Iteration 57/1000 | Loss: 0.00003519
Iteration 58/1000 | Loss: 0.00003519
Iteration 59/1000 | Loss: 0.00003519
Iteration 60/1000 | Loss: 0.00003519
Iteration 61/1000 | Loss: 0.00003519
Iteration 62/1000 | Loss: 0.00003519
Iteration 63/1000 | Loss: 0.00003519
Iteration 64/1000 | Loss: 0.00003519
Iteration 65/1000 | Loss: 0.00003519
Iteration 66/1000 | Loss: 0.00003519
Iteration 67/1000 | Loss: 0.00003519
Iteration 68/1000 | Loss: 0.00003519
Iteration 69/1000 | Loss: 0.00003519
Iteration 70/1000 | Loss: 0.00003519
Iteration 71/1000 | Loss: 0.00003519
Iteration 72/1000 | Loss: 0.00003519
Iteration 73/1000 | Loss: 0.00003519
Iteration 74/1000 | Loss: 0.00003519
Iteration 75/1000 | Loss: 0.00003519
Iteration 76/1000 | Loss: 0.00003519
Iteration 77/1000 | Loss: 0.00003519
Iteration 78/1000 | Loss: 0.00003519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [3.5191362258046865e-05, 3.5191362258046865e-05, 3.5191362258046865e-05, 3.5191362258046865e-05, 3.5191362258046865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.5191362258046865e-05

Optimization complete. Final v2v error: 5.058559417724609 mm

Highest mean error: 5.319977760314941 mm for frame 87

Lowest mean error: 4.900320053100586 mm for frame 32

Saving results

Total time: 32.59501504898071
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777253
Iteration 2/25 | Loss: 0.00160907
Iteration 3/25 | Loss: 0.00160711
Iteration 4/25 | Loss: 0.00140284
Iteration 5/25 | Loss: 0.00138229
Iteration 6/25 | Loss: 0.00137294
Iteration 7/25 | Loss: 0.00136464
Iteration 8/25 | Loss: 0.00136185
Iteration 9/25 | Loss: 0.00136129
Iteration 10/25 | Loss: 0.00136121
Iteration 11/25 | Loss: 0.00136121
Iteration 12/25 | Loss: 0.00136120
Iteration 13/25 | Loss: 0.00136120
Iteration 14/25 | Loss: 0.00136120
Iteration 15/25 | Loss: 0.00136120
Iteration 16/25 | Loss: 0.00136120
Iteration 17/25 | Loss: 0.00136120
Iteration 18/25 | Loss: 0.00136120
Iteration 19/25 | Loss: 0.00136120
Iteration 20/25 | Loss: 0.00136120
Iteration 21/25 | Loss: 0.00136119
Iteration 22/25 | Loss: 0.00136119
Iteration 23/25 | Loss: 0.00136119
Iteration 24/25 | Loss: 0.00136119
Iteration 25/25 | Loss: 0.00136119

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52965546
Iteration 2/25 | Loss: 0.00094659
Iteration 3/25 | Loss: 0.00094657
Iteration 4/25 | Loss: 0.00094657
Iteration 5/25 | Loss: 0.00094657
Iteration 6/25 | Loss: 0.00094657
Iteration 7/25 | Loss: 0.00094657
Iteration 8/25 | Loss: 0.00094657
Iteration 9/25 | Loss: 0.00094657
Iteration 10/25 | Loss: 0.00094657
Iteration 11/25 | Loss: 0.00094657
Iteration 12/25 | Loss: 0.00094657
Iteration 13/25 | Loss: 0.00094657
Iteration 14/25 | Loss: 0.00094657
Iteration 15/25 | Loss: 0.00094657
Iteration 16/25 | Loss: 0.00094657
Iteration 17/25 | Loss: 0.00094657
Iteration 18/25 | Loss: 0.00094657
Iteration 19/25 | Loss: 0.00094657
Iteration 20/25 | Loss: 0.00094657
Iteration 21/25 | Loss: 0.00094657
Iteration 22/25 | Loss: 0.00094657
Iteration 23/25 | Loss: 0.00094657
Iteration 24/25 | Loss: 0.00094657
Iteration 25/25 | Loss: 0.00094657

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094657
Iteration 2/1000 | Loss: 0.00005111
Iteration 3/1000 | Loss: 0.00003325
Iteration 4/1000 | Loss: 0.00002809
Iteration 5/1000 | Loss: 0.00002597
Iteration 6/1000 | Loss: 0.00002440
Iteration 7/1000 | Loss: 0.00002327
Iteration 8/1000 | Loss: 0.00002245
Iteration 9/1000 | Loss: 0.00002202
Iteration 10/1000 | Loss: 0.00002168
Iteration 11/1000 | Loss: 0.00002131
Iteration 12/1000 | Loss: 0.00002108
Iteration 13/1000 | Loss: 0.00002086
Iteration 14/1000 | Loss: 0.00002069
Iteration 15/1000 | Loss: 0.00002067
Iteration 16/1000 | Loss: 0.00002066
Iteration 17/1000 | Loss: 0.00002065
Iteration 18/1000 | Loss: 0.00002061
Iteration 19/1000 | Loss: 0.00002056
Iteration 20/1000 | Loss: 0.00002056
Iteration 21/1000 | Loss: 0.00002054
Iteration 22/1000 | Loss: 0.00002054
Iteration 23/1000 | Loss: 0.00002053
Iteration 24/1000 | Loss: 0.00002053
Iteration 25/1000 | Loss: 0.00002049
Iteration 26/1000 | Loss: 0.00002047
Iteration 27/1000 | Loss: 0.00002046
Iteration 28/1000 | Loss: 0.00002042
Iteration 29/1000 | Loss: 0.00002042
Iteration 30/1000 | Loss: 0.00002040
Iteration 31/1000 | Loss: 0.00002040
Iteration 32/1000 | Loss: 0.00002039
Iteration 33/1000 | Loss: 0.00002039
Iteration 34/1000 | Loss: 0.00002039
Iteration 35/1000 | Loss: 0.00002037
Iteration 36/1000 | Loss: 0.00002037
Iteration 37/1000 | Loss: 0.00002036
Iteration 38/1000 | Loss: 0.00002036
Iteration 39/1000 | Loss: 0.00002036
Iteration 40/1000 | Loss: 0.00002035
Iteration 41/1000 | Loss: 0.00002035
Iteration 42/1000 | Loss: 0.00002034
Iteration 43/1000 | Loss: 0.00002034
Iteration 44/1000 | Loss: 0.00002034
Iteration 45/1000 | Loss: 0.00002034
Iteration 46/1000 | Loss: 0.00002034
Iteration 47/1000 | Loss: 0.00002034
Iteration 48/1000 | Loss: 0.00002034
Iteration 49/1000 | Loss: 0.00002034
Iteration 50/1000 | Loss: 0.00002033
Iteration 51/1000 | Loss: 0.00002033
Iteration 52/1000 | Loss: 0.00002033
Iteration 53/1000 | Loss: 0.00002033
Iteration 54/1000 | Loss: 0.00002033
Iteration 55/1000 | Loss: 0.00002033
Iteration 56/1000 | Loss: 0.00002032
Iteration 57/1000 | Loss: 0.00002032
Iteration 58/1000 | Loss: 0.00002031
Iteration 59/1000 | Loss: 0.00002031
Iteration 60/1000 | Loss: 0.00002031
Iteration 61/1000 | Loss: 0.00002031
Iteration 62/1000 | Loss: 0.00002031
Iteration 63/1000 | Loss: 0.00002031
Iteration 64/1000 | Loss: 0.00002030
Iteration 65/1000 | Loss: 0.00002030
Iteration 66/1000 | Loss: 0.00002030
Iteration 67/1000 | Loss: 0.00002030
Iteration 68/1000 | Loss: 0.00002030
Iteration 69/1000 | Loss: 0.00002030
Iteration 70/1000 | Loss: 0.00002030
Iteration 71/1000 | Loss: 0.00002029
Iteration 72/1000 | Loss: 0.00002029
Iteration 73/1000 | Loss: 0.00002028
Iteration 74/1000 | Loss: 0.00002028
Iteration 75/1000 | Loss: 0.00002028
Iteration 76/1000 | Loss: 0.00002027
Iteration 77/1000 | Loss: 0.00002027
Iteration 78/1000 | Loss: 0.00002027
Iteration 79/1000 | Loss: 0.00002027
Iteration 80/1000 | Loss: 0.00002026
Iteration 81/1000 | Loss: 0.00002026
Iteration 82/1000 | Loss: 0.00002026
Iteration 83/1000 | Loss: 0.00002026
Iteration 84/1000 | Loss: 0.00002026
Iteration 85/1000 | Loss: 0.00002026
Iteration 86/1000 | Loss: 0.00002025
Iteration 87/1000 | Loss: 0.00002025
Iteration 88/1000 | Loss: 0.00002025
Iteration 89/1000 | Loss: 0.00002025
Iteration 90/1000 | Loss: 0.00002025
Iteration 91/1000 | Loss: 0.00002025
Iteration 92/1000 | Loss: 0.00002025
Iteration 93/1000 | Loss: 0.00002025
Iteration 94/1000 | Loss: 0.00002025
Iteration 95/1000 | Loss: 0.00002024
Iteration 96/1000 | Loss: 0.00002024
Iteration 97/1000 | Loss: 0.00002024
Iteration 98/1000 | Loss: 0.00002024
Iteration 99/1000 | Loss: 0.00002024
Iteration 100/1000 | Loss: 0.00002023
Iteration 101/1000 | Loss: 0.00002023
Iteration 102/1000 | Loss: 0.00002023
Iteration 103/1000 | Loss: 0.00002023
Iteration 104/1000 | Loss: 0.00002022
Iteration 105/1000 | Loss: 0.00002022
Iteration 106/1000 | Loss: 0.00002022
Iteration 107/1000 | Loss: 0.00002022
Iteration 108/1000 | Loss: 0.00002021
Iteration 109/1000 | Loss: 0.00002021
Iteration 110/1000 | Loss: 0.00002021
Iteration 111/1000 | Loss: 0.00002021
Iteration 112/1000 | Loss: 0.00002021
Iteration 113/1000 | Loss: 0.00002021
Iteration 114/1000 | Loss: 0.00002020
Iteration 115/1000 | Loss: 0.00002020
Iteration 116/1000 | Loss: 0.00002020
Iteration 117/1000 | Loss: 0.00002020
Iteration 118/1000 | Loss: 0.00002020
Iteration 119/1000 | Loss: 0.00002020
Iteration 120/1000 | Loss: 0.00002020
Iteration 121/1000 | Loss: 0.00002019
Iteration 122/1000 | Loss: 0.00002019
Iteration 123/1000 | Loss: 0.00002019
Iteration 124/1000 | Loss: 0.00002019
Iteration 125/1000 | Loss: 0.00002018
Iteration 126/1000 | Loss: 0.00002018
Iteration 127/1000 | Loss: 0.00002018
Iteration 128/1000 | Loss: 0.00002018
Iteration 129/1000 | Loss: 0.00002018
Iteration 130/1000 | Loss: 0.00002018
Iteration 131/1000 | Loss: 0.00002017
Iteration 132/1000 | Loss: 0.00002017
Iteration 133/1000 | Loss: 0.00002017
Iteration 134/1000 | Loss: 0.00002017
Iteration 135/1000 | Loss: 0.00002017
Iteration 136/1000 | Loss: 0.00002017
Iteration 137/1000 | Loss: 0.00002016
Iteration 138/1000 | Loss: 0.00002016
Iteration 139/1000 | Loss: 0.00002016
Iteration 140/1000 | Loss: 0.00002016
Iteration 141/1000 | Loss: 0.00002016
Iteration 142/1000 | Loss: 0.00002016
Iteration 143/1000 | Loss: 0.00002016
Iteration 144/1000 | Loss: 0.00002016
Iteration 145/1000 | Loss: 0.00002016
Iteration 146/1000 | Loss: 0.00002016
Iteration 147/1000 | Loss: 0.00002016
Iteration 148/1000 | Loss: 0.00002016
Iteration 149/1000 | Loss: 0.00002015
Iteration 150/1000 | Loss: 0.00002015
Iteration 151/1000 | Loss: 0.00002015
Iteration 152/1000 | Loss: 0.00002015
Iteration 153/1000 | Loss: 0.00002014
Iteration 154/1000 | Loss: 0.00002014
Iteration 155/1000 | Loss: 0.00002014
Iteration 156/1000 | Loss: 0.00002014
Iteration 157/1000 | Loss: 0.00002014
Iteration 158/1000 | Loss: 0.00002014
Iteration 159/1000 | Loss: 0.00002014
Iteration 160/1000 | Loss: 0.00002014
Iteration 161/1000 | Loss: 0.00002014
Iteration 162/1000 | Loss: 0.00002013
Iteration 163/1000 | Loss: 0.00002013
Iteration 164/1000 | Loss: 0.00002013
Iteration 165/1000 | Loss: 0.00002013
Iteration 166/1000 | Loss: 0.00002013
Iteration 167/1000 | Loss: 0.00002013
Iteration 168/1000 | Loss: 0.00002012
Iteration 169/1000 | Loss: 0.00002012
Iteration 170/1000 | Loss: 0.00002012
Iteration 171/1000 | Loss: 0.00002012
Iteration 172/1000 | Loss: 0.00002012
Iteration 173/1000 | Loss: 0.00002012
Iteration 174/1000 | Loss: 0.00002012
Iteration 175/1000 | Loss: 0.00002011
Iteration 176/1000 | Loss: 0.00002011
Iteration 177/1000 | Loss: 0.00002011
Iteration 178/1000 | Loss: 0.00002011
Iteration 179/1000 | Loss: 0.00002011
Iteration 180/1000 | Loss: 0.00002011
Iteration 181/1000 | Loss: 0.00002011
Iteration 182/1000 | Loss: 0.00002011
Iteration 183/1000 | Loss: 0.00002011
Iteration 184/1000 | Loss: 0.00002011
Iteration 185/1000 | Loss: 0.00002011
Iteration 186/1000 | Loss: 0.00002011
Iteration 187/1000 | Loss: 0.00002010
Iteration 188/1000 | Loss: 0.00002010
Iteration 189/1000 | Loss: 0.00002010
Iteration 190/1000 | Loss: 0.00002010
Iteration 191/1000 | Loss: 0.00002010
Iteration 192/1000 | Loss: 0.00002010
Iteration 193/1000 | Loss: 0.00002010
Iteration 194/1000 | Loss: 0.00002010
Iteration 195/1000 | Loss: 0.00002009
Iteration 196/1000 | Loss: 0.00002009
Iteration 197/1000 | Loss: 0.00002009
Iteration 198/1000 | Loss: 0.00002009
Iteration 199/1000 | Loss: 0.00002009
Iteration 200/1000 | Loss: 0.00002009
Iteration 201/1000 | Loss: 0.00002008
Iteration 202/1000 | Loss: 0.00002008
Iteration 203/1000 | Loss: 0.00002008
Iteration 204/1000 | Loss: 0.00002008
Iteration 205/1000 | Loss: 0.00002008
Iteration 206/1000 | Loss: 0.00002008
Iteration 207/1000 | Loss: 0.00002008
Iteration 208/1000 | Loss: 0.00002008
Iteration 209/1000 | Loss: 0.00002008
Iteration 210/1000 | Loss: 0.00002008
Iteration 211/1000 | Loss: 0.00002008
Iteration 212/1000 | Loss: 0.00002008
Iteration 213/1000 | Loss: 0.00002008
Iteration 214/1000 | Loss: 0.00002008
Iteration 215/1000 | Loss: 0.00002008
Iteration 216/1000 | Loss: 0.00002008
Iteration 217/1000 | Loss: 0.00002008
Iteration 218/1000 | Loss: 0.00002008
Iteration 219/1000 | Loss: 0.00002008
Iteration 220/1000 | Loss: 0.00002007
Iteration 221/1000 | Loss: 0.00002007
Iteration 222/1000 | Loss: 0.00002007
Iteration 223/1000 | Loss: 0.00002007
Iteration 224/1000 | Loss: 0.00002007
Iteration 225/1000 | Loss: 0.00002007
Iteration 226/1000 | Loss: 0.00002007
Iteration 227/1000 | Loss: 0.00002007
Iteration 228/1000 | Loss: 0.00002007
Iteration 229/1000 | Loss: 0.00002007
Iteration 230/1000 | Loss: 0.00002007
Iteration 231/1000 | Loss: 0.00002007
Iteration 232/1000 | Loss: 0.00002006
Iteration 233/1000 | Loss: 0.00002006
Iteration 234/1000 | Loss: 0.00002006
Iteration 235/1000 | Loss: 0.00002006
Iteration 236/1000 | Loss: 0.00002006
Iteration 237/1000 | Loss: 0.00002006
Iteration 238/1000 | Loss: 0.00002006
Iteration 239/1000 | Loss: 0.00002006
Iteration 240/1000 | Loss: 0.00002006
Iteration 241/1000 | Loss: 0.00002006
Iteration 242/1000 | Loss: 0.00002006
Iteration 243/1000 | Loss: 0.00002006
Iteration 244/1000 | Loss: 0.00002006
Iteration 245/1000 | Loss: 0.00002006
Iteration 246/1000 | Loss: 0.00002006
Iteration 247/1000 | Loss: 0.00002006
Iteration 248/1000 | Loss: 0.00002006
Iteration 249/1000 | Loss: 0.00002006
Iteration 250/1000 | Loss: 0.00002006
Iteration 251/1000 | Loss: 0.00002006
Iteration 252/1000 | Loss: 0.00002006
Iteration 253/1000 | Loss: 0.00002006
Iteration 254/1000 | Loss: 0.00002006
Iteration 255/1000 | Loss: 0.00002006
Iteration 256/1000 | Loss: 0.00002006
Iteration 257/1000 | Loss: 0.00002006
Iteration 258/1000 | Loss: 0.00002006
Iteration 259/1000 | Loss: 0.00002005
Iteration 260/1000 | Loss: 0.00002005
Iteration 261/1000 | Loss: 0.00002005
Iteration 262/1000 | Loss: 0.00002005
Iteration 263/1000 | Loss: 0.00002005
Iteration 264/1000 | Loss: 0.00002005
Iteration 265/1000 | Loss: 0.00002005
Iteration 266/1000 | Loss: 0.00002005
Iteration 267/1000 | Loss: 0.00002005
Iteration 268/1000 | Loss: 0.00002005
Iteration 269/1000 | Loss: 0.00002005
Iteration 270/1000 | Loss: 0.00002005
Iteration 271/1000 | Loss: 0.00002005
Iteration 272/1000 | Loss: 0.00002005
Iteration 273/1000 | Loss: 0.00002005
Iteration 274/1000 | Loss: 0.00002005
Iteration 275/1000 | Loss: 0.00002005
Iteration 276/1000 | Loss: 0.00002005
Iteration 277/1000 | Loss: 0.00002005
Iteration 278/1000 | Loss: 0.00002005
Iteration 279/1000 | Loss: 0.00002005
Iteration 280/1000 | Loss: 0.00002005
Iteration 281/1000 | Loss: 0.00002005
Iteration 282/1000 | Loss: 0.00002005
Iteration 283/1000 | Loss: 0.00002004
Iteration 284/1000 | Loss: 0.00002004
Iteration 285/1000 | Loss: 0.00002004
Iteration 286/1000 | Loss: 0.00002004
Iteration 287/1000 | Loss: 0.00002004
Iteration 288/1000 | Loss: 0.00002004
Iteration 289/1000 | Loss: 0.00002004
Iteration 290/1000 | Loss: 0.00002004
Iteration 291/1000 | Loss: 0.00002004
Iteration 292/1000 | Loss: 0.00002004
Iteration 293/1000 | Loss: 0.00002004
Iteration 294/1000 | Loss: 0.00002004
Iteration 295/1000 | Loss: 0.00002004
Iteration 296/1000 | Loss: 0.00002004
Iteration 297/1000 | Loss: 0.00002004
Iteration 298/1000 | Loss: 0.00002004
Iteration 299/1000 | Loss: 0.00002004
Iteration 300/1000 | Loss: 0.00002004
Iteration 301/1000 | Loss: 0.00002004
Iteration 302/1000 | Loss: 0.00002004
Iteration 303/1000 | Loss: 0.00002004
Iteration 304/1000 | Loss: 0.00002004
Iteration 305/1000 | Loss: 0.00002004
Iteration 306/1000 | Loss: 0.00002004
Iteration 307/1000 | Loss: 0.00002003
Iteration 308/1000 | Loss: 0.00002003
Iteration 309/1000 | Loss: 0.00002003
Iteration 310/1000 | Loss: 0.00002003
Iteration 311/1000 | Loss: 0.00002003
Iteration 312/1000 | Loss: 0.00002003
Iteration 313/1000 | Loss: 0.00002003
Iteration 314/1000 | Loss: 0.00002003
Iteration 315/1000 | Loss: 0.00002003
Iteration 316/1000 | Loss: 0.00002003
Iteration 317/1000 | Loss: 0.00002003
Iteration 318/1000 | Loss: 0.00002003
Iteration 319/1000 | Loss: 0.00002003
Iteration 320/1000 | Loss: 0.00002003
Iteration 321/1000 | Loss: 0.00002003
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 321. Stopping optimization.
Last 5 losses: [2.0031908206874505e-05, 2.0031908206874505e-05, 2.0031908206874505e-05, 2.0031908206874505e-05, 2.0031908206874505e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0031908206874505e-05

Optimization complete. Final v2v error: 3.6898746490478516 mm

Highest mean error: 4.510439395904541 mm for frame 51

Lowest mean error: 2.9739339351654053 mm for frame 168

Saving results

Total time: 66.36824083328247
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814695
Iteration 2/25 | Loss: 0.00138325
Iteration 3/25 | Loss: 0.00130581
Iteration 4/25 | Loss: 0.00129301
Iteration 5/25 | Loss: 0.00128905
Iteration 6/25 | Loss: 0.00128884
Iteration 7/25 | Loss: 0.00128884
Iteration 8/25 | Loss: 0.00128884
Iteration 9/25 | Loss: 0.00128884
Iteration 10/25 | Loss: 0.00128884
Iteration 11/25 | Loss: 0.00128884
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012888398487120867, 0.0012888398487120867, 0.0012888398487120867, 0.0012888398487120867, 0.0012888398487120867]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012888398487120867

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.30633473
Iteration 2/25 | Loss: 0.00089229
Iteration 3/25 | Loss: 0.00089228
Iteration 4/25 | Loss: 0.00089228
Iteration 5/25 | Loss: 0.00089228
Iteration 6/25 | Loss: 0.00089228
Iteration 7/25 | Loss: 0.00089228
Iteration 8/25 | Loss: 0.00089228
Iteration 9/25 | Loss: 0.00089228
Iteration 10/25 | Loss: 0.00089228
Iteration 11/25 | Loss: 0.00089228
Iteration 12/25 | Loss: 0.00089228
Iteration 13/25 | Loss: 0.00089228
Iteration 14/25 | Loss: 0.00089228
Iteration 15/25 | Loss: 0.00089228
Iteration 16/25 | Loss: 0.00089228
Iteration 17/25 | Loss: 0.00089228
Iteration 18/25 | Loss: 0.00089228
Iteration 19/25 | Loss: 0.00089228
Iteration 20/25 | Loss: 0.00089228
Iteration 21/25 | Loss: 0.00089228
Iteration 22/25 | Loss: 0.00089228
Iteration 23/25 | Loss: 0.00089228
Iteration 24/25 | Loss: 0.00089228
Iteration 25/25 | Loss: 0.00089228

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089228
Iteration 2/1000 | Loss: 0.00002786
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001796
Iteration 5/1000 | Loss: 0.00001698
Iteration 6/1000 | Loss: 0.00001631
Iteration 7/1000 | Loss: 0.00001576
Iteration 8/1000 | Loss: 0.00001548
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001479
Iteration 12/1000 | Loss: 0.00001474
Iteration 13/1000 | Loss: 0.00001470
Iteration 14/1000 | Loss: 0.00001455
Iteration 15/1000 | Loss: 0.00001455
Iteration 16/1000 | Loss: 0.00001448
Iteration 17/1000 | Loss: 0.00001447
Iteration 18/1000 | Loss: 0.00001445
Iteration 19/1000 | Loss: 0.00001444
Iteration 20/1000 | Loss: 0.00001444
Iteration 21/1000 | Loss: 0.00001442
Iteration 22/1000 | Loss: 0.00001441
Iteration 23/1000 | Loss: 0.00001437
Iteration 24/1000 | Loss: 0.00001436
Iteration 25/1000 | Loss: 0.00001436
Iteration 26/1000 | Loss: 0.00001435
Iteration 27/1000 | Loss: 0.00001434
Iteration 28/1000 | Loss: 0.00001433
Iteration 29/1000 | Loss: 0.00001433
Iteration 30/1000 | Loss: 0.00001431
Iteration 31/1000 | Loss: 0.00001431
Iteration 32/1000 | Loss: 0.00001431
Iteration 33/1000 | Loss: 0.00001431
Iteration 34/1000 | Loss: 0.00001430
Iteration 35/1000 | Loss: 0.00001426
Iteration 36/1000 | Loss: 0.00001424
Iteration 37/1000 | Loss: 0.00001421
Iteration 38/1000 | Loss: 0.00001420
Iteration 39/1000 | Loss: 0.00001415
Iteration 40/1000 | Loss: 0.00001413
Iteration 41/1000 | Loss: 0.00001413
Iteration 42/1000 | Loss: 0.00001412
Iteration 43/1000 | Loss: 0.00001412
Iteration 44/1000 | Loss: 0.00001411
Iteration 45/1000 | Loss: 0.00001409
Iteration 46/1000 | Loss: 0.00001408
Iteration 47/1000 | Loss: 0.00001408
Iteration 48/1000 | Loss: 0.00001407
Iteration 49/1000 | Loss: 0.00001407
Iteration 50/1000 | Loss: 0.00001406
Iteration 51/1000 | Loss: 0.00001406
Iteration 52/1000 | Loss: 0.00001405
Iteration 53/1000 | Loss: 0.00001404
Iteration 54/1000 | Loss: 0.00001404
Iteration 55/1000 | Loss: 0.00001404
Iteration 56/1000 | Loss: 0.00001403
Iteration 57/1000 | Loss: 0.00001403
Iteration 58/1000 | Loss: 0.00001403
Iteration 59/1000 | Loss: 0.00001402
Iteration 60/1000 | Loss: 0.00001400
Iteration 61/1000 | Loss: 0.00001400
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001398
Iteration 64/1000 | Loss: 0.00001398
Iteration 65/1000 | Loss: 0.00001397
Iteration 66/1000 | Loss: 0.00001397
Iteration 67/1000 | Loss: 0.00001397
Iteration 68/1000 | Loss: 0.00001396
Iteration 69/1000 | Loss: 0.00001396
Iteration 70/1000 | Loss: 0.00001396
Iteration 71/1000 | Loss: 0.00001395
Iteration 72/1000 | Loss: 0.00001395
Iteration 73/1000 | Loss: 0.00001395
Iteration 74/1000 | Loss: 0.00001395
Iteration 75/1000 | Loss: 0.00001395
Iteration 76/1000 | Loss: 0.00001395
Iteration 77/1000 | Loss: 0.00001395
Iteration 78/1000 | Loss: 0.00001394
Iteration 79/1000 | Loss: 0.00001394
Iteration 80/1000 | Loss: 0.00001394
Iteration 81/1000 | Loss: 0.00001394
Iteration 82/1000 | Loss: 0.00001394
Iteration 83/1000 | Loss: 0.00001394
Iteration 84/1000 | Loss: 0.00001394
Iteration 85/1000 | Loss: 0.00001394
Iteration 86/1000 | Loss: 0.00001394
Iteration 87/1000 | Loss: 0.00001393
Iteration 88/1000 | Loss: 0.00001393
Iteration 89/1000 | Loss: 0.00001393
Iteration 90/1000 | Loss: 0.00001393
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001392
Iteration 98/1000 | Loss: 0.00001392
Iteration 99/1000 | Loss: 0.00001392
Iteration 100/1000 | Loss: 0.00001392
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001392
Iteration 110/1000 | Loss: 0.00001391
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001390
Iteration 125/1000 | Loss: 0.00001390
Iteration 126/1000 | Loss: 0.00001390
Iteration 127/1000 | Loss: 0.00001390
Iteration 128/1000 | Loss: 0.00001390
Iteration 129/1000 | Loss: 0.00001390
Iteration 130/1000 | Loss: 0.00001390
Iteration 131/1000 | Loss: 0.00001390
Iteration 132/1000 | Loss: 0.00001390
Iteration 133/1000 | Loss: 0.00001390
Iteration 134/1000 | Loss: 0.00001390
Iteration 135/1000 | Loss: 0.00001390
Iteration 136/1000 | Loss: 0.00001390
Iteration 137/1000 | Loss: 0.00001390
Iteration 138/1000 | Loss: 0.00001390
Iteration 139/1000 | Loss: 0.00001390
Iteration 140/1000 | Loss: 0.00001390
Iteration 141/1000 | Loss: 0.00001390
Iteration 142/1000 | Loss: 0.00001390
Iteration 143/1000 | Loss: 0.00001390
Iteration 144/1000 | Loss: 0.00001390
Iteration 145/1000 | Loss: 0.00001390
Iteration 146/1000 | Loss: 0.00001389
Iteration 147/1000 | Loss: 0.00001389
Iteration 148/1000 | Loss: 0.00001389
Iteration 149/1000 | Loss: 0.00001389
Iteration 150/1000 | Loss: 0.00001389
Iteration 151/1000 | Loss: 0.00001389
Iteration 152/1000 | Loss: 0.00001389
Iteration 153/1000 | Loss: 0.00001389
Iteration 154/1000 | Loss: 0.00001389
Iteration 155/1000 | Loss: 0.00001389
Iteration 156/1000 | Loss: 0.00001389
Iteration 157/1000 | Loss: 0.00001389
Iteration 158/1000 | Loss: 0.00001389
Iteration 159/1000 | Loss: 0.00001389
Iteration 160/1000 | Loss: 0.00001389
Iteration 161/1000 | Loss: 0.00001389
Iteration 162/1000 | Loss: 0.00001389
Iteration 163/1000 | Loss: 0.00001389
Iteration 164/1000 | Loss: 0.00001389
Iteration 165/1000 | Loss: 0.00001389
Iteration 166/1000 | Loss: 0.00001389
Iteration 167/1000 | Loss: 0.00001389
Iteration 168/1000 | Loss: 0.00001389
Iteration 169/1000 | Loss: 0.00001389
Iteration 170/1000 | Loss: 0.00001389
Iteration 171/1000 | Loss: 0.00001389
Iteration 172/1000 | Loss: 0.00001389
Iteration 173/1000 | Loss: 0.00001389
Iteration 174/1000 | Loss: 0.00001389
Iteration 175/1000 | Loss: 0.00001389
Iteration 176/1000 | Loss: 0.00001389
Iteration 177/1000 | Loss: 0.00001389
Iteration 178/1000 | Loss: 0.00001389
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 178. Stopping optimization.
Last 5 losses: [1.3890229638491292e-05, 1.3890229638491292e-05, 1.3890229638491292e-05, 1.3890229638491292e-05, 1.3890229638491292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3890229638491292e-05

Optimization complete. Final v2v error: 3.1244161128997803 mm

Highest mean error: 4.200055122375488 mm for frame 56

Lowest mean error: 2.791494131088257 mm for frame 108

Saving results

Total time: 39.02648639678955
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00802722
Iteration 2/25 | Loss: 0.00145951
Iteration 3/25 | Loss: 0.00131526
Iteration 4/25 | Loss: 0.00130387
Iteration 5/25 | Loss: 0.00130214
Iteration 6/25 | Loss: 0.00130214
Iteration 7/25 | Loss: 0.00130214
Iteration 8/25 | Loss: 0.00130214
Iteration 9/25 | Loss: 0.00130214
Iteration 10/25 | Loss: 0.00130214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013021378545090556, 0.0013021378545090556, 0.0013021378545090556, 0.0013021378545090556, 0.0013021378545090556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013021378545090556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40174270
Iteration 2/25 | Loss: 0.00088161
Iteration 3/25 | Loss: 0.00088161
Iteration 4/25 | Loss: 0.00088161
Iteration 5/25 | Loss: 0.00088161
Iteration 6/25 | Loss: 0.00088161
Iteration 7/25 | Loss: 0.00088161
Iteration 8/25 | Loss: 0.00088161
Iteration 9/25 | Loss: 0.00088161
Iteration 10/25 | Loss: 0.00088161
Iteration 11/25 | Loss: 0.00088161
Iteration 12/25 | Loss: 0.00088161
Iteration 13/25 | Loss: 0.00088161
Iteration 14/25 | Loss: 0.00088161
Iteration 15/25 | Loss: 0.00088161
Iteration 16/25 | Loss: 0.00088161
Iteration 17/25 | Loss: 0.00088161
Iteration 18/25 | Loss: 0.00088161
Iteration 19/25 | Loss: 0.00088161
Iteration 20/25 | Loss: 0.00088161
Iteration 21/25 | Loss: 0.00088161
Iteration 22/25 | Loss: 0.00088161
Iteration 23/25 | Loss: 0.00088161
Iteration 24/25 | Loss: 0.00088161
Iteration 25/25 | Loss: 0.00088161

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088161
Iteration 2/1000 | Loss: 0.00002890
Iteration 3/1000 | Loss: 0.00001911
Iteration 4/1000 | Loss: 0.00001652
Iteration 5/1000 | Loss: 0.00001550
Iteration 6/1000 | Loss: 0.00001475
Iteration 7/1000 | Loss: 0.00001424
Iteration 8/1000 | Loss: 0.00001395
Iteration 9/1000 | Loss: 0.00001354
Iteration 10/1000 | Loss: 0.00001342
Iteration 11/1000 | Loss: 0.00001324
Iteration 12/1000 | Loss: 0.00001315
Iteration 13/1000 | Loss: 0.00001303
Iteration 14/1000 | Loss: 0.00001302
Iteration 15/1000 | Loss: 0.00001302
Iteration 16/1000 | Loss: 0.00001297
Iteration 17/1000 | Loss: 0.00001294
Iteration 18/1000 | Loss: 0.00001294
Iteration 19/1000 | Loss: 0.00001294
Iteration 20/1000 | Loss: 0.00001293
Iteration 21/1000 | Loss: 0.00001293
Iteration 22/1000 | Loss: 0.00001292
Iteration 23/1000 | Loss: 0.00001291
Iteration 24/1000 | Loss: 0.00001291
Iteration 25/1000 | Loss: 0.00001290
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001289
Iteration 28/1000 | Loss: 0.00001289
Iteration 29/1000 | Loss: 0.00001288
Iteration 30/1000 | Loss: 0.00001287
Iteration 31/1000 | Loss: 0.00001286
Iteration 32/1000 | Loss: 0.00001285
Iteration 33/1000 | Loss: 0.00001284
Iteration 34/1000 | Loss: 0.00001282
Iteration 35/1000 | Loss: 0.00001282
Iteration 36/1000 | Loss: 0.00001281
Iteration 37/1000 | Loss: 0.00001281
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001278
Iteration 40/1000 | Loss: 0.00001276
Iteration 41/1000 | Loss: 0.00001276
Iteration 42/1000 | Loss: 0.00001275
Iteration 43/1000 | Loss: 0.00001275
Iteration 44/1000 | Loss: 0.00001274
Iteration 45/1000 | Loss: 0.00001273
Iteration 46/1000 | Loss: 0.00001273
Iteration 47/1000 | Loss: 0.00001273
Iteration 48/1000 | Loss: 0.00001272
Iteration 49/1000 | Loss: 0.00001272
Iteration 50/1000 | Loss: 0.00001272
Iteration 51/1000 | Loss: 0.00001272
Iteration 52/1000 | Loss: 0.00001272
Iteration 53/1000 | Loss: 0.00001271
Iteration 54/1000 | Loss: 0.00001271
Iteration 55/1000 | Loss: 0.00001271
Iteration 56/1000 | Loss: 0.00001271
Iteration 57/1000 | Loss: 0.00001271
Iteration 58/1000 | Loss: 0.00001271
Iteration 59/1000 | Loss: 0.00001271
Iteration 60/1000 | Loss: 0.00001271
Iteration 61/1000 | Loss: 0.00001271
Iteration 62/1000 | Loss: 0.00001271
Iteration 63/1000 | Loss: 0.00001270
Iteration 64/1000 | Loss: 0.00001270
Iteration 65/1000 | Loss: 0.00001270
Iteration 66/1000 | Loss: 0.00001270
Iteration 67/1000 | Loss: 0.00001270
Iteration 68/1000 | Loss: 0.00001270
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001270
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001270
Iteration 73/1000 | Loss: 0.00001270
Iteration 74/1000 | Loss: 0.00001270
Iteration 75/1000 | Loss: 0.00001269
Iteration 76/1000 | Loss: 0.00001269
Iteration 77/1000 | Loss: 0.00001269
Iteration 78/1000 | Loss: 0.00001268
Iteration 79/1000 | Loss: 0.00001268
Iteration 80/1000 | Loss: 0.00001268
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001267
Iteration 83/1000 | Loss: 0.00001267
Iteration 84/1000 | Loss: 0.00001267
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001266
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001265
Iteration 89/1000 | Loss: 0.00001265
Iteration 90/1000 | Loss: 0.00001264
Iteration 91/1000 | Loss: 0.00001264
Iteration 92/1000 | Loss: 0.00001264
Iteration 93/1000 | Loss: 0.00001263
Iteration 94/1000 | Loss: 0.00001263
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001262
Iteration 97/1000 | Loss: 0.00001262
Iteration 98/1000 | Loss: 0.00001261
Iteration 99/1000 | Loss: 0.00001261
Iteration 100/1000 | Loss: 0.00001260
Iteration 101/1000 | Loss: 0.00001260
Iteration 102/1000 | Loss: 0.00001260
Iteration 103/1000 | Loss: 0.00001260
Iteration 104/1000 | Loss: 0.00001260
Iteration 105/1000 | Loss: 0.00001259
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001259
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001259
Iteration 113/1000 | Loss: 0.00001259
Iteration 114/1000 | Loss: 0.00001259
Iteration 115/1000 | Loss: 0.00001259
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001258
Iteration 119/1000 | Loss: 0.00001258
Iteration 120/1000 | Loss: 0.00001258
Iteration 121/1000 | Loss: 0.00001258
Iteration 122/1000 | Loss: 0.00001258
Iteration 123/1000 | Loss: 0.00001258
Iteration 124/1000 | Loss: 0.00001258
Iteration 125/1000 | Loss: 0.00001258
Iteration 126/1000 | Loss: 0.00001258
Iteration 127/1000 | Loss: 0.00001258
Iteration 128/1000 | Loss: 0.00001257
Iteration 129/1000 | Loss: 0.00001257
Iteration 130/1000 | Loss: 0.00001257
Iteration 131/1000 | Loss: 0.00001256
Iteration 132/1000 | Loss: 0.00001256
Iteration 133/1000 | Loss: 0.00001256
Iteration 134/1000 | Loss: 0.00001255
Iteration 135/1000 | Loss: 0.00001255
Iteration 136/1000 | Loss: 0.00001255
Iteration 137/1000 | Loss: 0.00001255
Iteration 138/1000 | Loss: 0.00001255
Iteration 139/1000 | Loss: 0.00001255
Iteration 140/1000 | Loss: 0.00001255
Iteration 141/1000 | Loss: 0.00001255
Iteration 142/1000 | Loss: 0.00001255
Iteration 143/1000 | Loss: 0.00001255
Iteration 144/1000 | Loss: 0.00001255
Iteration 145/1000 | Loss: 0.00001255
Iteration 146/1000 | Loss: 0.00001255
Iteration 147/1000 | Loss: 0.00001255
Iteration 148/1000 | Loss: 0.00001255
Iteration 149/1000 | Loss: 0.00001255
Iteration 150/1000 | Loss: 0.00001255
Iteration 151/1000 | Loss: 0.00001255
Iteration 152/1000 | Loss: 0.00001255
Iteration 153/1000 | Loss: 0.00001255
Iteration 154/1000 | Loss: 0.00001255
Iteration 155/1000 | Loss: 0.00001255
Iteration 156/1000 | Loss: 0.00001255
Iteration 157/1000 | Loss: 0.00001255
Iteration 158/1000 | Loss: 0.00001255
Iteration 159/1000 | Loss: 0.00001255
Iteration 160/1000 | Loss: 0.00001255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [1.2547499864012934e-05, 1.2547499864012934e-05, 1.2547499864012934e-05, 1.2547499864012934e-05, 1.2547499864012934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2547499864012934e-05

Optimization complete. Final v2v error: 3.0321033000946045 mm

Highest mean error: 3.184915065765381 mm for frame 150

Lowest mean error: 2.918401002883911 mm for frame 77

Saving results

Total time: 40.24964928627014
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00392994
Iteration 2/25 | Loss: 0.00132789
Iteration 3/25 | Loss: 0.00126264
Iteration 4/25 | Loss: 0.00125225
Iteration 5/25 | Loss: 0.00124938
Iteration 6/25 | Loss: 0.00124933
Iteration 7/25 | Loss: 0.00124933
Iteration 8/25 | Loss: 0.00124933
Iteration 9/25 | Loss: 0.00124933
Iteration 10/25 | Loss: 0.00124933
Iteration 11/25 | Loss: 0.00124933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012493260437622666, 0.0012493260437622666, 0.0012493260437622666, 0.0012493260437622666, 0.0012493260437622666]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012493260437622666

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48603559
Iteration 2/25 | Loss: 0.00086495
Iteration 3/25 | Loss: 0.00086495
Iteration 4/25 | Loss: 0.00086494
Iteration 5/25 | Loss: 0.00086494
Iteration 6/25 | Loss: 0.00086494
Iteration 7/25 | Loss: 0.00086494
Iteration 8/25 | Loss: 0.00086494
Iteration 9/25 | Loss: 0.00086494
Iteration 10/25 | Loss: 0.00086494
Iteration 11/25 | Loss: 0.00086494
Iteration 12/25 | Loss: 0.00086494
Iteration 13/25 | Loss: 0.00086494
Iteration 14/25 | Loss: 0.00086494
Iteration 15/25 | Loss: 0.00086494
Iteration 16/25 | Loss: 0.00086494
Iteration 17/25 | Loss: 0.00086494
Iteration 18/25 | Loss: 0.00086494
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0008649424416944385, 0.0008649424416944385, 0.0008649424416944385, 0.0008649424416944385, 0.0008649424416944385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008649424416944385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00086494
Iteration 2/1000 | Loss: 0.00001900
Iteration 3/1000 | Loss: 0.00001423
Iteration 4/1000 | Loss: 0.00001341
Iteration 5/1000 | Loss: 0.00001273
Iteration 6/1000 | Loss: 0.00001225
Iteration 7/1000 | Loss: 0.00001192
Iteration 8/1000 | Loss: 0.00001164
Iteration 9/1000 | Loss: 0.00001142
Iteration 10/1000 | Loss: 0.00001120
Iteration 11/1000 | Loss: 0.00001119
Iteration 12/1000 | Loss: 0.00001117
Iteration 13/1000 | Loss: 0.00001117
Iteration 14/1000 | Loss: 0.00001116
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001115
Iteration 17/1000 | Loss: 0.00001115
Iteration 18/1000 | Loss: 0.00001111
Iteration 19/1000 | Loss: 0.00001111
Iteration 20/1000 | Loss: 0.00001111
Iteration 21/1000 | Loss: 0.00001110
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001106
Iteration 24/1000 | Loss: 0.00001097
Iteration 25/1000 | Loss: 0.00001096
Iteration 26/1000 | Loss: 0.00001095
Iteration 27/1000 | Loss: 0.00001088
Iteration 28/1000 | Loss: 0.00001082
Iteration 29/1000 | Loss: 0.00001082
Iteration 30/1000 | Loss: 0.00001082
Iteration 31/1000 | Loss: 0.00001081
Iteration 32/1000 | Loss: 0.00001081
Iteration 33/1000 | Loss: 0.00001080
Iteration 34/1000 | Loss: 0.00001080
Iteration 35/1000 | Loss: 0.00001079
Iteration 36/1000 | Loss: 0.00001077
Iteration 37/1000 | Loss: 0.00001076
Iteration 38/1000 | Loss: 0.00001076
Iteration 39/1000 | Loss: 0.00001076
Iteration 40/1000 | Loss: 0.00001076
Iteration 41/1000 | Loss: 0.00001075
Iteration 42/1000 | Loss: 0.00001075
Iteration 43/1000 | Loss: 0.00001074
Iteration 44/1000 | Loss: 0.00001074
Iteration 45/1000 | Loss: 0.00001073
Iteration 46/1000 | Loss: 0.00001072
Iteration 47/1000 | Loss: 0.00001072
Iteration 48/1000 | Loss: 0.00001071
Iteration 49/1000 | Loss: 0.00001071
Iteration 50/1000 | Loss: 0.00001068
Iteration 51/1000 | Loss: 0.00001068
Iteration 52/1000 | Loss: 0.00001068
Iteration 53/1000 | Loss: 0.00001068
Iteration 54/1000 | Loss: 0.00001066
Iteration 55/1000 | Loss: 0.00001062
Iteration 56/1000 | Loss: 0.00001058
Iteration 57/1000 | Loss: 0.00001058
Iteration 58/1000 | Loss: 0.00001057
Iteration 59/1000 | Loss: 0.00001057
Iteration 60/1000 | Loss: 0.00001057
Iteration 61/1000 | Loss: 0.00001057
Iteration 62/1000 | Loss: 0.00001057
Iteration 63/1000 | Loss: 0.00001056
Iteration 64/1000 | Loss: 0.00001056
Iteration 65/1000 | Loss: 0.00001055
Iteration 66/1000 | Loss: 0.00001055
Iteration 67/1000 | Loss: 0.00001055
Iteration 68/1000 | Loss: 0.00001055
Iteration 69/1000 | Loss: 0.00001055
Iteration 70/1000 | Loss: 0.00001054
Iteration 71/1000 | Loss: 0.00001054
Iteration 72/1000 | Loss: 0.00001054
Iteration 73/1000 | Loss: 0.00001054
Iteration 74/1000 | Loss: 0.00001054
Iteration 75/1000 | Loss: 0.00001053
Iteration 76/1000 | Loss: 0.00001052
Iteration 77/1000 | Loss: 0.00001051
Iteration 78/1000 | Loss: 0.00001050
Iteration 79/1000 | Loss: 0.00001049
Iteration 80/1000 | Loss: 0.00001049
Iteration 81/1000 | Loss: 0.00001049
Iteration 82/1000 | Loss: 0.00001049
Iteration 83/1000 | Loss: 0.00001049
Iteration 84/1000 | Loss: 0.00001049
Iteration 85/1000 | Loss: 0.00001048
Iteration 86/1000 | Loss: 0.00001048
Iteration 87/1000 | Loss: 0.00001047
Iteration 88/1000 | Loss: 0.00001047
Iteration 89/1000 | Loss: 0.00001046
Iteration 90/1000 | Loss: 0.00001046
Iteration 91/1000 | Loss: 0.00001046
Iteration 92/1000 | Loss: 0.00001046
Iteration 93/1000 | Loss: 0.00001046
Iteration 94/1000 | Loss: 0.00001046
Iteration 95/1000 | Loss: 0.00001046
Iteration 96/1000 | Loss: 0.00001045
Iteration 97/1000 | Loss: 0.00001045
Iteration 98/1000 | Loss: 0.00001045
Iteration 99/1000 | Loss: 0.00001044
Iteration 100/1000 | Loss: 0.00001044
Iteration 101/1000 | Loss: 0.00001044
Iteration 102/1000 | Loss: 0.00001044
Iteration 103/1000 | Loss: 0.00001044
Iteration 104/1000 | Loss: 0.00001044
Iteration 105/1000 | Loss: 0.00001044
Iteration 106/1000 | Loss: 0.00001043
Iteration 107/1000 | Loss: 0.00001043
Iteration 108/1000 | Loss: 0.00001043
Iteration 109/1000 | Loss: 0.00001043
Iteration 110/1000 | Loss: 0.00001043
Iteration 111/1000 | Loss: 0.00001043
Iteration 112/1000 | Loss: 0.00001043
Iteration 113/1000 | Loss: 0.00001043
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.0434784599056002e-05, 1.0434784599056002e-05, 1.0434784599056002e-05, 1.0434784599056002e-05, 1.0434784599056002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0434784599056002e-05

Optimization complete. Final v2v error: 2.798848867416382 mm

Highest mean error: 2.8995518684387207 mm for frame 17

Lowest mean error: 2.7448573112487793 mm for frame 58

Saving results

Total time: 35.24257946014404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00699073
Iteration 2/25 | Loss: 0.00142094
Iteration 3/25 | Loss: 0.00132624
Iteration 4/25 | Loss: 0.00130084
Iteration 5/25 | Loss: 0.00129119
Iteration 6/25 | Loss: 0.00128867
Iteration 7/25 | Loss: 0.00128633
Iteration 8/25 | Loss: 0.00128606
Iteration 9/25 | Loss: 0.00128585
Iteration 10/25 | Loss: 0.00128565
Iteration 11/25 | Loss: 0.00128574
Iteration 12/25 | Loss: 0.00128540
Iteration 13/25 | Loss: 0.00128487
Iteration 14/25 | Loss: 0.00128476
Iteration 15/25 | Loss: 0.00128476
Iteration 16/25 | Loss: 0.00128476
Iteration 17/25 | Loss: 0.00128475
Iteration 18/25 | Loss: 0.00128475
Iteration 19/25 | Loss: 0.00128475
Iteration 20/25 | Loss: 0.00128475
Iteration 21/25 | Loss: 0.00128475
Iteration 22/25 | Loss: 0.00128475
Iteration 23/25 | Loss: 0.00128475
Iteration 24/25 | Loss: 0.00128475
Iteration 25/25 | Loss: 0.00128475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91616225
Iteration 2/25 | Loss: 0.00093101
Iteration 3/25 | Loss: 0.00093100
Iteration 4/25 | Loss: 0.00093100
Iteration 5/25 | Loss: 0.00093100
Iteration 6/25 | Loss: 0.00093100
Iteration 7/25 | Loss: 0.00093100
Iteration 8/25 | Loss: 0.00093100
Iteration 9/25 | Loss: 0.00093100
Iteration 10/25 | Loss: 0.00093100
Iteration 11/25 | Loss: 0.00093100
Iteration 12/25 | Loss: 0.00093100
Iteration 13/25 | Loss: 0.00093100
Iteration 14/25 | Loss: 0.00093100
Iteration 15/25 | Loss: 0.00093100
Iteration 16/25 | Loss: 0.00093100
Iteration 17/25 | Loss: 0.00093100
Iteration 18/25 | Loss: 0.00093100
Iteration 19/25 | Loss: 0.00093100
Iteration 20/25 | Loss: 0.00093100
Iteration 21/25 | Loss: 0.00093100
Iteration 22/25 | Loss: 0.00093100
Iteration 23/25 | Loss: 0.00093100
Iteration 24/25 | Loss: 0.00093100
Iteration 25/25 | Loss: 0.00093100

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093100
Iteration 2/1000 | Loss: 0.00002234
Iteration 3/1000 | Loss: 0.00001785
Iteration 4/1000 | Loss: 0.00007216
Iteration 5/1000 | Loss: 0.00002270
Iteration 6/1000 | Loss: 0.00001737
Iteration 7/1000 | Loss: 0.00001609
Iteration 8/1000 | Loss: 0.00001568
Iteration 9/1000 | Loss: 0.00001523
Iteration 10/1000 | Loss: 0.00006015
Iteration 11/1000 | Loss: 0.00001473
Iteration 12/1000 | Loss: 0.00006087
Iteration 13/1000 | Loss: 0.00004904
Iteration 14/1000 | Loss: 0.00001453
Iteration 15/1000 | Loss: 0.00001424
Iteration 16/1000 | Loss: 0.00001412
Iteration 17/1000 | Loss: 0.00001409
Iteration 18/1000 | Loss: 0.00001407
Iteration 19/1000 | Loss: 0.00001406
Iteration 20/1000 | Loss: 0.00001405
Iteration 21/1000 | Loss: 0.00001401
Iteration 22/1000 | Loss: 0.00001387
Iteration 23/1000 | Loss: 0.00001381
Iteration 24/1000 | Loss: 0.00001371
Iteration 25/1000 | Loss: 0.00001370
Iteration 26/1000 | Loss: 0.00001369
Iteration 27/1000 | Loss: 0.00001369
Iteration 28/1000 | Loss: 0.00001364
Iteration 29/1000 | Loss: 0.00001362
Iteration 30/1000 | Loss: 0.00001361
Iteration 31/1000 | Loss: 0.00001360
Iteration 32/1000 | Loss: 0.00001356
Iteration 33/1000 | Loss: 0.00001355
Iteration 34/1000 | Loss: 0.00001355
Iteration 35/1000 | Loss: 0.00001354
Iteration 36/1000 | Loss: 0.00001352
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00001352
Iteration 39/1000 | Loss: 0.00001351
Iteration 40/1000 | Loss: 0.00001351
Iteration 41/1000 | Loss: 0.00001351
Iteration 42/1000 | Loss: 0.00001351
Iteration 43/1000 | Loss: 0.00001347
Iteration 44/1000 | Loss: 0.00001347
Iteration 45/1000 | Loss: 0.00001347
Iteration 46/1000 | Loss: 0.00001347
Iteration 47/1000 | Loss: 0.00001347
Iteration 48/1000 | Loss: 0.00001347
Iteration 49/1000 | Loss: 0.00001346
Iteration 50/1000 | Loss: 0.00001346
Iteration 51/1000 | Loss: 0.00001346
Iteration 52/1000 | Loss: 0.00001346
Iteration 53/1000 | Loss: 0.00001345
Iteration 54/1000 | Loss: 0.00001343
Iteration 55/1000 | Loss: 0.00001343
Iteration 56/1000 | Loss: 0.00001342
Iteration 57/1000 | Loss: 0.00001341
Iteration 58/1000 | Loss: 0.00006741
Iteration 59/1000 | Loss: 0.00001653
Iteration 60/1000 | Loss: 0.00001521
Iteration 61/1000 | Loss: 0.00001346
Iteration 62/1000 | Loss: 0.00001337
Iteration 63/1000 | Loss: 0.00001334
Iteration 64/1000 | Loss: 0.00001333
Iteration 65/1000 | Loss: 0.00001333
Iteration 66/1000 | Loss: 0.00001333
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001332
Iteration 70/1000 | Loss: 0.00001332
Iteration 71/1000 | Loss: 0.00001332
Iteration 72/1000 | Loss: 0.00001332
Iteration 73/1000 | Loss: 0.00001332
Iteration 74/1000 | Loss: 0.00001332
Iteration 75/1000 | Loss: 0.00001332
Iteration 76/1000 | Loss: 0.00001332
Iteration 77/1000 | Loss: 0.00001332
Iteration 78/1000 | Loss: 0.00001332
Iteration 79/1000 | Loss: 0.00001332
Iteration 80/1000 | Loss: 0.00001331
Iteration 81/1000 | Loss: 0.00001331
Iteration 82/1000 | Loss: 0.00001331
Iteration 83/1000 | Loss: 0.00001331
Iteration 84/1000 | Loss: 0.00001331
Iteration 85/1000 | Loss: 0.00001331
Iteration 86/1000 | Loss: 0.00001331
Iteration 87/1000 | Loss: 0.00001331
Iteration 88/1000 | Loss: 0.00001331
Iteration 89/1000 | Loss: 0.00001331
Iteration 90/1000 | Loss: 0.00001331
Iteration 91/1000 | Loss: 0.00001331
Iteration 92/1000 | Loss: 0.00001331
Iteration 93/1000 | Loss: 0.00001331
Iteration 94/1000 | Loss: 0.00001331
Iteration 95/1000 | Loss: 0.00001331
Iteration 96/1000 | Loss: 0.00001331
Iteration 97/1000 | Loss: 0.00001331
Iteration 98/1000 | Loss: 0.00001331
Iteration 99/1000 | Loss: 0.00001331
Iteration 100/1000 | Loss: 0.00001330
Iteration 101/1000 | Loss: 0.00001330
Iteration 102/1000 | Loss: 0.00001330
Iteration 103/1000 | Loss: 0.00001330
Iteration 104/1000 | Loss: 0.00001330
Iteration 105/1000 | Loss: 0.00001330
Iteration 106/1000 | Loss: 0.00001330
Iteration 107/1000 | Loss: 0.00001330
Iteration 108/1000 | Loss: 0.00001330
Iteration 109/1000 | Loss: 0.00001330
Iteration 110/1000 | Loss: 0.00001330
Iteration 111/1000 | Loss: 0.00001330
Iteration 112/1000 | Loss: 0.00001330
Iteration 113/1000 | Loss: 0.00001330
Iteration 114/1000 | Loss: 0.00001330
Iteration 115/1000 | Loss: 0.00001330
Iteration 116/1000 | Loss: 0.00001330
Iteration 117/1000 | Loss: 0.00001330
Iteration 118/1000 | Loss: 0.00001330
Iteration 119/1000 | Loss: 0.00001330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.3296902579895686e-05, 1.3296902579895686e-05, 1.3296902579895686e-05, 1.3296902579895686e-05, 1.3296902579895686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3296902579895686e-05

Optimization complete. Final v2v error: 3.1221237182617188 mm

Highest mean error: 3.6043567657470703 mm for frame 136

Lowest mean error: 2.9192469120025635 mm for frame 262

Saving results

Total time: 70.28213047981262
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00773303
Iteration 2/25 | Loss: 0.00160750
Iteration 3/25 | Loss: 0.00145353
Iteration 4/25 | Loss: 0.00142489
Iteration 5/25 | Loss: 0.00141180
Iteration 6/25 | Loss: 0.00140137
Iteration 7/25 | Loss: 0.00139597
Iteration 8/25 | Loss: 0.00139293
Iteration 9/25 | Loss: 0.00138742
Iteration 10/25 | Loss: 0.00138652
Iteration 11/25 | Loss: 0.00138388
Iteration 12/25 | Loss: 0.00138283
Iteration 13/25 | Loss: 0.00138237
Iteration 14/25 | Loss: 0.00138202
Iteration 15/25 | Loss: 0.00138184
Iteration 16/25 | Loss: 0.00138163
Iteration 17/25 | Loss: 0.00138318
Iteration 18/25 | Loss: 0.00138089
Iteration 19/25 | Loss: 0.00138217
Iteration 20/25 | Loss: 0.00137981
Iteration 21/25 | Loss: 0.00137935
Iteration 22/25 | Loss: 0.00137919
Iteration 23/25 | Loss: 0.00137919
Iteration 24/25 | Loss: 0.00137918
Iteration 25/25 | Loss: 0.00137918

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38992786
Iteration 2/25 | Loss: 0.00143233
Iteration 3/25 | Loss: 0.00143232
Iteration 4/25 | Loss: 0.00143232
Iteration 5/25 | Loss: 0.00143231
Iteration 6/25 | Loss: 0.00143231
Iteration 7/25 | Loss: 0.00143231
Iteration 8/25 | Loss: 0.00143231
Iteration 9/25 | Loss: 0.00143231
Iteration 10/25 | Loss: 0.00143231
Iteration 11/25 | Loss: 0.00143231
Iteration 12/25 | Loss: 0.00143231
Iteration 13/25 | Loss: 0.00143231
Iteration 14/25 | Loss: 0.00143231
Iteration 15/25 | Loss: 0.00143231
Iteration 16/25 | Loss: 0.00143231
Iteration 17/25 | Loss: 0.00143231
Iteration 18/25 | Loss: 0.00143231
Iteration 19/25 | Loss: 0.00143231
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014323117211461067, 0.0014323117211461067, 0.0014323117211461067, 0.0014323117211461067, 0.0014323117211461067]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014323117211461067

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143231
Iteration 2/1000 | Loss: 0.00011616
Iteration 3/1000 | Loss: 0.00007383
Iteration 4/1000 | Loss: 0.00006335
Iteration 5/1000 | Loss: 0.00005681
Iteration 6/1000 | Loss: 0.00005334
Iteration 7/1000 | Loss: 0.00005035
Iteration 8/1000 | Loss: 0.00026020
Iteration 9/1000 | Loss: 0.00027724
Iteration 10/1000 | Loss: 0.00018563
Iteration 11/1000 | Loss: 0.00004770
Iteration 12/1000 | Loss: 0.00018572
Iteration 13/1000 | Loss: 0.00032188
Iteration 14/1000 | Loss: 0.00011136
Iteration 15/1000 | Loss: 0.00005161
Iteration 16/1000 | Loss: 0.00008233
Iteration 17/1000 | Loss: 0.00004389
Iteration 18/1000 | Loss: 0.00004174
Iteration 19/1000 | Loss: 0.00004063
Iteration 20/1000 | Loss: 0.00003933
Iteration 21/1000 | Loss: 0.00003836
Iteration 22/1000 | Loss: 0.00003740
Iteration 23/1000 | Loss: 0.00003672
Iteration 24/1000 | Loss: 0.00003618
Iteration 25/1000 | Loss: 0.00003577
Iteration 26/1000 | Loss: 0.00003544
Iteration 27/1000 | Loss: 0.00003513
Iteration 28/1000 | Loss: 0.00003490
Iteration 29/1000 | Loss: 0.00003471
Iteration 30/1000 | Loss: 0.00003450
Iteration 31/1000 | Loss: 0.00032630
Iteration 32/1000 | Loss: 0.00004203
Iteration 33/1000 | Loss: 0.00003758
Iteration 34/1000 | Loss: 0.00003633
Iteration 35/1000 | Loss: 0.00003491
Iteration 36/1000 | Loss: 0.00003367
Iteration 37/1000 | Loss: 0.00003302
Iteration 38/1000 | Loss: 0.00003263
Iteration 39/1000 | Loss: 0.00023120
Iteration 40/1000 | Loss: 0.00011107
Iteration 41/1000 | Loss: 0.00003327
Iteration 42/1000 | Loss: 0.00003249
Iteration 43/1000 | Loss: 0.00003235
Iteration 44/1000 | Loss: 0.00023008
Iteration 45/1000 | Loss: 0.00013451
Iteration 46/1000 | Loss: 0.00022576
Iteration 47/1000 | Loss: 0.00013945
Iteration 48/1000 | Loss: 0.00014774
Iteration 49/1000 | Loss: 0.00016479
Iteration 50/1000 | Loss: 0.00027560
Iteration 51/1000 | Loss: 0.00016582
Iteration 52/1000 | Loss: 0.00017495
Iteration 53/1000 | Loss: 0.00003594
Iteration 54/1000 | Loss: 0.00020953
Iteration 55/1000 | Loss: 0.00011814
Iteration 56/1000 | Loss: 0.00013844
Iteration 57/1000 | Loss: 0.00017582
Iteration 58/1000 | Loss: 0.00018533
Iteration 59/1000 | Loss: 0.00016823
Iteration 60/1000 | Loss: 0.00015515
Iteration 61/1000 | Loss: 0.00023145
Iteration 62/1000 | Loss: 0.00014463
Iteration 63/1000 | Loss: 0.00020817
Iteration 64/1000 | Loss: 0.00015453
Iteration 65/1000 | Loss: 0.00003387
Iteration 66/1000 | Loss: 0.00003278
Iteration 67/1000 | Loss: 0.00003265
Iteration 68/1000 | Loss: 0.00003251
Iteration 69/1000 | Loss: 0.00003247
Iteration 70/1000 | Loss: 0.00003244
Iteration 71/1000 | Loss: 0.00003240
Iteration 72/1000 | Loss: 0.00003239
Iteration 73/1000 | Loss: 0.00003239
Iteration 74/1000 | Loss: 0.00003238
Iteration 75/1000 | Loss: 0.00021366
Iteration 76/1000 | Loss: 0.00020398
Iteration 77/1000 | Loss: 0.00008317
Iteration 78/1000 | Loss: 0.00006977
Iteration 79/1000 | Loss: 0.00009705
Iteration 80/1000 | Loss: 0.00016240
Iteration 81/1000 | Loss: 0.00009797
Iteration 82/1000 | Loss: 0.00004317
Iteration 83/1000 | Loss: 0.00034177
Iteration 84/1000 | Loss: 0.00021012
Iteration 85/1000 | Loss: 0.00004091
Iteration 86/1000 | Loss: 0.00003653
Iteration 87/1000 | Loss: 0.00003859
Iteration 88/1000 | Loss: 0.00003501
Iteration 89/1000 | Loss: 0.00003443
Iteration 90/1000 | Loss: 0.00003388
Iteration 91/1000 | Loss: 0.00003361
Iteration 92/1000 | Loss: 0.00003320
Iteration 93/1000 | Loss: 0.00003286
Iteration 94/1000 | Loss: 0.00003238
Iteration 95/1000 | Loss: 0.00003195
Iteration 96/1000 | Loss: 0.00003184
Iteration 97/1000 | Loss: 0.00003168
Iteration 98/1000 | Loss: 0.00003166
Iteration 99/1000 | Loss: 0.00003164
Iteration 100/1000 | Loss: 0.00003163
Iteration 101/1000 | Loss: 0.00003162
Iteration 102/1000 | Loss: 0.00003162
Iteration 103/1000 | Loss: 0.00003161
Iteration 104/1000 | Loss: 0.00003161
Iteration 105/1000 | Loss: 0.00003160
Iteration 106/1000 | Loss: 0.00003160
Iteration 107/1000 | Loss: 0.00003160
Iteration 108/1000 | Loss: 0.00003160
Iteration 109/1000 | Loss: 0.00003159
Iteration 110/1000 | Loss: 0.00003159
Iteration 111/1000 | Loss: 0.00003158
Iteration 112/1000 | Loss: 0.00003158
Iteration 113/1000 | Loss: 0.00003157
Iteration 114/1000 | Loss: 0.00003156
Iteration 115/1000 | Loss: 0.00003156
Iteration 116/1000 | Loss: 0.00003155
Iteration 117/1000 | Loss: 0.00003155
Iteration 118/1000 | Loss: 0.00003155
Iteration 119/1000 | Loss: 0.00003154
Iteration 120/1000 | Loss: 0.00003154
Iteration 121/1000 | Loss: 0.00003153
Iteration 122/1000 | Loss: 0.00003153
Iteration 123/1000 | Loss: 0.00003153
Iteration 124/1000 | Loss: 0.00003152
Iteration 125/1000 | Loss: 0.00003152
Iteration 126/1000 | Loss: 0.00003152
Iteration 127/1000 | Loss: 0.00003152
Iteration 128/1000 | Loss: 0.00003151
Iteration 129/1000 | Loss: 0.00003151
Iteration 130/1000 | Loss: 0.00003151
Iteration 131/1000 | Loss: 0.00003150
Iteration 132/1000 | Loss: 0.00003150
Iteration 133/1000 | Loss: 0.00003150
Iteration 134/1000 | Loss: 0.00003150
Iteration 135/1000 | Loss: 0.00003150
Iteration 136/1000 | Loss: 0.00003149
Iteration 137/1000 | Loss: 0.00003149
Iteration 138/1000 | Loss: 0.00003148
Iteration 139/1000 | Loss: 0.00003148
Iteration 140/1000 | Loss: 0.00003148
Iteration 141/1000 | Loss: 0.00003147
Iteration 142/1000 | Loss: 0.00003147
Iteration 143/1000 | Loss: 0.00003147
Iteration 144/1000 | Loss: 0.00003146
Iteration 145/1000 | Loss: 0.00003146
Iteration 146/1000 | Loss: 0.00003146
Iteration 147/1000 | Loss: 0.00003146
Iteration 148/1000 | Loss: 0.00003146
Iteration 149/1000 | Loss: 0.00003145
Iteration 150/1000 | Loss: 0.00003145
Iteration 151/1000 | Loss: 0.00003145
Iteration 152/1000 | Loss: 0.00003145
Iteration 153/1000 | Loss: 0.00003145
Iteration 154/1000 | Loss: 0.00003145
Iteration 155/1000 | Loss: 0.00003145
Iteration 156/1000 | Loss: 0.00003145
Iteration 157/1000 | Loss: 0.00003145
Iteration 158/1000 | Loss: 0.00003145
Iteration 159/1000 | Loss: 0.00003145
Iteration 160/1000 | Loss: 0.00003145
Iteration 161/1000 | Loss: 0.00003144
Iteration 162/1000 | Loss: 0.00003144
Iteration 163/1000 | Loss: 0.00003144
Iteration 164/1000 | Loss: 0.00003144
Iteration 165/1000 | Loss: 0.00003144
Iteration 166/1000 | Loss: 0.00003144
Iteration 167/1000 | Loss: 0.00003144
Iteration 168/1000 | Loss: 0.00003144
Iteration 169/1000 | Loss: 0.00003144
Iteration 170/1000 | Loss: 0.00003144
Iteration 171/1000 | Loss: 0.00003144
Iteration 172/1000 | Loss: 0.00003144
Iteration 173/1000 | Loss: 0.00003144
Iteration 174/1000 | Loss: 0.00003143
Iteration 175/1000 | Loss: 0.00003143
Iteration 176/1000 | Loss: 0.00003143
Iteration 177/1000 | Loss: 0.00003143
Iteration 178/1000 | Loss: 0.00003143
Iteration 179/1000 | Loss: 0.00003143
Iteration 180/1000 | Loss: 0.00003143
Iteration 181/1000 | Loss: 0.00003143
Iteration 182/1000 | Loss: 0.00003143
Iteration 183/1000 | Loss: 0.00003143
Iteration 184/1000 | Loss: 0.00003143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [3.14333665301092e-05, 3.14333665301092e-05, 3.14333665301092e-05, 3.14333665301092e-05, 3.14333665301092e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.14333665301092e-05

Optimization complete. Final v2v error: 4.087472438812256 mm

Highest mean error: 11.414898872375488 mm for frame 19

Lowest mean error: 3.3200736045837402 mm for frame 209

Saving results

Total time: 192.58533239364624
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00545027
Iteration 2/25 | Loss: 0.00144311
Iteration 3/25 | Loss: 0.00136424
Iteration 4/25 | Loss: 0.00134892
Iteration 5/25 | Loss: 0.00134263
Iteration 6/25 | Loss: 0.00134225
Iteration 7/25 | Loss: 0.00134225
Iteration 8/25 | Loss: 0.00134225
Iteration 9/25 | Loss: 0.00134225
Iteration 10/25 | Loss: 0.00134225
Iteration 11/25 | Loss: 0.00134225
Iteration 12/25 | Loss: 0.00134225
Iteration 13/25 | Loss: 0.00134225
Iteration 14/25 | Loss: 0.00134225
Iteration 15/25 | Loss: 0.00134225
Iteration 16/25 | Loss: 0.00134225
Iteration 17/25 | Loss: 0.00134225
Iteration 18/25 | Loss: 0.00134225
Iteration 19/25 | Loss: 0.00134225
Iteration 20/25 | Loss: 0.00134225
Iteration 21/25 | Loss: 0.00134225
Iteration 22/25 | Loss: 0.00134225
Iteration 23/25 | Loss: 0.00134225
Iteration 24/25 | Loss: 0.00134225
Iteration 25/25 | Loss: 0.00134225
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.001342253526672721, 0.001342253526672721, 0.001342253526672721, 0.001342253526672721, 0.001342253526672721]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001342253526672721

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.77499801
Iteration 2/25 | Loss: 0.00080232
Iteration 3/25 | Loss: 0.00080232
Iteration 4/25 | Loss: 0.00080232
Iteration 5/25 | Loss: 0.00080232
Iteration 6/25 | Loss: 0.00080232
Iteration 7/25 | Loss: 0.00080232
Iteration 8/25 | Loss: 0.00080232
Iteration 9/25 | Loss: 0.00080232
Iteration 10/25 | Loss: 0.00080232
Iteration 11/25 | Loss: 0.00080232
Iteration 12/25 | Loss: 0.00080232
Iteration 13/25 | Loss: 0.00080232
Iteration 14/25 | Loss: 0.00080232
Iteration 15/25 | Loss: 0.00080232
Iteration 16/25 | Loss: 0.00080232
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008023193804547191, 0.0008023193804547191, 0.0008023193804547191, 0.0008023193804547191, 0.0008023193804547191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008023193804547191

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080232
Iteration 2/1000 | Loss: 0.00003837
Iteration 3/1000 | Loss: 0.00002833
Iteration 4/1000 | Loss: 0.00002604
Iteration 5/1000 | Loss: 0.00002525
Iteration 6/1000 | Loss: 0.00002416
Iteration 7/1000 | Loss: 0.00002386
Iteration 8/1000 | Loss: 0.00002345
Iteration 9/1000 | Loss: 0.00002287
Iteration 10/1000 | Loss: 0.00002247
Iteration 11/1000 | Loss: 0.00002217
Iteration 12/1000 | Loss: 0.00002177
Iteration 13/1000 | Loss: 0.00002150
Iteration 14/1000 | Loss: 0.00002147
Iteration 15/1000 | Loss: 0.00002125
Iteration 16/1000 | Loss: 0.00002114
Iteration 17/1000 | Loss: 0.00002091
Iteration 18/1000 | Loss: 0.00002083
Iteration 19/1000 | Loss: 0.00002073
Iteration 20/1000 | Loss: 0.00002067
Iteration 21/1000 | Loss: 0.00002066
Iteration 22/1000 | Loss: 0.00002065
Iteration 23/1000 | Loss: 0.00002064
Iteration 24/1000 | Loss: 0.00002063
Iteration 25/1000 | Loss: 0.00002062
Iteration 26/1000 | Loss: 0.00002061
Iteration 27/1000 | Loss: 0.00002061
Iteration 28/1000 | Loss: 0.00002061
Iteration 29/1000 | Loss: 0.00002058
Iteration 30/1000 | Loss: 0.00002057
Iteration 31/1000 | Loss: 0.00002054
Iteration 32/1000 | Loss: 0.00002052
Iteration 33/1000 | Loss: 0.00002051
Iteration 34/1000 | Loss: 0.00002051
Iteration 35/1000 | Loss: 0.00002051
Iteration 36/1000 | Loss: 0.00002050
Iteration 37/1000 | Loss: 0.00002050
Iteration 38/1000 | Loss: 0.00002050
Iteration 39/1000 | Loss: 0.00002050
Iteration 40/1000 | Loss: 0.00002049
Iteration 41/1000 | Loss: 0.00002049
Iteration 42/1000 | Loss: 0.00002048
Iteration 43/1000 | Loss: 0.00002047
Iteration 44/1000 | Loss: 0.00002047
Iteration 45/1000 | Loss: 0.00002047
Iteration 46/1000 | Loss: 0.00002047
Iteration 47/1000 | Loss: 0.00002047
Iteration 48/1000 | Loss: 0.00002046
Iteration 49/1000 | Loss: 0.00002046
Iteration 50/1000 | Loss: 0.00002046
Iteration 51/1000 | Loss: 0.00002045
Iteration 52/1000 | Loss: 0.00002045
Iteration 53/1000 | Loss: 0.00002045
Iteration 54/1000 | Loss: 0.00002045
Iteration 55/1000 | Loss: 0.00002045
Iteration 56/1000 | Loss: 0.00002044
Iteration 57/1000 | Loss: 0.00002044
Iteration 58/1000 | Loss: 0.00002044
Iteration 59/1000 | Loss: 0.00002043
Iteration 60/1000 | Loss: 0.00002043
Iteration 61/1000 | Loss: 0.00002043
Iteration 62/1000 | Loss: 0.00002043
Iteration 63/1000 | Loss: 0.00002043
Iteration 64/1000 | Loss: 0.00002043
Iteration 65/1000 | Loss: 0.00002043
Iteration 66/1000 | Loss: 0.00002043
Iteration 67/1000 | Loss: 0.00002043
Iteration 68/1000 | Loss: 0.00002043
Iteration 69/1000 | Loss: 0.00002043
Iteration 70/1000 | Loss: 0.00002043
Iteration 71/1000 | Loss: 0.00002042
Iteration 72/1000 | Loss: 0.00002041
Iteration 73/1000 | Loss: 0.00002040
Iteration 74/1000 | Loss: 0.00002040
Iteration 75/1000 | Loss: 0.00002040
Iteration 76/1000 | Loss: 0.00002040
Iteration 77/1000 | Loss: 0.00002040
Iteration 78/1000 | Loss: 0.00002040
Iteration 79/1000 | Loss: 0.00002040
Iteration 80/1000 | Loss: 0.00002040
Iteration 81/1000 | Loss: 0.00002040
Iteration 82/1000 | Loss: 0.00002040
Iteration 83/1000 | Loss: 0.00002040
Iteration 84/1000 | Loss: 0.00002040
Iteration 85/1000 | Loss: 0.00002040
Iteration 86/1000 | Loss: 0.00002040
Iteration 87/1000 | Loss: 0.00002039
Iteration 88/1000 | Loss: 0.00002039
Iteration 89/1000 | Loss: 0.00002038
Iteration 90/1000 | Loss: 0.00002038
Iteration 91/1000 | Loss: 0.00002038
Iteration 92/1000 | Loss: 0.00002038
Iteration 93/1000 | Loss: 0.00002037
Iteration 94/1000 | Loss: 0.00002037
Iteration 95/1000 | Loss: 0.00002037
Iteration 96/1000 | Loss: 0.00002037
Iteration 97/1000 | Loss: 0.00002037
Iteration 98/1000 | Loss: 0.00002037
Iteration 99/1000 | Loss: 0.00002036
Iteration 100/1000 | Loss: 0.00002036
Iteration 101/1000 | Loss: 0.00002036
Iteration 102/1000 | Loss: 0.00002036
Iteration 103/1000 | Loss: 0.00002036
Iteration 104/1000 | Loss: 0.00002036
Iteration 105/1000 | Loss: 0.00002036
Iteration 106/1000 | Loss: 0.00002036
Iteration 107/1000 | Loss: 0.00002036
Iteration 108/1000 | Loss: 0.00002035
Iteration 109/1000 | Loss: 0.00002035
Iteration 110/1000 | Loss: 0.00002035
Iteration 111/1000 | Loss: 0.00002035
Iteration 112/1000 | Loss: 0.00002035
Iteration 113/1000 | Loss: 0.00002035
Iteration 114/1000 | Loss: 0.00002034
Iteration 115/1000 | Loss: 0.00002034
Iteration 116/1000 | Loss: 0.00002034
Iteration 117/1000 | Loss: 0.00002034
Iteration 118/1000 | Loss: 0.00002034
Iteration 119/1000 | Loss: 0.00002033
Iteration 120/1000 | Loss: 0.00002032
Iteration 121/1000 | Loss: 0.00002032
Iteration 122/1000 | Loss: 0.00002032
Iteration 123/1000 | Loss: 0.00002032
Iteration 124/1000 | Loss: 0.00002030
Iteration 125/1000 | Loss: 0.00002030
Iteration 126/1000 | Loss: 0.00002030
Iteration 127/1000 | Loss: 0.00002030
Iteration 128/1000 | Loss: 0.00002030
Iteration 129/1000 | Loss: 0.00002030
Iteration 130/1000 | Loss: 0.00002030
Iteration 131/1000 | Loss: 0.00002030
Iteration 132/1000 | Loss: 0.00002030
Iteration 133/1000 | Loss: 0.00002030
Iteration 134/1000 | Loss: 0.00002030
Iteration 135/1000 | Loss: 0.00002030
Iteration 136/1000 | Loss: 0.00002030
Iteration 137/1000 | Loss: 0.00002030
Iteration 138/1000 | Loss: 0.00002030
Iteration 139/1000 | Loss: 0.00002030
Iteration 140/1000 | Loss: 0.00002030
Iteration 141/1000 | Loss: 0.00002030
Iteration 142/1000 | Loss: 0.00002030
Iteration 143/1000 | Loss: 0.00002030
Iteration 144/1000 | Loss: 0.00002030
Iteration 145/1000 | Loss: 0.00002030
Iteration 146/1000 | Loss: 0.00002030
Iteration 147/1000 | Loss: 0.00002030
Iteration 148/1000 | Loss: 0.00002030
Iteration 149/1000 | Loss: 0.00002030
Iteration 150/1000 | Loss: 0.00002030
Iteration 151/1000 | Loss: 0.00002030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.029705501627177e-05, 2.029705501627177e-05, 2.029705501627177e-05, 2.029705501627177e-05, 2.029705501627177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.029705501627177e-05

Optimization complete. Final v2v error: 3.81345534324646 mm

Highest mean error: 3.8634419441223145 mm for frame 151

Lowest mean error: 3.7120234966278076 mm for frame 10

Saving results

Total time: 44.42752242088318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00624931
Iteration 2/25 | Loss: 0.00145794
Iteration 3/25 | Loss: 0.00133056
Iteration 4/25 | Loss: 0.00130603
Iteration 5/25 | Loss: 0.00129864
Iteration 6/25 | Loss: 0.00129696
Iteration 7/25 | Loss: 0.00129696
Iteration 8/25 | Loss: 0.00129696
Iteration 9/25 | Loss: 0.00129696
Iteration 10/25 | Loss: 0.00129696
Iteration 11/25 | Loss: 0.00129696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012969635426998138, 0.0012969635426998138, 0.0012969635426998138, 0.0012969635426998138, 0.0012969635426998138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012969635426998138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42338014
Iteration 2/25 | Loss: 0.00109501
Iteration 3/25 | Loss: 0.00109501
Iteration 4/25 | Loss: 0.00109501
Iteration 5/25 | Loss: 0.00109501
Iteration 6/25 | Loss: 0.00109501
Iteration 7/25 | Loss: 0.00109500
Iteration 8/25 | Loss: 0.00109500
Iteration 9/25 | Loss: 0.00109500
Iteration 10/25 | Loss: 0.00109500
Iteration 11/25 | Loss: 0.00109500
Iteration 12/25 | Loss: 0.00109500
Iteration 13/25 | Loss: 0.00109500
Iteration 14/25 | Loss: 0.00109500
Iteration 15/25 | Loss: 0.00109500
Iteration 16/25 | Loss: 0.00109500
Iteration 17/25 | Loss: 0.00109500
Iteration 18/25 | Loss: 0.00109500
Iteration 19/25 | Loss: 0.00109500
Iteration 20/25 | Loss: 0.00109500
Iteration 21/25 | Loss: 0.00109500
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0010950034484267235, 0.0010950034484267235, 0.0010950034484267235, 0.0010950034484267235, 0.0010950034484267235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010950034484267235

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109500
Iteration 2/1000 | Loss: 0.00003560
Iteration 3/1000 | Loss: 0.00002500
Iteration 4/1000 | Loss: 0.00002040
Iteration 5/1000 | Loss: 0.00001872
Iteration 6/1000 | Loss: 0.00001739
Iteration 7/1000 | Loss: 0.00001664
Iteration 8/1000 | Loss: 0.00001610
Iteration 9/1000 | Loss: 0.00001567
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001517
Iteration 12/1000 | Loss: 0.00001496
Iteration 13/1000 | Loss: 0.00001492
Iteration 14/1000 | Loss: 0.00001481
Iteration 15/1000 | Loss: 0.00001477
Iteration 16/1000 | Loss: 0.00001470
Iteration 17/1000 | Loss: 0.00001466
Iteration 18/1000 | Loss: 0.00001465
Iteration 19/1000 | Loss: 0.00001463
Iteration 20/1000 | Loss: 0.00001463
Iteration 21/1000 | Loss: 0.00001463
Iteration 22/1000 | Loss: 0.00001461
Iteration 23/1000 | Loss: 0.00001460
Iteration 24/1000 | Loss: 0.00001460
Iteration 25/1000 | Loss: 0.00001460
Iteration 26/1000 | Loss: 0.00001456
Iteration 27/1000 | Loss: 0.00001456
Iteration 28/1000 | Loss: 0.00001455
Iteration 29/1000 | Loss: 0.00001455
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00001454
Iteration 32/1000 | Loss: 0.00001453
Iteration 33/1000 | Loss: 0.00001453
Iteration 34/1000 | Loss: 0.00001453
Iteration 35/1000 | Loss: 0.00001452
Iteration 36/1000 | Loss: 0.00001452
Iteration 37/1000 | Loss: 0.00001452
Iteration 38/1000 | Loss: 0.00001451
Iteration 39/1000 | Loss: 0.00001451
Iteration 40/1000 | Loss: 0.00001451
Iteration 41/1000 | Loss: 0.00001451
Iteration 42/1000 | Loss: 0.00001451
Iteration 43/1000 | Loss: 0.00001451
Iteration 44/1000 | Loss: 0.00001451
Iteration 45/1000 | Loss: 0.00001451
Iteration 46/1000 | Loss: 0.00001450
Iteration 47/1000 | Loss: 0.00001450
Iteration 48/1000 | Loss: 0.00001450
Iteration 49/1000 | Loss: 0.00001449
Iteration 50/1000 | Loss: 0.00001449
Iteration 51/1000 | Loss: 0.00001449
Iteration 52/1000 | Loss: 0.00001448
Iteration 53/1000 | Loss: 0.00001448
Iteration 54/1000 | Loss: 0.00001448
Iteration 55/1000 | Loss: 0.00001448
Iteration 56/1000 | Loss: 0.00001447
Iteration 57/1000 | Loss: 0.00001447
Iteration 58/1000 | Loss: 0.00001447
Iteration 59/1000 | Loss: 0.00001446
Iteration 60/1000 | Loss: 0.00001446
Iteration 61/1000 | Loss: 0.00001446
Iteration 62/1000 | Loss: 0.00001446
Iteration 63/1000 | Loss: 0.00001445
Iteration 64/1000 | Loss: 0.00001445
Iteration 65/1000 | Loss: 0.00001445
Iteration 66/1000 | Loss: 0.00001445
Iteration 67/1000 | Loss: 0.00001444
Iteration 68/1000 | Loss: 0.00001444
Iteration 69/1000 | Loss: 0.00001444
Iteration 70/1000 | Loss: 0.00001443
Iteration 71/1000 | Loss: 0.00001443
Iteration 72/1000 | Loss: 0.00001443
Iteration 73/1000 | Loss: 0.00001442
Iteration 74/1000 | Loss: 0.00001442
Iteration 75/1000 | Loss: 0.00001442
Iteration 76/1000 | Loss: 0.00001441
Iteration 77/1000 | Loss: 0.00001441
Iteration 78/1000 | Loss: 0.00001441
Iteration 79/1000 | Loss: 0.00001440
Iteration 80/1000 | Loss: 0.00001440
Iteration 81/1000 | Loss: 0.00001440
Iteration 82/1000 | Loss: 0.00001440
Iteration 83/1000 | Loss: 0.00001439
Iteration 84/1000 | Loss: 0.00001439
Iteration 85/1000 | Loss: 0.00001439
Iteration 86/1000 | Loss: 0.00001439
Iteration 87/1000 | Loss: 0.00001438
Iteration 88/1000 | Loss: 0.00001438
Iteration 89/1000 | Loss: 0.00001438
Iteration 90/1000 | Loss: 0.00001438
Iteration 91/1000 | Loss: 0.00001437
Iteration 92/1000 | Loss: 0.00001437
Iteration 93/1000 | Loss: 0.00001437
Iteration 94/1000 | Loss: 0.00001437
Iteration 95/1000 | Loss: 0.00001437
Iteration 96/1000 | Loss: 0.00001437
Iteration 97/1000 | Loss: 0.00001436
Iteration 98/1000 | Loss: 0.00001436
Iteration 99/1000 | Loss: 0.00001436
Iteration 100/1000 | Loss: 0.00001436
Iteration 101/1000 | Loss: 0.00001436
Iteration 102/1000 | Loss: 0.00001435
Iteration 103/1000 | Loss: 0.00001435
Iteration 104/1000 | Loss: 0.00001435
Iteration 105/1000 | Loss: 0.00001435
Iteration 106/1000 | Loss: 0.00001435
Iteration 107/1000 | Loss: 0.00001435
Iteration 108/1000 | Loss: 0.00001434
Iteration 109/1000 | Loss: 0.00001434
Iteration 110/1000 | Loss: 0.00001434
Iteration 111/1000 | Loss: 0.00001434
Iteration 112/1000 | Loss: 0.00001434
Iteration 113/1000 | Loss: 0.00001434
Iteration 114/1000 | Loss: 0.00001434
Iteration 115/1000 | Loss: 0.00001434
Iteration 116/1000 | Loss: 0.00001434
Iteration 117/1000 | Loss: 0.00001434
Iteration 118/1000 | Loss: 0.00001434
Iteration 119/1000 | Loss: 0.00001433
Iteration 120/1000 | Loss: 0.00001433
Iteration 121/1000 | Loss: 0.00001433
Iteration 122/1000 | Loss: 0.00001433
Iteration 123/1000 | Loss: 0.00001432
Iteration 124/1000 | Loss: 0.00001432
Iteration 125/1000 | Loss: 0.00001432
Iteration 126/1000 | Loss: 0.00001432
Iteration 127/1000 | Loss: 0.00001432
Iteration 128/1000 | Loss: 0.00001432
Iteration 129/1000 | Loss: 0.00001431
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001431
Iteration 132/1000 | Loss: 0.00001431
Iteration 133/1000 | Loss: 0.00001431
Iteration 134/1000 | Loss: 0.00001431
Iteration 135/1000 | Loss: 0.00001431
Iteration 136/1000 | Loss: 0.00001431
Iteration 137/1000 | Loss: 0.00001430
Iteration 138/1000 | Loss: 0.00001430
Iteration 139/1000 | Loss: 0.00001430
Iteration 140/1000 | Loss: 0.00001430
Iteration 141/1000 | Loss: 0.00001430
Iteration 142/1000 | Loss: 0.00001429
Iteration 143/1000 | Loss: 0.00001429
Iteration 144/1000 | Loss: 0.00001429
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001429
Iteration 147/1000 | Loss: 0.00001429
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001428
Iteration 150/1000 | Loss: 0.00001428
Iteration 151/1000 | Loss: 0.00001428
Iteration 152/1000 | Loss: 0.00001428
Iteration 153/1000 | Loss: 0.00001428
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001427
Iteration 156/1000 | Loss: 0.00001427
Iteration 157/1000 | Loss: 0.00001427
Iteration 158/1000 | Loss: 0.00001427
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001427
Iteration 161/1000 | Loss: 0.00001427
Iteration 162/1000 | Loss: 0.00001427
Iteration 163/1000 | Loss: 0.00001426
Iteration 164/1000 | Loss: 0.00001426
Iteration 165/1000 | Loss: 0.00001426
Iteration 166/1000 | Loss: 0.00001426
Iteration 167/1000 | Loss: 0.00001426
Iteration 168/1000 | Loss: 0.00001426
Iteration 169/1000 | Loss: 0.00001425
Iteration 170/1000 | Loss: 0.00001425
Iteration 171/1000 | Loss: 0.00001425
Iteration 172/1000 | Loss: 0.00001425
Iteration 173/1000 | Loss: 0.00001425
Iteration 174/1000 | Loss: 0.00001425
Iteration 175/1000 | Loss: 0.00001425
Iteration 176/1000 | Loss: 0.00001425
Iteration 177/1000 | Loss: 0.00001424
Iteration 178/1000 | Loss: 0.00001424
Iteration 179/1000 | Loss: 0.00001424
Iteration 180/1000 | Loss: 0.00001424
Iteration 181/1000 | Loss: 0.00001424
Iteration 182/1000 | Loss: 0.00001424
Iteration 183/1000 | Loss: 0.00001424
Iteration 184/1000 | Loss: 0.00001423
Iteration 185/1000 | Loss: 0.00001423
Iteration 186/1000 | Loss: 0.00001423
Iteration 187/1000 | Loss: 0.00001423
Iteration 188/1000 | Loss: 0.00001423
Iteration 189/1000 | Loss: 0.00001423
Iteration 190/1000 | Loss: 0.00001423
Iteration 191/1000 | Loss: 0.00001422
Iteration 192/1000 | Loss: 0.00001422
Iteration 193/1000 | Loss: 0.00001422
Iteration 194/1000 | Loss: 0.00001422
Iteration 195/1000 | Loss: 0.00001422
Iteration 196/1000 | Loss: 0.00001421
Iteration 197/1000 | Loss: 0.00001421
Iteration 198/1000 | Loss: 0.00001421
Iteration 199/1000 | Loss: 0.00001421
Iteration 200/1000 | Loss: 0.00001421
Iteration 201/1000 | Loss: 0.00001421
Iteration 202/1000 | Loss: 0.00001421
Iteration 203/1000 | Loss: 0.00001421
Iteration 204/1000 | Loss: 0.00001421
Iteration 205/1000 | Loss: 0.00001421
Iteration 206/1000 | Loss: 0.00001421
Iteration 207/1000 | Loss: 0.00001421
Iteration 208/1000 | Loss: 0.00001420
Iteration 209/1000 | Loss: 0.00001420
Iteration 210/1000 | Loss: 0.00001420
Iteration 211/1000 | Loss: 0.00001420
Iteration 212/1000 | Loss: 0.00001420
Iteration 213/1000 | Loss: 0.00001420
Iteration 214/1000 | Loss: 0.00001420
Iteration 215/1000 | Loss: 0.00001420
Iteration 216/1000 | Loss: 0.00001419
Iteration 217/1000 | Loss: 0.00001419
Iteration 218/1000 | Loss: 0.00001419
Iteration 219/1000 | Loss: 0.00001419
Iteration 220/1000 | Loss: 0.00001419
Iteration 221/1000 | Loss: 0.00001419
Iteration 222/1000 | Loss: 0.00001419
Iteration 223/1000 | Loss: 0.00001419
Iteration 224/1000 | Loss: 0.00001419
Iteration 225/1000 | Loss: 0.00001419
Iteration 226/1000 | Loss: 0.00001419
Iteration 227/1000 | Loss: 0.00001419
Iteration 228/1000 | Loss: 0.00001419
Iteration 229/1000 | Loss: 0.00001419
Iteration 230/1000 | Loss: 0.00001419
Iteration 231/1000 | Loss: 0.00001419
Iteration 232/1000 | Loss: 0.00001419
Iteration 233/1000 | Loss: 0.00001419
Iteration 234/1000 | Loss: 0.00001419
Iteration 235/1000 | Loss: 0.00001419
Iteration 236/1000 | Loss: 0.00001419
Iteration 237/1000 | Loss: 0.00001419
Iteration 238/1000 | Loss: 0.00001419
Iteration 239/1000 | Loss: 0.00001419
Iteration 240/1000 | Loss: 0.00001419
Iteration 241/1000 | Loss: 0.00001419
Iteration 242/1000 | Loss: 0.00001419
Iteration 243/1000 | Loss: 0.00001419
Iteration 244/1000 | Loss: 0.00001419
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.4186876796884462e-05, 1.4186876796884462e-05, 1.4186876796884462e-05, 1.4186876796884462e-05, 1.4186876796884462e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4186876796884462e-05

Optimization complete. Final v2v error: 3.2028098106384277 mm

Highest mean error: 3.831949234008789 mm for frame 72

Lowest mean error: 2.8249473571777344 mm for frame 132

Saving results

Total time: 44.7120885848999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00784210
Iteration 2/25 | Loss: 0.00150643
Iteration 3/25 | Loss: 0.00131360
Iteration 4/25 | Loss: 0.00129939
Iteration 5/25 | Loss: 0.00129419
Iteration 6/25 | Loss: 0.00129276
Iteration 7/25 | Loss: 0.00129276
Iteration 8/25 | Loss: 0.00129276
Iteration 9/25 | Loss: 0.00129276
Iteration 10/25 | Loss: 0.00129276
Iteration 11/25 | Loss: 0.00129276
Iteration 12/25 | Loss: 0.00129276
Iteration 13/25 | Loss: 0.00129276
Iteration 14/25 | Loss: 0.00129276
Iteration 15/25 | Loss: 0.00129276
Iteration 16/25 | Loss: 0.00129276
Iteration 17/25 | Loss: 0.00129276
Iteration 18/25 | Loss: 0.00129276
Iteration 19/25 | Loss: 0.00129276
Iteration 20/25 | Loss: 0.00129276
Iteration 21/25 | Loss: 0.00129276
Iteration 22/25 | Loss: 0.00129276
Iteration 23/25 | Loss: 0.00129276
Iteration 24/25 | Loss: 0.00129276
Iteration 25/25 | Loss: 0.00129276

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19425678
Iteration 2/25 | Loss: 0.00095754
Iteration 3/25 | Loss: 0.00095753
Iteration 4/25 | Loss: 0.00095753
Iteration 5/25 | Loss: 0.00095753
Iteration 6/25 | Loss: 0.00095753
Iteration 7/25 | Loss: 0.00095753
Iteration 8/25 | Loss: 0.00095753
Iteration 9/25 | Loss: 0.00095753
Iteration 10/25 | Loss: 0.00095753
Iteration 11/25 | Loss: 0.00095753
Iteration 12/25 | Loss: 0.00095753
Iteration 13/25 | Loss: 0.00095753
Iteration 14/25 | Loss: 0.00095753
Iteration 15/25 | Loss: 0.00095753
Iteration 16/25 | Loss: 0.00095753
Iteration 17/25 | Loss: 0.00095753
Iteration 18/25 | Loss: 0.00095753
Iteration 19/25 | Loss: 0.00095753
Iteration 20/25 | Loss: 0.00095753
Iteration 21/25 | Loss: 0.00095753
Iteration 22/25 | Loss: 0.00095753
Iteration 23/25 | Loss: 0.00095753
Iteration 24/25 | Loss: 0.00095753
Iteration 25/25 | Loss: 0.00095753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00095753
Iteration 2/1000 | Loss: 0.00005554
Iteration 3/1000 | Loss: 0.00003341
Iteration 4/1000 | Loss: 0.00002565
Iteration 5/1000 | Loss: 0.00002255
Iteration 6/1000 | Loss: 0.00002091
Iteration 7/1000 | Loss: 0.00001950
Iteration 8/1000 | Loss: 0.00001879
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001760
Iteration 11/1000 | Loss: 0.00001732
Iteration 12/1000 | Loss: 0.00001702
Iteration 13/1000 | Loss: 0.00001678
Iteration 14/1000 | Loss: 0.00001663
Iteration 15/1000 | Loss: 0.00001662
Iteration 16/1000 | Loss: 0.00001660
Iteration 17/1000 | Loss: 0.00001656
Iteration 18/1000 | Loss: 0.00001640
Iteration 19/1000 | Loss: 0.00001638
Iteration 20/1000 | Loss: 0.00001634
Iteration 21/1000 | Loss: 0.00001629
Iteration 22/1000 | Loss: 0.00001617
Iteration 23/1000 | Loss: 0.00001617
Iteration 24/1000 | Loss: 0.00001614
Iteration 25/1000 | Loss: 0.00001614
Iteration 26/1000 | Loss: 0.00001613
Iteration 27/1000 | Loss: 0.00001613
Iteration 28/1000 | Loss: 0.00001612
Iteration 29/1000 | Loss: 0.00001612
Iteration 30/1000 | Loss: 0.00001608
Iteration 31/1000 | Loss: 0.00001605
Iteration 32/1000 | Loss: 0.00001604
Iteration 33/1000 | Loss: 0.00001604
Iteration 34/1000 | Loss: 0.00001600
Iteration 35/1000 | Loss: 0.00001598
Iteration 36/1000 | Loss: 0.00001598
Iteration 37/1000 | Loss: 0.00001597
Iteration 38/1000 | Loss: 0.00001597
Iteration 39/1000 | Loss: 0.00001594
Iteration 40/1000 | Loss: 0.00001592
Iteration 41/1000 | Loss: 0.00001592
Iteration 42/1000 | Loss: 0.00001591
Iteration 43/1000 | Loss: 0.00001591
Iteration 44/1000 | Loss: 0.00001591
Iteration 45/1000 | Loss: 0.00001590
Iteration 46/1000 | Loss: 0.00001590
Iteration 47/1000 | Loss: 0.00001590
Iteration 48/1000 | Loss: 0.00001590
Iteration 49/1000 | Loss: 0.00001590
Iteration 50/1000 | Loss: 0.00001589
Iteration 51/1000 | Loss: 0.00001589
Iteration 52/1000 | Loss: 0.00001588
Iteration 53/1000 | Loss: 0.00001588
Iteration 54/1000 | Loss: 0.00001587
Iteration 55/1000 | Loss: 0.00001587
Iteration 56/1000 | Loss: 0.00001587
Iteration 57/1000 | Loss: 0.00001587
Iteration 58/1000 | Loss: 0.00001586
Iteration 59/1000 | Loss: 0.00001586
Iteration 60/1000 | Loss: 0.00001586
Iteration 61/1000 | Loss: 0.00001586
Iteration 62/1000 | Loss: 0.00001586
Iteration 63/1000 | Loss: 0.00001585
Iteration 64/1000 | Loss: 0.00001585
Iteration 65/1000 | Loss: 0.00001585
Iteration 66/1000 | Loss: 0.00001585
Iteration 67/1000 | Loss: 0.00001584
Iteration 68/1000 | Loss: 0.00001584
Iteration 69/1000 | Loss: 0.00001583
Iteration 70/1000 | Loss: 0.00001583
Iteration 71/1000 | Loss: 0.00001582
Iteration 72/1000 | Loss: 0.00001582
Iteration 73/1000 | Loss: 0.00001582
Iteration 74/1000 | Loss: 0.00001582
Iteration 75/1000 | Loss: 0.00001582
Iteration 76/1000 | Loss: 0.00001582
Iteration 77/1000 | Loss: 0.00001582
Iteration 78/1000 | Loss: 0.00001582
Iteration 79/1000 | Loss: 0.00001581
Iteration 80/1000 | Loss: 0.00001581
Iteration 81/1000 | Loss: 0.00001581
Iteration 82/1000 | Loss: 0.00001581
Iteration 83/1000 | Loss: 0.00001581
Iteration 84/1000 | Loss: 0.00001581
Iteration 85/1000 | Loss: 0.00001581
Iteration 86/1000 | Loss: 0.00001581
Iteration 87/1000 | Loss: 0.00001580
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001580
Iteration 90/1000 | Loss: 0.00001580
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001579
Iteration 93/1000 | Loss: 0.00001579
Iteration 94/1000 | Loss: 0.00001578
Iteration 95/1000 | Loss: 0.00001578
Iteration 96/1000 | Loss: 0.00001578
Iteration 97/1000 | Loss: 0.00001577
Iteration 98/1000 | Loss: 0.00001577
Iteration 99/1000 | Loss: 0.00001577
Iteration 100/1000 | Loss: 0.00001577
Iteration 101/1000 | Loss: 0.00001576
Iteration 102/1000 | Loss: 0.00001576
Iteration 103/1000 | Loss: 0.00001576
Iteration 104/1000 | Loss: 0.00001576
Iteration 105/1000 | Loss: 0.00001576
Iteration 106/1000 | Loss: 0.00001576
Iteration 107/1000 | Loss: 0.00001575
Iteration 108/1000 | Loss: 0.00001575
Iteration 109/1000 | Loss: 0.00001575
Iteration 110/1000 | Loss: 0.00001575
Iteration 111/1000 | Loss: 0.00001575
Iteration 112/1000 | Loss: 0.00001575
Iteration 113/1000 | Loss: 0.00001575
Iteration 114/1000 | Loss: 0.00001575
Iteration 115/1000 | Loss: 0.00001575
Iteration 116/1000 | Loss: 0.00001574
Iteration 117/1000 | Loss: 0.00001574
Iteration 118/1000 | Loss: 0.00001574
Iteration 119/1000 | Loss: 0.00001573
Iteration 120/1000 | Loss: 0.00001573
Iteration 121/1000 | Loss: 0.00001573
Iteration 122/1000 | Loss: 0.00001573
Iteration 123/1000 | Loss: 0.00001572
Iteration 124/1000 | Loss: 0.00001572
Iteration 125/1000 | Loss: 0.00001572
Iteration 126/1000 | Loss: 0.00001572
Iteration 127/1000 | Loss: 0.00001572
Iteration 128/1000 | Loss: 0.00001572
Iteration 129/1000 | Loss: 0.00001572
Iteration 130/1000 | Loss: 0.00001572
Iteration 131/1000 | Loss: 0.00001572
Iteration 132/1000 | Loss: 0.00001572
Iteration 133/1000 | Loss: 0.00001571
Iteration 134/1000 | Loss: 0.00001571
Iteration 135/1000 | Loss: 0.00001571
Iteration 136/1000 | Loss: 0.00001571
Iteration 137/1000 | Loss: 0.00001571
Iteration 138/1000 | Loss: 0.00001570
Iteration 139/1000 | Loss: 0.00001570
Iteration 140/1000 | Loss: 0.00001570
Iteration 141/1000 | Loss: 0.00001570
Iteration 142/1000 | Loss: 0.00001570
Iteration 143/1000 | Loss: 0.00001570
Iteration 144/1000 | Loss: 0.00001570
Iteration 145/1000 | Loss: 0.00001570
Iteration 146/1000 | Loss: 0.00001570
Iteration 147/1000 | Loss: 0.00001570
Iteration 148/1000 | Loss: 0.00001570
Iteration 149/1000 | Loss: 0.00001570
Iteration 150/1000 | Loss: 0.00001570
Iteration 151/1000 | Loss: 0.00001569
Iteration 152/1000 | Loss: 0.00001569
Iteration 153/1000 | Loss: 0.00001569
Iteration 154/1000 | Loss: 0.00001569
Iteration 155/1000 | Loss: 0.00001569
Iteration 156/1000 | Loss: 0.00001569
Iteration 157/1000 | Loss: 0.00001569
Iteration 158/1000 | Loss: 0.00001569
Iteration 159/1000 | Loss: 0.00001569
Iteration 160/1000 | Loss: 0.00001569
Iteration 161/1000 | Loss: 0.00001569
Iteration 162/1000 | Loss: 0.00001569
Iteration 163/1000 | Loss: 0.00001569
Iteration 164/1000 | Loss: 0.00001569
Iteration 165/1000 | Loss: 0.00001569
Iteration 166/1000 | Loss: 0.00001569
Iteration 167/1000 | Loss: 0.00001569
Iteration 168/1000 | Loss: 0.00001569
Iteration 169/1000 | Loss: 0.00001569
Iteration 170/1000 | Loss: 0.00001569
Iteration 171/1000 | Loss: 0.00001569
Iteration 172/1000 | Loss: 0.00001569
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.5689071005908772e-05, 1.5689071005908772e-05, 1.5689071005908772e-05, 1.5689071005908772e-05, 1.5689071005908772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5689071005908772e-05

Optimization complete. Final v2v error: 3.3049380779266357 mm

Highest mean error: 4.527867317199707 mm for frame 69

Lowest mean error: 2.8198752403259277 mm for frame 103

Saving results

Total time: 43.68124842643738
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00866289
Iteration 2/25 | Loss: 0.00158694
Iteration 3/25 | Loss: 0.00136443
Iteration 4/25 | Loss: 0.00135318
Iteration 5/25 | Loss: 0.00135046
Iteration 6/25 | Loss: 0.00135027
Iteration 7/25 | Loss: 0.00135027
Iteration 8/25 | Loss: 0.00135027
Iteration 9/25 | Loss: 0.00135027
Iteration 10/25 | Loss: 0.00135027
Iteration 11/25 | Loss: 0.00135027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001350266276858747, 0.001350266276858747, 0.001350266276858747, 0.001350266276858747, 0.001350266276858747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001350266276858747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94443762
Iteration 2/25 | Loss: 0.00063129
Iteration 3/25 | Loss: 0.00063129
Iteration 4/25 | Loss: 0.00063129
Iteration 5/25 | Loss: 0.00063129
Iteration 6/25 | Loss: 0.00063129
Iteration 7/25 | Loss: 0.00063129
Iteration 8/25 | Loss: 0.00063129
Iteration 9/25 | Loss: 0.00063129
Iteration 10/25 | Loss: 0.00063129
Iteration 11/25 | Loss: 0.00063129
Iteration 12/25 | Loss: 0.00063129
Iteration 13/25 | Loss: 0.00063129
Iteration 14/25 | Loss: 0.00063129
Iteration 15/25 | Loss: 0.00063129
Iteration 16/25 | Loss: 0.00063129
Iteration 17/25 | Loss: 0.00063129
Iteration 18/25 | Loss: 0.00063129
Iteration 19/25 | Loss: 0.00063129
Iteration 20/25 | Loss: 0.00063129
Iteration 21/25 | Loss: 0.00063129
Iteration 22/25 | Loss: 0.00063129
Iteration 23/25 | Loss: 0.00063129
Iteration 24/25 | Loss: 0.00063129
Iteration 25/25 | Loss: 0.00063129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063129
Iteration 2/1000 | Loss: 0.00004414
Iteration 3/1000 | Loss: 0.00003555
Iteration 4/1000 | Loss: 0.00003335
Iteration 5/1000 | Loss: 0.00003230
Iteration 6/1000 | Loss: 0.00003108
Iteration 7/1000 | Loss: 0.00003028
Iteration 8/1000 | Loss: 0.00003001
Iteration 9/1000 | Loss: 0.00002964
Iteration 10/1000 | Loss: 0.00002928
Iteration 11/1000 | Loss: 0.00002928
Iteration 12/1000 | Loss: 0.00002927
Iteration 13/1000 | Loss: 0.00002897
Iteration 14/1000 | Loss: 0.00002879
Iteration 15/1000 | Loss: 0.00002858
Iteration 16/1000 | Loss: 0.00002856
Iteration 17/1000 | Loss: 0.00002855
Iteration 18/1000 | Loss: 0.00002849
Iteration 19/1000 | Loss: 0.00002849
Iteration 20/1000 | Loss: 0.00002848
Iteration 21/1000 | Loss: 0.00002848
Iteration 22/1000 | Loss: 0.00002844
Iteration 23/1000 | Loss: 0.00002844
Iteration 24/1000 | Loss: 0.00002844
Iteration 25/1000 | Loss: 0.00002844
Iteration 26/1000 | Loss: 0.00002844
Iteration 27/1000 | Loss: 0.00002844
Iteration 28/1000 | Loss: 0.00002844
Iteration 29/1000 | Loss: 0.00002842
Iteration 30/1000 | Loss: 0.00002842
Iteration 31/1000 | Loss: 0.00002841
Iteration 32/1000 | Loss: 0.00002841
Iteration 33/1000 | Loss: 0.00002841
Iteration 34/1000 | Loss: 0.00002840
Iteration 35/1000 | Loss: 0.00002840
Iteration 36/1000 | Loss: 0.00002840
Iteration 37/1000 | Loss: 0.00002840
Iteration 38/1000 | Loss: 0.00002840
Iteration 39/1000 | Loss: 0.00002840
Iteration 40/1000 | Loss: 0.00002840
Iteration 41/1000 | Loss: 0.00002840
Iteration 42/1000 | Loss: 0.00002840
Iteration 43/1000 | Loss: 0.00002840
Iteration 44/1000 | Loss: 0.00002839
Iteration 45/1000 | Loss: 0.00002839
Iteration 46/1000 | Loss: 0.00002839
Iteration 47/1000 | Loss: 0.00002838
Iteration 48/1000 | Loss: 0.00002838
Iteration 49/1000 | Loss: 0.00002837
Iteration 50/1000 | Loss: 0.00002837
Iteration 51/1000 | Loss: 0.00002836
Iteration 52/1000 | Loss: 0.00002835
Iteration 53/1000 | Loss: 0.00002835
Iteration 54/1000 | Loss: 0.00002834
Iteration 55/1000 | Loss: 0.00002834
Iteration 56/1000 | Loss: 0.00002834
Iteration 57/1000 | Loss: 0.00002834
Iteration 58/1000 | Loss: 0.00002834
Iteration 59/1000 | Loss: 0.00002834
Iteration 60/1000 | Loss: 0.00002834
Iteration 61/1000 | Loss: 0.00002834
Iteration 62/1000 | Loss: 0.00002834
Iteration 63/1000 | Loss: 0.00002834
Iteration 64/1000 | Loss: 0.00002833
Iteration 65/1000 | Loss: 0.00002833
Iteration 66/1000 | Loss: 0.00002833
Iteration 67/1000 | Loss: 0.00002833
Iteration 68/1000 | Loss: 0.00002833
Iteration 69/1000 | Loss: 0.00002833
Iteration 70/1000 | Loss: 0.00002833
Iteration 71/1000 | Loss: 0.00002833
Iteration 72/1000 | Loss: 0.00002832
Iteration 73/1000 | Loss: 0.00002832
Iteration 74/1000 | Loss: 0.00002832
Iteration 75/1000 | Loss: 0.00002832
Iteration 76/1000 | Loss: 0.00002832
Iteration 77/1000 | Loss: 0.00002832
Iteration 78/1000 | Loss: 0.00002832
Iteration 79/1000 | Loss: 0.00002832
Iteration 80/1000 | Loss: 0.00002832
Iteration 81/1000 | Loss: 0.00002832
Iteration 82/1000 | Loss: 0.00002832
Iteration 83/1000 | Loss: 0.00002832
Iteration 84/1000 | Loss: 0.00002832
Iteration 85/1000 | Loss: 0.00002832
Iteration 86/1000 | Loss: 0.00002832
Iteration 87/1000 | Loss: 0.00002831
Iteration 88/1000 | Loss: 0.00002831
Iteration 89/1000 | Loss: 0.00002831
Iteration 90/1000 | Loss: 0.00002831
Iteration 91/1000 | Loss: 0.00002831
Iteration 92/1000 | Loss: 0.00002831
Iteration 93/1000 | Loss: 0.00002831
Iteration 94/1000 | Loss: 0.00002831
Iteration 95/1000 | Loss: 0.00002831
Iteration 96/1000 | Loss: 0.00002831
Iteration 97/1000 | Loss: 0.00002831
Iteration 98/1000 | Loss: 0.00002831
Iteration 99/1000 | Loss: 0.00002831
Iteration 100/1000 | Loss: 0.00002831
Iteration 101/1000 | Loss: 0.00002831
Iteration 102/1000 | Loss: 0.00002831
Iteration 103/1000 | Loss: 0.00002831
Iteration 104/1000 | Loss: 0.00002831
Iteration 105/1000 | Loss: 0.00002831
Iteration 106/1000 | Loss: 0.00002831
Iteration 107/1000 | Loss: 0.00002830
Iteration 108/1000 | Loss: 0.00002830
Iteration 109/1000 | Loss: 0.00002830
Iteration 110/1000 | Loss: 0.00002830
Iteration 111/1000 | Loss: 0.00002829
Iteration 112/1000 | Loss: 0.00002829
Iteration 113/1000 | Loss: 0.00002829
Iteration 114/1000 | Loss: 0.00002828
Iteration 115/1000 | Loss: 0.00002828
Iteration 116/1000 | Loss: 0.00002828
Iteration 117/1000 | Loss: 0.00002828
Iteration 118/1000 | Loss: 0.00002828
Iteration 119/1000 | Loss: 0.00002828
Iteration 120/1000 | Loss: 0.00002828
Iteration 121/1000 | Loss: 0.00002828
Iteration 122/1000 | Loss: 0.00002828
Iteration 123/1000 | Loss: 0.00002828
Iteration 124/1000 | Loss: 0.00002828
Iteration 125/1000 | Loss: 0.00002827
Iteration 126/1000 | Loss: 0.00002827
Iteration 127/1000 | Loss: 0.00002827
Iteration 128/1000 | Loss: 0.00002827
Iteration 129/1000 | Loss: 0.00002827
Iteration 130/1000 | Loss: 0.00002827
Iteration 131/1000 | Loss: 0.00002827
Iteration 132/1000 | Loss: 0.00002827
Iteration 133/1000 | Loss: 0.00002827
Iteration 134/1000 | Loss: 0.00002827
Iteration 135/1000 | Loss: 0.00002827
Iteration 136/1000 | Loss: 0.00002826
Iteration 137/1000 | Loss: 0.00002826
Iteration 138/1000 | Loss: 0.00002826
Iteration 139/1000 | Loss: 0.00002826
Iteration 140/1000 | Loss: 0.00002825
Iteration 141/1000 | Loss: 0.00002825
Iteration 142/1000 | Loss: 0.00002825
Iteration 143/1000 | Loss: 0.00002825
Iteration 144/1000 | Loss: 0.00002825
Iteration 145/1000 | Loss: 0.00002825
Iteration 146/1000 | Loss: 0.00002825
Iteration 147/1000 | Loss: 0.00002825
Iteration 148/1000 | Loss: 0.00002825
Iteration 149/1000 | Loss: 0.00002825
Iteration 150/1000 | Loss: 0.00002825
Iteration 151/1000 | Loss: 0.00002825
Iteration 152/1000 | Loss: 0.00002825
Iteration 153/1000 | Loss: 0.00002825
Iteration 154/1000 | Loss: 0.00002825
Iteration 155/1000 | Loss: 0.00002825
Iteration 156/1000 | Loss: 0.00002825
Iteration 157/1000 | Loss: 0.00002825
Iteration 158/1000 | Loss: 0.00002825
Iteration 159/1000 | Loss: 0.00002825
Iteration 160/1000 | Loss: 0.00002825
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.8245240173419006e-05, 2.8245240173419006e-05, 2.8245240173419006e-05, 2.8245240173419006e-05, 2.8245240173419006e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8245240173419006e-05

Optimization complete. Final v2v error: 4.499804496765137 mm

Highest mean error: 4.815097808837891 mm for frame 1

Lowest mean error: 4.1860222816467285 mm for frame 81

Saving results

Total time: 34.590311765670776
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01110304
Iteration 2/25 | Loss: 0.01110304
Iteration 3/25 | Loss: 0.01110304
Iteration 4/25 | Loss: 0.01110304
Iteration 5/25 | Loss: 0.01110304
Iteration 6/25 | Loss: 0.01110304
Iteration 7/25 | Loss: 0.01110304
Iteration 8/25 | Loss: 0.01110304
Iteration 9/25 | Loss: 0.01110304
Iteration 10/25 | Loss: 0.01110304
Iteration 11/25 | Loss: 0.01110304
Iteration 12/25 | Loss: 0.01110303
Iteration 13/25 | Loss: 0.01110303
Iteration 14/25 | Loss: 0.01110303
Iteration 15/25 | Loss: 0.01110303
Iteration 16/25 | Loss: 0.01110303
Iteration 17/25 | Loss: 0.01110303
Iteration 18/25 | Loss: 0.01110303
Iteration 19/25 | Loss: 0.01110303
Iteration 20/25 | Loss: 0.01110303
Iteration 21/25 | Loss: 0.01110303
Iteration 22/25 | Loss: 0.01110303
Iteration 23/25 | Loss: 0.01110303
Iteration 24/25 | Loss: 0.01110303
Iteration 25/25 | Loss: 0.01110303

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.58909225
Iteration 2/25 | Loss: 0.18641436
Iteration 3/25 | Loss: 0.18449008
Iteration 4/25 | Loss: 0.18396619
Iteration 5/25 | Loss: 0.18396606
Iteration 6/25 | Loss: 0.18396606
Iteration 7/25 | Loss: 0.18396606
Iteration 8/25 | Loss: 0.18396606
Iteration 9/25 | Loss: 0.18396606
Iteration 10/25 | Loss: 0.18396606
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.18396605551242828, 0.18396605551242828, 0.18396605551242828, 0.18396605551242828, 0.18396605551242828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.18396605551242828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.18396606
Iteration 2/1000 | Loss: 0.00266157
Iteration 3/1000 | Loss: 0.00108757
Iteration 4/1000 | Loss: 0.00025458
Iteration 5/1000 | Loss: 0.00017206
Iteration 6/1000 | Loss: 0.00010277
Iteration 7/1000 | Loss: 0.00004519
Iteration 8/1000 | Loss: 0.00003887
Iteration 9/1000 | Loss: 0.00003437
Iteration 10/1000 | Loss: 0.00003251
Iteration 11/1000 | Loss: 0.00003693
Iteration 12/1000 | Loss: 0.00002888
Iteration 13/1000 | Loss: 0.00003847
Iteration 14/1000 | Loss: 0.00003112
Iteration 15/1000 | Loss: 0.00002619
Iteration 16/1000 | Loss: 0.00002521
Iteration 17/1000 | Loss: 0.00002705
Iteration 18/1000 | Loss: 0.00002351
Iteration 19/1000 | Loss: 0.00003097
Iteration 20/1000 | Loss: 0.00005874
Iteration 21/1000 | Loss: 0.00002324
Iteration 22/1000 | Loss: 0.00002221
Iteration 23/1000 | Loss: 0.00002159
Iteration 24/1000 | Loss: 0.00005283
Iteration 25/1000 | Loss: 0.00002150
Iteration 26/1000 | Loss: 0.00002157
Iteration 27/1000 | Loss: 0.00002088
Iteration 28/1000 | Loss: 0.00002675
Iteration 29/1000 | Loss: 0.00003124
Iteration 30/1000 | Loss: 0.00002988
Iteration 31/1000 | Loss: 0.00003808
Iteration 32/1000 | Loss: 0.00002180
Iteration 33/1000 | Loss: 0.00002006
Iteration 34/1000 | Loss: 0.00002006
Iteration 35/1000 | Loss: 0.00002006
Iteration 36/1000 | Loss: 0.00002006
Iteration 37/1000 | Loss: 0.00002006
Iteration 38/1000 | Loss: 0.00002006
Iteration 39/1000 | Loss: 0.00002006
Iteration 40/1000 | Loss: 0.00002006
Iteration 41/1000 | Loss: 0.00002005
Iteration 42/1000 | Loss: 0.00003456
Iteration 43/1000 | Loss: 0.00002181
Iteration 44/1000 | Loss: 0.00001988
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001979
Iteration 47/1000 | Loss: 0.00002088
Iteration 48/1000 | Loss: 0.00002636
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002181
Iteration 51/1000 | Loss: 0.00006671
Iteration 52/1000 | Loss: 0.00002337
Iteration 53/1000 | Loss: 0.00002577
Iteration 54/1000 | Loss: 0.00004577
Iteration 55/1000 | Loss: 0.00002912
Iteration 56/1000 | Loss: 0.00002313
Iteration 57/1000 | Loss: 0.00002041
Iteration 58/1000 | Loss: 0.00002039
Iteration 59/1000 | Loss: 0.00003211
Iteration 60/1000 | Loss: 0.00001950
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001950
Iteration 63/1000 | Loss: 0.00001950
Iteration 64/1000 | Loss: 0.00001949
Iteration 65/1000 | Loss: 0.00001949
Iteration 66/1000 | Loss: 0.00001949
Iteration 67/1000 | Loss: 0.00001949
Iteration 68/1000 | Loss: 0.00001949
Iteration 69/1000 | Loss: 0.00001949
Iteration 70/1000 | Loss: 0.00001949
Iteration 71/1000 | Loss: 0.00001949
Iteration 72/1000 | Loss: 0.00001949
Iteration 73/1000 | Loss: 0.00001949
Iteration 74/1000 | Loss: 0.00001949
Iteration 75/1000 | Loss: 0.00001949
Iteration 76/1000 | Loss: 0.00001949
Iteration 77/1000 | Loss: 0.00001949
Iteration 78/1000 | Loss: 0.00001949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.949305624293629e-05, 1.949305624293629e-05, 1.949305624293629e-05, 1.949305624293629e-05, 1.949305624293629e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.949305624293629e-05

Optimization complete. Final v2v error: 3.699481725692749 mm

Highest mean error: 3.966374397277832 mm for frame 35

Lowest mean error: 3.4797825813293457 mm for frame 0

Saving results

Total time: 78.9775242805481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803267
Iteration 2/25 | Loss: 0.00136999
Iteration 3/25 | Loss: 0.00128168
Iteration 4/25 | Loss: 0.00127248
Iteration 5/25 | Loss: 0.00127025
Iteration 6/25 | Loss: 0.00127025
Iteration 7/25 | Loss: 0.00127025
Iteration 8/25 | Loss: 0.00127025
Iteration 9/25 | Loss: 0.00127025
Iteration 10/25 | Loss: 0.00127025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012702540261670947, 0.0012702540261670947, 0.0012702540261670947, 0.0012702540261670947, 0.0012702540261670947]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012702540261670947

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39994526
Iteration 2/25 | Loss: 0.00082666
Iteration 3/25 | Loss: 0.00082666
Iteration 4/25 | Loss: 0.00082666
Iteration 5/25 | Loss: 0.00082666
Iteration 6/25 | Loss: 0.00082666
Iteration 7/25 | Loss: 0.00082666
Iteration 8/25 | Loss: 0.00082665
Iteration 9/25 | Loss: 0.00082665
Iteration 10/25 | Loss: 0.00082665
Iteration 11/25 | Loss: 0.00082665
Iteration 12/25 | Loss: 0.00082665
Iteration 13/25 | Loss: 0.00082665
Iteration 14/25 | Loss: 0.00082665
Iteration 15/25 | Loss: 0.00082665
Iteration 16/25 | Loss: 0.00082665
Iteration 17/25 | Loss: 0.00082665
Iteration 18/25 | Loss: 0.00082665
Iteration 19/25 | Loss: 0.00082665
Iteration 20/25 | Loss: 0.00082665
Iteration 21/25 | Loss: 0.00082665
Iteration 22/25 | Loss: 0.00082665
Iteration 23/25 | Loss: 0.00082665
Iteration 24/25 | Loss: 0.00082665
Iteration 25/25 | Loss: 0.00082665

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00082665
Iteration 2/1000 | Loss: 0.00002838
Iteration 3/1000 | Loss: 0.00001920
Iteration 4/1000 | Loss: 0.00001635
Iteration 5/1000 | Loss: 0.00001537
Iteration 6/1000 | Loss: 0.00001458
Iteration 7/1000 | Loss: 0.00001393
Iteration 8/1000 | Loss: 0.00001351
Iteration 9/1000 | Loss: 0.00001329
Iteration 10/1000 | Loss: 0.00001327
Iteration 11/1000 | Loss: 0.00001326
Iteration 12/1000 | Loss: 0.00001307
Iteration 13/1000 | Loss: 0.00001285
Iteration 14/1000 | Loss: 0.00001273
Iteration 15/1000 | Loss: 0.00001270
Iteration 16/1000 | Loss: 0.00001270
Iteration 17/1000 | Loss: 0.00001268
Iteration 18/1000 | Loss: 0.00001267
Iteration 19/1000 | Loss: 0.00001266
Iteration 20/1000 | Loss: 0.00001266
Iteration 21/1000 | Loss: 0.00001262
Iteration 22/1000 | Loss: 0.00001252
Iteration 23/1000 | Loss: 0.00001252
Iteration 24/1000 | Loss: 0.00001251
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001250
Iteration 27/1000 | Loss: 0.00001249
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001247
Iteration 31/1000 | Loss: 0.00001246
Iteration 32/1000 | Loss: 0.00001246
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001243
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001242
Iteration 39/1000 | Loss: 0.00001242
Iteration 40/1000 | Loss: 0.00001241
Iteration 41/1000 | Loss: 0.00001241
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001237
Iteration 46/1000 | Loss: 0.00001235
Iteration 47/1000 | Loss: 0.00001235
Iteration 48/1000 | Loss: 0.00001235
Iteration 49/1000 | Loss: 0.00001234
Iteration 50/1000 | Loss: 0.00001234
Iteration 51/1000 | Loss: 0.00001233
Iteration 52/1000 | Loss: 0.00001232
Iteration 53/1000 | Loss: 0.00001227
Iteration 54/1000 | Loss: 0.00001226
Iteration 55/1000 | Loss: 0.00001224
Iteration 56/1000 | Loss: 0.00001224
Iteration 57/1000 | Loss: 0.00001222
Iteration 58/1000 | Loss: 0.00001222
Iteration 59/1000 | Loss: 0.00001222
Iteration 60/1000 | Loss: 0.00001222
Iteration 61/1000 | Loss: 0.00001218
Iteration 62/1000 | Loss: 0.00001218
Iteration 63/1000 | Loss: 0.00001217
Iteration 64/1000 | Loss: 0.00001217
Iteration 65/1000 | Loss: 0.00001216
Iteration 66/1000 | Loss: 0.00001216
Iteration 67/1000 | Loss: 0.00001216
Iteration 68/1000 | Loss: 0.00001215
Iteration 69/1000 | Loss: 0.00001215
Iteration 70/1000 | Loss: 0.00001215
Iteration 71/1000 | Loss: 0.00001215
Iteration 72/1000 | Loss: 0.00001214
Iteration 73/1000 | Loss: 0.00001214
Iteration 74/1000 | Loss: 0.00001214
Iteration 75/1000 | Loss: 0.00001214
Iteration 76/1000 | Loss: 0.00001214
Iteration 77/1000 | Loss: 0.00001213
Iteration 78/1000 | Loss: 0.00001213
Iteration 79/1000 | Loss: 0.00001213
Iteration 80/1000 | Loss: 0.00001212
Iteration 81/1000 | Loss: 0.00001212
Iteration 82/1000 | Loss: 0.00001211
Iteration 83/1000 | Loss: 0.00001211
Iteration 84/1000 | Loss: 0.00001211
Iteration 85/1000 | Loss: 0.00001211
Iteration 86/1000 | Loss: 0.00001211
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001210
Iteration 89/1000 | Loss: 0.00001209
Iteration 90/1000 | Loss: 0.00001207
Iteration 91/1000 | Loss: 0.00001207
Iteration 92/1000 | Loss: 0.00001207
Iteration 93/1000 | Loss: 0.00001207
Iteration 94/1000 | Loss: 0.00001207
Iteration 95/1000 | Loss: 0.00001207
Iteration 96/1000 | Loss: 0.00001207
Iteration 97/1000 | Loss: 0.00001206
Iteration 98/1000 | Loss: 0.00001206
Iteration 99/1000 | Loss: 0.00001206
Iteration 100/1000 | Loss: 0.00001205
Iteration 101/1000 | Loss: 0.00001205
Iteration 102/1000 | Loss: 0.00001205
Iteration 103/1000 | Loss: 0.00001205
Iteration 104/1000 | Loss: 0.00001205
Iteration 105/1000 | Loss: 0.00001205
Iteration 106/1000 | Loss: 0.00001205
Iteration 107/1000 | Loss: 0.00001205
Iteration 108/1000 | Loss: 0.00001205
Iteration 109/1000 | Loss: 0.00001205
Iteration 110/1000 | Loss: 0.00001204
Iteration 111/1000 | Loss: 0.00001204
Iteration 112/1000 | Loss: 0.00001204
Iteration 113/1000 | Loss: 0.00001204
Iteration 114/1000 | Loss: 0.00001204
Iteration 115/1000 | Loss: 0.00001204
Iteration 116/1000 | Loss: 0.00001204
Iteration 117/1000 | Loss: 0.00001204
Iteration 118/1000 | Loss: 0.00001203
Iteration 119/1000 | Loss: 0.00001203
Iteration 120/1000 | Loss: 0.00001203
Iteration 121/1000 | Loss: 0.00001203
Iteration 122/1000 | Loss: 0.00001203
Iteration 123/1000 | Loss: 0.00001202
Iteration 124/1000 | Loss: 0.00001202
Iteration 125/1000 | Loss: 0.00001202
Iteration 126/1000 | Loss: 0.00001202
Iteration 127/1000 | Loss: 0.00001202
Iteration 128/1000 | Loss: 0.00001202
Iteration 129/1000 | Loss: 0.00001202
Iteration 130/1000 | Loss: 0.00001202
Iteration 131/1000 | Loss: 0.00001202
Iteration 132/1000 | Loss: 0.00001202
Iteration 133/1000 | Loss: 0.00001202
Iteration 134/1000 | Loss: 0.00001202
Iteration 135/1000 | Loss: 0.00001202
Iteration 136/1000 | Loss: 0.00001202
Iteration 137/1000 | Loss: 0.00001201
Iteration 138/1000 | Loss: 0.00001201
Iteration 139/1000 | Loss: 0.00001201
Iteration 140/1000 | Loss: 0.00001201
Iteration 141/1000 | Loss: 0.00001201
Iteration 142/1000 | Loss: 0.00001201
Iteration 143/1000 | Loss: 0.00001201
Iteration 144/1000 | Loss: 0.00001201
Iteration 145/1000 | Loss: 0.00001201
Iteration 146/1000 | Loss: 0.00001201
Iteration 147/1000 | Loss: 0.00001201
Iteration 148/1000 | Loss: 0.00001201
Iteration 149/1000 | Loss: 0.00001201
Iteration 150/1000 | Loss: 0.00001201
Iteration 151/1000 | Loss: 0.00001200
Iteration 152/1000 | Loss: 0.00001200
Iteration 153/1000 | Loss: 0.00001200
Iteration 154/1000 | Loss: 0.00001200
Iteration 155/1000 | Loss: 0.00001200
Iteration 156/1000 | Loss: 0.00001200
Iteration 157/1000 | Loss: 0.00001200
Iteration 158/1000 | Loss: 0.00001199
Iteration 159/1000 | Loss: 0.00001199
Iteration 160/1000 | Loss: 0.00001199
Iteration 161/1000 | Loss: 0.00001198
Iteration 162/1000 | Loss: 0.00001198
Iteration 163/1000 | Loss: 0.00001198
Iteration 164/1000 | Loss: 0.00001197
Iteration 165/1000 | Loss: 0.00001197
Iteration 166/1000 | Loss: 0.00001197
Iteration 167/1000 | Loss: 0.00001197
Iteration 168/1000 | Loss: 0.00001197
Iteration 169/1000 | Loss: 0.00001196
Iteration 170/1000 | Loss: 0.00001196
Iteration 171/1000 | Loss: 0.00001196
Iteration 172/1000 | Loss: 0.00001196
Iteration 173/1000 | Loss: 0.00001196
Iteration 174/1000 | Loss: 0.00001196
Iteration 175/1000 | Loss: 0.00001196
Iteration 176/1000 | Loss: 0.00001196
Iteration 177/1000 | Loss: 0.00001195
Iteration 178/1000 | Loss: 0.00001195
Iteration 179/1000 | Loss: 0.00001195
Iteration 180/1000 | Loss: 0.00001195
Iteration 181/1000 | Loss: 0.00001195
Iteration 182/1000 | Loss: 0.00001195
Iteration 183/1000 | Loss: 0.00001195
Iteration 184/1000 | Loss: 0.00001194
Iteration 185/1000 | Loss: 0.00001194
Iteration 186/1000 | Loss: 0.00001194
Iteration 187/1000 | Loss: 0.00001194
Iteration 188/1000 | Loss: 0.00001194
Iteration 189/1000 | Loss: 0.00001194
Iteration 190/1000 | Loss: 0.00001194
Iteration 191/1000 | Loss: 0.00001194
Iteration 192/1000 | Loss: 0.00001194
Iteration 193/1000 | Loss: 0.00001194
Iteration 194/1000 | Loss: 0.00001194
Iteration 195/1000 | Loss: 0.00001194
Iteration 196/1000 | Loss: 0.00001193
Iteration 197/1000 | Loss: 0.00001193
Iteration 198/1000 | Loss: 0.00001193
Iteration 199/1000 | Loss: 0.00001193
Iteration 200/1000 | Loss: 0.00001193
Iteration 201/1000 | Loss: 0.00001193
Iteration 202/1000 | Loss: 0.00001193
Iteration 203/1000 | Loss: 0.00001193
Iteration 204/1000 | Loss: 0.00001193
Iteration 205/1000 | Loss: 0.00001193
Iteration 206/1000 | Loss: 0.00001193
Iteration 207/1000 | Loss: 0.00001193
Iteration 208/1000 | Loss: 0.00001193
Iteration 209/1000 | Loss: 0.00001193
Iteration 210/1000 | Loss: 0.00001193
Iteration 211/1000 | Loss: 0.00001193
Iteration 212/1000 | Loss: 0.00001193
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.1934994290641043e-05, 1.1934994290641043e-05, 1.1934994290641043e-05, 1.1934994290641043e-05, 1.1934994290641043e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1934994290641043e-05

Optimization complete. Final v2v error: 2.9469974040985107 mm

Highest mean error: 3.1671714782714844 mm for frame 68

Lowest mean error: 2.8091366291046143 mm for frame 171

Saving results

Total time: 42.790128231048584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856565
Iteration 2/25 | Loss: 0.00144458
Iteration 3/25 | Loss: 0.00135024
Iteration 4/25 | Loss: 0.00133598
Iteration 5/25 | Loss: 0.00133218
Iteration 6/25 | Loss: 0.00133155
Iteration 7/25 | Loss: 0.00133155
Iteration 8/25 | Loss: 0.00133155
Iteration 9/25 | Loss: 0.00133155
Iteration 10/25 | Loss: 0.00133155
Iteration 11/25 | Loss: 0.00133155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013315450632944703, 0.0013315450632944703, 0.0013315450632944703, 0.0013315450632944703, 0.0013315450632944703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013315450632944703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.60298705
Iteration 2/25 | Loss: 0.00101845
Iteration 3/25 | Loss: 0.00101842
Iteration 4/25 | Loss: 0.00101842
Iteration 5/25 | Loss: 0.00101842
Iteration 6/25 | Loss: 0.00101842
Iteration 7/25 | Loss: 0.00101842
Iteration 8/25 | Loss: 0.00101842
Iteration 9/25 | Loss: 0.00101842
Iteration 10/25 | Loss: 0.00101842
Iteration 11/25 | Loss: 0.00101842
Iteration 12/25 | Loss: 0.00101842
Iteration 13/25 | Loss: 0.00101842
Iteration 14/25 | Loss: 0.00101842
Iteration 15/25 | Loss: 0.00101842
Iteration 16/25 | Loss: 0.00101842
Iteration 17/25 | Loss: 0.00101842
Iteration 18/25 | Loss: 0.00101842
Iteration 19/25 | Loss: 0.00101842
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010184204438701272, 0.0010184204438701272, 0.0010184204438701272, 0.0010184204438701272, 0.0010184204438701272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010184204438701272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101842
Iteration 2/1000 | Loss: 0.00004239
Iteration 3/1000 | Loss: 0.00002660
Iteration 4/1000 | Loss: 0.00002157
Iteration 5/1000 | Loss: 0.00001993
Iteration 6/1000 | Loss: 0.00001902
Iteration 7/1000 | Loss: 0.00001860
Iteration 8/1000 | Loss: 0.00001811
Iteration 9/1000 | Loss: 0.00001778
Iteration 10/1000 | Loss: 0.00001756
Iteration 11/1000 | Loss: 0.00001727
Iteration 12/1000 | Loss: 0.00001716
Iteration 13/1000 | Loss: 0.00001709
Iteration 14/1000 | Loss: 0.00001699
Iteration 15/1000 | Loss: 0.00001692
Iteration 16/1000 | Loss: 0.00001680
Iteration 17/1000 | Loss: 0.00001670
Iteration 18/1000 | Loss: 0.00001668
Iteration 19/1000 | Loss: 0.00001668
Iteration 20/1000 | Loss: 0.00001664
Iteration 21/1000 | Loss: 0.00001659
Iteration 22/1000 | Loss: 0.00001655
Iteration 23/1000 | Loss: 0.00001654
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001649
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001647
Iteration 28/1000 | Loss: 0.00001646
Iteration 29/1000 | Loss: 0.00001645
Iteration 30/1000 | Loss: 0.00001645
Iteration 31/1000 | Loss: 0.00001644
Iteration 32/1000 | Loss: 0.00001644
Iteration 33/1000 | Loss: 0.00001643
Iteration 34/1000 | Loss: 0.00001643
Iteration 35/1000 | Loss: 0.00001643
Iteration 36/1000 | Loss: 0.00001643
Iteration 37/1000 | Loss: 0.00001642
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001641
Iteration 41/1000 | Loss: 0.00001640
Iteration 42/1000 | Loss: 0.00001640
Iteration 43/1000 | Loss: 0.00001639
Iteration 44/1000 | Loss: 0.00001639
Iteration 45/1000 | Loss: 0.00001639
Iteration 46/1000 | Loss: 0.00001639
Iteration 47/1000 | Loss: 0.00001637
Iteration 48/1000 | Loss: 0.00001637
Iteration 49/1000 | Loss: 0.00001637
Iteration 50/1000 | Loss: 0.00001636
Iteration 51/1000 | Loss: 0.00001636
Iteration 52/1000 | Loss: 0.00001635
Iteration 53/1000 | Loss: 0.00001634
Iteration 54/1000 | Loss: 0.00001634
Iteration 55/1000 | Loss: 0.00001634
Iteration 56/1000 | Loss: 0.00001633
Iteration 57/1000 | Loss: 0.00001633
Iteration 58/1000 | Loss: 0.00001633
Iteration 59/1000 | Loss: 0.00001633
Iteration 60/1000 | Loss: 0.00001633
Iteration 61/1000 | Loss: 0.00001632
Iteration 62/1000 | Loss: 0.00001631
Iteration 63/1000 | Loss: 0.00001631
Iteration 64/1000 | Loss: 0.00001631
Iteration 65/1000 | Loss: 0.00001630
Iteration 66/1000 | Loss: 0.00001626
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001624
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001623
Iteration 75/1000 | Loss: 0.00001623
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001622
Iteration 78/1000 | Loss: 0.00001622
Iteration 79/1000 | Loss: 0.00001622
Iteration 80/1000 | Loss: 0.00001621
Iteration 81/1000 | Loss: 0.00001621
Iteration 82/1000 | Loss: 0.00001621
Iteration 83/1000 | Loss: 0.00001621
Iteration 84/1000 | Loss: 0.00001621
Iteration 85/1000 | Loss: 0.00001621
Iteration 86/1000 | Loss: 0.00001621
Iteration 87/1000 | Loss: 0.00001621
Iteration 88/1000 | Loss: 0.00001621
Iteration 89/1000 | Loss: 0.00001620
Iteration 90/1000 | Loss: 0.00001620
Iteration 91/1000 | Loss: 0.00001620
Iteration 92/1000 | Loss: 0.00001620
Iteration 93/1000 | Loss: 0.00001620
Iteration 94/1000 | Loss: 0.00001620
Iteration 95/1000 | Loss: 0.00001620
Iteration 96/1000 | Loss: 0.00001620
Iteration 97/1000 | Loss: 0.00001620
Iteration 98/1000 | Loss: 0.00001620
Iteration 99/1000 | Loss: 0.00001620
Iteration 100/1000 | Loss: 0.00001619
Iteration 101/1000 | Loss: 0.00001619
Iteration 102/1000 | Loss: 0.00001619
Iteration 103/1000 | Loss: 0.00001619
Iteration 104/1000 | Loss: 0.00001619
Iteration 105/1000 | Loss: 0.00001619
Iteration 106/1000 | Loss: 0.00001619
Iteration 107/1000 | Loss: 0.00001619
Iteration 108/1000 | Loss: 0.00001619
Iteration 109/1000 | Loss: 0.00001619
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.6193658666452393e-05, 1.6193658666452393e-05, 1.6193658666452393e-05, 1.6193658666452393e-05, 1.6193658666452393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6193658666452393e-05

Optimization complete. Final v2v error: 3.392763614654541 mm

Highest mean error: 3.726719379425049 mm for frame 32

Lowest mean error: 3.1763617992401123 mm for frame 55

Saving results

Total time: 36.88759160041809
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00647575
Iteration 2/25 | Loss: 0.00148870
Iteration 3/25 | Loss: 0.00137004
Iteration 4/25 | Loss: 0.00136356
Iteration 5/25 | Loss: 0.00136272
Iteration 6/25 | Loss: 0.00136272
Iteration 7/25 | Loss: 0.00136272
Iteration 8/25 | Loss: 0.00136272
Iteration 9/25 | Loss: 0.00136272
Iteration 10/25 | Loss: 0.00136272
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013627249281853437, 0.0013627249281853437, 0.0013627249281853437, 0.0013627249281853437, 0.0013627249281853437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013627249281853437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33583629
Iteration 2/25 | Loss: 0.00101605
Iteration 3/25 | Loss: 0.00101605
Iteration 4/25 | Loss: 0.00101605
Iteration 5/25 | Loss: 0.00101605
Iteration 6/25 | Loss: 0.00101605
Iteration 7/25 | Loss: 0.00101605
Iteration 8/25 | Loss: 0.00101604
Iteration 9/25 | Loss: 0.00101604
Iteration 10/25 | Loss: 0.00101604
Iteration 11/25 | Loss: 0.00101604
Iteration 12/25 | Loss: 0.00101604
Iteration 13/25 | Loss: 0.00101604
Iteration 14/25 | Loss: 0.00101604
Iteration 15/25 | Loss: 0.00101604
Iteration 16/25 | Loss: 0.00101604
Iteration 17/25 | Loss: 0.00101604
Iteration 18/25 | Loss: 0.00101604
Iteration 19/25 | Loss: 0.00101604
Iteration 20/25 | Loss: 0.00101604
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001016044057905674, 0.001016044057905674, 0.001016044057905674, 0.001016044057905674, 0.001016044057905674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001016044057905674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101604
Iteration 2/1000 | Loss: 0.00003962
Iteration 3/1000 | Loss: 0.00002960
Iteration 4/1000 | Loss: 0.00002312
Iteration 5/1000 | Loss: 0.00002141
Iteration 6/1000 | Loss: 0.00002055
Iteration 7/1000 | Loss: 0.00001986
Iteration 8/1000 | Loss: 0.00001941
Iteration 9/1000 | Loss: 0.00001906
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001858
Iteration 12/1000 | Loss: 0.00001843
Iteration 13/1000 | Loss: 0.00001824
Iteration 14/1000 | Loss: 0.00001813
Iteration 15/1000 | Loss: 0.00001804
Iteration 16/1000 | Loss: 0.00001791
Iteration 17/1000 | Loss: 0.00001781
Iteration 18/1000 | Loss: 0.00001779
Iteration 19/1000 | Loss: 0.00001779
Iteration 20/1000 | Loss: 0.00001777
Iteration 21/1000 | Loss: 0.00001776
Iteration 22/1000 | Loss: 0.00001776
Iteration 23/1000 | Loss: 0.00001776
Iteration 24/1000 | Loss: 0.00001772
Iteration 25/1000 | Loss: 0.00001772
Iteration 26/1000 | Loss: 0.00001770
Iteration 27/1000 | Loss: 0.00001765
Iteration 28/1000 | Loss: 0.00001763
Iteration 29/1000 | Loss: 0.00001762
Iteration 30/1000 | Loss: 0.00001762
Iteration 31/1000 | Loss: 0.00001761
Iteration 32/1000 | Loss: 0.00001761
Iteration 33/1000 | Loss: 0.00001761
Iteration 34/1000 | Loss: 0.00001761
Iteration 35/1000 | Loss: 0.00001760
Iteration 36/1000 | Loss: 0.00001759
Iteration 37/1000 | Loss: 0.00001759
Iteration 38/1000 | Loss: 0.00001758
Iteration 39/1000 | Loss: 0.00001758
Iteration 40/1000 | Loss: 0.00001757
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001756
Iteration 44/1000 | Loss: 0.00001755
Iteration 45/1000 | Loss: 0.00001755
Iteration 46/1000 | Loss: 0.00001755
Iteration 47/1000 | Loss: 0.00001751
Iteration 48/1000 | Loss: 0.00001750
Iteration 49/1000 | Loss: 0.00001749
Iteration 50/1000 | Loss: 0.00001749
Iteration 51/1000 | Loss: 0.00001749
Iteration 52/1000 | Loss: 0.00001748
Iteration 53/1000 | Loss: 0.00001748
Iteration 54/1000 | Loss: 0.00001748
Iteration 55/1000 | Loss: 0.00001748
Iteration 56/1000 | Loss: 0.00001747
Iteration 57/1000 | Loss: 0.00001747
Iteration 58/1000 | Loss: 0.00001747
Iteration 59/1000 | Loss: 0.00001747
Iteration 60/1000 | Loss: 0.00001747
Iteration 61/1000 | Loss: 0.00001747
Iteration 62/1000 | Loss: 0.00001747
Iteration 63/1000 | Loss: 0.00001747
Iteration 64/1000 | Loss: 0.00001746
Iteration 65/1000 | Loss: 0.00001746
Iteration 66/1000 | Loss: 0.00001746
Iteration 67/1000 | Loss: 0.00001745
Iteration 68/1000 | Loss: 0.00001745
Iteration 69/1000 | Loss: 0.00001745
Iteration 70/1000 | Loss: 0.00001745
Iteration 71/1000 | Loss: 0.00001745
Iteration 72/1000 | Loss: 0.00001744
Iteration 73/1000 | Loss: 0.00001744
Iteration 74/1000 | Loss: 0.00001744
Iteration 75/1000 | Loss: 0.00001744
Iteration 76/1000 | Loss: 0.00001744
Iteration 77/1000 | Loss: 0.00001744
Iteration 78/1000 | Loss: 0.00001744
Iteration 79/1000 | Loss: 0.00001743
Iteration 80/1000 | Loss: 0.00001743
Iteration 81/1000 | Loss: 0.00001743
Iteration 82/1000 | Loss: 0.00001743
Iteration 83/1000 | Loss: 0.00001742
Iteration 84/1000 | Loss: 0.00001742
Iteration 85/1000 | Loss: 0.00001742
Iteration 86/1000 | Loss: 0.00001742
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001741
Iteration 89/1000 | Loss: 0.00001741
Iteration 90/1000 | Loss: 0.00001740
Iteration 91/1000 | Loss: 0.00001740
Iteration 92/1000 | Loss: 0.00001740
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001740
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00001740
Iteration 97/1000 | Loss: 0.00001740
Iteration 98/1000 | Loss: 0.00001740
Iteration 99/1000 | Loss: 0.00001740
Iteration 100/1000 | Loss: 0.00001740
Iteration 101/1000 | Loss: 0.00001740
Iteration 102/1000 | Loss: 0.00001740
Iteration 103/1000 | Loss: 0.00001740
Iteration 104/1000 | Loss: 0.00001740
Iteration 105/1000 | Loss: 0.00001740
Iteration 106/1000 | Loss: 0.00001740
Iteration 107/1000 | Loss: 0.00001740
Iteration 108/1000 | Loss: 0.00001740
Iteration 109/1000 | Loss: 0.00001740
Iteration 110/1000 | Loss: 0.00001740
Iteration 111/1000 | Loss: 0.00001740
Iteration 112/1000 | Loss: 0.00001740
Iteration 113/1000 | Loss: 0.00001740
Iteration 114/1000 | Loss: 0.00001740
Iteration 115/1000 | Loss: 0.00001740
Iteration 116/1000 | Loss: 0.00001740
Iteration 117/1000 | Loss: 0.00001740
Iteration 118/1000 | Loss: 0.00001740
Iteration 119/1000 | Loss: 0.00001740
Iteration 120/1000 | Loss: 0.00001740
Iteration 121/1000 | Loss: 0.00001740
Iteration 122/1000 | Loss: 0.00001740
Iteration 123/1000 | Loss: 0.00001740
Iteration 124/1000 | Loss: 0.00001740
Iteration 125/1000 | Loss: 0.00001740
Iteration 126/1000 | Loss: 0.00001740
Iteration 127/1000 | Loss: 0.00001740
Iteration 128/1000 | Loss: 0.00001740
Iteration 129/1000 | Loss: 0.00001740
Iteration 130/1000 | Loss: 0.00001740
Iteration 131/1000 | Loss: 0.00001740
Iteration 132/1000 | Loss: 0.00001740
Iteration 133/1000 | Loss: 0.00001740
Iteration 134/1000 | Loss: 0.00001740
Iteration 135/1000 | Loss: 0.00001740
Iteration 136/1000 | Loss: 0.00001740
Iteration 137/1000 | Loss: 0.00001740
Iteration 138/1000 | Loss: 0.00001740
Iteration 139/1000 | Loss: 0.00001740
Iteration 140/1000 | Loss: 0.00001740
Iteration 141/1000 | Loss: 0.00001740
Iteration 142/1000 | Loss: 0.00001740
Iteration 143/1000 | Loss: 0.00001740
Iteration 144/1000 | Loss: 0.00001740
Iteration 145/1000 | Loss: 0.00001740
Iteration 146/1000 | Loss: 0.00001740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.7396092516719364e-05, 1.7396092516719364e-05, 1.7396092516719364e-05, 1.7396092516719364e-05, 1.7396092516719364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7396092516719364e-05

Optimization complete. Final v2v error: 3.508051633834839 mm

Highest mean error: 3.8374557495117188 mm for frame 108

Lowest mean error: 3.0438215732574463 mm for frame 92

Saving results

Total time: 38.46115279197693
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987937
Iteration 2/25 | Loss: 0.00240087
Iteration 3/25 | Loss: 0.00210322
Iteration 4/25 | Loss: 0.00194072
Iteration 5/25 | Loss: 0.00197952
Iteration 6/25 | Loss: 0.00183355
Iteration 7/25 | Loss: 0.00148747
Iteration 8/25 | Loss: 0.00143874
Iteration 9/25 | Loss: 0.00143094
Iteration 10/25 | Loss: 0.00142674
Iteration 11/25 | Loss: 0.00142758
Iteration 12/25 | Loss: 0.00142677
Iteration 13/25 | Loss: 0.00142624
Iteration 14/25 | Loss: 0.00142614
Iteration 15/25 | Loss: 0.00142716
Iteration 16/25 | Loss: 0.00142646
Iteration 17/25 | Loss: 0.00142616
Iteration 18/25 | Loss: 0.00142651
Iteration 19/25 | Loss: 0.00142670
Iteration 20/25 | Loss: 0.00142576
Iteration 21/25 | Loss: 0.00142664
Iteration 22/25 | Loss: 0.00142627
Iteration 23/25 | Loss: 0.00142631
Iteration 24/25 | Loss: 0.00142611
Iteration 25/25 | Loss: 0.00142646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38534343
Iteration 2/25 | Loss: 0.00076452
Iteration 3/25 | Loss: 0.00076452
Iteration 4/25 | Loss: 0.00076451
Iteration 5/25 | Loss: 0.00076451
Iteration 6/25 | Loss: 0.00076451
Iteration 7/25 | Loss: 0.00076451
Iteration 8/25 | Loss: 0.00076451
Iteration 9/25 | Loss: 0.00076451
Iteration 10/25 | Loss: 0.00076451
Iteration 11/25 | Loss: 0.00076451
Iteration 12/25 | Loss: 0.00076451
Iteration 13/25 | Loss: 0.00076451
Iteration 14/25 | Loss: 0.00076451
Iteration 15/25 | Loss: 0.00076451
Iteration 16/25 | Loss: 0.00076451
Iteration 17/25 | Loss: 0.00076451
Iteration 18/25 | Loss: 0.00076451
Iteration 19/25 | Loss: 0.00076451
Iteration 20/25 | Loss: 0.00076451
Iteration 21/25 | Loss: 0.00076451
Iteration 22/25 | Loss: 0.00076451
Iteration 23/25 | Loss: 0.00076451
Iteration 24/25 | Loss: 0.00076451
Iteration 25/25 | Loss: 0.00076451

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076451
Iteration 2/1000 | Loss: 0.00005811
Iteration 3/1000 | Loss: 0.00003583
Iteration 4/1000 | Loss: 0.00003671
Iteration 5/1000 | Loss: 0.00004470
Iteration 6/1000 | Loss: 0.00003851
Iteration 7/1000 | Loss: 0.00004155
Iteration 8/1000 | Loss: 0.00004423
Iteration 9/1000 | Loss: 0.00003543
Iteration 10/1000 | Loss: 0.00004669
Iteration 11/1000 | Loss: 0.00003384
Iteration 12/1000 | Loss: 0.00004658
Iteration 13/1000 | Loss: 0.00004467
Iteration 14/1000 | Loss: 0.00003454
Iteration 15/1000 | Loss: 0.00004504
Iteration 16/1000 | Loss: 0.00004719
Iteration 17/1000 | Loss: 0.00004431
Iteration 18/1000 | Loss: 0.00003895
Iteration 19/1000 | Loss: 0.00004068
Iteration 20/1000 | Loss: 0.00004257
Iteration 21/1000 | Loss: 0.00005190
Iteration 22/1000 | Loss: 0.00004127
Iteration 23/1000 | Loss: 0.00004233
Iteration 24/1000 | Loss: 0.00004438
Iteration 25/1000 | Loss: 0.00004303
Iteration 26/1000 | Loss: 0.00004457
Iteration 27/1000 | Loss: 0.00004335
Iteration 28/1000 | Loss: 0.00004065
Iteration 29/1000 | Loss: 0.00005651
Iteration 30/1000 | Loss: 0.00003934
Iteration 31/1000 | Loss: 0.00003982
Iteration 32/1000 | Loss: 0.00004094
Iteration 33/1000 | Loss: 0.00004108
Iteration 34/1000 | Loss: 0.00004166
Iteration 35/1000 | Loss: 0.00003917
Iteration 36/1000 | Loss: 0.00003968
Iteration 37/1000 | Loss: 0.00004840
Iteration 38/1000 | Loss: 0.00003819
Iteration 39/1000 | Loss: 0.00003987
Iteration 40/1000 | Loss: 0.00005071
Iteration 41/1000 | Loss: 0.00005032
Iteration 42/1000 | Loss: 0.00003968
Iteration 43/1000 | Loss: 0.00004037
Iteration 44/1000 | Loss: 0.00004731
Iteration 45/1000 | Loss: 0.00004023
Iteration 46/1000 | Loss: 0.00005055
Iteration 47/1000 | Loss: 0.00004021
Iteration 48/1000 | Loss: 0.00005051
Iteration 49/1000 | Loss: 0.00003215
Iteration 50/1000 | Loss: 0.00003024
Iteration 51/1000 | Loss: 0.00002979
Iteration 52/1000 | Loss: 0.00002934
Iteration 53/1000 | Loss: 0.00002904
Iteration 54/1000 | Loss: 0.00002893
Iteration 55/1000 | Loss: 0.00002869
Iteration 56/1000 | Loss: 0.00002846
Iteration 57/1000 | Loss: 0.00002833
Iteration 58/1000 | Loss: 0.00002833
Iteration 59/1000 | Loss: 0.00002826
Iteration 60/1000 | Loss: 0.00002826
Iteration 61/1000 | Loss: 0.00002826
Iteration 62/1000 | Loss: 0.00002825
Iteration 63/1000 | Loss: 0.00002825
Iteration 64/1000 | Loss: 0.00002825
Iteration 65/1000 | Loss: 0.00002825
Iteration 66/1000 | Loss: 0.00002824
Iteration 67/1000 | Loss: 0.00002824
Iteration 68/1000 | Loss: 0.00002824
Iteration 69/1000 | Loss: 0.00002823
Iteration 70/1000 | Loss: 0.00002823
Iteration 71/1000 | Loss: 0.00002822
Iteration 72/1000 | Loss: 0.00002822
Iteration 73/1000 | Loss: 0.00002821
Iteration 74/1000 | Loss: 0.00002821
Iteration 75/1000 | Loss: 0.00002821
Iteration 76/1000 | Loss: 0.00002820
Iteration 77/1000 | Loss: 0.00002819
Iteration 78/1000 | Loss: 0.00002819
Iteration 79/1000 | Loss: 0.00002819
Iteration 80/1000 | Loss: 0.00002818
Iteration 81/1000 | Loss: 0.00002818
Iteration 82/1000 | Loss: 0.00002818
Iteration 83/1000 | Loss: 0.00002818
Iteration 84/1000 | Loss: 0.00002818
Iteration 85/1000 | Loss: 0.00002818
Iteration 86/1000 | Loss: 0.00002818
Iteration 87/1000 | Loss: 0.00002818
Iteration 88/1000 | Loss: 0.00002818
Iteration 89/1000 | Loss: 0.00002818
Iteration 90/1000 | Loss: 0.00002818
Iteration 91/1000 | Loss: 0.00002817
Iteration 92/1000 | Loss: 0.00002817
Iteration 93/1000 | Loss: 0.00002817
Iteration 94/1000 | Loss: 0.00002817
Iteration 95/1000 | Loss: 0.00002816
Iteration 96/1000 | Loss: 0.00002816
Iteration 97/1000 | Loss: 0.00002816
Iteration 98/1000 | Loss: 0.00002815
Iteration 99/1000 | Loss: 0.00002814
Iteration 100/1000 | Loss: 0.00002813
Iteration 101/1000 | Loss: 0.00002813
Iteration 102/1000 | Loss: 0.00002813
Iteration 103/1000 | Loss: 0.00002813
Iteration 104/1000 | Loss: 0.00002813
Iteration 105/1000 | Loss: 0.00002813
Iteration 106/1000 | Loss: 0.00002813
Iteration 107/1000 | Loss: 0.00002812
Iteration 108/1000 | Loss: 0.00002812
Iteration 109/1000 | Loss: 0.00002812
Iteration 110/1000 | Loss: 0.00002812
Iteration 111/1000 | Loss: 0.00002812
Iteration 112/1000 | Loss: 0.00002812
Iteration 113/1000 | Loss: 0.00002812
Iteration 114/1000 | Loss: 0.00002812
Iteration 115/1000 | Loss: 0.00002811
Iteration 116/1000 | Loss: 0.00002811
Iteration 117/1000 | Loss: 0.00002811
Iteration 118/1000 | Loss: 0.00002811
Iteration 119/1000 | Loss: 0.00002811
Iteration 120/1000 | Loss: 0.00002811
Iteration 121/1000 | Loss: 0.00002811
Iteration 122/1000 | Loss: 0.00002811
Iteration 123/1000 | Loss: 0.00002810
Iteration 124/1000 | Loss: 0.00002810
Iteration 125/1000 | Loss: 0.00002810
Iteration 126/1000 | Loss: 0.00002810
Iteration 127/1000 | Loss: 0.00002810
Iteration 128/1000 | Loss: 0.00002810
Iteration 129/1000 | Loss: 0.00002810
Iteration 130/1000 | Loss: 0.00002810
Iteration 131/1000 | Loss: 0.00002810
Iteration 132/1000 | Loss: 0.00002810
Iteration 133/1000 | Loss: 0.00002809
Iteration 134/1000 | Loss: 0.00002809
Iteration 135/1000 | Loss: 0.00002809
Iteration 136/1000 | Loss: 0.00002809
Iteration 137/1000 | Loss: 0.00002809
Iteration 138/1000 | Loss: 0.00002809
Iteration 139/1000 | Loss: 0.00002809
Iteration 140/1000 | Loss: 0.00002809
Iteration 141/1000 | Loss: 0.00002809
Iteration 142/1000 | Loss: 0.00002809
Iteration 143/1000 | Loss: 0.00002809
Iteration 144/1000 | Loss: 0.00002809
Iteration 145/1000 | Loss: 0.00002808
Iteration 146/1000 | Loss: 0.00002808
Iteration 147/1000 | Loss: 0.00002808
Iteration 148/1000 | Loss: 0.00002808
Iteration 149/1000 | Loss: 0.00002808
Iteration 150/1000 | Loss: 0.00002808
Iteration 151/1000 | Loss: 0.00002808
Iteration 152/1000 | Loss: 0.00002808
Iteration 153/1000 | Loss: 0.00002808
Iteration 154/1000 | Loss: 0.00002808
Iteration 155/1000 | Loss: 0.00002808
Iteration 156/1000 | Loss: 0.00002808
Iteration 157/1000 | Loss: 0.00002807
Iteration 158/1000 | Loss: 0.00002807
Iteration 159/1000 | Loss: 0.00002807
Iteration 160/1000 | Loss: 0.00002807
Iteration 161/1000 | Loss: 0.00002807
Iteration 162/1000 | Loss: 0.00002807
Iteration 163/1000 | Loss: 0.00002807
Iteration 164/1000 | Loss: 0.00002807
Iteration 165/1000 | Loss: 0.00002807
Iteration 166/1000 | Loss: 0.00002807
Iteration 167/1000 | Loss: 0.00002807
Iteration 168/1000 | Loss: 0.00002807
Iteration 169/1000 | Loss: 0.00002807
Iteration 170/1000 | Loss: 0.00002807
Iteration 171/1000 | Loss: 0.00002807
Iteration 172/1000 | Loss: 0.00002806
Iteration 173/1000 | Loss: 0.00002806
Iteration 174/1000 | Loss: 0.00002806
Iteration 175/1000 | Loss: 0.00002806
Iteration 176/1000 | Loss: 0.00002806
Iteration 177/1000 | Loss: 0.00002806
Iteration 178/1000 | Loss: 0.00002806
Iteration 179/1000 | Loss: 0.00002806
Iteration 180/1000 | Loss: 0.00002806
Iteration 181/1000 | Loss: 0.00002806
Iteration 182/1000 | Loss: 0.00002806
Iteration 183/1000 | Loss: 0.00002806
Iteration 184/1000 | Loss: 0.00002806
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [2.805975964292884e-05, 2.805975964292884e-05, 2.805975964292884e-05, 2.805975964292884e-05, 2.805975964292884e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.805975964292884e-05

Optimization complete. Final v2v error: 4.5497026443481445 mm

Highest mean error: 5.166359901428223 mm for frame 113

Lowest mean error: 4.249444484710693 mm for frame 238

Saving results

Total time: 146.58474230766296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_010/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_010/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041986
Iteration 2/25 | Loss: 0.00156727
Iteration 3/25 | Loss: 0.00138906
Iteration 4/25 | Loss: 0.00135189
Iteration 5/25 | Loss: 0.00135009
Iteration 6/25 | Loss: 0.00134402
Iteration 7/25 | Loss: 0.00134346
Iteration 8/25 | Loss: 0.00134329
Iteration 9/25 | Loss: 0.00134329
Iteration 10/25 | Loss: 0.00134329
Iteration 11/25 | Loss: 0.00134329
Iteration 12/25 | Loss: 0.00134329
Iteration 13/25 | Loss: 0.00134329
Iteration 14/25 | Loss: 0.00134329
Iteration 15/25 | Loss: 0.00134329
Iteration 16/25 | Loss: 0.00134329
Iteration 17/25 | Loss: 0.00134329
Iteration 18/25 | Loss: 0.00134329
Iteration 19/25 | Loss: 0.00134329
Iteration 20/25 | Loss: 0.00134329
Iteration 21/25 | Loss: 0.00134329
Iteration 22/25 | Loss: 0.00134329
Iteration 23/25 | Loss: 0.00134329
Iteration 24/25 | Loss: 0.00134329
Iteration 25/25 | Loss: 0.00134329

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38677526
Iteration 2/25 | Loss: 0.00106609
Iteration 3/25 | Loss: 0.00101157
Iteration 4/25 | Loss: 0.00101157
Iteration 5/25 | Loss: 0.00101157
Iteration 6/25 | Loss: 0.00101157
Iteration 7/25 | Loss: 0.00101157
Iteration 8/25 | Loss: 0.00101157
Iteration 9/25 | Loss: 0.00101157
Iteration 10/25 | Loss: 0.00101157
Iteration 11/25 | Loss: 0.00101157
Iteration 12/25 | Loss: 0.00101157
Iteration 13/25 | Loss: 0.00101157
Iteration 14/25 | Loss: 0.00101157
Iteration 15/25 | Loss: 0.00101157
Iteration 16/25 | Loss: 0.00101157
Iteration 17/25 | Loss: 0.00101157
Iteration 18/25 | Loss: 0.00101157
Iteration 19/25 | Loss: 0.00101157
Iteration 20/25 | Loss: 0.00101157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010115689365193248, 0.0010115689365193248, 0.0010115689365193248, 0.0010115689365193248, 0.0010115689365193248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010115689365193248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101157
Iteration 2/1000 | Loss: 0.00006506
Iteration 3/1000 | Loss: 0.00002274
Iteration 4/1000 | Loss: 0.00002009
Iteration 5/1000 | Loss: 0.00001928
Iteration 6/1000 | Loss: 0.00007237
Iteration 7/1000 | Loss: 0.00001868
Iteration 8/1000 | Loss: 0.00001818
Iteration 9/1000 | Loss: 0.00001794
Iteration 10/1000 | Loss: 0.00001793
Iteration 11/1000 | Loss: 0.00001766
Iteration 12/1000 | Loss: 0.00001740
Iteration 13/1000 | Loss: 0.00001737
Iteration 14/1000 | Loss: 0.00001724
Iteration 15/1000 | Loss: 0.00001723
Iteration 16/1000 | Loss: 0.00001722
Iteration 17/1000 | Loss: 0.00001722
Iteration 18/1000 | Loss: 0.00001706
Iteration 19/1000 | Loss: 0.00001695
Iteration 20/1000 | Loss: 0.00008656
Iteration 21/1000 | Loss: 0.00003366
Iteration 22/1000 | Loss: 0.00001661
Iteration 23/1000 | Loss: 0.00001661
Iteration 24/1000 | Loss: 0.00001661
Iteration 25/1000 | Loss: 0.00001661
Iteration 26/1000 | Loss: 0.00001661
Iteration 27/1000 | Loss: 0.00001660
Iteration 28/1000 | Loss: 0.00001660
Iteration 29/1000 | Loss: 0.00001660
Iteration 30/1000 | Loss: 0.00001660
Iteration 31/1000 | Loss: 0.00001660
Iteration 32/1000 | Loss: 0.00001660
Iteration 33/1000 | Loss: 0.00001660
Iteration 34/1000 | Loss: 0.00001660
Iteration 35/1000 | Loss: 0.00001659
Iteration 36/1000 | Loss: 0.00001659
Iteration 37/1000 | Loss: 0.00001657
Iteration 38/1000 | Loss: 0.00001656
Iteration 39/1000 | Loss: 0.00001656
Iteration 40/1000 | Loss: 0.00001655
Iteration 41/1000 | Loss: 0.00001655
Iteration 42/1000 | Loss: 0.00001654
Iteration 43/1000 | Loss: 0.00001653
Iteration 44/1000 | Loss: 0.00001652
Iteration 45/1000 | Loss: 0.00001651
Iteration 46/1000 | Loss: 0.00001650
Iteration 47/1000 | Loss: 0.00001650
Iteration 48/1000 | Loss: 0.00001650
Iteration 49/1000 | Loss: 0.00001649
Iteration 50/1000 | Loss: 0.00001649
Iteration 51/1000 | Loss: 0.00001649
Iteration 52/1000 | Loss: 0.00001649
Iteration 53/1000 | Loss: 0.00001649
Iteration 54/1000 | Loss: 0.00001649
Iteration 55/1000 | Loss: 0.00001649
Iteration 56/1000 | Loss: 0.00001649
Iteration 57/1000 | Loss: 0.00001648
Iteration 58/1000 | Loss: 0.00001646
Iteration 59/1000 | Loss: 0.00001644
Iteration 60/1000 | Loss: 0.00001643
Iteration 61/1000 | Loss: 0.00001643
Iteration 62/1000 | Loss: 0.00001643
Iteration 63/1000 | Loss: 0.00001643
Iteration 64/1000 | Loss: 0.00001642
Iteration 65/1000 | Loss: 0.00007400
Iteration 66/1000 | Loss: 0.00001695
Iteration 67/1000 | Loss: 0.00001637
Iteration 68/1000 | Loss: 0.00001634
Iteration 69/1000 | Loss: 0.00001634
Iteration 70/1000 | Loss: 0.00001634
Iteration 71/1000 | Loss: 0.00001634
Iteration 72/1000 | Loss: 0.00001633
Iteration 73/1000 | Loss: 0.00001633
Iteration 74/1000 | Loss: 0.00001632
Iteration 75/1000 | Loss: 0.00001632
Iteration 76/1000 | Loss: 0.00001632
Iteration 77/1000 | Loss: 0.00001631
Iteration 78/1000 | Loss: 0.00001631
Iteration 79/1000 | Loss: 0.00001631
Iteration 80/1000 | Loss: 0.00005003
Iteration 81/1000 | Loss: 0.00001638
Iteration 82/1000 | Loss: 0.00001632
Iteration 83/1000 | Loss: 0.00001631
Iteration 84/1000 | Loss: 0.00001630
Iteration 85/1000 | Loss: 0.00001630
Iteration 86/1000 | Loss: 0.00001630
Iteration 87/1000 | Loss: 0.00001629
Iteration 88/1000 | Loss: 0.00001629
Iteration 89/1000 | Loss: 0.00001628
Iteration 90/1000 | Loss: 0.00001628
Iteration 91/1000 | Loss: 0.00001627
Iteration 92/1000 | Loss: 0.00001626
Iteration 93/1000 | Loss: 0.00001626
Iteration 94/1000 | Loss: 0.00001626
Iteration 95/1000 | Loss: 0.00001626
Iteration 96/1000 | Loss: 0.00001626
Iteration 97/1000 | Loss: 0.00001625
Iteration 98/1000 | Loss: 0.00001625
Iteration 99/1000 | Loss: 0.00001625
Iteration 100/1000 | Loss: 0.00001625
Iteration 101/1000 | Loss: 0.00001625
Iteration 102/1000 | Loss: 0.00001623
Iteration 103/1000 | Loss: 0.00001623
Iteration 104/1000 | Loss: 0.00001623
Iteration 105/1000 | Loss: 0.00001623
Iteration 106/1000 | Loss: 0.00001623
Iteration 107/1000 | Loss: 0.00001622
Iteration 108/1000 | Loss: 0.00001622
Iteration 109/1000 | Loss: 0.00001622
Iteration 110/1000 | Loss: 0.00001621
Iteration 111/1000 | Loss: 0.00001621
Iteration 112/1000 | Loss: 0.00001621
Iteration 113/1000 | Loss: 0.00001621
Iteration 114/1000 | Loss: 0.00001621
Iteration 115/1000 | Loss: 0.00001621
Iteration 116/1000 | Loss: 0.00001621
Iteration 117/1000 | Loss: 0.00001620
Iteration 118/1000 | Loss: 0.00001620
Iteration 119/1000 | Loss: 0.00001620
Iteration 120/1000 | Loss: 0.00001620
Iteration 121/1000 | Loss: 0.00001619
Iteration 122/1000 | Loss: 0.00001619
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001619
Iteration 125/1000 | Loss: 0.00001619
Iteration 126/1000 | Loss: 0.00001619
Iteration 127/1000 | Loss: 0.00001619
Iteration 128/1000 | Loss: 0.00001619
Iteration 129/1000 | Loss: 0.00001619
Iteration 130/1000 | Loss: 0.00001619
Iteration 131/1000 | Loss: 0.00001618
Iteration 132/1000 | Loss: 0.00001618
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001618
Iteration 135/1000 | Loss: 0.00001618
Iteration 136/1000 | Loss: 0.00001618
Iteration 137/1000 | Loss: 0.00001618
Iteration 138/1000 | Loss: 0.00001618
Iteration 139/1000 | Loss: 0.00001618
Iteration 140/1000 | Loss: 0.00001618
Iteration 141/1000 | Loss: 0.00001618
Iteration 142/1000 | Loss: 0.00001618
Iteration 143/1000 | Loss: 0.00001617
Iteration 144/1000 | Loss: 0.00001617
Iteration 145/1000 | Loss: 0.00001617
Iteration 146/1000 | Loss: 0.00001617
Iteration 147/1000 | Loss: 0.00001617
Iteration 148/1000 | Loss: 0.00001617
Iteration 149/1000 | Loss: 0.00001617
Iteration 150/1000 | Loss: 0.00001617
Iteration 151/1000 | Loss: 0.00001617
Iteration 152/1000 | Loss: 0.00001617
Iteration 153/1000 | Loss: 0.00001617
Iteration 154/1000 | Loss: 0.00001616
Iteration 155/1000 | Loss: 0.00001616
Iteration 156/1000 | Loss: 0.00001616
Iteration 157/1000 | Loss: 0.00001616
Iteration 158/1000 | Loss: 0.00001615
Iteration 159/1000 | Loss: 0.00001615
Iteration 160/1000 | Loss: 0.00001615
Iteration 161/1000 | Loss: 0.00001615
Iteration 162/1000 | Loss: 0.00001615
Iteration 163/1000 | Loss: 0.00001615
Iteration 164/1000 | Loss: 0.00001615
Iteration 165/1000 | Loss: 0.00001615
Iteration 166/1000 | Loss: 0.00001615
Iteration 167/1000 | Loss: 0.00001615
Iteration 168/1000 | Loss: 0.00001615
Iteration 169/1000 | Loss: 0.00001615
Iteration 170/1000 | Loss: 0.00001615
Iteration 171/1000 | Loss: 0.00001615
Iteration 172/1000 | Loss: 0.00001615
Iteration 173/1000 | Loss: 0.00001615
Iteration 174/1000 | Loss: 0.00001615
Iteration 175/1000 | Loss: 0.00001615
Iteration 176/1000 | Loss: 0.00001615
Iteration 177/1000 | Loss: 0.00001615
Iteration 178/1000 | Loss: 0.00001615
Iteration 179/1000 | Loss: 0.00001615
Iteration 180/1000 | Loss: 0.00001615
Iteration 181/1000 | Loss: 0.00001615
Iteration 182/1000 | Loss: 0.00001615
Iteration 183/1000 | Loss: 0.00001615
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.6146572306752205e-05, 1.6146572306752205e-05, 1.6146572306752205e-05, 1.6146572306752205e-05, 1.6146572306752205e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6146572306752205e-05

Optimization complete. Final v2v error: 3.435235023498535 mm

Highest mean error: 4.079834938049316 mm for frame 20

Lowest mean error: 3.2763559818267822 mm for frame 3

Saving results

Total time: 50.693225383758545
