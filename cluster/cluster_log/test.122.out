Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=122, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 6832-6887
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848156
Iteration 2/25 | Loss: 0.00257929
Iteration 3/25 | Loss: 0.00187567
Iteration 4/25 | Loss: 0.00176552
Iteration 5/25 | Loss: 0.00162287
Iteration 6/25 | Loss: 0.00154048
Iteration 7/25 | Loss: 0.00150714
Iteration 8/25 | Loss: 0.00148346
Iteration 9/25 | Loss: 0.00146599
Iteration 10/25 | Loss: 0.00145574
Iteration 11/25 | Loss: 0.00145324
Iteration 12/25 | Loss: 0.00145024
Iteration 13/25 | Loss: 0.00144987
Iteration 14/25 | Loss: 0.00144608
Iteration 15/25 | Loss: 0.00144425
Iteration 16/25 | Loss: 0.00144048
Iteration 17/25 | Loss: 0.00143972
Iteration 18/25 | Loss: 0.00143944
Iteration 19/25 | Loss: 0.00143933
Iteration 20/25 | Loss: 0.00143933
Iteration 21/25 | Loss: 0.00143933
Iteration 22/25 | Loss: 0.00143932
Iteration 23/25 | Loss: 0.00143932
Iteration 24/25 | Loss: 0.00143932
Iteration 25/25 | Loss: 0.00143932

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97263634
Iteration 2/25 | Loss: 0.00096950
Iteration 3/25 | Loss: 0.00096950
Iteration 4/25 | Loss: 0.00096950
Iteration 5/25 | Loss: 0.00096950
Iteration 6/25 | Loss: 0.00096950
Iteration 7/25 | Loss: 0.00096950
Iteration 8/25 | Loss: 0.00096950
Iteration 9/25 | Loss: 0.00096950
Iteration 10/25 | Loss: 0.00096950
Iteration 11/25 | Loss: 0.00096950
Iteration 12/25 | Loss: 0.00096950
Iteration 13/25 | Loss: 0.00096950
Iteration 14/25 | Loss: 0.00096950
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.000969498127233237, 0.000969498127233237, 0.000969498127233237, 0.000969498127233237, 0.000969498127233237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000969498127233237

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096950
Iteration 2/1000 | Loss: 0.00005246
Iteration 3/1000 | Loss: 0.00003337
Iteration 4/1000 | Loss: 0.00002995
Iteration 5/1000 | Loss: 0.00002877
Iteration 6/1000 | Loss: 0.00002770
Iteration 7/1000 | Loss: 0.00002696
Iteration 8/1000 | Loss: 0.00002653
Iteration 9/1000 | Loss: 0.00002607
Iteration 10/1000 | Loss: 0.00002580
Iteration 11/1000 | Loss: 0.00002559
Iteration 12/1000 | Loss: 0.00002541
Iteration 13/1000 | Loss: 0.00002524
Iteration 14/1000 | Loss: 0.00002514
Iteration 15/1000 | Loss: 0.00002508
Iteration 16/1000 | Loss: 0.00002508
Iteration 17/1000 | Loss: 0.00002507
Iteration 18/1000 | Loss: 0.00002507
Iteration 19/1000 | Loss: 0.00002506
Iteration 20/1000 | Loss: 0.00002506
Iteration 21/1000 | Loss: 0.00002505
Iteration 22/1000 | Loss: 0.00002504
Iteration 23/1000 | Loss: 0.00002502
Iteration 24/1000 | Loss: 0.00002501
Iteration 25/1000 | Loss: 0.00002493
Iteration 26/1000 | Loss: 0.00002490
Iteration 27/1000 | Loss: 0.00002490
Iteration 28/1000 | Loss: 0.00002490
Iteration 29/1000 | Loss: 0.00002490
Iteration 30/1000 | Loss: 0.00002490
Iteration 31/1000 | Loss: 0.00002490
Iteration 32/1000 | Loss: 0.00002490
Iteration 33/1000 | Loss: 0.00002490
Iteration 34/1000 | Loss: 0.00002490
Iteration 35/1000 | Loss: 0.00002490
Iteration 36/1000 | Loss: 0.00002490
Iteration 37/1000 | Loss: 0.00002490
Iteration 38/1000 | Loss: 0.00002490
Iteration 39/1000 | Loss: 0.00002490
Iteration 40/1000 | Loss: 0.00002490
Iteration 41/1000 | Loss: 0.00002490
Iteration 42/1000 | Loss: 0.00002490
Iteration 43/1000 | Loss: 0.00002490
Iteration 44/1000 | Loss: 0.00002490
Iteration 45/1000 | Loss: 0.00002490
Iteration 46/1000 | Loss: 0.00002490
Iteration 47/1000 | Loss: 0.00002490
Iteration 48/1000 | Loss: 0.00002490
Iteration 49/1000 | Loss: 0.00002490
Iteration 50/1000 | Loss: 0.00002490
Iteration 51/1000 | Loss: 0.00002490
Iteration 52/1000 | Loss: 0.00002490
Iteration 53/1000 | Loss: 0.00002490
Iteration 54/1000 | Loss: 0.00002490
Iteration 55/1000 | Loss: 0.00002490
Iteration 56/1000 | Loss: 0.00002490
Iteration 57/1000 | Loss: 0.00002490
Iteration 58/1000 | Loss: 0.00002490
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [2.4904371457523666e-05, 2.4904371457523666e-05, 2.4904371457523666e-05, 2.4904371457523666e-05, 2.4904371457523666e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4904371457523666e-05

Optimization complete. Final v2v error: 4.131808280944824 mm

Highest mean error: 5.046877384185791 mm for frame 141

Lowest mean error: 3.6422553062438965 mm for frame 63

Saving results

Total time: 63.3928656578064
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037925
Iteration 2/25 | Loss: 0.01037925
Iteration 3/25 | Loss: 0.00297055
Iteration 4/25 | Loss: 0.00240471
Iteration 5/25 | Loss: 0.00214101
Iteration 6/25 | Loss: 0.00230072
Iteration 7/25 | Loss: 0.00176617
Iteration 8/25 | Loss: 0.00155089
Iteration 9/25 | Loss: 0.00144925
Iteration 10/25 | Loss: 0.00135446
Iteration 11/25 | Loss: 0.00132087
Iteration 12/25 | Loss: 0.00129712
Iteration 13/25 | Loss: 0.00129473
Iteration 14/25 | Loss: 0.00129307
Iteration 15/25 | Loss: 0.00129115
Iteration 16/25 | Loss: 0.00130498
Iteration 17/25 | Loss: 0.00128931
Iteration 18/25 | Loss: 0.00128147
Iteration 19/25 | Loss: 0.00128026
Iteration 20/25 | Loss: 0.00128037
Iteration 21/25 | Loss: 0.00128047
Iteration 22/25 | Loss: 0.00128020
Iteration 23/25 | Loss: 0.00127749
Iteration 24/25 | Loss: 0.00128160
Iteration 25/25 | Loss: 0.00128262

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35343289
Iteration 2/25 | Loss: 0.00138488
Iteration 3/25 | Loss: 0.00111984
Iteration 4/25 | Loss: 0.00111984
Iteration 5/25 | Loss: 0.00111984
Iteration 6/25 | Loss: 0.00111984
Iteration 7/25 | Loss: 0.00111983
Iteration 8/25 | Loss: 0.00111983
Iteration 9/25 | Loss: 0.00111983
Iteration 10/25 | Loss: 0.00111983
Iteration 11/25 | Loss: 0.00111983
Iteration 12/25 | Loss: 0.00111983
Iteration 13/25 | Loss: 0.00111983
Iteration 14/25 | Loss: 0.00111983
Iteration 15/25 | Loss: 0.00111983
Iteration 16/25 | Loss: 0.00111983
Iteration 17/25 | Loss: 0.00111983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011198337888345122, 0.0011198337888345122, 0.0011198337888345122, 0.0011198337888345122, 0.0011198337888345122]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011198337888345122

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111983
Iteration 2/1000 | Loss: 0.00044892
Iteration 3/1000 | Loss: 0.00077674
Iteration 4/1000 | Loss: 0.00030024
Iteration 5/1000 | Loss: 0.00137841
Iteration 6/1000 | Loss: 0.00002581
Iteration 7/1000 | Loss: 0.00002399
Iteration 8/1000 | Loss: 0.00002236
Iteration 9/1000 | Loss: 0.00002148
Iteration 10/1000 | Loss: 0.00002089
Iteration 11/1000 | Loss: 0.00002037
Iteration 12/1000 | Loss: 0.00001981
Iteration 13/1000 | Loss: 0.00043418
Iteration 14/1000 | Loss: 0.00002128
Iteration 15/1000 | Loss: 0.00001950
Iteration 16/1000 | Loss: 0.00001843
Iteration 17/1000 | Loss: 0.00001770
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001714
Iteration 20/1000 | Loss: 0.00001695
Iteration 21/1000 | Loss: 0.00001695
Iteration 22/1000 | Loss: 0.00001679
Iteration 23/1000 | Loss: 0.00001673
Iteration 24/1000 | Loss: 0.00001672
Iteration 25/1000 | Loss: 0.00001671
Iteration 26/1000 | Loss: 0.00001659
Iteration 27/1000 | Loss: 0.00001642
Iteration 28/1000 | Loss: 0.00001640
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001637
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001630
Iteration 33/1000 | Loss: 0.00001629
Iteration 34/1000 | Loss: 0.00001628
Iteration 35/1000 | Loss: 0.00001628
Iteration 36/1000 | Loss: 0.00001627
Iteration 37/1000 | Loss: 0.00001625
Iteration 38/1000 | Loss: 0.00001625
Iteration 39/1000 | Loss: 0.00001625
Iteration 40/1000 | Loss: 0.00001625
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001624
Iteration 43/1000 | Loss: 0.00001624
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001624
Iteration 46/1000 | Loss: 0.00001624
Iteration 47/1000 | Loss: 0.00001624
Iteration 48/1000 | Loss: 0.00001624
Iteration 49/1000 | Loss: 0.00001623
Iteration 50/1000 | Loss: 0.00001623
Iteration 51/1000 | Loss: 0.00001620
Iteration 52/1000 | Loss: 0.00001620
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001617
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001616
Iteration 64/1000 | Loss: 0.00001616
Iteration 65/1000 | Loss: 0.00001615
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001614
Iteration 69/1000 | Loss: 0.00001614
Iteration 70/1000 | Loss: 0.00001614
Iteration 71/1000 | Loss: 0.00001614
Iteration 72/1000 | Loss: 0.00001614
Iteration 73/1000 | Loss: 0.00001614
Iteration 74/1000 | Loss: 0.00001614
Iteration 75/1000 | Loss: 0.00001614
Iteration 76/1000 | Loss: 0.00001613
Iteration 77/1000 | Loss: 0.00001613
Iteration 78/1000 | Loss: 0.00001613
Iteration 79/1000 | Loss: 0.00001613
Iteration 80/1000 | Loss: 0.00001613
Iteration 81/1000 | Loss: 0.00001613
Iteration 82/1000 | Loss: 0.00001613
Iteration 83/1000 | Loss: 0.00001613
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001613
Iteration 87/1000 | Loss: 0.00001613
Iteration 88/1000 | Loss: 0.00001613
Iteration 89/1000 | Loss: 0.00001613
Iteration 90/1000 | Loss: 0.00001613
Iteration 91/1000 | Loss: 0.00001613
Iteration 92/1000 | Loss: 0.00001613
Iteration 93/1000 | Loss: 0.00001613
Iteration 94/1000 | Loss: 0.00001612
Iteration 95/1000 | Loss: 0.00001612
Iteration 96/1000 | Loss: 0.00001612
Iteration 97/1000 | Loss: 0.00001612
Iteration 98/1000 | Loss: 0.00001612
Iteration 99/1000 | Loss: 0.00001612
Iteration 100/1000 | Loss: 0.00001612
Iteration 101/1000 | Loss: 0.00001612
Iteration 102/1000 | Loss: 0.00001611
Iteration 103/1000 | Loss: 0.00001611
Iteration 104/1000 | Loss: 0.00001611
Iteration 105/1000 | Loss: 0.00001611
Iteration 106/1000 | Loss: 0.00001611
Iteration 107/1000 | Loss: 0.00001611
Iteration 108/1000 | Loss: 0.00001611
Iteration 109/1000 | Loss: 0.00001611
Iteration 110/1000 | Loss: 0.00001611
Iteration 111/1000 | Loss: 0.00001611
Iteration 112/1000 | Loss: 0.00001611
Iteration 113/1000 | Loss: 0.00001611
Iteration 114/1000 | Loss: 0.00001610
Iteration 115/1000 | Loss: 0.00001610
Iteration 116/1000 | Loss: 0.00001610
Iteration 117/1000 | Loss: 0.00001610
Iteration 118/1000 | Loss: 0.00001609
Iteration 119/1000 | Loss: 0.00001609
Iteration 120/1000 | Loss: 0.00001609
Iteration 121/1000 | Loss: 0.00001609
Iteration 122/1000 | Loss: 0.00001609
Iteration 123/1000 | Loss: 0.00001609
Iteration 124/1000 | Loss: 0.00001609
Iteration 125/1000 | Loss: 0.00001609
Iteration 126/1000 | Loss: 0.00001609
Iteration 127/1000 | Loss: 0.00001609
Iteration 128/1000 | Loss: 0.00001609
Iteration 129/1000 | Loss: 0.00001609
Iteration 130/1000 | Loss: 0.00001609
Iteration 131/1000 | Loss: 0.00001609
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.609038736205548e-05, 1.609038736205548e-05, 1.609038736205548e-05, 1.609038736205548e-05, 1.609038736205548e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.609038736205548e-05

Optimization complete. Final v2v error: 3.4020791053771973 mm

Highest mean error: 4.333561897277832 mm for frame 144

Lowest mean error: 3.2399325370788574 mm for frame 66

Saving results

Total time: 89.88927292823792
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00954760
Iteration 2/25 | Loss: 0.00954760
Iteration 3/25 | Loss: 0.00954760
Iteration 4/25 | Loss: 0.00954759
Iteration 5/25 | Loss: 0.00954759
Iteration 6/25 | Loss: 0.00954759
Iteration 7/25 | Loss: 0.00954759
Iteration 8/25 | Loss: 0.00954759
Iteration 9/25 | Loss: 0.00954759
Iteration 10/25 | Loss: 0.00954758
Iteration 11/25 | Loss: 0.00954758
Iteration 12/25 | Loss: 0.00954758
Iteration 13/25 | Loss: 0.00954758
Iteration 14/25 | Loss: 0.00954758
Iteration 15/25 | Loss: 0.00954758
Iteration 16/25 | Loss: 0.00954758
Iteration 17/25 | Loss: 0.00954757
Iteration 18/25 | Loss: 0.00954757
Iteration 19/25 | Loss: 0.00954757
Iteration 20/25 | Loss: 0.00954757
Iteration 21/25 | Loss: 0.00954757
Iteration 22/25 | Loss: 0.00954756
Iteration 23/25 | Loss: 0.00954756
Iteration 24/25 | Loss: 0.00954756
Iteration 25/25 | Loss: 0.00954756

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69462407
Iteration 2/25 | Loss: 0.20259732
Iteration 3/25 | Loss: 0.20252350
Iteration 4/25 | Loss: 0.20252341
Iteration 5/25 | Loss: 0.20252341
Iteration 6/25 | Loss: 0.20252341
Iteration 7/25 | Loss: 0.20252341
Iteration 8/25 | Loss: 0.20252341
Iteration 9/25 | Loss: 0.20252341
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 9. Stopping optimization.
Last 5 losses: [0.20252341032028198, 0.20252341032028198, 0.20252341032028198, 0.20252341032028198, 0.20252341032028198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.20252341032028198

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.20252341
Iteration 2/1000 | Loss: 0.00282707
Iteration 3/1000 | Loss: 0.00103091
Iteration 4/1000 | Loss: 0.00059747
Iteration 5/1000 | Loss: 0.00031274
Iteration 6/1000 | Loss: 0.00020383
Iteration 7/1000 | Loss: 0.00062527
Iteration 8/1000 | Loss: 0.00013401
Iteration 9/1000 | Loss: 0.00021989
Iteration 10/1000 | Loss: 0.00007047
Iteration 11/1000 | Loss: 0.00013742
Iteration 12/1000 | Loss: 0.00006478
Iteration 13/1000 | Loss: 0.00012925
Iteration 14/1000 | Loss: 0.00004228
Iteration 15/1000 | Loss: 0.00012078
Iteration 16/1000 | Loss: 0.00006300
Iteration 17/1000 | Loss: 0.00005246
Iteration 18/1000 | Loss: 0.00003877
Iteration 19/1000 | Loss: 0.00003000
Iteration 20/1000 | Loss: 0.00002755
Iteration 21/1000 | Loss: 0.00002547
Iteration 22/1000 | Loss: 0.00002439
Iteration 23/1000 | Loss: 0.00002328
Iteration 24/1000 | Loss: 0.00002265
Iteration 25/1000 | Loss: 0.00002221
Iteration 26/1000 | Loss: 0.00002177
Iteration 27/1000 | Loss: 0.00002141
Iteration 28/1000 | Loss: 0.00002112
Iteration 29/1000 | Loss: 0.00002075
Iteration 30/1000 | Loss: 0.00002040
Iteration 31/1000 | Loss: 0.00002017
Iteration 32/1000 | Loss: 0.00001996
Iteration 33/1000 | Loss: 0.00001995
Iteration 34/1000 | Loss: 0.00001978
Iteration 35/1000 | Loss: 0.00001976
Iteration 36/1000 | Loss: 0.00001975
Iteration 37/1000 | Loss: 0.00001974
Iteration 38/1000 | Loss: 0.00001973
Iteration 39/1000 | Loss: 0.00001967
Iteration 40/1000 | Loss: 0.00001965
Iteration 41/1000 | Loss: 0.00001965
Iteration 42/1000 | Loss: 0.00001963
Iteration 43/1000 | Loss: 0.00001962
Iteration 44/1000 | Loss: 0.00001961
Iteration 45/1000 | Loss: 0.00001961
Iteration 46/1000 | Loss: 0.00001961
Iteration 47/1000 | Loss: 0.00001961
Iteration 48/1000 | Loss: 0.00001959
Iteration 49/1000 | Loss: 0.00001959
Iteration 50/1000 | Loss: 0.00001958
Iteration 51/1000 | Loss: 0.00001958
Iteration 52/1000 | Loss: 0.00001958
Iteration 53/1000 | Loss: 0.00001958
Iteration 54/1000 | Loss: 0.00001958
Iteration 55/1000 | Loss: 0.00001958
Iteration 56/1000 | Loss: 0.00001957
Iteration 57/1000 | Loss: 0.00001957
Iteration 58/1000 | Loss: 0.00001957
Iteration 59/1000 | Loss: 0.00001956
Iteration 60/1000 | Loss: 0.00001956
Iteration 61/1000 | Loss: 0.00001955
Iteration 62/1000 | Loss: 0.00001955
Iteration 63/1000 | Loss: 0.00001954
Iteration 64/1000 | Loss: 0.00001954
Iteration 65/1000 | Loss: 0.00001953
Iteration 66/1000 | Loss: 0.00001952
Iteration 67/1000 | Loss: 0.00001952
Iteration 68/1000 | Loss: 0.00001951
Iteration 69/1000 | Loss: 0.00001951
Iteration 70/1000 | Loss: 0.00001951
Iteration 71/1000 | Loss: 0.00001951
Iteration 72/1000 | Loss: 0.00001951
Iteration 73/1000 | Loss: 0.00001951
Iteration 74/1000 | Loss: 0.00001951
Iteration 75/1000 | Loss: 0.00001951
Iteration 76/1000 | Loss: 0.00001951
Iteration 77/1000 | Loss: 0.00001951
Iteration 78/1000 | Loss: 0.00001951
Iteration 79/1000 | Loss: 0.00001951
Iteration 80/1000 | Loss: 0.00001950
Iteration 81/1000 | Loss: 0.00001950
Iteration 82/1000 | Loss: 0.00001950
Iteration 83/1000 | Loss: 0.00001950
Iteration 84/1000 | Loss: 0.00001950
Iteration 85/1000 | Loss: 0.00001950
Iteration 86/1000 | Loss: 0.00001950
Iteration 87/1000 | Loss: 0.00001949
Iteration 88/1000 | Loss: 0.00001949
Iteration 89/1000 | Loss: 0.00001949
Iteration 90/1000 | Loss: 0.00001948
Iteration 91/1000 | Loss: 0.00001948
Iteration 92/1000 | Loss: 0.00001948
Iteration 93/1000 | Loss: 0.00001948
Iteration 94/1000 | Loss: 0.00001948
Iteration 95/1000 | Loss: 0.00001948
Iteration 96/1000 | Loss: 0.00001948
Iteration 97/1000 | Loss: 0.00001948
Iteration 98/1000 | Loss: 0.00001948
Iteration 99/1000 | Loss: 0.00001948
Iteration 100/1000 | Loss: 0.00001947
Iteration 101/1000 | Loss: 0.00001947
Iteration 102/1000 | Loss: 0.00001947
Iteration 103/1000 | Loss: 0.00001946
Iteration 104/1000 | Loss: 0.00001946
Iteration 105/1000 | Loss: 0.00001946
Iteration 106/1000 | Loss: 0.00001946
Iteration 107/1000 | Loss: 0.00001946
Iteration 108/1000 | Loss: 0.00001946
Iteration 109/1000 | Loss: 0.00001945
Iteration 110/1000 | Loss: 0.00001945
Iteration 111/1000 | Loss: 0.00001945
Iteration 112/1000 | Loss: 0.00001945
Iteration 113/1000 | Loss: 0.00001944
Iteration 114/1000 | Loss: 0.00001944
Iteration 115/1000 | Loss: 0.00001943
Iteration 116/1000 | Loss: 0.00001943
Iteration 117/1000 | Loss: 0.00001943
Iteration 118/1000 | Loss: 0.00001943
Iteration 119/1000 | Loss: 0.00001943
Iteration 120/1000 | Loss: 0.00001943
Iteration 121/1000 | Loss: 0.00001942
Iteration 122/1000 | Loss: 0.00001942
Iteration 123/1000 | Loss: 0.00001942
Iteration 124/1000 | Loss: 0.00001942
Iteration 125/1000 | Loss: 0.00001942
Iteration 126/1000 | Loss: 0.00001942
Iteration 127/1000 | Loss: 0.00001942
Iteration 128/1000 | Loss: 0.00001942
Iteration 129/1000 | Loss: 0.00001942
Iteration 130/1000 | Loss: 0.00001942
Iteration 131/1000 | Loss: 0.00001941
Iteration 132/1000 | Loss: 0.00001941
Iteration 133/1000 | Loss: 0.00001941
Iteration 134/1000 | Loss: 0.00001941
Iteration 135/1000 | Loss: 0.00001941
Iteration 136/1000 | Loss: 0.00001940
Iteration 137/1000 | Loss: 0.00001940
Iteration 138/1000 | Loss: 0.00001940
Iteration 139/1000 | Loss: 0.00001940
Iteration 140/1000 | Loss: 0.00001940
Iteration 141/1000 | Loss: 0.00001940
Iteration 142/1000 | Loss: 0.00001940
Iteration 143/1000 | Loss: 0.00001940
Iteration 144/1000 | Loss: 0.00001940
Iteration 145/1000 | Loss: 0.00001940
Iteration 146/1000 | Loss: 0.00001940
Iteration 147/1000 | Loss: 0.00001940
Iteration 148/1000 | Loss: 0.00001939
Iteration 149/1000 | Loss: 0.00001939
Iteration 150/1000 | Loss: 0.00001939
Iteration 151/1000 | Loss: 0.00001939
Iteration 152/1000 | Loss: 0.00001939
Iteration 153/1000 | Loss: 0.00001939
Iteration 154/1000 | Loss: 0.00001939
Iteration 155/1000 | Loss: 0.00001939
Iteration 156/1000 | Loss: 0.00001939
Iteration 157/1000 | Loss: 0.00001939
Iteration 158/1000 | Loss: 0.00001938
Iteration 159/1000 | Loss: 0.00001938
Iteration 160/1000 | Loss: 0.00001938
Iteration 161/1000 | Loss: 0.00001938
Iteration 162/1000 | Loss: 0.00001938
Iteration 163/1000 | Loss: 0.00001938
Iteration 164/1000 | Loss: 0.00001938
Iteration 165/1000 | Loss: 0.00001938
Iteration 166/1000 | Loss: 0.00001938
Iteration 167/1000 | Loss: 0.00001938
Iteration 168/1000 | Loss: 0.00001938
Iteration 169/1000 | Loss: 0.00001938
Iteration 170/1000 | Loss: 0.00001938
Iteration 171/1000 | Loss: 0.00001938
Iteration 172/1000 | Loss: 0.00001938
Iteration 173/1000 | Loss: 0.00001938
Iteration 174/1000 | Loss: 0.00001938
Iteration 175/1000 | Loss: 0.00001938
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.937945853569545e-05, 1.937945853569545e-05, 1.937945853569545e-05, 1.937945853569545e-05, 1.937945853569545e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.937945853569545e-05

Optimization complete. Final v2v error: 3.7926595211029053 mm

Highest mean error: 3.9804723262786865 mm for frame 21

Lowest mean error: 3.554441452026367 mm for frame 123

Saving results

Total time: 71.2832498550415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00429593
Iteration 2/25 | Loss: 0.00135213
Iteration 3/25 | Loss: 0.00129168
Iteration 4/25 | Loss: 0.00127764
Iteration 5/25 | Loss: 0.00127398
Iteration 6/25 | Loss: 0.00127398
Iteration 7/25 | Loss: 0.00127398
Iteration 8/25 | Loss: 0.00127398
Iteration 9/25 | Loss: 0.00127398
Iteration 10/25 | Loss: 0.00127398
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012739793164655566, 0.0012739793164655566, 0.0012739793164655566, 0.0012739793164655566, 0.0012739793164655566]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012739793164655566

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40482402
Iteration 2/25 | Loss: 0.00075687
Iteration 3/25 | Loss: 0.00075686
Iteration 4/25 | Loss: 0.00075686
Iteration 5/25 | Loss: 0.00075686
Iteration 6/25 | Loss: 0.00075686
Iteration 7/25 | Loss: 0.00075686
Iteration 8/25 | Loss: 0.00075686
Iteration 9/25 | Loss: 0.00075686
Iteration 10/25 | Loss: 0.00075686
Iteration 11/25 | Loss: 0.00075686
Iteration 12/25 | Loss: 0.00075686
Iteration 13/25 | Loss: 0.00075686
Iteration 14/25 | Loss: 0.00075686
Iteration 15/25 | Loss: 0.00075686
Iteration 16/25 | Loss: 0.00075686
Iteration 17/25 | Loss: 0.00075686
Iteration 18/25 | Loss: 0.00075686
Iteration 19/25 | Loss: 0.00075686
Iteration 20/25 | Loss: 0.00075686
Iteration 21/25 | Loss: 0.00075686
Iteration 22/25 | Loss: 0.00075686
Iteration 23/25 | Loss: 0.00075686
Iteration 24/25 | Loss: 0.00075686
Iteration 25/25 | Loss: 0.00075686

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075686
Iteration 2/1000 | Loss: 0.00002491
Iteration 3/1000 | Loss: 0.00002025
Iteration 4/1000 | Loss: 0.00001921
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00001772
Iteration 7/1000 | Loss: 0.00001732
Iteration 8/1000 | Loss: 0.00001692
Iteration 9/1000 | Loss: 0.00001671
Iteration 10/1000 | Loss: 0.00001651
Iteration 11/1000 | Loss: 0.00001645
Iteration 12/1000 | Loss: 0.00001627
Iteration 13/1000 | Loss: 0.00001619
Iteration 14/1000 | Loss: 0.00001616
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001609
Iteration 17/1000 | Loss: 0.00001608
Iteration 18/1000 | Loss: 0.00001606
Iteration 19/1000 | Loss: 0.00001606
Iteration 20/1000 | Loss: 0.00001605
Iteration 21/1000 | Loss: 0.00001602
Iteration 22/1000 | Loss: 0.00001601
Iteration 23/1000 | Loss: 0.00001600
Iteration 24/1000 | Loss: 0.00001600
Iteration 25/1000 | Loss: 0.00001600
Iteration 26/1000 | Loss: 0.00001599
Iteration 27/1000 | Loss: 0.00001598
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001593
Iteration 30/1000 | Loss: 0.00001591
Iteration 31/1000 | Loss: 0.00001588
Iteration 32/1000 | Loss: 0.00001586
Iteration 33/1000 | Loss: 0.00001585
Iteration 34/1000 | Loss: 0.00001585
Iteration 35/1000 | Loss: 0.00001584
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001583
Iteration 38/1000 | Loss: 0.00001582
Iteration 39/1000 | Loss: 0.00001582
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001578
Iteration 42/1000 | Loss: 0.00001577
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001576
Iteration 45/1000 | Loss: 0.00001575
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001569
Iteration 49/1000 | Loss: 0.00001565
Iteration 50/1000 | Loss: 0.00001565
Iteration 51/1000 | Loss: 0.00001565
Iteration 52/1000 | Loss: 0.00001561
Iteration 53/1000 | Loss: 0.00001561
Iteration 54/1000 | Loss: 0.00001560
Iteration 55/1000 | Loss: 0.00001559
Iteration 56/1000 | Loss: 0.00001559
Iteration 57/1000 | Loss: 0.00001559
Iteration 58/1000 | Loss: 0.00001558
Iteration 59/1000 | Loss: 0.00001558
Iteration 60/1000 | Loss: 0.00001558
Iteration 61/1000 | Loss: 0.00001557
Iteration 62/1000 | Loss: 0.00001557
Iteration 63/1000 | Loss: 0.00001556
Iteration 64/1000 | Loss: 0.00001556
Iteration 65/1000 | Loss: 0.00001556
Iteration 66/1000 | Loss: 0.00001556
Iteration 67/1000 | Loss: 0.00001556
Iteration 68/1000 | Loss: 0.00001556
Iteration 69/1000 | Loss: 0.00001556
Iteration 70/1000 | Loss: 0.00001556
Iteration 71/1000 | Loss: 0.00001556
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001556
Iteration 74/1000 | Loss: 0.00001556
Iteration 75/1000 | Loss: 0.00001555
Iteration 76/1000 | Loss: 0.00001555
Iteration 77/1000 | Loss: 0.00001555
Iteration 78/1000 | Loss: 0.00001555
Iteration 79/1000 | Loss: 0.00001555
Iteration 80/1000 | Loss: 0.00001555
Iteration 81/1000 | Loss: 0.00001554
Iteration 82/1000 | Loss: 0.00001554
Iteration 83/1000 | Loss: 0.00001553
Iteration 84/1000 | Loss: 0.00001553
Iteration 85/1000 | Loss: 0.00001553
Iteration 86/1000 | Loss: 0.00001552
Iteration 87/1000 | Loss: 0.00001552
Iteration 88/1000 | Loss: 0.00001552
Iteration 89/1000 | Loss: 0.00001552
Iteration 90/1000 | Loss: 0.00001552
Iteration 91/1000 | Loss: 0.00001552
Iteration 92/1000 | Loss: 0.00001552
Iteration 93/1000 | Loss: 0.00001551
Iteration 94/1000 | Loss: 0.00001551
Iteration 95/1000 | Loss: 0.00001551
Iteration 96/1000 | Loss: 0.00001551
Iteration 97/1000 | Loss: 0.00001551
Iteration 98/1000 | Loss: 0.00001551
Iteration 99/1000 | Loss: 0.00001551
Iteration 100/1000 | Loss: 0.00001551
Iteration 101/1000 | Loss: 0.00001551
Iteration 102/1000 | Loss: 0.00001551
Iteration 103/1000 | Loss: 0.00001550
Iteration 104/1000 | Loss: 0.00001550
Iteration 105/1000 | Loss: 0.00001550
Iteration 106/1000 | Loss: 0.00001550
Iteration 107/1000 | Loss: 0.00001550
Iteration 108/1000 | Loss: 0.00001550
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001549
Iteration 111/1000 | Loss: 0.00001549
Iteration 112/1000 | Loss: 0.00001549
Iteration 113/1000 | Loss: 0.00001549
Iteration 114/1000 | Loss: 0.00001549
Iteration 115/1000 | Loss: 0.00001549
Iteration 116/1000 | Loss: 0.00001549
Iteration 117/1000 | Loss: 0.00001549
Iteration 118/1000 | Loss: 0.00001549
Iteration 119/1000 | Loss: 0.00001549
Iteration 120/1000 | Loss: 0.00001549
Iteration 121/1000 | Loss: 0.00001549
Iteration 122/1000 | Loss: 0.00001549
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001548
Iteration 125/1000 | Loss: 0.00001548
Iteration 126/1000 | Loss: 0.00001548
Iteration 127/1000 | Loss: 0.00001548
Iteration 128/1000 | Loss: 0.00001548
Iteration 129/1000 | Loss: 0.00001548
Iteration 130/1000 | Loss: 0.00001548
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.5484698451473378e-05, 1.5484698451473378e-05, 1.5484698451473378e-05, 1.5484698451473378e-05, 1.5484698451473378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5484698451473378e-05

Optimization complete. Final v2v error: 3.353882074356079 mm

Highest mean error: 3.441000461578369 mm for frame 161

Lowest mean error: 3.273454189300537 mm for frame 41

Saving results

Total time: 41.292819023132324
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00422125
Iteration 2/25 | Loss: 0.00147379
Iteration 3/25 | Loss: 0.00131641
Iteration 4/25 | Loss: 0.00129862
Iteration 5/25 | Loss: 0.00129415
Iteration 6/25 | Loss: 0.00129351
Iteration 7/25 | Loss: 0.00129351
Iteration 8/25 | Loss: 0.00129351
Iteration 9/25 | Loss: 0.00129351
Iteration 10/25 | Loss: 0.00129351
Iteration 11/25 | Loss: 0.00129351
Iteration 12/25 | Loss: 0.00129351
Iteration 13/25 | Loss: 0.00129351
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0012935141567140818, 0.0012935141567140818, 0.0012935141567140818, 0.0012935141567140818, 0.0012935141567140818]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012935141567140818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39971972
Iteration 2/25 | Loss: 0.00075769
Iteration 3/25 | Loss: 0.00075769
Iteration 4/25 | Loss: 0.00075769
Iteration 5/25 | Loss: 0.00075769
Iteration 6/25 | Loss: 0.00075769
Iteration 7/25 | Loss: 0.00075769
Iteration 8/25 | Loss: 0.00075769
Iteration 9/25 | Loss: 0.00075769
Iteration 10/25 | Loss: 0.00075769
Iteration 11/25 | Loss: 0.00075769
Iteration 12/25 | Loss: 0.00075769
Iteration 13/25 | Loss: 0.00075769
Iteration 14/25 | Loss: 0.00075769
Iteration 15/25 | Loss: 0.00075769
Iteration 16/25 | Loss: 0.00075769
Iteration 17/25 | Loss: 0.00075769
Iteration 18/25 | Loss: 0.00075769
Iteration 19/25 | Loss: 0.00075769
Iteration 20/25 | Loss: 0.00075769
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007576860371045768, 0.0007576860371045768, 0.0007576860371045768, 0.0007576860371045768, 0.0007576860371045768]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007576860371045768

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075769
Iteration 2/1000 | Loss: 0.00003474
Iteration 3/1000 | Loss: 0.00002304
Iteration 4/1000 | Loss: 0.00002000
Iteration 5/1000 | Loss: 0.00001911
Iteration 6/1000 | Loss: 0.00001826
Iteration 7/1000 | Loss: 0.00001772
Iteration 8/1000 | Loss: 0.00001733
Iteration 9/1000 | Loss: 0.00001704
Iteration 10/1000 | Loss: 0.00001703
Iteration 11/1000 | Loss: 0.00001672
Iteration 12/1000 | Loss: 0.00001651
Iteration 13/1000 | Loss: 0.00001637
Iteration 14/1000 | Loss: 0.00001620
Iteration 15/1000 | Loss: 0.00001611
Iteration 16/1000 | Loss: 0.00001608
Iteration 17/1000 | Loss: 0.00001608
Iteration 18/1000 | Loss: 0.00001607
Iteration 19/1000 | Loss: 0.00001606
Iteration 20/1000 | Loss: 0.00001603
Iteration 21/1000 | Loss: 0.00001598
Iteration 22/1000 | Loss: 0.00001597
Iteration 23/1000 | Loss: 0.00001595
Iteration 24/1000 | Loss: 0.00001594
Iteration 25/1000 | Loss: 0.00001592
Iteration 26/1000 | Loss: 0.00001592
Iteration 27/1000 | Loss: 0.00001592
Iteration 28/1000 | Loss: 0.00001592
Iteration 29/1000 | Loss: 0.00001592
Iteration 30/1000 | Loss: 0.00001592
Iteration 31/1000 | Loss: 0.00001591
Iteration 32/1000 | Loss: 0.00001590
Iteration 33/1000 | Loss: 0.00001590
Iteration 34/1000 | Loss: 0.00001589
Iteration 35/1000 | Loss: 0.00001589
Iteration 36/1000 | Loss: 0.00001588
Iteration 37/1000 | Loss: 0.00001587
Iteration 38/1000 | Loss: 0.00001586
Iteration 39/1000 | Loss: 0.00001585
Iteration 40/1000 | Loss: 0.00001585
Iteration 41/1000 | Loss: 0.00001582
Iteration 42/1000 | Loss: 0.00001581
Iteration 43/1000 | Loss: 0.00001580
Iteration 44/1000 | Loss: 0.00001579
Iteration 45/1000 | Loss: 0.00001576
Iteration 46/1000 | Loss: 0.00001576
Iteration 47/1000 | Loss: 0.00001576
Iteration 48/1000 | Loss: 0.00001575
Iteration 49/1000 | Loss: 0.00001574
Iteration 50/1000 | Loss: 0.00001574
Iteration 51/1000 | Loss: 0.00001574
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001573
Iteration 54/1000 | Loss: 0.00001573
Iteration 55/1000 | Loss: 0.00001573
Iteration 56/1000 | Loss: 0.00001573
Iteration 57/1000 | Loss: 0.00001572
Iteration 58/1000 | Loss: 0.00001572
Iteration 59/1000 | Loss: 0.00001572
Iteration 60/1000 | Loss: 0.00001571
Iteration 61/1000 | Loss: 0.00001571
Iteration 62/1000 | Loss: 0.00001570
Iteration 63/1000 | Loss: 0.00001570
Iteration 64/1000 | Loss: 0.00001570
Iteration 65/1000 | Loss: 0.00001570
Iteration 66/1000 | Loss: 0.00001569
Iteration 67/1000 | Loss: 0.00001569
Iteration 68/1000 | Loss: 0.00001569
Iteration 69/1000 | Loss: 0.00001569
Iteration 70/1000 | Loss: 0.00001569
Iteration 71/1000 | Loss: 0.00001569
Iteration 72/1000 | Loss: 0.00001569
Iteration 73/1000 | Loss: 0.00001568
Iteration 74/1000 | Loss: 0.00001568
Iteration 75/1000 | Loss: 0.00001568
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001568
Iteration 79/1000 | Loss: 0.00001568
Iteration 80/1000 | Loss: 0.00001568
Iteration 81/1000 | Loss: 0.00001567
Iteration 82/1000 | Loss: 0.00001567
Iteration 83/1000 | Loss: 0.00001567
Iteration 84/1000 | Loss: 0.00001567
Iteration 85/1000 | Loss: 0.00001567
Iteration 86/1000 | Loss: 0.00001567
Iteration 87/1000 | Loss: 0.00001567
Iteration 88/1000 | Loss: 0.00001567
Iteration 89/1000 | Loss: 0.00001567
Iteration 90/1000 | Loss: 0.00001567
Iteration 91/1000 | Loss: 0.00001566
Iteration 92/1000 | Loss: 0.00001566
Iteration 93/1000 | Loss: 0.00001566
Iteration 94/1000 | Loss: 0.00001566
Iteration 95/1000 | Loss: 0.00001566
Iteration 96/1000 | Loss: 0.00001566
Iteration 97/1000 | Loss: 0.00001566
Iteration 98/1000 | Loss: 0.00001566
Iteration 99/1000 | Loss: 0.00001566
Iteration 100/1000 | Loss: 0.00001566
Iteration 101/1000 | Loss: 0.00001566
Iteration 102/1000 | Loss: 0.00001566
Iteration 103/1000 | Loss: 0.00001565
Iteration 104/1000 | Loss: 0.00001565
Iteration 105/1000 | Loss: 0.00001565
Iteration 106/1000 | Loss: 0.00001565
Iteration 107/1000 | Loss: 0.00001565
Iteration 108/1000 | Loss: 0.00001564
Iteration 109/1000 | Loss: 0.00001564
Iteration 110/1000 | Loss: 0.00001564
Iteration 111/1000 | Loss: 0.00001564
Iteration 112/1000 | Loss: 0.00001564
Iteration 113/1000 | Loss: 0.00001564
Iteration 114/1000 | Loss: 0.00001564
Iteration 115/1000 | Loss: 0.00001564
Iteration 116/1000 | Loss: 0.00001563
Iteration 117/1000 | Loss: 0.00001563
Iteration 118/1000 | Loss: 0.00001563
Iteration 119/1000 | Loss: 0.00001563
Iteration 120/1000 | Loss: 0.00001562
Iteration 121/1000 | Loss: 0.00001562
Iteration 122/1000 | Loss: 0.00001562
Iteration 123/1000 | Loss: 0.00001562
Iteration 124/1000 | Loss: 0.00001562
Iteration 125/1000 | Loss: 0.00001562
Iteration 126/1000 | Loss: 0.00001562
Iteration 127/1000 | Loss: 0.00001562
Iteration 128/1000 | Loss: 0.00001562
Iteration 129/1000 | Loss: 0.00001562
Iteration 130/1000 | Loss: 0.00001562
Iteration 131/1000 | Loss: 0.00001562
Iteration 132/1000 | Loss: 0.00001562
Iteration 133/1000 | Loss: 0.00001562
Iteration 134/1000 | Loss: 0.00001561
Iteration 135/1000 | Loss: 0.00001561
Iteration 136/1000 | Loss: 0.00001561
Iteration 137/1000 | Loss: 0.00001561
Iteration 138/1000 | Loss: 0.00001560
Iteration 139/1000 | Loss: 0.00001560
Iteration 140/1000 | Loss: 0.00001560
Iteration 141/1000 | Loss: 0.00001560
Iteration 142/1000 | Loss: 0.00001560
Iteration 143/1000 | Loss: 0.00001560
Iteration 144/1000 | Loss: 0.00001560
Iteration 145/1000 | Loss: 0.00001560
Iteration 146/1000 | Loss: 0.00001560
Iteration 147/1000 | Loss: 0.00001559
Iteration 148/1000 | Loss: 0.00001559
Iteration 149/1000 | Loss: 0.00001559
Iteration 150/1000 | Loss: 0.00001559
Iteration 151/1000 | Loss: 0.00001558
Iteration 152/1000 | Loss: 0.00001558
Iteration 153/1000 | Loss: 0.00001557
Iteration 154/1000 | Loss: 0.00001557
Iteration 155/1000 | Loss: 0.00001557
Iteration 156/1000 | Loss: 0.00001557
Iteration 157/1000 | Loss: 0.00001556
Iteration 158/1000 | Loss: 0.00001556
Iteration 159/1000 | Loss: 0.00001556
Iteration 160/1000 | Loss: 0.00001556
Iteration 161/1000 | Loss: 0.00001556
Iteration 162/1000 | Loss: 0.00001556
Iteration 163/1000 | Loss: 0.00001556
Iteration 164/1000 | Loss: 0.00001556
Iteration 165/1000 | Loss: 0.00001555
Iteration 166/1000 | Loss: 0.00001555
Iteration 167/1000 | Loss: 0.00001555
Iteration 168/1000 | Loss: 0.00001555
Iteration 169/1000 | Loss: 0.00001555
Iteration 170/1000 | Loss: 0.00001555
Iteration 171/1000 | Loss: 0.00001555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.55543748405762e-05, 1.55543748405762e-05, 1.55543748405762e-05, 1.55543748405762e-05, 1.55543748405762e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.55543748405762e-05

Optimization complete. Final v2v error: 3.3465065956115723 mm

Highest mean error: 4.007086277008057 mm for frame 4

Lowest mean error: 3.089482307434082 mm for frame 195

Saving results

Total time: 47.44051504135132
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00907561
Iteration 2/25 | Loss: 0.00150053
Iteration 3/25 | Loss: 0.00132785
Iteration 4/25 | Loss: 0.00130932
Iteration 5/25 | Loss: 0.00130395
Iteration 6/25 | Loss: 0.00130359
Iteration 7/25 | Loss: 0.00130359
Iteration 8/25 | Loss: 0.00130359
Iteration 9/25 | Loss: 0.00130359
Iteration 10/25 | Loss: 0.00130359
Iteration 11/25 | Loss: 0.00130359
Iteration 12/25 | Loss: 0.00130359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013035897864028811, 0.0013035897864028811, 0.0013035897864028811, 0.0013035897864028811, 0.0013035897864028811]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013035897864028811

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.52021694
Iteration 2/25 | Loss: 0.00089738
Iteration 3/25 | Loss: 0.00089738
Iteration 4/25 | Loss: 0.00089738
Iteration 5/25 | Loss: 0.00089738
Iteration 6/25 | Loss: 0.00089738
Iteration 7/25 | Loss: 0.00089738
Iteration 8/25 | Loss: 0.00089738
Iteration 9/25 | Loss: 0.00089738
Iteration 10/25 | Loss: 0.00089738
Iteration 11/25 | Loss: 0.00089738
Iteration 12/25 | Loss: 0.00089738
Iteration 13/25 | Loss: 0.00089738
Iteration 14/25 | Loss: 0.00089738
Iteration 15/25 | Loss: 0.00089738
Iteration 16/25 | Loss: 0.00089738
Iteration 17/25 | Loss: 0.00089738
Iteration 18/25 | Loss: 0.00089738
Iteration 19/25 | Loss: 0.00089738
Iteration 20/25 | Loss: 0.00089738
Iteration 21/25 | Loss: 0.00089738
Iteration 22/25 | Loss: 0.00089738
Iteration 23/25 | Loss: 0.00089738
Iteration 24/25 | Loss: 0.00089738
Iteration 25/25 | Loss: 0.00089738

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00089738
Iteration 2/1000 | Loss: 0.00003098
Iteration 3/1000 | Loss: 0.00002244
Iteration 4/1000 | Loss: 0.00002065
Iteration 5/1000 | Loss: 0.00001958
Iteration 6/1000 | Loss: 0.00001865
Iteration 7/1000 | Loss: 0.00001806
Iteration 8/1000 | Loss: 0.00001768
Iteration 9/1000 | Loss: 0.00001740
Iteration 10/1000 | Loss: 0.00001707
Iteration 11/1000 | Loss: 0.00001691
Iteration 12/1000 | Loss: 0.00001685
Iteration 13/1000 | Loss: 0.00001671
Iteration 14/1000 | Loss: 0.00001660
Iteration 15/1000 | Loss: 0.00001655
Iteration 16/1000 | Loss: 0.00001655
Iteration 17/1000 | Loss: 0.00001653
Iteration 18/1000 | Loss: 0.00001650
Iteration 19/1000 | Loss: 0.00001650
Iteration 20/1000 | Loss: 0.00001649
Iteration 21/1000 | Loss: 0.00001647
Iteration 22/1000 | Loss: 0.00001647
Iteration 23/1000 | Loss: 0.00001642
Iteration 24/1000 | Loss: 0.00001638
Iteration 25/1000 | Loss: 0.00001637
Iteration 26/1000 | Loss: 0.00001636
Iteration 27/1000 | Loss: 0.00001635
Iteration 28/1000 | Loss: 0.00001635
Iteration 29/1000 | Loss: 0.00001634
Iteration 30/1000 | Loss: 0.00001634
Iteration 31/1000 | Loss: 0.00001632
Iteration 32/1000 | Loss: 0.00001632
Iteration 33/1000 | Loss: 0.00001631
Iteration 34/1000 | Loss: 0.00001631
Iteration 35/1000 | Loss: 0.00001631
Iteration 36/1000 | Loss: 0.00001630
Iteration 37/1000 | Loss: 0.00001629
Iteration 38/1000 | Loss: 0.00001629
Iteration 39/1000 | Loss: 0.00001628
Iteration 40/1000 | Loss: 0.00001628
Iteration 41/1000 | Loss: 0.00001628
Iteration 42/1000 | Loss: 0.00001627
Iteration 43/1000 | Loss: 0.00001627
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001626
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001625
Iteration 48/1000 | Loss: 0.00001625
Iteration 49/1000 | Loss: 0.00001624
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001624
Iteration 52/1000 | Loss: 0.00001624
Iteration 53/1000 | Loss: 0.00001623
Iteration 54/1000 | Loss: 0.00001623
Iteration 55/1000 | Loss: 0.00001623
Iteration 56/1000 | Loss: 0.00001623
Iteration 57/1000 | Loss: 0.00001623
Iteration 58/1000 | Loss: 0.00001622
Iteration 59/1000 | Loss: 0.00001622
Iteration 60/1000 | Loss: 0.00001622
Iteration 61/1000 | Loss: 0.00001621
Iteration 62/1000 | Loss: 0.00001621
Iteration 63/1000 | Loss: 0.00001620
Iteration 64/1000 | Loss: 0.00001620
Iteration 65/1000 | Loss: 0.00001619
Iteration 66/1000 | Loss: 0.00001619
Iteration 67/1000 | Loss: 0.00001619
Iteration 68/1000 | Loss: 0.00001618
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001617
Iteration 72/1000 | Loss: 0.00001617
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001616
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001616
Iteration 77/1000 | Loss: 0.00001615
Iteration 78/1000 | Loss: 0.00001615
Iteration 79/1000 | Loss: 0.00001615
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001614
Iteration 83/1000 | Loss: 0.00001614
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00001611
Iteration 92/1000 | Loss: 0.00001611
Iteration 93/1000 | Loss: 0.00001610
Iteration 94/1000 | Loss: 0.00001610
Iteration 95/1000 | Loss: 0.00001610
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001609
Iteration 100/1000 | Loss: 0.00001609
Iteration 101/1000 | Loss: 0.00001609
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001608
Iteration 104/1000 | Loss: 0.00001608
Iteration 105/1000 | Loss: 0.00001608
Iteration 106/1000 | Loss: 0.00001607
Iteration 107/1000 | Loss: 0.00001607
Iteration 108/1000 | Loss: 0.00001607
Iteration 109/1000 | Loss: 0.00001607
Iteration 110/1000 | Loss: 0.00001606
Iteration 111/1000 | Loss: 0.00001606
Iteration 112/1000 | Loss: 0.00001606
Iteration 113/1000 | Loss: 0.00001605
Iteration 114/1000 | Loss: 0.00001605
Iteration 115/1000 | Loss: 0.00001604
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001604
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001604
Iteration 120/1000 | Loss: 0.00001604
Iteration 121/1000 | Loss: 0.00001604
Iteration 122/1000 | Loss: 0.00001603
Iteration 123/1000 | Loss: 0.00001603
Iteration 124/1000 | Loss: 0.00001603
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001602
Iteration 127/1000 | Loss: 0.00001602
Iteration 128/1000 | Loss: 0.00001602
Iteration 129/1000 | Loss: 0.00001602
Iteration 130/1000 | Loss: 0.00001602
Iteration 131/1000 | Loss: 0.00001602
Iteration 132/1000 | Loss: 0.00001602
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.6024096112232655e-05, 1.6024096112232655e-05, 1.6024096112232655e-05, 1.6024096112232655e-05, 1.6024096112232655e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6024096112232655e-05

Optimization complete. Final v2v error: 3.371152639389038 mm

Highest mean error: 3.9245612621307373 mm for frame 1

Lowest mean error: 2.905168294906616 mm for frame 201

Saving results

Total time: 41.36319851875305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413394
Iteration 2/25 | Loss: 0.00143082
Iteration 3/25 | Loss: 0.00131432
Iteration 4/25 | Loss: 0.00129693
Iteration 5/25 | Loss: 0.00129187
Iteration 6/25 | Loss: 0.00129053
Iteration 7/25 | Loss: 0.00129053
Iteration 8/25 | Loss: 0.00129053
Iteration 9/25 | Loss: 0.00129053
Iteration 10/25 | Loss: 0.00129053
Iteration 11/25 | Loss: 0.00129053
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012905346229672432, 0.0012905346229672432, 0.0012905346229672432, 0.0012905346229672432, 0.0012905346229672432]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012905346229672432

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36912620
Iteration 2/25 | Loss: 0.00094659
Iteration 3/25 | Loss: 0.00094657
Iteration 4/25 | Loss: 0.00094657
Iteration 5/25 | Loss: 0.00094657
Iteration 6/25 | Loss: 0.00094657
Iteration 7/25 | Loss: 0.00094657
Iteration 8/25 | Loss: 0.00094657
Iteration 9/25 | Loss: 0.00094656
Iteration 10/25 | Loss: 0.00094656
Iteration 11/25 | Loss: 0.00094656
Iteration 12/25 | Loss: 0.00094656
Iteration 13/25 | Loss: 0.00094656
Iteration 14/25 | Loss: 0.00094656
Iteration 15/25 | Loss: 0.00094656
Iteration 16/25 | Loss: 0.00094656
Iteration 17/25 | Loss: 0.00094656
Iteration 18/25 | Loss: 0.00094656
Iteration 19/25 | Loss: 0.00094656
Iteration 20/25 | Loss: 0.00094656
Iteration 21/25 | Loss: 0.00094656
Iteration 22/25 | Loss: 0.00094656
Iteration 23/25 | Loss: 0.00094656
Iteration 24/25 | Loss: 0.00094656
Iteration 25/25 | Loss: 0.00094656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094656
Iteration 2/1000 | Loss: 0.00004432
Iteration 3/1000 | Loss: 0.00002991
Iteration 4/1000 | Loss: 0.00002452
Iteration 5/1000 | Loss: 0.00002255
Iteration 6/1000 | Loss: 0.00002101
Iteration 7/1000 | Loss: 0.00002004
Iteration 8/1000 | Loss: 0.00001947
Iteration 9/1000 | Loss: 0.00001885
Iteration 10/1000 | Loss: 0.00001850
Iteration 11/1000 | Loss: 0.00001827
Iteration 12/1000 | Loss: 0.00001799
Iteration 13/1000 | Loss: 0.00001784
Iteration 14/1000 | Loss: 0.00001779
Iteration 15/1000 | Loss: 0.00001776
Iteration 16/1000 | Loss: 0.00001775
Iteration 17/1000 | Loss: 0.00001775
Iteration 18/1000 | Loss: 0.00001773
Iteration 19/1000 | Loss: 0.00001772
Iteration 20/1000 | Loss: 0.00001768
Iteration 21/1000 | Loss: 0.00001767
Iteration 22/1000 | Loss: 0.00001767
Iteration 23/1000 | Loss: 0.00001767
Iteration 24/1000 | Loss: 0.00001766
Iteration 25/1000 | Loss: 0.00001766
Iteration 26/1000 | Loss: 0.00001766
Iteration 27/1000 | Loss: 0.00001765
Iteration 28/1000 | Loss: 0.00001765
Iteration 29/1000 | Loss: 0.00001764
Iteration 30/1000 | Loss: 0.00001764
Iteration 31/1000 | Loss: 0.00001763
Iteration 32/1000 | Loss: 0.00001763
Iteration 33/1000 | Loss: 0.00001762
Iteration 34/1000 | Loss: 0.00001760
Iteration 35/1000 | Loss: 0.00001760
Iteration 36/1000 | Loss: 0.00001759
Iteration 37/1000 | Loss: 0.00001758
Iteration 38/1000 | Loss: 0.00001757
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001751
Iteration 41/1000 | Loss: 0.00001747
Iteration 42/1000 | Loss: 0.00001747
Iteration 43/1000 | Loss: 0.00001746
Iteration 44/1000 | Loss: 0.00001746
Iteration 45/1000 | Loss: 0.00001742
Iteration 46/1000 | Loss: 0.00001742
Iteration 47/1000 | Loss: 0.00001741
Iteration 48/1000 | Loss: 0.00001741
Iteration 49/1000 | Loss: 0.00001740
Iteration 50/1000 | Loss: 0.00001740
Iteration 51/1000 | Loss: 0.00001740
Iteration 52/1000 | Loss: 0.00001739
Iteration 53/1000 | Loss: 0.00001738
Iteration 54/1000 | Loss: 0.00001738
Iteration 55/1000 | Loss: 0.00001737
Iteration 56/1000 | Loss: 0.00001737
Iteration 57/1000 | Loss: 0.00001736
Iteration 58/1000 | Loss: 0.00001736
Iteration 59/1000 | Loss: 0.00001736
Iteration 60/1000 | Loss: 0.00001735
Iteration 61/1000 | Loss: 0.00001735
Iteration 62/1000 | Loss: 0.00001734
Iteration 63/1000 | Loss: 0.00001734
Iteration 64/1000 | Loss: 0.00001733
Iteration 65/1000 | Loss: 0.00001733
Iteration 66/1000 | Loss: 0.00001733
Iteration 67/1000 | Loss: 0.00001733
Iteration 68/1000 | Loss: 0.00001732
Iteration 69/1000 | Loss: 0.00001732
Iteration 70/1000 | Loss: 0.00001732
Iteration 71/1000 | Loss: 0.00001731
Iteration 72/1000 | Loss: 0.00001731
Iteration 73/1000 | Loss: 0.00001731
Iteration 74/1000 | Loss: 0.00001730
Iteration 75/1000 | Loss: 0.00001730
Iteration 76/1000 | Loss: 0.00001730
Iteration 77/1000 | Loss: 0.00001730
Iteration 78/1000 | Loss: 0.00001730
Iteration 79/1000 | Loss: 0.00001729
Iteration 80/1000 | Loss: 0.00001729
Iteration 81/1000 | Loss: 0.00001729
Iteration 82/1000 | Loss: 0.00001729
Iteration 83/1000 | Loss: 0.00001729
Iteration 84/1000 | Loss: 0.00001728
Iteration 85/1000 | Loss: 0.00001728
Iteration 86/1000 | Loss: 0.00001728
Iteration 87/1000 | Loss: 0.00001728
Iteration 88/1000 | Loss: 0.00001727
Iteration 89/1000 | Loss: 0.00001727
Iteration 90/1000 | Loss: 0.00001727
Iteration 91/1000 | Loss: 0.00001727
Iteration 92/1000 | Loss: 0.00001726
Iteration 93/1000 | Loss: 0.00001726
Iteration 94/1000 | Loss: 0.00001726
Iteration 95/1000 | Loss: 0.00001726
Iteration 96/1000 | Loss: 0.00001725
Iteration 97/1000 | Loss: 0.00001725
Iteration 98/1000 | Loss: 0.00001725
Iteration 99/1000 | Loss: 0.00001725
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00001724
Iteration 103/1000 | Loss: 0.00001724
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001723
Iteration 109/1000 | Loss: 0.00001723
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001722
Iteration 113/1000 | Loss: 0.00001722
Iteration 114/1000 | Loss: 0.00001722
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001722
Iteration 117/1000 | Loss: 0.00001722
Iteration 118/1000 | Loss: 0.00001722
Iteration 119/1000 | Loss: 0.00001722
Iteration 120/1000 | Loss: 0.00001722
Iteration 121/1000 | Loss: 0.00001722
Iteration 122/1000 | Loss: 0.00001721
Iteration 123/1000 | Loss: 0.00001721
Iteration 124/1000 | Loss: 0.00001721
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001721
Iteration 131/1000 | Loss: 0.00001721
Iteration 132/1000 | Loss: 0.00001721
Iteration 133/1000 | Loss: 0.00001721
Iteration 134/1000 | Loss: 0.00001721
Iteration 135/1000 | Loss: 0.00001721
Iteration 136/1000 | Loss: 0.00001721
Iteration 137/1000 | Loss: 0.00001721
Iteration 138/1000 | Loss: 0.00001721
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001720
Iteration 141/1000 | Loss: 0.00001720
Iteration 142/1000 | Loss: 0.00001720
Iteration 143/1000 | Loss: 0.00001720
Iteration 144/1000 | Loss: 0.00001720
Iteration 145/1000 | Loss: 0.00001720
Iteration 146/1000 | Loss: 0.00001720
Iteration 147/1000 | Loss: 0.00001720
Iteration 148/1000 | Loss: 0.00001720
Iteration 149/1000 | Loss: 0.00001719
Iteration 150/1000 | Loss: 0.00001719
Iteration 151/1000 | Loss: 0.00001719
Iteration 152/1000 | Loss: 0.00001719
Iteration 153/1000 | Loss: 0.00001719
Iteration 154/1000 | Loss: 0.00001719
Iteration 155/1000 | Loss: 0.00001719
Iteration 156/1000 | Loss: 0.00001719
Iteration 157/1000 | Loss: 0.00001719
Iteration 158/1000 | Loss: 0.00001719
Iteration 159/1000 | Loss: 0.00001719
Iteration 160/1000 | Loss: 0.00001719
Iteration 161/1000 | Loss: 0.00001719
Iteration 162/1000 | Loss: 0.00001719
Iteration 163/1000 | Loss: 0.00001719
Iteration 164/1000 | Loss: 0.00001718
Iteration 165/1000 | Loss: 0.00001718
Iteration 166/1000 | Loss: 0.00001718
Iteration 167/1000 | Loss: 0.00001718
Iteration 168/1000 | Loss: 0.00001718
Iteration 169/1000 | Loss: 0.00001718
Iteration 170/1000 | Loss: 0.00001718
Iteration 171/1000 | Loss: 0.00001718
Iteration 172/1000 | Loss: 0.00001718
Iteration 173/1000 | Loss: 0.00001718
Iteration 174/1000 | Loss: 0.00001718
Iteration 175/1000 | Loss: 0.00001718
Iteration 176/1000 | Loss: 0.00001718
Iteration 177/1000 | Loss: 0.00001718
Iteration 178/1000 | Loss: 0.00001717
Iteration 179/1000 | Loss: 0.00001717
Iteration 180/1000 | Loss: 0.00001717
Iteration 181/1000 | Loss: 0.00001717
Iteration 182/1000 | Loss: 0.00001717
Iteration 183/1000 | Loss: 0.00001716
Iteration 184/1000 | Loss: 0.00001716
Iteration 185/1000 | Loss: 0.00001716
Iteration 186/1000 | Loss: 0.00001715
Iteration 187/1000 | Loss: 0.00001715
Iteration 188/1000 | Loss: 0.00001715
Iteration 189/1000 | Loss: 0.00001714
Iteration 190/1000 | Loss: 0.00001714
Iteration 191/1000 | Loss: 0.00001714
Iteration 192/1000 | Loss: 0.00001714
Iteration 193/1000 | Loss: 0.00001714
Iteration 194/1000 | Loss: 0.00001714
Iteration 195/1000 | Loss: 0.00001713
Iteration 196/1000 | Loss: 0.00001713
Iteration 197/1000 | Loss: 0.00001713
Iteration 198/1000 | Loss: 0.00001712
Iteration 199/1000 | Loss: 0.00001712
Iteration 200/1000 | Loss: 0.00001712
Iteration 201/1000 | Loss: 0.00001711
Iteration 202/1000 | Loss: 0.00001711
Iteration 203/1000 | Loss: 0.00001711
Iteration 204/1000 | Loss: 0.00001711
Iteration 205/1000 | Loss: 0.00001710
Iteration 206/1000 | Loss: 0.00001710
Iteration 207/1000 | Loss: 0.00001710
Iteration 208/1000 | Loss: 0.00001710
Iteration 209/1000 | Loss: 0.00001710
Iteration 210/1000 | Loss: 0.00001710
Iteration 211/1000 | Loss: 0.00001710
Iteration 212/1000 | Loss: 0.00001710
Iteration 213/1000 | Loss: 0.00001709
Iteration 214/1000 | Loss: 0.00001709
Iteration 215/1000 | Loss: 0.00001709
Iteration 216/1000 | Loss: 0.00001709
Iteration 217/1000 | Loss: 0.00001709
Iteration 218/1000 | Loss: 0.00001709
Iteration 219/1000 | Loss: 0.00001709
Iteration 220/1000 | Loss: 0.00001709
Iteration 221/1000 | Loss: 0.00001708
Iteration 222/1000 | Loss: 0.00001708
Iteration 223/1000 | Loss: 0.00001708
Iteration 224/1000 | Loss: 0.00001708
Iteration 225/1000 | Loss: 0.00001708
Iteration 226/1000 | Loss: 0.00001708
Iteration 227/1000 | Loss: 0.00001708
Iteration 228/1000 | Loss: 0.00001708
Iteration 229/1000 | Loss: 0.00001708
Iteration 230/1000 | Loss: 0.00001708
Iteration 231/1000 | Loss: 0.00001707
Iteration 232/1000 | Loss: 0.00001707
Iteration 233/1000 | Loss: 0.00001707
Iteration 234/1000 | Loss: 0.00001707
Iteration 235/1000 | Loss: 0.00001707
Iteration 236/1000 | Loss: 0.00001707
Iteration 237/1000 | Loss: 0.00001707
Iteration 238/1000 | Loss: 0.00001706
Iteration 239/1000 | Loss: 0.00001706
Iteration 240/1000 | Loss: 0.00001706
Iteration 241/1000 | Loss: 0.00001706
Iteration 242/1000 | Loss: 0.00001706
Iteration 243/1000 | Loss: 0.00001706
Iteration 244/1000 | Loss: 0.00001706
Iteration 245/1000 | Loss: 0.00001706
Iteration 246/1000 | Loss: 0.00001706
Iteration 247/1000 | Loss: 0.00001706
Iteration 248/1000 | Loss: 0.00001706
Iteration 249/1000 | Loss: 0.00001705
Iteration 250/1000 | Loss: 0.00001705
Iteration 251/1000 | Loss: 0.00001705
Iteration 252/1000 | Loss: 0.00001705
Iteration 253/1000 | Loss: 0.00001705
Iteration 254/1000 | Loss: 0.00001705
Iteration 255/1000 | Loss: 0.00001705
Iteration 256/1000 | Loss: 0.00001705
Iteration 257/1000 | Loss: 0.00001704
Iteration 258/1000 | Loss: 0.00001704
Iteration 259/1000 | Loss: 0.00001704
Iteration 260/1000 | Loss: 0.00001704
Iteration 261/1000 | Loss: 0.00001704
Iteration 262/1000 | Loss: 0.00001704
Iteration 263/1000 | Loss: 0.00001704
Iteration 264/1000 | Loss: 0.00001704
Iteration 265/1000 | Loss: 0.00001704
Iteration 266/1000 | Loss: 0.00001704
Iteration 267/1000 | Loss: 0.00001704
Iteration 268/1000 | Loss: 0.00001704
Iteration 269/1000 | Loss: 0.00001704
Iteration 270/1000 | Loss: 0.00001704
Iteration 271/1000 | Loss: 0.00001704
Iteration 272/1000 | Loss: 0.00001704
Iteration 273/1000 | Loss: 0.00001704
Iteration 274/1000 | Loss: 0.00001704
Iteration 275/1000 | Loss: 0.00001704
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 275. Stopping optimization.
Last 5 losses: [1.7039412341546267e-05, 1.7039412341546267e-05, 1.7039412341546267e-05, 1.7039412341546267e-05, 1.7039412341546267e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7039412341546267e-05

Optimization complete. Final v2v error: 3.3900115489959717 mm

Highest mean error: 5.412839412689209 mm for frame 74

Lowest mean error: 2.830613136291504 mm for frame 145

Saving results

Total time: 47.31826376914978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00970256
Iteration 2/25 | Loss: 0.00222801
Iteration 3/25 | Loss: 0.00173877
Iteration 4/25 | Loss: 0.00162562
Iteration 5/25 | Loss: 0.00161204
Iteration 6/25 | Loss: 0.00159440
Iteration 7/25 | Loss: 0.00164171
Iteration 8/25 | Loss: 0.00168141
Iteration 9/25 | Loss: 0.00157459
Iteration 10/25 | Loss: 0.00144835
Iteration 11/25 | Loss: 0.00140645
Iteration 12/25 | Loss: 0.00138441
Iteration 13/25 | Loss: 0.00138498
Iteration 14/25 | Loss: 0.00138535
Iteration 15/25 | Loss: 0.00137633
Iteration 16/25 | Loss: 0.00137261
Iteration 17/25 | Loss: 0.00136867
Iteration 18/25 | Loss: 0.00136761
Iteration 19/25 | Loss: 0.00136206
Iteration 20/25 | Loss: 0.00135695
Iteration 21/25 | Loss: 0.00135875
Iteration 22/25 | Loss: 0.00135260
Iteration 23/25 | Loss: 0.00135281
Iteration 24/25 | Loss: 0.00135634
Iteration 25/25 | Loss: 0.00135526

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42620850
Iteration 2/25 | Loss: 0.00101469
Iteration 3/25 | Loss: 0.00096857
Iteration 4/25 | Loss: 0.00096857
Iteration 5/25 | Loss: 0.00096857
Iteration 6/25 | Loss: 0.00096857
Iteration 7/25 | Loss: 0.00096857
Iteration 8/25 | Loss: 0.00096857
Iteration 9/25 | Loss: 0.00096857
Iteration 10/25 | Loss: 0.00096857
Iteration 11/25 | Loss: 0.00096857
Iteration 12/25 | Loss: 0.00096857
Iteration 13/25 | Loss: 0.00096857
Iteration 14/25 | Loss: 0.00096857
Iteration 15/25 | Loss: 0.00096857
Iteration 16/25 | Loss: 0.00096857
Iteration 17/25 | Loss: 0.00096857
Iteration 18/25 | Loss: 0.00096857
Iteration 19/25 | Loss: 0.00096857
Iteration 20/25 | Loss: 0.00096857
Iteration 21/25 | Loss: 0.00096857
Iteration 22/25 | Loss: 0.00096857
Iteration 23/25 | Loss: 0.00096857
Iteration 24/25 | Loss: 0.00096857
Iteration 25/25 | Loss: 0.00096857

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096857
Iteration 2/1000 | Loss: 0.00008687
Iteration 3/1000 | Loss: 0.00004561
Iteration 4/1000 | Loss: 0.00018568
Iteration 5/1000 | Loss: 0.00002666
Iteration 6/1000 | Loss: 0.00002463
Iteration 7/1000 | Loss: 0.00007944
Iteration 8/1000 | Loss: 0.00005780
Iteration 9/1000 | Loss: 0.00019118
Iteration 10/1000 | Loss: 0.00005202
Iteration 11/1000 | Loss: 0.00002197
Iteration 12/1000 | Loss: 0.00002142
Iteration 13/1000 | Loss: 0.00002104
Iteration 14/1000 | Loss: 0.00002075
Iteration 15/1000 | Loss: 0.00002053
Iteration 16/1000 | Loss: 0.00002036
Iteration 17/1000 | Loss: 0.00002022
Iteration 18/1000 | Loss: 0.00003474
Iteration 19/1000 | Loss: 0.00001992
Iteration 20/1000 | Loss: 0.00002941
Iteration 21/1000 | Loss: 0.00002878
Iteration 22/1000 | Loss: 0.00001971
Iteration 23/1000 | Loss: 0.00001971
Iteration 24/1000 | Loss: 0.00001970
Iteration 25/1000 | Loss: 0.00001964
Iteration 26/1000 | Loss: 0.00001960
Iteration 27/1000 | Loss: 0.00001959
Iteration 28/1000 | Loss: 0.00001958
Iteration 29/1000 | Loss: 0.00002309
Iteration 30/1000 | Loss: 0.00001943
Iteration 31/1000 | Loss: 0.00001943
Iteration 32/1000 | Loss: 0.00001943
Iteration 33/1000 | Loss: 0.00001943
Iteration 34/1000 | Loss: 0.00001943
Iteration 35/1000 | Loss: 0.00001943
Iteration 36/1000 | Loss: 0.00001943
Iteration 37/1000 | Loss: 0.00001942
Iteration 38/1000 | Loss: 0.00001942
Iteration 39/1000 | Loss: 0.00001942
Iteration 40/1000 | Loss: 0.00001942
Iteration 41/1000 | Loss: 0.00001942
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001938
Iteration 45/1000 | Loss: 0.00002741
Iteration 46/1000 | Loss: 0.00001931
Iteration 47/1000 | Loss: 0.00002797
Iteration 48/1000 | Loss: 0.00001924
Iteration 49/1000 | Loss: 0.00001924
Iteration 50/1000 | Loss: 0.00001923
Iteration 51/1000 | Loss: 0.00001923
Iteration 52/1000 | Loss: 0.00001922
Iteration 53/1000 | Loss: 0.00001922
Iteration 54/1000 | Loss: 0.00001922
Iteration 55/1000 | Loss: 0.00001922
Iteration 56/1000 | Loss: 0.00001921
Iteration 57/1000 | Loss: 0.00001921
Iteration 58/1000 | Loss: 0.00001921
Iteration 59/1000 | Loss: 0.00001921
Iteration 60/1000 | Loss: 0.00001921
Iteration 61/1000 | Loss: 0.00001920
Iteration 62/1000 | Loss: 0.00001920
Iteration 63/1000 | Loss: 0.00001920
Iteration 64/1000 | Loss: 0.00001919
Iteration 65/1000 | Loss: 0.00001919
Iteration 66/1000 | Loss: 0.00001919
Iteration 67/1000 | Loss: 0.00001918
Iteration 68/1000 | Loss: 0.00001918
Iteration 69/1000 | Loss: 0.00001918
Iteration 70/1000 | Loss: 0.00001918
Iteration 71/1000 | Loss: 0.00001918
Iteration 72/1000 | Loss: 0.00001918
Iteration 73/1000 | Loss: 0.00001917
Iteration 74/1000 | Loss: 0.00001917
Iteration 75/1000 | Loss: 0.00001917
Iteration 76/1000 | Loss: 0.00001917
Iteration 77/1000 | Loss: 0.00001917
Iteration 78/1000 | Loss: 0.00001916
Iteration 79/1000 | Loss: 0.00001916
Iteration 80/1000 | Loss: 0.00001916
Iteration 81/1000 | Loss: 0.00001916
Iteration 82/1000 | Loss: 0.00001916
Iteration 83/1000 | Loss: 0.00001915
Iteration 84/1000 | Loss: 0.00001915
Iteration 85/1000 | Loss: 0.00001915
Iteration 86/1000 | Loss: 0.00001914
Iteration 87/1000 | Loss: 0.00001914
Iteration 88/1000 | Loss: 0.00001914
Iteration 89/1000 | Loss: 0.00001914
Iteration 90/1000 | Loss: 0.00001914
Iteration 91/1000 | Loss: 0.00001913
Iteration 92/1000 | Loss: 0.00001913
Iteration 93/1000 | Loss: 0.00001913
Iteration 94/1000 | Loss: 0.00001913
Iteration 95/1000 | Loss: 0.00001913
Iteration 96/1000 | Loss: 0.00001913
Iteration 97/1000 | Loss: 0.00001913
Iteration 98/1000 | Loss: 0.00001913
Iteration 99/1000 | Loss: 0.00001912
Iteration 100/1000 | Loss: 0.00001912
Iteration 101/1000 | Loss: 0.00001912
Iteration 102/1000 | Loss: 0.00001912
Iteration 103/1000 | Loss: 0.00001912
Iteration 104/1000 | Loss: 0.00001912
Iteration 105/1000 | Loss: 0.00001912
Iteration 106/1000 | Loss: 0.00001912
Iteration 107/1000 | Loss: 0.00001911
Iteration 108/1000 | Loss: 0.00001911
Iteration 109/1000 | Loss: 0.00001911
Iteration 110/1000 | Loss: 0.00001911
Iteration 111/1000 | Loss: 0.00001911
Iteration 112/1000 | Loss: 0.00001910
Iteration 113/1000 | Loss: 0.00001910
Iteration 114/1000 | Loss: 0.00001910
Iteration 115/1000 | Loss: 0.00001910
Iteration 116/1000 | Loss: 0.00001910
Iteration 117/1000 | Loss: 0.00001910
Iteration 118/1000 | Loss: 0.00001910
Iteration 119/1000 | Loss: 0.00001910
Iteration 120/1000 | Loss: 0.00001910
Iteration 121/1000 | Loss: 0.00001910
Iteration 122/1000 | Loss: 0.00001910
Iteration 123/1000 | Loss: 0.00001910
Iteration 124/1000 | Loss: 0.00001910
Iteration 125/1000 | Loss: 0.00001909
Iteration 126/1000 | Loss: 0.00001909
Iteration 127/1000 | Loss: 0.00001909
Iteration 128/1000 | Loss: 0.00001909
Iteration 129/1000 | Loss: 0.00001909
Iteration 130/1000 | Loss: 0.00001909
Iteration 131/1000 | Loss: 0.00001909
Iteration 132/1000 | Loss: 0.00001908
Iteration 133/1000 | Loss: 0.00001908
Iteration 134/1000 | Loss: 0.00001908
Iteration 135/1000 | Loss: 0.00001908
Iteration 136/1000 | Loss: 0.00001908
Iteration 137/1000 | Loss: 0.00001908
Iteration 138/1000 | Loss: 0.00001908
Iteration 139/1000 | Loss: 0.00001908
Iteration 140/1000 | Loss: 0.00001907
Iteration 141/1000 | Loss: 0.00001907
Iteration 142/1000 | Loss: 0.00001907
Iteration 143/1000 | Loss: 0.00001907
Iteration 144/1000 | Loss: 0.00001907
Iteration 145/1000 | Loss: 0.00001907
Iteration 146/1000 | Loss: 0.00001906
Iteration 147/1000 | Loss: 0.00001906
Iteration 148/1000 | Loss: 0.00001906
Iteration 149/1000 | Loss: 0.00001906
Iteration 150/1000 | Loss: 0.00001906
Iteration 151/1000 | Loss: 0.00001906
Iteration 152/1000 | Loss: 0.00001906
Iteration 153/1000 | Loss: 0.00001906
Iteration 154/1000 | Loss: 0.00001906
Iteration 155/1000 | Loss: 0.00001906
Iteration 156/1000 | Loss: 0.00001906
Iteration 157/1000 | Loss: 0.00001906
Iteration 158/1000 | Loss: 0.00001906
Iteration 159/1000 | Loss: 0.00001906
Iteration 160/1000 | Loss: 0.00001906
Iteration 161/1000 | Loss: 0.00001905
Iteration 162/1000 | Loss: 0.00001905
Iteration 163/1000 | Loss: 0.00001905
Iteration 164/1000 | Loss: 0.00001905
Iteration 165/1000 | Loss: 0.00001905
Iteration 166/1000 | Loss: 0.00001905
Iteration 167/1000 | Loss: 0.00001905
Iteration 168/1000 | Loss: 0.00001905
Iteration 169/1000 | Loss: 0.00001905
Iteration 170/1000 | Loss: 0.00001904
Iteration 171/1000 | Loss: 0.00001904
Iteration 172/1000 | Loss: 0.00001904
Iteration 173/1000 | Loss: 0.00001904
Iteration 174/1000 | Loss: 0.00001904
Iteration 175/1000 | Loss: 0.00001904
Iteration 176/1000 | Loss: 0.00001904
Iteration 177/1000 | Loss: 0.00001904
Iteration 178/1000 | Loss: 0.00001904
Iteration 179/1000 | Loss: 0.00001904
Iteration 180/1000 | Loss: 0.00001904
Iteration 181/1000 | Loss: 0.00001904
Iteration 182/1000 | Loss: 0.00001904
Iteration 183/1000 | Loss: 0.00001904
Iteration 184/1000 | Loss: 0.00001904
Iteration 185/1000 | Loss: 0.00001904
Iteration 186/1000 | Loss: 0.00001904
Iteration 187/1000 | Loss: 0.00001904
Iteration 188/1000 | Loss: 0.00001904
Iteration 189/1000 | Loss: 0.00001904
Iteration 190/1000 | Loss: 0.00001904
Iteration 191/1000 | Loss: 0.00001904
Iteration 192/1000 | Loss: 0.00001904
Iteration 193/1000 | Loss: 0.00001904
Iteration 194/1000 | Loss: 0.00001904
Iteration 195/1000 | Loss: 0.00001904
Iteration 196/1000 | Loss: 0.00001904
Iteration 197/1000 | Loss: 0.00001904
Iteration 198/1000 | Loss: 0.00001904
Iteration 199/1000 | Loss: 0.00001904
Iteration 200/1000 | Loss: 0.00001904
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 200. Stopping optimization.
Last 5 losses: [1.9037106540054083e-05, 1.9037106540054083e-05, 1.9037106540054083e-05, 1.9037106540054083e-05, 1.9037106540054083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9037106540054083e-05

Optimization complete. Final v2v error: 3.6792306900024414 mm

Highest mean error: 6.206836700439453 mm for frame 200

Lowest mean error: 3.2719271183013916 mm for frame 5

Saving results

Total time: 106.88759708404541
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01006724
Iteration 2/25 | Loss: 0.00217248
Iteration 3/25 | Loss: 0.00158709
Iteration 4/25 | Loss: 0.00150605
Iteration 5/25 | Loss: 0.00148055
Iteration 6/25 | Loss: 0.00157321
Iteration 7/25 | Loss: 0.00146516
Iteration 8/25 | Loss: 0.00144625
Iteration 9/25 | Loss: 0.00137383
Iteration 10/25 | Loss: 0.00137143
Iteration 11/25 | Loss: 0.00134640
Iteration 12/25 | Loss: 0.00133227
Iteration 13/25 | Loss: 0.00133680
Iteration 14/25 | Loss: 0.00133341
Iteration 15/25 | Loss: 0.00132826
Iteration 16/25 | Loss: 0.00132749
Iteration 17/25 | Loss: 0.00132625
Iteration 18/25 | Loss: 0.00132626
Iteration 19/25 | Loss: 0.00132712
Iteration 20/25 | Loss: 0.00132590
Iteration 21/25 | Loss: 0.00132498
Iteration 22/25 | Loss: 0.00132509
Iteration 23/25 | Loss: 0.00132367
Iteration 24/25 | Loss: 0.00132247
Iteration 25/25 | Loss: 0.00132587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45867980
Iteration 2/25 | Loss: 0.00109028
Iteration 3/25 | Loss: 0.00109028
Iteration 4/25 | Loss: 0.00109028
Iteration 5/25 | Loss: 0.00109028
Iteration 6/25 | Loss: 0.00109028
Iteration 7/25 | Loss: 0.00109028
Iteration 8/25 | Loss: 0.00109028
Iteration 9/25 | Loss: 0.00109028
Iteration 10/25 | Loss: 0.00109028
Iteration 11/25 | Loss: 0.00109028
Iteration 12/25 | Loss: 0.00109028
Iteration 13/25 | Loss: 0.00109028
Iteration 14/25 | Loss: 0.00109028
Iteration 15/25 | Loss: 0.00109028
Iteration 16/25 | Loss: 0.00109028
Iteration 17/25 | Loss: 0.00109028
Iteration 18/25 | Loss: 0.00109028
Iteration 19/25 | Loss: 0.00109028
Iteration 20/25 | Loss: 0.00109028
Iteration 21/25 | Loss: 0.00109028
Iteration 22/25 | Loss: 0.00109028
Iteration 23/25 | Loss: 0.00109028
Iteration 24/25 | Loss: 0.00109028
Iteration 25/25 | Loss: 0.00109028

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109028
Iteration 2/1000 | Loss: 0.00007917
Iteration 3/1000 | Loss: 0.00020269
Iteration 4/1000 | Loss: 0.00009327
Iteration 5/1000 | Loss: 0.00014810
Iteration 6/1000 | Loss: 0.00018471
Iteration 7/1000 | Loss: 0.00017562
Iteration 8/1000 | Loss: 0.00022501
Iteration 9/1000 | Loss: 0.00009300
Iteration 10/1000 | Loss: 0.00008410
Iteration 11/1000 | Loss: 0.00008706
Iteration 12/1000 | Loss: 0.00018358
Iteration 13/1000 | Loss: 0.00020993
Iteration 14/1000 | Loss: 0.00019672
Iteration 15/1000 | Loss: 0.00008605
Iteration 16/1000 | Loss: 0.00003242
Iteration 17/1000 | Loss: 0.00004742
Iteration 18/1000 | Loss: 0.00022272
Iteration 19/1000 | Loss: 0.00016271
Iteration 20/1000 | Loss: 0.00006940
Iteration 21/1000 | Loss: 0.00018194
Iteration 22/1000 | Loss: 0.00005634
Iteration 23/1000 | Loss: 0.00005306
Iteration 24/1000 | Loss: 0.00004011
Iteration 25/1000 | Loss: 0.00002120
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00003938
Iteration 28/1000 | Loss: 0.00003562
Iteration 29/1000 | Loss: 0.00022140
Iteration 30/1000 | Loss: 0.00353710
Iteration 31/1000 | Loss: 0.00030980
Iteration 32/1000 | Loss: 0.00017272
Iteration 33/1000 | Loss: 0.00016396
Iteration 34/1000 | Loss: 0.00010514
Iteration 35/1000 | Loss: 0.00004939
Iteration 36/1000 | Loss: 0.00014168
Iteration 37/1000 | Loss: 0.00022637
Iteration 38/1000 | Loss: 0.00003407
Iteration 39/1000 | Loss: 0.00006549
Iteration 40/1000 | Loss: 0.00002646
Iteration 41/1000 | Loss: 0.00002415
Iteration 42/1000 | Loss: 0.00015172
Iteration 43/1000 | Loss: 0.00103116
Iteration 44/1000 | Loss: 0.00002423
Iteration 45/1000 | Loss: 0.00017627
Iteration 46/1000 | Loss: 0.00003310
Iteration 47/1000 | Loss: 0.00022575
Iteration 48/1000 | Loss: 0.00013100
Iteration 49/1000 | Loss: 0.00004214
Iteration 50/1000 | Loss: 0.00015977
Iteration 51/1000 | Loss: 0.00026646
Iteration 52/1000 | Loss: 0.00018823
Iteration 53/1000 | Loss: 0.00041743
Iteration 54/1000 | Loss: 0.00033597
Iteration 55/1000 | Loss: 0.00037252
Iteration 56/1000 | Loss: 0.00015619
Iteration 57/1000 | Loss: 0.00020083
Iteration 58/1000 | Loss: 0.00024652
Iteration 59/1000 | Loss: 0.00019228
Iteration 60/1000 | Loss: 0.00021826
Iteration 61/1000 | Loss: 0.00021484
Iteration 62/1000 | Loss: 0.00019157
Iteration 63/1000 | Loss: 0.00010335
Iteration 64/1000 | Loss: 0.00013460
Iteration 65/1000 | Loss: 0.00017376
Iteration 66/1000 | Loss: 0.00020973
Iteration 67/1000 | Loss: 0.00020053
Iteration 68/1000 | Loss: 0.00017196
Iteration 69/1000 | Loss: 0.00002187
Iteration 70/1000 | Loss: 0.00003276
Iteration 71/1000 | Loss: 0.00001924
Iteration 72/1000 | Loss: 0.00004075
Iteration 73/1000 | Loss: 0.00016746
Iteration 74/1000 | Loss: 0.00002149
Iteration 75/1000 | Loss: 0.00001966
Iteration 76/1000 | Loss: 0.00011597
Iteration 77/1000 | Loss: 0.00007613
Iteration 78/1000 | Loss: 0.00002026
Iteration 79/1000 | Loss: 0.00001926
Iteration 80/1000 | Loss: 0.00025432
Iteration 81/1000 | Loss: 0.00064504
Iteration 82/1000 | Loss: 0.00021828
Iteration 83/1000 | Loss: 0.00010017
Iteration 84/1000 | Loss: 0.00001978
Iteration 85/1000 | Loss: 0.00030507
Iteration 86/1000 | Loss: 0.00015047
Iteration 87/1000 | Loss: 0.00022049
Iteration 88/1000 | Loss: 0.00020996
Iteration 89/1000 | Loss: 0.00008366
Iteration 90/1000 | Loss: 0.00003156
Iteration 91/1000 | Loss: 0.00001923
Iteration 92/1000 | Loss: 0.00001756
Iteration 93/1000 | Loss: 0.00007564
Iteration 94/1000 | Loss: 0.00001662
Iteration 95/1000 | Loss: 0.00005214
Iteration 96/1000 | Loss: 0.00001613
Iteration 97/1000 | Loss: 0.00005651
Iteration 98/1000 | Loss: 0.00001583
Iteration 99/1000 | Loss: 0.00001555
Iteration 100/1000 | Loss: 0.00001538
Iteration 101/1000 | Loss: 0.00001534
Iteration 102/1000 | Loss: 0.00002413
Iteration 103/1000 | Loss: 0.00011922
Iteration 104/1000 | Loss: 0.00003643
Iteration 105/1000 | Loss: 0.00001573
Iteration 106/1000 | Loss: 0.00004337
Iteration 107/1000 | Loss: 0.00001513
Iteration 108/1000 | Loss: 0.00001483
Iteration 109/1000 | Loss: 0.00001481
Iteration 110/1000 | Loss: 0.00001467
Iteration 111/1000 | Loss: 0.00007183
Iteration 112/1000 | Loss: 0.00002009
Iteration 113/1000 | Loss: 0.00003432
Iteration 114/1000 | Loss: 0.00001474
Iteration 115/1000 | Loss: 0.00001462
Iteration 116/1000 | Loss: 0.00001458
Iteration 117/1000 | Loss: 0.00001454
Iteration 118/1000 | Loss: 0.00001450
Iteration 119/1000 | Loss: 0.00001450
Iteration 120/1000 | Loss: 0.00001449
Iteration 121/1000 | Loss: 0.00001448
Iteration 122/1000 | Loss: 0.00001442
Iteration 123/1000 | Loss: 0.00001442
Iteration 124/1000 | Loss: 0.00001442
Iteration 125/1000 | Loss: 0.00001441
Iteration 126/1000 | Loss: 0.00001441
Iteration 127/1000 | Loss: 0.00001441
Iteration 128/1000 | Loss: 0.00001441
Iteration 129/1000 | Loss: 0.00001441
Iteration 130/1000 | Loss: 0.00001441
Iteration 131/1000 | Loss: 0.00001441
Iteration 132/1000 | Loss: 0.00001441
Iteration 133/1000 | Loss: 0.00001440
Iteration 134/1000 | Loss: 0.00001438
Iteration 135/1000 | Loss: 0.00001437
Iteration 136/1000 | Loss: 0.00001437
Iteration 137/1000 | Loss: 0.00001437
Iteration 138/1000 | Loss: 0.00001437
Iteration 139/1000 | Loss: 0.00007778
Iteration 140/1000 | Loss: 0.00002041
Iteration 141/1000 | Loss: 0.00001444
Iteration 142/1000 | Loss: 0.00003667
Iteration 143/1000 | Loss: 0.00002077
Iteration 144/1000 | Loss: 0.00001513
Iteration 145/1000 | Loss: 0.00001466
Iteration 146/1000 | Loss: 0.00001439
Iteration 147/1000 | Loss: 0.00001437
Iteration 148/1000 | Loss: 0.00001437
Iteration 149/1000 | Loss: 0.00001435
Iteration 150/1000 | Loss: 0.00001435
Iteration 151/1000 | Loss: 0.00001435
Iteration 152/1000 | Loss: 0.00001435
Iteration 153/1000 | Loss: 0.00001435
Iteration 154/1000 | Loss: 0.00001435
Iteration 155/1000 | Loss: 0.00001435
Iteration 156/1000 | Loss: 0.00001434
Iteration 157/1000 | Loss: 0.00001434
Iteration 158/1000 | Loss: 0.00001434
Iteration 159/1000 | Loss: 0.00001434
Iteration 160/1000 | Loss: 0.00001434
Iteration 161/1000 | Loss: 0.00001434
Iteration 162/1000 | Loss: 0.00001433
Iteration 163/1000 | Loss: 0.00001433
Iteration 164/1000 | Loss: 0.00001433
Iteration 165/1000 | Loss: 0.00001433
Iteration 166/1000 | Loss: 0.00001433
Iteration 167/1000 | Loss: 0.00001432
Iteration 168/1000 | Loss: 0.00001432
Iteration 169/1000 | Loss: 0.00001432
Iteration 170/1000 | Loss: 0.00001431
Iteration 171/1000 | Loss: 0.00001431
Iteration 172/1000 | Loss: 0.00001431
Iteration 173/1000 | Loss: 0.00001431
Iteration 174/1000 | Loss: 0.00001431
Iteration 175/1000 | Loss: 0.00001431
Iteration 176/1000 | Loss: 0.00001431
Iteration 177/1000 | Loss: 0.00001431
Iteration 178/1000 | Loss: 0.00001431
Iteration 179/1000 | Loss: 0.00001430
Iteration 180/1000 | Loss: 0.00001430
Iteration 181/1000 | Loss: 0.00001430
Iteration 182/1000 | Loss: 0.00001430
Iteration 183/1000 | Loss: 0.00001430
Iteration 184/1000 | Loss: 0.00001430
Iteration 185/1000 | Loss: 0.00001430
Iteration 186/1000 | Loss: 0.00001430
Iteration 187/1000 | Loss: 0.00001430
Iteration 188/1000 | Loss: 0.00001430
Iteration 189/1000 | Loss: 0.00001430
Iteration 190/1000 | Loss: 0.00001430
Iteration 191/1000 | Loss: 0.00001430
Iteration 192/1000 | Loss: 0.00001430
Iteration 193/1000 | Loss: 0.00001430
Iteration 194/1000 | Loss: 0.00001430
Iteration 195/1000 | Loss: 0.00001430
Iteration 196/1000 | Loss: 0.00001430
Iteration 197/1000 | Loss: 0.00001430
Iteration 198/1000 | Loss: 0.00001430
Iteration 199/1000 | Loss: 0.00001430
Iteration 200/1000 | Loss: 0.00001430
Iteration 201/1000 | Loss: 0.00001430
Iteration 202/1000 | Loss: 0.00001430
Iteration 203/1000 | Loss: 0.00001430
Iteration 204/1000 | Loss: 0.00001430
Iteration 205/1000 | Loss: 0.00001430
Iteration 206/1000 | Loss: 0.00001430
Iteration 207/1000 | Loss: 0.00001430
Iteration 208/1000 | Loss: 0.00001430
Iteration 209/1000 | Loss: 0.00001430
Iteration 210/1000 | Loss: 0.00001430
Iteration 211/1000 | Loss: 0.00001430
Iteration 212/1000 | Loss: 0.00001430
Iteration 213/1000 | Loss: 0.00001430
Iteration 214/1000 | Loss: 0.00001430
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.430270367563935e-05, 1.430270367563935e-05, 1.430270367563935e-05, 1.430270367563935e-05, 1.430270367563935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.430270367563935e-05

Optimization complete. Final v2v error: 3.181950807571411 mm

Highest mean error: 4.284703254699707 mm for frame 64

Lowest mean error: 2.7309675216674805 mm for frame 4

Saving results

Total time: 214.79433584213257
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01029081
Iteration 2/25 | Loss: 0.00277079
Iteration 3/25 | Loss: 0.00188724
Iteration 4/25 | Loss: 0.00197712
Iteration 5/25 | Loss: 0.00168728
Iteration 6/25 | Loss: 0.00165038
Iteration 7/25 | Loss: 0.00163906
Iteration 8/25 | Loss: 0.00161060
Iteration 9/25 | Loss: 0.00158736
Iteration 10/25 | Loss: 0.00158535
Iteration 11/25 | Loss: 0.00158168
Iteration 12/25 | Loss: 0.00156457
Iteration 13/25 | Loss: 0.00155905
Iteration 14/25 | Loss: 0.00154904
Iteration 15/25 | Loss: 0.00154571
Iteration 16/25 | Loss: 0.00154506
Iteration 17/25 | Loss: 0.00154472
Iteration 18/25 | Loss: 0.00154491
Iteration 19/25 | Loss: 0.00154508
Iteration 20/25 | Loss: 0.00154528
Iteration 21/25 | Loss: 0.00154472
Iteration 22/25 | Loss: 0.00154445
Iteration 23/25 | Loss: 0.00154474
Iteration 24/25 | Loss: 0.00154473
Iteration 25/25 | Loss: 0.00154553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35546303
Iteration 2/25 | Loss: 0.00306853
Iteration 3/25 | Loss: 0.00306852
Iteration 4/25 | Loss: 0.00306852
Iteration 5/25 | Loss: 0.00306852
Iteration 6/25 | Loss: 0.00306852
Iteration 7/25 | Loss: 0.00306852
Iteration 8/25 | Loss: 0.00306852
Iteration 9/25 | Loss: 0.00306852
Iteration 10/25 | Loss: 0.00306852
Iteration 11/25 | Loss: 0.00306852
Iteration 12/25 | Loss: 0.00306852
Iteration 13/25 | Loss: 0.00306852
Iteration 14/25 | Loss: 0.00306852
Iteration 15/25 | Loss: 0.00306852
Iteration 16/25 | Loss: 0.00306852
Iteration 17/25 | Loss: 0.00306852
Iteration 18/25 | Loss: 0.00306852
Iteration 19/25 | Loss: 0.00306852
Iteration 20/25 | Loss: 0.00306852
Iteration 21/25 | Loss: 0.00306851
Iteration 22/25 | Loss: 0.00306851
Iteration 23/25 | Loss: 0.00306851
Iteration 24/25 | Loss: 0.00306851
Iteration 25/25 | Loss: 0.00306851

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306851
Iteration 2/1000 | Loss: 0.00303658
Iteration 3/1000 | Loss: 0.00470667
Iteration 4/1000 | Loss: 0.00343512
Iteration 5/1000 | Loss: 0.00077043
Iteration 6/1000 | Loss: 0.00287448
Iteration 7/1000 | Loss: 0.00275182
Iteration 8/1000 | Loss: 0.00137037
Iteration 9/1000 | Loss: 0.00038425
Iteration 10/1000 | Loss: 0.00180116
Iteration 11/1000 | Loss: 0.00189605
Iteration 12/1000 | Loss: 0.00130029
Iteration 13/1000 | Loss: 0.00146233
Iteration 14/1000 | Loss: 0.00179648
Iteration 15/1000 | Loss: 0.00185388
Iteration 16/1000 | Loss: 0.00150762
Iteration 17/1000 | Loss: 0.00041358
Iteration 18/1000 | Loss: 0.00017646
Iteration 19/1000 | Loss: 0.00052822
Iteration 20/1000 | Loss: 0.00125854
Iteration 21/1000 | Loss: 0.00049975
Iteration 22/1000 | Loss: 0.00022185
Iteration 23/1000 | Loss: 0.00014472
Iteration 24/1000 | Loss: 0.00063766
Iteration 25/1000 | Loss: 0.00012251
Iteration 26/1000 | Loss: 0.00014569
Iteration 27/1000 | Loss: 0.00259904
Iteration 28/1000 | Loss: 0.00897334
Iteration 29/1000 | Loss: 0.00816769
Iteration 30/1000 | Loss: 0.00697983
Iteration 31/1000 | Loss: 0.00794620
Iteration 32/1000 | Loss: 0.00263164
Iteration 33/1000 | Loss: 0.00225449
Iteration 34/1000 | Loss: 0.00114369
Iteration 35/1000 | Loss: 0.00060228
Iteration 36/1000 | Loss: 0.00060398
Iteration 37/1000 | Loss: 0.00057766
Iteration 38/1000 | Loss: 0.00231097
Iteration 39/1000 | Loss: 0.00174716
Iteration 40/1000 | Loss: 0.00045861
Iteration 41/1000 | Loss: 0.00141483
Iteration 42/1000 | Loss: 0.00101545
Iteration 43/1000 | Loss: 0.00088359
Iteration 44/1000 | Loss: 0.00071060
Iteration 45/1000 | Loss: 0.00074061
Iteration 46/1000 | Loss: 0.00042077
Iteration 47/1000 | Loss: 0.00067919
Iteration 48/1000 | Loss: 0.00072722
Iteration 49/1000 | Loss: 0.00050278
Iteration 50/1000 | Loss: 0.00023529
Iteration 51/1000 | Loss: 0.00006287
Iteration 52/1000 | Loss: 0.00123656
Iteration 53/1000 | Loss: 0.00050566
Iteration 54/1000 | Loss: 0.00073980
Iteration 55/1000 | Loss: 0.00062458
Iteration 56/1000 | Loss: 0.00009263
Iteration 57/1000 | Loss: 0.00015805
Iteration 58/1000 | Loss: 0.00018349
Iteration 59/1000 | Loss: 0.00045924
Iteration 60/1000 | Loss: 0.00020270
Iteration 61/1000 | Loss: 0.00021995
Iteration 62/1000 | Loss: 0.00004414
Iteration 63/1000 | Loss: 0.00041793
Iteration 64/1000 | Loss: 0.00010262
Iteration 65/1000 | Loss: 0.00013392
Iteration 66/1000 | Loss: 0.00045687
Iteration 67/1000 | Loss: 0.00039658
Iteration 68/1000 | Loss: 0.00014486
Iteration 69/1000 | Loss: 0.00037476
Iteration 70/1000 | Loss: 0.00022078
Iteration 71/1000 | Loss: 0.00013838
Iteration 72/1000 | Loss: 0.00005528
Iteration 73/1000 | Loss: 0.00008727
Iteration 74/1000 | Loss: 0.00008719
Iteration 75/1000 | Loss: 0.00004524
Iteration 76/1000 | Loss: 0.00004665
Iteration 77/1000 | Loss: 0.00004225
Iteration 78/1000 | Loss: 0.00003760
Iteration 79/1000 | Loss: 0.00006179
Iteration 80/1000 | Loss: 0.00004006
Iteration 81/1000 | Loss: 0.00063934
Iteration 82/1000 | Loss: 0.00091291
Iteration 83/1000 | Loss: 0.00064875
Iteration 84/1000 | Loss: 0.00054853
Iteration 85/1000 | Loss: 0.00050937
Iteration 86/1000 | Loss: 0.00084517
Iteration 87/1000 | Loss: 0.00048099
Iteration 88/1000 | Loss: 0.00017224
Iteration 89/1000 | Loss: 0.00024285
Iteration 90/1000 | Loss: 0.00021134
Iteration 91/1000 | Loss: 0.00014181
Iteration 92/1000 | Loss: 0.00004298
Iteration 93/1000 | Loss: 0.00021712
Iteration 94/1000 | Loss: 0.00094845
Iteration 95/1000 | Loss: 0.00026538
Iteration 96/1000 | Loss: 0.00029126
Iteration 97/1000 | Loss: 0.00033450
Iteration 98/1000 | Loss: 0.00008000
Iteration 99/1000 | Loss: 0.00040886
Iteration 100/1000 | Loss: 0.00003958
Iteration 101/1000 | Loss: 0.00002669
Iteration 102/1000 | Loss: 0.00006544
Iteration 103/1000 | Loss: 0.00050814
Iteration 104/1000 | Loss: 0.00015781
Iteration 105/1000 | Loss: 0.00012036
Iteration 106/1000 | Loss: 0.00049485
Iteration 107/1000 | Loss: 0.00004159
Iteration 108/1000 | Loss: 0.00005492
Iteration 109/1000 | Loss: 0.00002587
Iteration 110/1000 | Loss: 0.00008517
Iteration 111/1000 | Loss: 0.00008495
Iteration 112/1000 | Loss: 0.00008054
Iteration 113/1000 | Loss: 0.00003835
Iteration 114/1000 | Loss: 0.00009553
Iteration 115/1000 | Loss: 0.00005737
Iteration 116/1000 | Loss: 0.00044063
Iteration 117/1000 | Loss: 0.00008272
Iteration 118/1000 | Loss: 0.00008716
Iteration 119/1000 | Loss: 0.00013815
Iteration 120/1000 | Loss: 0.00009667
Iteration 121/1000 | Loss: 0.00003420
Iteration 122/1000 | Loss: 0.00003412
Iteration 123/1000 | Loss: 0.00042025
Iteration 124/1000 | Loss: 0.00012191
Iteration 125/1000 | Loss: 0.00002518
Iteration 126/1000 | Loss: 0.00002971
Iteration 127/1000 | Loss: 0.00030805
Iteration 128/1000 | Loss: 0.00015755
Iteration 129/1000 | Loss: 0.00040763
Iteration 130/1000 | Loss: 0.00016309
Iteration 131/1000 | Loss: 0.00003098
Iteration 132/1000 | Loss: 0.00003287
Iteration 133/1000 | Loss: 0.00051659
Iteration 134/1000 | Loss: 0.00002986
Iteration 135/1000 | Loss: 0.00002659
Iteration 136/1000 | Loss: 0.00003400
Iteration 137/1000 | Loss: 0.00002990
Iteration 138/1000 | Loss: 0.00003239
Iteration 139/1000 | Loss: 0.00005957
Iteration 140/1000 | Loss: 0.00004700
Iteration 141/1000 | Loss: 0.00003301
Iteration 142/1000 | Loss: 0.00002369
Iteration 143/1000 | Loss: 0.00003325
Iteration 144/1000 | Loss: 0.00003598
Iteration 145/1000 | Loss: 0.00003320
Iteration 146/1000 | Loss: 0.00003253
Iteration 147/1000 | Loss: 0.00003699
Iteration 148/1000 | Loss: 0.00004610
Iteration 149/1000 | Loss: 0.00004313
Iteration 150/1000 | Loss: 0.00004391
Iteration 151/1000 | Loss: 0.00003163
Iteration 152/1000 | Loss: 0.00003182
Iteration 153/1000 | Loss: 0.00005994
Iteration 154/1000 | Loss: 0.00003787
Iteration 155/1000 | Loss: 0.00020618
Iteration 156/1000 | Loss: 0.00006397
Iteration 157/1000 | Loss: 0.00009788
Iteration 158/1000 | Loss: 0.00003039
Iteration 159/1000 | Loss: 0.00002515
Iteration 160/1000 | Loss: 0.00002288
Iteration 161/1000 | Loss: 0.00002100
Iteration 162/1000 | Loss: 0.00002030
Iteration 163/1000 | Loss: 0.00002401
Iteration 164/1000 | Loss: 0.00001948
Iteration 165/1000 | Loss: 0.00027051
Iteration 166/1000 | Loss: 0.00001957
Iteration 167/1000 | Loss: 0.00002037
Iteration 168/1000 | Loss: 0.00002737
Iteration 169/1000 | Loss: 0.00001859
Iteration 170/1000 | Loss: 0.00001859
Iteration 171/1000 | Loss: 0.00001859
Iteration 172/1000 | Loss: 0.00001857
Iteration 173/1000 | Loss: 0.00001855
Iteration 174/1000 | Loss: 0.00001884
Iteration 175/1000 | Loss: 0.00001848
Iteration 176/1000 | Loss: 0.00001847
Iteration 177/1000 | Loss: 0.00001847
Iteration 178/1000 | Loss: 0.00001860
Iteration 179/1000 | Loss: 0.00002248
Iteration 180/1000 | Loss: 0.00001836
Iteration 181/1000 | Loss: 0.00002496
Iteration 182/1000 | Loss: 0.00002048
Iteration 183/1000 | Loss: 0.00002604
Iteration 184/1000 | Loss: 0.00001821
Iteration 185/1000 | Loss: 0.00001811
Iteration 186/1000 | Loss: 0.00001802
Iteration 187/1000 | Loss: 0.00001802
Iteration 188/1000 | Loss: 0.00001802
Iteration 189/1000 | Loss: 0.00001802
Iteration 190/1000 | Loss: 0.00001801
Iteration 191/1000 | Loss: 0.00001801
Iteration 192/1000 | Loss: 0.00001801
Iteration 193/1000 | Loss: 0.00001801
Iteration 194/1000 | Loss: 0.00001801
Iteration 195/1000 | Loss: 0.00001801
Iteration 196/1000 | Loss: 0.00001801
Iteration 197/1000 | Loss: 0.00001801
Iteration 198/1000 | Loss: 0.00001801
Iteration 199/1000 | Loss: 0.00001801
Iteration 200/1000 | Loss: 0.00001801
Iteration 201/1000 | Loss: 0.00001801
Iteration 202/1000 | Loss: 0.00001800
Iteration 203/1000 | Loss: 0.00001800
Iteration 204/1000 | Loss: 0.00001800
Iteration 205/1000 | Loss: 0.00002319
Iteration 206/1000 | Loss: 0.00002319
Iteration 207/1000 | Loss: 0.00001949
Iteration 208/1000 | Loss: 0.00001791
Iteration 209/1000 | Loss: 0.00001791
Iteration 210/1000 | Loss: 0.00001791
Iteration 211/1000 | Loss: 0.00001791
Iteration 212/1000 | Loss: 0.00001791
Iteration 213/1000 | Loss: 0.00001790
Iteration 214/1000 | Loss: 0.00001790
Iteration 215/1000 | Loss: 0.00001790
Iteration 216/1000 | Loss: 0.00001821
Iteration 217/1000 | Loss: 0.00001821
Iteration 218/1000 | Loss: 0.00001847
Iteration 219/1000 | Loss: 0.00001835
Iteration 220/1000 | Loss: 0.00001787
Iteration 221/1000 | Loss: 0.00001787
Iteration 222/1000 | Loss: 0.00001787
Iteration 223/1000 | Loss: 0.00001787
Iteration 224/1000 | Loss: 0.00001787
Iteration 225/1000 | Loss: 0.00001787
Iteration 226/1000 | Loss: 0.00001787
Iteration 227/1000 | Loss: 0.00001787
Iteration 228/1000 | Loss: 0.00001787
Iteration 229/1000 | Loss: 0.00001787
Iteration 230/1000 | Loss: 0.00001787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.7870599549496546e-05, 1.7870599549496546e-05, 1.7870599549496546e-05, 1.7870599549496546e-05, 1.7870599549496546e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7870599549496546e-05

Optimization complete. Final v2v error: 3.477078914642334 mm

Highest mean error: 7.485609531402588 mm for frame 25

Lowest mean error: 2.9916532039642334 mm for frame 77

Saving results

Total time: 335.84187388420105
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430728
Iteration 2/25 | Loss: 0.00158137
Iteration 3/25 | Loss: 0.00139167
Iteration 4/25 | Loss: 0.00136055
Iteration 5/25 | Loss: 0.00135514
Iteration 6/25 | Loss: 0.00135388
Iteration 7/25 | Loss: 0.00135388
Iteration 8/25 | Loss: 0.00135388
Iteration 9/25 | Loss: 0.00135388
Iteration 10/25 | Loss: 0.00135388
Iteration 11/25 | Loss: 0.00135388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013538756174966693, 0.0013538756174966693, 0.0013538756174966693, 0.0013538756174966693, 0.0013538756174966693]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013538756174966693

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40636313
Iteration 2/25 | Loss: 0.00063547
Iteration 3/25 | Loss: 0.00063547
Iteration 4/25 | Loss: 0.00063547
Iteration 5/25 | Loss: 0.00063547
Iteration 6/25 | Loss: 0.00063547
Iteration 7/25 | Loss: 0.00063547
Iteration 8/25 | Loss: 0.00063547
Iteration 9/25 | Loss: 0.00063547
Iteration 10/25 | Loss: 0.00063547
Iteration 11/25 | Loss: 0.00063547
Iteration 12/25 | Loss: 0.00063547
Iteration 13/25 | Loss: 0.00063547
Iteration 14/25 | Loss: 0.00063547
Iteration 15/25 | Loss: 0.00063547
Iteration 16/25 | Loss: 0.00063547
Iteration 17/25 | Loss: 0.00063547
Iteration 18/25 | Loss: 0.00063547
Iteration 19/25 | Loss: 0.00063547
Iteration 20/25 | Loss: 0.00063547
Iteration 21/25 | Loss: 0.00063547
Iteration 22/25 | Loss: 0.00063547
Iteration 23/25 | Loss: 0.00063547
Iteration 24/25 | Loss: 0.00063547
Iteration 25/25 | Loss: 0.00063547

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063547
Iteration 2/1000 | Loss: 0.00004861
Iteration 3/1000 | Loss: 0.00002884
Iteration 4/1000 | Loss: 0.00002690
Iteration 5/1000 | Loss: 0.00002597
Iteration 6/1000 | Loss: 0.00002508
Iteration 7/1000 | Loss: 0.00002436
Iteration 8/1000 | Loss: 0.00002391
Iteration 9/1000 | Loss: 0.00002350
Iteration 10/1000 | Loss: 0.00002311
Iteration 11/1000 | Loss: 0.00002282
Iteration 12/1000 | Loss: 0.00002270
Iteration 13/1000 | Loss: 0.00002268
Iteration 14/1000 | Loss: 0.00002252
Iteration 15/1000 | Loss: 0.00002248
Iteration 16/1000 | Loss: 0.00002245
Iteration 17/1000 | Loss: 0.00002245
Iteration 18/1000 | Loss: 0.00002242
Iteration 19/1000 | Loss: 0.00002238
Iteration 20/1000 | Loss: 0.00002237
Iteration 21/1000 | Loss: 0.00002237
Iteration 22/1000 | Loss: 0.00002236
Iteration 23/1000 | Loss: 0.00002235
Iteration 24/1000 | Loss: 0.00002235
Iteration 25/1000 | Loss: 0.00002234
Iteration 26/1000 | Loss: 0.00002233
Iteration 27/1000 | Loss: 0.00002233
Iteration 28/1000 | Loss: 0.00002233
Iteration 29/1000 | Loss: 0.00002232
Iteration 30/1000 | Loss: 0.00002232
Iteration 31/1000 | Loss: 0.00002232
Iteration 32/1000 | Loss: 0.00002230
Iteration 33/1000 | Loss: 0.00002230
Iteration 34/1000 | Loss: 0.00002230
Iteration 35/1000 | Loss: 0.00002229
Iteration 36/1000 | Loss: 0.00002229
Iteration 37/1000 | Loss: 0.00002229
Iteration 38/1000 | Loss: 0.00002228
Iteration 39/1000 | Loss: 0.00002228
Iteration 40/1000 | Loss: 0.00002228
Iteration 41/1000 | Loss: 0.00002227
Iteration 42/1000 | Loss: 0.00002227
Iteration 43/1000 | Loss: 0.00002226
Iteration 44/1000 | Loss: 0.00002226
Iteration 45/1000 | Loss: 0.00002226
Iteration 46/1000 | Loss: 0.00002226
Iteration 47/1000 | Loss: 0.00002226
Iteration 48/1000 | Loss: 0.00002226
Iteration 49/1000 | Loss: 0.00002226
Iteration 50/1000 | Loss: 0.00002226
Iteration 51/1000 | Loss: 0.00002225
Iteration 52/1000 | Loss: 0.00002225
Iteration 53/1000 | Loss: 0.00002225
Iteration 54/1000 | Loss: 0.00002225
Iteration 55/1000 | Loss: 0.00002225
Iteration 56/1000 | Loss: 0.00002224
Iteration 57/1000 | Loss: 0.00002224
Iteration 58/1000 | Loss: 0.00002224
Iteration 59/1000 | Loss: 0.00002224
Iteration 60/1000 | Loss: 0.00002224
Iteration 61/1000 | Loss: 0.00002224
Iteration 62/1000 | Loss: 0.00002224
Iteration 63/1000 | Loss: 0.00002224
Iteration 64/1000 | Loss: 0.00002224
Iteration 65/1000 | Loss: 0.00002224
Iteration 66/1000 | Loss: 0.00002224
Iteration 67/1000 | Loss: 0.00002223
Iteration 68/1000 | Loss: 0.00002223
Iteration 69/1000 | Loss: 0.00002223
Iteration 70/1000 | Loss: 0.00002223
Iteration 71/1000 | Loss: 0.00002223
Iteration 72/1000 | Loss: 0.00002223
Iteration 73/1000 | Loss: 0.00002222
Iteration 74/1000 | Loss: 0.00002222
Iteration 75/1000 | Loss: 0.00002222
Iteration 76/1000 | Loss: 0.00002222
Iteration 77/1000 | Loss: 0.00002222
Iteration 78/1000 | Loss: 0.00002221
Iteration 79/1000 | Loss: 0.00002221
Iteration 80/1000 | Loss: 0.00002221
Iteration 81/1000 | Loss: 0.00002221
Iteration 82/1000 | Loss: 0.00002220
Iteration 83/1000 | Loss: 0.00002220
Iteration 84/1000 | Loss: 0.00002220
Iteration 85/1000 | Loss: 0.00002220
Iteration 86/1000 | Loss: 0.00002219
Iteration 87/1000 | Loss: 0.00002219
Iteration 88/1000 | Loss: 0.00002219
Iteration 89/1000 | Loss: 0.00002218
Iteration 90/1000 | Loss: 0.00002218
Iteration 91/1000 | Loss: 0.00002218
Iteration 92/1000 | Loss: 0.00002218
Iteration 93/1000 | Loss: 0.00002218
Iteration 94/1000 | Loss: 0.00002218
Iteration 95/1000 | Loss: 0.00002218
Iteration 96/1000 | Loss: 0.00002217
Iteration 97/1000 | Loss: 0.00002217
Iteration 98/1000 | Loss: 0.00002217
Iteration 99/1000 | Loss: 0.00002217
Iteration 100/1000 | Loss: 0.00002217
Iteration 101/1000 | Loss: 0.00002217
Iteration 102/1000 | Loss: 0.00002217
Iteration 103/1000 | Loss: 0.00002217
Iteration 104/1000 | Loss: 0.00002216
Iteration 105/1000 | Loss: 0.00002216
Iteration 106/1000 | Loss: 0.00002216
Iteration 107/1000 | Loss: 0.00002216
Iteration 108/1000 | Loss: 0.00002216
Iteration 109/1000 | Loss: 0.00002216
Iteration 110/1000 | Loss: 0.00002216
Iteration 111/1000 | Loss: 0.00002216
Iteration 112/1000 | Loss: 0.00002216
Iteration 113/1000 | Loss: 0.00002216
Iteration 114/1000 | Loss: 0.00002216
Iteration 115/1000 | Loss: 0.00002215
Iteration 116/1000 | Loss: 0.00002215
Iteration 117/1000 | Loss: 0.00002215
Iteration 118/1000 | Loss: 0.00002215
Iteration 119/1000 | Loss: 0.00002215
Iteration 120/1000 | Loss: 0.00002215
Iteration 121/1000 | Loss: 0.00002215
Iteration 122/1000 | Loss: 0.00002215
Iteration 123/1000 | Loss: 0.00002215
Iteration 124/1000 | Loss: 0.00002215
Iteration 125/1000 | Loss: 0.00002215
Iteration 126/1000 | Loss: 0.00002214
Iteration 127/1000 | Loss: 0.00002214
Iteration 128/1000 | Loss: 0.00002214
Iteration 129/1000 | Loss: 0.00002214
Iteration 130/1000 | Loss: 0.00002214
Iteration 131/1000 | Loss: 0.00002214
Iteration 132/1000 | Loss: 0.00002213
Iteration 133/1000 | Loss: 0.00002213
Iteration 134/1000 | Loss: 0.00002213
Iteration 135/1000 | Loss: 0.00002213
Iteration 136/1000 | Loss: 0.00002213
Iteration 137/1000 | Loss: 0.00002213
Iteration 138/1000 | Loss: 0.00002213
Iteration 139/1000 | Loss: 0.00002213
Iteration 140/1000 | Loss: 0.00002213
Iteration 141/1000 | Loss: 0.00002213
Iteration 142/1000 | Loss: 0.00002213
Iteration 143/1000 | Loss: 0.00002213
Iteration 144/1000 | Loss: 0.00002212
Iteration 145/1000 | Loss: 0.00002212
Iteration 146/1000 | Loss: 0.00002212
Iteration 147/1000 | Loss: 0.00002212
Iteration 148/1000 | Loss: 0.00002212
Iteration 149/1000 | Loss: 0.00002212
Iteration 150/1000 | Loss: 0.00002212
Iteration 151/1000 | Loss: 0.00002212
Iteration 152/1000 | Loss: 0.00002212
Iteration 153/1000 | Loss: 0.00002212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [2.212211438745726e-05, 2.212211438745726e-05, 2.212211438745726e-05, 2.212211438745726e-05, 2.212211438745726e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.212211438745726e-05

Optimization complete. Final v2v error: 3.9358959197998047 mm

Highest mean error: 4.306040287017822 mm for frame 62

Lowest mean error: 3.495983362197876 mm for frame 0

Saving results

Total time: 40.09782886505127
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00886736
Iteration 2/25 | Loss: 0.00195032
Iteration 3/25 | Loss: 0.00148134
Iteration 4/25 | Loss: 0.00141058
Iteration 5/25 | Loss: 0.00140794
Iteration 6/25 | Loss: 0.00140786
Iteration 7/25 | Loss: 0.00140786
Iteration 8/25 | Loss: 0.00140786
Iteration 9/25 | Loss: 0.00140786
Iteration 10/25 | Loss: 0.00140786
Iteration 11/25 | Loss: 0.00140786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014078575186431408, 0.0014078575186431408, 0.0014078575186431408, 0.0014078575186431408, 0.0014078575186431408]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014078575186431408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36138940
Iteration 2/25 | Loss: 0.00091718
Iteration 3/25 | Loss: 0.00091713
Iteration 4/25 | Loss: 0.00091713
Iteration 5/25 | Loss: 0.00091712
Iteration 6/25 | Loss: 0.00091712
Iteration 7/25 | Loss: 0.00091712
Iteration 8/25 | Loss: 0.00091712
Iteration 9/25 | Loss: 0.00091712
Iteration 10/25 | Loss: 0.00091712
Iteration 11/25 | Loss: 0.00091712
Iteration 12/25 | Loss: 0.00091712
Iteration 13/25 | Loss: 0.00091712
Iteration 14/25 | Loss: 0.00091712
Iteration 15/25 | Loss: 0.00091712
Iteration 16/25 | Loss: 0.00091712
Iteration 17/25 | Loss: 0.00091712
Iteration 18/25 | Loss: 0.00091712
Iteration 19/25 | Loss: 0.00091712
Iteration 20/25 | Loss: 0.00091712
Iteration 21/25 | Loss: 0.00091712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0009171226411126554, 0.0009171226411126554, 0.0009171226411126554, 0.0009171226411126554, 0.0009171226411126554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009171226411126554

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091712
Iteration 2/1000 | Loss: 0.00006126
Iteration 3/1000 | Loss: 0.00004569
Iteration 4/1000 | Loss: 0.00003812
Iteration 5/1000 | Loss: 0.00003574
Iteration 6/1000 | Loss: 0.00003460
Iteration 7/1000 | Loss: 0.00003381
Iteration 8/1000 | Loss: 0.00003324
Iteration 9/1000 | Loss: 0.00003262
Iteration 10/1000 | Loss: 0.00003213
Iteration 11/1000 | Loss: 0.00003174
Iteration 12/1000 | Loss: 0.00003140
Iteration 13/1000 | Loss: 0.00003104
Iteration 14/1000 | Loss: 0.00003093
Iteration 15/1000 | Loss: 0.00003088
Iteration 16/1000 | Loss: 0.00003088
Iteration 17/1000 | Loss: 0.00003080
Iteration 18/1000 | Loss: 0.00003075
Iteration 19/1000 | Loss: 0.00003073
Iteration 20/1000 | Loss: 0.00003068
Iteration 21/1000 | Loss: 0.00003063
Iteration 22/1000 | Loss: 0.00003059
Iteration 23/1000 | Loss: 0.00003055
Iteration 24/1000 | Loss: 0.00003051
Iteration 25/1000 | Loss: 0.00003047
Iteration 26/1000 | Loss: 0.00003046
Iteration 27/1000 | Loss: 0.00003046
Iteration 28/1000 | Loss: 0.00003046
Iteration 29/1000 | Loss: 0.00003046
Iteration 30/1000 | Loss: 0.00003046
Iteration 31/1000 | Loss: 0.00003046
Iteration 32/1000 | Loss: 0.00003045
Iteration 33/1000 | Loss: 0.00003045
Iteration 34/1000 | Loss: 0.00003045
Iteration 35/1000 | Loss: 0.00003045
Iteration 36/1000 | Loss: 0.00003045
Iteration 37/1000 | Loss: 0.00003045
Iteration 38/1000 | Loss: 0.00003045
Iteration 39/1000 | Loss: 0.00003043
Iteration 40/1000 | Loss: 0.00003043
Iteration 41/1000 | Loss: 0.00003043
Iteration 42/1000 | Loss: 0.00003043
Iteration 43/1000 | Loss: 0.00003043
Iteration 44/1000 | Loss: 0.00003043
Iteration 45/1000 | Loss: 0.00003043
Iteration 46/1000 | Loss: 0.00003042
Iteration 47/1000 | Loss: 0.00003042
Iteration 48/1000 | Loss: 0.00003042
Iteration 49/1000 | Loss: 0.00003042
Iteration 50/1000 | Loss: 0.00003042
Iteration 51/1000 | Loss: 0.00003042
Iteration 52/1000 | Loss: 0.00003042
Iteration 53/1000 | Loss: 0.00003041
Iteration 54/1000 | Loss: 0.00003041
Iteration 55/1000 | Loss: 0.00003041
Iteration 56/1000 | Loss: 0.00003041
Iteration 57/1000 | Loss: 0.00003041
Iteration 58/1000 | Loss: 0.00003040
Iteration 59/1000 | Loss: 0.00003040
Iteration 60/1000 | Loss: 0.00003040
Iteration 61/1000 | Loss: 0.00003040
Iteration 62/1000 | Loss: 0.00003040
Iteration 63/1000 | Loss: 0.00003040
Iteration 64/1000 | Loss: 0.00003040
Iteration 65/1000 | Loss: 0.00003040
Iteration 66/1000 | Loss: 0.00003039
Iteration 67/1000 | Loss: 0.00003039
Iteration 68/1000 | Loss: 0.00003039
Iteration 69/1000 | Loss: 0.00003039
Iteration 70/1000 | Loss: 0.00003039
Iteration 71/1000 | Loss: 0.00003039
Iteration 72/1000 | Loss: 0.00003039
Iteration 73/1000 | Loss: 0.00003039
Iteration 74/1000 | Loss: 0.00003039
Iteration 75/1000 | Loss: 0.00003039
Iteration 76/1000 | Loss: 0.00003039
Iteration 77/1000 | Loss: 0.00003039
Iteration 78/1000 | Loss: 0.00003039
Iteration 79/1000 | Loss: 0.00003039
Iteration 80/1000 | Loss: 0.00003039
Iteration 81/1000 | Loss: 0.00003039
Iteration 82/1000 | Loss: 0.00003039
Iteration 83/1000 | Loss: 0.00003039
Iteration 84/1000 | Loss: 0.00003039
Iteration 85/1000 | Loss: 0.00003039
Iteration 86/1000 | Loss: 0.00003039
Iteration 87/1000 | Loss: 0.00003039
Iteration 88/1000 | Loss: 0.00003039
Iteration 89/1000 | Loss: 0.00003039
Iteration 90/1000 | Loss: 0.00003039
Iteration 91/1000 | Loss: 0.00003039
Iteration 92/1000 | Loss: 0.00003039
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [3.0385666832444258e-05, 3.0385666832444258e-05, 3.0385666832444258e-05, 3.0385666832444258e-05, 3.0385666832444258e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0385666832444258e-05

Optimization complete. Final v2v error: 4.601815700531006 mm

Highest mean error: 4.824805736541748 mm for frame 0

Lowest mean error: 4.330224990844727 mm for frame 67

Saving results

Total time: 34.63199591636658
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838439
Iteration 2/25 | Loss: 0.00144110
Iteration 3/25 | Loss: 0.00135959
Iteration 4/25 | Loss: 0.00133869
Iteration 5/25 | Loss: 0.00133172
Iteration 6/25 | Loss: 0.00133172
Iteration 7/25 | Loss: 0.00133172
Iteration 8/25 | Loss: 0.00133172
Iteration 9/25 | Loss: 0.00133172
Iteration 10/25 | Loss: 0.00133172
Iteration 11/25 | Loss: 0.00133172
Iteration 12/25 | Loss: 0.00133172
Iteration 13/25 | Loss: 0.00133172
Iteration 14/25 | Loss: 0.00133172
Iteration 15/25 | Loss: 0.00133172
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013317184057086706, 0.0013317184057086706, 0.0013317184057086706, 0.0013317184057086706, 0.0013317184057086706]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013317184057086706

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38940144
Iteration 2/25 | Loss: 0.00101295
Iteration 3/25 | Loss: 0.00101295
Iteration 4/25 | Loss: 0.00101295
Iteration 5/25 | Loss: 0.00101295
Iteration 6/25 | Loss: 0.00101295
Iteration 7/25 | Loss: 0.00101295
Iteration 8/25 | Loss: 0.00101295
Iteration 9/25 | Loss: 0.00101295
Iteration 10/25 | Loss: 0.00101295
Iteration 11/25 | Loss: 0.00101295
Iteration 12/25 | Loss: 0.00101295
Iteration 13/25 | Loss: 0.00101295
Iteration 14/25 | Loss: 0.00101295
Iteration 15/25 | Loss: 0.00101295
Iteration 16/25 | Loss: 0.00101295
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010129505535587668, 0.0010129505535587668, 0.0010129505535587668, 0.0010129505535587668, 0.0010129505535587668]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010129505535587668

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101295
Iteration 2/1000 | Loss: 0.00003200
Iteration 3/1000 | Loss: 0.00002540
Iteration 4/1000 | Loss: 0.00002425
Iteration 5/1000 | Loss: 0.00002366
Iteration 6/1000 | Loss: 0.00002332
Iteration 7/1000 | Loss: 0.00002300
Iteration 8/1000 | Loss: 0.00002261
Iteration 9/1000 | Loss: 0.00002240
Iteration 10/1000 | Loss: 0.00002230
Iteration 11/1000 | Loss: 0.00002222
Iteration 12/1000 | Loss: 0.00002209
Iteration 13/1000 | Loss: 0.00002196
Iteration 14/1000 | Loss: 0.00002191
Iteration 15/1000 | Loss: 0.00002190
Iteration 16/1000 | Loss: 0.00002182
Iteration 17/1000 | Loss: 0.00002182
Iteration 18/1000 | Loss: 0.00002178
Iteration 19/1000 | Loss: 0.00002177
Iteration 20/1000 | Loss: 0.00002177
Iteration 21/1000 | Loss: 0.00002177
Iteration 22/1000 | Loss: 0.00002177
Iteration 23/1000 | Loss: 0.00002177
Iteration 24/1000 | Loss: 0.00002177
Iteration 25/1000 | Loss: 0.00002177
Iteration 26/1000 | Loss: 0.00002177
Iteration 27/1000 | Loss: 0.00002177
Iteration 28/1000 | Loss: 0.00002177
Iteration 29/1000 | Loss: 0.00002177
Iteration 30/1000 | Loss: 0.00002176
Iteration 31/1000 | Loss: 0.00002176
Iteration 32/1000 | Loss: 0.00002174
Iteration 33/1000 | Loss: 0.00002174
Iteration 34/1000 | Loss: 0.00002174
Iteration 35/1000 | Loss: 0.00002174
Iteration 36/1000 | Loss: 0.00002174
Iteration 37/1000 | Loss: 0.00002174
Iteration 38/1000 | Loss: 0.00002173
Iteration 39/1000 | Loss: 0.00002173
Iteration 40/1000 | Loss: 0.00002173
Iteration 41/1000 | Loss: 0.00002173
Iteration 42/1000 | Loss: 0.00002173
Iteration 43/1000 | Loss: 0.00002173
Iteration 44/1000 | Loss: 0.00002173
Iteration 45/1000 | Loss: 0.00002173
Iteration 46/1000 | Loss: 0.00002172
Iteration 47/1000 | Loss: 0.00002172
Iteration 48/1000 | Loss: 0.00002171
Iteration 49/1000 | Loss: 0.00002171
Iteration 50/1000 | Loss: 0.00002171
Iteration 51/1000 | Loss: 0.00002171
Iteration 52/1000 | Loss: 0.00002170
Iteration 53/1000 | Loss: 0.00002170
Iteration 54/1000 | Loss: 0.00002170
Iteration 55/1000 | Loss: 0.00002170
Iteration 56/1000 | Loss: 0.00002170
Iteration 57/1000 | Loss: 0.00002170
Iteration 58/1000 | Loss: 0.00002170
Iteration 59/1000 | Loss: 0.00002170
Iteration 60/1000 | Loss: 0.00002170
Iteration 61/1000 | Loss: 0.00002169
Iteration 62/1000 | Loss: 0.00002169
Iteration 63/1000 | Loss: 0.00002169
Iteration 64/1000 | Loss: 0.00002169
Iteration 65/1000 | Loss: 0.00002169
Iteration 66/1000 | Loss: 0.00002169
Iteration 67/1000 | Loss: 0.00002168
Iteration 68/1000 | Loss: 0.00002168
Iteration 69/1000 | Loss: 0.00002168
Iteration 70/1000 | Loss: 0.00002168
Iteration 71/1000 | Loss: 0.00002168
Iteration 72/1000 | Loss: 0.00002168
Iteration 73/1000 | Loss: 0.00002168
Iteration 74/1000 | Loss: 0.00002168
Iteration 75/1000 | Loss: 0.00002168
Iteration 76/1000 | Loss: 0.00002167
Iteration 77/1000 | Loss: 0.00002167
Iteration 78/1000 | Loss: 0.00002167
Iteration 79/1000 | Loss: 0.00002167
Iteration 80/1000 | Loss: 0.00002166
Iteration 81/1000 | Loss: 0.00002166
Iteration 82/1000 | Loss: 0.00002166
Iteration 83/1000 | Loss: 0.00002166
Iteration 84/1000 | Loss: 0.00002166
Iteration 85/1000 | Loss: 0.00002166
Iteration 86/1000 | Loss: 0.00002166
Iteration 87/1000 | Loss: 0.00002165
Iteration 88/1000 | Loss: 0.00002165
Iteration 89/1000 | Loss: 0.00002165
Iteration 90/1000 | Loss: 0.00002165
Iteration 91/1000 | Loss: 0.00002165
Iteration 92/1000 | Loss: 0.00002164
Iteration 93/1000 | Loss: 0.00002164
Iteration 94/1000 | Loss: 0.00002163
Iteration 95/1000 | Loss: 0.00002163
Iteration 96/1000 | Loss: 0.00002162
Iteration 97/1000 | Loss: 0.00002162
Iteration 98/1000 | Loss: 0.00002162
Iteration 99/1000 | Loss: 0.00002162
Iteration 100/1000 | Loss: 0.00002161
Iteration 101/1000 | Loss: 0.00002161
Iteration 102/1000 | Loss: 0.00002161
Iteration 103/1000 | Loss: 0.00002161
Iteration 104/1000 | Loss: 0.00002161
Iteration 105/1000 | Loss: 0.00002161
Iteration 106/1000 | Loss: 0.00002161
Iteration 107/1000 | Loss: 0.00002161
Iteration 108/1000 | Loss: 0.00002161
Iteration 109/1000 | Loss: 0.00002161
Iteration 110/1000 | Loss: 0.00002161
Iteration 111/1000 | Loss: 0.00002161
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [2.1611025658785366e-05, 2.1611025658785366e-05, 2.1611025658785366e-05, 2.1611025658785366e-05, 2.1611025658785366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1611025658785366e-05

Optimization complete. Final v2v error: 3.9230706691741943 mm

Highest mean error: 4.005313396453857 mm for frame 191

Lowest mean error: 3.867239475250244 mm for frame 171

Saving results

Total time: 35.57904028892517
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00413383
Iteration 2/25 | Loss: 0.00140810
Iteration 3/25 | Loss: 0.00131203
Iteration 4/25 | Loss: 0.00129604
Iteration 5/25 | Loss: 0.00129135
Iteration 6/25 | Loss: 0.00129032
Iteration 7/25 | Loss: 0.00129032
Iteration 8/25 | Loss: 0.00129032
Iteration 9/25 | Loss: 0.00129032
Iteration 10/25 | Loss: 0.00129032
Iteration 11/25 | Loss: 0.00129032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012903152965009212, 0.0012903152965009212, 0.0012903152965009212, 0.0012903152965009212, 0.0012903152965009212]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012903152965009212

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36929607
Iteration 2/25 | Loss: 0.00094199
Iteration 3/25 | Loss: 0.00094197
Iteration 4/25 | Loss: 0.00094197
Iteration 5/25 | Loss: 0.00094197
Iteration 6/25 | Loss: 0.00094197
Iteration 7/25 | Loss: 0.00094197
Iteration 8/25 | Loss: 0.00094197
Iteration 9/25 | Loss: 0.00094197
Iteration 10/25 | Loss: 0.00094197
Iteration 11/25 | Loss: 0.00094197
Iteration 12/25 | Loss: 0.00094197
Iteration 13/25 | Loss: 0.00094197
Iteration 14/25 | Loss: 0.00094197
Iteration 15/25 | Loss: 0.00094197
Iteration 16/25 | Loss: 0.00094197
Iteration 17/25 | Loss: 0.00094197
Iteration 18/25 | Loss: 0.00094197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0009419663110747933, 0.0009419663110747933, 0.0009419663110747933, 0.0009419663110747933, 0.0009419663110747933]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009419663110747933

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094197
Iteration 2/1000 | Loss: 0.00005830
Iteration 3/1000 | Loss: 0.00003694
Iteration 4/1000 | Loss: 0.00002917
Iteration 5/1000 | Loss: 0.00002422
Iteration 6/1000 | Loss: 0.00002240
Iteration 7/1000 | Loss: 0.00002089
Iteration 8/1000 | Loss: 0.00001993
Iteration 9/1000 | Loss: 0.00001929
Iteration 10/1000 | Loss: 0.00001875
Iteration 11/1000 | Loss: 0.00001839
Iteration 12/1000 | Loss: 0.00001815
Iteration 13/1000 | Loss: 0.00001790
Iteration 14/1000 | Loss: 0.00001787
Iteration 15/1000 | Loss: 0.00001785
Iteration 16/1000 | Loss: 0.00001780
Iteration 17/1000 | Loss: 0.00001778
Iteration 18/1000 | Loss: 0.00001777
Iteration 19/1000 | Loss: 0.00001776
Iteration 20/1000 | Loss: 0.00001775
Iteration 21/1000 | Loss: 0.00001775
Iteration 22/1000 | Loss: 0.00001774
Iteration 23/1000 | Loss: 0.00001774
Iteration 24/1000 | Loss: 0.00001773
Iteration 25/1000 | Loss: 0.00001773
Iteration 26/1000 | Loss: 0.00001773
Iteration 27/1000 | Loss: 0.00001772
Iteration 28/1000 | Loss: 0.00001771
Iteration 29/1000 | Loss: 0.00001768
Iteration 30/1000 | Loss: 0.00001768
Iteration 31/1000 | Loss: 0.00001766
Iteration 32/1000 | Loss: 0.00001766
Iteration 33/1000 | Loss: 0.00001766
Iteration 34/1000 | Loss: 0.00001765
Iteration 35/1000 | Loss: 0.00001765
Iteration 36/1000 | Loss: 0.00001765
Iteration 37/1000 | Loss: 0.00001764
Iteration 38/1000 | Loss: 0.00001764
Iteration 39/1000 | Loss: 0.00001763
Iteration 40/1000 | Loss: 0.00001762
Iteration 41/1000 | Loss: 0.00001762
Iteration 42/1000 | Loss: 0.00001759
Iteration 43/1000 | Loss: 0.00001759
Iteration 44/1000 | Loss: 0.00001755
Iteration 45/1000 | Loss: 0.00001751
Iteration 46/1000 | Loss: 0.00001751
Iteration 47/1000 | Loss: 0.00001746
Iteration 48/1000 | Loss: 0.00001746
Iteration 49/1000 | Loss: 0.00001744
Iteration 50/1000 | Loss: 0.00001744
Iteration 51/1000 | Loss: 0.00001743
Iteration 52/1000 | Loss: 0.00001743
Iteration 53/1000 | Loss: 0.00001742
Iteration 54/1000 | Loss: 0.00001742
Iteration 55/1000 | Loss: 0.00001742
Iteration 56/1000 | Loss: 0.00001741
Iteration 57/1000 | Loss: 0.00001741
Iteration 58/1000 | Loss: 0.00001740
Iteration 59/1000 | Loss: 0.00001740
Iteration 60/1000 | Loss: 0.00001739
Iteration 61/1000 | Loss: 0.00001738
Iteration 62/1000 | Loss: 0.00001737
Iteration 63/1000 | Loss: 0.00001737
Iteration 64/1000 | Loss: 0.00001737
Iteration 65/1000 | Loss: 0.00001736
Iteration 66/1000 | Loss: 0.00001736
Iteration 67/1000 | Loss: 0.00001735
Iteration 68/1000 | Loss: 0.00001735
Iteration 69/1000 | Loss: 0.00001734
Iteration 70/1000 | Loss: 0.00001734
Iteration 71/1000 | Loss: 0.00001733
Iteration 72/1000 | Loss: 0.00001732
Iteration 73/1000 | Loss: 0.00001732
Iteration 74/1000 | Loss: 0.00001732
Iteration 75/1000 | Loss: 0.00001732
Iteration 76/1000 | Loss: 0.00001732
Iteration 77/1000 | Loss: 0.00001732
Iteration 78/1000 | Loss: 0.00001732
Iteration 79/1000 | Loss: 0.00001731
Iteration 80/1000 | Loss: 0.00001731
Iteration 81/1000 | Loss: 0.00001730
Iteration 82/1000 | Loss: 0.00001730
Iteration 83/1000 | Loss: 0.00001730
Iteration 84/1000 | Loss: 0.00001729
Iteration 85/1000 | Loss: 0.00001729
Iteration 86/1000 | Loss: 0.00001729
Iteration 87/1000 | Loss: 0.00001729
Iteration 88/1000 | Loss: 0.00001729
Iteration 89/1000 | Loss: 0.00001729
Iteration 90/1000 | Loss: 0.00001729
Iteration 91/1000 | Loss: 0.00001728
Iteration 92/1000 | Loss: 0.00001728
Iteration 93/1000 | Loss: 0.00001728
Iteration 94/1000 | Loss: 0.00001727
Iteration 95/1000 | Loss: 0.00001727
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001726
Iteration 98/1000 | Loss: 0.00001726
Iteration 99/1000 | Loss: 0.00001726
Iteration 100/1000 | Loss: 0.00001725
Iteration 101/1000 | Loss: 0.00001725
Iteration 102/1000 | Loss: 0.00001725
Iteration 103/1000 | Loss: 0.00001725
Iteration 104/1000 | Loss: 0.00001724
Iteration 105/1000 | Loss: 0.00001724
Iteration 106/1000 | Loss: 0.00001724
Iteration 107/1000 | Loss: 0.00001724
Iteration 108/1000 | Loss: 0.00001724
Iteration 109/1000 | Loss: 0.00001724
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001723
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001722
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001722
Iteration 117/1000 | Loss: 0.00001721
Iteration 118/1000 | Loss: 0.00001721
Iteration 119/1000 | Loss: 0.00001721
Iteration 120/1000 | Loss: 0.00001721
Iteration 121/1000 | Loss: 0.00001721
Iteration 122/1000 | Loss: 0.00001721
Iteration 123/1000 | Loss: 0.00001721
Iteration 124/1000 | Loss: 0.00001721
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001720
Iteration 129/1000 | Loss: 0.00001720
Iteration 130/1000 | Loss: 0.00001720
Iteration 131/1000 | Loss: 0.00001720
Iteration 132/1000 | Loss: 0.00001720
Iteration 133/1000 | Loss: 0.00001720
Iteration 134/1000 | Loss: 0.00001720
Iteration 135/1000 | Loss: 0.00001720
Iteration 136/1000 | Loss: 0.00001720
Iteration 137/1000 | Loss: 0.00001720
Iteration 138/1000 | Loss: 0.00001720
Iteration 139/1000 | Loss: 0.00001720
Iteration 140/1000 | Loss: 0.00001719
Iteration 141/1000 | Loss: 0.00001719
Iteration 142/1000 | Loss: 0.00001719
Iteration 143/1000 | Loss: 0.00001719
Iteration 144/1000 | Loss: 0.00001719
Iteration 145/1000 | Loss: 0.00001719
Iteration 146/1000 | Loss: 0.00001719
Iteration 147/1000 | Loss: 0.00001719
Iteration 148/1000 | Loss: 0.00001719
Iteration 149/1000 | Loss: 0.00001719
Iteration 150/1000 | Loss: 0.00001719
Iteration 151/1000 | Loss: 0.00001718
Iteration 152/1000 | Loss: 0.00001718
Iteration 153/1000 | Loss: 0.00001718
Iteration 154/1000 | Loss: 0.00001718
Iteration 155/1000 | Loss: 0.00001718
Iteration 156/1000 | Loss: 0.00001718
Iteration 157/1000 | Loss: 0.00001718
Iteration 158/1000 | Loss: 0.00001718
Iteration 159/1000 | Loss: 0.00001718
Iteration 160/1000 | Loss: 0.00001718
Iteration 161/1000 | Loss: 0.00001718
Iteration 162/1000 | Loss: 0.00001717
Iteration 163/1000 | Loss: 0.00001717
Iteration 164/1000 | Loss: 0.00001717
Iteration 165/1000 | Loss: 0.00001716
Iteration 166/1000 | Loss: 0.00001716
Iteration 167/1000 | Loss: 0.00001716
Iteration 168/1000 | Loss: 0.00001715
Iteration 169/1000 | Loss: 0.00001715
Iteration 170/1000 | Loss: 0.00001715
Iteration 171/1000 | Loss: 0.00001715
Iteration 172/1000 | Loss: 0.00001715
Iteration 173/1000 | Loss: 0.00001714
Iteration 174/1000 | Loss: 0.00001714
Iteration 175/1000 | Loss: 0.00001714
Iteration 176/1000 | Loss: 0.00001714
Iteration 177/1000 | Loss: 0.00001714
Iteration 178/1000 | Loss: 0.00001714
Iteration 179/1000 | Loss: 0.00001714
Iteration 180/1000 | Loss: 0.00001714
Iteration 181/1000 | Loss: 0.00001714
Iteration 182/1000 | Loss: 0.00001714
Iteration 183/1000 | Loss: 0.00001714
Iteration 184/1000 | Loss: 0.00001714
Iteration 185/1000 | Loss: 0.00001714
Iteration 186/1000 | Loss: 0.00001714
Iteration 187/1000 | Loss: 0.00001714
Iteration 188/1000 | Loss: 0.00001714
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 188. Stopping optimization.
Last 5 losses: [1.713517303869594e-05, 1.713517303869594e-05, 1.713517303869594e-05, 1.713517303869594e-05, 1.713517303869594e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.713517303869594e-05

Optimization complete. Final v2v error: 3.3977925777435303 mm

Highest mean error: 5.420908451080322 mm for frame 74

Lowest mean error: 2.8414227962493896 mm for frame 162

Saving results

Total time: 42.740477561950684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736621
Iteration 2/25 | Loss: 0.00179562
Iteration 3/25 | Loss: 0.00143491
Iteration 4/25 | Loss: 0.00138069
Iteration 5/25 | Loss: 0.00136887
Iteration 6/25 | Loss: 0.00135560
Iteration 7/25 | Loss: 0.00135786
Iteration 8/25 | Loss: 0.00134861
Iteration 9/25 | Loss: 0.00134674
Iteration 10/25 | Loss: 0.00134333
Iteration 11/25 | Loss: 0.00134364
Iteration 12/25 | Loss: 0.00134284
Iteration 13/25 | Loss: 0.00133787
Iteration 14/25 | Loss: 0.00133684
Iteration 15/25 | Loss: 0.00133582
Iteration 16/25 | Loss: 0.00133468
Iteration 17/25 | Loss: 0.00133451
Iteration 18/25 | Loss: 0.00133448
Iteration 19/25 | Loss: 0.00133447
Iteration 20/25 | Loss: 0.00133447
Iteration 21/25 | Loss: 0.00133447
Iteration 22/25 | Loss: 0.00133447
Iteration 23/25 | Loss: 0.00133447
Iteration 24/25 | Loss: 0.00133446
Iteration 25/25 | Loss: 0.00133446

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.86044025
Iteration 2/25 | Loss: 0.00090097
Iteration 3/25 | Loss: 0.00090084
Iteration 4/25 | Loss: 0.00090084
Iteration 5/25 | Loss: 0.00090084
Iteration 6/25 | Loss: 0.00090084
Iteration 7/25 | Loss: 0.00090084
Iteration 8/25 | Loss: 0.00090083
Iteration 9/25 | Loss: 0.00090083
Iteration 10/25 | Loss: 0.00090083
Iteration 11/25 | Loss: 0.00090083
Iteration 12/25 | Loss: 0.00090083
Iteration 13/25 | Loss: 0.00090083
Iteration 14/25 | Loss: 0.00090083
Iteration 15/25 | Loss: 0.00090083
Iteration 16/25 | Loss: 0.00090083
Iteration 17/25 | Loss: 0.00090083
Iteration 18/25 | Loss: 0.00090083
Iteration 19/25 | Loss: 0.00090083
Iteration 20/25 | Loss: 0.00090083
Iteration 21/25 | Loss: 0.00090083
Iteration 22/25 | Loss: 0.00090083
Iteration 23/25 | Loss: 0.00090083
Iteration 24/25 | Loss: 0.00090083
Iteration 25/25 | Loss: 0.00090083

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00090083
Iteration 2/1000 | Loss: 0.00014127
Iteration 3/1000 | Loss: 0.00009897
Iteration 4/1000 | Loss: 0.00006896
Iteration 5/1000 | Loss: 0.00006074
Iteration 6/1000 | Loss: 0.00003019
Iteration 7/1000 | Loss: 0.00002837
Iteration 8/1000 | Loss: 0.00013878
Iteration 9/1000 | Loss: 0.00013165
Iteration 10/1000 | Loss: 0.00004588
Iteration 11/1000 | Loss: 0.00002997
Iteration 12/1000 | Loss: 0.00002680
Iteration 13/1000 | Loss: 0.00016196
Iteration 14/1000 | Loss: 0.00002878
Iteration 15/1000 | Loss: 0.00002651
Iteration 16/1000 | Loss: 0.00003019
Iteration 17/1000 | Loss: 0.00002644
Iteration 18/1000 | Loss: 0.00002594
Iteration 19/1000 | Loss: 0.00002527
Iteration 20/1000 | Loss: 0.00002442
Iteration 21/1000 | Loss: 0.00002387
Iteration 22/1000 | Loss: 0.00002334
Iteration 23/1000 | Loss: 0.00002287
Iteration 24/1000 | Loss: 0.00002234
Iteration 25/1000 | Loss: 0.00002199
Iteration 26/1000 | Loss: 0.00002199
Iteration 27/1000 | Loss: 0.00002176
Iteration 28/1000 | Loss: 0.00002162
Iteration 29/1000 | Loss: 0.00002158
Iteration 30/1000 | Loss: 0.00002154
Iteration 31/1000 | Loss: 0.00002153
Iteration 32/1000 | Loss: 0.00002143
Iteration 33/1000 | Loss: 0.00002142
Iteration 34/1000 | Loss: 0.00002141
Iteration 35/1000 | Loss: 0.00002141
Iteration 36/1000 | Loss: 0.00002141
Iteration 37/1000 | Loss: 0.00002141
Iteration 38/1000 | Loss: 0.00002140
Iteration 39/1000 | Loss: 0.00002140
Iteration 40/1000 | Loss: 0.00002139
Iteration 41/1000 | Loss: 0.00002139
Iteration 42/1000 | Loss: 0.00002139
Iteration 43/1000 | Loss: 0.00002138
Iteration 44/1000 | Loss: 0.00002138
Iteration 45/1000 | Loss: 0.00002138
Iteration 46/1000 | Loss: 0.00002138
Iteration 47/1000 | Loss: 0.00002138
Iteration 48/1000 | Loss: 0.00002138
Iteration 49/1000 | Loss: 0.00002138
Iteration 50/1000 | Loss: 0.00002138
Iteration 51/1000 | Loss: 0.00002138
Iteration 52/1000 | Loss: 0.00002138
Iteration 53/1000 | Loss: 0.00002138
Iteration 54/1000 | Loss: 0.00002138
Iteration 55/1000 | Loss: 0.00002138
Iteration 56/1000 | Loss: 0.00002138
Iteration 57/1000 | Loss: 0.00002138
Iteration 58/1000 | Loss: 0.00002138
Iteration 59/1000 | Loss: 0.00002138
Iteration 60/1000 | Loss: 0.00002138
Iteration 61/1000 | Loss: 0.00002137
Iteration 62/1000 | Loss: 0.00002137
Iteration 63/1000 | Loss: 0.00002137
Iteration 64/1000 | Loss: 0.00002137
Iteration 65/1000 | Loss: 0.00002137
Iteration 66/1000 | Loss: 0.00002137
Iteration 67/1000 | Loss: 0.00002137
Iteration 68/1000 | Loss: 0.00002137
Iteration 69/1000 | Loss: 0.00002137
Iteration 70/1000 | Loss: 0.00002137
Iteration 71/1000 | Loss: 0.00002137
Iteration 72/1000 | Loss: 0.00002137
Iteration 73/1000 | Loss: 0.00002136
Iteration 74/1000 | Loss: 0.00002136
Iteration 75/1000 | Loss: 0.00002136
Iteration 76/1000 | Loss: 0.00002136
Iteration 77/1000 | Loss: 0.00002136
Iteration 78/1000 | Loss: 0.00002136
Iteration 79/1000 | Loss: 0.00002135
Iteration 80/1000 | Loss: 0.00002135
Iteration 81/1000 | Loss: 0.00002135
Iteration 82/1000 | Loss: 0.00002135
Iteration 83/1000 | Loss: 0.00002134
Iteration 84/1000 | Loss: 0.00002134
Iteration 85/1000 | Loss: 0.00002134
Iteration 86/1000 | Loss: 0.00002134
Iteration 87/1000 | Loss: 0.00002134
Iteration 88/1000 | Loss: 0.00002134
Iteration 89/1000 | Loss: 0.00002134
Iteration 90/1000 | Loss: 0.00002133
Iteration 91/1000 | Loss: 0.00002133
Iteration 92/1000 | Loss: 0.00002133
Iteration 93/1000 | Loss: 0.00002133
Iteration 94/1000 | Loss: 0.00002133
Iteration 95/1000 | Loss: 0.00002132
Iteration 96/1000 | Loss: 0.00002132
Iteration 97/1000 | Loss: 0.00002132
Iteration 98/1000 | Loss: 0.00002131
Iteration 99/1000 | Loss: 0.00002130
Iteration 100/1000 | Loss: 0.00002130
Iteration 101/1000 | Loss: 0.00002130
Iteration 102/1000 | Loss: 0.00002129
Iteration 103/1000 | Loss: 0.00002128
Iteration 104/1000 | Loss: 0.00002127
Iteration 105/1000 | Loss: 0.00002127
Iteration 106/1000 | Loss: 0.00002126
Iteration 107/1000 | Loss: 0.00002122
Iteration 108/1000 | Loss: 0.00002119
Iteration 109/1000 | Loss: 0.00002119
Iteration 110/1000 | Loss: 0.00002118
Iteration 111/1000 | Loss: 0.00002118
Iteration 112/1000 | Loss: 0.00002115
Iteration 113/1000 | Loss: 0.00002115
Iteration 114/1000 | Loss: 0.00002113
Iteration 115/1000 | Loss: 0.00002113
Iteration 116/1000 | Loss: 0.00002112
Iteration 117/1000 | Loss: 0.00002112
Iteration 118/1000 | Loss: 0.00002112
Iteration 119/1000 | Loss: 0.00002112
Iteration 120/1000 | Loss: 0.00002111
Iteration 121/1000 | Loss: 0.00002111
Iteration 122/1000 | Loss: 0.00002110
Iteration 123/1000 | Loss: 0.00002110
Iteration 124/1000 | Loss: 0.00002109
Iteration 125/1000 | Loss: 0.00002109
Iteration 126/1000 | Loss: 0.00002109
Iteration 127/1000 | Loss: 0.00002109
Iteration 128/1000 | Loss: 0.00002109
Iteration 129/1000 | Loss: 0.00002108
Iteration 130/1000 | Loss: 0.00002108
Iteration 131/1000 | Loss: 0.00002108
Iteration 132/1000 | Loss: 0.00002108
Iteration 133/1000 | Loss: 0.00002108
Iteration 134/1000 | Loss: 0.00002108
Iteration 135/1000 | Loss: 0.00002107
Iteration 136/1000 | Loss: 0.00002107
Iteration 137/1000 | Loss: 0.00002107
Iteration 138/1000 | Loss: 0.00002107
Iteration 139/1000 | Loss: 0.00002107
Iteration 140/1000 | Loss: 0.00002107
Iteration 141/1000 | Loss: 0.00002107
Iteration 142/1000 | Loss: 0.00002107
Iteration 143/1000 | Loss: 0.00002107
Iteration 144/1000 | Loss: 0.00002107
Iteration 145/1000 | Loss: 0.00002107
Iteration 146/1000 | Loss: 0.00002106
Iteration 147/1000 | Loss: 0.00002106
Iteration 148/1000 | Loss: 0.00002106
Iteration 149/1000 | Loss: 0.00002106
Iteration 150/1000 | Loss: 0.00002106
Iteration 151/1000 | Loss: 0.00002106
Iteration 152/1000 | Loss: 0.00002106
Iteration 153/1000 | Loss: 0.00002106
Iteration 154/1000 | Loss: 0.00002106
Iteration 155/1000 | Loss: 0.00002106
Iteration 156/1000 | Loss: 0.00002106
Iteration 157/1000 | Loss: 0.00002106
Iteration 158/1000 | Loss: 0.00002106
Iteration 159/1000 | Loss: 0.00002106
Iteration 160/1000 | Loss: 0.00002106
Iteration 161/1000 | Loss: 0.00002106
Iteration 162/1000 | Loss: 0.00002106
Iteration 163/1000 | Loss: 0.00002106
Iteration 164/1000 | Loss: 0.00002106
Iteration 165/1000 | Loss: 0.00002106
Iteration 166/1000 | Loss: 0.00002106
Iteration 167/1000 | Loss: 0.00002105
Iteration 168/1000 | Loss: 0.00002105
Iteration 169/1000 | Loss: 0.00002105
Iteration 170/1000 | Loss: 0.00002105
Iteration 171/1000 | Loss: 0.00002105
Iteration 172/1000 | Loss: 0.00002105
Iteration 173/1000 | Loss: 0.00002105
Iteration 174/1000 | Loss: 0.00002105
Iteration 175/1000 | Loss: 0.00002105
Iteration 176/1000 | Loss: 0.00002105
Iteration 177/1000 | Loss: 0.00002105
Iteration 178/1000 | Loss: 0.00002105
Iteration 179/1000 | Loss: 0.00002105
Iteration 180/1000 | Loss: 0.00002105
Iteration 181/1000 | Loss: 0.00002105
Iteration 182/1000 | Loss: 0.00002105
Iteration 183/1000 | Loss: 0.00002105
Iteration 184/1000 | Loss: 0.00002105
Iteration 185/1000 | Loss: 0.00002105
Iteration 186/1000 | Loss: 0.00002105
Iteration 187/1000 | Loss: 0.00002105
Iteration 188/1000 | Loss: 0.00002105
Iteration 189/1000 | Loss: 0.00002105
Iteration 190/1000 | Loss: 0.00002104
Iteration 191/1000 | Loss: 0.00002104
Iteration 192/1000 | Loss: 0.00002104
Iteration 193/1000 | Loss: 0.00002104
Iteration 194/1000 | Loss: 0.00002104
Iteration 195/1000 | Loss: 0.00002104
Iteration 196/1000 | Loss: 0.00002104
Iteration 197/1000 | Loss: 0.00002104
Iteration 198/1000 | Loss: 0.00002104
Iteration 199/1000 | Loss: 0.00002104
Iteration 200/1000 | Loss: 0.00002104
Iteration 201/1000 | Loss: 0.00002104
Iteration 202/1000 | Loss: 0.00002104
Iteration 203/1000 | Loss: 0.00002104
Iteration 204/1000 | Loss: 0.00002104
Iteration 205/1000 | Loss: 0.00002104
Iteration 206/1000 | Loss: 0.00002104
Iteration 207/1000 | Loss: 0.00002103
Iteration 208/1000 | Loss: 0.00002103
Iteration 209/1000 | Loss: 0.00002103
Iteration 210/1000 | Loss: 0.00002103
Iteration 211/1000 | Loss: 0.00002103
Iteration 212/1000 | Loss: 0.00002103
Iteration 213/1000 | Loss: 0.00002103
Iteration 214/1000 | Loss: 0.00002103
Iteration 215/1000 | Loss: 0.00002103
Iteration 216/1000 | Loss: 0.00002103
Iteration 217/1000 | Loss: 0.00002103
Iteration 218/1000 | Loss: 0.00002103
Iteration 219/1000 | Loss: 0.00002103
Iteration 220/1000 | Loss: 0.00002103
Iteration 221/1000 | Loss: 0.00002103
Iteration 222/1000 | Loss: 0.00002103
Iteration 223/1000 | Loss: 0.00002103
Iteration 224/1000 | Loss: 0.00002103
Iteration 225/1000 | Loss: 0.00002103
Iteration 226/1000 | Loss: 0.00002103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [2.1032845324953087e-05, 2.1032845324953087e-05, 2.1032845324953087e-05, 2.1032845324953087e-05, 2.1032845324953087e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1032845324953087e-05

Optimization complete. Final v2v error: 3.7996811866760254 mm

Highest mean error: 5.703205585479736 mm for frame 173

Lowest mean error: 3.0113630294799805 mm for frame 118

Saving results

Total time: 94.96036052703857
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00949258
Iteration 2/25 | Loss: 0.00949258
Iteration 3/25 | Loss: 0.00334676
Iteration 4/25 | Loss: 0.00241225
Iteration 5/25 | Loss: 0.00242957
Iteration 6/25 | Loss: 0.00227510
Iteration 7/25 | Loss: 0.00226801
Iteration 8/25 | Loss: 0.00225154
Iteration 9/25 | Loss: 0.00220990
Iteration 10/25 | Loss: 0.00212223
Iteration 11/25 | Loss: 0.00209199
Iteration 12/25 | Loss: 0.00207058
Iteration 13/25 | Loss: 0.00206566
Iteration 14/25 | Loss: 0.00204898
Iteration 15/25 | Loss: 0.00204465
Iteration 16/25 | Loss: 0.00204194
Iteration 17/25 | Loss: 0.00204103
Iteration 18/25 | Loss: 0.00204060
Iteration 19/25 | Loss: 0.00204374
Iteration 20/25 | Loss: 0.00203973
Iteration 21/25 | Loss: 0.00203888
Iteration 22/25 | Loss: 0.00203803
Iteration 23/25 | Loss: 0.00204138
Iteration 24/25 | Loss: 0.00203688
Iteration 25/25 | Loss: 0.00203598

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37347448
Iteration 2/25 | Loss: 0.00514320
Iteration 3/25 | Loss: 0.00514319
Iteration 4/25 | Loss: 0.00514319
Iteration 5/25 | Loss: 0.00514319
Iteration 6/25 | Loss: 0.00514319
Iteration 7/25 | Loss: 0.00514319
Iteration 8/25 | Loss: 0.00514319
Iteration 9/25 | Loss: 0.00514319
Iteration 10/25 | Loss: 0.00514319
Iteration 11/25 | Loss: 0.00514319
Iteration 12/25 | Loss: 0.00514319
Iteration 13/25 | Loss: 0.00514319
Iteration 14/25 | Loss: 0.00514319
Iteration 15/25 | Loss: 0.00514319
Iteration 16/25 | Loss: 0.00514319
Iteration 17/25 | Loss: 0.00514319
Iteration 18/25 | Loss: 0.00514319
Iteration 19/25 | Loss: 0.00514319
Iteration 20/25 | Loss: 0.00514319
Iteration 21/25 | Loss: 0.00514319
Iteration 22/25 | Loss: 0.00514319
Iteration 23/25 | Loss: 0.00514319
Iteration 24/25 | Loss: 0.00514319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.005143190734088421, 0.005143190734088421, 0.005143190734088421, 0.005143190734088421, 0.005143190734088421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005143190734088421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00514319
Iteration 2/1000 | Loss: 0.00062784
Iteration 3/1000 | Loss: 0.00050172
Iteration 4/1000 | Loss: 0.00044399
Iteration 5/1000 | Loss: 0.00040086
Iteration 6/1000 | Loss: 0.00037023
Iteration 7/1000 | Loss: 0.00034806
Iteration 8/1000 | Loss: 0.00033473
Iteration 9/1000 | Loss: 0.00032478
Iteration 10/1000 | Loss: 0.00031796
Iteration 11/1000 | Loss: 0.00567051
Iteration 12/1000 | Loss: 0.01447684
Iteration 13/1000 | Loss: 0.00052324
Iteration 14/1000 | Loss: 0.00034006
Iteration 15/1000 | Loss: 0.00023701
Iteration 16/1000 | Loss: 0.00015758
Iteration 17/1000 | Loss: 0.00010467
Iteration 18/1000 | Loss: 0.00008330
Iteration 19/1000 | Loss: 0.00006285
Iteration 20/1000 | Loss: 0.00004867
Iteration 21/1000 | Loss: 0.00003849
Iteration 22/1000 | Loss: 0.00003367
Iteration 23/1000 | Loss: 0.00002977
Iteration 24/1000 | Loss: 0.00002704
Iteration 25/1000 | Loss: 0.00002519
Iteration 26/1000 | Loss: 0.00002360
Iteration 27/1000 | Loss: 0.00002215
Iteration 28/1000 | Loss: 0.00002108
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00001963
Iteration 31/1000 | Loss: 0.00001917
Iteration 32/1000 | Loss: 0.00001891
Iteration 33/1000 | Loss: 0.00001875
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001868
Iteration 36/1000 | Loss: 0.00001862
Iteration 37/1000 | Loss: 0.00001861
Iteration 38/1000 | Loss: 0.00001859
Iteration 39/1000 | Loss: 0.00001858
Iteration 40/1000 | Loss: 0.00001858
Iteration 41/1000 | Loss: 0.00001858
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001857
Iteration 44/1000 | Loss: 0.00001857
Iteration 45/1000 | Loss: 0.00001855
Iteration 46/1000 | Loss: 0.00001855
Iteration 47/1000 | Loss: 0.00001855
Iteration 48/1000 | Loss: 0.00001855
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001855
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001854
Iteration 53/1000 | Loss: 0.00001854
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001851
Iteration 57/1000 | Loss: 0.00001851
Iteration 58/1000 | Loss: 0.00001851
Iteration 59/1000 | Loss: 0.00001851
Iteration 60/1000 | Loss: 0.00001851
Iteration 61/1000 | Loss: 0.00001850
Iteration 62/1000 | Loss: 0.00001849
Iteration 63/1000 | Loss: 0.00001849
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001849
Iteration 67/1000 | Loss: 0.00001849
Iteration 68/1000 | Loss: 0.00001849
Iteration 69/1000 | Loss: 0.00001849
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.8489105059416033e-05, 1.8489105059416033e-05, 1.8489105059416033e-05, 1.8489105059416033e-05, 1.8489105059416033e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8489105059416033e-05

Optimization complete. Final v2v error: 3.688121795654297 mm

Highest mean error: 3.911003589630127 mm for frame 75

Lowest mean error: 3.583695650100708 mm for frame 189

Saving results

Total time: 105.140709400177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945421
Iteration 2/25 | Loss: 0.00354175
Iteration 3/25 | Loss: 0.00239855
Iteration 4/25 | Loss: 0.00220952
Iteration 5/25 | Loss: 0.00203445
Iteration 6/25 | Loss: 0.00193207
Iteration 7/25 | Loss: 0.00186517
Iteration 8/25 | Loss: 0.00182456
Iteration 9/25 | Loss: 0.00182688
Iteration 10/25 | Loss: 0.00173969
Iteration 11/25 | Loss: 0.00168360
Iteration 12/25 | Loss: 0.00166349
Iteration 13/25 | Loss: 0.00166158
Iteration 14/25 | Loss: 0.00163758
Iteration 15/25 | Loss: 0.00163163
Iteration 16/25 | Loss: 0.00162184
Iteration 17/25 | Loss: 0.00161280
Iteration 18/25 | Loss: 0.00160206
Iteration 19/25 | Loss: 0.00158217
Iteration 20/25 | Loss: 0.00157246
Iteration 21/25 | Loss: 0.00156359
Iteration 22/25 | Loss: 0.00156544
Iteration 23/25 | Loss: 0.00154573
Iteration 24/25 | Loss: 0.00152809
Iteration 25/25 | Loss: 0.00153543

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39555621
Iteration 2/25 | Loss: 0.00253487
Iteration 3/25 | Loss: 0.00246984
Iteration 4/25 | Loss: 0.00246984
Iteration 5/25 | Loss: 0.00246984
Iteration 6/25 | Loss: 0.00246984
Iteration 7/25 | Loss: 0.00246984
Iteration 8/25 | Loss: 0.00246984
Iteration 9/25 | Loss: 0.00246984
Iteration 10/25 | Loss: 0.00246984
Iteration 11/25 | Loss: 0.00246984
Iteration 12/25 | Loss: 0.00246984
Iteration 13/25 | Loss: 0.00246984
Iteration 14/25 | Loss: 0.00246984
Iteration 15/25 | Loss: 0.00246984
Iteration 16/25 | Loss: 0.00246984
Iteration 17/25 | Loss: 0.00246984
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0024698376655578613, 0.0024698376655578613, 0.0024698376655578613, 0.0024698376655578613, 0.0024698376655578613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024698376655578613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00246984
Iteration 2/1000 | Loss: 0.00162383
Iteration 3/1000 | Loss: 0.00030627
Iteration 4/1000 | Loss: 0.00209630
Iteration 5/1000 | Loss: 0.00043410
Iteration 6/1000 | Loss: 0.00017467
Iteration 7/1000 | Loss: 0.00054320
Iteration 8/1000 | Loss: 0.00052952
Iteration 9/1000 | Loss: 0.00070617
Iteration 10/1000 | Loss: 0.00029221
Iteration 11/1000 | Loss: 0.00020240
Iteration 12/1000 | Loss: 0.00014567
Iteration 13/1000 | Loss: 0.00070261
Iteration 14/1000 | Loss: 0.00198675
Iteration 15/1000 | Loss: 0.00062913
Iteration 16/1000 | Loss: 0.00049062
Iteration 17/1000 | Loss: 0.00014247
Iteration 18/1000 | Loss: 0.00081005
Iteration 19/1000 | Loss: 0.00134938
Iteration 20/1000 | Loss: 0.00132385
Iteration 21/1000 | Loss: 0.00106902
Iteration 22/1000 | Loss: 0.00047172
Iteration 23/1000 | Loss: 0.00022171
Iteration 24/1000 | Loss: 0.00032672
Iteration 25/1000 | Loss: 0.00130128
Iteration 26/1000 | Loss: 0.00091065
Iteration 27/1000 | Loss: 0.00347541
Iteration 28/1000 | Loss: 0.00108852
Iteration 29/1000 | Loss: 0.00129538
Iteration 30/1000 | Loss: 0.00021837
Iteration 31/1000 | Loss: 0.00084829
Iteration 32/1000 | Loss: 0.00015195
Iteration 33/1000 | Loss: 0.00034382
Iteration 34/1000 | Loss: 0.00149192
Iteration 35/1000 | Loss: 0.00012260
Iteration 36/1000 | Loss: 0.00029839
Iteration 37/1000 | Loss: 0.00018564
Iteration 38/1000 | Loss: 0.00010076
Iteration 39/1000 | Loss: 0.00082859
Iteration 40/1000 | Loss: 0.00022798
Iteration 41/1000 | Loss: 0.00034372
Iteration 42/1000 | Loss: 0.00100891
Iteration 43/1000 | Loss: 0.00080526
Iteration 44/1000 | Loss: 0.00033239
Iteration 45/1000 | Loss: 0.00011117
Iteration 46/1000 | Loss: 0.00013537
Iteration 47/1000 | Loss: 0.00010715
Iteration 48/1000 | Loss: 0.00008901
Iteration 49/1000 | Loss: 0.00032594
Iteration 50/1000 | Loss: 0.00037636
Iteration 51/1000 | Loss: 0.00034844
Iteration 52/1000 | Loss: 0.00033598
Iteration 53/1000 | Loss: 0.00008004
Iteration 54/1000 | Loss: 0.00007285
Iteration 55/1000 | Loss: 0.00033393
Iteration 56/1000 | Loss: 0.00008722
Iteration 57/1000 | Loss: 0.00032147
Iteration 58/1000 | Loss: 0.00033027
Iteration 59/1000 | Loss: 0.00040479
Iteration 60/1000 | Loss: 0.00033172
Iteration 61/1000 | Loss: 0.00009469
Iteration 62/1000 | Loss: 0.00031847
Iteration 63/1000 | Loss: 0.00051283
Iteration 64/1000 | Loss: 0.00088348
Iteration 65/1000 | Loss: 0.00045575
Iteration 66/1000 | Loss: 0.00053299
Iteration 67/1000 | Loss: 0.00038662
Iteration 68/1000 | Loss: 0.00008613
Iteration 69/1000 | Loss: 0.00007105
Iteration 70/1000 | Loss: 0.00007282
Iteration 71/1000 | Loss: 0.00052269
Iteration 72/1000 | Loss: 0.00027332
Iteration 73/1000 | Loss: 0.00005999
Iteration 74/1000 | Loss: 0.00005383
Iteration 75/1000 | Loss: 0.00004988
Iteration 76/1000 | Loss: 0.00004698
Iteration 77/1000 | Loss: 0.00004562
Iteration 78/1000 | Loss: 0.00005328
Iteration 79/1000 | Loss: 0.00004468
Iteration 80/1000 | Loss: 0.00004306
Iteration 81/1000 | Loss: 0.00004201
Iteration 82/1000 | Loss: 0.00004105
Iteration 83/1000 | Loss: 0.00004067
Iteration 84/1000 | Loss: 0.00004002
Iteration 85/1000 | Loss: 0.00003928
Iteration 86/1000 | Loss: 0.00003883
Iteration 87/1000 | Loss: 0.00003845
Iteration 88/1000 | Loss: 0.00034194
Iteration 89/1000 | Loss: 0.00009321
Iteration 90/1000 | Loss: 0.00014376
Iteration 91/1000 | Loss: 0.00054213
Iteration 92/1000 | Loss: 0.00006308
Iteration 93/1000 | Loss: 0.00004970
Iteration 94/1000 | Loss: 0.00004359
Iteration 95/1000 | Loss: 0.00004094
Iteration 96/1000 | Loss: 0.00005284
Iteration 97/1000 | Loss: 0.00003908
Iteration 98/1000 | Loss: 0.00011965
Iteration 99/1000 | Loss: 0.00004686
Iteration 100/1000 | Loss: 0.00004105
Iteration 101/1000 | Loss: 0.00003834
Iteration 102/1000 | Loss: 0.00003776
Iteration 103/1000 | Loss: 0.00003758
Iteration 104/1000 | Loss: 0.00009320
Iteration 105/1000 | Loss: 0.00004046
Iteration 106/1000 | Loss: 0.00004132
Iteration 107/1000 | Loss: 0.00003717
Iteration 108/1000 | Loss: 0.00003716
Iteration 109/1000 | Loss: 0.00003716
Iteration 110/1000 | Loss: 0.00003716
Iteration 111/1000 | Loss: 0.00003715
Iteration 112/1000 | Loss: 0.00003713
Iteration 113/1000 | Loss: 0.00005815
Iteration 114/1000 | Loss: 0.00003704
Iteration 115/1000 | Loss: 0.00003700
Iteration 116/1000 | Loss: 0.00003697
Iteration 117/1000 | Loss: 0.00003696
Iteration 118/1000 | Loss: 0.00003696
Iteration 119/1000 | Loss: 0.00003696
Iteration 120/1000 | Loss: 0.00003696
Iteration 121/1000 | Loss: 0.00003696
Iteration 122/1000 | Loss: 0.00003696
Iteration 123/1000 | Loss: 0.00003696
Iteration 124/1000 | Loss: 0.00003696
Iteration 125/1000 | Loss: 0.00003696
Iteration 126/1000 | Loss: 0.00003696
Iteration 127/1000 | Loss: 0.00003696
Iteration 128/1000 | Loss: 0.00003696
Iteration 129/1000 | Loss: 0.00003695
Iteration 130/1000 | Loss: 0.00003695
Iteration 131/1000 | Loss: 0.00003693
Iteration 132/1000 | Loss: 0.00003692
Iteration 133/1000 | Loss: 0.00003692
Iteration 134/1000 | Loss: 0.00003691
Iteration 135/1000 | Loss: 0.00003691
Iteration 136/1000 | Loss: 0.00003691
Iteration 137/1000 | Loss: 0.00003690
Iteration 138/1000 | Loss: 0.00003690
Iteration 139/1000 | Loss: 0.00003687
Iteration 140/1000 | Loss: 0.00003686
Iteration 141/1000 | Loss: 0.00003685
Iteration 142/1000 | Loss: 0.00003680
Iteration 143/1000 | Loss: 0.00003679
Iteration 144/1000 | Loss: 0.00003678
Iteration 145/1000 | Loss: 0.00003678
Iteration 146/1000 | Loss: 0.00003677
Iteration 147/1000 | Loss: 0.00003677
Iteration 148/1000 | Loss: 0.00003677
Iteration 149/1000 | Loss: 0.00003677
Iteration 150/1000 | Loss: 0.00003676
Iteration 151/1000 | Loss: 0.00003676
Iteration 152/1000 | Loss: 0.00003676
Iteration 153/1000 | Loss: 0.00003675
Iteration 154/1000 | Loss: 0.00003675
Iteration 155/1000 | Loss: 0.00003675
Iteration 156/1000 | Loss: 0.00003675
Iteration 157/1000 | Loss: 0.00003674
Iteration 158/1000 | Loss: 0.00003674
Iteration 159/1000 | Loss: 0.00003674
Iteration 160/1000 | Loss: 0.00003674
Iteration 161/1000 | Loss: 0.00003673
Iteration 162/1000 | Loss: 0.00003673
Iteration 163/1000 | Loss: 0.00003673
Iteration 164/1000 | Loss: 0.00003673
Iteration 165/1000 | Loss: 0.00003673
Iteration 166/1000 | Loss: 0.00003673
Iteration 167/1000 | Loss: 0.00003673
Iteration 168/1000 | Loss: 0.00003673
Iteration 169/1000 | Loss: 0.00003673
Iteration 170/1000 | Loss: 0.00003673
Iteration 171/1000 | Loss: 0.00003672
Iteration 172/1000 | Loss: 0.00003672
Iteration 173/1000 | Loss: 0.00003672
Iteration 174/1000 | Loss: 0.00003672
Iteration 175/1000 | Loss: 0.00003672
Iteration 176/1000 | Loss: 0.00003672
Iteration 177/1000 | Loss: 0.00003672
Iteration 178/1000 | Loss: 0.00003672
Iteration 179/1000 | Loss: 0.00003672
Iteration 180/1000 | Loss: 0.00003672
Iteration 181/1000 | Loss: 0.00003672
Iteration 182/1000 | Loss: 0.00003672
Iteration 183/1000 | Loss: 0.00003671
Iteration 184/1000 | Loss: 0.00003671
Iteration 185/1000 | Loss: 0.00003671
Iteration 186/1000 | Loss: 0.00003671
Iteration 187/1000 | Loss: 0.00003671
Iteration 188/1000 | Loss: 0.00003671
Iteration 189/1000 | Loss: 0.00003671
Iteration 190/1000 | Loss: 0.00003670
Iteration 191/1000 | Loss: 0.00003670
Iteration 192/1000 | Loss: 0.00003670
Iteration 193/1000 | Loss: 0.00003670
Iteration 194/1000 | Loss: 0.00003670
Iteration 195/1000 | Loss: 0.00003670
Iteration 196/1000 | Loss: 0.00003669
Iteration 197/1000 | Loss: 0.00003669
Iteration 198/1000 | Loss: 0.00003669
Iteration 199/1000 | Loss: 0.00003669
Iteration 200/1000 | Loss: 0.00003669
Iteration 201/1000 | Loss: 0.00003669
Iteration 202/1000 | Loss: 0.00003669
Iteration 203/1000 | Loss: 0.00003669
Iteration 204/1000 | Loss: 0.00003668
Iteration 205/1000 | Loss: 0.00003668
Iteration 206/1000 | Loss: 0.00003668
Iteration 207/1000 | Loss: 0.00003668
Iteration 208/1000 | Loss: 0.00003668
Iteration 209/1000 | Loss: 0.00003668
Iteration 210/1000 | Loss: 0.00003668
Iteration 211/1000 | Loss: 0.00003668
Iteration 212/1000 | Loss: 0.00003668
Iteration 213/1000 | Loss: 0.00003668
Iteration 214/1000 | Loss: 0.00003668
Iteration 215/1000 | Loss: 0.00003667
Iteration 216/1000 | Loss: 0.00003667
Iteration 217/1000 | Loss: 0.00003667
Iteration 218/1000 | Loss: 0.00003667
Iteration 219/1000 | Loss: 0.00003667
Iteration 220/1000 | Loss: 0.00003667
Iteration 221/1000 | Loss: 0.00003667
Iteration 222/1000 | Loss: 0.00003667
Iteration 223/1000 | Loss: 0.00003667
Iteration 224/1000 | Loss: 0.00003667
Iteration 225/1000 | Loss: 0.00003667
Iteration 226/1000 | Loss: 0.00003667
Iteration 227/1000 | Loss: 0.00003667
Iteration 228/1000 | Loss: 0.00003667
Iteration 229/1000 | Loss: 0.00003667
Iteration 230/1000 | Loss: 0.00003667
Iteration 231/1000 | Loss: 0.00003667
Iteration 232/1000 | Loss: 0.00003667
Iteration 233/1000 | Loss: 0.00003667
Iteration 234/1000 | Loss: 0.00003667
Iteration 235/1000 | Loss: 0.00003667
Iteration 236/1000 | Loss: 0.00003667
Iteration 237/1000 | Loss: 0.00003667
Iteration 238/1000 | Loss: 0.00003667
Iteration 239/1000 | Loss: 0.00003667
Iteration 240/1000 | Loss: 0.00003667
Iteration 241/1000 | Loss: 0.00003667
Iteration 242/1000 | Loss: 0.00003667
Iteration 243/1000 | Loss: 0.00003667
Iteration 244/1000 | Loss: 0.00003667
Iteration 245/1000 | Loss: 0.00003667
Iteration 246/1000 | Loss: 0.00003667
Iteration 247/1000 | Loss: 0.00003667
Iteration 248/1000 | Loss: 0.00003667
Iteration 249/1000 | Loss: 0.00003667
Iteration 250/1000 | Loss: 0.00003667
Iteration 251/1000 | Loss: 0.00003667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 251. Stopping optimization.
Last 5 losses: [3.666978227556683e-05, 3.666978227556683e-05, 3.666978227556683e-05, 3.666978227556683e-05, 3.666978227556683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.666978227556683e-05

Optimization complete. Final v2v error: 4.04703426361084 mm

Highest mean error: 11.729793548583984 mm for frame 129

Lowest mean error: 3.378704786300659 mm for frame 8

Saving results

Total time: 210.17658472061157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_christine_posed_013/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_christine_posed_013/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00705921
Iteration 2/25 | Loss: 0.00153901
Iteration 3/25 | Loss: 0.00143178
Iteration 4/25 | Loss: 0.00142402
Iteration 5/25 | Loss: 0.00142214
Iteration 6/25 | Loss: 0.00142214
Iteration 7/25 | Loss: 0.00142214
Iteration 8/25 | Loss: 0.00142214
Iteration 9/25 | Loss: 0.00142214
Iteration 10/25 | Loss: 0.00142214
Iteration 11/25 | Loss: 0.00142214
Iteration 12/25 | Loss: 0.00142214
Iteration 13/25 | Loss: 0.00142214
Iteration 14/25 | Loss: 0.00142214
Iteration 15/25 | Loss: 0.00142214
Iteration 16/25 | Loss: 0.00142214
Iteration 17/25 | Loss: 0.00142214
Iteration 18/25 | Loss: 0.00142214
Iteration 19/25 | Loss: 0.00142214
Iteration 20/25 | Loss: 0.00142214
Iteration 21/25 | Loss: 0.00142214
Iteration 22/25 | Loss: 0.00142214
Iteration 23/25 | Loss: 0.00142214
Iteration 24/25 | Loss: 0.00142214
Iteration 25/25 | Loss: 0.00142214

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.84804535
Iteration 2/25 | Loss: 0.00092520
Iteration 3/25 | Loss: 0.00092520
Iteration 4/25 | Loss: 0.00092520
Iteration 5/25 | Loss: 0.00092520
Iteration 6/25 | Loss: 0.00092520
Iteration 7/25 | Loss: 0.00092519
Iteration 8/25 | Loss: 0.00092519
Iteration 9/25 | Loss: 0.00092519
Iteration 10/25 | Loss: 0.00092519
Iteration 11/25 | Loss: 0.00092519
Iteration 12/25 | Loss: 0.00092519
Iteration 13/25 | Loss: 0.00092519
Iteration 14/25 | Loss: 0.00092519
Iteration 15/25 | Loss: 0.00092519
Iteration 16/25 | Loss: 0.00092519
Iteration 17/25 | Loss: 0.00092519
Iteration 18/25 | Loss: 0.00092519
Iteration 19/25 | Loss: 0.00092519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009251941810362041, 0.0009251941810362041, 0.0009251941810362041, 0.0009251941810362041, 0.0009251941810362041]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009251941810362041

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092519
Iteration 2/1000 | Loss: 0.00004880
Iteration 3/1000 | Loss: 0.00003221
Iteration 4/1000 | Loss: 0.00002739
Iteration 5/1000 | Loss: 0.00002599
Iteration 6/1000 | Loss: 0.00002512
Iteration 7/1000 | Loss: 0.00002432
Iteration 8/1000 | Loss: 0.00002383
Iteration 9/1000 | Loss: 0.00002353
Iteration 10/1000 | Loss: 0.00002313
Iteration 11/1000 | Loss: 0.00002297
Iteration 12/1000 | Loss: 0.00002275
Iteration 13/1000 | Loss: 0.00002260
Iteration 14/1000 | Loss: 0.00002244
Iteration 15/1000 | Loss: 0.00002243
Iteration 16/1000 | Loss: 0.00002239
Iteration 17/1000 | Loss: 0.00002237
Iteration 18/1000 | Loss: 0.00002226
Iteration 19/1000 | Loss: 0.00002218
Iteration 20/1000 | Loss: 0.00002217
Iteration 21/1000 | Loss: 0.00002216
Iteration 22/1000 | Loss: 0.00002216
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002215
Iteration 25/1000 | Loss: 0.00002214
Iteration 26/1000 | Loss: 0.00002214
Iteration 27/1000 | Loss: 0.00002214
Iteration 28/1000 | Loss: 0.00002214
Iteration 29/1000 | Loss: 0.00002214
Iteration 30/1000 | Loss: 0.00002213
Iteration 31/1000 | Loss: 0.00002213
Iteration 32/1000 | Loss: 0.00002211
Iteration 33/1000 | Loss: 0.00002211
Iteration 34/1000 | Loss: 0.00002207
Iteration 35/1000 | Loss: 0.00002204
Iteration 36/1000 | Loss: 0.00002203
Iteration 37/1000 | Loss: 0.00002200
Iteration 38/1000 | Loss: 0.00002199
Iteration 39/1000 | Loss: 0.00002197
Iteration 40/1000 | Loss: 0.00002197
Iteration 41/1000 | Loss: 0.00002196
Iteration 42/1000 | Loss: 0.00002196
Iteration 43/1000 | Loss: 0.00002195
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00002194
Iteration 46/1000 | Loss: 0.00002194
Iteration 47/1000 | Loss: 0.00002194
Iteration 48/1000 | Loss: 0.00002193
Iteration 49/1000 | Loss: 0.00002193
Iteration 50/1000 | Loss: 0.00002193
Iteration 51/1000 | Loss: 0.00002192
Iteration 52/1000 | Loss: 0.00002192
Iteration 53/1000 | Loss: 0.00002191
Iteration 54/1000 | Loss: 0.00002191
Iteration 55/1000 | Loss: 0.00002190
Iteration 56/1000 | Loss: 0.00002190
Iteration 57/1000 | Loss: 0.00002189
Iteration 58/1000 | Loss: 0.00002189
Iteration 59/1000 | Loss: 0.00002189
Iteration 60/1000 | Loss: 0.00002189
Iteration 61/1000 | Loss: 0.00002188
Iteration 62/1000 | Loss: 0.00002188
Iteration 63/1000 | Loss: 0.00002187
Iteration 64/1000 | Loss: 0.00002187
Iteration 65/1000 | Loss: 0.00002187
Iteration 66/1000 | Loss: 0.00002187
Iteration 67/1000 | Loss: 0.00002186
Iteration 68/1000 | Loss: 0.00002186
Iteration 69/1000 | Loss: 0.00002186
Iteration 70/1000 | Loss: 0.00002186
Iteration 71/1000 | Loss: 0.00002186
Iteration 72/1000 | Loss: 0.00002185
Iteration 73/1000 | Loss: 0.00002185
Iteration 74/1000 | Loss: 0.00002185
Iteration 75/1000 | Loss: 0.00002185
Iteration 76/1000 | Loss: 0.00002185
Iteration 77/1000 | Loss: 0.00002185
Iteration 78/1000 | Loss: 0.00002185
Iteration 79/1000 | Loss: 0.00002185
Iteration 80/1000 | Loss: 0.00002185
Iteration 81/1000 | Loss: 0.00002185
Iteration 82/1000 | Loss: 0.00002185
Iteration 83/1000 | Loss: 0.00002184
Iteration 84/1000 | Loss: 0.00002184
Iteration 85/1000 | Loss: 0.00002184
Iteration 86/1000 | Loss: 0.00002184
Iteration 87/1000 | Loss: 0.00002184
Iteration 88/1000 | Loss: 0.00002184
Iteration 89/1000 | Loss: 0.00002184
Iteration 90/1000 | Loss: 0.00002183
Iteration 91/1000 | Loss: 0.00002183
Iteration 92/1000 | Loss: 0.00002183
Iteration 93/1000 | Loss: 0.00002183
Iteration 94/1000 | Loss: 0.00002183
Iteration 95/1000 | Loss: 0.00002183
Iteration 96/1000 | Loss: 0.00002183
Iteration 97/1000 | Loss: 0.00002183
Iteration 98/1000 | Loss: 0.00002183
Iteration 99/1000 | Loss: 0.00002183
Iteration 100/1000 | Loss: 0.00002183
Iteration 101/1000 | Loss: 0.00002183
Iteration 102/1000 | Loss: 0.00002183
Iteration 103/1000 | Loss: 0.00002182
Iteration 104/1000 | Loss: 0.00002182
Iteration 105/1000 | Loss: 0.00002182
Iteration 106/1000 | Loss: 0.00002182
Iteration 107/1000 | Loss: 0.00002182
Iteration 108/1000 | Loss: 0.00002182
Iteration 109/1000 | Loss: 0.00002182
Iteration 110/1000 | Loss: 0.00002182
Iteration 111/1000 | Loss: 0.00002182
Iteration 112/1000 | Loss: 0.00002182
Iteration 113/1000 | Loss: 0.00002182
Iteration 114/1000 | Loss: 0.00002182
Iteration 115/1000 | Loss: 0.00002182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [2.1820718757226132e-05, 2.1820718757226132e-05, 2.1820718757226132e-05, 2.1820718757226132e-05, 2.1820718757226132e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1820718757226132e-05

Optimization complete. Final v2v error: 3.882380962371826 mm

Highest mean error: 4.344076633453369 mm for frame 198

Lowest mean error: 3.5627753734588623 mm for frame 233

Saving results

Total time: 42.62808012962341
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797383
Iteration 2/25 | Loss: 0.00132263
Iteration 3/25 | Loss: 0.00116837
Iteration 4/25 | Loss: 0.00115362
Iteration 5/25 | Loss: 0.00114878
Iteration 6/25 | Loss: 0.00114781
Iteration 7/25 | Loss: 0.00114781
Iteration 8/25 | Loss: 0.00114781
Iteration 9/25 | Loss: 0.00114772
Iteration 10/25 | Loss: 0.00114772
Iteration 11/25 | Loss: 0.00114772
Iteration 12/25 | Loss: 0.00114772
Iteration 13/25 | Loss: 0.00114772
Iteration 14/25 | Loss: 0.00114772
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011477163061499596, 0.0011477163061499596, 0.0011477163061499596, 0.0011477163061499596, 0.0011477163061499596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011477163061499596

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30849183
Iteration 2/25 | Loss: 0.00060342
Iteration 3/25 | Loss: 0.00060337
Iteration 4/25 | Loss: 0.00060336
Iteration 5/25 | Loss: 0.00060336
Iteration 6/25 | Loss: 0.00060336
Iteration 7/25 | Loss: 0.00060336
Iteration 8/25 | Loss: 0.00060336
Iteration 9/25 | Loss: 0.00060336
Iteration 10/25 | Loss: 0.00060336
Iteration 11/25 | Loss: 0.00060336
Iteration 12/25 | Loss: 0.00060336
Iteration 13/25 | Loss: 0.00060336
Iteration 14/25 | Loss: 0.00060336
Iteration 15/25 | Loss: 0.00060336
Iteration 16/25 | Loss: 0.00060336
Iteration 17/25 | Loss: 0.00060336
Iteration 18/25 | Loss: 0.00060336
Iteration 19/25 | Loss: 0.00060336
Iteration 20/25 | Loss: 0.00060336
Iteration 21/25 | Loss: 0.00060336
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006033616373315454, 0.0006033616373315454, 0.0006033616373315454, 0.0006033616373315454, 0.0006033616373315454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006033616373315454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060336
Iteration 2/1000 | Loss: 0.00004097
Iteration 3/1000 | Loss: 0.00002586
Iteration 4/1000 | Loss: 0.00002204
Iteration 5/1000 | Loss: 0.00002044
Iteration 6/1000 | Loss: 0.00001928
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00001801
Iteration 9/1000 | Loss: 0.00001751
Iteration 10/1000 | Loss: 0.00001731
Iteration 11/1000 | Loss: 0.00001712
Iteration 12/1000 | Loss: 0.00001706
Iteration 13/1000 | Loss: 0.00001689
Iteration 14/1000 | Loss: 0.00001689
Iteration 15/1000 | Loss: 0.00001681
Iteration 16/1000 | Loss: 0.00001679
Iteration 17/1000 | Loss: 0.00001679
Iteration 18/1000 | Loss: 0.00001678
Iteration 19/1000 | Loss: 0.00001678
Iteration 20/1000 | Loss: 0.00001672
Iteration 21/1000 | Loss: 0.00001668
Iteration 22/1000 | Loss: 0.00001664
Iteration 23/1000 | Loss: 0.00001664
Iteration 24/1000 | Loss: 0.00001663
Iteration 25/1000 | Loss: 0.00001662
Iteration 26/1000 | Loss: 0.00001661
Iteration 27/1000 | Loss: 0.00001661
Iteration 28/1000 | Loss: 0.00001658
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001656
Iteration 31/1000 | Loss: 0.00001656
Iteration 32/1000 | Loss: 0.00001655
Iteration 33/1000 | Loss: 0.00001655
Iteration 34/1000 | Loss: 0.00001655
Iteration 35/1000 | Loss: 0.00001654
Iteration 36/1000 | Loss: 0.00001654
Iteration 37/1000 | Loss: 0.00001653
Iteration 38/1000 | Loss: 0.00001651
Iteration 39/1000 | Loss: 0.00001650
Iteration 40/1000 | Loss: 0.00001650
Iteration 41/1000 | Loss: 0.00001649
Iteration 42/1000 | Loss: 0.00001649
Iteration 43/1000 | Loss: 0.00001649
Iteration 44/1000 | Loss: 0.00001648
Iteration 45/1000 | Loss: 0.00001648
Iteration 46/1000 | Loss: 0.00001648
Iteration 47/1000 | Loss: 0.00001647
Iteration 48/1000 | Loss: 0.00001647
Iteration 49/1000 | Loss: 0.00001647
Iteration 50/1000 | Loss: 0.00001646
Iteration 51/1000 | Loss: 0.00001646
Iteration 52/1000 | Loss: 0.00001645
Iteration 53/1000 | Loss: 0.00001645
Iteration 54/1000 | Loss: 0.00001645
Iteration 55/1000 | Loss: 0.00001644
Iteration 56/1000 | Loss: 0.00001644
Iteration 57/1000 | Loss: 0.00001644
Iteration 58/1000 | Loss: 0.00001643
Iteration 59/1000 | Loss: 0.00001643
Iteration 60/1000 | Loss: 0.00001643
Iteration 61/1000 | Loss: 0.00001642
Iteration 62/1000 | Loss: 0.00001642
Iteration 63/1000 | Loss: 0.00001642
Iteration 64/1000 | Loss: 0.00001642
Iteration 65/1000 | Loss: 0.00001641
Iteration 66/1000 | Loss: 0.00001641
Iteration 67/1000 | Loss: 0.00001641
Iteration 68/1000 | Loss: 0.00001641
Iteration 69/1000 | Loss: 0.00001641
Iteration 70/1000 | Loss: 0.00001641
Iteration 71/1000 | Loss: 0.00001640
Iteration 72/1000 | Loss: 0.00001640
Iteration 73/1000 | Loss: 0.00001640
Iteration 74/1000 | Loss: 0.00001639
Iteration 75/1000 | Loss: 0.00001639
Iteration 76/1000 | Loss: 0.00001639
Iteration 77/1000 | Loss: 0.00001638
Iteration 78/1000 | Loss: 0.00001638
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001637
Iteration 81/1000 | Loss: 0.00001637
Iteration 82/1000 | Loss: 0.00001637
Iteration 83/1000 | Loss: 0.00001637
Iteration 84/1000 | Loss: 0.00001637
Iteration 85/1000 | Loss: 0.00001637
Iteration 86/1000 | Loss: 0.00001637
Iteration 87/1000 | Loss: 0.00001637
Iteration 88/1000 | Loss: 0.00001637
Iteration 89/1000 | Loss: 0.00001637
Iteration 90/1000 | Loss: 0.00001636
Iteration 91/1000 | Loss: 0.00001636
Iteration 92/1000 | Loss: 0.00001636
Iteration 93/1000 | Loss: 0.00001636
Iteration 94/1000 | Loss: 0.00001636
Iteration 95/1000 | Loss: 0.00001636
Iteration 96/1000 | Loss: 0.00001636
Iteration 97/1000 | Loss: 0.00001636
Iteration 98/1000 | Loss: 0.00001636
Iteration 99/1000 | Loss: 0.00001636
Iteration 100/1000 | Loss: 0.00001636
Iteration 101/1000 | Loss: 0.00001636
Iteration 102/1000 | Loss: 0.00001636
Iteration 103/1000 | Loss: 0.00001636
Iteration 104/1000 | Loss: 0.00001636
Iteration 105/1000 | Loss: 0.00001636
Iteration 106/1000 | Loss: 0.00001636
Iteration 107/1000 | Loss: 0.00001636
Iteration 108/1000 | Loss: 0.00001636
Iteration 109/1000 | Loss: 0.00001636
Iteration 110/1000 | Loss: 0.00001636
Iteration 111/1000 | Loss: 0.00001636
Iteration 112/1000 | Loss: 0.00001636
Iteration 113/1000 | Loss: 0.00001636
Iteration 114/1000 | Loss: 0.00001636
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.6364248949685134e-05, 1.6364248949685134e-05, 1.6364248949685134e-05, 1.6364248949685134e-05, 1.6364248949685134e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6364248949685134e-05

Optimization complete. Final v2v error: 3.351933717727661 mm

Highest mean error: 4.561877727508545 mm for frame 52

Lowest mean error: 2.710344076156616 mm for frame 83

Saving results

Total time: 35.41764712333679
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788213
Iteration 2/25 | Loss: 0.00134078
Iteration 3/25 | Loss: 0.00120750
Iteration 4/25 | Loss: 0.00116721
Iteration 5/25 | Loss: 0.00116431
Iteration 6/25 | Loss: 0.00116323
Iteration 7/25 | Loss: 0.00116240
Iteration 8/25 | Loss: 0.00117181
Iteration 9/25 | Loss: 0.00118422
Iteration 10/25 | Loss: 0.00116062
Iteration 11/25 | Loss: 0.00115366
Iteration 12/25 | Loss: 0.00115289
Iteration 13/25 | Loss: 0.00115211
Iteration 14/25 | Loss: 0.00115127
Iteration 15/25 | Loss: 0.00115076
Iteration 16/25 | Loss: 0.00115064
Iteration 17/25 | Loss: 0.00115063
Iteration 18/25 | Loss: 0.00115063
Iteration 19/25 | Loss: 0.00115063
Iteration 20/25 | Loss: 0.00115063
Iteration 21/25 | Loss: 0.00115063
Iteration 22/25 | Loss: 0.00115063
Iteration 23/25 | Loss: 0.00115062
Iteration 24/25 | Loss: 0.00115061
Iteration 25/25 | Loss: 0.00115060

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.93803930
Iteration 2/25 | Loss: 0.00058737
Iteration 3/25 | Loss: 0.00058730
Iteration 4/25 | Loss: 0.00058730
Iteration 5/25 | Loss: 0.00058730
Iteration 6/25 | Loss: 0.00058730
Iteration 7/25 | Loss: 0.00058730
Iteration 8/25 | Loss: 0.00058730
Iteration 9/25 | Loss: 0.00058730
Iteration 10/25 | Loss: 0.00058730
Iteration 11/25 | Loss: 0.00058730
Iteration 12/25 | Loss: 0.00058730
Iteration 13/25 | Loss: 0.00058730
Iteration 14/25 | Loss: 0.00058730
Iteration 15/25 | Loss: 0.00058730
Iteration 16/25 | Loss: 0.00058730
Iteration 17/25 | Loss: 0.00058730
Iteration 18/25 | Loss: 0.00058730
Iteration 19/25 | Loss: 0.00058730
Iteration 20/25 | Loss: 0.00058730
Iteration 21/25 | Loss: 0.00058730
Iteration 22/25 | Loss: 0.00058730
Iteration 23/25 | Loss: 0.00058730
Iteration 24/25 | Loss: 0.00058729
Iteration 25/25 | Loss: 0.00058730

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058729
Iteration 2/1000 | Loss: 0.00003105
Iteration 3/1000 | Loss: 0.00002003
Iteration 4/1000 | Loss: 0.00001794
Iteration 5/1000 | Loss: 0.00001700
Iteration 6/1000 | Loss: 0.00001658
Iteration 7/1000 | Loss: 0.00001609
Iteration 8/1000 | Loss: 0.00001587
Iteration 9/1000 | Loss: 0.00001563
Iteration 10/1000 | Loss: 0.00001541
Iteration 11/1000 | Loss: 0.00001526
Iteration 12/1000 | Loss: 0.00001516
Iteration 13/1000 | Loss: 0.00001512
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001503
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001502
Iteration 18/1000 | Loss: 0.00001501
Iteration 19/1000 | Loss: 0.00001501
Iteration 20/1000 | Loss: 0.00001500
Iteration 21/1000 | Loss: 0.00001500
Iteration 22/1000 | Loss: 0.00001499
Iteration 23/1000 | Loss: 0.00001499
Iteration 24/1000 | Loss: 0.00001499
Iteration 25/1000 | Loss: 0.00001498
Iteration 26/1000 | Loss: 0.00001498
Iteration 27/1000 | Loss: 0.00001497
Iteration 28/1000 | Loss: 0.00001495
Iteration 29/1000 | Loss: 0.00001495
Iteration 30/1000 | Loss: 0.00001495
Iteration 31/1000 | Loss: 0.00001494
Iteration 32/1000 | Loss: 0.00001493
Iteration 33/1000 | Loss: 0.00001493
Iteration 34/1000 | Loss: 0.00001493
Iteration 35/1000 | Loss: 0.00001493
Iteration 36/1000 | Loss: 0.00001493
Iteration 37/1000 | Loss: 0.00001493
Iteration 38/1000 | Loss: 0.00001493
Iteration 39/1000 | Loss: 0.00001493
Iteration 40/1000 | Loss: 0.00001492
Iteration 41/1000 | Loss: 0.00001492
Iteration 42/1000 | Loss: 0.00001492
Iteration 43/1000 | Loss: 0.00001492
Iteration 44/1000 | Loss: 0.00001492
Iteration 45/1000 | Loss: 0.00001492
Iteration 46/1000 | Loss: 0.00001492
Iteration 47/1000 | Loss: 0.00001492
Iteration 48/1000 | Loss: 0.00001492
Iteration 49/1000 | Loss: 0.00001492
Iteration 50/1000 | Loss: 0.00001492
Iteration 51/1000 | Loss: 0.00001492
Iteration 52/1000 | Loss: 0.00001491
Iteration 53/1000 | Loss: 0.00001491
Iteration 54/1000 | Loss: 0.00001491
Iteration 55/1000 | Loss: 0.00001491
Iteration 56/1000 | Loss: 0.00001491
Iteration 57/1000 | Loss: 0.00001489
Iteration 58/1000 | Loss: 0.00001489
Iteration 59/1000 | Loss: 0.00001488
Iteration 60/1000 | Loss: 0.00001488
Iteration 61/1000 | Loss: 0.00001488
Iteration 62/1000 | Loss: 0.00001488
Iteration 63/1000 | Loss: 0.00001488
Iteration 64/1000 | Loss: 0.00001488
Iteration 65/1000 | Loss: 0.00001487
Iteration 66/1000 | Loss: 0.00001487
Iteration 67/1000 | Loss: 0.00001486
Iteration 68/1000 | Loss: 0.00001486
Iteration 69/1000 | Loss: 0.00001486
Iteration 70/1000 | Loss: 0.00001486
Iteration 71/1000 | Loss: 0.00001485
Iteration 72/1000 | Loss: 0.00001485
Iteration 73/1000 | Loss: 0.00001485
Iteration 74/1000 | Loss: 0.00001485
Iteration 75/1000 | Loss: 0.00001484
Iteration 76/1000 | Loss: 0.00001484
Iteration 77/1000 | Loss: 0.00001484
Iteration 78/1000 | Loss: 0.00001483
Iteration 79/1000 | Loss: 0.00001483
Iteration 80/1000 | Loss: 0.00001482
Iteration 81/1000 | Loss: 0.00001482
Iteration 82/1000 | Loss: 0.00001482
Iteration 83/1000 | Loss: 0.00001481
Iteration 84/1000 | Loss: 0.00001481
Iteration 85/1000 | Loss: 0.00001481
Iteration 86/1000 | Loss: 0.00001480
Iteration 87/1000 | Loss: 0.00001480
Iteration 88/1000 | Loss: 0.00001479
Iteration 89/1000 | Loss: 0.00001479
Iteration 90/1000 | Loss: 0.00001479
Iteration 91/1000 | Loss: 0.00001479
Iteration 92/1000 | Loss: 0.00001478
Iteration 93/1000 | Loss: 0.00001478
Iteration 94/1000 | Loss: 0.00001478
Iteration 95/1000 | Loss: 0.00001477
Iteration 96/1000 | Loss: 0.00001477
Iteration 97/1000 | Loss: 0.00001477
Iteration 98/1000 | Loss: 0.00001477
Iteration 99/1000 | Loss: 0.00001477
Iteration 100/1000 | Loss: 0.00001476
Iteration 101/1000 | Loss: 0.00001476
Iteration 102/1000 | Loss: 0.00001476
Iteration 103/1000 | Loss: 0.00001476
Iteration 104/1000 | Loss: 0.00001476
Iteration 105/1000 | Loss: 0.00001475
Iteration 106/1000 | Loss: 0.00001475
Iteration 107/1000 | Loss: 0.00001475
Iteration 108/1000 | Loss: 0.00001474
Iteration 109/1000 | Loss: 0.00001474
Iteration 110/1000 | Loss: 0.00001473
Iteration 111/1000 | Loss: 0.00001473
Iteration 112/1000 | Loss: 0.00001473
Iteration 113/1000 | Loss: 0.00001473
Iteration 114/1000 | Loss: 0.00001473
Iteration 115/1000 | Loss: 0.00001473
Iteration 116/1000 | Loss: 0.00001472
Iteration 117/1000 | Loss: 0.00001472
Iteration 118/1000 | Loss: 0.00001472
Iteration 119/1000 | Loss: 0.00001472
Iteration 120/1000 | Loss: 0.00001471
Iteration 121/1000 | Loss: 0.00001471
Iteration 122/1000 | Loss: 0.00001471
Iteration 123/1000 | Loss: 0.00001470
Iteration 124/1000 | Loss: 0.00001470
Iteration 125/1000 | Loss: 0.00001470
Iteration 126/1000 | Loss: 0.00001470
Iteration 127/1000 | Loss: 0.00001470
Iteration 128/1000 | Loss: 0.00001470
Iteration 129/1000 | Loss: 0.00001470
Iteration 130/1000 | Loss: 0.00001469
Iteration 131/1000 | Loss: 0.00001469
Iteration 132/1000 | Loss: 0.00001469
Iteration 133/1000 | Loss: 0.00001468
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001468
Iteration 137/1000 | Loss: 0.00001467
Iteration 138/1000 | Loss: 0.00001467
Iteration 139/1000 | Loss: 0.00001467
Iteration 140/1000 | Loss: 0.00001467
Iteration 141/1000 | Loss: 0.00001467
Iteration 142/1000 | Loss: 0.00001466
Iteration 143/1000 | Loss: 0.00001466
Iteration 144/1000 | Loss: 0.00001466
Iteration 145/1000 | Loss: 0.00001466
Iteration 146/1000 | Loss: 0.00001466
Iteration 147/1000 | Loss: 0.00001466
Iteration 148/1000 | Loss: 0.00001466
Iteration 149/1000 | Loss: 0.00001466
Iteration 150/1000 | Loss: 0.00001466
Iteration 151/1000 | Loss: 0.00001466
Iteration 152/1000 | Loss: 0.00001466
Iteration 153/1000 | Loss: 0.00001466
Iteration 154/1000 | Loss: 0.00001466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [1.465712375647854e-05, 1.465712375647854e-05, 1.465712375647854e-05, 1.465712375647854e-05, 1.465712375647854e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.465712375647854e-05

Optimization complete. Final v2v error: 3.187793493270874 mm

Highest mean error: 3.733945608139038 mm for frame 165

Lowest mean error: 2.6372640132904053 mm for frame 1

Saving results

Total time: 60.83280372619629
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067077
Iteration 2/25 | Loss: 0.00487436
Iteration 3/25 | Loss: 0.00347207
Iteration 4/25 | Loss: 0.00299461
Iteration 5/25 | Loss: 0.00272901
Iteration 6/25 | Loss: 0.00246730
Iteration 7/25 | Loss: 0.00220554
Iteration 8/25 | Loss: 0.00208204
Iteration 9/25 | Loss: 0.00207710
Iteration 10/25 | Loss: 0.00206733
Iteration 11/25 | Loss: 0.00201280
Iteration 12/25 | Loss: 0.00197549
Iteration 13/25 | Loss: 0.00187785
Iteration 14/25 | Loss: 0.00185135
Iteration 15/25 | Loss: 0.00184671
Iteration 16/25 | Loss: 0.00181659
Iteration 17/25 | Loss: 0.00182931
Iteration 18/25 | Loss: 0.00180293
Iteration 19/25 | Loss: 0.00179648
Iteration 20/25 | Loss: 0.00177488
Iteration 21/25 | Loss: 0.00176830
Iteration 22/25 | Loss: 0.00175836
Iteration 23/25 | Loss: 0.00173687
Iteration 24/25 | Loss: 0.00173825
Iteration 25/25 | Loss: 0.00173828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.41271466
Iteration 2/25 | Loss: 0.00545252
Iteration 3/25 | Loss: 0.00462073
Iteration 4/25 | Loss: 0.00310837
Iteration 5/25 | Loss: 0.00387731
Iteration 6/25 | Loss: 0.00263550
Iteration 7/25 | Loss: 0.00263550
Iteration 8/25 | Loss: 0.00263549
Iteration 9/25 | Loss: 0.00263549
Iteration 10/25 | Loss: 0.00263549
Iteration 11/25 | Loss: 0.00263549
Iteration 12/25 | Loss: 0.00263549
Iteration 13/25 | Loss: 0.00263549
Iteration 14/25 | Loss: 0.00263549
Iteration 15/25 | Loss: 0.00263549
Iteration 16/25 | Loss: 0.00263549
Iteration 17/25 | Loss: 0.00263549
Iteration 18/25 | Loss: 0.00263549
Iteration 19/25 | Loss: 0.00263549
Iteration 20/25 | Loss: 0.00263549
Iteration 21/25 | Loss: 0.00263549
Iteration 22/25 | Loss: 0.00263549
Iteration 23/25 | Loss: 0.00263549
Iteration 24/25 | Loss: 0.00263549
Iteration 25/25 | Loss: 0.00263549

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00263549
Iteration 2/1000 | Loss: 0.00199274
Iteration 3/1000 | Loss: 0.00181596
Iteration 4/1000 | Loss: 0.00133729
Iteration 5/1000 | Loss: 0.00167126
Iteration 6/1000 | Loss: 0.00142574
Iteration 7/1000 | Loss: 0.00138198
Iteration 8/1000 | Loss: 0.00116821
Iteration 9/1000 | Loss: 0.00091726
Iteration 10/1000 | Loss: 0.00119866
Iteration 11/1000 | Loss: 0.00174050
Iteration 12/1000 | Loss: 0.00129740
Iteration 13/1000 | Loss: 0.00117253
Iteration 14/1000 | Loss: 0.00119894
Iteration 15/1000 | Loss: 0.00132701
Iteration 16/1000 | Loss: 0.00115391
Iteration 17/1000 | Loss: 0.00127572
Iteration 18/1000 | Loss: 0.00074062
Iteration 19/1000 | Loss: 0.00090112
Iteration 20/1000 | Loss: 0.00082436
Iteration 21/1000 | Loss: 0.00155666
Iteration 22/1000 | Loss: 0.00106412
Iteration 23/1000 | Loss: 0.00062216
Iteration 24/1000 | Loss: 0.00119963
Iteration 25/1000 | Loss: 0.00136234
Iteration 26/1000 | Loss: 0.00100730
Iteration 27/1000 | Loss: 0.00075692
Iteration 28/1000 | Loss: 0.00096948
Iteration 29/1000 | Loss: 0.00079132
Iteration 30/1000 | Loss: 0.00086873
Iteration 31/1000 | Loss: 0.00098584
Iteration 32/1000 | Loss: 0.00071771
Iteration 33/1000 | Loss: 0.00058683
Iteration 34/1000 | Loss: 0.00051827
Iteration 35/1000 | Loss: 0.00060218
Iteration 36/1000 | Loss: 0.00087829
Iteration 37/1000 | Loss: 0.00155399
Iteration 38/1000 | Loss: 0.00100342
Iteration 39/1000 | Loss: 0.00053033
Iteration 40/1000 | Loss: 0.00055970
Iteration 41/1000 | Loss: 0.00046669
Iteration 42/1000 | Loss: 0.00041447
Iteration 43/1000 | Loss: 0.00089529
Iteration 44/1000 | Loss: 0.00092942
Iteration 45/1000 | Loss: 0.00130041
Iteration 46/1000 | Loss: 0.00048756
Iteration 47/1000 | Loss: 0.00086618
Iteration 48/1000 | Loss: 0.00059303
Iteration 49/1000 | Loss: 0.00095570
Iteration 50/1000 | Loss: 0.00165895
Iteration 51/1000 | Loss: 0.00092169
Iteration 52/1000 | Loss: 0.00042325
Iteration 53/1000 | Loss: 0.00037658
Iteration 54/1000 | Loss: 0.00037499
Iteration 55/1000 | Loss: 0.00033835
Iteration 56/1000 | Loss: 0.00034012
Iteration 57/1000 | Loss: 0.00053953
Iteration 58/1000 | Loss: 0.00058944
Iteration 59/1000 | Loss: 0.00071986
Iteration 60/1000 | Loss: 0.00075480
Iteration 61/1000 | Loss: 0.00065302
Iteration 62/1000 | Loss: 0.00042241
Iteration 63/1000 | Loss: 0.00049098
Iteration 64/1000 | Loss: 0.00028245
Iteration 65/1000 | Loss: 0.00037398
Iteration 66/1000 | Loss: 0.00036487
Iteration 67/1000 | Loss: 0.00107085
Iteration 68/1000 | Loss: 0.00062844
Iteration 69/1000 | Loss: 0.00020544
Iteration 70/1000 | Loss: 0.00040894
Iteration 71/1000 | Loss: 0.00034592
Iteration 72/1000 | Loss: 0.00043511
Iteration 73/1000 | Loss: 0.00040404
Iteration 74/1000 | Loss: 0.00040841
Iteration 75/1000 | Loss: 0.00057545
Iteration 76/1000 | Loss: 0.00043009
Iteration 77/1000 | Loss: 0.00061543
Iteration 78/1000 | Loss: 0.00097736
Iteration 79/1000 | Loss: 0.00060810
Iteration 80/1000 | Loss: 0.00090661
Iteration 81/1000 | Loss: 0.00064371
Iteration 82/1000 | Loss: 0.00066592
Iteration 83/1000 | Loss: 0.00062680
Iteration 84/1000 | Loss: 0.00049096
Iteration 85/1000 | Loss: 0.00070662
Iteration 86/1000 | Loss: 0.00048244
Iteration 87/1000 | Loss: 0.00037185
Iteration 88/1000 | Loss: 0.00092853
Iteration 89/1000 | Loss: 0.00075787
Iteration 90/1000 | Loss: 0.00074052
Iteration 91/1000 | Loss: 0.00058055
Iteration 92/1000 | Loss: 0.00041272
Iteration 93/1000 | Loss: 0.00032314
Iteration 94/1000 | Loss: 0.00055083
Iteration 95/1000 | Loss: 0.00045536
Iteration 96/1000 | Loss: 0.00052613
Iteration 97/1000 | Loss: 0.00051714
Iteration 98/1000 | Loss: 0.00054063
Iteration 99/1000 | Loss: 0.00052732
Iteration 100/1000 | Loss: 0.00033710
Iteration 101/1000 | Loss: 0.00040026
Iteration 102/1000 | Loss: 0.00074401
Iteration 103/1000 | Loss: 0.00059355
Iteration 104/1000 | Loss: 0.00049005
Iteration 105/1000 | Loss: 0.00050541
Iteration 106/1000 | Loss: 0.00048226
Iteration 107/1000 | Loss: 0.00047030
Iteration 108/1000 | Loss: 0.00048972
Iteration 109/1000 | Loss: 0.00052231
Iteration 110/1000 | Loss: 0.00049691
Iteration 111/1000 | Loss: 0.00051302
Iteration 112/1000 | Loss: 0.00049995
Iteration 113/1000 | Loss: 0.00053907
Iteration 114/1000 | Loss: 0.00048471
Iteration 115/1000 | Loss: 0.00049360
Iteration 116/1000 | Loss: 0.00032267
Iteration 117/1000 | Loss: 0.00055221
Iteration 118/1000 | Loss: 0.00041675
Iteration 119/1000 | Loss: 0.00046649
Iteration 120/1000 | Loss: 0.00053689
Iteration 121/1000 | Loss: 0.00044501
Iteration 122/1000 | Loss: 0.00074394
Iteration 123/1000 | Loss: 0.00043063
Iteration 124/1000 | Loss: 0.00030977
Iteration 125/1000 | Loss: 0.00035490
Iteration 126/1000 | Loss: 0.00069241
Iteration 127/1000 | Loss: 0.00036752
Iteration 128/1000 | Loss: 0.00056374
Iteration 129/1000 | Loss: 0.00041904
Iteration 130/1000 | Loss: 0.00134560
Iteration 131/1000 | Loss: 0.00066840
Iteration 132/1000 | Loss: 0.00037098
Iteration 133/1000 | Loss: 0.00083261
Iteration 134/1000 | Loss: 0.00096431
Iteration 135/1000 | Loss: 0.00036561
Iteration 136/1000 | Loss: 0.00028558
Iteration 137/1000 | Loss: 0.00040769
Iteration 138/1000 | Loss: 0.00030796
Iteration 139/1000 | Loss: 0.00036849
Iteration 140/1000 | Loss: 0.00056889
Iteration 141/1000 | Loss: 0.00045730
Iteration 142/1000 | Loss: 0.00038889
Iteration 143/1000 | Loss: 0.00046236
Iteration 144/1000 | Loss: 0.00033969
Iteration 145/1000 | Loss: 0.00063793
Iteration 146/1000 | Loss: 0.00037647
Iteration 147/1000 | Loss: 0.00034697
Iteration 148/1000 | Loss: 0.00031659
Iteration 149/1000 | Loss: 0.00045532
Iteration 150/1000 | Loss: 0.00048826
Iteration 151/1000 | Loss: 0.00046061
Iteration 152/1000 | Loss: 0.00043184
Iteration 153/1000 | Loss: 0.00054154
Iteration 154/1000 | Loss: 0.00037210
Iteration 155/1000 | Loss: 0.00032109
Iteration 156/1000 | Loss: 0.00031431
Iteration 157/1000 | Loss: 0.00026724
Iteration 158/1000 | Loss: 0.00029585
Iteration 159/1000 | Loss: 0.00059673
Iteration 160/1000 | Loss: 0.00026007
Iteration 161/1000 | Loss: 0.00026210
Iteration 162/1000 | Loss: 0.00033917
Iteration 163/1000 | Loss: 0.00028366
Iteration 164/1000 | Loss: 0.00028930
Iteration 165/1000 | Loss: 0.00028224
Iteration 166/1000 | Loss: 0.00027634
Iteration 167/1000 | Loss: 0.00030453
Iteration 168/1000 | Loss: 0.00028558
Iteration 169/1000 | Loss: 0.00028479
Iteration 170/1000 | Loss: 0.00064661
Iteration 171/1000 | Loss: 0.00042625
Iteration 172/1000 | Loss: 0.00024495
Iteration 173/1000 | Loss: 0.00020809
Iteration 174/1000 | Loss: 0.00018667
Iteration 175/1000 | Loss: 0.00019555
Iteration 176/1000 | Loss: 0.00038367
Iteration 177/1000 | Loss: 0.00026438
Iteration 178/1000 | Loss: 0.00029884
Iteration 179/1000 | Loss: 0.00054462
Iteration 180/1000 | Loss: 0.00039271
Iteration 181/1000 | Loss: 0.00099822
Iteration 182/1000 | Loss: 0.00052324
Iteration 183/1000 | Loss: 0.00039642
Iteration 184/1000 | Loss: 0.00041393
Iteration 185/1000 | Loss: 0.00047283
Iteration 186/1000 | Loss: 0.00072478
Iteration 187/1000 | Loss: 0.00061306
Iteration 188/1000 | Loss: 0.00049834
Iteration 189/1000 | Loss: 0.00050687
Iteration 190/1000 | Loss: 0.00035683
Iteration 191/1000 | Loss: 0.00032862
Iteration 192/1000 | Loss: 0.00030848
Iteration 193/1000 | Loss: 0.00027616
Iteration 194/1000 | Loss: 0.00038976
Iteration 195/1000 | Loss: 0.00034595
Iteration 196/1000 | Loss: 0.00041156
Iteration 197/1000 | Loss: 0.00058336
Iteration 198/1000 | Loss: 0.00029534
Iteration 199/1000 | Loss: 0.00016003
Iteration 200/1000 | Loss: 0.00022801
Iteration 201/1000 | Loss: 0.00086589
Iteration 202/1000 | Loss: 0.00038293
Iteration 203/1000 | Loss: 0.00062791
Iteration 204/1000 | Loss: 0.00014538
Iteration 205/1000 | Loss: 0.00021885
Iteration 206/1000 | Loss: 0.00030034
Iteration 207/1000 | Loss: 0.00019493
Iteration 208/1000 | Loss: 0.00012140
Iteration 209/1000 | Loss: 0.00023012
Iteration 210/1000 | Loss: 0.00031488
Iteration 211/1000 | Loss: 0.00023037
Iteration 212/1000 | Loss: 0.00026339
Iteration 213/1000 | Loss: 0.00035965
Iteration 214/1000 | Loss: 0.00039612
Iteration 215/1000 | Loss: 0.00072766
Iteration 216/1000 | Loss: 0.00025598
Iteration 217/1000 | Loss: 0.00038280
Iteration 218/1000 | Loss: 0.00034228
Iteration 219/1000 | Loss: 0.00015162
Iteration 220/1000 | Loss: 0.00014459
Iteration 221/1000 | Loss: 0.00014802
Iteration 222/1000 | Loss: 0.00026071
Iteration 223/1000 | Loss: 0.00023559
Iteration 224/1000 | Loss: 0.00015856
Iteration 225/1000 | Loss: 0.00044596
Iteration 226/1000 | Loss: 0.00029745
Iteration 227/1000 | Loss: 0.00034341
Iteration 228/1000 | Loss: 0.00020196
Iteration 229/1000 | Loss: 0.00027071
Iteration 230/1000 | Loss: 0.00023849
Iteration 231/1000 | Loss: 0.00028053
Iteration 232/1000 | Loss: 0.00019916
Iteration 233/1000 | Loss: 0.00023222
Iteration 234/1000 | Loss: 0.00023808
Iteration 235/1000 | Loss: 0.00024239
Iteration 236/1000 | Loss: 0.00023619
Iteration 237/1000 | Loss: 0.00020967
Iteration 238/1000 | Loss: 0.00024280
Iteration 239/1000 | Loss: 0.00024513
Iteration 240/1000 | Loss: 0.00024880
Iteration 241/1000 | Loss: 0.00024435
Iteration 242/1000 | Loss: 0.00025905
Iteration 243/1000 | Loss: 0.00024638
Iteration 244/1000 | Loss: 0.00018216
Iteration 245/1000 | Loss: 0.00012202
Iteration 246/1000 | Loss: 0.00061881
Iteration 247/1000 | Loss: 0.00014058
Iteration 248/1000 | Loss: 0.00014017
Iteration 249/1000 | Loss: 0.00015421
Iteration 250/1000 | Loss: 0.00014912
Iteration 251/1000 | Loss: 0.00015362
Iteration 252/1000 | Loss: 0.00025241
Iteration 253/1000 | Loss: 0.00036652
Iteration 254/1000 | Loss: 0.00034536
Iteration 255/1000 | Loss: 0.00040760
Iteration 256/1000 | Loss: 0.00047840
Iteration 257/1000 | Loss: 0.00039852
Iteration 258/1000 | Loss: 0.00032115
Iteration 259/1000 | Loss: 0.00045810
Iteration 260/1000 | Loss: 0.00031075
Iteration 261/1000 | Loss: 0.00029030
Iteration 262/1000 | Loss: 0.00040362
Iteration 263/1000 | Loss: 0.00036749
Iteration 264/1000 | Loss: 0.00038749
Iteration 265/1000 | Loss: 0.00039423
Iteration 266/1000 | Loss: 0.00103721
Iteration 267/1000 | Loss: 0.00039911
Iteration 268/1000 | Loss: 0.00014855
Iteration 269/1000 | Loss: 0.00015713
Iteration 270/1000 | Loss: 0.00017506
Iteration 271/1000 | Loss: 0.00014753
Iteration 272/1000 | Loss: 0.00014533
Iteration 273/1000 | Loss: 0.00014489
Iteration 274/1000 | Loss: 0.00014559
Iteration 275/1000 | Loss: 0.00014503
Iteration 276/1000 | Loss: 0.00027069
Iteration 277/1000 | Loss: 0.00017738
Iteration 278/1000 | Loss: 0.00022841
Iteration 279/1000 | Loss: 0.00014878
Iteration 280/1000 | Loss: 0.00045116
Iteration 281/1000 | Loss: 0.00014476
Iteration 282/1000 | Loss: 0.00054717
Iteration 283/1000 | Loss: 0.00066574
Iteration 284/1000 | Loss: 0.00013100
Iteration 285/1000 | Loss: 0.00030246
Iteration 286/1000 | Loss: 0.00011346
Iteration 287/1000 | Loss: 0.00013812
Iteration 288/1000 | Loss: 0.00013711
Iteration 289/1000 | Loss: 0.00013296
Iteration 290/1000 | Loss: 0.00013523
Iteration 291/1000 | Loss: 0.00013209
Iteration 292/1000 | Loss: 0.00013432
Iteration 293/1000 | Loss: 0.00013531
Iteration 294/1000 | Loss: 0.00030019
Iteration 295/1000 | Loss: 0.00040004
Iteration 296/1000 | Loss: 0.00063002
Iteration 297/1000 | Loss: 0.00048160
Iteration 298/1000 | Loss: 0.00030133
Iteration 299/1000 | Loss: 0.00021479
Iteration 300/1000 | Loss: 0.00027672
Iteration 301/1000 | Loss: 0.00045234
Iteration 302/1000 | Loss: 0.00031068
Iteration 303/1000 | Loss: 0.00033856
Iteration 304/1000 | Loss: 0.00037449
Iteration 305/1000 | Loss: 0.00041217
Iteration 306/1000 | Loss: 0.00034430
Iteration 307/1000 | Loss: 0.00039384
Iteration 308/1000 | Loss: 0.00035532
Iteration 309/1000 | Loss: 0.00040137
Iteration 310/1000 | Loss: 0.00036672
Iteration 311/1000 | Loss: 0.00034460
Iteration 312/1000 | Loss: 0.00031297
Iteration 313/1000 | Loss: 0.00033931
Iteration 314/1000 | Loss: 0.00013082
Iteration 315/1000 | Loss: 0.00019028
Iteration 316/1000 | Loss: 0.00018313
Iteration 317/1000 | Loss: 0.00018664
Iteration 318/1000 | Loss: 0.00018796
Iteration 319/1000 | Loss: 0.00014104
Iteration 320/1000 | Loss: 0.00014383
Iteration 321/1000 | Loss: 0.00017298
Iteration 322/1000 | Loss: 0.00016902
Iteration 323/1000 | Loss: 0.00012227
Iteration 324/1000 | Loss: 0.00017864
Iteration 325/1000 | Loss: 0.00018253
Iteration 326/1000 | Loss: 0.00013680
Iteration 327/1000 | Loss: 0.00010148
Iteration 328/1000 | Loss: 0.00012362
Iteration 329/1000 | Loss: 0.00012800
Iteration 330/1000 | Loss: 0.00013811
Iteration 331/1000 | Loss: 0.00016484
Iteration 332/1000 | Loss: 0.00014881
Iteration 333/1000 | Loss: 0.00019138
Iteration 334/1000 | Loss: 0.00019652
Iteration 335/1000 | Loss: 0.00018262
Iteration 336/1000 | Loss: 0.00015434
Iteration 337/1000 | Loss: 0.00015113
Iteration 338/1000 | Loss: 0.00014061
Iteration 339/1000 | Loss: 0.00012830
Iteration 340/1000 | Loss: 0.00013250
Iteration 341/1000 | Loss: 0.00014268
Iteration 342/1000 | Loss: 0.00013891
Iteration 343/1000 | Loss: 0.00011802
Iteration 344/1000 | Loss: 0.00011518
Iteration 345/1000 | Loss: 0.00013220
Iteration 346/1000 | Loss: 0.00014190
Iteration 347/1000 | Loss: 0.00014783
Iteration 348/1000 | Loss: 0.00014495
Iteration 349/1000 | Loss: 0.00017577
Iteration 350/1000 | Loss: 0.00013838
Iteration 351/1000 | Loss: 0.00012072
Iteration 352/1000 | Loss: 0.00012716
Iteration 353/1000 | Loss: 0.00012066
Iteration 354/1000 | Loss: 0.00013267
Iteration 355/1000 | Loss: 0.00013464
Iteration 356/1000 | Loss: 0.00012474
Iteration 357/1000 | Loss: 0.00012669
Iteration 358/1000 | Loss: 0.00013314
Iteration 359/1000 | Loss: 0.00067692
Iteration 360/1000 | Loss: 0.00021419
Iteration 361/1000 | Loss: 0.00059029
Iteration 362/1000 | Loss: 0.00018020
Iteration 363/1000 | Loss: 0.00009973
Iteration 364/1000 | Loss: 0.00009204
Iteration 365/1000 | Loss: 0.00027096
Iteration 366/1000 | Loss: 0.00023658
Iteration 367/1000 | Loss: 0.00009555
Iteration 368/1000 | Loss: 0.00020137
Iteration 369/1000 | Loss: 0.00009463
Iteration 370/1000 | Loss: 0.00008594
Iteration 371/1000 | Loss: 0.00008347
Iteration 372/1000 | Loss: 0.00008230
Iteration 373/1000 | Loss: 0.00008145
Iteration 374/1000 | Loss: 0.00008194
Iteration 375/1000 | Loss: 0.00008782
Iteration 376/1000 | Loss: 0.00009047
Iteration 377/1000 | Loss: 0.00008201
Iteration 378/1000 | Loss: 0.00008869
Iteration 379/1000 | Loss: 0.00009279
Iteration 380/1000 | Loss: 0.00009261
Iteration 381/1000 | Loss: 0.00008941
Iteration 382/1000 | Loss: 0.00008984
Iteration 383/1000 | Loss: 0.00008721
Iteration 384/1000 | Loss: 0.00008421
Iteration 385/1000 | Loss: 0.00008527
Iteration 386/1000 | Loss: 0.00008205
Iteration 387/1000 | Loss: 0.00008262
Iteration 388/1000 | Loss: 0.00008938
Iteration 389/1000 | Loss: 0.00008889
Iteration 390/1000 | Loss: 0.00008853
Iteration 391/1000 | Loss: 0.00014918
Iteration 392/1000 | Loss: 0.00018206
Iteration 393/1000 | Loss: 0.00024372
Iteration 394/1000 | Loss: 0.00030593
Iteration 395/1000 | Loss: 0.00011828
Iteration 396/1000 | Loss: 0.00022921
Iteration 397/1000 | Loss: 0.00018094
Iteration 398/1000 | Loss: 0.00008778
Iteration 399/1000 | Loss: 0.00025089
Iteration 400/1000 | Loss: 0.00020860
Iteration 401/1000 | Loss: 0.00011359
Iteration 402/1000 | Loss: 0.00011470
Iteration 403/1000 | Loss: 0.00010521
Iteration 404/1000 | Loss: 0.00008436
Iteration 405/1000 | Loss: 0.00017657
Iteration 406/1000 | Loss: 0.00009819
Iteration 407/1000 | Loss: 0.00009462
Iteration 408/1000 | Loss: 0.00017535
Iteration 409/1000 | Loss: 0.00013525
Iteration 410/1000 | Loss: 0.00015971
Iteration 411/1000 | Loss: 0.00045251
Iteration 412/1000 | Loss: 0.00048981
Iteration 413/1000 | Loss: 0.00064421
Iteration 414/1000 | Loss: 0.00036672
Iteration 415/1000 | Loss: 0.00051947
Iteration 416/1000 | Loss: 0.00018849
Iteration 417/1000 | Loss: 0.00015165
Iteration 418/1000 | Loss: 0.00008593
Iteration 419/1000 | Loss: 0.00008145
Iteration 420/1000 | Loss: 0.00007985
Iteration 421/1000 | Loss: 0.00007899
Iteration 422/1000 | Loss: 0.00038816
Iteration 423/1000 | Loss: 0.00009456
Iteration 424/1000 | Loss: 0.00008322
Iteration 425/1000 | Loss: 0.00020009
Iteration 426/1000 | Loss: 0.00020503
Iteration 427/1000 | Loss: 0.00020047
Iteration 428/1000 | Loss: 0.00008521
Iteration 429/1000 | Loss: 0.00008192
Iteration 430/1000 | Loss: 0.00007997
Iteration 431/1000 | Loss: 0.00007919
Iteration 432/1000 | Loss: 0.00007835
Iteration 433/1000 | Loss: 0.00007730
Iteration 434/1000 | Loss: 0.00007654
Iteration 435/1000 | Loss: 0.00007607
Iteration 436/1000 | Loss: 0.00007583
Iteration 437/1000 | Loss: 0.00007564
Iteration 438/1000 | Loss: 0.00007558
Iteration 439/1000 | Loss: 0.00007554
Iteration 440/1000 | Loss: 0.00007554
Iteration 441/1000 | Loss: 0.00007550
Iteration 442/1000 | Loss: 0.00007547
Iteration 443/1000 | Loss: 0.00007547
Iteration 444/1000 | Loss: 0.00007546
Iteration 445/1000 | Loss: 0.00007546
Iteration 446/1000 | Loss: 0.00007546
Iteration 447/1000 | Loss: 0.00007546
Iteration 448/1000 | Loss: 0.00007546
Iteration 449/1000 | Loss: 0.00007546
Iteration 450/1000 | Loss: 0.00007546
Iteration 451/1000 | Loss: 0.00007546
Iteration 452/1000 | Loss: 0.00007546
Iteration 453/1000 | Loss: 0.00007546
Iteration 454/1000 | Loss: 0.00007545
Iteration 455/1000 | Loss: 0.00007545
Iteration 456/1000 | Loss: 0.00007545
Iteration 457/1000 | Loss: 0.00007545
Iteration 458/1000 | Loss: 0.00007545
Iteration 459/1000 | Loss: 0.00007545
Iteration 460/1000 | Loss: 0.00007545
Iteration 461/1000 | Loss: 0.00007545
Iteration 462/1000 | Loss: 0.00007545
Iteration 463/1000 | Loss: 0.00007545
Iteration 464/1000 | Loss: 0.00007544
Iteration 465/1000 | Loss: 0.00007544
Iteration 466/1000 | Loss: 0.00007538
Iteration 467/1000 | Loss: 0.00007537
Iteration 468/1000 | Loss: 0.00007536
Iteration 469/1000 | Loss: 0.00007536
Iteration 470/1000 | Loss: 0.00007536
Iteration 471/1000 | Loss: 0.00007536
Iteration 472/1000 | Loss: 0.00007536
Iteration 473/1000 | Loss: 0.00007536
Iteration 474/1000 | Loss: 0.00007536
Iteration 475/1000 | Loss: 0.00007536
Iteration 476/1000 | Loss: 0.00007536
Iteration 477/1000 | Loss: 0.00007535
Iteration 478/1000 | Loss: 0.00007535
Iteration 479/1000 | Loss: 0.00007535
Iteration 480/1000 | Loss: 0.00007534
Iteration 481/1000 | Loss: 0.00007534
Iteration 482/1000 | Loss: 0.00007534
Iteration 483/1000 | Loss: 0.00007534
Iteration 484/1000 | Loss: 0.00007534
Iteration 485/1000 | Loss: 0.00007534
Iteration 486/1000 | Loss: 0.00007534
Iteration 487/1000 | Loss: 0.00007534
Iteration 488/1000 | Loss: 0.00007534
Iteration 489/1000 | Loss: 0.00007533
Iteration 490/1000 | Loss: 0.00007533
Iteration 491/1000 | Loss: 0.00007533
Iteration 492/1000 | Loss: 0.00007533
Iteration 493/1000 | Loss: 0.00007533
Iteration 494/1000 | Loss: 0.00007533
Iteration 495/1000 | Loss: 0.00007533
Iteration 496/1000 | Loss: 0.00007533
Iteration 497/1000 | Loss: 0.00007533
Iteration 498/1000 | Loss: 0.00007533
Iteration 499/1000 | Loss: 0.00007533
Iteration 500/1000 | Loss: 0.00007532
Iteration 501/1000 | Loss: 0.00007532
Iteration 502/1000 | Loss: 0.00007532
Iteration 503/1000 | Loss: 0.00007532
Iteration 504/1000 | Loss: 0.00007532
Iteration 505/1000 | Loss: 0.00007532
Iteration 506/1000 | Loss: 0.00007532
Iteration 507/1000 | Loss: 0.00007531
Iteration 508/1000 | Loss: 0.00007531
Iteration 509/1000 | Loss: 0.00007531
Iteration 510/1000 | Loss: 0.00007531
Iteration 511/1000 | Loss: 0.00007531
Iteration 512/1000 | Loss: 0.00007531
Iteration 513/1000 | Loss: 0.00007531
Iteration 514/1000 | Loss: 0.00007531
Iteration 515/1000 | Loss: 0.00007531
Iteration 516/1000 | Loss: 0.00007531
Iteration 517/1000 | Loss: 0.00007530
Iteration 518/1000 | Loss: 0.00007530
Iteration 519/1000 | Loss: 0.00007530
Iteration 520/1000 | Loss: 0.00007530
Iteration 521/1000 | Loss: 0.00007530
Iteration 522/1000 | Loss: 0.00007530
Iteration 523/1000 | Loss: 0.00007530
Iteration 524/1000 | Loss: 0.00007530
Iteration 525/1000 | Loss: 0.00007530
Iteration 526/1000 | Loss: 0.00007530
Iteration 527/1000 | Loss: 0.00007530
Iteration 528/1000 | Loss: 0.00007530
Iteration 529/1000 | Loss: 0.00007530
Iteration 530/1000 | Loss: 0.00007530
Iteration 531/1000 | Loss: 0.00007530
Iteration 532/1000 | Loss: 0.00007530
Iteration 533/1000 | Loss: 0.00007530
Iteration 534/1000 | Loss: 0.00007530
Iteration 535/1000 | Loss: 0.00007530
Iteration 536/1000 | Loss: 0.00007530
Iteration 537/1000 | Loss: 0.00007530
Iteration 538/1000 | Loss: 0.00007530
Iteration 539/1000 | Loss: 0.00007530
Iteration 540/1000 | Loss: 0.00007530
Iteration 541/1000 | Loss: 0.00007530
Iteration 542/1000 | Loss: 0.00007530
Iteration 543/1000 | Loss: 0.00007530
Iteration 544/1000 | Loss: 0.00007530
Iteration 545/1000 | Loss: 0.00007530
Iteration 546/1000 | Loss: 0.00007530
Iteration 547/1000 | Loss: 0.00007530
Iteration 548/1000 | Loss: 0.00007530
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 548. Stopping optimization.
Last 5 losses: [7.529660069849342e-05, 7.529660069849342e-05, 7.529660069849342e-05, 7.529660069849342e-05, 7.529660069849342e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.529660069849342e-05

Optimization complete. Final v2v error: 6.291868209838867 mm

Highest mean error: 11.966611862182617 mm for frame 150

Lowest mean error: 4.4380598068237305 mm for frame 9

Saving results

Total time: 766.5400409698486
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00492955
Iteration 2/25 | Loss: 0.00152968
Iteration 3/25 | Loss: 0.00121157
Iteration 4/25 | Loss: 0.00115771
Iteration 5/25 | Loss: 0.00115270
Iteration 6/25 | Loss: 0.00115127
Iteration 7/25 | Loss: 0.00115117
Iteration 8/25 | Loss: 0.00115117
Iteration 9/25 | Loss: 0.00115116
Iteration 10/25 | Loss: 0.00115116
Iteration 11/25 | Loss: 0.00115116
Iteration 12/25 | Loss: 0.00115116
Iteration 13/25 | Loss: 0.00115116
Iteration 14/25 | Loss: 0.00115116
Iteration 15/25 | Loss: 0.00115116
Iteration 16/25 | Loss: 0.00115116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001151155331172049, 0.001151155331172049, 0.001151155331172049, 0.001151155331172049, 0.001151155331172049]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001151155331172049

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38546646
Iteration 2/25 | Loss: 0.00050744
Iteration 3/25 | Loss: 0.00050744
Iteration 4/25 | Loss: 0.00050744
Iteration 5/25 | Loss: 0.00050744
Iteration 6/25 | Loss: 0.00050744
Iteration 7/25 | Loss: 0.00050744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 7. Stopping optimization.
Last 5 losses: [0.0005074419896118343, 0.0005074419896118343, 0.0005074419896118343, 0.0005074419896118343, 0.0005074419896118343]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005074419896118343

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00050744
Iteration 2/1000 | Loss: 0.00005197
Iteration 3/1000 | Loss: 0.00002621
Iteration 4/1000 | Loss: 0.00001939
Iteration 5/1000 | Loss: 0.00001836
Iteration 6/1000 | Loss: 0.00001767
Iteration 7/1000 | Loss: 0.00001719
Iteration 8/1000 | Loss: 0.00001679
Iteration 9/1000 | Loss: 0.00001672
Iteration 10/1000 | Loss: 0.00001664
Iteration 11/1000 | Loss: 0.00001647
Iteration 12/1000 | Loss: 0.00001645
Iteration 13/1000 | Loss: 0.00001642
Iteration 14/1000 | Loss: 0.00001642
Iteration 15/1000 | Loss: 0.00001642
Iteration 16/1000 | Loss: 0.00001642
Iteration 17/1000 | Loss: 0.00001642
Iteration 18/1000 | Loss: 0.00001641
Iteration 19/1000 | Loss: 0.00001641
Iteration 20/1000 | Loss: 0.00001641
Iteration 21/1000 | Loss: 0.00001634
Iteration 22/1000 | Loss: 0.00001631
Iteration 23/1000 | Loss: 0.00001631
Iteration 24/1000 | Loss: 0.00001631
Iteration 25/1000 | Loss: 0.00001631
Iteration 26/1000 | Loss: 0.00001631
Iteration 27/1000 | Loss: 0.00001631
Iteration 28/1000 | Loss: 0.00001631
Iteration 29/1000 | Loss: 0.00001630
Iteration 30/1000 | Loss: 0.00001630
Iteration 31/1000 | Loss: 0.00001629
Iteration 32/1000 | Loss: 0.00001629
Iteration 33/1000 | Loss: 0.00001629
Iteration 34/1000 | Loss: 0.00001628
Iteration 35/1000 | Loss: 0.00001628
Iteration 36/1000 | Loss: 0.00001628
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001628
Iteration 39/1000 | Loss: 0.00001628
Iteration 40/1000 | Loss: 0.00001627
Iteration 41/1000 | Loss: 0.00001627
Iteration 42/1000 | Loss: 0.00001627
Iteration 43/1000 | Loss: 0.00001627
Iteration 44/1000 | Loss: 0.00001627
Iteration 45/1000 | Loss: 0.00001627
Iteration 46/1000 | Loss: 0.00001626
Iteration 47/1000 | Loss: 0.00001626
Iteration 48/1000 | Loss: 0.00001626
Iteration 49/1000 | Loss: 0.00001626
Iteration 50/1000 | Loss: 0.00001624
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001619
Iteration 53/1000 | Loss: 0.00001618
Iteration 54/1000 | Loss: 0.00001618
Iteration 55/1000 | Loss: 0.00001618
Iteration 56/1000 | Loss: 0.00001618
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001617
Iteration 62/1000 | Loss: 0.00001617
Iteration 63/1000 | Loss: 0.00001617
Iteration 64/1000 | Loss: 0.00001617
Iteration 65/1000 | Loss: 0.00001616
Iteration 66/1000 | Loss: 0.00001615
Iteration 67/1000 | Loss: 0.00001615
Iteration 68/1000 | Loss: 0.00001615
Iteration 69/1000 | Loss: 0.00001615
Iteration 70/1000 | Loss: 0.00001615
Iteration 71/1000 | Loss: 0.00001615
Iteration 72/1000 | Loss: 0.00001615
Iteration 73/1000 | Loss: 0.00001615
Iteration 74/1000 | Loss: 0.00001615
Iteration 75/1000 | Loss: 0.00001615
Iteration 76/1000 | Loss: 0.00001615
Iteration 77/1000 | Loss: 0.00001615
Iteration 78/1000 | Loss: 0.00001614
Iteration 79/1000 | Loss: 0.00001614
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00001612
Iteration 82/1000 | Loss: 0.00001612
Iteration 83/1000 | Loss: 0.00001612
Iteration 84/1000 | Loss: 0.00001612
Iteration 85/1000 | Loss: 0.00001612
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001612
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00001611
Iteration 92/1000 | Loss: 0.00001611
Iteration 93/1000 | Loss: 0.00001611
Iteration 94/1000 | Loss: 0.00001611
Iteration 95/1000 | Loss: 0.00001611
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001610
Iteration 101/1000 | Loss: 0.00001610
Iteration 102/1000 | Loss: 0.00001610
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001609
Iteration 107/1000 | Loss: 0.00001609
Iteration 108/1000 | Loss: 0.00001609
Iteration 109/1000 | Loss: 0.00001609
Iteration 110/1000 | Loss: 0.00001609
Iteration 111/1000 | Loss: 0.00001609
Iteration 112/1000 | Loss: 0.00001609
Iteration 113/1000 | Loss: 0.00001609
Iteration 114/1000 | Loss: 0.00001609
Iteration 115/1000 | Loss: 0.00001609
Iteration 116/1000 | Loss: 0.00001609
Iteration 117/1000 | Loss: 0.00001609
Iteration 118/1000 | Loss: 0.00001609
Iteration 119/1000 | Loss: 0.00001608
Iteration 120/1000 | Loss: 0.00001608
Iteration 121/1000 | Loss: 0.00001607
Iteration 122/1000 | Loss: 0.00001607
Iteration 123/1000 | Loss: 0.00001607
Iteration 124/1000 | Loss: 0.00001607
Iteration 125/1000 | Loss: 0.00001607
Iteration 126/1000 | Loss: 0.00001606
Iteration 127/1000 | Loss: 0.00001606
Iteration 128/1000 | Loss: 0.00001606
Iteration 129/1000 | Loss: 0.00001606
Iteration 130/1000 | Loss: 0.00001606
Iteration 131/1000 | Loss: 0.00001606
Iteration 132/1000 | Loss: 0.00001605
Iteration 133/1000 | Loss: 0.00001605
Iteration 134/1000 | Loss: 0.00001604
Iteration 135/1000 | Loss: 0.00001604
Iteration 136/1000 | Loss: 0.00001604
Iteration 137/1000 | Loss: 0.00001604
Iteration 138/1000 | Loss: 0.00001604
Iteration 139/1000 | Loss: 0.00001604
Iteration 140/1000 | Loss: 0.00001604
Iteration 141/1000 | Loss: 0.00001604
Iteration 142/1000 | Loss: 0.00001604
Iteration 143/1000 | Loss: 0.00001604
Iteration 144/1000 | Loss: 0.00001604
Iteration 145/1000 | Loss: 0.00001604
Iteration 146/1000 | Loss: 0.00001604
Iteration 147/1000 | Loss: 0.00001604
Iteration 148/1000 | Loss: 0.00001603
Iteration 149/1000 | Loss: 0.00001603
Iteration 150/1000 | Loss: 0.00001603
Iteration 151/1000 | Loss: 0.00001603
Iteration 152/1000 | Loss: 0.00001602
Iteration 153/1000 | Loss: 0.00001602
Iteration 154/1000 | Loss: 0.00001602
Iteration 155/1000 | Loss: 0.00001601
Iteration 156/1000 | Loss: 0.00001601
Iteration 157/1000 | Loss: 0.00001601
Iteration 158/1000 | Loss: 0.00001601
Iteration 159/1000 | Loss: 0.00001601
Iteration 160/1000 | Loss: 0.00001601
Iteration 161/1000 | Loss: 0.00001601
Iteration 162/1000 | Loss: 0.00001601
Iteration 163/1000 | Loss: 0.00001601
Iteration 164/1000 | Loss: 0.00001601
Iteration 165/1000 | Loss: 0.00001601
Iteration 166/1000 | Loss: 0.00001601
Iteration 167/1000 | Loss: 0.00001601
Iteration 168/1000 | Loss: 0.00001600
Iteration 169/1000 | Loss: 0.00001600
Iteration 170/1000 | Loss: 0.00001600
Iteration 171/1000 | Loss: 0.00001600
Iteration 172/1000 | Loss: 0.00001600
Iteration 173/1000 | Loss: 0.00001600
Iteration 174/1000 | Loss: 0.00001600
Iteration 175/1000 | Loss: 0.00001600
Iteration 176/1000 | Loss: 0.00001600
Iteration 177/1000 | Loss: 0.00001600
Iteration 178/1000 | Loss: 0.00001600
Iteration 179/1000 | Loss: 0.00001600
Iteration 180/1000 | Loss: 0.00001600
Iteration 181/1000 | Loss: 0.00001600
Iteration 182/1000 | Loss: 0.00001600
Iteration 183/1000 | Loss: 0.00001600
Iteration 184/1000 | Loss: 0.00001600
Iteration 185/1000 | Loss: 0.00001599
Iteration 186/1000 | Loss: 0.00001599
Iteration 187/1000 | Loss: 0.00001599
Iteration 188/1000 | Loss: 0.00001599
Iteration 189/1000 | Loss: 0.00001599
Iteration 190/1000 | Loss: 0.00001599
Iteration 191/1000 | Loss: 0.00001599
Iteration 192/1000 | Loss: 0.00001599
Iteration 193/1000 | Loss: 0.00001599
Iteration 194/1000 | Loss: 0.00001599
Iteration 195/1000 | Loss: 0.00001599
Iteration 196/1000 | Loss: 0.00001598
Iteration 197/1000 | Loss: 0.00001598
Iteration 198/1000 | Loss: 0.00001598
Iteration 199/1000 | Loss: 0.00001598
Iteration 200/1000 | Loss: 0.00001598
Iteration 201/1000 | Loss: 0.00001598
Iteration 202/1000 | Loss: 0.00001598
Iteration 203/1000 | Loss: 0.00001598
Iteration 204/1000 | Loss: 0.00001598
Iteration 205/1000 | Loss: 0.00001598
Iteration 206/1000 | Loss: 0.00001598
Iteration 207/1000 | Loss: 0.00001598
Iteration 208/1000 | Loss: 0.00001598
Iteration 209/1000 | Loss: 0.00001598
Iteration 210/1000 | Loss: 0.00001598
Iteration 211/1000 | Loss: 0.00001598
Iteration 212/1000 | Loss: 0.00001598
Iteration 213/1000 | Loss: 0.00001598
Iteration 214/1000 | Loss: 0.00001598
Iteration 215/1000 | Loss: 0.00001598
Iteration 216/1000 | Loss: 0.00001598
Iteration 217/1000 | Loss: 0.00001598
Iteration 218/1000 | Loss: 0.00001598
Iteration 219/1000 | Loss: 0.00001598
Iteration 220/1000 | Loss: 0.00001598
Iteration 221/1000 | Loss: 0.00001598
Iteration 222/1000 | Loss: 0.00001598
Iteration 223/1000 | Loss: 0.00001598
Iteration 224/1000 | Loss: 0.00001598
Iteration 225/1000 | Loss: 0.00001598
Iteration 226/1000 | Loss: 0.00001598
Iteration 227/1000 | Loss: 0.00001598
Iteration 228/1000 | Loss: 0.00001598
Iteration 229/1000 | Loss: 0.00001598
Iteration 230/1000 | Loss: 0.00001598
Iteration 231/1000 | Loss: 0.00001598
Iteration 232/1000 | Loss: 0.00001598
Iteration 233/1000 | Loss: 0.00001598
Iteration 234/1000 | Loss: 0.00001598
Iteration 235/1000 | Loss: 0.00001598
Iteration 236/1000 | Loss: 0.00001598
Iteration 237/1000 | Loss: 0.00001598
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.597640584805049e-05, 1.597640584805049e-05, 1.597640584805049e-05, 1.597640584805049e-05, 1.597640584805049e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.597640584805049e-05

Optimization complete. Final v2v error: 3.4555959701538086 mm

Highest mean error: 3.5685160160064697 mm for frame 49

Lowest mean error: 3.15716290473938 mm for frame 2

Saving results

Total time: 37.31791853904724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00855430
Iteration 2/25 | Loss: 0.00117280
Iteration 3/25 | Loss: 0.00109357
Iteration 4/25 | Loss: 0.00108210
Iteration 5/25 | Loss: 0.00107809
Iteration 6/25 | Loss: 0.00107781
Iteration 7/25 | Loss: 0.00107781
Iteration 8/25 | Loss: 0.00107781
Iteration 9/25 | Loss: 0.00107781
Iteration 10/25 | Loss: 0.00107781
Iteration 11/25 | Loss: 0.00107781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001077806344255805, 0.001077806344255805, 0.001077806344255805, 0.001077806344255805, 0.001077806344255805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001077806344255805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38898265
Iteration 2/25 | Loss: 0.00077150
Iteration 3/25 | Loss: 0.00077149
Iteration 4/25 | Loss: 0.00077149
Iteration 5/25 | Loss: 0.00077149
Iteration 6/25 | Loss: 0.00077149
Iteration 7/25 | Loss: 0.00077149
Iteration 8/25 | Loss: 0.00077149
Iteration 9/25 | Loss: 0.00077149
Iteration 10/25 | Loss: 0.00077149
Iteration 11/25 | Loss: 0.00077149
Iteration 12/25 | Loss: 0.00077149
Iteration 13/25 | Loss: 0.00077149
Iteration 14/25 | Loss: 0.00077149
Iteration 15/25 | Loss: 0.00077149
Iteration 16/25 | Loss: 0.00077149
Iteration 17/25 | Loss: 0.00077149
Iteration 18/25 | Loss: 0.00077149
Iteration 19/25 | Loss: 0.00077149
Iteration 20/25 | Loss: 0.00077149
Iteration 21/25 | Loss: 0.00077149
Iteration 22/25 | Loss: 0.00077149
Iteration 23/25 | Loss: 0.00077149
Iteration 24/25 | Loss: 0.00077149
Iteration 25/25 | Loss: 0.00077149

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077149
Iteration 2/1000 | Loss: 0.00003044
Iteration 3/1000 | Loss: 0.00002147
Iteration 4/1000 | Loss: 0.00001917
Iteration 5/1000 | Loss: 0.00001808
Iteration 6/1000 | Loss: 0.00001723
Iteration 7/1000 | Loss: 0.00001676
Iteration 8/1000 | Loss: 0.00001624
Iteration 9/1000 | Loss: 0.00001595
Iteration 10/1000 | Loss: 0.00001563
Iteration 11/1000 | Loss: 0.00001535
Iteration 12/1000 | Loss: 0.00001530
Iteration 13/1000 | Loss: 0.00001519
Iteration 14/1000 | Loss: 0.00001518
Iteration 15/1000 | Loss: 0.00001517
Iteration 16/1000 | Loss: 0.00001507
Iteration 17/1000 | Loss: 0.00001503
Iteration 18/1000 | Loss: 0.00001493
Iteration 19/1000 | Loss: 0.00001492
Iteration 20/1000 | Loss: 0.00001491
Iteration 21/1000 | Loss: 0.00001491
Iteration 22/1000 | Loss: 0.00001490
Iteration 23/1000 | Loss: 0.00001490
Iteration 24/1000 | Loss: 0.00001487
Iteration 25/1000 | Loss: 0.00001485
Iteration 26/1000 | Loss: 0.00001485
Iteration 27/1000 | Loss: 0.00001485
Iteration 28/1000 | Loss: 0.00001485
Iteration 29/1000 | Loss: 0.00001485
Iteration 30/1000 | Loss: 0.00001485
Iteration 31/1000 | Loss: 0.00001484
Iteration 32/1000 | Loss: 0.00001484
Iteration 33/1000 | Loss: 0.00001483
Iteration 34/1000 | Loss: 0.00001482
Iteration 35/1000 | Loss: 0.00001482
Iteration 36/1000 | Loss: 0.00001481
Iteration 37/1000 | Loss: 0.00001481
Iteration 38/1000 | Loss: 0.00001481
Iteration 39/1000 | Loss: 0.00001480
Iteration 40/1000 | Loss: 0.00001480
Iteration 41/1000 | Loss: 0.00001480
Iteration 42/1000 | Loss: 0.00001479
Iteration 43/1000 | Loss: 0.00001478
Iteration 44/1000 | Loss: 0.00001477
Iteration 45/1000 | Loss: 0.00001477
Iteration 46/1000 | Loss: 0.00001477
Iteration 47/1000 | Loss: 0.00001477
Iteration 48/1000 | Loss: 0.00001477
Iteration 49/1000 | Loss: 0.00001477
Iteration 50/1000 | Loss: 0.00001477
Iteration 51/1000 | Loss: 0.00001476
Iteration 52/1000 | Loss: 0.00001476
Iteration 53/1000 | Loss: 0.00001476
Iteration 54/1000 | Loss: 0.00001476
Iteration 55/1000 | Loss: 0.00001476
Iteration 56/1000 | Loss: 0.00001476
Iteration 57/1000 | Loss: 0.00001475
Iteration 58/1000 | Loss: 0.00001473
Iteration 59/1000 | Loss: 0.00001473
Iteration 60/1000 | Loss: 0.00001473
Iteration 61/1000 | Loss: 0.00001473
Iteration 62/1000 | Loss: 0.00001473
Iteration 63/1000 | Loss: 0.00001473
Iteration 64/1000 | Loss: 0.00001473
Iteration 65/1000 | Loss: 0.00001473
Iteration 66/1000 | Loss: 0.00001473
Iteration 67/1000 | Loss: 0.00001473
Iteration 68/1000 | Loss: 0.00001473
Iteration 69/1000 | Loss: 0.00001473
Iteration 70/1000 | Loss: 0.00001473
Iteration 71/1000 | Loss: 0.00001472
Iteration 72/1000 | Loss: 0.00001472
Iteration 73/1000 | Loss: 0.00001472
Iteration 74/1000 | Loss: 0.00001472
Iteration 75/1000 | Loss: 0.00001471
Iteration 76/1000 | Loss: 0.00001471
Iteration 77/1000 | Loss: 0.00001470
Iteration 78/1000 | Loss: 0.00001470
Iteration 79/1000 | Loss: 0.00001470
Iteration 80/1000 | Loss: 0.00001470
Iteration 81/1000 | Loss: 0.00001469
Iteration 82/1000 | Loss: 0.00001469
Iteration 83/1000 | Loss: 0.00001469
Iteration 84/1000 | Loss: 0.00001468
Iteration 85/1000 | Loss: 0.00001468
Iteration 86/1000 | Loss: 0.00001468
Iteration 87/1000 | Loss: 0.00001467
Iteration 88/1000 | Loss: 0.00001467
Iteration 89/1000 | Loss: 0.00001467
Iteration 90/1000 | Loss: 0.00001467
Iteration 91/1000 | Loss: 0.00001467
Iteration 92/1000 | Loss: 0.00001467
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001466
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001466
Iteration 97/1000 | Loss: 0.00001466
Iteration 98/1000 | Loss: 0.00001466
Iteration 99/1000 | Loss: 0.00001466
Iteration 100/1000 | Loss: 0.00001465
Iteration 101/1000 | Loss: 0.00001465
Iteration 102/1000 | Loss: 0.00001465
Iteration 103/1000 | Loss: 0.00001465
Iteration 104/1000 | Loss: 0.00001465
Iteration 105/1000 | Loss: 0.00001465
Iteration 106/1000 | Loss: 0.00001464
Iteration 107/1000 | Loss: 0.00001464
Iteration 108/1000 | Loss: 0.00001464
Iteration 109/1000 | Loss: 0.00001464
Iteration 110/1000 | Loss: 0.00001464
Iteration 111/1000 | Loss: 0.00001464
Iteration 112/1000 | Loss: 0.00001464
Iteration 113/1000 | Loss: 0.00001464
Iteration 114/1000 | Loss: 0.00001464
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001464
Iteration 117/1000 | Loss: 0.00001464
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001463
Iteration 121/1000 | Loss: 0.00001463
Iteration 122/1000 | Loss: 0.00001463
Iteration 123/1000 | Loss: 0.00001462
Iteration 124/1000 | Loss: 0.00001462
Iteration 125/1000 | Loss: 0.00001462
Iteration 126/1000 | Loss: 0.00001462
Iteration 127/1000 | Loss: 0.00001462
Iteration 128/1000 | Loss: 0.00001462
Iteration 129/1000 | Loss: 0.00001461
Iteration 130/1000 | Loss: 0.00001461
Iteration 131/1000 | Loss: 0.00001461
Iteration 132/1000 | Loss: 0.00001461
Iteration 133/1000 | Loss: 0.00001461
Iteration 134/1000 | Loss: 0.00001460
Iteration 135/1000 | Loss: 0.00001460
Iteration 136/1000 | Loss: 0.00001460
Iteration 137/1000 | Loss: 0.00001460
Iteration 138/1000 | Loss: 0.00001460
Iteration 139/1000 | Loss: 0.00001460
Iteration 140/1000 | Loss: 0.00001460
Iteration 141/1000 | Loss: 0.00001460
Iteration 142/1000 | Loss: 0.00001460
Iteration 143/1000 | Loss: 0.00001460
Iteration 144/1000 | Loss: 0.00001460
Iteration 145/1000 | Loss: 0.00001460
Iteration 146/1000 | Loss: 0.00001460
Iteration 147/1000 | Loss: 0.00001460
Iteration 148/1000 | Loss: 0.00001460
Iteration 149/1000 | Loss: 0.00001459
Iteration 150/1000 | Loss: 0.00001459
Iteration 151/1000 | Loss: 0.00001459
Iteration 152/1000 | Loss: 0.00001459
Iteration 153/1000 | Loss: 0.00001459
Iteration 154/1000 | Loss: 0.00001459
Iteration 155/1000 | Loss: 0.00001459
Iteration 156/1000 | Loss: 0.00001459
Iteration 157/1000 | Loss: 0.00001459
Iteration 158/1000 | Loss: 0.00001459
Iteration 159/1000 | Loss: 0.00001458
Iteration 160/1000 | Loss: 0.00001458
Iteration 161/1000 | Loss: 0.00001458
Iteration 162/1000 | Loss: 0.00001458
Iteration 163/1000 | Loss: 0.00001458
Iteration 164/1000 | Loss: 0.00001458
Iteration 165/1000 | Loss: 0.00001458
Iteration 166/1000 | Loss: 0.00001458
Iteration 167/1000 | Loss: 0.00001458
Iteration 168/1000 | Loss: 0.00001458
Iteration 169/1000 | Loss: 0.00001458
Iteration 170/1000 | Loss: 0.00001458
Iteration 171/1000 | Loss: 0.00001458
Iteration 172/1000 | Loss: 0.00001458
Iteration 173/1000 | Loss: 0.00001458
Iteration 174/1000 | Loss: 0.00001458
Iteration 175/1000 | Loss: 0.00001458
Iteration 176/1000 | Loss: 0.00001458
Iteration 177/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.4577301044482738e-05, 1.4577301044482738e-05, 1.4577301044482738e-05, 1.4577301044482738e-05, 1.4577301044482738e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4577301044482738e-05

Optimization complete. Final v2v error: 3.2329230308532715 mm

Highest mean error: 3.6115081310272217 mm for frame 151

Lowest mean error: 2.985368490219116 mm for frame 71

Saving results

Total time: 37.75237536430359
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00791185
Iteration 2/25 | Loss: 0.00130313
Iteration 3/25 | Loss: 0.00113553
Iteration 4/25 | Loss: 0.00111140
Iteration 5/25 | Loss: 0.00110621
Iteration 6/25 | Loss: 0.00110508
Iteration 7/25 | Loss: 0.00110508
Iteration 8/25 | Loss: 0.00110508
Iteration 9/25 | Loss: 0.00110508
Iteration 10/25 | Loss: 0.00110508
Iteration 11/25 | Loss: 0.00110508
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011050777975469828, 0.0011050777975469828, 0.0011050777975469828, 0.0011050777975469828, 0.0011050777975469828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011050777975469828

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39836788
Iteration 2/25 | Loss: 0.00056421
Iteration 3/25 | Loss: 0.00056421
Iteration 4/25 | Loss: 0.00056421
Iteration 5/25 | Loss: 0.00056421
Iteration 6/25 | Loss: 0.00056421
Iteration 7/25 | Loss: 0.00056421
Iteration 8/25 | Loss: 0.00056421
Iteration 9/25 | Loss: 0.00056421
Iteration 10/25 | Loss: 0.00056421
Iteration 11/25 | Loss: 0.00056421
Iteration 12/25 | Loss: 0.00056421
Iteration 13/25 | Loss: 0.00056421
Iteration 14/25 | Loss: 0.00056421
Iteration 15/25 | Loss: 0.00056421
Iteration 16/25 | Loss: 0.00056421
Iteration 17/25 | Loss: 0.00056421
Iteration 18/25 | Loss: 0.00056421
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0005642063333652914, 0.0005642063333652914, 0.0005642063333652914, 0.0005642063333652914, 0.0005642063333652914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005642063333652914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00056421
Iteration 2/1000 | Loss: 0.00003357
Iteration 3/1000 | Loss: 0.00002164
Iteration 4/1000 | Loss: 0.00001895
Iteration 5/1000 | Loss: 0.00001778
Iteration 6/1000 | Loss: 0.00001682
Iteration 7/1000 | Loss: 0.00001633
Iteration 8/1000 | Loss: 0.00001575
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001527
Iteration 11/1000 | Loss: 0.00001507
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001489
Iteration 14/1000 | Loss: 0.00001479
Iteration 15/1000 | Loss: 0.00001479
Iteration 16/1000 | Loss: 0.00001475
Iteration 17/1000 | Loss: 0.00001475
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001471
Iteration 20/1000 | Loss: 0.00001469
Iteration 21/1000 | Loss: 0.00001468
Iteration 22/1000 | Loss: 0.00001467
Iteration 23/1000 | Loss: 0.00001466
Iteration 24/1000 | Loss: 0.00001465
Iteration 25/1000 | Loss: 0.00001465
Iteration 26/1000 | Loss: 0.00001464
Iteration 27/1000 | Loss: 0.00001464
Iteration 28/1000 | Loss: 0.00001464
Iteration 29/1000 | Loss: 0.00001464
Iteration 30/1000 | Loss: 0.00001463
Iteration 31/1000 | Loss: 0.00001463
Iteration 32/1000 | Loss: 0.00001463
Iteration 33/1000 | Loss: 0.00001462
Iteration 34/1000 | Loss: 0.00001462
Iteration 35/1000 | Loss: 0.00001461
Iteration 36/1000 | Loss: 0.00001461
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001460
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001460
Iteration 43/1000 | Loss: 0.00001459
Iteration 44/1000 | Loss: 0.00001459
Iteration 45/1000 | Loss: 0.00001459
Iteration 46/1000 | Loss: 0.00001459
Iteration 47/1000 | Loss: 0.00001458
Iteration 48/1000 | Loss: 0.00001458
Iteration 49/1000 | Loss: 0.00001458
Iteration 50/1000 | Loss: 0.00001458
Iteration 51/1000 | Loss: 0.00001458
Iteration 52/1000 | Loss: 0.00001458
Iteration 53/1000 | Loss: 0.00001457
Iteration 54/1000 | Loss: 0.00001457
Iteration 55/1000 | Loss: 0.00001457
Iteration 56/1000 | Loss: 0.00001457
Iteration 57/1000 | Loss: 0.00001457
Iteration 58/1000 | Loss: 0.00001457
Iteration 59/1000 | Loss: 0.00001456
Iteration 60/1000 | Loss: 0.00001456
Iteration 61/1000 | Loss: 0.00001456
Iteration 62/1000 | Loss: 0.00001456
Iteration 63/1000 | Loss: 0.00001456
Iteration 64/1000 | Loss: 0.00001456
Iteration 65/1000 | Loss: 0.00001456
Iteration 66/1000 | Loss: 0.00001455
Iteration 67/1000 | Loss: 0.00001455
Iteration 68/1000 | Loss: 0.00001455
Iteration 69/1000 | Loss: 0.00001455
Iteration 70/1000 | Loss: 0.00001455
Iteration 71/1000 | Loss: 0.00001455
Iteration 72/1000 | Loss: 0.00001455
Iteration 73/1000 | Loss: 0.00001455
Iteration 74/1000 | Loss: 0.00001455
Iteration 75/1000 | Loss: 0.00001454
Iteration 76/1000 | Loss: 0.00001454
Iteration 77/1000 | Loss: 0.00001454
Iteration 78/1000 | Loss: 0.00001454
Iteration 79/1000 | Loss: 0.00001454
Iteration 80/1000 | Loss: 0.00001454
Iteration 81/1000 | Loss: 0.00001453
Iteration 82/1000 | Loss: 0.00001453
Iteration 83/1000 | Loss: 0.00001453
Iteration 84/1000 | Loss: 0.00001452
Iteration 85/1000 | Loss: 0.00001452
Iteration 86/1000 | Loss: 0.00001452
Iteration 87/1000 | Loss: 0.00001452
Iteration 88/1000 | Loss: 0.00001451
Iteration 89/1000 | Loss: 0.00001451
Iteration 90/1000 | Loss: 0.00001451
Iteration 91/1000 | Loss: 0.00001451
Iteration 92/1000 | Loss: 0.00001451
Iteration 93/1000 | Loss: 0.00001451
Iteration 94/1000 | Loss: 0.00001451
Iteration 95/1000 | Loss: 0.00001450
Iteration 96/1000 | Loss: 0.00001450
Iteration 97/1000 | Loss: 0.00001450
Iteration 98/1000 | Loss: 0.00001450
Iteration 99/1000 | Loss: 0.00001450
Iteration 100/1000 | Loss: 0.00001449
Iteration 101/1000 | Loss: 0.00001449
Iteration 102/1000 | Loss: 0.00001449
Iteration 103/1000 | Loss: 0.00001449
Iteration 104/1000 | Loss: 0.00001449
Iteration 105/1000 | Loss: 0.00001449
Iteration 106/1000 | Loss: 0.00001449
Iteration 107/1000 | Loss: 0.00001448
Iteration 108/1000 | Loss: 0.00001448
Iteration 109/1000 | Loss: 0.00001448
Iteration 110/1000 | Loss: 0.00001448
Iteration 111/1000 | Loss: 0.00001448
Iteration 112/1000 | Loss: 0.00001448
Iteration 113/1000 | Loss: 0.00001448
Iteration 114/1000 | Loss: 0.00001448
Iteration 115/1000 | Loss: 0.00001448
Iteration 116/1000 | Loss: 0.00001448
Iteration 117/1000 | Loss: 0.00001447
Iteration 118/1000 | Loss: 0.00001447
Iteration 119/1000 | Loss: 0.00001447
Iteration 120/1000 | Loss: 0.00001447
Iteration 121/1000 | Loss: 0.00001447
Iteration 122/1000 | Loss: 0.00001447
Iteration 123/1000 | Loss: 0.00001447
Iteration 124/1000 | Loss: 0.00001447
Iteration 125/1000 | Loss: 0.00001447
Iteration 126/1000 | Loss: 0.00001447
Iteration 127/1000 | Loss: 0.00001447
Iteration 128/1000 | Loss: 0.00001447
Iteration 129/1000 | Loss: 0.00001447
Iteration 130/1000 | Loss: 0.00001446
Iteration 131/1000 | Loss: 0.00001446
Iteration 132/1000 | Loss: 0.00001446
Iteration 133/1000 | Loss: 0.00001446
Iteration 134/1000 | Loss: 0.00001446
Iteration 135/1000 | Loss: 0.00001446
Iteration 136/1000 | Loss: 0.00001446
Iteration 137/1000 | Loss: 0.00001446
Iteration 138/1000 | Loss: 0.00001446
Iteration 139/1000 | Loss: 0.00001446
Iteration 140/1000 | Loss: 0.00001445
Iteration 141/1000 | Loss: 0.00001445
Iteration 142/1000 | Loss: 0.00001445
Iteration 143/1000 | Loss: 0.00001445
Iteration 144/1000 | Loss: 0.00001445
Iteration 145/1000 | Loss: 0.00001445
Iteration 146/1000 | Loss: 0.00001445
Iteration 147/1000 | Loss: 0.00001445
Iteration 148/1000 | Loss: 0.00001445
Iteration 149/1000 | Loss: 0.00001445
Iteration 150/1000 | Loss: 0.00001445
Iteration 151/1000 | Loss: 0.00001445
Iteration 152/1000 | Loss: 0.00001444
Iteration 153/1000 | Loss: 0.00001444
Iteration 154/1000 | Loss: 0.00001444
Iteration 155/1000 | Loss: 0.00001444
Iteration 156/1000 | Loss: 0.00001444
Iteration 157/1000 | Loss: 0.00001444
Iteration 158/1000 | Loss: 0.00001444
Iteration 159/1000 | Loss: 0.00001444
Iteration 160/1000 | Loss: 0.00001444
Iteration 161/1000 | Loss: 0.00001444
Iteration 162/1000 | Loss: 0.00001444
Iteration 163/1000 | Loss: 0.00001444
Iteration 164/1000 | Loss: 0.00001444
Iteration 165/1000 | Loss: 0.00001444
Iteration 166/1000 | Loss: 0.00001443
Iteration 167/1000 | Loss: 0.00001443
Iteration 168/1000 | Loss: 0.00001443
Iteration 169/1000 | Loss: 0.00001443
Iteration 170/1000 | Loss: 0.00001443
Iteration 171/1000 | Loss: 0.00001443
Iteration 172/1000 | Loss: 0.00001443
Iteration 173/1000 | Loss: 0.00001443
Iteration 174/1000 | Loss: 0.00001443
Iteration 175/1000 | Loss: 0.00001443
Iteration 176/1000 | Loss: 0.00001443
Iteration 177/1000 | Loss: 0.00001442
Iteration 178/1000 | Loss: 0.00001442
Iteration 179/1000 | Loss: 0.00001442
Iteration 180/1000 | Loss: 0.00001442
Iteration 181/1000 | Loss: 0.00001442
Iteration 182/1000 | Loss: 0.00001442
Iteration 183/1000 | Loss: 0.00001442
Iteration 184/1000 | Loss: 0.00001442
Iteration 185/1000 | Loss: 0.00001442
Iteration 186/1000 | Loss: 0.00001442
Iteration 187/1000 | Loss: 0.00001441
Iteration 188/1000 | Loss: 0.00001441
Iteration 189/1000 | Loss: 0.00001441
Iteration 190/1000 | Loss: 0.00001441
Iteration 191/1000 | Loss: 0.00001441
Iteration 192/1000 | Loss: 0.00001441
Iteration 193/1000 | Loss: 0.00001441
Iteration 194/1000 | Loss: 0.00001441
Iteration 195/1000 | Loss: 0.00001441
Iteration 196/1000 | Loss: 0.00001441
Iteration 197/1000 | Loss: 0.00001441
Iteration 198/1000 | Loss: 0.00001441
Iteration 199/1000 | Loss: 0.00001441
Iteration 200/1000 | Loss: 0.00001441
Iteration 201/1000 | Loss: 0.00001440
Iteration 202/1000 | Loss: 0.00001440
Iteration 203/1000 | Loss: 0.00001440
Iteration 204/1000 | Loss: 0.00001440
Iteration 205/1000 | Loss: 0.00001440
Iteration 206/1000 | Loss: 0.00001440
Iteration 207/1000 | Loss: 0.00001440
Iteration 208/1000 | Loss: 0.00001440
Iteration 209/1000 | Loss: 0.00001440
Iteration 210/1000 | Loss: 0.00001440
Iteration 211/1000 | Loss: 0.00001440
Iteration 212/1000 | Loss: 0.00001440
Iteration 213/1000 | Loss: 0.00001440
Iteration 214/1000 | Loss: 0.00001440
Iteration 215/1000 | Loss: 0.00001440
Iteration 216/1000 | Loss: 0.00001440
Iteration 217/1000 | Loss: 0.00001440
Iteration 218/1000 | Loss: 0.00001440
Iteration 219/1000 | Loss: 0.00001439
Iteration 220/1000 | Loss: 0.00001439
Iteration 221/1000 | Loss: 0.00001439
Iteration 222/1000 | Loss: 0.00001439
Iteration 223/1000 | Loss: 0.00001439
Iteration 224/1000 | Loss: 0.00001439
Iteration 225/1000 | Loss: 0.00001439
Iteration 226/1000 | Loss: 0.00001439
Iteration 227/1000 | Loss: 0.00001439
Iteration 228/1000 | Loss: 0.00001439
Iteration 229/1000 | Loss: 0.00001439
Iteration 230/1000 | Loss: 0.00001439
Iteration 231/1000 | Loss: 0.00001439
Iteration 232/1000 | Loss: 0.00001439
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.4391411241376773e-05, 1.4391411241376773e-05, 1.4391411241376773e-05, 1.4391411241376773e-05, 1.4391411241376773e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4391411241376773e-05

Optimization complete. Final v2v error: 3.1757051944732666 mm

Highest mean error: 3.8879997730255127 mm for frame 88

Lowest mean error: 2.780545473098755 mm for frame 56

Saving results

Total time: 40.47131037712097
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01072489
Iteration 2/25 | Loss: 0.01072489
Iteration 3/25 | Loss: 0.01072489
Iteration 4/25 | Loss: 0.01072489
Iteration 5/25 | Loss: 0.01072489
Iteration 6/25 | Loss: 0.01072488
Iteration 7/25 | Loss: 0.01072488
Iteration 8/25 | Loss: 0.01072488
Iteration 9/25 | Loss: 0.01072488
Iteration 10/25 | Loss: 0.01072488
Iteration 11/25 | Loss: 0.01072488
Iteration 12/25 | Loss: 0.01072488
Iteration 13/25 | Loss: 0.01072488
Iteration 14/25 | Loss: 0.01072488
Iteration 15/25 | Loss: 0.01072488
Iteration 16/25 | Loss: 0.01072488
Iteration 17/25 | Loss: 0.01072488
Iteration 18/25 | Loss: 0.01072488
Iteration 19/25 | Loss: 0.01072487
Iteration 20/25 | Loss: 0.01072487
Iteration 21/25 | Loss: 0.01072487
Iteration 22/25 | Loss: 0.01072487
Iteration 23/25 | Loss: 0.01072487
Iteration 24/25 | Loss: 0.01072487
Iteration 25/25 | Loss: 0.01072487

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61538589
Iteration 2/25 | Loss: 0.06347284
Iteration 3/25 | Loss: 0.06347266
Iteration 4/25 | Loss: 0.06347264
Iteration 5/25 | Loss: 0.06347264
Iteration 6/25 | Loss: 0.06347263
Iteration 7/25 | Loss: 0.06347263
Iteration 8/25 | Loss: 0.06347263
Iteration 9/25 | Loss: 0.06347263
Iteration 10/25 | Loss: 0.06347263
Iteration 11/25 | Loss: 0.06347263
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.06347262859344482, 0.06347262859344482, 0.06347262859344482, 0.06347262859344482, 0.06347262859344482]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.06347262859344482

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.06347262
Iteration 2/1000 | Loss: 0.01231408
Iteration 3/1000 | Loss: 0.00103056
Iteration 4/1000 | Loss: 0.00041422
Iteration 5/1000 | Loss: 0.00148889
Iteration 6/1000 | Loss: 0.00646319
Iteration 7/1000 | Loss: 0.00221432
Iteration 8/1000 | Loss: 0.00021179
Iteration 9/1000 | Loss: 0.00040865
Iteration 10/1000 | Loss: 0.00096033
Iteration 11/1000 | Loss: 0.00118971
Iteration 12/1000 | Loss: 0.00125404
Iteration 13/1000 | Loss: 0.00005749
Iteration 14/1000 | Loss: 0.00004863
Iteration 15/1000 | Loss: 0.00063671
Iteration 16/1000 | Loss: 0.00289959
Iteration 17/1000 | Loss: 0.00008654
Iteration 18/1000 | Loss: 0.00004198
Iteration 19/1000 | Loss: 0.00021993
Iteration 20/1000 | Loss: 0.00003855
Iteration 21/1000 | Loss: 0.00016344
Iteration 22/1000 | Loss: 0.00028188
Iteration 23/1000 | Loss: 0.00009494
Iteration 24/1000 | Loss: 0.00003689
Iteration 25/1000 | Loss: 0.00003420
Iteration 26/1000 | Loss: 0.00019766
Iteration 27/1000 | Loss: 0.00003175
Iteration 28/1000 | Loss: 0.00003035
Iteration 29/1000 | Loss: 0.00033840
Iteration 30/1000 | Loss: 0.00002928
Iteration 31/1000 | Loss: 0.00002756
Iteration 32/1000 | Loss: 0.00031489
Iteration 33/1000 | Loss: 0.00046104
Iteration 34/1000 | Loss: 0.00058287
Iteration 35/1000 | Loss: 0.00019044
Iteration 36/1000 | Loss: 0.00002722
Iteration 37/1000 | Loss: 0.00007412
Iteration 38/1000 | Loss: 0.00002579
Iteration 39/1000 | Loss: 0.00025989
Iteration 40/1000 | Loss: 0.00005038
Iteration 41/1000 | Loss: 0.00003163
Iteration 42/1000 | Loss: 0.00007951
Iteration 43/1000 | Loss: 0.00003932
Iteration 44/1000 | Loss: 0.00007192
Iteration 45/1000 | Loss: 0.00002967
Iteration 46/1000 | Loss: 0.00003010
Iteration 47/1000 | Loss: 0.00002379
Iteration 48/1000 | Loss: 0.00008380
Iteration 49/1000 | Loss: 0.00002340
Iteration 50/1000 | Loss: 0.00002286
Iteration 51/1000 | Loss: 0.00002239
Iteration 52/1000 | Loss: 0.00002209
Iteration 53/1000 | Loss: 0.00002172
Iteration 54/1000 | Loss: 0.00002130
Iteration 55/1000 | Loss: 0.00035109
Iteration 56/1000 | Loss: 0.00006922
Iteration 57/1000 | Loss: 0.00003025
Iteration 58/1000 | Loss: 0.00002157
Iteration 59/1000 | Loss: 0.00002103
Iteration 60/1000 | Loss: 0.00002089
Iteration 61/1000 | Loss: 0.00002082
Iteration 62/1000 | Loss: 0.00002081
Iteration 63/1000 | Loss: 0.00002081
Iteration 64/1000 | Loss: 0.00002080
Iteration 65/1000 | Loss: 0.00002075
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002074
Iteration 68/1000 | Loss: 0.00002073
Iteration 69/1000 | Loss: 0.00002067
Iteration 70/1000 | Loss: 0.00002065
Iteration 71/1000 | Loss: 0.00002064
Iteration 72/1000 | Loss: 0.00002064
Iteration 73/1000 | Loss: 0.00002063
Iteration 74/1000 | Loss: 0.00002063
Iteration 75/1000 | Loss: 0.00002062
Iteration 76/1000 | Loss: 0.00002060
Iteration 77/1000 | Loss: 0.00002058
Iteration 78/1000 | Loss: 0.00002057
Iteration 79/1000 | Loss: 0.00002057
Iteration 80/1000 | Loss: 0.00002057
Iteration 81/1000 | Loss: 0.00002053
Iteration 82/1000 | Loss: 0.00002053
Iteration 83/1000 | Loss: 0.00002047
Iteration 84/1000 | Loss: 0.00002047
Iteration 85/1000 | Loss: 0.00002047
Iteration 86/1000 | Loss: 0.00002047
Iteration 87/1000 | Loss: 0.00002047
Iteration 88/1000 | Loss: 0.00002047
Iteration 89/1000 | Loss: 0.00002047
Iteration 90/1000 | Loss: 0.00002047
Iteration 91/1000 | Loss: 0.00002047
Iteration 92/1000 | Loss: 0.00002046
Iteration 93/1000 | Loss: 0.00002046
Iteration 94/1000 | Loss: 0.00002046
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002045
Iteration 98/1000 | Loss: 0.00002045
Iteration 99/1000 | Loss: 0.00002044
Iteration 100/1000 | Loss: 0.00002044
Iteration 101/1000 | Loss: 0.00002044
Iteration 102/1000 | Loss: 0.00002044
Iteration 103/1000 | Loss: 0.00002043
Iteration 104/1000 | Loss: 0.00002043
Iteration 105/1000 | Loss: 0.00002043
Iteration 106/1000 | Loss: 0.00002043
Iteration 107/1000 | Loss: 0.00002042
Iteration 108/1000 | Loss: 0.00002042
Iteration 109/1000 | Loss: 0.00002042
Iteration 110/1000 | Loss: 0.00002042
Iteration 111/1000 | Loss: 0.00002042
Iteration 112/1000 | Loss: 0.00002042
Iteration 113/1000 | Loss: 0.00002042
Iteration 114/1000 | Loss: 0.00002042
Iteration 115/1000 | Loss: 0.00002042
Iteration 116/1000 | Loss: 0.00002042
Iteration 117/1000 | Loss: 0.00002042
Iteration 118/1000 | Loss: 0.00002042
Iteration 119/1000 | Loss: 0.00002041
Iteration 120/1000 | Loss: 0.00002041
Iteration 121/1000 | Loss: 0.00002041
Iteration 122/1000 | Loss: 0.00002041
Iteration 123/1000 | Loss: 0.00002041
Iteration 124/1000 | Loss: 0.00002041
Iteration 125/1000 | Loss: 0.00002041
Iteration 126/1000 | Loss: 0.00002041
Iteration 127/1000 | Loss: 0.00002040
Iteration 128/1000 | Loss: 0.00002040
Iteration 129/1000 | Loss: 0.00002039
Iteration 130/1000 | Loss: 0.00002039
Iteration 131/1000 | Loss: 0.00002039
Iteration 132/1000 | Loss: 0.00002039
Iteration 133/1000 | Loss: 0.00002038
Iteration 134/1000 | Loss: 0.00002037
Iteration 135/1000 | Loss: 0.00002037
Iteration 136/1000 | Loss: 0.00002037
Iteration 137/1000 | Loss: 0.00002036
Iteration 138/1000 | Loss: 0.00002036
Iteration 139/1000 | Loss: 0.00002036
Iteration 140/1000 | Loss: 0.00002035
Iteration 141/1000 | Loss: 0.00002035
Iteration 142/1000 | Loss: 0.00002035
Iteration 143/1000 | Loss: 0.00002035
Iteration 144/1000 | Loss: 0.00002035
Iteration 145/1000 | Loss: 0.00002035
Iteration 146/1000 | Loss: 0.00002034
Iteration 147/1000 | Loss: 0.00002034
Iteration 148/1000 | Loss: 0.00002034
Iteration 149/1000 | Loss: 0.00002034
Iteration 150/1000 | Loss: 0.00002034
Iteration 151/1000 | Loss: 0.00002034
Iteration 152/1000 | Loss: 0.00002034
Iteration 153/1000 | Loss: 0.00002034
Iteration 154/1000 | Loss: 0.00002034
Iteration 155/1000 | Loss: 0.00002034
Iteration 156/1000 | Loss: 0.00002034
Iteration 157/1000 | Loss: 0.00002034
Iteration 158/1000 | Loss: 0.00002033
Iteration 159/1000 | Loss: 0.00002033
Iteration 160/1000 | Loss: 0.00002033
Iteration 161/1000 | Loss: 0.00002033
Iteration 162/1000 | Loss: 0.00002033
Iteration 163/1000 | Loss: 0.00002033
Iteration 164/1000 | Loss: 0.00002033
Iteration 165/1000 | Loss: 0.00002033
Iteration 166/1000 | Loss: 0.00029671
Iteration 167/1000 | Loss: 0.00003306
Iteration 168/1000 | Loss: 0.00002059
Iteration 169/1000 | Loss: 0.00010058
Iteration 170/1000 | Loss: 0.00002392
Iteration 171/1000 | Loss: 0.00002052
Iteration 172/1000 | Loss: 0.00002037
Iteration 173/1000 | Loss: 0.00002037
Iteration 174/1000 | Loss: 0.00002036
Iteration 175/1000 | Loss: 0.00002034
Iteration 176/1000 | Loss: 0.00002034
Iteration 177/1000 | Loss: 0.00002034
Iteration 178/1000 | Loss: 0.00002034
Iteration 179/1000 | Loss: 0.00002034
Iteration 180/1000 | Loss: 0.00002033
Iteration 181/1000 | Loss: 0.00002033
Iteration 182/1000 | Loss: 0.00002033
Iteration 183/1000 | Loss: 0.00002033
Iteration 184/1000 | Loss: 0.00002033
Iteration 185/1000 | Loss: 0.00002033
Iteration 186/1000 | Loss: 0.00002031
Iteration 187/1000 | Loss: 0.00002031
Iteration 188/1000 | Loss: 0.00002029
Iteration 189/1000 | Loss: 0.00002028
Iteration 190/1000 | Loss: 0.00002028
Iteration 191/1000 | Loss: 0.00002028
Iteration 192/1000 | Loss: 0.00002028
Iteration 193/1000 | Loss: 0.00002028
Iteration 194/1000 | Loss: 0.00002028
Iteration 195/1000 | Loss: 0.00002028
Iteration 196/1000 | Loss: 0.00002028
Iteration 197/1000 | Loss: 0.00002028
Iteration 198/1000 | Loss: 0.00002028
Iteration 199/1000 | Loss: 0.00002027
Iteration 200/1000 | Loss: 0.00002027
Iteration 201/1000 | Loss: 0.00002027
Iteration 202/1000 | Loss: 0.00002027
Iteration 203/1000 | Loss: 0.00002027
Iteration 204/1000 | Loss: 0.00002027
Iteration 205/1000 | Loss: 0.00002027
Iteration 206/1000 | Loss: 0.00002027
Iteration 207/1000 | Loss: 0.00002027
Iteration 208/1000 | Loss: 0.00002027
Iteration 209/1000 | Loss: 0.00002027
Iteration 210/1000 | Loss: 0.00002027
Iteration 211/1000 | Loss: 0.00002027
Iteration 212/1000 | Loss: 0.00002027
Iteration 213/1000 | Loss: 0.00002027
Iteration 214/1000 | Loss: 0.00002027
Iteration 215/1000 | Loss: 0.00002027
Iteration 216/1000 | Loss: 0.00002027
Iteration 217/1000 | Loss: 0.00002027
Iteration 218/1000 | Loss: 0.00002027
Iteration 219/1000 | Loss: 0.00002027
Iteration 220/1000 | Loss: 0.00002027
Iteration 221/1000 | Loss: 0.00002027
Iteration 222/1000 | Loss: 0.00002027
Iteration 223/1000 | Loss: 0.00002027
Iteration 224/1000 | Loss: 0.00002027
Iteration 225/1000 | Loss: 0.00002027
Iteration 226/1000 | Loss: 0.00002027
Iteration 227/1000 | Loss: 0.00002027
Iteration 228/1000 | Loss: 0.00002027
Iteration 229/1000 | Loss: 0.00002027
Iteration 230/1000 | Loss: 0.00002027
Iteration 231/1000 | Loss: 0.00002027
Iteration 232/1000 | Loss: 0.00002027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [2.0269268134143203e-05, 2.0269268134143203e-05, 2.0269268134143203e-05, 2.0269268134143203e-05, 2.0269268134143203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0269268134143203e-05

Optimization complete. Final v2v error: 3.4469780921936035 mm

Highest mean error: 20.751544952392578 mm for frame 138

Lowest mean error: 2.8455607891082764 mm for frame 73

Saving results

Total time: 125.15610575675964
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485653
Iteration 2/25 | Loss: 0.00119368
Iteration 3/25 | Loss: 0.00112654
Iteration 4/25 | Loss: 0.00111701
Iteration 5/25 | Loss: 0.00111431
Iteration 6/25 | Loss: 0.00111394
Iteration 7/25 | Loss: 0.00111394
Iteration 8/25 | Loss: 0.00111394
Iteration 9/25 | Loss: 0.00111394
Iteration 10/25 | Loss: 0.00111394
Iteration 11/25 | Loss: 0.00111395
Iteration 12/25 | Loss: 0.00111395
Iteration 13/25 | Loss: 0.00111394
Iteration 14/25 | Loss: 0.00111395
Iteration 15/25 | Loss: 0.00111395
Iteration 16/25 | Loss: 0.00111394
Iteration 17/25 | Loss: 0.00111394
Iteration 18/25 | Loss: 0.00111394
Iteration 19/25 | Loss: 0.00111395
Iteration 20/25 | Loss: 0.00111394
Iteration 21/25 | Loss: 0.00111394
Iteration 22/25 | Loss: 0.00111394
Iteration 23/25 | Loss: 0.00111394
Iteration 24/25 | Loss: 0.00111394
Iteration 25/25 | Loss: 0.00111394
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011139449197798967, 0.0011139449197798967, 0.0011139449197798967, 0.0011139449197798967, 0.0011139449197798967]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011139449197798967

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42645800
Iteration 2/25 | Loss: 0.00060620
Iteration 3/25 | Loss: 0.00060616
Iteration 4/25 | Loss: 0.00060616
Iteration 5/25 | Loss: 0.00060616
Iteration 6/25 | Loss: 0.00060616
Iteration 7/25 | Loss: 0.00060616
Iteration 8/25 | Loss: 0.00060616
Iteration 9/25 | Loss: 0.00060616
Iteration 10/25 | Loss: 0.00060616
Iteration 11/25 | Loss: 0.00060616
Iteration 12/25 | Loss: 0.00060616
Iteration 13/25 | Loss: 0.00060616
Iteration 14/25 | Loss: 0.00060616
Iteration 15/25 | Loss: 0.00060616
Iteration 16/25 | Loss: 0.00060616
Iteration 17/25 | Loss: 0.00060616
Iteration 18/25 | Loss: 0.00060616
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006061617168597877, 0.0006061617168597877, 0.0006061617168597877, 0.0006061617168597877, 0.0006061617168597877]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006061617168597877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060616
Iteration 2/1000 | Loss: 0.00002687
Iteration 3/1000 | Loss: 0.00001750
Iteration 4/1000 | Loss: 0.00001573
Iteration 5/1000 | Loss: 0.00001505
Iteration 6/1000 | Loss: 0.00001460
Iteration 7/1000 | Loss: 0.00001423
Iteration 8/1000 | Loss: 0.00001400
Iteration 9/1000 | Loss: 0.00001398
Iteration 10/1000 | Loss: 0.00001397
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001367
Iteration 13/1000 | Loss: 0.00001351
Iteration 14/1000 | Loss: 0.00001349
Iteration 15/1000 | Loss: 0.00001344
Iteration 16/1000 | Loss: 0.00001343
Iteration 17/1000 | Loss: 0.00001343
Iteration 18/1000 | Loss: 0.00001342
Iteration 19/1000 | Loss: 0.00001339
Iteration 20/1000 | Loss: 0.00001338
Iteration 21/1000 | Loss: 0.00001338
Iteration 22/1000 | Loss: 0.00001337
Iteration 23/1000 | Loss: 0.00001337
Iteration 24/1000 | Loss: 0.00001337
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001336
Iteration 27/1000 | Loss: 0.00001336
Iteration 28/1000 | Loss: 0.00001334
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001332
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001331
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001329
Iteration 37/1000 | Loss: 0.00001329
Iteration 38/1000 | Loss: 0.00001328
Iteration 39/1000 | Loss: 0.00001328
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001328
Iteration 42/1000 | Loss: 0.00001327
Iteration 43/1000 | Loss: 0.00001327
Iteration 44/1000 | Loss: 0.00001324
Iteration 45/1000 | Loss: 0.00001323
Iteration 46/1000 | Loss: 0.00001322
Iteration 47/1000 | Loss: 0.00001322
Iteration 48/1000 | Loss: 0.00001322
Iteration 49/1000 | Loss: 0.00001322
Iteration 50/1000 | Loss: 0.00001321
Iteration 51/1000 | Loss: 0.00001321
Iteration 52/1000 | Loss: 0.00001321
Iteration 53/1000 | Loss: 0.00001320
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001319
Iteration 57/1000 | Loss: 0.00001319
Iteration 58/1000 | Loss: 0.00001318
Iteration 59/1000 | Loss: 0.00001318
Iteration 60/1000 | Loss: 0.00001318
Iteration 61/1000 | Loss: 0.00001318
Iteration 62/1000 | Loss: 0.00001317
Iteration 63/1000 | Loss: 0.00001317
Iteration 64/1000 | Loss: 0.00001317
Iteration 65/1000 | Loss: 0.00001317
Iteration 66/1000 | Loss: 0.00001317
Iteration 67/1000 | Loss: 0.00001316
Iteration 68/1000 | Loss: 0.00001316
Iteration 69/1000 | Loss: 0.00001316
Iteration 70/1000 | Loss: 0.00001316
Iteration 71/1000 | Loss: 0.00001316
Iteration 72/1000 | Loss: 0.00001316
Iteration 73/1000 | Loss: 0.00001315
Iteration 74/1000 | Loss: 0.00001315
Iteration 75/1000 | Loss: 0.00001315
Iteration 76/1000 | Loss: 0.00001315
Iteration 77/1000 | Loss: 0.00001314
Iteration 78/1000 | Loss: 0.00001314
Iteration 79/1000 | Loss: 0.00001313
Iteration 80/1000 | Loss: 0.00001313
Iteration 81/1000 | Loss: 0.00001313
Iteration 82/1000 | Loss: 0.00001313
Iteration 83/1000 | Loss: 0.00001313
Iteration 84/1000 | Loss: 0.00001313
Iteration 85/1000 | Loss: 0.00001312
Iteration 86/1000 | Loss: 0.00001312
Iteration 87/1000 | Loss: 0.00001312
Iteration 88/1000 | Loss: 0.00001311
Iteration 89/1000 | Loss: 0.00001311
Iteration 90/1000 | Loss: 0.00001311
Iteration 91/1000 | Loss: 0.00001310
Iteration 92/1000 | Loss: 0.00001310
Iteration 93/1000 | Loss: 0.00001310
Iteration 94/1000 | Loss: 0.00001309
Iteration 95/1000 | Loss: 0.00001309
Iteration 96/1000 | Loss: 0.00001309
Iteration 97/1000 | Loss: 0.00001308
Iteration 98/1000 | Loss: 0.00001307
Iteration 99/1000 | Loss: 0.00001307
Iteration 100/1000 | Loss: 0.00001306
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001305
Iteration 107/1000 | Loss: 0.00001305
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001304
Iteration 113/1000 | Loss: 0.00001303
Iteration 114/1000 | Loss: 0.00001303
Iteration 115/1000 | Loss: 0.00001303
Iteration 116/1000 | Loss: 0.00001303
Iteration 117/1000 | Loss: 0.00001303
Iteration 118/1000 | Loss: 0.00001303
Iteration 119/1000 | Loss: 0.00001303
Iteration 120/1000 | Loss: 0.00001303
Iteration 121/1000 | Loss: 0.00001303
Iteration 122/1000 | Loss: 0.00001302
Iteration 123/1000 | Loss: 0.00001302
Iteration 124/1000 | Loss: 0.00001302
Iteration 125/1000 | Loss: 0.00001302
Iteration 126/1000 | Loss: 0.00001301
Iteration 127/1000 | Loss: 0.00001301
Iteration 128/1000 | Loss: 0.00001301
Iteration 129/1000 | Loss: 0.00001300
Iteration 130/1000 | Loss: 0.00001300
Iteration 131/1000 | Loss: 0.00001299
Iteration 132/1000 | Loss: 0.00001299
Iteration 133/1000 | Loss: 0.00001299
Iteration 134/1000 | Loss: 0.00001298
Iteration 135/1000 | Loss: 0.00001298
Iteration 136/1000 | Loss: 0.00001298
Iteration 137/1000 | Loss: 0.00001297
Iteration 138/1000 | Loss: 0.00001297
Iteration 139/1000 | Loss: 0.00001296
Iteration 140/1000 | Loss: 0.00001296
Iteration 141/1000 | Loss: 0.00001296
Iteration 142/1000 | Loss: 0.00001296
Iteration 143/1000 | Loss: 0.00001296
Iteration 144/1000 | Loss: 0.00001296
Iteration 145/1000 | Loss: 0.00001296
Iteration 146/1000 | Loss: 0.00001295
Iteration 147/1000 | Loss: 0.00001295
Iteration 148/1000 | Loss: 0.00001295
Iteration 149/1000 | Loss: 0.00001295
Iteration 150/1000 | Loss: 0.00001295
Iteration 151/1000 | Loss: 0.00001295
Iteration 152/1000 | Loss: 0.00001295
Iteration 153/1000 | Loss: 0.00001295
Iteration 154/1000 | Loss: 0.00001295
Iteration 155/1000 | Loss: 0.00001294
Iteration 156/1000 | Loss: 0.00001294
Iteration 157/1000 | Loss: 0.00001294
Iteration 158/1000 | Loss: 0.00001294
Iteration 159/1000 | Loss: 0.00001294
Iteration 160/1000 | Loss: 0.00001293
Iteration 161/1000 | Loss: 0.00001293
Iteration 162/1000 | Loss: 0.00001293
Iteration 163/1000 | Loss: 0.00001293
Iteration 164/1000 | Loss: 0.00001293
Iteration 165/1000 | Loss: 0.00001293
Iteration 166/1000 | Loss: 0.00001293
Iteration 167/1000 | Loss: 0.00001293
Iteration 168/1000 | Loss: 0.00001293
Iteration 169/1000 | Loss: 0.00001293
Iteration 170/1000 | Loss: 0.00001293
Iteration 171/1000 | Loss: 0.00001292
Iteration 172/1000 | Loss: 0.00001292
Iteration 173/1000 | Loss: 0.00001292
Iteration 174/1000 | Loss: 0.00001292
Iteration 175/1000 | Loss: 0.00001292
Iteration 176/1000 | Loss: 0.00001292
Iteration 177/1000 | Loss: 0.00001292
Iteration 178/1000 | Loss: 0.00001292
Iteration 179/1000 | Loss: 0.00001292
Iteration 180/1000 | Loss: 0.00001292
Iteration 181/1000 | Loss: 0.00001292
Iteration 182/1000 | Loss: 0.00001292
Iteration 183/1000 | Loss: 0.00001292
Iteration 184/1000 | Loss: 0.00001292
Iteration 185/1000 | Loss: 0.00001292
Iteration 186/1000 | Loss: 0.00001292
Iteration 187/1000 | Loss: 0.00001292
Iteration 188/1000 | Loss: 0.00001292
Iteration 189/1000 | Loss: 0.00001292
Iteration 190/1000 | Loss: 0.00001291
Iteration 191/1000 | Loss: 0.00001291
Iteration 192/1000 | Loss: 0.00001291
Iteration 193/1000 | Loss: 0.00001291
Iteration 194/1000 | Loss: 0.00001291
Iteration 195/1000 | Loss: 0.00001291
Iteration 196/1000 | Loss: 0.00001291
Iteration 197/1000 | Loss: 0.00001291
Iteration 198/1000 | Loss: 0.00001291
Iteration 199/1000 | Loss: 0.00001291
Iteration 200/1000 | Loss: 0.00001291
Iteration 201/1000 | Loss: 0.00001291
Iteration 202/1000 | Loss: 0.00001291
Iteration 203/1000 | Loss: 0.00001291
Iteration 204/1000 | Loss: 0.00001291
Iteration 205/1000 | Loss: 0.00001291
Iteration 206/1000 | Loss: 0.00001291
Iteration 207/1000 | Loss: 0.00001291
Iteration 208/1000 | Loss: 0.00001291
Iteration 209/1000 | Loss: 0.00001291
Iteration 210/1000 | Loss: 0.00001291
Iteration 211/1000 | Loss: 0.00001291
Iteration 212/1000 | Loss: 0.00001291
Iteration 213/1000 | Loss: 0.00001291
Iteration 214/1000 | Loss: 0.00001291
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.2910528312204406e-05, 1.2910528312204406e-05, 1.2910528312204406e-05, 1.2910528312204406e-05, 1.2910528312204406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2910528312204406e-05

Optimization complete. Final v2v error: 3.016263246536255 mm

Highest mean error: 3.4758431911468506 mm for frame 48

Lowest mean error: 2.728456497192383 mm for frame 147

Saving results

Total time: 39.09643268585205
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026095
Iteration 2/25 | Loss: 0.01026095
Iteration 3/25 | Loss: 0.00281208
Iteration 4/25 | Loss: 0.00204607
Iteration 5/25 | Loss: 0.00186804
Iteration 6/25 | Loss: 0.00173391
Iteration 7/25 | Loss: 0.00168626
Iteration 8/25 | Loss: 0.00163935
Iteration 9/25 | Loss: 0.00157709
Iteration 10/25 | Loss: 0.00156631
Iteration 11/25 | Loss: 0.00150921
Iteration 12/25 | Loss: 0.00151421
Iteration 13/25 | Loss: 0.00149106
Iteration 14/25 | Loss: 0.00148368
Iteration 15/25 | Loss: 0.00147202
Iteration 16/25 | Loss: 0.00146240
Iteration 17/25 | Loss: 0.00145600
Iteration 18/25 | Loss: 0.00145204
Iteration 19/25 | Loss: 0.00145061
Iteration 20/25 | Loss: 0.00145010
Iteration 21/25 | Loss: 0.00144994
Iteration 22/25 | Loss: 0.00145092
Iteration 23/25 | Loss: 0.00144874
Iteration 24/25 | Loss: 0.00145458
Iteration 25/25 | Loss: 0.00145242

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35598385
Iteration 2/25 | Loss: 0.00565809
Iteration 3/25 | Loss: 0.00391181
Iteration 4/25 | Loss: 0.00391181
Iteration 5/25 | Loss: 0.00391181
Iteration 6/25 | Loss: 0.00391181
Iteration 7/25 | Loss: 0.00391181
Iteration 8/25 | Loss: 0.00391181
Iteration 9/25 | Loss: 0.00391181
Iteration 10/25 | Loss: 0.00391180
Iteration 11/25 | Loss: 0.00391180
Iteration 12/25 | Loss: 0.00391180
Iteration 13/25 | Loss: 0.00391180
Iteration 14/25 | Loss: 0.00391180
Iteration 15/25 | Loss: 0.00391180
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003911804873496294, 0.003911804873496294, 0.003911804873496294, 0.003911804873496294, 0.003911804873496294]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003911804873496294

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00391180
Iteration 2/1000 | Loss: 0.00115546
Iteration 3/1000 | Loss: 0.00264286
Iteration 4/1000 | Loss: 0.00385847
Iteration 5/1000 | Loss: 0.00079044
Iteration 6/1000 | Loss: 0.00094160
Iteration 7/1000 | Loss: 0.00115691
Iteration 8/1000 | Loss: 0.00076113
Iteration 9/1000 | Loss: 0.00037450
Iteration 10/1000 | Loss: 0.00137875
Iteration 11/1000 | Loss: 0.00099357
Iteration 12/1000 | Loss: 0.00063418
Iteration 13/1000 | Loss: 0.00180072
Iteration 14/1000 | Loss: 0.00189670
Iteration 15/1000 | Loss: 0.00391717
Iteration 16/1000 | Loss: 0.00299135
Iteration 17/1000 | Loss: 0.00063593
Iteration 18/1000 | Loss: 0.00076037
Iteration 19/1000 | Loss: 0.00061867
Iteration 20/1000 | Loss: 0.00105191
Iteration 21/1000 | Loss: 0.00088101
Iteration 22/1000 | Loss: 0.00091194
Iteration 23/1000 | Loss: 0.00154233
Iteration 24/1000 | Loss: 0.00063071
Iteration 25/1000 | Loss: 0.00052755
Iteration 26/1000 | Loss: 0.00125778
Iteration 27/1000 | Loss: 0.00048997
Iteration 28/1000 | Loss: 0.00061458
Iteration 29/1000 | Loss: 0.00133221
Iteration 30/1000 | Loss: 0.00423537
Iteration 31/1000 | Loss: 0.00339410
Iteration 32/1000 | Loss: 0.00119979
Iteration 33/1000 | Loss: 0.00046246
Iteration 34/1000 | Loss: 0.00057549
Iteration 35/1000 | Loss: 0.00052681
Iteration 36/1000 | Loss: 0.00156542
Iteration 37/1000 | Loss: 0.00081057
Iteration 38/1000 | Loss: 0.00086977
Iteration 39/1000 | Loss: 0.00036478
Iteration 40/1000 | Loss: 0.00049603
Iteration 41/1000 | Loss: 0.00083829
Iteration 42/1000 | Loss: 0.00152558
Iteration 43/1000 | Loss: 0.00230063
Iteration 44/1000 | Loss: 0.00153777
Iteration 45/1000 | Loss: 0.00120670
Iteration 46/1000 | Loss: 0.00046356
Iteration 47/1000 | Loss: 0.00056825
Iteration 48/1000 | Loss: 0.00074284
Iteration 49/1000 | Loss: 0.00031538
Iteration 50/1000 | Loss: 0.00066456
Iteration 51/1000 | Loss: 0.00113967
Iteration 52/1000 | Loss: 0.00048926
Iteration 53/1000 | Loss: 0.00093786
Iteration 54/1000 | Loss: 0.00400731
Iteration 55/1000 | Loss: 0.00035114
Iteration 56/1000 | Loss: 0.00084151
Iteration 57/1000 | Loss: 0.00052635
Iteration 58/1000 | Loss: 0.00049015
Iteration 59/1000 | Loss: 0.00018211
Iteration 60/1000 | Loss: 0.00012213
Iteration 61/1000 | Loss: 0.00065800
Iteration 62/1000 | Loss: 0.00078677
Iteration 63/1000 | Loss: 0.00072692
Iteration 64/1000 | Loss: 0.00160727
Iteration 65/1000 | Loss: 0.00029464
Iteration 66/1000 | Loss: 0.00050045
Iteration 67/1000 | Loss: 0.00012208
Iteration 68/1000 | Loss: 0.00010839
Iteration 69/1000 | Loss: 0.00025807
Iteration 70/1000 | Loss: 0.00051177
Iteration 71/1000 | Loss: 0.00016296
Iteration 72/1000 | Loss: 0.00012327
Iteration 73/1000 | Loss: 0.00009589
Iteration 74/1000 | Loss: 0.00009120
Iteration 75/1000 | Loss: 0.00008795
Iteration 76/1000 | Loss: 0.00008559
Iteration 77/1000 | Loss: 0.00008416
Iteration 78/1000 | Loss: 0.00008326
Iteration 79/1000 | Loss: 0.00021350
Iteration 80/1000 | Loss: 0.00009835
Iteration 81/1000 | Loss: 0.00009320
Iteration 82/1000 | Loss: 0.00008149
Iteration 83/1000 | Loss: 0.00082154
Iteration 84/1000 | Loss: 0.00027509
Iteration 85/1000 | Loss: 0.00017274
Iteration 86/1000 | Loss: 0.00008674
Iteration 87/1000 | Loss: 0.00018096
Iteration 88/1000 | Loss: 0.00012763
Iteration 89/1000 | Loss: 0.00012224
Iteration 90/1000 | Loss: 0.00008094
Iteration 91/1000 | Loss: 0.00008608
Iteration 92/1000 | Loss: 0.00016206
Iteration 93/1000 | Loss: 0.00007895
Iteration 94/1000 | Loss: 0.00007850
Iteration 95/1000 | Loss: 0.00017260
Iteration 96/1000 | Loss: 0.00007843
Iteration 97/1000 | Loss: 0.00007806
Iteration 98/1000 | Loss: 0.00017714
Iteration 99/1000 | Loss: 0.00007932
Iteration 100/1000 | Loss: 0.00007767
Iteration 101/1000 | Loss: 0.00007737
Iteration 102/1000 | Loss: 0.00015846
Iteration 103/1000 | Loss: 0.00021204
Iteration 104/1000 | Loss: 0.00036397
Iteration 105/1000 | Loss: 0.00009372
Iteration 106/1000 | Loss: 0.00009789
Iteration 107/1000 | Loss: 0.00007685
Iteration 108/1000 | Loss: 0.00030304
Iteration 109/1000 | Loss: 0.00011082
Iteration 110/1000 | Loss: 0.00008826
Iteration 111/1000 | Loss: 0.00013233
Iteration 112/1000 | Loss: 0.00017107
Iteration 113/1000 | Loss: 0.00007536
Iteration 114/1000 | Loss: 0.00007469
Iteration 115/1000 | Loss: 0.00027550
Iteration 116/1000 | Loss: 0.00012269
Iteration 117/1000 | Loss: 0.00008375
Iteration 118/1000 | Loss: 0.00007219
Iteration 119/1000 | Loss: 0.00018163
Iteration 120/1000 | Loss: 0.00010133
Iteration 121/1000 | Loss: 0.00007036
Iteration 122/1000 | Loss: 0.00015952
Iteration 123/1000 | Loss: 0.00155500
Iteration 124/1000 | Loss: 0.00083995
Iteration 125/1000 | Loss: 0.00076749
Iteration 126/1000 | Loss: 0.00034875
Iteration 127/1000 | Loss: 0.00128818
Iteration 128/1000 | Loss: 0.00032554
Iteration 129/1000 | Loss: 0.00034274
Iteration 130/1000 | Loss: 0.00008662
Iteration 131/1000 | Loss: 0.00005566
Iteration 132/1000 | Loss: 0.00036948
Iteration 133/1000 | Loss: 0.00080271
Iteration 134/1000 | Loss: 0.00086038
Iteration 135/1000 | Loss: 0.00076573
Iteration 136/1000 | Loss: 0.00149753
Iteration 137/1000 | Loss: 0.00009748
Iteration 138/1000 | Loss: 0.00006187
Iteration 139/1000 | Loss: 0.00051982
Iteration 140/1000 | Loss: 0.00004016
Iteration 141/1000 | Loss: 0.00003674
Iteration 142/1000 | Loss: 0.00003510
Iteration 143/1000 | Loss: 0.00015691
Iteration 144/1000 | Loss: 0.00007196
Iteration 145/1000 | Loss: 0.00017858
Iteration 146/1000 | Loss: 0.00106131
Iteration 147/1000 | Loss: 0.00130522
Iteration 148/1000 | Loss: 0.00024803
Iteration 149/1000 | Loss: 0.00026144
Iteration 150/1000 | Loss: 0.00010877
Iteration 151/1000 | Loss: 0.00005252
Iteration 152/1000 | Loss: 0.00003781
Iteration 153/1000 | Loss: 0.00023697
Iteration 154/1000 | Loss: 0.00016619
Iteration 155/1000 | Loss: 0.00057735
Iteration 156/1000 | Loss: 0.00090652
Iteration 157/1000 | Loss: 0.00031837
Iteration 158/1000 | Loss: 0.00013022
Iteration 159/1000 | Loss: 0.00048774
Iteration 160/1000 | Loss: 0.00003475
Iteration 161/1000 | Loss: 0.00003167
Iteration 162/1000 | Loss: 0.00011814
Iteration 163/1000 | Loss: 0.00015858
Iteration 164/1000 | Loss: 0.00003025
Iteration 165/1000 | Loss: 0.00008938
Iteration 166/1000 | Loss: 0.00002890
Iteration 167/1000 | Loss: 0.00002860
Iteration 168/1000 | Loss: 0.00002841
Iteration 169/1000 | Loss: 0.00002833
Iteration 170/1000 | Loss: 0.00002815
Iteration 171/1000 | Loss: 0.00002811
Iteration 172/1000 | Loss: 0.00002810
Iteration 173/1000 | Loss: 0.00002809
Iteration 174/1000 | Loss: 0.00013366
Iteration 175/1000 | Loss: 0.00002966
Iteration 176/1000 | Loss: 0.00009437
Iteration 177/1000 | Loss: 0.00002821
Iteration 178/1000 | Loss: 0.00002800
Iteration 179/1000 | Loss: 0.00002798
Iteration 180/1000 | Loss: 0.00002794
Iteration 181/1000 | Loss: 0.00002794
Iteration 182/1000 | Loss: 0.00002793
Iteration 183/1000 | Loss: 0.00002793
Iteration 184/1000 | Loss: 0.00002792
Iteration 185/1000 | Loss: 0.00002791
Iteration 186/1000 | Loss: 0.00002791
Iteration 187/1000 | Loss: 0.00002790
Iteration 188/1000 | Loss: 0.00002790
Iteration 189/1000 | Loss: 0.00002790
Iteration 190/1000 | Loss: 0.00002789
Iteration 191/1000 | Loss: 0.00002789
Iteration 192/1000 | Loss: 0.00002789
Iteration 193/1000 | Loss: 0.00002789
Iteration 194/1000 | Loss: 0.00002789
Iteration 195/1000 | Loss: 0.00002789
Iteration 196/1000 | Loss: 0.00002788
Iteration 197/1000 | Loss: 0.00002788
Iteration 198/1000 | Loss: 0.00002788
Iteration 199/1000 | Loss: 0.00002787
Iteration 200/1000 | Loss: 0.00002787
Iteration 201/1000 | Loss: 0.00002787
Iteration 202/1000 | Loss: 0.00002787
Iteration 203/1000 | Loss: 0.00002787
Iteration 204/1000 | Loss: 0.00002787
Iteration 205/1000 | Loss: 0.00002786
Iteration 206/1000 | Loss: 0.00002786
Iteration 207/1000 | Loss: 0.00002786
Iteration 208/1000 | Loss: 0.00002786
Iteration 209/1000 | Loss: 0.00002785
Iteration 210/1000 | Loss: 0.00002785
Iteration 211/1000 | Loss: 0.00002785
Iteration 212/1000 | Loss: 0.00002784
Iteration 213/1000 | Loss: 0.00002784
Iteration 214/1000 | Loss: 0.00002784
Iteration 215/1000 | Loss: 0.00002783
Iteration 216/1000 | Loss: 0.00002781
Iteration 217/1000 | Loss: 0.00002781
Iteration 218/1000 | Loss: 0.00002780
Iteration 219/1000 | Loss: 0.00012294
Iteration 220/1000 | Loss: 0.00003004
Iteration 221/1000 | Loss: 0.00004215
Iteration 222/1000 | Loss: 0.00002785
Iteration 223/1000 | Loss: 0.00002783
Iteration 224/1000 | Loss: 0.00002780
Iteration 225/1000 | Loss: 0.00002779
Iteration 226/1000 | Loss: 0.00002779
Iteration 227/1000 | Loss: 0.00002778
Iteration 228/1000 | Loss: 0.00002778
Iteration 229/1000 | Loss: 0.00002778
Iteration 230/1000 | Loss: 0.00002777
Iteration 231/1000 | Loss: 0.00002777
Iteration 232/1000 | Loss: 0.00002777
Iteration 233/1000 | Loss: 0.00002777
Iteration 234/1000 | Loss: 0.00002776
Iteration 235/1000 | Loss: 0.00002776
Iteration 236/1000 | Loss: 0.00002775
Iteration 237/1000 | Loss: 0.00002775
Iteration 238/1000 | Loss: 0.00002775
Iteration 239/1000 | Loss: 0.00002775
Iteration 240/1000 | Loss: 0.00002774
Iteration 241/1000 | Loss: 0.00002774
Iteration 242/1000 | Loss: 0.00002774
Iteration 243/1000 | Loss: 0.00002773
Iteration 244/1000 | Loss: 0.00002772
Iteration 245/1000 | Loss: 0.00002772
Iteration 246/1000 | Loss: 0.00002772
Iteration 247/1000 | Loss: 0.00002772
Iteration 248/1000 | Loss: 0.00002771
Iteration 249/1000 | Loss: 0.00002770
Iteration 250/1000 | Loss: 0.00002769
Iteration 251/1000 | Loss: 0.00002769
Iteration 252/1000 | Loss: 0.00002769
Iteration 253/1000 | Loss: 0.00002769
Iteration 254/1000 | Loss: 0.00002769
Iteration 255/1000 | Loss: 0.00002768
Iteration 256/1000 | Loss: 0.00002768
Iteration 257/1000 | Loss: 0.00002768
Iteration 258/1000 | Loss: 0.00002768
Iteration 259/1000 | Loss: 0.00002767
Iteration 260/1000 | Loss: 0.00002767
Iteration 261/1000 | Loss: 0.00002767
Iteration 262/1000 | Loss: 0.00002767
Iteration 263/1000 | Loss: 0.00002767
Iteration 264/1000 | Loss: 0.00002767
Iteration 265/1000 | Loss: 0.00002767
Iteration 266/1000 | Loss: 0.00002767
Iteration 267/1000 | Loss: 0.00002766
Iteration 268/1000 | Loss: 0.00002766
Iteration 269/1000 | Loss: 0.00002766
Iteration 270/1000 | Loss: 0.00002766
Iteration 271/1000 | Loss: 0.00002766
Iteration 272/1000 | Loss: 0.00002766
Iteration 273/1000 | Loss: 0.00002766
Iteration 274/1000 | Loss: 0.00002766
Iteration 275/1000 | Loss: 0.00002766
Iteration 276/1000 | Loss: 0.00002766
Iteration 277/1000 | Loss: 0.00002766
Iteration 278/1000 | Loss: 0.00002766
Iteration 279/1000 | Loss: 0.00002766
Iteration 280/1000 | Loss: 0.00002766
Iteration 281/1000 | Loss: 0.00002766
Iteration 282/1000 | Loss: 0.00002766
Iteration 283/1000 | Loss: 0.00002766
Iteration 284/1000 | Loss: 0.00002766
Iteration 285/1000 | Loss: 0.00002766
Iteration 286/1000 | Loss: 0.00002766
Iteration 287/1000 | Loss: 0.00002766
Iteration 288/1000 | Loss: 0.00002766
Iteration 289/1000 | Loss: 0.00002766
Iteration 290/1000 | Loss: 0.00002766
Iteration 291/1000 | Loss: 0.00002766
Iteration 292/1000 | Loss: 0.00002766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [2.7655074518406764e-05, 2.7655074518406764e-05, 2.7655074518406764e-05, 2.7655074518406764e-05, 2.7655074518406764e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7655074518406764e-05

Optimization complete. Final v2v error: 3.5197970867156982 mm

Highest mean error: 11.204750061035156 mm for frame 188

Lowest mean error: 2.9789559841156006 mm for frame 63

Saving results

Total time: 341.60349345207214
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404433
Iteration 2/25 | Loss: 0.00127844
Iteration 3/25 | Loss: 0.00110302
Iteration 4/25 | Loss: 0.00108938
Iteration 5/25 | Loss: 0.00108754
Iteration 6/25 | Loss: 0.00108696
Iteration 7/25 | Loss: 0.00108696
Iteration 8/25 | Loss: 0.00108696
Iteration 9/25 | Loss: 0.00108696
Iteration 10/25 | Loss: 0.00108696
Iteration 11/25 | Loss: 0.00108696
Iteration 12/25 | Loss: 0.00108696
Iteration 13/25 | Loss: 0.00108696
Iteration 14/25 | Loss: 0.00108696
Iteration 15/25 | Loss: 0.00108696
Iteration 16/25 | Loss: 0.00108696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010869598481804132, 0.0010869598481804132, 0.0010869598481804132, 0.0010869598481804132, 0.0010869598481804132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010869598481804132

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39225149
Iteration 2/25 | Loss: 0.00047987
Iteration 3/25 | Loss: 0.00047987
Iteration 4/25 | Loss: 0.00047987
Iteration 5/25 | Loss: 0.00047987
Iteration 6/25 | Loss: 0.00047987
Iteration 7/25 | Loss: 0.00047987
Iteration 8/25 | Loss: 0.00047987
Iteration 9/25 | Loss: 0.00047987
Iteration 10/25 | Loss: 0.00047986
Iteration 11/25 | Loss: 0.00047986
Iteration 12/25 | Loss: 0.00047986
Iteration 13/25 | Loss: 0.00047986
Iteration 14/25 | Loss: 0.00047986
Iteration 15/25 | Loss: 0.00047986
Iteration 16/25 | Loss: 0.00047986
Iteration 17/25 | Loss: 0.00047986
Iteration 18/25 | Loss: 0.00047986
Iteration 19/25 | Loss: 0.00047986
Iteration 20/25 | Loss: 0.00047986
Iteration 21/25 | Loss: 0.00047986
Iteration 22/25 | Loss: 0.00047986
Iteration 23/25 | Loss: 0.00047986
Iteration 24/25 | Loss: 0.00047986
Iteration 25/25 | Loss: 0.00047986

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00047986
Iteration 2/1000 | Loss: 0.00002434
Iteration 3/1000 | Loss: 0.00001692
Iteration 4/1000 | Loss: 0.00001528
Iteration 5/1000 | Loss: 0.00001433
Iteration 6/1000 | Loss: 0.00001384
Iteration 7/1000 | Loss: 0.00001335
Iteration 8/1000 | Loss: 0.00001308
Iteration 9/1000 | Loss: 0.00001289
Iteration 10/1000 | Loss: 0.00001276
Iteration 11/1000 | Loss: 0.00001274
Iteration 12/1000 | Loss: 0.00001264
Iteration 13/1000 | Loss: 0.00001257
Iteration 14/1000 | Loss: 0.00001250
Iteration 15/1000 | Loss: 0.00001250
Iteration 16/1000 | Loss: 0.00001246
Iteration 17/1000 | Loss: 0.00001245
Iteration 18/1000 | Loss: 0.00001245
Iteration 19/1000 | Loss: 0.00001245
Iteration 20/1000 | Loss: 0.00001243
Iteration 21/1000 | Loss: 0.00001241
Iteration 22/1000 | Loss: 0.00001241
Iteration 23/1000 | Loss: 0.00001240
Iteration 24/1000 | Loss: 0.00001240
Iteration 25/1000 | Loss: 0.00001240
Iteration 26/1000 | Loss: 0.00001239
Iteration 27/1000 | Loss: 0.00001238
Iteration 28/1000 | Loss: 0.00001231
Iteration 29/1000 | Loss: 0.00001230
Iteration 30/1000 | Loss: 0.00001229
Iteration 31/1000 | Loss: 0.00001227
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001225
Iteration 34/1000 | Loss: 0.00001224
Iteration 35/1000 | Loss: 0.00001224
Iteration 36/1000 | Loss: 0.00001224
Iteration 37/1000 | Loss: 0.00001224
Iteration 38/1000 | Loss: 0.00001224
Iteration 39/1000 | Loss: 0.00001224
Iteration 40/1000 | Loss: 0.00001223
Iteration 41/1000 | Loss: 0.00001222
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001221
Iteration 44/1000 | Loss: 0.00001221
Iteration 45/1000 | Loss: 0.00001220
Iteration 46/1000 | Loss: 0.00001220
Iteration 47/1000 | Loss: 0.00001220
Iteration 48/1000 | Loss: 0.00001219
Iteration 49/1000 | Loss: 0.00001218
Iteration 50/1000 | Loss: 0.00001218
Iteration 51/1000 | Loss: 0.00001217
Iteration 52/1000 | Loss: 0.00001217
Iteration 53/1000 | Loss: 0.00001217
Iteration 54/1000 | Loss: 0.00001217
Iteration 55/1000 | Loss: 0.00001217
Iteration 56/1000 | Loss: 0.00001217
Iteration 57/1000 | Loss: 0.00001217
Iteration 58/1000 | Loss: 0.00001217
Iteration 59/1000 | Loss: 0.00001215
Iteration 60/1000 | Loss: 0.00001215
Iteration 61/1000 | Loss: 0.00001215
Iteration 62/1000 | Loss: 0.00001215
Iteration 63/1000 | Loss: 0.00001214
Iteration 64/1000 | Loss: 0.00001214
Iteration 65/1000 | Loss: 0.00001214
Iteration 66/1000 | Loss: 0.00001214
Iteration 67/1000 | Loss: 0.00001214
Iteration 68/1000 | Loss: 0.00001214
Iteration 69/1000 | Loss: 0.00001213
Iteration 70/1000 | Loss: 0.00001213
Iteration 71/1000 | Loss: 0.00001212
Iteration 72/1000 | Loss: 0.00001211
Iteration 73/1000 | Loss: 0.00001211
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001209
Iteration 85/1000 | Loss: 0.00001209
Iteration 86/1000 | Loss: 0.00001209
Iteration 87/1000 | Loss: 0.00001206
Iteration 88/1000 | Loss: 0.00001205
Iteration 89/1000 | Loss: 0.00001203
Iteration 90/1000 | Loss: 0.00001203
Iteration 91/1000 | Loss: 0.00001203
Iteration 92/1000 | Loss: 0.00001203
Iteration 93/1000 | Loss: 0.00001203
Iteration 94/1000 | Loss: 0.00001203
Iteration 95/1000 | Loss: 0.00001202
Iteration 96/1000 | Loss: 0.00001202
Iteration 97/1000 | Loss: 0.00001202
Iteration 98/1000 | Loss: 0.00001202
Iteration 99/1000 | Loss: 0.00001202
Iteration 100/1000 | Loss: 0.00001202
Iteration 101/1000 | Loss: 0.00001202
Iteration 102/1000 | Loss: 0.00001202
Iteration 103/1000 | Loss: 0.00001202
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001199
Iteration 111/1000 | Loss: 0.00001199
Iteration 112/1000 | Loss: 0.00001199
Iteration 113/1000 | Loss: 0.00001199
Iteration 114/1000 | Loss: 0.00001199
Iteration 115/1000 | Loss: 0.00001199
Iteration 116/1000 | Loss: 0.00001199
Iteration 117/1000 | Loss: 0.00001199
Iteration 118/1000 | Loss: 0.00001196
Iteration 119/1000 | Loss: 0.00001195
Iteration 120/1000 | Loss: 0.00001195
Iteration 121/1000 | Loss: 0.00001195
Iteration 122/1000 | Loss: 0.00001194
Iteration 123/1000 | Loss: 0.00001194
Iteration 124/1000 | Loss: 0.00001194
Iteration 125/1000 | Loss: 0.00001194
Iteration 126/1000 | Loss: 0.00001194
Iteration 127/1000 | Loss: 0.00001194
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001192
Iteration 132/1000 | Loss: 0.00001192
Iteration 133/1000 | Loss: 0.00001192
Iteration 134/1000 | Loss: 0.00001192
Iteration 135/1000 | Loss: 0.00001192
Iteration 136/1000 | Loss: 0.00001192
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001191
Iteration 140/1000 | Loss: 0.00001191
Iteration 141/1000 | Loss: 0.00001191
Iteration 142/1000 | Loss: 0.00001190
Iteration 143/1000 | Loss: 0.00001190
Iteration 144/1000 | Loss: 0.00001190
Iteration 145/1000 | Loss: 0.00001190
Iteration 146/1000 | Loss: 0.00001190
Iteration 147/1000 | Loss: 0.00001190
Iteration 148/1000 | Loss: 0.00001190
Iteration 149/1000 | Loss: 0.00001190
Iteration 150/1000 | Loss: 0.00001190
Iteration 151/1000 | Loss: 0.00001190
Iteration 152/1000 | Loss: 0.00001190
Iteration 153/1000 | Loss: 0.00001190
Iteration 154/1000 | Loss: 0.00001190
Iteration 155/1000 | Loss: 0.00001190
Iteration 156/1000 | Loss: 0.00001190
Iteration 157/1000 | Loss: 0.00001190
Iteration 158/1000 | Loss: 0.00001190
Iteration 159/1000 | Loss: 0.00001189
Iteration 160/1000 | Loss: 0.00001189
Iteration 161/1000 | Loss: 0.00001189
Iteration 162/1000 | Loss: 0.00001189
Iteration 163/1000 | Loss: 0.00001189
Iteration 164/1000 | Loss: 0.00001189
Iteration 165/1000 | Loss: 0.00001189
Iteration 166/1000 | Loss: 0.00001189
Iteration 167/1000 | Loss: 0.00001189
Iteration 168/1000 | Loss: 0.00001189
Iteration 169/1000 | Loss: 0.00001189
Iteration 170/1000 | Loss: 0.00001189
Iteration 171/1000 | Loss: 0.00001189
Iteration 172/1000 | Loss: 0.00001189
Iteration 173/1000 | Loss: 0.00001189
Iteration 174/1000 | Loss: 0.00001189
Iteration 175/1000 | Loss: 0.00001188
Iteration 176/1000 | Loss: 0.00001188
Iteration 177/1000 | Loss: 0.00001188
Iteration 178/1000 | Loss: 0.00001188
Iteration 179/1000 | Loss: 0.00001188
Iteration 180/1000 | Loss: 0.00001188
Iteration 181/1000 | Loss: 0.00001188
Iteration 182/1000 | Loss: 0.00001188
Iteration 183/1000 | Loss: 0.00001188
Iteration 184/1000 | Loss: 0.00001188
Iteration 185/1000 | Loss: 0.00001188
Iteration 186/1000 | Loss: 0.00001188
Iteration 187/1000 | Loss: 0.00001188
Iteration 188/1000 | Loss: 0.00001188
Iteration 189/1000 | Loss: 0.00001188
Iteration 190/1000 | Loss: 0.00001187
Iteration 191/1000 | Loss: 0.00001187
Iteration 192/1000 | Loss: 0.00001187
Iteration 193/1000 | Loss: 0.00001187
Iteration 194/1000 | Loss: 0.00001187
Iteration 195/1000 | Loss: 0.00001187
Iteration 196/1000 | Loss: 0.00001187
Iteration 197/1000 | Loss: 0.00001187
Iteration 198/1000 | Loss: 0.00001187
Iteration 199/1000 | Loss: 0.00001187
Iteration 200/1000 | Loss: 0.00001187
Iteration 201/1000 | Loss: 0.00001187
Iteration 202/1000 | Loss: 0.00001187
Iteration 203/1000 | Loss: 0.00001187
Iteration 204/1000 | Loss: 0.00001187
Iteration 205/1000 | Loss: 0.00001187
Iteration 206/1000 | Loss: 0.00001187
Iteration 207/1000 | Loss: 0.00001186
Iteration 208/1000 | Loss: 0.00001186
Iteration 209/1000 | Loss: 0.00001186
Iteration 210/1000 | Loss: 0.00001186
Iteration 211/1000 | Loss: 0.00001186
Iteration 212/1000 | Loss: 0.00001186
Iteration 213/1000 | Loss: 0.00001186
Iteration 214/1000 | Loss: 0.00001186
Iteration 215/1000 | Loss: 0.00001186
Iteration 216/1000 | Loss: 0.00001186
Iteration 217/1000 | Loss: 0.00001185
Iteration 218/1000 | Loss: 0.00001185
Iteration 219/1000 | Loss: 0.00001185
Iteration 220/1000 | Loss: 0.00001185
Iteration 221/1000 | Loss: 0.00001185
Iteration 222/1000 | Loss: 0.00001185
Iteration 223/1000 | Loss: 0.00001185
Iteration 224/1000 | Loss: 0.00001185
Iteration 225/1000 | Loss: 0.00001185
Iteration 226/1000 | Loss: 0.00001185
Iteration 227/1000 | Loss: 0.00001185
Iteration 228/1000 | Loss: 0.00001185
Iteration 229/1000 | Loss: 0.00001185
Iteration 230/1000 | Loss: 0.00001185
Iteration 231/1000 | Loss: 0.00001185
Iteration 232/1000 | Loss: 0.00001185
Iteration 233/1000 | Loss: 0.00001185
Iteration 234/1000 | Loss: 0.00001185
Iteration 235/1000 | Loss: 0.00001185
Iteration 236/1000 | Loss: 0.00001185
Iteration 237/1000 | Loss: 0.00001185
Iteration 238/1000 | Loss: 0.00001185
Iteration 239/1000 | Loss: 0.00001185
Iteration 240/1000 | Loss: 0.00001185
Iteration 241/1000 | Loss: 0.00001185
Iteration 242/1000 | Loss: 0.00001185
Iteration 243/1000 | Loss: 0.00001185
Iteration 244/1000 | Loss: 0.00001185
Iteration 245/1000 | Loss: 0.00001185
Iteration 246/1000 | Loss: 0.00001185
Iteration 247/1000 | Loss: 0.00001185
Iteration 248/1000 | Loss: 0.00001185
Iteration 249/1000 | Loss: 0.00001185
Iteration 250/1000 | Loss: 0.00001185
Iteration 251/1000 | Loss: 0.00001185
Iteration 252/1000 | Loss: 0.00001185
Iteration 253/1000 | Loss: 0.00001185
Iteration 254/1000 | Loss: 0.00001185
Iteration 255/1000 | Loss: 0.00001185
Iteration 256/1000 | Loss: 0.00001185
Iteration 257/1000 | Loss: 0.00001185
Iteration 258/1000 | Loss: 0.00001185
Iteration 259/1000 | Loss: 0.00001185
Iteration 260/1000 | Loss: 0.00001185
Iteration 261/1000 | Loss: 0.00001185
Iteration 262/1000 | Loss: 0.00001185
Iteration 263/1000 | Loss: 0.00001185
Iteration 264/1000 | Loss: 0.00001185
Iteration 265/1000 | Loss: 0.00001185
Iteration 266/1000 | Loss: 0.00001185
Iteration 267/1000 | Loss: 0.00001185
Iteration 268/1000 | Loss: 0.00001185
Iteration 269/1000 | Loss: 0.00001185
Iteration 270/1000 | Loss: 0.00001185
Iteration 271/1000 | Loss: 0.00001185
Iteration 272/1000 | Loss: 0.00001185
Iteration 273/1000 | Loss: 0.00001185
Iteration 274/1000 | Loss: 0.00001185
Iteration 275/1000 | Loss: 0.00001185
Iteration 276/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 276. Stopping optimization.
Last 5 losses: [1.1854233889607713e-05, 1.1854233889607713e-05, 1.1854233889607713e-05, 1.1854233889607713e-05, 1.1854233889607713e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1854233889607713e-05

Optimization complete. Final v2v error: 2.9491422176361084 mm

Highest mean error: 3.0591251850128174 mm for frame 24

Lowest mean error: 2.839292049407959 mm for frame 134

Saving results

Total time: 41.91871953010559
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01094948
Iteration 2/25 | Loss: 0.01094948
Iteration 3/25 | Loss: 0.01094948
Iteration 4/25 | Loss: 0.01094948
Iteration 5/25 | Loss: 0.01094948
Iteration 6/25 | Loss: 0.01094948
Iteration 7/25 | Loss: 0.01094948
Iteration 8/25 | Loss: 0.01094948
Iteration 9/25 | Loss: 0.01094947
Iteration 10/25 | Loss: 0.01094947
Iteration 11/25 | Loss: 0.01094947
Iteration 12/25 | Loss: 0.01094947
Iteration 13/25 | Loss: 0.01094947
Iteration 14/25 | Loss: 0.01094947
Iteration 15/25 | Loss: 0.01094947
Iteration 16/25 | Loss: 0.01094947
Iteration 17/25 | Loss: 0.01094947
Iteration 18/25 | Loss: 0.01094947
Iteration 19/25 | Loss: 0.01094947
Iteration 20/25 | Loss: 0.01094947
Iteration 21/25 | Loss: 0.01094947
Iteration 22/25 | Loss: 0.01094947
Iteration 23/25 | Loss: 0.01094947
Iteration 24/25 | Loss: 0.01094946
Iteration 25/25 | Loss: 0.01094946

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.35662079
Iteration 2/25 | Loss: 0.17558096
Iteration 3/25 | Loss: 0.17499980
Iteration 4/25 | Loss: 0.17463203
Iteration 5/25 | Loss: 0.17463201
Iteration 6/25 | Loss: 0.17463197
Iteration 7/25 | Loss: 0.17463197
Iteration 8/25 | Loss: 0.17463197
Iteration 9/25 | Loss: 0.17463195
Iteration 10/25 | Loss: 0.17463195
Iteration 11/25 | Loss: 0.17463195
Iteration 12/25 | Loss: 0.17463194
Iteration 13/25 | Loss: 0.17463194
Iteration 14/25 | Loss: 0.17463194
Iteration 15/25 | Loss: 0.17463194
Iteration 16/25 | Loss: 0.17463194
Iteration 17/25 | Loss: 0.17463194
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.17463193833827972, 0.17463193833827972, 0.17463193833827972, 0.17463193833827972, 0.17463193833827972]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.17463193833827972

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.17463194
Iteration 2/1000 | Loss: 0.00673153
Iteration 3/1000 | Loss: 0.00063715
Iteration 4/1000 | Loss: 0.00063448
Iteration 5/1000 | Loss: 0.00039547
Iteration 6/1000 | Loss: 0.00031892
Iteration 7/1000 | Loss: 0.00074200
Iteration 8/1000 | Loss: 0.00016363
Iteration 9/1000 | Loss: 0.00002686
Iteration 10/1000 | Loss: 0.00005625
Iteration 11/1000 | Loss: 0.00019164
Iteration 12/1000 | Loss: 0.00003676
Iteration 13/1000 | Loss: 0.00023776
Iteration 14/1000 | Loss: 0.00002734
Iteration 15/1000 | Loss: 0.00003038
Iteration 16/1000 | Loss: 0.00030865
Iteration 17/1000 | Loss: 0.00005746
Iteration 18/1000 | Loss: 0.00004253
Iteration 19/1000 | Loss: 0.00001873
Iteration 20/1000 | Loss: 0.00007540
Iteration 21/1000 | Loss: 0.00003665
Iteration 22/1000 | Loss: 0.00005588
Iteration 23/1000 | Loss: 0.00007939
Iteration 24/1000 | Loss: 0.00002559
Iteration 25/1000 | Loss: 0.00003618
Iteration 26/1000 | Loss: 0.00003035
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001539
Iteration 29/1000 | Loss: 0.00001489
Iteration 30/1000 | Loss: 0.00001454
Iteration 31/1000 | Loss: 0.00013591
Iteration 32/1000 | Loss: 0.00001440
Iteration 33/1000 | Loss: 0.00001389
Iteration 34/1000 | Loss: 0.00004628
Iteration 35/1000 | Loss: 0.00015063
Iteration 36/1000 | Loss: 0.00001942
Iteration 37/1000 | Loss: 0.00001352
Iteration 38/1000 | Loss: 0.00006068
Iteration 39/1000 | Loss: 0.00001335
Iteration 40/1000 | Loss: 0.00001303
Iteration 41/1000 | Loss: 0.00001295
Iteration 42/1000 | Loss: 0.00001285
Iteration 43/1000 | Loss: 0.00006310
Iteration 44/1000 | Loss: 0.00001280
Iteration 45/1000 | Loss: 0.00001272
Iteration 46/1000 | Loss: 0.00001271
Iteration 47/1000 | Loss: 0.00001270
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001267
Iteration 50/1000 | Loss: 0.00001266
Iteration 51/1000 | Loss: 0.00001265
Iteration 52/1000 | Loss: 0.00001265
Iteration 53/1000 | Loss: 0.00001264
Iteration 54/1000 | Loss: 0.00006599
Iteration 55/1000 | Loss: 0.00001264
Iteration 56/1000 | Loss: 0.00008957
Iteration 57/1000 | Loss: 0.00001505
Iteration 58/1000 | Loss: 0.00001310
Iteration 59/1000 | Loss: 0.00001254
Iteration 60/1000 | Loss: 0.00001248
Iteration 61/1000 | Loss: 0.00001248
Iteration 62/1000 | Loss: 0.00001247
Iteration 63/1000 | Loss: 0.00001247
Iteration 64/1000 | Loss: 0.00001247
Iteration 65/1000 | Loss: 0.00001247
Iteration 66/1000 | Loss: 0.00001246
Iteration 67/1000 | Loss: 0.00001244
Iteration 68/1000 | Loss: 0.00001244
Iteration 69/1000 | Loss: 0.00001244
Iteration 70/1000 | Loss: 0.00001244
Iteration 71/1000 | Loss: 0.00001244
Iteration 72/1000 | Loss: 0.00001243
Iteration 73/1000 | Loss: 0.00001243
Iteration 74/1000 | Loss: 0.00001243
Iteration 75/1000 | Loss: 0.00001243
Iteration 76/1000 | Loss: 0.00001243
Iteration 77/1000 | Loss: 0.00001243
Iteration 78/1000 | Loss: 0.00001243
Iteration 79/1000 | Loss: 0.00001243
Iteration 80/1000 | Loss: 0.00001242
Iteration 81/1000 | Loss: 0.00001242
Iteration 82/1000 | Loss: 0.00001242
Iteration 83/1000 | Loss: 0.00001241
Iteration 84/1000 | Loss: 0.00001241
Iteration 85/1000 | Loss: 0.00001240
Iteration 86/1000 | Loss: 0.00001240
Iteration 87/1000 | Loss: 0.00001239
Iteration 88/1000 | Loss: 0.00001239
Iteration 89/1000 | Loss: 0.00001239
Iteration 90/1000 | Loss: 0.00001239
Iteration 91/1000 | Loss: 0.00001239
Iteration 92/1000 | Loss: 0.00001238
Iteration 93/1000 | Loss: 0.00001238
Iteration 94/1000 | Loss: 0.00001237
Iteration 95/1000 | Loss: 0.00001236
Iteration 96/1000 | Loss: 0.00001236
Iteration 97/1000 | Loss: 0.00001236
Iteration 98/1000 | Loss: 0.00001236
Iteration 99/1000 | Loss: 0.00001236
Iteration 100/1000 | Loss: 0.00001235
Iteration 101/1000 | Loss: 0.00001235
Iteration 102/1000 | Loss: 0.00001235
Iteration 103/1000 | Loss: 0.00001235
Iteration 104/1000 | Loss: 0.00001235
Iteration 105/1000 | Loss: 0.00001234
Iteration 106/1000 | Loss: 0.00001234
Iteration 107/1000 | Loss: 0.00001233
Iteration 108/1000 | Loss: 0.00001231
Iteration 109/1000 | Loss: 0.00001231
Iteration 110/1000 | Loss: 0.00001231
Iteration 111/1000 | Loss: 0.00001231
Iteration 112/1000 | Loss: 0.00001229
Iteration 113/1000 | Loss: 0.00001229
Iteration 114/1000 | Loss: 0.00001229
Iteration 115/1000 | Loss: 0.00001228
Iteration 116/1000 | Loss: 0.00001228
Iteration 117/1000 | Loss: 0.00001228
Iteration 118/1000 | Loss: 0.00001228
Iteration 119/1000 | Loss: 0.00001227
Iteration 120/1000 | Loss: 0.00001227
Iteration 121/1000 | Loss: 0.00001227
Iteration 122/1000 | Loss: 0.00001227
Iteration 123/1000 | Loss: 0.00001227
Iteration 124/1000 | Loss: 0.00001227
Iteration 125/1000 | Loss: 0.00001227
Iteration 126/1000 | Loss: 0.00001227
Iteration 127/1000 | Loss: 0.00001227
Iteration 128/1000 | Loss: 0.00001227
Iteration 129/1000 | Loss: 0.00001226
Iteration 130/1000 | Loss: 0.00001226
Iteration 131/1000 | Loss: 0.00001226
Iteration 132/1000 | Loss: 0.00001226
Iteration 133/1000 | Loss: 0.00001226
Iteration 134/1000 | Loss: 0.00001226
Iteration 135/1000 | Loss: 0.00001226
Iteration 136/1000 | Loss: 0.00001226
Iteration 137/1000 | Loss: 0.00001225
Iteration 138/1000 | Loss: 0.00001225
Iteration 139/1000 | Loss: 0.00001225
Iteration 140/1000 | Loss: 0.00001224
Iteration 141/1000 | Loss: 0.00001224
Iteration 142/1000 | Loss: 0.00001224
Iteration 143/1000 | Loss: 0.00001224
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001223
Iteration 147/1000 | Loss: 0.00001223
Iteration 148/1000 | Loss: 0.00001223
Iteration 149/1000 | Loss: 0.00001223
Iteration 150/1000 | Loss: 0.00001222
Iteration 151/1000 | Loss: 0.00001222
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001222
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001222
Iteration 163/1000 | Loss: 0.00001222
Iteration 164/1000 | Loss: 0.00001222
Iteration 165/1000 | Loss: 0.00001222
Iteration 166/1000 | Loss: 0.00001222
Iteration 167/1000 | Loss: 0.00001222
Iteration 168/1000 | Loss: 0.00001222
Iteration 169/1000 | Loss: 0.00001222
Iteration 170/1000 | Loss: 0.00001222
Iteration 171/1000 | Loss: 0.00001222
Iteration 172/1000 | Loss: 0.00001222
Iteration 173/1000 | Loss: 0.00001222
Iteration 174/1000 | Loss: 0.00001222
Iteration 175/1000 | Loss: 0.00001222
Iteration 176/1000 | Loss: 0.00001222
Iteration 177/1000 | Loss: 0.00001222
Iteration 178/1000 | Loss: 0.00001222
Iteration 179/1000 | Loss: 0.00001222
Iteration 180/1000 | Loss: 0.00001222
Iteration 181/1000 | Loss: 0.00001222
Iteration 182/1000 | Loss: 0.00001222
Iteration 183/1000 | Loss: 0.00001222
Iteration 184/1000 | Loss: 0.00001222
Iteration 185/1000 | Loss: 0.00001222
Iteration 186/1000 | Loss: 0.00001222
Iteration 187/1000 | Loss: 0.00001222
Iteration 188/1000 | Loss: 0.00001222
Iteration 189/1000 | Loss: 0.00001222
Iteration 190/1000 | Loss: 0.00001222
Iteration 191/1000 | Loss: 0.00001222
Iteration 192/1000 | Loss: 0.00001222
Iteration 193/1000 | Loss: 0.00001222
Iteration 194/1000 | Loss: 0.00001222
Iteration 195/1000 | Loss: 0.00001222
Iteration 196/1000 | Loss: 0.00001222
Iteration 197/1000 | Loss: 0.00001222
Iteration 198/1000 | Loss: 0.00001222
Iteration 199/1000 | Loss: 0.00001222
Iteration 200/1000 | Loss: 0.00001222
Iteration 201/1000 | Loss: 0.00001222
Iteration 202/1000 | Loss: 0.00001222
Iteration 203/1000 | Loss: 0.00001222
Iteration 204/1000 | Loss: 0.00001222
Iteration 205/1000 | Loss: 0.00001222
Iteration 206/1000 | Loss: 0.00001222
Iteration 207/1000 | Loss: 0.00001222
Iteration 208/1000 | Loss: 0.00001222
Iteration 209/1000 | Loss: 0.00001222
Iteration 210/1000 | Loss: 0.00001222
Iteration 211/1000 | Loss: 0.00001222
Iteration 212/1000 | Loss: 0.00001222
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.2218325537105557e-05, 1.2218325537105557e-05, 1.2218325537105557e-05, 1.2218325537105557e-05, 1.2218325537105557e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2218325537105557e-05

Optimization complete. Final v2v error: 2.9640159606933594 mm

Highest mean error: 3.391026020050049 mm for frame 156

Lowest mean error: 2.7319254875183105 mm for frame 10

Saving results

Total time: 94.77662062644958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00698856
Iteration 2/25 | Loss: 0.00149390
Iteration 3/25 | Loss: 0.00121431
Iteration 4/25 | Loss: 0.00118090
Iteration 5/25 | Loss: 0.00117701
Iteration 6/25 | Loss: 0.00117643
Iteration 7/25 | Loss: 0.00117643
Iteration 8/25 | Loss: 0.00117643
Iteration 9/25 | Loss: 0.00117643
Iteration 10/25 | Loss: 0.00117643
Iteration 11/25 | Loss: 0.00117643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011764266528189182, 0.0011764266528189182, 0.0011764266528189182, 0.0011764266528189182, 0.0011764266528189182]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011764266528189182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37110329
Iteration 2/25 | Loss: 0.00060454
Iteration 3/25 | Loss: 0.00060451
Iteration 4/25 | Loss: 0.00060451
Iteration 5/25 | Loss: 0.00060451
Iteration 6/25 | Loss: 0.00060451
Iteration 7/25 | Loss: 0.00060451
Iteration 8/25 | Loss: 0.00060451
Iteration 9/25 | Loss: 0.00060451
Iteration 10/25 | Loss: 0.00060451
Iteration 11/25 | Loss: 0.00060451
Iteration 12/25 | Loss: 0.00060451
Iteration 13/25 | Loss: 0.00060451
Iteration 14/25 | Loss: 0.00060451
Iteration 15/25 | Loss: 0.00060451
Iteration 16/25 | Loss: 0.00060451
Iteration 17/25 | Loss: 0.00060451
Iteration 18/25 | Loss: 0.00060451
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006045108893886209, 0.0006045108893886209, 0.0006045108893886209, 0.0006045108893886209, 0.0006045108893886209]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006045108893886209

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060451
Iteration 2/1000 | Loss: 0.00002978
Iteration 3/1000 | Loss: 0.00002233
Iteration 4/1000 | Loss: 0.00001982
Iteration 5/1000 | Loss: 0.00001907
Iteration 6/1000 | Loss: 0.00001846
Iteration 7/1000 | Loss: 0.00001808
Iteration 8/1000 | Loss: 0.00001782
Iteration 9/1000 | Loss: 0.00001754
Iteration 10/1000 | Loss: 0.00001743
Iteration 11/1000 | Loss: 0.00001739
Iteration 12/1000 | Loss: 0.00001738
Iteration 13/1000 | Loss: 0.00001727
Iteration 14/1000 | Loss: 0.00001707
Iteration 15/1000 | Loss: 0.00001691
Iteration 16/1000 | Loss: 0.00001674
Iteration 17/1000 | Loss: 0.00001674
Iteration 18/1000 | Loss: 0.00001661
Iteration 19/1000 | Loss: 0.00001659
Iteration 20/1000 | Loss: 0.00001650
Iteration 21/1000 | Loss: 0.00001646
Iteration 22/1000 | Loss: 0.00001644
Iteration 23/1000 | Loss: 0.00001643
Iteration 24/1000 | Loss: 0.00001641
Iteration 25/1000 | Loss: 0.00001641
Iteration 26/1000 | Loss: 0.00001640
Iteration 27/1000 | Loss: 0.00001640
Iteration 28/1000 | Loss: 0.00001639
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001638
Iteration 31/1000 | Loss: 0.00001638
Iteration 32/1000 | Loss: 0.00001638
Iteration 33/1000 | Loss: 0.00001638
Iteration 34/1000 | Loss: 0.00001637
Iteration 35/1000 | Loss: 0.00001637
Iteration 36/1000 | Loss: 0.00001636
Iteration 37/1000 | Loss: 0.00001636
Iteration 38/1000 | Loss: 0.00001635
Iteration 39/1000 | Loss: 0.00001635
Iteration 40/1000 | Loss: 0.00001635
Iteration 41/1000 | Loss: 0.00001634
Iteration 42/1000 | Loss: 0.00001634
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001634
Iteration 45/1000 | Loss: 0.00001633
Iteration 46/1000 | Loss: 0.00001633
Iteration 47/1000 | Loss: 0.00001632
Iteration 48/1000 | Loss: 0.00001632
Iteration 49/1000 | Loss: 0.00001632
Iteration 50/1000 | Loss: 0.00001631
Iteration 51/1000 | Loss: 0.00001631
Iteration 52/1000 | Loss: 0.00001631
Iteration 53/1000 | Loss: 0.00001631
Iteration 54/1000 | Loss: 0.00001631
Iteration 55/1000 | Loss: 0.00001630
Iteration 56/1000 | Loss: 0.00001630
Iteration 57/1000 | Loss: 0.00001630
Iteration 58/1000 | Loss: 0.00001629
Iteration 59/1000 | Loss: 0.00001628
Iteration 60/1000 | Loss: 0.00001628
Iteration 61/1000 | Loss: 0.00001628
Iteration 62/1000 | Loss: 0.00001627
Iteration 63/1000 | Loss: 0.00001627
Iteration 64/1000 | Loss: 0.00001627
Iteration 65/1000 | Loss: 0.00001627
Iteration 66/1000 | Loss: 0.00001627
Iteration 67/1000 | Loss: 0.00001626
Iteration 68/1000 | Loss: 0.00001626
Iteration 69/1000 | Loss: 0.00001625
Iteration 70/1000 | Loss: 0.00001625
Iteration 71/1000 | Loss: 0.00001625
Iteration 72/1000 | Loss: 0.00001625
Iteration 73/1000 | Loss: 0.00001625
Iteration 74/1000 | Loss: 0.00001625
Iteration 75/1000 | Loss: 0.00001625
Iteration 76/1000 | Loss: 0.00001625
Iteration 77/1000 | Loss: 0.00001625
Iteration 78/1000 | Loss: 0.00001624
Iteration 79/1000 | Loss: 0.00001624
Iteration 80/1000 | Loss: 0.00001624
Iteration 81/1000 | Loss: 0.00001624
Iteration 82/1000 | Loss: 0.00001624
Iteration 83/1000 | Loss: 0.00001624
Iteration 84/1000 | Loss: 0.00001624
Iteration 85/1000 | Loss: 0.00001624
Iteration 86/1000 | Loss: 0.00001624
Iteration 87/1000 | Loss: 0.00001623
Iteration 88/1000 | Loss: 0.00001623
Iteration 89/1000 | Loss: 0.00001623
Iteration 90/1000 | Loss: 0.00001623
Iteration 91/1000 | Loss: 0.00001623
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001623
Iteration 99/1000 | Loss: 0.00001623
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001622
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001622
Iteration 107/1000 | Loss: 0.00001622
Iteration 108/1000 | Loss: 0.00001622
Iteration 109/1000 | Loss: 0.00001622
Iteration 110/1000 | Loss: 0.00001622
Iteration 111/1000 | Loss: 0.00001622
Iteration 112/1000 | Loss: 0.00001622
Iteration 113/1000 | Loss: 0.00001622
Iteration 114/1000 | Loss: 0.00001622
Iteration 115/1000 | Loss: 0.00001622
Iteration 116/1000 | Loss: 0.00001622
Iteration 117/1000 | Loss: 0.00001621
Iteration 118/1000 | Loss: 0.00001621
Iteration 119/1000 | Loss: 0.00001621
Iteration 120/1000 | Loss: 0.00001621
Iteration 121/1000 | Loss: 0.00001621
Iteration 122/1000 | Loss: 0.00001621
Iteration 123/1000 | Loss: 0.00001621
Iteration 124/1000 | Loss: 0.00001621
Iteration 125/1000 | Loss: 0.00001621
Iteration 126/1000 | Loss: 0.00001621
Iteration 127/1000 | Loss: 0.00001621
Iteration 128/1000 | Loss: 0.00001621
Iteration 129/1000 | Loss: 0.00001621
Iteration 130/1000 | Loss: 0.00001621
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.621421324671246e-05, 1.621421324671246e-05, 1.621421324671246e-05, 1.621421324671246e-05, 1.621421324671246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.621421324671246e-05

Optimization complete. Final v2v error: 3.4125561714172363 mm

Highest mean error: 3.604484796524048 mm for frame 91

Lowest mean error: 3.2306594848632812 mm for frame 194

Saving results

Total time: 43.20749282836914
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00568202
Iteration 2/25 | Loss: 0.00126902
Iteration 3/25 | Loss: 0.00116040
Iteration 4/25 | Loss: 0.00113883
Iteration 5/25 | Loss: 0.00113182
Iteration 6/25 | Loss: 0.00113027
Iteration 7/25 | Loss: 0.00113006
Iteration 8/25 | Loss: 0.00113006
Iteration 9/25 | Loss: 0.00113006
Iteration 10/25 | Loss: 0.00113006
Iteration 11/25 | Loss: 0.00113006
Iteration 12/25 | Loss: 0.00113006
Iteration 13/25 | Loss: 0.00113006
Iteration 14/25 | Loss: 0.00113006
Iteration 15/25 | Loss: 0.00113006
Iteration 16/25 | Loss: 0.00113006
Iteration 17/25 | Loss: 0.00113006
Iteration 18/25 | Loss: 0.00113006
Iteration 19/25 | Loss: 0.00113006
Iteration 20/25 | Loss: 0.00113006
Iteration 21/25 | Loss: 0.00113006
Iteration 22/25 | Loss: 0.00113006
Iteration 23/25 | Loss: 0.00113006
Iteration 24/25 | Loss: 0.00113006
Iteration 25/25 | Loss: 0.00113006

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.83125114
Iteration 2/25 | Loss: 0.00072654
Iteration 3/25 | Loss: 0.00072654
Iteration 4/25 | Loss: 0.00072654
Iteration 5/25 | Loss: 0.00072654
Iteration 6/25 | Loss: 0.00072654
Iteration 7/25 | Loss: 0.00072654
Iteration 8/25 | Loss: 0.00072654
Iteration 9/25 | Loss: 0.00072654
Iteration 10/25 | Loss: 0.00072654
Iteration 11/25 | Loss: 0.00072654
Iteration 12/25 | Loss: 0.00072654
Iteration 13/25 | Loss: 0.00072654
Iteration 14/25 | Loss: 0.00072654
Iteration 15/25 | Loss: 0.00072654
Iteration 16/25 | Loss: 0.00072654
Iteration 17/25 | Loss: 0.00072654
Iteration 18/25 | Loss: 0.00072654
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0007265403401106596, 0.0007265403401106596, 0.0007265403401106596, 0.0007265403401106596, 0.0007265403401106596]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007265403401106596

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00072654
Iteration 2/1000 | Loss: 0.00004024
Iteration 3/1000 | Loss: 0.00002597
Iteration 4/1000 | Loss: 0.00002147
Iteration 5/1000 | Loss: 0.00002050
Iteration 6/1000 | Loss: 0.00001956
Iteration 7/1000 | Loss: 0.00001914
Iteration 8/1000 | Loss: 0.00001873
Iteration 9/1000 | Loss: 0.00001847
Iteration 10/1000 | Loss: 0.00001846
Iteration 11/1000 | Loss: 0.00001833
Iteration 12/1000 | Loss: 0.00001814
Iteration 13/1000 | Loss: 0.00001811
Iteration 14/1000 | Loss: 0.00001807
Iteration 15/1000 | Loss: 0.00001800
Iteration 16/1000 | Loss: 0.00001799
Iteration 17/1000 | Loss: 0.00001797
Iteration 18/1000 | Loss: 0.00001792
Iteration 19/1000 | Loss: 0.00001788
Iteration 20/1000 | Loss: 0.00001787
Iteration 21/1000 | Loss: 0.00001786
Iteration 22/1000 | Loss: 0.00001786
Iteration 23/1000 | Loss: 0.00001785
Iteration 24/1000 | Loss: 0.00001785
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001782
Iteration 27/1000 | Loss: 0.00001782
Iteration 28/1000 | Loss: 0.00001781
Iteration 29/1000 | Loss: 0.00001781
Iteration 30/1000 | Loss: 0.00001779
Iteration 31/1000 | Loss: 0.00001777
Iteration 32/1000 | Loss: 0.00001776
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001776
Iteration 35/1000 | Loss: 0.00001775
Iteration 36/1000 | Loss: 0.00001775
Iteration 37/1000 | Loss: 0.00001775
Iteration 38/1000 | Loss: 0.00001774
Iteration 39/1000 | Loss: 0.00001774
Iteration 40/1000 | Loss: 0.00001774
Iteration 41/1000 | Loss: 0.00001774
Iteration 42/1000 | Loss: 0.00001773
Iteration 43/1000 | Loss: 0.00001773
Iteration 44/1000 | Loss: 0.00001772
Iteration 45/1000 | Loss: 0.00001771
Iteration 46/1000 | Loss: 0.00001771
Iteration 47/1000 | Loss: 0.00001771
Iteration 48/1000 | Loss: 0.00001770
Iteration 49/1000 | Loss: 0.00001770
Iteration 50/1000 | Loss: 0.00001770
Iteration 51/1000 | Loss: 0.00001769
Iteration 52/1000 | Loss: 0.00001769
Iteration 53/1000 | Loss: 0.00001768
Iteration 54/1000 | Loss: 0.00001768
Iteration 55/1000 | Loss: 0.00001768
Iteration 56/1000 | Loss: 0.00001768
Iteration 57/1000 | Loss: 0.00001767
Iteration 58/1000 | Loss: 0.00001767
Iteration 59/1000 | Loss: 0.00001767
Iteration 60/1000 | Loss: 0.00001766
Iteration 61/1000 | Loss: 0.00001766
Iteration 62/1000 | Loss: 0.00001766
Iteration 63/1000 | Loss: 0.00001766
Iteration 64/1000 | Loss: 0.00001765
Iteration 65/1000 | Loss: 0.00001765
Iteration 66/1000 | Loss: 0.00001765
Iteration 67/1000 | Loss: 0.00001765
Iteration 68/1000 | Loss: 0.00001764
Iteration 69/1000 | Loss: 0.00001764
Iteration 70/1000 | Loss: 0.00001764
Iteration 71/1000 | Loss: 0.00001764
Iteration 72/1000 | Loss: 0.00001763
Iteration 73/1000 | Loss: 0.00001763
Iteration 74/1000 | Loss: 0.00001763
Iteration 75/1000 | Loss: 0.00001763
Iteration 76/1000 | Loss: 0.00001762
Iteration 77/1000 | Loss: 0.00001762
Iteration 78/1000 | Loss: 0.00001762
Iteration 79/1000 | Loss: 0.00001762
Iteration 80/1000 | Loss: 0.00001761
Iteration 81/1000 | Loss: 0.00001761
Iteration 82/1000 | Loss: 0.00001761
Iteration 83/1000 | Loss: 0.00001760
Iteration 84/1000 | Loss: 0.00001760
Iteration 85/1000 | Loss: 0.00001760
Iteration 86/1000 | Loss: 0.00001759
Iteration 87/1000 | Loss: 0.00001759
Iteration 88/1000 | Loss: 0.00001759
Iteration 89/1000 | Loss: 0.00001758
Iteration 90/1000 | Loss: 0.00001758
Iteration 91/1000 | Loss: 0.00001758
Iteration 92/1000 | Loss: 0.00001757
Iteration 93/1000 | Loss: 0.00001757
Iteration 94/1000 | Loss: 0.00001757
Iteration 95/1000 | Loss: 0.00001757
Iteration 96/1000 | Loss: 0.00001757
Iteration 97/1000 | Loss: 0.00001756
Iteration 98/1000 | Loss: 0.00001756
Iteration 99/1000 | Loss: 0.00001756
Iteration 100/1000 | Loss: 0.00001755
Iteration 101/1000 | Loss: 0.00001755
Iteration 102/1000 | Loss: 0.00001755
Iteration 103/1000 | Loss: 0.00001754
Iteration 104/1000 | Loss: 0.00001754
Iteration 105/1000 | Loss: 0.00001753
Iteration 106/1000 | Loss: 0.00001753
Iteration 107/1000 | Loss: 0.00001753
Iteration 108/1000 | Loss: 0.00001753
Iteration 109/1000 | Loss: 0.00001752
Iteration 110/1000 | Loss: 0.00001752
Iteration 111/1000 | Loss: 0.00001752
Iteration 112/1000 | Loss: 0.00001752
Iteration 113/1000 | Loss: 0.00001751
Iteration 114/1000 | Loss: 0.00001751
Iteration 115/1000 | Loss: 0.00001751
Iteration 116/1000 | Loss: 0.00001751
Iteration 117/1000 | Loss: 0.00001750
Iteration 118/1000 | Loss: 0.00001750
Iteration 119/1000 | Loss: 0.00001750
Iteration 120/1000 | Loss: 0.00001750
Iteration 121/1000 | Loss: 0.00001750
Iteration 122/1000 | Loss: 0.00001750
Iteration 123/1000 | Loss: 0.00001749
Iteration 124/1000 | Loss: 0.00001749
Iteration 125/1000 | Loss: 0.00001749
Iteration 126/1000 | Loss: 0.00001749
Iteration 127/1000 | Loss: 0.00001749
Iteration 128/1000 | Loss: 0.00001749
Iteration 129/1000 | Loss: 0.00001749
Iteration 130/1000 | Loss: 0.00001749
Iteration 131/1000 | Loss: 0.00001748
Iteration 132/1000 | Loss: 0.00001748
Iteration 133/1000 | Loss: 0.00001748
Iteration 134/1000 | Loss: 0.00001748
Iteration 135/1000 | Loss: 0.00001748
Iteration 136/1000 | Loss: 0.00001748
Iteration 137/1000 | Loss: 0.00001748
Iteration 138/1000 | Loss: 0.00001748
Iteration 139/1000 | Loss: 0.00001747
Iteration 140/1000 | Loss: 0.00001747
Iteration 141/1000 | Loss: 0.00001747
Iteration 142/1000 | Loss: 0.00001747
Iteration 143/1000 | Loss: 0.00001747
Iteration 144/1000 | Loss: 0.00001747
Iteration 145/1000 | Loss: 0.00001747
Iteration 146/1000 | Loss: 0.00001746
Iteration 147/1000 | Loss: 0.00001746
Iteration 148/1000 | Loss: 0.00001746
Iteration 149/1000 | Loss: 0.00001746
Iteration 150/1000 | Loss: 0.00001746
Iteration 151/1000 | Loss: 0.00001745
Iteration 152/1000 | Loss: 0.00001745
Iteration 153/1000 | Loss: 0.00001745
Iteration 154/1000 | Loss: 0.00001745
Iteration 155/1000 | Loss: 0.00001745
Iteration 156/1000 | Loss: 0.00001745
Iteration 157/1000 | Loss: 0.00001744
Iteration 158/1000 | Loss: 0.00001744
Iteration 159/1000 | Loss: 0.00001744
Iteration 160/1000 | Loss: 0.00001744
Iteration 161/1000 | Loss: 0.00001744
Iteration 162/1000 | Loss: 0.00001743
Iteration 163/1000 | Loss: 0.00001743
Iteration 164/1000 | Loss: 0.00001743
Iteration 165/1000 | Loss: 0.00001743
Iteration 166/1000 | Loss: 0.00001742
Iteration 167/1000 | Loss: 0.00001742
Iteration 168/1000 | Loss: 0.00001742
Iteration 169/1000 | Loss: 0.00001742
Iteration 170/1000 | Loss: 0.00001742
Iteration 171/1000 | Loss: 0.00001742
Iteration 172/1000 | Loss: 0.00001742
Iteration 173/1000 | Loss: 0.00001742
Iteration 174/1000 | Loss: 0.00001742
Iteration 175/1000 | Loss: 0.00001742
Iteration 176/1000 | Loss: 0.00001742
Iteration 177/1000 | Loss: 0.00001742
Iteration 178/1000 | Loss: 0.00001741
Iteration 179/1000 | Loss: 0.00001741
Iteration 180/1000 | Loss: 0.00001741
Iteration 181/1000 | Loss: 0.00001741
Iteration 182/1000 | Loss: 0.00001741
Iteration 183/1000 | Loss: 0.00001741
Iteration 184/1000 | Loss: 0.00001741
Iteration 185/1000 | Loss: 0.00001741
Iteration 186/1000 | Loss: 0.00001741
Iteration 187/1000 | Loss: 0.00001741
Iteration 188/1000 | Loss: 0.00001741
Iteration 189/1000 | Loss: 0.00001741
Iteration 190/1000 | Loss: 0.00001741
Iteration 191/1000 | Loss: 0.00001741
Iteration 192/1000 | Loss: 0.00001740
Iteration 193/1000 | Loss: 0.00001740
Iteration 194/1000 | Loss: 0.00001740
Iteration 195/1000 | Loss: 0.00001740
Iteration 196/1000 | Loss: 0.00001740
Iteration 197/1000 | Loss: 0.00001740
Iteration 198/1000 | Loss: 0.00001740
Iteration 199/1000 | Loss: 0.00001740
Iteration 200/1000 | Loss: 0.00001740
Iteration 201/1000 | Loss: 0.00001740
Iteration 202/1000 | Loss: 0.00001740
Iteration 203/1000 | Loss: 0.00001740
Iteration 204/1000 | Loss: 0.00001740
Iteration 205/1000 | Loss: 0.00001740
Iteration 206/1000 | Loss: 0.00001740
Iteration 207/1000 | Loss: 0.00001740
Iteration 208/1000 | Loss: 0.00001740
Iteration 209/1000 | Loss: 0.00001740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.7397202100255527e-05, 1.7397202100255527e-05, 1.7397202100255527e-05, 1.7397202100255527e-05, 1.7397202100255527e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7397202100255527e-05

Optimization complete. Final v2v error: 3.4496965408325195 mm

Highest mean error: 4.966917514801025 mm for frame 97

Lowest mean error: 2.7742233276367188 mm for frame 43

Saving results

Total time: 41.00860786437988
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00473370
Iteration 2/25 | Loss: 0.00116603
Iteration 3/25 | Loss: 0.00107830
Iteration 4/25 | Loss: 0.00106648
Iteration 5/25 | Loss: 0.00106297
Iteration 6/25 | Loss: 0.00106219
Iteration 7/25 | Loss: 0.00106219
Iteration 8/25 | Loss: 0.00106219
Iteration 9/25 | Loss: 0.00106219
Iteration 10/25 | Loss: 0.00106219
Iteration 11/25 | Loss: 0.00106219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010621909750625491, 0.0010621909750625491, 0.0010621909750625491, 0.0010621909750625491, 0.0010621909750625491]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010621909750625491

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.81405902
Iteration 2/25 | Loss: 0.00059255
Iteration 3/25 | Loss: 0.00059253
Iteration 4/25 | Loss: 0.00059253
Iteration 5/25 | Loss: 0.00059253
Iteration 6/25 | Loss: 0.00059253
Iteration 7/25 | Loss: 0.00059253
Iteration 8/25 | Loss: 0.00059253
Iteration 9/25 | Loss: 0.00059253
Iteration 10/25 | Loss: 0.00059253
Iteration 11/25 | Loss: 0.00059253
Iteration 12/25 | Loss: 0.00059253
Iteration 13/25 | Loss: 0.00059253
Iteration 14/25 | Loss: 0.00059253
Iteration 15/25 | Loss: 0.00059253
Iteration 16/25 | Loss: 0.00059253
Iteration 17/25 | Loss: 0.00059253
Iteration 18/25 | Loss: 0.00059253
Iteration 19/25 | Loss: 0.00059253
Iteration 20/25 | Loss: 0.00059253
Iteration 21/25 | Loss: 0.00059253
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000592525175306946, 0.000592525175306946, 0.000592525175306946, 0.000592525175306946, 0.000592525175306946]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000592525175306946

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059253
Iteration 2/1000 | Loss: 0.00002091
Iteration 3/1000 | Loss: 0.00001506
Iteration 4/1000 | Loss: 0.00001345
Iteration 5/1000 | Loss: 0.00001243
Iteration 6/1000 | Loss: 0.00001184
Iteration 7/1000 | Loss: 0.00001148
Iteration 8/1000 | Loss: 0.00001139
Iteration 9/1000 | Loss: 0.00001136
Iteration 10/1000 | Loss: 0.00001099
Iteration 11/1000 | Loss: 0.00001092
Iteration 12/1000 | Loss: 0.00001077
Iteration 13/1000 | Loss: 0.00001076
Iteration 14/1000 | Loss: 0.00001066
Iteration 15/1000 | Loss: 0.00001054
Iteration 16/1000 | Loss: 0.00001051
Iteration 17/1000 | Loss: 0.00001050
Iteration 18/1000 | Loss: 0.00001050
Iteration 19/1000 | Loss: 0.00001049
Iteration 20/1000 | Loss: 0.00001049
Iteration 21/1000 | Loss: 0.00001047
Iteration 22/1000 | Loss: 0.00001047
Iteration 23/1000 | Loss: 0.00001046
Iteration 24/1000 | Loss: 0.00001045
Iteration 25/1000 | Loss: 0.00001045
Iteration 26/1000 | Loss: 0.00001045
Iteration 27/1000 | Loss: 0.00001045
Iteration 28/1000 | Loss: 0.00001043
Iteration 29/1000 | Loss: 0.00001043
Iteration 30/1000 | Loss: 0.00001042
Iteration 31/1000 | Loss: 0.00001042
Iteration 32/1000 | Loss: 0.00001042
Iteration 33/1000 | Loss: 0.00001041
Iteration 34/1000 | Loss: 0.00001041
Iteration 35/1000 | Loss: 0.00001040
Iteration 36/1000 | Loss: 0.00001039
Iteration 37/1000 | Loss: 0.00001039
Iteration 38/1000 | Loss: 0.00001039
Iteration 39/1000 | Loss: 0.00001039
Iteration 40/1000 | Loss: 0.00001038
Iteration 41/1000 | Loss: 0.00001038
Iteration 42/1000 | Loss: 0.00001037
Iteration 43/1000 | Loss: 0.00001037
Iteration 44/1000 | Loss: 0.00001036
Iteration 45/1000 | Loss: 0.00001032
Iteration 46/1000 | Loss: 0.00001031
Iteration 47/1000 | Loss: 0.00001029
Iteration 48/1000 | Loss: 0.00001028
Iteration 49/1000 | Loss: 0.00001028
Iteration 50/1000 | Loss: 0.00001027
Iteration 51/1000 | Loss: 0.00001027
Iteration 52/1000 | Loss: 0.00001026
Iteration 53/1000 | Loss: 0.00001025
Iteration 54/1000 | Loss: 0.00001025
Iteration 55/1000 | Loss: 0.00001023
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001023
Iteration 59/1000 | Loss: 0.00001023
Iteration 60/1000 | Loss: 0.00001023
Iteration 61/1000 | Loss: 0.00001023
Iteration 62/1000 | Loss: 0.00001023
Iteration 63/1000 | Loss: 0.00001022
Iteration 64/1000 | Loss: 0.00001022
Iteration 65/1000 | Loss: 0.00001022
Iteration 66/1000 | Loss: 0.00001022
Iteration 67/1000 | Loss: 0.00001021
Iteration 68/1000 | Loss: 0.00001021
Iteration 69/1000 | Loss: 0.00001021
Iteration 70/1000 | Loss: 0.00001020
Iteration 71/1000 | Loss: 0.00001020
Iteration 72/1000 | Loss: 0.00001020
Iteration 73/1000 | Loss: 0.00001019
Iteration 74/1000 | Loss: 0.00001019
Iteration 75/1000 | Loss: 0.00001019
Iteration 76/1000 | Loss: 0.00001018
Iteration 77/1000 | Loss: 0.00001018
Iteration 78/1000 | Loss: 0.00001017
Iteration 79/1000 | Loss: 0.00001017
Iteration 80/1000 | Loss: 0.00001016
Iteration 81/1000 | Loss: 0.00001016
Iteration 82/1000 | Loss: 0.00001016
Iteration 83/1000 | Loss: 0.00001015
Iteration 84/1000 | Loss: 0.00001015
Iteration 85/1000 | Loss: 0.00001015
Iteration 86/1000 | Loss: 0.00001015
Iteration 87/1000 | Loss: 0.00001015
Iteration 88/1000 | Loss: 0.00001015
Iteration 89/1000 | Loss: 0.00001015
Iteration 90/1000 | Loss: 0.00001014
Iteration 91/1000 | Loss: 0.00001014
Iteration 92/1000 | Loss: 0.00001013
Iteration 93/1000 | Loss: 0.00001013
Iteration 94/1000 | Loss: 0.00001013
Iteration 95/1000 | Loss: 0.00001013
Iteration 96/1000 | Loss: 0.00001012
Iteration 97/1000 | Loss: 0.00001012
Iteration 98/1000 | Loss: 0.00001012
Iteration 99/1000 | Loss: 0.00001012
Iteration 100/1000 | Loss: 0.00001012
Iteration 101/1000 | Loss: 0.00001012
Iteration 102/1000 | Loss: 0.00001012
Iteration 103/1000 | Loss: 0.00001011
Iteration 104/1000 | Loss: 0.00001011
Iteration 105/1000 | Loss: 0.00001011
Iteration 106/1000 | Loss: 0.00001011
Iteration 107/1000 | Loss: 0.00001011
Iteration 108/1000 | Loss: 0.00001010
Iteration 109/1000 | Loss: 0.00001010
Iteration 110/1000 | Loss: 0.00001009
Iteration 111/1000 | Loss: 0.00001009
Iteration 112/1000 | Loss: 0.00001009
Iteration 113/1000 | Loss: 0.00001009
Iteration 114/1000 | Loss: 0.00001008
Iteration 115/1000 | Loss: 0.00001008
Iteration 116/1000 | Loss: 0.00001008
Iteration 117/1000 | Loss: 0.00001008
Iteration 118/1000 | Loss: 0.00001008
Iteration 119/1000 | Loss: 0.00001007
Iteration 120/1000 | Loss: 0.00001007
Iteration 121/1000 | Loss: 0.00001006
Iteration 122/1000 | Loss: 0.00001006
Iteration 123/1000 | Loss: 0.00001006
Iteration 124/1000 | Loss: 0.00001005
Iteration 125/1000 | Loss: 0.00001005
Iteration 126/1000 | Loss: 0.00001005
Iteration 127/1000 | Loss: 0.00001005
Iteration 128/1000 | Loss: 0.00001005
Iteration 129/1000 | Loss: 0.00001005
Iteration 130/1000 | Loss: 0.00001005
Iteration 131/1000 | Loss: 0.00001005
Iteration 132/1000 | Loss: 0.00001005
Iteration 133/1000 | Loss: 0.00001005
Iteration 134/1000 | Loss: 0.00001004
Iteration 135/1000 | Loss: 0.00001003
Iteration 136/1000 | Loss: 0.00001003
Iteration 137/1000 | Loss: 0.00001003
Iteration 138/1000 | Loss: 0.00001003
Iteration 139/1000 | Loss: 0.00001003
Iteration 140/1000 | Loss: 0.00001003
Iteration 141/1000 | Loss: 0.00001003
Iteration 142/1000 | Loss: 0.00001003
Iteration 143/1000 | Loss: 0.00001003
Iteration 144/1000 | Loss: 0.00001003
Iteration 145/1000 | Loss: 0.00001002
Iteration 146/1000 | Loss: 0.00001002
Iteration 147/1000 | Loss: 0.00001002
Iteration 148/1000 | Loss: 0.00001002
Iteration 149/1000 | Loss: 0.00001002
Iteration 150/1000 | Loss: 0.00001002
Iteration 151/1000 | Loss: 0.00001002
Iteration 152/1000 | Loss: 0.00001002
Iteration 153/1000 | Loss: 0.00001002
Iteration 154/1000 | Loss: 0.00001002
Iteration 155/1000 | Loss: 0.00001002
Iteration 156/1000 | Loss: 0.00001002
Iteration 157/1000 | Loss: 0.00001002
Iteration 158/1000 | Loss: 0.00001001
Iteration 159/1000 | Loss: 0.00001001
Iteration 160/1000 | Loss: 0.00001001
Iteration 161/1000 | Loss: 0.00001001
Iteration 162/1000 | Loss: 0.00001001
Iteration 163/1000 | Loss: 0.00001001
Iteration 164/1000 | Loss: 0.00001001
Iteration 165/1000 | Loss: 0.00001001
Iteration 166/1000 | Loss: 0.00001001
Iteration 167/1000 | Loss: 0.00001001
Iteration 168/1000 | Loss: 0.00001000
Iteration 169/1000 | Loss: 0.00001000
Iteration 170/1000 | Loss: 0.00001000
Iteration 171/1000 | Loss: 0.00001000
Iteration 172/1000 | Loss: 0.00001000
Iteration 173/1000 | Loss: 0.00001000
Iteration 174/1000 | Loss: 0.00000999
Iteration 175/1000 | Loss: 0.00000999
Iteration 176/1000 | Loss: 0.00000999
Iteration 177/1000 | Loss: 0.00000999
Iteration 178/1000 | Loss: 0.00000999
Iteration 179/1000 | Loss: 0.00000999
Iteration 180/1000 | Loss: 0.00000999
Iteration 181/1000 | Loss: 0.00000999
Iteration 182/1000 | Loss: 0.00000998
Iteration 183/1000 | Loss: 0.00000998
Iteration 184/1000 | Loss: 0.00000998
Iteration 185/1000 | Loss: 0.00000998
Iteration 186/1000 | Loss: 0.00000997
Iteration 187/1000 | Loss: 0.00000997
Iteration 188/1000 | Loss: 0.00000997
Iteration 189/1000 | Loss: 0.00000997
Iteration 190/1000 | Loss: 0.00000997
Iteration 191/1000 | Loss: 0.00000997
Iteration 192/1000 | Loss: 0.00000997
Iteration 193/1000 | Loss: 0.00000997
Iteration 194/1000 | Loss: 0.00000997
Iteration 195/1000 | Loss: 0.00000996
Iteration 196/1000 | Loss: 0.00000996
Iteration 197/1000 | Loss: 0.00000996
Iteration 198/1000 | Loss: 0.00000996
Iteration 199/1000 | Loss: 0.00000996
Iteration 200/1000 | Loss: 0.00000996
Iteration 201/1000 | Loss: 0.00000996
Iteration 202/1000 | Loss: 0.00000996
Iteration 203/1000 | Loss: 0.00000996
Iteration 204/1000 | Loss: 0.00000996
Iteration 205/1000 | Loss: 0.00000996
Iteration 206/1000 | Loss: 0.00000995
Iteration 207/1000 | Loss: 0.00000995
Iteration 208/1000 | Loss: 0.00000995
Iteration 209/1000 | Loss: 0.00000995
Iteration 210/1000 | Loss: 0.00000995
Iteration 211/1000 | Loss: 0.00000995
Iteration 212/1000 | Loss: 0.00000995
Iteration 213/1000 | Loss: 0.00000995
Iteration 214/1000 | Loss: 0.00000995
Iteration 215/1000 | Loss: 0.00000995
Iteration 216/1000 | Loss: 0.00000995
Iteration 217/1000 | Loss: 0.00000995
Iteration 218/1000 | Loss: 0.00000995
Iteration 219/1000 | Loss: 0.00000995
Iteration 220/1000 | Loss: 0.00000995
Iteration 221/1000 | Loss: 0.00000995
Iteration 222/1000 | Loss: 0.00000995
Iteration 223/1000 | Loss: 0.00000995
Iteration 224/1000 | Loss: 0.00000995
Iteration 225/1000 | Loss: 0.00000995
Iteration 226/1000 | Loss: 0.00000995
Iteration 227/1000 | Loss: 0.00000995
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 227. Stopping optimization.
Last 5 losses: [9.95040772977518e-06, 9.95040772977518e-06, 9.95040772977518e-06, 9.95040772977518e-06, 9.95040772977518e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.95040772977518e-06

Optimization complete. Final v2v error: 2.7093539237976074 mm

Highest mean error: 3.4066877365112305 mm for frame 187

Lowest mean error: 2.4500784873962402 mm for frame 70

Saving results

Total time: 47.317596197128296
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00404851
Iteration 2/25 | Loss: 0.00115867
Iteration 3/25 | Loss: 0.00107765
Iteration 4/25 | Loss: 0.00106730
Iteration 5/25 | Loss: 0.00106429
Iteration 6/25 | Loss: 0.00106352
Iteration 7/25 | Loss: 0.00106352
Iteration 8/25 | Loss: 0.00106352
Iteration 9/25 | Loss: 0.00106352
Iteration 10/25 | Loss: 0.00106352
Iteration 11/25 | Loss: 0.00106352
Iteration 12/25 | Loss: 0.00106352
Iteration 13/25 | Loss: 0.00106352
Iteration 14/25 | Loss: 0.00106352
Iteration 15/25 | Loss: 0.00106352
Iteration 16/25 | Loss: 0.00106352
Iteration 17/25 | Loss: 0.00106352
Iteration 18/25 | Loss: 0.00106352
Iteration 19/25 | Loss: 0.00106352
Iteration 20/25 | Loss: 0.00106352
Iteration 21/25 | Loss: 0.00106352
Iteration 22/25 | Loss: 0.00106352
Iteration 23/25 | Loss: 0.00106352
Iteration 24/25 | Loss: 0.00106352
Iteration 25/25 | Loss: 0.00106352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48902154
Iteration 2/25 | Loss: 0.00061982
Iteration 3/25 | Loss: 0.00061982
Iteration 4/25 | Loss: 0.00061982
Iteration 5/25 | Loss: 0.00061982
Iteration 6/25 | Loss: 0.00061982
Iteration 7/25 | Loss: 0.00061982
Iteration 8/25 | Loss: 0.00061982
Iteration 9/25 | Loss: 0.00061982
Iteration 10/25 | Loss: 0.00061982
Iteration 11/25 | Loss: 0.00061982
Iteration 12/25 | Loss: 0.00061982
Iteration 13/25 | Loss: 0.00061982
Iteration 14/25 | Loss: 0.00061982
Iteration 15/25 | Loss: 0.00061982
Iteration 16/25 | Loss: 0.00061982
Iteration 17/25 | Loss: 0.00061982
Iteration 18/25 | Loss: 0.00061982
Iteration 19/25 | Loss: 0.00061982
Iteration 20/25 | Loss: 0.00061982
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006198200862854719, 0.0006198200862854719, 0.0006198200862854719, 0.0006198200862854719, 0.0006198200862854719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006198200862854719

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061982
Iteration 2/1000 | Loss: 0.00002560
Iteration 3/1000 | Loss: 0.00001482
Iteration 4/1000 | Loss: 0.00001153
Iteration 5/1000 | Loss: 0.00001052
Iteration 6/1000 | Loss: 0.00001011
Iteration 7/1000 | Loss: 0.00000980
Iteration 8/1000 | Loss: 0.00000970
Iteration 9/1000 | Loss: 0.00000970
Iteration 10/1000 | Loss: 0.00000959
Iteration 11/1000 | Loss: 0.00000951
Iteration 12/1000 | Loss: 0.00000937
Iteration 13/1000 | Loss: 0.00000934
Iteration 14/1000 | Loss: 0.00000931
Iteration 15/1000 | Loss: 0.00000931
Iteration 16/1000 | Loss: 0.00000930
Iteration 17/1000 | Loss: 0.00000929
Iteration 18/1000 | Loss: 0.00000928
Iteration 19/1000 | Loss: 0.00000927
Iteration 20/1000 | Loss: 0.00000927
Iteration 21/1000 | Loss: 0.00000926
Iteration 22/1000 | Loss: 0.00000926
Iteration 23/1000 | Loss: 0.00000922
Iteration 24/1000 | Loss: 0.00000919
Iteration 25/1000 | Loss: 0.00000918
Iteration 26/1000 | Loss: 0.00000915
Iteration 27/1000 | Loss: 0.00000914
Iteration 28/1000 | Loss: 0.00000913
Iteration 29/1000 | Loss: 0.00000913
Iteration 30/1000 | Loss: 0.00000912
Iteration 31/1000 | Loss: 0.00000912
Iteration 32/1000 | Loss: 0.00000911
Iteration 33/1000 | Loss: 0.00000911
Iteration 34/1000 | Loss: 0.00000911
Iteration 35/1000 | Loss: 0.00000910
Iteration 36/1000 | Loss: 0.00000910
Iteration 37/1000 | Loss: 0.00000907
Iteration 38/1000 | Loss: 0.00000907
Iteration 39/1000 | Loss: 0.00000907
Iteration 40/1000 | Loss: 0.00000907
Iteration 41/1000 | Loss: 0.00000907
Iteration 42/1000 | Loss: 0.00000907
Iteration 43/1000 | Loss: 0.00000907
Iteration 44/1000 | Loss: 0.00000907
Iteration 45/1000 | Loss: 0.00000906
Iteration 46/1000 | Loss: 0.00000906
Iteration 47/1000 | Loss: 0.00000906
Iteration 48/1000 | Loss: 0.00000906
Iteration 49/1000 | Loss: 0.00000906
Iteration 50/1000 | Loss: 0.00000905
Iteration 51/1000 | Loss: 0.00000905
Iteration 52/1000 | Loss: 0.00000905
Iteration 53/1000 | Loss: 0.00000903
Iteration 54/1000 | Loss: 0.00000903
Iteration 55/1000 | Loss: 0.00000903
Iteration 56/1000 | Loss: 0.00000903
Iteration 57/1000 | Loss: 0.00000903
Iteration 58/1000 | Loss: 0.00000903
Iteration 59/1000 | Loss: 0.00000903
Iteration 60/1000 | Loss: 0.00000903
Iteration 61/1000 | Loss: 0.00000903
Iteration 62/1000 | Loss: 0.00000903
Iteration 63/1000 | Loss: 0.00000902
Iteration 64/1000 | Loss: 0.00000902
Iteration 65/1000 | Loss: 0.00000901
Iteration 66/1000 | Loss: 0.00000901
Iteration 67/1000 | Loss: 0.00000900
Iteration 68/1000 | Loss: 0.00000900
Iteration 69/1000 | Loss: 0.00000900
Iteration 70/1000 | Loss: 0.00000899
Iteration 71/1000 | Loss: 0.00000899
Iteration 72/1000 | Loss: 0.00000899
Iteration 73/1000 | Loss: 0.00000899
Iteration 74/1000 | Loss: 0.00000899
Iteration 75/1000 | Loss: 0.00000899
Iteration 76/1000 | Loss: 0.00000899
Iteration 77/1000 | Loss: 0.00000899
Iteration 78/1000 | Loss: 0.00000899
Iteration 79/1000 | Loss: 0.00000899
Iteration 80/1000 | Loss: 0.00000898
Iteration 81/1000 | Loss: 0.00000898
Iteration 82/1000 | Loss: 0.00000898
Iteration 83/1000 | Loss: 0.00000898
Iteration 84/1000 | Loss: 0.00000898
Iteration 85/1000 | Loss: 0.00000898
Iteration 86/1000 | Loss: 0.00000898
Iteration 87/1000 | Loss: 0.00000898
Iteration 88/1000 | Loss: 0.00000898
Iteration 89/1000 | Loss: 0.00000898
Iteration 90/1000 | Loss: 0.00000897
Iteration 91/1000 | Loss: 0.00000897
Iteration 92/1000 | Loss: 0.00000897
Iteration 93/1000 | Loss: 0.00000897
Iteration 94/1000 | Loss: 0.00000897
Iteration 95/1000 | Loss: 0.00000897
Iteration 96/1000 | Loss: 0.00000897
Iteration 97/1000 | Loss: 0.00000897
Iteration 98/1000 | Loss: 0.00000897
Iteration 99/1000 | Loss: 0.00000897
Iteration 100/1000 | Loss: 0.00000896
Iteration 101/1000 | Loss: 0.00000896
Iteration 102/1000 | Loss: 0.00000896
Iteration 103/1000 | Loss: 0.00000896
Iteration 104/1000 | Loss: 0.00000896
Iteration 105/1000 | Loss: 0.00000896
Iteration 106/1000 | Loss: 0.00000896
Iteration 107/1000 | Loss: 0.00000895
Iteration 108/1000 | Loss: 0.00000895
Iteration 109/1000 | Loss: 0.00000895
Iteration 110/1000 | Loss: 0.00000895
Iteration 111/1000 | Loss: 0.00000895
Iteration 112/1000 | Loss: 0.00000895
Iteration 113/1000 | Loss: 0.00000895
Iteration 114/1000 | Loss: 0.00000895
Iteration 115/1000 | Loss: 0.00000895
Iteration 116/1000 | Loss: 0.00000895
Iteration 117/1000 | Loss: 0.00000895
Iteration 118/1000 | Loss: 0.00000895
Iteration 119/1000 | Loss: 0.00000895
Iteration 120/1000 | Loss: 0.00000895
Iteration 121/1000 | Loss: 0.00000894
Iteration 122/1000 | Loss: 0.00000894
Iteration 123/1000 | Loss: 0.00000894
Iteration 124/1000 | Loss: 0.00000894
Iteration 125/1000 | Loss: 0.00000894
Iteration 126/1000 | Loss: 0.00000894
Iteration 127/1000 | Loss: 0.00000894
Iteration 128/1000 | Loss: 0.00000894
Iteration 129/1000 | Loss: 0.00000894
Iteration 130/1000 | Loss: 0.00000893
Iteration 131/1000 | Loss: 0.00000893
Iteration 132/1000 | Loss: 0.00000893
Iteration 133/1000 | Loss: 0.00000893
Iteration 134/1000 | Loss: 0.00000893
Iteration 135/1000 | Loss: 0.00000893
Iteration 136/1000 | Loss: 0.00000893
Iteration 137/1000 | Loss: 0.00000893
Iteration 138/1000 | Loss: 0.00000893
Iteration 139/1000 | Loss: 0.00000893
Iteration 140/1000 | Loss: 0.00000893
Iteration 141/1000 | Loss: 0.00000893
Iteration 142/1000 | Loss: 0.00000893
Iteration 143/1000 | Loss: 0.00000893
Iteration 144/1000 | Loss: 0.00000893
Iteration 145/1000 | Loss: 0.00000893
Iteration 146/1000 | Loss: 0.00000893
Iteration 147/1000 | Loss: 0.00000893
Iteration 148/1000 | Loss: 0.00000893
Iteration 149/1000 | Loss: 0.00000893
Iteration 150/1000 | Loss: 0.00000893
Iteration 151/1000 | Loss: 0.00000893
Iteration 152/1000 | Loss: 0.00000893
Iteration 153/1000 | Loss: 0.00000893
Iteration 154/1000 | Loss: 0.00000893
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 154. Stopping optimization.
Last 5 losses: [8.926406735554338e-06, 8.926406735554338e-06, 8.926406735554338e-06, 8.926406735554338e-06, 8.926406735554338e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.926406735554338e-06

Optimization complete. Final v2v error: 2.578493118286133 mm

Highest mean error: 3.3726303577423096 mm for frame 59

Lowest mean error: 2.2938849925994873 mm for frame 84

Saving results

Total time: 34.04699349403381
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00371185
Iteration 2/25 | Loss: 0.00125405
Iteration 3/25 | Loss: 0.00110176
Iteration 4/25 | Loss: 0.00108384
Iteration 5/25 | Loss: 0.00107877
Iteration 6/25 | Loss: 0.00107804
Iteration 7/25 | Loss: 0.00107804
Iteration 8/25 | Loss: 0.00107804
Iteration 9/25 | Loss: 0.00107804
Iteration 10/25 | Loss: 0.00107804
Iteration 11/25 | Loss: 0.00107804
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010780434822663665, 0.0010780434822663665, 0.0010780434822663665, 0.0010780434822663665, 0.0010780434822663665]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010780434822663665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.82185757
Iteration 2/25 | Loss: 0.00065576
Iteration 3/25 | Loss: 0.00065576
Iteration 4/25 | Loss: 0.00065576
Iteration 5/25 | Loss: 0.00065575
Iteration 6/25 | Loss: 0.00065575
Iteration 7/25 | Loss: 0.00065575
Iteration 8/25 | Loss: 0.00065575
Iteration 9/25 | Loss: 0.00065575
Iteration 10/25 | Loss: 0.00065575
Iteration 11/25 | Loss: 0.00065575
Iteration 12/25 | Loss: 0.00065575
Iteration 13/25 | Loss: 0.00065575
Iteration 14/25 | Loss: 0.00065575
Iteration 15/25 | Loss: 0.00065575
Iteration 16/25 | Loss: 0.00065575
Iteration 17/25 | Loss: 0.00065575
Iteration 18/25 | Loss: 0.00065575
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0006557522574439645, 0.0006557522574439645, 0.0006557522574439645, 0.0006557522574439645, 0.0006557522574439645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006557522574439645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065575
Iteration 2/1000 | Loss: 0.00002416
Iteration 3/1000 | Loss: 0.00001628
Iteration 4/1000 | Loss: 0.00001475
Iteration 5/1000 | Loss: 0.00001366
Iteration 6/1000 | Loss: 0.00001299
Iteration 7/1000 | Loss: 0.00001251
Iteration 8/1000 | Loss: 0.00001222
Iteration 9/1000 | Loss: 0.00001190
Iteration 10/1000 | Loss: 0.00001159
Iteration 11/1000 | Loss: 0.00001150
Iteration 12/1000 | Loss: 0.00001145
Iteration 13/1000 | Loss: 0.00001135
Iteration 14/1000 | Loss: 0.00001127
Iteration 15/1000 | Loss: 0.00001115
Iteration 16/1000 | Loss: 0.00001109
Iteration 17/1000 | Loss: 0.00001104
Iteration 18/1000 | Loss: 0.00001103
Iteration 19/1000 | Loss: 0.00001103
Iteration 20/1000 | Loss: 0.00001103
Iteration 21/1000 | Loss: 0.00001102
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001101
Iteration 24/1000 | Loss: 0.00001101
Iteration 25/1000 | Loss: 0.00001100
Iteration 26/1000 | Loss: 0.00001100
Iteration 27/1000 | Loss: 0.00001100
Iteration 28/1000 | Loss: 0.00001100
Iteration 29/1000 | Loss: 0.00001100
Iteration 30/1000 | Loss: 0.00001099
Iteration 31/1000 | Loss: 0.00001099
Iteration 32/1000 | Loss: 0.00001098
Iteration 33/1000 | Loss: 0.00001098
Iteration 34/1000 | Loss: 0.00001098
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001093
Iteration 38/1000 | Loss: 0.00001093
Iteration 39/1000 | Loss: 0.00001092
Iteration 40/1000 | Loss: 0.00001092
Iteration 41/1000 | Loss: 0.00001092
Iteration 42/1000 | Loss: 0.00001091
Iteration 43/1000 | Loss: 0.00001090
Iteration 44/1000 | Loss: 0.00001089
Iteration 45/1000 | Loss: 0.00001089
Iteration 46/1000 | Loss: 0.00001088
Iteration 47/1000 | Loss: 0.00001088
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001087
Iteration 52/1000 | Loss: 0.00001086
Iteration 53/1000 | Loss: 0.00001086
Iteration 54/1000 | Loss: 0.00001086
Iteration 55/1000 | Loss: 0.00001086
Iteration 56/1000 | Loss: 0.00001086
Iteration 57/1000 | Loss: 0.00001085
Iteration 58/1000 | Loss: 0.00001085
Iteration 59/1000 | Loss: 0.00001085
Iteration 60/1000 | Loss: 0.00001085
Iteration 61/1000 | Loss: 0.00001085
Iteration 62/1000 | Loss: 0.00001085
Iteration 63/1000 | Loss: 0.00001085
Iteration 64/1000 | Loss: 0.00001085
Iteration 65/1000 | Loss: 0.00001084
Iteration 66/1000 | Loss: 0.00001084
Iteration 67/1000 | Loss: 0.00001084
Iteration 68/1000 | Loss: 0.00001083
Iteration 69/1000 | Loss: 0.00001083
Iteration 70/1000 | Loss: 0.00001083
Iteration 71/1000 | Loss: 0.00001082
Iteration 72/1000 | Loss: 0.00001082
Iteration 73/1000 | Loss: 0.00001082
Iteration 74/1000 | Loss: 0.00001082
Iteration 75/1000 | Loss: 0.00001081
Iteration 76/1000 | Loss: 0.00001081
Iteration 77/1000 | Loss: 0.00001081
Iteration 78/1000 | Loss: 0.00001081
Iteration 79/1000 | Loss: 0.00001081
Iteration 80/1000 | Loss: 0.00001080
Iteration 81/1000 | Loss: 0.00001080
Iteration 82/1000 | Loss: 0.00001080
Iteration 83/1000 | Loss: 0.00001080
Iteration 84/1000 | Loss: 0.00001079
Iteration 85/1000 | Loss: 0.00001079
Iteration 86/1000 | Loss: 0.00001079
Iteration 87/1000 | Loss: 0.00001078
Iteration 88/1000 | Loss: 0.00001078
Iteration 89/1000 | Loss: 0.00001078
Iteration 90/1000 | Loss: 0.00001078
Iteration 91/1000 | Loss: 0.00001078
Iteration 92/1000 | Loss: 0.00001078
Iteration 93/1000 | Loss: 0.00001078
Iteration 94/1000 | Loss: 0.00001078
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001077
Iteration 97/1000 | Loss: 0.00001077
Iteration 98/1000 | Loss: 0.00001077
Iteration 99/1000 | Loss: 0.00001077
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001076
Iteration 102/1000 | Loss: 0.00001076
Iteration 103/1000 | Loss: 0.00001075
Iteration 104/1000 | Loss: 0.00001075
Iteration 105/1000 | Loss: 0.00001075
Iteration 106/1000 | Loss: 0.00001075
Iteration 107/1000 | Loss: 0.00001075
Iteration 108/1000 | Loss: 0.00001075
Iteration 109/1000 | Loss: 0.00001075
Iteration 110/1000 | Loss: 0.00001075
Iteration 111/1000 | Loss: 0.00001075
Iteration 112/1000 | Loss: 0.00001075
Iteration 113/1000 | Loss: 0.00001074
Iteration 114/1000 | Loss: 0.00001074
Iteration 115/1000 | Loss: 0.00001074
Iteration 116/1000 | Loss: 0.00001074
Iteration 117/1000 | Loss: 0.00001074
Iteration 118/1000 | Loss: 0.00001074
Iteration 119/1000 | Loss: 0.00001074
Iteration 120/1000 | Loss: 0.00001074
Iteration 121/1000 | Loss: 0.00001073
Iteration 122/1000 | Loss: 0.00001073
Iteration 123/1000 | Loss: 0.00001073
Iteration 124/1000 | Loss: 0.00001073
Iteration 125/1000 | Loss: 0.00001073
Iteration 126/1000 | Loss: 0.00001073
Iteration 127/1000 | Loss: 0.00001073
Iteration 128/1000 | Loss: 0.00001073
Iteration 129/1000 | Loss: 0.00001073
Iteration 130/1000 | Loss: 0.00001073
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.0729172572609968e-05, 1.0729172572609968e-05, 1.0729172572609968e-05, 1.0729172572609968e-05, 1.0729172572609968e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0729172572609968e-05

Optimization complete. Final v2v error: 2.825076103210449 mm

Highest mean error: 3.3190743923187256 mm for frame 75

Lowest mean error: 2.3862662315368652 mm for frame 11

Saving results

Total time: 40.88854169845581
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053647
Iteration 2/25 | Loss: 0.00148579
Iteration 3/25 | Loss: 0.00112139
Iteration 4/25 | Loss: 0.00110339
Iteration 5/25 | Loss: 0.00109069
Iteration 6/25 | Loss: 0.00108574
Iteration 7/25 | Loss: 0.00108550
Iteration 8/25 | Loss: 0.00107604
Iteration 9/25 | Loss: 0.00107245
Iteration 10/25 | Loss: 0.00107135
Iteration 11/25 | Loss: 0.00107099
Iteration 12/25 | Loss: 0.00107089
Iteration 13/25 | Loss: 0.00107087
Iteration 14/25 | Loss: 0.00107087
Iteration 15/25 | Loss: 0.00107086
Iteration 16/25 | Loss: 0.00107086
Iteration 17/25 | Loss: 0.00107086
Iteration 18/25 | Loss: 0.00107086
Iteration 19/25 | Loss: 0.00107086
Iteration 20/25 | Loss: 0.00107086
Iteration 21/25 | Loss: 0.00107085
Iteration 22/25 | Loss: 0.00107085
Iteration 23/25 | Loss: 0.00107085
Iteration 24/25 | Loss: 0.00107085
Iteration 25/25 | Loss: 0.00107085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34529793
Iteration 2/25 | Loss: 0.00051825
Iteration 3/25 | Loss: 0.00051825
Iteration 4/25 | Loss: 0.00051825
Iteration 5/25 | Loss: 0.00051825
Iteration 6/25 | Loss: 0.00051825
Iteration 7/25 | Loss: 0.00051825
Iteration 8/25 | Loss: 0.00051825
Iteration 9/25 | Loss: 0.00051825
Iteration 10/25 | Loss: 0.00051825
Iteration 11/25 | Loss: 0.00051825
Iteration 12/25 | Loss: 0.00051825
Iteration 13/25 | Loss: 0.00051825
Iteration 14/25 | Loss: 0.00051825
Iteration 15/25 | Loss: 0.00051825
Iteration 16/25 | Loss: 0.00051825
Iteration 17/25 | Loss: 0.00051825
Iteration 18/25 | Loss: 0.00051825
Iteration 19/25 | Loss: 0.00051825
Iteration 20/25 | Loss: 0.00051825
Iteration 21/25 | Loss: 0.00051825
Iteration 22/25 | Loss: 0.00051825
Iteration 23/25 | Loss: 0.00051825
Iteration 24/25 | Loss: 0.00051825
Iteration 25/25 | Loss: 0.00051825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00051825
Iteration 2/1000 | Loss: 0.00008584
Iteration 3/1000 | Loss: 0.00003437
Iteration 4/1000 | Loss: 0.00003972
Iteration 5/1000 | Loss: 0.00002092
Iteration 6/1000 | Loss: 0.00001763
Iteration 7/1000 | Loss: 0.00001563
Iteration 8/1000 | Loss: 0.00010464
Iteration 9/1000 | Loss: 0.00001499
Iteration 10/1000 | Loss: 0.00001439
Iteration 11/1000 | Loss: 0.00001421
Iteration 12/1000 | Loss: 0.00009868
Iteration 13/1000 | Loss: 0.00001707
Iteration 14/1000 | Loss: 0.00001371
Iteration 15/1000 | Loss: 0.00001370
Iteration 16/1000 | Loss: 0.00001370
Iteration 17/1000 | Loss: 0.00001369
Iteration 18/1000 | Loss: 0.00001368
Iteration 19/1000 | Loss: 0.00001368
Iteration 20/1000 | Loss: 0.00004936
Iteration 21/1000 | Loss: 0.00065314
Iteration 22/1000 | Loss: 0.00004490
Iteration 23/1000 | Loss: 0.00001755
Iteration 24/1000 | Loss: 0.00001479
Iteration 25/1000 | Loss: 0.00001308
Iteration 26/1000 | Loss: 0.00004851
Iteration 27/1000 | Loss: 0.00001180
Iteration 28/1000 | Loss: 0.00001120
Iteration 29/1000 | Loss: 0.00001082
Iteration 30/1000 | Loss: 0.00005693
Iteration 31/1000 | Loss: 0.00009964
Iteration 32/1000 | Loss: 0.00005614
Iteration 33/1000 | Loss: 0.00003681
Iteration 34/1000 | Loss: 0.00001065
Iteration 35/1000 | Loss: 0.00001057
Iteration 36/1000 | Loss: 0.00001057
Iteration 37/1000 | Loss: 0.00002518
Iteration 38/1000 | Loss: 0.00001637
Iteration 39/1000 | Loss: 0.00002358
Iteration 40/1000 | Loss: 0.00001051
Iteration 41/1000 | Loss: 0.00001051
Iteration 42/1000 | Loss: 0.00001051
Iteration 43/1000 | Loss: 0.00001051
Iteration 44/1000 | Loss: 0.00001051
Iteration 45/1000 | Loss: 0.00001051
Iteration 46/1000 | Loss: 0.00001051
Iteration 47/1000 | Loss: 0.00001051
Iteration 48/1000 | Loss: 0.00001051
Iteration 49/1000 | Loss: 0.00001050
Iteration 50/1000 | Loss: 0.00001050
Iteration 51/1000 | Loss: 0.00001048
Iteration 52/1000 | Loss: 0.00001048
Iteration 53/1000 | Loss: 0.00001047
Iteration 54/1000 | Loss: 0.00001047
Iteration 55/1000 | Loss: 0.00001047
Iteration 56/1000 | Loss: 0.00001046
Iteration 57/1000 | Loss: 0.00001046
Iteration 58/1000 | Loss: 0.00001045
Iteration 59/1000 | Loss: 0.00001045
Iteration 60/1000 | Loss: 0.00001044
Iteration 61/1000 | Loss: 0.00001044
Iteration 62/1000 | Loss: 0.00001043
Iteration 63/1000 | Loss: 0.00001043
Iteration 64/1000 | Loss: 0.00001043
Iteration 65/1000 | Loss: 0.00001043
Iteration 66/1000 | Loss: 0.00001042
Iteration 67/1000 | Loss: 0.00001042
Iteration 68/1000 | Loss: 0.00001042
Iteration 69/1000 | Loss: 0.00001042
Iteration 70/1000 | Loss: 0.00001041
Iteration 71/1000 | Loss: 0.00001041
Iteration 72/1000 | Loss: 0.00001041
Iteration 73/1000 | Loss: 0.00001041
Iteration 74/1000 | Loss: 0.00001041
Iteration 75/1000 | Loss: 0.00001040
Iteration 76/1000 | Loss: 0.00001040
Iteration 77/1000 | Loss: 0.00001040
Iteration 78/1000 | Loss: 0.00001040
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001039
Iteration 81/1000 | Loss: 0.00001038
Iteration 82/1000 | Loss: 0.00001038
Iteration 83/1000 | Loss: 0.00001038
Iteration 84/1000 | Loss: 0.00001038
Iteration 85/1000 | Loss: 0.00001038
Iteration 86/1000 | Loss: 0.00001038
Iteration 87/1000 | Loss: 0.00001038
Iteration 88/1000 | Loss: 0.00001038
Iteration 89/1000 | Loss: 0.00001038
Iteration 90/1000 | Loss: 0.00001038
Iteration 91/1000 | Loss: 0.00001037
Iteration 92/1000 | Loss: 0.00001037
Iteration 93/1000 | Loss: 0.00001037
Iteration 94/1000 | Loss: 0.00001037
Iteration 95/1000 | Loss: 0.00001036
Iteration 96/1000 | Loss: 0.00001036
Iteration 97/1000 | Loss: 0.00001035
Iteration 98/1000 | Loss: 0.00001035
Iteration 99/1000 | Loss: 0.00001035
Iteration 100/1000 | Loss: 0.00001035
Iteration 101/1000 | Loss: 0.00001035
Iteration 102/1000 | Loss: 0.00001035
Iteration 103/1000 | Loss: 0.00001035
Iteration 104/1000 | Loss: 0.00001035
Iteration 105/1000 | Loss: 0.00001035
Iteration 106/1000 | Loss: 0.00001035
Iteration 107/1000 | Loss: 0.00001035
Iteration 108/1000 | Loss: 0.00001035
Iteration 109/1000 | Loss: 0.00001035
Iteration 110/1000 | Loss: 0.00001035
Iteration 111/1000 | Loss: 0.00001034
Iteration 112/1000 | Loss: 0.00001034
Iteration 113/1000 | Loss: 0.00001034
Iteration 114/1000 | Loss: 0.00001034
Iteration 115/1000 | Loss: 0.00001034
Iteration 116/1000 | Loss: 0.00001034
Iteration 117/1000 | Loss: 0.00001034
Iteration 118/1000 | Loss: 0.00001034
Iteration 119/1000 | Loss: 0.00001034
Iteration 120/1000 | Loss: 0.00001033
Iteration 121/1000 | Loss: 0.00001033
Iteration 122/1000 | Loss: 0.00001033
Iteration 123/1000 | Loss: 0.00001033
Iteration 124/1000 | Loss: 0.00001033
Iteration 125/1000 | Loss: 0.00001033
Iteration 126/1000 | Loss: 0.00001033
Iteration 127/1000 | Loss: 0.00001033
Iteration 128/1000 | Loss: 0.00001032
Iteration 129/1000 | Loss: 0.00001032
Iteration 130/1000 | Loss: 0.00001032
Iteration 131/1000 | Loss: 0.00001032
Iteration 132/1000 | Loss: 0.00001032
Iteration 133/1000 | Loss: 0.00001032
Iteration 134/1000 | Loss: 0.00001032
Iteration 135/1000 | Loss: 0.00001032
Iteration 136/1000 | Loss: 0.00001032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.032345971907489e-05, 1.032345971907489e-05, 1.032345971907489e-05, 1.032345971907489e-05, 1.032345971907489e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.032345971907489e-05

Optimization complete. Final v2v error: 2.7319767475128174 mm

Highest mean error: 3.6162588596343994 mm for frame 62

Lowest mean error: 2.6039555072784424 mm for frame 10

Saving results

Total time: 69.79507851600647
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00832884
Iteration 2/25 | Loss: 0.00135287
Iteration 3/25 | Loss: 0.00114936
Iteration 4/25 | Loss: 0.00111456
Iteration 5/25 | Loss: 0.00110766
Iteration 6/25 | Loss: 0.00110649
Iteration 7/25 | Loss: 0.00110649
Iteration 8/25 | Loss: 0.00110649
Iteration 9/25 | Loss: 0.00110649
Iteration 10/25 | Loss: 0.00110649
Iteration 11/25 | Loss: 0.00110649
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011064889840781689, 0.0011064889840781689, 0.0011064889840781689, 0.0011064889840781689, 0.0011064889840781689]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011064889840781689

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00329125
Iteration 2/25 | Loss: 0.00036765
Iteration 3/25 | Loss: 0.00036764
Iteration 4/25 | Loss: 0.00036764
Iteration 5/25 | Loss: 0.00036764
Iteration 6/25 | Loss: 0.00036764
Iteration 7/25 | Loss: 0.00036764
Iteration 8/25 | Loss: 0.00036764
Iteration 9/25 | Loss: 0.00036764
Iteration 10/25 | Loss: 0.00036764
Iteration 11/25 | Loss: 0.00036764
Iteration 12/25 | Loss: 0.00036764
Iteration 13/25 | Loss: 0.00036764
Iteration 14/25 | Loss: 0.00036764
Iteration 15/25 | Loss: 0.00036764
Iteration 16/25 | Loss: 0.00036764
Iteration 17/25 | Loss: 0.00036764
Iteration 18/25 | Loss: 0.00036764
Iteration 19/25 | Loss: 0.00036764
Iteration 20/25 | Loss: 0.00036764
Iteration 21/25 | Loss: 0.00036764
Iteration 22/25 | Loss: 0.00036764
Iteration 23/25 | Loss: 0.00036764
Iteration 24/25 | Loss: 0.00036764
Iteration 25/25 | Loss: 0.00036764

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00036764
Iteration 2/1000 | Loss: 0.00003825
Iteration 3/1000 | Loss: 0.00002896
Iteration 4/1000 | Loss: 0.00002627
Iteration 5/1000 | Loss: 0.00002494
Iteration 6/1000 | Loss: 0.00002368
Iteration 7/1000 | Loss: 0.00002302
Iteration 8/1000 | Loss: 0.00002245
Iteration 9/1000 | Loss: 0.00002210
Iteration 10/1000 | Loss: 0.00002187
Iteration 11/1000 | Loss: 0.00002163
Iteration 12/1000 | Loss: 0.00002162
Iteration 13/1000 | Loss: 0.00002146
Iteration 14/1000 | Loss: 0.00002135
Iteration 15/1000 | Loss: 0.00002131
Iteration 16/1000 | Loss: 0.00002129
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00002123
Iteration 19/1000 | Loss: 0.00002123
Iteration 20/1000 | Loss: 0.00002122
Iteration 21/1000 | Loss: 0.00002121
Iteration 22/1000 | Loss: 0.00002120
Iteration 23/1000 | Loss: 0.00002119
Iteration 24/1000 | Loss: 0.00002119
Iteration 25/1000 | Loss: 0.00002117
Iteration 26/1000 | Loss: 0.00002117
Iteration 27/1000 | Loss: 0.00002116
Iteration 28/1000 | Loss: 0.00002116
Iteration 29/1000 | Loss: 0.00002115
Iteration 30/1000 | Loss: 0.00002115
Iteration 31/1000 | Loss: 0.00002115
Iteration 32/1000 | Loss: 0.00002114
Iteration 33/1000 | Loss: 0.00002114
Iteration 34/1000 | Loss: 0.00002114
Iteration 35/1000 | Loss: 0.00002113
Iteration 36/1000 | Loss: 0.00002113
Iteration 37/1000 | Loss: 0.00002113
Iteration 38/1000 | Loss: 0.00002113
Iteration 39/1000 | Loss: 0.00002112
Iteration 40/1000 | Loss: 0.00002112
Iteration 41/1000 | Loss: 0.00002112
Iteration 42/1000 | Loss: 0.00002112
Iteration 43/1000 | Loss: 0.00002112
Iteration 44/1000 | Loss: 0.00002111
Iteration 45/1000 | Loss: 0.00002111
Iteration 46/1000 | Loss: 0.00002111
Iteration 47/1000 | Loss: 0.00002111
Iteration 48/1000 | Loss: 0.00002110
Iteration 49/1000 | Loss: 0.00002110
Iteration 50/1000 | Loss: 0.00002110
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002109
Iteration 54/1000 | Loss: 0.00002109
Iteration 55/1000 | Loss: 0.00002108
Iteration 56/1000 | Loss: 0.00002108
Iteration 57/1000 | Loss: 0.00002108
Iteration 58/1000 | Loss: 0.00002107
Iteration 59/1000 | Loss: 0.00002107
Iteration 60/1000 | Loss: 0.00002107
Iteration 61/1000 | Loss: 0.00002107
Iteration 62/1000 | Loss: 0.00002107
Iteration 63/1000 | Loss: 0.00002106
Iteration 64/1000 | Loss: 0.00002106
Iteration 65/1000 | Loss: 0.00002106
Iteration 66/1000 | Loss: 0.00002106
Iteration 67/1000 | Loss: 0.00002106
Iteration 68/1000 | Loss: 0.00002106
Iteration 69/1000 | Loss: 0.00002106
Iteration 70/1000 | Loss: 0.00002106
Iteration 71/1000 | Loss: 0.00002106
Iteration 72/1000 | Loss: 0.00002105
Iteration 73/1000 | Loss: 0.00002105
Iteration 74/1000 | Loss: 0.00002105
Iteration 75/1000 | Loss: 0.00002105
Iteration 76/1000 | Loss: 0.00002105
Iteration 77/1000 | Loss: 0.00002105
Iteration 78/1000 | Loss: 0.00002105
Iteration 79/1000 | Loss: 0.00002104
Iteration 80/1000 | Loss: 0.00002104
Iteration 81/1000 | Loss: 0.00002104
Iteration 82/1000 | Loss: 0.00002104
Iteration 83/1000 | Loss: 0.00002104
Iteration 84/1000 | Loss: 0.00002104
Iteration 85/1000 | Loss: 0.00002104
Iteration 86/1000 | Loss: 0.00002103
Iteration 87/1000 | Loss: 0.00002103
Iteration 88/1000 | Loss: 0.00002103
Iteration 89/1000 | Loss: 0.00002103
Iteration 90/1000 | Loss: 0.00002103
Iteration 91/1000 | Loss: 0.00002103
Iteration 92/1000 | Loss: 0.00002103
Iteration 93/1000 | Loss: 0.00002103
Iteration 94/1000 | Loss: 0.00002103
Iteration 95/1000 | Loss: 0.00002103
Iteration 96/1000 | Loss: 0.00002103
Iteration 97/1000 | Loss: 0.00002103
Iteration 98/1000 | Loss: 0.00002103
Iteration 99/1000 | Loss: 0.00002102
Iteration 100/1000 | Loss: 0.00002102
Iteration 101/1000 | Loss: 0.00002102
Iteration 102/1000 | Loss: 0.00002102
Iteration 103/1000 | Loss: 0.00002102
Iteration 104/1000 | Loss: 0.00002102
Iteration 105/1000 | Loss: 0.00002102
Iteration 106/1000 | Loss: 0.00002102
Iteration 107/1000 | Loss: 0.00002102
Iteration 108/1000 | Loss: 0.00002102
Iteration 109/1000 | Loss: 0.00002102
Iteration 110/1000 | Loss: 0.00002102
Iteration 111/1000 | Loss: 0.00002101
Iteration 112/1000 | Loss: 0.00002101
Iteration 113/1000 | Loss: 0.00002101
Iteration 114/1000 | Loss: 0.00002101
Iteration 115/1000 | Loss: 0.00002101
Iteration 116/1000 | Loss: 0.00002101
Iteration 117/1000 | Loss: 0.00002101
Iteration 118/1000 | Loss: 0.00002101
Iteration 119/1000 | Loss: 0.00002101
Iteration 120/1000 | Loss: 0.00002101
Iteration 121/1000 | Loss: 0.00002101
Iteration 122/1000 | Loss: 0.00002101
Iteration 123/1000 | Loss: 0.00002101
Iteration 124/1000 | Loss: 0.00002100
Iteration 125/1000 | Loss: 0.00002100
Iteration 126/1000 | Loss: 0.00002100
Iteration 127/1000 | Loss: 0.00002100
Iteration 128/1000 | Loss: 0.00002100
Iteration 129/1000 | Loss: 0.00002100
Iteration 130/1000 | Loss: 0.00002100
Iteration 131/1000 | Loss: 0.00002099
Iteration 132/1000 | Loss: 0.00002099
Iteration 133/1000 | Loss: 0.00002099
Iteration 134/1000 | Loss: 0.00002099
Iteration 135/1000 | Loss: 0.00002099
Iteration 136/1000 | Loss: 0.00002099
Iteration 137/1000 | Loss: 0.00002098
Iteration 138/1000 | Loss: 0.00002098
Iteration 139/1000 | Loss: 0.00002098
Iteration 140/1000 | Loss: 0.00002098
Iteration 141/1000 | Loss: 0.00002098
Iteration 142/1000 | Loss: 0.00002098
Iteration 143/1000 | Loss: 0.00002097
Iteration 144/1000 | Loss: 0.00002097
Iteration 145/1000 | Loss: 0.00002097
Iteration 146/1000 | Loss: 0.00002097
Iteration 147/1000 | Loss: 0.00002096
Iteration 148/1000 | Loss: 0.00002096
Iteration 149/1000 | Loss: 0.00002096
Iteration 150/1000 | Loss: 0.00002095
Iteration 151/1000 | Loss: 0.00002095
Iteration 152/1000 | Loss: 0.00002095
Iteration 153/1000 | Loss: 0.00002095
Iteration 154/1000 | Loss: 0.00002095
Iteration 155/1000 | Loss: 0.00002095
Iteration 156/1000 | Loss: 0.00002094
Iteration 157/1000 | Loss: 0.00002094
Iteration 158/1000 | Loss: 0.00002094
Iteration 159/1000 | Loss: 0.00002094
Iteration 160/1000 | Loss: 0.00002094
Iteration 161/1000 | Loss: 0.00002094
Iteration 162/1000 | Loss: 0.00002094
Iteration 163/1000 | Loss: 0.00002094
Iteration 164/1000 | Loss: 0.00002093
Iteration 165/1000 | Loss: 0.00002093
Iteration 166/1000 | Loss: 0.00002093
Iteration 167/1000 | Loss: 0.00002093
Iteration 168/1000 | Loss: 0.00002093
Iteration 169/1000 | Loss: 0.00002093
Iteration 170/1000 | Loss: 0.00002093
Iteration 171/1000 | Loss: 0.00002093
Iteration 172/1000 | Loss: 0.00002093
Iteration 173/1000 | Loss: 0.00002093
Iteration 174/1000 | Loss: 0.00002093
Iteration 175/1000 | Loss: 0.00002093
Iteration 176/1000 | Loss: 0.00002092
Iteration 177/1000 | Loss: 0.00002092
Iteration 178/1000 | Loss: 0.00002092
Iteration 179/1000 | Loss: 0.00002092
Iteration 180/1000 | Loss: 0.00002092
Iteration 181/1000 | Loss: 0.00002092
Iteration 182/1000 | Loss: 0.00002092
Iteration 183/1000 | Loss: 0.00002092
Iteration 184/1000 | Loss: 0.00002092
Iteration 185/1000 | Loss: 0.00002091
Iteration 186/1000 | Loss: 0.00002091
Iteration 187/1000 | Loss: 0.00002091
Iteration 188/1000 | Loss: 0.00002091
Iteration 189/1000 | Loss: 0.00002091
Iteration 190/1000 | Loss: 0.00002091
Iteration 191/1000 | Loss: 0.00002090
Iteration 192/1000 | Loss: 0.00002090
Iteration 193/1000 | Loss: 0.00002090
Iteration 194/1000 | Loss: 0.00002090
Iteration 195/1000 | Loss: 0.00002090
Iteration 196/1000 | Loss: 0.00002090
Iteration 197/1000 | Loss: 0.00002090
Iteration 198/1000 | Loss: 0.00002090
Iteration 199/1000 | Loss: 0.00002090
Iteration 200/1000 | Loss: 0.00002090
Iteration 201/1000 | Loss: 0.00002089
Iteration 202/1000 | Loss: 0.00002089
Iteration 203/1000 | Loss: 0.00002089
Iteration 204/1000 | Loss: 0.00002089
Iteration 205/1000 | Loss: 0.00002089
Iteration 206/1000 | Loss: 0.00002089
Iteration 207/1000 | Loss: 0.00002089
Iteration 208/1000 | Loss: 0.00002089
Iteration 209/1000 | Loss: 0.00002089
Iteration 210/1000 | Loss: 0.00002089
Iteration 211/1000 | Loss: 0.00002089
Iteration 212/1000 | Loss: 0.00002089
Iteration 213/1000 | Loss: 0.00002089
Iteration 214/1000 | Loss: 0.00002089
Iteration 215/1000 | Loss: 0.00002089
Iteration 216/1000 | Loss: 0.00002089
Iteration 217/1000 | Loss: 0.00002089
Iteration 218/1000 | Loss: 0.00002089
Iteration 219/1000 | Loss: 0.00002089
Iteration 220/1000 | Loss: 0.00002089
Iteration 221/1000 | Loss: 0.00002089
Iteration 222/1000 | Loss: 0.00002089
Iteration 223/1000 | Loss: 0.00002089
Iteration 224/1000 | Loss: 0.00002089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.0891176973236725e-05, 2.0891176973236725e-05, 2.0891176973236725e-05, 2.0891176973236725e-05, 2.0891176973236725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0891176973236725e-05

Optimization complete. Final v2v error: 3.883774757385254 mm

Highest mean error: 4.188704967498779 mm for frame 149

Lowest mean error: 3.7132022380828857 mm for frame 24

Saving results

Total time: 40.3234543800354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00382822
Iteration 2/25 | Loss: 0.00113730
Iteration 3/25 | Loss: 0.00106724
Iteration 4/25 | Loss: 0.00105649
Iteration 5/25 | Loss: 0.00105191
Iteration 6/25 | Loss: 0.00105072
Iteration 7/25 | Loss: 0.00105072
Iteration 8/25 | Loss: 0.00105072
Iteration 9/25 | Loss: 0.00105072
Iteration 10/25 | Loss: 0.00105072
Iteration 11/25 | Loss: 0.00105072
Iteration 12/25 | Loss: 0.00105072
Iteration 13/25 | Loss: 0.00105072
Iteration 14/25 | Loss: 0.00105072
Iteration 15/25 | Loss: 0.00105072
Iteration 16/25 | Loss: 0.00105072
Iteration 17/25 | Loss: 0.00105072
Iteration 18/25 | Loss: 0.00105072
Iteration 19/25 | Loss: 0.00105072
Iteration 20/25 | Loss: 0.00105072
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010507245315238833, 0.0010507245315238833, 0.0010507245315238833, 0.0010507245315238833, 0.0010507245315238833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010507245315238833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.94220114
Iteration 2/25 | Loss: 0.00062008
Iteration 3/25 | Loss: 0.00062008
Iteration 4/25 | Loss: 0.00062008
Iteration 5/25 | Loss: 0.00062008
Iteration 6/25 | Loss: 0.00062008
Iteration 7/25 | Loss: 0.00062008
Iteration 8/25 | Loss: 0.00062008
Iteration 9/25 | Loss: 0.00062008
Iteration 10/25 | Loss: 0.00062008
Iteration 11/25 | Loss: 0.00062008
Iteration 12/25 | Loss: 0.00062008
Iteration 13/25 | Loss: 0.00062008
Iteration 14/25 | Loss: 0.00062008
Iteration 15/25 | Loss: 0.00062008
Iteration 16/25 | Loss: 0.00062008
Iteration 17/25 | Loss: 0.00062008
Iteration 18/25 | Loss: 0.00062008
Iteration 19/25 | Loss: 0.00062008
Iteration 20/25 | Loss: 0.00062008
Iteration 21/25 | Loss: 0.00062008
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006200785865075886, 0.0006200785865075886, 0.0006200785865075886, 0.0006200785865075886, 0.0006200785865075886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006200785865075886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062008
Iteration 2/1000 | Loss: 0.00001643
Iteration 3/1000 | Loss: 0.00001215
Iteration 4/1000 | Loss: 0.00001120
Iteration 5/1000 | Loss: 0.00001076
Iteration 6/1000 | Loss: 0.00001042
Iteration 7/1000 | Loss: 0.00001021
Iteration 8/1000 | Loss: 0.00001014
Iteration 9/1000 | Loss: 0.00001003
Iteration 10/1000 | Loss: 0.00000982
Iteration 11/1000 | Loss: 0.00000973
Iteration 12/1000 | Loss: 0.00000970
Iteration 13/1000 | Loss: 0.00000968
Iteration 14/1000 | Loss: 0.00000963
Iteration 15/1000 | Loss: 0.00000963
Iteration 16/1000 | Loss: 0.00000962
Iteration 17/1000 | Loss: 0.00000962
Iteration 18/1000 | Loss: 0.00000956
Iteration 19/1000 | Loss: 0.00000956
Iteration 20/1000 | Loss: 0.00000956
Iteration 21/1000 | Loss: 0.00000956
Iteration 22/1000 | Loss: 0.00000956
Iteration 23/1000 | Loss: 0.00000955
Iteration 24/1000 | Loss: 0.00000955
Iteration 25/1000 | Loss: 0.00000950
Iteration 26/1000 | Loss: 0.00000950
Iteration 27/1000 | Loss: 0.00000950
Iteration 28/1000 | Loss: 0.00000950
Iteration 29/1000 | Loss: 0.00000949
Iteration 30/1000 | Loss: 0.00000949
Iteration 31/1000 | Loss: 0.00000946
Iteration 32/1000 | Loss: 0.00000946
Iteration 33/1000 | Loss: 0.00000946
Iteration 34/1000 | Loss: 0.00000946
Iteration 35/1000 | Loss: 0.00000946
Iteration 36/1000 | Loss: 0.00000946
Iteration 37/1000 | Loss: 0.00000945
Iteration 38/1000 | Loss: 0.00000945
Iteration 39/1000 | Loss: 0.00000944
Iteration 40/1000 | Loss: 0.00000943
Iteration 41/1000 | Loss: 0.00000943
Iteration 42/1000 | Loss: 0.00000942
Iteration 43/1000 | Loss: 0.00000942
Iteration 44/1000 | Loss: 0.00000942
Iteration 45/1000 | Loss: 0.00000942
Iteration 46/1000 | Loss: 0.00000942
Iteration 47/1000 | Loss: 0.00000942
Iteration 48/1000 | Loss: 0.00000941
Iteration 49/1000 | Loss: 0.00000941
Iteration 50/1000 | Loss: 0.00000941
Iteration 51/1000 | Loss: 0.00000940
Iteration 52/1000 | Loss: 0.00000940
Iteration 53/1000 | Loss: 0.00000940
Iteration 54/1000 | Loss: 0.00000940
Iteration 55/1000 | Loss: 0.00000940
Iteration 56/1000 | Loss: 0.00000939
Iteration 57/1000 | Loss: 0.00000939
Iteration 58/1000 | Loss: 0.00000939
Iteration 59/1000 | Loss: 0.00000939
Iteration 60/1000 | Loss: 0.00000939
Iteration 61/1000 | Loss: 0.00000939
Iteration 62/1000 | Loss: 0.00000939
Iteration 63/1000 | Loss: 0.00000938
Iteration 64/1000 | Loss: 0.00000938
Iteration 65/1000 | Loss: 0.00000938
Iteration 66/1000 | Loss: 0.00000938
Iteration 67/1000 | Loss: 0.00000938
Iteration 68/1000 | Loss: 0.00000938
Iteration 69/1000 | Loss: 0.00000938
Iteration 70/1000 | Loss: 0.00000937
Iteration 71/1000 | Loss: 0.00000937
Iteration 72/1000 | Loss: 0.00000937
Iteration 73/1000 | Loss: 0.00000936
Iteration 74/1000 | Loss: 0.00000936
Iteration 75/1000 | Loss: 0.00000935
Iteration 76/1000 | Loss: 0.00000935
Iteration 77/1000 | Loss: 0.00000935
Iteration 78/1000 | Loss: 0.00000934
Iteration 79/1000 | Loss: 0.00000934
Iteration 80/1000 | Loss: 0.00000933
Iteration 81/1000 | Loss: 0.00000933
Iteration 82/1000 | Loss: 0.00000932
Iteration 83/1000 | Loss: 0.00000932
Iteration 84/1000 | Loss: 0.00000931
Iteration 85/1000 | Loss: 0.00000930
Iteration 86/1000 | Loss: 0.00000930
Iteration 87/1000 | Loss: 0.00000929
Iteration 88/1000 | Loss: 0.00000929
Iteration 89/1000 | Loss: 0.00000929
Iteration 90/1000 | Loss: 0.00000929
Iteration 91/1000 | Loss: 0.00000928
Iteration 92/1000 | Loss: 0.00000928
Iteration 93/1000 | Loss: 0.00000928
Iteration 94/1000 | Loss: 0.00000927
Iteration 95/1000 | Loss: 0.00000927
Iteration 96/1000 | Loss: 0.00000927
Iteration 97/1000 | Loss: 0.00000926
Iteration 98/1000 | Loss: 0.00000926
Iteration 99/1000 | Loss: 0.00000925
Iteration 100/1000 | Loss: 0.00000925
Iteration 101/1000 | Loss: 0.00000925
Iteration 102/1000 | Loss: 0.00000925
Iteration 103/1000 | Loss: 0.00000925
Iteration 104/1000 | Loss: 0.00000925
Iteration 105/1000 | Loss: 0.00000925
Iteration 106/1000 | Loss: 0.00000925
Iteration 107/1000 | Loss: 0.00000925
Iteration 108/1000 | Loss: 0.00000925
Iteration 109/1000 | Loss: 0.00000924
Iteration 110/1000 | Loss: 0.00000924
Iteration 111/1000 | Loss: 0.00000924
Iteration 112/1000 | Loss: 0.00000924
Iteration 113/1000 | Loss: 0.00000923
Iteration 114/1000 | Loss: 0.00000923
Iteration 115/1000 | Loss: 0.00000923
Iteration 116/1000 | Loss: 0.00000923
Iteration 117/1000 | Loss: 0.00000923
Iteration 118/1000 | Loss: 0.00000923
Iteration 119/1000 | Loss: 0.00000923
Iteration 120/1000 | Loss: 0.00000923
Iteration 121/1000 | Loss: 0.00000922
Iteration 122/1000 | Loss: 0.00000922
Iteration 123/1000 | Loss: 0.00000922
Iteration 124/1000 | Loss: 0.00000922
Iteration 125/1000 | Loss: 0.00000922
Iteration 126/1000 | Loss: 0.00000922
Iteration 127/1000 | Loss: 0.00000922
Iteration 128/1000 | Loss: 0.00000922
Iteration 129/1000 | Loss: 0.00000922
Iteration 130/1000 | Loss: 0.00000922
Iteration 131/1000 | Loss: 0.00000922
Iteration 132/1000 | Loss: 0.00000922
Iteration 133/1000 | Loss: 0.00000922
Iteration 134/1000 | Loss: 0.00000922
Iteration 135/1000 | Loss: 0.00000922
Iteration 136/1000 | Loss: 0.00000922
Iteration 137/1000 | Loss: 0.00000922
Iteration 138/1000 | Loss: 0.00000922
Iteration 139/1000 | Loss: 0.00000922
Iteration 140/1000 | Loss: 0.00000922
Iteration 141/1000 | Loss: 0.00000922
Iteration 142/1000 | Loss: 0.00000922
Iteration 143/1000 | Loss: 0.00000922
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [9.218307241098955e-06, 9.218307241098955e-06, 9.218307241098955e-06, 9.218307241098955e-06, 9.218307241098955e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.218307241098955e-06

Optimization complete. Final v2v error: 2.611293315887451 mm

Highest mean error: 2.7907161712646484 mm for frame 75

Lowest mean error: 2.4962470531463623 mm for frame 35

Saving results

Total time: 34.99394488334656
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055023
Iteration 2/25 | Loss: 0.00222023
Iteration 3/25 | Loss: 0.00200308
Iteration 4/25 | Loss: 0.00195224
Iteration 5/25 | Loss: 0.00135110
Iteration 6/25 | Loss: 0.00129969
Iteration 7/25 | Loss: 0.00120780
Iteration 8/25 | Loss: 0.00111331
Iteration 9/25 | Loss: 0.00112551
Iteration 10/25 | Loss: 0.00108403
Iteration 11/25 | Loss: 0.00107519
Iteration 12/25 | Loss: 0.00107402
Iteration 13/25 | Loss: 0.00107389
Iteration 14/25 | Loss: 0.00107386
Iteration 15/25 | Loss: 0.00107386
Iteration 16/25 | Loss: 0.00107386
Iteration 17/25 | Loss: 0.00107385
Iteration 18/25 | Loss: 0.00107385
Iteration 19/25 | Loss: 0.00107385
Iteration 20/25 | Loss: 0.00107385
Iteration 21/25 | Loss: 0.00107385
Iteration 22/25 | Loss: 0.00107385
Iteration 23/25 | Loss: 0.00107385
Iteration 24/25 | Loss: 0.00107385
Iteration 25/25 | Loss: 0.00107385

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36607170
Iteration 2/25 | Loss: 0.00063636
Iteration 3/25 | Loss: 0.00063635
Iteration 4/25 | Loss: 0.00063635
Iteration 5/25 | Loss: 0.00063635
Iteration 6/25 | Loss: 0.00063635
Iteration 7/25 | Loss: 0.00063635
Iteration 8/25 | Loss: 0.00063635
Iteration 9/25 | Loss: 0.00063635
Iteration 10/25 | Loss: 0.00063635
Iteration 11/25 | Loss: 0.00063635
Iteration 12/25 | Loss: 0.00063635
Iteration 13/25 | Loss: 0.00063635
Iteration 14/25 | Loss: 0.00063635
Iteration 15/25 | Loss: 0.00063635
Iteration 16/25 | Loss: 0.00063635
Iteration 17/25 | Loss: 0.00063635
Iteration 18/25 | Loss: 0.00063635
Iteration 19/25 | Loss: 0.00063635
Iteration 20/25 | Loss: 0.00063635
Iteration 21/25 | Loss: 0.00063635
Iteration 22/25 | Loss: 0.00063635
Iteration 23/25 | Loss: 0.00063635
Iteration 24/25 | Loss: 0.00063635
Iteration 25/25 | Loss: 0.00063635

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063635
Iteration 2/1000 | Loss: 0.00002690
Iteration 3/1000 | Loss: 0.00002043
Iteration 4/1000 | Loss: 0.00001852
Iteration 5/1000 | Loss: 0.00001760
Iteration 6/1000 | Loss: 0.00001703
Iteration 7/1000 | Loss: 0.00001647
Iteration 8/1000 | Loss: 0.00001620
Iteration 9/1000 | Loss: 0.00001583
Iteration 10/1000 | Loss: 0.00054395
Iteration 11/1000 | Loss: 0.00001999
Iteration 12/1000 | Loss: 0.00001614
Iteration 13/1000 | Loss: 0.00001458
Iteration 14/1000 | Loss: 0.00001399
Iteration 15/1000 | Loss: 0.00001367
Iteration 16/1000 | Loss: 0.00001335
Iteration 17/1000 | Loss: 0.00001309
Iteration 18/1000 | Loss: 0.00001305
Iteration 19/1000 | Loss: 0.00001300
Iteration 20/1000 | Loss: 0.00001300
Iteration 21/1000 | Loss: 0.00001300
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001299
Iteration 24/1000 | Loss: 0.00001299
Iteration 25/1000 | Loss: 0.00001299
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001299
Iteration 28/1000 | Loss: 0.00001299
Iteration 29/1000 | Loss: 0.00001298
Iteration 30/1000 | Loss: 0.00001296
Iteration 31/1000 | Loss: 0.00001295
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001288
Iteration 34/1000 | Loss: 0.00001288
Iteration 35/1000 | Loss: 0.00001288
Iteration 36/1000 | Loss: 0.00001287
Iteration 37/1000 | Loss: 0.00001281
Iteration 38/1000 | Loss: 0.00001278
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001272
Iteration 41/1000 | Loss: 0.00001271
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001265
Iteration 44/1000 | Loss: 0.00001265
Iteration 45/1000 | Loss: 0.00001265
Iteration 46/1000 | Loss: 0.00001264
Iteration 47/1000 | Loss: 0.00001264
Iteration 48/1000 | Loss: 0.00001263
Iteration 49/1000 | Loss: 0.00001263
Iteration 50/1000 | Loss: 0.00001263
Iteration 51/1000 | Loss: 0.00001263
Iteration 52/1000 | Loss: 0.00001263
Iteration 53/1000 | Loss: 0.00001263
Iteration 54/1000 | Loss: 0.00001263
Iteration 55/1000 | Loss: 0.00001263
Iteration 56/1000 | Loss: 0.00001263
Iteration 57/1000 | Loss: 0.00001263
Iteration 58/1000 | Loss: 0.00001263
Iteration 59/1000 | Loss: 0.00001263
Iteration 60/1000 | Loss: 0.00001263
Iteration 61/1000 | Loss: 0.00001263
Iteration 62/1000 | Loss: 0.00001263
Iteration 63/1000 | Loss: 0.00001263
Iteration 64/1000 | Loss: 0.00001263
Iteration 65/1000 | Loss: 0.00001263
Iteration 66/1000 | Loss: 0.00001263
Iteration 67/1000 | Loss: 0.00001263
Iteration 68/1000 | Loss: 0.00001263
Iteration 69/1000 | Loss: 0.00001263
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.2628548574866727e-05, 1.2628548574866727e-05, 1.2628548574866727e-05, 1.2628548574866727e-05, 1.2628548574866727e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2628548574866727e-05

Optimization complete. Final v2v error: 3.018993377685547 mm

Highest mean error: 4.684293746948242 mm for frame 50

Lowest mean error: 2.7689895629882812 mm for frame 85

Saving results

Total time: 53.61251735687256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00500010
Iteration 2/25 | Loss: 0.00125807
Iteration 3/25 | Loss: 0.00114536
Iteration 4/25 | Loss: 0.00113624
Iteration 5/25 | Loss: 0.00113432
Iteration 6/25 | Loss: 0.00113432
Iteration 7/25 | Loss: 0.00113432
Iteration 8/25 | Loss: 0.00113432
Iteration 9/25 | Loss: 0.00113432
Iteration 10/25 | Loss: 0.00113432
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011343180667608976, 0.0011343180667608976, 0.0011343180667608976, 0.0011343180667608976, 0.0011343180667608976]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011343180667608976

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85682815
Iteration 2/25 | Loss: 0.00052298
Iteration 3/25 | Loss: 0.00052298
Iteration 4/25 | Loss: 0.00052298
Iteration 5/25 | Loss: 0.00052298
Iteration 6/25 | Loss: 0.00052297
Iteration 7/25 | Loss: 0.00052297
Iteration 8/25 | Loss: 0.00052297
Iteration 9/25 | Loss: 0.00052297
Iteration 10/25 | Loss: 0.00052297
Iteration 11/25 | Loss: 0.00052297
Iteration 12/25 | Loss: 0.00052297
Iteration 13/25 | Loss: 0.00052297
Iteration 14/25 | Loss: 0.00052297
Iteration 15/25 | Loss: 0.00052297
Iteration 16/25 | Loss: 0.00052297
Iteration 17/25 | Loss: 0.00052297
Iteration 18/25 | Loss: 0.00052297
Iteration 19/25 | Loss: 0.00052297
Iteration 20/25 | Loss: 0.00052297
Iteration 21/25 | Loss: 0.00052297
Iteration 22/25 | Loss: 0.00052297
Iteration 23/25 | Loss: 0.00052297
Iteration 24/25 | Loss: 0.00052297
Iteration 25/25 | Loss: 0.00052297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0005229723174124956, 0.0005229723174124956, 0.0005229723174124956, 0.0005229723174124956, 0.0005229723174124956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005229723174124956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052297
Iteration 2/1000 | Loss: 0.00002575
Iteration 3/1000 | Loss: 0.00001881
Iteration 4/1000 | Loss: 0.00001707
Iteration 5/1000 | Loss: 0.00001620
Iteration 6/1000 | Loss: 0.00001577
Iteration 7/1000 | Loss: 0.00001548
Iteration 8/1000 | Loss: 0.00001526
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001478
Iteration 11/1000 | Loss: 0.00001465
Iteration 12/1000 | Loss: 0.00001447
Iteration 13/1000 | Loss: 0.00001436
Iteration 14/1000 | Loss: 0.00001434
Iteration 15/1000 | Loss: 0.00001433
Iteration 16/1000 | Loss: 0.00001433
Iteration 17/1000 | Loss: 0.00001433
Iteration 18/1000 | Loss: 0.00001433
Iteration 19/1000 | Loss: 0.00001424
Iteration 20/1000 | Loss: 0.00001424
Iteration 21/1000 | Loss: 0.00001421
Iteration 22/1000 | Loss: 0.00001421
Iteration 23/1000 | Loss: 0.00001420
Iteration 24/1000 | Loss: 0.00001420
Iteration 25/1000 | Loss: 0.00001419
Iteration 26/1000 | Loss: 0.00001419
Iteration 27/1000 | Loss: 0.00001414
Iteration 28/1000 | Loss: 0.00001414
Iteration 29/1000 | Loss: 0.00001414
Iteration 30/1000 | Loss: 0.00001414
Iteration 31/1000 | Loss: 0.00001414
Iteration 32/1000 | Loss: 0.00001414
Iteration 33/1000 | Loss: 0.00001413
Iteration 34/1000 | Loss: 0.00001413
Iteration 35/1000 | Loss: 0.00001413
Iteration 36/1000 | Loss: 0.00001413
Iteration 37/1000 | Loss: 0.00001411
Iteration 38/1000 | Loss: 0.00001411
Iteration 39/1000 | Loss: 0.00001410
Iteration 40/1000 | Loss: 0.00001410
Iteration 41/1000 | Loss: 0.00001406
Iteration 42/1000 | Loss: 0.00001403
Iteration 43/1000 | Loss: 0.00001403
Iteration 44/1000 | Loss: 0.00001401
Iteration 45/1000 | Loss: 0.00001401
Iteration 46/1000 | Loss: 0.00001401
Iteration 47/1000 | Loss: 0.00001401
Iteration 48/1000 | Loss: 0.00001401
Iteration 49/1000 | Loss: 0.00001400
Iteration 50/1000 | Loss: 0.00001400
Iteration 51/1000 | Loss: 0.00001399
Iteration 52/1000 | Loss: 0.00001398
Iteration 53/1000 | Loss: 0.00001395
Iteration 54/1000 | Loss: 0.00001395
Iteration 55/1000 | Loss: 0.00001395
Iteration 56/1000 | Loss: 0.00001391
Iteration 57/1000 | Loss: 0.00001391
Iteration 58/1000 | Loss: 0.00001390
Iteration 59/1000 | Loss: 0.00001390
Iteration 60/1000 | Loss: 0.00001390
Iteration 61/1000 | Loss: 0.00001390
Iteration 62/1000 | Loss: 0.00001389
Iteration 63/1000 | Loss: 0.00001388
Iteration 64/1000 | Loss: 0.00001388
Iteration 65/1000 | Loss: 0.00001388
Iteration 66/1000 | Loss: 0.00001388
Iteration 67/1000 | Loss: 0.00001388
Iteration 68/1000 | Loss: 0.00001388
Iteration 69/1000 | Loss: 0.00001388
Iteration 70/1000 | Loss: 0.00001388
Iteration 71/1000 | Loss: 0.00001388
Iteration 72/1000 | Loss: 0.00001388
Iteration 73/1000 | Loss: 0.00001387
Iteration 74/1000 | Loss: 0.00001387
Iteration 75/1000 | Loss: 0.00001387
Iteration 76/1000 | Loss: 0.00001387
Iteration 77/1000 | Loss: 0.00001387
Iteration 78/1000 | Loss: 0.00001387
Iteration 79/1000 | Loss: 0.00001387
Iteration 80/1000 | Loss: 0.00001387
Iteration 81/1000 | Loss: 0.00001387
Iteration 82/1000 | Loss: 0.00001387
Iteration 83/1000 | Loss: 0.00001387
Iteration 84/1000 | Loss: 0.00001387
Iteration 85/1000 | Loss: 0.00001387
Iteration 86/1000 | Loss: 0.00001387
Iteration 87/1000 | Loss: 0.00001386
Iteration 88/1000 | Loss: 0.00001386
Iteration 89/1000 | Loss: 0.00001386
Iteration 90/1000 | Loss: 0.00001386
Iteration 91/1000 | Loss: 0.00001386
Iteration 92/1000 | Loss: 0.00001386
Iteration 93/1000 | Loss: 0.00001386
Iteration 94/1000 | Loss: 0.00001385
Iteration 95/1000 | Loss: 0.00001385
Iteration 96/1000 | Loss: 0.00001385
Iteration 97/1000 | Loss: 0.00001385
Iteration 98/1000 | Loss: 0.00001385
Iteration 99/1000 | Loss: 0.00001385
Iteration 100/1000 | Loss: 0.00001385
Iteration 101/1000 | Loss: 0.00001385
Iteration 102/1000 | Loss: 0.00001385
Iteration 103/1000 | Loss: 0.00001385
Iteration 104/1000 | Loss: 0.00001385
Iteration 105/1000 | Loss: 0.00001385
Iteration 106/1000 | Loss: 0.00001385
Iteration 107/1000 | Loss: 0.00001385
Iteration 108/1000 | Loss: 0.00001384
Iteration 109/1000 | Loss: 0.00001384
Iteration 110/1000 | Loss: 0.00001384
Iteration 111/1000 | Loss: 0.00001384
Iteration 112/1000 | Loss: 0.00001384
Iteration 113/1000 | Loss: 0.00001384
Iteration 114/1000 | Loss: 0.00001384
Iteration 115/1000 | Loss: 0.00001384
Iteration 116/1000 | Loss: 0.00001383
Iteration 117/1000 | Loss: 0.00001383
Iteration 118/1000 | Loss: 0.00001383
Iteration 119/1000 | Loss: 0.00001383
Iteration 120/1000 | Loss: 0.00001383
Iteration 121/1000 | Loss: 0.00001383
Iteration 122/1000 | Loss: 0.00001383
Iteration 123/1000 | Loss: 0.00001383
Iteration 124/1000 | Loss: 0.00001382
Iteration 125/1000 | Loss: 0.00001382
Iteration 126/1000 | Loss: 0.00001382
Iteration 127/1000 | Loss: 0.00001382
Iteration 128/1000 | Loss: 0.00001382
Iteration 129/1000 | Loss: 0.00001382
Iteration 130/1000 | Loss: 0.00001382
Iteration 131/1000 | Loss: 0.00001382
Iteration 132/1000 | Loss: 0.00001382
Iteration 133/1000 | Loss: 0.00001382
Iteration 134/1000 | Loss: 0.00001382
Iteration 135/1000 | Loss: 0.00001382
Iteration 136/1000 | Loss: 0.00001382
Iteration 137/1000 | Loss: 0.00001381
Iteration 138/1000 | Loss: 0.00001381
Iteration 139/1000 | Loss: 0.00001381
Iteration 140/1000 | Loss: 0.00001381
Iteration 141/1000 | Loss: 0.00001381
Iteration 142/1000 | Loss: 0.00001381
Iteration 143/1000 | Loss: 0.00001381
Iteration 144/1000 | Loss: 0.00001381
Iteration 145/1000 | Loss: 0.00001381
Iteration 146/1000 | Loss: 0.00001381
Iteration 147/1000 | Loss: 0.00001381
Iteration 148/1000 | Loss: 0.00001381
Iteration 149/1000 | Loss: 0.00001381
Iteration 150/1000 | Loss: 0.00001381
Iteration 151/1000 | Loss: 0.00001381
Iteration 152/1000 | Loss: 0.00001381
Iteration 153/1000 | Loss: 0.00001381
Iteration 154/1000 | Loss: 0.00001381
Iteration 155/1000 | Loss: 0.00001381
Iteration 156/1000 | Loss: 0.00001380
Iteration 157/1000 | Loss: 0.00001380
Iteration 158/1000 | Loss: 0.00001380
Iteration 159/1000 | Loss: 0.00001380
Iteration 160/1000 | Loss: 0.00001380
Iteration 161/1000 | Loss: 0.00001380
Iteration 162/1000 | Loss: 0.00001380
Iteration 163/1000 | Loss: 0.00001380
Iteration 164/1000 | Loss: 0.00001380
Iteration 165/1000 | Loss: 0.00001380
Iteration 166/1000 | Loss: 0.00001380
Iteration 167/1000 | Loss: 0.00001380
Iteration 168/1000 | Loss: 0.00001380
Iteration 169/1000 | Loss: 0.00001380
Iteration 170/1000 | Loss: 0.00001380
Iteration 171/1000 | Loss: 0.00001380
Iteration 172/1000 | Loss: 0.00001380
Iteration 173/1000 | Loss: 0.00001380
Iteration 174/1000 | Loss: 0.00001380
Iteration 175/1000 | Loss: 0.00001380
Iteration 176/1000 | Loss: 0.00001380
Iteration 177/1000 | Loss: 0.00001380
Iteration 178/1000 | Loss: 0.00001380
Iteration 179/1000 | Loss: 0.00001380
Iteration 180/1000 | Loss: 0.00001380
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.3802010471408721e-05, 1.3802010471408721e-05, 1.3802010471408721e-05, 1.3802010471408721e-05, 1.3802010471408721e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3802010471408721e-05

Optimization complete. Final v2v error: 3.12052059173584 mm

Highest mean error: 3.1494150161743164 mm for frame 154

Lowest mean error: 3.0761818885803223 mm for frame 60

Saving results

Total time: 37.62075901031494
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00790023
Iteration 2/25 | Loss: 0.00152548
Iteration 3/25 | Loss: 0.00123643
Iteration 4/25 | Loss: 0.00120549
Iteration 5/25 | Loss: 0.00119783
Iteration 6/25 | Loss: 0.00119594
Iteration 7/25 | Loss: 0.00119594
Iteration 8/25 | Loss: 0.00119594
Iteration 9/25 | Loss: 0.00119594
Iteration 10/25 | Loss: 0.00119594
Iteration 11/25 | Loss: 0.00119594
Iteration 12/25 | Loss: 0.00119594
Iteration 13/25 | Loss: 0.00119594
Iteration 14/25 | Loss: 0.00119594
Iteration 15/25 | Loss: 0.00119594
Iteration 16/25 | Loss: 0.00119594
Iteration 17/25 | Loss: 0.00119594
Iteration 18/25 | Loss: 0.00119594
Iteration 19/25 | Loss: 0.00119594
Iteration 20/25 | Loss: 0.00119594
Iteration 21/25 | Loss: 0.00119594
Iteration 22/25 | Loss: 0.00119594
Iteration 23/25 | Loss: 0.00119594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011959356488659978, 0.0011959356488659978, 0.0011959356488659978, 0.0011959356488659978, 0.0011959356488659978]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011959356488659978

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41049373
Iteration 2/25 | Loss: 0.00073501
Iteration 3/25 | Loss: 0.00073500
Iteration 4/25 | Loss: 0.00073500
Iteration 5/25 | Loss: 0.00073500
Iteration 6/25 | Loss: 0.00073500
Iteration 7/25 | Loss: 0.00073500
Iteration 8/25 | Loss: 0.00073500
Iteration 9/25 | Loss: 0.00073500
Iteration 10/25 | Loss: 0.00073500
Iteration 11/25 | Loss: 0.00073500
Iteration 12/25 | Loss: 0.00073500
Iteration 13/25 | Loss: 0.00073500
Iteration 14/25 | Loss: 0.00073500
Iteration 15/25 | Loss: 0.00073500
Iteration 16/25 | Loss: 0.00073500
Iteration 17/25 | Loss: 0.00073500
Iteration 18/25 | Loss: 0.00073500
Iteration 19/25 | Loss: 0.00073500
Iteration 20/25 | Loss: 0.00073500
Iteration 21/25 | Loss: 0.00073500
Iteration 22/25 | Loss: 0.00073500
Iteration 23/25 | Loss: 0.00073500
Iteration 24/25 | Loss: 0.00073500
Iteration 25/25 | Loss: 0.00073500

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00073500
Iteration 2/1000 | Loss: 0.00004870
Iteration 3/1000 | Loss: 0.00003304
Iteration 4/1000 | Loss: 0.00002912
Iteration 5/1000 | Loss: 0.00002706
Iteration 6/1000 | Loss: 0.00002552
Iteration 7/1000 | Loss: 0.00002480
Iteration 8/1000 | Loss: 0.00002415
Iteration 9/1000 | Loss: 0.00002378
Iteration 10/1000 | Loss: 0.00002343
Iteration 11/1000 | Loss: 0.00002307
Iteration 12/1000 | Loss: 0.00002286
Iteration 13/1000 | Loss: 0.00002267
Iteration 14/1000 | Loss: 0.00002258
Iteration 15/1000 | Loss: 0.00002253
Iteration 16/1000 | Loss: 0.00002243
Iteration 17/1000 | Loss: 0.00002238
Iteration 18/1000 | Loss: 0.00002236
Iteration 19/1000 | Loss: 0.00002236
Iteration 20/1000 | Loss: 0.00002235
Iteration 21/1000 | Loss: 0.00002235
Iteration 22/1000 | Loss: 0.00002234
Iteration 23/1000 | Loss: 0.00002233
Iteration 24/1000 | Loss: 0.00002232
Iteration 25/1000 | Loss: 0.00002231
Iteration 26/1000 | Loss: 0.00002230
Iteration 27/1000 | Loss: 0.00002229
Iteration 28/1000 | Loss: 0.00002228
Iteration 29/1000 | Loss: 0.00002228
Iteration 30/1000 | Loss: 0.00002226
Iteration 31/1000 | Loss: 0.00002226
Iteration 32/1000 | Loss: 0.00002225
Iteration 33/1000 | Loss: 0.00002224
Iteration 34/1000 | Loss: 0.00002224
Iteration 35/1000 | Loss: 0.00002223
Iteration 36/1000 | Loss: 0.00002223
Iteration 37/1000 | Loss: 0.00002223
Iteration 38/1000 | Loss: 0.00002223
Iteration 39/1000 | Loss: 0.00002222
Iteration 40/1000 | Loss: 0.00002222
Iteration 41/1000 | Loss: 0.00002222
Iteration 42/1000 | Loss: 0.00002222
Iteration 43/1000 | Loss: 0.00002222
Iteration 44/1000 | Loss: 0.00002222
Iteration 45/1000 | Loss: 0.00002222
Iteration 46/1000 | Loss: 0.00002222
Iteration 47/1000 | Loss: 0.00002222
Iteration 48/1000 | Loss: 0.00002222
Iteration 49/1000 | Loss: 0.00002222
Iteration 50/1000 | Loss: 0.00002221
Iteration 51/1000 | Loss: 0.00002221
Iteration 52/1000 | Loss: 0.00002220
Iteration 53/1000 | Loss: 0.00002220
Iteration 54/1000 | Loss: 0.00002220
Iteration 55/1000 | Loss: 0.00002219
Iteration 56/1000 | Loss: 0.00002219
Iteration 57/1000 | Loss: 0.00002219
Iteration 58/1000 | Loss: 0.00002219
Iteration 59/1000 | Loss: 0.00002218
Iteration 60/1000 | Loss: 0.00002218
Iteration 61/1000 | Loss: 0.00002218
Iteration 62/1000 | Loss: 0.00002217
Iteration 63/1000 | Loss: 0.00002217
Iteration 64/1000 | Loss: 0.00002217
Iteration 65/1000 | Loss: 0.00002217
Iteration 66/1000 | Loss: 0.00002217
Iteration 67/1000 | Loss: 0.00002217
Iteration 68/1000 | Loss: 0.00002216
Iteration 69/1000 | Loss: 0.00002216
Iteration 70/1000 | Loss: 0.00002216
Iteration 71/1000 | Loss: 0.00002216
Iteration 72/1000 | Loss: 0.00002215
Iteration 73/1000 | Loss: 0.00002215
Iteration 74/1000 | Loss: 0.00002215
Iteration 75/1000 | Loss: 0.00002215
Iteration 76/1000 | Loss: 0.00002215
Iteration 77/1000 | Loss: 0.00002215
Iteration 78/1000 | Loss: 0.00002215
Iteration 79/1000 | Loss: 0.00002215
Iteration 80/1000 | Loss: 0.00002215
Iteration 81/1000 | Loss: 0.00002214
Iteration 82/1000 | Loss: 0.00002214
Iteration 83/1000 | Loss: 0.00002214
Iteration 84/1000 | Loss: 0.00002213
Iteration 85/1000 | Loss: 0.00002213
Iteration 86/1000 | Loss: 0.00002213
Iteration 87/1000 | Loss: 0.00002213
Iteration 88/1000 | Loss: 0.00002213
Iteration 89/1000 | Loss: 0.00002213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 89. Stopping optimization.
Last 5 losses: [2.213491461589001e-05, 2.213491461589001e-05, 2.213491461589001e-05, 2.213491461589001e-05, 2.213491461589001e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.213491461589001e-05

Optimization complete. Final v2v error: 3.904046058654785 mm

Highest mean error: 4.531829833984375 mm for frame 167

Lowest mean error: 3.2265050411224365 mm for frame 198

Saving results

Total time: 40.80170917510986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00516436
Iteration 2/25 | Loss: 0.00127733
Iteration 3/25 | Loss: 0.00116018
Iteration 4/25 | Loss: 0.00115209
Iteration 5/25 | Loss: 0.00114937
Iteration 6/25 | Loss: 0.00114867
Iteration 7/25 | Loss: 0.00114867
Iteration 8/25 | Loss: 0.00114867
Iteration 9/25 | Loss: 0.00114867
Iteration 10/25 | Loss: 0.00114867
Iteration 11/25 | Loss: 0.00114867
Iteration 12/25 | Loss: 0.00114867
Iteration 13/25 | Loss: 0.00114867
Iteration 14/25 | Loss: 0.00114867
Iteration 15/25 | Loss: 0.00114867
Iteration 16/25 | Loss: 0.00114867
Iteration 17/25 | Loss: 0.00114867
Iteration 18/25 | Loss: 0.00114867
Iteration 19/25 | Loss: 0.00114867
Iteration 20/25 | Loss: 0.00114867
Iteration 21/25 | Loss: 0.00114867
Iteration 22/25 | Loss: 0.00114867
Iteration 23/25 | Loss: 0.00114867
Iteration 24/25 | Loss: 0.00114867
Iteration 25/25 | Loss: 0.00114867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011486723087728024, 0.0011486723087728024, 0.0011486723087728024, 0.0011486723087728024, 0.0011486723087728024]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011486723087728024

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40746856
Iteration 2/25 | Loss: 0.00075186
Iteration 3/25 | Loss: 0.00075185
Iteration 4/25 | Loss: 0.00075185
Iteration 5/25 | Loss: 0.00075185
Iteration 6/25 | Loss: 0.00075185
Iteration 7/25 | Loss: 0.00075185
Iteration 8/25 | Loss: 0.00075185
Iteration 9/25 | Loss: 0.00075185
Iteration 10/25 | Loss: 0.00075185
Iteration 11/25 | Loss: 0.00075185
Iteration 12/25 | Loss: 0.00075185
Iteration 13/25 | Loss: 0.00075185
Iteration 14/25 | Loss: 0.00075185
Iteration 15/25 | Loss: 0.00075185
Iteration 16/25 | Loss: 0.00075185
Iteration 17/25 | Loss: 0.00075185
Iteration 18/25 | Loss: 0.00075185
Iteration 19/25 | Loss: 0.00075185
Iteration 20/25 | Loss: 0.00075185
Iteration 21/25 | Loss: 0.00075185
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0007518469356000423, 0.0007518469356000423, 0.0007518469356000423, 0.0007518469356000423, 0.0007518469356000423]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007518469356000423

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075185
Iteration 2/1000 | Loss: 0.00004644
Iteration 3/1000 | Loss: 0.00003025
Iteration 4/1000 | Loss: 0.00002758
Iteration 5/1000 | Loss: 0.00002585
Iteration 6/1000 | Loss: 0.00002480
Iteration 7/1000 | Loss: 0.00002408
Iteration 8/1000 | Loss: 0.00002359
Iteration 9/1000 | Loss: 0.00002329
Iteration 10/1000 | Loss: 0.00002288
Iteration 11/1000 | Loss: 0.00002265
Iteration 12/1000 | Loss: 0.00002245
Iteration 13/1000 | Loss: 0.00002240
Iteration 14/1000 | Loss: 0.00002233
Iteration 15/1000 | Loss: 0.00002231
Iteration 16/1000 | Loss: 0.00002228
Iteration 17/1000 | Loss: 0.00002226
Iteration 18/1000 | Loss: 0.00002224
Iteration 19/1000 | Loss: 0.00002221
Iteration 20/1000 | Loss: 0.00002221
Iteration 21/1000 | Loss: 0.00002219
Iteration 22/1000 | Loss: 0.00002218
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002214
Iteration 25/1000 | Loss: 0.00002214
Iteration 26/1000 | Loss: 0.00002214
Iteration 27/1000 | Loss: 0.00002213
Iteration 28/1000 | Loss: 0.00002213
Iteration 29/1000 | Loss: 0.00002212
Iteration 30/1000 | Loss: 0.00002211
Iteration 31/1000 | Loss: 0.00002211
Iteration 32/1000 | Loss: 0.00002211
Iteration 33/1000 | Loss: 0.00002211
Iteration 34/1000 | Loss: 0.00002210
Iteration 35/1000 | Loss: 0.00002210
Iteration 36/1000 | Loss: 0.00002210
Iteration 37/1000 | Loss: 0.00002210
Iteration 38/1000 | Loss: 0.00002209
Iteration 39/1000 | Loss: 0.00002209
Iteration 40/1000 | Loss: 0.00002209
Iteration 41/1000 | Loss: 0.00002209
Iteration 42/1000 | Loss: 0.00002209
Iteration 43/1000 | Loss: 0.00002209
Iteration 44/1000 | Loss: 0.00002209
Iteration 45/1000 | Loss: 0.00002209
Iteration 46/1000 | Loss: 0.00002208
Iteration 47/1000 | Loss: 0.00002208
Iteration 48/1000 | Loss: 0.00002208
Iteration 49/1000 | Loss: 0.00002207
Iteration 50/1000 | Loss: 0.00002207
Iteration 51/1000 | Loss: 0.00002207
Iteration 52/1000 | Loss: 0.00002207
Iteration 53/1000 | Loss: 0.00002207
Iteration 54/1000 | Loss: 0.00002206
Iteration 55/1000 | Loss: 0.00002206
Iteration 56/1000 | Loss: 0.00002206
Iteration 57/1000 | Loss: 0.00002206
Iteration 58/1000 | Loss: 0.00002206
Iteration 59/1000 | Loss: 0.00002206
Iteration 60/1000 | Loss: 0.00002205
Iteration 61/1000 | Loss: 0.00002205
Iteration 62/1000 | Loss: 0.00002205
Iteration 63/1000 | Loss: 0.00002205
Iteration 64/1000 | Loss: 0.00002204
Iteration 65/1000 | Loss: 0.00002204
Iteration 66/1000 | Loss: 0.00002204
Iteration 67/1000 | Loss: 0.00002203
Iteration 68/1000 | Loss: 0.00002203
Iteration 69/1000 | Loss: 0.00002203
Iteration 70/1000 | Loss: 0.00002203
Iteration 71/1000 | Loss: 0.00002203
Iteration 72/1000 | Loss: 0.00002203
Iteration 73/1000 | Loss: 0.00002203
Iteration 74/1000 | Loss: 0.00002203
Iteration 75/1000 | Loss: 0.00002203
Iteration 76/1000 | Loss: 0.00002203
Iteration 77/1000 | Loss: 0.00002203
Iteration 78/1000 | Loss: 0.00002202
Iteration 79/1000 | Loss: 0.00002202
Iteration 80/1000 | Loss: 0.00002202
Iteration 81/1000 | Loss: 0.00002201
Iteration 82/1000 | Loss: 0.00002201
Iteration 83/1000 | Loss: 0.00002200
Iteration 84/1000 | Loss: 0.00002200
Iteration 85/1000 | Loss: 0.00002200
Iteration 86/1000 | Loss: 0.00002200
Iteration 87/1000 | Loss: 0.00002200
Iteration 88/1000 | Loss: 0.00002200
Iteration 89/1000 | Loss: 0.00002199
Iteration 90/1000 | Loss: 0.00002199
Iteration 91/1000 | Loss: 0.00002199
Iteration 92/1000 | Loss: 0.00002199
Iteration 93/1000 | Loss: 0.00002198
Iteration 94/1000 | Loss: 0.00002198
Iteration 95/1000 | Loss: 0.00002198
Iteration 96/1000 | Loss: 0.00002198
Iteration 97/1000 | Loss: 0.00002198
Iteration 98/1000 | Loss: 0.00002198
Iteration 99/1000 | Loss: 0.00002198
Iteration 100/1000 | Loss: 0.00002198
Iteration 101/1000 | Loss: 0.00002198
Iteration 102/1000 | Loss: 0.00002198
Iteration 103/1000 | Loss: 0.00002198
Iteration 104/1000 | Loss: 0.00002198
Iteration 105/1000 | Loss: 0.00002197
Iteration 106/1000 | Loss: 0.00002196
Iteration 107/1000 | Loss: 0.00002196
Iteration 108/1000 | Loss: 0.00002196
Iteration 109/1000 | Loss: 0.00002195
Iteration 110/1000 | Loss: 0.00002195
Iteration 111/1000 | Loss: 0.00002195
Iteration 112/1000 | Loss: 0.00002195
Iteration 113/1000 | Loss: 0.00002195
Iteration 114/1000 | Loss: 0.00002195
Iteration 115/1000 | Loss: 0.00002194
Iteration 116/1000 | Loss: 0.00002194
Iteration 117/1000 | Loss: 0.00002194
Iteration 118/1000 | Loss: 0.00002194
Iteration 119/1000 | Loss: 0.00002194
Iteration 120/1000 | Loss: 0.00002194
Iteration 121/1000 | Loss: 0.00002194
Iteration 122/1000 | Loss: 0.00002194
Iteration 123/1000 | Loss: 0.00002193
Iteration 124/1000 | Loss: 0.00002193
Iteration 125/1000 | Loss: 0.00002193
Iteration 126/1000 | Loss: 0.00002193
Iteration 127/1000 | Loss: 0.00002193
Iteration 128/1000 | Loss: 0.00002193
Iteration 129/1000 | Loss: 0.00002192
Iteration 130/1000 | Loss: 0.00002192
Iteration 131/1000 | Loss: 0.00002192
Iteration 132/1000 | Loss: 0.00002192
Iteration 133/1000 | Loss: 0.00002192
Iteration 134/1000 | Loss: 0.00002192
Iteration 135/1000 | Loss: 0.00002192
Iteration 136/1000 | Loss: 0.00002191
Iteration 137/1000 | Loss: 0.00002191
Iteration 138/1000 | Loss: 0.00002191
Iteration 139/1000 | Loss: 0.00002191
Iteration 140/1000 | Loss: 0.00002191
Iteration 141/1000 | Loss: 0.00002191
Iteration 142/1000 | Loss: 0.00002191
Iteration 143/1000 | Loss: 0.00002191
Iteration 144/1000 | Loss: 0.00002191
Iteration 145/1000 | Loss: 0.00002191
Iteration 146/1000 | Loss: 0.00002191
Iteration 147/1000 | Loss: 0.00002191
Iteration 148/1000 | Loss: 0.00002191
Iteration 149/1000 | Loss: 0.00002190
Iteration 150/1000 | Loss: 0.00002190
Iteration 151/1000 | Loss: 0.00002190
Iteration 152/1000 | Loss: 0.00002190
Iteration 153/1000 | Loss: 0.00002190
Iteration 154/1000 | Loss: 0.00002190
Iteration 155/1000 | Loss: 0.00002190
Iteration 156/1000 | Loss: 0.00002190
Iteration 157/1000 | Loss: 0.00002190
Iteration 158/1000 | Loss: 0.00002190
Iteration 159/1000 | Loss: 0.00002190
Iteration 160/1000 | Loss: 0.00002190
Iteration 161/1000 | Loss: 0.00002190
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.1897236365475692e-05, 2.1897236365475692e-05, 2.1897236365475692e-05, 2.1897236365475692e-05, 2.1897236365475692e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1897236365475692e-05

Optimization complete. Final v2v error: 3.657848834991455 mm

Highest mean error: 4.305894374847412 mm for frame 10

Lowest mean error: 2.7377355098724365 mm for frame 63

Saving results

Total time: 41.16185426712036
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00556272
Iteration 2/25 | Loss: 0.00161243
Iteration 3/25 | Loss: 0.00128153
Iteration 4/25 | Loss: 0.00126015
Iteration 5/25 | Loss: 0.00125734
Iteration 6/25 | Loss: 0.00125671
Iteration 7/25 | Loss: 0.00125671
Iteration 8/25 | Loss: 0.00125671
Iteration 9/25 | Loss: 0.00125671
Iteration 10/25 | Loss: 0.00125671
Iteration 11/25 | Loss: 0.00125671
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012567067751660943, 0.0012567067751660943, 0.0012567067751660943, 0.0012567067751660943, 0.0012567067751660943]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012567067751660943

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.00561798
Iteration 2/25 | Loss: 0.00085543
Iteration 3/25 | Loss: 0.00085541
Iteration 4/25 | Loss: 0.00085541
Iteration 5/25 | Loss: 0.00085541
Iteration 6/25 | Loss: 0.00085541
Iteration 7/25 | Loss: 0.00085541
Iteration 8/25 | Loss: 0.00085541
Iteration 9/25 | Loss: 0.00085541
Iteration 10/25 | Loss: 0.00085541
Iteration 11/25 | Loss: 0.00085541
Iteration 12/25 | Loss: 0.00085541
Iteration 13/25 | Loss: 0.00085541
Iteration 14/25 | Loss: 0.00085541
Iteration 15/25 | Loss: 0.00085541
Iteration 16/25 | Loss: 0.00085541
Iteration 17/25 | Loss: 0.00085541
Iteration 18/25 | Loss: 0.00085541
Iteration 19/25 | Loss: 0.00085541
Iteration 20/25 | Loss: 0.00085541
Iteration 21/25 | Loss: 0.00085541
Iteration 22/25 | Loss: 0.00085541
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008554100058972836, 0.0008554100058972836, 0.0008554100058972836, 0.0008554100058972836, 0.0008554100058972836]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008554100058972836

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085541
Iteration 2/1000 | Loss: 0.00004516
Iteration 3/1000 | Loss: 0.00002945
Iteration 4/1000 | Loss: 0.00002678
Iteration 5/1000 | Loss: 0.00002531
Iteration 6/1000 | Loss: 0.00002468
Iteration 7/1000 | Loss: 0.00002412
Iteration 8/1000 | Loss: 0.00002361
Iteration 9/1000 | Loss: 0.00002331
Iteration 10/1000 | Loss: 0.00002300
Iteration 11/1000 | Loss: 0.00002274
Iteration 12/1000 | Loss: 0.00002251
Iteration 13/1000 | Loss: 0.00002229
Iteration 14/1000 | Loss: 0.00002210
Iteration 15/1000 | Loss: 0.00002194
Iteration 16/1000 | Loss: 0.00002187
Iteration 17/1000 | Loss: 0.00002187
Iteration 18/1000 | Loss: 0.00002183
Iteration 19/1000 | Loss: 0.00002172
Iteration 20/1000 | Loss: 0.00002166
Iteration 21/1000 | Loss: 0.00002163
Iteration 22/1000 | Loss: 0.00002163
Iteration 23/1000 | Loss: 0.00002162
Iteration 24/1000 | Loss: 0.00002162
Iteration 25/1000 | Loss: 0.00002161
Iteration 26/1000 | Loss: 0.00002161
Iteration 27/1000 | Loss: 0.00002160
Iteration 28/1000 | Loss: 0.00002159
Iteration 29/1000 | Loss: 0.00002159
Iteration 30/1000 | Loss: 0.00002159
Iteration 31/1000 | Loss: 0.00002158
Iteration 32/1000 | Loss: 0.00002158
Iteration 33/1000 | Loss: 0.00002157
Iteration 34/1000 | Loss: 0.00002157
Iteration 35/1000 | Loss: 0.00002156
Iteration 36/1000 | Loss: 0.00002155
Iteration 37/1000 | Loss: 0.00002153
Iteration 38/1000 | Loss: 0.00002153
Iteration 39/1000 | Loss: 0.00002152
Iteration 40/1000 | Loss: 0.00002152
Iteration 41/1000 | Loss: 0.00002149
Iteration 42/1000 | Loss: 0.00002149
Iteration 43/1000 | Loss: 0.00002149
Iteration 44/1000 | Loss: 0.00002149
Iteration 45/1000 | Loss: 0.00002148
Iteration 46/1000 | Loss: 0.00002148
Iteration 47/1000 | Loss: 0.00002148
Iteration 48/1000 | Loss: 0.00002147
Iteration 49/1000 | Loss: 0.00002147
Iteration 50/1000 | Loss: 0.00002146
Iteration 51/1000 | Loss: 0.00002146
Iteration 52/1000 | Loss: 0.00002146
Iteration 53/1000 | Loss: 0.00002146
Iteration 54/1000 | Loss: 0.00002146
Iteration 55/1000 | Loss: 0.00002146
Iteration 56/1000 | Loss: 0.00002146
Iteration 57/1000 | Loss: 0.00002146
Iteration 58/1000 | Loss: 0.00002145
Iteration 59/1000 | Loss: 0.00002145
Iteration 60/1000 | Loss: 0.00002145
Iteration 61/1000 | Loss: 0.00002145
Iteration 62/1000 | Loss: 0.00002145
Iteration 63/1000 | Loss: 0.00002145
Iteration 64/1000 | Loss: 0.00002145
Iteration 65/1000 | Loss: 0.00002145
Iteration 66/1000 | Loss: 0.00002145
Iteration 67/1000 | Loss: 0.00002145
Iteration 68/1000 | Loss: 0.00002145
Iteration 69/1000 | Loss: 0.00002145
Iteration 70/1000 | Loss: 0.00002145
Iteration 71/1000 | Loss: 0.00002145
Iteration 72/1000 | Loss: 0.00002145
Iteration 73/1000 | Loss: 0.00002145
Iteration 74/1000 | Loss: 0.00002145
Iteration 75/1000 | Loss: 0.00002145
Iteration 76/1000 | Loss: 0.00002145
Iteration 77/1000 | Loss: 0.00002145
Iteration 78/1000 | Loss: 0.00002145
Iteration 79/1000 | Loss: 0.00002145
Iteration 80/1000 | Loss: 0.00002145
Iteration 81/1000 | Loss: 0.00002145
Iteration 82/1000 | Loss: 0.00002145
Iteration 83/1000 | Loss: 0.00002145
Iteration 84/1000 | Loss: 0.00002145
Iteration 85/1000 | Loss: 0.00002145
Iteration 86/1000 | Loss: 0.00002145
Iteration 87/1000 | Loss: 0.00002145
Iteration 88/1000 | Loss: 0.00002145
Iteration 89/1000 | Loss: 0.00002145
Iteration 90/1000 | Loss: 0.00002145
Iteration 91/1000 | Loss: 0.00002145
Iteration 92/1000 | Loss: 0.00002145
Iteration 93/1000 | Loss: 0.00002145
Iteration 94/1000 | Loss: 0.00002145
Iteration 95/1000 | Loss: 0.00002145
Iteration 96/1000 | Loss: 0.00002145
Iteration 97/1000 | Loss: 0.00002145
Iteration 98/1000 | Loss: 0.00002145
Iteration 99/1000 | Loss: 0.00002145
Iteration 100/1000 | Loss: 0.00002145
Iteration 101/1000 | Loss: 0.00002145
Iteration 102/1000 | Loss: 0.00002145
Iteration 103/1000 | Loss: 0.00002145
Iteration 104/1000 | Loss: 0.00002145
Iteration 105/1000 | Loss: 0.00002145
Iteration 106/1000 | Loss: 0.00002145
Iteration 107/1000 | Loss: 0.00002145
Iteration 108/1000 | Loss: 0.00002145
Iteration 109/1000 | Loss: 0.00002145
Iteration 110/1000 | Loss: 0.00002145
Iteration 111/1000 | Loss: 0.00002145
Iteration 112/1000 | Loss: 0.00002145
Iteration 113/1000 | Loss: 0.00002145
Iteration 114/1000 | Loss: 0.00002145
Iteration 115/1000 | Loss: 0.00002145
Iteration 116/1000 | Loss: 0.00002145
Iteration 117/1000 | Loss: 0.00002145
Iteration 118/1000 | Loss: 0.00002145
Iteration 119/1000 | Loss: 0.00002145
Iteration 120/1000 | Loss: 0.00002145
Iteration 121/1000 | Loss: 0.00002145
Iteration 122/1000 | Loss: 0.00002145
Iteration 123/1000 | Loss: 0.00002145
Iteration 124/1000 | Loss: 0.00002145
Iteration 125/1000 | Loss: 0.00002145
Iteration 126/1000 | Loss: 0.00002145
Iteration 127/1000 | Loss: 0.00002145
Iteration 128/1000 | Loss: 0.00002145
Iteration 129/1000 | Loss: 0.00002145
Iteration 130/1000 | Loss: 0.00002145
Iteration 131/1000 | Loss: 0.00002145
Iteration 132/1000 | Loss: 0.00002145
Iteration 133/1000 | Loss: 0.00002145
Iteration 134/1000 | Loss: 0.00002145
Iteration 135/1000 | Loss: 0.00002145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.144922655133996e-05, 2.144922655133996e-05, 2.144922655133996e-05, 2.144922655133996e-05, 2.144922655133996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.144922655133996e-05

Optimization complete. Final v2v error: 3.6845030784606934 mm

Highest mean error: 4.761993408203125 mm for frame 59

Lowest mean error: 2.8356661796569824 mm for frame 137

Saving results

Total time: 40.30497717857361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987245
Iteration 2/25 | Loss: 0.00290941
Iteration 3/25 | Loss: 0.00208522
Iteration 4/25 | Loss: 0.00188780
Iteration 5/25 | Loss: 0.00190084
Iteration 6/25 | Loss: 0.00187915
Iteration 7/25 | Loss: 0.00171767
Iteration 8/25 | Loss: 0.00166016
Iteration 9/25 | Loss: 0.00164214
Iteration 10/25 | Loss: 0.00162458
Iteration 11/25 | Loss: 0.00160767
Iteration 12/25 | Loss: 0.00160649
Iteration 13/25 | Loss: 0.00160730
Iteration 14/25 | Loss: 0.00160158
Iteration 15/25 | Loss: 0.00160043
Iteration 16/25 | Loss: 0.00159765
Iteration 17/25 | Loss: 0.00159980
Iteration 18/25 | Loss: 0.00159162
Iteration 19/25 | Loss: 0.00158674
Iteration 20/25 | Loss: 0.00158905
Iteration 21/25 | Loss: 0.00158774
Iteration 22/25 | Loss: 0.00159257
Iteration 23/25 | Loss: 0.00158522
Iteration 24/25 | Loss: 0.00158384
Iteration 25/25 | Loss: 0.00158064

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40577495
Iteration 2/25 | Loss: 0.00436548
Iteration 3/25 | Loss: 0.00419110
Iteration 4/25 | Loss: 0.00420482
Iteration 5/25 | Loss: 0.00416760
Iteration 6/25 | Loss: 0.00416742
Iteration 7/25 | Loss: 0.00416742
Iteration 8/25 | Loss: 0.00416742
Iteration 9/25 | Loss: 0.00416742
Iteration 10/25 | Loss: 0.00416742
Iteration 11/25 | Loss: 0.00416742
Iteration 12/25 | Loss: 0.00416742
Iteration 13/25 | Loss: 0.00416742
Iteration 14/25 | Loss: 0.00416742
Iteration 15/25 | Loss: 0.00416742
Iteration 16/25 | Loss: 0.00416742
Iteration 17/25 | Loss: 0.00416742
Iteration 18/25 | Loss: 0.00416742
Iteration 19/25 | Loss: 0.00416742
Iteration 20/25 | Loss: 0.00416742
Iteration 21/25 | Loss: 0.00416742
Iteration 22/25 | Loss: 0.00416742
Iteration 23/25 | Loss: 0.00416742
Iteration 24/25 | Loss: 0.00416742
Iteration 25/25 | Loss: 0.00416742
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.004167420323938131, 0.004167420323938131, 0.004167420323938131, 0.004167420323938131, 0.004167420323938131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004167420323938131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00416742
Iteration 2/1000 | Loss: 0.00081115
Iteration 3/1000 | Loss: 0.00094588
Iteration 4/1000 | Loss: 0.00068430
Iteration 5/1000 | Loss: 0.00230972
Iteration 6/1000 | Loss: 0.00250688
Iteration 7/1000 | Loss: 0.00074288
Iteration 8/1000 | Loss: 0.00227606
Iteration 9/1000 | Loss: 0.00128194
Iteration 10/1000 | Loss: 0.00106776
Iteration 11/1000 | Loss: 0.00575817
Iteration 12/1000 | Loss: 0.00084860
Iteration 13/1000 | Loss: 0.00092059
Iteration 14/1000 | Loss: 0.00031632
Iteration 15/1000 | Loss: 0.00030168
Iteration 16/1000 | Loss: 0.00085813
Iteration 17/1000 | Loss: 0.00083587
Iteration 18/1000 | Loss: 0.00050882
Iteration 19/1000 | Loss: 0.00115346
Iteration 20/1000 | Loss: 0.00169663
Iteration 21/1000 | Loss: 0.00101013
Iteration 22/1000 | Loss: 0.00124882
Iteration 23/1000 | Loss: 0.00183726
Iteration 24/1000 | Loss: 0.00119127
Iteration 25/1000 | Loss: 0.00060267
Iteration 26/1000 | Loss: 0.00060153
Iteration 27/1000 | Loss: 0.00071690
Iteration 28/1000 | Loss: 0.00052861
Iteration 29/1000 | Loss: 0.00171173
Iteration 30/1000 | Loss: 0.00026333
Iteration 31/1000 | Loss: 0.00051873
Iteration 32/1000 | Loss: 0.00022174
Iteration 33/1000 | Loss: 0.00022278
Iteration 34/1000 | Loss: 0.00040873
Iteration 35/1000 | Loss: 0.00052444
Iteration 36/1000 | Loss: 0.00022260
Iteration 37/1000 | Loss: 0.00039503
Iteration 38/1000 | Loss: 0.00045016
Iteration 39/1000 | Loss: 0.00049289
Iteration 40/1000 | Loss: 0.00061863
Iteration 41/1000 | Loss: 0.00050556
Iteration 42/1000 | Loss: 0.00059615
Iteration 43/1000 | Loss: 0.00155760
Iteration 44/1000 | Loss: 0.00116489
Iteration 45/1000 | Loss: 0.00090040
Iteration 46/1000 | Loss: 0.00027227
Iteration 47/1000 | Loss: 0.00063920
Iteration 48/1000 | Loss: 0.00080962
Iteration 49/1000 | Loss: 0.00090975
Iteration 50/1000 | Loss: 0.00057961
Iteration 51/1000 | Loss: 0.00028904
Iteration 52/1000 | Loss: 0.00021793
Iteration 53/1000 | Loss: 0.00021946
Iteration 54/1000 | Loss: 0.00030383
Iteration 55/1000 | Loss: 0.00052796
Iteration 56/1000 | Loss: 0.00027129
Iteration 57/1000 | Loss: 0.00062573
Iteration 58/1000 | Loss: 0.00072287
Iteration 59/1000 | Loss: 0.00031858
Iteration 60/1000 | Loss: 0.00023473
Iteration 61/1000 | Loss: 0.00020875
Iteration 62/1000 | Loss: 0.00036903
Iteration 63/1000 | Loss: 0.00023361
Iteration 64/1000 | Loss: 0.00036235
Iteration 65/1000 | Loss: 0.00024642
Iteration 66/1000 | Loss: 0.00242468
Iteration 67/1000 | Loss: 0.00431165
Iteration 68/1000 | Loss: 0.00304498
Iteration 69/1000 | Loss: 0.00142547
Iteration 70/1000 | Loss: 0.00102642
Iteration 71/1000 | Loss: 0.00027130
Iteration 72/1000 | Loss: 0.00047077
Iteration 73/1000 | Loss: 0.00139493
Iteration 74/1000 | Loss: 0.00093349
Iteration 75/1000 | Loss: 0.00103702
Iteration 76/1000 | Loss: 0.00024806
Iteration 77/1000 | Loss: 0.00079371
Iteration 78/1000 | Loss: 0.00094367
Iteration 79/1000 | Loss: 0.00079915
Iteration 80/1000 | Loss: 0.00089052
Iteration 81/1000 | Loss: 0.00059282
Iteration 82/1000 | Loss: 0.00136718
Iteration 83/1000 | Loss: 0.00014083
Iteration 84/1000 | Loss: 0.00062623
Iteration 85/1000 | Loss: 0.00040019
Iteration 86/1000 | Loss: 0.00045781
Iteration 87/1000 | Loss: 0.00049729
Iteration 88/1000 | Loss: 0.00068780
Iteration 89/1000 | Loss: 0.00029251
Iteration 90/1000 | Loss: 0.00024397
Iteration 91/1000 | Loss: 0.00085160
Iteration 92/1000 | Loss: 0.00015856
Iteration 93/1000 | Loss: 0.00078556
Iteration 94/1000 | Loss: 0.00056513
Iteration 95/1000 | Loss: 0.00069761
Iteration 96/1000 | Loss: 0.00051756
Iteration 97/1000 | Loss: 0.00020413
Iteration 98/1000 | Loss: 0.00015050
Iteration 99/1000 | Loss: 0.00040436
Iteration 100/1000 | Loss: 0.00034930
Iteration 101/1000 | Loss: 0.00035428
Iteration 102/1000 | Loss: 0.00010807
Iteration 103/1000 | Loss: 0.00026287
Iteration 104/1000 | Loss: 0.00019782
Iteration 105/1000 | Loss: 0.00135730
Iteration 106/1000 | Loss: 0.00035451
Iteration 107/1000 | Loss: 0.00029955
Iteration 108/1000 | Loss: 0.00010328
Iteration 109/1000 | Loss: 0.00010788
Iteration 110/1000 | Loss: 0.00031732
Iteration 111/1000 | Loss: 0.00038024
Iteration 112/1000 | Loss: 0.00015448
Iteration 113/1000 | Loss: 0.00007741
Iteration 114/1000 | Loss: 0.00052607
Iteration 115/1000 | Loss: 0.00046142
Iteration 116/1000 | Loss: 0.00037288
Iteration 117/1000 | Loss: 0.00011564
Iteration 118/1000 | Loss: 0.00010196
Iteration 119/1000 | Loss: 0.00015219
Iteration 120/1000 | Loss: 0.00006781
Iteration 121/1000 | Loss: 0.00021438
Iteration 122/1000 | Loss: 0.00123150
Iteration 123/1000 | Loss: 0.00008426
Iteration 124/1000 | Loss: 0.00006215
Iteration 125/1000 | Loss: 0.00005882
Iteration 126/1000 | Loss: 0.00051858
Iteration 127/1000 | Loss: 0.00038529
Iteration 128/1000 | Loss: 0.00043350
Iteration 129/1000 | Loss: 0.00010259
Iteration 130/1000 | Loss: 0.00028237
Iteration 131/1000 | Loss: 0.00075568
Iteration 132/1000 | Loss: 0.00052225
Iteration 133/1000 | Loss: 0.00022194
Iteration 134/1000 | Loss: 0.00040589
Iteration 135/1000 | Loss: 0.00041109
Iteration 136/1000 | Loss: 0.00008578
Iteration 137/1000 | Loss: 0.00021098
Iteration 138/1000 | Loss: 0.00055134
Iteration 139/1000 | Loss: 0.00033012
Iteration 140/1000 | Loss: 0.00040323
Iteration 141/1000 | Loss: 0.00017273
Iteration 142/1000 | Loss: 0.00011300
Iteration 143/1000 | Loss: 0.00006924
Iteration 144/1000 | Loss: 0.00007918
Iteration 145/1000 | Loss: 0.00012972
Iteration 146/1000 | Loss: 0.00023090
Iteration 147/1000 | Loss: 0.00003948
Iteration 148/1000 | Loss: 0.00014734
Iteration 149/1000 | Loss: 0.00021388
Iteration 150/1000 | Loss: 0.00004771
Iteration 151/1000 | Loss: 0.00004074
Iteration 152/1000 | Loss: 0.00003810
Iteration 153/1000 | Loss: 0.00003541
Iteration 154/1000 | Loss: 0.00005364
Iteration 155/1000 | Loss: 0.00035340
Iteration 156/1000 | Loss: 0.00030871
Iteration 157/1000 | Loss: 0.00076842
Iteration 158/1000 | Loss: 0.00111758
Iteration 159/1000 | Loss: 0.00087246
Iteration 160/1000 | Loss: 0.00048495
Iteration 161/1000 | Loss: 0.00049333
Iteration 162/1000 | Loss: 0.00057813
Iteration 163/1000 | Loss: 0.00168671
Iteration 164/1000 | Loss: 0.00134919
Iteration 165/1000 | Loss: 0.00076689
Iteration 166/1000 | Loss: 0.00116650
Iteration 167/1000 | Loss: 0.00068899
Iteration 168/1000 | Loss: 0.00007084
Iteration 169/1000 | Loss: 0.00017369
Iteration 170/1000 | Loss: 0.00004468
Iteration 171/1000 | Loss: 0.00006034
Iteration 172/1000 | Loss: 0.00004853
Iteration 173/1000 | Loss: 0.00005784
Iteration 174/1000 | Loss: 0.00009282
Iteration 175/1000 | Loss: 0.00007391
Iteration 176/1000 | Loss: 0.00002926
Iteration 177/1000 | Loss: 0.00043696
Iteration 178/1000 | Loss: 0.00049204
Iteration 179/1000 | Loss: 0.00047334
Iteration 180/1000 | Loss: 0.00003278
Iteration 181/1000 | Loss: 0.00002645
Iteration 182/1000 | Loss: 0.00008916
Iteration 183/1000 | Loss: 0.00004495
Iteration 184/1000 | Loss: 0.00016925
Iteration 185/1000 | Loss: 0.00002486
Iteration 186/1000 | Loss: 0.00002542
Iteration 187/1000 | Loss: 0.00006078
Iteration 188/1000 | Loss: 0.00003323
Iteration 189/1000 | Loss: 0.00006309
Iteration 190/1000 | Loss: 0.00002683
Iteration 191/1000 | Loss: 0.00002400
Iteration 192/1000 | Loss: 0.00002178
Iteration 193/1000 | Loss: 0.00004588
Iteration 194/1000 | Loss: 0.00002140
Iteration 195/1000 | Loss: 0.00002118
Iteration 196/1000 | Loss: 0.00006496
Iteration 197/1000 | Loss: 0.00003117
Iteration 198/1000 | Loss: 0.00002775
Iteration 199/1000 | Loss: 0.00002249
Iteration 200/1000 | Loss: 0.00002092
Iteration 201/1000 | Loss: 0.00002086
Iteration 202/1000 | Loss: 0.00002086
Iteration 203/1000 | Loss: 0.00002086
Iteration 204/1000 | Loss: 0.00002086
Iteration 205/1000 | Loss: 0.00002086
Iteration 206/1000 | Loss: 0.00002085
Iteration 207/1000 | Loss: 0.00002085
Iteration 208/1000 | Loss: 0.00002074
Iteration 209/1000 | Loss: 0.00004921
Iteration 210/1000 | Loss: 0.00003638
Iteration 211/1000 | Loss: 0.00002071
Iteration 212/1000 | Loss: 0.00002070
Iteration 213/1000 | Loss: 0.00002077
Iteration 214/1000 | Loss: 0.00002059
Iteration 215/1000 | Loss: 0.00002059
Iteration 216/1000 | Loss: 0.00002059
Iteration 217/1000 | Loss: 0.00002059
Iteration 218/1000 | Loss: 0.00002059
Iteration 219/1000 | Loss: 0.00002059
Iteration 220/1000 | Loss: 0.00002059
Iteration 221/1000 | Loss: 0.00002059
Iteration 222/1000 | Loss: 0.00002059
Iteration 223/1000 | Loss: 0.00002059
Iteration 224/1000 | Loss: 0.00002059
Iteration 225/1000 | Loss: 0.00002058
Iteration 226/1000 | Loss: 0.00002057
Iteration 227/1000 | Loss: 0.00002057
Iteration 228/1000 | Loss: 0.00002056
Iteration 229/1000 | Loss: 0.00002054
Iteration 230/1000 | Loss: 0.00002054
Iteration 231/1000 | Loss: 0.00002054
Iteration 232/1000 | Loss: 0.00002054
Iteration 233/1000 | Loss: 0.00002054
Iteration 234/1000 | Loss: 0.00002054
Iteration 235/1000 | Loss: 0.00004625
Iteration 236/1000 | Loss: 0.00002077
Iteration 237/1000 | Loss: 0.00002074
Iteration 238/1000 | Loss: 0.00002054
Iteration 239/1000 | Loss: 0.00002049
Iteration 240/1000 | Loss: 0.00002049
Iteration 241/1000 | Loss: 0.00002049
Iteration 242/1000 | Loss: 0.00002049
Iteration 243/1000 | Loss: 0.00002048
Iteration 244/1000 | Loss: 0.00002048
Iteration 245/1000 | Loss: 0.00002048
Iteration 246/1000 | Loss: 0.00002048
Iteration 247/1000 | Loss: 0.00002048
Iteration 248/1000 | Loss: 0.00002048
Iteration 249/1000 | Loss: 0.00002048
Iteration 250/1000 | Loss: 0.00002048
Iteration 251/1000 | Loss: 0.00002048
Iteration 252/1000 | Loss: 0.00002048
Iteration 253/1000 | Loss: 0.00002048
Iteration 254/1000 | Loss: 0.00002048
Iteration 255/1000 | Loss: 0.00002048
Iteration 256/1000 | Loss: 0.00002047
Iteration 257/1000 | Loss: 0.00002047
Iteration 258/1000 | Loss: 0.00002047
Iteration 259/1000 | Loss: 0.00002047
Iteration 260/1000 | Loss: 0.00002047
Iteration 261/1000 | Loss: 0.00002047
Iteration 262/1000 | Loss: 0.00002047
Iteration 263/1000 | Loss: 0.00002047
Iteration 264/1000 | Loss: 0.00002047
Iteration 265/1000 | Loss: 0.00002047
Iteration 266/1000 | Loss: 0.00002047
Iteration 267/1000 | Loss: 0.00002047
Iteration 268/1000 | Loss: 0.00002047
Iteration 269/1000 | Loss: 0.00002047
Iteration 270/1000 | Loss: 0.00002046
Iteration 271/1000 | Loss: 0.00002046
Iteration 272/1000 | Loss: 0.00002046
Iteration 273/1000 | Loss: 0.00002046
Iteration 274/1000 | Loss: 0.00002046
Iteration 275/1000 | Loss: 0.00002046
Iteration 276/1000 | Loss: 0.00002046
Iteration 277/1000 | Loss: 0.00002046
Iteration 278/1000 | Loss: 0.00002046
Iteration 279/1000 | Loss: 0.00002046
Iteration 280/1000 | Loss: 0.00002045
Iteration 281/1000 | Loss: 0.00002045
Iteration 282/1000 | Loss: 0.00002044
Iteration 283/1000 | Loss: 0.00002044
Iteration 284/1000 | Loss: 0.00002044
Iteration 285/1000 | Loss: 0.00002043
Iteration 286/1000 | Loss: 0.00002043
Iteration 287/1000 | Loss: 0.00002042
Iteration 288/1000 | Loss: 0.00002042
Iteration 289/1000 | Loss: 0.00002042
Iteration 290/1000 | Loss: 0.00002042
Iteration 291/1000 | Loss: 0.00004961
Iteration 292/1000 | Loss: 0.00002073
Iteration 293/1000 | Loss: 0.00002338
Iteration 294/1000 | Loss: 0.00002042
Iteration 295/1000 | Loss: 0.00002041
Iteration 296/1000 | Loss: 0.00002041
Iteration 297/1000 | Loss: 0.00002041
Iteration 298/1000 | Loss: 0.00002041
Iteration 299/1000 | Loss: 0.00002039
Iteration 300/1000 | Loss: 0.00002039
Iteration 301/1000 | Loss: 0.00002039
Iteration 302/1000 | Loss: 0.00002039
Iteration 303/1000 | Loss: 0.00002039
Iteration 304/1000 | Loss: 0.00002038
Iteration 305/1000 | Loss: 0.00002037
Iteration 306/1000 | Loss: 0.00002037
Iteration 307/1000 | Loss: 0.00002037
Iteration 308/1000 | Loss: 0.00002036
Iteration 309/1000 | Loss: 0.00002036
Iteration 310/1000 | Loss: 0.00002036
Iteration 311/1000 | Loss: 0.00002035
Iteration 312/1000 | Loss: 0.00002035
Iteration 313/1000 | Loss: 0.00002035
Iteration 314/1000 | Loss: 0.00002035
Iteration 315/1000 | Loss: 0.00002035
Iteration 316/1000 | Loss: 0.00002035
Iteration 317/1000 | Loss: 0.00002035
Iteration 318/1000 | Loss: 0.00002035
Iteration 319/1000 | Loss: 0.00002035
Iteration 320/1000 | Loss: 0.00002035
Iteration 321/1000 | Loss: 0.00002034
Iteration 322/1000 | Loss: 0.00002034
Iteration 323/1000 | Loss: 0.00002034
Iteration 324/1000 | Loss: 0.00002034
Iteration 325/1000 | Loss: 0.00002034
Iteration 326/1000 | Loss: 0.00002034
Iteration 327/1000 | Loss: 0.00002034
Iteration 328/1000 | Loss: 0.00002034
Iteration 329/1000 | Loss: 0.00002034
Iteration 330/1000 | Loss: 0.00002034
Iteration 331/1000 | Loss: 0.00002034
Iteration 332/1000 | Loss: 0.00002034
Iteration 333/1000 | Loss: 0.00002034
Iteration 334/1000 | Loss: 0.00002034
Iteration 335/1000 | Loss: 0.00002034
Iteration 336/1000 | Loss: 0.00002034
Iteration 337/1000 | Loss: 0.00002034
Iteration 338/1000 | Loss: 0.00002034
Iteration 339/1000 | Loss: 0.00002034
Iteration 340/1000 | Loss: 0.00002034
Iteration 341/1000 | Loss: 0.00002034
Iteration 342/1000 | Loss: 0.00002034
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 342. Stopping optimization.
Last 5 losses: [2.034121826000046e-05, 2.034121826000046e-05, 2.034121826000046e-05, 2.034121826000046e-05, 2.034121826000046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.034121826000046e-05

Optimization complete. Final v2v error: 3.102980613708496 mm

Highest mean error: 11.081451416015625 mm for frame 163

Lowest mean error: 2.6420681476593018 mm for frame 127

Saving results

Total time: 392.02129316329956
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424588
Iteration 2/25 | Loss: 0.00115272
Iteration 3/25 | Loss: 0.00109533
Iteration 4/25 | Loss: 0.00108065
Iteration 5/25 | Loss: 0.00107644
Iteration 6/25 | Loss: 0.00107555
Iteration 7/25 | Loss: 0.00107550
Iteration 8/25 | Loss: 0.00107550
Iteration 9/25 | Loss: 0.00107550
Iteration 10/25 | Loss: 0.00107550
Iteration 11/25 | Loss: 0.00107550
Iteration 12/25 | Loss: 0.00107550
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010754967806860805, 0.0010754967806860805, 0.0010754967806860805, 0.0010754967806860805, 0.0010754967806860805]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010754967806860805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66408420
Iteration 2/25 | Loss: 0.00061139
Iteration 3/25 | Loss: 0.00061139
Iteration 4/25 | Loss: 0.00061139
Iteration 5/25 | Loss: 0.00061139
Iteration 6/25 | Loss: 0.00061139
Iteration 7/25 | Loss: 0.00061138
Iteration 8/25 | Loss: 0.00061138
Iteration 9/25 | Loss: 0.00061138
Iteration 10/25 | Loss: 0.00061138
Iteration 11/25 | Loss: 0.00061138
Iteration 12/25 | Loss: 0.00061138
Iteration 13/25 | Loss: 0.00061138
Iteration 14/25 | Loss: 0.00061138
Iteration 15/25 | Loss: 0.00061138
Iteration 16/25 | Loss: 0.00061138
Iteration 17/25 | Loss: 0.00061138
Iteration 18/25 | Loss: 0.00061138
Iteration 19/25 | Loss: 0.00061138
Iteration 20/25 | Loss: 0.00061138
Iteration 21/25 | Loss: 0.00061138
Iteration 22/25 | Loss: 0.00061138
Iteration 23/25 | Loss: 0.00061138
Iteration 24/25 | Loss: 0.00061138
Iteration 25/25 | Loss: 0.00061138

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061138
Iteration 2/1000 | Loss: 0.00002212
Iteration 3/1000 | Loss: 0.00001452
Iteration 4/1000 | Loss: 0.00001362
Iteration 5/1000 | Loss: 0.00001312
Iteration 6/1000 | Loss: 0.00001297
Iteration 7/1000 | Loss: 0.00001293
Iteration 8/1000 | Loss: 0.00001267
Iteration 9/1000 | Loss: 0.00001246
Iteration 10/1000 | Loss: 0.00001235
Iteration 11/1000 | Loss: 0.00001216
Iteration 12/1000 | Loss: 0.00001213
Iteration 13/1000 | Loss: 0.00001213
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001198
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001197
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001186
Iteration 22/1000 | Loss: 0.00001185
Iteration 23/1000 | Loss: 0.00001184
Iteration 24/1000 | Loss: 0.00001183
Iteration 25/1000 | Loss: 0.00001183
Iteration 26/1000 | Loss: 0.00001183
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001182
Iteration 30/1000 | Loss: 0.00001182
Iteration 31/1000 | Loss: 0.00001181
Iteration 32/1000 | Loss: 0.00001180
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001179
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001177
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001175
Iteration 46/1000 | Loss: 0.00001175
Iteration 47/1000 | Loss: 0.00001175
Iteration 48/1000 | Loss: 0.00001175
Iteration 49/1000 | Loss: 0.00001174
Iteration 50/1000 | Loss: 0.00001174
Iteration 51/1000 | Loss: 0.00001174
Iteration 52/1000 | Loss: 0.00001173
Iteration 53/1000 | Loss: 0.00001173
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001171
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001171
Iteration 61/1000 | Loss: 0.00001171
Iteration 62/1000 | Loss: 0.00001171
Iteration 63/1000 | Loss: 0.00001171
Iteration 64/1000 | Loss: 0.00001171
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001170
Iteration 67/1000 | Loss: 0.00001170
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001169
Iteration 70/1000 | Loss: 0.00001169
Iteration 71/1000 | Loss: 0.00001169
Iteration 72/1000 | Loss: 0.00001169
Iteration 73/1000 | Loss: 0.00001169
Iteration 74/1000 | Loss: 0.00001169
Iteration 75/1000 | Loss: 0.00001169
Iteration 76/1000 | Loss: 0.00001168
Iteration 77/1000 | Loss: 0.00001168
Iteration 78/1000 | Loss: 0.00001168
Iteration 79/1000 | Loss: 0.00001168
Iteration 80/1000 | Loss: 0.00001168
Iteration 81/1000 | Loss: 0.00001168
Iteration 82/1000 | Loss: 0.00001167
Iteration 83/1000 | Loss: 0.00001167
Iteration 84/1000 | Loss: 0.00001167
Iteration 85/1000 | Loss: 0.00001167
Iteration 86/1000 | Loss: 0.00001167
Iteration 87/1000 | Loss: 0.00001167
Iteration 88/1000 | Loss: 0.00001167
Iteration 89/1000 | Loss: 0.00001167
Iteration 90/1000 | Loss: 0.00001166
Iteration 91/1000 | Loss: 0.00001166
Iteration 92/1000 | Loss: 0.00001166
Iteration 93/1000 | Loss: 0.00001165
Iteration 94/1000 | Loss: 0.00001165
Iteration 95/1000 | Loss: 0.00001165
Iteration 96/1000 | Loss: 0.00001164
Iteration 97/1000 | Loss: 0.00001164
Iteration 98/1000 | Loss: 0.00001164
Iteration 99/1000 | Loss: 0.00001164
Iteration 100/1000 | Loss: 0.00001164
Iteration 101/1000 | Loss: 0.00001164
Iteration 102/1000 | Loss: 0.00001164
Iteration 103/1000 | Loss: 0.00001164
Iteration 104/1000 | Loss: 0.00001163
Iteration 105/1000 | Loss: 0.00001163
Iteration 106/1000 | Loss: 0.00001163
Iteration 107/1000 | Loss: 0.00001162
Iteration 108/1000 | Loss: 0.00001161
Iteration 109/1000 | Loss: 0.00001161
Iteration 110/1000 | Loss: 0.00001161
Iteration 111/1000 | Loss: 0.00001161
Iteration 112/1000 | Loss: 0.00001161
Iteration 113/1000 | Loss: 0.00001161
Iteration 114/1000 | Loss: 0.00001161
Iteration 115/1000 | Loss: 0.00001160
Iteration 116/1000 | Loss: 0.00001160
Iteration 117/1000 | Loss: 0.00001160
Iteration 118/1000 | Loss: 0.00001159
Iteration 119/1000 | Loss: 0.00001159
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001158
Iteration 122/1000 | Loss: 0.00001158
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001157
Iteration 126/1000 | Loss: 0.00001157
Iteration 127/1000 | Loss: 0.00001157
Iteration 128/1000 | Loss: 0.00001157
Iteration 129/1000 | Loss: 0.00001157
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001156
Iteration 132/1000 | Loss: 0.00001156
Iteration 133/1000 | Loss: 0.00001156
Iteration 134/1000 | Loss: 0.00001156
Iteration 135/1000 | Loss: 0.00001156
Iteration 136/1000 | Loss: 0.00001156
Iteration 137/1000 | Loss: 0.00001155
Iteration 138/1000 | Loss: 0.00001155
Iteration 139/1000 | Loss: 0.00001155
Iteration 140/1000 | Loss: 0.00001155
Iteration 141/1000 | Loss: 0.00001155
Iteration 142/1000 | Loss: 0.00001155
Iteration 143/1000 | Loss: 0.00001155
Iteration 144/1000 | Loss: 0.00001155
Iteration 145/1000 | Loss: 0.00001155
Iteration 146/1000 | Loss: 0.00001154
Iteration 147/1000 | Loss: 0.00001154
Iteration 148/1000 | Loss: 0.00001154
Iteration 149/1000 | Loss: 0.00001154
Iteration 150/1000 | Loss: 0.00001154
Iteration 151/1000 | Loss: 0.00001154
Iteration 152/1000 | Loss: 0.00001154
Iteration 153/1000 | Loss: 0.00001154
Iteration 154/1000 | Loss: 0.00001154
Iteration 155/1000 | Loss: 0.00001154
Iteration 156/1000 | Loss: 0.00001154
Iteration 157/1000 | Loss: 0.00001154
Iteration 158/1000 | Loss: 0.00001154
Iteration 159/1000 | Loss: 0.00001154
Iteration 160/1000 | Loss: 0.00001154
Iteration 161/1000 | Loss: 0.00001154
Iteration 162/1000 | Loss: 0.00001154
Iteration 163/1000 | Loss: 0.00001154
Iteration 164/1000 | Loss: 0.00001153
Iteration 165/1000 | Loss: 0.00001153
Iteration 166/1000 | Loss: 0.00001153
Iteration 167/1000 | Loss: 0.00001153
Iteration 168/1000 | Loss: 0.00001153
Iteration 169/1000 | Loss: 0.00001153
Iteration 170/1000 | Loss: 0.00001153
Iteration 171/1000 | Loss: 0.00001153
Iteration 172/1000 | Loss: 0.00001153
Iteration 173/1000 | Loss: 0.00001153
Iteration 174/1000 | Loss: 0.00001153
Iteration 175/1000 | Loss: 0.00001153
Iteration 176/1000 | Loss: 0.00001153
Iteration 177/1000 | Loss: 0.00001153
Iteration 178/1000 | Loss: 0.00001153
Iteration 179/1000 | Loss: 0.00001153
Iteration 180/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [1.152839649876114e-05, 1.152839649876114e-05, 1.152839649876114e-05, 1.152839649876114e-05, 1.152839649876114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.152839649876114e-05

Optimization complete. Final v2v error: 2.903557538986206 mm

Highest mean error: 3.4066333770751953 mm for frame 62

Lowest mean error: 2.724281072616577 mm for frame 85

Saving results

Total time: 36.58273267745972
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746233
Iteration 2/25 | Loss: 0.00177992
Iteration 3/25 | Loss: 0.00131341
Iteration 4/25 | Loss: 0.00125986
Iteration 5/25 | Loss: 0.00124495
Iteration 6/25 | Loss: 0.00122825
Iteration 7/25 | Loss: 0.00121851
Iteration 8/25 | Loss: 0.00121050
Iteration 9/25 | Loss: 0.00120784
Iteration 10/25 | Loss: 0.00120559
Iteration 11/25 | Loss: 0.00120475
Iteration 12/25 | Loss: 0.00120454
Iteration 13/25 | Loss: 0.00120451
Iteration 14/25 | Loss: 0.00120451
Iteration 15/25 | Loss: 0.00120451
Iteration 16/25 | Loss: 0.00120451
Iteration 17/25 | Loss: 0.00120451
Iteration 18/25 | Loss: 0.00120450
Iteration 19/25 | Loss: 0.00120450
Iteration 20/25 | Loss: 0.00120450
Iteration 21/25 | Loss: 0.00120450
Iteration 22/25 | Loss: 0.00120450
Iteration 23/25 | Loss: 0.00120450
Iteration 24/25 | Loss: 0.00120449
Iteration 25/25 | Loss: 0.00120449

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37565351
Iteration 2/25 | Loss: 0.00049902
Iteration 3/25 | Loss: 0.00049899
Iteration 4/25 | Loss: 0.00049899
Iteration 5/25 | Loss: 0.00049899
Iteration 6/25 | Loss: 0.00049899
Iteration 7/25 | Loss: 0.00049899
Iteration 8/25 | Loss: 0.00049899
Iteration 9/25 | Loss: 0.00049899
Iteration 10/25 | Loss: 0.00049899
Iteration 11/25 | Loss: 0.00049898
Iteration 12/25 | Loss: 0.00049898
Iteration 13/25 | Loss: 0.00049898
Iteration 14/25 | Loss: 0.00049898
Iteration 15/25 | Loss: 0.00049898
Iteration 16/25 | Loss: 0.00049898
Iteration 17/25 | Loss: 0.00049898
Iteration 18/25 | Loss: 0.00049898
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000498984765727073, 0.000498984765727073, 0.000498984765727073, 0.000498984765727073, 0.000498984765727073]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000498984765727073

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00049898
Iteration 2/1000 | Loss: 0.00004415
Iteration 3/1000 | Loss: 0.00002954
Iteration 4/1000 | Loss: 0.00002646
Iteration 5/1000 | Loss: 0.00002514
Iteration 6/1000 | Loss: 0.00002420
Iteration 7/1000 | Loss: 0.00002345
Iteration 8/1000 | Loss: 0.00002298
Iteration 9/1000 | Loss: 0.00002251
Iteration 10/1000 | Loss: 0.00002210
Iteration 11/1000 | Loss: 0.00002187
Iteration 12/1000 | Loss: 0.00002167
Iteration 13/1000 | Loss: 0.00002163
Iteration 14/1000 | Loss: 0.00002157
Iteration 15/1000 | Loss: 0.00002146
Iteration 16/1000 | Loss: 0.00002146
Iteration 17/1000 | Loss: 0.00002142
Iteration 18/1000 | Loss: 0.00002141
Iteration 19/1000 | Loss: 0.00002138
Iteration 20/1000 | Loss: 0.00002129
Iteration 21/1000 | Loss: 0.00002128
Iteration 22/1000 | Loss: 0.00002128
Iteration 23/1000 | Loss: 0.00002128
Iteration 24/1000 | Loss: 0.00002128
Iteration 25/1000 | Loss: 0.00002128
Iteration 26/1000 | Loss: 0.00002128
Iteration 27/1000 | Loss: 0.00002128
Iteration 28/1000 | Loss: 0.00002127
Iteration 29/1000 | Loss: 0.00002127
Iteration 30/1000 | Loss: 0.00002127
Iteration 31/1000 | Loss: 0.00002127
Iteration 32/1000 | Loss: 0.00002127
Iteration 33/1000 | Loss: 0.00002127
Iteration 34/1000 | Loss: 0.00002127
Iteration 35/1000 | Loss: 0.00002126
Iteration 36/1000 | Loss: 0.00002125
Iteration 37/1000 | Loss: 0.00002124
Iteration 38/1000 | Loss: 0.00002124
Iteration 39/1000 | Loss: 0.00002124
Iteration 40/1000 | Loss: 0.00002124
Iteration 41/1000 | Loss: 0.00002123
Iteration 42/1000 | Loss: 0.00002123
Iteration 43/1000 | Loss: 0.00002123
Iteration 44/1000 | Loss: 0.00002122
Iteration 45/1000 | Loss: 0.00002122
Iteration 46/1000 | Loss: 0.00002122
Iteration 47/1000 | Loss: 0.00002122
Iteration 48/1000 | Loss: 0.00002122
Iteration 49/1000 | Loss: 0.00002121
Iteration 50/1000 | Loss: 0.00002121
Iteration 51/1000 | Loss: 0.00002121
Iteration 52/1000 | Loss: 0.00002121
Iteration 53/1000 | Loss: 0.00002121
Iteration 54/1000 | Loss: 0.00002121
Iteration 55/1000 | Loss: 0.00002121
Iteration 56/1000 | Loss: 0.00002121
Iteration 57/1000 | Loss: 0.00002120
Iteration 58/1000 | Loss: 0.00002120
Iteration 59/1000 | Loss: 0.00002120
Iteration 60/1000 | Loss: 0.00002120
Iteration 61/1000 | Loss: 0.00002120
Iteration 62/1000 | Loss: 0.00002120
Iteration 63/1000 | Loss: 0.00002119
Iteration 64/1000 | Loss: 0.00002119
Iteration 65/1000 | Loss: 0.00002119
Iteration 66/1000 | Loss: 0.00002119
Iteration 67/1000 | Loss: 0.00002119
Iteration 68/1000 | Loss: 0.00002119
Iteration 69/1000 | Loss: 0.00002119
Iteration 70/1000 | Loss: 0.00002119
Iteration 71/1000 | Loss: 0.00002119
Iteration 72/1000 | Loss: 0.00002118
Iteration 73/1000 | Loss: 0.00002118
Iteration 74/1000 | Loss: 0.00002118
Iteration 75/1000 | Loss: 0.00002118
Iteration 76/1000 | Loss: 0.00002118
Iteration 77/1000 | Loss: 0.00002117
Iteration 78/1000 | Loss: 0.00002117
Iteration 79/1000 | Loss: 0.00002117
Iteration 80/1000 | Loss: 0.00002117
Iteration 81/1000 | Loss: 0.00002117
Iteration 82/1000 | Loss: 0.00002117
Iteration 83/1000 | Loss: 0.00002117
Iteration 84/1000 | Loss: 0.00002117
Iteration 85/1000 | Loss: 0.00002117
Iteration 86/1000 | Loss: 0.00002116
Iteration 87/1000 | Loss: 0.00002116
Iteration 88/1000 | Loss: 0.00002116
Iteration 89/1000 | Loss: 0.00002116
Iteration 90/1000 | Loss: 0.00002115
Iteration 91/1000 | Loss: 0.00002115
Iteration 92/1000 | Loss: 0.00002115
Iteration 93/1000 | Loss: 0.00002115
Iteration 94/1000 | Loss: 0.00002114
Iteration 95/1000 | Loss: 0.00002114
Iteration 96/1000 | Loss: 0.00002114
Iteration 97/1000 | Loss: 0.00002114
Iteration 98/1000 | Loss: 0.00002114
Iteration 99/1000 | Loss: 0.00002114
Iteration 100/1000 | Loss: 0.00002114
Iteration 101/1000 | Loss: 0.00002114
Iteration 102/1000 | Loss: 0.00002114
Iteration 103/1000 | Loss: 0.00002113
Iteration 104/1000 | Loss: 0.00002113
Iteration 105/1000 | Loss: 0.00002113
Iteration 106/1000 | Loss: 0.00002112
Iteration 107/1000 | Loss: 0.00002112
Iteration 108/1000 | Loss: 0.00002112
Iteration 109/1000 | Loss: 0.00002112
Iteration 110/1000 | Loss: 0.00002112
Iteration 111/1000 | Loss: 0.00002112
Iteration 112/1000 | Loss: 0.00002111
Iteration 113/1000 | Loss: 0.00002111
Iteration 114/1000 | Loss: 0.00002111
Iteration 115/1000 | Loss: 0.00002111
Iteration 116/1000 | Loss: 0.00002111
Iteration 117/1000 | Loss: 0.00002111
Iteration 118/1000 | Loss: 0.00002111
Iteration 119/1000 | Loss: 0.00002111
Iteration 120/1000 | Loss: 0.00002111
Iteration 121/1000 | Loss: 0.00002111
Iteration 122/1000 | Loss: 0.00002111
Iteration 123/1000 | Loss: 0.00002110
Iteration 124/1000 | Loss: 0.00002110
Iteration 125/1000 | Loss: 0.00002110
Iteration 126/1000 | Loss: 0.00002110
Iteration 127/1000 | Loss: 0.00002110
Iteration 128/1000 | Loss: 0.00002110
Iteration 129/1000 | Loss: 0.00002110
Iteration 130/1000 | Loss: 0.00002109
Iteration 131/1000 | Loss: 0.00002109
Iteration 132/1000 | Loss: 0.00002109
Iteration 133/1000 | Loss: 0.00002109
Iteration 134/1000 | Loss: 0.00002109
Iteration 135/1000 | Loss: 0.00002109
Iteration 136/1000 | Loss: 0.00002108
Iteration 137/1000 | Loss: 0.00002108
Iteration 138/1000 | Loss: 0.00002108
Iteration 139/1000 | Loss: 0.00002108
Iteration 140/1000 | Loss: 0.00002108
Iteration 141/1000 | Loss: 0.00002108
Iteration 142/1000 | Loss: 0.00002108
Iteration 143/1000 | Loss: 0.00002108
Iteration 144/1000 | Loss: 0.00002108
Iteration 145/1000 | Loss: 0.00002107
Iteration 146/1000 | Loss: 0.00002107
Iteration 147/1000 | Loss: 0.00002107
Iteration 148/1000 | Loss: 0.00002107
Iteration 149/1000 | Loss: 0.00002107
Iteration 150/1000 | Loss: 0.00002107
Iteration 151/1000 | Loss: 0.00002107
Iteration 152/1000 | Loss: 0.00002107
Iteration 153/1000 | Loss: 0.00002107
Iteration 154/1000 | Loss: 0.00002107
Iteration 155/1000 | Loss: 0.00002107
Iteration 156/1000 | Loss: 0.00002107
Iteration 157/1000 | Loss: 0.00002106
Iteration 158/1000 | Loss: 0.00002106
Iteration 159/1000 | Loss: 0.00002106
Iteration 160/1000 | Loss: 0.00002106
Iteration 161/1000 | Loss: 0.00002106
Iteration 162/1000 | Loss: 0.00002106
Iteration 163/1000 | Loss: 0.00002106
Iteration 164/1000 | Loss: 0.00002106
Iteration 165/1000 | Loss: 0.00002106
Iteration 166/1000 | Loss: 0.00002106
Iteration 167/1000 | Loss: 0.00002106
Iteration 168/1000 | Loss: 0.00002106
Iteration 169/1000 | Loss: 0.00002106
Iteration 170/1000 | Loss: 0.00002105
Iteration 171/1000 | Loss: 0.00002105
Iteration 172/1000 | Loss: 0.00002105
Iteration 173/1000 | Loss: 0.00002105
Iteration 174/1000 | Loss: 0.00002105
Iteration 175/1000 | Loss: 0.00002105
Iteration 176/1000 | Loss: 0.00002105
Iteration 177/1000 | Loss: 0.00002105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [2.1054045646451414e-05, 2.1054045646451414e-05, 2.1054045646451414e-05, 2.1054045646451414e-05, 2.1054045646451414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1054045646451414e-05

Optimization complete. Final v2v error: 3.856172800064087 mm

Highest mean error: 4.8738555908203125 mm for frame 181

Lowest mean error: 3.4135847091674805 mm for frame 214

Saving results

Total time: 59.62363576889038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838373
Iteration 2/25 | Loss: 0.00128405
Iteration 3/25 | Loss: 0.00118157
Iteration 4/25 | Loss: 0.00117079
Iteration 5/25 | Loss: 0.00116734
Iteration 6/25 | Loss: 0.00116675
Iteration 7/25 | Loss: 0.00116675
Iteration 8/25 | Loss: 0.00116675
Iteration 9/25 | Loss: 0.00116675
Iteration 10/25 | Loss: 0.00116675
Iteration 11/25 | Loss: 0.00116675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001166750444099307, 0.001166750444099307, 0.001166750444099307, 0.001166750444099307, 0.001166750444099307]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001166750444099307

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37461543
Iteration 2/25 | Loss: 0.00071438
Iteration 3/25 | Loss: 0.00071438
Iteration 4/25 | Loss: 0.00071438
Iteration 5/25 | Loss: 0.00071438
Iteration 6/25 | Loss: 0.00071438
Iteration 7/25 | Loss: 0.00071438
Iteration 8/25 | Loss: 0.00071438
Iteration 9/25 | Loss: 0.00071438
Iteration 10/25 | Loss: 0.00071438
Iteration 11/25 | Loss: 0.00071438
Iteration 12/25 | Loss: 0.00071438
Iteration 13/25 | Loss: 0.00071438
Iteration 14/25 | Loss: 0.00071438
Iteration 15/25 | Loss: 0.00071438
Iteration 16/25 | Loss: 0.00071438
Iteration 17/25 | Loss: 0.00071438
Iteration 18/25 | Loss: 0.00071438
Iteration 19/25 | Loss: 0.00071438
Iteration 20/25 | Loss: 0.00071438
Iteration 21/25 | Loss: 0.00071438
Iteration 22/25 | Loss: 0.00071438
Iteration 23/25 | Loss: 0.00071438
Iteration 24/25 | Loss: 0.00071438
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0007143777329474688, 0.0007143777329474688, 0.0007143777329474688, 0.0007143777329474688, 0.0007143777329474688]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007143777329474688

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00071438
Iteration 2/1000 | Loss: 0.00002873
Iteration 3/1000 | Loss: 0.00002269
Iteration 4/1000 | Loss: 0.00002112
Iteration 5/1000 | Loss: 0.00002036
Iteration 6/1000 | Loss: 0.00001987
Iteration 7/1000 | Loss: 0.00001949
Iteration 8/1000 | Loss: 0.00001931
Iteration 9/1000 | Loss: 0.00001906
Iteration 10/1000 | Loss: 0.00001886
Iteration 11/1000 | Loss: 0.00001874
Iteration 12/1000 | Loss: 0.00001869
Iteration 13/1000 | Loss: 0.00001860
Iteration 14/1000 | Loss: 0.00001860
Iteration 15/1000 | Loss: 0.00001859
Iteration 16/1000 | Loss: 0.00001858
Iteration 17/1000 | Loss: 0.00001856
Iteration 18/1000 | Loss: 0.00001856
Iteration 19/1000 | Loss: 0.00001855
Iteration 20/1000 | Loss: 0.00001855
Iteration 21/1000 | Loss: 0.00001855
Iteration 22/1000 | Loss: 0.00001853
Iteration 23/1000 | Loss: 0.00001853
Iteration 24/1000 | Loss: 0.00001853
Iteration 25/1000 | Loss: 0.00001853
Iteration 26/1000 | Loss: 0.00001853
Iteration 27/1000 | Loss: 0.00001853
Iteration 28/1000 | Loss: 0.00001852
Iteration 29/1000 | Loss: 0.00001852
Iteration 30/1000 | Loss: 0.00001852
Iteration 31/1000 | Loss: 0.00001852
Iteration 32/1000 | Loss: 0.00001852
Iteration 33/1000 | Loss: 0.00001852
Iteration 34/1000 | Loss: 0.00001852
Iteration 35/1000 | Loss: 0.00001851
Iteration 36/1000 | Loss: 0.00001851
Iteration 37/1000 | Loss: 0.00001851
Iteration 38/1000 | Loss: 0.00001851
Iteration 39/1000 | Loss: 0.00001851
Iteration 40/1000 | Loss: 0.00001850
Iteration 41/1000 | Loss: 0.00001850
Iteration 42/1000 | Loss: 0.00001850
Iteration 43/1000 | Loss: 0.00001849
Iteration 44/1000 | Loss: 0.00001849
Iteration 45/1000 | Loss: 0.00001849
Iteration 46/1000 | Loss: 0.00001848
Iteration 47/1000 | Loss: 0.00001848
Iteration 48/1000 | Loss: 0.00001848
Iteration 49/1000 | Loss: 0.00001847
Iteration 50/1000 | Loss: 0.00001847
Iteration 51/1000 | Loss: 0.00001846
Iteration 52/1000 | Loss: 0.00001845
Iteration 53/1000 | Loss: 0.00001845
Iteration 54/1000 | Loss: 0.00001845
Iteration 55/1000 | Loss: 0.00001845
Iteration 56/1000 | Loss: 0.00001844
Iteration 57/1000 | Loss: 0.00001844
Iteration 58/1000 | Loss: 0.00001844
Iteration 59/1000 | Loss: 0.00001843
Iteration 60/1000 | Loss: 0.00001843
Iteration 61/1000 | Loss: 0.00001843
Iteration 62/1000 | Loss: 0.00001843
Iteration 63/1000 | Loss: 0.00001843
Iteration 64/1000 | Loss: 0.00001843
Iteration 65/1000 | Loss: 0.00001843
Iteration 66/1000 | Loss: 0.00001843
Iteration 67/1000 | Loss: 0.00001842
Iteration 68/1000 | Loss: 0.00001842
Iteration 69/1000 | Loss: 0.00001842
Iteration 70/1000 | Loss: 0.00001842
Iteration 71/1000 | Loss: 0.00001842
Iteration 72/1000 | Loss: 0.00001842
Iteration 73/1000 | Loss: 0.00001842
Iteration 74/1000 | Loss: 0.00001841
Iteration 75/1000 | Loss: 0.00001841
Iteration 76/1000 | Loss: 0.00001841
Iteration 77/1000 | Loss: 0.00001841
Iteration 78/1000 | Loss: 0.00001841
Iteration 79/1000 | Loss: 0.00001841
Iteration 80/1000 | Loss: 0.00001840
Iteration 81/1000 | Loss: 0.00001840
Iteration 82/1000 | Loss: 0.00001840
Iteration 83/1000 | Loss: 0.00001840
Iteration 84/1000 | Loss: 0.00001840
Iteration 85/1000 | Loss: 0.00001840
Iteration 86/1000 | Loss: 0.00001840
Iteration 87/1000 | Loss: 0.00001840
Iteration 88/1000 | Loss: 0.00001839
Iteration 89/1000 | Loss: 0.00001839
Iteration 90/1000 | Loss: 0.00001839
Iteration 91/1000 | Loss: 0.00001839
Iteration 92/1000 | Loss: 0.00001839
Iteration 93/1000 | Loss: 0.00001839
Iteration 94/1000 | Loss: 0.00001838
Iteration 95/1000 | Loss: 0.00001838
Iteration 96/1000 | Loss: 0.00001838
Iteration 97/1000 | Loss: 0.00001838
Iteration 98/1000 | Loss: 0.00001838
Iteration 99/1000 | Loss: 0.00001838
Iteration 100/1000 | Loss: 0.00001837
Iteration 101/1000 | Loss: 0.00001837
Iteration 102/1000 | Loss: 0.00001837
Iteration 103/1000 | Loss: 0.00001837
Iteration 104/1000 | Loss: 0.00001837
Iteration 105/1000 | Loss: 0.00001836
Iteration 106/1000 | Loss: 0.00001835
Iteration 107/1000 | Loss: 0.00001835
Iteration 108/1000 | Loss: 0.00001835
Iteration 109/1000 | Loss: 0.00001835
Iteration 110/1000 | Loss: 0.00001835
Iteration 111/1000 | Loss: 0.00001835
Iteration 112/1000 | Loss: 0.00001835
Iteration 113/1000 | Loss: 0.00001835
Iteration 114/1000 | Loss: 0.00001835
Iteration 115/1000 | Loss: 0.00001834
Iteration 116/1000 | Loss: 0.00001834
Iteration 117/1000 | Loss: 0.00001833
Iteration 118/1000 | Loss: 0.00001833
Iteration 119/1000 | Loss: 0.00001833
Iteration 120/1000 | Loss: 0.00001832
Iteration 121/1000 | Loss: 0.00001832
Iteration 122/1000 | Loss: 0.00001832
Iteration 123/1000 | Loss: 0.00001832
Iteration 124/1000 | Loss: 0.00001832
Iteration 125/1000 | Loss: 0.00001832
Iteration 126/1000 | Loss: 0.00001832
Iteration 127/1000 | Loss: 0.00001832
Iteration 128/1000 | Loss: 0.00001831
Iteration 129/1000 | Loss: 0.00001831
Iteration 130/1000 | Loss: 0.00001831
Iteration 131/1000 | Loss: 0.00001830
Iteration 132/1000 | Loss: 0.00001830
Iteration 133/1000 | Loss: 0.00001830
Iteration 134/1000 | Loss: 0.00001830
Iteration 135/1000 | Loss: 0.00001830
Iteration 136/1000 | Loss: 0.00001829
Iteration 137/1000 | Loss: 0.00001829
Iteration 138/1000 | Loss: 0.00001829
Iteration 139/1000 | Loss: 0.00001829
Iteration 140/1000 | Loss: 0.00001829
Iteration 141/1000 | Loss: 0.00001829
Iteration 142/1000 | Loss: 0.00001829
Iteration 143/1000 | Loss: 0.00001829
Iteration 144/1000 | Loss: 0.00001829
Iteration 145/1000 | Loss: 0.00001828
Iteration 146/1000 | Loss: 0.00001828
Iteration 147/1000 | Loss: 0.00001828
Iteration 148/1000 | Loss: 0.00001828
Iteration 149/1000 | Loss: 0.00001827
Iteration 150/1000 | Loss: 0.00001827
Iteration 151/1000 | Loss: 0.00001827
Iteration 152/1000 | Loss: 0.00001827
Iteration 153/1000 | Loss: 0.00001827
Iteration 154/1000 | Loss: 0.00001826
Iteration 155/1000 | Loss: 0.00001826
Iteration 156/1000 | Loss: 0.00001826
Iteration 157/1000 | Loss: 0.00001825
Iteration 158/1000 | Loss: 0.00001825
Iteration 159/1000 | Loss: 0.00001825
Iteration 160/1000 | Loss: 0.00001824
Iteration 161/1000 | Loss: 0.00001824
Iteration 162/1000 | Loss: 0.00001824
Iteration 163/1000 | Loss: 0.00001824
Iteration 164/1000 | Loss: 0.00001824
Iteration 165/1000 | Loss: 0.00001824
Iteration 166/1000 | Loss: 0.00001823
Iteration 167/1000 | Loss: 0.00001823
Iteration 168/1000 | Loss: 0.00001823
Iteration 169/1000 | Loss: 0.00001823
Iteration 170/1000 | Loss: 0.00001823
Iteration 171/1000 | Loss: 0.00001823
Iteration 172/1000 | Loss: 0.00001823
Iteration 173/1000 | Loss: 0.00001823
Iteration 174/1000 | Loss: 0.00001823
Iteration 175/1000 | Loss: 0.00001823
Iteration 176/1000 | Loss: 0.00001823
Iteration 177/1000 | Loss: 0.00001823
Iteration 178/1000 | Loss: 0.00001822
Iteration 179/1000 | Loss: 0.00001822
Iteration 180/1000 | Loss: 0.00001822
Iteration 181/1000 | Loss: 0.00001822
Iteration 182/1000 | Loss: 0.00001822
Iteration 183/1000 | Loss: 0.00001822
Iteration 184/1000 | Loss: 0.00001822
Iteration 185/1000 | Loss: 0.00001822
Iteration 186/1000 | Loss: 0.00001822
Iteration 187/1000 | Loss: 0.00001822
Iteration 188/1000 | Loss: 0.00001822
Iteration 189/1000 | Loss: 0.00001822
Iteration 190/1000 | Loss: 0.00001822
Iteration 191/1000 | Loss: 0.00001822
Iteration 192/1000 | Loss: 0.00001822
Iteration 193/1000 | Loss: 0.00001822
Iteration 194/1000 | Loss: 0.00001822
Iteration 195/1000 | Loss: 0.00001822
Iteration 196/1000 | Loss: 0.00001822
Iteration 197/1000 | Loss: 0.00001822
Iteration 198/1000 | Loss: 0.00001822
Iteration 199/1000 | Loss: 0.00001822
Iteration 200/1000 | Loss: 0.00001822
Iteration 201/1000 | Loss: 0.00001822
Iteration 202/1000 | Loss: 0.00001822
Iteration 203/1000 | Loss: 0.00001822
Iteration 204/1000 | Loss: 0.00001822
Iteration 205/1000 | Loss: 0.00001822
Iteration 206/1000 | Loss: 0.00001822
Iteration 207/1000 | Loss: 0.00001822
Iteration 208/1000 | Loss: 0.00001822
Iteration 209/1000 | Loss: 0.00001822
Iteration 210/1000 | Loss: 0.00001822
Iteration 211/1000 | Loss: 0.00001822
Iteration 212/1000 | Loss: 0.00001822
Iteration 213/1000 | Loss: 0.00001822
Iteration 214/1000 | Loss: 0.00001822
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 214. Stopping optimization.
Last 5 losses: [1.821539808588568e-05, 1.821539808588568e-05, 1.821539808588568e-05, 1.821539808588568e-05, 1.821539808588568e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.821539808588568e-05

Optimization complete. Final v2v error: 3.566542863845825 mm

Highest mean error: 3.7970967292785645 mm for frame 82

Lowest mean error: 3.2277276515960693 mm for frame 21

Saving results

Total time: 37.150267124176025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00820504
Iteration 2/25 | Loss: 0.00117123
Iteration 3/25 | Loss: 0.00106818
Iteration 4/25 | Loss: 0.00105530
Iteration 5/25 | Loss: 0.00105234
Iteration 6/25 | Loss: 0.00105234
Iteration 7/25 | Loss: 0.00105234
Iteration 8/25 | Loss: 0.00105234
Iteration 9/25 | Loss: 0.00105234
Iteration 10/25 | Loss: 0.00105234
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001052342471666634, 0.001052342471666634, 0.001052342471666634, 0.001052342471666634, 0.001052342471666634]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001052342471666634

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39554524
Iteration 2/25 | Loss: 0.00052917
Iteration 3/25 | Loss: 0.00052917
Iteration 4/25 | Loss: 0.00052917
Iteration 5/25 | Loss: 0.00052917
Iteration 6/25 | Loss: 0.00052917
Iteration 7/25 | Loss: 0.00052917
Iteration 8/25 | Loss: 0.00052916
Iteration 9/25 | Loss: 0.00052916
Iteration 10/25 | Loss: 0.00052916
Iteration 11/25 | Loss: 0.00052916
Iteration 12/25 | Loss: 0.00052916
Iteration 13/25 | Loss: 0.00052916
Iteration 14/25 | Loss: 0.00052916
Iteration 15/25 | Loss: 0.00052916
Iteration 16/25 | Loss: 0.00052916
Iteration 17/25 | Loss: 0.00052916
Iteration 18/25 | Loss: 0.00052916
Iteration 19/25 | Loss: 0.00052916
Iteration 20/25 | Loss: 0.00052916
Iteration 21/25 | Loss: 0.00052916
Iteration 22/25 | Loss: 0.00052916
Iteration 23/25 | Loss: 0.00052916
Iteration 24/25 | Loss: 0.00052916
Iteration 25/25 | Loss: 0.00052916

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052916
Iteration 2/1000 | Loss: 0.00001883
Iteration 3/1000 | Loss: 0.00001357
Iteration 4/1000 | Loss: 0.00001230
Iteration 5/1000 | Loss: 0.00001142
Iteration 6/1000 | Loss: 0.00001074
Iteration 7/1000 | Loss: 0.00001041
Iteration 8/1000 | Loss: 0.00001034
Iteration 9/1000 | Loss: 0.00001017
Iteration 10/1000 | Loss: 0.00000993
Iteration 11/1000 | Loss: 0.00000988
Iteration 12/1000 | Loss: 0.00000988
Iteration 13/1000 | Loss: 0.00000982
Iteration 14/1000 | Loss: 0.00000981
Iteration 15/1000 | Loss: 0.00000978
Iteration 16/1000 | Loss: 0.00000977
Iteration 17/1000 | Loss: 0.00000975
Iteration 18/1000 | Loss: 0.00000975
Iteration 19/1000 | Loss: 0.00000975
Iteration 20/1000 | Loss: 0.00000975
Iteration 21/1000 | Loss: 0.00000974
Iteration 22/1000 | Loss: 0.00000974
Iteration 23/1000 | Loss: 0.00000974
Iteration 24/1000 | Loss: 0.00000974
Iteration 25/1000 | Loss: 0.00000974
Iteration 26/1000 | Loss: 0.00000973
Iteration 27/1000 | Loss: 0.00000973
Iteration 28/1000 | Loss: 0.00000972
Iteration 29/1000 | Loss: 0.00000969
Iteration 30/1000 | Loss: 0.00000969
Iteration 31/1000 | Loss: 0.00000968
Iteration 32/1000 | Loss: 0.00000968
Iteration 33/1000 | Loss: 0.00000966
Iteration 34/1000 | Loss: 0.00000966
Iteration 35/1000 | Loss: 0.00000966
Iteration 36/1000 | Loss: 0.00000965
Iteration 37/1000 | Loss: 0.00000965
Iteration 38/1000 | Loss: 0.00000965
Iteration 39/1000 | Loss: 0.00000965
Iteration 40/1000 | Loss: 0.00000965
Iteration 41/1000 | Loss: 0.00000965
Iteration 42/1000 | Loss: 0.00000965
Iteration 43/1000 | Loss: 0.00000964
Iteration 44/1000 | Loss: 0.00000964
Iteration 45/1000 | Loss: 0.00000964
Iteration 46/1000 | Loss: 0.00000963
Iteration 47/1000 | Loss: 0.00000963
Iteration 48/1000 | Loss: 0.00000962
Iteration 49/1000 | Loss: 0.00000961
Iteration 50/1000 | Loss: 0.00000961
Iteration 51/1000 | Loss: 0.00000961
Iteration 52/1000 | Loss: 0.00000961
Iteration 53/1000 | Loss: 0.00000960
Iteration 54/1000 | Loss: 0.00000960
Iteration 55/1000 | Loss: 0.00000960
Iteration 56/1000 | Loss: 0.00000960
Iteration 57/1000 | Loss: 0.00000959
Iteration 58/1000 | Loss: 0.00000959
Iteration 59/1000 | Loss: 0.00000959
Iteration 60/1000 | Loss: 0.00000959
Iteration 61/1000 | Loss: 0.00000959
Iteration 62/1000 | Loss: 0.00000958
Iteration 63/1000 | Loss: 0.00000958
Iteration 64/1000 | Loss: 0.00000958
Iteration 65/1000 | Loss: 0.00000957
Iteration 66/1000 | Loss: 0.00000957
Iteration 67/1000 | Loss: 0.00000957
Iteration 68/1000 | Loss: 0.00000956
Iteration 69/1000 | Loss: 0.00000956
Iteration 70/1000 | Loss: 0.00000956
Iteration 71/1000 | Loss: 0.00000956
Iteration 72/1000 | Loss: 0.00000956
Iteration 73/1000 | Loss: 0.00000955
Iteration 74/1000 | Loss: 0.00000955
Iteration 75/1000 | Loss: 0.00000954
Iteration 76/1000 | Loss: 0.00000954
Iteration 77/1000 | Loss: 0.00000953
Iteration 78/1000 | Loss: 0.00000953
Iteration 79/1000 | Loss: 0.00000952
Iteration 80/1000 | Loss: 0.00000952
Iteration 81/1000 | Loss: 0.00000952
Iteration 82/1000 | Loss: 0.00000950
Iteration 83/1000 | Loss: 0.00000950
Iteration 84/1000 | Loss: 0.00000949
Iteration 85/1000 | Loss: 0.00000947
Iteration 86/1000 | Loss: 0.00000946
Iteration 87/1000 | Loss: 0.00000946
Iteration 88/1000 | Loss: 0.00000945
Iteration 89/1000 | Loss: 0.00000944
Iteration 90/1000 | Loss: 0.00000944
Iteration 91/1000 | Loss: 0.00000943
Iteration 92/1000 | Loss: 0.00000943
Iteration 93/1000 | Loss: 0.00000943
Iteration 94/1000 | Loss: 0.00000943
Iteration 95/1000 | Loss: 0.00000942
Iteration 96/1000 | Loss: 0.00000942
Iteration 97/1000 | Loss: 0.00000942
Iteration 98/1000 | Loss: 0.00000942
Iteration 99/1000 | Loss: 0.00000942
Iteration 100/1000 | Loss: 0.00000941
Iteration 101/1000 | Loss: 0.00000941
Iteration 102/1000 | Loss: 0.00000939
Iteration 103/1000 | Loss: 0.00000939
Iteration 104/1000 | Loss: 0.00000939
Iteration 105/1000 | Loss: 0.00000939
Iteration 106/1000 | Loss: 0.00000939
Iteration 107/1000 | Loss: 0.00000939
Iteration 108/1000 | Loss: 0.00000938
Iteration 109/1000 | Loss: 0.00000938
Iteration 110/1000 | Loss: 0.00000938
Iteration 111/1000 | Loss: 0.00000938
Iteration 112/1000 | Loss: 0.00000938
Iteration 113/1000 | Loss: 0.00000938
Iteration 114/1000 | Loss: 0.00000938
Iteration 115/1000 | Loss: 0.00000937
Iteration 116/1000 | Loss: 0.00000937
Iteration 117/1000 | Loss: 0.00000937
Iteration 118/1000 | Loss: 0.00000936
Iteration 119/1000 | Loss: 0.00000935
Iteration 120/1000 | Loss: 0.00000935
Iteration 121/1000 | Loss: 0.00000935
Iteration 122/1000 | Loss: 0.00000935
Iteration 123/1000 | Loss: 0.00000935
Iteration 124/1000 | Loss: 0.00000934
Iteration 125/1000 | Loss: 0.00000934
Iteration 126/1000 | Loss: 0.00000934
Iteration 127/1000 | Loss: 0.00000934
Iteration 128/1000 | Loss: 0.00000934
Iteration 129/1000 | Loss: 0.00000934
Iteration 130/1000 | Loss: 0.00000933
Iteration 131/1000 | Loss: 0.00000932
Iteration 132/1000 | Loss: 0.00000931
Iteration 133/1000 | Loss: 0.00000931
Iteration 134/1000 | Loss: 0.00000931
Iteration 135/1000 | Loss: 0.00000931
Iteration 136/1000 | Loss: 0.00000931
Iteration 137/1000 | Loss: 0.00000931
Iteration 138/1000 | Loss: 0.00000931
Iteration 139/1000 | Loss: 0.00000931
Iteration 140/1000 | Loss: 0.00000931
Iteration 141/1000 | Loss: 0.00000931
Iteration 142/1000 | Loss: 0.00000930
Iteration 143/1000 | Loss: 0.00000930
Iteration 144/1000 | Loss: 0.00000930
Iteration 145/1000 | Loss: 0.00000930
Iteration 146/1000 | Loss: 0.00000930
Iteration 147/1000 | Loss: 0.00000930
Iteration 148/1000 | Loss: 0.00000930
Iteration 149/1000 | Loss: 0.00000929
Iteration 150/1000 | Loss: 0.00000929
Iteration 151/1000 | Loss: 0.00000929
Iteration 152/1000 | Loss: 0.00000929
Iteration 153/1000 | Loss: 0.00000929
Iteration 154/1000 | Loss: 0.00000929
Iteration 155/1000 | Loss: 0.00000929
Iteration 156/1000 | Loss: 0.00000928
Iteration 157/1000 | Loss: 0.00000928
Iteration 158/1000 | Loss: 0.00000928
Iteration 159/1000 | Loss: 0.00000928
Iteration 160/1000 | Loss: 0.00000928
Iteration 161/1000 | Loss: 0.00000928
Iteration 162/1000 | Loss: 0.00000928
Iteration 163/1000 | Loss: 0.00000928
Iteration 164/1000 | Loss: 0.00000927
Iteration 165/1000 | Loss: 0.00000927
Iteration 166/1000 | Loss: 0.00000927
Iteration 167/1000 | Loss: 0.00000927
Iteration 168/1000 | Loss: 0.00000927
Iteration 169/1000 | Loss: 0.00000927
Iteration 170/1000 | Loss: 0.00000927
Iteration 171/1000 | Loss: 0.00000927
Iteration 172/1000 | Loss: 0.00000927
Iteration 173/1000 | Loss: 0.00000926
Iteration 174/1000 | Loss: 0.00000926
Iteration 175/1000 | Loss: 0.00000926
Iteration 176/1000 | Loss: 0.00000926
Iteration 177/1000 | Loss: 0.00000926
Iteration 178/1000 | Loss: 0.00000926
Iteration 179/1000 | Loss: 0.00000926
Iteration 180/1000 | Loss: 0.00000926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [9.264292202715296e-06, 9.264292202715296e-06, 9.264292202715296e-06, 9.264292202715296e-06, 9.264292202715296e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.264292202715296e-06

Optimization complete. Final v2v error: 2.6171109676361084 mm

Highest mean error: 2.775521755218506 mm for frame 31

Lowest mean error: 2.4430902004241943 mm for frame 13

Saving results

Total time: 41.01220965385437
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00823344
Iteration 2/25 | Loss: 0.00128156
Iteration 3/25 | Loss: 0.00115010
Iteration 4/25 | Loss: 0.00111811
Iteration 5/25 | Loss: 0.00111034
Iteration 6/25 | Loss: 0.00111474
Iteration 7/25 | Loss: 0.00112379
Iteration 8/25 | Loss: 0.00112817
Iteration 9/25 | Loss: 0.00112358
Iteration 10/25 | Loss: 0.00111619
Iteration 11/25 | Loss: 0.00110954
Iteration 12/25 | Loss: 0.00110611
Iteration 13/25 | Loss: 0.00110546
Iteration 14/25 | Loss: 0.00109967
Iteration 15/25 | Loss: 0.00109831
Iteration 16/25 | Loss: 0.00109587
Iteration 17/25 | Loss: 0.00109408
Iteration 18/25 | Loss: 0.00109260
Iteration 19/25 | Loss: 0.00109122
Iteration 20/25 | Loss: 0.00109239
Iteration 21/25 | Loss: 0.00109282
Iteration 22/25 | Loss: 0.00109223
Iteration 23/25 | Loss: 0.00109133
Iteration 24/25 | Loss: 0.00109159
Iteration 25/25 | Loss: 0.00109086

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39802396
Iteration 2/25 | Loss: 0.00083630
Iteration 3/25 | Loss: 0.00083629
Iteration 4/25 | Loss: 0.00083629
Iteration 5/25 | Loss: 0.00083629
Iteration 6/25 | Loss: 0.00083629
Iteration 7/25 | Loss: 0.00083629
Iteration 8/25 | Loss: 0.00083629
Iteration 9/25 | Loss: 0.00083629
Iteration 10/25 | Loss: 0.00083629
Iteration 11/25 | Loss: 0.00083629
Iteration 12/25 | Loss: 0.00083629
Iteration 13/25 | Loss: 0.00083629
Iteration 14/25 | Loss: 0.00083629
Iteration 15/25 | Loss: 0.00083629
Iteration 16/25 | Loss: 0.00083629
Iteration 17/25 | Loss: 0.00083629
Iteration 18/25 | Loss: 0.00083629
Iteration 19/25 | Loss: 0.00083629
Iteration 20/25 | Loss: 0.00083629
Iteration 21/25 | Loss: 0.00083629
Iteration 22/25 | Loss: 0.00083629
Iteration 23/25 | Loss: 0.00083629
Iteration 24/25 | Loss: 0.00083629
Iteration 25/25 | Loss: 0.00083629

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083629
Iteration 2/1000 | Loss: 0.00006086
Iteration 3/1000 | Loss: 0.00005772
Iteration 4/1000 | Loss: 0.00003629
Iteration 5/1000 | Loss: 0.00002889
Iteration 6/1000 | Loss: 0.00002473
Iteration 7/1000 | Loss: 0.00003243
Iteration 8/1000 | Loss: 0.00003093
Iteration 9/1000 | Loss: 0.00003765
Iteration 10/1000 | Loss: 0.00002929
Iteration 11/1000 | Loss: 0.00003266
Iteration 12/1000 | Loss: 0.00002381
Iteration 13/1000 | Loss: 0.00004188
Iteration 14/1000 | Loss: 0.00004436
Iteration 15/1000 | Loss: 0.00006429
Iteration 16/1000 | Loss: 0.00002420
Iteration 17/1000 | Loss: 0.00002256
Iteration 18/1000 | Loss: 0.00002116
Iteration 19/1000 | Loss: 0.00001931
Iteration 20/1000 | Loss: 0.00002995
Iteration 21/1000 | Loss: 0.00016042
Iteration 22/1000 | Loss: 0.00002427
Iteration 23/1000 | Loss: 0.00002058
Iteration 24/1000 | Loss: 0.00001922
Iteration 25/1000 | Loss: 0.00001848
Iteration 26/1000 | Loss: 0.00001818
Iteration 27/1000 | Loss: 0.00001789
Iteration 28/1000 | Loss: 0.00001761
Iteration 29/1000 | Loss: 0.00001759
Iteration 30/1000 | Loss: 0.00001728
Iteration 31/1000 | Loss: 0.00001702
Iteration 32/1000 | Loss: 0.00001678
Iteration 33/1000 | Loss: 0.00001659
Iteration 34/1000 | Loss: 0.00001650
Iteration 35/1000 | Loss: 0.00001650
Iteration 36/1000 | Loss: 0.00001648
Iteration 37/1000 | Loss: 0.00001647
Iteration 38/1000 | Loss: 0.00001646
Iteration 39/1000 | Loss: 0.00001645
Iteration 40/1000 | Loss: 0.00001643
Iteration 41/1000 | Loss: 0.00001641
Iteration 42/1000 | Loss: 0.00001641
Iteration 43/1000 | Loss: 0.00001640
Iteration 44/1000 | Loss: 0.00001639
Iteration 45/1000 | Loss: 0.00001635
Iteration 46/1000 | Loss: 0.00001631
Iteration 47/1000 | Loss: 0.00001630
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001628
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001627
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001626
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001625
Iteration 62/1000 | Loss: 0.00001625
Iteration 63/1000 | Loss: 0.00001624
Iteration 64/1000 | Loss: 0.00001624
Iteration 65/1000 | Loss: 0.00001623
Iteration 66/1000 | Loss: 0.00001623
Iteration 67/1000 | Loss: 0.00001620
Iteration 68/1000 | Loss: 0.00001620
Iteration 69/1000 | Loss: 0.00001618
Iteration 70/1000 | Loss: 0.00001618
Iteration 71/1000 | Loss: 0.00001617
Iteration 72/1000 | Loss: 0.00001617
Iteration 73/1000 | Loss: 0.00001617
Iteration 74/1000 | Loss: 0.00001616
Iteration 75/1000 | Loss: 0.00001616
Iteration 76/1000 | Loss: 0.00001616
Iteration 77/1000 | Loss: 0.00001615
Iteration 78/1000 | Loss: 0.00001615
Iteration 79/1000 | Loss: 0.00001614
Iteration 80/1000 | Loss: 0.00001614
Iteration 81/1000 | Loss: 0.00001614
Iteration 82/1000 | Loss: 0.00001614
Iteration 83/1000 | Loss: 0.00001613
Iteration 84/1000 | Loss: 0.00001613
Iteration 85/1000 | Loss: 0.00001613
Iteration 86/1000 | Loss: 0.00001612
Iteration 87/1000 | Loss: 0.00001612
Iteration 88/1000 | Loss: 0.00001611
Iteration 89/1000 | Loss: 0.00001611
Iteration 90/1000 | Loss: 0.00001611
Iteration 91/1000 | Loss: 0.00001610
Iteration 92/1000 | Loss: 0.00001610
Iteration 93/1000 | Loss: 0.00001610
Iteration 94/1000 | Loss: 0.00001610
Iteration 95/1000 | Loss: 0.00001610
Iteration 96/1000 | Loss: 0.00001610
Iteration 97/1000 | Loss: 0.00001610
Iteration 98/1000 | Loss: 0.00001610
Iteration 99/1000 | Loss: 0.00001610
Iteration 100/1000 | Loss: 0.00001609
Iteration 101/1000 | Loss: 0.00001609
Iteration 102/1000 | Loss: 0.00001609
Iteration 103/1000 | Loss: 0.00001609
Iteration 104/1000 | Loss: 0.00001609
Iteration 105/1000 | Loss: 0.00001609
Iteration 106/1000 | Loss: 0.00001608
Iteration 107/1000 | Loss: 0.00001608
Iteration 108/1000 | Loss: 0.00001608
Iteration 109/1000 | Loss: 0.00001608
Iteration 110/1000 | Loss: 0.00001608
Iteration 111/1000 | Loss: 0.00001607
Iteration 112/1000 | Loss: 0.00001607
Iteration 113/1000 | Loss: 0.00001607
Iteration 114/1000 | Loss: 0.00001607
Iteration 115/1000 | Loss: 0.00001606
Iteration 116/1000 | Loss: 0.00019202
Iteration 117/1000 | Loss: 0.00021916
Iteration 118/1000 | Loss: 0.00006920
Iteration 119/1000 | Loss: 0.00002055
Iteration 120/1000 | Loss: 0.00001807
Iteration 121/1000 | Loss: 0.00001751
Iteration 122/1000 | Loss: 0.00019389
Iteration 123/1000 | Loss: 0.00006477
Iteration 124/1000 | Loss: 0.00001708
Iteration 125/1000 | Loss: 0.00018259
Iteration 126/1000 | Loss: 0.00005744
Iteration 127/1000 | Loss: 0.00001698
Iteration 128/1000 | Loss: 0.00016958
Iteration 129/1000 | Loss: 0.00005739
Iteration 130/1000 | Loss: 0.00001894
Iteration 131/1000 | Loss: 0.00001663
Iteration 132/1000 | Loss: 0.00015632
Iteration 133/1000 | Loss: 0.00002855
Iteration 134/1000 | Loss: 0.00001948
Iteration 135/1000 | Loss: 0.00001769
Iteration 136/1000 | Loss: 0.00001692
Iteration 137/1000 | Loss: 0.00001630
Iteration 138/1000 | Loss: 0.00001605
Iteration 139/1000 | Loss: 0.00001602
Iteration 140/1000 | Loss: 0.00001601
Iteration 141/1000 | Loss: 0.00001600
Iteration 142/1000 | Loss: 0.00001600
Iteration 143/1000 | Loss: 0.00001599
Iteration 144/1000 | Loss: 0.00001599
Iteration 145/1000 | Loss: 0.00001598
Iteration 146/1000 | Loss: 0.00001598
Iteration 147/1000 | Loss: 0.00001597
Iteration 148/1000 | Loss: 0.00001597
Iteration 149/1000 | Loss: 0.00001596
Iteration 150/1000 | Loss: 0.00001596
Iteration 151/1000 | Loss: 0.00001595
Iteration 152/1000 | Loss: 0.00001595
Iteration 153/1000 | Loss: 0.00001594
Iteration 154/1000 | Loss: 0.00001594
Iteration 155/1000 | Loss: 0.00001593
Iteration 156/1000 | Loss: 0.00001593
Iteration 157/1000 | Loss: 0.00001592
Iteration 158/1000 | Loss: 0.00001592
Iteration 159/1000 | Loss: 0.00001591
Iteration 160/1000 | Loss: 0.00001591
Iteration 161/1000 | Loss: 0.00001591
Iteration 162/1000 | Loss: 0.00001590
Iteration 163/1000 | Loss: 0.00001590
Iteration 164/1000 | Loss: 0.00001590
Iteration 165/1000 | Loss: 0.00001590
Iteration 166/1000 | Loss: 0.00001589
Iteration 167/1000 | Loss: 0.00001589
Iteration 168/1000 | Loss: 0.00001588
Iteration 169/1000 | Loss: 0.00001587
Iteration 170/1000 | Loss: 0.00001587
Iteration 171/1000 | Loss: 0.00001587
Iteration 172/1000 | Loss: 0.00001586
Iteration 173/1000 | Loss: 0.00001586
Iteration 174/1000 | Loss: 0.00001585
Iteration 175/1000 | Loss: 0.00001585
Iteration 176/1000 | Loss: 0.00001585
Iteration 177/1000 | Loss: 0.00001584
Iteration 178/1000 | Loss: 0.00001584
Iteration 179/1000 | Loss: 0.00001584
Iteration 180/1000 | Loss: 0.00001583
Iteration 181/1000 | Loss: 0.00001583
Iteration 182/1000 | Loss: 0.00001583
Iteration 183/1000 | Loss: 0.00001583
Iteration 184/1000 | Loss: 0.00001583
Iteration 185/1000 | Loss: 0.00001583
Iteration 186/1000 | Loss: 0.00001583
Iteration 187/1000 | Loss: 0.00001583
Iteration 188/1000 | Loss: 0.00001583
Iteration 189/1000 | Loss: 0.00001582
Iteration 190/1000 | Loss: 0.00001582
Iteration 191/1000 | Loss: 0.00001582
Iteration 192/1000 | Loss: 0.00001582
Iteration 193/1000 | Loss: 0.00001582
Iteration 194/1000 | Loss: 0.00001582
Iteration 195/1000 | Loss: 0.00001582
Iteration 196/1000 | Loss: 0.00001582
Iteration 197/1000 | Loss: 0.00001582
Iteration 198/1000 | Loss: 0.00001582
Iteration 199/1000 | Loss: 0.00001582
Iteration 200/1000 | Loss: 0.00001582
Iteration 201/1000 | Loss: 0.00001582
Iteration 202/1000 | Loss: 0.00001582
Iteration 203/1000 | Loss: 0.00001582
Iteration 204/1000 | Loss: 0.00001582
Iteration 205/1000 | Loss: 0.00001582
Iteration 206/1000 | Loss: 0.00001582
Iteration 207/1000 | Loss: 0.00001582
Iteration 208/1000 | Loss: 0.00001582
Iteration 209/1000 | Loss: 0.00001582
Iteration 210/1000 | Loss: 0.00001582
Iteration 211/1000 | Loss: 0.00001582
Iteration 212/1000 | Loss: 0.00001581
Iteration 213/1000 | Loss: 0.00001581
Iteration 214/1000 | Loss: 0.00001581
Iteration 215/1000 | Loss: 0.00001581
Iteration 216/1000 | Loss: 0.00001581
Iteration 217/1000 | Loss: 0.00001581
Iteration 218/1000 | Loss: 0.00001581
Iteration 219/1000 | Loss: 0.00001581
Iteration 220/1000 | Loss: 0.00001581
Iteration 221/1000 | Loss: 0.00001581
Iteration 222/1000 | Loss: 0.00001581
Iteration 223/1000 | Loss: 0.00001581
Iteration 224/1000 | Loss: 0.00001581
Iteration 225/1000 | Loss: 0.00001581
Iteration 226/1000 | Loss: 0.00001581
Iteration 227/1000 | Loss: 0.00001581
Iteration 228/1000 | Loss: 0.00001581
Iteration 229/1000 | Loss: 0.00001581
Iteration 230/1000 | Loss: 0.00001581
Iteration 231/1000 | Loss: 0.00001581
Iteration 232/1000 | Loss: 0.00001581
Iteration 233/1000 | Loss: 0.00001581
Iteration 234/1000 | Loss: 0.00001580
Iteration 235/1000 | Loss: 0.00001580
Iteration 236/1000 | Loss: 0.00001580
Iteration 237/1000 | Loss: 0.00001580
Iteration 238/1000 | Loss: 0.00001580
Iteration 239/1000 | Loss: 0.00001580
Iteration 240/1000 | Loss: 0.00001580
Iteration 241/1000 | Loss: 0.00001580
Iteration 242/1000 | Loss: 0.00001580
Iteration 243/1000 | Loss: 0.00001580
Iteration 244/1000 | Loss: 0.00001580
Iteration 245/1000 | Loss: 0.00001580
Iteration 246/1000 | Loss: 0.00001580
Iteration 247/1000 | Loss: 0.00001580
Iteration 248/1000 | Loss: 0.00001580
Iteration 249/1000 | Loss: 0.00001580
Iteration 250/1000 | Loss: 0.00001580
Iteration 251/1000 | Loss: 0.00001580
Iteration 252/1000 | Loss: 0.00001580
Iteration 253/1000 | Loss: 0.00001580
Iteration 254/1000 | Loss: 0.00001580
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [1.580334901518654e-05, 1.580334901518654e-05, 1.580334901518654e-05, 1.580334901518654e-05, 1.580334901518654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.580334901518654e-05

Optimization complete. Final v2v error: 3.2533698081970215 mm

Highest mean error: 5.7414679527282715 mm for frame 228

Lowest mean error: 2.5636656284332275 mm for frame 12

Saving results

Total time: 157.61204147338867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828993
Iteration 2/25 | Loss: 0.00163340
Iteration 3/25 | Loss: 0.00129323
Iteration 4/25 | Loss: 0.00119742
Iteration 5/25 | Loss: 0.00117717
Iteration 6/25 | Loss: 0.00122929
Iteration 7/25 | Loss: 0.00121997
Iteration 8/25 | Loss: 0.00115449
Iteration 9/25 | Loss: 0.00114327
Iteration 10/25 | Loss: 0.00113850
Iteration 11/25 | Loss: 0.00113420
Iteration 12/25 | Loss: 0.00113833
Iteration 13/25 | Loss: 0.00113473
Iteration 14/25 | Loss: 0.00113225
Iteration 15/25 | Loss: 0.00113105
Iteration 16/25 | Loss: 0.00112857
Iteration 17/25 | Loss: 0.00112622
Iteration 18/25 | Loss: 0.00112799
Iteration 19/25 | Loss: 0.00112539
Iteration 20/25 | Loss: 0.00112736
Iteration 21/25 | Loss: 0.00112546
Iteration 22/25 | Loss: 0.00112491
Iteration 23/25 | Loss: 0.00112991
Iteration 24/25 | Loss: 0.00112855
Iteration 25/25 | Loss: 0.00112726

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.28455114
Iteration 2/25 | Loss: 0.00078060
Iteration 3/25 | Loss: 0.00078057
Iteration 4/25 | Loss: 0.00078057
Iteration 5/25 | Loss: 0.00078057
Iteration 6/25 | Loss: 0.00078057
Iteration 7/25 | Loss: 0.00078057
Iteration 8/25 | Loss: 0.00078057
Iteration 9/25 | Loss: 0.00078057
Iteration 10/25 | Loss: 0.00078057
Iteration 11/25 | Loss: 0.00078057
Iteration 12/25 | Loss: 0.00078057
Iteration 13/25 | Loss: 0.00078057
Iteration 14/25 | Loss: 0.00078057
Iteration 15/25 | Loss: 0.00078057
Iteration 16/25 | Loss: 0.00078057
Iteration 17/25 | Loss: 0.00078057
Iteration 18/25 | Loss: 0.00078057
Iteration 19/25 | Loss: 0.00078057
Iteration 20/25 | Loss: 0.00078057
Iteration 21/25 | Loss: 0.00078057
Iteration 22/25 | Loss: 0.00078057
Iteration 23/25 | Loss: 0.00078057
Iteration 24/25 | Loss: 0.00078057
Iteration 25/25 | Loss: 0.00078057
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007805659552104771, 0.0007805659552104771, 0.0007805659552104771, 0.0007805659552104771, 0.0007805659552104771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007805659552104771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078057
Iteration 2/1000 | Loss: 0.00057893
Iteration 3/1000 | Loss: 0.00045584
Iteration 4/1000 | Loss: 0.00145386
Iteration 5/1000 | Loss: 0.00006420
Iteration 6/1000 | Loss: 0.00035730
Iteration 7/1000 | Loss: 0.00041033
Iteration 8/1000 | Loss: 0.00019231
Iteration 9/1000 | Loss: 0.00009447
Iteration 10/1000 | Loss: 0.00002087
Iteration 11/1000 | Loss: 0.00001998
Iteration 12/1000 | Loss: 0.00001920
Iteration 13/1000 | Loss: 0.00004915
Iteration 14/1000 | Loss: 0.00002666
Iteration 15/1000 | Loss: 0.00008097
Iteration 16/1000 | Loss: 0.00055022
Iteration 17/1000 | Loss: 0.00001643
Iteration 18/1000 | Loss: 0.00001510
Iteration 19/1000 | Loss: 0.00001439
Iteration 20/1000 | Loss: 0.00001412
Iteration 21/1000 | Loss: 0.00001416
Iteration 22/1000 | Loss: 0.00001370
Iteration 23/1000 | Loss: 0.00001352
Iteration 24/1000 | Loss: 0.00001332
Iteration 25/1000 | Loss: 0.00001326
Iteration 26/1000 | Loss: 0.00001325
Iteration 27/1000 | Loss: 0.00001322
Iteration 28/1000 | Loss: 0.00001314
Iteration 29/1000 | Loss: 0.00001310
Iteration 30/1000 | Loss: 0.00001310
Iteration 31/1000 | Loss: 0.00001309
Iteration 32/1000 | Loss: 0.00001309
Iteration 33/1000 | Loss: 0.00001308
Iteration 34/1000 | Loss: 0.00001308
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001306
Iteration 37/1000 | Loss: 0.00001306
Iteration 38/1000 | Loss: 0.00001306
Iteration 39/1000 | Loss: 0.00001305
Iteration 40/1000 | Loss: 0.00001305
Iteration 41/1000 | Loss: 0.00001303
Iteration 42/1000 | Loss: 0.00001302
Iteration 43/1000 | Loss: 0.00001302
Iteration 44/1000 | Loss: 0.00001302
Iteration 45/1000 | Loss: 0.00001302
Iteration 46/1000 | Loss: 0.00001301
Iteration 47/1000 | Loss: 0.00001301
Iteration 48/1000 | Loss: 0.00001301
Iteration 49/1000 | Loss: 0.00001300
Iteration 50/1000 | Loss: 0.00001300
Iteration 51/1000 | Loss: 0.00001299
Iteration 52/1000 | Loss: 0.00001299
Iteration 53/1000 | Loss: 0.00001299
Iteration 54/1000 | Loss: 0.00001299
Iteration 55/1000 | Loss: 0.00001299
Iteration 56/1000 | Loss: 0.00001298
Iteration 57/1000 | Loss: 0.00001298
Iteration 58/1000 | Loss: 0.00001298
Iteration 59/1000 | Loss: 0.00001298
Iteration 60/1000 | Loss: 0.00001298
Iteration 61/1000 | Loss: 0.00001298
Iteration 62/1000 | Loss: 0.00001297
Iteration 63/1000 | Loss: 0.00001297
Iteration 64/1000 | Loss: 0.00001297
Iteration 65/1000 | Loss: 0.00001297
Iteration 66/1000 | Loss: 0.00001296
Iteration 67/1000 | Loss: 0.00001296
Iteration 68/1000 | Loss: 0.00001296
Iteration 69/1000 | Loss: 0.00001295
Iteration 70/1000 | Loss: 0.00001295
Iteration 71/1000 | Loss: 0.00001294
Iteration 72/1000 | Loss: 0.00001294
Iteration 73/1000 | Loss: 0.00001294
Iteration 74/1000 | Loss: 0.00001294
Iteration 75/1000 | Loss: 0.00001294
Iteration 76/1000 | Loss: 0.00001294
Iteration 77/1000 | Loss: 0.00001293
Iteration 78/1000 | Loss: 0.00001293
Iteration 79/1000 | Loss: 0.00001293
Iteration 80/1000 | Loss: 0.00001292
Iteration 81/1000 | Loss: 0.00001292
Iteration 82/1000 | Loss: 0.00001291
Iteration 83/1000 | Loss: 0.00001291
Iteration 84/1000 | Loss: 0.00001291
Iteration 85/1000 | Loss: 0.00001291
Iteration 86/1000 | Loss: 0.00001291
Iteration 87/1000 | Loss: 0.00001291
Iteration 88/1000 | Loss: 0.00001290
Iteration 89/1000 | Loss: 0.00001290
Iteration 90/1000 | Loss: 0.00001290
Iteration 91/1000 | Loss: 0.00001290
Iteration 92/1000 | Loss: 0.00001290
Iteration 93/1000 | Loss: 0.00001290
Iteration 94/1000 | Loss: 0.00001290
Iteration 95/1000 | Loss: 0.00001290
Iteration 96/1000 | Loss: 0.00001290
Iteration 97/1000 | Loss: 0.00001289
Iteration 98/1000 | Loss: 0.00001289
Iteration 99/1000 | Loss: 0.00001289
Iteration 100/1000 | Loss: 0.00001289
Iteration 101/1000 | Loss: 0.00001289
Iteration 102/1000 | Loss: 0.00001289
Iteration 103/1000 | Loss: 0.00001289
Iteration 104/1000 | Loss: 0.00001289
Iteration 105/1000 | Loss: 0.00001289
Iteration 106/1000 | Loss: 0.00001288
Iteration 107/1000 | Loss: 0.00001288
Iteration 108/1000 | Loss: 0.00001288
Iteration 109/1000 | Loss: 0.00001288
Iteration 110/1000 | Loss: 0.00001288
Iteration 111/1000 | Loss: 0.00001287
Iteration 112/1000 | Loss: 0.00001287
Iteration 113/1000 | Loss: 0.00001287
Iteration 114/1000 | Loss: 0.00001287
Iteration 115/1000 | Loss: 0.00001287
Iteration 116/1000 | Loss: 0.00001287
Iteration 117/1000 | Loss: 0.00001286
Iteration 118/1000 | Loss: 0.00001286
Iteration 119/1000 | Loss: 0.00001286
Iteration 120/1000 | Loss: 0.00001286
Iteration 121/1000 | Loss: 0.00001286
Iteration 122/1000 | Loss: 0.00001286
Iteration 123/1000 | Loss: 0.00001286
Iteration 124/1000 | Loss: 0.00001286
Iteration 125/1000 | Loss: 0.00001286
Iteration 126/1000 | Loss: 0.00001286
Iteration 127/1000 | Loss: 0.00001286
Iteration 128/1000 | Loss: 0.00001286
Iteration 129/1000 | Loss: 0.00001286
Iteration 130/1000 | Loss: 0.00001286
Iteration 131/1000 | Loss: 0.00001286
Iteration 132/1000 | Loss: 0.00001286
Iteration 133/1000 | Loss: 0.00001285
Iteration 134/1000 | Loss: 0.00001285
Iteration 135/1000 | Loss: 0.00001285
Iteration 136/1000 | Loss: 0.00001285
Iteration 137/1000 | Loss: 0.00001285
Iteration 138/1000 | Loss: 0.00001285
Iteration 139/1000 | Loss: 0.00001285
Iteration 140/1000 | Loss: 0.00001284
Iteration 141/1000 | Loss: 0.00001284
Iteration 142/1000 | Loss: 0.00001284
Iteration 143/1000 | Loss: 0.00001284
Iteration 144/1000 | Loss: 0.00001284
Iteration 145/1000 | Loss: 0.00001283
Iteration 146/1000 | Loss: 0.00001283
Iteration 147/1000 | Loss: 0.00001283
Iteration 148/1000 | Loss: 0.00001283
Iteration 149/1000 | Loss: 0.00001283
Iteration 150/1000 | Loss: 0.00001283
Iteration 151/1000 | Loss: 0.00001283
Iteration 152/1000 | Loss: 0.00001283
Iteration 153/1000 | Loss: 0.00001283
Iteration 154/1000 | Loss: 0.00001283
Iteration 155/1000 | Loss: 0.00001283
Iteration 156/1000 | Loss: 0.00001283
Iteration 157/1000 | Loss: 0.00001283
Iteration 158/1000 | Loss: 0.00001283
Iteration 159/1000 | Loss: 0.00001283
Iteration 160/1000 | Loss: 0.00001283
Iteration 161/1000 | Loss: 0.00001283
Iteration 162/1000 | Loss: 0.00001282
Iteration 163/1000 | Loss: 0.00001282
Iteration 164/1000 | Loss: 0.00001282
Iteration 165/1000 | Loss: 0.00001282
Iteration 166/1000 | Loss: 0.00001282
Iteration 167/1000 | Loss: 0.00001282
Iteration 168/1000 | Loss: 0.00001282
Iteration 169/1000 | Loss: 0.00001282
Iteration 170/1000 | Loss: 0.00001282
Iteration 171/1000 | Loss: 0.00001282
Iteration 172/1000 | Loss: 0.00001282
Iteration 173/1000 | Loss: 0.00001282
Iteration 174/1000 | Loss: 0.00001282
Iteration 175/1000 | Loss: 0.00001282
Iteration 176/1000 | Loss: 0.00001282
Iteration 177/1000 | Loss: 0.00001282
Iteration 178/1000 | Loss: 0.00001282
Iteration 179/1000 | Loss: 0.00001282
Iteration 180/1000 | Loss: 0.00001282
Iteration 181/1000 | Loss: 0.00001282
Iteration 182/1000 | Loss: 0.00001282
Iteration 183/1000 | Loss: 0.00001282
Iteration 184/1000 | Loss: 0.00001282
Iteration 185/1000 | Loss: 0.00001282
Iteration 186/1000 | Loss: 0.00001282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.2824759323848411e-05, 1.2824759323848411e-05, 1.2824759323848411e-05, 1.2824759323848411e-05, 1.2824759323848411e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2824759323848411e-05

Optimization complete. Final v2v error: 2.977418899536133 mm

Highest mean error: 5.797314643859863 mm for frame 90

Lowest mean error: 2.50750994682312 mm for frame 226

Saving results

Total time: 105.48580813407898
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00609395
Iteration 2/25 | Loss: 0.00165945
Iteration 3/25 | Loss: 0.00129712
Iteration 4/25 | Loss: 0.00128090
Iteration 5/25 | Loss: 0.00127549
Iteration 6/25 | Loss: 0.00127386
Iteration 7/25 | Loss: 0.00127386
Iteration 8/25 | Loss: 0.00127386
Iteration 9/25 | Loss: 0.00127386
Iteration 10/25 | Loss: 0.00127386
Iteration 11/25 | Loss: 0.00127386
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012738627847284079, 0.0012738627847284079, 0.0012738627847284079, 0.0012738627847284079, 0.0012738627847284079]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012738627847284079

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89843917
Iteration 2/25 | Loss: 0.00083211
Iteration 3/25 | Loss: 0.00083210
Iteration 4/25 | Loss: 0.00083210
Iteration 5/25 | Loss: 0.00083210
Iteration 6/25 | Loss: 0.00083210
Iteration 7/25 | Loss: 0.00083210
Iteration 8/25 | Loss: 0.00083210
Iteration 9/25 | Loss: 0.00083210
Iteration 10/25 | Loss: 0.00083210
Iteration 11/25 | Loss: 0.00083210
Iteration 12/25 | Loss: 0.00083210
Iteration 13/25 | Loss: 0.00083210
Iteration 14/25 | Loss: 0.00083210
Iteration 15/25 | Loss: 0.00083210
Iteration 16/25 | Loss: 0.00083210
Iteration 17/25 | Loss: 0.00083210
Iteration 18/25 | Loss: 0.00083210
Iteration 19/25 | Loss: 0.00083210
Iteration 20/25 | Loss: 0.00083210
Iteration 21/25 | Loss: 0.00083210
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008320987690240145, 0.0008320987690240145, 0.0008320987690240145, 0.0008320987690240145, 0.0008320987690240145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008320987690240145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00083210
Iteration 2/1000 | Loss: 0.00005672
Iteration 3/1000 | Loss: 0.00003880
Iteration 4/1000 | Loss: 0.00003401
Iteration 5/1000 | Loss: 0.00003234
Iteration 6/1000 | Loss: 0.00003152
Iteration 7/1000 | Loss: 0.00003082
Iteration 8/1000 | Loss: 0.00003022
Iteration 9/1000 | Loss: 0.00002993
Iteration 10/1000 | Loss: 0.00002958
Iteration 11/1000 | Loss: 0.00002927
Iteration 12/1000 | Loss: 0.00002898
Iteration 13/1000 | Loss: 0.00002878
Iteration 14/1000 | Loss: 0.00002851
Iteration 15/1000 | Loss: 0.00002829
Iteration 16/1000 | Loss: 0.00002810
Iteration 17/1000 | Loss: 0.00002793
Iteration 18/1000 | Loss: 0.00002785
Iteration 19/1000 | Loss: 0.00002773
Iteration 20/1000 | Loss: 0.00002767
Iteration 21/1000 | Loss: 0.00002767
Iteration 22/1000 | Loss: 0.00002767
Iteration 23/1000 | Loss: 0.00002765
Iteration 24/1000 | Loss: 0.00002765
Iteration 25/1000 | Loss: 0.00002763
Iteration 26/1000 | Loss: 0.00002763
Iteration 27/1000 | Loss: 0.00002763
Iteration 28/1000 | Loss: 0.00002762
Iteration 29/1000 | Loss: 0.00002761
Iteration 30/1000 | Loss: 0.00002761
Iteration 31/1000 | Loss: 0.00002761
Iteration 32/1000 | Loss: 0.00002761
Iteration 33/1000 | Loss: 0.00002761
Iteration 34/1000 | Loss: 0.00002761
Iteration 35/1000 | Loss: 0.00002760
Iteration 36/1000 | Loss: 0.00002760
Iteration 37/1000 | Loss: 0.00002760
Iteration 38/1000 | Loss: 0.00002759
Iteration 39/1000 | Loss: 0.00002759
Iteration 40/1000 | Loss: 0.00002759
Iteration 41/1000 | Loss: 0.00002759
Iteration 42/1000 | Loss: 0.00002759
Iteration 43/1000 | Loss: 0.00002758
Iteration 44/1000 | Loss: 0.00002758
Iteration 45/1000 | Loss: 0.00002758
Iteration 46/1000 | Loss: 0.00002758
Iteration 47/1000 | Loss: 0.00002758
Iteration 48/1000 | Loss: 0.00002757
Iteration 49/1000 | Loss: 0.00002757
Iteration 50/1000 | Loss: 0.00002757
Iteration 51/1000 | Loss: 0.00002757
Iteration 52/1000 | Loss: 0.00002757
Iteration 53/1000 | Loss: 0.00002757
Iteration 54/1000 | Loss: 0.00002756
Iteration 55/1000 | Loss: 0.00002756
Iteration 56/1000 | Loss: 0.00002756
Iteration 57/1000 | Loss: 0.00002756
Iteration 58/1000 | Loss: 0.00002756
Iteration 59/1000 | Loss: 0.00002756
Iteration 60/1000 | Loss: 0.00002755
Iteration 61/1000 | Loss: 0.00002755
Iteration 62/1000 | Loss: 0.00002755
Iteration 63/1000 | Loss: 0.00002755
Iteration 64/1000 | Loss: 0.00002755
Iteration 65/1000 | Loss: 0.00002754
Iteration 66/1000 | Loss: 0.00002754
Iteration 67/1000 | Loss: 0.00002754
Iteration 68/1000 | Loss: 0.00002754
Iteration 69/1000 | Loss: 0.00002753
Iteration 70/1000 | Loss: 0.00002753
Iteration 71/1000 | Loss: 0.00002753
Iteration 72/1000 | Loss: 0.00002753
Iteration 73/1000 | Loss: 0.00002753
Iteration 74/1000 | Loss: 0.00002753
Iteration 75/1000 | Loss: 0.00002753
Iteration 76/1000 | Loss: 0.00002753
Iteration 77/1000 | Loss: 0.00002753
Iteration 78/1000 | Loss: 0.00002753
Iteration 79/1000 | Loss: 0.00002753
Iteration 80/1000 | Loss: 0.00002752
Iteration 81/1000 | Loss: 0.00002752
Iteration 82/1000 | Loss: 0.00002752
Iteration 83/1000 | Loss: 0.00002751
Iteration 84/1000 | Loss: 0.00002751
Iteration 85/1000 | Loss: 0.00002751
Iteration 86/1000 | Loss: 0.00002751
Iteration 87/1000 | Loss: 0.00002751
Iteration 88/1000 | Loss: 0.00002751
Iteration 89/1000 | Loss: 0.00002751
Iteration 90/1000 | Loss: 0.00002751
Iteration 91/1000 | Loss: 0.00002751
Iteration 92/1000 | Loss: 0.00002750
Iteration 93/1000 | Loss: 0.00002750
Iteration 94/1000 | Loss: 0.00002750
Iteration 95/1000 | Loss: 0.00002750
Iteration 96/1000 | Loss: 0.00002749
Iteration 97/1000 | Loss: 0.00002749
Iteration 98/1000 | Loss: 0.00002749
Iteration 99/1000 | Loss: 0.00002749
Iteration 100/1000 | Loss: 0.00002749
Iteration 101/1000 | Loss: 0.00002749
Iteration 102/1000 | Loss: 0.00002748
Iteration 103/1000 | Loss: 0.00002748
Iteration 104/1000 | Loss: 0.00002748
Iteration 105/1000 | Loss: 0.00002748
Iteration 106/1000 | Loss: 0.00002748
Iteration 107/1000 | Loss: 0.00002748
Iteration 108/1000 | Loss: 0.00002748
Iteration 109/1000 | Loss: 0.00002748
Iteration 110/1000 | Loss: 0.00002748
Iteration 111/1000 | Loss: 0.00002748
Iteration 112/1000 | Loss: 0.00002748
Iteration 113/1000 | Loss: 0.00002747
Iteration 114/1000 | Loss: 0.00002747
Iteration 115/1000 | Loss: 0.00002747
Iteration 116/1000 | Loss: 0.00002747
Iteration 117/1000 | Loss: 0.00002747
Iteration 118/1000 | Loss: 0.00002747
Iteration 119/1000 | Loss: 0.00002747
Iteration 120/1000 | Loss: 0.00002747
Iteration 121/1000 | Loss: 0.00002747
Iteration 122/1000 | Loss: 0.00002747
Iteration 123/1000 | Loss: 0.00002747
Iteration 124/1000 | Loss: 0.00002747
Iteration 125/1000 | Loss: 0.00002747
Iteration 126/1000 | Loss: 0.00002747
Iteration 127/1000 | Loss: 0.00002747
Iteration 128/1000 | Loss: 0.00002747
Iteration 129/1000 | Loss: 0.00002747
Iteration 130/1000 | Loss: 0.00002746
Iteration 131/1000 | Loss: 0.00002746
Iteration 132/1000 | Loss: 0.00002746
Iteration 133/1000 | Loss: 0.00002746
Iteration 134/1000 | Loss: 0.00002746
Iteration 135/1000 | Loss: 0.00002746
Iteration 136/1000 | Loss: 0.00002746
Iteration 137/1000 | Loss: 0.00002746
Iteration 138/1000 | Loss: 0.00002746
Iteration 139/1000 | Loss: 0.00002746
Iteration 140/1000 | Loss: 0.00002746
Iteration 141/1000 | Loss: 0.00002745
Iteration 142/1000 | Loss: 0.00002745
Iteration 143/1000 | Loss: 0.00002745
Iteration 144/1000 | Loss: 0.00002745
Iteration 145/1000 | Loss: 0.00002745
Iteration 146/1000 | Loss: 0.00002745
Iteration 147/1000 | Loss: 0.00002745
Iteration 148/1000 | Loss: 0.00002745
Iteration 149/1000 | Loss: 0.00002745
Iteration 150/1000 | Loss: 0.00002745
Iteration 151/1000 | Loss: 0.00002745
Iteration 152/1000 | Loss: 0.00002745
Iteration 153/1000 | Loss: 0.00002745
Iteration 154/1000 | Loss: 0.00002745
Iteration 155/1000 | Loss: 0.00002745
Iteration 156/1000 | Loss: 0.00002745
Iteration 157/1000 | Loss: 0.00002745
Iteration 158/1000 | Loss: 0.00002745
Iteration 159/1000 | Loss: 0.00002745
Iteration 160/1000 | Loss: 0.00002745
Iteration 161/1000 | Loss: 0.00002745
Iteration 162/1000 | Loss: 0.00002745
Iteration 163/1000 | Loss: 0.00002745
Iteration 164/1000 | Loss: 0.00002745
Iteration 165/1000 | Loss: 0.00002745
Iteration 166/1000 | Loss: 0.00002745
Iteration 167/1000 | Loss: 0.00002745
Iteration 168/1000 | Loss: 0.00002745
Iteration 169/1000 | Loss: 0.00002745
Iteration 170/1000 | Loss: 0.00002745
Iteration 171/1000 | Loss: 0.00002745
Iteration 172/1000 | Loss: 0.00002745
Iteration 173/1000 | Loss: 0.00002745
Iteration 174/1000 | Loss: 0.00002745
Iteration 175/1000 | Loss: 0.00002745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [2.7450409106677398e-05, 2.7450409106677398e-05, 2.7450409106677398e-05, 2.7450409106677398e-05, 2.7450409106677398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7450409106677398e-05

Optimization complete. Final v2v error: 4.035104274749756 mm

Highest mean error: 4.892098426818848 mm for frame 94

Lowest mean error: 3.140066385269165 mm for frame 36

Saving results

Total time: 44.57653450965881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008714
Iteration 2/25 | Loss: 0.00231191
Iteration 3/25 | Loss: 0.00183597
Iteration 4/25 | Loss: 0.00159705
Iteration 5/25 | Loss: 0.00154384
Iteration 6/25 | Loss: 0.00125649
Iteration 7/25 | Loss: 0.00121793
Iteration 8/25 | Loss: 0.00121054
Iteration 9/25 | Loss: 0.00120784
Iteration 10/25 | Loss: 0.00120697
Iteration 11/25 | Loss: 0.00120663
Iteration 12/25 | Loss: 0.00120656
Iteration 13/25 | Loss: 0.00120656
Iteration 14/25 | Loss: 0.00120656
Iteration 15/25 | Loss: 0.00120656
Iteration 16/25 | Loss: 0.00120656
Iteration 17/25 | Loss: 0.00120656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012065586633980274, 0.0012065586633980274, 0.0012065586633980274, 0.0012065586633980274, 0.0012065586633980274]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012065586633980274

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36493301
Iteration 2/25 | Loss: 0.00068753
Iteration 3/25 | Loss: 0.00068753
Iteration 4/25 | Loss: 0.00068753
Iteration 5/25 | Loss: 0.00068753
Iteration 6/25 | Loss: 0.00068753
Iteration 7/25 | Loss: 0.00068753
Iteration 8/25 | Loss: 0.00068753
Iteration 9/25 | Loss: 0.00068753
Iteration 10/25 | Loss: 0.00068753
Iteration 11/25 | Loss: 0.00068753
Iteration 12/25 | Loss: 0.00068753
Iteration 13/25 | Loss: 0.00068753
Iteration 14/25 | Loss: 0.00068753
Iteration 15/25 | Loss: 0.00068753
Iteration 16/25 | Loss: 0.00068753
Iteration 17/25 | Loss: 0.00068753
Iteration 18/25 | Loss: 0.00068753
Iteration 19/25 | Loss: 0.00068753
Iteration 20/25 | Loss: 0.00068753
Iteration 21/25 | Loss: 0.00068753
Iteration 22/25 | Loss: 0.00068753
Iteration 23/25 | Loss: 0.00068753
Iteration 24/25 | Loss: 0.00068753
Iteration 25/25 | Loss: 0.00068753

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068753
Iteration 2/1000 | Loss: 0.00003277
Iteration 3/1000 | Loss: 0.00002469
Iteration 4/1000 | Loss: 0.00002353
Iteration 5/1000 | Loss: 0.00002305
Iteration 6/1000 | Loss: 0.00002248
Iteration 7/1000 | Loss: 0.00002210
Iteration 8/1000 | Loss: 0.00002187
Iteration 9/1000 | Loss: 0.00002167
Iteration 10/1000 | Loss: 0.00002158
Iteration 11/1000 | Loss: 0.00002154
Iteration 12/1000 | Loss: 0.00002143
Iteration 13/1000 | Loss: 0.00002130
Iteration 14/1000 | Loss: 0.00002126
Iteration 15/1000 | Loss: 0.00002123
Iteration 16/1000 | Loss: 0.00002122
Iteration 17/1000 | Loss: 0.00002122
Iteration 18/1000 | Loss: 0.00002121
Iteration 19/1000 | Loss: 0.00002121
Iteration 20/1000 | Loss: 0.00002120
Iteration 21/1000 | Loss: 0.00002120
Iteration 22/1000 | Loss: 0.00002120
Iteration 23/1000 | Loss: 0.00002120
Iteration 24/1000 | Loss: 0.00002120
Iteration 25/1000 | Loss: 0.00002119
Iteration 26/1000 | Loss: 0.00002119
Iteration 27/1000 | Loss: 0.00002119
Iteration 28/1000 | Loss: 0.00002119
Iteration 29/1000 | Loss: 0.00002118
Iteration 30/1000 | Loss: 0.00002118
Iteration 31/1000 | Loss: 0.00002117
Iteration 32/1000 | Loss: 0.00002116
Iteration 33/1000 | Loss: 0.00002114
Iteration 34/1000 | Loss: 0.00002113
Iteration 35/1000 | Loss: 0.00002113
Iteration 36/1000 | Loss: 0.00002113
Iteration 37/1000 | Loss: 0.00002112
Iteration 38/1000 | Loss: 0.00002112
Iteration 39/1000 | Loss: 0.00002112
Iteration 40/1000 | Loss: 0.00002112
Iteration 41/1000 | Loss: 0.00002112
Iteration 42/1000 | Loss: 0.00002111
Iteration 43/1000 | Loss: 0.00002111
Iteration 44/1000 | Loss: 0.00002110
Iteration 45/1000 | Loss: 0.00002110
Iteration 46/1000 | Loss: 0.00002110
Iteration 47/1000 | Loss: 0.00002110
Iteration 48/1000 | Loss: 0.00002110
Iteration 49/1000 | Loss: 0.00002110
Iteration 50/1000 | Loss: 0.00002110
Iteration 51/1000 | Loss: 0.00002109
Iteration 52/1000 | Loss: 0.00002109
Iteration 53/1000 | Loss: 0.00002109
Iteration 54/1000 | Loss: 0.00002109
Iteration 55/1000 | Loss: 0.00002108
Iteration 56/1000 | Loss: 0.00002108
Iteration 57/1000 | Loss: 0.00002108
Iteration 58/1000 | Loss: 0.00002108
Iteration 59/1000 | Loss: 0.00002107
Iteration 60/1000 | Loss: 0.00002107
Iteration 61/1000 | Loss: 0.00002107
Iteration 62/1000 | Loss: 0.00002107
Iteration 63/1000 | Loss: 0.00002107
Iteration 64/1000 | Loss: 0.00002107
Iteration 65/1000 | Loss: 0.00002107
Iteration 66/1000 | Loss: 0.00002107
Iteration 67/1000 | Loss: 0.00002107
Iteration 68/1000 | Loss: 0.00002107
Iteration 69/1000 | Loss: 0.00002107
Iteration 70/1000 | Loss: 0.00002107
Iteration 71/1000 | Loss: 0.00002107
Iteration 72/1000 | Loss: 0.00002107
Iteration 73/1000 | Loss: 0.00002107
Iteration 74/1000 | Loss: 0.00002107
Iteration 75/1000 | Loss: 0.00002107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.106525789713487e-05, 2.106525789713487e-05, 2.106525789713487e-05, 2.106525789713487e-05, 2.106525789713487e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.106525789713487e-05

Optimization complete. Final v2v error: 3.920638084411621 mm

Highest mean error: 4.029903888702393 mm for frame 239

Lowest mean error: 3.8569953441619873 mm for frame 132

Saving results

Total time: 45.5475070476532
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996395
Iteration 2/25 | Loss: 0.00214189
Iteration 3/25 | Loss: 0.00157007
Iteration 4/25 | Loss: 0.00148736
Iteration 5/25 | Loss: 0.00147679
Iteration 6/25 | Loss: 0.00166236
Iteration 7/25 | Loss: 0.00144307
Iteration 8/25 | Loss: 0.00137617
Iteration 9/25 | Loss: 0.00135783
Iteration 10/25 | Loss: 0.00133039
Iteration 11/25 | Loss: 0.00131816
Iteration 12/25 | Loss: 0.00131045
Iteration 13/25 | Loss: 0.00130949
Iteration 14/25 | Loss: 0.00131400
Iteration 15/25 | Loss: 0.00130757
Iteration 16/25 | Loss: 0.00130649
Iteration 17/25 | Loss: 0.00130369
Iteration 18/25 | Loss: 0.00130121
Iteration 19/25 | Loss: 0.00130216
Iteration 20/25 | Loss: 0.00130445
Iteration 21/25 | Loss: 0.00130443
Iteration 22/25 | Loss: 0.00130046
Iteration 23/25 | Loss: 0.00130076
Iteration 24/25 | Loss: 0.00130140
Iteration 25/25 | Loss: 0.00130248

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.91675210
Iteration 2/25 | Loss: 0.00097567
Iteration 3/25 | Loss: 0.00097565
Iteration 4/25 | Loss: 0.00097565
Iteration 5/25 | Loss: 0.00097565
Iteration 6/25 | Loss: 0.00097565
Iteration 7/25 | Loss: 0.00097565
Iteration 8/25 | Loss: 0.00097565
Iteration 9/25 | Loss: 0.00097565
Iteration 10/25 | Loss: 0.00097565
Iteration 11/25 | Loss: 0.00097565
Iteration 12/25 | Loss: 0.00097565
Iteration 13/25 | Loss: 0.00097565
Iteration 14/25 | Loss: 0.00097565
Iteration 15/25 | Loss: 0.00097565
Iteration 16/25 | Loss: 0.00097565
Iteration 17/25 | Loss: 0.00097565
Iteration 18/25 | Loss: 0.00097565
Iteration 19/25 | Loss: 0.00097565
Iteration 20/25 | Loss: 0.00097565
Iteration 21/25 | Loss: 0.00097565
Iteration 22/25 | Loss: 0.00097565
Iteration 23/25 | Loss: 0.00097565
Iteration 24/25 | Loss: 0.00097565
Iteration 25/25 | Loss: 0.00097565

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097565
Iteration 2/1000 | Loss: 0.00014413
Iteration 3/1000 | Loss: 0.00010936
Iteration 4/1000 | Loss: 0.00009669
Iteration 5/1000 | Loss: 0.00008627
Iteration 6/1000 | Loss: 0.00012448
Iteration 7/1000 | Loss: 0.00009421
Iteration 8/1000 | Loss: 0.00011716
Iteration 9/1000 | Loss: 0.00023903
Iteration 10/1000 | Loss: 0.00013176
Iteration 11/1000 | Loss: 0.00010342
Iteration 12/1000 | Loss: 0.00007335
Iteration 13/1000 | Loss: 0.00008027
Iteration 14/1000 | Loss: 0.00008072
Iteration 15/1000 | Loss: 0.00007207
Iteration 16/1000 | Loss: 0.00006835
Iteration 17/1000 | Loss: 0.00008648
Iteration 18/1000 | Loss: 0.00007853
Iteration 19/1000 | Loss: 0.00008283
Iteration 20/1000 | Loss: 0.00007674
Iteration 21/1000 | Loss: 0.00008505
Iteration 22/1000 | Loss: 0.00007389
Iteration 23/1000 | Loss: 0.00006856
Iteration 24/1000 | Loss: 0.00006693
Iteration 25/1000 | Loss: 0.00007122
Iteration 26/1000 | Loss: 0.00009500
Iteration 27/1000 | Loss: 0.00009272
Iteration 28/1000 | Loss: 0.00010140
Iteration 29/1000 | Loss: 0.00010073
Iteration 30/1000 | Loss: 0.00010680
Iteration 31/1000 | Loss: 0.00008862
Iteration 32/1000 | Loss: 0.00010257
Iteration 33/1000 | Loss: 0.00009036
Iteration 34/1000 | Loss: 0.00009964
Iteration 35/1000 | Loss: 0.00009774
Iteration 36/1000 | Loss: 0.00009345
Iteration 37/1000 | Loss: 0.00007960
Iteration 38/1000 | Loss: 0.00007002
Iteration 39/1000 | Loss: 0.00006613
Iteration 40/1000 | Loss: 0.00006993
Iteration 41/1000 | Loss: 0.00007633
Iteration 42/1000 | Loss: 0.00007625
Iteration 43/1000 | Loss: 0.00009523
Iteration 44/1000 | Loss: 0.00008007
Iteration 45/1000 | Loss: 0.00010168
Iteration 46/1000 | Loss: 0.00010033
Iteration 47/1000 | Loss: 0.00007043
Iteration 48/1000 | Loss: 0.00007844
Iteration 49/1000 | Loss: 0.00008762
Iteration 50/1000 | Loss: 0.00007163
Iteration 51/1000 | Loss: 0.00006743
Iteration 52/1000 | Loss: 0.00005684
Iteration 53/1000 | Loss: 0.00006481
Iteration 54/1000 | Loss: 0.00005531
Iteration 55/1000 | Loss: 0.00008547
Iteration 56/1000 | Loss: 0.00008027
Iteration 57/1000 | Loss: 0.00005611
Iteration 58/1000 | Loss: 0.00005603
Iteration 59/1000 | Loss: 0.00009611
Iteration 60/1000 | Loss: 0.00007610
Iteration 61/1000 | Loss: 0.00008218
Iteration 62/1000 | Loss: 0.00007582
Iteration 63/1000 | Loss: 0.00007051
Iteration 64/1000 | Loss: 0.00007642
Iteration 65/1000 | Loss: 0.00006475
Iteration 66/1000 | Loss: 0.00005639
Iteration 67/1000 | Loss: 0.00004747
Iteration 68/1000 | Loss: 0.00004315
Iteration 69/1000 | Loss: 0.00004303
Iteration 70/1000 | Loss: 0.00004151
Iteration 71/1000 | Loss: 0.00003999
Iteration 72/1000 | Loss: 0.00006561
Iteration 73/1000 | Loss: 0.00006740
Iteration 74/1000 | Loss: 0.00007667
Iteration 75/1000 | Loss: 0.00005499
Iteration 76/1000 | Loss: 0.00006595
Iteration 77/1000 | Loss: 0.00005838
Iteration 78/1000 | Loss: 0.00007848
Iteration 79/1000 | Loss: 0.00006983
Iteration 80/1000 | Loss: 0.00007344
Iteration 81/1000 | Loss: 0.00005258
Iteration 82/1000 | Loss: 0.00006420
Iteration 83/1000 | Loss: 0.00007020
Iteration 84/1000 | Loss: 0.00005948
Iteration 85/1000 | Loss: 0.00009158
Iteration 86/1000 | Loss: 0.00006563
Iteration 87/1000 | Loss: 0.00007664
Iteration 88/1000 | Loss: 0.00008045
Iteration 89/1000 | Loss: 0.00007584
Iteration 90/1000 | Loss: 0.00008510
Iteration 91/1000 | Loss: 0.00008086
Iteration 92/1000 | Loss: 0.00006453
Iteration 93/1000 | Loss: 0.00008173
Iteration 94/1000 | Loss: 0.00005059
Iteration 95/1000 | Loss: 0.00005685
Iteration 96/1000 | Loss: 0.00006499
Iteration 97/1000 | Loss: 0.00007867
Iteration 98/1000 | Loss: 0.00009034
Iteration 99/1000 | Loss: 0.00004386
Iteration 100/1000 | Loss: 0.00006334
Iteration 101/1000 | Loss: 0.00006283
Iteration 102/1000 | Loss: 0.00006237
Iteration 103/1000 | Loss: 0.00005959
Iteration 104/1000 | Loss: 0.00006162
Iteration 105/1000 | Loss: 0.00006549
Iteration 106/1000 | Loss: 0.00005182
Iteration 107/1000 | Loss: 0.00005715
Iteration 108/1000 | Loss: 0.00005004
Iteration 109/1000 | Loss: 0.00005764
Iteration 110/1000 | Loss: 0.00006524
Iteration 111/1000 | Loss: 0.00006677
Iteration 112/1000 | Loss: 0.00006500
Iteration 113/1000 | Loss: 0.00006161
Iteration 114/1000 | Loss: 0.00006217
Iteration 115/1000 | Loss: 0.00005962
Iteration 116/1000 | Loss: 0.00005342
Iteration 117/1000 | Loss: 0.00006928
Iteration 118/1000 | Loss: 0.00006847
Iteration 119/1000 | Loss: 0.00007203
Iteration 120/1000 | Loss: 0.00006756
Iteration 121/1000 | Loss: 0.00005220
Iteration 122/1000 | Loss: 0.00006526
Iteration 123/1000 | Loss: 0.00006130
Iteration 124/1000 | Loss: 0.00005984
Iteration 125/1000 | Loss: 0.00006330
Iteration 126/1000 | Loss: 0.00006097
Iteration 127/1000 | Loss: 0.00006942
Iteration 128/1000 | Loss: 0.00006245
Iteration 129/1000 | Loss: 0.00005027
Iteration 130/1000 | Loss: 0.00006843
Iteration 131/1000 | Loss: 0.00006818
Iteration 132/1000 | Loss: 0.00006950
Iteration 133/1000 | Loss: 0.00007636
Iteration 134/1000 | Loss: 0.00006899
Iteration 135/1000 | Loss: 0.00007152
Iteration 136/1000 | Loss: 0.00005794
Iteration 137/1000 | Loss: 0.00005566
Iteration 138/1000 | Loss: 0.00005414
Iteration 139/1000 | Loss: 0.00005212
Iteration 140/1000 | Loss: 0.00008041
Iteration 141/1000 | Loss: 0.00008150
Iteration 142/1000 | Loss: 0.00006779
Iteration 143/1000 | Loss: 0.00007071
Iteration 144/1000 | Loss: 0.00006800
Iteration 145/1000 | Loss: 0.00006700
Iteration 146/1000 | Loss: 0.00005911
Iteration 147/1000 | Loss: 0.00007277
Iteration 148/1000 | Loss: 0.00006804
Iteration 149/1000 | Loss: 0.00003740
Iteration 150/1000 | Loss: 0.00004619
Iteration 151/1000 | Loss: 0.00006533
Iteration 152/1000 | Loss: 0.00006711
Iteration 153/1000 | Loss: 0.00005745
Iteration 154/1000 | Loss: 0.00005880
Iteration 155/1000 | Loss: 0.00006280
Iteration 156/1000 | Loss: 0.00005855
Iteration 157/1000 | Loss: 0.00006137
Iteration 158/1000 | Loss: 0.00005890
Iteration 159/1000 | Loss: 0.00005563
Iteration 160/1000 | Loss: 0.00006295
Iteration 161/1000 | Loss: 0.00006684
Iteration 162/1000 | Loss: 0.00005612
Iteration 163/1000 | Loss: 0.00006909
Iteration 164/1000 | Loss: 0.00006913
Iteration 165/1000 | Loss: 0.00005589
Iteration 166/1000 | Loss: 0.00004942
Iteration 167/1000 | Loss: 0.00004452
Iteration 168/1000 | Loss: 0.00004465
Iteration 169/1000 | Loss: 0.00004665
Iteration 170/1000 | Loss: 0.00004653
Iteration 171/1000 | Loss: 0.00003641
Iteration 172/1000 | Loss: 0.00003569
Iteration 173/1000 | Loss: 0.00006884
Iteration 174/1000 | Loss: 0.00005863
Iteration 175/1000 | Loss: 0.00007441
Iteration 176/1000 | Loss: 0.00006386
Iteration 177/1000 | Loss: 0.00003541
Iteration 178/1000 | Loss: 0.00006521
Iteration 179/1000 | Loss: 0.00006089
Iteration 180/1000 | Loss: 0.00004023
Iteration 181/1000 | Loss: 0.00004330
Iteration 182/1000 | Loss: 0.00005501
Iteration 183/1000 | Loss: 0.00004246
Iteration 184/1000 | Loss: 0.00003135
Iteration 185/1000 | Loss: 0.00002955
Iteration 186/1000 | Loss: 0.00003770
Iteration 187/1000 | Loss: 0.00003037
Iteration 188/1000 | Loss: 0.00002872
Iteration 189/1000 | Loss: 0.00003133
Iteration 190/1000 | Loss: 0.00002653
Iteration 191/1000 | Loss: 0.00004035
Iteration 192/1000 | Loss: 0.00004094
Iteration 193/1000 | Loss: 0.00004418
Iteration 194/1000 | Loss: 0.00003913
Iteration 195/1000 | Loss: 0.00003786
Iteration 196/1000 | Loss: 0.00003767
Iteration 197/1000 | Loss: 0.00003600
Iteration 198/1000 | Loss: 0.00003960
Iteration 199/1000 | Loss: 0.00004532
Iteration 200/1000 | Loss: 0.00003270
Iteration 201/1000 | Loss: 0.00003495
Iteration 202/1000 | Loss: 0.00003424
Iteration 203/1000 | Loss: 0.00003426
Iteration 204/1000 | Loss: 0.00003167
Iteration 205/1000 | Loss: 0.00003352
Iteration 206/1000 | Loss: 0.00003959
Iteration 207/1000 | Loss: 0.00003030
Iteration 208/1000 | Loss: 0.00003537
Iteration 209/1000 | Loss: 0.00002728
Iteration 210/1000 | Loss: 0.00002580
Iteration 211/1000 | Loss: 0.00002497
Iteration 212/1000 | Loss: 0.00002389
Iteration 213/1000 | Loss: 0.00002345
Iteration 214/1000 | Loss: 0.00002321
Iteration 215/1000 | Loss: 0.00002315
Iteration 216/1000 | Loss: 0.00002308
Iteration 217/1000 | Loss: 0.00002307
Iteration 218/1000 | Loss: 0.00002299
Iteration 219/1000 | Loss: 0.00002295
Iteration 220/1000 | Loss: 0.00002290
Iteration 221/1000 | Loss: 0.00002290
Iteration 222/1000 | Loss: 0.00002290
Iteration 223/1000 | Loss: 0.00002288
Iteration 224/1000 | Loss: 0.00002287
Iteration 225/1000 | Loss: 0.00002287
Iteration 226/1000 | Loss: 0.00002287
Iteration 227/1000 | Loss: 0.00002285
Iteration 228/1000 | Loss: 0.00002285
Iteration 229/1000 | Loss: 0.00002285
Iteration 230/1000 | Loss: 0.00002284
Iteration 231/1000 | Loss: 0.00002284
Iteration 232/1000 | Loss: 0.00002280
Iteration 233/1000 | Loss: 0.00002280
Iteration 234/1000 | Loss: 0.00002280
Iteration 235/1000 | Loss: 0.00002279
Iteration 236/1000 | Loss: 0.00002279
Iteration 237/1000 | Loss: 0.00002279
Iteration 238/1000 | Loss: 0.00002279
Iteration 239/1000 | Loss: 0.00002278
Iteration 240/1000 | Loss: 0.00002278
Iteration 241/1000 | Loss: 0.00002278
Iteration 242/1000 | Loss: 0.00002278
Iteration 243/1000 | Loss: 0.00002278
Iteration 244/1000 | Loss: 0.00002278
Iteration 245/1000 | Loss: 0.00002277
Iteration 246/1000 | Loss: 0.00002277
Iteration 247/1000 | Loss: 0.00002277
Iteration 248/1000 | Loss: 0.00002276
Iteration 249/1000 | Loss: 0.00002276
Iteration 250/1000 | Loss: 0.00002276
Iteration 251/1000 | Loss: 0.00002276
Iteration 252/1000 | Loss: 0.00002276
Iteration 253/1000 | Loss: 0.00002276
Iteration 254/1000 | Loss: 0.00002275
Iteration 255/1000 | Loss: 0.00002275
Iteration 256/1000 | Loss: 0.00002275
Iteration 257/1000 | Loss: 0.00002274
Iteration 258/1000 | Loss: 0.00002274
Iteration 259/1000 | Loss: 0.00002274
Iteration 260/1000 | Loss: 0.00002274
Iteration 261/1000 | Loss: 0.00002274
Iteration 262/1000 | Loss: 0.00002274
Iteration 263/1000 | Loss: 0.00002274
Iteration 264/1000 | Loss: 0.00002274
Iteration 265/1000 | Loss: 0.00002274
Iteration 266/1000 | Loss: 0.00002274
Iteration 267/1000 | Loss: 0.00002273
Iteration 268/1000 | Loss: 0.00002273
Iteration 269/1000 | Loss: 0.00002273
Iteration 270/1000 | Loss: 0.00002273
Iteration 271/1000 | Loss: 0.00002273
Iteration 272/1000 | Loss: 0.00002273
Iteration 273/1000 | Loss: 0.00002273
Iteration 274/1000 | Loss: 0.00002273
Iteration 275/1000 | Loss: 0.00002273
Iteration 276/1000 | Loss: 0.00002272
Iteration 277/1000 | Loss: 0.00002272
Iteration 278/1000 | Loss: 0.00002272
Iteration 279/1000 | Loss: 0.00002272
Iteration 280/1000 | Loss: 0.00002272
Iteration 281/1000 | Loss: 0.00002272
Iteration 282/1000 | Loss: 0.00002272
Iteration 283/1000 | Loss: 0.00002272
Iteration 284/1000 | Loss: 0.00002272
Iteration 285/1000 | Loss: 0.00002272
Iteration 286/1000 | Loss: 0.00002272
Iteration 287/1000 | Loss: 0.00002272
Iteration 288/1000 | Loss: 0.00002272
Iteration 289/1000 | Loss: 0.00002272
Iteration 290/1000 | Loss: 0.00002271
Iteration 291/1000 | Loss: 0.00002271
Iteration 292/1000 | Loss: 0.00002271
Iteration 293/1000 | Loss: 0.00002271
Iteration 294/1000 | Loss: 0.00002271
Iteration 295/1000 | Loss: 0.00002271
Iteration 296/1000 | Loss: 0.00002270
Iteration 297/1000 | Loss: 0.00002270
Iteration 298/1000 | Loss: 0.00002270
Iteration 299/1000 | Loss: 0.00002270
Iteration 300/1000 | Loss: 0.00002270
Iteration 301/1000 | Loss: 0.00002269
Iteration 302/1000 | Loss: 0.00002269
Iteration 303/1000 | Loss: 0.00002269
Iteration 304/1000 | Loss: 0.00002269
Iteration 305/1000 | Loss: 0.00002269
Iteration 306/1000 | Loss: 0.00002269
Iteration 307/1000 | Loss: 0.00002269
Iteration 308/1000 | Loss: 0.00002269
Iteration 309/1000 | Loss: 0.00002269
Iteration 310/1000 | Loss: 0.00002269
Iteration 311/1000 | Loss: 0.00002268
Iteration 312/1000 | Loss: 0.00002268
Iteration 313/1000 | Loss: 0.00002268
Iteration 314/1000 | Loss: 0.00002268
Iteration 315/1000 | Loss: 0.00002268
Iteration 316/1000 | Loss: 0.00002268
Iteration 317/1000 | Loss: 0.00002268
Iteration 318/1000 | Loss: 0.00002267
Iteration 319/1000 | Loss: 0.00002267
Iteration 320/1000 | Loss: 0.00002267
Iteration 321/1000 | Loss: 0.00002267
Iteration 322/1000 | Loss: 0.00002267
Iteration 323/1000 | Loss: 0.00002267
Iteration 324/1000 | Loss: 0.00002267
Iteration 325/1000 | Loss: 0.00002266
Iteration 326/1000 | Loss: 0.00002266
Iteration 327/1000 | Loss: 0.00002266
Iteration 328/1000 | Loss: 0.00002266
Iteration 329/1000 | Loss: 0.00002266
Iteration 330/1000 | Loss: 0.00002266
Iteration 331/1000 | Loss: 0.00002266
Iteration 332/1000 | Loss: 0.00002266
Iteration 333/1000 | Loss: 0.00002265
Iteration 334/1000 | Loss: 0.00002265
Iteration 335/1000 | Loss: 0.00002265
Iteration 336/1000 | Loss: 0.00002265
Iteration 337/1000 | Loss: 0.00002265
Iteration 338/1000 | Loss: 0.00002265
Iteration 339/1000 | Loss: 0.00002265
Iteration 340/1000 | Loss: 0.00002265
Iteration 341/1000 | Loss: 0.00002265
Iteration 342/1000 | Loss: 0.00002265
Iteration 343/1000 | Loss: 0.00002265
Iteration 344/1000 | Loss: 0.00002265
Iteration 345/1000 | Loss: 0.00002265
Iteration 346/1000 | Loss: 0.00002264
Iteration 347/1000 | Loss: 0.00002264
Iteration 348/1000 | Loss: 0.00002264
Iteration 349/1000 | Loss: 0.00002264
Iteration 350/1000 | Loss: 0.00002264
Iteration 351/1000 | Loss: 0.00002264
Iteration 352/1000 | Loss: 0.00002264
Iteration 353/1000 | Loss: 0.00002264
Iteration 354/1000 | Loss: 0.00002264
Iteration 355/1000 | Loss: 0.00002264
Iteration 356/1000 | Loss: 0.00002264
Iteration 357/1000 | Loss: 0.00002264
Iteration 358/1000 | Loss: 0.00002264
Iteration 359/1000 | Loss: 0.00002264
Iteration 360/1000 | Loss: 0.00002263
Iteration 361/1000 | Loss: 0.00002263
Iteration 362/1000 | Loss: 0.00002263
Iteration 363/1000 | Loss: 0.00002263
Iteration 364/1000 | Loss: 0.00002263
Iteration 365/1000 | Loss: 0.00002263
Iteration 366/1000 | Loss: 0.00002263
Iteration 367/1000 | Loss: 0.00002263
Iteration 368/1000 | Loss: 0.00002263
Iteration 369/1000 | Loss: 0.00002263
Iteration 370/1000 | Loss: 0.00002263
Iteration 371/1000 | Loss: 0.00002263
Iteration 372/1000 | Loss: 0.00002263
Iteration 373/1000 | Loss: 0.00002263
Iteration 374/1000 | Loss: 0.00002263
Iteration 375/1000 | Loss: 0.00002263
Iteration 376/1000 | Loss: 0.00002263
Iteration 377/1000 | Loss: 0.00002262
Iteration 378/1000 | Loss: 0.00002262
Iteration 379/1000 | Loss: 0.00002262
Iteration 380/1000 | Loss: 0.00002262
Iteration 381/1000 | Loss: 0.00002262
Iteration 382/1000 | Loss: 0.00002262
Iteration 383/1000 | Loss: 0.00002262
Iteration 384/1000 | Loss: 0.00002262
Iteration 385/1000 | Loss: 0.00002262
Iteration 386/1000 | Loss: 0.00002262
Iteration 387/1000 | Loss: 0.00002262
Iteration 388/1000 | Loss: 0.00002262
Iteration 389/1000 | Loss: 0.00002262
Iteration 390/1000 | Loss: 0.00002262
Iteration 391/1000 | Loss: 0.00002262
Iteration 392/1000 | Loss: 0.00002261
Iteration 393/1000 | Loss: 0.00002261
Iteration 394/1000 | Loss: 0.00002261
Iteration 395/1000 | Loss: 0.00002261
Iteration 396/1000 | Loss: 0.00002261
Iteration 397/1000 | Loss: 0.00002261
Iteration 398/1000 | Loss: 0.00002261
Iteration 399/1000 | Loss: 0.00002261
Iteration 400/1000 | Loss: 0.00002261
Iteration 401/1000 | Loss: 0.00002261
Iteration 402/1000 | Loss: 0.00002261
Iteration 403/1000 | Loss: 0.00002261
Iteration 404/1000 | Loss: 0.00002261
Iteration 405/1000 | Loss: 0.00002261
Iteration 406/1000 | Loss: 0.00002261
Iteration 407/1000 | Loss: 0.00002261
Iteration 408/1000 | Loss: 0.00002261
Iteration 409/1000 | Loss: 0.00002260
Iteration 410/1000 | Loss: 0.00002260
Iteration 411/1000 | Loss: 0.00002260
Iteration 412/1000 | Loss: 0.00002260
Iteration 413/1000 | Loss: 0.00002260
Iteration 414/1000 | Loss: 0.00002260
Iteration 415/1000 | Loss: 0.00002259
Iteration 416/1000 | Loss: 0.00002259
Iteration 417/1000 | Loss: 0.00002259
Iteration 418/1000 | Loss: 0.00002259
Iteration 419/1000 | Loss: 0.00002259
Iteration 420/1000 | Loss: 0.00002259
Iteration 421/1000 | Loss: 0.00002259
Iteration 422/1000 | Loss: 0.00002259
Iteration 423/1000 | Loss: 0.00002259
Iteration 424/1000 | Loss: 0.00002259
Iteration 425/1000 | Loss: 0.00002259
Iteration 426/1000 | Loss: 0.00002259
Iteration 427/1000 | Loss: 0.00002259
Iteration 428/1000 | Loss: 0.00002259
Iteration 429/1000 | Loss: 0.00002259
Iteration 430/1000 | Loss: 0.00002259
Iteration 431/1000 | Loss: 0.00002259
Iteration 432/1000 | Loss: 0.00002259
Iteration 433/1000 | Loss: 0.00002259
Iteration 434/1000 | Loss: 0.00002259
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 434. Stopping optimization.
Last 5 losses: [2.2589803847949952e-05, 2.2589803847949952e-05, 2.2589803847949952e-05, 2.2589803847949952e-05, 2.2589803847949952e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2589803847949952e-05

Optimization complete. Final v2v error: 3.756462574005127 mm

Highest mean error: 4.952369213104248 mm for frame 112

Lowest mean error: 3.447313070297241 mm for frame 1

Saving results

Total time: 413.74559688568115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813885
Iteration 2/25 | Loss: 0.00130645
Iteration 3/25 | Loss: 0.00111335
Iteration 4/25 | Loss: 0.00110097
Iteration 5/25 | Loss: 0.00109822
Iteration 6/25 | Loss: 0.00109815
Iteration 7/25 | Loss: 0.00109815
Iteration 8/25 | Loss: 0.00109815
Iteration 9/25 | Loss: 0.00109815
Iteration 10/25 | Loss: 0.00109815
Iteration 11/25 | Loss: 0.00109815
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010981500381603837, 0.0010981500381603837, 0.0010981500381603837, 0.0010981500381603837, 0.0010981500381603837]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010981500381603837

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40262365
Iteration 2/25 | Loss: 0.00062076
Iteration 3/25 | Loss: 0.00062076
Iteration 4/25 | Loss: 0.00062076
Iteration 5/25 | Loss: 0.00062076
Iteration 6/25 | Loss: 0.00062076
Iteration 7/25 | Loss: 0.00062076
Iteration 8/25 | Loss: 0.00062076
Iteration 9/25 | Loss: 0.00062076
Iteration 10/25 | Loss: 0.00062076
Iteration 11/25 | Loss: 0.00062076
Iteration 12/25 | Loss: 0.00062076
Iteration 13/25 | Loss: 0.00062076
Iteration 14/25 | Loss: 0.00062076
Iteration 15/25 | Loss: 0.00062076
Iteration 16/25 | Loss: 0.00062076
Iteration 17/25 | Loss: 0.00062076
Iteration 18/25 | Loss: 0.00062076
Iteration 19/25 | Loss: 0.00062076
Iteration 20/25 | Loss: 0.00062076
Iteration 21/25 | Loss: 0.00062076
Iteration 22/25 | Loss: 0.00062076
Iteration 23/25 | Loss: 0.00062076
Iteration 24/25 | Loss: 0.00062076
Iteration 25/25 | Loss: 0.00062076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00062076
Iteration 2/1000 | Loss: 0.00002352
Iteration 3/1000 | Loss: 0.00001433
Iteration 4/1000 | Loss: 0.00001279
Iteration 5/1000 | Loss: 0.00001216
Iteration 6/1000 | Loss: 0.00001165
Iteration 7/1000 | Loss: 0.00001132
Iteration 8/1000 | Loss: 0.00001104
Iteration 9/1000 | Loss: 0.00001070
Iteration 10/1000 | Loss: 0.00001055
Iteration 11/1000 | Loss: 0.00001046
Iteration 12/1000 | Loss: 0.00001044
Iteration 13/1000 | Loss: 0.00001041
Iteration 14/1000 | Loss: 0.00001035
Iteration 15/1000 | Loss: 0.00001033
Iteration 16/1000 | Loss: 0.00001032
Iteration 17/1000 | Loss: 0.00001032
Iteration 18/1000 | Loss: 0.00001032
Iteration 19/1000 | Loss: 0.00001031
Iteration 20/1000 | Loss: 0.00001031
Iteration 21/1000 | Loss: 0.00001031
Iteration 22/1000 | Loss: 0.00001031
Iteration 23/1000 | Loss: 0.00001030
Iteration 24/1000 | Loss: 0.00001030
Iteration 25/1000 | Loss: 0.00001030
Iteration 26/1000 | Loss: 0.00001029
Iteration 27/1000 | Loss: 0.00001029
Iteration 28/1000 | Loss: 0.00001029
Iteration 29/1000 | Loss: 0.00001028
Iteration 30/1000 | Loss: 0.00001027
Iteration 31/1000 | Loss: 0.00001027
Iteration 32/1000 | Loss: 0.00001027
Iteration 33/1000 | Loss: 0.00001027
Iteration 34/1000 | Loss: 0.00001026
Iteration 35/1000 | Loss: 0.00001026
Iteration 36/1000 | Loss: 0.00001025
Iteration 37/1000 | Loss: 0.00001025
Iteration 38/1000 | Loss: 0.00001024
Iteration 39/1000 | Loss: 0.00001022
Iteration 40/1000 | Loss: 0.00001022
Iteration 41/1000 | Loss: 0.00001022
Iteration 42/1000 | Loss: 0.00001021
Iteration 43/1000 | Loss: 0.00001021
Iteration 44/1000 | Loss: 0.00001021
Iteration 45/1000 | Loss: 0.00001020
Iteration 46/1000 | Loss: 0.00001020
Iteration 47/1000 | Loss: 0.00001020
Iteration 48/1000 | Loss: 0.00001019
Iteration 49/1000 | Loss: 0.00001018
Iteration 50/1000 | Loss: 0.00001018
Iteration 51/1000 | Loss: 0.00001017
Iteration 52/1000 | Loss: 0.00001017
Iteration 53/1000 | Loss: 0.00001017
Iteration 54/1000 | Loss: 0.00001017
Iteration 55/1000 | Loss: 0.00001017
Iteration 56/1000 | Loss: 0.00001016
Iteration 57/1000 | Loss: 0.00001016
Iteration 58/1000 | Loss: 0.00001016
Iteration 59/1000 | Loss: 0.00001015
Iteration 60/1000 | Loss: 0.00001015
Iteration 61/1000 | Loss: 0.00001015
Iteration 62/1000 | Loss: 0.00001015
Iteration 63/1000 | Loss: 0.00001015
Iteration 64/1000 | Loss: 0.00001014
Iteration 65/1000 | Loss: 0.00001014
Iteration 66/1000 | Loss: 0.00001014
Iteration 67/1000 | Loss: 0.00001013
Iteration 68/1000 | Loss: 0.00001013
Iteration 69/1000 | Loss: 0.00001013
Iteration 70/1000 | Loss: 0.00001013
Iteration 71/1000 | Loss: 0.00001012
Iteration 72/1000 | Loss: 0.00001012
Iteration 73/1000 | Loss: 0.00001012
Iteration 74/1000 | Loss: 0.00001012
Iteration 75/1000 | Loss: 0.00001012
Iteration 76/1000 | Loss: 0.00001012
Iteration 77/1000 | Loss: 0.00001012
Iteration 78/1000 | Loss: 0.00001012
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.0123377251147758e-05, 1.0123377251147758e-05, 1.0123377251147758e-05, 1.0123377251147758e-05, 1.0123377251147758e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0123377251147758e-05

Optimization complete. Final v2v error: 2.7390708923339844 mm

Highest mean error: 2.9923031330108643 mm for frame 114

Lowest mean error: 2.595355987548828 mm for frame 60

Saving results

Total time: 33.60720348358154
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00923568
Iteration 2/25 | Loss: 0.00135357
Iteration 3/25 | Loss: 0.00124643
Iteration 4/25 | Loss: 0.00123517
Iteration 5/25 | Loss: 0.00123222
Iteration 6/25 | Loss: 0.00123212
Iteration 7/25 | Loss: 0.00123212
Iteration 8/25 | Loss: 0.00123212
Iteration 9/25 | Loss: 0.00123212
Iteration 10/25 | Loss: 0.00123212
Iteration 11/25 | Loss: 0.00123212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012321172980591655, 0.0012321172980591655, 0.0012321172980591655, 0.0012321172980591655, 0.0012321172980591655]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012321172980591655

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01949501
Iteration 2/25 | Loss: 0.00080855
Iteration 3/25 | Loss: 0.00080854
Iteration 4/25 | Loss: 0.00080854
Iteration 5/25 | Loss: 0.00080854
Iteration 6/25 | Loss: 0.00080854
Iteration 7/25 | Loss: 0.00080854
Iteration 8/25 | Loss: 0.00080854
Iteration 9/25 | Loss: 0.00080854
Iteration 10/25 | Loss: 0.00080854
Iteration 11/25 | Loss: 0.00080854
Iteration 12/25 | Loss: 0.00080854
Iteration 13/25 | Loss: 0.00080854
Iteration 14/25 | Loss: 0.00080854
Iteration 15/25 | Loss: 0.00080854
Iteration 16/25 | Loss: 0.00080854
Iteration 17/25 | Loss: 0.00080854
Iteration 18/25 | Loss: 0.00080854
Iteration 19/25 | Loss: 0.00080854
Iteration 20/25 | Loss: 0.00080854
Iteration 21/25 | Loss: 0.00080854
Iteration 22/25 | Loss: 0.00080854
Iteration 23/25 | Loss: 0.00080854
Iteration 24/25 | Loss: 0.00080854
Iteration 25/25 | Loss: 0.00080854
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008085420704446733, 0.0008085420704446733, 0.0008085420704446733, 0.0008085420704446733, 0.0008085420704446733]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008085420704446733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080854
Iteration 2/1000 | Loss: 0.00004739
Iteration 3/1000 | Loss: 0.00002941
Iteration 4/1000 | Loss: 0.00002584
Iteration 5/1000 | Loss: 0.00002449
Iteration 6/1000 | Loss: 0.00002376
Iteration 7/1000 | Loss: 0.00002335
Iteration 8/1000 | Loss: 0.00002300
Iteration 9/1000 | Loss: 0.00002274
Iteration 10/1000 | Loss: 0.00002251
Iteration 11/1000 | Loss: 0.00002224
Iteration 12/1000 | Loss: 0.00002208
Iteration 13/1000 | Loss: 0.00002193
Iteration 14/1000 | Loss: 0.00002187
Iteration 15/1000 | Loss: 0.00002187
Iteration 16/1000 | Loss: 0.00002186
Iteration 17/1000 | Loss: 0.00002185
Iteration 18/1000 | Loss: 0.00002184
Iteration 19/1000 | Loss: 0.00002184
Iteration 20/1000 | Loss: 0.00002184
Iteration 21/1000 | Loss: 0.00002183
Iteration 22/1000 | Loss: 0.00002183
Iteration 23/1000 | Loss: 0.00002183
Iteration 24/1000 | Loss: 0.00002183
Iteration 25/1000 | Loss: 0.00002183
Iteration 26/1000 | Loss: 0.00002183
Iteration 27/1000 | Loss: 0.00002183
Iteration 28/1000 | Loss: 0.00002183
Iteration 29/1000 | Loss: 0.00002183
Iteration 30/1000 | Loss: 0.00002182
Iteration 31/1000 | Loss: 0.00002180
Iteration 32/1000 | Loss: 0.00002180
Iteration 33/1000 | Loss: 0.00002179
Iteration 34/1000 | Loss: 0.00002179
Iteration 35/1000 | Loss: 0.00002178
Iteration 36/1000 | Loss: 0.00002178
Iteration 37/1000 | Loss: 0.00002177
Iteration 38/1000 | Loss: 0.00002177
Iteration 39/1000 | Loss: 0.00002177
Iteration 40/1000 | Loss: 0.00002177
Iteration 41/1000 | Loss: 0.00002176
Iteration 42/1000 | Loss: 0.00002176
Iteration 43/1000 | Loss: 0.00002175
Iteration 44/1000 | Loss: 0.00002175
Iteration 45/1000 | Loss: 0.00002175
Iteration 46/1000 | Loss: 0.00002174
Iteration 47/1000 | Loss: 0.00002174
Iteration 48/1000 | Loss: 0.00002174
Iteration 49/1000 | Loss: 0.00002174
Iteration 50/1000 | Loss: 0.00002173
Iteration 51/1000 | Loss: 0.00002173
Iteration 52/1000 | Loss: 0.00002173
Iteration 53/1000 | Loss: 0.00002173
Iteration 54/1000 | Loss: 0.00002173
Iteration 55/1000 | Loss: 0.00002173
Iteration 56/1000 | Loss: 0.00002173
Iteration 57/1000 | Loss: 0.00002173
Iteration 58/1000 | Loss: 0.00002173
Iteration 59/1000 | Loss: 0.00002173
Iteration 60/1000 | Loss: 0.00002172
Iteration 61/1000 | Loss: 0.00002172
Iteration 62/1000 | Loss: 0.00002172
Iteration 63/1000 | Loss: 0.00002171
Iteration 64/1000 | Loss: 0.00002170
Iteration 65/1000 | Loss: 0.00002170
Iteration 66/1000 | Loss: 0.00002170
Iteration 67/1000 | Loss: 0.00002170
Iteration 68/1000 | Loss: 0.00002170
Iteration 69/1000 | Loss: 0.00002170
Iteration 70/1000 | Loss: 0.00002170
Iteration 71/1000 | Loss: 0.00002170
Iteration 72/1000 | Loss: 0.00002170
Iteration 73/1000 | Loss: 0.00002169
Iteration 74/1000 | Loss: 0.00002169
Iteration 75/1000 | Loss: 0.00002169
Iteration 76/1000 | Loss: 0.00002169
Iteration 77/1000 | Loss: 0.00002169
Iteration 78/1000 | Loss: 0.00002168
Iteration 79/1000 | Loss: 0.00002167
Iteration 80/1000 | Loss: 0.00002167
Iteration 81/1000 | Loss: 0.00002167
Iteration 82/1000 | Loss: 0.00002166
Iteration 83/1000 | Loss: 0.00002166
Iteration 84/1000 | Loss: 0.00002166
Iteration 85/1000 | Loss: 0.00002165
Iteration 86/1000 | Loss: 0.00002165
Iteration 87/1000 | Loss: 0.00002165
Iteration 88/1000 | Loss: 0.00002165
Iteration 89/1000 | Loss: 0.00002164
Iteration 90/1000 | Loss: 0.00002164
Iteration 91/1000 | Loss: 0.00002164
Iteration 92/1000 | Loss: 0.00002164
Iteration 93/1000 | Loss: 0.00002164
Iteration 94/1000 | Loss: 0.00002163
Iteration 95/1000 | Loss: 0.00002163
Iteration 96/1000 | Loss: 0.00002163
Iteration 97/1000 | Loss: 0.00002163
Iteration 98/1000 | Loss: 0.00002163
Iteration 99/1000 | Loss: 0.00002163
Iteration 100/1000 | Loss: 0.00002163
Iteration 101/1000 | Loss: 0.00002162
Iteration 102/1000 | Loss: 0.00002162
Iteration 103/1000 | Loss: 0.00002162
Iteration 104/1000 | Loss: 0.00002162
Iteration 105/1000 | Loss: 0.00002162
Iteration 106/1000 | Loss: 0.00002162
Iteration 107/1000 | Loss: 0.00002162
Iteration 108/1000 | Loss: 0.00002162
Iteration 109/1000 | Loss: 0.00002161
Iteration 110/1000 | Loss: 0.00002161
Iteration 111/1000 | Loss: 0.00002161
Iteration 112/1000 | Loss: 0.00002161
Iteration 113/1000 | Loss: 0.00002161
Iteration 114/1000 | Loss: 0.00002161
Iteration 115/1000 | Loss: 0.00002161
Iteration 116/1000 | Loss: 0.00002161
Iteration 117/1000 | Loss: 0.00002161
Iteration 118/1000 | Loss: 0.00002160
Iteration 119/1000 | Loss: 0.00002160
Iteration 120/1000 | Loss: 0.00002160
Iteration 121/1000 | Loss: 0.00002159
Iteration 122/1000 | Loss: 0.00002159
Iteration 123/1000 | Loss: 0.00002159
Iteration 124/1000 | Loss: 0.00002159
Iteration 125/1000 | Loss: 0.00002158
Iteration 126/1000 | Loss: 0.00002158
Iteration 127/1000 | Loss: 0.00002158
Iteration 128/1000 | Loss: 0.00002157
Iteration 129/1000 | Loss: 0.00002157
Iteration 130/1000 | Loss: 0.00002157
Iteration 131/1000 | Loss: 0.00002157
Iteration 132/1000 | Loss: 0.00002157
Iteration 133/1000 | Loss: 0.00002156
Iteration 134/1000 | Loss: 0.00002156
Iteration 135/1000 | Loss: 0.00002156
Iteration 136/1000 | Loss: 0.00002156
Iteration 137/1000 | Loss: 0.00002156
Iteration 138/1000 | Loss: 0.00002155
Iteration 139/1000 | Loss: 0.00002155
Iteration 140/1000 | Loss: 0.00002155
Iteration 141/1000 | Loss: 0.00002154
Iteration 142/1000 | Loss: 0.00002154
Iteration 143/1000 | Loss: 0.00002154
Iteration 144/1000 | Loss: 0.00002154
Iteration 145/1000 | Loss: 0.00002154
Iteration 146/1000 | Loss: 0.00002154
Iteration 147/1000 | Loss: 0.00002154
Iteration 148/1000 | Loss: 0.00002154
Iteration 149/1000 | Loss: 0.00002154
Iteration 150/1000 | Loss: 0.00002153
Iteration 151/1000 | Loss: 0.00002153
Iteration 152/1000 | Loss: 0.00002153
Iteration 153/1000 | Loss: 0.00002153
Iteration 154/1000 | Loss: 0.00002153
Iteration 155/1000 | Loss: 0.00002153
Iteration 156/1000 | Loss: 0.00002153
Iteration 157/1000 | Loss: 0.00002153
Iteration 158/1000 | Loss: 0.00002153
Iteration 159/1000 | Loss: 0.00002153
Iteration 160/1000 | Loss: 0.00002153
Iteration 161/1000 | Loss: 0.00002153
Iteration 162/1000 | Loss: 0.00002153
Iteration 163/1000 | Loss: 0.00002153
Iteration 164/1000 | Loss: 0.00002153
Iteration 165/1000 | Loss: 0.00002153
Iteration 166/1000 | Loss: 0.00002153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [2.1525982447201386e-05, 2.1525982447201386e-05, 2.1525982447201386e-05, 2.1525982447201386e-05, 2.1525982447201386e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1525982447201386e-05

Optimization complete. Final v2v error: 3.788217067718506 mm

Highest mean error: 4.486541748046875 mm for frame 81

Lowest mean error: 3.2831127643585205 mm for frame 0

Saving results

Total time: 37.246726512908936
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00713243
Iteration 2/25 | Loss: 0.00138511
Iteration 3/25 | Loss: 0.00123892
Iteration 4/25 | Loss: 0.00121567
Iteration 5/25 | Loss: 0.00120710
Iteration 6/25 | Loss: 0.00120594
Iteration 7/25 | Loss: 0.00120594
Iteration 8/25 | Loss: 0.00120594
Iteration 9/25 | Loss: 0.00120594
Iteration 10/25 | Loss: 0.00120594
Iteration 11/25 | Loss: 0.00120594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012059388682246208, 0.0012059388682246208, 0.0012059388682246208, 0.0012059388682246208, 0.0012059388682246208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012059388682246208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.12748551
Iteration 2/25 | Loss: 0.00080856
Iteration 3/25 | Loss: 0.00080855
Iteration 4/25 | Loss: 0.00080855
Iteration 5/25 | Loss: 0.00080855
Iteration 6/25 | Loss: 0.00080855
Iteration 7/25 | Loss: 0.00080855
Iteration 8/25 | Loss: 0.00080855
Iteration 9/25 | Loss: 0.00080855
Iteration 10/25 | Loss: 0.00080855
Iteration 11/25 | Loss: 0.00080855
Iteration 12/25 | Loss: 0.00080855
Iteration 13/25 | Loss: 0.00080855
Iteration 14/25 | Loss: 0.00080855
Iteration 15/25 | Loss: 0.00080855
Iteration 16/25 | Loss: 0.00080855
Iteration 17/25 | Loss: 0.00080855
Iteration 18/25 | Loss: 0.00080855
Iteration 19/25 | Loss: 0.00080855
Iteration 20/25 | Loss: 0.00080855
Iteration 21/25 | Loss: 0.00080855
Iteration 22/25 | Loss: 0.00080855
Iteration 23/25 | Loss: 0.00080855
Iteration 24/25 | Loss: 0.00080855
Iteration 25/25 | Loss: 0.00080855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080855
Iteration 2/1000 | Loss: 0.00003260
Iteration 3/1000 | Loss: 0.00002648
Iteration 4/1000 | Loss: 0.00002480
Iteration 5/1000 | Loss: 0.00002388
Iteration 6/1000 | Loss: 0.00002347
Iteration 7/1000 | Loss: 0.00002315
Iteration 8/1000 | Loss: 0.00002269
Iteration 9/1000 | Loss: 0.00002223
Iteration 10/1000 | Loss: 0.00002197
Iteration 11/1000 | Loss: 0.00002176
Iteration 12/1000 | Loss: 0.00002168
Iteration 13/1000 | Loss: 0.00002160
Iteration 14/1000 | Loss: 0.00002159
Iteration 15/1000 | Loss: 0.00002144
Iteration 16/1000 | Loss: 0.00002132
Iteration 17/1000 | Loss: 0.00002130
Iteration 18/1000 | Loss: 0.00002127
Iteration 19/1000 | Loss: 0.00002121
Iteration 20/1000 | Loss: 0.00002121
Iteration 21/1000 | Loss: 0.00002120
Iteration 22/1000 | Loss: 0.00002120
Iteration 23/1000 | Loss: 0.00002118
Iteration 24/1000 | Loss: 0.00002115
Iteration 25/1000 | Loss: 0.00002114
Iteration 26/1000 | Loss: 0.00002113
Iteration 27/1000 | Loss: 0.00002112
Iteration 28/1000 | Loss: 0.00002111
Iteration 29/1000 | Loss: 0.00002110
Iteration 30/1000 | Loss: 0.00002102
Iteration 31/1000 | Loss: 0.00002101
Iteration 32/1000 | Loss: 0.00002101
Iteration 33/1000 | Loss: 0.00002101
Iteration 34/1000 | Loss: 0.00002101
Iteration 35/1000 | Loss: 0.00002101
Iteration 36/1000 | Loss: 0.00002099
Iteration 37/1000 | Loss: 0.00002098
Iteration 38/1000 | Loss: 0.00002097
Iteration 39/1000 | Loss: 0.00002095
Iteration 40/1000 | Loss: 0.00002095
Iteration 41/1000 | Loss: 0.00002095
Iteration 42/1000 | Loss: 0.00002095
Iteration 43/1000 | Loss: 0.00002095
Iteration 44/1000 | Loss: 0.00002095
Iteration 45/1000 | Loss: 0.00002095
Iteration 46/1000 | Loss: 0.00002095
Iteration 47/1000 | Loss: 0.00002094
Iteration 48/1000 | Loss: 0.00002094
Iteration 49/1000 | Loss: 0.00002094
Iteration 50/1000 | Loss: 0.00002092
Iteration 51/1000 | Loss: 0.00002092
Iteration 52/1000 | Loss: 0.00002092
Iteration 53/1000 | Loss: 0.00002092
Iteration 54/1000 | Loss: 0.00002092
Iteration 55/1000 | Loss: 0.00002091
Iteration 56/1000 | Loss: 0.00002091
Iteration 57/1000 | Loss: 0.00002091
Iteration 58/1000 | Loss: 0.00002091
Iteration 59/1000 | Loss: 0.00002090
Iteration 60/1000 | Loss: 0.00002090
Iteration 61/1000 | Loss: 0.00002089
Iteration 62/1000 | Loss: 0.00002089
Iteration 63/1000 | Loss: 0.00002089
Iteration 64/1000 | Loss: 0.00002088
Iteration 65/1000 | Loss: 0.00002088
Iteration 66/1000 | Loss: 0.00002088
Iteration 67/1000 | Loss: 0.00002088
Iteration 68/1000 | Loss: 0.00002088
Iteration 69/1000 | Loss: 0.00002087
Iteration 70/1000 | Loss: 0.00002087
Iteration 71/1000 | Loss: 0.00002087
Iteration 72/1000 | Loss: 0.00002087
Iteration 73/1000 | Loss: 0.00002087
Iteration 74/1000 | Loss: 0.00002087
Iteration 75/1000 | Loss: 0.00002087
Iteration 76/1000 | Loss: 0.00002087
Iteration 77/1000 | Loss: 0.00002086
Iteration 78/1000 | Loss: 0.00002086
Iteration 79/1000 | Loss: 0.00002086
Iteration 80/1000 | Loss: 0.00002086
Iteration 81/1000 | Loss: 0.00002085
Iteration 82/1000 | Loss: 0.00002085
Iteration 83/1000 | Loss: 0.00002085
Iteration 84/1000 | Loss: 0.00002085
Iteration 85/1000 | Loss: 0.00002084
Iteration 86/1000 | Loss: 0.00002084
Iteration 87/1000 | Loss: 0.00002084
Iteration 88/1000 | Loss: 0.00002084
Iteration 89/1000 | Loss: 0.00002084
Iteration 90/1000 | Loss: 0.00002084
Iteration 91/1000 | Loss: 0.00002084
Iteration 92/1000 | Loss: 0.00002084
Iteration 93/1000 | Loss: 0.00002084
Iteration 94/1000 | Loss: 0.00002084
Iteration 95/1000 | Loss: 0.00002084
Iteration 96/1000 | Loss: 0.00002084
Iteration 97/1000 | Loss: 0.00002084
Iteration 98/1000 | Loss: 0.00002084
Iteration 99/1000 | Loss: 0.00002084
Iteration 100/1000 | Loss: 0.00002083
Iteration 101/1000 | Loss: 0.00002083
Iteration 102/1000 | Loss: 0.00002083
Iteration 103/1000 | Loss: 0.00002083
Iteration 104/1000 | Loss: 0.00002083
Iteration 105/1000 | Loss: 0.00002083
Iteration 106/1000 | Loss: 0.00002083
Iteration 107/1000 | Loss: 0.00002083
Iteration 108/1000 | Loss: 0.00002083
Iteration 109/1000 | Loss: 0.00002083
Iteration 110/1000 | Loss: 0.00002083
Iteration 111/1000 | Loss: 0.00002083
Iteration 112/1000 | Loss: 0.00002083
Iteration 113/1000 | Loss: 0.00002083
Iteration 114/1000 | Loss: 0.00002083
Iteration 115/1000 | Loss: 0.00002083
Iteration 116/1000 | Loss: 0.00002083
Iteration 117/1000 | Loss: 0.00002083
Iteration 118/1000 | Loss: 0.00002083
Iteration 119/1000 | Loss: 0.00002083
Iteration 120/1000 | Loss: 0.00002083
Iteration 121/1000 | Loss: 0.00002083
Iteration 122/1000 | Loss: 0.00002083
Iteration 123/1000 | Loss: 0.00002083
Iteration 124/1000 | Loss: 0.00002083
Iteration 125/1000 | Loss: 0.00002083
Iteration 126/1000 | Loss: 0.00002083
Iteration 127/1000 | Loss: 0.00002083
Iteration 128/1000 | Loss: 0.00002083
Iteration 129/1000 | Loss: 0.00002083
Iteration 130/1000 | Loss: 0.00002083
Iteration 131/1000 | Loss: 0.00002083
Iteration 132/1000 | Loss: 0.00002083
Iteration 133/1000 | Loss: 0.00002083
Iteration 134/1000 | Loss: 0.00002083
Iteration 135/1000 | Loss: 0.00002083
Iteration 136/1000 | Loss: 0.00002083
Iteration 137/1000 | Loss: 0.00002083
Iteration 138/1000 | Loss: 0.00002083
Iteration 139/1000 | Loss: 0.00002083
Iteration 140/1000 | Loss: 0.00002083
Iteration 141/1000 | Loss: 0.00002083
Iteration 142/1000 | Loss: 0.00002083
Iteration 143/1000 | Loss: 0.00002083
Iteration 144/1000 | Loss: 0.00002083
Iteration 145/1000 | Loss: 0.00002083
Iteration 146/1000 | Loss: 0.00002083
Iteration 147/1000 | Loss: 0.00002083
Iteration 148/1000 | Loss: 0.00002083
Iteration 149/1000 | Loss: 0.00002083
Iteration 150/1000 | Loss: 0.00002083
Iteration 151/1000 | Loss: 0.00002083
Iteration 152/1000 | Loss: 0.00002083
Iteration 153/1000 | Loss: 0.00002083
Iteration 154/1000 | Loss: 0.00002083
Iteration 155/1000 | Loss: 0.00002083
Iteration 156/1000 | Loss: 0.00002083
Iteration 157/1000 | Loss: 0.00002083
Iteration 158/1000 | Loss: 0.00002083
Iteration 159/1000 | Loss: 0.00002083
Iteration 160/1000 | Loss: 0.00002083
Iteration 161/1000 | Loss: 0.00002083
Iteration 162/1000 | Loss: 0.00002083
Iteration 163/1000 | Loss: 0.00002083
Iteration 164/1000 | Loss: 0.00002083
Iteration 165/1000 | Loss: 0.00002083
Iteration 166/1000 | Loss: 0.00002083
Iteration 167/1000 | Loss: 0.00002083
Iteration 168/1000 | Loss: 0.00002083
Iteration 169/1000 | Loss: 0.00002083
Iteration 170/1000 | Loss: 0.00002083
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [2.0831399524467997e-05, 2.0831399524467997e-05, 2.0831399524467997e-05, 2.0831399524467997e-05, 2.0831399524467997e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0831399524467997e-05

Optimization complete. Final v2v error: 3.827603816986084 mm

Highest mean error: 4.482617378234863 mm for frame 180

Lowest mean error: 3.1793572902679443 mm for frame 134

Saving results

Total time: 43.77576684951782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_jessica_posed_009/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_jessica_posed_009/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812072
Iteration 2/25 | Loss: 0.00138207
Iteration 3/25 | Loss: 0.00113751
Iteration 4/25 | Loss: 0.00111643
Iteration 5/25 | Loss: 0.00111361
Iteration 6/25 | Loss: 0.00111360
Iteration 7/25 | Loss: 0.00111360
Iteration 8/25 | Loss: 0.00111360
Iteration 9/25 | Loss: 0.00111360
Iteration 10/25 | Loss: 0.00111360
Iteration 11/25 | Loss: 0.00111360
Iteration 12/25 | Loss: 0.00111360
Iteration 13/25 | Loss: 0.00111360
Iteration 14/25 | Loss: 0.00111360
Iteration 15/25 | Loss: 0.00111360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011135974200442433, 0.0011135974200442433, 0.0011135974200442433, 0.0011135974200442433, 0.0011135974200442433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011135974200442433

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37534344
Iteration 2/25 | Loss: 0.00046321
Iteration 3/25 | Loss: 0.00046320
Iteration 4/25 | Loss: 0.00046320
Iteration 5/25 | Loss: 0.00046320
Iteration 6/25 | Loss: 0.00046320
Iteration 7/25 | Loss: 0.00046320
Iteration 8/25 | Loss: 0.00046320
Iteration 9/25 | Loss: 0.00046320
Iteration 10/25 | Loss: 0.00046320
Iteration 11/25 | Loss: 0.00046320
Iteration 12/25 | Loss: 0.00046320
Iteration 13/25 | Loss: 0.00046320
Iteration 14/25 | Loss: 0.00046320
Iteration 15/25 | Loss: 0.00046320
Iteration 16/25 | Loss: 0.00046320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004632003838196397, 0.0004632003838196397, 0.0004632003838196397, 0.0004632003838196397, 0.0004632003838196397]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004632003838196397

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046320
Iteration 2/1000 | Loss: 0.00002785
Iteration 3/1000 | Loss: 0.00002045
Iteration 4/1000 | Loss: 0.00001836
Iteration 5/1000 | Loss: 0.00001751
Iteration 6/1000 | Loss: 0.00001685
Iteration 7/1000 | Loss: 0.00001643
Iteration 8/1000 | Loss: 0.00001608
Iteration 9/1000 | Loss: 0.00001581
Iteration 10/1000 | Loss: 0.00001545
Iteration 11/1000 | Loss: 0.00001522
Iteration 12/1000 | Loss: 0.00001495
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001465
Iteration 15/1000 | Loss: 0.00001462
Iteration 16/1000 | Loss: 0.00001460
Iteration 17/1000 | Loss: 0.00001458
Iteration 18/1000 | Loss: 0.00001457
Iteration 19/1000 | Loss: 0.00001457
Iteration 20/1000 | Loss: 0.00001453
Iteration 21/1000 | Loss: 0.00001453
Iteration 22/1000 | Loss: 0.00001453
Iteration 23/1000 | Loss: 0.00001453
Iteration 24/1000 | Loss: 0.00001452
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001452
Iteration 27/1000 | Loss: 0.00001452
Iteration 28/1000 | Loss: 0.00001451
Iteration 29/1000 | Loss: 0.00001450
Iteration 30/1000 | Loss: 0.00001449
Iteration 31/1000 | Loss: 0.00001449
Iteration 32/1000 | Loss: 0.00001449
Iteration 33/1000 | Loss: 0.00001449
Iteration 34/1000 | Loss: 0.00001448
Iteration 35/1000 | Loss: 0.00001448
Iteration 36/1000 | Loss: 0.00001448
Iteration 37/1000 | Loss: 0.00001448
Iteration 38/1000 | Loss: 0.00001448
Iteration 39/1000 | Loss: 0.00001448
Iteration 40/1000 | Loss: 0.00001448
Iteration 41/1000 | Loss: 0.00001448
Iteration 42/1000 | Loss: 0.00001448
Iteration 43/1000 | Loss: 0.00001447
Iteration 44/1000 | Loss: 0.00001443
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001439
Iteration 48/1000 | Loss: 0.00001439
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001438
Iteration 51/1000 | Loss: 0.00001438
Iteration 52/1000 | Loss: 0.00001437
Iteration 53/1000 | Loss: 0.00001437
Iteration 54/1000 | Loss: 0.00001437
Iteration 55/1000 | Loss: 0.00001437
Iteration 56/1000 | Loss: 0.00001437
Iteration 57/1000 | Loss: 0.00001437
Iteration 58/1000 | Loss: 0.00001436
Iteration 59/1000 | Loss: 0.00001436
Iteration 60/1000 | Loss: 0.00001436
Iteration 61/1000 | Loss: 0.00001436
Iteration 62/1000 | Loss: 0.00001435
Iteration 63/1000 | Loss: 0.00001435
Iteration 64/1000 | Loss: 0.00001435
Iteration 65/1000 | Loss: 0.00001434
Iteration 66/1000 | Loss: 0.00001434
Iteration 67/1000 | Loss: 0.00001434
Iteration 68/1000 | Loss: 0.00001434
Iteration 69/1000 | Loss: 0.00001434
Iteration 70/1000 | Loss: 0.00001433
Iteration 71/1000 | Loss: 0.00001433
Iteration 72/1000 | Loss: 0.00001433
Iteration 73/1000 | Loss: 0.00001433
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001432
Iteration 77/1000 | Loss: 0.00001432
Iteration 78/1000 | Loss: 0.00001432
Iteration 79/1000 | Loss: 0.00001432
Iteration 80/1000 | Loss: 0.00001432
Iteration 81/1000 | Loss: 0.00001432
Iteration 82/1000 | Loss: 0.00001432
Iteration 83/1000 | Loss: 0.00001432
Iteration 84/1000 | Loss: 0.00001431
Iteration 85/1000 | Loss: 0.00001431
Iteration 86/1000 | Loss: 0.00001431
Iteration 87/1000 | Loss: 0.00001431
Iteration 88/1000 | Loss: 0.00001431
Iteration 89/1000 | Loss: 0.00001431
Iteration 90/1000 | Loss: 0.00001430
Iteration 91/1000 | Loss: 0.00001430
Iteration 92/1000 | Loss: 0.00001430
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001429
Iteration 96/1000 | Loss: 0.00001429
Iteration 97/1000 | Loss: 0.00001429
Iteration 98/1000 | Loss: 0.00001429
Iteration 99/1000 | Loss: 0.00001429
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001428
Iteration 102/1000 | Loss: 0.00001428
Iteration 103/1000 | Loss: 0.00001427
Iteration 104/1000 | Loss: 0.00001427
Iteration 105/1000 | Loss: 0.00001427
Iteration 106/1000 | Loss: 0.00001427
Iteration 107/1000 | Loss: 0.00001427
Iteration 108/1000 | Loss: 0.00001427
Iteration 109/1000 | Loss: 0.00001427
Iteration 110/1000 | Loss: 0.00001427
Iteration 111/1000 | Loss: 0.00001427
Iteration 112/1000 | Loss: 0.00001427
Iteration 113/1000 | Loss: 0.00001427
Iteration 114/1000 | Loss: 0.00001427
Iteration 115/1000 | Loss: 0.00001427
Iteration 116/1000 | Loss: 0.00001427
Iteration 117/1000 | Loss: 0.00001427
Iteration 118/1000 | Loss: 0.00001427
Iteration 119/1000 | Loss: 0.00001427
Iteration 120/1000 | Loss: 0.00001427
Iteration 121/1000 | Loss: 0.00001426
Iteration 122/1000 | Loss: 0.00001426
Iteration 123/1000 | Loss: 0.00001426
Iteration 124/1000 | Loss: 0.00001426
Iteration 125/1000 | Loss: 0.00001426
Iteration 126/1000 | Loss: 0.00001426
Iteration 127/1000 | Loss: 0.00001426
Iteration 128/1000 | Loss: 0.00001426
Iteration 129/1000 | Loss: 0.00001426
Iteration 130/1000 | Loss: 0.00001426
Iteration 131/1000 | Loss: 0.00001426
Iteration 132/1000 | Loss: 0.00001426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 132. Stopping optimization.
Last 5 losses: [1.4263689990912098e-05, 1.4263689990912098e-05, 1.4263689990912098e-05, 1.4263689990912098e-05, 1.4263689990912098e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4263689990912098e-05

Optimization complete. Final v2v error: 3.139878034591675 mm

Highest mean error: 3.2491557598114014 mm for frame 18

Lowest mean error: 3.0619754791259766 mm for frame 106

Saving results

Total time: 34.33347988128662
