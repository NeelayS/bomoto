Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=229, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 12824-12879
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00696848
Iteration 2/25 | Loss: 0.00089790
Iteration 3/25 | Loss: 0.00078576
Iteration 4/25 | Loss: 0.00076577
Iteration 5/25 | Loss: 0.00075911
Iteration 6/25 | Loss: 0.00075757
Iteration 7/25 | Loss: 0.00075711
Iteration 8/25 | Loss: 0.00075711
Iteration 9/25 | Loss: 0.00075711
Iteration 10/25 | Loss: 0.00075711
Iteration 11/25 | Loss: 0.00075711
Iteration 12/25 | Loss: 0.00075711
Iteration 13/25 | Loss: 0.00075711
Iteration 14/25 | Loss: 0.00075711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007571138557977974, 0.0007571138557977974, 0.0007571138557977974, 0.0007571138557977974, 0.0007571138557977974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007571138557977974

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.44477892
Iteration 2/25 | Loss: 0.00125199
Iteration 3/25 | Loss: 0.00125199
Iteration 4/25 | Loss: 0.00125199
Iteration 5/25 | Loss: 0.00125199
Iteration 6/25 | Loss: 0.00125199
Iteration 7/25 | Loss: 0.00125199
Iteration 8/25 | Loss: 0.00125199
Iteration 9/25 | Loss: 0.00125199
Iteration 10/25 | Loss: 0.00125199
Iteration 11/25 | Loss: 0.00125199
Iteration 12/25 | Loss: 0.00125199
Iteration 13/25 | Loss: 0.00125199
Iteration 14/25 | Loss: 0.00125199
Iteration 15/25 | Loss: 0.00125199
Iteration 16/25 | Loss: 0.00125199
Iteration 17/25 | Loss: 0.00125199
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012519885785877705, 0.0012519885785877705, 0.0012519885785877705, 0.0012519885785877705, 0.0012519885785877705]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012519885785877705

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125199
Iteration 2/1000 | Loss: 0.00002946
Iteration 3/1000 | Loss: 0.00001754
Iteration 4/1000 | Loss: 0.00001598
Iteration 5/1000 | Loss: 0.00001470
Iteration 6/1000 | Loss: 0.00001430
Iteration 7/1000 | Loss: 0.00001398
Iteration 8/1000 | Loss: 0.00001377
Iteration 9/1000 | Loss: 0.00001374
Iteration 10/1000 | Loss: 0.00001373
Iteration 11/1000 | Loss: 0.00001372
Iteration 12/1000 | Loss: 0.00001359
Iteration 13/1000 | Loss: 0.00001349
Iteration 14/1000 | Loss: 0.00001347
Iteration 15/1000 | Loss: 0.00001347
Iteration 16/1000 | Loss: 0.00001342
Iteration 17/1000 | Loss: 0.00001341
Iteration 18/1000 | Loss: 0.00001341
Iteration 19/1000 | Loss: 0.00001338
Iteration 20/1000 | Loss: 0.00001337
Iteration 21/1000 | Loss: 0.00001337
Iteration 22/1000 | Loss: 0.00001336
Iteration 23/1000 | Loss: 0.00001334
Iteration 24/1000 | Loss: 0.00001334
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001333
Iteration 27/1000 | Loss: 0.00001333
Iteration 28/1000 | Loss: 0.00001333
Iteration 29/1000 | Loss: 0.00001332
Iteration 30/1000 | Loss: 0.00001332
Iteration 31/1000 | Loss: 0.00001331
Iteration 32/1000 | Loss: 0.00001331
Iteration 33/1000 | Loss: 0.00001331
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001330
Iteration 37/1000 | Loss: 0.00001330
Iteration 38/1000 | Loss: 0.00001329
Iteration 39/1000 | Loss: 0.00001329
Iteration 40/1000 | Loss: 0.00001328
Iteration 41/1000 | Loss: 0.00001328
Iteration 42/1000 | Loss: 0.00001328
Iteration 43/1000 | Loss: 0.00001328
Iteration 44/1000 | Loss: 0.00001328
Iteration 45/1000 | Loss: 0.00001328
Iteration 46/1000 | Loss: 0.00001328
Iteration 47/1000 | Loss: 0.00001328
Iteration 48/1000 | Loss: 0.00001327
Iteration 49/1000 | Loss: 0.00001327
Iteration 50/1000 | Loss: 0.00001327
Iteration 51/1000 | Loss: 0.00001327
Iteration 52/1000 | Loss: 0.00001327
Iteration 53/1000 | Loss: 0.00001327
Iteration 54/1000 | Loss: 0.00001327
Iteration 55/1000 | Loss: 0.00001326
Iteration 56/1000 | Loss: 0.00001325
Iteration 57/1000 | Loss: 0.00001325
Iteration 58/1000 | Loss: 0.00001324
Iteration 59/1000 | Loss: 0.00001324
Iteration 60/1000 | Loss: 0.00001323
Iteration 61/1000 | Loss: 0.00001323
Iteration 62/1000 | Loss: 0.00001323
Iteration 63/1000 | Loss: 0.00001323
Iteration 64/1000 | Loss: 0.00001323
Iteration 65/1000 | Loss: 0.00001323
Iteration 66/1000 | Loss: 0.00001323
Iteration 67/1000 | Loss: 0.00001323
Iteration 68/1000 | Loss: 0.00001323
Iteration 69/1000 | Loss: 0.00001323
Iteration 70/1000 | Loss: 0.00001323
Iteration 71/1000 | Loss: 0.00001322
Iteration 72/1000 | Loss: 0.00001322
Iteration 73/1000 | Loss: 0.00001321
Iteration 74/1000 | Loss: 0.00001320
Iteration 75/1000 | Loss: 0.00001320
Iteration 76/1000 | Loss: 0.00001320
Iteration 77/1000 | Loss: 0.00001319
Iteration 78/1000 | Loss: 0.00001319
Iteration 79/1000 | Loss: 0.00001318
Iteration 80/1000 | Loss: 0.00001318
Iteration 81/1000 | Loss: 0.00001317
Iteration 82/1000 | Loss: 0.00001316
Iteration 83/1000 | Loss: 0.00001315
Iteration 84/1000 | Loss: 0.00001315
Iteration 85/1000 | Loss: 0.00001315
Iteration 86/1000 | Loss: 0.00001315
Iteration 87/1000 | Loss: 0.00001314
Iteration 88/1000 | Loss: 0.00001314
Iteration 89/1000 | Loss: 0.00001314
Iteration 90/1000 | Loss: 0.00001314
Iteration 91/1000 | Loss: 0.00001314
Iteration 92/1000 | Loss: 0.00001314
Iteration 93/1000 | Loss: 0.00001313
Iteration 94/1000 | Loss: 0.00001313
Iteration 95/1000 | Loss: 0.00001313
Iteration 96/1000 | Loss: 0.00001313
Iteration 97/1000 | Loss: 0.00001313
Iteration 98/1000 | Loss: 0.00001313
Iteration 99/1000 | Loss: 0.00001312
Iteration 100/1000 | Loss: 0.00001312
Iteration 101/1000 | Loss: 0.00001312
Iteration 102/1000 | Loss: 0.00001312
Iteration 103/1000 | Loss: 0.00001312
Iteration 104/1000 | Loss: 0.00001312
Iteration 105/1000 | Loss: 0.00001312
Iteration 106/1000 | Loss: 0.00001312
Iteration 107/1000 | Loss: 0.00001312
Iteration 108/1000 | Loss: 0.00001312
Iteration 109/1000 | Loss: 0.00001312
Iteration 110/1000 | Loss: 0.00001312
Iteration 111/1000 | Loss: 0.00001312
Iteration 112/1000 | Loss: 0.00001312
Iteration 113/1000 | Loss: 0.00001312
Iteration 114/1000 | Loss: 0.00001312
Iteration 115/1000 | Loss: 0.00001311
Iteration 116/1000 | Loss: 0.00001311
Iteration 117/1000 | Loss: 0.00001311
Iteration 118/1000 | Loss: 0.00001311
Iteration 119/1000 | Loss: 0.00001311
Iteration 120/1000 | Loss: 0.00001311
Iteration 121/1000 | Loss: 0.00001311
Iteration 122/1000 | Loss: 0.00001311
Iteration 123/1000 | Loss: 0.00001311
Iteration 124/1000 | Loss: 0.00001311
Iteration 125/1000 | Loss: 0.00001311
Iteration 126/1000 | Loss: 0.00001311
Iteration 127/1000 | Loss: 0.00001311
Iteration 128/1000 | Loss: 0.00001311
Iteration 129/1000 | Loss: 0.00001311
Iteration 130/1000 | Loss: 0.00001311
Iteration 131/1000 | Loss: 0.00001311
Iteration 132/1000 | Loss: 0.00001311
Iteration 133/1000 | Loss: 0.00001311
Iteration 134/1000 | Loss: 0.00001311
Iteration 135/1000 | Loss: 0.00001311
Iteration 136/1000 | Loss: 0.00001311
Iteration 137/1000 | Loss: 0.00001311
Iteration 138/1000 | Loss: 0.00001311
Iteration 139/1000 | Loss: 0.00001311
Iteration 140/1000 | Loss: 0.00001311
Iteration 141/1000 | Loss: 0.00001311
Iteration 142/1000 | Loss: 0.00001311
Iteration 143/1000 | Loss: 0.00001311
Iteration 144/1000 | Loss: 0.00001311
Iteration 145/1000 | Loss: 0.00001311
Iteration 146/1000 | Loss: 0.00001311
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00001311
Iteration 149/1000 | Loss: 0.00001311
Iteration 150/1000 | Loss: 0.00001311
Iteration 151/1000 | Loss: 0.00001311
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.3105304788041394e-05, 1.3105304788041394e-05, 1.3105304788041394e-05, 1.3105304788041394e-05, 1.3105304788041394e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3105304788041394e-05

Optimization complete. Final v2v error: 3.094771385192871 mm

Highest mean error: 3.2422306537628174 mm for frame 109

Lowest mean error: 2.9087436199188232 mm for frame 52

Saving results

Total time: 36.86078882217407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00532143
Iteration 2/25 | Loss: 0.00121987
Iteration 3/25 | Loss: 0.00088424
Iteration 4/25 | Loss: 0.00083857
Iteration 5/25 | Loss: 0.00082795
Iteration 6/25 | Loss: 0.00082447
Iteration 7/25 | Loss: 0.00082367
Iteration 8/25 | Loss: 0.00082328
Iteration 9/25 | Loss: 0.00082318
Iteration 10/25 | Loss: 0.00082318
Iteration 11/25 | Loss: 0.00082318
Iteration 12/25 | Loss: 0.00082318
Iteration 13/25 | Loss: 0.00082318
Iteration 14/25 | Loss: 0.00082318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0008231785614043474, 0.0008231785614043474, 0.0008231785614043474, 0.0008231785614043474, 0.0008231785614043474]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008231785614043474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.21382618
Iteration 2/25 | Loss: 0.00125381
Iteration 3/25 | Loss: 0.00125378
Iteration 4/25 | Loss: 0.00125378
Iteration 5/25 | Loss: 0.00125378
Iteration 6/25 | Loss: 0.00125378
Iteration 7/25 | Loss: 0.00125378
Iteration 8/25 | Loss: 0.00125378
Iteration 9/25 | Loss: 0.00125378
Iteration 10/25 | Loss: 0.00125378
Iteration 11/25 | Loss: 0.00125378
Iteration 12/25 | Loss: 0.00125378
Iteration 13/25 | Loss: 0.00125378
Iteration 14/25 | Loss: 0.00125378
Iteration 15/25 | Loss: 0.00125378
Iteration 16/25 | Loss: 0.00125378
Iteration 17/25 | Loss: 0.00125378
Iteration 18/25 | Loss: 0.00125378
Iteration 19/25 | Loss: 0.00125378
Iteration 20/25 | Loss: 0.00125378
Iteration 21/25 | Loss: 0.00125378
Iteration 22/25 | Loss: 0.00125378
Iteration 23/25 | Loss: 0.00125378
Iteration 24/25 | Loss: 0.00125378
Iteration 25/25 | Loss: 0.00125378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125378
Iteration 2/1000 | Loss: 0.00003724
Iteration 3/1000 | Loss: 0.00002638
Iteration 4/1000 | Loss: 0.00002358
Iteration 5/1000 | Loss: 0.00002232
Iteration 6/1000 | Loss: 0.00002145
Iteration 7/1000 | Loss: 0.00002088
Iteration 8/1000 | Loss: 0.00002047
Iteration 9/1000 | Loss: 0.00002016
Iteration 10/1000 | Loss: 0.00001988
Iteration 11/1000 | Loss: 0.00001969
Iteration 12/1000 | Loss: 0.00001964
Iteration 13/1000 | Loss: 0.00001963
Iteration 14/1000 | Loss: 0.00001947
Iteration 15/1000 | Loss: 0.00001933
Iteration 16/1000 | Loss: 0.00001931
Iteration 17/1000 | Loss: 0.00001929
Iteration 18/1000 | Loss: 0.00001929
Iteration 19/1000 | Loss: 0.00001928
Iteration 20/1000 | Loss: 0.00001928
Iteration 21/1000 | Loss: 0.00001925
Iteration 22/1000 | Loss: 0.00001925
Iteration 23/1000 | Loss: 0.00001924
Iteration 24/1000 | Loss: 0.00001923
Iteration 25/1000 | Loss: 0.00001923
Iteration 26/1000 | Loss: 0.00001922
Iteration 27/1000 | Loss: 0.00001922
Iteration 28/1000 | Loss: 0.00001921
Iteration 29/1000 | Loss: 0.00001921
Iteration 30/1000 | Loss: 0.00001921
Iteration 31/1000 | Loss: 0.00001920
Iteration 32/1000 | Loss: 0.00001920
Iteration 33/1000 | Loss: 0.00001920
Iteration 34/1000 | Loss: 0.00001920
Iteration 35/1000 | Loss: 0.00001920
Iteration 36/1000 | Loss: 0.00001919
Iteration 37/1000 | Loss: 0.00001919
Iteration 38/1000 | Loss: 0.00001919
Iteration 39/1000 | Loss: 0.00001919
Iteration 40/1000 | Loss: 0.00001919
Iteration 41/1000 | Loss: 0.00001918
Iteration 42/1000 | Loss: 0.00001918
Iteration 43/1000 | Loss: 0.00001917
Iteration 44/1000 | Loss: 0.00001917
Iteration 45/1000 | Loss: 0.00001917
Iteration 46/1000 | Loss: 0.00001917
Iteration 47/1000 | Loss: 0.00001917
Iteration 48/1000 | Loss: 0.00001917
Iteration 49/1000 | Loss: 0.00001916
Iteration 50/1000 | Loss: 0.00001915
Iteration 51/1000 | Loss: 0.00001915
Iteration 52/1000 | Loss: 0.00001914
Iteration 53/1000 | Loss: 0.00001914
Iteration 54/1000 | Loss: 0.00001914
Iteration 55/1000 | Loss: 0.00001914
Iteration 56/1000 | Loss: 0.00001913
Iteration 57/1000 | Loss: 0.00001912
Iteration 58/1000 | Loss: 0.00001912
Iteration 59/1000 | Loss: 0.00001912
Iteration 60/1000 | Loss: 0.00001911
Iteration 61/1000 | Loss: 0.00001911
Iteration 62/1000 | Loss: 0.00001910
Iteration 63/1000 | Loss: 0.00001910
Iteration 64/1000 | Loss: 0.00001910
Iteration 65/1000 | Loss: 0.00001909
Iteration 66/1000 | Loss: 0.00001909
Iteration 67/1000 | Loss: 0.00001909
Iteration 68/1000 | Loss: 0.00001908
Iteration 69/1000 | Loss: 0.00001908
Iteration 70/1000 | Loss: 0.00001908
Iteration 71/1000 | Loss: 0.00001908
Iteration 72/1000 | Loss: 0.00001908
Iteration 73/1000 | Loss: 0.00001907
Iteration 74/1000 | Loss: 0.00001907
Iteration 75/1000 | Loss: 0.00001907
Iteration 76/1000 | Loss: 0.00001907
Iteration 77/1000 | Loss: 0.00001906
Iteration 78/1000 | Loss: 0.00001906
Iteration 79/1000 | Loss: 0.00001906
Iteration 80/1000 | Loss: 0.00001906
Iteration 81/1000 | Loss: 0.00001905
Iteration 82/1000 | Loss: 0.00001905
Iteration 83/1000 | Loss: 0.00001905
Iteration 84/1000 | Loss: 0.00001905
Iteration 85/1000 | Loss: 0.00001905
Iteration 86/1000 | Loss: 0.00001905
Iteration 87/1000 | Loss: 0.00001905
Iteration 88/1000 | Loss: 0.00001905
Iteration 89/1000 | Loss: 0.00001905
Iteration 90/1000 | Loss: 0.00001905
Iteration 91/1000 | Loss: 0.00001905
Iteration 92/1000 | Loss: 0.00001905
Iteration 93/1000 | Loss: 0.00001905
Iteration 94/1000 | Loss: 0.00001905
Iteration 95/1000 | Loss: 0.00001905
Iteration 96/1000 | Loss: 0.00001905
Iteration 97/1000 | Loss: 0.00001905
Iteration 98/1000 | Loss: 0.00001905
Iteration 99/1000 | Loss: 0.00001905
Iteration 100/1000 | Loss: 0.00001905
Iteration 101/1000 | Loss: 0.00001905
Iteration 102/1000 | Loss: 0.00001905
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.905120188894216e-05, 1.905120188894216e-05, 1.905120188894216e-05, 1.905120188894216e-05, 1.905120188894216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.905120188894216e-05

Optimization complete. Final v2v error: 3.631674289703369 mm

Highest mean error: 4.8895344734191895 mm for frame 60

Lowest mean error: 2.8218958377838135 mm for frame 72

Saving results

Total time: 46.42416334152222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00463300
Iteration 2/25 | Loss: 0.00086722
Iteration 3/25 | Loss: 0.00077338
Iteration 4/25 | Loss: 0.00075390
Iteration 5/25 | Loss: 0.00075226
Iteration 6/25 | Loss: 0.00075226
Iteration 7/25 | Loss: 0.00075226
Iteration 8/25 | Loss: 0.00075226
Iteration 9/25 | Loss: 0.00075226
Iteration 10/25 | Loss: 0.00075226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0007522609666921198, 0.0007522609666921198, 0.0007522609666921198, 0.0007522609666921198, 0.0007522609666921198]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007522609666921198

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.18783855
Iteration 2/25 | Loss: 0.00131473
Iteration 3/25 | Loss: 0.00131468
Iteration 4/25 | Loss: 0.00131468
Iteration 5/25 | Loss: 0.00131468
Iteration 6/25 | Loss: 0.00131468
Iteration 7/25 | Loss: 0.00131468
Iteration 8/25 | Loss: 0.00131468
Iteration 9/25 | Loss: 0.00131468
Iteration 10/25 | Loss: 0.00131468
Iteration 11/25 | Loss: 0.00131468
Iteration 12/25 | Loss: 0.00131468
Iteration 13/25 | Loss: 0.00131468
Iteration 14/25 | Loss: 0.00131468
Iteration 15/25 | Loss: 0.00131468
Iteration 16/25 | Loss: 0.00131468
Iteration 17/25 | Loss: 0.00131468
Iteration 18/25 | Loss: 0.00131468
Iteration 19/25 | Loss: 0.00131468
Iteration 20/25 | Loss: 0.00131468
Iteration 21/25 | Loss: 0.00131468
Iteration 22/25 | Loss: 0.00131468
Iteration 23/25 | Loss: 0.00131468
Iteration 24/25 | Loss: 0.00131468
Iteration 25/25 | Loss: 0.00131468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131468
Iteration 2/1000 | Loss: 0.00002658
Iteration 3/1000 | Loss: 0.00001990
Iteration 4/1000 | Loss: 0.00001788
Iteration 5/1000 | Loss: 0.00001665
Iteration 6/1000 | Loss: 0.00001581
Iteration 7/1000 | Loss: 0.00001545
Iteration 8/1000 | Loss: 0.00001521
Iteration 9/1000 | Loss: 0.00001503
Iteration 10/1000 | Loss: 0.00001485
Iteration 11/1000 | Loss: 0.00001477
Iteration 12/1000 | Loss: 0.00001471
Iteration 13/1000 | Loss: 0.00001470
Iteration 14/1000 | Loss: 0.00001459
Iteration 15/1000 | Loss: 0.00001454
Iteration 16/1000 | Loss: 0.00001453
Iteration 17/1000 | Loss: 0.00001453
Iteration 18/1000 | Loss: 0.00001452
Iteration 19/1000 | Loss: 0.00001452
Iteration 20/1000 | Loss: 0.00001452
Iteration 21/1000 | Loss: 0.00001451
Iteration 22/1000 | Loss: 0.00001450
Iteration 23/1000 | Loss: 0.00001447
Iteration 24/1000 | Loss: 0.00001447
Iteration 25/1000 | Loss: 0.00001447
Iteration 26/1000 | Loss: 0.00001447
Iteration 27/1000 | Loss: 0.00001446
Iteration 28/1000 | Loss: 0.00001446
Iteration 29/1000 | Loss: 0.00001445
Iteration 30/1000 | Loss: 0.00001444
Iteration 31/1000 | Loss: 0.00001444
Iteration 32/1000 | Loss: 0.00001444
Iteration 33/1000 | Loss: 0.00001443
Iteration 34/1000 | Loss: 0.00001443
Iteration 35/1000 | Loss: 0.00001443
Iteration 36/1000 | Loss: 0.00001443
Iteration 37/1000 | Loss: 0.00001443
Iteration 38/1000 | Loss: 0.00001443
Iteration 39/1000 | Loss: 0.00001443
Iteration 40/1000 | Loss: 0.00001443
Iteration 41/1000 | Loss: 0.00001441
Iteration 42/1000 | Loss: 0.00001441
Iteration 43/1000 | Loss: 0.00001441
Iteration 44/1000 | Loss: 0.00001441
Iteration 45/1000 | Loss: 0.00001440
Iteration 46/1000 | Loss: 0.00001439
Iteration 47/1000 | Loss: 0.00001438
Iteration 48/1000 | Loss: 0.00001438
Iteration 49/1000 | Loss: 0.00001438
Iteration 50/1000 | Loss: 0.00001437
Iteration 51/1000 | Loss: 0.00001437
Iteration 52/1000 | Loss: 0.00001436
Iteration 53/1000 | Loss: 0.00001436
Iteration 54/1000 | Loss: 0.00001436
Iteration 55/1000 | Loss: 0.00001435
Iteration 56/1000 | Loss: 0.00001434
Iteration 57/1000 | Loss: 0.00001434
Iteration 58/1000 | Loss: 0.00001434
Iteration 59/1000 | Loss: 0.00001434
Iteration 60/1000 | Loss: 0.00001434
Iteration 61/1000 | Loss: 0.00001434
Iteration 62/1000 | Loss: 0.00001434
Iteration 63/1000 | Loss: 0.00001433
Iteration 64/1000 | Loss: 0.00001433
Iteration 65/1000 | Loss: 0.00001433
Iteration 66/1000 | Loss: 0.00001433
Iteration 67/1000 | Loss: 0.00001433
Iteration 68/1000 | Loss: 0.00001433
Iteration 69/1000 | Loss: 0.00001433
Iteration 70/1000 | Loss: 0.00001432
Iteration 71/1000 | Loss: 0.00001432
Iteration 72/1000 | Loss: 0.00001432
Iteration 73/1000 | Loss: 0.00001432
Iteration 74/1000 | Loss: 0.00001432
Iteration 75/1000 | Loss: 0.00001432
Iteration 76/1000 | Loss: 0.00001431
Iteration 77/1000 | Loss: 0.00001431
Iteration 78/1000 | Loss: 0.00001431
Iteration 79/1000 | Loss: 0.00001431
Iteration 80/1000 | Loss: 0.00001431
Iteration 81/1000 | Loss: 0.00001430
Iteration 82/1000 | Loss: 0.00001430
Iteration 83/1000 | Loss: 0.00001430
Iteration 84/1000 | Loss: 0.00001430
Iteration 85/1000 | Loss: 0.00001430
Iteration 86/1000 | Loss: 0.00001430
Iteration 87/1000 | Loss: 0.00001430
Iteration 88/1000 | Loss: 0.00001430
Iteration 89/1000 | Loss: 0.00001430
Iteration 90/1000 | Loss: 0.00001429
Iteration 91/1000 | Loss: 0.00001429
Iteration 92/1000 | Loss: 0.00001429
Iteration 93/1000 | Loss: 0.00001429
Iteration 94/1000 | Loss: 0.00001429
Iteration 95/1000 | Loss: 0.00001428
Iteration 96/1000 | Loss: 0.00001428
Iteration 97/1000 | Loss: 0.00001428
Iteration 98/1000 | Loss: 0.00001428
Iteration 99/1000 | Loss: 0.00001428
Iteration 100/1000 | Loss: 0.00001428
Iteration 101/1000 | Loss: 0.00001428
Iteration 102/1000 | Loss: 0.00001428
Iteration 103/1000 | Loss: 0.00001428
Iteration 104/1000 | Loss: 0.00001428
Iteration 105/1000 | Loss: 0.00001428
Iteration 106/1000 | Loss: 0.00001428
Iteration 107/1000 | Loss: 0.00001427
Iteration 108/1000 | Loss: 0.00001427
Iteration 109/1000 | Loss: 0.00001427
Iteration 110/1000 | Loss: 0.00001426
Iteration 111/1000 | Loss: 0.00001426
Iteration 112/1000 | Loss: 0.00001426
Iteration 113/1000 | Loss: 0.00001426
Iteration 114/1000 | Loss: 0.00001425
Iteration 115/1000 | Loss: 0.00001425
Iteration 116/1000 | Loss: 0.00001425
Iteration 117/1000 | Loss: 0.00001425
Iteration 118/1000 | Loss: 0.00001425
Iteration 119/1000 | Loss: 0.00001425
Iteration 120/1000 | Loss: 0.00001425
Iteration 121/1000 | Loss: 0.00001424
Iteration 122/1000 | Loss: 0.00001424
Iteration 123/1000 | Loss: 0.00001424
Iteration 124/1000 | Loss: 0.00001424
Iteration 125/1000 | Loss: 0.00001424
Iteration 126/1000 | Loss: 0.00001424
Iteration 127/1000 | Loss: 0.00001424
Iteration 128/1000 | Loss: 0.00001424
Iteration 129/1000 | Loss: 0.00001424
Iteration 130/1000 | Loss: 0.00001424
Iteration 131/1000 | Loss: 0.00001424
Iteration 132/1000 | Loss: 0.00001424
Iteration 133/1000 | Loss: 0.00001424
Iteration 134/1000 | Loss: 0.00001424
Iteration 135/1000 | Loss: 0.00001424
Iteration 136/1000 | Loss: 0.00001424
Iteration 137/1000 | Loss: 0.00001424
Iteration 138/1000 | Loss: 0.00001424
Iteration 139/1000 | Loss: 0.00001424
Iteration 140/1000 | Loss: 0.00001424
Iteration 141/1000 | Loss: 0.00001424
Iteration 142/1000 | Loss: 0.00001424
Iteration 143/1000 | Loss: 0.00001424
Iteration 144/1000 | Loss: 0.00001424
Iteration 145/1000 | Loss: 0.00001424
Iteration 146/1000 | Loss: 0.00001424
Iteration 147/1000 | Loss: 0.00001424
Iteration 148/1000 | Loss: 0.00001424
Iteration 149/1000 | Loss: 0.00001424
Iteration 150/1000 | Loss: 0.00001424
Iteration 151/1000 | Loss: 0.00001424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [1.4238511539588217e-05, 1.4238511539588217e-05, 1.4238511539588217e-05, 1.4238511539588217e-05, 1.4238511539588217e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4238511539588217e-05

Optimization complete. Final v2v error: 3.230272054672241 mm

Highest mean error: 3.451725721359253 mm for frame 0

Lowest mean error: 3.0440514087677 mm for frame 125

Saving results

Total time: 39.72213411331177
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00951822
Iteration 2/25 | Loss: 0.00130393
Iteration 3/25 | Loss: 0.00103441
Iteration 4/25 | Loss: 0.00099395
Iteration 5/25 | Loss: 0.00098994
Iteration 6/25 | Loss: 0.00096974
Iteration 7/25 | Loss: 0.00095039
Iteration 8/25 | Loss: 0.00094416
Iteration 9/25 | Loss: 0.00093797
Iteration 10/25 | Loss: 0.00093438
Iteration 11/25 | Loss: 0.00093181
Iteration 12/25 | Loss: 0.00093136
Iteration 13/25 | Loss: 0.00093125
Iteration 14/25 | Loss: 0.00093125
Iteration 15/25 | Loss: 0.00093125
Iteration 16/25 | Loss: 0.00093125
Iteration 17/25 | Loss: 0.00093124
Iteration 18/25 | Loss: 0.00093124
Iteration 19/25 | Loss: 0.00093124
Iteration 20/25 | Loss: 0.00093124
Iteration 21/25 | Loss: 0.00093124
Iteration 22/25 | Loss: 0.00093124
Iteration 23/25 | Loss: 0.00093124
Iteration 24/25 | Loss: 0.00093124
Iteration 25/25 | Loss: 0.00093124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56639421
Iteration 2/25 | Loss: 0.00137693
Iteration 3/25 | Loss: 0.00137687
Iteration 4/25 | Loss: 0.00137687
Iteration 5/25 | Loss: 0.00137687
Iteration 6/25 | Loss: 0.00137687
Iteration 7/25 | Loss: 0.00137687
Iteration 8/25 | Loss: 0.00137687
Iteration 9/25 | Loss: 0.00137687
Iteration 10/25 | Loss: 0.00137687
Iteration 11/25 | Loss: 0.00137687
Iteration 12/25 | Loss: 0.00137687
Iteration 13/25 | Loss: 0.00137687
Iteration 14/25 | Loss: 0.00137687
Iteration 15/25 | Loss: 0.00137687
Iteration 16/25 | Loss: 0.00137687
Iteration 17/25 | Loss: 0.00137687
Iteration 18/25 | Loss: 0.00137687
Iteration 19/25 | Loss: 0.00137687
Iteration 20/25 | Loss: 0.00137687
Iteration 21/25 | Loss: 0.00137687
Iteration 22/25 | Loss: 0.00137687
Iteration 23/25 | Loss: 0.00137687
Iteration 24/25 | Loss: 0.00137687
Iteration 25/25 | Loss: 0.00137687

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137687
Iteration 2/1000 | Loss: 0.00005218
Iteration 3/1000 | Loss: 0.00003421
Iteration 4/1000 | Loss: 0.00003026
Iteration 5/1000 | Loss: 0.00002874
Iteration 6/1000 | Loss: 0.00002771
Iteration 7/1000 | Loss: 0.00002701
Iteration 8/1000 | Loss: 0.00002646
Iteration 9/1000 | Loss: 0.00002598
Iteration 10/1000 | Loss: 0.00002573
Iteration 11/1000 | Loss: 0.00002547
Iteration 12/1000 | Loss: 0.00002531
Iteration 13/1000 | Loss: 0.00002524
Iteration 14/1000 | Loss: 0.00002521
Iteration 15/1000 | Loss: 0.00002514
Iteration 16/1000 | Loss: 0.00002508
Iteration 17/1000 | Loss: 0.00002501
Iteration 18/1000 | Loss: 0.00002495
Iteration 19/1000 | Loss: 0.00002488
Iteration 20/1000 | Loss: 0.00002487
Iteration 21/1000 | Loss: 0.00002486
Iteration 22/1000 | Loss: 0.00002485
Iteration 23/1000 | Loss: 0.00002485
Iteration 24/1000 | Loss: 0.00002484
Iteration 25/1000 | Loss: 0.00002484
Iteration 26/1000 | Loss: 0.00002484
Iteration 27/1000 | Loss: 0.00002483
Iteration 28/1000 | Loss: 0.00002482
Iteration 29/1000 | Loss: 0.00002482
Iteration 30/1000 | Loss: 0.00002482
Iteration 31/1000 | Loss: 0.00002481
Iteration 32/1000 | Loss: 0.00002481
Iteration 33/1000 | Loss: 0.00002481
Iteration 34/1000 | Loss: 0.00002480
Iteration 35/1000 | Loss: 0.00002480
Iteration 36/1000 | Loss: 0.00002479
Iteration 37/1000 | Loss: 0.00002479
Iteration 38/1000 | Loss: 0.00002479
Iteration 39/1000 | Loss: 0.00002478
Iteration 40/1000 | Loss: 0.00002477
Iteration 41/1000 | Loss: 0.00002477
Iteration 42/1000 | Loss: 0.00002477
Iteration 43/1000 | Loss: 0.00002477
Iteration 44/1000 | Loss: 0.00002477
Iteration 45/1000 | Loss: 0.00002477
Iteration 46/1000 | Loss: 0.00002476
Iteration 47/1000 | Loss: 0.00002476
Iteration 48/1000 | Loss: 0.00002476
Iteration 49/1000 | Loss: 0.00002475
Iteration 50/1000 | Loss: 0.00002475
Iteration 51/1000 | Loss: 0.00002474
Iteration 52/1000 | Loss: 0.00002474
Iteration 53/1000 | Loss: 0.00002474
Iteration 54/1000 | Loss: 0.00002474
Iteration 55/1000 | Loss: 0.00002474
Iteration 56/1000 | Loss: 0.00002474
Iteration 57/1000 | Loss: 0.00002473
Iteration 58/1000 | Loss: 0.00002473
Iteration 59/1000 | Loss: 0.00002473
Iteration 60/1000 | Loss: 0.00002472
Iteration 61/1000 | Loss: 0.00002472
Iteration 62/1000 | Loss: 0.00002472
Iteration 63/1000 | Loss: 0.00002472
Iteration 64/1000 | Loss: 0.00002472
Iteration 65/1000 | Loss: 0.00002472
Iteration 66/1000 | Loss: 0.00002472
Iteration 67/1000 | Loss: 0.00002471
Iteration 68/1000 | Loss: 0.00002471
Iteration 69/1000 | Loss: 0.00002471
Iteration 70/1000 | Loss: 0.00002471
Iteration 71/1000 | Loss: 0.00002471
Iteration 72/1000 | Loss: 0.00002471
Iteration 73/1000 | Loss: 0.00002470
Iteration 74/1000 | Loss: 0.00002470
Iteration 75/1000 | Loss: 0.00002470
Iteration 76/1000 | Loss: 0.00002470
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002470
Iteration 79/1000 | Loss: 0.00002470
Iteration 80/1000 | Loss: 0.00002470
Iteration 81/1000 | Loss: 0.00002469
Iteration 82/1000 | Loss: 0.00002469
Iteration 83/1000 | Loss: 0.00002469
Iteration 84/1000 | Loss: 0.00002468
Iteration 85/1000 | Loss: 0.00002468
Iteration 86/1000 | Loss: 0.00002468
Iteration 87/1000 | Loss: 0.00002467
Iteration 88/1000 | Loss: 0.00002467
Iteration 89/1000 | Loss: 0.00002467
Iteration 90/1000 | Loss: 0.00002467
Iteration 91/1000 | Loss: 0.00002466
Iteration 92/1000 | Loss: 0.00002466
Iteration 93/1000 | Loss: 0.00002466
Iteration 94/1000 | Loss: 0.00002465
Iteration 95/1000 | Loss: 0.00002465
Iteration 96/1000 | Loss: 0.00002465
Iteration 97/1000 | Loss: 0.00002464
Iteration 98/1000 | Loss: 0.00002464
Iteration 99/1000 | Loss: 0.00002464
Iteration 100/1000 | Loss: 0.00002464
Iteration 101/1000 | Loss: 0.00002464
Iteration 102/1000 | Loss: 0.00002463
Iteration 103/1000 | Loss: 0.00002463
Iteration 104/1000 | Loss: 0.00002463
Iteration 105/1000 | Loss: 0.00002463
Iteration 106/1000 | Loss: 0.00002462
Iteration 107/1000 | Loss: 0.00002462
Iteration 108/1000 | Loss: 0.00002462
Iteration 109/1000 | Loss: 0.00002461
Iteration 110/1000 | Loss: 0.00002461
Iteration 111/1000 | Loss: 0.00002461
Iteration 112/1000 | Loss: 0.00002461
Iteration 113/1000 | Loss: 0.00002461
Iteration 114/1000 | Loss: 0.00002461
Iteration 115/1000 | Loss: 0.00002460
Iteration 116/1000 | Loss: 0.00002460
Iteration 117/1000 | Loss: 0.00002460
Iteration 118/1000 | Loss: 0.00002460
Iteration 119/1000 | Loss: 0.00002459
Iteration 120/1000 | Loss: 0.00002459
Iteration 121/1000 | Loss: 0.00002459
Iteration 122/1000 | Loss: 0.00002459
Iteration 123/1000 | Loss: 0.00002459
Iteration 124/1000 | Loss: 0.00002459
Iteration 125/1000 | Loss: 0.00002459
Iteration 126/1000 | Loss: 0.00002459
Iteration 127/1000 | Loss: 0.00002459
Iteration 128/1000 | Loss: 0.00002459
Iteration 129/1000 | Loss: 0.00002459
Iteration 130/1000 | Loss: 0.00002459
Iteration 131/1000 | Loss: 0.00002459
Iteration 132/1000 | Loss: 0.00002459
Iteration 133/1000 | Loss: 0.00002459
Iteration 134/1000 | Loss: 0.00002459
Iteration 135/1000 | Loss: 0.00002459
Iteration 136/1000 | Loss: 0.00002459
Iteration 137/1000 | Loss: 0.00002459
Iteration 138/1000 | Loss: 0.00002459
Iteration 139/1000 | Loss: 0.00002459
Iteration 140/1000 | Loss: 0.00002459
Iteration 141/1000 | Loss: 0.00002459
Iteration 142/1000 | Loss: 0.00002459
Iteration 143/1000 | Loss: 0.00002459
Iteration 144/1000 | Loss: 0.00002459
Iteration 145/1000 | Loss: 0.00002459
Iteration 146/1000 | Loss: 0.00002459
Iteration 147/1000 | Loss: 0.00002459
Iteration 148/1000 | Loss: 0.00002459
Iteration 149/1000 | Loss: 0.00002459
Iteration 150/1000 | Loss: 0.00002459
Iteration 151/1000 | Loss: 0.00002459
Iteration 152/1000 | Loss: 0.00002459
Iteration 153/1000 | Loss: 0.00002459
Iteration 154/1000 | Loss: 0.00002459
Iteration 155/1000 | Loss: 0.00002459
Iteration 156/1000 | Loss: 0.00002459
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [2.4590830435045063e-05, 2.4590830435045063e-05, 2.4590830435045063e-05, 2.4590830435045063e-05, 2.4590830435045063e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4590830435045063e-05

Optimization complete. Final v2v error: 4.030189514160156 mm

Highest mean error: 6.661948204040527 mm for frame 97

Lowest mean error: 3.5825376510620117 mm for frame 233

Saving results

Total time: 61.13537406921387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952217
Iteration 2/25 | Loss: 0.00173853
Iteration 3/25 | Loss: 0.00122550
Iteration 4/25 | Loss: 0.00112188
Iteration 5/25 | Loss: 0.00108471
Iteration 6/25 | Loss: 0.00106433
Iteration 7/25 | Loss: 0.00101031
Iteration 8/25 | Loss: 0.00098369
Iteration 9/25 | Loss: 0.00096365
Iteration 10/25 | Loss: 0.00095469
Iteration 11/25 | Loss: 0.00094572
Iteration 12/25 | Loss: 0.00094685
Iteration 13/25 | Loss: 0.00094376
Iteration 14/25 | Loss: 0.00093881
Iteration 15/25 | Loss: 0.00093694
Iteration 16/25 | Loss: 0.00093626
Iteration 17/25 | Loss: 0.00093610
Iteration 18/25 | Loss: 0.00094007
Iteration 19/25 | Loss: 0.00093649
Iteration 20/25 | Loss: 0.00093520
Iteration 21/25 | Loss: 0.00093473
Iteration 22/25 | Loss: 0.00093946
Iteration 23/25 | Loss: 0.00094305
Iteration 24/25 | Loss: 0.00092968
Iteration 25/25 | Loss: 0.00092546

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39692760
Iteration 2/25 | Loss: 0.00113348
Iteration 3/25 | Loss: 0.00113345
Iteration 4/25 | Loss: 0.00113345
Iteration 5/25 | Loss: 0.00113345
Iteration 6/25 | Loss: 0.00113345
Iteration 7/25 | Loss: 0.00113345
Iteration 8/25 | Loss: 0.00113345
Iteration 9/25 | Loss: 0.00113345
Iteration 10/25 | Loss: 0.00113345
Iteration 11/25 | Loss: 0.00113345
Iteration 12/25 | Loss: 0.00113345
Iteration 13/25 | Loss: 0.00113345
Iteration 14/25 | Loss: 0.00113345
Iteration 15/25 | Loss: 0.00113345
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011334506561979651, 0.0011334506561979651, 0.0011334506561979651, 0.0011334506561979651, 0.0011334506561979651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011334506561979651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113345
Iteration 2/1000 | Loss: 0.00049259
Iteration 3/1000 | Loss: 0.00072862
Iteration 4/1000 | Loss: 0.00028485
Iteration 5/1000 | Loss: 0.00022478
Iteration 6/1000 | Loss: 0.00014264
Iteration 7/1000 | Loss: 0.00011767
Iteration 8/1000 | Loss: 0.00010591
Iteration 9/1000 | Loss: 0.00047110
Iteration 10/1000 | Loss: 0.00052030
Iteration 11/1000 | Loss: 0.00025479
Iteration 12/1000 | Loss: 0.00015868
Iteration 13/1000 | Loss: 0.00010826
Iteration 14/1000 | Loss: 0.00013313
Iteration 15/1000 | Loss: 0.00009571
Iteration 16/1000 | Loss: 0.00008563
Iteration 17/1000 | Loss: 0.00007402
Iteration 18/1000 | Loss: 0.00007009
Iteration 19/1000 | Loss: 0.00017374
Iteration 20/1000 | Loss: 0.00008398
Iteration 21/1000 | Loss: 0.00012132
Iteration 22/1000 | Loss: 0.00008037
Iteration 23/1000 | Loss: 0.00007222
Iteration 24/1000 | Loss: 0.00008220
Iteration 25/1000 | Loss: 0.00006870
Iteration 26/1000 | Loss: 0.00009343
Iteration 27/1000 | Loss: 0.00012495
Iteration 28/1000 | Loss: 0.00008015
Iteration 29/1000 | Loss: 0.00006222
Iteration 30/1000 | Loss: 0.00007509
Iteration 31/1000 | Loss: 0.00006171
Iteration 32/1000 | Loss: 0.00005656
Iteration 33/1000 | Loss: 0.00020148
Iteration 34/1000 | Loss: 0.00006349
Iteration 35/1000 | Loss: 0.00007707
Iteration 36/1000 | Loss: 0.00005511
Iteration 37/1000 | Loss: 0.00005298
Iteration 38/1000 | Loss: 0.00005021
Iteration 39/1000 | Loss: 0.00010635
Iteration 40/1000 | Loss: 0.00005395
Iteration 41/1000 | Loss: 0.00005055
Iteration 42/1000 | Loss: 0.00004687
Iteration 43/1000 | Loss: 0.00004397
Iteration 44/1000 | Loss: 0.00004265
Iteration 45/1000 | Loss: 0.00004151
Iteration 46/1000 | Loss: 0.00005200
Iteration 47/1000 | Loss: 0.00004533
Iteration 48/1000 | Loss: 0.00004228
Iteration 49/1000 | Loss: 0.00004058
Iteration 50/1000 | Loss: 0.00003887
Iteration 51/1000 | Loss: 0.00003808
Iteration 52/1000 | Loss: 0.00003729
Iteration 53/1000 | Loss: 0.00003660
Iteration 54/1000 | Loss: 0.00003606
Iteration 55/1000 | Loss: 0.00003539
Iteration 56/1000 | Loss: 0.00003470
Iteration 57/1000 | Loss: 0.00003447
Iteration 58/1000 | Loss: 0.00003421
Iteration 59/1000 | Loss: 0.00003393
Iteration 60/1000 | Loss: 0.00003338
Iteration 61/1000 | Loss: 0.00003300
Iteration 62/1000 | Loss: 0.00006033
Iteration 63/1000 | Loss: 0.00009546
Iteration 64/1000 | Loss: 0.00005266
Iteration 65/1000 | Loss: 0.00006009
Iteration 66/1000 | Loss: 0.00006034
Iteration 67/1000 | Loss: 0.00006251
Iteration 68/1000 | Loss: 0.00006152
Iteration 69/1000 | Loss: 0.00005368
Iteration 70/1000 | Loss: 0.00006103
Iteration 71/1000 | Loss: 0.00005070
Iteration 72/1000 | Loss: 0.00006171
Iteration 73/1000 | Loss: 0.00010390
Iteration 74/1000 | Loss: 0.00006697
Iteration 75/1000 | Loss: 0.00006020
Iteration 76/1000 | Loss: 0.00010086
Iteration 77/1000 | Loss: 0.00007036
Iteration 78/1000 | Loss: 0.00004823
Iteration 79/1000 | Loss: 0.00004759
Iteration 80/1000 | Loss: 0.00004218
Iteration 81/1000 | Loss: 0.00004451
Iteration 82/1000 | Loss: 0.00003976
Iteration 83/1000 | Loss: 0.00003771
Iteration 84/1000 | Loss: 0.00003911
Iteration 85/1000 | Loss: 0.00004203
Iteration 86/1000 | Loss: 0.00003956
Iteration 87/1000 | Loss: 0.00003508
Iteration 88/1000 | Loss: 0.00003456
Iteration 89/1000 | Loss: 0.00003420
Iteration 90/1000 | Loss: 0.00003375
Iteration 91/1000 | Loss: 0.00003335
Iteration 92/1000 | Loss: 0.00004698
Iteration 93/1000 | Loss: 0.00003752
Iteration 94/1000 | Loss: 0.00003385
Iteration 95/1000 | Loss: 0.00003304
Iteration 96/1000 | Loss: 0.00003221
Iteration 97/1000 | Loss: 0.00003154
Iteration 98/1000 | Loss: 0.00003107
Iteration 99/1000 | Loss: 0.00003082
Iteration 100/1000 | Loss: 0.00003067
Iteration 101/1000 | Loss: 0.00003066
Iteration 102/1000 | Loss: 0.00003066
Iteration 103/1000 | Loss: 0.00003064
Iteration 104/1000 | Loss: 0.00003058
Iteration 105/1000 | Loss: 0.00003056
Iteration 106/1000 | Loss: 0.00003056
Iteration 107/1000 | Loss: 0.00003055
Iteration 108/1000 | Loss: 0.00003055
Iteration 109/1000 | Loss: 0.00003055
Iteration 110/1000 | Loss: 0.00003055
Iteration 111/1000 | Loss: 0.00003055
Iteration 112/1000 | Loss: 0.00003055
Iteration 113/1000 | Loss: 0.00003055
Iteration 114/1000 | Loss: 0.00003055
Iteration 115/1000 | Loss: 0.00003055
Iteration 116/1000 | Loss: 0.00003055
Iteration 117/1000 | Loss: 0.00003054
Iteration 118/1000 | Loss: 0.00003054
Iteration 119/1000 | Loss: 0.00003053
Iteration 120/1000 | Loss: 0.00003053
Iteration 121/1000 | Loss: 0.00003053
Iteration 122/1000 | Loss: 0.00003052
Iteration 123/1000 | Loss: 0.00003052
Iteration 124/1000 | Loss: 0.00003052
Iteration 125/1000 | Loss: 0.00003051
Iteration 126/1000 | Loss: 0.00003051
Iteration 127/1000 | Loss: 0.00003051
Iteration 128/1000 | Loss: 0.00003051
Iteration 129/1000 | Loss: 0.00003051
Iteration 130/1000 | Loss: 0.00003051
Iteration 131/1000 | Loss: 0.00003051
Iteration 132/1000 | Loss: 0.00003051
Iteration 133/1000 | Loss: 0.00003051
Iteration 134/1000 | Loss: 0.00003050
Iteration 135/1000 | Loss: 0.00003050
Iteration 136/1000 | Loss: 0.00003050
Iteration 137/1000 | Loss: 0.00003050
Iteration 138/1000 | Loss: 0.00003050
Iteration 139/1000 | Loss: 0.00003049
Iteration 140/1000 | Loss: 0.00003049
Iteration 141/1000 | Loss: 0.00003049
Iteration 142/1000 | Loss: 0.00003049
Iteration 143/1000 | Loss: 0.00003049
Iteration 144/1000 | Loss: 0.00003049
Iteration 145/1000 | Loss: 0.00003049
Iteration 146/1000 | Loss: 0.00003049
Iteration 147/1000 | Loss: 0.00003049
Iteration 148/1000 | Loss: 0.00003049
Iteration 149/1000 | Loss: 0.00003049
Iteration 150/1000 | Loss: 0.00003049
Iteration 151/1000 | Loss: 0.00003049
Iteration 152/1000 | Loss: 0.00003049
Iteration 153/1000 | Loss: 0.00003048
Iteration 154/1000 | Loss: 0.00003048
Iteration 155/1000 | Loss: 0.00003048
Iteration 156/1000 | Loss: 0.00003048
Iteration 157/1000 | Loss: 0.00003048
Iteration 158/1000 | Loss: 0.00003048
Iteration 159/1000 | Loss: 0.00003048
Iteration 160/1000 | Loss: 0.00003048
Iteration 161/1000 | Loss: 0.00003047
Iteration 162/1000 | Loss: 0.00003047
Iteration 163/1000 | Loss: 0.00003047
Iteration 164/1000 | Loss: 0.00003047
Iteration 165/1000 | Loss: 0.00003047
Iteration 166/1000 | Loss: 0.00003047
Iteration 167/1000 | Loss: 0.00003047
Iteration 168/1000 | Loss: 0.00003047
Iteration 169/1000 | Loss: 0.00003047
Iteration 170/1000 | Loss: 0.00003047
Iteration 171/1000 | Loss: 0.00003047
Iteration 172/1000 | Loss: 0.00003047
Iteration 173/1000 | Loss: 0.00003047
Iteration 174/1000 | Loss: 0.00003047
Iteration 175/1000 | Loss: 0.00003047
Iteration 176/1000 | Loss: 0.00003047
Iteration 177/1000 | Loss: 0.00003046
Iteration 178/1000 | Loss: 0.00003046
Iteration 179/1000 | Loss: 0.00003046
Iteration 180/1000 | Loss: 0.00003046
Iteration 181/1000 | Loss: 0.00003046
Iteration 182/1000 | Loss: 0.00003046
Iteration 183/1000 | Loss: 0.00003046
Iteration 184/1000 | Loss: 0.00003045
Iteration 185/1000 | Loss: 0.00003045
Iteration 186/1000 | Loss: 0.00003045
Iteration 187/1000 | Loss: 0.00003045
Iteration 188/1000 | Loss: 0.00003045
Iteration 189/1000 | Loss: 0.00003045
Iteration 190/1000 | Loss: 0.00003045
Iteration 191/1000 | Loss: 0.00003045
Iteration 192/1000 | Loss: 0.00003044
Iteration 193/1000 | Loss: 0.00003044
Iteration 194/1000 | Loss: 0.00003044
Iteration 195/1000 | Loss: 0.00003044
Iteration 196/1000 | Loss: 0.00003044
Iteration 197/1000 | Loss: 0.00003044
Iteration 198/1000 | Loss: 0.00003044
Iteration 199/1000 | Loss: 0.00003044
Iteration 200/1000 | Loss: 0.00003044
Iteration 201/1000 | Loss: 0.00003043
Iteration 202/1000 | Loss: 0.00003043
Iteration 203/1000 | Loss: 0.00003043
Iteration 204/1000 | Loss: 0.00003043
Iteration 205/1000 | Loss: 0.00003042
Iteration 206/1000 | Loss: 0.00003042
Iteration 207/1000 | Loss: 0.00003042
Iteration 208/1000 | Loss: 0.00003042
Iteration 209/1000 | Loss: 0.00003042
Iteration 210/1000 | Loss: 0.00003042
Iteration 211/1000 | Loss: 0.00003042
Iteration 212/1000 | Loss: 0.00003042
Iteration 213/1000 | Loss: 0.00003042
Iteration 214/1000 | Loss: 0.00003041
Iteration 215/1000 | Loss: 0.00003041
Iteration 216/1000 | Loss: 0.00003041
Iteration 217/1000 | Loss: 0.00003041
Iteration 218/1000 | Loss: 0.00003041
Iteration 219/1000 | Loss: 0.00003041
Iteration 220/1000 | Loss: 0.00003041
Iteration 221/1000 | Loss: 0.00003041
Iteration 222/1000 | Loss: 0.00003040
Iteration 223/1000 | Loss: 0.00003040
Iteration 224/1000 | Loss: 0.00003040
Iteration 225/1000 | Loss: 0.00003040
Iteration 226/1000 | Loss: 0.00003040
Iteration 227/1000 | Loss: 0.00003040
Iteration 228/1000 | Loss: 0.00003040
Iteration 229/1000 | Loss: 0.00003040
Iteration 230/1000 | Loss: 0.00003040
Iteration 231/1000 | Loss: 0.00003040
Iteration 232/1000 | Loss: 0.00003039
Iteration 233/1000 | Loss: 0.00003039
Iteration 234/1000 | Loss: 0.00003039
Iteration 235/1000 | Loss: 0.00003039
Iteration 236/1000 | Loss: 0.00003039
Iteration 237/1000 | Loss: 0.00003039
Iteration 238/1000 | Loss: 0.00003039
Iteration 239/1000 | Loss: 0.00003039
Iteration 240/1000 | Loss: 0.00003039
Iteration 241/1000 | Loss: 0.00003038
Iteration 242/1000 | Loss: 0.00003038
Iteration 243/1000 | Loss: 0.00003038
Iteration 244/1000 | Loss: 0.00003038
Iteration 245/1000 | Loss: 0.00003038
Iteration 246/1000 | Loss: 0.00003038
Iteration 247/1000 | Loss: 0.00003038
Iteration 248/1000 | Loss: 0.00003038
Iteration 249/1000 | Loss: 0.00003038
Iteration 250/1000 | Loss: 0.00003038
Iteration 251/1000 | Loss: 0.00003038
Iteration 252/1000 | Loss: 0.00003038
Iteration 253/1000 | Loss: 0.00003038
Iteration 254/1000 | Loss: 0.00003038
Iteration 255/1000 | Loss: 0.00003038
Iteration 256/1000 | Loss: 0.00003038
Iteration 257/1000 | Loss: 0.00003038
Iteration 258/1000 | Loss: 0.00003038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 258. Stopping optimization.
Last 5 losses: [3.0376719223568216e-05, 3.0376719223568216e-05, 3.0376719223568216e-05, 3.0376719223568216e-05, 3.0376719223568216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.0376719223568216e-05

Optimization complete. Final v2v error: 4.379545211791992 mm

Highest mean error: 8.097390174865723 mm for frame 104

Lowest mean error: 3.2656161785125732 mm for frame 75

Saving results

Total time: 209.29482626914978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01054242
Iteration 2/25 | Loss: 0.00217241
Iteration 3/25 | Loss: 0.00173966
Iteration 4/25 | Loss: 0.00160528
Iteration 5/25 | Loss: 0.00168217
Iteration 6/25 | Loss: 0.00128876
Iteration 7/25 | Loss: 0.00105158
Iteration 8/25 | Loss: 0.00094558
Iteration 9/25 | Loss: 0.00091280
Iteration 10/25 | Loss: 0.00090442
Iteration 11/25 | Loss: 0.00090098
Iteration 12/25 | Loss: 0.00089912
Iteration 13/25 | Loss: 0.00089940
Iteration 14/25 | Loss: 0.00089736
Iteration 15/25 | Loss: 0.00089580
Iteration 16/25 | Loss: 0.00089528
Iteration 17/25 | Loss: 0.00089491
Iteration 18/25 | Loss: 0.00089485
Iteration 19/25 | Loss: 0.00089485
Iteration 20/25 | Loss: 0.00089485
Iteration 21/25 | Loss: 0.00089485
Iteration 22/25 | Loss: 0.00089485
Iteration 23/25 | Loss: 0.00089485
Iteration 24/25 | Loss: 0.00089485
Iteration 25/25 | Loss: 0.00089485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59966278
Iteration 2/25 | Loss: 0.00127904
Iteration 3/25 | Loss: 0.00127903
Iteration 4/25 | Loss: 0.00127903
Iteration 5/25 | Loss: 0.00127903
Iteration 6/25 | Loss: 0.00127903
Iteration 7/25 | Loss: 0.00127903
Iteration 8/25 | Loss: 0.00127903
Iteration 9/25 | Loss: 0.00127903
Iteration 10/25 | Loss: 0.00127903
Iteration 11/25 | Loss: 0.00127903
Iteration 12/25 | Loss: 0.00127903
Iteration 13/25 | Loss: 0.00127903
Iteration 14/25 | Loss: 0.00127903
Iteration 15/25 | Loss: 0.00127903
Iteration 16/25 | Loss: 0.00127903
Iteration 17/25 | Loss: 0.00127903
Iteration 18/25 | Loss: 0.00127903
Iteration 19/25 | Loss: 0.00127903
Iteration 20/25 | Loss: 0.00127903
Iteration 21/25 | Loss: 0.00127903
Iteration 22/25 | Loss: 0.00127903
Iteration 23/25 | Loss: 0.00127903
Iteration 24/25 | Loss: 0.00127903
Iteration 25/25 | Loss: 0.00127903

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127903
Iteration 2/1000 | Loss: 0.00003234
Iteration 3/1000 | Loss: 0.00002362
Iteration 4/1000 | Loss: 0.00002202
Iteration 5/1000 | Loss: 0.00002101
Iteration 6/1000 | Loss: 0.00002044
Iteration 7/1000 | Loss: 0.00002013
Iteration 8/1000 | Loss: 0.00001996
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001988
Iteration 11/1000 | Loss: 0.00001987
Iteration 12/1000 | Loss: 0.00001986
Iteration 13/1000 | Loss: 0.00001983
Iteration 14/1000 | Loss: 0.00001975
Iteration 15/1000 | Loss: 0.00001972
Iteration 16/1000 | Loss: 0.00001969
Iteration 17/1000 | Loss: 0.00001968
Iteration 18/1000 | Loss: 0.00001968
Iteration 19/1000 | Loss: 0.00001968
Iteration 20/1000 | Loss: 0.00001968
Iteration 21/1000 | Loss: 0.00001967
Iteration 22/1000 | Loss: 0.00001963
Iteration 23/1000 | Loss: 0.00001963
Iteration 24/1000 | Loss: 0.00001963
Iteration 25/1000 | Loss: 0.00001963
Iteration 26/1000 | Loss: 0.00001963
Iteration 27/1000 | Loss: 0.00001963
Iteration 28/1000 | Loss: 0.00001963
Iteration 29/1000 | Loss: 0.00001963
Iteration 30/1000 | Loss: 0.00001963
Iteration 31/1000 | Loss: 0.00001963
Iteration 32/1000 | Loss: 0.00001963
Iteration 33/1000 | Loss: 0.00001962
Iteration 34/1000 | Loss: 0.00001962
Iteration 35/1000 | Loss: 0.00001961
Iteration 36/1000 | Loss: 0.00001961
Iteration 37/1000 | Loss: 0.00001961
Iteration 38/1000 | Loss: 0.00001960
Iteration 39/1000 | Loss: 0.00001960
Iteration 40/1000 | Loss: 0.00001960
Iteration 41/1000 | Loss: 0.00001960
Iteration 42/1000 | Loss: 0.00001960
Iteration 43/1000 | Loss: 0.00001960
Iteration 44/1000 | Loss: 0.00001959
Iteration 45/1000 | Loss: 0.00001958
Iteration 46/1000 | Loss: 0.00001957
Iteration 47/1000 | Loss: 0.00001957
Iteration 48/1000 | Loss: 0.00001956
Iteration 49/1000 | Loss: 0.00001956
Iteration 50/1000 | Loss: 0.00001956
Iteration 51/1000 | Loss: 0.00001956
Iteration 52/1000 | Loss: 0.00001956
Iteration 53/1000 | Loss: 0.00001956
Iteration 54/1000 | Loss: 0.00001956
Iteration 55/1000 | Loss: 0.00001956
Iteration 56/1000 | Loss: 0.00001956
Iteration 57/1000 | Loss: 0.00001956
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 57. Stopping optimization.
Last 5 losses: [1.9561557564884424e-05, 1.9561557564884424e-05, 1.9561557564884424e-05, 1.9561557564884424e-05, 1.9561557564884424e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9561557564884424e-05

Optimization complete. Final v2v error: 3.6891672611236572 mm

Highest mean error: 3.825160264968872 mm for frame 194

Lowest mean error: 3.603327512741089 mm for frame 183

Saving results

Total time: 54.98864150047302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396065
Iteration 2/25 | Loss: 0.00085446
Iteration 3/25 | Loss: 0.00076597
Iteration 4/25 | Loss: 0.00074267
Iteration 5/25 | Loss: 0.00073597
Iteration 6/25 | Loss: 0.00073420
Iteration 7/25 | Loss: 0.00073381
Iteration 8/25 | Loss: 0.00073381
Iteration 9/25 | Loss: 0.00073381
Iteration 10/25 | Loss: 0.00073381
Iteration 11/25 | Loss: 0.00073381
Iteration 12/25 | Loss: 0.00073381
Iteration 13/25 | Loss: 0.00073381
Iteration 14/25 | Loss: 0.00073381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007338073337450624, 0.0007338073337450624, 0.0007338073337450624, 0.0007338073337450624, 0.0007338073337450624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007338073337450624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.06671453
Iteration 2/25 | Loss: 0.00125373
Iteration 3/25 | Loss: 0.00125373
Iteration 4/25 | Loss: 0.00125373
Iteration 5/25 | Loss: 0.00125373
Iteration 6/25 | Loss: 0.00125373
Iteration 7/25 | Loss: 0.00125373
Iteration 8/25 | Loss: 0.00125373
Iteration 9/25 | Loss: 0.00125373
Iteration 10/25 | Loss: 0.00125373
Iteration 11/25 | Loss: 0.00125373
Iteration 12/25 | Loss: 0.00125373
Iteration 13/25 | Loss: 0.00125373
Iteration 14/25 | Loss: 0.00125373
Iteration 15/25 | Loss: 0.00125373
Iteration 16/25 | Loss: 0.00125373
Iteration 17/25 | Loss: 0.00125373
Iteration 18/25 | Loss: 0.00125373
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001253727707080543, 0.001253727707080543, 0.001253727707080543, 0.001253727707080543, 0.001253727707080543]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001253727707080543

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125373
Iteration 2/1000 | Loss: 0.00002206
Iteration 3/1000 | Loss: 0.00001669
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001460
Iteration 6/1000 | Loss: 0.00001421
Iteration 7/1000 | Loss: 0.00001392
Iteration 8/1000 | Loss: 0.00001382
Iteration 9/1000 | Loss: 0.00001379
Iteration 10/1000 | Loss: 0.00001359
Iteration 11/1000 | Loss: 0.00001356
Iteration 12/1000 | Loss: 0.00001347
Iteration 13/1000 | Loss: 0.00001342
Iteration 14/1000 | Loss: 0.00001339
Iteration 15/1000 | Loss: 0.00001339
Iteration 16/1000 | Loss: 0.00001337
Iteration 17/1000 | Loss: 0.00001334
Iteration 18/1000 | Loss: 0.00001333
Iteration 19/1000 | Loss: 0.00001332
Iteration 20/1000 | Loss: 0.00001331
Iteration 21/1000 | Loss: 0.00001331
Iteration 22/1000 | Loss: 0.00001329
Iteration 23/1000 | Loss: 0.00001328
Iteration 24/1000 | Loss: 0.00001326
Iteration 25/1000 | Loss: 0.00001326
Iteration 26/1000 | Loss: 0.00001324
Iteration 27/1000 | Loss: 0.00001324
Iteration 28/1000 | Loss: 0.00001324
Iteration 29/1000 | Loss: 0.00001324
Iteration 30/1000 | Loss: 0.00001324
Iteration 31/1000 | Loss: 0.00001323
Iteration 32/1000 | Loss: 0.00001323
Iteration 33/1000 | Loss: 0.00001323
Iteration 34/1000 | Loss: 0.00001323
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001322
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001321
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001320
Iteration 47/1000 | Loss: 0.00001320
Iteration 48/1000 | Loss: 0.00001320
Iteration 49/1000 | Loss: 0.00001320
Iteration 50/1000 | Loss: 0.00001319
Iteration 51/1000 | Loss: 0.00001319
Iteration 52/1000 | Loss: 0.00001319
Iteration 53/1000 | Loss: 0.00001319
Iteration 54/1000 | Loss: 0.00001319
Iteration 55/1000 | Loss: 0.00001319
Iteration 56/1000 | Loss: 0.00001318
Iteration 57/1000 | Loss: 0.00001318
Iteration 58/1000 | Loss: 0.00001317
Iteration 59/1000 | Loss: 0.00001317
Iteration 60/1000 | Loss: 0.00001317
Iteration 61/1000 | Loss: 0.00001316
Iteration 62/1000 | Loss: 0.00001316
Iteration 63/1000 | Loss: 0.00001315
Iteration 64/1000 | Loss: 0.00001315
Iteration 65/1000 | Loss: 0.00001315
Iteration 66/1000 | Loss: 0.00001314
Iteration 67/1000 | Loss: 0.00001314
Iteration 68/1000 | Loss: 0.00001313
Iteration 69/1000 | Loss: 0.00001313
Iteration 70/1000 | Loss: 0.00001312
Iteration 71/1000 | Loss: 0.00001309
Iteration 72/1000 | Loss: 0.00001307
Iteration 73/1000 | Loss: 0.00001307
Iteration 74/1000 | Loss: 0.00001306
Iteration 75/1000 | Loss: 0.00001306
Iteration 76/1000 | Loss: 0.00001305
Iteration 77/1000 | Loss: 0.00001305
Iteration 78/1000 | Loss: 0.00001305
Iteration 79/1000 | Loss: 0.00001305
Iteration 80/1000 | Loss: 0.00001305
Iteration 81/1000 | Loss: 0.00001305
Iteration 82/1000 | Loss: 0.00001305
Iteration 83/1000 | Loss: 0.00001305
Iteration 84/1000 | Loss: 0.00001305
Iteration 85/1000 | Loss: 0.00001305
Iteration 86/1000 | Loss: 0.00001304
Iteration 87/1000 | Loss: 0.00001304
Iteration 88/1000 | Loss: 0.00001304
Iteration 89/1000 | Loss: 0.00001304
Iteration 90/1000 | Loss: 0.00001304
Iteration 91/1000 | Loss: 0.00001304
Iteration 92/1000 | Loss: 0.00001304
Iteration 93/1000 | Loss: 0.00001303
Iteration 94/1000 | Loss: 0.00001303
Iteration 95/1000 | Loss: 0.00001303
Iteration 96/1000 | Loss: 0.00001303
Iteration 97/1000 | Loss: 0.00001303
Iteration 98/1000 | Loss: 0.00001303
Iteration 99/1000 | Loss: 0.00001302
Iteration 100/1000 | Loss: 0.00001302
Iteration 101/1000 | Loss: 0.00001302
Iteration 102/1000 | Loss: 0.00001302
Iteration 103/1000 | Loss: 0.00001302
Iteration 104/1000 | Loss: 0.00001302
Iteration 105/1000 | Loss: 0.00001302
Iteration 106/1000 | Loss: 0.00001301
Iteration 107/1000 | Loss: 0.00001301
Iteration 108/1000 | Loss: 0.00001301
Iteration 109/1000 | Loss: 0.00001301
Iteration 110/1000 | Loss: 0.00001301
Iteration 111/1000 | Loss: 0.00001301
Iteration 112/1000 | Loss: 0.00001301
Iteration 113/1000 | Loss: 0.00001301
Iteration 114/1000 | Loss: 0.00001301
Iteration 115/1000 | Loss: 0.00001301
Iteration 116/1000 | Loss: 0.00001301
Iteration 117/1000 | Loss: 0.00001301
Iteration 118/1000 | Loss: 0.00001301
Iteration 119/1000 | Loss: 0.00001300
Iteration 120/1000 | Loss: 0.00001300
Iteration 121/1000 | Loss: 0.00001300
Iteration 122/1000 | Loss: 0.00001300
Iteration 123/1000 | Loss: 0.00001300
Iteration 124/1000 | Loss: 0.00001300
Iteration 125/1000 | Loss: 0.00001300
Iteration 126/1000 | Loss: 0.00001300
Iteration 127/1000 | Loss: 0.00001300
Iteration 128/1000 | Loss: 0.00001300
Iteration 129/1000 | Loss: 0.00001300
Iteration 130/1000 | Loss: 0.00001300
Iteration 131/1000 | Loss: 0.00001300
Iteration 132/1000 | Loss: 0.00001300
Iteration 133/1000 | Loss: 0.00001300
Iteration 134/1000 | Loss: 0.00001300
Iteration 135/1000 | Loss: 0.00001300
Iteration 136/1000 | Loss: 0.00001299
Iteration 137/1000 | Loss: 0.00001299
Iteration 138/1000 | Loss: 0.00001299
Iteration 139/1000 | Loss: 0.00001299
Iteration 140/1000 | Loss: 0.00001299
Iteration 141/1000 | Loss: 0.00001299
Iteration 142/1000 | Loss: 0.00001299
Iteration 143/1000 | Loss: 0.00001299
Iteration 144/1000 | Loss: 0.00001299
Iteration 145/1000 | Loss: 0.00001299
Iteration 146/1000 | Loss: 0.00001299
Iteration 147/1000 | Loss: 0.00001299
Iteration 148/1000 | Loss: 0.00001299
Iteration 149/1000 | Loss: 0.00001299
Iteration 150/1000 | Loss: 0.00001299
Iteration 151/1000 | Loss: 0.00001299
Iteration 152/1000 | Loss: 0.00001299
Iteration 153/1000 | Loss: 0.00001299
Iteration 154/1000 | Loss: 0.00001299
Iteration 155/1000 | Loss: 0.00001299
Iteration 156/1000 | Loss: 0.00001298
Iteration 157/1000 | Loss: 0.00001298
Iteration 158/1000 | Loss: 0.00001298
Iteration 159/1000 | Loss: 0.00001298
Iteration 160/1000 | Loss: 0.00001298
Iteration 161/1000 | Loss: 0.00001298
Iteration 162/1000 | Loss: 0.00001298
Iteration 163/1000 | Loss: 0.00001298
Iteration 164/1000 | Loss: 0.00001298
Iteration 165/1000 | Loss: 0.00001298
Iteration 166/1000 | Loss: 0.00001298
Iteration 167/1000 | Loss: 0.00001298
Iteration 168/1000 | Loss: 0.00001298
Iteration 169/1000 | Loss: 0.00001298
Iteration 170/1000 | Loss: 0.00001298
Iteration 171/1000 | Loss: 0.00001298
Iteration 172/1000 | Loss: 0.00001298
Iteration 173/1000 | Loss: 0.00001298
Iteration 174/1000 | Loss: 0.00001298
Iteration 175/1000 | Loss: 0.00001298
Iteration 176/1000 | Loss: 0.00001298
Iteration 177/1000 | Loss: 0.00001298
Iteration 178/1000 | Loss: 0.00001298
Iteration 179/1000 | Loss: 0.00001298
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.297812741540838e-05, 1.297812741540838e-05, 1.297812741540838e-05, 1.297812741540838e-05, 1.297812741540838e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.297812741540838e-05

Optimization complete. Final v2v error: 3.085218906402588 mm

Highest mean error: 3.2824110984802246 mm for frame 115

Lowest mean error: 2.9384114742279053 mm for frame 140

Saving results

Total time: 41.58558249473572
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746767
Iteration 2/25 | Loss: 0.00128058
Iteration 3/25 | Loss: 0.00097824
Iteration 4/25 | Loss: 0.00091793
Iteration 5/25 | Loss: 0.00090279
Iteration 6/25 | Loss: 0.00089647
Iteration 7/25 | Loss: 0.00089526
Iteration 8/25 | Loss: 0.00089400
Iteration 9/25 | Loss: 0.00089582
Iteration 10/25 | Loss: 0.00089426
Iteration 11/25 | Loss: 0.00089175
Iteration 12/25 | Loss: 0.00089499
Iteration 13/25 | Loss: 0.00089469
Iteration 14/25 | Loss: 0.00089330
Iteration 15/25 | Loss: 0.00088944
Iteration 16/25 | Loss: 0.00088767
Iteration 17/25 | Loss: 0.00088851
Iteration 18/25 | Loss: 0.00088784
Iteration 19/25 | Loss: 0.00088753
Iteration 20/25 | Loss: 0.00088780
Iteration 21/25 | Loss: 0.00088729
Iteration 22/25 | Loss: 0.00088542
Iteration 23/25 | Loss: 0.00088477
Iteration 24/25 | Loss: 0.00088437
Iteration 25/25 | Loss: 0.00088422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47273254
Iteration 2/25 | Loss: 0.00167206
Iteration 3/25 | Loss: 0.00167202
Iteration 4/25 | Loss: 0.00167202
Iteration 5/25 | Loss: 0.00167202
Iteration 6/25 | Loss: 0.00167202
Iteration 7/25 | Loss: 0.00167202
Iteration 8/25 | Loss: 0.00167202
Iteration 9/25 | Loss: 0.00167202
Iteration 10/25 | Loss: 0.00167202
Iteration 11/25 | Loss: 0.00167202
Iteration 12/25 | Loss: 0.00167202
Iteration 13/25 | Loss: 0.00167202
Iteration 14/25 | Loss: 0.00167202
Iteration 15/25 | Loss: 0.00167202
Iteration 16/25 | Loss: 0.00167202
Iteration 17/25 | Loss: 0.00167202
Iteration 18/25 | Loss: 0.00167202
Iteration 19/25 | Loss: 0.00167202
Iteration 20/25 | Loss: 0.00167202
Iteration 21/25 | Loss: 0.00167202
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0016720163403078914, 0.0016720163403078914, 0.0016720163403078914, 0.0016720163403078914, 0.0016720163403078914]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016720163403078914

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167202
Iteration 2/1000 | Loss: 0.00007748
Iteration 3/1000 | Loss: 0.00006072
Iteration 4/1000 | Loss: 0.00005501
Iteration 5/1000 | Loss: 0.00005222
Iteration 6/1000 | Loss: 0.00005070
Iteration 7/1000 | Loss: 0.00004929
Iteration 8/1000 | Loss: 0.00004819
Iteration 9/1000 | Loss: 0.00004696
Iteration 10/1000 | Loss: 0.00004594
Iteration 11/1000 | Loss: 0.00004527
Iteration 12/1000 | Loss: 0.00004446
Iteration 13/1000 | Loss: 0.00095071
Iteration 14/1000 | Loss: 0.00006054
Iteration 15/1000 | Loss: 0.00004942
Iteration 16/1000 | Loss: 0.00004577
Iteration 17/1000 | Loss: 0.00004473
Iteration 18/1000 | Loss: 0.00004355
Iteration 19/1000 | Loss: 0.00050216
Iteration 20/1000 | Loss: 0.00040370
Iteration 21/1000 | Loss: 0.00007602
Iteration 22/1000 | Loss: 0.00006593
Iteration 23/1000 | Loss: 0.00005588
Iteration 24/1000 | Loss: 0.00005134
Iteration 25/1000 | Loss: 0.00005330
Iteration 26/1000 | Loss: 0.00004923
Iteration 27/1000 | Loss: 0.00005933
Iteration 28/1000 | Loss: 0.00004877
Iteration 29/1000 | Loss: 0.00006158
Iteration 30/1000 | Loss: 0.00004588
Iteration 31/1000 | Loss: 0.00005884
Iteration 32/1000 | Loss: 0.00004519
Iteration 33/1000 | Loss: 0.00004403
Iteration 34/1000 | Loss: 0.00004322
Iteration 35/1000 | Loss: 0.00004233
Iteration 36/1000 | Loss: 0.00004681
Iteration 37/1000 | Loss: 0.00004347
Iteration 38/1000 | Loss: 0.00004568
Iteration 39/1000 | Loss: 0.00004313
Iteration 40/1000 | Loss: 0.00004255
Iteration 41/1000 | Loss: 0.00004854
Iteration 42/1000 | Loss: 0.00004496
Iteration 43/1000 | Loss: 0.00004407
Iteration 44/1000 | Loss: 0.00179192
Iteration 45/1000 | Loss: 0.00098110
Iteration 46/1000 | Loss: 0.00011484
Iteration 47/1000 | Loss: 0.00008773
Iteration 48/1000 | Loss: 0.00005310
Iteration 49/1000 | Loss: 0.00006318
Iteration 50/1000 | Loss: 0.00005054
Iteration 51/1000 | Loss: 0.00005971
Iteration 52/1000 | Loss: 0.00004504
Iteration 53/1000 | Loss: 0.00004856
Iteration 54/1000 | Loss: 0.00004775
Iteration 55/1000 | Loss: 0.00004063
Iteration 56/1000 | Loss: 0.00005679
Iteration 57/1000 | Loss: 0.00004494
Iteration 58/1000 | Loss: 0.00005047
Iteration 59/1000 | Loss: 0.00004395
Iteration 60/1000 | Loss: 0.00004624
Iteration 61/1000 | Loss: 0.00004243
Iteration 62/1000 | Loss: 0.00004306
Iteration 63/1000 | Loss: 0.00004601
Iteration 64/1000 | Loss: 0.00005903
Iteration 65/1000 | Loss: 0.00004568
Iteration 66/1000 | Loss: 0.00004255
Iteration 67/1000 | Loss: 0.00004837
Iteration 68/1000 | Loss: 0.00006377
Iteration 69/1000 | Loss: 0.00004435
Iteration 70/1000 | Loss: 0.00004963
Iteration 71/1000 | Loss: 0.00004455
Iteration 72/1000 | Loss: 0.00005323
Iteration 73/1000 | Loss: 0.00004962
Iteration 74/1000 | Loss: 0.00006178
Iteration 75/1000 | Loss: 0.00004995
Iteration 76/1000 | Loss: 0.00005273
Iteration 77/1000 | Loss: 0.00005150
Iteration 78/1000 | Loss: 0.00005678
Iteration 79/1000 | Loss: 0.00005057
Iteration 80/1000 | Loss: 0.00005755
Iteration 81/1000 | Loss: 0.00005048
Iteration 82/1000 | Loss: 0.00005767
Iteration 83/1000 | Loss: 0.00005077
Iteration 84/1000 | Loss: 0.00005728
Iteration 85/1000 | Loss: 0.00005002
Iteration 86/1000 | Loss: 0.00003559
Iteration 87/1000 | Loss: 0.00004437
Iteration 88/1000 | Loss: 0.00004877
Iteration 89/1000 | Loss: 0.00004329
Iteration 90/1000 | Loss: 0.00005185
Iteration 91/1000 | Loss: 0.00005600
Iteration 92/1000 | Loss: 0.00005128
Iteration 93/1000 | Loss: 0.00005200
Iteration 94/1000 | Loss: 0.00004356
Iteration 95/1000 | Loss: 0.00004515
Iteration 96/1000 | Loss: 0.00004321
Iteration 97/1000 | Loss: 0.00004555
Iteration 98/1000 | Loss: 0.00004408
Iteration 99/1000 | Loss: 0.00004459
Iteration 100/1000 | Loss: 0.00004330
Iteration 101/1000 | Loss: 0.00004495
Iteration 102/1000 | Loss: 0.00004327
Iteration 103/1000 | Loss: 0.00004047
Iteration 104/1000 | Loss: 0.00005681
Iteration 105/1000 | Loss: 0.00004197
Iteration 106/1000 | Loss: 0.00004357
Iteration 107/1000 | Loss: 0.00005073
Iteration 108/1000 | Loss: 0.00004010
Iteration 109/1000 | Loss: 0.00004294
Iteration 110/1000 | Loss: 0.00004424
Iteration 111/1000 | Loss: 0.00004689
Iteration 112/1000 | Loss: 0.00004621
Iteration 113/1000 | Loss: 0.00003470
Iteration 114/1000 | Loss: 0.00003291
Iteration 115/1000 | Loss: 0.00003188
Iteration 116/1000 | Loss: 0.00003150
Iteration 117/1000 | Loss: 0.00003138
Iteration 118/1000 | Loss: 0.00003135
Iteration 119/1000 | Loss: 0.00003135
Iteration 120/1000 | Loss: 0.00003135
Iteration 121/1000 | Loss: 0.00003135
Iteration 122/1000 | Loss: 0.00003134
Iteration 123/1000 | Loss: 0.00003134
Iteration 124/1000 | Loss: 0.00003134
Iteration 125/1000 | Loss: 0.00003134
Iteration 126/1000 | Loss: 0.00003134
Iteration 127/1000 | Loss: 0.00003134
Iteration 128/1000 | Loss: 0.00003134
Iteration 129/1000 | Loss: 0.00003134
Iteration 130/1000 | Loss: 0.00003134
Iteration 131/1000 | Loss: 0.00003134
Iteration 132/1000 | Loss: 0.00003134
Iteration 133/1000 | Loss: 0.00003134
Iteration 134/1000 | Loss: 0.00003133
Iteration 135/1000 | Loss: 0.00003133
Iteration 136/1000 | Loss: 0.00003133
Iteration 137/1000 | Loss: 0.00003133
Iteration 138/1000 | Loss: 0.00003133
Iteration 139/1000 | Loss: 0.00003131
Iteration 140/1000 | Loss: 0.00003131
Iteration 141/1000 | Loss: 0.00003131
Iteration 142/1000 | Loss: 0.00003131
Iteration 143/1000 | Loss: 0.00003131
Iteration 144/1000 | Loss: 0.00003131
Iteration 145/1000 | Loss: 0.00003131
Iteration 146/1000 | Loss: 0.00003131
Iteration 147/1000 | Loss: 0.00003130
Iteration 148/1000 | Loss: 0.00003130
Iteration 149/1000 | Loss: 0.00003130
Iteration 150/1000 | Loss: 0.00003130
Iteration 151/1000 | Loss: 0.00003128
Iteration 152/1000 | Loss: 0.00003128
Iteration 153/1000 | Loss: 0.00003128
Iteration 154/1000 | Loss: 0.00003127
Iteration 155/1000 | Loss: 0.00003127
Iteration 156/1000 | Loss: 0.00003127
Iteration 157/1000 | Loss: 0.00003126
Iteration 158/1000 | Loss: 0.00003126
Iteration 159/1000 | Loss: 0.00003126
Iteration 160/1000 | Loss: 0.00003126
Iteration 161/1000 | Loss: 0.00003126
Iteration 162/1000 | Loss: 0.00003126
Iteration 163/1000 | Loss: 0.00003126
Iteration 164/1000 | Loss: 0.00003125
Iteration 165/1000 | Loss: 0.00003125
Iteration 166/1000 | Loss: 0.00003125
Iteration 167/1000 | Loss: 0.00003125
Iteration 168/1000 | Loss: 0.00003124
Iteration 169/1000 | Loss: 0.00003124
Iteration 170/1000 | Loss: 0.00003124
Iteration 171/1000 | Loss: 0.00003124
Iteration 172/1000 | Loss: 0.00003124
Iteration 173/1000 | Loss: 0.00003124
Iteration 174/1000 | Loss: 0.00003124
Iteration 175/1000 | Loss: 0.00003123
Iteration 176/1000 | Loss: 0.00003123
Iteration 177/1000 | Loss: 0.00003123
Iteration 178/1000 | Loss: 0.00003123
Iteration 179/1000 | Loss: 0.00003123
Iteration 180/1000 | Loss: 0.00003123
Iteration 181/1000 | Loss: 0.00003123
Iteration 182/1000 | Loss: 0.00003123
Iteration 183/1000 | Loss: 0.00003123
Iteration 184/1000 | Loss: 0.00003123
Iteration 185/1000 | Loss: 0.00003123
Iteration 186/1000 | Loss: 0.00003123
Iteration 187/1000 | Loss: 0.00003123
Iteration 188/1000 | Loss: 0.00003123
Iteration 189/1000 | Loss: 0.00003123
Iteration 190/1000 | Loss: 0.00003123
Iteration 191/1000 | Loss: 0.00003123
Iteration 192/1000 | Loss: 0.00003123
Iteration 193/1000 | Loss: 0.00003123
Iteration 194/1000 | Loss: 0.00003122
Iteration 195/1000 | Loss: 0.00003122
Iteration 196/1000 | Loss: 0.00003122
Iteration 197/1000 | Loss: 0.00003122
Iteration 198/1000 | Loss: 0.00003122
Iteration 199/1000 | Loss: 0.00003122
Iteration 200/1000 | Loss: 0.00003122
Iteration 201/1000 | Loss: 0.00003122
Iteration 202/1000 | Loss: 0.00003122
Iteration 203/1000 | Loss: 0.00003122
Iteration 204/1000 | Loss: 0.00003122
Iteration 205/1000 | Loss: 0.00003122
Iteration 206/1000 | Loss: 0.00003122
Iteration 207/1000 | Loss: 0.00003122
Iteration 208/1000 | Loss: 0.00003122
Iteration 209/1000 | Loss: 0.00003122
Iteration 210/1000 | Loss: 0.00003122
Iteration 211/1000 | Loss: 0.00003122
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [3.1219638913171366e-05, 3.1219638913171366e-05, 3.1219638913171366e-05, 3.1219638913171366e-05, 3.1219638913171366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1219638913171366e-05

Optimization complete. Final v2v error: 3.8890223503112793 mm

Highest mean error: 12.235418319702148 mm for frame 90

Lowest mean error: 3.2278406620025635 mm for frame 235

Saving results

Total time: 245.3497610092163
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00969288
Iteration 2/25 | Loss: 0.00217704
Iteration 3/25 | Loss: 0.00128847
Iteration 4/25 | Loss: 0.00116544
Iteration 5/25 | Loss: 0.00109341
Iteration 6/25 | Loss: 0.00111015
Iteration 7/25 | Loss: 0.00109845
Iteration 8/25 | Loss: 0.00106682
Iteration 9/25 | Loss: 0.00104802
Iteration 10/25 | Loss: 0.00103863
Iteration 11/25 | Loss: 0.00102516
Iteration 12/25 | Loss: 0.00101670
Iteration 13/25 | Loss: 0.00100823
Iteration 14/25 | Loss: 0.00100439
Iteration 15/25 | Loss: 0.00100226
Iteration 16/25 | Loss: 0.00100861
Iteration 17/25 | Loss: 0.00100650
Iteration 18/25 | Loss: 0.00100586
Iteration 19/25 | Loss: 0.00100833
Iteration 20/25 | Loss: 0.00100503
Iteration 21/25 | Loss: 0.00100697
Iteration 22/25 | Loss: 0.00100404
Iteration 23/25 | Loss: 0.00100471
Iteration 24/25 | Loss: 0.00100291
Iteration 25/25 | Loss: 0.00100347

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.58994865
Iteration 2/25 | Loss: 0.00314949
Iteration 3/25 | Loss: 0.00314949
Iteration 4/25 | Loss: 0.00314949
Iteration 5/25 | Loss: 0.00314949
Iteration 6/25 | Loss: 0.00314949
Iteration 7/25 | Loss: 0.00314949
Iteration 8/25 | Loss: 0.00314949
Iteration 9/25 | Loss: 0.00314949
Iteration 10/25 | Loss: 0.00314949
Iteration 11/25 | Loss: 0.00314949
Iteration 12/25 | Loss: 0.00314949
Iteration 13/25 | Loss: 0.00314949
Iteration 14/25 | Loss: 0.00314949
Iteration 15/25 | Loss: 0.00314949
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.003149488940834999, 0.003149488940834999, 0.003149488940834999, 0.003149488940834999, 0.003149488940834999]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003149488940834999

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00314949
Iteration 2/1000 | Loss: 0.00045284
Iteration 3/1000 | Loss: 0.00180516
Iteration 4/1000 | Loss: 0.00328611
Iteration 5/1000 | Loss: 0.00202709
Iteration 6/1000 | Loss: 0.00505221
Iteration 7/1000 | Loss: 0.00044570
Iteration 8/1000 | Loss: 0.00522639
Iteration 9/1000 | Loss: 0.00015330
Iteration 10/1000 | Loss: 0.00407621
Iteration 11/1000 | Loss: 0.00211485
Iteration 12/1000 | Loss: 0.00126690
Iteration 13/1000 | Loss: 0.00011613
Iteration 14/1000 | Loss: 0.00087218
Iteration 15/1000 | Loss: 0.00035280
Iteration 16/1000 | Loss: 0.00027462
Iteration 17/1000 | Loss: 0.00009938
Iteration 18/1000 | Loss: 0.00016179
Iteration 19/1000 | Loss: 0.00019274
Iteration 20/1000 | Loss: 0.00105030
Iteration 21/1000 | Loss: 0.00007397
Iteration 22/1000 | Loss: 0.00041960
Iteration 23/1000 | Loss: 0.00023222
Iteration 24/1000 | Loss: 0.00006732
Iteration 25/1000 | Loss: 0.00005270
Iteration 26/1000 | Loss: 0.00004585
Iteration 27/1000 | Loss: 0.00004154
Iteration 28/1000 | Loss: 0.00023301
Iteration 29/1000 | Loss: 0.00162332
Iteration 30/1000 | Loss: 0.00026427
Iteration 31/1000 | Loss: 0.00028320
Iteration 32/1000 | Loss: 0.00006993
Iteration 33/1000 | Loss: 0.00007571
Iteration 34/1000 | Loss: 0.00016850
Iteration 35/1000 | Loss: 0.00004625
Iteration 36/1000 | Loss: 0.00137861
Iteration 37/1000 | Loss: 0.00027104
Iteration 38/1000 | Loss: 0.00011338
Iteration 39/1000 | Loss: 0.00019358
Iteration 40/1000 | Loss: 0.00013651
Iteration 41/1000 | Loss: 0.00005106
Iteration 42/1000 | Loss: 0.00010126
Iteration 43/1000 | Loss: 0.00022109
Iteration 44/1000 | Loss: 0.00003685
Iteration 45/1000 | Loss: 0.00003152
Iteration 46/1000 | Loss: 0.00002899
Iteration 47/1000 | Loss: 0.00002747
Iteration 48/1000 | Loss: 0.00002635
Iteration 49/1000 | Loss: 0.00002527
Iteration 50/1000 | Loss: 0.00002408
Iteration 51/1000 | Loss: 0.00002344
Iteration 52/1000 | Loss: 0.00002281
Iteration 53/1000 | Loss: 0.00028668
Iteration 54/1000 | Loss: 0.00015720
Iteration 55/1000 | Loss: 0.00023854
Iteration 56/1000 | Loss: 0.00014701
Iteration 57/1000 | Loss: 0.00003378
Iteration 58/1000 | Loss: 0.00003030
Iteration 59/1000 | Loss: 0.00003412
Iteration 60/1000 | Loss: 0.00002734
Iteration 61/1000 | Loss: 0.00002543
Iteration 62/1000 | Loss: 0.00002401
Iteration 63/1000 | Loss: 0.00002297
Iteration 64/1000 | Loss: 0.00002178
Iteration 65/1000 | Loss: 0.00002104
Iteration 66/1000 | Loss: 0.00002045
Iteration 67/1000 | Loss: 0.00002007
Iteration 68/1000 | Loss: 0.00001971
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001943
Iteration 71/1000 | Loss: 0.00001940
Iteration 72/1000 | Loss: 0.00001940
Iteration 73/1000 | Loss: 0.00001939
Iteration 74/1000 | Loss: 0.00001939
Iteration 75/1000 | Loss: 0.00001938
Iteration 76/1000 | Loss: 0.00001937
Iteration 77/1000 | Loss: 0.00001937
Iteration 78/1000 | Loss: 0.00001937
Iteration 79/1000 | Loss: 0.00001936
Iteration 80/1000 | Loss: 0.00001936
Iteration 81/1000 | Loss: 0.00001936
Iteration 82/1000 | Loss: 0.00001932
Iteration 83/1000 | Loss: 0.00001930
Iteration 84/1000 | Loss: 0.00001929
Iteration 85/1000 | Loss: 0.00001927
Iteration 86/1000 | Loss: 0.00001926
Iteration 87/1000 | Loss: 0.00001925
Iteration 88/1000 | Loss: 0.00001924
Iteration 89/1000 | Loss: 0.00001923
Iteration 90/1000 | Loss: 0.00001923
Iteration 91/1000 | Loss: 0.00001922
Iteration 92/1000 | Loss: 0.00001922
Iteration 93/1000 | Loss: 0.00001922
Iteration 94/1000 | Loss: 0.00001921
Iteration 95/1000 | Loss: 0.00001921
Iteration 96/1000 | Loss: 0.00001921
Iteration 97/1000 | Loss: 0.00001920
Iteration 98/1000 | Loss: 0.00001920
Iteration 99/1000 | Loss: 0.00001919
Iteration 100/1000 | Loss: 0.00001917
Iteration 101/1000 | Loss: 0.00001917
Iteration 102/1000 | Loss: 0.00001916
Iteration 103/1000 | Loss: 0.00001916
Iteration 104/1000 | Loss: 0.00001915
Iteration 105/1000 | Loss: 0.00001914
Iteration 106/1000 | Loss: 0.00001914
Iteration 107/1000 | Loss: 0.00001914
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001912
Iteration 111/1000 | Loss: 0.00001912
Iteration 112/1000 | Loss: 0.00001912
Iteration 113/1000 | Loss: 0.00001910
Iteration 114/1000 | Loss: 0.00001908
Iteration 115/1000 | Loss: 0.00001904
Iteration 116/1000 | Loss: 0.00001904
Iteration 117/1000 | Loss: 0.00001901
Iteration 118/1000 | Loss: 0.00001901
Iteration 119/1000 | Loss: 0.00001900
Iteration 120/1000 | Loss: 0.00001900
Iteration 121/1000 | Loss: 0.00001899
Iteration 122/1000 | Loss: 0.00001899
Iteration 123/1000 | Loss: 0.00001899
Iteration 124/1000 | Loss: 0.00001898
Iteration 125/1000 | Loss: 0.00001898
Iteration 126/1000 | Loss: 0.00001898
Iteration 127/1000 | Loss: 0.00001897
Iteration 128/1000 | Loss: 0.00001897
Iteration 129/1000 | Loss: 0.00001897
Iteration 130/1000 | Loss: 0.00001897
Iteration 131/1000 | Loss: 0.00001896
Iteration 132/1000 | Loss: 0.00001896
Iteration 133/1000 | Loss: 0.00001896
Iteration 134/1000 | Loss: 0.00001896
Iteration 135/1000 | Loss: 0.00001896
Iteration 136/1000 | Loss: 0.00001896
Iteration 137/1000 | Loss: 0.00001895
Iteration 138/1000 | Loss: 0.00001895
Iteration 139/1000 | Loss: 0.00001895
Iteration 140/1000 | Loss: 0.00001895
Iteration 141/1000 | Loss: 0.00001895
Iteration 142/1000 | Loss: 0.00001895
Iteration 143/1000 | Loss: 0.00020362
Iteration 144/1000 | Loss: 0.00002382
Iteration 145/1000 | Loss: 0.00002214
Iteration 146/1000 | Loss: 0.00002120
Iteration 147/1000 | Loss: 0.00027323
Iteration 148/1000 | Loss: 0.00003842
Iteration 149/1000 | Loss: 0.00002934
Iteration 150/1000 | Loss: 0.00002167
Iteration 151/1000 | Loss: 0.00002014
Iteration 152/1000 | Loss: 0.00001942
Iteration 153/1000 | Loss: 0.00001899
Iteration 154/1000 | Loss: 0.00003046
Iteration 155/1000 | Loss: 0.00002681
Iteration 156/1000 | Loss: 0.00003001
Iteration 157/1000 | Loss: 0.00001907
Iteration 158/1000 | Loss: 0.00002531
Iteration 159/1000 | Loss: 0.00002794
Iteration 160/1000 | Loss: 0.00002014
Iteration 161/1000 | Loss: 0.00001887
Iteration 162/1000 | Loss: 0.00001874
Iteration 163/1000 | Loss: 0.00001873
Iteration 164/1000 | Loss: 0.00001873
Iteration 165/1000 | Loss: 0.00001867
Iteration 166/1000 | Loss: 0.00001867
Iteration 167/1000 | Loss: 0.00001864
Iteration 168/1000 | Loss: 0.00001863
Iteration 169/1000 | Loss: 0.00001863
Iteration 170/1000 | Loss: 0.00001861
Iteration 171/1000 | Loss: 0.00001846
Iteration 172/1000 | Loss: 0.00001840
Iteration 173/1000 | Loss: 0.00001834
Iteration 174/1000 | Loss: 0.00001832
Iteration 175/1000 | Loss: 0.00001829
Iteration 176/1000 | Loss: 0.00001829
Iteration 177/1000 | Loss: 0.00001829
Iteration 178/1000 | Loss: 0.00001829
Iteration 179/1000 | Loss: 0.00001829
Iteration 180/1000 | Loss: 0.00001829
Iteration 181/1000 | Loss: 0.00001829
Iteration 182/1000 | Loss: 0.00001829
Iteration 183/1000 | Loss: 0.00001829
Iteration 184/1000 | Loss: 0.00001829
Iteration 185/1000 | Loss: 0.00001829
Iteration 186/1000 | Loss: 0.00001828
Iteration 187/1000 | Loss: 0.00001828
Iteration 188/1000 | Loss: 0.00001828
Iteration 189/1000 | Loss: 0.00001827
Iteration 190/1000 | Loss: 0.00001827
Iteration 191/1000 | Loss: 0.00001827
Iteration 192/1000 | Loss: 0.00001827
Iteration 193/1000 | Loss: 0.00001827
Iteration 194/1000 | Loss: 0.00001827
Iteration 195/1000 | Loss: 0.00001827
Iteration 196/1000 | Loss: 0.00001827
Iteration 197/1000 | Loss: 0.00001826
Iteration 198/1000 | Loss: 0.00001826
Iteration 199/1000 | Loss: 0.00001826
Iteration 200/1000 | Loss: 0.00001826
Iteration 201/1000 | Loss: 0.00001825
Iteration 202/1000 | Loss: 0.00001825
Iteration 203/1000 | Loss: 0.00001825
Iteration 204/1000 | Loss: 0.00001825
Iteration 205/1000 | Loss: 0.00001825
Iteration 206/1000 | Loss: 0.00001824
Iteration 207/1000 | Loss: 0.00001824
Iteration 208/1000 | Loss: 0.00001824
Iteration 209/1000 | Loss: 0.00001823
Iteration 210/1000 | Loss: 0.00001823
Iteration 211/1000 | Loss: 0.00001823
Iteration 212/1000 | Loss: 0.00001823
Iteration 213/1000 | Loss: 0.00001823
Iteration 214/1000 | Loss: 0.00001823
Iteration 215/1000 | Loss: 0.00001823
Iteration 216/1000 | Loss: 0.00001823
Iteration 217/1000 | Loss: 0.00001822
Iteration 218/1000 | Loss: 0.00001822
Iteration 219/1000 | Loss: 0.00001822
Iteration 220/1000 | Loss: 0.00001822
Iteration 221/1000 | Loss: 0.00001822
Iteration 222/1000 | Loss: 0.00001822
Iteration 223/1000 | Loss: 0.00001822
Iteration 224/1000 | Loss: 0.00001822
Iteration 225/1000 | Loss: 0.00001822
Iteration 226/1000 | Loss: 0.00001822
Iteration 227/1000 | Loss: 0.00001821
Iteration 228/1000 | Loss: 0.00001821
Iteration 229/1000 | Loss: 0.00001821
Iteration 230/1000 | Loss: 0.00001821
Iteration 231/1000 | Loss: 0.00001821
Iteration 232/1000 | Loss: 0.00001821
Iteration 233/1000 | Loss: 0.00001821
Iteration 234/1000 | Loss: 0.00001821
Iteration 235/1000 | Loss: 0.00001821
Iteration 236/1000 | Loss: 0.00001821
Iteration 237/1000 | Loss: 0.00001820
Iteration 238/1000 | Loss: 0.00001820
Iteration 239/1000 | Loss: 0.00001820
Iteration 240/1000 | Loss: 0.00001820
Iteration 241/1000 | Loss: 0.00001820
Iteration 242/1000 | Loss: 0.00001820
Iteration 243/1000 | Loss: 0.00001820
Iteration 244/1000 | Loss: 0.00001820
Iteration 245/1000 | Loss: 0.00001820
Iteration 246/1000 | Loss: 0.00001820
Iteration 247/1000 | Loss: 0.00001820
Iteration 248/1000 | Loss: 0.00001820
Iteration 249/1000 | Loss: 0.00001819
Iteration 250/1000 | Loss: 0.00001819
Iteration 251/1000 | Loss: 0.00001819
Iteration 252/1000 | Loss: 0.00001819
Iteration 253/1000 | Loss: 0.00001819
Iteration 254/1000 | Loss: 0.00001819
Iteration 255/1000 | Loss: 0.00001819
Iteration 256/1000 | Loss: 0.00001818
Iteration 257/1000 | Loss: 0.00001818
Iteration 258/1000 | Loss: 0.00001818
Iteration 259/1000 | Loss: 0.00001818
Iteration 260/1000 | Loss: 0.00001818
Iteration 261/1000 | Loss: 0.00001818
Iteration 262/1000 | Loss: 0.00001818
Iteration 263/1000 | Loss: 0.00001818
Iteration 264/1000 | Loss: 0.00001818
Iteration 265/1000 | Loss: 0.00001818
Iteration 266/1000 | Loss: 0.00001818
Iteration 267/1000 | Loss: 0.00001818
Iteration 268/1000 | Loss: 0.00001817
Iteration 269/1000 | Loss: 0.00001817
Iteration 270/1000 | Loss: 0.00001817
Iteration 271/1000 | Loss: 0.00001817
Iteration 272/1000 | Loss: 0.00001817
Iteration 273/1000 | Loss: 0.00001817
Iteration 274/1000 | Loss: 0.00001817
Iteration 275/1000 | Loss: 0.00001816
Iteration 276/1000 | Loss: 0.00001816
Iteration 277/1000 | Loss: 0.00001816
Iteration 278/1000 | Loss: 0.00001816
Iteration 279/1000 | Loss: 0.00001816
Iteration 280/1000 | Loss: 0.00001816
Iteration 281/1000 | Loss: 0.00001816
Iteration 282/1000 | Loss: 0.00001816
Iteration 283/1000 | Loss: 0.00001816
Iteration 284/1000 | Loss: 0.00001816
Iteration 285/1000 | Loss: 0.00001816
Iteration 286/1000 | Loss: 0.00001816
Iteration 287/1000 | Loss: 0.00001816
Iteration 288/1000 | Loss: 0.00001816
Iteration 289/1000 | Loss: 0.00001816
Iteration 290/1000 | Loss: 0.00001816
Iteration 291/1000 | Loss: 0.00001816
Iteration 292/1000 | Loss: 0.00001816
Iteration 293/1000 | Loss: 0.00001816
Iteration 294/1000 | Loss: 0.00001816
Iteration 295/1000 | Loss: 0.00001816
Iteration 296/1000 | Loss: 0.00001816
Iteration 297/1000 | Loss: 0.00001816
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 297. Stopping optimization.
Last 5 losses: [1.8156821170123294e-05, 1.8156821170123294e-05, 1.8156821170123294e-05, 1.8156821170123294e-05, 1.8156821170123294e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8156821170123294e-05

Optimization complete. Final v2v error: 3.4794182777404785 mm

Highest mean error: 5.665761947631836 mm for frame 62

Lowest mean error: 2.8181698322296143 mm for frame 119

Saving results

Total time: 195.27187752723694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952261
Iteration 2/25 | Loss: 0.00127007
Iteration 3/25 | Loss: 0.00094937
Iteration 4/25 | Loss: 0.00089754
Iteration 5/25 | Loss: 0.00087767
Iteration 6/25 | Loss: 0.00087194
Iteration 7/25 | Loss: 0.00087051
Iteration 8/25 | Loss: 0.00087032
Iteration 9/25 | Loss: 0.00087032
Iteration 10/25 | Loss: 0.00087032
Iteration 11/25 | Loss: 0.00087032
Iteration 12/25 | Loss: 0.00087032
Iteration 13/25 | Loss: 0.00087032
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008703212370164692, 0.0008703212370164692, 0.0008703212370164692, 0.0008703212370164692, 0.0008703212370164692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008703212370164692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.52573502
Iteration 2/25 | Loss: 0.00115324
Iteration 3/25 | Loss: 0.00115321
Iteration 4/25 | Loss: 0.00115321
Iteration 5/25 | Loss: 0.00115321
Iteration 6/25 | Loss: 0.00115320
Iteration 7/25 | Loss: 0.00115320
Iteration 8/25 | Loss: 0.00115320
Iteration 9/25 | Loss: 0.00115320
Iteration 10/25 | Loss: 0.00115320
Iteration 11/25 | Loss: 0.00115320
Iteration 12/25 | Loss: 0.00115320
Iteration 13/25 | Loss: 0.00115320
Iteration 14/25 | Loss: 0.00115320
Iteration 15/25 | Loss: 0.00115320
Iteration 16/25 | Loss: 0.00115320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001153203658759594, 0.001153203658759594, 0.001153203658759594, 0.001153203658759594, 0.001153203658759594]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001153203658759594

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115320
Iteration 2/1000 | Loss: 0.00005930
Iteration 3/1000 | Loss: 0.00004049
Iteration 4/1000 | Loss: 0.00003423
Iteration 5/1000 | Loss: 0.00003209
Iteration 6/1000 | Loss: 0.00003067
Iteration 7/1000 | Loss: 0.00002969
Iteration 8/1000 | Loss: 0.00002856
Iteration 9/1000 | Loss: 0.00002795
Iteration 10/1000 | Loss: 0.00002753
Iteration 11/1000 | Loss: 0.00002728
Iteration 12/1000 | Loss: 0.00002716
Iteration 13/1000 | Loss: 0.00002692
Iteration 14/1000 | Loss: 0.00002672
Iteration 15/1000 | Loss: 0.00002672
Iteration 16/1000 | Loss: 0.00002668
Iteration 17/1000 | Loss: 0.00002667
Iteration 18/1000 | Loss: 0.00002667
Iteration 19/1000 | Loss: 0.00002665
Iteration 20/1000 | Loss: 0.00002662
Iteration 21/1000 | Loss: 0.00002660
Iteration 22/1000 | Loss: 0.00002659
Iteration 23/1000 | Loss: 0.00002658
Iteration 24/1000 | Loss: 0.00002658
Iteration 25/1000 | Loss: 0.00002657
Iteration 26/1000 | Loss: 0.00002656
Iteration 27/1000 | Loss: 0.00002656
Iteration 28/1000 | Loss: 0.00002655
Iteration 29/1000 | Loss: 0.00002655
Iteration 30/1000 | Loss: 0.00002655
Iteration 31/1000 | Loss: 0.00002654
Iteration 32/1000 | Loss: 0.00002654
Iteration 33/1000 | Loss: 0.00002653
Iteration 34/1000 | Loss: 0.00002652
Iteration 35/1000 | Loss: 0.00002652
Iteration 36/1000 | Loss: 0.00002652
Iteration 37/1000 | Loss: 0.00002651
Iteration 38/1000 | Loss: 0.00002651
Iteration 39/1000 | Loss: 0.00002651
Iteration 40/1000 | Loss: 0.00002650
Iteration 41/1000 | Loss: 0.00002650
Iteration 42/1000 | Loss: 0.00002650
Iteration 43/1000 | Loss: 0.00002649
Iteration 44/1000 | Loss: 0.00002649
Iteration 45/1000 | Loss: 0.00002648
Iteration 46/1000 | Loss: 0.00002648
Iteration 47/1000 | Loss: 0.00002647
Iteration 48/1000 | Loss: 0.00002647
Iteration 49/1000 | Loss: 0.00002647
Iteration 50/1000 | Loss: 0.00002647
Iteration 51/1000 | Loss: 0.00002647
Iteration 52/1000 | Loss: 0.00002646
Iteration 53/1000 | Loss: 0.00002646
Iteration 54/1000 | Loss: 0.00002646
Iteration 55/1000 | Loss: 0.00002645
Iteration 56/1000 | Loss: 0.00002645
Iteration 57/1000 | Loss: 0.00002645
Iteration 58/1000 | Loss: 0.00002644
Iteration 59/1000 | Loss: 0.00002644
Iteration 60/1000 | Loss: 0.00002644
Iteration 61/1000 | Loss: 0.00002644
Iteration 62/1000 | Loss: 0.00002643
Iteration 63/1000 | Loss: 0.00002643
Iteration 64/1000 | Loss: 0.00002643
Iteration 65/1000 | Loss: 0.00002643
Iteration 66/1000 | Loss: 0.00002642
Iteration 67/1000 | Loss: 0.00002642
Iteration 68/1000 | Loss: 0.00002641
Iteration 69/1000 | Loss: 0.00002640
Iteration 70/1000 | Loss: 0.00002640
Iteration 71/1000 | Loss: 0.00002640
Iteration 72/1000 | Loss: 0.00002640
Iteration 73/1000 | Loss: 0.00002639
Iteration 74/1000 | Loss: 0.00002639
Iteration 75/1000 | Loss: 0.00002638
Iteration 76/1000 | Loss: 0.00002638
Iteration 77/1000 | Loss: 0.00002638
Iteration 78/1000 | Loss: 0.00002637
Iteration 79/1000 | Loss: 0.00002637
Iteration 80/1000 | Loss: 0.00002637
Iteration 81/1000 | Loss: 0.00002637
Iteration 82/1000 | Loss: 0.00002636
Iteration 83/1000 | Loss: 0.00002636
Iteration 84/1000 | Loss: 0.00002636
Iteration 85/1000 | Loss: 0.00002636
Iteration 86/1000 | Loss: 0.00002636
Iteration 87/1000 | Loss: 0.00002636
Iteration 88/1000 | Loss: 0.00002635
Iteration 89/1000 | Loss: 0.00002635
Iteration 90/1000 | Loss: 0.00002635
Iteration 91/1000 | Loss: 0.00002634
Iteration 92/1000 | Loss: 0.00002634
Iteration 93/1000 | Loss: 0.00002634
Iteration 94/1000 | Loss: 0.00002634
Iteration 95/1000 | Loss: 0.00002634
Iteration 96/1000 | Loss: 0.00002634
Iteration 97/1000 | Loss: 0.00002633
Iteration 98/1000 | Loss: 0.00002633
Iteration 99/1000 | Loss: 0.00002633
Iteration 100/1000 | Loss: 0.00002633
Iteration 101/1000 | Loss: 0.00002633
Iteration 102/1000 | Loss: 0.00002633
Iteration 103/1000 | Loss: 0.00002632
Iteration 104/1000 | Loss: 0.00002632
Iteration 105/1000 | Loss: 0.00002632
Iteration 106/1000 | Loss: 0.00002632
Iteration 107/1000 | Loss: 0.00002632
Iteration 108/1000 | Loss: 0.00002632
Iteration 109/1000 | Loss: 0.00002631
Iteration 110/1000 | Loss: 0.00002631
Iteration 111/1000 | Loss: 0.00002631
Iteration 112/1000 | Loss: 0.00002631
Iteration 113/1000 | Loss: 0.00002631
Iteration 114/1000 | Loss: 0.00002631
Iteration 115/1000 | Loss: 0.00002631
Iteration 116/1000 | Loss: 0.00002631
Iteration 117/1000 | Loss: 0.00002631
Iteration 118/1000 | Loss: 0.00002630
Iteration 119/1000 | Loss: 0.00002630
Iteration 120/1000 | Loss: 0.00002630
Iteration 121/1000 | Loss: 0.00002630
Iteration 122/1000 | Loss: 0.00002630
Iteration 123/1000 | Loss: 0.00002630
Iteration 124/1000 | Loss: 0.00002630
Iteration 125/1000 | Loss: 0.00002630
Iteration 126/1000 | Loss: 0.00002630
Iteration 127/1000 | Loss: 0.00002630
Iteration 128/1000 | Loss: 0.00002630
Iteration 129/1000 | Loss: 0.00002630
Iteration 130/1000 | Loss: 0.00002629
Iteration 131/1000 | Loss: 0.00002629
Iteration 132/1000 | Loss: 0.00002629
Iteration 133/1000 | Loss: 0.00002629
Iteration 134/1000 | Loss: 0.00002629
Iteration 135/1000 | Loss: 0.00002629
Iteration 136/1000 | Loss: 0.00002629
Iteration 137/1000 | Loss: 0.00002629
Iteration 138/1000 | Loss: 0.00002629
Iteration 139/1000 | Loss: 0.00002629
Iteration 140/1000 | Loss: 0.00002629
Iteration 141/1000 | Loss: 0.00002629
Iteration 142/1000 | Loss: 0.00002629
Iteration 143/1000 | Loss: 0.00002629
Iteration 144/1000 | Loss: 0.00002628
Iteration 145/1000 | Loss: 0.00002628
Iteration 146/1000 | Loss: 0.00002628
Iteration 147/1000 | Loss: 0.00002628
Iteration 148/1000 | Loss: 0.00002628
Iteration 149/1000 | Loss: 0.00002628
Iteration 150/1000 | Loss: 0.00002628
Iteration 151/1000 | Loss: 0.00002628
Iteration 152/1000 | Loss: 0.00002628
Iteration 153/1000 | Loss: 0.00002628
Iteration 154/1000 | Loss: 0.00002628
Iteration 155/1000 | Loss: 0.00002628
Iteration 156/1000 | Loss: 0.00002628
Iteration 157/1000 | Loss: 0.00002628
Iteration 158/1000 | Loss: 0.00002628
Iteration 159/1000 | Loss: 0.00002628
Iteration 160/1000 | Loss: 0.00002628
Iteration 161/1000 | Loss: 0.00002628
Iteration 162/1000 | Loss: 0.00002627
Iteration 163/1000 | Loss: 0.00002627
Iteration 164/1000 | Loss: 0.00002627
Iteration 165/1000 | Loss: 0.00002627
Iteration 166/1000 | Loss: 0.00002627
Iteration 167/1000 | Loss: 0.00002627
Iteration 168/1000 | Loss: 0.00002627
Iteration 169/1000 | Loss: 0.00002627
Iteration 170/1000 | Loss: 0.00002627
Iteration 171/1000 | Loss: 0.00002627
Iteration 172/1000 | Loss: 0.00002627
Iteration 173/1000 | Loss: 0.00002627
Iteration 174/1000 | Loss: 0.00002627
Iteration 175/1000 | Loss: 0.00002627
Iteration 176/1000 | Loss: 0.00002627
Iteration 177/1000 | Loss: 0.00002627
Iteration 178/1000 | Loss: 0.00002627
Iteration 179/1000 | Loss: 0.00002627
Iteration 180/1000 | Loss: 0.00002627
Iteration 181/1000 | Loss: 0.00002627
Iteration 182/1000 | Loss: 0.00002627
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.6269950467394665e-05, 2.6269950467394665e-05, 2.6269950467394665e-05, 2.6269950467394665e-05, 2.6269950467394665e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6269950467394665e-05

Optimization complete. Final v2v error: 4.2691755294799805 mm

Highest mean error: 5.284151077270508 mm for frame 59

Lowest mean error: 3.758546829223633 mm for frame 46

Saving results

Total time: 43.144688844680786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00857959
Iteration 2/25 | Loss: 0.00088938
Iteration 3/25 | Loss: 0.00075645
Iteration 4/25 | Loss: 0.00073790
Iteration 5/25 | Loss: 0.00073310
Iteration 6/25 | Loss: 0.00073164
Iteration 7/25 | Loss: 0.00073148
Iteration 8/25 | Loss: 0.00073148
Iteration 9/25 | Loss: 0.00073148
Iteration 10/25 | Loss: 0.00073148
Iteration 11/25 | Loss: 0.00073148
Iteration 12/25 | Loss: 0.00073148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007314806571230292, 0.0007314806571230292, 0.0007314806571230292, 0.0007314806571230292, 0.0007314806571230292]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007314806571230292

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59116805
Iteration 2/25 | Loss: 0.00121705
Iteration 3/25 | Loss: 0.00121705
Iteration 4/25 | Loss: 0.00121705
Iteration 5/25 | Loss: 0.00121705
Iteration 6/25 | Loss: 0.00121705
Iteration 7/25 | Loss: 0.00121705
Iteration 8/25 | Loss: 0.00121705
Iteration 9/25 | Loss: 0.00121704
Iteration 10/25 | Loss: 0.00121704
Iteration 11/25 | Loss: 0.00121704
Iteration 12/25 | Loss: 0.00121704
Iteration 13/25 | Loss: 0.00121704
Iteration 14/25 | Loss: 0.00121704
Iteration 15/25 | Loss: 0.00121704
Iteration 16/25 | Loss: 0.00121704
Iteration 17/25 | Loss: 0.00121704
Iteration 18/25 | Loss: 0.00121704
Iteration 19/25 | Loss: 0.00121704
Iteration 20/25 | Loss: 0.00121704
Iteration 21/25 | Loss: 0.00121704
Iteration 22/25 | Loss: 0.00121704
Iteration 23/25 | Loss: 0.00121704
Iteration 24/25 | Loss: 0.00121704
Iteration 25/25 | Loss: 0.00121704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121704
Iteration 2/1000 | Loss: 0.00002220
Iteration 3/1000 | Loss: 0.00001451
Iteration 4/1000 | Loss: 0.00001316
Iteration 5/1000 | Loss: 0.00001226
Iteration 6/1000 | Loss: 0.00001178
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001146
Iteration 9/1000 | Loss: 0.00001141
Iteration 10/1000 | Loss: 0.00001135
Iteration 11/1000 | Loss: 0.00001117
Iteration 12/1000 | Loss: 0.00001107
Iteration 13/1000 | Loss: 0.00001105
Iteration 14/1000 | Loss: 0.00001104
Iteration 15/1000 | Loss: 0.00001103
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001100
Iteration 18/1000 | Loss: 0.00001099
Iteration 19/1000 | Loss: 0.00001098
Iteration 20/1000 | Loss: 0.00001097
Iteration 21/1000 | Loss: 0.00001096
Iteration 22/1000 | Loss: 0.00001096
Iteration 23/1000 | Loss: 0.00001096
Iteration 24/1000 | Loss: 0.00001096
Iteration 25/1000 | Loss: 0.00001096
Iteration 26/1000 | Loss: 0.00001095
Iteration 27/1000 | Loss: 0.00001095
Iteration 28/1000 | Loss: 0.00001095
Iteration 29/1000 | Loss: 0.00001095
Iteration 30/1000 | Loss: 0.00001095
Iteration 31/1000 | Loss: 0.00001094
Iteration 32/1000 | Loss: 0.00001094
Iteration 33/1000 | Loss: 0.00001093
Iteration 34/1000 | Loss: 0.00001093
Iteration 35/1000 | Loss: 0.00001093
Iteration 36/1000 | Loss: 0.00001093
Iteration 37/1000 | Loss: 0.00001092
Iteration 38/1000 | Loss: 0.00001091
Iteration 39/1000 | Loss: 0.00001090
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001087
Iteration 42/1000 | Loss: 0.00001087
Iteration 43/1000 | Loss: 0.00001086
Iteration 44/1000 | Loss: 0.00001086
Iteration 45/1000 | Loss: 0.00001085
Iteration 46/1000 | Loss: 0.00001084
Iteration 47/1000 | Loss: 0.00001084
Iteration 48/1000 | Loss: 0.00001083
Iteration 49/1000 | Loss: 0.00001083
Iteration 50/1000 | Loss: 0.00001083
Iteration 51/1000 | Loss: 0.00001083
Iteration 52/1000 | Loss: 0.00001083
Iteration 53/1000 | Loss: 0.00001083
Iteration 54/1000 | Loss: 0.00001083
Iteration 55/1000 | Loss: 0.00001082
Iteration 56/1000 | Loss: 0.00001082
Iteration 57/1000 | Loss: 0.00001082
Iteration 58/1000 | Loss: 0.00001082
Iteration 59/1000 | Loss: 0.00001081
Iteration 60/1000 | Loss: 0.00001081
Iteration 61/1000 | Loss: 0.00001081
Iteration 62/1000 | Loss: 0.00001081
Iteration 63/1000 | Loss: 0.00001081
Iteration 64/1000 | Loss: 0.00001081
Iteration 65/1000 | Loss: 0.00001081
Iteration 66/1000 | Loss: 0.00001081
Iteration 67/1000 | Loss: 0.00001081
Iteration 68/1000 | Loss: 0.00001081
Iteration 69/1000 | Loss: 0.00001081
Iteration 70/1000 | Loss: 0.00001080
Iteration 71/1000 | Loss: 0.00001080
Iteration 72/1000 | Loss: 0.00001080
Iteration 73/1000 | Loss: 0.00001080
Iteration 74/1000 | Loss: 0.00001080
Iteration 75/1000 | Loss: 0.00001080
Iteration 76/1000 | Loss: 0.00001080
Iteration 77/1000 | Loss: 0.00001080
Iteration 78/1000 | Loss: 0.00001080
Iteration 79/1000 | Loss: 0.00001080
Iteration 80/1000 | Loss: 0.00001080
Iteration 81/1000 | Loss: 0.00001080
Iteration 82/1000 | Loss: 0.00001080
Iteration 83/1000 | Loss: 0.00001079
Iteration 84/1000 | Loss: 0.00001079
Iteration 85/1000 | Loss: 0.00001079
Iteration 86/1000 | Loss: 0.00001079
Iteration 87/1000 | Loss: 0.00001079
Iteration 88/1000 | Loss: 0.00001079
Iteration 89/1000 | Loss: 0.00001079
Iteration 90/1000 | Loss: 0.00001078
Iteration 91/1000 | Loss: 0.00001078
Iteration 92/1000 | Loss: 0.00001077
Iteration 93/1000 | Loss: 0.00001077
Iteration 94/1000 | Loss: 0.00001077
Iteration 95/1000 | Loss: 0.00001077
Iteration 96/1000 | Loss: 0.00001077
Iteration 97/1000 | Loss: 0.00001076
Iteration 98/1000 | Loss: 0.00001076
Iteration 99/1000 | Loss: 0.00001076
Iteration 100/1000 | Loss: 0.00001076
Iteration 101/1000 | Loss: 0.00001074
Iteration 102/1000 | Loss: 0.00001074
Iteration 103/1000 | Loss: 0.00001074
Iteration 104/1000 | Loss: 0.00001074
Iteration 105/1000 | Loss: 0.00001074
Iteration 106/1000 | Loss: 0.00001074
Iteration 107/1000 | Loss: 0.00001074
Iteration 108/1000 | Loss: 0.00001074
Iteration 109/1000 | Loss: 0.00001074
Iteration 110/1000 | Loss: 0.00001074
Iteration 111/1000 | Loss: 0.00001073
Iteration 112/1000 | Loss: 0.00001073
Iteration 113/1000 | Loss: 0.00001073
Iteration 114/1000 | Loss: 0.00001073
Iteration 115/1000 | Loss: 0.00001073
Iteration 116/1000 | Loss: 0.00001073
Iteration 117/1000 | Loss: 0.00001073
Iteration 118/1000 | Loss: 0.00001072
Iteration 119/1000 | Loss: 0.00001071
Iteration 120/1000 | Loss: 0.00001070
Iteration 121/1000 | Loss: 0.00001070
Iteration 122/1000 | Loss: 0.00001070
Iteration 123/1000 | Loss: 0.00001070
Iteration 124/1000 | Loss: 0.00001070
Iteration 125/1000 | Loss: 0.00001070
Iteration 126/1000 | Loss: 0.00001070
Iteration 127/1000 | Loss: 0.00001070
Iteration 128/1000 | Loss: 0.00001070
Iteration 129/1000 | Loss: 0.00001070
Iteration 130/1000 | Loss: 0.00001070
Iteration 131/1000 | Loss: 0.00001070
Iteration 132/1000 | Loss: 0.00001070
Iteration 133/1000 | Loss: 0.00001070
Iteration 134/1000 | Loss: 0.00001069
Iteration 135/1000 | Loss: 0.00001069
Iteration 136/1000 | Loss: 0.00001069
Iteration 137/1000 | Loss: 0.00001069
Iteration 138/1000 | Loss: 0.00001068
Iteration 139/1000 | Loss: 0.00001068
Iteration 140/1000 | Loss: 0.00001068
Iteration 141/1000 | Loss: 0.00001068
Iteration 142/1000 | Loss: 0.00001068
Iteration 143/1000 | Loss: 0.00001068
Iteration 144/1000 | Loss: 0.00001068
Iteration 145/1000 | Loss: 0.00001068
Iteration 146/1000 | Loss: 0.00001068
Iteration 147/1000 | Loss: 0.00001068
Iteration 148/1000 | Loss: 0.00001068
Iteration 149/1000 | Loss: 0.00001067
Iteration 150/1000 | Loss: 0.00001067
Iteration 151/1000 | Loss: 0.00001067
Iteration 152/1000 | Loss: 0.00001067
Iteration 153/1000 | Loss: 0.00001067
Iteration 154/1000 | Loss: 0.00001067
Iteration 155/1000 | Loss: 0.00001067
Iteration 156/1000 | Loss: 0.00001067
Iteration 157/1000 | Loss: 0.00001066
Iteration 158/1000 | Loss: 0.00001066
Iteration 159/1000 | Loss: 0.00001066
Iteration 160/1000 | Loss: 0.00001066
Iteration 161/1000 | Loss: 0.00001066
Iteration 162/1000 | Loss: 0.00001066
Iteration 163/1000 | Loss: 0.00001066
Iteration 164/1000 | Loss: 0.00001066
Iteration 165/1000 | Loss: 0.00001066
Iteration 166/1000 | Loss: 0.00001066
Iteration 167/1000 | Loss: 0.00001066
Iteration 168/1000 | Loss: 0.00001065
Iteration 169/1000 | Loss: 0.00001065
Iteration 170/1000 | Loss: 0.00001065
Iteration 171/1000 | Loss: 0.00001065
Iteration 172/1000 | Loss: 0.00001065
Iteration 173/1000 | Loss: 0.00001065
Iteration 174/1000 | Loss: 0.00001065
Iteration 175/1000 | Loss: 0.00001065
Iteration 176/1000 | Loss: 0.00001065
Iteration 177/1000 | Loss: 0.00001065
Iteration 178/1000 | Loss: 0.00001065
Iteration 179/1000 | Loss: 0.00001065
Iteration 180/1000 | Loss: 0.00001064
Iteration 181/1000 | Loss: 0.00001064
Iteration 182/1000 | Loss: 0.00001064
Iteration 183/1000 | Loss: 0.00001064
Iteration 184/1000 | Loss: 0.00001064
Iteration 185/1000 | Loss: 0.00001064
Iteration 186/1000 | Loss: 0.00001064
Iteration 187/1000 | Loss: 0.00001064
Iteration 188/1000 | Loss: 0.00001064
Iteration 189/1000 | Loss: 0.00001064
Iteration 190/1000 | Loss: 0.00001064
Iteration 191/1000 | Loss: 0.00001064
Iteration 192/1000 | Loss: 0.00001064
Iteration 193/1000 | Loss: 0.00001064
Iteration 194/1000 | Loss: 0.00001064
Iteration 195/1000 | Loss: 0.00001064
Iteration 196/1000 | Loss: 0.00001064
Iteration 197/1000 | Loss: 0.00001064
Iteration 198/1000 | Loss: 0.00001064
Iteration 199/1000 | Loss: 0.00001064
Iteration 200/1000 | Loss: 0.00001064
Iteration 201/1000 | Loss: 0.00001064
Iteration 202/1000 | Loss: 0.00001064
Iteration 203/1000 | Loss: 0.00001064
Iteration 204/1000 | Loss: 0.00001064
Iteration 205/1000 | Loss: 0.00001064
Iteration 206/1000 | Loss: 0.00001064
Iteration 207/1000 | Loss: 0.00001064
Iteration 208/1000 | Loss: 0.00001064
Iteration 209/1000 | Loss: 0.00001064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [1.0643851055647247e-05, 1.0643851055647247e-05, 1.0643851055647247e-05, 1.0643851055647247e-05, 1.0643851055647247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0643851055647247e-05

Optimization complete. Final v2v error: 2.7617762088775635 mm

Highest mean error: 3.0121407508850098 mm for frame 107

Lowest mean error: 2.5857186317443848 mm for frame 195

Saving results

Total time: 39.89278960227966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00640932
Iteration 2/25 | Loss: 0.00150190
Iteration 3/25 | Loss: 0.00113961
Iteration 4/25 | Loss: 0.00106974
Iteration 5/25 | Loss: 0.00104841
Iteration 6/25 | Loss: 0.00103664
Iteration 7/25 | Loss: 0.00102659
Iteration 8/25 | Loss: 0.00102359
Iteration 9/25 | Loss: 0.00101640
Iteration 10/25 | Loss: 0.00101771
Iteration 11/25 | Loss: 0.00101845
Iteration 12/25 | Loss: 0.00101164
Iteration 13/25 | Loss: 0.00100645
Iteration 14/25 | Loss: 0.00100347
Iteration 15/25 | Loss: 0.00100253
Iteration 16/25 | Loss: 0.00100176
Iteration 17/25 | Loss: 0.00101068
Iteration 18/25 | Loss: 0.00101127
Iteration 19/25 | Loss: 0.00100430
Iteration 20/25 | Loss: 0.00099706
Iteration 21/25 | Loss: 0.00099066
Iteration 22/25 | Loss: 0.00099499
Iteration 23/25 | Loss: 0.00099326
Iteration 24/25 | Loss: 0.00099040
Iteration 25/25 | Loss: 0.00098781

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58458650
Iteration 2/25 | Loss: 0.00268930
Iteration 3/25 | Loss: 0.00268930
Iteration 4/25 | Loss: 0.00268929
Iteration 5/25 | Loss: 0.00268929
Iteration 6/25 | Loss: 0.00268929
Iteration 7/25 | Loss: 0.00268929
Iteration 8/25 | Loss: 0.00268929
Iteration 9/25 | Loss: 0.00268929
Iteration 10/25 | Loss: 0.00268929
Iteration 11/25 | Loss: 0.00268929
Iteration 12/25 | Loss: 0.00268929
Iteration 13/25 | Loss: 0.00268929
Iteration 14/25 | Loss: 0.00268929
Iteration 15/25 | Loss: 0.00268929
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002689292887225747, 0.002689292887225747, 0.002689292887225747, 0.002689292887225747, 0.002689292887225747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002689292887225747

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268929
Iteration 2/1000 | Loss: 0.00036808
Iteration 3/1000 | Loss: 0.00023706
Iteration 4/1000 | Loss: 0.00017405
Iteration 5/1000 | Loss: 0.00111136
Iteration 6/1000 | Loss: 0.00392589
Iteration 7/1000 | Loss: 0.00125151
Iteration 8/1000 | Loss: 0.00196933
Iteration 9/1000 | Loss: 0.00103338
Iteration 10/1000 | Loss: 0.00253779
Iteration 11/1000 | Loss: 0.00160697
Iteration 12/1000 | Loss: 0.00236383
Iteration 13/1000 | Loss: 0.00149408
Iteration 14/1000 | Loss: 0.00135699
Iteration 15/1000 | Loss: 0.00070380
Iteration 16/1000 | Loss: 0.00189855
Iteration 17/1000 | Loss: 0.00175547
Iteration 18/1000 | Loss: 0.00096904
Iteration 19/1000 | Loss: 0.00020370
Iteration 20/1000 | Loss: 0.00056358
Iteration 21/1000 | Loss: 0.00094215
Iteration 22/1000 | Loss: 0.00307729
Iteration 23/1000 | Loss: 0.00054697
Iteration 24/1000 | Loss: 0.00012860
Iteration 25/1000 | Loss: 0.00010950
Iteration 26/1000 | Loss: 0.00137674
Iteration 27/1000 | Loss: 0.00066519
Iteration 28/1000 | Loss: 0.00105696
Iteration 29/1000 | Loss: 0.00010604
Iteration 30/1000 | Loss: 0.00009105
Iteration 31/1000 | Loss: 0.00043228
Iteration 32/1000 | Loss: 0.00023265
Iteration 33/1000 | Loss: 0.00020502
Iteration 34/1000 | Loss: 0.00007011
Iteration 35/1000 | Loss: 0.00006586
Iteration 36/1000 | Loss: 0.00055569
Iteration 37/1000 | Loss: 0.00018304
Iteration 38/1000 | Loss: 0.00006216
Iteration 39/1000 | Loss: 0.00005995
Iteration 40/1000 | Loss: 0.00005833
Iteration 41/1000 | Loss: 0.00036720
Iteration 42/1000 | Loss: 0.00229889
Iteration 43/1000 | Loss: 0.00012112
Iteration 44/1000 | Loss: 0.00055395
Iteration 45/1000 | Loss: 0.00007100
Iteration 46/1000 | Loss: 0.00038463
Iteration 47/1000 | Loss: 0.00006291
Iteration 48/1000 | Loss: 0.00005784
Iteration 49/1000 | Loss: 0.00025842
Iteration 50/1000 | Loss: 0.00280573
Iteration 51/1000 | Loss: 0.00311741
Iteration 52/1000 | Loss: 0.00054985
Iteration 53/1000 | Loss: 0.00040811
Iteration 54/1000 | Loss: 0.00005674
Iteration 55/1000 | Loss: 0.00044555
Iteration 56/1000 | Loss: 0.00026506
Iteration 57/1000 | Loss: 0.00005827
Iteration 58/1000 | Loss: 0.00004834
Iteration 59/1000 | Loss: 0.00004495
Iteration 60/1000 | Loss: 0.00068415
Iteration 61/1000 | Loss: 0.00075428
Iteration 62/1000 | Loss: 0.00247280
Iteration 63/1000 | Loss: 0.00118169
Iteration 64/1000 | Loss: 0.00093024
Iteration 65/1000 | Loss: 0.00060959
Iteration 66/1000 | Loss: 0.00035246
Iteration 67/1000 | Loss: 0.00033558
Iteration 68/1000 | Loss: 0.00016385
Iteration 69/1000 | Loss: 0.00025809
Iteration 70/1000 | Loss: 0.00004428
Iteration 71/1000 | Loss: 0.00003823
Iteration 72/1000 | Loss: 0.00003569
Iteration 73/1000 | Loss: 0.00003295
Iteration 74/1000 | Loss: 0.00003153
Iteration 75/1000 | Loss: 0.00003045
Iteration 76/1000 | Loss: 0.00080123
Iteration 77/1000 | Loss: 0.00036082
Iteration 78/1000 | Loss: 0.00111445
Iteration 79/1000 | Loss: 0.00074777
Iteration 80/1000 | Loss: 0.00006723
Iteration 81/1000 | Loss: 0.00004007
Iteration 82/1000 | Loss: 0.00002974
Iteration 83/1000 | Loss: 0.00066030
Iteration 84/1000 | Loss: 0.00037644
Iteration 85/1000 | Loss: 0.00003725
Iteration 86/1000 | Loss: 0.00063528
Iteration 87/1000 | Loss: 0.00063474
Iteration 88/1000 | Loss: 0.00003692
Iteration 89/1000 | Loss: 0.00002889
Iteration 90/1000 | Loss: 0.00064605
Iteration 91/1000 | Loss: 0.00034397
Iteration 92/1000 | Loss: 0.00041223
Iteration 93/1000 | Loss: 0.00004089
Iteration 94/1000 | Loss: 0.00003389
Iteration 95/1000 | Loss: 0.00002971
Iteration 96/1000 | Loss: 0.00002835
Iteration 97/1000 | Loss: 0.00002634
Iteration 98/1000 | Loss: 0.00002476
Iteration 99/1000 | Loss: 0.00002422
Iteration 100/1000 | Loss: 0.00002399
Iteration 101/1000 | Loss: 0.00002388
Iteration 102/1000 | Loss: 0.00002363
Iteration 103/1000 | Loss: 0.00002357
Iteration 104/1000 | Loss: 0.00002352
Iteration 105/1000 | Loss: 0.00002348
Iteration 106/1000 | Loss: 0.00002347
Iteration 107/1000 | Loss: 0.00002347
Iteration 108/1000 | Loss: 0.00002346
Iteration 109/1000 | Loss: 0.00002346
Iteration 110/1000 | Loss: 0.00002346
Iteration 111/1000 | Loss: 0.00002345
Iteration 112/1000 | Loss: 0.00002345
Iteration 113/1000 | Loss: 0.00002345
Iteration 114/1000 | Loss: 0.00002344
Iteration 115/1000 | Loss: 0.00002343
Iteration 116/1000 | Loss: 0.00002342
Iteration 117/1000 | Loss: 0.00002339
Iteration 118/1000 | Loss: 0.00002338
Iteration 119/1000 | Loss: 0.00002337
Iteration 120/1000 | Loss: 0.00002335
Iteration 121/1000 | Loss: 0.00002334
Iteration 122/1000 | Loss: 0.00002332
Iteration 123/1000 | Loss: 0.00002331
Iteration 124/1000 | Loss: 0.00002330
Iteration 125/1000 | Loss: 0.00002329
Iteration 126/1000 | Loss: 0.00002327
Iteration 127/1000 | Loss: 0.00002327
Iteration 128/1000 | Loss: 0.00002326
Iteration 129/1000 | Loss: 0.00002326
Iteration 130/1000 | Loss: 0.00002325
Iteration 131/1000 | Loss: 0.00002325
Iteration 132/1000 | Loss: 0.00002324
Iteration 133/1000 | Loss: 0.00002324
Iteration 134/1000 | Loss: 0.00002323
Iteration 135/1000 | Loss: 0.00002323
Iteration 136/1000 | Loss: 0.00002323
Iteration 137/1000 | Loss: 0.00002322
Iteration 138/1000 | Loss: 0.00002322
Iteration 139/1000 | Loss: 0.00002322
Iteration 140/1000 | Loss: 0.00002322
Iteration 141/1000 | Loss: 0.00002321
Iteration 142/1000 | Loss: 0.00002321
Iteration 143/1000 | Loss: 0.00002321
Iteration 144/1000 | Loss: 0.00002321
Iteration 145/1000 | Loss: 0.00002321
Iteration 146/1000 | Loss: 0.00002321
Iteration 147/1000 | Loss: 0.00002321
Iteration 148/1000 | Loss: 0.00002320
Iteration 149/1000 | Loss: 0.00002320
Iteration 150/1000 | Loss: 0.00002320
Iteration 151/1000 | Loss: 0.00002320
Iteration 152/1000 | Loss: 0.00002320
Iteration 153/1000 | Loss: 0.00002320
Iteration 154/1000 | Loss: 0.00002320
Iteration 155/1000 | Loss: 0.00002320
Iteration 156/1000 | Loss: 0.00002319
Iteration 157/1000 | Loss: 0.00002319
Iteration 158/1000 | Loss: 0.00002319
Iteration 159/1000 | Loss: 0.00002319
Iteration 160/1000 | Loss: 0.00002319
Iteration 161/1000 | Loss: 0.00002319
Iteration 162/1000 | Loss: 0.00002319
Iteration 163/1000 | Loss: 0.00002318
Iteration 164/1000 | Loss: 0.00002318
Iteration 165/1000 | Loss: 0.00002318
Iteration 166/1000 | Loss: 0.00002318
Iteration 167/1000 | Loss: 0.00002318
Iteration 168/1000 | Loss: 0.00002318
Iteration 169/1000 | Loss: 0.00002318
Iteration 170/1000 | Loss: 0.00002318
Iteration 171/1000 | Loss: 0.00002317
Iteration 172/1000 | Loss: 0.00002317
Iteration 173/1000 | Loss: 0.00002317
Iteration 174/1000 | Loss: 0.00002317
Iteration 175/1000 | Loss: 0.00002317
Iteration 176/1000 | Loss: 0.00002317
Iteration 177/1000 | Loss: 0.00002317
Iteration 178/1000 | Loss: 0.00002317
Iteration 179/1000 | Loss: 0.00002317
Iteration 180/1000 | Loss: 0.00002317
Iteration 181/1000 | Loss: 0.00002317
Iteration 182/1000 | Loss: 0.00002317
Iteration 183/1000 | Loss: 0.00002317
Iteration 184/1000 | Loss: 0.00002317
Iteration 185/1000 | Loss: 0.00002317
Iteration 186/1000 | Loss: 0.00002317
Iteration 187/1000 | Loss: 0.00002317
Iteration 188/1000 | Loss: 0.00002317
Iteration 189/1000 | Loss: 0.00002317
Iteration 190/1000 | Loss: 0.00002317
Iteration 191/1000 | Loss: 0.00002316
Iteration 192/1000 | Loss: 0.00002316
Iteration 193/1000 | Loss: 0.00002316
Iteration 194/1000 | Loss: 0.00002316
Iteration 195/1000 | Loss: 0.00002316
Iteration 196/1000 | Loss: 0.00002316
Iteration 197/1000 | Loss: 0.00002316
Iteration 198/1000 | Loss: 0.00002316
Iteration 199/1000 | Loss: 0.00002316
Iteration 200/1000 | Loss: 0.00002316
Iteration 201/1000 | Loss: 0.00002316
Iteration 202/1000 | Loss: 0.00002316
Iteration 203/1000 | Loss: 0.00002316
Iteration 204/1000 | Loss: 0.00002316
Iteration 205/1000 | Loss: 0.00002316
Iteration 206/1000 | Loss: 0.00002316
Iteration 207/1000 | Loss: 0.00002316
Iteration 208/1000 | Loss: 0.00002316
Iteration 209/1000 | Loss: 0.00002316
Iteration 210/1000 | Loss: 0.00002316
Iteration 211/1000 | Loss: 0.00002316
Iteration 212/1000 | Loss: 0.00002316
Iteration 213/1000 | Loss: 0.00002316
Iteration 214/1000 | Loss: 0.00002316
Iteration 215/1000 | Loss: 0.00002316
Iteration 216/1000 | Loss: 0.00002316
Iteration 217/1000 | Loss: 0.00002316
Iteration 218/1000 | Loss: 0.00002316
Iteration 219/1000 | Loss: 0.00002316
Iteration 220/1000 | Loss: 0.00002316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [2.3162825527833775e-05, 2.3162825527833775e-05, 2.3162825527833775e-05, 2.3162825527833775e-05, 2.3162825527833775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3162825527833775e-05

Optimization complete. Final v2v error: 3.8008816242218018 mm

Highest mean error: 11.29781723022461 mm for frame 14

Lowest mean error: 3.0817582607269287 mm for frame 81

Saving results

Total time: 226.18769145011902
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01050537
Iteration 2/25 | Loss: 0.01050536
Iteration 3/25 | Loss: 0.00246853
Iteration 4/25 | Loss: 0.00132133
Iteration 5/25 | Loss: 0.00108154
Iteration 6/25 | Loss: 0.00103022
Iteration 7/25 | Loss: 0.00091035
Iteration 8/25 | Loss: 0.00085975
Iteration 9/25 | Loss: 0.00084762
Iteration 10/25 | Loss: 0.00083518
Iteration 11/25 | Loss: 0.00081714
Iteration 12/25 | Loss: 0.00081437
Iteration 13/25 | Loss: 0.00081130
Iteration 14/25 | Loss: 0.00081328
Iteration 15/25 | Loss: 0.00080677
Iteration 16/25 | Loss: 0.00080457
Iteration 17/25 | Loss: 0.00080394
Iteration 18/25 | Loss: 0.00080349
Iteration 19/25 | Loss: 0.00080200
Iteration 20/25 | Loss: 0.00080155
Iteration 21/25 | Loss: 0.00080139
Iteration 22/25 | Loss: 0.00080128
Iteration 23/25 | Loss: 0.00080322
Iteration 24/25 | Loss: 0.00080222
Iteration 25/25 | Loss: 0.00079954

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.71438050
Iteration 2/25 | Loss: 0.00165192
Iteration 3/25 | Loss: 0.00159790
Iteration 4/25 | Loss: 0.00159790
Iteration 5/25 | Loss: 0.00159790
Iteration 6/25 | Loss: 0.00159790
Iteration 7/25 | Loss: 0.00159790
Iteration 8/25 | Loss: 0.00159790
Iteration 9/25 | Loss: 0.00159790
Iteration 10/25 | Loss: 0.00159790
Iteration 11/25 | Loss: 0.00159790
Iteration 12/25 | Loss: 0.00159790
Iteration 13/25 | Loss: 0.00159790
Iteration 14/25 | Loss: 0.00159790
Iteration 15/25 | Loss: 0.00159790
Iteration 16/25 | Loss: 0.00159790
Iteration 17/25 | Loss: 0.00159790
Iteration 18/25 | Loss: 0.00159790
Iteration 19/25 | Loss: 0.00159790
Iteration 20/25 | Loss: 0.00159790
Iteration 21/25 | Loss: 0.00159790
Iteration 22/25 | Loss: 0.00159790
Iteration 23/25 | Loss: 0.00159790
Iteration 24/25 | Loss: 0.00159790
Iteration 25/25 | Loss: 0.00159790

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00159790
Iteration 2/1000 | Loss: 0.00031193
Iteration 3/1000 | Loss: 0.00026771
Iteration 4/1000 | Loss: 0.00012271
Iteration 5/1000 | Loss: 0.00005009
Iteration 6/1000 | Loss: 0.00032805
Iteration 7/1000 | Loss: 0.00008239
Iteration 8/1000 | Loss: 0.00005926
Iteration 9/1000 | Loss: 0.00063393
Iteration 10/1000 | Loss: 0.00035795
Iteration 11/1000 | Loss: 0.00005338
Iteration 12/1000 | Loss: 0.00002803
Iteration 13/1000 | Loss: 0.00002777
Iteration 14/1000 | Loss: 0.00003233
Iteration 15/1000 | Loss: 0.00009395
Iteration 16/1000 | Loss: 0.00015746
Iteration 17/1000 | Loss: 0.00003303
Iteration 18/1000 | Loss: 0.00002915
Iteration 19/1000 | Loss: 0.00063902
Iteration 20/1000 | Loss: 0.00015638
Iteration 21/1000 | Loss: 0.00002393
Iteration 22/1000 | Loss: 0.00003414
Iteration 23/1000 | Loss: 0.00006027
Iteration 24/1000 | Loss: 0.00009270
Iteration 25/1000 | Loss: 0.00005826
Iteration 26/1000 | Loss: 0.00003837
Iteration 27/1000 | Loss: 0.00001863
Iteration 28/1000 | Loss: 0.00001624
Iteration 29/1000 | Loss: 0.00001753
Iteration 30/1000 | Loss: 0.00001579
Iteration 31/1000 | Loss: 0.00001540
Iteration 32/1000 | Loss: 0.00002013
Iteration 33/1000 | Loss: 0.00001595
Iteration 34/1000 | Loss: 0.00042441
Iteration 35/1000 | Loss: 0.00025408
Iteration 36/1000 | Loss: 0.00008007
Iteration 37/1000 | Loss: 0.00002518
Iteration 38/1000 | Loss: 0.00024357
Iteration 39/1000 | Loss: 0.00001704
Iteration 40/1000 | Loss: 0.00001545
Iteration 41/1000 | Loss: 0.00009015
Iteration 42/1000 | Loss: 0.00001592
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001551
Iteration 45/1000 | Loss: 0.00001491
Iteration 46/1000 | Loss: 0.00001489
Iteration 47/1000 | Loss: 0.00001488
Iteration 48/1000 | Loss: 0.00001488
Iteration 49/1000 | Loss: 0.00001488
Iteration 50/1000 | Loss: 0.00001488
Iteration 51/1000 | Loss: 0.00001488
Iteration 52/1000 | Loss: 0.00001488
Iteration 53/1000 | Loss: 0.00001488
Iteration 54/1000 | Loss: 0.00001488
Iteration 55/1000 | Loss: 0.00001488
Iteration 56/1000 | Loss: 0.00001722
Iteration 57/1000 | Loss: 0.00043218
Iteration 58/1000 | Loss: 0.00051273
Iteration 59/1000 | Loss: 0.00008620
Iteration 60/1000 | Loss: 0.00006053
Iteration 61/1000 | Loss: 0.00002546
Iteration 62/1000 | Loss: 0.00003925
Iteration 63/1000 | Loss: 0.00002458
Iteration 64/1000 | Loss: 0.00001613
Iteration 65/1000 | Loss: 0.00010443
Iteration 66/1000 | Loss: 0.00018777
Iteration 67/1000 | Loss: 0.00006308
Iteration 68/1000 | Loss: 0.00034787
Iteration 69/1000 | Loss: 0.00001342
Iteration 70/1000 | Loss: 0.00006704
Iteration 71/1000 | Loss: 0.00001306
Iteration 72/1000 | Loss: 0.00001297
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001295
Iteration 75/1000 | Loss: 0.00001295
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001399
Iteration 78/1000 | Loss: 0.00001293
Iteration 79/1000 | Loss: 0.00001291
Iteration 80/1000 | Loss: 0.00001290
Iteration 81/1000 | Loss: 0.00001290
Iteration 82/1000 | Loss: 0.00001289
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001288
Iteration 85/1000 | Loss: 0.00001284
Iteration 86/1000 | Loss: 0.00001478
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001369
Iteration 90/1000 | Loss: 0.00001279
Iteration 91/1000 | Loss: 0.00001278
Iteration 92/1000 | Loss: 0.00001278
Iteration 93/1000 | Loss: 0.00001277
Iteration 94/1000 | Loss: 0.00001277
Iteration 95/1000 | Loss: 0.00001277
Iteration 96/1000 | Loss: 0.00001277
Iteration 97/1000 | Loss: 0.00001277
Iteration 98/1000 | Loss: 0.00001277
Iteration 99/1000 | Loss: 0.00001277
Iteration 100/1000 | Loss: 0.00001277
Iteration 101/1000 | Loss: 0.00001277
Iteration 102/1000 | Loss: 0.00001277
Iteration 103/1000 | Loss: 0.00001277
Iteration 104/1000 | Loss: 0.00001277
Iteration 105/1000 | Loss: 0.00001277
Iteration 106/1000 | Loss: 0.00001277
Iteration 107/1000 | Loss: 0.00001277
Iteration 108/1000 | Loss: 0.00001277
Iteration 109/1000 | Loss: 0.00001277
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.2768648957717232e-05, 1.2768648957717232e-05, 1.2768648957717232e-05, 1.2768648957717232e-05, 1.2768648957717232e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2768648957717232e-05

Optimization complete. Final v2v error: 3.036461114883423 mm

Highest mean error: 4.483857154846191 mm for frame 182

Lowest mean error: 2.776670217514038 mm for frame 81

Saving results

Total time: 153.947411775589
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00828888
Iteration 2/25 | Loss: 0.00088239
Iteration 3/25 | Loss: 0.00076185
Iteration 4/25 | Loss: 0.00074545
Iteration 5/25 | Loss: 0.00074100
Iteration 6/25 | Loss: 0.00073956
Iteration 7/25 | Loss: 0.00073924
Iteration 8/25 | Loss: 0.00073924
Iteration 9/25 | Loss: 0.00073924
Iteration 10/25 | Loss: 0.00073924
Iteration 11/25 | Loss: 0.00073924
Iteration 12/25 | Loss: 0.00073924
Iteration 13/25 | Loss: 0.00073924
Iteration 14/25 | Loss: 0.00073924
Iteration 15/25 | Loss: 0.00073924
Iteration 16/25 | Loss: 0.00073924
Iteration 17/25 | Loss: 0.00073924
Iteration 18/25 | Loss: 0.00073924
Iteration 19/25 | Loss: 0.00073924
Iteration 20/25 | Loss: 0.00073924
Iteration 21/25 | Loss: 0.00073924
Iteration 22/25 | Loss: 0.00073924
Iteration 23/25 | Loss: 0.00073924
Iteration 24/25 | Loss: 0.00073924
Iteration 25/25 | Loss: 0.00073924

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60009253
Iteration 2/25 | Loss: 0.00130800
Iteration 3/25 | Loss: 0.00130800
Iteration 4/25 | Loss: 0.00130800
Iteration 5/25 | Loss: 0.00130800
Iteration 6/25 | Loss: 0.00130800
Iteration 7/25 | Loss: 0.00130800
Iteration 8/25 | Loss: 0.00130800
Iteration 9/25 | Loss: 0.00130800
Iteration 10/25 | Loss: 0.00130800
Iteration 11/25 | Loss: 0.00130800
Iteration 12/25 | Loss: 0.00130800
Iteration 13/25 | Loss: 0.00130800
Iteration 14/25 | Loss: 0.00130800
Iteration 15/25 | Loss: 0.00130800
Iteration 16/25 | Loss: 0.00130800
Iteration 17/25 | Loss: 0.00130800
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013080007629469037, 0.0013080007629469037, 0.0013080007629469037, 0.0013080007629469037, 0.0013080007629469037]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013080007629469037

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130800
Iteration 2/1000 | Loss: 0.00002109
Iteration 3/1000 | Loss: 0.00001415
Iteration 4/1000 | Loss: 0.00001236
Iteration 5/1000 | Loss: 0.00001177
Iteration 6/1000 | Loss: 0.00001124
Iteration 7/1000 | Loss: 0.00001097
Iteration 8/1000 | Loss: 0.00001076
Iteration 9/1000 | Loss: 0.00001074
Iteration 10/1000 | Loss: 0.00001069
Iteration 11/1000 | Loss: 0.00001060
Iteration 12/1000 | Loss: 0.00001060
Iteration 13/1000 | Loss: 0.00001058
Iteration 14/1000 | Loss: 0.00001057
Iteration 15/1000 | Loss: 0.00001057
Iteration 16/1000 | Loss: 0.00001057
Iteration 17/1000 | Loss: 0.00001057
Iteration 18/1000 | Loss: 0.00001056
Iteration 19/1000 | Loss: 0.00001056
Iteration 20/1000 | Loss: 0.00001056
Iteration 21/1000 | Loss: 0.00001056
Iteration 22/1000 | Loss: 0.00001056
Iteration 23/1000 | Loss: 0.00001055
Iteration 24/1000 | Loss: 0.00001054
Iteration 25/1000 | Loss: 0.00001054
Iteration 26/1000 | Loss: 0.00001054
Iteration 27/1000 | Loss: 0.00001053
Iteration 28/1000 | Loss: 0.00001053
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001052
Iteration 31/1000 | Loss: 0.00001051
Iteration 32/1000 | Loss: 0.00001051
Iteration 33/1000 | Loss: 0.00001051
Iteration 34/1000 | Loss: 0.00001051
Iteration 35/1000 | Loss: 0.00001050
Iteration 36/1000 | Loss: 0.00001050
Iteration 37/1000 | Loss: 0.00001050
Iteration 38/1000 | Loss: 0.00001050
Iteration 39/1000 | Loss: 0.00001050
Iteration 40/1000 | Loss: 0.00001049
Iteration 41/1000 | Loss: 0.00001049
Iteration 42/1000 | Loss: 0.00001049
Iteration 43/1000 | Loss: 0.00001049
Iteration 44/1000 | Loss: 0.00001049
Iteration 45/1000 | Loss: 0.00001049
Iteration 46/1000 | Loss: 0.00001049
Iteration 47/1000 | Loss: 0.00001048
Iteration 48/1000 | Loss: 0.00001048
Iteration 49/1000 | Loss: 0.00001048
Iteration 50/1000 | Loss: 0.00001046
Iteration 51/1000 | Loss: 0.00001045
Iteration 52/1000 | Loss: 0.00001044
Iteration 53/1000 | Loss: 0.00001044
Iteration 54/1000 | Loss: 0.00001043
Iteration 55/1000 | Loss: 0.00001042
Iteration 56/1000 | Loss: 0.00001042
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001041
Iteration 59/1000 | Loss: 0.00001041
Iteration 60/1000 | Loss: 0.00001041
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001040
Iteration 67/1000 | Loss: 0.00001040
Iteration 68/1000 | Loss: 0.00001040
Iteration 69/1000 | Loss: 0.00001040
Iteration 70/1000 | Loss: 0.00001040
Iteration 71/1000 | Loss: 0.00001040
Iteration 72/1000 | Loss: 0.00001040
Iteration 73/1000 | Loss: 0.00001040
Iteration 74/1000 | Loss: 0.00001040
Iteration 75/1000 | Loss: 0.00001040
Iteration 76/1000 | Loss: 0.00001040
Iteration 77/1000 | Loss: 0.00001040
Iteration 78/1000 | Loss: 0.00001040
Iteration 79/1000 | Loss: 0.00001040
Iteration 80/1000 | Loss: 0.00001040
Iteration 81/1000 | Loss: 0.00001040
Iteration 82/1000 | Loss: 0.00001040
Iteration 83/1000 | Loss: 0.00001040
Iteration 84/1000 | Loss: 0.00001040
Iteration 85/1000 | Loss: 0.00001040
Iteration 86/1000 | Loss: 0.00001040
Iteration 87/1000 | Loss: 0.00001040
Iteration 88/1000 | Loss: 0.00001040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.0403095075162128e-05, 1.0403095075162128e-05, 1.0403095075162128e-05, 1.0403095075162128e-05, 1.0403095075162128e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0403095075162128e-05

Optimization complete. Final v2v error: 2.7310843467712402 mm

Highest mean error: 2.8474857807159424 mm for frame 37

Lowest mean error: 2.620103597640991 mm for frame 2

Saving results

Total time: 28.945555686950684
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00809428
Iteration 2/25 | Loss: 0.00105366
Iteration 3/25 | Loss: 0.00083704
Iteration 4/25 | Loss: 0.00079911
Iteration 5/25 | Loss: 0.00078781
Iteration 6/25 | Loss: 0.00078629
Iteration 7/25 | Loss: 0.00078574
Iteration 8/25 | Loss: 0.00078574
Iteration 9/25 | Loss: 0.00078574
Iteration 10/25 | Loss: 0.00078574
Iteration 11/25 | Loss: 0.00078574
Iteration 12/25 | Loss: 0.00078574
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007857406744733453, 0.0007857406744733453, 0.0007857406744733453, 0.0007857406744733453, 0.0007857406744733453]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007857406744733453

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55202353
Iteration 2/25 | Loss: 0.00148658
Iteration 3/25 | Loss: 0.00148658
Iteration 4/25 | Loss: 0.00148658
Iteration 5/25 | Loss: 0.00148658
Iteration 6/25 | Loss: 0.00148658
Iteration 7/25 | Loss: 0.00148658
Iteration 8/25 | Loss: 0.00148658
Iteration 9/25 | Loss: 0.00148658
Iteration 10/25 | Loss: 0.00148658
Iteration 11/25 | Loss: 0.00148658
Iteration 12/25 | Loss: 0.00148658
Iteration 13/25 | Loss: 0.00148658
Iteration 14/25 | Loss: 0.00148658
Iteration 15/25 | Loss: 0.00148658
Iteration 16/25 | Loss: 0.00148658
Iteration 17/25 | Loss: 0.00148658
Iteration 18/25 | Loss: 0.00148658
Iteration 19/25 | Loss: 0.00148658
Iteration 20/25 | Loss: 0.00148658
Iteration 21/25 | Loss: 0.00148658
Iteration 22/25 | Loss: 0.00148658
Iteration 23/25 | Loss: 0.00148658
Iteration 24/25 | Loss: 0.00148658
Iteration 25/25 | Loss: 0.00148658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148658
Iteration 2/1000 | Loss: 0.00003809
Iteration 3/1000 | Loss: 0.00002772
Iteration 4/1000 | Loss: 0.00002314
Iteration 5/1000 | Loss: 0.00002175
Iteration 6/1000 | Loss: 0.00002109
Iteration 7/1000 | Loss: 0.00002036
Iteration 8/1000 | Loss: 0.00001976
Iteration 9/1000 | Loss: 0.00001920
Iteration 10/1000 | Loss: 0.00001885
Iteration 11/1000 | Loss: 0.00001860
Iteration 12/1000 | Loss: 0.00001840
Iteration 13/1000 | Loss: 0.00001837
Iteration 14/1000 | Loss: 0.00001831
Iteration 15/1000 | Loss: 0.00001815
Iteration 16/1000 | Loss: 0.00001809
Iteration 17/1000 | Loss: 0.00001809
Iteration 18/1000 | Loss: 0.00001808
Iteration 19/1000 | Loss: 0.00001808
Iteration 20/1000 | Loss: 0.00001807
Iteration 21/1000 | Loss: 0.00001805
Iteration 22/1000 | Loss: 0.00001804
Iteration 23/1000 | Loss: 0.00001800
Iteration 24/1000 | Loss: 0.00001799
Iteration 25/1000 | Loss: 0.00001799
Iteration 26/1000 | Loss: 0.00001799
Iteration 27/1000 | Loss: 0.00001798
Iteration 28/1000 | Loss: 0.00001798
Iteration 29/1000 | Loss: 0.00001796
Iteration 30/1000 | Loss: 0.00001796
Iteration 31/1000 | Loss: 0.00001796
Iteration 32/1000 | Loss: 0.00001796
Iteration 33/1000 | Loss: 0.00001795
Iteration 34/1000 | Loss: 0.00001795
Iteration 35/1000 | Loss: 0.00001795
Iteration 36/1000 | Loss: 0.00001795
Iteration 37/1000 | Loss: 0.00001795
Iteration 38/1000 | Loss: 0.00001794
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001794
Iteration 41/1000 | Loss: 0.00001794
Iteration 42/1000 | Loss: 0.00001794
Iteration 43/1000 | Loss: 0.00001793
Iteration 44/1000 | Loss: 0.00001793
Iteration 45/1000 | Loss: 0.00001792
Iteration 46/1000 | Loss: 0.00001791
Iteration 47/1000 | Loss: 0.00001791
Iteration 48/1000 | Loss: 0.00001791
Iteration 49/1000 | Loss: 0.00001790
Iteration 50/1000 | Loss: 0.00001790
Iteration 51/1000 | Loss: 0.00001790
Iteration 52/1000 | Loss: 0.00001789
Iteration 53/1000 | Loss: 0.00001789
Iteration 54/1000 | Loss: 0.00001789
Iteration 55/1000 | Loss: 0.00001789
Iteration 56/1000 | Loss: 0.00001788
Iteration 57/1000 | Loss: 0.00001788
Iteration 58/1000 | Loss: 0.00001788
Iteration 59/1000 | Loss: 0.00001788
Iteration 60/1000 | Loss: 0.00001787
Iteration 61/1000 | Loss: 0.00001787
Iteration 62/1000 | Loss: 0.00001787
Iteration 63/1000 | Loss: 0.00001787
Iteration 64/1000 | Loss: 0.00001787
Iteration 65/1000 | Loss: 0.00001786
Iteration 66/1000 | Loss: 0.00001786
Iteration 67/1000 | Loss: 0.00001786
Iteration 68/1000 | Loss: 0.00001785
Iteration 69/1000 | Loss: 0.00001785
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001784
Iteration 79/1000 | Loss: 0.00001784
Iteration 80/1000 | Loss: 0.00001784
Iteration 81/1000 | Loss: 0.00001783
Iteration 82/1000 | Loss: 0.00001783
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001783
Iteration 86/1000 | Loss: 0.00001783
Iteration 87/1000 | Loss: 0.00001783
Iteration 88/1000 | Loss: 0.00001783
Iteration 89/1000 | Loss: 0.00001783
Iteration 90/1000 | Loss: 0.00001783
Iteration 91/1000 | Loss: 0.00001783
Iteration 92/1000 | Loss: 0.00001783
Iteration 93/1000 | Loss: 0.00001783
Iteration 94/1000 | Loss: 0.00001782
Iteration 95/1000 | Loss: 0.00001782
Iteration 96/1000 | Loss: 0.00001782
Iteration 97/1000 | Loss: 0.00001782
Iteration 98/1000 | Loss: 0.00001782
Iteration 99/1000 | Loss: 0.00001782
Iteration 100/1000 | Loss: 0.00001781
Iteration 101/1000 | Loss: 0.00001781
Iteration 102/1000 | Loss: 0.00001781
Iteration 103/1000 | Loss: 0.00001781
Iteration 104/1000 | Loss: 0.00001781
Iteration 105/1000 | Loss: 0.00001781
Iteration 106/1000 | Loss: 0.00001781
Iteration 107/1000 | Loss: 0.00001781
Iteration 108/1000 | Loss: 0.00001781
Iteration 109/1000 | Loss: 0.00001781
Iteration 110/1000 | Loss: 0.00001781
Iteration 111/1000 | Loss: 0.00001781
Iteration 112/1000 | Loss: 0.00001781
Iteration 113/1000 | Loss: 0.00001781
Iteration 114/1000 | Loss: 0.00001781
Iteration 115/1000 | Loss: 0.00001781
Iteration 116/1000 | Loss: 0.00001781
Iteration 117/1000 | Loss: 0.00001781
Iteration 118/1000 | Loss: 0.00001781
Iteration 119/1000 | Loss: 0.00001781
Iteration 120/1000 | Loss: 0.00001781
Iteration 121/1000 | Loss: 0.00001781
Iteration 122/1000 | Loss: 0.00001781
Iteration 123/1000 | Loss: 0.00001781
Iteration 124/1000 | Loss: 0.00001781
Iteration 125/1000 | Loss: 0.00001781
Iteration 126/1000 | Loss: 0.00001781
Iteration 127/1000 | Loss: 0.00001781
Iteration 128/1000 | Loss: 0.00001781
Iteration 129/1000 | Loss: 0.00001781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.7805494280764833e-05, 1.7805494280764833e-05, 1.7805494280764833e-05, 1.7805494280764833e-05, 1.7805494280764833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7805494280764833e-05

Optimization complete. Final v2v error: 3.51773738861084 mm

Highest mean error: 3.870161771774292 mm for frame 45

Lowest mean error: 3.2088136672973633 mm for frame 18

Saving results

Total time: 38.845717668533325
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906677
Iteration 2/25 | Loss: 0.00136511
Iteration 3/25 | Loss: 0.00103721
Iteration 4/25 | Loss: 0.00097424
Iteration 5/25 | Loss: 0.00094559
Iteration 6/25 | Loss: 0.00093281
Iteration 7/25 | Loss: 0.00092695
Iteration 8/25 | Loss: 0.00092672
Iteration 9/25 | Loss: 0.00092575
Iteration 10/25 | Loss: 0.00092352
Iteration 11/25 | Loss: 0.00092504
Iteration 12/25 | Loss: 0.00092470
Iteration 13/25 | Loss: 0.00092534
Iteration 14/25 | Loss: 0.00092447
Iteration 15/25 | Loss: 0.00092606
Iteration 16/25 | Loss: 0.00092600
Iteration 17/25 | Loss: 0.00092509
Iteration 18/25 | Loss: 0.00092569
Iteration 19/25 | Loss: 0.00092560
Iteration 20/25 | Loss: 0.00092601
Iteration 21/25 | Loss: 0.00092471
Iteration 22/25 | Loss: 0.00092527
Iteration 23/25 | Loss: 0.00092469
Iteration 24/25 | Loss: 0.00092278
Iteration 25/25 | Loss: 0.00092318

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.51707149
Iteration 2/25 | Loss: 0.00139829
Iteration 3/25 | Loss: 0.00139829
Iteration 4/25 | Loss: 0.00139829
Iteration 5/25 | Loss: 0.00139829
Iteration 6/25 | Loss: 0.00139829
Iteration 7/25 | Loss: 0.00139829
Iteration 8/25 | Loss: 0.00139829
Iteration 9/25 | Loss: 0.00139829
Iteration 10/25 | Loss: 0.00139829
Iteration 11/25 | Loss: 0.00139829
Iteration 12/25 | Loss: 0.00139829
Iteration 13/25 | Loss: 0.00139829
Iteration 14/25 | Loss: 0.00139829
Iteration 15/25 | Loss: 0.00139829
Iteration 16/25 | Loss: 0.00139829
Iteration 17/25 | Loss: 0.00139829
Iteration 18/25 | Loss: 0.00139829
Iteration 19/25 | Loss: 0.00139829
Iteration 20/25 | Loss: 0.00139829
Iteration 21/25 | Loss: 0.00139829
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001398287946358323, 0.001398287946358323, 0.001398287946358323, 0.001398287946358323, 0.001398287946358323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001398287946358323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00139829
Iteration 2/1000 | Loss: 0.00005138
Iteration 3/1000 | Loss: 0.00003563
Iteration 4/1000 | Loss: 0.00003113
Iteration 5/1000 | Loss: 0.00002944
Iteration 6/1000 | Loss: 0.00002816
Iteration 7/1000 | Loss: 0.00002741
Iteration 8/1000 | Loss: 0.00002676
Iteration 9/1000 | Loss: 0.00002616
Iteration 10/1000 | Loss: 0.00002568
Iteration 11/1000 | Loss: 0.00002536
Iteration 12/1000 | Loss: 0.00002511
Iteration 13/1000 | Loss: 0.00002503
Iteration 14/1000 | Loss: 0.00002482
Iteration 15/1000 | Loss: 0.00002481
Iteration 16/1000 | Loss: 0.00002476
Iteration 17/1000 | Loss: 0.00002459
Iteration 18/1000 | Loss: 0.00002452
Iteration 19/1000 | Loss: 0.00002439
Iteration 20/1000 | Loss: 0.00002436
Iteration 21/1000 | Loss: 0.00002435
Iteration 22/1000 | Loss: 0.00002435
Iteration 23/1000 | Loss: 0.00002434
Iteration 24/1000 | Loss: 0.00002432
Iteration 25/1000 | Loss: 0.00002431
Iteration 26/1000 | Loss: 0.00002431
Iteration 27/1000 | Loss: 0.00002430
Iteration 28/1000 | Loss: 0.00002429
Iteration 29/1000 | Loss: 0.00002428
Iteration 30/1000 | Loss: 0.00002428
Iteration 31/1000 | Loss: 0.00002427
Iteration 32/1000 | Loss: 0.00002426
Iteration 33/1000 | Loss: 0.00002425
Iteration 34/1000 | Loss: 0.00002424
Iteration 35/1000 | Loss: 0.00002424
Iteration 36/1000 | Loss: 0.00002423
Iteration 37/1000 | Loss: 0.00002423
Iteration 38/1000 | Loss: 0.00002422
Iteration 39/1000 | Loss: 0.00002422
Iteration 40/1000 | Loss: 0.00002421
Iteration 41/1000 | Loss: 0.00002421
Iteration 42/1000 | Loss: 0.00002420
Iteration 43/1000 | Loss: 0.00002420
Iteration 44/1000 | Loss: 0.00002420
Iteration 45/1000 | Loss: 0.00002419
Iteration 46/1000 | Loss: 0.00002419
Iteration 47/1000 | Loss: 0.00002418
Iteration 48/1000 | Loss: 0.00002418
Iteration 49/1000 | Loss: 0.00002417
Iteration 50/1000 | Loss: 0.00002417
Iteration 51/1000 | Loss: 0.00002417
Iteration 52/1000 | Loss: 0.00002416
Iteration 53/1000 | Loss: 0.00002416
Iteration 54/1000 | Loss: 0.00002415
Iteration 55/1000 | Loss: 0.00002415
Iteration 56/1000 | Loss: 0.00002415
Iteration 57/1000 | Loss: 0.00002415
Iteration 58/1000 | Loss: 0.00002415
Iteration 59/1000 | Loss: 0.00002414
Iteration 60/1000 | Loss: 0.00002414
Iteration 61/1000 | Loss: 0.00002413
Iteration 62/1000 | Loss: 0.00002413
Iteration 63/1000 | Loss: 0.00002413
Iteration 64/1000 | Loss: 0.00002412
Iteration 65/1000 | Loss: 0.00002412
Iteration 66/1000 | Loss: 0.00002412
Iteration 67/1000 | Loss: 0.00002411
Iteration 68/1000 | Loss: 0.00002411
Iteration 69/1000 | Loss: 0.00002410
Iteration 70/1000 | Loss: 0.00002410
Iteration 71/1000 | Loss: 0.00002410
Iteration 72/1000 | Loss: 0.00002410
Iteration 73/1000 | Loss: 0.00002409
Iteration 74/1000 | Loss: 0.00002409
Iteration 75/1000 | Loss: 0.00002409
Iteration 76/1000 | Loss: 0.00002409
Iteration 77/1000 | Loss: 0.00002408
Iteration 78/1000 | Loss: 0.00002408
Iteration 79/1000 | Loss: 0.00002408
Iteration 80/1000 | Loss: 0.00002407
Iteration 81/1000 | Loss: 0.00002407
Iteration 82/1000 | Loss: 0.00002407
Iteration 83/1000 | Loss: 0.00002406
Iteration 84/1000 | Loss: 0.00002406
Iteration 85/1000 | Loss: 0.00002406
Iteration 86/1000 | Loss: 0.00002405
Iteration 87/1000 | Loss: 0.00002405
Iteration 88/1000 | Loss: 0.00002405
Iteration 89/1000 | Loss: 0.00002404
Iteration 90/1000 | Loss: 0.00002404
Iteration 91/1000 | Loss: 0.00002404
Iteration 92/1000 | Loss: 0.00002404
Iteration 93/1000 | Loss: 0.00002403
Iteration 94/1000 | Loss: 0.00002403
Iteration 95/1000 | Loss: 0.00002403
Iteration 96/1000 | Loss: 0.00002403
Iteration 97/1000 | Loss: 0.00002402
Iteration 98/1000 | Loss: 0.00002402
Iteration 99/1000 | Loss: 0.00002402
Iteration 100/1000 | Loss: 0.00002402
Iteration 101/1000 | Loss: 0.00002402
Iteration 102/1000 | Loss: 0.00002402
Iteration 103/1000 | Loss: 0.00002402
Iteration 104/1000 | Loss: 0.00002401
Iteration 105/1000 | Loss: 0.00002401
Iteration 106/1000 | Loss: 0.00002401
Iteration 107/1000 | Loss: 0.00002401
Iteration 108/1000 | Loss: 0.00002401
Iteration 109/1000 | Loss: 0.00002401
Iteration 110/1000 | Loss: 0.00002401
Iteration 111/1000 | Loss: 0.00002401
Iteration 112/1000 | Loss: 0.00002401
Iteration 113/1000 | Loss: 0.00002401
Iteration 114/1000 | Loss: 0.00002401
Iteration 115/1000 | Loss: 0.00002401
Iteration 116/1000 | Loss: 0.00002401
Iteration 117/1000 | Loss: 0.00002401
Iteration 118/1000 | Loss: 0.00002400
Iteration 119/1000 | Loss: 0.00002400
Iteration 120/1000 | Loss: 0.00002400
Iteration 121/1000 | Loss: 0.00002400
Iteration 122/1000 | Loss: 0.00002400
Iteration 123/1000 | Loss: 0.00002400
Iteration 124/1000 | Loss: 0.00002399
Iteration 125/1000 | Loss: 0.00002399
Iteration 126/1000 | Loss: 0.00002399
Iteration 127/1000 | Loss: 0.00002399
Iteration 128/1000 | Loss: 0.00002399
Iteration 129/1000 | Loss: 0.00002399
Iteration 130/1000 | Loss: 0.00002399
Iteration 131/1000 | Loss: 0.00002399
Iteration 132/1000 | Loss: 0.00002399
Iteration 133/1000 | Loss: 0.00002398
Iteration 134/1000 | Loss: 0.00002398
Iteration 135/1000 | Loss: 0.00002398
Iteration 136/1000 | Loss: 0.00002398
Iteration 137/1000 | Loss: 0.00002398
Iteration 138/1000 | Loss: 0.00002398
Iteration 139/1000 | Loss: 0.00002398
Iteration 140/1000 | Loss: 0.00002398
Iteration 141/1000 | Loss: 0.00002398
Iteration 142/1000 | Loss: 0.00002397
Iteration 143/1000 | Loss: 0.00002397
Iteration 144/1000 | Loss: 0.00002397
Iteration 145/1000 | Loss: 0.00002397
Iteration 146/1000 | Loss: 0.00002397
Iteration 147/1000 | Loss: 0.00002397
Iteration 148/1000 | Loss: 0.00002397
Iteration 149/1000 | Loss: 0.00002397
Iteration 150/1000 | Loss: 0.00002397
Iteration 151/1000 | Loss: 0.00002397
Iteration 152/1000 | Loss: 0.00002396
Iteration 153/1000 | Loss: 0.00002396
Iteration 154/1000 | Loss: 0.00002396
Iteration 155/1000 | Loss: 0.00002396
Iteration 156/1000 | Loss: 0.00002396
Iteration 157/1000 | Loss: 0.00002396
Iteration 158/1000 | Loss: 0.00002396
Iteration 159/1000 | Loss: 0.00002396
Iteration 160/1000 | Loss: 0.00002395
Iteration 161/1000 | Loss: 0.00002395
Iteration 162/1000 | Loss: 0.00002395
Iteration 163/1000 | Loss: 0.00002395
Iteration 164/1000 | Loss: 0.00002395
Iteration 165/1000 | Loss: 0.00002395
Iteration 166/1000 | Loss: 0.00002395
Iteration 167/1000 | Loss: 0.00002395
Iteration 168/1000 | Loss: 0.00002395
Iteration 169/1000 | Loss: 0.00002395
Iteration 170/1000 | Loss: 0.00002394
Iteration 171/1000 | Loss: 0.00002394
Iteration 172/1000 | Loss: 0.00002394
Iteration 173/1000 | Loss: 0.00002394
Iteration 174/1000 | Loss: 0.00002394
Iteration 175/1000 | Loss: 0.00002394
Iteration 176/1000 | Loss: 0.00002394
Iteration 177/1000 | Loss: 0.00002394
Iteration 178/1000 | Loss: 0.00002394
Iteration 179/1000 | Loss: 0.00002393
Iteration 180/1000 | Loss: 0.00002393
Iteration 181/1000 | Loss: 0.00002393
Iteration 182/1000 | Loss: 0.00002393
Iteration 183/1000 | Loss: 0.00002393
Iteration 184/1000 | Loss: 0.00002393
Iteration 185/1000 | Loss: 0.00002392
Iteration 186/1000 | Loss: 0.00002392
Iteration 187/1000 | Loss: 0.00002392
Iteration 188/1000 | Loss: 0.00002392
Iteration 189/1000 | Loss: 0.00002392
Iteration 190/1000 | Loss: 0.00002392
Iteration 191/1000 | Loss: 0.00002392
Iteration 192/1000 | Loss: 0.00002392
Iteration 193/1000 | Loss: 0.00002392
Iteration 194/1000 | Loss: 0.00002392
Iteration 195/1000 | Loss: 0.00002392
Iteration 196/1000 | Loss: 0.00002392
Iteration 197/1000 | Loss: 0.00002392
Iteration 198/1000 | Loss: 0.00002392
Iteration 199/1000 | Loss: 0.00002392
Iteration 200/1000 | Loss: 0.00002392
Iteration 201/1000 | Loss: 0.00002392
Iteration 202/1000 | Loss: 0.00002392
Iteration 203/1000 | Loss: 0.00002392
Iteration 204/1000 | Loss: 0.00002392
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.3920523744891398e-05, 2.3920523744891398e-05, 2.3920523744891398e-05, 2.3920523744891398e-05, 2.3920523744891398e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3920523744891398e-05

Optimization complete. Final v2v error: 3.971898078918457 mm

Highest mean error: 4.841574192047119 mm for frame 209

Lowest mean error: 3.3850138187408447 mm for frame 103

Saving results

Total time: 94.06728005409241
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01066364
Iteration 2/25 | Loss: 0.01066360
Iteration 3/25 | Loss: 0.01066357
Iteration 4/25 | Loss: 0.01066353
Iteration 5/25 | Loss: 0.00214596
Iteration 6/25 | Loss: 0.00164652
Iteration 7/25 | Loss: 0.00120459
Iteration 8/25 | Loss: 0.00108638
Iteration 9/25 | Loss: 0.00102692
Iteration 10/25 | Loss: 0.00099329
Iteration 11/25 | Loss: 0.00093253
Iteration 12/25 | Loss: 0.00088427
Iteration 13/25 | Loss: 0.00085194
Iteration 14/25 | Loss: 0.00083766
Iteration 15/25 | Loss: 0.00082328
Iteration 16/25 | Loss: 0.00081353
Iteration 17/25 | Loss: 0.00081236
Iteration 18/25 | Loss: 0.00081393
Iteration 19/25 | Loss: 0.00081012
Iteration 20/25 | Loss: 0.00081199
Iteration 21/25 | Loss: 0.00081150
Iteration 22/25 | Loss: 0.00080834
Iteration 23/25 | Loss: 0.00080833
Iteration 24/25 | Loss: 0.00080833
Iteration 25/25 | Loss: 0.00080833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58799803
Iteration 2/25 | Loss: 0.00133168
Iteration 3/25 | Loss: 0.00133167
Iteration 4/25 | Loss: 0.00133167
Iteration 5/25 | Loss: 0.00133167
Iteration 6/25 | Loss: 0.00133167
Iteration 7/25 | Loss: 0.00133167
Iteration 8/25 | Loss: 0.00133167
Iteration 9/25 | Loss: 0.00133167
Iteration 10/25 | Loss: 0.00133167
Iteration 11/25 | Loss: 0.00133167
Iteration 12/25 | Loss: 0.00133167
Iteration 13/25 | Loss: 0.00133167
Iteration 14/25 | Loss: 0.00133167
Iteration 15/25 | Loss: 0.00133167
Iteration 16/25 | Loss: 0.00133167
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013316716067492962, 0.0013316716067492962, 0.0013316716067492962, 0.0013316716067492962, 0.0013316716067492962]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013316716067492962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133167
Iteration 2/1000 | Loss: 0.00003259
Iteration 3/1000 | Loss: 0.00002113
Iteration 4/1000 | Loss: 0.00001938
Iteration 5/1000 | Loss: 0.00005683
Iteration 6/1000 | Loss: 0.00002743
Iteration 7/1000 | Loss: 0.00001762
Iteration 8/1000 | Loss: 0.00001682
Iteration 9/1000 | Loss: 0.00001661
Iteration 10/1000 | Loss: 0.00001637
Iteration 11/1000 | Loss: 0.00001628
Iteration 12/1000 | Loss: 0.00001618
Iteration 13/1000 | Loss: 0.00001618
Iteration 14/1000 | Loss: 0.00001618
Iteration 15/1000 | Loss: 0.00001618
Iteration 16/1000 | Loss: 0.00001617
Iteration 17/1000 | Loss: 0.00001617
Iteration 18/1000 | Loss: 0.00001617
Iteration 19/1000 | Loss: 0.00001617
Iteration 20/1000 | Loss: 0.00001616
Iteration 21/1000 | Loss: 0.00001616
Iteration 22/1000 | Loss: 0.00001615
Iteration 23/1000 | Loss: 0.00001615
Iteration 24/1000 | Loss: 0.00001615
Iteration 25/1000 | Loss: 0.00001614
Iteration 26/1000 | Loss: 0.00001614
Iteration 27/1000 | Loss: 0.00001614
Iteration 28/1000 | Loss: 0.00001613
Iteration 29/1000 | Loss: 0.00001613
Iteration 30/1000 | Loss: 0.00001613
Iteration 31/1000 | Loss: 0.00001612
Iteration 32/1000 | Loss: 0.00001612
Iteration 33/1000 | Loss: 0.00001611
Iteration 34/1000 | Loss: 0.00001611
Iteration 35/1000 | Loss: 0.00001610
Iteration 36/1000 | Loss: 0.00001609
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001609
Iteration 39/1000 | Loss: 0.00001609
Iteration 40/1000 | Loss: 0.00001608
Iteration 41/1000 | Loss: 0.00001608
Iteration 42/1000 | Loss: 0.00001608
Iteration 43/1000 | Loss: 0.00001608
Iteration 44/1000 | Loss: 0.00001608
Iteration 45/1000 | Loss: 0.00001608
Iteration 46/1000 | Loss: 0.00001608
Iteration 47/1000 | Loss: 0.00001608
Iteration 48/1000 | Loss: 0.00001608
Iteration 49/1000 | Loss: 0.00005634
Iteration 50/1000 | Loss: 0.00001610
Iteration 51/1000 | Loss: 0.00001608
Iteration 52/1000 | Loss: 0.00001607
Iteration 53/1000 | Loss: 0.00001607
Iteration 54/1000 | Loss: 0.00001606
Iteration 55/1000 | Loss: 0.00001606
Iteration 56/1000 | Loss: 0.00001606
Iteration 57/1000 | Loss: 0.00001606
Iteration 58/1000 | Loss: 0.00001606
Iteration 59/1000 | Loss: 0.00001606
Iteration 60/1000 | Loss: 0.00001605
Iteration 61/1000 | Loss: 0.00001605
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001605
Iteration 64/1000 | Loss: 0.00001605
Iteration 65/1000 | Loss: 0.00001605
Iteration 66/1000 | Loss: 0.00001605
Iteration 67/1000 | Loss: 0.00001604
Iteration 68/1000 | Loss: 0.00001604
Iteration 69/1000 | Loss: 0.00001604
Iteration 70/1000 | Loss: 0.00001604
Iteration 71/1000 | Loss: 0.00001604
Iteration 72/1000 | Loss: 0.00001604
Iteration 73/1000 | Loss: 0.00001604
Iteration 74/1000 | Loss: 0.00001603
Iteration 75/1000 | Loss: 0.00001603
Iteration 76/1000 | Loss: 0.00001603
Iteration 77/1000 | Loss: 0.00001603
Iteration 78/1000 | Loss: 0.00001603
Iteration 79/1000 | Loss: 0.00001603
Iteration 80/1000 | Loss: 0.00001603
Iteration 81/1000 | Loss: 0.00001603
Iteration 82/1000 | Loss: 0.00001603
Iteration 83/1000 | Loss: 0.00001603
Iteration 84/1000 | Loss: 0.00001603
Iteration 85/1000 | Loss: 0.00001603
Iteration 86/1000 | Loss: 0.00001603
Iteration 87/1000 | Loss: 0.00001603
Iteration 88/1000 | Loss: 0.00001603
Iteration 89/1000 | Loss: 0.00001603
Iteration 90/1000 | Loss: 0.00001602
Iteration 91/1000 | Loss: 0.00001602
Iteration 92/1000 | Loss: 0.00001602
Iteration 93/1000 | Loss: 0.00001602
Iteration 94/1000 | Loss: 0.00001602
Iteration 95/1000 | Loss: 0.00001602
Iteration 96/1000 | Loss: 0.00001602
Iteration 97/1000 | Loss: 0.00001602
Iteration 98/1000 | Loss: 0.00001602
Iteration 99/1000 | Loss: 0.00001602
Iteration 100/1000 | Loss: 0.00001602
Iteration 101/1000 | Loss: 0.00001602
Iteration 102/1000 | Loss: 0.00001602
Iteration 103/1000 | Loss: 0.00001602
Iteration 104/1000 | Loss: 0.00001602
Iteration 105/1000 | Loss: 0.00001602
Iteration 106/1000 | Loss: 0.00001602
Iteration 107/1000 | Loss: 0.00001602
Iteration 108/1000 | Loss: 0.00004582
Iteration 109/1000 | Loss: 0.00002174
Iteration 110/1000 | Loss: 0.00001602
Iteration 111/1000 | Loss: 0.00001602
Iteration 112/1000 | Loss: 0.00002896
Iteration 113/1000 | Loss: 0.00001603
Iteration 114/1000 | Loss: 0.00001603
Iteration 115/1000 | Loss: 0.00001603
Iteration 116/1000 | Loss: 0.00001602
Iteration 117/1000 | Loss: 0.00001602
Iteration 118/1000 | Loss: 0.00001601
Iteration 119/1000 | Loss: 0.00001600
Iteration 120/1000 | Loss: 0.00001600
Iteration 121/1000 | Loss: 0.00001600
Iteration 122/1000 | Loss: 0.00001600
Iteration 123/1000 | Loss: 0.00001600
Iteration 124/1000 | Loss: 0.00001600
Iteration 125/1000 | Loss: 0.00001600
Iteration 126/1000 | Loss: 0.00001600
Iteration 127/1000 | Loss: 0.00001600
Iteration 128/1000 | Loss: 0.00001600
Iteration 129/1000 | Loss: 0.00001599
Iteration 130/1000 | Loss: 0.00001599
Iteration 131/1000 | Loss: 0.00001599
Iteration 132/1000 | Loss: 0.00001599
Iteration 133/1000 | Loss: 0.00001599
Iteration 134/1000 | Loss: 0.00001599
Iteration 135/1000 | Loss: 0.00001599
Iteration 136/1000 | Loss: 0.00001599
Iteration 137/1000 | Loss: 0.00001599
Iteration 138/1000 | Loss: 0.00001599
Iteration 139/1000 | Loss: 0.00001599
Iteration 140/1000 | Loss: 0.00001598
Iteration 141/1000 | Loss: 0.00001598
Iteration 142/1000 | Loss: 0.00001598
Iteration 143/1000 | Loss: 0.00001598
Iteration 144/1000 | Loss: 0.00001598
Iteration 145/1000 | Loss: 0.00001597
Iteration 146/1000 | Loss: 0.00001597
Iteration 147/1000 | Loss: 0.00001597
Iteration 148/1000 | Loss: 0.00001597
Iteration 149/1000 | Loss: 0.00001597
Iteration 150/1000 | Loss: 0.00001597
Iteration 151/1000 | Loss: 0.00001597
Iteration 152/1000 | Loss: 0.00001597
Iteration 153/1000 | Loss: 0.00001597
Iteration 154/1000 | Loss: 0.00001596
Iteration 155/1000 | Loss: 0.00001596
Iteration 156/1000 | Loss: 0.00001596
Iteration 157/1000 | Loss: 0.00001596
Iteration 158/1000 | Loss: 0.00001596
Iteration 159/1000 | Loss: 0.00001595
Iteration 160/1000 | Loss: 0.00001595
Iteration 161/1000 | Loss: 0.00001595
Iteration 162/1000 | Loss: 0.00001594
Iteration 163/1000 | Loss: 0.00001594
Iteration 164/1000 | Loss: 0.00001594
Iteration 165/1000 | Loss: 0.00001594
Iteration 166/1000 | Loss: 0.00001594
Iteration 167/1000 | Loss: 0.00001593
Iteration 168/1000 | Loss: 0.00001593
Iteration 169/1000 | Loss: 0.00001593
Iteration 170/1000 | Loss: 0.00001592
Iteration 171/1000 | Loss: 0.00001592
Iteration 172/1000 | Loss: 0.00001592
Iteration 173/1000 | Loss: 0.00001592
Iteration 174/1000 | Loss: 0.00001592
Iteration 175/1000 | Loss: 0.00001592
Iteration 176/1000 | Loss: 0.00001592
Iteration 177/1000 | Loss: 0.00001592
Iteration 178/1000 | Loss: 0.00001592
Iteration 179/1000 | Loss: 0.00001591
Iteration 180/1000 | Loss: 0.00001591
Iteration 181/1000 | Loss: 0.00001591
Iteration 182/1000 | Loss: 0.00001591
Iteration 183/1000 | Loss: 0.00001591
Iteration 184/1000 | Loss: 0.00001591
Iteration 185/1000 | Loss: 0.00001591
Iteration 186/1000 | Loss: 0.00001591
Iteration 187/1000 | Loss: 0.00001591
Iteration 188/1000 | Loss: 0.00001591
Iteration 189/1000 | Loss: 0.00001591
Iteration 190/1000 | Loss: 0.00001591
Iteration 191/1000 | Loss: 0.00001591
Iteration 192/1000 | Loss: 0.00001591
Iteration 193/1000 | Loss: 0.00001591
Iteration 194/1000 | Loss: 0.00001591
Iteration 195/1000 | Loss: 0.00001591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 195. Stopping optimization.
Last 5 losses: [1.591046020621434e-05, 1.591046020621434e-05, 1.591046020621434e-05, 1.591046020621434e-05, 1.591046020621434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.591046020621434e-05

Optimization complete. Final v2v error: 3.303436279296875 mm

Highest mean error: 3.5782506465911865 mm for frame 127

Lowest mean error: 3.121993064880371 mm for frame 49

Saving results

Total time: 79.98795437812805
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048770
Iteration 2/25 | Loss: 0.00308964
Iteration 3/25 | Loss: 0.00176228
Iteration 4/25 | Loss: 0.00150939
Iteration 5/25 | Loss: 0.00144451
Iteration 6/25 | Loss: 0.00165415
Iteration 7/25 | Loss: 0.00132862
Iteration 8/25 | Loss: 0.00123369
Iteration 9/25 | Loss: 0.00112016
Iteration 10/25 | Loss: 0.00104686
Iteration 11/25 | Loss: 0.00096424
Iteration 12/25 | Loss: 0.00096027
Iteration 13/25 | Loss: 0.00094396
Iteration 14/25 | Loss: 0.00092832
Iteration 15/25 | Loss: 0.00092602
Iteration 16/25 | Loss: 0.00092601
Iteration 17/25 | Loss: 0.00092601
Iteration 18/25 | Loss: 0.00092601
Iteration 19/25 | Loss: 0.00092601
Iteration 20/25 | Loss: 0.00092601
Iteration 21/25 | Loss: 0.00092601
Iteration 22/25 | Loss: 0.00092601
Iteration 23/25 | Loss: 0.00092601
Iteration 24/25 | Loss: 0.00092601
Iteration 25/25 | Loss: 0.00092600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60394526
Iteration 2/25 | Loss: 0.00171044
Iteration 3/25 | Loss: 0.00158026
Iteration 4/25 | Loss: 0.00158026
Iteration 5/25 | Loss: 0.00158026
Iteration 6/25 | Loss: 0.00158026
Iteration 7/25 | Loss: 0.00158026
Iteration 8/25 | Loss: 0.00158026
Iteration 9/25 | Loss: 0.00158026
Iteration 10/25 | Loss: 0.00158026
Iteration 11/25 | Loss: 0.00158026
Iteration 12/25 | Loss: 0.00158026
Iteration 13/25 | Loss: 0.00158026
Iteration 14/25 | Loss: 0.00158026
Iteration 15/25 | Loss: 0.00158026
Iteration 16/25 | Loss: 0.00158026
Iteration 17/25 | Loss: 0.00158026
Iteration 18/25 | Loss: 0.00158026
Iteration 19/25 | Loss: 0.00158026
Iteration 20/25 | Loss: 0.00158026
Iteration 21/25 | Loss: 0.00158026
Iteration 22/25 | Loss: 0.00158026
Iteration 23/25 | Loss: 0.00158026
Iteration 24/25 | Loss: 0.00158026
Iteration 25/25 | Loss: 0.00158026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158026
Iteration 2/1000 | Loss: 0.00023720
Iteration 3/1000 | Loss: 0.00033718
Iteration 4/1000 | Loss: 0.00029467
Iteration 5/1000 | Loss: 0.00004111
Iteration 6/1000 | Loss: 0.00021260
Iteration 7/1000 | Loss: 0.00003374
Iteration 8/1000 | Loss: 0.00004724
Iteration 9/1000 | Loss: 0.00003442
Iteration 10/1000 | Loss: 0.00020981
Iteration 11/1000 | Loss: 0.00024709
Iteration 12/1000 | Loss: 0.00015235
Iteration 13/1000 | Loss: 0.00003804
Iteration 14/1000 | Loss: 0.00030007
Iteration 15/1000 | Loss: 0.00006284
Iteration 16/1000 | Loss: 0.00006613
Iteration 17/1000 | Loss: 0.00003646
Iteration 18/1000 | Loss: 0.00020114
Iteration 19/1000 | Loss: 0.00009721
Iteration 20/1000 | Loss: 0.00013047
Iteration 21/1000 | Loss: 0.00003695
Iteration 22/1000 | Loss: 0.00003345
Iteration 23/1000 | Loss: 0.00006514
Iteration 24/1000 | Loss: 0.00003548
Iteration 25/1000 | Loss: 0.00004576
Iteration 26/1000 | Loss: 0.00005395
Iteration 27/1000 | Loss: 0.00002800
Iteration 28/1000 | Loss: 0.00005535
Iteration 29/1000 | Loss: 0.00003508
Iteration 30/1000 | Loss: 0.00011417
Iteration 31/1000 | Loss: 0.00003939
Iteration 32/1000 | Loss: 0.00006306
Iteration 33/1000 | Loss: 0.00002775
Iteration 34/1000 | Loss: 0.00004131
Iteration 35/1000 | Loss: 0.00003077
Iteration 36/1000 | Loss: 0.00007617
Iteration 37/1000 | Loss: 0.00003251
Iteration 38/1000 | Loss: 0.00005634
Iteration 39/1000 | Loss: 0.00003480
Iteration 40/1000 | Loss: 0.00003140
Iteration 41/1000 | Loss: 0.00002766
Iteration 42/1000 | Loss: 0.00002766
Iteration 43/1000 | Loss: 0.00002765
Iteration 44/1000 | Loss: 0.00002765
Iteration 45/1000 | Loss: 0.00002765
Iteration 46/1000 | Loss: 0.00004789
Iteration 47/1000 | Loss: 0.00003051
Iteration 48/1000 | Loss: 0.00003407
Iteration 49/1000 | Loss: 0.00004137
Iteration 50/1000 | Loss: 0.00002862
Iteration 51/1000 | Loss: 0.00003378
Iteration 52/1000 | Loss: 0.00002760
Iteration 53/1000 | Loss: 0.00002757
Iteration 54/1000 | Loss: 0.00002757
Iteration 55/1000 | Loss: 0.00002757
Iteration 56/1000 | Loss: 0.00002757
Iteration 57/1000 | Loss: 0.00002757
Iteration 58/1000 | Loss: 0.00002757
Iteration 59/1000 | Loss: 0.00002757
Iteration 60/1000 | Loss: 0.00002757
Iteration 61/1000 | Loss: 0.00002757
Iteration 62/1000 | Loss: 0.00002756
Iteration 63/1000 | Loss: 0.00003141
Iteration 64/1000 | Loss: 0.00003141
Iteration 65/1000 | Loss: 0.00024645
Iteration 66/1000 | Loss: 0.00006820
Iteration 67/1000 | Loss: 0.00002904
Iteration 68/1000 | Loss: 0.00002959
Iteration 69/1000 | Loss: 0.00002947
Iteration 70/1000 | Loss: 0.00002745
Iteration 71/1000 | Loss: 0.00002745
Iteration 72/1000 | Loss: 0.00002744
Iteration 73/1000 | Loss: 0.00002822
Iteration 74/1000 | Loss: 0.00002742
Iteration 75/1000 | Loss: 0.00002742
Iteration 76/1000 | Loss: 0.00002742
Iteration 77/1000 | Loss: 0.00002742
Iteration 78/1000 | Loss: 0.00002742
Iteration 79/1000 | Loss: 0.00002742
Iteration 80/1000 | Loss: 0.00002742
Iteration 81/1000 | Loss: 0.00002742
Iteration 82/1000 | Loss: 0.00002742
Iteration 83/1000 | Loss: 0.00002742
Iteration 84/1000 | Loss: 0.00002742
Iteration 85/1000 | Loss: 0.00002742
Iteration 86/1000 | Loss: 0.00002742
Iteration 87/1000 | Loss: 0.00002742
Iteration 88/1000 | Loss: 0.00002742
Iteration 89/1000 | Loss: 0.00002741
Iteration 90/1000 | Loss: 0.00002741
Iteration 91/1000 | Loss: 0.00002741
Iteration 92/1000 | Loss: 0.00002741
Iteration 93/1000 | Loss: 0.00002741
Iteration 94/1000 | Loss: 0.00002741
Iteration 95/1000 | Loss: 0.00002741
Iteration 96/1000 | Loss: 0.00002741
Iteration 97/1000 | Loss: 0.00002741
Iteration 98/1000 | Loss: 0.00002741
Iteration 99/1000 | Loss: 0.00002741
Iteration 100/1000 | Loss: 0.00002741
Iteration 101/1000 | Loss: 0.00002741
Iteration 102/1000 | Loss: 0.00002741
Iteration 103/1000 | Loss: 0.00002741
Iteration 104/1000 | Loss: 0.00002741
Iteration 105/1000 | Loss: 0.00002741
Iteration 106/1000 | Loss: 0.00002741
Iteration 107/1000 | Loss: 0.00002741
Iteration 108/1000 | Loss: 0.00002741
Iteration 109/1000 | Loss: 0.00002741
Iteration 110/1000 | Loss: 0.00002741
Iteration 111/1000 | Loss: 0.00002741
Iteration 112/1000 | Loss: 0.00002741
Iteration 113/1000 | Loss: 0.00002741
Iteration 114/1000 | Loss: 0.00002741
Iteration 115/1000 | Loss: 0.00002741
Iteration 116/1000 | Loss: 0.00002741
Iteration 117/1000 | Loss: 0.00002741
Iteration 118/1000 | Loss: 0.00002741
Iteration 119/1000 | Loss: 0.00002741
Iteration 120/1000 | Loss: 0.00002741
Iteration 121/1000 | Loss: 0.00002741
Iteration 122/1000 | Loss: 0.00002741
Iteration 123/1000 | Loss: 0.00002741
Iteration 124/1000 | Loss: 0.00002741
Iteration 125/1000 | Loss: 0.00002741
Iteration 126/1000 | Loss: 0.00002741
Iteration 127/1000 | Loss: 0.00002741
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [2.741287426033523e-05, 2.741287426033523e-05, 2.741287426033523e-05, 2.741287426033523e-05, 2.741287426033523e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.741287426033523e-05

Optimization complete. Final v2v error: 4.340527534484863 mm

Highest mean error: 4.641340732574463 mm for frame 51

Lowest mean error: 4.0845794677734375 mm for frame 97

Saving results

Total time: 117.78345680236816
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00467584
Iteration 2/25 | Loss: 0.00106071
Iteration 3/25 | Loss: 0.00082993
Iteration 4/25 | Loss: 0.00080012
Iteration 5/25 | Loss: 0.00079385
Iteration 6/25 | Loss: 0.00079215
Iteration 7/25 | Loss: 0.00079184
Iteration 8/25 | Loss: 0.00079184
Iteration 9/25 | Loss: 0.00079184
Iteration 10/25 | Loss: 0.00079184
Iteration 11/25 | Loss: 0.00079184
Iteration 12/25 | Loss: 0.00079184
Iteration 13/25 | Loss: 0.00079184
Iteration 14/25 | Loss: 0.00079184
Iteration 15/25 | Loss: 0.00079184
Iteration 16/25 | Loss: 0.00079184
Iteration 17/25 | Loss: 0.00079184
Iteration 18/25 | Loss: 0.00079184
Iteration 19/25 | Loss: 0.00079184
Iteration 20/25 | Loss: 0.00079184
Iteration 21/25 | Loss: 0.00079184
Iteration 22/25 | Loss: 0.00079184
Iteration 23/25 | Loss: 0.00079184
Iteration 24/25 | Loss: 0.00079184
Iteration 25/25 | Loss: 0.00079184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0007918400224298239, 0.0007918400224298239, 0.0007918400224298239, 0.0007918400224298239, 0.0007918400224298239]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007918400224298239

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.74642015
Iteration 2/25 | Loss: 0.00124309
Iteration 3/25 | Loss: 0.00124307
Iteration 4/25 | Loss: 0.00124307
Iteration 5/25 | Loss: 0.00124307
Iteration 6/25 | Loss: 0.00124307
Iteration 7/25 | Loss: 0.00124307
Iteration 8/25 | Loss: 0.00124307
Iteration 9/25 | Loss: 0.00124307
Iteration 10/25 | Loss: 0.00124307
Iteration 11/25 | Loss: 0.00124307
Iteration 12/25 | Loss: 0.00124307
Iteration 13/25 | Loss: 0.00124307
Iteration 14/25 | Loss: 0.00124307
Iteration 15/25 | Loss: 0.00124307
Iteration 16/25 | Loss: 0.00124307
Iteration 17/25 | Loss: 0.00124307
Iteration 18/25 | Loss: 0.00124307
Iteration 19/25 | Loss: 0.00124307
Iteration 20/25 | Loss: 0.00124307
Iteration 21/25 | Loss: 0.00124307
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012430681381374598, 0.0012430681381374598, 0.0012430681381374598, 0.0012430681381374598, 0.0012430681381374598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012430681381374598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124307
Iteration 2/1000 | Loss: 0.00003020
Iteration 3/1000 | Loss: 0.00002174
Iteration 4/1000 | Loss: 0.00001998
Iteration 5/1000 | Loss: 0.00001908
Iteration 6/1000 | Loss: 0.00001845
Iteration 7/1000 | Loss: 0.00001809
Iteration 8/1000 | Loss: 0.00001773
Iteration 9/1000 | Loss: 0.00001752
Iteration 10/1000 | Loss: 0.00001750
Iteration 11/1000 | Loss: 0.00001745
Iteration 12/1000 | Loss: 0.00001733
Iteration 13/1000 | Loss: 0.00001731
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001726
Iteration 16/1000 | Loss: 0.00001726
Iteration 17/1000 | Loss: 0.00001721
Iteration 18/1000 | Loss: 0.00001718
Iteration 19/1000 | Loss: 0.00001717
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001715
Iteration 22/1000 | Loss: 0.00001715
Iteration 23/1000 | Loss: 0.00001714
Iteration 24/1000 | Loss: 0.00001714
Iteration 25/1000 | Loss: 0.00001714
Iteration 26/1000 | Loss: 0.00001714
Iteration 27/1000 | Loss: 0.00001714
Iteration 28/1000 | Loss: 0.00001713
Iteration 29/1000 | Loss: 0.00001713
Iteration 30/1000 | Loss: 0.00001713
Iteration 31/1000 | Loss: 0.00001713
Iteration 32/1000 | Loss: 0.00001713
Iteration 33/1000 | Loss: 0.00001713
Iteration 34/1000 | Loss: 0.00001713
Iteration 35/1000 | Loss: 0.00001713
Iteration 36/1000 | Loss: 0.00001712
Iteration 37/1000 | Loss: 0.00001712
Iteration 38/1000 | Loss: 0.00001711
Iteration 39/1000 | Loss: 0.00001711
Iteration 40/1000 | Loss: 0.00001711
Iteration 41/1000 | Loss: 0.00001711
Iteration 42/1000 | Loss: 0.00001711
Iteration 43/1000 | Loss: 0.00001710
Iteration 44/1000 | Loss: 0.00001710
Iteration 45/1000 | Loss: 0.00001710
Iteration 46/1000 | Loss: 0.00001710
Iteration 47/1000 | Loss: 0.00001710
Iteration 48/1000 | Loss: 0.00001710
Iteration 49/1000 | Loss: 0.00001710
Iteration 50/1000 | Loss: 0.00001709
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001709
Iteration 54/1000 | Loss: 0.00001709
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001708
Iteration 61/1000 | Loss: 0.00001708
Iteration 62/1000 | Loss: 0.00001708
Iteration 63/1000 | Loss: 0.00001708
Iteration 64/1000 | Loss: 0.00001707
Iteration 65/1000 | Loss: 0.00001707
Iteration 66/1000 | Loss: 0.00001707
Iteration 67/1000 | Loss: 0.00001706
Iteration 68/1000 | Loss: 0.00001706
Iteration 69/1000 | Loss: 0.00001706
Iteration 70/1000 | Loss: 0.00001706
Iteration 71/1000 | Loss: 0.00001705
Iteration 72/1000 | Loss: 0.00001705
Iteration 73/1000 | Loss: 0.00001705
Iteration 74/1000 | Loss: 0.00001705
Iteration 75/1000 | Loss: 0.00001705
Iteration 76/1000 | Loss: 0.00001705
Iteration 77/1000 | Loss: 0.00001705
Iteration 78/1000 | Loss: 0.00001705
Iteration 79/1000 | Loss: 0.00001705
Iteration 80/1000 | Loss: 0.00001704
Iteration 81/1000 | Loss: 0.00001704
Iteration 82/1000 | Loss: 0.00001704
Iteration 83/1000 | Loss: 0.00001703
Iteration 84/1000 | Loss: 0.00001703
Iteration 85/1000 | Loss: 0.00001703
Iteration 86/1000 | Loss: 0.00001703
Iteration 87/1000 | Loss: 0.00001703
Iteration 88/1000 | Loss: 0.00001703
Iteration 89/1000 | Loss: 0.00001703
Iteration 90/1000 | Loss: 0.00001703
Iteration 91/1000 | Loss: 0.00001702
Iteration 92/1000 | Loss: 0.00001702
Iteration 93/1000 | Loss: 0.00001702
Iteration 94/1000 | Loss: 0.00001702
Iteration 95/1000 | Loss: 0.00001702
Iteration 96/1000 | Loss: 0.00001701
Iteration 97/1000 | Loss: 0.00001701
Iteration 98/1000 | Loss: 0.00001701
Iteration 99/1000 | Loss: 0.00001701
Iteration 100/1000 | Loss: 0.00001700
Iteration 101/1000 | Loss: 0.00001700
Iteration 102/1000 | Loss: 0.00001700
Iteration 103/1000 | Loss: 0.00001700
Iteration 104/1000 | Loss: 0.00001700
Iteration 105/1000 | Loss: 0.00001699
Iteration 106/1000 | Loss: 0.00001699
Iteration 107/1000 | Loss: 0.00001699
Iteration 108/1000 | Loss: 0.00001698
Iteration 109/1000 | Loss: 0.00001698
Iteration 110/1000 | Loss: 0.00001698
Iteration 111/1000 | Loss: 0.00001698
Iteration 112/1000 | Loss: 0.00001698
Iteration 113/1000 | Loss: 0.00001698
Iteration 114/1000 | Loss: 0.00001698
Iteration 115/1000 | Loss: 0.00001698
Iteration 116/1000 | Loss: 0.00001698
Iteration 117/1000 | Loss: 0.00001698
Iteration 118/1000 | Loss: 0.00001698
Iteration 119/1000 | Loss: 0.00001698
Iteration 120/1000 | Loss: 0.00001698
Iteration 121/1000 | Loss: 0.00001698
Iteration 122/1000 | Loss: 0.00001697
Iteration 123/1000 | Loss: 0.00001697
Iteration 124/1000 | Loss: 0.00001697
Iteration 125/1000 | Loss: 0.00001697
Iteration 126/1000 | Loss: 0.00001697
Iteration 127/1000 | Loss: 0.00001696
Iteration 128/1000 | Loss: 0.00001696
Iteration 129/1000 | Loss: 0.00001696
Iteration 130/1000 | Loss: 0.00001696
Iteration 131/1000 | Loss: 0.00001696
Iteration 132/1000 | Loss: 0.00001696
Iteration 133/1000 | Loss: 0.00001696
Iteration 134/1000 | Loss: 0.00001696
Iteration 135/1000 | Loss: 0.00001696
Iteration 136/1000 | Loss: 0.00001696
Iteration 137/1000 | Loss: 0.00001696
Iteration 138/1000 | Loss: 0.00001696
Iteration 139/1000 | Loss: 0.00001696
Iteration 140/1000 | Loss: 0.00001696
Iteration 141/1000 | Loss: 0.00001696
Iteration 142/1000 | Loss: 0.00001696
Iteration 143/1000 | Loss: 0.00001696
Iteration 144/1000 | Loss: 0.00001696
Iteration 145/1000 | Loss: 0.00001696
Iteration 146/1000 | Loss: 0.00001696
Iteration 147/1000 | Loss: 0.00001696
Iteration 148/1000 | Loss: 0.00001696
Iteration 149/1000 | Loss: 0.00001696
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.6960744687821716e-05, 1.6960744687821716e-05, 1.6960744687821716e-05, 1.6960744687821716e-05, 1.6960744687821716e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6960744687821716e-05

Optimization complete. Final v2v error: 3.456620693206787 mm

Highest mean error: 4.258157730102539 mm for frame 147

Lowest mean error: 2.845191478729248 mm for frame 6

Saving results

Total time: 36.35040616989136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00862178
Iteration 2/25 | Loss: 0.00131781
Iteration 3/25 | Loss: 0.00098627
Iteration 4/25 | Loss: 0.00092125
Iteration 5/25 | Loss: 0.00094500
Iteration 6/25 | Loss: 0.00089472
Iteration 7/25 | Loss: 0.00087081
Iteration 8/25 | Loss: 0.00085986
Iteration 9/25 | Loss: 0.00085588
Iteration 10/25 | Loss: 0.00085387
Iteration 11/25 | Loss: 0.00085304
Iteration 12/25 | Loss: 0.00085290
Iteration 13/25 | Loss: 0.00085289
Iteration 14/25 | Loss: 0.00085289
Iteration 15/25 | Loss: 0.00085289
Iteration 16/25 | Loss: 0.00085289
Iteration 17/25 | Loss: 0.00085288
Iteration 18/25 | Loss: 0.00085288
Iteration 19/25 | Loss: 0.00085288
Iteration 20/25 | Loss: 0.00085288
Iteration 21/25 | Loss: 0.00085288
Iteration 22/25 | Loss: 0.00085287
Iteration 23/25 | Loss: 0.00085287
Iteration 24/25 | Loss: 0.00085287
Iteration 25/25 | Loss: 0.00085287

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50085485
Iteration 2/25 | Loss: 0.00141857
Iteration 3/25 | Loss: 0.00141853
Iteration 4/25 | Loss: 0.00141853
Iteration 5/25 | Loss: 0.00141853
Iteration 6/25 | Loss: 0.00141853
Iteration 7/25 | Loss: 0.00141853
Iteration 8/25 | Loss: 0.00141853
Iteration 9/25 | Loss: 0.00141853
Iteration 10/25 | Loss: 0.00141853
Iteration 11/25 | Loss: 0.00141853
Iteration 12/25 | Loss: 0.00141853
Iteration 13/25 | Loss: 0.00141853
Iteration 14/25 | Loss: 0.00141853
Iteration 15/25 | Loss: 0.00141853
Iteration 16/25 | Loss: 0.00141853
Iteration 17/25 | Loss: 0.00141853
Iteration 18/25 | Loss: 0.00141853
Iteration 19/25 | Loss: 0.00141853
Iteration 20/25 | Loss: 0.00141853
Iteration 21/25 | Loss: 0.00141853
Iteration 22/25 | Loss: 0.00141853
Iteration 23/25 | Loss: 0.00141853
Iteration 24/25 | Loss: 0.00141853
Iteration 25/25 | Loss: 0.00141853

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141853
Iteration 2/1000 | Loss: 0.00004385
Iteration 3/1000 | Loss: 0.00003499
Iteration 4/1000 | Loss: 0.00002934
Iteration 5/1000 | Loss: 0.00002708
Iteration 6/1000 | Loss: 0.00002581
Iteration 7/1000 | Loss: 0.00002498
Iteration 8/1000 | Loss: 0.00002426
Iteration 9/1000 | Loss: 0.00002379
Iteration 10/1000 | Loss: 0.00002350
Iteration 11/1000 | Loss: 0.00002321
Iteration 12/1000 | Loss: 0.00002296
Iteration 13/1000 | Loss: 0.00002265
Iteration 14/1000 | Loss: 0.00002237
Iteration 15/1000 | Loss: 0.00037240
Iteration 16/1000 | Loss: 0.00051521
Iteration 17/1000 | Loss: 0.00038741
Iteration 18/1000 | Loss: 0.00033551
Iteration 19/1000 | Loss: 0.00040720
Iteration 20/1000 | Loss: 0.00003787
Iteration 21/1000 | Loss: 0.00090784
Iteration 22/1000 | Loss: 0.00055436
Iteration 23/1000 | Loss: 0.00009907
Iteration 24/1000 | Loss: 0.00023389
Iteration 25/1000 | Loss: 0.00004201
Iteration 26/1000 | Loss: 0.00003768
Iteration 27/1000 | Loss: 0.00003404
Iteration 28/1000 | Loss: 0.00003053
Iteration 29/1000 | Loss: 0.00002785
Iteration 30/1000 | Loss: 0.00035854
Iteration 31/1000 | Loss: 0.00024905
Iteration 32/1000 | Loss: 0.00003320
Iteration 33/1000 | Loss: 0.00002892
Iteration 34/1000 | Loss: 0.00030827
Iteration 35/1000 | Loss: 0.00003288
Iteration 36/1000 | Loss: 0.00036382
Iteration 37/1000 | Loss: 0.00031180
Iteration 38/1000 | Loss: 0.00038220
Iteration 39/1000 | Loss: 0.00031286
Iteration 40/1000 | Loss: 0.00032446
Iteration 41/1000 | Loss: 0.00032459
Iteration 42/1000 | Loss: 0.00031476
Iteration 43/1000 | Loss: 0.00021047
Iteration 44/1000 | Loss: 0.00025792
Iteration 45/1000 | Loss: 0.00018578
Iteration 46/1000 | Loss: 0.00017282
Iteration 47/1000 | Loss: 0.00002408
Iteration 48/1000 | Loss: 0.00002288
Iteration 49/1000 | Loss: 0.00002230
Iteration 50/1000 | Loss: 0.00002207
Iteration 51/1000 | Loss: 0.00002184
Iteration 52/1000 | Loss: 0.00002162
Iteration 53/1000 | Loss: 0.00002135
Iteration 54/1000 | Loss: 0.00002109
Iteration 55/1000 | Loss: 0.00032384
Iteration 56/1000 | Loss: 0.00035671
Iteration 57/1000 | Loss: 0.00003027
Iteration 58/1000 | Loss: 0.00002373
Iteration 59/1000 | Loss: 0.00002110
Iteration 60/1000 | Loss: 0.00002021
Iteration 61/1000 | Loss: 0.00002001
Iteration 62/1000 | Loss: 0.00001984
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001961
Iteration 65/1000 | Loss: 0.00001961
Iteration 66/1000 | Loss: 0.00001960
Iteration 67/1000 | Loss: 0.00001960
Iteration 68/1000 | Loss: 0.00001958
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001945
Iteration 71/1000 | Loss: 0.00001944
Iteration 72/1000 | Loss: 0.00001944
Iteration 73/1000 | Loss: 0.00001942
Iteration 74/1000 | Loss: 0.00001942
Iteration 75/1000 | Loss: 0.00001941
Iteration 76/1000 | Loss: 0.00001941
Iteration 77/1000 | Loss: 0.00001941
Iteration 78/1000 | Loss: 0.00001940
Iteration 79/1000 | Loss: 0.00001940
Iteration 80/1000 | Loss: 0.00001940
Iteration 81/1000 | Loss: 0.00001940
Iteration 82/1000 | Loss: 0.00001940
Iteration 83/1000 | Loss: 0.00001940
Iteration 84/1000 | Loss: 0.00001940
Iteration 85/1000 | Loss: 0.00001940
Iteration 86/1000 | Loss: 0.00001940
Iteration 87/1000 | Loss: 0.00001939
Iteration 88/1000 | Loss: 0.00001939
Iteration 89/1000 | Loss: 0.00001939
Iteration 90/1000 | Loss: 0.00001939
Iteration 91/1000 | Loss: 0.00001939
Iteration 92/1000 | Loss: 0.00001939
Iteration 93/1000 | Loss: 0.00001939
Iteration 94/1000 | Loss: 0.00001939
Iteration 95/1000 | Loss: 0.00001939
Iteration 96/1000 | Loss: 0.00001939
Iteration 97/1000 | Loss: 0.00001938
Iteration 98/1000 | Loss: 0.00001938
Iteration 99/1000 | Loss: 0.00001938
Iteration 100/1000 | Loss: 0.00001938
Iteration 101/1000 | Loss: 0.00001938
Iteration 102/1000 | Loss: 0.00001938
Iteration 103/1000 | Loss: 0.00001938
Iteration 104/1000 | Loss: 0.00001937
Iteration 105/1000 | Loss: 0.00001937
Iteration 106/1000 | Loss: 0.00001937
Iteration 107/1000 | Loss: 0.00001937
Iteration 108/1000 | Loss: 0.00001937
Iteration 109/1000 | Loss: 0.00001937
Iteration 110/1000 | Loss: 0.00001937
Iteration 111/1000 | Loss: 0.00001937
Iteration 112/1000 | Loss: 0.00001937
Iteration 113/1000 | Loss: 0.00001937
Iteration 114/1000 | Loss: 0.00001937
Iteration 115/1000 | Loss: 0.00001937
Iteration 116/1000 | Loss: 0.00001937
Iteration 117/1000 | Loss: 0.00001936
Iteration 118/1000 | Loss: 0.00001936
Iteration 119/1000 | Loss: 0.00001936
Iteration 120/1000 | Loss: 0.00001936
Iteration 121/1000 | Loss: 0.00001936
Iteration 122/1000 | Loss: 0.00001936
Iteration 123/1000 | Loss: 0.00001936
Iteration 124/1000 | Loss: 0.00001936
Iteration 125/1000 | Loss: 0.00001936
Iteration 126/1000 | Loss: 0.00001936
Iteration 127/1000 | Loss: 0.00001936
Iteration 128/1000 | Loss: 0.00001936
Iteration 129/1000 | Loss: 0.00001936
Iteration 130/1000 | Loss: 0.00001935
Iteration 131/1000 | Loss: 0.00001935
Iteration 132/1000 | Loss: 0.00001935
Iteration 133/1000 | Loss: 0.00001935
Iteration 134/1000 | Loss: 0.00001935
Iteration 135/1000 | Loss: 0.00001935
Iteration 136/1000 | Loss: 0.00001935
Iteration 137/1000 | Loss: 0.00001935
Iteration 138/1000 | Loss: 0.00001935
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001934
Iteration 144/1000 | Loss: 0.00001934
Iteration 145/1000 | Loss: 0.00001933
Iteration 146/1000 | Loss: 0.00001933
Iteration 147/1000 | Loss: 0.00001933
Iteration 148/1000 | Loss: 0.00001933
Iteration 149/1000 | Loss: 0.00001933
Iteration 150/1000 | Loss: 0.00001933
Iteration 151/1000 | Loss: 0.00001933
Iteration 152/1000 | Loss: 0.00001933
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.9332845113240182e-05, 1.9332845113240182e-05, 1.9332845113240182e-05, 1.9332845113240182e-05, 1.9332845113240182e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9332845113240182e-05

Optimization complete. Final v2v error: 3.6503424644470215 mm

Highest mean error: 5.008358478546143 mm for frame 30

Lowest mean error: 3.0415642261505127 mm for frame 20

Saving results

Total time: 118.52405261993408
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01030096
Iteration 2/25 | Loss: 0.00160061
Iteration 3/25 | Loss: 0.00096423
Iteration 4/25 | Loss: 0.00087746
Iteration 5/25 | Loss: 0.00087090
Iteration 6/25 | Loss: 0.00086997
Iteration 7/25 | Loss: 0.00086997
Iteration 8/25 | Loss: 0.00086997
Iteration 9/25 | Loss: 0.00086997
Iteration 10/25 | Loss: 0.00086997
Iteration 11/25 | Loss: 0.00086997
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008699681493453681, 0.0008699681493453681, 0.0008699681493453681, 0.0008699681493453681, 0.0008699681493453681]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008699681493453681

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58689809
Iteration 2/25 | Loss: 0.00120681
Iteration 3/25 | Loss: 0.00120681
Iteration 4/25 | Loss: 0.00120681
Iteration 5/25 | Loss: 0.00120681
Iteration 6/25 | Loss: 0.00120681
Iteration 7/25 | Loss: 0.00120681
Iteration 8/25 | Loss: 0.00120681
Iteration 9/25 | Loss: 0.00120681
Iteration 10/25 | Loss: 0.00120681
Iteration 11/25 | Loss: 0.00120681
Iteration 12/25 | Loss: 0.00120681
Iteration 13/25 | Loss: 0.00120681
Iteration 14/25 | Loss: 0.00120681
Iteration 15/25 | Loss: 0.00120681
Iteration 16/25 | Loss: 0.00120681
Iteration 17/25 | Loss: 0.00120681
Iteration 18/25 | Loss: 0.00120681
Iteration 19/25 | Loss: 0.00120681
Iteration 20/25 | Loss: 0.00120681
Iteration 21/25 | Loss: 0.00120681
Iteration 22/25 | Loss: 0.00120681
Iteration 23/25 | Loss: 0.00120681
Iteration 24/25 | Loss: 0.00120681
Iteration 25/25 | Loss: 0.00120681

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120681
Iteration 2/1000 | Loss: 0.00003271
Iteration 3/1000 | Loss: 0.00002434
Iteration 4/1000 | Loss: 0.00002279
Iteration 5/1000 | Loss: 0.00002180
Iteration 6/1000 | Loss: 0.00002128
Iteration 7/1000 | Loss: 0.00002091
Iteration 8/1000 | Loss: 0.00002075
Iteration 9/1000 | Loss: 0.00002066
Iteration 10/1000 | Loss: 0.00002057
Iteration 11/1000 | Loss: 0.00002054
Iteration 12/1000 | Loss: 0.00002053
Iteration 13/1000 | Loss: 0.00002048
Iteration 14/1000 | Loss: 0.00002046
Iteration 15/1000 | Loss: 0.00002044
Iteration 16/1000 | Loss: 0.00002044
Iteration 17/1000 | Loss: 0.00002043
Iteration 18/1000 | Loss: 0.00002042
Iteration 19/1000 | Loss: 0.00002040
Iteration 20/1000 | Loss: 0.00002040
Iteration 21/1000 | Loss: 0.00002040
Iteration 22/1000 | Loss: 0.00002040
Iteration 23/1000 | Loss: 0.00002039
Iteration 24/1000 | Loss: 0.00002039
Iteration 25/1000 | Loss: 0.00002039
Iteration 26/1000 | Loss: 0.00002039
Iteration 27/1000 | Loss: 0.00002039
Iteration 28/1000 | Loss: 0.00002039
Iteration 29/1000 | Loss: 0.00002039
Iteration 30/1000 | Loss: 0.00002039
Iteration 31/1000 | Loss: 0.00002039
Iteration 32/1000 | Loss: 0.00002038
Iteration 33/1000 | Loss: 0.00002038
Iteration 34/1000 | Loss: 0.00002038
Iteration 35/1000 | Loss: 0.00002038
Iteration 36/1000 | Loss: 0.00002037
Iteration 37/1000 | Loss: 0.00002037
Iteration 38/1000 | Loss: 0.00002037
Iteration 39/1000 | Loss: 0.00002037
Iteration 40/1000 | Loss: 0.00002037
Iteration 41/1000 | Loss: 0.00002037
Iteration 42/1000 | Loss: 0.00002037
Iteration 43/1000 | Loss: 0.00002037
Iteration 44/1000 | Loss: 0.00002037
Iteration 45/1000 | Loss: 0.00002037
Iteration 46/1000 | Loss: 0.00002036
Iteration 47/1000 | Loss: 0.00002036
Iteration 48/1000 | Loss: 0.00002036
Iteration 49/1000 | Loss: 0.00002035
Iteration 50/1000 | Loss: 0.00002035
Iteration 51/1000 | Loss: 0.00002035
Iteration 52/1000 | Loss: 0.00002035
Iteration 53/1000 | Loss: 0.00002035
Iteration 54/1000 | Loss: 0.00002034
Iteration 55/1000 | Loss: 0.00002034
Iteration 56/1000 | Loss: 0.00002033
Iteration 57/1000 | Loss: 0.00002033
Iteration 58/1000 | Loss: 0.00002033
Iteration 59/1000 | Loss: 0.00002032
Iteration 60/1000 | Loss: 0.00002032
Iteration 61/1000 | Loss: 0.00002032
Iteration 62/1000 | Loss: 0.00002032
Iteration 63/1000 | Loss: 0.00002032
Iteration 64/1000 | Loss: 0.00002032
Iteration 65/1000 | Loss: 0.00002032
Iteration 66/1000 | Loss: 0.00002032
Iteration 67/1000 | Loss: 0.00002031
Iteration 68/1000 | Loss: 0.00002031
Iteration 69/1000 | Loss: 0.00002031
Iteration 70/1000 | Loss: 0.00002031
Iteration 71/1000 | Loss: 0.00002031
Iteration 72/1000 | Loss: 0.00002031
Iteration 73/1000 | Loss: 0.00002031
Iteration 74/1000 | Loss: 0.00002031
Iteration 75/1000 | Loss: 0.00002031
Iteration 76/1000 | Loss: 0.00002031
Iteration 77/1000 | Loss: 0.00002030
Iteration 78/1000 | Loss: 0.00002030
Iteration 79/1000 | Loss: 0.00002030
Iteration 80/1000 | Loss: 0.00002030
Iteration 81/1000 | Loss: 0.00002030
Iteration 82/1000 | Loss: 0.00002030
Iteration 83/1000 | Loss: 0.00002030
Iteration 84/1000 | Loss: 0.00002030
Iteration 85/1000 | Loss: 0.00002030
Iteration 86/1000 | Loss: 0.00002030
Iteration 87/1000 | Loss: 0.00002030
Iteration 88/1000 | Loss: 0.00002030
Iteration 89/1000 | Loss: 0.00002030
Iteration 90/1000 | Loss: 0.00002030
Iteration 91/1000 | Loss: 0.00002030
Iteration 92/1000 | Loss: 0.00002030
Iteration 93/1000 | Loss: 0.00002030
Iteration 94/1000 | Loss: 0.00002030
Iteration 95/1000 | Loss: 0.00002030
Iteration 96/1000 | Loss: 0.00002030
Iteration 97/1000 | Loss: 0.00002030
Iteration 98/1000 | Loss: 0.00002030
Iteration 99/1000 | Loss: 0.00002030
Iteration 100/1000 | Loss: 0.00002030
Iteration 101/1000 | Loss: 0.00002030
Iteration 102/1000 | Loss: 0.00002030
Iteration 103/1000 | Loss: 0.00002030
Iteration 104/1000 | Loss: 0.00002030
Iteration 105/1000 | Loss: 0.00002030
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [2.0302042685216293e-05, 2.0302042685216293e-05, 2.0302042685216293e-05, 2.0302042685216293e-05, 2.0302042685216293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0302042685216293e-05

Optimization complete. Final v2v error: 3.6974124908447266 mm

Highest mean error: 3.7312798500061035 mm for frame 169

Lowest mean error: 3.5910823345184326 mm for frame 1

Saving results

Total time: 28.909430980682373
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053784
Iteration 2/25 | Loss: 0.00258875
Iteration 3/25 | Loss: 0.00247360
Iteration 4/25 | Loss: 0.00235506
Iteration 5/25 | Loss: 0.00185995
Iteration 6/25 | Loss: 0.00180824
Iteration 7/25 | Loss: 0.00172793
Iteration 8/25 | Loss: 0.00157269
Iteration 9/25 | Loss: 0.00145146
Iteration 10/25 | Loss: 0.00133897
Iteration 11/25 | Loss: 0.00127570
Iteration 12/25 | Loss: 0.00122429
Iteration 13/25 | Loss: 0.00120194
Iteration 14/25 | Loss: 0.00121036
Iteration 15/25 | Loss: 0.00122920
Iteration 16/25 | Loss: 0.00119290
Iteration 17/25 | Loss: 0.00119267
Iteration 18/25 | Loss: 0.00118605
Iteration 19/25 | Loss: 0.00118699
Iteration 20/25 | Loss: 0.00118458
Iteration 21/25 | Loss: 0.00118035
Iteration 22/25 | Loss: 0.00117997
Iteration 23/25 | Loss: 0.00117967
Iteration 24/25 | Loss: 0.00117951
Iteration 25/25 | Loss: 0.00117942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60462594
Iteration 2/25 | Loss: 0.00376182
Iteration 3/25 | Loss: 0.00264664
Iteration 4/25 | Loss: 0.00261923
Iteration 5/25 | Loss: 0.00261923
Iteration 6/25 | Loss: 0.00261923
Iteration 7/25 | Loss: 0.00261923
Iteration 8/25 | Loss: 0.00261923
Iteration 9/25 | Loss: 0.00261923
Iteration 10/25 | Loss: 0.00261923
Iteration 11/25 | Loss: 0.00261923
Iteration 12/25 | Loss: 0.00261923
Iteration 13/25 | Loss: 0.00261923
Iteration 14/25 | Loss: 0.00261923
Iteration 15/25 | Loss: 0.00261923
Iteration 16/25 | Loss: 0.00261923
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002619227860122919, 0.002619227860122919, 0.002619227860122919, 0.002619227860122919, 0.002619227860122919]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002619227860122919

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261923
Iteration 2/1000 | Loss: 0.00517552
Iteration 3/1000 | Loss: 0.00289181
Iteration 4/1000 | Loss: 0.00304519
Iteration 5/1000 | Loss: 0.00050413
Iteration 6/1000 | Loss: 0.00107135
Iteration 7/1000 | Loss: 0.00268291
Iteration 8/1000 | Loss: 0.00036675
Iteration 9/1000 | Loss: 0.00011857
Iteration 10/1000 | Loss: 0.00067339
Iteration 11/1000 | Loss: 0.00507160
Iteration 12/1000 | Loss: 0.00867858
Iteration 13/1000 | Loss: 0.00535011
Iteration 14/1000 | Loss: 0.00848822
Iteration 15/1000 | Loss: 0.00069000
Iteration 16/1000 | Loss: 0.00061745
Iteration 17/1000 | Loss: 0.00006124
Iteration 18/1000 | Loss: 0.00055357
Iteration 19/1000 | Loss: 0.00004398
Iteration 20/1000 | Loss: 0.00025061
Iteration 21/1000 | Loss: 0.00021323
Iteration 22/1000 | Loss: 0.00014994
Iteration 23/1000 | Loss: 0.00041950
Iteration 24/1000 | Loss: 0.00016431
Iteration 25/1000 | Loss: 0.00003406
Iteration 26/1000 | Loss: 0.00003001
Iteration 27/1000 | Loss: 0.00002764
Iteration 28/1000 | Loss: 0.00053758
Iteration 29/1000 | Loss: 0.00023024
Iteration 30/1000 | Loss: 0.00020607
Iteration 31/1000 | Loss: 0.00003580
Iteration 32/1000 | Loss: 0.00007888
Iteration 33/1000 | Loss: 0.00006843
Iteration 34/1000 | Loss: 0.00002470
Iteration 35/1000 | Loss: 0.00014055
Iteration 36/1000 | Loss: 0.00002328
Iteration 37/1000 | Loss: 0.00002206
Iteration 38/1000 | Loss: 0.00002526
Iteration 39/1000 | Loss: 0.00014030
Iteration 40/1000 | Loss: 0.00002134
Iteration 41/1000 | Loss: 0.00002104
Iteration 42/1000 | Loss: 0.00002082
Iteration 43/1000 | Loss: 0.00002073
Iteration 44/1000 | Loss: 0.00002070
Iteration 45/1000 | Loss: 0.00013036
Iteration 46/1000 | Loss: 0.00002081
Iteration 47/1000 | Loss: 0.00002063
Iteration 48/1000 | Loss: 0.00002062
Iteration 49/1000 | Loss: 0.00002060
Iteration 50/1000 | Loss: 0.00002060
Iteration 51/1000 | Loss: 0.00002059
Iteration 52/1000 | Loss: 0.00002059
Iteration 53/1000 | Loss: 0.00002058
Iteration 54/1000 | Loss: 0.00002058
Iteration 55/1000 | Loss: 0.00002058
Iteration 56/1000 | Loss: 0.00002058
Iteration 57/1000 | Loss: 0.00002058
Iteration 58/1000 | Loss: 0.00002058
Iteration 59/1000 | Loss: 0.00002058
Iteration 60/1000 | Loss: 0.00002058
Iteration 61/1000 | Loss: 0.00002058
Iteration 62/1000 | Loss: 0.00002058
Iteration 63/1000 | Loss: 0.00002057
Iteration 64/1000 | Loss: 0.00002056
Iteration 65/1000 | Loss: 0.00002055
Iteration 66/1000 | Loss: 0.00002055
Iteration 67/1000 | Loss: 0.00002055
Iteration 68/1000 | Loss: 0.00002055
Iteration 69/1000 | Loss: 0.00002054
Iteration 70/1000 | Loss: 0.00002054
Iteration 71/1000 | Loss: 0.00002054
Iteration 72/1000 | Loss: 0.00002054
Iteration 73/1000 | Loss: 0.00002054
Iteration 74/1000 | Loss: 0.00002051
Iteration 75/1000 | Loss: 0.00002051
Iteration 76/1000 | Loss: 0.00002051
Iteration 77/1000 | Loss: 0.00002050
Iteration 78/1000 | Loss: 0.00002050
Iteration 79/1000 | Loss: 0.00002049
Iteration 80/1000 | Loss: 0.00002049
Iteration 81/1000 | Loss: 0.00002049
Iteration 82/1000 | Loss: 0.00002048
Iteration 83/1000 | Loss: 0.00002048
Iteration 84/1000 | Loss: 0.00002048
Iteration 85/1000 | Loss: 0.00002048
Iteration 86/1000 | Loss: 0.00002048
Iteration 87/1000 | Loss: 0.00002048
Iteration 88/1000 | Loss: 0.00002047
Iteration 89/1000 | Loss: 0.00002047
Iteration 90/1000 | Loss: 0.00002047
Iteration 91/1000 | Loss: 0.00002047
Iteration 92/1000 | Loss: 0.00002047
Iteration 93/1000 | Loss: 0.00002046
Iteration 94/1000 | Loss: 0.00002046
Iteration 95/1000 | Loss: 0.00002046
Iteration 96/1000 | Loss: 0.00002046
Iteration 97/1000 | Loss: 0.00002046
Iteration 98/1000 | Loss: 0.00002046
Iteration 99/1000 | Loss: 0.00002046
Iteration 100/1000 | Loss: 0.00002046
Iteration 101/1000 | Loss: 0.00002046
Iteration 102/1000 | Loss: 0.00002046
Iteration 103/1000 | Loss: 0.00002046
Iteration 104/1000 | Loss: 0.00002046
Iteration 105/1000 | Loss: 0.00002046
Iteration 106/1000 | Loss: 0.00002046
Iteration 107/1000 | Loss: 0.00002046
Iteration 108/1000 | Loss: 0.00002046
Iteration 109/1000 | Loss: 0.00002046
Iteration 110/1000 | Loss: 0.00002046
Iteration 111/1000 | Loss: 0.00002046
Iteration 112/1000 | Loss: 0.00002046
Iteration 113/1000 | Loss: 0.00002046
Iteration 114/1000 | Loss: 0.00002046
Iteration 115/1000 | Loss: 0.00002046
Iteration 116/1000 | Loss: 0.00002046
Iteration 117/1000 | Loss: 0.00002046
Iteration 118/1000 | Loss: 0.00002046
Iteration 119/1000 | Loss: 0.00002046
Iteration 120/1000 | Loss: 0.00002046
Iteration 121/1000 | Loss: 0.00002046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [2.0455212506931275e-05, 2.0455212506931275e-05, 2.0455212506931275e-05, 2.0455212506931275e-05, 2.0455212506931275e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0455212506931275e-05

Optimization complete. Final v2v error: 3.718571186065674 mm

Highest mean error: 9.339378356933594 mm for frame 52

Lowest mean error: 3.318211793899536 mm for frame 98

Saving results

Total time: 123.38661646842957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00590681
Iteration 2/25 | Loss: 0.00113266
Iteration 3/25 | Loss: 0.00097433
Iteration 4/25 | Loss: 0.00093353
Iteration 5/25 | Loss: 0.00092741
Iteration 6/25 | Loss: 0.00092580
Iteration 7/25 | Loss: 0.00092555
Iteration 8/25 | Loss: 0.00092555
Iteration 9/25 | Loss: 0.00092555
Iteration 10/25 | Loss: 0.00092555
Iteration 11/25 | Loss: 0.00092555
Iteration 12/25 | Loss: 0.00092555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009255459881387651, 0.0009255459881387651, 0.0009255459881387651, 0.0009255459881387651, 0.0009255459881387651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009255459881387651

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17162764
Iteration 2/25 | Loss: 0.00118959
Iteration 3/25 | Loss: 0.00118959
Iteration 4/25 | Loss: 0.00118958
Iteration 5/25 | Loss: 0.00118958
Iteration 6/25 | Loss: 0.00118958
Iteration 7/25 | Loss: 0.00118958
Iteration 8/25 | Loss: 0.00118958
Iteration 9/25 | Loss: 0.00118958
Iteration 10/25 | Loss: 0.00118958
Iteration 11/25 | Loss: 0.00118958
Iteration 12/25 | Loss: 0.00118958
Iteration 13/25 | Loss: 0.00118958
Iteration 14/25 | Loss: 0.00118958
Iteration 15/25 | Loss: 0.00118958
Iteration 16/25 | Loss: 0.00118958
Iteration 17/25 | Loss: 0.00118958
Iteration 18/25 | Loss: 0.00118958
Iteration 19/25 | Loss: 0.00118958
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011895832139998674, 0.0011895832139998674, 0.0011895832139998674, 0.0011895832139998674, 0.0011895832139998674]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011895832139998674

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118958
Iteration 2/1000 | Loss: 0.00005910
Iteration 3/1000 | Loss: 0.00004569
Iteration 4/1000 | Loss: 0.00004187
Iteration 5/1000 | Loss: 0.00003918
Iteration 6/1000 | Loss: 0.00003736
Iteration 7/1000 | Loss: 0.00003623
Iteration 8/1000 | Loss: 0.00003577
Iteration 9/1000 | Loss: 0.00003558
Iteration 10/1000 | Loss: 0.00003532
Iteration 11/1000 | Loss: 0.00003510
Iteration 12/1000 | Loss: 0.00003507
Iteration 13/1000 | Loss: 0.00003497
Iteration 14/1000 | Loss: 0.00003497
Iteration 15/1000 | Loss: 0.00003497
Iteration 16/1000 | Loss: 0.00003497
Iteration 17/1000 | Loss: 0.00003497
Iteration 18/1000 | Loss: 0.00003497
Iteration 19/1000 | Loss: 0.00003497
Iteration 20/1000 | Loss: 0.00003496
Iteration 21/1000 | Loss: 0.00003489
Iteration 22/1000 | Loss: 0.00003483
Iteration 23/1000 | Loss: 0.00003483
Iteration 24/1000 | Loss: 0.00003483
Iteration 25/1000 | Loss: 0.00003483
Iteration 26/1000 | Loss: 0.00003483
Iteration 27/1000 | Loss: 0.00003483
Iteration 28/1000 | Loss: 0.00003483
Iteration 29/1000 | Loss: 0.00003483
Iteration 30/1000 | Loss: 0.00003483
Iteration 31/1000 | Loss: 0.00003483
Iteration 32/1000 | Loss: 0.00003483
Iteration 33/1000 | Loss: 0.00003483
Iteration 34/1000 | Loss: 0.00003483
Iteration 35/1000 | Loss: 0.00003483
Iteration 36/1000 | Loss: 0.00003483
Iteration 37/1000 | Loss: 0.00003483
Iteration 38/1000 | Loss: 0.00003483
Iteration 39/1000 | Loss: 0.00003483
Iteration 40/1000 | Loss: 0.00003483
Iteration 41/1000 | Loss: 0.00003483
Iteration 42/1000 | Loss: 0.00003483
Iteration 43/1000 | Loss: 0.00003483
Iteration 44/1000 | Loss: 0.00003483
Iteration 45/1000 | Loss: 0.00003483
Iteration 46/1000 | Loss: 0.00003483
Iteration 47/1000 | Loss: 0.00003483
Iteration 48/1000 | Loss: 0.00003483
Iteration 49/1000 | Loss: 0.00003483
Iteration 50/1000 | Loss: 0.00003483
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 50. Stopping optimization.
Last 5 losses: [3.482680403976701e-05, 3.482680403976701e-05, 3.482680403976701e-05, 3.482680403976701e-05, 3.482680403976701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.482680403976701e-05

Optimization complete. Final v2v error: 4.955682277679443 mm

Highest mean error: 5.069504261016846 mm for frame 16

Lowest mean error: 4.763876914978027 mm for frame 143

Saving results

Total time: 29.225301027297974
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816571
Iteration 2/25 | Loss: 0.00147281
Iteration 3/25 | Loss: 0.00109350
Iteration 4/25 | Loss: 0.00098413
Iteration 5/25 | Loss: 0.00097808
Iteration 6/25 | Loss: 0.00095966
Iteration 7/25 | Loss: 0.00097332
Iteration 8/25 | Loss: 0.00095638
Iteration 9/25 | Loss: 0.00096716
Iteration 10/25 | Loss: 0.00095589
Iteration 11/25 | Loss: 0.00095705
Iteration 12/25 | Loss: 0.00095946
Iteration 13/25 | Loss: 0.00094412
Iteration 14/25 | Loss: 0.00094945
Iteration 15/25 | Loss: 0.00093665
Iteration 16/25 | Loss: 0.00093372
Iteration 17/25 | Loss: 0.00093312
Iteration 18/25 | Loss: 0.00093274
Iteration 19/25 | Loss: 0.00093098
Iteration 20/25 | Loss: 0.00092556
Iteration 21/25 | Loss: 0.00093908
Iteration 22/25 | Loss: 0.00093190
Iteration 23/25 | Loss: 0.00092639
Iteration 24/25 | Loss: 0.00092620
Iteration 25/25 | Loss: 0.00091977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.05542278
Iteration 2/25 | Loss: 0.00287701
Iteration 3/25 | Loss: 0.00287683
Iteration 4/25 | Loss: 0.00287683
Iteration 5/25 | Loss: 0.00287683
Iteration 6/25 | Loss: 0.00287683
Iteration 7/25 | Loss: 0.00287683
Iteration 8/25 | Loss: 0.00287683
Iteration 9/25 | Loss: 0.00287683
Iteration 10/25 | Loss: 0.00287683
Iteration 11/25 | Loss: 0.00287683
Iteration 12/25 | Loss: 0.00287683
Iteration 13/25 | Loss: 0.00287683
Iteration 14/25 | Loss: 0.00287683
Iteration 15/25 | Loss: 0.00287683
Iteration 16/25 | Loss: 0.00287683
Iteration 17/25 | Loss: 0.00287683
Iteration 18/25 | Loss: 0.00287683
Iteration 19/25 | Loss: 0.00287683
Iteration 20/25 | Loss: 0.00287683
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0028768256306648254, 0.0028768256306648254, 0.0028768256306648254, 0.0028768256306648254, 0.0028768256306648254]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028768256306648254

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00287683
Iteration 2/1000 | Loss: 0.00174800
Iteration 3/1000 | Loss: 0.01674918
Iteration 4/1000 | Loss: 0.01912082
Iteration 5/1000 | Loss: 0.00033399
Iteration 6/1000 | Loss: 0.00015121
Iteration 7/1000 | Loss: 0.00011060
Iteration 8/1000 | Loss: 0.00014033
Iteration 9/1000 | Loss: 0.00006489
Iteration 10/1000 | Loss: 0.00007810
Iteration 11/1000 | Loss: 0.00016489
Iteration 12/1000 | Loss: 0.00005732
Iteration 13/1000 | Loss: 0.00009548
Iteration 14/1000 | Loss: 0.00007903
Iteration 15/1000 | Loss: 0.00022569
Iteration 16/1000 | Loss: 0.00016211
Iteration 17/1000 | Loss: 0.00023339
Iteration 18/1000 | Loss: 0.00018950
Iteration 19/1000 | Loss: 0.00020554
Iteration 20/1000 | Loss: 0.00022279
Iteration 21/1000 | Loss: 0.00020878
Iteration 22/1000 | Loss: 0.00020839
Iteration 23/1000 | Loss: 0.00020645
Iteration 24/1000 | Loss: 0.00007375
Iteration 25/1000 | Loss: 0.00020827
Iteration 26/1000 | Loss: 0.00057000
Iteration 27/1000 | Loss: 0.00011642
Iteration 28/1000 | Loss: 0.00107084
Iteration 29/1000 | Loss: 0.00056752
Iteration 30/1000 | Loss: 0.00082239
Iteration 31/1000 | Loss: 0.00026065
Iteration 32/1000 | Loss: 0.00011866
Iteration 33/1000 | Loss: 0.00034373
Iteration 34/1000 | Loss: 0.00015423
Iteration 35/1000 | Loss: 0.00008069
Iteration 36/1000 | Loss: 0.00019649
Iteration 37/1000 | Loss: 0.00015232
Iteration 38/1000 | Loss: 0.00018987
Iteration 39/1000 | Loss: 0.00015896
Iteration 40/1000 | Loss: 0.00018595
Iteration 41/1000 | Loss: 0.00016324
Iteration 42/1000 | Loss: 0.00020359
Iteration 43/1000 | Loss: 0.00021256
Iteration 44/1000 | Loss: 0.00027675
Iteration 45/1000 | Loss: 0.00015056
Iteration 46/1000 | Loss: 0.00014279
Iteration 47/1000 | Loss: 0.00008057
Iteration 48/1000 | Loss: 0.00008390
Iteration 49/1000 | Loss: 0.00018949
Iteration 50/1000 | Loss: 0.00024488
Iteration 51/1000 | Loss: 0.00024975
Iteration 52/1000 | Loss: 0.00020235
Iteration 53/1000 | Loss: 0.00003512
Iteration 54/1000 | Loss: 0.00003397
Iteration 55/1000 | Loss: 0.00003313
Iteration 56/1000 | Loss: 0.00006024
Iteration 57/1000 | Loss: 0.00018450
Iteration 58/1000 | Loss: 0.00018701
Iteration 59/1000 | Loss: 0.00006435
Iteration 60/1000 | Loss: 0.00020275
Iteration 61/1000 | Loss: 0.00011669
Iteration 62/1000 | Loss: 0.00017397
Iteration 63/1000 | Loss: 0.00014511
Iteration 64/1000 | Loss: 0.00020793
Iteration 65/1000 | Loss: 0.00026538
Iteration 66/1000 | Loss: 0.00010948
Iteration 67/1000 | Loss: 0.00003797
Iteration 68/1000 | Loss: 0.00004682
Iteration 69/1000 | Loss: 0.00019228
Iteration 70/1000 | Loss: 0.00039031
Iteration 71/1000 | Loss: 0.00013235
Iteration 72/1000 | Loss: 0.00018193
Iteration 73/1000 | Loss: 0.00065446
Iteration 74/1000 | Loss: 0.00041007
Iteration 75/1000 | Loss: 0.00010798
Iteration 76/1000 | Loss: 0.00020206
Iteration 77/1000 | Loss: 0.00014412
Iteration 78/1000 | Loss: 0.00023045
Iteration 79/1000 | Loss: 0.00038454
Iteration 80/1000 | Loss: 0.00005962
Iteration 81/1000 | Loss: 0.00003898
Iteration 82/1000 | Loss: 0.00003507
Iteration 83/1000 | Loss: 0.00003305
Iteration 84/1000 | Loss: 0.00003881
Iteration 85/1000 | Loss: 0.00003260
Iteration 86/1000 | Loss: 0.00003063
Iteration 87/1000 | Loss: 0.00002966
Iteration 88/1000 | Loss: 0.00002887
Iteration 89/1000 | Loss: 0.00002843
Iteration 90/1000 | Loss: 0.00002783
Iteration 91/1000 | Loss: 0.00002698
Iteration 92/1000 | Loss: 0.00002628
Iteration 93/1000 | Loss: 0.00002577
Iteration 94/1000 | Loss: 0.00002544
Iteration 95/1000 | Loss: 0.00002519
Iteration 96/1000 | Loss: 0.00002517
Iteration 97/1000 | Loss: 0.00002516
Iteration 98/1000 | Loss: 0.00002512
Iteration 99/1000 | Loss: 0.00002499
Iteration 100/1000 | Loss: 0.00002499
Iteration 101/1000 | Loss: 0.00002490
Iteration 102/1000 | Loss: 0.00002486
Iteration 103/1000 | Loss: 0.00002486
Iteration 104/1000 | Loss: 0.00002483
Iteration 105/1000 | Loss: 0.00002482
Iteration 106/1000 | Loss: 0.00002482
Iteration 107/1000 | Loss: 0.00002481
Iteration 108/1000 | Loss: 0.00002481
Iteration 109/1000 | Loss: 0.00002480
Iteration 110/1000 | Loss: 0.00002480
Iteration 111/1000 | Loss: 0.00002480
Iteration 112/1000 | Loss: 0.00002479
Iteration 113/1000 | Loss: 0.00002479
Iteration 114/1000 | Loss: 0.00002478
Iteration 115/1000 | Loss: 0.00002478
Iteration 116/1000 | Loss: 0.00002478
Iteration 117/1000 | Loss: 0.00002477
Iteration 118/1000 | Loss: 0.00002477
Iteration 119/1000 | Loss: 0.00002477
Iteration 120/1000 | Loss: 0.00002477
Iteration 121/1000 | Loss: 0.00002476
Iteration 122/1000 | Loss: 0.00002476
Iteration 123/1000 | Loss: 0.00002476
Iteration 124/1000 | Loss: 0.00002475
Iteration 125/1000 | Loss: 0.00002475
Iteration 126/1000 | Loss: 0.00002475
Iteration 127/1000 | Loss: 0.00002475
Iteration 128/1000 | Loss: 0.00002475
Iteration 129/1000 | Loss: 0.00002475
Iteration 130/1000 | Loss: 0.00002474
Iteration 131/1000 | Loss: 0.00002474
Iteration 132/1000 | Loss: 0.00002474
Iteration 133/1000 | Loss: 0.00002474
Iteration 134/1000 | Loss: 0.00002474
Iteration 135/1000 | Loss: 0.00002473
Iteration 136/1000 | Loss: 0.00002473
Iteration 137/1000 | Loss: 0.00002473
Iteration 138/1000 | Loss: 0.00002473
Iteration 139/1000 | Loss: 0.00002472
Iteration 140/1000 | Loss: 0.00002471
Iteration 141/1000 | Loss: 0.00002471
Iteration 142/1000 | Loss: 0.00002471
Iteration 143/1000 | Loss: 0.00002471
Iteration 144/1000 | Loss: 0.00002470
Iteration 145/1000 | Loss: 0.00002470
Iteration 146/1000 | Loss: 0.00002470
Iteration 147/1000 | Loss: 0.00002470
Iteration 148/1000 | Loss: 0.00002470
Iteration 149/1000 | Loss: 0.00002470
Iteration 150/1000 | Loss: 0.00002469
Iteration 151/1000 | Loss: 0.00002469
Iteration 152/1000 | Loss: 0.00002469
Iteration 153/1000 | Loss: 0.00002469
Iteration 154/1000 | Loss: 0.00002469
Iteration 155/1000 | Loss: 0.00002469
Iteration 156/1000 | Loss: 0.00002469
Iteration 157/1000 | Loss: 0.00002469
Iteration 158/1000 | Loss: 0.00002469
Iteration 159/1000 | Loss: 0.00002469
Iteration 160/1000 | Loss: 0.00002469
Iteration 161/1000 | Loss: 0.00002468
Iteration 162/1000 | Loss: 0.00002468
Iteration 163/1000 | Loss: 0.00002468
Iteration 164/1000 | Loss: 0.00002468
Iteration 165/1000 | Loss: 0.00002468
Iteration 166/1000 | Loss: 0.00002468
Iteration 167/1000 | Loss: 0.00002468
Iteration 168/1000 | Loss: 0.00002468
Iteration 169/1000 | Loss: 0.00002468
Iteration 170/1000 | Loss: 0.00002468
Iteration 171/1000 | Loss: 0.00002468
Iteration 172/1000 | Loss: 0.00002468
Iteration 173/1000 | Loss: 0.00002468
Iteration 174/1000 | Loss: 0.00002467
Iteration 175/1000 | Loss: 0.00002467
Iteration 176/1000 | Loss: 0.00002467
Iteration 177/1000 | Loss: 0.00002467
Iteration 178/1000 | Loss: 0.00002467
Iteration 179/1000 | Loss: 0.00002467
Iteration 180/1000 | Loss: 0.00002467
Iteration 181/1000 | Loss: 0.00002467
Iteration 182/1000 | Loss: 0.00002467
Iteration 183/1000 | Loss: 0.00002467
Iteration 184/1000 | Loss: 0.00002467
Iteration 185/1000 | Loss: 0.00002467
Iteration 186/1000 | Loss: 0.00002466
Iteration 187/1000 | Loss: 0.00002466
Iteration 188/1000 | Loss: 0.00002466
Iteration 189/1000 | Loss: 0.00002466
Iteration 190/1000 | Loss: 0.00002466
Iteration 191/1000 | Loss: 0.00002466
Iteration 192/1000 | Loss: 0.00002466
Iteration 193/1000 | Loss: 0.00002466
Iteration 194/1000 | Loss: 0.00002465
Iteration 195/1000 | Loss: 0.00002465
Iteration 196/1000 | Loss: 0.00002465
Iteration 197/1000 | Loss: 0.00002465
Iteration 198/1000 | Loss: 0.00002465
Iteration 199/1000 | Loss: 0.00002465
Iteration 200/1000 | Loss: 0.00002465
Iteration 201/1000 | Loss: 0.00002465
Iteration 202/1000 | Loss: 0.00002465
Iteration 203/1000 | Loss: 0.00002464
Iteration 204/1000 | Loss: 0.00002464
Iteration 205/1000 | Loss: 0.00002464
Iteration 206/1000 | Loss: 0.00002464
Iteration 207/1000 | Loss: 0.00002464
Iteration 208/1000 | Loss: 0.00002464
Iteration 209/1000 | Loss: 0.00002464
Iteration 210/1000 | Loss: 0.00002464
Iteration 211/1000 | Loss: 0.00002463
Iteration 212/1000 | Loss: 0.00002463
Iteration 213/1000 | Loss: 0.00002463
Iteration 214/1000 | Loss: 0.00002463
Iteration 215/1000 | Loss: 0.00002463
Iteration 216/1000 | Loss: 0.00002463
Iteration 217/1000 | Loss: 0.00002463
Iteration 218/1000 | Loss: 0.00002463
Iteration 219/1000 | Loss: 0.00002463
Iteration 220/1000 | Loss: 0.00002463
Iteration 221/1000 | Loss: 0.00002463
Iteration 222/1000 | Loss: 0.00002463
Iteration 223/1000 | Loss: 0.00002463
Iteration 224/1000 | Loss: 0.00002463
Iteration 225/1000 | Loss: 0.00002463
Iteration 226/1000 | Loss: 0.00002463
Iteration 227/1000 | Loss: 0.00002463
Iteration 228/1000 | Loss: 0.00002463
Iteration 229/1000 | Loss: 0.00002463
Iteration 230/1000 | Loss: 0.00002463
Iteration 231/1000 | Loss: 0.00002463
Iteration 232/1000 | Loss: 0.00002463
Iteration 233/1000 | Loss: 0.00002463
Iteration 234/1000 | Loss: 0.00002463
Iteration 235/1000 | Loss: 0.00002463
Iteration 236/1000 | Loss: 0.00002463
Iteration 237/1000 | Loss: 0.00002463
Iteration 238/1000 | Loss: 0.00002463
Iteration 239/1000 | Loss: 0.00002463
Iteration 240/1000 | Loss: 0.00002463
Iteration 241/1000 | Loss: 0.00002463
Iteration 242/1000 | Loss: 0.00002463
Iteration 243/1000 | Loss: 0.00002463
Iteration 244/1000 | Loss: 0.00002463
Iteration 245/1000 | Loss: 0.00002463
Iteration 246/1000 | Loss: 0.00002463
Iteration 247/1000 | Loss: 0.00002463
Iteration 248/1000 | Loss: 0.00002463
Iteration 249/1000 | Loss: 0.00002463
Iteration 250/1000 | Loss: 0.00002463
Iteration 251/1000 | Loss: 0.00002463
Iteration 252/1000 | Loss: 0.00002463
Iteration 253/1000 | Loss: 0.00002463
Iteration 254/1000 | Loss: 0.00002463
Iteration 255/1000 | Loss: 0.00002463
Iteration 256/1000 | Loss: 0.00002463
Iteration 257/1000 | Loss: 0.00002463
Iteration 258/1000 | Loss: 0.00002463
Iteration 259/1000 | Loss: 0.00002463
Iteration 260/1000 | Loss: 0.00002463
Iteration 261/1000 | Loss: 0.00002463
Iteration 262/1000 | Loss: 0.00002463
Iteration 263/1000 | Loss: 0.00002463
Iteration 264/1000 | Loss: 0.00002463
Iteration 265/1000 | Loss: 0.00002463
Iteration 266/1000 | Loss: 0.00002463
Iteration 267/1000 | Loss: 0.00002463
Iteration 268/1000 | Loss: 0.00002463
Iteration 269/1000 | Loss: 0.00002463
Iteration 270/1000 | Loss: 0.00002463
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 270. Stopping optimization.
Last 5 losses: [2.462816337356344e-05, 2.462816337356344e-05, 2.462816337356344e-05, 2.462816337356344e-05, 2.462816337356344e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.462816337356344e-05

Optimization complete. Final v2v error: 3.954267978668213 mm

Highest mean error: 6.454590797424316 mm for frame 34

Lowest mean error: 2.8083486557006836 mm for frame 70

Saving results

Total time: 195.16528844833374
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00385583
Iteration 2/25 | Loss: 0.00113736
Iteration 3/25 | Loss: 0.00092457
Iteration 4/25 | Loss: 0.00086176
Iteration 5/25 | Loss: 0.00083934
Iteration 6/25 | Loss: 0.00083437
Iteration 7/25 | Loss: 0.00083293
Iteration 8/25 | Loss: 0.00083248
Iteration 9/25 | Loss: 0.00083248
Iteration 10/25 | Loss: 0.00083248
Iteration 11/25 | Loss: 0.00083248
Iteration 12/25 | Loss: 0.00083248
Iteration 13/25 | Loss: 0.00083248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008324783411808312, 0.0008324783411808312, 0.0008324783411808312, 0.0008324783411808312, 0.0008324783411808312]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008324783411808312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50378251
Iteration 2/25 | Loss: 0.00145995
Iteration 3/25 | Loss: 0.00145994
Iteration 4/25 | Loss: 0.00145994
Iteration 5/25 | Loss: 0.00145994
Iteration 6/25 | Loss: 0.00145994
Iteration 7/25 | Loss: 0.00145994
Iteration 8/25 | Loss: 0.00145994
Iteration 9/25 | Loss: 0.00145994
Iteration 10/25 | Loss: 0.00145994
Iteration 11/25 | Loss: 0.00145994
Iteration 12/25 | Loss: 0.00145994
Iteration 13/25 | Loss: 0.00145994
Iteration 14/25 | Loss: 0.00145994
Iteration 15/25 | Loss: 0.00145994
Iteration 16/25 | Loss: 0.00145994
Iteration 17/25 | Loss: 0.00145994
Iteration 18/25 | Loss: 0.00145994
Iteration 19/25 | Loss: 0.00145994
Iteration 20/25 | Loss: 0.00145994
Iteration 21/25 | Loss: 0.00145994
Iteration 22/25 | Loss: 0.00145994
Iteration 23/25 | Loss: 0.00145994
Iteration 24/25 | Loss: 0.00145994
Iteration 25/25 | Loss: 0.00145994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145994
Iteration 2/1000 | Loss: 0.00004444
Iteration 3/1000 | Loss: 0.00003127
Iteration 4/1000 | Loss: 0.00002744
Iteration 5/1000 | Loss: 0.00002599
Iteration 6/1000 | Loss: 0.00002467
Iteration 7/1000 | Loss: 0.00002406
Iteration 8/1000 | Loss: 0.00002354
Iteration 9/1000 | Loss: 0.00002321
Iteration 10/1000 | Loss: 0.00002292
Iteration 11/1000 | Loss: 0.00002270
Iteration 12/1000 | Loss: 0.00002249
Iteration 13/1000 | Loss: 0.00002233
Iteration 14/1000 | Loss: 0.00002231
Iteration 15/1000 | Loss: 0.00002227
Iteration 16/1000 | Loss: 0.00002226
Iteration 17/1000 | Loss: 0.00002225
Iteration 18/1000 | Loss: 0.00002224
Iteration 19/1000 | Loss: 0.00002223
Iteration 20/1000 | Loss: 0.00002223
Iteration 21/1000 | Loss: 0.00002220
Iteration 22/1000 | Loss: 0.00002219
Iteration 23/1000 | Loss: 0.00002217
Iteration 24/1000 | Loss: 0.00002217
Iteration 25/1000 | Loss: 0.00002217
Iteration 26/1000 | Loss: 0.00002217
Iteration 27/1000 | Loss: 0.00002217
Iteration 28/1000 | Loss: 0.00002217
Iteration 29/1000 | Loss: 0.00002217
Iteration 30/1000 | Loss: 0.00002217
Iteration 31/1000 | Loss: 0.00002217
Iteration 32/1000 | Loss: 0.00002217
Iteration 33/1000 | Loss: 0.00002217
Iteration 34/1000 | Loss: 0.00002216
Iteration 35/1000 | Loss: 0.00002216
Iteration 36/1000 | Loss: 0.00002216
Iteration 37/1000 | Loss: 0.00002216
Iteration 38/1000 | Loss: 0.00002214
Iteration 39/1000 | Loss: 0.00002214
Iteration 40/1000 | Loss: 0.00002213
Iteration 41/1000 | Loss: 0.00002213
Iteration 42/1000 | Loss: 0.00002213
Iteration 43/1000 | Loss: 0.00002212
Iteration 44/1000 | Loss: 0.00002212
Iteration 45/1000 | Loss: 0.00002212
Iteration 46/1000 | Loss: 0.00002211
Iteration 47/1000 | Loss: 0.00002211
Iteration 48/1000 | Loss: 0.00002211
Iteration 49/1000 | Loss: 0.00002210
Iteration 50/1000 | Loss: 0.00002210
Iteration 51/1000 | Loss: 0.00002210
Iteration 52/1000 | Loss: 0.00002210
Iteration 53/1000 | Loss: 0.00002209
Iteration 54/1000 | Loss: 0.00002209
Iteration 55/1000 | Loss: 0.00002209
Iteration 56/1000 | Loss: 0.00002209
Iteration 57/1000 | Loss: 0.00002208
Iteration 58/1000 | Loss: 0.00002208
Iteration 59/1000 | Loss: 0.00002208
Iteration 60/1000 | Loss: 0.00002208
Iteration 61/1000 | Loss: 0.00002208
Iteration 62/1000 | Loss: 0.00002208
Iteration 63/1000 | Loss: 0.00002208
Iteration 64/1000 | Loss: 0.00002208
Iteration 65/1000 | Loss: 0.00002208
Iteration 66/1000 | Loss: 0.00002207
Iteration 67/1000 | Loss: 0.00002207
Iteration 68/1000 | Loss: 0.00002207
Iteration 69/1000 | Loss: 0.00002207
Iteration 70/1000 | Loss: 0.00002206
Iteration 71/1000 | Loss: 0.00002206
Iteration 72/1000 | Loss: 0.00002206
Iteration 73/1000 | Loss: 0.00002205
Iteration 74/1000 | Loss: 0.00002205
Iteration 75/1000 | Loss: 0.00002205
Iteration 76/1000 | Loss: 0.00002205
Iteration 77/1000 | Loss: 0.00002205
Iteration 78/1000 | Loss: 0.00002204
Iteration 79/1000 | Loss: 0.00002204
Iteration 80/1000 | Loss: 0.00002204
Iteration 81/1000 | Loss: 0.00002204
Iteration 82/1000 | Loss: 0.00002204
Iteration 83/1000 | Loss: 0.00002204
Iteration 84/1000 | Loss: 0.00002204
Iteration 85/1000 | Loss: 0.00002204
Iteration 86/1000 | Loss: 0.00002204
Iteration 87/1000 | Loss: 0.00002204
Iteration 88/1000 | Loss: 0.00002204
Iteration 89/1000 | Loss: 0.00002204
Iteration 90/1000 | Loss: 0.00002203
Iteration 91/1000 | Loss: 0.00002203
Iteration 92/1000 | Loss: 0.00002203
Iteration 93/1000 | Loss: 0.00002203
Iteration 94/1000 | Loss: 0.00002203
Iteration 95/1000 | Loss: 0.00002202
Iteration 96/1000 | Loss: 0.00002202
Iteration 97/1000 | Loss: 0.00002202
Iteration 98/1000 | Loss: 0.00002201
Iteration 99/1000 | Loss: 0.00002201
Iteration 100/1000 | Loss: 0.00002201
Iteration 101/1000 | Loss: 0.00002201
Iteration 102/1000 | Loss: 0.00002201
Iteration 103/1000 | Loss: 0.00002201
Iteration 104/1000 | Loss: 0.00002201
Iteration 105/1000 | Loss: 0.00002201
Iteration 106/1000 | Loss: 0.00002200
Iteration 107/1000 | Loss: 0.00002200
Iteration 108/1000 | Loss: 0.00002200
Iteration 109/1000 | Loss: 0.00002200
Iteration 110/1000 | Loss: 0.00002200
Iteration 111/1000 | Loss: 0.00002200
Iteration 112/1000 | Loss: 0.00002200
Iteration 113/1000 | Loss: 0.00002200
Iteration 114/1000 | Loss: 0.00002200
Iteration 115/1000 | Loss: 0.00002200
Iteration 116/1000 | Loss: 0.00002200
Iteration 117/1000 | Loss: 0.00002200
Iteration 118/1000 | Loss: 0.00002200
Iteration 119/1000 | Loss: 0.00002200
Iteration 120/1000 | Loss: 0.00002200
Iteration 121/1000 | Loss: 0.00002200
Iteration 122/1000 | Loss: 0.00002200
Iteration 123/1000 | Loss: 0.00002200
Iteration 124/1000 | Loss: 0.00002200
Iteration 125/1000 | Loss: 0.00002200
Iteration 126/1000 | Loss: 0.00002200
Iteration 127/1000 | Loss: 0.00002200
Iteration 128/1000 | Loss: 0.00002200
Iteration 129/1000 | Loss: 0.00002200
Iteration 130/1000 | Loss: 0.00002200
Iteration 131/1000 | Loss: 0.00002200
Iteration 132/1000 | Loss: 0.00002200
Iteration 133/1000 | Loss: 0.00002200
Iteration 134/1000 | Loss: 0.00002200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [2.1995485440129414e-05, 2.1995485440129414e-05, 2.1995485440129414e-05, 2.1995485440129414e-05, 2.1995485440129414e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1995485440129414e-05

Optimization complete. Final v2v error: 3.8604748249053955 mm

Highest mean error: 4.493063926696777 mm for frame 239

Lowest mean error: 2.7715978622436523 mm for frame 80

Saving results

Total time: 44.2763352394104
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00789368
Iteration 2/25 | Loss: 0.00112443
Iteration 3/25 | Loss: 0.00090042
Iteration 4/25 | Loss: 0.00085139
Iteration 5/25 | Loss: 0.00084346
Iteration 6/25 | Loss: 0.00084261
Iteration 7/25 | Loss: 0.00084261
Iteration 8/25 | Loss: 0.00084261
Iteration 9/25 | Loss: 0.00084261
Iteration 10/25 | Loss: 0.00084261
Iteration 11/25 | Loss: 0.00084261
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008426100248470902, 0.0008426100248470902, 0.0008426100248470902, 0.0008426100248470902, 0.0008426100248470902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008426100248470902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59979343
Iteration 2/25 | Loss: 0.00148516
Iteration 3/25 | Loss: 0.00148512
Iteration 4/25 | Loss: 0.00148512
Iteration 5/25 | Loss: 0.00148512
Iteration 6/25 | Loss: 0.00148512
Iteration 7/25 | Loss: 0.00148512
Iteration 8/25 | Loss: 0.00148512
Iteration 9/25 | Loss: 0.00148512
Iteration 10/25 | Loss: 0.00148512
Iteration 11/25 | Loss: 0.00148512
Iteration 12/25 | Loss: 0.00148512
Iteration 13/25 | Loss: 0.00148512
Iteration 14/25 | Loss: 0.00148512
Iteration 15/25 | Loss: 0.00148512
Iteration 16/25 | Loss: 0.00148512
Iteration 17/25 | Loss: 0.00148512
Iteration 18/25 | Loss: 0.00148512
Iteration 19/25 | Loss: 0.00148512
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00148511934094131, 0.00148511934094131, 0.00148511934094131, 0.00148511934094131, 0.00148511934094131]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00148511934094131

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148512
Iteration 2/1000 | Loss: 0.00006209
Iteration 3/1000 | Loss: 0.00003959
Iteration 4/1000 | Loss: 0.00003422
Iteration 5/1000 | Loss: 0.00003155
Iteration 6/1000 | Loss: 0.00002991
Iteration 7/1000 | Loss: 0.00002874
Iteration 8/1000 | Loss: 0.00002788
Iteration 9/1000 | Loss: 0.00002746
Iteration 10/1000 | Loss: 0.00002709
Iteration 11/1000 | Loss: 0.00002676
Iteration 12/1000 | Loss: 0.00002655
Iteration 13/1000 | Loss: 0.00002652
Iteration 14/1000 | Loss: 0.00002642
Iteration 15/1000 | Loss: 0.00002627
Iteration 16/1000 | Loss: 0.00002624
Iteration 17/1000 | Loss: 0.00002622
Iteration 18/1000 | Loss: 0.00002616
Iteration 19/1000 | Loss: 0.00002616
Iteration 20/1000 | Loss: 0.00002615
Iteration 21/1000 | Loss: 0.00002615
Iteration 22/1000 | Loss: 0.00002614
Iteration 23/1000 | Loss: 0.00002614
Iteration 24/1000 | Loss: 0.00002614
Iteration 25/1000 | Loss: 0.00002614
Iteration 26/1000 | Loss: 0.00002613
Iteration 27/1000 | Loss: 0.00002613
Iteration 28/1000 | Loss: 0.00002613
Iteration 29/1000 | Loss: 0.00002613
Iteration 30/1000 | Loss: 0.00002612
Iteration 31/1000 | Loss: 0.00002611
Iteration 32/1000 | Loss: 0.00002611
Iteration 33/1000 | Loss: 0.00002611
Iteration 34/1000 | Loss: 0.00002611
Iteration 35/1000 | Loss: 0.00002610
Iteration 36/1000 | Loss: 0.00002610
Iteration 37/1000 | Loss: 0.00002610
Iteration 38/1000 | Loss: 0.00002610
Iteration 39/1000 | Loss: 0.00002609
Iteration 40/1000 | Loss: 0.00002609
Iteration 41/1000 | Loss: 0.00002609
Iteration 42/1000 | Loss: 0.00002609
Iteration 43/1000 | Loss: 0.00002608
Iteration 44/1000 | Loss: 0.00002608
Iteration 45/1000 | Loss: 0.00002608
Iteration 46/1000 | Loss: 0.00002607
Iteration 47/1000 | Loss: 0.00002607
Iteration 48/1000 | Loss: 0.00002607
Iteration 49/1000 | Loss: 0.00002607
Iteration 50/1000 | Loss: 0.00002606
Iteration 51/1000 | Loss: 0.00002606
Iteration 52/1000 | Loss: 0.00002606
Iteration 53/1000 | Loss: 0.00002606
Iteration 54/1000 | Loss: 0.00002606
Iteration 55/1000 | Loss: 0.00002606
Iteration 56/1000 | Loss: 0.00002606
Iteration 57/1000 | Loss: 0.00002605
Iteration 58/1000 | Loss: 0.00002605
Iteration 59/1000 | Loss: 0.00002605
Iteration 60/1000 | Loss: 0.00002605
Iteration 61/1000 | Loss: 0.00002605
Iteration 62/1000 | Loss: 0.00002604
Iteration 63/1000 | Loss: 0.00002604
Iteration 64/1000 | Loss: 0.00002604
Iteration 65/1000 | Loss: 0.00002604
Iteration 66/1000 | Loss: 0.00002604
Iteration 67/1000 | Loss: 0.00002604
Iteration 68/1000 | Loss: 0.00002604
Iteration 69/1000 | Loss: 0.00002604
Iteration 70/1000 | Loss: 0.00002603
Iteration 71/1000 | Loss: 0.00002603
Iteration 72/1000 | Loss: 0.00002603
Iteration 73/1000 | Loss: 0.00002602
Iteration 74/1000 | Loss: 0.00002602
Iteration 75/1000 | Loss: 0.00002602
Iteration 76/1000 | Loss: 0.00002602
Iteration 77/1000 | Loss: 0.00002602
Iteration 78/1000 | Loss: 0.00002601
Iteration 79/1000 | Loss: 0.00002601
Iteration 80/1000 | Loss: 0.00002601
Iteration 81/1000 | Loss: 0.00002601
Iteration 82/1000 | Loss: 0.00002601
Iteration 83/1000 | Loss: 0.00002601
Iteration 84/1000 | Loss: 0.00002601
Iteration 85/1000 | Loss: 0.00002601
Iteration 86/1000 | Loss: 0.00002601
Iteration 87/1000 | Loss: 0.00002600
Iteration 88/1000 | Loss: 0.00002600
Iteration 89/1000 | Loss: 0.00002600
Iteration 90/1000 | Loss: 0.00002600
Iteration 91/1000 | Loss: 0.00002600
Iteration 92/1000 | Loss: 0.00002600
Iteration 93/1000 | Loss: 0.00002599
Iteration 94/1000 | Loss: 0.00002599
Iteration 95/1000 | Loss: 0.00002599
Iteration 96/1000 | Loss: 0.00002599
Iteration 97/1000 | Loss: 0.00002598
Iteration 98/1000 | Loss: 0.00002598
Iteration 99/1000 | Loss: 0.00002598
Iteration 100/1000 | Loss: 0.00002598
Iteration 101/1000 | Loss: 0.00002597
Iteration 102/1000 | Loss: 0.00002597
Iteration 103/1000 | Loss: 0.00002597
Iteration 104/1000 | Loss: 0.00002597
Iteration 105/1000 | Loss: 0.00002597
Iteration 106/1000 | Loss: 0.00002597
Iteration 107/1000 | Loss: 0.00002596
Iteration 108/1000 | Loss: 0.00002596
Iteration 109/1000 | Loss: 0.00002596
Iteration 110/1000 | Loss: 0.00002596
Iteration 111/1000 | Loss: 0.00002596
Iteration 112/1000 | Loss: 0.00002596
Iteration 113/1000 | Loss: 0.00002596
Iteration 114/1000 | Loss: 0.00002596
Iteration 115/1000 | Loss: 0.00002596
Iteration 116/1000 | Loss: 0.00002595
Iteration 117/1000 | Loss: 0.00002595
Iteration 118/1000 | Loss: 0.00002595
Iteration 119/1000 | Loss: 0.00002595
Iteration 120/1000 | Loss: 0.00002595
Iteration 121/1000 | Loss: 0.00002595
Iteration 122/1000 | Loss: 0.00002595
Iteration 123/1000 | Loss: 0.00002595
Iteration 124/1000 | Loss: 0.00002595
Iteration 125/1000 | Loss: 0.00002595
Iteration 126/1000 | Loss: 0.00002595
Iteration 127/1000 | Loss: 0.00002594
Iteration 128/1000 | Loss: 0.00002594
Iteration 129/1000 | Loss: 0.00002594
Iteration 130/1000 | Loss: 0.00002594
Iteration 131/1000 | Loss: 0.00002594
Iteration 132/1000 | Loss: 0.00002594
Iteration 133/1000 | Loss: 0.00002594
Iteration 134/1000 | Loss: 0.00002594
Iteration 135/1000 | Loss: 0.00002594
Iteration 136/1000 | Loss: 0.00002594
Iteration 137/1000 | Loss: 0.00002594
Iteration 138/1000 | Loss: 0.00002594
Iteration 139/1000 | Loss: 0.00002594
Iteration 140/1000 | Loss: 0.00002594
Iteration 141/1000 | Loss: 0.00002594
Iteration 142/1000 | Loss: 0.00002594
Iteration 143/1000 | Loss: 0.00002594
Iteration 144/1000 | Loss: 0.00002594
Iteration 145/1000 | Loss: 0.00002594
Iteration 146/1000 | Loss: 0.00002594
Iteration 147/1000 | Loss: 0.00002594
Iteration 148/1000 | Loss: 0.00002594
Iteration 149/1000 | Loss: 0.00002594
Iteration 150/1000 | Loss: 0.00002594
Iteration 151/1000 | Loss: 0.00002594
Iteration 152/1000 | Loss: 0.00002594
Iteration 153/1000 | Loss: 0.00002594
Iteration 154/1000 | Loss: 0.00002594
Iteration 155/1000 | Loss: 0.00002594
Iteration 156/1000 | Loss: 0.00002594
Iteration 157/1000 | Loss: 0.00002594
Iteration 158/1000 | Loss: 0.00002594
Iteration 159/1000 | Loss: 0.00002594
Iteration 160/1000 | Loss: 0.00002594
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 160. Stopping optimization.
Last 5 losses: [2.5939949409803376e-05, 2.5939949409803376e-05, 2.5939949409803376e-05, 2.5939949409803376e-05, 2.5939949409803376e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5939949409803376e-05

Optimization complete. Final v2v error: 4.2021708488464355 mm

Highest mean error: 4.477594375610352 mm for frame 182

Lowest mean error: 3.7902045249938965 mm for frame 209

Saving results

Total time: 44.45936179161072
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01121657
Iteration 2/25 | Loss: 0.00251078
Iteration 3/25 | Loss: 0.00161079
Iteration 4/25 | Loss: 0.00143234
Iteration 5/25 | Loss: 0.00154565
Iteration 6/25 | Loss: 0.00170613
Iteration 7/25 | Loss: 0.00160011
Iteration 8/25 | Loss: 0.00143013
Iteration 9/25 | Loss: 0.00131870
Iteration 10/25 | Loss: 0.00122643
Iteration 11/25 | Loss: 0.00116907
Iteration 12/25 | Loss: 0.00113032
Iteration 13/25 | Loss: 0.00109023
Iteration 14/25 | Loss: 0.00105639
Iteration 15/25 | Loss: 0.00104200
Iteration 16/25 | Loss: 0.00103458
Iteration 17/25 | Loss: 0.00101387
Iteration 18/25 | Loss: 0.00098257
Iteration 19/25 | Loss: 0.00098044
Iteration 20/25 | Loss: 0.00096260
Iteration 21/25 | Loss: 0.00096114
Iteration 22/25 | Loss: 0.00096919
Iteration 23/25 | Loss: 0.00096465
Iteration 24/25 | Loss: 0.00096766
Iteration 25/25 | Loss: 0.00096234

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15693521
Iteration 2/25 | Loss: 0.00210321
Iteration 3/25 | Loss: 0.00178361
Iteration 4/25 | Loss: 0.00178361
Iteration 5/25 | Loss: 0.00178361
Iteration 6/25 | Loss: 0.00178361
Iteration 7/25 | Loss: 0.00178361
Iteration 8/25 | Loss: 0.00178361
Iteration 9/25 | Loss: 0.00178361
Iteration 10/25 | Loss: 0.00178361
Iteration 11/25 | Loss: 0.00178361
Iteration 12/25 | Loss: 0.00178361
Iteration 13/25 | Loss: 0.00178361
Iteration 14/25 | Loss: 0.00178361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0017836092738434672, 0.0017836092738434672, 0.0017836092738434672, 0.0017836092738434672, 0.0017836092738434672]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0017836092738434672

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178361
Iteration 2/1000 | Loss: 0.00083238
Iteration 3/1000 | Loss: 0.00019869
Iteration 4/1000 | Loss: 0.00035017
Iteration 5/1000 | Loss: 0.00027376
Iteration 6/1000 | Loss: 0.00027307
Iteration 7/1000 | Loss: 0.00072579
Iteration 8/1000 | Loss: 0.00032034
Iteration 9/1000 | Loss: 0.00025761
Iteration 10/1000 | Loss: 0.00041888
Iteration 11/1000 | Loss: 0.00034435
Iteration 12/1000 | Loss: 0.00022031
Iteration 13/1000 | Loss: 0.00025339
Iteration 14/1000 | Loss: 0.00012125
Iteration 15/1000 | Loss: 0.00013400
Iteration 16/1000 | Loss: 0.00051562
Iteration 17/1000 | Loss: 0.00011288
Iteration 18/1000 | Loss: 0.00010335
Iteration 19/1000 | Loss: 0.00116956
Iteration 20/1000 | Loss: 0.00111602
Iteration 21/1000 | Loss: 0.00028832
Iteration 22/1000 | Loss: 0.00020853
Iteration 23/1000 | Loss: 0.00021068
Iteration 24/1000 | Loss: 0.00032377
Iteration 25/1000 | Loss: 0.00013799
Iteration 26/1000 | Loss: 0.00011818
Iteration 27/1000 | Loss: 0.00028998
Iteration 28/1000 | Loss: 0.00034724
Iteration 29/1000 | Loss: 0.00038624
Iteration 30/1000 | Loss: 0.00054345
Iteration 31/1000 | Loss: 0.00074951
Iteration 32/1000 | Loss: 0.00061455
Iteration 33/1000 | Loss: 0.00121758
Iteration 34/1000 | Loss: 0.00105534
Iteration 35/1000 | Loss: 0.00089246
Iteration 36/1000 | Loss: 0.00050722
Iteration 37/1000 | Loss: 0.00068226
Iteration 38/1000 | Loss: 0.00074205
Iteration 39/1000 | Loss: 0.00056726
Iteration 40/1000 | Loss: 0.00014103
Iteration 41/1000 | Loss: 0.00052658
Iteration 42/1000 | Loss: 0.00039205
Iteration 43/1000 | Loss: 0.00064470
Iteration 44/1000 | Loss: 0.00229698
Iteration 45/1000 | Loss: 0.00047408
Iteration 46/1000 | Loss: 0.00010935
Iteration 47/1000 | Loss: 0.00033558
Iteration 48/1000 | Loss: 0.00022705
Iteration 49/1000 | Loss: 0.00026652
Iteration 50/1000 | Loss: 0.00084279
Iteration 51/1000 | Loss: 0.00048482
Iteration 52/1000 | Loss: 0.00021644
Iteration 53/1000 | Loss: 0.00016722
Iteration 54/1000 | Loss: 0.00035323
Iteration 55/1000 | Loss: 0.00095048
Iteration 56/1000 | Loss: 0.00039253
Iteration 57/1000 | Loss: 0.00062431
Iteration 58/1000 | Loss: 0.00045051
Iteration 59/1000 | Loss: 0.00024793
Iteration 60/1000 | Loss: 0.00022091
Iteration 61/1000 | Loss: 0.00030762
Iteration 62/1000 | Loss: 0.00055959
Iteration 63/1000 | Loss: 0.00063493
Iteration 64/1000 | Loss: 0.00023931
Iteration 65/1000 | Loss: 0.00009222
Iteration 66/1000 | Loss: 0.00028528
Iteration 67/1000 | Loss: 0.00056384
Iteration 68/1000 | Loss: 0.00021663
Iteration 69/1000 | Loss: 0.00020419
Iteration 70/1000 | Loss: 0.00031464
Iteration 71/1000 | Loss: 0.00100518
Iteration 72/1000 | Loss: 0.00102575
Iteration 73/1000 | Loss: 0.00027543
Iteration 74/1000 | Loss: 0.00019475
Iteration 75/1000 | Loss: 0.00009231
Iteration 76/1000 | Loss: 0.00038651
Iteration 77/1000 | Loss: 0.00034299
Iteration 78/1000 | Loss: 0.00043769
Iteration 79/1000 | Loss: 0.00046143
Iteration 80/1000 | Loss: 0.00047310
Iteration 81/1000 | Loss: 0.00023161
Iteration 82/1000 | Loss: 0.00045684
Iteration 83/1000 | Loss: 0.00045416
Iteration 84/1000 | Loss: 0.00058839
Iteration 85/1000 | Loss: 0.00055581
Iteration 86/1000 | Loss: 0.00026794
Iteration 87/1000 | Loss: 0.00058242
Iteration 88/1000 | Loss: 0.00040688
Iteration 89/1000 | Loss: 0.00014504
Iteration 90/1000 | Loss: 0.00100473
Iteration 91/1000 | Loss: 0.00059774
Iteration 92/1000 | Loss: 0.00083204
Iteration 93/1000 | Loss: 0.00080435
Iteration 94/1000 | Loss: 0.00037234
Iteration 95/1000 | Loss: 0.00009804
Iteration 96/1000 | Loss: 0.00032188
Iteration 97/1000 | Loss: 0.00103469
Iteration 98/1000 | Loss: 0.00109208
Iteration 99/1000 | Loss: 0.00044173
Iteration 100/1000 | Loss: 0.00080443
Iteration 101/1000 | Loss: 0.00070982
Iteration 102/1000 | Loss: 0.00019704
Iteration 103/1000 | Loss: 0.00066104
Iteration 104/1000 | Loss: 0.00072512
Iteration 105/1000 | Loss: 0.00095725
Iteration 106/1000 | Loss: 0.00034925
Iteration 107/1000 | Loss: 0.00025798
Iteration 108/1000 | Loss: 0.00049831
Iteration 109/1000 | Loss: 0.00058172
Iteration 110/1000 | Loss: 0.00027143
Iteration 111/1000 | Loss: 0.00062760
Iteration 112/1000 | Loss: 0.00052495
Iteration 113/1000 | Loss: 0.00043915
Iteration 114/1000 | Loss: 0.00046230
Iteration 115/1000 | Loss: 0.00013559
Iteration 116/1000 | Loss: 0.00031341
Iteration 117/1000 | Loss: 0.00053955
Iteration 118/1000 | Loss: 0.00015662
Iteration 119/1000 | Loss: 0.00008581
Iteration 120/1000 | Loss: 0.00065243
Iteration 121/1000 | Loss: 0.00081317
Iteration 122/1000 | Loss: 0.00123650
Iteration 123/1000 | Loss: 0.00056682
Iteration 124/1000 | Loss: 0.00049265
Iteration 125/1000 | Loss: 0.00058829
Iteration 126/1000 | Loss: 0.00023183
Iteration 127/1000 | Loss: 0.00030048
Iteration 128/1000 | Loss: 0.00026940
Iteration 129/1000 | Loss: 0.00112603
Iteration 130/1000 | Loss: 0.00061735
Iteration 131/1000 | Loss: 0.00088303
Iteration 132/1000 | Loss: 0.00062692
Iteration 133/1000 | Loss: 0.00078750
Iteration 134/1000 | Loss: 0.00062171
Iteration 135/1000 | Loss: 0.00081003
Iteration 136/1000 | Loss: 0.00017363
Iteration 137/1000 | Loss: 0.00012367
Iteration 138/1000 | Loss: 0.00107277
Iteration 139/1000 | Loss: 0.00031560
Iteration 140/1000 | Loss: 0.00079590
Iteration 141/1000 | Loss: 0.00033901
Iteration 142/1000 | Loss: 0.00061232
Iteration 143/1000 | Loss: 0.00072403
Iteration 144/1000 | Loss: 0.00013684
Iteration 145/1000 | Loss: 0.00042037
Iteration 146/1000 | Loss: 0.00174130
Iteration 147/1000 | Loss: 0.00082257
Iteration 148/1000 | Loss: 0.00034328
Iteration 149/1000 | Loss: 0.00022093
Iteration 150/1000 | Loss: 0.00054180
Iteration 151/1000 | Loss: 0.00034976
Iteration 152/1000 | Loss: 0.00017858
Iteration 153/1000 | Loss: 0.00016397
Iteration 154/1000 | Loss: 0.00015014
Iteration 155/1000 | Loss: 0.00066961
Iteration 156/1000 | Loss: 0.00096103
Iteration 157/1000 | Loss: 0.00010165
Iteration 158/1000 | Loss: 0.00013205
Iteration 159/1000 | Loss: 0.00030381
Iteration 160/1000 | Loss: 0.00015173
Iteration 161/1000 | Loss: 0.00021666
Iteration 162/1000 | Loss: 0.00011163
Iteration 163/1000 | Loss: 0.00012335
Iteration 164/1000 | Loss: 0.00013789
Iteration 165/1000 | Loss: 0.00012514
Iteration 166/1000 | Loss: 0.00025978
Iteration 167/1000 | Loss: 0.00018948
Iteration 168/1000 | Loss: 0.00020688
Iteration 169/1000 | Loss: 0.00012481
Iteration 170/1000 | Loss: 0.00011555
Iteration 171/1000 | Loss: 0.00010918
Iteration 172/1000 | Loss: 0.00009441
Iteration 173/1000 | Loss: 0.00010168
Iteration 174/1000 | Loss: 0.00009985
Iteration 175/1000 | Loss: 0.00010494
Iteration 176/1000 | Loss: 0.00011750
Iteration 177/1000 | Loss: 0.00010619
Iteration 178/1000 | Loss: 0.00011654
Iteration 179/1000 | Loss: 0.00013196
Iteration 180/1000 | Loss: 0.00011904
Iteration 181/1000 | Loss: 0.00013217
Iteration 182/1000 | Loss: 0.00012545
Iteration 183/1000 | Loss: 0.00013083
Iteration 184/1000 | Loss: 0.00011981
Iteration 185/1000 | Loss: 0.00012621
Iteration 186/1000 | Loss: 0.00011658
Iteration 187/1000 | Loss: 0.00014022
Iteration 188/1000 | Loss: 0.00010440
Iteration 189/1000 | Loss: 0.00010198
Iteration 190/1000 | Loss: 0.00009931
Iteration 191/1000 | Loss: 0.00013027
Iteration 192/1000 | Loss: 0.00010710
Iteration 193/1000 | Loss: 0.00010845
Iteration 194/1000 | Loss: 0.00011871
Iteration 195/1000 | Loss: 0.00011211
Iteration 196/1000 | Loss: 0.00011742
Iteration 197/1000 | Loss: 0.00010366
Iteration 198/1000 | Loss: 0.00014428
Iteration 199/1000 | Loss: 0.00013081
Iteration 200/1000 | Loss: 0.00011389
Iteration 201/1000 | Loss: 0.00009661
Iteration 202/1000 | Loss: 0.00015378
Iteration 203/1000 | Loss: 0.00081746
Iteration 204/1000 | Loss: 0.00028117
Iteration 205/1000 | Loss: 0.00013826
Iteration 206/1000 | Loss: 0.00011762
Iteration 207/1000 | Loss: 0.00015506
Iteration 208/1000 | Loss: 0.00034233
Iteration 209/1000 | Loss: 0.00017852
Iteration 210/1000 | Loss: 0.00013586
Iteration 211/1000 | Loss: 0.00012990
Iteration 212/1000 | Loss: 0.00009592
Iteration 213/1000 | Loss: 0.00006778
Iteration 214/1000 | Loss: 0.00030041
Iteration 215/1000 | Loss: 0.00015753
Iteration 216/1000 | Loss: 0.00042129
Iteration 217/1000 | Loss: 0.00015084
Iteration 218/1000 | Loss: 0.00011376
Iteration 219/1000 | Loss: 0.00012621
Iteration 220/1000 | Loss: 0.00017332
Iteration 221/1000 | Loss: 0.00018719
Iteration 222/1000 | Loss: 0.00014999
Iteration 223/1000 | Loss: 0.00017523
Iteration 224/1000 | Loss: 0.00011446
Iteration 225/1000 | Loss: 0.00014978
Iteration 226/1000 | Loss: 0.00010186
Iteration 227/1000 | Loss: 0.00016426
Iteration 228/1000 | Loss: 0.00012436
Iteration 229/1000 | Loss: 0.00008450
Iteration 230/1000 | Loss: 0.00015665
Iteration 231/1000 | Loss: 0.00009331
Iteration 232/1000 | Loss: 0.00009748
Iteration 233/1000 | Loss: 0.00008384
Iteration 234/1000 | Loss: 0.00007352
Iteration 235/1000 | Loss: 0.00022068
Iteration 236/1000 | Loss: 0.00035013
Iteration 237/1000 | Loss: 0.00031600
Iteration 238/1000 | Loss: 0.00038777
Iteration 239/1000 | Loss: 0.00007501
Iteration 240/1000 | Loss: 0.00006829
Iteration 241/1000 | Loss: 0.00006813
Iteration 242/1000 | Loss: 0.00005269
Iteration 243/1000 | Loss: 0.00003693
Iteration 244/1000 | Loss: 0.00004786
Iteration 245/1000 | Loss: 0.00005711
Iteration 246/1000 | Loss: 0.00004838
Iteration 247/1000 | Loss: 0.00004311
Iteration 248/1000 | Loss: 0.00004339
Iteration 249/1000 | Loss: 0.00007474
Iteration 250/1000 | Loss: 0.00003362
Iteration 251/1000 | Loss: 0.00003709
Iteration 252/1000 | Loss: 0.00006345
Iteration 253/1000 | Loss: 0.00005207
Iteration 254/1000 | Loss: 0.00004688
Iteration 255/1000 | Loss: 0.00003965
Iteration 256/1000 | Loss: 0.00005318
Iteration 257/1000 | Loss: 0.00004202
Iteration 258/1000 | Loss: 0.00004903
Iteration 259/1000 | Loss: 0.00004166
Iteration 260/1000 | Loss: 0.00005928
Iteration 261/1000 | Loss: 0.00005272
Iteration 262/1000 | Loss: 0.00004932
Iteration 263/1000 | Loss: 0.00004771
Iteration 264/1000 | Loss: 0.00005992
Iteration 265/1000 | Loss: 0.00006410
Iteration 266/1000 | Loss: 0.00004919
Iteration 267/1000 | Loss: 0.00005545
Iteration 268/1000 | Loss: 0.00004415
Iteration 269/1000 | Loss: 0.00004907
Iteration 270/1000 | Loss: 0.00005764
Iteration 271/1000 | Loss: 0.00004593
Iteration 272/1000 | Loss: 0.00003890
Iteration 273/1000 | Loss: 0.00006251
Iteration 274/1000 | Loss: 0.00006660
Iteration 275/1000 | Loss: 0.00004801
Iteration 276/1000 | Loss: 0.00006218
Iteration 277/1000 | Loss: 0.00009873
Iteration 278/1000 | Loss: 0.00009142
Iteration 279/1000 | Loss: 0.00011395
Iteration 280/1000 | Loss: 0.00034986
Iteration 281/1000 | Loss: 0.00063684
Iteration 282/1000 | Loss: 0.00052069
Iteration 283/1000 | Loss: 0.00017645
Iteration 284/1000 | Loss: 0.00005907
Iteration 285/1000 | Loss: 0.00003581
Iteration 286/1000 | Loss: 0.00003093
Iteration 287/1000 | Loss: 0.00067668
Iteration 288/1000 | Loss: 0.00008386
Iteration 289/1000 | Loss: 0.00040259
Iteration 290/1000 | Loss: 0.00004427
Iteration 291/1000 | Loss: 0.00003407
Iteration 292/1000 | Loss: 0.00003021
Iteration 293/1000 | Loss: 0.00002583
Iteration 294/1000 | Loss: 0.00002345
Iteration 295/1000 | Loss: 0.00002211
Iteration 296/1000 | Loss: 0.00002111
Iteration 297/1000 | Loss: 0.00002066
Iteration 298/1000 | Loss: 0.00002036
Iteration 299/1000 | Loss: 0.00002013
Iteration 300/1000 | Loss: 0.00001995
Iteration 301/1000 | Loss: 0.00001990
Iteration 302/1000 | Loss: 0.00001984
Iteration 303/1000 | Loss: 0.00001979
Iteration 304/1000 | Loss: 0.00001978
Iteration 305/1000 | Loss: 0.00001977
Iteration 306/1000 | Loss: 0.00001976
Iteration 307/1000 | Loss: 0.00001976
Iteration 308/1000 | Loss: 0.00001974
Iteration 309/1000 | Loss: 0.00001974
Iteration 310/1000 | Loss: 0.00001974
Iteration 311/1000 | Loss: 0.00001973
Iteration 312/1000 | Loss: 0.00001970
Iteration 313/1000 | Loss: 0.00001970
Iteration 314/1000 | Loss: 0.00001968
Iteration 315/1000 | Loss: 0.00001968
Iteration 316/1000 | Loss: 0.00001966
Iteration 317/1000 | Loss: 0.00001965
Iteration 318/1000 | Loss: 0.00001960
Iteration 319/1000 | Loss: 0.00001953
Iteration 320/1000 | Loss: 0.00001949
Iteration 321/1000 | Loss: 0.00001948
Iteration 322/1000 | Loss: 0.00001948
Iteration 323/1000 | Loss: 0.00001946
Iteration 324/1000 | Loss: 0.00001945
Iteration 325/1000 | Loss: 0.00001935
Iteration 326/1000 | Loss: 0.00001935
Iteration 327/1000 | Loss: 0.00001934
Iteration 328/1000 | Loss: 0.00020633
Iteration 329/1000 | Loss: 0.00002640
Iteration 330/1000 | Loss: 0.00002371
Iteration 331/1000 | Loss: 0.00002180
Iteration 332/1000 | Loss: 0.00002093
Iteration 333/1000 | Loss: 0.00002053
Iteration 334/1000 | Loss: 0.00002018
Iteration 335/1000 | Loss: 0.00001981
Iteration 336/1000 | Loss: 0.00001950
Iteration 337/1000 | Loss: 0.00001929
Iteration 338/1000 | Loss: 0.00001921
Iteration 339/1000 | Loss: 0.00001907
Iteration 340/1000 | Loss: 0.00001903
Iteration 341/1000 | Loss: 0.00019553
Iteration 342/1000 | Loss: 0.00002607
Iteration 343/1000 | Loss: 0.00002282
Iteration 344/1000 | Loss: 0.00002094
Iteration 345/1000 | Loss: 0.00002028
Iteration 346/1000 | Loss: 0.00001992
Iteration 347/1000 | Loss: 0.00001947
Iteration 348/1000 | Loss: 0.00001909
Iteration 349/1000 | Loss: 0.00019832
Iteration 350/1000 | Loss: 0.00002530
Iteration 351/1000 | Loss: 0.00002296
Iteration 352/1000 | Loss: 0.00002128
Iteration 353/1000 | Loss: 0.00002049
Iteration 354/1000 | Loss: 0.00002010
Iteration 355/1000 | Loss: 0.00020165
Iteration 356/1000 | Loss: 0.00018561
Iteration 357/1000 | Loss: 0.00003185
Iteration 358/1000 | Loss: 0.00002709
Iteration 359/1000 | Loss: 0.00002367
Iteration 360/1000 | Loss: 0.00002242
Iteration 361/1000 | Loss: 0.00002180
Iteration 362/1000 | Loss: 0.00002108
Iteration 363/1000 | Loss: 0.00002046
Iteration 364/1000 | Loss: 0.00002003
Iteration 365/1000 | Loss: 0.00001981
Iteration 366/1000 | Loss: 0.00001963
Iteration 367/1000 | Loss: 0.00001953
Iteration 368/1000 | Loss: 0.00001950
Iteration 369/1000 | Loss: 0.00001950
Iteration 370/1000 | Loss: 0.00001950
Iteration 371/1000 | Loss: 0.00001949
Iteration 372/1000 | Loss: 0.00001945
Iteration 373/1000 | Loss: 0.00001934
Iteration 374/1000 | Loss: 0.00001917
Iteration 375/1000 | Loss: 0.00001896
Iteration 376/1000 | Loss: 0.00019074
Iteration 377/1000 | Loss: 0.00002513
Iteration 378/1000 | Loss: 0.00002278
Iteration 379/1000 | Loss: 0.00002103
Iteration 380/1000 | Loss: 0.00002027
Iteration 381/1000 | Loss: 0.00001989
Iteration 382/1000 | Loss: 0.00001966
Iteration 383/1000 | Loss: 0.00001922
Iteration 384/1000 | Loss: 0.00001893
Iteration 385/1000 | Loss: 0.00001887
Iteration 386/1000 | Loss: 0.00001878
Iteration 387/1000 | Loss: 0.00021183
Iteration 388/1000 | Loss: 0.00017643
Iteration 389/1000 | Loss: 0.00016220
Iteration 390/1000 | Loss: 0.00012124
Iteration 391/1000 | Loss: 0.00006352
Iteration 392/1000 | Loss: 0.00031964
Iteration 393/1000 | Loss: 0.00003736
Iteration 394/1000 | Loss: 0.00002773
Iteration 395/1000 | Loss: 0.00002351
Iteration 396/1000 | Loss: 0.00002142
Iteration 397/1000 | Loss: 0.00002072
Iteration 398/1000 | Loss: 0.00002003
Iteration 399/1000 | Loss: 0.00001942
Iteration 400/1000 | Loss: 0.00001900
Iteration 401/1000 | Loss: 0.00001875
Iteration 402/1000 | Loss: 0.00001865
Iteration 403/1000 | Loss: 0.00001859
Iteration 404/1000 | Loss: 0.00001854
Iteration 405/1000 | Loss: 0.00001852
Iteration 406/1000 | Loss: 0.00001851
Iteration 407/1000 | Loss: 0.00001850
Iteration 408/1000 | Loss: 0.00001849
Iteration 409/1000 | Loss: 0.00001848
Iteration 410/1000 | Loss: 0.00001846
Iteration 411/1000 | Loss: 0.00001845
Iteration 412/1000 | Loss: 0.00001845
Iteration 413/1000 | Loss: 0.00001844
Iteration 414/1000 | Loss: 0.00001844
Iteration 415/1000 | Loss: 0.00001843
Iteration 416/1000 | Loss: 0.00001842
Iteration 417/1000 | Loss: 0.00001840
Iteration 418/1000 | Loss: 0.00001836
Iteration 419/1000 | Loss: 0.00001829
Iteration 420/1000 | Loss: 0.00001822
Iteration 421/1000 | Loss: 0.00001819
Iteration 422/1000 | Loss: 0.00020096
Iteration 423/1000 | Loss: 0.00002367
Iteration 424/1000 | Loss: 0.00002159
Iteration 425/1000 | Loss: 0.00002030
Iteration 426/1000 | Loss: 0.00001957
Iteration 427/1000 | Loss: 0.00001926
Iteration 428/1000 | Loss: 0.00001892
Iteration 429/1000 | Loss: 0.00001857
Iteration 430/1000 | Loss: 0.00001834
Iteration 431/1000 | Loss: 0.00001833
Iteration 432/1000 | Loss: 0.00001822
Iteration 433/1000 | Loss: 0.00001818
Iteration 434/1000 | Loss: 0.00001811
Iteration 435/1000 | Loss: 0.00001806
Iteration 436/1000 | Loss: 0.00001806
Iteration 437/1000 | Loss: 0.00001805
Iteration 438/1000 | Loss: 0.00021033
Iteration 439/1000 | Loss: 0.00017963
Iteration 440/1000 | Loss: 0.00003070
Iteration 441/1000 | Loss: 0.00002627
Iteration 442/1000 | Loss: 0.00002267
Iteration 443/1000 | Loss: 0.00002137
Iteration 444/1000 | Loss: 0.00002065
Iteration 445/1000 | Loss: 0.00002012
Iteration 446/1000 | Loss: 0.00001951
Iteration 447/1000 | Loss: 0.00001903
Iteration 448/1000 | Loss: 0.00001893
Iteration 449/1000 | Loss: 0.00001878
Iteration 450/1000 | Loss: 0.00001873
Iteration 451/1000 | Loss: 0.00001866
Iteration 452/1000 | Loss: 0.00001862
Iteration 453/1000 | Loss: 0.00001862
Iteration 454/1000 | Loss: 0.00001861
Iteration 455/1000 | Loss: 0.00001861
Iteration 456/1000 | Loss: 0.00001861
Iteration 457/1000 | Loss: 0.00001860
Iteration 458/1000 | Loss: 0.00001860
Iteration 459/1000 | Loss: 0.00001855
Iteration 460/1000 | Loss: 0.00001845
Iteration 461/1000 | Loss: 0.00001845
Iteration 462/1000 | Loss: 0.00001845
Iteration 463/1000 | Loss: 0.00001844
Iteration 464/1000 | Loss: 0.00001844
Iteration 465/1000 | Loss: 0.00001844
Iteration 466/1000 | Loss: 0.00001843
Iteration 467/1000 | Loss: 0.00001843
Iteration 468/1000 | Loss: 0.00001842
Iteration 469/1000 | Loss: 0.00001842
Iteration 470/1000 | Loss: 0.00001840
Iteration 471/1000 | Loss: 0.00001838
Iteration 472/1000 | Loss: 0.00001838
Iteration 473/1000 | Loss: 0.00001833
Iteration 474/1000 | Loss: 0.00001832
Iteration 475/1000 | Loss: 0.00001830
Iteration 476/1000 | Loss: 0.00001829
Iteration 477/1000 | Loss: 0.00001829
Iteration 478/1000 | Loss: 0.00001828
Iteration 479/1000 | Loss: 0.00018031
Iteration 480/1000 | Loss: 0.00002382
Iteration 481/1000 | Loss: 0.00002171
Iteration 482/1000 | Loss: 0.00002007
Iteration 483/1000 | Loss: 0.00001961
Iteration 484/1000 | Loss: 0.00001920
Iteration 485/1000 | Loss: 0.00001893
Iteration 486/1000 | Loss: 0.00001860
Iteration 487/1000 | Loss: 0.00001831
Iteration 488/1000 | Loss: 0.00001816
Iteration 489/1000 | Loss: 0.00001813
Iteration 490/1000 | Loss: 0.00020859
Iteration 491/1000 | Loss: 0.00020858
Iteration 492/1000 | Loss: 0.00035153
Iteration 493/1000 | Loss: 0.00022051
Iteration 494/1000 | Loss: 0.00003000
Iteration 495/1000 | Loss: 0.00002455
Iteration 496/1000 | Loss: 0.00002223
Iteration 497/1000 | Loss: 0.00002085
Iteration 498/1000 | Loss: 0.00001997
Iteration 499/1000 | Loss: 0.00001904
Iteration 500/1000 | Loss: 0.00001832
Iteration 501/1000 | Loss: 0.00001797
Iteration 502/1000 | Loss: 0.00001787
Iteration 503/1000 | Loss: 0.00001773
Iteration 504/1000 | Loss: 0.00001772
Iteration 505/1000 | Loss: 0.00001769
Iteration 506/1000 | Loss: 0.00001767
Iteration 507/1000 | Loss: 0.00001766
Iteration 508/1000 | Loss: 0.00001766
Iteration 509/1000 | Loss: 0.00001766
Iteration 510/1000 | Loss: 0.00001766
Iteration 511/1000 | Loss: 0.00001766
Iteration 512/1000 | Loss: 0.00001766
Iteration 513/1000 | Loss: 0.00001766
Iteration 514/1000 | Loss: 0.00001766
Iteration 515/1000 | Loss: 0.00001765
Iteration 516/1000 | Loss: 0.00001765
Iteration 517/1000 | Loss: 0.00001764
Iteration 518/1000 | Loss: 0.00001764
Iteration 519/1000 | Loss: 0.00001763
Iteration 520/1000 | Loss: 0.00001763
Iteration 521/1000 | Loss: 0.00001763
Iteration 522/1000 | Loss: 0.00001763
Iteration 523/1000 | Loss: 0.00001763
Iteration 524/1000 | Loss: 0.00001763
Iteration 525/1000 | Loss: 0.00001763
Iteration 526/1000 | Loss: 0.00001762
Iteration 527/1000 | Loss: 0.00001762
Iteration 528/1000 | Loss: 0.00001762
Iteration 529/1000 | Loss: 0.00001762
Iteration 530/1000 | Loss: 0.00001762
Iteration 531/1000 | Loss: 0.00001762
Iteration 532/1000 | Loss: 0.00001761
Iteration 533/1000 | Loss: 0.00001761
Iteration 534/1000 | Loss: 0.00001761
Iteration 535/1000 | Loss: 0.00001761
Iteration 536/1000 | Loss: 0.00001761
Iteration 537/1000 | Loss: 0.00001761
Iteration 538/1000 | Loss: 0.00001761
Iteration 539/1000 | Loss: 0.00001760
Iteration 540/1000 | Loss: 0.00001760
Iteration 541/1000 | Loss: 0.00001760
Iteration 542/1000 | Loss: 0.00001760
Iteration 543/1000 | Loss: 0.00001760
Iteration 544/1000 | Loss: 0.00001760
Iteration 545/1000 | Loss: 0.00001760
Iteration 546/1000 | Loss: 0.00001760
Iteration 547/1000 | Loss: 0.00001760
Iteration 548/1000 | Loss: 0.00001759
Iteration 549/1000 | Loss: 0.00001759
Iteration 550/1000 | Loss: 0.00001759
Iteration 551/1000 | Loss: 0.00001759
Iteration 552/1000 | Loss: 0.00001759
Iteration 553/1000 | Loss: 0.00001759
Iteration 554/1000 | Loss: 0.00001758
Iteration 555/1000 | Loss: 0.00001758
Iteration 556/1000 | Loss: 0.00001758
Iteration 557/1000 | Loss: 0.00001758
Iteration 558/1000 | Loss: 0.00001758
Iteration 559/1000 | Loss: 0.00001757
Iteration 560/1000 | Loss: 0.00001757
Iteration 561/1000 | Loss: 0.00001757
Iteration 562/1000 | Loss: 0.00001757
Iteration 563/1000 | Loss: 0.00001757
Iteration 564/1000 | Loss: 0.00001757
Iteration 565/1000 | Loss: 0.00001757
Iteration 566/1000 | Loss: 0.00001757
Iteration 567/1000 | Loss: 0.00001757
Iteration 568/1000 | Loss: 0.00001757
Iteration 569/1000 | Loss: 0.00001757
Iteration 570/1000 | Loss: 0.00001757
Iteration 571/1000 | Loss: 0.00001756
Iteration 572/1000 | Loss: 0.00001756
Iteration 573/1000 | Loss: 0.00001756
Iteration 574/1000 | Loss: 0.00001756
Iteration 575/1000 | Loss: 0.00001756
Iteration 576/1000 | Loss: 0.00001756
Iteration 577/1000 | Loss: 0.00001756
Iteration 578/1000 | Loss: 0.00001756
Iteration 579/1000 | Loss: 0.00001756
Iteration 580/1000 | Loss: 0.00001756
Iteration 581/1000 | Loss: 0.00001756
Iteration 582/1000 | Loss: 0.00001755
Iteration 583/1000 | Loss: 0.00001755
Iteration 584/1000 | Loss: 0.00001755
Iteration 585/1000 | Loss: 0.00001755
Iteration 586/1000 | Loss: 0.00001755
Iteration 587/1000 | Loss: 0.00001754
Iteration 588/1000 | Loss: 0.00001754
Iteration 589/1000 | Loss: 0.00001754
Iteration 590/1000 | Loss: 0.00001754
Iteration 591/1000 | Loss: 0.00001754
Iteration 592/1000 | Loss: 0.00001754
Iteration 593/1000 | Loss: 0.00001753
Iteration 594/1000 | Loss: 0.00001753
Iteration 595/1000 | Loss: 0.00001753
Iteration 596/1000 | Loss: 0.00021033
Iteration 597/1000 | Loss: 0.00002296
Iteration 598/1000 | Loss: 0.00002072
Iteration 599/1000 | Loss: 0.00001965
Iteration 600/1000 | Loss: 0.00001920
Iteration 601/1000 | Loss: 0.00001887
Iteration 602/1000 | Loss: 0.00001864
Iteration 603/1000 | Loss: 0.00001831
Iteration 604/1000 | Loss: 0.00001809
Iteration 605/1000 | Loss: 0.00001803
Iteration 606/1000 | Loss: 0.00001802
Iteration 607/1000 | Loss: 0.00001796
Iteration 608/1000 | Loss: 0.00001792
Iteration 609/1000 | Loss: 0.00001792
Iteration 610/1000 | Loss: 0.00001792
Iteration 611/1000 | Loss: 0.00001792
Iteration 612/1000 | Loss: 0.00001792
Iteration 613/1000 | Loss: 0.00001792
Iteration 614/1000 | Loss: 0.00001791
Iteration 615/1000 | Loss: 0.00001790
Iteration 616/1000 | Loss: 0.00001788
Iteration 617/1000 | Loss: 0.00001788
Iteration 618/1000 | Loss: 0.00001788
Iteration 619/1000 | Loss: 0.00001788
Iteration 620/1000 | Loss: 0.00001788
Iteration 621/1000 | Loss: 0.00001788
Iteration 622/1000 | Loss: 0.00001788
Iteration 623/1000 | Loss: 0.00001788
Iteration 624/1000 | Loss: 0.00001787
Iteration 625/1000 | Loss: 0.00001787
Iteration 626/1000 | Loss: 0.00001787
Iteration 627/1000 | Loss: 0.00001787
Iteration 628/1000 | Loss: 0.00001787
Iteration 629/1000 | Loss: 0.00001786
Iteration 630/1000 | Loss: 0.00001786
Iteration 631/1000 | Loss: 0.00001786
Iteration 632/1000 | Loss: 0.00001785
Iteration 633/1000 | Loss: 0.00001785
Iteration 634/1000 | Loss: 0.00001785
Iteration 635/1000 | Loss: 0.00001785
Iteration 636/1000 | Loss: 0.00001784
Iteration 637/1000 | Loss: 0.00001784
Iteration 638/1000 | Loss: 0.00001784
Iteration 639/1000 | Loss: 0.00001784
Iteration 640/1000 | Loss: 0.00001784
Iteration 641/1000 | Loss: 0.00001783
Iteration 642/1000 | Loss: 0.00001783
Iteration 643/1000 | Loss: 0.00001783
Iteration 644/1000 | Loss: 0.00001783
Iteration 645/1000 | Loss: 0.00001783
Iteration 646/1000 | Loss: 0.00001783
Iteration 647/1000 | Loss: 0.00001783
Iteration 648/1000 | Loss: 0.00001782
Iteration 649/1000 | Loss: 0.00001782
Iteration 650/1000 | Loss: 0.00001782
Iteration 651/1000 | Loss: 0.00001782
Iteration 652/1000 | Loss: 0.00001782
Iteration 653/1000 | Loss: 0.00001782
Iteration 654/1000 | Loss: 0.00001782
Iteration 655/1000 | Loss: 0.00001782
Iteration 656/1000 | Loss: 0.00001782
Iteration 657/1000 | Loss: 0.00001782
Iteration 658/1000 | Loss: 0.00001782
Iteration 659/1000 | Loss: 0.00001781
Iteration 660/1000 | Loss: 0.00001781
Iteration 661/1000 | Loss: 0.00001781
Iteration 662/1000 | Loss: 0.00001780
Iteration 663/1000 | Loss: 0.00001779
Iteration 664/1000 | Loss: 0.00001779
Iteration 665/1000 | Loss: 0.00001779
Iteration 666/1000 | Loss: 0.00001778
Iteration 667/1000 | Loss: 0.00001778
Iteration 668/1000 | Loss: 0.00001777
Iteration 669/1000 | Loss: 0.00001777
Iteration 670/1000 | Loss: 0.00001777
Iteration 671/1000 | Loss: 0.00001777
Iteration 672/1000 | Loss: 0.00001777
Iteration 673/1000 | Loss: 0.00001777
Iteration 674/1000 | Loss: 0.00001777
Iteration 675/1000 | Loss: 0.00001777
Iteration 676/1000 | Loss: 0.00001777
Iteration 677/1000 | Loss: 0.00001777
Iteration 678/1000 | Loss: 0.00001777
Iteration 679/1000 | Loss: 0.00001777
Iteration 680/1000 | Loss: 0.00001777
Iteration 681/1000 | Loss: 0.00001776
Iteration 682/1000 | Loss: 0.00001776
Iteration 683/1000 | Loss: 0.00001776
Iteration 684/1000 | Loss: 0.00001775
Iteration 685/1000 | Loss: 0.00001775
Iteration 686/1000 | Loss: 0.00001775
Iteration 687/1000 | Loss: 0.00001775
Iteration 688/1000 | Loss: 0.00001775
Iteration 689/1000 | Loss: 0.00001775
Iteration 690/1000 | Loss: 0.00001775
Iteration 691/1000 | Loss: 0.00001775
Iteration 692/1000 | Loss: 0.00001775
Iteration 693/1000 | Loss: 0.00001775
Iteration 694/1000 | Loss: 0.00001774
Iteration 695/1000 | Loss: 0.00001774
Iteration 696/1000 | Loss: 0.00001774
Iteration 697/1000 | Loss: 0.00001774
Iteration 698/1000 | Loss: 0.00001774
Iteration 699/1000 | Loss: 0.00001773
Iteration 700/1000 | Loss: 0.00001773
Iteration 701/1000 | Loss: 0.00001773
Iteration 702/1000 | Loss: 0.00001773
Iteration 703/1000 | Loss: 0.00001773
Iteration 704/1000 | Loss: 0.00001773
Iteration 705/1000 | Loss: 0.00001772
Iteration 706/1000 | Loss: 0.00001772
Iteration 707/1000 | Loss: 0.00001772
Iteration 708/1000 | Loss: 0.00001771
Iteration 709/1000 | Loss: 0.00001771
Iteration 710/1000 | Loss: 0.00001771
Iteration 711/1000 | Loss: 0.00001771
Iteration 712/1000 | Loss: 0.00001770
Iteration 713/1000 | Loss: 0.00001770
Iteration 714/1000 | Loss: 0.00001770
Iteration 715/1000 | Loss: 0.00001768
Iteration 716/1000 | Loss: 0.00001767
Iteration 717/1000 | Loss: 0.00001767
Iteration 718/1000 | Loss: 0.00001767
Iteration 719/1000 | Loss: 0.00001767
Iteration 720/1000 | Loss: 0.00001764
Iteration 721/1000 | Loss: 0.00001764
Iteration 722/1000 | Loss: 0.00001763
Iteration 723/1000 | Loss: 0.00001763
Iteration 724/1000 | Loss: 0.00001763
Iteration 725/1000 | Loss: 0.00001763
Iteration 726/1000 | Loss: 0.00001763
Iteration 727/1000 | Loss: 0.00001763
Iteration 728/1000 | Loss: 0.00001763
Iteration 729/1000 | Loss: 0.00001763
Iteration 730/1000 | Loss: 0.00001763
Iteration 731/1000 | Loss: 0.00001761
Iteration 732/1000 | Loss: 0.00001760
Iteration 733/1000 | Loss: 0.00001757
Iteration 734/1000 | Loss: 0.00001757
Iteration 735/1000 | Loss: 0.00001747
Iteration 736/1000 | Loss: 0.00001744
Iteration 737/1000 | Loss: 0.00001744
Iteration 738/1000 | Loss: 0.00001744
Iteration 739/1000 | Loss: 0.00001744
Iteration 740/1000 | Loss: 0.00001743
Iteration 741/1000 | Loss: 0.00001742
Iteration 742/1000 | Loss: 0.00001742
Iteration 743/1000 | Loss: 0.00001741
Iteration 744/1000 | Loss: 0.00001739
Iteration 745/1000 | Loss: 0.00001737
Iteration 746/1000 | Loss: 0.00001737
Iteration 747/1000 | Loss: 0.00001736
Iteration 748/1000 | Loss: 0.00001736
Iteration 749/1000 | Loss: 0.00001733
Iteration 750/1000 | Loss: 0.00001731
Iteration 751/1000 | Loss: 0.00001731
Iteration 752/1000 | Loss: 0.00001729
Iteration 753/1000 | Loss: 0.00001724
Iteration 754/1000 | Loss: 0.00001724
Iteration 755/1000 | Loss: 0.00020189
Iteration 756/1000 | Loss: 0.00002404
Iteration 757/1000 | Loss: 0.00002146
Iteration 758/1000 | Loss: 0.00001979
Iteration 759/1000 | Loss: 0.00001905
Iteration 760/1000 | Loss: 0.00001861
Iteration 761/1000 | Loss: 0.00001824
Iteration 762/1000 | Loss: 0.00001799
Iteration 763/1000 | Loss: 0.00001776
Iteration 764/1000 | Loss: 0.00001772
Iteration 765/1000 | Loss: 0.00001772
Iteration 766/1000 | Loss: 0.00001771
Iteration 767/1000 | Loss: 0.00001771
Iteration 768/1000 | Loss: 0.00001768
Iteration 769/1000 | Loss: 0.00001767
Iteration 770/1000 | Loss: 0.00001767
Iteration 771/1000 | Loss: 0.00001766
Iteration 772/1000 | Loss: 0.00001766
Iteration 773/1000 | Loss: 0.00001765
Iteration 774/1000 | Loss: 0.00001761
Iteration 775/1000 | Loss: 0.00001760
Iteration 776/1000 | Loss: 0.00001759
Iteration 777/1000 | Loss: 0.00001759
Iteration 778/1000 | Loss: 0.00001758
Iteration 779/1000 | Loss: 0.00001758
Iteration 780/1000 | Loss: 0.00001758
Iteration 781/1000 | Loss: 0.00001758
Iteration 782/1000 | Loss: 0.00001758
Iteration 783/1000 | Loss: 0.00001758
Iteration 784/1000 | Loss: 0.00001756
Iteration 785/1000 | Loss: 0.00001755
Iteration 786/1000 | Loss: 0.00001755
Iteration 787/1000 | Loss: 0.00001755
Iteration 788/1000 | Loss: 0.00001755
Iteration 789/1000 | Loss: 0.00001754
Iteration 790/1000 | Loss: 0.00001754
Iteration 791/1000 | Loss: 0.00001754
Iteration 792/1000 | Loss: 0.00001754
Iteration 793/1000 | Loss: 0.00001754
Iteration 794/1000 | Loss: 0.00001754
Iteration 795/1000 | Loss: 0.00001753
Iteration 796/1000 | Loss: 0.00001753
Iteration 797/1000 | Loss: 0.00001753
Iteration 798/1000 | Loss: 0.00001753
Iteration 799/1000 | Loss: 0.00001753
Iteration 800/1000 | Loss: 0.00001753
Iteration 801/1000 | Loss: 0.00001753
Iteration 802/1000 | Loss: 0.00001753
Iteration 803/1000 | Loss: 0.00001753
Iteration 804/1000 | Loss: 0.00001752
Iteration 805/1000 | Loss: 0.00001752
Iteration 806/1000 | Loss: 0.00001752
Iteration 807/1000 | Loss: 0.00001752
Iteration 808/1000 | Loss: 0.00001752
Iteration 809/1000 | Loss: 0.00001752
Iteration 810/1000 | Loss: 0.00001752
Iteration 811/1000 | Loss: 0.00001752
Iteration 812/1000 | Loss: 0.00001752
Iteration 813/1000 | Loss: 0.00001752
Iteration 814/1000 | Loss: 0.00001752
Iteration 815/1000 | Loss: 0.00001752
Iteration 816/1000 | Loss: 0.00001751
Iteration 817/1000 | Loss: 0.00001751
Iteration 818/1000 | Loss: 0.00001751
Iteration 819/1000 | Loss: 0.00001750
Iteration 820/1000 | Loss: 0.00001750
Iteration 821/1000 | Loss: 0.00001750
Iteration 822/1000 | Loss: 0.00001749
Iteration 823/1000 | Loss: 0.00001749
Iteration 824/1000 | Loss: 0.00001748
Iteration 825/1000 | Loss: 0.00001748
Iteration 826/1000 | Loss: 0.00001748
Iteration 827/1000 | Loss: 0.00001747
Iteration 828/1000 | Loss: 0.00001747
Iteration 829/1000 | Loss: 0.00001747
Iteration 830/1000 | Loss: 0.00001747
Iteration 831/1000 | Loss: 0.00001746
Iteration 832/1000 | Loss: 0.00001746
Iteration 833/1000 | Loss: 0.00001746
Iteration 834/1000 | Loss: 0.00001746
Iteration 835/1000 | Loss: 0.00001746
Iteration 836/1000 | Loss: 0.00001746
Iteration 837/1000 | Loss: 0.00001745
Iteration 838/1000 | Loss: 0.00001745
Iteration 839/1000 | Loss: 0.00001745
Iteration 840/1000 | Loss: 0.00001745
Iteration 841/1000 | Loss: 0.00001745
Iteration 842/1000 | Loss: 0.00001745
Iteration 843/1000 | Loss: 0.00001745
Iteration 844/1000 | Loss: 0.00001745
Iteration 845/1000 | Loss: 0.00001745
Iteration 846/1000 | Loss: 0.00001745
Iteration 847/1000 | Loss: 0.00001745
Iteration 848/1000 | Loss: 0.00001745
Iteration 849/1000 | Loss: 0.00001745
Iteration 850/1000 | Loss: 0.00001745
Iteration 851/1000 | Loss: 0.00001745
Iteration 852/1000 | Loss: 0.00001745
Iteration 853/1000 | Loss: 0.00001745
Iteration 854/1000 | Loss: 0.00001745
Iteration 855/1000 | Loss: 0.00001745
Iteration 856/1000 | Loss: 0.00001745
Iteration 857/1000 | Loss: 0.00001745
Iteration 858/1000 | Loss: 0.00001745
Iteration 859/1000 | Loss: 0.00001745
Iteration 860/1000 | Loss: 0.00001745
Iteration 861/1000 | Loss: 0.00001745
Iteration 862/1000 | Loss: 0.00001745
Iteration 863/1000 | Loss: 0.00001745
Iteration 864/1000 | Loss: 0.00001745
Iteration 865/1000 | Loss: 0.00001745
Iteration 866/1000 | Loss: 0.00001745
Iteration 867/1000 | Loss: 0.00001745
Iteration 868/1000 | Loss: 0.00001745
Iteration 869/1000 | Loss: 0.00001745
Iteration 870/1000 | Loss: 0.00001745
Iteration 871/1000 | Loss: 0.00001745
Iteration 872/1000 | Loss: 0.00001745
Iteration 873/1000 | Loss: 0.00001745
Iteration 874/1000 | Loss: 0.00001745
Iteration 875/1000 | Loss: 0.00001745
Iteration 876/1000 | Loss: 0.00001745
Iteration 877/1000 | Loss: 0.00001745
Iteration 878/1000 | Loss: 0.00001745
Iteration 879/1000 | Loss: 0.00001745
Iteration 880/1000 | Loss: 0.00001745
Iteration 881/1000 | Loss: 0.00001745
Iteration 882/1000 | Loss: 0.00001745
Iteration 883/1000 | Loss: 0.00001745
Iteration 884/1000 | Loss: 0.00001745
Iteration 885/1000 | Loss: 0.00001745
Iteration 886/1000 | Loss: 0.00001745
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 886. Stopping optimization.
Last 5 losses: [1.7446534911869094e-05, 1.7446534911869094e-05, 1.7446534911869094e-05, 1.7446534911869094e-05, 1.7446534911869094e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7446534911869094e-05

Optimization complete. Final v2v error: 3.4297406673431396 mm

Highest mean error: 6.884971618652344 mm for frame 9

Lowest mean error: 3.1829652786254883 mm for frame 136

Saving results

Total time: 732.0616631507874
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00455373
Iteration 2/25 | Loss: 0.00124258
Iteration 3/25 | Loss: 0.00092780
Iteration 4/25 | Loss: 0.00087791
Iteration 5/25 | Loss: 0.00085980
Iteration 6/25 | Loss: 0.00085631
Iteration 7/25 | Loss: 0.00085511
Iteration 8/25 | Loss: 0.00085475
Iteration 9/25 | Loss: 0.00085475
Iteration 10/25 | Loss: 0.00085475
Iteration 11/25 | Loss: 0.00085475
Iteration 12/25 | Loss: 0.00085475
Iteration 13/25 | Loss: 0.00085475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008547509205527604, 0.0008547509205527604, 0.0008547509205527604, 0.0008547509205527604, 0.0008547509205527604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008547509205527604

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32871211
Iteration 2/25 | Loss: 0.00106477
Iteration 3/25 | Loss: 0.00106475
Iteration 4/25 | Loss: 0.00106475
Iteration 5/25 | Loss: 0.00106475
Iteration 6/25 | Loss: 0.00106475
Iteration 7/25 | Loss: 0.00106475
Iteration 8/25 | Loss: 0.00106475
Iteration 9/25 | Loss: 0.00106474
Iteration 10/25 | Loss: 0.00106474
Iteration 11/25 | Loss: 0.00106474
Iteration 12/25 | Loss: 0.00106474
Iteration 13/25 | Loss: 0.00106474
Iteration 14/25 | Loss: 0.00106474
Iteration 15/25 | Loss: 0.00106474
Iteration 16/25 | Loss: 0.00106474
Iteration 17/25 | Loss: 0.00106474
Iteration 18/25 | Loss: 0.00106474
Iteration 19/25 | Loss: 0.00106474
Iteration 20/25 | Loss: 0.00106474
Iteration 21/25 | Loss: 0.00106474
Iteration 22/25 | Loss: 0.00106474
Iteration 23/25 | Loss: 0.00106474
Iteration 24/25 | Loss: 0.00106474
Iteration 25/25 | Loss: 0.00106474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106474
Iteration 2/1000 | Loss: 0.00007584
Iteration 3/1000 | Loss: 0.00004988
Iteration 4/1000 | Loss: 0.00004027
Iteration 5/1000 | Loss: 0.00003788
Iteration 6/1000 | Loss: 0.00003640
Iteration 7/1000 | Loss: 0.00003535
Iteration 8/1000 | Loss: 0.00003471
Iteration 9/1000 | Loss: 0.00003413
Iteration 10/1000 | Loss: 0.00003363
Iteration 11/1000 | Loss: 0.00003331
Iteration 12/1000 | Loss: 0.00003306
Iteration 13/1000 | Loss: 0.00003281
Iteration 14/1000 | Loss: 0.00003260
Iteration 15/1000 | Loss: 0.00003250
Iteration 16/1000 | Loss: 0.00003230
Iteration 17/1000 | Loss: 0.00003223
Iteration 18/1000 | Loss: 0.00003223
Iteration 19/1000 | Loss: 0.00003222
Iteration 20/1000 | Loss: 0.00003218
Iteration 21/1000 | Loss: 0.00003212
Iteration 22/1000 | Loss: 0.00003207
Iteration 23/1000 | Loss: 0.00003198
Iteration 24/1000 | Loss: 0.00003198
Iteration 25/1000 | Loss: 0.00003196
Iteration 26/1000 | Loss: 0.00003195
Iteration 27/1000 | Loss: 0.00003195
Iteration 28/1000 | Loss: 0.00003193
Iteration 29/1000 | Loss: 0.00003191
Iteration 30/1000 | Loss: 0.00003188
Iteration 31/1000 | Loss: 0.00003184
Iteration 32/1000 | Loss: 0.00003184
Iteration 33/1000 | Loss: 0.00003183
Iteration 34/1000 | Loss: 0.00003181
Iteration 35/1000 | Loss: 0.00003178
Iteration 36/1000 | Loss: 0.00003177
Iteration 37/1000 | Loss: 0.00003177
Iteration 38/1000 | Loss: 0.00003176
Iteration 39/1000 | Loss: 0.00003175
Iteration 40/1000 | Loss: 0.00003175
Iteration 41/1000 | Loss: 0.00003174
Iteration 42/1000 | Loss: 0.00003173
Iteration 43/1000 | Loss: 0.00003173
Iteration 44/1000 | Loss: 0.00003172
Iteration 45/1000 | Loss: 0.00003172
Iteration 46/1000 | Loss: 0.00003171
Iteration 47/1000 | Loss: 0.00003171
Iteration 48/1000 | Loss: 0.00003170
Iteration 49/1000 | Loss: 0.00003170
Iteration 50/1000 | Loss: 0.00003170
Iteration 51/1000 | Loss: 0.00003169
Iteration 52/1000 | Loss: 0.00003168
Iteration 53/1000 | Loss: 0.00003168
Iteration 54/1000 | Loss: 0.00003168
Iteration 55/1000 | Loss: 0.00003167
Iteration 56/1000 | Loss: 0.00003167
Iteration 57/1000 | Loss: 0.00003167
Iteration 58/1000 | Loss: 0.00003166
Iteration 59/1000 | Loss: 0.00003166
Iteration 60/1000 | Loss: 0.00003166
Iteration 61/1000 | Loss: 0.00003165
Iteration 62/1000 | Loss: 0.00003165
Iteration 63/1000 | Loss: 0.00003164
Iteration 64/1000 | Loss: 0.00003164
Iteration 65/1000 | Loss: 0.00003164
Iteration 66/1000 | Loss: 0.00003163
Iteration 67/1000 | Loss: 0.00003163
Iteration 68/1000 | Loss: 0.00003163
Iteration 69/1000 | Loss: 0.00003162
Iteration 70/1000 | Loss: 0.00003162
Iteration 71/1000 | Loss: 0.00003162
Iteration 72/1000 | Loss: 0.00003162
Iteration 73/1000 | Loss: 0.00003161
Iteration 74/1000 | Loss: 0.00003161
Iteration 75/1000 | Loss: 0.00003161
Iteration 76/1000 | Loss: 0.00003161
Iteration 77/1000 | Loss: 0.00003161
Iteration 78/1000 | Loss: 0.00003161
Iteration 79/1000 | Loss: 0.00003161
Iteration 80/1000 | Loss: 0.00003160
Iteration 81/1000 | Loss: 0.00003160
Iteration 82/1000 | Loss: 0.00003160
Iteration 83/1000 | Loss: 0.00003160
Iteration 84/1000 | Loss: 0.00003160
Iteration 85/1000 | Loss: 0.00003159
Iteration 86/1000 | Loss: 0.00003159
Iteration 87/1000 | Loss: 0.00003159
Iteration 88/1000 | Loss: 0.00003158
Iteration 89/1000 | Loss: 0.00003158
Iteration 90/1000 | Loss: 0.00003158
Iteration 91/1000 | Loss: 0.00003158
Iteration 92/1000 | Loss: 0.00003157
Iteration 93/1000 | Loss: 0.00003157
Iteration 94/1000 | Loss: 0.00003157
Iteration 95/1000 | Loss: 0.00003157
Iteration 96/1000 | Loss: 0.00003157
Iteration 97/1000 | Loss: 0.00003157
Iteration 98/1000 | Loss: 0.00003156
Iteration 99/1000 | Loss: 0.00003156
Iteration 100/1000 | Loss: 0.00003156
Iteration 101/1000 | Loss: 0.00003155
Iteration 102/1000 | Loss: 0.00003155
Iteration 103/1000 | Loss: 0.00003155
Iteration 104/1000 | Loss: 0.00003155
Iteration 105/1000 | Loss: 0.00003155
Iteration 106/1000 | Loss: 0.00003154
Iteration 107/1000 | Loss: 0.00003154
Iteration 108/1000 | Loss: 0.00003154
Iteration 109/1000 | Loss: 0.00003154
Iteration 110/1000 | Loss: 0.00003154
Iteration 111/1000 | Loss: 0.00003154
Iteration 112/1000 | Loss: 0.00003154
Iteration 113/1000 | Loss: 0.00003154
Iteration 114/1000 | Loss: 0.00003154
Iteration 115/1000 | Loss: 0.00003154
Iteration 116/1000 | Loss: 0.00003154
Iteration 117/1000 | Loss: 0.00003154
Iteration 118/1000 | Loss: 0.00003153
Iteration 119/1000 | Loss: 0.00003153
Iteration 120/1000 | Loss: 0.00003153
Iteration 121/1000 | Loss: 0.00003153
Iteration 122/1000 | Loss: 0.00003153
Iteration 123/1000 | Loss: 0.00003153
Iteration 124/1000 | Loss: 0.00003153
Iteration 125/1000 | Loss: 0.00003152
Iteration 126/1000 | Loss: 0.00003152
Iteration 127/1000 | Loss: 0.00003152
Iteration 128/1000 | Loss: 0.00003152
Iteration 129/1000 | Loss: 0.00003152
Iteration 130/1000 | Loss: 0.00003152
Iteration 131/1000 | Loss: 0.00003152
Iteration 132/1000 | Loss: 0.00003152
Iteration 133/1000 | Loss: 0.00003152
Iteration 134/1000 | Loss: 0.00003152
Iteration 135/1000 | Loss: 0.00003151
Iteration 136/1000 | Loss: 0.00003151
Iteration 137/1000 | Loss: 0.00003151
Iteration 138/1000 | Loss: 0.00003151
Iteration 139/1000 | Loss: 0.00003150
Iteration 140/1000 | Loss: 0.00003150
Iteration 141/1000 | Loss: 0.00003150
Iteration 142/1000 | Loss: 0.00003150
Iteration 143/1000 | Loss: 0.00003150
Iteration 144/1000 | Loss: 0.00003150
Iteration 145/1000 | Loss: 0.00003150
Iteration 146/1000 | Loss: 0.00003150
Iteration 147/1000 | Loss: 0.00003150
Iteration 148/1000 | Loss: 0.00003150
Iteration 149/1000 | Loss: 0.00003150
Iteration 150/1000 | Loss: 0.00003149
Iteration 151/1000 | Loss: 0.00003149
Iteration 152/1000 | Loss: 0.00003149
Iteration 153/1000 | Loss: 0.00003149
Iteration 154/1000 | Loss: 0.00003148
Iteration 155/1000 | Loss: 0.00003148
Iteration 156/1000 | Loss: 0.00003148
Iteration 157/1000 | Loss: 0.00003148
Iteration 158/1000 | Loss: 0.00003147
Iteration 159/1000 | Loss: 0.00003147
Iteration 160/1000 | Loss: 0.00003147
Iteration 161/1000 | Loss: 0.00003147
Iteration 162/1000 | Loss: 0.00003147
Iteration 163/1000 | Loss: 0.00003147
Iteration 164/1000 | Loss: 0.00003147
Iteration 165/1000 | Loss: 0.00003147
Iteration 166/1000 | Loss: 0.00003147
Iteration 167/1000 | Loss: 0.00003146
Iteration 168/1000 | Loss: 0.00003146
Iteration 169/1000 | Loss: 0.00003146
Iteration 170/1000 | Loss: 0.00003146
Iteration 171/1000 | Loss: 0.00003146
Iteration 172/1000 | Loss: 0.00003146
Iteration 173/1000 | Loss: 0.00003146
Iteration 174/1000 | Loss: 0.00003146
Iteration 175/1000 | Loss: 0.00003146
Iteration 176/1000 | Loss: 0.00003146
Iteration 177/1000 | Loss: 0.00003146
Iteration 178/1000 | Loss: 0.00003145
Iteration 179/1000 | Loss: 0.00003145
Iteration 180/1000 | Loss: 0.00003145
Iteration 181/1000 | Loss: 0.00003145
Iteration 182/1000 | Loss: 0.00003145
Iteration 183/1000 | Loss: 0.00003145
Iteration 184/1000 | Loss: 0.00003145
Iteration 185/1000 | Loss: 0.00003145
Iteration 186/1000 | Loss: 0.00003145
Iteration 187/1000 | Loss: 0.00003145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [3.145254959235899e-05, 3.145254959235899e-05, 3.145254959235899e-05, 3.145254959235899e-05, 3.145254959235899e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.145254959235899e-05

Optimization complete. Final v2v error: 4.5182318687438965 mm

Highest mean error: 6.277066707611084 mm for frame 81

Lowest mean error: 3.4517176151275635 mm for frame 48

Saving results

Total time: 51.99532985687256
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01047084
Iteration 2/25 | Loss: 0.00189379
Iteration 3/25 | Loss: 0.00132110
Iteration 4/25 | Loss: 0.00119356
Iteration 5/25 | Loss: 0.00121832
Iteration 6/25 | Loss: 0.00114285
Iteration 7/25 | Loss: 0.00108410
Iteration 8/25 | Loss: 0.00101571
Iteration 9/25 | Loss: 0.00099067
Iteration 10/25 | Loss: 0.00097928
Iteration 11/25 | Loss: 0.00097551
Iteration 12/25 | Loss: 0.00097471
Iteration 13/25 | Loss: 0.00097440
Iteration 14/25 | Loss: 0.00097426
Iteration 15/25 | Loss: 0.00097425
Iteration 16/25 | Loss: 0.00097425
Iteration 17/25 | Loss: 0.00097425
Iteration 18/25 | Loss: 0.00097425
Iteration 19/25 | Loss: 0.00097424
Iteration 20/25 | Loss: 0.00097424
Iteration 21/25 | Loss: 0.00097424
Iteration 22/25 | Loss: 0.00097424
Iteration 23/25 | Loss: 0.00097424
Iteration 24/25 | Loss: 0.00097424
Iteration 25/25 | Loss: 0.00097424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.99499404
Iteration 2/25 | Loss: 0.00160113
Iteration 3/25 | Loss: 0.00153168
Iteration 4/25 | Loss: 0.00153168
Iteration 5/25 | Loss: 0.00153168
Iteration 6/25 | Loss: 0.00153168
Iteration 7/25 | Loss: 0.00153168
Iteration 8/25 | Loss: 0.00153168
Iteration 9/25 | Loss: 0.00153168
Iteration 10/25 | Loss: 0.00153168
Iteration 11/25 | Loss: 0.00153168
Iteration 12/25 | Loss: 0.00153168
Iteration 13/25 | Loss: 0.00153168
Iteration 14/25 | Loss: 0.00153168
Iteration 15/25 | Loss: 0.00153168
Iteration 16/25 | Loss: 0.00153168
Iteration 17/25 | Loss: 0.00153168
Iteration 18/25 | Loss: 0.00153168
Iteration 19/25 | Loss: 0.00153168
Iteration 20/25 | Loss: 0.00153168
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001531678601168096, 0.001531678601168096, 0.001531678601168096, 0.001531678601168096, 0.001531678601168096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001531678601168096

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153168
Iteration 2/1000 | Loss: 0.00005564
Iteration 3/1000 | Loss: 0.00004150
Iteration 4/1000 | Loss: 0.00003466
Iteration 5/1000 | Loss: 0.00003269
Iteration 6/1000 | Loss: 0.00003156
Iteration 7/1000 | Loss: 0.00003098
Iteration 8/1000 | Loss: 0.00070855
Iteration 9/1000 | Loss: 0.00018057
Iteration 10/1000 | Loss: 0.00003711
Iteration 11/1000 | Loss: 0.00065058
Iteration 12/1000 | Loss: 0.00022199
Iteration 13/1000 | Loss: 0.00003300
Iteration 14/1000 | Loss: 0.00003093
Iteration 15/1000 | Loss: 0.00036332
Iteration 16/1000 | Loss: 0.00019161
Iteration 17/1000 | Loss: 0.00019678
Iteration 18/1000 | Loss: 0.00013873
Iteration 19/1000 | Loss: 0.00017423
Iteration 20/1000 | Loss: 0.00010344
Iteration 21/1000 | Loss: 0.00031602
Iteration 22/1000 | Loss: 0.00022233
Iteration 23/1000 | Loss: 0.00053817
Iteration 24/1000 | Loss: 0.00022005
Iteration 25/1000 | Loss: 0.00019951
Iteration 26/1000 | Loss: 0.00015357
Iteration 27/1000 | Loss: 0.00006761
Iteration 28/1000 | Loss: 0.00012357
Iteration 29/1000 | Loss: 0.00025629
Iteration 30/1000 | Loss: 0.00012573
Iteration 31/1000 | Loss: 0.00007247
Iteration 32/1000 | Loss: 0.00011349
Iteration 33/1000 | Loss: 0.00020831
Iteration 34/1000 | Loss: 0.00011766
Iteration 35/1000 | Loss: 0.00020081
Iteration 36/1000 | Loss: 0.00013811
Iteration 37/1000 | Loss: 0.00024661
Iteration 38/1000 | Loss: 0.00029964
Iteration 39/1000 | Loss: 0.00069918
Iteration 40/1000 | Loss: 0.00009361
Iteration 41/1000 | Loss: 0.00020507
Iteration 42/1000 | Loss: 0.00010819
Iteration 43/1000 | Loss: 0.00046059
Iteration 44/1000 | Loss: 0.00039381
Iteration 45/1000 | Loss: 0.00009790
Iteration 46/1000 | Loss: 0.00008482
Iteration 47/1000 | Loss: 0.00035625
Iteration 48/1000 | Loss: 0.00009695
Iteration 49/1000 | Loss: 0.00017746
Iteration 50/1000 | Loss: 0.00033363
Iteration 51/1000 | Loss: 0.00009873
Iteration 52/1000 | Loss: 0.00004711
Iteration 53/1000 | Loss: 0.00048986
Iteration 54/1000 | Loss: 0.00039718
Iteration 55/1000 | Loss: 0.00021353
Iteration 56/1000 | Loss: 0.00004222
Iteration 57/1000 | Loss: 0.00003766
Iteration 58/1000 | Loss: 0.00057699
Iteration 59/1000 | Loss: 0.00066273
Iteration 60/1000 | Loss: 0.00004532
Iteration 61/1000 | Loss: 0.00002964
Iteration 62/1000 | Loss: 0.00002819
Iteration 63/1000 | Loss: 0.00002705
Iteration 64/1000 | Loss: 0.00002646
Iteration 65/1000 | Loss: 0.00002606
Iteration 66/1000 | Loss: 0.00002585
Iteration 67/1000 | Loss: 0.00002584
Iteration 68/1000 | Loss: 0.00002580
Iteration 69/1000 | Loss: 0.00002572
Iteration 70/1000 | Loss: 0.00002572
Iteration 71/1000 | Loss: 0.00002572
Iteration 72/1000 | Loss: 0.00002571
Iteration 73/1000 | Loss: 0.00002571
Iteration 74/1000 | Loss: 0.00002571
Iteration 75/1000 | Loss: 0.00002570
Iteration 76/1000 | Loss: 0.00002569
Iteration 77/1000 | Loss: 0.00002569
Iteration 78/1000 | Loss: 0.00002569
Iteration 79/1000 | Loss: 0.00002568
Iteration 80/1000 | Loss: 0.00002568
Iteration 81/1000 | Loss: 0.00002567
Iteration 82/1000 | Loss: 0.00002567
Iteration 83/1000 | Loss: 0.00002567
Iteration 84/1000 | Loss: 0.00002566
Iteration 85/1000 | Loss: 0.00002566
Iteration 86/1000 | Loss: 0.00002566
Iteration 87/1000 | Loss: 0.00002565
Iteration 88/1000 | Loss: 0.00002565
Iteration 89/1000 | Loss: 0.00002565
Iteration 90/1000 | Loss: 0.00002565
Iteration 91/1000 | Loss: 0.00002565
Iteration 92/1000 | Loss: 0.00002564
Iteration 93/1000 | Loss: 0.00002564
Iteration 94/1000 | Loss: 0.00002564
Iteration 95/1000 | Loss: 0.00002564
Iteration 96/1000 | Loss: 0.00002564
Iteration 97/1000 | Loss: 0.00002564
Iteration 98/1000 | Loss: 0.00002564
Iteration 99/1000 | Loss: 0.00002564
Iteration 100/1000 | Loss: 0.00002564
Iteration 101/1000 | Loss: 0.00002564
Iteration 102/1000 | Loss: 0.00002564
Iteration 103/1000 | Loss: 0.00002564
Iteration 104/1000 | Loss: 0.00002564
Iteration 105/1000 | Loss: 0.00002564
Iteration 106/1000 | Loss: 0.00002564
Iteration 107/1000 | Loss: 0.00002564
Iteration 108/1000 | Loss: 0.00002564
Iteration 109/1000 | Loss: 0.00002564
Iteration 110/1000 | Loss: 0.00002563
Iteration 111/1000 | Loss: 0.00002563
Iteration 112/1000 | Loss: 0.00002563
Iteration 113/1000 | Loss: 0.00002563
Iteration 114/1000 | Loss: 0.00002563
Iteration 115/1000 | Loss: 0.00002563
Iteration 116/1000 | Loss: 0.00002563
Iteration 117/1000 | Loss: 0.00002563
Iteration 118/1000 | Loss: 0.00002563
Iteration 119/1000 | Loss: 0.00002563
Iteration 120/1000 | Loss: 0.00002563
Iteration 121/1000 | Loss: 0.00002563
Iteration 122/1000 | Loss: 0.00002562
Iteration 123/1000 | Loss: 0.00002562
Iteration 124/1000 | Loss: 0.00002562
Iteration 125/1000 | Loss: 0.00002562
Iteration 126/1000 | Loss: 0.00002562
Iteration 127/1000 | Loss: 0.00002562
Iteration 128/1000 | Loss: 0.00002562
Iteration 129/1000 | Loss: 0.00002562
Iteration 130/1000 | Loss: 0.00002561
Iteration 131/1000 | Loss: 0.00002561
Iteration 132/1000 | Loss: 0.00002561
Iteration 133/1000 | Loss: 0.00002560
Iteration 134/1000 | Loss: 0.00002560
Iteration 135/1000 | Loss: 0.00002560
Iteration 136/1000 | Loss: 0.00002559
Iteration 137/1000 | Loss: 0.00002559
Iteration 138/1000 | Loss: 0.00002559
Iteration 139/1000 | Loss: 0.00002559
Iteration 140/1000 | Loss: 0.00002559
Iteration 141/1000 | Loss: 0.00002559
Iteration 142/1000 | Loss: 0.00002559
Iteration 143/1000 | Loss: 0.00002559
Iteration 144/1000 | Loss: 0.00002559
Iteration 145/1000 | Loss: 0.00002559
Iteration 146/1000 | Loss: 0.00002559
Iteration 147/1000 | Loss: 0.00002559
Iteration 148/1000 | Loss: 0.00002558
Iteration 149/1000 | Loss: 0.00002558
Iteration 150/1000 | Loss: 0.00002558
Iteration 151/1000 | Loss: 0.00002558
Iteration 152/1000 | Loss: 0.00002558
Iteration 153/1000 | Loss: 0.00002558
Iteration 154/1000 | Loss: 0.00002558
Iteration 155/1000 | Loss: 0.00002558
Iteration 156/1000 | Loss: 0.00002558
Iteration 157/1000 | Loss: 0.00002558
Iteration 158/1000 | Loss: 0.00002558
Iteration 159/1000 | Loss: 0.00002558
Iteration 160/1000 | Loss: 0.00002558
Iteration 161/1000 | Loss: 0.00002558
Iteration 162/1000 | Loss: 0.00002558
Iteration 163/1000 | Loss: 0.00002558
Iteration 164/1000 | Loss: 0.00002558
Iteration 165/1000 | Loss: 0.00002558
Iteration 166/1000 | Loss: 0.00002558
Iteration 167/1000 | Loss: 0.00002558
Iteration 168/1000 | Loss: 0.00002558
Iteration 169/1000 | Loss: 0.00002558
Iteration 170/1000 | Loss: 0.00002558
Iteration 171/1000 | Loss: 0.00002558
Iteration 172/1000 | Loss: 0.00002558
Iteration 173/1000 | Loss: 0.00002558
Iteration 174/1000 | Loss: 0.00002558
Iteration 175/1000 | Loss: 0.00002558
Iteration 176/1000 | Loss: 0.00002558
Iteration 177/1000 | Loss: 0.00002558
Iteration 178/1000 | Loss: 0.00002558
Iteration 179/1000 | Loss: 0.00002558
Iteration 180/1000 | Loss: 0.00002558
Iteration 181/1000 | Loss: 0.00002558
Iteration 182/1000 | Loss: 0.00002558
Iteration 183/1000 | Loss: 0.00002558
Iteration 184/1000 | Loss: 0.00002558
Iteration 185/1000 | Loss: 0.00002558
Iteration 186/1000 | Loss: 0.00002558
Iteration 187/1000 | Loss: 0.00002558
Iteration 188/1000 | Loss: 0.00002558
Iteration 189/1000 | Loss: 0.00002558
Iteration 190/1000 | Loss: 0.00002558
Iteration 191/1000 | Loss: 0.00002558
Iteration 192/1000 | Loss: 0.00002558
Iteration 193/1000 | Loss: 0.00002558
Iteration 194/1000 | Loss: 0.00002558
Iteration 195/1000 | Loss: 0.00002558
Iteration 196/1000 | Loss: 0.00002558
Iteration 197/1000 | Loss: 0.00002558
Iteration 198/1000 | Loss: 0.00002558
Iteration 199/1000 | Loss: 0.00002558
Iteration 200/1000 | Loss: 0.00002558
Iteration 201/1000 | Loss: 0.00002558
Iteration 202/1000 | Loss: 0.00002558
Iteration 203/1000 | Loss: 0.00002558
Iteration 204/1000 | Loss: 0.00002558
Iteration 205/1000 | Loss: 0.00002558
Iteration 206/1000 | Loss: 0.00002558
Iteration 207/1000 | Loss: 0.00002558
Iteration 208/1000 | Loss: 0.00002558
Iteration 209/1000 | Loss: 0.00002558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 209. Stopping optimization.
Last 5 losses: [2.5583622118574567e-05, 2.5583622118574567e-05, 2.5583622118574567e-05, 2.5583622118574567e-05, 2.5583622118574567e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5583622118574567e-05

Optimization complete. Final v2v error: 4.030599594116211 mm

Highest mean error: 11.03018856048584 mm for frame 118

Lowest mean error: 3.342268705368042 mm for frame 8

Saving results

Total time: 124.99697756767273
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01107941
Iteration 2/25 | Loss: 0.00207212
Iteration 3/25 | Loss: 0.00133717
Iteration 4/25 | Loss: 0.00112297
Iteration 5/25 | Loss: 0.00110255
Iteration 6/25 | Loss: 0.00104713
Iteration 7/25 | Loss: 0.00096942
Iteration 8/25 | Loss: 0.00092746
Iteration 9/25 | Loss: 0.00091033
Iteration 10/25 | Loss: 0.00091009
Iteration 11/25 | Loss: 0.00090797
Iteration 12/25 | Loss: 0.00091777
Iteration 13/25 | Loss: 0.00090584
Iteration 14/25 | Loss: 0.00089623
Iteration 15/25 | Loss: 0.00089750
Iteration 16/25 | Loss: 0.00089494
Iteration 17/25 | Loss: 0.00088954
Iteration 18/25 | Loss: 0.00088892
Iteration 19/25 | Loss: 0.00088610
Iteration 20/25 | Loss: 0.00088263
Iteration 21/25 | Loss: 0.00088111
Iteration 22/25 | Loss: 0.00088059
Iteration 23/25 | Loss: 0.00088719
Iteration 24/25 | Loss: 0.00088355
Iteration 25/25 | Loss: 0.00087845

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.75991666
Iteration 2/25 | Loss: 0.00207647
Iteration 3/25 | Loss: 0.00185209
Iteration 4/25 | Loss: 0.00185209
Iteration 5/25 | Loss: 0.00185209
Iteration 6/25 | Loss: 0.00185209
Iteration 7/25 | Loss: 0.00185209
Iteration 8/25 | Loss: 0.00185209
Iteration 9/25 | Loss: 0.00185209
Iteration 10/25 | Loss: 0.00185209
Iteration 11/25 | Loss: 0.00185209
Iteration 12/25 | Loss: 0.00185209
Iteration 13/25 | Loss: 0.00185209
Iteration 14/25 | Loss: 0.00185209
Iteration 15/25 | Loss: 0.00185209
Iteration 16/25 | Loss: 0.00185209
Iteration 17/25 | Loss: 0.00185209
Iteration 18/25 | Loss: 0.00185209
Iteration 19/25 | Loss: 0.00185209
Iteration 20/25 | Loss: 0.00185209
Iteration 21/25 | Loss: 0.00185209
Iteration 22/25 | Loss: 0.00185209
Iteration 23/25 | Loss: 0.00185209
Iteration 24/25 | Loss: 0.00185209
Iteration 25/25 | Loss: 0.00185209
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0018520921003073454, 0.0018520921003073454, 0.0018520921003073454, 0.0018520921003073454, 0.0018520921003073454]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018520921003073454

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185209
Iteration 2/1000 | Loss: 0.00008338
Iteration 3/1000 | Loss: 0.00004922
Iteration 4/1000 | Loss: 0.00006130
Iteration 5/1000 | Loss: 0.00004040
Iteration 6/1000 | Loss: 0.00006408
Iteration 7/1000 | Loss: 0.00052012
Iteration 8/1000 | Loss: 0.00076148
Iteration 9/1000 | Loss: 0.00010968
Iteration 10/1000 | Loss: 0.00004902
Iteration 11/1000 | Loss: 0.00046731
Iteration 12/1000 | Loss: 0.00187741
Iteration 13/1000 | Loss: 0.00050345
Iteration 14/1000 | Loss: 0.00007196
Iteration 15/1000 | Loss: 0.00003545
Iteration 16/1000 | Loss: 0.00043947
Iteration 17/1000 | Loss: 0.00004127
Iteration 18/1000 | Loss: 0.00002927
Iteration 19/1000 | Loss: 0.00002624
Iteration 20/1000 | Loss: 0.00029712
Iteration 21/1000 | Loss: 0.00066156
Iteration 22/1000 | Loss: 0.00006059
Iteration 23/1000 | Loss: 0.00006467
Iteration 24/1000 | Loss: 0.00005220
Iteration 25/1000 | Loss: 0.00005317
Iteration 26/1000 | Loss: 0.00034611
Iteration 27/1000 | Loss: 0.00004799
Iteration 28/1000 | Loss: 0.00003167
Iteration 29/1000 | Loss: 0.00002337
Iteration 30/1000 | Loss: 0.00019874
Iteration 31/1000 | Loss: 0.00004105
Iteration 32/1000 | Loss: 0.00002668
Iteration 33/1000 | Loss: 0.00003544
Iteration 34/1000 | Loss: 0.00004684
Iteration 35/1000 | Loss: 0.00003475
Iteration 36/1000 | Loss: 0.00002354
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00003757
Iteration 39/1000 | Loss: 0.00003483
Iteration 40/1000 | Loss: 0.00004574
Iteration 41/1000 | Loss: 0.00004507
Iteration 42/1000 | Loss: 0.00004457
Iteration 43/1000 | Loss: 0.00073045
Iteration 44/1000 | Loss: 0.00007754
Iteration 45/1000 | Loss: 0.00003691
Iteration 46/1000 | Loss: 0.00002230
Iteration 47/1000 | Loss: 0.00003701
Iteration 48/1000 | Loss: 0.00040776
Iteration 49/1000 | Loss: 0.00004270
Iteration 50/1000 | Loss: 0.00002226
Iteration 51/1000 | Loss: 0.00011472
Iteration 52/1000 | Loss: 0.00006254
Iteration 53/1000 | Loss: 0.00006906
Iteration 54/1000 | Loss: 0.00003515
Iteration 55/1000 | Loss: 0.00002765
Iteration 56/1000 | Loss: 0.00002818
Iteration 57/1000 | Loss: 0.00030094
Iteration 58/1000 | Loss: 0.00003984
Iteration 59/1000 | Loss: 0.00002610
Iteration 60/1000 | Loss: 0.00018489
Iteration 61/1000 | Loss: 0.00004605
Iteration 62/1000 | Loss: 0.00002279
Iteration 63/1000 | Loss: 0.00017019
Iteration 64/1000 | Loss: 0.00003395
Iteration 65/1000 | Loss: 0.00002159
Iteration 66/1000 | Loss: 0.00002004
Iteration 67/1000 | Loss: 0.00042328
Iteration 68/1000 | Loss: 0.00004709
Iteration 69/1000 | Loss: 0.00004464
Iteration 70/1000 | Loss: 0.00001993
Iteration 71/1000 | Loss: 0.00001907
Iteration 72/1000 | Loss: 0.00001853
Iteration 73/1000 | Loss: 0.00003532
Iteration 74/1000 | Loss: 0.00002202
Iteration 75/1000 | Loss: 0.00002034
Iteration 76/1000 | Loss: 0.00001969
Iteration 77/1000 | Loss: 0.00001917
Iteration 78/1000 | Loss: 0.00004052
Iteration 79/1000 | Loss: 0.00002215
Iteration 80/1000 | Loss: 0.00002121
Iteration 81/1000 | Loss: 0.00002051
Iteration 82/1000 | Loss: 0.00003342
Iteration 83/1000 | Loss: 0.00003161
Iteration 84/1000 | Loss: 0.00017085
Iteration 85/1000 | Loss: 0.00003444
Iteration 86/1000 | Loss: 0.00003634
Iteration 87/1000 | Loss: 0.00002475
Iteration 88/1000 | Loss: 0.00003132
Iteration 89/1000 | Loss: 0.00006655
Iteration 90/1000 | Loss: 0.00002218
Iteration 91/1000 | Loss: 0.00001882
Iteration 92/1000 | Loss: 0.00001820
Iteration 93/1000 | Loss: 0.00001779
Iteration 94/1000 | Loss: 0.00001745
Iteration 95/1000 | Loss: 0.00001732
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00001723
Iteration 98/1000 | Loss: 0.00001722
Iteration 99/1000 | Loss: 0.00001721
Iteration 100/1000 | Loss: 0.00001721
Iteration 101/1000 | Loss: 0.00001720
Iteration 102/1000 | Loss: 0.00001720
Iteration 103/1000 | Loss: 0.00001720
Iteration 104/1000 | Loss: 0.00001719
Iteration 105/1000 | Loss: 0.00001719
Iteration 106/1000 | Loss: 0.00001715
Iteration 107/1000 | Loss: 0.00001713
Iteration 108/1000 | Loss: 0.00001712
Iteration 109/1000 | Loss: 0.00001712
Iteration 110/1000 | Loss: 0.00001712
Iteration 111/1000 | Loss: 0.00001711
Iteration 112/1000 | Loss: 0.00001711
Iteration 113/1000 | Loss: 0.00001711
Iteration 114/1000 | Loss: 0.00001711
Iteration 115/1000 | Loss: 0.00001711
Iteration 116/1000 | Loss: 0.00001710
Iteration 117/1000 | Loss: 0.00001710
Iteration 118/1000 | Loss: 0.00001710
Iteration 119/1000 | Loss: 0.00001710
Iteration 120/1000 | Loss: 0.00001710
Iteration 121/1000 | Loss: 0.00001710
Iteration 122/1000 | Loss: 0.00001710
Iteration 123/1000 | Loss: 0.00001710
Iteration 124/1000 | Loss: 0.00001710
Iteration 125/1000 | Loss: 0.00001710
Iteration 126/1000 | Loss: 0.00001710
Iteration 127/1000 | Loss: 0.00001710
Iteration 128/1000 | Loss: 0.00001710
Iteration 129/1000 | Loss: 0.00001710
Iteration 130/1000 | Loss: 0.00001709
Iteration 131/1000 | Loss: 0.00001709
Iteration 132/1000 | Loss: 0.00001709
Iteration 133/1000 | Loss: 0.00001709
Iteration 134/1000 | Loss: 0.00001709
Iteration 135/1000 | Loss: 0.00001709
Iteration 136/1000 | Loss: 0.00001709
Iteration 137/1000 | Loss: 0.00001709
Iteration 138/1000 | Loss: 0.00001709
Iteration 139/1000 | Loss: 0.00001709
Iteration 140/1000 | Loss: 0.00001709
Iteration 141/1000 | Loss: 0.00001708
Iteration 142/1000 | Loss: 0.00001708
Iteration 143/1000 | Loss: 0.00001708
Iteration 144/1000 | Loss: 0.00001708
Iteration 145/1000 | Loss: 0.00001708
Iteration 146/1000 | Loss: 0.00001708
Iteration 147/1000 | Loss: 0.00001708
Iteration 148/1000 | Loss: 0.00001708
Iteration 149/1000 | Loss: 0.00001708
Iteration 150/1000 | Loss: 0.00001708
Iteration 151/1000 | Loss: 0.00001708
Iteration 152/1000 | Loss: 0.00001708
Iteration 153/1000 | Loss: 0.00001707
Iteration 154/1000 | Loss: 0.00001707
Iteration 155/1000 | Loss: 0.00001707
Iteration 156/1000 | Loss: 0.00001707
Iteration 157/1000 | Loss: 0.00001707
Iteration 158/1000 | Loss: 0.00001707
Iteration 159/1000 | Loss: 0.00001707
Iteration 160/1000 | Loss: 0.00001707
Iteration 161/1000 | Loss: 0.00001707
Iteration 162/1000 | Loss: 0.00001707
Iteration 163/1000 | Loss: 0.00001707
Iteration 164/1000 | Loss: 0.00001707
Iteration 165/1000 | Loss: 0.00001707
Iteration 166/1000 | Loss: 0.00001707
Iteration 167/1000 | Loss: 0.00001707
Iteration 168/1000 | Loss: 0.00001707
Iteration 169/1000 | Loss: 0.00001707
Iteration 170/1000 | Loss: 0.00001707
Iteration 171/1000 | Loss: 0.00001707
Iteration 172/1000 | Loss: 0.00001707
Iteration 173/1000 | Loss: 0.00001707
Iteration 174/1000 | Loss: 0.00001707
Iteration 175/1000 | Loss: 0.00001707
Iteration 176/1000 | Loss: 0.00001707
Iteration 177/1000 | Loss: 0.00001707
Iteration 178/1000 | Loss: 0.00001707
Iteration 179/1000 | Loss: 0.00001707
Iteration 180/1000 | Loss: 0.00001707
Iteration 181/1000 | Loss: 0.00001707
Iteration 182/1000 | Loss: 0.00001707
Iteration 183/1000 | Loss: 0.00001707
Iteration 184/1000 | Loss: 0.00001707
Iteration 185/1000 | Loss: 0.00001707
Iteration 186/1000 | Loss: 0.00001707
Iteration 187/1000 | Loss: 0.00001707
Iteration 188/1000 | Loss: 0.00001707
Iteration 189/1000 | Loss: 0.00001707
Iteration 190/1000 | Loss: 0.00001707
Iteration 191/1000 | Loss: 0.00001707
Iteration 192/1000 | Loss: 0.00001707
Iteration 193/1000 | Loss: 0.00001707
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 193. Stopping optimization.
Last 5 losses: [1.7067599401343614e-05, 1.7067599401343614e-05, 1.7067599401343614e-05, 1.7067599401343614e-05, 1.7067599401343614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7067599401343614e-05

Optimization complete. Final v2v error: 3.483616590499878 mm

Highest mean error: 4.402218341827393 mm for frame 113

Lowest mean error: 3.0099146366119385 mm for frame 91

Saving results

Total time: 187.28243041038513
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01019216
Iteration 2/25 | Loss: 0.00287759
Iteration 3/25 | Loss: 0.00145012
Iteration 4/25 | Loss: 0.00129370
Iteration 5/25 | Loss: 0.00126808
Iteration 6/25 | Loss: 0.00125465
Iteration 7/25 | Loss: 0.00121027
Iteration 8/25 | Loss: 0.00117599
Iteration 9/25 | Loss: 0.00116631
Iteration 10/25 | Loss: 0.00116758
Iteration 11/25 | Loss: 0.00115856
Iteration 12/25 | Loss: 0.00115230
Iteration 13/25 | Loss: 0.00114899
Iteration 14/25 | Loss: 0.00114278
Iteration 15/25 | Loss: 0.00114064
Iteration 16/25 | Loss: 0.00113392
Iteration 17/25 | Loss: 0.00113248
Iteration 18/25 | Loss: 0.00114817
Iteration 19/25 | Loss: 0.00114668
Iteration 20/25 | Loss: 0.00114049
Iteration 21/25 | Loss: 0.00113938
Iteration 22/25 | Loss: 0.00113618
Iteration 23/25 | Loss: 0.00113116
Iteration 24/25 | Loss: 0.00113624
Iteration 25/25 | Loss: 0.00113485

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63087738
Iteration 2/25 | Loss: 0.00390165
Iteration 3/25 | Loss: 0.00380636
Iteration 4/25 | Loss: 0.00380636
Iteration 5/25 | Loss: 0.00380635
Iteration 6/25 | Loss: 0.00380635
Iteration 7/25 | Loss: 0.00380635
Iteration 8/25 | Loss: 0.00380635
Iteration 9/25 | Loss: 0.00380635
Iteration 10/25 | Loss: 0.00380635
Iteration 11/25 | Loss: 0.00380635
Iteration 12/25 | Loss: 0.00380635
Iteration 13/25 | Loss: 0.00380635
Iteration 14/25 | Loss: 0.00380635
Iteration 15/25 | Loss: 0.00380635
Iteration 16/25 | Loss: 0.00380635
Iteration 17/25 | Loss: 0.00380635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0038063505198806524, 0.0038063505198806524, 0.0038063505198806524, 0.0038063505198806524, 0.0038063505198806524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038063505198806524

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00380635
Iteration 2/1000 | Loss: 0.00579491
Iteration 3/1000 | Loss: 0.00095742
Iteration 4/1000 | Loss: 0.00071730
Iteration 5/1000 | Loss: 0.00040091
Iteration 6/1000 | Loss: 0.00034361
Iteration 7/1000 | Loss: 0.00044793
Iteration 8/1000 | Loss: 0.00088078
Iteration 9/1000 | Loss: 0.00051605
Iteration 10/1000 | Loss: 0.00090703
Iteration 11/1000 | Loss: 0.00014252
Iteration 12/1000 | Loss: 0.00010597
Iteration 13/1000 | Loss: 0.00014984
Iteration 14/1000 | Loss: 0.00033704
Iteration 15/1000 | Loss: 0.00022151
Iteration 16/1000 | Loss: 0.00055933
Iteration 17/1000 | Loss: 0.00053648
Iteration 18/1000 | Loss: 0.00052360
Iteration 19/1000 | Loss: 0.00055220
Iteration 20/1000 | Loss: 0.00054008
Iteration 21/1000 | Loss: 0.00065560
Iteration 22/1000 | Loss: 0.00072356
Iteration 23/1000 | Loss: 0.00035429
Iteration 24/1000 | Loss: 0.00044867
Iteration 25/1000 | Loss: 0.00043993
Iteration 26/1000 | Loss: 0.00056694
Iteration 27/1000 | Loss: 0.00051324
Iteration 28/1000 | Loss: 0.00048149
Iteration 29/1000 | Loss: 0.00006282
Iteration 30/1000 | Loss: 0.00004381
Iteration 31/1000 | Loss: 0.00003942
Iteration 32/1000 | Loss: 0.00003503
Iteration 33/1000 | Loss: 0.00003235
Iteration 34/1000 | Loss: 0.00003115
Iteration 35/1000 | Loss: 0.00003001
Iteration 36/1000 | Loss: 0.00002887
Iteration 37/1000 | Loss: 0.00017146
Iteration 38/1000 | Loss: 0.00026119
Iteration 39/1000 | Loss: 0.00016697
Iteration 40/1000 | Loss: 0.00015206
Iteration 41/1000 | Loss: 0.00029163
Iteration 42/1000 | Loss: 0.00009732
Iteration 43/1000 | Loss: 0.00003113
Iteration 44/1000 | Loss: 0.00016700
Iteration 45/1000 | Loss: 0.00008729
Iteration 46/1000 | Loss: 0.00004194
Iteration 47/1000 | Loss: 0.00013027
Iteration 48/1000 | Loss: 0.00009694
Iteration 49/1000 | Loss: 0.00003427
Iteration 50/1000 | Loss: 0.00012475
Iteration 51/1000 | Loss: 0.00015315
Iteration 52/1000 | Loss: 0.00003912
Iteration 53/1000 | Loss: 0.00003462
Iteration 54/1000 | Loss: 0.00003127
Iteration 55/1000 | Loss: 0.00002877
Iteration 56/1000 | Loss: 0.00002725
Iteration 57/1000 | Loss: 0.00002609
Iteration 58/1000 | Loss: 0.00002542
Iteration 59/1000 | Loss: 0.00002513
Iteration 60/1000 | Loss: 0.00003862
Iteration 61/1000 | Loss: 0.00002712
Iteration 62/1000 | Loss: 0.00002553
Iteration 63/1000 | Loss: 0.00002456
Iteration 64/1000 | Loss: 0.00002411
Iteration 65/1000 | Loss: 0.00002389
Iteration 66/1000 | Loss: 0.00002372
Iteration 67/1000 | Loss: 0.00002369
Iteration 68/1000 | Loss: 0.00002364
Iteration 69/1000 | Loss: 0.00002348
Iteration 70/1000 | Loss: 0.00002347
Iteration 71/1000 | Loss: 0.00002340
Iteration 72/1000 | Loss: 0.00002337
Iteration 73/1000 | Loss: 0.00002334
Iteration 74/1000 | Loss: 0.00002334
Iteration 75/1000 | Loss: 0.00002333
Iteration 76/1000 | Loss: 0.00002333
Iteration 77/1000 | Loss: 0.00002333
Iteration 78/1000 | Loss: 0.00002333
Iteration 79/1000 | Loss: 0.00002332
Iteration 80/1000 | Loss: 0.00002332
Iteration 81/1000 | Loss: 0.00002332
Iteration 82/1000 | Loss: 0.00002331
Iteration 83/1000 | Loss: 0.00002331
Iteration 84/1000 | Loss: 0.00002331
Iteration 85/1000 | Loss: 0.00002331
Iteration 86/1000 | Loss: 0.00002331
Iteration 87/1000 | Loss: 0.00002330
Iteration 88/1000 | Loss: 0.00002330
Iteration 89/1000 | Loss: 0.00002330
Iteration 90/1000 | Loss: 0.00002330
Iteration 91/1000 | Loss: 0.00002329
Iteration 92/1000 | Loss: 0.00002329
Iteration 93/1000 | Loss: 0.00002329
Iteration 94/1000 | Loss: 0.00002329
Iteration 95/1000 | Loss: 0.00002329
Iteration 96/1000 | Loss: 0.00002329
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002328
Iteration 99/1000 | Loss: 0.00002328
Iteration 100/1000 | Loss: 0.00002328
Iteration 101/1000 | Loss: 0.00002327
Iteration 102/1000 | Loss: 0.00002327
Iteration 103/1000 | Loss: 0.00002327
Iteration 104/1000 | Loss: 0.00002327
Iteration 105/1000 | Loss: 0.00002326
Iteration 106/1000 | Loss: 0.00002326
Iteration 107/1000 | Loss: 0.00002326
Iteration 108/1000 | Loss: 0.00002326
Iteration 109/1000 | Loss: 0.00002326
Iteration 110/1000 | Loss: 0.00002326
Iteration 111/1000 | Loss: 0.00002326
Iteration 112/1000 | Loss: 0.00002326
Iteration 113/1000 | Loss: 0.00002326
Iteration 114/1000 | Loss: 0.00002325
Iteration 115/1000 | Loss: 0.00002325
Iteration 116/1000 | Loss: 0.00002325
Iteration 117/1000 | Loss: 0.00002325
Iteration 118/1000 | Loss: 0.00002325
Iteration 119/1000 | Loss: 0.00002325
Iteration 120/1000 | Loss: 0.00002325
Iteration 121/1000 | Loss: 0.00002325
Iteration 122/1000 | Loss: 0.00002325
Iteration 123/1000 | Loss: 0.00002324
Iteration 124/1000 | Loss: 0.00002324
Iteration 125/1000 | Loss: 0.00002324
Iteration 126/1000 | Loss: 0.00002323
Iteration 127/1000 | Loss: 0.00002323
Iteration 128/1000 | Loss: 0.00002323
Iteration 129/1000 | Loss: 0.00002322
Iteration 130/1000 | Loss: 0.00002322
Iteration 131/1000 | Loss: 0.00002321
Iteration 132/1000 | Loss: 0.00002321
Iteration 133/1000 | Loss: 0.00002321
Iteration 134/1000 | Loss: 0.00002321
Iteration 135/1000 | Loss: 0.00002321
Iteration 136/1000 | Loss: 0.00002321
Iteration 137/1000 | Loss: 0.00002321
Iteration 138/1000 | Loss: 0.00002321
Iteration 139/1000 | Loss: 0.00002321
Iteration 140/1000 | Loss: 0.00002321
Iteration 141/1000 | Loss: 0.00002320
Iteration 142/1000 | Loss: 0.00002319
Iteration 143/1000 | Loss: 0.00002319
Iteration 144/1000 | Loss: 0.00002319
Iteration 145/1000 | Loss: 0.00002318
Iteration 146/1000 | Loss: 0.00002318
Iteration 147/1000 | Loss: 0.00002318
Iteration 148/1000 | Loss: 0.00002317
Iteration 149/1000 | Loss: 0.00002317
Iteration 150/1000 | Loss: 0.00002317
Iteration 151/1000 | Loss: 0.00002316
Iteration 152/1000 | Loss: 0.00002316
Iteration 153/1000 | Loss: 0.00002316
Iteration 154/1000 | Loss: 0.00002315
Iteration 155/1000 | Loss: 0.00002315
Iteration 156/1000 | Loss: 0.00002315
Iteration 157/1000 | Loss: 0.00002314
Iteration 158/1000 | Loss: 0.00002314
Iteration 159/1000 | Loss: 0.00002314
Iteration 160/1000 | Loss: 0.00002314
Iteration 161/1000 | Loss: 0.00002314
Iteration 162/1000 | Loss: 0.00002314
Iteration 163/1000 | Loss: 0.00002314
Iteration 164/1000 | Loss: 0.00002314
Iteration 165/1000 | Loss: 0.00002314
Iteration 166/1000 | Loss: 0.00002314
Iteration 167/1000 | Loss: 0.00002314
Iteration 168/1000 | Loss: 0.00002314
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [2.3142341888160445e-05, 2.3142341888160445e-05, 2.3142341888160445e-05, 2.3142341888160445e-05, 2.3142341888160445e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3142341888160445e-05

Optimization complete. Final v2v error: 3.960676431655884 mm

Highest mean error: 6.339465141296387 mm for frame 2

Lowest mean error: 3.518686532974243 mm for frame 103

Saving results

Total time: 153.56294202804565
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01067682
Iteration 2/25 | Loss: 0.00195262
Iteration 3/25 | Loss: 0.00164150
Iteration 4/25 | Loss: 0.00139169
Iteration 5/25 | Loss: 0.00124009
Iteration 6/25 | Loss: 0.00104419
Iteration 7/25 | Loss: 0.00094805
Iteration 8/25 | Loss: 0.00091809
Iteration 9/25 | Loss: 0.00090256
Iteration 10/25 | Loss: 0.00087514
Iteration 11/25 | Loss: 0.00085418
Iteration 12/25 | Loss: 0.00083312
Iteration 13/25 | Loss: 0.00083076
Iteration 14/25 | Loss: 0.00082108
Iteration 15/25 | Loss: 0.00081930
Iteration 16/25 | Loss: 0.00081830
Iteration 17/25 | Loss: 0.00081946
Iteration 18/25 | Loss: 0.00081804
Iteration 19/25 | Loss: 0.00081802
Iteration 20/25 | Loss: 0.00081801
Iteration 21/25 | Loss: 0.00081801
Iteration 22/25 | Loss: 0.00081801
Iteration 23/25 | Loss: 0.00081801
Iteration 24/25 | Loss: 0.00081801
Iteration 25/25 | Loss: 0.00081801

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78915024
Iteration 2/25 | Loss: 0.00129608
Iteration 3/25 | Loss: 0.00129608
Iteration 4/25 | Loss: 0.00121090
Iteration 5/25 | Loss: 0.00121090
Iteration 6/25 | Loss: 0.00121090
Iteration 7/25 | Loss: 0.00121090
Iteration 8/25 | Loss: 0.00121090
Iteration 9/25 | Loss: 0.00121090
Iteration 10/25 | Loss: 0.00121090
Iteration 11/25 | Loss: 0.00121090
Iteration 12/25 | Loss: 0.00121090
Iteration 13/25 | Loss: 0.00121090
Iteration 14/25 | Loss: 0.00121090
Iteration 15/25 | Loss: 0.00121090
Iteration 16/25 | Loss: 0.00121090
Iteration 17/25 | Loss: 0.00121090
Iteration 18/25 | Loss: 0.00121090
Iteration 19/25 | Loss: 0.00121090
Iteration 20/25 | Loss: 0.00121090
Iteration 21/25 | Loss: 0.00121090
Iteration 22/25 | Loss: 0.00121090
Iteration 23/25 | Loss: 0.00121090
Iteration 24/25 | Loss: 0.00121090
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001210901071317494, 0.001210901071317494, 0.001210901071317494, 0.001210901071317494, 0.001210901071317494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001210901071317494

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121090
Iteration 2/1000 | Loss: 0.00004316
Iteration 3/1000 | Loss: 0.00011282
Iteration 4/1000 | Loss: 0.00012379
Iteration 5/1000 | Loss: 0.00006543
Iteration 6/1000 | Loss: 0.00024426
Iteration 7/1000 | Loss: 0.00003020
Iteration 8/1000 | Loss: 0.00002425
Iteration 9/1000 | Loss: 0.00033631
Iteration 10/1000 | Loss: 0.00013204
Iteration 11/1000 | Loss: 0.00010666
Iteration 12/1000 | Loss: 0.00056292
Iteration 13/1000 | Loss: 0.00020290
Iteration 14/1000 | Loss: 0.00004397
Iteration 15/1000 | Loss: 0.00002342
Iteration 16/1000 | Loss: 0.00006994
Iteration 17/1000 | Loss: 0.00005278
Iteration 18/1000 | Loss: 0.00003576
Iteration 19/1000 | Loss: 0.00002398
Iteration 20/1000 | Loss: 0.00007707
Iteration 21/1000 | Loss: 0.00004466
Iteration 22/1000 | Loss: 0.00004198
Iteration 23/1000 | Loss: 0.00002447
Iteration 24/1000 | Loss: 0.00002644
Iteration 25/1000 | Loss: 0.00003944
Iteration 26/1000 | Loss: 0.00002145
Iteration 27/1000 | Loss: 0.00004238
Iteration 28/1000 | Loss: 0.00002127
Iteration 29/1000 | Loss: 0.00004332
Iteration 30/1000 | Loss: 0.00002916
Iteration 31/1000 | Loss: 0.00002270
Iteration 32/1000 | Loss: 0.00002244
Iteration 33/1000 | Loss: 0.00002119
Iteration 34/1000 | Loss: 0.00002102
Iteration 35/1000 | Loss: 0.00002091
Iteration 36/1000 | Loss: 0.00002090
Iteration 37/1000 | Loss: 0.00002090
Iteration 38/1000 | Loss: 0.00002089
Iteration 39/1000 | Loss: 0.00002088
Iteration 40/1000 | Loss: 0.00011416
Iteration 41/1000 | Loss: 0.00003061
Iteration 42/1000 | Loss: 0.00002225
Iteration 43/1000 | Loss: 0.00007338
Iteration 44/1000 | Loss: 0.00020294
Iteration 45/1000 | Loss: 0.00002907
Iteration 46/1000 | Loss: 0.00003222
Iteration 47/1000 | Loss: 0.00004062
Iteration 48/1000 | Loss: 0.00002086
Iteration 49/1000 | Loss: 0.00003980
Iteration 50/1000 | Loss: 0.00004150
Iteration 51/1000 | Loss: 0.00003407
Iteration 52/1000 | Loss: 0.00002232
Iteration 53/1000 | Loss: 0.00007174
Iteration 54/1000 | Loss: 0.00047174
Iteration 55/1000 | Loss: 0.00008974
Iteration 56/1000 | Loss: 0.00018481
Iteration 57/1000 | Loss: 0.00009372
Iteration 58/1000 | Loss: 0.00004657
Iteration 59/1000 | Loss: 0.00023360
Iteration 60/1000 | Loss: 0.00026648
Iteration 61/1000 | Loss: 0.00006850
Iteration 62/1000 | Loss: 0.00006728
Iteration 63/1000 | Loss: 0.00010676
Iteration 64/1000 | Loss: 0.00005850
Iteration 65/1000 | Loss: 0.00006206
Iteration 66/1000 | Loss: 0.00002898
Iteration 67/1000 | Loss: 0.00004553
Iteration 68/1000 | Loss: 0.00002229
Iteration 69/1000 | Loss: 0.00002130
Iteration 70/1000 | Loss: 0.00002062
Iteration 71/1000 | Loss: 0.00005064
Iteration 72/1000 | Loss: 0.00004553
Iteration 73/1000 | Loss: 0.00002397
Iteration 74/1000 | Loss: 0.00001998
Iteration 75/1000 | Loss: 0.00002559
Iteration 76/1000 | Loss: 0.00001993
Iteration 77/1000 | Loss: 0.00010005
Iteration 78/1000 | Loss: 0.00002233
Iteration 79/1000 | Loss: 0.00003251
Iteration 80/1000 | Loss: 0.00001964
Iteration 81/1000 | Loss: 0.00001964
Iteration 82/1000 | Loss: 0.00001963
Iteration 83/1000 | Loss: 0.00001963
Iteration 84/1000 | Loss: 0.00001963
Iteration 85/1000 | Loss: 0.00001963
Iteration 86/1000 | Loss: 0.00001963
Iteration 87/1000 | Loss: 0.00001963
Iteration 88/1000 | Loss: 0.00001963
Iteration 89/1000 | Loss: 0.00001962
Iteration 90/1000 | Loss: 0.00001961
Iteration 91/1000 | Loss: 0.00001960
Iteration 92/1000 | Loss: 0.00001960
Iteration 93/1000 | Loss: 0.00002561
Iteration 94/1000 | Loss: 0.00002000
Iteration 95/1000 | Loss: 0.00001957
Iteration 96/1000 | Loss: 0.00001957
Iteration 97/1000 | Loss: 0.00001957
Iteration 98/1000 | Loss: 0.00001957
Iteration 99/1000 | Loss: 0.00001956
Iteration 100/1000 | Loss: 0.00001956
Iteration 101/1000 | Loss: 0.00001956
Iteration 102/1000 | Loss: 0.00001956
Iteration 103/1000 | Loss: 0.00001956
Iteration 104/1000 | Loss: 0.00001956
Iteration 105/1000 | Loss: 0.00001956
Iteration 106/1000 | Loss: 0.00001956
Iteration 107/1000 | Loss: 0.00001956
Iteration 108/1000 | Loss: 0.00001956
Iteration 109/1000 | Loss: 0.00001956
Iteration 110/1000 | Loss: 0.00001955
Iteration 111/1000 | Loss: 0.00002407
Iteration 112/1000 | Loss: 0.00001952
Iteration 113/1000 | Loss: 0.00001952
Iteration 114/1000 | Loss: 0.00002158
Iteration 115/1000 | Loss: 0.00001951
Iteration 116/1000 | Loss: 0.00001951
Iteration 117/1000 | Loss: 0.00001951
Iteration 118/1000 | Loss: 0.00001951
Iteration 119/1000 | Loss: 0.00001950
Iteration 120/1000 | Loss: 0.00001950
Iteration 121/1000 | Loss: 0.00001950
Iteration 122/1000 | Loss: 0.00001950
Iteration 123/1000 | Loss: 0.00001950
Iteration 124/1000 | Loss: 0.00001949
Iteration 125/1000 | Loss: 0.00001949
Iteration 126/1000 | Loss: 0.00002255
Iteration 127/1000 | Loss: 0.00001949
Iteration 128/1000 | Loss: 0.00001948
Iteration 129/1000 | Loss: 0.00001948
Iteration 130/1000 | Loss: 0.00001948
Iteration 131/1000 | Loss: 0.00001948
Iteration 132/1000 | Loss: 0.00001948
Iteration 133/1000 | Loss: 0.00001947
Iteration 134/1000 | Loss: 0.00001947
Iteration 135/1000 | Loss: 0.00001947
Iteration 136/1000 | Loss: 0.00001947
Iteration 137/1000 | Loss: 0.00001947
Iteration 138/1000 | Loss: 0.00001947
Iteration 139/1000 | Loss: 0.00001947
Iteration 140/1000 | Loss: 0.00001947
Iteration 141/1000 | Loss: 0.00001947
Iteration 142/1000 | Loss: 0.00001947
Iteration 143/1000 | Loss: 0.00001946
Iteration 144/1000 | Loss: 0.00001946
Iteration 145/1000 | Loss: 0.00001946
Iteration 146/1000 | Loss: 0.00001946
Iteration 147/1000 | Loss: 0.00001946
Iteration 148/1000 | Loss: 0.00001946
Iteration 149/1000 | Loss: 0.00001946
Iteration 150/1000 | Loss: 0.00001946
Iteration 151/1000 | Loss: 0.00001945
Iteration 152/1000 | Loss: 0.00001945
Iteration 153/1000 | Loss: 0.00001945
Iteration 154/1000 | Loss: 0.00001945
Iteration 155/1000 | Loss: 0.00001945
Iteration 156/1000 | Loss: 0.00001945
Iteration 157/1000 | Loss: 0.00001945
Iteration 158/1000 | Loss: 0.00001945
Iteration 159/1000 | Loss: 0.00001945
Iteration 160/1000 | Loss: 0.00001945
Iteration 161/1000 | Loss: 0.00001944
Iteration 162/1000 | Loss: 0.00001944
Iteration 163/1000 | Loss: 0.00001944
Iteration 164/1000 | Loss: 0.00001944
Iteration 165/1000 | Loss: 0.00001944
Iteration 166/1000 | Loss: 0.00001944
Iteration 167/1000 | Loss: 0.00001944
Iteration 168/1000 | Loss: 0.00001944
Iteration 169/1000 | Loss: 0.00001944
Iteration 170/1000 | Loss: 0.00001944
Iteration 171/1000 | Loss: 0.00001944
Iteration 172/1000 | Loss: 0.00001944
Iteration 173/1000 | Loss: 0.00001943
Iteration 174/1000 | Loss: 0.00001943
Iteration 175/1000 | Loss: 0.00001943
Iteration 176/1000 | Loss: 0.00001943
Iteration 177/1000 | Loss: 0.00001943
Iteration 178/1000 | Loss: 0.00001943
Iteration 179/1000 | Loss: 0.00001943
Iteration 180/1000 | Loss: 0.00001943
Iteration 181/1000 | Loss: 0.00001943
Iteration 182/1000 | Loss: 0.00001943
Iteration 183/1000 | Loss: 0.00001943
Iteration 184/1000 | Loss: 0.00001943
Iteration 185/1000 | Loss: 0.00001943
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 185. Stopping optimization.
Last 5 losses: [1.943401002790779e-05, 1.943401002790779e-05, 1.943401002790779e-05, 1.943401002790779e-05, 1.943401002790779e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.943401002790779e-05

Optimization complete. Final v2v error: 3.694257974624634 mm

Highest mean error: 4.586271286010742 mm for frame 176

Lowest mean error: 3.4643137454986572 mm for frame 217

Saving results

Total time: 168.0790364742279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00671467
Iteration 2/25 | Loss: 0.00136465
Iteration 3/25 | Loss: 0.00093112
Iteration 4/25 | Loss: 0.00085147
Iteration 5/25 | Loss: 0.00084498
Iteration 6/25 | Loss: 0.00083008
Iteration 7/25 | Loss: 0.00082738
Iteration 8/25 | Loss: 0.00082985
Iteration 9/25 | Loss: 0.00082532
Iteration 10/25 | Loss: 0.00082408
Iteration 11/25 | Loss: 0.00082365
Iteration 12/25 | Loss: 0.00082338
Iteration 13/25 | Loss: 0.00082329
Iteration 14/25 | Loss: 0.00082329
Iteration 15/25 | Loss: 0.00082329
Iteration 16/25 | Loss: 0.00082328
Iteration 17/25 | Loss: 0.00082328
Iteration 18/25 | Loss: 0.00082328
Iteration 19/25 | Loss: 0.00082328
Iteration 20/25 | Loss: 0.00082328
Iteration 21/25 | Loss: 0.00082328
Iteration 22/25 | Loss: 0.00082328
Iteration 23/25 | Loss: 0.00082328
Iteration 24/25 | Loss: 0.00082328
Iteration 25/25 | Loss: 0.00082328

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11610055
Iteration 2/25 | Loss: 0.00168292
Iteration 3/25 | Loss: 0.00168291
Iteration 4/25 | Loss: 0.00168291
Iteration 5/25 | Loss: 0.00168291
Iteration 6/25 | Loss: 0.00168291
Iteration 7/25 | Loss: 0.00168291
Iteration 8/25 | Loss: 0.00168291
Iteration 9/25 | Loss: 0.00168291
Iteration 10/25 | Loss: 0.00168291
Iteration 11/25 | Loss: 0.00168290
Iteration 12/25 | Loss: 0.00168290
Iteration 13/25 | Loss: 0.00168290
Iteration 14/25 | Loss: 0.00168290
Iteration 15/25 | Loss: 0.00168290
Iteration 16/25 | Loss: 0.00168290
Iteration 17/25 | Loss: 0.00168290
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0016829045489430428, 0.0016829045489430428, 0.0016829045489430428, 0.0016829045489430428, 0.0016829045489430428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016829045489430428

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00168290
Iteration 2/1000 | Loss: 0.00008596
Iteration 3/1000 | Loss: 0.00005248
Iteration 4/1000 | Loss: 0.00004373
Iteration 5/1000 | Loss: 0.00006175
Iteration 6/1000 | Loss: 0.00005107
Iteration 7/1000 | Loss: 0.00003586
Iteration 8/1000 | Loss: 0.00010896
Iteration 9/1000 | Loss: 0.00003220
Iteration 10/1000 | Loss: 0.00003103
Iteration 11/1000 | Loss: 0.00003048
Iteration 12/1000 | Loss: 0.00003008
Iteration 13/1000 | Loss: 0.00006448
Iteration 14/1000 | Loss: 0.00003331
Iteration 15/1000 | Loss: 0.00003532
Iteration 16/1000 | Loss: 0.00002926
Iteration 17/1000 | Loss: 0.00002888
Iteration 18/1000 | Loss: 0.00061602
Iteration 19/1000 | Loss: 0.00076045
Iteration 20/1000 | Loss: 0.00026607
Iteration 21/1000 | Loss: 0.00004285
Iteration 22/1000 | Loss: 0.00039957
Iteration 23/1000 | Loss: 0.00010363
Iteration 24/1000 | Loss: 0.00013146
Iteration 25/1000 | Loss: 0.00002456
Iteration 26/1000 | Loss: 0.00002163
Iteration 27/1000 | Loss: 0.00002052
Iteration 28/1000 | Loss: 0.00001978
Iteration 29/1000 | Loss: 0.00001935
Iteration 30/1000 | Loss: 0.00004277
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001878
Iteration 33/1000 | Loss: 0.00001876
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001858
Iteration 36/1000 | Loss: 0.00001858
Iteration 37/1000 | Loss: 0.00004466
Iteration 38/1000 | Loss: 0.00001866
Iteration 39/1000 | Loss: 0.00001838
Iteration 40/1000 | Loss: 0.00001831
Iteration 41/1000 | Loss: 0.00001827
Iteration 42/1000 | Loss: 0.00001826
Iteration 43/1000 | Loss: 0.00001826
Iteration 44/1000 | Loss: 0.00001826
Iteration 45/1000 | Loss: 0.00001825
Iteration 46/1000 | Loss: 0.00001825
Iteration 47/1000 | Loss: 0.00001824
Iteration 48/1000 | Loss: 0.00001824
Iteration 49/1000 | Loss: 0.00001823
Iteration 50/1000 | Loss: 0.00001822
Iteration 51/1000 | Loss: 0.00001819
Iteration 52/1000 | Loss: 0.00001817
Iteration 53/1000 | Loss: 0.00001817
Iteration 54/1000 | Loss: 0.00001816
Iteration 55/1000 | Loss: 0.00001816
Iteration 56/1000 | Loss: 0.00001815
Iteration 57/1000 | Loss: 0.00001815
Iteration 58/1000 | Loss: 0.00001814
Iteration 59/1000 | Loss: 0.00001814
Iteration 60/1000 | Loss: 0.00001813
Iteration 61/1000 | Loss: 0.00001813
Iteration 62/1000 | Loss: 0.00001812
Iteration 63/1000 | Loss: 0.00001812
Iteration 64/1000 | Loss: 0.00001812
Iteration 65/1000 | Loss: 0.00001811
Iteration 66/1000 | Loss: 0.00001811
Iteration 67/1000 | Loss: 0.00001810
Iteration 68/1000 | Loss: 0.00001810
Iteration 69/1000 | Loss: 0.00001810
Iteration 70/1000 | Loss: 0.00001810
Iteration 71/1000 | Loss: 0.00001810
Iteration 72/1000 | Loss: 0.00001810
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001809
Iteration 75/1000 | Loss: 0.00001809
Iteration 76/1000 | Loss: 0.00001809
Iteration 77/1000 | Loss: 0.00001808
Iteration 78/1000 | Loss: 0.00001808
Iteration 79/1000 | Loss: 0.00001808
Iteration 80/1000 | Loss: 0.00001807
Iteration 81/1000 | Loss: 0.00001807
Iteration 82/1000 | Loss: 0.00001807
Iteration 83/1000 | Loss: 0.00001807
Iteration 84/1000 | Loss: 0.00001807
Iteration 85/1000 | Loss: 0.00001807
Iteration 86/1000 | Loss: 0.00001807
Iteration 87/1000 | Loss: 0.00001807
Iteration 88/1000 | Loss: 0.00001806
Iteration 89/1000 | Loss: 0.00001806
Iteration 90/1000 | Loss: 0.00001806
Iteration 91/1000 | Loss: 0.00001806
Iteration 92/1000 | Loss: 0.00001806
Iteration 93/1000 | Loss: 0.00001805
Iteration 94/1000 | Loss: 0.00001805
Iteration 95/1000 | Loss: 0.00001805
Iteration 96/1000 | Loss: 0.00001805
Iteration 97/1000 | Loss: 0.00001805
Iteration 98/1000 | Loss: 0.00001805
Iteration 99/1000 | Loss: 0.00001804
Iteration 100/1000 | Loss: 0.00001804
Iteration 101/1000 | Loss: 0.00001804
Iteration 102/1000 | Loss: 0.00001804
Iteration 103/1000 | Loss: 0.00001804
Iteration 104/1000 | Loss: 0.00001804
Iteration 105/1000 | Loss: 0.00001804
Iteration 106/1000 | Loss: 0.00001804
Iteration 107/1000 | Loss: 0.00001804
Iteration 108/1000 | Loss: 0.00001803
Iteration 109/1000 | Loss: 0.00001803
Iteration 110/1000 | Loss: 0.00001803
Iteration 111/1000 | Loss: 0.00001803
Iteration 112/1000 | Loss: 0.00001803
Iteration 113/1000 | Loss: 0.00001803
Iteration 114/1000 | Loss: 0.00001803
Iteration 115/1000 | Loss: 0.00001803
Iteration 116/1000 | Loss: 0.00001802
Iteration 117/1000 | Loss: 0.00001802
Iteration 118/1000 | Loss: 0.00001802
Iteration 119/1000 | Loss: 0.00001802
Iteration 120/1000 | Loss: 0.00001802
Iteration 121/1000 | Loss: 0.00001802
Iteration 122/1000 | Loss: 0.00001801
Iteration 123/1000 | Loss: 0.00001801
Iteration 124/1000 | Loss: 0.00001801
Iteration 125/1000 | Loss: 0.00001801
Iteration 126/1000 | Loss: 0.00001801
Iteration 127/1000 | Loss: 0.00001801
Iteration 128/1000 | Loss: 0.00001801
Iteration 129/1000 | Loss: 0.00001801
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.8013764929492027e-05, 1.8013764929492027e-05, 1.8013764929492027e-05, 1.8013764929492027e-05, 1.8013764929492027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8013764929492027e-05

Optimization complete. Final v2v error: 3.5160622596740723 mm

Highest mean error: 4.488145351409912 mm for frame 175

Lowest mean error: 2.88456654548645 mm for frame 139

Saving results

Total time: 94.50978016853333
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00481823
Iteration 2/25 | Loss: 0.00087782
Iteration 3/25 | Loss: 0.00076817
Iteration 4/25 | Loss: 0.00074870
Iteration 5/25 | Loss: 0.00074245
Iteration 6/25 | Loss: 0.00074095
Iteration 7/25 | Loss: 0.00074084
Iteration 8/25 | Loss: 0.00074084
Iteration 9/25 | Loss: 0.00074084
Iteration 10/25 | Loss: 0.00074084
Iteration 11/25 | Loss: 0.00074084
Iteration 12/25 | Loss: 0.00074084
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007408391102217138, 0.0007408391102217138, 0.0007408391102217138, 0.0007408391102217138, 0.0007408391102217138]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007408391102217138

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.96362829
Iteration 2/25 | Loss: 0.00126476
Iteration 3/25 | Loss: 0.00126474
Iteration 4/25 | Loss: 0.00126474
Iteration 5/25 | Loss: 0.00126474
Iteration 6/25 | Loss: 0.00126474
Iteration 7/25 | Loss: 0.00126474
Iteration 8/25 | Loss: 0.00126474
Iteration 9/25 | Loss: 0.00126473
Iteration 10/25 | Loss: 0.00126473
Iteration 11/25 | Loss: 0.00126473
Iteration 12/25 | Loss: 0.00126473
Iteration 13/25 | Loss: 0.00126473
Iteration 14/25 | Loss: 0.00126473
Iteration 15/25 | Loss: 0.00126473
Iteration 16/25 | Loss: 0.00126473
Iteration 17/25 | Loss: 0.00126473
Iteration 18/25 | Loss: 0.00126473
Iteration 19/25 | Loss: 0.00126473
Iteration 20/25 | Loss: 0.00126473
Iteration 21/25 | Loss: 0.00126473
Iteration 22/25 | Loss: 0.00126473
Iteration 23/25 | Loss: 0.00126473
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001264734542928636, 0.001264734542928636, 0.001264734542928636, 0.001264734542928636, 0.001264734542928636]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001264734542928636

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126473
Iteration 2/1000 | Loss: 0.00002322
Iteration 3/1000 | Loss: 0.00001665
Iteration 4/1000 | Loss: 0.00001531
Iteration 5/1000 | Loss: 0.00001426
Iteration 6/1000 | Loss: 0.00001391
Iteration 7/1000 | Loss: 0.00001360
Iteration 8/1000 | Loss: 0.00001332
Iteration 9/1000 | Loss: 0.00001315
Iteration 10/1000 | Loss: 0.00001311
Iteration 11/1000 | Loss: 0.00001307
Iteration 12/1000 | Loss: 0.00001307
Iteration 13/1000 | Loss: 0.00001306
Iteration 14/1000 | Loss: 0.00001299
Iteration 15/1000 | Loss: 0.00001292
Iteration 16/1000 | Loss: 0.00001289
Iteration 17/1000 | Loss: 0.00001289
Iteration 18/1000 | Loss: 0.00001288
Iteration 19/1000 | Loss: 0.00001288
Iteration 20/1000 | Loss: 0.00001288
Iteration 21/1000 | Loss: 0.00001288
Iteration 22/1000 | Loss: 0.00001288
Iteration 23/1000 | Loss: 0.00001287
Iteration 24/1000 | Loss: 0.00001287
Iteration 25/1000 | Loss: 0.00001287
Iteration 26/1000 | Loss: 0.00001286
Iteration 27/1000 | Loss: 0.00001286
Iteration 28/1000 | Loss: 0.00001286
Iteration 29/1000 | Loss: 0.00001286
Iteration 30/1000 | Loss: 0.00001285
Iteration 31/1000 | Loss: 0.00001285
Iteration 32/1000 | Loss: 0.00001285
Iteration 33/1000 | Loss: 0.00001285
Iteration 34/1000 | Loss: 0.00001284
Iteration 35/1000 | Loss: 0.00001284
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001284
Iteration 39/1000 | Loss: 0.00001284
Iteration 40/1000 | Loss: 0.00001283
Iteration 41/1000 | Loss: 0.00001283
Iteration 42/1000 | Loss: 0.00001283
Iteration 43/1000 | Loss: 0.00001283
Iteration 44/1000 | Loss: 0.00001282
Iteration 45/1000 | Loss: 0.00001282
Iteration 46/1000 | Loss: 0.00001282
Iteration 47/1000 | Loss: 0.00001282
Iteration 48/1000 | Loss: 0.00001282
Iteration 49/1000 | Loss: 0.00001282
Iteration 50/1000 | Loss: 0.00001282
Iteration 51/1000 | Loss: 0.00001282
Iteration 52/1000 | Loss: 0.00001281
Iteration 53/1000 | Loss: 0.00001281
Iteration 54/1000 | Loss: 0.00001281
Iteration 55/1000 | Loss: 0.00001280
Iteration 56/1000 | Loss: 0.00001280
Iteration 57/1000 | Loss: 0.00001280
Iteration 58/1000 | Loss: 0.00001280
Iteration 59/1000 | Loss: 0.00001280
Iteration 60/1000 | Loss: 0.00001280
Iteration 61/1000 | Loss: 0.00001280
Iteration 62/1000 | Loss: 0.00001280
Iteration 63/1000 | Loss: 0.00001280
Iteration 64/1000 | Loss: 0.00001279
Iteration 65/1000 | Loss: 0.00001279
Iteration 66/1000 | Loss: 0.00001278
Iteration 67/1000 | Loss: 0.00001278
Iteration 68/1000 | Loss: 0.00001277
Iteration 69/1000 | Loss: 0.00001277
Iteration 70/1000 | Loss: 0.00001277
Iteration 71/1000 | Loss: 0.00001276
Iteration 72/1000 | Loss: 0.00001276
Iteration 73/1000 | Loss: 0.00001275
Iteration 74/1000 | Loss: 0.00001275
Iteration 75/1000 | Loss: 0.00001275
Iteration 76/1000 | Loss: 0.00001274
Iteration 77/1000 | Loss: 0.00001274
Iteration 78/1000 | Loss: 0.00001274
Iteration 79/1000 | Loss: 0.00001274
Iteration 80/1000 | Loss: 0.00001273
Iteration 81/1000 | Loss: 0.00001273
Iteration 82/1000 | Loss: 0.00001273
Iteration 83/1000 | Loss: 0.00001272
Iteration 84/1000 | Loss: 0.00001272
Iteration 85/1000 | Loss: 0.00001272
Iteration 86/1000 | Loss: 0.00001272
Iteration 87/1000 | Loss: 0.00001272
Iteration 88/1000 | Loss: 0.00001271
Iteration 89/1000 | Loss: 0.00001271
Iteration 90/1000 | Loss: 0.00001271
Iteration 91/1000 | Loss: 0.00001271
Iteration 92/1000 | Loss: 0.00001271
Iteration 93/1000 | Loss: 0.00001271
Iteration 94/1000 | Loss: 0.00001271
Iteration 95/1000 | Loss: 0.00001271
Iteration 96/1000 | Loss: 0.00001271
Iteration 97/1000 | Loss: 0.00001271
Iteration 98/1000 | Loss: 0.00001271
Iteration 99/1000 | Loss: 0.00001271
Iteration 100/1000 | Loss: 0.00001271
Iteration 101/1000 | Loss: 0.00001271
Iteration 102/1000 | Loss: 0.00001271
Iteration 103/1000 | Loss: 0.00001271
Iteration 104/1000 | Loss: 0.00001271
Iteration 105/1000 | Loss: 0.00001271
Iteration 106/1000 | Loss: 0.00001271
Iteration 107/1000 | Loss: 0.00001271
Iteration 108/1000 | Loss: 0.00001271
Iteration 109/1000 | Loss: 0.00001271
Iteration 110/1000 | Loss: 0.00001271
Iteration 111/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [1.2710169357887935e-05, 1.2710169357887935e-05, 1.2710169357887935e-05, 1.2710169357887935e-05, 1.2710169357887935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2710169357887935e-05

Optimization complete. Final v2v error: 3.0601212978363037 mm

Highest mean error: 3.640422821044922 mm for frame 187

Lowest mean error: 2.769489049911499 mm for frame 212

Saving results

Total time: 36.09474730491638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00599050
Iteration 2/25 | Loss: 0.00122430
Iteration 3/25 | Loss: 0.00088946
Iteration 4/25 | Loss: 0.00080296
Iteration 5/25 | Loss: 0.00076152
Iteration 6/25 | Loss: 0.00075108
Iteration 7/25 | Loss: 0.00075075
Iteration 8/25 | Loss: 0.00075117
Iteration 9/25 | Loss: 0.00074973
Iteration 10/25 | Loss: 0.00074750
Iteration 11/25 | Loss: 0.00074682
Iteration 12/25 | Loss: 0.00074610
Iteration 13/25 | Loss: 0.00074553
Iteration 14/25 | Loss: 0.00074527
Iteration 15/25 | Loss: 0.00074507
Iteration 16/25 | Loss: 0.00074498
Iteration 17/25 | Loss: 0.00074498
Iteration 18/25 | Loss: 0.00074497
Iteration 19/25 | Loss: 0.00074497
Iteration 20/25 | Loss: 0.00074497
Iteration 21/25 | Loss: 0.00074497
Iteration 22/25 | Loss: 0.00074497
Iteration 23/25 | Loss: 0.00074497
Iteration 24/25 | Loss: 0.00074497
Iteration 25/25 | Loss: 0.00074497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.74889851
Iteration 2/25 | Loss: 0.00130733
Iteration 3/25 | Loss: 0.00130729
Iteration 4/25 | Loss: 0.00130729
Iteration 5/25 | Loss: 0.00130729
Iteration 6/25 | Loss: 0.00130729
Iteration 7/25 | Loss: 0.00130729
Iteration 8/25 | Loss: 0.00130729
Iteration 9/25 | Loss: 0.00130729
Iteration 10/25 | Loss: 0.00130729
Iteration 11/25 | Loss: 0.00130729
Iteration 12/25 | Loss: 0.00130728
Iteration 13/25 | Loss: 0.00130728
Iteration 14/25 | Loss: 0.00130728
Iteration 15/25 | Loss: 0.00130728
Iteration 16/25 | Loss: 0.00130728
Iteration 17/25 | Loss: 0.00130728
Iteration 18/25 | Loss: 0.00130728
Iteration 19/25 | Loss: 0.00130728
Iteration 20/25 | Loss: 0.00130728
Iteration 21/25 | Loss: 0.00130728
Iteration 22/25 | Loss: 0.00130728
Iteration 23/25 | Loss: 0.00130728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0013072849251329899, 0.0013072849251329899, 0.0013072849251329899, 0.0013072849251329899, 0.0013072849251329899]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013072849251329899

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130728
Iteration 2/1000 | Loss: 0.00002421
Iteration 3/1000 | Loss: 0.00005692
Iteration 4/1000 | Loss: 0.00001532
Iteration 5/1000 | Loss: 0.00001416
Iteration 6/1000 | Loss: 0.00001335
Iteration 7/1000 | Loss: 0.00001304
Iteration 8/1000 | Loss: 0.00001272
Iteration 9/1000 | Loss: 0.00001268
Iteration 10/1000 | Loss: 0.00001263
Iteration 11/1000 | Loss: 0.00001246
Iteration 12/1000 | Loss: 0.00001234
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001212
Iteration 15/1000 | Loss: 0.00001211
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001205
Iteration 18/1000 | Loss: 0.00001204
Iteration 19/1000 | Loss: 0.00001203
Iteration 20/1000 | Loss: 0.00001200
Iteration 21/1000 | Loss: 0.00001199
Iteration 22/1000 | Loss: 0.00001199
Iteration 23/1000 | Loss: 0.00001198
Iteration 24/1000 | Loss: 0.00001198
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001198
Iteration 27/1000 | Loss: 0.00001197
Iteration 28/1000 | Loss: 0.00001197
Iteration 29/1000 | Loss: 0.00001197
Iteration 30/1000 | Loss: 0.00001196
Iteration 31/1000 | Loss: 0.00001196
Iteration 32/1000 | Loss: 0.00001195
Iteration 33/1000 | Loss: 0.00001195
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001195
Iteration 36/1000 | Loss: 0.00001194
Iteration 37/1000 | Loss: 0.00001194
Iteration 38/1000 | Loss: 0.00001193
Iteration 39/1000 | Loss: 0.00001193
Iteration 40/1000 | Loss: 0.00001193
Iteration 41/1000 | Loss: 0.00001192
Iteration 42/1000 | Loss: 0.00001192
Iteration 43/1000 | Loss: 0.00001192
Iteration 44/1000 | Loss: 0.00001192
Iteration 45/1000 | Loss: 0.00001191
Iteration 46/1000 | Loss: 0.00001191
Iteration 47/1000 | Loss: 0.00001191
Iteration 48/1000 | Loss: 0.00001191
Iteration 49/1000 | Loss: 0.00001191
Iteration 50/1000 | Loss: 0.00001190
Iteration 51/1000 | Loss: 0.00001190
Iteration 52/1000 | Loss: 0.00001190
Iteration 53/1000 | Loss: 0.00001190
Iteration 54/1000 | Loss: 0.00001190
Iteration 55/1000 | Loss: 0.00001190
Iteration 56/1000 | Loss: 0.00001189
Iteration 57/1000 | Loss: 0.00001188
Iteration 58/1000 | Loss: 0.00001188
Iteration 59/1000 | Loss: 0.00001188
Iteration 60/1000 | Loss: 0.00001188
Iteration 61/1000 | Loss: 0.00001188
Iteration 62/1000 | Loss: 0.00001188
Iteration 63/1000 | Loss: 0.00001188
Iteration 64/1000 | Loss: 0.00001187
Iteration 65/1000 | Loss: 0.00001187
Iteration 66/1000 | Loss: 0.00001187
Iteration 67/1000 | Loss: 0.00001187
Iteration 68/1000 | Loss: 0.00001186
Iteration 69/1000 | Loss: 0.00001186
Iteration 70/1000 | Loss: 0.00001185
Iteration 71/1000 | Loss: 0.00001185
Iteration 72/1000 | Loss: 0.00001185
Iteration 73/1000 | Loss: 0.00001184
Iteration 74/1000 | Loss: 0.00001184
Iteration 75/1000 | Loss: 0.00001184
Iteration 76/1000 | Loss: 0.00001183
Iteration 77/1000 | Loss: 0.00001183
Iteration 78/1000 | Loss: 0.00001183
Iteration 79/1000 | Loss: 0.00001183
Iteration 80/1000 | Loss: 0.00001183
Iteration 81/1000 | Loss: 0.00001182
Iteration 82/1000 | Loss: 0.00001182
Iteration 83/1000 | Loss: 0.00001181
Iteration 84/1000 | Loss: 0.00001181
Iteration 85/1000 | Loss: 0.00001181
Iteration 86/1000 | Loss: 0.00001180
Iteration 87/1000 | Loss: 0.00001180
Iteration 88/1000 | Loss: 0.00001180
Iteration 89/1000 | Loss: 0.00001180
Iteration 90/1000 | Loss: 0.00001180
Iteration 91/1000 | Loss: 0.00001180
Iteration 92/1000 | Loss: 0.00001180
Iteration 93/1000 | Loss: 0.00001180
Iteration 94/1000 | Loss: 0.00001180
Iteration 95/1000 | Loss: 0.00001180
Iteration 96/1000 | Loss: 0.00001180
Iteration 97/1000 | Loss: 0.00001180
Iteration 98/1000 | Loss: 0.00001180
Iteration 99/1000 | Loss: 0.00001180
Iteration 100/1000 | Loss: 0.00001179
Iteration 101/1000 | Loss: 0.00001179
Iteration 102/1000 | Loss: 0.00001179
Iteration 103/1000 | Loss: 0.00001179
Iteration 104/1000 | Loss: 0.00001179
Iteration 105/1000 | Loss: 0.00001179
Iteration 106/1000 | Loss: 0.00001179
Iteration 107/1000 | Loss: 0.00001179
Iteration 108/1000 | Loss: 0.00001179
Iteration 109/1000 | Loss: 0.00001178
Iteration 110/1000 | Loss: 0.00001178
Iteration 111/1000 | Loss: 0.00001178
Iteration 112/1000 | Loss: 0.00001178
Iteration 113/1000 | Loss: 0.00001178
Iteration 114/1000 | Loss: 0.00001178
Iteration 115/1000 | Loss: 0.00001178
Iteration 116/1000 | Loss: 0.00001178
Iteration 117/1000 | Loss: 0.00001178
Iteration 118/1000 | Loss: 0.00001178
Iteration 119/1000 | Loss: 0.00001178
Iteration 120/1000 | Loss: 0.00001178
Iteration 121/1000 | Loss: 0.00001178
Iteration 122/1000 | Loss: 0.00001178
Iteration 123/1000 | Loss: 0.00001178
Iteration 124/1000 | Loss: 0.00001178
Iteration 125/1000 | Loss: 0.00001177
Iteration 126/1000 | Loss: 0.00001177
Iteration 127/1000 | Loss: 0.00001177
Iteration 128/1000 | Loss: 0.00001177
Iteration 129/1000 | Loss: 0.00001177
Iteration 130/1000 | Loss: 0.00001177
Iteration 131/1000 | Loss: 0.00001177
Iteration 132/1000 | Loss: 0.00001177
Iteration 133/1000 | Loss: 0.00001177
Iteration 134/1000 | Loss: 0.00001177
Iteration 135/1000 | Loss: 0.00001177
Iteration 136/1000 | Loss: 0.00001177
Iteration 137/1000 | Loss: 0.00001177
Iteration 138/1000 | Loss: 0.00001177
Iteration 139/1000 | Loss: 0.00001177
Iteration 140/1000 | Loss: 0.00001177
Iteration 141/1000 | Loss: 0.00001177
Iteration 142/1000 | Loss: 0.00001177
Iteration 143/1000 | Loss: 0.00001177
Iteration 144/1000 | Loss: 0.00001177
Iteration 145/1000 | Loss: 0.00001176
Iteration 146/1000 | Loss: 0.00001176
Iteration 147/1000 | Loss: 0.00001176
Iteration 148/1000 | Loss: 0.00001176
Iteration 149/1000 | Loss: 0.00001176
Iteration 150/1000 | Loss: 0.00001176
Iteration 151/1000 | Loss: 0.00001176
Iteration 152/1000 | Loss: 0.00001176
Iteration 153/1000 | Loss: 0.00001176
Iteration 154/1000 | Loss: 0.00001176
Iteration 155/1000 | Loss: 0.00001176
Iteration 156/1000 | Loss: 0.00001176
Iteration 157/1000 | Loss: 0.00001176
Iteration 158/1000 | Loss: 0.00001176
Iteration 159/1000 | Loss: 0.00001176
Iteration 160/1000 | Loss: 0.00001176
Iteration 161/1000 | Loss: 0.00001176
Iteration 162/1000 | Loss: 0.00001176
Iteration 163/1000 | Loss: 0.00001176
Iteration 164/1000 | Loss: 0.00001176
Iteration 165/1000 | Loss: 0.00001176
Iteration 166/1000 | Loss: 0.00001176
Iteration 167/1000 | Loss: 0.00001176
Iteration 168/1000 | Loss: 0.00001176
Iteration 169/1000 | Loss: 0.00001176
Iteration 170/1000 | Loss: 0.00001176
Iteration 171/1000 | Loss: 0.00001176
Iteration 172/1000 | Loss: 0.00001176
Iteration 173/1000 | Loss: 0.00001176
Iteration 174/1000 | Loss: 0.00001176
Iteration 175/1000 | Loss: 0.00001176
Iteration 176/1000 | Loss: 0.00001176
Iteration 177/1000 | Loss: 0.00001176
Iteration 178/1000 | Loss: 0.00001176
Iteration 179/1000 | Loss: 0.00001176
Iteration 180/1000 | Loss: 0.00001176
Iteration 181/1000 | Loss: 0.00001176
Iteration 182/1000 | Loss: 0.00001176
Iteration 183/1000 | Loss: 0.00001176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.1756771527871024e-05, 1.1756771527871024e-05, 1.1756771527871024e-05, 1.1756771527871024e-05, 1.1756771527871024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1756771527871024e-05

Optimization complete. Final v2v error: 2.912452220916748 mm

Highest mean error: 3.3319671154022217 mm for frame 2

Lowest mean error: 2.5153071880340576 mm for frame 113

Saving results

Total time: 63.31795787811279
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00845812
Iteration 2/25 | Loss: 0.00129398
Iteration 3/25 | Loss: 0.00097702
Iteration 4/25 | Loss: 0.00090209
Iteration 5/25 | Loss: 0.00088456
Iteration 6/25 | Loss: 0.00087768
Iteration 7/25 | Loss: 0.00087261
Iteration 8/25 | Loss: 0.00086587
Iteration 9/25 | Loss: 0.00086166
Iteration 10/25 | Loss: 0.00085626
Iteration 11/25 | Loss: 0.00085444
Iteration 12/25 | Loss: 0.00085370
Iteration 13/25 | Loss: 0.00085348
Iteration 14/25 | Loss: 0.00085333
Iteration 15/25 | Loss: 0.00085322
Iteration 16/25 | Loss: 0.00085310
Iteration 17/25 | Loss: 0.00085280
Iteration 18/25 | Loss: 0.00085627
Iteration 19/25 | Loss: 0.00085435
Iteration 20/25 | Loss: 0.00085244
Iteration 21/25 | Loss: 0.00085528
Iteration 22/25 | Loss: 0.00085398
Iteration 23/25 | Loss: 0.00085250
Iteration 24/25 | Loss: 0.00085189
Iteration 25/25 | Loss: 0.00085121

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68246162
Iteration 2/25 | Loss: 0.00155603
Iteration 3/25 | Loss: 0.00155602
Iteration 4/25 | Loss: 0.00155602
Iteration 5/25 | Loss: 0.00155602
Iteration 6/25 | Loss: 0.00155602
Iteration 7/25 | Loss: 0.00155602
Iteration 8/25 | Loss: 0.00155602
Iteration 9/25 | Loss: 0.00155602
Iteration 10/25 | Loss: 0.00155602
Iteration 11/25 | Loss: 0.00155602
Iteration 12/25 | Loss: 0.00155602
Iteration 13/25 | Loss: 0.00155602
Iteration 14/25 | Loss: 0.00155602
Iteration 15/25 | Loss: 0.00155602
Iteration 16/25 | Loss: 0.00155602
Iteration 17/25 | Loss: 0.00155602
Iteration 18/25 | Loss: 0.00155602
Iteration 19/25 | Loss: 0.00155602
Iteration 20/25 | Loss: 0.00155602
Iteration 21/25 | Loss: 0.00155602
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0015560162719339132, 0.0015560162719339132, 0.0015560162719339132, 0.0015560162719339132, 0.0015560162719339132]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015560162719339132

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00155602
Iteration 2/1000 | Loss: 0.00008838
Iteration 3/1000 | Loss: 0.00006250
Iteration 4/1000 | Loss: 0.00005508
Iteration 5/1000 | Loss: 0.00005029
Iteration 6/1000 | Loss: 0.00007298
Iteration 7/1000 | Loss: 0.00005264
Iteration 8/1000 | Loss: 0.00004531
Iteration 9/1000 | Loss: 0.00004142
Iteration 10/1000 | Loss: 0.00003878
Iteration 11/1000 | Loss: 0.00003754
Iteration 12/1000 | Loss: 0.00003626
Iteration 13/1000 | Loss: 0.00049571
Iteration 14/1000 | Loss: 0.00174610
Iteration 15/1000 | Loss: 0.00013804
Iteration 16/1000 | Loss: 0.00007529
Iteration 17/1000 | Loss: 0.00004568
Iteration 18/1000 | Loss: 0.00041369
Iteration 19/1000 | Loss: 0.00006724
Iteration 20/1000 | Loss: 0.00004411
Iteration 21/1000 | Loss: 0.00003800
Iteration 22/1000 | Loss: 0.00003616
Iteration 23/1000 | Loss: 0.00003499
Iteration 24/1000 | Loss: 0.00036342
Iteration 25/1000 | Loss: 0.00035791
Iteration 26/1000 | Loss: 0.00053813
Iteration 27/1000 | Loss: 0.00015273
Iteration 28/1000 | Loss: 0.00006611
Iteration 29/1000 | Loss: 0.00008358
Iteration 30/1000 | Loss: 0.00006821
Iteration 31/1000 | Loss: 0.00005432
Iteration 32/1000 | Loss: 0.00005741
Iteration 33/1000 | Loss: 0.00079977
Iteration 34/1000 | Loss: 0.00067248
Iteration 35/1000 | Loss: 0.00020362
Iteration 36/1000 | Loss: 0.00005610
Iteration 37/1000 | Loss: 0.00040843
Iteration 38/1000 | Loss: 0.00029269
Iteration 39/1000 | Loss: 0.00041631
Iteration 40/1000 | Loss: 0.00044700
Iteration 41/1000 | Loss: 0.00035938
Iteration 42/1000 | Loss: 0.00004876
Iteration 43/1000 | Loss: 0.00013740
Iteration 44/1000 | Loss: 0.00007724
Iteration 45/1000 | Loss: 0.00017614
Iteration 46/1000 | Loss: 0.00020907
Iteration 47/1000 | Loss: 0.00029893
Iteration 48/1000 | Loss: 0.00025958
Iteration 49/1000 | Loss: 0.00022457
Iteration 50/1000 | Loss: 0.00015145
Iteration 51/1000 | Loss: 0.00017390
Iteration 52/1000 | Loss: 0.00030560
Iteration 53/1000 | Loss: 0.00007046
Iteration 54/1000 | Loss: 0.00010722
Iteration 55/1000 | Loss: 0.00007399
Iteration 56/1000 | Loss: 0.00007306
Iteration 57/1000 | Loss: 0.00016465
Iteration 58/1000 | Loss: 0.00007958
Iteration 59/1000 | Loss: 0.00006696
Iteration 60/1000 | Loss: 0.00026904
Iteration 61/1000 | Loss: 0.00019090
Iteration 62/1000 | Loss: 0.00016039
Iteration 63/1000 | Loss: 0.00031209
Iteration 64/1000 | Loss: 0.00003376
Iteration 65/1000 | Loss: 0.00018581
Iteration 66/1000 | Loss: 0.00008908
Iteration 67/1000 | Loss: 0.00003205
Iteration 68/1000 | Loss: 0.00003124
Iteration 69/1000 | Loss: 0.00011087
Iteration 70/1000 | Loss: 0.00006666
Iteration 71/1000 | Loss: 0.00010065
Iteration 72/1000 | Loss: 0.00020187
Iteration 73/1000 | Loss: 0.00006876
Iteration 74/1000 | Loss: 0.00034219
Iteration 75/1000 | Loss: 0.00030367
Iteration 76/1000 | Loss: 0.00035010
Iteration 77/1000 | Loss: 0.00061749
Iteration 78/1000 | Loss: 0.00022906
Iteration 79/1000 | Loss: 0.00024550
Iteration 80/1000 | Loss: 0.00006280
Iteration 81/1000 | Loss: 0.00003959
Iteration 82/1000 | Loss: 0.00003285
Iteration 83/1000 | Loss: 0.00002983
Iteration 84/1000 | Loss: 0.00031165
Iteration 85/1000 | Loss: 0.00007146
Iteration 86/1000 | Loss: 0.00019454
Iteration 87/1000 | Loss: 0.00009105
Iteration 88/1000 | Loss: 0.00003707
Iteration 89/1000 | Loss: 0.00003113
Iteration 90/1000 | Loss: 0.00016010
Iteration 91/1000 | Loss: 0.00007803
Iteration 92/1000 | Loss: 0.00005362
Iteration 93/1000 | Loss: 0.00003725
Iteration 94/1000 | Loss: 0.00002879
Iteration 95/1000 | Loss: 0.00002808
Iteration 96/1000 | Loss: 0.00002788
Iteration 97/1000 | Loss: 0.00002763
Iteration 98/1000 | Loss: 0.00002742
Iteration 99/1000 | Loss: 0.00002739
Iteration 100/1000 | Loss: 0.00002735
Iteration 101/1000 | Loss: 0.00008846
Iteration 102/1000 | Loss: 0.00003973
Iteration 103/1000 | Loss: 0.00032995
Iteration 104/1000 | Loss: 0.00013784
Iteration 105/1000 | Loss: 0.00002824
Iteration 106/1000 | Loss: 0.00012374
Iteration 107/1000 | Loss: 0.00026875
Iteration 108/1000 | Loss: 0.00032559
Iteration 109/1000 | Loss: 0.00026105
Iteration 110/1000 | Loss: 0.00033080
Iteration 111/1000 | Loss: 0.00015948
Iteration 112/1000 | Loss: 0.00015346
Iteration 113/1000 | Loss: 0.00009809
Iteration 114/1000 | Loss: 0.00026179
Iteration 115/1000 | Loss: 0.00013651
Iteration 116/1000 | Loss: 0.00023282
Iteration 117/1000 | Loss: 0.00016694
Iteration 118/1000 | Loss: 0.00014710
Iteration 119/1000 | Loss: 0.00009452
Iteration 120/1000 | Loss: 0.00002829
Iteration 121/1000 | Loss: 0.00014173
Iteration 122/1000 | Loss: 0.00022966
Iteration 123/1000 | Loss: 0.00009993
Iteration 124/1000 | Loss: 0.00002885
Iteration 125/1000 | Loss: 0.00021134
Iteration 126/1000 | Loss: 0.00006766
Iteration 127/1000 | Loss: 0.00013534
Iteration 128/1000 | Loss: 0.00006686
Iteration 129/1000 | Loss: 0.00010012
Iteration 130/1000 | Loss: 0.00006043
Iteration 131/1000 | Loss: 0.00012089
Iteration 132/1000 | Loss: 0.00005426
Iteration 133/1000 | Loss: 0.00016469
Iteration 134/1000 | Loss: 0.00011631
Iteration 135/1000 | Loss: 0.00003199
Iteration 136/1000 | Loss: 0.00002919
Iteration 137/1000 | Loss: 0.00009827
Iteration 138/1000 | Loss: 0.00016394
Iteration 139/1000 | Loss: 0.00024295
Iteration 140/1000 | Loss: 0.00006504
Iteration 141/1000 | Loss: 0.00002824
Iteration 142/1000 | Loss: 0.00002760
Iteration 143/1000 | Loss: 0.00002738
Iteration 144/1000 | Loss: 0.00002736
Iteration 145/1000 | Loss: 0.00002736
Iteration 146/1000 | Loss: 0.00002736
Iteration 147/1000 | Loss: 0.00002735
Iteration 148/1000 | Loss: 0.00002735
Iteration 149/1000 | Loss: 0.00002735
Iteration 150/1000 | Loss: 0.00002735
Iteration 151/1000 | Loss: 0.00002735
Iteration 152/1000 | Loss: 0.00002734
Iteration 153/1000 | Loss: 0.00002734
Iteration 154/1000 | Loss: 0.00002734
Iteration 155/1000 | Loss: 0.00002734
Iteration 156/1000 | Loss: 0.00002734
Iteration 157/1000 | Loss: 0.00002734
Iteration 158/1000 | Loss: 0.00002734
Iteration 159/1000 | Loss: 0.00002733
Iteration 160/1000 | Loss: 0.00002733
Iteration 161/1000 | Loss: 0.00002733
Iteration 162/1000 | Loss: 0.00002733
Iteration 163/1000 | Loss: 0.00002733
Iteration 164/1000 | Loss: 0.00002733
Iteration 165/1000 | Loss: 0.00002733
Iteration 166/1000 | Loss: 0.00002733
Iteration 167/1000 | Loss: 0.00002733
Iteration 168/1000 | Loss: 0.00002733
Iteration 169/1000 | Loss: 0.00002733
Iteration 170/1000 | Loss: 0.00002733
Iteration 171/1000 | Loss: 0.00002733
Iteration 172/1000 | Loss: 0.00002733
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.732818029471673e-05, 2.732818029471673e-05, 2.732818029471673e-05, 2.732818029471673e-05, 2.732818029471673e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.732818029471673e-05

Optimization complete. Final v2v error: 4.149131774902344 mm

Highest mean error: 12.136160850524902 mm for frame 167

Lowest mean error: 3.5033504962921143 mm for frame 148

Saving results

Total time: 281.1606652736664
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891217
Iteration 2/25 | Loss: 0.00241964
Iteration 3/25 | Loss: 0.00150932
Iteration 4/25 | Loss: 0.00125013
Iteration 5/25 | Loss: 0.00119656
Iteration 6/25 | Loss: 0.00108051
Iteration 7/25 | Loss: 0.00103577
Iteration 8/25 | Loss: 0.00096305
Iteration 9/25 | Loss: 0.00091393
Iteration 10/25 | Loss: 0.00089709
Iteration 11/25 | Loss: 0.00088296
Iteration 12/25 | Loss: 0.00087788
Iteration 13/25 | Loss: 0.00086735
Iteration 14/25 | Loss: 0.00086096
Iteration 15/25 | Loss: 0.00085889
Iteration 16/25 | Loss: 0.00085523
Iteration 17/25 | Loss: 0.00085427
Iteration 18/25 | Loss: 0.00085407
Iteration 19/25 | Loss: 0.00085402
Iteration 20/25 | Loss: 0.00085402
Iteration 21/25 | Loss: 0.00085402
Iteration 22/25 | Loss: 0.00085402
Iteration 23/25 | Loss: 0.00085402
Iteration 24/25 | Loss: 0.00085401
Iteration 25/25 | Loss: 0.00085401

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23901904
Iteration 2/25 | Loss: 0.00136935
Iteration 3/25 | Loss: 0.00136934
Iteration 4/25 | Loss: 0.00136934
Iteration 5/25 | Loss: 0.00136934
Iteration 6/25 | Loss: 0.00136934
Iteration 7/25 | Loss: 0.00136934
Iteration 8/25 | Loss: 0.00136934
Iteration 9/25 | Loss: 0.00136934
Iteration 10/25 | Loss: 0.00136934
Iteration 11/25 | Loss: 0.00136934
Iteration 12/25 | Loss: 0.00136934
Iteration 13/25 | Loss: 0.00136934
Iteration 14/25 | Loss: 0.00136934
Iteration 15/25 | Loss: 0.00136934
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0013693412765860558, 0.0013693412765860558, 0.0013693412765860558, 0.0013693412765860558, 0.0013693412765860558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013693412765860558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00136934
Iteration 2/1000 | Loss: 0.00003227
Iteration 3/1000 | Loss: 0.00002290
Iteration 4/1000 | Loss: 0.00002064
Iteration 5/1000 | Loss: 0.00001973
Iteration 6/1000 | Loss: 0.00001911
Iteration 7/1000 | Loss: 0.00001885
Iteration 8/1000 | Loss: 0.00001855
Iteration 9/1000 | Loss: 0.00001835
Iteration 10/1000 | Loss: 0.00001832
Iteration 11/1000 | Loss: 0.00001826
Iteration 12/1000 | Loss: 0.00001821
Iteration 13/1000 | Loss: 0.00001821
Iteration 14/1000 | Loss: 0.00001821
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001820
Iteration 17/1000 | Loss: 0.00001820
Iteration 18/1000 | Loss: 0.00001816
Iteration 19/1000 | Loss: 0.00001812
Iteration 20/1000 | Loss: 0.00001808
Iteration 21/1000 | Loss: 0.00001804
Iteration 22/1000 | Loss: 0.00001801
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001792
Iteration 25/1000 | Loss: 0.00001791
Iteration 26/1000 | Loss: 0.00001790
Iteration 27/1000 | Loss: 0.00001790
Iteration 28/1000 | Loss: 0.00001789
Iteration 29/1000 | Loss: 0.00001789
Iteration 30/1000 | Loss: 0.00001788
Iteration 31/1000 | Loss: 0.00001787
Iteration 32/1000 | Loss: 0.00001787
Iteration 33/1000 | Loss: 0.00001787
Iteration 34/1000 | Loss: 0.00001787
Iteration 35/1000 | Loss: 0.00001787
Iteration 36/1000 | Loss: 0.00001787
Iteration 37/1000 | Loss: 0.00001787
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 37. Stopping optimization.
Last 5 losses: [1.787215660442598e-05, 1.787215660442598e-05, 1.787215660442598e-05, 1.787215660442598e-05, 1.787215660442598e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.787215660442598e-05

Optimization complete. Final v2v error: 3.4997494220733643 mm

Highest mean error: 4.34458065032959 mm for frame 99

Lowest mean error: 3.0856168270111084 mm for frame 211

Saving results

Total time: 60.68544149398804
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00846144
Iteration 2/25 | Loss: 0.00127726
Iteration 3/25 | Loss: 0.00098591
Iteration 4/25 | Loss: 0.00091193
Iteration 5/25 | Loss: 0.00088650
Iteration 6/25 | Loss: 0.00088135
Iteration 7/25 | Loss: 0.00088020
Iteration 8/25 | Loss: 0.00088020
Iteration 9/25 | Loss: 0.00088020
Iteration 10/25 | Loss: 0.00088020
Iteration 11/25 | Loss: 0.00088020
Iteration 12/25 | Loss: 0.00088020
Iteration 13/25 | Loss: 0.00088020
Iteration 14/25 | Loss: 0.00088020
Iteration 15/25 | Loss: 0.00088020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008801965741440654, 0.0008801965741440654, 0.0008801965741440654, 0.0008801965741440654, 0.0008801965741440654]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008801965741440654

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60101199
Iteration 2/25 | Loss: 0.00138558
Iteration 3/25 | Loss: 0.00138558
Iteration 4/25 | Loss: 0.00138558
Iteration 5/25 | Loss: 0.00138558
Iteration 6/25 | Loss: 0.00138558
Iteration 7/25 | Loss: 0.00138558
Iteration 8/25 | Loss: 0.00138558
Iteration 9/25 | Loss: 0.00138558
Iteration 10/25 | Loss: 0.00138558
Iteration 11/25 | Loss: 0.00138558
Iteration 12/25 | Loss: 0.00138558
Iteration 13/25 | Loss: 0.00138558
Iteration 14/25 | Loss: 0.00138558
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0013855788856744766, 0.0013855788856744766, 0.0013855788856744766, 0.0013855788856744766, 0.0013855788856744766]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013855788856744766

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138558
Iteration 2/1000 | Loss: 0.00003961
Iteration 3/1000 | Loss: 0.00002647
Iteration 4/1000 | Loss: 0.00002488
Iteration 5/1000 | Loss: 0.00002354
Iteration 6/1000 | Loss: 0.00002282
Iteration 7/1000 | Loss: 0.00002224
Iteration 8/1000 | Loss: 0.00002193
Iteration 9/1000 | Loss: 0.00002163
Iteration 10/1000 | Loss: 0.00002137
Iteration 11/1000 | Loss: 0.00002117
Iteration 12/1000 | Loss: 0.00002110
Iteration 13/1000 | Loss: 0.00002105
Iteration 14/1000 | Loss: 0.00002104
Iteration 15/1000 | Loss: 0.00002102
Iteration 16/1000 | Loss: 0.00002100
Iteration 17/1000 | Loss: 0.00002099
Iteration 18/1000 | Loss: 0.00002098
Iteration 19/1000 | Loss: 0.00002098
Iteration 20/1000 | Loss: 0.00002097
Iteration 21/1000 | Loss: 0.00002097
Iteration 22/1000 | Loss: 0.00002096
Iteration 23/1000 | Loss: 0.00002095
Iteration 24/1000 | Loss: 0.00002095
Iteration 25/1000 | Loss: 0.00002095
Iteration 26/1000 | Loss: 0.00002094
Iteration 27/1000 | Loss: 0.00002094
Iteration 28/1000 | Loss: 0.00002094
Iteration 29/1000 | Loss: 0.00002093
Iteration 30/1000 | Loss: 0.00002093
Iteration 31/1000 | Loss: 0.00002093
Iteration 32/1000 | Loss: 0.00002093
Iteration 33/1000 | Loss: 0.00002093
Iteration 34/1000 | Loss: 0.00002093
Iteration 35/1000 | Loss: 0.00002093
Iteration 36/1000 | Loss: 0.00002093
Iteration 37/1000 | Loss: 0.00002093
Iteration 38/1000 | Loss: 0.00002092
Iteration 39/1000 | Loss: 0.00002092
Iteration 40/1000 | Loss: 0.00002092
Iteration 41/1000 | Loss: 0.00002092
Iteration 42/1000 | Loss: 0.00002092
Iteration 43/1000 | Loss: 0.00002092
Iteration 44/1000 | Loss: 0.00002092
Iteration 45/1000 | Loss: 0.00002092
Iteration 46/1000 | Loss: 0.00002092
Iteration 47/1000 | Loss: 0.00002092
Iteration 48/1000 | Loss: 0.00002092
Iteration 49/1000 | Loss: 0.00002092
Iteration 50/1000 | Loss: 0.00002092
Iteration 51/1000 | Loss: 0.00002092
Iteration 52/1000 | Loss: 0.00002091
Iteration 53/1000 | Loss: 0.00002091
Iteration 54/1000 | Loss: 0.00002091
Iteration 55/1000 | Loss: 0.00002091
Iteration 56/1000 | Loss: 0.00002091
Iteration 57/1000 | Loss: 0.00002091
Iteration 58/1000 | Loss: 0.00002091
Iteration 59/1000 | Loss: 0.00002091
Iteration 60/1000 | Loss: 0.00002091
Iteration 61/1000 | Loss: 0.00002091
Iteration 62/1000 | Loss: 0.00002090
Iteration 63/1000 | Loss: 0.00002090
Iteration 64/1000 | Loss: 0.00002090
Iteration 65/1000 | Loss: 0.00002090
Iteration 66/1000 | Loss: 0.00002090
Iteration 67/1000 | Loss: 0.00002090
Iteration 68/1000 | Loss: 0.00002090
Iteration 69/1000 | Loss: 0.00002090
Iteration 70/1000 | Loss: 0.00002090
Iteration 71/1000 | Loss: 0.00002089
Iteration 72/1000 | Loss: 0.00002089
Iteration 73/1000 | Loss: 0.00002089
Iteration 74/1000 | Loss: 0.00002089
Iteration 75/1000 | Loss: 0.00002089
Iteration 76/1000 | Loss: 0.00002089
Iteration 77/1000 | Loss: 0.00002089
Iteration 78/1000 | Loss: 0.00002089
Iteration 79/1000 | Loss: 0.00002089
Iteration 80/1000 | Loss: 0.00002089
Iteration 81/1000 | Loss: 0.00002089
Iteration 82/1000 | Loss: 0.00002089
Iteration 83/1000 | Loss: 0.00002089
Iteration 84/1000 | Loss: 0.00002089
Iteration 85/1000 | Loss: 0.00002089
Iteration 86/1000 | Loss: 0.00002089
Iteration 87/1000 | Loss: 0.00002089
Iteration 88/1000 | Loss: 0.00002089
Iteration 89/1000 | Loss: 0.00002089
Iteration 90/1000 | Loss: 0.00002089
Iteration 91/1000 | Loss: 0.00002089
Iteration 92/1000 | Loss: 0.00002089
Iteration 93/1000 | Loss: 0.00002089
Iteration 94/1000 | Loss: 0.00002089
Iteration 95/1000 | Loss: 0.00002089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.089157896989491e-05, 2.089157896989491e-05, 2.089157896989491e-05, 2.089157896989491e-05, 2.089157896989491e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.089157896989491e-05

Optimization complete. Final v2v error: 3.7648589611053467 mm

Highest mean error: 4.136034965515137 mm for frame 191

Lowest mean error: 3.5511491298675537 mm for frame 217

Saving results

Total time: 37.03733015060425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00995775
Iteration 2/25 | Loss: 0.00221740
Iteration 3/25 | Loss: 0.00156512
Iteration 4/25 | Loss: 0.00136365
Iteration 5/25 | Loss: 0.00122707
Iteration 6/25 | Loss: 0.00116864
Iteration 7/25 | Loss: 0.00111103
Iteration 8/25 | Loss: 0.00109324
Iteration 9/25 | Loss: 0.00110569
Iteration 10/25 | Loss: 0.00109765
Iteration 11/25 | Loss: 0.00109219
Iteration 12/25 | Loss: 0.00108269
Iteration 13/25 | Loss: 0.00109578
Iteration 14/25 | Loss: 0.00107376
Iteration 15/25 | Loss: 0.00102154
Iteration 16/25 | Loss: 0.00100477
Iteration 17/25 | Loss: 0.00100206
Iteration 18/25 | Loss: 0.00099638
Iteration 19/25 | Loss: 0.00103438
Iteration 20/25 | Loss: 0.00101641
Iteration 21/25 | Loss: 0.00101764
Iteration 22/25 | Loss: 0.00098561
Iteration 23/25 | Loss: 0.00096120
Iteration 24/25 | Loss: 0.00095162
Iteration 25/25 | Loss: 0.00094950

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64779472
Iteration 2/25 | Loss: 0.00229406
Iteration 3/25 | Loss: 0.00219925
Iteration 4/25 | Loss: 0.00219925
Iteration 5/25 | Loss: 0.00219925
Iteration 6/25 | Loss: 0.00219924
Iteration 7/25 | Loss: 0.00219924
Iteration 8/25 | Loss: 0.00219924
Iteration 9/25 | Loss: 0.00219924
Iteration 10/25 | Loss: 0.00219924
Iteration 11/25 | Loss: 0.00219924
Iteration 12/25 | Loss: 0.00219924
Iteration 13/25 | Loss: 0.00219924
Iteration 14/25 | Loss: 0.00219924
Iteration 15/25 | Loss: 0.00219924
Iteration 16/25 | Loss: 0.00219924
Iteration 17/25 | Loss: 0.00219924
Iteration 18/25 | Loss: 0.00219924
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021992423571646214, 0.0021992423571646214, 0.0021992423571646214, 0.0021992423571646214, 0.0021992423571646214]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021992423571646214

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00219924
Iteration 2/1000 | Loss: 0.00013671
Iteration 3/1000 | Loss: 0.00017674
Iteration 4/1000 | Loss: 0.00008751
Iteration 5/1000 | Loss: 0.00030918
Iteration 6/1000 | Loss: 0.00007782
Iteration 7/1000 | Loss: 0.00007035
Iteration 8/1000 | Loss: 0.00032210
Iteration 9/1000 | Loss: 0.00007087
Iteration 10/1000 | Loss: 0.00006383
Iteration 11/1000 | Loss: 0.00006084
Iteration 12/1000 | Loss: 0.00005927
Iteration 13/1000 | Loss: 0.00005795
Iteration 14/1000 | Loss: 0.00005733
Iteration 15/1000 | Loss: 0.00005652
Iteration 16/1000 | Loss: 0.00005567
Iteration 17/1000 | Loss: 0.00005494
Iteration 18/1000 | Loss: 0.00005438
Iteration 19/1000 | Loss: 0.00178254
Iteration 20/1000 | Loss: 0.00501491
Iteration 21/1000 | Loss: 0.00011085
Iteration 22/1000 | Loss: 0.00007219
Iteration 23/1000 | Loss: 0.00006009
Iteration 24/1000 | Loss: 0.00007177
Iteration 25/1000 | Loss: 0.00007969
Iteration 26/1000 | Loss: 0.00003897
Iteration 27/1000 | Loss: 0.00007675
Iteration 28/1000 | Loss: 0.00003397
Iteration 29/1000 | Loss: 0.00003242
Iteration 30/1000 | Loss: 0.00004038
Iteration 31/1000 | Loss: 0.00003243
Iteration 32/1000 | Loss: 0.00002996
Iteration 33/1000 | Loss: 0.00002926
Iteration 34/1000 | Loss: 0.00002946
Iteration 35/1000 | Loss: 0.00002912
Iteration 36/1000 | Loss: 0.00002841
Iteration 37/1000 | Loss: 0.00002828
Iteration 38/1000 | Loss: 0.00002801
Iteration 39/1000 | Loss: 0.00002796
Iteration 40/1000 | Loss: 0.00002791
Iteration 41/1000 | Loss: 0.00002790
Iteration 42/1000 | Loss: 0.00002785
Iteration 43/1000 | Loss: 0.00002785
Iteration 44/1000 | Loss: 0.00002778
Iteration 45/1000 | Loss: 0.00002777
Iteration 46/1000 | Loss: 0.00002775
Iteration 47/1000 | Loss: 0.00002774
Iteration 48/1000 | Loss: 0.00002774
Iteration 49/1000 | Loss: 0.00002773
Iteration 50/1000 | Loss: 0.00002773
Iteration 51/1000 | Loss: 0.00002768
Iteration 52/1000 | Loss: 0.00002768
Iteration 53/1000 | Loss: 0.00002768
Iteration 54/1000 | Loss: 0.00002768
Iteration 55/1000 | Loss: 0.00002768
Iteration 56/1000 | Loss: 0.00002767
Iteration 57/1000 | Loss: 0.00002767
Iteration 58/1000 | Loss: 0.00002767
Iteration 59/1000 | Loss: 0.00002767
Iteration 60/1000 | Loss: 0.00002766
Iteration 61/1000 | Loss: 0.00002766
Iteration 62/1000 | Loss: 0.00002765
Iteration 63/1000 | Loss: 0.00002765
Iteration 64/1000 | Loss: 0.00002765
Iteration 65/1000 | Loss: 0.00002764
Iteration 66/1000 | Loss: 0.00002764
Iteration 67/1000 | Loss: 0.00002763
Iteration 68/1000 | Loss: 0.00002763
Iteration 69/1000 | Loss: 0.00002763
Iteration 70/1000 | Loss: 0.00002762
Iteration 71/1000 | Loss: 0.00002762
Iteration 72/1000 | Loss: 0.00002761
Iteration 73/1000 | Loss: 0.00002761
Iteration 74/1000 | Loss: 0.00002761
Iteration 75/1000 | Loss: 0.00002760
Iteration 76/1000 | Loss: 0.00002760
Iteration 77/1000 | Loss: 0.00002759
Iteration 78/1000 | Loss: 0.00002759
Iteration 79/1000 | Loss: 0.00002759
Iteration 80/1000 | Loss: 0.00002758
Iteration 81/1000 | Loss: 0.00002758
Iteration 82/1000 | Loss: 0.00002758
Iteration 83/1000 | Loss: 0.00002757
Iteration 84/1000 | Loss: 0.00002757
Iteration 85/1000 | Loss: 0.00002757
Iteration 86/1000 | Loss: 0.00002756
Iteration 87/1000 | Loss: 0.00002756
Iteration 88/1000 | Loss: 0.00002756
Iteration 89/1000 | Loss: 0.00002756
Iteration 90/1000 | Loss: 0.00002756
Iteration 91/1000 | Loss: 0.00002755
Iteration 92/1000 | Loss: 0.00002755
Iteration 93/1000 | Loss: 0.00002755
Iteration 94/1000 | Loss: 0.00002755
Iteration 95/1000 | Loss: 0.00002754
Iteration 96/1000 | Loss: 0.00003356
Iteration 97/1000 | Loss: 0.00003356
Iteration 98/1000 | Loss: 0.00002756
Iteration 99/1000 | Loss: 0.00002749
Iteration 100/1000 | Loss: 0.00002749
Iteration 101/1000 | Loss: 0.00002748
Iteration 102/1000 | Loss: 0.00002748
Iteration 103/1000 | Loss: 0.00002748
Iteration 104/1000 | Loss: 0.00002748
Iteration 105/1000 | Loss: 0.00002748
Iteration 106/1000 | Loss: 0.00002748
Iteration 107/1000 | Loss: 0.00002748
Iteration 108/1000 | Loss: 0.00002748
Iteration 109/1000 | Loss: 0.00002748
Iteration 110/1000 | Loss: 0.00002748
Iteration 111/1000 | Loss: 0.00002748
Iteration 112/1000 | Loss: 0.00002748
Iteration 113/1000 | Loss: 0.00002748
Iteration 114/1000 | Loss: 0.00002748
Iteration 115/1000 | Loss: 0.00002748
Iteration 116/1000 | Loss: 0.00002748
Iteration 117/1000 | Loss: 0.00002748
Iteration 118/1000 | Loss: 0.00002748
Iteration 119/1000 | Loss: 0.00002748
Iteration 120/1000 | Loss: 0.00002748
Iteration 121/1000 | Loss: 0.00002748
Iteration 122/1000 | Loss: 0.00002748
Iteration 123/1000 | Loss: 0.00002748
Iteration 124/1000 | Loss: 0.00002748
Iteration 125/1000 | Loss: 0.00002748
Iteration 126/1000 | Loss: 0.00002748
Iteration 127/1000 | Loss: 0.00002748
Iteration 128/1000 | Loss: 0.00002748
Iteration 129/1000 | Loss: 0.00002748
Iteration 130/1000 | Loss: 0.00002748
Iteration 131/1000 | Loss: 0.00002748
Iteration 132/1000 | Loss: 0.00002748
Iteration 133/1000 | Loss: 0.00002748
Iteration 134/1000 | Loss: 0.00002748
Iteration 135/1000 | Loss: 0.00002748
Iteration 136/1000 | Loss: 0.00002748
Iteration 137/1000 | Loss: 0.00002748
Iteration 138/1000 | Loss: 0.00002748
Iteration 139/1000 | Loss: 0.00002748
Iteration 140/1000 | Loss: 0.00002748
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [2.7478954507387243e-05, 2.7478954507387243e-05, 2.7478954507387243e-05, 2.7478954507387243e-05, 2.7478954507387243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7478954507387243e-05

Optimization complete. Final v2v error: 4.24640417098999 mm

Highest mean error: 5.386809349060059 mm for frame 45

Lowest mean error: 3.212343692779541 mm for frame 3

Saving results

Total time: 111.54501676559448
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00911364
Iteration 2/25 | Loss: 0.00231536
Iteration 3/25 | Loss: 0.00170015
Iteration 4/25 | Loss: 0.00130860
Iteration 5/25 | Loss: 0.00125208
Iteration 6/25 | Loss: 0.00125262
Iteration 7/25 | Loss: 0.00123242
Iteration 8/25 | Loss: 0.00122230
Iteration 9/25 | Loss: 0.00123507
Iteration 10/25 | Loss: 0.00120306
Iteration 11/25 | Loss: 0.00120537
Iteration 12/25 | Loss: 0.00117870
Iteration 13/25 | Loss: 0.00116909
Iteration 14/25 | Loss: 0.00116078
Iteration 15/25 | Loss: 0.00115371
Iteration 16/25 | Loss: 0.00114508
Iteration 17/25 | Loss: 0.00114144
Iteration 18/25 | Loss: 0.00114033
Iteration 19/25 | Loss: 0.00114094
Iteration 20/25 | Loss: 0.00113961
Iteration 21/25 | Loss: 0.00113978
Iteration 22/25 | Loss: 0.00114185
Iteration 23/25 | Loss: 0.00113944
Iteration 24/25 | Loss: 0.00113990
Iteration 25/25 | Loss: 0.00114105

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.53452682
Iteration 2/25 | Loss: 0.00479018
Iteration 3/25 | Loss: 0.00459966
Iteration 4/25 | Loss: 0.00459966
Iteration 5/25 | Loss: 0.00459966
Iteration 6/25 | Loss: 0.00459966
Iteration 7/25 | Loss: 0.00459966
Iteration 8/25 | Loss: 0.00459966
Iteration 9/25 | Loss: 0.00459966
Iteration 10/25 | Loss: 0.00459966
Iteration 11/25 | Loss: 0.00459966
Iteration 12/25 | Loss: 0.00459966
Iteration 13/25 | Loss: 0.00459966
Iteration 14/25 | Loss: 0.00459966
Iteration 15/25 | Loss: 0.00459966
Iteration 16/25 | Loss: 0.00459966
Iteration 17/25 | Loss: 0.00459966
Iteration 18/25 | Loss: 0.00459966
Iteration 19/25 | Loss: 0.00459966
Iteration 20/25 | Loss: 0.00459966
Iteration 21/25 | Loss: 0.00459966
Iteration 22/25 | Loss: 0.00459966
Iteration 23/25 | Loss: 0.00459966
Iteration 24/25 | Loss: 0.00459966
Iteration 25/25 | Loss: 0.00459966

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00459966
Iteration 2/1000 | Loss: 0.00096967
Iteration 3/1000 | Loss: 0.00168821
Iteration 4/1000 | Loss: 0.00132075
Iteration 5/1000 | Loss: 0.00022173
Iteration 6/1000 | Loss: 0.00129049
Iteration 7/1000 | Loss: 0.00081295
Iteration 8/1000 | Loss: 0.00028312
Iteration 9/1000 | Loss: 0.00034733
Iteration 10/1000 | Loss: 0.00020341
Iteration 11/1000 | Loss: 0.00007001
Iteration 12/1000 | Loss: 0.00087163
Iteration 13/1000 | Loss: 0.00006625
Iteration 14/1000 | Loss: 0.00104647
Iteration 15/1000 | Loss: 0.00031222
Iteration 16/1000 | Loss: 0.00065946
Iteration 17/1000 | Loss: 0.00030306
Iteration 18/1000 | Loss: 0.00038679
Iteration 19/1000 | Loss: 0.00005472
Iteration 20/1000 | Loss: 0.00004674
Iteration 21/1000 | Loss: 0.00004236
Iteration 22/1000 | Loss: 0.00035037
Iteration 23/1000 | Loss: 0.00004604
Iteration 24/1000 | Loss: 0.00003975
Iteration 25/1000 | Loss: 0.00003688
Iteration 26/1000 | Loss: 0.00034025
Iteration 27/1000 | Loss: 0.00004567
Iteration 28/1000 | Loss: 0.00003519
Iteration 29/1000 | Loss: 0.00003299
Iteration 30/1000 | Loss: 0.00003163
Iteration 31/1000 | Loss: 0.00003074
Iteration 32/1000 | Loss: 0.00035932
Iteration 33/1000 | Loss: 0.00005467
Iteration 34/1000 | Loss: 0.00032512
Iteration 35/1000 | Loss: 0.00005618
Iteration 36/1000 | Loss: 0.00004120
Iteration 37/1000 | Loss: 0.00003435
Iteration 38/1000 | Loss: 0.00003045
Iteration 39/1000 | Loss: 0.00002813
Iteration 40/1000 | Loss: 0.00002706
Iteration 41/1000 | Loss: 0.00002599
Iteration 42/1000 | Loss: 0.00002535
Iteration 43/1000 | Loss: 0.00002495
Iteration 44/1000 | Loss: 0.00002465
Iteration 45/1000 | Loss: 0.00002444
Iteration 46/1000 | Loss: 0.00002444
Iteration 47/1000 | Loss: 0.00002443
Iteration 48/1000 | Loss: 0.00002437
Iteration 49/1000 | Loss: 0.00002433
Iteration 50/1000 | Loss: 0.00002428
Iteration 51/1000 | Loss: 0.00002420
Iteration 52/1000 | Loss: 0.00002410
Iteration 53/1000 | Loss: 0.00002407
Iteration 54/1000 | Loss: 0.00002407
Iteration 55/1000 | Loss: 0.00002406
Iteration 56/1000 | Loss: 0.00002405
Iteration 57/1000 | Loss: 0.00002403
Iteration 58/1000 | Loss: 0.00002403
Iteration 59/1000 | Loss: 0.00002402
Iteration 60/1000 | Loss: 0.00002402
Iteration 61/1000 | Loss: 0.00002402
Iteration 62/1000 | Loss: 0.00002401
Iteration 63/1000 | Loss: 0.00002398
Iteration 64/1000 | Loss: 0.00002395
Iteration 65/1000 | Loss: 0.00002394
Iteration 66/1000 | Loss: 0.00002394
Iteration 67/1000 | Loss: 0.00002393
Iteration 68/1000 | Loss: 0.00002391
Iteration 69/1000 | Loss: 0.00002390
Iteration 70/1000 | Loss: 0.00002390
Iteration 71/1000 | Loss: 0.00002387
Iteration 72/1000 | Loss: 0.00002382
Iteration 73/1000 | Loss: 0.00002382
Iteration 74/1000 | Loss: 0.00002381
Iteration 75/1000 | Loss: 0.00002379
Iteration 76/1000 | Loss: 0.00002379
Iteration 77/1000 | Loss: 0.00002378
Iteration 78/1000 | Loss: 0.00002378
Iteration 79/1000 | Loss: 0.00002377
Iteration 80/1000 | Loss: 0.00002377
Iteration 81/1000 | Loss: 0.00002376
Iteration 82/1000 | Loss: 0.00002375
Iteration 83/1000 | Loss: 0.00002372
Iteration 84/1000 | Loss: 0.00002372
Iteration 85/1000 | Loss: 0.00002372
Iteration 86/1000 | Loss: 0.00002372
Iteration 87/1000 | Loss: 0.00002372
Iteration 88/1000 | Loss: 0.00002371
Iteration 89/1000 | Loss: 0.00002371
Iteration 90/1000 | Loss: 0.00002371
Iteration 91/1000 | Loss: 0.00002370
Iteration 92/1000 | Loss: 0.00002370
Iteration 93/1000 | Loss: 0.00002370
Iteration 94/1000 | Loss: 0.00002369
Iteration 95/1000 | Loss: 0.00002369
Iteration 96/1000 | Loss: 0.00002369
Iteration 97/1000 | Loss: 0.00002368
Iteration 98/1000 | Loss: 0.00002368
Iteration 99/1000 | Loss: 0.00002368
Iteration 100/1000 | Loss: 0.00002368
Iteration 101/1000 | Loss: 0.00002368
Iteration 102/1000 | Loss: 0.00002368
Iteration 103/1000 | Loss: 0.00002368
Iteration 104/1000 | Loss: 0.00002367
Iteration 105/1000 | Loss: 0.00002367
Iteration 106/1000 | Loss: 0.00002367
Iteration 107/1000 | Loss: 0.00002367
Iteration 108/1000 | Loss: 0.00002367
Iteration 109/1000 | Loss: 0.00002366
Iteration 110/1000 | Loss: 0.00002366
Iteration 111/1000 | Loss: 0.00002366
Iteration 112/1000 | Loss: 0.00002366
Iteration 113/1000 | Loss: 0.00002366
Iteration 114/1000 | Loss: 0.00002366
Iteration 115/1000 | Loss: 0.00002366
Iteration 116/1000 | Loss: 0.00002366
Iteration 117/1000 | Loss: 0.00002366
Iteration 118/1000 | Loss: 0.00002366
Iteration 119/1000 | Loss: 0.00002366
Iteration 120/1000 | Loss: 0.00002365
Iteration 121/1000 | Loss: 0.00002365
Iteration 122/1000 | Loss: 0.00002365
Iteration 123/1000 | Loss: 0.00002365
Iteration 124/1000 | Loss: 0.00002365
Iteration 125/1000 | Loss: 0.00002365
Iteration 126/1000 | Loss: 0.00002365
Iteration 127/1000 | Loss: 0.00002365
Iteration 128/1000 | Loss: 0.00002365
Iteration 129/1000 | Loss: 0.00002365
Iteration 130/1000 | Loss: 0.00002365
Iteration 131/1000 | Loss: 0.00002365
Iteration 132/1000 | Loss: 0.00002365
Iteration 133/1000 | Loss: 0.00002365
Iteration 134/1000 | Loss: 0.00002365
Iteration 135/1000 | Loss: 0.00002365
Iteration 136/1000 | Loss: 0.00002364
Iteration 137/1000 | Loss: 0.00002364
Iteration 138/1000 | Loss: 0.00002364
Iteration 139/1000 | Loss: 0.00002364
Iteration 140/1000 | Loss: 0.00002364
Iteration 141/1000 | Loss: 0.00002364
Iteration 142/1000 | Loss: 0.00002364
Iteration 143/1000 | Loss: 0.00002364
Iteration 144/1000 | Loss: 0.00002364
Iteration 145/1000 | Loss: 0.00002364
Iteration 146/1000 | Loss: 0.00002364
Iteration 147/1000 | Loss: 0.00002364
Iteration 148/1000 | Loss: 0.00002364
Iteration 149/1000 | Loss: 0.00002364
Iteration 150/1000 | Loss: 0.00002364
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [2.364095053053461e-05, 2.364095053053461e-05, 2.364095053053461e-05, 2.364095053053461e-05, 2.364095053053461e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.364095053053461e-05

Optimization complete. Final v2v error: 3.8809220790863037 mm

Highest mean error: 6.706737995147705 mm for frame 74

Lowest mean error: 2.9562535285949707 mm for frame 26

Saving results

Total time: 132.77858924865723
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00444619
Iteration 2/25 | Loss: 0.00106927
Iteration 3/25 | Loss: 0.00086616
Iteration 4/25 | Loss: 0.00083319
Iteration 5/25 | Loss: 0.00082392
Iteration 6/25 | Loss: 0.00082095
Iteration 7/25 | Loss: 0.00082044
Iteration 8/25 | Loss: 0.00082044
Iteration 9/25 | Loss: 0.00082044
Iteration 10/25 | Loss: 0.00082044
Iteration 11/25 | Loss: 0.00082044
Iteration 12/25 | Loss: 0.00082044
Iteration 13/25 | Loss: 0.00082044
Iteration 14/25 | Loss: 0.00082044
Iteration 15/25 | Loss: 0.00082044
Iteration 16/25 | Loss: 0.00082044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008204441983252764, 0.0008204441983252764, 0.0008204441983252764, 0.0008204441983252764, 0.0008204441983252764]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008204441983252764

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59077716
Iteration 2/25 | Loss: 0.00145193
Iteration 3/25 | Loss: 0.00145193
Iteration 4/25 | Loss: 0.00145193
Iteration 5/25 | Loss: 0.00145193
Iteration 6/25 | Loss: 0.00145193
Iteration 7/25 | Loss: 0.00145193
Iteration 8/25 | Loss: 0.00145193
Iteration 9/25 | Loss: 0.00145193
Iteration 10/25 | Loss: 0.00145193
Iteration 11/25 | Loss: 0.00145193
Iteration 12/25 | Loss: 0.00145193
Iteration 13/25 | Loss: 0.00145193
Iteration 14/25 | Loss: 0.00145193
Iteration 15/25 | Loss: 0.00145193
Iteration 16/25 | Loss: 0.00145193
Iteration 17/25 | Loss: 0.00145193
Iteration 18/25 | Loss: 0.00145193
Iteration 19/25 | Loss: 0.00145193
Iteration 20/25 | Loss: 0.00145193
Iteration 21/25 | Loss: 0.00145193
Iteration 22/25 | Loss: 0.00145193
Iteration 23/25 | Loss: 0.00145193
Iteration 24/25 | Loss: 0.00145193
Iteration 25/25 | Loss: 0.00145193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145193
Iteration 2/1000 | Loss: 0.00003341
Iteration 3/1000 | Loss: 0.00002392
Iteration 4/1000 | Loss: 0.00002186
Iteration 5/1000 | Loss: 0.00002075
Iteration 6/1000 | Loss: 0.00001996
Iteration 7/1000 | Loss: 0.00001944
Iteration 8/1000 | Loss: 0.00001906
Iteration 9/1000 | Loss: 0.00001902
Iteration 10/1000 | Loss: 0.00001889
Iteration 11/1000 | Loss: 0.00001871
Iteration 12/1000 | Loss: 0.00001870
Iteration 13/1000 | Loss: 0.00001861
Iteration 14/1000 | Loss: 0.00001853
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001850
Iteration 17/1000 | Loss: 0.00001849
Iteration 18/1000 | Loss: 0.00001849
Iteration 19/1000 | Loss: 0.00001849
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001847
Iteration 23/1000 | Loss: 0.00001846
Iteration 24/1000 | Loss: 0.00001846
Iteration 25/1000 | Loss: 0.00001846
Iteration 26/1000 | Loss: 0.00001846
Iteration 27/1000 | Loss: 0.00001844
Iteration 28/1000 | Loss: 0.00001843
Iteration 29/1000 | Loss: 0.00001843
Iteration 30/1000 | Loss: 0.00001843
Iteration 31/1000 | Loss: 0.00001842
Iteration 32/1000 | Loss: 0.00001841
Iteration 33/1000 | Loss: 0.00001841
Iteration 34/1000 | Loss: 0.00001841
Iteration 35/1000 | Loss: 0.00001840
Iteration 36/1000 | Loss: 0.00001840
Iteration 37/1000 | Loss: 0.00001840
Iteration 38/1000 | Loss: 0.00001840
Iteration 39/1000 | Loss: 0.00001839
Iteration 40/1000 | Loss: 0.00001839
Iteration 41/1000 | Loss: 0.00001838
Iteration 42/1000 | Loss: 0.00001838
Iteration 43/1000 | Loss: 0.00001838
Iteration 44/1000 | Loss: 0.00001838
Iteration 45/1000 | Loss: 0.00001837
Iteration 46/1000 | Loss: 0.00001837
Iteration 47/1000 | Loss: 0.00001837
Iteration 48/1000 | Loss: 0.00001836
Iteration 49/1000 | Loss: 0.00001836
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00001835
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001835
Iteration 54/1000 | Loss: 0.00001834
Iteration 55/1000 | Loss: 0.00001834
Iteration 56/1000 | Loss: 0.00001834
Iteration 57/1000 | Loss: 0.00001833
Iteration 58/1000 | Loss: 0.00001833
Iteration 59/1000 | Loss: 0.00001833
Iteration 60/1000 | Loss: 0.00001832
Iteration 61/1000 | Loss: 0.00001832
Iteration 62/1000 | Loss: 0.00001832
Iteration 63/1000 | Loss: 0.00001831
Iteration 64/1000 | Loss: 0.00001831
Iteration 65/1000 | Loss: 0.00001831
Iteration 66/1000 | Loss: 0.00001831
Iteration 67/1000 | Loss: 0.00001831
Iteration 68/1000 | Loss: 0.00001831
Iteration 69/1000 | Loss: 0.00001831
Iteration 70/1000 | Loss: 0.00001831
Iteration 71/1000 | Loss: 0.00001831
Iteration 72/1000 | Loss: 0.00001831
Iteration 73/1000 | Loss: 0.00001831
Iteration 74/1000 | Loss: 0.00001831
Iteration 75/1000 | Loss: 0.00001830
Iteration 76/1000 | Loss: 0.00001830
Iteration 77/1000 | Loss: 0.00001830
Iteration 78/1000 | Loss: 0.00001830
Iteration 79/1000 | Loss: 0.00001829
Iteration 80/1000 | Loss: 0.00001829
Iteration 81/1000 | Loss: 0.00001829
Iteration 82/1000 | Loss: 0.00001828
Iteration 83/1000 | Loss: 0.00001828
Iteration 84/1000 | Loss: 0.00001828
Iteration 85/1000 | Loss: 0.00001828
Iteration 86/1000 | Loss: 0.00001828
Iteration 87/1000 | Loss: 0.00001827
Iteration 88/1000 | Loss: 0.00001827
Iteration 89/1000 | Loss: 0.00001827
Iteration 90/1000 | Loss: 0.00001827
Iteration 91/1000 | Loss: 0.00001826
Iteration 92/1000 | Loss: 0.00001826
Iteration 93/1000 | Loss: 0.00001826
Iteration 94/1000 | Loss: 0.00001826
Iteration 95/1000 | Loss: 0.00001826
Iteration 96/1000 | Loss: 0.00001826
Iteration 97/1000 | Loss: 0.00001825
Iteration 98/1000 | Loss: 0.00001825
Iteration 99/1000 | Loss: 0.00001825
Iteration 100/1000 | Loss: 0.00001825
Iteration 101/1000 | Loss: 0.00001825
Iteration 102/1000 | Loss: 0.00001825
Iteration 103/1000 | Loss: 0.00001825
Iteration 104/1000 | Loss: 0.00001825
Iteration 105/1000 | Loss: 0.00001824
Iteration 106/1000 | Loss: 0.00001824
Iteration 107/1000 | Loss: 0.00001824
Iteration 108/1000 | Loss: 0.00001824
Iteration 109/1000 | Loss: 0.00001824
Iteration 110/1000 | Loss: 0.00001824
Iteration 111/1000 | Loss: 0.00001823
Iteration 112/1000 | Loss: 0.00001823
Iteration 113/1000 | Loss: 0.00001823
Iteration 114/1000 | Loss: 0.00001822
Iteration 115/1000 | Loss: 0.00001822
Iteration 116/1000 | Loss: 0.00001822
Iteration 117/1000 | Loss: 0.00001822
Iteration 118/1000 | Loss: 0.00001822
Iteration 119/1000 | Loss: 0.00001821
Iteration 120/1000 | Loss: 0.00001821
Iteration 121/1000 | Loss: 0.00001821
Iteration 122/1000 | Loss: 0.00001821
Iteration 123/1000 | Loss: 0.00001821
Iteration 124/1000 | Loss: 0.00001821
Iteration 125/1000 | Loss: 0.00001821
Iteration 126/1000 | Loss: 0.00001821
Iteration 127/1000 | Loss: 0.00001821
Iteration 128/1000 | Loss: 0.00001821
Iteration 129/1000 | Loss: 0.00001821
Iteration 130/1000 | Loss: 0.00001821
Iteration 131/1000 | Loss: 0.00001821
Iteration 132/1000 | Loss: 0.00001821
Iteration 133/1000 | Loss: 0.00001820
Iteration 134/1000 | Loss: 0.00001820
Iteration 135/1000 | Loss: 0.00001820
Iteration 136/1000 | Loss: 0.00001820
Iteration 137/1000 | Loss: 0.00001820
Iteration 138/1000 | Loss: 0.00001820
Iteration 139/1000 | Loss: 0.00001820
Iteration 140/1000 | Loss: 0.00001820
Iteration 141/1000 | Loss: 0.00001820
Iteration 142/1000 | Loss: 0.00001820
Iteration 143/1000 | Loss: 0.00001820
Iteration 144/1000 | Loss: 0.00001820
Iteration 145/1000 | Loss: 0.00001820
Iteration 146/1000 | Loss: 0.00001820
Iteration 147/1000 | Loss: 0.00001820
Iteration 148/1000 | Loss: 0.00001820
Iteration 149/1000 | Loss: 0.00001820
Iteration 150/1000 | Loss: 0.00001820
Iteration 151/1000 | Loss: 0.00001820
Iteration 152/1000 | Loss: 0.00001820
Iteration 153/1000 | Loss: 0.00001820
Iteration 154/1000 | Loss: 0.00001820
Iteration 155/1000 | Loss: 0.00001820
Iteration 156/1000 | Loss: 0.00001820
Iteration 157/1000 | Loss: 0.00001820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.8196125893155113e-05, 1.8196125893155113e-05, 1.8196125893155113e-05, 1.8196125893155113e-05, 1.8196125893155113e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8196125893155113e-05

Optimization complete. Final v2v error: 3.5684502124786377 mm

Highest mean error: 4.560432434082031 mm for frame 229

Lowest mean error: 3.0368380546569824 mm for frame 91

Saving results

Total time: 41.63293957710266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00522242
Iteration 2/25 | Loss: 0.00095567
Iteration 3/25 | Loss: 0.00084395
Iteration 4/25 | Loss: 0.00082016
Iteration 5/25 | Loss: 0.00081222
Iteration 6/25 | Loss: 0.00081058
Iteration 7/25 | Loss: 0.00081046
Iteration 8/25 | Loss: 0.00081046
Iteration 9/25 | Loss: 0.00081046
Iteration 10/25 | Loss: 0.00081046
Iteration 11/25 | Loss: 0.00081046
Iteration 12/25 | Loss: 0.00081046
Iteration 13/25 | Loss: 0.00081046
Iteration 14/25 | Loss: 0.00081046
Iteration 15/25 | Loss: 0.00081046
Iteration 16/25 | Loss: 0.00081046
Iteration 17/25 | Loss: 0.00081046
Iteration 18/25 | Loss: 0.00081046
Iteration 19/25 | Loss: 0.00081046
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008104640292003751, 0.0008104640292003751, 0.0008104640292003751, 0.0008104640292003751, 0.0008104640292003751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008104640292003751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58882236
Iteration 2/25 | Loss: 0.00112105
Iteration 3/25 | Loss: 0.00112102
Iteration 4/25 | Loss: 0.00112101
Iteration 5/25 | Loss: 0.00112101
Iteration 6/25 | Loss: 0.00112101
Iteration 7/25 | Loss: 0.00112101
Iteration 8/25 | Loss: 0.00112101
Iteration 9/25 | Loss: 0.00112101
Iteration 10/25 | Loss: 0.00112101
Iteration 11/25 | Loss: 0.00112101
Iteration 12/25 | Loss: 0.00112101
Iteration 13/25 | Loss: 0.00112101
Iteration 14/25 | Loss: 0.00112101
Iteration 15/25 | Loss: 0.00112101
Iteration 16/25 | Loss: 0.00112101
Iteration 17/25 | Loss: 0.00112101
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011210128432139754, 0.0011210128432139754, 0.0011210128432139754, 0.0011210128432139754, 0.0011210128432139754]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011210128432139754

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112101
Iteration 2/1000 | Loss: 0.00003925
Iteration 3/1000 | Loss: 0.00002750
Iteration 4/1000 | Loss: 0.00002381
Iteration 5/1000 | Loss: 0.00002215
Iteration 6/1000 | Loss: 0.00002092
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001922
Iteration 10/1000 | Loss: 0.00001892
Iteration 11/1000 | Loss: 0.00001868
Iteration 12/1000 | Loss: 0.00001847
Iteration 13/1000 | Loss: 0.00001843
Iteration 14/1000 | Loss: 0.00001841
Iteration 15/1000 | Loss: 0.00001829
Iteration 16/1000 | Loss: 0.00001825
Iteration 17/1000 | Loss: 0.00001825
Iteration 18/1000 | Loss: 0.00001824
Iteration 19/1000 | Loss: 0.00001824
Iteration 20/1000 | Loss: 0.00001821
Iteration 21/1000 | Loss: 0.00001821
Iteration 22/1000 | Loss: 0.00001819
Iteration 23/1000 | Loss: 0.00001819
Iteration 24/1000 | Loss: 0.00001819
Iteration 25/1000 | Loss: 0.00001819
Iteration 26/1000 | Loss: 0.00001819
Iteration 27/1000 | Loss: 0.00001819
Iteration 28/1000 | Loss: 0.00001819
Iteration 29/1000 | Loss: 0.00001819
Iteration 30/1000 | Loss: 0.00001819
Iteration 31/1000 | Loss: 0.00001818
Iteration 32/1000 | Loss: 0.00001818
Iteration 33/1000 | Loss: 0.00001818
Iteration 34/1000 | Loss: 0.00001816
Iteration 35/1000 | Loss: 0.00001816
Iteration 36/1000 | Loss: 0.00001816
Iteration 37/1000 | Loss: 0.00001816
Iteration 38/1000 | Loss: 0.00001816
Iteration 39/1000 | Loss: 0.00001815
Iteration 40/1000 | Loss: 0.00001815
Iteration 41/1000 | Loss: 0.00001815
Iteration 42/1000 | Loss: 0.00001815
Iteration 43/1000 | Loss: 0.00001815
Iteration 44/1000 | Loss: 0.00001815
Iteration 45/1000 | Loss: 0.00001815
Iteration 46/1000 | Loss: 0.00001814
Iteration 47/1000 | Loss: 0.00001814
Iteration 48/1000 | Loss: 0.00001813
Iteration 49/1000 | Loss: 0.00001812
Iteration 50/1000 | Loss: 0.00001812
Iteration 51/1000 | Loss: 0.00001811
Iteration 52/1000 | Loss: 0.00001811
Iteration 53/1000 | Loss: 0.00001810
Iteration 54/1000 | Loss: 0.00001810
Iteration 55/1000 | Loss: 0.00001809
Iteration 56/1000 | Loss: 0.00001809
Iteration 57/1000 | Loss: 0.00001809
Iteration 58/1000 | Loss: 0.00001808
Iteration 59/1000 | Loss: 0.00001808
Iteration 60/1000 | Loss: 0.00001808
Iteration 61/1000 | Loss: 0.00001807
Iteration 62/1000 | Loss: 0.00001807
Iteration 63/1000 | Loss: 0.00001806
Iteration 64/1000 | Loss: 0.00001806
Iteration 65/1000 | Loss: 0.00001806
Iteration 66/1000 | Loss: 0.00001806
Iteration 67/1000 | Loss: 0.00001806
Iteration 68/1000 | Loss: 0.00001806
Iteration 69/1000 | Loss: 0.00001805
Iteration 70/1000 | Loss: 0.00001805
Iteration 71/1000 | Loss: 0.00001805
Iteration 72/1000 | Loss: 0.00001805
Iteration 73/1000 | Loss: 0.00001805
Iteration 74/1000 | Loss: 0.00001804
Iteration 75/1000 | Loss: 0.00001804
Iteration 76/1000 | Loss: 0.00001804
Iteration 77/1000 | Loss: 0.00001804
Iteration 78/1000 | Loss: 0.00001803
Iteration 79/1000 | Loss: 0.00001803
Iteration 80/1000 | Loss: 0.00001802
Iteration 81/1000 | Loss: 0.00001802
Iteration 82/1000 | Loss: 0.00001802
Iteration 83/1000 | Loss: 0.00001802
Iteration 84/1000 | Loss: 0.00001802
Iteration 85/1000 | Loss: 0.00001801
Iteration 86/1000 | Loss: 0.00001801
Iteration 87/1000 | Loss: 0.00001801
Iteration 88/1000 | Loss: 0.00001800
Iteration 89/1000 | Loss: 0.00001800
Iteration 90/1000 | Loss: 0.00001800
Iteration 91/1000 | Loss: 0.00001800
Iteration 92/1000 | Loss: 0.00001800
Iteration 93/1000 | Loss: 0.00001800
Iteration 94/1000 | Loss: 0.00001800
Iteration 95/1000 | Loss: 0.00001800
Iteration 96/1000 | Loss: 0.00001800
Iteration 97/1000 | Loss: 0.00001799
Iteration 98/1000 | Loss: 0.00001799
Iteration 99/1000 | Loss: 0.00001799
Iteration 100/1000 | Loss: 0.00001799
Iteration 101/1000 | Loss: 0.00001798
Iteration 102/1000 | Loss: 0.00001798
Iteration 103/1000 | Loss: 0.00001798
Iteration 104/1000 | Loss: 0.00001798
Iteration 105/1000 | Loss: 0.00001798
Iteration 106/1000 | Loss: 0.00001798
Iteration 107/1000 | Loss: 0.00001798
Iteration 108/1000 | Loss: 0.00001798
Iteration 109/1000 | Loss: 0.00001797
Iteration 110/1000 | Loss: 0.00001797
Iteration 111/1000 | Loss: 0.00001797
Iteration 112/1000 | Loss: 0.00001797
Iteration 113/1000 | Loss: 0.00001796
Iteration 114/1000 | Loss: 0.00001796
Iteration 115/1000 | Loss: 0.00001796
Iteration 116/1000 | Loss: 0.00001796
Iteration 117/1000 | Loss: 0.00001796
Iteration 118/1000 | Loss: 0.00001796
Iteration 119/1000 | Loss: 0.00001796
Iteration 120/1000 | Loss: 0.00001796
Iteration 121/1000 | Loss: 0.00001796
Iteration 122/1000 | Loss: 0.00001796
Iteration 123/1000 | Loss: 0.00001796
Iteration 124/1000 | Loss: 0.00001795
Iteration 125/1000 | Loss: 0.00001795
Iteration 126/1000 | Loss: 0.00001795
Iteration 127/1000 | Loss: 0.00001795
Iteration 128/1000 | Loss: 0.00001795
Iteration 129/1000 | Loss: 0.00001795
Iteration 130/1000 | Loss: 0.00001795
Iteration 131/1000 | Loss: 0.00001795
Iteration 132/1000 | Loss: 0.00001795
Iteration 133/1000 | Loss: 0.00001795
Iteration 134/1000 | Loss: 0.00001794
Iteration 135/1000 | Loss: 0.00001794
Iteration 136/1000 | Loss: 0.00001794
Iteration 137/1000 | Loss: 0.00001794
Iteration 138/1000 | Loss: 0.00001794
Iteration 139/1000 | Loss: 0.00001794
Iteration 140/1000 | Loss: 0.00001794
Iteration 141/1000 | Loss: 0.00001794
Iteration 142/1000 | Loss: 0.00001794
Iteration 143/1000 | Loss: 0.00001794
Iteration 144/1000 | Loss: 0.00001794
Iteration 145/1000 | Loss: 0.00001794
Iteration 146/1000 | Loss: 0.00001794
Iteration 147/1000 | Loss: 0.00001794
Iteration 148/1000 | Loss: 0.00001794
Iteration 149/1000 | Loss: 0.00001794
Iteration 150/1000 | Loss: 0.00001794
Iteration 151/1000 | Loss: 0.00001794
Iteration 152/1000 | Loss: 0.00001794
Iteration 153/1000 | Loss: 0.00001794
Iteration 154/1000 | Loss: 0.00001794
Iteration 155/1000 | Loss: 0.00001794
Iteration 156/1000 | Loss: 0.00001794
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 156. Stopping optimization.
Last 5 losses: [1.7939575627679005e-05, 1.7939575627679005e-05, 1.7939575627679005e-05, 1.7939575627679005e-05, 1.7939575627679005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7939575627679005e-05

Optimization complete. Final v2v error: 3.5305190086364746 mm

Highest mean error: 4.4456586837768555 mm for frame 157

Lowest mean error: 2.9689228534698486 mm for frame 8

Saving results

Total time: 44.42226219177246
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044465
Iteration 2/25 | Loss: 0.00274868
Iteration 3/25 | Loss: 0.00164079
Iteration 4/25 | Loss: 0.00132173
Iteration 5/25 | Loss: 0.00136276
Iteration 6/25 | Loss: 0.00125034
Iteration 7/25 | Loss: 0.00106897
Iteration 8/25 | Loss: 0.00096454
Iteration 9/25 | Loss: 0.00093499
Iteration 10/25 | Loss: 0.00091523
Iteration 11/25 | Loss: 0.00090932
Iteration 12/25 | Loss: 0.00090594
Iteration 13/25 | Loss: 0.00090489
Iteration 14/25 | Loss: 0.00089656
Iteration 15/25 | Loss: 0.00088800
Iteration 16/25 | Loss: 0.00088462
Iteration 17/25 | Loss: 0.00088346
Iteration 18/25 | Loss: 0.00088401
Iteration 19/25 | Loss: 0.00088451
Iteration 20/25 | Loss: 0.00088388
Iteration 21/25 | Loss: 0.00088378
Iteration 22/25 | Loss: 0.00088402
Iteration 23/25 | Loss: 0.00088446
Iteration 24/25 | Loss: 0.00088446
Iteration 25/25 | Loss: 0.00088408

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58508945
Iteration 2/25 | Loss: 0.00135829
Iteration 3/25 | Loss: 0.00135829
Iteration 4/25 | Loss: 0.00135829
Iteration 5/25 | Loss: 0.00135829
Iteration 6/25 | Loss: 0.00135829
Iteration 7/25 | Loss: 0.00135829
Iteration 8/25 | Loss: 0.00135829
Iteration 9/25 | Loss: 0.00135829
Iteration 10/25 | Loss: 0.00135829
Iteration 11/25 | Loss: 0.00135829
Iteration 12/25 | Loss: 0.00135829
Iteration 13/25 | Loss: 0.00135829
Iteration 14/25 | Loss: 0.00135829
Iteration 15/25 | Loss: 0.00135829
Iteration 16/25 | Loss: 0.00135829
Iteration 17/25 | Loss: 0.00135829
Iteration 18/25 | Loss: 0.00135829
Iteration 19/25 | Loss: 0.00135829
Iteration 20/25 | Loss: 0.00135829
Iteration 21/25 | Loss: 0.00135829
Iteration 22/25 | Loss: 0.00135829
Iteration 23/25 | Loss: 0.00135829
Iteration 24/25 | Loss: 0.00135829
Iteration 25/25 | Loss: 0.00135829

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135829
Iteration 2/1000 | Loss: 0.00002959
Iteration 3/1000 | Loss: 0.00002697
Iteration 4/1000 | Loss: 0.00003392
Iteration 5/1000 | Loss: 0.00003116
Iteration 6/1000 | Loss: 0.00002252
Iteration 7/1000 | Loss: 0.00003297
Iteration 8/1000 | Loss: 0.00002369
Iteration 9/1000 | Loss: 0.00004137
Iteration 10/1000 | Loss: 0.00003552
Iteration 11/1000 | Loss: 0.00002997
Iteration 12/1000 | Loss: 0.00003180
Iteration 13/1000 | Loss: 0.00004273
Iteration 14/1000 | Loss: 0.00002149
Iteration 15/1000 | Loss: 0.00001997
Iteration 16/1000 | Loss: 0.00001971
Iteration 17/1000 | Loss: 0.00001949
Iteration 18/1000 | Loss: 0.00001928
Iteration 19/1000 | Loss: 0.00001918
Iteration 20/1000 | Loss: 0.00001915
Iteration 21/1000 | Loss: 0.00001913
Iteration 22/1000 | Loss: 0.00001912
Iteration 23/1000 | Loss: 0.00001912
Iteration 24/1000 | Loss: 0.00001912
Iteration 25/1000 | Loss: 0.00001912
Iteration 26/1000 | Loss: 0.00001912
Iteration 27/1000 | Loss: 0.00001912
Iteration 28/1000 | Loss: 0.00001912
Iteration 29/1000 | Loss: 0.00001912
Iteration 30/1000 | Loss: 0.00001912
Iteration 31/1000 | Loss: 0.00001912
Iteration 32/1000 | Loss: 0.00001911
Iteration 33/1000 | Loss: 0.00001911
Iteration 34/1000 | Loss: 0.00001911
Iteration 35/1000 | Loss: 0.00001911
Iteration 36/1000 | Loss: 0.00001911
Iteration 37/1000 | Loss: 0.00001911
Iteration 38/1000 | Loss: 0.00001911
Iteration 39/1000 | Loss: 0.00001910
Iteration 40/1000 | Loss: 0.00001910
Iteration 41/1000 | Loss: 0.00001910
Iteration 42/1000 | Loss: 0.00001910
Iteration 43/1000 | Loss: 0.00001910
Iteration 44/1000 | Loss: 0.00001910
Iteration 45/1000 | Loss: 0.00001910
Iteration 46/1000 | Loss: 0.00001910
Iteration 47/1000 | Loss: 0.00001909
Iteration 48/1000 | Loss: 0.00001909
Iteration 49/1000 | Loss: 0.00001909
Iteration 50/1000 | Loss: 0.00001909
Iteration 51/1000 | Loss: 0.00001909
Iteration 52/1000 | Loss: 0.00001909
Iteration 53/1000 | Loss: 0.00001908
Iteration 54/1000 | Loss: 0.00001908
Iteration 55/1000 | Loss: 0.00001908
Iteration 56/1000 | Loss: 0.00001907
Iteration 57/1000 | Loss: 0.00001907
Iteration 58/1000 | Loss: 0.00001907
Iteration 59/1000 | Loss: 0.00001907
Iteration 60/1000 | Loss: 0.00001907
Iteration 61/1000 | Loss: 0.00001907
Iteration 62/1000 | Loss: 0.00001907
Iteration 63/1000 | Loss: 0.00001907
Iteration 64/1000 | Loss: 0.00001907
Iteration 65/1000 | Loss: 0.00001907
Iteration 66/1000 | Loss: 0.00001907
Iteration 67/1000 | Loss: 0.00001907
Iteration 68/1000 | Loss: 0.00001906
Iteration 69/1000 | Loss: 0.00001906
Iteration 70/1000 | Loss: 0.00001906
Iteration 71/1000 | Loss: 0.00001906
Iteration 72/1000 | Loss: 0.00001905
Iteration 73/1000 | Loss: 0.00001905
Iteration 74/1000 | Loss: 0.00001905
Iteration 75/1000 | Loss: 0.00001905
Iteration 76/1000 | Loss: 0.00001905
Iteration 77/1000 | Loss: 0.00001905
Iteration 78/1000 | Loss: 0.00001905
Iteration 79/1000 | Loss: 0.00001905
Iteration 80/1000 | Loss: 0.00001905
Iteration 81/1000 | Loss: 0.00001905
Iteration 82/1000 | Loss: 0.00001904
Iteration 83/1000 | Loss: 0.00001904
Iteration 84/1000 | Loss: 0.00001904
Iteration 85/1000 | Loss: 0.00001904
Iteration 86/1000 | Loss: 0.00001904
Iteration 87/1000 | Loss: 0.00001904
Iteration 88/1000 | Loss: 0.00001904
Iteration 89/1000 | Loss: 0.00001904
Iteration 90/1000 | Loss: 0.00001904
Iteration 91/1000 | Loss: 0.00001904
Iteration 92/1000 | Loss: 0.00001904
Iteration 93/1000 | Loss: 0.00001903
Iteration 94/1000 | Loss: 0.00001903
Iteration 95/1000 | Loss: 0.00001903
Iteration 96/1000 | Loss: 0.00001903
Iteration 97/1000 | Loss: 0.00001903
Iteration 98/1000 | Loss: 0.00001903
Iteration 99/1000 | Loss: 0.00001903
Iteration 100/1000 | Loss: 0.00001903
Iteration 101/1000 | Loss: 0.00001903
Iteration 102/1000 | Loss: 0.00001903
Iteration 103/1000 | Loss: 0.00001903
Iteration 104/1000 | Loss: 0.00001903
Iteration 105/1000 | Loss: 0.00001903
Iteration 106/1000 | Loss: 0.00001903
Iteration 107/1000 | Loss: 0.00001903
Iteration 108/1000 | Loss: 0.00001903
Iteration 109/1000 | Loss: 0.00001903
Iteration 110/1000 | Loss: 0.00001903
Iteration 111/1000 | Loss: 0.00001903
Iteration 112/1000 | Loss: 0.00001903
Iteration 113/1000 | Loss: 0.00001903
Iteration 114/1000 | Loss: 0.00001903
Iteration 115/1000 | Loss: 0.00001903
Iteration 116/1000 | Loss: 0.00001903
Iteration 117/1000 | Loss: 0.00001902
Iteration 118/1000 | Loss: 0.00001902
Iteration 119/1000 | Loss: 0.00001902
Iteration 120/1000 | Loss: 0.00001902
Iteration 121/1000 | Loss: 0.00001902
Iteration 122/1000 | Loss: 0.00001902
Iteration 123/1000 | Loss: 0.00001902
Iteration 124/1000 | Loss: 0.00001902
Iteration 125/1000 | Loss: 0.00001902
Iteration 126/1000 | Loss: 0.00001902
Iteration 127/1000 | Loss: 0.00001902
Iteration 128/1000 | Loss: 0.00001902
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.902376243378967e-05, 1.902376243378967e-05, 1.902376243378967e-05, 1.902376243378967e-05, 1.902376243378967e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.902376243378967e-05

Optimization complete. Final v2v error: 3.5463383197784424 mm

Highest mean error: 4.077102184295654 mm for frame 201

Lowest mean error: 3.259474515914917 mm for frame 123

Saving results

Total time: 90.55471205711365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044179
Iteration 2/25 | Loss: 0.00106917
Iteration 3/25 | Loss: 0.00083497
Iteration 4/25 | Loss: 0.00080319
Iteration 5/25 | Loss: 0.00079685
Iteration 6/25 | Loss: 0.00079569
Iteration 7/25 | Loss: 0.00079568
Iteration 8/25 | Loss: 0.00079568
Iteration 9/25 | Loss: 0.00079568
Iteration 10/25 | Loss: 0.00079568
Iteration 11/25 | Loss: 0.00079568
Iteration 12/25 | Loss: 0.00079568
Iteration 13/25 | Loss: 0.00079568
Iteration 14/25 | Loss: 0.00079568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007956809713505208, 0.0007956809713505208, 0.0007956809713505208, 0.0007956809713505208, 0.0007956809713505208]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007956809713505208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57474351
Iteration 2/25 | Loss: 0.00131593
Iteration 3/25 | Loss: 0.00131593
Iteration 4/25 | Loss: 0.00131593
Iteration 5/25 | Loss: 0.00131593
Iteration 6/25 | Loss: 0.00131593
Iteration 7/25 | Loss: 0.00131593
Iteration 8/25 | Loss: 0.00131593
Iteration 9/25 | Loss: 0.00131593
Iteration 10/25 | Loss: 0.00131593
Iteration 11/25 | Loss: 0.00131593
Iteration 12/25 | Loss: 0.00131593
Iteration 13/25 | Loss: 0.00131593
Iteration 14/25 | Loss: 0.00131593
Iteration 15/25 | Loss: 0.00131593
Iteration 16/25 | Loss: 0.00131593
Iteration 17/25 | Loss: 0.00131593
Iteration 18/25 | Loss: 0.00131593
Iteration 19/25 | Loss: 0.00131593
Iteration 20/25 | Loss: 0.00131593
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0013159321388229728, 0.0013159321388229728, 0.0013159321388229728, 0.0013159321388229728, 0.0013159321388229728]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013159321388229728

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131593
Iteration 2/1000 | Loss: 0.00002232
Iteration 3/1000 | Loss: 0.00001590
Iteration 4/1000 | Loss: 0.00001497
Iteration 5/1000 | Loss: 0.00001423
Iteration 6/1000 | Loss: 0.00001388
Iteration 7/1000 | Loss: 0.00001358
Iteration 8/1000 | Loss: 0.00001348
Iteration 9/1000 | Loss: 0.00001335
Iteration 10/1000 | Loss: 0.00001330
Iteration 11/1000 | Loss: 0.00001329
Iteration 12/1000 | Loss: 0.00001329
Iteration 13/1000 | Loss: 0.00001329
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001326
Iteration 17/1000 | Loss: 0.00001324
Iteration 18/1000 | Loss: 0.00001323
Iteration 19/1000 | Loss: 0.00001319
Iteration 20/1000 | Loss: 0.00001316
Iteration 21/1000 | Loss: 0.00001315
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001315
Iteration 24/1000 | Loss: 0.00001315
Iteration 25/1000 | Loss: 0.00001315
Iteration 26/1000 | Loss: 0.00001315
Iteration 27/1000 | Loss: 0.00001315
Iteration 28/1000 | Loss: 0.00001315
Iteration 29/1000 | Loss: 0.00001315
Iteration 30/1000 | Loss: 0.00001315
Iteration 31/1000 | Loss: 0.00001314
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001312
Iteration 35/1000 | Loss: 0.00001312
Iteration 36/1000 | Loss: 0.00001312
Iteration 37/1000 | Loss: 0.00001311
Iteration 38/1000 | Loss: 0.00001311
Iteration 39/1000 | Loss: 0.00001311
Iteration 40/1000 | Loss: 0.00001311
Iteration 41/1000 | Loss: 0.00001311
Iteration 42/1000 | Loss: 0.00001311
Iteration 43/1000 | Loss: 0.00001311
Iteration 44/1000 | Loss: 0.00001310
Iteration 45/1000 | Loss: 0.00001309
Iteration 46/1000 | Loss: 0.00001309
Iteration 47/1000 | Loss: 0.00001309
Iteration 48/1000 | Loss: 0.00001308
Iteration 49/1000 | Loss: 0.00001308
Iteration 50/1000 | Loss: 0.00001308
Iteration 51/1000 | Loss: 0.00001308
Iteration 52/1000 | Loss: 0.00001307
Iteration 53/1000 | Loss: 0.00001307
Iteration 54/1000 | Loss: 0.00001306
Iteration 55/1000 | Loss: 0.00001306
Iteration 56/1000 | Loss: 0.00001306
Iteration 57/1000 | Loss: 0.00001305
Iteration 58/1000 | Loss: 0.00001305
Iteration 59/1000 | Loss: 0.00001305
Iteration 60/1000 | Loss: 0.00001304
Iteration 61/1000 | Loss: 0.00001303
Iteration 62/1000 | Loss: 0.00001303
Iteration 63/1000 | Loss: 0.00001303
Iteration 64/1000 | Loss: 0.00001302
Iteration 65/1000 | Loss: 0.00001302
Iteration 66/1000 | Loss: 0.00001302
Iteration 67/1000 | Loss: 0.00001301
Iteration 68/1000 | Loss: 0.00001300
Iteration 69/1000 | Loss: 0.00001300
Iteration 70/1000 | Loss: 0.00001300
Iteration 71/1000 | Loss: 0.00001300
Iteration 72/1000 | Loss: 0.00001300
Iteration 73/1000 | Loss: 0.00001300
Iteration 74/1000 | Loss: 0.00001300
Iteration 75/1000 | Loss: 0.00001300
Iteration 76/1000 | Loss: 0.00001300
Iteration 77/1000 | Loss: 0.00001300
Iteration 78/1000 | Loss: 0.00001300
Iteration 79/1000 | Loss: 0.00001300
Iteration 80/1000 | Loss: 0.00001299
Iteration 81/1000 | Loss: 0.00001299
Iteration 82/1000 | Loss: 0.00001299
Iteration 83/1000 | Loss: 0.00001299
Iteration 84/1000 | Loss: 0.00001299
Iteration 85/1000 | Loss: 0.00001299
Iteration 86/1000 | Loss: 0.00001299
Iteration 87/1000 | Loss: 0.00001299
Iteration 88/1000 | Loss: 0.00001299
Iteration 89/1000 | Loss: 0.00001299
Iteration 90/1000 | Loss: 0.00001299
Iteration 91/1000 | Loss: 0.00001299
Iteration 92/1000 | Loss: 0.00001299
Iteration 93/1000 | Loss: 0.00001299
Iteration 94/1000 | Loss: 0.00001299
Iteration 95/1000 | Loss: 0.00001299
Iteration 96/1000 | Loss: 0.00001299
Iteration 97/1000 | Loss: 0.00001299
Iteration 98/1000 | Loss: 0.00001299
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 98. Stopping optimization.
Last 5 losses: [1.2986824003746733e-05, 1.2986824003746733e-05, 1.2986824003746733e-05, 1.2986824003746733e-05, 1.2986824003746733e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2986824003746733e-05

Optimization complete. Final v2v error: 3.0679919719696045 mm

Highest mean error: 3.316962242126465 mm for frame 182

Lowest mean error: 2.8506622314453125 mm for frame 135

Saving results

Total time: 29.877639532089233
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00848514
Iteration 2/25 | Loss: 0.00097293
Iteration 3/25 | Loss: 0.00077471
Iteration 4/25 | Loss: 0.00074673
Iteration 5/25 | Loss: 0.00073981
Iteration 6/25 | Loss: 0.00073774
Iteration 7/25 | Loss: 0.00073703
Iteration 8/25 | Loss: 0.00073703
Iteration 9/25 | Loss: 0.00073703
Iteration 10/25 | Loss: 0.00073703
Iteration 11/25 | Loss: 0.00073703
Iteration 12/25 | Loss: 0.00073703
Iteration 13/25 | Loss: 0.00073703
Iteration 14/25 | Loss: 0.00073703
Iteration 15/25 | Loss: 0.00073703
Iteration 16/25 | Loss: 0.00073703
Iteration 17/25 | Loss: 0.00073703
Iteration 18/25 | Loss: 0.00073703
Iteration 19/25 | Loss: 0.00073703
Iteration 20/25 | Loss: 0.00073703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007370252278633416, 0.0007370252278633416, 0.0007370252278633416, 0.0007370252278633416, 0.0007370252278633416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007370252278633416

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59322464
Iteration 2/25 | Loss: 0.00122818
Iteration 3/25 | Loss: 0.00122817
Iteration 4/25 | Loss: 0.00122817
Iteration 5/25 | Loss: 0.00122817
Iteration 6/25 | Loss: 0.00122817
Iteration 7/25 | Loss: 0.00122817
Iteration 8/25 | Loss: 0.00122817
Iteration 9/25 | Loss: 0.00122817
Iteration 10/25 | Loss: 0.00122817
Iteration 11/25 | Loss: 0.00122817
Iteration 12/25 | Loss: 0.00122817
Iteration 13/25 | Loss: 0.00122817
Iteration 14/25 | Loss: 0.00122817
Iteration 15/25 | Loss: 0.00122817
Iteration 16/25 | Loss: 0.00122817
Iteration 17/25 | Loss: 0.00122817
Iteration 18/25 | Loss: 0.00122817
Iteration 19/25 | Loss: 0.00122817
Iteration 20/25 | Loss: 0.00122817
Iteration 21/25 | Loss: 0.00122817
Iteration 22/25 | Loss: 0.00122817
Iteration 23/25 | Loss: 0.00122817
Iteration 24/25 | Loss: 0.00122817
Iteration 25/25 | Loss: 0.00122817
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0012281725648790598, 0.0012281725648790598, 0.0012281725648790598, 0.0012281725648790598, 0.0012281725648790598]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012281725648790598

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122817
Iteration 2/1000 | Loss: 0.00002296
Iteration 3/1000 | Loss: 0.00001519
Iteration 4/1000 | Loss: 0.00001351
Iteration 5/1000 | Loss: 0.00001274
Iteration 6/1000 | Loss: 0.00001217
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001178
Iteration 9/1000 | Loss: 0.00001176
Iteration 10/1000 | Loss: 0.00001171
Iteration 11/1000 | Loss: 0.00001165
Iteration 12/1000 | Loss: 0.00001164
Iteration 13/1000 | Loss: 0.00001160
Iteration 14/1000 | Loss: 0.00001148
Iteration 15/1000 | Loss: 0.00001146
Iteration 16/1000 | Loss: 0.00001145
Iteration 17/1000 | Loss: 0.00001144
Iteration 18/1000 | Loss: 0.00001143
Iteration 19/1000 | Loss: 0.00001142
Iteration 20/1000 | Loss: 0.00001137
Iteration 21/1000 | Loss: 0.00001135
Iteration 22/1000 | Loss: 0.00001134
Iteration 23/1000 | Loss: 0.00001134
Iteration 24/1000 | Loss: 0.00001133
Iteration 25/1000 | Loss: 0.00001133
Iteration 26/1000 | Loss: 0.00001132
Iteration 27/1000 | Loss: 0.00001131
Iteration 28/1000 | Loss: 0.00001131
Iteration 29/1000 | Loss: 0.00001129
Iteration 30/1000 | Loss: 0.00001128
Iteration 31/1000 | Loss: 0.00001128
Iteration 32/1000 | Loss: 0.00001127
Iteration 33/1000 | Loss: 0.00001126
Iteration 34/1000 | Loss: 0.00001125
Iteration 35/1000 | Loss: 0.00001124
Iteration 36/1000 | Loss: 0.00001123
Iteration 37/1000 | Loss: 0.00001123
Iteration 38/1000 | Loss: 0.00001122
Iteration 39/1000 | Loss: 0.00001122
Iteration 40/1000 | Loss: 0.00001121
Iteration 41/1000 | Loss: 0.00001120
Iteration 42/1000 | Loss: 0.00001119
Iteration 43/1000 | Loss: 0.00001119
Iteration 44/1000 | Loss: 0.00001119
Iteration 45/1000 | Loss: 0.00001119
Iteration 46/1000 | Loss: 0.00001117
Iteration 47/1000 | Loss: 0.00001117
Iteration 48/1000 | Loss: 0.00001117
Iteration 49/1000 | Loss: 0.00001116
Iteration 50/1000 | Loss: 0.00001116
Iteration 51/1000 | Loss: 0.00001116
Iteration 52/1000 | Loss: 0.00001115
Iteration 53/1000 | Loss: 0.00001115
Iteration 54/1000 | Loss: 0.00001115
Iteration 55/1000 | Loss: 0.00001115
Iteration 56/1000 | Loss: 0.00001115
Iteration 57/1000 | Loss: 0.00001115
Iteration 58/1000 | Loss: 0.00001115
Iteration 59/1000 | Loss: 0.00001115
Iteration 60/1000 | Loss: 0.00001114
Iteration 61/1000 | Loss: 0.00001114
Iteration 62/1000 | Loss: 0.00001114
Iteration 63/1000 | Loss: 0.00001114
Iteration 64/1000 | Loss: 0.00001113
Iteration 65/1000 | Loss: 0.00001113
Iteration 66/1000 | Loss: 0.00001113
Iteration 67/1000 | Loss: 0.00001113
Iteration 68/1000 | Loss: 0.00001112
Iteration 69/1000 | Loss: 0.00001112
Iteration 70/1000 | Loss: 0.00001112
Iteration 71/1000 | Loss: 0.00001112
Iteration 72/1000 | Loss: 0.00001111
Iteration 73/1000 | Loss: 0.00001111
Iteration 74/1000 | Loss: 0.00001111
Iteration 75/1000 | Loss: 0.00001111
Iteration 76/1000 | Loss: 0.00001110
Iteration 77/1000 | Loss: 0.00001110
Iteration 78/1000 | Loss: 0.00001110
Iteration 79/1000 | Loss: 0.00001110
Iteration 80/1000 | Loss: 0.00001110
Iteration 81/1000 | Loss: 0.00001110
Iteration 82/1000 | Loss: 0.00001109
Iteration 83/1000 | Loss: 0.00001109
Iteration 84/1000 | Loss: 0.00001109
Iteration 85/1000 | Loss: 0.00001109
Iteration 86/1000 | Loss: 0.00001109
Iteration 87/1000 | Loss: 0.00001109
Iteration 88/1000 | Loss: 0.00001108
Iteration 89/1000 | Loss: 0.00001108
Iteration 90/1000 | Loss: 0.00001108
Iteration 91/1000 | Loss: 0.00001108
Iteration 92/1000 | Loss: 0.00001108
Iteration 93/1000 | Loss: 0.00001107
Iteration 94/1000 | Loss: 0.00001107
Iteration 95/1000 | Loss: 0.00001106
Iteration 96/1000 | Loss: 0.00001106
Iteration 97/1000 | Loss: 0.00001106
Iteration 98/1000 | Loss: 0.00001106
Iteration 99/1000 | Loss: 0.00001106
Iteration 100/1000 | Loss: 0.00001106
Iteration 101/1000 | Loss: 0.00001106
Iteration 102/1000 | Loss: 0.00001106
Iteration 103/1000 | Loss: 0.00001106
Iteration 104/1000 | Loss: 0.00001105
Iteration 105/1000 | Loss: 0.00001105
Iteration 106/1000 | Loss: 0.00001105
Iteration 107/1000 | Loss: 0.00001105
Iteration 108/1000 | Loss: 0.00001105
Iteration 109/1000 | Loss: 0.00001105
Iteration 110/1000 | Loss: 0.00001105
Iteration 111/1000 | Loss: 0.00001105
Iteration 112/1000 | Loss: 0.00001105
Iteration 113/1000 | Loss: 0.00001105
Iteration 114/1000 | Loss: 0.00001105
Iteration 115/1000 | Loss: 0.00001105
Iteration 116/1000 | Loss: 0.00001105
Iteration 117/1000 | Loss: 0.00001105
Iteration 118/1000 | Loss: 0.00001105
Iteration 119/1000 | Loss: 0.00001105
Iteration 120/1000 | Loss: 0.00001105
Iteration 121/1000 | Loss: 0.00001105
Iteration 122/1000 | Loss: 0.00001104
Iteration 123/1000 | Loss: 0.00001104
Iteration 124/1000 | Loss: 0.00001104
Iteration 125/1000 | Loss: 0.00001104
Iteration 126/1000 | Loss: 0.00001104
Iteration 127/1000 | Loss: 0.00001104
Iteration 128/1000 | Loss: 0.00001104
Iteration 129/1000 | Loss: 0.00001104
Iteration 130/1000 | Loss: 0.00001104
Iteration 131/1000 | Loss: 0.00001103
Iteration 132/1000 | Loss: 0.00001103
Iteration 133/1000 | Loss: 0.00001103
Iteration 134/1000 | Loss: 0.00001103
Iteration 135/1000 | Loss: 0.00001103
Iteration 136/1000 | Loss: 0.00001103
Iteration 137/1000 | Loss: 0.00001103
Iteration 138/1000 | Loss: 0.00001103
Iteration 139/1000 | Loss: 0.00001103
Iteration 140/1000 | Loss: 0.00001103
Iteration 141/1000 | Loss: 0.00001103
Iteration 142/1000 | Loss: 0.00001103
Iteration 143/1000 | Loss: 0.00001103
Iteration 144/1000 | Loss: 0.00001102
Iteration 145/1000 | Loss: 0.00001102
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001102
Iteration 150/1000 | Loss: 0.00001102
Iteration 151/1000 | Loss: 0.00001102
Iteration 152/1000 | Loss: 0.00001102
Iteration 153/1000 | Loss: 0.00001102
Iteration 154/1000 | Loss: 0.00001102
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001102
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001102
Iteration 159/1000 | Loss: 0.00001102
Iteration 160/1000 | Loss: 0.00001102
Iteration 161/1000 | Loss: 0.00001102
Iteration 162/1000 | Loss: 0.00001102
Iteration 163/1000 | Loss: 0.00001101
Iteration 164/1000 | Loss: 0.00001101
Iteration 165/1000 | Loss: 0.00001101
Iteration 166/1000 | Loss: 0.00001101
Iteration 167/1000 | Loss: 0.00001101
Iteration 168/1000 | Loss: 0.00001101
Iteration 169/1000 | Loss: 0.00001101
Iteration 170/1000 | Loss: 0.00001101
Iteration 171/1000 | Loss: 0.00001101
Iteration 172/1000 | Loss: 0.00001101
Iteration 173/1000 | Loss: 0.00001101
Iteration 174/1000 | Loss: 0.00001101
Iteration 175/1000 | Loss: 0.00001101
Iteration 176/1000 | Loss: 0.00001101
Iteration 177/1000 | Loss: 0.00001101
Iteration 178/1000 | Loss: 0.00001101
Iteration 179/1000 | Loss: 0.00001100
Iteration 180/1000 | Loss: 0.00001100
Iteration 181/1000 | Loss: 0.00001100
Iteration 182/1000 | Loss: 0.00001100
Iteration 183/1000 | Loss: 0.00001100
Iteration 184/1000 | Loss: 0.00001100
Iteration 185/1000 | Loss: 0.00001100
Iteration 186/1000 | Loss: 0.00001100
Iteration 187/1000 | Loss: 0.00001100
Iteration 188/1000 | Loss: 0.00001100
Iteration 189/1000 | Loss: 0.00001100
Iteration 190/1000 | Loss: 0.00001100
Iteration 191/1000 | Loss: 0.00001100
Iteration 192/1000 | Loss: 0.00001100
Iteration 193/1000 | Loss: 0.00001100
Iteration 194/1000 | Loss: 0.00001100
Iteration 195/1000 | Loss: 0.00001100
Iteration 196/1000 | Loss: 0.00001100
Iteration 197/1000 | Loss: 0.00001100
Iteration 198/1000 | Loss: 0.00001100
Iteration 199/1000 | Loss: 0.00001100
Iteration 200/1000 | Loss: 0.00001100
Iteration 201/1000 | Loss: 0.00001100
Iteration 202/1000 | Loss: 0.00001100
Iteration 203/1000 | Loss: 0.00001100
Iteration 204/1000 | Loss: 0.00001100
Iteration 205/1000 | Loss: 0.00001100
Iteration 206/1000 | Loss: 0.00001100
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 206. Stopping optimization.
Last 5 losses: [1.099550718208775e-05, 1.099550718208775e-05, 1.099550718208775e-05, 1.099550718208775e-05, 1.099550718208775e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.099550718208775e-05

Optimization complete. Final v2v error: 2.7977724075317383 mm

Highest mean error: 3.0819265842437744 mm for frame 90

Lowest mean error: 2.603396415710449 mm for frame 157

Saving results

Total time: 40.08671474456787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00362271
Iteration 2/25 | Loss: 0.00105185
Iteration 3/25 | Loss: 0.00086510
Iteration 4/25 | Loss: 0.00081218
Iteration 5/25 | Loss: 0.00079707
Iteration 6/25 | Loss: 0.00079314
Iteration 7/25 | Loss: 0.00079170
Iteration 8/25 | Loss: 0.00079126
Iteration 9/25 | Loss: 0.00079126
Iteration 10/25 | Loss: 0.00079126
Iteration 11/25 | Loss: 0.00079126
Iteration 12/25 | Loss: 0.00079126
Iteration 13/25 | Loss: 0.00079126
Iteration 14/25 | Loss: 0.00079126
Iteration 15/25 | Loss: 0.00079126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007912577129900455, 0.0007912577129900455, 0.0007912577129900455, 0.0007912577129900455, 0.0007912577129900455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007912577129900455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66066074
Iteration 2/25 | Loss: 0.00172203
Iteration 3/25 | Loss: 0.00172203
Iteration 4/25 | Loss: 0.00172203
Iteration 5/25 | Loss: 0.00172203
Iteration 6/25 | Loss: 0.00172203
Iteration 7/25 | Loss: 0.00172203
Iteration 8/25 | Loss: 0.00172203
Iteration 9/25 | Loss: 0.00172203
Iteration 10/25 | Loss: 0.00172203
Iteration 11/25 | Loss: 0.00172203
Iteration 12/25 | Loss: 0.00172203
Iteration 13/25 | Loss: 0.00172203
Iteration 14/25 | Loss: 0.00172203
Iteration 15/25 | Loss: 0.00172203
Iteration 16/25 | Loss: 0.00172203
Iteration 17/25 | Loss: 0.00172203
Iteration 18/25 | Loss: 0.00172203
Iteration 19/25 | Loss: 0.00172203
Iteration 20/25 | Loss: 0.00172203
Iteration 21/25 | Loss: 0.00172203
Iteration 22/25 | Loss: 0.00172203
Iteration 23/25 | Loss: 0.00172203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001722026034258306, 0.001722026034258306, 0.001722026034258306, 0.001722026034258306, 0.001722026034258306]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001722026034258306

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172203
Iteration 2/1000 | Loss: 0.00004141
Iteration 3/1000 | Loss: 0.00002962
Iteration 4/1000 | Loss: 0.00002255
Iteration 5/1000 | Loss: 0.00002073
Iteration 6/1000 | Loss: 0.00001937
Iteration 7/1000 | Loss: 0.00001855
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001758
Iteration 10/1000 | Loss: 0.00001728
Iteration 11/1000 | Loss: 0.00001704
Iteration 12/1000 | Loss: 0.00001688
Iteration 13/1000 | Loss: 0.00001685
Iteration 14/1000 | Loss: 0.00001674
Iteration 15/1000 | Loss: 0.00001657
Iteration 16/1000 | Loss: 0.00001653
Iteration 17/1000 | Loss: 0.00001652
Iteration 18/1000 | Loss: 0.00001651
Iteration 19/1000 | Loss: 0.00001649
Iteration 20/1000 | Loss: 0.00001648
Iteration 21/1000 | Loss: 0.00001647
Iteration 22/1000 | Loss: 0.00001647
Iteration 23/1000 | Loss: 0.00001646
Iteration 24/1000 | Loss: 0.00001645
Iteration 25/1000 | Loss: 0.00001644
Iteration 26/1000 | Loss: 0.00001642
Iteration 27/1000 | Loss: 0.00001641
Iteration 28/1000 | Loss: 0.00001641
Iteration 29/1000 | Loss: 0.00001641
Iteration 30/1000 | Loss: 0.00001640
Iteration 31/1000 | Loss: 0.00001637
Iteration 32/1000 | Loss: 0.00001637
Iteration 33/1000 | Loss: 0.00001634
Iteration 34/1000 | Loss: 0.00001633
Iteration 35/1000 | Loss: 0.00001633
Iteration 36/1000 | Loss: 0.00001633
Iteration 37/1000 | Loss: 0.00001633
Iteration 38/1000 | Loss: 0.00001633
Iteration 39/1000 | Loss: 0.00001633
Iteration 40/1000 | Loss: 0.00001633
Iteration 41/1000 | Loss: 0.00001632
Iteration 42/1000 | Loss: 0.00001632
Iteration 43/1000 | Loss: 0.00001631
Iteration 44/1000 | Loss: 0.00001630
Iteration 45/1000 | Loss: 0.00001630
Iteration 46/1000 | Loss: 0.00001630
Iteration 47/1000 | Loss: 0.00001629
Iteration 48/1000 | Loss: 0.00001629
Iteration 49/1000 | Loss: 0.00001629
Iteration 50/1000 | Loss: 0.00001629
Iteration 51/1000 | Loss: 0.00001628
Iteration 52/1000 | Loss: 0.00001628
Iteration 53/1000 | Loss: 0.00001628
Iteration 54/1000 | Loss: 0.00001627
Iteration 55/1000 | Loss: 0.00001627
Iteration 56/1000 | Loss: 0.00001627
Iteration 57/1000 | Loss: 0.00001627
Iteration 58/1000 | Loss: 0.00001626
Iteration 59/1000 | Loss: 0.00001626
Iteration 60/1000 | Loss: 0.00001626
Iteration 61/1000 | Loss: 0.00001626
Iteration 62/1000 | Loss: 0.00001626
Iteration 63/1000 | Loss: 0.00001626
Iteration 64/1000 | Loss: 0.00001625
Iteration 65/1000 | Loss: 0.00001625
Iteration 66/1000 | Loss: 0.00001625
Iteration 67/1000 | Loss: 0.00001625
Iteration 68/1000 | Loss: 0.00001625
Iteration 69/1000 | Loss: 0.00001624
Iteration 70/1000 | Loss: 0.00001624
Iteration 71/1000 | Loss: 0.00001624
Iteration 72/1000 | Loss: 0.00001624
Iteration 73/1000 | Loss: 0.00001624
Iteration 74/1000 | Loss: 0.00001624
Iteration 75/1000 | Loss: 0.00001624
Iteration 76/1000 | Loss: 0.00001623
Iteration 77/1000 | Loss: 0.00001623
Iteration 78/1000 | Loss: 0.00001623
Iteration 79/1000 | Loss: 0.00001623
Iteration 80/1000 | Loss: 0.00001623
Iteration 81/1000 | Loss: 0.00001623
Iteration 82/1000 | Loss: 0.00001622
Iteration 83/1000 | Loss: 0.00001622
Iteration 84/1000 | Loss: 0.00001622
Iteration 85/1000 | Loss: 0.00001622
Iteration 86/1000 | Loss: 0.00001622
Iteration 87/1000 | Loss: 0.00001622
Iteration 88/1000 | Loss: 0.00001622
Iteration 89/1000 | Loss: 0.00001622
Iteration 90/1000 | Loss: 0.00001622
Iteration 91/1000 | Loss: 0.00001622
Iteration 92/1000 | Loss: 0.00001622
Iteration 93/1000 | Loss: 0.00001622
Iteration 94/1000 | Loss: 0.00001622
Iteration 95/1000 | Loss: 0.00001622
Iteration 96/1000 | Loss: 0.00001622
Iteration 97/1000 | Loss: 0.00001622
Iteration 98/1000 | Loss: 0.00001622
Iteration 99/1000 | Loss: 0.00001622
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001622
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001622
Iteration 105/1000 | Loss: 0.00001622
Iteration 106/1000 | Loss: 0.00001622
Iteration 107/1000 | Loss: 0.00001622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [1.6218262317124754e-05, 1.6218262317124754e-05, 1.6218262317124754e-05, 1.6218262317124754e-05, 1.6218262317124754e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6218262317124754e-05

Optimization complete. Final v2v error: 3.3270316123962402 mm

Highest mean error: 4.671680927276611 mm for frame 151

Lowest mean error: 2.522543430328369 mm for frame 166

Saving results

Total time: 40.84625840187073
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470910
Iteration 2/25 | Loss: 0.00101928
Iteration 3/25 | Loss: 0.00088138
Iteration 4/25 | Loss: 0.00084549
Iteration 5/25 | Loss: 0.00083759
Iteration 6/25 | Loss: 0.00083505
Iteration 7/25 | Loss: 0.00083500
Iteration 8/25 | Loss: 0.00083500
Iteration 9/25 | Loss: 0.00083500
Iteration 10/25 | Loss: 0.00083500
Iteration 11/25 | Loss: 0.00083500
Iteration 12/25 | Loss: 0.00083500
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008350034477189183, 0.0008350034477189183, 0.0008350034477189183, 0.0008350034477189183, 0.0008350034477189183]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008350034477189183

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.98416805
Iteration 2/25 | Loss: 0.00121637
Iteration 3/25 | Loss: 0.00121636
Iteration 4/25 | Loss: 0.00121635
Iteration 5/25 | Loss: 0.00121635
Iteration 6/25 | Loss: 0.00121635
Iteration 7/25 | Loss: 0.00121635
Iteration 8/25 | Loss: 0.00121635
Iteration 9/25 | Loss: 0.00121635
Iteration 10/25 | Loss: 0.00121635
Iteration 11/25 | Loss: 0.00121635
Iteration 12/25 | Loss: 0.00121635
Iteration 13/25 | Loss: 0.00121635
Iteration 14/25 | Loss: 0.00121635
Iteration 15/25 | Loss: 0.00121635
Iteration 16/25 | Loss: 0.00121635
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012163525680080056, 0.0012163525680080056, 0.0012163525680080056, 0.0012163525680080056, 0.0012163525680080056]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012163525680080056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121635
Iteration 2/1000 | Loss: 0.00003793
Iteration 3/1000 | Loss: 0.00002742
Iteration 4/1000 | Loss: 0.00002576
Iteration 5/1000 | Loss: 0.00002463
Iteration 6/1000 | Loss: 0.00002406
Iteration 7/1000 | Loss: 0.00002361
Iteration 8/1000 | Loss: 0.00002328
Iteration 9/1000 | Loss: 0.00002303
Iteration 10/1000 | Loss: 0.00002292
Iteration 11/1000 | Loss: 0.00002281
Iteration 12/1000 | Loss: 0.00002278
Iteration 13/1000 | Loss: 0.00002278
Iteration 14/1000 | Loss: 0.00002277
Iteration 15/1000 | Loss: 0.00002277
Iteration 16/1000 | Loss: 0.00002277
Iteration 17/1000 | Loss: 0.00002273
Iteration 18/1000 | Loss: 0.00002273
Iteration 19/1000 | Loss: 0.00002269
Iteration 20/1000 | Loss: 0.00002269
Iteration 21/1000 | Loss: 0.00002269
Iteration 22/1000 | Loss: 0.00002269
Iteration 23/1000 | Loss: 0.00002269
Iteration 24/1000 | Loss: 0.00002268
Iteration 25/1000 | Loss: 0.00002268
Iteration 26/1000 | Loss: 0.00002268
Iteration 27/1000 | Loss: 0.00002268
Iteration 28/1000 | Loss: 0.00002268
Iteration 29/1000 | Loss: 0.00002268
Iteration 30/1000 | Loss: 0.00002268
Iteration 31/1000 | Loss: 0.00002268
Iteration 32/1000 | Loss: 0.00002268
Iteration 33/1000 | Loss: 0.00002268
Iteration 34/1000 | Loss: 0.00002268
Iteration 35/1000 | Loss: 0.00002268
Iteration 36/1000 | Loss: 0.00002267
Iteration 37/1000 | Loss: 0.00002267
Iteration 38/1000 | Loss: 0.00002266
Iteration 39/1000 | Loss: 0.00002265
Iteration 40/1000 | Loss: 0.00002265
Iteration 41/1000 | Loss: 0.00002265
Iteration 42/1000 | Loss: 0.00002264
Iteration 43/1000 | Loss: 0.00002264
Iteration 44/1000 | Loss: 0.00002264
Iteration 45/1000 | Loss: 0.00002264
Iteration 46/1000 | Loss: 0.00002263
Iteration 47/1000 | Loss: 0.00002263
Iteration 48/1000 | Loss: 0.00002262
Iteration 49/1000 | Loss: 0.00002262
Iteration 50/1000 | Loss: 0.00002262
Iteration 51/1000 | Loss: 0.00002262
Iteration 52/1000 | Loss: 0.00002262
Iteration 53/1000 | Loss: 0.00002262
Iteration 54/1000 | Loss: 0.00002262
Iteration 55/1000 | Loss: 0.00002261
Iteration 56/1000 | Loss: 0.00002261
Iteration 57/1000 | Loss: 0.00002261
Iteration 58/1000 | Loss: 0.00002261
Iteration 59/1000 | Loss: 0.00002261
Iteration 60/1000 | Loss: 0.00002261
Iteration 61/1000 | Loss: 0.00002261
Iteration 62/1000 | Loss: 0.00002261
Iteration 63/1000 | Loss: 0.00002261
Iteration 64/1000 | Loss: 0.00002260
Iteration 65/1000 | Loss: 0.00002260
Iteration 66/1000 | Loss: 0.00002259
Iteration 67/1000 | Loss: 0.00002259
Iteration 68/1000 | Loss: 0.00002259
Iteration 69/1000 | Loss: 0.00002259
Iteration 70/1000 | Loss: 0.00002259
Iteration 71/1000 | Loss: 0.00002258
Iteration 72/1000 | Loss: 0.00002258
Iteration 73/1000 | Loss: 0.00002258
Iteration 74/1000 | Loss: 0.00002258
Iteration 75/1000 | Loss: 0.00002258
Iteration 76/1000 | Loss: 0.00002258
Iteration 77/1000 | Loss: 0.00002258
Iteration 78/1000 | Loss: 0.00002258
Iteration 79/1000 | Loss: 0.00002258
Iteration 80/1000 | Loss: 0.00002256
Iteration 81/1000 | Loss: 0.00002256
Iteration 82/1000 | Loss: 0.00002256
Iteration 83/1000 | Loss: 0.00002256
Iteration 84/1000 | Loss: 0.00002255
Iteration 85/1000 | Loss: 0.00002255
Iteration 86/1000 | Loss: 0.00002255
Iteration 87/1000 | Loss: 0.00002255
Iteration 88/1000 | Loss: 0.00002255
Iteration 89/1000 | Loss: 0.00002255
Iteration 90/1000 | Loss: 0.00002255
Iteration 91/1000 | Loss: 0.00002255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [2.2554049792233855e-05, 2.2554049792233855e-05, 2.2554049792233855e-05, 2.2554049792233855e-05, 2.2554049792233855e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2554049792233855e-05

Optimization complete. Final v2v error: 3.962857246398926 mm

Highest mean error: 4.476535320281982 mm for frame 227

Lowest mean error: 3.598480701446533 mm for frame 70

Saving results

Total time: 37.26268720626831
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01055177
Iteration 2/25 | Loss: 0.00427379
Iteration 3/25 | Loss: 0.00257930
Iteration 4/25 | Loss: 0.00230053
Iteration 5/25 | Loss: 0.00202827
Iteration 6/25 | Loss: 0.00171653
Iteration 7/25 | Loss: 0.00174153
Iteration 8/25 | Loss: 0.00171022
Iteration 9/25 | Loss: 0.00156132
Iteration 10/25 | Loss: 0.00150091
Iteration 11/25 | Loss: 0.00145114
Iteration 12/25 | Loss: 0.00144869
Iteration 13/25 | Loss: 0.00138126
Iteration 14/25 | Loss: 0.00132677
Iteration 15/25 | Loss: 0.00133555
Iteration 16/25 | Loss: 0.00129183
Iteration 17/25 | Loss: 0.00124025
Iteration 18/25 | Loss: 0.00120221
Iteration 19/25 | Loss: 0.00116450
Iteration 20/25 | Loss: 0.00115832
Iteration 21/25 | Loss: 0.00117971
Iteration 22/25 | Loss: 0.00116925
Iteration 23/25 | Loss: 0.00114798
Iteration 24/25 | Loss: 0.00115127
Iteration 25/25 | Loss: 0.00114352

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.72696197
Iteration 2/25 | Loss: 0.00463113
Iteration 3/25 | Loss: 0.00400887
Iteration 4/25 | Loss: 0.00400887
Iteration 5/25 | Loss: 0.00400887
Iteration 6/25 | Loss: 0.00400887
Iteration 7/25 | Loss: 0.00400887
Iteration 8/25 | Loss: 0.00400887
Iteration 9/25 | Loss: 0.00400887
Iteration 10/25 | Loss: 0.00400887
Iteration 11/25 | Loss: 0.00400887
Iteration 12/25 | Loss: 0.00400887
Iteration 13/25 | Loss: 0.00400887
Iteration 14/25 | Loss: 0.00400887
Iteration 15/25 | Loss: 0.00400887
Iteration 16/25 | Loss: 0.00400887
Iteration 17/25 | Loss: 0.00400887
Iteration 18/25 | Loss: 0.00400887
Iteration 19/25 | Loss: 0.00400887
Iteration 20/25 | Loss: 0.00400887
Iteration 21/25 | Loss: 0.00400887
Iteration 22/25 | Loss: 0.00400887
Iteration 23/25 | Loss: 0.00400887
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004008866380900145, 0.004008866380900145, 0.004008866380900145, 0.004008866380900145, 0.004008866380900145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004008866380900145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00400887
Iteration 2/1000 | Loss: 0.00481309
Iteration 3/1000 | Loss: 0.00100194
Iteration 4/1000 | Loss: 0.00056964
Iteration 5/1000 | Loss: 0.00080922
Iteration 6/1000 | Loss: 0.00145034
Iteration 7/1000 | Loss: 0.00184245
Iteration 8/1000 | Loss: 0.00467102
Iteration 9/1000 | Loss: 0.00377378
Iteration 10/1000 | Loss: 0.00163506
Iteration 11/1000 | Loss: 0.00088041
Iteration 12/1000 | Loss: 0.00011824
Iteration 13/1000 | Loss: 0.00098678
Iteration 14/1000 | Loss: 0.00008550
Iteration 15/1000 | Loss: 0.00043618
Iteration 16/1000 | Loss: 0.00020876
Iteration 17/1000 | Loss: 0.00006406
Iteration 18/1000 | Loss: 0.00010335
Iteration 19/1000 | Loss: 0.00095639
Iteration 20/1000 | Loss: 0.00291914
Iteration 21/1000 | Loss: 0.00267175
Iteration 22/1000 | Loss: 0.00219164
Iteration 23/1000 | Loss: 0.00133849
Iteration 24/1000 | Loss: 0.00012719
Iteration 25/1000 | Loss: 0.00008739
Iteration 26/1000 | Loss: 0.00006319
Iteration 27/1000 | Loss: 0.00077608
Iteration 28/1000 | Loss: 0.00044756
Iteration 29/1000 | Loss: 0.00100478
Iteration 30/1000 | Loss: 0.00064152
Iteration 31/1000 | Loss: 0.00043415
Iteration 32/1000 | Loss: 0.00129513
Iteration 33/1000 | Loss: 0.00093261
Iteration 34/1000 | Loss: 0.00021677
Iteration 35/1000 | Loss: 0.00009679
Iteration 36/1000 | Loss: 0.00024207
Iteration 37/1000 | Loss: 0.00004689
Iteration 38/1000 | Loss: 0.00004224
Iteration 39/1000 | Loss: 0.00003945
Iteration 40/1000 | Loss: 0.00016914
Iteration 41/1000 | Loss: 0.00003617
Iteration 42/1000 | Loss: 0.00003483
Iteration 43/1000 | Loss: 0.00003356
Iteration 44/1000 | Loss: 0.00021701
Iteration 45/1000 | Loss: 0.00003633
Iteration 46/1000 | Loss: 0.00006732
Iteration 47/1000 | Loss: 0.00003531
Iteration 48/1000 | Loss: 0.00003474
Iteration 49/1000 | Loss: 0.00049830
Iteration 50/1000 | Loss: 0.00018198
Iteration 51/1000 | Loss: 0.00009890
Iteration 52/1000 | Loss: 0.00113897
Iteration 53/1000 | Loss: 0.00114743
Iteration 54/1000 | Loss: 0.00058990
Iteration 55/1000 | Loss: 0.00006315
Iteration 56/1000 | Loss: 0.00003804
Iteration 57/1000 | Loss: 0.00011524
Iteration 58/1000 | Loss: 0.00003101
Iteration 59/1000 | Loss: 0.00010035
Iteration 60/1000 | Loss: 0.00002929
Iteration 61/1000 | Loss: 0.00002855
Iteration 62/1000 | Loss: 0.00024511
Iteration 63/1000 | Loss: 0.00002821
Iteration 64/1000 | Loss: 0.00002785
Iteration 65/1000 | Loss: 0.00002766
Iteration 66/1000 | Loss: 0.00002751
Iteration 67/1000 | Loss: 0.00002751
Iteration 68/1000 | Loss: 0.00002748
Iteration 69/1000 | Loss: 0.00002740
Iteration 70/1000 | Loss: 0.00002740
Iteration 71/1000 | Loss: 0.00002740
Iteration 72/1000 | Loss: 0.00002740
Iteration 73/1000 | Loss: 0.00002740
Iteration 74/1000 | Loss: 0.00002740
Iteration 75/1000 | Loss: 0.00002740
Iteration 76/1000 | Loss: 0.00002740
Iteration 77/1000 | Loss: 0.00002740
Iteration 78/1000 | Loss: 0.00002740
Iteration 79/1000 | Loss: 0.00002740
Iteration 80/1000 | Loss: 0.00002740
Iteration 81/1000 | Loss: 0.00002740
Iteration 82/1000 | Loss: 0.00002740
Iteration 83/1000 | Loss: 0.00002740
Iteration 84/1000 | Loss: 0.00002740
Iteration 85/1000 | Loss: 0.00002740
Iteration 86/1000 | Loss: 0.00002740
Iteration 87/1000 | Loss: 0.00002740
Iteration 88/1000 | Loss: 0.00002740
Iteration 89/1000 | Loss: 0.00002740
Iteration 90/1000 | Loss: 0.00002740
Iteration 91/1000 | Loss: 0.00002740
Iteration 92/1000 | Loss: 0.00002740
Iteration 93/1000 | Loss: 0.00002740
Iteration 94/1000 | Loss: 0.00002740
Iteration 95/1000 | Loss: 0.00002740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.7396599762141705e-05, 2.7396599762141705e-05, 2.7396599762141705e-05, 2.7396599762141705e-05, 2.7396599762141705e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7396599762141705e-05

Optimization complete. Final v2v error: 3.7591569423675537 mm

Highest mean error: 11.31636905670166 mm for frame 60

Lowest mean error: 3.3114686012268066 mm for frame 32

Saving results

Total time: 144.79743885993958
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00546469
Iteration 2/25 | Loss: 0.00091377
Iteration 3/25 | Loss: 0.00078950
Iteration 4/25 | Loss: 0.00076211
Iteration 5/25 | Loss: 0.00075747
Iteration 6/25 | Loss: 0.00075636
Iteration 7/25 | Loss: 0.00075611
Iteration 8/25 | Loss: 0.00075611
Iteration 9/25 | Loss: 0.00075611
Iteration 10/25 | Loss: 0.00075611
Iteration 11/25 | Loss: 0.00075611
Iteration 12/25 | Loss: 0.00075611
Iteration 13/25 | Loss: 0.00075611
Iteration 14/25 | Loss: 0.00075611
Iteration 15/25 | Loss: 0.00075611
Iteration 16/25 | Loss: 0.00075611
Iteration 17/25 | Loss: 0.00075611
Iteration 18/25 | Loss: 0.00075611
Iteration 19/25 | Loss: 0.00075611
Iteration 20/25 | Loss: 0.00075611
Iteration 21/25 | Loss: 0.00075611
Iteration 22/25 | Loss: 0.00075611
Iteration 23/25 | Loss: 0.00075611
Iteration 24/25 | Loss: 0.00075611
Iteration 25/25 | Loss: 0.00075611

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.92503071
Iteration 2/25 | Loss: 0.00124208
Iteration 3/25 | Loss: 0.00124207
Iteration 4/25 | Loss: 0.00124206
Iteration 5/25 | Loss: 0.00124206
Iteration 6/25 | Loss: 0.00124206
Iteration 7/25 | Loss: 0.00124206
Iteration 8/25 | Loss: 0.00124206
Iteration 9/25 | Loss: 0.00124206
Iteration 10/25 | Loss: 0.00124206
Iteration 11/25 | Loss: 0.00124206
Iteration 12/25 | Loss: 0.00124206
Iteration 13/25 | Loss: 0.00124206
Iteration 14/25 | Loss: 0.00124206
Iteration 15/25 | Loss: 0.00124206
Iteration 16/25 | Loss: 0.00124206
Iteration 17/25 | Loss: 0.00124206
Iteration 18/25 | Loss: 0.00124206
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012420624261721969, 0.0012420624261721969, 0.0012420624261721969, 0.0012420624261721969, 0.0012420624261721969]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012420624261721969

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124206
Iteration 2/1000 | Loss: 0.00002801
Iteration 3/1000 | Loss: 0.00001892
Iteration 4/1000 | Loss: 0.00001749
Iteration 5/1000 | Loss: 0.00001639
Iteration 6/1000 | Loss: 0.00001593
Iteration 7/1000 | Loss: 0.00001545
Iteration 8/1000 | Loss: 0.00001521
Iteration 9/1000 | Loss: 0.00001498
Iteration 10/1000 | Loss: 0.00001494
Iteration 11/1000 | Loss: 0.00001491
Iteration 12/1000 | Loss: 0.00001490
Iteration 13/1000 | Loss: 0.00001490
Iteration 14/1000 | Loss: 0.00001489
Iteration 15/1000 | Loss: 0.00001482
Iteration 16/1000 | Loss: 0.00001476
Iteration 17/1000 | Loss: 0.00001473
Iteration 18/1000 | Loss: 0.00001471
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001467
Iteration 21/1000 | Loss: 0.00001467
Iteration 22/1000 | Loss: 0.00001466
Iteration 23/1000 | Loss: 0.00001466
Iteration 24/1000 | Loss: 0.00001465
Iteration 25/1000 | Loss: 0.00001463
Iteration 26/1000 | Loss: 0.00001463
Iteration 27/1000 | Loss: 0.00001462
Iteration 28/1000 | Loss: 0.00001462
Iteration 29/1000 | Loss: 0.00001462
Iteration 30/1000 | Loss: 0.00001462
Iteration 31/1000 | Loss: 0.00001462
Iteration 32/1000 | Loss: 0.00001462
Iteration 33/1000 | Loss: 0.00001462
Iteration 34/1000 | Loss: 0.00001461
Iteration 35/1000 | Loss: 0.00001461
Iteration 36/1000 | Loss: 0.00001461
Iteration 37/1000 | Loss: 0.00001461
Iteration 38/1000 | Loss: 0.00001461
Iteration 39/1000 | Loss: 0.00001461
Iteration 40/1000 | Loss: 0.00001460
Iteration 41/1000 | Loss: 0.00001460
Iteration 42/1000 | Loss: 0.00001460
Iteration 43/1000 | Loss: 0.00001460
Iteration 44/1000 | Loss: 0.00001460
Iteration 45/1000 | Loss: 0.00001460
Iteration 46/1000 | Loss: 0.00001460
Iteration 47/1000 | Loss: 0.00001460
Iteration 48/1000 | Loss: 0.00001460
Iteration 49/1000 | Loss: 0.00001460
Iteration 50/1000 | Loss: 0.00001460
Iteration 51/1000 | Loss: 0.00001460
Iteration 52/1000 | Loss: 0.00001460
Iteration 53/1000 | Loss: 0.00001460
Iteration 54/1000 | Loss: 0.00001459
Iteration 55/1000 | Loss: 0.00001459
Iteration 56/1000 | Loss: 0.00001459
Iteration 57/1000 | Loss: 0.00001459
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001459
Iteration 60/1000 | Loss: 0.00001459
Iteration 61/1000 | Loss: 0.00001458
Iteration 62/1000 | Loss: 0.00001458
Iteration 63/1000 | Loss: 0.00001458
Iteration 64/1000 | Loss: 0.00001458
Iteration 65/1000 | Loss: 0.00001458
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001458
Iteration 69/1000 | Loss: 0.00001458
Iteration 70/1000 | Loss: 0.00001458
Iteration 71/1000 | Loss: 0.00001458
Iteration 72/1000 | Loss: 0.00001458
Iteration 73/1000 | Loss: 0.00001458
Iteration 74/1000 | Loss: 0.00001458
Iteration 75/1000 | Loss: 0.00001458
Iteration 76/1000 | Loss: 0.00001458
Iteration 77/1000 | Loss: 0.00001458
Iteration 78/1000 | Loss: 0.00001458
Iteration 79/1000 | Loss: 0.00001458
Iteration 80/1000 | Loss: 0.00001458
Iteration 81/1000 | Loss: 0.00001458
Iteration 82/1000 | Loss: 0.00001458
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [1.457735379517544e-05, 1.457735379517544e-05, 1.457735379517544e-05, 1.457735379517544e-05, 1.457735379517544e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.457735379517544e-05

Optimization complete. Final v2v error: 3.2314987182617188 mm

Highest mean error: 3.9054300785064697 mm for frame 70

Lowest mean error: 2.9345502853393555 mm for frame 27

Saving results

Total time: 31.036897659301758
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00430173
Iteration 2/25 | Loss: 0.00104359
Iteration 3/25 | Loss: 0.00079428
Iteration 4/25 | Loss: 0.00075391
Iteration 5/25 | Loss: 0.00074630
Iteration 6/25 | Loss: 0.00074395
Iteration 7/25 | Loss: 0.00074326
Iteration 8/25 | Loss: 0.00074326
Iteration 9/25 | Loss: 0.00074326
Iteration 10/25 | Loss: 0.00074326
Iteration 11/25 | Loss: 0.00074326
Iteration 12/25 | Loss: 0.00074326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007432631100527942, 0.0007432631100527942, 0.0007432631100527942, 0.0007432631100527942, 0.0007432631100527942]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007432631100527942

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59146023
Iteration 2/25 | Loss: 0.00125954
Iteration 3/25 | Loss: 0.00125954
Iteration 4/25 | Loss: 0.00125954
Iteration 5/25 | Loss: 0.00125953
Iteration 6/25 | Loss: 0.00125953
Iteration 7/25 | Loss: 0.00125953
Iteration 8/25 | Loss: 0.00125953
Iteration 9/25 | Loss: 0.00125953
Iteration 10/25 | Loss: 0.00125953
Iteration 11/25 | Loss: 0.00125953
Iteration 12/25 | Loss: 0.00125953
Iteration 13/25 | Loss: 0.00125953
Iteration 14/25 | Loss: 0.00125953
Iteration 15/25 | Loss: 0.00125953
Iteration 16/25 | Loss: 0.00125953
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012595331063494086, 0.0012595331063494086, 0.0012595331063494086, 0.0012595331063494086, 0.0012595331063494086]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012595331063494086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125953
Iteration 2/1000 | Loss: 0.00002385
Iteration 3/1000 | Loss: 0.00001486
Iteration 4/1000 | Loss: 0.00001313
Iteration 5/1000 | Loss: 0.00001231
Iteration 6/1000 | Loss: 0.00001165
Iteration 7/1000 | Loss: 0.00001131
Iteration 8/1000 | Loss: 0.00001108
Iteration 9/1000 | Loss: 0.00001094
Iteration 10/1000 | Loss: 0.00001084
Iteration 11/1000 | Loss: 0.00001083
Iteration 12/1000 | Loss: 0.00001083
Iteration 13/1000 | Loss: 0.00001081
Iteration 14/1000 | Loss: 0.00001077
Iteration 15/1000 | Loss: 0.00001073
Iteration 16/1000 | Loss: 0.00001070
Iteration 17/1000 | Loss: 0.00001063
Iteration 18/1000 | Loss: 0.00001057
Iteration 19/1000 | Loss: 0.00001056
Iteration 20/1000 | Loss: 0.00001056
Iteration 21/1000 | Loss: 0.00001056
Iteration 22/1000 | Loss: 0.00001055
Iteration 23/1000 | Loss: 0.00001055
Iteration 24/1000 | Loss: 0.00001054
Iteration 25/1000 | Loss: 0.00001054
Iteration 26/1000 | Loss: 0.00001053
Iteration 27/1000 | Loss: 0.00001053
Iteration 28/1000 | Loss: 0.00001053
Iteration 29/1000 | Loss: 0.00001052
Iteration 30/1000 | Loss: 0.00001052
Iteration 31/1000 | Loss: 0.00001052
Iteration 32/1000 | Loss: 0.00001050
Iteration 33/1000 | Loss: 0.00001048
Iteration 34/1000 | Loss: 0.00001048
Iteration 35/1000 | Loss: 0.00001047
Iteration 36/1000 | Loss: 0.00001047
Iteration 37/1000 | Loss: 0.00001047
Iteration 38/1000 | Loss: 0.00001046
Iteration 39/1000 | Loss: 0.00001046
Iteration 40/1000 | Loss: 0.00001045
Iteration 41/1000 | Loss: 0.00001045
Iteration 42/1000 | Loss: 0.00001044
Iteration 43/1000 | Loss: 0.00001044
Iteration 44/1000 | Loss: 0.00001044
Iteration 45/1000 | Loss: 0.00001043
Iteration 46/1000 | Loss: 0.00001043
Iteration 47/1000 | Loss: 0.00001043
Iteration 48/1000 | Loss: 0.00001043
Iteration 49/1000 | Loss: 0.00001042
Iteration 50/1000 | Loss: 0.00001042
Iteration 51/1000 | Loss: 0.00001042
Iteration 52/1000 | Loss: 0.00001041
Iteration 53/1000 | Loss: 0.00001041
Iteration 54/1000 | Loss: 0.00001041
Iteration 55/1000 | Loss: 0.00001041
Iteration 56/1000 | Loss: 0.00001041
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001041
Iteration 59/1000 | Loss: 0.00001041
Iteration 60/1000 | Loss: 0.00001040
Iteration 61/1000 | Loss: 0.00001040
Iteration 62/1000 | Loss: 0.00001040
Iteration 63/1000 | Loss: 0.00001040
Iteration 64/1000 | Loss: 0.00001040
Iteration 65/1000 | Loss: 0.00001040
Iteration 66/1000 | Loss: 0.00001040
Iteration 67/1000 | Loss: 0.00001040
Iteration 68/1000 | Loss: 0.00001040
Iteration 69/1000 | Loss: 0.00001040
Iteration 70/1000 | Loss: 0.00001040
Iteration 71/1000 | Loss: 0.00001039
Iteration 72/1000 | Loss: 0.00001039
Iteration 73/1000 | Loss: 0.00001039
Iteration 74/1000 | Loss: 0.00001039
Iteration 75/1000 | Loss: 0.00001039
Iteration 76/1000 | Loss: 0.00001039
Iteration 77/1000 | Loss: 0.00001039
Iteration 78/1000 | Loss: 0.00001039
Iteration 79/1000 | Loss: 0.00001039
Iteration 80/1000 | Loss: 0.00001039
Iteration 81/1000 | Loss: 0.00001039
Iteration 82/1000 | Loss: 0.00001039
Iteration 83/1000 | Loss: 0.00001038
Iteration 84/1000 | Loss: 0.00001038
Iteration 85/1000 | Loss: 0.00001038
Iteration 86/1000 | Loss: 0.00001038
Iteration 87/1000 | Loss: 0.00001037
Iteration 88/1000 | Loss: 0.00001037
Iteration 89/1000 | Loss: 0.00001037
Iteration 90/1000 | Loss: 0.00001037
Iteration 91/1000 | Loss: 0.00001037
Iteration 92/1000 | Loss: 0.00001037
Iteration 93/1000 | Loss: 0.00001037
Iteration 94/1000 | Loss: 0.00001037
Iteration 95/1000 | Loss: 0.00001037
Iteration 96/1000 | Loss: 0.00001037
Iteration 97/1000 | Loss: 0.00001037
Iteration 98/1000 | Loss: 0.00001036
Iteration 99/1000 | Loss: 0.00001036
Iteration 100/1000 | Loss: 0.00001036
Iteration 101/1000 | Loss: 0.00001036
Iteration 102/1000 | Loss: 0.00001036
Iteration 103/1000 | Loss: 0.00001036
Iteration 104/1000 | Loss: 0.00001036
Iteration 105/1000 | Loss: 0.00001036
Iteration 106/1000 | Loss: 0.00001036
Iteration 107/1000 | Loss: 0.00001036
Iteration 108/1000 | Loss: 0.00001036
Iteration 109/1000 | Loss: 0.00001036
Iteration 110/1000 | Loss: 0.00001036
Iteration 111/1000 | Loss: 0.00001036
Iteration 112/1000 | Loss: 0.00001036
Iteration 113/1000 | Loss: 0.00001036
Iteration 114/1000 | Loss: 0.00001036
Iteration 115/1000 | Loss: 0.00001036
Iteration 116/1000 | Loss: 0.00001036
Iteration 117/1000 | Loss: 0.00001036
Iteration 118/1000 | Loss: 0.00001036
Iteration 119/1000 | Loss: 0.00001036
Iteration 120/1000 | Loss: 0.00001036
Iteration 121/1000 | Loss: 0.00001036
Iteration 122/1000 | Loss: 0.00001036
Iteration 123/1000 | Loss: 0.00001036
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.0362639841332566e-05, 1.0362639841332566e-05, 1.0362639841332566e-05, 1.0362639841332566e-05, 1.0362639841332566e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0362639841332566e-05

Optimization complete. Final v2v error: 2.748481273651123 mm

Highest mean error: 3.44173002243042 mm for frame 90

Lowest mean error: 2.6250758171081543 mm for frame 15

Saving results

Total time: 38.4206223487854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_013/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_013/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00427342
Iteration 2/25 | Loss: 0.00091515
Iteration 3/25 | Loss: 0.00081949
Iteration 4/25 | Loss: 0.00079282
Iteration 5/25 | Loss: 0.00078767
Iteration 6/25 | Loss: 0.00078618
Iteration 7/25 | Loss: 0.00078600
Iteration 8/25 | Loss: 0.00078600
Iteration 9/25 | Loss: 0.00078600
Iteration 10/25 | Loss: 0.00078600
Iteration 11/25 | Loss: 0.00078600
Iteration 12/25 | Loss: 0.00078600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007860020268708467, 0.0007860020268708467, 0.0007860020268708467, 0.0007860020268708467, 0.0007860020268708467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007860020268708467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68119967
Iteration 2/25 | Loss: 0.00126789
Iteration 3/25 | Loss: 0.00126789
Iteration 4/25 | Loss: 0.00126789
Iteration 5/25 | Loss: 0.00126789
Iteration 6/25 | Loss: 0.00126789
Iteration 7/25 | Loss: 0.00126789
Iteration 8/25 | Loss: 0.00126789
Iteration 9/25 | Loss: 0.00126789
Iteration 10/25 | Loss: 0.00126789
Iteration 11/25 | Loss: 0.00126789
Iteration 12/25 | Loss: 0.00126789
Iteration 13/25 | Loss: 0.00126789
Iteration 14/25 | Loss: 0.00126789
Iteration 15/25 | Loss: 0.00126789
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012678877683356404, 0.0012678877683356404, 0.0012678877683356404, 0.0012678877683356404, 0.0012678877683356404]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012678877683356404

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126789
Iteration 2/1000 | Loss: 0.00003843
Iteration 3/1000 | Loss: 0.00002217
Iteration 4/1000 | Loss: 0.00002069
Iteration 5/1000 | Loss: 0.00001947
Iteration 6/1000 | Loss: 0.00001935
Iteration 7/1000 | Loss: 0.00001878
Iteration 8/1000 | Loss: 0.00001845
Iteration 9/1000 | Loss: 0.00001842
Iteration 10/1000 | Loss: 0.00001840
Iteration 11/1000 | Loss: 0.00001840
Iteration 12/1000 | Loss: 0.00001816
Iteration 13/1000 | Loss: 0.00001809
Iteration 14/1000 | Loss: 0.00001806
Iteration 15/1000 | Loss: 0.00001804
Iteration 16/1000 | Loss: 0.00001802
Iteration 17/1000 | Loss: 0.00001798
Iteration 18/1000 | Loss: 0.00001798
Iteration 19/1000 | Loss: 0.00001795
Iteration 20/1000 | Loss: 0.00001794
Iteration 21/1000 | Loss: 0.00001794
Iteration 22/1000 | Loss: 0.00001794
Iteration 23/1000 | Loss: 0.00001794
Iteration 24/1000 | Loss: 0.00001793
Iteration 25/1000 | Loss: 0.00001793
Iteration 26/1000 | Loss: 0.00001791
Iteration 27/1000 | Loss: 0.00001791
Iteration 28/1000 | Loss: 0.00001791
Iteration 29/1000 | Loss: 0.00001791
Iteration 30/1000 | Loss: 0.00001790
Iteration 31/1000 | Loss: 0.00001790
Iteration 32/1000 | Loss: 0.00001790
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001788
Iteration 37/1000 | Loss: 0.00001788
Iteration 38/1000 | Loss: 0.00001788
Iteration 39/1000 | Loss: 0.00001788
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001788
Iteration 42/1000 | Loss: 0.00001788
Iteration 43/1000 | Loss: 0.00001788
Iteration 44/1000 | Loss: 0.00001787
Iteration 45/1000 | Loss: 0.00001787
Iteration 46/1000 | Loss: 0.00001787
Iteration 47/1000 | Loss: 0.00001787
Iteration 48/1000 | Loss: 0.00001786
Iteration 49/1000 | Loss: 0.00001786
Iteration 50/1000 | Loss: 0.00001786
Iteration 51/1000 | Loss: 0.00001786
Iteration 52/1000 | Loss: 0.00001786
Iteration 53/1000 | Loss: 0.00001786
Iteration 54/1000 | Loss: 0.00001785
Iteration 55/1000 | Loss: 0.00001785
Iteration 56/1000 | Loss: 0.00001785
Iteration 57/1000 | Loss: 0.00001785
Iteration 58/1000 | Loss: 0.00001785
Iteration 59/1000 | Loss: 0.00001785
Iteration 60/1000 | Loss: 0.00001785
Iteration 61/1000 | Loss: 0.00001785
Iteration 62/1000 | Loss: 0.00001784
Iteration 63/1000 | Loss: 0.00001784
Iteration 64/1000 | Loss: 0.00001784
Iteration 65/1000 | Loss: 0.00001784
Iteration 66/1000 | Loss: 0.00001784
Iteration 67/1000 | Loss: 0.00001784
Iteration 68/1000 | Loss: 0.00001784
Iteration 69/1000 | Loss: 0.00001784
Iteration 70/1000 | Loss: 0.00001784
Iteration 71/1000 | Loss: 0.00001784
Iteration 72/1000 | Loss: 0.00001784
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001784
Iteration 75/1000 | Loss: 0.00001784
Iteration 76/1000 | Loss: 0.00001784
Iteration 77/1000 | Loss: 0.00001784
Iteration 78/1000 | Loss: 0.00001783
Iteration 79/1000 | Loss: 0.00001783
Iteration 80/1000 | Loss: 0.00001783
Iteration 81/1000 | Loss: 0.00001783
Iteration 82/1000 | Loss: 0.00001783
Iteration 83/1000 | Loss: 0.00001783
Iteration 84/1000 | Loss: 0.00001783
Iteration 85/1000 | Loss: 0.00001783
Iteration 86/1000 | Loss: 0.00001783
Iteration 87/1000 | Loss: 0.00001783
Iteration 88/1000 | Loss: 0.00001783
Iteration 89/1000 | Loss: 0.00001783
Iteration 90/1000 | Loss: 0.00001783
Iteration 91/1000 | Loss: 0.00001783
Iteration 92/1000 | Loss: 0.00001782
Iteration 93/1000 | Loss: 0.00001782
Iteration 94/1000 | Loss: 0.00001782
Iteration 95/1000 | Loss: 0.00001782
Iteration 96/1000 | Loss: 0.00001782
Iteration 97/1000 | Loss: 0.00001781
Iteration 98/1000 | Loss: 0.00001781
Iteration 99/1000 | Loss: 0.00001781
Iteration 100/1000 | Loss: 0.00001780
Iteration 101/1000 | Loss: 0.00001780
Iteration 102/1000 | Loss: 0.00001780
Iteration 103/1000 | Loss: 0.00001780
Iteration 104/1000 | Loss: 0.00001780
Iteration 105/1000 | Loss: 0.00001779
Iteration 106/1000 | Loss: 0.00001779
Iteration 107/1000 | Loss: 0.00001779
Iteration 108/1000 | Loss: 0.00001779
Iteration 109/1000 | Loss: 0.00001778
Iteration 110/1000 | Loss: 0.00001778
Iteration 111/1000 | Loss: 0.00001777
Iteration 112/1000 | Loss: 0.00001777
Iteration 113/1000 | Loss: 0.00001776
Iteration 114/1000 | Loss: 0.00001776
Iteration 115/1000 | Loss: 0.00001775
Iteration 116/1000 | Loss: 0.00001775
Iteration 117/1000 | Loss: 0.00001774
Iteration 118/1000 | Loss: 0.00001774
Iteration 119/1000 | Loss: 0.00001774
Iteration 120/1000 | Loss: 0.00001774
Iteration 121/1000 | Loss: 0.00001774
Iteration 122/1000 | Loss: 0.00001773
Iteration 123/1000 | Loss: 0.00001773
Iteration 124/1000 | Loss: 0.00001773
Iteration 125/1000 | Loss: 0.00001773
Iteration 126/1000 | Loss: 0.00001773
Iteration 127/1000 | Loss: 0.00001773
Iteration 128/1000 | Loss: 0.00001773
Iteration 129/1000 | Loss: 0.00001773
Iteration 130/1000 | Loss: 0.00001773
Iteration 131/1000 | Loss: 0.00001773
Iteration 132/1000 | Loss: 0.00001773
Iteration 133/1000 | Loss: 0.00001773
Iteration 134/1000 | Loss: 0.00001772
Iteration 135/1000 | Loss: 0.00001772
Iteration 136/1000 | Loss: 0.00001772
Iteration 137/1000 | Loss: 0.00001771
Iteration 138/1000 | Loss: 0.00001771
Iteration 139/1000 | Loss: 0.00001771
Iteration 140/1000 | Loss: 0.00001770
Iteration 141/1000 | Loss: 0.00001770
Iteration 142/1000 | Loss: 0.00001770
Iteration 143/1000 | Loss: 0.00001770
Iteration 144/1000 | Loss: 0.00001770
Iteration 145/1000 | Loss: 0.00001770
Iteration 146/1000 | Loss: 0.00001770
Iteration 147/1000 | Loss: 0.00001770
Iteration 148/1000 | Loss: 0.00001770
Iteration 149/1000 | Loss: 0.00001770
Iteration 150/1000 | Loss: 0.00001769
Iteration 151/1000 | Loss: 0.00001769
Iteration 152/1000 | Loss: 0.00001769
Iteration 153/1000 | Loss: 0.00001769
Iteration 154/1000 | Loss: 0.00001768
Iteration 155/1000 | Loss: 0.00001768
Iteration 156/1000 | Loss: 0.00001768
Iteration 157/1000 | Loss: 0.00001768
Iteration 158/1000 | Loss: 0.00001768
Iteration 159/1000 | Loss: 0.00001768
Iteration 160/1000 | Loss: 0.00001768
Iteration 161/1000 | Loss: 0.00001768
Iteration 162/1000 | Loss: 0.00001768
Iteration 163/1000 | Loss: 0.00001768
Iteration 164/1000 | Loss: 0.00001768
Iteration 165/1000 | Loss: 0.00001768
Iteration 166/1000 | Loss: 0.00001768
Iteration 167/1000 | Loss: 0.00001767
Iteration 168/1000 | Loss: 0.00001767
Iteration 169/1000 | Loss: 0.00001767
Iteration 170/1000 | Loss: 0.00001767
Iteration 171/1000 | Loss: 0.00001767
Iteration 172/1000 | Loss: 0.00001767
Iteration 173/1000 | Loss: 0.00001767
Iteration 174/1000 | Loss: 0.00001767
Iteration 175/1000 | Loss: 0.00001767
Iteration 176/1000 | Loss: 0.00001767
Iteration 177/1000 | Loss: 0.00001767
Iteration 178/1000 | Loss: 0.00001767
Iteration 179/1000 | Loss: 0.00001767
Iteration 180/1000 | Loss: 0.00001766
Iteration 181/1000 | Loss: 0.00001766
Iteration 182/1000 | Loss: 0.00001766
Iteration 183/1000 | Loss: 0.00001766
Iteration 184/1000 | Loss: 0.00001766
Iteration 185/1000 | Loss: 0.00001766
Iteration 186/1000 | Loss: 0.00001766
Iteration 187/1000 | Loss: 0.00001766
Iteration 188/1000 | Loss: 0.00001766
Iteration 189/1000 | Loss: 0.00001766
Iteration 190/1000 | Loss: 0.00001766
Iteration 191/1000 | Loss: 0.00001766
Iteration 192/1000 | Loss: 0.00001766
Iteration 193/1000 | Loss: 0.00001766
Iteration 194/1000 | Loss: 0.00001766
Iteration 195/1000 | Loss: 0.00001766
Iteration 196/1000 | Loss: 0.00001766
Iteration 197/1000 | Loss: 0.00001766
Iteration 198/1000 | Loss: 0.00001766
Iteration 199/1000 | Loss: 0.00001766
Iteration 200/1000 | Loss: 0.00001766
Iteration 201/1000 | Loss: 0.00001766
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.7663436665316112e-05, 1.7663436665316112e-05, 1.7663436665316112e-05, 1.7663436665316112e-05, 1.7663436665316112e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7663436665316112e-05

Optimization complete. Final v2v error: 3.538116931915283 mm

Highest mean error: 4.099478721618652 mm for frame 121

Lowest mean error: 3.349843978881836 mm for frame 106

Saving results

Total time: 37.152822732925415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01106513
Iteration 2/25 | Loss: 0.00339483
Iteration 3/25 | Loss: 0.00247573
Iteration 4/25 | Loss: 0.00220383
Iteration 5/25 | Loss: 0.00228159
Iteration 6/25 | Loss: 0.00192496
Iteration 7/25 | Loss: 0.00187292
Iteration 8/25 | Loss: 0.00178557
Iteration 9/25 | Loss: 0.00175324
Iteration 10/25 | Loss: 0.00173651
Iteration 11/25 | Loss: 0.00170775
Iteration 12/25 | Loss: 0.00170207
Iteration 13/25 | Loss: 0.00170174
Iteration 14/25 | Loss: 0.00169971
Iteration 15/25 | Loss: 0.00169966
Iteration 16/25 | Loss: 0.00170012
Iteration 17/25 | Loss: 0.00170114
Iteration 18/25 | Loss: 0.00169875
Iteration 19/25 | Loss: 0.00169964
Iteration 20/25 | Loss: 0.00169527
Iteration 21/25 | Loss: 0.00169459
Iteration 22/25 | Loss: 0.00169439
Iteration 23/25 | Loss: 0.00169428
Iteration 24/25 | Loss: 0.00169829
Iteration 25/25 | Loss: 0.00169864

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.48253846
Iteration 2/25 | Loss: 0.00528282
Iteration 3/25 | Loss: 0.00528282
Iteration 4/25 | Loss: 0.00528282
Iteration 5/25 | Loss: 0.00528282
Iteration 6/25 | Loss: 0.00528282
Iteration 7/25 | Loss: 0.00528282
Iteration 8/25 | Loss: 0.00528282
Iteration 9/25 | Loss: 0.00528282
Iteration 10/25 | Loss: 0.00528282
Iteration 11/25 | Loss: 0.00528282
Iteration 12/25 | Loss: 0.00528282
Iteration 13/25 | Loss: 0.00528282
Iteration 14/25 | Loss: 0.00528282
Iteration 15/25 | Loss: 0.00528282
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.005282819736748934, 0.005282819736748934, 0.005282819736748934, 0.005282819736748934, 0.005282819736748934]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005282819736748934

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00528282
Iteration 2/1000 | Loss: 0.00279693
Iteration 3/1000 | Loss: 0.00825880
Iteration 4/1000 | Loss: 0.00978370
Iteration 5/1000 | Loss: 0.00662405
Iteration 6/1000 | Loss: 0.00828558
Iteration 7/1000 | Loss: 0.00063588
Iteration 8/1000 | Loss: 0.00241145
Iteration 9/1000 | Loss: 0.00022566
Iteration 10/1000 | Loss: 0.00108884
Iteration 11/1000 | Loss: 0.00047372
Iteration 12/1000 | Loss: 0.00019141
Iteration 13/1000 | Loss: 0.00019742
Iteration 14/1000 | Loss: 0.00019733
Iteration 15/1000 | Loss: 0.00015220
Iteration 16/1000 | Loss: 0.00016053
Iteration 17/1000 | Loss: 0.00128738
Iteration 18/1000 | Loss: 0.00021432
Iteration 19/1000 | Loss: 0.00018832
Iteration 20/1000 | Loss: 0.00013707
Iteration 21/1000 | Loss: 0.00013919
Iteration 22/1000 | Loss: 0.00008956
Iteration 23/1000 | Loss: 0.00038888
Iteration 24/1000 | Loss: 0.00020477
Iteration 25/1000 | Loss: 0.00024144
Iteration 26/1000 | Loss: 0.00033502
Iteration 27/1000 | Loss: 0.00008929
Iteration 28/1000 | Loss: 0.00074749
Iteration 29/1000 | Loss: 0.00038161
Iteration 30/1000 | Loss: 0.00041098
Iteration 31/1000 | Loss: 0.00052044
Iteration 32/1000 | Loss: 0.00019663
Iteration 33/1000 | Loss: 0.00006693
Iteration 34/1000 | Loss: 0.00014928
Iteration 35/1000 | Loss: 0.00007505
Iteration 36/1000 | Loss: 0.00015914
Iteration 37/1000 | Loss: 0.00017905
Iteration 38/1000 | Loss: 0.00016253
Iteration 39/1000 | Loss: 0.00017276
Iteration 40/1000 | Loss: 0.00010896
Iteration 41/1000 | Loss: 0.00051754
Iteration 42/1000 | Loss: 0.00114518
Iteration 43/1000 | Loss: 0.00057201
Iteration 44/1000 | Loss: 0.00012714
Iteration 45/1000 | Loss: 0.00011100
Iteration 46/1000 | Loss: 0.00016721
Iteration 47/1000 | Loss: 0.00016889
Iteration 48/1000 | Loss: 0.00024074
Iteration 49/1000 | Loss: 0.00015897
Iteration 50/1000 | Loss: 0.00023270
Iteration 51/1000 | Loss: 0.00014613
Iteration 52/1000 | Loss: 0.00006139
Iteration 53/1000 | Loss: 0.00005423
Iteration 54/1000 | Loss: 0.00008348
Iteration 55/1000 | Loss: 0.00036991
Iteration 56/1000 | Loss: 0.00015370
Iteration 57/1000 | Loss: 0.00049023
Iteration 58/1000 | Loss: 0.00042580
Iteration 59/1000 | Loss: 0.00058475
Iteration 60/1000 | Loss: 0.00070191
Iteration 61/1000 | Loss: 0.00005513
Iteration 62/1000 | Loss: 0.00005023
Iteration 63/1000 | Loss: 0.00026235
Iteration 64/1000 | Loss: 0.00027211
Iteration 65/1000 | Loss: 0.00005841
Iteration 66/1000 | Loss: 0.00005591
Iteration 67/1000 | Loss: 0.00005217
Iteration 68/1000 | Loss: 0.00004815
Iteration 69/1000 | Loss: 0.00045249
Iteration 70/1000 | Loss: 0.00011050
Iteration 71/1000 | Loss: 0.00004653
Iteration 72/1000 | Loss: 0.00004555
Iteration 73/1000 | Loss: 0.00004496
Iteration 74/1000 | Loss: 0.00004406
Iteration 75/1000 | Loss: 0.00004335
Iteration 76/1000 | Loss: 0.00011671
Iteration 77/1000 | Loss: 0.00007596
Iteration 78/1000 | Loss: 0.00004289
Iteration 79/1000 | Loss: 0.00037485
Iteration 80/1000 | Loss: 0.00011353
Iteration 81/1000 | Loss: 0.00027165
Iteration 82/1000 | Loss: 0.00016797
Iteration 83/1000 | Loss: 0.00030035
Iteration 84/1000 | Loss: 0.00035566
Iteration 85/1000 | Loss: 0.00066110
Iteration 86/1000 | Loss: 0.00030984
Iteration 87/1000 | Loss: 0.00051051
Iteration 88/1000 | Loss: 0.00005246
Iteration 89/1000 | Loss: 0.00004810
Iteration 90/1000 | Loss: 0.00004476
Iteration 91/1000 | Loss: 0.00004286
Iteration 92/1000 | Loss: 0.00004156
Iteration 93/1000 | Loss: 0.00004054
Iteration 94/1000 | Loss: 0.00003987
Iteration 95/1000 | Loss: 0.00003948
Iteration 96/1000 | Loss: 0.00003942
Iteration 97/1000 | Loss: 0.00003934
Iteration 98/1000 | Loss: 0.00003933
Iteration 99/1000 | Loss: 0.00003924
Iteration 100/1000 | Loss: 0.00003917
Iteration 101/1000 | Loss: 0.00003917
Iteration 102/1000 | Loss: 0.00003917
Iteration 103/1000 | Loss: 0.00003916
Iteration 104/1000 | Loss: 0.00003916
Iteration 105/1000 | Loss: 0.00003915
Iteration 106/1000 | Loss: 0.00003914
Iteration 107/1000 | Loss: 0.00003914
Iteration 108/1000 | Loss: 0.00003914
Iteration 109/1000 | Loss: 0.00003913
Iteration 110/1000 | Loss: 0.00003912
Iteration 111/1000 | Loss: 0.00003912
Iteration 112/1000 | Loss: 0.00003911
Iteration 113/1000 | Loss: 0.00003911
Iteration 114/1000 | Loss: 0.00003910
Iteration 115/1000 | Loss: 0.00003910
Iteration 116/1000 | Loss: 0.00003905
Iteration 117/1000 | Loss: 0.00003904
Iteration 118/1000 | Loss: 0.00003903
Iteration 119/1000 | Loss: 0.00003903
Iteration 120/1000 | Loss: 0.00003902
Iteration 121/1000 | Loss: 0.00003902
Iteration 122/1000 | Loss: 0.00003902
Iteration 123/1000 | Loss: 0.00003902
Iteration 124/1000 | Loss: 0.00003901
Iteration 125/1000 | Loss: 0.00003901
Iteration 126/1000 | Loss: 0.00003901
Iteration 127/1000 | Loss: 0.00003900
Iteration 128/1000 | Loss: 0.00003900
Iteration 129/1000 | Loss: 0.00003900
Iteration 130/1000 | Loss: 0.00003899
Iteration 131/1000 | Loss: 0.00003899
Iteration 132/1000 | Loss: 0.00003899
Iteration 133/1000 | Loss: 0.00003898
Iteration 134/1000 | Loss: 0.00003898
Iteration 135/1000 | Loss: 0.00003895
Iteration 136/1000 | Loss: 0.00003895
Iteration 137/1000 | Loss: 0.00003895
Iteration 138/1000 | Loss: 0.00003893
Iteration 139/1000 | Loss: 0.00003893
Iteration 140/1000 | Loss: 0.00003893
Iteration 141/1000 | Loss: 0.00003893
Iteration 142/1000 | Loss: 0.00003893
Iteration 143/1000 | Loss: 0.00003893
Iteration 144/1000 | Loss: 0.00003893
Iteration 145/1000 | Loss: 0.00003892
Iteration 146/1000 | Loss: 0.00003892
Iteration 147/1000 | Loss: 0.00003892
Iteration 148/1000 | Loss: 0.00003892
Iteration 149/1000 | Loss: 0.00003892
Iteration 150/1000 | Loss: 0.00003892
Iteration 151/1000 | Loss: 0.00003892
Iteration 152/1000 | Loss: 0.00003892
Iteration 153/1000 | Loss: 0.00003892
Iteration 154/1000 | Loss: 0.00003892
Iteration 155/1000 | Loss: 0.00003891
Iteration 156/1000 | Loss: 0.00003891
Iteration 157/1000 | Loss: 0.00003891
Iteration 158/1000 | Loss: 0.00003891
Iteration 159/1000 | Loss: 0.00003891
Iteration 160/1000 | Loss: 0.00003891
Iteration 161/1000 | Loss: 0.00003891
Iteration 162/1000 | Loss: 0.00003891
Iteration 163/1000 | Loss: 0.00003890
Iteration 164/1000 | Loss: 0.00003890
Iteration 165/1000 | Loss: 0.00003890
Iteration 166/1000 | Loss: 0.00003890
Iteration 167/1000 | Loss: 0.00003890
Iteration 168/1000 | Loss: 0.00003890
Iteration 169/1000 | Loss: 0.00003890
Iteration 170/1000 | Loss: 0.00003889
Iteration 171/1000 | Loss: 0.00003891
Iteration 172/1000 | Loss: 0.00003891
Iteration 173/1000 | Loss: 0.00003891
Iteration 174/1000 | Loss: 0.00003891
Iteration 175/1000 | Loss: 0.00003890
Iteration 176/1000 | Loss: 0.00003890
Iteration 177/1000 | Loss: 0.00003890
Iteration 178/1000 | Loss: 0.00003889
Iteration 179/1000 | Loss: 0.00003889
Iteration 180/1000 | Loss: 0.00003889
Iteration 181/1000 | Loss: 0.00003889
Iteration 182/1000 | Loss: 0.00003889
Iteration 183/1000 | Loss: 0.00003887
Iteration 184/1000 | Loss: 0.00003887
Iteration 185/1000 | Loss: 0.00003887
Iteration 186/1000 | Loss: 0.00003887
Iteration 187/1000 | Loss: 0.00003887
Iteration 188/1000 | Loss: 0.00003887
Iteration 189/1000 | Loss: 0.00003887
Iteration 190/1000 | Loss: 0.00003887
Iteration 191/1000 | Loss: 0.00003887
Iteration 192/1000 | Loss: 0.00003887
Iteration 193/1000 | Loss: 0.00003887
Iteration 194/1000 | Loss: 0.00003887
Iteration 195/1000 | Loss: 0.00003887
Iteration 196/1000 | Loss: 0.00003887
Iteration 197/1000 | Loss: 0.00003887
Iteration 198/1000 | Loss: 0.00003887
Iteration 199/1000 | Loss: 0.00003887
Iteration 200/1000 | Loss: 0.00003887
Iteration 201/1000 | Loss: 0.00003887
Iteration 202/1000 | Loss: 0.00003887
Iteration 203/1000 | Loss: 0.00003887
Iteration 204/1000 | Loss: 0.00003887
Iteration 205/1000 | Loss: 0.00003887
Iteration 206/1000 | Loss: 0.00003887
Iteration 207/1000 | Loss: 0.00003887
Iteration 208/1000 | Loss: 0.00003887
Iteration 209/1000 | Loss: 0.00003887
Iteration 210/1000 | Loss: 0.00003887
Iteration 211/1000 | Loss: 0.00003887
Iteration 212/1000 | Loss: 0.00003887
Iteration 213/1000 | Loss: 0.00003887
Iteration 214/1000 | Loss: 0.00003887
Iteration 215/1000 | Loss: 0.00003887
Iteration 216/1000 | Loss: 0.00003887
Iteration 217/1000 | Loss: 0.00003887
Iteration 218/1000 | Loss: 0.00003887
Iteration 219/1000 | Loss: 0.00003887
Iteration 220/1000 | Loss: 0.00003887
Iteration 221/1000 | Loss: 0.00003887
Iteration 222/1000 | Loss: 0.00003887
Iteration 223/1000 | Loss: 0.00003887
Iteration 224/1000 | Loss: 0.00003887
Iteration 225/1000 | Loss: 0.00003887
Iteration 226/1000 | Loss: 0.00003887
Iteration 227/1000 | Loss: 0.00003887
Iteration 228/1000 | Loss: 0.00003887
Iteration 229/1000 | Loss: 0.00003887
Iteration 230/1000 | Loss: 0.00003887
Iteration 231/1000 | Loss: 0.00003887
Iteration 232/1000 | Loss: 0.00003887
Iteration 233/1000 | Loss: 0.00003887
Iteration 234/1000 | Loss: 0.00003887
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 234. Stopping optimization.
Last 5 losses: [3.8868161936989054e-05, 3.8868161936989054e-05, 3.8868161936989054e-05, 3.8868161936989054e-05, 3.8868161936989054e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.8868161936989054e-05

Optimization complete. Final v2v error: 5.229039192199707 mm

Highest mean error: 9.806112289428711 mm for frame 15

Lowest mean error: 4.4629316329956055 mm for frame 140

Saving results

Total time: 210.10196805000305
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01002497
Iteration 2/25 | Loss: 0.00236914
Iteration 3/25 | Loss: 0.00171015
Iteration 4/25 | Loss: 0.00164048
Iteration 5/25 | Loss: 0.00157868
Iteration 6/25 | Loss: 0.00149185
Iteration 7/25 | Loss: 0.00146681
Iteration 8/25 | Loss: 0.00146184
Iteration 9/25 | Loss: 0.00145611
Iteration 10/25 | Loss: 0.00145219
Iteration 11/25 | Loss: 0.00144175
Iteration 12/25 | Loss: 0.00143308
Iteration 13/25 | Loss: 0.00142904
Iteration 14/25 | Loss: 0.00143436
Iteration 15/25 | Loss: 0.00142345
Iteration 16/25 | Loss: 0.00142110
Iteration 17/25 | Loss: 0.00142323
Iteration 18/25 | Loss: 0.00142136
Iteration 19/25 | Loss: 0.00142345
Iteration 20/25 | Loss: 0.00142023
Iteration 21/25 | Loss: 0.00142042
Iteration 22/25 | Loss: 0.00142354
Iteration 23/25 | Loss: 0.00142196
Iteration 24/25 | Loss: 0.00141948
Iteration 25/25 | Loss: 0.00142022

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.86625481
Iteration 2/25 | Loss: 0.00169329
Iteration 3/25 | Loss: 0.00169320
Iteration 4/25 | Loss: 0.00169320
Iteration 5/25 | Loss: 0.00169320
Iteration 6/25 | Loss: 0.00169320
Iteration 7/25 | Loss: 0.00169320
Iteration 8/25 | Loss: 0.00169320
Iteration 9/25 | Loss: 0.00169320
Iteration 10/25 | Loss: 0.00169320
Iteration 11/25 | Loss: 0.00169320
Iteration 12/25 | Loss: 0.00169320
Iteration 13/25 | Loss: 0.00169320
Iteration 14/25 | Loss: 0.00169320
Iteration 15/25 | Loss: 0.00169320
Iteration 16/25 | Loss: 0.00169320
Iteration 17/25 | Loss: 0.00169320
Iteration 18/25 | Loss: 0.00169320
Iteration 19/25 | Loss: 0.00169320
Iteration 20/25 | Loss: 0.00169320
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0016931958962231874, 0.0016931958962231874, 0.0016931958962231874, 0.0016931958962231874, 0.0016931958962231874]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016931958962231874

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169320
Iteration 2/1000 | Loss: 0.00439357
Iteration 3/1000 | Loss: 0.01187011
Iteration 4/1000 | Loss: 0.01226424
Iteration 5/1000 | Loss: 0.00276769
Iteration 6/1000 | Loss: 0.00771749
Iteration 7/1000 | Loss: 0.00578945
Iteration 8/1000 | Loss: 0.00658127
Iteration 9/1000 | Loss: 0.00273868
Iteration 10/1000 | Loss: 0.00818341
Iteration 11/1000 | Loss: 0.00379525
Iteration 12/1000 | Loss: 0.00938100
Iteration 13/1000 | Loss: 0.00785953
Iteration 14/1000 | Loss: 0.00503026
Iteration 15/1000 | Loss: 0.00493662
Iteration 16/1000 | Loss: 0.00540076
Iteration 17/1000 | Loss: 0.00542676
Iteration 18/1000 | Loss: 0.00237690
Iteration 19/1000 | Loss: 0.00330554
Iteration 20/1000 | Loss: 0.00312630
Iteration 21/1000 | Loss: 0.00153938
Iteration 22/1000 | Loss: 0.00805477
Iteration 23/1000 | Loss: 0.00507396
Iteration 24/1000 | Loss: 0.00364774
Iteration 25/1000 | Loss: 0.00049623
Iteration 26/1000 | Loss: 0.00163163
Iteration 27/1000 | Loss: 0.00397105
Iteration 28/1000 | Loss: 0.00163516
Iteration 29/1000 | Loss: 0.00024557
Iteration 30/1000 | Loss: 0.00071901
Iteration 31/1000 | Loss: 0.00116000
Iteration 32/1000 | Loss: 0.00128485
Iteration 33/1000 | Loss: 0.00031072
Iteration 34/1000 | Loss: 0.00244235
Iteration 35/1000 | Loss: 0.00239701
Iteration 36/1000 | Loss: 0.00069425
Iteration 37/1000 | Loss: 0.00209309
Iteration 38/1000 | Loss: 0.00193618
Iteration 39/1000 | Loss: 0.00166624
Iteration 40/1000 | Loss: 0.00139300
Iteration 41/1000 | Loss: 0.00067908
Iteration 42/1000 | Loss: 0.00335069
Iteration 43/1000 | Loss: 0.00047596
Iteration 44/1000 | Loss: 0.00014938
Iteration 45/1000 | Loss: 0.00135288
Iteration 46/1000 | Loss: 0.00204896
Iteration 47/1000 | Loss: 0.00223353
Iteration 48/1000 | Loss: 0.00072775
Iteration 49/1000 | Loss: 0.00095181
Iteration 50/1000 | Loss: 0.00105747
Iteration 51/1000 | Loss: 0.00140023
Iteration 52/1000 | Loss: 0.00188951
Iteration 53/1000 | Loss: 0.00171508
Iteration 54/1000 | Loss: 0.00128256
Iteration 55/1000 | Loss: 0.00155489
Iteration 56/1000 | Loss: 0.00190815
Iteration 57/1000 | Loss: 0.00131160
Iteration 58/1000 | Loss: 0.00196585
Iteration 59/1000 | Loss: 0.00081809
Iteration 60/1000 | Loss: 0.00290243
Iteration 61/1000 | Loss: 0.00202764
Iteration 62/1000 | Loss: 0.00233032
Iteration 63/1000 | Loss: 0.00237589
Iteration 64/1000 | Loss: 0.00259135
Iteration 65/1000 | Loss: 0.00076429
Iteration 66/1000 | Loss: 0.00112653
Iteration 67/1000 | Loss: 0.00347910
Iteration 68/1000 | Loss: 0.00224790
Iteration 69/1000 | Loss: 0.00367164
Iteration 70/1000 | Loss: 0.00238422
Iteration 71/1000 | Loss: 0.00178024
Iteration 72/1000 | Loss: 0.00305269
Iteration 73/1000 | Loss: 0.00301377
Iteration 74/1000 | Loss: 0.00118379
Iteration 75/1000 | Loss: 0.00191066
Iteration 76/1000 | Loss: 0.00182525
Iteration 77/1000 | Loss: 0.00151828
Iteration 78/1000 | Loss: 0.00208676
Iteration 79/1000 | Loss: 0.00407903
Iteration 80/1000 | Loss: 0.00152689
Iteration 81/1000 | Loss: 0.00175875
Iteration 82/1000 | Loss: 0.00123757
Iteration 83/1000 | Loss: 0.00086675
Iteration 84/1000 | Loss: 0.00085609
Iteration 85/1000 | Loss: 0.00040924
Iteration 86/1000 | Loss: 0.00041032
Iteration 87/1000 | Loss: 0.00015285
Iteration 88/1000 | Loss: 0.00032365
Iteration 89/1000 | Loss: 0.00041569
Iteration 90/1000 | Loss: 0.00068802
Iteration 91/1000 | Loss: 0.00035637
Iteration 92/1000 | Loss: 0.00022181
Iteration 93/1000 | Loss: 0.00027298
Iteration 94/1000 | Loss: 0.00038678
Iteration 95/1000 | Loss: 0.00042738
Iteration 96/1000 | Loss: 0.00040231
Iteration 97/1000 | Loss: 0.00025550
Iteration 98/1000 | Loss: 0.00029896
Iteration 99/1000 | Loss: 0.00034629
Iteration 100/1000 | Loss: 0.00040246
Iteration 101/1000 | Loss: 0.00009139
Iteration 102/1000 | Loss: 0.00006633
Iteration 103/1000 | Loss: 0.00009452
Iteration 104/1000 | Loss: 0.00007976
Iteration 105/1000 | Loss: 0.00005579
Iteration 106/1000 | Loss: 0.00027409
Iteration 107/1000 | Loss: 0.00016262
Iteration 108/1000 | Loss: 0.00010190
Iteration 109/1000 | Loss: 0.00037608
Iteration 110/1000 | Loss: 0.00036220
Iteration 111/1000 | Loss: 0.00042359
Iteration 112/1000 | Loss: 0.00066827
Iteration 113/1000 | Loss: 0.00044610
Iteration 114/1000 | Loss: 0.00051570
Iteration 115/1000 | Loss: 0.00036514
Iteration 116/1000 | Loss: 0.00055592
Iteration 117/1000 | Loss: 0.00007916
Iteration 118/1000 | Loss: 0.00005327
Iteration 119/1000 | Loss: 0.00004709
Iteration 120/1000 | Loss: 0.00004406
Iteration 121/1000 | Loss: 0.00004283
Iteration 122/1000 | Loss: 0.00059834
Iteration 123/1000 | Loss: 0.00005219
Iteration 124/1000 | Loss: 0.00004493
Iteration 125/1000 | Loss: 0.00004279
Iteration 126/1000 | Loss: 0.00004193
Iteration 127/1000 | Loss: 0.00004158
Iteration 128/1000 | Loss: 0.00004126
Iteration 129/1000 | Loss: 0.00039597
Iteration 130/1000 | Loss: 0.00061626
Iteration 131/1000 | Loss: 0.00006825
Iteration 132/1000 | Loss: 0.00005277
Iteration 133/1000 | Loss: 0.00045478
Iteration 134/1000 | Loss: 0.00004467
Iteration 135/1000 | Loss: 0.00044793
Iteration 136/1000 | Loss: 0.00020924
Iteration 137/1000 | Loss: 0.00004927
Iteration 138/1000 | Loss: 0.00052921
Iteration 139/1000 | Loss: 0.00021150
Iteration 140/1000 | Loss: 0.00047566
Iteration 141/1000 | Loss: 0.00020604
Iteration 142/1000 | Loss: 0.00048644
Iteration 143/1000 | Loss: 0.00017681
Iteration 144/1000 | Loss: 0.00043726
Iteration 145/1000 | Loss: 0.00016127
Iteration 146/1000 | Loss: 0.00033387
Iteration 147/1000 | Loss: 0.00022936
Iteration 148/1000 | Loss: 0.00046843
Iteration 149/1000 | Loss: 0.00030977
Iteration 150/1000 | Loss: 0.00005089
Iteration 151/1000 | Loss: 0.00043002
Iteration 152/1000 | Loss: 0.00055709
Iteration 153/1000 | Loss: 0.00045333
Iteration 154/1000 | Loss: 0.00044782
Iteration 155/1000 | Loss: 0.00039260
Iteration 156/1000 | Loss: 0.00036815
Iteration 157/1000 | Loss: 0.00033212
Iteration 158/1000 | Loss: 0.00050224
Iteration 159/1000 | Loss: 0.00037879
Iteration 160/1000 | Loss: 0.00005512
Iteration 161/1000 | Loss: 0.00004693
Iteration 162/1000 | Loss: 0.00004026
Iteration 163/1000 | Loss: 0.00003811
Iteration 164/1000 | Loss: 0.00003694
Iteration 165/1000 | Loss: 0.00003640
Iteration 166/1000 | Loss: 0.00003603
Iteration 167/1000 | Loss: 0.00003583
Iteration 168/1000 | Loss: 0.00003555
Iteration 169/1000 | Loss: 0.00003531
Iteration 170/1000 | Loss: 0.00003509
Iteration 171/1000 | Loss: 0.00003488
Iteration 172/1000 | Loss: 0.00003473
Iteration 173/1000 | Loss: 0.00003464
Iteration 174/1000 | Loss: 0.00003457
Iteration 175/1000 | Loss: 0.00003453
Iteration 176/1000 | Loss: 0.00003452
Iteration 177/1000 | Loss: 0.00003450
Iteration 178/1000 | Loss: 0.00003450
Iteration 179/1000 | Loss: 0.00004148
Iteration 180/1000 | Loss: 0.00003782
Iteration 181/1000 | Loss: 0.00003733
Iteration 182/1000 | Loss: 0.00003716
Iteration 183/1000 | Loss: 0.00003687
Iteration 184/1000 | Loss: 0.00003652
Iteration 185/1000 | Loss: 0.00003601
Iteration 186/1000 | Loss: 0.00003576
Iteration 187/1000 | Loss: 0.00004276
Iteration 188/1000 | Loss: 0.00003910
Iteration 189/1000 | Loss: 0.00003846
Iteration 190/1000 | Loss: 0.00003809
Iteration 191/1000 | Loss: 0.00004237
Iteration 192/1000 | Loss: 0.00003973
Iteration 193/1000 | Loss: 0.00003905
Iteration 194/1000 | Loss: 0.00004779
Iteration 195/1000 | Loss: 0.00004357
Iteration 196/1000 | Loss: 0.00004052
Iteration 197/1000 | Loss: 0.00003696
Iteration 198/1000 | Loss: 0.00003566
Iteration 199/1000 | Loss: 0.00003524
Iteration 200/1000 | Loss: 0.00004019
Iteration 201/1000 | Loss: 0.00003768
Iteration 202/1000 | Loss: 0.00003705
Iteration 203/1000 | Loss: 0.00003637
Iteration 204/1000 | Loss: 0.00003571
Iteration 205/1000 | Loss: 0.00004604
Iteration 206/1000 | Loss: 0.00004061
Iteration 207/1000 | Loss: 0.00003811
Iteration 208/1000 | Loss: 0.00003541
Iteration 209/1000 | Loss: 0.00003407
Iteration 210/1000 | Loss: 0.00003337
Iteration 211/1000 | Loss: 0.00003317
Iteration 212/1000 | Loss: 0.00003311
Iteration 213/1000 | Loss: 0.00003310
Iteration 214/1000 | Loss: 0.00003310
Iteration 215/1000 | Loss: 0.00003310
Iteration 216/1000 | Loss: 0.00003309
Iteration 217/1000 | Loss: 0.00003309
Iteration 218/1000 | Loss: 0.00003308
Iteration 219/1000 | Loss: 0.00003308
Iteration 220/1000 | Loss: 0.00003306
Iteration 221/1000 | Loss: 0.00003305
Iteration 222/1000 | Loss: 0.00003302
Iteration 223/1000 | Loss: 0.00003301
Iteration 224/1000 | Loss: 0.00003301
Iteration 225/1000 | Loss: 0.00003300
Iteration 226/1000 | Loss: 0.00003300
Iteration 227/1000 | Loss: 0.00003300
Iteration 228/1000 | Loss: 0.00003300
Iteration 229/1000 | Loss: 0.00003300
Iteration 230/1000 | Loss: 0.00003300
Iteration 231/1000 | Loss: 0.00003299
Iteration 232/1000 | Loss: 0.00003299
Iteration 233/1000 | Loss: 0.00003299
Iteration 234/1000 | Loss: 0.00003299
Iteration 235/1000 | Loss: 0.00003299
Iteration 236/1000 | Loss: 0.00003299
Iteration 237/1000 | Loss: 0.00003299
Iteration 238/1000 | Loss: 0.00003299
Iteration 239/1000 | Loss: 0.00003299
Iteration 240/1000 | Loss: 0.00003299
Iteration 241/1000 | Loss: 0.00003298
Iteration 242/1000 | Loss: 0.00003298
Iteration 243/1000 | Loss: 0.00003298
Iteration 244/1000 | Loss: 0.00003298
Iteration 245/1000 | Loss: 0.00003298
Iteration 246/1000 | Loss: 0.00003298
Iteration 247/1000 | Loss: 0.00003298
Iteration 248/1000 | Loss: 0.00003298
Iteration 249/1000 | Loss: 0.00003298
Iteration 250/1000 | Loss: 0.00003298
Iteration 251/1000 | Loss: 0.00003298
Iteration 252/1000 | Loss: 0.00003298
Iteration 253/1000 | Loss: 0.00003298
Iteration 254/1000 | Loss: 0.00003298
Iteration 255/1000 | Loss: 0.00003298
Iteration 256/1000 | Loss: 0.00003298
Iteration 257/1000 | Loss: 0.00003297
Iteration 258/1000 | Loss: 0.00003297
Iteration 259/1000 | Loss: 0.00003297
Iteration 260/1000 | Loss: 0.00003297
Iteration 261/1000 | Loss: 0.00003297
Iteration 262/1000 | Loss: 0.00003297
Iteration 263/1000 | Loss: 0.00003297
Iteration 264/1000 | Loss: 0.00003297
Iteration 265/1000 | Loss: 0.00003297
Iteration 266/1000 | Loss: 0.00003297
Iteration 267/1000 | Loss: 0.00003296
Iteration 268/1000 | Loss: 0.00003296
Iteration 269/1000 | Loss: 0.00003296
Iteration 270/1000 | Loss: 0.00003296
Iteration 271/1000 | Loss: 0.00003296
Iteration 272/1000 | Loss: 0.00003296
Iteration 273/1000 | Loss: 0.00003296
Iteration 274/1000 | Loss: 0.00003296
Iteration 275/1000 | Loss: 0.00003296
Iteration 276/1000 | Loss: 0.00003296
Iteration 277/1000 | Loss: 0.00003296
Iteration 278/1000 | Loss: 0.00003296
Iteration 279/1000 | Loss: 0.00003296
Iteration 280/1000 | Loss: 0.00003295
Iteration 281/1000 | Loss: 0.00003295
Iteration 282/1000 | Loss: 0.00003295
Iteration 283/1000 | Loss: 0.00003295
Iteration 284/1000 | Loss: 0.00003295
Iteration 285/1000 | Loss: 0.00003295
Iteration 286/1000 | Loss: 0.00003295
Iteration 287/1000 | Loss: 0.00003295
Iteration 288/1000 | Loss: 0.00003295
Iteration 289/1000 | Loss: 0.00003295
Iteration 290/1000 | Loss: 0.00003295
Iteration 291/1000 | Loss: 0.00003295
Iteration 292/1000 | Loss: 0.00003295
Iteration 293/1000 | Loss: 0.00003295
Iteration 294/1000 | Loss: 0.00003295
Iteration 295/1000 | Loss: 0.00003295
Iteration 296/1000 | Loss: 0.00003295
Iteration 297/1000 | Loss: 0.00003295
Iteration 298/1000 | Loss: 0.00003295
Iteration 299/1000 | Loss: 0.00003295
Iteration 300/1000 | Loss: 0.00003295
Iteration 301/1000 | Loss: 0.00003295
Iteration 302/1000 | Loss: 0.00003295
Iteration 303/1000 | Loss: 0.00003295
Iteration 304/1000 | Loss: 0.00003295
Iteration 305/1000 | Loss: 0.00003295
Iteration 306/1000 | Loss: 0.00003295
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 306. Stopping optimization.
Last 5 losses: [3.294696944067255e-05, 3.294696944067255e-05, 3.294696944067255e-05, 3.294696944067255e-05, 3.294696944067255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.294696944067255e-05

Optimization complete. Final v2v error: 4.802570819854736 mm

Highest mean error: 7.345635890960693 mm for frame 139

Lowest mean error: 3.698878288269043 mm for frame 191

Saving results

Total time: 391.3632245063782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01109917
Iteration 2/25 | Loss: 0.00349424
Iteration 3/25 | Loss: 0.00240946
Iteration 4/25 | Loss: 0.00220545
Iteration 5/25 | Loss: 0.00202521
Iteration 6/25 | Loss: 0.00188117
Iteration 7/25 | Loss: 0.00184649
Iteration 8/25 | Loss: 0.00179995
Iteration 9/25 | Loss: 0.00176805
Iteration 10/25 | Loss: 0.00175636
Iteration 11/25 | Loss: 0.00175289
Iteration 12/25 | Loss: 0.00173531
Iteration 13/25 | Loss: 0.00172972
Iteration 14/25 | Loss: 0.00172623
Iteration 15/25 | Loss: 0.00172165
Iteration 16/25 | Loss: 0.00170952
Iteration 17/25 | Loss: 0.00169542
Iteration 18/25 | Loss: 0.00171053
Iteration 19/25 | Loss: 0.00168923
Iteration 20/25 | Loss: 0.00169493
Iteration 21/25 | Loss: 0.00168029
Iteration 22/25 | Loss: 0.00167904
Iteration 23/25 | Loss: 0.00168622
Iteration 24/25 | Loss: 0.00166404
Iteration 25/25 | Loss: 0.00166778

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59351623
Iteration 2/25 | Loss: 0.01993329
Iteration 3/25 | Loss: 0.02339528
Iteration 4/25 | Loss: 0.01281430
Iteration 5/25 | Loss: 0.01555791
Iteration 6/25 | Loss: 0.00580471
Iteration 7/25 | Loss: 0.00563489
Iteration 8/25 | Loss: 0.00529758
Iteration 9/25 | Loss: 0.00529758
Iteration 10/25 | Loss: 0.00529758
Iteration 11/25 | Loss: 0.00529758
Iteration 12/25 | Loss: 0.00529758
Iteration 13/25 | Loss: 0.00529758
Iteration 14/25 | Loss: 0.00529758
Iteration 15/25 | Loss: 0.00529758
Iteration 16/25 | Loss: 0.00529758
Iteration 17/25 | Loss: 0.00529758
Iteration 18/25 | Loss: 0.00529758
Iteration 19/25 | Loss: 0.00529758
Iteration 20/25 | Loss: 0.00529758
Iteration 21/25 | Loss: 0.00529758
Iteration 22/25 | Loss: 0.00529758
Iteration 23/25 | Loss: 0.00529758
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.005297577939927578, 0.005297577939927578, 0.005297577939927578, 0.005297577939927578, 0.005297577939927578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005297577939927578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00529758
Iteration 2/1000 | Loss: 0.01069449
Iteration 3/1000 | Loss: 0.00468496
Iteration 4/1000 | Loss: 0.00143214
Iteration 5/1000 | Loss: 0.00259811
Iteration 6/1000 | Loss: 0.00147162
Iteration 7/1000 | Loss: 0.00201150
Iteration 8/1000 | Loss: 0.00056732
Iteration 9/1000 | Loss: 0.00108892
Iteration 10/1000 | Loss: 0.00083904
Iteration 11/1000 | Loss: 0.00156625
Iteration 12/1000 | Loss: 0.00095168
Iteration 13/1000 | Loss: 0.00135274
Iteration 14/1000 | Loss: 0.00238953
Iteration 15/1000 | Loss: 0.00557964
Iteration 16/1000 | Loss: 0.00173255
Iteration 17/1000 | Loss: 0.00222324
Iteration 18/1000 | Loss: 0.00133956
Iteration 19/1000 | Loss: 0.00143154
Iteration 20/1000 | Loss: 0.00314763
Iteration 21/1000 | Loss: 0.00854245
Iteration 22/1000 | Loss: 0.00286876
Iteration 23/1000 | Loss: 0.00203052
Iteration 24/1000 | Loss: 0.00310235
Iteration 25/1000 | Loss: 0.00241494
Iteration 26/1000 | Loss: 0.00267494
Iteration 27/1000 | Loss: 0.00436463
Iteration 28/1000 | Loss: 0.00088868
Iteration 29/1000 | Loss: 0.00173116
Iteration 30/1000 | Loss: 0.00580087
Iteration 31/1000 | Loss: 0.00557589
Iteration 32/1000 | Loss: 0.00346706
Iteration 33/1000 | Loss: 0.00202846
Iteration 34/1000 | Loss: 0.00535237
Iteration 35/1000 | Loss: 0.00184769
Iteration 36/1000 | Loss: 0.00083837
Iteration 37/1000 | Loss: 0.00065338
Iteration 38/1000 | Loss: 0.00128713
Iteration 39/1000 | Loss: 0.00062132
Iteration 40/1000 | Loss: 0.00063232
Iteration 41/1000 | Loss: 0.00058409
Iteration 42/1000 | Loss: 0.00073844
Iteration 43/1000 | Loss: 0.00126665
Iteration 44/1000 | Loss: 0.00028632
Iteration 45/1000 | Loss: 0.00303776
Iteration 46/1000 | Loss: 0.00559082
Iteration 47/1000 | Loss: 0.00212652
Iteration 48/1000 | Loss: 0.00307944
Iteration 49/1000 | Loss: 0.00599426
Iteration 50/1000 | Loss: 0.00444856
Iteration 51/1000 | Loss: 0.00287766
Iteration 52/1000 | Loss: 0.00256103
Iteration 53/1000 | Loss: 0.00309353
Iteration 54/1000 | Loss: 0.00344609
Iteration 55/1000 | Loss: 0.00023894
Iteration 56/1000 | Loss: 0.00104258
Iteration 57/1000 | Loss: 0.00084096
Iteration 58/1000 | Loss: 0.00039212
Iteration 59/1000 | Loss: 0.00021333
Iteration 60/1000 | Loss: 0.00178347
Iteration 61/1000 | Loss: 0.00249194
Iteration 62/1000 | Loss: 0.00105617
Iteration 63/1000 | Loss: 0.00322263
Iteration 64/1000 | Loss: 0.00239903
Iteration 65/1000 | Loss: 0.00093755
Iteration 66/1000 | Loss: 0.00074414
Iteration 67/1000 | Loss: 0.00177597
Iteration 68/1000 | Loss: 0.00136485
Iteration 69/1000 | Loss: 0.00026880
Iteration 70/1000 | Loss: 0.00207139
Iteration 71/1000 | Loss: 0.00043593
Iteration 72/1000 | Loss: 0.00018140
Iteration 73/1000 | Loss: 0.00157025
Iteration 74/1000 | Loss: 0.00185368
Iteration 75/1000 | Loss: 0.00174080
Iteration 76/1000 | Loss: 0.00070120
Iteration 77/1000 | Loss: 0.00151211
Iteration 78/1000 | Loss: 0.00037211
Iteration 79/1000 | Loss: 0.00024152
Iteration 80/1000 | Loss: 0.00202383
Iteration 81/1000 | Loss: 0.00087908
Iteration 82/1000 | Loss: 0.00037399
Iteration 83/1000 | Loss: 0.00033133
Iteration 84/1000 | Loss: 0.00047979
Iteration 85/1000 | Loss: 0.00030530
Iteration 86/1000 | Loss: 0.00013957
Iteration 87/1000 | Loss: 0.00247185
Iteration 88/1000 | Loss: 0.00071961
Iteration 89/1000 | Loss: 0.00056829
Iteration 90/1000 | Loss: 0.00043312
Iteration 91/1000 | Loss: 0.00161421
Iteration 92/1000 | Loss: 0.00165838
Iteration 93/1000 | Loss: 0.00159001
Iteration 94/1000 | Loss: 0.00170001
Iteration 95/1000 | Loss: 0.00152170
Iteration 96/1000 | Loss: 0.00047037
Iteration 97/1000 | Loss: 0.00129640
Iteration 98/1000 | Loss: 0.00010864
Iteration 99/1000 | Loss: 0.00123062
Iteration 100/1000 | Loss: 0.00037467
Iteration 101/1000 | Loss: 0.00292018
Iteration 102/1000 | Loss: 0.00477820
Iteration 103/1000 | Loss: 0.00061146
Iteration 104/1000 | Loss: 0.00036510
Iteration 105/1000 | Loss: 0.00062450
Iteration 106/1000 | Loss: 0.00108891
Iteration 107/1000 | Loss: 0.00043621
Iteration 108/1000 | Loss: 0.00197179
Iteration 109/1000 | Loss: 0.00134287
Iteration 110/1000 | Loss: 0.00277434
Iteration 111/1000 | Loss: 0.00043189
Iteration 112/1000 | Loss: 0.00013264
Iteration 113/1000 | Loss: 0.00016265
Iteration 114/1000 | Loss: 0.00116102
Iteration 115/1000 | Loss: 0.00056265
Iteration 116/1000 | Loss: 0.00045424
Iteration 117/1000 | Loss: 0.00026283
Iteration 118/1000 | Loss: 0.00008763
Iteration 119/1000 | Loss: 0.00033357
Iteration 120/1000 | Loss: 0.00033209
Iteration 121/1000 | Loss: 0.00012216
Iteration 122/1000 | Loss: 0.00009796
Iteration 123/1000 | Loss: 0.00041156
Iteration 124/1000 | Loss: 0.00008882
Iteration 125/1000 | Loss: 0.00027828
Iteration 126/1000 | Loss: 0.00047616
Iteration 127/1000 | Loss: 0.00009838
Iteration 128/1000 | Loss: 0.00011930
Iteration 129/1000 | Loss: 0.00059911
Iteration 130/1000 | Loss: 0.00008208
Iteration 131/1000 | Loss: 0.00012616
Iteration 132/1000 | Loss: 0.00033768
Iteration 133/1000 | Loss: 0.00029693
Iteration 134/1000 | Loss: 0.00017984
Iteration 135/1000 | Loss: 0.00030086
Iteration 136/1000 | Loss: 0.00045658
Iteration 137/1000 | Loss: 0.00011536
Iteration 138/1000 | Loss: 0.00007270
Iteration 139/1000 | Loss: 0.00112399
Iteration 140/1000 | Loss: 0.00111365
Iteration 141/1000 | Loss: 0.00028751
Iteration 142/1000 | Loss: 0.00008494
Iteration 143/1000 | Loss: 0.00006909
Iteration 144/1000 | Loss: 0.00006627
Iteration 145/1000 | Loss: 0.00006252
Iteration 146/1000 | Loss: 0.00024350
Iteration 147/1000 | Loss: 0.00005996
Iteration 148/1000 | Loss: 0.00036984
Iteration 149/1000 | Loss: 0.00006496
Iteration 150/1000 | Loss: 0.00005948
Iteration 151/1000 | Loss: 0.00006852
Iteration 152/1000 | Loss: 0.00005646
Iteration 153/1000 | Loss: 0.00005530
Iteration 154/1000 | Loss: 0.00008258
Iteration 155/1000 | Loss: 0.00005431
Iteration 156/1000 | Loss: 0.00010956
Iteration 157/1000 | Loss: 0.00006592
Iteration 158/1000 | Loss: 0.00011157
Iteration 159/1000 | Loss: 0.00005490
Iteration 160/1000 | Loss: 0.00005338
Iteration 161/1000 | Loss: 0.00007562
Iteration 162/1000 | Loss: 0.00012020
Iteration 163/1000 | Loss: 0.00005165
Iteration 164/1000 | Loss: 0.00005098
Iteration 165/1000 | Loss: 0.00005823
Iteration 166/1000 | Loss: 0.00005024
Iteration 167/1000 | Loss: 0.00005008
Iteration 168/1000 | Loss: 0.00004995
Iteration 169/1000 | Loss: 0.00004991
Iteration 170/1000 | Loss: 0.00004990
Iteration 171/1000 | Loss: 0.00004990
Iteration 172/1000 | Loss: 0.00004984
Iteration 173/1000 | Loss: 0.00004983
Iteration 174/1000 | Loss: 0.00004982
Iteration 175/1000 | Loss: 0.00004982
Iteration 176/1000 | Loss: 0.00004979
Iteration 177/1000 | Loss: 0.00004979
Iteration 178/1000 | Loss: 0.00004966
Iteration 179/1000 | Loss: 0.00004966
Iteration 180/1000 | Loss: 0.00004965
Iteration 181/1000 | Loss: 0.00004965
Iteration 182/1000 | Loss: 0.00004964
Iteration 183/1000 | Loss: 0.00004964
Iteration 184/1000 | Loss: 0.00004963
Iteration 185/1000 | Loss: 0.00004959
Iteration 186/1000 | Loss: 0.00004957
Iteration 187/1000 | Loss: 0.00004957
Iteration 188/1000 | Loss: 0.00004957
Iteration 189/1000 | Loss: 0.00004956
Iteration 190/1000 | Loss: 0.00004955
Iteration 191/1000 | Loss: 0.00004955
Iteration 192/1000 | Loss: 0.00004954
Iteration 193/1000 | Loss: 0.00004954
Iteration 194/1000 | Loss: 0.00004954
Iteration 195/1000 | Loss: 0.00004953
Iteration 196/1000 | Loss: 0.00004953
Iteration 197/1000 | Loss: 0.00004953
Iteration 198/1000 | Loss: 0.00004952
Iteration 199/1000 | Loss: 0.00004952
Iteration 200/1000 | Loss: 0.00004952
Iteration 201/1000 | Loss: 0.00004952
Iteration 202/1000 | Loss: 0.00004951
Iteration 203/1000 | Loss: 0.00004951
Iteration 204/1000 | Loss: 0.00004951
Iteration 205/1000 | Loss: 0.00004950
Iteration 206/1000 | Loss: 0.00004950
Iteration 207/1000 | Loss: 0.00004950
Iteration 208/1000 | Loss: 0.00004949
Iteration 209/1000 | Loss: 0.00004949
Iteration 210/1000 | Loss: 0.00004949
Iteration 211/1000 | Loss: 0.00004949
Iteration 212/1000 | Loss: 0.00004949
Iteration 213/1000 | Loss: 0.00004949
Iteration 214/1000 | Loss: 0.00004949
Iteration 215/1000 | Loss: 0.00004949
Iteration 216/1000 | Loss: 0.00004949
Iteration 217/1000 | Loss: 0.00004948
Iteration 218/1000 | Loss: 0.00004948
Iteration 219/1000 | Loss: 0.00004948
Iteration 220/1000 | Loss: 0.00004948
Iteration 221/1000 | Loss: 0.00004948
Iteration 222/1000 | Loss: 0.00004948
Iteration 223/1000 | Loss: 0.00004948
Iteration 224/1000 | Loss: 0.00004947
Iteration 225/1000 | Loss: 0.00004947
Iteration 226/1000 | Loss: 0.00004947
Iteration 227/1000 | Loss: 0.00004947
Iteration 228/1000 | Loss: 0.00004947
Iteration 229/1000 | Loss: 0.00004947
Iteration 230/1000 | Loss: 0.00004947
Iteration 231/1000 | Loss: 0.00004947
Iteration 232/1000 | Loss: 0.00004947
Iteration 233/1000 | Loss: 0.00004947
Iteration 234/1000 | Loss: 0.00004947
Iteration 235/1000 | Loss: 0.00004947
Iteration 236/1000 | Loss: 0.00004947
Iteration 237/1000 | Loss: 0.00004947
Iteration 238/1000 | Loss: 0.00004947
Iteration 239/1000 | Loss: 0.00004947
Iteration 240/1000 | Loss: 0.00004947
Iteration 241/1000 | Loss: 0.00004946
Iteration 242/1000 | Loss: 0.00004946
Iteration 243/1000 | Loss: 0.00004946
Iteration 244/1000 | Loss: 0.00004946
Iteration 245/1000 | Loss: 0.00004946
Iteration 246/1000 | Loss: 0.00004946
Iteration 247/1000 | Loss: 0.00004946
Iteration 248/1000 | Loss: 0.00004946
Iteration 249/1000 | Loss: 0.00004946
Iteration 250/1000 | Loss: 0.00004946
Iteration 251/1000 | Loss: 0.00004946
Iteration 252/1000 | Loss: 0.00004946
Iteration 253/1000 | Loss: 0.00004946
Iteration 254/1000 | Loss: 0.00004946
Iteration 255/1000 | Loss: 0.00004946
Iteration 256/1000 | Loss: 0.00004946
Iteration 257/1000 | Loss: 0.00004946
Iteration 258/1000 | Loss: 0.00004946
Iteration 259/1000 | Loss: 0.00004946
Iteration 260/1000 | Loss: 0.00004946
Iteration 261/1000 | Loss: 0.00004946
Iteration 262/1000 | Loss: 0.00004946
Iteration 263/1000 | Loss: 0.00004946
Iteration 264/1000 | Loss: 0.00004946
Iteration 265/1000 | Loss: 0.00004946
Iteration 266/1000 | Loss: 0.00004946
Iteration 267/1000 | Loss: 0.00004946
Iteration 268/1000 | Loss: 0.00004946
Iteration 269/1000 | Loss: 0.00004946
Iteration 270/1000 | Loss: 0.00004946
Iteration 271/1000 | Loss: 0.00004946
Iteration 272/1000 | Loss: 0.00004946
Iteration 273/1000 | Loss: 0.00004946
Iteration 274/1000 | Loss: 0.00004946
Iteration 275/1000 | Loss: 0.00004946
Iteration 276/1000 | Loss: 0.00004946
Iteration 277/1000 | Loss: 0.00004946
Iteration 278/1000 | Loss: 0.00004946
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 278. Stopping optimization.
Last 5 losses: [4.9455960834166035e-05, 4.9455960834166035e-05, 4.9455960834166035e-05, 4.9455960834166035e-05, 4.9455960834166035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.9455960834166035e-05

Optimization complete. Final v2v error: 5.660974502563477 mm

Highest mean error: 15.189026832580566 mm for frame 20

Lowest mean error: 4.227097034454346 mm for frame 5

Saving results

Total time: 343.8948724269867
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00399756
Iteration 2/25 | Loss: 0.00138980
Iteration 3/25 | Loss: 0.00128864
Iteration 4/25 | Loss: 0.00126878
Iteration 5/25 | Loss: 0.00126484
Iteration 6/25 | Loss: 0.00126324
Iteration 7/25 | Loss: 0.00126304
Iteration 8/25 | Loss: 0.00126304
Iteration 9/25 | Loss: 0.00126304
Iteration 10/25 | Loss: 0.00126304
Iteration 11/25 | Loss: 0.00126304
Iteration 12/25 | Loss: 0.00126304
Iteration 13/25 | Loss: 0.00126304
Iteration 14/25 | Loss: 0.00126304
Iteration 15/25 | Loss: 0.00126304
Iteration 16/25 | Loss: 0.00126304
Iteration 17/25 | Loss: 0.00126304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012630437267944217, 0.0012630437267944217, 0.0012630437267944217, 0.0012630437267944217, 0.0012630437267944217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012630437267944217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47018206
Iteration 2/25 | Loss: 0.00107838
Iteration 3/25 | Loss: 0.00107838
Iteration 4/25 | Loss: 0.00107838
Iteration 5/25 | Loss: 0.00107838
Iteration 6/25 | Loss: 0.00107838
Iteration 7/25 | Loss: 0.00107838
Iteration 8/25 | Loss: 0.00107838
Iteration 9/25 | Loss: 0.00107838
Iteration 10/25 | Loss: 0.00107838
Iteration 11/25 | Loss: 0.00107838
Iteration 12/25 | Loss: 0.00107838
Iteration 13/25 | Loss: 0.00107838
Iteration 14/25 | Loss: 0.00107838
Iteration 15/25 | Loss: 0.00107838
Iteration 16/25 | Loss: 0.00107838
Iteration 17/25 | Loss: 0.00107838
Iteration 18/25 | Loss: 0.00107838
Iteration 19/25 | Loss: 0.00107838
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010783778270706534, 0.0010783778270706534, 0.0010783778270706534, 0.0010783778270706534, 0.0010783778270706534]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010783778270706534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107838
Iteration 2/1000 | Loss: 0.00003702
Iteration 3/1000 | Loss: 0.00002803
Iteration 4/1000 | Loss: 0.00002448
Iteration 5/1000 | Loss: 0.00002337
Iteration 6/1000 | Loss: 0.00002284
Iteration 7/1000 | Loss: 0.00002237
Iteration 8/1000 | Loss: 0.00002207
Iteration 9/1000 | Loss: 0.00002204
Iteration 10/1000 | Loss: 0.00002202
Iteration 11/1000 | Loss: 0.00002189
Iteration 12/1000 | Loss: 0.00002188
Iteration 13/1000 | Loss: 0.00002182
Iteration 14/1000 | Loss: 0.00002180
Iteration 15/1000 | Loss: 0.00002179
Iteration 16/1000 | Loss: 0.00002174
Iteration 17/1000 | Loss: 0.00002173
Iteration 18/1000 | Loss: 0.00002173
Iteration 19/1000 | Loss: 0.00002172
Iteration 20/1000 | Loss: 0.00002169
Iteration 21/1000 | Loss: 0.00002169
Iteration 22/1000 | Loss: 0.00002168
Iteration 23/1000 | Loss: 0.00002168
Iteration 24/1000 | Loss: 0.00002168
Iteration 25/1000 | Loss: 0.00002166
Iteration 26/1000 | Loss: 0.00002164
Iteration 27/1000 | Loss: 0.00002163
Iteration 28/1000 | Loss: 0.00002163
Iteration 29/1000 | Loss: 0.00002162
Iteration 30/1000 | Loss: 0.00002161
Iteration 31/1000 | Loss: 0.00002161
Iteration 32/1000 | Loss: 0.00002159
Iteration 33/1000 | Loss: 0.00002159
Iteration 34/1000 | Loss: 0.00002159
Iteration 35/1000 | Loss: 0.00002159
Iteration 36/1000 | Loss: 0.00002159
Iteration 37/1000 | Loss: 0.00002159
Iteration 38/1000 | Loss: 0.00002159
Iteration 39/1000 | Loss: 0.00002159
Iteration 40/1000 | Loss: 0.00002159
Iteration 41/1000 | Loss: 0.00002159
Iteration 42/1000 | Loss: 0.00002158
Iteration 43/1000 | Loss: 0.00002158
Iteration 44/1000 | Loss: 0.00002158
Iteration 45/1000 | Loss: 0.00002157
Iteration 46/1000 | Loss: 0.00002157
Iteration 47/1000 | Loss: 0.00002157
Iteration 48/1000 | Loss: 0.00002157
Iteration 49/1000 | Loss: 0.00002157
Iteration 50/1000 | Loss: 0.00002156
Iteration 51/1000 | Loss: 0.00002156
Iteration 52/1000 | Loss: 0.00002156
Iteration 53/1000 | Loss: 0.00002156
Iteration 54/1000 | Loss: 0.00002156
Iteration 55/1000 | Loss: 0.00002156
Iteration 56/1000 | Loss: 0.00002156
Iteration 57/1000 | Loss: 0.00002156
Iteration 58/1000 | Loss: 0.00002156
Iteration 59/1000 | Loss: 0.00002156
Iteration 60/1000 | Loss: 0.00002156
Iteration 61/1000 | Loss: 0.00002156
Iteration 62/1000 | Loss: 0.00002156
Iteration 63/1000 | Loss: 0.00002155
Iteration 64/1000 | Loss: 0.00002155
Iteration 65/1000 | Loss: 0.00002155
Iteration 66/1000 | Loss: 0.00002155
Iteration 67/1000 | Loss: 0.00002155
Iteration 68/1000 | Loss: 0.00002155
Iteration 69/1000 | Loss: 0.00002155
Iteration 70/1000 | Loss: 0.00002155
Iteration 71/1000 | Loss: 0.00002155
Iteration 72/1000 | Loss: 0.00002155
Iteration 73/1000 | Loss: 0.00002154
Iteration 74/1000 | Loss: 0.00002154
Iteration 75/1000 | Loss: 0.00002154
Iteration 76/1000 | Loss: 0.00002154
Iteration 77/1000 | Loss: 0.00002154
Iteration 78/1000 | Loss: 0.00002154
Iteration 79/1000 | Loss: 0.00002154
Iteration 80/1000 | Loss: 0.00002154
Iteration 81/1000 | Loss: 0.00002154
Iteration 82/1000 | Loss: 0.00002154
Iteration 83/1000 | Loss: 0.00002154
Iteration 84/1000 | Loss: 0.00002154
Iteration 85/1000 | Loss: 0.00002154
Iteration 86/1000 | Loss: 0.00002154
Iteration 87/1000 | Loss: 0.00002153
Iteration 88/1000 | Loss: 0.00002153
Iteration 89/1000 | Loss: 0.00002153
Iteration 90/1000 | Loss: 0.00002153
Iteration 91/1000 | Loss: 0.00002153
Iteration 92/1000 | Loss: 0.00002153
Iteration 93/1000 | Loss: 0.00002153
Iteration 94/1000 | Loss: 0.00002153
Iteration 95/1000 | Loss: 0.00002153
Iteration 96/1000 | Loss: 0.00002152
Iteration 97/1000 | Loss: 0.00002152
Iteration 98/1000 | Loss: 0.00002152
Iteration 99/1000 | Loss: 0.00002152
Iteration 100/1000 | Loss: 0.00002152
Iteration 101/1000 | Loss: 0.00002152
Iteration 102/1000 | Loss: 0.00002152
Iteration 103/1000 | Loss: 0.00002152
Iteration 104/1000 | Loss: 0.00002152
Iteration 105/1000 | Loss: 0.00002152
Iteration 106/1000 | Loss: 0.00002152
Iteration 107/1000 | Loss: 0.00002152
Iteration 108/1000 | Loss: 0.00002152
Iteration 109/1000 | Loss: 0.00002152
Iteration 110/1000 | Loss: 0.00002152
Iteration 111/1000 | Loss: 0.00002152
Iteration 112/1000 | Loss: 0.00002152
Iteration 113/1000 | Loss: 0.00002152
Iteration 114/1000 | Loss: 0.00002152
Iteration 115/1000 | Loss: 0.00002152
Iteration 116/1000 | Loss: 0.00002152
Iteration 117/1000 | Loss: 0.00002152
Iteration 118/1000 | Loss: 0.00002152
Iteration 119/1000 | Loss: 0.00002152
Iteration 120/1000 | Loss: 0.00002152
Iteration 121/1000 | Loss: 0.00002152
Iteration 122/1000 | Loss: 0.00002152
Iteration 123/1000 | Loss: 0.00002152
Iteration 124/1000 | Loss: 0.00002152
Iteration 125/1000 | Loss: 0.00002152
Iteration 126/1000 | Loss: 0.00002152
Iteration 127/1000 | Loss: 0.00002152
Iteration 128/1000 | Loss: 0.00002152
Iteration 129/1000 | Loss: 0.00002152
Iteration 130/1000 | Loss: 0.00002152
Iteration 131/1000 | Loss: 0.00002152
Iteration 132/1000 | Loss: 0.00002152
Iteration 133/1000 | Loss: 0.00002152
Iteration 134/1000 | Loss: 0.00002152
Iteration 135/1000 | Loss: 0.00002152
Iteration 136/1000 | Loss: 0.00002152
Iteration 137/1000 | Loss: 0.00002152
Iteration 138/1000 | Loss: 0.00002152
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 138. Stopping optimization.
Last 5 losses: [2.1522177121369168e-05, 2.1522177121369168e-05, 2.1522177121369168e-05, 2.1522177121369168e-05, 2.1522177121369168e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1522177121369168e-05

Optimization complete. Final v2v error: 4.042943954467773 mm

Highest mean error: 4.208584785461426 mm for frame 130

Lowest mean error: 3.8837926387786865 mm for frame 151

Saving results

Total time: 31.19384479522705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_40_us_0660/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_40_us_0660/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813570
Iteration 2/25 | Loss: 0.00166326
Iteration 3/25 | Loss: 0.00141114
Iteration 4/25 | Loss: 0.00135897
Iteration 5/25 | Loss: 0.00134452
Iteration 6/25 | Loss: 0.00134043
Iteration 7/25 | Loss: 0.00133868
Iteration 8/25 | Loss: 0.00133855
Iteration 9/25 | Loss: 0.00133855
Iteration 10/25 | Loss: 0.00133855
Iteration 11/25 | Loss: 0.00133855
Iteration 12/25 | Loss: 0.00133855
Iteration 13/25 | Loss: 0.00133855
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0013385500060394406, 0.0013385500060394406, 0.0013385500060394406, 0.0013385500060394406, 0.0013385500060394406]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013385500060394406

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70527220
Iteration 2/25 | Loss: 0.00140281
Iteration 3/25 | Loss: 0.00140281
Iteration 4/25 | Loss: 0.00140281
Iteration 5/25 | Loss: 0.00140281
Iteration 6/25 | Loss: 0.00140281
Iteration 7/25 | Loss: 0.00140281
Iteration 8/25 | Loss: 0.00140281
Iteration 9/25 | Loss: 0.00140281
Iteration 10/25 | Loss: 0.00140281
Iteration 11/25 | Loss: 0.00140281
Iteration 12/25 | Loss: 0.00140281
Iteration 13/25 | Loss: 0.00140281
Iteration 14/25 | Loss: 0.00140281
Iteration 15/25 | Loss: 0.00140281
Iteration 16/25 | Loss: 0.00140281
Iteration 17/25 | Loss: 0.00140281
Iteration 18/25 | Loss: 0.00140281
Iteration 19/25 | Loss: 0.00140281
Iteration 20/25 | Loss: 0.00140281
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001402806374244392, 0.001402806374244392, 0.001402806374244392, 0.001402806374244392, 0.001402806374244392]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001402806374244392

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140281
Iteration 2/1000 | Loss: 0.00006553
Iteration 3/1000 | Loss: 0.00004216
Iteration 4/1000 | Loss: 0.00003556
Iteration 5/1000 | Loss: 0.00003183
Iteration 6/1000 | Loss: 0.00002977
Iteration 7/1000 | Loss: 0.00002811
Iteration 8/1000 | Loss: 0.00002716
Iteration 9/1000 | Loss: 0.00002628
Iteration 10/1000 | Loss: 0.00002566
Iteration 11/1000 | Loss: 0.00002518
Iteration 12/1000 | Loss: 0.00002482
Iteration 13/1000 | Loss: 0.00002449
Iteration 14/1000 | Loss: 0.00002424
Iteration 15/1000 | Loss: 0.00002422
Iteration 16/1000 | Loss: 0.00002418
Iteration 17/1000 | Loss: 0.00002406
Iteration 18/1000 | Loss: 0.00002401
Iteration 19/1000 | Loss: 0.00002400
Iteration 20/1000 | Loss: 0.00002392
Iteration 21/1000 | Loss: 0.00002391
Iteration 22/1000 | Loss: 0.00002391
Iteration 23/1000 | Loss: 0.00002390
Iteration 24/1000 | Loss: 0.00002388
Iteration 25/1000 | Loss: 0.00002387
Iteration 26/1000 | Loss: 0.00002385
Iteration 27/1000 | Loss: 0.00002384
Iteration 28/1000 | Loss: 0.00002380
Iteration 29/1000 | Loss: 0.00002380
Iteration 30/1000 | Loss: 0.00002377
Iteration 31/1000 | Loss: 0.00002377
Iteration 32/1000 | Loss: 0.00002376
Iteration 33/1000 | Loss: 0.00002376
Iteration 34/1000 | Loss: 0.00002375
Iteration 35/1000 | Loss: 0.00002374
Iteration 36/1000 | Loss: 0.00002373
Iteration 37/1000 | Loss: 0.00002372
Iteration 38/1000 | Loss: 0.00002372
Iteration 39/1000 | Loss: 0.00002372
Iteration 40/1000 | Loss: 0.00002372
Iteration 41/1000 | Loss: 0.00002371
Iteration 42/1000 | Loss: 0.00002371
Iteration 43/1000 | Loss: 0.00002370
Iteration 44/1000 | Loss: 0.00002370
Iteration 45/1000 | Loss: 0.00002369
Iteration 46/1000 | Loss: 0.00002369
Iteration 47/1000 | Loss: 0.00002368
Iteration 48/1000 | Loss: 0.00002367
Iteration 49/1000 | Loss: 0.00002367
Iteration 50/1000 | Loss: 0.00002367
Iteration 51/1000 | Loss: 0.00002366
Iteration 52/1000 | Loss: 0.00002366
Iteration 53/1000 | Loss: 0.00002366
Iteration 54/1000 | Loss: 0.00002365
Iteration 55/1000 | Loss: 0.00002365
Iteration 56/1000 | Loss: 0.00002365
Iteration 57/1000 | Loss: 0.00002364
Iteration 58/1000 | Loss: 0.00002364
Iteration 59/1000 | Loss: 0.00002364
Iteration 60/1000 | Loss: 0.00002364
Iteration 61/1000 | Loss: 0.00002364
Iteration 62/1000 | Loss: 0.00002364
Iteration 63/1000 | Loss: 0.00002363
Iteration 64/1000 | Loss: 0.00002363
Iteration 65/1000 | Loss: 0.00002363
Iteration 66/1000 | Loss: 0.00002363
Iteration 67/1000 | Loss: 0.00002363
Iteration 68/1000 | Loss: 0.00002363
Iteration 69/1000 | Loss: 0.00002363
Iteration 70/1000 | Loss: 0.00002363
Iteration 71/1000 | Loss: 0.00002362
Iteration 72/1000 | Loss: 0.00002362
Iteration 73/1000 | Loss: 0.00002362
Iteration 74/1000 | Loss: 0.00002362
Iteration 75/1000 | Loss: 0.00002361
Iteration 76/1000 | Loss: 0.00002361
Iteration 77/1000 | Loss: 0.00002361
Iteration 78/1000 | Loss: 0.00002361
Iteration 79/1000 | Loss: 0.00002361
Iteration 80/1000 | Loss: 0.00002361
Iteration 81/1000 | Loss: 0.00002361
Iteration 82/1000 | Loss: 0.00002360
Iteration 83/1000 | Loss: 0.00002360
Iteration 84/1000 | Loss: 0.00002360
Iteration 85/1000 | Loss: 0.00002360
Iteration 86/1000 | Loss: 0.00002360
Iteration 87/1000 | Loss: 0.00002359
Iteration 88/1000 | Loss: 0.00002359
Iteration 89/1000 | Loss: 0.00002359
Iteration 90/1000 | Loss: 0.00002359
Iteration 91/1000 | Loss: 0.00002359
Iteration 92/1000 | Loss: 0.00002359
Iteration 93/1000 | Loss: 0.00002359
Iteration 94/1000 | Loss: 0.00002359
Iteration 95/1000 | Loss: 0.00002359
Iteration 96/1000 | Loss: 0.00002359
Iteration 97/1000 | Loss: 0.00002359
Iteration 98/1000 | Loss: 0.00002359
Iteration 99/1000 | Loss: 0.00002359
Iteration 100/1000 | Loss: 0.00002359
Iteration 101/1000 | Loss: 0.00002359
Iteration 102/1000 | Loss: 0.00002359
Iteration 103/1000 | Loss: 0.00002359
Iteration 104/1000 | Loss: 0.00002359
Iteration 105/1000 | Loss: 0.00002359
Iteration 106/1000 | Loss: 0.00002359
Iteration 107/1000 | Loss: 0.00002359
Iteration 108/1000 | Loss: 0.00002359
Iteration 109/1000 | Loss: 0.00002359
Iteration 110/1000 | Loss: 0.00002359
Iteration 111/1000 | Loss: 0.00002359
Iteration 112/1000 | Loss: 0.00002359
Iteration 113/1000 | Loss: 0.00002359
Iteration 114/1000 | Loss: 0.00002359
Iteration 115/1000 | Loss: 0.00002359
Iteration 116/1000 | Loss: 0.00002359
Iteration 117/1000 | Loss: 0.00002359
Iteration 118/1000 | Loss: 0.00002359
Iteration 119/1000 | Loss: 0.00002359
Iteration 120/1000 | Loss: 0.00002359
Iteration 121/1000 | Loss: 0.00002359
Iteration 122/1000 | Loss: 0.00002359
Iteration 123/1000 | Loss: 0.00002359
Iteration 124/1000 | Loss: 0.00002359
Iteration 125/1000 | Loss: 0.00002359
Iteration 126/1000 | Loss: 0.00002359
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [2.358635356358718e-05, 2.358635356358718e-05, 2.358635356358718e-05, 2.358635356358718e-05, 2.358635356358718e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.358635356358718e-05

Optimization complete. Final v2v error: 4.263154029846191 mm

Highest mean error: 5.083629608154297 mm for frame 65

Lowest mean error: 3.419604539871216 mm for frame 3

Saving results

Total time: 47.89777159690857
