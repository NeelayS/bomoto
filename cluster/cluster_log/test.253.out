Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=253, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 14168-14223
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403657
Iteration 2/25 | Loss: 0.00112655
Iteration 3/25 | Loss: 0.00105436
Iteration 4/25 | Loss: 0.00104870
Iteration 5/25 | Loss: 0.00104723
Iteration 6/25 | Loss: 0.00104691
Iteration 7/25 | Loss: 0.00104691
Iteration 8/25 | Loss: 0.00104691
Iteration 9/25 | Loss: 0.00104691
Iteration 10/25 | Loss: 0.00104691
Iteration 11/25 | Loss: 0.00104691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010469094850122929, 0.0010469094850122929, 0.0010469094850122929, 0.0010469094850122929, 0.0010469094850122929]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010469094850122929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36541939
Iteration 2/25 | Loss: 0.00087485
Iteration 3/25 | Loss: 0.00087484
Iteration 4/25 | Loss: 0.00087484
Iteration 5/25 | Loss: 0.00087484
Iteration 6/25 | Loss: 0.00087484
Iteration 7/25 | Loss: 0.00087484
Iteration 8/25 | Loss: 0.00087484
Iteration 9/25 | Loss: 0.00087484
Iteration 10/25 | Loss: 0.00087484
Iteration 11/25 | Loss: 0.00087484
Iteration 12/25 | Loss: 0.00087484
Iteration 13/25 | Loss: 0.00087484
Iteration 14/25 | Loss: 0.00087484
Iteration 15/25 | Loss: 0.00087484
Iteration 16/25 | Loss: 0.00087484
Iteration 17/25 | Loss: 0.00087484
Iteration 18/25 | Loss: 0.00087484
Iteration 19/25 | Loss: 0.00087484
Iteration 20/25 | Loss: 0.00087484
Iteration 21/25 | Loss: 0.00087484
Iteration 22/25 | Loss: 0.00087484
Iteration 23/25 | Loss: 0.00087484
Iteration 24/25 | Loss: 0.00087484
Iteration 25/25 | Loss: 0.00087484
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008748375694267452, 0.0008748375694267452, 0.0008748375694267452, 0.0008748375694267452, 0.0008748375694267452]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008748375694267452

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087484
Iteration 2/1000 | Loss: 0.00001770
Iteration 3/1000 | Loss: 0.00001200
Iteration 4/1000 | Loss: 0.00001062
Iteration 5/1000 | Loss: 0.00000994
Iteration 6/1000 | Loss: 0.00000951
Iteration 7/1000 | Loss: 0.00000917
Iteration 8/1000 | Loss: 0.00000899
Iteration 9/1000 | Loss: 0.00000890
Iteration 10/1000 | Loss: 0.00000879
Iteration 11/1000 | Loss: 0.00000878
Iteration 12/1000 | Loss: 0.00000877
Iteration 13/1000 | Loss: 0.00000873
Iteration 14/1000 | Loss: 0.00000870
Iteration 15/1000 | Loss: 0.00000867
Iteration 16/1000 | Loss: 0.00000866
Iteration 17/1000 | Loss: 0.00000865
Iteration 18/1000 | Loss: 0.00000865
Iteration 19/1000 | Loss: 0.00000865
Iteration 20/1000 | Loss: 0.00000864
Iteration 21/1000 | Loss: 0.00000863
Iteration 22/1000 | Loss: 0.00000862
Iteration 23/1000 | Loss: 0.00000862
Iteration 24/1000 | Loss: 0.00000859
Iteration 25/1000 | Loss: 0.00000859
Iteration 26/1000 | Loss: 0.00000858
Iteration 27/1000 | Loss: 0.00000858
Iteration 28/1000 | Loss: 0.00000857
Iteration 29/1000 | Loss: 0.00000857
Iteration 30/1000 | Loss: 0.00000852
Iteration 31/1000 | Loss: 0.00000852
Iteration 32/1000 | Loss: 0.00000849
Iteration 33/1000 | Loss: 0.00000849
Iteration 34/1000 | Loss: 0.00000847
Iteration 35/1000 | Loss: 0.00000847
Iteration 36/1000 | Loss: 0.00000846
Iteration 37/1000 | Loss: 0.00000845
Iteration 38/1000 | Loss: 0.00000845
Iteration 39/1000 | Loss: 0.00000845
Iteration 40/1000 | Loss: 0.00000845
Iteration 41/1000 | Loss: 0.00000845
Iteration 42/1000 | Loss: 0.00000844
Iteration 43/1000 | Loss: 0.00000844
Iteration 44/1000 | Loss: 0.00000843
Iteration 45/1000 | Loss: 0.00000843
Iteration 46/1000 | Loss: 0.00000842
Iteration 47/1000 | Loss: 0.00000842
Iteration 48/1000 | Loss: 0.00000842
Iteration 49/1000 | Loss: 0.00000841
Iteration 50/1000 | Loss: 0.00000840
Iteration 51/1000 | Loss: 0.00000840
Iteration 52/1000 | Loss: 0.00000840
Iteration 53/1000 | Loss: 0.00000839
Iteration 54/1000 | Loss: 0.00000839
Iteration 55/1000 | Loss: 0.00000839
Iteration 56/1000 | Loss: 0.00000838
Iteration 57/1000 | Loss: 0.00000838
Iteration 58/1000 | Loss: 0.00000838
Iteration 59/1000 | Loss: 0.00000838
Iteration 60/1000 | Loss: 0.00000838
Iteration 61/1000 | Loss: 0.00000837
Iteration 62/1000 | Loss: 0.00000837
Iteration 63/1000 | Loss: 0.00000837
Iteration 64/1000 | Loss: 0.00000837
Iteration 65/1000 | Loss: 0.00000836
Iteration 66/1000 | Loss: 0.00000836
Iteration 67/1000 | Loss: 0.00000836
Iteration 68/1000 | Loss: 0.00000836
Iteration 69/1000 | Loss: 0.00000835
Iteration 70/1000 | Loss: 0.00000835
Iteration 71/1000 | Loss: 0.00000834
Iteration 72/1000 | Loss: 0.00000834
Iteration 73/1000 | Loss: 0.00000833
Iteration 74/1000 | Loss: 0.00000833
Iteration 75/1000 | Loss: 0.00000833
Iteration 76/1000 | Loss: 0.00000832
Iteration 77/1000 | Loss: 0.00000832
Iteration 78/1000 | Loss: 0.00000831
Iteration 79/1000 | Loss: 0.00000831
Iteration 80/1000 | Loss: 0.00000831
Iteration 81/1000 | Loss: 0.00000830
Iteration 82/1000 | Loss: 0.00000830
Iteration 83/1000 | Loss: 0.00000830
Iteration 84/1000 | Loss: 0.00000830
Iteration 85/1000 | Loss: 0.00000830
Iteration 86/1000 | Loss: 0.00000830
Iteration 87/1000 | Loss: 0.00000830
Iteration 88/1000 | Loss: 0.00000830
Iteration 89/1000 | Loss: 0.00000829
Iteration 90/1000 | Loss: 0.00000829
Iteration 91/1000 | Loss: 0.00000829
Iteration 92/1000 | Loss: 0.00000829
Iteration 93/1000 | Loss: 0.00000829
Iteration 94/1000 | Loss: 0.00000828
Iteration 95/1000 | Loss: 0.00000828
Iteration 96/1000 | Loss: 0.00000827
Iteration 97/1000 | Loss: 0.00000827
Iteration 98/1000 | Loss: 0.00000827
Iteration 99/1000 | Loss: 0.00000827
Iteration 100/1000 | Loss: 0.00000827
Iteration 101/1000 | Loss: 0.00000827
Iteration 102/1000 | Loss: 0.00000827
Iteration 103/1000 | Loss: 0.00000827
Iteration 104/1000 | Loss: 0.00000826
Iteration 105/1000 | Loss: 0.00000826
Iteration 106/1000 | Loss: 0.00000826
Iteration 107/1000 | Loss: 0.00000826
Iteration 108/1000 | Loss: 0.00000826
Iteration 109/1000 | Loss: 0.00000826
Iteration 110/1000 | Loss: 0.00000826
Iteration 111/1000 | Loss: 0.00000825
Iteration 112/1000 | Loss: 0.00000825
Iteration 113/1000 | Loss: 0.00000825
Iteration 114/1000 | Loss: 0.00000825
Iteration 115/1000 | Loss: 0.00000825
Iteration 116/1000 | Loss: 0.00000825
Iteration 117/1000 | Loss: 0.00000825
Iteration 118/1000 | Loss: 0.00000825
Iteration 119/1000 | Loss: 0.00000824
Iteration 120/1000 | Loss: 0.00000824
Iteration 121/1000 | Loss: 0.00000824
Iteration 122/1000 | Loss: 0.00000824
Iteration 123/1000 | Loss: 0.00000824
Iteration 124/1000 | Loss: 0.00000824
Iteration 125/1000 | Loss: 0.00000824
Iteration 126/1000 | Loss: 0.00000823
Iteration 127/1000 | Loss: 0.00000823
Iteration 128/1000 | Loss: 0.00000823
Iteration 129/1000 | Loss: 0.00000822
Iteration 130/1000 | Loss: 0.00000822
Iteration 131/1000 | Loss: 0.00000822
Iteration 132/1000 | Loss: 0.00000822
Iteration 133/1000 | Loss: 0.00000822
Iteration 134/1000 | Loss: 0.00000822
Iteration 135/1000 | Loss: 0.00000822
Iteration 136/1000 | Loss: 0.00000822
Iteration 137/1000 | Loss: 0.00000822
Iteration 138/1000 | Loss: 0.00000821
Iteration 139/1000 | Loss: 0.00000821
Iteration 140/1000 | Loss: 0.00000821
Iteration 141/1000 | Loss: 0.00000821
Iteration 142/1000 | Loss: 0.00000821
Iteration 143/1000 | Loss: 0.00000821
Iteration 144/1000 | Loss: 0.00000820
Iteration 145/1000 | Loss: 0.00000820
Iteration 146/1000 | Loss: 0.00000820
Iteration 147/1000 | Loss: 0.00000820
Iteration 148/1000 | Loss: 0.00000820
Iteration 149/1000 | Loss: 0.00000820
Iteration 150/1000 | Loss: 0.00000820
Iteration 151/1000 | Loss: 0.00000820
Iteration 152/1000 | Loss: 0.00000820
Iteration 153/1000 | Loss: 0.00000820
Iteration 154/1000 | Loss: 0.00000819
Iteration 155/1000 | Loss: 0.00000819
Iteration 156/1000 | Loss: 0.00000819
Iteration 157/1000 | Loss: 0.00000819
Iteration 158/1000 | Loss: 0.00000819
Iteration 159/1000 | Loss: 0.00000819
Iteration 160/1000 | Loss: 0.00000819
Iteration 161/1000 | Loss: 0.00000818
Iteration 162/1000 | Loss: 0.00000818
Iteration 163/1000 | Loss: 0.00000818
Iteration 164/1000 | Loss: 0.00000818
Iteration 165/1000 | Loss: 0.00000818
Iteration 166/1000 | Loss: 0.00000818
Iteration 167/1000 | Loss: 0.00000818
Iteration 168/1000 | Loss: 0.00000818
Iteration 169/1000 | Loss: 0.00000818
Iteration 170/1000 | Loss: 0.00000818
Iteration 171/1000 | Loss: 0.00000818
Iteration 172/1000 | Loss: 0.00000818
Iteration 173/1000 | Loss: 0.00000818
Iteration 174/1000 | Loss: 0.00000818
Iteration 175/1000 | Loss: 0.00000818
Iteration 176/1000 | Loss: 0.00000818
Iteration 177/1000 | Loss: 0.00000818
Iteration 178/1000 | Loss: 0.00000818
Iteration 179/1000 | Loss: 0.00000818
Iteration 180/1000 | Loss: 0.00000818
Iteration 181/1000 | Loss: 0.00000818
Iteration 182/1000 | Loss: 0.00000818
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [8.178615644283127e-06, 8.178615644283127e-06, 8.178615644283127e-06, 8.178615644283127e-06, 8.178615644283127e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.178615644283127e-06

Optimization complete. Final v2v error: 2.4416425228118896 mm

Highest mean error: 3.3718128204345703 mm for frame 54

Lowest mean error: 2.288486957550049 mm for frame 137

Saving results

Total time: 39.17081904411316
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00460819
Iteration 2/25 | Loss: 0.00117335
Iteration 3/25 | Loss: 0.00109805
Iteration 4/25 | Loss: 0.00108788
Iteration 5/25 | Loss: 0.00108519
Iteration 6/25 | Loss: 0.00108519
Iteration 7/25 | Loss: 0.00108519
Iteration 8/25 | Loss: 0.00108519
Iteration 9/25 | Loss: 0.00108519
Iteration 10/25 | Loss: 0.00108519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010851894039660692, 0.0010851894039660692, 0.0010851894039660692, 0.0010851894039660692, 0.0010851894039660692]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010851894039660692

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.64971733
Iteration 2/25 | Loss: 0.00077865
Iteration 3/25 | Loss: 0.00077864
Iteration 4/25 | Loss: 0.00077864
Iteration 5/25 | Loss: 0.00077864
Iteration 6/25 | Loss: 0.00077864
Iteration 7/25 | Loss: 0.00077864
Iteration 8/25 | Loss: 0.00077864
Iteration 9/25 | Loss: 0.00077864
Iteration 10/25 | Loss: 0.00077864
Iteration 11/25 | Loss: 0.00077864
Iteration 12/25 | Loss: 0.00077864
Iteration 13/25 | Loss: 0.00077864
Iteration 14/25 | Loss: 0.00077864
Iteration 15/25 | Loss: 0.00077864
Iteration 16/25 | Loss: 0.00077864
Iteration 17/25 | Loss: 0.00077864
Iteration 18/25 | Loss: 0.00077864
Iteration 19/25 | Loss: 0.00077864
Iteration 20/25 | Loss: 0.00077864
Iteration 21/25 | Loss: 0.00077864
Iteration 22/25 | Loss: 0.00077864
Iteration 23/25 | Loss: 0.00077864
Iteration 24/25 | Loss: 0.00077864
Iteration 25/25 | Loss: 0.00077864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077864
Iteration 2/1000 | Loss: 0.00001940
Iteration 3/1000 | Loss: 0.00001395
Iteration 4/1000 | Loss: 0.00001308
Iteration 5/1000 | Loss: 0.00001251
Iteration 6/1000 | Loss: 0.00001207
Iteration 7/1000 | Loss: 0.00001189
Iteration 8/1000 | Loss: 0.00001163
Iteration 9/1000 | Loss: 0.00001136
Iteration 10/1000 | Loss: 0.00001124
Iteration 11/1000 | Loss: 0.00001116
Iteration 12/1000 | Loss: 0.00001115
Iteration 13/1000 | Loss: 0.00001114
Iteration 14/1000 | Loss: 0.00001112
Iteration 15/1000 | Loss: 0.00001108
Iteration 16/1000 | Loss: 0.00001107
Iteration 17/1000 | Loss: 0.00001106
Iteration 18/1000 | Loss: 0.00001106
Iteration 19/1000 | Loss: 0.00001104
Iteration 20/1000 | Loss: 0.00001100
Iteration 21/1000 | Loss: 0.00001100
Iteration 22/1000 | Loss: 0.00001095
Iteration 23/1000 | Loss: 0.00001093
Iteration 24/1000 | Loss: 0.00001093
Iteration 25/1000 | Loss: 0.00001092
Iteration 26/1000 | Loss: 0.00001091
Iteration 27/1000 | Loss: 0.00001091
Iteration 28/1000 | Loss: 0.00001090
Iteration 29/1000 | Loss: 0.00001089
Iteration 30/1000 | Loss: 0.00001089
Iteration 31/1000 | Loss: 0.00001089
Iteration 32/1000 | Loss: 0.00001089
Iteration 33/1000 | Loss: 0.00001088
Iteration 34/1000 | Loss: 0.00001088
Iteration 35/1000 | Loss: 0.00001088
Iteration 36/1000 | Loss: 0.00001088
Iteration 37/1000 | Loss: 0.00001088
Iteration 38/1000 | Loss: 0.00001088
Iteration 39/1000 | Loss: 0.00001088
Iteration 40/1000 | Loss: 0.00001086
Iteration 41/1000 | Loss: 0.00001084
Iteration 42/1000 | Loss: 0.00001083
Iteration 43/1000 | Loss: 0.00001083
Iteration 44/1000 | Loss: 0.00001083
Iteration 45/1000 | Loss: 0.00001082
Iteration 46/1000 | Loss: 0.00001082
Iteration 47/1000 | Loss: 0.00001080
Iteration 48/1000 | Loss: 0.00001080
Iteration 49/1000 | Loss: 0.00001079
Iteration 50/1000 | Loss: 0.00001079
Iteration 51/1000 | Loss: 0.00001078
Iteration 52/1000 | Loss: 0.00001078
Iteration 53/1000 | Loss: 0.00001076
Iteration 54/1000 | Loss: 0.00001075
Iteration 55/1000 | Loss: 0.00001074
Iteration 56/1000 | Loss: 0.00001074
Iteration 57/1000 | Loss: 0.00001073
Iteration 58/1000 | Loss: 0.00001073
Iteration 59/1000 | Loss: 0.00001073
Iteration 60/1000 | Loss: 0.00001073
Iteration 61/1000 | Loss: 0.00001073
Iteration 62/1000 | Loss: 0.00001073
Iteration 63/1000 | Loss: 0.00001073
Iteration 64/1000 | Loss: 0.00001073
Iteration 65/1000 | Loss: 0.00001073
Iteration 66/1000 | Loss: 0.00001072
Iteration 67/1000 | Loss: 0.00001072
Iteration 68/1000 | Loss: 0.00001072
Iteration 69/1000 | Loss: 0.00001072
Iteration 70/1000 | Loss: 0.00001070
Iteration 71/1000 | Loss: 0.00001070
Iteration 72/1000 | Loss: 0.00001069
Iteration 73/1000 | Loss: 0.00001069
Iteration 74/1000 | Loss: 0.00001069
Iteration 75/1000 | Loss: 0.00001068
Iteration 76/1000 | Loss: 0.00001068
Iteration 77/1000 | Loss: 0.00001068
Iteration 78/1000 | Loss: 0.00001068
Iteration 79/1000 | Loss: 0.00001067
Iteration 80/1000 | Loss: 0.00001067
Iteration 81/1000 | Loss: 0.00001067
Iteration 82/1000 | Loss: 0.00001067
Iteration 83/1000 | Loss: 0.00001067
Iteration 84/1000 | Loss: 0.00001066
Iteration 85/1000 | Loss: 0.00001066
Iteration 86/1000 | Loss: 0.00001066
Iteration 87/1000 | Loss: 0.00001066
Iteration 88/1000 | Loss: 0.00001066
Iteration 89/1000 | Loss: 0.00001066
Iteration 90/1000 | Loss: 0.00001065
Iteration 91/1000 | Loss: 0.00001065
Iteration 92/1000 | Loss: 0.00001065
Iteration 93/1000 | Loss: 0.00001064
Iteration 94/1000 | Loss: 0.00001064
Iteration 95/1000 | Loss: 0.00001064
Iteration 96/1000 | Loss: 0.00001064
Iteration 97/1000 | Loss: 0.00001064
Iteration 98/1000 | Loss: 0.00001064
Iteration 99/1000 | Loss: 0.00001064
Iteration 100/1000 | Loss: 0.00001064
Iteration 101/1000 | Loss: 0.00001064
Iteration 102/1000 | Loss: 0.00001064
Iteration 103/1000 | Loss: 0.00001064
Iteration 104/1000 | Loss: 0.00001064
Iteration 105/1000 | Loss: 0.00001064
Iteration 106/1000 | Loss: 0.00001064
Iteration 107/1000 | Loss: 0.00001064
Iteration 108/1000 | Loss: 0.00001064
Iteration 109/1000 | Loss: 0.00001064
Iteration 110/1000 | Loss: 0.00001064
Iteration 111/1000 | Loss: 0.00001064
Iteration 112/1000 | Loss: 0.00001064
Iteration 113/1000 | Loss: 0.00001064
Iteration 114/1000 | Loss: 0.00001064
Iteration 115/1000 | Loss: 0.00001064
Iteration 116/1000 | Loss: 0.00001064
Iteration 117/1000 | Loss: 0.00001064
Iteration 118/1000 | Loss: 0.00001064
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.0636513252393343e-05, 1.0636513252393343e-05, 1.0636513252393343e-05, 1.0636513252393343e-05, 1.0636513252393343e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0636513252393343e-05

Optimization complete. Final v2v error: 2.801490068435669 mm

Highest mean error: 3.12388277053833 mm for frame 53

Lowest mean error: 2.5472166538238525 mm for frame 37

Saving results

Total time: 36.19136571884155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807156
Iteration 2/25 | Loss: 0.00150836
Iteration 3/25 | Loss: 0.00113723
Iteration 4/25 | Loss: 0.00109986
Iteration 5/25 | Loss: 0.00109555
Iteration 6/25 | Loss: 0.00109435
Iteration 7/25 | Loss: 0.00109383
Iteration 8/25 | Loss: 0.00109354
Iteration 9/25 | Loss: 0.00109332
Iteration 10/25 | Loss: 0.00109565
Iteration 11/25 | Loss: 0.00109542
Iteration 12/25 | Loss: 0.00109684
Iteration 13/25 | Loss: 0.00109495
Iteration 14/25 | Loss: 0.00109404
Iteration 15/25 | Loss: 0.00109433
Iteration 16/25 | Loss: 0.00109419
Iteration 17/25 | Loss: 0.00109514
Iteration 18/25 | Loss: 0.00109441
Iteration 19/25 | Loss: 0.00109485
Iteration 20/25 | Loss: 0.00109486
Iteration 21/25 | Loss: 0.00109536
Iteration 22/25 | Loss: 0.00109484
Iteration 23/25 | Loss: 0.00109276
Iteration 24/25 | Loss: 0.00109393
Iteration 25/25 | Loss: 0.00109377

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35638356
Iteration 2/25 | Loss: 0.00079136
Iteration 3/25 | Loss: 0.00079136
Iteration 4/25 | Loss: 0.00079136
Iteration 5/25 | Loss: 0.00079136
Iteration 6/25 | Loss: 0.00079136
Iteration 7/25 | Loss: 0.00079136
Iteration 8/25 | Loss: 0.00079136
Iteration 9/25 | Loss: 0.00079136
Iteration 10/25 | Loss: 0.00079136
Iteration 11/25 | Loss: 0.00079136
Iteration 12/25 | Loss: 0.00079136
Iteration 13/25 | Loss: 0.00079136
Iteration 14/25 | Loss: 0.00079136
Iteration 15/25 | Loss: 0.00079136
Iteration 16/25 | Loss: 0.00079136
Iteration 17/25 | Loss: 0.00079136
Iteration 18/25 | Loss: 0.00079136
Iteration 19/25 | Loss: 0.00079136
Iteration 20/25 | Loss: 0.00079136
Iteration 21/25 | Loss: 0.00079136
Iteration 22/25 | Loss: 0.00079136
Iteration 23/25 | Loss: 0.00079136
Iteration 24/25 | Loss: 0.00079136
Iteration 25/25 | Loss: 0.00079136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079136
Iteration 2/1000 | Loss: 0.00005229
Iteration 3/1000 | Loss: 0.00003680
Iteration 4/1000 | Loss: 0.00002388
Iteration 5/1000 | Loss: 0.00002394
Iteration 6/1000 | Loss: 0.00003619
Iteration 7/1000 | Loss: 0.00005075
Iteration 8/1000 | Loss: 0.00005103
Iteration 9/1000 | Loss: 0.00004837
Iteration 10/1000 | Loss: 0.00004990
Iteration 11/1000 | Loss: 0.00004388
Iteration 12/1000 | Loss: 0.00003139
Iteration 13/1000 | Loss: 0.00004421
Iteration 14/1000 | Loss: 0.00003973
Iteration 15/1000 | Loss: 0.00004715
Iteration 16/1000 | Loss: 0.00004133
Iteration 17/1000 | Loss: 0.00004774
Iteration 18/1000 | Loss: 0.00005051
Iteration 19/1000 | Loss: 0.00002181
Iteration 20/1000 | Loss: 0.00001858
Iteration 21/1000 | Loss: 0.00003923
Iteration 22/1000 | Loss: 0.00001981
Iteration 23/1000 | Loss: 0.00001968
Iteration 24/1000 | Loss: 0.00001446
Iteration 25/1000 | Loss: 0.00001273
Iteration 26/1000 | Loss: 0.00001177
Iteration 27/1000 | Loss: 0.00001129
Iteration 28/1000 | Loss: 0.00001084
Iteration 29/1000 | Loss: 0.00001041
Iteration 30/1000 | Loss: 0.00001032
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001011
Iteration 33/1000 | Loss: 0.00001007
Iteration 34/1000 | Loss: 0.00001000
Iteration 35/1000 | Loss: 0.00000998
Iteration 36/1000 | Loss: 0.00000996
Iteration 37/1000 | Loss: 0.00000996
Iteration 38/1000 | Loss: 0.00000994
Iteration 39/1000 | Loss: 0.00000991
Iteration 40/1000 | Loss: 0.00000991
Iteration 41/1000 | Loss: 0.00000990
Iteration 42/1000 | Loss: 0.00000990
Iteration 43/1000 | Loss: 0.00000989
Iteration 44/1000 | Loss: 0.00000987
Iteration 45/1000 | Loss: 0.00000986
Iteration 46/1000 | Loss: 0.00000986
Iteration 47/1000 | Loss: 0.00000985
Iteration 48/1000 | Loss: 0.00000985
Iteration 49/1000 | Loss: 0.00000984
Iteration 50/1000 | Loss: 0.00000984
Iteration 51/1000 | Loss: 0.00000983
Iteration 52/1000 | Loss: 0.00000983
Iteration 53/1000 | Loss: 0.00000983
Iteration 54/1000 | Loss: 0.00000982
Iteration 55/1000 | Loss: 0.00000981
Iteration 56/1000 | Loss: 0.00000981
Iteration 57/1000 | Loss: 0.00000980
Iteration 58/1000 | Loss: 0.00000980
Iteration 59/1000 | Loss: 0.00000980
Iteration 60/1000 | Loss: 0.00000980
Iteration 61/1000 | Loss: 0.00000980
Iteration 62/1000 | Loss: 0.00000980
Iteration 63/1000 | Loss: 0.00000980
Iteration 64/1000 | Loss: 0.00000980
Iteration 65/1000 | Loss: 0.00000980
Iteration 66/1000 | Loss: 0.00000980
Iteration 67/1000 | Loss: 0.00000979
Iteration 68/1000 | Loss: 0.00000979
Iteration 69/1000 | Loss: 0.00000979
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000977
Iteration 72/1000 | Loss: 0.00000976
Iteration 73/1000 | Loss: 0.00000976
Iteration 74/1000 | Loss: 0.00000976
Iteration 75/1000 | Loss: 0.00000976
Iteration 76/1000 | Loss: 0.00000975
Iteration 77/1000 | Loss: 0.00000975
Iteration 78/1000 | Loss: 0.00000975
Iteration 79/1000 | Loss: 0.00000975
Iteration 80/1000 | Loss: 0.00000975
Iteration 81/1000 | Loss: 0.00000975
Iteration 82/1000 | Loss: 0.00000975
Iteration 83/1000 | Loss: 0.00000974
Iteration 84/1000 | Loss: 0.00000974
Iteration 85/1000 | Loss: 0.00000974
Iteration 86/1000 | Loss: 0.00000974
Iteration 87/1000 | Loss: 0.00000974
Iteration 88/1000 | Loss: 0.00000973
Iteration 89/1000 | Loss: 0.00000973
Iteration 90/1000 | Loss: 0.00000973
Iteration 91/1000 | Loss: 0.00000973
Iteration 92/1000 | Loss: 0.00000973
Iteration 93/1000 | Loss: 0.00000973
Iteration 94/1000 | Loss: 0.00000973
Iteration 95/1000 | Loss: 0.00000973
Iteration 96/1000 | Loss: 0.00000972
Iteration 97/1000 | Loss: 0.00000972
Iteration 98/1000 | Loss: 0.00000972
Iteration 99/1000 | Loss: 0.00000972
Iteration 100/1000 | Loss: 0.00000972
Iteration 101/1000 | Loss: 0.00000972
Iteration 102/1000 | Loss: 0.00000972
Iteration 103/1000 | Loss: 0.00000971
Iteration 104/1000 | Loss: 0.00000971
Iteration 105/1000 | Loss: 0.00000971
Iteration 106/1000 | Loss: 0.00000971
Iteration 107/1000 | Loss: 0.00000971
Iteration 108/1000 | Loss: 0.00000971
Iteration 109/1000 | Loss: 0.00000971
Iteration 110/1000 | Loss: 0.00000971
Iteration 111/1000 | Loss: 0.00000970
Iteration 112/1000 | Loss: 0.00000970
Iteration 113/1000 | Loss: 0.00000970
Iteration 114/1000 | Loss: 0.00000969
Iteration 115/1000 | Loss: 0.00000969
Iteration 116/1000 | Loss: 0.00000969
Iteration 117/1000 | Loss: 0.00000969
Iteration 118/1000 | Loss: 0.00000969
Iteration 119/1000 | Loss: 0.00000969
Iteration 120/1000 | Loss: 0.00000969
Iteration 121/1000 | Loss: 0.00000969
Iteration 122/1000 | Loss: 0.00000969
Iteration 123/1000 | Loss: 0.00000969
Iteration 124/1000 | Loss: 0.00000969
Iteration 125/1000 | Loss: 0.00000969
Iteration 126/1000 | Loss: 0.00000969
Iteration 127/1000 | Loss: 0.00000969
Iteration 128/1000 | Loss: 0.00000969
Iteration 129/1000 | Loss: 0.00000968
Iteration 130/1000 | Loss: 0.00000968
Iteration 131/1000 | Loss: 0.00000968
Iteration 132/1000 | Loss: 0.00000968
Iteration 133/1000 | Loss: 0.00000968
Iteration 134/1000 | Loss: 0.00000968
Iteration 135/1000 | Loss: 0.00000968
Iteration 136/1000 | Loss: 0.00000968
Iteration 137/1000 | Loss: 0.00000968
Iteration 138/1000 | Loss: 0.00000968
Iteration 139/1000 | Loss: 0.00000968
Iteration 140/1000 | Loss: 0.00000968
Iteration 141/1000 | Loss: 0.00000968
Iteration 142/1000 | Loss: 0.00000968
Iteration 143/1000 | Loss: 0.00000968
Iteration 144/1000 | Loss: 0.00000968
Iteration 145/1000 | Loss: 0.00000968
Iteration 146/1000 | Loss: 0.00000968
Iteration 147/1000 | Loss: 0.00000968
Iteration 148/1000 | Loss: 0.00000968
Iteration 149/1000 | Loss: 0.00000968
Iteration 150/1000 | Loss: 0.00000967
Iteration 151/1000 | Loss: 0.00000967
Iteration 152/1000 | Loss: 0.00000967
Iteration 153/1000 | Loss: 0.00000967
Iteration 154/1000 | Loss: 0.00000967
Iteration 155/1000 | Loss: 0.00000967
Iteration 156/1000 | Loss: 0.00000967
Iteration 157/1000 | Loss: 0.00000967
Iteration 158/1000 | Loss: 0.00000967
Iteration 159/1000 | Loss: 0.00000967
Iteration 160/1000 | Loss: 0.00000967
Iteration 161/1000 | Loss: 0.00000967
Iteration 162/1000 | Loss: 0.00000967
Iteration 163/1000 | Loss: 0.00000967
Iteration 164/1000 | Loss: 0.00000967
Iteration 165/1000 | Loss: 0.00000966
Iteration 166/1000 | Loss: 0.00000966
Iteration 167/1000 | Loss: 0.00000966
Iteration 168/1000 | Loss: 0.00000966
Iteration 169/1000 | Loss: 0.00000966
Iteration 170/1000 | Loss: 0.00000966
Iteration 171/1000 | Loss: 0.00000966
Iteration 172/1000 | Loss: 0.00000966
Iteration 173/1000 | Loss: 0.00000966
Iteration 174/1000 | Loss: 0.00000966
Iteration 175/1000 | Loss: 0.00000966
Iteration 176/1000 | Loss: 0.00000966
Iteration 177/1000 | Loss: 0.00000966
Iteration 178/1000 | Loss: 0.00000966
Iteration 179/1000 | Loss: 0.00000966
Iteration 180/1000 | Loss: 0.00000966
Iteration 181/1000 | Loss: 0.00000965
Iteration 182/1000 | Loss: 0.00000965
Iteration 183/1000 | Loss: 0.00000965
Iteration 184/1000 | Loss: 0.00000965
Iteration 185/1000 | Loss: 0.00000965
Iteration 186/1000 | Loss: 0.00000965
Iteration 187/1000 | Loss: 0.00000965
Iteration 188/1000 | Loss: 0.00000964
Iteration 189/1000 | Loss: 0.00000964
Iteration 190/1000 | Loss: 0.00000964
Iteration 191/1000 | Loss: 0.00000964
Iteration 192/1000 | Loss: 0.00000964
Iteration 193/1000 | Loss: 0.00000964
Iteration 194/1000 | Loss: 0.00000964
Iteration 195/1000 | Loss: 0.00000964
Iteration 196/1000 | Loss: 0.00000964
Iteration 197/1000 | Loss: 0.00000964
Iteration 198/1000 | Loss: 0.00000964
Iteration 199/1000 | Loss: 0.00000964
Iteration 200/1000 | Loss: 0.00000964
Iteration 201/1000 | Loss: 0.00000964
Iteration 202/1000 | Loss: 0.00000964
Iteration 203/1000 | Loss: 0.00000964
Iteration 204/1000 | Loss: 0.00000964
Iteration 205/1000 | Loss: 0.00000964
Iteration 206/1000 | Loss: 0.00000964
Iteration 207/1000 | Loss: 0.00000964
Iteration 208/1000 | Loss: 0.00000964
Iteration 209/1000 | Loss: 0.00000964
Iteration 210/1000 | Loss: 0.00000964
Iteration 211/1000 | Loss: 0.00000964
Iteration 212/1000 | Loss: 0.00000964
Iteration 213/1000 | Loss: 0.00000964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [9.643036719353404e-06, 9.643036719353404e-06, 9.643036719353404e-06, 9.643036719353404e-06, 9.643036719353404e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.643036719353404e-06

Optimization complete. Final v2v error: 2.679591417312622 mm

Highest mean error: 3.7847955226898193 mm for frame 53

Lowest mean error: 2.566255807876587 mm for frame 165

Saving results

Total time: 104.76379370689392
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00538999
Iteration 2/25 | Loss: 0.00114444
Iteration 3/25 | Loss: 0.00108137
Iteration 4/25 | Loss: 0.00107171
Iteration 5/25 | Loss: 0.00106855
Iteration 6/25 | Loss: 0.00106781
Iteration 7/25 | Loss: 0.00106781
Iteration 8/25 | Loss: 0.00106781
Iteration 9/25 | Loss: 0.00106781
Iteration 10/25 | Loss: 0.00106781
Iteration 11/25 | Loss: 0.00106781
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010678106918931007, 0.0010678106918931007, 0.0010678106918931007, 0.0010678106918931007, 0.0010678106918931007]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010678106918931007

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.42513371
Iteration 2/25 | Loss: 0.00074632
Iteration 3/25 | Loss: 0.00074631
Iteration 4/25 | Loss: 0.00074631
Iteration 5/25 | Loss: 0.00074631
Iteration 6/25 | Loss: 0.00074631
Iteration 7/25 | Loss: 0.00074631
Iteration 8/25 | Loss: 0.00074631
Iteration 9/25 | Loss: 0.00074631
Iteration 10/25 | Loss: 0.00074631
Iteration 11/25 | Loss: 0.00074631
Iteration 12/25 | Loss: 0.00074631
Iteration 13/25 | Loss: 0.00074631
Iteration 14/25 | Loss: 0.00074631
Iteration 15/25 | Loss: 0.00074631
Iteration 16/25 | Loss: 0.00074631
Iteration 17/25 | Loss: 0.00074631
Iteration 18/25 | Loss: 0.00074631
Iteration 19/25 | Loss: 0.00074631
Iteration 20/25 | Loss: 0.00074631
Iteration 21/25 | Loss: 0.00074631
Iteration 22/25 | Loss: 0.00074631
Iteration 23/25 | Loss: 0.00074631
Iteration 24/25 | Loss: 0.00074631
Iteration 25/25 | Loss: 0.00074631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074631
Iteration 2/1000 | Loss: 0.00002338
Iteration 3/1000 | Loss: 0.00001651
Iteration 4/1000 | Loss: 0.00001411
Iteration 5/1000 | Loss: 0.00001318
Iteration 6/1000 | Loss: 0.00001263
Iteration 7/1000 | Loss: 0.00001224
Iteration 8/1000 | Loss: 0.00001187
Iteration 9/1000 | Loss: 0.00001162
Iteration 10/1000 | Loss: 0.00001135
Iteration 11/1000 | Loss: 0.00001118
Iteration 12/1000 | Loss: 0.00001104
Iteration 13/1000 | Loss: 0.00001101
Iteration 14/1000 | Loss: 0.00001100
Iteration 15/1000 | Loss: 0.00001097
Iteration 16/1000 | Loss: 0.00001095
Iteration 17/1000 | Loss: 0.00001091
Iteration 18/1000 | Loss: 0.00001084
Iteration 19/1000 | Loss: 0.00001083
Iteration 20/1000 | Loss: 0.00001083
Iteration 21/1000 | Loss: 0.00001083
Iteration 22/1000 | Loss: 0.00001081
Iteration 23/1000 | Loss: 0.00001081
Iteration 24/1000 | Loss: 0.00001080
Iteration 25/1000 | Loss: 0.00001080
Iteration 26/1000 | Loss: 0.00001079
Iteration 27/1000 | Loss: 0.00001079
Iteration 28/1000 | Loss: 0.00001079
Iteration 29/1000 | Loss: 0.00001078
Iteration 30/1000 | Loss: 0.00001077
Iteration 31/1000 | Loss: 0.00001077
Iteration 32/1000 | Loss: 0.00001076
Iteration 33/1000 | Loss: 0.00001076
Iteration 34/1000 | Loss: 0.00001075
Iteration 35/1000 | Loss: 0.00001075
Iteration 36/1000 | Loss: 0.00001075
Iteration 37/1000 | Loss: 0.00001075
Iteration 38/1000 | Loss: 0.00001074
Iteration 39/1000 | Loss: 0.00001073
Iteration 40/1000 | Loss: 0.00001073
Iteration 41/1000 | Loss: 0.00001070
Iteration 42/1000 | Loss: 0.00001070
Iteration 43/1000 | Loss: 0.00001070
Iteration 44/1000 | Loss: 0.00001069
Iteration 45/1000 | Loss: 0.00001069
Iteration 46/1000 | Loss: 0.00001067
Iteration 47/1000 | Loss: 0.00001066
Iteration 48/1000 | Loss: 0.00001065
Iteration 49/1000 | Loss: 0.00001064
Iteration 50/1000 | Loss: 0.00001062
Iteration 51/1000 | Loss: 0.00001061
Iteration 52/1000 | Loss: 0.00001061
Iteration 53/1000 | Loss: 0.00001060
Iteration 54/1000 | Loss: 0.00001060
Iteration 55/1000 | Loss: 0.00001059
Iteration 56/1000 | Loss: 0.00001059
Iteration 57/1000 | Loss: 0.00001058
Iteration 58/1000 | Loss: 0.00001058
Iteration 59/1000 | Loss: 0.00001058
Iteration 60/1000 | Loss: 0.00001058
Iteration 61/1000 | Loss: 0.00001057
Iteration 62/1000 | Loss: 0.00001057
Iteration 63/1000 | Loss: 0.00001057
Iteration 64/1000 | Loss: 0.00001056
Iteration 65/1000 | Loss: 0.00001056
Iteration 66/1000 | Loss: 0.00001056
Iteration 67/1000 | Loss: 0.00001055
Iteration 68/1000 | Loss: 0.00001055
Iteration 69/1000 | Loss: 0.00001055
Iteration 70/1000 | Loss: 0.00001054
Iteration 71/1000 | Loss: 0.00001054
Iteration 72/1000 | Loss: 0.00001054
Iteration 73/1000 | Loss: 0.00001054
Iteration 74/1000 | Loss: 0.00001053
Iteration 75/1000 | Loss: 0.00001052
Iteration 76/1000 | Loss: 0.00001052
Iteration 77/1000 | Loss: 0.00001051
Iteration 78/1000 | Loss: 0.00001051
Iteration 79/1000 | Loss: 0.00001051
Iteration 80/1000 | Loss: 0.00001051
Iteration 81/1000 | Loss: 0.00001051
Iteration 82/1000 | Loss: 0.00001051
Iteration 83/1000 | Loss: 0.00001051
Iteration 84/1000 | Loss: 0.00001051
Iteration 85/1000 | Loss: 0.00001051
Iteration 86/1000 | Loss: 0.00001051
Iteration 87/1000 | Loss: 0.00001050
Iteration 88/1000 | Loss: 0.00001050
Iteration 89/1000 | Loss: 0.00001050
Iteration 90/1000 | Loss: 0.00001050
Iteration 91/1000 | Loss: 0.00001050
Iteration 92/1000 | Loss: 0.00001049
Iteration 93/1000 | Loss: 0.00001049
Iteration 94/1000 | Loss: 0.00001048
Iteration 95/1000 | Loss: 0.00001048
Iteration 96/1000 | Loss: 0.00001048
Iteration 97/1000 | Loss: 0.00001048
Iteration 98/1000 | Loss: 0.00001047
Iteration 99/1000 | Loss: 0.00001047
Iteration 100/1000 | Loss: 0.00001047
Iteration 101/1000 | Loss: 0.00001047
Iteration 102/1000 | Loss: 0.00001046
Iteration 103/1000 | Loss: 0.00001046
Iteration 104/1000 | Loss: 0.00001046
Iteration 105/1000 | Loss: 0.00001045
Iteration 106/1000 | Loss: 0.00001045
Iteration 107/1000 | Loss: 0.00001045
Iteration 108/1000 | Loss: 0.00001044
Iteration 109/1000 | Loss: 0.00001044
Iteration 110/1000 | Loss: 0.00001044
Iteration 111/1000 | Loss: 0.00001043
Iteration 112/1000 | Loss: 0.00001043
Iteration 113/1000 | Loss: 0.00001043
Iteration 114/1000 | Loss: 0.00001043
Iteration 115/1000 | Loss: 0.00001043
Iteration 116/1000 | Loss: 0.00001043
Iteration 117/1000 | Loss: 0.00001043
Iteration 118/1000 | Loss: 0.00001043
Iteration 119/1000 | Loss: 0.00001043
Iteration 120/1000 | Loss: 0.00001043
Iteration 121/1000 | Loss: 0.00001043
Iteration 122/1000 | Loss: 0.00001043
Iteration 123/1000 | Loss: 0.00001042
Iteration 124/1000 | Loss: 0.00001042
Iteration 125/1000 | Loss: 0.00001042
Iteration 126/1000 | Loss: 0.00001042
Iteration 127/1000 | Loss: 0.00001042
Iteration 128/1000 | Loss: 0.00001042
Iteration 129/1000 | Loss: 0.00001042
Iteration 130/1000 | Loss: 0.00001042
Iteration 131/1000 | Loss: 0.00001042
Iteration 132/1000 | Loss: 0.00001042
Iteration 133/1000 | Loss: 0.00001042
Iteration 134/1000 | Loss: 0.00001042
Iteration 135/1000 | Loss: 0.00001042
Iteration 136/1000 | Loss: 0.00001042
Iteration 137/1000 | Loss: 0.00001042
Iteration 138/1000 | Loss: 0.00001042
Iteration 139/1000 | Loss: 0.00001042
Iteration 140/1000 | Loss: 0.00001042
Iteration 141/1000 | Loss: 0.00001042
Iteration 142/1000 | Loss: 0.00001042
Iteration 143/1000 | Loss: 0.00001041
Iteration 144/1000 | Loss: 0.00001041
Iteration 145/1000 | Loss: 0.00001041
Iteration 146/1000 | Loss: 0.00001041
Iteration 147/1000 | Loss: 0.00001041
Iteration 148/1000 | Loss: 0.00001041
Iteration 149/1000 | Loss: 0.00001041
Iteration 150/1000 | Loss: 0.00001041
Iteration 151/1000 | Loss: 0.00001041
Iteration 152/1000 | Loss: 0.00001041
Iteration 153/1000 | Loss: 0.00001041
Iteration 154/1000 | Loss: 0.00001041
Iteration 155/1000 | Loss: 0.00001041
Iteration 156/1000 | Loss: 0.00001041
Iteration 157/1000 | Loss: 0.00001041
Iteration 158/1000 | Loss: 0.00001041
Iteration 159/1000 | Loss: 0.00001041
Iteration 160/1000 | Loss: 0.00001041
Iteration 161/1000 | Loss: 0.00001041
Iteration 162/1000 | Loss: 0.00001041
Iteration 163/1000 | Loss: 0.00001041
Iteration 164/1000 | Loss: 0.00001041
Iteration 165/1000 | Loss: 0.00001041
Iteration 166/1000 | Loss: 0.00001040
Iteration 167/1000 | Loss: 0.00001040
Iteration 168/1000 | Loss: 0.00001040
Iteration 169/1000 | Loss: 0.00001040
Iteration 170/1000 | Loss: 0.00001040
Iteration 171/1000 | Loss: 0.00001040
Iteration 172/1000 | Loss: 0.00001040
Iteration 173/1000 | Loss: 0.00001040
Iteration 174/1000 | Loss: 0.00001040
Iteration 175/1000 | Loss: 0.00001040
Iteration 176/1000 | Loss: 0.00001040
Iteration 177/1000 | Loss: 0.00001040
Iteration 178/1000 | Loss: 0.00001040
Iteration 179/1000 | Loss: 0.00001040
Iteration 180/1000 | Loss: 0.00001040
Iteration 181/1000 | Loss: 0.00001040
Iteration 182/1000 | Loss: 0.00001040
Iteration 183/1000 | Loss: 0.00001040
Iteration 184/1000 | Loss: 0.00001040
Iteration 185/1000 | Loss: 0.00001040
Iteration 186/1000 | Loss: 0.00001040
Iteration 187/1000 | Loss: 0.00001039
Iteration 188/1000 | Loss: 0.00001039
Iteration 189/1000 | Loss: 0.00001039
Iteration 190/1000 | Loss: 0.00001039
Iteration 191/1000 | Loss: 0.00001039
Iteration 192/1000 | Loss: 0.00001039
Iteration 193/1000 | Loss: 0.00001039
Iteration 194/1000 | Loss: 0.00001039
Iteration 195/1000 | Loss: 0.00001039
Iteration 196/1000 | Loss: 0.00001039
Iteration 197/1000 | Loss: 0.00001039
Iteration 198/1000 | Loss: 0.00001039
Iteration 199/1000 | Loss: 0.00001039
Iteration 200/1000 | Loss: 0.00001039
Iteration 201/1000 | Loss: 0.00001039
Iteration 202/1000 | Loss: 0.00001039
Iteration 203/1000 | Loss: 0.00001039
Iteration 204/1000 | Loss: 0.00001039
Iteration 205/1000 | Loss: 0.00001039
Iteration 206/1000 | Loss: 0.00001039
Iteration 207/1000 | Loss: 0.00001038
Iteration 208/1000 | Loss: 0.00001038
Iteration 209/1000 | Loss: 0.00001038
Iteration 210/1000 | Loss: 0.00001038
Iteration 211/1000 | Loss: 0.00001038
Iteration 212/1000 | Loss: 0.00001038
Iteration 213/1000 | Loss: 0.00001038
Iteration 214/1000 | Loss: 0.00001038
Iteration 215/1000 | Loss: 0.00001038
Iteration 216/1000 | Loss: 0.00001038
Iteration 217/1000 | Loss: 0.00001038
Iteration 218/1000 | Loss: 0.00001038
Iteration 219/1000 | Loss: 0.00001038
Iteration 220/1000 | Loss: 0.00001038
Iteration 221/1000 | Loss: 0.00001038
Iteration 222/1000 | Loss: 0.00001038
Iteration 223/1000 | Loss: 0.00001038
Iteration 224/1000 | Loss: 0.00001038
Iteration 225/1000 | Loss: 0.00001038
Iteration 226/1000 | Loss: 0.00001038
Iteration 227/1000 | Loss: 0.00001038
Iteration 228/1000 | Loss: 0.00001038
Iteration 229/1000 | Loss: 0.00001038
Iteration 230/1000 | Loss: 0.00001038
Iteration 231/1000 | Loss: 0.00001038
Iteration 232/1000 | Loss: 0.00001038
Iteration 233/1000 | Loss: 0.00001038
Iteration 234/1000 | Loss: 0.00001038
Iteration 235/1000 | Loss: 0.00001038
Iteration 236/1000 | Loss: 0.00001038
Iteration 237/1000 | Loss: 0.00001038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.037704168993514e-05, 1.037704168993514e-05, 1.037704168993514e-05, 1.037704168993514e-05, 1.037704168993514e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.037704168993514e-05

Optimization complete. Final v2v error: 2.740750551223755 mm

Highest mean error: 3.4291114807128906 mm for frame 92

Lowest mean error: 2.4277727603912354 mm for frame 33

Saving results

Total time: 44.97349286079407
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00795939
Iteration 2/25 | Loss: 0.00116470
Iteration 3/25 | Loss: 0.00106189
Iteration 4/25 | Loss: 0.00104988
Iteration 5/25 | Loss: 0.00104750
Iteration 6/25 | Loss: 0.00104750
Iteration 7/25 | Loss: 0.00104750
Iteration 8/25 | Loss: 0.00104750
Iteration 9/25 | Loss: 0.00104750
Iteration 10/25 | Loss: 0.00104750
Iteration 11/25 | Loss: 0.00104750
Iteration 12/25 | Loss: 0.00104750
Iteration 13/25 | Loss: 0.00104750
Iteration 14/25 | Loss: 0.00104750
Iteration 15/25 | Loss: 0.00104750
Iteration 16/25 | Loss: 0.00104750
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010474964510649443, 0.0010474964510649443, 0.0010474964510649443, 0.0010474964510649443, 0.0010474964510649443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010474964510649443

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35793650
Iteration 2/25 | Loss: 0.00074988
Iteration 3/25 | Loss: 0.00074988
Iteration 4/25 | Loss: 0.00074988
Iteration 5/25 | Loss: 0.00074988
Iteration 6/25 | Loss: 0.00074988
Iteration 7/25 | Loss: 0.00074988
Iteration 8/25 | Loss: 0.00074988
Iteration 9/25 | Loss: 0.00074988
Iteration 10/25 | Loss: 0.00074988
Iteration 11/25 | Loss: 0.00074988
Iteration 12/25 | Loss: 0.00074988
Iteration 13/25 | Loss: 0.00074988
Iteration 14/25 | Loss: 0.00074988
Iteration 15/25 | Loss: 0.00074988
Iteration 16/25 | Loss: 0.00074988
Iteration 17/25 | Loss: 0.00074988
Iteration 18/25 | Loss: 0.00074988
Iteration 19/25 | Loss: 0.00074988
Iteration 20/25 | Loss: 0.00074988
Iteration 21/25 | Loss: 0.00074988
Iteration 22/25 | Loss: 0.00074988
Iteration 23/25 | Loss: 0.00074988
Iteration 24/25 | Loss: 0.00074988
Iteration 25/25 | Loss: 0.00074988

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074988
Iteration 2/1000 | Loss: 0.00001792
Iteration 3/1000 | Loss: 0.00001246
Iteration 4/1000 | Loss: 0.00001097
Iteration 5/1000 | Loss: 0.00001030
Iteration 6/1000 | Loss: 0.00000992
Iteration 7/1000 | Loss: 0.00000955
Iteration 8/1000 | Loss: 0.00000941
Iteration 9/1000 | Loss: 0.00000918
Iteration 10/1000 | Loss: 0.00000918
Iteration 11/1000 | Loss: 0.00000897
Iteration 12/1000 | Loss: 0.00000894
Iteration 13/1000 | Loss: 0.00000893
Iteration 14/1000 | Loss: 0.00000891
Iteration 15/1000 | Loss: 0.00000891
Iteration 16/1000 | Loss: 0.00000889
Iteration 17/1000 | Loss: 0.00000889
Iteration 18/1000 | Loss: 0.00000888
Iteration 19/1000 | Loss: 0.00000881
Iteration 20/1000 | Loss: 0.00000880
Iteration 21/1000 | Loss: 0.00000875
Iteration 22/1000 | Loss: 0.00000873
Iteration 23/1000 | Loss: 0.00000870
Iteration 24/1000 | Loss: 0.00000869
Iteration 25/1000 | Loss: 0.00000869
Iteration 26/1000 | Loss: 0.00000869
Iteration 27/1000 | Loss: 0.00000869
Iteration 28/1000 | Loss: 0.00000868
Iteration 29/1000 | Loss: 0.00000867
Iteration 30/1000 | Loss: 0.00000866
Iteration 31/1000 | Loss: 0.00000866
Iteration 32/1000 | Loss: 0.00000865
Iteration 33/1000 | Loss: 0.00000865
Iteration 34/1000 | Loss: 0.00000865
Iteration 35/1000 | Loss: 0.00000864
Iteration 36/1000 | Loss: 0.00000864
Iteration 37/1000 | Loss: 0.00000864
Iteration 38/1000 | Loss: 0.00000863
Iteration 39/1000 | Loss: 0.00000862
Iteration 40/1000 | Loss: 0.00000862
Iteration 41/1000 | Loss: 0.00000862
Iteration 42/1000 | Loss: 0.00000862
Iteration 43/1000 | Loss: 0.00000862
Iteration 44/1000 | Loss: 0.00000862
Iteration 45/1000 | Loss: 0.00000862
Iteration 46/1000 | Loss: 0.00000861
Iteration 47/1000 | Loss: 0.00000861
Iteration 48/1000 | Loss: 0.00000861
Iteration 49/1000 | Loss: 0.00000861
Iteration 50/1000 | Loss: 0.00000861
Iteration 51/1000 | Loss: 0.00000861
Iteration 52/1000 | Loss: 0.00000860
Iteration 53/1000 | Loss: 0.00000860
Iteration 54/1000 | Loss: 0.00000859
Iteration 55/1000 | Loss: 0.00000859
Iteration 56/1000 | Loss: 0.00000859
Iteration 57/1000 | Loss: 0.00000858
Iteration 58/1000 | Loss: 0.00000858
Iteration 59/1000 | Loss: 0.00000858
Iteration 60/1000 | Loss: 0.00000857
Iteration 61/1000 | Loss: 0.00000857
Iteration 62/1000 | Loss: 0.00000857
Iteration 63/1000 | Loss: 0.00000856
Iteration 64/1000 | Loss: 0.00000856
Iteration 65/1000 | Loss: 0.00000855
Iteration 66/1000 | Loss: 0.00000854
Iteration 67/1000 | Loss: 0.00000854
Iteration 68/1000 | Loss: 0.00000853
Iteration 69/1000 | Loss: 0.00000853
Iteration 70/1000 | Loss: 0.00000852
Iteration 71/1000 | Loss: 0.00000852
Iteration 72/1000 | Loss: 0.00000851
Iteration 73/1000 | Loss: 0.00000851
Iteration 74/1000 | Loss: 0.00000851
Iteration 75/1000 | Loss: 0.00000851
Iteration 76/1000 | Loss: 0.00000851
Iteration 77/1000 | Loss: 0.00000850
Iteration 78/1000 | Loss: 0.00000850
Iteration 79/1000 | Loss: 0.00000850
Iteration 80/1000 | Loss: 0.00000850
Iteration 81/1000 | Loss: 0.00000850
Iteration 82/1000 | Loss: 0.00000849
Iteration 83/1000 | Loss: 0.00000849
Iteration 84/1000 | Loss: 0.00000849
Iteration 85/1000 | Loss: 0.00000849
Iteration 86/1000 | Loss: 0.00000848
Iteration 87/1000 | Loss: 0.00000848
Iteration 88/1000 | Loss: 0.00000848
Iteration 89/1000 | Loss: 0.00000848
Iteration 90/1000 | Loss: 0.00000848
Iteration 91/1000 | Loss: 0.00000848
Iteration 92/1000 | Loss: 0.00000847
Iteration 93/1000 | Loss: 0.00000847
Iteration 94/1000 | Loss: 0.00000847
Iteration 95/1000 | Loss: 0.00000847
Iteration 96/1000 | Loss: 0.00000847
Iteration 97/1000 | Loss: 0.00000847
Iteration 98/1000 | Loss: 0.00000847
Iteration 99/1000 | Loss: 0.00000846
Iteration 100/1000 | Loss: 0.00000846
Iteration 101/1000 | Loss: 0.00000846
Iteration 102/1000 | Loss: 0.00000846
Iteration 103/1000 | Loss: 0.00000846
Iteration 104/1000 | Loss: 0.00000846
Iteration 105/1000 | Loss: 0.00000846
Iteration 106/1000 | Loss: 0.00000845
Iteration 107/1000 | Loss: 0.00000845
Iteration 108/1000 | Loss: 0.00000845
Iteration 109/1000 | Loss: 0.00000845
Iteration 110/1000 | Loss: 0.00000844
Iteration 111/1000 | Loss: 0.00000844
Iteration 112/1000 | Loss: 0.00000844
Iteration 113/1000 | Loss: 0.00000843
Iteration 114/1000 | Loss: 0.00000843
Iteration 115/1000 | Loss: 0.00000843
Iteration 116/1000 | Loss: 0.00000842
Iteration 117/1000 | Loss: 0.00000842
Iteration 118/1000 | Loss: 0.00000842
Iteration 119/1000 | Loss: 0.00000842
Iteration 120/1000 | Loss: 0.00000842
Iteration 121/1000 | Loss: 0.00000842
Iteration 122/1000 | Loss: 0.00000841
Iteration 123/1000 | Loss: 0.00000841
Iteration 124/1000 | Loss: 0.00000841
Iteration 125/1000 | Loss: 0.00000841
Iteration 126/1000 | Loss: 0.00000841
Iteration 127/1000 | Loss: 0.00000841
Iteration 128/1000 | Loss: 0.00000841
Iteration 129/1000 | Loss: 0.00000841
Iteration 130/1000 | Loss: 0.00000841
Iteration 131/1000 | Loss: 0.00000841
Iteration 132/1000 | Loss: 0.00000841
Iteration 133/1000 | Loss: 0.00000840
Iteration 134/1000 | Loss: 0.00000840
Iteration 135/1000 | Loss: 0.00000840
Iteration 136/1000 | Loss: 0.00000840
Iteration 137/1000 | Loss: 0.00000840
Iteration 138/1000 | Loss: 0.00000839
Iteration 139/1000 | Loss: 0.00000839
Iteration 140/1000 | Loss: 0.00000839
Iteration 141/1000 | Loss: 0.00000839
Iteration 142/1000 | Loss: 0.00000838
Iteration 143/1000 | Loss: 0.00000838
Iteration 144/1000 | Loss: 0.00000838
Iteration 145/1000 | Loss: 0.00000838
Iteration 146/1000 | Loss: 0.00000838
Iteration 147/1000 | Loss: 0.00000838
Iteration 148/1000 | Loss: 0.00000838
Iteration 149/1000 | Loss: 0.00000837
Iteration 150/1000 | Loss: 0.00000837
Iteration 151/1000 | Loss: 0.00000837
Iteration 152/1000 | Loss: 0.00000837
Iteration 153/1000 | Loss: 0.00000837
Iteration 154/1000 | Loss: 0.00000837
Iteration 155/1000 | Loss: 0.00000837
Iteration 156/1000 | Loss: 0.00000836
Iteration 157/1000 | Loss: 0.00000836
Iteration 158/1000 | Loss: 0.00000836
Iteration 159/1000 | Loss: 0.00000836
Iteration 160/1000 | Loss: 0.00000836
Iteration 161/1000 | Loss: 0.00000836
Iteration 162/1000 | Loss: 0.00000836
Iteration 163/1000 | Loss: 0.00000836
Iteration 164/1000 | Loss: 0.00000836
Iteration 165/1000 | Loss: 0.00000836
Iteration 166/1000 | Loss: 0.00000836
Iteration 167/1000 | Loss: 0.00000836
Iteration 168/1000 | Loss: 0.00000836
Iteration 169/1000 | Loss: 0.00000836
Iteration 170/1000 | Loss: 0.00000836
Iteration 171/1000 | Loss: 0.00000836
Iteration 172/1000 | Loss: 0.00000835
Iteration 173/1000 | Loss: 0.00000835
Iteration 174/1000 | Loss: 0.00000835
Iteration 175/1000 | Loss: 0.00000835
Iteration 176/1000 | Loss: 0.00000835
Iteration 177/1000 | Loss: 0.00000835
Iteration 178/1000 | Loss: 0.00000835
Iteration 179/1000 | Loss: 0.00000835
Iteration 180/1000 | Loss: 0.00000835
Iteration 181/1000 | Loss: 0.00000835
Iteration 182/1000 | Loss: 0.00000834
Iteration 183/1000 | Loss: 0.00000834
Iteration 184/1000 | Loss: 0.00000834
Iteration 185/1000 | Loss: 0.00000834
Iteration 186/1000 | Loss: 0.00000834
Iteration 187/1000 | Loss: 0.00000834
Iteration 188/1000 | Loss: 0.00000834
Iteration 189/1000 | Loss: 0.00000834
Iteration 190/1000 | Loss: 0.00000834
Iteration 191/1000 | Loss: 0.00000834
Iteration 192/1000 | Loss: 0.00000834
Iteration 193/1000 | Loss: 0.00000834
Iteration 194/1000 | Loss: 0.00000834
Iteration 195/1000 | Loss: 0.00000834
Iteration 196/1000 | Loss: 0.00000834
Iteration 197/1000 | Loss: 0.00000833
Iteration 198/1000 | Loss: 0.00000833
Iteration 199/1000 | Loss: 0.00000833
Iteration 200/1000 | Loss: 0.00000833
Iteration 201/1000 | Loss: 0.00000833
Iteration 202/1000 | Loss: 0.00000833
Iteration 203/1000 | Loss: 0.00000833
Iteration 204/1000 | Loss: 0.00000833
Iteration 205/1000 | Loss: 0.00000833
Iteration 206/1000 | Loss: 0.00000833
Iteration 207/1000 | Loss: 0.00000833
Iteration 208/1000 | Loss: 0.00000833
Iteration 209/1000 | Loss: 0.00000833
Iteration 210/1000 | Loss: 0.00000832
Iteration 211/1000 | Loss: 0.00000832
Iteration 212/1000 | Loss: 0.00000832
Iteration 213/1000 | Loss: 0.00000832
Iteration 214/1000 | Loss: 0.00000832
Iteration 215/1000 | Loss: 0.00000832
Iteration 216/1000 | Loss: 0.00000832
Iteration 217/1000 | Loss: 0.00000832
Iteration 218/1000 | Loss: 0.00000832
Iteration 219/1000 | Loss: 0.00000832
Iteration 220/1000 | Loss: 0.00000832
Iteration 221/1000 | Loss: 0.00000831
Iteration 222/1000 | Loss: 0.00000831
Iteration 223/1000 | Loss: 0.00000831
Iteration 224/1000 | Loss: 0.00000831
Iteration 225/1000 | Loss: 0.00000831
Iteration 226/1000 | Loss: 0.00000831
Iteration 227/1000 | Loss: 0.00000831
Iteration 228/1000 | Loss: 0.00000831
Iteration 229/1000 | Loss: 0.00000831
Iteration 230/1000 | Loss: 0.00000831
Iteration 231/1000 | Loss: 0.00000831
Iteration 232/1000 | Loss: 0.00000831
Iteration 233/1000 | Loss: 0.00000831
Iteration 234/1000 | Loss: 0.00000831
Iteration 235/1000 | Loss: 0.00000831
Iteration 236/1000 | Loss: 0.00000831
Iteration 237/1000 | Loss: 0.00000831
Iteration 238/1000 | Loss: 0.00000831
Iteration 239/1000 | Loss: 0.00000831
Iteration 240/1000 | Loss: 0.00000831
Iteration 241/1000 | Loss: 0.00000831
Iteration 242/1000 | Loss: 0.00000831
Iteration 243/1000 | Loss: 0.00000831
Iteration 244/1000 | Loss: 0.00000831
Iteration 245/1000 | Loss: 0.00000831
Iteration 246/1000 | Loss: 0.00000831
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [8.310114026244264e-06, 8.310114026244264e-06, 8.310114026244264e-06, 8.310114026244264e-06, 8.310114026244264e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.310114026244264e-06

Optimization complete. Final v2v error: 2.4630751609802246 mm

Highest mean error: 2.6441266536712646 mm for frame 41

Lowest mean error: 2.316171884536743 mm for frame 165

Saving results

Total time: 40.85789394378662
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788362
Iteration 2/25 | Loss: 0.00125517
Iteration 3/25 | Loss: 0.00111939
Iteration 4/25 | Loss: 0.00110754
Iteration 5/25 | Loss: 0.00110618
Iteration 6/25 | Loss: 0.00110618
Iteration 7/25 | Loss: 0.00110618
Iteration 8/25 | Loss: 0.00110618
Iteration 9/25 | Loss: 0.00110618
Iteration 10/25 | Loss: 0.00110618
Iteration 11/25 | Loss: 0.00110618
Iteration 12/25 | Loss: 0.00110618
Iteration 13/25 | Loss: 0.00110618
Iteration 14/25 | Loss: 0.00110618
Iteration 15/25 | Loss: 0.00110618
Iteration 16/25 | Loss: 0.00110618
Iteration 17/25 | Loss: 0.00110618
Iteration 18/25 | Loss: 0.00110618
Iteration 19/25 | Loss: 0.00110618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011061768746003509, 0.0011061768746003509, 0.0011061768746003509, 0.0011061768746003509, 0.0011061768746003509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011061768746003509

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.41360259
Iteration 2/25 | Loss: 0.00074427
Iteration 3/25 | Loss: 0.00074427
Iteration 4/25 | Loss: 0.00074427
Iteration 5/25 | Loss: 0.00074427
Iteration 6/25 | Loss: 0.00074427
Iteration 7/25 | Loss: 0.00074427
Iteration 8/25 | Loss: 0.00074427
Iteration 9/25 | Loss: 0.00074427
Iteration 10/25 | Loss: 0.00074427
Iteration 11/25 | Loss: 0.00074427
Iteration 12/25 | Loss: 0.00074427
Iteration 13/25 | Loss: 0.00074427
Iteration 14/25 | Loss: 0.00074427
Iteration 15/25 | Loss: 0.00074427
Iteration 16/25 | Loss: 0.00074427
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007442699279636145, 0.0007442699279636145, 0.0007442699279636145, 0.0007442699279636145, 0.0007442699279636145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007442699279636145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074427
Iteration 2/1000 | Loss: 0.00002043
Iteration 3/1000 | Loss: 0.00001445
Iteration 4/1000 | Loss: 0.00001303
Iteration 5/1000 | Loss: 0.00001218
Iteration 6/1000 | Loss: 0.00001160
Iteration 7/1000 | Loss: 0.00001127
Iteration 8/1000 | Loss: 0.00001121
Iteration 9/1000 | Loss: 0.00001101
Iteration 10/1000 | Loss: 0.00001081
Iteration 11/1000 | Loss: 0.00001078
Iteration 12/1000 | Loss: 0.00001071
Iteration 13/1000 | Loss: 0.00001070
Iteration 14/1000 | Loss: 0.00001070
Iteration 15/1000 | Loss: 0.00001069
Iteration 16/1000 | Loss: 0.00001060
Iteration 17/1000 | Loss: 0.00001053
Iteration 18/1000 | Loss: 0.00001053
Iteration 19/1000 | Loss: 0.00001051
Iteration 20/1000 | Loss: 0.00001050
Iteration 21/1000 | Loss: 0.00001046
Iteration 22/1000 | Loss: 0.00001046
Iteration 23/1000 | Loss: 0.00001045
Iteration 24/1000 | Loss: 0.00001045
Iteration 25/1000 | Loss: 0.00001044
Iteration 26/1000 | Loss: 0.00001044
Iteration 27/1000 | Loss: 0.00001043
Iteration 28/1000 | Loss: 0.00001042
Iteration 29/1000 | Loss: 0.00001041
Iteration 30/1000 | Loss: 0.00001040
Iteration 31/1000 | Loss: 0.00001039
Iteration 32/1000 | Loss: 0.00001035
Iteration 33/1000 | Loss: 0.00001035
Iteration 34/1000 | Loss: 0.00001033
Iteration 35/1000 | Loss: 0.00001033
Iteration 36/1000 | Loss: 0.00001032
Iteration 37/1000 | Loss: 0.00001032
Iteration 38/1000 | Loss: 0.00001032
Iteration 39/1000 | Loss: 0.00001032
Iteration 40/1000 | Loss: 0.00001032
Iteration 41/1000 | Loss: 0.00001032
Iteration 42/1000 | Loss: 0.00001031
Iteration 43/1000 | Loss: 0.00001031
Iteration 44/1000 | Loss: 0.00001030
Iteration 45/1000 | Loss: 0.00001030
Iteration 46/1000 | Loss: 0.00001030
Iteration 47/1000 | Loss: 0.00001029
Iteration 48/1000 | Loss: 0.00001029
Iteration 49/1000 | Loss: 0.00001029
Iteration 50/1000 | Loss: 0.00001029
Iteration 51/1000 | Loss: 0.00001028
Iteration 52/1000 | Loss: 0.00001028
Iteration 53/1000 | Loss: 0.00001028
Iteration 54/1000 | Loss: 0.00001028
Iteration 55/1000 | Loss: 0.00001028
Iteration 56/1000 | Loss: 0.00001027
Iteration 57/1000 | Loss: 0.00001027
Iteration 58/1000 | Loss: 0.00001027
Iteration 59/1000 | Loss: 0.00001026
Iteration 60/1000 | Loss: 0.00001025
Iteration 61/1000 | Loss: 0.00001024
Iteration 62/1000 | Loss: 0.00001024
Iteration 63/1000 | Loss: 0.00001023
Iteration 64/1000 | Loss: 0.00001023
Iteration 65/1000 | Loss: 0.00001022
Iteration 66/1000 | Loss: 0.00001022
Iteration 67/1000 | Loss: 0.00001021
Iteration 68/1000 | Loss: 0.00001021
Iteration 69/1000 | Loss: 0.00001020
Iteration 70/1000 | Loss: 0.00001020
Iteration 71/1000 | Loss: 0.00001020
Iteration 72/1000 | Loss: 0.00001020
Iteration 73/1000 | Loss: 0.00001020
Iteration 74/1000 | Loss: 0.00001020
Iteration 75/1000 | Loss: 0.00001020
Iteration 76/1000 | Loss: 0.00001020
Iteration 77/1000 | Loss: 0.00001020
Iteration 78/1000 | Loss: 0.00001020
Iteration 79/1000 | Loss: 0.00001020
Iteration 80/1000 | Loss: 0.00001020
Iteration 81/1000 | Loss: 0.00001020
Iteration 82/1000 | Loss: 0.00001020
Iteration 83/1000 | Loss: 0.00001020
Iteration 84/1000 | Loss: 0.00001020
Iteration 85/1000 | Loss: 0.00001020
Iteration 86/1000 | Loss: 0.00001020
Iteration 87/1000 | Loss: 0.00001020
Iteration 88/1000 | Loss: 0.00001020
Iteration 89/1000 | Loss: 0.00001020
Iteration 90/1000 | Loss: 0.00001020
Iteration 91/1000 | Loss: 0.00001020
Iteration 92/1000 | Loss: 0.00001020
Iteration 93/1000 | Loss: 0.00001020
Iteration 94/1000 | Loss: 0.00001020
Iteration 95/1000 | Loss: 0.00001020
Iteration 96/1000 | Loss: 0.00001020
Iteration 97/1000 | Loss: 0.00001020
Iteration 98/1000 | Loss: 0.00001020
Iteration 99/1000 | Loss: 0.00001020
Iteration 100/1000 | Loss: 0.00001020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.0195632057730108e-05, 1.0195632057730108e-05, 1.0195632057730108e-05, 1.0195632057730108e-05, 1.0195632057730108e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0195632057730108e-05

Optimization complete. Final v2v error: 2.661027669906616 mm

Highest mean error: 3.1179585456848145 mm for frame 81

Lowest mean error: 2.267951726913452 mm for frame 47

Saving results

Total time: 33.04462742805481
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398196
Iteration 2/25 | Loss: 0.00130638
Iteration 3/25 | Loss: 0.00109610
Iteration 4/25 | Loss: 0.00107930
Iteration 5/25 | Loss: 0.00107704
Iteration 6/25 | Loss: 0.00107641
Iteration 7/25 | Loss: 0.00107641
Iteration 8/25 | Loss: 0.00107641
Iteration 9/25 | Loss: 0.00107641
Iteration 10/25 | Loss: 0.00107641
Iteration 11/25 | Loss: 0.00107641
Iteration 12/25 | Loss: 0.00107641
Iteration 13/25 | Loss: 0.00107641
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0010764056351035833, 0.0010764056351035833, 0.0010764056351035833, 0.0010764056351035833, 0.0010764056351035833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010764056351035833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34738111
Iteration 2/25 | Loss: 0.00060249
Iteration 3/25 | Loss: 0.00060248
Iteration 4/25 | Loss: 0.00060248
Iteration 5/25 | Loss: 0.00060248
Iteration 6/25 | Loss: 0.00060248
Iteration 7/25 | Loss: 0.00060248
Iteration 8/25 | Loss: 0.00060248
Iteration 9/25 | Loss: 0.00060248
Iteration 10/25 | Loss: 0.00060248
Iteration 11/25 | Loss: 0.00060248
Iteration 12/25 | Loss: 0.00060248
Iteration 13/25 | Loss: 0.00060248
Iteration 14/25 | Loss: 0.00060248
Iteration 15/25 | Loss: 0.00060248
Iteration 16/25 | Loss: 0.00060248
Iteration 17/25 | Loss: 0.00060248
Iteration 18/25 | Loss: 0.00060248
Iteration 19/25 | Loss: 0.00060248
Iteration 20/25 | Loss: 0.00060248
Iteration 21/25 | Loss: 0.00060248
Iteration 22/25 | Loss: 0.00060248
Iteration 23/25 | Loss: 0.00060248
Iteration 24/25 | Loss: 0.00060248
Iteration 25/25 | Loss: 0.00060248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00060248
Iteration 2/1000 | Loss: 0.00002374
Iteration 3/1000 | Loss: 0.00001555
Iteration 4/1000 | Loss: 0.00001403
Iteration 5/1000 | Loss: 0.00001330
Iteration 6/1000 | Loss: 0.00001267
Iteration 7/1000 | Loss: 0.00001225
Iteration 8/1000 | Loss: 0.00001214
Iteration 9/1000 | Loss: 0.00001186
Iteration 10/1000 | Loss: 0.00001176
Iteration 11/1000 | Loss: 0.00001168
Iteration 12/1000 | Loss: 0.00001165
Iteration 13/1000 | Loss: 0.00001164
Iteration 14/1000 | Loss: 0.00001158
Iteration 15/1000 | Loss: 0.00001154
Iteration 16/1000 | Loss: 0.00001154
Iteration 17/1000 | Loss: 0.00001153
Iteration 18/1000 | Loss: 0.00001152
Iteration 19/1000 | Loss: 0.00001151
Iteration 20/1000 | Loss: 0.00001150
Iteration 21/1000 | Loss: 0.00001150
Iteration 22/1000 | Loss: 0.00001149
Iteration 23/1000 | Loss: 0.00001149
Iteration 24/1000 | Loss: 0.00001148
Iteration 25/1000 | Loss: 0.00001147
Iteration 26/1000 | Loss: 0.00001146
Iteration 27/1000 | Loss: 0.00001145
Iteration 28/1000 | Loss: 0.00001144
Iteration 29/1000 | Loss: 0.00001144
Iteration 30/1000 | Loss: 0.00001143
Iteration 31/1000 | Loss: 0.00001143
Iteration 32/1000 | Loss: 0.00001142
Iteration 33/1000 | Loss: 0.00001142
Iteration 34/1000 | Loss: 0.00001138
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001137
Iteration 37/1000 | Loss: 0.00001137
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001137
Iteration 41/1000 | Loss: 0.00001137
Iteration 42/1000 | Loss: 0.00001137
Iteration 43/1000 | Loss: 0.00001137
Iteration 44/1000 | Loss: 0.00001136
Iteration 45/1000 | Loss: 0.00001134
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001133
Iteration 48/1000 | Loss: 0.00001133
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001133
Iteration 51/1000 | Loss: 0.00001133
Iteration 52/1000 | Loss: 0.00001133
Iteration 53/1000 | Loss: 0.00001133
Iteration 54/1000 | Loss: 0.00001133
Iteration 55/1000 | Loss: 0.00001133
Iteration 56/1000 | Loss: 0.00001133
Iteration 57/1000 | Loss: 0.00001133
Iteration 58/1000 | Loss: 0.00001132
Iteration 59/1000 | Loss: 0.00001129
Iteration 60/1000 | Loss: 0.00001129
Iteration 61/1000 | Loss: 0.00001128
Iteration 62/1000 | Loss: 0.00001128
Iteration 63/1000 | Loss: 0.00001128
Iteration 64/1000 | Loss: 0.00001127
Iteration 65/1000 | Loss: 0.00001127
Iteration 66/1000 | Loss: 0.00001127
Iteration 67/1000 | Loss: 0.00001127
Iteration 68/1000 | Loss: 0.00001127
Iteration 69/1000 | Loss: 0.00001127
Iteration 70/1000 | Loss: 0.00001127
Iteration 71/1000 | Loss: 0.00001126
Iteration 72/1000 | Loss: 0.00001126
Iteration 73/1000 | Loss: 0.00001126
Iteration 74/1000 | Loss: 0.00001126
Iteration 75/1000 | Loss: 0.00001126
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001126
Iteration 78/1000 | Loss: 0.00001126
Iteration 79/1000 | Loss: 0.00001126
Iteration 80/1000 | Loss: 0.00001126
Iteration 81/1000 | Loss: 0.00001126
Iteration 82/1000 | Loss: 0.00001126
Iteration 83/1000 | Loss: 0.00001126
Iteration 84/1000 | Loss: 0.00001126
Iteration 85/1000 | Loss: 0.00001126
Iteration 86/1000 | Loss: 0.00001126
Iteration 87/1000 | Loss: 0.00001126
Iteration 88/1000 | Loss: 0.00001126
Iteration 89/1000 | Loss: 0.00001126
Iteration 90/1000 | Loss: 0.00001126
Iteration 91/1000 | Loss: 0.00001126
Iteration 92/1000 | Loss: 0.00001126
Iteration 93/1000 | Loss: 0.00001126
Iteration 94/1000 | Loss: 0.00001126
Iteration 95/1000 | Loss: 0.00001126
Iteration 96/1000 | Loss: 0.00001126
Iteration 97/1000 | Loss: 0.00001126
Iteration 98/1000 | Loss: 0.00001126
Iteration 99/1000 | Loss: 0.00001126
Iteration 100/1000 | Loss: 0.00001126
Iteration 101/1000 | Loss: 0.00001126
Iteration 102/1000 | Loss: 0.00001126
Iteration 103/1000 | Loss: 0.00001126
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 103. Stopping optimization.
Last 5 losses: [1.1255781828367617e-05, 1.1255781828367617e-05, 1.1255781828367617e-05, 1.1255781828367617e-05, 1.1255781828367617e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1255781828367617e-05

Optimization complete. Final v2v error: 2.9126157760620117 mm

Highest mean error: 3.0504114627838135 mm for frame 69

Lowest mean error: 2.7773592472076416 mm for frame 128

Saving results

Total time: 29.757118463516235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386968
Iteration 2/25 | Loss: 0.00122357
Iteration 3/25 | Loss: 0.00112192
Iteration 4/25 | Loss: 0.00111084
Iteration 5/25 | Loss: 0.00110837
Iteration 6/25 | Loss: 0.00110805
Iteration 7/25 | Loss: 0.00110805
Iteration 8/25 | Loss: 0.00110805
Iteration 9/25 | Loss: 0.00110805
Iteration 10/25 | Loss: 0.00110805
Iteration 11/25 | Loss: 0.00110805
Iteration 12/25 | Loss: 0.00110805
Iteration 13/25 | Loss: 0.00110805
Iteration 14/25 | Loss: 0.00110805
Iteration 15/25 | Loss: 0.00110805
Iteration 16/25 | Loss: 0.00110805
Iteration 17/25 | Loss: 0.00110805
Iteration 18/25 | Loss: 0.00110805
Iteration 19/25 | Loss: 0.00110805
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011080520926043391, 0.0011080520926043391, 0.0011080520926043391, 0.0011080520926043391, 0.0011080520926043391]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011080520926043391

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36022329
Iteration 2/25 | Loss: 0.00087350
Iteration 3/25 | Loss: 0.00087350
Iteration 4/25 | Loss: 0.00087350
Iteration 5/25 | Loss: 0.00087350
Iteration 6/25 | Loss: 0.00087350
Iteration 7/25 | Loss: 0.00087350
Iteration 8/25 | Loss: 0.00087350
Iteration 9/25 | Loss: 0.00087350
Iteration 10/25 | Loss: 0.00087350
Iteration 11/25 | Loss: 0.00087350
Iteration 12/25 | Loss: 0.00087350
Iteration 13/25 | Loss: 0.00087350
Iteration 14/25 | Loss: 0.00087350
Iteration 15/25 | Loss: 0.00087350
Iteration 16/25 | Loss: 0.00087350
Iteration 17/25 | Loss: 0.00087350
Iteration 18/25 | Loss: 0.00087350
Iteration 19/25 | Loss: 0.00087350
Iteration 20/25 | Loss: 0.00087350
Iteration 21/25 | Loss: 0.00087350
Iteration 22/25 | Loss: 0.00087350
Iteration 23/25 | Loss: 0.00087350
Iteration 24/25 | Loss: 0.00087350
Iteration 25/25 | Loss: 0.00087350
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0008734991424717009, 0.0008734991424717009, 0.0008734991424717009, 0.0008734991424717009, 0.0008734991424717009]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008734991424717009

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087350
Iteration 2/1000 | Loss: 0.00003008
Iteration 3/1000 | Loss: 0.00001836
Iteration 4/1000 | Loss: 0.00001638
Iteration 5/1000 | Loss: 0.00001524
Iteration 6/1000 | Loss: 0.00001468
Iteration 7/1000 | Loss: 0.00001405
Iteration 8/1000 | Loss: 0.00001370
Iteration 9/1000 | Loss: 0.00001343
Iteration 10/1000 | Loss: 0.00001320
Iteration 11/1000 | Loss: 0.00001320
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001299
Iteration 14/1000 | Loss: 0.00001291
Iteration 15/1000 | Loss: 0.00001289
Iteration 16/1000 | Loss: 0.00001289
Iteration 17/1000 | Loss: 0.00001289
Iteration 18/1000 | Loss: 0.00001289
Iteration 19/1000 | Loss: 0.00001288
Iteration 20/1000 | Loss: 0.00001283
Iteration 21/1000 | Loss: 0.00001283
Iteration 22/1000 | Loss: 0.00001283
Iteration 23/1000 | Loss: 0.00001282
Iteration 24/1000 | Loss: 0.00001282
Iteration 25/1000 | Loss: 0.00001281
Iteration 26/1000 | Loss: 0.00001280
Iteration 27/1000 | Loss: 0.00001280
Iteration 28/1000 | Loss: 0.00001279
Iteration 29/1000 | Loss: 0.00001279
Iteration 30/1000 | Loss: 0.00001278
Iteration 31/1000 | Loss: 0.00001277
Iteration 32/1000 | Loss: 0.00001277
Iteration 33/1000 | Loss: 0.00001275
Iteration 34/1000 | Loss: 0.00001274
Iteration 35/1000 | Loss: 0.00001274
Iteration 36/1000 | Loss: 0.00001274
Iteration 37/1000 | Loss: 0.00001273
Iteration 38/1000 | Loss: 0.00001273
Iteration 39/1000 | Loss: 0.00001273
Iteration 40/1000 | Loss: 0.00001273
Iteration 41/1000 | Loss: 0.00001273
Iteration 42/1000 | Loss: 0.00001272
Iteration 43/1000 | Loss: 0.00001271
Iteration 44/1000 | Loss: 0.00001270
Iteration 45/1000 | Loss: 0.00001269
Iteration 46/1000 | Loss: 0.00001269
Iteration 47/1000 | Loss: 0.00001269
Iteration 48/1000 | Loss: 0.00001269
Iteration 49/1000 | Loss: 0.00001268
Iteration 50/1000 | Loss: 0.00001268
Iteration 51/1000 | Loss: 0.00001268
Iteration 52/1000 | Loss: 0.00001268
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001268
Iteration 55/1000 | Loss: 0.00001268
Iteration 56/1000 | Loss: 0.00001267
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001265
Iteration 63/1000 | Loss: 0.00001265
Iteration 64/1000 | Loss: 0.00001265
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001265
Iteration 67/1000 | Loss: 0.00001265
Iteration 68/1000 | Loss: 0.00001265
Iteration 69/1000 | Loss: 0.00001264
Iteration 70/1000 | Loss: 0.00001264
Iteration 71/1000 | Loss: 0.00001264
Iteration 72/1000 | Loss: 0.00001264
Iteration 73/1000 | Loss: 0.00001262
Iteration 74/1000 | Loss: 0.00001262
Iteration 75/1000 | Loss: 0.00001261
Iteration 76/1000 | Loss: 0.00001261
Iteration 77/1000 | Loss: 0.00001261
Iteration 78/1000 | Loss: 0.00001261
Iteration 79/1000 | Loss: 0.00001261
Iteration 80/1000 | Loss: 0.00001261
Iteration 81/1000 | Loss: 0.00001261
Iteration 82/1000 | Loss: 0.00001261
Iteration 83/1000 | Loss: 0.00001261
Iteration 84/1000 | Loss: 0.00001260
Iteration 85/1000 | Loss: 0.00001260
Iteration 86/1000 | Loss: 0.00001260
Iteration 87/1000 | Loss: 0.00001259
Iteration 88/1000 | Loss: 0.00001259
Iteration 89/1000 | Loss: 0.00001259
Iteration 90/1000 | Loss: 0.00001258
Iteration 91/1000 | Loss: 0.00001258
Iteration 92/1000 | Loss: 0.00001258
Iteration 93/1000 | Loss: 0.00001258
Iteration 94/1000 | Loss: 0.00001258
Iteration 95/1000 | Loss: 0.00001258
Iteration 96/1000 | Loss: 0.00001258
Iteration 97/1000 | Loss: 0.00001258
Iteration 98/1000 | Loss: 0.00001258
Iteration 99/1000 | Loss: 0.00001257
Iteration 100/1000 | Loss: 0.00001257
Iteration 101/1000 | Loss: 0.00001257
Iteration 102/1000 | Loss: 0.00001257
Iteration 103/1000 | Loss: 0.00001257
Iteration 104/1000 | Loss: 0.00001257
Iteration 105/1000 | Loss: 0.00001257
Iteration 106/1000 | Loss: 0.00001257
Iteration 107/1000 | Loss: 0.00001256
Iteration 108/1000 | Loss: 0.00001256
Iteration 109/1000 | Loss: 0.00001256
Iteration 110/1000 | Loss: 0.00001256
Iteration 111/1000 | Loss: 0.00001256
Iteration 112/1000 | Loss: 0.00001256
Iteration 113/1000 | Loss: 0.00001256
Iteration 114/1000 | Loss: 0.00001256
Iteration 115/1000 | Loss: 0.00001255
Iteration 116/1000 | Loss: 0.00001255
Iteration 117/1000 | Loss: 0.00001255
Iteration 118/1000 | Loss: 0.00001255
Iteration 119/1000 | Loss: 0.00001255
Iteration 120/1000 | Loss: 0.00001255
Iteration 121/1000 | Loss: 0.00001255
Iteration 122/1000 | Loss: 0.00001255
Iteration 123/1000 | Loss: 0.00001255
Iteration 124/1000 | Loss: 0.00001254
Iteration 125/1000 | Loss: 0.00001254
Iteration 126/1000 | Loss: 0.00001254
Iteration 127/1000 | Loss: 0.00001253
Iteration 128/1000 | Loss: 0.00001253
Iteration 129/1000 | Loss: 0.00001253
Iteration 130/1000 | Loss: 0.00001253
Iteration 131/1000 | Loss: 0.00001253
Iteration 132/1000 | Loss: 0.00001253
Iteration 133/1000 | Loss: 0.00001253
Iteration 134/1000 | Loss: 0.00001253
Iteration 135/1000 | Loss: 0.00001252
Iteration 136/1000 | Loss: 0.00001252
Iteration 137/1000 | Loss: 0.00001251
Iteration 138/1000 | Loss: 0.00001251
Iteration 139/1000 | Loss: 0.00001251
Iteration 140/1000 | Loss: 0.00001250
Iteration 141/1000 | Loss: 0.00001250
Iteration 142/1000 | Loss: 0.00001250
Iteration 143/1000 | Loss: 0.00001250
Iteration 144/1000 | Loss: 0.00001250
Iteration 145/1000 | Loss: 0.00001249
Iteration 146/1000 | Loss: 0.00001249
Iteration 147/1000 | Loss: 0.00001249
Iteration 148/1000 | Loss: 0.00001249
Iteration 149/1000 | Loss: 0.00001249
Iteration 150/1000 | Loss: 0.00001249
Iteration 151/1000 | Loss: 0.00001249
Iteration 152/1000 | Loss: 0.00001249
Iteration 153/1000 | Loss: 0.00001248
Iteration 154/1000 | Loss: 0.00001248
Iteration 155/1000 | Loss: 0.00001248
Iteration 156/1000 | Loss: 0.00001248
Iteration 157/1000 | Loss: 0.00001248
Iteration 158/1000 | Loss: 0.00001248
Iteration 159/1000 | Loss: 0.00001247
Iteration 160/1000 | Loss: 0.00001247
Iteration 161/1000 | Loss: 0.00001247
Iteration 162/1000 | Loss: 0.00001247
Iteration 163/1000 | Loss: 0.00001247
Iteration 164/1000 | Loss: 0.00001247
Iteration 165/1000 | Loss: 0.00001247
Iteration 166/1000 | Loss: 0.00001247
Iteration 167/1000 | Loss: 0.00001247
Iteration 168/1000 | Loss: 0.00001247
Iteration 169/1000 | Loss: 0.00001247
Iteration 170/1000 | Loss: 0.00001247
Iteration 171/1000 | Loss: 0.00001247
Iteration 172/1000 | Loss: 0.00001247
Iteration 173/1000 | Loss: 0.00001246
Iteration 174/1000 | Loss: 0.00001246
Iteration 175/1000 | Loss: 0.00001246
Iteration 176/1000 | Loss: 0.00001246
Iteration 177/1000 | Loss: 0.00001246
Iteration 178/1000 | Loss: 0.00001246
Iteration 179/1000 | Loss: 0.00001246
Iteration 180/1000 | Loss: 0.00001246
Iteration 181/1000 | Loss: 0.00001246
Iteration 182/1000 | Loss: 0.00001245
Iteration 183/1000 | Loss: 0.00001245
Iteration 184/1000 | Loss: 0.00001245
Iteration 185/1000 | Loss: 0.00001245
Iteration 186/1000 | Loss: 0.00001245
Iteration 187/1000 | Loss: 0.00001245
Iteration 188/1000 | Loss: 0.00001245
Iteration 189/1000 | Loss: 0.00001245
Iteration 190/1000 | Loss: 0.00001245
Iteration 191/1000 | Loss: 0.00001245
Iteration 192/1000 | Loss: 0.00001244
Iteration 193/1000 | Loss: 0.00001244
Iteration 194/1000 | Loss: 0.00001244
Iteration 195/1000 | Loss: 0.00001244
Iteration 196/1000 | Loss: 0.00001244
Iteration 197/1000 | Loss: 0.00001244
Iteration 198/1000 | Loss: 0.00001244
Iteration 199/1000 | Loss: 0.00001244
Iteration 200/1000 | Loss: 0.00001244
Iteration 201/1000 | Loss: 0.00001244
Iteration 202/1000 | Loss: 0.00001244
Iteration 203/1000 | Loss: 0.00001243
Iteration 204/1000 | Loss: 0.00001243
Iteration 205/1000 | Loss: 0.00001243
Iteration 206/1000 | Loss: 0.00001243
Iteration 207/1000 | Loss: 0.00001243
Iteration 208/1000 | Loss: 0.00001243
Iteration 209/1000 | Loss: 0.00001243
Iteration 210/1000 | Loss: 0.00001243
Iteration 211/1000 | Loss: 0.00001243
Iteration 212/1000 | Loss: 0.00001243
Iteration 213/1000 | Loss: 0.00001243
Iteration 214/1000 | Loss: 0.00001243
Iteration 215/1000 | Loss: 0.00001243
Iteration 216/1000 | Loss: 0.00001242
Iteration 217/1000 | Loss: 0.00001242
Iteration 218/1000 | Loss: 0.00001242
Iteration 219/1000 | Loss: 0.00001242
Iteration 220/1000 | Loss: 0.00001242
Iteration 221/1000 | Loss: 0.00001242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 221. Stopping optimization.
Last 5 losses: [1.242481084773317e-05, 1.242481084773317e-05, 1.242481084773317e-05, 1.242481084773317e-05, 1.242481084773317e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.242481084773317e-05

Optimization complete. Final v2v error: 2.908470392227173 mm

Highest mean error: 3.3450140953063965 mm for frame 88

Lowest mean error: 2.409005641937256 mm for frame 16

Saving results

Total time: 41.04016065597534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00451709
Iteration 2/25 | Loss: 0.00126779
Iteration 3/25 | Loss: 0.00113329
Iteration 4/25 | Loss: 0.00112565
Iteration 5/25 | Loss: 0.00112536
Iteration 6/25 | Loss: 0.00112536
Iteration 7/25 | Loss: 0.00112536
Iteration 8/25 | Loss: 0.00112536
Iteration 9/25 | Loss: 0.00112536
Iteration 10/25 | Loss: 0.00112536
Iteration 11/25 | Loss: 0.00112536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011253594420850277, 0.0011253594420850277, 0.0011253594420850277, 0.0011253594420850277, 0.0011253594420850277]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011253594420850277

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33489847
Iteration 2/25 | Loss: 0.00052674
Iteration 3/25 | Loss: 0.00052674
Iteration 4/25 | Loss: 0.00052674
Iteration 5/25 | Loss: 0.00052674
Iteration 6/25 | Loss: 0.00052674
Iteration 7/25 | Loss: 0.00052674
Iteration 8/25 | Loss: 0.00052674
Iteration 9/25 | Loss: 0.00052674
Iteration 10/25 | Loss: 0.00052674
Iteration 11/25 | Loss: 0.00052674
Iteration 12/25 | Loss: 0.00052674
Iteration 13/25 | Loss: 0.00052674
Iteration 14/25 | Loss: 0.00052674
Iteration 15/25 | Loss: 0.00052674
Iteration 16/25 | Loss: 0.00052674
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0005267407395876944, 0.0005267407395876944, 0.0005267407395876944, 0.0005267407395876944, 0.0005267407395876944]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005267407395876944

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00052674
Iteration 2/1000 | Loss: 0.00003361
Iteration 3/1000 | Loss: 0.00002370
Iteration 4/1000 | Loss: 0.00002109
Iteration 5/1000 | Loss: 0.00002014
Iteration 6/1000 | Loss: 0.00001920
Iteration 7/1000 | Loss: 0.00001839
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001735
Iteration 10/1000 | Loss: 0.00001714
Iteration 11/1000 | Loss: 0.00001703
Iteration 12/1000 | Loss: 0.00001701
Iteration 13/1000 | Loss: 0.00001695
Iteration 14/1000 | Loss: 0.00001690
Iteration 15/1000 | Loss: 0.00001690
Iteration 16/1000 | Loss: 0.00001689
Iteration 17/1000 | Loss: 0.00001689
Iteration 18/1000 | Loss: 0.00001688
Iteration 19/1000 | Loss: 0.00001688
Iteration 20/1000 | Loss: 0.00001687
Iteration 21/1000 | Loss: 0.00001686
Iteration 22/1000 | Loss: 0.00001686
Iteration 23/1000 | Loss: 0.00001686
Iteration 24/1000 | Loss: 0.00001686
Iteration 25/1000 | Loss: 0.00001686
Iteration 26/1000 | Loss: 0.00001686
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00001686
Iteration 29/1000 | Loss: 0.00001685
Iteration 30/1000 | Loss: 0.00001685
Iteration 31/1000 | Loss: 0.00001684
Iteration 32/1000 | Loss: 0.00001684
Iteration 33/1000 | Loss: 0.00001684
Iteration 34/1000 | Loss: 0.00001684
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001683
Iteration 37/1000 | Loss: 0.00001683
Iteration 38/1000 | Loss: 0.00001682
Iteration 39/1000 | Loss: 0.00001682
Iteration 40/1000 | Loss: 0.00001682
Iteration 41/1000 | Loss: 0.00001682
Iteration 42/1000 | Loss: 0.00001681
Iteration 43/1000 | Loss: 0.00001681
Iteration 44/1000 | Loss: 0.00001681
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001680
Iteration 47/1000 | Loss: 0.00001679
Iteration 48/1000 | Loss: 0.00001678
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001676
Iteration 51/1000 | Loss: 0.00001676
Iteration 52/1000 | Loss: 0.00001676
Iteration 53/1000 | Loss: 0.00001676
Iteration 54/1000 | Loss: 0.00001676
Iteration 55/1000 | Loss: 0.00001676
Iteration 56/1000 | Loss: 0.00001676
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001674
Iteration 64/1000 | Loss: 0.00001674
Iteration 65/1000 | Loss: 0.00001674
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001673
Iteration 68/1000 | Loss: 0.00001673
Iteration 69/1000 | Loss: 0.00001673
Iteration 70/1000 | Loss: 0.00001672
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001672
Iteration 73/1000 | Loss: 0.00001671
Iteration 74/1000 | Loss: 0.00001671
Iteration 75/1000 | Loss: 0.00001670
Iteration 76/1000 | Loss: 0.00001669
Iteration 77/1000 | Loss: 0.00001667
Iteration 78/1000 | Loss: 0.00001666
Iteration 79/1000 | Loss: 0.00001666
Iteration 80/1000 | Loss: 0.00001665
Iteration 81/1000 | Loss: 0.00001665
Iteration 82/1000 | Loss: 0.00001665
Iteration 83/1000 | Loss: 0.00001665
Iteration 84/1000 | Loss: 0.00001665
Iteration 85/1000 | Loss: 0.00001665
Iteration 86/1000 | Loss: 0.00001665
Iteration 87/1000 | Loss: 0.00001665
Iteration 88/1000 | Loss: 0.00001665
Iteration 89/1000 | Loss: 0.00001665
Iteration 90/1000 | Loss: 0.00001665
Iteration 91/1000 | Loss: 0.00001665
Iteration 92/1000 | Loss: 0.00001664
Iteration 93/1000 | Loss: 0.00001664
Iteration 94/1000 | Loss: 0.00001664
Iteration 95/1000 | Loss: 0.00001664
Iteration 96/1000 | Loss: 0.00001663
Iteration 97/1000 | Loss: 0.00001663
Iteration 98/1000 | Loss: 0.00001663
Iteration 99/1000 | Loss: 0.00001663
Iteration 100/1000 | Loss: 0.00001663
Iteration 101/1000 | Loss: 0.00001662
Iteration 102/1000 | Loss: 0.00001662
Iteration 103/1000 | Loss: 0.00001662
Iteration 104/1000 | Loss: 0.00001662
Iteration 105/1000 | Loss: 0.00001662
Iteration 106/1000 | Loss: 0.00001662
Iteration 107/1000 | Loss: 0.00001662
Iteration 108/1000 | Loss: 0.00001662
Iteration 109/1000 | Loss: 0.00001661
Iteration 110/1000 | Loss: 0.00001661
Iteration 111/1000 | Loss: 0.00001661
Iteration 112/1000 | Loss: 0.00001661
Iteration 113/1000 | Loss: 0.00001661
Iteration 114/1000 | Loss: 0.00001661
Iteration 115/1000 | Loss: 0.00001661
Iteration 116/1000 | Loss: 0.00001660
Iteration 117/1000 | Loss: 0.00001660
Iteration 118/1000 | Loss: 0.00001660
Iteration 119/1000 | Loss: 0.00001660
Iteration 120/1000 | Loss: 0.00001660
Iteration 121/1000 | Loss: 0.00001659
Iteration 122/1000 | Loss: 0.00001659
Iteration 123/1000 | Loss: 0.00001659
Iteration 124/1000 | Loss: 0.00001659
Iteration 125/1000 | Loss: 0.00001659
Iteration 126/1000 | Loss: 0.00001659
Iteration 127/1000 | Loss: 0.00001659
Iteration 128/1000 | Loss: 0.00001659
Iteration 129/1000 | Loss: 0.00001659
Iteration 130/1000 | Loss: 0.00001659
Iteration 131/1000 | Loss: 0.00001659
Iteration 132/1000 | Loss: 0.00001659
Iteration 133/1000 | Loss: 0.00001659
Iteration 134/1000 | Loss: 0.00001659
Iteration 135/1000 | Loss: 0.00001659
Iteration 136/1000 | Loss: 0.00001659
Iteration 137/1000 | Loss: 0.00001658
Iteration 138/1000 | Loss: 0.00001658
Iteration 139/1000 | Loss: 0.00001658
Iteration 140/1000 | Loss: 0.00001658
Iteration 141/1000 | Loss: 0.00001657
Iteration 142/1000 | Loss: 0.00001657
Iteration 143/1000 | Loss: 0.00001657
Iteration 144/1000 | Loss: 0.00001657
Iteration 145/1000 | Loss: 0.00001657
Iteration 146/1000 | Loss: 0.00001657
Iteration 147/1000 | Loss: 0.00001657
Iteration 148/1000 | Loss: 0.00001656
Iteration 149/1000 | Loss: 0.00001656
Iteration 150/1000 | Loss: 0.00001656
Iteration 151/1000 | Loss: 0.00001656
Iteration 152/1000 | Loss: 0.00001656
Iteration 153/1000 | Loss: 0.00001655
Iteration 154/1000 | Loss: 0.00001655
Iteration 155/1000 | Loss: 0.00001655
Iteration 156/1000 | Loss: 0.00001655
Iteration 157/1000 | Loss: 0.00001655
Iteration 158/1000 | Loss: 0.00001654
Iteration 159/1000 | Loss: 0.00001654
Iteration 160/1000 | Loss: 0.00001654
Iteration 161/1000 | Loss: 0.00001654
Iteration 162/1000 | Loss: 0.00001654
Iteration 163/1000 | Loss: 0.00001654
Iteration 164/1000 | Loss: 0.00001653
Iteration 165/1000 | Loss: 0.00001653
Iteration 166/1000 | Loss: 0.00001653
Iteration 167/1000 | Loss: 0.00001653
Iteration 168/1000 | Loss: 0.00001653
Iteration 169/1000 | Loss: 0.00001653
Iteration 170/1000 | Loss: 0.00001653
Iteration 171/1000 | Loss: 0.00001653
Iteration 172/1000 | Loss: 0.00001653
Iteration 173/1000 | Loss: 0.00001653
Iteration 174/1000 | Loss: 0.00001653
Iteration 175/1000 | Loss: 0.00001653
Iteration 176/1000 | Loss: 0.00001652
Iteration 177/1000 | Loss: 0.00001652
Iteration 178/1000 | Loss: 0.00001652
Iteration 179/1000 | Loss: 0.00001652
Iteration 180/1000 | Loss: 0.00001652
Iteration 181/1000 | Loss: 0.00001652
Iteration 182/1000 | Loss: 0.00001652
Iteration 183/1000 | Loss: 0.00001652
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [1.652450555411633e-05, 1.652450555411633e-05, 1.652450555411633e-05, 1.652450555411633e-05, 1.652450555411633e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.652450555411633e-05

Optimization complete. Final v2v error: 3.4113197326660156 mm

Highest mean error: 3.6076862812042236 mm for frame 1

Lowest mean error: 3.2409822940826416 mm for frame 131

Saving results

Total time: 35.77924656867981
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814120
Iteration 2/25 | Loss: 0.00125101
Iteration 3/25 | Loss: 0.00110488
Iteration 4/25 | Loss: 0.00108822
Iteration 5/25 | Loss: 0.00108400
Iteration 6/25 | Loss: 0.00108290
Iteration 7/25 | Loss: 0.00108290
Iteration 8/25 | Loss: 0.00108290
Iteration 9/25 | Loss: 0.00108290
Iteration 10/25 | Loss: 0.00108290
Iteration 11/25 | Loss: 0.00108290
Iteration 12/25 | Loss: 0.00108290
Iteration 13/25 | Loss: 0.00108290
Iteration 14/25 | Loss: 0.00108290
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010828959057107568, 0.0010828959057107568, 0.0010828959057107568, 0.0010828959057107568, 0.0010828959057107568]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010828959057107568

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36903572
Iteration 2/25 | Loss: 0.00087909
Iteration 3/25 | Loss: 0.00087909
Iteration 4/25 | Loss: 0.00087909
Iteration 5/25 | Loss: 0.00087909
Iteration 6/25 | Loss: 0.00087909
Iteration 7/25 | Loss: 0.00087909
Iteration 8/25 | Loss: 0.00087909
Iteration 9/25 | Loss: 0.00087909
Iteration 10/25 | Loss: 0.00087909
Iteration 11/25 | Loss: 0.00087909
Iteration 12/25 | Loss: 0.00087909
Iteration 13/25 | Loss: 0.00087909
Iteration 14/25 | Loss: 0.00087909
Iteration 15/25 | Loss: 0.00087909
Iteration 16/25 | Loss: 0.00087909
Iteration 17/25 | Loss: 0.00087909
Iteration 18/25 | Loss: 0.00087909
Iteration 19/25 | Loss: 0.00087909
Iteration 20/25 | Loss: 0.00087909
Iteration 21/25 | Loss: 0.00087909
Iteration 22/25 | Loss: 0.00087909
Iteration 23/25 | Loss: 0.00087909
Iteration 24/25 | Loss: 0.00087909
Iteration 25/25 | Loss: 0.00087909

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087909
Iteration 2/1000 | Loss: 0.00002061
Iteration 3/1000 | Loss: 0.00001416
Iteration 4/1000 | Loss: 0.00001261
Iteration 5/1000 | Loss: 0.00001171
Iteration 6/1000 | Loss: 0.00001125
Iteration 7/1000 | Loss: 0.00001086
Iteration 8/1000 | Loss: 0.00001064
Iteration 9/1000 | Loss: 0.00001062
Iteration 10/1000 | Loss: 0.00001060
Iteration 11/1000 | Loss: 0.00001056
Iteration 12/1000 | Loss: 0.00001051
Iteration 13/1000 | Loss: 0.00001050
Iteration 14/1000 | Loss: 0.00001049
Iteration 15/1000 | Loss: 0.00001047
Iteration 16/1000 | Loss: 0.00001027
Iteration 17/1000 | Loss: 0.00001014
Iteration 18/1000 | Loss: 0.00001013
Iteration 19/1000 | Loss: 0.00001012
Iteration 20/1000 | Loss: 0.00001011
Iteration 21/1000 | Loss: 0.00001010
Iteration 22/1000 | Loss: 0.00001009
Iteration 23/1000 | Loss: 0.00001005
Iteration 24/1000 | Loss: 0.00001004
Iteration 25/1000 | Loss: 0.00001001
Iteration 26/1000 | Loss: 0.00000998
Iteration 27/1000 | Loss: 0.00000997
Iteration 28/1000 | Loss: 0.00000996
Iteration 29/1000 | Loss: 0.00000996
Iteration 30/1000 | Loss: 0.00000995
Iteration 31/1000 | Loss: 0.00000995
Iteration 32/1000 | Loss: 0.00000995
Iteration 33/1000 | Loss: 0.00000994
Iteration 34/1000 | Loss: 0.00000994
Iteration 35/1000 | Loss: 0.00000992
Iteration 36/1000 | Loss: 0.00000992
Iteration 37/1000 | Loss: 0.00000991
Iteration 38/1000 | Loss: 0.00000991
Iteration 39/1000 | Loss: 0.00000991
Iteration 40/1000 | Loss: 0.00000990
Iteration 41/1000 | Loss: 0.00000990
Iteration 42/1000 | Loss: 0.00000989
Iteration 43/1000 | Loss: 0.00000989
Iteration 44/1000 | Loss: 0.00000988
Iteration 45/1000 | Loss: 0.00000987
Iteration 46/1000 | Loss: 0.00000986
Iteration 47/1000 | Loss: 0.00000985
Iteration 48/1000 | Loss: 0.00000985
Iteration 49/1000 | Loss: 0.00000981
Iteration 50/1000 | Loss: 0.00000980
Iteration 51/1000 | Loss: 0.00000980
Iteration 52/1000 | Loss: 0.00000979
Iteration 53/1000 | Loss: 0.00000979
Iteration 54/1000 | Loss: 0.00000979
Iteration 55/1000 | Loss: 0.00000979
Iteration 56/1000 | Loss: 0.00000979
Iteration 57/1000 | Loss: 0.00000979
Iteration 58/1000 | Loss: 0.00000979
Iteration 59/1000 | Loss: 0.00000979
Iteration 60/1000 | Loss: 0.00000979
Iteration 61/1000 | Loss: 0.00000978
Iteration 62/1000 | Loss: 0.00000978
Iteration 63/1000 | Loss: 0.00000978
Iteration 64/1000 | Loss: 0.00000978
Iteration 65/1000 | Loss: 0.00000978
Iteration 66/1000 | Loss: 0.00000978
Iteration 67/1000 | Loss: 0.00000978
Iteration 68/1000 | Loss: 0.00000978
Iteration 69/1000 | Loss: 0.00000977
Iteration 70/1000 | Loss: 0.00000977
Iteration 71/1000 | Loss: 0.00000977
Iteration 72/1000 | Loss: 0.00000976
Iteration 73/1000 | Loss: 0.00000976
Iteration 74/1000 | Loss: 0.00000976
Iteration 75/1000 | Loss: 0.00000976
Iteration 76/1000 | Loss: 0.00000975
Iteration 77/1000 | Loss: 0.00000975
Iteration 78/1000 | Loss: 0.00000975
Iteration 79/1000 | Loss: 0.00000974
Iteration 80/1000 | Loss: 0.00000974
Iteration 81/1000 | Loss: 0.00000974
Iteration 82/1000 | Loss: 0.00000973
Iteration 83/1000 | Loss: 0.00000973
Iteration 84/1000 | Loss: 0.00000973
Iteration 85/1000 | Loss: 0.00000973
Iteration 86/1000 | Loss: 0.00000972
Iteration 87/1000 | Loss: 0.00000972
Iteration 88/1000 | Loss: 0.00000972
Iteration 89/1000 | Loss: 0.00000972
Iteration 90/1000 | Loss: 0.00000972
Iteration 91/1000 | Loss: 0.00000972
Iteration 92/1000 | Loss: 0.00000972
Iteration 93/1000 | Loss: 0.00000971
Iteration 94/1000 | Loss: 0.00000971
Iteration 95/1000 | Loss: 0.00000970
Iteration 96/1000 | Loss: 0.00000970
Iteration 97/1000 | Loss: 0.00000970
Iteration 98/1000 | Loss: 0.00000970
Iteration 99/1000 | Loss: 0.00000970
Iteration 100/1000 | Loss: 0.00000970
Iteration 101/1000 | Loss: 0.00000969
Iteration 102/1000 | Loss: 0.00000969
Iteration 103/1000 | Loss: 0.00000969
Iteration 104/1000 | Loss: 0.00000969
Iteration 105/1000 | Loss: 0.00000969
Iteration 106/1000 | Loss: 0.00000968
Iteration 107/1000 | Loss: 0.00000968
Iteration 108/1000 | Loss: 0.00000968
Iteration 109/1000 | Loss: 0.00000968
Iteration 110/1000 | Loss: 0.00000968
Iteration 111/1000 | Loss: 0.00000967
Iteration 112/1000 | Loss: 0.00000967
Iteration 113/1000 | Loss: 0.00000967
Iteration 114/1000 | Loss: 0.00000966
Iteration 115/1000 | Loss: 0.00000966
Iteration 116/1000 | Loss: 0.00000966
Iteration 117/1000 | Loss: 0.00000966
Iteration 118/1000 | Loss: 0.00000966
Iteration 119/1000 | Loss: 0.00000966
Iteration 120/1000 | Loss: 0.00000965
Iteration 121/1000 | Loss: 0.00000965
Iteration 122/1000 | Loss: 0.00000965
Iteration 123/1000 | Loss: 0.00000965
Iteration 124/1000 | Loss: 0.00000965
Iteration 125/1000 | Loss: 0.00000965
Iteration 126/1000 | Loss: 0.00000965
Iteration 127/1000 | Loss: 0.00000964
Iteration 128/1000 | Loss: 0.00000964
Iteration 129/1000 | Loss: 0.00000964
Iteration 130/1000 | Loss: 0.00000964
Iteration 131/1000 | Loss: 0.00000964
Iteration 132/1000 | Loss: 0.00000964
Iteration 133/1000 | Loss: 0.00000963
Iteration 134/1000 | Loss: 0.00000963
Iteration 135/1000 | Loss: 0.00000963
Iteration 136/1000 | Loss: 0.00000963
Iteration 137/1000 | Loss: 0.00000963
Iteration 138/1000 | Loss: 0.00000963
Iteration 139/1000 | Loss: 0.00000963
Iteration 140/1000 | Loss: 0.00000963
Iteration 141/1000 | Loss: 0.00000963
Iteration 142/1000 | Loss: 0.00000963
Iteration 143/1000 | Loss: 0.00000963
Iteration 144/1000 | Loss: 0.00000963
Iteration 145/1000 | Loss: 0.00000963
Iteration 146/1000 | Loss: 0.00000962
Iteration 147/1000 | Loss: 0.00000962
Iteration 148/1000 | Loss: 0.00000962
Iteration 149/1000 | Loss: 0.00000962
Iteration 150/1000 | Loss: 0.00000961
Iteration 151/1000 | Loss: 0.00000961
Iteration 152/1000 | Loss: 0.00000961
Iteration 153/1000 | Loss: 0.00000961
Iteration 154/1000 | Loss: 0.00000960
Iteration 155/1000 | Loss: 0.00000960
Iteration 156/1000 | Loss: 0.00000960
Iteration 157/1000 | Loss: 0.00000960
Iteration 158/1000 | Loss: 0.00000959
Iteration 159/1000 | Loss: 0.00000959
Iteration 160/1000 | Loss: 0.00000959
Iteration 161/1000 | Loss: 0.00000959
Iteration 162/1000 | Loss: 0.00000959
Iteration 163/1000 | Loss: 0.00000959
Iteration 164/1000 | Loss: 0.00000959
Iteration 165/1000 | Loss: 0.00000959
Iteration 166/1000 | Loss: 0.00000959
Iteration 167/1000 | Loss: 0.00000959
Iteration 168/1000 | Loss: 0.00000959
Iteration 169/1000 | Loss: 0.00000959
Iteration 170/1000 | Loss: 0.00000958
Iteration 171/1000 | Loss: 0.00000958
Iteration 172/1000 | Loss: 0.00000958
Iteration 173/1000 | Loss: 0.00000958
Iteration 174/1000 | Loss: 0.00000958
Iteration 175/1000 | Loss: 0.00000957
Iteration 176/1000 | Loss: 0.00000957
Iteration 177/1000 | Loss: 0.00000957
Iteration 178/1000 | Loss: 0.00000957
Iteration 179/1000 | Loss: 0.00000957
Iteration 180/1000 | Loss: 0.00000957
Iteration 181/1000 | Loss: 0.00000957
Iteration 182/1000 | Loss: 0.00000956
Iteration 183/1000 | Loss: 0.00000956
Iteration 184/1000 | Loss: 0.00000956
Iteration 185/1000 | Loss: 0.00000956
Iteration 186/1000 | Loss: 0.00000956
Iteration 187/1000 | Loss: 0.00000956
Iteration 188/1000 | Loss: 0.00000956
Iteration 189/1000 | Loss: 0.00000956
Iteration 190/1000 | Loss: 0.00000956
Iteration 191/1000 | Loss: 0.00000956
Iteration 192/1000 | Loss: 0.00000956
Iteration 193/1000 | Loss: 0.00000956
Iteration 194/1000 | Loss: 0.00000956
Iteration 195/1000 | Loss: 0.00000956
Iteration 196/1000 | Loss: 0.00000956
Iteration 197/1000 | Loss: 0.00000956
Iteration 198/1000 | Loss: 0.00000956
Iteration 199/1000 | Loss: 0.00000955
Iteration 200/1000 | Loss: 0.00000955
Iteration 201/1000 | Loss: 0.00000955
Iteration 202/1000 | Loss: 0.00000955
Iteration 203/1000 | Loss: 0.00000955
Iteration 204/1000 | Loss: 0.00000955
Iteration 205/1000 | Loss: 0.00000955
Iteration 206/1000 | Loss: 0.00000955
Iteration 207/1000 | Loss: 0.00000955
Iteration 208/1000 | Loss: 0.00000955
Iteration 209/1000 | Loss: 0.00000955
Iteration 210/1000 | Loss: 0.00000955
Iteration 211/1000 | Loss: 0.00000955
Iteration 212/1000 | Loss: 0.00000955
Iteration 213/1000 | Loss: 0.00000955
Iteration 214/1000 | Loss: 0.00000955
Iteration 215/1000 | Loss: 0.00000954
Iteration 216/1000 | Loss: 0.00000954
Iteration 217/1000 | Loss: 0.00000954
Iteration 218/1000 | Loss: 0.00000954
Iteration 219/1000 | Loss: 0.00000954
Iteration 220/1000 | Loss: 0.00000954
Iteration 221/1000 | Loss: 0.00000954
Iteration 222/1000 | Loss: 0.00000954
Iteration 223/1000 | Loss: 0.00000954
Iteration 224/1000 | Loss: 0.00000954
Iteration 225/1000 | Loss: 0.00000954
Iteration 226/1000 | Loss: 0.00000954
Iteration 227/1000 | Loss: 0.00000954
Iteration 228/1000 | Loss: 0.00000954
Iteration 229/1000 | Loss: 0.00000954
Iteration 230/1000 | Loss: 0.00000954
Iteration 231/1000 | Loss: 0.00000954
Iteration 232/1000 | Loss: 0.00000954
Iteration 233/1000 | Loss: 0.00000954
Iteration 234/1000 | Loss: 0.00000954
Iteration 235/1000 | Loss: 0.00000954
Iteration 236/1000 | Loss: 0.00000954
Iteration 237/1000 | Loss: 0.00000954
Iteration 238/1000 | Loss: 0.00000954
Iteration 239/1000 | Loss: 0.00000954
Iteration 240/1000 | Loss: 0.00000954
Iteration 241/1000 | Loss: 0.00000954
Iteration 242/1000 | Loss: 0.00000954
Iteration 243/1000 | Loss: 0.00000954
Iteration 244/1000 | Loss: 0.00000954
Iteration 245/1000 | Loss: 0.00000954
Iteration 246/1000 | Loss: 0.00000954
Iteration 247/1000 | Loss: 0.00000954
Iteration 248/1000 | Loss: 0.00000954
Iteration 249/1000 | Loss: 0.00000954
Iteration 250/1000 | Loss: 0.00000954
Iteration 251/1000 | Loss: 0.00000954
Iteration 252/1000 | Loss: 0.00000954
Iteration 253/1000 | Loss: 0.00000954
Iteration 254/1000 | Loss: 0.00000954
Iteration 255/1000 | Loss: 0.00000954
Iteration 256/1000 | Loss: 0.00000954
Iteration 257/1000 | Loss: 0.00000954
Iteration 258/1000 | Loss: 0.00000954
Iteration 259/1000 | Loss: 0.00000954
Iteration 260/1000 | Loss: 0.00000954
Iteration 261/1000 | Loss: 0.00000954
Iteration 262/1000 | Loss: 0.00000954
Iteration 263/1000 | Loss: 0.00000954
Iteration 264/1000 | Loss: 0.00000954
Iteration 265/1000 | Loss: 0.00000954
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [9.535372555546928e-06, 9.535372555546928e-06, 9.535372555546928e-06, 9.535372555546928e-06, 9.535372555546928e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.535372555546928e-06

Optimization complete. Final v2v error: 2.6134865283966064 mm

Highest mean error: 3.6095213890075684 mm for frame 85

Lowest mean error: 2.299133777618408 mm for frame 23

Saving results

Total time: 43.29024314880371
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01041928
Iteration 2/25 | Loss: 0.00226961
Iteration 3/25 | Loss: 0.00167605
Iteration 4/25 | Loss: 0.00159958
Iteration 5/25 | Loss: 0.00159211
Iteration 6/25 | Loss: 0.00155360
Iteration 7/25 | Loss: 0.00147968
Iteration 8/25 | Loss: 0.00146345
Iteration 9/25 | Loss: 0.00146496
Iteration 10/25 | Loss: 0.00144963
Iteration 11/25 | Loss: 0.00145339
Iteration 12/25 | Loss: 0.00142684
Iteration 13/25 | Loss: 0.00141380
Iteration 14/25 | Loss: 0.00139358
Iteration 15/25 | Loss: 0.00139153
Iteration 16/25 | Loss: 0.00137467
Iteration 17/25 | Loss: 0.00137825
Iteration 18/25 | Loss: 0.00137263
Iteration 19/25 | Loss: 0.00136526
Iteration 20/25 | Loss: 0.00135939
Iteration 21/25 | Loss: 0.00135867
Iteration 22/25 | Loss: 0.00135800
Iteration 23/25 | Loss: 0.00135706
Iteration 24/25 | Loss: 0.00135847
Iteration 25/25 | Loss: 0.00135682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32187784
Iteration 2/25 | Loss: 0.00308656
Iteration 3/25 | Loss: 0.00273086
Iteration 4/25 | Loss: 0.00273086
Iteration 5/25 | Loss: 0.00273086
Iteration 6/25 | Loss: 0.00273086
Iteration 7/25 | Loss: 0.00273086
Iteration 8/25 | Loss: 0.00273086
Iteration 9/25 | Loss: 0.00273086
Iteration 10/25 | Loss: 0.00273086
Iteration 11/25 | Loss: 0.00273086
Iteration 12/25 | Loss: 0.00273086
Iteration 13/25 | Loss: 0.00273086
Iteration 14/25 | Loss: 0.00273086
Iteration 15/25 | Loss: 0.00273086
Iteration 16/25 | Loss: 0.00273086
Iteration 17/25 | Loss: 0.00273086
Iteration 18/25 | Loss: 0.00273086
Iteration 19/25 | Loss: 0.00273086
Iteration 20/25 | Loss: 0.00273086
Iteration 21/25 | Loss: 0.00273086
Iteration 22/25 | Loss: 0.00273086
Iteration 23/25 | Loss: 0.00273086
Iteration 24/25 | Loss: 0.00273086
Iteration 25/25 | Loss: 0.00273086

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00273086
Iteration 2/1000 | Loss: 0.00259192
Iteration 3/1000 | Loss: 0.00722592
Iteration 4/1000 | Loss: 0.01069856
Iteration 5/1000 | Loss: 0.00658238
Iteration 6/1000 | Loss: 0.00366736
Iteration 7/1000 | Loss: 0.00034575
Iteration 8/1000 | Loss: 0.00092376
Iteration 9/1000 | Loss: 0.00489360
Iteration 10/1000 | Loss: 0.00273710
Iteration 11/1000 | Loss: 0.00441143
Iteration 12/1000 | Loss: 0.00424779
Iteration 13/1000 | Loss: 0.00238801
Iteration 14/1000 | Loss: 0.00033140
Iteration 15/1000 | Loss: 0.00021703
Iteration 16/1000 | Loss: 0.00089996
Iteration 17/1000 | Loss: 0.00026743
Iteration 18/1000 | Loss: 0.00021239
Iteration 19/1000 | Loss: 0.00157739
Iteration 20/1000 | Loss: 0.00068729
Iteration 21/1000 | Loss: 0.00044798
Iteration 22/1000 | Loss: 0.00049804
Iteration 23/1000 | Loss: 0.00059808
Iteration 24/1000 | Loss: 0.00139698
Iteration 25/1000 | Loss: 0.00508178
Iteration 26/1000 | Loss: 0.00309858
Iteration 27/1000 | Loss: 0.00622169
Iteration 28/1000 | Loss: 0.00362177
Iteration 29/1000 | Loss: 0.00308071
Iteration 30/1000 | Loss: 0.00409669
Iteration 31/1000 | Loss: 0.00711240
Iteration 32/1000 | Loss: 0.00412154
Iteration 33/1000 | Loss: 0.00762438
Iteration 34/1000 | Loss: 0.00560072
Iteration 35/1000 | Loss: 0.00625149
Iteration 36/1000 | Loss: 0.00316797
Iteration 37/1000 | Loss: 0.00111807
Iteration 38/1000 | Loss: 0.00101071
Iteration 39/1000 | Loss: 0.00088065
Iteration 40/1000 | Loss: 0.00194015
Iteration 41/1000 | Loss: 0.00051489
Iteration 42/1000 | Loss: 0.00014863
Iteration 43/1000 | Loss: 0.00102560
Iteration 44/1000 | Loss: 0.00046196
Iteration 45/1000 | Loss: 0.00098757
Iteration 46/1000 | Loss: 0.00032292
Iteration 47/1000 | Loss: 0.00026124
Iteration 48/1000 | Loss: 0.00103354
Iteration 49/1000 | Loss: 0.00048667
Iteration 50/1000 | Loss: 0.00041690
Iteration 51/1000 | Loss: 0.00030141
Iteration 52/1000 | Loss: 0.00041662
Iteration 53/1000 | Loss: 0.00008522
Iteration 54/1000 | Loss: 0.00073523
Iteration 55/1000 | Loss: 0.00064861
Iteration 56/1000 | Loss: 0.00111256
Iteration 57/1000 | Loss: 0.00154473
Iteration 58/1000 | Loss: 0.00272378
Iteration 59/1000 | Loss: 0.00103183
Iteration 60/1000 | Loss: 0.00145353
Iteration 61/1000 | Loss: 0.00043096
Iteration 62/1000 | Loss: 0.00083446
Iteration 63/1000 | Loss: 0.00063802
Iteration 64/1000 | Loss: 0.00047102
Iteration 65/1000 | Loss: 0.00016624
Iteration 66/1000 | Loss: 0.00005690
Iteration 67/1000 | Loss: 0.00054652
Iteration 68/1000 | Loss: 0.00035378
Iteration 69/1000 | Loss: 0.00008745
Iteration 70/1000 | Loss: 0.00005893
Iteration 71/1000 | Loss: 0.00005752
Iteration 72/1000 | Loss: 0.00011025
Iteration 73/1000 | Loss: 0.00007404
Iteration 74/1000 | Loss: 0.00021426
Iteration 75/1000 | Loss: 0.00002530
Iteration 76/1000 | Loss: 0.00002404
Iteration 77/1000 | Loss: 0.00002035
Iteration 78/1000 | Loss: 0.00004277
Iteration 79/1000 | Loss: 0.00004308
Iteration 80/1000 | Loss: 0.00002250
Iteration 81/1000 | Loss: 0.00001894
Iteration 82/1000 | Loss: 0.00007088
Iteration 83/1000 | Loss: 0.00001766
Iteration 84/1000 | Loss: 0.00004463
Iteration 85/1000 | Loss: 0.00008725
Iteration 86/1000 | Loss: 0.00001775
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00002308
Iteration 89/1000 | Loss: 0.00001493
Iteration 90/1000 | Loss: 0.00002640
Iteration 91/1000 | Loss: 0.00001797
Iteration 92/1000 | Loss: 0.00001414
Iteration 93/1000 | Loss: 0.00001410
Iteration 94/1000 | Loss: 0.00001409
Iteration 95/1000 | Loss: 0.00001630
Iteration 96/1000 | Loss: 0.00002101
Iteration 97/1000 | Loss: 0.00002805
Iteration 98/1000 | Loss: 0.00001534
Iteration 99/1000 | Loss: 0.00001479
Iteration 100/1000 | Loss: 0.00001381
Iteration 101/1000 | Loss: 0.00001381
Iteration 102/1000 | Loss: 0.00001381
Iteration 103/1000 | Loss: 0.00001381
Iteration 104/1000 | Loss: 0.00001381
Iteration 105/1000 | Loss: 0.00001381
Iteration 106/1000 | Loss: 0.00001381
Iteration 107/1000 | Loss: 0.00001381
Iteration 108/1000 | Loss: 0.00001381
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.3809578376822174e-05, 1.3809578376822174e-05, 1.3809578376822174e-05, 1.3809578376822174e-05, 1.3809578376822174e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3809578376822174e-05

Optimization complete. Final v2v error: 3.0594615936279297 mm

Highest mean error: 5.149484634399414 mm for frame 51

Lowest mean error: 2.5375351905822754 mm for frame 94

Saving results

Total time: 189.04088354110718
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390208
Iteration 2/25 | Loss: 0.00115936
Iteration 3/25 | Loss: 0.00107587
Iteration 4/25 | Loss: 0.00106782
Iteration 5/25 | Loss: 0.00106444
Iteration 6/25 | Loss: 0.00106404
Iteration 7/25 | Loss: 0.00106404
Iteration 8/25 | Loss: 0.00106404
Iteration 9/25 | Loss: 0.00106404
Iteration 10/25 | Loss: 0.00106404
Iteration 11/25 | Loss: 0.00106404
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010640380205586553, 0.0010640380205586553, 0.0010640380205586553, 0.0010640380205586553, 0.0010640380205586553]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010640380205586553

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60782504
Iteration 2/25 | Loss: 0.00064140
Iteration 3/25 | Loss: 0.00064140
Iteration 4/25 | Loss: 0.00064140
Iteration 5/25 | Loss: 0.00064140
Iteration 6/25 | Loss: 0.00064139
Iteration 7/25 | Loss: 0.00064139
Iteration 8/25 | Loss: 0.00064139
Iteration 9/25 | Loss: 0.00064139
Iteration 10/25 | Loss: 0.00064139
Iteration 11/25 | Loss: 0.00064139
Iteration 12/25 | Loss: 0.00064139
Iteration 13/25 | Loss: 0.00064139
Iteration 14/25 | Loss: 0.00064139
Iteration 15/25 | Loss: 0.00064139
Iteration 16/25 | Loss: 0.00064139
Iteration 17/25 | Loss: 0.00064139
Iteration 18/25 | Loss: 0.00064139
Iteration 19/25 | Loss: 0.00064139
Iteration 20/25 | Loss: 0.00064139
Iteration 21/25 | Loss: 0.00064139
Iteration 22/25 | Loss: 0.00064139
Iteration 23/25 | Loss: 0.00064139
Iteration 24/25 | Loss: 0.00064139
Iteration 25/25 | Loss: 0.00064139

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064139
Iteration 2/1000 | Loss: 0.00002147
Iteration 3/1000 | Loss: 0.00001362
Iteration 4/1000 | Loss: 0.00001229
Iteration 5/1000 | Loss: 0.00001136
Iteration 6/1000 | Loss: 0.00001087
Iteration 7/1000 | Loss: 0.00001049
Iteration 8/1000 | Loss: 0.00001043
Iteration 9/1000 | Loss: 0.00001033
Iteration 10/1000 | Loss: 0.00001009
Iteration 11/1000 | Loss: 0.00000976
Iteration 12/1000 | Loss: 0.00000975
Iteration 13/1000 | Loss: 0.00000972
Iteration 14/1000 | Loss: 0.00000965
Iteration 15/1000 | Loss: 0.00000957
Iteration 16/1000 | Loss: 0.00000954
Iteration 17/1000 | Loss: 0.00000953
Iteration 18/1000 | Loss: 0.00000953
Iteration 19/1000 | Loss: 0.00000952
Iteration 20/1000 | Loss: 0.00000952
Iteration 21/1000 | Loss: 0.00000951
Iteration 22/1000 | Loss: 0.00000947
Iteration 23/1000 | Loss: 0.00000937
Iteration 24/1000 | Loss: 0.00000933
Iteration 25/1000 | Loss: 0.00000927
Iteration 26/1000 | Loss: 0.00000927
Iteration 27/1000 | Loss: 0.00000927
Iteration 28/1000 | Loss: 0.00000927
Iteration 29/1000 | Loss: 0.00000927
Iteration 30/1000 | Loss: 0.00000927
Iteration 31/1000 | Loss: 0.00000927
Iteration 32/1000 | Loss: 0.00000927
Iteration 33/1000 | Loss: 0.00000926
Iteration 34/1000 | Loss: 0.00000926
Iteration 35/1000 | Loss: 0.00000926
Iteration 36/1000 | Loss: 0.00000923
Iteration 37/1000 | Loss: 0.00000923
Iteration 38/1000 | Loss: 0.00000922
Iteration 39/1000 | Loss: 0.00000922
Iteration 40/1000 | Loss: 0.00000922
Iteration 41/1000 | Loss: 0.00000922
Iteration 42/1000 | Loss: 0.00000922
Iteration 43/1000 | Loss: 0.00000922
Iteration 44/1000 | Loss: 0.00000922
Iteration 45/1000 | Loss: 0.00000922
Iteration 46/1000 | Loss: 0.00000922
Iteration 47/1000 | Loss: 0.00000922
Iteration 48/1000 | Loss: 0.00000922
Iteration 49/1000 | Loss: 0.00000922
Iteration 50/1000 | Loss: 0.00000921
Iteration 51/1000 | Loss: 0.00000921
Iteration 52/1000 | Loss: 0.00000921
Iteration 53/1000 | Loss: 0.00000920
Iteration 54/1000 | Loss: 0.00000920
Iteration 55/1000 | Loss: 0.00000919
Iteration 56/1000 | Loss: 0.00000919
Iteration 57/1000 | Loss: 0.00000918
Iteration 58/1000 | Loss: 0.00000918
Iteration 59/1000 | Loss: 0.00000918
Iteration 60/1000 | Loss: 0.00000918
Iteration 61/1000 | Loss: 0.00000918
Iteration 62/1000 | Loss: 0.00000917
Iteration 63/1000 | Loss: 0.00000917
Iteration 64/1000 | Loss: 0.00000917
Iteration 65/1000 | Loss: 0.00000916
Iteration 66/1000 | Loss: 0.00000915
Iteration 67/1000 | Loss: 0.00000915
Iteration 68/1000 | Loss: 0.00000915
Iteration 69/1000 | Loss: 0.00000915
Iteration 70/1000 | Loss: 0.00000915
Iteration 71/1000 | Loss: 0.00000915
Iteration 72/1000 | Loss: 0.00000915
Iteration 73/1000 | Loss: 0.00000915
Iteration 74/1000 | Loss: 0.00000914
Iteration 75/1000 | Loss: 0.00000913
Iteration 76/1000 | Loss: 0.00000913
Iteration 77/1000 | Loss: 0.00000913
Iteration 78/1000 | Loss: 0.00000913
Iteration 79/1000 | Loss: 0.00000912
Iteration 80/1000 | Loss: 0.00000912
Iteration 81/1000 | Loss: 0.00000912
Iteration 82/1000 | Loss: 0.00000912
Iteration 83/1000 | Loss: 0.00000912
Iteration 84/1000 | Loss: 0.00000911
Iteration 85/1000 | Loss: 0.00000911
Iteration 86/1000 | Loss: 0.00000911
Iteration 87/1000 | Loss: 0.00000911
Iteration 88/1000 | Loss: 0.00000911
Iteration 89/1000 | Loss: 0.00000911
Iteration 90/1000 | Loss: 0.00000911
Iteration 91/1000 | Loss: 0.00000911
Iteration 92/1000 | Loss: 0.00000911
Iteration 93/1000 | Loss: 0.00000911
Iteration 94/1000 | Loss: 0.00000910
Iteration 95/1000 | Loss: 0.00000910
Iteration 96/1000 | Loss: 0.00000910
Iteration 97/1000 | Loss: 0.00000910
Iteration 98/1000 | Loss: 0.00000910
Iteration 99/1000 | Loss: 0.00000910
Iteration 100/1000 | Loss: 0.00000910
Iteration 101/1000 | Loss: 0.00000909
Iteration 102/1000 | Loss: 0.00000909
Iteration 103/1000 | Loss: 0.00000909
Iteration 104/1000 | Loss: 0.00000909
Iteration 105/1000 | Loss: 0.00000909
Iteration 106/1000 | Loss: 0.00000909
Iteration 107/1000 | Loss: 0.00000909
Iteration 108/1000 | Loss: 0.00000909
Iteration 109/1000 | Loss: 0.00000909
Iteration 110/1000 | Loss: 0.00000909
Iteration 111/1000 | Loss: 0.00000909
Iteration 112/1000 | Loss: 0.00000909
Iteration 113/1000 | Loss: 0.00000909
Iteration 114/1000 | Loss: 0.00000909
Iteration 115/1000 | Loss: 0.00000909
Iteration 116/1000 | Loss: 0.00000909
Iteration 117/1000 | Loss: 0.00000909
Iteration 118/1000 | Loss: 0.00000908
Iteration 119/1000 | Loss: 0.00000908
Iteration 120/1000 | Loss: 0.00000908
Iteration 121/1000 | Loss: 0.00000908
Iteration 122/1000 | Loss: 0.00000908
Iteration 123/1000 | Loss: 0.00000908
Iteration 124/1000 | Loss: 0.00000908
Iteration 125/1000 | Loss: 0.00000908
Iteration 126/1000 | Loss: 0.00000908
Iteration 127/1000 | Loss: 0.00000908
Iteration 128/1000 | Loss: 0.00000908
Iteration 129/1000 | Loss: 0.00000908
Iteration 130/1000 | Loss: 0.00000908
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [9.083233635465149e-06, 9.083233635465149e-06, 9.083233635465149e-06, 9.083233635465149e-06, 9.083233635465149e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.083233635465149e-06

Optimization complete. Final v2v error: 2.5886404514312744 mm

Highest mean error: 2.9699199199676514 mm for frame 219

Lowest mean error: 2.309844732284546 mm for frame 243

Saving results

Total time: 39.15216064453125
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00973617
Iteration 2/25 | Loss: 0.00146718
Iteration 3/25 | Loss: 0.00121438
Iteration 4/25 | Loss: 0.00117147
Iteration 5/25 | Loss: 0.00116672
Iteration 6/25 | Loss: 0.00116653
Iteration 7/25 | Loss: 0.00116653
Iteration 8/25 | Loss: 0.00116653
Iteration 9/25 | Loss: 0.00116653
Iteration 10/25 | Loss: 0.00116653
Iteration 11/25 | Loss: 0.00116653
Iteration 12/25 | Loss: 0.00116653
Iteration 13/25 | Loss: 0.00116653
Iteration 14/25 | Loss: 0.00116653
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011665328638628125, 0.0011665328638628125, 0.0011665328638628125, 0.0011665328638628125, 0.0011665328638628125]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011665328638628125

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35432994
Iteration 2/25 | Loss: 0.00059657
Iteration 3/25 | Loss: 0.00059657
Iteration 4/25 | Loss: 0.00059657
Iteration 5/25 | Loss: 0.00059657
Iteration 6/25 | Loss: 0.00059656
Iteration 7/25 | Loss: 0.00059656
Iteration 8/25 | Loss: 0.00059656
Iteration 9/25 | Loss: 0.00059656
Iteration 10/25 | Loss: 0.00059656
Iteration 11/25 | Loss: 0.00059656
Iteration 12/25 | Loss: 0.00059656
Iteration 13/25 | Loss: 0.00059656
Iteration 14/25 | Loss: 0.00059656
Iteration 15/25 | Loss: 0.00059656
Iteration 16/25 | Loss: 0.00059656
Iteration 17/25 | Loss: 0.00059656
Iteration 18/25 | Loss: 0.00059656
Iteration 19/25 | Loss: 0.00059656
Iteration 20/25 | Loss: 0.00059656
Iteration 21/25 | Loss: 0.00059656
Iteration 22/25 | Loss: 0.00059656
Iteration 23/25 | Loss: 0.00059656
Iteration 24/25 | Loss: 0.00059656
Iteration 25/25 | Loss: 0.00059656

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00059656
Iteration 2/1000 | Loss: 0.00002703
Iteration 3/1000 | Loss: 0.00002049
Iteration 4/1000 | Loss: 0.00001899
Iteration 5/1000 | Loss: 0.00001846
Iteration 6/1000 | Loss: 0.00001802
Iteration 7/1000 | Loss: 0.00001760
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001711
Iteration 10/1000 | Loss: 0.00001692
Iteration 11/1000 | Loss: 0.00001687
Iteration 12/1000 | Loss: 0.00001686
Iteration 13/1000 | Loss: 0.00001670
Iteration 14/1000 | Loss: 0.00001664
Iteration 15/1000 | Loss: 0.00001660
Iteration 16/1000 | Loss: 0.00001655
Iteration 17/1000 | Loss: 0.00001655
Iteration 18/1000 | Loss: 0.00001654
Iteration 19/1000 | Loss: 0.00001653
Iteration 20/1000 | Loss: 0.00001653
Iteration 21/1000 | Loss: 0.00001652
Iteration 22/1000 | Loss: 0.00001652
Iteration 23/1000 | Loss: 0.00001652
Iteration 24/1000 | Loss: 0.00001652
Iteration 25/1000 | Loss: 0.00001652
Iteration 26/1000 | Loss: 0.00001651
Iteration 27/1000 | Loss: 0.00001651
Iteration 28/1000 | Loss: 0.00001647
Iteration 29/1000 | Loss: 0.00001647
Iteration 30/1000 | Loss: 0.00001646
Iteration 31/1000 | Loss: 0.00001645
Iteration 32/1000 | Loss: 0.00001645
Iteration 33/1000 | Loss: 0.00001645
Iteration 34/1000 | Loss: 0.00001644
Iteration 35/1000 | Loss: 0.00001643
Iteration 36/1000 | Loss: 0.00001643
Iteration 37/1000 | Loss: 0.00001643
Iteration 38/1000 | Loss: 0.00001642
Iteration 39/1000 | Loss: 0.00001642
Iteration 40/1000 | Loss: 0.00001642
Iteration 41/1000 | Loss: 0.00001641
Iteration 42/1000 | Loss: 0.00001640
Iteration 43/1000 | Loss: 0.00001639
Iteration 44/1000 | Loss: 0.00001639
Iteration 45/1000 | Loss: 0.00001639
Iteration 46/1000 | Loss: 0.00001639
Iteration 47/1000 | Loss: 0.00001638
Iteration 48/1000 | Loss: 0.00001637
Iteration 49/1000 | Loss: 0.00001637
Iteration 50/1000 | Loss: 0.00001637
Iteration 51/1000 | Loss: 0.00001637
Iteration 52/1000 | Loss: 0.00001637
Iteration 53/1000 | Loss: 0.00001637
Iteration 54/1000 | Loss: 0.00001637
Iteration 55/1000 | Loss: 0.00001637
Iteration 56/1000 | Loss: 0.00001637
Iteration 57/1000 | Loss: 0.00001637
Iteration 58/1000 | Loss: 0.00001637
Iteration 59/1000 | Loss: 0.00001636
Iteration 60/1000 | Loss: 0.00001635
Iteration 61/1000 | Loss: 0.00001635
Iteration 62/1000 | Loss: 0.00001635
Iteration 63/1000 | Loss: 0.00001635
Iteration 64/1000 | Loss: 0.00001635
Iteration 65/1000 | Loss: 0.00001635
Iteration 66/1000 | Loss: 0.00001635
Iteration 67/1000 | Loss: 0.00001635
Iteration 68/1000 | Loss: 0.00001635
Iteration 69/1000 | Loss: 0.00001635
Iteration 70/1000 | Loss: 0.00001635
Iteration 71/1000 | Loss: 0.00001635
Iteration 72/1000 | Loss: 0.00001635
Iteration 73/1000 | Loss: 0.00001635
Iteration 74/1000 | Loss: 0.00001635
Iteration 75/1000 | Loss: 0.00001635
Iteration 76/1000 | Loss: 0.00001635
Iteration 77/1000 | Loss: 0.00001635
Iteration 78/1000 | Loss: 0.00001635
Iteration 79/1000 | Loss: 0.00001635
Iteration 80/1000 | Loss: 0.00001635
Iteration 81/1000 | Loss: 0.00001635
Iteration 82/1000 | Loss: 0.00001635
Iteration 83/1000 | Loss: 0.00001635
Iteration 84/1000 | Loss: 0.00001635
Iteration 85/1000 | Loss: 0.00001635
Iteration 86/1000 | Loss: 0.00001635
Iteration 87/1000 | Loss: 0.00001635
Iteration 88/1000 | Loss: 0.00001635
Iteration 89/1000 | Loss: 0.00001635
Iteration 90/1000 | Loss: 0.00001635
Iteration 91/1000 | Loss: 0.00001635
Iteration 92/1000 | Loss: 0.00001635
Iteration 93/1000 | Loss: 0.00001635
Iteration 94/1000 | Loss: 0.00001635
Iteration 95/1000 | Loss: 0.00001635
Iteration 96/1000 | Loss: 0.00001635
Iteration 97/1000 | Loss: 0.00001635
Iteration 98/1000 | Loss: 0.00001635
Iteration 99/1000 | Loss: 0.00001635
Iteration 100/1000 | Loss: 0.00001635
Iteration 101/1000 | Loss: 0.00001635
Iteration 102/1000 | Loss: 0.00001635
Iteration 103/1000 | Loss: 0.00001635
Iteration 104/1000 | Loss: 0.00001635
Iteration 105/1000 | Loss: 0.00001635
Iteration 106/1000 | Loss: 0.00001635
Iteration 107/1000 | Loss: 0.00001635
Iteration 108/1000 | Loss: 0.00001635
Iteration 109/1000 | Loss: 0.00001635
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.6349527868442237e-05, 1.6349527868442237e-05, 1.6349527868442237e-05, 1.6349527868442237e-05, 1.6349527868442237e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6349527868442237e-05

Optimization complete. Final v2v error: 3.4196131229400635 mm

Highest mean error: 3.5828261375427246 mm for frame 112

Lowest mean error: 2.6973679065704346 mm for frame 1

Saving results

Total time: 30.025492668151855
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818965
Iteration 2/25 | Loss: 0.00138774
Iteration 3/25 | Loss: 0.00115494
Iteration 4/25 | Loss: 0.00113678
Iteration 5/25 | Loss: 0.00113358
Iteration 6/25 | Loss: 0.00113355
Iteration 7/25 | Loss: 0.00113355
Iteration 8/25 | Loss: 0.00113355
Iteration 9/25 | Loss: 0.00113355
Iteration 10/25 | Loss: 0.00113355
Iteration 11/25 | Loss: 0.00113355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011335507733747363, 0.0011335507733747363, 0.0011335507733747363, 0.0011335507733747363, 0.0011335507733747363]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011335507733747363

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90818954
Iteration 2/25 | Loss: 0.00046538
Iteration 3/25 | Loss: 0.00046538
Iteration 4/25 | Loss: 0.00046538
Iteration 5/25 | Loss: 0.00046538
Iteration 6/25 | Loss: 0.00046538
Iteration 7/25 | Loss: 0.00046538
Iteration 8/25 | Loss: 0.00046538
Iteration 9/25 | Loss: 0.00046538
Iteration 10/25 | Loss: 0.00046538
Iteration 11/25 | Loss: 0.00046538
Iteration 12/25 | Loss: 0.00046538
Iteration 13/25 | Loss: 0.00046538
Iteration 14/25 | Loss: 0.00046538
Iteration 15/25 | Loss: 0.00046538
Iteration 16/25 | Loss: 0.00046538
Iteration 17/25 | Loss: 0.00046538
Iteration 18/25 | Loss: 0.00046538
Iteration 19/25 | Loss: 0.00046538
Iteration 20/25 | Loss: 0.00046538
Iteration 21/25 | Loss: 0.00046538
Iteration 22/25 | Loss: 0.00046538
Iteration 23/25 | Loss: 0.00046538
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.00046537574962712824, 0.00046537574962712824, 0.00046537574962712824, 0.00046537574962712824, 0.00046537574962712824]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00046537574962712824

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00046538
Iteration 2/1000 | Loss: 0.00002886
Iteration 3/1000 | Loss: 0.00002311
Iteration 4/1000 | Loss: 0.00002161
Iteration 5/1000 | Loss: 0.00002077
Iteration 6/1000 | Loss: 0.00002026
Iteration 7/1000 | Loss: 0.00001992
Iteration 8/1000 | Loss: 0.00001961
Iteration 9/1000 | Loss: 0.00001942
Iteration 10/1000 | Loss: 0.00001937
Iteration 11/1000 | Loss: 0.00001935
Iteration 12/1000 | Loss: 0.00001935
Iteration 13/1000 | Loss: 0.00001935
Iteration 14/1000 | Loss: 0.00001934
Iteration 15/1000 | Loss: 0.00001934
Iteration 16/1000 | Loss: 0.00001933
Iteration 17/1000 | Loss: 0.00001928
Iteration 18/1000 | Loss: 0.00001920
Iteration 19/1000 | Loss: 0.00001920
Iteration 20/1000 | Loss: 0.00001920
Iteration 21/1000 | Loss: 0.00001920
Iteration 22/1000 | Loss: 0.00001920
Iteration 23/1000 | Loss: 0.00001920
Iteration 24/1000 | Loss: 0.00001919
Iteration 25/1000 | Loss: 0.00001919
Iteration 26/1000 | Loss: 0.00001919
Iteration 27/1000 | Loss: 0.00001919
Iteration 28/1000 | Loss: 0.00001909
Iteration 29/1000 | Loss: 0.00001908
Iteration 30/1000 | Loss: 0.00001906
Iteration 31/1000 | Loss: 0.00001906
Iteration 32/1000 | Loss: 0.00001906
Iteration 33/1000 | Loss: 0.00001906
Iteration 34/1000 | Loss: 0.00001906
Iteration 35/1000 | Loss: 0.00001906
Iteration 36/1000 | Loss: 0.00001906
Iteration 37/1000 | Loss: 0.00001905
Iteration 38/1000 | Loss: 0.00001905
Iteration 39/1000 | Loss: 0.00001904
Iteration 40/1000 | Loss: 0.00001904
Iteration 41/1000 | Loss: 0.00001904
Iteration 42/1000 | Loss: 0.00001903
Iteration 43/1000 | Loss: 0.00001903
Iteration 44/1000 | Loss: 0.00001903
Iteration 45/1000 | Loss: 0.00001903
Iteration 46/1000 | Loss: 0.00001903
Iteration 47/1000 | Loss: 0.00001900
Iteration 48/1000 | Loss: 0.00001899
Iteration 49/1000 | Loss: 0.00001899
Iteration 50/1000 | Loss: 0.00001898
Iteration 51/1000 | Loss: 0.00001898
Iteration 52/1000 | Loss: 0.00001898
Iteration 53/1000 | Loss: 0.00001898
Iteration 54/1000 | Loss: 0.00001898
Iteration 55/1000 | Loss: 0.00001898
Iteration 56/1000 | Loss: 0.00001898
Iteration 57/1000 | Loss: 0.00001898
Iteration 58/1000 | Loss: 0.00001897
Iteration 59/1000 | Loss: 0.00001897
Iteration 60/1000 | Loss: 0.00001897
Iteration 61/1000 | Loss: 0.00001897
Iteration 62/1000 | Loss: 0.00001897
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001897
Iteration 65/1000 | Loss: 0.00001897
Iteration 66/1000 | Loss: 0.00001897
Iteration 67/1000 | Loss: 0.00001897
Iteration 68/1000 | Loss: 0.00001897
Iteration 69/1000 | Loss: 0.00001897
Iteration 70/1000 | Loss: 0.00001896
Iteration 71/1000 | Loss: 0.00001896
Iteration 72/1000 | Loss: 0.00001896
Iteration 73/1000 | Loss: 0.00001894
Iteration 74/1000 | Loss: 0.00001894
Iteration 75/1000 | Loss: 0.00001894
Iteration 76/1000 | Loss: 0.00001894
Iteration 77/1000 | Loss: 0.00001894
Iteration 78/1000 | Loss: 0.00001894
Iteration 79/1000 | Loss: 0.00001894
Iteration 80/1000 | Loss: 0.00001894
Iteration 81/1000 | Loss: 0.00001893
Iteration 82/1000 | Loss: 0.00001893
Iteration 83/1000 | Loss: 0.00001893
Iteration 84/1000 | Loss: 0.00001893
Iteration 85/1000 | Loss: 0.00001893
Iteration 86/1000 | Loss: 0.00001893
Iteration 87/1000 | Loss: 0.00001892
Iteration 88/1000 | Loss: 0.00001892
Iteration 89/1000 | Loss: 0.00001892
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001891
Iteration 92/1000 | Loss: 0.00001891
Iteration 93/1000 | Loss: 0.00001891
Iteration 94/1000 | Loss: 0.00001891
Iteration 95/1000 | Loss: 0.00001891
Iteration 96/1000 | Loss: 0.00001891
Iteration 97/1000 | Loss: 0.00001891
Iteration 98/1000 | Loss: 0.00001891
Iteration 99/1000 | Loss: 0.00001891
Iteration 100/1000 | Loss: 0.00001891
Iteration 101/1000 | Loss: 0.00001891
Iteration 102/1000 | Loss: 0.00001891
Iteration 103/1000 | Loss: 0.00001891
Iteration 104/1000 | Loss: 0.00001891
Iteration 105/1000 | Loss: 0.00001891
Iteration 106/1000 | Loss: 0.00001891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.8912651285063475e-05, 1.8912651285063475e-05, 1.8912651285063475e-05, 1.8912651285063475e-05, 1.8912651285063475e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8912651285063475e-05

Optimization complete. Final v2v error: 3.622498035430908 mm

Highest mean error: 3.8335304260253906 mm for frame 153

Lowest mean error: 3.448381185531616 mm for frame 186

Saving results

Total time: 35.13023924827576
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814966
Iteration 2/25 | Loss: 0.00115907
Iteration 3/25 | Loss: 0.00106304
Iteration 4/25 | Loss: 0.00105333
Iteration 5/25 | Loss: 0.00105147
Iteration 6/25 | Loss: 0.00105147
Iteration 7/25 | Loss: 0.00105147
Iteration 8/25 | Loss: 0.00105147
Iteration 9/25 | Loss: 0.00105147
Iteration 10/25 | Loss: 0.00105147
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010514731984585524, 0.0010514731984585524, 0.0010514731984585524, 0.0010514731984585524, 0.0010514731984585524]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010514731984585524

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35637593
Iteration 2/25 | Loss: 0.00075039
Iteration 3/25 | Loss: 0.00075039
Iteration 4/25 | Loss: 0.00075039
Iteration 5/25 | Loss: 0.00075039
Iteration 6/25 | Loss: 0.00075039
Iteration 7/25 | Loss: 0.00075039
Iteration 8/25 | Loss: 0.00075039
Iteration 9/25 | Loss: 0.00075039
Iteration 10/25 | Loss: 0.00075039
Iteration 11/25 | Loss: 0.00075039
Iteration 12/25 | Loss: 0.00075039
Iteration 13/25 | Loss: 0.00075039
Iteration 14/25 | Loss: 0.00075039
Iteration 15/25 | Loss: 0.00075039
Iteration 16/25 | Loss: 0.00075039
Iteration 17/25 | Loss: 0.00075039
Iteration 18/25 | Loss: 0.00075039
Iteration 19/25 | Loss: 0.00075039
Iteration 20/25 | Loss: 0.00075039
Iteration 21/25 | Loss: 0.00075039
Iteration 22/25 | Loss: 0.00075039
Iteration 23/25 | Loss: 0.00075039
Iteration 24/25 | Loss: 0.00075039
Iteration 25/25 | Loss: 0.00075039

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00075039
Iteration 2/1000 | Loss: 0.00001834
Iteration 3/1000 | Loss: 0.00001276
Iteration 4/1000 | Loss: 0.00001129
Iteration 5/1000 | Loss: 0.00001053
Iteration 6/1000 | Loss: 0.00000997
Iteration 7/1000 | Loss: 0.00000969
Iteration 8/1000 | Loss: 0.00000949
Iteration 9/1000 | Loss: 0.00000919
Iteration 10/1000 | Loss: 0.00000908
Iteration 11/1000 | Loss: 0.00000905
Iteration 12/1000 | Loss: 0.00000904
Iteration 13/1000 | Loss: 0.00000898
Iteration 14/1000 | Loss: 0.00000895
Iteration 15/1000 | Loss: 0.00000893
Iteration 16/1000 | Loss: 0.00000893
Iteration 17/1000 | Loss: 0.00000892
Iteration 18/1000 | Loss: 0.00000892
Iteration 19/1000 | Loss: 0.00000891
Iteration 20/1000 | Loss: 0.00000889
Iteration 21/1000 | Loss: 0.00000889
Iteration 22/1000 | Loss: 0.00000888
Iteration 23/1000 | Loss: 0.00000887
Iteration 24/1000 | Loss: 0.00000886
Iteration 25/1000 | Loss: 0.00000886
Iteration 26/1000 | Loss: 0.00000885
Iteration 27/1000 | Loss: 0.00000885
Iteration 28/1000 | Loss: 0.00000882
Iteration 29/1000 | Loss: 0.00000880
Iteration 30/1000 | Loss: 0.00000880
Iteration 31/1000 | Loss: 0.00000880
Iteration 32/1000 | Loss: 0.00000879
Iteration 33/1000 | Loss: 0.00000879
Iteration 34/1000 | Loss: 0.00000879
Iteration 35/1000 | Loss: 0.00000879
Iteration 36/1000 | Loss: 0.00000879
Iteration 37/1000 | Loss: 0.00000879
Iteration 38/1000 | Loss: 0.00000879
Iteration 39/1000 | Loss: 0.00000878
Iteration 40/1000 | Loss: 0.00000878
Iteration 41/1000 | Loss: 0.00000877
Iteration 42/1000 | Loss: 0.00000877
Iteration 43/1000 | Loss: 0.00000876
Iteration 44/1000 | Loss: 0.00000876
Iteration 45/1000 | Loss: 0.00000876
Iteration 46/1000 | Loss: 0.00000874
Iteration 47/1000 | Loss: 0.00000874
Iteration 48/1000 | Loss: 0.00000874
Iteration 49/1000 | Loss: 0.00000873
Iteration 50/1000 | Loss: 0.00000872
Iteration 51/1000 | Loss: 0.00000872
Iteration 52/1000 | Loss: 0.00000872
Iteration 53/1000 | Loss: 0.00000871
Iteration 54/1000 | Loss: 0.00000871
Iteration 55/1000 | Loss: 0.00000870
Iteration 56/1000 | Loss: 0.00000870
Iteration 57/1000 | Loss: 0.00000869
Iteration 58/1000 | Loss: 0.00000869
Iteration 59/1000 | Loss: 0.00000868
Iteration 60/1000 | Loss: 0.00000867
Iteration 61/1000 | Loss: 0.00000867
Iteration 62/1000 | Loss: 0.00000866
Iteration 63/1000 | Loss: 0.00000865
Iteration 64/1000 | Loss: 0.00000865
Iteration 65/1000 | Loss: 0.00000864
Iteration 66/1000 | Loss: 0.00000864
Iteration 67/1000 | Loss: 0.00000864
Iteration 68/1000 | Loss: 0.00000864
Iteration 69/1000 | Loss: 0.00000863
Iteration 70/1000 | Loss: 0.00000863
Iteration 71/1000 | Loss: 0.00000863
Iteration 72/1000 | Loss: 0.00000863
Iteration 73/1000 | Loss: 0.00000862
Iteration 74/1000 | Loss: 0.00000861
Iteration 75/1000 | Loss: 0.00000861
Iteration 76/1000 | Loss: 0.00000859
Iteration 77/1000 | Loss: 0.00000859
Iteration 78/1000 | Loss: 0.00000859
Iteration 79/1000 | Loss: 0.00000859
Iteration 80/1000 | Loss: 0.00000859
Iteration 81/1000 | Loss: 0.00000859
Iteration 82/1000 | Loss: 0.00000858
Iteration 83/1000 | Loss: 0.00000858
Iteration 84/1000 | Loss: 0.00000858
Iteration 85/1000 | Loss: 0.00000858
Iteration 86/1000 | Loss: 0.00000857
Iteration 87/1000 | Loss: 0.00000857
Iteration 88/1000 | Loss: 0.00000857
Iteration 89/1000 | Loss: 0.00000857
Iteration 90/1000 | Loss: 0.00000857
Iteration 91/1000 | Loss: 0.00000856
Iteration 92/1000 | Loss: 0.00000856
Iteration 93/1000 | Loss: 0.00000856
Iteration 94/1000 | Loss: 0.00000856
Iteration 95/1000 | Loss: 0.00000855
Iteration 96/1000 | Loss: 0.00000855
Iteration 97/1000 | Loss: 0.00000855
Iteration 98/1000 | Loss: 0.00000855
Iteration 99/1000 | Loss: 0.00000855
Iteration 100/1000 | Loss: 0.00000855
Iteration 101/1000 | Loss: 0.00000855
Iteration 102/1000 | Loss: 0.00000854
Iteration 103/1000 | Loss: 0.00000854
Iteration 104/1000 | Loss: 0.00000854
Iteration 105/1000 | Loss: 0.00000854
Iteration 106/1000 | Loss: 0.00000854
Iteration 107/1000 | Loss: 0.00000853
Iteration 108/1000 | Loss: 0.00000853
Iteration 109/1000 | Loss: 0.00000852
Iteration 110/1000 | Loss: 0.00000852
Iteration 111/1000 | Loss: 0.00000852
Iteration 112/1000 | Loss: 0.00000852
Iteration 113/1000 | Loss: 0.00000852
Iteration 114/1000 | Loss: 0.00000852
Iteration 115/1000 | Loss: 0.00000852
Iteration 116/1000 | Loss: 0.00000852
Iteration 117/1000 | Loss: 0.00000852
Iteration 118/1000 | Loss: 0.00000852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [8.519426046404988e-06, 8.519426046404988e-06, 8.519426046404988e-06, 8.519426046404988e-06, 8.519426046404988e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.519426046404988e-06

Optimization complete. Final v2v error: 2.4939959049224854 mm

Highest mean error: 2.6695644855499268 mm for frame 57

Lowest mean error: 2.37260103225708 mm for frame 6

Saving results

Total time: 37.04068851470947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01024159
Iteration 2/25 | Loss: 0.01024159
Iteration 3/25 | Loss: 0.01024158
Iteration 4/25 | Loss: 0.00311731
Iteration 5/25 | Loss: 0.00206380
Iteration 6/25 | Loss: 0.00200083
Iteration 7/25 | Loss: 0.00177851
Iteration 8/25 | Loss: 0.00171965
Iteration 9/25 | Loss: 0.00163523
Iteration 10/25 | Loss: 0.00160119
Iteration 11/25 | Loss: 0.00155373
Iteration 12/25 | Loss: 0.00154460
Iteration 13/25 | Loss: 0.00154654
Iteration 14/25 | Loss: 0.00151959
Iteration 15/25 | Loss: 0.00148905
Iteration 16/25 | Loss: 0.00148483
Iteration 17/25 | Loss: 0.00147044
Iteration 18/25 | Loss: 0.00147319
Iteration 19/25 | Loss: 0.00146109
Iteration 20/25 | Loss: 0.00145371
Iteration 21/25 | Loss: 0.00145202
Iteration 22/25 | Loss: 0.00146102
Iteration 23/25 | Loss: 0.00145736
Iteration 24/25 | Loss: 0.00145735
Iteration 25/25 | Loss: 0.00144972

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30886900
Iteration 2/25 | Loss: 0.00544361
Iteration 3/25 | Loss: 0.00390490
Iteration 4/25 | Loss: 0.00390490
Iteration 5/25 | Loss: 0.00390490
Iteration 6/25 | Loss: 0.00390490
Iteration 7/25 | Loss: 0.00390490
Iteration 8/25 | Loss: 0.00390490
Iteration 9/25 | Loss: 0.00390490
Iteration 10/25 | Loss: 0.00390490
Iteration 11/25 | Loss: 0.00390490
Iteration 12/25 | Loss: 0.00390490
Iteration 13/25 | Loss: 0.00390490
Iteration 14/25 | Loss: 0.00390490
Iteration 15/25 | Loss: 0.00390490
Iteration 16/25 | Loss: 0.00390490
Iteration 17/25 | Loss: 0.00390490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0039048981852829456, 0.0039048981852829456, 0.0039048981852829456, 0.0039048981852829456, 0.0039048981852829456]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0039048981852829456

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00390490
Iteration 2/1000 | Loss: 0.00127084
Iteration 3/1000 | Loss: 0.00073864
Iteration 4/1000 | Loss: 0.00087334
Iteration 5/1000 | Loss: 0.00054569
Iteration 6/1000 | Loss: 0.00036188
Iteration 7/1000 | Loss: 0.00095308
Iteration 8/1000 | Loss: 0.00026379
Iteration 9/1000 | Loss: 0.00241557
Iteration 10/1000 | Loss: 0.00027531
Iteration 11/1000 | Loss: 0.00185469
Iteration 12/1000 | Loss: 0.00353270
Iteration 13/1000 | Loss: 0.00629018
Iteration 14/1000 | Loss: 0.00159105
Iteration 15/1000 | Loss: 0.00133224
Iteration 16/1000 | Loss: 0.00084725
Iteration 17/1000 | Loss: 0.00057472
Iteration 18/1000 | Loss: 0.00040795
Iteration 19/1000 | Loss: 0.00050861
Iteration 20/1000 | Loss: 0.00091010
Iteration 21/1000 | Loss: 0.00210508
Iteration 22/1000 | Loss: 0.00083393
Iteration 23/1000 | Loss: 0.00085408
Iteration 24/1000 | Loss: 0.00054877
Iteration 25/1000 | Loss: 0.00056268
Iteration 26/1000 | Loss: 0.00033821
Iteration 27/1000 | Loss: 0.00085773
Iteration 28/1000 | Loss: 0.00069490
Iteration 29/1000 | Loss: 0.00106347
Iteration 30/1000 | Loss: 0.00105720
Iteration 31/1000 | Loss: 0.00110204
Iteration 32/1000 | Loss: 0.00134826
Iteration 33/1000 | Loss: 0.00056020
Iteration 34/1000 | Loss: 0.00080193
Iteration 35/1000 | Loss: 0.00078548
Iteration 36/1000 | Loss: 0.00091884
Iteration 37/1000 | Loss: 0.00050567
Iteration 38/1000 | Loss: 0.00046152
Iteration 39/1000 | Loss: 0.00094204
Iteration 40/1000 | Loss: 0.00147554
Iteration 41/1000 | Loss: 0.00072000
Iteration 42/1000 | Loss: 0.00114651
Iteration 43/1000 | Loss: 0.00029961
Iteration 44/1000 | Loss: 0.00023403
Iteration 45/1000 | Loss: 0.00115141
Iteration 46/1000 | Loss: 0.00083062
Iteration 47/1000 | Loss: 0.00285950
Iteration 48/1000 | Loss: 0.00351439
Iteration 49/1000 | Loss: 0.00118673
Iteration 50/1000 | Loss: 0.00257306
Iteration 51/1000 | Loss: 0.00385913
Iteration 52/1000 | Loss: 0.00252480
Iteration 53/1000 | Loss: 0.00124614
Iteration 54/1000 | Loss: 0.00042916
Iteration 55/1000 | Loss: 0.00024624
Iteration 56/1000 | Loss: 0.00077429
Iteration 57/1000 | Loss: 0.00126528
Iteration 58/1000 | Loss: 0.00116666
Iteration 59/1000 | Loss: 0.00162198
Iteration 60/1000 | Loss: 0.00168104
Iteration 61/1000 | Loss: 0.00154641
Iteration 62/1000 | Loss: 0.00065505
Iteration 63/1000 | Loss: 0.00025956
Iteration 64/1000 | Loss: 0.00014642
Iteration 65/1000 | Loss: 0.00139795
Iteration 66/1000 | Loss: 0.00115864
Iteration 67/1000 | Loss: 0.00033878
Iteration 68/1000 | Loss: 0.00113843
Iteration 69/1000 | Loss: 0.00029557
Iteration 70/1000 | Loss: 0.00028133
Iteration 71/1000 | Loss: 0.00065195
Iteration 72/1000 | Loss: 0.00040340
Iteration 73/1000 | Loss: 0.00030081
Iteration 74/1000 | Loss: 0.00037027
Iteration 75/1000 | Loss: 0.00042038
Iteration 76/1000 | Loss: 0.00106598
Iteration 77/1000 | Loss: 0.00075158
Iteration 78/1000 | Loss: 0.00069256
Iteration 79/1000 | Loss: 0.00086487
Iteration 80/1000 | Loss: 0.00011393
Iteration 81/1000 | Loss: 0.00010485
Iteration 82/1000 | Loss: 0.00012221
Iteration 83/1000 | Loss: 0.00021343
Iteration 84/1000 | Loss: 0.00082664
Iteration 85/1000 | Loss: 0.00045552
Iteration 86/1000 | Loss: 0.00014459
Iteration 87/1000 | Loss: 0.00028173
Iteration 88/1000 | Loss: 0.00010767
Iteration 89/1000 | Loss: 0.00009558
Iteration 90/1000 | Loss: 0.00040484
Iteration 91/1000 | Loss: 0.00009378
Iteration 92/1000 | Loss: 0.00009195
Iteration 93/1000 | Loss: 0.00009085
Iteration 94/1000 | Loss: 0.00029658
Iteration 95/1000 | Loss: 0.00019001
Iteration 96/1000 | Loss: 0.00018012
Iteration 97/1000 | Loss: 0.00022506
Iteration 98/1000 | Loss: 0.00008922
Iteration 99/1000 | Loss: 0.00013273
Iteration 100/1000 | Loss: 0.00008813
Iteration 101/1000 | Loss: 0.00008778
Iteration 102/1000 | Loss: 0.00022787
Iteration 103/1000 | Loss: 0.00015365
Iteration 104/1000 | Loss: 0.00029435
Iteration 105/1000 | Loss: 0.00036031
Iteration 106/1000 | Loss: 0.00013618
Iteration 107/1000 | Loss: 0.00009928
Iteration 108/1000 | Loss: 0.00008819
Iteration 109/1000 | Loss: 0.00008714
Iteration 110/1000 | Loss: 0.00017299
Iteration 111/1000 | Loss: 0.00010069
Iteration 112/1000 | Loss: 0.00008679
Iteration 113/1000 | Loss: 0.00020479
Iteration 114/1000 | Loss: 0.00013340
Iteration 115/1000 | Loss: 0.00073787
Iteration 116/1000 | Loss: 0.00009793
Iteration 117/1000 | Loss: 0.00010103
Iteration 118/1000 | Loss: 0.00008677
Iteration 119/1000 | Loss: 0.00012051
Iteration 120/1000 | Loss: 0.00008651
Iteration 121/1000 | Loss: 0.00018398
Iteration 122/1000 | Loss: 0.00011157
Iteration 123/1000 | Loss: 0.00017672
Iteration 124/1000 | Loss: 0.00008661
Iteration 125/1000 | Loss: 0.00012699
Iteration 126/1000 | Loss: 0.00008585
Iteration 127/1000 | Loss: 0.00018482
Iteration 128/1000 | Loss: 0.00008578
Iteration 129/1000 | Loss: 0.00008439
Iteration 130/1000 | Loss: 0.00029685
Iteration 131/1000 | Loss: 0.00008240
Iteration 132/1000 | Loss: 0.00008103
Iteration 133/1000 | Loss: 0.00031171
Iteration 134/1000 | Loss: 0.00007862
Iteration 135/1000 | Loss: 0.00012249
Iteration 136/1000 | Loss: 0.00015149
Iteration 137/1000 | Loss: 0.00007625
Iteration 138/1000 | Loss: 0.00007508
Iteration 139/1000 | Loss: 0.00016152
Iteration 140/1000 | Loss: 0.00032774
Iteration 141/1000 | Loss: 0.00047073
Iteration 142/1000 | Loss: 0.00007690
Iteration 143/1000 | Loss: 0.00007352
Iteration 144/1000 | Loss: 0.00020414
Iteration 145/1000 | Loss: 0.00026431
Iteration 146/1000 | Loss: 0.00008461
Iteration 147/1000 | Loss: 0.00020106
Iteration 148/1000 | Loss: 0.00031995
Iteration 149/1000 | Loss: 0.00008292
Iteration 150/1000 | Loss: 0.00039130
Iteration 151/1000 | Loss: 0.00222501
Iteration 152/1000 | Loss: 0.00106208
Iteration 153/1000 | Loss: 0.00050296
Iteration 154/1000 | Loss: 0.00013753
Iteration 155/1000 | Loss: 0.00034248
Iteration 156/1000 | Loss: 0.00016469
Iteration 157/1000 | Loss: 0.00086830
Iteration 158/1000 | Loss: 0.00011080
Iteration 159/1000 | Loss: 0.00009832
Iteration 160/1000 | Loss: 0.00006905
Iteration 161/1000 | Loss: 0.00007568
Iteration 162/1000 | Loss: 0.00004605
Iteration 163/1000 | Loss: 0.00004769
Iteration 164/1000 | Loss: 0.00032944
Iteration 165/1000 | Loss: 0.00008920
Iteration 166/1000 | Loss: 0.00004396
Iteration 167/1000 | Loss: 0.00003640
Iteration 168/1000 | Loss: 0.00015110
Iteration 169/1000 | Loss: 0.00025570
Iteration 170/1000 | Loss: 0.00105099
Iteration 171/1000 | Loss: 0.00005280
Iteration 172/1000 | Loss: 0.00022350
Iteration 173/1000 | Loss: 0.00037417
Iteration 174/1000 | Loss: 0.00004181
Iteration 175/1000 | Loss: 0.00003345
Iteration 176/1000 | Loss: 0.00003161
Iteration 177/1000 | Loss: 0.00024829
Iteration 178/1000 | Loss: 0.00003032
Iteration 179/1000 | Loss: 0.00009066
Iteration 180/1000 | Loss: 0.00048706
Iteration 181/1000 | Loss: 0.00004307
Iteration 182/1000 | Loss: 0.00004196
Iteration 183/1000 | Loss: 0.00003944
Iteration 184/1000 | Loss: 0.00003025
Iteration 185/1000 | Loss: 0.00002872
Iteration 186/1000 | Loss: 0.00011568
Iteration 187/1000 | Loss: 0.00002826
Iteration 188/1000 | Loss: 0.00002797
Iteration 189/1000 | Loss: 0.00002770
Iteration 190/1000 | Loss: 0.00002762
Iteration 191/1000 | Loss: 0.00002758
Iteration 192/1000 | Loss: 0.00002757
Iteration 193/1000 | Loss: 0.00002756
Iteration 194/1000 | Loss: 0.00002756
Iteration 195/1000 | Loss: 0.00002756
Iteration 196/1000 | Loss: 0.00002755
Iteration 197/1000 | Loss: 0.00002755
Iteration 198/1000 | Loss: 0.00002755
Iteration 199/1000 | Loss: 0.00002755
Iteration 200/1000 | Loss: 0.00002754
Iteration 201/1000 | Loss: 0.00002754
Iteration 202/1000 | Loss: 0.00002754
Iteration 203/1000 | Loss: 0.00002754
Iteration 204/1000 | Loss: 0.00002751
Iteration 205/1000 | Loss: 0.00002750
Iteration 206/1000 | Loss: 0.00002750
Iteration 207/1000 | Loss: 0.00002749
Iteration 208/1000 | Loss: 0.00002749
Iteration 209/1000 | Loss: 0.00002749
Iteration 210/1000 | Loss: 0.00002749
Iteration 211/1000 | Loss: 0.00002749
Iteration 212/1000 | Loss: 0.00002748
Iteration 213/1000 | Loss: 0.00002748
Iteration 214/1000 | Loss: 0.00002748
Iteration 215/1000 | Loss: 0.00002747
Iteration 216/1000 | Loss: 0.00002746
Iteration 217/1000 | Loss: 0.00002746
Iteration 218/1000 | Loss: 0.00002746
Iteration 219/1000 | Loss: 0.00002745
Iteration 220/1000 | Loss: 0.00002745
Iteration 221/1000 | Loss: 0.00002745
Iteration 222/1000 | Loss: 0.00002745
Iteration 223/1000 | Loss: 0.00002744
Iteration 224/1000 | Loss: 0.00002744
Iteration 225/1000 | Loss: 0.00002744
Iteration 226/1000 | Loss: 0.00002744
Iteration 227/1000 | Loss: 0.00002743
Iteration 228/1000 | Loss: 0.00002740
Iteration 229/1000 | Loss: 0.00002735
Iteration 230/1000 | Loss: 0.00002735
Iteration 231/1000 | Loss: 0.00002723
Iteration 232/1000 | Loss: 0.00002722
Iteration 233/1000 | Loss: 0.00002705
Iteration 234/1000 | Loss: 0.00002702
Iteration 235/1000 | Loss: 0.00014462
Iteration 236/1000 | Loss: 0.00014343
Iteration 237/1000 | Loss: 0.00024023
Iteration 238/1000 | Loss: 0.00017698
Iteration 239/1000 | Loss: 0.00062459
Iteration 240/1000 | Loss: 0.00013325
Iteration 241/1000 | Loss: 0.00021601
Iteration 242/1000 | Loss: 0.00016821
Iteration 243/1000 | Loss: 0.00003490
Iteration 244/1000 | Loss: 0.00003082
Iteration 245/1000 | Loss: 0.00002884
Iteration 246/1000 | Loss: 0.00009520
Iteration 247/1000 | Loss: 0.00049908
Iteration 248/1000 | Loss: 0.00016858
Iteration 249/1000 | Loss: 0.00005391
Iteration 250/1000 | Loss: 0.00002836
Iteration 251/1000 | Loss: 0.00002628
Iteration 252/1000 | Loss: 0.00002564
Iteration 253/1000 | Loss: 0.00022202
Iteration 254/1000 | Loss: 0.00020042
Iteration 255/1000 | Loss: 0.00021220
Iteration 256/1000 | Loss: 0.00003911
Iteration 257/1000 | Loss: 0.00003136
Iteration 258/1000 | Loss: 0.00002819
Iteration 259/1000 | Loss: 0.00002676
Iteration 260/1000 | Loss: 0.00002594
Iteration 261/1000 | Loss: 0.00002527
Iteration 262/1000 | Loss: 0.00002476
Iteration 263/1000 | Loss: 0.00002437
Iteration 264/1000 | Loss: 0.00002415
Iteration 265/1000 | Loss: 0.00002402
Iteration 266/1000 | Loss: 0.00002398
Iteration 267/1000 | Loss: 0.00002398
Iteration 268/1000 | Loss: 0.00002393
Iteration 269/1000 | Loss: 0.00002392
Iteration 270/1000 | Loss: 0.00002390
Iteration 271/1000 | Loss: 0.00002390
Iteration 272/1000 | Loss: 0.00002389
Iteration 273/1000 | Loss: 0.00002387
Iteration 274/1000 | Loss: 0.00002387
Iteration 275/1000 | Loss: 0.00002387
Iteration 276/1000 | Loss: 0.00002387
Iteration 277/1000 | Loss: 0.00002387
Iteration 278/1000 | Loss: 0.00002387
Iteration 279/1000 | Loss: 0.00002387
Iteration 280/1000 | Loss: 0.00002387
Iteration 281/1000 | Loss: 0.00002387
Iteration 282/1000 | Loss: 0.00002387
Iteration 283/1000 | Loss: 0.00002387
Iteration 284/1000 | Loss: 0.00002387
Iteration 285/1000 | Loss: 0.00002386
Iteration 286/1000 | Loss: 0.00002386
Iteration 287/1000 | Loss: 0.00002386
Iteration 288/1000 | Loss: 0.00002386
Iteration 289/1000 | Loss: 0.00002386
Iteration 290/1000 | Loss: 0.00002386
Iteration 291/1000 | Loss: 0.00002386
Iteration 292/1000 | Loss: 0.00002386
Iteration 293/1000 | Loss: 0.00002386
Iteration 294/1000 | Loss: 0.00002386
Iteration 295/1000 | Loss: 0.00002386
Iteration 296/1000 | Loss: 0.00002386
Iteration 297/1000 | Loss: 0.00002385
Iteration 298/1000 | Loss: 0.00002385
Iteration 299/1000 | Loss: 0.00002385
Iteration 300/1000 | Loss: 0.00002385
Iteration 301/1000 | Loss: 0.00002385
Iteration 302/1000 | Loss: 0.00002385
Iteration 303/1000 | Loss: 0.00002385
Iteration 304/1000 | Loss: 0.00002384
Iteration 305/1000 | Loss: 0.00002384
Iteration 306/1000 | Loss: 0.00002384
Iteration 307/1000 | Loss: 0.00002383
Iteration 308/1000 | Loss: 0.00002383
Iteration 309/1000 | Loss: 0.00009185
Iteration 310/1000 | Loss: 0.00002921
Iteration 311/1000 | Loss: 0.00002466
Iteration 312/1000 | Loss: 0.00007583
Iteration 313/1000 | Loss: 0.00002381
Iteration 314/1000 | Loss: 0.00002379
Iteration 315/1000 | Loss: 0.00002376
Iteration 316/1000 | Loss: 0.00002376
Iteration 317/1000 | Loss: 0.00002376
Iteration 318/1000 | Loss: 0.00002376
Iteration 319/1000 | Loss: 0.00002375
Iteration 320/1000 | Loss: 0.00002375
Iteration 321/1000 | Loss: 0.00002375
Iteration 322/1000 | Loss: 0.00002375
Iteration 323/1000 | Loss: 0.00002374
Iteration 324/1000 | Loss: 0.00002373
Iteration 325/1000 | Loss: 0.00002373
Iteration 326/1000 | Loss: 0.00002373
Iteration 327/1000 | Loss: 0.00002372
Iteration 328/1000 | Loss: 0.00002372
Iteration 329/1000 | Loss: 0.00002372
Iteration 330/1000 | Loss: 0.00002372
Iteration 331/1000 | Loss: 0.00002372
Iteration 332/1000 | Loss: 0.00002371
Iteration 333/1000 | Loss: 0.00002371
Iteration 334/1000 | Loss: 0.00002371
Iteration 335/1000 | Loss: 0.00002370
Iteration 336/1000 | Loss: 0.00002370
Iteration 337/1000 | Loss: 0.00002370
Iteration 338/1000 | Loss: 0.00002369
Iteration 339/1000 | Loss: 0.00002369
Iteration 340/1000 | Loss: 0.00002369
Iteration 341/1000 | Loss: 0.00002369
Iteration 342/1000 | Loss: 0.00002369
Iteration 343/1000 | Loss: 0.00002369
Iteration 344/1000 | Loss: 0.00002368
Iteration 345/1000 | Loss: 0.00002368
Iteration 346/1000 | Loss: 0.00002368
Iteration 347/1000 | Loss: 0.00002368
Iteration 348/1000 | Loss: 0.00002367
Iteration 349/1000 | Loss: 0.00002367
Iteration 350/1000 | Loss: 0.00002366
Iteration 351/1000 | Loss: 0.00002364
Iteration 352/1000 | Loss: 0.00002351
Iteration 353/1000 | Loss: 0.00029791
Iteration 354/1000 | Loss: 0.00012361
Iteration 355/1000 | Loss: 0.00017701
Iteration 356/1000 | Loss: 0.00030680
Iteration 357/1000 | Loss: 0.00029581
Iteration 358/1000 | Loss: 0.00015415
Iteration 359/1000 | Loss: 0.00003120
Iteration 360/1000 | Loss: 0.00002416
Iteration 361/1000 | Loss: 0.00004373
Iteration 362/1000 | Loss: 0.00002348
Iteration 363/1000 | Loss: 0.00013086
Iteration 364/1000 | Loss: 0.00036908
Iteration 365/1000 | Loss: 0.00014101
Iteration 366/1000 | Loss: 0.00003358
Iteration 367/1000 | Loss: 0.00002608
Iteration 368/1000 | Loss: 0.00002405
Iteration 369/1000 | Loss: 0.00002316
Iteration 370/1000 | Loss: 0.00002267
Iteration 371/1000 | Loss: 0.00002244
Iteration 372/1000 | Loss: 0.00002232
Iteration 373/1000 | Loss: 0.00002224
Iteration 374/1000 | Loss: 0.00002201
Iteration 375/1000 | Loss: 0.00014165
Iteration 376/1000 | Loss: 0.00022706
Iteration 377/1000 | Loss: 0.00012884
Iteration 378/1000 | Loss: 0.00012985
Iteration 379/1000 | Loss: 0.00023021
Iteration 380/1000 | Loss: 0.00003896
Iteration 381/1000 | Loss: 0.00002951
Iteration 382/1000 | Loss: 0.00019701
Iteration 383/1000 | Loss: 0.00015884
Iteration 384/1000 | Loss: 0.00004296
Iteration 385/1000 | Loss: 0.00008901
Iteration 386/1000 | Loss: 0.00002227
Iteration 387/1000 | Loss: 0.00002160
Iteration 388/1000 | Loss: 0.00002117
Iteration 389/1000 | Loss: 0.00002085
Iteration 390/1000 | Loss: 0.00002077
Iteration 391/1000 | Loss: 0.00002069
Iteration 392/1000 | Loss: 0.00002069
Iteration 393/1000 | Loss: 0.00002067
Iteration 394/1000 | Loss: 0.00002067
Iteration 395/1000 | Loss: 0.00002066
Iteration 396/1000 | Loss: 0.00002065
Iteration 397/1000 | Loss: 0.00002065
Iteration 398/1000 | Loss: 0.00002064
Iteration 399/1000 | Loss: 0.00002064
Iteration 400/1000 | Loss: 0.00002064
Iteration 401/1000 | Loss: 0.00002063
Iteration 402/1000 | Loss: 0.00002063
Iteration 403/1000 | Loss: 0.00002063
Iteration 404/1000 | Loss: 0.00002063
Iteration 405/1000 | Loss: 0.00002063
Iteration 406/1000 | Loss: 0.00002062
Iteration 407/1000 | Loss: 0.00002062
Iteration 408/1000 | Loss: 0.00002061
Iteration 409/1000 | Loss: 0.00002061
Iteration 410/1000 | Loss: 0.00002058
Iteration 411/1000 | Loss: 0.00002058
Iteration 412/1000 | Loss: 0.00002057
Iteration 413/1000 | Loss: 0.00002056
Iteration 414/1000 | Loss: 0.00002056
Iteration 415/1000 | Loss: 0.00002056
Iteration 416/1000 | Loss: 0.00002053
Iteration 417/1000 | Loss: 0.00002053
Iteration 418/1000 | Loss: 0.00002050
Iteration 419/1000 | Loss: 0.00002050
Iteration 420/1000 | Loss: 0.00002050
Iteration 421/1000 | Loss: 0.00002049
Iteration 422/1000 | Loss: 0.00002049
Iteration 423/1000 | Loss: 0.00002049
Iteration 424/1000 | Loss: 0.00002048
Iteration 425/1000 | Loss: 0.00002048
Iteration 426/1000 | Loss: 0.00002048
Iteration 427/1000 | Loss: 0.00002048
Iteration 428/1000 | Loss: 0.00002048
Iteration 429/1000 | Loss: 0.00002048
Iteration 430/1000 | Loss: 0.00002048
Iteration 431/1000 | Loss: 0.00002047
Iteration 432/1000 | Loss: 0.00002047
Iteration 433/1000 | Loss: 0.00002047
Iteration 434/1000 | Loss: 0.00002047
Iteration 435/1000 | Loss: 0.00002047
Iteration 436/1000 | Loss: 0.00002046
Iteration 437/1000 | Loss: 0.00002046
Iteration 438/1000 | Loss: 0.00002045
Iteration 439/1000 | Loss: 0.00002045
Iteration 440/1000 | Loss: 0.00002045
Iteration 441/1000 | Loss: 0.00002044
Iteration 442/1000 | Loss: 0.00002044
Iteration 443/1000 | Loss: 0.00002044
Iteration 444/1000 | Loss: 0.00002044
Iteration 445/1000 | Loss: 0.00002044
Iteration 446/1000 | Loss: 0.00002044
Iteration 447/1000 | Loss: 0.00002044
Iteration 448/1000 | Loss: 0.00002044
Iteration 449/1000 | Loss: 0.00002044
Iteration 450/1000 | Loss: 0.00002043
Iteration 451/1000 | Loss: 0.00002043
Iteration 452/1000 | Loss: 0.00002043
Iteration 453/1000 | Loss: 0.00002043
Iteration 454/1000 | Loss: 0.00002043
Iteration 455/1000 | Loss: 0.00002042
Iteration 456/1000 | Loss: 0.00002042
Iteration 457/1000 | Loss: 0.00002042
Iteration 458/1000 | Loss: 0.00002042
Iteration 459/1000 | Loss: 0.00002042
Iteration 460/1000 | Loss: 0.00002042
Iteration 461/1000 | Loss: 0.00002041
Iteration 462/1000 | Loss: 0.00002041
Iteration 463/1000 | Loss: 0.00002041
Iteration 464/1000 | Loss: 0.00002041
Iteration 465/1000 | Loss: 0.00002041
Iteration 466/1000 | Loss: 0.00002041
Iteration 467/1000 | Loss: 0.00002041
Iteration 468/1000 | Loss: 0.00002041
Iteration 469/1000 | Loss: 0.00002041
Iteration 470/1000 | Loss: 0.00002041
Iteration 471/1000 | Loss: 0.00002041
Iteration 472/1000 | Loss: 0.00002041
Iteration 473/1000 | Loss: 0.00002041
Iteration 474/1000 | Loss: 0.00002041
Iteration 475/1000 | Loss: 0.00002041
Iteration 476/1000 | Loss: 0.00002041
Iteration 477/1000 | Loss: 0.00002041
Iteration 478/1000 | Loss: 0.00002041
Iteration 479/1000 | Loss: 0.00002040
Iteration 480/1000 | Loss: 0.00002040
Iteration 481/1000 | Loss: 0.00002040
Iteration 482/1000 | Loss: 0.00002040
Iteration 483/1000 | Loss: 0.00002040
Iteration 484/1000 | Loss: 0.00002040
Iteration 485/1000 | Loss: 0.00002040
Iteration 486/1000 | Loss: 0.00002040
Iteration 487/1000 | Loss: 0.00002040
Iteration 488/1000 | Loss: 0.00002040
Iteration 489/1000 | Loss: 0.00002040
Iteration 490/1000 | Loss: 0.00002040
Iteration 491/1000 | Loss: 0.00002039
Iteration 492/1000 | Loss: 0.00002039
Iteration 493/1000 | Loss: 0.00002039
Iteration 494/1000 | Loss: 0.00002039
Iteration 495/1000 | Loss: 0.00002039
Iteration 496/1000 | Loss: 0.00002039
Iteration 497/1000 | Loss: 0.00002039
Iteration 498/1000 | Loss: 0.00002039
Iteration 499/1000 | Loss: 0.00002039
Iteration 500/1000 | Loss: 0.00002039
Iteration 501/1000 | Loss: 0.00002039
Iteration 502/1000 | Loss: 0.00002039
Iteration 503/1000 | Loss: 0.00002038
Iteration 504/1000 | Loss: 0.00002038
Iteration 505/1000 | Loss: 0.00002038
Iteration 506/1000 | Loss: 0.00002038
Iteration 507/1000 | Loss: 0.00002038
Iteration 508/1000 | Loss: 0.00002038
Iteration 509/1000 | Loss: 0.00002038
Iteration 510/1000 | Loss: 0.00002038
Iteration 511/1000 | Loss: 0.00002038
Iteration 512/1000 | Loss: 0.00002038
Iteration 513/1000 | Loss: 0.00002038
Iteration 514/1000 | Loss: 0.00002038
Iteration 515/1000 | Loss: 0.00002038
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 515. Stopping optimization.
Last 5 losses: [2.0382556613185443e-05, 2.0382556613185443e-05, 2.0382556613185443e-05, 2.0382556613185443e-05, 2.0382556613185443e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0382556613185443e-05

Optimization complete. Final v2v error: 3.2974698543548584 mm

Highest mean error: 10.759095191955566 mm for frame 73

Lowest mean error: 2.828418493270874 mm for frame 59

Saving results

Total time: 500.0590262413025
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003271
Iteration 2/25 | Loss: 0.01003271
Iteration 3/25 | Loss: 0.01003271
Iteration 4/25 | Loss: 0.01003271
Iteration 5/25 | Loss: 0.01003271
Iteration 6/25 | Loss: 0.01003270
Iteration 7/25 | Loss: 0.01003270
Iteration 8/25 | Loss: 0.01003270
Iteration 9/25 | Loss: 0.01003270
Iteration 10/25 | Loss: 0.01003270
Iteration 11/25 | Loss: 0.01003269
Iteration 12/25 | Loss: 0.01003269
Iteration 13/25 | Loss: 0.01003269
Iteration 14/25 | Loss: 0.01003269
Iteration 15/25 | Loss: 0.01003269
Iteration 16/25 | Loss: 0.01003269
Iteration 17/25 | Loss: 0.01003268
Iteration 18/25 | Loss: 0.01003268
Iteration 19/25 | Loss: 0.01003268
Iteration 20/25 | Loss: 0.01003268
Iteration 21/25 | Loss: 0.01003268
Iteration 22/25 | Loss: 0.01003267
Iteration 23/25 | Loss: 0.01003267
Iteration 24/25 | Loss: 0.01003267
Iteration 25/25 | Loss: 0.01003267

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67359459
Iteration 2/25 | Loss: 0.16676874
Iteration 3/25 | Loss: 0.16270980
Iteration 4/25 | Loss: 0.15810350
Iteration 5/25 | Loss: 0.15810347
Iteration 6/25 | Loss: 0.15810344
Iteration 7/25 | Loss: 0.15810344
Iteration 8/25 | Loss: 0.15810342
Iteration 9/25 | Loss: 0.15810339
Iteration 10/25 | Loss: 0.15810339
Iteration 11/25 | Loss: 0.15810339
Iteration 12/25 | Loss: 0.15810338
Iteration 13/25 | Loss: 0.15810338
Iteration 14/25 | Loss: 0.15810338
Iteration 15/25 | Loss: 0.15810338
Iteration 16/25 | Loss: 0.15810338
Iteration 17/25 | Loss: 0.15810338
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.15810337662696838, 0.15810337662696838, 0.15810337662696838, 0.15810337662696838, 0.15810337662696838]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.15810337662696838

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15810338
Iteration 2/1000 | Loss: 0.00572153
Iteration 3/1000 | Loss: 0.00247293
Iteration 4/1000 | Loss: 0.00068569
Iteration 5/1000 | Loss: 0.00040811
Iteration 6/1000 | Loss: 0.00043224
Iteration 7/1000 | Loss: 0.00140118
Iteration 8/1000 | Loss: 0.00025717
Iteration 9/1000 | Loss: 0.00042259
Iteration 10/1000 | Loss: 0.00032029
Iteration 11/1000 | Loss: 0.00006864
Iteration 12/1000 | Loss: 0.00048552
Iteration 13/1000 | Loss: 0.00005388
Iteration 14/1000 | Loss: 0.00034830
Iteration 15/1000 | Loss: 0.00004362
Iteration 16/1000 | Loss: 0.00019382
Iteration 17/1000 | Loss: 0.00022821
Iteration 18/1000 | Loss: 0.00028840
Iteration 19/1000 | Loss: 0.00003815
Iteration 20/1000 | Loss: 0.00003853
Iteration 21/1000 | Loss: 0.00020517
Iteration 22/1000 | Loss: 0.00003094
Iteration 23/1000 | Loss: 0.00015284
Iteration 24/1000 | Loss: 0.00002865
Iteration 25/1000 | Loss: 0.00008495
Iteration 26/1000 | Loss: 0.00002629
Iteration 27/1000 | Loss: 0.00023482
Iteration 28/1000 | Loss: 0.00037938
Iteration 29/1000 | Loss: 0.00004370
Iteration 30/1000 | Loss: 0.00002509
Iteration 31/1000 | Loss: 0.00002428
Iteration 32/1000 | Loss: 0.00029559
Iteration 33/1000 | Loss: 0.00002417
Iteration 34/1000 | Loss: 0.00002303
Iteration 35/1000 | Loss: 0.00002247
Iteration 36/1000 | Loss: 0.00044029
Iteration 37/1000 | Loss: 0.00005265
Iteration 38/1000 | Loss: 0.00007081
Iteration 39/1000 | Loss: 0.00006119
Iteration 40/1000 | Loss: 0.00054896
Iteration 41/1000 | Loss: 0.00004449
Iteration 42/1000 | Loss: 0.00002132
Iteration 43/1000 | Loss: 0.00002095
Iteration 44/1000 | Loss: 0.00002061
Iteration 45/1000 | Loss: 0.00002055
Iteration 46/1000 | Loss: 0.00002035
Iteration 47/1000 | Loss: 0.00002026
Iteration 48/1000 | Loss: 0.00002011
Iteration 49/1000 | Loss: 0.00002005
Iteration 50/1000 | Loss: 0.00001998
Iteration 51/1000 | Loss: 0.00001996
Iteration 52/1000 | Loss: 0.00001995
Iteration 53/1000 | Loss: 0.00001995
Iteration 54/1000 | Loss: 0.00001995
Iteration 55/1000 | Loss: 0.00001995
Iteration 56/1000 | Loss: 0.00001995
Iteration 57/1000 | Loss: 0.00001994
Iteration 58/1000 | Loss: 0.00001994
Iteration 59/1000 | Loss: 0.00016358
Iteration 60/1000 | Loss: 0.00002009
Iteration 61/1000 | Loss: 0.00001990
Iteration 62/1000 | Loss: 0.00001990
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001983
Iteration 65/1000 | Loss: 0.00001982
Iteration 66/1000 | Loss: 0.00001982
Iteration 67/1000 | Loss: 0.00001982
Iteration 68/1000 | Loss: 0.00001982
Iteration 69/1000 | Loss: 0.00001982
Iteration 70/1000 | Loss: 0.00001981
Iteration 71/1000 | Loss: 0.00001981
Iteration 72/1000 | Loss: 0.00001981
Iteration 73/1000 | Loss: 0.00001981
Iteration 74/1000 | Loss: 0.00001980
Iteration 75/1000 | Loss: 0.00001980
Iteration 76/1000 | Loss: 0.00001980
Iteration 77/1000 | Loss: 0.00001980
Iteration 78/1000 | Loss: 0.00001980
Iteration 79/1000 | Loss: 0.00001980
Iteration 80/1000 | Loss: 0.00001980
Iteration 81/1000 | Loss: 0.00001980
Iteration 82/1000 | Loss: 0.00001980
Iteration 83/1000 | Loss: 0.00001980
Iteration 84/1000 | Loss: 0.00001980
Iteration 85/1000 | Loss: 0.00001980
Iteration 86/1000 | Loss: 0.00001979
Iteration 87/1000 | Loss: 0.00001979
Iteration 88/1000 | Loss: 0.00001979
Iteration 89/1000 | Loss: 0.00001979
Iteration 90/1000 | Loss: 0.00001979
Iteration 91/1000 | Loss: 0.00001979
Iteration 92/1000 | Loss: 0.00001979
Iteration 93/1000 | Loss: 0.00001979
Iteration 94/1000 | Loss: 0.00001979
Iteration 95/1000 | Loss: 0.00001979
Iteration 96/1000 | Loss: 0.00001979
Iteration 97/1000 | Loss: 0.00001979
Iteration 98/1000 | Loss: 0.00001979
Iteration 99/1000 | Loss: 0.00001979
Iteration 100/1000 | Loss: 0.00001979
Iteration 101/1000 | Loss: 0.00001979
Iteration 102/1000 | Loss: 0.00001979
Iteration 103/1000 | Loss: 0.00001979
Iteration 104/1000 | Loss: 0.00001979
Iteration 105/1000 | Loss: 0.00001979
Iteration 106/1000 | Loss: 0.00001979
Iteration 107/1000 | Loss: 0.00001979
Iteration 108/1000 | Loss: 0.00001979
Iteration 109/1000 | Loss: 0.00001979
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.9790351871051826e-05, 1.9790351871051826e-05, 1.9790351871051826e-05, 1.9790351871051826e-05, 1.9790351871051826e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9790351871051826e-05

Optimization complete. Final v2v error: 3.806715250015259 mm

Highest mean error: 4.431459903717041 mm for frame 227

Lowest mean error: 3.430250406265259 mm for frame 178

Saving results

Total time: 94.01235008239746
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01026586
Iteration 2/25 | Loss: 0.00273644
Iteration 3/25 | Loss: 0.00201958
Iteration 4/25 | Loss: 0.00186666
Iteration 5/25 | Loss: 0.00200815
Iteration 6/25 | Loss: 0.00178509
Iteration 7/25 | Loss: 0.00168894
Iteration 8/25 | Loss: 0.00173629
Iteration 9/25 | Loss: 0.00182516
Iteration 10/25 | Loss: 0.00165832
Iteration 11/25 | Loss: 0.00146855
Iteration 12/25 | Loss: 0.00130679
Iteration 13/25 | Loss: 0.00125519
Iteration 14/25 | Loss: 0.00122448
Iteration 15/25 | Loss: 0.00119819
Iteration 16/25 | Loss: 0.00118539
Iteration 17/25 | Loss: 0.00117002
Iteration 18/25 | Loss: 0.00114241
Iteration 19/25 | Loss: 0.00111854
Iteration 20/25 | Loss: 0.00110635
Iteration 21/25 | Loss: 0.00110213
Iteration 22/25 | Loss: 0.00109663
Iteration 23/25 | Loss: 0.00109431
Iteration 24/25 | Loss: 0.00109516
Iteration 25/25 | Loss: 0.00109245

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55726135
Iteration 2/25 | Loss: 0.00061125
Iteration 3/25 | Loss: 0.00061125
Iteration 4/25 | Loss: 0.00061125
Iteration 5/25 | Loss: 0.00061125
Iteration 6/25 | Loss: 0.00061125
Iteration 7/25 | Loss: 0.00061125
Iteration 8/25 | Loss: 0.00061124
Iteration 9/25 | Loss: 0.00061124
Iteration 10/25 | Loss: 0.00061124
Iteration 11/25 | Loss: 0.00061124
Iteration 12/25 | Loss: 0.00061124
Iteration 13/25 | Loss: 0.00061124
Iteration 14/25 | Loss: 0.00061124
Iteration 15/25 | Loss: 0.00061124
Iteration 16/25 | Loss: 0.00061124
Iteration 17/25 | Loss: 0.00061124
Iteration 18/25 | Loss: 0.00061124
Iteration 19/25 | Loss: 0.00061124
Iteration 20/25 | Loss: 0.00061124
Iteration 21/25 | Loss: 0.00061124
Iteration 22/25 | Loss: 0.00061124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.000611244176980108, 0.000611244176980108, 0.000611244176980108, 0.000611244176980108, 0.000611244176980108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000611244176980108

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00061124
Iteration 2/1000 | Loss: 0.00010457
Iteration 3/1000 | Loss: 0.00002457
Iteration 4/1000 | Loss: 0.00010592
Iteration 5/1000 | Loss: 0.00003551
Iteration 6/1000 | Loss: 0.00009376
Iteration 7/1000 | Loss: 0.00003861
Iteration 8/1000 | Loss: 0.00006495
Iteration 9/1000 | Loss: 0.00011551
Iteration 10/1000 | Loss: 0.00009698
Iteration 11/1000 | Loss: 0.00003432
Iteration 12/1000 | Loss: 0.00006353
Iteration 13/1000 | Loss: 0.00001936
Iteration 14/1000 | Loss: 0.00001731
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00021440
Iteration 17/1000 | Loss: 0.00001688
Iteration 18/1000 | Loss: 0.00009382
Iteration 19/1000 | Loss: 0.00001955
Iteration 20/1000 | Loss: 0.00011827
Iteration 21/1000 | Loss: 0.00002391
Iteration 22/1000 | Loss: 0.00008379
Iteration 23/1000 | Loss: 0.00004014
Iteration 24/1000 | Loss: 0.00006556
Iteration 25/1000 | Loss: 0.00010050
Iteration 26/1000 | Loss: 0.00011992
Iteration 27/1000 | Loss: 0.00008453
Iteration 28/1000 | Loss: 0.00007865
Iteration 29/1000 | Loss: 0.00011833
Iteration 30/1000 | Loss: 0.00008717
Iteration 31/1000 | Loss: 0.00010812
Iteration 32/1000 | Loss: 0.00020436
Iteration 33/1000 | Loss: 0.00004741
Iteration 34/1000 | Loss: 0.00004150
Iteration 35/1000 | Loss: 0.00004273
Iteration 36/1000 | Loss: 0.00003706
Iteration 37/1000 | Loss: 0.00019756
Iteration 38/1000 | Loss: 0.00007941
Iteration 39/1000 | Loss: 0.00017712
Iteration 40/1000 | Loss: 0.00019048
Iteration 41/1000 | Loss: 0.00005914
Iteration 42/1000 | Loss: 0.00017537
Iteration 43/1000 | Loss: 0.00015417
Iteration 44/1000 | Loss: 0.00010140
Iteration 45/1000 | Loss: 0.00015675
Iteration 46/1000 | Loss: 0.00002261
Iteration 47/1000 | Loss: 0.00001743
Iteration 48/1000 | Loss: 0.00051098
Iteration 49/1000 | Loss: 0.00002556
Iteration 50/1000 | Loss: 0.00001935
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00002330
Iteration 53/1000 | Loss: 0.00001495
Iteration 54/1000 | Loss: 0.00001877
Iteration 55/1000 | Loss: 0.00001393
Iteration 56/1000 | Loss: 0.00001370
Iteration 57/1000 | Loss: 0.00001360
Iteration 58/1000 | Loss: 0.00001951
Iteration 59/1000 | Loss: 0.00001951
Iteration 60/1000 | Loss: 0.00001338
Iteration 61/1000 | Loss: 0.00002408
Iteration 62/1000 | Loss: 0.00001528
Iteration 63/1000 | Loss: 0.00001545
Iteration 64/1000 | Loss: 0.00001329
Iteration 65/1000 | Loss: 0.00001328
Iteration 66/1000 | Loss: 0.00001327
Iteration 67/1000 | Loss: 0.00001327
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001335
Iteration 70/1000 | Loss: 0.00001318
Iteration 71/1000 | Loss: 0.00001318
Iteration 72/1000 | Loss: 0.00001318
Iteration 73/1000 | Loss: 0.00001318
Iteration 74/1000 | Loss: 0.00001318
Iteration 75/1000 | Loss: 0.00001318
Iteration 76/1000 | Loss: 0.00001317
Iteration 77/1000 | Loss: 0.00001317
Iteration 78/1000 | Loss: 0.00001317
Iteration 79/1000 | Loss: 0.00001317
Iteration 80/1000 | Loss: 0.00001317
Iteration 81/1000 | Loss: 0.00001316
Iteration 82/1000 | Loss: 0.00001316
Iteration 83/1000 | Loss: 0.00001316
Iteration 84/1000 | Loss: 0.00001316
Iteration 85/1000 | Loss: 0.00001316
Iteration 86/1000 | Loss: 0.00001316
Iteration 87/1000 | Loss: 0.00001316
Iteration 88/1000 | Loss: 0.00001316
Iteration 89/1000 | Loss: 0.00001316
Iteration 90/1000 | Loss: 0.00001316
Iteration 91/1000 | Loss: 0.00001316
Iteration 92/1000 | Loss: 0.00001316
Iteration 93/1000 | Loss: 0.00001315
Iteration 94/1000 | Loss: 0.00001315
Iteration 95/1000 | Loss: 0.00001314
Iteration 96/1000 | Loss: 0.00001443
Iteration 97/1000 | Loss: 0.00001321
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001299
Iteration 100/1000 | Loss: 0.00001295
Iteration 101/1000 | Loss: 0.00001294
Iteration 102/1000 | Loss: 0.00001294
Iteration 103/1000 | Loss: 0.00001293
Iteration 104/1000 | Loss: 0.00001292
Iteration 105/1000 | Loss: 0.00001292
Iteration 106/1000 | Loss: 0.00001292
Iteration 107/1000 | Loss: 0.00001291
Iteration 108/1000 | Loss: 0.00001291
Iteration 109/1000 | Loss: 0.00001290
Iteration 110/1000 | Loss: 0.00001290
Iteration 111/1000 | Loss: 0.00001289
Iteration 112/1000 | Loss: 0.00001289
Iteration 113/1000 | Loss: 0.00001289
Iteration 114/1000 | Loss: 0.00001287
Iteration 115/1000 | Loss: 0.00001286
Iteration 116/1000 | Loss: 0.00001285
Iteration 117/1000 | Loss: 0.00001285
Iteration 118/1000 | Loss: 0.00001284
Iteration 119/1000 | Loss: 0.00001284
Iteration 120/1000 | Loss: 0.00001288
Iteration 121/1000 | Loss: 0.00001265
Iteration 122/1000 | Loss: 0.00001264
Iteration 123/1000 | Loss: 0.00001263
Iteration 124/1000 | Loss: 0.00001263
Iteration 125/1000 | Loss: 0.00001263
Iteration 126/1000 | Loss: 0.00001263
Iteration 127/1000 | Loss: 0.00001263
Iteration 128/1000 | Loss: 0.00001263
Iteration 129/1000 | Loss: 0.00001263
Iteration 130/1000 | Loss: 0.00001262
Iteration 131/1000 | Loss: 0.00001262
Iteration 132/1000 | Loss: 0.00001262
Iteration 133/1000 | Loss: 0.00001262
Iteration 134/1000 | Loss: 0.00001262
Iteration 135/1000 | Loss: 0.00001262
Iteration 136/1000 | Loss: 0.00001262
Iteration 137/1000 | Loss: 0.00001262
Iteration 138/1000 | Loss: 0.00001262
Iteration 139/1000 | Loss: 0.00001262
Iteration 140/1000 | Loss: 0.00001262
Iteration 141/1000 | Loss: 0.00001262
Iteration 142/1000 | Loss: 0.00001262
Iteration 143/1000 | Loss: 0.00001262
Iteration 144/1000 | Loss: 0.00001261
Iteration 145/1000 | Loss: 0.00001261
Iteration 146/1000 | Loss: 0.00001261
Iteration 147/1000 | Loss: 0.00001261
Iteration 148/1000 | Loss: 0.00001261
Iteration 149/1000 | Loss: 0.00001261
Iteration 150/1000 | Loss: 0.00001261
Iteration 151/1000 | Loss: 0.00001261
Iteration 152/1000 | Loss: 0.00001261
Iteration 153/1000 | Loss: 0.00001261
Iteration 154/1000 | Loss: 0.00001260
Iteration 155/1000 | Loss: 0.00001382
Iteration 156/1000 | Loss: 0.00001260
Iteration 157/1000 | Loss: 0.00001260
Iteration 158/1000 | Loss: 0.00001260
Iteration 159/1000 | Loss: 0.00001259
Iteration 160/1000 | Loss: 0.00001259
Iteration 161/1000 | Loss: 0.00001259
Iteration 162/1000 | Loss: 0.00001259
Iteration 163/1000 | Loss: 0.00001259
Iteration 164/1000 | Loss: 0.00001259
Iteration 165/1000 | Loss: 0.00001259
Iteration 166/1000 | Loss: 0.00001259
Iteration 167/1000 | Loss: 0.00001259
Iteration 168/1000 | Loss: 0.00001259
Iteration 169/1000 | Loss: 0.00001258
Iteration 170/1000 | Loss: 0.00001258
Iteration 171/1000 | Loss: 0.00001258
Iteration 172/1000 | Loss: 0.00001258
Iteration 173/1000 | Loss: 0.00001258
Iteration 174/1000 | Loss: 0.00001258
Iteration 175/1000 | Loss: 0.00001258
Iteration 176/1000 | Loss: 0.00001258
Iteration 177/1000 | Loss: 0.00001257
Iteration 178/1000 | Loss: 0.00001257
Iteration 179/1000 | Loss: 0.00001257
Iteration 180/1000 | Loss: 0.00001257
Iteration 181/1000 | Loss: 0.00001257
Iteration 182/1000 | Loss: 0.00001257
Iteration 183/1000 | Loss: 0.00001257
Iteration 184/1000 | Loss: 0.00001257
Iteration 185/1000 | Loss: 0.00001257
Iteration 186/1000 | Loss: 0.00001257
Iteration 187/1000 | Loss: 0.00001257
Iteration 188/1000 | Loss: 0.00001257
Iteration 189/1000 | Loss: 0.00001257
Iteration 190/1000 | Loss: 0.00001257
Iteration 191/1000 | Loss: 0.00001257
Iteration 192/1000 | Loss: 0.00001257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.257109033758752e-05, 1.257109033758752e-05, 1.257109033758752e-05, 1.257109033758752e-05, 1.257109033758752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.257109033758752e-05

Optimization complete. Final v2v error: 3.0571162700653076 mm

Highest mean error: 4.595970153808594 mm for frame 7

Lowest mean error: 2.902437210083008 mm for frame 218

Saving results

Total time: 168.7784547805786
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00393313
Iteration 2/25 | Loss: 0.00125178
Iteration 3/25 | Loss: 0.00109673
Iteration 4/25 | Loss: 0.00107493
Iteration 5/25 | Loss: 0.00106978
Iteration 6/25 | Loss: 0.00106866
Iteration 7/25 | Loss: 0.00106866
Iteration 8/25 | Loss: 0.00106866
Iteration 9/25 | Loss: 0.00106866
Iteration 10/25 | Loss: 0.00106866
Iteration 11/25 | Loss: 0.00106866
Iteration 12/25 | Loss: 0.00106866
Iteration 13/25 | Loss: 0.00106866
Iteration 14/25 | Loss: 0.00106866
Iteration 15/25 | Loss: 0.00106866
Iteration 16/25 | Loss: 0.00106866
Iteration 17/25 | Loss: 0.00106866
Iteration 18/25 | Loss: 0.00106866
Iteration 19/25 | Loss: 0.00106866
Iteration 20/25 | Loss: 0.00106866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010686622699722648, 0.0010686622699722648, 0.0010686622699722648, 0.0010686622699722648, 0.0010686622699722648]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010686622699722648

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35773838
Iteration 2/25 | Loss: 0.00087346
Iteration 3/25 | Loss: 0.00087346
Iteration 4/25 | Loss: 0.00087346
Iteration 5/25 | Loss: 0.00087346
Iteration 6/25 | Loss: 0.00087346
Iteration 7/25 | Loss: 0.00087346
Iteration 8/25 | Loss: 0.00087346
Iteration 9/25 | Loss: 0.00087346
Iteration 10/25 | Loss: 0.00087346
Iteration 11/25 | Loss: 0.00087346
Iteration 12/25 | Loss: 0.00087346
Iteration 13/25 | Loss: 0.00087346
Iteration 14/25 | Loss: 0.00087346
Iteration 15/25 | Loss: 0.00087346
Iteration 16/25 | Loss: 0.00087346
Iteration 17/25 | Loss: 0.00087346
Iteration 18/25 | Loss: 0.00087346
Iteration 19/25 | Loss: 0.00087346
Iteration 20/25 | Loss: 0.00087346
Iteration 21/25 | Loss: 0.00087346
Iteration 22/25 | Loss: 0.00087346
Iteration 23/25 | Loss: 0.00087346
Iteration 24/25 | Loss: 0.00087346
Iteration 25/25 | Loss: 0.00087346

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00087346
Iteration 2/1000 | Loss: 0.00004484
Iteration 3/1000 | Loss: 0.00003079
Iteration 4/1000 | Loss: 0.00002296
Iteration 5/1000 | Loss: 0.00002018
Iteration 6/1000 | Loss: 0.00001831
Iteration 7/1000 | Loss: 0.00001664
Iteration 8/1000 | Loss: 0.00001589
Iteration 9/1000 | Loss: 0.00001530
Iteration 10/1000 | Loss: 0.00001480
Iteration 11/1000 | Loss: 0.00001445
Iteration 12/1000 | Loss: 0.00001414
Iteration 13/1000 | Loss: 0.00001409
Iteration 14/1000 | Loss: 0.00001394
Iteration 15/1000 | Loss: 0.00001384
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001379
Iteration 18/1000 | Loss: 0.00001370
Iteration 19/1000 | Loss: 0.00001364
Iteration 20/1000 | Loss: 0.00001363
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001353
Iteration 23/1000 | Loss: 0.00001353
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001352
Iteration 26/1000 | Loss: 0.00001352
Iteration 27/1000 | Loss: 0.00001351
Iteration 28/1000 | Loss: 0.00001350
Iteration 29/1000 | Loss: 0.00001349
Iteration 30/1000 | Loss: 0.00001348
Iteration 31/1000 | Loss: 0.00001346
Iteration 32/1000 | Loss: 0.00001344
Iteration 33/1000 | Loss: 0.00001344
Iteration 34/1000 | Loss: 0.00001343
Iteration 35/1000 | Loss: 0.00001343
Iteration 36/1000 | Loss: 0.00001342
Iteration 37/1000 | Loss: 0.00001341
Iteration 38/1000 | Loss: 0.00001341
Iteration 39/1000 | Loss: 0.00001340
Iteration 40/1000 | Loss: 0.00001340
Iteration 41/1000 | Loss: 0.00001340
Iteration 42/1000 | Loss: 0.00001339
Iteration 43/1000 | Loss: 0.00001339
Iteration 44/1000 | Loss: 0.00001339
Iteration 45/1000 | Loss: 0.00001339
Iteration 46/1000 | Loss: 0.00001339
Iteration 47/1000 | Loss: 0.00001338
Iteration 48/1000 | Loss: 0.00001338
Iteration 49/1000 | Loss: 0.00001337
Iteration 50/1000 | Loss: 0.00001337
Iteration 51/1000 | Loss: 0.00001336
Iteration 52/1000 | Loss: 0.00001336
Iteration 53/1000 | Loss: 0.00001335
Iteration 54/1000 | Loss: 0.00001335
Iteration 55/1000 | Loss: 0.00001335
Iteration 56/1000 | Loss: 0.00001334
Iteration 57/1000 | Loss: 0.00001334
Iteration 58/1000 | Loss: 0.00001334
Iteration 59/1000 | Loss: 0.00001334
Iteration 60/1000 | Loss: 0.00001333
Iteration 61/1000 | Loss: 0.00001333
Iteration 62/1000 | Loss: 0.00001333
Iteration 63/1000 | Loss: 0.00001333
Iteration 64/1000 | Loss: 0.00001333
Iteration 65/1000 | Loss: 0.00001332
Iteration 66/1000 | Loss: 0.00001332
Iteration 67/1000 | Loss: 0.00001332
Iteration 68/1000 | Loss: 0.00001332
Iteration 69/1000 | Loss: 0.00001331
Iteration 70/1000 | Loss: 0.00001331
Iteration 71/1000 | Loss: 0.00001331
Iteration 72/1000 | Loss: 0.00001331
Iteration 73/1000 | Loss: 0.00001331
Iteration 74/1000 | Loss: 0.00001331
Iteration 75/1000 | Loss: 0.00001331
Iteration 76/1000 | Loss: 0.00001331
Iteration 77/1000 | Loss: 0.00001331
Iteration 78/1000 | Loss: 0.00001331
Iteration 79/1000 | Loss: 0.00001330
Iteration 80/1000 | Loss: 0.00001330
Iteration 81/1000 | Loss: 0.00001330
Iteration 82/1000 | Loss: 0.00001330
Iteration 83/1000 | Loss: 0.00001329
Iteration 84/1000 | Loss: 0.00001329
Iteration 85/1000 | Loss: 0.00001329
Iteration 86/1000 | Loss: 0.00001329
Iteration 87/1000 | Loss: 0.00001329
Iteration 88/1000 | Loss: 0.00001329
Iteration 89/1000 | Loss: 0.00001329
Iteration 90/1000 | Loss: 0.00001329
Iteration 91/1000 | Loss: 0.00001329
Iteration 92/1000 | Loss: 0.00001329
Iteration 93/1000 | Loss: 0.00001328
Iteration 94/1000 | Loss: 0.00001328
Iteration 95/1000 | Loss: 0.00001328
Iteration 96/1000 | Loss: 0.00001328
Iteration 97/1000 | Loss: 0.00001328
Iteration 98/1000 | Loss: 0.00001328
Iteration 99/1000 | Loss: 0.00001328
Iteration 100/1000 | Loss: 0.00001328
Iteration 101/1000 | Loss: 0.00001328
Iteration 102/1000 | Loss: 0.00001328
Iteration 103/1000 | Loss: 0.00001327
Iteration 104/1000 | Loss: 0.00001327
Iteration 105/1000 | Loss: 0.00001327
Iteration 106/1000 | Loss: 0.00001327
Iteration 107/1000 | Loss: 0.00001327
Iteration 108/1000 | Loss: 0.00001326
Iteration 109/1000 | Loss: 0.00001326
Iteration 110/1000 | Loss: 0.00001326
Iteration 111/1000 | Loss: 0.00001326
Iteration 112/1000 | Loss: 0.00001326
Iteration 113/1000 | Loss: 0.00001325
Iteration 114/1000 | Loss: 0.00001325
Iteration 115/1000 | Loss: 0.00001325
Iteration 116/1000 | Loss: 0.00001324
Iteration 117/1000 | Loss: 0.00001324
Iteration 118/1000 | Loss: 0.00001324
Iteration 119/1000 | Loss: 0.00001324
Iteration 120/1000 | Loss: 0.00001324
Iteration 121/1000 | Loss: 0.00001323
Iteration 122/1000 | Loss: 0.00001323
Iteration 123/1000 | Loss: 0.00001323
Iteration 124/1000 | Loss: 0.00001323
Iteration 125/1000 | Loss: 0.00001323
Iteration 126/1000 | Loss: 0.00001322
Iteration 127/1000 | Loss: 0.00001322
Iteration 128/1000 | Loss: 0.00001322
Iteration 129/1000 | Loss: 0.00001322
Iteration 130/1000 | Loss: 0.00001321
Iteration 131/1000 | Loss: 0.00001321
Iteration 132/1000 | Loss: 0.00001321
Iteration 133/1000 | Loss: 0.00001321
Iteration 134/1000 | Loss: 0.00001321
Iteration 135/1000 | Loss: 0.00001320
Iteration 136/1000 | Loss: 0.00001320
Iteration 137/1000 | Loss: 0.00001320
Iteration 138/1000 | Loss: 0.00001320
Iteration 139/1000 | Loss: 0.00001320
Iteration 140/1000 | Loss: 0.00001320
Iteration 141/1000 | Loss: 0.00001319
Iteration 142/1000 | Loss: 0.00001319
Iteration 143/1000 | Loss: 0.00001319
Iteration 144/1000 | Loss: 0.00001319
Iteration 145/1000 | Loss: 0.00001319
Iteration 146/1000 | Loss: 0.00001319
Iteration 147/1000 | Loss: 0.00001319
Iteration 148/1000 | Loss: 0.00001319
Iteration 149/1000 | Loss: 0.00001319
Iteration 150/1000 | Loss: 0.00001319
Iteration 151/1000 | Loss: 0.00001319
Iteration 152/1000 | Loss: 0.00001318
Iteration 153/1000 | Loss: 0.00001318
Iteration 154/1000 | Loss: 0.00001318
Iteration 155/1000 | Loss: 0.00001318
Iteration 156/1000 | Loss: 0.00001318
Iteration 157/1000 | Loss: 0.00001318
Iteration 158/1000 | Loss: 0.00001318
Iteration 159/1000 | Loss: 0.00001318
Iteration 160/1000 | Loss: 0.00001318
Iteration 161/1000 | Loss: 0.00001318
Iteration 162/1000 | Loss: 0.00001318
Iteration 163/1000 | Loss: 0.00001318
Iteration 164/1000 | Loss: 0.00001318
Iteration 165/1000 | Loss: 0.00001318
Iteration 166/1000 | Loss: 0.00001318
Iteration 167/1000 | Loss: 0.00001318
Iteration 168/1000 | Loss: 0.00001318
Iteration 169/1000 | Loss: 0.00001318
Iteration 170/1000 | Loss: 0.00001318
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.3180057976569515e-05, 1.3180057976569515e-05, 1.3180057976569515e-05, 1.3180057976569515e-05, 1.3180057976569515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3180057976569515e-05

Optimization complete. Final v2v error: 3.0833816528320312 mm

Highest mean error: 4.072732448577881 mm for frame 5

Lowest mean error: 2.4769179821014404 mm for frame 129

Saving results

Total time: 49.1652147769928
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00808197
Iteration 2/25 | Loss: 0.00119500
Iteration 3/25 | Loss: 0.00106687
Iteration 4/25 | Loss: 0.00105412
Iteration 5/25 | Loss: 0.00105169
Iteration 6/25 | Loss: 0.00105142
Iteration 7/25 | Loss: 0.00105142
Iteration 8/25 | Loss: 0.00105142
Iteration 9/25 | Loss: 0.00105142
Iteration 10/25 | Loss: 0.00105142
Iteration 11/25 | Loss: 0.00105142
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001051417551934719, 0.001051417551934719, 0.001051417551934719, 0.001051417551934719, 0.001051417551934719]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001051417551934719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35709357
Iteration 2/25 | Loss: 0.00077412
Iteration 3/25 | Loss: 0.00077411
Iteration 4/25 | Loss: 0.00077411
Iteration 5/25 | Loss: 0.00077411
Iteration 6/25 | Loss: 0.00077411
Iteration 7/25 | Loss: 0.00077411
Iteration 8/25 | Loss: 0.00077411
Iteration 9/25 | Loss: 0.00077411
Iteration 10/25 | Loss: 0.00077411
Iteration 11/25 | Loss: 0.00077411
Iteration 12/25 | Loss: 0.00077411
Iteration 13/25 | Loss: 0.00077411
Iteration 14/25 | Loss: 0.00077411
Iteration 15/25 | Loss: 0.00077411
Iteration 16/25 | Loss: 0.00077411
Iteration 17/25 | Loss: 0.00077411
Iteration 18/25 | Loss: 0.00077411
Iteration 19/25 | Loss: 0.00077411
Iteration 20/25 | Loss: 0.00077411
Iteration 21/25 | Loss: 0.00077411
Iteration 22/25 | Loss: 0.00077411
Iteration 23/25 | Loss: 0.00077411
Iteration 24/25 | Loss: 0.00077411
Iteration 25/25 | Loss: 0.00077411

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077411
Iteration 2/1000 | Loss: 0.00002100
Iteration 3/1000 | Loss: 0.00001374
Iteration 4/1000 | Loss: 0.00001186
Iteration 5/1000 | Loss: 0.00001101
Iteration 6/1000 | Loss: 0.00001040
Iteration 7/1000 | Loss: 0.00001006
Iteration 8/1000 | Loss: 0.00000984
Iteration 9/1000 | Loss: 0.00000967
Iteration 10/1000 | Loss: 0.00000958
Iteration 11/1000 | Loss: 0.00000939
Iteration 12/1000 | Loss: 0.00000934
Iteration 13/1000 | Loss: 0.00000933
Iteration 14/1000 | Loss: 0.00000933
Iteration 15/1000 | Loss: 0.00000933
Iteration 16/1000 | Loss: 0.00000933
Iteration 17/1000 | Loss: 0.00000933
Iteration 18/1000 | Loss: 0.00000932
Iteration 19/1000 | Loss: 0.00000932
Iteration 20/1000 | Loss: 0.00000931
Iteration 21/1000 | Loss: 0.00000931
Iteration 22/1000 | Loss: 0.00000929
Iteration 23/1000 | Loss: 0.00000927
Iteration 24/1000 | Loss: 0.00000926
Iteration 25/1000 | Loss: 0.00000926
Iteration 26/1000 | Loss: 0.00000925
Iteration 27/1000 | Loss: 0.00000925
Iteration 28/1000 | Loss: 0.00000924
Iteration 29/1000 | Loss: 0.00000924
Iteration 30/1000 | Loss: 0.00000923
Iteration 31/1000 | Loss: 0.00000922
Iteration 32/1000 | Loss: 0.00000922
Iteration 33/1000 | Loss: 0.00000922
Iteration 34/1000 | Loss: 0.00000922
Iteration 35/1000 | Loss: 0.00000921
Iteration 36/1000 | Loss: 0.00000921
Iteration 37/1000 | Loss: 0.00000920
Iteration 38/1000 | Loss: 0.00000919
Iteration 39/1000 | Loss: 0.00000917
Iteration 40/1000 | Loss: 0.00000916
Iteration 41/1000 | Loss: 0.00000911
Iteration 42/1000 | Loss: 0.00000911
Iteration 43/1000 | Loss: 0.00000910
Iteration 44/1000 | Loss: 0.00000909
Iteration 45/1000 | Loss: 0.00000909
Iteration 46/1000 | Loss: 0.00000909
Iteration 47/1000 | Loss: 0.00000908
Iteration 48/1000 | Loss: 0.00000907
Iteration 49/1000 | Loss: 0.00000907
Iteration 50/1000 | Loss: 0.00000907
Iteration 51/1000 | Loss: 0.00000907
Iteration 52/1000 | Loss: 0.00000906
Iteration 53/1000 | Loss: 0.00000906
Iteration 54/1000 | Loss: 0.00000906
Iteration 55/1000 | Loss: 0.00000905
Iteration 56/1000 | Loss: 0.00000905
Iteration 57/1000 | Loss: 0.00000904
Iteration 58/1000 | Loss: 0.00000904
Iteration 59/1000 | Loss: 0.00000903
Iteration 60/1000 | Loss: 0.00000902
Iteration 61/1000 | Loss: 0.00000902
Iteration 62/1000 | Loss: 0.00000901
Iteration 63/1000 | Loss: 0.00000901
Iteration 64/1000 | Loss: 0.00000901
Iteration 65/1000 | Loss: 0.00000900
Iteration 66/1000 | Loss: 0.00000900
Iteration 67/1000 | Loss: 0.00000900
Iteration 68/1000 | Loss: 0.00000900
Iteration 69/1000 | Loss: 0.00000900
Iteration 70/1000 | Loss: 0.00000899
Iteration 71/1000 | Loss: 0.00000899
Iteration 72/1000 | Loss: 0.00000898
Iteration 73/1000 | Loss: 0.00000898
Iteration 74/1000 | Loss: 0.00000897
Iteration 75/1000 | Loss: 0.00000897
Iteration 76/1000 | Loss: 0.00000897
Iteration 77/1000 | Loss: 0.00000897
Iteration 78/1000 | Loss: 0.00000896
Iteration 79/1000 | Loss: 0.00000896
Iteration 80/1000 | Loss: 0.00000895
Iteration 81/1000 | Loss: 0.00000895
Iteration 82/1000 | Loss: 0.00000894
Iteration 83/1000 | Loss: 0.00000893
Iteration 84/1000 | Loss: 0.00000893
Iteration 85/1000 | Loss: 0.00000892
Iteration 86/1000 | Loss: 0.00000892
Iteration 87/1000 | Loss: 0.00000891
Iteration 88/1000 | Loss: 0.00000891
Iteration 89/1000 | Loss: 0.00000891
Iteration 90/1000 | Loss: 0.00000891
Iteration 91/1000 | Loss: 0.00000891
Iteration 92/1000 | Loss: 0.00000890
Iteration 93/1000 | Loss: 0.00000889
Iteration 94/1000 | Loss: 0.00000889
Iteration 95/1000 | Loss: 0.00000889
Iteration 96/1000 | Loss: 0.00000889
Iteration 97/1000 | Loss: 0.00000888
Iteration 98/1000 | Loss: 0.00000887
Iteration 99/1000 | Loss: 0.00000887
Iteration 100/1000 | Loss: 0.00000887
Iteration 101/1000 | Loss: 0.00000886
Iteration 102/1000 | Loss: 0.00000886
Iteration 103/1000 | Loss: 0.00000886
Iteration 104/1000 | Loss: 0.00000885
Iteration 105/1000 | Loss: 0.00000884
Iteration 106/1000 | Loss: 0.00000884
Iteration 107/1000 | Loss: 0.00000884
Iteration 108/1000 | Loss: 0.00000883
Iteration 109/1000 | Loss: 0.00000883
Iteration 110/1000 | Loss: 0.00000883
Iteration 111/1000 | Loss: 0.00000883
Iteration 112/1000 | Loss: 0.00000882
Iteration 113/1000 | Loss: 0.00000882
Iteration 114/1000 | Loss: 0.00000882
Iteration 115/1000 | Loss: 0.00000881
Iteration 116/1000 | Loss: 0.00000881
Iteration 117/1000 | Loss: 0.00000881
Iteration 118/1000 | Loss: 0.00000881
Iteration 119/1000 | Loss: 0.00000881
Iteration 120/1000 | Loss: 0.00000881
Iteration 121/1000 | Loss: 0.00000880
Iteration 122/1000 | Loss: 0.00000880
Iteration 123/1000 | Loss: 0.00000880
Iteration 124/1000 | Loss: 0.00000880
Iteration 125/1000 | Loss: 0.00000880
Iteration 126/1000 | Loss: 0.00000879
Iteration 127/1000 | Loss: 0.00000879
Iteration 128/1000 | Loss: 0.00000879
Iteration 129/1000 | Loss: 0.00000879
Iteration 130/1000 | Loss: 0.00000879
Iteration 131/1000 | Loss: 0.00000879
Iteration 132/1000 | Loss: 0.00000879
Iteration 133/1000 | Loss: 0.00000878
Iteration 134/1000 | Loss: 0.00000878
Iteration 135/1000 | Loss: 0.00000878
Iteration 136/1000 | Loss: 0.00000878
Iteration 137/1000 | Loss: 0.00000878
Iteration 138/1000 | Loss: 0.00000878
Iteration 139/1000 | Loss: 0.00000878
Iteration 140/1000 | Loss: 0.00000878
Iteration 141/1000 | Loss: 0.00000878
Iteration 142/1000 | Loss: 0.00000877
Iteration 143/1000 | Loss: 0.00000877
Iteration 144/1000 | Loss: 0.00000877
Iteration 145/1000 | Loss: 0.00000877
Iteration 146/1000 | Loss: 0.00000877
Iteration 147/1000 | Loss: 0.00000877
Iteration 148/1000 | Loss: 0.00000877
Iteration 149/1000 | Loss: 0.00000877
Iteration 150/1000 | Loss: 0.00000876
Iteration 151/1000 | Loss: 0.00000876
Iteration 152/1000 | Loss: 0.00000876
Iteration 153/1000 | Loss: 0.00000876
Iteration 154/1000 | Loss: 0.00000876
Iteration 155/1000 | Loss: 0.00000876
Iteration 156/1000 | Loss: 0.00000876
Iteration 157/1000 | Loss: 0.00000875
Iteration 158/1000 | Loss: 0.00000875
Iteration 159/1000 | Loss: 0.00000875
Iteration 160/1000 | Loss: 0.00000875
Iteration 161/1000 | Loss: 0.00000875
Iteration 162/1000 | Loss: 0.00000875
Iteration 163/1000 | Loss: 0.00000875
Iteration 164/1000 | Loss: 0.00000875
Iteration 165/1000 | Loss: 0.00000875
Iteration 166/1000 | Loss: 0.00000874
Iteration 167/1000 | Loss: 0.00000874
Iteration 168/1000 | Loss: 0.00000874
Iteration 169/1000 | Loss: 0.00000874
Iteration 170/1000 | Loss: 0.00000874
Iteration 171/1000 | Loss: 0.00000874
Iteration 172/1000 | Loss: 0.00000874
Iteration 173/1000 | Loss: 0.00000874
Iteration 174/1000 | Loss: 0.00000874
Iteration 175/1000 | Loss: 0.00000874
Iteration 176/1000 | Loss: 0.00000873
Iteration 177/1000 | Loss: 0.00000873
Iteration 178/1000 | Loss: 0.00000873
Iteration 179/1000 | Loss: 0.00000873
Iteration 180/1000 | Loss: 0.00000873
Iteration 181/1000 | Loss: 0.00000873
Iteration 182/1000 | Loss: 0.00000873
Iteration 183/1000 | Loss: 0.00000873
Iteration 184/1000 | Loss: 0.00000872
Iteration 185/1000 | Loss: 0.00000872
Iteration 186/1000 | Loss: 0.00000872
Iteration 187/1000 | Loss: 0.00000872
Iteration 188/1000 | Loss: 0.00000872
Iteration 189/1000 | Loss: 0.00000872
Iteration 190/1000 | Loss: 0.00000872
Iteration 191/1000 | Loss: 0.00000872
Iteration 192/1000 | Loss: 0.00000872
Iteration 193/1000 | Loss: 0.00000872
Iteration 194/1000 | Loss: 0.00000872
Iteration 195/1000 | Loss: 0.00000872
Iteration 196/1000 | Loss: 0.00000872
Iteration 197/1000 | Loss: 0.00000872
Iteration 198/1000 | Loss: 0.00000871
Iteration 199/1000 | Loss: 0.00000871
Iteration 200/1000 | Loss: 0.00000871
Iteration 201/1000 | Loss: 0.00000871
Iteration 202/1000 | Loss: 0.00000871
Iteration 203/1000 | Loss: 0.00000871
Iteration 204/1000 | Loss: 0.00000871
Iteration 205/1000 | Loss: 0.00000871
Iteration 206/1000 | Loss: 0.00000871
Iteration 207/1000 | Loss: 0.00000871
Iteration 208/1000 | Loss: 0.00000871
Iteration 209/1000 | Loss: 0.00000871
Iteration 210/1000 | Loss: 0.00000871
Iteration 211/1000 | Loss: 0.00000871
Iteration 212/1000 | Loss: 0.00000871
Iteration 213/1000 | Loss: 0.00000870
Iteration 214/1000 | Loss: 0.00000870
Iteration 215/1000 | Loss: 0.00000870
Iteration 216/1000 | Loss: 0.00000870
Iteration 217/1000 | Loss: 0.00000870
Iteration 218/1000 | Loss: 0.00000870
Iteration 219/1000 | Loss: 0.00000870
Iteration 220/1000 | Loss: 0.00000870
Iteration 221/1000 | Loss: 0.00000870
Iteration 222/1000 | Loss: 0.00000870
Iteration 223/1000 | Loss: 0.00000870
Iteration 224/1000 | Loss: 0.00000870
Iteration 225/1000 | Loss: 0.00000870
Iteration 226/1000 | Loss: 0.00000870
Iteration 227/1000 | Loss: 0.00000870
Iteration 228/1000 | Loss: 0.00000870
Iteration 229/1000 | Loss: 0.00000870
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [8.704982064955402e-06, 8.704982064955402e-06, 8.704982064955402e-06, 8.704982064955402e-06, 8.704982064955402e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.704982064955402e-06

Optimization complete. Final v2v error: 2.5050132274627686 mm

Highest mean error: 2.738309860229492 mm for frame 57

Lowest mean error: 2.319190502166748 mm for frame 153

Saving results

Total time: 40.24509811401367
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00775897
Iteration 2/25 | Loss: 0.00130550
Iteration 3/25 | Loss: 0.00114230
Iteration 4/25 | Loss: 0.00111312
Iteration 5/25 | Loss: 0.00110520
Iteration 6/25 | Loss: 0.00110264
Iteration 7/25 | Loss: 0.00110228
Iteration 8/25 | Loss: 0.00110228
Iteration 9/25 | Loss: 0.00110228
Iteration 10/25 | Loss: 0.00110228
Iteration 11/25 | Loss: 0.00110228
Iteration 12/25 | Loss: 0.00110228
Iteration 13/25 | Loss: 0.00110228
Iteration 14/25 | Loss: 0.00110228
Iteration 15/25 | Loss: 0.00110228
Iteration 16/25 | Loss: 0.00110228
Iteration 17/25 | Loss: 0.00110228
Iteration 18/25 | Loss: 0.00110228
Iteration 19/25 | Loss: 0.00110228
Iteration 20/25 | Loss: 0.00110228
Iteration 21/25 | Loss: 0.00110228
Iteration 22/25 | Loss: 0.00110228
Iteration 23/25 | Loss: 0.00110228
Iteration 24/25 | Loss: 0.00110228
Iteration 25/25 | Loss: 0.00110228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34045529
Iteration 2/25 | Loss: 0.00088919
Iteration 3/25 | Loss: 0.00088918
Iteration 4/25 | Loss: 0.00088918
Iteration 5/25 | Loss: 0.00088918
Iteration 6/25 | Loss: 0.00088918
Iteration 7/25 | Loss: 0.00088918
Iteration 8/25 | Loss: 0.00088918
Iteration 9/25 | Loss: 0.00088918
Iteration 10/25 | Loss: 0.00088918
Iteration 11/25 | Loss: 0.00088918
Iteration 12/25 | Loss: 0.00088918
Iteration 13/25 | Loss: 0.00088918
Iteration 14/25 | Loss: 0.00088918
Iteration 15/25 | Loss: 0.00088918
Iteration 16/25 | Loss: 0.00088918
Iteration 17/25 | Loss: 0.00088918
Iteration 18/25 | Loss: 0.00088918
Iteration 19/25 | Loss: 0.00088918
Iteration 20/25 | Loss: 0.00088918
Iteration 21/25 | Loss: 0.00088918
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0008891763282008469, 0.0008891763282008469, 0.0008891763282008469, 0.0008891763282008469, 0.0008891763282008469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008891763282008469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088918
Iteration 2/1000 | Loss: 0.00004018
Iteration 3/1000 | Loss: 0.00002788
Iteration 4/1000 | Loss: 0.00002532
Iteration 5/1000 | Loss: 0.00002374
Iteration 6/1000 | Loss: 0.00002269
Iteration 7/1000 | Loss: 0.00002200
Iteration 8/1000 | Loss: 0.00002143
Iteration 9/1000 | Loss: 0.00002106
Iteration 10/1000 | Loss: 0.00002066
Iteration 11/1000 | Loss: 0.00002045
Iteration 12/1000 | Loss: 0.00002026
Iteration 13/1000 | Loss: 0.00002026
Iteration 14/1000 | Loss: 0.00002025
Iteration 15/1000 | Loss: 0.00002022
Iteration 16/1000 | Loss: 0.00002010
Iteration 17/1000 | Loss: 0.00002009
Iteration 18/1000 | Loss: 0.00002008
Iteration 19/1000 | Loss: 0.00002006
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00002005
Iteration 22/1000 | Loss: 0.00002005
Iteration 23/1000 | Loss: 0.00002001
Iteration 24/1000 | Loss: 0.00001998
Iteration 25/1000 | Loss: 0.00001997
Iteration 26/1000 | Loss: 0.00001996
Iteration 27/1000 | Loss: 0.00001996
Iteration 28/1000 | Loss: 0.00001995
Iteration 29/1000 | Loss: 0.00001995
Iteration 30/1000 | Loss: 0.00001992
Iteration 31/1000 | Loss: 0.00001992
Iteration 32/1000 | Loss: 0.00001991
Iteration 33/1000 | Loss: 0.00001991
Iteration 34/1000 | Loss: 0.00001990
Iteration 35/1000 | Loss: 0.00001990
Iteration 36/1000 | Loss: 0.00001990
Iteration 37/1000 | Loss: 0.00001989
Iteration 38/1000 | Loss: 0.00001989
Iteration 39/1000 | Loss: 0.00001988
Iteration 40/1000 | Loss: 0.00001988
Iteration 41/1000 | Loss: 0.00001988
Iteration 42/1000 | Loss: 0.00001987
Iteration 43/1000 | Loss: 0.00001987
Iteration 44/1000 | Loss: 0.00001987
Iteration 45/1000 | Loss: 0.00001987
Iteration 46/1000 | Loss: 0.00001987
Iteration 47/1000 | Loss: 0.00001986
Iteration 48/1000 | Loss: 0.00001986
Iteration 49/1000 | Loss: 0.00001986
Iteration 50/1000 | Loss: 0.00001985
Iteration 51/1000 | Loss: 0.00001985
Iteration 52/1000 | Loss: 0.00001985
Iteration 53/1000 | Loss: 0.00001984
Iteration 54/1000 | Loss: 0.00001984
Iteration 55/1000 | Loss: 0.00001984
Iteration 56/1000 | Loss: 0.00001984
Iteration 57/1000 | Loss: 0.00001984
Iteration 58/1000 | Loss: 0.00001984
Iteration 59/1000 | Loss: 0.00001983
Iteration 60/1000 | Loss: 0.00001983
Iteration 61/1000 | Loss: 0.00001983
Iteration 62/1000 | Loss: 0.00001982
Iteration 63/1000 | Loss: 0.00001982
Iteration 64/1000 | Loss: 0.00001982
Iteration 65/1000 | Loss: 0.00001981
Iteration 66/1000 | Loss: 0.00001981
Iteration 67/1000 | Loss: 0.00001981
Iteration 68/1000 | Loss: 0.00001980
Iteration 69/1000 | Loss: 0.00001980
Iteration 70/1000 | Loss: 0.00001980
Iteration 71/1000 | Loss: 0.00001980
Iteration 72/1000 | Loss: 0.00001980
Iteration 73/1000 | Loss: 0.00001980
Iteration 74/1000 | Loss: 0.00001979
Iteration 75/1000 | Loss: 0.00001979
Iteration 76/1000 | Loss: 0.00001979
Iteration 77/1000 | Loss: 0.00001979
Iteration 78/1000 | Loss: 0.00001979
Iteration 79/1000 | Loss: 0.00001978
Iteration 80/1000 | Loss: 0.00001978
Iteration 81/1000 | Loss: 0.00001978
Iteration 82/1000 | Loss: 0.00001978
Iteration 83/1000 | Loss: 0.00001978
Iteration 84/1000 | Loss: 0.00001977
Iteration 85/1000 | Loss: 0.00001977
Iteration 86/1000 | Loss: 0.00001977
Iteration 87/1000 | Loss: 0.00001977
Iteration 88/1000 | Loss: 0.00001976
Iteration 89/1000 | Loss: 0.00001976
Iteration 90/1000 | Loss: 0.00001976
Iteration 91/1000 | Loss: 0.00001976
Iteration 92/1000 | Loss: 0.00001976
Iteration 93/1000 | Loss: 0.00001976
Iteration 94/1000 | Loss: 0.00001976
Iteration 95/1000 | Loss: 0.00001975
Iteration 96/1000 | Loss: 0.00001975
Iteration 97/1000 | Loss: 0.00001975
Iteration 98/1000 | Loss: 0.00001975
Iteration 99/1000 | Loss: 0.00001975
Iteration 100/1000 | Loss: 0.00001975
Iteration 101/1000 | Loss: 0.00001975
Iteration 102/1000 | Loss: 0.00001975
Iteration 103/1000 | Loss: 0.00001975
Iteration 104/1000 | Loss: 0.00001974
Iteration 105/1000 | Loss: 0.00001974
Iteration 106/1000 | Loss: 0.00001974
Iteration 107/1000 | Loss: 0.00001974
Iteration 108/1000 | Loss: 0.00001974
Iteration 109/1000 | Loss: 0.00001974
Iteration 110/1000 | Loss: 0.00001974
Iteration 111/1000 | Loss: 0.00001974
Iteration 112/1000 | Loss: 0.00001973
Iteration 113/1000 | Loss: 0.00001973
Iteration 114/1000 | Loss: 0.00001973
Iteration 115/1000 | Loss: 0.00001973
Iteration 116/1000 | Loss: 0.00001973
Iteration 117/1000 | Loss: 0.00001973
Iteration 118/1000 | Loss: 0.00001972
Iteration 119/1000 | Loss: 0.00001972
Iteration 120/1000 | Loss: 0.00001972
Iteration 121/1000 | Loss: 0.00001972
Iteration 122/1000 | Loss: 0.00001972
Iteration 123/1000 | Loss: 0.00001972
Iteration 124/1000 | Loss: 0.00001972
Iteration 125/1000 | Loss: 0.00001971
Iteration 126/1000 | Loss: 0.00001971
Iteration 127/1000 | Loss: 0.00001971
Iteration 128/1000 | Loss: 0.00001971
Iteration 129/1000 | Loss: 0.00001971
Iteration 130/1000 | Loss: 0.00001971
Iteration 131/1000 | Loss: 0.00001971
Iteration 132/1000 | Loss: 0.00001971
Iteration 133/1000 | Loss: 0.00001971
Iteration 134/1000 | Loss: 0.00001971
Iteration 135/1000 | Loss: 0.00001971
Iteration 136/1000 | Loss: 0.00001971
Iteration 137/1000 | Loss: 0.00001971
Iteration 138/1000 | Loss: 0.00001971
Iteration 139/1000 | Loss: 0.00001971
Iteration 140/1000 | Loss: 0.00001971
Iteration 141/1000 | Loss: 0.00001971
Iteration 142/1000 | Loss: 0.00001971
Iteration 143/1000 | Loss: 0.00001971
Iteration 144/1000 | Loss: 0.00001971
Iteration 145/1000 | Loss: 0.00001971
Iteration 146/1000 | Loss: 0.00001971
Iteration 147/1000 | Loss: 0.00001971
Iteration 148/1000 | Loss: 0.00001971
Iteration 149/1000 | Loss: 0.00001971
Iteration 150/1000 | Loss: 0.00001971
Iteration 151/1000 | Loss: 0.00001971
Iteration 152/1000 | Loss: 0.00001971
Iteration 153/1000 | Loss: 0.00001971
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.970851189980749e-05, 1.970851189980749e-05, 1.970851189980749e-05, 1.970851189980749e-05, 1.970851189980749e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.970851189980749e-05

Optimization complete. Final v2v error: 3.6449029445648193 mm

Highest mean error: 4.847966194152832 mm for frame 153

Lowest mean error: 2.892531633377075 mm for frame 10

Saving results

Total time: 41.63781261444092
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00816809
Iteration 2/25 | Loss: 0.00114678
Iteration 3/25 | Loss: 0.00106062
Iteration 4/25 | Loss: 0.00105149
Iteration 5/25 | Loss: 0.00104942
Iteration 6/25 | Loss: 0.00104942
Iteration 7/25 | Loss: 0.00104942
Iteration 8/25 | Loss: 0.00104942
Iteration 9/25 | Loss: 0.00104942
Iteration 10/25 | Loss: 0.00104942
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010494233574718237, 0.0010494233574718237, 0.0010494233574718237, 0.0010494233574718237, 0.0010494233574718237]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010494233574718237

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35511220
Iteration 2/25 | Loss: 0.00077535
Iteration 3/25 | Loss: 0.00077534
Iteration 4/25 | Loss: 0.00077534
Iteration 5/25 | Loss: 0.00077534
Iteration 6/25 | Loss: 0.00077534
Iteration 7/25 | Loss: 0.00077534
Iteration 8/25 | Loss: 0.00077534
Iteration 9/25 | Loss: 0.00077534
Iteration 10/25 | Loss: 0.00077534
Iteration 11/25 | Loss: 0.00077534
Iteration 12/25 | Loss: 0.00077534
Iteration 13/25 | Loss: 0.00077534
Iteration 14/25 | Loss: 0.00077534
Iteration 15/25 | Loss: 0.00077534
Iteration 16/25 | Loss: 0.00077534
Iteration 17/25 | Loss: 0.00077534
Iteration 18/25 | Loss: 0.00077534
Iteration 19/25 | Loss: 0.00077534
Iteration 20/25 | Loss: 0.00077534
Iteration 21/25 | Loss: 0.00077534
Iteration 22/25 | Loss: 0.00077534
Iteration 23/25 | Loss: 0.00077534
Iteration 24/25 | Loss: 0.00077534
Iteration 25/25 | Loss: 0.00077534

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077534
Iteration 2/1000 | Loss: 0.00001916
Iteration 3/1000 | Loss: 0.00001220
Iteration 4/1000 | Loss: 0.00001088
Iteration 5/1000 | Loss: 0.00001022
Iteration 6/1000 | Loss: 0.00000967
Iteration 7/1000 | Loss: 0.00000947
Iteration 8/1000 | Loss: 0.00000928
Iteration 9/1000 | Loss: 0.00000903
Iteration 10/1000 | Loss: 0.00000888
Iteration 11/1000 | Loss: 0.00000885
Iteration 12/1000 | Loss: 0.00000883
Iteration 13/1000 | Loss: 0.00000882
Iteration 14/1000 | Loss: 0.00000881
Iteration 15/1000 | Loss: 0.00000881
Iteration 16/1000 | Loss: 0.00000879
Iteration 17/1000 | Loss: 0.00000879
Iteration 18/1000 | Loss: 0.00000877
Iteration 19/1000 | Loss: 0.00000876
Iteration 20/1000 | Loss: 0.00000876
Iteration 21/1000 | Loss: 0.00000875
Iteration 22/1000 | Loss: 0.00000874
Iteration 23/1000 | Loss: 0.00000874
Iteration 24/1000 | Loss: 0.00000873
Iteration 25/1000 | Loss: 0.00000873
Iteration 26/1000 | Loss: 0.00000872
Iteration 27/1000 | Loss: 0.00000871
Iteration 28/1000 | Loss: 0.00000870
Iteration 29/1000 | Loss: 0.00000870
Iteration 30/1000 | Loss: 0.00000870
Iteration 31/1000 | Loss: 0.00000870
Iteration 32/1000 | Loss: 0.00000869
Iteration 33/1000 | Loss: 0.00000869
Iteration 34/1000 | Loss: 0.00000869
Iteration 35/1000 | Loss: 0.00000868
Iteration 36/1000 | Loss: 0.00000868
Iteration 37/1000 | Loss: 0.00000865
Iteration 38/1000 | Loss: 0.00000865
Iteration 39/1000 | Loss: 0.00000864
Iteration 40/1000 | Loss: 0.00000864
Iteration 41/1000 | Loss: 0.00000863
Iteration 42/1000 | Loss: 0.00000863
Iteration 43/1000 | Loss: 0.00000863
Iteration 44/1000 | Loss: 0.00000860
Iteration 45/1000 | Loss: 0.00000860
Iteration 46/1000 | Loss: 0.00000859
Iteration 47/1000 | Loss: 0.00000859
Iteration 48/1000 | Loss: 0.00000858
Iteration 49/1000 | Loss: 0.00000858
Iteration 50/1000 | Loss: 0.00000858
Iteration 51/1000 | Loss: 0.00000857
Iteration 52/1000 | Loss: 0.00000857
Iteration 53/1000 | Loss: 0.00000856
Iteration 54/1000 | Loss: 0.00000856
Iteration 55/1000 | Loss: 0.00000855
Iteration 56/1000 | Loss: 0.00000855
Iteration 57/1000 | Loss: 0.00000855
Iteration 58/1000 | Loss: 0.00000854
Iteration 59/1000 | Loss: 0.00000854
Iteration 60/1000 | Loss: 0.00000854
Iteration 61/1000 | Loss: 0.00000854
Iteration 62/1000 | Loss: 0.00000854
Iteration 63/1000 | Loss: 0.00000853
Iteration 64/1000 | Loss: 0.00000852
Iteration 65/1000 | Loss: 0.00000852
Iteration 66/1000 | Loss: 0.00000852
Iteration 67/1000 | Loss: 0.00000852
Iteration 68/1000 | Loss: 0.00000852
Iteration 69/1000 | Loss: 0.00000852
Iteration 70/1000 | Loss: 0.00000851
Iteration 71/1000 | Loss: 0.00000850
Iteration 72/1000 | Loss: 0.00000850
Iteration 73/1000 | Loss: 0.00000849
Iteration 74/1000 | Loss: 0.00000849
Iteration 75/1000 | Loss: 0.00000849
Iteration 76/1000 | Loss: 0.00000848
Iteration 77/1000 | Loss: 0.00000847
Iteration 78/1000 | Loss: 0.00000844
Iteration 79/1000 | Loss: 0.00000843
Iteration 80/1000 | Loss: 0.00000842
Iteration 81/1000 | Loss: 0.00000842
Iteration 82/1000 | Loss: 0.00000842
Iteration 83/1000 | Loss: 0.00000842
Iteration 84/1000 | Loss: 0.00000841
Iteration 85/1000 | Loss: 0.00000841
Iteration 86/1000 | Loss: 0.00000840
Iteration 87/1000 | Loss: 0.00000839
Iteration 88/1000 | Loss: 0.00000839
Iteration 89/1000 | Loss: 0.00000838
Iteration 90/1000 | Loss: 0.00000838
Iteration 91/1000 | Loss: 0.00000838
Iteration 92/1000 | Loss: 0.00000838
Iteration 93/1000 | Loss: 0.00000838
Iteration 94/1000 | Loss: 0.00000838
Iteration 95/1000 | Loss: 0.00000838
Iteration 96/1000 | Loss: 0.00000838
Iteration 97/1000 | Loss: 0.00000838
Iteration 98/1000 | Loss: 0.00000837
Iteration 99/1000 | Loss: 0.00000836
Iteration 100/1000 | Loss: 0.00000836
Iteration 101/1000 | Loss: 0.00000836
Iteration 102/1000 | Loss: 0.00000836
Iteration 103/1000 | Loss: 0.00000836
Iteration 104/1000 | Loss: 0.00000836
Iteration 105/1000 | Loss: 0.00000835
Iteration 106/1000 | Loss: 0.00000835
Iteration 107/1000 | Loss: 0.00000835
Iteration 108/1000 | Loss: 0.00000835
Iteration 109/1000 | Loss: 0.00000835
Iteration 110/1000 | Loss: 0.00000835
Iteration 111/1000 | Loss: 0.00000834
Iteration 112/1000 | Loss: 0.00000834
Iteration 113/1000 | Loss: 0.00000834
Iteration 114/1000 | Loss: 0.00000833
Iteration 115/1000 | Loss: 0.00000833
Iteration 116/1000 | Loss: 0.00000833
Iteration 117/1000 | Loss: 0.00000833
Iteration 118/1000 | Loss: 0.00000833
Iteration 119/1000 | Loss: 0.00000832
Iteration 120/1000 | Loss: 0.00000832
Iteration 121/1000 | Loss: 0.00000832
Iteration 122/1000 | Loss: 0.00000832
Iteration 123/1000 | Loss: 0.00000832
Iteration 124/1000 | Loss: 0.00000832
Iteration 125/1000 | Loss: 0.00000831
Iteration 126/1000 | Loss: 0.00000831
Iteration 127/1000 | Loss: 0.00000831
Iteration 128/1000 | Loss: 0.00000830
Iteration 129/1000 | Loss: 0.00000830
Iteration 130/1000 | Loss: 0.00000830
Iteration 131/1000 | Loss: 0.00000830
Iteration 132/1000 | Loss: 0.00000829
Iteration 133/1000 | Loss: 0.00000829
Iteration 134/1000 | Loss: 0.00000829
Iteration 135/1000 | Loss: 0.00000829
Iteration 136/1000 | Loss: 0.00000829
Iteration 137/1000 | Loss: 0.00000829
Iteration 138/1000 | Loss: 0.00000829
Iteration 139/1000 | Loss: 0.00000829
Iteration 140/1000 | Loss: 0.00000829
Iteration 141/1000 | Loss: 0.00000829
Iteration 142/1000 | Loss: 0.00000829
Iteration 143/1000 | Loss: 0.00000829
Iteration 144/1000 | Loss: 0.00000829
Iteration 145/1000 | Loss: 0.00000828
Iteration 146/1000 | Loss: 0.00000828
Iteration 147/1000 | Loss: 0.00000828
Iteration 148/1000 | Loss: 0.00000827
Iteration 149/1000 | Loss: 0.00000827
Iteration 150/1000 | Loss: 0.00000827
Iteration 151/1000 | Loss: 0.00000826
Iteration 152/1000 | Loss: 0.00000826
Iteration 153/1000 | Loss: 0.00000826
Iteration 154/1000 | Loss: 0.00000826
Iteration 155/1000 | Loss: 0.00000826
Iteration 156/1000 | Loss: 0.00000826
Iteration 157/1000 | Loss: 0.00000826
Iteration 158/1000 | Loss: 0.00000826
Iteration 159/1000 | Loss: 0.00000825
Iteration 160/1000 | Loss: 0.00000825
Iteration 161/1000 | Loss: 0.00000825
Iteration 162/1000 | Loss: 0.00000825
Iteration 163/1000 | Loss: 0.00000825
Iteration 164/1000 | Loss: 0.00000825
Iteration 165/1000 | Loss: 0.00000825
Iteration 166/1000 | Loss: 0.00000825
Iteration 167/1000 | Loss: 0.00000825
Iteration 168/1000 | Loss: 0.00000824
Iteration 169/1000 | Loss: 0.00000824
Iteration 170/1000 | Loss: 0.00000824
Iteration 171/1000 | Loss: 0.00000824
Iteration 172/1000 | Loss: 0.00000824
Iteration 173/1000 | Loss: 0.00000824
Iteration 174/1000 | Loss: 0.00000824
Iteration 175/1000 | Loss: 0.00000824
Iteration 176/1000 | Loss: 0.00000824
Iteration 177/1000 | Loss: 0.00000824
Iteration 178/1000 | Loss: 0.00000824
Iteration 179/1000 | Loss: 0.00000824
Iteration 180/1000 | Loss: 0.00000823
Iteration 181/1000 | Loss: 0.00000823
Iteration 182/1000 | Loss: 0.00000823
Iteration 183/1000 | Loss: 0.00000823
Iteration 184/1000 | Loss: 0.00000823
Iteration 185/1000 | Loss: 0.00000823
Iteration 186/1000 | Loss: 0.00000823
Iteration 187/1000 | Loss: 0.00000823
Iteration 188/1000 | Loss: 0.00000823
Iteration 189/1000 | Loss: 0.00000822
Iteration 190/1000 | Loss: 0.00000822
Iteration 191/1000 | Loss: 0.00000822
Iteration 192/1000 | Loss: 0.00000822
Iteration 193/1000 | Loss: 0.00000822
Iteration 194/1000 | Loss: 0.00000822
Iteration 195/1000 | Loss: 0.00000822
Iteration 196/1000 | Loss: 0.00000822
Iteration 197/1000 | Loss: 0.00000822
Iteration 198/1000 | Loss: 0.00000822
Iteration 199/1000 | Loss: 0.00000822
Iteration 200/1000 | Loss: 0.00000821
Iteration 201/1000 | Loss: 0.00000821
Iteration 202/1000 | Loss: 0.00000820
Iteration 203/1000 | Loss: 0.00000820
Iteration 204/1000 | Loss: 0.00000820
Iteration 205/1000 | Loss: 0.00000820
Iteration 206/1000 | Loss: 0.00000820
Iteration 207/1000 | Loss: 0.00000820
Iteration 208/1000 | Loss: 0.00000820
Iteration 209/1000 | Loss: 0.00000820
Iteration 210/1000 | Loss: 0.00000820
Iteration 211/1000 | Loss: 0.00000820
Iteration 212/1000 | Loss: 0.00000819
Iteration 213/1000 | Loss: 0.00000819
Iteration 214/1000 | Loss: 0.00000819
Iteration 215/1000 | Loss: 0.00000819
Iteration 216/1000 | Loss: 0.00000819
Iteration 217/1000 | Loss: 0.00000819
Iteration 218/1000 | Loss: 0.00000819
Iteration 219/1000 | Loss: 0.00000819
Iteration 220/1000 | Loss: 0.00000819
Iteration 221/1000 | Loss: 0.00000818
Iteration 222/1000 | Loss: 0.00000818
Iteration 223/1000 | Loss: 0.00000818
Iteration 224/1000 | Loss: 0.00000818
Iteration 225/1000 | Loss: 0.00000818
Iteration 226/1000 | Loss: 0.00000818
Iteration 227/1000 | Loss: 0.00000818
Iteration 228/1000 | Loss: 0.00000818
Iteration 229/1000 | Loss: 0.00000818
Iteration 230/1000 | Loss: 0.00000818
Iteration 231/1000 | Loss: 0.00000817
Iteration 232/1000 | Loss: 0.00000817
Iteration 233/1000 | Loss: 0.00000817
Iteration 234/1000 | Loss: 0.00000817
Iteration 235/1000 | Loss: 0.00000817
Iteration 236/1000 | Loss: 0.00000817
Iteration 237/1000 | Loss: 0.00000817
Iteration 238/1000 | Loss: 0.00000817
Iteration 239/1000 | Loss: 0.00000817
Iteration 240/1000 | Loss: 0.00000817
Iteration 241/1000 | Loss: 0.00000817
Iteration 242/1000 | Loss: 0.00000817
Iteration 243/1000 | Loss: 0.00000817
Iteration 244/1000 | Loss: 0.00000817
Iteration 245/1000 | Loss: 0.00000817
Iteration 246/1000 | Loss: 0.00000817
Iteration 247/1000 | Loss: 0.00000817
Iteration 248/1000 | Loss: 0.00000817
Iteration 249/1000 | Loss: 0.00000817
Iteration 250/1000 | Loss: 0.00000817
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [8.170661203621421e-06, 8.170661203621421e-06, 8.170661203621421e-06, 8.170661203621421e-06, 8.170661203621421e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.170661203621421e-06

Optimization complete. Final v2v error: 2.4408376216888428 mm

Highest mean error: 2.609164237976074 mm for frame 110

Lowest mean error: 2.3149380683898926 mm for frame 199

Saving results

Total time: 43.21526575088501
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779946
Iteration 2/25 | Loss: 0.00159103
Iteration 3/25 | Loss: 0.00130411
Iteration 4/25 | Loss: 0.00128151
Iteration 5/25 | Loss: 0.00127602
Iteration 6/25 | Loss: 0.00127453
Iteration 7/25 | Loss: 0.00127443
Iteration 8/25 | Loss: 0.00127443
Iteration 9/25 | Loss: 0.00127443
Iteration 10/25 | Loss: 0.00127443
Iteration 11/25 | Loss: 0.00127443
Iteration 12/25 | Loss: 0.00127443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012744310079142451, 0.0012744310079142451, 0.0012744310079142451, 0.0012744310079142451, 0.0012744310079142451]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012744310079142451

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36021125
Iteration 2/25 | Loss: 0.00085656
Iteration 3/25 | Loss: 0.00085653
Iteration 4/25 | Loss: 0.00085653
Iteration 5/25 | Loss: 0.00085653
Iteration 6/25 | Loss: 0.00085653
Iteration 7/25 | Loss: 0.00085653
Iteration 8/25 | Loss: 0.00085653
Iteration 9/25 | Loss: 0.00085653
Iteration 10/25 | Loss: 0.00085653
Iteration 11/25 | Loss: 0.00085653
Iteration 12/25 | Loss: 0.00085653
Iteration 13/25 | Loss: 0.00085653
Iteration 14/25 | Loss: 0.00085653
Iteration 15/25 | Loss: 0.00085653
Iteration 16/25 | Loss: 0.00085653
Iteration 17/25 | Loss: 0.00085653
Iteration 18/25 | Loss: 0.00085653
Iteration 19/25 | Loss: 0.00085653
Iteration 20/25 | Loss: 0.00085653
Iteration 21/25 | Loss: 0.00085653
Iteration 22/25 | Loss: 0.00085653
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0008565280586481094, 0.0008565280586481094, 0.0008565280586481094, 0.0008565280586481094, 0.0008565280586481094]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008565280586481094

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00085653
Iteration 2/1000 | Loss: 0.00005851
Iteration 3/1000 | Loss: 0.00003568
Iteration 4/1000 | Loss: 0.00002914
Iteration 5/1000 | Loss: 0.00002753
Iteration 6/1000 | Loss: 0.00002698
Iteration 7/1000 | Loss: 0.00002664
Iteration 8/1000 | Loss: 0.00002634
Iteration 9/1000 | Loss: 0.00002613
Iteration 10/1000 | Loss: 0.00002595
Iteration 11/1000 | Loss: 0.00002585
Iteration 12/1000 | Loss: 0.00002582
Iteration 13/1000 | Loss: 0.00002571
Iteration 14/1000 | Loss: 0.00002568
Iteration 15/1000 | Loss: 0.00002561
Iteration 16/1000 | Loss: 0.00002561
Iteration 17/1000 | Loss: 0.00002558
Iteration 18/1000 | Loss: 0.00002557
Iteration 19/1000 | Loss: 0.00002557
Iteration 20/1000 | Loss: 0.00002556
Iteration 21/1000 | Loss: 0.00002556
Iteration 22/1000 | Loss: 0.00002556
Iteration 23/1000 | Loss: 0.00002556
Iteration 24/1000 | Loss: 0.00002556
Iteration 25/1000 | Loss: 0.00002556
Iteration 26/1000 | Loss: 0.00002555
Iteration 27/1000 | Loss: 0.00002555
Iteration 28/1000 | Loss: 0.00002555
Iteration 29/1000 | Loss: 0.00002554
Iteration 30/1000 | Loss: 0.00002554
Iteration 31/1000 | Loss: 0.00002554
Iteration 32/1000 | Loss: 0.00002554
Iteration 33/1000 | Loss: 0.00002554
Iteration 34/1000 | Loss: 0.00002554
Iteration 35/1000 | Loss: 0.00002553
Iteration 36/1000 | Loss: 0.00002553
Iteration 37/1000 | Loss: 0.00002553
Iteration 38/1000 | Loss: 0.00002553
Iteration 39/1000 | Loss: 0.00002553
Iteration 40/1000 | Loss: 0.00002552
Iteration 41/1000 | Loss: 0.00002552
Iteration 42/1000 | Loss: 0.00002552
Iteration 43/1000 | Loss: 0.00002552
Iteration 44/1000 | Loss: 0.00002552
Iteration 45/1000 | Loss: 0.00002552
Iteration 46/1000 | Loss: 0.00002552
Iteration 47/1000 | Loss: 0.00002552
Iteration 48/1000 | Loss: 0.00002552
Iteration 49/1000 | Loss: 0.00002552
Iteration 50/1000 | Loss: 0.00002552
Iteration 51/1000 | Loss: 0.00002552
Iteration 52/1000 | Loss: 0.00002552
Iteration 53/1000 | Loss: 0.00002552
Iteration 54/1000 | Loss: 0.00002552
Iteration 55/1000 | Loss: 0.00002552
Iteration 56/1000 | Loss: 0.00002552
Iteration 57/1000 | Loss: 0.00002552
Iteration 58/1000 | Loss: 0.00002552
Iteration 59/1000 | Loss: 0.00002552
Iteration 60/1000 | Loss: 0.00002552
Iteration 61/1000 | Loss: 0.00002552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 61. Stopping optimization.
Last 5 losses: [2.5519795599393547e-05, 2.5519795599393547e-05, 2.5519795599393547e-05, 2.5519795599393547e-05, 2.5519795599393547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5519795599393547e-05

Optimization complete. Final v2v error: 4.034434795379639 mm

Highest mean error: 4.381972789764404 mm for frame 87

Lowest mean error: 3.15822696685791 mm for frame 0

Saving results

Total time: 29.632893323898315
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741481
Iteration 2/25 | Loss: 0.00121924
Iteration 3/25 | Loss: 0.00112948
Iteration 4/25 | Loss: 0.00111596
Iteration 5/25 | Loss: 0.00111271
Iteration 6/25 | Loss: 0.00111194
Iteration 7/25 | Loss: 0.00111194
Iteration 8/25 | Loss: 0.00111194
Iteration 9/25 | Loss: 0.00111194
Iteration 10/25 | Loss: 0.00111194
Iteration 11/25 | Loss: 0.00111194
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011119392002001405, 0.0011119392002001405, 0.0011119392002001405, 0.0011119392002001405, 0.0011119392002001405]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011119392002001405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34750891
Iteration 2/25 | Loss: 0.00065203
Iteration 3/25 | Loss: 0.00065195
Iteration 4/25 | Loss: 0.00065195
Iteration 5/25 | Loss: 0.00065195
Iteration 6/25 | Loss: 0.00065195
Iteration 7/25 | Loss: 0.00065195
Iteration 8/25 | Loss: 0.00065195
Iteration 9/25 | Loss: 0.00065195
Iteration 10/25 | Loss: 0.00065195
Iteration 11/25 | Loss: 0.00065195
Iteration 12/25 | Loss: 0.00065195
Iteration 13/25 | Loss: 0.00065195
Iteration 14/25 | Loss: 0.00065195
Iteration 15/25 | Loss: 0.00065195
Iteration 16/25 | Loss: 0.00065195
Iteration 17/25 | Loss: 0.00065195
Iteration 18/25 | Loss: 0.00065195
Iteration 19/25 | Loss: 0.00065195
Iteration 20/25 | Loss: 0.00065195
Iteration 21/25 | Loss: 0.00065195
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0006519489106722176, 0.0006519489106722176, 0.0006519489106722176, 0.0006519489106722176, 0.0006519489106722176]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006519489106722176

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065195
Iteration 2/1000 | Loss: 0.00003771
Iteration 3/1000 | Loss: 0.00002725
Iteration 4/1000 | Loss: 0.00002217
Iteration 5/1000 | Loss: 0.00002059
Iteration 6/1000 | Loss: 0.00001971
Iteration 7/1000 | Loss: 0.00001911
Iteration 8/1000 | Loss: 0.00001858
Iteration 9/1000 | Loss: 0.00001829
Iteration 10/1000 | Loss: 0.00001799
Iteration 11/1000 | Loss: 0.00001795
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001761
Iteration 14/1000 | Loss: 0.00001741
Iteration 15/1000 | Loss: 0.00001723
Iteration 16/1000 | Loss: 0.00001708
Iteration 17/1000 | Loss: 0.00001704
Iteration 18/1000 | Loss: 0.00001699
Iteration 19/1000 | Loss: 0.00001695
Iteration 20/1000 | Loss: 0.00001694
Iteration 21/1000 | Loss: 0.00001693
Iteration 22/1000 | Loss: 0.00001693
Iteration 23/1000 | Loss: 0.00001692
Iteration 24/1000 | Loss: 0.00001692
Iteration 25/1000 | Loss: 0.00001692
Iteration 26/1000 | Loss: 0.00001691
Iteration 27/1000 | Loss: 0.00001691
Iteration 28/1000 | Loss: 0.00001691
Iteration 29/1000 | Loss: 0.00001691
Iteration 30/1000 | Loss: 0.00001691
Iteration 31/1000 | Loss: 0.00001691
Iteration 32/1000 | Loss: 0.00001691
Iteration 33/1000 | Loss: 0.00001691
Iteration 34/1000 | Loss: 0.00001689
Iteration 35/1000 | Loss: 0.00001689
Iteration 36/1000 | Loss: 0.00001688
Iteration 37/1000 | Loss: 0.00001687
Iteration 38/1000 | Loss: 0.00001687
Iteration 39/1000 | Loss: 0.00001687
Iteration 40/1000 | Loss: 0.00001686
Iteration 41/1000 | Loss: 0.00001686
Iteration 42/1000 | Loss: 0.00001685
Iteration 43/1000 | Loss: 0.00001685
Iteration 44/1000 | Loss: 0.00001684
Iteration 45/1000 | Loss: 0.00001684
Iteration 46/1000 | Loss: 0.00001684
Iteration 47/1000 | Loss: 0.00001684
Iteration 48/1000 | Loss: 0.00001683
Iteration 49/1000 | Loss: 0.00001683
Iteration 50/1000 | Loss: 0.00001683
Iteration 51/1000 | Loss: 0.00001683
Iteration 52/1000 | Loss: 0.00001683
Iteration 53/1000 | Loss: 0.00001683
Iteration 54/1000 | Loss: 0.00001682
Iteration 55/1000 | Loss: 0.00001682
Iteration 56/1000 | Loss: 0.00001682
Iteration 57/1000 | Loss: 0.00001682
Iteration 58/1000 | Loss: 0.00001681
Iteration 59/1000 | Loss: 0.00001681
Iteration 60/1000 | Loss: 0.00001681
Iteration 61/1000 | Loss: 0.00001681
Iteration 62/1000 | Loss: 0.00001681
Iteration 63/1000 | Loss: 0.00001681
Iteration 64/1000 | Loss: 0.00001681
Iteration 65/1000 | Loss: 0.00001680
Iteration 66/1000 | Loss: 0.00001680
Iteration 67/1000 | Loss: 0.00001680
Iteration 68/1000 | Loss: 0.00001680
Iteration 69/1000 | Loss: 0.00001680
Iteration 70/1000 | Loss: 0.00001680
Iteration 71/1000 | Loss: 0.00001680
Iteration 72/1000 | Loss: 0.00001679
Iteration 73/1000 | Loss: 0.00001679
Iteration 74/1000 | Loss: 0.00001679
Iteration 75/1000 | Loss: 0.00001679
Iteration 76/1000 | Loss: 0.00001679
Iteration 77/1000 | Loss: 0.00001679
Iteration 78/1000 | Loss: 0.00001678
Iteration 79/1000 | Loss: 0.00001678
Iteration 80/1000 | Loss: 0.00001677
Iteration 81/1000 | Loss: 0.00001677
Iteration 82/1000 | Loss: 0.00001677
Iteration 83/1000 | Loss: 0.00001676
Iteration 84/1000 | Loss: 0.00001676
Iteration 85/1000 | Loss: 0.00001676
Iteration 86/1000 | Loss: 0.00001676
Iteration 87/1000 | Loss: 0.00001676
Iteration 88/1000 | Loss: 0.00001676
Iteration 89/1000 | Loss: 0.00001676
Iteration 90/1000 | Loss: 0.00001676
Iteration 91/1000 | Loss: 0.00001676
Iteration 92/1000 | Loss: 0.00001676
Iteration 93/1000 | Loss: 0.00001676
Iteration 94/1000 | Loss: 0.00001675
Iteration 95/1000 | Loss: 0.00001675
Iteration 96/1000 | Loss: 0.00001675
Iteration 97/1000 | Loss: 0.00001675
Iteration 98/1000 | Loss: 0.00001675
Iteration 99/1000 | Loss: 0.00001674
Iteration 100/1000 | Loss: 0.00001673
Iteration 101/1000 | Loss: 0.00001673
Iteration 102/1000 | Loss: 0.00001673
Iteration 103/1000 | Loss: 0.00001673
Iteration 104/1000 | Loss: 0.00001673
Iteration 105/1000 | Loss: 0.00001673
Iteration 106/1000 | Loss: 0.00001673
Iteration 107/1000 | Loss: 0.00001672
Iteration 108/1000 | Loss: 0.00001672
Iteration 109/1000 | Loss: 0.00001672
Iteration 110/1000 | Loss: 0.00001672
Iteration 111/1000 | Loss: 0.00001672
Iteration 112/1000 | Loss: 0.00001672
Iteration 113/1000 | Loss: 0.00001672
Iteration 114/1000 | Loss: 0.00001672
Iteration 115/1000 | Loss: 0.00001671
Iteration 116/1000 | Loss: 0.00001671
Iteration 117/1000 | Loss: 0.00001671
Iteration 118/1000 | Loss: 0.00001671
Iteration 119/1000 | Loss: 0.00001670
Iteration 120/1000 | Loss: 0.00001670
Iteration 121/1000 | Loss: 0.00001670
Iteration 122/1000 | Loss: 0.00001670
Iteration 123/1000 | Loss: 0.00001670
Iteration 124/1000 | Loss: 0.00001670
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001670
Iteration 128/1000 | Loss: 0.00001670
Iteration 129/1000 | Loss: 0.00001670
Iteration 130/1000 | Loss: 0.00001670
Iteration 131/1000 | Loss: 0.00001670
Iteration 132/1000 | Loss: 0.00001669
Iteration 133/1000 | Loss: 0.00001669
Iteration 134/1000 | Loss: 0.00001669
Iteration 135/1000 | Loss: 0.00001669
Iteration 136/1000 | Loss: 0.00001668
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001668
Iteration 139/1000 | Loss: 0.00001668
Iteration 140/1000 | Loss: 0.00001667
Iteration 141/1000 | Loss: 0.00001667
Iteration 142/1000 | Loss: 0.00001667
Iteration 143/1000 | Loss: 0.00001667
Iteration 144/1000 | Loss: 0.00001667
Iteration 145/1000 | Loss: 0.00001667
Iteration 146/1000 | Loss: 0.00001667
Iteration 147/1000 | Loss: 0.00001667
Iteration 148/1000 | Loss: 0.00001666
Iteration 149/1000 | Loss: 0.00001666
Iteration 150/1000 | Loss: 0.00001666
Iteration 151/1000 | Loss: 0.00001665
Iteration 152/1000 | Loss: 0.00001665
Iteration 153/1000 | Loss: 0.00001665
Iteration 154/1000 | Loss: 0.00001665
Iteration 155/1000 | Loss: 0.00001664
Iteration 156/1000 | Loss: 0.00001664
Iteration 157/1000 | Loss: 0.00001664
Iteration 158/1000 | Loss: 0.00001664
Iteration 159/1000 | Loss: 0.00001664
Iteration 160/1000 | Loss: 0.00001664
Iteration 161/1000 | Loss: 0.00001664
Iteration 162/1000 | Loss: 0.00001664
Iteration 163/1000 | Loss: 0.00001664
Iteration 164/1000 | Loss: 0.00001663
Iteration 165/1000 | Loss: 0.00001663
Iteration 166/1000 | Loss: 0.00001663
Iteration 167/1000 | Loss: 0.00001662
Iteration 168/1000 | Loss: 0.00001662
Iteration 169/1000 | Loss: 0.00001662
Iteration 170/1000 | Loss: 0.00001662
Iteration 171/1000 | Loss: 0.00001662
Iteration 172/1000 | Loss: 0.00001662
Iteration 173/1000 | Loss: 0.00001662
Iteration 174/1000 | Loss: 0.00001662
Iteration 175/1000 | Loss: 0.00001662
Iteration 176/1000 | Loss: 0.00001662
Iteration 177/1000 | Loss: 0.00001662
Iteration 178/1000 | Loss: 0.00001662
Iteration 179/1000 | Loss: 0.00001662
Iteration 180/1000 | Loss: 0.00001662
Iteration 181/1000 | Loss: 0.00001662
Iteration 182/1000 | Loss: 0.00001662
Iteration 183/1000 | Loss: 0.00001662
Iteration 184/1000 | Loss: 0.00001661
Iteration 185/1000 | Loss: 0.00001661
Iteration 186/1000 | Loss: 0.00001661
Iteration 187/1000 | Loss: 0.00001661
Iteration 188/1000 | Loss: 0.00001661
Iteration 189/1000 | Loss: 0.00001661
Iteration 190/1000 | Loss: 0.00001661
Iteration 191/1000 | Loss: 0.00001661
Iteration 192/1000 | Loss: 0.00001661
Iteration 193/1000 | Loss: 0.00001661
Iteration 194/1000 | Loss: 0.00001661
Iteration 195/1000 | Loss: 0.00001661
Iteration 196/1000 | Loss: 0.00001661
Iteration 197/1000 | Loss: 0.00001661
Iteration 198/1000 | Loss: 0.00001661
Iteration 199/1000 | Loss: 0.00001661
Iteration 200/1000 | Loss: 0.00001660
Iteration 201/1000 | Loss: 0.00001660
Iteration 202/1000 | Loss: 0.00001660
Iteration 203/1000 | Loss: 0.00001660
Iteration 204/1000 | Loss: 0.00001660
Iteration 205/1000 | Loss: 0.00001660
Iteration 206/1000 | Loss: 0.00001660
Iteration 207/1000 | Loss: 0.00001660
Iteration 208/1000 | Loss: 0.00001660
Iteration 209/1000 | Loss: 0.00001660
Iteration 210/1000 | Loss: 0.00001660
Iteration 211/1000 | Loss: 0.00001660
Iteration 212/1000 | Loss: 0.00001659
Iteration 213/1000 | Loss: 0.00001659
Iteration 214/1000 | Loss: 0.00001659
Iteration 215/1000 | Loss: 0.00001659
Iteration 216/1000 | Loss: 0.00001659
Iteration 217/1000 | Loss: 0.00001659
Iteration 218/1000 | Loss: 0.00001659
Iteration 219/1000 | Loss: 0.00001659
Iteration 220/1000 | Loss: 0.00001659
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 220. Stopping optimization.
Last 5 losses: [1.659433291933965e-05, 1.659433291933965e-05, 1.659433291933965e-05, 1.659433291933965e-05, 1.659433291933965e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.659433291933965e-05

Optimization complete. Final v2v error: 3.409606695175171 mm

Highest mean error: 3.6651628017425537 mm for frame 41

Lowest mean error: 3.2643072605133057 mm for frame 98

Saving results

Total time: 44.44520115852356
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01086065
Iteration 2/25 | Loss: 0.00226086
Iteration 3/25 | Loss: 0.00156017
Iteration 4/25 | Loss: 0.00143719
Iteration 5/25 | Loss: 0.00140336
Iteration 6/25 | Loss: 0.00139017
Iteration 7/25 | Loss: 0.00137148
Iteration 8/25 | Loss: 0.00136180
Iteration 9/25 | Loss: 0.00135537
Iteration 10/25 | Loss: 0.00135420
Iteration 11/25 | Loss: 0.00135356
Iteration 12/25 | Loss: 0.00135251
Iteration 13/25 | Loss: 0.00135139
Iteration 14/25 | Loss: 0.00134938
Iteration 15/25 | Loss: 0.00134774
Iteration 16/25 | Loss: 0.00134726
Iteration 17/25 | Loss: 0.00134723
Iteration 18/25 | Loss: 0.00134723
Iteration 19/25 | Loss: 0.00134723
Iteration 20/25 | Loss: 0.00134723
Iteration 21/25 | Loss: 0.00134723
Iteration 22/25 | Loss: 0.00134723
Iteration 23/25 | Loss: 0.00134723
Iteration 24/25 | Loss: 0.00134723
Iteration 25/25 | Loss: 0.00134723

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.45506239
Iteration 2/25 | Loss: 0.00176826
Iteration 3/25 | Loss: 0.00176826
Iteration 4/25 | Loss: 0.00176826
Iteration 5/25 | Loss: 0.00176826
Iteration 6/25 | Loss: 0.00176826
Iteration 7/25 | Loss: 0.00176826
Iteration 8/25 | Loss: 0.00176826
Iteration 9/25 | Loss: 0.00176826
Iteration 10/25 | Loss: 0.00176826
Iteration 11/25 | Loss: 0.00176826
Iteration 12/25 | Loss: 0.00176826
Iteration 13/25 | Loss: 0.00176826
Iteration 14/25 | Loss: 0.00176826
Iteration 15/25 | Loss: 0.00176826
Iteration 16/25 | Loss: 0.00176826
Iteration 17/25 | Loss: 0.00176826
Iteration 18/25 | Loss: 0.00176826
Iteration 19/25 | Loss: 0.00176826
Iteration 20/25 | Loss: 0.00176826
Iteration 21/25 | Loss: 0.00176826
Iteration 22/25 | Loss: 0.00176826
Iteration 23/25 | Loss: 0.00176826
Iteration 24/25 | Loss: 0.00176826
Iteration 25/25 | Loss: 0.00176826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176826
Iteration 2/1000 | Loss: 0.00021162
Iteration 3/1000 | Loss: 0.00039085
Iteration 4/1000 | Loss: 0.00018517
Iteration 5/1000 | Loss: 0.00025254
Iteration 6/1000 | Loss: 0.00030158
Iteration 7/1000 | Loss: 0.00014939
Iteration 8/1000 | Loss: 0.00010712
Iteration 9/1000 | Loss: 0.00008595
Iteration 10/1000 | Loss: 0.00007898
Iteration 11/1000 | Loss: 0.00006916
Iteration 12/1000 | Loss: 0.00007265
Iteration 13/1000 | Loss: 0.00011475
Iteration 14/1000 | Loss: 0.00010146
Iteration 15/1000 | Loss: 0.00009786
Iteration 16/1000 | Loss: 0.00007609
Iteration 17/1000 | Loss: 0.00007191
Iteration 18/1000 | Loss: 0.00006447
Iteration 19/1000 | Loss: 0.00006516
Iteration 20/1000 | Loss: 0.00005311
Iteration 21/1000 | Loss: 0.00006454
Iteration 22/1000 | Loss: 0.00006832
Iteration 23/1000 | Loss: 0.00006983
Iteration 24/1000 | Loss: 0.00005041
Iteration 25/1000 | Loss: 0.00006341
Iteration 26/1000 | Loss: 0.00006719
Iteration 27/1000 | Loss: 0.00008024
Iteration 28/1000 | Loss: 0.00008277
Iteration 29/1000 | Loss: 0.00006943
Iteration 30/1000 | Loss: 0.00005535
Iteration 31/1000 | Loss: 0.00004811
Iteration 32/1000 | Loss: 0.00004386
Iteration 33/1000 | Loss: 0.00005499
Iteration 34/1000 | Loss: 0.00004317
Iteration 35/1000 | Loss: 0.00004201
Iteration 36/1000 | Loss: 0.00004391
Iteration 37/1000 | Loss: 0.00009808
Iteration 38/1000 | Loss: 0.00005132
Iteration 39/1000 | Loss: 0.00004494
Iteration 40/1000 | Loss: 0.00004682
Iteration 41/1000 | Loss: 0.00004401
Iteration 42/1000 | Loss: 0.00004660
Iteration 43/1000 | Loss: 0.00009354
Iteration 44/1000 | Loss: 0.00004647
Iteration 45/1000 | Loss: 0.00004515
Iteration 46/1000 | Loss: 0.00004438
Iteration 47/1000 | Loss: 0.00004485
Iteration 48/1000 | Loss: 0.00004112
Iteration 49/1000 | Loss: 0.00005262
Iteration 50/1000 | Loss: 0.00004527
Iteration 51/1000 | Loss: 0.00005385
Iteration 52/1000 | Loss: 0.00005908
Iteration 53/1000 | Loss: 0.00005914
Iteration 54/1000 | Loss: 0.00005840
Iteration 55/1000 | Loss: 0.00006267
Iteration 56/1000 | Loss: 0.00004535
Iteration 57/1000 | Loss: 0.00006411
Iteration 58/1000 | Loss: 0.00005611
Iteration 59/1000 | Loss: 0.00005651
Iteration 60/1000 | Loss: 0.00004719
Iteration 61/1000 | Loss: 0.00005422
Iteration 62/1000 | Loss: 0.00004548
Iteration 63/1000 | Loss: 0.00005740
Iteration 64/1000 | Loss: 0.00006164
Iteration 65/1000 | Loss: 0.00009679
Iteration 66/1000 | Loss: 0.00005901
Iteration 67/1000 | Loss: 0.00008111
Iteration 68/1000 | Loss: 0.00004833
Iteration 69/1000 | Loss: 0.00006132
Iteration 70/1000 | Loss: 0.00005785
Iteration 71/1000 | Loss: 0.00005929
Iteration 72/1000 | Loss: 0.00004992
Iteration 73/1000 | Loss: 0.00004654
Iteration 74/1000 | Loss: 0.00005277
Iteration 75/1000 | Loss: 0.00005648
Iteration 76/1000 | Loss: 0.00005743
Iteration 77/1000 | Loss: 0.00004924
Iteration 78/1000 | Loss: 0.00005425
Iteration 79/1000 | Loss: 0.00005325
Iteration 80/1000 | Loss: 0.00005933
Iteration 81/1000 | Loss: 0.00006382
Iteration 82/1000 | Loss: 0.00005904
Iteration 83/1000 | Loss: 0.00006200
Iteration 84/1000 | Loss: 0.00005057
Iteration 85/1000 | Loss: 0.00004486
Iteration 86/1000 | Loss: 0.00004352
Iteration 87/1000 | Loss: 0.00005620
Iteration 88/1000 | Loss: 0.00006051
Iteration 89/1000 | Loss: 0.00006049
Iteration 90/1000 | Loss: 0.00008785
Iteration 91/1000 | Loss: 0.00005172
Iteration 92/1000 | Loss: 0.00004591
Iteration 93/1000 | Loss: 0.00005520
Iteration 94/1000 | Loss: 0.00006636
Iteration 95/1000 | Loss: 0.00006447
Iteration 96/1000 | Loss: 0.00005111
Iteration 97/1000 | Loss: 0.00005369
Iteration 98/1000 | Loss: 0.00005758
Iteration 99/1000 | Loss: 0.00006021
Iteration 100/1000 | Loss: 0.00006377
Iteration 101/1000 | Loss: 0.00005491
Iteration 102/1000 | Loss: 0.00006422
Iteration 103/1000 | Loss: 0.00005244
Iteration 104/1000 | Loss: 0.00006499
Iteration 105/1000 | Loss: 0.00005842
Iteration 106/1000 | Loss: 0.00005801
Iteration 107/1000 | Loss: 0.00005822
Iteration 108/1000 | Loss: 0.00004850
Iteration 109/1000 | Loss: 0.00004512
Iteration 110/1000 | Loss: 0.00004295
Iteration 111/1000 | Loss: 0.00004600
Iteration 112/1000 | Loss: 0.00005526
Iteration 113/1000 | Loss: 0.00005862
Iteration 114/1000 | Loss: 0.00005012
Iteration 115/1000 | Loss: 0.00004992
Iteration 116/1000 | Loss: 0.00004709
Iteration 117/1000 | Loss: 0.00004917
Iteration 118/1000 | Loss: 0.00004901
Iteration 119/1000 | Loss: 0.00004770
Iteration 120/1000 | Loss: 0.00007909
Iteration 121/1000 | Loss: 0.00004686
Iteration 122/1000 | Loss: 0.00007912
Iteration 123/1000 | Loss: 0.00007831
Iteration 124/1000 | Loss: 0.00009012
Iteration 125/1000 | Loss: 0.00004792
Iteration 126/1000 | Loss: 0.00007496
Iteration 127/1000 | Loss: 0.00005518
Iteration 128/1000 | Loss: 0.00007393
Iteration 129/1000 | Loss: 0.00005456
Iteration 130/1000 | Loss: 0.00006904
Iteration 131/1000 | Loss: 0.00005313
Iteration 132/1000 | Loss: 0.00006058
Iteration 133/1000 | Loss: 0.00005383
Iteration 134/1000 | Loss: 0.00006100
Iteration 135/1000 | Loss: 0.00005221
Iteration 136/1000 | Loss: 0.00006905
Iteration 137/1000 | Loss: 0.00005773
Iteration 138/1000 | Loss: 0.00004856
Iteration 139/1000 | Loss: 0.00004976
Iteration 140/1000 | Loss: 0.00007251
Iteration 141/1000 | Loss: 0.00005025
Iteration 142/1000 | Loss: 0.00005168
Iteration 143/1000 | Loss: 0.00004755
Iteration 144/1000 | Loss: 0.00004983
Iteration 145/1000 | Loss: 0.00005177
Iteration 146/1000 | Loss: 0.00005661
Iteration 147/1000 | Loss: 0.00004743
Iteration 148/1000 | Loss: 0.00004879
Iteration 149/1000 | Loss: 0.00004593
Iteration 150/1000 | Loss: 0.00004802
Iteration 151/1000 | Loss: 0.00004605
Iteration 152/1000 | Loss: 0.00004563
Iteration 153/1000 | Loss: 0.00004929
Iteration 154/1000 | Loss: 0.00004423
Iteration 155/1000 | Loss: 0.00004804
Iteration 156/1000 | Loss: 0.00005168
Iteration 157/1000 | Loss: 0.00004639
Iteration 158/1000 | Loss: 0.00005321
Iteration 159/1000 | Loss: 0.00004822
Iteration 160/1000 | Loss: 0.00004894
Iteration 161/1000 | Loss: 0.00005171
Iteration 162/1000 | Loss: 0.00004525
Iteration 163/1000 | Loss: 0.00004428
Iteration 164/1000 | Loss: 0.00004959
Iteration 165/1000 | Loss: 0.00005196
Iteration 166/1000 | Loss: 0.00004868
Iteration 167/1000 | Loss: 0.00005312
Iteration 168/1000 | Loss: 0.00004608
Iteration 169/1000 | Loss: 0.00004194
Iteration 170/1000 | Loss: 0.00004625
Iteration 171/1000 | Loss: 0.00004749
Iteration 172/1000 | Loss: 0.00004377
Iteration 173/1000 | Loss: 0.00010561
Iteration 174/1000 | Loss: 0.00004363
Iteration 175/1000 | Loss: 0.00005079
Iteration 176/1000 | Loss: 0.00004329
Iteration 177/1000 | Loss: 0.00004614
Iteration 178/1000 | Loss: 0.00006547
Iteration 179/1000 | Loss: 0.00004454
Iteration 180/1000 | Loss: 0.00005036
Iteration 181/1000 | Loss: 0.00012142
Iteration 182/1000 | Loss: 0.00005865
Iteration 183/1000 | Loss: 0.00005446
Iteration 184/1000 | Loss: 0.00004931
Iteration 185/1000 | Loss: 0.00004481
Iteration 186/1000 | Loss: 0.00007649
Iteration 187/1000 | Loss: 0.00005291
Iteration 188/1000 | Loss: 0.00007737
Iteration 189/1000 | Loss: 0.00004381
Iteration 190/1000 | Loss: 0.00004100
Iteration 191/1000 | Loss: 0.00003981
Iteration 192/1000 | Loss: 0.00005083
Iteration 193/1000 | Loss: 0.00005352
Iteration 194/1000 | Loss: 0.00008426
Iteration 195/1000 | Loss: 0.00004511
Iteration 196/1000 | Loss: 0.00004302
Iteration 197/1000 | Loss: 0.00004667
Iteration 198/1000 | Loss: 0.00005179
Iteration 199/1000 | Loss: 0.00004655
Iteration 200/1000 | Loss: 0.00005011
Iteration 201/1000 | Loss: 0.00003957
Iteration 202/1000 | Loss: 0.00004746
Iteration 203/1000 | Loss: 0.00005107
Iteration 204/1000 | Loss: 0.00004618
Iteration 205/1000 | Loss: 0.00005009
Iteration 206/1000 | Loss: 0.00004446
Iteration 207/1000 | Loss: 0.00009102
Iteration 208/1000 | Loss: 0.00007575
Iteration 209/1000 | Loss: 0.00005459
Iteration 210/1000 | Loss: 0.00004623
Iteration 211/1000 | Loss: 0.00006541
Iteration 212/1000 | Loss: 0.00004593
Iteration 213/1000 | Loss: 0.00005055
Iteration 214/1000 | Loss: 0.00005055
Iteration 215/1000 | Loss: 0.00006347
Iteration 216/1000 | Loss: 0.00006238
Iteration 217/1000 | Loss: 0.00004203
Iteration 218/1000 | Loss: 0.00007231
Iteration 219/1000 | Loss: 0.00004356
Iteration 220/1000 | Loss: 0.00004178
Iteration 221/1000 | Loss: 0.00004085
Iteration 222/1000 | Loss: 0.00004669
Iteration 223/1000 | Loss: 0.00004618
Iteration 224/1000 | Loss: 0.00004051
Iteration 225/1000 | Loss: 0.00003934
Iteration 226/1000 | Loss: 0.00007044
Iteration 227/1000 | Loss: 0.00007284
Iteration 228/1000 | Loss: 0.00004908
Iteration 229/1000 | Loss: 0.00004978
Iteration 230/1000 | Loss: 0.00004083
Iteration 231/1000 | Loss: 0.00006881
Iteration 232/1000 | Loss: 0.00006097
Iteration 233/1000 | Loss: 0.00005859
Iteration 234/1000 | Loss: 0.00004529
Iteration 235/1000 | Loss: 0.00004219
Iteration 236/1000 | Loss: 0.00004113
Iteration 237/1000 | Loss: 0.00006569
Iteration 238/1000 | Loss: 0.00004035
Iteration 239/1000 | Loss: 0.00003850
Iteration 240/1000 | Loss: 0.00005395
Iteration 241/1000 | Loss: 0.00006763
Iteration 242/1000 | Loss: 0.00004412
Iteration 243/1000 | Loss: 0.00006001
Iteration 244/1000 | Loss: 0.00003909
Iteration 245/1000 | Loss: 0.00005358
Iteration 246/1000 | Loss: 0.00006275
Iteration 247/1000 | Loss: 0.00004583
Iteration 248/1000 | Loss: 0.00005053
Iteration 249/1000 | Loss: 0.00005034
Iteration 250/1000 | Loss: 0.00005316
Iteration 251/1000 | Loss: 0.00005878
Iteration 252/1000 | Loss: 0.00004977
Iteration 253/1000 | Loss: 0.00005091
Iteration 254/1000 | Loss: 0.00003939
Iteration 255/1000 | Loss: 0.00003819
Iteration 256/1000 | Loss: 0.00003768
Iteration 257/1000 | Loss: 0.00003766
Iteration 258/1000 | Loss: 0.00003761
Iteration 259/1000 | Loss: 0.00003761
Iteration 260/1000 | Loss: 0.00003760
Iteration 261/1000 | Loss: 0.00003760
Iteration 262/1000 | Loss: 0.00003760
Iteration 263/1000 | Loss: 0.00003759
Iteration 264/1000 | Loss: 0.00003759
Iteration 265/1000 | Loss: 0.00003759
Iteration 266/1000 | Loss: 0.00003758
Iteration 267/1000 | Loss: 0.00003758
Iteration 268/1000 | Loss: 0.00003758
Iteration 269/1000 | Loss: 0.00003758
Iteration 270/1000 | Loss: 0.00003757
Iteration 271/1000 | Loss: 0.00003754
Iteration 272/1000 | Loss: 0.00003754
Iteration 273/1000 | Loss: 0.00003754
Iteration 274/1000 | Loss: 0.00003754
Iteration 275/1000 | Loss: 0.00003754
Iteration 276/1000 | Loss: 0.00003754
Iteration 277/1000 | Loss: 0.00003754
Iteration 278/1000 | Loss: 0.00003754
Iteration 279/1000 | Loss: 0.00003754
Iteration 280/1000 | Loss: 0.00003754
Iteration 281/1000 | Loss: 0.00003754
Iteration 282/1000 | Loss: 0.00003754
Iteration 283/1000 | Loss: 0.00003754
Iteration 284/1000 | Loss: 0.00003754
Iteration 285/1000 | Loss: 0.00003754
Iteration 286/1000 | Loss: 0.00003752
Iteration 287/1000 | Loss: 0.00003752
Iteration 288/1000 | Loss: 0.00003752
Iteration 289/1000 | Loss: 0.00003752
Iteration 290/1000 | Loss: 0.00003752
Iteration 291/1000 | Loss: 0.00003752
Iteration 292/1000 | Loss: 0.00003752
Iteration 293/1000 | Loss: 0.00005258
Iteration 294/1000 | Loss: 0.00004130
Iteration 295/1000 | Loss: 0.00005387
Iteration 296/1000 | Loss: 0.00004002
Iteration 297/1000 | Loss: 0.00003911
Iteration 298/1000 | Loss: 0.00003865
Iteration 299/1000 | Loss: 0.00003844
Iteration 300/1000 | Loss: 0.00003813
Iteration 301/1000 | Loss: 0.00003761
Iteration 302/1000 | Loss: 0.00003726
Iteration 303/1000 | Loss: 0.00003705
Iteration 304/1000 | Loss: 0.00003702
Iteration 305/1000 | Loss: 0.00003699
Iteration 306/1000 | Loss: 0.00003698
Iteration 307/1000 | Loss: 0.00003698
Iteration 308/1000 | Loss: 0.00003698
Iteration 309/1000 | Loss: 0.00003698
Iteration 310/1000 | Loss: 0.00003698
Iteration 311/1000 | Loss: 0.00003697
Iteration 312/1000 | Loss: 0.00003697
Iteration 313/1000 | Loss: 0.00003696
Iteration 314/1000 | Loss: 0.00003696
Iteration 315/1000 | Loss: 0.00003695
Iteration 316/1000 | Loss: 0.00003694
Iteration 317/1000 | Loss: 0.00003694
Iteration 318/1000 | Loss: 0.00003694
Iteration 319/1000 | Loss: 0.00003694
Iteration 320/1000 | Loss: 0.00003694
Iteration 321/1000 | Loss: 0.00003694
Iteration 322/1000 | Loss: 0.00003694
Iteration 323/1000 | Loss: 0.00003694
Iteration 324/1000 | Loss: 0.00003694
Iteration 325/1000 | Loss: 0.00003694
Iteration 326/1000 | Loss: 0.00003694
Iteration 327/1000 | Loss: 0.00003694
Iteration 328/1000 | Loss: 0.00003694
Iteration 329/1000 | Loss: 0.00003694
Iteration 330/1000 | Loss: 0.00003693
Iteration 331/1000 | Loss: 0.00003693
Iteration 332/1000 | Loss: 0.00003693
Iteration 333/1000 | Loss: 0.00003693
Iteration 334/1000 | Loss: 0.00003693
Iteration 335/1000 | Loss: 0.00003693
Iteration 336/1000 | Loss: 0.00003693
Iteration 337/1000 | Loss: 0.00003693
Iteration 338/1000 | Loss: 0.00003693
Iteration 339/1000 | Loss: 0.00003692
Iteration 340/1000 | Loss: 0.00003692
Iteration 341/1000 | Loss: 0.00003692
Iteration 342/1000 | Loss: 0.00003692
Iteration 343/1000 | Loss: 0.00003691
Iteration 344/1000 | Loss: 0.00003691
Iteration 345/1000 | Loss: 0.00003691
Iteration 346/1000 | Loss: 0.00003691
Iteration 347/1000 | Loss: 0.00003690
Iteration 348/1000 | Loss: 0.00003690
Iteration 349/1000 | Loss: 0.00003690
Iteration 350/1000 | Loss: 0.00003690
Iteration 351/1000 | Loss: 0.00003689
Iteration 352/1000 | Loss: 0.00003689
Iteration 353/1000 | Loss: 0.00003689
Iteration 354/1000 | Loss: 0.00003689
Iteration 355/1000 | Loss: 0.00003688
Iteration 356/1000 | Loss: 0.00003688
Iteration 357/1000 | Loss: 0.00003688
Iteration 358/1000 | Loss: 0.00003688
Iteration 359/1000 | Loss: 0.00003687
Iteration 360/1000 | Loss: 0.00003687
Iteration 361/1000 | Loss: 0.00003687
Iteration 362/1000 | Loss: 0.00003687
Iteration 363/1000 | Loss: 0.00003686
Iteration 364/1000 | Loss: 0.00003686
Iteration 365/1000 | Loss: 0.00003686
Iteration 366/1000 | Loss: 0.00003685
Iteration 367/1000 | Loss: 0.00003685
Iteration 368/1000 | Loss: 0.00003685
Iteration 369/1000 | Loss: 0.00003684
Iteration 370/1000 | Loss: 0.00003684
Iteration 371/1000 | Loss: 0.00003684
Iteration 372/1000 | Loss: 0.00003683
Iteration 373/1000 | Loss: 0.00003683
Iteration 374/1000 | Loss: 0.00003683
Iteration 375/1000 | Loss: 0.00003682
Iteration 376/1000 | Loss: 0.00003682
Iteration 377/1000 | Loss: 0.00003682
Iteration 378/1000 | Loss: 0.00003681
Iteration 379/1000 | Loss: 0.00003680
Iteration 380/1000 | Loss: 0.00003680
Iteration 381/1000 | Loss: 0.00003680
Iteration 382/1000 | Loss: 0.00003680
Iteration 383/1000 | Loss: 0.00003680
Iteration 384/1000 | Loss: 0.00003680
Iteration 385/1000 | Loss: 0.00003680
Iteration 386/1000 | Loss: 0.00003679
Iteration 387/1000 | Loss: 0.00003679
Iteration 388/1000 | Loss: 0.00003679
Iteration 389/1000 | Loss: 0.00003679
Iteration 390/1000 | Loss: 0.00003679
Iteration 391/1000 | Loss: 0.00003679
Iteration 392/1000 | Loss: 0.00003679
Iteration 393/1000 | Loss: 0.00003679
Iteration 394/1000 | Loss: 0.00003679
Iteration 395/1000 | Loss: 0.00003679
Iteration 396/1000 | Loss: 0.00003679
Iteration 397/1000 | Loss: 0.00003679
Iteration 398/1000 | Loss: 0.00003679
Iteration 399/1000 | Loss: 0.00003679
Iteration 400/1000 | Loss: 0.00003679
Iteration 401/1000 | Loss: 0.00003679
Iteration 402/1000 | Loss: 0.00003679
Iteration 403/1000 | Loss: 0.00003679
Iteration 404/1000 | Loss: 0.00003679
Iteration 405/1000 | Loss: 0.00003679
Iteration 406/1000 | Loss: 0.00003679
Iteration 407/1000 | Loss: 0.00003679
Iteration 408/1000 | Loss: 0.00003679
Iteration 409/1000 | Loss: 0.00003679
Iteration 410/1000 | Loss: 0.00003678
Iteration 411/1000 | Loss: 0.00003678
Iteration 412/1000 | Loss: 0.00003678
Iteration 413/1000 | Loss: 0.00003678
Iteration 414/1000 | Loss: 0.00003678
Iteration 415/1000 | Loss: 0.00003678
Iteration 416/1000 | Loss: 0.00003678
Iteration 417/1000 | Loss: 0.00003678
Iteration 418/1000 | Loss: 0.00003678
Iteration 419/1000 | Loss: 0.00003678
Iteration 420/1000 | Loss: 0.00003678
Iteration 421/1000 | Loss: 0.00003678
Iteration 422/1000 | Loss: 0.00003678
Iteration 423/1000 | Loss: 0.00003678
Iteration 424/1000 | Loss: 0.00003678
Iteration 425/1000 | Loss: 0.00003677
Iteration 426/1000 | Loss: 0.00003677
Iteration 427/1000 | Loss: 0.00003677
Iteration 428/1000 | Loss: 0.00003677
Iteration 429/1000 | Loss: 0.00003677
Iteration 430/1000 | Loss: 0.00003677
Iteration 431/1000 | Loss: 0.00003677
Iteration 432/1000 | Loss: 0.00003677
Iteration 433/1000 | Loss: 0.00003677
Iteration 434/1000 | Loss: 0.00003677
Iteration 435/1000 | Loss: 0.00003677
Iteration 436/1000 | Loss: 0.00003677
Iteration 437/1000 | Loss: 0.00003677
Iteration 438/1000 | Loss: 0.00003677
Iteration 439/1000 | Loss: 0.00003677
Iteration 440/1000 | Loss: 0.00003676
Iteration 441/1000 | Loss: 0.00003676
Iteration 442/1000 | Loss: 0.00003676
Iteration 443/1000 | Loss: 0.00003676
Iteration 444/1000 | Loss: 0.00003676
Iteration 445/1000 | Loss: 0.00003676
Iteration 446/1000 | Loss: 0.00003676
Iteration 447/1000 | Loss: 0.00003676
Iteration 448/1000 | Loss: 0.00003676
Iteration 449/1000 | Loss: 0.00003676
Iteration 450/1000 | Loss: 0.00003676
Iteration 451/1000 | Loss: 0.00003676
Iteration 452/1000 | Loss: 0.00003676
Iteration 453/1000 | Loss: 0.00003676
Iteration 454/1000 | Loss: 0.00003676
Iteration 455/1000 | Loss: 0.00003676
Iteration 456/1000 | Loss: 0.00003676
Iteration 457/1000 | Loss: 0.00003676
Iteration 458/1000 | Loss: 0.00003676
Iteration 459/1000 | Loss: 0.00003676
Iteration 460/1000 | Loss: 0.00003675
Iteration 461/1000 | Loss: 0.00003675
Iteration 462/1000 | Loss: 0.00003675
Iteration 463/1000 | Loss: 0.00003675
Iteration 464/1000 | Loss: 0.00003675
Iteration 465/1000 | Loss: 0.00003675
Iteration 466/1000 | Loss: 0.00003675
Iteration 467/1000 | Loss: 0.00003675
Iteration 468/1000 | Loss: 0.00003675
Iteration 469/1000 | Loss: 0.00003675
Iteration 470/1000 | Loss: 0.00003675
Iteration 471/1000 | Loss: 0.00003675
Iteration 472/1000 | Loss: 0.00003675
Iteration 473/1000 | Loss: 0.00003675
Iteration 474/1000 | Loss: 0.00003675
Iteration 475/1000 | Loss: 0.00003675
Iteration 476/1000 | Loss: 0.00003675
Iteration 477/1000 | Loss: 0.00003675
Iteration 478/1000 | Loss: 0.00003675
Iteration 479/1000 | Loss: 0.00003675
Iteration 480/1000 | Loss: 0.00003675
Iteration 481/1000 | Loss: 0.00003675
Iteration 482/1000 | Loss: 0.00003675
Iteration 483/1000 | Loss: 0.00003675
Iteration 484/1000 | Loss: 0.00003675
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 484. Stopping optimization.
Last 5 losses: [3.675223342725076e-05, 3.675223342725076e-05, 3.675223342725076e-05, 3.675223342725076e-05, 3.675223342725076e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.675223342725076e-05

Optimization complete. Final v2v error: 4.904924392700195 mm

Highest mean error: 6.683193206787109 mm for frame 192

Lowest mean error: 3.596644878387451 mm for frame 6

Saving results

Total time: 480.0664608478546
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00468858
Iteration 2/25 | Loss: 0.00113373
Iteration 3/25 | Loss: 0.00106566
Iteration 4/25 | Loss: 0.00105604
Iteration 5/25 | Loss: 0.00105407
Iteration 6/25 | Loss: 0.00105407
Iteration 7/25 | Loss: 0.00105407
Iteration 8/25 | Loss: 0.00105407
Iteration 9/25 | Loss: 0.00105407
Iteration 10/25 | Loss: 0.00105407
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0010540671646595001, 0.0010540671646595001, 0.0010540671646595001, 0.0010540671646595001, 0.0010540671646595001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010540671646595001

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.81704926
Iteration 2/25 | Loss: 0.00074626
Iteration 3/25 | Loss: 0.00074625
Iteration 4/25 | Loss: 0.00074625
Iteration 5/25 | Loss: 0.00074625
Iteration 6/25 | Loss: 0.00074625
Iteration 7/25 | Loss: 0.00074625
Iteration 8/25 | Loss: 0.00074625
Iteration 9/25 | Loss: 0.00074625
Iteration 10/25 | Loss: 0.00074625
Iteration 11/25 | Loss: 0.00074625
Iteration 12/25 | Loss: 0.00074625
Iteration 13/25 | Loss: 0.00074625
Iteration 14/25 | Loss: 0.00074625
Iteration 15/25 | Loss: 0.00074625
Iteration 16/25 | Loss: 0.00074625
Iteration 17/25 | Loss: 0.00074625
Iteration 18/25 | Loss: 0.00074625
Iteration 19/25 | Loss: 0.00074625
Iteration 20/25 | Loss: 0.00074625
Iteration 21/25 | Loss: 0.00074625
Iteration 22/25 | Loss: 0.00074625
Iteration 23/25 | Loss: 0.00074625
Iteration 24/25 | Loss: 0.00074625
Iteration 25/25 | Loss: 0.00074625

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00074625
Iteration 2/1000 | Loss: 0.00001740
Iteration 3/1000 | Loss: 0.00001336
Iteration 4/1000 | Loss: 0.00001238
Iteration 5/1000 | Loss: 0.00001171
Iteration 6/1000 | Loss: 0.00001125
Iteration 7/1000 | Loss: 0.00001102
Iteration 8/1000 | Loss: 0.00001061
Iteration 9/1000 | Loss: 0.00001043
Iteration 10/1000 | Loss: 0.00001037
Iteration 11/1000 | Loss: 0.00001036
Iteration 12/1000 | Loss: 0.00001023
Iteration 13/1000 | Loss: 0.00001020
Iteration 14/1000 | Loss: 0.00001010
Iteration 15/1000 | Loss: 0.00001008
Iteration 16/1000 | Loss: 0.00000992
Iteration 17/1000 | Loss: 0.00000990
Iteration 18/1000 | Loss: 0.00000980
Iteration 19/1000 | Loss: 0.00000975
Iteration 20/1000 | Loss: 0.00000974
Iteration 21/1000 | Loss: 0.00000973
Iteration 22/1000 | Loss: 0.00000969
Iteration 23/1000 | Loss: 0.00000965
Iteration 24/1000 | Loss: 0.00000965
Iteration 25/1000 | Loss: 0.00000963
Iteration 26/1000 | Loss: 0.00000961
Iteration 27/1000 | Loss: 0.00000960
Iteration 28/1000 | Loss: 0.00000959
Iteration 29/1000 | Loss: 0.00000959
Iteration 30/1000 | Loss: 0.00000959
Iteration 31/1000 | Loss: 0.00000958
Iteration 32/1000 | Loss: 0.00000957
Iteration 33/1000 | Loss: 0.00000957
Iteration 34/1000 | Loss: 0.00000957
Iteration 35/1000 | Loss: 0.00000956
Iteration 36/1000 | Loss: 0.00000956
Iteration 37/1000 | Loss: 0.00000955
Iteration 38/1000 | Loss: 0.00000950
Iteration 39/1000 | Loss: 0.00000949
Iteration 40/1000 | Loss: 0.00000948
Iteration 41/1000 | Loss: 0.00000948
Iteration 42/1000 | Loss: 0.00000947
Iteration 43/1000 | Loss: 0.00000945
Iteration 44/1000 | Loss: 0.00000945
Iteration 45/1000 | Loss: 0.00000942
Iteration 46/1000 | Loss: 0.00000939
Iteration 47/1000 | Loss: 0.00000938
Iteration 48/1000 | Loss: 0.00000937
Iteration 49/1000 | Loss: 0.00000935
Iteration 50/1000 | Loss: 0.00000934
Iteration 51/1000 | Loss: 0.00000934
Iteration 52/1000 | Loss: 0.00000933
Iteration 53/1000 | Loss: 0.00000932
Iteration 54/1000 | Loss: 0.00000932
Iteration 55/1000 | Loss: 0.00000931
Iteration 56/1000 | Loss: 0.00000931
Iteration 57/1000 | Loss: 0.00000929
Iteration 58/1000 | Loss: 0.00000929
Iteration 59/1000 | Loss: 0.00000929
Iteration 60/1000 | Loss: 0.00000928
Iteration 61/1000 | Loss: 0.00000928
Iteration 62/1000 | Loss: 0.00000928
Iteration 63/1000 | Loss: 0.00000927
Iteration 64/1000 | Loss: 0.00000927
Iteration 65/1000 | Loss: 0.00000927
Iteration 66/1000 | Loss: 0.00000926
Iteration 67/1000 | Loss: 0.00000926
Iteration 68/1000 | Loss: 0.00000925
Iteration 69/1000 | Loss: 0.00000925
Iteration 70/1000 | Loss: 0.00000925
Iteration 71/1000 | Loss: 0.00000925
Iteration 72/1000 | Loss: 0.00000925
Iteration 73/1000 | Loss: 0.00000925
Iteration 74/1000 | Loss: 0.00000925
Iteration 75/1000 | Loss: 0.00000925
Iteration 76/1000 | Loss: 0.00000925
Iteration 77/1000 | Loss: 0.00000924
Iteration 78/1000 | Loss: 0.00000924
Iteration 79/1000 | Loss: 0.00000924
Iteration 80/1000 | Loss: 0.00000923
Iteration 81/1000 | Loss: 0.00000923
Iteration 82/1000 | Loss: 0.00000922
Iteration 83/1000 | Loss: 0.00000922
Iteration 84/1000 | Loss: 0.00000922
Iteration 85/1000 | Loss: 0.00000921
Iteration 86/1000 | Loss: 0.00000921
Iteration 87/1000 | Loss: 0.00000921
Iteration 88/1000 | Loss: 0.00000921
Iteration 89/1000 | Loss: 0.00000921
Iteration 90/1000 | Loss: 0.00000921
Iteration 91/1000 | Loss: 0.00000921
Iteration 92/1000 | Loss: 0.00000921
Iteration 93/1000 | Loss: 0.00000921
Iteration 94/1000 | Loss: 0.00000921
Iteration 95/1000 | Loss: 0.00000920
Iteration 96/1000 | Loss: 0.00000920
Iteration 97/1000 | Loss: 0.00000920
Iteration 98/1000 | Loss: 0.00000920
Iteration 99/1000 | Loss: 0.00000920
Iteration 100/1000 | Loss: 0.00000920
Iteration 101/1000 | Loss: 0.00000920
Iteration 102/1000 | Loss: 0.00000920
Iteration 103/1000 | Loss: 0.00000920
Iteration 104/1000 | Loss: 0.00000920
Iteration 105/1000 | Loss: 0.00000920
Iteration 106/1000 | Loss: 0.00000919
Iteration 107/1000 | Loss: 0.00000919
Iteration 108/1000 | Loss: 0.00000919
Iteration 109/1000 | Loss: 0.00000919
Iteration 110/1000 | Loss: 0.00000919
Iteration 111/1000 | Loss: 0.00000919
Iteration 112/1000 | Loss: 0.00000919
Iteration 113/1000 | Loss: 0.00000919
Iteration 114/1000 | Loss: 0.00000919
Iteration 115/1000 | Loss: 0.00000919
Iteration 116/1000 | Loss: 0.00000919
Iteration 117/1000 | Loss: 0.00000919
Iteration 118/1000 | Loss: 0.00000919
Iteration 119/1000 | Loss: 0.00000919
Iteration 120/1000 | Loss: 0.00000919
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 120. Stopping optimization.
Last 5 losses: [9.190846867568325e-06, 9.190846867568325e-06, 9.190846867568325e-06, 9.190846867568325e-06, 9.190846867568325e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.190846867568325e-06

Optimization complete. Final v2v error: 2.6436023712158203 mm

Highest mean error: 2.8901443481445312 mm for frame 111

Lowest mean error: 2.471074104309082 mm for frame 4

Saving results

Total time: 42.140918254852295
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00389100
Iteration 2/25 | Loss: 0.00120077
Iteration 3/25 | Loss: 0.00107878
Iteration 4/25 | Loss: 0.00106358
Iteration 5/25 | Loss: 0.00106085
Iteration 6/25 | Loss: 0.00106078
Iteration 7/25 | Loss: 0.00106078
Iteration 8/25 | Loss: 0.00106078
Iteration 9/25 | Loss: 0.00106078
Iteration 10/25 | Loss: 0.00106078
Iteration 11/25 | Loss: 0.00106078
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010607815347611904, 0.0010607815347611904, 0.0010607815347611904, 0.0010607815347611904, 0.0010607815347611904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010607815347611904

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34179652
Iteration 2/25 | Loss: 0.00066739
Iteration 3/25 | Loss: 0.00066738
Iteration 4/25 | Loss: 0.00066738
Iteration 5/25 | Loss: 0.00066738
Iteration 6/25 | Loss: 0.00066738
Iteration 7/25 | Loss: 0.00066738
Iteration 8/25 | Loss: 0.00066738
Iteration 9/25 | Loss: 0.00066738
Iteration 10/25 | Loss: 0.00066738
Iteration 11/25 | Loss: 0.00066738
Iteration 12/25 | Loss: 0.00066738
Iteration 13/25 | Loss: 0.00066738
Iteration 14/25 | Loss: 0.00066738
Iteration 15/25 | Loss: 0.00066738
Iteration 16/25 | Loss: 0.00066738
Iteration 17/25 | Loss: 0.00066738
Iteration 18/25 | Loss: 0.00066738
Iteration 19/25 | Loss: 0.00066738
Iteration 20/25 | Loss: 0.00066738
Iteration 21/25 | Loss: 0.00066738
Iteration 22/25 | Loss: 0.00066738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0006673795869573951, 0.0006673795869573951, 0.0006673795869573951, 0.0006673795869573951, 0.0006673795869573951]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006673795869573951

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066738
Iteration 2/1000 | Loss: 0.00002876
Iteration 3/1000 | Loss: 0.00002128
Iteration 4/1000 | Loss: 0.00001803
Iteration 5/1000 | Loss: 0.00001668
Iteration 6/1000 | Loss: 0.00001570
Iteration 7/1000 | Loss: 0.00001512
Iteration 8/1000 | Loss: 0.00001463
Iteration 9/1000 | Loss: 0.00001429
Iteration 10/1000 | Loss: 0.00001393
Iteration 11/1000 | Loss: 0.00001365
Iteration 12/1000 | Loss: 0.00001351
Iteration 13/1000 | Loss: 0.00001349
Iteration 14/1000 | Loss: 0.00001330
Iteration 15/1000 | Loss: 0.00001323
Iteration 16/1000 | Loss: 0.00001318
Iteration 17/1000 | Loss: 0.00001315
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001311
Iteration 20/1000 | Loss: 0.00001311
Iteration 21/1000 | Loss: 0.00001309
Iteration 22/1000 | Loss: 0.00001307
Iteration 23/1000 | Loss: 0.00001303
Iteration 24/1000 | Loss: 0.00001295
Iteration 25/1000 | Loss: 0.00001293
Iteration 26/1000 | Loss: 0.00001292
Iteration 27/1000 | Loss: 0.00001288
Iteration 28/1000 | Loss: 0.00001288
Iteration 29/1000 | Loss: 0.00001284
Iteration 30/1000 | Loss: 0.00001284
Iteration 31/1000 | Loss: 0.00001284
Iteration 32/1000 | Loss: 0.00001283
Iteration 33/1000 | Loss: 0.00001283
Iteration 34/1000 | Loss: 0.00001282
Iteration 35/1000 | Loss: 0.00001279
Iteration 36/1000 | Loss: 0.00001279
Iteration 37/1000 | Loss: 0.00001279
Iteration 38/1000 | Loss: 0.00001279
Iteration 39/1000 | Loss: 0.00001279
Iteration 40/1000 | Loss: 0.00001279
Iteration 41/1000 | Loss: 0.00001279
Iteration 42/1000 | Loss: 0.00001279
Iteration 43/1000 | Loss: 0.00001279
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001279
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001277
Iteration 51/1000 | Loss: 0.00001277
Iteration 52/1000 | Loss: 0.00001276
Iteration 53/1000 | Loss: 0.00001275
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001275
Iteration 57/1000 | Loss: 0.00001275
Iteration 58/1000 | Loss: 0.00001275
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001274
Iteration 61/1000 | Loss: 0.00001274
Iteration 62/1000 | Loss: 0.00001274
Iteration 63/1000 | Loss: 0.00001274
Iteration 64/1000 | Loss: 0.00001274
Iteration 65/1000 | Loss: 0.00001274
Iteration 66/1000 | Loss: 0.00001274
Iteration 67/1000 | Loss: 0.00001274
Iteration 68/1000 | Loss: 0.00001273
Iteration 69/1000 | Loss: 0.00001270
Iteration 70/1000 | Loss: 0.00001269
Iteration 71/1000 | Loss: 0.00001269
Iteration 72/1000 | Loss: 0.00001269
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001268
Iteration 76/1000 | Loss: 0.00001268
Iteration 77/1000 | Loss: 0.00001268
Iteration 78/1000 | Loss: 0.00001267
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001267
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001266
Iteration 85/1000 | Loss: 0.00001266
Iteration 86/1000 | Loss: 0.00001266
Iteration 87/1000 | Loss: 0.00001266
Iteration 88/1000 | Loss: 0.00001266
Iteration 89/1000 | Loss: 0.00001266
Iteration 90/1000 | Loss: 0.00001265
Iteration 91/1000 | Loss: 0.00001265
Iteration 92/1000 | Loss: 0.00001265
Iteration 93/1000 | Loss: 0.00001265
Iteration 94/1000 | Loss: 0.00001265
Iteration 95/1000 | Loss: 0.00001264
Iteration 96/1000 | Loss: 0.00001264
Iteration 97/1000 | Loss: 0.00001264
Iteration 98/1000 | Loss: 0.00001264
Iteration 99/1000 | Loss: 0.00001264
Iteration 100/1000 | Loss: 0.00001264
Iteration 101/1000 | Loss: 0.00001264
Iteration 102/1000 | Loss: 0.00001264
Iteration 103/1000 | Loss: 0.00001264
Iteration 104/1000 | Loss: 0.00001263
Iteration 105/1000 | Loss: 0.00001263
Iteration 106/1000 | Loss: 0.00001263
Iteration 107/1000 | Loss: 0.00001262
Iteration 108/1000 | Loss: 0.00001262
Iteration 109/1000 | Loss: 0.00001261
Iteration 110/1000 | Loss: 0.00001261
Iteration 111/1000 | Loss: 0.00001261
Iteration 112/1000 | Loss: 0.00001261
Iteration 113/1000 | Loss: 0.00001261
Iteration 114/1000 | Loss: 0.00001260
Iteration 115/1000 | Loss: 0.00001260
Iteration 116/1000 | Loss: 0.00001260
Iteration 117/1000 | Loss: 0.00001259
Iteration 118/1000 | Loss: 0.00001259
Iteration 119/1000 | Loss: 0.00001259
Iteration 120/1000 | Loss: 0.00001259
Iteration 121/1000 | Loss: 0.00001259
Iteration 122/1000 | Loss: 0.00001258
Iteration 123/1000 | Loss: 0.00001258
Iteration 124/1000 | Loss: 0.00001258
Iteration 125/1000 | Loss: 0.00001258
Iteration 126/1000 | Loss: 0.00001258
Iteration 127/1000 | Loss: 0.00001257
Iteration 128/1000 | Loss: 0.00001257
Iteration 129/1000 | Loss: 0.00001257
Iteration 130/1000 | Loss: 0.00001257
Iteration 131/1000 | Loss: 0.00001257
Iteration 132/1000 | Loss: 0.00001256
Iteration 133/1000 | Loss: 0.00001256
Iteration 134/1000 | Loss: 0.00001256
Iteration 135/1000 | Loss: 0.00001256
Iteration 136/1000 | Loss: 0.00001256
Iteration 137/1000 | Loss: 0.00001256
Iteration 138/1000 | Loss: 0.00001255
Iteration 139/1000 | Loss: 0.00001255
Iteration 140/1000 | Loss: 0.00001255
Iteration 141/1000 | Loss: 0.00001255
Iteration 142/1000 | Loss: 0.00001255
Iteration 143/1000 | Loss: 0.00001255
Iteration 144/1000 | Loss: 0.00001255
Iteration 145/1000 | Loss: 0.00001255
Iteration 146/1000 | Loss: 0.00001255
Iteration 147/1000 | Loss: 0.00001254
Iteration 148/1000 | Loss: 0.00001254
Iteration 149/1000 | Loss: 0.00001254
Iteration 150/1000 | Loss: 0.00001254
Iteration 151/1000 | Loss: 0.00001254
Iteration 152/1000 | Loss: 0.00001253
Iteration 153/1000 | Loss: 0.00001253
Iteration 154/1000 | Loss: 0.00001253
Iteration 155/1000 | Loss: 0.00001253
Iteration 156/1000 | Loss: 0.00001253
Iteration 157/1000 | Loss: 0.00001253
Iteration 158/1000 | Loss: 0.00001253
Iteration 159/1000 | Loss: 0.00001253
Iteration 160/1000 | Loss: 0.00001253
Iteration 161/1000 | Loss: 0.00001253
Iteration 162/1000 | Loss: 0.00001252
Iteration 163/1000 | Loss: 0.00001252
Iteration 164/1000 | Loss: 0.00001252
Iteration 165/1000 | Loss: 0.00001252
Iteration 166/1000 | Loss: 0.00001252
Iteration 167/1000 | Loss: 0.00001252
Iteration 168/1000 | Loss: 0.00001252
Iteration 169/1000 | Loss: 0.00001252
Iteration 170/1000 | Loss: 0.00001251
Iteration 171/1000 | Loss: 0.00001251
Iteration 172/1000 | Loss: 0.00001251
Iteration 173/1000 | Loss: 0.00001251
Iteration 174/1000 | Loss: 0.00001251
Iteration 175/1000 | Loss: 0.00001251
Iteration 176/1000 | Loss: 0.00001251
Iteration 177/1000 | Loss: 0.00001250
Iteration 178/1000 | Loss: 0.00001250
Iteration 179/1000 | Loss: 0.00001250
Iteration 180/1000 | Loss: 0.00001249
Iteration 181/1000 | Loss: 0.00001249
Iteration 182/1000 | Loss: 0.00001249
Iteration 183/1000 | Loss: 0.00001249
Iteration 184/1000 | Loss: 0.00001249
Iteration 185/1000 | Loss: 0.00001249
Iteration 186/1000 | Loss: 0.00001249
Iteration 187/1000 | Loss: 0.00001249
Iteration 188/1000 | Loss: 0.00001249
Iteration 189/1000 | Loss: 0.00001248
Iteration 190/1000 | Loss: 0.00001248
Iteration 191/1000 | Loss: 0.00001248
Iteration 192/1000 | Loss: 0.00001248
Iteration 193/1000 | Loss: 0.00001248
Iteration 194/1000 | Loss: 0.00001248
Iteration 195/1000 | Loss: 0.00001248
Iteration 196/1000 | Loss: 0.00001248
Iteration 197/1000 | Loss: 0.00001248
Iteration 198/1000 | Loss: 0.00001248
Iteration 199/1000 | Loss: 0.00001248
Iteration 200/1000 | Loss: 0.00001248
Iteration 201/1000 | Loss: 0.00001248
Iteration 202/1000 | Loss: 0.00001248
Iteration 203/1000 | Loss: 0.00001248
Iteration 204/1000 | Loss: 0.00001248
Iteration 205/1000 | Loss: 0.00001248
Iteration 206/1000 | Loss: 0.00001248
Iteration 207/1000 | Loss: 0.00001248
Iteration 208/1000 | Loss: 0.00001248
Iteration 209/1000 | Loss: 0.00001248
Iteration 210/1000 | Loss: 0.00001248
Iteration 211/1000 | Loss: 0.00001248
Iteration 212/1000 | Loss: 0.00001248
Iteration 213/1000 | Loss: 0.00001248
Iteration 214/1000 | Loss: 0.00001248
Iteration 215/1000 | Loss: 0.00001248
Iteration 216/1000 | Loss: 0.00001248
Iteration 217/1000 | Loss: 0.00001248
Iteration 218/1000 | Loss: 0.00001248
Iteration 219/1000 | Loss: 0.00001248
Iteration 220/1000 | Loss: 0.00001248
Iteration 221/1000 | Loss: 0.00001248
Iteration 222/1000 | Loss: 0.00001248
Iteration 223/1000 | Loss: 0.00001248
Iteration 224/1000 | Loss: 0.00001248
Iteration 225/1000 | Loss: 0.00001248
Iteration 226/1000 | Loss: 0.00001248
Iteration 227/1000 | Loss: 0.00001248
Iteration 228/1000 | Loss: 0.00001248
Iteration 229/1000 | Loss: 0.00001248
Iteration 230/1000 | Loss: 0.00001248
Iteration 231/1000 | Loss: 0.00001248
Iteration 232/1000 | Loss: 0.00001248
Iteration 233/1000 | Loss: 0.00001248
Iteration 234/1000 | Loss: 0.00001248
Iteration 235/1000 | Loss: 0.00001248
Iteration 236/1000 | Loss: 0.00001248
Iteration 237/1000 | Loss: 0.00001248
Iteration 238/1000 | Loss: 0.00001248
Iteration 239/1000 | Loss: 0.00001248
Iteration 240/1000 | Loss: 0.00001248
Iteration 241/1000 | Loss: 0.00001248
Iteration 242/1000 | Loss: 0.00001248
Iteration 243/1000 | Loss: 0.00001248
Iteration 244/1000 | Loss: 0.00001248
Iteration 245/1000 | Loss: 0.00001248
Iteration 246/1000 | Loss: 0.00001248
Iteration 247/1000 | Loss: 0.00001248
Iteration 248/1000 | Loss: 0.00001248
Iteration 249/1000 | Loss: 0.00001248
Iteration 250/1000 | Loss: 0.00001248
Iteration 251/1000 | Loss: 0.00001248
Iteration 252/1000 | Loss: 0.00001248
Iteration 253/1000 | Loss: 0.00001248
Iteration 254/1000 | Loss: 0.00001248
Iteration 255/1000 | Loss: 0.00001248
Iteration 256/1000 | Loss: 0.00001248
Iteration 257/1000 | Loss: 0.00001248
Iteration 258/1000 | Loss: 0.00001248
Iteration 259/1000 | Loss: 0.00001248
Iteration 260/1000 | Loss: 0.00001248
Iteration 261/1000 | Loss: 0.00001248
Iteration 262/1000 | Loss: 0.00001248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 262. Stopping optimization.
Last 5 losses: [1.2482093552534934e-05, 1.2482093552534934e-05, 1.2482093552534934e-05, 1.2482093552534934e-05, 1.2482093552534934e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2482093552534934e-05

Optimization complete. Final v2v error: 2.938961982727051 mm

Highest mean error: 3.4479455947875977 mm for frame 100

Lowest mean error: 2.512110471725464 mm for frame 27

Saving results

Total time: 46.10572004318237
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00895996
Iteration 2/25 | Loss: 0.00290054
Iteration 3/25 | Loss: 0.00201259
Iteration 4/25 | Loss: 0.00169934
Iteration 5/25 | Loss: 0.00156270
Iteration 6/25 | Loss: 0.00144251
Iteration 7/25 | Loss: 0.00141466
Iteration 8/25 | Loss: 0.00142580
Iteration 9/25 | Loss: 0.00137516
Iteration 10/25 | Loss: 0.00136040
Iteration 11/25 | Loss: 0.00133626
Iteration 12/25 | Loss: 0.00132036
Iteration 13/25 | Loss: 0.00131083
Iteration 14/25 | Loss: 0.00130974
Iteration 15/25 | Loss: 0.00130445
Iteration 16/25 | Loss: 0.00130289
Iteration 17/25 | Loss: 0.00130291
Iteration 18/25 | Loss: 0.00129998
Iteration 19/25 | Loss: 0.00129783
Iteration 20/25 | Loss: 0.00129806
Iteration 21/25 | Loss: 0.00129624
Iteration 22/25 | Loss: 0.00129390
Iteration 23/25 | Loss: 0.00129299
Iteration 24/25 | Loss: 0.00129221
Iteration 25/25 | Loss: 0.00129208

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.53538561
Iteration 2/25 | Loss: 0.00500265
Iteration 3/25 | Loss: 0.00258756
Iteration 4/25 | Loss: 0.00258756
Iteration 5/25 | Loss: 0.00258756
Iteration 6/25 | Loss: 0.00258756
Iteration 7/25 | Loss: 0.00258756
Iteration 8/25 | Loss: 0.00258756
Iteration 9/25 | Loss: 0.00258756
Iteration 10/25 | Loss: 0.00258756
Iteration 11/25 | Loss: 0.00258756
Iteration 12/25 | Loss: 0.00258756
Iteration 13/25 | Loss: 0.00258756
Iteration 14/25 | Loss: 0.00258756
Iteration 15/25 | Loss: 0.00258756
Iteration 16/25 | Loss: 0.00258756
Iteration 17/25 | Loss: 0.00258756
Iteration 18/25 | Loss: 0.00258756
Iteration 19/25 | Loss: 0.00258756
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0025875552091747522, 0.0025875552091747522, 0.0025875552091747522, 0.0025875552091747522, 0.0025875552091747522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025875552091747522

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00258756
Iteration 2/1000 | Loss: 0.00149942
Iteration 3/1000 | Loss: 0.00204689
Iteration 4/1000 | Loss: 0.00470116
Iteration 5/1000 | Loss: 0.00023987
Iteration 6/1000 | Loss: 0.00365196
Iteration 7/1000 | Loss: 0.00194476
Iteration 8/1000 | Loss: 0.00210020
Iteration 9/1000 | Loss: 0.00052210
Iteration 10/1000 | Loss: 0.00189559
Iteration 11/1000 | Loss: 0.00203492
Iteration 12/1000 | Loss: 0.00038028
Iteration 13/1000 | Loss: 0.00099715
Iteration 14/1000 | Loss: 0.00034915
Iteration 15/1000 | Loss: 0.00072064
Iteration 16/1000 | Loss: 0.00015723
Iteration 17/1000 | Loss: 0.00090269
Iteration 18/1000 | Loss: 0.00011067
Iteration 19/1000 | Loss: 0.00009626
Iteration 20/1000 | Loss: 0.00048661
Iteration 21/1000 | Loss: 0.00046554
Iteration 22/1000 | Loss: 0.00027786
Iteration 23/1000 | Loss: 0.00116008
Iteration 24/1000 | Loss: 0.00069814
Iteration 25/1000 | Loss: 0.00018103
Iteration 26/1000 | Loss: 0.00011449
Iteration 27/1000 | Loss: 0.00028796
Iteration 28/1000 | Loss: 0.00028117
Iteration 29/1000 | Loss: 0.00029160
Iteration 30/1000 | Loss: 0.00073746
Iteration 31/1000 | Loss: 0.00195386
Iteration 32/1000 | Loss: 0.00050887
Iteration 33/1000 | Loss: 0.00064973
Iteration 34/1000 | Loss: 0.00077933
Iteration 35/1000 | Loss: 0.00034710
Iteration 36/1000 | Loss: 0.00080846
Iteration 37/1000 | Loss: 0.00114584
Iteration 38/1000 | Loss: 0.00130363
Iteration 39/1000 | Loss: 0.00162886
Iteration 40/1000 | Loss: 0.00402593
Iteration 41/1000 | Loss: 0.00174740
Iteration 42/1000 | Loss: 0.00171491
Iteration 43/1000 | Loss: 0.00095496
Iteration 44/1000 | Loss: 0.00119606
Iteration 45/1000 | Loss: 0.00278094
Iteration 46/1000 | Loss: 0.00066667
Iteration 47/1000 | Loss: 0.00080780
Iteration 48/1000 | Loss: 0.00168284
Iteration 49/1000 | Loss: 0.00069501
Iteration 50/1000 | Loss: 0.00168307
Iteration 51/1000 | Loss: 0.00077058
Iteration 52/1000 | Loss: 0.00056093
Iteration 53/1000 | Loss: 0.00087935
Iteration 54/1000 | Loss: 0.00008679
Iteration 55/1000 | Loss: 0.00017681
Iteration 56/1000 | Loss: 0.00006466
Iteration 57/1000 | Loss: 0.00006484
Iteration 58/1000 | Loss: 0.00032949
Iteration 59/1000 | Loss: 0.00022183
Iteration 60/1000 | Loss: 0.00030400
Iteration 61/1000 | Loss: 0.00015340
Iteration 62/1000 | Loss: 0.00029467
Iteration 63/1000 | Loss: 0.00137040
Iteration 64/1000 | Loss: 0.00003924
Iteration 65/1000 | Loss: 0.00023321
Iteration 66/1000 | Loss: 0.00003492
Iteration 67/1000 | Loss: 0.00007042
Iteration 68/1000 | Loss: 0.00003096
Iteration 69/1000 | Loss: 0.00016691
Iteration 70/1000 | Loss: 0.00012932
Iteration 71/1000 | Loss: 0.00012955
Iteration 72/1000 | Loss: 0.00205960
Iteration 73/1000 | Loss: 0.00022539
Iteration 74/1000 | Loss: 0.00105058
Iteration 75/1000 | Loss: 0.00023013
Iteration 76/1000 | Loss: 0.00003413
Iteration 77/1000 | Loss: 0.00018365
Iteration 78/1000 | Loss: 0.00018179
Iteration 79/1000 | Loss: 0.00002478
Iteration 80/1000 | Loss: 0.00002309
Iteration 81/1000 | Loss: 0.00002200
Iteration 82/1000 | Loss: 0.00002110
Iteration 83/1000 | Loss: 0.00002030
Iteration 84/1000 | Loss: 0.00013473
Iteration 85/1000 | Loss: 0.00007190
Iteration 86/1000 | Loss: 0.00008144
Iteration 87/1000 | Loss: 0.00002264
Iteration 88/1000 | Loss: 0.00001922
Iteration 89/1000 | Loss: 0.00001829
Iteration 90/1000 | Loss: 0.00001798
Iteration 91/1000 | Loss: 0.00001779
Iteration 92/1000 | Loss: 0.00001776
Iteration 93/1000 | Loss: 0.00004699
Iteration 94/1000 | Loss: 0.00001954
Iteration 95/1000 | Loss: 0.00001729
Iteration 96/1000 | Loss: 0.00001727
Iteration 97/1000 | Loss: 0.00003396
Iteration 98/1000 | Loss: 0.00003066
Iteration 99/1000 | Loss: 0.00001745
Iteration 100/1000 | Loss: 0.00001707
Iteration 101/1000 | Loss: 0.00001697
Iteration 102/1000 | Loss: 0.00001695
Iteration 103/1000 | Loss: 0.00001694
Iteration 104/1000 | Loss: 0.00001693
Iteration 105/1000 | Loss: 0.00002244
Iteration 106/1000 | Loss: 0.00001692
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001690
Iteration 109/1000 | Loss: 0.00001689
Iteration 110/1000 | Loss: 0.00001683
Iteration 111/1000 | Loss: 0.00001683
Iteration 112/1000 | Loss: 0.00001676
Iteration 113/1000 | Loss: 0.00001674
Iteration 114/1000 | Loss: 0.00001674
Iteration 115/1000 | Loss: 0.00001674
Iteration 116/1000 | Loss: 0.00001674
Iteration 117/1000 | Loss: 0.00001673
Iteration 118/1000 | Loss: 0.00001673
Iteration 119/1000 | Loss: 0.00001673
Iteration 120/1000 | Loss: 0.00001673
Iteration 121/1000 | Loss: 0.00001672
Iteration 122/1000 | Loss: 0.00001672
Iteration 123/1000 | Loss: 0.00001671
Iteration 124/1000 | Loss: 0.00001671
Iteration 125/1000 | Loss: 0.00001670
Iteration 126/1000 | Loss: 0.00001670
Iteration 127/1000 | Loss: 0.00001669
Iteration 128/1000 | Loss: 0.00001669
Iteration 129/1000 | Loss: 0.00001669
Iteration 130/1000 | Loss: 0.00001669
Iteration 131/1000 | Loss: 0.00001669
Iteration 132/1000 | Loss: 0.00001669
Iteration 133/1000 | Loss: 0.00001669
Iteration 134/1000 | Loss: 0.00001669
Iteration 135/1000 | Loss: 0.00001668
Iteration 136/1000 | Loss: 0.00001668
Iteration 137/1000 | Loss: 0.00001668
Iteration 138/1000 | Loss: 0.00001668
Iteration 139/1000 | Loss: 0.00001667
Iteration 140/1000 | Loss: 0.00001667
Iteration 141/1000 | Loss: 0.00001667
Iteration 142/1000 | Loss: 0.00001667
Iteration 143/1000 | Loss: 0.00001667
Iteration 144/1000 | Loss: 0.00001666
Iteration 145/1000 | Loss: 0.00001666
Iteration 146/1000 | Loss: 0.00001666
Iteration 147/1000 | Loss: 0.00001666
Iteration 148/1000 | Loss: 0.00001666
Iteration 149/1000 | Loss: 0.00001666
Iteration 150/1000 | Loss: 0.00001665
Iteration 151/1000 | Loss: 0.00001665
Iteration 152/1000 | Loss: 0.00001665
Iteration 153/1000 | Loss: 0.00001665
Iteration 154/1000 | Loss: 0.00001665
Iteration 155/1000 | Loss: 0.00001665
Iteration 156/1000 | Loss: 0.00001665
Iteration 157/1000 | Loss: 0.00001665
Iteration 158/1000 | Loss: 0.00001665
Iteration 159/1000 | Loss: 0.00001665
Iteration 160/1000 | Loss: 0.00001665
Iteration 161/1000 | Loss: 0.00001665
Iteration 162/1000 | Loss: 0.00001665
Iteration 163/1000 | Loss: 0.00001664
Iteration 164/1000 | Loss: 0.00001664
Iteration 165/1000 | Loss: 0.00001664
Iteration 166/1000 | Loss: 0.00001664
Iteration 167/1000 | Loss: 0.00001664
Iteration 168/1000 | Loss: 0.00001664
Iteration 169/1000 | Loss: 0.00001664
Iteration 170/1000 | Loss: 0.00001664
Iteration 171/1000 | Loss: 0.00001664
Iteration 172/1000 | Loss: 0.00001664
Iteration 173/1000 | Loss: 0.00001664
Iteration 174/1000 | Loss: 0.00001664
Iteration 175/1000 | Loss: 0.00001664
Iteration 176/1000 | Loss: 0.00001664
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.6635758584016003e-05, 1.6635758584016003e-05, 1.6635758584016003e-05, 1.6635758584016003e-05, 1.6635758584016003e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6635758584016003e-05

Optimization complete. Final v2v error: 3.1663970947265625 mm

Highest mean error: 13.914252281188965 mm for frame 78

Lowest mean error: 2.6425840854644775 mm for frame 135

Saving results

Total time: 204.33197498321533
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00531356
Iteration 2/25 | Loss: 0.00122111
Iteration 3/25 | Loss: 0.00112784
Iteration 4/25 | Loss: 0.00111884
Iteration 5/25 | Loss: 0.00111641
Iteration 6/25 | Loss: 0.00111582
Iteration 7/25 | Loss: 0.00111582
Iteration 8/25 | Loss: 0.00111582
Iteration 9/25 | Loss: 0.00111582
Iteration 10/25 | Loss: 0.00111582
Iteration 11/25 | Loss: 0.00111582
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011158200213685632, 0.0011158200213685632, 0.0011158200213685632, 0.0011158200213685632, 0.0011158200213685632]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011158200213685632

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.08055496
Iteration 2/25 | Loss: 0.00094687
Iteration 3/25 | Loss: 0.00094687
Iteration 4/25 | Loss: 0.00094687
Iteration 5/25 | Loss: 0.00094687
Iteration 6/25 | Loss: 0.00094687
Iteration 7/25 | Loss: 0.00094687
Iteration 8/25 | Loss: 0.00094687
Iteration 9/25 | Loss: 0.00094687
Iteration 10/25 | Loss: 0.00094687
Iteration 11/25 | Loss: 0.00094687
Iteration 12/25 | Loss: 0.00094687
Iteration 13/25 | Loss: 0.00094687
Iteration 14/25 | Loss: 0.00094687
Iteration 15/25 | Loss: 0.00094687
Iteration 16/25 | Loss: 0.00094687
Iteration 17/25 | Loss: 0.00094687
Iteration 18/25 | Loss: 0.00094687
Iteration 19/25 | Loss: 0.00094687
Iteration 20/25 | Loss: 0.00094687
Iteration 21/25 | Loss: 0.00094687
Iteration 22/25 | Loss: 0.00094687
Iteration 23/25 | Loss: 0.00094687
Iteration 24/25 | Loss: 0.00094687
Iteration 25/25 | Loss: 0.00094687
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009468650678172708, 0.0009468650678172708, 0.0009468650678172708, 0.0009468650678172708, 0.0009468650678172708]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009468650678172708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094687
Iteration 2/1000 | Loss: 0.00002951
Iteration 3/1000 | Loss: 0.00001834
Iteration 4/1000 | Loss: 0.00001447
Iteration 5/1000 | Loss: 0.00001339
Iteration 6/1000 | Loss: 0.00001286
Iteration 7/1000 | Loss: 0.00001244
Iteration 8/1000 | Loss: 0.00001218
Iteration 9/1000 | Loss: 0.00001197
Iteration 10/1000 | Loss: 0.00001197
Iteration 11/1000 | Loss: 0.00001194
Iteration 12/1000 | Loss: 0.00001194
Iteration 13/1000 | Loss: 0.00001192
Iteration 14/1000 | Loss: 0.00001185
Iteration 15/1000 | Loss: 0.00001170
Iteration 16/1000 | Loss: 0.00001166
Iteration 17/1000 | Loss: 0.00001165
Iteration 18/1000 | Loss: 0.00001165
Iteration 19/1000 | Loss: 0.00001164
Iteration 20/1000 | Loss: 0.00001163
Iteration 21/1000 | Loss: 0.00001157
Iteration 22/1000 | Loss: 0.00001154
Iteration 23/1000 | Loss: 0.00001154
Iteration 24/1000 | Loss: 0.00001152
Iteration 25/1000 | Loss: 0.00001152
Iteration 26/1000 | Loss: 0.00001151
Iteration 27/1000 | Loss: 0.00001151
Iteration 28/1000 | Loss: 0.00001150
Iteration 29/1000 | Loss: 0.00001149
Iteration 30/1000 | Loss: 0.00001149
Iteration 31/1000 | Loss: 0.00001148
Iteration 32/1000 | Loss: 0.00001148
Iteration 33/1000 | Loss: 0.00001148
Iteration 34/1000 | Loss: 0.00001148
Iteration 35/1000 | Loss: 0.00001147
Iteration 36/1000 | Loss: 0.00001147
Iteration 37/1000 | Loss: 0.00001146
Iteration 38/1000 | Loss: 0.00001146
Iteration 39/1000 | Loss: 0.00001145
Iteration 40/1000 | Loss: 0.00001145
Iteration 41/1000 | Loss: 0.00001144
Iteration 42/1000 | Loss: 0.00001143
Iteration 43/1000 | Loss: 0.00001143
Iteration 44/1000 | Loss: 0.00001142
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001141
Iteration 47/1000 | Loss: 0.00001140
Iteration 48/1000 | Loss: 0.00001139
Iteration 49/1000 | Loss: 0.00001139
Iteration 50/1000 | Loss: 0.00001139
Iteration 51/1000 | Loss: 0.00001139
Iteration 52/1000 | Loss: 0.00001138
Iteration 53/1000 | Loss: 0.00001138
Iteration 54/1000 | Loss: 0.00001137
Iteration 55/1000 | Loss: 0.00001137
Iteration 56/1000 | Loss: 0.00001137
Iteration 57/1000 | Loss: 0.00001137
Iteration 58/1000 | Loss: 0.00001137
Iteration 59/1000 | Loss: 0.00001136
Iteration 60/1000 | Loss: 0.00001136
Iteration 61/1000 | Loss: 0.00001136
Iteration 62/1000 | Loss: 0.00001136
Iteration 63/1000 | Loss: 0.00001135
Iteration 64/1000 | Loss: 0.00001135
Iteration 65/1000 | Loss: 0.00001135
Iteration 66/1000 | Loss: 0.00001134
Iteration 67/1000 | Loss: 0.00001134
Iteration 68/1000 | Loss: 0.00001134
Iteration 69/1000 | Loss: 0.00001134
Iteration 70/1000 | Loss: 0.00001134
Iteration 71/1000 | Loss: 0.00001133
Iteration 72/1000 | Loss: 0.00001133
Iteration 73/1000 | Loss: 0.00001133
Iteration 74/1000 | Loss: 0.00001132
Iteration 75/1000 | Loss: 0.00001132
Iteration 76/1000 | Loss: 0.00001132
Iteration 77/1000 | Loss: 0.00001132
Iteration 78/1000 | Loss: 0.00001132
Iteration 79/1000 | Loss: 0.00001131
Iteration 80/1000 | Loss: 0.00001131
Iteration 81/1000 | Loss: 0.00001131
Iteration 82/1000 | Loss: 0.00001131
Iteration 83/1000 | Loss: 0.00001131
Iteration 84/1000 | Loss: 0.00001131
Iteration 85/1000 | Loss: 0.00001131
Iteration 86/1000 | Loss: 0.00001131
Iteration 87/1000 | Loss: 0.00001130
Iteration 88/1000 | Loss: 0.00001130
Iteration 89/1000 | Loss: 0.00001130
Iteration 90/1000 | Loss: 0.00001130
Iteration 91/1000 | Loss: 0.00001130
Iteration 92/1000 | Loss: 0.00001130
Iteration 93/1000 | Loss: 0.00001130
Iteration 94/1000 | Loss: 0.00001130
Iteration 95/1000 | Loss: 0.00001130
Iteration 96/1000 | Loss: 0.00001130
Iteration 97/1000 | Loss: 0.00001130
Iteration 98/1000 | Loss: 0.00001130
Iteration 99/1000 | Loss: 0.00001130
Iteration 100/1000 | Loss: 0.00001130
Iteration 101/1000 | Loss: 0.00001130
Iteration 102/1000 | Loss: 0.00001130
Iteration 103/1000 | Loss: 0.00001130
Iteration 104/1000 | Loss: 0.00001130
Iteration 105/1000 | Loss: 0.00001130
Iteration 106/1000 | Loss: 0.00001129
Iteration 107/1000 | Loss: 0.00001129
Iteration 108/1000 | Loss: 0.00001129
Iteration 109/1000 | Loss: 0.00001129
Iteration 110/1000 | Loss: 0.00001129
Iteration 111/1000 | Loss: 0.00001129
Iteration 112/1000 | Loss: 0.00001129
Iteration 113/1000 | Loss: 0.00001129
Iteration 114/1000 | Loss: 0.00001129
Iteration 115/1000 | Loss: 0.00001129
Iteration 116/1000 | Loss: 0.00001129
Iteration 117/1000 | Loss: 0.00001129
Iteration 118/1000 | Loss: 0.00001129
Iteration 119/1000 | Loss: 0.00001129
Iteration 120/1000 | Loss: 0.00001129
Iteration 121/1000 | Loss: 0.00001129
Iteration 122/1000 | Loss: 0.00001128
Iteration 123/1000 | Loss: 0.00001128
Iteration 124/1000 | Loss: 0.00001128
Iteration 125/1000 | Loss: 0.00001128
Iteration 126/1000 | Loss: 0.00001128
Iteration 127/1000 | Loss: 0.00001128
Iteration 128/1000 | Loss: 0.00001128
Iteration 129/1000 | Loss: 0.00001128
Iteration 130/1000 | Loss: 0.00001128
Iteration 131/1000 | Loss: 0.00001128
Iteration 132/1000 | Loss: 0.00001128
Iteration 133/1000 | Loss: 0.00001128
Iteration 134/1000 | Loss: 0.00001128
Iteration 135/1000 | Loss: 0.00001128
Iteration 136/1000 | Loss: 0.00001127
Iteration 137/1000 | Loss: 0.00001127
Iteration 138/1000 | Loss: 0.00001127
Iteration 139/1000 | Loss: 0.00001127
Iteration 140/1000 | Loss: 0.00001127
Iteration 141/1000 | Loss: 0.00001127
Iteration 142/1000 | Loss: 0.00001127
Iteration 143/1000 | Loss: 0.00001127
Iteration 144/1000 | Loss: 0.00001127
Iteration 145/1000 | Loss: 0.00001127
Iteration 146/1000 | Loss: 0.00001127
Iteration 147/1000 | Loss: 0.00001127
Iteration 148/1000 | Loss: 0.00001127
Iteration 149/1000 | Loss: 0.00001127
Iteration 150/1000 | Loss: 0.00001126
Iteration 151/1000 | Loss: 0.00001126
Iteration 152/1000 | Loss: 0.00001126
Iteration 153/1000 | Loss: 0.00001125
Iteration 154/1000 | Loss: 0.00001125
Iteration 155/1000 | Loss: 0.00001125
Iteration 156/1000 | Loss: 0.00001125
Iteration 157/1000 | Loss: 0.00001125
Iteration 158/1000 | Loss: 0.00001125
Iteration 159/1000 | Loss: 0.00001125
Iteration 160/1000 | Loss: 0.00001125
Iteration 161/1000 | Loss: 0.00001125
Iteration 162/1000 | Loss: 0.00001125
Iteration 163/1000 | Loss: 0.00001125
Iteration 164/1000 | Loss: 0.00001125
Iteration 165/1000 | Loss: 0.00001124
Iteration 166/1000 | Loss: 0.00001124
Iteration 167/1000 | Loss: 0.00001124
Iteration 168/1000 | Loss: 0.00001124
Iteration 169/1000 | Loss: 0.00001124
Iteration 170/1000 | Loss: 0.00001124
Iteration 171/1000 | Loss: 0.00001124
Iteration 172/1000 | Loss: 0.00001124
Iteration 173/1000 | Loss: 0.00001124
Iteration 174/1000 | Loss: 0.00001124
Iteration 175/1000 | Loss: 0.00001124
Iteration 176/1000 | Loss: 0.00001124
Iteration 177/1000 | Loss: 0.00001124
Iteration 178/1000 | Loss: 0.00001123
Iteration 179/1000 | Loss: 0.00001123
Iteration 180/1000 | Loss: 0.00001123
Iteration 181/1000 | Loss: 0.00001123
Iteration 182/1000 | Loss: 0.00001123
Iteration 183/1000 | Loss: 0.00001123
Iteration 184/1000 | Loss: 0.00001123
Iteration 185/1000 | Loss: 0.00001123
Iteration 186/1000 | Loss: 0.00001123
Iteration 187/1000 | Loss: 0.00001123
Iteration 188/1000 | Loss: 0.00001123
Iteration 189/1000 | Loss: 0.00001123
Iteration 190/1000 | Loss: 0.00001123
Iteration 191/1000 | Loss: 0.00001123
Iteration 192/1000 | Loss: 0.00001123
Iteration 193/1000 | Loss: 0.00001123
Iteration 194/1000 | Loss: 0.00001123
Iteration 195/1000 | Loss: 0.00001123
Iteration 196/1000 | Loss: 0.00001122
Iteration 197/1000 | Loss: 0.00001122
Iteration 198/1000 | Loss: 0.00001122
Iteration 199/1000 | Loss: 0.00001122
Iteration 200/1000 | Loss: 0.00001122
Iteration 201/1000 | Loss: 0.00001122
Iteration 202/1000 | Loss: 0.00001122
Iteration 203/1000 | Loss: 0.00001122
Iteration 204/1000 | Loss: 0.00001122
Iteration 205/1000 | Loss: 0.00001122
Iteration 206/1000 | Loss: 0.00001122
Iteration 207/1000 | Loss: 0.00001122
Iteration 208/1000 | Loss: 0.00001122
Iteration 209/1000 | Loss: 0.00001122
Iteration 210/1000 | Loss: 0.00001121
Iteration 211/1000 | Loss: 0.00001121
Iteration 212/1000 | Loss: 0.00001121
Iteration 213/1000 | Loss: 0.00001121
Iteration 214/1000 | Loss: 0.00001121
Iteration 215/1000 | Loss: 0.00001121
Iteration 216/1000 | Loss: 0.00001121
Iteration 217/1000 | Loss: 0.00001121
Iteration 218/1000 | Loss: 0.00001121
Iteration 219/1000 | Loss: 0.00001121
Iteration 220/1000 | Loss: 0.00001121
Iteration 221/1000 | Loss: 0.00001121
Iteration 222/1000 | Loss: 0.00001121
Iteration 223/1000 | Loss: 0.00001121
Iteration 224/1000 | Loss: 0.00001121
Iteration 225/1000 | Loss: 0.00001120
Iteration 226/1000 | Loss: 0.00001120
Iteration 227/1000 | Loss: 0.00001120
Iteration 228/1000 | Loss: 0.00001120
Iteration 229/1000 | Loss: 0.00001120
Iteration 230/1000 | Loss: 0.00001120
Iteration 231/1000 | Loss: 0.00001120
Iteration 232/1000 | Loss: 0.00001120
Iteration 233/1000 | Loss: 0.00001120
Iteration 234/1000 | Loss: 0.00001120
Iteration 235/1000 | Loss: 0.00001119
Iteration 236/1000 | Loss: 0.00001119
Iteration 237/1000 | Loss: 0.00001119
Iteration 238/1000 | Loss: 0.00001119
Iteration 239/1000 | Loss: 0.00001119
Iteration 240/1000 | Loss: 0.00001119
Iteration 241/1000 | Loss: 0.00001119
Iteration 242/1000 | Loss: 0.00001119
Iteration 243/1000 | Loss: 0.00001119
Iteration 244/1000 | Loss: 0.00001119
Iteration 245/1000 | Loss: 0.00001119
Iteration 246/1000 | Loss: 0.00001119
Iteration 247/1000 | Loss: 0.00001119
Iteration 248/1000 | Loss: 0.00001119
Iteration 249/1000 | Loss: 0.00001119
Iteration 250/1000 | Loss: 0.00001119
Iteration 251/1000 | Loss: 0.00001119
Iteration 252/1000 | Loss: 0.00001119
Iteration 253/1000 | Loss: 0.00001119
Iteration 254/1000 | Loss: 0.00001119
Iteration 255/1000 | Loss: 0.00001119
Iteration 256/1000 | Loss: 0.00001119
Iteration 257/1000 | Loss: 0.00001119
Iteration 258/1000 | Loss: 0.00001119
Iteration 259/1000 | Loss: 0.00001119
Iteration 260/1000 | Loss: 0.00001119
Iteration 261/1000 | Loss: 0.00001119
Iteration 262/1000 | Loss: 0.00001119
Iteration 263/1000 | Loss: 0.00001119
Iteration 264/1000 | Loss: 0.00001119
Iteration 265/1000 | Loss: 0.00001119
Iteration 266/1000 | Loss: 0.00001119
Iteration 267/1000 | Loss: 0.00001119
Iteration 268/1000 | Loss: 0.00001119
Iteration 269/1000 | Loss: 0.00001119
Iteration 270/1000 | Loss: 0.00001119
Iteration 271/1000 | Loss: 0.00001119
Iteration 272/1000 | Loss: 0.00001119
Iteration 273/1000 | Loss: 0.00001119
Iteration 274/1000 | Loss: 0.00001119
Iteration 275/1000 | Loss: 0.00001119
Iteration 276/1000 | Loss: 0.00001119
Iteration 277/1000 | Loss: 0.00001119
Iteration 278/1000 | Loss: 0.00001119
Iteration 279/1000 | Loss: 0.00001119
Iteration 280/1000 | Loss: 0.00001119
Iteration 281/1000 | Loss: 0.00001119
Iteration 282/1000 | Loss: 0.00001119
Iteration 283/1000 | Loss: 0.00001119
Iteration 284/1000 | Loss: 0.00001119
Iteration 285/1000 | Loss: 0.00001119
Iteration 286/1000 | Loss: 0.00001119
Iteration 287/1000 | Loss: 0.00001119
Iteration 288/1000 | Loss: 0.00001119
Iteration 289/1000 | Loss: 0.00001119
Iteration 290/1000 | Loss: 0.00001119
Iteration 291/1000 | Loss: 0.00001119
Iteration 292/1000 | Loss: 0.00001119
Iteration 293/1000 | Loss: 0.00001119
Iteration 294/1000 | Loss: 0.00001119
Iteration 295/1000 | Loss: 0.00001119
Iteration 296/1000 | Loss: 0.00001119
Iteration 297/1000 | Loss: 0.00001119
Iteration 298/1000 | Loss: 0.00001119
Iteration 299/1000 | Loss: 0.00001119
Iteration 300/1000 | Loss: 0.00001119
Iteration 301/1000 | Loss: 0.00001119
Iteration 302/1000 | Loss: 0.00001119
Iteration 303/1000 | Loss: 0.00001119
Iteration 304/1000 | Loss: 0.00001119
Iteration 305/1000 | Loss: 0.00001119
Iteration 306/1000 | Loss: 0.00001119
Iteration 307/1000 | Loss: 0.00001119
Iteration 308/1000 | Loss: 0.00001119
Iteration 309/1000 | Loss: 0.00001119
Iteration 310/1000 | Loss: 0.00001119
Iteration 311/1000 | Loss: 0.00001119
Iteration 312/1000 | Loss: 0.00001119
Iteration 313/1000 | Loss: 0.00001119
Iteration 314/1000 | Loss: 0.00001119
Iteration 315/1000 | Loss: 0.00001119
Iteration 316/1000 | Loss: 0.00001119
Iteration 317/1000 | Loss: 0.00001119
Iteration 318/1000 | Loss: 0.00001119
Iteration 319/1000 | Loss: 0.00001119
Iteration 320/1000 | Loss: 0.00001119
Iteration 321/1000 | Loss: 0.00001119
Iteration 322/1000 | Loss: 0.00001119
Iteration 323/1000 | Loss: 0.00001119
Iteration 324/1000 | Loss: 0.00001119
Iteration 325/1000 | Loss: 0.00001119
Iteration 326/1000 | Loss: 0.00001119
Iteration 327/1000 | Loss: 0.00001119
Iteration 328/1000 | Loss: 0.00001119
Iteration 329/1000 | Loss: 0.00001119
Iteration 330/1000 | Loss: 0.00001119
Iteration 331/1000 | Loss: 0.00001119
Iteration 332/1000 | Loss: 0.00001119
Iteration 333/1000 | Loss: 0.00001119
Iteration 334/1000 | Loss: 0.00001119
Iteration 335/1000 | Loss: 0.00001119
Iteration 336/1000 | Loss: 0.00001119
Iteration 337/1000 | Loss: 0.00001119
Iteration 338/1000 | Loss: 0.00001119
Iteration 339/1000 | Loss: 0.00001119
Iteration 340/1000 | Loss: 0.00001119
Iteration 341/1000 | Loss: 0.00001119
Iteration 342/1000 | Loss: 0.00001119
Iteration 343/1000 | Loss: 0.00001119
Iteration 344/1000 | Loss: 0.00001119
Iteration 345/1000 | Loss: 0.00001119
Iteration 346/1000 | Loss: 0.00001119
Iteration 347/1000 | Loss: 0.00001119
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 347. Stopping optimization.
Last 5 losses: [1.1192271813342813e-05, 1.1192271813342813e-05, 1.1192271813342813e-05, 1.1192271813342813e-05, 1.1192271813342813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1192271813342813e-05

Optimization complete. Final v2v error: 2.8270821571350098 mm

Highest mean error: 3.4900481700897217 mm for frame 63

Lowest mean error: 2.5446114540100098 mm for frame 1

Saving results

Total time: 42.353208780288696
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01048425
Iteration 2/25 | Loss: 0.00187587
Iteration 3/25 | Loss: 0.00147037
Iteration 4/25 | Loss: 0.00141964
Iteration 5/25 | Loss: 0.00136238
Iteration 6/25 | Loss: 0.00134328
Iteration 7/25 | Loss: 0.00131373
Iteration 8/25 | Loss: 0.00132828
Iteration 9/25 | Loss: 0.00126311
Iteration 10/25 | Loss: 0.00125072
Iteration 11/25 | Loss: 0.00124778
Iteration 12/25 | Loss: 0.00125189
Iteration 13/25 | Loss: 0.00126710
Iteration 14/25 | Loss: 0.00127035
Iteration 15/25 | Loss: 0.00127070
Iteration 16/25 | Loss: 0.00125639
Iteration 17/25 | Loss: 0.00124595
Iteration 18/25 | Loss: 0.00124250
Iteration 19/25 | Loss: 0.00124025
Iteration 20/25 | Loss: 0.00123960
Iteration 21/25 | Loss: 0.00123916
Iteration 22/25 | Loss: 0.00123892
Iteration 23/25 | Loss: 0.00123889
Iteration 24/25 | Loss: 0.00123889
Iteration 25/25 | Loss: 0.00123889

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22440767
Iteration 2/25 | Loss: 0.00066501
Iteration 3/25 | Loss: 0.00066497
Iteration 4/25 | Loss: 0.00066497
Iteration 5/25 | Loss: 0.00066497
Iteration 6/25 | Loss: 0.00066497
Iteration 7/25 | Loss: 0.00066497
Iteration 8/25 | Loss: 0.00066497
Iteration 9/25 | Loss: 0.00066497
Iteration 10/25 | Loss: 0.00066497
Iteration 11/25 | Loss: 0.00066497
Iteration 12/25 | Loss: 0.00066497
Iteration 13/25 | Loss: 0.00066497
Iteration 14/25 | Loss: 0.00066497
Iteration 15/25 | Loss: 0.00066497
Iteration 16/25 | Loss: 0.00066497
Iteration 17/25 | Loss: 0.00066497
Iteration 18/25 | Loss: 0.00066497
Iteration 19/25 | Loss: 0.00066497
Iteration 20/25 | Loss: 0.00066497
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0006649659480899572, 0.0006649659480899572, 0.0006649659480899572, 0.0006649659480899572, 0.0006649659480899572]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006649659480899572

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00066497
Iteration 2/1000 | Loss: 0.00005064
Iteration 3/1000 | Loss: 0.00003371
Iteration 4/1000 | Loss: 0.00002904
Iteration 5/1000 | Loss: 0.00002765
Iteration 6/1000 | Loss: 0.00002637
Iteration 7/1000 | Loss: 0.00002580
Iteration 8/1000 | Loss: 0.00002520
Iteration 9/1000 | Loss: 0.00002473
Iteration 10/1000 | Loss: 0.00002440
Iteration 11/1000 | Loss: 0.00002419
Iteration 12/1000 | Loss: 0.00002408
Iteration 13/1000 | Loss: 0.00002401
Iteration 14/1000 | Loss: 0.00002392
Iteration 15/1000 | Loss: 0.00002386
Iteration 16/1000 | Loss: 0.00002383
Iteration 17/1000 | Loss: 0.00002370
Iteration 18/1000 | Loss: 0.00002361
Iteration 19/1000 | Loss: 0.00002357
Iteration 20/1000 | Loss: 0.00002353
Iteration 21/1000 | Loss: 0.00002352
Iteration 22/1000 | Loss: 0.00002350
Iteration 23/1000 | Loss: 0.00002350
Iteration 24/1000 | Loss: 0.00002350
Iteration 25/1000 | Loss: 0.00002349
Iteration 26/1000 | Loss: 0.00002349
Iteration 27/1000 | Loss: 0.00002349
Iteration 28/1000 | Loss: 0.00002349
Iteration 29/1000 | Loss: 0.00002349
Iteration 30/1000 | Loss: 0.00002349
Iteration 31/1000 | Loss: 0.00002348
Iteration 32/1000 | Loss: 0.00002347
Iteration 33/1000 | Loss: 0.00002346
Iteration 34/1000 | Loss: 0.00002345
Iteration 35/1000 | Loss: 0.00002344
Iteration 36/1000 | Loss: 0.00002344
Iteration 37/1000 | Loss: 0.00002344
Iteration 38/1000 | Loss: 0.00002344
Iteration 39/1000 | Loss: 0.00002344
Iteration 40/1000 | Loss: 0.00002343
Iteration 41/1000 | Loss: 0.00002343
Iteration 42/1000 | Loss: 0.00002343
Iteration 43/1000 | Loss: 0.00002343
Iteration 44/1000 | Loss: 0.00002343
Iteration 45/1000 | Loss: 0.00002343
Iteration 46/1000 | Loss: 0.00002343
Iteration 47/1000 | Loss: 0.00002343
Iteration 48/1000 | Loss: 0.00002342
Iteration 49/1000 | Loss: 0.00002342
Iteration 50/1000 | Loss: 0.00002342
Iteration 51/1000 | Loss: 0.00002342
Iteration 52/1000 | Loss: 0.00002342
Iteration 53/1000 | Loss: 0.00002342
Iteration 54/1000 | Loss: 0.00002342
Iteration 55/1000 | Loss: 0.00002339
Iteration 56/1000 | Loss: 0.00002339
Iteration 57/1000 | Loss: 0.00002339
Iteration 58/1000 | Loss: 0.00002339
Iteration 59/1000 | Loss: 0.00002339
Iteration 60/1000 | Loss: 0.00002339
Iteration 61/1000 | Loss: 0.00002339
Iteration 62/1000 | Loss: 0.00002339
Iteration 63/1000 | Loss: 0.00002339
Iteration 64/1000 | Loss: 0.00002338
Iteration 65/1000 | Loss: 0.00002338
Iteration 66/1000 | Loss: 0.00002338
Iteration 67/1000 | Loss: 0.00002337
Iteration 68/1000 | Loss: 0.00002335
Iteration 69/1000 | Loss: 0.00002335
Iteration 70/1000 | Loss: 0.00002335
Iteration 71/1000 | Loss: 0.00002335
Iteration 72/1000 | Loss: 0.00002335
Iteration 73/1000 | Loss: 0.00002335
Iteration 74/1000 | Loss: 0.00002335
Iteration 75/1000 | Loss: 0.00002335
Iteration 76/1000 | Loss: 0.00002335
Iteration 77/1000 | Loss: 0.00002334
Iteration 78/1000 | Loss: 0.00002334
Iteration 79/1000 | Loss: 0.00002334
Iteration 80/1000 | Loss: 0.00002334
Iteration 81/1000 | Loss: 0.00002334
Iteration 82/1000 | Loss: 0.00002334
Iteration 83/1000 | Loss: 0.00002334
Iteration 84/1000 | Loss: 0.00002333
Iteration 85/1000 | Loss: 0.00002333
Iteration 86/1000 | Loss: 0.00002333
Iteration 87/1000 | Loss: 0.00002332
Iteration 88/1000 | Loss: 0.00002331
Iteration 89/1000 | Loss: 0.00002331
Iteration 90/1000 | Loss: 0.00002331
Iteration 91/1000 | Loss: 0.00002330
Iteration 92/1000 | Loss: 0.00002330
Iteration 93/1000 | Loss: 0.00002329
Iteration 94/1000 | Loss: 0.00002329
Iteration 95/1000 | Loss: 0.00002329
Iteration 96/1000 | Loss: 0.00002329
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002329
Iteration 99/1000 | Loss: 0.00002328
Iteration 100/1000 | Loss: 0.00002328
Iteration 101/1000 | Loss: 0.00002328
Iteration 102/1000 | Loss: 0.00002328
Iteration 103/1000 | Loss: 0.00002328
Iteration 104/1000 | Loss: 0.00002328
Iteration 105/1000 | Loss: 0.00002328
Iteration 106/1000 | Loss: 0.00002327
Iteration 107/1000 | Loss: 0.00002327
Iteration 108/1000 | Loss: 0.00002327
Iteration 109/1000 | Loss: 0.00002326
Iteration 110/1000 | Loss: 0.00002325
Iteration 111/1000 | Loss: 0.00002325
Iteration 112/1000 | Loss: 0.00002325
Iteration 113/1000 | Loss: 0.00002325
Iteration 114/1000 | Loss: 0.00002325
Iteration 115/1000 | Loss: 0.00002325
Iteration 116/1000 | Loss: 0.00002324
Iteration 117/1000 | Loss: 0.00002324
Iteration 118/1000 | Loss: 0.00002324
Iteration 119/1000 | Loss: 0.00002323
Iteration 120/1000 | Loss: 0.00002322
Iteration 121/1000 | Loss: 0.00002322
Iteration 122/1000 | Loss: 0.00002322
Iteration 123/1000 | Loss: 0.00002322
Iteration 124/1000 | Loss: 0.00002322
Iteration 125/1000 | Loss: 0.00002322
Iteration 126/1000 | Loss: 0.00002322
Iteration 127/1000 | Loss: 0.00002322
Iteration 128/1000 | Loss: 0.00002322
Iteration 129/1000 | Loss: 0.00002322
Iteration 130/1000 | Loss: 0.00002322
Iteration 131/1000 | Loss: 0.00002322
Iteration 132/1000 | Loss: 0.00002322
Iteration 133/1000 | Loss: 0.00002322
Iteration 134/1000 | Loss: 0.00002322
Iteration 135/1000 | Loss: 0.00002322
Iteration 136/1000 | Loss: 0.00002322
Iteration 137/1000 | Loss: 0.00002322
Iteration 138/1000 | Loss: 0.00002322
Iteration 139/1000 | Loss: 0.00002322
Iteration 140/1000 | Loss: 0.00002322
Iteration 141/1000 | Loss: 0.00002322
Iteration 142/1000 | Loss: 0.00002322
Iteration 143/1000 | Loss: 0.00002322
Iteration 144/1000 | Loss: 0.00002322
Iteration 145/1000 | Loss: 0.00002322
Iteration 146/1000 | Loss: 0.00002322
Iteration 147/1000 | Loss: 0.00002322
Iteration 148/1000 | Loss: 0.00002322
Iteration 149/1000 | Loss: 0.00002322
Iteration 150/1000 | Loss: 0.00002322
Iteration 151/1000 | Loss: 0.00002322
Iteration 152/1000 | Loss: 0.00002322
Iteration 153/1000 | Loss: 0.00002322
Iteration 154/1000 | Loss: 0.00002322
Iteration 155/1000 | Loss: 0.00002322
Iteration 156/1000 | Loss: 0.00002322
Iteration 157/1000 | Loss: 0.00002322
Iteration 158/1000 | Loss: 0.00002322
Iteration 159/1000 | Loss: 0.00002322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 159. Stopping optimization.
Last 5 losses: [2.3219679860631004e-05, 2.3219679860631004e-05, 2.3219679860631004e-05, 2.3219679860631004e-05, 2.3219679860631004e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3219679860631004e-05

Optimization complete. Final v2v error: 3.8707892894744873 mm

Highest mean error: 5.2907891273498535 mm for frame 230

Lowest mean error: 3.379549741744995 mm for frame 54

Saving results

Total time: 79.82282710075378
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00812157
Iteration 2/25 | Loss: 0.00137063
Iteration 3/25 | Loss: 0.00111939
Iteration 4/25 | Loss: 0.00109197
Iteration 5/25 | Loss: 0.00108771
Iteration 6/25 | Loss: 0.00108663
Iteration 7/25 | Loss: 0.00108608
Iteration 8/25 | Loss: 0.00108567
Iteration 9/25 | Loss: 0.00108700
Iteration 10/25 | Loss: 0.00108902
Iteration 11/25 | Loss: 0.00108808
Iteration 12/25 | Loss: 0.00108359
Iteration 13/25 | Loss: 0.00108145
Iteration 14/25 | Loss: 0.00108101
Iteration 15/25 | Loss: 0.00108123
Iteration 16/25 | Loss: 0.00108104
Iteration 17/25 | Loss: 0.00108143
Iteration 18/25 | Loss: 0.00108159
Iteration 19/25 | Loss: 0.00108052
Iteration 20/25 | Loss: 0.00108115
Iteration 21/25 | Loss: 0.00108112
Iteration 22/25 | Loss: 0.00108110
Iteration 23/25 | Loss: 0.00108112
Iteration 24/25 | Loss: 0.00108138
Iteration 25/25 | Loss: 0.00108127

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35754943
Iteration 2/25 | Loss: 0.00081399
Iteration 3/25 | Loss: 0.00081399
Iteration 4/25 | Loss: 0.00081399
Iteration 5/25 | Loss: 0.00081399
Iteration 6/25 | Loss: 0.00081399
Iteration 7/25 | Loss: 0.00081399
Iteration 8/25 | Loss: 0.00081399
Iteration 9/25 | Loss: 0.00081399
Iteration 10/25 | Loss: 0.00081399
Iteration 11/25 | Loss: 0.00081399
Iteration 12/25 | Loss: 0.00081399
Iteration 13/25 | Loss: 0.00081399
Iteration 14/25 | Loss: 0.00081399
Iteration 15/25 | Loss: 0.00081399
Iteration 16/25 | Loss: 0.00081399
Iteration 17/25 | Loss: 0.00081399
Iteration 18/25 | Loss: 0.00081399
Iteration 19/25 | Loss: 0.00081399
Iteration 20/25 | Loss: 0.00081399
Iteration 21/25 | Loss: 0.00081399
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.000813988852314651, 0.000813988852314651, 0.000813988852314651, 0.000813988852314651, 0.000813988852314651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000813988852314651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081399
Iteration 2/1000 | Loss: 0.00002815
Iteration 3/1000 | Loss: 0.00001893
Iteration 4/1000 | Loss: 0.00002765
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00002662
Iteration 7/1000 | Loss: 0.00003605
Iteration 8/1000 | Loss: 0.00002197
Iteration 9/1000 | Loss: 0.00002767
Iteration 10/1000 | Loss: 0.00002025
Iteration 11/1000 | Loss: 0.00002831
Iteration 12/1000 | Loss: 0.00002675
Iteration 13/1000 | Loss: 0.00001827
Iteration 14/1000 | Loss: 0.00001277
Iteration 15/1000 | Loss: 0.00001146
Iteration 16/1000 | Loss: 0.00001092
Iteration 17/1000 | Loss: 0.00001060
Iteration 18/1000 | Loss: 0.00001042
Iteration 19/1000 | Loss: 0.00001019
Iteration 20/1000 | Loss: 0.00001007
Iteration 21/1000 | Loss: 0.00001005
Iteration 22/1000 | Loss: 0.00000994
Iteration 23/1000 | Loss: 0.00000993
Iteration 24/1000 | Loss: 0.00000975
Iteration 25/1000 | Loss: 0.00000970
Iteration 26/1000 | Loss: 0.00000969
Iteration 27/1000 | Loss: 0.00000969
Iteration 28/1000 | Loss: 0.00000969
Iteration 29/1000 | Loss: 0.00000968
Iteration 30/1000 | Loss: 0.00000968
Iteration 31/1000 | Loss: 0.00000968
Iteration 32/1000 | Loss: 0.00000968
Iteration 33/1000 | Loss: 0.00000968
Iteration 34/1000 | Loss: 0.00000968
Iteration 35/1000 | Loss: 0.00000967
Iteration 36/1000 | Loss: 0.00000966
Iteration 37/1000 | Loss: 0.00000966
Iteration 38/1000 | Loss: 0.00000965
Iteration 39/1000 | Loss: 0.00000961
Iteration 40/1000 | Loss: 0.00000960
Iteration 41/1000 | Loss: 0.00000960
Iteration 42/1000 | Loss: 0.00000957
Iteration 43/1000 | Loss: 0.00000956
Iteration 44/1000 | Loss: 0.00000956
Iteration 45/1000 | Loss: 0.00000955
Iteration 46/1000 | Loss: 0.00000954
Iteration 47/1000 | Loss: 0.00000954
Iteration 48/1000 | Loss: 0.00000954
Iteration 49/1000 | Loss: 0.00000953
Iteration 50/1000 | Loss: 0.00000953
Iteration 51/1000 | Loss: 0.00000953
Iteration 52/1000 | Loss: 0.00000953
Iteration 53/1000 | Loss: 0.00000952
Iteration 54/1000 | Loss: 0.00000952
Iteration 55/1000 | Loss: 0.00000952
Iteration 56/1000 | Loss: 0.00000952
Iteration 57/1000 | Loss: 0.00000952
Iteration 58/1000 | Loss: 0.00000952
Iteration 59/1000 | Loss: 0.00000952
Iteration 60/1000 | Loss: 0.00000952
Iteration 61/1000 | Loss: 0.00000952
Iteration 62/1000 | Loss: 0.00000952
Iteration 63/1000 | Loss: 0.00000952
Iteration 64/1000 | Loss: 0.00000952
Iteration 65/1000 | Loss: 0.00000951
Iteration 66/1000 | Loss: 0.00000951
Iteration 67/1000 | Loss: 0.00000951
Iteration 68/1000 | Loss: 0.00000951
Iteration 69/1000 | Loss: 0.00000950
Iteration 70/1000 | Loss: 0.00000950
Iteration 71/1000 | Loss: 0.00000950
Iteration 72/1000 | Loss: 0.00000950
Iteration 73/1000 | Loss: 0.00000950
Iteration 74/1000 | Loss: 0.00000949
Iteration 75/1000 | Loss: 0.00000949
Iteration 76/1000 | Loss: 0.00000949
Iteration 77/1000 | Loss: 0.00000949
Iteration 78/1000 | Loss: 0.00000949
Iteration 79/1000 | Loss: 0.00000949
Iteration 80/1000 | Loss: 0.00000949
Iteration 81/1000 | Loss: 0.00000949
Iteration 82/1000 | Loss: 0.00000949
Iteration 83/1000 | Loss: 0.00000949
Iteration 84/1000 | Loss: 0.00000948
Iteration 85/1000 | Loss: 0.00000948
Iteration 86/1000 | Loss: 0.00000948
Iteration 87/1000 | Loss: 0.00000947
Iteration 88/1000 | Loss: 0.00000947
Iteration 89/1000 | Loss: 0.00000947
Iteration 90/1000 | Loss: 0.00000947
Iteration 91/1000 | Loss: 0.00000947
Iteration 92/1000 | Loss: 0.00000947
Iteration 93/1000 | Loss: 0.00000947
Iteration 94/1000 | Loss: 0.00000947
Iteration 95/1000 | Loss: 0.00000946
Iteration 96/1000 | Loss: 0.00000946
Iteration 97/1000 | Loss: 0.00000946
Iteration 98/1000 | Loss: 0.00000946
Iteration 99/1000 | Loss: 0.00000946
Iteration 100/1000 | Loss: 0.00000946
Iteration 101/1000 | Loss: 0.00000946
Iteration 102/1000 | Loss: 0.00000946
Iteration 103/1000 | Loss: 0.00000946
Iteration 104/1000 | Loss: 0.00000945
Iteration 105/1000 | Loss: 0.00000945
Iteration 106/1000 | Loss: 0.00000945
Iteration 107/1000 | Loss: 0.00000945
Iteration 108/1000 | Loss: 0.00000945
Iteration 109/1000 | Loss: 0.00000944
Iteration 110/1000 | Loss: 0.00000944
Iteration 111/1000 | Loss: 0.00000944
Iteration 112/1000 | Loss: 0.00000944
Iteration 113/1000 | Loss: 0.00000944
Iteration 114/1000 | Loss: 0.00000944
Iteration 115/1000 | Loss: 0.00000944
Iteration 116/1000 | Loss: 0.00000944
Iteration 117/1000 | Loss: 0.00000944
Iteration 118/1000 | Loss: 0.00000944
Iteration 119/1000 | Loss: 0.00000944
Iteration 120/1000 | Loss: 0.00000944
Iteration 121/1000 | Loss: 0.00000943
Iteration 122/1000 | Loss: 0.00000943
Iteration 123/1000 | Loss: 0.00000943
Iteration 124/1000 | Loss: 0.00000943
Iteration 125/1000 | Loss: 0.00000943
Iteration 126/1000 | Loss: 0.00000943
Iteration 127/1000 | Loss: 0.00000943
Iteration 128/1000 | Loss: 0.00000942
Iteration 129/1000 | Loss: 0.00000942
Iteration 130/1000 | Loss: 0.00000942
Iteration 131/1000 | Loss: 0.00000942
Iteration 132/1000 | Loss: 0.00000942
Iteration 133/1000 | Loss: 0.00000942
Iteration 134/1000 | Loss: 0.00000942
Iteration 135/1000 | Loss: 0.00000941
Iteration 136/1000 | Loss: 0.00000941
Iteration 137/1000 | Loss: 0.00000941
Iteration 138/1000 | Loss: 0.00000941
Iteration 139/1000 | Loss: 0.00000941
Iteration 140/1000 | Loss: 0.00000941
Iteration 141/1000 | Loss: 0.00000941
Iteration 142/1000 | Loss: 0.00000940
Iteration 143/1000 | Loss: 0.00000940
Iteration 144/1000 | Loss: 0.00000939
Iteration 145/1000 | Loss: 0.00000939
Iteration 146/1000 | Loss: 0.00000938
Iteration 147/1000 | Loss: 0.00000938
Iteration 148/1000 | Loss: 0.00000937
Iteration 149/1000 | Loss: 0.00000937
Iteration 150/1000 | Loss: 0.00000937
Iteration 151/1000 | Loss: 0.00000937
Iteration 152/1000 | Loss: 0.00000937
Iteration 153/1000 | Loss: 0.00000936
Iteration 154/1000 | Loss: 0.00000936
Iteration 155/1000 | Loss: 0.00000936
Iteration 156/1000 | Loss: 0.00000935
Iteration 157/1000 | Loss: 0.00000935
Iteration 158/1000 | Loss: 0.00000935
Iteration 159/1000 | Loss: 0.00000934
Iteration 160/1000 | Loss: 0.00000934
Iteration 161/1000 | Loss: 0.00000934
Iteration 162/1000 | Loss: 0.00000934
Iteration 163/1000 | Loss: 0.00000934
Iteration 164/1000 | Loss: 0.00000933
Iteration 165/1000 | Loss: 0.00000933
Iteration 166/1000 | Loss: 0.00000933
Iteration 167/1000 | Loss: 0.00000933
Iteration 168/1000 | Loss: 0.00000932
Iteration 169/1000 | Loss: 0.00000932
Iteration 170/1000 | Loss: 0.00000932
Iteration 171/1000 | Loss: 0.00000932
Iteration 172/1000 | Loss: 0.00000931
Iteration 173/1000 | Loss: 0.00000931
Iteration 174/1000 | Loss: 0.00000931
Iteration 175/1000 | Loss: 0.00000931
Iteration 176/1000 | Loss: 0.00000931
Iteration 177/1000 | Loss: 0.00000931
Iteration 178/1000 | Loss: 0.00000931
Iteration 179/1000 | Loss: 0.00000931
Iteration 180/1000 | Loss: 0.00000931
Iteration 181/1000 | Loss: 0.00000931
Iteration 182/1000 | Loss: 0.00000930
Iteration 183/1000 | Loss: 0.00000930
Iteration 184/1000 | Loss: 0.00000930
Iteration 185/1000 | Loss: 0.00000930
Iteration 186/1000 | Loss: 0.00000930
Iteration 187/1000 | Loss: 0.00000930
Iteration 188/1000 | Loss: 0.00000930
Iteration 189/1000 | Loss: 0.00000930
Iteration 190/1000 | Loss: 0.00000930
Iteration 191/1000 | Loss: 0.00000930
Iteration 192/1000 | Loss: 0.00000929
Iteration 193/1000 | Loss: 0.00000929
Iteration 194/1000 | Loss: 0.00000929
Iteration 195/1000 | Loss: 0.00000929
Iteration 196/1000 | Loss: 0.00000929
Iteration 197/1000 | Loss: 0.00000929
Iteration 198/1000 | Loss: 0.00000929
Iteration 199/1000 | Loss: 0.00000929
Iteration 200/1000 | Loss: 0.00000929
Iteration 201/1000 | Loss: 0.00000929
Iteration 202/1000 | Loss: 0.00000928
Iteration 203/1000 | Loss: 0.00000928
Iteration 204/1000 | Loss: 0.00000928
Iteration 205/1000 | Loss: 0.00000928
Iteration 206/1000 | Loss: 0.00000928
Iteration 207/1000 | Loss: 0.00000928
Iteration 208/1000 | Loss: 0.00000928
Iteration 209/1000 | Loss: 0.00000928
Iteration 210/1000 | Loss: 0.00000928
Iteration 211/1000 | Loss: 0.00000928
Iteration 212/1000 | Loss: 0.00000928
Iteration 213/1000 | Loss: 0.00000928
Iteration 214/1000 | Loss: 0.00000928
Iteration 215/1000 | Loss: 0.00000928
Iteration 216/1000 | Loss: 0.00000928
Iteration 217/1000 | Loss: 0.00000928
Iteration 218/1000 | Loss: 0.00000928
Iteration 219/1000 | Loss: 0.00000928
Iteration 220/1000 | Loss: 0.00000928
Iteration 221/1000 | Loss: 0.00000928
Iteration 222/1000 | Loss: 0.00000928
Iteration 223/1000 | Loss: 0.00000928
Iteration 224/1000 | Loss: 0.00000928
Iteration 225/1000 | Loss: 0.00000928
Iteration 226/1000 | Loss: 0.00000928
Iteration 227/1000 | Loss: 0.00000928
Iteration 228/1000 | Loss: 0.00000928
Iteration 229/1000 | Loss: 0.00000928
Iteration 230/1000 | Loss: 0.00000928
Iteration 231/1000 | Loss: 0.00000928
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 231. Stopping optimization.
Last 5 losses: [9.27843211684376e-06, 9.27843211684376e-06, 9.27843211684376e-06, 9.27843211684376e-06, 9.27843211684376e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.27843211684376e-06

Optimization complete. Final v2v error: 2.621569871902466 mm

Highest mean error: 3.7970094680786133 mm for frame 65

Lowest mean error: 2.4547643661499023 mm for frame 191

Saving results

Total time: 94.07929849624634
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_beatrice_posed_036/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_beatrice_posed_036/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815801
Iteration 2/25 | Loss: 0.00136153
Iteration 3/25 | Loss: 0.00113820
Iteration 4/25 | Loss: 0.00113010
Iteration 5/25 | Loss: 0.00112786
Iteration 6/25 | Loss: 0.00112786
Iteration 7/25 | Loss: 0.00112786
Iteration 8/25 | Loss: 0.00112786
Iteration 9/25 | Loss: 0.00112786
Iteration 10/25 | Loss: 0.00112786
Iteration 11/25 | Loss: 0.00112786
Iteration 12/25 | Loss: 0.00112786
Iteration 13/25 | Loss: 0.00112786
Iteration 14/25 | Loss: 0.00112786
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011278626043349504, 0.0011278626043349504, 0.0011278626043349504, 0.0011278626043349504, 0.0011278626043349504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011278626043349504

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.90693891
Iteration 2/25 | Loss: 0.00045457
Iteration 3/25 | Loss: 0.00045457
Iteration 4/25 | Loss: 0.00045457
Iteration 5/25 | Loss: 0.00045457
Iteration 6/25 | Loss: 0.00045457
Iteration 7/25 | Loss: 0.00045457
Iteration 8/25 | Loss: 0.00045457
Iteration 9/25 | Loss: 0.00045457
Iteration 10/25 | Loss: 0.00045457
Iteration 11/25 | Loss: 0.00045457
Iteration 12/25 | Loss: 0.00045457
Iteration 13/25 | Loss: 0.00045457
Iteration 14/25 | Loss: 0.00045457
Iteration 15/25 | Loss: 0.00045457
Iteration 16/25 | Loss: 0.00045457
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0004545701085589826, 0.0004545701085589826, 0.0004545701085589826, 0.0004545701085589826, 0.0004545701085589826]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0004545701085589826

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045457
Iteration 2/1000 | Loss: 0.00002861
Iteration 3/1000 | Loss: 0.00002277
Iteration 4/1000 | Loss: 0.00002138
Iteration 5/1000 | Loss: 0.00002045
Iteration 6/1000 | Loss: 0.00001990
Iteration 7/1000 | Loss: 0.00001978
Iteration 8/1000 | Loss: 0.00001940
Iteration 9/1000 | Loss: 0.00001917
Iteration 10/1000 | Loss: 0.00001913
Iteration 11/1000 | Loss: 0.00001899
Iteration 12/1000 | Loss: 0.00001891
Iteration 13/1000 | Loss: 0.00001890
Iteration 14/1000 | Loss: 0.00001889
Iteration 15/1000 | Loss: 0.00001886
Iteration 16/1000 | Loss: 0.00001885
Iteration 17/1000 | Loss: 0.00001885
Iteration 18/1000 | Loss: 0.00001884
Iteration 19/1000 | Loss: 0.00001884
Iteration 20/1000 | Loss: 0.00001881
Iteration 21/1000 | Loss: 0.00001880
Iteration 22/1000 | Loss: 0.00001875
Iteration 23/1000 | Loss: 0.00001872
Iteration 24/1000 | Loss: 0.00001872
Iteration 25/1000 | Loss: 0.00001872
Iteration 26/1000 | Loss: 0.00001872
Iteration 27/1000 | Loss: 0.00001872
Iteration 28/1000 | Loss: 0.00001871
Iteration 29/1000 | Loss: 0.00001871
Iteration 30/1000 | Loss: 0.00001871
Iteration 31/1000 | Loss: 0.00001871
Iteration 32/1000 | Loss: 0.00001870
Iteration 33/1000 | Loss: 0.00001870
Iteration 34/1000 | Loss: 0.00001870
Iteration 35/1000 | Loss: 0.00001870
Iteration 36/1000 | Loss: 0.00001870
Iteration 37/1000 | Loss: 0.00001870
Iteration 38/1000 | Loss: 0.00001869
Iteration 39/1000 | Loss: 0.00001869
Iteration 40/1000 | Loss: 0.00001869
Iteration 41/1000 | Loss: 0.00001869
Iteration 42/1000 | Loss: 0.00001869
Iteration 43/1000 | Loss: 0.00001869
Iteration 44/1000 | Loss: 0.00001869
Iteration 45/1000 | Loss: 0.00001869
Iteration 46/1000 | Loss: 0.00001869
Iteration 47/1000 | Loss: 0.00001869
Iteration 48/1000 | Loss: 0.00001869
Iteration 49/1000 | Loss: 0.00001868
Iteration 50/1000 | Loss: 0.00001868
Iteration 51/1000 | Loss: 0.00001867
Iteration 52/1000 | Loss: 0.00001866
Iteration 53/1000 | Loss: 0.00001866
Iteration 54/1000 | Loss: 0.00001865
Iteration 55/1000 | Loss: 0.00001865
Iteration 56/1000 | Loss: 0.00001865
Iteration 57/1000 | Loss: 0.00001865
Iteration 58/1000 | Loss: 0.00001865
Iteration 59/1000 | Loss: 0.00001865
Iteration 60/1000 | Loss: 0.00001865
Iteration 61/1000 | Loss: 0.00001865
Iteration 62/1000 | Loss: 0.00001865
Iteration 63/1000 | Loss: 0.00001865
Iteration 64/1000 | Loss: 0.00001865
Iteration 65/1000 | Loss: 0.00001865
Iteration 66/1000 | Loss: 0.00001864
Iteration 67/1000 | Loss: 0.00001864
Iteration 68/1000 | Loss: 0.00001863
Iteration 69/1000 | Loss: 0.00001863
Iteration 70/1000 | Loss: 0.00001863
Iteration 71/1000 | Loss: 0.00001863
Iteration 72/1000 | Loss: 0.00001863
Iteration 73/1000 | Loss: 0.00001862
Iteration 74/1000 | Loss: 0.00001862
Iteration 75/1000 | Loss: 0.00001862
Iteration 76/1000 | Loss: 0.00001862
Iteration 77/1000 | Loss: 0.00001862
Iteration 78/1000 | Loss: 0.00001862
Iteration 79/1000 | Loss: 0.00001862
Iteration 80/1000 | Loss: 0.00001862
Iteration 81/1000 | Loss: 0.00001862
Iteration 82/1000 | Loss: 0.00001861
Iteration 83/1000 | Loss: 0.00001861
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001861
Iteration 86/1000 | Loss: 0.00001861
Iteration 87/1000 | Loss: 0.00001861
Iteration 88/1000 | Loss: 0.00001861
Iteration 89/1000 | Loss: 0.00001861
Iteration 90/1000 | Loss: 0.00001860
Iteration 91/1000 | Loss: 0.00001860
Iteration 92/1000 | Loss: 0.00001860
Iteration 93/1000 | Loss: 0.00001860
Iteration 94/1000 | Loss: 0.00001860
Iteration 95/1000 | Loss: 0.00001860
Iteration 96/1000 | Loss: 0.00001860
Iteration 97/1000 | Loss: 0.00001860
Iteration 98/1000 | Loss: 0.00001860
Iteration 99/1000 | Loss: 0.00001860
Iteration 100/1000 | Loss: 0.00001860
Iteration 101/1000 | Loss: 0.00001860
Iteration 102/1000 | Loss: 0.00001860
Iteration 103/1000 | Loss: 0.00001860
Iteration 104/1000 | Loss: 0.00001860
Iteration 105/1000 | Loss: 0.00001860
Iteration 106/1000 | Loss: 0.00001860
Iteration 107/1000 | Loss: 0.00001860
Iteration 108/1000 | Loss: 0.00001860
Iteration 109/1000 | Loss: 0.00001860
Iteration 110/1000 | Loss: 0.00001860
Iteration 111/1000 | Loss: 0.00001860
Iteration 112/1000 | Loss: 0.00001860
Iteration 113/1000 | Loss: 0.00001860
Iteration 114/1000 | Loss: 0.00001860
Iteration 115/1000 | Loss: 0.00001860
Iteration 116/1000 | Loss: 0.00001859
Iteration 117/1000 | Loss: 0.00001858
Iteration 118/1000 | Loss: 0.00001858
Iteration 119/1000 | Loss: 0.00001858
Iteration 120/1000 | Loss: 0.00001858
Iteration 121/1000 | Loss: 0.00001858
Iteration 122/1000 | Loss: 0.00001858
Iteration 123/1000 | Loss: 0.00001858
Iteration 124/1000 | Loss: 0.00001858
Iteration 125/1000 | Loss: 0.00001858
Iteration 126/1000 | Loss: 0.00001858
Iteration 127/1000 | Loss: 0.00001858
Iteration 128/1000 | Loss: 0.00001858
Iteration 129/1000 | Loss: 0.00001858
Iteration 130/1000 | Loss: 0.00001858
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [1.8582280972623266e-05, 1.8582280972623266e-05, 1.8582280972623266e-05, 1.8582280972623266e-05, 1.8582280972623266e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8582280972623266e-05

Optimization complete. Final v2v error: 3.5907399654388428 mm

Highest mean error: 3.7139832973480225 mm for frame 103

Lowest mean error: 3.482001781463623 mm for frame 184

Saving results

Total time: 35.37533521652222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987462
Iteration 2/25 | Loss: 0.00300502
Iteration 3/25 | Loss: 0.00282206
Iteration 4/25 | Loss: 0.00280565
Iteration 5/25 | Loss: 0.00280129
Iteration 6/25 | Loss: 0.00280125
Iteration 7/25 | Loss: 0.00280125
Iteration 8/25 | Loss: 0.00280125
Iteration 9/25 | Loss: 0.00280125
Iteration 10/25 | Loss: 0.00280125
Iteration 11/25 | Loss: 0.00280125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0028012488037347794, 0.0028012488037347794, 0.0028012488037347794, 0.0028012488037347794, 0.0028012488037347794]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028012488037347794

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59718609
Iteration 2/25 | Loss: 0.00350301
Iteration 3/25 | Loss: 0.00350301
Iteration 4/25 | Loss: 0.00350300
Iteration 5/25 | Loss: 0.00350300
Iteration 6/25 | Loss: 0.00350300
Iteration 7/25 | Loss: 0.00350300
Iteration 8/25 | Loss: 0.00350300
Iteration 9/25 | Loss: 0.00350300
Iteration 10/25 | Loss: 0.00350300
Iteration 11/25 | Loss: 0.00350300
Iteration 12/25 | Loss: 0.00350300
Iteration 13/25 | Loss: 0.00350300
Iteration 14/25 | Loss: 0.00350300
Iteration 15/25 | Loss: 0.00350300
Iteration 16/25 | Loss: 0.00350300
Iteration 17/25 | Loss: 0.00350300
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0035030024591833353, 0.0035030024591833353, 0.0035030024591833353, 0.0035030024591833353, 0.0035030024591833353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0035030024591833353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00350300
Iteration 2/1000 | Loss: 0.00016610
Iteration 3/1000 | Loss: 0.00009961
Iteration 4/1000 | Loss: 0.00008768
Iteration 5/1000 | Loss: 0.00008189
Iteration 6/1000 | Loss: 0.00007979
Iteration 7/1000 | Loss: 0.00007852
Iteration 8/1000 | Loss: 0.00007786
Iteration 9/1000 | Loss: 0.00007726
Iteration 10/1000 | Loss: 0.00007684
Iteration 11/1000 | Loss: 0.00007668
Iteration 12/1000 | Loss: 0.00007653
Iteration 13/1000 | Loss: 0.00007643
Iteration 14/1000 | Loss: 0.00007642
Iteration 15/1000 | Loss: 0.00007633
Iteration 16/1000 | Loss: 0.00007624
Iteration 17/1000 | Loss: 0.00007623
Iteration 18/1000 | Loss: 0.00007623
Iteration 19/1000 | Loss: 0.00007621
Iteration 20/1000 | Loss: 0.00007621
Iteration 21/1000 | Loss: 0.00007621
Iteration 22/1000 | Loss: 0.00007621
Iteration 23/1000 | Loss: 0.00007620
Iteration 24/1000 | Loss: 0.00007620
Iteration 25/1000 | Loss: 0.00007620
Iteration 26/1000 | Loss: 0.00007620
Iteration 27/1000 | Loss: 0.00007620
Iteration 28/1000 | Loss: 0.00007620
Iteration 29/1000 | Loss: 0.00007619
Iteration 30/1000 | Loss: 0.00007619
Iteration 31/1000 | Loss: 0.00007618
Iteration 32/1000 | Loss: 0.00007617
Iteration 33/1000 | Loss: 0.00007617
Iteration 34/1000 | Loss: 0.00007616
Iteration 35/1000 | Loss: 0.00007616
Iteration 36/1000 | Loss: 0.00007615
Iteration 37/1000 | Loss: 0.00007615
Iteration 38/1000 | Loss: 0.00007615
Iteration 39/1000 | Loss: 0.00007615
Iteration 40/1000 | Loss: 0.00007615
Iteration 41/1000 | Loss: 0.00007615
Iteration 42/1000 | Loss: 0.00007615
Iteration 43/1000 | Loss: 0.00007615
Iteration 44/1000 | Loss: 0.00007615
Iteration 45/1000 | Loss: 0.00007615
Iteration 46/1000 | Loss: 0.00007615
Iteration 47/1000 | Loss: 0.00007615
Iteration 48/1000 | Loss: 0.00007614
Iteration 49/1000 | Loss: 0.00007614
Iteration 50/1000 | Loss: 0.00007613
Iteration 51/1000 | Loss: 0.00007613
Iteration 52/1000 | Loss: 0.00007612
Iteration 53/1000 | Loss: 0.00007611
Iteration 54/1000 | Loss: 0.00007611
Iteration 55/1000 | Loss: 0.00007611
Iteration 56/1000 | Loss: 0.00007611
Iteration 57/1000 | Loss: 0.00007611
Iteration 58/1000 | Loss: 0.00007611
Iteration 59/1000 | Loss: 0.00007611
Iteration 60/1000 | Loss: 0.00007611
Iteration 61/1000 | Loss: 0.00007611
Iteration 62/1000 | Loss: 0.00007610
Iteration 63/1000 | Loss: 0.00007610
Iteration 64/1000 | Loss: 0.00007610
Iteration 65/1000 | Loss: 0.00007610
Iteration 66/1000 | Loss: 0.00007610
Iteration 67/1000 | Loss: 0.00007610
Iteration 68/1000 | Loss: 0.00007610
Iteration 69/1000 | Loss: 0.00007610
Iteration 70/1000 | Loss: 0.00007610
Iteration 71/1000 | Loss: 0.00007610
Iteration 72/1000 | Loss: 0.00007610
Iteration 73/1000 | Loss: 0.00007610
Iteration 74/1000 | Loss: 0.00007610
Iteration 75/1000 | Loss: 0.00007610
Iteration 76/1000 | Loss: 0.00007610
Iteration 77/1000 | Loss: 0.00007610
Iteration 78/1000 | Loss: 0.00007610
Iteration 79/1000 | Loss: 0.00007609
Iteration 80/1000 | Loss: 0.00007609
Iteration 81/1000 | Loss: 0.00007609
Iteration 82/1000 | Loss: 0.00007609
Iteration 83/1000 | Loss: 0.00007609
Iteration 84/1000 | Loss: 0.00007609
Iteration 85/1000 | Loss: 0.00007609
Iteration 86/1000 | Loss: 0.00007609
Iteration 87/1000 | Loss: 0.00007608
Iteration 88/1000 | Loss: 0.00007608
Iteration 89/1000 | Loss: 0.00007608
Iteration 90/1000 | Loss: 0.00007608
Iteration 91/1000 | Loss: 0.00007608
Iteration 92/1000 | Loss: 0.00007608
Iteration 93/1000 | Loss: 0.00007608
Iteration 94/1000 | Loss: 0.00007608
Iteration 95/1000 | Loss: 0.00007608
Iteration 96/1000 | Loss: 0.00007608
Iteration 97/1000 | Loss: 0.00007608
Iteration 98/1000 | Loss: 0.00007608
Iteration 99/1000 | Loss: 0.00007608
Iteration 100/1000 | Loss: 0.00007608
Iteration 101/1000 | Loss: 0.00007608
Iteration 102/1000 | Loss: 0.00007608
Iteration 103/1000 | Loss: 0.00007608
Iteration 104/1000 | Loss: 0.00007608
Iteration 105/1000 | Loss: 0.00007608
Iteration 106/1000 | Loss: 0.00007608
Iteration 107/1000 | Loss: 0.00007608
Iteration 108/1000 | Loss: 0.00007608
Iteration 109/1000 | Loss: 0.00007608
Iteration 110/1000 | Loss: 0.00007608
Iteration 111/1000 | Loss: 0.00007608
Iteration 112/1000 | Loss: 0.00007608
Iteration 113/1000 | Loss: 0.00007608
Iteration 114/1000 | Loss: 0.00007608
Iteration 115/1000 | Loss: 0.00007608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [7.607973384438083e-05, 7.607973384438083e-05, 7.607973384438083e-05, 7.607973384438083e-05, 7.607973384438083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.607973384438083e-05

Optimization complete. Final v2v error: 7.6885881423950195 mm

Highest mean error: 7.945430278778076 mm for frame 222

Lowest mean error: 7.464339256286621 mm for frame 57

Saving results

Total time: 38.62268114089966
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01060361
Iteration 2/25 | Loss: 0.00324293
Iteration 3/25 | Loss: 0.00303297
Iteration 4/25 | Loss: 0.00297699
Iteration 5/25 | Loss: 0.00296039
Iteration 6/25 | Loss: 0.00295845
Iteration 7/25 | Loss: 0.00295794
Iteration 8/25 | Loss: 0.00295894
Iteration 9/25 | Loss: 0.00295460
Iteration 10/25 | Loss: 0.00295429
Iteration 11/25 | Loss: 0.00295425
Iteration 12/25 | Loss: 0.00295425
Iteration 13/25 | Loss: 0.00295425
Iteration 14/25 | Loss: 0.00295425
Iteration 15/25 | Loss: 0.00295425
Iteration 16/25 | Loss: 0.00295425
Iteration 17/25 | Loss: 0.00295425
Iteration 18/25 | Loss: 0.00295425
Iteration 19/25 | Loss: 0.00295425
Iteration 20/25 | Loss: 0.00295424
Iteration 21/25 | Loss: 0.00295424
Iteration 22/25 | Loss: 0.00295424
Iteration 23/25 | Loss: 0.00295424
Iteration 24/25 | Loss: 0.00295424
Iteration 25/25 | Loss: 0.00295424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.97919464
Iteration 2/25 | Loss: 0.00421304
Iteration 3/25 | Loss: 0.00421303
Iteration 4/25 | Loss: 0.00421303
Iteration 5/25 | Loss: 0.00421303
Iteration 6/25 | Loss: 0.00421303
Iteration 7/25 | Loss: 0.00421303
Iteration 8/25 | Loss: 0.00421303
Iteration 9/25 | Loss: 0.00421303
Iteration 10/25 | Loss: 0.00421303
Iteration 11/25 | Loss: 0.00421303
Iteration 12/25 | Loss: 0.00421303
Iteration 13/25 | Loss: 0.00421303
Iteration 14/25 | Loss: 0.00421303
Iteration 15/25 | Loss: 0.00421303
Iteration 16/25 | Loss: 0.00421303
Iteration 17/25 | Loss: 0.00421303
Iteration 18/25 | Loss: 0.00421303
Iteration 19/25 | Loss: 0.00421303
Iteration 20/25 | Loss: 0.00421303
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0042130290530622005, 0.0042130290530622005, 0.0042130290530622005, 0.0042130290530622005, 0.0042130290530622005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0042130290530622005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00421303
Iteration 2/1000 | Loss: 0.00015304
Iteration 3/1000 | Loss: 0.00010973
Iteration 4/1000 | Loss: 0.00009066
Iteration 5/1000 | Loss: 0.00008407
Iteration 6/1000 | Loss: 0.00007975
Iteration 7/1000 | Loss: 0.00007767
Iteration 8/1000 | Loss: 0.00007649
Iteration 9/1000 | Loss: 0.00007560
Iteration 10/1000 | Loss: 0.00007494
Iteration 11/1000 | Loss: 0.00007465
Iteration 12/1000 | Loss: 0.00007446
Iteration 13/1000 | Loss: 0.00007440
Iteration 14/1000 | Loss: 0.00007430
Iteration 15/1000 | Loss: 0.00007428
Iteration 16/1000 | Loss: 0.00007427
Iteration 17/1000 | Loss: 0.00007427
Iteration 18/1000 | Loss: 0.00007426
Iteration 19/1000 | Loss: 0.00007425
Iteration 20/1000 | Loss: 0.00007425
Iteration 21/1000 | Loss: 0.00007425
Iteration 22/1000 | Loss: 0.00007425
Iteration 23/1000 | Loss: 0.00007425
Iteration 24/1000 | Loss: 0.00007425
Iteration 25/1000 | Loss: 0.00007425
Iteration 26/1000 | Loss: 0.00007424
Iteration 27/1000 | Loss: 0.00007424
Iteration 28/1000 | Loss: 0.00007424
Iteration 29/1000 | Loss: 0.00007423
Iteration 30/1000 | Loss: 0.00007422
Iteration 31/1000 | Loss: 0.00007421
Iteration 32/1000 | Loss: 0.00007421
Iteration 33/1000 | Loss: 0.00007421
Iteration 34/1000 | Loss: 0.00007421
Iteration 35/1000 | Loss: 0.00007421
Iteration 36/1000 | Loss: 0.00007420
Iteration 37/1000 | Loss: 0.00007420
Iteration 38/1000 | Loss: 0.00007420
Iteration 39/1000 | Loss: 0.00007419
Iteration 40/1000 | Loss: 0.00007419
Iteration 41/1000 | Loss: 0.00007419
Iteration 42/1000 | Loss: 0.00007418
Iteration 43/1000 | Loss: 0.00007418
Iteration 44/1000 | Loss: 0.00007418
Iteration 45/1000 | Loss: 0.00007417
Iteration 46/1000 | Loss: 0.00007417
Iteration 47/1000 | Loss: 0.00007416
Iteration 48/1000 | Loss: 0.00007416
Iteration 49/1000 | Loss: 0.00007416
Iteration 50/1000 | Loss: 0.00007416
Iteration 51/1000 | Loss: 0.00007416
Iteration 52/1000 | Loss: 0.00007416
Iteration 53/1000 | Loss: 0.00007416
Iteration 54/1000 | Loss: 0.00007416
Iteration 55/1000 | Loss: 0.00007415
Iteration 56/1000 | Loss: 0.00007415
Iteration 57/1000 | Loss: 0.00007415
Iteration 58/1000 | Loss: 0.00007415
Iteration 59/1000 | Loss: 0.00007415
Iteration 60/1000 | Loss: 0.00007414
Iteration 61/1000 | Loss: 0.00007414
Iteration 62/1000 | Loss: 0.00007414
Iteration 63/1000 | Loss: 0.00007414
Iteration 64/1000 | Loss: 0.00007414
Iteration 65/1000 | Loss: 0.00007414
Iteration 66/1000 | Loss: 0.00007414
Iteration 67/1000 | Loss: 0.00007414
Iteration 68/1000 | Loss: 0.00007414
Iteration 69/1000 | Loss: 0.00007414
Iteration 70/1000 | Loss: 0.00007414
Iteration 71/1000 | Loss: 0.00007414
Iteration 72/1000 | Loss: 0.00007414
Iteration 73/1000 | Loss: 0.00007414
Iteration 74/1000 | Loss: 0.00007414
Iteration 75/1000 | Loss: 0.00007413
Iteration 76/1000 | Loss: 0.00007413
Iteration 77/1000 | Loss: 0.00007413
Iteration 78/1000 | Loss: 0.00007413
Iteration 79/1000 | Loss: 0.00007413
Iteration 80/1000 | Loss: 0.00007413
Iteration 81/1000 | Loss: 0.00007413
Iteration 82/1000 | Loss: 0.00007413
Iteration 83/1000 | Loss: 0.00007413
Iteration 84/1000 | Loss: 0.00007413
Iteration 85/1000 | Loss: 0.00007413
Iteration 86/1000 | Loss: 0.00007413
Iteration 87/1000 | Loss: 0.00007413
Iteration 88/1000 | Loss: 0.00007413
Iteration 89/1000 | Loss: 0.00007413
Iteration 90/1000 | Loss: 0.00007413
Iteration 91/1000 | Loss: 0.00007413
Iteration 92/1000 | Loss: 0.00007413
Iteration 93/1000 | Loss: 0.00007413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [7.412802369799465e-05, 7.412802369799465e-05, 7.412802369799465e-05, 7.412802369799465e-05, 7.412802369799465e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.412802369799465e-05

Optimization complete. Final v2v error: 7.626153945922852 mm

Highest mean error: 7.874586582183838 mm for frame 126

Lowest mean error: 7.361240863800049 mm for frame 21

Saving results

Total time: 42.10550618171692
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01237206
Iteration 2/25 | Loss: 0.00323315
Iteration 3/25 | Loss: 0.00304628
Iteration 4/25 | Loss: 0.00255175
Iteration 5/25 | Loss: 0.00245619
Iteration 6/25 | Loss: 0.00243095
Iteration 7/25 | Loss: 0.00241816
Iteration 8/25 | Loss: 0.00242351
Iteration 9/25 | Loss: 0.00240400
Iteration 10/25 | Loss: 0.00239885
Iteration 11/25 | Loss: 0.00239382
Iteration 12/25 | Loss: 0.00239246
Iteration 13/25 | Loss: 0.00239146
Iteration 14/25 | Loss: 0.00239167
Iteration 15/25 | Loss: 0.00239164
Iteration 16/25 | Loss: 0.00239193
Iteration 17/25 | Loss: 0.00239198
Iteration 18/25 | Loss: 0.00239203
Iteration 19/25 | Loss: 0.00239185
Iteration 20/25 | Loss: 0.00239218
Iteration 21/25 | Loss: 0.00239219
Iteration 22/25 | Loss: 0.00239073
Iteration 23/25 | Loss: 0.00239148
Iteration 24/25 | Loss: 0.00239117
Iteration 25/25 | Loss: 0.00239222

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45685363
Iteration 2/25 | Loss: 0.00268744
Iteration 3/25 | Loss: 0.00268744
Iteration 4/25 | Loss: 0.00268744
Iteration 5/25 | Loss: 0.00268744
Iteration 6/25 | Loss: 0.00268744
Iteration 7/25 | Loss: 0.00268744
Iteration 8/25 | Loss: 0.00268744
Iteration 9/25 | Loss: 0.00268744
Iteration 10/25 | Loss: 0.00268744
Iteration 11/25 | Loss: 0.00268744
Iteration 12/25 | Loss: 0.00268744
Iteration 13/25 | Loss: 0.00268744
Iteration 14/25 | Loss: 0.00268744
Iteration 15/25 | Loss: 0.00268744
Iteration 16/25 | Loss: 0.00268744
Iteration 17/25 | Loss: 0.00268744
Iteration 18/25 | Loss: 0.00268744
Iteration 19/25 | Loss: 0.00268744
Iteration 20/25 | Loss: 0.00268744
Iteration 21/25 | Loss: 0.00268744
Iteration 22/25 | Loss: 0.00268744
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0026874379254877567, 0.0026874379254877567, 0.0026874379254877567, 0.0026874379254877567, 0.0026874379254877567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026874379254877567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00268744
Iteration 2/1000 | Loss: 0.00025203
Iteration 3/1000 | Loss: 0.00014526
Iteration 4/1000 | Loss: 0.00011187
Iteration 5/1000 | Loss: 0.00010970
Iteration 6/1000 | Loss: 0.00018660
Iteration 7/1000 | Loss: 0.00008975
Iteration 8/1000 | Loss: 0.00008484
Iteration 9/1000 | Loss: 0.00023572
Iteration 10/1000 | Loss: 0.00007990
Iteration 11/1000 | Loss: 0.00007797
Iteration 12/1000 | Loss: 0.00007702
Iteration 13/1000 | Loss: 0.00007631
Iteration 14/1000 | Loss: 0.00007534
Iteration 15/1000 | Loss: 0.00007467
Iteration 16/1000 | Loss: 0.00007405
Iteration 17/1000 | Loss: 0.00007378
Iteration 18/1000 | Loss: 0.00007362
Iteration 19/1000 | Loss: 0.00021189
Iteration 20/1000 | Loss: 0.00007343
Iteration 21/1000 | Loss: 0.00023957
Iteration 22/1000 | Loss: 0.00095374
Iteration 23/1000 | Loss: 0.00015023
Iteration 24/1000 | Loss: 0.00009315
Iteration 25/1000 | Loss: 0.00007470
Iteration 26/1000 | Loss: 0.00008545
Iteration 27/1000 | Loss: 0.00007347
Iteration 28/1000 | Loss: 0.00007325
Iteration 29/1000 | Loss: 0.00022693
Iteration 30/1000 | Loss: 0.00011213
Iteration 31/1000 | Loss: 0.00007955
Iteration 32/1000 | Loss: 0.00007338
Iteration 33/1000 | Loss: 0.00020139
Iteration 34/1000 | Loss: 0.00007340
Iteration 35/1000 | Loss: 0.00022537
Iteration 36/1000 | Loss: 0.00013046
Iteration 37/1000 | Loss: 0.00008729
Iteration 38/1000 | Loss: 0.00015636
Iteration 39/1000 | Loss: 0.00007329
Iteration 40/1000 | Loss: 0.00007316
Iteration 41/1000 | Loss: 0.00007315
Iteration 42/1000 | Loss: 0.00007315
Iteration 43/1000 | Loss: 0.00007315
Iteration 44/1000 | Loss: 0.00007314
Iteration 45/1000 | Loss: 0.00007313
Iteration 46/1000 | Loss: 0.00007312
Iteration 47/1000 | Loss: 0.00007312
Iteration 48/1000 | Loss: 0.00007312
Iteration 49/1000 | Loss: 0.00007312
Iteration 50/1000 | Loss: 0.00007312
Iteration 51/1000 | Loss: 0.00007312
Iteration 52/1000 | Loss: 0.00007311
Iteration 53/1000 | Loss: 0.00007311
Iteration 54/1000 | Loss: 0.00007311
Iteration 55/1000 | Loss: 0.00007311
Iteration 56/1000 | Loss: 0.00007310
Iteration 57/1000 | Loss: 0.00007310
Iteration 58/1000 | Loss: 0.00007310
Iteration 59/1000 | Loss: 0.00007310
Iteration 60/1000 | Loss: 0.00007309
Iteration 61/1000 | Loss: 0.00007309
Iteration 62/1000 | Loss: 0.00007309
Iteration 63/1000 | Loss: 0.00007309
Iteration 64/1000 | Loss: 0.00007309
Iteration 65/1000 | Loss: 0.00007309
Iteration 66/1000 | Loss: 0.00007309
Iteration 67/1000 | Loss: 0.00007309
Iteration 68/1000 | Loss: 0.00007308
Iteration 69/1000 | Loss: 0.00007308
Iteration 70/1000 | Loss: 0.00007308
Iteration 71/1000 | Loss: 0.00007308
Iteration 72/1000 | Loss: 0.00007308
Iteration 73/1000 | Loss: 0.00007308
Iteration 74/1000 | Loss: 0.00007308
Iteration 75/1000 | Loss: 0.00007308
Iteration 76/1000 | Loss: 0.00007308
Iteration 77/1000 | Loss: 0.00007308
Iteration 78/1000 | Loss: 0.00007308
Iteration 79/1000 | Loss: 0.00007308
Iteration 80/1000 | Loss: 0.00007308
Iteration 81/1000 | Loss: 0.00007308
Iteration 82/1000 | Loss: 0.00007308
Iteration 83/1000 | Loss: 0.00007308
Iteration 84/1000 | Loss: 0.00007308
Iteration 85/1000 | Loss: 0.00007308
Iteration 86/1000 | Loss: 0.00007307
Iteration 87/1000 | Loss: 0.00007307
Iteration 88/1000 | Loss: 0.00007307
Iteration 89/1000 | Loss: 0.00007307
Iteration 90/1000 | Loss: 0.00007306
Iteration 91/1000 | Loss: 0.00007306
Iteration 92/1000 | Loss: 0.00007306
Iteration 93/1000 | Loss: 0.00007306
Iteration 94/1000 | Loss: 0.00007306
Iteration 95/1000 | Loss: 0.00007306
Iteration 96/1000 | Loss: 0.00007306
Iteration 97/1000 | Loss: 0.00007306
Iteration 98/1000 | Loss: 0.00007306
Iteration 99/1000 | Loss: 0.00007306
Iteration 100/1000 | Loss: 0.00007306
Iteration 101/1000 | Loss: 0.00007306
Iteration 102/1000 | Loss: 0.00007306
Iteration 103/1000 | Loss: 0.00007306
Iteration 104/1000 | Loss: 0.00007306
Iteration 105/1000 | Loss: 0.00007306
Iteration 106/1000 | Loss: 0.00007306
Iteration 107/1000 | Loss: 0.00007306
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 107. Stopping optimization.
Last 5 losses: [7.306301267817616e-05, 7.306301267817616e-05, 7.306301267817616e-05, 7.306301267817616e-05, 7.306301267817616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.306301267817616e-05

Optimization complete. Final v2v error: 7.146193027496338 mm

Highest mean error: 17.961917877197266 mm for frame 141

Lowest mean error: 6.657306671142578 mm for frame 53

Saving results

Total time: 102.07486200332642
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00596117
Iteration 2/25 | Loss: 0.00292039
Iteration 3/25 | Loss: 0.00280911
Iteration 4/25 | Loss: 0.00279771
Iteration 5/25 | Loss: 0.00279363
Iteration 6/25 | Loss: 0.00279214
Iteration 7/25 | Loss: 0.00279208
Iteration 8/25 | Loss: 0.00279208
Iteration 9/25 | Loss: 0.00279208
Iteration 10/25 | Loss: 0.00279208
Iteration 11/25 | Loss: 0.00279208
Iteration 12/25 | Loss: 0.00279208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002792076440528035, 0.002792076440528035, 0.002792076440528035, 0.002792076440528035, 0.002792076440528035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002792076440528035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.32151604
Iteration 2/25 | Loss: 0.00369657
Iteration 3/25 | Loss: 0.00369650
Iteration 4/25 | Loss: 0.00369650
Iteration 5/25 | Loss: 0.00369650
Iteration 6/25 | Loss: 0.00369650
Iteration 7/25 | Loss: 0.00369650
Iteration 8/25 | Loss: 0.00369650
Iteration 9/25 | Loss: 0.00369650
Iteration 10/25 | Loss: 0.00369650
Iteration 11/25 | Loss: 0.00369650
Iteration 12/25 | Loss: 0.00369650
Iteration 13/25 | Loss: 0.00369650
Iteration 14/25 | Loss: 0.00369650
Iteration 15/25 | Loss: 0.00369650
Iteration 16/25 | Loss: 0.00369650
Iteration 17/25 | Loss: 0.00369650
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.003696498926728964, 0.003696498926728964, 0.003696498926728964, 0.003696498926728964, 0.003696498926728964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003696498926728964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00369650
Iteration 2/1000 | Loss: 0.00020000
Iteration 3/1000 | Loss: 0.00013385
Iteration 4/1000 | Loss: 0.00010469
Iteration 5/1000 | Loss: 0.00009606
Iteration 6/1000 | Loss: 0.00009239
Iteration 7/1000 | Loss: 0.00008965
Iteration 8/1000 | Loss: 0.00008792
Iteration 9/1000 | Loss: 0.00008652
Iteration 10/1000 | Loss: 0.00008525
Iteration 11/1000 | Loss: 0.00008425
Iteration 12/1000 | Loss: 0.00008346
Iteration 13/1000 | Loss: 0.00008287
Iteration 14/1000 | Loss: 0.00008260
Iteration 15/1000 | Loss: 0.00008237
Iteration 16/1000 | Loss: 0.00008219
Iteration 17/1000 | Loss: 0.00008208
Iteration 18/1000 | Loss: 0.00008207
Iteration 19/1000 | Loss: 0.00008199
Iteration 20/1000 | Loss: 0.00008195
Iteration 21/1000 | Loss: 0.00008195
Iteration 22/1000 | Loss: 0.00008194
Iteration 23/1000 | Loss: 0.00008193
Iteration 24/1000 | Loss: 0.00008192
Iteration 25/1000 | Loss: 0.00008191
Iteration 26/1000 | Loss: 0.00008191
Iteration 27/1000 | Loss: 0.00008191
Iteration 28/1000 | Loss: 0.00008190
Iteration 29/1000 | Loss: 0.00008190
Iteration 30/1000 | Loss: 0.00008190
Iteration 31/1000 | Loss: 0.00008190
Iteration 32/1000 | Loss: 0.00008189
Iteration 33/1000 | Loss: 0.00008189
Iteration 34/1000 | Loss: 0.00008189
Iteration 35/1000 | Loss: 0.00008189
Iteration 36/1000 | Loss: 0.00008189
Iteration 37/1000 | Loss: 0.00008189
Iteration 38/1000 | Loss: 0.00008188
Iteration 39/1000 | Loss: 0.00008188
Iteration 40/1000 | Loss: 0.00008188
Iteration 41/1000 | Loss: 0.00008187
Iteration 42/1000 | Loss: 0.00008187
Iteration 43/1000 | Loss: 0.00008187
Iteration 44/1000 | Loss: 0.00008187
Iteration 45/1000 | Loss: 0.00008187
Iteration 46/1000 | Loss: 0.00008187
Iteration 47/1000 | Loss: 0.00008187
Iteration 48/1000 | Loss: 0.00008187
Iteration 49/1000 | Loss: 0.00008187
Iteration 50/1000 | Loss: 0.00008187
Iteration 51/1000 | Loss: 0.00008187
Iteration 52/1000 | Loss: 0.00008187
Iteration 53/1000 | Loss: 0.00008187
Iteration 54/1000 | Loss: 0.00008187
Iteration 55/1000 | Loss: 0.00008187
Iteration 56/1000 | Loss: 0.00008187
Iteration 57/1000 | Loss: 0.00008187
Iteration 58/1000 | Loss: 0.00008187
Iteration 59/1000 | Loss: 0.00008187
Iteration 60/1000 | Loss: 0.00008187
Iteration 61/1000 | Loss: 0.00008187
Iteration 62/1000 | Loss: 0.00008187
Iteration 63/1000 | Loss: 0.00008187
Iteration 64/1000 | Loss: 0.00008187
Iteration 65/1000 | Loss: 0.00008187
Iteration 66/1000 | Loss: 0.00008187
Iteration 67/1000 | Loss: 0.00008187
Iteration 68/1000 | Loss: 0.00008187
Iteration 69/1000 | Loss: 0.00008187
Iteration 70/1000 | Loss: 0.00008187
Iteration 71/1000 | Loss: 0.00008187
Iteration 72/1000 | Loss: 0.00008187
Iteration 73/1000 | Loss: 0.00008187
Iteration 74/1000 | Loss: 0.00008187
Iteration 75/1000 | Loss: 0.00008187
Iteration 76/1000 | Loss: 0.00008187
Iteration 77/1000 | Loss: 0.00008187
Iteration 78/1000 | Loss: 0.00008187
Iteration 79/1000 | Loss: 0.00008187
Iteration 80/1000 | Loss: 0.00008187
Iteration 81/1000 | Loss: 0.00008187
Iteration 82/1000 | Loss: 0.00008187
Iteration 83/1000 | Loss: 0.00008187
Iteration 84/1000 | Loss: 0.00008187
Iteration 85/1000 | Loss: 0.00008187
Iteration 86/1000 | Loss: 0.00008187
Iteration 87/1000 | Loss: 0.00008187
Iteration 88/1000 | Loss: 0.00008187
Iteration 89/1000 | Loss: 0.00008187
Iteration 90/1000 | Loss: 0.00008187
Iteration 91/1000 | Loss: 0.00008187
Iteration 92/1000 | Loss: 0.00008187
Iteration 93/1000 | Loss: 0.00008187
Iteration 94/1000 | Loss: 0.00008187
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [8.18664557300508e-05, 8.18664557300508e-05, 8.18664557300508e-05, 8.18664557300508e-05, 8.18664557300508e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.18664557300508e-05

Optimization complete. Final v2v error: 8.028106689453125 mm

Highest mean error: 8.312577247619629 mm for frame 70

Lowest mean error: 7.746653079986572 mm for frame 81

Saving results

Total time: 37.923964977264404
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01044371
Iteration 2/25 | Loss: 0.00462903
Iteration 3/25 | Loss: 0.00372412
Iteration 4/25 | Loss: 0.00331019
Iteration 5/25 | Loss: 0.00300794
Iteration 6/25 | Loss: 0.00289862
Iteration 7/25 | Loss: 0.00286871
Iteration 8/25 | Loss: 0.00285369
Iteration 9/25 | Loss: 0.00286062
Iteration 10/25 | Loss: 0.00285551
Iteration 11/25 | Loss: 0.00284309
Iteration 12/25 | Loss: 0.00284965
Iteration 13/25 | Loss: 0.00284291
Iteration 14/25 | Loss: 0.00284603
Iteration 15/25 | Loss: 0.00284245
Iteration 16/25 | Loss: 0.00285716
Iteration 17/25 | Loss: 0.00283311
Iteration 18/25 | Loss: 0.00282553
Iteration 19/25 | Loss: 0.00282827
Iteration 20/25 | Loss: 0.00282763
Iteration 21/25 | Loss: 0.00282801
Iteration 22/25 | Loss: 0.00283065
Iteration 23/25 | Loss: 0.00282876
Iteration 24/25 | Loss: 0.00282772
Iteration 25/25 | Loss: 0.00282842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47144711
Iteration 2/25 | Loss: 0.00854361
Iteration 3/25 | Loss: 0.00851992
Iteration 4/25 | Loss: 0.00851991
Iteration 5/25 | Loss: 0.00851991
Iteration 6/25 | Loss: 0.00851991
Iteration 7/25 | Loss: 0.00851991
Iteration 8/25 | Loss: 0.00851991
Iteration 9/25 | Loss: 0.00851991
Iteration 10/25 | Loss: 0.00851991
Iteration 11/25 | Loss: 0.00851991
Iteration 12/25 | Loss: 0.00851991
Iteration 13/25 | Loss: 0.00851991
Iteration 14/25 | Loss: 0.00851991
Iteration 15/25 | Loss: 0.00851991
Iteration 16/25 | Loss: 0.00851991
Iteration 17/25 | Loss: 0.00851991
Iteration 18/25 | Loss: 0.00851991
Iteration 19/25 | Loss: 0.00851991
Iteration 20/25 | Loss: 0.00851991
Iteration 21/25 | Loss: 0.00851991
Iteration 22/25 | Loss: 0.00851991
Iteration 23/25 | Loss: 0.00851991
Iteration 24/25 | Loss: 0.00851991
Iteration 25/25 | Loss: 0.00851991

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00851991
Iteration 2/1000 | Loss: 0.00093374
Iteration 3/1000 | Loss: 0.00068495
Iteration 4/1000 | Loss: 0.00086773
Iteration 5/1000 | Loss: 0.00107136
Iteration 6/1000 | Loss: 0.00052177
Iteration 7/1000 | Loss: 0.00052594
Iteration 8/1000 | Loss: 0.00049693
Iteration 9/1000 | Loss: 0.00088890
Iteration 10/1000 | Loss: 0.00048676
Iteration 11/1000 | Loss: 0.00259468
Iteration 12/1000 | Loss: 0.05240620
Iteration 13/1000 | Loss: 0.01527217
Iteration 14/1000 | Loss: 0.00301385
Iteration 15/1000 | Loss: 0.00271237
Iteration 16/1000 | Loss: 0.00572307
Iteration 17/1000 | Loss: 0.00235665
Iteration 18/1000 | Loss: 0.00254167
Iteration 19/1000 | Loss: 0.00159238
Iteration 20/1000 | Loss: 0.00144485
Iteration 21/1000 | Loss: 0.00175567
Iteration 22/1000 | Loss: 0.00051208
Iteration 23/1000 | Loss: 0.00065431
Iteration 24/1000 | Loss: 0.00088337
Iteration 25/1000 | Loss: 0.00119570
Iteration 26/1000 | Loss: 0.00108758
Iteration 27/1000 | Loss: 0.00041274
Iteration 28/1000 | Loss: 0.00033172
Iteration 29/1000 | Loss: 0.00053364
Iteration 30/1000 | Loss: 0.00039879
Iteration 31/1000 | Loss: 0.00045681
Iteration 32/1000 | Loss: 0.00033537
Iteration 33/1000 | Loss: 0.00030966
Iteration 34/1000 | Loss: 0.00023881
Iteration 35/1000 | Loss: 0.00020700
Iteration 36/1000 | Loss: 0.00065280
Iteration 37/1000 | Loss: 0.00063543
Iteration 38/1000 | Loss: 0.00047262
Iteration 39/1000 | Loss: 0.00015945
Iteration 40/1000 | Loss: 0.00017394
Iteration 41/1000 | Loss: 0.00030090
Iteration 42/1000 | Loss: 0.00015689
Iteration 43/1000 | Loss: 0.00031285
Iteration 44/1000 | Loss: 0.00061247
Iteration 45/1000 | Loss: 0.00025041
Iteration 46/1000 | Loss: 0.00054114
Iteration 47/1000 | Loss: 0.00036764
Iteration 48/1000 | Loss: 0.00062396
Iteration 49/1000 | Loss: 0.00022837
Iteration 50/1000 | Loss: 0.00028151
Iteration 51/1000 | Loss: 0.00034772
Iteration 52/1000 | Loss: 0.00024225
Iteration 53/1000 | Loss: 0.00021080
Iteration 54/1000 | Loss: 0.00018017
Iteration 55/1000 | Loss: 0.00016898
Iteration 56/1000 | Loss: 0.00016114
Iteration 57/1000 | Loss: 0.00029105
Iteration 58/1000 | Loss: 0.00014418
Iteration 59/1000 | Loss: 0.00013392
Iteration 60/1000 | Loss: 0.00014232
Iteration 61/1000 | Loss: 0.00013207
Iteration 62/1000 | Loss: 0.00077280
Iteration 63/1000 | Loss: 0.00055465
Iteration 64/1000 | Loss: 0.00078805
Iteration 65/1000 | Loss: 0.00058591
Iteration 66/1000 | Loss: 0.00051991
Iteration 67/1000 | Loss: 0.00050993
Iteration 68/1000 | Loss: 0.00049046
Iteration 69/1000 | Loss: 0.00049883
Iteration 70/1000 | Loss: 0.00013329
Iteration 71/1000 | Loss: 0.00013811
Iteration 72/1000 | Loss: 0.00013202
Iteration 73/1000 | Loss: 0.00012431
Iteration 74/1000 | Loss: 0.00012245
Iteration 75/1000 | Loss: 0.00014715
Iteration 76/1000 | Loss: 0.00012364
Iteration 77/1000 | Loss: 0.00012007
Iteration 78/1000 | Loss: 0.00069186
Iteration 79/1000 | Loss: 0.00014251
Iteration 80/1000 | Loss: 0.00068725
Iteration 81/1000 | Loss: 0.00018005
Iteration 82/1000 | Loss: 0.00013495
Iteration 83/1000 | Loss: 0.00025811
Iteration 84/1000 | Loss: 0.00012570
Iteration 85/1000 | Loss: 0.00012251
Iteration 86/1000 | Loss: 0.00011732
Iteration 87/1000 | Loss: 0.00011650
Iteration 88/1000 | Loss: 0.00011549
Iteration 89/1000 | Loss: 0.00012074
Iteration 90/1000 | Loss: 0.00011536
Iteration 91/1000 | Loss: 0.00011522
Iteration 92/1000 | Loss: 0.00012003
Iteration 93/1000 | Loss: 0.00011498
Iteration 94/1000 | Loss: 0.00011431
Iteration 95/1000 | Loss: 0.00012740
Iteration 96/1000 | Loss: 0.00011763
Iteration 97/1000 | Loss: 0.00064070
Iteration 98/1000 | Loss: 0.00103685
Iteration 99/1000 | Loss: 0.00022942
Iteration 100/1000 | Loss: 0.00013187
Iteration 101/1000 | Loss: 0.00013528
Iteration 102/1000 | Loss: 0.00014230
Iteration 103/1000 | Loss: 0.00012126
Iteration 104/1000 | Loss: 0.00011279
Iteration 105/1000 | Loss: 0.00011182
Iteration 106/1000 | Loss: 0.00010905
Iteration 107/1000 | Loss: 0.00010832
Iteration 108/1000 | Loss: 0.00010801
Iteration 109/1000 | Loss: 0.00011277
Iteration 110/1000 | Loss: 0.00010694
Iteration 111/1000 | Loss: 0.00011291
Iteration 112/1000 | Loss: 0.00012524
Iteration 113/1000 | Loss: 0.00012114
Iteration 114/1000 | Loss: 0.00011131
Iteration 115/1000 | Loss: 0.00010647
Iteration 116/1000 | Loss: 0.00010541
Iteration 117/1000 | Loss: 0.00012204
Iteration 118/1000 | Loss: 0.00010974
Iteration 119/1000 | Loss: 0.00010608
Iteration 120/1000 | Loss: 0.00012061
Iteration 121/1000 | Loss: 0.00011617
Iteration 122/1000 | Loss: 0.00010647
Iteration 123/1000 | Loss: 0.00010527
Iteration 124/1000 | Loss: 0.00011919
Iteration 125/1000 | Loss: 0.00011250
Iteration 126/1000 | Loss: 0.00012088
Iteration 127/1000 | Loss: 0.00011542
Iteration 128/1000 | Loss: 0.00013274
Iteration 129/1000 | Loss: 0.00011376
Iteration 130/1000 | Loss: 0.00010964
Iteration 131/1000 | Loss: 0.00012878
Iteration 132/1000 | Loss: 0.00011257
Iteration 133/1000 | Loss: 0.00012769
Iteration 134/1000 | Loss: 0.00012119
Iteration 135/1000 | Loss: 0.00012265
Iteration 136/1000 | Loss: 0.00011626
Iteration 137/1000 | Loss: 0.00012005
Iteration 138/1000 | Loss: 0.00011329
Iteration 139/1000 | Loss: 0.00011782
Iteration 140/1000 | Loss: 0.00011218
Iteration 141/1000 | Loss: 0.00011066
Iteration 142/1000 | Loss: 0.00067897
Iteration 143/1000 | Loss: 0.00023704
Iteration 144/1000 | Loss: 0.00017042
Iteration 145/1000 | Loss: 0.00052661
Iteration 146/1000 | Loss: 0.00019310
Iteration 147/1000 | Loss: 0.00013964
Iteration 148/1000 | Loss: 0.00012308
Iteration 149/1000 | Loss: 0.00010515
Iteration 150/1000 | Loss: 0.00010441
Iteration 151/1000 | Loss: 0.00010336
Iteration 152/1000 | Loss: 0.00010282
Iteration 153/1000 | Loss: 0.00010825
Iteration 154/1000 | Loss: 0.00010388
Iteration 155/1000 | Loss: 0.00010618
Iteration 156/1000 | Loss: 0.00010281
Iteration 157/1000 | Loss: 0.00010672
Iteration 158/1000 | Loss: 0.00010235
Iteration 159/1000 | Loss: 0.00010211
Iteration 160/1000 | Loss: 0.00010210
Iteration 161/1000 | Loss: 0.00010209
Iteration 162/1000 | Loss: 0.00010688
Iteration 163/1000 | Loss: 0.00013148
Iteration 164/1000 | Loss: 0.00011398
Iteration 165/1000 | Loss: 0.00010450
Iteration 166/1000 | Loss: 0.00011453
Iteration 167/1000 | Loss: 0.00012259
Iteration 168/1000 | Loss: 0.00011147
Iteration 169/1000 | Loss: 0.00012538
Iteration 170/1000 | Loss: 0.00015540
Iteration 171/1000 | Loss: 0.00010665
Iteration 172/1000 | Loss: 0.00010772
Iteration 173/1000 | Loss: 0.00010589
Iteration 174/1000 | Loss: 0.00010550
Iteration 175/1000 | Loss: 0.00010500
Iteration 176/1000 | Loss: 0.00011491
Iteration 177/1000 | Loss: 0.00010596
Iteration 178/1000 | Loss: 0.00011843
Iteration 179/1000 | Loss: 0.00010734
Iteration 180/1000 | Loss: 0.00011940
Iteration 181/1000 | Loss: 0.00010706
Iteration 182/1000 | Loss: 0.00011770
Iteration 183/1000 | Loss: 0.00010322
Iteration 184/1000 | Loss: 0.00010273
Iteration 185/1000 | Loss: 0.00010278
Iteration 186/1000 | Loss: 0.00010367
Iteration 187/1000 | Loss: 0.00010339
Iteration 188/1000 | Loss: 0.00011652
Iteration 189/1000 | Loss: 0.00010648
Iteration 190/1000 | Loss: 0.00011525
Iteration 191/1000 | Loss: 0.00011060
Iteration 192/1000 | Loss: 0.00011030
Iteration 193/1000 | Loss: 0.00011241
Iteration 194/1000 | Loss: 0.00010992
Iteration 195/1000 | Loss: 0.00011166
Iteration 196/1000 | Loss: 0.00011221
Iteration 197/1000 | Loss: 0.00016593
Iteration 198/1000 | Loss: 0.00011443
Iteration 199/1000 | Loss: 0.00011354
Iteration 200/1000 | Loss: 0.00010465
Iteration 201/1000 | Loss: 0.00010866
Iteration 202/1000 | Loss: 0.00010958
Iteration 203/1000 | Loss: 0.00011425
Iteration 204/1000 | Loss: 0.00011116
Iteration 205/1000 | Loss: 0.00010898
Iteration 206/1000 | Loss: 0.00011573
Iteration 207/1000 | Loss: 0.00011109
Iteration 208/1000 | Loss: 0.00011846
Iteration 209/1000 | Loss: 0.00011647
Iteration 210/1000 | Loss: 0.00011474
Iteration 211/1000 | Loss: 0.00013721
Iteration 212/1000 | Loss: 0.00011707
Iteration 213/1000 | Loss: 0.00011092
Iteration 214/1000 | Loss: 0.00010537
Iteration 215/1000 | Loss: 0.00011721
Iteration 216/1000 | Loss: 0.00010548
Iteration 217/1000 | Loss: 0.00011508
Iteration 218/1000 | Loss: 0.00010530
Iteration 219/1000 | Loss: 0.00010502
Iteration 220/1000 | Loss: 0.00011514
Iteration 221/1000 | Loss: 0.00010685
Iteration 222/1000 | Loss: 0.00011361
Iteration 223/1000 | Loss: 0.00010668
Iteration 224/1000 | Loss: 0.00011611
Iteration 225/1000 | Loss: 0.00010699
Iteration 226/1000 | Loss: 0.00011311
Iteration 227/1000 | Loss: 0.00010846
Iteration 228/1000 | Loss: 0.00011155
Iteration 229/1000 | Loss: 0.00010754
Iteration 230/1000 | Loss: 0.00011822
Iteration 231/1000 | Loss: 0.00010810
Iteration 232/1000 | Loss: 0.00012246
Iteration 233/1000 | Loss: 0.00011305
Iteration 234/1000 | Loss: 0.00012137
Iteration 235/1000 | Loss: 0.00011215
Iteration 236/1000 | Loss: 0.00011619
Iteration 237/1000 | Loss: 0.00011033
Iteration 238/1000 | Loss: 0.00011913
Iteration 239/1000 | Loss: 0.00011054
Iteration 240/1000 | Loss: 0.00011734
Iteration 241/1000 | Loss: 0.00010835
Iteration 242/1000 | Loss: 0.00012284
Iteration 243/1000 | Loss: 0.00011168
Iteration 244/1000 | Loss: 0.00011858
Iteration 245/1000 | Loss: 0.00011032
Iteration 246/1000 | Loss: 0.00011031
Iteration 247/1000 | Loss: 0.00011031
Iteration 248/1000 | Loss: 0.00011031
Iteration 249/1000 | Loss: 0.00011031
Iteration 250/1000 | Loss: 0.00011031
Iteration 251/1000 | Loss: 0.00011031
Iteration 252/1000 | Loss: 0.00010940
Iteration 253/1000 | Loss: 0.00011239
Iteration 254/1000 | Loss: 0.00012077
Iteration 255/1000 | Loss: 0.00011402
Iteration 256/1000 | Loss: 0.00010504
Iteration 257/1000 | Loss: 0.00011192
Iteration 258/1000 | Loss: 0.00011209
Iteration 259/1000 | Loss: 0.00010292
Iteration 260/1000 | Loss: 0.00010704
Iteration 261/1000 | Loss: 0.00011353
Iteration 262/1000 | Loss: 0.00011860
Iteration 263/1000 | Loss: 0.00010392
Iteration 264/1000 | Loss: 0.00010423
Iteration 265/1000 | Loss: 0.00010342
Iteration 266/1000 | Loss: 0.00011468
Iteration 267/1000 | Loss: 0.00011055
Iteration 268/1000 | Loss: 0.00011759
Iteration 269/1000 | Loss: 0.00011042
Iteration 270/1000 | Loss: 0.00011267
Iteration 271/1000 | Loss: 0.00011166
Iteration 272/1000 | Loss: 0.00011221
Iteration 273/1000 | Loss: 0.00011790
Iteration 274/1000 | Loss: 0.00011213
Iteration 275/1000 | Loss: 0.00010963
Iteration 276/1000 | Loss: 0.00010460
Iteration 277/1000 | Loss: 0.00010272
Iteration 278/1000 | Loss: 0.00012213
Iteration 279/1000 | Loss: 0.00011497
Iteration 280/1000 | Loss: 0.00011737
Iteration 281/1000 | Loss: 0.00012567
Iteration 282/1000 | Loss: 0.00010925
Iteration 283/1000 | Loss: 0.00010479
Iteration 284/1000 | Loss: 0.00011124
Iteration 285/1000 | Loss: 0.00010497
Iteration 286/1000 | Loss: 0.00011203
Iteration 287/1000 | Loss: 0.00010835
Iteration 288/1000 | Loss: 0.00011044
Iteration 289/1000 | Loss: 0.00011467
Iteration 290/1000 | Loss: 0.00011185
Iteration 291/1000 | Loss: 0.00011172
Iteration 292/1000 | Loss: 0.00011396
Iteration 293/1000 | Loss: 0.00011087
Iteration 294/1000 | Loss: 0.00011222
Iteration 295/1000 | Loss: 0.00010929
Iteration 296/1000 | Loss: 0.00010578
Iteration 297/1000 | Loss: 0.00011026
Iteration 298/1000 | Loss: 0.00011000
Iteration 299/1000 | Loss: 0.00011083
Iteration 300/1000 | Loss: 0.00011369
Iteration 301/1000 | Loss: 0.00010900
Iteration 302/1000 | Loss: 0.00011040
Iteration 303/1000 | Loss: 0.00011096
Iteration 304/1000 | Loss: 0.00010647
Iteration 305/1000 | Loss: 0.00011767
Iteration 306/1000 | Loss: 0.00010748
Iteration 307/1000 | Loss: 0.00011368
Iteration 308/1000 | Loss: 0.00011066
Iteration 309/1000 | Loss: 0.00011841
Iteration 310/1000 | Loss: 0.00010861
Iteration 311/1000 | Loss: 0.00011471
Iteration 312/1000 | Loss: 0.00010794
Iteration 313/1000 | Loss: 0.00011369
Iteration 314/1000 | Loss: 0.00011214
Iteration 315/1000 | Loss: 0.00010353
Iteration 316/1000 | Loss: 0.00012738
Iteration 317/1000 | Loss: 0.00010950
Iteration 318/1000 | Loss: 0.00011939
Iteration 319/1000 | Loss: 0.00010841
Iteration 320/1000 | Loss: 0.00011809
Iteration 321/1000 | Loss: 0.00010760
Iteration 322/1000 | Loss: 0.00010363
Iteration 323/1000 | Loss: 0.00010486
Iteration 324/1000 | Loss: 0.00011764
Iteration 325/1000 | Loss: 0.00010413
Iteration 326/1000 | Loss: 0.00010355
Iteration 327/1000 | Loss: 0.00010701
Iteration 328/1000 | Loss: 0.00010853
Iteration 329/1000 | Loss: 0.00010672
Iteration 330/1000 | Loss: 0.00010462
Iteration 331/1000 | Loss: 0.00011236
Iteration 332/1000 | Loss: 0.00011140
Iteration 333/1000 | Loss: 0.00011328
Iteration 334/1000 | Loss: 0.00011614
Iteration 335/1000 | Loss: 0.00011156
Iteration 336/1000 | Loss: 0.00011043
Iteration 337/1000 | Loss: 0.00011278
Iteration 338/1000 | Loss: 0.00011150
Iteration 339/1000 | Loss: 0.00011333
Iteration 340/1000 | Loss: 0.00010892
Iteration 341/1000 | Loss: 0.00011678
Iteration 342/1000 | Loss: 0.00010814
Iteration 343/1000 | Loss: 0.00010324
Iteration 344/1000 | Loss: 0.00010268
Iteration 345/1000 | Loss: 0.00011974
Iteration 346/1000 | Loss: 0.00010359
Iteration 347/1000 | Loss: 0.00010308
Iteration 348/1000 | Loss: 0.00012045
Iteration 349/1000 | Loss: 0.00010293
Iteration 350/1000 | Loss: 0.00010279
Iteration 351/1000 | Loss: 0.00010278
Iteration 352/1000 | Loss: 0.00010278
Iteration 353/1000 | Loss: 0.00012121
Iteration 354/1000 | Loss: 0.00010322
Iteration 355/1000 | Loss: 0.00010275
Iteration 356/1000 | Loss: 0.00012485
Iteration 357/1000 | Loss: 0.00012349
Iteration 358/1000 | Loss: 0.00013177
Iteration 359/1000 | Loss: 0.00011474
Iteration 360/1000 | Loss: 0.00011421
Iteration 361/1000 | Loss: 0.00011043
Iteration 362/1000 | Loss: 0.00011206
Iteration 363/1000 | Loss: 0.00012801
Iteration 364/1000 | Loss: 0.00011262
Iteration 365/1000 | Loss: 0.00011700
Iteration 366/1000 | Loss: 0.00012130
Iteration 367/1000 | Loss: 0.00011036
Iteration 368/1000 | Loss: 0.00010475
Iteration 369/1000 | Loss: 0.00011821
Iteration 370/1000 | Loss: 0.00010819
Iteration 371/1000 | Loss: 0.00011645
Iteration 372/1000 | Loss: 0.00011296
Iteration 373/1000 | Loss: 0.00011883
Iteration 374/1000 | Loss: 0.00011286
Iteration 375/1000 | Loss: 0.00011596
Iteration 376/1000 | Loss: 0.00011865
Iteration 377/1000 | Loss: 0.00011764
Iteration 378/1000 | Loss: 0.00011974
Iteration 379/1000 | Loss: 0.00011426
Iteration 380/1000 | Loss: 0.00012434
Iteration 381/1000 | Loss: 0.00011282
Iteration 382/1000 | Loss: 0.00011645
Iteration 383/1000 | Loss: 0.00012379
Iteration 384/1000 | Loss: 0.00011500
Iteration 385/1000 | Loss: 0.00010481
Iteration 386/1000 | Loss: 0.00010370
Iteration 387/1000 | Loss: 0.00011482
Iteration 388/1000 | Loss: 0.00010352
Iteration 389/1000 | Loss: 0.00012189
Iteration 390/1000 | Loss: 0.00010338
Iteration 391/1000 | Loss: 0.00010256
Iteration 392/1000 | Loss: 0.00010197
Iteration 393/1000 | Loss: 0.00012425
Iteration 394/1000 | Loss: 0.00010357
Iteration 395/1000 | Loss: 0.00010311
Iteration 396/1000 | Loss: 0.00012326
Iteration 397/1000 | Loss: 0.00012408
Iteration 398/1000 | Loss: 0.00012746
Iteration 399/1000 | Loss: 0.00011549
Iteration 400/1000 | Loss: 0.00011628
Iteration 401/1000 | Loss: 0.00011110
Iteration 402/1000 | Loss: 0.00011220
Iteration 403/1000 | Loss: 0.00013180
Iteration 404/1000 | Loss: 0.00010701
Iteration 405/1000 | Loss: 0.00011837
Iteration 406/1000 | Loss: 0.00011746
Iteration 407/1000 | Loss: 0.00011764
Iteration 408/1000 | Loss: 0.00011448
Iteration 409/1000 | Loss: 0.00011519
Iteration 410/1000 | Loss: 0.00011377
Iteration 411/1000 | Loss: 0.00012353
Iteration 412/1000 | Loss: 0.00011300
Iteration 413/1000 | Loss: 0.00012100
Iteration 414/1000 | Loss: 0.00011235
Iteration 415/1000 | Loss: 0.00011235
Iteration 416/1000 | Loss: 0.00011235
Iteration 417/1000 | Loss: 0.00011235
Iteration 418/1000 | Loss: 0.00011235
Iteration 419/1000 | Loss: 0.00011777
Iteration 420/1000 | Loss: 0.00011217
Iteration 421/1000 | Loss: 0.00011184
Iteration 422/1000 | Loss: 0.00011180
Iteration 423/1000 | Loss: 0.00011422
Iteration 424/1000 | Loss: 0.00012258
Iteration 425/1000 | Loss: 0.00011319
Iteration 426/1000 | Loss: 0.00012372
Iteration 427/1000 | Loss: 0.00011900
Iteration 428/1000 | Loss: 0.00012000
Iteration 429/1000 | Loss: 0.00011770
Iteration 430/1000 | Loss: 0.00011889
Iteration 431/1000 | Loss: 0.00013168
Iteration 432/1000 | Loss: 0.00011564
Iteration 433/1000 | Loss: 0.00013085
Iteration 434/1000 | Loss: 0.00011588
Iteration 435/1000 | Loss: 0.00010455
Iteration 436/1000 | Loss: 0.00010739
Iteration 437/1000 | Loss: 0.00011647
Iteration 438/1000 | Loss: 0.00010430
Iteration 439/1000 | Loss: 0.00010289
Iteration 440/1000 | Loss: 0.00011993
Iteration 441/1000 | Loss: 0.00010379
Iteration 442/1000 | Loss: 0.00010328
Iteration 443/1000 | Loss: 0.00010545
Iteration 444/1000 | Loss: 0.00010232
Iteration 445/1000 | Loss: 0.00010775
Iteration 446/1000 | Loss: 0.00010366
Iteration 447/1000 | Loss: 0.00011081
Iteration 448/1000 | Loss: 0.00010408
Iteration 449/1000 | Loss: 0.00010329
Iteration 450/1000 | Loss: 0.00011563
Iteration 451/1000 | Loss: 0.00010358
Iteration 452/1000 | Loss: 0.00010756
Iteration 453/1000 | Loss: 0.00011026
Iteration 454/1000 | Loss: 0.00011830
Iteration 455/1000 | Loss: 0.00010661
Iteration 456/1000 | Loss: 0.00011337
Iteration 457/1000 | Loss: 0.00011577
Iteration 458/1000 | Loss: 0.00011439
Iteration 459/1000 | Loss: 0.00011952
Iteration 460/1000 | Loss: 0.00010234
Iteration 461/1000 | Loss: 0.00012363
Iteration 462/1000 | Loss: 0.00010358
Iteration 463/1000 | Loss: 0.00011038
Iteration 464/1000 | Loss: 0.00010281
Iteration 465/1000 | Loss: 0.00011026
Iteration 466/1000 | Loss: 0.00010684
Iteration 467/1000 | Loss: 0.00010288
Iteration 468/1000 | Loss: 0.00011046
Iteration 469/1000 | Loss: 0.00010327
Iteration 470/1000 | Loss: 0.00012671
Iteration 471/1000 | Loss: 0.00010279
Iteration 472/1000 | Loss: 0.00010963
Iteration 473/1000 | Loss: 0.00011160
Iteration 474/1000 | Loss: 0.00010388
Iteration 475/1000 | Loss: 0.00010830
Iteration 476/1000 | Loss: 0.00012708
Iteration 477/1000 | Loss: 0.00010435
Iteration 478/1000 | Loss: 0.00011457
Iteration 479/1000 | Loss: 0.00010370
Iteration 480/1000 | Loss: 0.00010341
Iteration 481/1000 | Loss: 0.00010400
Iteration 482/1000 | Loss: 0.00010672
Iteration 483/1000 | Loss: 0.00011306
Iteration 484/1000 | Loss: 0.00010441
Iteration 485/1000 | Loss: 0.00010299
Iteration 486/1000 | Loss: 0.00010243
Iteration 487/1000 | Loss: 0.00012572
Iteration 488/1000 | Loss: 0.00010988
Iteration 489/1000 | Loss: 0.00011340
Iteration 490/1000 | Loss: 0.00010790
Iteration 491/1000 | Loss: 0.00010798
Iteration 492/1000 | Loss: 0.00010672
Iteration 493/1000 | Loss: 0.00010977
Iteration 494/1000 | Loss: 0.00012121
Iteration 495/1000 | Loss: 0.00011463
Iteration 496/1000 | Loss: 0.00011820
Iteration 497/1000 | Loss: 0.00011997
Iteration 498/1000 | Loss: 0.00010601
Iteration 499/1000 | Loss: 0.00011722
Iteration 500/1000 | Loss: 0.00011588
Iteration 501/1000 | Loss: 0.00011594
Iteration 502/1000 | Loss: 0.00011352
Iteration 503/1000 | Loss: 0.00011827
Iteration 504/1000 | Loss: 0.00011131
Iteration 505/1000 | Loss: 0.00010708
Iteration 506/1000 | Loss: 0.00010779
Iteration 507/1000 | Loss: 0.00010944
Iteration 508/1000 | Loss: 0.00010245
Iteration 509/1000 | Loss: 0.00011511
Iteration 510/1000 | Loss: 0.00010302
Iteration 511/1000 | Loss: 0.00011054
Iteration 512/1000 | Loss: 0.00011599
Iteration 513/1000 | Loss: 0.00010896
Iteration 514/1000 | Loss: 0.00012182
Iteration 515/1000 | Loss: 0.00010643
Iteration 516/1000 | Loss: 0.00011486
Iteration 517/1000 | Loss: 0.00011292
Iteration 518/1000 | Loss: 0.00012104
Iteration 519/1000 | Loss: 0.00011753
Iteration 520/1000 | Loss: 0.00011204
Iteration 521/1000 | Loss: 0.00011682
Iteration 522/1000 | Loss: 0.00011604
Iteration 523/1000 | Loss: 0.00011126
Iteration 524/1000 | Loss: 0.00011296
Iteration 525/1000 | Loss: 0.00011234
Iteration 526/1000 | Loss: 0.00011783
Iteration 527/1000 | Loss: 0.00011435
Iteration 528/1000 | Loss: 0.00011384
Iteration 529/1000 | Loss: 0.00011540
Iteration 530/1000 | Loss: 0.00011497
Iteration 531/1000 | Loss: 0.00011400
Iteration 532/1000 | Loss: 0.00011689
Iteration 533/1000 | Loss: 0.00010776
Iteration 534/1000 | Loss: 0.00010679
Iteration 535/1000 | Loss: 0.00010977
Iteration 536/1000 | Loss: 0.00010771
Iteration 537/1000 | Loss: 0.00011539
Iteration 538/1000 | Loss: 0.00011264
Iteration 539/1000 | Loss: 0.00011544
Iteration 540/1000 | Loss: 0.00010946
Iteration 541/1000 | Loss: 0.00011464
Iteration 542/1000 | Loss: 0.00010927
Iteration 543/1000 | Loss: 0.00012284
Iteration 544/1000 | Loss: 0.00010920
Iteration 545/1000 | Loss: 0.00011477
Iteration 546/1000 | Loss: 0.00011567
Iteration 547/1000 | Loss: 0.00011301
Iteration 548/1000 | Loss: 0.00011167
Iteration 549/1000 | Loss: 0.00010937
Iteration 550/1000 | Loss: 0.00011418
Iteration 551/1000 | Loss: 0.00011119
Iteration 552/1000 | Loss: 0.00011626
Iteration 553/1000 | Loss: 0.00011816
Iteration 554/1000 | Loss: 0.00011409
Iteration 555/1000 | Loss: 0.00010583
Iteration 556/1000 | Loss: 0.00011799
Iteration 557/1000 | Loss: 0.00010495
Iteration 558/1000 | Loss: 0.00010344
Iteration 559/1000 | Loss: 0.00010266
Iteration 560/1000 | Loss: 0.00010877
Iteration 561/1000 | Loss: 0.00010255
Iteration 562/1000 | Loss: 0.00010471
Iteration 563/1000 | Loss: 0.00010243
Iteration 564/1000 | Loss: 0.00011592
Iteration 565/1000 | Loss: 0.00010418
Iteration 566/1000 | Loss: 0.00010305
Iteration 567/1000 | Loss: 0.00010466
Iteration 568/1000 | Loss: 0.00010212
Iteration 569/1000 | Loss: 0.00011308
Iteration 570/1000 | Loss: 0.00010569
Iteration 571/1000 | Loss: 0.00011453
Iteration 572/1000 | Loss: 0.00010627
Iteration 573/1000 | Loss: 0.00011409
Iteration 574/1000 | Loss: 0.00010570
Iteration 575/1000 | Loss: 0.00010979
Iteration 576/1000 | Loss: 0.00010649
Iteration 577/1000 | Loss: 0.00010953
Iteration 578/1000 | Loss: 0.00010790
Iteration 579/1000 | Loss: 0.00010581
Iteration 580/1000 | Loss: 0.00010616
Iteration 581/1000 | Loss: 0.00011496
Iteration 582/1000 | Loss: 0.00010648
Iteration 583/1000 | Loss: 0.00011046
Iteration 584/1000 | Loss: 0.00010921
Iteration 585/1000 | Loss: 0.00010293
Iteration 586/1000 | Loss: 0.00011642
Iteration 587/1000 | Loss: 0.00010352
Iteration 588/1000 | Loss: 0.00011204
Iteration 589/1000 | Loss: 0.00010521
Iteration 590/1000 | Loss: 0.00010680
Iteration 591/1000 | Loss: 0.00010273
Iteration 592/1000 | Loss: 0.00011155
Iteration 593/1000 | Loss: 0.00010634
Iteration 594/1000 | Loss: 0.00011133
Iteration 595/1000 | Loss: 0.00010523
Iteration 596/1000 | Loss: 0.00010457
Iteration 597/1000 | Loss: 0.00010780
Iteration 598/1000 | Loss: 0.00010504
Iteration 599/1000 | Loss: 0.00010424
Iteration 600/1000 | Loss: 0.00011976
Iteration 601/1000 | Loss: 0.00011203
Iteration 602/1000 | Loss: 0.00012103
Iteration 603/1000 | Loss: 0.00011511
Iteration 604/1000 | Loss: 0.00011842
Iteration 605/1000 | Loss: 0.00012029
Iteration 606/1000 | Loss: 0.00011555
Iteration 607/1000 | Loss: 0.00012382
Iteration 608/1000 | Loss: 0.00010678
Iteration 609/1000 | Loss: 0.00010658
Iteration 610/1000 | Loss: 0.00010237
Iteration 611/1000 | Loss: 0.00010873
Iteration 612/1000 | Loss: 0.00010905
Iteration 613/1000 | Loss: 0.00011183
Iteration 614/1000 | Loss: 0.00010371
Iteration 615/1000 | Loss: 0.00012535
Iteration 616/1000 | Loss: 0.00010494
Iteration 617/1000 | Loss: 0.00011964
Iteration 618/1000 | Loss: 0.00010543
Iteration 619/1000 | Loss: 0.00011610
Iteration 620/1000 | Loss: 0.00010716
Iteration 621/1000 | Loss: 0.00011037
Iteration 622/1000 | Loss: 0.00010426
Iteration 623/1000 | Loss: 0.00010895
Iteration 624/1000 | Loss: 0.00010525
Iteration 625/1000 | Loss: 0.00011766
Iteration 626/1000 | Loss: 0.00010981
Iteration 627/1000 | Loss: 0.00010742
Iteration 628/1000 | Loss: 0.00010603
Iteration 629/1000 | Loss: 0.00012220
Iteration 630/1000 | Loss: 0.00010853
Iteration 631/1000 | Loss: 0.00011070
Iteration 632/1000 | Loss: 0.00010583
Iteration 633/1000 | Loss: 0.00012977
Iteration 634/1000 | Loss: 0.00011090
Iteration 635/1000 | Loss: 0.00012860
Iteration 636/1000 | Loss: 0.00010507
Iteration 637/1000 | Loss: 0.00012694
Iteration 638/1000 | Loss: 0.00013199
Iteration 639/1000 | Loss: 0.00012540
Iteration 640/1000 | Loss: 0.00012253
Iteration 641/1000 | Loss: 0.00013222
Iteration 642/1000 | Loss: 0.00011959
Iteration 643/1000 | Loss: 0.00011942
Iteration 644/1000 | Loss: 0.00011542
Iteration 645/1000 | Loss: 0.00011829
Iteration 646/1000 | Loss: 0.00011691
Iteration 647/1000 | Loss: 0.00011671
Iteration 648/1000 | Loss: 0.00011542
Iteration 649/1000 | Loss: 0.00011601
Iteration 650/1000 | Loss: 0.00011473
Iteration 651/1000 | Loss: 0.00011377
Iteration 652/1000 | Loss: 0.00011398
Iteration 653/1000 | Loss: 0.00011337
Iteration 654/1000 | Loss: 0.00011350
Iteration 655/1000 | Loss: 0.00011302
Iteration 656/1000 | Loss: 0.00011328
Iteration 657/1000 | Loss: 0.00011286
Iteration 658/1000 | Loss: 0.00011091
Iteration 659/1000 | Loss: 0.00011195
Iteration 660/1000 | Loss: 0.00010655
Iteration 661/1000 | Loss: 0.00010236
Iteration 662/1000 | Loss: 0.00011059
Iteration 663/1000 | Loss: 0.00010355
Iteration 664/1000 | Loss: 0.00010584
Iteration 665/1000 | Loss: 0.00010373
Iteration 666/1000 | Loss: 0.00011637
Iteration 667/1000 | Loss: 0.00010837
Iteration 668/1000 | Loss: 0.00010502
Iteration 669/1000 | Loss: 0.00011500
Iteration 670/1000 | Loss: 0.00011649
Iteration 671/1000 | Loss: 0.00011707
Iteration 672/1000 | Loss: 0.00011409
Iteration 673/1000 | Loss: 0.00011544
Iteration 674/1000 | Loss: 0.00011496
Iteration 675/1000 | Loss: 0.00011150
Iteration 676/1000 | Loss: 0.00011583
Iteration 677/1000 | Loss: 0.00011249
Iteration 678/1000 | Loss: 0.00011296
Iteration 679/1000 | Loss: 0.00011221
Iteration 680/1000 | Loss: 0.00011513
Iteration 681/1000 | Loss: 0.00011570
Iteration 682/1000 | Loss: 0.00011730
Iteration 683/1000 | Loss: 0.00011290
Iteration 684/1000 | Loss: 0.00012279
Iteration 685/1000 | Loss: 0.00011216
Iteration 686/1000 | Loss: 0.00011780
Iteration 687/1000 | Loss: 0.00011145
Iteration 688/1000 | Loss: 0.00012031
Iteration 689/1000 | Loss: 0.00011109
Iteration 690/1000 | Loss: 0.00011109
Iteration 691/1000 | Loss: 0.00011108
Iteration 692/1000 | Loss: 0.00011108
Iteration 693/1000 | Loss: 0.00012513
Iteration 694/1000 | Loss: 0.00011046
Iteration 695/1000 | Loss: 0.00012155
Iteration 696/1000 | Loss: 0.00011017
Iteration 697/1000 | Loss: 0.00010548
Iteration 698/1000 | Loss: 0.00011564
Iteration 699/1000 | Loss: 0.00010928
Iteration 700/1000 | Loss: 0.00011119
Iteration 701/1000 | Loss: 0.00010990
Iteration 702/1000 | Loss: 0.00011084
Iteration 703/1000 | Loss: 0.00010879
Iteration 704/1000 | Loss: 0.00010966
Iteration 705/1000 | Loss: 0.00010916
Iteration 706/1000 | Loss: 0.00011018
Iteration 707/1000 | Loss: 0.00010849
Iteration 708/1000 | Loss: 0.00011224
Iteration 709/1000 | Loss: 0.00011073
Iteration 710/1000 | Loss: 0.00011186
Iteration 711/1000 | Loss: 0.00010988
Iteration 712/1000 | Loss: 0.00011535
Iteration 713/1000 | Loss: 0.00011169
Iteration 714/1000 | Loss: 0.00011517
Iteration 715/1000 | Loss: 0.00011345
Iteration 716/1000 | Loss: 0.00011310
Iteration 717/1000 | Loss: 0.00011366
Iteration 718/1000 | Loss: 0.00010573
Iteration 719/1000 | Loss: 0.00010391
Iteration 720/1000 | Loss: 0.00010944
Iteration 721/1000 | Loss: 0.00010473
Iteration 722/1000 | Loss: 0.00010334
Iteration 723/1000 | Loss: 0.00010909
Iteration 724/1000 | Loss: 0.00011289
Iteration 725/1000 | Loss: 0.00011302
Iteration 726/1000 | Loss: 0.00011160
Iteration 727/1000 | Loss: 0.00011082
Iteration 728/1000 | Loss: 0.00012596
Iteration 729/1000 | Loss: 0.00011622
Iteration 730/1000 | Loss: 0.00012182
Iteration 731/1000 | Loss: 0.00011472
Iteration 732/1000 | Loss: 0.00012302
Iteration 733/1000 | Loss: 0.00010693
Iteration 734/1000 | Loss: 0.00010454
Iteration 735/1000 | Loss: 0.00011766
Iteration 736/1000 | Loss: 0.00011178
Iteration 737/1000 | Loss: 0.00010944
Iteration 738/1000 | Loss: 0.00012276
Iteration 739/1000 | Loss: 0.00010935
Iteration 740/1000 | Loss: 0.00011628
Iteration 741/1000 | Loss: 0.00011202
Iteration 742/1000 | Loss: 0.00011307
Iteration 743/1000 | Loss: 0.00011111
Iteration 744/1000 | Loss: 0.00012447
Iteration 745/1000 | Loss: 0.00010911
Iteration 746/1000 | Loss: 0.00011420
Iteration 747/1000 | Loss: 0.00011437
Iteration 748/1000 | Loss: 0.00012192
Iteration 749/1000 | Loss: 0.00011223
Iteration 750/1000 | Loss: 0.00011288
Iteration 751/1000 | Loss: 0.00011011
Iteration 752/1000 | Loss: 0.00011579
Iteration 753/1000 | Loss: 0.00010950
Iteration 754/1000 | Loss: 0.00012841
Iteration 755/1000 | Loss: 0.00010846
Iteration 756/1000 | Loss: 0.00011734
Iteration 757/1000 | Loss: 0.00010946
Iteration 758/1000 | Loss: 0.00011267
Iteration 759/1000 | Loss: 0.00010824
Iteration 760/1000 | Loss: 0.00011083
Iteration 761/1000 | Loss: 0.00010833
Iteration 762/1000 | Loss: 0.00011174
Iteration 763/1000 | Loss: 0.00010880
Iteration 764/1000 | Loss: 0.00011762
Iteration 765/1000 | Loss: 0.00010873
Iteration 766/1000 | Loss: 0.00011605
Iteration 767/1000 | Loss: 0.00010767
Iteration 768/1000 | Loss: 0.00011989
Iteration 769/1000 | Loss: 0.00010654
Iteration 770/1000 | Loss: 0.00012010
Iteration 771/1000 | Loss: 0.00010698
Iteration 772/1000 | Loss: 0.00011416
Iteration 773/1000 | Loss: 0.00010864
Iteration 774/1000 | Loss: 0.00011839
Iteration 775/1000 | Loss: 0.00010874
Iteration 776/1000 | Loss: 0.00011711
Iteration 777/1000 | Loss: 0.00010860
Iteration 778/1000 | Loss: 0.00011619
Iteration 779/1000 | Loss: 0.00010659
Iteration 780/1000 | Loss: 0.00011007
Iteration 781/1000 | Loss: 0.00010610
Iteration 782/1000 | Loss: 0.00010563
Iteration 783/1000 | Loss: 0.00010546
Iteration 784/1000 | Loss: 0.00010928
Iteration 785/1000 | Loss: 0.00010602
Iteration 786/1000 | Loss: 0.00011476
Iteration 787/1000 | Loss: 0.00010896
Iteration 788/1000 | Loss: 0.00011943
Iteration 789/1000 | Loss: 0.00010750
Iteration 790/1000 | Loss: 0.00011603
Iteration 791/1000 | Loss: 0.00010648
Iteration 792/1000 | Loss: 0.00011309
Iteration 793/1000 | Loss: 0.00010590
Iteration 794/1000 | Loss: 0.00011015
Iteration 795/1000 | Loss: 0.00010566
Iteration 796/1000 | Loss: 0.00010755
Iteration 797/1000 | Loss: 0.00010378
Iteration 798/1000 | Loss: 0.00010311
Iteration 799/1000 | Loss: 0.00010341
Iteration 800/1000 | Loss: 0.00010371
Iteration 801/1000 | Loss: 0.00010356
Iteration 802/1000 | Loss: 0.00010869
Iteration 803/1000 | Loss: 0.00010598
Iteration 804/1000 | Loss: 0.00010360
Iteration 805/1000 | Loss: 0.00010293
Iteration 806/1000 | Loss: 0.00011123
Iteration 807/1000 | Loss: 0.00011151
Iteration 808/1000 | Loss: 0.00011501
Iteration 809/1000 | Loss: 0.00010613
Iteration 810/1000 | Loss: 0.00011061
Iteration 811/1000 | Loss: 0.00010682
Iteration 812/1000 | Loss: 0.00011649
Iteration 813/1000 | Loss: 0.00010623
Iteration 814/1000 | Loss: 0.00011493
Iteration 815/1000 | Loss: 0.00010572
Iteration 816/1000 | Loss: 0.00012436
Iteration 817/1000 | Loss: 0.00010428
Iteration 818/1000 | Loss: 0.00010241
Iteration 819/1000 | Loss: 0.00010759
Iteration 820/1000 | Loss: 0.00010468
Iteration 821/1000 | Loss: 0.00010830
Iteration 822/1000 | Loss: 0.00010731
Iteration 823/1000 | Loss: 0.00010950
Iteration 824/1000 | Loss: 0.00011551
Iteration 825/1000 | Loss: 0.00010969
Iteration 826/1000 | Loss: 0.00011705
Iteration 827/1000 | Loss: 0.00010961
Iteration 828/1000 | Loss: 0.00010353
Iteration 829/1000 | Loss: 0.00010504
Iteration 830/1000 | Loss: 0.00010643
Iteration 831/1000 | Loss: 0.00011377
Iteration 832/1000 | Loss: 0.00010459
Iteration 833/1000 | Loss: 0.00010403
Iteration 834/1000 | Loss: 0.00010651
Iteration 835/1000 | Loss: 0.00010671
Iteration 836/1000 | Loss: 0.00010694
Iteration 837/1000 | Loss: 0.00011447
Iteration 838/1000 | Loss: 0.00011080
Iteration 839/1000 | Loss: 0.00010562
Iteration 840/1000 | Loss: 0.00010219
Iteration 841/1000 | Loss: 0.00010215
Iteration 842/1000 | Loss: 0.00010211
Iteration 843/1000 | Loss: 0.00010210
Iteration 844/1000 | Loss: 0.00010429
Iteration 845/1000 | Loss: 0.00010655
Iteration 846/1000 | Loss: 0.00010290
Iteration 847/1000 | Loss: 0.00010852
Iteration 848/1000 | Loss: 0.00011236
Iteration 849/1000 | Loss: 0.00011007
Iteration 850/1000 | Loss: 0.00011004
Iteration 851/1000 | Loss: 0.00010802
Iteration 852/1000 | Loss: 0.00011652
Iteration 853/1000 | Loss: 0.00010804
Iteration 854/1000 | Loss: 0.00010803
Iteration 855/1000 | Loss: 0.00012035
Iteration 856/1000 | Loss: 0.00010802
Iteration 857/1000 | Loss: 0.00010843
Iteration 858/1000 | Loss: 0.00010692
Iteration 859/1000 | Loss: 0.00010593
Iteration 860/1000 | Loss: 0.00011100
Iteration 861/1000 | Loss: 0.00010787
Iteration 862/1000 | Loss: 0.00010800
Iteration 863/1000 | Loss: 0.00010737
Iteration 864/1000 | Loss: 0.00011471
Iteration 865/1000 | Loss: 0.00010771
Iteration 866/1000 | Loss: 0.00011157
Iteration 867/1000 | Loss: 0.00011987
Iteration 868/1000 | Loss: 0.00010883
Iteration 869/1000 | Loss: 0.00010891
Iteration 870/1000 | Loss: 0.00010403
Iteration 871/1000 | Loss: 0.00010757
Iteration 872/1000 | Loss: 0.00010439
Iteration 873/1000 | Loss: 0.00010742
Iteration 874/1000 | Loss: 0.00010590
Iteration 875/1000 | Loss: 0.00010825
Iteration 876/1000 | Loss: 0.00010303
Iteration 877/1000 | Loss: 0.00010232
Iteration 878/1000 | Loss: 0.00010708
Iteration 879/1000 | Loss: 0.00010774
Iteration 880/1000 | Loss: 0.00011489
Iteration 881/1000 | Loss: 0.00010344
Iteration 882/1000 | Loss: 0.00010561
Iteration 883/1000 | Loss: 0.00011455
Iteration 884/1000 | Loss: 0.00010801
Iteration 885/1000 | Loss: 0.00011714
Iteration 886/1000 | Loss: 0.00011225
Iteration 887/1000 | Loss: 0.00011763
Iteration 888/1000 | Loss: 0.00010858
Iteration 889/1000 | Loss: 0.00013424
Iteration 890/1000 | Loss: 0.00011036
Iteration 891/1000 | Loss: 0.00010603
Iteration 892/1000 | Loss: 0.00011164
Iteration 893/1000 | Loss: 0.00010791
Iteration 894/1000 | Loss: 0.00010374
Iteration 895/1000 | Loss: 0.00011047
Iteration 896/1000 | Loss: 0.00010597
Iteration 897/1000 | Loss: 0.00010729
Iteration 898/1000 | Loss: 0.00010713
Iteration 899/1000 | Loss: 0.00010621
Iteration 900/1000 | Loss: 0.00010330
Iteration 901/1000 | Loss: 0.00010433
Iteration 902/1000 | Loss: 0.00010366
Iteration 903/1000 | Loss: 0.00011227
Iteration 904/1000 | Loss: 0.00011464
Iteration 905/1000 | Loss: 0.00011255
Iteration 906/1000 | Loss: 0.00011484
Iteration 907/1000 | Loss: 0.00010958
Iteration 908/1000 | Loss: 0.00010639
Iteration 909/1000 | Loss: 0.00010482
Iteration 910/1000 | Loss: 0.00010559
Iteration 911/1000 | Loss: 0.00010468
Iteration 912/1000 | Loss: 0.00010738
Iteration 913/1000 | Loss: 0.00010449
Iteration 914/1000 | Loss: 0.00011660
Iteration 915/1000 | Loss: 0.00010475
Iteration 916/1000 | Loss: 0.00011304
Iteration 917/1000 | Loss: 0.00010657
Iteration 918/1000 | Loss: 0.00011108
Iteration 919/1000 | Loss: 0.00011118
Iteration 920/1000 | Loss: 0.00011056
Iteration 921/1000 | Loss: 0.00010459
Iteration 922/1000 | Loss: 0.00010320
Iteration 923/1000 | Loss: 0.00010581
Iteration 924/1000 | Loss: 0.00010287
Iteration 925/1000 | Loss: 0.00010229
Iteration 926/1000 | Loss: 0.00011182
Iteration 927/1000 | Loss: 0.00010573
Iteration 928/1000 | Loss: 0.00011133
Iteration 929/1000 | Loss: 0.00010764
Iteration 930/1000 | Loss: 0.00010776
Iteration 931/1000 | Loss: 0.00010908
Iteration 932/1000 | Loss: 0.00010807
Iteration 933/1000 | Loss: 0.00011264
Iteration 934/1000 | Loss: 0.00011380
Iteration 935/1000 | Loss: 0.00011872
Iteration 936/1000 | Loss: 0.00011651
Iteration 937/1000 | Loss: 0.00011446
Iteration 938/1000 | Loss: 0.00011336
Iteration 939/1000 | Loss: 0.00011562
Iteration 940/1000 | Loss: 0.00011508
Iteration 941/1000 | Loss: 0.00011418
Iteration 942/1000 | Loss: 0.00011399
Iteration 943/1000 | Loss: 0.00011248
Iteration 944/1000 | Loss: 0.00011300
Iteration 945/1000 | Loss: 0.00011091
Iteration 946/1000 | Loss: 0.00011187
Iteration 947/1000 | Loss: 0.00010694
Iteration 948/1000 | Loss: 0.00010345
Iteration 949/1000 | Loss: 0.00010277
Iteration 950/1000 | Loss: 0.00010230
Iteration 951/1000 | Loss: 0.00010313
Iteration 952/1000 | Loss: 0.00011376
Iteration 953/1000 | Loss: 0.00010532
Iteration 954/1000 | Loss: 0.00011447
Iteration 955/1000 | Loss: 0.00010958
Iteration 956/1000 | Loss: 0.00010620
Iteration 957/1000 | Loss: 0.00010618
Iteration 958/1000 | Loss: 0.00010552
Iteration 959/1000 | Loss: 0.00011111
Iteration 960/1000 | Loss: 0.00010538
Iteration 961/1000 | Loss: 0.00010936
Iteration 962/1000 | Loss: 0.00010771
Iteration 963/1000 | Loss: 0.00011697
Iteration 964/1000 | Loss: 0.00011211
Iteration 965/1000 | Loss: 0.00012075
Iteration 966/1000 | Loss: 0.00010560
Iteration 967/1000 | Loss: 0.00010337
Iteration 968/1000 | Loss: 0.00010267
Iteration 969/1000 | Loss: 0.00010959
Iteration 970/1000 | Loss: 0.00010802
Iteration 971/1000 | Loss: 0.00011143
Iteration 972/1000 | Loss: 0.00011478
Iteration 973/1000 | Loss: 0.00011359
Iteration 974/1000 | Loss: 0.00011193
Iteration 975/1000 | Loss: 0.00011340
Iteration 976/1000 | Loss: 0.00011708
Iteration 977/1000 | Loss: 0.00010807
Iteration 978/1000 | Loss: 0.00011088
Iteration 979/1000 | Loss: 0.00010928
Iteration 980/1000 | Loss: 0.00011269
Iteration 981/1000 | Loss: 0.00011237
Iteration 982/1000 | Loss: 0.00012184
Iteration 983/1000 | Loss: 0.00011066
Iteration 984/1000 | Loss: 0.00011269
Iteration 985/1000 | Loss: 0.00011135
Iteration 986/1000 | Loss: 0.00010739
Iteration 987/1000 | Loss: 0.00010875
Iteration 988/1000 | Loss: 0.00010943
Iteration 989/1000 | Loss: 0.00011803
Iteration 990/1000 | Loss: 0.00011076
Iteration 991/1000 | Loss: 0.00011691
Iteration 992/1000 | Loss: 0.00011045
Iteration 993/1000 | Loss: 0.00010687
Iteration 994/1000 | Loss: 0.00010449
Iteration 995/1000 | Loss: 0.00010709
Iteration 996/1000 | Loss: 0.00010271
Iteration 997/1000 | Loss: 0.00010690
Iteration 998/1000 | Loss: 0.00010475
Iteration 999/1000 | Loss: 0.00010870
Iteration 1000/1000 | Loss: 0.00010500

Optimization complete. Final v2v error: 8.027822494506836 mm

Highest mean error: 24.19822883605957 mm for frame 180

Lowest mean error: 7.006898880004883 mm for frame 171

Saving results

Total time: 1493.6966652870178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00613113
Iteration 2/25 | Loss: 0.00300708
Iteration 3/25 | Loss: 0.00293680
Iteration 4/25 | Loss: 0.00292251
Iteration 5/25 | Loss: 0.00291836
Iteration 6/25 | Loss: 0.00291623
Iteration 7/25 | Loss: 0.00291623
Iteration 8/25 | Loss: 0.00291623
Iteration 9/25 | Loss: 0.00291623
Iteration 10/25 | Loss: 0.00291623
Iteration 11/25 | Loss: 0.00291623
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0029162256978452206, 0.0029162256978452206, 0.0029162256978452206, 0.0029162256978452206, 0.0029162256978452206]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029162256978452206

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18411696
Iteration 2/25 | Loss: 0.00455864
Iteration 3/25 | Loss: 0.00455863
Iteration 4/25 | Loss: 0.00455863
Iteration 5/25 | Loss: 0.00455863
Iteration 6/25 | Loss: 0.00455863
Iteration 7/25 | Loss: 0.00455862
Iteration 8/25 | Loss: 0.00455862
Iteration 9/25 | Loss: 0.00455862
Iteration 10/25 | Loss: 0.00455862
Iteration 11/25 | Loss: 0.00455862
Iteration 12/25 | Loss: 0.00455862
Iteration 13/25 | Loss: 0.00455862
Iteration 14/25 | Loss: 0.00455862
Iteration 15/25 | Loss: 0.00455862
Iteration 16/25 | Loss: 0.00455862
Iteration 17/25 | Loss: 0.00455862
Iteration 18/25 | Loss: 0.00455862
Iteration 19/25 | Loss: 0.00455862
Iteration 20/25 | Loss: 0.00455862
Iteration 21/25 | Loss: 0.00455862
Iteration 22/25 | Loss: 0.00455862
Iteration 23/25 | Loss: 0.00455862
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0045586246997118, 0.0045586246997118, 0.0045586246997118, 0.0045586246997118, 0.0045586246997118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0045586246997118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00455862
Iteration 2/1000 | Loss: 0.00015686
Iteration 3/1000 | Loss: 0.00010156
Iteration 4/1000 | Loss: 0.00008584
Iteration 5/1000 | Loss: 0.00008061
Iteration 6/1000 | Loss: 0.00007810
Iteration 7/1000 | Loss: 0.00007652
Iteration 8/1000 | Loss: 0.00007407
Iteration 9/1000 | Loss: 0.00007279
Iteration 10/1000 | Loss: 0.00007150
Iteration 11/1000 | Loss: 0.00007064
Iteration 12/1000 | Loss: 0.00006993
Iteration 13/1000 | Loss: 0.00006935
Iteration 14/1000 | Loss: 0.00006896
Iteration 15/1000 | Loss: 0.00006871
Iteration 16/1000 | Loss: 0.00006848
Iteration 17/1000 | Loss: 0.00006831
Iteration 18/1000 | Loss: 0.00006829
Iteration 19/1000 | Loss: 0.00006817
Iteration 20/1000 | Loss: 0.00006817
Iteration 21/1000 | Loss: 0.00006816
Iteration 22/1000 | Loss: 0.00006812
Iteration 23/1000 | Loss: 0.00006812
Iteration 24/1000 | Loss: 0.00006812
Iteration 25/1000 | Loss: 0.00006811
Iteration 26/1000 | Loss: 0.00006811
Iteration 27/1000 | Loss: 0.00006811
Iteration 28/1000 | Loss: 0.00006811
Iteration 29/1000 | Loss: 0.00006810
Iteration 30/1000 | Loss: 0.00006810
Iteration 31/1000 | Loss: 0.00006810
Iteration 32/1000 | Loss: 0.00006810
Iteration 33/1000 | Loss: 0.00006810
Iteration 34/1000 | Loss: 0.00006810
Iteration 35/1000 | Loss: 0.00006810
Iteration 36/1000 | Loss: 0.00006810
Iteration 37/1000 | Loss: 0.00006810
Iteration 38/1000 | Loss: 0.00006810
Iteration 39/1000 | Loss: 0.00006810
Iteration 40/1000 | Loss: 0.00006810
Iteration 41/1000 | Loss: 0.00006809
Iteration 42/1000 | Loss: 0.00006809
Iteration 43/1000 | Loss: 0.00006809
Iteration 44/1000 | Loss: 0.00006809
Iteration 45/1000 | Loss: 0.00006809
Iteration 46/1000 | Loss: 0.00006809
Iteration 47/1000 | Loss: 0.00006809
Iteration 48/1000 | Loss: 0.00006809
Iteration 49/1000 | Loss: 0.00006809
Iteration 50/1000 | Loss: 0.00006808
Iteration 51/1000 | Loss: 0.00006808
Iteration 52/1000 | Loss: 0.00006808
Iteration 53/1000 | Loss: 0.00006808
Iteration 54/1000 | Loss: 0.00006807
Iteration 55/1000 | Loss: 0.00006807
Iteration 56/1000 | Loss: 0.00006806
Iteration 57/1000 | Loss: 0.00006806
Iteration 58/1000 | Loss: 0.00006806
Iteration 59/1000 | Loss: 0.00006806
Iteration 60/1000 | Loss: 0.00006806
Iteration 61/1000 | Loss: 0.00006805
Iteration 62/1000 | Loss: 0.00006805
Iteration 63/1000 | Loss: 0.00006805
Iteration 64/1000 | Loss: 0.00006805
Iteration 65/1000 | Loss: 0.00006805
Iteration 66/1000 | Loss: 0.00006804
Iteration 67/1000 | Loss: 0.00006804
Iteration 68/1000 | Loss: 0.00006804
Iteration 69/1000 | Loss: 0.00006804
Iteration 70/1000 | Loss: 0.00006804
Iteration 71/1000 | Loss: 0.00006804
Iteration 72/1000 | Loss: 0.00006804
Iteration 73/1000 | Loss: 0.00006804
Iteration 74/1000 | Loss: 0.00006804
Iteration 75/1000 | Loss: 0.00006804
Iteration 76/1000 | Loss: 0.00006804
Iteration 77/1000 | Loss: 0.00006804
Iteration 78/1000 | Loss: 0.00006803
Iteration 79/1000 | Loss: 0.00006803
Iteration 80/1000 | Loss: 0.00006803
Iteration 81/1000 | Loss: 0.00006803
Iteration 82/1000 | Loss: 0.00006803
Iteration 83/1000 | Loss: 0.00006803
Iteration 84/1000 | Loss: 0.00006803
Iteration 85/1000 | Loss: 0.00006802
Iteration 86/1000 | Loss: 0.00006802
Iteration 87/1000 | Loss: 0.00006802
Iteration 88/1000 | Loss: 0.00006802
Iteration 89/1000 | Loss: 0.00006801
Iteration 90/1000 | Loss: 0.00006801
Iteration 91/1000 | Loss: 0.00006801
Iteration 92/1000 | Loss: 0.00006801
Iteration 93/1000 | Loss: 0.00006800
Iteration 94/1000 | Loss: 0.00006800
Iteration 95/1000 | Loss: 0.00006800
Iteration 96/1000 | Loss: 0.00006800
Iteration 97/1000 | Loss: 0.00006799
Iteration 98/1000 | Loss: 0.00006799
Iteration 99/1000 | Loss: 0.00006799
Iteration 100/1000 | Loss: 0.00006799
Iteration 101/1000 | Loss: 0.00006799
Iteration 102/1000 | Loss: 0.00006799
Iteration 103/1000 | Loss: 0.00006799
Iteration 104/1000 | Loss: 0.00006799
Iteration 105/1000 | Loss: 0.00006799
Iteration 106/1000 | Loss: 0.00006799
Iteration 107/1000 | Loss: 0.00006799
Iteration 108/1000 | Loss: 0.00006799
Iteration 109/1000 | Loss: 0.00006799
Iteration 110/1000 | Loss: 0.00006799
Iteration 111/1000 | Loss: 0.00006799
Iteration 112/1000 | Loss: 0.00006799
Iteration 113/1000 | Loss: 0.00006798
Iteration 114/1000 | Loss: 0.00006798
Iteration 115/1000 | Loss: 0.00006798
Iteration 116/1000 | Loss: 0.00006798
Iteration 117/1000 | Loss: 0.00006798
Iteration 118/1000 | Loss: 0.00006798
Iteration 119/1000 | Loss: 0.00006798
Iteration 120/1000 | Loss: 0.00006798
Iteration 121/1000 | Loss: 0.00006798
Iteration 122/1000 | Loss: 0.00006798
Iteration 123/1000 | Loss: 0.00006798
Iteration 124/1000 | Loss: 0.00006798
Iteration 125/1000 | Loss: 0.00006797
Iteration 126/1000 | Loss: 0.00006797
Iteration 127/1000 | Loss: 0.00006797
Iteration 128/1000 | Loss: 0.00006797
Iteration 129/1000 | Loss: 0.00006797
Iteration 130/1000 | Loss: 0.00006797
Iteration 131/1000 | Loss: 0.00006797
Iteration 132/1000 | Loss: 0.00006797
Iteration 133/1000 | Loss: 0.00006797
Iteration 134/1000 | Loss: 0.00006796
Iteration 135/1000 | Loss: 0.00006796
Iteration 136/1000 | Loss: 0.00006796
Iteration 137/1000 | Loss: 0.00006796
Iteration 138/1000 | Loss: 0.00006796
Iteration 139/1000 | Loss: 0.00006796
Iteration 140/1000 | Loss: 0.00006796
Iteration 141/1000 | Loss: 0.00006796
Iteration 142/1000 | Loss: 0.00006795
Iteration 143/1000 | Loss: 0.00006795
Iteration 144/1000 | Loss: 0.00006795
Iteration 145/1000 | Loss: 0.00006795
Iteration 146/1000 | Loss: 0.00006795
Iteration 147/1000 | Loss: 0.00006795
Iteration 148/1000 | Loss: 0.00006795
Iteration 149/1000 | Loss: 0.00006794
Iteration 150/1000 | Loss: 0.00006794
Iteration 151/1000 | Loss: 0.00006794
Iteration 152/1000 | Loss: 0.00006794
Iteration 153/1000 | Loss: 0.00006794
Iteration 154/1000 | Loss: 0.00006794
Iteration 155/1000 | Loss: 0.00006794
Iteration 156/1000 | Loss: 0.00006794
Iteration 157/1000 | Loss: 0.00006794
Iteration 158/1000 | Loss: 0.00006793
Iteration 159/1000 | Loss: 0.00006793
Iteration 160/1000 | Loss: 0.00006793
Iteration 161/1000 | Loss: 0.00006793
Iteration 162/1000 | Loss: 0.00006793
Iteration 163/1000 | Loss: 0.00006793
Iteration 164/1000 | Loss: 0.00006793
Iteration 165/1000 | Loss: 0.00006793
Iteration 166/1000 | Loss: 0.00006793
Iteration 167/1000 | Loss: 0.00006792
Iteration 168/1000 | Loss: 0.00006792
Iteration 169/1000 | Loss: 0.00006792
Iteration 170/1000 | Loss: 0.00006792
Iteration 171/1000 | Loss: 0.00006792
Iteration 172/1000 | Loss: 0.00006792
Iteration 173/1000 | Loss: 0.00006792
Iteration 174/1000 | Loss: 0.00006792
Iteration 175/1000 | Loss: 0.00006792
Iteration 176/1000 | Loss: 0.00006792
Iteration 177/1000 | Loss: 0.00006792
Iteration 178/1000 | Loss: 0.00006792
Iteration 179/1000 | Loss: 0.00006792
Iteration 180/1000 | Loss: 0.00006792
Iteration 181/1000 | Loss: 0.00006792
Iteration 182/1000 | Loss: 0.00006792
Iteration 183/1000 | Loss: 0.00006792
Iteration 184/1000 | Loss: 0.00006792
Iteration 185/1000 | Loss: 0.00006792
Iteration 186/1000 | Loss: 0.00006792
Iteration 187/1000 | Loss: 0.00006792
Iteration 188/1000 | Loss: 0.00006792
Iteration 189/1000 | Loss: 0.00006792
Iteration 190/1000 | Loss: 0.00006792
Iteration 191/1000 | Loss: 0.00006792
Iteration 192/1000 | Loss: 0.00006792
Iteration 193/1000 | Loss: 0.00006792
Iteration 194/1000 | Loss: 0.00006792
Iteration 195/1000 | Loss: 0.00006792
Iteration 196/1000 | Loss: 0.00006792
Iteration 197/1000 | Loss: 0.00006792
Iteration 198/1000 | Loss: 0.00006792
Iteration 199/1000 | Loss: 0.00006792
Iteration 200/1000 | Loss: 0.00006792
Iteration 201/1000 | Loss: 0.00006792
Iteration 202/1000 | Loss: 0.00006792
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [6.79249205859378e-05, 6.79249205859378e-05, 6.79249205859378e-05, 6.79249205859378e-05, 6.79249205859378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.79249205859378e-05

Optimization complete. Final v2v error: 7.345362186431885 mm

Highest mean error: 7.416423797607422 mm for frame 117

Lowest mean error: 7.299458026885986 mm for frame 16

Saving results

Total time: 44.899420976638794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00558526
Iteration 2/25 | Loss: 0.00283087
Iteration 3/25 | Loss: 0.00278985
Iteration 4/25 | Loss: 0.00278292
Iteration 5/25 | Loss: 0.00277889
Iteration 6/25 | Loss: 0.00277848
Iteration 7/25 | Loss: 0.00277848
Iteration 8/25 | Loss: 0.00277848
Iteration 9/25 | Loss: 0.00277848
Iteration 10/25 | Loss: 0.00277848
Iteration 11/25 | Loss: 0.00277848
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002778479130938649, 0.002778479130938649, 0.002778479130938649, 0.002778479130938649, 0.002778479130938649]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002778479130938649

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42535365
Iteration 2/25 | Loss: 0.00387529
Iteration 3/25 | Loss: 0.00387529
Iteration 4/25 | Loss: 0.00387528
Iteration 5/25 | Loss: 0.00387528
Iteration 6/25 | Loss: 0.00387528
Iteration 7/25 | Loss: 0.00387528
Iteration 8/25 | Loss: 0.00387528
Iteration 9/25 | Loss: 0.00387528
Iteration 10/25 | Loss: 0.00387528
Iteration 11/25 | Loss: 0.00387528
Iteration 12/25 | Loss: 0.00387528
Iteration 13/25 | Loss: 0.00387528
Iteration 14/25 | Loss: 0.00387528
Iteration 15/25 | Loss: 0.00387528
Iteration 16/25 | Loss: 0.00387528
Iteration 17/25 | Loss: 0.00387528
Iteration 18/25 | Loss: 0.00387528
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0038752825930714607, 0.0038752825930714607, 0.0038752825930714607, 0.0038752825930714607, 0.0038752825930714607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038752825930714607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00387528
Iteration 2/1000 | Loss: 0.00017066
Iteration 3/1000 | Loss: 0.00010697
Iteration 4/1000 | Loss: 0.00008981
Iteration 5/1000 | Loss: 0.00008485
Iteration 6/1000 | Loss: 0.00008193
Iteration 7/1000 | Loss: 0.00008032
Iteration 8/1000 | Loss: 0.00007930
Iteration 9/1000 | Loss: 0.00007789
Iteration 10/1000 | Loss: 0.00007673
Iteration 11/1000 | Loss: 0.00007570
Iteration 12/1000 | Loss: 0.00007498
Iteration 13/1000 | Loss: 0.00007458
Iteration 14/1000 | Loss: 0.00007427
Iteration 15/1000 | Loss: 0.00007403
Iteration 16/1000 | Loss: 0.00007388
Iteration 17/1000 | Loss: 0.00007381
Iteration 18/1000 | Loss: 0.00007377
Iteration 19/1000 | Loss: 0.00007375
Iteration 20/1000 | Loss: 0.00007375
Iteration 21/1000 | Loss: 0.00007375
Iteration 22/1000 | Loss: 0.00007374
Iteration 23/1000 | Loss: 0.00007373
Iteration 24/1000 | Loss: 0.00007373
Iteration 25/1000 | Loss: 0.00007373
Iteration 26/1000 | Loss: 0.00007373
Iteration 27/1000 | Loss: 0.00007373
Iteration 28/1000 | Loss: 0.00007373
Iteration 29/1000 | Loss: 0.00007372
Iteration 30/1000 | Loss: 0.00007372
Iteration 31/1000 | Loss: 0.00007371
Iteration 32/1000 | Loss: 0.00007371
Iteration 33/1000 | Loss: 0.00007370
Iteration 34/1000 | Loss: 0.00007370
Iteration 35/1000 | Loss: 0.00007370
Iteration 36/1000 | Loss: 0.00007370
Iteration 37/1000 | Loss: 0.00007370
Iteration 38/1000 | Loss: 0.00007368
Iteration 39/1000 | Loss: 0.00007368
Iteration 40/1000 | Loss: 0.00007368
Iteration 41/1000 | Loss: 0.00007368
Iteration 42/1000 | Loss: 0.00007367
Iteration 43/1000 | Loss: 0.00007367
Iteration 44/1000 | Loss: 0.00007367
Iteration 45/1000 | Loss: 0.00007367
Iteration 46/1000 | Loss: 0.00007366
Iteration 47/1000 | Loss: 0.00007366
Iteration 48/1000 | Loss: 0.00007366
Iteration 49/1000 | Loss: 0.00007366
Iteration 50/1000 | Loss: 0.00007366
Iteration 51/1000 | Loss: 0.00007366
Iteration 52/1000 | Loss: 0.00007366
Iteration 53/1000 | Loss: 0.00007366
Iteration 54/1000 | Loss: 0.00007366
Iteration 55/1000 | Loss: 0.00007366
Iteration 56/1000 | Loss: 0.00007365
Iteration 57/1000 | Loss: 0.00007365
Iteration 58/1000 | Loss: 0.00007365
Iteration 59/1000 | Loss: 0.00007365
Iteration 60/1000 | Loss: 0.00007365
Iteration 61/1000 | Loss: 0.00007364
Iteration 62/1000 | Loss: 0.00007364
Iteration 63/1000 | Loss: 0.00007364
Iteration 64/1000 | Loss: 0.00007364
Iteration 65/1000 | Loss: 0.00007364
Iteration 66/1000 | Loss: 0.00007363
Iteration 67/1000 | Loss: 0.00007363
Iteration 68/1000 | Loss: 0.00007363
Iteration 69/1000 | Loss: 0.00007363
Iteration 70/1000 | Loss: 0.00007363
Iteration 71/1000 | Loss: 0.00007363
Iteration 72/1000 | Loss: 0.00007363
Iteration 73/1000 | Loss: 0.00007363
Iteration 74/1000 | Loss: 0.00007363
Iteration 75/1000 | Loss: 0.00007363
Iteration 76/1000 | Loss: 0.00007363
Iteration 77/1000 | Loss: 0.00007362
Iteration 78/1000 | Loss: 0.00007362
Iteration 79/1000 | Loss: 0.00007362
Iteration 80/1000 | Loss: 0.00007362
Iteration 81/1000 | Loss: 0.00007362
Iteration 82/1000 | Loss: 0.00007362
Iteration 83/1000 | Loss: 0.00007362
Iteration 84/1000 | Loss: 0.00007362
Iteration 85/1000 | Loss: 0.00007362
Iteration 86/1000 | Loss: 0.00007361
Iteration 87/1000 | Loss: 0.00007361
Iteration 88/1000 | Loss: 0.00007361
Iteration 89/1000 | Loss: 0.00007361
Iteration 90/1000 | Loss: 0.00007361
Iteration 91/1000 | Loss: 0.00007361
Iteration 92/1000 | Loss: 0.00007361
Iteration 93/1000 | Loss: 0.00007361
Iteration 94/1000 | Loss: 0.00007361
Iteration 95/1000 | Loss: 0.00007361
Iteration 96/1000 | Loss: 0.00007361
Iteration 97/1000 | Loss: 0.00007361
Iteration 98/1000 | Loss: 0.00007361
Iteration 99/1000 | Loss: 0.00007361
Iteration 100/1000 | Loss: 0.00007360
Iteration 101/1000 | Loss: 0.00007360
Iteration 102/1000 | Loss: 0.00007360
Iteration 103/1000 | Loss: 0.00007360
Iteration 104/1000 | Loss: 0.00007360
Iteration 105/1000 | Loss: 0.00007360
Iteration 106/1000 | Loss: 0.00007360
Iteration 107/1000 | Loss: 0.00007360
Iteration 108/1000 | Loss: 0.00007360
Iteration 109/1000 | Loss: 0.00007360
Iteration 110/1000 | Loss: 0.00007360
Iteration 111/1000 | Loss: 0.00007360
Iteration 112/1000 | Loss: 0.00007360
Iteration 113/1000 | Loss: 0.00007360
Iteration 114/1000 | Loss: 0.00007360
Iteration 115/1000 | Loss: 0.00007360
Iteration 116/1000 | Loss: 0.00007360
Iteration 117/1000 | Loss: 0.00007360
Iteration 118/1000 | Loss: 0.00007360
Iteration 119/1000 | Loss: 0.00007360
Iteration 120/1000 | Loss: 0.00007360
Iteration 121/1000 | Loss: 0.00007360
Iteration 122/1000 | Loss: 0.00007360
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [7.360376912401989e-05, 7.360376912401989e-05, 7.360376912401989e-05, 7.360376912401989e-05, 7.360376912401989e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.360376912401989e-05

Optimization complete. Final v2v error: 7.682018280029297 mm

Highest mean error: 7.916770935058594 mm for frame 169

Lowest mean error: 7.495675086975098 mm for frame 151

Saving results

Total time: 40.00499248504639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027808
Iteration 2/25 | Loss: 0.00287793
Iteration 3/25 | Loss: 0.00282015
Iteration 4/25 | Loss: 0.00281062
Iteration 5/25 | Loss: 0.00280631
Iteration 6/25 | Loss: 0.00280593
Iteration 7/25 | Loss: 0.00280593
Iteration 8/25 | Loss: 0.00280593
Iteration 9/25 | Loss: 0.00280593
Iteration 10/25 | Loss: 0.00280593
Iteration 11/25 | Loss: 0.00280593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.002805926138535142, 0.002805926138535142, 0.002805926138535142, 0.002805926138535142, 0.002805926138535142]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002805926138535142

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.99040508
Iteration 2/25 | Loss: 0.00403125
Iteration 3/25 | Loss: 0.00403121
Iteration 4/25 | Loss: 0.00403121
Iteration 5/25 | Loss: 0.00403121
Iteration 6/25 | Loss: 0.00403121
Iteration 7/25 | Loss: 0.00403121
Iteration 8/25 | Loss: 0.00403121
Iteration 9/25 | Loss: 0.00403121
Iteration 10/25 | Loss: 0.00403121
Iteration 11/25 | Loss: 0.00403121
Iteration 12/25 | Loss: 0.00403121
Iteration 13/25 | Loss: 0.00403121
Iteration 14/25 | Loss: 0.00403121
Iteration 15/25 | Loss: 0.00403121
Iteration 16/25 | Loss: 0.00403121
Iteration 17/25 | Loss: 0.00403121
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004031206015497446, 0.004031206015497446, 0.004031206015497446, 0.004031206015497446, 0.004031206015497446]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004031206015497446

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00403121
Iteration 2/1000 | Loss: 0.00020599
Iteration 3/1000 | Loss: 0.00012073
Iteration 4/1000 | Loss: 0.00009675
Iteration 5/1000 | Loss: 0.00009135
Iteration 6/1000 | Loss: 0.00008867
Iteration 7/1000 | Loss: 0.00008696
Iteration 8/1000 | Loss: 0.00008590
Iteration 9/1000 | Loss: 0.00008491
Iteration 10/1000 | Loss: 0.00008392
Iteration 11/1000 | Loss: 0.00008302
Iteration 12/1000 | Loss: 0.00008238
Iteration 13/1000 | Loss: 0.00008193
Iteration 14/1000 | Loss: 0.00008170
Iteration 15/1000 | Loss: 0.00008149
Iteration 16/1000 | Loss: 0.00008143
Iteration 17/1000 | Loss: 0.00008130
Iteration 18/1000 | Loss: 0.00008130
Iteration 19/1000 | Loss: 0.00008123
Iteration 20/1000 | Loss: 0.00008123
Iteration 21/1000 | Loss: 0.00008123
Iteration 22/1000 | Loss: 0.00008122
Iteration 23/1000 | Loss: 0.00008122
Iteration 24/1000 | Loss: 0.00008122
Iteration 25/1000 | Loss: 0.00008122
Iteration 26/1000 | Loss: 0.00008121
Iteration 27/1000 | Loss: 0.00008120
Iteration 28/1000 | Loss: 0.00008120
Iteration 29/1000 | Loss: 0.00008120
Iteration 30/1000 | Loss: 0.00008120
Iteration 31/1000 | Loss: 0.00008120
Iteration 32/1000 | Loss: 0.00008120
Iteration 33/1000 | Loss: 0.00008119
Iteration 34/1000 | Loss: 0.00008119
Iteration 35/1000 | Loss: 0.00008119
Iteration 36/1000 | Loss: 0.00008118
Iteration 37/1000 | Loss: 0.00008118
Iteration 38/1000 | Loss: 0.00008118
Iteration 39/1000 | Loss: 0.00008118
Iteration 40/1000 | Loss: 0.00008118
Iteration 41/1000 | Loss: 0.00008118
Iteration 42/1000 | Loss: 0.00008118
Iteration 43/1000 | Loss: 0.00008118
Iteration 44/1000 | Loss: 0.00008118
Iteration 45/1000 | Loss: 0.00008118
Iteration 46/1000 | Loss: 0.00008118
Iteration 47/1000 | Loss: 0.00008118
Iteration 48/1000 | Loss: 0.00008118
Iteration 49/1000 | Loss: 0.00008118
Iteration 50/1000 | Loss: 0.00008118
Iteration 51/1000 | Loss: 0.00008118
Iteration 52/1000 | Loss: 0.00008118
Iteration 53/1000 | Loss: 0.00008118
Iteration 54/1000 | Loss: 0.00008118
Iteration 55/1000 | Loss: 0.00008117
Iteration 56/1000 | Loss: 0.00008117
Iteration 57/1000 | Loss: 0.00008117
Iteration 58/1000 | Loss: 0.00008117
Iteration 59/1000 | Loss: 0.00008117
Iteration 60/1000 | Loss: 0.00008117
Iteration 61/1000 | Loss: 0.00008117
Iteration 62/1000 | Loss: 0.00008117
Iteration 63/1000 | Loss: 0.00008117
Iteration 64/1000 | Loss: 0.00008117
Iteration 65/1000 | Loss: 0.00008117
Iteration 66/1000 | Loss: 0.00008116
Iteration 67/1000 | Loss: 0.00008116
Iteration 68/1000 | Loss: 0.00008116
Iteration 69/1000 | Loss: 0.00008116
Iteration 70/1000 | Loss: 0.00008116
Iteration 71/1000 | Loss: 0.00008116
Iteration 72/1000 | Loss: 0.00008116
Iteration 73/1000 | Loss: 0.00008116
Iteration 74/1000 | Loss: 0.00008116
Iteration 75/1000 | Loss: 0.00008116
Iteration 76/1000 | Loss: 0.00008116
Iteration 77/1000 | Loss: 0.00008116
Iteration 78/1000 | Loss: 0.00008116
Iteration 79/1000 | Loss: 0.00008116
Iteration 80/1000 | Loss: 0.00008116
Iteration 81/1000 | Loss: 0.00008116
Iteration 82/1000 | Loss: 0.00008116
Iteration 83/1000 | Loss: 0.00008116
Iteration 84/1000 | Loss: 0.00008116
Iteration 85/1000 | Loss: 0.00008116
Iteration 86/1000 | Loss: 0.00008116
Iteration 87/1000 | Loss: 0.00008116
Iteration 88/1000 | Loss: 0.00008116
Iteration 89/1000 | Loss: 0.00008116
Iteration 90/1000 | Loss: 0.00008116
Iteration 91/1000 | Loss: 0.00008116
Iteration 92/1000 | Loss: 0.00008116
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [8.115741366054863e-05, 8.115741366054863e-05, 8.115741366054863e-05, 8.115741366054863e-05, 8.115741366054863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.115741366054863e-05

Optimization complete. Final v2v error: 8.002604484558105 mm

Highest mean error: 8.247238159179688 mm for frame 29

Lowest mean error: 7.7709221839904785 mm for frame 0

Saving results

Total time: 36.00357532501221
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00565350
Iteration 2/25 | Loss: 0.00289318
Iteration 3/25 | Loss: 0.00283834
Iteration 4/25 | Loss: 0.00282783
Iteration 5/25 | Loss: 0.00282477
Iteration 6/25 | Loss: 0.00282477
Iteration 7/25 | Loss: 0.00282477
Iteration 8/25 | Loss: 0.00282477
Iteration 9/25 | Loss: 0.00282477
Iteration 10/25 | Loss: 0.00282477
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.002824765397235751, 0.002824765397235751, 0.002824765397235751, 0.002824765397235751, 0.002824765397235751]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002824765397235751

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47426736
Iteration 2/25 | Loss: 0.00424998
Iteration 3/25 | Loss: 0.00424998
Iteration 4/25 | Loss: 0.00424998
Iteration 5/25 | Loss: 0.00424998
Iteration 6/25 | Loss: 0.00424998
Iteration 7/25 | Loss: 0.00424998
Iteration 8/25 | Loss: 0.00424998
Iteration 9/25 | Loss: 0.00424998
Iteration 10/25 | Loss: 0.00424998
Iteration 11/25 | Loss: 0.00424998
Iteration 12/25 | Loss: 0.00424998
Iteration 13/25 | Loss: 0.00424998
Iteration 14/25 | Loss: 0.00424998
Iteration 15/25 | Loss: 0.00424998
Iteration 16/25 | Loss: 0.00424998
Iteration 17/25 | Loss: 0.00424998
Iteration 18/25 | Loss: 0.00424998
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.004249975550919771, 0.004249975550919771, 0.004249975550919771, 0.004249975550919771, 0.004249975550919771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004249975550919771

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00424998
Iteration 2/1000 | Loss: 0.00018185
Iteration 3/1000 | Loss: 0.00010320
Iteration 4/1000 | Loss: 0.00008783
Iteration 5/1000 | Loss: 0.00008341
Iteration 6/1000 | Loss: 0.00008135
Iteration 7/1000 | Loss: 0.00007960
Iteration 8/1000 | Loss: 0.00007841
Iteration 9/1000 | Loss: 0.00007715
Iteration 10/1000 | Loss: 0.00007615
Iteration 11/1000 | Loss: 0.00007517
Iteration 12/1000 | Loss: 0.00007467
Iteration 13/1000 | Loss: 0.00007427
Iteration 14/1000 | Loss: 0.00007401
Iteration 15/1000 | Loss: 0.00007381
Iteration 16/1000 | Loss: 0.00007372
Iteration 17/1000 | Loss: 0.00007370
Iteration 18/1000 | Loss: 0.00007370
Iteration 19/1000 | Loss: 0.00007365
Iteration 20/1000 | Loss: 0.00007362
Iteration 21/1000 | Loss: 0.00007361
Iteration 22/1000 | Loss: 0.00007361
Iteration 23/1000 | Loss: 0.00007361
Iteration 24/1000 | Loss: 0.00007361
Iteration 25/1000 | Loss: 0.00007360
Iteration 26/1000 | Loss: 0.00007360
Iteration 27/1000 | Loss: 0.00007360
Iteration 28/1000 | Loss: 0.00007359
Iteration 29/1000 | Loss: 0.00007359
Iteration 30/1000 | Loss: 0.00007359
Iteration 31/1000 | Loss: 0.00007359
Iteration 32/1000 | Loss: 0.00007359
Iteration 33/1000 | Loss: 0.00007358
Iteration 34/1000 | Loss: 0.00007358
Iteration 35/1000 | Loss: 0.00007358
Iteration 36/1000 | Loss: 0.00007358
Iteration 37/1000 | Loss: 0.00007358
Iteration 38/1000 | Loss: 0.00007358
Iteration 39/1000 | Loss: 0.00007358
Iteration 40/1000 | Loss: 0.00007358
Iteration 41/1000 | Loss: 0.00007358
Iteration 42/1000 | Loss: 0.00007358
Iteration 43/1000 | Loss: 0.00007358
Iteration 44/1000 | Loss: 0.00007357
Iteration 45/1000 | Loss: 0.00007357
Iteration 46/1000 | Loss: 0.00007357
Iteration 47/1000 | Loss: 0.00007357
Iteration 48/1000 | Loss: 0.00007357
Iteration 49/1000 | Loss: 0.00007357
Iteration 50/1000 | Loss: 0.00007357
Iteration 51/1000 | Loss: 0.00007357
Iteration 52/1000 | Loss: 0.00007357
Iteration 53/1000 | Loss: 0.00007357
Iteration 54/1000 | Loss: 0.00007357
Iteration 55/1000 | Loss: 0.00007357
Iteration 56/1000 | Loss: 0.00007356
Iteration 57/1000 | Loss: 0.00007356
Iteration 58/1000 | Loss: 0.00007356
Iteration 59/1000 | Loss: 0.00007356
Iteration 60/1000 | Loss: 0.00007356
Iteration 61/1000 | Loss: 0.00007356
Iteration 62/1000 | Loss: 0.00007356
Iteration 63/1000 | Loss: 0.00007356
Iteration 64/1000 | Loss: 0.00007356
Iteration 65/1000 | Loss: 0.00007355
Iteration 66/1000 | Loss: 0.00007355
Iteration 67/1000 | Loss: 0.00007355
Iteration 68/1000 | Loss: 0.00007355
Iteration 69/1000 | Loss: 0.00007355
Iteration 70/1000 | Loss: 0.00007355
Iteration 71/1000 | Loss: 0.00007355
Iteration 72/1000 | Loss: 0.00007355
Iteration 73/1000 | Loss: 0.00007355
Iteration 74/1000 | Loss: 0.00007355
Iteration 75/1000 | Loss: 0.00007355
Iteration 76/1000 | Loss: 0.00007355
Iteration 77/1000 | Loss: 0.00007355
Iteration 78/1000 | Loss: 0.00007355
Iteration 79/1000 | Loss: 0.00007355
Iteration 80/1000 | Loss: 0.00007355
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [7.354788249358535e-05, 7.354788249358535e-05, 7.354788249358535e-05, 7.354788249358535e-05, 7.354788249358535e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.354788249358535e-05

Optimization complete. Final v2v error: 7.602992057800293 mm

Highest mean error: 7.774807453155518 mm for frame 16

Lowest mean error: 7.453984260559082 mm for frame 128

Saving results

Total time: 34.621004819869995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058161
Iteration 2/25 | Loss: 0.00442255
Iteration 3/25 | Loss: 0.00296906
Iteration 4/25 | Loss: 0.00246091
Iteration 5/25 | Loss: 0.00237140
Iteration 6/25 | Loss: 0.00202953
Iteration 7/25 | Loss: 0.00196012
Iteration 8/25 | Loss: 0.00189720
Iteration 9/25 | Loss: 0.00185398
Iteration 10/25 | Loss: 0.00182524
Iteration 11/25 | Loss: 0.00181911
Iteration 12/25 | Loss: 0.00179617
Iteration 13/25 | Loss: 0.00179674
Iteration 14/25 | Loss: 0.00179564
Iteration 15/25 | Loss: 0.00179410
Iteration 16/25 | Loss: 0.00178847
Iteration 17/25 | Loss: 0.00178227
Iteration 18/25 | Loss: 0.00178391
Iteration 19/25 | Loss: 0.00177826
Iteration 20/25 | Loss: 0.00177992
Iteration 21/25 | Loss: 0.00178143
Iteration 22/25 | Loss: 0.00177749
Iteration 23/25 | Loss: 0.00177862
Iteration 24/25 | Loss: 0.00177730
Iteration 25/25 | Loss: 0.00177719

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56923658
Iteration 2/25 | Loss: 0.00287363
Iteration 3/25 | Loss: 0.00287363
Iteration 4/25 | Loss: 0.00287363
Iteration 5/25 | Loss: 0.00280335
Iteration 6/25 | Loss: 0.00280320
Iteration 7/25 | Loss: 0.00280320
Iteration 8/25 | Loss: 0.00280320
Iteration 9/25 | Loss: 0.00280320
Iteration 10/25 | Loss: 0.00280320
Iteration 11/25 | Loss: 0.00280320
Iteration 12/25 | Loss: 0.00280320
Iteration 13/25 | Loss: 0.00280319
Iteration 14/25 | Loss: 0.00280319
Iteration 15/25 | Loss: 0.00280319
Iteration 16/25 | Loss: 0.00280319
Iteration 17/25 | Loss: 0.00280319
Iteration 18/25 | Loss: 0.00280319
Iteration 19/25 | Loss: 0.00280319
Iteration 20/25 | Loss: 0.00280319
Iteration 21/25 | Loss: 0.00280319
Iteration 22/25 | Loss: 0.00280319
Iteration 23/25 | Loss: 0.00280319
Iteration 24/25 | Loss: 0.00280319
Iteration 25/25 | Loss: 0.00280319
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0028031941037625074, 0.0028031941037625074, 0.0028031941037625074, 0.0028031941037625074, 0.0028031941037625074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028031941037625074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00280319
Iteration 2/1000 | Loss: 0.00066380
Iteration 3/1000 | Loss: 0.00309502
Iteration 4/1000 | Loss: 0.00042773
Iteration 5/1000 | Loss: 0.00095917
Iteration 6/1000 | Loss: 0.00483337
Iteration 7/1000 | Loss: 0.00033828
Iteration 8/1000 | Loss: 0.00032080
Iteration 9/1000 | Loss: 0.00045210
Iteration 10/1000 | Loss: 0.00030415
Iteration 11/1000 | Loss: 0.00034169
Iteration 12/1000 | Loss: 0.00034710
Iteration 13/1000 | Loss: 0.00037555
Iteration 14/1000 | Loss: 0.00029960
Iteration 15/1000 | Loss: 0.00026895
Iteration 16/1000 | Loss: 0.00327608
Iteration 17/1000 | Loss: 0.00030366
Iteration 18/1000 | Loss: 0.00030263
Iteration 19/1000 | Loss: 0.00069260
Iteration 20/1000 | Loss: 0.00155298
Iteration 21/1000 | Loss: 0.00264447
Iteration 22/1000 | Loss: 0.00156466
Iteration 23/1000 | Loss: 0.00593789
Iteration 24/1000 | Loss: 0.00363268
Iteration 25/1000 | Loss: 0.00034918
Iteration 26/1000 | Loss: 0.00096117
Iteration 27/1000 | Loss: 0.00068920
Iteration 28/1000 | Loss: 0.00152862
Iteration 29/1000 | Loss: 0.00055938
Iteration 30/1000 | Loss: 0.00292598
Iteration 31/1000 | Loss: 0.00386156
Iteration 32/1000 | Loss: 0.00020046
Iteration 33/1000 | Loss: 0.00132462
Iteration 34/1000 | Loss: 0.00015408
Iteration 35/1000 | Loss: 0.00040013
Iteration 36/1000 | Loss: 0.00192028
Iteration 37/1000 | Loss: 0.00015870
Iteration 38/1000 | Loss: 0.00016766
Iteration 39/1000 | Loss: 0.00033826
Iteration 40/1000 | Loss: 0.00038707
Iteration 41/1000 | Loss: 0.00023418
Iteration 42/1000 | Loss: 0.00058266
Iteration 43/1000 | Loss: 0.00013913
Iteration 44/1000 | Loss: 0.00011200
Iteration 45/1000 | Loss: 0.00063784
Iteration 46/1000 | Loss: 0.00023700
Iteration 47/1000 | Loss: 0.00038078
Iteration 48/1000 | Loss: 0.00021859
Iteration 49/1000 | Loss: 0.00024217
Iteration 50/1000 | Loss: 0.00013881
Iteration 51/1000 | Loss: 0.00024538
Iteration 52/1000 | Loss: 0.00010654
Iteration 53/1000 | Loss: 0.00055549
Iteration 54/1000 | Loss: 0.00018324
Iteration 55/1000 | Loss: 0.00019467
Iteration 56/1000 | Loss: 0.00012463
Iteration 57/1000 | Loss: 0.00013062
Iteration 58/1000 | Loss: 0.00010426
Iteration 59/1000 | Loss: 0.00020396
Iteration 60/1000 | Loss: 0.00010349
Iteration 61/1000 | Loss: 0.00010308
Iteration 62/1000 | Loss: 0.00020835
Iteration 63/1000 | Loss: 0.00029767
Iteration 64/1000 | Loss: 0.00045240
Iteration 65/1000 | Loss: 0.00036986
Iteration 66/1000 | Loss: 0.00013345
Iteration 67/1000 | Loss: 0.00010295
Iteration 68/1000 | Loss: 0.00015596
Iteration 69/1000 | Loss: 0.00070748
Iteration 70/1000 | Loss: 0.00019082
Iteration 71/1000 | Loss: 0.00028591
Iteration 72/1000 | Loss: 0.00012001
Iteration 73/1000 | Loss: 0.00013753
Iteration 74/1000 | Loss: 0.00028733
Iteration 75/1000 | Loss: 0.00010763
Iteration 76/1000 | Loss: 0.00010235
Iteration 77/1000 | Loss: 0.00010234
Iteration 78/1000 | Loss: 0.00010233
Iteration 79/1000 | Loss: 0.00010233
Iteration 80/1000 | Loss: 0.00010233
Iteration 81/1000 | Loss: 0.00010233
Iteration 82/1000 | Loss: 0.00010232
Iteration 83/1000 | Loss: 0.00010232
Iteration 84/1000 | Loss: 0.00010232
Iteration 85/1000 | Loss: 0.00010223
Iteration 86/1000 | Loss: 0.00010223
Iteration 87/1000 | Loss: 0.00010219
Iteration 88/1000 | Loss: 0.00010219
Iteration 89/1000 | Loss: 0.00010219
Iteration 90/1000 | Loss: 0.00010218
Iteration 91/1000 | Loss: 0.00010218
Iteration 92/1000 | Loss: 0.00010218
Iteration 93/1000 | Loss: 0.00010218
Iteration 94/1000 | Loss: 0.00010217
Iteration 95/1000 | Loss: 0.00010217
Iteration 96/1000 | Loss: 0.00010217
Iteration 97/1000 | Loss: 0.00010217
Iteration 98/1000 | Loss: 0.00010217
Iteration 99/1000 | Loss: 0.00010217
Iteration 100/1000 | Loss: 0.00010217
Iteration 101/1000 | Loss: 0.00010217
Iteration 102/1000 | Loss: 0.00010217
Iteration 103/1000 | Loss: 0.00010217
Iteration 104/1000 | Loss: 0.00010217
Iteration 105/1000 | Loss: 0.00010217
Iteration 106/1000 | Loss: 0.00010217
Iteration 107/1000 | Loss: 0.00010216
Iteration 108/1000 | Loss: 0.00010216
Iteration 109/1000 | Loss: 0.00010216
Iteration 110/1000 | Loss: 0.00010216
Iteration 111/1000 | Loss: 0.00010216
Iteration 112/1000 | Loss: 0.00010216
Iteration 113/1000 | Loss: 0.00010216
Iteration 114/1000 | Loss: 0.00010216
Iteration 115/1000 | Loss: 0.00010215
Iteration 116/1000 | Loss: 0.00010215
Iteration 117/1000 | Loss: 0.00010215
Iteration 118/1000 | Loss: 0.00010215
Iteration 119/1000 | Loss: 0.00010215
Iteration 120/1000 | Loss: 0.00010215
Iteration 121/1000 | Loss: 0.00010215
Iteration 122/1000 | Loss: 0.00010215
Iteration 123/1000 | Loss: 0.00010215
Iteration 124/1000 | Loss: 0.00010215
Iteration 125/1000 | Loss: 0.00010215
Iteration 126/1000 | Loss: 0.00010215
Iteration 127/1000 | Loss: 0.00010215
Iteration 128/1000 | Loss: 0.00010214
Iteration 129/1000 | Loss: 0.00010214
Iteration 130/1000 | Loss: 0.00010214
Iteration 131/1000 | Loss: 0.00010214
Iteration 132/1000 | Loss: 0.00010214
Iteration 133/1000 | Loss: 0.00010214
Iteration 134/1000 | Loss: 0.00010214
Iteration 135/1000 | Loss: 0.00010214
Iteration 136/1000 | Loss: 0.00010214
Iteration 137/1000 | Loss: 0.00010214
Iteration 138/1000 | Loss: 0.00010214
Iteration 139/1000 | Loss: 0.00010214
Iteration 140/1000 | Loss: 0.00010214
Iteration 141/1000 | Loss: 0.00010214
Iteration 142/1000 | Loss: 0.00010213
Iteration 143/1000 | Loss: 0.00010213
Iteration 144/1000 | Loss: 0.00010213
Iteration 145/1000 | Loss: 0.00010213
Iteration 146/1000 | Loss: 0.00010213
Iteration 147/1000 | Loss: 0.00010213
Iteration 148/1000 | Loss: 0.00010213
Iteration 149/1000 | Loss: 0.00010213
Iteration 150/1000 | Loss: 0.00010213
Iteration 151/1000 | Loss: 0.00010213
Iteration 152/1000 | Loss: 0.00010213
Iteration 153/1000 | Loss: 0.00010213
Iteration 154/1000 | Loss: 0.00010213
Iteration 155/1000 | Loss: 0.00010213
Iteration 156/1000 | Loss: 0.00010213
Iteration 157/1000 | Loss: 0.00010213
Iteration 158/1000 | Loss: 0.00010213
Iteration 159/1000 | Loss: 0.00010213
Iteration 160/1000 | Loss: 0.00010213
Iteration 161/1000 | Loss: 0.00010213
Iteration 162/1000 | Loss: 0.00010213
Iteration 163/1000 | Loss: 0.00010213
Iteration 164/1000 | Loss: 0.00010213
Iteration 165/1000 | Loss: 0.00010213
Iteration 166/1000 | Loss: 0.00010213
Iteration 167/1000 | Loss: 0.00010213
Iteration 168/1000 | Loss: 0.00010213
Iteration 169/1000 | Loss: 0.00010213
Iteration 170/1000 | Loss: 0.00010213
Iteration 171/1000 | Loss: 0.00010213
Iteration 172/1000 | Loss: 0.00010213
Iteration 173/1000 | Loss: 0.00010213
Iteration 174/1000 | Loss: 0.00010213
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [0.00010213280620519072, 0.00010213280620519072, 0.00010213280620519072, 0.00010213280620519072, 0.00010213280620519072]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00010213280620519072

Optimization complete. Final v2v error: 8.169490814208984 mm

Highest mean error: 12.559561729431152 mm for frame 52

Lowest mean error: 6.938606262207031 mm for frame 31

Saving results

Total time: 161.07441997528076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00771225
Iteration 2/25 | Loss: 0.00306645
Iteration 3/25 | Loss: 0.00293745
Iteration 4/25 | Loss: 0.00292253
Iteration 5/25 | Loss: 0.00291735
Iteration 6/25 | Loss: 0.00291614
Iteration 7/25 | Loss: 0.00291614
Iteration 8/25 | Loss: 0.00291614
Iteration 9/25 | Loss: 0.00291614
Iteration 10/25 | Loss: 0.00291614
Iteration 11/25 | Loss: 0.00291614
Iteration 12/25 | Loss: 0.00291614
Iteration 13/25 | Loss: 0.00291614
Iteration 14/25 | Loss: 0.00291614
Iteration 15/25 | Loss: 0.00291614
Iteration 16/25 | Loss: 0.00291614
Iteration 17/25 | Loss: 0.00291614
Iteration 18/25 | Loss: 0.00291614
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002916141180321574, 0.002916141180321574, 0.002916141180321574, 0.002916141180321574, 0.002916141180321574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002916141180321574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42331207
Iteration 2/25 | Loss: 0.00389315
Iteration 3/25 | Loss: 0.00389312
Iteration 4/25 | Loss: 0.00389311
Iteration 5/25 | Loss: 0.00389311
Iteration 6/25 | Loss: 0.00389311
Iteration 7/25 | Loss: 0.00389311
Iteration 8/25 | Loss: 0.00389311
Iteration 9/25 | Loss: 0.00389311
Iteration 10/25 | Loss: 0.00389311
Iteration 11/25 | Loss: 0.00389311
Iteration 12/25 | Loss: 0.00389311
Iteration 13/25 | Loss: 0.00389311
Iteration 14/25 | Loss: 0.00389311
Iteration 15/25 | Loss: 0.00389311
Iteration 16/25 | Loss: 0.00389311
Iteration 17/25 | Loss: 0.00389311
Iteration 18/25 | Loss: 0.00389311
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0038931118324398994, 0.0038931118324398994, 0.0038931118324398994, 0.0038931118324398994, 0.0038931118324398994]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0038931118324398994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00389311
Iteration 2/1000 | Loss: 0.00017918
Iteration 3/1000 | Loss: 0.00012226
Iteration 4/1000 | Loss: 0.00009778
Iteration 5/1000 | Loss: 0.00008952
Iteration 6/1000 | Loss: 0.00008647
Iteration 7/1000 | Loss: 0.00008385
Iteration 8/1000 | Loss: 0.00008248
Iteration 9/1000 | Loss: 0.00008146
Iteration 10/1000 | Loss: 0.00008064
Iteration 11/1000 | Loss: 0.00007993
Iteration 12/1000 | Loss: 0.00007950
Iteration 13/1000 | Loss: 0.00007918
Iteration 14/1000 | Loss: 0.00007898
Iteration 15/1000 | Loss: 0.00007884
Iteration 16/1000 | Loss: 0.00007875
Iteration 17/1000 | Loss: 0.00007873
Iteration 18/1000 | Loss: 0.00007870
Iteration 19/1000 | Loss: 0.00007869
Iteration 20/1000 | Loss: 0.00007869
Iteration 21/1000 | Loss: 0.00007869
Iteration 22/1000 | Loss: 0.00007868
Iteration 23/1000 | Loss: 0.00007867
Iteration 24/1000 | Loss: 0.00007867
Iteration 25/1000 | Loss: 0.00007866
Iteration 26/1000 | Loss: 0.00007866
Iteration 27/1000 | Loss: 0.00007866
Iteration 28/1000 | Loss: 0.00007866
Iteration 29/1000 | Loss: 0.00007866
Iteration 30/1000 | Loss: 0.00007866
Iteration 31/1000 | Loss: 0.00007866
Iteration 32/1000 | Loss: 0.00007866
Iteration 33/1000 | Loss: 0.00007866
Iteration 34/1000 | Loss: 0.00007866
Iteration 35/1000 | Loss: 0.00007866
Iteration 36/1000 | Loss: 0.00007866
Iteration 37/1000 | Loss: 0.00007866
Iteration 38/1000 | Loss: 0.00007865
Iteration 39/1000 | Loss: 0.00007864
Iteration 40/1000 | Loss: 0.00007864
Iteration 41/1000 | Loss: 0.00007864
Iteration 42/1000 | Loss: 0.00007864
Iteration 43/1000 | Loss: 0.00007864
Iteration 44/1000 | Loss: 0.00007863
Iteration 45/1000 | Loss: 0.00007863
Iteration 46/1000 | Loss: 0.00007863
Iteration 47/1000 | Loss: 0.00007862
Iteration 48/1000 | Loss: 0.00007862
Iteration 49/1000 | Loss: 0.00007862
Iteration 50/1000 | Loss: 0.00007862
Iteration 51/1000 | Loss: 0.00007862
Iteration 52/1000 | Loss: 0.00007862
Iteration 53/1000 | Loss: 0.00007862
Iteration 54/1000 | Loss: 0.00007862
Iteration 55/1000 | Loss: 0.00007861
Iteration 56/1000 | Loss: 0.00007861
Iteration 57/1000 | Loss: 0.00007861
Iteration 58/1000 | Loss: 0.00007861
Iteration 59/1000 | Loss: 0.00007861
Iteration 60/1000 | Loss: 0.00007861
Iteration 61/1000 | Loss: 0.00007861
Iteration 62/1000 | Loss: 0.00007861
Iteration 63/1000 | Loss: 0.00007861
Iteration 64/1000 | Loss: 0.00007861
Iteration 65/1000 | Loss: 0.00007861
Iteration 66/1000 | Loss: 0.00007861
Iteration 67/1000 | Loss: 0.00007861
Iteration 68/1000 | Loss: 0.00007860
Iteration 69/1000 | Loss: 0.00007860
Iteration 70/1000 | Loss: 0.00007860
Iteration 71/1000 | Loss: 0.00007860
Iteration 72/1000 | Loss: 0.00007860
Iteration 73/1000 | Loss: 0.00007860
Iteration 74/1000 | Loss: 0.00007860
Iteration 75/1000 | Loss: 0.00007860
Iteration 76/1000 | Loss: 0.00007860
Iteration 77/1000 | Loss: 0.00007860
Iteration 78/1000 | Loss: 0.00007860
Iteration 79/1000 | Loss: 0.00007860
Iteration 80/1000 | Loss: 0.00007860
Iteration 81/1000 | Loss: 0.00007860
Iteration 82/1000 | Loss: 0.00007860
Iteration 83/1000 | Loss: 0.00007860
Iteration 84/1000 | Loss: 0.00007860
Iteration 85/1000 | Loss: 0.00007860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [7.860307960072532e-05, 7.860307960072532e-05, 7.860307960072532e-05, 7.860307960072532e-05, 7.860307960072532e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.860307960072532e-05

Optimization complete. Final v2v error: 7.828786849975586 mm

Highest mean error: 8.047407150268555 mm for frame 112

Lowest mean error: 7.575201988220215 mm for frame 21

Saving results

Total time: 35.39703679084778
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01003355
Iteration 2/25 | Loss: 0.00305625
Iteration 3/25 | Loss: 0.00290076
Iteration 4/25 | Loss: 0.00287569
Iteration 5/25 | Loss: 0.00287204
Iteration 6/25 | Loss: 0.00287204
Iteration 7/25 | Loss: 0.00287204
Iteration 8/25 | Loss: 0.00287204
Iteration 9/25 | Loss: 0.00287204
Iteration 10/25 | Loss: 0.00287204
Iteration 11/25 | Loss: 0.00287204
Iteration 12/25 | Loss: 0.00287204
Iteration 13/25 | Loss: 0.00287204
Iteration 14/25 | Loss: 0.00287204
Iteration 15/25 | Loss: 0.00287204
Iteration 16/25 | Loss: 0.00287204
Iteration 17/25 | Loss: 0.00287204
Iteration 18/25 | Loss: 0.00287204
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.002872043987736106, 0.002872043987736106, 0.002872043987736106, 0.002872043987736106, 0.002872043987736106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002872043987736106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42772651
Iteration 2/25 | Loss: 0.00409946
Iteration 3/25 | Loss: 0.00409946
Iteration 4/25 | Loss: 0.00409946
Iteration 5/25 | Loss: 0.00409946
Iteration 6/25 | Loss: 0.00409945
Iteration 7/25 | Loss: 0.00409945
Iteration 8/25 | Loss: 0.00409945
Iteration 9/25 | Loss: 0.00409945
Iteration 10/25 | Loss: 0.00409945
Iteration 11/25 | Loss: 0.00409945
Iteration 12/25 | Loss: 0.00409945
Iteration 13/25 | Loss: 0.00409945
Iteration 14/25 | Loss: 0.00409945
Iteration 15/25 | Loss: 0.00409945
Iteration 16/25 | Loss: 0.00409945
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004099453333765268, 0.004099453333765268, 0.004099453333765268, 0.004099453333765268, 0.004099453333765268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004099453333765268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00409945
Iteration 2/1000 | Loss: 0.00017621
Iteration 3/1000 | Loss: 0.00011332
Iteration 4/1000 | Loss: 0.00010145
Iteration 5/1000 | Loss: 0.00009321
Iteration 6/1000 | Loss: 0.00008905
Iteration 7/1000 | Loss: 0.00008688
Iteration 8/1000 | Loss: 0.00008548
Iteration 9/1000 | Loss: 0.00008424
Iteration 10/1000 | Loss: 0.00008328
Iteration 11/1000 | Loss: 0.00008252
Iteration 12/1000 | Loss: 0.00008217
Iteration 13/1000 | Loss: 0.00008182
Iteration 14/1000 | Loss: 0.00008157
Iteration 15/1000 | Loss: 0.00008140
Iteration 16/1000 | Loss: 0.00008130
Iteration 17/1000 | Loss: 0.00008123
Iteration 18/1000 | Loss: 0.00008116
Iteration 19/1000 | Loss: 0.00008116
Iteration 20/1000 | Loss: 0.00008116
Iteration 21/1000 | Loss: 0.00008115
Iteration 22/1000 | Loss: 0.00008115
Iteration 23/1000 | Loss: 0.00008115
Iteration 24/1000 | Loss: 0.00008114
Iteration 25/1000 | Loss: 0.00008113
Iteration 26/1000 | Loss: 0.00008113
Iteration 27/1000 | Loss: 0.00008112
Iteration 28/1000 | Loss: 0.00008112
Iteration 29/1000 | Loss: 0.00008112
Iteration 30/1000 | Loss: 0.00008111
Iteration 31/1000 | Loss: 0.00008111
Iteration 32/1000 | Loss: 0.00008110
Iteration 33/1000 | Loss: 0.00008110
Iteration 34/1000 | Loss: 0.00008110
Iteration 35/1000 | Loss: 0.00008109
Iteration 36/1000 | Loss: 0.00008109
Iteration 37/1000 | Loss: 0.00008109
Iteration 38/1000 | Loss: 0.00008109
Iteration 39/1000 | Loss: 0.00008109
Iteration 40/1000 | Loss: 0.00008109
Iteration 41/1000 | Loss: 0.00008108
Iteration 42/1000 | Loss: 0.00008108
Iteration 43/1000 | Loss: 0.00008108
Iteration 44/1000 | Loss: 0.00008108
Iteration 45/1000 | Loss: 0.00008108
Iteration 46/1000 | Loss: 0.00008108
Iteration 47/1000 | Loss: 0.00008107
Iteration 48/1000 | Loss: 0.00008107
Iteration 49/1000 | Loss: 0.00008107
Iteration 50/1000 | Loss: 0.00008107
Iteration 51/1000 | Loss: 0.00008106
Iteration 52/1000 | Loss: 0.00008106
Iteration 53/1000 | Loss: 0.00008106
Iteration 54/1000 | Loss: 0.00008106
Iteration 55/1000 | Loss: 0.00008106
Iteration 56/1000 | Loss: 0.00008106
Iteration 57/1000 | Loss: 0.00008106
Iteration 58/1000 | Loss: 0.00008106
Iteration 59/1000 | Loss: 0.00008105
Iteration 60/1000 | Loss: 0.00008105
Iteration 61/1000 | Loss: 0.00008104
Iteration 62/1000 | Loss: 0.00008104
Iteration 63/1000 | Loss: 0.00008104
Iteration 64/1000 | Loss: 0.00008104
Iteration 65/1000 | Loss: 0.00008103
Iteration 66/1000 | Loss: 0.00008103
Iteration 67/1000 | Loss: 0.00008103
Iteration 68/1000 | Loss: 0.00008103
Iteration 69/1000 | Loss: 0.00008102
Iteration 70/1000 | Loss: 0.00008102
Iteration 71/1000 | Loss: 0.00008102
Iteration 72/1000 | Loss: 0.00008102
Iteration 73/1000 | Loss: 0.00008101
Iteration 74/1000 | Loss: 0.00008101
Iteration 75/1000 | Loss: 0.00008101
Iteration 76/1000 | Loss: 0.00008101
Iteration 77/1000 | Loss: 0.00008101
Iteration 78/1000 | Loss: 0.00008101
Iteration 79/1000 | Loss: 0.00008101
Iteration 80/1000 | Loss: 0.00008101
Iteration 81/1000 | Loss: 0.00008100
Iteration 82/1000 | Loss: 0.00008100
Iteration 83/1000 | Loss: 0.00008100
Iteration 84/1000 | Loss: 0.00008100
Iteration 85/1000 | Loss: 0.00008100
Iteration 86/1000 | Loss: 0.00008100
Iteration 87/1000 | Loss: 0.00008100
Iteration 88/1000 | Loss: 0.00008099
Iteration 89/1000 | Loss: 0.00008099
Iteration 90/1000 | Loss: 0.00008099
Iteration 91/1000 | Loss: 0.00008099
Iteration 92/1000 | Loss: 0.00008099
Iteration 93/1000 | Loss: 0.00008099
Iteration 94/1000 | Loss: 0.00008099
Iteration 95/1000 | Loss: 0.00008099
Iteration 96/1000 | Loss: 0.00008099
Iteration 97/1000 | Loss: 0.00008099
Iteration 98/1000 | Loss: 0.00008099
Iteration 99/1000 | Loss: 0.00008099
Iteration 100/1000 | Loss: 0.00008099
Iteration 101/1000 | Loss: 0.00008099
Iteration 102/1000 | Loss: 0.00008099
Iteration 103/1000 | Loss: 0.00008099
Iteration 104/1000 | Loss: 0.00008098
Iteration 105/1000 | Loss: 0.00008098
Iteration 106/1000 | Loss: 0.00008098
Iteration 107/1000 | Loss: 0.00008098
Iteration 108/1000 | Loss: 0.00008098
Iteration 109/1000 | Loss: 0.00008098
Iteration 110/1000 | Loss: 0.00008098
Iteration 111/1000 | Loss: 0.00008098
Iteration 112/1000 | Loss: 0.00008098
Iteration 113/1000 | Loss: 0.00008098
Iteration 114/1000 | Loss: 0.00008098
Iteration 115/1000 | Loss: 0.00008098
Iteration 116/1000 | Loss: 0.00008098
Iteration 117/1000 | Loss: 0.00008098
Iteration 118/1000 | Loss: 0.00008098
Iteration 119/1000 | Loss: 0.00008098
Iteration 120/1000 | Loss: 0.00008098
Iteration 121/1000 | Loss: 0.00008098
Iteration 122/1000 | Loss: 0.00008098
Iteration 123/1000 | Loss: 0.00008098
Iteration 124/1000 | Loss: 0.00008098
Iteration 125/1000 | Loss: 0.00008098
Iteration 126/1000 | Loss: 0.00008098
Iteration 127/1000 | Loss: 0.00008098
Iteration 128/1000 | Loss: 0.00008098
Iteration 129/1000 | Loss: 0.00008098
Iteration 130/1000 | Loss: 0.00008098
Iteration 131/1000 | Loss: 0.00008098
Iteration 132/1000 | Loss: 0.00008098
Iteration 133/1000 | Loss: 0.00008098
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [8.098212856566533e-05, 8.098212856566533e-05, 8.098212856566533e-05, 8.098212856566533e-05, 8.098212856566533e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.098212856566533e-05

Optimization complete. Final v2v error: 7.974534511566162 mm

Highest mean error: 8.299762725830078 mm for frame 189

Lowest mean error: 7.601104259490967 mm for frame 62

Saving results

Total time: 44.07579326629639
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00688395
Iteration 2/25 | Loss: 0.00321129
Iteration 3/25 | Loss: 0.00295085
Iteration 4/25 | Loss: 0.00292322
Iteration 5/25 | Loss: 0.00291091
Iteration 6/25 | Loss: 0.00290700
Iteration 7/25 | Loss: 0.00290700
Iteration 8/25 | Loss: 0.00290700
Iteration 9/25 | Loss: 0.00290700
Iteration 10/25 | Loss: 0.00290700
Iteration 11/25 | Loss: 0.00290700
Iteration 12/25 | Loss: 0.00290700
Iteration 13/25 | Loss: 0.00290700
Iteration 14/25 | Loss: 0.00290700
Iteration 15/25 | Loss: 0.00290700
Iteration 16/25 | Loss: 0.00290700
Iteration 17/25 | Loss: 0.00290700
Iteration 18/25 | Loss: 0.00290700
Iteration 19/25 | Loss: 0.00290700
Iteration 20/25 | Loss: 0.00290700
Iteration 21/25 | Loss: 0.00290700
Iteration 22/25 | Loss: 0.00290700
Iteration 23/25 | Loss: 0.00290700
Iteration 24/25 | Loss: 0.00290700
Iteration 25/25 | Loss: 0.00290700

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20347500
Iteration 2/25 | Loss: 0.00450251
Iteration 3/25 | Loss: 0.00450251
Iteration 4/25 | Loss: 0.00450250
Iteration 5/25 | Loss: 0.00450250
Iteration 6/25 | Loss: 0.00450250
Iteration 7/25 | Loss: 0.00450250
Iteration 8/25 | Loss: 0.00450250
Iteration 9/25 | Loss: 0.00450250
Iteration 10/25 | Loss: 0.00450250
Iteration 11/25 | Loss: 0.00450250
Iteration 12/25 | Loss: 0.00450250
Iteration 13/25 | Loss: 0.00450250
Iteration 14/25 | Loss: 0.00450250
Iteration 15/25 | Loss: 0.00450250
Iteration 16/25 | Loss: 0.00450250
Iteration 17/25 | Loss: 0.00450250
Iteration 18/25 | Loss: 0.00450250
Iteration 19/25 | Loss: 0.00450250
Iteration 20/25 | Loss: 0.00450250
Iteration 21/25 | Loss: 0.00450250
Iteration 22/25 | Loss: 0.00450250
Iteration 23/25 | Loss: 0.00450250
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.004502502735704184, 0.004502502735704184, 0.004502502735704184, 0.004502502735704184, 0.004502502735704184]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004502502735704184

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00450250
Iteration 2/1000 | Loss: 0.00018290
Iteration 3/1000 | Loss: 0.00012828
Iteration 4/1000 | Loss: 0.00011291
Iteration 5/1000 | Loss: 0.00010405
Iteration 6/1000 | Loss: 0.00009890
Iteration 7/1000 | Loss: 0.00009398
Iteration 8/1000 | Loss: 0.00008963
Iteration 9/1000 | Loss: 0.00008725
Iteration 10/1000 | Loss: 0.00008558
Iteration 11/1000 | Loss: 0.00008324
Iteration 12/1000 | Loss: 0.00008196
Iteration 13/1000 | Loss: 0.00008062
Iteration 14/1000 | Loss: 0.00007952
Iteration 15/1000 | Loss: 0.00007879
Iteration 16/1000 | Loss: 0.00007842
Iteration 17/1000 | Loss: 0.00007811
Iteration 18/1000 | Loss: 0.00007789
Iteration 19/1000 | Loss: 0.00007765
Iteration 20/1000 | Loss: 0.00007745
Iteration 21/1000 | Loss: 0.00007729
Iteration 22/1000 | Loss: 0.00007722
Iteration 23/1000 | Loss: 0.00007720
Iteration 24/1000 | Loss: 0.00007719
Iteration 25/1000 | Loss: 0.00007715
Iteration 26/1000 | Loss: 0.00007711
Iteration 27/1000 | Loss: 0.00007709
Iteration 28/1000 | Loss: 0.00007709
Iteration 29/1000 | Loss: 0.00007706
Iteration 30/1000 | Loss: 0.00007706
Iteration 31/1000 | Loss: 0.00007705
Iteration 32/1000 | Loss: 0.00007704
Iteration 33/1000 | Loss: 0.00007703
Iteration 34/1000 | Loss: 0.00007700
Iteration 35/1000 | Loss: 0.00007699
Iteration 36/1000 | Loss: 0.00007698
Iteration 37/1000 | Loss: 0.00007697
Iteration 38/1000 | Loss: 0.00007697
Iteration 39/1000 | Loss: 0.00007697
Iteration 40/1000 | Loss: 0.00007697
Iteration 41/1000 | Loss: 0.00007697
Iteration 42/1000 | Loss: 0.00007697
Iteration 43/1000 | Loss: 0.00007697
Iteration 44/1000 | Loss: 0.00007696
Iteration 45/1000 | Loss: 0.00007696
Iteration 46/1000 | Loss: 0.00007696
Iteration 47/1000 | Loss: 0.00007696
Iteration 48/1000 | Loss: 0.00007696
Iteration 49/1000 | Loss: 0.00007696
Iteration 50/1000 | Loss: 0.00007696
Iteration 51/1000 | Loss: 0.00007695
Iteration 52/1000 | Loss: 0.00007695
Iteration 53/1000 | Loss: 0.00007695
Iteration 54/1000 | Loss: 0.00007694
Iteration 55/1000 | Loss: 0.00007694
Iteration 56/1000 | Loss: 0.00007694
Iteration 57/1000 | Loss: 0.00007694
Iteration 58/1000 | Loss: 0.00007694
Iteration 59/1000 | Loss: 0.00007694
Iteration 60/1000 | Loss: 0.00007694
Iteration 61/1000 | Loss: 0.00007694
Iteration 62/1000 | Loss: 0.00007694
Iteration 63/1000 | Loss: 0.00007694
Iteration 64/1000 | Loss: 0.00007694
Iteration 65/1000 | Loss: 0.00007693
Iteration 66/1000 | Loss: 0.00007693
Iteration 67/1000 | Loss: 0.00007693
Iteration 68/1000 | Loss: 0.00007693
Iteration 69/1000 | Loss: 0.00007693
Iteration 70/1000 | Loss: 0.00007693
Iteration 71/1000 | Loss: 0.00007693
Iteration 72/1000 | Loss: 0.00007693
Iteration 73/1000 | Loss: 0.00007693
Iteration 74/1000 | Loss: 0.00007693
Iteration 75/1000 | Loss: 0.00007693
Iteration 76/1000 | Loss: 0.00007691
Iteration 77/1000 | Loss: 0.00007691
Iteration 78/1000 | Loss: 0.00007691
Iteration 79/1000 | Loss: 0.00007691
Iteration 80/1000 | Loss: 0.00007691
Iteration 81/1000 | Loss: 0.00007691
Iteration 82/1000 | Loss: 0.00007691
Iteration 83/1000 | Loss: 0.00007690
Iteration 84/1000 | Loss: 0.00007690
Iteration 85/1000 | Loss: 0.00007690
Iteration 86/1000 | Loss: 0.00007690
Iteration 87/1000 | Loss: 0.00007690
Iteration 88/1000 | Loss: 0.00007690
Iteration 89/1000 | Loss: 0.00007690
Iteration 90/1000 | Loss: 0.00007690
Iteration 91/1000 | Loss: 0.00007689
Iteration 92/1000 | Loss: 0.00007689
Iteration 93/1000 | Loss: 0.00007689
Iteration 94/1000 | Loss: 0.00007689
Iteration 95/1000 | Loss: 0.00007689
Iteration 96/1000 | Loss: 0.00007689
Iteration 97/1000 | Loss: 0.00007688
Iteration 98/1000 | Loss: 0.00007688
Iteration 99/1000 | Loss: 0.00007688
Iteration 100/1000 | Loss: 0.00007688
Iteration 101/1000 | Loss: 0.00007687
Iteration 102/1000 | Loss: 0.00007687
Iteration 103/1000 | Loss: 0.00007687
Iteration 104/1000 | Loss: 0.00007687
Iteration 105/1000 | Loss: 0.00007687
Iteration 106/1000 | Loss: 0.00007686
Iteration 107/1000 | Loss: 0.00007686
Iteration 108/1000 | Loss: 0.00007686
Iteration 109/1000 | Loss: 0.00007686
Iteration 110/1000 | Loss: 0.00007686
Iteration 111/1000 | Loss: 0.00007686
Iteration 112/1000 | Loss: 0.00007686
Iteration 113/1000 | Loss: 0.00007686
Iteration 114/1000 | Loss: 0.00007686
Iteration 115/1000 | Loss: 0.00007686
Iteration 116/1000 | Loss: 0.00007686
Iteration 117/1000 | Loss: 0.00007685
Iteration 118/1000 | Loss: 0.00007685
Iteration 119/1000 | Loss: 0.00007685
Iteration 120/1000 | Loss: 0.00007685
Iteration 121/1000 | Loss: 0.00007685
Iteration 122/1000 | Loss: 0.00007685
Iteration 123/1000 | Loss: 0.00007685
Iteration 124/1000 | Loss: 0.00007685
Iteration 125/1000 | Loss: 0.00007685
Iteration 126/1000 | Loss: 0.00007685
Iteration 127/1000 | Loss: 0.00007685
Iteration 128/1000 | Loss: 0.00007685
Iteration 129/1000 | Loss: 0.00007685
Iteration 130/1000 | Loss: 0.00007685
Iteration 131/1000 | Loss: 0.00007685
Iteration 132/1000 | Loss: 0.00007685
Iteration 133/1000 | Loss: 0.00007685
Iteration 134/1000 | Loss: 0.00007685
Iteration 135/1000 | Loss: 0.00007685
Iteration 136/1000 | Loss: 0.00007685
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [7.684993761358783e-05, 7.684993761358783e-05, 7.684993761358783e-05, 7.684993761358783e-05, 7.684993761358783e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.684993761358783e-05

Optimization complete. Final v2v error: 7.563687324523926 mm

Highest mean error: 9.43816089630127 mm for frame 161

Lowest mean error: 6.876553058624268 mm for frame 61

Saving results

Total time: 52.4970908164978
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00646910
Iteration 2/25 | Loss: 0.00291544
Iteration 3/25 | Loss: 0.00284090
Iteration 4/25 | Loss: 0.00283085
Iteration 5/25 | Loss: 0.00282683
Iteration 6/25 | Loss: 0.00282591
Iteration 7/25 | Loss: 0.00282591
Iteration 8/25 | Loss: 0.00282591
Iteration 9/25 | Loss: 0.00282591
Iteration 10/25 | Loss: 0.00282591
Iteration 11/25 | Loss: 0.00282591
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0028259148821234703, 0.0028259148821234703, 0.0028259148821234703, 0.0028259148821234703, 0.0028259148821234703]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028259148821234703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.58421326
Iteration 2/25 | Loss: 0.00395022
Iteration 3/25 | Loss: 0.00395022
Iteration 4/25 | Loss: 0.00395021
Iteration 5/25 | Loss: 0.00395021
Iteration 6/25 | Loss: 0.00395021
Iteration 7/25 | Loss: 0.00395021
Iteration 8/25 | Loss: 0.00395021
Iteration 9/25 | Loss: 0.00395021
Iteration 10/25 | Loss: 0.00395021
Iteration 11/25 | Loss: 0.00395021
Iteration 12/25 | Loss: 0.00395021
Iteration 13/25 | Loss: 0.00395021
Iteration 14/25 | Loss: 0.00395021
Iteration 15/25 | Loss: 0.00395021
Iteration 16/25 | Loss: 0.00395021
Iteration 17/25 | Loss: 0.00395021
Iteration 18/25 | Loss: 0.00395021
Iteration 19/25 | Loss: 0.00395021
Iteration 20/25 | Loss: 0.00395021
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.003950210753828287, 0.003950210753828287, 0.003950210753828287, 0.003950210753828287, 0.003950210753828287]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003950210753828287

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00395021
Iteration 2/1000 | Loss: 0.00016745
Iteration 3/1000 | Loss: 0.00011678
Iteration 4/1000 | Loss: 0.00009447
Iteration 5/1000 | Loss: 0.00008895
Iteration 6/1000 | Loss: 0.00008583
Iteration 7/1000 | Loss: 0.00008345
Iteration 8/1000 | Loss: 0.00008195
Iteration 9/1000 | Loss: 0.00008074
Iteration 10/1000 | Loss: 0.00007986
Iteration 11/1000 | Loss: 0.00007909
Iteration 12/1000 | Loss: 0.00007837
Iteration 13/1000 | Loss: 0.00007787
Iteration 14/1000 | Loss: 0.00007754
Iteration 15/1000 | Loss: 0.00007731
Iteration 16/1000 | Loss: 0.00007714
Iteration 17/1000 | Loss: 0.00007700
Iteration 18/1000 | Loss: 0.00007699
Iteration 19/1000 | Loss: 0.00007698
Iteration 20/1000 | Loss: 0.00007697
Iteration 21/1000 | Loss: 0.00007697
Iteration 22/1000 | Loss: 0.00007696
Iteration 23/1000 | Loss: 0.00007696
Iteration 24/1000 | Loss: 0.00007696
Iteration 25/1000 | Loss: 0.00007696
Iteration 26/1000 | Loss: 0.00007696
Iteration 27/1000 | Loss: 0.00007696
Iteration 28/1000 | Loss: 0.00007696
Iteration 29/1000 | Loss: 0.00007696
Iteration 30/1000 | Loss: 0.00007696
Iteration 31/1000 | Loss: 0.00007695
Iteration 32/1000 | Loss: 0.00007695
Iteration 33/1000 | Loss: 0.00007695
Iteration 34/1000 | Loss: 0.00007694
Iteration 35/1000 | Loss: 0.00007694
Iteration 36/1000 | Loss: 0.00007694
Iteration 37/1000 | Loss: 0.00007694
Iteration 38/1000 | Loss: 0.00007694
Iteration 39/1000 | Loss: 0.00007693
Iteration 40/1000 | Loss: 0.00007693
Iteration 41/1000 | Loss: 0.00007693
Iteration 42/1000 | Loss: 0.00007692
Iteration 43/1000 | Loss: 0.00007692
Iteration 44/1000 | Loss: 0.00007692
Iteration 45/1000 | Loss: 0.00007692
Iteration 46/1000 | Loss: 0.00007692
Iteration 47/1000 | Loss: 0.00007692
Iteration 48/1000 | Loss: 0.00007692
Iteration 49/1000 | Loss: 0.00007692
Iteration 50/1000 | Loss: 0.00007692
Iteration 51/1000 | Loss: 0.00007692
Iteration 52/1000 | Loss: 0.00007692
Iteration 53/1000 | Loss: 0.00007692
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 53. Stopping optimization.
Last 5 losses: [7.69197940826416e-05, 7.69197940826416e-05, 7.69197940826416e-05, 7.69197940826416e-05, 7.69197940826416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.69197940826416e-05

Optimization complete. Final v2v error: 7.708230972290039 mm

Highest mean error: 8.406242370605469 mm for frame 37

Lowest mean error: 7.2214741706848145 mm for frame 30

Saving results

Total time: 34.317919969558716
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00622803
Iteration 2/25 | Loss: 0.00291739
Iteration 3/25 | Loss: 0.00285212
Iteration 4/25 | Loss: 0.00284588
Iteration 5/25 | Loss: 0.00284537
Iteration 6/25 | Loss: 0.00284537
Iteration 7/25 | Loss: 0.00284537
Iteration 8/25 | Loss: 0.00284537
Iteration 9/25 | Loss: 0.00284537
Iteration 10/25 | Loss: 0.00284537
Iteration 11/25 | Loss: 0.00284537
Iteration 12/25 | Loss: 0.00284537
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0028453662525862455, 0.0028453662525862455, 0.0028453662525862455, 0.0028453662525862455, 0.0028453662525862455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028453662525862455

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.43193686
Iteration 2/25 | Loss: 0.00426567
Iteration 3/25 | Loss: 0.00426565
Iteration 4/25 | Loss: 0.00426565
Iteration 5/25 | Loss: 0.00426565
Iteration 6/25 | Loss: 0.00426565
Iteration 7/25 | Loss: 0.00426565
Iteration 8/25 | Loss: 0.00426565
Iteration 9/25 | Loss: 0.00426565
Iteration 10/25 | Loss: 0.00426565
Iteration 11/25 | Loss: 0.00426565
Iteration 12/25 | Loss: 0.00426565
Iteration 13/25 | Loss: 0.00426565
Iteration 14/25 | Loss: 0.00426565
Iteration 15/25 | Loss: 0.00426565
Iteration 16/25 | Loss: 0.00426565
Iteration 17/25 | Loss: 0.00426565
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004265649244189262, 0.004265649244189262, 0.004265649244189262, 0.004265649244189262, 0.004265649244189262]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004265649244189262

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00426565
Iteration 2/1000 | Loss: 0.00019905
Iteration 3/1000 | Loss: 0.00011900
Iteration 4/1000 | Loss: 0.00010042
Iteration 5/1000 | Loss: 0.00009353
Iteration 6/1000 | Loss: 0.00009016
Iteration 7/1000 | Loss: 0.00008786
Iteration 8/1000 | Loss: 0.00008665
Iteration 9/1000 | Loss: 0.00008527
Iteration 10/1000 | Loss: 0.00008430
Iteration 11/1000 | Loss: 0.00008356
Iteration 12/1000 | Loss: 0.00008297
Iteration 13/1000 | Loss: 0.00008254
Iteration 14/1000 | Loss: 0.00008213
Iteration 15/1000 | Loss: 0.00008182
Iteration 16/1000 | Loss: 0.00008156
Iteration 17/1000 | Loss: 0.00008139
Iteration 18/1000 | Loss: 0.00008133
Iteration 19/1000 | Loss: 0.00008132
Iteration 20/1000 | Loss: 0.00008129
Iteration 21/1000 | Loss: 0.00008129
Iteration 22/1000 | Loss: 0.00008127
Iteration 23/1000 | Loss: 0.00008126
Iteration 24/1000 | Loss: 0.00008126
Iteration 25/1000 | Loss: 0.00008124
Iteration 26/1000 | Loss: 0.00008123
Iteration 27/1000 | Loss: 0.00008123
Iteration 28/1000 | Loss: 0.00008122
Iteration 29/1000 | Loss: 0.00008122
Iteration 30/1000 | Loss: 0.00008120
Iteration 31/1000 | Loss: 0.00008120
Iteration 32/1000 | Loss: 0.00008118
Iteration 33/1000 | Loss: 0.00008118
Iteration 34/1000 | Loss: 0.00008117
Iteration 35/1000 | Loss: 0.00008117
Iteration 36/1000 | Loss: 0.00008117
Iteration 37/1000 | Loss: 0.00008117
Iteration 38/1000 | Loss: 0.00008117
Iteration 39/1000 | Loss: 0.00008114
Iteration 40/1000 | Loss: 0.00008114
Iteration 41/1000 | Loss: 0.00008114
Iteration 42/1000 | Loss: 0.00008111
Iteration 43/1000 | Loss: 0.00008111
Iteration 44/1000 | Loss: 0.00008111
Iteration 45/1000 | Loss: 0.00008110
Iteration 46/1000 | Loss: 0.00008110
Iteration 47/1000 | Loss: 0.00008109
Iteration 48/1000 | Loss: 0.00008109
Iteration 49/1000 | Loss: 0.00008109
Iteration 50/1000 | Loss: 0.00008109
Iteration 51/1000 | Loss: 0.00008109
Iteration 52/1000 | Loss: 0.00008109
Iteration 53/1000 | Loss: 0.00008109
Iteration 54/1000 | Loss: 0.00008109
Iteration 55/1000 | Loss: 0.00008109
Iteration 56/1000 | Loss: 0.00008109
Iteration 57/1000 | Loss: 0.00008109
Iteration 58/1000 | Loss: 0.00008108
Iteration 59/1000 | Loss: 0.00008108
Iteration 60/1000 | Loss: 0.00008108
Iteration 61/1000 | Loss: 0.00008108
Iteration 62/1000 | Loss: 0.00008108
Iteration 63/1000 | Loss: 0.00008108
Iteration 64/1000 | Loss: 0.00008108
Iteration 65/1000 | Loss: 0.00008107
Iteration 66/1000 | Loss: 0.00008107
Iteration 67/1000 | Loss: 0.00008107
Iteration 68/1000 | Loss: 0.00008107
Iteration 69/1000 | Loss: 0.00008107
Iteration 70/1000 | Loss: 0.00008107
Iteration 71/1000 | Loss: 0.00008107
Iteration 72/1000 | Loss: 0.00008107
Iteration 73/1000 | Loss: 0.00008107
Iteration 74/1000 | Loss: 0.00008107
Iteration 75/1000 | Loss: 0.00008107
Iteration 76/1000 | Loss: 0.00008107
Iteration 77/1000 | Loss: 0.00008106
Iteration 78/1000 | Loss: 0.00008106
Iteration 79/1000 | Loss: 0.00008106
Iteration 80/1000 | Loss: 0.00008106
Iteration 81/1000 | Loss: 0.00008106
Iteration 82/1000 | Loss: 0.00008106
Iteration 83/1000 | Loss: 0.00008106
Iteration 84/1000 | Loss: 0.00008106
Iteration 85/1000 | Loss: 0.00008106
Iteration 86/1000 | Loss: 0.00008106
Iteration 87/1000 | Loss: 0.00008106
Iteration 88/1000 | Loss: 0.00008105
Iteration 89/1000 | Loss: 0.00008105
Iteration 90/1000 | Loss: 0.00008105
Iteration 91/1000 | Loss: 0.00008105
Iteration 92/1000 | Loss: 0.00008105
Iteration 93/1000 | Loss: 0.00008105
Iteration 94/1000 | Loss: 0.00008105
Iteration 95/1000 | Loss: 0.00008105
Iteration 96/1000 | Loss: 0.00008105
Iteration 97/1000 | Loss: 0.00008105
Iteration 98/1000 | Loss: 0.00008105
Iteration 99/1000 | Loss: 0.00008105
Iteration 100/1000 | Loss: 0.00008105
Iteration 101/1000 | Loss: 0.00008105
Iteration 102/1000 | Loss: 0.00008105
Iteration 103/1000 | Loss: 0.00008105
Iteration 104/1000 | Loss: 0.00008105
Iteration 105/1000 | Loss: 0.00008105
Iteration 106/1000 | Loss: 0.00008105
Iteration 107/1000 | Loss: 0.00008105
Iteration 108/1000 | Loss: 0.00008105
Iteration 109/1000 | Loss: 0.00008105
Iteration 110/1000 | Loss: 0.00008105
Iteration 111/1000 | Loss: 0.00008105
Iteration 112/1000 | Loss: 0.00008105
Iteration 113/1000 | Loss: 0.00008105
Iteration 114/1000 | Loss: 0.00008105
Iteration 115/1000 | Loss: 0.00008105
Iteration 116/1000 | Loss: 0.00008105
Iteration 117/1000 | Loss: 0.00008105
Iteration 118/1000 | Loss: 0.00008105
Iteration 119/1000 | Loss: 0.00008105
Iteration 120/1000 | Loss: 0.00008105
Iteration 121/1000 | Loss: 0.00008105
Iteration 122/1000 | Loss: 0.00008105
Iteration 123/1000 | Loss: 0.00008105
Iteration 124/1000 | Loss: 0.00008105
Iteration 125/1000 | Loss: 0.00008105
Iteration 126/1000 | Loss: 0.00008105
Iteration 127/1000 | Loss: 0.00008105
Iteration 128/1000 | Loss: 0.00008105
Iteration 129/1000 | Loss: 0.00008105
Iteration 130/1000 | Loss: 0.00008105
Iteration 131/1000 | Loss: 0.00008105
Iteration 132/1000 | Loss: 0.00008105
Iteration 133/1000 | Loss: 0.00008105
Iteration 134/1000 | Loss: 0.00008105
Iteration 135/1000 | Loss: 0.00008105
Iteration 136/1000 | Loss: 0.00008105
Iteration 137/1000 | Loss: 0.00008105
Iteration 138/1000 | Loss: 0.00008105
Iteration 139/1000 | Loss: 0.00008105
Iteration 140/1000 | Loss: 0.00008105
Iteration 141/1000 | Loss: 0.00008105
Iteration 142/1000 | Loss: 0.00008105
Iteration 143/1000 | Loss: 0.00008105
Iteration 144/1000 | Loss: 0.00008105
Iteration 145/1000 | Loss: 0.00008105
Iteration 146/1000 | Loss: 0.00008105
Iteration 147/1000 | Loss: 0.00008105
Iteration 148/1000 | Loss: 0.00008105
Iteration 149/1000 | Loss: 0.00008105
Iteration 150/1000 | Loss: 0.00008105
Iteration 151/1000 | Loss: 0.00008105
Iteration 152/1000 | Loss: 0.00008105
Iteration 153/1000 | Loss: 0.00008105
Iteration 154/1000 | Loss: 0.00008105
Iteration 155/1000 | Loss: 0.00008105
Iteration 156/1000 | Loss: 0.00008105
Iteration 157/1000 | Loss: 0.00008105
Iteration 158/1000 | Loss: 0.00008105
Iteration 159/1000 | Loss: 0.00008105
Iteration 160/1000 | Loss: 0.00008105
Iteration 161/1000 | Loss: 0.00008105
Iteration 162/1000 | Loss: 0.00008105
Iteration 163/1000 | Loss: 0.00008105
Iteration 164/1000 | Loss: 0.00008105
Iteration 165/1000 | Loss: 0.00008105
Iteration 166/1000 | Loss: 0.00008105
Iteration 167/1000 | Loss: 0.00008105
Iteration 168/1000 | Loss: 0.00008105
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 168. Stopping optimization.
Last 5 losses: [8.105351298581809e-05, 8.105351298581809e-05, 8.105351298581809e-05, 8.105351298581809e-05, 8.105351298581809e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.105351298581809e-05

Optimization complete. Final v2v error: 7.919353485107422 mm

Highest mean error: 8.298614501953125 mm for frame 184

Lowest mean error: 7.52926778793335 mm for frame 71

Saving results

Total time: 44.32127356529236
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01154982
Iteration 2/25 | Loss: 0.00353453
Iteration 3/25 | Loss: 0.00279103
Iteration 4/25 | Loss: 0.00264482
Iteration 5/25 | Loss: 0.00260135
Iteration 6/25 | Loss: 0.00259944
Iteration 7/25 | Loss: 0.00255563
Iteration 8/25 | Loss: 0.00245469
Iteration 9/25 | Loss: 0.00243067
Iteration 10/25 | Loss: 0.00238003
Iteration 11/25 | Loss: 0.00234859
Iteration 12/25 | Loss: 0.00235036
Iteration 13/25 | Loss: 0.00241362
Iteration 14/25 | Loss: 0.00235758
Iteration 15/25 | Loss: 0.00230719
Iteration 16/25 | Loss: 0.00228048
Iteration 17/25 | Loss: 0.00225567
Iteration 18/25 | Loss: 0.00223801
Iteration 19/25 | Loss: 0.00223375
Iteration 20/25 | Loss: 0.00223895
Iteration 21/25 | Loss: 0.00222726
Iteration 22/25 | Loss: 0.00222054
Iteration 23/25 | Loss: 0.00223157
Iteration 24/25 | Loss: 0.00222702
Iteration 25/25 | Loss: 0.00223195

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.55911100
Iteration 2/25 | Loss: 0.00277393
Iteration 3/25 | Loss: 0.00256496
Iteration 4/25 | Loss: 0.00256496
Iteration 5/25 | Loss: 0.00256492
Iteration 6/25 | Loss: 0.00256492
Iteration 7/25 | Loss: 0.00256492
Iteration 8/25 | Loss: 0.00256492
Iteration 9/25 | Loss: 0.00256492
Iteration 10/25 | Loss: 0.00256492
Iteration 11/25 | Loss: 0.00256492
Iteration 12/25 | Loss: 0.00256492
Iteration 13/25 | Loss: 0.00256492
Iteration 14/25 | Loss: 0.00256492
Iteration 15/25 | Loss: 0.00256492
Iteration 16/25 | Loss: 0.00256492
Iteration 17/25 | Loss: 0.00256492
Iteration 18/25 | Loss: 0.00256492
Iteration 19/25 | Loss: 0.00256492
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002564916154369712, 0.002564916154369712, 0.002564916154369712, 0.002564916154369712, 0.002564916154369712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002564916154369712

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00256492
Iteration 2/1000 | Loss: 0.00046943
Iteration 3/1000 | Loss: 0.00056062
Iteration 4/1000 | Loss: 0.00042937
Iteration 5/1000 | Loss: 0.00045186
Iteration 6/1000 | Loss: 0.00012043
Iteration 7/1000 | Loss: 0.00015519
Iteration 8/1000 | Loss: 0.00012439
Iteration 9/1000 | Loss: 0.00011592
Iteration 10/1000 | Loss: 0.00014394
Iteration 11/1000 | Loss: 0.00010660
Iteration 12/1000 | Loss: 0.00012042
Iteration 13/1000 | Loss: 0.00009480
Iteration 14/1000 | Loss: 0.00009100
Iteration 15/1000 | Loss: 0.00008935
Iteration 16/1000 | Loss: 0.00008802
Iteration 17/1000 | Loss: 0.00043099
Iteration 18/1000 | Loss: 0.00010859
Iteration 19/1000 | Loss: 0.00011339
Iteration 20/1000 | Loss: 0.00009194
Iteration 21/1000 | Loss: 0.00009907
Iteration 22/1000 | Loss: 0.00037740
Iteration 23/1000 | Loss: 0.00025763
Iteration 24/1000 | Loss: 0.00012603
Iteration 25/1000 | Loss: 0.00009677
Iteration 26/1000 | Loss: 0.00011587
Iteration 27/1000 | Loss: 0.00009638
Iteration 28/1000 | Loss: 0.00025853
Iteration 29/1000 | Loss: 0.00010803
Iteration 30/1000 | Loss: 0.00011722
Iteration 31/1000 | Loss: 0.00009570
Iteration 32/1000 | Loss: 0.00009463
Iteration 33/1000 | Loss: 0.00009307
Iteration 34/1000 | Loss: 0.00009915
Iteration 35/1000 | Loss: 0.00010284
Iteration 36/1000 | Loss: 0.00011257
Iteration 37/1000 | Loss: 0.00010703
Iteration 38/1000 | Loss: 0.00011877
Iteration 39/1000 | Loss: 0.00009849
Iteration 40/1000 | Loss: 0.00012699
Iteration 41/1000 | Loss: 0.00011145
Iteration 42/1000 | Loss: 0.00009840
Iteration 43/1000 | Loss: 0.00039584
Iteration 44/1000 | Loss: 0.00024944
Iteration 45/1000 | Loss: 0.00009205
Iteration 46/1000 | Loss: 0.00008316
Iteration 47/1000 | Loss: 0.00020607
Iteration 48/1000 | Loss: 0.00008033
Iteration 49/1000 | Loss: 0.00007941
Iteration 50/1000 | Loss: 0.00007859
Iteration 51/1000 | Loss: 0.00007814
Iteration 52/1000 | Loss: 0.00007783
Iteration 53/1000 | Loss: 0.00007766
Iteration 54/1000 | Loss: 0.00007758
Iteration 55/1000 | Loss: 0.00007743
Iteration 56/1000 | Loss: 0.00007738
Iteration 57/1000 | Loss: 0.00026716
Iteration 58/1000 | Loss: 0.00010318
Iteration 59/1000 | Loss: 0.00007729
Iteration 60/1000 | Loss: 0.00017022
Iteration 61/1000 | Loss: 0.00007725
Iteration 62/1000 | Loss: 0.00007706
Iteration 63/1000 | Loss: 0.00007698
Iteration 64/1000 | Loss: 0.00007697
Iteration 65/1000 | Loss: 0.00007694
Iteration 66/1000 | Loss: 0.00007694
Iteration 67/1000 | Loss: 0.00007694
Iteration 68/1000 | Loss: 0.00007693
Iteration 69/1000 | Loss: 0.00007693
Iteration 70/1000 | Loss: 0.00007693
Iteration 71/1000 | Loss: 0.00007692
Iteration 72/1000 | Loss: 0.00007692
Iteration 73/1000 | Loss: 0.00007691
Iteration 74/1000 | Loss: 0.00007691
Iteration 75/1000 | Loss: 0.00007691
Iteration 76/1000 | Loss: 0.00007690
Iteration 77/1000 | Loss: 0.00007690
Iteration 78/1000 | Loss: 0.00007690
Iteration 79/1000 | Loss: 0.00007689
Iteration 80/1000 | Loss: 0.00007689
Iteration 81/1000 | Loss: 0.00007688
Iteration 82/1000 | Loss: 0.00007688
Iteration 83/1000 | Loss: 0.00007688
Iteration 84/1000 | Loss: 0.00007688
Iteration 85/1000 | Loss: 0.00007688
Iteration 86/1000 | Loss: 0.00007687
Iteration 87/1000 | Loss: 0.00007687
Iteration 88/1000 | Loss: 0.00007687
Iteration 89/1000 | Loss: 0.00007687
Iteration 90/1000 | Loss: 0.00007686
Iteration 91/1000 | Loss: 0.00007686
Iteration 92/1000 | Loss: 0.00007686
Iteration 93/1000 | Loss: 0.00007686
Iteration 94/1000 | Loss: 0.00007685
Iteration 95/1000 | Loss: 0.00007685
Iteration 96/1000 | Loss: 0.00007685
Iteration 97/1000 | Loss: 0.00007684
Iteration 98/1000 | Loss: 0.00007684
Iteration 99/1000 | Loss: 0.00007683
Iteration 100/1000 | Loss: 0.00007683
Iteration 101/1000 | Loss: 0.00007682
Iteration 102/1000 | Loss: 0.00007682
Iteration 103/1000 | Loss: 0.00007682
Iteration 104/1000 | Loss: 0.00007682
Iteration 105/1000 | Loss: 0.00007682
Iteration 106/1000 | Loss: 0.00007682
Iteration 107/1000 | Loss: 0.00007682
Iteration 108/1000 | Loss: 0.00007682
Iteration 109/1000 | Loss: 0.00007682
Iteration 110/1000 | Loss: 0.00007682
Iteration 111/1000 | Loss: 0.00007682
Iteration 112/1000 | Loss: 0.00007681
Iteration 113/1000 | Loss: 0.00007681
Iteration 114/1000 | Loss: 0.00007681
Iteration 115/1000 | Loss: 0.00007680
Iteration 116/1000 | Loss: 0.00007680
Iteration 117/1000 | Loss: 0.00007680
Iteration 118/1000 | Loss: 0.00007680
Iteration 119/1000 | Loss: 0.00007680
Iteration 120/1000 | Loss: 0.00007680
Iteration 121/1000 | Loss: 0.00007679
Iteration 122/1000 | Loss: 0.00007679
Iteration 123/1000 | Loss: 0.00007679
Iteration 124/1000 | Loss: 0.00007679
Iteration 125/1000 | Loss: 0.00007679
Iteration 126/1000 | Loss: 0.00007679
Iteration 127/1000 | Loss: 0.00007679
Iteration 128/1000 | Loss: 0.00007678
Iteration 129/1000 | Loss: 0.00007678
Iteration 130/1000 | Loss: 0.00007678
Iteration 131/1000 | Loss: 0.00007678
Iteration 132/1000 | Loss: 0.00007678
Iteration 133/1000 | Loss: 0.00007678
Iteration 134/1000 | Loss: 0.00007677
Iteration 135/1000 | Loss: 0.00007677
Iteration 136/1000 | Loss: 0.00007677
Iteration 137/1000 | Loss: 0.00007677
Iteration 138/1000 | Loss: 0.00007677
Iteration 139/1000 | Loss: 0.00007676
Iteration 140/1000 | Loss: 0.00007676
Iteration 141/1000 | Loss: 0.00007676
Iteration 142/1000 | Loss: 0.00007676
Iteration 143/1000 | Loss: 0.00007676
Iteration 144/1000 | Loss: 0.00007676
Iteration 145/1000 | Loss: 0.00007676
Iteration 146/1000 | Loss: 0.00007676
Iteration 147/1000 | Loss: 0.00007676
Iteration 148/1000 | Loss: 0.00007676
Iteration 149/1000 | Loss: 0.00007676
Iteration 150/1000 | Loss: 0.00007676
Iteration 151/1000 | Loss: 0.00007676
Iteration 152/1000 | Loss: 0.00007676
Iteration 153/1000 | Loss: 0.00007676
Iteration 154/1000 | Loss: 0.00007676
Iteration 155/1000 | Loss: 0.00007676
Iteration 156/1000 | Loss: 0.00007676
Iteration 157/1000 | Loss: 0.00007676
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [7.675910455873236e-05, 7.675910455873236e-05, 7.675910455873236e-05, 7.675910455873236e-05, 7.675910455873236e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 7.675910455873236e-05

Optimization complete. Final v2v error: 7.062427997589111 mm

Highest mean error: 18.984642028808594 mm for frame 40

Lowest mean error: 6.1510419845581055 mm for frame 2

Saving results

Total time: 137.79972457885742
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00666728
Iteration 2/25 | Loss: 0.00309690
Iteration 3/25 | Loss: 0.00300782
Iteration 4/25 | Loss: 0.00298456
Iteration 5/25 | Loss: 0.00297549
Iteration 6/25 | Loss: 0.00297284
Iteration 7/25 | Loss: 0.00297276
Iteration 8/25 | Loss: 0.00297276
Iteration 9/25 | Loss: 0.00297276
Iteration 10/25 | Loss: 0.00297276
Iteration 11/25 | Loss: 0.00297276
Iteration 12/25 | Loss: 0.00297276
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.002972764428704977, 0.002972764428704977, 0.002972764428704977, 0.002972764428704977, 0.002972764428704977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002972764428704977

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50325906
Iteration 2/25 | Loss: 0.00555693
Iteration 3/25 | Loss: 0.00555693
Iteration 4/25 | Loss: 0.00555693
Iteration 5/25 | Loss: 0.00555692
Iteration 6/25 | Loss: 0.00555692
Iteration 7/25 | Loss: 0.00555692
Iteration 8/25 | Loss: 0.00555692
Iteration 9/25 | Loss: 0.00555692
Iteration 10/25 | Loss: 0.00555692
Iteration 11/25 | Loss: 0.00555692
Iteration 12/25 | Loss: 0.00555692
Iteration 13/25 | Loss: 0.00555692
Iteration 14/25 | Loss: 0.00555692
Iteration 15/25 | Loss: 0.00555692
Iteration 16/25 | Loss: 0.00555692
Iteration 17/25 | Loss: 0.00555692
Iteration 18/25 | Loss: 0.00555692
Iteration 19/25 | Loss: 0.00555692
Iteration 20/25 | Loss: 0.00555692
Iteration 21/25 | Loss: 0.00555692
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.005556922871619463, 0.005556922871619463, 0.005556922871619463, 0.005556922871619463, 0.005556922871619463]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.005556922871619463

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00555692
Iteration 2/1000 | Loss: 0.00020255
Iteration 3/1000 | Loss: 0.00012872
Iteration 4/1000 | Loss: 0.00010840
Iteration 5/1000 | Loss: 0.00010053
Iteration 6/1000 | Loss: 0.00009426
Iteration 7/1000 | Loss: 0.00008916
Iteration 8/1000 | Loss: 0.00008437
Iteration 9/1000 | Loss: 0.00008023
Iteration 10/1000 | Loss: 0.00007757
Iteration 11/1000 | Loss: 0.00007620
Iteration 12/1000 | Loss: 0.00007450
Iteration 13/1000 | Loss: 0.00007280
Iteration 14/1000 | Loss: 0.00007177
Iteration 15/1000 | Loss: 0.00007117
Iteration 16/1000 | Loss: 0.00007080
Iteration 17/1000 | Loss: 0.00007055
Iteration 18/1000 | Loss: 0.00007033
Iteration 19/1000 | Loss: 0.00007007
Iteration 20/1000 | Loss: 0.00006984
Iteration 21/1000 | Loss: 0.00006977
Iteration 22/1000 | Loss: 0.00006967
Iteration 23/1000 | Loss: 0.00006959
Iteration 24/1000 | Loss: 0.00006944
Iteration 25/1000 | Loss: 0.00006934
Iteration 26/1000 | Loss: 0.00006930
Iteration 27/1000 | Loss: 0.00006929
Iteration 28/1000 | Loss: 0.00006929
Iteration 29/1000 | Loss: 0.00006928
Iteration 30/1000 | Loss: 0.00006928
Iteration 31/1000 | Loss: 0.00006927
Iteration 32/1000 | Loss: 0.00006927
Iteration 33/1000 | Loss: 0.00006927
Iteration 34/1000 | Loss: 0.00006926
Iteration 35/1000 | Loss: 0.00006925
Iteration 36/1000 | Loss: 0.00006924
Iteration 37/1000 | Loss: 0.00006924
Iteration 38/1000 | Loss: 0.00006924
Iteration 39/1000 | Loss: 0.00006924
Iteration 40/1000 | Loss: 0.00006924
Iteration 41/1000 | Loss: 0.00006924
Iteration 42/1000 | Loss: 0.00006924
Iteration 43/1000 | Loss: 0.00006924
Iteration 44/1000 | Loss: 0.00006924
Iteration 45/1000 | Loss: 0.00006924
Iteration 46/1000 | Loss: 0.00006924
Iteration 47/1000 | Loss: 0.00006923
Iteration 48/1000 | Loss: 0.00006923
Iteration 49/1000 | Loss: 0.00006923
Iteration 50/1000 | Loss: 0.00006923
Iteration 51/1000 | Loss: 0.00006923
Iteration 52/1000 | Loss: 0.00006922
Iteration 53/1000 | Loss: 0.00006922
Iteration 54/1000 | Loss: 0.00006922
Iteration 55/1000 | Loss: 0.00006922
Iteration 56/1000 | Loss: 0.00006922
Iteration 57/1000 | Loss: 0.00006922
Iteration 58/1000 | Loss: 0.00006922
Iteration 59/1000 | Loss: 0.00006922
Iteration 60/1000 | Loss: 0.00006921
Iteration 61/1000 | Loss: 0.00006921
Iteration 62/1000 | Loss: 0.00006920
Iteration 63/1000 | Loss: 0.00006920
Iteration 64/1000 | Loss: 0.00006919
Iteration 65/1000 | Loss: 0.00006919
Iteration 66/1000 | Loss: 0.00006919
Iteration 67/1000 | Loss: 0.00006918
Iteration 68/1000 | Loss: 0.00006918
Iteration 69/1000 | Loss: 0.00006918
Iteration 70/1000 | Loss: 0.00006918
Iteration 71/1000 | Loss: 0.00006918
Iteration 72/1000 | Loss: 0.00006918
Iteration 73/1000 | Loss: 0.00006918
Iteration 74/1000 | Loss: 0.00006917
Iteration 75/1000 | Loss: 0.00006917
Iteration 76/1000 | Loss: 0.00006917
Iteration 77/1000 | Loss: 0.00006917
Iteration 78/1000 | Loss: 0.00006917
Iteration 79/1000 | Loss: 0.00006917
Iteration 80/1000 | Loss: 0.00006917
Iteration 81/1000 | Loss: 0.00006917
Iteration 82/1000 | Loss: 0.00006917
Iteration 83/1000 | Loss: 0.00006917
Iteration 84/1000 | Loss: 0.00006916
Iteration 85/1000 | Loss: 0.00006916
Iteration 86/1000 | Loss: 0.00006916
Iteration 87/1000 | Loss: 0.00006916
Iteration 88/1000 | Loss: 0.00006916
Iteration 89/1000 | Loss: 0.00006916
Iteration 90/1000 | Loss: 0.00006916
Iteration 91/1000 | Loss: 0.00006915
Iteration 92/1000 | Loss: 0.00006915
Iteration 93/1000 | Loss: 0.00006915
Iteration 94/1000 | Loss: 0.00006915
Iteration 95/1000 | Loss: 0.00006915
Iteration 96/1000 | Loss: 0.00006914
Iteration 97/1000 | Loss: 0.00006914
Iteration 98/1000 | Loss: 0.00006914
Iteration 99/1000 | Loss: 0.00006914
Iteration 100/1000 | Loss: 0.00006914
Iteration 101/1000 | Loss: 0.00006914
Iteration 102/1000 | Loss: 0.00006914
Iteration 103/1000 | Loss: 0.00006914
Iteration 104/1000 | Loss: 0.00006913
Iteration 105/1000 | Loss: 0.00006913
Iteration 106/1000 | Loss: 0.00006913
Iteration 107/1000 | Loss: 0.00006913
Iteration 108/1000 | Loss: 0.00006913
Iteration 109/1000 | Loss: 0.00006913
Iteration 110/1000 | Loss: 0.00006913
Iteration 111/1000 | Loss: 0.00006913
Iteration 112/1000 | Loss: 0.00006913
Iteration 113/1000 | Loss: 0.00006912
Iteration 114/1000 | Loss: 0.00006912
Iteration 115/1000 | Loss: 0.00006912
Iteration 116/1000 | Loss: 0.00006912
Iteration 117/1000 | Loss: 0.00006911
Iteration 118/1000 | Loss: 0.00006911
Iteration 119/1000 | Loss: 0.00006911
Iteration 120/1000 | Loss: 0.00006911
Iteration 121/1000 | Loss: 0.00006911
Iteration 122/1000 | Loss: 0.00006911
Iteration 123/1000 | Loss: 0.00006911
Iteration 124/1000 | Loss: 0.00006911
Iteration 125/1000 | Loss: 0.00006911
Iteration 126/1000 | Loss: 0.00006911
Iteration 127/1000 | Loss: 0.00006911
Iteration 128/1000 | Loss: 0.00006911
Iteration 129/1000 | Loss: 0.00006911
Iteration 130/1000 | Loss: 0.00006911
Iteration 131/1000 | Loss: 0.00006911
Iteration 132/1000 | Loss: 0.00006910
Iteration 133/1000 | Loss: 0.00006910
Iteration 134/1000 | Loss: 0.00006910
Iteration 135/1000 | Loss: 0.00006910
Iteration 136/1000 | Loss: 0.00006910
Iteration 137/1000 | Loss: 0.00006910
Iteration 138/1000 | Loss: 0.00006910
Iteration 139/1000 | Loss: 0.00006910
Iteration 140/1000 | Loss: 0.00006910
Iteration 141/1000 | Loss: 0.00006910
Iteration 142/1000 | Loss: 0.00006910
Iteration 143/1000 | Loss: 0.00006910
Iteration 144/1000 | Loss: 0.00006909
Iteration 145/1000 | Loss: 0.00006909
Iteration 146/1000 | Loss: 0.00006909
Iteration 147/1000 | Loss: 0.00006909
Iteration 148/1000 | Loss: 0.00006909
Iteration 149/1000 | Loss: 0.00006909
Iteration 150/1000 | Loss: 0.00006909
Iteration 151/1000 | Loss: 0.00006909
Iteration 152/1000 | Loss: 0.00006909
Iteration 153/1000 | Loss: 0.00006909
Iteration 154/1000 | Loss: 0.00006909
Iteration 155/1000 | Loss: 0.00006908
Iteration 156/1000 | Loss: 0.00006908
Iteration 157/1000 | Loss: 0.00006908
Iteration 158/1000 | Loss: 0.00006908
Iteration 159/1000 | Loss: 0.00006908
Iteration 160/1000 | Loss: 0.00006908
Iteration 161/1000 | Loss: 0.00006908
Iteration 162/1000 | Loss: 0.00006908
Iteration 163/1000 | Loss: 0.00006908
Iteration 164/1000 | Loss: 0.00006908
Iteration 165/1000 | Loss: 0.00006908
Iteration 166/1000 | Loss: 0.00006908
Iteration 167/1000 | Loss: 0.00006908
Iteration 168/1000 | Loss: 0.00006908
Iteration 169/1000 | Loss: 0.00006908
Iteration 170/1000 | Loss: 0.00006908
Iteration 171/1000 | Loss: 0.00006907
Iteration 172/1000 | Loss: 0.00006907
Iteration 173/1000 | Loss: 0.00006907
Iteration 174/1000 | Loss: 0.00006907
Iteration 175/1000 | Loss: 0.00006907
Iteration 176/1000 | Loss: 0.00006907
Iteration 177/1000 | Loss: 0.00006907
Iteration 178/1000 | Loss: 0.00006907
Iteration 179/1000 | Loss: 0.00006907
Iteration 180/1000 | Loss: 0.00006907
Iteration 181/1000 | Loss: 0.00006907
Iteration 182/1000 | Loss: 0.00006907
Iteration 183/1000 | Loss: 0.00006907
Iteration 184/1000 | Loss: 0.00006907
Iteration 185/1000 | Loss: 0.00006906
Iteration 186/1000 | Loss: 0.00006906
Iteration 187/1000 | Loss: 0.00006906
Iteration 188/1000 | Loss: 0.00006906
Iteration 189/1000 | Loss: 0.00006906
Iteration 190/1000 | Loss: 0.00006906
Iteration 191/1000 | Loss: 0.00006906
Iteration 192/1000 | Loss: 0.00006906
Iteration 193/1000 | Loss: 0.00006905
Iteration 194/1000 | Loss: 0.00006905
Iteration 195/1000 | Loss: 0.00006905
Iteration 196/1000 | Loss: 0.00006905
Iteration 197/1000 | Loss: 0.00006905
Iteration 198/1000 | Loss: 0.00006905
Iteration 199/1000 | Loss: 0.00006905
Iteration 200/1000 | Loss: 0.00006905
Iteration 201/1000 | Loss: 0.00006905
Iteration 202/1000 | Loss: 0.00006905
Iteration 203/1000 | Loss: 0.00006905
Iteration 204/1000 | Loss: 0.00006905
Iteration 205/1000 | Loss: 0.00006905
Iteration 206/1000 | Loss: 0.00006905
Iteration 207/1000 | Loss: 0.00006905
Iteration 208/1000 | Loss: 0.00006905
Iteration 209/1000 | Loss: 0.00006905
Iteration 210/1000 | Loss: 0.00006905
Iteration 211/1000 | Loss: 0.00006905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [6.905161717440933e-05, 6.905161717440933e-05, 6.905161717440933e-05, 6.905161717440933e-05, 6.905161717440933e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.905161717440933e-05

Optimization complete. Final v2v error: 7.234681129455566 mm

Highest mean error: 7.470813274383545 mm for frame 116

Lowest mean error: 6.930444240570068 mm for frame 192

Saving results

Total time: 56.48264122009277
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01245547
Iteration 2/25 | Loss: 0.00318073
Iteration 3/25 | Loss: 0.00298332
Iteration 4/25 | Loss: 0.00300392
Iteration 5/25 | Loss: 0.00277692
Iteration 6/25 | Loss: 0.00264669
Iteration 7/25 | Loss: 0.00257766
Iteration 8/25 | Loss: 0.00258979
Iteration 9/25 | Loss: 0.00254492
Iteration 10/25 | Loss: 0.00254188
Iteration 11/25 | Loss: 0.00256615
Iteration 12/25 | Loss: 0.00255396
Iteration 13/25 | Loss: 0.00252427
Iteration 14/25 | Loss: 0.00252801
Iteration 15/25 | Loss: 0.00252189
Iteration 16/25 | Loss: 0.00252175
Iteration 17/25 | Loss: 0.00252165
Iteration 18/25 | Loss: 0.00252154
Iteration 19/25 | Loss: 0.00252138
Iteration 20/25 | Loss: 0.00252121
Iteration 21/25 | Loss: 0.00252109
Iteration 22/25 | Loss: 0.00252086
Iteration 23/25 | Loss: 0.00252051
Iteration 24/25 | Loss: 0.00252031
Iteration 25/25 | Loss: 0.00252023

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.45384109
Iteration 2/25 | Loss: 0.00393240
Iteration 3/25 | Loss: 0.00307933
Iteration 4/25 | Loss: 0.00307933
Iteration 5/25 | Loss: 0.00307933
Iteration 6/25 | Loss: 0.00307933
Iteration 7/25 | Loss: 0.00307933
Iteration 8/25 | Loss: 0.00307933
Iteration 9/25 | Loss: 0.00307933
Iteration 10/25 | Loss: 0.00307933
Iteration 11/25 | Loss: 0.00307933
Iteration 12/25 | Loss: 0.00307933
Iteration 13/25 | Loss: 0.00307933
Iteration 14/25 | Loss: 0.00307933
Iteration 15/25 | Loss: 0.00307933
Iteration 16/25 | Loss: 0.00307933
Iteration 17/25 | Loss: 0.00307933
Iteration 18/25 | Loss: 0.00307933
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.003079329850152135, 0.003079329850152135, 0.003079329850152135, 0.003079329850152135, 0.003079329850152135]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.003079329850152135

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00307933
Iteration 2/1000 | Loss: 0.00020936
Iteration 3/1000 | Loss: 0.00077787
Iteration 4/1000 | Loss: 0.00172004
Iteration 5/1000 | Loss: 0.00015900
Iteration 6/1000 | Loss: 0.00012093
Iteration 7/1000 | Loss: 0.00009893
Iteration 8/1000 | Loss: 0.00008948
Iteration 9/1000 | Loss: 0.00008331
Iteration 10/1000 | Loss: 0.00095382
Iteration 11/1000 | Loss: 0.00049502
Iteration 12/1000 | Loss: 0.00047089
Iteration 13/1000 | Loss: 0.00008574
Iteration 14/1000 | Loss: 0.00007859
Iteration 15/1000 | Loss: 0.00007491
Iteration 16/1000 | Loss: 0.00007319
Iteration 17/1000 | Loss: 0.00007221
Iteration 18/1000 | Loss: 0.00007160
Iteration 19/1000 | Loss: 0.00007080
Iteration 20/1000 | Loss: 0.00007036
Iteration 21/1000 | Loss: 0.00007000
Iteration 22/1000 | Loss: 0.00006978
Iteration 23/1000 | Loss: 0.00006958
Iteration 24/1000 | Loss: 0.00006956
Iteration 25/1000 | Loss: 0.00006948
Iteration 26/1000 | Loss: 0.00006946
Iteration 27/1000 | Loss: 0.00006942
Iteration 28/1000 | Loss: 0.00006941
Iteration 29/1000 | Loss: 0.00006938
Iteration 30/1000 | Loss: 0.00006935
Iteration 31/1000 | Loss: 0.00006934
Iteration 32/1000 | Loss: 0.00006934
Iteration 33/1000 | Loss: 0.00006933
Iteration 34/1000 | Loss: 0.00006933
Iteration 35/1000 | Loss: 0.00006933
Iteration 36/1000 | Loss: 0.00006932
Iteration 37/1000 | Loss: 0.00006932
Iteration 38/1000 | Loss: 0.00006932
Iteration 39/1000 | Loss: 0.00006931
Iteration 40/1000 | Loss: 0.00006931
Iteration 41/1000 | Loss: 0.00006931
Iteration 42/1000 | Loss: 0.00006931
Iteration 43/1000 | Loss: 0.00006931
Iteration 44/1000 | Loss: 0.00006931
Iteration 45/1000 | Loss: 0.00006931
Iteration 46/1000 | Loss: 0.00006931
Iteration 47/1000 | Loss: 0.00006930
Iteration 48/1000 | Loss: 0.00006930
Iteration 49/1000 | Loss: 0.00006930
Iteration 50/1000 | Loss: 0.00006929
Iteration 51/1000 | Loss: 0.00006929
Iteration 52/1000 | Loss: 0.00006929
Iteration 53/1000 | Loss: 0.00006929
Iteration 54/1000 | Loss: 0.00006929
Iteration 55/1000 | Loss: 0.00006929
Iteration 56/1000 | Loss: 0.00006928
Iteration 57/1000 | Loss: 0.00006928
Iteration 58/1000 | Loss: 0.00006928
Iteration 59/1000 | Loss: 0.00006928
Iteration 60/1000 | Loss: 0.00006928
Iteration 61/1000 | Loss: 0.00006928
Iteration 62/1000 | Loss: 0.00006927
Iteration 63/1000 | Loss: 0.00006927
Iteration 64/1000 | Loss: 0.00006927
Iteration 65/1000 | Loss: 0.00006926
Iteration 66/1000 | Loss: 0.00006926
Iteration 67/1000 | Loss: 0.00006926
Iteration 68/1000 | Loss: 0.00006926
Iteration 69/1000 | Loss: 0.00006926
Iteration 70/1000 | Loss: 0.00006926
Iteration 71/1000 | Loss: 0.00006926
Iteration 72/1000 | Loss: 0.00006926
Iteration 73/1000 | Loss: 0.00006926
Iteration 74/1000 | Loss: 0.00006926
Iteration 75/1000 | Loss: 0.00006926
Iteration 76/1000 | Loss: 0.00006926
Iteration 77/1000 | Loss: 0.00006926
Iteration 78/1000 | Loss: 0.00006926
Iteration 79/1000 | Loss: 0.00006926
Iteration 80/1000 | Loss: 0.00006926
Iteration 81/1000 | Loss: 0.00006926
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 81. Stopping optimization.
Last 5 losses: [6.925925117684528e-05, 6.925925117684528e-05, 6.925925117684528e-05, 6.925925117684528e-05, 6.925925117684528e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.925925117684528e-05

Optimization complete. Final v2v error: 7.331080436706543 mm

Highest mean error: 12.962748527526855 mm for frame 99

Lowest mean error: 7.028028964996338 mm for frame 86

Saving results

Total time: 83.42291045188904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01278331
Iteration 2/25 | Loss: 0.00400695
Iteration 3/25 | Loss: 0.00343519
Iteration 4/25 | Loss: 0.00336130
Iteration 5/25 | Loss: 0.00319038
Iteration 6/25 | Loss: 0.00311059
Iteration 7/25 | Loss: 0.00307745
Iteration 8/25 | Loss: 0.00303753
Iteration 9/25 | Loss: 0.00301477
Iteration 10/25 | Loss: 0.00300775
Iteration 11/25 | Loss: 0.00300624
Iteration 12/25 | Loss: 0.00300705
Iteration 13/25 | Loss: 0.00300668
Iteration 14/25 | Loss: 0.00301202
Iteration 15/25 | Loss: 0.00300645
Iteration 16/25 | Loss: 0.00300301
Iteration 17/25 | Loss: 0.00301153
Iteration 18/25 | Loss: 0.00300209
Iteration 19/25 | Loss: 0.00299295
Iteration 20/25 | Loss: 0.00299695
Iteration 21/25 | Loss: 0.00299468
Iteration 22/25 | Loss: 0.00299236
Iteration 23/25 | Loss: 0.00298761
Iteration 24/25 | Loss: 0.00298656
Iteration 25/25 | Loss: 0.00298636

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.84490418
Iteration 2/25 | Loss: 0.00609455
Iteration 3/25 | Loss: 0.00609455
Iteration 4/25 | Loss: 0.00609455
Iteration 5/25 | Loss: 0.00609455
Iteration 6/25 | Loss: 0.00609455
Iteration 7/25 | Loss: 0.00609455
Iteration 8/25 | Loss: 0.00609455
Iteration 9/25 | Loss: 0.00609455
Iteration 10/25 | Loss: 0.00609455
Iteration 11/25 | Loss: 0.00609455
Iteration 12/25 | Loss: 0.00609455
Iteration 13/25 | Loss: 0.00609455
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0060945479199290276, 0.0060945479199290276, 0.0060945479199290276, 0.0060945479199290276, 0.0060945479199290276]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0060945479199290276

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00609455
Iteration 2/1000 | Loss: 0.00061468
Iteration 3/1000 | Loss: 0.00063828
Iteration 4/1000 | Loss: 0.00038984
Iteration 5/1000 | Loss: 0.00033621
Iteration 6/1000 | Loss: 0.00041392
Iteration 7/1000 | Loss: 0.00222738
Iteration 8/1000 | Loss: 0.00233631
Iteration 9/1000 | Loss: 0.00064255
Iteration 10/1000 | Loss: 0.00121784
Iteration 11/1000 | Loss: 0.00093947
Iteration 12/1000 | Loss: 0.00039236
Iteration 13/1000 | Loss: 0.00037631
Iteration 14/1000 | Loss: 0.00029593
Iteration 15/1000 | Loss: 0.00100601
Iteration 16/1000 | Loss: 0.00170618
Iteration 17/1000 | Loss: 0.00035451
Iteration 18/1000 | Loss: 0.00081920
Iteration 19/1000 | Loss: 0.00190011
Iteration 20/1000 | Loss: 0.00613269
Iteration 21/1000 | Loss: 0.01089394
Iteration 22/1000 | Loss: 0.00450496
Iteration 23/1000 | Loss: 0.00536680
Iteration 24/1000 | Loss: 0.00229829
Iteration 25/1000 | Loss: 0.00303453
Iteration 26/1000 | Loss: 0.00224290
Iteration 27/1000 | Loss: 0.00151180
Iteration 28/1000 | Loss: 0.00218347
Iteration 29/1000 | Loss: 0.00341692
Iteration 30/1000 | Loss: 0.00266273
Iteration 31/1000 | Loss: 0.00258786
Iteration 32/1000 | Loss: 0.00273167
Iteration 33/1000 | Loss: 0.00219308
Iteration 34/1000 | Loss: 0.00275383
Iteration 35/1000 | Loss: 0.00224868
Iteration 36/1000 | Loss: 0.00206463
Iteration 37/1000 | Loss: 0.00148657
Iteration 38/1000 | Loss: 0.00210054
Iteration 39/1000 | Loss: 0.00158867
Iteration 40/1000 | Loss: 0.00178563
Iteration 41/1000 | Loss: 0.00221999
Iteration 42/1000 | Loss: 0.00215944
Iteration 43/1000 | Loss: 0.00234808
Iteration 44/1000 | Loss: 0.00258428
Iteration 45/1000 | Loss: 0.00335372
Iteration 46/1000 | Loss: 0.00255913
Iteration 47/1000 | Loss: 0.00333522
Iteration 48/1000 | Loss: 0.00140394
Iteration 49/1000 | Loss: 0.00191980
Iteration 50/1000 | Loss: 0.00179508
Iteration 51/1000 | Loss: 0.00186090
Iteration 52/1000 | Loss: 0.00268375
Iteration 53/1000 | Loss: 0.00180472
Iteration 54/1000 | Loss: 0.00196902
Iteration 55/1000 | Loss: 0.00193228
Iteration 56/1000 | Loss: 0.00155317
Iteration 57/1000 | Loss: 0.00226802
Iteration 58/1000 | Loss: 0.00214560
Iteration 59/1000 | Loss: 0.00248565
Iteration 60/1000 | Loss: 0.00129205
Iteration 61/1000 | Loss: 0.00157376
Iteration 62/1000 | Loss: 0.00201983
Iteration 63/1000 | Loss: 0.00167021
Iteration 64/1000 | Loss: 0.00177735
Iteration 65/1000 | Loss: 0.00172414
Iteration 66/1000 | Loss: 0.00244763
Iteration 67/1000 | Loss: 0.00111334
Iteration 68/1000 | Loss: 0.00220687
Iteration 69/1000 | Loss: 0.00143934
Iteration 70/1000 | Loss: 0.00189792
Iteration 71/1000 | Loss: 0.00173632
Iteration 72/1000 | Loss: 0.00119624
Iteration 73/1000 | Loss: 0.00116442
Iteration 74/1000 | Loss: 0.00144781
Iteration 75/1000 | Loss: 0.00181523
Iteration 76/1000 | Loss: 0.00116595
Iteration 77/1000 | Loss: 0.00208229
Iteration 78/1000 | Loss: 0.00275990
Iteration 79/1000 | Loss: 0.00264090
Iteration 80/1000 | Loss: 0.00201467
Iteration 81/1000 | Loss: 0.00243043
Iteration 82/1000 | Loss: 0.00291115
Iteration 83/1000 | Loss: 0.00200796
Iteration 84/1000 | Loss: 0.00181462
Iteration 85/1000 | Loss: 0.00218546
Iteration 86/1000 | Loss: 0.00033599
Iteration 87/1000 | Loss: 0.00058870
Iteration 88/1000 | Loss: 0.00031180
Iteration 89/1000 | Loss: 0.00022730
Iteration 90/1000 | Loss: 0.00020826
Iteration 91/1000 | Loss: 0.00022001
Iteration 92/1000 | Loss: 0.00019465
Iteration 93/1000 | Loss: 0.00018567
Iteration 94/1000 | Loss: 0.00023370
Iteration 95/1000 | Loss: 0.00117461
Iteration 96/1000 | Loss: 0.00051795
Iteration 97/1000 | Loss: 0.00055461
Iteration 98/1000 | Loss: 0.00081669
Iteration 99/1000 | Loss: 0.00071434
Iteration 100/1000 | Loss: 0.00042593
Iteration 101/1000 | Loss: 0.00018323
Iteration 102/1000 | Loss: 0.00065824
Iteration 103/1000 | Loss: 0.00044752
Iteration 104/1000 | Loss: 0.00056322
Iteration 105/1000 | Loss: 0.00046582
Iteration 106/1000 | Loss: 0.00019713
Iteration 107/1000 | Loss: 0.00017785
Iteration 108/1000 | Loss: 0.00016891
Iteration 109/1000 | Loss: 0.00019949
Iteration 110/1000 | Loss: 0.00019407
Iteration 111/1000 | Loss: 0.00016855
Iteration 112/1000 | Loss: 0.00020426
Iteration 113/1000 | Loss: 0.00020265
Iteration 114/1000 | Loss: 0.00021173
Iteration 115/1000 | Loss: 0.00020460
Iteration 116/1000 | Loss: 0.00020755
Iteration 117/1000 | Loss: 0.00019893
Iteration 118/1000 | Loss: 0.00021405
Iteration 119/1000 | Loss: 0.00019409
Iteration 120/1000 | Loss: 0.00021451
Iteration 121/1000 | Loss: 0.00019788
Iteration 122/1000 | Loss: 0.00021433
Iteration 123/1000 | Loss: 0.00019533
Iteration 124/1000 | Loss: 0.00021621
Iteration 125/1000 | Loss: 0.00052477
Iteration 126/1000 | Loss: 0.00067186
Iteration 127/1000 | Loss: 0.00054497
Iteration 128/1000 | Loss: 0.00059615
Iteration 129/1000 | Loss: 0.00038716
Iteration 130/1000 | Loss: 0.00043509
Iteration 131/1000 | Loss: 0.00034394
Iteration 132/1000 | Loss: 0.00052373
Iteration 133/1000 | Loss: 0.00026575
Iteration 134/1000 | Loss: 0.00040613
Iteration 135/1000 | Loss: 0.00053256
Iteration 136/1000 | Loss: 0.00018604
Iteration 137/1000 | Loss: 0.00016638
Iteration 138/1000 | Loss: 0.00016171
Iteration 139/1000 | Loss: 0.00015902
Iteration 140/1000 | Loss: 0.00015696
Iteration 141/1000 | Loss: 0.00015562
Iteration 142/1000 | Loss: 0.00015492
Iteration 143/1000 | Loss: 0.00046581
Iteration 144/1000 | Loss: 0.00142215
Iteration 145/1000 | Loss: 0.00043969
Iteration 146/1000 | Loss: 0.00078502
Iteration 147/1000 | Loss: 0.00078972
Iteration 148/1000 | Loss: 0.00064595
Iteration 149/1000 | Loss: 0.00020992
Iteration 150/1000 | Loss: 0.00092325
Iteration 151/1000 | Loss: 0.00069557
Iteration 152/1000 | Loss: 0.00250293
Iteration 153/1000 | Loss: 0.00131598
Iteration 154/1000 | Loss: 0.00226237
Iteration 155/1000 | Loss: 0.00181004
Iteration 156/1000 | Loss: 0.00060360
Iteration 157/1000 | Loss: 0.00046771
Iteration 158/1000 | Loss: 0.00019412
Iteration 159/1000 | Loss: 0.00017564
Iteration 160/1000 | Loss: 0.00194559
Iteration 161/1000 | Loss: 0.00077179
Iteration 162/1000 | Loss: 0.00255625
Iteration 163/1000 | Loss: 0.00206817
Iteration 164/1000 | Loss: 0.00134796
Iteration 165/1000 | Loss: 0.00113784
Iteration 166/1000 | Loss: 0.00073544
Iteration 167/1000 | Loss: 0.00087227
Iteration 168/1000 | Loss: 0.00128627
Iteration 169/1000 | Loss: 0.00130809
Iteration 170/1000 | Loss: 0.00080744
Iteration 171/1000 | Loss: 0.00087441
Iteration 172/1000 | Loss: 0.00083074
Iteration 173/1000 | Loss: 0.00080763
Iteration 174/1000 | Loss: 0.00089856
Iteration 175/1000 | Loss: 0.00050003
Iteration 176/1000 | Loss: 0.00075578
Iteration 177/1000 | Loss: 0.00069941
Iteration 178/1000 | Loss: 0.00120198
Iteration 179/1000 | Loss: 0.00066962
Iteration 180/1000 | Loss: 0.00075821
Iteration 181/1000 | Loss: 0.00068310
Iteration 182/1000 | Loss: 0.00053140
Iteration 183/1000 | Loss: 0.00067083
Iteration 184/1000 | Loss: 0.00058409
Iteration 185/1000 | Loss: 0.00082981
Iteration 186/1000 | Loss: 0.00069322
Iteration 187/1000 | Loss: 0.00053574
Iteration 188/1000 | Loss: 0.00076740
Iteration 189/1000 | Loss: 0.00062984
Iteration 190/1000 | Loss: 0.00067010
Iteration 191/1000 | Loss: 0.00050330
Iteration 192/1000 | Loss: 0.00069577
Iteration 193/1000 | Loss: 0.00070673
Iteration 194/1000 | Loss: 0.00057440
Iteration 195/1000 | Loss: 0.00102138
Iteration 196/1000 | Loss: 0.00056081
Iteration 197/1000 | Loss: 0.00060565
Iteration 198/1000 | Loss: 0.00056499
Iteration 199/1000 | Loss: 0.00078673
Iteration 200/1000 | Loss: 0.00058932
Iteration 201/1000 | Loss: 0.00071421
Iteration 202/1000 | Loss: 0.00040585
Iteration 203/1000 | Loss: 0.00053287
Iteration 204/1000 | Loss: 0.00054484
Iteration 205/1000 | Loss: 0.00081255
Iteration 206/1000 | Loss: 0.00053149
Iteration 207/1000 | Loss: 0.00066333
Iteration 208/1000 | Loss: 0.00018670
Iteration 209/1000 | Loss: 0.00057019
Iteration 210/1000 | Loss: 0.00075967
Iteration 211/1000 | Loss: 0.00109976
Iteration 212/1000 | Loss: 0.00098588
Iteration 213/1000 | Loss: 0.00017350
Iteration 214/1000 | Loss: 0.00016222
Iteration 215/1000 | Loss: 0.00104010
Iteration 216/1000 | Loss: 0.00016485
Iteration 217/1000 | Loss: 0.00015354
Iteration 218/1000 | Loss: 0.00014969
Iteration 219/1000 | Loss: 0.00094986
Iteration 220/1000 | Loss: 0.00046833
Iteration 221/1000 | Loss: 0.00034838
Iteration 222/1000 | Loss: 0.00015787
Iteration 223/1000 | Loss: 0.00015294
Iteration 224/1000 | Loss: 0.00015008
Iteration 225/1000 | Loss: 0.00014674
Iteration 226/1000 | Loss: 0.00014349
Iteration 227/1000 | Loss: 0.00014045
Iteration 228/1000 | Loss: 0.00013819
Iteration 229/1000 | Loss: 0.00013667
Iteration 230/1000 | Loss: 0.00013544
Iteration 231/1000 | Loss: 0.00013400
Iteration 232/1000 | Loss: 0.00013300
Iteration 233/1000 | Loss: 0.00013228
Iteration 234/1000 | Loss: 0.00013163
Iteration 235/1000 | Loss: 0.00013113
Iteration 236/1000 | Loss: 0.00013087
Iteration 237/1000 | Loss: 0.00013075
Iteration 238/1000 | Loss: 0.00013056
Iteration 239/1000 | Loss: 0.00013047
Iteration 240/1000 | Loss: 0.00013046
Iteration 241/1000 | Loss: 0.00013045
Iteration 242/1000 | Loss: 0.00013045
Iteration 243/1000 | Loss: 0.00013045
Iteration 244/1000 | Loss: 0.00013045
Iteration 245/1000 | Loss: 0.00013045
Iteration 246/1000 | Loss: 0.00013045
Iteration 247/1000 | Loss: 0.00013044
Iteration 248/1000 | Loss: 0.00013044
Iteration 249/1000 | Loss: 0.00013044
Iteration 250/1000 | Loss: 0.00013044
Iteration 251/1000 | Loss: 0.00013044
Iteration 252/1000 | Loss: 0.00013044
Iteration 253/1000 | Loss: 0.00013044
Iteration 254/1000 | Loss: 0.00013044
Iteration 255/1000 | Loss: 0.00013044
Iteration 256/1000 | Loss: 0.00013043
Iteration 257/1000 | Loss: 0.00013043
Iteration 258/1000 | Loss: 0.00013043
Iteration 259/1000 | Loss: 0.00013043
Iteration 260/1000 | Loss: 0.00013042
Iteration 261/1000 | Loss: 0.00013042
Iteration 262/1000 | Loss: 0.00013042
Iteration 263/1000 | Loss: 0.00013042
Iteration 264/1000 | Loss: 0.00013042
Iteration 265/1000 | Loss: 0.00013042
Iteration 266/1000 | Loss: 0.00013042
Iteration 267/1000 | Loss: 0.00013042
Iteration 268/1000 | Loss: 0.00013042
Iteration 269/1000 | Loss: 0.00013042
Iteration 270/1000 | Loss: 0.00013042
Iteration 271/1000 | Loss: 0.00013042
Iteration 272/1000 | Loss: 0.00013042
Iteration 273/1000 | Loss: 0.00013042
Iteration 274/1000 | Loss: 0.00013042
Iteration 275/1000 | Loss: 0.00013042
Iteration 276/1000 | Loss: 0.00013042
Iteration 277/1000 | Loss: 0.00013042
Iteration 278/1000 | Loss: 0.00013042
Iteration 279/1000 | Loss: 0.00013042
Iteration 280/1000 | Loss: 0.00013042
Iteration 281/1000 | Loss: 0.00013042
Iteration 282/1000 | Loss: 0.00013042
Iteration 283/1000 | Loss: 0.00013042
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [0.00013042031787335873, 0.00013042031787335873, 0.00013042031787335873, 0.00013042031787335873, 0.00013042031787335873]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00013042031787335873

Optimization complete. Final v2v error: 8.519838333129883 mm

Highest mean error: 16.429737091064453 mm for frame 52

Lowest mean error: 7.403291702270508 mm for frame 23

Saving results

Total time: 379.8029923439026
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01009907
Iteration 2/25 | Loss: 0.00317224
Iteration 3/25 | Loss: 0.00287878
Iteration 4/25 | Loss: 0.00285643
Iteration 5/25 | Loss: 0.00285127
Iteration 6/25 | Loss: 0.00285019
Iteration 7/25 | Loss: 0.00285019
Iteration 8/25 | Loss: 0.00285019
Iteration 9/25 | Loss: 0.00285019
Iteration 10/25 | Loss: 0.00285019
Iteration 11/25 | Loss: 0.00285019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0028501919005066156, 0.0028501919005066156, 0.0028501919005066156, 0.0028501919005066156, 0.0028501919005066156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028501919005066156

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35744023
Iteration 2/25 | Loss: 0.00417894
Iteration 3/25 | Loss: 0.00417894
Iteration 4/25 | Loss: 0.00417894
Iteration 5/25 | Loss: 0.00417894
Iteration 6/25 | Loss: 0.00417894
Iteration 7/25 | Loss: 0.00417894
Iteration 8/25 | Loss: 0.00417894
Iteration 9/25 | Loss: 0.00417894
Iteration 10/25 | Loss: 0.00417894
Iteration 11/25 | Loss: 0.00417894
Iteration 12/25 | Loss: 0.00417894
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.004178937524557114, 0.004178937524557114, 0.004178937524557114, 0.004178937524557114, 0.004178937524557114]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004178937524557114

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00417894
Iteration 2/1000 | Loss: 0.00017788
Iteration 3/1000 | Loss: 0.00012129
Iteration 4/1000 | Loss: 0.00008897
Iteration 5/1000 | Loss: 0.00008008
Iteration 6/1000 | Loss: 0.00007582
Iteration 7/1000 | Loss: 0.00007321
Iteration 8/1000 | Loss: 0.00007184
Iteration 9/1000 | Loss: 0.00007086
Iteration 10/1000 | Loss: 0.00007013
Iteration 11/1000 | Loss: 0.00006957
Iteration 12/1000 | Loss: 0.00006927
Iteration 13/1000 | Loss: 0.00006903
Iteration 14/1000 | Loss: 0.00006896
Iteration 15/1000 | Loss: 0.00006896
Iteration 16/1000 | Loss: 0.00006894
Iteration 17/1000 | Loss: 0.00006891
Iteration 18/1000 | Loss: 0.00006890
Iteration 19/1000 | Loss: 0.00006889
Iteration 20/1000 | Loss: 0.00006886
Iteration 21/1000 | Loss: 0.00006886
Iteration 22/1000 | Loss: 0.00006885
Iteration 23/1000 | Loss: 0.00006885
Iteration 24/1000 | Loss: 0.00006885
Iteration 25/1000 | Loss: 0.00006885
Iteration 26/1000 | Loss: 0.00006885
Iteration 27/1000 | Loss: 0.00006880
Iteration 28/1000 | Loss: 0.00006877
Iteration 29/1000 | Loss: 0.00006877
Iteration 30/1000 | Loss: 0.00006876
Iteration 31/1000 | Loss: 0.00006876
Iteration 32/1000 | Loss: 0.00006876
Iteration 33/1000 | Loss: 0.00006876
Iteration 34/1000 | Loss: 0.00006876
Iteration 35/1000 | Loss: 0.00006875
Iteration 36/1000 | Loss: 0.00006875
Iteration 37/1000 | Loss: 0.00006875
Iteration 38/1000 | Loss: 0.00006875
Iteration 39/1000 | Loss: 0.00006875
Iteration 40/1000 | Loss: 0.00006875
Iteration 41/1000 | Loss: 0.00006875
Iteration 42/1000 | Loss: 0.00006875
Iteration 43/1000 | Loss: 0.00006875
Iteration 44/1000 | Loss: 0.00006875
Iteration 45/1000 | Loss: 0.00006875
Iteration 46/1000 | Loss: 0.00006875
Iteration 47/1000 | Loss: 0.00006873
Iteration 48/1000 | Loss: 0.00006873
Iteration 49/1000 | Loss: 0.00006872
Iteration 50/1000 | Loss: 0.00006872
Iteration 51/1000 | Loss: 0.00006872
Iteration 52/1000 | Loss: 0.00006872
Iteration 53/1000 | Loss: 0.00006872
Iteration 54/1000 | Loss: 0.00006871
Iteration 55/1000 | Loss: 0.00006871
Iteration 56/1000 | Loss: 0.00006870
Iteration 57/1000 | Loss: 0.00006870
Iteration 58/1000 | Loss: 0.00006869
Iteration 59/1000 | Loss: 0.00006869
Iteration 60/1000 | Loss: 0.00006869
Iteration 61/1000 | Loss: 0.00006869
Iteration 62/1000 | Loss: 0.00006869
Iteration 63/1000 | Loss: 0.00006869
Iteration 64/1000 | Loss: 0.00006869
Iteration 65/1000 | Loss: 0.00006869
Iteration 66/1000 | Loss: 0.00006868
Iteration 67/1000 | Loss: 0.00006868
Iteration 68/1000 | Loss: 0.00006868
Iteration 69/1000 | Loss: 0.00006868
Iteration 70/1000 | Loss: 0.00006868
Iteration 71/1000 | Loss: 0.00006868
Iteration 72/1000 | Loss: 0.00006868
Iteration 73/1000 | Loss: 0.00006868
Iteration 74/1000 | Loss: 0.00006868
Iteration 75/1000 | Loss: 0.00006868
Iteration 76/1000 | Loss: 0.00006868
Iteration 77/1000 | Loss: 0.00006867
Iteration 78/1000 | Loss: 0.00006867
Iteration 79/1000 | Loss: 0.00006867
Iteration 80/1000 | Loss: 0.00006867
Iteration 81/1000 | Loss: 0.00006867
Iteration 82/1000 | Loss: 0.00006867
Iteration 83/1000 | Loss: 0.00006867
Iteration 84/1000 | Loss: 0.00006867
Iteration 85/1000 | Loss: 0.00006867
Iteration 86/1000 | Loss: 0.00006867
Iteration 87/1000 | Loss: 0.00006867
Iteration 88/1000 | Loss: 0.00006867
Iteration 89/1000 | Loss: 0.00006867
Iteration 90/1000 | Loss: 0.00006866
Iteration 91/1000 | Loss: 0.00006866
Iteration 92/1000 | Loss: 0.00006866
Iteration 93/1000 | Loss: 0.00006865
Iteration 94/1000 | Loss: 0.00006865
Iteration 95/1000 | Loss: 0.00006865
Iteration 96/1000 | Loss: 0.00006865
Iteration 97/1000 | Loss: 0.00006864
Iteration 98/1000 | Loss: 0.00006864
Iteration 99/1000 | Loss: 0.00006864
Iteration 100/1000 | Loss: 0.00006864
Iteration 101/1000 | Loss: 0.00006864
Iteration 102/1000 | Loss: 0.00006864
Iteration 103/1000 | Loss: 0.00006864
Iteration 104/1000 | Loss: 0.00006864
Iteration 105/1000 | Loss: 0.00006863
Iteration 106/1000 | Loss: 0.00006863
Iteration 107/1000 | Loss: 0.00006863
Iteration 108/1000 | Loss: 0.00006863
Iteration 109/1000 | Loss: 0.00006863
Iteration 110/1000 | Loss: 0.00006863
Iteration 111/1000 | Loss: 0.00006863
Iteration 112/1000 | Loss: 0.00006863
Iteration 113/1000 | Loss: 0.00006863
Iteration 114/1000 | Loss: 0.00006863
Iteration 115/1000 | Loss: 0.00006862
Iteration 116/1000 | Loss: 0.00006862
Iteration 117/1000 | Loss: 0.00006862
Iteration 118/1000 | Loss: 0.00006862
Iteration 119/1000 | Loss: 0.00006862
Iteration 120/1000 | Loss: 0.00006862
Iteration 121/1000 | Loss: 0.00006862
Iteration 122/1000 | Loss: 0.00006862
Iteration 123/1000 | Loss: 0.00006862
Iteration 124/1000 | Loss: 0.00006862
Iteration 125/1000 | Loss: 0.00006862
Iteration 126/1000 | Loss: 0.00006862
Iteration 127/1000 | Loss: 0.00006862
Iteration 128/1000 | Loss: 0.00006862
Iteration 129/1000 | Loss: 0.00006862
Iteration 130/1000 | Loss: 0.00006862
Iteration 131/1000 | Loss: 0.00006862
Iteration 132/1000 | Loss: 0.00006862
Iteration 133/1000 | Loss: 0.00006862
Iteration 134/1000 | Loss: 0.00006862
Iteration 135/1000 | Loss: 0.00006862
Iteration 136/1000 | Loss: 0.00006862
Iteration 137/1000 | Loss: 0.00006862
Iteration 138/1000 | Loss: 0.00006862
Iteration 139/1000 | Loss: 0.00006862
Iteration 140/1000 | Loss: 0.00006862
Iteration 141/1000 | Loss: 0.00006862
Iteration 142/1000 | Loss: 0.00006862
Iteration 143/1000 | Loss: 0.00006862
Iteration 144/1000 | Loss: 0.00006862
Iteration 145/1000 | Loss: 0.00006862
Iteration 146/1000 | Loss: 0.00006862
Iteration 147/1000 | Loss: 0.00006862
Iteration 148/1000 | Loss: 0.00006862
Iteration 149/1000 | Loss: 0.00006862
Iteration 150/1000 | Loss: 0.00006862
Iteration 151/1000 | Loss: 0.00006862
Iteration 152/1000 | Loss: 0.00006862
Iteration 153/1000 | Loss: 0.00006862
Iteration 154/1000 | Loss: 0.00006862
Iteration 155/1000 | Loss: 0.00006862
Iteration 156/1000 | Loss: 0.00006862
Iteration 157/1000 | Loss: 0.00006862
Iteration 158/1000 | Loss: 0.00006862
Iteration 159/1000 | Loss: 0.00006862
Iteration 160/1000 | Loss: 0.00006862
Iteration 161/1000 | Loss: 0.00006862
Iteration 162/1000 | Loss: 0.00006862
Iteration 163/1000 | Loss: 0.00006862
Iteration 164/1000 | Loss: 0.00006862
Iteration 165/1000 | Loss: 0.00006862
Iteration 166/1000 | Loss: 0.00006862
Iteration 167/1000 | Loss: 0.00006862
Iteration 168/1000 | Loss: 0.00006862
Iteration 169/1000 | Loss: 0.00006862
Iteration 170/1000 | Loss: 0.00006862
Iteration 171/1000 | Loss: 0.00006862
Iteration 172/1000 | Loss: 0.00006862
Iteration 173/1000 | Loss: 0.00006862
Iteration 174/1000 | Loss: 0.00006862
Iteration 175/1000 | Loss: 0.00006862
Iteration 176/1000 | Loss: 0.00006862
Iteration 177/1000 | Loss: 0.00006862
Iteration 178/1000 | Loss: 0.00006862
Iteration 179/1000 | Loss: 0.00006862
Iteration 180/1000 | Loss: 0.00006862
Iteration 181/1000 | Loss: 0.00006862
Iteration 182/1000 | Loss: 0.00006862
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [6.86150960973464e-05, 6.86150960973464e-05, 6.86150960973464e-05, 6.86150960973464e-05, 6.86150960973464e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.86150960973464e-05

Optimization complete. Final v2v error: 7.296287536621094 mm

Highest mean error: 7.462231159210205 mm for frame 3

Lowest mean error: 7.128692626953125 mm for frame 38

Saving results

Total time: 38.897252798080444
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01226979
Iteration 2/25 | Loss: 0.00392733
Iteration 3/25 | Loss: 0.00328496
Iteration 4/25 | Loss: 0.00320781
Iteration 5/25 | Loss: 0.00306514
Iteration 6/25 | Loss: 0.00297403
Iteration 7/25 | Loss: 0.00297690
Iteration 8/25 | Loss: 0.00291970
Iteration 9/25 | Loss: 0.00289082
Iteration 10/25 | Loss: 0.00287435
Iteration 11/25 | Loss: 0.00285757
Iteration 12/25 | Loss: 0.00285323
Iteration 13/25 | Loss: 0.00283548
Iteration 14/25 | Loss: 0.00283919
Iteration 15/25 | Loss: 0.00283753
Iteration 16/25 | Loss: 0.00284036
Iteration 17/25 | Loss: 0.00285998
Iteration 18/25 | Loss: 0.00281579
Iteration 19/25 | Loss: 0.00280615
Iteration 20/25 | Loss: 0.00279483
Iteration 21/25 | Loss: 0.00278906
Iteration 22/25 | Loss: 0.00279281
Iteration 23/25 | Loss: 0.00278606
Iteration 24/25 | Loss: 0.00278120
Iteration 25/25 | Loss: 0.00278258

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.39671397
Iteration 2/25 | Loss: 0.00661878
Iteration 3/25 | Loss: 0.00621510
Iteration 4/25 | Loss: 0.00621510
Iteration 5/25 | Loss: 0.00621509
Iteration 6/25 | Loss: 0.00621509
Iteration 7/25 | Loss: 0.00621509
Iteration 8/25 | Loss: 0.00621509
Iteration 9/25 | Loss: 0.00621509
Iteration 10/25 | Loss: 0.00621509
Iteration 11/25 | Loss: 0.00621509
Iteration 12/25 | Loss: 0.00621509
Iteration 13/25 | Loss: 0.00621509
Iteration 14/25 | Loss: 0.00621509
Iteration 15/25 | Loss: 0.00621509
Iteration 16/25 | Loss: 0.00621509
Iteration 17/25 | Loss: 0.00621509
Iteration 18/25 | Loss: 0.00621509
Iteration 19/25 | Loss: 0.00621509
Iteration 20/25 | Loss: 0.00621509
Iteration 21/25 | Loss: 0.00621509
Iteration 22/25 | Loss: 0.00621509
Iteration 23/25 | Loss: 0.00621509
Iteration 24/25 | Loss: 0.00621509
Iteration 25/25 | Loss: 0.00621509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00621509
Iteration 2/1000 | Loss: 0.00201978
Iteration 3/1000 | Loss: 0.00229472
Iteration 4/1000 | Loss: 0.00349399
Iteration 5/1000 | Loss: 0.00092665
Iteration 6/1000 | Loss: 0.01103714
Iteration 7/1000 | Loss: 0.00318634
Iteration 8/1000 | Loss: 0.00626706
Iteration 9/1000 | Loss: 0.00534383
Iteration 10/1000 | Loss: 0.00144735
Iteration 11/1000 | Loss: 0.00927264
Iteration 12/1000 | Loss: 0.00424851
Iteration 13/1000 | Loss: 0.00310774
Iteration 14/1000 | Loss: 0.00147644
Iteration 15/1000 | Loss: 0.00224322
Iteration 16/1000 | Loss: 0.00281576
Iteration 17/1000 | Loss: 0.00215795
Iteration 18/1000 | Loss: 0.00214770
Iteration 19/1000 | Loss: 0.00125475
Iteration 20/1000 | Loss: 0.00127777
Iteration 21/1000 | Loss: 0.00146540
Iteration 22/1000 | Loss: 0.00129272
Iteration 23/1000 | Loss: 0.00139905
Iteration 24/1000 | Loss: 0.00117044
Iteration 25/1000 | Loss: 0.00136888
Iteration 26/1000 | Loss: 0.00247417
Iteration 27/1000 | Loss: 0.00172562
Iteration 28/1000 | Loss: 0.00104748
Iteration 29/1000 | Loss: 0.00097606
Iteration 30/1000 | Loss: 0.00146806
Iteration 31/1000 | Loss: 0.00131530
Iteration 32/1000 | Loss: 0.00115263
Iteration 33/1000 | Loss: 0.00098822
Iteration 34/1000 | Loss: 0.00149816
Iteration 35/1000 | Loss: 0.00105338
Iteration 36/1000 | Loss: 0.00147142
Iteration 37/1000 | Loss: 0.00086519
Iteration 38/1000 | Loss: 0.00154002
Iteration 39/1000 | Loss: 0.00148689
Iteration 40/1000 | Loss: 0.00176922
Iteration 41/1000 | Loss: 0.00218890
Iteration 42/1000 | Loss: 0.00326782
Iteration 43/1000 | Loss: 0.00094060
Iteration 44/1000 | Loss: 0.00075114
Iteration 45/1000 | Loss: 0.00077997
Iteration 46/1000 | Loss: 0.00142815
Iteration 47/1000 | Loss: 0.00109472
Iteration 48/1000 | Loss: 0.00109317
Iteration 49/1000 | Loss: 0.00142568
Iteration 50/1000 | Loss: 0.00111885
Iteration 51/1000 | Loss: 0.00136035
Iteration 52/1000 | Loss: 0.00141221
Iteration 53/1000 | Loss: 0.00098897
Iteration 54/1000 | Loss: 0.00099827
Iteration 55/1000 | Loss: 0.00094658
Iteration 56/1000 | Loss: 0.00079198
Iteration 57/1000 | Loss: 0.00069705
Iteration 58/1000 | Loss: 0.00054017
Iteration 59/1000 | Loss: 0.00086709
Iteration 60/1000 | Loss: 0.00099489
Iteration 61/1000 | Loss: 0.00104740
Iteration 62/1000 | Loss: 0.00161917
Iteration 63/1000 | Loss: 0.00114184
Iteration 64/1000 | Loss: 0.00054209
Iteration 65/1000 | Loss: 0.00182925
Iteration 66/1000 | Loss: 0.00149193
Iteration 67/1000 | Loss: 0.00085097
Iteration 68/1000 | Loss: 0.00094346
Iteration 69/1000 | Loss: 0.00081125
Iteration 70/1000 | Loss: 0.00137015
Iteration 71/1000 | Loss: 0.00106618
Iteration 72/1000 | Loss: 0.00111402
Iteration 73/1000 | Loss: 0.00085575
Iteration 74/1000 | Loss: 0.00077893
Iteration 75/1000 | Loss: 0.00212243
Iteration 76/1000 | Loss: 0.00126504
Iteration 77/1000 | Loss: 0.00212759
Iteration 78/1000 | Loss: 0.00127640
Iteration 79/1000 | Loss: 0.00215981
Iteration 80/1000 | Loss: 0.00167690
Iteration 81/1000 | Loss: 0.00181973
Iteration 82/1000 | Loss: 0.00179298
Iteration 83/1000 | Loss: 0.00179635
Iteration 84/1000 | Loss: 0.00137167
Iteration 85/1000 | Loss: 0.00119325
Iteration 86/1000 | Loss: 0.00076092
Iteration 87/1000 | Loss: 0.00091794
Iteration 88/1000 | Loss: 0.00072171
Iteration 89/1000 | Loss: 0.00109939
Iteration 90/1000 | Loss: 0.00076841
Iteration 91/1000 | Loss: 0.00173489
Iteration 92/1000 | Loss: 0.00092765
Iteration 93/1000 | Loss: 0.00104359
Iteration 94/1000 | Loss: 0.00072248
Iteration 95/1000 | Loss: 0.00143372
Iteration 96/1000 | Loss: 0.00179642
Iteration 97/1000 | Loss: 0.00056933
Iteration 98/1000 | Loss: 0.00133631
Iteration 99/1000 | Loss: 0.00123738
Iteration 100/1000 | Loss: 0.00073459
Iteration 101/1000 | Loss: 0.00108729
Iteration 102/1000 | Loss: 0.00127280
Iteration 103/1000 | Loss: 0.00127320
Iteration 104/1000 | Loss: 0.00177576
Iteration 105/1000 | Loss: 0.00102260
Iteration 106/1000 | Loss: 0.00089776
Iteration 107/1000 | Loss: 0.00090698
Iteration 108/1000 | Loss: 0.00091958
Iteration 109/1000 | Loss: 0.00242602
Iteration 110/1000 | Loss: 0.00113508
Iteration 111/1000 | Loss: 0.00082492
Iteration 112/1000 | Loss: 0.00118546
Iteration 113/1000 | Loss: 0.00163331
Iteration 114/1000 | Loss: 0.00118384
Iteration 115/1000 | Loss: 0.00102056
Iteration 116/1000 | Loss: 0.00157576
Iteration 117/1000 | Loss: 0.00109357
Iteration 118/1000 | Loss: 0.00065992
Iteration 119/1000 | Loss: 0.00087274
Iteration 120/1000 | Loss: 0.00175604
Iteration 121/1000 | Loss: 0.00195889
Iteration 122/1000 | Loss: 0.00116608
Iteration 123/1000 | Loss: 0.00091349
Iteration 124/1000 | Loss: 0.00092884
Iteration 125/1000 | Loss: 0.00085611
Iteration 126/1000 | Loss: 0.00089682
Iteration 127/1000 | Loss: 0.00076281
Iteration 128/1000 | Loss: 0.00079770
Iteration 129/1000 | Loss: 0.00076284
Iteration 130/1000 | Loss: 0.00086755
Iteration 131/1000 | Loss: 0.00071066
Iteration 132/1000 | Loss: 0.00101650
Iteration 133/1000 | Loss: 0.00071833
Iteration 134/1000 | Loss: 0.00142981
Iteration 135/1000 | Loss: 0.00074030
Iteration 136/1000 | Loss: 0.00101821
Iteration 137/1000 | Loss: 0.00160569
Iteration 138/1000 | Loss: 0.00056181
Iteration 139/1000 | Loss: 0.00082716
Iteration 140/1000 | Loss: 0.00079562
Iteration 141/1000 | Loss: 0.00094031
Iteration 142/1000 | Loss: 0.00049808
Iteration 143/1000 | Loss: 0.00112684
Iteration 144/1000 | Loss: 0.00079735
Iteration 145/1000 | Loss: 0.00079331
Iteration 146/1000 | Loss: 0.00065848
Iteration 147/1000 | Loss: 0.00117142
Iteration 148/1000 | Loss: 0.00088864
Iteration 149/1000 | Loss: 0.00073016
Iteration 150/1000 | Loss: 0.00102409
Iteration 151/1000 | Loss: 0.00380863
Iteration 152/1000 | Loss: 0.00385226
Iteration 153/1000 | Loss: 0.00280473
Iteration 154/1000 | Loss: 0.00186695
Iteration 155/1000 | Loss: 0.00185881
Iteration 156/1000 | Loss: 0.00350324
Iteration 157/1000 | Loss: 0.00243591
Iteration 158/1000 | Loss: 0.00300889
Iteration 159/1000 | Loss: 0.00230126
Iteration 160/1000 | Loss: 0.00166789
Iteration 161/1000 | Loss: 0.00218082
Iteration 162/1000 | Loss: 0.00160233
Iteration 163/1000 | Loss: 0.00175496
Iteration 164/1000 | Loss: 0.00198400
Iteration 165/1000 | Loss: 0.00097186
Iteration 166/1000 | Loss: 0.00120856
Iteration 167/1000 | Loss: 0.00095885
Iteration 168/1000 | Loss: 0.00117389
Iteration 169/1000 | Loss: 0.00242330
Iteration 170/1000 | Loss: 0.00239215
Iteration 171/1000 | Loss: 0.00109657
Iteration 172/1000 | Loss: 0.00053963
Iteration 173/1000 | Loss: 0.00163194
Iteration 174/1000 | Loss: 0.00067435
Iteration 175/1000 | Loss: 0.00033447
Iteration 176/1000 | Loss: 0.00065014
Iteration 177/1000 | Loss: 0.00100003
Iteration 178/1000 | Loss: 0.00030383
Iteration 179/1000 | Loss: 0.00065841
Iteration 180/1000 | Loss: 0.00092205
Iteration 181/1000 | Loss: 0.00156831
Iteration 182/1000 | Loss: 0.00067988
Iteration 183/1000 | Loss: 0.00074133
Iteration 184/1000 | Loss: 0.00058036
Iteration 185/1000 | Loss: 0.00095402
Iteration 186/1000 | Loss: 0.00085540
Iteration 187/1000 | Loss: 0.00075474
Iteration 188/1000 | Loss: 0.00119378
Iteration 189/1000 | Loss: 0.00071070
Iteration 190/1000 | Loss: 0.00076837
Iteration 191/1000 | Loss: 0.00066587
Iteration 192/1000 | Loss: 0.00057312
Iteration 193/1000 | Loss: 0.00076290
Iteration 194/1000 | Loss: 0.00099876
Iteration 195/1000 | Loss: 0.00136618
Iteration 196/1000 | Loss: 0.00153322
Iteration 197/1000 | Loss: 0.00090534
Iteration 198/1000 | Loss: 0.00115644
Iteration 199/1000 | Loss: 0.00101714
Iteration 200/1000 | Loss: 0.00247284
Iteration 201/1000 | Loss: 0.00073426
Iteration 202/1000 | Loss: 0.00178686
Iteration 203/1000 | Loss: 0.00294406
Iteration 204/1000 | Loss: 0.00192775
Iteration 205/1000 | Loss: 0.00106442
Iteration 206/1000 | Loss: 0.00162890
Iteration 207/1000 | Loss: 0.00104037
Iteration 208/1000 | Loss: 0.00085638
Iteration 209/1000 | Loss: 0.00082167
Iteration 210/1000 | Loss: 0.00137713
Iteration 211/1000 | Loss: 0.00166533
Iteration 212/1000 | Loss: 0.00114750
Iteration 213/1000 | Loss: 0.00104214
Iteration 214/1000 | Loss: 0.00119150
Iteration 215/1000 | Loss: 0.00216064
Iteration 216/1000 | Loss: 0.00135267
Iteration 217/1000 | Loss: 0.00030452
Iteration 218/1000 | Loss: 0.00033052
Iteration 219/1000 | Loss: 0.00023612
Iteration 220/1000 | Loss: 0.00101158
Iteration 221/1000 | Loss: 0.00041951
Iteration 222/1000 | Loss: 0.00072630
Iteration 223/1000 | Loss: 0.00015357
Iteration 224/1000 | Loss: 0.00026112
Iteration 225/1000 | Loss: 0.00167340
Iteration 226/1000 | Loss: 0.00060611
Iteration 227/1000 | Loss: 0.00028755
Iteration 228/1000 | Loss: 0.00040195
Iteration 229/1000 | Loss: 0.00061492
Iteration 230/1000 | Loss: 0.00040346
Iteration 231/1000 | Loss: 0.00073723
Iteration 232/1000 | Loss: 0.00017978
Iteration 233/1000 | Loss: 0.00035550
Iteration 234/1000 | Loss: 0.00018135
Iteration 235/1000 | Loss: 0.00060831
Iteration 236/1000 | Loss: 0.00054528
Iteration 237/1000 | Loss: 0.00031067
Iteration 238/1000 | Loss: 0.00050221
Iteration 239/1000 | Loss: 0.00042682
Iteration 240/1000 | Loss: 0.00016493
Iteration 241/1000 | Loss: 0.00030827
Iteration 242/1000 | Loss: 0.00015566
Iteration 243/1000 | Loss: 0.00016054
Iteration 244/1000 | Loss: 0.00108263
Iteration 245/1000 | Loss: 0.00068486
Iteration 246/1000 | Loss: 0.00025457
Iteration 247/1000 | Loss: 0.00024598
Iteration 248/1000 | Loss: 0.00094580
Iteration 249/1000 | Loss: 0.00029174
Iteration 250/1000 | Loss: 0.00017068
Iteration 251/1000 | Loss: 0.00029307
Iteration 252/1000 | Loss: 0.00022689
Iteration 253/1000 | Loss: 0.00032335
Iteration 254/1000 | Loss: 0.00021331
Iteration 255/1000 | Loss: 0.00023827
Iteration 256/1000 | Loss: 0.00025988
Iteration 257/1000 | Loss: 0.00026393
Iteration 258/1000 | Loss: 0.00034955
Iteration 259/1000 | Loss: 0.00026048
Iteration 260/1000 | Loss: 0.00030473
Iteration 261/1000 | Loss: 0.00028100
Iteration 262/1000 | Loss: 0.00036742
Iteration 263/1000 | Loss: 0.00090328
Iteration 264/1000 | Loss: 0.00030407
Iteration 265/1000 | Loss: 0.00016789
Iteration 266/1000 | Loss: 0.00062495
Iteration 267/1000 | Loss: 0.00060102
Iteration 268/1000 | Loss: 0.00119289
Iteration 269/1000 | Loss: 0.00018217
Iteration 270/1000 | Loss: 0.00016310
Iteration 271/1000 | Loss: 0.00050025
Iteration 272/1000 | Loss: 0.00059997
Iteration 273/1000 | Loss: 0.00035315
Iteration 274/1000 | Loss: 0.00014645
Iteration 275/1000 | Loss: 0.00063301
Iteration 276/1000 | Loss: 0.00033058
Iteration 277/1000 | Loss: 0.00066560
Iteration 278/1000 | Loss: 0.00060257
Iteration 279/1000 | Loss: 0.00037084
Iteration 280/1000 | Loss: 0.00045965
Iteration 281/1000 | Loss: 0.00038289
Iteration 282/1000 | Loss: 0.00060411
Iteration 283/1000 | Loss: 0.00060044
Iteration 284/1000 | Loss: 0.00053143
Iteration 285/1000 | Loss: 0.00048167
Iteration 286/1000 | Loss: 0.00086542
Iteration 287/1000 | Loss: 0.00028343
Iteration 288/1000 | Loss: 0.00023276
Iteration 289/1000 | Loss: 0.00078681
Iteration 290/1000 | Loss: 0.00063219
Iteration 291/1000 | Loss: 0.00052242
Iteration 292/1000 | Loss: 0.00066233
Iteration 293/1000 | Loss: 0.00051463
Iteration 294/1000 | Loss: 0.00016219
Iteration 295/1000 | Loss: 0.00014735
Iteration 296/1000 | Loss: 0.00014245
Iteration 297/1000 | Loss: 0.00013931
Iteration 298/1000 | Loss: 0.00159721
Iteration 299/1000 | Loss: 0.00146874
Iteration 300/1000 | Loss: 0.00049961
Iteration 301/1000 | Loss: 0.00072904
Iteration 302/1000 | Loss: 0.00034626
Iteration 303/1000 | Loss: 0.00057413
Iteration 304/1000 | Loss: 0.00017168
Iteration 305/1000 | Loss: 0.00013962
Iteration 306/1000 | Loss: 0.00013261
Iteration 307/1000 | Loss: 0.00037672
Iteration 308/1000 | Loss: 0.00035906
Iteration 309/1000 | Loss: 0.00028282
Iteration 310/1000 | Loss: 0.00050568
Iteration 311/1000 | Loss: 0.00045095
Iteration 312/1000 | Loss: 0.00148104
Iteration 313/1000 | Loss: 0.00088476
Iteration 314/1000 | Loss: 0.00134130
Iteration 315/1000 | Loss: 0.00055756
Iteration 316/1000 | Loss: 0.00055669
Iteration 317/1000 | Loss: 0.00047259
Iteration 318/1000 | Loss: 0.00055988
Iteration 319/1000 | Loss: 0.00053328
Iteration 320/1000 | Loss: 0.00053643
Iteration 321/1000 | Loss: 0.00046965
Iteration 322/1000 | Loss: 0.00029796
Iteration 323/1000 | Loss: 0.00037536
Iteration 324/1000 | Loss: 0.00026504
Iteration 325/1000 | Loss: 0.00024360
Iteration 326/1000 | Loss: 0.00023237
Iteration 327/1000 | Loss: 0.00017920
Iteration 328/1000 | Loss: 0.00019808
Iteration 329/1000 | Loss: 0.00020057
Iteration 330/1000 | Loss: 0.00029840
Iteration 331/1000 | Loss: 0.00050343
Iteration 332/1000 | Loss: 0.00021138
Iteration 333/1000 | Loss: 0.00035338
Iteration 334/1000 | Loss: 0.00044915
Iteration 335/1000 | Loss: 0.00018091
Iteration 336/1000 | Loss: 0.00032152
Iteration 337/1000 | Loss: 0.00054493
Iteration 338/1000 | Loss: 0.00031295
Iteration 339/1000 | Loss: 0.00056866
Iteration 340/1000 | Loss: 0.00047816
Iteration 341/1000 | Loss: 0.00046227
Iteration 342/1000 | Loss: 0.00054007
Iteration 343/1000 | Loss: 0.00039263
Iteration 344/1000 | Loss: 0.00042451
Iteration 345/1000 | Loss: 0.00050395
Iteration 346/1000 | Loss: 0.00040853
Iteration 347/1000 | Loss: 0.00040461
Iteration 348/1000 | Loss: 0.00056961
Iteration 349/1000 | Loss: 0.00014715
Iteration 350/1000 | Loss: 0.00032897
Iteration 351/1000 | Loss: 0.00014027
Iteration 352/1000 | Loss: 0.00013278
Iteration 353/1000 | Loss: 0.00041495
Iteration 354/1000 | Loss: 0.00019101
Iteration 355/1000 | Loss: 0.00042253
Iteration 356/1000 | Loss: 0.00025895
Iteration 357/1000 | Loss: 0.00023691
Iteration 358/1000 | Loss: 0.00033011
Iteration 359/1000 | Loss: 0.00027196
Iteration 360/1000 | Loss: 0.00032818
Iteration 361/1000 | Loss: 0.00019796
Iteration 362/1000 | Loss: 0.00020265
Iteration 363/1000 | Loss: 0.00019159
Iteration 364/1000 | Loss: 0.00014606
Iteration 365/1000 | Loss: 0.00065900
Iteration 366/1000 | Loss: 0.00072046
Iteration 367/1000 | Loss: 0.00039516
Iteration 368/1000 | Loss: 0.00014075
Iteration 369/1000 | Loss: 0.00013300
Iteration 370/1000 | Loss: 0.00061794
Iteration 371/1000 | Loss: 0.00044647
Iteration 372/1000 | Loss: 0.00056298
Iteration 373/1000 | Loss: 0.00013779
Iteration 374/1000 | Loss: 0.00012946
Iteration 375/1000 | Loss: 0.00012707
Iteration 376/1000 | Loss: 0.00012552
Iteration 377/1000 | Loss: 0.00099382
Iteration 378/1000 | Loss: 0.00015946
Iteration 379/1000 | Loss: 0.00013011
Iteration 380/1000 | Loss: 0.00012666
Iteration 381/1000 | Loss: 0.00081072
Iteration 382/1000 | Loss: 0.00049595
Iteration 383/1000 | Loss: 0.00066442
Iteration 384/1000 | Loss: 0.00104083
Iteration 385/1000 | Loss: 0.00015062
Iteration 386/1000 | Loss: 0.00014960
Iteration 387/1000 | Loss: 0.00014810
Iteration 388/1000 | Loss: 0.00012617
Iteration 389/1000 | Loss: 0.00012393
Iteration 390/1000 | Loss: 0.00012279
Iteration 391/1000 | Loss: 0.00012197
Iteration 392/1000 | Loss: 0.00012168
Iteration 393/1000 | Loss: 0.00012162
Iteration 394/1000 | Loss: 0.00012162
Iteration 395/1000 | Loss: 0.00012152
Iteration 396/1000 | Loss: 0.00012151
Iteration 397/1000 | Loss: 0.00012150
Iteration 398/1000 | Loss: 0.00012148
Iteration 399/1000 | Loss: 0.00012140
Iteration 400/1000 | Loss: 0.00012139
Iteration 401/1000 | Loss: 0.00012139
Iteration 402/1000 | Loss: 0.00012138
Iteration 403/1000 | Loss: 0.00012137
Iteration 404/1000 | Loss: 0.00012137
Iteration 405/1000 | Loss: 0.00012137
Iteration 406/1000 | Loss: 0.00012136
Iteration 407/1000 | Loss: 0.00012136
Iteration 408/1000 | Loss: 0.00012135
Iteration 409/1000 | Loss: 0.00012135
Iteration 410/1000 | Loss: 0.00012135
Iteration 411/1000 | Loss: 0.00012133
Iteration 412/1000 | Loss: 0.00012133
Iteration 413/1000 | Loss: 0.00012132
Iteration 414/1000 | Loss: 0.00012132
Iteration 415/1000 | Loss: 0.00091895
Iteration 416/1000 | Loss: 0.00013883
Iteration 417/1000 | Loss: 0.00012665
Iteration 418/1000 | Loss: 0.00012407
Iteration 419/1000 | Loss: 0.00050904
Iteration 420/1000 | Loss: 0.00063881
Iteration 421/1000 | Loss: 0.00050696
Iteration 422/1000 | Loss: 0.00012371
Iteration 423/1000 | Loss: 0.00012316
Iteration 424/1000 | Loss: 0.00093610
Iteration 425/1000 | Loss: 0.00070253
Iteration 426/1000 | Loss: 0.00063100
Iteration 427/1000 | Loss: 0.00062854
Iteration 428/1000 | Loss: 0.00078375
Iteration 429/1000 | Loss: 0.00016546
Iteration 430/1000 | Loss: 0.00013615
Iteration 431/1000 | Loss: 0.00046140
Iteration 432/1000 | Loss: 0.00063353
Iteration 433/1000 | Loss: 0.00160966
Iteration 434/1000 | Loss: 0.00142168
Iteration 435/1000 | Loss: 0.00087905
Iteration 436/1000 | Loss: 0.00050949
Iteration 437/1000 | Loss: 0.00048037
Iteration 438/1000 | Loss: 0.00066444
Iteration 439/1000 | Loss: 0.00071016
Iteration 440/1000 | Loss: 0.00059180
Iteration 441/1000 | Loss: 0.00047127
Iteration 442/1000 | Loss: 0.00057807
Iteration 443/1000 | Loss: 0.00045917
Iteration 444/1000 | Loss: 0.00038023
Iteration 445/1000 | Loss: 0.00052294
Iteration 446/1000 | Loss: 0.00061821
Iteration 447/1000 | Loss: 0.00045159
Iteration 448/1000 | Loss: 0.00048608
Iteration 449/1000 | Loss: 0.00046323
Iteration 450/1000 | Loss: 0.00086748
Iteration 451/1000 | Loss: 0.00035186
Iteration 452/1000 | Loss: 0.00099817
Iteration 453/1000 | Loss: 0.00014063
Iteration 454/1000 | Loss: 0.00012766
Iteration 455/1000 | Loss: 0.00012271
Iteration 456/1000 | Loss: 0.00012024
Iteration 457/1000 | Loss: 0.00011886
Iteration 458/1000 | Loss: 0.00089487
Iteration 459/1000 | Loss: 0.00014343
Iteration 460/1000 | Loss: 0.00012279
Iteration 461/1000 | Loss: 0.00011992
Iteration 462/1000 | Loss: 0.00069041
Iteration 463/1000 | Loss: 0.00072276
Iteration 464/1000 | Loss: 0.00114429
Iteration 465/1000 | Loss: 0.00053752
Iteration 466/1000 | Loss: 0.00023827
Iteration 467/1000 | Loss: 0.00012930
Iteration 468/1000 | Loss: 0.00011934
Iteration 469/1000 | Loss: 0.00011682
Iteration 470/1000 | Loss: 0.00011598
Iteration 471/1000 | Loss: 0.00011547
Iteration 472/1000 | Loss: 0.00011526
Iteration 473/1000 | Loss: 0.00011517
Iteration 474/1000 | Loss: 0.00011506
Iteration 475/1000 | Loss: 0.00011502
Iteration 476/1000 | Loss: 0.00011502
Iteration 477/1000 | Loss: 0.00011501
Iteration 478/1000 | Loss: 0.00011501
Iteration 479/1000 | Loss: 0.00011500
Iteration 480/1000 | Loss: 0.00011500
Iteration 481/1000 | Loss: 0.00011500
Iteration 482/1000 | Loss: 0.00011499
Iteration 483/1000 | Loss: 0.00011499
Iteration 484/1000 | Loss: 0.00011499
Iteration 485/1000 | Loss: 0.00011498
Iteration 486/1000 | Loss: 0.00011498
Iteration 487/1000 | Loss: 0.00011498
Iteration 488/1000 | Loss: 0.00011497
Iteration 489/1000 | Loss: 0.00011497
Iteration 490/1000 | Loss: 0.00011494
Iteration 491/1000 | Loss: 0.00011494
Iteration 492/1000 | Loss: 0.00011494
Iteration 493/1000 | Loss: 0.00011494
Iteration 494/1000 | Loss: 0.00011494
Iteration 495/1000 | Loss: 0.00011493
Iteration 496/1000 | Loss: 0.00011493
Iteration 497/1000 | Loss: 0.00011492
Iteration 498/1000 | Loss: 0.00011492
Iteration 499/1000 | Loss: 0.00011491
Iteration 500/1000 | Loss: 0.00011491
Iteration 501/1000 | Loss: 0.00011491
Iteration 502/1000 | Loss: 0.00011491
Iteration 503/1000 | Loss: 0.00011490
Iteration 504/1000 | Loss: 0.00011490
Iteration 505/1000 | Loss: 0.00011489
Iteration 506/1000 | Loss: 0.00011489
Iteration 507/1000 | Loss: 0.00011488
Iteration 508/1000 | Loss: 0.00011488
Iteration 509/1000 | Loss: 0.00011488
Iteration 510/1000 | Loss: 0.00011488
Iteration 511/1000 | Loss: 0.00011488
Iteration 512/1000 | Loss: 0.00011488
Iteration 513/1000 | Loss: 0.00011487
Iteration 514/1000 | Loss: 0.00011487
Iteration 515/1000 | Loss: 0.00011487
Iteration 516/1000 | Loss: 0.00011487
Iteration 517/1000 | Loss: 0.00011487
Iteration 518/1000 | Loss: 0.00011487
Iteration 519/1000 | Loss: 0.00011486
Iteration 520/1000 | Loss: 0.00011486
Iteration 521/1000 | Loss: 0.00011485
Iteration 522/1000 | Loss: 0.00011484
Iteration 523/1000 | Loss: 0.00011484
Iteration 524/1000 | Loss: 0.00011484
Iteration 525/1000 | Loss: 0.00011484
Iteration 526/1000 | Loss: 0.00011484
Iteration 527/1000 | Loss: 0.00011483
Iteration 528/1000 | Loss: 0.00011483
Iteration 529/1000 | Loss: 0.00011483
Iteration 530/1000 | Loss: 0.00011483
Iteration 531/1000 | Loss: 0.00011483
Iteration 532/1000 | Loss: 0.00011482
Iteration 533/1000 | Loss: 0.00011482
Iteration 534/1000 | Loss: 0.00011482
Iteration 535/1000 | Loss: 0.00011482
Iteration 536/1000 | Loss: 0.00011482
Iteration 537/1000 | Loss: 0.00011482
Iteration 538/1000 | Loss: 0.00011481
Iteration 539/1000 | Loss: 0.00011481
Iteration 540/1000 | Loss: 0.00011481
Iteration 541/1000 | Loss: 0.00011481
Iteration 542/1000 | Loss: 0.00011481
Iteration 543/1000 | Loss: 0.00011480
Iteration 544/1000 | Loss: 0.00011480
Iteration 545/1000 | Loss: 0.00011480
Iteration 546/1000 | Loss: 0.00011480
Iteration 547/1000 | Loss: 0.00011480
Iteration 548/1000 | Loss: 0.00011480
Iteration 549/1000 | Loss: 0.00011480
Iteration 550/1000 | Loss: 0.00011480
Iteration 551/1000 | Loss: 0.00011480
Iteration 552/1000 | Loss: 0.00011480
Iteration 553/1000 | Loss: 0.00011480
Iteration 554/1000 | Loss: 0.00011480
Iteration 555/1000 | Loss: 0.00011480
Iteration 556/1000 | Loss: 0.00011480
Iteration 557/1000 | Loss: 0.00011480
Iteration 558/1000 | Loss: 0.00011480
Iteration 559/1000 | Loss: 0.00011480
Iteration 560/1000 | Loss: 0.00011480
Iteration 561/1000 | Loss: 0.00011480
Iteration 562/1000 | Loss: 0.00011480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 562. Stopping optimization.
Last 5 losses: [0.0001148002702393569, 0.0001148002702393569, 0.0001148002702393569, 0.0001148002702393569, 0.0001148002702393569]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0001148002702393569

Optimization complete. Final v2v error: 8.20897388458252 mm

Highest mean error: 15.902740478515625 mm for frame 220

Lowest mean error: 6.927407264709473 mm for frame 71

Saving results

Total time: 797.1645858287811
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01046073
Iteration 2/25 | Loss: 0.00346859
Iteration 3/25 | Loss: 0.00325675
Iteration 4/25 | Loss: 0.00320465
Iteration 5/25 | Loss: 0.00312119
Iteration 6/25 | Loss: 0.00305754
Iteration 7/25 | Loss: 0.00302386
Iteration 8/25 | Loss: 0.00301387
Iteration 9/25 | Loss: 0.00300868
Iteration 10/25 | Loss: 0.00299843
Iteration 11/25 | Loss: 0.00299965
Iteration 12/25 | Loss: 0.00298670
Iteration 13/25 | Loss: 0.00297351
Iteration 14/25 | Loss: 0.00296869
Iteration 15/25 | Loss: 0.00297102
Iteration 16/25 | Loss: 0.00296859
Iteration 17/25 | Loss: 0.00296868
Iteration 18/25 | Loss: 0.00296714
Iteration 19/25 | Loss: 0.00296663
Iteration 20/25 | Loss: 0.00296598
Iteration 21/25 | Loss: 0.00296667
Iteration 22/25 | Loss: 0.00296507
Iteration 23/25 | Loss: 0.00296616
Iteration 24/25 | Loss: 0.00296551
Iteration 25/25 | Loss: 0.00296665

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48658812
Iteration 2/25 | Loss: 0.00436830
Iteration 3/25 | Loss: 0.00436828
Iteration 4/25 | Loss: 0.00436828
Iteration 5/25 | Loss: 0.00436828
Iteration 6/25 | Loss: 0.00436828
Iteration 7/25 | Loss: 0.00436828
Iteration 8/25 | Loss: 0.00436828
Iteration 9/25 | Loss: 0.00436828
Iteration 10/25 | Loss: 0.00436828
Iteration 11/25 | Loss: 0.00436828
Iteration 12/25 | Loss: 0.00436828
Iteration 13/25 | Loss: 0.00436828
Iteration 14/25 | Loss: 0.00436828
Iteration 15/25 | Loss: 0.00436828
Iteration 16/25 | Loss: 0.00436828
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.004368276800960302, 0.004368276800960302, 0.004368276800960302, 0.004368276800960302, 0.004368276800960302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004368276800960302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00436828
Iteration 2/1000 | Loss: 0.00293955
Iteration 3/1000 | Loss: 0.00025218
Iteration 4/1000 | Loss: 0.00026487
Iteration 5/1000 | Loss: 0.00016324
Iteration 6/1000 | Loss: 0.00033137
Iteration 7/1000 | Loss: 0.00205031
Iteration 8/1000 | Loss: 0.00141576
Iteration 9/1000 | Loss: 0.00014464
Iteration 10/1000 | Loss: 0.00224993
Iteration 11/1000 | Loss: 0.00127472
Iteration 12/1000 | Loss: 0.00192825
Iteration 13/1000 | Loss: 0.00108952
Iteration 14/1000 | Loss: 0.00185128
Iteration 15/1000 | Loss: 0.00148228
Iteration 16/1000 | Loss: 0.00151284
Iteration 17/1000 | Loss: 0.00067810
Iteration 18/1000 | Loss: 0.00076628
Iteration 19/1000 | Loss: 0.00036217
Iteration 20/1000 | Loss: 0.00033484
Iteration 21/1000 | Loss: 0.00026476
Iteration 22/1000 | Loss: 0.00024824
Iteration 23/1000 | Loss: 0.00118908
Iteration 24/1000 | Loss: 0.00098497
Iteration 25/1000 | Loss: 0.00059180
Iteration 26/1000 | Loss: 0.00120944
Iteration 27/1000 | Loss: 0.00058318
Iteration 28/1000 | Loss: 0.00041509
Iteration 29/1000 | Loss: 0.00098962
Iteration 30/1000 | Loss: 0.00063749
Iteration 31/1000 | Loss: 0.00064038
Iteration 32/1000 | Loss: 0.00057174
Iteration 33/1000 | Loss: 0.00068104
Iteration 34/1000 | Loss: 0.00063383
Iteration 35/1000 | Loss: 0.00041638
Iteration 36/1000 | Loss: 0.00057892
Iteration 37/1000 | Loss: 0.00056413
Iteration 38/1000 | Loss: 0.00054846
Iteration 39/1000 | Loss: 0.00059607
Iteration 40/1000 | Loss: 0.00046553
Iteration 41/1000 | Loss: 0.00044409
Iteration 42/1000 | Loss: 0.00043137
Iteration 43/1000 | Loss: 0.00064125
Iteration 44/1000 | Loss: 0.00055121
Iteration 45/1000 | Loss: 0.00043691
Iteration 46/1000 | Loss: 0.00030294
Iteration 47/1000 | Loss: 0.00042647
Iteration 48/1000 | Loss: 0.00036397
Iteration 49/1000 | Loss: 0.00032498
Iteration 50/1000 | Loss: 0.00015484
Iteration 51/1000 | Loss: 0.00030624
Iteration 52/1000 | Loss: 0.00050623
Iteration 53/1000 | Loss: 0.00063219
Iteration 54/1000 | Loss: 0.00112796
Iteration 55/1000 | Loss: 0.00038661
Iteration 56/1000 | Loss: 0.00025971
Iteration 57/1000 | Loss: 0.00036358
Iteration 58/1000 | Loss: 0.00029768
Iteration 59/1000 | Loss: 0.00037028
Iteration 60/1000 | Loss: 0.00034334
Iteration 61/1000 | Loss: 0.00035091
Iteration 62/1000 | Loss: 0.00038227
Iteration 63/1000 | Loss: 0.00044837
Iteration 64/1000 | Loss: 0.00039327
Iteration 65/1000 | Loss: 0.00018589
Iteration 66/1000 | Loss: 0.00051443
Iteration 67/1000 | Loss: 0.00027266
Iteration 68/1000 | Loss: 0.00033156
Iteration 69/1000 | Loss: 0.00037460
Iteration 70/1000 | Loss: 0.00021540
Iteration 71/1000 | Loss: 0.00025313
Iteration 72/1000 | Loss: 0.00017108
Iteration 73/1000 | Loss: 0.00035390
Iteration 74/1000 | Loss: 0.00039271
Iteration 75/1000 | Loss: 0.00028315
Iteration 76/1000 | Loss: 0.00025761
Iteration 77/1000 | Loss: 0.00030034
Iteration 78/1000 | Loss: 0.00054411
Iteration 79/1000 | Loss: 0.00027068
Iteration 80/1000 | Loss: 0.00027010
Iteration 81/1000 | Loss: 0.00026966
Iteration 82/1000 | Loss: 0.00018546
Iteration 83/1000 | Loss: 0.00041113
Iteration 84/1000 | Loss: 0.00016376
Iteration 85/1000 | Loss: 0.00014017
Iteration 86/1000 | Loss: 0.00016153
Iteration 87/1000 | Loss: 0.00016029
Iteration 88/1000 | Loss: 0.00029440
Iteration 89/1000 | Loss: 0.00025875
Iteration 90/1000 | Loss: 0.00014986
Iteration 91/1000 | Loss: 0.00021018
Iteration 92/1000 | Loss: 0.00030202
Iteration 93/1000 | Loss: 0.00070725
Iteration 94/1000 | Loss: 0.00076087
Iteration 95/1000 | Loss: 0.00043514
Iteration 96/1000 | Loss: 0.00051494
Iteration 97/1000 | Loss: 0.00024815
Iteration 98/1000 | Loss: 0.00015351
Iteration 99/1000 | Loss: 0.00015186
Iteration 100/1000 | Loss: 0.00016497
Iteration 101/1000 | Loss: 0.00012910
Iteration 102/1000 | Loss: 0.00027103
Iteration 103/1000 | Loss: 0.00022044
Iteration 104/1000 | Loss: 0.00020684
Iteration 105/1000 | Loss: 0.00025436
Iteration 106/1000 | Loss: 0.00014842
Iteration 107/1000 | Loss: 0.00042462
Iteration 108/1000 | Loss: 0.00033026
Iteration 109/1000 | Loss: 0.00012776
Iteration 110/1000 | Loss: 0.00012802
Iteration 111/1000 | Loss: 0.00013936
Iteration 112/1000 | Loss: 0.00030028
Iteration 113/1000 | Loss: 0.00039138
Iteration 114/1000 | Loss: 0.00033508
Iteration 115/1000 | Loss: 0.00028325
Iteration 116/1000 | Loss: 0.00019318
Iteration 117/1000 | Loss: 0.00017037
Iteration 118/1000 | Loss: 0.00027133
Iteration 119/1000 | Loss: 0.00013737
Iteration 120/1000 | Loss: 0.00027761
Iteration 121/1000 | Loss: 0.00025558
Iteration 122/1000 | Loss: 0.00015167
Iteration 123/1000 | Loss: 0.00031188
Iteration 124/1000 | Loss: 0.00071030
Iteration 125/1000 | Loss: 0.00032030
Iteration 126/1000 | Loss: 0.00024143
Iteration 127/1000 | Loss: 0.00032850
Iteration 128/1000 | Loss: 0.00016858
Iteration 129/1000 | Loss: 0.00012915
Iteration 130/1000 | Loss: 0.00013288
Iteration 131/1000 | Loss: 0.00015441
Iteration 132/1000 | Loss: 0.00013689
Iteration 133/1000 | Loss: 0.00012124
Iteration 134/1000 | Loss: 0.00012193
Iteration 135/1000 | Loss: 0.00010879
Iteration 136/1000 | Loss: 0.00013487
Iteration 137/1000 | Loss: 0.00014348
Iteration 138/1000 | Loss: 0.00036457
Iteration 139/1000 | Loss: 0.00026361
Iteration 140/1000 | Loss: 0.00032846
Iteration 141/1000 | Loss: 0.00024673
Iteration 142/1000 | Loss: 0.00031358
Iteration 143/1000 | Loss: 0.00022430
Iteration 144/1000 | Loss: 0.00013621
Iteration 145/1000 | Loss: 0.00014279
Iteration 146/1000 | Loss: 0.00031880
Iteration 147/1000 | Loss: 0.00027684
Iteration 148/1000 | Loss: 0.00031751
Iteration 149/1000 | Loss: 0.00012009
Iteration 150/1000 | Loss: 0.00011322
Iteration 151/1000 | Loss: 0.00027946
Iteration 152/1000 | Loss: 0.00013013
Iteration 153/1000 | Loss: 0.00011554
Iteration 154/1000 | Loss: 0.00011621
Iteration 155/1000 | Loss: 0.00010538
Iteration 156/1000 | Loss: 0.00011642
Iteration 157/1000 | Loss: 0.00012002
Iteration 158/1000 | Loss: 0.00010761
Iteration 159/1000 | Loss: 0.00010763
Iteration 160/1000 | Loss: 0.00010619
Iteration 161/1000 | Loss: 0.00011627
Iteration 162/1000 | Loss: 0.00011780
Iteration 163/1000 | Loss: 0.00010826
Iteration 164/1000 | Loss: 0.00012302
Iteration 165/1000 | Loss: 0.00010358
Iteration 166/1000 | Loss: 0.00010822
Iteration 167/1000 | Loss: 0.00009382
Iteration 168/1000 | Loss: 0.00009683
Iteration 169/1000 | Loss: 0.00009026
Iteration 170/1000 | Loss: 0.00009078
Iteration 171/1000 | Loss: 0.00010170
Iteration 172/1000 | Loss: 0.00012056
Iteration 173/1000 | Loss: 0.00010317
Iteration 174/1000 | Loss: 0.00008988
Iteration 175/1000 | Loss: 0.00010277
Iteration 176/1000 | Loss: 0.00010911
Iteration 177/1000 | Loss: 0.00009820
Iteration 178/1000 | Loss: 0.00010384
Iteration 179/1000 | Loss: 0.00010023
Iteration 180/1000 | Loss: 0.00009458
Iteration 181/1000 | Loss: 0.00010503
Iteration 182/1000 | Loss: 0.00010101
Iteration 183/1000 | Loss: 0.00010310
Iteration 184/1000 | Loss: 0.00009957
Iteration 185/1000 | Loss: 0.00009830
Iteration 186/1000 | Loss: 0.00009577
Iteration 187/1000 | Loss: 0.00010861
Iteration 188/1000 | Loss: 0.00011245
Iteration 189/1000 | Loss: 0.00010841
Iteration 190/1000 | Loss: 0.00011219
Iteration 191/1000 | Loss: 0.00010146
Iteration 192/1000 | Loss: 0.00011439
Iteration 193/1000 | Loss: 0.00009551
Iteration 194/1000 | Loss: 0.00009786
Iteration 195/1000 | Loss: 0.00011413
Iteration 196/1000 | Loss: 0.00010172
Iteration 197/1000 | Loss: 0.00009379
Iteration 198/1000 | Loss: 0.00009929
Iteration 199/1000 | Loss: 0.00009419
Iteration 200/1000 | Loss: 0.00011110
Iteration 201/1000 | Loss: 0.00009616
Iteration 202/1000 | Loss: 0.00011860
Iteration 203/1000 | Loss: 0.00010462
Iteration 204/1000 | Loss: 0.00009236
Iteration 205/1000 | Loss: 0.00011188
Iteration 206/1000 | Loss: 0.00009224
Iteration 207/1000 | Loss: 0.00011286
Iteration 208/1000 | Loss: 0.00008968
Iteration 209/1000 | Loss: 0.00008548
Iteration 210/1000 | Loss: 0.00008345
Iteration 211/1000 | Loss: 0.00008304
Iteration 212/1000 | Loss: 0.00008254
Iteration 213/1000 | Loss: 0.00031220
Iteration 214/1000 | Loss: 0.00031219
Iteration 215/1000 | Loss: 0.00024031
Iteration 216/1000 | Loss: 0.00029772
Iteration 217/1000 | Loss: 0.00018797
Iteration 218/1000 | Loss: 0.00016654
Iteration 219/1000 | Loss: 0.00025151
Iteration 220/1000 | Loss: 0.00018629
Iteration 221/1000 | Loss: 0.00008260
Iteration 222/1000 | Loss: 0.00008210
Iteration 223/1000 | Loss: 0.00008197
Iteration 224/1000 | Loss: 0.00008197
Iteration 225/1000 | Loss: 0.00008196
Iteration 226/1000 | Loss: 0.00008196
Iteration 227/1000 | Loss: 0.00008196
Iteration 228/1000 | Loss: 0.00008195
Iteration 229/1000 | Loss: 0.00008194
Iteration 230/1000 | Loss: 0.00008194
Iteration 231/1000 | Loss: 0.00008194
Iteration 232/1000 | Loss: 0.00008194
Iteration 233/1000 | Loss: 0.00008194
Iteration 234/1000 | Loss: 0.00008193
Iteration 235/1000 | Loss: 0.00008193
Iteration 236/1000 | Loss: 0.00008193
Iteration 237/1000 | Loss: 0.00008193
Iteration 238/1000 | Loss: 0.00008193
Iteration 239/1000 | Loss: 0.00008193
Iteration 240/1000 | Loss: 0.00008193
Iteration 241/1000 | Loss: 0.00008193
Iteration 242/1000 | Loss: 0.00008193
Iteration 243/1000 | Loss: 0.00008193
Iteration 244/1000 | Loss: 0.00008193
Iteration 245/1000 | Loss: 0.00008192
Iteration 246/1000 | Loss: 0.00008192
Iteration 247/1000 | Loss: 0.00008191
Iteration 248/1000 | Loss: 0.00008191
Iteration 249/1000 | Loss: 0.00008191
Iteration 250/1000 | Loss: 0.00008191
Iteration 251/1000 | Loss: 0.00008191
Iteration 252/1000 | Loss: 0.00008191
Iteration 253/1000 | Loss: 0.00008191
Iteration 254/1000 | Loss: 0.00008191
Iteration 255/1000 | Loss: 0.00008191
Iteration 256/1000 | Loss: 0.00008191
Iteration 257/1000 | Loss: 0.00008190
Iteration 258/1000 | Loss: 0.00008190
Iteration 259/1000 | Loss: 0.00008190
Iteration 260/1000 | Loss: 0.00008190
Iteration 261/1000 | Loss: 0.00008190
Iteration 262/1000 | Loss: 0.00008190
Iteration 263/1000 | Loss: 0.00008190
Iteration 264/1000 | Loss: 0.00008190
Iteration 265/1000 | Loss: 0.00008190
Iteration 266/1000 | Loss: 0.00008190
Iteration 267/1000 | Loss: 0.00008189
Iteration 268/1000 | Loss: 0.00008189
Iteration 269/1000 | Loss: 0.00008189
Iteration 270/1000 | Loss: 0.00008189
Iteration 271/1000 | Loss: 0.00008189
Iteration 272/1000 | Loss: 0.00008189
Iteration 273/1000 | Loss: 0.00008189
Iteration 274/1000 | Loss: 0.00008189
Iteration 275/1000 | Loss: 0.00008188
Iteration 276/1000 | Loss: 0.00008188
Iteration 277/1000 | Loss: 0.00008188
Iteration 278/1000 | Loss: 0.00008188
Iteration 279/1000 | Loss: 0.00008188
Iteration 280/1000 | Loss: 0.00008188
Iteration 281/1000 | Loss: 0.00008188
Iteration 282/1000 | Loss: 0.00008188
Iteration 283/1000 | Loss: 0.00008187
Iteration 284/1000 | Loss: 0.00008187
Iteration 285/1000 | Loss: 0.00008187
Iteration 286/1000 | Loss: 0.00008187
Iteration 287/1000 | Loss: 0.00008187
Iteration 288/1000 | Loss: 0.00008187
Iteration 289/1000 | Loss: 0.00008187
Iteration 290/1000 | Loss: 0.00008186
Iteration 291/1000 | Loss: 0.00008186
Iteration 292/1000 | Loss: 0.00008185
Iteration 293/1000 | Loss: 0.00008184
Iteration 294/1000 | Loss: 0.00008184
Iteration 295/1000 | Loss: 0.00008184
Iteration 296/1000 | Loss: 0.00008184
Iteration 297/1000 | Loss: 0.00008184
Iteration 298/1000 | Loss: 0.00008184
Iteration 299/1000 | Loss: 0.00008184
Iteration 300/1000 | Loss: 0.00008184
Iteration 301/1000 | Loss: 0.00008183
Iteration 302/1000 | Loss: 0.00008183
Iteration 303/1000 | Loss: 0.00008183
Iteration 304/1000 | Loss: 0.00008183
Iteration 305/1000 | Loss: 0.00008183
Iteration 306/1000 | Loss: 0.00008183
Iteration 307/1000 | Loss: 0.00008183
Iteration 308/1000 | Loss: 0.00008183
Iteration 309/1000 | Loss: 0.00008182
Iteration 310/1000 | Loss: 0.00008182
Iteration 311/1000 | Loss: 0.00008182
Iteration 312/1000 | Loss: 0.00008182
Iteration 313/1000 | Loss: 0.00008182
Iteration 314/1000 | Loss: 0.00008182
Iteration 315/1000 | Loss: 0.00008181
Iteration 316/1000 | Loss: 0.00008181
Iteration 317/1000 | Loss: 0.00008181
Iteration 318/1000 | Loss: 0.00008181
Iteration 319/1000 | Loss: 0.00008181
Iteration 320/1000 | Loss: 0.00008181
Iteration 321/1000 | Loss: 0.00008181
Iteration 322/1000 | Loss: 0.00008181
Iteration 323/1000 | Loss: 0.00008181
Iteration 324/1000 | Loss: 0.00008181
Iteration 325/1000 | Loss: 0.00008181
Iteration 326/1000 | Loss: 0.00008180
Iteration 327/1000 | Loss: 0.00008180
Iteration 328/1000 | Loss: 0.00008180
Iteration 329/1000 | Loss: 0.00008180
Iteration 330/1000 | Loss: 0.00008179
Iteration 331/1000 | Loss: 0.00008179
Iteration 332/1000 | Loss: 0.00008179
Iteration 333/1000 | Loss: 0.00008179
Iteration 334/1000 | Loss: 0.00008179
Iteration 335/1000 | Loss: 0.00008179
Iteration 336/1000 | Loss: 0.00008179
Iteration 337/1000 | Loss: 0.00008179
Iteration 338/1000 | Loss: 0.00008179
Iteration 339/1000 | Loss: 0.00008179
Iteration 340/1000 | Loss: 0.00008179
Iteration 341/1000 | Loss: 0.00008179
Iteration 342/1000 | Loss: 0.00008179
Iteration 343/1000 | Loss: 0.00008179
Iteration 344/1000 | Loss: 0.00008179
Iteration 345/1000 | Loss: 0.00008179
Iteration 346/1000 | Loss: 0.00008179
Iteration 347/1000 | Loss: 0.00008179
Iteration 348/1000 | Loss: 0.00008179
Iteration 349/1000 | Loss: 0.00008179
Iteration 350/1000 | Loss: 0.00008179
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 350. Stopping optimization.
Last 5 losses: [8.178732969099656e-05, 8.178732969099656e-05, 8.178732969099656e-05, 8.178732969099656e-05, 8.178732969099656e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.178732969099656e-05

Optimization complete. Final v2v error: 8.012720108032227 mm

Highest mean error: 9.7677640914917 mm for frame 84

Lowest mean error: 7.4531989097595215 mm for frame 114

Saving results

Total time: 420.97679901123047
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01276191
Iteration 2/25 | Loss: 0.00338221
Iteration 3/25 | Loss: 0.00292291
Iteration 4/25 | Loss: 0.00291362
Iteration 5/25 | Loss: 0.00284471
Iteration 6/25 | Loss: 0.00283351
Iteration 7/25 | Loss: 0.00283116
Iteration 8/25 | Loss: 0.00283107
Iteration 9/25 | Loss: 0.00283107
Iteration 10/25 | Loss: 0.00283107
Iteration 11/25 | Loss: 0.00283107
Iteration 12/25 | Loss: 0.00283107
Iteration 13/25 | Loss: 0.00283107
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0028310678899288177, 0.0028310678899288177, 0.0028310678899288177, 0.0028310678899288177, 0.0028310678899288177]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0028310678899288177

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42579412
Iteration 2/25 | Loss: 0.00390242
Iteration 3/25 | Loss: 0.00390242
Iteration 4/25 | Loss: 0.00390242
Iteration 5/25 | Loss: 0.00390242
Iteration 6/25 | Loss: 0.00390242
Iteration 7/25 | Loss: 0.00390242
Iteration 8/25 | Loss: 0.00390242
Iteration 9/25 | Loss: 0.00390242
Iteration 10/25 | Loss: 0.00390242
Iteration 11/25 | Loss: 0.00390242
Iteration 12/25 | Loss: 0.00390242
Iteration 13/25 | Loss: 0.00390242
Iteration 14/25 | Loss: 0.00390242
Iteration 15/25 | Loss: 0.00390242
Iteration 16/25 | Loss: 0.00390242
Iteration 17/25 | Loss: 0.00390242
Iteration 18/25 | Loss: 0.00390242
Iteration 19/25 | Loss: 0.00390242
Iteration 20/25 | Loss: 0.00390242
Iteration 21/25 | Loss: 0.00390242
Iteration 22/25 | Loss: 0.00390242
Iteration 23/25 | Loss: 0.00390242
Iteration 24/25 | Loss: 0.00390242
Iteration 25/25 | Loss: 0.00390242

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00390242
Iteration 2/1000 | Loss: 0.00017604
Iteration 3/1000 | Loss: 0.00010575
Iteration 4/1000 | Loss: 0.00008680
Iteration 5/1000 | Loss: 0.00007921
Iteration 6/1000 | Loss: 0.00007490
Iteration 7/1000 | Loss: 0.00007294
Iteration 8/1000 | Loss: 0.00007174
Iteration 9/1000 | Loss: 0.00007101
Iteration 10/1000 | Loss: 0.00007040
Iteration 11/1000 | Loss: 0.00006995
Iteration 12/1000 | Loss: 0.00006976
Iteration 13/1000 | Loss: 0.00006960
Iteration 14/1000 | Loss: 0.00006955
Iteration 15/1000 | Loss: 0.00006953
Iteration 16/1000 | Loss: 0.00006953
Iteration 17/1000 | Loss: 0.00006953
Iteration 18/1000 | Loss: 0.00006952
Iteration 19/1000 | Loss: 0.00006952
Iteration 20/1000 | Loss: 0.00006951
Iteration 21/1000 | Loss: 0.00006950
Iteration 22/1000 | Loss: 0.00006950
Iteration 23/1000 | Loss: 0.00006949
Iteration 24/1000 | Loss: 0.00006949
Iteration 25/1000 | Loss: 0.00006949
Iteration 26/1000 | Loss: 0.00006948
Iteration 27/1000 | Loss: 0.00006948
Iteration 28/1000 | Loss: 0.00006947
Iteration 29/1000 | Loss: 0.00006947
Iteration 30/1000 | Loss: 0.00006947
Iteration 31/1000 | Loss: 0.00006946
Iteration 32/1000 | Loss: 0.00006946
Iteration 33/1000 | Loss: 0.00006945
Iteration 34/1000 | Loss: 0.00006945
Iteration 35/1000 | Loss: 0.00006945
Iteration 36/1000 | Loss: 0.00006944
Iteration 37/1000 | Loss: 0.00006944
Iteration 38/1000 | Loss: 0.00006944
Iteration 39/1000 | Loss: 0.00006944
Iteration 40/1000 | Loss: 0.00006943
Iteration 41/1000 | Loss: 0.00006943
Iteration 42/1000 | Loss: 0.00006943
Iteration 43/1000 | Loss: 0.00006943
Iteration 44/1000 | Loss: 0.00006943
Iteration 45/1000 | Loss: 0.00006943
Iteration 46/1000 | Loss: 0.00006943
Iteration 47/1000 | Loss: 0.00006942
Iteration 48/1000 | Loss: 0.00006942
Iteration 49/1000 | Loss: 0.00006942
Iteration 50/1000 | Loss: 0.00006942
Iteration 51/1000 | Loss: 0.00006942
Iteration 52/1000 | Loss: 0.00006942
Iteration 53/1000 | Loss: 0.00006942
Iteration 54/1000 | Loss: 0.00006942
Iteration 55/1000 | Loss: 0.00006942
Iteration 56/1000 | Loss: 0.00006942
Iteration 57/1000 | Loss: 0.00006942
Iteration 58/1000 | Loss: 0.00006941
Iteration 59/1000 | Loss: 0.00006941
Iteration 60/1000 | Loss: 0.00006941
Iteration 61/1000 | Loss: 0.00006941
Iteration 62/1000 | Loss: 0.00006940
Iteration 63/1000 | Loss: 0.00006940
Iteration 64/1000 | Loss: 0.00006939
Iteration 65/1000 | Loss: 0.00006939
Iteration 66/1000 | Loss: 0.00006939
Iteration 67/1000 | Loss: 0.00006939
Iteration 68/1000 | Loss: 0.00006939
Iteration 69/1000 | Loss: 0.00006939
Iteration 70/1000 | Loss: 0.00006939
Iteration 71/1000 | Loss: 0.00006939
Iteration 72/1000 | Loss: 0.00006939
Iteration 73/1000 | Loss: 0.00006939
Iteration 74/1000 | Loss: 0.00006939
Iteration 75/1000 | Loss: 0.00006939
Iteration 76/1000 | Loss: 0.00006939
Iteration 77/1000 | Loss: 0.00006939
Iteration 78/1000 | Loss: 0.00006938
Iteration 79/1000 | Loss: 0.00006938
Iteration 80/1000 | Loss: 0.00006938
Iteration 81/1000 | Loss: 0.00006938
Iteration 82/1000 | Loss: 0.00006938
Iteration 83/1000 | Loss: 0.00006938
Iteration 84/1000 | Loss: 0.00006938
Iteration 85/1000 | Loss: 0.00006937
Iteration 86/1000 | Loss: 0.00006937
Iteration 87/1000 | Loss: 0.00006937
Iteration 88/1000 | Loss: 0.00006937
Iteration 89/1000 | Loss: 0.00006937
Iteration 90/1000 | Loss: 0.00006937
Iteration 91/1000 | Loss: 0.00006937
Iteration 92/1000 | Loss: 0.00006937
Iteration 93/1000 | Loss: 0.00006937
Iteration 94/1000 | Loss: 0.00006937
Iteration 95/1000 | Loss: 0.00006937
Iteration 96/1000 | Loss: 0.00006937
Iteration 97/1000 | Loss: 0.00006936
Iteration 98/1000 | Loss: 0.00006936
Iteration 99/1000 | Loss: 0.00006936
Iteration 100/1000 | Loss: 0.00006936
Iteration 101/1000 | Loss: 0.00006936
Iteration 102/1000 | Loss: 0.00006936
Iteration 103/1000 | Loss: 0.00006936
Iteration 104/1000 | Loss: 0.00006936
Iteration 105/1000 | Loss: 0.00006936
Iteration 106/1000 | Loss: 0.00006936
Iteration 107/1000 | Loss: 0.00006936
Iteration 108/1000 | Loss: 0.00006935
Iteration 109/1000 | Loss: 0.00006935
Iteration 110/1000 | Loss: 0.00006935
Iteration 111/1000 | Loss: 0.00006935
Iteration 112/1000 | Loss: 0.00006935
Iteration 113/1000 | Loss: 0.00006935
Iteration 114/1000 | Loss: 0.00006935
Iteration 115/1000 | Loss: 0.00006935
Iteration 116/1000 | Loss: 0.00006935
Iteration 117/1000 | Loss: 0.00006935
Iteration 118/1000 | Loss: 0.00006935
Iteration 119/1000 | Loss: 0.00006935
Iteration 120/1000 | Loss: 0.00006935
Iteration 121/1000 | Loss: 0.00006935
Iteration 122/1000 | Loss: 0.00006935
Iteration 123/1000 | Loss: 0.00006935
Iteration 124/1000 | Loss: 0.00006935
Iteration 125/1000 | Loss: 0.00006934
Iteration 126/1000 | Loss: 0.00006934
Iteration 127/1000 | Loss: 0.00006934
Iteration 128/1000 | Loss: 0.00006934
Iteration 129/1000 | Loss: 0.00006934
Iteration 130/1000 | Loss: 0.00006934
Iteration 131/1000 | Loss: 0.00006934
Iteration 132/1000 | Loss: 0.00006934
Iteration 133/1000 | Loss: 0.00006934
Iteration 134/1000 | Loss: 0.00006934
Iteration 135/1000 | Loss: 0.00006934
Iteration 136/1000 | Loss: 0.00006934
Iteration 137/1000 | Loss: 0.00006934
Iteration 138/1000 | Loss: 0.00006934
Iteration 139/1000 | Loss: 0.00006934
Iteration 140/1000 | Loss: 0.00006934
Iteration 141/1000 | Loss: 0.00006934
Iteration 142/1000 | Loss: 0.00006934
Iteration 143/1000 | Loss: 0.00006934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [6.934219709364697e-05, 6.934219709364697e-05, 6.934219709364697e-05, 6.934219709364697e-05, 6.934219709364697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.934219709364697e-05

Optimization complete. Final v2v error: 7.280574321746826 mm

Highest mean error: 7.518567085266113 mm for frame 131

Lowest mean error: 7.13956356048584 mm for frame 14

Saving results

Total time: 38.1438250541687
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_70_us_0553/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_70_us_0553/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01235697
Iteration 2/25 | Loss: 0.00318462
Iteration 3/25 | Loss: 0.00298426
Iteration 4/25 | Loss: 0.00295294
Iteration 5/25 | Loss: 0.00293924
Iteration 6/25 | Loss: 0.00293544
Iteration 7/25 | Loss: 0.00293499
Iteration 8/25 | Loss: 0.00293499
Iteration 9/25 | Loss: 0.00293499
Iteration 10/25 | Loss: 0.00293499
Iteration 11/25 | Loss: 0.00293499
Iteration 12/25 | Loss: 0.00293499
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0029349899850785732, 0.0029349899850785732, 0.0029349899850785732, 0.0029349899850785732, 0.0029349899850785732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0029349899850785732

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.85063130
Iteration 2/25 | Loss: 0.00447401
Iteration 3/25 | Loss: 0.00447401
Iteration 4/25 | Loss: 0.00447401
Iteration 5/25 | Loss: 0.00447401
Iteration 6/25 | Loss: 0.00447401
Iteration 7/25 | Loss: 0.00447401
Iteration 8/25 | Loss: 0.00447401
Iteration 9/25 | Loss: 0.00447401
Iteration 10/25 | Loss: 0.00447401
Iteration 11/25 | Loss: 0.00447401
Iteration 12/25 | Loss: 0.00447401
Iteration 13/25 | Loss: 0.00447401
Iteration 14/25 | Loss: 0.00447401
Iteration 15/25 | Loss: 0.00447401
Iteration 16/25 | Loss: 0.00447401
Iteration 17/25 | Loss: 0.00447401
Iteration 18/25 | Loss: 0.00447401
Iteration 19/25 | Loss: 0.00447401
Iteration 20/25 | Loss: 0.00447401
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.004474007990211248, 0.004474007990211248, 0.004474007990211248, 0.004474007990211248, 0.004474007990211248]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004474007990211248

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00447401
Iteration 2/1000 | Loss: 0.00020318
Iteration 3/1000 | Loss: 0.00013751
Iteration 4/1000 | Loss: 0.00011302
Iteration 5/1000 | Loss: 0.00010380
Iteration 6/1000 | Loss: 0.00009716
Iteration 7/1000 | Loss: 0.00009303
Iteration 8/1000 | Loss: 0.00009093
Iteration 9/1000 | Loss: 0.00008938
Iteration 10/1000 | Loss: 0.00008829
Iteration 11/1000 | Loss: 0.00008728
Iteration 12/1000 | Loss: 0.00008677
Iteration 13/1000 | Loss: 0.00008617
Iteration 14/1000 | Loss: 0.00008570
Iteration 15/1000 | Loss: 0.00008530
Iteration 16/1000 | Loss: 0.00008499
Iteration 17/1000 | Loss: 0.00008477
Iteration 18/1000 | Loss: 0.00008471
Iteration 19/1000 | Loss: 0.00008456
Iteration 20/1000 | Loss: 0.00008454
Iteration 21/1000 | Loss: 0.00008454
Iteration 22/1000 | Loss: 0.00008453
Iteration 23/1000 | Loss: 0.00008452
Iteration 24/1000 | Loss: 0.00008452
Iteration 25/1000 | Loss: 0.00008451
Iteration 26/1000 | Loss: 0.00008451
Iteration 27/1000 | Loss: 0.00008451
Iteration 28/1000 | Loss: 0.00008450
Iteration 29/1000 | Loss: 0.00008450
Iteration 30/1000 | Loss: 0.00008450
Iteration 31/1000 | Loss: 0.00008448
Iteration 32/1000 | Loss: 0.00008448
Iteration 33/1000 | Loss: 0.00008447
Iteration 34/1000 | Loss: 0.00008447
Iteration 35/1000 | Loss: 0.00008446
Iteration 36/1000 | Loss: 0.00008443
Iteration 37/1000 | Loss: 0.00008443
Iteration 38/1000 | Loss: 0.00008443
Iteration 39/1000 | Loss: 0.00008443
Iteration 40/1000 | Loss: 0.00008443
Iteration 41/1000 | Loss: 0.00008442
Iteration 42/1000 | Loss: 0.00008442
Iteration 43/1000 | Loss: 0.00008442
Iteration 44/1000 | Loss: 0.00008442
Iteration 45/1000 | Loss: 0.00008442
Iteration 46/1000 | Loss: 0.00008442
Iteration 47/1000 | Loss: 0.00008442
Iteration 48/1000 | Loss: 0.00008442
Iteration 49/1000 | Loss: 0.00008442
Iteration 50/1000 | Loss: 0.00008442
Iteration 51/1000 | Loss: 0.00008442
Iteration 52/1000 | Loss: 0.00008441
Iteration 53/1000 | Loss: 0.00008441
Iteration 54/1000 | Loss: 0.00008440
Iteration 55/1000 | Loss: 0.00008440
Iteration 56/1000 | Loss: 0.00008440
Iteration 57/1000 | Loss: 0.00008440
Iteration 58/1000 | Loss: 0.00008439
Iteration 59/1000 | Loss: 0.00008439
Iteration 60/1000 | Loss: 0.00008439
Iteration 61/1000 | Loss: 0.00008439
Iteration 62/1000 | Loss: 0.00008439
Iteration 63/1000 | Loss: 0.00008439
Iteration 64/1000 | Loss: 0.00008439
Iteration 65/1000 | Loss: 0.00008439
Iteration 66/1000 | Loss: 0.00008438
Iteration 67/1000 | Loss: 0.00008438
Iteration 68/1000 | Loss: 0.00008438
Iteration 69/1000 | Loss: 0.00008438
Iteration 70/1000 | Loss: 0.00008438
Iteration 71/1000 | Loss: 0.00008438
Iteration 72/1000 | Loss: 0.00008437
Iteration 73/1000 | Loss: 0.00008437
Iteration 74/1000 | Loss: 0.00008437
Iteration 75/1000 | Loss: 0.00008437
Iteration 76/1000 | Loss: 0.00008437
Iteration 77/1000 | Loss: 0.00008437
Iteration 78/1000 | Loss: 0.00008437
Iteration 79/1000 | Loss: 0.00008437
Iteration 80/1000 | Loss: 0.00008437
Iteration 81/1000 | Loss: 0.00008437
Iteration 82/1000 | Loss: 0.00008437
Iteration 83/1000 | Loss: 0.00008436
Iteration 84/1000 | Loss: 0.00008436
Iteration 85/1000 | Loss: 0.00008436
Iteration 86/1000 | Loss: 0.00008436
Iteration 87/1000 | Loss: 0.00008435
Iteration 88/1000 | Loss: 0.00008435
Iteration 89/1000 | Loss: 0.00008435
Iteration 90/1000 | Loss: 0.00008435
Iteration 91/1000 | Loss: 0.00008434
Iteration 92/1000 | Loss: 0.00008434
Iteration 93/1000 | Loss: 0.00008434
Iteration 94/1000 | Loss: 0.00008434
Iteration 95/1000 | Loss: 0.00008434
Iteration 96/1000 | Loss: 0.00008434
Iteration 97/1000 | Loss: 0.00008434
Iteration 98/1000 | Loss: 0.00008434
Iteration 99/1000 | Loss: 0.00008434
Iteration 100/1000 | Loss: 0.00008433
Iteration 101/1000 | Loss: 0.00008433
Iteration 102/1000 | Loss: 0.00008433
Iteration 103/1000 | Loss: 0.00008433
Iteration 104/1000 | Loss: 0.00008432
Iteration 105/1000 | Loss: 0.00008432
Iteration 106/1000 | Loss: 0.00008432
Iteration 107/1000 | Loss: 0.00008432
Iteration 108/1000 | Loss: 0.00008432
Iteration 109/1000 | Loss: 0.00008432
Iteration 110/1000 | Loss: 0.00008431
Iteration 111/1000 | Loss: 0.00008431
Iteration 112/1000 | Loss: 0.00008431
Iteration 113/1000 | Loss: 0.00008430
Iteration 114/1000 | Loss: 0.00008430
Iteration 115/1000 | Loss: 0.00008430
Iteration 116/1000 | Loss: 0.00008430
Iteration 117/1000 | Loss: 0.00008430
Iteration 118/1000 | Loss: 0.00008430
Iteration 119/1000 | Loss: 0.00008430
Iteration 120/1000 | Loss: 0.00008430
Iteration 121/1000 | Loss: 0.00008429
Iteration 122/1000 | Loss: 0.00008429
Iteration 123/1000 | Loss: 0.00008429
Iteration 124/1000 | Loss: 0.00008429
Iteration 125/1000 | Loss: 0.00008429
Iteration 126/1000 | Loss: 0.00008429
Iteration 127/1000 | Loss: 0.00008428
Iteration 128/1000 | Loss: 0.00008428
Iteration 129/1000 | Loss: 0.00008428
Iteration 130/1000 | Loss: 0.00008428
Iteration 131/1000 | Loss: 0.00008428
Iteration 132/1000 | Loss: 0.00008427
Iteration 133/1000 | Loss: 0.00008427
Iteration 134/1000 | Loss: 0.00008427
Iteration 135/1000 | Loss: 0.00008427
Iteration 136/1000 | Loss: 0.00008427
Iteration 137/1000 | Loss: 0.00008427
Iteration 138/1000 | Loss: 0.00008427
Iteration 139/1000 | Loss: 0.00008427
Iteration 140/1000 | Loss: 0.00008427
Iteration 141/1000 | Loss: 0.00008427
Iteration 142/1000 | Loss: 0.00008427
Iteration 143/1000 | Loss: 0.00008427
Iteration 144/1000 | Loss: 0.00008427
Iteration 145/1000 | Loss: 0.00008426
Iteration 146/1000 | Loss: 0.00008426
Iteration 147/1000 | Loss: 0.00008426
Iteration 148/1000 | Loss: 0.00008426
Iteration 149/1000 | Loss: 0.00008426
Iteration 150/1000 | Loss: 0.00008426
Iteration 151/1000 | Loss: 0.00008426
Iteration 152/1000 | Loss: 0.00008425
Iteration 153/1000 | Loss: 0.00008425
Iteration 154/1000 | Loss: 0.00008425
Iteration 155/1000 | Loss: 0.00008425
Iteration 156/1000 | Loss: 0.00008425
Iteration 157/1000 | Loss: 0.00008425
Iteration 158/1000 | Loss: 0.00008425
Iteration 159/1000 | Loss: 0.00008425
Iteration 160/1000 | Loss: 0.00008425
Iteration 161/1000 | Loss: 0.00008424
Iteration 162/1000 | Loss: 0.00008424
Iteration 163/1000 | Loss: 0.00008424
Iteration 164/1000 | Loss: 0.00008424
Iteration 165/1000 | Loss: 0.00008424
Iteration 166/1000 | Loss: 0.00008424
Iteration 167/1000 | Loss: 0.00008424
Iteration 168/1000 | Loss: 0.00008424
Iteration 169/1000 | Loss: 0.00008424
Iteration 170/1000 | Loss: 0.00008424
Iteration 171/1000 | Loss: 0.00008424
Iteration 172/1000 | Loss: 0.00008424
Iteration 173/1000 | Loss: 0.00008424
Iteration 174/1000 | Loss: 0.00008423
Iteration 175/1000 | Loss: 0.00008423
Iteration 176/1000 | Loss: 0.00008423
Iteration 177/1000 | Loss: 0.00008423
Iteration 178/1000 | Loss: 0.00008423
Iteration 179/1000 | Loss: 0.00008423
Iteration 180/1000 | Loss: 0.00008423
Iteration 181/1000 | Loss: 0.00008423
Iteration 182/1000 | Loss: 0.00008423
Iteration 183/1000 | Loss: 0.00008423
Iteration 184/1000 | Loss: 0.00008423
Iteration 185/1000 | Loss: 0.00008422
Iteration 186/1000 | Loss: 0.00008422
Iteration 187/1000 | Loss: 0.00008422
Iteration 188/1000 | Loss: 0.00008422
Iteration 189/1000 | Loss: 0.00008422
Iteration 190/1000 | Loss: 0.00008422
Iteration 191/1000 | Loss: 0.00008422
Iteration 192/1000 | Loss: 0.00008422
Iteration 193/1000 | Loss: 0.00008422
Iteration 194/1000 | Loss: 0.00008422
Iteration 195/1000 | Loss: 0.00008422
Iteration 196/1000 | Loss: 0.00008422
Iteration 197/1000 | Loss: 0.00008422
Iteration 198/1000 | Loss: 0.00008422
Iteration 199/1000 | Loss: 0.00008422
Iteration 200/1000 | Loss: 0.00008422
Iteration 201/1000 | Loss: 0.00008422
Iteration 202/1000 | Loss: 0.00008422
Iteration 203/1000 | Loss: 0.00008422
Iteration 204/1000 | Loss: 0.00008422
Iteration 205/1000 | Loss: 0.00008422
Iteration 206/1000 | Loss: 0.00008422
Iteration 207/1000 | Loss: 0.00008422
Iteration 208/1000 | Loss: 0.00008422
Iteration 209/1000 | Loss: 0.00008422
Iteration 210/1000 | Loss: 0.00008422
Iteration 211/1000 | Loss: 0.00008422
Iteration 212/1000 | Loss: 0.00008422
Iteration 213/1000 | Loss: 0.00008422
Iteration 214/1000 | Loss: 0.00008422
Iteration 215/1000 | Loss: 0.00008422
Iteration 216/1000 | Loss: 0.00008422
Iteration 217/1000 | Loss: 0.00008422
Iteration 218/1000 | Loss: 0.00008422
Iteration 219/1000 | Loss: 0.00008422
Iteration 220/1000 | Loss: 0.00008422
Iteration 221/1000 | Loss: 0.00008422
Iteration 222/1000 | Loss: 0.00008422
Iteration 223/1000 | Loss: 0.00008422
Iteration 224/1000 | Loss: 0.00008422
Iteration 225/1000 | Loss: 0.00008422
Iteration 226/1000 | Loss: 0.00008422
Iteration 227/1000 | Loss: 0.00008422
Iteration 228/1000 | Loss: 0.00008422
Iteration 229/1000 | Loss: 0.00008422
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 229. Stopping optimization.
Last 5 losses: [8.421679376624525e-05, 8.421679376624525e-05, 8.421679376624525e-05, 8.421679376624525e-05, 8.421679376624525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.421679376624525e-05

Optimization complete. Final v2v error: 8.000093460083008 mm

Highest mean error: 8.832109451293945 mm for frame 0

Lowest mean error: 7.573614597320557 mm for frame 131

Saving results

Total time: 48.91052293777466
