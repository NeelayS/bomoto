Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=244, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13664-13719
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926757
Iteration 2/25 | Loss: 0.00149122
Iteration 3/25 | Loss: 0.00140780
Iteration 4/25 | Loss: 0.00139691
Iteration 5/25 | Loss: 0.00139367
Iteration 6/25 | Loss: 0.00139323
Iteration 7/25 | Loss: 0.00139323
Iteration 8/25 | Loss: 0.00139323
Iteration 9/25 | Loss: 0.00139323
Iteration 10/25 | Loss: 0.00139323
Iteration 11/25 | Loss: 0.00139323
Iteration 12/25 | Loss: 0.00139323
Iteration 13/25 | Loss: 0.00139323
Iteration 14/25 | Loss: 0.00139323
Iteration 15/25 | Loss: 0.00139323
Iteration 16/25 | Loss: 0.00139323
Iteration 17/25 | Loss: 0.00139323
Iteration 18/25 | Loss: 0.00139323
Iteration 19/25 | Loss: 0.00139323
Iteration 20/25 | Loss: 0.00139323
Iteration 21/25 | Loss: 0.00139323
Iteration 22/25 | Loss: 0.00139323
Iteration 23/25 | Loss: 0.00139323
Iteration 24/25 | Loss: 0.00139323
Iteration 25/25 | Loss: 0.00139323

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42718613
Iteration 2/25 | Loss: 0.00200045
Iteration 3/25 | Loss: 0.00200045
Iteration 4/25 | Loss: 0.00200045
Iteration 5/25 | Loss: 0.00200045
Iteration 6/25 | Loss: 0.00200044
Iteration 7/25 | Loss: 0.00200044
Iteration 8/25 | Loss: 0.00200044
Iteration 9/25 | Loss: 0.00200044
Iteration 10/25 | Loss: 0.00200044
Iteration 11/25 | Loss: 0.00200044
Iteration 12/25 | Loss: 0.00200044
Iteration 13/25 | Loss: 0.00200044
Iteration 14/25 | Loss: 0.00200044
Iteration 15/25 | Loss: 0.00200044
Iteration 16/25 | Loss: 0.00200044
Iteration 17/25 | Loss: 0.00200044
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0020004434045404196, 0.0020004434045404196, 0.0020004434045404196, 0.0020004434045404196, 0.0020004434045404196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0020004434045404196

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00200044
Iteration 2/1000 | Loss: 0.00002820
Iteration 3/1000 | Loss: 0.00002109
Iteration 4/1000 | Loss: 0.00001870
Iteration 5/1000 | Loss: 0.00001776
Iteration 6/1000 | Loss: 0.00001704
Iteration 7/1000 | Loss: 0.00001659
Iteration 8/1000 | Loss: 0.00001634
Iteration 9/1000 | Loss: 0.00001623
Iteration 10/1000 | Loss: 0.00001618
Iteration 11/1000 | Loss: 0.00001617
Iteration 12/1000 | Loss: 0.00001616
Iteration 13/1000 | Loss: 0.00001611
Iteration 14/1000 | Loss: 0.00001606
Iteration 15/1000 | Loss: 0.00001606
Iteration 16/1000 | Loss: 0.00001605
Iteration 17/1000 | Loss: 0.00001598
Iteration 18/1000 | Loss: 0.00001596
Iteration 19/1000 | Loss: 0.00001595
Iteration 20/1000 | Loss: 0.00001593
Iteration 21/1000 | Loss: 0.00001592
Iteration 22/1000 | Loss: 0.00001591
Iteration 23/1000 | Loss: 0.00001590
Iteration 24/1000 | Loss: 0.00001589
Iteration 25/1000 | Loss: 0.00001583
Iteration 26/1000 | Loss: 0.00001583
Iteration 27/1000 | Loss: 0.00001582
Iteration 28/1000 | Loss: 0.00001577
Iteration 29/1000 | Loss: 0.00001577
Iteration 30/1000 | Loss: 0.00001575
Iteration 31/1000 | Loss: 0.00001572
Iteration 32/1000 | Loss: 0.00001572
Iteration 33/1000 | Loss: 0.00001572
Iteration 34/1000 | Loss: 0.00001572
Iteration 35/1000 | Loss: 0.00001571
Iteration 36/1000 | Loss: 0.00001571
Iteration 37/1000 | Loss: 0.00001571
Iteration 38/1000 | Loss: 0.00001571
Iteration 39/1000 | Loss: 0.00001571
Iteration 40/1000 | Loss: 0.00001571
Iteration 41/1000 | Loss: 0.00001571
Iteration 42/1000 | Loss: 0.00001571
Iteration 43/1000 | Loss: 0.00001570
Iteration 44/1000 | Loss: 0.00001570
Iteration 45/1000 | Loss: 0.00001570
Iteration 46/1000 | Loss: 0.00001569
Iteration 47/1000 | Loss: 0.00001569
Iteration 48/1000 | Loss: 0.00001569
Iteration 49/1000 | Loss: 0.00001569
Iteration 50/1000 | Loss: 0.00001568
Iteration 51/1000 | Loss: 0.00001568
Iteration 52/1000 | Loss: 0.00001568
Iteration 53/1000 | Loss: 0.00001568
Iteration 54/1000 | Loss: 0.00001567
Iteration 55/1000 | Loss: 0.00001567
Iteration 56/1000 | Loss: 0.00001567
Iteration 57/1000 | Loss: 0.00001567
Iteration 58/1000 | Loss: 0.00001567
Iteration 59/1000 | Loss: 0.00001567
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001567
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001567
Iteration 64/1000 | Loss: 0.00001566
Iteration 65/1000 | Loss: 0.00001566
Iteration 66/1000 | Loss: 0.00001566
Iteration 67/1000 | Loss: 0.00001566
Iteration 68/1000 | Loss: 0.00001566
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001566
Iteration 71/1000 | Loss: 0.00001566
Iteration 72/1000 | Loss: 0.00001566
Iteration 73/1000 | Loss: 0.00001566
Iteration 74/1000 | Loss: 0.00001566
Iteration 75/1000 | Loss: 0.00001566
Iteration 76/1000 | Loss: 0.00001566
Iteration 77/1000 | Loss: 0.00001566
Iteration 78/1000 | Loss: 0.00001565
Iteration 79/1000 | Loss: 0.00001565
Iteration 80/1000 | Loss: 0.00001565
Iteration 81/1000 | Loss: 0.00001565
Iteration 82/1000 | Loss: 0.00001565
Iteration 83/1000 | Loss: 0.00001564
Iteration 84/1000 | Loss: 0.00001564
Iteration 85/1000 | Loss: 0.00001564
Iteration 86/1000 | Loss: 0.00001563
Iteration 87/1000 | Loss: 0.00001563
Iteration 88/1000 | Loss: 0.00001563
Iteration 89/1000 | Loss: 0.00001563
Iteration 90/1000 | Loss: 0.00001563
Iteration 91/1000 | Loss: 0.00001563
Iteration 92/1000 | Loss: 0.00001563
Iteration 93/1000 | Loss: 0.00001563
Iteration 94/1000 | Loss: 0.00001563
Iteration 95/1000 | Loss: 0.00001563
Iteration 96/1000 | Loss: 0.00001563
Iteration 97/1000 | Loss: 0.00001563
Iteration 98/1000 | Loss: 0.00001563
Iteration 99/1000 | Loss: 0.00001563
Iteration 100/1000 | Loss: 0.00001563
Iteration 101/1000 | Loss: 0.00001563
Iteration 102/1000 | Loss: 0.00001563
Iteration 103/1000 | Loss: 0.00001563
Iteration 104/1000 | Loss: 0.00001563
Iteration 105/1000 | Loss: 0.00001563
Iteration 106/1000 | Loss: 0.00001563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.5627540051355027e-05, 1.5627540051355027e-05, 1.5627540051355027e-05, 1.5627540051355027e-05, 1.5627540051355027e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5627540051355027e-05

Optimization complete. Final v2v error: 3.3844292163848877 mm

Highest mean error: 4.102781295776367 mm for frame 88

Lowest mean error: 3.0308525562286377 mm for frame 47

Saving results

Total time: 36.38671255111694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00919039
Iteration 2/25 | Loss: 0.00163899
Iteration 3/25 | Loss: 0.00144964
Iteration 4/25 | Loss: 0.00142833
Iteration 5/25 | Loss: 0.00142596
Iteration 6/25 | Loss: 0.00143713
Iteration 7/25 | Loss: 0.00142732
Iteration 8/25 | Loss: 0.00141341
Iteration 9/25 | Loss: 0.00141187
Iteration 10/25 | Loss: 0.00140357
Iteration 11/25 | Loss: 0.00140237
Iteration 12/25 | Loss: 0.00140325
Iteration 13/25 | Loss: 0.00139504
Iteration 14/25 | Loss: 0.00139162
Iteration 15/25 | Loss: 0.00139030
Iteration 16/25 | Loss: 0.00138891
Iteration 17/25 | Loss: 0.00138838
Iteration 18/25 | Loss: 0.00138812
Iteration 19/25 | Loss: 0.00138811
Iteration 20/25 | Loss: 0.00138811
Iteration 21/25 | Loss: 0.00138811
Iteration 22/25 | Loss: 0.00138810
Iteration 23/25 | Loss: 0.00138810
Iteration 24/25 | Loss: 0.00138810
Iteration 25/25 | Loss: 0.00138810

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.05465555
Iteration 2/25 | Loss: 0.00172890
Iteration 3/25 | Loss: 0.00172887
Iteration 4/25 | Loss: 0.00172887
Iteration 5/25 | Loss: 0.00172887
Iteration 6/25 | Loss: 0.00172887
Iteration 7/25 | Loss: 0.00172886
Iteration 8/25 | Loss: 0.00172886
Iteration 9/25 | Loss: 0.00172886
Iteration 10/25 | Loss: 0.00172886
Iteration 11/25 | Loss: 0.00172886
Iteration 12/25 | Loss: 0.00172886
Iteration 13/25 | Loss: 0.00172886
Iteration 14/25 | Loss: 0.00172886
Iteration 15/25 | Loss: 0.00172886
Iteration 16/25 | Loss: 0.00172886
Iteration 17/25 | Loss: 0.00172886
Iteration 18/25 | Loss: 0.00172886
Iteration 19/25 | Loss: 0.00172886
Iteration 20/25 | Loss: 0.00172886
Iteration 21/25 | Loss: 0.00172886
Iteration 22/25 | Loss: 0.00172886
Iteration 23/25 | Loss: 0.00172886
Iteration 24/25 | Loss: 0.00172886
Iteration 25/25 | Loss: 0.00172886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00172886
Iteration 2/1000 | Loss: 0.00004449
Iteration 3/1000 | Loss: 0.00003068
Iteration 4/1000 | Loss: 0.00002621
Iteration 5/1000 | Loss: 0.00002462
Iteration 6/1000 | Loss: 0.00002361
Iteration 7/1000 | Loss: 0.00002294
Iteration 8/1000 | Loss: 0.00002261
Iteration 9/1000 | Loss: 0.00002236
Iteration 10/1000 | Loss: 0.00002221
Iteration 11/1000 | Loss: 0.00002218
Iteration 12/1000 | Loss: 0.00002215
Iteration 13/1000 | Loss: 0.00002215
Iteration 14/1000 | Loss: 0.00002215
Iteration 15/1000 | Loss: 0.00002213
Iteration 16/1000 | Loss: 0.00002211
Iteration 17/1000 | Loss: 0.00002211
Iteration 18/1000 | Loss: 0.00002211
Iteration 19/1000 | Loss: 0.00002210
Iteration 20/1000 | Loss: 0.00002210
Iteration 21/1000 | Loss: 0.00002209
Iteration 22/1000 | Loss: 0.00002208
Iteration 23/1000 | Loss: 0.00002207
Iteration 24/1000 | Loss: 0.00002207
Iteration 25/1000 | Loss: 0.00002201
Iteration 26/1000 | Loss: 0.00002201
Iteration 27/1000 | Loss: 0.00002201
Iteration 28/1000 | Loss: 0.00002201
Iteration 29/1000 | Loss: 0.00002201
Iteration 30/1000 | Loss: 0.00002200
Iteration 31/1000 | Loss: 0.00002200
Iteration 32/1000 | Loss: 0.00002200
Iteration 33/1000 | Loss: 0.00002200
Iteration 34/1000 | Loss: 0.00002199
Iteration 35/1000 | Loss: 0.00002197
Iteration 36/1000 | Loss: 0.00002197
Iteration 37/1000 | Loss: 0.00002197
Iteration 38/1000 | Loss: 0.00002197
Iteration 39/1000 | Loss: 0.00002196
Iteration 40/1000 | Loss: 0.00002196
Iteration 41/1000 | Loss: 0.00002196
Iteration 42/1000 | Loss: 0.00002196
Iteration 43/1000 | Loss: 0.00002196
Iteration 44/1000 | Loss: 0.00002195
Iteration 45/1000 | Loss: 0.00002195
Iteration 46/1000 | Loss: 0.00002194
Iteration 47/1000 | Loss: 0.00002194
Iteration 48/1000 | Loss: 0.00002194
Iteration 49/1000 | Loss: 0.00002194
Iteration 50/1000 | Loss: 0.00002194
Iteration 51/1000 | Loss: 0.00002194
Iteration 52/1000 | Loss: 0.00002194
Iteration 53/1000 | Loss: 0.00002194
Iteration 54/1000 | Loss: 0.00002194
Iteration 55/1000 | Loss: 0.00002194
Iteration 56/1000 | Loss: 0.00002193
Iteration 57/1000 | Loss: 0.00002193
Iteration 58/1000 | Loss: 0.00002192
Iteration 59/1000 | Loss: 0.00002192
Iteration 60/1000 | Loss: 0.00002191
Iteration 61/1000 | Loss: 0.00002191
Iteration 62/1000 | Loss: 0.00002191
Iteration 63/1000 | Loss: 0.00002190
Iteration 64/1000 | Loss: 0.00002190
Iteration 65/1000 | Loss: 0.00002190
Iteration 66/1000 | Loss: 0.00002189
Iteration 67/1000 | Loss: 0.00002189
Iteration 68/1000 | Loss: 0.00002188
Iteration 69/1000 | Loss: 0.00002188
Iteration 70/1000 | Loss: 0.00002188
Iteration 71/1000 | Loss: 0.00002187
Iteration 72/1000 | Loss: 0.00002187
Iteration 73/1000 | Loss: 0.00002187
Iteration 74/1000 | Loss: 0.00002187
Iteration 75/1000 | Loss: 0.00002187
Iteration 76/1000 | Loss: 0.00002187
Iteration 77/1000 | Loss: 0.00002187
Iteration 78/1000 | Loss: 0.00002186
Iteration 79/1000 | Loss: 0.00002186
Iteration 80/1000 | Loss: 0.00002186
Iteration 81/1000 | Loss: 0.00002185
Iteration 82/1000 | Loss: 0.00002185
Iteration 83/1000 | Loss: 0.00002185
Iteration 84/1000 | Loss: 0.00002185
Iteration 85/1000 | Loss: 0.00002185
Iteration 86/1000 | Loss: 0.00002184
Iteration 87/1000 | Loss: 0.00002184
Iteration 88/1000 | Loss: 0.00002184
Iteration 89/1000 | Loss: 0.00002184
Iteration 90/1000 | Loss: 0.00002184
Iteration 91/1000 | Loss: 0.00002184
Iteration 92/1000 | Loss: 0.00002184
Iteration 93/1000 | Loss: 0.00002184
Iteration 94/1000 | Loss: 0.00002183
Iteration 95/1000 | Loss: 0.00002183
Iteration 96/1000 | Loss: 0.00002183
Iteration 97/1000 | Loss: 0.00002183
Iteration 98/1000 | Loss: 0.00002183
Iteration 99/1000 | Loss: 0.00002183
Iteration 100/1000 | Loss: 0.00002183
Iteration 101/1000 | Loss: 0.00002182
Iteration 102/1000 | Loss: 0.00002182
Iteration 103/1000 | Loss: 0.00002182
Iteration 104/1000 | Loss: 0.00002182
Iteration 105/1000 | Loss: 0.00002182
Iteration 106/1000 | Loss: 0.00002182
Iteration 107/1000 | Loss: 0.00002182
Iteration 108/1000 | Loss: 0.00002182
Iteration 109/1000 | Loss: 0.00002182
Iteration 110/1000 | Loss: 0.00002182
Iteration 111/1000 | Loss: 0.00002182
Iteration 112/1000 | Loss: 0.00002182
Iteration 113/1000 | Loss: 0.00002182
Iteration 114/1000 | Loss: 0.00002182
Iteration 115/1000 | Loss: 0.00002181
Iteration 116/1000 | Loss: 0.00002181
Iteration 117/1000 | Loss: 0.00002181
Iteration 118/1000 | Loss: 0.00002181
Iteration 119/1000 | Loss: 0.00002181
Iteration 120/1000 | Loss: 0.00002181
Iteration 121/1000 | Loss: 0.00002180
Iteration 122/1000 | Loss: 0.00002180
Iteration 123/1000 | Loss: 0.00002180
Iteration 124/1000 | Loss: 0.00002180
Iteration 125/1000 | Loss: 0.00002180
Iteration 126/1000 | Loss: 0.00002180
Iteration 127/1000 | Loss: 0.00002180
Iteration 128/1000 | Loss: 0.00002180
Iteration 129/1000 | Loss: 0.00002180
Iteration 130/1000 | Loss: 0.00002179
Iteration 131/1000 | Loss: 0.00002179
Iteration 132/1000 | Loss: 0.00002179
Iteration 133/1000 | Loss: 0.00002179
Iteration 134/1000 | Loss: 0.00002179
Iteration 135/1000 | Loss: 0.00002178
Iteration 136/1000 | Loss: 0.00002178
Iteration 137/1000 | Loss: 0.00002178
Iteration 138/1000 | Loss: 0.00002178
Iteration 139/1000 | Loss: 0.00002178
Iteration 140/1000 | Loss: 0.00002178
Iteration 141/1000 | Loss: 0.00002178
Iteration 142/1000 | Loss: 0.00002178
Iteration 143/1000 | Loss: 0.00002177
Iteration 144/1000 | Loss: 0.00002177
Iteration 145/1000 | Loss: 0.00002177
Iteration 146/1000 | Loss: 0.00002177
Iteration 147/1000 | Loss: 0.00002177
Iteration 148/1000 | Loss: 0.00002177
Iteration 149/1000 | Loss: 0.00002177
Iteration 150/1000 | Loss: 0.00002177
Iteration 151/1000 | Loss: 0.00002177
Iteration 152/1000 | Loss: 0.00002177
Iteration 153/1000 | Loss: 0.00002177
Iteration 154/1000 | Loss: 0.00002177
Iteration 155/1000 | Loss: 0.00002177
Iteration 156/1000 | Loss: 0.00002177
Iteration 157/1000 | Loss: 0.00002177
Iteration 158/1000 | Loss: 0.00002177
Iteration 159/1000 | Loss: 0.00002177
Iteration 160/1000 | Loss: 0.00002177
Iteration 161/1000 | Loss: 0.00002177
Iteration 162/1000 | Loss: 0.00002177
Iteration 163/1000 | Loss: 0.00002177
Iteration 164/1000 | Loss: 0.00002177
Iteration 165/1000 | Loss: 0.00002177
Iteration 166/1000 | Loss: 0.00002177
Iteration 167/1000 | Loss: 0.00002177
Iteration 168/1000 | Loss: 0.00002177
Iteration 169/1000 | Loss: 0.00002177
Iteration 170/1000 | Loss: 0.00002177
Iteration 171/1000 | Loss: 0.00002177
Iteration 172/1000 | Loss: 0.00002177
Iteration 173/1000 | Loss: 0.00002177
Iteration 174/1000 | Loss: 0.00002177
Iteration 175/1000 | Loss: 0.00002177
Iteration 176/1000 | Loss: 0.00002177
Iteration 177/1000 | Loss: 0.00002177
Iteration 178/1000 | Loss: 0.00002177
Iteration 179/1000 | Loss: 0.00002177
Iteration 180/1000 | Loss: 0.00002177
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.1768786609754898e-05, 2.1768786609754898e-05, 2.1768786609754898e-05, 2.1768786609754898e-05, 2.1768786609754898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1768786609754898e-05

Optimization complete. Final v2v error: 3.3891072273254395 mm

Highest mean error: 21.481916427612305 mm for frame 167

Lowest mean error: 2.7677392959594727 mm for frame 110

Saving results

Total time: 59.98369765281677
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053330
Iteration 2/25 | Loss: 0.01053330
Iteration 3/25 | Loss: 0.00287776
Iteration 4/25 | Loss: 0.00231530
Iteration 5/25 | Loss: 0.00230893
Iteration 6/25 | Loss: 0.00183613
Iteration 7/25 | Loss: 0.00164655
Iteration 8/25 | Loss: 0.00158872
Iteration 9/25 | Loss: 0.00155503
Iteration 10/25 | Loss: 0.00151956
Iteration 11/25 | Loss: 0.00150907
Iteration 12/25 | Loss: 0.00147606
Iteration 13/25 | Loss: 0.00145951
Iteration 14/25 | Loss: 0.00145200
Iteration 15/25 | Loss: 0.00144352
Iteration 16/25 | Loss: 0.00144336
Iteration 17/25 | Loss: 0.00144168
Iteration 18/25 | Loss: 0.00143958
Iteration 19/25 | Loss: 0.00143837
Iteration 20/25 | Loss: 0.00143798
Iteration 21/25 | Loss: 0.00143691
Iteration 22/25 | Loss: 0.00143814
Iteration 23/25 | Loss: 0.00143783
Iteration 24/25 | Loss: 0.00143822
Iteration 25/25 | Loss: 0.00143808

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18991435
Iteration 2/25 | Loss: 0.00214686
Iteration 3/25 | Loss: 0.00214684
Iteration 4/25 | Loss: 0.00214684
Iteration 5/25 | Loss: 0.00214684
Iteration 6/25 | Loss: 0.00214684
Iteration 7/25 | Loss: 0.00214684
Iteration 8/25 | Loss: 0.00214684
Iteration 9/25 | Loss: 0.00214684
Iteration 10/25 | Loss: 0.00214684
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.002146839164197445, 0.002146839164197445, 0.002146839164197445, 0.002146839164197445, 0.002146839164197445]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002146839164197445

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00214684
Iteration 2/1000 | Loss: 0.00011781
Iteration 3/1000 | Loss: 0.00019492
Iteration 4/1000 | Loss: 0.00010051
Iteration 5/1000 | Loss: 0.00005700
Iteration 6/1000 | Loss: 0.00009684
Iteration 7/1000 | Loss: 0.00011570
Iteration 8/1000 | Loss: 0.00004344
Iteration 9/1000 | Loss: 0.00004700
Iteration 10/1000 | Loss: 0.00016231
Iteration 11/1000 | Loss: 0.00005493
Iteration 12/1000 | Loss: 0.00003081
Iteration 13/1000 | Loss: 0.00003039
Iteration 14/1000 | Loss: 0.00002475
Iteration 15/1000 | Loss: 0.00002993
Iteration 16/1000 | Loss: 0.00003033
Iteration 17/1000 | Loss: 0.00002342
Iteration 18/1000 | Loss: 0.00003453
Iteration 19/1000 | Loss: 0.00002516
Iteration 20/1000 | Loss: 0.00003033
Iteration 21/1000 | Loss: 0.00002101
Iteration 22/1000 | Loss: 0.00002502
Iteration 23/1000 | Loss: 0.00002440
Iteration 24/1000 | Loss: 0.00003205
Iteration 25/1000 | Loss: 0.00002975
Iteration 26/1000 | Loss: 0.00002767
Iteration 27/1000 | Loss: 0.00003046
Iteration 28/1000 | Loss: 0.00003402
Iteration 29/1000 | Loss: 0.00002964
Iteration 30/1000 | Loss: 0.00003048
Iteration 31/1000 | Loss: 0.00003160
Iteration 32/1000 | Loss: 0.00004270
Iteration 33/1000 | Loss: 0.00003409
Iteration 34/1000 | Loss: 0.00003556
Iteration 35/1000 | Loss: 0.00003988
Iteration 36/1000 | Loss: 0.00003072
Iteration 37/1000 | Loss: 0.00002513
Iteration 38/1000 | Loss: 0.00003070
Iteration 39/1000 | Loss: 0.00002919
Iteration 40/1000 | Loss: 0.00003109
Iteration 41/1000 | Loss: 0.00003547
Iteration 42/1000 | Loss: 0.00002985
Iteration 43/1000 | Loss: 0.00002929
Iteration 44/1000 | Loss: 0.00003118
Iteration 45/1000 | Loss: 0.00002811
Iteration 46/1000 | Loss: 0.00002829
Iteration 47/1000 | Loss: 0.00002591
Iteration 48/1000 | Loss: 0.00002781
Iteration 49/1000 | Loss: 0.00002936
Iteration 50/1000 | Loss: 0.00002432
Iteration 51/1000 | Loss: 0.00003158
Iteration 52/1000 | Loss: 0.00002809
Iteration 53/1000 | Loss: 0.00002677
Iteration 54/1000 | Loss: 0.00002947
Iteration 55/1000 | Loss: 0.00003084
Iteration 56/1000 | Loss: 0.00004751
Iteration 57/1000 | Loss: 0.00002902
Iteration 58/1000 | Loss: 0.00003042
Iteration 59/1000 | Loss: 0.00003732
Iteration 60/1000 | Loss: 0.00003086
Iteration 61/1000 | Loss: 0.00003687
Iteration 62/1000 | Loss: 0.00003070
Iteration 63/1000 | Loss: 0.00004871
Iteration 64/1000 | Loss: 0.00003456
Iteration 65/1000 | Loss: 0.00002012
Iteration 66/1000 | Loss: 0.00001908
Iteration 67/1000 | Loss: 0.00001829
Iteration 68/1000 | Loss: 0.00001798
Iteration 69/1000 | Loss: 0.00001791
Iteration 70/1000 | Loss: 0.00001787
Iteration 71/1000 | Loss: 0.00001785
Iteration 72/1000 | Loss: 0.00001785
Iteration 73/1000 | Loss: 0.00001784
Iteration 74/1000 | Loss: 0.00001783
Iteration 75/1000 | Loss: 0.00001783
Iteration 76/1000 | Loss: 0.00001783
Iteration 77/1000 | Loss: 0.00001783
Iteration 78/1000 | Loss: 0.00001782
Iteration 79/1000 | Loss: 0.00001782
Iteration 80/1000 | Loss: 0.00001782
Iteration 81/1000 | Loss: 0.00001782
Iteration 82/1000 | Loss: 0.00001782
Iteration 83/1000 | Loss: 0.00001782
Iteration 84/1000 | Loss: 0.00001782
Iteration 85/1000 | Loss: 0.00001782
Iteration 86/1000 | Loss: 0.00001782
Iteration 87/1000 | Loss: 0.00001782
Iteration 88/1000 | Loss: 0.00001782
Iteration 89/1000 | Loss: 0.00001782
Iteration 90/1000 | Loss: 0.00001781
Iteration 91/1000 | Loss: 0.00001781
Iteration 92/1000 | Loss: 0.00001781
Iteration 93/1000 | Loss: 0.00001781
Iteration 94/1000 | Loss: 0.00001778
Iteration 95/1000 | Loss: 0.00001778
Iteration 96/1000 | Loss: 0.00001777
Iteration 97/1000 | Loss: 0.00001776
Iteration 98/1000 | Loss: 0.00001776
Iteration 99/1000 | Loss: 0.00001776
Iteration 100/1000 | Loss: 0.00001775
Iteration 101/1000 | Loss: 0.00001775
Iteration 102/1000 | Loss: 0.00001774
Iteration 103/1000 | Loss: 0.00001774
Iteration 104/1000 | Loss: 0.00001773
Iteration 105/1000 | Loss: 0.00001773
Iteration 106/1000 | Loss: 0.00001772
Iteration 107/1000 | Loss: 0.00001772
Iteration 108/1000 | Loss: 0.00001771
Iteration 109/1000 | Loss: 0.00001769
Iteration 110/1000 | Loss: 0.00001768
Iteration 111/1000 | Loss: 0.00001768
Iteration 112/1000 | Loss: 0.00001768
Iteration 113/1000 | Loss: 0.00001768
Iteration 114/1000 | Loss: 0.00001768
Iteration 115/1000 | Loss: 0.00001768
Iteration 116/1000 | Loss: 0.00001768
Iteration 117/1000 | Loss: 0.00001768
Iteration 118/1000 | Loss: 0.00001768
Iteration 119/1000 | Loss: 0.00001768
Iteration 120/1000 | Loss: 0.00001768
Iteration 121/1000 | Loss: 0.00001768
Iteration 122/1000 | Loss: 0.00001768
Iteration 123/1000 | Loss: 0.00001768
Iteration 124/1000 | Loss: 0.00001768
Iteration 125/1000 | Loss: 0.00001768
Iteration 126/1000 | Loss: 0.00001768
Iteration 127/1000 | Loss: 0.00001768
Iteration 128/1000 | Loss: 0.00001768
Iteration 129/1000 | Loss: 0.00001768
Iteration 130/1000 | Loss: 0.00001768
Iteration 131/1000 | Loss: 0.00001768
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.767602407198865e-05, 1.767602407198865e-05, 1.767602407198865e-05, 1.767602407198865e-05, 1.767602407198865e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.767602407198865e-05

Optimization complete. Final v2v error: 3.383622407913208 mm

Highest mean error: 10.395572662353516 mm for frame 100

Lowest mean error: 2.8920536041259766 mm for frame 62

Saving results

Total time: 166.42491698265076
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00482187
Iteration 2/25 | Loss: 0.00153465
Iteration 3/25 | Loss: 0.00143687
Iteration 4/25 | Loss: 0.00141734
Iteration 5/25 | Loss: 0.00141052
Iteration 6/25 | Loss: 0.00140851
Iteration 7/25 | Loss: 0.00140851
Iteration 8/25 | Loss: 0.00140851
Iteration 9/25 | Loss: 0.00140851
Iteration 10/25 | Loss: 0.00140851
Iteration 11/25 | Loss: 0.00140851
Iteration 12/25 | Loss: 0.00140851
Iteration 13/25 | Loss: 0.00140851
Iteration 14/25 | Loss: 0.00140851
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001408505835570395, 0.001408505835570395, 0.001408505835570395, 0.001408505835570395, 0.001408505835570395]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001408505835570395

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24461925
Iteration 2/25 | Loss: 0.00210775
Iteration 3/25 | Loss: 0.00210775
Iteration 4/25 | Loss: 0.00210775
Iteration 5/25 | Loss: 0.00210775
Iteration 6/25 | Loss: 0.00210775
Iteration 7/25 | Loss: 0.00210775
Iteration 8/25 | Loss: 0.00210775
Iteration 9/25 | Loss: 0.00210775
Iteration 10/25 | Loss: 0.00210775
Iteration 11/25 | Loss: 0.00210775
Iteration 12/25 | Loss: 0.00210775
Iteration 13/25 | Loss: 0.00210775
Iteration 14/25 | Loss: 0.00210775
Iteration 15/25 | Loss: 0.00210775
Iteration 16/25 | Loss: 0.00210775
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0021077452693134546, 0.0021077452693134546, 0.0021077452693134546, 0.0021077452693134546, 0.0021077452693134546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021077452693134546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00210775
Iteration 2/1000 | Loss: 0.00003571
Iteration 3/1000 | Loss: 0.00002754
Iteration 4/1000 | Loss: 0.00002432
Iteration 5/1000 | Loss: 0.00002318
Iteration 6/1000 | Loss: 0.00002237
Iteration 7/1000 | Loss: 0.00002153
Iteration 8/1000 | Loss: 0.00002106
Iteration 9/1000 | Loss: 0.00002080
Iteration 10/1000 | Loss: 0.00002056
Iteration 11/1000 | Loss: 0.00002038
Iteration 12/1000 | Loss: 0.00002036
Iteration 13/1000 | Loss: 0.00002019
Iteration 14/1000 | Loss: 0.00002016
Iteration 15/1000 | Loss: 0.00002016
Iteration 16/1000 | Loss: 0.00002013
Iteration 17/1000 | Loss: 0.00002012
Iteration 18/1000 | Loss: 0.00002012
Iteration 19/1000 | Loss: 0.00002009
Iteration 20/1000 | Loss: 0.00002006
Iteration 21/1000 | Loss: 0.00002005
Iteration 22/1000 | Loss: 0.00002001
Iteration 23/1000 | Loss: 0.00002001
Iteration 24/1000 | Loss: 0.00002000
Iteration 25/1000 | Loss: 0.00002000
Iteration 26/1000 | Loss: 0.00002000
Iteration 27/1000 | Loss: 0.00002000
Iteration 28/1000 | Loss: 0.00001999
Iteration 29/1000 | Loss: 0.00001999
Iteration 30/1000 | Loss: 0.00001998
Iteration 31/1000 | Loss: 0.00001998
Iteration 32/1000 | Loss: 0.00001997
Iteration 33/1000 | Loss: 0.00001996
Iteration 34/1000 | Loss: 0.00001996
Iteration 35/1000 | Loss: 0.00001996
Iteration 36/1000 | Loss: 0.00001996
Iteration 37/1000 | Loss: 0.00001996
Iteration 38/1000 | Loss: 0.00001996
Iteration 39/1000 | Loss: 0.00001996
Iteration 40/1000 | Loss: 0.00001995
Iteration 41/1000 | Loss: 0.00001995
Iteration 42/1000 | Loss: 0.00001995
Iteration 43/1000 | Loss: 0.00001995
Iteration 44/1000 | Loss: 0.00001995
Iteration 45/1000 | Loss: 0.00001994
Iteration 46/1000 | Loss: 0.00001994
Iteration 47/1000 | Loss: 0.00001994
Iteration 48/1000 | Loss: 0.00001994
Iteration 49/1000 | Loss: 0.00001993
Iteration 50/1000 | Loss: 0.00001993
Iteration 51/1000 | Loss: 0.00001993
Iteration 52/1000 | Loss: 0.00001993
Iteration 53/1000 | Loss: 0.00001993
Iteration 54/1000 | Loss: 0.00001993
Iteration 55/1000 | Loss: 0.00001993
Iteration 56/1000 | Loss: 0.00001992
Iteration 57/1000 | Loss: 0.00001992
Iteration 58/1000 | Loss: 0.00001992
Iteration 59/1000 | Loss: 0.00001991
Iteration 60/1000 | Loss: 0.00001991
Iteration 61/1000 | Loss: 0.00001991
Iteration 62/1000 | Loss: 0.00001991
Iteration 63/1000 | Loss: 0.00001990
Iteration 64/1000 | Loss: 0.00001990
Iteration 65/1000 | Loss: 0.00001990
Iteration 66/1000 | Loss: 0.00001990
Iteration 67/1000 | Loss: 0.00001990
Iteration 68/1000 | Loss: 0.00001990
Iteration 69/1000 | Loss: 0.00001990
Iteration 70/1000 | Loss: 0.00001989
Iteration 71/1000 | Loss: 0.00001989
Iteration 72/1000 | Loss: 0.00001989
Iteration 73/1000 | Loss: 0.00001989
Iteration 74/1000 | Loss: 0.00001989
Iteration 75/1000 | Loss: 0.00001989
Iteration 76/1000 | Loss: 0.00001989
Iteration 77/1000 | Loss: 0.00001989
Iteration 78/1000 | Loss: 0.00001988
Iteration 79/1000 | Loss: 0.00001988
Iteration 80/1000 | Loss: 0.00001988
Iteration 81/1000 | Loss: 0.00001988
Iteration 82/1000 | Loss: 0.00001988
Iteration 83/1000 | Loss: 0.00001987
Iteration 84/1000 | Loss: 0.00001987
Iteration 85/1000 | Loss: 0.00001987
Iteration 86/1000 | Loss: 0.00001986
Iteration 87/1000 | Loss: 0.00001986
Iteration 88/1000 | Loss: 0.00001986
Iteration 89/1000 | Loss: 0.00001986
Iteration 90/1000 | Loss: 0.00001986
Iteration 91/1000 | Loss: 0.00001986
Iteration 92/1000 | Loss: 0.00001986
Iteration 93/1000 | Loss: 0.00001985
Iteration 94/1000 | Loss: 0.00001985
Iteration 95/1000 | Loss: 0.00001985
Iteration 96/1000 | Loss: 0.00001985
Iteration 97/1000 | Loss: 0.00001985
Iteration 98/1000 | Loss: 0.00001985
Iteration 99/1000 | Loss: 0.00001985
Iteration 100/1000 | Loss: 0.00001984
Iteration 101/1000 | Loss: 0.00001984
Iteration 102/1000 | Loss: 0.00001984
Iteration 103/1000 | Loss: 0.00001984
Iteration 104/1000 | Loss: 0.00001984
Iteration 105/1000 | Loss: 0.00001984
Iteration 106/1000 | Loss: 0.00001984
Iteration 107/1000 | Loss: 0.00001983
Iteration 108/1000 | Loss: 0.00001983
Iteration 109/1000 | Loss: 0.00001983
Iteration 110/1000 | Loss: 0.00001983
Iteration 111/1000 | Loss: 0.00001983
Iteration 112/1000 | Loss: 0.00001983
Iteration 113/1000 | Loss: 0.00001983
Iteration 114/1000 | Loss: 0.00001983
Iteration 115/1000 | Loss: 0.00001983
Iteration 116/1000 | Loss: 0.00001983
Iteration 117/1000 | Loss: 0.00001983
Iteration 118/1000 | Loss: 0.00001983
Iteration 119/1000 | Loss: 0.00001982
Iteration 120/1000 | Loss: 0.00001982
Iteration 121/1000 | Loss: 0.00001982
Iteration 122/1000 | Loss: 0.00001982
Iteration 123/1000 | Loss: 0.00001982
Iteration 124/1000 | Loss: 0.00001982
Iteration 125/1000 | Loss: 0.00001982
Iteration 126/1000 | Loss: 0.00001982
Iteration 127/1000 | Loss: 0.00001982
Iteration 128/1000 | Loss: 0.00001982
Iteration 129/1000 | Loss: 0.00001981
Iteration 130/1000 | Loss: 0.00001981
Iteration 131/1000 | Loss: 0.00001981
Iteration 132/1000 | Loss: 0.00001981
Iteration 133/1000 | Loss: 0.00001981
Iteration 134/1000 | Loss: 0.00001981
Iteration 135/1000 | Loss: 0.00001981
Iteration 136/1000 | Loss: 0.00001981
Iteration 137/1000 | Loss: 0.00001981
Iteration 138/1000 | Loss: 0.00001981
Iteration 139/1000 | Loss: 0.00001981
Iteration 140/1000 | Loss: 0.00001981
Iteration 141/1000 | Loss: 0.00001981
Iteration 142/1000 | Loss: 0.00001981
Iteration 143/1000 | Loss: 0.00001981
Iteration 144/1000 | Loss: 0.00001981
Iteration 145/1000 | Loss: 0.00001981
Iteration 146/1000 | Loss: 0.00001981
Iteration 147/1000 | Loss: 0.00001981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 147. Stopping optimization.
Last 5 losses: [1.981436253117863e-05, 1.981436253117863e-05, 1.981436253117863e-05, 1.981436253117863e-05, 1.981436253117863e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.981436253117863e-05

Optimization complete. Final v2v error: 3.8294858932495117 mm

Highest mean error: 4.308502674102783 mm for frame 194

Lowest mean error: 3.337998151779175 mm for frame 0

Saving results

Total time: 41.38249397277832
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511049
Iteration 2/25 | Loss: 0.00158920
Iteration 3/25 | Loss: 0.00143217
Iteration 4/25 | Loss: 0.00141931
Iteration 5/25 | Loss: 0.00141747
Iteration 6/25 | Loss: 0.00141747
Iteration 7/25 | Loss: 0.00141747
Iteration 8/25 | Loss: 0.00141747
Iteration 9/25 | Loss: 0.00141747
Iteration 10/25 | Loss: 0.00141747
Iteration 11/25 | Loss: 0.00141747
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001417473773472011, 0.001417473773472011, 0.001417473773472011, 0.001417473773472011, 0.001417473773472011]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001417473773472011

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.21169353
Iteration 2/25 | Loss: 0.00176485
Iteration 3/25 | Loss: 0.00176485
Iteration 4/25 | Loss: 0.00176485
Iteration 5/25 | Loss: 0.00176485
Iteration 6/25 | Loss: 0.00176485
Iteration 7/25 | Loss: 0.00176485
Iteration 8/25 | Loss: 0.00176485
Iteration 9/25 | Loss: 0.00176485
Iteration 10/25 | Loss: 0.00176485
Iteration 11/25 | Loss: 0.00176485
Iteration 12/25 | Loss: 0.00176485
Iteration 13/25 | Loss: 0.00176485
Iteration 14/25 | Loss: 0.00176485
Iteration 15/25 | Loss: 0.00176485
Iteration 16/25 | Loss: 0.00176485
Iteration 17/25 | Loss: 0.00176485
Iteration 18/25 | Loss: 0.00176485
Iteration 19/25 | Loss: 0.00176485
Iteration 20/25 | Loss: 0.00176485
Iteration 21/25 | Loss: 0.00176485
Iteration 22/25 | Loss: 0.00176485
Iteration 23/25 | Loss: 0.00176485
Iteration 24/25 | Loss: 0.00176485
Iteration 25/25 | Loss: 0.00176485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00176485
Iteration 2/1000 | Loss: 0.00006797
Iteration 3/1000 | Loss: 0.00004997
Iteration 4/1000 | Loss: 0.00004536
Iteration 5/1000 | Loss: 0.00004333
Iteration 6/1000 | Loss: 0.00004238
Iteration 7/1000 | Loss: 0.00004157
Iteration 8/1000 | Loss: 0.00004071
Iteration 9/1000 | Loss: 0.00004032
Iteration 10/1000 | Loss: 0.00003998
Iteration 11/1000 | Loss: 0.00003971
Iteration 12/1000 | Loss: 0.00003938
Iteration 13/1000 | Loss: 0.00003904
Iteration 14/1000 | Loss: 0.00003877
Iteration 15/1000 | Loss: 0.00003856
Iteration 16/1000 | Loss: 0.00003841
Iteration 17/1000 | Loss: 0.00003825
Iteration 18/1000 | Loss: 0.00003809
Iteration 19/1000 | Loss: 0.00003801
Iteration 20/1000 | Loss: 0.00003788
Iteration 21/1000 | Loss: 0.00003785
Iteration 22/1000 | Loss: 0.00003781
Iteration 23/1000 | Loss: 0.00003780
Iteration 24/1000 | Loss: 0.00003777
Iteration 25/1000 | Loss: 0.00003776
Iteration 26/1000 | Loss: 0.00003776
Iteration 27/1000 | Loss: 0.00003775
Iteration 28/1000 | Loss: 0.00003775
Iteration 29/1000 | Loss: 0.00003775
Iteration 30/1000 | Loss: 0.00003775
Iteration 31/1000 | Loss: 0.00003775
Iteration 32/1000 | Loss: 0.00003775
Iteration 33/1000 | Loss: 0.00003775
Iteration 34/1000 | Loss: 0.00003775
Iteration 35/1000 | Loss: 0.00003774
Iteration 36/1000 | Loss: 0.00003774
Iteration 37/1000 | Loss: 0.00003772
Iteration 38/1000 | Loss: 0.00003772
Iteration 39/1000 | Loss: 0.00003771
Iteration 40/1000 | Loss: 0.00003771
Iteration 41/1000 | Loss: 0.00003770
Iteration 42/1000 | Loss: 0.00003769
Iteration 43/1000 | Loss: 0.00003768
Iteration 44/1000 | Loss: 0.00003768
Iteration 45/1000 | Loss: 0.00003767
Iteration 46/1000 | Loss: 0.00003767
Iteration 47/1000 | Loss: 0.00003764
Iteration 48/1000 | Loss: 0.00003764
Iteration 49/1000 | Loss: 0.00003764
Iteration 50/1000 | Loss: 0.00003764
Iteration 51/1000 | Loss: 0.00003764
Iteration 52/1000 | Loss: 0.00003764
Iteration 53/1000 | Loss: 0.00003764
Iteration 54/1000 | Loss: 0.00003764
Iteration 55/1000 | Loss: 0.00003763
Iteration 56/1000 | Loss: 0.00003763
Iteration 57/1000 | Loss: 0.00003763
Iteration 58/1000 | Loss: 0.00003763
Iteration 59/1000 | Loss: 0.00003762
Iteration 60/1000 | Loss: 0.00003761
Iteration 61/1000 | Loss: 0.00003760
Iteration 62/1000 | Loss: 0.00003760
Iteration 63/1000 | Loss: 0.00003760
Iteration 64/1000 | Loss: 0.00003759
Iteration 65/1000 | Loss: 0.00003759
Iteration 66/1000 | Loss: 0.00003759
Iteration 67/1000 | Loss: 0.00003758
Iteration 68/1000 | Loss: 0.00003758
Iteration 69/1000 | Loss: 0.00003758
Iteration 70/1000 | Loss: 0.00003757
Iteration 71/1000 | Loss: 0.00003757
Iteration 72/1000 | Loss: 0.00003756
Iteration 73/1000 | Loss: 0.00003756
Iteration 74/1000 | Loss: 0.00003756
Iteration 75/1000 | Loss: 0.00003756
Iteration 76/1000 | Loss: 0.00003755
Iteration 77/1000 | Loss: 0.00003755
Iteration 78/1000 | Loss: 0.00003755
Iteration 79/1000 | Loss: 0.00003755
Iteration 80/1000 | Loss: 0.00003754
Iteration 81/1000 | Loss: 0.00003754
Iteration 82/1000 | Loss: 0.00003754
Iteration 83/1000 | Loss: 0.00003754
Iteration 84/1000 | Loss: 0.00003754
Iteration 85/1000 | Loss: 0.00003754
Iteration 86/1000 | Loss: 0.00003754
Iteration 87/1000 | Loss: 0.00003754
Iteration 88/1000 | Loss: 0.00003754
Iteration 89/1000 | Loss: 0.00003754
Iteration 90/1000 | Loss: 0.00003754
Iteration 91/1000 | Loss: 0.00003754
Iteration 92/1000 | Loss: 0.00003754
Iteration 93/1000 | Loss: 0.00003754
Iteration 94/1000 | Loss: 0.00003754
Iteration 95/1000 | Loss: 0.00003754
Iteration 96/1000 | Loss: 0.00003753
Iteration 97/1000 | Loss: 0.00003752
Iteration 98/1000 | Loss: 0.00003752
Iteration 99/1000 | Loss: 0.00003752
Iteration 100/1000 | Loss: 0.00003752
Iteration 101/1000 | Loss: 0.00003752
Iteration 102/1000 | Loss: 0.00003752
Iteration 103/1000 | Loss: 0.00003752
Iteration 104/1000 | Loss: 0.00003752
Iteration 105/1000 | Loss: 0.00003752
Iteration 106/1000 | Loss: 0.00003752
Iteration 107/1000 | Loss: 0.00003752
Iteration 108/1000 | Loss: 0.00003752
Iteration 109/1000 | Loss: 0.00003752
Iteration 110/1000 | Loss: 0.00003752
Iteration 111/1000 | Loss: 0.00003752
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 111. Stopping optimization.
Last 5 losses: [3.751921030925587e-05, 3.751921030925587e-05, 3.751921030925587e-05, 3.751921030925587e-05, 3.751921030925587e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.751921030925587e-05

Optimization complete. Final v2v error: 4.731503963470459 mm

Highest mean error: 5.291640281677246 mm for frame 235

Lowest mean error: 4.092870235443115 mm for frame 7

Saving results

Total time: 49.860241174697876
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436396
Iteration 2/25 | Loss: 0.00144167
Iteration 3/25 | Loss: 0.00138328
Iteration 4/25 | Loss: 0.00137668
Iteration 5/25 | Loss: 0.00137416
Iteration 6/25 | Loss: 0.00137347
Iteration 7/25 | Loss: 0.00137347
Iteration 8/25 | Loss: 0.00137347
Iteration 9/25 | Loss: 0.00137347
Iteration 10/25 | Loss: 0.00137347
Iteration 11/25 | Loss: 0.00137347
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0013734694803133607, 0.0013734694803133607, 0.0013734694803133607, 0.0013734694803133607, 0.0013734694803133607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013734694803133607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57475460
Iteration 2/25 | Loss: 0.00224417
Iteration 3/25 | Loss: 0.00224417
Iteration 4/25 | Loss: 0.00224417
Iteration 5/25 | Loss: 0.00224417
Iteration 6/25 | Loss: 0.00224417
Iteration 7/25 | Loss: 0.00224417
Iteration 8/25 | Loss: 0.00224417
Iteration 9/25 | Loss: 0.00224417
Iteration 10/25 | Loss: 0.00224417
Iteration 11/25 | Loss: 0.00224416
Iteration 12/25 | Loss: 0.00224416
Iteration 13/25 | Loss: 0.00224416
Iteration 14/25 | Loss: 0.00224416
Iteration 15/25 | Loss: 0.00224416
Iteration 16/25 | Loss: 0.00224416
Iteration 17/25 | Loss: 0.00224416
Iteration 18/25 | Loss: 0.00224416
Iteration 19/25 | Loss: 0.00224416
Iteration 20/25 | Loss: 0.00224416
Iteration 21/25 | Loss: 0.00224416
Iteration 22/25 | Loss: 0.00224416
Iteration 23/25 | Loss: 0.00224416
Iteration 24/25 | Loss: 0.00224416
Iteration 25/25 | Loss: 0.00224416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00224416
Iteration 2/1000 | Loss: 0.00005246
Iteration 3/1000 | Loss: 0.00002889
Iteration 4/1000 | Loss: 0.00002210
Iteration 5/1000 | Loss: 0.00002006
Iteration 6/1000 | Loss: 0.00001916
Iteration 7/1000 | Loss: 0.00001856
Iteration 8/1000 | Loss: 0.00001802
Iteration 9/1000 | Loss: 0.00001763
Iteration 10/1000 | Loss: 0.00001751
Iteration 11/1000 | Loss: 0.00001748
Iteration 12/1000 | Loss: 0.00001746
Iteration 13/1000 | Loss: 0.00001735
Iteration 14/1000 | Loss: 0.00001719
Iteration 15/1000 | Loss: 0.00001718
Iteration 16/1000 | Loss: 0.00001716
Iteration 17/1000 | Loss: 0.00001715
Iteration 18/1000 | Loss: 0.00001714
Iteration 19/1000 | Loss: 0.00001713
Iteration 20/1000 | Loss: 0.00001706
Iteration 21/1000 | Loss: 0.00001705
Iteration 22/1000 | Loss: 0.00001705
Iteration 23/1000 | Loss: 0.00001704
Iteration 24/1000 | Loss: 0.00001702
Iteration 25/1000 | Loss: 0.00001700
Iteration 26/1000 | Loss: 0.00001698
Iteration 27/1000 | Loss: 0.00001697
Iteration 28/1000 | Loss: 0.00001696
Iteration 29/1000 | Loss: 0.00001692
Iteration 30/1000 | Loss: 0.00001691
Iteration 31/1000 | Loss: 0.00001686
Iteration 32/1000 | Loss: 0.00001686
Iteration 33/1000 | Loss: 0.00001685
Iteration 34/1000 | Loss: 0.00001685
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001683
Iteration 37/1000 | Loss: 0.00001683
Iteration 38/1000 | Loss: 0.00001683
Iteration 39/1000 | Loss: 0.00001682
Iteration 40/1000 | Loss: 0.00001682
Iteration 41/1000 | Loss: 0.00001681
Iteration 42/1000 | Loss: 0.00001681
Iteration 43/1000 | Loss: 0.00001680
Iteration 44/1000 | Loss: 0.00001680
Iteration 45/1000 | Loss: 0.00001680
Iteration 46/1000 | Loss: 0.00001680
Iteration 47/1000 | Loss: 0.00001679
Iteration 48/1000 | Loss: 0.00001679
Iteration 49/1000 | Loss: 0.00001678
Iteration 50/1000 | Loss: 0.00001678
Iteration 51/1000 | Loss: 0.00001678
Iteration 52/1000 | Loss: 0.00001677
Iteration 53/1000 | Loss: 0.00001677
Iteration 54/1000 | Loss: 0.00001677
Iteration 55/1000 | Loss: 0.00001677
Iteration 56/1000 | Loss: 0.00001677
Iteration 57/1000 | Loss: 0.00001677
Iteration 58/1000 | Loss: 0.00001676
Iteration 59/1000 | Loss: 0.00001676
Iteration 60/1000 | Loss: 0.00001676
Iteration 61/1000 | Loss: 0.00001676
Iteration 62/1000 | Loss: 0.00001676
Iteration 63/1000 | Loss: 0.00001676
Iteration 64/1000 | Loss: 0.00001676
Iteration 65/1000 | Loss: 0.00001676
Iteration 66/1000 | Loss: 0.00001675
Iteration 67/1000 | Loss: 0.00001675
Iteration 68/1000 | Loss: 0.00001675
Iteration 69/1000 | Loss: 0.00001675
Iteration 70/1000 | Loss: 0.00001674
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00001673
Iteration 73/1000 | Loss: 0.00001673
Iteration 74/1000 | Loss: 0.00001673
Iteration 75/1000 | Loss: 0.00001672
Iteration 76/1000 | Loss: 0.00001672
Iteration 77/1000 | Loss: 0.00001672
Iteration 78/1000 | Loss: 0.00001672
Iteration 79/1000 | Loss: 0.00001671
Iteration 80/1000 | Loss: 0.00001671
Iteration 81/1000 | Loss: 0.00001671
Iteration 82/1000 | Loss: 0.00001671
Iteration 83/1000 | Loss: 0.00001670
Iteration 84/1000 | Loss: 0.00001670
Iteration 85/1000 | Loss: 0.00001670
Iteration 86/1000 | Loss: 0.00001670
Iteration 87/1000 | Loss: 0.00001670
Iteration 88/1000 | Loss: 0.00001670
Iteration 89/1000 | Loss: 0.00001670
Iteration 90/1000 | Loss: 0.00001670
Iteration 91/1000 | Loss: 0.00001670
Iteration 92/1000 | Loss: 0.00001670
Iteration 93/1000 | Loss: 0.00001670
Iteration 94/1000 | Loss: 0.00001669
Iteration 95/1000 | Loss: 0.00001669
Iteration 96/1000 | Loss: 0.00001669
Iteration 97/1000 | Loss: 0.00001668
Iteration 98/1000 | Loss: 0.00001668
Iteration 99/1000 | Loss: 0.00001668
Iteration 100/1000 | Loss: 0.00001667
Iteration 101/1000 | Loss: 0.00001667
Iteration 102/1000 | Loss: 0.00001667
Iteration 103/1000 | Loss: 0.00001667
Iteration 104/1000 | Loss: 0.00001667
Iteration 105/1000 | Loss: 0.00001667
Iteration 106/1000 | Loss: 0.00001667
Iteration 107/1000 | Loss: 0.00001667
Iteration 108/1000 | Loss: 0.00001666
Iteration 109/1000 | Loss: 0.00001666
Iteration 110/1000 | Loss: 0.00001666
Iteration 111/1000 | Loss: 0.00001666
Iteration 112/1000 | Loss: 0.00001666
Iteration 113/1000 | Loss: 0.00001666
Iteration 114/1000 | Loss: 0.00001665
Iteration 115/1000 | Loss: 0.00001665
Iteration 116/1000 | Loss: 0.00001665
Iteration 117/1000 | Loss: 0.00001665
Iteration 118/1000 | Loss: 0.00001665
Iteration 119/1000 | Loss: 0.00001665
Iteration 120/1000 | Loss: 0.00001664
Iteration 121/1000 | Loss: 0.00001664
Iteration 122/1000 | Loss: 0.00001664
Iteration 123/1000 | Loss: 0.00001664
Iteration 124/1000 | Loss: 0.00001664
Iteration 125/1000 | Loss: 0.00001663
Iteration 126/1000 | Loss: 0.00001663
Iteration 127/1000 | Loss: 0.00001663
Iteration 128/1000 | Loss: 0.00001663
Iteration 129/1000 | Loss: 0.00001663
Iteration 130/1000 | Loss: 0.00001663
Iteration 131/1000 | Loss: 0.00001663
Iteration 132/1000 | Loss: 0.00001663
Iteration 133/1000 | Loss: 0.00001663
Iteration 134/1000 | Loss: 0.00001663
Iteration 135/1000 | Loss: 0.00001663
Iteration 136/1000 | Loss: 0.00001663
Iteration 137/1000 | Loss: 0.00001662
Iteration 138/1000 | Loss: 0.00001662
Iteration 139/1000 | Loss: 0.00001662
Iteration 140/1000 | Loss: 0.00001662
Iteration 141/1000 | Loss: 0.00001662
Iteration 142/1000 | Loss: 0.00001662
Iteration 143/1000 | Loss: 0.00001662
Iteration 144/1000 | Loss: 0.00001662
Iteration 145/1000 | Loss: 0.00001662
Iteration 146/1000 | Loss: 0.00001662
Iteration 147/1000 | Loss: 0.00001662
Iteration 148/1000 | Loss: 0.00001662
Iteration 149/1000 | Loss: 0.00001662
Iteration 150/1000 | Loss: 0.00001661
Iteration 151/1000 | Loss: 0.00001661
Iteration 152/1000 | Loss: 0.00001661
Iteration 153/1000 | Loss: 0.00001661
Iteration 154/1000 | Loss: 0.00001661
Iteration 155/1000 | Loss: 0.00001661
Iteration 156/1000 | Loss: 0.00001661
Iteration 157/1000 | Loss: 0.00001661
Iteration 158/1000 | Loss: 0.00001661
Iteration 159/1000 | Loss: 0.00001661
Iteration 160/1000 | Loss: 0.00001661
Iteration 161/1000 | Loss: 0.00001661
Iteration 162/1000 | Loss: 0.00001661
Iteration 163/1000 | Loss: 0.00001661
Iteration 164/1000 | Loss: 0.00001661
Iteration 165/1000 | Loss: 0.00001661
Iteration 166/1000 | Loss: 0.00001661
Iteration 167/1000 | Loss: 0.00001661
Iteration 168/1000 | Loss: 0.00001661
Iteration 169/1000 | Loss: 0.00001661
Iteration 170/1000 | Loss: 0.00001661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 170. Stopping optimization.
Last 5 losses: [1.6608870282652788e-05, 1.6608870282652788e-05, 1.6608870282652788e-05, 1.6608870282652788e-05, 1.6608870282652788e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6608870282652788e-05

Optimization complete. Final v2v error: 3.429900884628296 mm

Highest mean error: 4.133549690246582 mm for frame 73

Lowest mean error: 2.974215269088745 mm for frame 131

Saving results

Total time: 40.14383316040039
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00759637
Iteration 2/25 | Loss: 0.00184672
Iteration 3/25 | Loss: 0.00153171
Iteration 4/25 | Loss: 0.00150011
Iteration 5/25 | Loss: 0.00149531
Iteration 6/25 | Loss: 0.00149478
Iteration 7/25 | Loss: 0.00149478
Iteration 8/25 | Loss: 0.00149478
Iteration 9/25 | Loss: 0.00149478
Iteration 10/25 | Loss: 0.00149478
Iteration 11/25 | Loss: 0.00149478
Iteration 12/25 | Loss: 0.00149478
Iteration 13/25 | Loss: 0.00149478
Iteration 14/25 | Loss: 0.00149478
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0014947769232094288, 0.0014947769232094288, 0.0014947769232094288, 0.0014947769232094288, 0.0014947769232094288]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014947769232094288

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31582332
Iteration 2/25 | Loss: 0.00158582
Iteration 3/25 | Loss: 0.00158582
Iteration 4/25 | Loss: 0.00158582
Iteration 5/25 | Loss: 0.00158582
Iteration 6/25 | Loss: 0.00158582
Iteration 7/25 | Loss: 0.00158582
Iteration 8/25 | Loss: 0.00158582
Iteration 9/25 | Loss: 0.00158582
Iteration 10/25 | Loss: 0.00158582
Iteration 11/25 | Loss: 0.00158582
Iteration 12/25 | Loss: 0.00158582
Iteration 13/25 | Loss: 0.00158582
Iteration 14/25 | Loss: 0.00158582
Iteration 15/25 | Loss: 0.00158582
Iteration 16/25 | Loss: 0.00158582
Iteration 17/25 | Loss: 0.00158582
Iteration 18/25 | Loss: 0.00158582
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015858153346925974, 0.0015858153346925974, 0.0015858153346925974, 0.0015858153346925974, 0.0015858153346925974]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015858153346925974

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158582
Iteration 2/1000 | Loss: 0.00006084
Iteration 3/1000 | Loss: 0.00003934
Iteration 4/1000 | Loss: 0.00003253
Iteration 5/1000 | Loss: 0.00003070
Iteration 6/1000 | Loss: 0.00002981
Iteration 7/1000 | Loss: 0.00002924
Iteration 8/1000 | Loss: 0.00002877
Iteration 9/1000 | Loss: 0.00002849
Iteration 10/1000 | Loss: 0.00002823
Iteration 11/1000 | Loss: 0.00002808
Iteration 12/1000 | Loss: 0.00002793
Iteration 13/1000 | Loss: 0.00002779
Iteration 14/1000 | Loss: 0.00002777
Iteration 15/1000 | Loss: 0.00002775
Iteration 16/1000 | Loss: 0.00002775
Iteration 17/1000 | Loss: 0.00002766
Iteration 18/1000 | Loss: 0.00002762
Iteration 19/1000 | Loss: 0.00002759
Iteration 20/1000 | Loss: 0.00002758
Iteration 21/1000 | Loss: 0.00002758
Iteration 22/1000 | Loss: 0.00002755
Iteration 23/1000 | Loss: 0.00002755
Iteration 24/1000 | Loss: 0.00002749
Iteration 25/1000 | Loss: 0.00002748
Iteration 26/1000 | Loss: 0.00002747
Iteration 27/1000 | Loss: 0.00002747
Iteration 28/1000 | Loss: 0.00002746
Iteration 29/1000 | Loss: 0.00002745
Iteration 30/1000 | Loss: 0.00002745
Iteration 31/1000 | Loss: 0.00002745
Iteration 32/1000 | Loss: 0.00002745
Iteration 33/1000 | Loss: 0.00002745
Iteration 34/1000 | Loss: 0.00002745
Iteration 35/1000 | Loss: 0.00002745
Iteration 36/1000 | Loss: 0.00002745
Iteration 37/1000 | Loss: 0.00002745
Iteration 38/1000 | Loss: 0.00002745
Iteration 39/1000 | Loss: 0.00002745
Iteration 40/1000 | Loss: 0.00002745
Iteration 41/1000 | Loss: 0.00002744
Iteration 42/1000 | Loss: 0.00002744
Iteration 43/1000 | Loss: 0.00002742
Iteration 44/1000 | Loss: 0.00002741
Iteration 45/1000 | Loss: 0.00002740
Iteration 46/1000 | Loss: 0.00002739
Iteration 47/1000 | Loss: 0.00002739
Iteration 48/1000 | Loss: 0.00002738
Iteration 49/1000 | Loss: 0.00002737
Iteration 50/1000 | Loss: 0.00002737
Iteration 51/1000 | Loss: 0.00002736
Iteration 52/1000 | Loss: 0.00002735
Iteration 53/1000 | Loss: 0.00002734
Iteration 54/1000 | Loss: 0.00002734
Iteration 55/1000 | Loss: 0.00002733
Iteration 56/1000 | Loss: 0.00002733
Iteration 57/1000 | Loss: 0.00002733
Iteration 58/1000 | Loss: 0.00002731
Iteration 59/1000 | Loss: 0.00002731
Iteration 60/1000 | Loss: 0.00002730
Iteration 61/1000 | Loss: 0.00002730
Iteration 62/1000 | Loss: 0.00002729
Iteration 63/1000 | Loss: 0.00002729
Iteration 64/1000 | Loss: 0.00002728
Iteration 65/1000 | Loss: 0.00002727
Iteration 66/1000 | Loss: 0.00002727
Iteration 67/1000 | Loss: 0.00002726
Iteration 68/1000 | Loss: 0.00002725
Iteration 69/1000 | Loss: 0.00002725
Iteration 70/1000 | Loss: 0.00002725
Iteration 71/1000 | Loss: 0.00002722
Iteration 72/1000 | Loss: 0.00002722
Iteration 73/1000 | Loss: 0.00002721
Iteration 74/1000 | Loss: 0.00002720
Iteration 75/1000 | Loss: 0.00002720
Iteration 76/1000 | Loss: 0.00002720
Iteration 77/1000 | Loss: 0.00002720
Iteration 78/1000 | Loss: 0.00002720
Iteration 79/1000 | Loss: 0.00002720
Iteration 80/1000 | Loss: 0.00002720
Iteration 81/1000 | Loss: 0.00002719
Iteration 82/1000 | Loss: 0.00002719
Iteration 83/1000 | Loss: 0.00002718
Iteration 84/1000 | Loss: 0.00002718
Iteration 85/1000 | Loss: 0.00002718
Iteration 86/1000 | Loss: 0.00002718
Iteration 87/1000 | Loss: 0.00002718
Iteration 88/1000 | Loss: 0.00002718
Iteration 89/1000 | Loss: 0.00002718
Iteration 90/1000 | Loss: 0.00002717
Iteration 91/1000 | Loss: 0.00002717
Iteration 92/1000 | Loss: 0.00002716
Iteration 93/1000 | Loss: 0.00002716
Iteration 94/1000 | Loss: 0.00002715
Iteration 95/1000 | Loss: 0.00002715
Iteration 96/1000 | Loss: 0.00002715
Iteration 97/1000 | Loss: 0.00002715
Iteration 98/1000 | Loss: 0.00002714
Iteration 99/1000 | Loss: 0.00002714
Iteration 100/1000 | Loss: 0.00002714
Iteration 101/1000 | Loss: 0.00002714
Iteration 102/1000 | Loss: 0.00002714
Iteration 103/1000 | Loss: 0.00002714
Iteration 104/1000 | Loss: 0.00002714
Iteration 105/1000 | Loss: 0.00002713
Iteration 106/1000 | Loss: 0.00002713
Iteration 107/1000 | Loss: 0.00002713
Iteration 108/1000 | Loss: 0.00002713
Iteration 109/1000 | Loss: 0.00002713
Iteration 110/1000 | Loss: 0.00002712
Iteration 111/1000 | Loss: 0.00002712
Iteration 112/1000 | Loss: 0.00002712
Iteration 113/1000 | Loss: 0.00002712
Iteration 114/1000 | Loss: 0.00002711
Iteration 115/1000 | Loss: 0.00002711
Iteration 116/1000 | Loss: 0.00002711
Iteration 117/1000 | Loss: 0.00002711
Iteration 118/1000 | Loss: 0.00002711
Iteration 119/1000 | Loss: 0.00002710
Iteration 120/1000 | Loss: 0.00002710
Iteration 121/1000 | Loss: 0.00002710
Iteration 122/1000 | Loss: 0.00002710
Iteration 123/1000 | Loss: 0.00002710
Iteration 124/1000 | Loss: 0.00002710
Iteration 125/1000 | Loss: 0.00002710
Iteration 126/1000 | Loss: 0.00002709
Iteration 127/1000 | Loss: 0.00002709
Iteration 128/1000 | Loss: 0.00002709
Iteration 129/1000 | Loss: 0.00002709
Iteration 130/1000 | Loss: 0.00002708
Iteration 131/1000 | Loss: 0.00002708
Iteration 132/1000 | Loss: 0.00002708
Iteration 133/1000 | Loss: 0.00002707
Iteration 134/1000 | Loss: 0.00002707
Iteration 135/1000 | Loss: 0.00002707
Iteration 136/1000 | Loss: 0.00002707
Iteration 137/1000 | Loss: 0.00002707
Iteration 138/1000 | Loss: 0.00002707
Iteration 139/1000 | Loss: 0.00002707
Iteration 140/1000 | Loss: 0.00002706
Iteration 141/1000 | Loss: 0.00002706
Iteration 142/1000 | Loss: 0.00002706
Iteration 143/1000 | Loss: 0.00002706
Iteration 144/1000 | Loss: 0.00002706
Iteration 145/1000 | Loss: 0.00002706
Iteration 146/1000 | Loss: 0.00002705
Iteration 147/1000 | Loss: 0.00002705
Iteration 148/1000 | Loss: 0.00002705
Iteration 149/1000 | Loss: 0.00002705
Iteration 150/1000 | Loss: 0.00002705
Iteration 151/1000 | Loss: 0.00002705
Iteration 152/1000 | Loss: 0.00002705
Iteration 153/1000 | Loss: 0.00002705
Iteration 154/1000 | Loss: 0.00002705
Iteration 155/1000 | Loss: 0.00002705
Iteration 156/1000 | Loss: 0.00002705
Iteration 157/1000 | Loss: 0.00002705
Iteration 158/1000 | Loss: 0.00002705
Iteration 159/1000 | Loss: 0.00002705
Iteration 160/1000 | Loss: 0.00002705
Iteration 161/1000 | Loss: 0.00002705
Iteration 162/1000 | Loss: 0.00002705
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [2.7050153221352957e-05, 2.7050153221352957e-05, 2.7050153221352957e-05, 2.7050153221352957e-05, 2.7050153221352957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7050153221352957e-05

Optimization complete. Final v2v error: 4.287161350250244 mm

Highest mean error: 4.806504249572754 mm for frame 16

Lowest mean error: 3.707103967666626 mm for frame 211

Saving results

Total time: 49.0149130821228
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01077408
Iteration 2/25 | Loss: 0.00216251
Iteration 3/25 | Loss: 0.00168842
Iteration 4/25 | Loss: 0.00161640
Iteration 5/25 | Loss: 0.00162490
Iteration 6/25 | Loss: 0.00161869
Iteration 7/25 | Loss: 0.00160667
Iteration 8/25 | Loss: 0.00160644
Iteration 9/25 | Loss: 0.00159834
Iteration 10/25 | Loss: 0.00159350
Iteration 11/25 | Loss: 0.00161084
Iteration 12/25 | Loss: 0.00160805
Iteration 13/25 | Loss: 0.00159928
Iteration 14/25 | Loss: 0.00158734
Iteration 15/25 | Loss: 0.00158766
Iteration 16/25 | Loss: 0.00159024
Iteration 17/25 | Loss: 0.00158088
Iteration 18/25 | Loss: 0.00157089
Iteration 19/25 | Loss: 0.00156602
Iteration 20/25 | Loss: 0.00156422
Iteration 21/25 | Loss: 0.00156295
Iteration 22/25 | Loss: 0.00156186
Iteration 23/25 | Loss: 0.00155060
Iteration 24/25 | Loss: 0.00154568
Iteration 25/25 | Loss: 0.00154033

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81705701
Iteration 2/25 | Loss: 0.00281041
Iteration 3/25 | Loss: 0.00281040
Iteration 4/25 | Loss: 0.00281040
Iteration 5/25 | Loss: 0.00281039
Iteration 6/25 | Loss: 0.00281039
Iteration 7/25 | Loss: 0.00281039
Iteration 8/25 | Loss: 0.00281039
Iteration 9/25 | Loss: 0.00281039
Iteration 10/25 | Loss: 0.00281039
Iteration 11/25 | Loss: 0.00281039
Iteration 12/25 | Loss: 0.00281039
Iteration 13/25 | Loss: 0.00281039
Iteration 14/25 | Loss: 0.00281039
Iteration 15/25 | Loss: 0.00281039
Iteration 16/25 | Loss: 0.00281039
Iteration 17/25 | Loss: 0.00281039
Iteration 18/25 | Loss: 0.00281039
Iteration 19/25 | Loss: 0.00281039
Iteration 20/25 | Loss: 0.00281039
Iteration 21/25 | Loss: 0.00281039
Iteration 22/25 | Loss: 0.00281039
Iteration 23/25 | Loss: 0.00281039
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.002810391364619136, 0.002810391364619136, 0.002810391364619136, 0.002810391364619136, 0.002810391364619136]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002810391364619136

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00281039
Iteration 2/1000 | Loss: 0.00041875
Iteration 3/1000 | Loss: 0.00064624
Iteration 4/1000 | Loss: 0.00061859
Iteration 5/1000 | Loss: 0.00031265
Iteration 6/1000 | Loss: 0.00051144
Iteration 7/1000 | Loss: 0.00041348
Iteration 8/1000 | Loss: 0.00046325
Iteration 9/1000 | Loss: 0.00063339
Iteration 10/1000 | Loss: 0.00079085
Iteration 11/1000 | Loss: 0.00061663
Iteration 12/1000 | Loss: 0.00046657
Iteration 13/1000 | Loss: 0.00043543
Iteration 14/1000 | Loss: 0.00028375
Iteration 15/1000 | Loss: 0.00038860
Iteration 16/1000 | Loss: 0.00030393
Iteration 17/1000 | Loss: 0.00027067
Iteration 18/1000 | Loss: 0.00032111
Iteration 19/1000 | Loss: 0.00036693
Iteration 20/1000 | Loss: 0.00036286
Iteration 21/1000 | Loss: 0.00044456
Iteration 22/1000 | Loss: 0.00033923
Iteration 23/1000 | Loss: 0.00042193
Iteration 24/1000 | Loss: 0.00035685
Iteration 25/1000 | Loss: 0.00028296
Iteration 26/1000 | Loss: 0.00013682
Iteration 27/1000 | Loss: 0.00036933
Iteration 28/1000 | Loss: 0.00034702
Iteration 29/1000 | Loss: 0.00034493
Iteration 30/1000 | Loss: 0.00041902
Iteration 31/1000 | Loss: 0.00038202
Iteration 32/1000 | Loss: 0.00024913
Iteration 33/1000 | Loss: 0.00025709
Iteration 34/1000 | Loss: 0.00035413
Iteration 35/1000 | Loss: 0.00038723
Iteration 36/1000 | Loss: 0.00037158
Iteration 37/1000 | Loss: 0.00048125
Iteration 38/1000 | Loss: 0.00047427
Iteration 39/1000 | Loss: 0.00047483
Iteration 40/1000 | Loss: 0.00039365
Iteration 41/1000 | Loss: 0.00042492
Iteration 42/1000 | Loss: 0.00067736
Iteration 43/1000 | Loss: 0.00048376
Iteration 44/1000 | Loss: 0.00059118
Iteration 45/1000 | Loss: 0.00065594
Iteration 46/1000 | Loss: 0.00032326
Iteration 47/1000 | Loss: 0.00025886
Iteration 48/1000 | Loss: 0.00040729
Iteration 49/1000 | Loss: 0.00038682
Iteration 50/1000 | Loss: 0.00045681
Iteration 51/1000 | Loss: 0.00023992
Iteration 52/1000 | Loss: 0.00020626
Iteration 53/1000 | Loss: 0.00023316
Iteration 54/1000 | Loss: 0.00020351
Iteration 55/1000 | Loss: 0.00034161
Iteration 56/1000 | Loss: 0.00030665
Iteration 57/1000 | Loss: 0.00033627
Iteration 58/1000 | Loss: 0.00030788
Iteration 59/1000 | Loss: 0.00027034
Iteration 60/1000 | Loss: 0.00028052
Iteration 61/1000 | Loss: 0.00027105
Iteration 62/1000 | Loss: 0.00053986
Iteration 63/1000 | Loss: 0.00033425
Iteration 64/1000 | Loss: 0.00027600
Iteration 65/1000 | Loss: 0.00026262
Iteration 66/1000 | Loss: 0.00035326
Iteration 67/1000 | Loss: 0.00038237
Iteration 68/1000 | Loss: 0.00026616
Iteration 69/1000 | Loss: 0.00028068
Iteration 70/1000 | Loss: 0.00026936
Iteration 71/1000 | Loss: 0.00029321
Iteration 72/1000 | Loss: 0.00016673
Iteration 73/1000 | Loss: 0.00009125
Iteration 74/1000 | Loss: 0.00028627
Iteration 75/1000 | Loss: 0.00022151
Iteration 76/1000 | Loss: 0.00024295
Iteration 77/1000 | Loss: 0.00027184
Iteration 78/1000 | Loss: 0.00027548
Iteration 79/1000 | Loss: 0.00110813
Iteration 80/1000 | Loss: 0.00032461
Iteration 81/1000 | Loss: 0.00022866
Iteration 82/1000 | Loss: 0.00059692
Iteration 83/1000 | Loss: 0.00091497
Iteration 84/1000 | Loss: 0.00047025
Iteration 85/1000 | Loss: 0.00025665
Iteration 86/1000 | Loss: 0.00033831
Iteration 87/1000 | Loss: 0.00033766
Iteration 88/1000 | Loss: 0.00027777
Iteration 89/1000 | Loss: 0.00028830
Iteration 90/1000 | Loss: 0.00023140
Iteration 91/1000 | Loss: 0.00042318
Iteration 92/1000 | Loss: 0.00137241
Iteration 93/1000 | Loss: 0.00031452
Iteration 94/1000 | Loss: 0.00052344
Iteration 95/1000 | Loss: 0.00060535
Iteration 96/1000 | Loss: 0.00029248
Iteration 97/1000 | Loss: 0.00031715
Iteration 98/1000 | Loss: 0.00024459
Iteration 99/1000 | Loss: 0.00020951
Iteration 100/1000 | Loss: 0.00027610
Iteration 101/1000 | Loss: 0.00030368
Iteration 102/1000 | Loss: 0.00031843
Iteration 103/1000 | Loss: 0.00034482
Iteration 104/1000 | Loss: 0.00034825
Iteration 105/1000 | Loss: 0.00033785
Iteration 106/1000 | Loss: 0.00124130
Iteration 107/1000 | Loss: 0.00030999
Iteration 108/1000 | Loss: 0.00056045
Iteration 109/1000 | Loss: 0.00017083
Iteration 110/1000 | Loss: 0.00020172
Iteration 111/1000 | Loss: 0.00025872
Iteration 112/1000 | Loss: 0.00053512
Iteration 113/1000 | Loss: 0.00020977
Iteration 114/1000 | Loss: 0.00013821
Iteration 115/1000 | Loss: 0.00009727
Iteration 116/1000 | Loss: 0.00044778
Iteration 117/1000 | Loss: 0.00028307
Iteration 118/1000 | Loss: 0.00014500
Iteration 119/1000 | Loss: 0.00010341
Iteration 120/1000 | Loss: 0.00024129
Iteration 121/1000 | Loss: 0.00022280
Iteration 122/1000 | Loss: 0.00009052
Iteration 123/1000 | Loss: 0.00014308
Iteration 124/1000 | Loss: 0.00079542
Iteration 125/1000 | Loss: 0.00020899
Iteration 126/1000 | Loss: 0.00014909
Iteration 127/1000 | Loss: 0.00011161
Iteration 128/1000 | Loss: 0.00066764
Iteration 129/1000 | Loss: 0.00013487
Iteration 130/1000 | Loss: 0.00016174
Iteration 131/1000 | Loss: 0.00134240
Iteration 132/1000 | Loss: 0.00125336
Iteration 133/1000 | Loss: 0.00016601
Iteration 134/1000 | Loss: 0.00017909
Iteration 135/1000 | Loss: 0.00023343
Iteration 136/1000 | Loss: 0.00015716
Iteration 137/1000 | Loss: 0.00021629
Iteration 138/1000 | Loss: 0.00026940
Iteration 139/1000 | Loss: 0.00029971
Iteration 140/1000 | Loss: 0.00029191
Iteration 141/1000 | Loss: 0.00016099
Iteration 142/1000 | Loss: 0.00023105
Iteration 143/1000 | Loss: 0.00059056
Iteration 144/1000 | Loss: 0.00034890
Iteration 145/1000 | Loss: 0.00021357
Iteration 146/1000 | Loss: 0.00016603
Iteration 147/1000 | Loss: 0.00015066
Iteration 148/1000 | Loss: 0.00022277
Iteration 149/1000 | Loss: 0.00058945
Iteration 150/1000 | Loss: 0.00067929
Iteration 151/1000 | Loss: 0.00027006
Iteration 152/1000 | Loss: 0.00025178
Iteration 153/1000 | Loss: 0.00022960
Iteration 154/1000 | Loss: 0.00025393
Iteration 155/1000 | Loss: 0.00026793
Iteration 156/1000 | Loss: 0.00026047
Iteration 157/1000 | Loss: 0.00024234
Iteration 158/1000 | Loss: 0.00022777
Iteration 159/1000 | Loss: 0.00020696
Iteration 160/1000 | Loss: 0.00020706
Iteration 161/1000 | Loss: 0.00051819
Iteration 162/1000 | Loss: 0.00029599
Iteration 163/1000 | Loss: 0.00031723
Iteration 164/1000 | Loss: 0.00034456
Iteration 165/1000 | Loss: 0.00019427
Iteration 166/1000 | Loss: 0.00015595
Iteration 167/1000 | Loss: 0.00022609
Iteration 168/1000 | Loss: 0.00020974
Iteration 169/1000 | Loss: 0.00015586
Iteration 170/1000 | Loss: 0.00014120
Iteration 171/1000 | Loss: 0.00022632
Iteration 172/1000 | Loss: 0.00022064
Iteration 173/1000 | Loss: 0.00024774
Iteration 174/1000 | Loss: 0.00024046
Iteration 175/1000 | Loss: 0.00033937
Iteration 176/1000 | Loss: 0.00025264
Iteration 177/1000 | Loss: 0.00039581
Iteration 178/1000 | Loss: 0.00021302
Iteration 179/1000 | Loss: 0.00017086
Iteration 180/1000 | Loss: 0.00019615
Iteration 181/1000 | Loss: 0.00027607
Iteration 182/1000 | Loss: 0.00026780
Iteration 183/1000 | Loss: 0.00030656
Iteration 184/1000 | Loss: 0.00031715
Iteration 185/1000 | Loss: 0.00031558
Iteration 186/1000 | Loss: 0.00029174
Iteration 187/1000 | Loss: 0.00055055
Iteration 188/1000 | Loss: 0.00218751
Iteration 189/1000 | Loss: 0.00028502
Iteration 190/1000 | Loss: 0.00022272
Iteration 191/1000 | Loss: 0.00014482
Iteration 192/1000 | Loss: 0.00022013
Iteration 193/1000 | Loss: 0.00013022
Iteration 194/1000 | Loss: 0.00015418
Iteration 195/1000 | Loss: 0.00037047
Iteration 196/1000 | Loss: 0.00038884
Iteration 197/1000 | Loss: 0.00066963
Iteration 198/1000 | Loss: 0.00145660
Iteration 199/1000 | Loss: 0.00015452
Iteration 200/1000 | Loss: 0.00074563
Iteration 201/1000 | Loss: 0.00013800
Iteration 202/1000 | Loss: 0.00012752
Iteration 203/1000 | Loss: 0.00014839
Iteration 204/1000 | Loss: 0.00046122
Iteration 205/1000 | Loss: 0.00016048
Iteration 206/1000 | Loss: 0.00016890
Iteration 207/1000 | Loss: 0.00031885
Iteration 208/1000 | Loss: 0.00030334
Iteration 209/1000 | Loss: 0.00025688
Iteration 210/1000 | Loss: 0.00026424
Iteration 211/1000 | Loss: 0.00024759
Iteration 212/1000 | Loss: 0.00026659
Iteration 213/1000 | Loss: 0.00013656
Iteration 214/1000 | Loss: 0.00036063
Iteration 215/1000 | Loss: 0.00025891
Iteration 216/1000 | Loss: 0.00038129
Iteration 217/1000 | Loss: 0.00019523
Iteration 218/1000 | Loss: 0.00019345
Iteration 219/1000 | Loss: 0.00021511
Iteration 220/1000 | Loss: 0.00014615
Iteration 221/1000 | Loss: 0.00027939
Iteration 222/1000 | Loss: 0.00011653
Iteration 223/1000 | Loss: 0.00038631
Iteration 224/1000 | Loss: 0.00029293
Iteration 225/1000 | Loss: 0.00025865
Iteration 226/1000 | Loss: 0.00030127
Iteration 227/1000 | Loss: 0.00033370
Iteration 228/1000 | Loss: 0.00023713
Iteration 229/1000 | Loss: 0.00026294
Iteration 230/1000 | Loss: 0.00011404
Iteration 231/1000 | Loss: 0.00006898
Iteration 232/1000 | Loss: 0.00005411
Iteration 233/1000 | Loss: 0.00005226
Iteration 234/1000 | Loss: 0.00011169
Iteration 235/1000 | Loss: 0.00010388
Iteration 236/1000 | Loss: 0.00009719
Iteration 237/1000 | Loss: 0.00011674
Iteration 238/1000 | Loss: 0.00006286
Iteration 239/1000 | Loss: 0.00010918
Iteration 240/1000 | Loss: 0.00009457
Iteration 241/1000 | Loss: 0.00010263
Iteration 242/1000 | Loss: 0.00051379
Iteration 243/1000 | Loss: 0.00016047
Iteration 244/1000 | Loss: 0.00009416
Iteration 245/1000 | Loss: 0.00006867
Iteration 246/1000 | Loss: 0.00011080
Iteration 247/1000 | Loss: 0.00009504
Iteration 248/1000 | Loss: 0.00013751
Iteration 249/1000 | Loss: 0.00013800
Iteration 250/1000 | Loss: 0.00008179
Iteration 251/1000 | Loss: 0.00010873
Iteration 252/1000 | Loss: 0.00010018
Iteration 253/1000 | Loss: 0.00016393
Iteration 254/1000 | Loss: 0.00010928
Iteration 255/1000 | Loss: 0.00013002
Iteration 256/1000 | Loss: 0.00022310
Iteration 257/1000 | Loss: 0.00020031
Iteration 258/1000 | Loss: 0.00014434
Iteration 259/1000 | Loss: 0.00011124
Iteration 260/1000 | Loss: 0.00016426
Iteration 261/1000 | Loss: 0.00023160
Iteration 262/1000 | Loss: 0.00009628
Iteration 263/1000 | Loss: 0.00009900
Iteration 264/1000 | Loss: 0.00017513
Iteration 265/1000 | Loss: 0.00023177
Iteration 266/1000 | Loss: 0.00026471
Iteration 267/1000 | Loss: 0.00022894
Iteration 268/1000 | Loss: 0.00006425
Iteration 269/1000 | Loss: 0.00016433
Iteration 270/1000 | Loss: 0.00012859
Iteration 271/1000 | Loss: 0.00024657
Iteration 272/1000 | Loss: 0.00014814
Iteration 273/1000 | Loss: 0.00073566
Iteration 274/1000 | Loss: 0.00011163
Iteration 275/1000 | Loss: 0.00005044
Iteration 276/1000 | Loss: 0.00004689
Iteration 277/1000 | Loss: 0.00067881
Iteration 278/1000 | Loss: 0.00020007
Iteration 279/1000 | Loss: 0.00031247
Iteration 280/1000 | Loss: 0.00005321
Iteration 281/1000 | Loss: 0.00004721
Iteration 282/1000 | Loss: 0.00004575
Iteration 283/1000 | Loss: 0.00004299
Iteration 284/1000 | Loss: 0.00004186
Iteration 285/1000 | Loss: 0.00004095
Iteration 286/1000 | Loss: 0.00049841
Iteration 287/1000 | Loss: 0.00029474
Iteration 288/1000 | Loss: 0.00038902
Iteration 289/1000 | Loss: 0.00043817
Iteration 290/1000 | Loss: 0.00037283
Iteration 291/1000 | Loss: 0.00040064
Iteration 292/1000 | Loss: 0.00031643
Iteration 293/1000 | Loss: 0.00034143
Iteration 294/1000 | Loss: 0.00028866
Iteration 295/1000 | Loss: 0.00005281
Iteration 296/1000 | Loss: 0.00045907
Iteration 297/1000 | Loss: 0.00036508
Iteration 298/1000 | Loss: 0.00004622
Iteration 299/1000 | Loss: 0.00050562
Iteration 300/1000 | Loss: 0.00028859
Iteration 301/1000 | Loss: 0.00047050
Iteration 302/1000 | Loss: 0.00007030
Iteration 303/1000 | Loss: 0.00004719
Iteration 304/1000 | Loss: 0.00004042
Iteration 305/1000 | Loss: 0.00003895
Iteration 306/1000 | Loss: 0.00003756
Iteration 307/1000 | Loss: 0.00003653
Iteration 308/1000 | Loss: 0.00049506
Iteration 309/1000 | Loss: 0.00029568
Iteration 310/1000 | Loss: 0.00038750
Iteration 311/1000 | Loss: 0.00006260
Iteration 312/1000 | Loss: 0.00004375
Iteration 313/1000 | Loss: 0.00003570
Iteration 314/1000 | Loss: 0.00003461
Iteration 315/1000 | Loss: 0.00003384
Iteration 316/1000 | Loss: 0.00003332
Iteration 317/1000 | Loss: 0.00003286
Iteration 318/1000 | Loss: 0.00003264
Iteration 319/1000 | Loss: 0.00003254
Iteration 320/1000 | Loss: 0.00003253
Iteration 321/1000 | Loss: 0.00003243
Iteration 322/1000 | Loss: 0.00003233
Iteration 323/1000 | Loss: 0.00003233
Iteration 324/1000 | Loss: 0.00003229
Iteration 325/1000 | Loss: 0.00003228
Iteration 326/1000 | Loss: 0.00003222
Iteration 327/1000 | Loss: 0.00008867
Iteration 328/1000 | Loss: 0.00007521
Iteration 329/1000 | Loss: 0.00003218
Iteration 330/1000 | Loss: 0.00003217
Iteration 331/1000 | Loss: 0.00003217
Iteration 332/1000 | Loss: 0.00003216
Iteration 333/1000 | Loss: 0.00003216
Iteration 334/1000 | Loss: 0.00003215
Iteration 335/1000 | Loss: 0.00003213
Iteration 336/1000 | Loss: 0.00003211
Iteration 337/1000 | Loss: 0.00003210
Iteration 338/1000 | Loss: 0.00003209
Iteration 339/1000 | Loss: 0.00003207
Iteration 340/1000 | Loss: 0.00003207
Iteration 341/1000 | Loss: 0.00003206
Iteration 342/1000 | Loss: 0.00003201
Iteration 343/1000 | Loss: 0.00003201
Iteration 344/1000 | Loss: 0.00015820
Iteration 345/1000 | Loss: 0.00019850
Iteration 346/1000 | Loss: 0.00017842
Iteration 347/1000 | Loss: 0.00004578
Iteration 348/1000 | Loss: 0.00004118
Iteration 349/1000 | Loss: 0.00003899
Iteration 350/1000 | Loss: 0.00003670
Iteration 351/1000 | Loss: 0.00003527
Iteration 352/1000 | Loss: 0.00003424
Iteration 353/1000 | Loss: 0.00003347
Iteration 354/1000 | Loss: 0.00003285
Iteration 355/1000 | Loss: 0.00003233
Iteration 356/1000 | Loss: 0.00003195
Iteration 357/1000 | Loss: 0.00003176
Iteration 358/1000 | Loss: 0.00003174
Iteration 359/1000 | Loss: 0.00003167
Iteration 360/1000 | Loss: 0.00003158
Iteration 361/1000 | Loss: 0.00003156
Iteration 362/1000 | Loss: 0.00003155
Iteration 363/1000 | Loss: 0.00003155
Iteration 364/1000 | Loss: 0.00003155
Iteration 365/1000 | Loss: 0.00003154
Iteration 366/1000 | Loss: 0.00003154
Iteration 367/1000 | Loss: 0.00003153
Iteration 368/1000 | Loss: 0.00003153
Iteration 369/1000 | Loss: 0.00003152
Iteration 370/1000 | Loss: 0.00003152
Iteration 371/1000 | Loss: 0.00003152
Iteration 372/1000 | Loss: 0.00003151
Iteration 373/1000 | Loss: 0.00003151
Iteration 374/1000 | Loss: 0.00003150
Iteration 375/1000 | Loss: 0.00003148
Iteration 376/1000 | Loss: 0.00003148
Iteration 377/1000 | Loss: 0.00003148
Iteration 378/1000 | Loss: 0.00003147
Iteration 379/1000 | Loss: 0.00003147
Iteration 380/1000 | Loss: 0.00003147
Iteration 381/1000 | Loss: 0.00003147
Iteration 382/1000 | Loss: 0.00003146
Iteration 383/1000 | Loss: 0.00003146
Iteration 384/1000 | Loss: 0.00003146
Iteration 385/1000 | Loss: 0.00003146
Iteration 386/1000 | Loss: 0.00003145
Iteration 387/1000 | Loss: 0.00003145
Iteration 388/1000 | Loss: 0.00003145
Iteration 389/1000 | Loss: 0.00003144
Iteration 390/1000 | Loss: 0.00003144
Iteration 391/1000 | Loss: 0.00003143
Iteration 392/1000 | Loss: 0.00003142
Iteration 393/1000 | Loss: 0.00003142
Iteration 394/1000 | Loss: 0.00003142
Iteration 395/1000 | Loss: 0.00003142
Iteration 396/1000 | Loss: 0.00003142
Iteration 397/1000 | Loss: 0.00003142
Iteration 398/1000 | Loss: 0.00003142
Iteration 399/1000 | Loss: 0.00003142
Iteration 400/1000 | Loss: 0.00003141
Iteration 401/1000 | Loss: 0.00003141
Iteration 402/1000 | Loss: 0.00003141
Iteration 403/1000 | Loss: 0.00003141
Iteration 404/1000 | Loss: 0.00003141
Iteration 405/1000 | Loss: 0.00003141
Iteration 406/1000 | Loss: 0.00003141
Iteration 407/1000 | Loss: 0.00003141
Iteration 408/1000 | Loss: 0.00003141
Iteration 409/1000 | Loss: 0.00003140
Iteration 410/1000 | Loss: 0.00003140
Iteration 411/1000 | Loss: 0.00003140
Iteration 412/1000 | Loss: 0.00003140
Iteration 413/1000 | Loss: 0.00003140
Iteration 414/1000 | Loss: 0.00003140
Iteration 415/1000 | Loss: 0.00003140
Iteration 416/1000 | Loss: 0.00003138
Iteration 417/1000 | Loss: 0.00003137
Iteration 418/1000 | Loss: 0.00003137
Iteration 419/1000 | Loss: 0.00003137
Iteration 420/1000 | Loss: 0.00003137
Iteration 421/1000 | Loss: 0.00003136
Iteration 422/1000 | Loss: 0.00003136
Iteration 423/1000 | Loss: 0.00003135
Iteration 424/1000 | Loss: 0.00003135
Iteration 425/1000 | Loss: 0.00003135
Iteration 426/1000 | Loss: 0.00003134
Iteration 427/1000 | Loss: 0.00003134
Iteration 428/1000 | Loss: 0.00003134
Iteration 429/1000 | Loss: 0.00003134
Iteration 430/1000 | Loss: 0.00003134
Iteration 431/1000 | Loss: 0.00003134
Iteration 432/1000 | Loss: 0.00003134
Iteration 433/1000 | Loss: 0.00003134
Iteration 434/1000 | Loss: 0.00003134
Iteration 435/1000 | Loss: 0.00003134
Iteration 436/1000 | Loss: 0.00003134
Iteration 437/1000 | Loss: 0.00003133
Iteration 438/1000 | Loss: 0.00003133
Iteration 439/1000 | Loss: 0.00003133
Iteration 440/1000 | Loss: 0.00003132
Iteration 441/1000 | Loss: 0.00003132
Iteration 442/1000 | Loss: 0.00003132
Iteration 443/1000 | Loss: 0.00003132
Iteration 444/1000 | Loss: 0.00003132
Iteration 445/1000 | Loss: 0.00003132
Iteration 446/1000 | Loss: 0.00003131
Iteration 447/1000 | Loss: 0.00003131
Iteration 448/1000 | Loss: 0.00003131
Iteration 449/1000 | Loss: 0.00003131
Iteration 450/1000 | Loss: 0.00003131
Iteration 451/1000 | Loss: 0.00003131
Iteration 452/1000 | Loss: 0.00003131
Iteration 453/1000 | Loss: 0.00003131
Iteration 454/1000 | Loss: 0.00003131
Iteration 455/1000 | Loss: 0.00003131
Iteration 456/1000 | Loss: 0.00003131
Iteration 457/1000 | Loss: 0.00003131
Iteration 458/1000 | Loss: 0.00003131
Iteration 459/1000 | Loss: 0.00003131
Iteration 460/1000 | Loss: 0.00003131
Iteration 461/1000 | Loss: 0.00003131
Iteration 462/1000 | Loss: 0.00003131
Iteration 463/1000 | Loss: 0.00003131
Iteration 464/1000 | Loss: 0.00003131
Iteration 465/1000 | Loss: 0.00003131
Iteration 466/1000 | Loss: 0.00003131
Iteration 467/1000 | Loss: 0.00003131
Iteration 468/1000 | Loss: 0.00003131
Iteration 469/1000 | Loss: 0.00003131
Iteration 470/1000 | Loss: 0.00003131
Iteration 471/1000 | Loss: 0.00003131
Iteration 472/1000 | Loss: 0.00003131
Iteration 473/1000 | Loss: 0.00003131
Iteration 474/1000 | Loss: 0.00003131
Iteration 475/1000 | Loss: 0.00003131
Iteration 476/1000 | Loss: 0.00003131
Iteration 477/1000 | Loss: 0.00003131
Iteration 478/1000 | Loss: 0.00003131
Iteration 479/1000 | Loss: 0.00003131
Iteration 480/1000 | Loss: 0.00003131
Iteration 481/1000 | Loss: 0.00003131
Iteration 482/1000 | Loss: 0.00003131
Iteration 483/1000 | Loss: 0.00003131
Iteration 484/1000 | Loss: 0.00003131
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 484. Stopping optimization.
Last 5 losses: [3.130698314635083e-05, 3.130698314635083e-05, 3.130698314635083e-05, 3.130698314635083e-05, 3.130698314635083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.130698314635083e-05

Optimization complete. Final v2v error: 4.371599197387695 mm

Highest mean error: 16.01083755493164 mm for frame 188

Lowest mean error: 3.9346842765808105 mm for frame 15

Saving results

Total time: 619.278960943222
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00864687
Iteration 2/25 | Loss: 0.00203343
Iteration 3/25 | Loss: 0.00157364
Iteration 4/25 | Loss: 0.00155665
Iteration 5/25 | Loss: 0.00155208
Iteration 6/25 | Loss: 0.00155085
Iteration 7/25 | Loss: 0.00155085
Iteration 8/25 | Loss: 0.00155085
Iteration 9/25 | Loss: 0.00155085
Iteration 10/25 | Loss: 0.00155085
Iteration 11/25 | Loss: 0.00155085
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015508468495681882, 0.0015508468495681882, 0.0015508468495681882, 0.0015508468495681882, 0.0015508468495681882]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015508468495681882

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.56717986
Iteration 2/25 | Loss: 0.00128377
Iteration 3/25 | Loss: 0.00128377
Iteration 4/25 | Loss: 0.00128377
Iteration 5/25 | Loss: 0.00128377
Iteration 6/25 | Loss: 0.00128377
Iteration 7/25 | Loss: 0.00128377
Iteration 8/25 | Loss: 0.00128377
Iteration 9/25 | Loss: 0.00128377
Iteration 10/25 | Loss: 0.00128377
Iteration 11/25 | Loss: 0.00128377
Iteration 12/25 | Loss: 0.00128377
Iteration 13/25 | Loss: 0.00128377
Iteration 14/25 | Loss: 0.00128377
Iteration 15/25 | Loss: 0.00128377
Iteration 16/25 | Loss: 0.00128377
Iteration 17/25 | Loss: 0.00128377
Iteration 18/25 | Loss: 0.00128377
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012837706599384546, 0.0012837706599384546, 0.0012837706599384546, 0.0012837706599384546, 0.0012837706599384546]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012837706599384546

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128377
Iteration 2/1000 | Loss: 0.00013774
Iteration 3/1000 | Loss: 0.00009383
Iteration 4/1000 | Loss: 0.00008456
Iteration 5/1000 | Loss: 0.00008085
Iteration 6/1000 | Loss: 0.00007795
Iteration 7/1000 | Loss: 0.00007636
Iteration 8/1000 | Loss: 0.00007481
Iteration 9/1000 | Loss: 0.00007318
Iteration 10/1000 | Loss: 0.00007185
Iteration 11/1000 | Loss: 0.00007097
Iteration 12/1000 | Loss: 0.00006998
Iteration 13/1000 | Loss: 0.00006909
Iteration 14/1000 | Loss: 0.00006818
Iteration 15/1000 | Loss: 0.00006748
Iteration 16/1000 | Loss: 0.00006688
Iteration 17/1000 | Loss: 0.00006634
Iteration 18/1000 | Loss: 0.00006582
Iteration 19/1000 | Loss: 0.00006551
Iteration 20/1000 | Loss: 0.00006525
Iteration 21/1000 | Loss: 0.00006504
Iteration 22/1000 | Loss: 0.00006487
Iteration 23/1000 | Loss: 0.00006464
Iteration 24/1000 | Loss: 0.00006448
Iteration 25/1000 | Loss: 0.00006448
Iteration 26/1000 | Loss: 0.00006445
Iteration 27/1000 | Loss: 0.00006441
Iteration 28/1000 | Loss: 0.00006441
Iteration 29/1000 | Loss: 0.00006441
Iteration 30/1000 | Loss: 0.00006439
Iteration 31/1000 | Loss: 0.00006438
Iteration 32/1000 | Loss: 0.00006438
Iteration 33/1000 | Loss: 0.00006435
Iteration 34/1000 | Loss: 0.00006435
Iteration 35/1000 | Loss: 0.00006430
Iteration 36/1000 | Loss: 0.00006426
Iteration 37/1000 | Loss: 0.00006422
Iteration 38/1000 | Loss: 0.00006422
Iteration 39/1000 | Loss: 0.00006419
Iteration 40/1000 | Loss: 0.00006417
Iteration 41/1000 | Loss: 0.00006417
Iteration 42/1000 | Loss: 0.00006415
Iteration 43/1000 | Loss: 0.00006415
Iteration 44/1000 | Loss: 0.00006415
Iteration 45/1000 | Loss: 0.00006415
Iteration 46/1000 | Loss: 0.00006415
Iteration 47/1000 | Loss: 0.00006415
Iteration 48/1000 | Loss: 0.00006415
Iteration 49/1000 | Loss: 0.00006415
Iteration 50/1000 | Loss: 0.00006414
Iteration 51/1000 | Loss: 0.00006414
Iteration 52/1000 | Loss: 0.00006414
Iteration 53/1000 | Loss: 0.00006414
Iteration 54/1000 | Loss: 0.00006414
Iteration 55/1000 | Loss: 0.00006414
Iteration 56/1000 | Loss: 0.00006414
Iteration 57/1000 | Loss: 0.00006413
Iteration 58/1000 | Loss: 0.00006413
Iteration 59/1000 | Loss: 0.00006413
Iteration 60/1000 | Loss: 0.00006413
Iteration 61/1000 | Loss: 0.00006413
Iteration 62/1000 | Loss: 0.00006412
Iteration 63/1000 | Loss: 0.00006412
Iteration 64/1000 | Loss: 0.00006412
Iteration 65/1000 | Loss: 0.00006411
Iteration 66/1000 | Loss: 0.00006411
Iteration 67/1000 | Loss: 0.00006411
Iteration 68/1000 | Loss: 0.00006411
Iteration 69/1000 | Loss: 0.00006411
Iteration 70/1000 | Loss: 0.00006411
Iteration 71/1000 | Loss: 0.00006411
Iteration 72/1000 | Loss: 0.00006411
Iteration 73/1000 | Loss: 0.00006411
Iteration 74/1000 | Loss: 0.00006411
Iteration 75/1000 | Loss: 0.00006411
Iteration 76/1000 | Loss: 0.00006410
Iteration 77/1000 | Loss: 0.00006410
Iteration 78/1000 | Loss: 0.00006410
Iteration 79/1000 | Loss: 0.00006410
Iteration 80/1000 | Loss: 0.00006410
Iteration 81/1000 | Loss: 0.00006410
Iteration 82/1000 | Loss: 0.00006410
Iteration 83/1000 | Loss: 0.00006410
Iteration 84/1000 | Loss: 0.00006409
Iteration 85/1000 | Loss: 0.00006408
Iteration 86/1000 | Loss: 0.00006408
Iteration 87/1000 | Loss: 0.00006408
Iteration 88/1000 | Loss: 0.00006408
Iteration 89/1000 | Loss: 0.00006408
Iteration 90/1000 | Loss: 0.00006408
Iteration 91/1000 | Loss: 0.00006407
Iteration 92/1000 | Loss: 0.00006407
Iteration 93/1000 | Loss: 0.00006407
Iteration 94/1000 | Loss: 0.00006407
Iteration 95/1000 | Loss: 0.00006407
Iteration 96/1000 | Loss: 0.00006406
Iteration 97/1000 | Loss: 0.00006406
Iteration 98/1000 | Loss: 0.00006406
Iteration 99/1000 | Loss: 0.00006406
Iteration 100/1000 | Loss: 0.00006406
Iteration 101/1000 | Loss: 0.00006406
Iteration 102/1000 | Loss: 0.00006406
Iteration 103/1000 | Loss: 0.00006406
Iteration 104/1000 | Loss: 0.00006406
Iteration 105/1000 | Loss: 0.00006406
Iteration 106/1000 | Loss: 0.00006406
Iteration 107/1000 | Loss: 0.00006406
Iteration 108/1000 | Loss: 0.00006406
Iteration 109/1000 | Loss: 0.00006406
Iteration 110/1000 | Loss: 0.00006406
Iteration 111/1000 | Loss: 0.00006406
Iteration 112/1000 | Loss: 0.00006406
Iteration 113/1000 | Loss: 0.00006406
Iteration 114/1000 | Loss: 0.00006405
Iteration 115/1000 | Loss: 0.00006405
Iteration 116/1000 | Loss: 0.00006405
Iteration 117/1000 | Loss: 0.00006405
Iteration 118/1000 | Loss: 0.00006405
Iteration 119/1000 | Loss: 0.00006405
Iteration 120/1000 | Loss: 0.00006405
Iteration 121/1000 | Loss: 0.00006405
Iteration 122/1000 | Loss: 0.00006404
Iteration 123/1000 | Loss: 0.00006404
Iteration 124/1000 | Loss: 0.00006404
Iteration 125/1000 | Loss: 0.00006404
Iteration 126/1000 | Loss: 0.00006404
Iteration 127/1000 | Loss: 0.00006404
Iteration 128/1000 | Loss: 0.00006404
Iteration 129/1000 | Loss: 0.00006404
Iteration 130/1000 | Loss: 0.00006404
Iteration 131/1000 | Loss: 0.00006404
Iteration 132/1000 | Loss: 0.00006404
Iteration 133/1000 | Loss: 0.00006403
Iteration 134/1000 | Loss: 0.00006403
Iteration 135/1000 | Loss: 0.00006403
Iteration 136/1000 | Loss: 0.00006403
Iteration 137/1000 | Loss: 0.00006403
Iteration 138/1000 | Loss: 0.00006403
Iteration 139/1000 | Loss: 0.00006403
Iteration 140/1000 | Loss: 0.00006403
Iteration 141/1000 | Loss: 0.00006403
Iteration 142/1000 | Loss: 0.00006403
Iteration 143/1000 | Loss: 0.00006403
Iteration 144/1000 | Loss: 0.00006402
Iteration 145/1000 | Loss: 0.00006402
Iteration 146/1000 | Loss: 0.00006402
Iteration 147/1000 | Loss: 0.00006402
Iteration 148/1000 | Loss: 0.00006402
Iteration 149/1000 | Loss: 0.00006402
Iteration 150/1000 | Loss: 0.00006402
Iteration 151/1000 | Loss: 0.00006402
Iteration 152/1000 | Loss: 0.00006402
Iteration 153/1000 | Loss: 0.00006402
Iteration 154/1000 | Loss: 0.00006402
Iteration 155/1000 | Loss: 0.00006402
Iteration 156/1000 | Loss: 0.00006402
Iteration 157/1000 | Loss: 0.00006402
Iteration 158/1000 | Loss: 0.00006402
Iteration 159/1000 | Loss: 0.00006402
Iteration 160/1000 | Loss: 0.00006402
Iteration 161/1000 | Loss: 0.00006402
Iteration 162/1000 | Loss: 0.00006402
Iteration 163/1000 | Loss: 0.00006402
Iteration 164/1000 | Loss: 0.00006402
Iteration 165/1000 | Loss: 0.00006402
Iteration 166/1000 | Loss: 0.00006402
Iteration 167/1000 | Loss: 0.00006402
Iteration 168/1000 | Loss: 0.00006401
Iteration 169/1000 | Loss: 0.00006401
Iteration 170/1000 | Loss: 0.00006401
Iteration 171/1000 | Loss: 0.00006401
Iteration 172/1000 | Loss: 0.00006401
Iteration 173/1000 | Loss: 0.00006401
Iteration 174/1000 | Loss: 0.00006401
Iteration 175/1000 | Loss: 0.00006401
Iteration 176/1000 | Loss: 0.00006401
Iteration 177/1000 | Loss: 0.00006401
Iteration 178/1000 | Loss: 0.00006401
Iteration 179/1000 | Loss: 0.00006401
Iteration 180/1000 | Loss: 0.00006401
Iteration 181/1000 | Loss: 0.00006401
Iteration 182/1000 | Loss: 0.00006401
Iteration 183/1000 | Loss: 0.00006401
Iteration 184/1000 | Loss: 0.00006401
Iteration 185/1000 | Loss: 0.00006401
Iteration 186/1000 | Loss: 0.00006401
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [6.401380960596725e-05, 6.401380960596725e-05, 6.401380960596725e-05, 6.401380960596725e-05, 6.401380960596725e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.401380960596725e-05

Optimization complete. Final v2v error: 6.42893648147583 mm

Highest mean error: 7.588752746582031 mm for frame 164

Lowest mean error: 4.949336528778076 mm for frame 1

Saving results

Total time: 60.90129637718201
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871208
Iteration 2/25 | Loss: 0.00164258
Iteration 3/25 | Loss: 0.00149841
Iteration 4/25 | Loss: 0.00147992
Iteration 5/25 | Loss: 0.00147457
Iteration 6/25 | Loss: 0.00147343
Iteration 7/25 | Loss: 0.00147343
Iteration 8/25 | Loss: 0.00147343
Iteration 9/25 | Loss: 0.00147343
Iteration 10/25 | Loss: 0.00147343
Iteration 11/25 | Loss: 0.00147343
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0014734311262145638, 0.0014734311262145638, 0.0014734311262145638, 0.0014734311262145638, 0.0014734311262145638]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014734311262145638

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06190979
Iteration 2/25 | Loss: 0.00194132
Iteration 3/25 | Loss: 0.00194129
Iteration 4/25 | Loss: 0.00194129
Iteration 5/25 | Loss: 0.00194129
Iteration 6/25 | Loss: 0.00194129
Iteration 7/25 | Loss: 0.00194129
Iteration 8/25 | Loss: 0.00194129
Iteration 9/25 | Loss: 0.00194129
Iteration 10/25 | Loss: 0.00194129
Iteration 11/25 | Loss: 0.00194129
Iteration 12/25 | Loss: 0.00194129
Iteration 13/25 | Loss: 0.00194129
Iteration 14/25 | Loss: 0.00194129
Iteration 15/25 | Loss: 0.00194129
Iteration 16/25 | Loss: 0.00194129
Iteration 17/25 | Loss: 0.00194129
Iteration 18/25 | Loss: 0.00194129
Iteration 19/25 | Loss: 0.00194129
Iteration 20/25 | Loss: 0.00194129
Iteration 21/25 | Loss: 0.00194129
Iteration 22/25 | Loss: 0.00194129
Iteration 23/25 | Loss: 0.00194129
Iteration 24/25 | Loss: 0.00194129
Iteration 25/25 | Loss: 0.00194129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00194129
Iteration 2/1000 | Loss: 0.00006757
Iteration 3/1000 | Loss: 0.00005013
Iteration 4/1000 | Loss: 0.00004399
Iteration 5/1000 | Loss: 0.00004141
Iteration 6/1000 | Loss: 0.00004013
Iteration 7/1000 | Loss: 0.00003932
Iteration 8/1000 | Loss: 0.00003862
Iteration 9/1000 | Loss: 0.00003801
Iteration 10/1000 | Loss: 0.00003767
Iteration 11/1000 | Loss: 0.00003738
Iteration 12/1000 | Loss: 0.00003715
Iteration 13/1000 | Loss: 0.00003690
Iteration 14/1000 | Loss: 0.00003670
Iteration 15/1000 | Loss: 0.00003657
Iteration 16/1000 | Loss: 0.00003641
Iteration 17/1000 | Loss: 0.00003626
Iteration 18/1000 | Loss: 0.00003607
Iteration 19/1000 | Loss: 0.00003602
Iteration 20/1000 | Loss: 0.00003600
Iteration 21/1000 | Loss: 0.00003587
Iteration 22/1000 | Loss: 0.00003581
Iteration 23/1000 | Loss: 0.00003575
Iteration 24/1000 | Loss: 0.00003572
Iteration 25/1000 | Loss: 0.00003571
Iteration 26/1000 | Loss: 0.00003571
Iteration 27/1000 | Loss: 0.00003570
Iteration 28/1000 | Loss: 0.00003568
Iteration 29/1000 | Loss: 0.00003568
Iteration 30/1000 | Loss: 0.00003567
Iteration 31/1000 | Loss: 0.00003566
Iteration 32/1000 | Loss: 0.00003566
Iteration 33/1000 | Loss: 0.00003565
Iteration 34/1000 | Loss: 0.00003565
Iteration 35/1000 | Loss: 0.00003565
Iteration 36/1000 | Loss: 0.00003562
Iteration 37/1000 | Loss: 0.00003562
Iteration 38/1000 | Loss: 0.00003562
Iteration 39/1000 | Loss: 0.00003561
Iteration 40/1000 | Loss: 0.00003561
Iteration 41/1000 | Loss: 0.00003561
Iteration 42/1000 | Loss: 0.00003561
Iteration 43/1000 | Loss: 0.00003561
Iteration 44/1000 | Loss: 0.00003561
Iteration 45/1000 | Loss: 0.00003560
Iteration 46/1000 | Loss: 0.00003560
Iteration 47/1000 | Loss: 0.00003559
Iteration 48/1000 | Loss: 0.00003557
Iteration 49/1000 | Loss: 0.00003557
Iteration 50/1000 | Loss: 0.00003557
Iteration 51/1000 | Loss: 0.00003556
Iteration 52/1000 | Loss: 0.00003556
Iteration 53/1000 | Loss: 0.00003556
Iteration 54/1000 | Loss: 0.00003556
Iteration 55/1000 | Loss: 0.00003556
Iteration 56/1000 | Loss: 0.00003556
Iteration 57/1000 | Loss: 0.00003556
Iteration 58/1000 | Loss: 0.00003556
Iteration 59/1000 | Loss: 0.00003555
Iteration 60/1000 | Loss: 0.00003555
Iteration 61/1000 | Loss: 0.00003554
Iteration 62/1000 | Loss: 0.00003554
Iteration 63/1000 | Loss: 0.00003553
Iteration 64/1000 | Loss: 0.00003553
Iteration 65/1000 | Loss: 0.00003552
Iteration 66/1000 | Loss: 0.00003552
Iteration 67/1000 | Loss: 0.00003552
Iteration 68/1000 | Loss: 0.00003552
Iteration 69/1000 | Loss: 0.00003552
Iteration 70/1000 | Loss: 0.00003552
Iteration 71/1000 | Loss: 0.00003551
Iteration 72/1000 | Loss: 0.00003551
Iteration 73/1000 | Loss: 0.00003551
Iteration 74/1000 | Loss: 0.00003550
Iteration 75/1000 | Loss: 0.00003550
Iteration 76/1000 | Loss: 0.00003550
Iteration 77/1000 | Loss: 0.00003550
Iteration 78/1000 | Loss: 0.00003550
Iteration 79/1000 | Loss: 0.00003550
Iteration 80/1000 | Loss: 0.00003550
Iteration 81/1000 | Loss: 0.00003549
Iteration 82/1000 | Loss: 0.00003549
Iteration 83/1000 | Loss: 0.00003549
Iteration 84/1000 | Loss: 0.00003549
Iteration 85/1000 | Loss: 0.00003549
Iteration 86/1000 | Loss: 0.00003549
Iteration 87/1000 | Loss: 0.00003549
Iteration 88/1000 | Loss: 0.00003548
Iteration 89/1000 | Loss: 0.00003548
Iteration 90/1000 | Loss: 0.00003548
Iteration 91/1000 | Loss: 0.00003548
Iteration 92/1000 | Loss: 0.00003548
Iteration 93/1000 | Loss: 0.00003548
Iteration 94/1000 | Loss: 0.00003548
Iteration 95/1000 | Loss: 0.00003547
Iteration 96/1000 | Loss: 0.00003547
Iteration 97/1000 | Loss: 0.00003547
Iteration 98/1000 | Loss: 0.00003547
Iteration 99/1000 | Loss: 0.00003546
Iteration 100/1000 | Loss: 0.00003546
Iteration 101/1000 | Loss: 0.00003546
Iteration 102/1000 | Loss: 0.00003546
Iteration 103/1000 | Loss: 0.00003546
Iteration 104/1000 | Loss: 0.00003546
Iteration 105/1000 | Loss: 0.00003546
Iteration 106/1000 | Loss: 0.00003546
Iteration 107/1000 | Loss: 0.00003546
Iteration 108/1000 | Loss: 0.00003546
Iteration 109/1000 | Loss: 0.00003546
Iteration 110/1000 | Loss: 0.00003545
Iteration 111/1000 | Loss: 0.00003545
Iteration 112/1000 | Loss: 0.00003545
Iteration 113/1000 | Loss: 0.00003545
Iteration 114/1000 | Loss: 0.00003545
Iteration 115/1000 | Loss: 0.00003545
Iteration 116/1000 | Loss: 0.00003545
Iteration 117/1000 | Loss: 0.00003545
Iteration 118/1000 | Loss: 0.00003544
Iteration 119/1000 | Loss: 0.00003544
Iteration 120/1000 | Loss: 0.00003544
Iteration 121/1000 | Loss: 0.00003544
Iteration 122/1000 | Loss: 0.00003544
Iteration 123/1000 | Loss: 0.00003543
Iteration 124/1000 | Loss: 0.00003543
Iteration 125/1000 | Loss: 0.00003543
Iteration 126/1000 | Loss: 0.00003543
Iteration 127/1000 | Loss: 0.00003543
Iteration 128/1000 | Loss: 0.00003543
Iteration 129/1000 | Loss: 0.00003543
Iteration 130/1000 | Loss: 0.00003543
Iteration 131/1000 | Loss: 0.00003543
Iteration 132/1000 | Loss: 0.00003543
Iteration 133/1000 | Loss: 0.00003542
Iteration 134/1000 | Loss: 0.00003542
Iteration 135/1000 | Loss: 0.00003542
Iteration 136/1000 | Loss: 0.00003542
Iteration 137/1000 | Loss: 0.00003541
Iteration 138/1000 | Loss: 0.00003541
Iteration 139/1000 | Loss: 0.00003541
Iteration 140/1000 | Loss: 0.00003541
Iteration 141/1000 | Loss: 0.00003541
Iteration 142/1000 | Loss: 0.00003541
Iteration 143/1000 | Loss: 0.00003540
Iteration 144/1000 | Loss: 0.00003540
Iteration 145/1000 | Loss: 0.00003540
Iteration 146/1000 | Loss: 0.00003540
Iteration 147/1000 | Loss: 0.00003540
Iteration 148/1000 | Loss: 0.00003540
Iteration 149/1000 | Loss: 0.00003540
Iteration 150/1000 | Loss: 0.00003540
Iteration 151/1000 | Loss: 0.00003540
Iteration 152/1000 | Loss: 0.00003539
Iteration 153/1000 | Loss: 0.00003539
Iteration 154/1000 | Loss: 0.00003539
Iteration 155/1000 | Loss: 0.00003539
Iteration 156/1000 | Loss: 0.00003539
Iteration 157/1000 | Loss: 0.00003538
Iteration 158/1000 | Loss: 0.00003538
Iteration 159/1000 | Loss: 0.00003538
Iteration 160/1000 | Loss: 0.00003538
Iteration 161/1000 | Loss: 0.00003538
Iteration 162/1000 | Loss: 0.00003538
Iteration 163/1000 | Loss: 0.00003538
Iteration 164/1000 | Loss: 0.00003538
Iteration 165/1000 | Loss: 0.00003538
Iteration 166/1000 | Loss: 0.00003538
Iteration 167/1000 | Loss: 0.00003538
Iteration 168/1000 | Loss: 0.00003538
Iteration 169/1000 | Loss: 0.00003538
Iteration 170/1000 | Loss: 0.00003537
Iteration 171/1000 | Loss: 0.00003537
Iteration 172/1000 | Loss: 0.00003537
Iteration 173/1000 | Loss: 0.00003537
Iteration 174/1000 | Loss: 0.00003537
Iteration 175/1000 | Loss: 0.00003537
Iteration 176/1000 | Loss: 0.00003537
Iteration 177/1000 | Loss: 0.00003537
Iteration 178/1000 | Loss: 0.00003537
Iteration 179/1000 | Loss: 0.00003537
Iteration 180/1000 | Loss: 0.00003536
Iteration 181/1000 | Loss: 0.00003536
Iteration 182/1000 | Loss: 0.00003536
Iteration 183/1000 | Loss: 0.00003536
Iteration 184/1000 | Loss: 0.00003536
Iteration 185/1000 | Loss: 0.00003536
Iteration 186/1000 | Loss: 0.00003536
Iteration 187/1000 | Loss: 0.00003536
Iteration 188/1000 | Loss: 0.00003536
Iteration 189/1000 | Loss: 0.00003535
Iteration 190/1000 | Loss: 0.00003535
Iteration 191/1000 | Loss: 0.00003535
Iteration 192/1000 | Loss: 0.00003535
Iteration 193/1000 | Loss: 0.00003535
Iteration 194/1000 | Loss: 0.00003535
Iteration 195/1000 | Loss: 0.00003534
Iteration 196/1000 | Loss: 0.00003534
Iteration 197/1000 | Loss: 0.00003534
Iteration 198/1000 | Loss: 0.00003534
Iteration 199/1000 | Loss: 0.00003534
Iteration 200/1000 | Loss: 0.00003534
Iteration 201/1000 | Loss: 0.00003534
Iteration 202/1000 | Loss: 0.00003534
Iteration 203/1000 | Loss: 0.00003534
Iteration 204/1000 | Loss: 0.00003533
Iteration 205/1000 | Loss: 0.00003533
Iteration 206/1000 | Loss: 0.00003533
Iteration 207/1000 | Loss: 0.00003533
Iteration 208/1000 | Loss: 0.00003533
Iteration 209/1000 | Loss: 0.00003532
Iteration 210/1000 | Loss: 0.00003532
Iteration 211/1000 | Loss: 0.00003532
Iteration 212/1000 | Loss: 0.00003532
Iteration 213/1000 | Loss: 0.00003532
Iteration 214/1000 | Loss: 0.00003532
Iteration 215/1000 | Loss: 0.00003532
Iteration 216/1000 | Loss: 0.00003532
Iteration 217/1000 | Loss: 0.00003532
Iteration 218/1000 | Loss: 0.00003531
Iteration 219/1000 | Loss: 0.00003531
Iteration 220/1000 | Loss: 0.00003531
Iteration 221/1000 | Loss: 0.00003531
Iteration 222/1000 | Loss: 0.00003531
Iteration 223/1000 | Loss: 0.00003531
Iteration 224/1000 | Loss: 0.00003531
Iteration 225/1000 | Loss: 0.00003531
Iteration 226/1000 | Loss: 0.00003531
Iteration 227/1000 | Loss: 0.00003531
Iteration 228/1000 | Loss: 0.00003531
Iteration 229/1000 | Loss: 0.00003531
Iteration 230/1000 | Loss: 0.00003531
Iteration 231/1000 | Loss: 0.00003531
Iteration 232/1000 | Loss: 0.00003531
Iteration 233/1000 | Loss: 0.00003530
Iteration 234/1000 | Loss: 0.00003530
Iteration 235/1000 | Loss: 0.00003530
Iteration 236/1000 | Loss: 0.00003530
Iteration 237/1000 | Loss: 0.00003530
Iteration 238/1000 | Loss: 0.00003530
Iteration 239/1000 | Loss: 0.00003530
Iteration 240/1000 | Loss: 0.00003530
Iteration 241/1000 | Loss: 0.00003530
Iteration 242/1000 | Loss: 0.00003530
Iteration 243/1000 | Loss: 0.00003530
Iteration 244/1000 | Loss: 0.00003530
Iteration 245/1000 | Loss: 0.00003530
Iteration 246/1000 | Loss: 0.00003530
Iteration 247/1000 | Loss: 0.00003530
Iteration 248/1000 | Loss: 0.00003529
Iteration 249/1000 | Loss: 0.00003529
Iteration 250/1000 | Loss: 0.00003529
Iteration 251/1000 | Loss: 0.00003529
Iteration 252/1000 | Loss: 0.00003529
Iteration 253/1000 | Loss: 0.00003529
Iteration 254/1000 | Loss: 0.00003529
Iteration 255/1000 | Loss: 0.00003529
Iteration 256/1000 | Loss: 0.00003529
Iteration 257/1000 | Loss: 0.00003529
Iteration 258/1000 | Loss: 0.00003529
Iteration 259/1000 | Loss: 0.00003529
Iteration 260/1000 | Loss: 0.00003529
Iteration 261/1000 | Loss: 0.00003529
Iteration 262/1000 | Loss: 0.00003528
Iteration 263/1000 | Loss: 0.00003528
Iteration 264/1000 | Loss: 0.00003528
Iteration 265/1000 | Loss: 0.00003528
Iteration 266/1000 | Loss: 0.00003528
Iteration 267/1000 | Loss: 0.00003528
Iteration 268/1000 | Loss: 0.00003528
Iteration 269/1000 | Loss: 0.00003528
Iteration 270/1000 | Loss: 0.00003528
Iteration 271/1000 | Loss: 0.00003528
Iteration 272/1000 | Loss: 0.00003528
Iteration 273/1000 | Loss: 0.00003528
Iteration 274/1000 | Loss: 0.00003528
Iteration 275/1000 | Loss: 0.00003528
Iteration 276/1000 | Loss: 0.00003528
Iteration 277/1000 | Loss: 0.00003528
Iteration 278/1000 | Loss: 0.00003528
Iteration 279/1000 | Loss: 0.00003528
Iteration 280/1000 | Loss: 0.00003528
Iteration 281/1000 | Loss: 0.00003528
Iteration 282/1000 | Loss: 0.00003528
Iteration 283/1000 | Loss: 0.00003528
Iteration 284/1000 | Loss: 0.00003527
Iteration 285/1000 | Loss: 0.00003527
Iteration 286/1000 | Loss: 0.00003527
Iteration 287/1000 | Loss: 0.00003527
Iteration 288/1000 | Loss: 0.00003527
Iteration 289/1000 | Loss: 0.00003527
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 289. Stopping optimization.
Last 5 losses: [3.527483204379678e-05, 3.527483204379678e-05, 3.527483204379678e-05, 3.527483204379678e-05, 3.527483204379678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.527483204379678e-05

Optimization complete. Final v2v error: 4.77444314956665 mm

Highest mean error: 5.864988327026367 mm for frame 59

Lowest mean error: 3.564368963241577 mm for frame 4

Saving results

Total time: 57.6971275806427
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_40_us_2158/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_40_us_2158/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01078459
Iteration 2/25 | Loss: 0.00239955
Iteration 3/25 | Loss: 0.00183824
Iteration 4/25 | Loss: 0.00177406
Iteration 5/25 | Loss: 0.00184916
Iteration 6/25 | Loss: 0.00171324
Iteration 7/25 | Loss: 0.00158762
Iteration 8/25 | Loss: 0.00156478
Iteration 9/25 | Loss: 0.00155513
Iteration 10/25 | Loss: 0.00152811
Iteration 11/25 | Loss: 0.00152572
Iteration 12/25 | Loss: 0.00152550
Iteration 13/25 | Loss: 0.00152428
Iteration 14/25 | Loss: 0.00150648
Iteration 15/25 | Loss: 0.00150840
Iteration 16/25 | Loss: 0.00150193
Iteration 17/25 | Loss: 0.00149384
Iteration 18/25 | Loss: 0.00149003
Iteration 19/25 | Loss: 0.00149171
Iteration 20/25 | Loss: 0.00149453
Iteration 21/25 | Loss: 0.00148972
Iteration 22/25 | Loss: 0.00149034
Iteration 23/25 | Loss: 0.00148541
Iteration 24/25 | Loss: 0.00148942
Iteration 25/25 | Loss: 0.00148754

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.80410385
Iteration 2/25 | Loss: 0.00167467
Iteration 3/25 | Loss: 0.00167467
Iteration 4/25 | Loss: 0.00167467
Iteration 5/25 | Loss: 0.00167467
Iteration 6/25 | Loss: 0.00167467
Iteration 7/25 | Loss: 0.00167467
Iteration 8/25 | Loss: 0.00167467
Iteration 9/25 | Loss: 0.00167467
Iteration 10/25 | Loss: 0.00167467
Iteration 11/25 | Loss: 0.00167467
Iteration 12/25 | Loss: 0.00167467
Iteration 13/25 | Loss: 0.00167467
Iteration 14/25 | Loss: 0.00167467
Iteration 15/25 | Loss: 0.00167467
Iteration 16/25 | Loss: 0.00167467
Iteration 17/25 | Loss: 0.00167467
Iteration 18/25 | Loss: 0.00167467
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016746696783229709, 0.0016746696783229709, 0.0016746696783229709, 0.0016746696783229709, 0.0016746696783229709]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016746696783229709

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00167467
Iteration 2/1000 | Loss: 0.00040974
Iteration 3/1000 | Loss: 0.00016679
Iteration 4/1000 | Loss: 0.00032857
Iteration 5/1000 | Loss: 0.00011385
Iteration 6/1000 | Loss: 0.00018031
Iteration 7/1000 | Loss: 0.00014089
Iteration 8/1000 | Loss: 0.00013146
Iteration 9/1000 | Loss: 0.00015894
Iteration 10/1000 | Loss: 0.00011753
Iteration 11/1000 | Loss: 0.00010614
Iteration 12/1000 | Loss: 0.00008929
Iteration 13/1000 | Loss: 0.00010075
Iteration 14/1000 | Loss: 0.00010416
Iteration 15/1000 | Loss: 0.00010723
Iteration 16/1000 | Loss: 0.00010905
Iteration 17/1000 | Loss: 0.00013168
Iteration 18/1000 | Loss: 0.00011025
Iteration 19/1000 | Loss: 0.00010495
Iteration 20/1000 | Loss: 0.00009211
Iteration 21/1000 | Loss: 0.00013082
Iteration 22/1000 | Loss: 0.00012664
Iteration 23/1000 | Loss: 0.00011780
Iteration 24/1000 | Loss: 0.00011745
Iteration 25/1000 | Loss: 0.00011906
Iteration 26/1000 | Loss: 0.00011516
Iteration 27/1000 | Loss: 0.00011635
Iteration 28/1000 | Loss: 0.00014958
Iteration 29/1000 | Loss: 0.00012255
Iteration 30/1000 | Loss: 0.00011537
Iteration 31/1000 | Loss: 0.00010767
Iteration 32/1000 | Loss: 0.00010098
Iteration 33/1000 | Loss: 0.00010674
Iteration 34/1000 | Loss: 0.00011919
Iteration 35/1000 | Loss: 0.00011348
Iteration 36/1000 | Loss: 0.00012204
Iteration 37/1000 | Loss: 0.00013954
Iteration 38/1000 | Loss: 0.00012441
Iteration 39/1000 | Loss: 0.00007971
Iteration 40/1000 | Loss: 0.00008136
Iteration 41/1000 | Loss: 0.00010008
Iteration 42/1000 | Loss: 0.00013620
Iteration 43/1000 | Loss: 0.00012119
Iteration 44/1000 | Loss: 0.00009753
Iteration 45/1000 | Loss: 0.00009816
Iteration 46/1000 | Loss: 0.00007560
Iteration 47/1000 | Loss: 0.00009021
Iteration 48/1000 | Loss: 0.00008257
Iteration 49/1000 | Loss: 0.00008431
Iteration 50/1000 | Loss: 0.00009804
Iteration 51/1000 | Loss: 0.00007282
Iteration 52/1000 | Loss: 0.00007502
Iteration 53/1000 | Loss: 0.00007645
Iteration 54/1000 | Loss: 0.00006120
Iteration 55/1000 | Loss: 0.00006700
Iteration 56/1000 | Loss: 0.00006867
Iteration 57/1000 | Loss: 0.00008227
Iteration 58/1000 | Loss: 0.00005997
Iteration 59/1000 | Loss: 0.00005245
Iteration 60/1000 | Loss: 0.00005531
Iteration 61/1000 | Loss: 0.00005488
Iteration 62/1000 | Loss: 0.00004795
Iteration 63/1000 | Loss: 0.00005788
Iteration 64/1000 | Loss: 0.00006258
Iteration 65/1000 | Loss: 0.00006143
Iteration 66/1000 | Loss: 0.00005899
Iteration 67/1000 | Loss: 0.00005678
Iteration 68/1000 | Loss: 0.00004754
Iteration 69/1000 | Loss: 0.00004040
Iteration 70/1000 | Loss: 0.00003815
Iteration 71/1000 | Loss: 0.00004437
Iteration 72/1000 | Loss: 0.00003744
Iteration 73/1000 | Loss: 0.00004239
Iteration 74/1000 | Loss: 0.00004314
Iteration 75/1000 | Loss: 0.00004347
Iteration 76/1000 | Loss: 0.00004108
Iteration 77/1000 | Loss: 0.00004066
Iteration 78/1000 | Loss: 0.00004535
Iteration 79/1000 | Loss: 0.00004514
Iteration 80/1000 | Loss: 0.00004441
Iteration 81/1000 | Loss: 0.00005052
Iteration 82/1000 | Loss: 0.00004521
Iteration 83/1000 | Loss: 0.00004097
Iteration 84/1000 | Loss: 0.00003992
Iteration 85/1000 | Loss: 0.00003513
Iteration 86/1000 | Loss: 0.00003444
Iteration 87/1000 | Loss: 0.00003416
Iteration 88/1000 | Loss: 0.00003691
Iteration 89/1000 | Loss: 0.00003554
Iteration 90/1000 | Loss: 0.00003405
Iteration 91/1000 | Loss: 0.00004103
Iteration 92/1000 | Loss: 0.00003993
Iteration 93/1000 | Loss: 0.00003982
Iteration 94/1000 | Loss: 0.00003807
Iteration 95/1000 | Loss: 0.00003890
Iteration 96/1000 | Loss: 0.00003962
Iteration 97/1000 | Loss: 0.00004835
Iteration 98/1000 | Loss: 0.00004351
Iteration 99/1000 | Loss: 0.00004351
Iteration 100/1000 | Loss: 0.00004443
Iteration 101/1000 | Loss: 0.00004361
Iteration 102/1000 | Loss: 0.00004285
Iteration 103/1000 | Loss: 0.00004400
Iteration 104/1000 | Loss: 0.00003786
Iteration 105/1000 | Loss: 0.00004155
Iteration 106/1000 | Loss: 0.00004393
Iteration 107/1000 | Loss: 0.00004537
Iteration 108/1000 | Loss: 0.00004284
Iteration 109/1000 | Loss: 0.00004495
Iteration 110/1000 | Loss: 0.00004338
Iteration 111/1000 | Loss: 0.00003793
Iteration 112/1000 | Loss: 0.00003983
Iteration 113/1000 | Loss: 0.00003985
Iteration 114/1000 | Loss: 0.00004593
Iteration 115/1000 | Loss: 0.00004179
Iteration 116/1000 | Loss: 0.00003877
Iteration 117/1000 | Loss: 0.00003433
Iteration 118/1000 | Loss: 0.00004043
Iteration 119/1000 | Loss: 0.00003979
Iteration 120/1000 | Loss: 0.00003979
Iteration 121/1000 | Loss: 0.00005146
Iteration 122/1000 | Loss: 0.00004379
Iteration 123/1000 | Loss: 0.00004190
Iteration 124/1000 | Loss: 0.00004178
Iteration 125/1000 | Loss: 0.00004178
Iteration 126/1000 | Loss: 0.00004190
Iteration 127/1000 | Loss: 0.00004388
Iteration 128/1000 | Loss: 0.00004272
Iteration 129/1000 | Loss: 0.00003577
Iteration 130/1000 | Loss: 0.00004033
Iteration 131/1000 | Loss: 0.00004369
Iteration 132/1000 | Loss: 0.00004061
Iteration 133/1000 | Loss: 0.00004070
Iteration 134/1000 | Loss: 0.00003911
Iteration 135/1000 | Loss: 0.00004013
Iteration 136/1000 | Loss: 0.00003833
Iteration 137/1000 | Loss: 0.00004428
Iteration 138/1000 | Loss: 0.00003948
Iteration 139/1000 | Loss: 0.00004131
Iteration 140/1000 | Loss: 0.00003916
Iteration 141/1000 | Loss: 0.00004238
Iteration 142/1000 | Loss: 0.00003969
Iteration 143/1000 | Loss: 0.00004213
Iteration 144/1000 | Loss: 0.00004104
Iteration 145/1000 | Loss: 0.00003979
Iteration 146/1000 | Loss: 0.00004410
Iteration 147/1000 | Loss: 0.00004259
Iteration 148/1000 | Loss: 0.00004374
Iteration 149/1000 | Loss: 0.00004183
Iteration 150/1000 | Loss: 0.00004425
Iteration 151/1000 | Loss: 0.00004182
Iteration 152/1000 | Loss: 0.00004419
Iteration 153/1000 | Loss: 0.00004213
Iteration 154/1000 | Loss: 0.00004515
Iteration 155/1000 | Loss: 0.00004304
Iteration 156/1000 | Loss: 0.00004409
Iteration 157/1000 | Loss: 0.00004174
Iteration 158/1000 | Loss: 0.00004136
Iteration 159/1000 | Loss: 0.00004188
Iteration 160/1000 | Loss: 0.00004251
Iteration 161/1000 | Loss: 0.00004064
Iteration 162/1000 | Loss: 0.00004051
Iteration 163/1000 | Loss: 0.00003967
Iteration 164/1000 | Loss: 0.00004584
Iteration 165/1000 | Loss: 0.00003497
Iteration 166/1000 | Loss: 0.00003446
Iteration 167/1000 | Loss: 0.00003395
Iteration 168/1000 | Loss: 0.00003351
Iteration 169/1000 | Loss: 0.00003306
Iteration 170/1000 | Loss: 0.00003284
Iteration 171/1000 | Loss: 0.00003272
Iteration 172/1000 | Loss: 0.00003272
Iteration 173/1000 | Loss: 0.00003272
Iteration 174/1000 | Loss: 0.00003272
Iteration 175/1000 | Loss: 0.00003272
Iteration 176/1000 | Loss: 0.00003272
Iteration 177/1000 | Loss: 0.00003272
Iteration 178/1000 | Loss: 0.00003272
Iteration 179/1000 | Loss: 0.00003271
Iteration 180/1000 | Loss: 0.00003271
Iteration 181/1000 | Loss: 0.00003271
Iteration 182/1000 | Loss: 0.00003271
Iteration 183/1000 | Loss: 0.00003271
Iteration 184/1000 | Loss: 0.00003271
Iteration 185/1000 | Loss: 0.00003271
Iteration 186/1000 | Loss: 0.00003271
Iteration 187/1000 | Loss: 0.00003269
Iteration 188/1000 | Loss: 0.00003253
Iteration 189/1000 | Loss: 0.00003242
Iteration 190/1000 | Loss: 0.00003242
Iteration 191/1000 | Loss: 0.00003239
Iteration 192/1000 | Loss: 0.00003239
Iteration 193/1000 | Loss: 0.00003237
Iteration 194/1000 | Loss: 0.00003237
Iteration 195/1000 | Loss: 0.00003233
Iteration 196/1000 | Loss: 0.00003233
Iteration 197/1000 | Loss: 0.00003232
Iteration 198/1000 | Loss: 0.00003232
Iteration 199/1000 | Loss: 0.00003232
Iteration 200/1000 | Loss: 0.00003232
Iteration 201/1000 | Loss: 0.00003232
Iteration 202/1000 | Loss: 0.00003232
Iteration 203/1000 | Loss: 0.00003232
Iteration 204/1000 | Loss: 0.00003232
Iteration 205/1000 | Loss: 0.00003232
Iteration 206/1000 | Loss: 0.00003231
Iteration 207/1000 | Loss: 0.00003231
Iteration 208/1000 | Loss: 0.00003231
Iteration 209/1000 | Loss: 0.00003231
Iteration 210/1000 | Loss: 0.00003231
Iteration 211/1000 | Loss: 0.00003231
Iteration 212/1000 | Loss: 0.00003230
Iteration 213/1000 | Loss: 0.00003230
Iteration 214/1000 | Loss: 0.00003230
Iteration 215/1000 | Loss: 0.00003230
Iteration 216/1000 | Loss: 0.00003230
Iteration 217/1000 | Loss: 0.00003229
Iteration 218/1000 | Loss: 0.00003229
Iteration 219/1000 | Loss: 0.00003229
Iteration 220/1000 | Loss: 0.00003229
Iteration 221/1000 | Loss: 0.00003229
Iteration 222/1000 | Loss: 0.00003229
Iteration 223/1000 | Loss: 0.00003229
Iteration 224/1000 | Loss: 0.00003229
Iteration 225/1000 | Loss: 0.00003229
Iteration 226/1000 | Loss: 0.00003229
Iteration 227/1000 | Loss: 0.00003229
Iteration 228/1000 | Loss: 0.00003229
Iteration 229/1000 | Loss: 0.00003229
Iteration 230/1000 | Loss: 0.00003229
Iteration 231/1000 | Loss: 0.00003228
Iteration 232/1000 | Loss: 0.00003228
Iteration 233/1000 | Loss: 0.00003228
Iteration 234/1000 | Loss: 0.00003228
Iteration 235/1000 | Loss: 0.00003228
Iteration 236/1000 | Loss: 0.00003228
Iteration 237/1000 | Loss: 0.00003228
Iteration 238/1000 | Loss: 0.00003228
Iteration 239/1000 | Loss: 0.00003227
Iteration 240/1000 | Loss: 0.00003227
Iteration 241/1000 | Loss: 0.00003227
Iteration 242/1000 | Loss: 0.00003227
Iteration 243/1000 | Loss: 0.00003227
Iteration 244/1000 | Loss: 0.00003227
Iteration 245/1000 | Loss: 0.00003227
Iteration 246/1000 | Loss: 0.00003227
Iteration 247/1000 | Loss: 0.00003227
Iteration 248/1000 | Loss: 0.00003227
Iteration 249/1000 | Loss: 0.00003227
Iteration 250/1000 | Loss: 0.00003227
Iteration 251/1000 | Loss: 0.00003227
Iteration 252/1000 | Loss: 0.00003226
Iteration 253/1000 | Loss: 0.00003226
Iteration 254/1000 | Loss: 0.00003226
Iteration 255/1000 | Loss: 0.00003226
Iteration 256/1000 | Loss: 0.00003226
Iteration 257/1000 | Loss: 0.00003226
Iteration 258/1000 | Loss: 0.00003226
Iteration 259/1000 | Loss: 0.00003226
Iteration 260/1000 | Loss: 0.00003226
Iteration 261/1000 | Loss: 0.00003226
Iteration 262/1000 | Loss: 0.00003226
Iteration 263/1000 | Loss: 0.00003226
Iteration 264/1000 | Loss: 0.00003226
Iteration 265/1000 | Loss: 0.00003226
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 265. Stopping optimization.
Last 5 losses: [3.2261865271721035e-05, 3.2261865271721035e-05, 3.2261865271721035e-05, 3.2261865271721035e-05, 3.2261865271721035e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2261865271721035e-05

Optimization complete. Final v2v error: 4.554988861083984 mm

Highest mean error: 5.883666038513184 mm for frame 239

Lowest mean error: 4.114778995513916 mm for frame 0

Saving results

Total time: 340.5199213027954
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1042/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1042.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1042
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00847364
Iteration 2/25 | Loss: 0.00166826
Iteration 3/25 | Loss: 0.00137259
Iteration 4/25 | Loss: 0.00129539
Iteration 5/25 | Loss: 0.00130686
Iteration 6/25 | Loss: 0.00125185
Iteration 7/25 | Loss: 0.00122877
Iteration 8/25 | Loss: 0.00123377
Iteration 9/25 | Loss: 0.00122472
Iteration 10/25 | Loss: 0.00121876
Iteration 11/25 | Loss: 0.00121575
Iteration 12/25 | Loss: 0.00122353
Iteration 13/25 | Loss: 0.00121986
Iteration 14/25 | Loss: 0.00121587
Iteration 15/25 | Loss: 0.00121162
Iteration 16/25 | Loss: 0.00121138
Iteration 17/25 | Loss: 0.00121126
Iteration 18/25 | Loss: 0.00121579
Iteration 19/25 | Loss: 0.00121486
Iteration 20/25 | Loss: 0.00121187
Iteration 21/25 | Loss: 0.00121430
Iteration 22/25 | Loss: 0.00121923
Iteration 23/25 | Loss: 0.00120964
Iteration 24/25 | Loss: 0.00120948
Iteration 25/25 | Loss: 0.00120944

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 12.29740906
Iteration 2/25 | Loss: 0.00164356
Iteration 3/25 | Loss: 0.00150879
Iteration 4/25 | Loss: 0.00150879
Iteration 5/25 | Loss: 0.00150879
Iteration 6/25 | Loss: 0.00150879
Iteration 7/25 | Loss: 0.00150879
Iteration 8/25 | Loss: 0.00150879
Iteration 9/25 | Loss: 0.00150879
Iteration 10/25 | Loss: 0.00150879
Iteration 11/25 | Loss: 0.00150879
Iteration 12/25 | Loss: 0.00150879
Iteration 13/25 | Loss: 0.00150879
Iteration 14/25 | Loss: 0.00150879
Iteration 15/25 | Loss: 0.00150879
Iteration 16/25 | Loss: 0.00150879
Iteration 17/25 | Loss: 0.00150879
Iteration 18/25 | Loss: 0.00150879
Iteration 19/25 | Loss: 0.00150879
Iteration 20/25 | Loss: 0.00150879
Iteration 21/25 | Loss: 0.00150879
Iteration 22/25 | Loss: 0.00150879
Iteration 23/25 | Loss: 0.00150879
Iteration 24/25 | Loss: 0.00150879
Iteration 25/25 | Loss: 0.00150879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00150879
Iteration 2/1000 | Loss: 0.00003847
Iteration 3/1000 | Loss: 0.00016781
Iteration 4/1000 | Loss: 0.00002231
Iteration 5/1000 | Loss: 0.00002056
Iteration 6/1000 | Loss: 0.00008350
Iteration 7/1000 | Loss: 0.00006978
Iteration 8/1000 | Loss: 0.00001780
Iteration 9/1000 | Loss: 0.00001682
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001614
Iteration 12/1000 | Loss: 0.00001605
Iteration 13/1000 | Loss: 0.00001587
Iteration 14/1000 | Loss: 0.00001569
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001559
Iteration 17/1000 | Loss: 0.00001557
Iteration 18/1000 | Loss: 0.00001557
Iteration 19/1000 | Loss: 0.00001556
Iteration 20/1000 | Loss: 0.00001555
Iteration 21/1000 | Loss: 0.00001554
Iteration 22/1000 | Loss: 0.00001553
Iteration 23/1000 | Loss: 0.00001553
Iteration 24/1000 | Loss: 0.00001552
Iteration 25/1000 | Loss: 0.00008185
Iteration 26/1000 | Loss: 0.00007428
Iteration 27/1000 | Loss: 0.00001894
Iteration 28/1000 | Loss: 0.00004636
Iteration 29/1000 | Loss: 0.00001555
Iteration 30/1000 | Loss: 0.00004280
Iteration 31/1000 | Loss: 0.00001566
Iteration 32/1000 | Loss: 0.00001541
Iteration 33/1000 | Loss: 0.00001539
Iteration 34/1000 | Loss: 0.00001538
Iteration 35/1000 | Loss: 0.00001537
Iteration 36/1000 | Loss: 0.00001537
Iteration 37/1000 | Loss: 0.00001536
Iteration 38/1000 | Loss: 0.00001536
Iteration 39/1000 | Loss: 0.00001536
Iteration 40/1000 | Loss: 0.00001536
Iteration 41/1000 | Loss: 0.00001536
Iteration 42/1000 | Loss: 0.00001536
Iteration 43/1000 | Loss: 0.00001536
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001536
Iteration 46/1000 | Loss: 0.00001536
Iteration 47/1000 | Loss: 0.00001536
Iteration 48/1000 | Loss: 0.00001535
Iteration 49/1000 | Loss: 0.00001535
Iteration 50/1000 | Loss: 0.00001535
Iteration 51/1000 | Loss: 0.00001534
Iteration 52/1000 | Loss: 0.00001534
Iteration 53/1000 | Loss: 0.00001533
Iteration 54/1000 | Loss: 0.00001533
Iteration 55/1000 | Loss: 0.00001533
Iteration 56/1000 | Loss: 0.00001532
Iteration 57/1000 | Loss: 0.00001532
Iteration 58/1000 | Loss: 0.00003753
Iteration 59/1000 | Loss: 0.00001542
Iteration 60/1000 | Loss: 0.00001529
Iteration 61/1000 | Loss: 0.00001525
Iteration 62/1000 | Loss: 0.00001525
Iteration 63/1000 | Loss: 0.00001524
Iteration 64/1000 | Loss: 0.00001523
Iteration 65/1000 | Loss: 0.00001522
Iteration 66/1000 | Loss: 0.00001522
Iteration 67/1000 | Loss: 0.00001522
Iteration 68/1000 | Loss: 0.00001522
Iteration 69/1000 | Loss: 0.00001522
Iteration 70/1000 | Loss: 0.00001522
Iteration 71/1000 | Loss: 0.00001522
Iteration 72/1000 | Loss: 0.00001522
Iteration 73/1000 | Loss: 0.00001522
Iteration 74/1000 | Loss: 0.00001522
Iteration 75/1000 | Loss: 0.00001522
Iteration 76/1000 | Loss: 0.00001521
Iteration 77/1000 | Loss: 0.00001521
Iteration 78/1000 | Loss: 0.00001521
Iteration 79/1000 | Loss: 0.00001521
Iteration 80/1000 | Loss: 0.00001521
Iteration 81/1000 | Loss: 0.00001521
Iteration 82/1000 | Loss: 0.00001521
Iteration 83/1000 | Loss: 0.00001521
Iteration 84/1000 | Loss: 0.00001521
Iteration 85/1000 | Loss: 0.00001521
Iteration 86/1000 | Loss: 0.00001521
Iteration 87/1000 | Loss: 0.00001521
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 87. Stopping optimization.
Last 5 losses: [1.520728983450681e-05, 1.520728983450681e-05, 1.520728983450681e-05, 1.520728983450681e-05, 1.520728983450681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.520728983450681e-05

Optimization complete. Final v2v error: 3.310411214828491 mm

Highest mean error: 3.9931130409240723 mm for frame 77

Lowest mean error: 2.7000222206115723 mm for frame 2

Saving results

Total time: 86.81742525100708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00431324
Iteration 2/25 | Loss: 0.00135783
Iteration 3/25 | Loss: 0.00124093
Iteration 4/25 | Loss: 0.00123210
Iteration 5/25 | Loss: 0.00122928
Iteration 6/25 | Loss: 0.00122894
Iteration 7/25 | Loss: 0.00122888
Iteration 8/25 | Loss: 0.00122888
Iteration 9/25 | Loss: 0.00122888
Iteration 10/25 | Loss: 0.00122888
Iteration 11/25 | Loss: 0.00122888
Iteration 12/25 | Loss: 0.00122888
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012288844445720315, 0.0012288844445720315, 0.0012288844445720315, 0.0012288844445720315, 0.0012288844445720315]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012288844445720315

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56779814
Iteration 2/25 | Loss: 0.00115703
Iteration 3/25 | Loss: 0.00115702
Iteration 4/25 | Loss: 0.00115702
Iteration 5/25 | Loss: 0.00115702
Iteration 6/25 | Loss: 0.00115702
Iteration 7/25 | Loss: 0.00115702
Iteration 8/25 | Loss: 0.00115702
Iteration 9/25 | Loss: 0.00115702
Iteration 10/25 | Loss: 0.00115702
Iteration 11/25 | Loss: 0.00115701
Iteration 12/25 | Loss: 0.00115701
Iteration 13/25 | Loss: 0.00115701
Iteration 14/25 | Loss: 0.00115701
Iteration 15/25 | Loss: 0.00115701
Iteration 16/25 | Loss: 0.00115701
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001157014979980886, 0.001157014979980886, 0.001157014979980886, 0.001157014979980886, 0.001157014979980886]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001157014979980886

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115701
Iteration 2/1000 | Loss: 0.00002650
Iteration 3/1000 | Loss: 0.00001858
Iteration 4/1000 | Loss: 0.00001666
Iteration 5/1000 | Loss: 0.00001565
Iteration 6/1000 | Loss: 0.00001500
Iteration 7/1000 | Loss: 0.00001459
Iteration 8/1000 | Loss: 0.00001433
Iteration 9/1000 | Loss: 0.00001408
Iteration 10/1000 | Loss: 0.00001386
Iteration 11/1000 | Loss: 0.00001371
Iteration 12/1000 | Loss: 0.00001369
Iteration 13/1000 | Loss: 0.00001364
Iteration 14/1000 | Loss: 0.00001357
Iteration 15/1000 | Loss: 0.00001355
Iteration 16/1000 | Loss: 0.00001354
Iteration 17/1000 | Loss: 0.00001349
Iteration 18/1000 | Loss: 0.00001346
Iteration 19/1000 | Loss: 0.00001342
Iteration 20/1000 | Loss: 0.00001339
Iteration 21/1000 | Loss: 0.00001339
Iteration 22/1000 | Loss: 0.00001338
Iteration 23/1000 | Loss: 0.00001338
Iteration 24/1000 | Loss: 0.00001338
Iteration 25/1000 | Loss: 0.00001337
Iteration 26/1000 | Loss: 0.00001337
Iteration 27/1000 | Loss: 0.00001335
Iteration 28/1000 | Loss: 0.00001335
Iteration 29/1000 | Loss: 0.00001334
Iteration 30/1000 | Loss: 0.00001333
Iteration 31/1000 | Loss: 0.00001332
Iteration 32/1000 | Loss: 0.00001332
Iteration 33/1000 | Loss: 0.00001331
Iteration 34/1000 | Loss: 0.00001327
Iteration 35/1000 | Loss: 0.00001327
Iteration 36/1000 | Loss: 0.00001324
Iteration 37/1000 | Loss: 0.00001324
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001320
Iteration 44/1000 | Loss: 0.00001319
Iteration 45/1000 | Loss: 0.00001319
Iteration 46/1000 | Loss: 0.00001318
Iteration 47/1000 | Loss: 0.00001318
Iteration 48/1000 | Loss: 0.00001317
Iteration 49/1000 | Loss: 0.00001317
Iteration 50/1000 | Loss: 0.00001317
Iteration 51/1000 | Loss: 0.00001316
Iteration 52/1000 | Loss: 0.00001315
Iteration 53/1000 | Loss: 0.00001315
Iteration 54/1000 | Loss: 0.00001315
Iteration 55/1000 | Loss: 0.00001314
Iteration 56/1000 | Loss: 0.00001314
Iteration 57/1000 | Loss: 0.00001314
Iteration 58/1000 | Loss: 0.00001313
Iteration 59/1000 | Loss: 0.00001312
Iteration 60/1000 | Loss: 0.00001312
Iteration 61/1000 | Loss: 0.00001312
Iteration 62/1000 | Loss: 0.00001312
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001312
Iteration 65/1000 | Loss: 0.00001311
Iteration 66/1000 | Loss: 0.00001311
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001310
Iteration 69/1000 | Loss: 0.00001310
Iteration 70/1000 | Loss: 0.00001309
Iteration 71/1000 | Loss: 0.00001309
Iteration 72/1000 | Loss: 0.00001309
Iteration 73/1000 | Loss: 0.00001308
Iteration 74/1000 | Loss: 0.00001308
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001307
Iteration 78/1000 | Loss: 0.00001306
Iteration 79/1000 | Loss: 0.00001306
Iteration 80/1000 | Loss: 0.00001306
Iteration 81/1000 | Loss: 0.00001306
Iteration 82/1000 | Loss: 0.00001306
Iteration 83/1000 | Loss: 0.00001306
Iteration 84/1000 | Loss: 0.00001306
Iteration 85/1000 | Loss: 0.00001306
Iteration 86/1000 | Loss: 0.00001306
Iteration 87/1000 | Loss: 0.00001306
Iteration 88/1000 | Loss: 0.00001305
Iteration 89/1000 | Loss: 0.00001305
Iteration 90/1000 | Loss: 0.00001305
Iteration 91/1000 | Loss: 0.00001305
Iteration 92/1000 | Loss: 0.00001305
Iteration 93/1000 | Loss: 0.00001305
Iteration 94/1000 | Loss: 0.00001304
Iteration 95/1000 | Loss: 0.00001303
Iteration 96/1000 | Loss: 0.00001303
Iteration 97/1000 | Loss: 0.00001303
Iteration 98/1000 | Loss: 0.00001303
Iteration 99/1000 | Loss: 0.00001303
Iteration 100/1000 | Loss: 0.00001303
Iteration 101/1000 | Loss: 0.00001302
Iteration 102/1000 | Loss: 0.00001302
Iteration 103/1000 | Loss: 0.00001302
Iteration 104/1000 | Loss: 0.00001301
Iteration 105/1000 | Loss: 0.00001301
Iteration 106/1000 | Loss: 0.00001301
Iteration 107/1000 | Loss: 0.00001301
Iteration 108/1000 | Loss: 0.00001301
Iteration 109/1000 | Loss: 0.00001300
Iteration 110/1000 | Loss: 0.00001300
Iteration 111/1000 | Loss: 0.00001300
Iteration 112/1000 | Loss: 0.00001300
Iteration 113/1000 | Loss: 0.00001300
Iteration 114/1000 | Loss: 0.00001300
Iteration 115/1000 | Loss: 0.00001299
Iteration 116/1000 | Loss: 0.00001299
Iteration 117/1000 | Loss: 0.00001299
Iteration 118/1000 | Loss: 0.00001299
Iteration 119/1000 | Loss: 0.00001299
Iteration 120/1000 | Loss: 0.00001299
Iteration 121/1000 | Loss: 0.00001299
Iteration 122/1000 | Loss: 0.00001298
Iteration 123/1000 | Loss: 0.00001298
Iteration 124/1000 | Loss: 0.00001298
Iteration 125/1000 | Loss: 0.00001298
Iteration 126/1000 | Loss: 0.00001298
Iteration 127/1000 | Loss: 0.00001298
Iteration 128/1000 | Loss: 0.00001297
Iteration 129/1000 | Loss: 0.00001297
Iteration 130/1000 | Loss: 0.00001297
Iteration 131/1000 | Loss: 0.00001296
Iteration 132/1000 | Loss: 0.00001296
Iteration 133/1000 | Loss: 0.00001296
Iteration 134/1000 | Loss: 0.00001296
Iteration 135/1000 | Loss: 0.00001296
Iteration 136/1000 | Loss: 0.00001296
Iteration 137/1000 | Loss: 0.00001296
Iteration 138/1000 | Loss: 0.00001296
Iteration 139/1000 | Loss: 0.00001295
Iteration 140/1000 | Loss: 0.00001295
Iteration 141/1000 | Loss: 0.00001295
Iteration 142/1000 | Loss: 0.00001295
Iteration 143/1000 | Loss: 0.00001295
Iteration 144/1000 | Loss: 0.00001294
Iteration 145/1000 | Loss: 0.00001294
Iteration 146/1000 | Loss: 0.00001294
Iteration 147/1000 | Loss: 0.00001294
Iteration 148/1000 | Loss: 0.00001294
Iteration 149/1000 | Loss: 0.00001294
Iteration 150/1000 | Loss: 0.00001294
Iteration 151/1000 | Loss: 0.00001294
Iteration 152/1000 | Loss: 0.00001294
Iteration 153/1000 | Loss: 0.00001294
Iteration 154/1000 | Loss: 0.00001293
Iteration 155/1000 | Loss: 0.00001293
Iteration 156/1000 | Loss: 0.00001293
Iteration 157/1000 | Loss: 0.00001293
Iteration 158/1000 | Loss: 0.00001293
Iteration 159/1000 | Loss: 0.00001292
Iteration 160/1000 | Loss: 0.00001292
Iteration 161/1000 | Loss: 0.00001292
Iteration 162/1000 | Loss: 0.00001292
Iteration 163/1000 | Loss: 0.00001292
Iteration 164/1000 | Loss: 0.00001292
Iteration 165/1000 | Loss: 0.00001292
Iteration 166/1000 | Loss: 0.00001292
Iteration 167/1000 | Loss: 0.00001292
Iteration 168/1000 | Loss: 0.00001292
Iteration 169/1000 | Loss: 0.00001292
Iteration 170/1000 | Loss: 0.00001291
Iteration 171/1000 | Loss: 0.00001291
Iteration 172/1000 | Loss: 0.00001291
Iteration 173/1000 | Loss: 0.00001291
Iteration 174/1000 | Loss: 0.00001291
Iteration 175/1000 | Loss: 0.00001291
Iteration 176/1000 | Loss: 0.00001291
Iteration 177/1000 | Loss: 0.00001291
Iteration 178/1000 | Loss: 0.00001291
Iteration 179/1000 | Loss: 0.00001291
Iteration 180/1000 | Loss: 0.00001290
Iteration 181/1000 | Loss: 0.00001290
Iteration 182/1000 | Loss: 0.00001290
Iteration 183/1000 | Loss: 0.00001290
Iteration 184/1000 | Loss: 0.00001290
Iteration 185/1000 | Loss: 0.00001290
Iteration 186/1000 | Loss: 0.00001289
Iteration 187/1000 | Loss: 0.00001289
Iteration 188/1000 | Loss: 0.00001289
Iteration 189/1000 | Loss: 0.00001289
Iteration 190/1000 | Loss: 0.00001289
Iteration 191/1000 | Loss: 0.00001289
Iteration 192/1000 | Loss: 0.00001288
Iteration 193/1000 | Loss: 0.00001288
Iteration 194/1000 | Loss: 0.00001288
Iteration 195/1000 | Loss: 0.00001288
Iteration 196/1000 | Loss: 0.00001288
Iteration 197/1000 | Loss: 0.00001288
Iteration 198/1000 | Loss: 0.00001288
Iteration 199/1000 | Loss: 0.00001288
Iteration 200/1000 | Loss: 0.00001287
Iteration 201/1000 | Loss: 0.00001287
Iteration 202/1000 | Loss: 0.00001287
Iteration 203/1000 | Loss: 0.00001287
Iteration 204/1000 | Loss: 0.00001287
Iteration 205/1000 | Loss: 0.00001287
Iteration 206/1000 | Loss: 0.00001287
Iteration 207/1000 | Loss: 0.00001287
Iteration 208/1000 | Loss: 0.00001287
Iteration 209/1000 | Loss: 0.00001287
Iteration 210/1000 | Loss: 0.00001287
Iteration 211/1000 | Loss: 0.00001286
Iteration 212/1000 | Loss: 0.00001286
Iteration 213/1000 | Loss: 0.00001286
Iteration 214/1000 | Loss: 0.00001286
Iteration 215/1000 | Loss: 0.00001286
Iteration 216/1000 | Loss: 0.00001286
Iteration 217/1000 | Loss: 0.00001286
Iteration 218/1000 | Loss: 0.00001286
Iteration 219/1000 | Loss: 0.00001286
Iteration 220/1000 | Loss: 0.00001286
Iteration 221/1000 | Loss: 0.00001286
Iteration 222/1000 | Loss: 0.00001286
Iteration 223/1000 | Loss: 0.00001286
Iteration 224/1000 | Loss: 0.00001286
Iteration 225/1000 | Loss: 0.00001286
Iteration 226/1000 | Loss: 0.00001286
Iteration 227/1000 | Loss: 0.00001286
Iteration 228/1000 | Loss: 0.00001286
Iteration 229/1000 | Loss: 0.00001286
Iteration 230/1000 | Loss: 0.00001286
Iteration 231/1000 | Loss: 0.00001286
Iteration 232/1000 | Loss: 0.00001286
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 232. Stopping optimization.
Last 5 losses: [1.2855951354140416e-05, 1.2855951354140416e-05, 1.2855951354140416e-05, 1.2855951354140416e-05, 1.2855951354140416e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2855951354140416e-05

Optimization complete. Final v2v error: 3.0385544300079346 mm

Highest mean error: 3.6402597427368164 mm for frame 84

Lowest mean error: 2.5418882369995117 mm for frame 189

Saving results

Total time: 51.17249870300293
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00825697
Iteration 2/25 | Loss: 0.00212058
Iteration 3/25 | Loss: 0.00161979
Iteration 4/25 | Loss: 0.00155761
Iteration 5/25 | Loss: 0.00154541
Iteration 6/25 | Loss: 0.00149122
Iteration 7/25 | Loss: 0.00144762
Iteration 8/25 | Loss: 0.00143384
Iteration 9/25 | Loss: 0.00142714
Iteration 10/25 | Loss: 0.00141724
Iteration 11/25 | Loss: 0.00141478
Iteration 12/25 | Loss: 0.00141777
Iteration 13/25 | Loss: 0.00141721
Iteration 14/25 | Loss: 0.00141678
Iteration 15/25 | Loss: 0.00141514
Iteration 16/25 | Loss: 0.00141585
Iteration 17/25 | Loss: 0.00141750
Iteration 18/25 | Loss: 0.00141602
Iteration 19/25 | Loss: 0.00141313
Iteration 20/25 | Loss: 0.00141379
Iteration 21/25 | Loss: 0.00141450
Iteration 22/25 | Loss: 0.00141555
Iteration 23/25 | Loss: 0.00141431
Iteration 24/25 | Loss: 0.00141290
Iteration 25/25 | Loss: 0.00141394

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32992017
Iteration 2/25 | Loss: 0.00117996
Iteration 3/25 | Loss: 0.00117994
Iteration 4/25 | Loss: 0.00117994
Iteration 5/25 | Loss: 0.00117994
Iteration 6/25 | Loss: 0.00117994
Iteration 7/25 | Loss: 0.00117994
Iteration 8/25 | Loss: 0.00117994
Iteration 9/25 | Loss: 0.00117994
Iteration 10/25 | Loss: 0.00117994
Iteration 11/25 | Loss: 0.00117994
Iteration 12/25 | Loss: 0.00117994
Iteration 13/25 | Loss: 0.00117994
Iteration 14/25 | Loss: 0.00117994
Iteration 15/25 | Loss: 0.00117994
Iteration 16/25 | Loss: 0.00117994
Iteration 17/25 | Loss: 0.00117994
Iteration 18/25 | Loss: 0.00117994
Iteration 19/25 | Loss: 0.00117994
Iteration 20/25 | Loss: 0.00117994
Iteration 21/25 | Loss: 0.00117994
Iteration 22/25 | Loss: 0.00117994
Iteration 23/25 | Loss: 0.00117994
Iteration 24/25 | Loss: 0.00117994
Iteration 25/25 | Loss: 0.00117994

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117994
Iteration 2/1000 | Loss: 0.00007088
Iteration 3/1000 | Loss: 0.00006350
Iteration 4/1000 | Loss: 0.00006267
Iteration 5/1000 | Loss: 0.00013655
Iteration 6/1000 | Loss: 0.00007504
Iteration 7/1000 | Loss: 0.00007592
Iteration 8/1000 | Loss: 0.00008362
Iteration 9/1000 | Loss: 0.00004441
Iteration 10/1000 | Loss: 0.00005675
Iteration 11/1000 | Loss: 0.00005174
Iteration 12/1000 | Loss: 0.00004393
Iteration 13/1000 | Loss: 0.00009541
Iteration 14/1000 | Loss: 0.00007417
Iteration 15/1000 | Loss: 0.00006864
Iteration 16/1000 | Loss: 0.00006558
Iteration 17/1000 | Loss: 0.00008623
Iteration 18/1000 | Loss: 0.00008539
Iteration 19/1000 | Loss: 0.00007663
Iteration 20/1000 | Loss: 0.00007627
Iteration 21/1000 | Loss: 0.00007872
Iteration 22/1000 | Loss: 0.00014562
Iteration 23/1000 | Loss: 0.00009731
Iteration 24/1000 | Loss: 0.00006880
Iteration 25/1000 | Loss: 0.00013539
Iteration 26/1000 | Loss: 0.00007859
Iteration 27/1000 | Loss: 0.00007394
Iteration 28/1000 | Loss: 0.00006892
Iteration 29/1000 | Loss: 0.00005952
Iteration 30/1000 | Loss: 0.00006254
Iteration 31/1000 | Loss: 0.00007196
Iteration 32/1000 | Loss: 0.00005989
Iteration 33/1000 | Loss: 0.00006316
Iteration 34/1000 | Loss: 0.00007028
Iteration 35/1000 | Loss: 0.00006402
Iteration 36/1000 | Loss: 0.00006658
Iteration 37/1000 | Loss: 0.00006439
Iteration 38/1000 | Loss: 0.00004738
Iteration 39/1000 | Loss: 0.00006694
Iteration 40/1000 | Loss: 0.00007858
Iteration 41/1000 | Loss: 0.00007834
Iteration 42/1000 | Loss: 0.00006970
Iteration 43/1000 | Loss: 0.00007329
Iteration 44/1000 | Loss: 0.00007187
Iteration 45/1000 | Loss: 0.00004554
Iteration 46/1000 | Loss: 0.00006509
Iteration 47/1000 | Loss: 0.00006633
Iteration 48/1000 | Loss: 0.00004908
Iteration 49/1000 | Loss: 0.00005020
Iteration 50/1000 | Loss: 0.00004512
Iteration 51/1000 | Loss: 0.00005328
Iteration 52/1000 | Loss: 0.00004581
Iteration 53/1000 | Loss: 0.00006472
Iteration 54/1000 | Loss: 0.00005407
Iteration 55/1000 | Loss: 0.00006301
Iteration 56/1000 | Loss: 0.00005812
Iteration 57/1000 | Loss: 0.00005004
Iteration 58/1000 | Loss: 0.00004797
Iteration 59/1000 | Loss: 0.00005025
Iteration 60/1000 | Loss: 0.00004867
Iteration 61/1000 | Loss: 0.00004834
Iteration 62/1000 | Loss: 0.00004901
Iteration 63/1000 | Loss: 0.00004685
Iteration 64/1000 | Loss: 0.00003216
Iteration 65/1000 | Loss: 0.00004849
Iteration 66/1000 | Loss: 0.00004232
Iteration 67/1000 | Loss: 0.00003517
Iteration 68/1000 | Loss: 0.00003523
Iteration 69/1000 | Loss: 0.00004484
Iteration 70/1000 | Loss: 0.00005679
Iteration 71/1000 | Loss: 0.00004054
Iteration 72/1000 | Loss: 0.00005821
Iteration 73/1000 | Loss: 0.00004947
Iteration 74/1000 | Loss: 0.00002984
Iteration 75/1000 | Loss: 0.00002668
Iteration 76/1000 | Loss: 0.00002523
Iteration 77/1000 | Loss: 0.00002440
Iteration 78/1000 | Loss: 0.00002392
Iteration 79/1000 | Loss: 0.00002353
Iteration 80/1000 | Loss: 0.00002322
Iteration 81/1000 | Loss: 0.00002288
Iteration 82/1000 | Loss: 0.00002271
Iteration 83/1000 | Loss: 0.00002271
Iteration 84/1000 | Loss: 0.00002258
Iteration 85/1000 | Loss: 0.00002254
Iteration 86/1000 | Loss: 0.00002253
Iteration 87/1000 | Loss: 0.00002245
Iteration 88/1000 | Loss: 0.00002239
Iteration 89/1000 | Loss: 0.00002239
Iteration 90/1000 | Loss: 0.00002239
Iteration 91/1000 | Loss: 0.00002238
Iteration 92/1000 | Loss: 0.00002238
Iteration 93/1000 | Loss: 0.00002237
Iteration 94/1000 | Loss: 0.00002237
Iteration 95/1000 | Loss: 0.00002237
Iteration 96/1000 | Loss: 0.00002237
Iteration 97/1000 | Loss: 0.00002237
Iteration 98/1000 | Loss: 0.00002237
Iteration 99/1000 | Loss: 0.00002237
Iteration 100/1000 | Loss: 0.00002236
Iteration 101/1000 | Loss: 0.00002235
Iteration 102/1000 | Loss: 0.00002235
Iteration 103/1000 | Loss: 0.00002234
Iteration 104/1000 | Loss: 0.00002234
Iteration 105/1000 | Loss: 0.00002234
Iteration 106/1000 | Loss: 0.00002234
Iteration 107/1000 | Loss: 0.00002234
Iteration 108/1000 | Loss: 0.00002233
Iteration 109/1000 | Loss: 0.00002233
Iteration 110/1000 | Loss: 0.00002233
Iteration 111/1000 | Loss: 0.00002232
Iteration 112/1000 | Loss: 0.00002232
Iteration 113/1000 | Loss: 0.00002231
Iteration 114/1000 | Loss: 0.00002231
Iteration 115/1000 | Loss: 0.00002231
Iteration 116/1000 | Loss: 0.00002230
Iteration 117/1000 | Loss: 0.00002230
Iteration 118/1000 | Loss: 0.00002230
Iteration 119/1000 | Loss: 0.00002230
Iteration 120/1000 | Loss: 0.00002230
Iteration 121/1000 | Loss: 0.00002230
Iteration 122/1000 | Loss: 0.00002230
Iteration 123/1000 | Loss: 0.00002230
Iteration 124/1000 | Loss: 0.00002229
Iteration 125/1000 | Loss: 0.00002229
Iteration 126/1000 | Loss: 0.00002229
Iteration 127/1000 | Loss: 0.00002229
Iteration 128/1000 | Loss: 0.00002229
Iteration 129/1000 | Loss: 0.00002229
Iteration 130/1000 | Loss: 0.00002228
Iteration 131/1000 | Loss: 0.00002228
Iteration 132/1000 | Loss: 0.00002228
Iteration 133/1000 | Loss: 0.00002228
Iteration 134/1000 | Loss: 0.00002228
Iteration 135/1000 | Loss: 0.00002228
Iteration 136/1000 | Loss: 0.00002228
Iteration 137/1000 | Loss: 0.00002228
Iteration 138/1000 | Loss: 0.00002227
Iteration 139/1000 | Loss: 0.00002227
Iteration 140/1000 | Loss: 0.00002227
Iteration 141/1000 | Loss: 0.00002227
Iteration 142/1000 | Loss: 0.00002227
Iteration 143/1000 | Loss: 0.00002227
Iteration 144/1000 | Loss: 0.00002227
Iteration 145/1000 | Loss: 0.00002226
Iteration 146/1000 | Loss: 0.00002226
Iteration 147/1000 | Loss: 0.00002226
Iteration 148/1000 | Loss: 0.00002226
Iteration 149/1000 | Loss: 0.00002226
Iteration 150/1000 | Loss: 0.00002226
Iteration 151/1000 | Loss: 0.00002226
Iteration 152/1000 | Loss: 0.00002226
Iteration 153/1000 | Loss: 0.00002225
Iteration 154/1000 | Loss: 0.00002225
Iteration 155/1000 | Loss: 0.00002225
Iteration 156/1000 | Loss: 0.00002225
Iteration 157/1000 | Loss: 0.00002225
Iteration 158/1000 | Loss: 0.00002225
Iteration 159/1000 | Loss: 0.00002225
Iteration 160/1000 | Loss: 0.00002225
Iteration 161/1000 | Loss: 0.00002224
Iteration 162/1000 | Loss: 0.00002224
Iteration 163/1000 | Loss: 0.00002224
Iteration 164/1000 | Loss: 0.00002224
Iteration 165/1000 | Loss: 0.00002224
Iteration 166/1000 | Loss: 0.00002224
Iteration 167/1000 | Loss: 0.00002224
Iteration 168/1000 | Loss: 0.00002224
Iteration 169/1000 | Loss: 0.00002224
Iteration 170/1000 | Loss: 0.00002224
Iteration 171/1000 | Loss: 0.00002224
Iteration 172/1000 | Loss: 0.00002224
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [2.2237198209040798e-05, 2.2237198209040798e-05, 2.2237198209040798e-05, 2.2237198209040798e-05, 2.2237198209040798e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2237198209040798e-05

Optimization complete. Final v2v error: 3.9481399059295654 mm

Highest mean error: 8.10848331451416 mm for frame 236

Lowest mean error: 3.5868115425109863 mm for frame 74

Saving results

Total time: 197.1636734008789
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00806895
Iteration 2/25 | Loss: 0.00188654
Iteration 3/25 | Loss: 0.00145166
Iteration 4/25 | Loss: 0.00139046
Iteration 5/25 | Loss: 0.00140865
Iteration 6/25 | Loss: 0.00140154
Iteration 7/25 | Loss: 0.00134357
Iteration 8/25 | Loss: 0.00132005
Iteration 9/25 | Loss: 0.00130705
Iteration 10/25 | Loss: 0.00130428
Iteration 11/25 | Loss: 0.00130365
Iteration 12/25 | Loss: 0.00131141
Iteration 13/25 | Loss: 0.00131113
Iteration 14/25 | Loss: 0.00131324
Iteration 15/25 | Loss: 0.00130672
Iteration 16/25 | Loss: 0.00130213
Iteration 17/25 | Loss: 0.00130106
Iteration 18/25 | Loss: 0.00130077
Iteration 19/25 | Loss: 0.00130067
Iteration 20/25 | Loss: 0.00130066
Iteration 21/25 | Loss: 0.00130066
Iteration 22/25 | Loss: 0.00130066
Iteration 23/25 | Loss: 0.00130066
Iteration 24/25 | Loss: 0.00130066
Iteration 25/25 | Loss: 0.00130065

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27908933
Iteration 2/25 | Loss: 0.00111955
Iteration 3/25 | Loss: 0.00111955
Iteration 4/25 | Loss: 0.00111955
Iteration 5/25 | Loss: 0.00111955
Iteration 6/25 | Loss: 0.00111954
Iteration 7/25 | Loss: 0.00111954
Iteration 8/25 | Loss: 0.00111954
Iteration 9/25 | Loss: 0.00111954
Iteration 10/25 | Loss: 0.00111954
Iteration 11/25 | Loss: 0.00111954
Iteration 12/25 | Loss: 0.00111954
Iteration 13/25 | Loss: 0.00111954
Iteration 14/25 | Loss: 0.00111954
Iteration 15/25 | Loss: 0.00111954
Iteration 16/25 | Loss: 0.00111954
Iteration 17/25 | Loss: 0.00111954
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011195435654371977, 0.0011195435654371977, 0.0011195435654371977, 0.0011195435654371977, 0.0011195435654371977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011195435654371977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111954
Iteration 2/1000 | Loss: 0.00396979
Iteration 3/1000 | Loss: 0.00098055
Iteration 4/1000 | Loss: 0.00056337
Iteration 5/1000 | Loss: 0.00103085
Iteration 6/1000 | Loss: 0.00049902
Iteration 7/1000 | Loss: 0.00030007
Iteration 8/1000 | Loss: 0.00074399
Iteration 9/1000 | Loss: 0.00095844
Iteration 10/1000 | Loss: 0.00022096
Iteration 11/1000 | Loss: 0.00040057
Iteration 12/1000 | Loss: 0.00047977
Iteration 13/1000 | Loss: 0.00091580
Iteration 14/1000 | Loss: 0.00091469
Iteration 15/1000 | Loss: 0.00058605
Iteration 16/1000 | Loss: 0.00014306
Iteration 17/1000 | Loss: 0.00045423
Iteration 18/1000 | Loss: 0.00010006
Iteration 19/1000 | Loss: 0.00009163
Iteration 20/1000 | Loss: 0.00025684
Iteration 21/1000 | Loss: 0.00058495
Iteration 22/1000 | Loss: 0.00067314
Iteration 23/1000 | Loss: 0.00030295
Iteration 24/1000 | Loss: 0.00058389
Iteration 25/1000 | Loss: 0.00016638
Iteration 26/1000 | Loss: 0.00010611
Iteration 27/1000 | Loss: 0.00039669
Iteration 28/1000 | Loss: 0.00020486
Iteration 29/1000 | Loss: 0.00010209
Iteration 30/1000 | Loss: 0.00009263
Iteration 31/1000 | Loss: 0.00067396
Iteration 32/1000 | Loss: 0.00070624
Iteration 33/1000 | Loss: 0.00013702
Iteration 34/1000 | Loss: 0.00009531
Iteration 35/1000 | Loss: 0.00012105
Iteration 36/1000 | Loss: 0.00018003
Iteration 37/1000 | Loss: 0.00009722
Iteration 38/1000 | Loss: 0.00010508
Iteration 39/1000 | Loss: 0.00011708
Iteration 40/1000 | Loss: 0.00020983
Iteration 41/1000 | Loss: 0.00012470
Iteration 42/1000 | Loss: 0.00014469
Iteration 43/1000 | Loss: 0.00029709
Iteration 44/1000 | Loss: 0.00018566
Iteration 45/1000 | Loss: 0.00010290
Iteration 46/1000 | Loss: 0.00013933
Iteration 47/1000 | Loss: 0.00008674
Iteration 48/1000 | Loss: 0.00016747
Iteration 49/1000 | Loss: 0.00016537
Iteration 50/1000 | Loss: 0.00009923
Iteration 51/1000 | Loss: 0.00007843
Iteration 52/1000 | Loss: 0.00018864
Iteration 53/1000 | Loss: 0.00031360
Iteration 54/1000 | Loss: 0.00022325
Iteration 55/1000 | Loss: 0.00012303
Iteration 56/1000 | Loss: 0.00017234
Iteration 57/1000 | Loss: 0.00014756
Iteration 58/1000 | Loss: 0.00027112
Iteration 59/1000 | Loss: 0.00016107
Iteration 60/1000 | Loss: 0.00011505
Iteration 61/1000 | Loss: 0.00013826
Iteration 62/1000 | Loss: 0.00010944
Iteration 63/1000 | Loss: 0.00012733
Iteration 64/1000 | Loss: 0.00016172
Iteration 65/1000 | Loss: 0.00008362
Iteration 66/1000 | Loss: 0.00025774
Iteration 67/1000 | Loss: 0.00011916
Iteration 68/1000 | Loss: 0.00012394
Iteration 69/1000 | Loss: 0.00012024
Iteration 70/1000 | Loss: 0.00006746
Iteration 71/1000 | Loss: 0.00007013
Iteration 72/1000 | Loss: 0.00006441
Iteration 73/1000 | Loss: 0.00018371
Iteration 74/1000 | Loss: 0.00039422
Iteration 75/1000 | Loss: 0.00038248
Iteration 76/1000 | Loss: 0.00034195
Iteration 77/1000 | Loss: 0.00028611
Iteration 78/1000 | Loss: 0.00015164
Iteration 79/1000 | Loss: 0.00008592
Iteration 80/1000 | Loss: 0.00009367
Iteration 81/1000 | Loss: 0.00010603
Iteration 82/1000 | Loss: 0.00007553
Iteration 83/1000 | Loss: 0.00008693
Iteration 84/1000 | Loss: 0.00007681
Iteration 85/1000 | Loss: 0.00006871
Iteration 86/1000 | Loss: 0.00007368
Iteration 87/1000 | Loss: 0.00028325
Iteration 88/1000 | Loss: 0.00010067
Iteration 89/1000 | Loss: 0.00007304
Iteration 90/1000 | Loss: 0.00006084
Iteration 91/1000 | Loss: 0.00011611
Iteration 92/1000 | Loss: 0.00019634
Iteration 93/1000 | Loss: 0.00012056
Iteration 94/1000 | Loss: 0.00005890
Iteration 95/1000 | Loss: 0.00005166
Iteration 96/1000 | Loss: 0.00005611
Iteration 97/1000 | Loss: 0.00004560
Iteration 98/1000 | Loss: 0.00003844
Iteration 99/1000 | Loss: 0.00003784
Iteration 100/1000 | Loss: 0.00004642
Iteration 101/1000 | Loss: 0.00004558
Iteration 102/1000 | Loss: 0.00003646
Iteration 103/1000 | Loss: 0.00004058
Iteration 104/1000 | Loss: 0.00004413
Iteration 105/1000 | Loss: 0.00004318
Iteration 106/1000 | Loss: 0.00003484
Iteration 107/1000 | Loss: 0.00003201
Iteration 108/1000 | Loss: 0.00003198
Iteration 109/1000 | Loss: 0.00004208
Iteration 110/1000 | Loss: 0.00003528
Iteration 111/1000 | Loss: 0.00003311
Iteration 112/1000 | Loss: 0.00003295
Iteration 113/1000 | Loss: 0.00004239
Iteration 114/1000 | Loss: 0.00004263
Iteration 115/1000 | Loss: 0.00005004
Iteration 116/1000 | Loss: 0.00003412
Iteration 117/1000 | Loss: 0.00005234
Iteration 118/1000 | Loss: 0.00004072
Iteration 119/1000 | Loss: 0.00003012
Iteration 120/1000 | Loss: 0.00004725
Iteration 121/1000 | Loss: 0.00004007
Iteration 122/1000 | Loss: 0.00004442
Iteration 123/1000 | Loss: 0.00017676
Iteration 124/1000 | Loss: 0.00008666
Iteration 125/1000 | Loss: 0.00004980
Iteration 126/1000 | Loss: 0.00003764
Iteration 127/1000 | Loss: 0.00017414
Iteration 128/1000 | Loss: 0.00008739
Iteration 129/1000 | Loss: 0.00005769
Iteration 130/1000 | Loss: 0.00003847
Iteration 131/1000 | Loss: 0.00004610
Iteration 132/1000 | Loss: 0.00004340
Iteration 133/1000 | Loss: 0.00004031
Iteration 134/1000 | Loss: 0.00003168
Iteration 135/1000 | Loss: 0.00003103
Iteration 136/1000 | Loss: 0.00040449
Iteration 137/1000 | Loss: 0.00006592
Iteration 138/1000 | Loss: 0.00005213
Iteration 139/1000 | Loss: 0.00006274
Iteration 140/1000 | Loss: 0.00005930
Iteration 141/1000 | Loss: 0.00005894
Iteration 142/1000 | Loss: 0.00005643
Iteration 143/1000 | Loss: 0.00004206
Iteration 144/1000 | Loss: 0.00004259
Iteration 145/1000 | Loss: 0.00004038
Iteration 146/1000 | Loss: 0.00004189
Iteration 147/1000 | Loss: 0.00003959
Iteration 148/1000 | Loss: 0.00004054
Iteration 149/1000 | Loss: 0.00003960
Iteration 150/1000 | Loss: 0.00003998
Iteration 151/1000 | Loss: 0.00003969
Iteration 152/1000 | Loss: 0.00003958
Iteration 153/1000 | Loss: 0.00005170
Iteration 154/1000 | Loss: 0.00003550
Iteration 155/1000 | Loss: 0.00004356
Iteration 156/1000 | Loss: 0.00004120
Iteration 157/1000 | Loss: 0.00003880
Iteration 158/1000 | Loss: 0.00003858
Iteration 159/1000 | Loss: 0.00003398
Iteration 160/1000 | Loss: 0.00003261
Iteration 161/1000 | Loss: 0.00002758
Iteration 162/1000 | Loss: 0.00002746
Iteration 163/1000 | Loss: 0.00002736
Iteration 164/1000 | Loss: 0.00002629
Iteration 165/1000 | Loss: 0.00002570
Iteration 166/1000 | Loss: 0.00002538
Iteration 167/1000 | Loss: 0.00002506
Iteration 168/1000 | Loss: 0.00002486
Iteration 169/1000 | Loss: 0.00002478
Iteration 170/1000 | Loss: 0.00002460
Iteration 171/1000 | Loss: 0.00002457
Iteration 172/1000 | Loss: 0.00002456
Iteration 173/1000 | Loss: 0.00002454
Iteration 174/1000 | Loss: 0.00002437
Iteration 175/1000 | Loss: 0.00002419
Iteration 176/1000 | Loss: 0.00002406
Iteration 177/1000 | Loss: 0.00002402
Iteration 178/1000 | Loss: 0.00002393
Iteration 179/1000 | Loss: 0.00002390
Iteration 180/1000 | Loss: 0.00002388
Iteration 181/1000 | Loss: 0.00002386
Iteration 182/1000 | Loss: 0.00002385
Iteration 183/1000 | Loss: 0.00002385
Iteration 184/1000 | Loss: 0.00002382
Iteration 185/1000 | Loss: 0.00002380
Iteration 186/1000 | Loss: 0.00002378
Iteration 187/1000 | Loss: 0.00002377
Iteration 188/1000 | Loss: 0.00002377
Iteration 189/1000 | Loss: 0.00002377
Iteration 190/1000 | Loss: 0.00002377
Iteration 191/1000 | Loss: 0.00002377
Iteration 192/1000 | Loss: 0.00002377
Iteration 193/1000 | Loss: 0.00002377
Iteration 194/1000 | Loss: 0.00002377
Iteration 195/1000 | Loss: 0.00002377
Iteration 196/1000 | Loss: 0.00002377
Iteration 197/1000 | Loss: 0.00002377
Iteration 198/1000 | Loss: 0.00002377
Iteration 199/1000 | Loss: 0.00002377
Iteration 200/1000 | Loss: 0.00002377
Iteration 201/1000 | Loss: 0.00002377
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [2.3767252059769817e-05, 2.3767252059769817e-05, 2.3767252059769817e-05, 2.3767252059769817e-05, 2.3767252059769817e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3767252059769817e-05

Optimization complete. Final v2v error: 3.897554874420166 mm

Highest mean error: 5.857757091522217 mm for frame 147

Lowest mean error: 3.2390096187591553 mm for frame 151

Saving results

Total time: 322.8337593078613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00736708
Iteration 2/25 | Loss: 0.00133607
Iteration 3/25 | Loss: 0.00125797
Iteration 4/25 | Loss: 0.00124710
Iteration 5/25 | Loss: 0.00124447
Iteration 6/25 | Loss: 0.00124387
Iteration 7/25 | Loss: 0.00124387
Iteration 8/25 | Loss: 0.00124387
Iteration 9/25 | Loss: 0.00124387
Iteration 10/25 | Loss: 0.00124387
Iteration 11/25 | Loss: 0.00124387
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012438726844266057, 0.0012438726844266057, 0.0012438726844266057, 0.0012438726844266057, 0.0012438726844266057]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012438726844266057

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31279480
Iteration 2/25 | Loss: 0.00100746
Iteration 3/25 | Loss: 0.00100738
Iteration 4/25 | Loss: 0.00100738
Iteration 5/25 | Loss: 0.00100738
Iteration 6/25 | Loss: 0.00100738
Iteration 7/25 | Loss: 0.00100738
Iteration 8/25 | Loss: 0.00100738
Iteration 9/25 | Loss: 0.00100738
Iteration 10/25 | Loss: 0.00100738
Iteration 11/25 | Loss: 0.00100738
Iteration 12/25 | Loss: 0.00100738
Iteration 13/25 | Loss: 0.00100738
Iteration 14/25 | Loss: 0.00100738
Iteration 15/25 | Loss: 0.00100738
Iteration 16/25 | Loss: 0.00100738
Iteration 17/25 | Loss: 0.00100738
Iteration 18/25 | Loss: 0.00100738
Iteration 19/25 | Loss: 0.00100738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0010073769371956587, 0.0010073769371956587, 0.0010073769371956587, 0.0010073769371956587, 0.0010073769371956587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010073769371956587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100738
Iteration 2/1000 | Loss: 0.00004007
Iteration 3/1000 | Loss: 0.00002761
Iteration 4/1000 | Loss: 0.00002373
Iteration 5/1000 | Loss: 0.00002254
Iteration 6/1000 | Loss: 0.00002119
Iteration 7/1000 | Loss: 0.00002055
Iteration 8/1000 | Loss: 0.00001995
Iteration 9/1000 | Loss: 0.00001965
Iteration 10/1000 | Loss: 0.00001930
Iteration 11/1000 | Loss: 0.00001909
Iteration 12/1000 | Loss: 0.00001887
Iteration 13/1000 | Loss: 0.00001886
Iteration 14/1000 | Loss: 0.00001861
Iteration 15/1000 | Loss: 0.00001842
Iteration 16/1000 | Loss: 0.00001816
Iteration 17/1000 | Loss: 0.00001801
Iteration 18/1000 | Loss: 0.00001800
Iteration 19/1000 | Loss: 0.00001799
Iteration 20/1000 | Loss: 0.00001797
Iteration 21/1000 | Loss: 0.00001797
Iteration 22/1000 | Loss: 0.00001796
Iteration 23/1000 | Loss: 0.00001795
Iteration 24/1000 | Loss: 0.00001795
Iteration 25/1000 | Loss: 0.00001794
Iteration 26/1000 | Loss: 0.00001794
Iteration 27/1000 | Loss: 0.00001793
Iteration 28/1000 | Loss: 0.00001793
Iteration 29/1000 | Loss: 0.00001792
Iteration 30/1000 | Loss: 0.00001791
Iteration 31/1000 | Loss: 0.00001791
Iteration 32/1000 | Loss: 0.00001791
Iteration 33/1000 | Loss: 0.00001790
Iteration 34/1000 | Loss: 0.00001789
Iteration 35/1000 | Loss: 0.00001789
Iteration 36/1000 | Loss: 0.00001789
Iteration 37/1000 | Loss: 0.00001789
Iteration 38/1000 | Loss: 0.00001789
Iteration 39/1000 | Loss: 0.00001789
Iteration 40/1000 | Loss: 0.00001788
Iteration 41/1000 | Loss: 0.00001788
Iteration 42/1000 | Loss: 0.00001787
Iteration 43/1000 | Loss: 0.00001787
Iteration 44/1000 | Loss: 0.00001786
Iteration 45/1000 | Loss: 0.00001786
Iteration 46/1000 | Loss: 0.00001786
Iteration 47/1000 | Loss: 0.00001785
Iteration 48/1000 | Loss: 0.00001785
Iteration 49/1000 | Loss: 0.00001785
Iteration 50/1000 | Loss: 0.00001784
Iteration 51/1000 | Loss: 0.00001784
Iteration 52/1000 | Loss: 0.00001783
Iteration 53/1000 | Loss: 0.00001783
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001781
Iteration 57/1000 | Loss: 0.00001781
Iteration 58/1000 | Loss: 0.00001781
Iteration 59/1000 | Loss: 0.00001780
Iteration 60/1000 | Loss: 0.00001780
Iteration 61/1000 | Loss: 0.00001780
Iteration 62/1000 | Loss: 0.00001779
Iteration 63/1000 | Loss: 0.00001778
Iteration 64/1000 | Loss: 0.00001778
Iteration 65/1000 | Loss: 0.00001777
Iteration 66/1000 | Loss: 0.00001777
Iteration 67/1000 | Loss: 0.00001777
Iteration 68/1000 | Loss: 0.00001777
Iteration 69/1000 | Loss: 0.00001777
Iteration 70/1000 | Loss: 0.00001777
Iteration 71/1000 | Loss: 0.00001777
Iteration 72/1000 | Loss: 0.00001777
Iteration 73/1000 | Loss: 0.00001777
Iteration 74/1000 | Loss: 0.00001777
Iteration 75/1000 | Loss: 0.00001776
Iteration 76/1000 | Loss: 0.00001776
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001774
Iteration 79/1000 | Loss: 0.00001774
Iteration 80/1000 | Loss: 0.00001774
Iteration 81/1000 | Loss: 0.00001774
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001773
Iteration 86/1000 | Loss: 0.00001773
Iteration 87/1000 | Loss: 0.00001772
Iteration 88/1000 | Loss: 0.00001772
Iteration 89/1000 | Loss: 0.00001772
Iteration 90/1000 | Loss: 0.00001771
Iteration 91/1000 | Loss: 0.00001771
Iteration 92/1000 | Loss: 0.00001770
Iteration 93/1000 | Loss: 0.00001770
Iteration 94/1000 | Loss: 0.00001770
Iteration 95/1000 | Loss: 0.00001770
Iteration 96/1000 | Loss: 0.00001770
Iteration 97/1000 | Loss: 0.00001770
Iteration 98/1000 | Loss: 0.00001770
Iteration 99/1000 | Loss: 0.00001769
Iteration 100/1000 | Loss: 0.00001769
Iteration 101/1000 | Loss: 0.00001769
Iteration 102/1000 | Loss: 0.00001769
Iteration 103/1000 | Loss: 0.00001768
Iteration 104/1000 | Loss: 0.00001768
Iteration 105/1000 | Loss: 0.00001768
Iteration 106/1000 | Loss: 0.00001767
Iteration 107/1000 | Loss: 0.00001767
Iteration 108/1000 | Loss: 0.00001767
Iteration 109/1000 | Loss: 0.00001767
Iteration 110/1000 | Loss: 0.00001767
Iteration 111/1000 | Loss: 0.00001766
Iteration 112/1000 | Loss: 0.00001766
Iteration 113/1000 | Loss: 0.00001766
Iteration 114/1000 | Loss: 0.00001765
Iteration 115/1000 | Loss: 0.00001765
Iteration 116/1000 | Loss: 0.00001765
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001765
Iteration 119/1000 | Loss: 0.00001764
Iteration 120/1000 | Loss: 0.00001764
Iteration 121/1000 | Loss: 0.00001764
Iteration 122/1000 | Loss: 0.00001762
Iteration 123/1000 | Loss: 0.00001762
Iteration 124/1000 | Loss: 0.00001762
Iteration 125/1000 | Loss: 0.00001762
Iteration 126/1000 | Loss: 0.00001761
Iteration 127/1000 | Loss: 0.00001761
Iteration 128/1000 | Loss: 0.00001761
Iteration 129/1000 | Loss: 0.00001761
Iteration 130/1000 | Loss: 0.00001761
Iteration 131/1000 | Loss: 0.00001761
Iteration 132/1000 | Loss: 0.00001761
Iteration 133/1000 | Loss: 0.00001761
Iteration 134/1000 | Loss: 0.00001760
Iteration 135/1000 | Loss: 0.00001760
Iteration 136/1000 | Loss: 0.00001760
Iteration 137/1000 | Loss: 0.00001760
Iteration 138/1000 | Loss: 0.00001760
Iteration 139/1000 | Loss: 0.00001760
Iteration 140/1000 | Loss: 0.00001760
Iteration 141/1000 | Loss: 0.00001760
Iteration 142/1000 | Loss: 0.00001760
Iteration 143/1000 | Loss: 0.00001759
Iteration 144/1000 | Loss: 0.00001759
Iteration 145/1000 | Loss: 0.00001759
Iteration 146/1000 | Loss: 0.00001759
Iteration 147/1000 | Loss: 0.00001759
Iteration 148/1000 | Loss: 0.00001758
Iteration 149/1000 | Loss: 0.00001758
Iteration 150/1000 | Loss: 0.00001758
Iteration 151/1000 | Loss: 0.00001758
Iteration 152/1000 | Loss: 0.00001758
Iteration 153/1000 | Loss: 0.00001758
Iteration 154/1000 | Loss: 0.00001758
Iteration 155/1000 | Loss: 0.00001758
Iteration 156/1000 | Loss: 0.00001758
Iteration 157/1000 | Loss: 0.00001758
Iteration 158/1000 | Loss: 0.00001758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [1.7583197404746898e-05, 1.7583197404746898e-05, 1.7583197404746898e-05, 1.7583197404746898e-05, 1.7583197404746898e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7583197404746898e-05

Optimization complete. Final v2v error: 3.5281383991241455 mm

Highest mean error: 3.7476348876953125 mm for frame 41

Lowest mean error: 3.4035301208496094 mm for frame 98

Saving results

Total time: 42.55249333381653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1048/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1048.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1048
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01036823
Iteration 2/25 | Loss: 0.00231567
Iteration 3/25 | Loss: 0.00160450
Iteration 4/25 | Loss: 0.00156552
Iteration 5/25 | Loss: 0.00155153
Iteration 6/25 | Loss: 0.00142918
Iteration 7/25 | Loss: 0.00142774
Iteration 8/25 | Loss: 0.00145415
Iteration 9/25 | Loss: 0.00133916
Iteration 10/25 | Loss: 0.00133143
Iteration 11/25 | Loss: 0.00129013
Iteration 12/25 | Loss: 0.00128797
Iteration 13/25 | Loss: 0.00126110
Iteration 14/25 | Loss: 0.00126071
Iteration 15/25 | Loss: 0.00126067
Iteration 16/25 | Loss: 0.00126067
Iteration 17/25 | Loss: 0.00126067
Iteration 18/25 | Loss: 0.00126067
Iteration 19/25 | Loss: 0.00126067
Iteration 20/25 | Loss: 0.00126066
Iteration 21/25 | Loss: 0.00126066
Iteration 22/25 | Loss: 0.00126063
Iteration 23/25 | Loss: 0.00126063
Iteration 24/25 | Loss: 0.00126063
Iteration 25/25 | Loss: 0.00126062

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30547154
Iteration 2/25 | Loss: 0.00183481
Iteration 3/25 | Loss: 0.00129469
Iteration 4/25 | Loss: 0.00129469
Iteration 5/25 | Loss: 0.00129468
Iteration 6/25 | Loss: 0.00129468
Iteration 7/25 | Loss: 0.00129468
Iteration 8/25 | Loss: 0.00129468
Iteration 9/25 | Loss: 0.00129468
Iteration 10/25 | Loss: 0.00129468
Iteration 11/25 | Loss: 0.00129468
Iteration 12/25 | Loss: 0.00129468
Iteration 13/25 | Loss: 0.00129468
Iteration 14/25 | Loss: 0.00129468
Iteration 15/25 | Loss: 0.00129468
Iteration 16/25 | Loss: 0.00129468
Iteration 17/25 | Loss: 0.00129468
Iteration 18/25 | Loss: 0.00129468
Iteration 19/25 | Loss: 0.00129468
Iteration 20/25 | Loss: 0.00129468
Iteration 21/25 | Loss: 0.00129468
Iteration 22/25 | Loss: 0.00129468
Iteration 23/25 | Loss: 0.00129468
Iteration 24/25 | Loss: 0.00129468
Iteration 25/25 | Loss: 0.00129468

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129468
Iteration 2/1000 | Loss: 0.00021793
Iteration 3/1000 | Loss: 0.00002576
Iteration 4/1000 | Loss: 0.00002331
Iteration 5/1000 | Loss: 0.00002232
Iteration 6/1000 | Loss: 0.00002161
Iteration 7/1000 | Loss: 0.00002111
Iteration 8/1000 | Loss: 0.00002074
Iteration 9/1000 | Loss: 0.00002038
Iteration 10/1000 | Loss: 0.00002016
Iteration 11/1000 | Loss: 0.00001990
Iteration 12/1000 | Loss: 0.00028078
Iteration 13/1000 | Loss: 0.00037220
Iteration 14/1000 | Loss: 0.00066047
Iteration 15/1000 | Loss: 0.00049333
Iteration 16/1000 | Loss: 0.00021015
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00001988
Iteration 19/1000 | Loss: 0.00026942
Iteration 20/1000 | Loss: 0.00002151
Iteration 21/1000 | Loss: 0.00001963
Iteration 22/1000 | Loss: 0.00001910
Iteration 23/1000 | Loss: 0.00001851
Iteration 24/1000 | Loss: 0.00001822
Iteration 25/1000 | Loss: 0.00001818
Iteration 26/1000 | Loss: 0.00001813
Iteration 27/1000 | Loss: 0.00001813
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001812
Iteration 30/1000 | Loss: 0.00001811
Iteration 31/1000 | Loss: 0.00001806
Iteration 32/1000 | Loss: 0.00001805
Iteration 33/1000 | Loss: 0.00001805
Iteration 34/1000 | Loss: 0.00001800
Iteration 35/1000 | Loss: 0.00001798
Iteration 36/1000 | Loss: 0.00001797
Iteration 37/1000 | Loss: 0.00001794
Iteration 38/1000 | Loss: 0.00001794
Iteration 39/1000 | Loss: 0.00001794
Iteration 40/1000 | Loss: 0.00001794
Iteration 41/1000 | Loss: 0.00001793
Iteration 42/1000 | Loss: 0.00001793
Iteration 43/1000 | Loss: 0.00001793
Iteration 44/1000 | Loss: 0.00001793
Iteration 45/1000 | Loss: 0.00001793
Iteration 46/1000 | Loss: 0.00001791
Iteration 47/1000 | Loss: 0.00001790
Iteration 48/1000 | Loss: 0.00001790
Iteration 49/1000 | Loss: 0.00001789
Iteration 50/1000 | Loss: 0.00001787
Iteration 51/1000 | Loss: 0.00001785
Iteration 52/1000 | Loss: 0.00001785
Iteration 53/1000 | Loss: 0.00001782
Iteration 54/1000 | Loss: 0.00001782
Iteration 55/1000 | Loss: 0.00001781
Iteration 56/1000 | Loss: 0.00001780
Iteration 57/1000 | Loss: 0.00001779
Iteration 58/1000 | Loss: 0.00001779
Iteration 59/1000 | Loss: 0.00001777
Iteration 60/1000 | Loss: 0.00001777
Iteration 61/1000 | Loss: 0.00001777
Iteration 62/1000 | Loss: 0.00001776
Iteration 63/1000 | Loss: 0.00001776
Iteration 64/1000 | Loss: 0.00001776
Iteration 65/1000 | Loss: 0.00001776
Iteration 66/1000 | Loss: 0.00001776
Iteration 67/1000 | Loss: 0.00001776
Iteration 68/1000 | Loss: 0.00001776
Iteration 69/1000 | Loss: 0.00001775
Iteration 70/1000 | Loss: 0.00001775
Iteration 71/1000 | Loss: 0.00001775
Iteration 72/1000 | Loss: 0.00001775
Iteration 73/1000 | Loss: 0.00001775
Iteration 74/1000 | Loss: 0.00001775
Iteration 75/1000 | Loss: 0.00001775
Iteration 76/1000 | Loss: 0.00001775
Iteration 77/1000 | Loss: 0.00001775
Iteration 78/1000 | Loss: 0.00001775
Iteration 79/1000 | Loss: 0.00001774
Iteration 80/1000 | Loss: 0.00001774
Iteration 81/1000 | Loss: 0.00001774
Iteration 82/1000 | Loss: 0.00001774
Iteration 83/1000 | Loss: 0.00001774
Iteration 84/1000 | Loss: 0.00001774
Iteration 85/1000 | Loss: 0.00001774
Iteration 86/1000 | Loss: 0.00001774
Iteration 87/1000 | Loss: 0.00001774
Iteration 88/1000 | Loss: 0.00001774
Iteration 89/1000 | Loss: 0.00001774
Iteration 90/1000 | Loss: 0.00001774
Iteration 91/1000 | Loss: 0.00001774
Iteration 92/1000 | Loss: 0.00001773
Iteration 93/1000 | Loss: 0.00001773
Iteration 94/1000 | Loss: 0.00001773
Iteration 95/1000 | Loss: 0.00001773
Iteration 96/1000 | Loss: 0.00001773
Iteration 97/1000 | Loss: 0.00001773
Iteration 98/1000 | Loss: 0.00001773
Iteration 99/1000 | Loss: 0.00001773
Iteration 100/1000 | Loss: 0.00001773
Iteration 101/1000 | Loss: 0.00001773
Iteration 102/1000 | Loss: 0.00001773
Iteration 103/1000 | Loss: 0.00001773
Iteration 104/1000 | Loss: 0.00001773
Iteration 105/1000 | Loss: 0.00001773
Iteration 106/1000 | Loss: 0.00001773
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.77298115886515e-05, 1.77298115886515e-05, 1.77298115886515e-05, 1.77298115886515e-05, 1.77298115886515e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.77298115886515e-05

Optimization complete. Final v2v error: 3.6021370887756348 mm

Highest mean error: 4.657680511474609 mm for frame 67

Lowest mean error: 3.4710237979888916 mm for frame 167

Saving results

Total time: 81.92015480995178
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00582294
Iteration 2/25 | Loss: 0.00161649
Iteration 3/25 | Loss: 0.00129109
Iteration 4/25 | Loss: 0.00126221
Iteration 5/25 | Loss: 0.00125755
Iteration 6/25 | Loss: 0.00125661
Iteration 7/25 | Loss: 0.00125661
Iteration 8/25 | Loss: 0.00125661
Iteration 9/25 | Loss: 0.00125661
Iteration 10/25 | Loss: 0.00125661
Iteration 11/25 | Loss: 0.00125661
Iteration 12/25 | Loss: 0.00125661
Iteration 13/25 | Loss: 0.00125661
Iteration 14/25 | Loss: 0.00125661
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012566139921545982, 0.0012566139921545982, 0.0012566139921545982, 0.0012566139921545982, 0.0012566139921545982]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012566139921545982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17328346
Iteration 2/25 | Loss: 0.00091323
Iteration 3/25 | Loss: 0.00091323
Iteration 4/25 | Loss: 0.00091323
Iteration 5/25 | Loss: 0.00091323
Iteration 6/25 | Loss: 0.00091323
Iteration 7/25 | Loss: 0.00091323
Iteration 8/25 | Loss: 0.00091323
Iteration 9/25 | Loss: 0.00091323
Iteration 10/25 | Loss: 0.00091323
Iteration 11/25 | Loss: 0.00091323
Iteration 12/25 | Loss: 0.00091323
Iteration 13/25 | Loss: 0.00091323
Iteration 14/25 | Loss: 0.00091323
Iteration 15/25 | Loss: 0.00091323
Iteration 16/25 | Loss: 0.00091323
Iteration 17/25 | Loss: 0.00091323
Iteration 18/25 | Loss: 0.00091323
Iteration 19/25 | Loss: 0.00091323
Iteration 20/25 | Loss: 0.00091323
Iteration 21/25 | Loss: 0.00091323
Iteration 22/25 | Loss: 0.00091323
Iteration 23/25 | Loss: 0.00091323
Iteration 24/25 | Loss: 0.00091323
Iteration 25/25 | Loss: 0.00091323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091323
Iteration 2/1000 | Loss: 0.00003257
Iteration 3/1000 | Loss: 0.00002143
Iteration 4/1000 | Loss: 0.00001768
Iteration 5/1000 | Loss: 0.00001661
Iteration 6/1000 | Loss: 0.00001587
Iteration 7/1000 | Loss: 0.00001538
Iteration 8/1000 | Loss: 0.00001502
Iteration 9/1000 | Loss: 0.00001476
Iteration 10/1000 | Loss: 0.00001457
Iteration 11/1000 | Loss: 0.00001437
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001423
Iteration 14/1000 | Loss: 0.00001415
Iteration 15/1000 | Loss: 0.00001415
Iteration 16/1000 | Loss: 0.00001413
Iteration 17/1000 | Loss: 0.00001408
Iteration 18/1000 | Loss: 0.00001403
Iteration 19/1000 | Loss: 0.00001398
Iteration 20/1000 | Loss: 0.00001398
Iteration 21/1000 | Loss: 0.00001397
Iteration 22/1000 | Loss: 0.00001396
Iteration 23/1000 | Loss: 0.00001396
Iteration 24/1000 | Loss: 0.00001395
Iteration 25/1000 | Loss: 0.00001394
Iteration 26/1000 | Loss: 0.00001393
Iteration 27/1000 | Loss: 0.00001386
Iteration 28/1000 | Loss: 0.00001385
Iteration 29/1000 | Loss: 0.00001384
Iteration 30/1000 | Loss: 0.00001384
Iteration 31/1000 | Loss: 0.00001383
Iteration 32/1000 | Loss: 0.00001383
Iteration 33/1000 | Loss: 0.00001381
Iteration 34/1000 | Loss: 0.00001380
Iteration 35/1000 | Loss: 0.00001380
Iteration 36/1000 | Loss: 0.00001380
Iteration 37/1000 | Loss: 0.00001379
Iteration 38/1000 | Loss: 0.00001378
Iteration 39/1000 | Loss: 0.00001378
Iteration 40/1000 | Loss: 0.00001378
Iteration 41/1000 | Loss: 0.00001377
Iteration 42/1000 | Loss: 0.00001374
Iteration 43/1000 | Loss: 0.00001373
Iteration 44/1000 | Loss: 0.00001373
Iteration 45/1000 | Loss: 0.00001372
Iteration 46/1000 | Loss: 0.00001368
Iteration 47/1000 | Loss: 0.00001366
Iteration 48/1000 | Loss: 0.00001365
Iteration 49/1000 | Loss: 0.00001365
Iteration 50/1000 | Loss: 0.00001365
Iteration 51/1000 | Loss: 0.00001365
Iteration 52/1000 | Loss: 0.00001365
Iteration 53/1000 | Loss: 0.00001365
Iteration 54/1000 | Loss: 0.00001365
Iteration 55/1000 | Loss: 0.00001365
Iteration 56/1000 | Loss: 0.00001365
Iteration 57/1000 | Loss: 0.00001365
Iteration 58/1000 | Loss: 0.00001364
Iteration 59/1000 | Loss: 0.00001364
Iteration 60/1000 | Loss: 0.00001364
Iteration 61/1000 | Loss: 0.00001364
Iteration 62/1000 | Loss: 0.00001364
Iteration 63/1000 | Loss: 0.00001364
Iteration 64/1000 | Loss: 0.00001364
Iteration 65/1000 | Loss: 0.00001364
Iteration 66/1000 | Loss: 0.00001363
Iteration 67/1000 | Loss: 0.00001363
Iteration 68/1000 | Loss: 0.00001363
Iteration 69/1000 | Loss: 0.00001363
Iteration 70/1000 | Loss: 0.00001363
Iteration 71/1000 | Loss: 0.00001363
Iteration 72/1000 | Loss: 0.00001362
Iteration 73/1000 | Loss: 0.00001362
Iteration 74/1000 | Loss: 0.00001362
Iteration 75/1000 | Loss: 0.00001362
Iteration 76/1000 | Loss: 0.00001362
Iteration 77/1000 | Loss: 0.00001362
Iteration 78/1000 | Loss: 0.00001362
Iteration 79/1000 | Loss: 0.00001361
Iteration 80/1000 | Loss: 0.00001361
Iteration 81/1000 | Loss: 0.00001361
Iteration 82/1000 | Loss: 0.00001360
Iteration 83/1000 | Loss: 0.00001360
Iteration 84/1000 | Loss: 0.00001360
Iteration 85/1000 | Loss: 0.00001360
Iteration 86/1000 | Loss: 0.00001360
Iteration 87/1000 | Loss: 0.00001360
Iteration 88/1000 | Loss: 0.00001359
Iteration 89/1000 | Loss: 0.00001359
Iteration 90/1000 | Loss: 0.00001359
Iteration 91/1000 | Loss: 0.00001359
Iteration 92/1000 | Loss: 0.00001359
Iteration 93/1000 | Loss: 0.00001359
Iteration 94/1000 | Loss: 0.00001359
Iteration 95/1000 | Loss: 0.00001358
Iteration 96/1000 | Loss: 0.00001358
Iteration 97/1000 | Loss: 0.00001358
Iteration 98/1000 | Loss: 0.00001358
Iteration 99/1000 | Loss: 0.00001358
Iteration 100/1000 | Loss: 0.00001358
Iteration 101/1000 | Loss: 0.00001358
Iteration 102/1000 | Loss: 0.00001358
Iteration 103/1000 | Loss: 0.00001358
Iteration 104/1000 | Loss: 0.00001358
Iteration 105/1000 | Loss: 0.00001358
Iteration 106/1000 | Loss: 0.00001357
Iteration 107/1000 | Loss: 0.00001357
Iteration 108/1000 | Loss: 0.00001357
Iteration 109/1000 | Loss: 0.00001356
Iteration 110/1000 | Loss: 0.00001356
Iteration 111/1000 | Loss: 0.00001356
Iteration 112/1000 | Loss: 0.00001356
Iteration 113/1000 | Loss: 0.00001356
Iteration 114/1000 | Loss: 0.00001355
Iteration 115/1000 | Loss: 0.00001355
Iteration 116/1000 | Loss: 0.00001355
Iteration 117/1000 | Loss: 0.00001355
Iteration 118/1000 | Loss: 0.00001355
Iteration 119/1000 | Loss: 0.00001355
Iteration 120/1000 | Loss: 0.00001354
Iteration 121/1000 | Loss: 0.00001354
Iteration 122/1000 | Loss: 0.00001354
Iteration 123/1000 | Loss: 0.00001354
Iteration 124/1000 | Loss: 0.00001354
Iteration 125/1000 | Loss: 0.00001354
Iteration 126/1000 | Loss: 0.00001354
Iteration 127/1000 | Loss: 0.00001354
Iteration 128/1000 | Loss: 0.00001354
Iteration 129/1000 | Loss: 0.00001354
Iteration 130/1000 | Loss: 0.00001354
Iteration 131/1000 | Loss: 0.00001354
Iteration 132/1000 | Loss: 0.00001354
Iteration 133/1000 | Loss: 0.00001354
Iteration 134/1000 | Loss: 0.00001354
Iteration 135/1000 | Loss: 0.00001354
Iteration 136/1000 | Loss: 0.00001354
Iteration 137/1000 | Loss: 0.00001354
Iteration 138/1000 | Loss: 0.00001353
Iteration 139/1000 | Loss: 0.00001353
Iteration 140/1000 | Loss: 0.00001353
Iteration 141/1000 | Loss: 0.00001353
Iteration 142/1000 | Loss: 0.00001352
Iteration 143/1000 | Loss: 0.00001352
Iteration 144/1000 | Loss: 0.00001352
Iteration 145/1000 | Loss: 0.00001352
Iteration 146/1000 | Loss: 0.00001352
Iteration 147/1000 | Loss: 0.00001352
Iteration 148/1000 | Loss: 0.00001352
Iteration 149/1000 | Loss: 0.00001351
Iteration 150/1000 | Loss: 0.00001351
Iteration 151/1000 | Loss: 0.00001351
Iteration 152/1000 | Loss: 0.00001351
Iteration 153/1000 | Loss: 0.00001351
Iteration 154/1000 | Loss: 0.00001351
Iteration 155/1000 | Loss: 0.00001351
Iteration 156/1000 | Loss: 0.00001351
Iteration 157/1000 | Loss: 0.00001351
Iteration 158/1000 | Loss: 0.00001350
Iteration 159/1000 | Loss: 0.00001350
Iteration 160/1000 | Loss: 0.00001350
Iteration 161/1000 | Loss: 0.00001350
Iteration 162/1000 | Loss: 0.00001350
Iteration 163/1000 | Loss: 0.00001350
Iteration 164/1000 | Loss: 0.00001350
Iteration 165/1000 | Loss: 0.00001350
Iteration 166/1000 | Loss: 0.00001350
Iteration 167/1000 | Loss: 0.00001350
Iteration 168/1000 | Loss: 0.00001350
Iteration 169/1000 | Loss: 0.00001349
Iteration 170/1000 | Loss: 0.00001349
Iteration 171/1000 | Loss: 0.00001349
Iteration 172/1000 | Loss: 0.00001349
Iteration 173/1000 | Loss: 0.00001349
Iteration 174/1000 | Loss: 0.00001349
Iteration 175/1000 | Loss: 0.00001349
Iteration 176/1000 | Loss: 0.00001349
Iteration 177/1000 | Loss: 0.00001349
Iteration 178/1000 | Loss: 0.00001349
Iteration 179/1000 | Loss: 0.00001349
Iteration 180/1000 | Loss: 0.00001349
Iteration 181/1000 | Loss: 0.00001349
Iteration 182/1000 | Loss: 0.00001349
Iteration 183/1000 | Loss: 0.00001349
Iteration 184/1000 | Loss: 0.00001349
Iteration 185/1000 | Loss: 0.00001349
Iteration 186/1000 | Loss: 0.00001349
Iteration 187/1000 | Loss: 0.00001349
Iteration 188/1000 | Loss: 0.00001349
Iteration 189/1000 | Loss: 0.00001349
Iteration 190/1000 | Loss: 0.00001349
Iteration 191/1000 | Loss: 0.00001349
Iteration 192/1000 | Loss: 0.00001349
Iteration 193/1000 | Loss: 0.00001349
Iteration 194/1000 | Loss: 0.00001349
Iteration 195/1000 | Loss: 0.00001349
Iteration 196/1000 | Loss: 0.00001349
Iteration 197/1000 | Loss: 0.00001349
Iteration 198/1000 | Loss: 0.00001349
Iteration 199/1000 | Loss: 0.00001349
Iteration 200/1000 | Loss: 0.00001349
Iteration 201/1000 | Loss: 0.00001349
Iteration 202/1000 | Loss: 0.00001349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 202. Stopping optimization.
Last 5 losses: [1.348674140899675e-05, 1.348674140899675e-05, 1.348674140899675e-05, 1.348674140899675e-05, 1.348674140899675e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.348674140899675e-05

Optimization complete. Final v2v error: 3.0865049362182617 mm

Highest mean error: 3.4685208797454834 mm for frame 79

Lowest mean error: 2.9642930030822754 mm for frame 168

Saving results

Total time: 43.09773349761963
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00834185
Iteration 2/25 | Loss: 0.00156075
Iteration 3/25 | Loss: 0.00137598
Iteration 4/25 | Loss: 0.00135559
Iteration 5/25 | Loss: 0.00135297
Iteration 6/25 | Loss: 0.00135297
Iteration 7/25 | Loss: 0.00135297
Iteration 8/25 | Loss: 0.00135297
Iteration 9/25 | Loss: 0.00135297
Iteration 10/25 | Loss: 0.00135297
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0013529695570468903, 0.0013529695570468903, 0.0013529695570468903, 0.0013529695570468903, 0.0013529695570468903]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013529695570468903

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94824517
Iteration 2/25 | Loss: 0.00080269
Iteration 3/25 | Loss: 0.00080269
Iteration 4/25 | Loss: 0.00080269
Iteration 5/25 | Loss: 0.00080268
Iteration 6/25 | Loss: 0.00080268
Iteration 7/25 | Loss: 0.00080268
Iteration 8/25 | Loss: 0.00080268
Iteration 9/25 | Loss: 0.00080268
Iteration 10/25 | Loss: 0.00080268
Iteration 11/25 | Loss: 0.00080268
Iteration 12/25 | Loss: 0.00080268
Iteration 13/25 | Loss: 0.00080268
Iteration 14/25 | Loss: 0.00080268
Iteration 15/25 | Loss: 0.00080268
Iteration 16/25 | Loss: 0.00080268
Iteration 17/25 | Loss: 0.00080268
Iteration 18/25 | Loss: 0.00080268
Iteration 19/25 | Loss: 0.00080268
Iteration 20/25 | Loss: 0.00080268
Iteration 21/25 | Loss: 0.00080268
Iteration 22/25 | Loss: 0.00080268
Iteration 23/25 | Loss: 0.00080268
Iteration 24/25 | Loss: 0.00080268
Iteration 25/25 | Loss: 0.00080268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00080268
Iteration 2/1000 | Loss: 0.00004086
Iteration 3/1000 | Loss: 0.00003078
Iteration 4/1000 | Loss: 0.00002751
Iteration 5/1000 | Loss: 0.00002605
Iteration 6/1000 | Loss: 0.00002550
Iteration 7/1000 | Loss: 0.00002478
Iteration 8/1000 | Loss: 0.00002429
Iteration 9/1000 | Loss: 0.00002415
Iteration 10/1000 | Loss: 0.00002370
Iteration 11/1000 | Loss: 0.00002342
Iteration 12/1000 | Loss: 0.00002317
Iteration 13/1000 | Loss: 0.00002285
Iteration 14/1000 | Loss: 0.00002283
Iteration 15/1000 | Loss: 0.00002273
Iteration 16/1000 | Loss: 0.00002261
Iteration 17/1000 | Loss: 0.00002261
Iteration 18/1000 | Loss: 0.00002260
Iteration 19/1000 | Loss: 0.00002253
Iteration 20/1000 | Loss: 0.00002252
Iteration 21/1000 | Loss: 0.00002239
Iteration 22/1000 | Loss: 0.00002239
Iteration 23/1000 | Loss: 0.00002239
Iteration 24/1000 | Loss: 0.00002239
Iteration 25/1000 | Loss: 0.00002239
Iteration 26/1000 | Loss: 0.00002239
Iteration 27/1000 | Loss: 0.00002239
Iteration 28/1000 | Loss: 0.00002239
Iteration 29/1000 | Loss: 0.00002239
Iteration 30/1000 | Loss: 0.00002239
Iteration 31/1000 | Loss: 0.00002238
Iteration 32/1000 | Loss: 0.00002238
Iteration 33/1000 | Loss: 0.00002238
Iteration 34/1000 | Loss: 0.00002237
Iteration 35/1000 | Loss: 0.00002236
Iteration 36/1000 | Loss: 0.00002236
Iteration 37/1000 | Loss: 0.00002236
Iteration 38/1000 | Loss: 0.00002235
Iteration 39/1000 | Loss: 0.00002234
Iteration 40/1000 | Loss: 0.00002226
Iteration 41/1000 | Loss: 0.00002226
Iteration 42/1000 | Loss: 0.00002223
Iteration 43/1000 | Loss: 0.00002222
Iteration 44/1000 | Loss: 0.00002222
Iteration 45/1000 | Loss: 0.00002217
Iteration 46/1000 | Loss: 0.00002217
Iteration 47/1000 | Loss: 0.00002217
Iteration 48/1000 | Loss: 0.00002217
Iteration 49/1000 | Loss: 0.00002217
Iteration 50/1000 | Loss: 0.00002217
Iteration 51/1000 | Loss: 0.00002217
Iteration 52/1000 | Loss: 0.00002211
Iteration 53/1000 | Loss: 0.00002210
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002207
Iteration 56/1000 | Loss: 0.00002207
Iteration 57/1000 | Loss: 0.00002207
Iteration 58/1000 | Loss: 0.00002206
Iteration 59/1000 | Loss: 0.00002206
Iteration 60/1000 | Loss: 0.00002206
Iteration 61/1000 | Loss: 0.00002206
Iteration 62/1000 | Loss: 0.00002206
Iteration 63/1000 | Loss: 0.00002206
Iteration 64/1000 | Loss: 0.00002206
Iteration 65/1000 | Loss: 0.00002206
Iteration 66/1000 | Loss: 0.00002206
Iteration 67/1000 | Loss: 0.00002206
Iteration 68/1000 | Loss: 0.00002205
Iteration 69/1000 | Loss: 0.00002205
Iteration 70/1000 | Loss: 0.00002205
Iteration 71/1000 | Loss: 0.00002205
Iteration 72/1000 | Loss: 0.00002205
Iteration 73/1000 | Loss: 0.00002205
Iteration 74/1000 | Loss: 0.00002205
Iteration 75/1000 | Loss: 0.00002205
Iteration 76/1000 | Loss: 0.00002204
Iteration 77/1000 | Loss: 0.00002204
Iteration 78/1000 | Loss: 0.00002204
Iteration 79/1000 | Loss: 0.00002204
Iteration 80/1000 | Loss: 0.00002204
Iteration 81/1000 | Loss: 0.00002204
Iteration 82/1000 | Loss: 0.00002204
Iteration 83/1000 | Loss: 0.00002204
Iteration 84/1000 | Loss: 0.00002204
Iteration 85/1000 | Loss: 0.00002204
Iteration 86/1000 | Loss: 0.00002204
Iteration 87/1000 | Loss: 0.00002204
Iteration 88/1000 | Loss: 0.00002204
Iteration 89/1000 | Loss: 0.00002204
Iteration 90/1000 | Loss: 0.00002204
Iteration 91/1000 | Loss: 0.00002204
Iteration 92/1000 | Loss: 0.00002203
Iteration 93/1000 | Loss: 0.00002203
Iteration 94/1000 | Loss: 0.00002203
Iteration 95/1000 | Loss: 0.00002203
Iteration 96/1000 | Loss: 0.00002203
Iteration 97/1000 | Loss: 0.00002203
Iteration 98/1000 | Loss: 0.00002203
Iteration 99/1000 | Loss: 0.00002203
Iteration 100/1000 | Loss: 0.00002203
Iteration 101/1000 | Loss: 0.00002203
Iteration 102/1000 | Loss: 0.00002203
Iteration 103/1000 | Loss: 0.00002202
Iteration 104/1000 | Loss: 0.00002202
Iteration 105/1000 | Loss: 0.00002202
Iteration 106/1000 | Loss: 0.00002202
Iteration 107/1000 | Loss: 0.00002202
Iteration 108/1000 | Loss: 0.00002202
Iteration 109/1000 | Loss: 0.00002202
Iteration 110/1000 | Loss: 0.00002202
Iteration 111/1000 | Loss: 0.00002202
Iteration 112/1000 | Loss: 0.00002202
Iteration 113/1000 | Loss: 0.00002202
Iteration 114/1000 | Loss: 0.00002202
Iteration 115/1000 | Loss: 0.00002202
Iteration 116/1000 | Loss: 0.00002202
Iteration 117/1000 | Loss: 0.00002202
Iteration 118/1000 | Loss: 0.00002202
Iteration 119/1000 | Loss: 0.00002202
Iteration 120/1000 | Loss: 0.00002202
Iteration 121/1000 | Loss: 0.00002201
Iteration 122/1000 | Loss: 0.00002201
Iteration 123/1000 | Loss: 0.00002201
Iteration 124/1000 | Loss: 0.00002201
Iteration 125/1000 | Loss: 0.00002201
Iteration 126/1000 | Loss: 0.00002201
Iteration 127/1000 | Loss: 0.00002201
Iteration 128/1000 | Loss: 0.00002201
Iteration 129/1000 | Loss: 0.00002201
Iteration 130/1000 | Loss: 0.00002201
Iteration 131/1000 | Loss: 0.00002201
Iteration 132/1000 | Loss: 0.00002201
Iteration 133/1000 | Loss: 0.00002201
Iteration 134/1000 | Loss: 0.00002201
Iteration 135/1000 | Loss: 0.00002201
Iteration 136/1000 | Loss: 0.00002201
Iteration 137/1000 | Loss: 0.00002201
Iteration 138/1000 | Loss: 0.00002201
Iteration 139/1000 | Loss: 0.00002201
Iteration 140/1000 | Loss: 0.00002201
Iteration 141/1000 | Loss: 0.00002201
Iteration 142/1000 | Loss: 0.00002201
Iteration 143/1000 | Loss: 0.00002201
Iteration 144/1000 | Loss: 0.00002201
Iteration 145/1000 | Loss: 0.00002201
Iteration 146/1000 | Loss: 0.00002201
Iteration 147/1000 | Loss: 0.00002201
Iteration 148/1000 | Loss: 0.00002201
Iteration 149/1000 | Loss: 0.00002201
Iteration 150/1000 | Loss: 0.00002201
Iteration 151/1000 | Loss: 0.00002201
Iteration 152/1000 | Loss: 0.00002201
Iteration 153/1000 | Loss: 0.00002201
Iteration 154/1000 | Loss: 0.00002201
Iteration 155/1000 | Loss: 0.00002201
Iteration 156/1000 | Loss: 0.00002201
Iteration 157/1000 | Loss: 0.00002201
Iteration 158/1000 | Loss: 0.00002201
Iteration 159/1000 | Loss: 0.00002201
Iteration 160/1000 | Loss: 0.00002201
Iteration 161/1000 | Loss: 0.00002201
Iteration 162/1000 | Loss: 0.00002201
Iteration 163/1000 | Loss: 0.00002201
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.20051733776927e-05, 2.20051733776927e-05, 2.20051733776927e-05, 2.20051733776927e-05, 2.20051733776927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.20051733776927e-05

Optimization complete. Final v2v error: 3.9850683212280273 mm

Highest mean error: 4.043724060058594 mm for frame 126

Lowest mean error: 3.9243264198303223 mm for frame 143

Saving results

Total time: 39.839046239852905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822625
Iteration 2/25 | Loss: 0.00154081
Iteration 3/25 | Loss: 0.00130482
Iteration 4/25 | Loss: 0.00128095
Iteration 5/25 | Loss: 0.00127898
Iteration 6/25 | Loss: 0.00127898
Iteration 7/25 | Loss: 0.00127898
Iteration 8/25 | Loss: 0.00127898
Iteration 9/25 | Loss: 0.00127898
Iteration 10/25 | Loss: 0.00127898
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001278977608308196, 0.001278977608308196, 0.001278977608308196, 0.001278977608308196, 0.001278977608308196]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001278977608308196

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94238514
Iteration 2/25 | Loss: 0.00068925
Iteration 3/25 | Loss: 0.00068924
Iteration 4/25 | Loss: 0.00068924
Iteration 5/25 | Loss: 0.00068924
Iteration 6/25 | Loss: 0.00068924
Iteration 7/25 | Loss: 0.00068924
Iteration 8/25 | Loss: 0.00068924
Iteration 9/25 | Loss: 0.00068924
Iteration 10/25 | Loss: 0.00068924
Iteration 11/25 | Loss: 0.00068924
Iteration 12/25 | Loss: 0.00068924
Iteration 13/25 | Loss: 0.00068924
Iteration 14/25 | Loss: 0.00068924
Iteration 15/25 | Loss: 0.00068924
Iteration 16/25 | Loss: 0.00068924
Iteration 17/25 | Loss: 0.00068924
Iteration 18/25 | Loss: 0.00068924
Iteration 19/25 | Loss: 0.00068924
Iteration 20/25 | Loss: 0.00068924
Iteration 21/25 | Loss: 0.00068924
Iteration 22/25 | Loss: 0.00068924
Iteration 23/25 | Loss: 0.00068924
Iteration 24/25 | Loss: 0.00068924
Iteration 25/25 | Loss: 0.00068924

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068924
Iteration 2/1000 | Loss: 0.00003561
Iteration 3/1000 | Loss: 0.00002453
Iteration 4/1000 | Loss: 0.00002188
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00002027
Iteration 7/1000 | Loss: 0.00001968
Iteration 8/1000 | Loss: 0.00001927
Iteration 9/1000 | Loss: 0.00001899
Iteration 10/1000 | Loss: 0.00001869
Iteration 11/1000 | Loss: 0.00001853
Iteration 12/1000 | Loss: 0.00001852
Iteration 13/1000 | Loss: 0.00001849
Iteration 14/1000 | Loss: 0.00001842
Iteration 15/1000 | Loss: 0.00001829
Iteration 16/1000 | Loss: 0.00001825
Iteration 17/1000 | Loss: 0.00001825
Iteration 18/1000 | Loss: 0.00001823
Iteration 19/1000 | Loss: 0.00001822
Iteration 20/1000 | Loss: 0.00001822
Iteration 21/1000 | Loss: 0.00001821
Iteration 22/1000 | Loss: 0.00001821
Iteration 23/1000 | Loss: 0.00001817
Iteration 24/1000 | Loss: 0.00001817
Iteration 25/1000 | Loss: 0.00001815
Iteration 26/1000 | Loss: 0.00001813
Iteration 27/1000 | Loss: 0.00001813
Iteration 28/1000 | Loss: 0.00001812
Iteration 29/1000 | Loss: 0.00001812
Iteration 30/1000 | Loss: 0.00001810
Iteration 31/1000 | Loss: 0.00001809
Iteration 32/1000 | Loss: 0.00001809
Iteration 33/1000 | Loss: 0.00001809
Iteration 34/1000 | Loss: 0.00001809
Iteration 35/1000 | Loss: 0.00001809
Iteration 36/1000 | Loss: 0.00001809
Iteration 37/1000 | Loss: 0.00001809
Iteration 38/1000 | Loss: 0.00001808
Iteration 39/1000 | Loss: 0.00001808
Iteration 40/1000 | Loss: 0.00001807
Iteration 41/1000 | Loss: 0.00001807
Iteration 42/1000 | Loss: 0.00001807
Iteration 43/1000 | Loss: 0.00001807
Iteration 44/1000 | Loss: 0.00001807
Iteration 45/1000 | Loss: 0.00001807
Iteration 46/1000 | Loss: 0.00001807
Iteration 47/1000 | Loss: 0.00001807
Iteration 48/1000 | Loss: 0.00001807
Iteration 49/1000 | Loss: 0.00001807
Iteration 50/1000 | Loss: 0.00001806
Iteration 51/1000 | Loss: 0.00001806
Iteration 52/1000 | Loss: 0.00001806
Iteration 53/1000 | Loss: 0.00001806
Iteration 54/1000 | Loss: 0.00001806
Iteration 55/1000 | Loss: 0.00001805
Iteration 56/1000 | Loss: 0.00001805
Iteration 57/1000 | Loss: 0.00001804
Iteration 58/1000 | Loss: 0.00001804
Iteration 59/1000 | Loss: 0.00001802
Iteration 60/1000 | Loss: 0.00001800
Iteration 61/1000 | Loss: 0.00001799
Iteration 62/1000 | Loss: 0.00001798
Iteration 63/1000 | Loss: 0.00001798
Iteration 64/1000 | Loss: 0.00001798
Iteration 65/1000 | Loss: 0.00001797
Iteration 66/1000 | Loss: 0.00001797
Iteration 67/1000 | Loss: 0.00001797
Iteration 68/1000 | Loss: 0.00001796
Iteration 69/1000 | Loss: 0.00001796
Iteration 70/1000 | Loss: 0.00001795
Iteration 71/1000 | Loss: 0.00001795
Iteration 72/1000 | Loss: 0.00001795
Iteration 73/1000 | Loss: 0.00001795
Iteration 74/1000 | Loss: 0.00001795
Iteration 75/1000 | Loss: 0.00001795
Iteration 76/1000 | Loss: 0.00001794
Iteration 77/1000 | Loss: 0.00001794
Iteration 78/1000 | Loss: 0.00001794
Iteration 79/1000 | Loss: 0.00001794
Iteration 80/1000 | Loss: 0.00001794
Iteration 81/1000 | Loss: 0.00001794
Iteration 82/1000 | Loss: 0.00001793
Iteration 83/1000 | Loss: 0.00001793
Iteration 84/1000 | Loss: 0.00001793
Iteration 85/1000 | Loss: 0.00001793
Iteration 86/1000 | Loss: 0.00001793
Iteration 87/1000 | Loss: 0.00001793
Iteration 88/1000 | Loss: 0.00001793
Iteration 89/1000 | Loss: 0.00001792
Iteration 90/1000 | Loss: 0.00001792
Iteration 91/1000 | Loss: 0.00001792
Iteration 92/1000 | Loss: 0.00001791
Iteration 93/1000 | Loss: 0.00001791
Iteration 94/1000 | Loss: 0.00001791
Iteration 95/1000 | Loss: 0.00001791
Iteration 96/1000 | Loss: 0.00001791
Iteration 97/1000 | Loss: 0.00001790
Iteration 98/1000 | Loss: 0.00001790
Iteration 99/1000 | Loss: 0.00001790
Iteration 100/1000 | Loss: 0.00001789
Iteration 101/1000 | Loss: 0.00001789
Iteration 102/1000 | Loss: 0.00001789
Iteration 103/1000 | Loss: 0.00001788
Iteration 104/1000 | Loss: 0.00001787
Iteration 105/1000 | Loss: 0.00001787
Iteration 106/1000 | Loss: 0.00001787
Iteration 107/1000 | Loss: 0.00001786
Iteration 108/1000 | Loss: 0.00001786
Iteration 109/1000 | Loss: 0.00001786
Iteration 110/1000 | Loss: 0.00001786
Iteration 111/1000 | Loss: 0.00001786
Iteration 112/1000 | Loss: 0.00001785
Iteration 113/1000 | Loss: 0.00001785
Iteration 114/1000 | Loss: 0.00001785
Iteration 115/1000 | Loss: 0.00001785
Iteration 116/1000 | Loss: 0.00001785
Iteration 117/1000 | Loss: 0.00001785
Iteration 118/1000 | Loss: 0.00001785
Iteration 119/1000 | Loss: 0.00001785
Iteration 120/1000 | Loss: 0.00001785
Iteration 121/1000 | Loss: 0.00001785
Iteration 122/1000 | Loss: 0.00001785
Iteration 123/1000 | Loss: 0.00001785
Iteration 124/1000 | Loss: 0.00001785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [1.7852386008598842e-05, 1.7852386008598842e-05, 1.7852386008598842e-05, 1.7852386008598842e-05, 1.7852386008598842e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7852386008598842e-05

Optimization complete. Final v2v error: 3.562079668045044 mm

Highest mean error: 3.7557437419891357 mm for frame 22

Lowest mean error: 3.4084620475769043 mm for frame 87

Saving results

Total time: 36.61845684051514
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989179
Iteration 2/25 | Loss: 0.00989179
Iteration 3/25 | Loss: 0.00989178
Iteration 4/25 | Loss: 0.00989178
Iteration 5/25 | Loss: 0.00989178
Iteration 6/25 | Loss: 0.00989178
Iteration 7/25 | Loss: 0.00989178
Iteration 8/25 | Loss: 0.00989177
Iteration 9/25 | Loss: 0.00989177
Iteration 10/25 | Loss: 0.00989177
Iteration 11/25 | Loss: 0.00989177
Iteration 12/25 | Loss: 0.00989177
Iteration 13/25 | Loss: 0.00989176
Iteration 14/25 | Loss: 0.00989176
Iteration 15/25 | Loss: 0.00989176
Iteration 16/25 | Loss: 0.00277222
Iteration 17/25 | Loss: 0.00231040
Iteration 18/25 | Loss: 0.00206084
Iteration 19/25 | Loss: 0.00190867
Iteration 20/25 | Loss: 0.00173568
Iteration 21/25 | Loss: 0.00164100
Iteration 22/25 | Loss: 0.00163918
Iteration 23/25 | Loss: 0.00158425
Iteration 24/25 | Loss: 0.00154746
Iteration 25/25 | Loss: 0.00150863

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62159538
Iteration 2/25 | Loss: 0.00448433
Iteration 3/25 | Loss: 0.00285173
Iteration 4/25 | Loss: 0.00285173
Iteration 5/25 | Loss: 0.00285173
Iteration 6/25 | Loss: 0.00285173
Iteration 7/25 | Loss: 0.00285173
Iteration 8/25 | Loss: 0.00285173
Iteration 9/25 | Loss: 0.00285173
Iteration 10/25 | Loss: 0.00285173
Iteration 11/25 | Loss: 0.00285173
Iteration 12/25 | Loss: 0.00285173
Iteration 13/25 | Loss: 0.00285173
Iteration 14/25 | Loss: 0.00285173
Iteration 15/25 | Loss: 0.00285173
Iteration 16/25 | Loss: 0.00285173
Iteration 17/25 | Loss: 0.00285173
Iteration 18/25 | Loss: 0.00285173
Iteration 19/25 | Loss: 0.00285173
Iteration 20/25 | Loss: 0.00285173
Iteration 21/25 | Loss: 0.00285173
Iteration 22/25 | Loss: 0.00285173
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.002851726021617651, 0.002851726021617651, 0.002851726021617651, 0.002851726021617651, 0.002851726021617651]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002851726021617651

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00285173
Iteration 2/1000 | Loss: 0.00200648
Iteration 3/1000 | Loss: 0.00117561
Iteration 4/1000 | Loss: 0.00043121
Iteration 5/1000 | Loss: 0.00019009
Iteration 6/1000 | Loss: 0.00063211
Iteration 7/1000 | Loss: 0.00042433
Iteration 8/1000 | Loss: 0.00097686
Iteration 9/1000 | Loss: 0.00078684
Iteration 10/1000 | Loss: 0.00033650
Iteration 11/1000 | Loss: 0.00013242
Iteration 12/1000 | Loss: 0.00072946
Iteration 13/1000 | Loss: 0.00081314
Iteration 14/1000 | Loss: 0.00063514
Iteration 15/1000 | Loss: 0.00058614
Iteration 16/1000 | Loss: 0.00084823
Iteration 17/1000 | Loss: 0.00083704
Iteration 18/1000 | Loss: 0.00060588
Iteration 19/1000 | Loss: 0.00049216
Iteration 20/1000 | Loss: 0.00044146
Iteration 21/1000 | Loss: 0.00033871
Iteration 22/1000 | Loss: 0.00034260
Iteration 23/1000 | Loss: 0.00044495
Iteration 24/1000 | Loss: 0.00017418
Iteration 25/1000 | Loss: 0.00011588
Iteration 26/1000 | Loss: 0.00010071
Iteration 27/1000 | Loss: 0.00048357
Iteration 28/1000 | Loss: 0.00119725
Iteration 29/1000 | Loss: 0.00058315
Iteration 30/1000 | Loss: 0.00045759
Iteration 31/1000 | Loss: 0.00069313
Iteration 32/1000 | Loss: 0.00042690
Iteration 33/1000 | Loss: 0.00046381
Iteration 34/1000 | Loss: 0.00023188
Iteration 35/1000 | Loss: 0.00021999
Iteration 36/1000 | Loss: 0.00011452
Iteration 37/1000 | Loss: 0.00039685
Iteration 38/1000 | Loss: 0.00035712
Iteration 39/1000 | Loss: 0.00037257
Iteration 40/1000 | Loss: 0.00054399
Iteration 41/1000 | Loss: 0.00052490
Iteration 42/1000 | Loss: 0.00048738
Iteration 43/1000 | Loss: 0.00033567
Iteration 44/1000 | Loss: 0.00047152
Iteration 45/1000 | Loss: 0.00080953
Iteration 46/1000 | Loss: 0.00047502
Iteration 47/1000 | Loss: 0.00021961
Iteration 48/1000 | Loss: 0.00021924
Iteration 49/1000 | Loss: 0.00046286
Iteration 50/1000 | Loss: 0.00014098
Iteration 51/1000 | Loss: 0.00023832
Iteration 52/1000 | Loss: 0.00031188
Iteration 53/1000 | Loss: 0.00040230
Iteration 54/1000 | Loss: 0.00033874
Iteration 55/1000 | Loss: 0.00016835
Iteration 56/1000 | Loss: 0.00009178
Iteration 57/1000 | Loss: 0.00018070
Iteration 58/1000 | Loss: 0.00041063
Iteration 59/1000 | Loss: 0.00038763
Iteration 60/1000 | Loss: 0.00018896
Iteration 61/1000 | Loss: 0.00008703
Iteration 62/1000 | Loss: 0.00008314
Iteration 63/1000 | Loss: 0.00013661
Iteration 64/1000 | Loss: 0.00012996
Iteration 65/1000 | Loss: 0.00041575
Iteration 66/1000 | Loss: 0.00009833
Iteration 67/1000 | Loss: 0.00008109
Iteration 68/1000 | Loss: 0.00007962
Iteration 69/1000 | Loss: 0.00043703
Iteration 70/1000 | Loss: 0.00052143
Iteration 71/1000 | Loss: 0.00019177
Iteration 72/1000 | Loss: 0.00018303
Iteration 73/1000 | Loss: 0.00013323
Iteration 74/1000 | Loss: 0.00011703
Iteration 75/1000 | Loss: 0.00011623
Iteration 76/1000 | Loss: 0.00011171
Iteration 77/1000 | Loss: 0.00012687
Iteration 78/1000 | Loss: 0.00007653
Iteration 79/1000 | Loss: 0.00011815
Iteration 80/1000 | Loss: 0.00008824
Iteration 81/1000 | Loss: 0.00007405
Iteration 82/1000 | Loss: 0.00007310
Iteration 83/1000 | Loss: 0.00008751
Iteration 84/1000 | Loss: 0.00007362
Iteration 85/1000 | Loss: 0.00007278
Iteration 86/1000 | Loss: 0.00007209
Iteration 87/1000 | Loss: 0.00019698
Iteration 88/1000 | Loss: 0.00007152
Iteration 89/1000 | Loss: 0.00007103
Iteration 90/1000 | Loss: 0.00015797
Iteration 91/1000 | Loss: 0.00007430
Iteration 92/1000 | Loss: 0.00007555
Iteration 93/1000 | Loss: 0.00007030
Iteration 94/1000 | Loss: 0.00013766
Iteration 95/1000 | Loss: 0.00013766
Iteration 96/1000 | Loss: 0.00012932
Iteration 97/1000 | Loss: 0.00007272
Iteration 98/1000 | Loss: 0.00006974
Iteration 99/1000 | Loss: 0.00006948
Iteration 100/1000 | Loss: 0.00006935
Iteration 101/1000 | Loss: 0.00014039
Iteration 102/1000 | Loss: 0.00012907
Iteration 103/1000 | Loss: 0.00011884
Iteration 104/1000 | Loss: 0.00007140
Iteration 105/1000 | Loss: 0.00007026
Iteration 106/1000 | Loss: 0.00032954
Iteration 107/1000 | Loss: 0.00023725
Iteration 108/1000 | Loss: 0.00008205
Iteration 109/1000 | Loss: 0.00007092
Iteration 110/1000 | Loss: 0.00006948
Iteration 111/1000 | Loss: 0.00006850
Iteration 112/1000 | Loss: 0.00011958
Iteration 113/1000 | Loss: 0.00017265
Iteration 114/1000 | Loss: 0.00007717
Iteration 115/1000 | Loss: 0.00007192
Iteration 116/1000 | Loss: 0.00015438
Iteration 117/1000 | Loss: 0.00006810
Iteration 118/1000 | Loss: 0.00006788
Iteration 119/1000 | Loss: 0.00006787
Iteration 120/1000 | Loss: 0.00006786
Iteration 121/1000 | Loss: 0.00006786
Iteration 122/1000 | Loss: 0.00006785
Iteration 123/1000 | Loss: 0.00006779
Iteration 124/1000 | Loss: 0.00006778
Iteration 125/1000 | Loss: 0.00006777
Iteration 126/1000 | Loss: 0.00006768
Iteration 127/1000 | Loss: 0.00006761
Iteration 128/1000 | Loss: 0.00006761
Iteration 129/1000 | Loss: 0.00006756
Iteration 130/1000 | Loss: 0.00006755
Iteration 131/1000 | Loss: 0.00006754
Iteration 132/1000 | Loss: 0.00006747
Iteration 133/1000 | Loss: 0.00006747
Iteration 134/1000 | Loss: 0.00006746
Iteration 135/1000 | Loss: 0.00006745
Iteration 136/1000 | Loss: 0.00006744
Iteration 137/1000 | Loss: 0.00006729
Iteration 138/1000 | Loss: 0.00006728
Iteration 139/1000 | Loss: 0.00006728
Iteration 140/1000 | Loss: 0.00006726
Iteration 141/1000 | Loss: 0.00006725
Iteration 142/1000 | Loss: 0.00006725
Iteration 143/1000 | Loss: 0.00006724
Iteration 144/1000 | Loss: 0.00006724
Iteration 145/1000 | Loss: 0.00006723
Iteration 146/1000 | Loss: 0.00006723
Iteration 147/1000 | Loss: 0.00006722
Iteration 148/1000 | Loss: 0.00006718
Iteration 149/1000 | Loss: 0.00006718
Iteration 150/1000 | Loss: 0.00006718
Iteration 151/1000 | Loss: 0.00006717
Iteration 152/1000 | Loss: 0.00006716
Iteration 153/1000 | Loss: 0.00006716
Iteration 154/1000 | Loss: 0.00006716
Iteration 155/1000 | Loss: 0.00006715
Iteration 156/1000 | Loss: 0.00006715
Iteration 157/1000 | Loss: 0.00006715
Iteration 158/1000 | Loss: 0.00006714
Iteration 159/1000 | Loss: 0.00006714
Iteration 160/1000 | Loss: 0.00006713
Iteration 161/1000 | Loss: 0.00006712
Iteration 162/1000 | Loss: 0.00006712
Iteration 163/1000 | Loss: 0.00006711
Iteration 164/1000 | Loss: 0.00006711
Iteration 165/1000 | Loss: 0.00006710
Iteration 166/1000 | Loss: 0.00006710
Iteration 167/1000 | Loss: 0.00006709
Iteration 168/1000 | Loss: 0.00006707
Iteration 169/1000 | Loss: 0.00006707
Iteration 170/1000 | Loss: 0.00006707
Iteration 171/1000 | Loss: 0.00006707
Iteration 172/1000 | Loss: 0.00006707
Iteration 173/1000 | Loss: 0.00006707
Iteration 174/1000 | Loss: 0.00006707
Iteration 175/1000 | Loss: 0.00006706
Iteration 176/1000 | Loss: 0.00006706
Iteration 177/1000 | Loss: 0.00006706
Iteration 178/1000 | Loss: 0.00006706
Iteration 179/1000 | Loss: 0.00006706
Iteration 180/1000 | Loss: 0.00006703
Iteration 181/1000 | Loss: 0.00006703
Iteration 182/1000 | Loss: 0.00006703
Iteration 183/1000 | Loss: 0.00006703
Iteration 184/1000 | Loss: 0.00006702
Iteration 185/1000 | Loss: 0.00006702
Iteration 186/1000 | Loss: 0.00006701
Iteration 187/1000 | Loss: 0.00006700
Iteration 188/1000 | Loss: 0.00006700
Iteration 189/1000 | Loss: 0.00006699
Iteration 190/1000 | Loss: 0.00006699
Iteration 191/1000 | Loss: 0.00006699
Iteration 192/1000 | Loss: 0.00006699
Iteration 193/1000 | Loss: 0.00006699
Iteration 194/1000 | Loss: 0.00006699
Iteration 195/1000 | Loss: 0.00006699
Iteration 196/1000 | Loss: 0.00006699
Iteration 197/1000 | Loss: 0.00006699
Iteration 198/1000 | Loss: 0.00006699
Iteration 199/1000 | Loss: 0.00006698
Iteration 200/1000 | Loss: 0.00006698
Iteration 201/1000 | Loss: 0.00006698
Iteration 202/1000 | Loss: 0.00006698
Iteration 203/1000 | Loss: 0.00006697
Iteration 204/1000 | Loss: 0.00006697
Iteration 205/1000 | Loss: 0.00006697
Iteration 206/1000 | Loss: 0.00006697
Iteration 207/1000 | Loss: 0.00006697
Iteration 208/1000 | Loss: 0.00006696
Iteration 209/1000 | Loss: 0.00006696
Iteration 210/1000 | Loss: 0.00006696
Iteration 211/1000 | Loss: 0.00006696
Iteration 212/1000 | Loss: 0.00006696
Iteration 213/1000 | Loss: 0.00006696
Iteration 214/1000 | Loss: 0.00006696
Iteration 215/1000 | Loss: 0.00006696
Iteration 216/1000 | Loss: 0.00006695
Iteration 217/1000 | Loss: 0.00006695
Iteration 218/1000 | Loss: 0.00006695
Iteration 219/1000 | Loss: 0.00006695
Iteration 220/1000 | Loss: 0.00006695
Iteration 221/1000 | Loss: 0.00006695
Iteration 222/1000 | Loss: 0.00006695
Iteration 223/1000 | Loss: 0.00006695
Iteration 224/1000 | Loss: 0.00006695
Iteration 225/1000 | Loss: 0.00006694
Iteration 226/1000 | Loss: 0.00006694
Iteration 227/1000 | Loss: 0.00006694
Iteration 228/1000 | Loss: 0.00006693
Iteration 229/1000 | Loss: 0.00006693
Iteration 230/1000 | Loss: 0.00006693
Iteration 231/1000 | Loss: 0.00006693
Iteration 232/1000 | Loss: 0.00006692
Iteration 233/1000 | Loss: 0.00006692
Iteration 234/1000 | Loss: 0.00006692
Iteration 235/1000 | Loss: 0.00006692
Iteration 236/1000 | Loss: 0.00006692
Iteration 237/1000 | Loss: 0.00006692
Iteration 238/1000 | Loss: 0.00006692
Iteration 239/1000 | Loss: 0.00006692
Iteration 240/1000 | Loss: 0.00006692
Iteration 241/1000 | Loss: 0.00006692
Iteration 242/1000 | Loss: 0.00006691
Iteration 243/1000 | Loss: 0.00006691
Iteration 244/1000 | Loss: 0.00006691
Iteration 245/1000 | Loss: 0.00006690
Iteration 246/1000 | Loss: 0.00006690
Iteration 247/1000 | Loss: 0.00006689
Iteration 248/1000 | Loss: 0.00006689
Iteration 249/1000 | Loss: 0.00006689
Iteration 250/1000 | Loss: 0.00006689
Iteration 251/1000 | Loss: 0.00006688
Iteration 252/1000 | Loss: 0.00006688
Iteration 253/1000 | Loss: 0.00006688
Iteration 254/1000 | Loss: 0.00006688
Iteration 255/1000 | Loss: 0.00006688
Iteration 256/1000 | Loss: 0.00006688
Iteration 257/1000 | Loss: 0.00006688
Iteration 258/1000 | Loss: 0.00006688
Iteration 259/1000 | Loss: 0.00006688
Iteration 260/1000 | Loss: 0.00006688
Iteration 261/1000 | Loss: 0.00006687
Iteration 262/1000 | Loss: 0.00006687
Iteration 263/1000 | Loss: 0.00006687
Iteration 264/1000 | Loss: 0.00006687
Iteration 265/1000 | Loss: 0.00006687
Iteration 266/1000 | Loss: 0.00006687
Iteration 267/1000 | Loss: 0.00006687
Iteration 268/1000 | Loss: 0.00006687
Iteration 269/1000 | Loss: 0.00006687
Iteration 270/1000 | Loss: 0.00006687
Iteration 271/1000 | Loss: 0.00006687
Iteration 272/1000 | Loss: 0.00006687
Iteration 273/1000 | Loss: 0.00006687
Iteration 274/1000 | Loss: 0.00006687
Iteration 275/1000 | Loss: 0.00006687
Iteration 276/1000 | Loss: 0.00006687
Iteration 277/1000 | Loss: 0.00006687
Iteration 278/1000 | Loss: 0.00006687
Iteration 279/1000 | Loss: 0.00006687
Iteration 280/1000 | Loss: 0.00006687
Iteration 281/1000 | Loss: 0.00006687
Iteration 282/1000 | Loss: 0.00006687
Iteration 283/1000 | Loss: 0.00006687
Iteration 284/1000 | Loss: 0.00006687
Iteration 285/1000 | Loss: 0.00006687
Iteration 286/1000 | Loss: 0.00006687
Iteration 287/1000 | Loss: 0.00006687
Iteration 288/1000 | Loss: 0.00006687
Iteration 289/1000 | Loss: 0.00006687
Iteration 290/1000 | Loss: 0.00006687
Iteration 291/1000 | Loss: 0.00006687
Iteration 292/1000 | Loss: 0.00006687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 292. Stopping optimization.
Last 5 losses: [6.687169661745429e-05, 6.687169661745429e-05, 6.687169661745429e-05, 6.687169661745429e-05, 6.687169661745429e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.687169661745429e-05

Optimization complete. Final v2v error: 4.763456344604492 mm

Highest mean error: 12.035560607910156 mm for frame 232

Lowest mean error: 3.1239216327667236 mm for frame 43

Saving results

Total time: 237.72732830047607
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00462904
Iteration 2/25 | Loss: 0.00130452
Iteration 3/25 | Loss: 0.00122458
Iteration 4/25 | Loss: 0.00121517
Iteration 5/25 | Loss: 0.00121357
Iteration 6/25 | Loss: 0.00121357
Iteration 7/25 | Loss: 0.00121357
Iteration 8/25 | Loss: 0.00121357
Iteration 9/25 | Loss: 0.00121357
Iteration 10/25 | Loss: 0.00121357
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001213567447848618, 0.001213567447848618, 0.001213567447848618, 0.001213567447848618, 0.001213567447848618]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001213567447848618

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.39226389
Iteration 2/25 | Loss: 0.00105928
Iteration 3/25 | Loss: 0.00105928
Iteration 4/25 | Loss: 0.00105928
Iteration 5/25 | Loss: 0.00105928
Iteration 6/25 | Loss: 0.00105927
Iteration 7/25 | Loss: 0.00105927
Iteration 8/25 | Loss: 0.00105927
Iteration 9/25 | Loss: 0.00105927
Iteration 10/25 | Loss: 0.00105927
Iteration 11/25 | Loss: 0.00105927
Iteration 12/25 | Loss: 0.00105927
Iteration 13/25 | Loss: 0.00105927
Iteration 14/25 | Loss: 0.00105927
Iteration 15/25 | Loss: 0.00105927
Iteration 16/25 | Loss: 0.00105927
Iteration 17/25 | Loss: 0.00105927
Iteration 18/25 | Loss: 0.00105927
Iteration 19/25 | Loss: 0.00105927
Iteration 20/25 | Loss: 0.00105927
Iteration 21/25 | Loss: 0.00105927
Iteration 22/25 | Loss: 0.00105927
Iteration 23/25 | Loss: 0.00105927
Iteration 24/25 | Loss: 0.00105927
Iteration 25/25 | Loss: 0.00105927

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105927
Iteration 2/1000 | Loss: 0.00001994
Iteration 3/1000 | Loss: 0.00001678
Iteration 4/1000 | Loss: 0.00001541
Iteration 5/1000 | Loss: 0.00001458
Iteration 6/1000 | Loss: 0.00001407
Iteration 7/1000 | Loss: 0.00001365
Iteration 8/1000 | Loss: 0.00001347
Iteration 9/1000 | Loss: 0.00001317
Iteration 10/1000 | Loss: 0.00001313
Iteration 11/1000 | Loss: 0.00001301
Iteration 12/1000 | Loss: 0.00001295
Iteration 13/1000 | Loss: 0.00001286
Iteration 14/1000 | Loss: 0.00001272
Iteration 15/1000 | Loss: 0.00001269
Iteration 16/1000 | Loss: 0.00001254
Iteration 17/1000 | Loss: 0.00001250
Iteration 18/1000 | Loss: 0.00001239
Iteration 19/1000 | Loss: 0.00001235
Iteration 20/1000 | Loss: 0.00001235
Iteration 21/1000 | Loss: 0.00001234
Iteration 22/1000 | Loss: 0.00001233
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001219
Iteration 25/1000 | Loss: 0.00001217
Iteration 26/1000 | Loss: 0.00001216
Iteration 27/1000 | Loss: 0.00001214
Iteration 28/1000 | Loss: 0.00001214
Iteration 29/1000 | Loss: 0.00001214
Iteration 30/1000 | Loss: 0.00001212
Iteration 31/1000 | Loss: 0.00001212
Iteration 32/1000 | Loss: 0.00001211
Iteration 33/1000 | Loss: 0.00001210
Iteration 34/1000 | Loss: 0.00001209
Iteration 35/1000 | Loss: 0.00001207
Iteration 36/1000 | Loss: 0.00001207
Iteration 37/1000 | Loss: 0.00001206
Iteration 38/1000 | Loss: 0.00001206
Iteration 39/1000 | Loss: 0.00001204
Iteration 40/1000 | Loss: 0.00001203
Iteration 41/1000 | Loss: 0.00001202
Iteration 42/1000 | Loss: 0.00001202
Iteration 43/1000 | Loss: 0.00001201
Iteration 44/1000 | Loss: 0.00001200
Iteration 45/1000 | Loss: 0.00001200
Iteration 46/1000 | Loss: 0.00001200
Iteration 47/1000 | Loss: 0.00001200
Iteration 48/1000 | Loss: 0.00001200
Iteration 49/1000 | Loss: 0.00001200
Iteration 50/1000 | Loss: 0.00001200
Iteration 51/1000 | Loss: 0.00001199
Iteration 52/1000 | Loss: 0.00001199
Iteration 53/1000 | Loss: 0.00001198
Iteration 54/1000 | Loss: 0.00001197
Iteration 55/1000 | Loss: 0.00001197
Iteration 56/1000 | Loss: 0.00001196
Iteration 57/1000 | Loss: 0.00001196
Iteration 58/1000 | Loss: 0.00001196
Iteration 59/1000 | Loss: 0.00001194
Iteration 60/1000 | Loss: 0.00001194
Iteration 61/1000 | Loss: 0.00001193
Iteration 62/1000 | Loss: 0.00001193
Iteration 63/1000 | Loss: 0.00001192
Iteration 64/1000 | Loss: 0.00001192
Iteration 65/1000 | Loss: 0.00001192
Iteration 66/1000 | Loss: 0.00001191
Iteration 67/1000 | Loss: 0.00001191
Iteration 68/1000 | Loss: 0.00001191
Iteration 69/1000 | Loss: 0.00001191
Iteration 70/1000 | Loss: 0.00001190
Iteration 71/1000 | Loss: 0.00001190
Iteration 72/1000 | Loss: 0.00001189
Iteration 73/1000 | Loss: 0.00001189
Iteration 74/1000 | Loss: 0.00001189
Iteration 75/1000 | Loss: 0.00001189
Iteration 76/1000 | Loss: 0.00001189
Iteration 77/1000 | Loss: 0.00001188
Iteration 78/1000 | Loss: 0.00001188
Iteration 79/1000 | Loss: 0.00001188
Iteration 80/1000 | Loss: 0.00001188
Iteration 81/1000 | Loss: 0.00001187
Iteration 82/1000 | Loss: 0.00001187
Iteration 83/1000 | Loss: 0.00001187
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001185
Iteration 93/1000 | Loss: 0.00001185
Iteration 94/1000 | Loss: 0.00001185
Iteration 95/1000 | Loss: 0.00001185
Iteration 96/1000 | Loss: 0.00001185
Iteration 97/1000 | Loss: 0.00001185
Iteration 98/1000 | Loss: 0.00001184
Iteration 99/1000 | Loss: 0.00001184
Iteration 100/1000 | Loss: 0.00001184
Iteration 101/1000 | Loss: 0.00001184
Iteration 102/1000 | Loss: 0.00001184
Iteration 103/1000 | Loss: 0.00001184
Iteration 104/1000 | Loss: 0.00001183
Iteration 105/1000 | Loss: 0.00001183
Iteration 106/1000 | Loss: 0.00001183
Iteration 107/1000 | Loss: 0.00001183
Iteration 108/1000 | Loss: 0.00001183
Iteration 109/1000 | Loss: 0.00001183
Iteration 110/1000 | Loss: 0.00001183
Iteration 111/1000 | Loss: 0.00001182
Iteration 112/1000 | Loss: 0.00001182
Iteration 113/1000 | Loss: 0.00001182
Iteration 114/1000 | Loss: 0.00001182
Iteration 115/1000 | Loss: 0.00001182
Iteration 116/1000 | Loss: 0.00001182
Iteration 117/1000 | Loss: 0.00001182
Iteration 118/1000 | Loss: 0.00001182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 118. Stopping optimization.
Last 5 losses: [1.1821703083114699e-05, 1.1821703083114699e-05, 1.1821703083114699e-05, 1.1821703083114699e-05, 1.1821703083114699e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1821703083114699e-05

Optimization complete. Final v2v error: 2.9553563594818115 mm

Highest mean error: 3.092092990875244 mm for frame 250

Lowest mean error: 2.808182954788208 mm for frame 222

Saving results

Total time: 42.60985612869263
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00967047
Iteration 2/25 | Loss: 0.00374142
Iteration 3/25 | Loss: 0.00281405
Iteration 4/25 | Loss: 0.00230655
Iteration 5/25 | Loss: 0.00236547
Iteration 6/25 | Loss: 0.00220693
Iteration 7/25 | Loss: 0.00222047
Iteration 8/25 | Loss: 0.00200708
Iteration 9/25 | Loss: 0.00185667
Iteration 10/25 | Loss: 0.00184429
Iteration 11/25 | Loss: 0.00186877
Iteration 12/25 | Loss: 0.00179343
Iteration 13/25 | Loss: 0.00177503
Iteration 14/25 | Loss: 0.00174042
Iteration 15/25 | Loss: 0.00169037
Iteration 16/25 | Loss: 0.00169516
Iteration 17/25 | Loss: 0.00166686
Iteration 18/25 | Loss: 0.00165785
Iteration 19/25 | Loss: 0.00165695
Iteration 20/25 | Loss: 0.00165390
Iteration 21/25 | Loss: 0.00163277
Iteration 22/25 | Loss: 0.00163107
Iteration 23/25 | Loss: 0.00163059
Iteration 24/25 | Loss: 0.00163045
Iteration 25/25 | Loss: 0.00163021

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95355535
Iteration 2/25 | Loss: 0.00825750
Iteration 3/25 | Loss: 0.00464389
Iteration 4/25 | Loss: 0.00464200
Iteration 5/25 | Loss: 0.00464200
Iteration 6/25 | Loss: 0.00464200
Iteration 7/25 | Loss: 0.00464200
Iteration 8/25 | Loss: 0.00464199
Iteration 9/25 | Loss: 0.00464199
Iteration 10/25 | Loss: 0.00464199
Iteration 11/25 | Loss: 0.00464199
Iteration 12/25 | Loss: 0.00464199
Iteration 13/25 | Loss: 0.00464199
Iteration 14/25 | Loss: 0.00464199
Iteration 15/25 | Loss: 0.00464199
Iteration 16/25 | Loss: 0.00464199
Iteration 17/25 | Loss: 0.00464199
Iteration 18/25 | Loss: 0.00464199
Iteration 19/25 | Loss: 0.00464199
Iteration 20/25 | Loss: 0.00464199
Iteration 21/25 | Loss: 0.00464199
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.004641992039978504, 0.004641992039978504, 0.004641992039978504, 0.004641992039978504, 0.004641992039978504]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004641992039978504

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00464199
Iteration 2/1000 | Loss: 0.00064835
Iteration 3/1000 | Loss: 0.00051153
Iteration 4/1000 | Loss: 0.00036126
Iteration 5/1000 | Loss: 0.00034233
Iteration 6/1000 | Loss: 0.00201029
Iteration 7/1000 | Loss: 0.00340976
Iteration 8/1000 | Loss: 0.00341425
Iteration 9/1000 | Loss: 0.00066236
Iteration 10/1000 | Loss: 0.00112895
Iteration 11/1000 | Loss: 0.00262524
Iteration 12/1000 | Loss: 0.00139496
Iteration 13/1000 | Loss: 0.00047918
Iteration 14/1000 | Loss: 0.00051013
Iteration 15/1000 | Loss: 0.00317102
Iteration 16/1000 | Loss: 0.00387825
Iteration 17/1000 | Loss: 0.00189735
Iteration 18/1000 | Loss: 0.00363163
Iteration 19/1000 | Loss: 0.00102008
Iteration 20/1000 | Loss: 0.00200060
Iteration 21/1000 | Loss: 0.00066744
Iteration 22/1000 | Loss: 0.00114071
Iteration 23/1000 | Loss: 0.00165715
Iteration 24/1000 | Loss: 0.00200115
Iteration 25/1000 | Loss: 0.00240400
Iteration 26/1000 | Loss: 0.00036521
Iteration 27/1000 | Loss: 0.00063134
Iteration 28/1000 | Loss: 0.00279138
Iteration 29/1000 | Loss: 0.00290301
Iteration 30/1000 | Loss: 0.00180723
Iteration 31/1000 | Loss: 0.00140823
Iteration 32/1000 | Loss: 0.00026714
Iteration 33/1000 | Loss: 0.00029732
Iteration 34/1000 | Loss: 0.00303721
Iteration 35/1000 | Loss: 0.00187787
Iteration 36/1000 | Loss: 0.00540860
Iteration 37/1000 | Loss: 0.00840173
Iteration 38/1000 | Loss: 0.00835866
Iteration 39/1000 | Loss: 0.00611000
Iteration 40/1000 | Loss: 0.00042019
Iteration 41/1000 | Loss: 0.00056200
Iteration 42/1000 | Loss: 0.00141721
Iteration 43/1000 | Loss: 0.00017712
Iteration 44/1000 | Loss: 0.00015806
Iteration 45/1000 | Loss: 0.00014654
Iteration 46/1000 | Loss: 0.00297367
Iteration 47/1000 | Loss: 0.00039851
Iteration 48/1000 | Loss: 0.00014543
Iteration 49/1000 | Loss: 0.00240539
Iteration 50/1000 | Loss: 0.00014190
Iteration 51/1000 | Loss: 0.00454425
Iteration 52/1000 | Loss: 0.00502846
Iteration 53/1000 | Loss: 0.00125513
Iteration 54/1000 | Loss: 0.00234100
Iteration 55/1000 | Loss: 0.00231147
Iteration 56/1000 | Loss: 0.00329496
Iteration 57/1000 | Loss: 0.00231355
Iteration 58/1000 | Loss: 0.00255770
Iteration 59/1000 | Loss: 0.00167328
Iteration 60/1000 | Loss: 0.00031701
Iteration 61/1000 | Loss: 0.00145649
Iteration 62/1000 | Loss: 0.00232719
Iteration 63/1000 | Loss: 0.00357303
Iteration 64/1000 | Loss: 0.00266843
Iteration 65/1000 | Loss: 0.00226219
Iteration 66/1000 | Loss: 0.00091415
Iteration 67/1000 | Loss: 0.00130787
Iteration 68/1000 | Loss: 0.00201491
Iteration 69/1000 | Loss: 0.00053633
Iteration 70/1000 | Loss: 0.00156070
Iteration 71/1000 | Loss: 0.00141509
Iteration 72/1000 | Loss: 0.00159659
Iteration 73/1000 | Loss: 0.00207709
Iteration 74/1000 | Loss: 0.00396767
Iteration 75/1000 | Loss: 0.00190502
Iteration 76/1000 | Loss: 0.00139735
Iteration 77/1000 | Loss: 0.00122958
Iteration 78/1000 | Loss: 0.00128651
Iteration 79/1000 | Loss: 0.00205304
Iteration 80/1000 | Loss: 0.00182876
Iteration 81/1000 | Loss: 0.00152757
Iteration 82/1000 | Loss: 0.00174945
Iteration 83/1000 | Loss: 0.00062280
Iteration 84/1000 | Loss: 0.00212469
Iteration 85/1000 | Loss: 0.00152488
Iteration 86/1000 | Loss: 0.00145716
Iteration 87/1000 | Loss: 0.00087235
Iteration 88/1000 | Loss: 0.00018699
Iteration 89/1000 | Loss: 0.00092897
Iteration 90/1000 | Loss: 0.00364415
Iteration 91/1000 | Loss: 0.00247081
Iteration 92/1000 | Loss: 0.00090482
Iteration 93/1000 | Loss: 0.00129956
Iteration 94/1000 | Loss: 0.00227127
Iteration 95/1000 | Loss: 0.00167673
Iteration 96/1000 | Loss: 0.00411255
Iteration 97/1000 | Loss: 0.00218667
Iteration 98/1000 | Loss: 0.00411398
Iteration 99/1000 | Loss: 0.00102056
Iteration 100/1000 | Loss: 0.00138607
Iteration 101/1000 | Loss: 0.00157195
Iteration 102/1000 | Loss: 0.00222701
Iteration 103/1000 | Loss: 0.00267370
Iteration 104/1000 | Loss: 0.00087130
Iteration 105/1000 | Loss: 0.00012196
Iteration 106/1000 | Loss: 0.00011273
Iteration 107/1000 | Loss: 0.00105361
Iteration 108/1000 | Loss: 0.00010991
Iteration 109/1000 | Loss: 0.00191245
Iteration 110/1000 | Loss: 0.00012027
Iteration 111/1000 | Loss: 0.00026614
Iteration 112/1000 | Loss: 0.00392506
Iteration 113/1000 | Loss: 0.00172047
Iteration 114/1000 | Loss: 0.00011702
Iteration 115/1000 | Loss: 0.00037501
Iteration 116/1000 | Loss: 0.00011485
Iteration 117/1000 | Loss: 0.00010262
Iteration 118/1000 | Loss: 0.00103718
Iteration 119/1000 | Loss: 0.00466802
Iteration 120/1000 | Loss: 0.00032480
Iteration 121/1000 | Loss: 0.00011624
Iteration 122/1000 | Loss: 0.00009741
Iteration 123/1000 | Loss: 0.00009289
Iteration 124/1000 | Loss: 0.00009063
Iteration 125/1000 | Loss: 0.00030266
Iteration 126/1000 | Loss: 0.00122101
Iteration 127/1000 | Loss: 0.00009661
Iteration 128/1000 | Loss: 0.00008962
Iteration 129/1000 | Loss: 0.00008584
Iteration 130/1000 | Loss: 0.00008412
Iteration 131/1000 | Loss: 0.00263431
Iteration 132/1000 | Loss: 0.00046337
Iteration 133/1000 | Loss: 0.00010370
Iteration 134/1000 | Loss: 0.00026978
Iteration 135/1000 | Loss: 0.00718757
Iteration 136/1000 | Loss: 0.00062487
Iteration 137/1000 | Loss: 0.00332307
Iteration 138/1000 | Loss: 0.00043635
Iteration 139/1000 | Loss: 0.00440945
Iteration 140/1000 | Loss: 0.00016996
Iteration 141/1000 | Loss: 0.00346558
Iteration 142/1000 | Loss: 0.00406307
Iteration 143/1000 | Loss: 0.00526307
Iteration 144/1000 | Loss: 0.00108024
Iteration 145/1000 | Loss: 0.00009503
Iteration 146/1000 | Loss: 0.00118983
Iteration 147/1000 | Loss: 0.00008342
Iteration 148/1000 | Loss: 0.00094502
Iteration 149/1000 | Loss: 0.00371949
Iteration 150/1000 | Loss: 0.00021284
Iteration 151/1000 | Loss: 0.00010088
Iteration 152/1000 | Loss: 0.00007428
Iteration 153/1000 | Loss: 0.00005954
Iteration 154/1000 | Loss: 0.00095785
Iteration 155/1000 | Loss: 0.00100230
Iteration 156/1000 | Loss: 0.00181695
Iteration 157/1000 | Loss: 0.00007216
Iteration 158/1000 | Loss: 0.00042944
Iteration 159/1000 | Loss: 0.00005735
Iteration 160/1000 | Loss: 0.00005258
Iteration 161/1000 | Loss: 0.00005608
Iteration 162/1000 | Loss: 0.00084029
Iteration 163/1000 | Loss: 0.00031266
Iteration 164/1000 | Loss: 0.00006349
Iteration 165/1000 | Loss: 0.00004525
Iteration 166/1000 | Loss: 0.00004149
Iteration 167/1000 | Loss: 0.00009254
Iteration 168/1000 | Loss: 0.00003876
Iteration 169/1000 | Loss: 0.00003725
Iteration 170/1000 | Loss: 0.00003612
Iteration 171/1000 | Loss: 0.00012570
Iteration 172/1000 | Loss: 0.00003522
Iteration 173/1000 | Loss: 0.00003451
Iteration 174/1000 | Loss: 0.00003416
Iteration 175/1000 | Loss: 0.00003384
Iteration 176/1000 | Loss: 0.00003341
Iteration 177/1000 | Loss: 0.00003312
Iteration 178/1000 | Loss: 0.00082675
Iteration 179/1000 | Loss: 0.00046120
Iteration 180/1000 | Loss: 0.00020113
Iteration 181/1000 | Loss: 0.00003622
Iteration 182/1000 | Loss: 0.00056744
Iteration 183/1000 | Loss: 0.00165172
Iteration 184/1000 | Loss: 0.00104597
Iteration 185/1000 | Loss: 0.00270249
Iteration 186/1000 | Loss: 0.00014352
Iteration 187/1000 | Loss: 0.00006492
Iteration 188/1000 | Loss: 0.00003844
Iteration 189/1000 | Loss: 0.00003356
Iteration 190/1000 | Loss: 0.00005745
Iteration 191/1000 | Loss: 0.00107042
Iteration 192/1000 | Loss: 0.00010155
Iteration 193/1000 | Loss: 0.00005183
Iteration 194/1000 | Loss: 0.00005127
Iteration 195/1000 | Loss: 0.00002764
Iteration 196/1000 | Loss: 0.00002607
Iteration 197/1000 | Loss: 0.00002619
Iteration 198/1000 | Loss: 0.00002583
Iteration 199/1000 | Loss: 0.00002511
Iteration 200/1000 | Loss: 0.00002565
Iteration 201/1000 | Loss: 0.00002878
Iteration 202/1000 | Loss: 0.00002438
Iteration 203/1000 | Loss: 0.00002422
Iteration 204/1000 | Loss: 0.00002407
Iteration 205/1000 | Loss: 0.00002398
Iteration 206/1000 | Loss: 0.00002398
Iteration 207/1000 | Loss: 0.00002397
Iteration 208/1000 | Loss: 0.00002390
Iteration 209/1000 | Loss: 0.00002387
Iteration 210/1000 | Loss: 0.00002387
Iteration 211/1000 | Loss: 0.00002386
Iteration 212/1000 | Loss: 0.00002386
Iteration 213/1000 | Loss: 0.00002385
Iteration 214/1000 | Loss: 0.00002384
Iteration 215/1000 | Loss: 0.00002384
Iteration 216/1000 | Loss: 0.00002384
Iteration 217/1000 | Loss: 0.00002383
Iteration 218/1000 | Loss: 0.00002383
Iteration 219/1000 | Loss: 0.00002382
Iteration 220/1000 | Loss: 0.00002382
Iteration 221/1000 | Loss: 0.00002381
Iteration 222/1000 | Loss: 0.00002381
Iteration 223/1000 | Loss: 0.00002381
Iteration 224/1000 | Loss: 0.00002380
Iteration 225/1000 | Loss: 0.00002377
Iteration 226/1000 | Loss: 0.00002377
Iteration 227/1000 | Loss: 0.00002377
Iteration 228/1000 | Loss: 0.00002377
Iteration 229/1000 | Loss: 0.00002377
Iteration 230/1000 | Loss: 0.00002377
Iteration 231/1000 | Loss: 0.00002377
Iteration 232/1000 | Loss: 0.00002376
Iteration 233/1000 | Loss: 0.00002376
Iteration 234/1000 | Loss: 0.00002376
Iteration 235/1000 | Loss: 0.00002376
Iteration 236/1000 | Loss: 0.00002376
Iteration 237/1000 | Loss: 0.00002376
Iteration 238/1000 | Loss: 0.00002376
Iteration 239/1000 | Loss: 0.00002375
Iteration 240/1000 | Loss: 0.00002374
Iteration 241/1000 | Loss: 0.00002373
Iteration 242/1000 | Loss: 0.00002373
Iteration 243/1000 | Loss: 0.00002371
Iteration 244/1000 | Loss: 0.00002371
Iteration 245/1000 | Loss: 0.00002370
Iteration 246/1000 | Loss: 0.00002370
Iteration 247/1000 | Loss: 0.00002370
Iteration 248/1000 | Loss: 0.00002370
Iteration 249/1000 | Loss: 0.00002370
Iteration 250/1000 | Loss: 0.00002370
Iteration 251/1000 | Loss: 0.00002369
Iteration 252/1000 | Loss: 0.00002369
Iteration 253/1000 | Loss: 0.00002369
Iteration 254/1000 | Loss: 0.00002369
Iteration 255/1000 | Loss: 0.00002369
Iteration 256/1000 | Loss: 0.00002368
Iteration 257/1000 | Loss: 0.00002368
Iteration 258/1000 | Loss: 0.00002368
Iteration 259/1000 | Loss: 0.00002368
Iteration 260/1000 | Loss: 0.00002368
Iteration 261/1000 | Loss: 0.00002368
Iteration 262/1000 | Loss: 0.00002367
Iteration 263/1000 | Loss: 0.00002367
Iteration 264/1000 | Loss: 0.00002367
Iteration 265/1000 | Loss: 0.00002367
Iteration 266/1000 | Loss: 0.00002367
Iteration 267/1000 | Loss: 0.00002367
Iteration 268/1000 | Loss: 0.00002367
Iteration 269/1000 | Loss: 0.00002367
Iteration 270/1000 | Loss: 0.00002367
Iteration 271/1000 | Loss: 0.00002366
Iteration 272/1000 | Loss: 0.00002366
Iteration 273/1000 | Loss: 0.00002366
Iteration 274/1000 | Loss: 0.00002365
Iteration 275/1000 | Loss: 0.00002364
Iteration 276/1000 | Loss: 0.00002364
Iteration 277/1000 | Loss: 0.00002364
Iteration 278/1000 | Loss: 0.00002364
Iteration 279/1000 | Loss: 0.00002363
Iteration 280/1000 | Loss: 0.00002363
Iteration 281/1000 | Loss: 0.00002363
Iteration 282/1000 | Loss: 0.00002362
Iteration 283/1000 | Loss: 0.00002362
Iteration 284/1000 | Loss: 0.00002361
Iteration 285/1000 | Loss: 0.00002361
Iteration 286/1000 | Loss: 0.00002361
Iteration 287/1000 | Loss: 0.00002361
Iteration 288/1000 | Loss: 0.00002361
Iteration 289/1000 | Loss: 0.00002361
Iteration 290/1000 | Loss: 0.00002360
Iteration 291/1000 | Loss: 0.00002360
Iteration 292/1000 | Loss: 0.00002360
Iteration 293/1000 | Loss: 0.00002360
Iteration 294/1000 | Loss: 0.00002360
Iteration 295/1000 | Loss: 0.00002360
Iteration 296/1000 | Loss: 0.00002359
Iteration 297/1000 | Loss: 0.00002359
Iteration 298/1000 | Loss: 0.00002359
Iteration 299/1000 | Loss: 0.00002359
Iteration 300/1000 | Loss: 0.00002358
Iteration 301/1000 | Loss: 0.00002358
Iteration 302/1000 | Loss: 0.00002358
Iteration 303/1000 | Loss: 0.00002358
Iteration 304/1000 | Loss: 0.00002358
Iteration 305/1000 | Loss: 0.00002358
Iteration 306/1000 | Loss: 0.00002358
Iteration 307/1000 | Loss: 0.00002358
Iteration 308/1000 | Loss: 0.00002358
Iteration 309/1000 | Loss: 0.00002358
Iteration 310/1000 | Loss: 0.00002358
Iteration 311/1000 | Loss: 0.00002358
Iteration 312/1000 | Loss: 0.00002358
Iteration 313/1000 | Loss: 0.00002358
Iteration 314/1000 | Loss: 0.00002358
Iteration 315/1000 | Loss: 0.00002358
Iteration 316/1000 | Loss: 0.00002358
Iteration 317/1000 | Loss: 0.00002357
Iteration 318/1000 | Loss: 0.00002357
Iteration 319/1000 | Loss: 0.00002357
Iteration 320/1000 | Loss: 0.00002357
Iteration 321/1000 | Loss: 0.00002357
Iteration 322/1000 | Loss: 0.00002357
Iteration 323/1000 | Loss: 0.00002357
Iteration 324/1000 | Loss: 0.00002357
Iteration 325/1000 | Loss: 0.00002357
Iteration 326/1000 | Loss: 0.00002357
Iteration 327/1000 | Loss: 0.00002356
Iteration 328/1000 | Loss: 0.00002356
Iteration 329/1000 | Loss: 0.00002356
Iteration 330/1000 | Loss: 0.00002356
Iteration 331/1000 | Loss: 0.00002356
Iteration 332/1000 | Loss: 0.00002356
Iteration 333/1000 | Loss: 0.00002356
Iteration 334/1000 | Loss: 0.00002356
Iteration 335/1000 | Loss: 0.00002356
Iteration 336/1000 | Loss: 0.00002356
Iteration 337/1000 | Loss: 0.00002355
Iteration 338/1000 | Loss: 0.00002355
Iteration 339/1000 | Loss: 0.00002355
Iteration 340/1000 | Loss: 0.00002355
Iteration 341/1000 | Loss: 0.00002355
Iteration 342/1000 | Loss: 0.00002355
Iteration 343/1000 | Loss: 0.00002355
Iteration 344/1000 | Loss: 0.00002355
Iteration 345/1000 | Loss: 0.00002354
Iteration 346/1000 | Loss: 0.00002354
Iteration 347/1000 | Loss: 0.00002354
Iteration 348/1000 | Loss: 0.00002354
Iteration 349/1000 | Loss: 0.00002354
Iteration 350/1000 | Loss: 0.00002354
Iteration 351/1000 | Loss: 0.00002354
Iteration 352/1000 | Loss: 0.00002354
Iteration 353/1000 | Loss: 0.00002354
Iteration 354/1000 | Loss: 0.00002354
Iteration 355/1000 | Loss: 0.00002354
Iteration 356/1000 | Loss: 0.00002354
Iteration 357/1000 | Loss: 0.00002354
Iteration 358/1000 | Loss: 0.00002354
Iteration 359/1000 | Loss: 0.00002353
Iteration 360/1000 | Loss: 0.00002353
Iteration 361/1000 | Loss: 0.00002353
Iteration 362/1000 | Loss: 0.00002353
Iteration 363/1000 | Loss: 0.00002353
Iteration 364/1000 | Loss: 0.00002353
Iteration 365/1000 | Loss: 0.00002353
Iteration 366/1000 | Loss: 0.00002353
Iteration 367/1000 | Loss: 0.00002353
Iteration 368/1000 | Loss: 0.00002353
Iteration 369/1000 | Loss: 0.00002353
Iteration 370/1000 | Loss: 0.00002353
Iteration 371/1000 | Loss: 0.00002353
Iteration 372/1000 | Loss: 0.00002353
Iteration 373/1000 | Loss: 0.00002353
Iteration 374/1000 | Loss: 0.00002353
Iteration 375/1000 | Loss: 0.00002353
Iteration 376/1000 | Loss: 0.00002353
Iteration 377/1000 | Loss: 0.00002353
Iteration 378/1000 | Loss: 0.00002353
Iteration 379/1000 | Loss: 0.00002353
Iteration 380/1000 | Loss: 0.00002353
Iteration 381/1000 | Loss: 0.00002353
Iteration 382/1000 | Loss: 0.00002353
Iteration 383/1000 | Loss: 0.00002353
Iteration 384/1000 | Loss: 0.00002353
Iteration 385/1000 | Loss: 0.00002353
Iteration 386/1000 | Loss: 0.00002353
Iteration 387/1000 | Loss: 0.00002353
Iteration 388/1000 | Loss: 0.00002353
Iteration 389/1000 | Loss: 0.00002353
Iteration 390/1000 | Loss: 0.00002353
Iteration 391/1000 | Loss: 0.00002353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 391. Stopping optimization.
Last 5 losses: [2.3526159566245042e-05, 2.3526159566245042e-05, 2.3526159566245042e-05, 2.3526159566245042e-05, 2.3526159566245042e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3526159566245042e-05

Optimization complete. Final v2v error: 3.6963179111480713 mm

Highest mean error: 13.395612716674805 mm for frame 30

Lowest mean error: 2.704340696334839 mm for frame 96

Saving results

Total time: 350.19248032569885
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01080380
Iteration 2/25 | Loss: 0.00195757
Iteration 3/25 | Loss: 0.00148339
Iteration 4/25 | Loss: 0.00143115
Iteration 5/25 | Loss: 0.00142162
Iteration 6/25 | Loss: 0.00143230
Iteration 7/25 | Loss: 0.00142581
Iteration 8/25 | Loss: 0.00140169
Iteration 9/25 | Loss: 0.00140912
Iteration 10/25 | Loss: 0.00140495
Iteration 11/25 | Loss: 0.00140781
Iteration 12/25 | Loss: 0.00139791
Iteration 13/25 | Loss: 0.00138794
Iteration 14/25 | Loss: 0.00138337
Iteration 15/25 | Loss: 0.00138203
Iteration 16/25 | Loss: 0.00138060
Iteration 17/25 | Loss: 0.00138130
Iteration 18/25 | Loss: 0.00138136
Iteration 19/25 | Loss: 0.00138540
Iteration 20/25 | Loss: 0.00138154
Iteration 21/25 | Loss: 0.00137808
Iteration 22/25 | Loss: 0.00137964
Iteration 23/25 | Loss: 0.00137699
Iteration 24/25 | Loss: 0.00137761
Iteration 25/25 | Loss: 0.00137715

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98259348
Iteration 2/25 | Loss: 0.00185256
Iteration 3/25 | Loss: 0.00185255
Iteration 4/25 | Loss: 0.00185255
Iteration 5/25 | Loss: 0.00185255
Iteration 6/25 | Loss: 0.00185254
Iteration 7/25 | Loss: 0.00185254
Iteration 8/25 | Loss: 0.00185254
Iteration 9/25 | Loss: 0.00185254
Iteration 10/25 | Loss: 0.00185254
Iteration 11/25 | Loss: 0.00185254
Iteration 12/25 | Loss: 0.00185254
Iteration 13/25 | Loss: 0.00185254
Iteration 14/25 | Loss: 0.00185254
Iteration 15/25 | Loss: 0.00185254
Iteration 16/25 | Loss: 0.00185254
Iteration 17/25 | Loss: 0.00185254
Iteration 18/25 | Loss: 0.00185254
Iteration 19/25 | Loss: 0.00185254
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0018525428604334593, 0.0018525428604334593, 0.0018525428604334593, 0.0018525428604334593, 0.0018525428604334593]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018525428604334593

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00185254
Iteration 2/1000 | Loss: 0.00022845
Iteration 3/1000 | Loss: 0.00020425
Iteration 4/1000 | Loss: 0.00011755
Iteration 5/1000 | Loss: 0.00013286
Iteration 6/1000 | Loss: 0.00022399
Iteration 7/1000 | Loss: 0.00020261
Iteration 8/1000 | Loss: 0.00019084
Iteration 9/1000 | Loss: 0.00020717
Iteration 10/1000 | Loss: 0.00011819
Iteration 11/1000 | Loss: 0.00011881
Iteration 12/1000 | Loss: 0.00003647
Iteration 13/1000 | Loss: 0.00009201
Iteration 14/1000 | Loss: 0.00008686
Iteration 15/1000 | Loss: 0.00008780
Iteration 16/1000 | Loss: 0.00009084
Iteration 17/1000 | Loss: 0.00012000
Iteration 18/1000 | Loss: 0.00010647
Iteration 19/1000 | Loss: 0.00009956
Iteration 20/1000 | Loss: 0.00007140
Iteration 21/1000 | Loss: 0.00012411
Iteration 22/1000 | Loss: 0.00010192
Iteration 23/1000 | Loss: 0.00022229
Iteration 24/1000 | Loss: 0.00013925
Iteration 25/1000 | Loss: 0.00028833
Iteration 26/1000 | Loss: 0.00026990
Iteration 27/1000 | Loss: 0.00029989
Iteration 28/1000 | Loss: 0.00046244
Iteration 29/1000 | Loss: 0.00063098
Iteration 30/1000 | Loss: 0.00037326
Iteration 31/1000 | Loss: 0.00006721
Iteration 32/1000 | Loss: 0.00034609
Iteration 33/1000 | Loss: 0.00024817
Iteration 34/1000 | Loss: 0.00025469
Iteration 35/1000 | Loss: 0.00026516
Iteration 36/1000 | Loss: 0.00068371
Iteration 37/1000 | Loss: 0.00060262
Iteration 38/1000 | Loss: 0.00015654
Iteration 39/1000 | Loss: 0.00026157
Iteration 40/1000 | Loss: 0.00011147
Iteration 41/1000 | Loss: 0.00010437
Iteration 42/1000 | Loss: 0.00016001
Iteration 43/1000 | Loss: 0.00015680
Iteration 44/1000 | Loss: 0.00012183
Iteration 45/1000 | Loss: 0.00004929
Iteration 46/1000 | Loss: 0.00004852
Iteration 47/1000 | Loss: 0.00006190
Iteration 48/1000 | Loss: 0.00004562
Iteration 49/1000 | Loss: 0.00013305
Iteration 50/1000 | Loss: 0.00009960
Iteration 51/1000 | Loss: 0.00009709
Iteration 52/1000 | Loss: 0.00029746
Iteration 53/1000 | Loss: 0.00004709
Iteration 54/1000 | Loss: 0.00004655
Iteration 55/1000 | Loss: 0.00025144
Iteration 56/1000 | Loss: 0.00051268
Iteration 57/1000 | Loss: 0.00019536
Iteration 58/1000 | Loss: 0.00021201
Iteration 59/1000 | Loss: 0.00015912
Iteration 60/1000 | Loss: 0.00025848
Iteration 61/1000 | Loss: 0.00004349
Iteration 62/1000 | Loss: 0.00004275
Iteration 63/1000 | Loss: 0.00004033
Iteration 64/1000 | Loss: 0.00003376
Iteration 65/1000 | Loss: 0.00004264
Iteration 66/1000 | Loss: 0.00003251
Iteration 67/1000 | Loss: 0.00003839
Iteration 68/1000 | Loss: 0.00003702
Iteration 69/1000 | Loss: 0.00003623
Iteration 70/1000 | Loss: 0.00003698
Iteration 71/1000 | Loss: 0.00003258
Iteration 72/1000 | Loss: 0.00003496
Iteration 73/1000 | Loss: 0.00004192
Iteration 74/1000 | Loss: 0.00004276
Iteration 75/1000 | Loss: 0.00004257
Iteration 76/1000 | Loss: 0.00004170
Iteration 77/1000 | Loss: 0.00004409
Iteration 78/1000 | Loss: 0.00004090
Iteration 79/1000 | Loss: 0.00004440
Iteration 80/1000 | Loss: 0.00003196
Iteration 81/1000 | Loss: 0.00004137
Iteration 82/1000 | Loss: 0.00004319
Iteration 83/1000 | Loss: 0.00003971
Iteration 84/1000 | Loss: 0.00004256
Iteration 85/1000 | Loss: 0.00003937
Iteration 86/1000 | Loss: 0.00004217
Iteration 87/1000 | Loss: 0.00003954
Iteration 88/1000 | Loss: 0.00003167
Iteration 89/1000 | Loss: 0.00003178
Iteration 90/1000 | Loss: 0.00003174
Iteration 91/1000 | Loss: 0.00004064
Iteration 92/1000 | Loss: 0.00003920
Iteration 93/1000 | Loss: 0.00003811
Iteration 94/1000 | Loss: 0.00003580
Iteration 95/1000 | Loss: 0.00003999
Iteration 96/1000 | Loss: 0.00003141
Iteration 97/1000 | Loss: 0.00005223
Iteration 98/1000 | Loss: 0.00003386
Iteration 99/1000 | Loss: 0.00003001
Iteration 100/1000 | Loss: 0.00002809
Iteration 101/1000 | Loss: 0.00002749
Iteration 102/1000 | Loss: 0.00002714
Iteration 103/1000 | Loss: 0.00002675
Iteration 104/1000 | Loss: 0.00061670
Iteration 105/1000 | Loss: 0.00003098
Iteration 106/1000 | Loss: 0.00011657
Iteration 107/1000 | Loss: 0.00008694
Iteration 108/1000 | Loss: 0.00010462
Iteration 109/1000 | Loss: 0.00009632
Iteration 110/1000 | Loss: 0.00002907
Iteration 111/1000 | Loss: 0.00002663
Iteration 112/1000 | Loss: 0.00002548
Iteration 113/1000 | Loss: 0.00002505
Iteration 114/1000 | Loss: 0.00002459
Iteration 115/1000 | Loss: 0.00002424
Iteration 116/1000 | Loss: 0.00002405
Iteration 117/1000 | Loss: 0.00002390
Iteration 118/1000 | Loss: 0.00002385
Iteration 119/1000 | Loss: 0.00002368
Iteration 120/1000 | Loss: 0.00002360
Iteration 121/1000 | Loss: 0.00002358
Iteration 122/1000 | Loss: 0.00002358
Iteration 123/1000 | Loss: 0.00002358
Iteration 124/1000 | Loss: 0.00002358
Iteration 125/1000 | Loss: 0.00002358
Iteration 126/1000 | Loss: 0.00002358
Iteration 127/1000 | Loss: 0.00002357
Iteration 128/1000 | Loss: 0.00002357
Iteration 129/1000 | Loss: 0.00002356
Iteration 130/1000 | Loss: 0.00002356
Iteration 131/1000 | Loss: 0.00002356
Iteration 132/1000 | Loss: 0.00002355
Iteration 133/1000 | Loss: 0.00002355
Iteration 134/1000 | Loss: 0.00002354
Iteration 135/1000 | Loss: 0.00002354
Iteration 136/1000 | Loss: 0.00002354
Iteration 137/1000 | Loss: 0.00002353
Iteration 138/1000 | Loss: 0.00002353
Iteration 139/1000 | Loss: 0.00002352
Iteration 140/1000 | Loss: 0.00002352
Iteration 141/1000 | Loss: 0.00002352
Iteration 142/1000 | Loss: 0.00002351
Iteration 143/1000 | Loss: 0.00002351
Iteration 144/1000 | Loss: 0.00002350
Iteration 145/1000 | Loss: 0.00002350
Iteration 146/1000 | Loss: 0.00002350
Iteration 147/1000 | Loss: 0.00002350
Iteration 148/1000 | Loss: 0.00002350
Iteration 149/1000 | Loss: 0.00002350
Iteration 150/1000 | Loss: 0.00002349
Iteration 151/1000 | Loss: 0.00002349
Iteration 152/1000 | Loss: 0.00002349
Iteration 153/1000 | Loss: 0.00002349
Iteration 154/1000 | Loss: 0.00002349
Iteration 155/1000 | Loss: 0.00002348
Iteration 156/1000 | Loss: 0.00002348
Iteration 157/1000 | Loss: 0.00002348
Iteration 158/1000 | Loss: 0.00002348
Iteration 159/1000 | Loss: 0.00002348
Iteration 160/1000 | Loss: 0.00002347
Iteration 161/1000 | Loss: 0.00002347
Iteration 162/1000 | Loss: 0.00002347
Iteration 163/1000 | Loss: 0.00002347
Iteration 164/1000 | Loss: 0.00002347
Iteration 165/1000 | Loss: 0.00002347
Iteration 166/1000 | Loss: 0.00002347
Iteration 167/1000 | Loss: 0.00002347
Iteration 168/1000 | Loss: 0.00002347
Iteration 169/1000 | Loss: 0.00002347
Iteration 170/1000 | Loss: 0.00002347
Iteration 171/1000 | Loss: 0.00002347
Iteration 172/1000 | Loss: 0.00002347
Iteration 173/1000 | Loss: 0.00002347
Iteration 174/1000 | Loss: 0.00002347
Iteration 175/1000 | Loss: 0.00002347
Iteration 176/1000 | Loss: 0.00002347
Iteration 177/1000 | Loss: 0.00002347
Iteration 178/1000 | Loss: 0.00002346
Iteration 179/1000 | Loss: 0.00002346
Iteration 180/1000 | Loss: 0.00002346
Iteration 181/1000 | Loss: 0.00002346
Iteration 182/1000 | Loss: 0.00002346
Iteration 183/1000 | Loss: 0.00002346
Iteration 184/1000 | Loss: 0.00002346
Iteration 185/1000 | Loss: 0.00002346
Iteration 186/1000 | Loss: 0.00002346
Iteration 187/1000 | Loss: 0.00002346
Iteration 188/1000 | Loss: 0.00002346
Iteration 189/1000 | Loss: 0.00002346
Iteration 190/1000 | Loss: 0.00002346
Iteration 191/1000 | Loss: 0.00002346
Iteration 192/1000 | Loss: 0.00002346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [2.3463746401830576e-05, 2.3463746401830576e-05, 2.3463746401830576e-05, 2.3463746401830576e-05, 2.3463746401830576e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3463746401830576e-05

Optimization complete. Final v2v error: 3.9811110496520996 mm

Highest mean error: 7.696539402008057 mm for frame 153

Lowest mean error: 3.319075107574463 mm for frame 23

Saving results

Total time: 233.98939299583435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00807526
Iteration 2/25 | Loss: 0.00149504
Iteration 3/25 | Loss: 0.00132351
Iteration 4/25 | Loss: 0.00130694
Iteration 5/25 | Loss: 0.00130196
Iteration 6/25 | Loss: 0.00129994
Iteration 7/25 | Loss: 0.00129964
Iteration 8/25 | Loss: 0.00129964
Iteration 9/25 | Loss: 0.00129964
Iteration 10/25 | Loss: 0.00129964
Iteration 11/25 | Loss: 0.00129964
Iteration 12/25 | Loss: 0.00129964
Iteration 13/25 | Loss: 0.00129964
Iteration 14/25 | Loss: 0.00129964
Iteration 15/25 | Loss: 0.00129964
Iteration 16/25 | Loss: 0.00129964
Iteration 17/25 | Loss: 0.00129964
Iteration 18/25 | Loss: 0.00129964
Iteration 19/25 | Loss: 0.00129964
Iteration 20/25 | Loss: 0.00129964
Iteration 21/25 | Loss: 0.00129964
Iteration 22/25 | Loss: 0.00129964
Iteration 23/25 | Loss: 0.00129964
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001299640047363937, 0.001299640047363937, 0.001299640047363937, 0.001299640047363937, 0.001299640047363937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001299640047363937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.20728505
Iteration 2/25 | Loss: 0.00218222
Iteration 3/25 | Loss: 0.00218222
Iteration 4/25 | Loss: 0.00218222
Iteration 5/25 | Loss: 0.00218222
Iteration 6/25 | Loss: 0.00218222
Iteration 7/25 | Loss: 0.00218222
Iteration 8/25 | Loss: 0.00218222
Iteration 9/25 | Loss: 0.00218222
Iteration 10/25 | Loss: 0.00218222
Iteration 11/25 | Loss: 0.00218222
Iteration 12/25 | Loss: 0.00218222
Iteration 13/25 | Loss: 0.00218222
Iteration 14/25 | Loss: 0.00218222
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.002182216150686145, 0.002182216150686145, 0.002182216150686145, 0.002182216150686145, 0.002182216150686145]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002182216150686145

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00218222
Iteration 2/1000 | Loss: 0.00006834
Iteration 3/1000 | Loss: 0.00004996
Iteration 4/1000 | Loss: 0.00003919
Iteration 5/1000 | Loss: 0.00003742
Iteration 6/1000 | Loss: 0.00003652
Iteration 7/1000 | Loss: 0.00003566
Iteration 8/1000 | Loss: 0.00003446
Iteration 9/1000 | Loss: 0.00003343
Iteration 10/1000 | Loss: 0.00003293
Iteration 11/1000 | Loss: 0.00003251
Iteration 12/1000 | Loss: 0.00003210
Iteration 13/1000 | Loss: 0.00003187
Iteration 14/1000 | Loss: 0.00003166
Iteration 15/1000 | Loss: 0.00003154
Iteration 16/1000 | Loss: 0.00003146
Iteration 17/1000 | Loss: 0.00003142
Iteration 18/1000 | Loss: 0.00003139
Iteration 19/1000 | Loss: 0.00003139
Iteration 20/1000 | Loss: 0.00003127
Iteration 21/1000 | Loss: 0.00003127
Iteration 22/1000 | Loss: 0.00003127
Iteration 23/1000 | Loss: 0.00003125
Iteration 24/1000 | Loss: 0.00003125
Iteration 25/1000 | Loss: 0.00003125
Iteration 26/1000 | Loss: 0.00003125
Iteration 27/1000 | Loss: 0.00003125
Iteration 28/1000 | Loss: 0.00003125
Iteration 29/1000 | Loss: 0.00003125
Iteration 30/1000 | Loss: 0.00003123
Iteration 31/1000 | Loss: 0.00003121
Iteration 32/1000 | Loss: 0.00003121
Iteration 33/1000 | Loss: 0.00003121
Iteration 34/1000 | Loss: 0.00003121
Iteration 35/1000 | Loss: 0.00003121
Iteration 36/1000 | Loss: 0.00003120
Iteration 37/1000 | Loss: 0.00003120
Iteration 38/1000 | Loss: 0.00003120
Iteration 39/1000 | Loss: 0.00003120
Iteration 40/1000 | Loss: 0.00003120
Iteration 41/1000 | Loss: 0.00003120
Iteration 42/1000 | Loss: 0.00003120
Iteration 43/1000 | Loss: 0.00003120
Iteration 44/1000 | Loss: 0.00003120
Iteration 45/1000 | Loss: 0.00003119
Iteration 46/1000 | Loss: 0.00003119
Iteration 47/1000 | Loss: 0.00003119
Iteration 48/1000 | Loss: 0.00003118
Iteration 49/1000 | Loss: 0.00003118
Iteration 50/1000 | Loss: 0.00003118
Iteration 51/1000 | Loss: 0.00003117
Iteration 52/1000 | Loss: 0.00003117
Iteration 53/1000 | Loss: 0.00003116
Iteration 54/1000 | Loss: 0.00003115
Iteration 55/1000 | Loss: 0.00003115
Iteration 56/1000 | Loss: 0.00003114
Iteration 57/1000 | Loss: 0.00003113
Iteration 58/1000 | Loss: 0.00003113
Iteration 59/1000 | Loss: 0.00003112
Iteration 60/1000 | Loss: 0.00003112
Iteration 61/1000 | Loss: 0.00003112
Iteration 62/1000 | Loss: 0.00003112
Iteration 63/1000 | Loss: 0.00003112
Iteration 64/1000 | Loss: 0.00003112
Iteration 65/1000 | Loss: 0.00003112
Iteration 66/1000 | Loss: 0.00003112
Iteration 67/1000 | Loss: 0.00003111
Iteration 68/1000 | Loss: 0.00003111
Iteration 69/1000 | Loss: 0.00003111
Iteration 70/1000 | Loss: 0.00003111
Iteration 71/1000 | Loss: 0.00003111
Iteration 72/1000 | Loss: 0.00003111
Iteration 73/1000 | Loss: 0.00003111
Iteration 74/1000 | Loss: 0.00003110
Iteration 75/1000 | Loss: 0.00003110
Iteration 76/1000 | Loss: 0.00003110
Iteration 77/1000 | Loss: 0.00003110
Iteration 78/1000 | Loss: 0.00003110
Iteration 79/1000 | Loss: 0.00003110
Iteration 80/1000 | Loss: 0.00003110
Iteration 81/1000 | Loss: 0.00003109
Iteration 82/1000 | Loss: 0.00003109
Iteration 83/1000 | Loss: 0.00003109
Iteration 84/1000 | Loss: 0.00003109
Iteration 85/1000 | Loss: 0.00003109
Iteration 86/1000 | Loss: 0.00003109
Iteration 87/1000 | Loss: 0.00003109
Iteration 88/1000 | Loss: 0.00003108
Iteration 89/1000 | Loss: 0.00003108
Iteration 90/1000 | Loss: 0.00003108
Iteration 91/1000 | Loss: 0.00003108
Iteration 92/1000 | Loss: 0.00003108
Iteration 93/1000 | Loss: 0.00003108
Iteration 94/1000 | Loss: 0.00003108
Iteration 95/1000 | Loss: 0.00003108
Iteration 96/1000 | Loss: 0.00003108
Iteration 97/1000 | Loss: 0.00003108
Iteration 98/1000 | Loss: 0.00003107
Iteration 99/1000 | Loss: 0.00003107
Iteration 100/1000 | Loss: 0.00003107
Iteration 101/1000 | Loss: 0.00003107
Iteration 102/1000 | Loss: 0.00003107
Iteration 103/1000 | Loss: 0.00003107
Iteration 104/1000 | Loss: 0.00003107
Iteration 105/1000 | Loss: 0.00003107
Iteration 106/1000 | Loss: 0.00003107
Iteration 107/1000 | Loss: 0.00003107
Iteration 108/1000 | Loss: 0.00003107
Iteration 109/1000 | Loss: 0.00003106
Iteration 110/1000 | Loss: 0.00003106
Iteration 111/1000 | Loss: 0.00003106
Iteration 112/1000 | Loss: 0.00003106
Iteration 113/1000 | Loss: 0.00003106
Iteration 114/1000 | Loss: 0.00003106
Iteration 115/1000 | Loss: 0.00003106
Iteration 116/1000 | Loss: 0.00003106
Iteration 117/1000 | Loss: 0.00003106
Iteration 118/1000 | Loss: 0.00003105
Iteration 119/1000 | Loss: 0.00003105
Iteration 120/1000 | Loss: 0.00003105
Iteration 121/1000 | Loss: 0.00003105
Iteration 122/1000 | Loss: 0.00003105
Iteration 123/1000 | Loss: 0.00003105
Iteration 124/1000 | Loss: 0.00003105
Iteration 125/1000 | Loss: 0.00003105
Iteration 126/1000 | Loss: 0.00003104
Iteration 127/1000 | Loss: 0.00003104
Iteration 128/1000 | Loss: 0.00003104
Iteration 129/1000 | Loss: 0.00003104
Iteration 130/1000 | Loss: 0.00003104
Iteration 131/1000 | Loss: 0.00003104
Iteration 132/1000 | Loss: 0.00003104
Iteration 133/1000 | Loss: 0.00003104
Iteration 134/1000 | Loss: 0.00003104
Iteration 135/1000 | Loss: 0.00003104
Iteration 136/1000 | Loss: 0.00003104
Iteration 137/1000 | Loss: 0.00003104
Iteration 138/1000 | Loss: 0.00003104
Iteration 139/1000 | Loss: 0.00003104
Iteration 140/1000 | Loss: 0.00003104
Iteration 141/1000 | Loss: 0.00003103
Iteration 142/1000 | Loss: 0.00003103
Iteration 143/1000 | Loss: 0.00003103
Iteration 144/1000 | Loss: 0.00003103
Iteration 145/1000 | Loss: 0.00003103
Iteration 146/1000 | Loss: 0.00003103
Iteration 147/1000 | Loss: 0.00003103
Iteration 148/1000 | Loss: 0.00003103
Iteration 149/1000 | Loss: 0.00003103
Iteration 150/1000 | Loss: 0.00003103
Iteration 151/1000 | Loss: 0.00003103
Iteration 152/1000 | Loss: 0.00003103
Iteration 153/1000 | Loss: 0.00003103
Iteration 154/1000 | Loss: 0.00003103
Iteration 155/1000 | Loss: 0.00003103
Iteration 156/1000 | Loss: 0.00003103
Iteration 157/1000 | Loss: 0.00003103
Iteration 158/1000 | Loss: 0.00003103
Iteration 159/1000 | Loss: 0.00003103
Iteration 160/1000 | Loss: 0.00003103
Iteration 161/1000 | Loss: 0.00003103
Iteration 162/1000 | Loss: 0.00003103
Iteration 163/1000 | Loss: 0.00003103
Iteration 164/1000 | Loss: 0.00003103
Iteration 165/1000 | Loss: 0.00003103
Iteration 166/1000 | Loss: 0.00003103
Iteration 167/1000 | Loss: 0.00003103
Iteration 168/1000 | Loss: 0.00003103
Iteration 169/1000 | Loss: 0.00003103
Iteration 170/1000 | Loss: 0.00003103
Iteration 171/1000 | Loss: 0.00003103
Iteration 172/1000 | Loss: 0.00003103
Iteration 173/1000 | Loss: 0.00003103
Iteration 174/1000 | Loss: 0.00003103
Iteration 175/1000 | Loss: 0.00003103
Iteration 176/1000 | Loss: 0.00003103
Iteration 177/1000 | Loss: 0.00003103
Iteration 178/1000 | Loss: 0.00003103
Iteration 179/1000 | Loss: 0.00003103
Iteration 180/1000 | Loss: 0.00003103
Iteration 181/1000 | Loss: 0.00003103
Iteration 182/1000 | Loss: 0.00003103
Iteration 183/1000 | Loss: 0.00003103
Iteration 184/1000 | Loss: 0.00003103
Iteration 185/1000 | Loss: 0.00003103
Iteration 186/1000 | Loss: 0.00003103
Iteration 187/1000 | Loss: 0.00003103
Iteration 188/1000 | Loss: 0.00003103
Iteration 189/1000 | Loss: 0.00003103
Iteration 190/1000 | Loss: 0.00003103
Iteration 191/1000 | Loss: 0.00003103
Iteration 192/1000 | Loss: 0.00003103
Iteration 193/1000 | Loss: 0.00003103
Iteration 194/1000 | Loss: 0.00003103
Iteration 195/1000 | Loss: 0.00003103
Iteration 196/1000 | Loss: 0.00003103
Iteration 197/1000 | Loss: 0.00003103
Iteration 198/1000 | Loss: 0.00003103
Iteration 199/1000 | Loss: 0.00003103
Iteration 200/1000 | Loss: 0.00003103
Iteration 201/1000 | Loss: 0.00003103
Iteration 202/1000 | Loss: 0.00003103
Iteration 203/1000 | Loss: 0.00003103
Iteration 204/1000 | Loss: 0.00003103
Iteration 205/1000 | Loss: 0.00003103
Iteration 206/1000 | Loss: 0.00003103
Iteration 207/1000 | Loss: 0.00003103
Iteration 208/1000 | Loss: 0.00003103
Iteration 209/1000 | Loss: 0.00003103
Iteration 210/1000 | Loss: 0.00003103
Iteration 211/1000 | Loss: 0.00003103
Iteration 212/1000 | Loss: 0.00003103
Iteration 213/1000 | Loss: 0.00003103
Iteration 214/1000 | Loss: 0.00003103
Iteration 215/1000 | Loss: 0.00003103
Iteration 216/1000 | Loss: 0.00003103
Iteration 217/1000 | Loss: 0.00003103
Iteration 218/1000 | Loss: 0.00003103
Iteration 219/1000 | Loss: 0.00003103
Iteration 220/1000 | Loss: 0.00003103
Iteration 221/1000 | Loss: 0.00003103
Iteration 222/1000 | Loss: 0.00003103
Iteration 223/1000 | Loss: 0.00003103
Iteration 224/1000 | Loss: 0.00003103
Iteration 225/1000 | Loss: 0.00003103
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [3.10250798065681e-05, 3.10250798065681e-05, 3.10250798065681e-05, 3.10250798065681e-05, 3.10250798065681e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.10250798065681e-05

Optimization complete. Final v2v error: 4.49138879776001 mm

Highest mean error: 4.559637069702148 mm for frame 33

Lowest mean error: 4.343420028686523 mm for frame 4

Saving results

Total time: 44.0858838558197
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1092/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1092.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1092
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00987094
Iteration 2/25 | Loss: 0.00213979
Iteration 3/25 | Loss: 0.00166779
Iteration 4/25 | Loss: 0.00172435
Iteration 5/25 | Loss: 0.00166523
Iteration 6/25 | Loss: 0.00140982
Iteration 7/25 | Loss: 0.00133643
Iteration 8/25 | Loss: 0.00132526
Iteration 9/25 | Loss: 0.00130891
Iteration 10/25 | Loss: 0.00130345
Iteration 11/25 | Loss: 0.00129176
Iteration 12/25 | Loss: 0.00127384
Iteration 13/25 | Loss: 0.00127117
Iteration 14/25 | Loss: 0.00127020
Iteration 15/25 | Loss: 0.00127389
Iteration 16/25 | Loss: 0.00127145
Iteration 17/25 | Loss: 0.00126868
Iteration 18/25 | Loss: 0.00126608
Iteration 19/25 | Loss: 0.00126216
Iteration 20/25 | Loss: 0.00126673
Iteration 21/25 | Loss: 0.00126369
Iteration 22/25 | Loss: 0.00126323
Iteration 23/25 | Loss: 0.00126686
Iteration 24/25 | Loss: 0.00126930
Iteration 25/25 | Loss: 0.00126815

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36392224
Iteration 2/25 | Loss: 0.00167308
Iteration 3/25 | Loss: 0.00145340
Iteration 4/25 | Loss: 0.00145340
Iteration 5/25 | Loss: 0.00145340
Iteration 6/25 | Loss: 0.00145340
Iteration 7/25 | Loss: 0.00145340
Iteration 8/25 | Loss: 0.00145340
Iteration 9/25 | Loss: 0.00145340
Iteration 10/25 | Loss: 0.00145340
Iteration 11/25 | Loss: 0.00145340
Iteration 12/25 | Loss: 0.00145340
Iteration 13/25 | Loss: 0.00145340
Iteration 14/25 | Loss: 0.00145340
Iteration 15/25 | Loss: 0.00145340
Iteration 16/25 | Loss: 0.00145340
Iteration 17/25 | Loss: 0.00145340
Iteration 18/25 | Loss: 0.00145340
Iteration 19/25 | Loss: 0.00145340
Iteration 20/25 | Loss: 0.00145340
Iteration 21/25 | Loss: 0.00145340
Iteration 22/25 | Loss: 0.00145340
Iteration 23/25 | Loss: 0.00145340
Iteration 24/25 | Loss: 0.00145340
Iteration 25/25 | Loss: 0.00145340
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014533970970660448, 0.0014533970970660448, 0.0014533970970660448, 0.0014533970970660448, 0.0014533970970660448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014533970970660448

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145340
Iteration 2/1000 | Loss: 0.00041731
Iteration 3/1000 | Loss: 0.00012809
Iteration 4/1000 | Loss: 0.00011677
Iteration 5/1000 | Loss: 0.00010512
Iteration 6/1000 | Loss: 0.00012091
Iteration 7/1000 | Loss: 0.00028030
Iteration 8/1000 | Loss: 0.00016673
Iteration 9/1000 | Loss: 0.00020606
Iteration 10/1000 | Loss: 0.00003017
Iteration 11/1000 | Loss: 0.00005495
Iteration 12/1000 | Loss: 0.00003165
Iteration 13/1000 | Loss: 0.00002415
Iteration 14/1000 | Loss: 0.00024685
Iteration 15/1000 | Loss: 0.00033613
Iteration 16/1000 | Loss: 0.00016099
Iteration 17/1000 | Loss: 0.00004414
Iteration 18/1000 | Loss: 0.00003108
Iteration 19/1000 | Loss: 0.00002534
Iteration 20/1000 | Loss: 0.00005317
Iteration 21/1000 | Loss: 0.00002062
Iteration 22/1000 | Loss: 0.00004082
Iteration 23/1000 | Loss: 0.00004030
Iteration 24/1000 | Loss: 0.00001988
Iteration 25/1000 | Loss: 0.00001778
Iteration 26/1000 | Loss: 0.00005742
Iteration 27/1000 | Loss: 0.00001686
Iteration 28/1000 | Loss: 0.00004021
Iteration 29/1000 | Loss: 0.00001658
Iteration 30/1000 | Loss: 0.00001625
Iteration 31/1000 | Loss: 0.00001584
Iteration 32/1000 | Loss: 0.00003025
Iteration 33/1000 | Loss: 0.00001567
Iteration 34/1000 | Loss: 0.00001555
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001554
Iteration 37/1000 | Loss: 0.00001554
Iteration 38/1000 | Loss: 0.00001554
Iteration 39/1000 | Loss: 0.00001554
Iteration 40/1000 | Loss: 0.00001554
Iteration 41/1000 | Loss: 0.00001554
Iteration 42/1000 | Loss: 0.00001554
Iteration 43/1000 | Loss: 0.00001554
Iteration 44/1000 | Loss: 0.00001554
Iteration 45/1000 | Loss: 0.00001553
Iteration 46/1000 | Loss: 0.00001553
Iteration 47/1000 | Loss: 0.00001553
Iteration 48/1000 | Loss: 0.00001552
Iteration 49/1000 | Loss: 0.00001552
Iteration 50/1000 | Loss: 0.00001551
Iteration 51/1000 | Loss: 0.00001550
Iteration 52/1000 | Loss: 0.00012305
Iteration 53/1000 | Loss: 0.00002942
Iteration 54/1000 | Loss: 0.00002604
Iteration 55/1000 | Loss: 0.00002578
Iteration 56/1000 | Loss: 0.00002369
Iteration 57/1000 | Loss: 0.00001610
Iteration 58/1000 | Loss: 0.00002028
Iteration 59/1000 | Loss: 0.00001570
Iteration 60/1000 | Loss: 0.00001849
Iteration 61/1000 | Loss: 0.00001849
Iteration 62/1000 | Loss: 0.00003490
Iteration 63/1000 | Loss: 0.00001760
Iteration 64/1000 | Loss: 0.00002158
Iteration 65/1000 | Loss: 0.00019800
Iteration 66/1000 | Loss: 0.00020399
Iteration 67/1000 | Loss: 0.00065317
Iteration 68/1000 | Loss: 0.00008116
Iteration 69/1000 | Loss: 0.00035871
Iteration 70/1000 | Loss: 0.00002020
Iteration 71/1000 | Loss: 0.00001669
Iteration 72/1000 | Loss: 0.00001556
Iteration 73/1000 | Loss: 0.00001539
Iteration 74/1000 | Loss: 0.00001537
Iteration 75/1000 | Loss: 0.00001534
Iteration 76/1000 | Loss: 0.00001534
Iteration 77/1000 | Loss: 0.00001533
Iteration 78/1000 | Loss: 0.00001529
Iteration 79/1000 | Loss: 0.00001528
Iteration 80/1000 | Loss: 0.00001528
Iteration 81/1000 | Loss: 0.00001528
Iteration 82/1000 | Loss: 0.00001528
Iteration 83/1000 | Loss: 0.00001527
Iteration 84/1000 | Loss: 0.00001527
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001526
Iteration 87/1000 | Loss: 0.00001526
Iteration 88/1000 | Loss: 0.00001525
Iteration 89/1000 | Loss: 0.00001524
Iteration 90/1000 | Loss: 0.00001524
Iteration 91/1000 | Loss: 0.00001523
Iteration 92/1000 | Loss: 0.00001523
Iteration 93/1000 | Loss: 0.00001522
Iteration 94/1000 | Loss: 0.00001522
Iteration 95/1000 | Loss: 0.00001522
Iteration 96/1000 | Loss: 0.00001522
Iteration 97/1000 | Loss: 0.00001522
Iteration 98/1000 | Loss: 0.00001522
Iteration 99/1000 | Loss: 0.00001522
Iteration 100/1000 | Loss: 0.00001521
Iteration 101/1000 | Loss: 0.00001521
Iteration 102/1000 | Loss: 0.00001520
Iteration 103/1000 | Loss: 0.00001519
Iteration 104/1000 | Loss: 0.00001518
Iteration 105/1000 | Loss: 0.00001518
Iteration 106/1000 | Loss: 0.00004139
Iteration 107/1000 | Loss: 0.00001563
Iteration 108/1000 | Loss: 0.00001510
Iteration 109/1000 | Loss: 0.00001507
Iteration 110/1000 | Loss: 0.00001507
Iteration 111/1000 | Loss: 0.00001507
Iteration 112/1000 | Loss: 0.00001507
Iteration 113/1000 | Loss: 0.00001507
Iteration 114/1000 | Loss: 0.00001507
Iteration 115/1000 | Loss: 0.00001507
Iteration 116/1000 | Loss: 0.00001507
Iteration 117/1000 | Loss: 0.00001507
Iteration 118/1000 | Loss: 0.00001506
Iteration 119/1000 | Loss: 0.00001506
Iteration 120/1000 | Loss: 0.00001506
Iteration 121/1000 | Loss: 0.00001506
Iteration 122/1000 | Loss: 0.00001506
Iteration 123/1000 | Loss: 0.00001506
Iteration 124/1000 | Loss: 0.00001506
Iteration 125/1000 | Loss: 0.00001506
Iteration 126/1000 | Loss: 0.00001506
Iteration 127/1000 | Loss: 0.00001506
Iteration 128/1000 | Loss: 0.00001506
Iteration 129/1000 | Loss: 0.00001506
Iteration 130/1000 | Loss: 0.00001506
Iteration 131/1000 | Loss: 0.00001506
Iteration 132/1000 | Loss: 0.00001506
Iteration 133/1000 | Loss: 0.00001506
Iteration 134/1000 | Loss: 0.00001506
Iteration 135/1000 | Loss: 0.00001506
Iteration 136/1000 | Loss: 0.00001506
Iteration 137/1000 | Loss: 0.00001506
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 137. Stopping optimization.
Last 5 losses: [1.5060663827171084e-05, 1.5060663827171084e-05, 1.5060663827171084e-05, 1.5060663827171084e-05, 1.5060663827171084e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5060663827171084e-05

Optimization complete. Final v2v error: 3.159161329269409 mm

Highest mean error: 5.285764217376709 mm for frame 68

Lowest mean error: 2.6644270420074463 mm for frame 147

Saving results

Total time: 133.4792993068695
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00640046
Iteration 2/25 | Loss: 0.00135736
Iteration 3/25 | Loss: 0.00126957
Iteration 4/25 | Loss: 0.00125955
Iteration 5/25 | Loss: 0.00125701
Iteration 6/25 | Loss: 0.00125677
Iteration 7/25 | Loss: 0.00125677
Iteration 8/25 | Loss: 0.00125677
Iteration 9/25 | Loss: 0.00125677
Iteration 10/25 | Loss: 0.00125677
Iteration 11/25 | Loss: 0.00125677
Iteration 12/25 | Loss: 0.00125677
Iteration 13/25 | Loss: 0.00125677
Iteration 14/25 | Loss: 0.00125677
Iteration 15/25 | Loss: 0.00125677
Iteration 16/25 | Loss: 0.00125677
Iteration 17/25 | Loss: 0.00125677
Iteration 18/25 | Loss: 0.00125677
Iteration 19/25 | Loss: 0.00125677
Iteration 20/25 | Loss: 0.00125677
Iteration 21/25 | Loss: 0.00125677
Iteration 22/25 | Loss: 0.00125677
Iteration 23/25 | Loss: 0.00125677
Iteration 24/25 | Loss: 0.00125677
Iteration 25/25 | Loss: 0.00125677

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.03805399
Iteration 2/25 | Loss: 0.00111353
Iteration 3/25 | Loss: 0.00111346
Iteration 4/25 | Loss: 0.00111346
Iteration 5/25 | Loss: 0.00111346
Iteration 6/25 | Loss: 0.00111346
Iteration 7/25 | Loss: 0.00111346
Iteration 8/25 | Loss: 0.00111346
Iteration 9/25 | Loss: 0.00111346
Iteration 10/25 | Loss: 0.00111346
Iteration 11/25 | Loss: 0.00111346
Iteration 12/25 | Loss: 0.00111346
Iteration 13/25 | Loss: 0.00111346
Iteration 14/25 | Loss: 0.00111346
Iteration 15/25 | Loss: 0.00111346
Iteration 16/25 | Loss: 0.00111346
Iteration 17/25 | Loss: 0.00111346
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001113459817133844, 0.001113459817133844, 0.001113459817133844, 0.001113459817133844, 0.001113459817133844]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001113459817133844

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111346
Iteration 2/1000 | Loss: 0.00002198
Iteration 3/1000 | Loss: 0.00001771
Iteration 4/1000 | Loss: 0.00001662
Iteration 5/1000 | Loss: 0.00001611
Iteration 6/1000 | Loss: 0.00001561
Iteration 7/1000 | Loss: 0.00001521
Iteration 8/1000 | Loss: 0.00001483
Iteration 9/1000 | Loss: 0.00001452
Iteration 10/1000 | Loss: 0.00001431
Iteration 11/1000 | Loss: 0.00001413
Iteration 12/1000 | Loss: 0.00001412
Iteration 13/1000 | Loss: 0.00001398
Iteration 14/1000 | Loss: 0.00001393
Iteration 15/1000 | Loss: 0.00001392
Iteration 16/1000 | Loss: 0.00001384
Iteration 17/1000 | Loss: 0.00001383
Iteration 18/1000 | Loss: 0.00001382
Iteration 19/1000 | Loss: 0.00001381
Iteration 20/1000 | Loss: 0.00001381
Iteration 21/1000 | Loss: 0.00001381
Iteration 22/1000 | Loss: 0.00001380
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001376
Iteration 25/1000 | Loss: 0.00001376
Iteration 26/1000 | Loss: 0.00001376
Iteration 27/1000 | Loss: 0.00001374
Iteration 28/1000 | Loss: 0.00001373
Iteration 29/1000 | Loss: 0.00001369
Iteration 30/1000 | Loss: 0.00001366
Iteration 31/1000 | Loss: 0.00001366
Iteration 32/1000 | Loss: 0.00001361
Iteration 33/1000 | Loss: 0.00001361
Iteration 34/1000 | Loss: 0.00001360
Iteration 35/1000 | Loss: 0.00001360
Iteration 36/1000 | Loss: 0.00001359
Iteration 37/1000 | Loss: 0.00001358
Iteration 38/1000 | Loss: 0.00001358
Iteration 39/1000 | Loss: 0.00001358
Iteration 40/1000 | Loss: 0.00001357
Iteration 41/1000 | Loss: 0.00001357
Iteration 42/1000 | Loss: 0.00001357
Iteration 43/1000 | Loss: 0.00001357
Iteration 44/1000 | Loss: 0.00001356
Iteration 45/1000 | Loss: 0.00001356
Iteration 46/1000 | Loss: 0.00001356
Iteration 47/1000 | Loss: 0.00001355
Iteration 48/1000 | Loss: 0.00001355
Iteration 49/1000 | Loss: 0.00001354
Iteration 50/1000 | Loss: 0.00001354
Iteration 51/1000 | Loss: 0.00001354
Iteration 52/1000 | Loss: 0.00001354
Iteration 53/1000 | Loss: 0.00001354
Iteration 54/1000 | Loss: 0.00001354
Iteration 55/1000 | Loss: 0.00001354
Iteration 56/1000 | Loss: 0.00001353
Iteration 57/1000 | Loss: 0.00001353
Iteration 58/1000 | Loss: 0.00001352
Iteration 59/1000 | Loss: 0.00001352
Iteration 60/1000 | Loss: 0.00001351
Iteration 61/1000 | Loss: 0.00001351
Iteration 62/1000 | Loss: 0.00001350
Iteration 63/1000 | Loss: 0.00001350
Iteration 64/1000 | Loss: 0.00001350
Iteration 65/1000 | Loss: 0.00001349
Iteration 66/1000 | Loss: 0.00001349
Iteration 67/1000 | Loss: 0.00001349
Iteration 68/1000 | Loss: 0.00001349
Iteration 69/1000 | Loss: 0.00001349
Iteration 70/1000 | Loss: 0.00001349
Iteration 71/1000 | Loss: 0.00001349
Iteration 72/1000 | Loss: 0.00001348
Iteration 73/1000 | Loss: 0.00001348
Iteration 74/1000 | Loss: 0.00001348
Iteration 75/1000 | Loss: 0.00001348
Iteration 76/1000 | Loss: 0.00001348
Iteration 77/1000 | Loss: 0.00001348
Iteration 78/1000 | Loss: 0.00001348
Iteration 79/1000 | Loss: 0.00001347
Iteration 80/1000 | Loss: 0.00001347
Iteration 81/1000 | Loss: 0.00001347
Iteration 82/1000 | Loss: 0.00001347
Iteration 83/1000 | Loss: 0.00001347
Iteration 84/1000 | Loss: 0.00001347
Iteration 85/1000 | Loss: 0.00001347
Iteration 86/1000 | Loss: 0.00001347
Iteration 87/1000 | Loss: 0.00001347
Iteration 88/1000 | Loss: 0.00001347
Iteration 89/1000 | Loss: 0.00001347
Iteration 90/1000 | Loss: 0.00001347
Iteration 91/1000 | Loss: 0.00001347
Iteration 92/1000 | Loss: 0.00001346
Iteration 93/1000 | Loss: 0.00001346
Iteration 94/1000 | Loss: 0.00001346
Iteration 95/1000 | Loss: 0.00001345
Iteration 96/1000 | Loss: 0.00001345
Iteration 97/1000 | Loss: 0.00001345
Iteration 98/1000 | Loss: 0.00001345
Iteration 99/1000 | Loss: 0.00001345
Iteration 100/1000 | Loss: 0.00001345
Iteration 101/1000 | Loss: 0.00001345
Iteration 102/1000 | Loss: 0.00001344
Iteration 103/1000 | Loss: 0.00001344
Iteration 104/1000 | Loss: 0.00001344
Iteration 105/1000 | Loss: 0.00001344
Iteration 106/1000 | Loss: 0.00001344
Iteration 107/1000 | Loss: 0.00001344
Iteration 108/1000 | Loss: 0.00001343
Iteration 109/1000 | Loss: 0.00001343
Iteration 110/1000 | Loss: 0.00001343
Iteration 111/1000 | Loss: 0.00001343
Iteration 112/1000 | Loss: 0.00001343
Iteration 113/1000 | Loss: 0.00001343
Iteration 114/1000 | Loss: 0.00001343
Iteration 115/1000 | Loss: 0.00001342
Iteration 116/1000 | Loss: 0.00001342
Iteration 117/1000 | Loss: 0.00001342
Iteration 118/1000 | Loss: 0.00001342
Iteration 119/1000 | Loss: 0.00001342
Iteration 120/1000 | Loss: 0.00001342
Iteration 121/1000 | Loss: 0.00001342
Iteration 122/1000 | Loss: 0.00001342
Iteration 123/1000 | Loss: 0.00001342
Iteration 124/1000 | Loss: 0.00001342
Iteration 125/1000 | Loss: 0.00001342
Iteration 126/1000 | Loss: 0.00001342
Iteration 127/1000 | Loss: 0.00001342
Iteration 128/1000 | Loss: 0.00001342
Iteration 129/1000 | Loss: 0.00001341
Iteration 130/1000 | Loss: 0.00001341
Iteration 131/1000 | Loss: 0.00001341
Iteration 132/1000 | Loss: 0.00001341
Iteration 133/1000 | Loss: 0.00001341
Iteration 134/1000 | Loss: 0.00001341
Iteration 135/1000 | Loss: 0.00001341
Iteration 136/1000 | Loss: 0.00001341
Iteration 137/1000 | Loss: 0.00001341
Iteration 138/1000 | Loss: 0.00001341
Iteration 139/1000 | Loss: 0.00001341
Iteration 140/1000 | Loss: 0.00001341
Iteration 141/1000 | Loss: 0.00001341
Iteration 142/1000 | Loss: 0.00001341
Iteration 143/1000 | Loss: 0.00001341
Iteration 144/1000 | Loss: 0.00001341
Iteration 145/1000 | Loss: 0.00001341
Iteration 146/1000 | Loss: 0.00001341
Iteration 147/1000 | Loss: 0.00001341
Iteration 148/1000 | Loss: 0.00001341
Iteration 149/1000 | Loss: 0.00001341
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.340784365311265e-05, 1.340784365311265e-05, 1.340784365311265e-05, 1.340784365311265e-05, 1.340784365311265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.340784365311265e-05

Optimization complete. Final v2v error: 3.104440927505493 mm

Highest mean error: 3.6623427867889404 mm for frame 140

Lowest mean error: 2.8687729835510254 mm for frame 16

Saving results

Total time: 39.55859708786011
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00726906
Iteration 2/25 | Loss: 0.00174201
Iteration 3/25 | Loss: 0.00147891
Iteration 4/25 | Loss: 0.00139113
Iteration 5/25 | Loss: 0.00139429
Iteration 6/25 | Loss: 0.00140181
Iteration 7/25 | Loss: 0.00133077
Iteration 8/25 | Loss: 0.00130314
Iteration 9/25 | Loss: 0.00128417
Iteration 10/25 | Loss: 0.00128297
Iteration 11/25 | Loss: 0.00128506
Iteration 12/25 | Loss: 0.00127704
Iteration 13/25 | Loss: 0.00127190
Iteration 14/25 | Loss: 0.00127019
Iteration 15/25 | Loss: 0.00126956
Iteration 16/25 | Loss: 0.00126998
Iteration 17/25 | Loss: 0.00126996
Iteration 18/25 | Loss: 0.00126939
Iteration 19/25 | Loss: 0.00126979
Iteration 20/25 | Loss: 0.00126968
Iteration 21/25 | Loss: 0.00126979
Iteration 22/25 | Loss: 0.00126983
Iteration 23/25 | Loss: 0.00126982
Iteration 24/25 | Loss: 0.00126982
Iteration 25/25 | Loss: 0.00126982

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.04786205
Iteration 2/25 | Loss: 0.00140226
Iteration 3/25 | Loss: 0.00138298
Iteration 4/25 | Loss: 0.00138297
Iteration 5/25 | Loss: 0.00138297
Iteration 6/25 | Loss: 0.00138297
Iteration 7/25 | Loss: 0.00138297
Iteration 8/25 | Loss: 0.00138297
Iteration 9/25 | Loss: 0.00138297
Iteration 10/25 | Loss: 0.00138297
Iteration 11/25 | Loss: 0.00138297
Iteration 12/25 | Loss: 0.00138297
Iteration 13/25 | Loss: 0.00138297
Iteration 14/25 | Loss: 0.00138297
Iteration 15/25 | Loss: 0.00138297
Iteration 16/25 | Loss: 0.00138297
Iteration 17/25 | Loss: 0.00138297
Iteration 18/25 | Loss: 0.00138297
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013829674571752548, 0.0013829674571752548, 0.0013829674571752548, 0.0013829674571752548, 0.0013829674571752548]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013829674571752548

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138297
Iteration 2/1000 | Loss: 0.00007224
Iteration 3/1000 | Loss: 0.00003896
Iteration 4/1000 | Loss: 0.00003763
Iteration 5/1000 | Loss: 0.00003922
Iteration 6/1000 | Loss: 0.00003021
Iteration 7/1000 | Loss: 0.00003653
Iteration 8/1000 | Loss: 0.00002621
Iteration 9/1000 | Loss: 0.00004211
Iteration 10/1000 | Loss: 0.00002868
Iteration 11/1000 | Loss: 0.00002638
Iteration 12/1000 | Loss: 0.00002520
Iteration 13/1000 | Loss: 0.00007714
Iteration 14/1000 | Loss: 0.00003782
Iteration 15/1000 | Loss: 0.00003414
Iteration 16/1000 | Loss: 0.00003989
Iteration 17/1000 | Loss: 0.00004037
Iteration 18/1000 | Loss: 0.00003435
Iteration 19/1000 | Loss: 0.00003328
Iteration 20/1000 | Loss: 0.00003255
Iteration 21/1000 | Loss: 0.00003259
Iteration 22/1000 | Loss: 0.00002709
Iteration 23/1000 | Loss: 0.00003421
Iteration 24/1000 | Loss: 0.00004012
Iteration 25/1000 | Loss: 0.00002945
Iteration 26/1000 | Loss: 0.00002441
Iteration 27/1000 | Loss: 0.00003442
Iteration 28/1000 | Loss: 0.00006395
Iteration 29/1000 | Loss: 0.00003329
Iteration 30/1000 | Loss: 0.00006341
Iteration 31/1000 | Loss: 0.00003444
Iteration 32/1000 | Loss: 0.00003279
Iteration 33/1000 | Loss: 0.00003147
Iteration 34/1000 | Loss: 0.00006373
Iteration 35/1000 | Loss: 0.00003263
Iteration 36/1000 | Loss: 0.00003438
Iteration 37/1000 | Loss: 0.00002931
Iteration 38/1000 | Loss: 0.00003375
Iteration 39/1000 | Loss: 0.00002674
Iteration 40/1000 | Loss: 0.00002348
Iteration 41/1000 | Loss: 0.00002252
Iteration 42/1000 | Loss: 0.00002212
Iteration 43/1000 | Loss: 0.00002179
Iteration 44/1000 | Loss: 0.00002152
Iteration 45/1000 | Loss: 0.00002126
Iteration 46/1000 | Loss: 0.00002106
Iteration 47/1000 | Loss: 0.00002101
Iteration 48/1000 | Loss: 0.00002099
Iteration 49/1000 | Loss: 0.00002096
Iteration 50/1000 | Loss: 0.00002094
Iteration 51/1000 | Loss: 0.00002086
Iteration 52/1000 | Loss: 0.00002082
Iteration 53/1000 | Loss: 0.00002082
Iteration 54/1000 | Loss: 0.00002082
Iteration 55/1000 | Loss: 0.00002082
Iteration 56/1000 | Loss: 0.00002082
Iteration 57/1000 | Loss: 0.00002082
Iteration 58/1000 | Loss: 0.00002082
Iteration 59/1000 | Loss: 0.00002082
Iteration 60/1000 | Loss: 0.00002082
Iteration 61/1000 | Loss: 0.00002081
Iteration 62/1000 | Loss: 0.00002081
Iteration 63/1000 | Loss: 0.00002081
Iteration 64/1000 | Loss: 0.00002081
Iteration 65/1000 | Loss: 0.00002081
Iteration 66/1000 | Loss: 0.00002081
Iteration 67/1000 | Loss: 0.00002081
Iteration 68/1000 | Loss: 0.00002080
Iteration 69/1000 | Loss: 0.00002080
Iteration 70/1000 | Loss: 0.00002080
Iteration 71/1000 | Loss: 0.00002079
Iteration 72/1000 | Loss: 0.00002079
Iteration 73/1000 | Loss: 0.00002079
Iteration 74/1000 | Loss: 0.00002079
Iteration 75/1000 | Loss: 0.00002079
Iteration 76/1000 | Loss: 0.00002079
Iteration 77/1000 | Loss: 0.00002079
Iteration 78/1000 | Loss: 0.00002078
Iteration 79/1000 | Loss: 0.00002078
Iteration 80/1000 | Loss: 0.00002078
Iteration 81/1000 | Loss: 0.00002078
Iteration 82/1000 | Loss: 0.00002078
Iteration 83/1000 | Loss: 0.00002078
Iteration 84/1000 | Loss: 0.00002078
Iteration 85/1000 | Loss: 0.00002078
Iteration 86/1000 | Loss: 0.00002078
Iteration 87/1000 | Loss: 0.00002078
Iteration 88/1000 | Loss: 0.00002078
Iteration 89/1000 | Loss: 0.00002077
Iteration 90/1000 | Loss: 0.00002077
Iteration 91/1000 | Loss: 0.00002077
Iteration 92/1000 | Loss: 0.00002077
Iteration 93/1000 | Loss: 0.00002077
Iteration 94/1000 | Loss: 0.00002077
Iteration 95/1000 | Loss: 0.00002077
Iteration 96/1000 | Loss: 0.00002077
Iteration 97/1000 | Loss: 0.00002077
Iteration 98/1000 | Loss: 0.00002077
Iteration 99/1000 | Loss: 0.00002077
Iteration 100/1000 | Loss: 0.00002077
Iteration 101/1000 | Loss: 0.00002077
Iteration 102/1000 | Loss: 0.00002076
Iteration 103/1000 | Loss: 0.00002076
Iteration 104/1000 | Loss: 0.00002076
Iteration 105/1000 | Loss: 0.00002076
Iteration 106/1000 | Loss: 0.00002076
Iteration 107/1000 | Loss: 0.00002076
Iteration 108/1000 | Loss: 0.00002076
Iteration 109/1000 | Loss: 0.00002076
Iteration 110/1000 | Loss: 0.00002076
Iteration 111/1000 | Loss: 0.00002076
Iteration 112/1000 | Loss: 0.00002075
Iteration 113/1000 | Loss: 0.00002075
Iteration 114/1000 | Loss: 0.00002075
Iteration 115/1000 | Loss: 0.00002075
Iteration 116/1000 | Loss: 0.00002075
Iteration 117/1000 | Loss: 0.00002075
Iteration 118/1000 | Loss: 0.00002075
Iteration 119/1000 | Loss: 0.00002075
Iteration 120/1000 | Loss: 0.00002075
Iteration 121/1000 | Loss: 0.00002075
Iteration 122/1000 | Loss: 0.00002075
Iteration 123/1000 | Loss: 0.00002075
Iteration 124/1000 | Loss: 0.00002075
Iteration 125/1000 | Loss: 0.00002075
Iteration 126/1000 | Loss: 0.00002074
Iteration 127/1000 | Loss: 0.00002074
Iteration 128/1000 | Loss: 0.00002074
Iteration 129/1000 | Loss: 0.00002074
Iteration 130/1000 | Loss: 0.00002074
Iteration 131/1000 | Loss: 0.00002074
Iteration 132/1000 | Loss: 0.00002074
Iteration 133/1000 | Loss: 0.00002074
Iteration 134/1000 | Loss: 0.00002074
Iteration 135/1000 | Loss: 0.00002074
Iteration 136/1000 | Loss: 0.00002074
Iteration 137/1000 | Loss: 0.00002073
Iteration 138/1000 | Loss: 0.00002073
Iteration 139/1000 | Loss: 0.00002073
Iteration 140/1000 | Loss: 0.00002073
Iteration 141/1000 | Loss: 0.00002072
Iteration 142/1000 | Loss: 0.00002070
Iteration 143/1000 | Loss: 0.00002070
Iteration 144/1000 | Loss: 0.00002069
Iteration 145/1000 | Loss: 0.00002068
Iteration 146/1000 | Loss: 0.00018602
Iteration 147/1000 | Loss: 0.00014440
Iteration 148/1000 | Loss: 0.00004488
Iteration 149/1000 | Loss: 0.00002709
Iteration 150/1000 | Loss: 0.00002314
Iteration 151/1000 | Loss: 0.00002136
Iteration 152/1000 | Loss: 0.00002059
Iteration 153/1000 | Loss: 0.00001983
Iteration 154/1000 | Loss: 0.00001924
Iteration 155/1000 | Loss: 0.00001889
Iteration 156/1000 | Loss: 0.00001879
Iteration 157/1000 | Loss: 0.00001860
Iteration 158/1000 | Loss: 0.00001840
Iteration 159/1000 | Loss: 0.00001836
Iteration 160/1000 | Loss: 0.00001835
Iteration 161/1000 | Loss: 0.00001835
Iteration 162/1000 | Loss: 0.00001832
Iteration 163/1000 | Loss: 0.00001832
Iteration 164/1000 | Loss: 0.00001831
Iteration 165/1000 | Loss: 0.00001827
Iteration 166/1000 | Loss: 0.00001818
Iteration 167/1000 | Loss: 0.00001817
Iteration 168/1000 | Loss: 0.00001812
Iteration 169/1000 | Loss: 0.00001804
Iteration 170/1000 | Loss: 0.00001803
Iteration 171/1000 | Loss: 0.00001803
Iteration 172/1000 | Loss: 0.00001802
Iteration 173/1000 | Loss: 0.00001800
Iteration 174/1000 | Loss: 0.00001795
Iteration 175/1000 | Loss: 0.00001795
Iteration 176/1000 | Loss: 0.00001795
Iteration 177/1000 | Loss: 0.00001795
Iteration 178/1000 | Loss: 0.00001795
Iteration 179/1000 | Loss: 0.00001794
Iteration 180/1000 | Loss: 0.00001794
Iteration 181/1000 | Loss: 0.00001794
Iteration 182/1000 | Loss: 0.00001793
Iteration 183/1000 | Loss: 0.00001793
Iteration 184/1000 | Loss: 0.00001793
Iteration 185/1000 | Loss: 0.00001793
Iteration 186/1000 | Loss: 0.00001793
Iteration 187/1000 | Loss: 0.00001793
Iteration 188/1000 | Loss: 0.00001793
Iteration 189/1000 | Loss: 0.00001792
Iteration 190/1000 | Loss: 0.00001792
Iteration 191/1000 | Loss: 0.00001792
Iteration 192/1000 | Loss: 0.00001791
Iteration 193/1000 | Loss: 0.00001791
Iteration 194/1000 | Loss: 0.00001791
Iteration 195/1000 | Loss: 0.00001791
Iteration 196/1000 | Loss: 0.00001790
Iteration 197/1000 | Loss: 0.00001790
Iteration 198/1000 | Loss: 0.00001790
Iteration 199/1000 | Loss: 0.00001790
Iteration 200/1000 | Loss: 0.00001790
Iteration 201/1000 | Loss: 0.00001790
Iteration 202/1000 | Loss: 0.00001789
Iteration 203/1000 | Loss: 0.00001789
Iteration 204/1000 | Loss: 0.00001789
Iteration 205/1000 | Loss: 0.00001789
Iteration 206/1000 | Loss: 0.00001788
Iteration 207/1000 | Loss: 0.00001788
Iteration 208/1000 | Loss: 0.00001788
Iteration 209/1000 | Loss: 0.00001788
Iteration 210/1000 | Loss: 0.00001788
Iteration 211/1000 | Loss: 0.00001787
Iteration 212/1000 | Loss: 0.00001787
Iteration 213/1000 | Loss: 0.00001787
Iteration 214/1000 | Loss: 0.00001787
Iteration 215/1000 | Loss: 0.00001787
Iteration 216/1000 | Loss: 0.00001787
Iteration 217/1000 | Loss: 0.00001787
Iteration 218/1000 | Loss: 0.00001786
Iteration 219/1000 | Loss: 0.00001786
Iteration 220/1000 | Loss: 0.00001786
Iteration 221/1000 | Loss: 0.00001786
Iteration 222/1000 | Loss: 0.00001786
Iteration 223/1000 | Loss: 0.00001786
Iteration 224/1000 | Loss: 0.00001785
Iteration 225/1000 | Loss: 0.00001785
Iteration 226/1000 | Loss: 0.00001785
Iteration 227/1000 | Loss: 0.00001785
Iteration 228/1000 | Loss: 0.00001785
Iteration 229/1000 | Loss: 0.00001785
Iteration 230/1000 | Loss: 0.00001785
Iteration 231/1000 | Loss: 0.00001785
Iteration 232/1000 | Loss: 0.00001785
Iteration 233/1000 | Loss: 0.00001785
Iteration 234/1000 | Loss: 0.00001785
Iteration 235/1000 | Loss: 0.00001785
Iteration 236/1000 | Loss: 0.00001784
Iteration 237/1000 | Loss: 0.00001784
Iteration 238/1000 | Loss: 0.00001784
Iteration 239/1000 | Loss: 0.00001784
Iteration 240/1000 | Loss: 0.00001784
Iteration 241/1000 | Loss: 0.00001784
Iteration 242/1000 | Loss: 0.00001784
Iteration 243/1000 | Loss: 0.00001784
Iteration 244/1000 | Loss: 0.00001784
Iteration 245/1000 | Loss: 0.00001784
Iteration 246/1000 | Loss: 0.00001784
Iteration 247/1000 | Loss: 0.00001784
Iteration 248/1000 | Loss: 0.00001784
Iteration 249/1000 | Loss: 0.00001784
Iteration 250/1000 | Loss: 0.00001784
Iteration 251/1000 | Loss: 0.00001784
Iteration 252/1000 | Loss: 0.00001784
Iteration 253/1000 | Loss: 0.00001784
Iteration 254/1000 | Loss: 0.00001784
Iteration 255/1000 | Loss: 0.00001784
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.7837413906818256e-05, 1.7837413906818256e-05, 1.7837413906818256e-05, 1.7837413906818256e-05, 1.7837413906818256e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7837413906818256e-05

Optimization complete. Final v2v error: 3.516237735748291 mm

Highest mean error: 4.7041401863098145 mm for frame 105

Lowest mean error: 2.714716911315918 mm for frame 148

Saving results

Total time: 163.43373489379883
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818441
Iteration 2/25 | Loss: 0.00125133
Iteration 3/25 | Loss: 0.00120785
Iteration 4/25 | Loss: 0.00120002
Iteration 5/25 | Loss: 0.00119860
Iteration 6/25 | Loss: 0.00119860
Iteration 7/25 | Loss: 0.00119860
Iteration 8/25 | Loss: 0.00119860
Iteration 9/25 | Loss: 0.00119860
Iteration 10/25 | Loss: 0.00119860
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001198599231429398, 0.001198599231429398, 0.001198599231429398, 0.001198599231429398, 0.001198599231429398]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001198599231429398

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.98835325
Iteration 2/25 | Loss: 0.00126867
Iteration 3/25 | Loss: 0.00126865
Iteration 4/25 | Loss: 0.00126865
Iteration 5/25 | Loss: 0.00126865
Iteration 6/25 | Loss: 0.00126865
Iteration 7/25 | Loss: 0.00126865
Iteration 8/25 | Loss: 0.00126865
Iteration 9/25 | Loss: 0.00126865
Iteration 10/25 | Loss: 0.00126864
Iteration 11/25 | Loss: 0.00126864
Iteration 12/25 | Loss: 0.00126864
Iteration 13/25 | Loss: 0.00126864
Iteration 14/25 | Loss: 0.00126864
Iteration 15/25 | Loss: 0.00126864
Iteration 16/25 | Loss: 0.00126864
Iteration 17/25 | Loss: 0.00126864
Iteration 18/25 | Loss: 0.00126864
Iteration 19/25 | Loss: 0.00126864
Iteration 20/25 | Loss: 0.00126864
Iteration 21/25 | Loss: 0.00126864
Iteration 22/25 | Loss: 0.00126864
Iteration 23/25 | Loss: 0.00126864
Iteration 24/25 | Loss: 0.00126864
Iteration 25/25 | Loss: 0.00126864

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126864
Iteration 2/1000 | Loss: 0.00002738
Iteration 3/1000 | Loss: 0.00002119
Iteration 4/1000 | Loss: 0.00001833
Iteration 5/1000 | Loss: 0.00001677
Iteration 6/1000 | Loss: 0.00001592
Iteration 7/1000 | Loss: 0.00001546
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001464
Iteration 10/1000 | Loss: 0.00001436
Iteration 11/1000 | Loss: 0.00001435
Iteration 12/1000 | Loss: 0.00001427
Iteration 13/1000 | Loss: 0.00001410
Iteration 14/1000 | Loss: 0.00001408
Iteration 15/1000 | Loss: 0.00001403
Iteration 16/1000 | Loss: 0.00001399
Iteration 17/1000 | Loss: 0.00001397
Iteration 18/1000 | Loss: 0.00001395
Iteration 19/1000 | Loss: 0.00001387
Iteration 20/1000 | Loss: 0.00001383
Iteration 21/1000 | Loss: 0.00001371
Iteration 22/1000 | Loss: 0.00001366
Iteration 23/1000 | Loss: 0.00001361
Iteration 24/1000 | Loss: 0.00001355
Iteration 25/1000 | Loss: 0.00001355
Iteration 26/1000 | Loss: 0.00001353
Iteration 27/1000 | Loss: 0.00001350
Iteration 28/1000 | Loss: 0.00001347
Iteration 29/1000 | Loss: 0.00001346
Iteration 30/1000 | Loss: 0.00001345
Iteration 31/1000 | Loss: 0.00001344
Iteration 32/1000 | Loss: 0.00001343
Iteration 33/1000 | Loss: 0.00001343
Iteration 34/1000 | Loss: 0.00001341
Iteration 35/1000 | Loss: 0.00001341
Iteration 36/1000 | Loss: 0.00001340
Iteration 37/1000 | Loss: 0.00001339
Iteration 38/1000 | Loss: 0.00001339
Iteration 39/1000 | Loss: 0.00001339
Iteration 40/1000 | Loss: 0.00001338
Iteration 41/1000 | Loss: 0.00001338
Iteration 42/1000 | Loss: 0.00001337
Iteration 43/1000 | Loss: 0.00001337
Iteration 44/1000 | Loss: 0.00001336
Iteration 45/1000 | Loss: 0.00001336
Iteration 46/1000 | Loss: 0.00001335
Iteration 47/1000 | Loss: 0.00001335
Iteration 48/1000 | Loss: 0.00001335
Iteration 49/1000 | Loss: 0.00001334
Iteration 50/1000 | Loss: 0.00001334
Iteration 51/1000 | Loss: 0.00001333
Iteration 52/1000 | Loss: 0.00001333
Iteration 53/1000 | Loss: 0.00001332
Iteration 54/1000 | Loss: 0.00001332
Iteration 55/1000 | Loss: 0.00001331
Iteration 56/1000 | Loss: 0.00001330
Iteration 57/1000 | Loss: 0.00001330
Iteration 58/1000 | Loss: 0.00001329
Iteration 59/1000 | Loss: 0.00001329
Iteration 60/1000 | Loss: 0.00001328
Iteration 61/1000 | Loss: 0.00001328
Iteration 62/1000 | Loss: 0.00001328
Iteration 63/1000 | Loss: 0.00001328
Iteration 64/1000 | Loss: 0.00001327
Iteration 65/1000 | Loss: 0.00001327
Iteration 66/1000 | Loss: 0.00001326
Iteration 67/1000 | Loss: 0.00001326
Iteration 68/1000 | Loss: 0.00001326
Iteration 69/1000 | Loss: 0.00001325
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001323
Iteration 73/1000 | Loss: 0.00001323
Iteration 74/1000 | Loss: 0.00001323
Iteration 75/1000 | Loss: 0.00001323
Iteration 76/1000 | Loss: 0.00001322
Iteration 77/1000 | Loss: 0.00001322
Iteration 78/1000 | Loss: 0.00001322
Iteration 79/1000 | Loss: 0.00001322
Iteration 80/1000 | Loss: 0.00001322
Iteration 81/1000 | Loss: 0.00001322
Iteration 82/1000 | Loss: 0.00001322
Iteration 83/1000 | Loss: 0.00001322
Iteration 84/1000 | Loss: 0.00001322
Iteration 85/1000 | Loss: 0.00001321
Iteration 86/1000 | Loss: 0.00001321
Iteration 87/1000 | Loss: 0.00001321
Iteration 88/1000 | Loss: 0.00001321
Iteration 89/1000 | Loss: 0.00001321
Iteration 90/1000 | Loss: 0.00001320
Iteration 91/1000 | Loss: 0.00001320
Iteration 92/1000 | Loss: 0.00001320
Iteration 93/1000 | Loss: 0.00001320
Iteration 94/1000 | Loss: 0.00001319
Iteration 95/1000 | Loss: 0.00001319
Iteration 96/1000 | Loss: 0.00001319
Iteration 97/1000 | Loss: 0.00001319
Iteration 98/1000 | Loss: 0.00001319
Iteration 99/1000 | Loss: 0.00001319
Iteration 100/1000 | Loss: 0.00001319
Iteration 101/1000 | Loss: 0.00001319
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001318
Iteration 105/1000 | Loss: 0.00001318
Iteration 106/1000 | Loss: 0.00001318
Iteration 107/1000 | Loss: 0.00001318
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00001317
Iteration 113/1000 | Loss: 0.00001317
Iteration 114/1000 | Loss: 0.00001317
Iteration 115/1000 | Loss: 0.00001317
Iteration 116/1000 | Loss: 0.00001316
Iteration 117/1000 | Loss: 0.00001316
Iteration 118/1000 | Loss: 0.00001316
Iteration 119/1000 | Loss: 0.00001316
Iteration 120/1000 | Loss: 0.00001316
Iteration 121/1000 | Loss: 0.00001316
Iteration 122/1000 | Loss: 0.00001316
Iteration 123/1000 | Loss: 0.00001315
Iteration 124/1000 | Loss: 0.00001315
Iteration 125/1000 | Loss: 0.00001315
Iteration 126/1000 | Loss: 0.00001315
Iteration 127/1000 | Loss: 0.00001315
Iteration 128/1000 | Loss: 0.00001315
Iteration 129/1000 | Loss: 0.00001315
Iteration 130/1000 | Loss: 0.00001315
Iteration 131/1000 | Loss: 0.00001315
Iteration 132/1000 | Loss: 0.00001315
Iteration 133/1000 | Loss: 0.00001315
Iteration 134/1000 | Loss: 0.00001315
Iteration 135/1000 | Loss: 0.00001315
Iteration 136/1000 | Loss: 0.00001315
Iteration 137/1000 | Loss: 0.00001315
Iteration 138/1000 | Loss: 0.00001315
Iteration 139/1000 | Loss: 0.00001315
Iteration 140/1000 | Loss: 0.00001314
Iteration 141/1000 | Loss: 0.00001314
Iteration 142/1000 | Loss: 0.00001314
Iteration 143/1000 | Loss: 0.00001314
Iteration 144/1000 | Loss: 0.00001314
Iteration 145/1000 | Loss: 0.00001314
Iteration 146/1000 | Loss: 0.00001314
Iteration 147/1000 | Loss: 0.00001314
Iteration 148/1000 | Loss: 0.00001314
Iteration 149/1000 | Loss: 0.00001314
Iteration 150/1000 | Loss: 0.00001314
Iteration 151/1000 | Loss: 0.00001314
Iteration 152/1000 | Loss: 0.00001314
Iteration 153/1000 | Loss: 0.00001314
Iteration 154/1000 | Loss: 0.00001314
Iteration 155/1000 | Loss: 0.00001314
Iteration 156/1000 | Loss: 0.00001313
Iteration 157/1000 | Loss: 0.00001313
Iteration 158/1000 | Loss: 0.00001313
Iteration 159/1000 | Loss: 0.00001313
Iteration 160/1000 | Loss: 0.00001313
Iteration 161/1000 | Loss: 0.00001313
Iteration 162/1000 | Loss: 0.00001313
Iteration 163/1000 | Loss: 0.00001313
Iteration 164/1000 | Loss: 0.00001313
Iteration 165/1000 | Loss: 0.00001313
Iteration 166/1000 | Loss: 0.00001313
Iteration 167/1000 | Loss: 0.00001313
Iteration 168/1000 | Loss: 0.00001313
Iteration 169/1000 | Loss: 0.00001313
Iteration 170/1000 | Loss: 0.00001313
Iteration 171/1000 | Loss: 0.00001313
Iteration 172/1000 | Loss: 0.00001313
Iteration 173/1000 | Loss: 0.00001313
Iteration 174/1000 | Loss: 0.00001313
Iteration 175/1000 | Loss: 0.00001313
Iteration 176/1000 | Loss: 0.00001313
Iteration 177/1000 | Loss: 0.00001313
Iteration 178/1000 | Loss: 0.00001313
Iteration 179/1000 | Loss: 0.00001313
Iteration 180/1000 | Loss: 0.00001313
Iteration 181/1000 | Loss: 0.00001313
Iteration 182/1000 | Loss: 0.00001313
Iteration 183/1000 | Loss: 0.00001313
Iteration 184/1000 | Loss: 0.00001313
Iteration 185/1000 | Loss: 0.00001313
Iteration 186/1000 | Loss: 0.00001313
Iteration 187/1000 | Loss: 0.00001313
Iteration 188/1000 | Loss: 0.00001313
Iteration 189/1000 | Loss: 0.00001313
Iteration 190/1000 | Loss: 0.00001313
Iteration 191/1000 | Loss: 0.00001313
Iteration 192/1000 | Loss: 0.00001313
Iteration 193/1000 | Loss: 0.00001313
Iteration 194/1000 | Loss: 0.00001313
Iteration 195/1000 | Loss: 0.00001313
Iteration 196/1000 | Loss: 0.00001313
Iteration 197/1000 | Loss: 0.00001313
Iteration 198/1000 | Loss: 0.00001313
Iteration 199/1000 | Loss: 0.00001313
Iteration 200/1000 | Loss: 0.00001313
Iteration 201/1000 | Loss: 0.00001313
Iteration 202/1000 | Loss: 0.00001313
Iteration 203/1000 | Loss: 0.00001313
Iteration 204/1000 | Loss: 0.00001313
Iteration 205/1000 | Loss: 0.00001313
Iteration 206/1000 | Loss: 0.00001313
Iteration 207/1000 | Loss: 0.00001313
Iteration 208/1000 | Loss: 0.00001313
Iteration 209/1000 | Loss: 0.00001313
Iteration 210/1000 | Loss: 0.00001313
Iteration 211/1000 | Loss: 0.00001313
Iteration 212/1000 | Loss: 0.00001313
Iteration 213/1000 | Loss: 0.00001313
Iteration 214/1000 | Loss: 0.00001313
Iteration 215/1000 | Loss: 0.00001313
Iteration 216/1000 | Loss: 0.00001313
Iteration 217/1000 | Loss: 0.00001313
Iteration 218/1000 | Loss: 0.00001313
Iteration 219/1000 | Loss: 0.00001313
Iteration 220/1000 | Loss: 0.00001313
Iteration 221/1000 | Loss: 0.00001313
Iteration 222/1000 | Loss: 0.00001313
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 222. Stopping optimization.
Last 5 losses: [1.3131010746292304e-05, 1.3131010746292304e-05, 1.3131010746292304e-05, 1.3131010746292304e-05, 1.3131010746292304e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3131010746292304e-05

Optimization complete. Final v2v error: 3.1017725467681885 mm

Highest mean error: 3.727442979812622 mm for frame 141

Lowest mean error: 2.6996119022369385 mm for frame 22

Saving results

Total time: 48.568862438201904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00829088
Iteration 2/25 | Loss: 0.00138757
Iteration 3/25 | Loss: 0.00124105
Iteration 4/25 | Loss: 0.00119024
Iteration 5/25 | Loss: 0.00118587
Iteration 6/25 | Loss: 0.00118215
Iteration 7/25 | Loss: 0.00118150
Iteration 8/25 | Loss: 0.00118135
Iteration 9/25 | Loss: 0.00118135
Iteration 10/25 | Loss: 0.00118135
Iteration 11/25 | Loss: 0.00118135
Iteration 12/25 | Loss: 0.00118135
Iteration 13/25 | Loss: 0.00118135
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.001181345316581428, 0.001181345316581428, 0.001181345316581428, 0.001181345316581428, 0.001181345316581428]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001181345316581428

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.66445541
Iteration 2/25 | Loss: 0.00117684
Iteration 3/25 | Loss: 0.00117684
Iteration 4/25 | Loss: 0.00117684
Iteration 5/25 | Loss: 0.00117684
Iteration 6/25 | Loss: 0.00117684
Iteration 7/25 | Loss: 0.00117683
Iteration 8/25 | Loss: 0.00117683
Iteration 9/25 | Loss: 0.00117683
Iteration 10/25 | Loss: 0.00117683
Iteration 11/25 | Loss: 0.00117683
Iteration 12/25 | Loss: 0.00117683
Iteration 13/25 | Loss: 0.00117683
Iteration 14/25 | Loss: 0.00117683
Iteration 15/25 | Loss: 0.00117683
Iteration 16/25 | Loss: 0.00117683
Iteration 17/25 | Loss: 0.00117683
Iteration 18/25 | Loss: 0.00117683
Iteration 19/25 | Loss: 0.00117683
Iteration 20/25 | Loss: 0.00117683
Iteration 21/25 | Loss: 0.00117683
Iteration 22/25 | Loss: 0.00117683
Iteration 23/25 | Loss: 0.00117683
Iteration 24/25 | Loss: 0.00117683
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011768331751227379, 0.0011768331751227379, 0.0011768331751227379, 0.0011768331751227379, 0.0011768331751227379]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011768331751227379

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117683
Iteration 2/1000 | Loss: 0.00001964
Iteration 3/1000 | Loss: 0.00001424
Iteration 4/1000 | Loss: 0.00001368
Iteration 5/1000 | Loss: 0.00001598
Iteration 6/1000 | Loss: 0.00001375
Iteration 7/1000 | Loss: 0.00001162
Iteration 8/1000 | Loss: 0.00001129
Iteration 9/1000 | Loss: 0.00001093
Iteration 10/1000 | Loss: 0.00001067
Iteration 11/1000 | Loss: 0.00002787
Iteration 12/1000 | Loss: 0.00001058
Iteration 13/1000 | Loss: 0.00001656
Iteration 14/1000 | Loss: 0.00001042
Iteration 15/1000 | Loss: 0.00001030
Iteration 16/1000 | Loss: 0.00001030
Iteration 17/1000 | Loss: 0.00001030
Iteration 18/1000 | Loss: 0.00001030
Iteration 19/1000 | Loss: 0.00001030
Iteration 20/1000 | Loss: 0.00001030
Iteration 21/1000 | Loss: 0.00001030
Iteration 22/1000 | Loss: 0.00001030
Iteration 23/1000 | Loss: 0.00001030
Iteration 24/1000 | Loss: 0.00001029
Iteration 25/1000 | Loss: 0.00001029
Iteration 26/1000 | Loss: 0.00001029
Iteration 27/1000 | Loss: 0.00001029
Iteration 28/1000 | Loss: 0.00001029
Iteration 29/1000 | Loss: 0.00001029
Iteration 30/1000 | Loss: 0.00001029
Iteration 31/1000 | Loss: 0.00001029
Iteration 32/1000 | Loss: 0.00001029
Iteration 33/1000 | Loss: 0.00001028
Iteration 34/1000 | Loss: 0.00001025
Iteration 35/1000 | Loss: 0.00002199
Iteration 36/1000 | Loss: 0.00003924
Iteration 37/1000 | Loss: 0.00001685
Iteration 38/1000 | Loss: 0.00001011
Iteration 39/1000 | Loss: 0.00001008
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001007
Iteration 42/1000 | Loss: 0.00001007
Iteration 43/1000 | Loss: 0.00001007
Iteration 44/1000 | Loss: 0.00001007
Iteration 45/1000 | Loss: 0.00001007
Iteration 46/1000 | Loss: 0.00001007
Iteration 47/1000 | Loss: 0.00001007
Iteration 48/1000 | Loss: 0.00001006
Iteration 49/1000 | Loss: 0.00001006
Iteration 50/1000 | Loss: 0.00001006
Iteration 51/1000 | Loss: 0.00001005
Iteration 52/1000 | Loss: 0.00001005
Iteration 53/1000 | Loss: 0.00001005
Iteration 54/1000 | Loss: 0.00001004
Iteration 55/1000 | Loss: 0.00001003
Iteration 56/1000 | Loss: 0.00001003
Iteration 57/1000 | Loss: 0.00001003
Iteration 58/1000 | Loss: 0.00001003
Iteration 59/1000 | Loss: 0.00001002
Iteration 60/1000 | Loss: 0.00001002
Iteration 61/1000 | Loss: 0.00001002
Iteration 62/1000 | Loss: 0.00001002
Iteration 63/1000 | Loss: 0.00001002
Iteration 64/1000 | Loss: 0.00001002
Iteration 65/1000 | Loss: 0.00001002
Iteration 66/1000 | Loss: 0.00001001
Iteration 67/1000 | Loss: 0.00001001
Iteration 68/1000 | Loss: 0.00001000
Iteration 69/1000 | Loss: 0.00000998
Iteration 70/1000 | Loss: 0.00000997
Iteration 71/1000 | Loss: 0.00000997
Iteration 72/1000 | Loss: 0.00000996
Iteration 73/1000 | Loss: 0.00000995
Iteration 74/1000 | Loss: 0.00000994
Iteration 75/1000 | Loss: 0.00000994
Iteration 76/1000 | Loss: 0.00000993
Iteration 77/1000 | Loss: 0.00000992
Iteration 78/1000 | Loss: 0.00000992
Iteration 79/1000 | Loss: 0.00000992
Iteration 80/1000 | Loss: 0.00000992
Iteration 81/1000 | Loss: 0.00000992
Iteration 82/1000 | Loss: 0.00000992
Iteration 83/1000 | Loss: 0.00000992
Iteration 84/1000 | Loss: 0.00000992
Iteration 85/1000 | Loss: 0.00000992
Iteration 86/1000 | Loss: 0.00000991
Iteration 87/1000 | Loss: 0.00000991
Iteration 88/1000 | Loss: 0.00000990
Iteration 89/1000 | Loss: 0.00000990
Iteration 90/1000 | Loss: 0.00000990
Iteration 91/1000 | Loss: 0.00000990
Iteration 92/1000 | Loss: 0.00000990
Iteration 93/1000 | Loss: 0.00000990
Iteration 94/1000 | Loss: 0.00000989
Iteration 95/1000 | Loss: 0.00000989
Iteration 96/1000 | Loss: 0.00000989
Iteration 97/1000 | Loss: 0.00000989
Iteration 98/1000 | Loss: 0.00000989
Iteration 99/1000 | Loss: 0.00000989
Iteration 100/1000 | Loss: 0.00000989
Iteration 101/1000 | Loss: 0.00000989
Iteration 102/1000 | Loss: 0.00000989
Iteration 103/1000 | Loss: 0.00000989
Iteration 104/1000 | Loss: 0.00000989
Iteration 105/1000 | Loss: 0.00000989
Iteration 106/1000 | Loss: 0.00000989
Iteration 107/1000 | Loss: 0.00000989
Iteration 108/1000 | Loss: 0.00000989
Iteration 109/1000 | Loss: 0.00000989
Iteration 110/1000 | Loss: 0.00000989
Iteration 111/1000 | Loss: 0.00000989
Iteration 112/1000 | Loss: 0.00000988
Iteration 113/1000 | Loss: 0.00000988
Iteration 114/1000 | Loss: 0.00000988
Iteration 115/1000 | Loss: 0.00000988
Iteration 116/1000 | Loss: 0.00000988
Iteration 117/1000 | Loss: 0.00000988
Iteration 118/1000 | Loss: 0.00000988
Iteration 119/1000 | Loss: 0.00000988
Iteration 120/1000 | Loss: 0.00000988
Iteration 121/1000 | Loss: 0.00000988
Iteration 122/1000 | Loss: 0.00000987
Iteration 123/1000 | Loss: 0.00000987
Iteration 124/1000 | Loss: 0.00000987
Iteration 125/1000 | Loss: 0.00000987
Iteration 126/1000 | Loss: 0.00000987
Iteration 127/1000 | Loss: 0.00000987
Iteration 128/1000 | Loss: 0.00000987
Iteration 129/1000 | Loss: 0.00000987
Iteration 130/1000 | Loss: 0.00000986
Iteration 131/1000 | Loss: 0.00000986
Iteration 132/1000 | Loss: 0.00000986
Iteration 133/1000 | Loss: 0.00000986
Iteration 134/1000 | Loss: 0.00000986
Iteration 135/1000 | Loss: 0.00000986
Iteration 136/1000 | Loss: 0.00000986
Iteration 137/1000 | Loss: 0.00000986
Iteration 138/1000 | Loss: 0.00000986
Iteration 139/1000 | Loss: 0.00000986
Iteration 140/1000 | Loss: 0.00000986
Iteration 141/1000 | Loss: 0.00000986
Iteration 142/1000 | Loss: 0.00000986
Iteration 143/1000 | Loss: 0.00000985
Iteration 144/1000 | Loss: 0.00000985
Iteration 145/1000 | Loss: 0.00000985
Iteration 146/1000 | Loss: 0.00000985
Iteration 147/1000 | Loss: 0.00000985
Iteration 148/1000 | Loss: 0.00000985
Iteration 149/1000 | Loss: 0.00000985
Iteration 150/1000 | Loss: 0.00000985
Iteration 151/1000 | Loss: 0.00000985
Iteration 152/1000 | Loss: 0.00000985
Iteration 153/1000 | Loss: 0.00000985
Iteration 154/1000 | Loss: 0.00000985
Iteration 155/1000 | Loss: 0.00000985
Iteration 156/1000 | Loss: 0.00000985
Iteration 157/1000 | Loss: 0.00000985
Iteration 158/1000 | Loss: 0.00000985
Iteration 159/1000 | Loss: 0.00000985
Iteration 160/1000 | Loss: 0.00000985
Iteration 161/1000 | Loss: 0.00000985
Iteration 162/1000 | Loss: 0.00000985
Iteration 163/1000 | Loss: 0.00000985
Iteration 164/1000 | Loss: 0.00000985
Iteration 165/1000 | Loss: 0.00000985
Iteration 166/1000 | Loss: 0.00000985
Iteration 167/1000 | Loss: 0.00000985
Iteration 168/1000 | Loss: 0.00000985
Iteration 169/1000 | Loss: 0.00000985
Iteration 170/1000 | Loss: 0.00000985
Iteration 171/1000 | Loss: 0.00000985
Iteration 172/1000 | Loss: 0.00000985
Iteration 173/1000 | Loss: 0.00000985
Iteration 174/1000 | Loss: 0.00000985
Iteration 175/1000 | Loss: 0.00000985
Iteration 176/1000 | Loss: 0.00000985
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [9.853588380792644e-06, 9.853588380792644e-06, 9.853588380792644e-06, 9.853588380792644e-06, 9.853588380792644e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.853588380792644e-06

Optimization complete. Final v2v error: 2.732145071029663 mm

Highest mean error: 3.2878401279449463 mm for frame 43

Lowest mean error: 2.545252799987793 mm for frame 226

Saving results

Total time: 52.614914417266846
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00486499
Iteration 2/25 | Loss: 0.00147821
Iteration 3/25 | Loss: 0.00129045
Iteration 4/25 | Loss: 0.00126875
Iteration 5/25 | Loss: 0.00126460
Iteration 6/25 | Loss: 0.00126348
Iteration 7/25 | Loss: 0.00126348
Iteration 8/25 | Loss: 0.00126348
Iteration 9/25 | Loss: 0.00126348
Iteration 10/25 | Loss: 0.00126348
Iteration 11/25 | Loss: 0.00126348
Iteration 12/25 | Loss: 0.00126348
Iteration 13/25 | Loss: 0.00126348
Iteration 14/25 | Loss: 0.00126348
Iteration 15/25 | Loss: 0.00126348
Iteration 16/25 | Loss: 0.00126348
Iteration 17/25 | Loss: 0.00126348
Iteration 18/25 | Loss: 0.00126348
Iteration 19/25 | Loss: 0.00126348
Iteration 20/25 | Loss: 0.00126348
Iteration 21/25 | Loss: 0.00126348
Iteration 22/25 | Loss: 0.00126348
Iteration 23/25 | Loss: 0.00126348
Iteration 24/25 | Loss: 0.00126348
Iteration 25/25 | Loss: 0.00126348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32845771
Iteration 2/25 | Loss: 0.00121459
Iteration 3/25 | Loss: 0.00121457
Iteration 4/25 | Loss: 0.00121457
Iteration 5/25 | Loss: 0.00121457
Iteration 6/25 | Loss: 0.00121457
Iteration 7/25 | Loss: 0.00121457
Iteration 8/25 | Loss: 0.00121457
Iteration 9/25 | Loss: 0.00121457
Iteration 10/25 | Loss: 0.00121457
Iteration 11/25 | Loss: 0.00121457
Iteration 12/25 | Loss: 0.00121457
Iteration 13/25 | Loss: 0.00121457
Iteration 14/25 | Loss: 0.00121457
Iteration 15/25 | Loss: 0.00121457
Iteration 16/25 | Loss: 0.00121457
Iteration 17/25 | Loss: 0.00121457
Iteration 18/25 | Loss: 0.00121457
Iteration 19/25 | Loss: 0.00121457
Iteration 20/25 | Loss: 0.00121457
Iteration 21/25 | Loss: 0.00121457
Iteration 22/25 | Loss: 0.00121457
Iteration 23/25 | Loss: 0.00121457
Iteration 24/25 | Loss: 0.00121457
Iteration 25/25 | Loss: 0.00121457

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121457
Iteration 2/1000 | Loss: 0.00004600
Iteration 3/1000 | Loss: 0.00002800
Iteration 4/1000 | Loss: 0.00002358
Iteration 5/1000 | Loss: 0.00002153
Iteration 6/1000 | Loss: 0.00001977
Iteration 7/1000 | Loss: 0.00001882
Iteration 8/1000 | Loss: 0.00001813
Iteration 9/1000 | Loss: 0.00001764
Iteration 10/1000 | Loss: 0.00001718
Iteration 11/1000 | Loss: 0.00001689
Iteration 12/1000 | Loss: 0.00001662
Iteration 13/1000 | Loss: 0.00001647
Iteration 14/1000 | Loss: 0.00001644
Iteration 15/1000 | Loss: 0.00001644
Iteration 16/1000 | Loss: 0.00001639
Iteration 17/1000 | Loss: 0.00001639
Iteration 18/1000 | Loss: 0.00001638
Iteration 19/1000 | Loss: 0.00001636
Iteration 20/1000 | Loss: 0.00001625
Iteration 21/1000 | Loss: 0.00001622
Iteration 22/1000 | Loss: 0.00001616
Iteration 23/1000 | Loss: 0.00001614
Iteration 24/1000 | Loss: 0.00001606
Iteration 25/1000 | Loss: 0.00001601
Iteration 26/1000 | Loss: 0.00001599
Iteration 27/1000 | Loss: 0.00001599
Iteration 28/1000 | Loss: 0.00001598
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001594
Iteration 33/1000 | Loss: 0.00001592
Iteration 34/1000 | Loss: 0.00001592
Iteration 35/1000 | Loss: 0.00001591
Iteration 36/1000 | Loss: 0.00001591
Iteration 37/1000 | Loss: 0.00001591
Iteration 38/1000 | Loss: 0.00001590
Iteration 39/1000 | Loss: 0.00001589
Iteration 40/1000 | Loss: 0.00001589
Iteration 41/1000 | Loss: 0.00001588
Iteration 42/1000 | Loss: 0.00001587
Iteration 43/1000 | Loss: 0.00001586
Iteration 44/1000 | Loss: 0.00001585
Iteration 45/1000 | Loss: 0.00001585
Iteration 46/1000 | Loss: 0.00001585
Iteration 47/1000 | Loss: 0.00001583
Iteration 48/1000 | Loss: 0.00001582
Iteration 49/1000 | Loss: 0.00001582
Iteration 50/1000 | Loss: 0.00001580
Iteration 51/1000 | Loss: 0.00001578
Iteration 52/1000 | Loss: 0.00001578
Iteration 53/1000 | Loss: 0.00001577
Iteration 54/1000 | Loss: 0.00001576
Iteration 55/1000 | Loss: 0.00001576
Iteration 56/1000 | Loss: 0.00001576
Iteration 57/1000 | Loss: 0.00001576
Iteration 58/1000 | Loss: 0.00001576
Iteration 59/1000 | Loss: 0.00001575
Iteration 60/1000 | Loss: 0.00001575
Iteration 61/1000 | Loss: 0.00001574
Iteration 62/1000 | Loss: 0.00001574
Iteration 63/1000 | Loss: 0.00001574
Iteration 64/1000 | Loss: 0.00001574
Iteration 65/1000 | Loss: 0.00001573
Iteration 66/1000 | Loss: 0.00001573
Iteration 67/1000 | Loss: 0.00001573
Iteration 68/1000 | Loss: 0.00001573
Iteration 69/1000 | Loss: 0.00001572
Iteration 70/1000 | Loss: 0.00001572
Iteration 71/1000 | Loss: 0.00001572
Iteration 72/1000 | Loss: 0.00001572
Iteration 73/1000 | Loss: 0.00001572
Iteration 74/1000 | Loss: 0.00001572
Iteration 75/1000 | Loss: 0.00001572
Iteration 76/1000 | Loss: 0.00001572
Iteration 77/1000 | Loss: 0.00001572
Iteration 78/1000 | Loss: 0.00001572
Iteration 79/1000 | Loss: 0.00001572
Iteration 80/1000 | Loss: 0.00001571
Iteration 81/1000 | Loss: 0.00001571
Iteration 82/1000 | Loss: 0.00001571
Iteration 83/1000 | Loss: 0.00001570
Iteration 84/1000 | Loss: 0.00001570
Iteration 85/1000 | Loss: 0.00001570
Iteration 86/1000 | Loss: 0.00001569
Iteration 87/1000 | Loss: 0.00001569
Iteration 88/1000 | Loss: 0.00001569
Iteration 89/1000 | Loss: 0.00001569
Iteration 90/1000 | Loss: 0.00001569
Iteration 91/1000 | Loss: 0.00001568
Iteration 92/1000 | Loss: 0.00001568
Iteration 93/1000 | Loss: 0.00001568
Iteration 94/1000 | Loss: 0.00001568
Iteration 95/1000 | Loss: 0.00001568
Iteration 96/1000 | Loss: 0.00001567
Iteration 97/1000 | Loss: 0.00001567
Iteration 98/1000 | Loss: 0.00001567
Iteration 99/1000 | Loss: 0.00001567
Iteration 100/1000 | Loss: 0.00001567
Iteration 101/1000 | Loss: 0.00001567
Iteration 102/1000 | Loss: 0.00001567
Iteration 103/1000 | Loss: 0.00001567
Iteration 104/1000 | Loss: 0.00001567
Iteration 105/1000 | Loss: 0.00001567
Iteration 106/1000 | Loss: 0.00001566
Iteration 107/1000 | Loss: 0.00001566
Iteration 108/1000 | Loss: 0.00001566
Iteration 109/1000 | Loss: 0.00001566
Iteration 110/1000 | Loss: 0.00001566
Iteration 111/1000 | Loss: 0.00001565
Iteration 112/1000 | Loss: 0.00001565
Iteration 113/1000 | Loss: 0.00001565
Iteration 114/1000 | Loss: 0.00001565
Iteration 115/1000 | Loss: 0.00001565
Iteration 116/1000 | Loss: 0.00001565
Iteration 117/1000 | Loss: 0.00001565
Iteration 118/1000 | Loss: 0.00001565
Iteration 119/1000 | Loss: 0.00001564
Iteration 120/1000 | Loss: 0.00001564
Iteration 121/1000 | Loss: 0.00001564
Iteration 122/1000 | Loss: 0.00001564
Iteration 123/1000 | Loss: 0.00001564
Iteration 124/1000 | Loss: 0.00001564
Iteration 125/1000 | Loss: 0.00001564
Iteration 126/1000 | Loss: 0.00001564
Iteration 127/1000 | Loss: 0.00001564
Iteration 128/1000 | Loss: 0.00001564
Iteration 129/1000 | Loss: 0.00001563
Iteration 130/1000 | Loss: 0.00001563
Iteration 131/1000 | Loss: 0.00001563
Iteration 132/1000 | Loss: 0.00001563
Iteration 133/1000 | Loss: 0.00001563
Iteration 134/1000 | Loss: 0.00001563
Iteration 135/1000 | Loss: 0.00001563
Iteration 136/1000 | Loss: 0.00001563
Iteration 137/1000 | Loss: 0.00001563
Iteration 138/1000 | Loss: 0.00001563
Iteration 139/1000 | Loss: 0.00001563
Iteration 140/1000 | Loss: 0.00001562
Iteration 141/1000 | Loss: 0.00001562
Iteration 142/1000 | Loss: 0.00001562
Iteration 143/1000 | Loss: 0.00001562
Iteration 144/1000 | Loss: 0.00001562
Iteration 145/1000 | Loss: 0.00001562
Iteration 146/1000 | Loss: 0.00001562
Iteration 147/1000 | Loss: 0.00001562
Iteration 148/1000 | Loss: 0.00001562
Iteration 149/1000 | Loss: 0.00001562
Iteration 150/1000 | Loss: 0.00001561
Iteration 151/1000 | Loss: 0.00001561
Iteration 152/1000 | Loss: 0.00001561
Iteration 153/1000 | Loss: 0.00001561
Iteration 154/1000 | Loss: 0.00001561
Iteration 155/1000 | Loss: 0.00001560
Iteration 156/1000 | Loss: 0.00001560
Iteration 157/1000 | Loss: 0.00001560
Iteration 158/1000 | Loss: 0.00001560
Iteration 159/1000 | Loss: 0.00001560
Iteration 160/1000 | Loss: 0.00001560
Iteration 161/1000 | Loss: 0.00001560
Iteration 162/1000 | Loss: 0.00001560
Iteration 163/1000 | Loss: 0.00001559
Iteration 164/1000 | Loss: 0.00001559
Iteration 165/1000 | Loss: 0.00001559
Iteration 166/1000 | Loss: 0.00001559
Iteration 167/1000 | Loss: 0.00001559
Iteration 168/1000 | Loss: 0.00001558
Iteration 169/1000 | Loss: 0.00001558
Iteration 170/1000 | Loss: 0.00001558
Iteration 171/1000 | Loss: 0.00001558
Iteration 172/1000 | Loss: 0.00001558
Iteration 173/1000 | Loss: 0.00001558
Iteration 174/1000 | Loss: 0.00001558
Iteration 175/1000 | Loss: 0.00001558
Iteration 176/1000 | Loss: 0.00001557
Iteration 177/1000 | Loss: 0.00001557
Iteration 178/1000 | Loss: 0.00001557
Iteration 179/1000 | Loss: 0.00001557
Iteration 180/1000 | Loss: 0.00001557
Iteration 181/1000 | Loss: 0.00001557
Iteration 182/1000 | Loss: 0.00001557
Iteration 183/1000 | Loss: 0.00001557
Iteration 184/1000 | Loss: 0.00001557
Iteration 185/1000 | Loss: 0.00001557
Iteration 186/1000 | Loss: 0.00001557
Iteration 187/1000 | Loss: 0.00001557
Iteration 188/1000 | Loss: 0.00001557
Iteration 189/1000 | Loss: 0.00001557
Iteration 190/1000 | Loss: 0.00001557
Iteration 191/1000 | Loss: 0.00001557
Iteration 192/1000 | Loss: 0.00001557
Iteration 193/1000 | Loss: 0.00001556
Iteration 194/1000 | Loss: 0.00001556
Iteration 195/1000 | Loss: 0.00001556
Iteration 196/1000 | Loss: 0.00001556
Iteration 197/1000 | Loss: 0.00001556
Iteration 198/1000 | Loss: 0.00001556
Iteration 199/1000 | Loss: 0.00001556
Iteration 200/1000 | Loss: 0.00001556
Iteration 201/1000 | Loss: 0.00001556
Iteration 202/1000 | Loss: 0.00001555
Iteration 203/1000 | Loss: 0.00001555
Iteration 204/1000 | Loss: 0.00001555
Iteration 205/1000 | Loss: 0.00001555
Iteration 206/1000 | Loss: 0.00001555
Iteration 207/1000 | Loss: 0.00001555
Iteration 208/1000 | Loss: 0.00001555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 208. Stopping optimization.
Last 5 losses: [1.555230119265616e-05, 1.555230119265616e-05, 1.555230119265616e-05, 1.555230119265616e-05, 1.555230119265616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.555230119265616e-05

Optimization complete. Final v2v error: 3.3421366214752197 mm

Highest mean error: 4.176537036895752 mm for frame 135

Lowest mean error: 2.890126943588257 mm for frame 90

Saving results

Total time: 48.345865964889526
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1030/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1030.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1030
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00398493
Iteration 2/25 | Loss: 0.00125366
Iteration 3/25 | Loss: 0.00117862
Iteration 4/25 | Loss: 0.00117027
Iteration 5/25 | Loss: 0.00116821
Iteration 6/25 | Loss: 0.00116821
Iteration 7/25 | Loss: 0.00116821
Iteration 8/25 | Loss: 0.00116821
Iteration 9/25 | Loss: 0.00116821
Iteration 10/25 | Loss: 0.00116821
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001168214250355959, 0.001168214250355959, 0.001168214250355959, 0.001168214250355959, 0.001168214250355959]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001168214250355959

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.11028862
Iteration 2/25 | Loss: 0.00106980
Iteration 3/25 | Loss: 0.00106980
Iteration 4/25 | Loss: 0.00106980
Iteration 5/25 | Loss: 0.00106980
Iteration 6/25 | Loss: 0.00106979
Iteration 7/25 | Loss: 0.00106979
Iteration 8/25 | Loss: 0.00106979
Iteration 9/25 | Loss: 0.00106979
Iteration 10/25 | Loss: 0.00106979
Iteration 11/25 | Loss: 0.00106979
Iteration 12/25 | Loss: 0.00106979
Iteration 13/25 | Loss: 0.00106979
Iteration 14/25 | Loss: 0.00106979
Iteration 15/25 | Loss: 0.00106979
Iteration 16/25 | Loss: 0.00106979
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010697938269004226, 0.0010697938269004226, 0.0010697938269004226, 0.0010697938269004226, 0.0010697938269004226]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010697938269004226

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106979
Iteration 2/1000 | Loss: 0.00001887
Iteration 3/1000 | Loss: 0.00001394
Iteration 4/1000 | Loss: 0.00001276
Iteration 5/1000 | Loss: 0.00001203
Iteration 6/1000 | Loss: 0.00001159
Iteration 7/1000 | Loss: 0.00001127
Iteration 8/1000 | Loss: 0.00001086
Iteration 9/1000 | Loss: 0.00001081
Iteration 10/1000 | Loss: 0.00001072
Iteration 11/1000 | Loss: 0.00001053
Iteration 12/1000 | Loss: 0.00001052
Iteration 13/1000 | Loss: 0.00001052
Iteration 14/1000 | Loss: 0.00001047
Iteration 15/1000 | Loss: 0.00001040
Iteration 16/1000 | Loss: 0.00001040
Iteration 17/1000 | Loss: 0.00001038
Iteration 18/1000 | Loss: 0.00001038
Iteration 19/1000 | Loss: 0.00001032
Iteration 20/1000 | Loss: 0.00001031
Iteration 21/1000 | Loss: 0.00001031
Iteration 22/1000 | Loss: 0.00001025
Iteration 23/1000 | Loss: 0.00001025
Iteration 24/1000 | Loss: 0.00001023
Iteration 25/1000 | Loss: 0.00001022
Iteration 26/1000 | Loss: 0.00001020
Iteration 27/1000 | Loss: 0.00001018
Iteration 28/1000 | Loss: 0.00001016
Iteration 29/1000 | Loss: 0.00001014
Iteration 30/1000 | Loss: 0.00001012
Iteration 31/1000 | Loss: 0.00001011
Iteration 32/1000 | Loss: 0.00001011
Iteration 33/1000 | Loss: 0.00001010
Iteration 34/1000 | Loss: 0.00001009
Iteration 35/1000 | Loss: 0.00001008
Iteration 36/1000 | Loss: 0.00001008
Iteration 37/1000 | Loss: 0.00001008
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001007
Iteration 40/1000 | Loss: 0.00001007
Iteration 41/1000 | Loss: 0.00001007
Iteration 42/1000 | Loss: 0.00001006
Iteration 43/1000 | Loss: 0.00001005
Iteration 44/1000 | Loss: 0.00001000
Iteration 45/1000 | Loss: 0.00001000
Iteration 46/1000 | Loss: 0.00001000
Iteration 47/1000 | Loss: 0.00001000
Iteration 48/1000 | Loss: 0.00001000
Iteration 49/1000 | Loss: 0.00001000
Iteration 50/1000 | Loss: 0.00001000
Iteration 51/1000 | Loss: 0.00000999
Iteration 52/1000 | Loss: 0.00000999
Iteration 53/1000 | Loss: 0.00000999
Iteration 54/1000 | Loss: 0.00000997
Iteration 55/1000 | Loss: 0.00000997
Iteration 56/1000 | Loss: 0.00000996
Iteration 57/1000 | Loss: 0.00000995
Iteration 58/1000 | Loss: 0.00000995
Iteration 59/1000 | Loss: 0.00000995
Iteration 60/1000 | Loss: 0.00000995
Iteration 61/1000 | Loss: 0.00000995
Iteration 62/1000 | Loss: 0.00000995
Iteration 63/1000 | Loss: 0.00000994
Iteration 64/1000 | Loss: 0.00000994
Iteration 65/1000 | Loss: 0.00000993
Iteration 66/1000 | Loss: 0.00000993
Iteration 67/1000 | Loss: 0.00000992
Iteration 68/1000 | Loss: 0.00000992
Iteration 69/1000 | Loss: 0.00000992
Iteration 70/1000 | Loss: 0.00000991
Iteration 71/1000 | Loss: 0.00000990
Iteration 72/1000 | Loss: 0.00000990
Iteration 73/1000 | Loss: 0.00000990
Iteration 74/1000 | Loss: 0.00000990
Iteration 75/1000 | Loss: 0.00000989
Iteration 76/1000 | Loss: 0.00000988
Iteration 77/1000 | Loss: 0.00000988
Iteration 78/1000 | Loss: 0.00000988
Iteration 79/1000 | Loss: 0.00000987
Iteration 80/1000 | Loss: 0.00000987
Iteration 81/1000 | Loss: 0.00000987
Iteration 82/1000 | Loss: 0.00000986
Iteration 83/1000 | Loss: 0.00000986
Iteration 84/1000 | Loss: 0.00000986
Iteration 85/1000 | Loss: 0.00000986
Iteration 86/1000 | Loss: 0.00000985
Iteration 87/1000 | Loss: 0.00000985
Iteration 88/1000 | Loss: 0.00000984
Iteration 89/1000 | Loss: 0.00000984
Iteration 90/1000 | Loss: 0.00000984
Iteration 91/1000 | Loss: 0.00000983
Iteration 92/1000 | Loss: 0.00000983
Iteration 93/1000 | Loss: 0.00000983
Iteration 94/1000 | Loss: 0.00000982
Iteration 95/1000 | Loss: 0.00000982
Iteration 96/1000 | Loss: 0.00000981
Iteration 97/1000 | Loss: 0.00000981
Iteration 98/1000 | Loss: 0.00000981
Iteration 99/1000 | Loss: 0.00000981
Iteration 100/1000 | Loss: 0.00000980
Iteration 101/1000 | Loss: 0.00000980
Iteration 102/1000 | Loss: 0.00000980
Iteration 103/1000 | Loss: 0.00000979
Iteration 104/1000 | Loss: 0.00000979
Iteration 105/1000 | Loss: 0.00000979
Iteration 106/1000 | Loss: 0.00000978
Iteration 107/1000 | Loss: 0.00000978
Iteration 108/1000 | Loss: 0.00000978
Iteration 109/1000 | Loss: 0.00000977
Iteration 110/1000 | Loss: 0.00000977
Iteration 111/1000 | Loss: 0.00000977
Iteration 112/1000 | Loss: 0.00000977
Iteration 113/1000 | Loss: 0.00000976
Iteration 114/1000 | Loss: 0.00000976
Iteration 115/1000 | Loss: 0.00000976
Iteration 116/1000 | Loss: 0.00000976
Iteration 117/1000 | Loss: 0.00000976
Iteration 118/1000 | Loss: 0.00000976
Iteration 119/1000 | Loss: 0.00000975
Iteration 120/1000 | Loss: 0.00000975
Iteration 121/1000 | Loss: 0.00000975
Iteration 122/1000 | Loss: 0.00000974
Iteration 123/1000 | Loss: 0.00000974
Iteration 124/1000 | Loss: 0.00000974
Iteration 125/1000 | Loss: 0.00000974
Iteration 126/1000 | Loss: 0.00000974
Iteration 127/1000 | Loss: 0.00000974
Iteration 128/1000 | Loss: 0.00000974
Iteration 129/1000 | Loss: 0.00000974
Iteration 130/1000 | Loss: 0.00000974
Iteration 131/1000 | Loss: 0.00000974
Iteration 132/1000 | Loss: 0.00000974
Iteration 133/1000 | Loss: 0.00000974
Iteration 134/1000 | Loss: 0.00000974
Iteration 135/1000 | Loss: 0.00000974
Iteration 136/1000 | Loss: 0.00000974
Iteration 137/1000 | Loss: 0.00000974
Iteration 138/1000 | Loss: 0.00000974
Iteration 139/1000 | Loss: 0.00000974
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [9.73771238932386e-06, 9.73771238932386e-06, 9.73771238932386e-06, 9.73771238932386e-06, 9.73771238932386e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.73771238932386e-06

Optimization complete. Final v2v error: 2.699659824371338 mm

Highest mean error: 3.1447067260742188 mm for frame 210

Lowest mean error: 2.5932717323303223 mm for frame 141

Saving results

Total time: 41.55917263031006
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00856647
Iteration 2/25 | Loss: 0.00130742
Iteration 3/25 | Loss: 0.00123006
Iteration 4/25 | Loss: 0.00121641
Iteration 5/25 | Loss: 0.00121192
Iteration 6/25 | Loss: 0.00121128
Iteration 7/25 | Loss: 0.00121128
Iteration 8/25 | Loss: 0.00121128
Iteration 9/25 | Loss: 0.00121128
Iteration 10/25 | Loss: 0.00121128
Iteration 11/25 | Loss: 0.00121128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012112823314964771, 0.0012112823314964771, 0.0012112823314964771, 0.0012112823314964771, 0.0012112823314964771]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012112823314964771

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.46186495
Iteration 2/25 | Loss: 0.00110500
Iteration 3/25 | Loss: 0.00110500
Iteration 4/25 | Loss: 0.00110499
Iteration 5/25 | Loss: 0.00110499
Iteration 6/25 | Loss: 0.00110499
Iteration 7/25 | Loss: 0.00110499
Iteration 8/25 | Loss: 0.00110499
Iteration 9/25 | Loss: 0.00110499
Iteration 10/25 | Loss: 0.00110499
Iteration 11/25 | Loss: 0.00110499
Iteration 12/25 | Loss: 0.00110499
Iteration 13/25 | Loss: 0.00110499
Iteration 14/25 | Loss: 0.00110499
Iteration 15/25 | Loss: 0.00110499
Iteration 16/25 | Loss: 0.00110499
Iteration 17/25 | Loss: 0.00110499
Iteration 18/25 | Loss: 0.00110499
Iteration 19/25 | Loss: 0.00110499
Iteration 20/25 | Loss: 0.00110499
Iteration 21/25 | Loss: 0.00110499
Iteration 22/25 | Loss: 0.00110499
Iteration 23/25 | Loss: 0.00110499
Iteration 24/25 | Loss: 0.00110499
Iteration 25/25 | Loss: 0.00110499

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110499
Iteration 2/1000 | Loss: 0.00002262
Iteration 3/1000 | Loss: 0.00001758
Iteration 4/1000 | Loss: 0.00001629
Iteration 5/1000 | Loss: 0.00001552
Iteration 6/1000 | Loss: 0.00001505
Iteration 7/1000 | Loss: 0.00001466
Iteration 8/1000 | Loss: 0.00001432
Iteration 9/1000 | Loss: 0.00001402
Iteration 10/1000 | Loss: 0.00001391
Iteration 11/1000 | Loss: 0.00001373
Iteration 12/1000 | Loss: 0.00001372
Iteration 13/1000 | Loss: 0.00001371
Iteration 14/1000 | Loss: 0.00001362
Iteration 15/1000 | Loss: 0.00001346
Iteration 16/1000 | Loss: 0.00001338
Iteration 17/1000 | Loss: 0.00001329
Iteration 18/1000 | Loss: 0.00001326
Iteration 19/1000 | Loss: 0.00001325
Iteration 20/1000 | Loss: 0.00001324
Iteration 21/1000 | Loss: 0.00001320
Iteration 22/1000 | Loss: 0.00001320
Iteration 23/1000 | Loss: 0.00001319
Iteration 24/1000 | Loss: 0.00001319
Iteration 25/1000 | Loss: 0.00001318
Iteration 26/1000 | Loss: 0.00001317
Iteration 27/1000 | Loss: 0.00001314
Iteration 28/1000 | Loss: 0.00001313
Iteration 29/1000 | Loss: 0.00001309
Iteration 30/1000 | Loss: 0.00001308
Iteration 31/1000 | Loss: 0.00001306
Iteration 32/1000 | Loss: 0.00001306
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001305
Iteration 35/1000 | Loss: 0.00001305
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001300
Iteration 39/1000 | Loss: 0.00001299
Iteration 40/1000 | Loss: 0.00001299
Iteration 41/1000 | Loss: 0.00001298
Iteration 42/1000 | Loss: 0.00001298
Iteration 43/1000 | Loss: 0.00001298
Iteration 44/1000 | Loss: 0.00001297
Iteration 45/1000 | Loss: 0.00001297
Iteration 46/1000 | Loss: 0.00001296
Iteration 47/1000 | Loss: 0.00001294
Iteration 48/1000 | Loss: 0.00001294
Iteration 49/1000 | Loss: 0.00001294
Iteration 50/1000 | Loss: 0.00001294
Iteration 51/1000 | Loss: 0.00001293
Iteration 52/1000 | Loss: 0.00001293
Iteration 53/1000 | Loss: 0.00001293
Iteration 54/1000 | Loss: 0.00001293
Iteration 55/1000 | Loss: 0.00001293
Iteration 56/1000 | Loss: 0.00001293
Iteration 57/1000 | Loss: 0.00001291
Iteration 58/1000 | Loss: 0.00001291
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001290
Iteration 61/1000 | Loss: 0.00001290
Iteration 62/1000 | Loss: 0.00001290
Iteration 63/1000 | Loss: 0.00001290
Iteration 64/1000 | Loss: 0.00001290
Iteration 65/1000 | Loss: 0.00001290
Iteration 66/1000 | Loss: 0.00001290
Iteration 67/1000 | Loss: 0.00001290
Iteration 68/1000 | Loss: 0.00001289
Iteration 69/1000 | Loss: 0.00001289
Iteration 70/1000 | Loss: 0.00001289
Iteration 71/1000 | Loss: 0.00001288
Iteration 72/1000 | Loss: 0.00001288
Iteration 73/1000 | Loss: 0.00001287
Iteration 74/1000 | Loss: 0.00001287
Iteration 75/1000 | Loss: 0.00001287
Iteration 76/1000 | Loss: 0.00001287
Iteration 77/1000 | Loss: 0.00001287
Iteration 78/1000 | Loss: 0.00001287
Iteration 79/1000 | Loss: 0.00001286
Iteration 80/1000 | Loss: 0.00001286
Iteration 81/1000 | Loss: 0.00001286
Iteration 82/1000 | Loss: 0.00001286
Iteration 83/1000 | Loss: 0.00001286
Iteration 84/1000 | Loss: 0.00001286
Iteration 85/1000 | Loss: 0.00001286
Iteration 86/1000 | Loss: 0.00001286
Iteration 87/1000 | Loss: 0.00001285
Iteration 88/1000 | Loss: 0.00001285
Iteration 89/1000 | Loss: 0.00001285
Iteration 90/1000 | Loss: 0.00001285
Iteration 91/1000 | Loss: 0.00001285
Iteration 92/1000 | Loss: 0.00001285
Iteration 93/1000 | Loss: 0.00001285
Iteration 94/1000 | Loss: 0.00001285
Iteration 95/1000 | Loss: 0.00001285
Iteration 96/1000 | Loss: 0.00001285
Iteration 97/1000 | Loss: 0.00001285
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001285
Iteration 102/1000 | Loss: 0.00001285
Iteration 103/1000 | Loss: 0.00001285
Iteration 104/1000 | Loss: 0.00001285
Iteration 105/1000 | Loss: 0.00001285
Iteration 106/1000 | Loss: 0.00001285
Iteration 107/1000 | Loss: 0.00001285
Iteration 108/1000 | Loss: 0.00001285
Iteration 109/1000 | Loss: 0.00001285
Iteration 110/1000 | Loss: 0.00001285
Iteration 111/1000 | Loss: 0.00001285
Iteration 112/1000 | Loss: 0.00001285
Iteration 113/1000 | Loss: 0.00001285
Iteration 114/1000 | Loss: 0.00001285
Iteration 115/1000 | Loss: 0.00001285
Iteration 116/1000 | Loss: 0.00001285
Iteration 117/1000 | Loss: 0.00001285
Iteration 118/1000 | Loss: 0.00001285
Iteration 119/1000 | Loss: 0.00001285
Iteration 120/1000 | Loss: 0.00001285
Iteration 121/1000 | Loss: 0.00001285
Iteration 122/1000 | Loss: 0.00001285
Iteration 123/1000 | Loss: 0.00001285
Iteration 124/1000 | Loss: 0.00001285
Iteration 125/1000 | Loss: 0.00001285
Iteration 126/1000 | Loss: 0.00001285
Iteration 127/1000 | Loss: 0.00001285
Iteration 128/1000 | Loss: 0.00001285
Iteration 129/1000 | Loss: 0.00001285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.2845961464336142e-05, 1.2845961464336142e-05, 1.2845961464336142e-05, 1.2845961464336142e-05, 1.2845961464336142e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2845961464336142e-05

Optimization complete. Final v2v error: 3.0980000495910645 mm

Highest mean error: 3.449415683746338 mm for frame 180

Lowest mean error: 2.8805899620056152 mm for frame 14

Saving results

Total time: 39.587214946746826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00780685
Iteration 2/25 | Loss: 0.00141054
Iteration 3/25 | Loss: 0.00129889
Iteration 4/25 | Loss: 0.00128261
Iteration 5/25 | Loss: 0.00127540
Iteration 6/25 | Loss: 0.00127319
Iteration 7/25 | Loss: 0.00127315
Iteration 8/25 | Loss: 0.00127315
Iteration 9/25 | Loss: 0.00127315
Iteration 10/25 | Loss: 0.00127315
Iteration 11/25 | Loss: 0.00127315
Iteration 12/25 | Loss: 0.00127315
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012731454335153103, 0.0012731454335153103, 0.0012731454335153103, 0.0012731454335153103, 0.0012731454335153103]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012731454335153103

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.22256541
Iteration 2/25 | Loss: 0.00131390
Iteration 3/25 | Loss: 0.00131390
Iteration 4/25 | Loss: 0.00131390
Iteration 5/25 | Loss: 0.00131390
Iteration 6/25 | Loss: 0.00131390
Iteration 7/25 | Loss: 0.00131390
Iteration 8/25 | Loss: 0.00131390
Iteration 9/25 | Loss: 0.00131390
Iteration 10/25 | Loss: 0.00131390
Iteration 11/25 | Loss: 0.00131390
Iteration 12/25 | Loss: 0.00131390
Iteration 13/25 | Loss: 0.00131390
Iteration 14/25 | Loss: 0.00131390
Iteration 15/25 | Loss: 0.00131390
Iteration 16/25 | Loss: 0.00131390
Iteration 17/25 | Loss: 0.00131390
Iteration 18/25 | Loss: 0.00131390
Iteration 19/25 | Loss: 0.00131390
Iteration 20/25 | Loss: 0.00131390
Iteration 21/25 | Loss: 0.00131390
Iteration 22/25 | Loss: 0.00131390
Iteration 23/25 | Loss: 0.00131390
Iteration 24/25 | Loss: 0.00131390
Iteration 25/25 | Loss: 0.00131390

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131390
Iteration 2/1000 | Loss: 0.00003233
Iteration 3/1000 | Loss: 0.00002192
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001940
Iteration 6/1000 | Loss: 0.00001901
Iteration 7/1000 | Loss: 0.00001864
Iteration 8/1000 | Loss: 0.00001861
Iteration 9/1000 | Loss: 0.00001833
Iteration 10/1000 | Loss: 0.00001795
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001759
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001750
Iteration 15/1000 | Loss: 0.00001749
Iteration 16/1000 | Loss: 0.00001749
Iteration 17/1000 | Loss: 0.00001749
Iteration 18/1000 | Loss: 0.00001738
Iteration 19/1000 | Loss: 0.00001738
Iteration 20/1000 | Loss: 0.00001737
Iteration 21/1000 | Loss: 0.00001736
Iteration 22/1000 | Loss: 0.00001734
Iteration 23/1000 | Loss: 0.00001731
Iteration 24/1000 | Loss: 0.00001730
Iteration 25/1000 | Loss: 0.00001730
Iteration 26/1000 | Loss: 0.00001728
Iteration 27/1000 | Loss: 0.00001726
Iteration 28/1000 | Loss: 0.00001724
Iteration 29/1000 | Loss: 0.00001723
Iteration 30/1000 | Loss: 0.00001722
Iteration 31/1000 | Loss: 0.00001722
Iteration 32/1000 | Loss: 0.00001717
Iteration 33/1000 | Loss: 0.00001716
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001716
Iteration 36/1000 | Loss: 0.00001713
Iteration 37/1000 | Loss: 0.00001713
Iteration 38/1000 | Loss: 0.00001713
Iteration 39/1000 | Loss: 0.00001713
Iteration 40/1000 | Loss: 0.00001712
Iteration 41/1000 | Loss: 0.00001712
Iteration 42/1000 | Loss: 0.00001712
Iteration 43/1000 | Loss: 0.00001712
Iteration 44/1000 | Loss: 0.00001712
Iteration 45/1000 | Loss: 0.00001712
Iteration 46/1000 | Loss: 0.00001712
Iteration 47/1000 | Loss: 0.00001711
Iteration 48/1000 | Loss: 0.00001711
Iteration 49/1000 | Loss: 0.00001710
Iteration 50/1000 | Loss: 0.00001710
Iteration 51/1000 | Loss: 0.00001709
Iteration 52/1000 | Loss: 0.00001709
Iteration 53/1000 | Loss: 0.00001708
Iteration 54/1000 | Loss: 0.00001708
Iteration 55/1000 | Loss: 0.00001708
Iteration 56/1000 | Loss: 0.00001708
Iteration 57/1000 | Loss: 0.00001708
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001705
Iteration 63/1000 | Loss: 0.00001704
Iteration 64/1000 | Loss: 0.00001704
Iteration 65/1000 | Loss: 0.00001704
Iteration 66/1000 | Loss: 0.00001704
Iteration 67/1000 | Loss: 0.00001704
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001703
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001702
Iteration 78/1000 | Loss: 0.00001702
Iteration 79/1000 | Loss: 0.00001702
Iteration 80/1000 | Loss: 0.00001701
Iteration 81/1000 | Loss: 0.00001701
Iteration 82/1000 | Loss: 0.00001701
Iteration 83/1000 | Loss: 0.00001700
Iteration 84/1000 | Loss: 0.00001700
Iteration 85/1000 | Loss: 0.00001699
Iteration 86/1000 | Loss: 0.00001699
Iteration 87/1000 | Loss: 0.00001698
Iteration 88/1000 | Loss: 0.00001698
Iteration 89/1000 | Loss: 0.00001698
Iteration 90/1000 | Loss: 0.00001698
Iteration 91/1000 | Loss: 0.00001698
Iteration 92/1000 | Loss: 0.00001698
Iteration 93/1000 | Loss: 0.00001697
Iteration 94/1000 | Loss: 0.00001697
Iteration 95/1000 | Loss: 0.00001697
Iteration 96/1000 | Loss: 0.00001696
Iteration 97/1000 | Loss: 0.00001696
Iteration 98/1000 | Loss: 0.00001695
Iteration 99/1000 | Loss: 0.00001695
Iteration 100/1000 | Loss: 0.00001694
Iteration 101/1000 | Loss: 0.00001694
Iteration 102/1000 | Loss: 0.00001693
Iteration 103/1000 | Loss: 0.00001693
Iteration 104/1000 | Loss: 0.00001693
Iteration 105/1000 | Loss: 0.00001693
Iteration 106/1000 | Loss: 0.00001693
Iteration 107/1000 | Loss: 0.00001693
Iteration 108/1000 | Loss: 0.00001693
Iteration 109/1000 | Loss: 0.00001693
Iteration 110/1000 | Loss: 0.00001692
Iteration 111/1000 | Loss: 0.00001692
Iteration 112/1000 | Loss: 0.00001692
Iteration 113/1000 | Loss: 0.00001692
Iteration 114/1000 | Loss: 0.00001692
Iteration 115/1000 | Loss: 0.00001692
Iteration 116/1000 | Loss: 0.00001691
Iteration 117/1000 | Loss: 0.00001691
Iteration 118/1000 | Loss: 0.00001691
Iteration 119/1000 | Loss: 0.00001690
Iteration 120/1000 | Loss: 0.00001690
Iteration 121/1000 | Loss: 0.00001689
Iteration 122/1000 | Loss: 0.00001689
Iteration 123/1000 | Loss: 0.00001689
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001689
Iteration 126/1000 | Loss: 0.00001689
Iteration 127/1000 | Loss: 0.00001689
Iteration 128/1000 | Loss: 0.00001689
Iteration 129/1000 | Loss: 0.00001689
Iteration 130/1000 | Loss: 0.00001689
Iteration 131/1000 | Loss: 0.00001689
Iteration 132/1000 | Loss: 0.00001688
Iteration 133/1000 | Loss: 0.00001688
Iteration 134/1000 | Loss: 0.00001688
Iteration 135/1000 | Loss: 0.00001688
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001687
Iteration 139/1000 | Loss: 0.00001687
Iteration 140/1000 | Loss: 0.00001687
Iteration 141/1000 | Loss: 0.00001687
Iteration 142/1000 | Loss: 0.00001687
Iteration 143/1000 | Loss: 0.00001687
Iteration 144/1000 | Loss: 0.00001686
Iteration 145/1000 | Loss: 0.00001686
Iteration 146/1000 | Loss: 0.00001686
Iteration 147/1000 | Loss: 0.00001686
Iteration 148/1000 | Loss: 0.00001686
Iteration 149/1000 | Loss: 0.00001686
Iteration 150/1000 | Loss: 0.00001686
Iteration 151/1000 | Loss: 0.00001686
Iteration 152/1000 | Loss: 0.00001686
Iteration 153/1000 | Loss: 0.00001686
Iteration 154/1000 | Loss: 0.00001686
Iteration 155/1000 | Loss: 0.00001686
Iteration 156/1000 | Loss: 0.00001686
Iteration 157/1000 | Loss: 0.00001686
Iteration 158/1000 | Loss: 0.00001686
Iteration 159/1000 | Loss: 0.00001686
Iteration 160/1000 | Loss: 0.00001685
Iteration 161/1000 | Loss: 0.00001685
Iteration 162/1000 | Loss: 0.00001685
Iteration 163/1000 | Loss: 0.00001685
Iteration 164/1000 | Loss: 0.00001685
Iteration 165/1000 | Loss: 0.00001685
Iteration 166/1000 | Loss: 0.00001685
Iteration 167/1000 | Loss: 0.00001685
Iteration 168/1000 | Loss: 0.00001685
Iteration 169/1000 | Loss: 0.00001685
Iteration 170/1000 | Loss: 0.00001685
Iteration 171/1000 | Loss: 0.00001685
Iteration 172/1000 | Loss: 0.00001685
Iteration 173/1000 | Loss: 0.00001685
Iteration 174/1000 | Loss: 0.00001685
Iteration 175/1000 | Loss: 0.00001685
Iteration 176/1000 | Loss: 0.00001685
Iteration 177/1000 | Loss: 0.00001685
Iteration 178/1000 | Loss: 0.00001685
Iteration 179/1000 | Loss: 0.00001684
Iteration 180/1000 | Loss: 0.00001684
Iteration 181/1000 | Loss: 0.00001684
Iteration 182/1000 | Loss: 0.00001684
Iteration 183/1000 | Loss: 0.00001684
Iteration 184/1000 | Loss: 0.00001684
Iteration 185/1000 | Loss: 0.00001684
Iteration 186/1000 | Loss: 0.00001683
Iteration 187/1000 | Loss: 0.00001683
Iteration 188/1000 | Loss: 0.00001683
Iteration 189/1000 | Loss: 0.00001683
Iteration 190/1000 | Loss: 0.00001683
Iteration 191/1000 | Loss: 0.00001682
Iteration 192/1000 | Loss: 0.00001682
Iteration 193/1000 | Loss: 0.00001682
Iteration 194/1000 | Loss: 0.00001682
Iteration 195/1000 | Loss: 0.00001682
Iteration 196/1000 | Loss: 0.00001682
Iteration 197/1000 | Loss: 0.00001682
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [1.6824438716867007e-05, 1.6824438716867007e-05, 1.6824438716867007e-05, 1.6824438716867007e-05, 1.6824438716867007e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6824438716867007e-05

Optimization complete. Final v2v error: 3.4609944820404053 mm

Highest mean error: 3.6343026161193848 mm for frame 122

Lowest mean error: 3.3158249855041504 mm for frame 43

Saving results

Total time: 40.119375467300415
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1067/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1067.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1067
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01031325
Iteration 2/25 | Loss: 0.00148998
Iteration 3/25 | Loss: 0.00128238
Iteration 4/25 | Loss: 0.00127149
Iteration 5/25 | Loss: 0.00125957
Iteration 6/25 | Loss: 0.00124382
Iteration 7/25 | Loss: 0.00123271
Iteration 8/25 | Loss: 0.00123421
Iteration 9/25 | Loss: 0.00123412
Iteration 10/25 | Loss: 0.00122984
Iteration 11/25 | Loss: 0.00122890
Iteration 12/25 | Loss: 0.00123008
Iteration 13/25 | Loss: 0.00122522
Iteration 14/25 | Loss: 0.00121754
Iteration 15/25 | Loss: 0.00121701
Iteration 16/25 | Loss: 0.00121865
Iteration 17/25 | Loss: 0.00121854
Iteration 18/25 | Loss: 0.00121713
Iteration 19/25 | Loss: 0.00121752
Iteration 20/25 | Loss: 0.00121659
Iteration 21/25 | Loss: 0.00121670
Iteration 22/25 | Loss: 0.00121621
Iteration 23/25 | Loss: 0.00121706
Iteration 24/25 | Loss: 0.00121661
Iteration 25/25 | Loss: 0.00121700

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.53617358
Iteration 2/25 | Loss: 0.00142298
Iteration 3/25 | Loss: 0.00142298
Iteration 4/25 | Loss: 0.00142298
Iteration 5/25 | Loss: 0.00142298
Iteration 6/25 | Loss: 0.00142298
Iteration 7/25 | Loss: 0.00142298
Iteration 8/25 | Loss: 0.00142298
Iteration 9/25 | Loss: 0.00142298
Iteration 10/25 | Loss: 0.00142298
Iteration 11/25 | Loss: 0.00142298
Iteration 12/25 | Loss: 0.00142298
Iteration 13/25 | Loss: 0.00142298
Iteration 14/25 | Loss: 0.00142298
Iteration 15/25 | Loss: 0.00142298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001422980218194425, 0.001422980218194425, 0.001422980218194425, 0.001422980218194425, 0.001422980218194425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001422980218194425

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00142298
Iteration 2/1000 | Loss: 0.00002821
Iteration 3/1000 | Loss: 0.00001979
Iteration 4/1000 | Loss: 0.00014413
Iteration 5/1000 | Loss: 0.00011491
Iteration 6/1000 | Loss: 0.00013506
Iteration 7/1000 | Loss: 0.00009884
Iteration 8/1000 | Loss: 0.00001994
Iteration 9/1000 | Loss: 0.00013883
Iteration 10/1000 | Loss: 0.00006419
Iteration 11/1000 | Loss: 0.00016560
Iteration 12/1000 | Loss: 0.00010742
Iteration 13/1000 | Loss: 0.00006956
Iteration 14/1000 | Loss: 0.00001635
Iteration 15/1000 | Loss: 0.00007843
Iteration 16/1000 | Loss: 0.00009332
Iteration 17/1000 | Loss: 0.00007404
Iteration 18/1000 | Loss: 0.00008639
Iteration 19/1000 | Loss: 0.00002044
Iteration 20/1000 | Loss: 0.00001659
Iteration 21/1000 | Loss: 0.00001504
Iteration 22/1000 | Loss: 0.00001448
Iteration 23/1000 | Loss: 0.00001418
Iteration 24/1000 | Loss: 0.00001373
Iteration 25/1000 | Loss: 0.00001526
Iteration 26/1000 | Loss: 0.00001424
Iteration 27/1000 | Loss: 0.00001311
Iteration 28/1000 | Loss: 0.00001262
Iteration 29/1000 | Loss: 0.00001245
Iteration 30/1000 | Loss: 0.00001244
Iteration 31/1000 | Loss: 0.00001240
Iteration 32/1000 | Loss: 0.00001237
Iteration 33/1000 | Loss: 0.00001236
Iteration 34/1000 | Loss: 0.00001234
Iteration 35/1000 | Loss: 0.00001234
Iteration 36/1000 | Loss: 0.00001231
Iteration 37/1000 | Loss: 0.00001229
Iteration 38/1000 | Loss: 0.00001228
Iteration 39/1000 | Loss: 0.00001227
Iteration 40/1000 | Loss: 0.00001226
Iteration 41/1000 | Loss: 0.00001223
Iteration 42/1000 | Loss: 0.00001222
Iteration 43/1000 | Loss: 0.00001221
Iteration 44/1000 | Loss: 0.00001220
Iteration 45/1000 | Loss: 0.00001214
Iteration 46/1000 | Loss: 0.00001202
Iteration 47/1000 | Loss: 0.00001197
Iteration 48/1000 | Loss: 0.00001195
Iteration 49/1000 | Loss: 0.00001195
Iteration 50/1000 | Loss: 0.00001302
Iteration 51/1000 | Loss: 0.00001302
Iteration 52/1000 | Loss: 0.00001215
Iteration 53/1000 | Loss: 0.00001215
Iteration 54/1000 | Loss: 0.00001214
Iteration 55/1000 | Loss: 0.00001248
Iteration 56/1000 | Loss: 0.00001181
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001180
Iteration 59/1000 | Loss: 0.00001180
Iteration 60/1000 | Loss: 0.00001180
Iteration 61/1000 | Loss: 0.00001179
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001175
Iteration 65/1000 | Loss: 0.00001174
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001171
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001157
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001156
Iteration 72/1000 | Loss: 0.00001156
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001156
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001155
Iteration 79/1000 | Loss: 0.00001155
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001153
Iteration 84/1000 | Loss: 0.00001153
Iteration 85/1000 | Loss: 0.00001153
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001152
Iteration 88/1000 | Loss: 0.00001152
Iteration 89/1000 | Loss: 0.00001152
Iteration 90/1000 | Loss: 0.00001152
Iteration 91/1000 | Loss: 0.00001152
Iteration 92/1000 | Loss: 0.00001152
Iteration 93/1000 | Loss: 0.00001152
Iteration 94/1000 | Loss: 0.00001152
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001152
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001151
Iteration 100/1000 | Loss: 0.00001151
Iteration 101/1000 | Loss: 0.00001151
Iteration 102/1000 | Loss: 0.00001151
Iteration 103/1000 | Loss: 0.00001151
Iteration 104/1000 | Loss: 0.00001151
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001151
Iteration 108/1000 | Loss: 0.00001150
Iteration 109/1000 | Loss: 0.00001150
Iteration 110/1000 | Loss: 0.00001150
Iteration 111/1000 | Loss: 0.00001150
Iteration 112/1000 | Loss: 0.00001150
Iteration 113/1000 | Loss: 0.00001150
Iteration 114/1000 | Loss: 0.00001150
Iteration 115/1000 | Loss: 0.00001150
Iteration 116/1000 | Loss: 0.00001150
Iteration 117/1000 | Loss: 0.00001150
Iteration 118/1000 | Loss: 0.00001150
Iteration 119/1000 | Loss: 0.00001150
Iteration 120/1000 | Loss: 0.00001149
Iteration 121/1000 | Loss: 0.00001149
Iteration 122/1000 | Loss: 0.00001149
Iteration 123/1000 | Loss: 0.00001149
Iteration 124/1000 | Loss: 0.00001149
Iteration 125/1000 | Loss: 0.00001149
Iteration 126/1000 | Loss: 0.00001149
Iteration 127/1000 | Loss: 0.00001149
Iteration 128/1000 | Loss: 0.00001149
Iteration 129/1000 | Loss: 0.00001148
Iteration 130/1000 | Loss: 0.00001148
Iteration 131/1000 | Loss: 0.00001148
Iteration 132/1000 | Loss: 0.00001148
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001148
Iteration 137/1000 | Loss: 0.00001148
Iteration 138/1000 | Loss: 0.00001148
Iteration 139/1000 | Loss: 0.00001147
Iteration 140/1000 | Loss: 0.00001147
Iteration 141/1000 | Loss: 0.00001147
Iteration 142/1000 | Loss: 0.00001147
Iteration 143/1000 | Loss: 0.00001147
Iteration 144/1000 | Loss: 0.00001147
Iteration 145/1000 | Loss: 0.00001147
Iteration 146/1000 | Loss: 0.00001147
Iteration 147/1000 | Loss: 0.00001146
Iteration 148/1000 | Loss: 0.00001146
Iteration 149/1000 | Loss: 0.00001146
Iteration 150/1000 | Loss: 0.00001146
Iteration 151/1000 | Loss: 0.00001146
Iteration 152/1000 | Loss: 0.00001146
Iteration 153/1000 | Loss: 0.00001146
Iteration 154/1000 | Loss: 0.00001146
Iteration 155/1000 | Loss: 0.00001146
Iteration 156/1000 | Loss: 0.00001146
Iteration 157/1000 | Loss: 0.00001146
Iteration 158/1000 | Loss: 0.00001146
Iteration 159/1000 | Loss: 0.00001146
Iteration 160/1000 | Loss: 0.00001146
Iteration 161/1000 | Loss: 0.00001146
Iteration 162/1000 | Loss: 0.00001146
Iteration 163/1000 | Loss: 0.00001146
Iteration 164/1000 | Loss: 0.00001146
Iteration 165/1000 | Loss: 0.00001145
Iteration 166/1000 | Loss: 0.00001145
Iteration 167/1000 | Loss: 0.00001145
Iteration 168/1000 | Loss: 0.00001145
Iteration 169/1000 | Loss: 0.00001145
Iteration 170/1000 | Loss: 0.00001145
Iteration 171/1000 | Loss: 0.00001145
Iteration 172/1000 | Loss: 0.00001145
Iteration 173/1000 | Loss: 0.00001145
Iteration 174/1000 | Loss: 0.00001145
Iteration 175/1000 | Loss: 0.00001145
Iteration 176/1000 | Loss: 0.00001145
Iteration 177/1000 | Loss: 0.00001145
Iteration 178/1000 | Loss: 0.00001145
Iteration 179/1000 | Loss: 0.00001145
Iteration 180/1000 | Loss: 0.00001144
Iteration 181/1000 | Loss: 0.00001144
Iteration 182/1000 | Loss: 0.00001144
Iteration 183/1000 | Loss: 0.00001144
Iteration 184/1000 | Loss: 0.00001144
Iteration 185/1000 | Loss: 0.00001144
Iteration 186/1000 | Loss: 0.00001144
Iteration 187/1000 | Loss: 0.00001144
Iteration 188/1000 | Loss: 0.00001144
Iteration 189/1000 | Loss: 0.00001144
Iteration 190/1000 | Loss: 0.00001144
Iteration 191/1000 | Loss: 0.00001144
Iteration 192/1000 | Loss: 0.00001144
Iteration 193/1000 | Loss: 0.00001144
Iteration 194/1000 | Loss: 0.00001144
Iteration 195/1000 | Loss: 0.00001144
Iteration 196/1000 | Loss: 0.00001144
Iteration 197/1000 | Loss: 0.00001144
Iteration 198/1000 | Loss: 0.00001144
Iteration 199/1000 | Loss: 0.00001144
Iteration 200/1000 | Loss: 0.00001144
Iteration 201/1000 | Loss: 0.00001144
Iteration 202/1000 | Loss: 0.00001144
Iteration 203/1000 | Loss: 0.00001144
Iteration 204/1000 | Loss: 0.00001144
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [1.1442612048995215e-05, 1.1442612048995215e-05, 1.1442612048995215e-05, 1.1442612048995215e-05, 1.1442612048995215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1442612048995215e-05

Optimization complete. Final v2v error: 2.8778626918792725 mm

Highest mean error: 3.931877613067627 mm for frame 0

Lowest mean error: 2.587592601776123 mm for frame 205

Saving results

Total time: 126.62915420532227
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00386042
Iteration 2/25 | Loss: 0.00134525
Iteration 3/25 | Loss: 0.00122803
Iteration 4/25 | Loss: 0.00121100
Iteration 5/25 | Loss: 0.00120602
Iteration 6/25 | Loss: 0.00120552
Iteration 7/25 | Loss: 0.00120552
Iteration 8/25 | Loss: 0.00120552
Iteration 9/25 | Loss: 0.00120552
Iteration 10/25 | Loss: 0.00120552
Iteration 11/25 | Loss: 0.00120552
Iteration 12/25 | Loss: 0.00120552
Iteration 13/25 | Loss: 0.00120552
Iteration 14/25 | Loss: 0.00120552
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012055195402354002, 0.0012055195402354002, 0.0012055195402354002, 0.0012055195402354002, 0.0012055195402354002]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012055195402354002

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30956578
Iteration 2/25 | Loss: 0.00141153
Iteration 3/25 | Loss: 0.00141153
Iteration 4/25 | Loss: 0.00141153
Iteration 5/25 | Loss: 0.00141153
Iteration 6/25 | Loss: 0.00141153
Iteration 7/25 | Loss: 0.00141153
Iteration 8/25 | Loss: 0.00141153
Iteration 9/25 | Loss: 0.00141153
Iteration 10/25 | Loss: 0.00141153
Iteration 11/25 | Loss: 0.00141153
Iteration 12/25 | Loss: 0.00141153
Iteration 13/25 | Loss: 0.00141153
Iteration 14/25 | Loss: 0.00141153
Iteration 15/25 | Loss: 0.00141153
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0014115266967564821, 0.0014115266967564821, 0.0014115266967564821, 0.0014115266967564821, 0.0014115266967564821]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014115266967564821

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141153
Iteration 2/1000 | Loss: 0.00004423
Iteration 3/1000 | Loss: 0.00003212
Iteration 4/1000 | Loss: 0.00002572
Iteration 5/1000 | Loss: 0.00002262
Iteration 6/1000 | Loss: 0.00002084
Iteration 7/1000 | Loss: 0.00001954
Iteration 8/1000 | Loss: 0.00001872
Iteration 9/1000 | Loss: 0.00001817
Iteration 10/1000 | Loss: 0.00001775
Iteration 11/1000 | Loss: 0.00001743
Iteration 12/1000 | Loss: 0.00001715
Iteration 13/1000 | Loss: 0.00001705
Iteration 14/1000 | Loss: 0.00001685
Iteration 15/1000 | Loss: 0.00001667
Iteration 16/1000 | Loss: 0.00001664
Iteration 17/1000 | Loss: 0.00001662
Iteration 18/1000 | Loss: 0.00001657
Iteration 19/1000 | Loss: 0.00001655
Iteration 20/1000 | Loss: 0.00001655
Iteration 21/1000 | Loss: 0.00001654
Iteration 22/1000 | Loss: 0.00001653
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001652
Iteration 25/1000 | Loss: 0.00001650
Iteration 26/1000 | Loss: 0.00001648
Iteration 27/1000 | Loss: 0.00001645
Iteration 28/1000 | Loss: 0.00001642
Iteration 29/1000 | Loss: 0.00001639
Iteration 30/1000 | Loss: 0.00001636
Iteration 31/1000 | Loss: 0.00001633
Iteration 32/1000 | Loss: 0.00001633
Iteration 33/1000 | Loss: 0.00001632
Iteration 34/1000 | Loss: 0.00001632
Iteration 35/1000 | Loss: 0.00001629
Iteration 36/1000 | Loss: 0.00001629
Iteration 37/1000 | Loss: 0.00001628
Iteration 38/1000 | Loss: 0.00001627
Iteration 39/1000 | Loss: 0.00001626
Iteration 40/1000 | Loss: 0.00001626
Iteration 41/1000 | Loss: 0.00001625
Iteration 42/1000 | Loss: 0.00001625
Iteration 43/1000 | Loss: 0.00001625
Iteration 44/1000 | Loss: 0.00001624
Iteration 45/1000 | Loss: 0.00001624
Iteration 46/1000 | Loss: 0.00001623
Iteration 47/1000 | Loss: 0.00001623
Iteration 48/1000 | Loss: 0.00001622
Iteration 49/1000 | Loss: 0.00001622
Iteration 50/1000 | Loss: 0.00001621
Iteration 51/1000 | Loss: 0.00001621
Iteration 52/1000 | Loss: 0.00001620
Iteration 53/1000 | Loss: 0.00001620
Iteration 54/1000 | Loss: 0.00001619
Iteration 55/1000 | Loss: 0.00001619
Iteration 56/1000 | Loss: 0.00001619
Iteration 57/1000 | Loss: 0.00001618
Iteration 58/1000 | Loss: 0.00001618
Iteration 59/1000 | Loss: 0.00001617
Iteration 60/1000 | Loss: 0.00001617
Iteration 61/1000 | Loss: 0.00001616
Iteration 62/1000 | Loss: 0.00001616
Iteration 63/1000 | Loss: 0.00001615
Iteration 64/1000 | Loss: 0.00001615
Iteration 65/1000 | Loss: 0.00001614
Iteration 66/1000 | Loss: 0.00001614
Iteration 67/1000 | Loss: 0.00001614
Iteration 68/1000 | Loss: 0.00001613
Iteration 69/1000 | Loss: 0.00001613
Iteration 70/1000 | Loss: 0.00001613
Iteration 71/1000 | Loss: 0.00001612
Iteration 72/1000 | Loss: 0.00001612
Iteration 73/1000 | Loss: 0.00001612
Iteration 74/1000 | Loss: 0.00001612
Iteration 75/1000 | Loss: 0.00001612
Iteration 76/1000 | Loss: 0.00001612
Iteration 77/1000 | Loss: 0.00001612
Iteration 78/1000 | Loss: 0.00001611
Iteration 79/1000 | Loss: 0.00001611
Iteration 80/1000 | Loss: 0.00001611
Iteration 81/1000 | Loss: 0.00001611
Iteration 82/1000 | Loss: 0.00001611
Iteration 83/1000 | Loss: 0.00001610
Iteration 84/1000 | Loss: 0.00001610
Iteration 85/1000 | Loss: 0.00001610
Iteration 86/1000 | Loss: 0.00001610
Iteration 87/1000 | Loss: 0.00001609
Iteration 88/1000 | Loss: 0.00001609
Iteration 89/1000 | Loss: 0.00001609
Iteration 90/1000 | Loss: 0.00001609
Iteration 91/1000 | Loss: 0.00001609
Iteration 92/1000 | Loss: 0.00001609
Iteration 93/1000 | Loss: 0.00001608
Iteration 94/1000 | Loss: 0.00001608
Iteration 95/1000 | Loss: 0.00001608
Iteration 96/1000 | Loss: 0.00001608
Iteration 97/1000 | Loss: 0.00001608
Iteration 98/1000 | Loss: 0.00001607
Iteration 99/1000 | Loss: 0.00001607
Iteration 100/1000 | Loss: 0.00001607
Iteration 101/1000 | Loss: 0.00001606
Iteration 102/1000 | Loss: 0.00001606
Iteration 103/1000 | Loss: 0.00001606
Iteration 104/1000 | Loss: 0.00001606
Iteration 105/1000 | Loss: 0.00001605
Iteration 106/1000 | Loss: 0.00001605
Iteration 107/1000 | Loss: 0.00001605
Iteration 108/1000 | Loss: 0.00001605
Iteration 109/1000 | Loss: 0.00001605
Iteration 110/1000 | Loss: 0.00001605
Iteration 111/1000 | Loss: 0.00001604
Iteration 112/1000 | Loss: 0.00001604
Iteration 113/1000 | Loss: 0.00001604
Iteration 114/1000 | Loss: 0.00001604
Iteration 115/1000 | Loss: 0.00001604
Iteration 116/1000 | Loss: 0.00001604
Iteration 117/1000 | Loss: 0.00001604
Iteration 118/1000 | Loss: 0.00001604
Iteration 119/1000 | Loss: 0.00001603
Iteration 120/1000 | Loss: 0.00001603
Iteration 121/1000 | Loss: 0.00001603
Iteration 122/1000 | Loss: 0.00001603
Iteration 123/1000 | Loss: 0.00001602
Iteration 124/1000 | Loss: 0.00001602
Iteration 125/1000 | Loss: 0.00001602
Iteration 126/1000 | Loss: 0.00001601
Iteration 127/1000 | Loss: 0.00001601
Iteration 128/1000 | Loss: 0.00001601
Iteration 129/1000 | Loss: 0.00001601
Iteration 130/1000 | Loss: 0.00001600
Iteration 131/1000 | Loss: 0.00001600
Iteration 132/1000 | Loss: 0.00001600
Iteration 133/1000 | Loss: 0.00001600
Iteration 134/1000 | Loss: 0.00001600
Iteration 135/1000 | Loss: 0.00001600
Iteration 136/1000 | Loss: 0.00001600
Iteration 137/1000 | Loss: 0.00001600
Iteration 138/1000 | Loss: 0.00001600
Iteration 139/1000 | Loss: 0.00001599
Iteration 140/1000 | Loss: 0.00001599
Iteration 141/1000 | Loss: 0.00001599
Iteration 142/1000 | Loss: 0.00001599
Iteration 143/1000 | Loss: 0.00001598
Iteration 144/1000 | Loss: 0.00001598
Iteration 145/1000 | Loss: 0.00001598
Iteration 146/1000 | Loss: 0.00001598
Iteration 147/1000 | Loss: 0.00001597
Iteration 148/1000 | Loss: 0.00001597
Iteration 149/1000 | Loss: 0.00001597
Iteration 150/1000 | Loss: 0.00001597
Iteration 151/1000 | Loss: 0.00001597
Iteration 152/1000 | Loss: 0.00001596
Iteration 153/1000 | Loss: 0.00001596
Iteration 154/1000 | Loss: 0.00001596
Iteration 155/1000 | Loss: 0.00001596
Iteration 156/1000 | Loss: 0.00001596
Iteration 157/1000 | Loss: 0.00001596
Iteration 158/1000 | Loss: 0.00001595
Iteration 159/1000 | Loss: 0.00001595
Iteration 160/1000 | Loss: 0.00001595
Iteration 161/1000 | Loss: 0.00001594
Iteration 162/1000 | Loss: 0.00001594
Iteration 163/1000 | Loss: 0.00001594
Iteration 164/1000 | Loss: 0.00001594
Iteration 165/1000 | Loss: 0.00001593
Iteration 166/1000 | Loss: 0.00001593
Iteration 167/1000 | Loss: 0.00001593
Iteration 168/1000 | Loss: 0.00001593
Iteration 169/1000 | Loss: 0.00001592
Iteration 170/1000 | Loss: 0.00001592
Iteration 171/1000 | Loss: 0.00001592
Iteration 172/1000 | Loss: 0.00001592
Iteration 173/1000 | Loss: 0.00001591
Iteration 174/1000 | Loss: 0.00001591
Iteration 175/1000 | Loss: 0.00001591
Iteration 176/1000 | Loss: 0.00001591
Iteration 177/1000 | Loss: 0.00001591
Iteration 178/1000 | Loss: 0.00001591
Iteration 179/1000 | Loss: 0.00001591
Iteration 180/1000 | Loss: 0.00001591
Iteration 181/1000 | Loss: 0.00001591
Iteration 182/1000 | Loss: 0.00001590
Iteration 183/1000 | Loss: 0.00001590
Iteration 184/1000 | Loss: 0.00001590
Iteration 185/1000 | Loss: 0.00001590
Iteration 186/1000 | Loss: 0.00001590
Iteration 187/1000 | Loss: 0.00001590
Iteration 188/1000 | Loss: 0.00001590
Iteration 189/1000 | Loss: 0.00001590
Iteration 190/1000 | Loss: 0.00001590
Iteration 191/1000 | Loss: 0.00001590
Iteration 192/1000 | Loss: 0.00001590
Iteration 193/1000 | Loss: 0.00001590
Iteration 194/1000 | Loss: 0.00001590
Iteration 195/1000 | Loss: 0.00001589
Iteration 196/1000 | Loss: 0.00001589
Iteration 197/1000 | Loss: 0.00001589
Iteration 198/1000 | Loss: 0.00001589
Iteration 199/1000 | Loss: 0.00001589
Iteration 200/1000 | Loss: 0.00001589
Iteration 201/1000 | Loss: 0.00001588
Iteration 202/1000 | Loss: 0.00001588
Iteration 203/1000 | Loss: 0.00001588
Iteration 204/1000 | Loss: 0.00001588
Iteration 205/1000 | Loss: 0.00001588
Iteration 206/1000 | Loss: 0.00001587
Iteration 207/1000 | Loss: 0.00001587
Iteration 208/1000 | Loss: 0.00001587
Iteration 209/1000 | Loss: 0.00001587
Iteration 210/1000 | Loss: 0.00001587
Iteration 211/1000 | Loss: 0.00001587
Iteration 212/1000 | Loss: 0.00001587
Iteration 213/1000 | Loss: 0.00001587
Iteration 214/1000 | Loss: 0.00001587
Iteration 215/1000 | Loss: 0.00001586
Iteration 216/1000 | Loss: 0.00001586
Iteration 217/1000 | Loss: 0.00001586
Iteration 218/1000 | Loss: 0.00001586
Iteration 219/1000 | Loss: 0.00001586
Iteration 220/1000 | Loss: 0.00001586
Iteration 221/1000 | Loss: 0.00001586
Iteration 222/1000 | Loss: 0.00001586
Iteration 223/1000 | Loss: 0.00001586
Iteration 224/1000 | Loss: 0.00001586
Iteration 225/1000 | Loss: 0.00001586
Iteration 226/1000 | Loss: 0.00001586
Iteration 227/1000 | Loss: 0.00001586
Iteration 228/1000 | Loss: 0.00001585
Iteration 229/1000 | Loss: 0.00001585
Iteration 230/1000 | Loss: 0.00001585
Iteration 231/1000 | Loss: 0.00001585
Iteration 232/1000 | Loss: 0.00001585
Iteration 233/1000 | Loss: 0.00001585
Iteration 234/1000 | Loss: 0.00001585
Iteration 235/1000 | Loss: 0.00001585
Iteration 236/1000 | Loss: 0.00001585
Iteration 237/1000 | Loss: 0.00001585
Iteration 238/1000 | Loss: 0.00001585
Iteration 239/1000 | Loss: 0.00001584
Iteration 240/1000 | Loss: 0.00001584
Iteration 241/1000 | Loss: 0.00001584
Iteration 242/1000 | Loss: 0.00001584
Iteration 243/1000 | Loss: 0.00001584
Iteration 244/1000 | Loss: 0.00001584
Iteration 245/1000 | Loss: 0.00001584
Iteration 246/1000 | Loss: 0.00001584
Iteration 247/1000 | Loss: 0.00001584
Iteration 248/1000 | Loss: 0.00001584
Iteration 249/1000 | Loss: 0.00001584
Iteration 250/1000 | Loss: 0.00001584
Iteration 251/1000 | Loss: 0.00001584
Iteration 252/1000 | Loss: 0.00001584
Iteration 253/1000 | Loss: 0.00001584
Iteration 254/1000 | Loss: 0.00001584
Iteration 255/1000 | Loss: 0.00001584
Iteration 256/1000 | Loss: 0.00001584
Iteration 257/1000 | Loss: 0.00001584
Iteration 258/1000 | Loss: 0.00001584
Iteration 259/1000 | Loss: 0.00001584
Iteration 260/1000 | Loss: 0.00001584
Iteration 261/1000 | Loss: 0.00001584
Iteration 262/1000 | Loss: 0.00001584
Iteration 263/1000 | Loss: 0.00001584
Iteration 264/1000 | Loss: 0.00001584
Iteration 265/1000 | Loss: 0.00001584
Iteration 266/1000 | Loss: 0.00001584
Iteration 267/1000 | Loss: 0.00001584
Iteration 268/1000 | Loss: 0.00001584
Iteration 269/1000 | Loss: 0.00001584
Iteration 270/1000 | Loss: 0.00001584
Iteration 271/1000 | Loss: 0.00001584
Iteration 272/1000 | Loss: 0.00001584
Iteration 273/1000 | Loss: 0.00001584
Iteration 274/1000 | Loss: 0.00001584
Iteration 275/1000 | Loss: 0.00001584
Iteration 276/1000 | Loss: 0.00001584
Iteration 277/1000 | Loss: 0.00001584
Iteration 278/1000 | Loss: 0.00001584
Iteration 279/1000 | Loss: 0.00001584
Iteration 280/1000 | Loss: 0.00001584
Iteration 281/1000 | Loss: 0.00001584
Iteration 282/1000 | Loss: 0.00001584
Iteration 283/1000 | Loss: 0.00001584
Iteration 284/1000 | Loss: 0.00001584
Iteration 285/1000 | Loss: 0.00001584
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 285. Stopping optimization.
Last 5 losses: [1.5840363630559295e-05, 1.5840363630559295e-05, 1.5840363630559295e-05, 1.5840363630559295e-05, 1.5840363630559295e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5840363630559295e-05

Optimization complete. Final v2v error: 3.314840078353882 mm

Highest mean error: 4.1681413650512695 mm for frame 143

Lowest mean error: 2.607893705368042 mm for frame 2

Saving results

Total time: 56.321977615356445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813239
Iteration 2/25 | Loss: 0.00128073
Iteration 3/25 | Loss: 0.00120500
Iteration 4/25 | Loss: 0.00119150
Iteration 5/25 | Loss: 0.00118793
Iteration 6/25 | Loss: 0.00118740
Iteration 7/25 | Loss: 0.00118740
Iteration 8/25 | Loss: 0.00118740
Iteration 9/25 | Loss: 0.00118740
Iteration 10/25 | Loss: 0.00118740
Iteration 11/25 | Loss: 0.00118740
Iteration 12/25 | Loss: 0.00118740
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011874004267156124, 0.0011874004267156124, 0.0011874004267156124, 0.0011874004267156124, 0.0011874004267156124]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011874004267156124

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.34738016
Iteration 2/25 | Loss: 0.00110867
Iteration 3/25 | Loss: 0.00110866
Iteration 4/25 | Loss: 0.00110866
Iteration 5/25 | Loss: 0.00110866
Iteration 6/25 | Loss: 0.00110866
Iteration 7/25 | Loss: 0.00110866
Iteration 8/25 | Loss: 0.00110866
Iteration 9/25 | Loss: 0.00110866
Iteration 10/25 | Loss: 0.00110866
Iteration 11/25 | Loss: 0.00110866
Iteration 12/25 | Loss: 0.00110866
Iteration 13/25 | Loss: 0.00110866
Iteration 14/25 | Loss: 0.00110866
Iteration 15/25 | Loss: 0.00110866
Iteration 16/25 | Loss: 0.00110866
Iteration 17/25 | Loss: 0.00110866
Iteration 18/25 | Loss: 0.00110866
Iteration 19/25 | Loss: 0.00110866
Iteration 20/25 | Loss: 0.00110866
Iteration 21/25 | Loss: 0.00110866
Iteration 22/25 | Loss: 0.00110866
Iteration 23/25 | Loss: 0.00110866
Iteration 24/25 | Loss: 0.00110866
Iteration 25/25 | Loss: 0.00110866

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110866
Iteration 2/1000 | Loss: 0.00002522
Iteration 3/1000 | Loss: 0.00001636
Iteration 4/1000 | Loss: 0.00001505
Iteration 5/1000 | Loss: 0.00001429
Iteration 6/1000 | Loss: 0.00001376
Iteration 7/1000 | Loss: 0.00001327
Iteration 8/1000 | Loss: 0.00001297
Iteration 9/1000 | Loss: 0.00001271
Iteration 10/1000 | Loss: 0.00001246
Iteration 11/1000 | Loss: 0.00001241
Iteration 12/1000 | Loss: 0.00001229
Iteration 13/1000 | Loss: 0.00001218
Iteration 14/1000 | Loss: 0.00001218
Iteration 15/1000 | Loss: 0.00001210
Iteration 16/1000 | Loss: 0.00001210
Iteration 17/1000 | Loss: 0.00001209
Iteration 18/1000 | Loss: 0.00001209
Iteration 19/1000 | Loss: 0.00001208
Iteration 20/1000 | Loss: 0.00001208
Iteration 21/1000 | Loss: 0.00001208
Iteration 22/1000 | Loss: 0.00001208
Iteration 23/1000 | Loss: 0.00001206
Iteration 24/1000 | Loss: 0.00001202
Iteration 25/1000 | Loss: 0.00001198
Iteration 26/1000 | Loss: 0.00001195
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001194
Iteration 29/1000 | Loss: 0.00001193
Iteration 30/1000 | Loss: 0.00001193
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001190
Iteration 37/1000 | Loss: 0.00001186
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001185
Iteration 40/1000 | Loss: 0.00001185
Iteration 41/1000 | Loss: 0.00001184
Iteration 42/1000 | Loss: 0.00001184
Iteration 43/1000 | Loss: 0.00001184
Iteration 44/1000 | Loss: 0.00001184
Iteration 45/1000 | Loss: 0.00001184
Iteration 46/1000 | Loss: 0.00001184
Iteration 47/1000 | Loss: 0.00001183
Iteration 48/1000 | Loss: 0.00001183
Iteration 49/1000 | Loss: 0.00001182
Iteration 50/1000 | Loss: 0.00001182
Iteration 51/1000 | Loss: 0.00001182
Iteration 52/1000 | Loss: 0.00001181
Iteration 53/1000 | Loss: 0.00001181
Iteration 54/1000 | Loss: 0.00001181
Iteration 55/1000 | Loss: 0.00001180
Iteration 56/1000 | Loss: 0.00001180
Iteration 57/1000 | Loss: 0.00001180
Iteration 58/1000 | Loss: 0.00001179
Iteration 59/1000 | Loss: 0.00001179
Iteration 60/1000 | Loss: 0.00001179
Iteration 61/1000 | Loss: 0.00001178
Iteration 62/1000 | Loss: 0.00001178
Iteration 63/1000 | Loss: 0.00001178
Iteration 64/1000 | Loss: 0.00001177
Iteration 65/1000 | Loss: 0.00001177
Iteration 66/1000 | Loss: 0.00001177
Iteration 67/1000 | Loss: 0.00001176
Iteration 68/1000 | Loss: 0.00001176
Iteration 69/1000 | Loss: 0.00001175
Iteration 70/1000 | Loss: 0.00001175
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001173
Iteration 75/1000 | Loss: 0.00001172
Iteration 76/1000 | Loss: 0.00001172
Iteration 77/1000 | Loss: 0.00001172
Iteration 78/1000 | Loss: 0.00001171
Iteration 79/1000 | Loss: 0.00001171
Iteration 80/1000 | Loss: 0.00001171
Iteration 81/1000 | Loss: 0.00001170
Iteration 82/1000 | Loss: 0.00001169
Iteration 83/1000 | Loss: 0.00001168
Iteration 84/1000 | Loss: 0.00001168
Iteration 85/1000 | Loss: 0.00001168
Iteration 86/1000 | Loss: 0.00001168
Iteration 87/1000 | Loss: 0.00001168
Iteration 88/1000 | Loss: 0.00001168
Iteration 89/1000 | Loss: 0.00001167
Iteration 90/1000 | Loss: 0.00001167
Iteration 91/1000 | Loss: 0.00001166
Iteration 92/1000 | Loss: 0.00001166
Iteration 93/1000 | Loss: 0.00001165
Iteration 94/1000 | Loss: 0.00001165
Iteration 95/1000 | Loss: 0.00001165
Iteration 96/1000 | Loss: 0.00001164
Iteration 97/1000 | Loss: 0.00001163
Iteration 98/1000 | Loss: 0.00001163
Iteration 99/1000 | Loss: 0.00001163
Iteration 100/1000 | Loss: 0.00001162
Iteration 101/1000 | Loss: 0.00001162
Iteration 102/1000 | Loss: 0.00001162
Iteration 103/1000 | Loss: 0.00001162
Iteration 104/1000 | Loss: 0.00001162
Iteration 105/1000 | Loss: 0.00001162
Iteration 106/1000 | Loss: 0.00001162
Iteration 107/1000 | Loss: 0.00001162
Iteration 108/1000 | Loss: 0.00001162
Iteration 109/1000 | Loss: 0.00001162
Iteration 110/1000 | Loss: 0.00001162
Iteration 111/1000 | Loss: 0.00001162
Iteration 112/1000 | Loss: 0.00001162
Iteration 113/1000 | Loss: 0.00001162
Iteration 114/1000 | Loss: 0.00001162
Iteration 115/1000 | Loss: 0.00001162
Iteration 116/1000 | Loss: 0.00001162
Iteration 117/1000 | Loss: 0.00001162
Iteration 118/1000 | Loss: 0.00001162
Iteration 119/1000 | Loss: 0.00001162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [1.1623360478552058e-05, 1.1623360478552058e-05, 1.1623360478552058e-05, 1.1623360478552058e-05, 1.1623360478552058e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1623360478552058e-05

Optimization complete. Final v2v error: 2.9670770168304443 mm

Highest mean error: 3.0165088176727295 mm for frame 71

Lowest mean error: 2.9134151935577393 mm for frame 95

Saving results

Total time: 34.72363901138306
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882867
Iteration 2/25 | Loss: 0.00136772
Iteration 3/25 | Loss: 0.00125566
Iteration 4/25 | Loss: 0.00123684
Iteration 5/25 | Loss: 0.00123057
Iteration 6/25 | Loss: 0.00122883
Iteration 7/25 | Loss: 0.00122883
Iteration 8/25 | Loss: 0.00122883
Iteration 9/25 | Loss: 0.00122883
Iteration 10/25 | Loss: 0.00122883
Iteration 11/25 | Loss: 0.00122883
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001228834385983646, 0.001228834385983646, 0.001228834385983646, 0.001228834385983646, 0.001228834385983646]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001228834385983646

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33905435
Iteration 2/25 | Loss: 0.00117712
Iteration 3/25 | Loss: 0.00117711
Iteration 4/25 | Loss: 0.00117711
Iteration 5/25 | Loss: 0.00117711
Iteration 6/25 | Loss: 0.00117711
Iteration 7/25 | Loss: 0.00117711
Iteration 8/25 | Loss: 0.00117711
Iteration 9/25 | Loss: 0.00117711
Iteration 10/25 | Loss: 0.00117710
Iteration 11/25 | Loss: 0.00117710
Iteration 12/25 | Loss: 0.00117710
Iteration 13/25 | Loss: 0.00117710
Iteration 14/25 | Loss: 0.00117710
Iteration 15/25 | Loss: 0.00117710
Iteration 16/25 | Loss: 0.00117710
Iteration 17/25 | Loss: 0.00117710
Iteration 18/25 | Loss: 0.00117710
Iteration 19/25 | Loss: 0.00117710
Iteration 20/25 | Loss: 0.00117710
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011771046556532383, 0.0011771046556532383, 0.0011771046556532383, 0.0011771046556532383, 0.0011771046556532383]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011771046556532383

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117710
Iteration 2/1000 | Loss: 0.00004068
Iteration 3/1000 | Loss: 0.00002636
Iteration 4/1000 | Loss: 0.00002210
Iteration 5/1000 | Loss: 0.00002052
Iteration 6/1000 | Loss: 0.00001961
Iteration 7/1000 | Loss: 0.00001891
Iteration 8/1000 | Loss: 0.00001839
Iteration 9/1000 | Loss: 0.00001789
Iteration 10/1000 | Loss: 0.00001757
Iteration 11/1000 | Loss: 0.00001734
Iteration 12/1000 | Loss: 0.00001714
Iteration 13/1000 | Loss: 0.00001711
Iteration 14/1000 | Loss: 0.00001701
Iteration 15/1000 | Loss: 0.00001700
Iteration 16/1000 | Loss: 0.00001695
Iteration 17/1000 | Loss: 0.00001694
Iteration 18/1000 | Loss: 0.00001692
Iteration 19/1000 | Loss: 0.00001690
Iteration 20/1000 | Loss: 0.00001690
Iteration 21/1000 | Loss: 0.00001687
Iteration 22/1000 | Loss: 0.00001684
Iteration 23/1000 | Loss: 0.00001683
Iteration 24/1000 | Loss: 0.00001682
Iteration 25/1000 | Loss: 0.00001682
Iteration 26/1000 | Loss: 0.00001681
Iteration 27/1000 | Loss: 0.00001676
Iteration 28/1000 | Loss: 0.00001672
Iteration 29/1000 | Loss: 0.00001671
Iteration 30/1000 | Loss: 0.00001671
Iteration 31/1000 | Loss: 0.00001671
Iteration 32/1000 | Loss: 0.00001670
Iteration 33/1000 | Loss: 0.00001670
Iteration 34/1000 | Loss: 0.00001669
Iteration 35/1000 | Loss: 0.00001669
Iteration 36/1000 | Loss: 0.00001668
Iteration 37/1000 | Loss: 0.00001668
Iteration 38/1000 | Loss: 0.00001668
Iteration 39/1000 | Loss: 0.00001668
Iteration 40/1000 | Loss: 0.00001667
Iteration 41/1000 | Loss: 0.00001667
Iteration 42/1000 | Loss: 0.00001667
Iteration 43/1000 | Loss: 0.00001666
Iteration 44/1000 | Loss: 0.00001666
Iteration 45/1000 | Loss: 0.00001665
Iteration 46/1000 | Loss: 0.00001664
Iteration 47/1000 | Loss: 0.00001664
Iteration 48/1000 | Loss: 0.00001663
Iteration 49/1000 | Loss: 0.00001663
Iteration 50/1000 | Loss: 0.00001662
Iteration 51/1000 | Loss: 0.00001661
Iteration 52/1000 | Loss: 0.00001661
Iteration 53/1000 | Loss: 0.00001660
Iteration 54/1000 | Loss: 0.00001659
Iteration 55/1000 | Loss: 0.00001659
Iteration 56/1000 | Loss: 0.00001659
Iteration 57/1000 | Loss: 0.00001656
Iteration 58/1000 | Loss: 0.00001656
Iteration 59/1000 | Loss: 0.00001655
Iteration 60/1000 | Loss: 0.00001653
Iteration 61/1000 | Loss: 0.00001653
Iteration 62/1000 | Loss: 0.00001653
Iteration 63/1000 | Loss: 0.00001653
Iteration 64/1000 | Loss: 0.00001653
Iteration 65/1000 | Loss: 0.00001653
Iteration 66/1000 | Loss: 0.00001652
Iteration 67/1000 | Loss: 0.00001652
Iteration 68/1000 | Loss: 0.00001652
Iteration 69/1000 | Loss: 0.00001652
Iteration 70/1000 | Loss: 0.00001652
Iteration 71/1000 | Loss: 0.00001652
Iteration 72/1000 | Loss: 0.00001652
Iteration 73/1000 | Loss: 0.00001652
Iteration 74/1000 | Loss: 0.00001651
Iteration 75/1000 | Loss: 0.00001651
Iteration 76/1000 | Loss: 0.00001649
Iteration 77/1000 | Loss: 0.00001649
Iteration 78/1000 | Loss: 0.00001649
Iteration 79/1000 | Loss: 0.00001649
Iteration 80/1000 | Loss: 0.00001648
Iteration 81/1000 | Loss: 0.00001648
Iteration 82/1000 | Loss: 0.00001648
Iteration 83/1000 | Loss: 0.00001647
Iteration 84/1000 | Loss: 0.00001647
Iteration 85/1000 | Loss: 0.00001647
Iteration 86/1000 | Loss: 0.00001647
Iteration 87/1000 | Loss: 0.00001647
Iteration 88/1000 | Loss: 0.00001646
Iteration 89/1000 | Loss: 0.00001646
Iteration 90/1000 | Loss: 0.00001645
Iteration 91/1000 | Loss: 0.00001645
Iteration 92/1000 | Loss: 0.00001645
Iteration 93/1000 | Loss: 0.00001645
Iteration 94/1000 | Loss: 0.00001645
Iteration 95/1000 | Loss: 0.00001645
Iteration 96/1000 | Loss: 0.00001644
Iteration 97/1000 | Loss: 0.00001644
Iteration 98/1000 | Loss: 0.00001644
Iteration 99/1000 | Loss: 0.00001643
Iteration 100/1000 | Loss: 0.00001643
Iteration 101/1000 | Loss: 0.00001643
Iteration 102/1000 | Loss: 0.00001643
Iteration 103/1000 | Loss: 0.00001642
Iteration 104/1000 | Loss: 0.00001642
Iteration 105/1000 | Loss: 0.00001642
Iteration 106/1000 | Loss: 0.00001642
Iteration 107/1000 | Loss: 0.00001642
Iteration 108/1000 | Loss: 0.00001641
Iteration 109/1000 | Loss: 0.00001641
Iteration 110/1000 | Loss: 0.00001641
Iteration 111/1000 | Loss: 0.00001641
Iteration 112/1000 | Loss: 0.00001641
Iteration 113/1000 | Loss: 0.00001641
Iteration 114/1000 | Loss: 0.00001640
Iteration 115/1000 | Loss: 0.00001640
Iteration 116/1000 | Loss: 0.00001640
Iteration 117/1000 | Loss: 0.00001639
Iteration 118/1000 | Loss: 0.00001639
Iteration 119/1000 | Loss: 0.00001638
Iteration 120/1000 | Loss: 0.00001638
Iteration 121/1000 | Loss: 0.00001638
Iteration 122/1000 | Loss: 0.00001638
Iteration 123/1000 | Loss: 0.00001637
Iteration 124/1000 | Loss: 0.00001637
Iteration 125/1000 | Loss: 0.00001637
Iteration 126/1000 | Loss: 0.00001637
Iteration 127/1000 | Loss: 0.00001637
Iteration 128/1000 | Loss: 0.00001637
Iteration 129/1000 | Loss: 0.00001636
Iteration 130/1000 | Loss: 0.00001636
Iteration 131/1000 | Loss: 0.00001635
Iteration 132/1000 | Loss: 0.00001635
Iteration 133/1000 | Loss: 0.00001635
Iteration 134/1000 | Loss: 0.00001635
Iteration 135/1000 | Loss: 0.00001635
Iteration 136/1000 | Loss: 0.00001634
Iteration 137/1000 | Loss: 0.00001634
Iteration 138/1000 | Loss: 0.00001634
Iteration 139/1000 | Loss: 0.00001634
Iteration 140/1000 | Loss: 0.00001633
Iteration 141/1000 | Loss: 0.00001633
Iteration 142/1000 | Loss: 0.00001633
Iteration 143/1000 | Loss: 0.00001633
Iteration 144/1000 | Loss: 0.00001633
Iteration 145/1000 | Loss: 0.00001632
Iteration 146/1000 | Loss: 0.00001632
Iteration 147/1000 | Loss: 0.00001632
Iteration 148/1000 | Loss: 0.00001631
Iteration 149/1000 | Loss: 0.00001631
Iteration 150/1000 | Loss: 0.00001631
Iteration 151/1000 | Loss: 0.00001630
Iteration 152/1000 | Loss: 0.00001630
Iteration 153/1000 | Loss: 0.00001630
Iteration 154/1000 | Loss: 0.00001630
Iteration 155/1000 | Loss: 0.00001630
Iteration 156/1000 | Loss: 0.00001629
Iteration 157/1000 | Loss: 0.00001629
Iteration 158/1000 | Loss: 0.00001629
Iteration 159/1000 | Loss: 0.00001629
Iteration 160/1000 | Loss: 0.00001629
Iteration 161/1000 | Loss: 0.00001629
Iteration 162/1000 | Loss: 0.00001628
Iteration 163/1000 | Loss: 0.00001628
Iteration 164/1000 | Loss: 0.00001628
Iteration 165/1000 | Loss: 0.00001628
Iteration 166/1000 | Loss: 0.00001627
Iteration 167/1000 | Loss: 0.00001627
Iteration 168/1000 | Loss: 0.00001627
Iteration 169/1000 | Loss: 0.00001627
Iteration 170/1000 | Loss: 0.00001627
Iteration 171/1000 | Loss: 0.00001627
Iteration 172/1000 | Loss: 0.00001627
Iteration 173/1000 | Loss: 0.00001626
Iteration 174/1000 | Loss: 0.00001626
Iteration 175/1000 | Loss: 0.00001626
Iteration 176/1000 | Loss: 0.00001626
Iteration 177/1000 | Loss: 0.00001626
Iteration 178/1000 | Loss: 0.00001626
Iteration 179/1000 | Loss: 0.00001625
Iteration 180/1000 | Loss: 0.00001625
Iteration 181/1000 | Loss: 0.00001625
Iteration 182/1000 | Loss: 0.00001625
Iteration 183/1000 | Loss: 0.00001625
Iteration 184/1000 | Loss: 0.00001625
Iteration 185/1000 | Loss: 0.00001625
Iteration 186/1000 | Loss: 0.00001625
Iteration 187/1000 | Loss: 0.00001625
Iteration 188/1000 | Loss: 0.00001625
Iteration 189/1000 | Loss: 0.00001625
Iteration 190/1000 | Loss: 0.00001625
Iteration 191/1000 | Loss: 0.00001625
Iteration 192/1000 | Loss: 0.00001625
Iteration 193/1000 | Loss: 0.00001625
Iteration 194/1000 | Loss: 0.00001625
Iteration 195/1000 | Loss: 0.00001625
Iteration 196/1000 | Loss: 0.00001625
Iteration 197/1000 | Loss: 0.00001625
Iteration 198/1000 | Loss: 0.00001625
Iteration 199/1000 | Loss: 0.00001624
Iteration 200/1000 | Loss: 0.00001624
Iteration 201/1000 | Loss: 0.00001624
Iteration 202/1000 | Loss: 0.00001624
Iteration 203/1000 | Loss: 0.00001624
Iteration 204/1000 | Loss: 0.00001624
Iteration 205/1000 | Loss: 0.00001624
Iteration 206/1000 | Loss: 0.00001624
Iteration 207/1000 | Loss: 0.00001624
Iteration 208/1000 | Loss: 0.00001624
Iteration 209/1000 | Loss: 0.00001624
Iteration 210/1000 | Loss: 0.00001624
Iteration 211/1000 | Loss: 0.00001624
Iteration 212/1000 | Loss: 0.00001624
Iteration 213/1000 | Loss: 0.00001624
Iteration 214/1000 | Loss: 0.00001623
Iteration 215/1000 | Loss: 0.00001623
Iteration 216/1000 | Loss: 0.00001623
Iteration 217/1000 | Loss: 0.00001623
Iteration 218/1000 | Loss: 0.00001623
Iteration 219/1000 | Loss: 0.00001623
Iteration 220/1000 | Loss: 0.00001623
Iteration 221/1000 | Loss: 0.00001623
Iteration 222/1000 | Loss: 0.00001623
Iteration 223/1000 | Loss: 0.00001623
Iteration 224/1000 | Loss: 0.00001623
Iteration 225/1000 | Loss: 0.00001623
Iteration 226/1000 | Loss: 0.00001623
Iteration 227/1000 | Loss: 0.00001623
Iteration 228/1000 | Loss: 0.00001623
Iteration 229/1000 | Loss: 0.00001623
Iteration 230/1000 | Loss: 0.00001623
Iteration 231/1000 | Loss: 0.00001623
Iteration 232/1000 | Loss: 0.00001623
Iteration 233/1000 | Loss: 0.00001623
Iteration 234/1000 | Loss: 0.00001623
Iteration 235/1000 | Loss: 0.00001622
Iteration 236/1000 | Loss: 0.00001622
Iteration 237/1000 | Loss: 0.00001622
Iteration 238/1000 | Loss: 0.00001622
Iteration 239/1000 | Loss: 0.00001622
Iteration 240/1000 | Loss: 0.00001622
Iteration 241/1000 | Loss: 0.00001622
Iteration 242/1000 | Loss: 0.00001622
Iteration 243/1000 | Loss: 0.00001622
Iteration 244/1000 | Loss: 0.00001622
Iteration 245/1000 | Loss: 0.00001622
Iteration 246/1000 | Loss: 0.00001622
Iteration 247/1000 | Loss: 0.00001622
Iteration 248/1000 | Loss: 0.00001622
Iteration 249/1000 | Loss: 0.00001622
Iteration 250/1000 | Loss: 0.00001622
Iteration 251/1000 | Loss: 0.00001622
Iteration 252/1000 | Loss: 0.00001622
Iteration 253/1000 | Loss: 0.00001622
Iteration 254/1000 | Loss: 0.00001622
Iteration 255/1000 | Loss: 0.00001622
Iteration 256/1000 | Loss: 0.00001622
Iteration 257/1000 | Loss: 0.00001622
Iteration 258/1000 | Loss: 0.00001622
Iteration 259/1000 | Loss: 0.00001622
Iteration 260/1000 | Loss: 0.00001622
Iteration 261/1000 | Loss: 0.00001622
Iteration 262/1000 | Loss: 0.00001622
Iteration 263/1000 | Loss: 0.00001622
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 263. Stopping optimization.
Last 5 losses: [1.6215548384934664e-05, 1.6215548384934664e-05, 1.6215548384934664e-05, 1.6215548384934664e-05, 1.6215548384934664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6215548384934664e-05

Optimization complete. Final v2v error: 3.406463384628296 mm

Highest mean error: 5.045254707336426 mm for frame 69

Lowest mean error: 2.973383665084839 mm for frame 50

Saving results

Total time: 48.09910774230957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01008228
Iteration 2/25 | Loss: 0.00154367
Iteration 3/25 | Loss: 0.00135826
Iteration 4/25 | Loss: 0.00133256
Iteration 5/25 | Loss: 0.00132486
Iteration 6/25 | Loss: 0.00132287
Iteration 7/25 | Loss: 0.00132282
Iteration 8/25 | Loss: 0.00132282
Iteration 9/25 | Loss: 0.00132282
Iteration 10/25 | Loss: 0.00132282
Iteration 11/25 | Loss: 0.00132282
Iteration 12/25 | Loss: 0.00132282
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0013228156603872776, 0.0013228156603872776, 0.0013228156603872776, 0.0013228156603872776, 0.0013228156603872776]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013228156603872776

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.69719583
Iteration 2/25 | Loss: 0.00144897
Iteration 3/25 | Loss: 0.00144893
Iteration 4/25 | Loss: 0.00144893
Iteration 5/25 | Loss: 0.00144893
Iteration 6/25 | Loss: 0.00144893
Iteration 7/25 | Loss: 0.00144893
Iteration 8/25 | Loss: 0.00144892
Iteration 9/25 | Loss: 0.00144892
Iteration 10/25 | Loss: 0.00144892
Iteration 11/25 | Loss: 0.00144892
Iteration 12/25 | Loss: 0.00144892
Iteration 13/25 | Loss: 0.00144892
Iteration 14/25 | Loss: 0.00144892
Iteration 15/25 | Loss: 0.00144892
Iteration 16/25 | Loss: 0.00144892
Iteration 17/25 | Loss: 0.00144892
Iteration 18/25 | Loss: 0.00144892
Iteration 19/25 | Loss: 0.00144892
Iteration 20/25 | Loss: 0.00144892
Iteration 21/25 | Loss: 0.00144892
Iteration 22/25 | Loss: 0.00144892
Iteration 23/25 | Loss: 0.00144892
Iteration 24/25 | Loss: 0.00144892
Iteration 25/25 | Loss: 0.00144892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144892
Iteration 2/1000 | Loss: 0.00005954
Iteration 3/1000 | Loss: 0.00003911
Iteration 4/1000 | Loss: 0.00003367
Iteration 5/1000 | Loss: 0.00003197
Iteration 6/1000 | Loss: 0.00003069
Iteration 7/1000 | Loss: 0.00002987
Iteration 8/1000 | Loss: 0.00002899
Iteration 9/1000 | Loss: 0.00002849
Iteration 10/1000 | Loss: 0.00002804
Iteration 11/1000 | Loss: 0.00002773
Iteration 12/1000 | Loss: 0.00002740
Iteration 13/1000 | Loss: 0.00002720
Iteration 14/1000 | Loss: 0.00002699
Iteration 15/1000 | Loss: 0.00002681
Iteration 16/1000 | Loss: 0.00002672
Iteration 17/1000 | Loss: 0.00002664
Iteration 18/1000 | Loss: 0.00002658
Iteration 19/1000 | Loss: 0.00002658
Iteration 20/1000 | Loss: 0.00002653
Iteration 21/1000 | Loss: 0.00002652
Iteration 22/1000 | Loss: 0.00002651
Iteration 23/1000 | Loss: 0.00002649
Iteration 24/1000 | Loss: 0.00002649
Iteration 25/1000 | Loss: 0.00002649
Iteration 26/1000 | Loss: 0.00002648
Iteration 27/1000 | Loss: 0.00002647
Iteration 28/1000 | Loss: 0.00002647
Iteration 29/1000 | Loss: 0.00002647
Iteration 30/1000 | Loss: 0.00002647
Iteration 31/1000 | Loss: 0.00002647
Iteration 32/1000 | Loss: 0.00002647
Iteration 33/1000 | Loss: 0.00002646
Iteration 34/1000 | Loss: 0.00002646
Iteration 35/1000 | Loss: 0.00002646
Iteration 36/1000 | Loss: 0.00002644
Iteration 37/1000 | Loss: 0.00002644
Iteration 38/1000 | Loss: 0.00002644
Iteration 39/1000 | Loss: 0.00002644
Iteration 40/1000 | Loss: 0.00002644
Iteration 41/1000 | Loss: 0.00002644
Iteration 42/1000 | Loss: 0.00002644
Iteration 43/1000 | Loss: 0.00002644
Iteration 44/1000 | Loss: 0.00002644
Iteration 45/1000 | Loss: 0.00002644
Iteration 46/1000 | Loss: 0.00002644
Iteration 47/1000 | Loss: 0.00002643
Iteration 48/1000 | Loss: 0.00002643
Iteration 49/1000 | Loss: 0.00002643
Iteration 50/1000 | Loss: 0.00002643
Iteration 51/1000 | Loss: 0.00002643
Iteration 52/1000 | Loss: 0.00002643
Iteration 53/1000 | Loss: 0.00002642
Iteration 54/1000 | Loss: 0.00002642
Iteration 55/1000 | Loss: 0.00002642
Iteration 56/1000 | Loss: 0.00002642
Iteration 57/1000 | Loss: 0.00002642
Iteration 58/1000 | Loss: 0.00002642
Iteration 59/1000 | Loss: 0.00002642
Iteration 60/1000 | Loss: 0.00002642
Iteration 61/1000 | Loss: 0.00002642
Iteration 62/1000 | Loss: 0.00002642
Iteration 63/1000 | Loss: 0.00002642
Iteration 64/1000 | Loss: 0.00002642
Iteration 65/1000 | Loss: 0.00002642
Iteration 66/1000 | Loss: 0.00002641
Iteration 67/1000 | Loss: 0.00002641
Iteration 68/1000 | Loss: 0.00002641
Iteration 69/1000 | Loss: 0.00002641
Iteration 70/1000 | Loss: 0.00002641
Iteration 71/1000 | Loss: 0.00002641
Iteration 72/1000 | Loss: 0.00002640
Iteration 73/1000 | Loss: 0.00002640
Iteration 74/1000 | Loss: 0.00002640
Iteration 75/1000 | Loss: 0.00002640
Iteration 76/1000 | Loss: 0.00002640
Iteration 77/1000 | Loss: 0.00002640
Iteration 78/1000 | Loss: 0.00002640
Iteration 79/1000 | Loss: 0.00002640
Iteration 80/1000 | Loss: 0.00002640
Iteration 81/1000 | Loss: 0.00002640
Iteration 82/1000 | Loss: 0.00002640
Iteration 83/1000 | Loss: 0.00002640
Iteration 84/1000 | Loss: 0.00002640
Iteration 85/1000 | Loss: 0.00002639
Iteration 86/1000 | Loss: 0.00002639
Iteration 87/1000 | Loss: 0.00002639
Iteration 88/1000 | Loss: 0.00002639
Iteration 89/1000 | Loss: 0.00002639
Iteration 90/1000 | Loss: 0.00002639
Iteration 91/1000 | Loss: 0.00002639
Iteration 92/1000 | Loss: 0.00002639
Iteration 93/1000 | Loss: 0.00002639
Iteration 94/1000 | Loss: 0.00002639
Iteration 95/1000 | Loss: 0.00002639
Iteration 96/1000 | Loss: 0.00002639
Iteration 97/1000 | Loss: 0.00002639
Iteration 98/1000 | Loss: 0.00002638
Iteration 99/1000 | Loss: 0.00002638
Iteration 100/1000 | Loss: 0.00002638
Iteration 101/1000 | Loss: 0.00002638
Iteration 102/1000 | Loss: 0.00002638
Iteration 103/1000 | Loss: 0.00002638
Iteration 104/1000 | Loss: 0.00002638
Iteration 105/1000 | Loss: 0.00002638
Iteration 106/1000 | Loss: 0.00002638
Iteration 107/1000 | Loss: 0.00002638
Iteration 108/1000 | Loss: 0.00002638
Iteration 109/1000 | Loss: 0.00002638
Iteration 110/1000 | Loss: 0.00002638
Iteration 111/1000 | Loss: 0.00002638
Iteration 112/1000 | Loss: 0.00002638
Iteration 113/1000 | Loss: 0.00002638
Iteration 114/1000 | Loss: 0.00002638
Iteration 115/1000 | Loss: 0.00002638
Iteration 116/1000 | Loss: 0.00002638
Iteration 117/1000 | Loss: 0.00002638
Iteration 118/1000 | Loss: 0.00002638
Iteration 119/1000 | Loss: 0.00002638
Iteration 120/1000 | Loss: 0.00002638
Iteration 121/1000 | Loss: 0.00002638
Iteration 122/1000 | Loss: 0.00002638
Iteration 123/1000 | Loss: 0.00002638
Iteration 124/1000 | Loss: 0.00002638
Iteration 125/1000 | Loss: 0.00002638
Iteration 126/1000 | Loss: 0.00002638
Iteration 127/1000 | Loss: 0.00002638
Iteration 128/1000 | Loss: 0.00002638
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [2.637728175614029e-05, 2.637728175614029e-05, 2.637728175614029e-05, 2.637728175614029e-05, 2.637728175614029e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.637728175614029e-05

Optimization complete. Final v2v error: 4.352063179016113 mm

Highest mean error: 4.891911029815674 mm for frame 70

Lowest mean error: 4.024917125701904 mm for frame 56

Saving results

Total time: 41.53618240356445
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00667493
Iteration 2/25 | Loss: 0.00129869
Iteration 3/25 | Loss: 0.00123041
Iteration 4/25 | Loss: 0.00121615
Iteration 5/25 | Loss: 0.00121104
Iteration 6/25 | Loss: 0.00120965
Iteration 7/25 | Loss: 0.00120965
Iteration 8/25 | Loss: 0.00120965
Iteration 9/25 | Loss: 0.00120965
Iteration 10/25 | Loss: 0.00120965
Iteration 11/25 | Loss: 0.00120965
Iteration 12/25 | Loss: 0.00120965
Iteration 13/25 | Loss: 0.00120965
Iteration 14/25 | Loss: 0.00120965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.001209645182825625, 0.001209645182825625, 0.001209645182825625, 0.001209645182825625, 0.001209645182825625]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001209645182825625

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.90527678
Iteration 2/25 | Loss: 0.00116365
Iteration 3/25 | Loss: 0.00116365
Iteration 4/25 | Loss: 0.00116365
Iteration 5/25 | Loss: 0.00116364
Iteration 6/25 | Loss: 0.00116364
Iteration 7/25 | Loss: 0.00116364
Iteration 8/25 | Loss: 0.00116364
Iteration 9/25 | Loss: 0.00116364
Iteration 10/25 | Loss: 0.00116364
Iteration 11/25 | Loss: 0.00116364
Iteration 12/25 | Loss: 0.00116364
Iteration 13/25 | Loss: 0.00116364
Iteration 14/25 | Loss: 0.00116364
Iteration 15/25 | Loss: 0.00116364
Iteration 16/25 | Loss: 0.00116364
Iteration 17/25 | Loss: 0.00116364
Iteration 18/25 | Loss: 0.00116364
Iteration 19/25 | Loss: 0.00116364
Iteration 20/25 | Loss: 0.00116364
Iteration 21/25 | Loss: 0.00116364
Iteration 22/25 | Loss: 0.00116364
Iteration 23/25 | Loss: 0.00116364
Iteration 24/25 | Loss: 0.00116364
Iteration 25/25 | Loss: 0.00116364

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116364
Iteration 2/1000 | Loss: 0.00003016
Iteration 3/1000 | Loss: 0.00001841
Iteration 4/1000 | Loss: 0.00001642
Iteration 5/1000 | Loss: 0.00001538
Iteration 6/1000 | Loss: 0.00001482
Iteration 7/1000 | Loss: 0.00001430
Iteration 8/1000 | Loss: 0.00001394
Iteration 9/1000 | Loss: 0.00001367
Iteration 10/1000 | Loss: 0.00001349
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001330
Iteration 13/1000 | Loss: 0.00001323
Iteration 14/1000 | Loss: 0.00001316
Iteration 15/1000 | Loss: 0.00001315
Iteration 16/1000 | Loss: 0.00001314
Iteration 17/1000 | Loss: 0.00001313
Iteration 18/1000 | Loss: 0.00001311
Iteration 19/1000 | Loss: 0.00001309
Iteration 20/1000 | Loss: 0.00001308
Iteration 21/1000 | Loss: 0.00001308
Iteration 22/1000 | Loss: 0.00001299
Iteration 23/1000 | Loss: 0.00001296
Iteration 24/1000 | Loss: 0.00001295
Iteration 25/1000 | Loss: 0.00001295
Iteration 26/1000 | Loss: 0.00001294
Iteration 27/1000 | Loss: 0.00001293
Iteration 28/1000 | Loss: 0.00001290
Iteration 29/1000 | Loss: 0.00001290
Iteration 30/1000 | Loss: 0.00001289
Iteration 31/1000 | Loss: 0.00001289
Iteration 32/1000 | Loss: 0.00001288
Iteration 33/1000 | Loss: 0.00001288
Iteration 34/1000 | Loss: 0.00001287
Iteration 35/1000 | Loss: 0.00001286
Iteration 36/1000 | Loss: 0.00001285
Iteration 37/1000 | Loss: 0.00001284
Iteration 38/1000 | Loss: 0.00001283
Iteration 39/1000 | Loss: 0.00001283
Iteration 40/1000 | Loss: 0.00001282
Iteration 41/1000 | Loss: 0.00001281
Iteration 42/1000 | Loss: 0.00001280
Iteration 43/1000 | Loss: 0.00001280
Iteration 44/1000 | Loss: 0.00001279
Iteration 45/1000 | Loss: 0.00001279
Iteration 46/1000 | Loss: 0.00001278
Iteration 47/1000 | Loss: 0.00001278
Iteration 48/1000 | Loss: 0.00001278
Iteration 49/1000 | Loss: 0.00001277
Iteration 50/1000 | Loss: 0.00001277
Iteration 51/1000 | Loss: 0.00001277
Iteration 52/1000 | Loss: 0.00001276
Iteration 53/1000 | Loss: 0.00001276
Iteration 54/1000 | Loss: 0.00001275
Iteration 55/1000 | Loss: 0.00001275
Iteration 56/1000 | Loss: 0.00001274
Iteration 57/1000 | Loss: 0.00001274
Iteration 58/1000 | Loss: 0.00001274
Iteration 59/1000 | Loss: 0.00001274
Iteration 60/1000 | Loss: 0.00001273
Iteration 61/1000 | Loss: 0.00001273
Iteration 62/1000 | Loss: 0.00001272
Iteration 63/1000 | Loss: 0.00001272
Iteration 64/1000 | Loss: 0.00001272
Iteration 65/1000 | Loss: 0.00001272
Iteration 66/1000 | Loss: 0.00001271
Iteration 67/1000 | Loss: 0.00001271
Iteration 68/1000 | Loss: 0.00001271
Iteration 69/1000 | Loss: 0.00001271
Iteration 70/1000 | Loss: 0.00001271
Iteration 71/1000 | Loss: 0.00001270
Iteration 72/1000 | Loss: 0.00001270
Iteration 73/1000 | Loss: 0.00001268
Iteration 74/1000 | Loss: 0.00001268
Iteration 75/1000 | Loss: 0.00001268
Iteration 76/1000 | Loss: 0.00001267
Iteration 77/1000 | Loss: 0.00001267
Iteration 78/1000 | Loss: 0.00001267
Iteration 79/1000 | Loss: 0.00001267
Iteration 80/1000 | Loss: 0.00001267
Iteration 81/1000 | Loss: 0.00001266
Iteration 82/1000 | Loss: 0.00001266
Iteration 83/1000 | Loss: 0.00001266
Iteration 84/1000 | Loss: 0.00001265
Iteration 85/1000 | Loss: 0.00001265
Iteration 86/1000 | Loss: 0.00001265
Iteration 87/1000 | Loss: 0.00001265
Iteration 88/1000 | Loss: 0.00001264
Iteration 89/1000 | Loss: 0.00001264
Iteration 90/1000 | Loss: 0.00001264
Iteration 91/1000 | Loss: 0.00001263
Iteration 92/1000 | Loss: 0.00001263
Iteration 93/1000 | Loss: 0.00001263
Iteration 94/1000 | Loss: 0.00001262
Iteration 95/1000 | Loss: 0.00001262
Iteration 96/1000 | Loss: 0.00001262
Iteration 97/1000 | Loss: 0.00001262
Iteration 98/1000 | Loss: 0.00001261
Iteration 99/1000 | Loss: 0.00001261
Iteration 100/1000 | Loss: 0.00001261
Iteration 101/1000 | Loss: 0.00001261
Iteration 102/1000 | Loss: 0.00001260
Iteration 103/1000 | Loss: 0.00001260
Iteration 104/1000 | Loss: 0.00001260
Iteration 105/1000 | Loss: 0.00001260
Iteration 106/1000 | Loss: 0.00001259
Iteration 107/1000 | Loss: 0.00001259
Iteration 108/1000 | Loss: 0.00001259
Iteration 109/1000 | Loss: 0.00001259
Iteration 110/1000 | Loss: 0.00001259
Iteration 111/1000 | Loss: 0.00001259
Iteration 112/1000 | Loss: 0.00001259
Iteration 113/1000 | Loss: 0.00001259
Iteration 114/1000 | Loss: 0.00001259
Iteration 115/1000 | Loss: 0.00001259
Iteration 116/1000 | Loss: 0.00001258
Iteration 117/1000 | Loss: 0.00001258
Iteration 118/1000 | Loss: 0.00001258
Iteration 119/1000 | Loss: 0.00001258
Iteration 120/1000 | Loss: 0.00001258
Iteration 121/1000 | Loss: 0.00001258
Iteration 122/1000 | Loss: 0.00001258
Iteration 123/1000 | Loss: 0.00001258
Iteration 124/1000 | Loss: 0.00001258
Iteration 125/1000 | Loss: 0.00001258
Iteration 126/1000 | Loss: 0.00001258
Iteration 127/1000 | Loss: 0.00001258
Iteration 128/1000 | Loss: 0.00001258
Iteration 129/1000 | Loss: 0.00001258
Iteration 130/1000 | Loss: 0.00001258
Iteration 131/1000 | Loss: 0.00001258
Iteration 132/1000 | Loss: 0.00001258
Iteration 133/1000 | Loss: 0.00001258
Iteration 134/1000 | Loss: 0.00001257
Iteration 135/1000 | Loss: 0.00001257
Iteration 136/1000 | Loss: 0.00001257
Iteration 137/1000 | Loss: 0.00001257
Iteration 138/1000 | Loss: 0.00001257
Iteration 139/1000 | Loss: 0.00001257
Iteration 140/1000 | Loss: 0.00001257
Iteration 141/1000 | Loss: 0.00001257
Iteration 142/1000 | Loss: 0.00001257
Iteration 143/1000 | Loss: 0.00001257
Iteration 144/1000 | Loss: 0.00001257
Iteration 145/1000 | Loss: 0.00001257
Iteration 146/1000 | Loss: 0.00001257
Iteration 147/1000 | Loss: 0.00001257
Iteration 148/1000 | Loss: 0.00001257
Iteration 149/1000 | Loss: 0.00001257
Iteration 150/1000 | Loss: 0.00001257
Iteration 151/1000 | Loss: 0.00001257
Iteration 152/1000 | Loss: 0.00001257
Iteration 153/1000 | Loss: 0.00001257
Iteration 154/1000 | Loss: 0.00001257
Iteration 155/1000 | Loss: 0.00001257
Iteration 156/1000 | Loss: 0.00001257
Iteration 157/1000 | Loss: 0.00001257
Iteration 158/1000 | Loss: 0.00001257
Iteration 159/1000 | Loss: 0.00001257
Iteration 160/1000 | Loss: 0.00001257
Iteration 161/1000 | Loss: 0.00001257
Iteration 162/1000 | Loss: 0.00001257
Iteration 163/1000 | Loss: 0.00001257
Iteration 164/1000 | Loss: 0.00001257
Iteration 165/1000 | Loss: 0.00001257
Iteration 166/1000 | Loss: 0.00001257
Iteration 167/1000 | Loss: 0.00001257
Iteration 168/1000 | Loss: 0.00001257
Iteration 169/1000 | Loss: 0.00001257
Iteration 170/1000 | Loss: 0.00001257
Iteration 171/1000 | Loss: 0.00001257
Iteration 172/1000 | Loss: 0.00001257
Iteration 173/1000 | Loss: 0.00001257
Iteration 174/1000 | Loss: 0.00001257
Iteration 175/1000 | Loss: 0.00001257
Iteration 176/1000 | Loss: 0.00001257
Iteration 177/1000 | Loss: 0.00001257
Iteration 178/1000 | Loss: 0.00001257
Iteration 179/1000 | Loss: 0.00001257
Iteration 180/1000 | Loss: 0.00001257
Iteration 181/1000 | Loss: 0.00001257
Iteration 182/1000 | Loss: 0.00001257
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [1.2565954421006609e-05, 1.2565954421006609e-05, 1.2565954421006609e-05, 1.2565954421006609e-05, 1.2565954421006609e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2565954421006609e-05

Optimization complete. Final v2v error: 3.066627264022827 mm

Highest mean error: 3.402968168258667 mm for frame 102

Lowest mean error: 2.817868232727051 mm for frame 23

Saving results

Total time: 40.52373719215393
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1099/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1099.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1099
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00416703
Iteration 2/25 | Loss: 0.00129516
Iteration 3/25 | Loss: 0.00122711
Iteration 4/25 | Loss: 0.00121090
Iteration 5/25 | Loss: 0.00120526
Iteration 6/25 | Loss: 0.00120420
Iteration 7/25 | Loss: 0.00120420
Iteration 8/25 | Loss: 0.00120420
Iteration 9/25 | Loss: 0.00120420
Iteration 10/25 | Loss: 0.00120420
Iteration 11/25 | Loss: 0.00120420
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012042047455906868, 0.0012042047455906868, 0.0012042047455906868, 0.0012042047455906868, 0.0012042047455906868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012042047455906868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38812006
Iteration 2/25 | Loss: 0.00116879
Iteration 3/25 | Loss: 0.00116879
Iteration 4/25 | Loss: 0.00116879
Iteration 5/25 | Loss: 0.00116878
Iteration 6/25 | Loss: 0.00116878
Iteration 7/25 | Loss: 0.00116878
Iteration 8/25 | Loss: 0.00116878
Iteration 9/25 | Loss: 0.00116878
Iteration 10/25 | Loss: 0.00116878
Iteration 11/25 | Loss: 0.00116878
Iteration 12/25 | Loss: 0.00116878
Iteration 13/25 | Loss: 0.00116878
Iteration 14/25 | Loss: 0.00116878
Iteration 15/25 | Loss: 0.00116878
Iteration 16/25 | Loss: 0.00116878
Iteration 17/25 | Loss: 0.00116878
Iteration 18/25 | Loss: 0.00116878
Iteration 19/25 | Loss: 0.00116878
Iteration 20/25 | Loss: 0.00116878
Iteration 21/25 | Loss: 0.00116878
Iteration 22/25 | Loss: 0.00116878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001168782589957118, 0.001168782589957118, 0.001168782589957118, 0.001168782589957118, 0.001168782589957118]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001168782589957118

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116878
Iteration 2/1000 | Loss: 0.00002360
Iteration 3/1000 | Loss: 0.00001637
Iteration 4/1000 | Loss: 0.00001520
Iteration 5/1000 | Loss: 0.00001445
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001361
Iteration 8/1000 | Loss: 0.00001333
Iteration 9/1000 | Loss: 0.00001305
Iteration 10/1000 | Loss: 0.00001289
Iteration 11/1000 | Loss: 0.00001288
Iteration 12/1000 | Loss: 0.00001287
Iteration 13/1000 | Loss: 0.00001287
Iteration 14/1000 | Loss: 0.00001285
Iteration 15/1000 | Loss: 0.00001274
Iteration 16/1000 | Loss: 0.00001271
Iteration 17/1000 | Loss: 0.00001269
Iteration 18/1000 | Loss: 0.00001268
Iteration 19/1000 | Loss: 0.00001264
Iteration 20/1000 | Loss: 0.00001263
Iteration 21/1000 | Loss: 0.00001261
Iteration 22/1000 | Loss: 0.00001261
Iteration 23/1000 | Loss: 0.00001257
Iteration 24/1000 | Loss: 0.00001254
Iteration 25/1000 | Loss: 0.00001254
Iteration 26/1000 | Loss: 0.00001253
Iteration 27/1000 | Loss: 0.00001252
Iteration 28/1000 | Loss: 0.00001248
Iteration 29/1000 | Loss: 0.00001247
Iteration 30/1000 | Loss: 0.00001246
Iteration 31/1000 | Loss: 0.00001245
Iteration 32/1000 | Loss: 0.00001245
Iteration 33/1000 | Loss: 0.00001245
Iteration 34/1000 | Loss: 0.00001244
Iteration 35/1000 | Loss: 0.00001243
Iteration 36/1000 | Loss: 0.00001242
Iteration 37/1000 | Loss: 0.00001242
Iteration 38/1000 | Loss: 0.00001241
Iteration 39/1000 | Loss: 0.00001241
Iteration 40/1000 | Loss: 0.00001240
Iteration 41/1000 | Loss: 0.00001240
Iteration 42/1000 | Loss: 0.00001240
Iteration 43/1000 | Loss: 0.00001240
Iteration 44/1000 | Loss: 0.00001239
Iteration 45/1000 | Loss: 0.00001236
Iteration 46/1000 | Loss: 0.00001236
Iteration 47/1000 | Loss: 0.00001235
Iteration 48/1000 | Loss: 0.00001235
Iteration 49/1000 | Loss: 0.00001232
Iteration 50/1000 | Loss: 0.00001232
Iteration 51/1000 | Loss: 0.00001231
Iteration 52/1000 | Loss: 0.00001230
Iteration 53/1000 | Loss: 0.00001230
Iteration 54/1000 | Loss: 0.00001229
Iteration 55/1000 | Loss: 0.00001229
Iteration 56/1000 | Loss: 0.00001227
Iteration 57/1000 | Loss: 0.00001226
Iteration 58/1000 | Loss: 0.00001226
Iteration 59/1000 | Loss: 0.00001225
Iteration 60/1000 | Loss: 0.00001225
Iteration 61/1000 | Loss: 0.00001225
Iteration 62/1000 | Loss: 0.00001224
Iteration 63/1000 | Loss: 0.00001224
Iteration 64/1000 | Loss: 0.00001224
Iteration 65/1000 | Loss: 0.00001224
Iteration 66/1000 | Loss: 0.00001223
Iteration 67/1000 | Loss: 0.00001223
Iteration 68/1000 | Loss: 0.00001222
Iteration 69/1000 | Loss: 0.00001222
Iteration 70/1000 | Loss: 0.00001222
Iteration 71/1000 | Loss: 0.00001221
Iteration 72/1000 | Loss: 0.00001221
Iteration 73/1000 | Loss: 0.00001221
Iteration 74/1000 | Loss: 0.00001220
Iteration 75/1000 | Loss: 0.00001220
Iteration 76/1000 | Loss: 0.00001219
Iteration 77/1000 | Loss: 0.00001219
Iteration 78/1000 | Loss: 0.00001219
Iteration 79/1000 | Loss: 0.00001218
Iteration 80/1000 | Loss: 0.00001218
Iteration 81/1000 | Loss: 0.00001218
Iteration 82/1000 | Loss: 0.00001218
Iteration 83/1000 | Loss: 0.00001218
Iteration 84/1000 | Loss: 0.00001218
Iteration 85/1000 | Loss: 0.00001218
Iteration 86/1000 | Loss: 0.00001218
Iteration 87/1000 | Loss: 0.00001218
Iteration 88/1000 | Loss: 0.00001218
Iteration 89/1000 | Loss: 0.00001218
Iteration 90/1000 | Loss: 0.00001218
Iteration 91/1000 | Loss: 0.00001217
Iteration 92/1000 | Loss: 0.00001217
Iteration 93/1000 | Loss: 0.00001217
Iteration 94/1000 | Loss: 0.00001217
Iteration 95/1000 | Loss: 0.00001216
Iteration 96/1000 | Loss: 0.00001216
Iteration 97/1000 | Loss: 0.00001216
Iteration 98/1000 | Loss: 0.00001216
Iteration 99/1000 | Loss: 0.00001216
Iteration 100/1000 | Loss: 0.00001215
Iteration 101/1000 | Loss: 0.00001215
Iteration 102/1000 | Loss: 0.00001215
Iteration 103/1000 | Loss: 0.00001215
Iteration 104/1000 | Loss: 0.00001215
Iteration 105/1000 | Loss: 0.00001215
Iteration 106/1000 | Loss: 0.00001215
Iteration 107/1000 | Loss: 0.00001215
Iteration 108/1000 | Loss: 0.00001215
Iteration 109/1000 | Loss: 0.00001215
Iteration 110/1000 | Loss: 0.00001215
Iteration 111/1000 | Loss: 0.00001214
Iteration 112/1000 | Loss: 0.00001214
Iteration 113/1000 | Loss: 0.00001214
Iteration 114/1000 | Loss: 0.00001214
Iteration 115/1000 | Loss: 0.00001213
Iteration 116/1000 | Loss: 0.00001213
Iteration 117/1000 | Loss: 0.00001213
Iteration 118/1000 | Loss: 0.00001213
Iteration 119/1000 | Loss: 0.00001213
Iteration 120/1000 | Loss: 0.00001213
Iteration 121/1000 | Loss: 0.00001213
Iteration 122/1000 | Loss: 0.00001213
Iteration 123/1000 | Loss: 0.00001213
Iteration 124/1000 | Loss: 0.00001213
Iteration 125/1000 | Loss: 0.00001213
Iteration 126/1000 | Loss: 0.00001212
Iteration 127/1000 | Loss: 0.00001212
Iteration 128/1000 | Loss: 0.00001212
Iteration 129/1000 | Loss: 0.00001212
Iteration 130/1000 | Loss: 0.00001212
Iteration 131/1000 | Loss: 0.00001212
Iteration 132/1000 | Loss: 0.00001212
Iteration 133/1000 | Loss: 0.00001212
Iteration 134/1000 | Loss: 0.00001212
Iteration 135/1000 | Loss: 0.00001212
Iteration 136/1000 | Loss: 0.00001212
Iteration 137/1000 | Loss: 0.00001212
Iteration 138/1000 | Loss: 0.00001211
Iteration 139/1000 | Loss: 0.00001211
Iteration 140/1000 | Loss: 0.00001211
Iteration 141/1000 | Loss: 0.00001211
Iteration 142/1000 | Loss: 0.00001211
Iteration 143/1000 | Loss: 0.00001211
Iteration 144/1000 | Loss: 0.00001211
Iteration 145/1000 | Loss: 0.00001211
Iteration 146/1000 | Loss: 0.00001211
Iteration 147/1000 | Loss: 0.00001211
Iteration 148/1000 | Loss: 0.00001211
Iteration 149/1000 | Loss: 0.00001211
Iteration 150/1000 | Loss: 0.00001210
Iteration 151/1000 | Loss: 0.00001210
Iteration 152/1000 | Loss: 0.00001210
Iteration 153/1000 | Loss: 0.00001210
Iteration 154/1000 | Loss: 0.00001210
Iteration 155/1000 | Loss: 0.00001210
Iteration 156/1000 | Loss: 0.00001210
Iteration 157/1000 | Loss: 0.00001210
Iteration 158/1000 | Loss: 0.00001210
Iteration 159/1000 | Loss: 0.00001210
Iteration 160/1000 | Loss: 0.00001210
Iteration 161/1000 | Loss: 0.00001210
Iteration 162/1000 | Loss: 0.00001210
Iteration 163/1000 | Loss: 0.00001210
Iteration 164/1000 | Loss: 0.00001210
Iteration 165/1000 | Loss: 0.00001210
Iteration 166/1000 | Loss: 0.00001210
Iteration 167/1000 | Loss: 0.00001210
Iteration 168/1000 | Loss: 0.00001210
Iteration 169/1000 | Loss: 0.00001209
Iteration 170/1000 | Loss: 0.00001209
Iteration 171/1000 | Loss: 0.00001209
Iteration 172/1000 | Loss: 0.00001209
Iteration 173/1000 | Loss: 0.00001209
Iteration 174/1000 | Loss: 0.00001209
Iteration 175/1000 | Loss: 0.00001209
Iteration 176/1000 | Loss: 0.00001209
Iteration 177/1000 | Loss: 0.00001209
Iteration 178/1000 | Loss: 0.00001209
Iteration 179/1000 | Loss: 0.00001209
Iteration 180/1000 | Loss: 0.00001209
Iteration 181/1000 | Loss: 0.00001209
Iteration 182/1000 | Loss: 0.00001209
Iteration 183/1000 | Loss: 0.00001209
Iteration 184/1000 | Loss: 0.00001208
Iteration 185/1000 | Loss: 0.00001208
Iteration 186/1000 | Loss: 0.00001208
Iteration 187/1000 | Loss: 0.00001208
Iteration 188/1000 | Loss: 0.00001208
Iteration 189/1000 | Loss: 0.00001208
Iteration 190/1000 | Loss: 0.00001208
Iteration 191/1000 | Loss: 0.00001208
Iteration 192/1000 | Loss: 0.00001208
Iteration 193/1000 | Loss: 0.00001208
Iteration 194/1000 | Loss: 0.00001208
Iteration 195/1000 | Loss: 0.00001208
Iteration 196/1000 | Loss: 0.00001208
Iteration 197/1000 | Loss: 0.00001208
Iteration 198/1000 | Loss: 0.00001208
Iteration 199/1000 | Loss: 0.00001208
Iteration 200/1000 | Loss: 0.00001208
Iteration 201/1000 | Loss: 0.00001208
Iteration 202/1000 | Loss: 0.00001208
Iteration 203/1000 | Loss: 0.00001208
Iteration 204/1000 | Loss: 0.00001208
Iteration 205/1000 | Loss: 0.00001208
Iteration 206/1000 | Loss: 0.00001208
Iteration 207/1000 | Loss: 0.00001208
Iteration 208/1000 | Loss: 0.00001208
Iteration 209/1000 | Loss: 0.00001208
Iteration 210/1000 | Loss: 0.00001208
Iteration 211/1000 | Loss: 0.00001208
Iteration 212/1000 | Loss: 0.00001208
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [1.208193862112239e-05, 1.208193862112239e-05, 1.208193862112239e-05, 1.208193862112239e-05, 1.208193862112239e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.208193862112239e-05

Optimization complete. Final v2v error: 3.009936571121216 mm

Highest mean error: 3.293854236602783 mm for frame 115

Lowest mean error: 2.899461269378662 mm for frame 134

Saving results

Total time: 40.74241805076599
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00667609
Iteration 2/25 | Loss: 0.00158653
Iteration 3/25 | Loss: 0.00136708
Iteration 4/25 | Loss: 0.00125488
Iteration 5/25 | Loss: 0.00124438
Iteration 6/25 | Loss: 0.00124351
Iteration 7/25 | Loss: 0.00123612
Iteration 8/25 | Loss: 0.00123281
Iteration 9/25 | Loss: 0.00123135
Iteration 10/25 | Loss: 0.00123075
Iteration 11/25 | Loss: 0.00123053
Iteration 12/25 | Loss: 0.00123039
Iteration 13/25 | Loss: 0.00123037
Iteration 14/25 | Loss: 0.00123036
Iteration 15/25 | Loss: 0.00123036
Iteration 16/25 | Loss: 0.00123035
Iteration 17/25 | Loss: 0.00123035
Iteration 18/25 | Loss: 0.00123035
Iteration 19/25 | Loss: 0.00123035
Iteration 20/25 | Loss: 0.00123035
Iteration 21/25 | Loss: 0.00123035
Iteration 22/25 | Loss: 0.00123035
Iteration 23/25 | Loss: 0.00123035
Iteration 24/25 | Loss: 0.00123035
Iteration 25/25 | Loss: 0.00123035

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.54211569
Iteration 2/25 | Loss: 0.00106589
Iteration 3/25 | Loss: 0.00106555
Iteration 4/25 | Loss: 0.00106555
Iteration 5/25 | Loss: 0.00106555
Iteration 6/25 | Loss: 0.00106555
Iteration 7/25 | Loss: 0.00106555
Iteration 8/25 | Loss: 0.00106555
Iteration 9/25 | Loss: 0.00106555
Iteration 10/25 | Loss: 0.00106555
Iteration 11/25 | Loss: 0.00106555
Iteration 12/25 | Loss: 0.00106555
Iteration 13/25 | Loss: 0.00106555
Iteration 14/25 | Loss: 0.00106555
Iteration 15/25 | Loss: 0.00106555
Iteration 16/25 | Loss: 0.00106555
Iteration 17/25 | Loss: 0.00106555
Iteration 18/25 | Loss: 0.00106555
Iteration 19/25 | Loss: 0.00106555
Iteration 20/25 | Loss: 0.00106555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001065545598976314, 0.001065545598976314, 0.001065545598976314, 0.001065545598976314, 0.001065545598976314]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001065545598976314

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106555
Iteration 2/1000 | Loss: 0.00003228
Iteration 3/1000 | Loss: 0.00007737
Iteration 4/1000 | Loss: 0.00001989
Iteration 5/1000 | Loss: 0.00001826
Iteration 6/1000 | Loss: 0.00010120
Iteration 7/1000 | Loss: 0.00001685
Iteration 8/1000 | Loss: 0.00006993
Iteration 9/1000 | Loss: 0.00001607
Iteration 10/1000 | Loss: 0.00001580
Iteration 11/1000 | Loss: 0.00008276
Iteration 12/1000 | Loss: 0.00003186
Iteration 13/1000 | Loss: 0.00001569
Iteration 14/1000 | Loss: 0.00001535
Iteration 15/1000 | Loss: 0.00001527
Iteration 16/1000 | Loss: 0.00001509
Iteration 17/1000 | Loss: 0.00001493
Iteration 18/1000 | Loss: 0.00003884
Iteration 19/1000 | Loss: 0.00001520
Iteration 20/1000 | Loss: 0.00002281
Iteration 21/1000 | Loss: 0.00001479
Iteration 22/1000 | Loss: 0.00001474
Iteration 23/1000 | Loss: 0.00001474
Iteration 24/1000 | Loss: 0.00001474
Iteration 25/1000 | Loss: 0.00001474
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00002397
Iteration 29/1000 | Loss: 0.00001554
Iteration 30/1000 | Loss: 0.00001595
Iteration 31/1000 | Loss: 0.00001594
Iteration 32/1000 | Loss: 0.00001534
Iteration 33/1000 | Loss: 0.00003961
Iteration 34/1000 | Loss: 0.00001459
Iteration 35/1000 | Loss: 0.00001459
Iteration 36/1000 | Loss: 0.00001459
Iteration 37/1000 | Loss: 0.00001459
Iteration 38/1000 | Loss: 0.00001458
Iteration 39/1000 | Loss: 0.00001458
Iteration 40/1000 | Loss: 0.00001458
Iteration 41/1000 | Loss: 0.00001458
Iteration 42/1000 | Loss: 0.00001457
Iteration 43/1000 | Loss: 0.00001454
Iteration 44/1000 | Loss: 0.00001453
Iteration 45/1000 | Loss: 0.00001453
Iteration 46/1000 | Loss: 0.00001444
Iteration 47/1000 | Loss: 0.00001443
Iteration 48/1000 | Loss: 0.00001443
Iteration 49/1000 | Loss: 0.00001442
Iteration 50/1000 | Loss: 0.00001441
Iteration 51/1000 | Loss: 0.00001441
Iteration 52/1000 | Loss: 0.00001441
Iteration 53/1000 | Loss: 0.00001439
Iteration 54/1000 | Loss: 0.00001439
Iteration 55/1000 | Loss: 0.00001439
Iteration 56/1000 | Loss: 0.00001439
Iteration 57/1000 | Loss: 0.00001439
Iteration 58/1000 | Loss: 0.00001439
Iteration 59/1000 | Loss: 0.00001438
Iteration 60/1000 | Loss: 0.00001438
Iteration 61/1000 | Loss: 0.00001438
Iteration 62/1000 | Loss: 0.00001438
Iteration 63/1000 | Loss: 0.00001438
Iteration 64/1000 | Loss: 0.00001437
Iteration 65/1000 | Loss: 0.00001437
Iteration 66/1000 | Loss: 0.00001437
Iteration 67/1000 | Loss: 0.00001437
Iteration 68/1000 | Loss: 0.00001437
Iteration 69/1000 | Loss: 0.00001437
Iteration 70/1000 | Loss: 0.00001437
Iteration 71/1000 | Loss: 0.00001437
Iteration 72/1000 | Loss: 0.00001437
Iteration 73/1000 | Loss: 0.00001437
Iteration 74/1000 | Loss: 0.00001437
Iteration 75/1000 | Loss: 0.00001437
Iteration 76/1000 | Loss: 0.00001437
Iteration 77/1000 | Loss: 0.00001436
Iteration 78/1000 | Loss: 0.00001436
Iteration 79/1000 | Loss: 0.00001436
Iteration 80/1000 | Loss: 0.00001436
Iteration 81/1000 | Loss: 0.00001436
Iteration 82/1000 | Loss: 0.00001436
Iteration 83/1000 | Loss: 0.00001436
Iteration 84/1000 | Loss: 0.00001436
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001436
Iteration 91/1000 | Loss: 0.00001436
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001434
Iteration 99/1000 | Loss: 0.00001434
Iteration 100/1000 | Loss: 0.00001434
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001433
Iteration 106/1000 | Loss: 0.00001433
Iteration 107/1000 | Loss: 0.00001433
Iteration 108/1000 | Loss: 0.00001433
Iteration 109/1000 | Loss: 0.00001433
Iteration 110/1000 | Loss: 0.00001433
Iteration 111/1000 | Loss: 0.00001433
Iteration 112/1000 | Loss: 0.00001433
Iteration 113/1000 | Loss: 0.00001433
Iteration 114/1000 | Loss: 0.00001433
Iteration 115/1000 | Loss: 0.00001433
Iteration 116/1000 | Loss: 0.00001433
Iteration 117/1000 | Loss: 0.00001432
Iteration 118/1000 | Loss: 0.00001432
Iteration 119/1000 | Loss: 0.00001432
Iteration 120/1000 | Loss: 0.00001432
Iteration 121/1000 | Loss: 0.00001432
Iteration 122/1000 | Loss: 0.00001432
Iteration 123/1000 | Loss: 0.00001431
Iteration 124/1000 | Loss: 0.00001431
Iteration 125/1000 | Loss: 0.00001431
Iteration 126/1000 | Loss: 0.00001431
Iteration 127/1000 | Loss: 0.00001431
Iteration 128/1000 | Loss: 0.00001431
Iteration 129/1000 | Loss: 0.00001431
Iteration 130/1000 | Loss: 0.00001431
Iteration 131/1000 | Loss: 0.00001430
Iteration 132/1000 | Loss: 0.00001430
Iteration 133/1000 | Loss: 0.00001430
Iteration 134/1000 | Loss: 0.00001430
Iteration 135/1000 | Loss: 0.00001430
Iteration 136/1000 | Loss: 0.00001429
Iteration 137/1000 | Loss: 0.00001429
Iteration 138/1000 | Loss: 0.00001429
Iteration 139/1000 | Loss: 0.00001429
Iteration 140/1000 | Loss: 0.00001429
Iteration 141/1000 | Loss: 0.00001429
Iteration 142/1000 | Loss: 0.00001429
Iteration 143/1000 | Loss: 0.00001429
Iteration 144/1000 | Loss: 0.00001429
Iteration 145/1000 | Loss: 0.00001429
Iteration 146/1000 | Loss: 0.00001428
Iteration 147/1000 | Loss: 0.00001428
Iteration 148/1000 | Loss: 0.00001428
Iteration 149/1000 | Loss: 0.00001428
Iteration 150/1000 | Loss: 0.00001427
Iteration 151/1000 | Loss: 0.00001427
Iteration 152/1000 | Loss: 0.00001427
Iteration 153/1000 | Loss: 0.00001427
Iteration 154/1000 | Loss: 0.00001427
Iteration 155/1000 | Loss: 0.00001427
Iteration 156/1000 | Loss: 0.00001427
Iteration 157/1000 | Loss: 0.00001427
Iteration 158/1000 | Loss: 0.00001427
Iteration 159/1000 | Loss: 0.00001427
Iteration 160/1000 | Loss: 0.00001426
Iteration 161/1000 | Loss: 0.00001426
Iteration 162/1000 | Loss: 0.00001426
Iteration 163/1000 | Loss: 0.00001426
Iteration 164/1000 | Loss: 0.00001426
Iteration 165/1000 | Loss: 0.00001426
Iteration 166/1000 | Loss: 0.00001426
Iteration 167/1000 | Loss: 0.00001426
Iteration 168/1000 | Loss: 0.00001426
Iteration 169/1000 | Loss: 0.00001426
Iteration 170/1000 | Loss: 0.00001426
Iteration 171/1000 | Loss: 0.00001426
Iteration 172/1000 | Loss: 0.00001426
Iteration 173/1000 | Loss: 0.00001426
Iteration 174/1000 | Loss: 0.00001426
Iteration 175/1000 | Loss: 0.00001426
Iteration 176/1000 | Loss: 0.00001426
Iteration 177/1000 | Loss: 0.00001426
Iteration 178/1000 | Loss: 0.00001426
Iteration 179/1000 | Loss: 0.00001426
Iteration 180/1000 | Loss: 0.00001426
Iteration 181/1000 | Loss: 0.00001426
Iteration 182/1000 | Loss: 0.00001426
Iteration 183/1000 | Loss: 0.00001426
Iteration 184/1000 | Loss: 0.00001426
Iteration 185/1000 | Loss: 0.00001426
Iteration 186/1000 | Loss: 0.00001426
Iteration 187/1000 | Loss: 0.00001426
Iteration 188/1000 | Loss: 0.00001426
Iteration 189/1000 | Loss: 0.00001426
Iteration 190/1000 | Loss: 0.00001426
Iteration 191/1000 | Loss: 0.00001426
Iteration 192/1000 | Loss: 0.00001426
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 192. Stopping optimization.
Last 5 losses: [1.4260323951020837e-05, 1.4260323951020837e-05, 1.4260323951020837e-05, 1.4260323951020837e-05, 1.4260323951020837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4260323951020837e-05

Optimization complete. Final v2v error: 3.212712526321411 mm

Highest mean error: 3.807116746902466 mm for frame 7

Lowest mean error: 2.8562960624694824 mm for frame 57

Saving results

Total time: 79.34188485145569
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1078/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1078.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1078
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00996181
Iteration 2/25 | Loss: 0.00206135
Iteration 3/25 | Loss: 0.00163495
Iteration 4/25 | Loss: 0.00160669
Iteration 5/25 | Loss: 0.00162917
Iteration 6/25 | Loss: 0.00159376
Iteration 7/25 | Loss: 0.00153546
Iteration 8/25 | Loss: 0.00149005
Iteration 9/25 | Loss: 0.00146275
Iteration 10/25 | Loss: 0.00143707
Iteration 11/25 | Loss: 0.00141624
Iteration 12/25 | Loss: 0.00141462
Iteration 13/25 | Loss: 0.00139203
Iteration 14/25 | Loss: 0.00137125
Iteration 15/25 | Loss: 0.00134929
Iteration 16/25 | Loss: 0.00133247
Iteration 17/25 | Loss: 0.00132758
Iteration 18/25 | Loss: 0.00133176
Iteration 19/25 | Loss: 0.00133591
Iteration 20/25 | Loss: 0.00133232
Iteration 21/25 | Loss: 0.00133213
Iteration 22/25 | Loss: 0.00133586
Iteration 23/25 | Loss: 0.00133639
Iteration 24/25 | Loss: 0.00132846
Iteration 25/25 | Loss: 0.00132561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19757378
Iteration 2/25 | Loss: 0.00196227
Iteration 3/25 | Loss: 0.00196225
Iteration 4/25 | Loss: 0.00196225
Iteration 5/25 | Loss: 0.00196225
Iteration 6/25 | Loss: 0.00196225
Iteration 7/25 | Loss: 0.00196225
Iteration 8/25 | Loss: 0.00196225
Iteration 9/25 | Loss: 0.00196224
Iteration 10/25 | Loss: 0.00196224
Iteration 11/25 | Loss: 0.00196224
Iteration 12/25 | Loss: 0.00196224
Iteration 13/25 | Loss: 0.00196224
Iteration 14/25 | Loss: 0.00196224
Iteration 15/25 | Loss: 0.00196224
Iteration 16/25 | Loss: 0.00196224
Iteration 17/25 | Loss: 0.00196224
Iteration 18/25 | Loss: 0.00196224
Iteration 19/25 | Loss: 0.00196224
Iteration 20/25 | Loss: 0.00196224
Iteration 21/25 | Loss: 0.00196224
Iteration 22/25 | Loss: 0.00196224
Iteration 23/25 | Loss: 0.00196224
Iteration 24/25 | Loss: 0.00196224
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001962243812158704, 0.001962243812158704, 0.001962243812158704, 0.001962243812158704, 0.001962243812158704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001962243812158704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00196224
Iteration 2/1000 | Loss: 0.00055984
Iteration 3/1000 | Loss: 0.00036588
Iteration 4/1000 | Loss: 0.00056478
Iteration 5/1000 | Loss: 0.00044869
Iteration 6/1000 | Loss: 0.00061611
Iteration 7/1000 | Loss: 0.00061209
Iteration 8/1000 | Loss: 0.00054645
Iteration 9/1000 | Loss: 0.00056864
Iteration 10/1000 | Loss: 0.00044809
Iteration 11/1000 | Loss: 0.00066115
Iteration 12/1000 | Loss: 0.00028108
Iteration 13/1000 | Loss: 0.00030857
Iteration 14/1000 | Loss: 0.00029346
Iteration 15/1000 | Loss: 0.00038049
Iteration 16/1000 | Loss: 0.00026649
Iteration 17/1000 | Loss: 0.00011252
Iteration 18/1000 | Loss: 0.00018041
Iteration 19/1000 | Loss: 0.00031448
Iteration 20/1000 | Loss: 0.00035477
Iteration 21/1000 | Loss: 0.00025490
Iteration 22/1000 | Loss: 0.00028387
Iteration 23/1000 | Loss: 0.00028122
Iteration 24/1000 | Loss: 0.00023596
Iteration 25/1000 | Loss: 0.00033140
Iteration 26/1000 | Loss: 0.00041247
Iteration 27/1000 | Loss: 0.00030663
Iteration 28/1000 | Loss: 0.00025592
Iteration 29/1000 | Loss: 0.00035206
Iteration 30/1000 | Loss: 0.00055593
Iteration 31/1000 | Loss: 0.00041877
Iteration 32/1000 | Loss: 0.00019130
Iteration 33/1000 | Loss: 0.00029953
Iteration 34/1000 | Loss: 0.00031109
Iteration 35/1000 | Loss: 0.00043455
Iteration 36/1000 | Loss: 0.00060691
Iteration 37/1000 | Loss: 0.00038736
Iteration 38/1000 | Loss: 0.00030580
Iteration 39/1000 | Loss: 0.00031247
Iteration 40/1000 | Loss: 0.00082026
Iteration 41/1000 | Loss: 0.00058492
Iteration 42/1000 | Loss: 0.00023878
Iteration 43/1000 | Loss: 0.00011426
Iteration 44/1000 | Loss: 0.00007984
Iteration 45/1000 | Loss: 0.00017809
Iteration 46/1000 | Loss: 0.00022002
Iteration 47/1000 | Loss: 0.00015600
Iteration 48/1000 | Loss: 0.00033314
Iteration 49/1000 | Loss: 0.00034144
Iteration 50/1000 | Loss: 0.00038145
Iteration 51/1000 | Loss: 0.00037044
Iteration 52/1000 | Loss: 0.00043498
Iteration 53/1000 | Loss: 0.00030796
Iteration 54/1000 | Loss: 0.00025687
Iteration 55/1000 | Loss: 0.00028362
Iteration 56/1000 | Loss: 0.00020795
Iteration 57/1000 | Loss: 0.00010305
Iteration 58/1000 | Loss: 0.00017806
Iteration 59/1000 | Loss: 0.00013663
Iteration 60/1000 | Loss: 0.00020569
Iteration 61/1000 | Loss: 0.00014909
Iteration 62/1000 | Loss: 0.00017899
Iteration 63/1000 | Loss: 0.00083727
Iteration 64/1000 | Loss: 0.00015579
Iteration 65/1000 | Loss: 0.00009077
Iteration 66/1000 | Loss: 0.00036568
Iteration 67/1000 | Loss: 0.00018219
Iteration 68/1000 | Loss: 0.00019660
Iteration 69/1000 | Loss: 0.00016954
Iteration 70/1000 | Loss: 0.00020234
Iteration 71/1000 | Loss: 0.00009454
Iteration 72/1000 | Loss: 0.00006510
Iteration 73/1000 | Loss: 0.00007545
Iteration 74/1000 | Loss: 0.00007827
Iteration 75/1000 | Loss: 0.00007243
Iteration 76/1000 | Loss: 0.00006409
Iteration 77/1000 | Loss: 0.00007265
Iteration 78/1000 | Loss: 0.00009343
Iteration 79/1000 | Loss: 0.00021497
Iteration 80/1000 | Loss: 0.00017215
Iteration 81/1000 | Loss: 0.00008367
Iteration 82/1000 | Loss: 0.00008091
Iteration 83/1000 | Loss: 0.00008724
Iteration 84/1000 | Loss: 0.00008257
Iteration 85/1000 | Loss: 0.00009409
Iteration 86/1000 | Loss: 0.00006461
Iteration 87/1000 | Loss: 0.00017204
Iteration 88/1000 | Loss: 0.00015115
Iteration 89/1000 | Loss: 0.00032010
Iteration 90/1000 | Loss: 0.00024374
Iteration 91/1000 | Loss: 0.00016047
Iteration 92/1000 | Loss: 0.00016031
Iteration 93/1000 | Loss: 0.00015400
Iteration 94/1000 | Loss: 0.00010333
Iteration 95/1000 | Loss: 0.00021891
Iteration 96/1000 | Loss: 0.00015418
Iteration 97/1000 | Loss: 0.00013948
Iteration 98/1000 | Loss: 0.00008369
Iteration 99/1000 | Loss: 0.00007626
Iteration 100/1000 | Loss: 0.00014546
Iteration 101/1000 | Loss: 0.00017415
Iteration 102/1000 | Loss: 0.00006197
Iteration 103/1000 | Loss: 0.00007371
Iteration 104/1000 | Loss: 0.00007330
Iteration 105/1000 | Loss: 0.00007235
Iteration 106/1000 | Loss: 0.00018772
Iteration 107/1000 | Loss: 0.00015000
Iteration 108/1000 | Loss: 0.00016741
Iteration 109/1000 | Loss: 0.00015322
Iteration 110/1000 | Loss: 0.00014014
Iteration 111/1000 | Loss: 0.00015877
Iteration 112/1000 | Loss: 0.00013072
Iteration 113/1000 | Loss: 0.00015395
Iteration 114/1000 | Loss: 0.00012681
Iteration 115/1000 | Loss: 0.00014844
Iteration 116/1000 | Loss: 0.00015007
Iteration 117/1000 | Loss: 0.00020125
Iteration 118/1000 | Loss: 0.00014935
Iteration 119/1000 | Loss: 0.00017034
Iteration 120/1000 | Loss: 0.00022018
Iteration 121/1000 | Loss: 0.00010150
Iteration 122/1000 | Loss: 0.00012523
Iteration 123/1000 | Loss: 0.00024576
Iteration 124/1000 | Loss: 0.00010979
Iteration 125/1000 | Loss: 0.00019849
Iteration 126/1000 | Loss: 0.00011022
Iteration 127/1000 | Loss: 0.00011802
Iteration 128/1000 | Loss: 0.00016431
Iteration 129/1000 | Loss: 0.00013138
Iteration 130/1000 | Loss: 0.00017391
Iteration 131/1000 | Loss: 0.00015327
Iteration 132/1000 | Loss: 0.00015173
Iteration 133/1000 | Loss: 0.00021297
Iteration 134/1000 | Loss: 0.00021221
Iteration 135/1000 | Loss: 0.00025615
Iteration 136/1000 | Loss: 0.00032647
Iteration 137/1000 | Loss: 0.00029037
Iteration 138/1000 | Loss: 0.00026227
Iteration 139/1000 | Loss: 0.00015439
Iteration 140/1000 | Loss: 0.00015703
Iteration 141/1000 | Loss: 0.00009604
Iteration 142/1000 | Loss: 0.00014604
Iteration 143/1000 | Loss: 0.00009417
Iteration 144/1000 | Loss: 0.00014349
Iteration 145/1000 | Loss: 0.00011344
Iteration 146/1000 | Loss: 0.00007000
Iteration 147/1000 | Loss: 0.00034381
Iteration 148/1000 | Loss: 0.00021790
Iteration 149/1000 | Loss: 0.00019773
Iteration 150/1000 | Loss: 0.00015876
Iteration 151/1000 | Loss: 0.00023070
Iteration 152/1000 | Loss: 0.00011227
Iteration 153/1000 | Loss: 0.00006621
Iteration 154/1000 | Loss: 0.00012238
Iteration 155/1000 | Loss: 0.00011395
Iteration 156/1000 | Loss: 0.00007514
Iteration 157/1000 | Loss: 0.00012364
Iteration 158/1000 | Loss: 0.00010018
Iteration 159/1000 | Loss: 0.00012751
Iteration 160/1000 | Loss: 0.00009000
Iteration 161/1000 | Loss: 0.00012607
Iteration 162/1000 | Loss: 0.00024866
Iteration 163/1000 | Loss: 0.00011537
Iteration 164/1000 | Loss: 0.00011984
Iteration 165/1000 | Loss: 0.00023588
Iteration 166/1000 | Loss: 0.00029787
Iteration 167/1000 | Loss: 0.00022702
Iteration 168/1000 | Loss: 0.00008325
Iteration 169/1000 | Loss: 0.00005776
Iteration 170/1000 | Loss: 0.00022925
Iteration 171/1000 | Loss: 0.00018579
Iteration 172/1000 | Loss: 0.00020410
Iteration 173/1000 | Loss: 0.00017285
Iteration 174/1000 | Loss: 0.00018067
Iteration 175/1000 | Loss: 0.00014556
Iteration 176/1000 | Loss: 0.00014761
Iteration 177/1000 | Loss: 0.00013269
Iteration 178/1000 | Loss: 0.00005946
Iteration 179/1000 | Loss: 0.00005107
Iteration 180/1000 | Loss: 0.00005424
Iteration 181/1000 | Loss: 0.00020434
Iteration 182/1000 | Loss: 0.00019739
Iteration 183/1000 | Loss: 0.00019059
Iteration 184/1000 | Loss: 0.00008913
Iteration 185/1000 | Loss: 0.00012346
Iteration 186/1000 | Loss: 0.00004680
Iteration 187/1000 | Loss: 0.00004462
Iteration 188/1000 | Loss: 0.00004509
Iteration 189/1000 | Loss: 0.00005057
Iteration 190/1000 | Loss: 0.00004470
Iteration 191/1000 | Loss: 0.00005524
Iteration 192/1000 | Loss: 0.00005205
Iteration 193/1000 | Loss: 0.00005035
Iteration 194/1000 | Loss: 0.00005438
Iteration 195/1000 | Loss: 0.00005274
Iteration 196/1000 | Loss: 0.00004387
Iteration 197/1000 | Loss: 0.00004199
Iteration 198/1000 | Loss: 0.00005210
Iteration 199/1000 | Loss: 0.00004274
Iteration 200/1000 | Loss: 0.00004588
Iteration 201/1000 | Loss: 0.00004949
Iteration 202/1000 | Loss: 0.00004913
Iteration 203/1000 | Loss: 0.00004785
Iteration 204/1000 | Loss: 0.00004651
Iteration 205/1000 | Loss: 0.00004451
Iteration 206/1000 | Loss: 0.00004459
Iteration 207/1000 | Loss: 0.00004675
Iteration 208/1000 | Loss: 0.00004993
Iteration 209/1000 | Loss: 0.00004548
Iteration 210/1000 | Loss: 0.00005008
Iteration 211/1000 | Loss: 0.00005228
Iteration 212/1000 | Loss: 0.00004418
Iteration 213/1000 | Loss: 0.00004864
Iteration 214/1000 | Loss: 0.00005157
Iteration 215/1000 | Loss: 0.00005104
Iteration 216/1000 | Loss: 0.00005306
Iteration 217/1000 | Loss: 0.00005108
Iteration 218/1000 | Loss: 0.00005045
Iteration 219/1000 | Loss: 0.00004863
Iteration 220/1000 | Loss: 0.00005089
Iteration 221/1000 | Loss: 0.00005017
Iteration 222/1000 | Loss: 0.00004907
Iteration 223/1000 | Loss: 0.00004845
Iteration 224/1000 | Loss: 0.00004861
Iteration 225/1000 | Loss: 0.00004996
Iteration 226/1000 | Loss: 0.00005123
Iteration 227/1000 | Loss: 0.00004936
Iteration 228/1000 | Loss: 0.00004949
Iteration 229/1000 | Loss: 0.00004906
Iteration 230/1000 | Loss: 0.00004925
Iteration 231/1000 | Loss: 0.00004464
Iteration 232/1000 | Loss: 0.00004295
Iteration 233/1000 | Loss: 0.00008026
Iteration 234/1000 | Loss: 0.00004591
Iteration 235/1000 | Loss: 0.00004139
Iteration 236/1000 | Loss: 0.00004086
Iteration 237/1000 | Loss: 0.00004041
Iteration 238/1000 | Loss: 0.00004017
Iteration 239/1000 | Loss: 0.00004001
Iteration 240/1000 | Loss: 0.00003992
Iteration 241/1000 | Loss: 0.00003970
Iteration 242/1000 | Loss: 0.00003967
Iteration 243/1000 | Loss: 0.00003948
Iteration 244/1000 | Loss: 0.00003929
Iteration 245/1000 | Loss: 0.00004215
Iteration 246/1000 | Loss: 0.00003993
Iteration 247/1000 | Loss: 0.00003909
Iteration 248/1000 | Loss: 0.00003882
Iteration 249/1000 | Loss: 0.00003877
Iteration 250/1000 | Loss: 0.00003872
Iteration 251/1000 | Loss: 0.00003865
Iteration 252/1000 | Loss: 0.00003859
Iteration 253/1000 | Loss: 0.00003857
Iteration 254/1000 | Loss: 0.00003850
Iteration 255/1000 | Loss: 0.00003850
Iteration 256/1000 | Loss: 0.00003846
Iteration 257/1000 | Loss: 0.00003846
Iteration 258/1000 | Loss: 0.00003844
Iteration 259/1000 | Loss: 0.00003843
Iteration 260/1000 | Loss: 0.00003843
Iteration 261/1000 | Loss: 0.00003837
Iteration 262/1000 | Loss: 0.00003829
Iteration 263/1000 | Loss: 0.00020271
Iteration 264/1000 | Loss: 0.00011430
Iteration 265/1000 | Loss: 0.00004077
Iteration 266/1000 | Loss: 0.00003906
Iteration 267/1000 | Loss: 0.00003851
Iteration 268/1000 | Loss: 0.00015893
Iteration 269/1000 | Loss: 0.00007603
Iteration 270/1000 | Loss: 0.00004023
Iteration 271/1000 | Loss: 0.00003943
Iteration 272/1000 | Loss: 0.00008231
Iteration 273/1000 | Loss: 0.00004325
Iteration 274/1000 | Loss: 0.00003858
Iteration 275/1000 | Loss: 0.00007054
Iteration 276/1000 | Loss: 0.00003876
Iteration 277/1000 | Loss: 0.00003789
Iteration 278/1000 | Loss: 0.00003734
Iteration 279/1000 | Loss: 0.00003701
Iteration 280/1000 | Loss: 0.00003698
Iteration 281/1000 | Loss: 0.00003697
Iteration 282/1000 | Loss: 0.00003694
Iteration 283/1000 | Loss: 0.00003694
Iteration 284/1000 | Loss: 0.00003693
Iteration 285/1000 | Loss: 0.00003692
Iteration 286/1000 | Loss: 0.00003691
Iteration 287/1000 | Loss: 0.00003690
Iteration 288/1000 | Loss: 0.00003690
Iteration 289/1000 | Loss: 0.00003689
Iteration 290/1000 | Loss: 0.00003687
Iteration 291/1000 | Loss: 0.00003687
Iteration 292/1000 | Loss: 0.00003682
Iteration 293/1000 | Loss: 0.00003682
Iteration 294/1000 | Loss: 0.00003681
Iteration 295/1000 | Loss: 0.00003680
Iteration 296/1000 | Loss: 0.00003680
Iteration 297/1000 | Loss: 0.00003679
Iteration 298/1000 | Loss: 0.00003678
Iteration 299/1000 | Loss: 0.00020829
Iteration 300/1000 | Loss: 0.00010667
Iteration 301/1000 | Loss: 0.00017179
Iteration 302/1000 | Loss: 0.00014358
Iteration 303/1000 | Loss: 0.00016061
Iteration 304/1000 | Loss: 0.00004025
Iteration 305/1000 | Loss: 0.00003748
Iteration 306/1000 | Loss: 0.00003660
Iteration 307/1000 | Loss: 0.00003598
Iteration 308/1000 | Loss: 0.00003572
Iteration 309/1000 | Loss: 0.00003551
Iteration 310/1000 | Loss: 0.00003545
Iteration 311/1000 | Loss: 0.00003539
Iteration 312/1000 | Loss: 0.00003535
Iteration 313/1000 | Loss: 0.00003534
Iteration 314/1000 | Loss: 0.00003534
Iteration 315/1000 | Loss: 0.00003533
Iteration 316/1000 | Loss: 0.00003533
Iteration 317/1000 | Loss: 0.00003533
Iteration 318/1000 | Loss: 0.00003532
Iteration 319/1000 | Loss: 0.00003532
Iteration 320/1000 | Loss: 0.00003531
Iteration 321/1000 | Loss: 0.00003531
Iteration 322/1000 | Loss: 0.00003528
Iteration 323/1000 | Loss: 0.00003527
Iteration 324/1000 | Loss: 0.00003527
Iteration 325/1000 | Loss: 0.00003526
Iteration 326/1000 | Loss: 0.00003526
Iteration 327/1000 | Loss: 0.00003524
Iteration 328/1000 | Loss: 0.00003524
Iteration 329/1000 | Loss: 0.00003523
Iteration 330/1000 | Loss: 0.00003523
Iteration 331/1000 | Loss: 0.00003523
Iteration 332/1000 | Loss: 0.00003523
Iteration 333/1000 | Loss: 0.00003523
Iteration 334/1000 | Loss: 0.00003523
Iteration 335/1000 | Loss: 0.00003523
Iteration 336/1000 | Loss: 0.00003523
Iteration 337/1000 | Loss: 0.00003523
Iteration 338/1000 | Loss: 0.00003523
Iteration 339/1000 | Loss: 0.00021543
Iteration 340/1000 | Loss: 0.00011504
Iteration 341/1000 | Loss: 0.00003542
Iteration 342/1000 | Loss: 0.00019701
Iteration 343/1000 | Loss: 0.00060160
Iteration 344/1000 | Loss: 0.00031108
Iteration 345/1000 | Loss: 0.00004483
Iteration 346/1000 | Loss: 0.00003641
Iteration 347/1000 | Loss: 0.00003560
Iteration 348/1000 | Loss: 0.00003483
Iteration 349/1000 | Loss: 0.00020807
Iteration 350/1000 | Loss: 0.00010896
Iteration 351/1000 | Loss: 0.00016750
Iteration 352/1000 | Loss: 0.00067705
Iteration 353/1000 | Loss: 0.00029464
Iteration 354/1000 | Loss: 0.00024029
Iteration 355/1000 | Loss: 0.00014005
Iteration 356/1000 | Loss: 0.00019548
Iteration 357/1000 | Loss: 0.00041159
Iteration 358/1000 | Loss: 0.00024427
Iteration 359/1000 | Loss: 0.00029899
Iteration 360/1000 | Loss: 0.00003970
Iteration 361/1000 | Loss: 0.00003545
Iteration 362/1000 | Loss: 0.00003308
Iteration 363/1000 | Loss: 0.00003113
Iteration 364/1000 | Loss: 0.00020994
Iteration 365/1000 | Loss: 0.00011132
Iteration 366/1000 | Loss: 0.00047610
Iteration 367/1000 | Loss: 0.00018847
Iteration 368/1000 | Loss: 0.00073328
Iteration 369/1000 | Loss: 0.00035900
Iteration 370/1000 | Loss: 0.00055287
Iteration 371/1000 | Loss: 0.00035277
Iteration 372/1000 | Loss: 0.00032630
Iteration 373/1000 | Loss: 0.00030695
Iteration 374/1000 | Loss: 0.00012452
Iteration 375/1000 | Loss: 0.00019311
Iteration 376/1000 | Loss: 0.00003298
Iteration 377/1000 | Loss: 0.00002916
Iteration 378/1000 | Loss: 0.00002671
Iteration 379/1000 | Loss: 0.00002439
Iteration 380/1000 | Loss: 0.00002362
Iteration 381/1000 | Loss: 0.00002302
Iteration 382/1000 | Loss: 0.00002254
Iteration 383/1000 | Loss: 0.00002214
Iteration 384/1000 | Loss: 0.00002180
Iteration 385/1000 | Loss: 0.00002146
Iteration 386/1000 | Loss: 0.00002117
Iteration 387/1000 | Loss: 0.00002088
Iteration 388/1000 | Loss: 0.00002069
Iteration 389/1000 | Loss: 0.00002068
Iteration 390/1000 | Loss: 0.00002065
Iteration 391/1000 | Loss: 0.00002055
Iteration 392/1000 | Loss: 0.00002052
Iteration 393/1000 | Loss: 0.00002048
Iteration 394/1000 | Loss: 0.00002045
Iteration 395/1000 | Loss: 0.00002045
Iteration 396/1000 | Loss: 0.00002044
Iteration 397/1000 | Loss: 0.00002044
Iteration 398/1000 | Loss: 0.00002044
Iteration 399/1000 | Loss: 0.00002043
Iteration 400/1000 | Loss: 0.00002043
Iteration 401/1000 | Loss: 0.00002042
Iteration 402/1000 | Loss: 0.00002042
Iteration 403/1000 | Loss: 0.00002041
Iteration 404/1000 | Loss: 0.00002041
Iteration 405/1000 | Loss: 0.00002041
Iteration 406/1000 | Loss: 0.00002040
Iteration 407/1000 | Loss: 0.00002038
Iteration 408/1000 | Loss: 0.00002038
Iteration 409/1000 | Loss: 0.00002038
Iteration 410/1000 | Loss: 0.00002038
Iteration 411/1000 | Loss: 0.00002037
Iteration 412/1000 | Loss: 0.00002037
Iteration 413/1000 | Loss: 0.00002036
Iteration 414/1000 | Loss: 0.00002035
Iteration 415/1000 | Loss: 0.00002035
Iteration 416/1000 | Loss: 0.00002035
Iteration 417/1000 | Loss: 0.00002034
Iteration 418/1000 | Loss: 0.00002034
Iteration 419/1000 | Loss: 0.00002034
Iteration 420/1000 | Loss: 0.00002034
Iteration 421/1000 | Loss: 0.00002033
Iteration 422/1000 | Loss: 0.00002033
Iteration 423/1000 | Loss: 0.00002033
Iteration 424/1000 | Loss: 0.00002033
Iteration 425/1000 | Loss: 0.00002033
Iteration 426/1000 | Loss: 0.00002033
Iteration 427/1000 | Loss: 0.00002032
Iteration 428/1000 | Loss: 0.00002032
Iteration 429/1000 | Loss: 0.00002032
Iteration 430/1000 | Loss: 0.00002032
Iteration 431/1000 | Loss: 0.00002032
Iteration 432/1000 | Loss: 0.00002032
Iteration 433/1000 | Loss: 0.00002032
Iteration 434/1000 | Loss: 0.00002032
Iteration 435/1000 | Loss: 0.00002031
Iteration 436/1000 | Loss: 0.00002031
Iteration 437/1000 | Loss: 0.00002031
Iteration 438/1000 | Loss: 0.00002031
Iteration 439/1000 | Loss: 0.00002031
Iteration 440/1000 | Loss: 0.00002031
Iteration 441/1000 | Loss: 0.00002031
Iteration 442/1000 | Loss: 0.00002031
Iteration 443/1000 | Loss: 0.00002031
Iteration 444/1000 | Loss: 0.00002031
Iteration 445/1000 | Loss: 0.00002031
Iteration 446/1000 | Loss: 0.00002031
Iteration 447/1000 | Loss: 0.00002031
Iteration 448/1000 | Loss: 0.00002031
Iteration 449/1000 | Loss: 0.00002031
Iteration 450/1000 | Loss: 0.00002031
Iteration 451/1000 | Loss: 0.00002031
Iteration 452/1000 | Loss: 0.00002031
Iteration 453/1000 | Loss: 0.00002031
Iteration 454/1000 | Loss: 0.00002031
Iteration 455/1000 | Loss: 0.00002031
Iteration 456/1000 | Loss: 0.00002030
Iteration 457/1000 | Loss: 0.00002030
Iteration 458/1000 | Loss: 0.00002030
Iteration 459/1000 | Loss: 0.00002030
Iteration 460/1000 | Loss: 0.00002030
Iteration 461/1000 | Loss: 0.00002030
Iteration 462/1000 | Loss: 0.00002030
Iteration 463/1000 | Loss: 0.00002030
Iteration 464/1000 | Loss: 0.00002030
Iteration 465/1000 | Loss: 0.00002030
Iteration 466/1000 | Loss: 0.00002030
Iteration 467/1000 | Loss: 0.00002030
Iteration 468/1000 | Loss: 0.00002029
Iteration 469/1000 | Loss: 0.00002029
Iteration 470/1000 | Loss: 0.00002029
Iteration 471/1000 | Loss: 0.00002029
Iteration 472/1000 | Loss: 0.00002029
Iteration 473/1000 | Loss: 0.00002029
Iteration 474/1000 | Loss: 0.00002029
Iteration 475/1000 | Loss: 0.00002029
Iteration 476/1000 | Loss: 0.00002029
Iteration 477/1000 | Loss: 0.00002029
Iteration 478/1000 | Loss: 0.00002029
Iteration 479/1000 | Loss: 0.00002029
Iteration 480/1000 | Loss: 0.00002029
Iteration 481/1000 | Loss: 0.00002029
Iteration 482/1000 | Loss: 0.00002029
Iteration 483/1000 | Loss: 0.00002029
Iteration 484/1000 | Loss: 0.00002029
Iteration 485/1000 | Loss: 0.00002029
Iteration 486/1000 | Loss: 0.00002029
Iteration 487/1000 | Loss: 0.00002029
Iteration 488/1000 | Loss: 0.00002029
Iteration 489/1000 | Loss: 0.00002029
Iteration 490/1000 | Loss: 0.00002029
Iteration 491/1000 | Loss: 0.00002029
Iteration 492/1000 | Loss: 0.00002029
Iteration 493/1000 | Loss: 0.00002029
Iteration 494/1000 | Loss: 0.00002029
Iteration 495/1000 | Loss: 0.00002029
Iteration 496/1000 | Loss: 0.00002029
Iteration 497/1000 | Loss: 0.00002029
Iteration 498/1000 | Loss: 0.00002029
Iteration 499/1000 | Loss: 0.00002029
Iteration 500/1000 | Loss: 0.00002029
Iteration 501/1000 | Loss: 0.00002029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 501. Stopping optimization.
Last 5 losses: [2.0293502529966645e-05, 2.0293502529966645e-05, 2.0293502529966645e-05, 2.0293502529966645e-05, 2.0293502529966645e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0293502529966645e-05

Optimization complete. Final v2v error: 3.5509355068206787 mm

Highest mean error: 11.15344524383545 mm for frame 69

Lowest mean error: 3.244023561477661 mm for frame 217

Saving results

Total time: 604.0783610343933
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1028/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1028.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1028
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00409501
Iteration 2/25 | Loss: 0.00149945
Iteration 3/25 | Loss: 0.00125239
Iteration 4/25 | Loss: 0.00122220
Iteration 5/25 | Loss: 0.00121840
Iteration 6/25 | Loss: 0.00121718
Iteration 7/25 | Loss: 0.00121718
Iteration 8/25 | Loss: 0.00121718
Iteration 9/25 | Loss: 0.00121718
Iteration 10/25 | Loss: 0.00121718
Iteration 11/25 | Loss: 0.00121718
Iteration 12/25 | Loss: 0.00121718
Iteration 13/25 | Loss: 0.00121718
Iteration 14/25 | Loss: 0.00121718
Iteration 15/25 | Loss: 0.00121718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012171808630228043, 0.0012171808630228043, 0.0012171808630228043, 0.0012171808630228043, 0.0012171808630228043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012171808630228043

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32280052
Iteration 2/25 | Loss: 0.00120056
Iteration 3/25 | Loss: 0.00120056
Iteration 4/25 | Loss: 0.00120056
Iteration 5/25 | Loss: 0.00120056
Iteration 6/25 | Loss: 0.00120056
Iteration 7/25 | Loss: 0.00120056
Iteration 8/25 | Loss: 0.00120056
Iteration 9/25 | Loss: 0.00120056
Iteration 10/25 | Loss: 0.00120056
Iteration 11/25 | Loss: 0.00120056
Iteration 12/25 | Loss: 0.00120056
Iteration 13/25 | Loss: 0.00120056
Iteration 14/25 | Loss: 0.00120056
Iteration 15/25 | Loss: 0.00120056
Iteration 16/25 | Loss: 0.00120056
Iteration 17/25 | Loss: 0.00120056
Iteration 18/25 | Loss: 0.00120056
Iteration 19/25 | Loss: 0.00120056
Iteration 20/25 | Loss: 0.00120056
Iteration 21/25 | Loss: 0.00120056
Iteration 22/25 | Loss: 0.00120056
Iteration 23/25 | Loss: 0.00120056
Iteration 24/25 | Loss: 0.00120056
Iteration 25/25 | Loss: 0.00120056

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120056
Iteration 2/1000 | Loss: 0.00003138
Iteration 3/1000 | Loss: 0.00001890
Iteration 4/1000 | Loss: 0.00001597
Iteration 5/1000 | Loss: 0.00001465
Iteration 6/1000 | Loss: 0.00001369
Iteration 7/1000 | Loss: 0.00001306
Iteration 8/1000 | Loss: 0.00001261
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001187
Iteration 12/1000 | Loss: 0.00001167
Iteration 13/1000 | Loss: 0.00001163
Iteration 14/1000 | Loss: 0.00001161
Iteration 15/1000 | Loss: 0.00001158
Iteration 16/1000 | Loss: 0.00001155
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001144
Iteration 19/1000 | Loss: 0.00001141
Iteration 20/1000 | Loss: 0.00001140
Iteration 21/1000 | Loss: 0.00001139
Iteration 22/1000 | Loss: 0.00001139
Iteration 23/1000 | Loss: 0.00001139
Iteration 24/1000 | Loss: 0.00001137
Iteration 25/1000 | Loss: 0.00001135
Iteration 26/1000 | Loss: 0.00001135
Iteration 27/1000 | Loss: 0.00001135
Iteration 28/1000 | Loss: 0.00001134
Iteration 29/1000 | Loss: 0.00001134
Iteration 30/1000 | Loss: 0.00001133
Iteration 31/1000 | Loss: 0.00001133
Iteration 32/1000 | Loss: 0.00001133
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001132
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001131
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001130
Iteration 40/1000 | Loss: 0.00001130
Iteration 41/1000 | Loss: 0.00001129
Iteration 42/1000 | Loss: 0.00001129
Iteration 43/1000 | Loss: 0.00001128
Iteration 44/1000 | Loss: 0.00001128
Iteration 45/1000 | Loss: 0.00001128
Iteration 46/1000 | Loss: 0.00001127
Iteration 47/1000 | Loss: 0.00001126
Iteration 48/1000 | Loss: 0.00001125
Iteration 49/1000 | Loss: 0.00001125
Iteration 50/1000 | Loss: 0.00001124
Iteration 51/1000 | Loss: 0.00001124
Iteration 52/1000 | Loss: 0.00001123
Iteration 53/1000 | Loss: 0.00001123
Iteration 54/1000 | Loss: 0.00001123
Iteration 55/1000 | Loss: 0.00001123
Iteration 56/1000 | Loss: 0.00001122
Iteration 57/1000 | Loss: 0.00001122
Iteration 58/1000 | Loss: 0.00001122
Iteration 59/1000 | Loss: 0.00001121
Iteration 60/1000 | Loss: 0.00001121
Iteration 61/1000 | Loss: 0.00001121
Iteration 62/1000 | Loss: 0.00001120
Iteration 63/1000 | Loss: 0.00001120
Iteration 64/1000 | Loss: 0.00001120
Iteration 65/1000 | Loss: 0.00001119
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001119
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001118
Iteration 72/1000 | Loss: 0.00001118
Iteration 73/1000 | Loss: 0.00001118
Iteration 74/1000 | Loss: 0.00001117
Iteration 75/1000 | Loss: 0.00001117
Iteration 76/1000 | Loss: 0.00001116
Iteration 77/1000 | Loss: 0.00001116
Iteration 78/1000 | Loss: 0.00001115
Iteration 79/1000 | Loss: 0.00001115
Iteration 80/1000 | Loss: 0.00001115
Iteration 81/1000 | Loss: 0.00001115
Iteration 82/1000 | Loss: 0.00001115
Iteration 83/1000 | Loss: 0.00001114
Iteration 84/1000 | Loss: 0.00001113
Iteration 85/1000 | Loss: 0.00001113
Iteration 86/1000 | Loss: 0.00001113
Iteration 87/1000 | Loss: 0.00001112
Iteration 88/1000 | Loss: 0.00001112
Iteration 89/1000 | Loss: 0.00001112
Iteration 90/1000 | Loss: 0.00001112
Iteration 91/1000 | Loss: 0.00001112
Iteration 92/1000 | Loss: 0.00001111
Iteration 93/1000 | Loss: 0.00001111
Iteration 94/1000 | Loss: 0.00001111
Iteration 95/1000 | Loss: 0.00001111
Iteration 96/1000 | Loss: 0.00001111
Iteration 97/1000 | Loss: 0.00001111
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001109
Iteration 104/1000 | Loss: 0.00001109
Iteration 105/1000 | Loss: 0.00001109
Iteration 106/1000 | Loss: 0.00001108
Iteration 107/1000 | Loss: 0.00001108
Iteration 108/1000 | Loss: 0.00001108
Iteration 109/1000 | Loss: 0.00001108
Iteration 110/1000 | Loss: 0.00001108
Iteration 111/1000 | Loss: 0.00001108
Iteration 112/1000 | Loss: 0.00001108
Iteration 113/1000 | Loss: 0.00001108
Iteration 114/1000 | Loss: 0.00001108
Iteration 115/1000 | Loss: 0.00001108
Iteration 116/1000 | Loss: 0.00001108
Iteration 117/1000 | Loss: 0.00001107
Iteration 118/1000 | Loss: 0.00001107
Iteration 119/1000 | Loss: 0.00001106
Iteration 120/1000 | Loss: 0.00001106
Iteration 121/1000 | Loss: 0.00001106
Iteration 122/1000 | Loss: 0.00001106
Iteration 123/1000 | Loss: 0.00001106
Iteration 124/1000 | Loss: 0.00001106
Iteration 125/1000 | Loss: 0.00001106
Iteration 126/1000 | Loss: 0.00001106
Iteration 127/1000 | Loss: 0.00001106
Iteration 128/1000 | Loss: 0.00001105
Iteration 129/1000 | Loss: 0.00001105
Iteration 130/1000 | Loss: 0.00001105
Iteration 131/1000 | Loss: 0.00001105
Iteration 132/1000 | Loss: 0.00001105
Iteration 133/1000 | Loss: 0.00001105
Iteration 134/1000 | Loss: 0.00001105
Iteration 135/1000 | Loss: 0.00001104
Iteration 136/1000 | Loss: 0.00001104
Iteration 137/1000 | Loss: 0.00001104
Iteration 138/1000 | Loss: 0.00001104
Iteration 139/1000 | Loss: 0.00001104
Iteration 140/1000 | Loss: 0.00001104
Iteration 141/1000 | Loss: 0.00001104
Iteration 142/1000 | Loss: 0.00001104
Iteration 143/1000 | Loss: 0.00001104
Iteration 144/1000 | Loss: 0.00001104
Iteration 145/1000 | Loss: 0.00001103
Iteration 146/1000 | Loss: 0.00001103
Iteration 147/1000 | Loss: 0.00001103
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001102
Iteration 150/1000 | Loss: 0.00001102
Iteration 151/1000 | Loss: 0.00001102
Iteration 152/1000 | Loss: 0.00001102
Iteration 153/1000 | Loss: 0.00001102
Iteration 154/1000 | Loss: 0.00001102
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001102
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001102
Iteration 159/1000 | Loss: 0.00001102
Iteration 160/1000 | Loss: 0.00001102
Iteration 161/1000 | Loss: 0.00001102
Iteration 162/1000 | Loss: 0.00001102
Iteration 163/1000 | Loss: 0.00001102
Iteration 164/1000 | Loss: 0.00001102
Iteration 165/1000 | Loss: 0.00001102
Iteration 166/1000 | Loss: 0.00001102
Iteration 167/1000 | Loss: 0.00001102
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001102
Iteration 170/1000 | Loss: 0.00001102
Iteration 171/1000 | Loss: 0.00001102
Iteration 172/1000 | Loss: 0.00001102
Iteration 173/1000 | Loss: 0.00001102
Iteration 174/1000 | Loss: 0.00001102
Iteration 175/1000 | Loss: 0.00001102
Iteration 176/1000 | Loss: 0.00001102
Iteration 177/1000 | Loss: 0.00001102
Iteration 178/1000 | Loss: 0.00001102
Iteration 179/1000 | Loss: 0.00001102
Iteration 180/1000 | Loss: 0.00001102
Iteration 181/1000 | Loss: 0.00001102
Iteration 182/1000 | Loss: 0.00001102
Iteration 183/1000 | Loss: 0.00001102
Iteration 184/1000 | Loss: 0.00001102
Iteration 185/1000 | Loss: 0.00001102
Iteration 186/1000 | Loss: 0.00001102
Iteration 187/1000 | Loss: 0.00001102
Iteration 188/1000 | Loss: 0.00001102
Iteration 189/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.1015843483619392e-05, 1.1015843483619392e-05, 1.1015843483619392e-05, 1.1015843483619392e-05, 1.1015843483619392e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1015843483619392e-05

Optimization complete. Final v2v error: 2.865752696990967 mm

Highest mean error: 3.5429153442382812 mm for frame 75

Lowest mean error: 2.641749143600464 mm for frame 146

Saving results

Total time: 42.17970871925354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1071/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1071.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1071
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00293030
Iteration 2/25 | Loss: 0.00142243
Iteration 3/25 | Loss: 0.00121698
Iteration 4/25 | Loss: 0.00119087
Iteration 5/25 | Loss: 0.00118329
Iteration 6/25 | Loss: 0.00117994
Iteration 7/25 | Loss: 0.00117883
Iteration 8/25 | Loss: 0.00117821
Iteration 9/25 | Loss: 0.00117808
Iteration 10/25 | Loss: 0.00117808
Iteration 11/25 | Loss: 0.00117808
Iteration 12/25 | Loss: 0.00117808
Iteration 13/25 | Loss: 0.00117808
Iteration 14/25 | Loss: 0.00117808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0011780839413404465, 0.0011780839413404465, 0.0011780839413404465, 0.0011780839413404465, 0.0011780839413404465]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011780839413404465

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27653193
Iteration 2/25 | Loss: 0.00157418
Iteration 3/25 | Loss: 0.00157417
Iteration 4/25 | Loss: 0.00157417
Iteration 5/25 | Loss: 0.00157417
Iteration 6/25 | Loss: 0.00157417
Iteration 7/25 | Loss: 0.00157417
Iteration 8/25 | Loss: 0.00157417
Iteration 9/25 | Loss: 0.00157417
Iteration 10/25 | Loss: 0.00157417
Iteration 11/25 | Loss: 0.00157417
Iteration 12/25 | Loss: 0.00157417
Iteration 13/25 | Loss: 0.00157417
Iteration 14/25 | Loss: 0.00157417
Iteration 15/25 | Loss: 0.00157417
Iteration 16/25 | Loss: 0.00157417
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0015741722891107202, 0.0015741722891107202, 0.0015741722891107202, 0.0015741722891107202, 0.0015741722891107202]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015741722891107202

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00157417
Iteration 2/1000 | Loss: 0.00004379
Iteration 3/1000 | Loss: 0.00002753
Iteration 4/1000 | Loss: 0.00001899
Iteration 5/1000 | Loss: 0.00001722
Iteration 6/1000 | Loss: 0.00001621
Iteration 7/1000 | Loss: 0.00001526
Iteration 8/1000 | Loss: 0.00001480
Iteration 9/1000 | Loss: 0.00001432
Iteration 10/1000 | Loss: 0.00001401
Iteration 11/1000 | Loss: 0.00001376
Iteration 12/1000 | Loss: 0.00001356
Iteration 13/1000 | Loss: 0.00001355
Iteration 14/1000 | Loss: 0.00001353
Iteration 15/1000 | Loss: 0.00001352
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001333
Iteration 18/1000 | Loss: 0.00001329
Iteration 19/1000 | Loss: 0.00001323
Iteration 20/1000 | Loss: 0.00001320
Iteration 21/1000 | Loss: 0.00001319
Iteration 22/1000 | Loss: 0.00001318
Iteration 23/1000 | Loss: 0.00001317
Iteration 24/1000 | Loss: 0.00001313
Iteration 25/1000 | Loss: 0.00001309
Iteration 26/1000 | Loss: 0.00001309
Iteration 27/1000 | Loss: 0.00001308
Iteration 28/1000 | Loss: 0.00001308
Iteration 29/1000 | Loss: 0.00001307
Iteration 30/1000 | Loss: 0.00001307
Iteration 31/1000 | Loss: 0.00001306
Iteration 32/1000 | Loss: 0.00001306
Iteration 33/1000 | Loss: 0.00001305
Iteration 34/1000 | Loss: 0.00001305
Iteration 35/1000 | Loss: 0.00001304
Iteration 36/1000 | Loss: 0.00001303
Iteration 37/1000 | Loss: 0.00001303
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001301
Iteration 42/1000 | Loss: 0.00001300
Iteration 43/1000 | Loss: 0.00001300
Iteration 44/1000 | Loss: 0.00001299
Iteration 45/1000 | Loss: 0.00001298
Iteration 46/1000 | Loss: 0.00001298
Iteration 47/1000 | Loss: 0.00001297
Iteration 48/1000 | Loss: 0.00001297
Iteration 49/1000 | Loss: 0.00001296
Iteration 50/1000 | Loss: 0.00001296
Iteration 51/1000 | Loss: 0.00001295
Iteration 52/1000 | Loss: 0.00001295
Iteration 53/1000 | Loss: 0.00001295
Iteration 54/1000 | Loss: 0.00001294
Iteration 55/1000 | Loss: 0.00001294
Iteration 56/1000 | Loss: 0.00001294
Iteration 57/1000 | Loss: 0.00001294
Iteration 58/1000 | Loss: 0.00001294
Iteration 59/1000 | Loss: 0.00001294
Iteration 60/1000 | Loss: 0.00001294
Iteration 61/1000 | Loss: 0.00001294
Iteration 62/1000 | Loss: 0.00001294
Iteration 63/1000 | Loss: 0.00001294
Iteration 64/1000 | Loss: 0.00001293
Iteration 65/1000 | Loss: 0.00001293
Iteration 66/1000 | Loss: 0.00001292
Iteration 67/1000 | Loss: 0.00001292
Iteration 68/1000 | Loss: 0.00001292
Iteration 69/1000 | Loss: 0.00001292
Iteration 70/1000 | Loss: 0.00001292
Iteration 71/1000 | Loss: 0.00001292
Iteration 72/1000 | Loss: 0.00001292
Iteration 73/1000 | Loss: 0.00001292
Iteration 74/1000 | Loss: 0.00001291
Iteration 75/1000 | Loss: 0.00001291
Iteration 76/1000 | Loss: 0.00001291
Iteration 77/1000 | Loss: 0.00001291
Iteration 78/1000 | Loss: 0.00001291
Iteration 79/1000 | Loss: 0.00001291
Iteration 80/1000 | Loss: 0.00001290
Iteration 81/1000 | Loss: 0.00001290
Iteration 82/1000 | Loss: 0.00001290
Iteration 83/1000 | Loss: 0.00001289
Iteration 84/1000 | Loss: 0.00001289
Iteration 85/1000 | Loss: 0.00001289
Iteration 86/1000 | Loss: 0.00001289
Iteration 87/1000 | Loss: 0.00001289
Iteration 88/1000 | Loss: 0.00001288
Iteration 89/1000 | Loss: 0.00001288
Iteration 90/1000 | Loss: 0.00001288
Iteration 91/1000 | Loss: 0.00001287
Iteration 92/1000 | Loss: 0.00001286
Iteration 93/1000 | Loss: 0.00001286
Iteration 94/1000 | Loss: 0.00001286
Iteration 95/1000 | Loss: 0.00001286
Iteration 96/1000 | Loss: 0.00001285
Iteration 97/1000 | Loss: 0.00001285
Iteration 98/1000 | Loss: 0.00001285
Iteration 99/1000 | Loss: 0.00001285
Iteration 100/1000 | Loss: 0.00001285
Iteration 101/1000 | Loss: 0.00001285
Iteration 102/1000 | Loss: 0.00001285
Iteration 103/1000 | Loss: 0.00001284
Iteration 104/1000 | Loss: 0.00001284
Iteration 105/1000 | Loss: 0.00001284
Iteration 106/1000 | Loss: 0.00001284
Iteration 107/1000 | Loss: 0.00001284
Iteration 108/1000 | Loss: 0.00001283
Iteration 109/1000 | Loss: 0.00001283
Iteration 110/1000 | Loss: 0.00001283
Iteration 111/1000 | Loss: 0.00001283
Iteration 112/1000 | Loss: 0.00001283
Iteration 113/1000 | Loss: 0.00001282
Iteration 114/1000 | Loss: 0.00001282
Iteration 115/1000 | Loss: 0.00001282
Iteration 116/1000 | Loss: 0.00001281
Iteration 117/1000 | Loss: 0.00001281
Iteration 118/1000 | Loss: 0.00001281
Iteration 119/1000 | Loss: 0.00001281
Iteration 120/1000 | Loss: 0.00001280
Iteration 121/1000 | Loss: 0.00001280
Iteration 122/1000 | Loss: 0.00001280
Iteration 123/1000 | Loss: 0.00001279
Iteration 124/1000 | Loss: 0.00001279
Iteration 125/1000 | Loss: 0.00001279
Iteration 126/1000 | Loss: 0.00001279
Iteration 127/1000 | Loss: 0.00001279
Iteration 128/1000 | Loss: 0.00001278
Iteration 129/1000 | Loss: 0.00001278
Iteration 130/1000 | Loss: 0.00001278
Iteration 131/1000 | Loss: 0.00001278
Iteration 132/1000 | Loss: 0.00001277
Iteration 133/1000 | Loss: 0.00001277
Iteration 134/1000 | Loss: 0.00001277
Iteration 135/1000 | Loss: 0.00001277
Iteration 136/1000 | Loss: 0.00001277
Iteration 137/1000 | Loss: 0.00001276
Iteration 138/1000 | Loss: 0.00001276
Iteration 139/1000 | Loss: 0.00001276
Iteration 140/1000 | Loss: 0.00001276
Iteration 141/1000 | Loss: 0.00001276
Iteration 142/1000 | Loss: 0.00001276
Iteration 143/1000 | Loss: 0.00001275
Iteration 144/1000 | Loss: 0.00001275
Iteration 145/1000 | Loss: 0.00001275
Iteration 146/1000 | Loss: 0.00001275
Iteration 147/1000 | Loss: 0.00001275
Iteration 148/1000 | Loss: 0.00001275
Iteration 149/1000 | Loss: 0.00001275
Iteration 150/1000 | Loss: 0.00001274
Iteration 151/1000 | Loss: 0.00001274
Iteration 152/1000 | Loss: 0.00001274
Iteration 153/1000 | Loss: 0.00001274
Iteration 154/1000 | Loss: 0.00001273
Iteration 155/1000 | Loss: 0.00001273
Iteration 156/1000 | Loss: 0.00001273
Iteration 157/1000 | Loss: 0.00001273
Iteration 158/1000 | Loss: 0.00001273
Iteration 159/1000 | Loss: 0.00001273
Iteration 160/1000 | Loss: 0.00001273
Iteration 161/1000 | Loss: 0.00001273
Iteration 162/1000 | Loss: 0.00001273
Iteration 163/1000 | Loss: 0.00001272
Iteration 164/1000 | Loss: 0.00001272
Iteration 165/1000 | Loss: 0.00001272
Iteration 166/1000 | Loss: 0.00001272
Iteration 167/1000 | Loss: 0.00001272
Iteration 168/1000 | Loss: 0.00001272
Iteration 169/1000 | Loss: 0.00001272
Iteration 170/1000 | Loss: 0.00001272
Iteration 171/1000 | Loss: 0.00001271
Iteration 172/1000 | Loss: 0.00001271
Iteration 173/1000 | Loss: 0.00001271
Iteration 174/1000 | Loss: 0.00001271
Iteration 175/1000 | Loss: 0.00001271
Iteration 176/1000 | Loss: 0.00001271
Iteration 177/1000 | Loss: 0.00001271
Iteration 178/1000 | Loss: 0.00001271
Iteration 179/1000 | Loss: 0.00001271
Iteration 180/1000 | Loss: 0.00001271
Iteration 181/1000 | Loss: 0.00001271
Iteration 182/1000 | Loss: 0.00001270
Iteration 183/1000 | Loss: 0.00001270
Iteration 184/1000 | Loss: 0.00001270
Iteration 185/1000 | Loss: 0.00001270
Iteration 186/1000 | Loss: 0.00001270
Iteration 187/1000 | Loss: 0.00001270
Iteration 188/1000 | Loss: 0.00001270
Iteration 189/1000 | Loss: 0.00001270
Iteration 190/1000 | Loss: 0.00001269
Iteration 191/1000 | Loss: 0.00001269
Iteration 192/1000 | Loss: 0.00001269
Iteration 193/1000 | Loss: 0.00001269
Iteration 194/1000 | Loss: 0.00001269
Iteration 195/1000 | Loss: 0.00001269
Iteration 196/1000 | Loss: 0.00001269
Iteration 197/1000 | Loss: 0.00001269
Iteration 198/1000 | Loss: 0.00001269
Iteration 199/1000 | Loss: 0.00001269
Iteration 200/1000 | Loss: 0.00001269
Iteration 201/1000 | Loss: 0.00001269
Iteration 202/1000 | Loss: 0.00001269
Iteration 203/1000 | Loss: 0.00001268
Iteration 204/1000 | Loss: 0.00001268
Iteration 205/1000 | Loss: 0.00001268
Iteration 206/1000 | Loss: 0.00001268
Iteration 207/1000 | Loss: 0.00001268
Iteration 208/1000 | Loss: 0.00001268
Iteration 209/1000 | Loss: 0.00001268
Iteration 210/1000 | Loss: 0.00001268
Iteration 211/1000 | Loss: 0.00001268
Iteration 212/1000 | Loss: 0.00001268
Iteration 213/1000 | Loss: 0.00001268
Iteration 214/1000 | Loss: 0.00001268
Iteration 215/1000 | Loss: 0.00001268
Iteration 216/1000 | Loss: 0.00001268
Iteration 217/1000 | Loss: 0.00001268
Iteration 218/1000 | Loss: 0.00001268
Iteration 219/1000 | Loss: 0.00001268
Iteration 220/1000 | Loss: 0.00001267
Iteration 221/1000 | Loss: 0.00001267
Iteration 222/1000 | Loss: 0.00001267
Iteration 223/1000 | Loss: 0.00001267
Iteration 224/1000 | Loss: 0.00001267
Iteration 225/1000 | Loss: 0.00001267
Iteration 226/1000 | Loss: 0.00001267
Iteration 227/1000 | Loss: 0.00001267
Iteration 228/1000 | Loss: 0.00001267
Iteration 229/1000 | Loss: 0.00001267
Iteration 230/1000 | Loss: 0.00001267
Iteration 231/1000 | Loss: 0.00001267
Iteration 232/1000 | Loss: 0.00001267
Iteration 233/1000 | Loss: 0.00001267
Iteration 234/1000 | Loss: 0.00001267
Iteration 235/1000 | Loss: 0.00001266
Iteration 236/1000 | Loss: 0.00001266
Iteration 237/1000 | Loss: 0.00001266
Iteration 238/1000 | Loss: 0.00001266
Iteration 239/1000 | Loss: 0.00001266
Iteration 240/1000 | Loss: 0.00001266
Iteration 241/1000 | Loss: 0.00001266
Iteration 242/1000 | Loss: 0.00001266
Iteration 243/1000 | Loss: 0.00001266
Iteration 244/1000 | Loss: 0.00001266
Iteration 245/1000 | Loss: 0.00001266
Iteration 246/1000 | Loss: 0.00001266
Iteration 247/1000 | Loss: 0.00001266
Iteration 248/1000 | Loss: 0.00001266
Iteration 249/1000 | Loss: 0.00001265
Iteration 250/1000 | Loss: 0.00001265
Iteration 251/1000 | Loss: 0.00001265
Iteration 252/1000 | Loss: 0.00001265
Iteration 253/1000 | Loss: 0.00001265
Iteration 254/1000 | Loss: 0.00001265
Iteration 255/1000 | Loss: 0.00001265
Iteration 256/1000 | Loss: 0.00001265
Iteration 257/1000 | Loss: 0.00001265
Iteration 258/1000 | Loss: 0.00001265
Iteration 259/1000 | Loss: 0.00001265
Iteration 260/1000 | Loss: 0.00001265
Iteration 261/1000 | Loss: 0.00001264
Iteration 262/1000 | Loss: 0.00001264
Iteration 263/1000 | Loss: 0.00001264
Iteration 264/1000 | Loss: 0.00001264
Iteration 265/1000 | Loss: 0.00001264
Iteration 266/1000 | Loss: 0.00001264
Iteration 267/1000 | Loss: 0.00001264
Iteration 268/1000 | Loss: 0.00001264
Iteration 269/1000 | Loss: 0.00001264
Iteration 270/1000 | Loss: 0.00001264
Iteration 271/1000 | Loss: 0.00001264
Iteration 272/1000 | Loss: 0.00001264
Iteration 273/1000 | Loss: 0.00001264
Iteration 274/1000 | Loss: 0.00001264
Iteration 275/1000 | Loss: 0.00001264
Iteration 276/1000 | Loss: 0.00001264
Iteration 277/1000 | Loss: 0.00001264
Iteration 278/1000 | Loss: 0.00001264
Iteration 279/1000 | Loss: 0.00001264
Iteration 280/1000 | Loss: 0.00001264
Iteration 281/1000 | Loss: 0.00001264
Iteration 282/1000 | Loss: 0.00001264
Iteration 283/1000 | Loss: 0.00001264
Iteration 284/1000 | Loss: 0.00001264
Iteration 285/1000 | Loss: 0.00001264
Iteration 286/1000 | Loss: 0.00001264
Iteration 287/1000 | Loss: 0.00001264
Iteration 288/1000 | Loss: 0.00001264
Iteration 289/1000 | Loss: 0.00001264
Iteration 290/1000 | Loss: 0.00001264
Iteration 291/1000 | Loss: 0.00001264
Iteration 292/1000 | Loss: 0.00001264
Iteration 293/1000 | Loss: 0.00001264
Iteration 294/1000 | Loss: 0.00001264
Iteration 295/1000 | Loss: 0.00001264
Iteration 296/1000 | Loss: 0.00001264
Iteration 297/1000 | Loss: 0.00001264
Iteration 298/1000 | Loss: 0.00001264
Iteration 299/1000 | Loss: 0.00001264
Iteration 300/1000 | Loss: 0.00001264
Iteration 301/1000 | Loss: 0.00001264
Iteration 302/1000 | Loss: 0.00001264
Iteration 303/1000 | Loss: 0.00001264
Iteration 304/1000 | Loss: 0.00001264
Iteration 305/1000 | Loss: 0.00001264
Iteration 306/1000 | Loss: 0.00001264
Iteration 307/1000 | Loss: 0.00001264
Iteration 308/1000 | Loss: 0.00001264
Iteration 309/1000 | Loss: 0.00001264
Iteration 310/1000 | Loss: 0.00001264
Iteration 311/1000 | Loss: 0.00001264
Iteration 312/1000 | Loss: 0.00001264
Iteration 313/1000 | Loss: 0.00001264
Iteration 314/1000 | Loss: 0.00001264
Iteration 315/1000 | Loss: 0.00001264
Iteration 316/1000 | Loss: 0.00001264
Iteration 317/1000 | Loss: 0.00001264
Iteration 318/1000 | Loss: 0.00001264
Iteration 319/1000 | Loss: 0.00001264
Iteration 320/1000 | Loss: 0.00001264
Iteration 321/1000 | Loss: 0.00001264
Iteration 322/1000 | Loss: 0.00001264
Iteration 323/1000 | Loss: 0.00001264
Iteration 324/1000 | Loss: 0.00001264
Iteration 325/1000 | Loss: 0.00001264
Iteration 326/1000 | Loss: 0.00001264
Iteration 327/1000 | Loss: 0.00001264
Iteration 328/1000 | Loss: 0.00001264
Iteration 329/1000 | Loss: 0.00001264
Iteration 330/1000 | Loss: 0.00001264
Iteration 331/1000 | Loss: 0.00001264
Iteration 332/1000 | Loss: 0.00001264
Iteration 333/1000 | Loss: 0.00001264
Iteration 334/1000 | Loss: 0.00001264
Iteration 335/1000 | Loss: 0.00001264
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 335. Stopping optimization.
Last 5 losses: [1.263568083231803e-05, 1.263568083231803e-05, 1.263568083231803e-05, 1.263568083231803e-05, 1.263568083231803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.263568083231803e-05

Optimization complete. Final v2v error: 2.990001678466797 mm

Highest mean error: 3.7271878719329834 mm for frame 107

Lowest mean error: 2.6177401542663574 mm for frame 48

Saving results

Total time: 53.865761041641235
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00580689
Iteration 2/25 | Loss: 0.00146131
Iteration 3/25 | Loss: 0.00129214
Iteration 4/25 | Loss: 0.00127206
Iteration 5/25 | Loss: 0.00126710
Iteration 6/25 | Loss: 0.00126537
Iteration 7/25 | Loss: 0.00126493
Iteration 8/25 | Loss: 0.00126473
Iteration 9/25 | Loss: 0.00126466
Iteration 10/25 | Loss: 0.00126466
Iteration 11/25 | Loss: 0.00126466
Iteration 12/25 | Loss: 0.00126466
Iteration 13/25 | Loss: 0.00126466
Iteration 14/25 | Loss: 0.00126466
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012646580580621958, 0.0012646580580621958, 0.0012646580580621958, 0.0012646580580621958, 0.0012646580580621958]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012646580580621958

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.01574636
Iteration 2/25 | Loss: 0.00129055
Iteration 3/25 | Loss: 0.00129052
Iteration 4/25 | Loss: 0.00129051
Iteration 5/25 | Loss: 0.00129051
Iteration 6/25 | Loss: 0.00129051
Iteration 7/25 | Loss: 0.00129051
Iteration 8/25 | Loss: 0.00129051
Iteration 9/25 | Loss: 0.00129051
Iteration 10/25 | Loss: 0.00129051
Iteration 11/25 | Loss: 0.00129051
Iteration 12/25 | Loss: 0.00129051
Iteration 13/25 | Loss: 0.00129051
Iteration 14/25 | Loss: 0.00129051
Iteration 15/25 | Loss: 0.00129051
Iteration 16/25 | Loss: 0.00129051
Iteration 17/25 | Loss: 0.00129051
Iteration 18/25 | Loss: 0.00129051
Iteration 19/25 | Loss: 0.00129051
Iteration 20/25 | Loss: 0.00129051
Iteration 21/25 | Loss: 0.00129051
Iteration 22/25 | Loss: 0.00129051
Iteration 23/25 | Loss: 0.00129051
Iteration 24/25 | Loss: 0.00129051
Iteration 25/25 | Loss: 0.00129051

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129051
Iteration 2/1000 | Loss: 0.00004672
Iteration 3/1000 | Loss: 0.00002934
Iteration 4/1000 | Loss: 0.00002157
Iteration 5/1000 | Loss: 0.00004437
Iteration 6/1000 | Loss: 0.00002082
Iteration 7/1000 | Loss: 0.00001844
Iteration 8/1000 | Loss: 0.00001764
Iteration 9/1000 | Loss: 0.00001689
Iteration 10/1000 | Loss: 0.00001645
Iteration 11/1000 | Loss: 0.00001615
Iteration 12/1000 | Loss: 0.00001570
Iteration 13/1000 | Loss: 0.00001532
Iteration 14/1000 | Loss: 0.00001513
Iteration 15/1000 | Loss: 0.00001510
Iteration 16/1000 | Loss: 0.00001488
Iteration 17/1000 | Loss: 0.00001456
Iteration 18/1000 | Loss: 0.00001448
Iteration 19/1000 | Loss: 0.00001447
Iteration 20/1000 | Loss: 0.00001445
Iteration 21/1000 | Loss: 0.00001429
Iteration 22/1000 | Loss: 0.00001405
Iteration 23/1000 | Loss: 0.00001378
Iteration 24/1000 | Loss: 0.00001353
Iteration 25/1000 | Loss: 0.00001334
Iteration 26/1000 | Loss: 0.00001327
Iteration 27/1000 | Loss: 0.00001321
Iteration 28/1000 | Loss: 0.00001319
Iteration 29/1000 | Loss: 0.00001316
Iteration 30/1000 | Loss: 0.00001315
Iteration 31/1000 | Loss: 0.00001315
Iteration 32/1000 | Loss: 0.00001314
Iteration 33/1000 | Loss: 0.00001314
Iteration 34/1000 | Loss: 0.00001313
Iteration 35/1000 | Loss: 0.00001313
Iteration 36/1000 | Loss: 0.00001313
Iteration 37/1000 | Loss: 0.00001313
Iteration 38/1000 | Loss: 0.00001313
Iteration 39/1000 | Loss: 0.00001313
Iteration 40/1000 | Loss: 0.00001313
Iteration 41/1000 | Loss: 0.00001313
Iteration 42/1000 | Loss: 0.00001313
Iteration 43/1000 | Loss: 0.00001313
Iteration 44/1000 | Loss: 0.00001312
Iteration 45/1000 | Loss: 0.00001312
Iteration 46/1000 | Loss: 0.00001312
Iteration 47/1000 | Loss: 0.00001311
Iteration 48/1000 | Loss: 0.00001311
Iteration 49/1000 | Loss: 0.00001311
Iteration 50/1000 | Loss: 0.00001311
Iteration 51/1000 | Loss: 0.00001311
Iteration 52/1000 | Loss: 0.00001310
Iteration 53/1000 | Loss: 0.00001310
Iteration 54/1000 | Loss: 0.00001310
Iteration 55/1000 | Loss: 0.00001310
Iteration 56/1000 | Loss: 0.00001310
Iteration 57/1000 | Loss: 0.00001309
Iteration 58/1000 | Loss: 0.00001309
Iteration 59/1000 | Loss: 0.00001309
Iteration 60/1000 | Loss: 0.00001309
Iteration 61/1000 | Loss: 0.00001309
Iteration 62/1000 | Loss: 0.00001309
Iteration 63/1000 | Loss: 0.00001309
Iteration 64/1000 | Loss: 0.00001309
Iteration 65/1000 | Loss: 0.00001308
Iteration 66/1000 | Loss: 0.00001308
Iteration 67/1000 | Loss: 0.00001308
Iteration 68/1000 | Loss: 0.00001308
Iteration 69/1000 | Loss: 0.00001308
Iteration 70/1000 | Loss: 0.00001308
Iteration 71/1000 | Loss: 0.00001308
Iteration 72/1000 | Loss: 0.00001308
Iteration 73/1000 | Loss: 0.00001308
Iteration 74/1000 | Loss: 0.00001308
Iteration 75/1000 | Loss: 0.00001308
Iteration 76/1000 | Loss: 0.00001308
Iteration 77/1000 | Loss: 0.00001308
Iteration 78/1000 | Loss: 0.00001308
Iteration 79/1000 | Loss: 0.00001308
Iteration 80/1000 | Loss: 0.00001308
Iteration 81/1000 | Loss: 0.00001308
Iteration 82/1000 | Loss: 0.00001307
Iteration 83/1000 | Loss: 0.00001307
Iteration 84/1000 | Loss: 0.00001307
Iteration 85/1000 | Loss: 0.00001307
Iteration 86/1000 | Loss: 0.00001307
Iteration 87/1000 | Loss: 0.00001307
Iteration 88/1000 | Loss: 0.00001307
Iteration 89/1000 | Loss: 0.00001307
Iteration 90/1000 | Loss: 0.00001307
Iteration 91/1000 | Loss: 0.00001307
Iteration 92/1000 | Loss: 0.00001307
Iteration 93/1000 | Loss: 0.00001307
Iteration 94/1000 | Loss: 0.00001307
Iteration 95/1000 | Loss: 0.00001307
Iteration 96/1000 | Loss: 0.00001307
Iteration 97/1000 | Loss: 0.00001306
Iteration 98/1000 | Loss: 0.00001306
Iteration 99/1000 | Loss: 0.00001306
Iteration 100/1000 | Loss: 0.00001306
Iteration 101/1000 | Loss: 0.00001306
Iteration 102/1000 | Loss: 0.00001306
Iteration 103/1000 | Loss: 0.00001306
Iteration 104/1000 | Loss: 0.00001306
Iteration 105/1000 | Loss: 0.00001306
Iteration 106/1000 | Loss: 0.00001306
Iteration 107/1000 | Loss: 0.00001306
Iteration 108/1000 | Loss: 0.00001305
Iteration 109/1000 | Loss: 0.00001305
Iteration 110/1000 | Loss: 0.00001305
Iteration 111/1000 | Loss: 0.00001305
Iteration 112/1000 | Loss: 0.00001305
Iteration 113/1000 | Loss: 0.00001305
Iteration 114/1000 | Loss: 0.00001305
Iteration 115/1000 | Loss: 0.00001305
Iteration 116/1000 | Loss: 0.00001305
Iteration 117/1000 | Loss: 0.00001305
Iteration 118/1000 | Loss: 0.00001305
Iteration 119/1000 | Loss: 0.00001305
Iteration 120/1000 | Loss: 0.00001305
Iteration 121/1000 | Loss: 0.00001305
Iteration 122/1000 | Loss: 0.00001305
Iteration 123/1000 | Loss: 0.00001305
Iteration 124/1000 | Loss: 0.00001305
Iteration 125/1000 | Loss: 0.00001305
Iteration 126/1000 | Loss: 0.00001304
Iteration 127/1000 | Loss: 0.00001304
Iteration 128/1000 | Loss: 0.00001304
Iteration 129/1000 | Loss: 0.00001304
Iteration 130/1000 | Loss: 0.00001304
Iteration 131/1000 | Loss: 0.00001304
Iteration 132/1000 | Loss: 0.00001304
Iteration 133/1000 | Loss: 0.00001304
Iteration 134/1000 | Loss: 0.00001304
Iteration 135/1000 | Loss: 0.00001304
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001304
Iteration 138/1000 | Loss: 0.00001304
Iteration 139/1000 | Loss: 0.00001304
Iteration 140/1000 | Loss: 0.00001304
Iteration 141/1000 | Loss: 0.00001304
Iteration 142/1000 | Loss: 0.00001304
Iteration 143/1000 | Loss: 0.00001304
Iteration 144/1000 | Loss: 0.00001304
Iteration 145/1000 | Loss: 0.00001304
Iteration 146/1000 | Loss: 0.00001304
Iteration 147/1000 | Loss: 0.00001304
Iteration 148/1000 | Loss: 0.00001304
Iteration 149/1000 | Loss: 0.00001304
Iteration 150/1000 | Loss: 0.00001304
Iteration 151/1000 | Loss: 0.00001304
Iteration 152/1000 | Loss: 0.00001304
Iteration 153/1000 | Loss: 0.00001304
Iteration 154/1000 | Loss: 0.00001304
Iteration 155/1000 | Loss: 0.00001304
Iteration 156/1000 | Loss: 0.00001304
Iteration 157/1000 | Loss: 0.00001304
Iteration 158/1000 | Loss: 0.00001304
Iteration 159/1000 | Loss: 0.00001304
Iteration 160/1000 | Loss: 0.00001304
Iteration 161/1000 | Loss: 0.00001304
Iteration 162/1000 | Loss: 0.00001304
Iteration 163/1000 | Loss: 0.00001304
Iteration 164/1000 | Loss: 0.00001304
Iteration 165/1000 | Loss: 0.00001304
Iteration 166/1000 | Loss: 0.00001304
Iteration 167/1000 | Loss: 0.00001304
Iteration 168/1000 | Loss: 0.00001304
Iteration 169/1000 | Loss: 0.00001304
Iteration 170/1000 | Loss: 0.00001304
Iteration 171/1000 | Loss: 0.00001304
Iteration 172/1000 | Loss: 0.00001304
Iteration 173/1000 | Loss: 0.00001304
Iteration 174/1000 | Loss: 0.00001304
Iteration 175/1000 | Loss: 0.00001304
Iteration 176/1000 | Loss: 0.00001304
Iteration 177/1000 | Loss: 0.00001304
Iteration 178/1000 | Loss: 0.00001304
Iteration 179/1000 | Loss: 0.00001304
Iteration 180/1000 | Loss: 0.00001304
Iteration 181/1000 | Loss: 0.00001304
Iteration 182/1000 | Loss: 0.00001304
Iteration 183/1000 | Loss: 0.00001304
Iteration 184/1000 | Loss: 0.00001304
Iteration 185/1000 | Loss: 0.00001304
Iteration 186/1000 | Loss: 0.00001304
Iteration 187/1000 | Loss: 0.00001304
Iteration 188/1000 | Loss: 0.00001304
Iteration 189/1000 | Loss: 0.00001304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.3040124940744136e-05, 1.3040124940744136e-05, 1.3040124940744136e-05, 1.3040124940744136e-05, 1.3040124940744136e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3040124940744136e-05

Optimization complete. Final v2v error: 3.0531928539276123 mm

Highest mean error: 4.009669303894043 mm for frame 88

Lowest mean error: 2.7093350887298584 mm for frame 0

Saving results

Total time: 55.46789288520813
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1045/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1045.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1045
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509320
Iteration 2/25 | Loss: 0.00131835
Iteration 3/25 | Loss: 0.00124552
Iteration 4/25 | Loss: 0.00122979
Iteration 5/25 | Loss: 0.00122454
Iteration 6/25 | Loss: 0.00122330
Iteration 7/25 | Loss: 0.00122330
Iteration 8/25 | Loss: 0.00122330
Iteration 9/25 | Loss: 0.00122330
Iteration 10/25 | Loss: 0.00122330
Iteration 11/25 | Loss: 0.00122330
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012233040761202574, 0.0012233040761202574, 0.0012233040761202574, 0.0012233040761202574, 0.0012233040761202574]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012233040761202574

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.99175787
Iteration 2/25 | Loss: 0.00129504
Iteration 3/25 | Loss: 0.00129504
Iteration 4/25 | Loss: 0.00129504
Iteration 5/25 | Loss: 0.00129504
Iteration 6/25 | Loss: 0.00129504
Iteration 7/25 | Loss: 0.00129504
Iteration 8/25 | Loss: 0.00129504
Iteration 9/25 | Loss: 0.00129504
Iteration 10/25 | Loss: 0.00129504
Iteration 11/25 | Loss: 0.00129504
Iteration 12/25 | Loss: 0.00129504
Iteration 13/25 | Loss: 0.00129504
Iteration 14/25 | Loss: 0.00129504
Iteration 15/25 | Loss: 0.00129504
Iteration 16/25 | Loss: 0.00129504
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0012950376840308309, 0.0012950376840308309, 0.0012950376840308309, 0.0012950376840308309, 0.0012950376840308309]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012950376840308309

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129504
Iteration 2/1000 | Loss: 0.00003942
Iteration 3/1000 | Loss: 0.00002640
Iteration 4/1000 | Loss: 0.00002181
Iteration 5/1000 | Loss: 0.00002060
Iteration 6/1000 | Loss: 0.00001989
Iteration 7/1000 | Loss: 0.00001936
Iteration 8/1000 | Loss: 0.00001891
Iteration 9/1000 | Loss: 0.00001853
Iteration 10/1000 | Loss: 0.00001831
Iteration 11/1000 | Loss: 0.00001805
Iteration 12/1000 | Loss: 0.00001785
Iteration 13/1000 | Loss: 0.00001772
Iteration 14/1000 | Loss: 0.00001758
Iteration 15/1000 | Loss: 0.00001754
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001737
Iteration 18/1000 | Loss: 0.00001736
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001731
Iteration 21/1000 | Loss: 0.00001729
Iteration 22/1000 | Loss: 0.00001729
Iteration 23/1000 | Loss: 0.00001724
Iteration 24/1000 | Loss: 0.00001724
Iteration 25/1000 | Loss: 0.00001719
Iteration 26/1000 | Loss: 0.00001719
Iteration 27/1000 | Loss: 0.00001719
Iteration 28/1000 | Loss: 0.00001718
Iteration 29/1000 | Loss: 0.00001717
Iteration 30/1000 | Loss: 0.00001717
Iteration 31/1000 | Loss: 0.00001717
Iteration 32/1000 | Loss: 0.00001716
Iteration 33/1000 | Loss: 0.00001716
Iteration 34/1000 | Loss: 0.00001716
Iteration 35/1000 | Loss: 0.00001715
Iteration 36/1000 | Loss: 0.00001715
Iteration 37/1000 | Loss: 0.00001714
Iteration 38/1000 | Loss: 0.00001714
Iteration 39/1000 | Loss: 0.00001713
Iteration 40/1000 | Loss: 0.00001713
Iteration 41/1000 | Loss: 0.00001713
Iteration 42/1000 | Loss: 0.00001713
Iteration 43/1000 | Loss: 0.00001713
Iteration 44/1000 | Loss: 0.00001712
Iteration 45/1000 | Loss: 0.00001712
Iteration 46/1000 | Loss: 0.00001712
Iteration 47/1000 | Loss: 0.00001712
Iteration 48/1000 | Loss: 0.00001712
Iteration 49/1000 | Loss: 0.00001711
Iteration 50/1000 | Loss: 0.00001711
Iteration 51/1000 | Loss: 0.00001711
Iteration 52/1000 | Loss: 0.00001710
Iteration 53/1000 | Loss: 0.00001710
Iteration 54/1000 | Loss: 0.00001710
Iteration 55/1000 | Loss: 0.00001709
Iteration 56/1000 | Loss: 0.00001709
Iteration 57/1000 | Loss: 0.00001709
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001708
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001707
Iteration 63/1000 | Loss: 0.00001707
Iteration 64/1000 | Loss: 0.00001706
Iteration 65/1000 | Loss: 0.00001706
Iteration 66/1000 | Loss: 0.00001706
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001703
Iteration 71/1000 | Loss: 0.00001703
Iteration 72/1000 | Loss: 0.00001703
Iteration 73/1000 | Loss: 0.00001702
Iteration 74/1000 | Loss: 0.00001702
Iteration 75/1000 | Loss: 0.00001701
Iteration 76/1000 | Loss: 0.00001701
Iteration 77/1000 | Loss: 0.00001701
Iteration 78/1000 | Loss: 0.00001700
Iteration 79/1000 | Loss: 0.00001700
Iteration 80/1000 | Loss: 0.00001700
Iteration 81/1000 | Loss: 0.00001699
Iteration 82/1000 | Loss: 0.00001699
Iteration 83/1000 | Loss: 0.00001699
Iteration 84/1000 | Loss: 0.00001699
Iteration 85/1000 | Loss: 0.00001698
Iteration 86/1000 | Loss: 0.00001698
Iteration 87/1000 | Loss: 0.00001697
Iteration 88/1000 | Loss: 0.00001697
Iteration 89/1000 | Loss: 0.00001697
Iteration 90/1000 | Loss: 0.00001696
Iteration 91/1000 | Loss: 0.00001696
Iteration 92/1000 | Loss: 0.00001695
Iteration 93/1000 | Loss: 0.00001695
Iteration 94/1000 | Loss: 0.00001695
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001694
Iteration 99/1000 | Loss: 0.00001693
Iteration 100/1000 | Loss: 0.00001693
Iteration 101/1000 | Loss: 0.00001693
Iteration 102/1000 | Loss: 0.00001692
Iteration 103/1000 | Loss: 0.00001692
Iteration 104/1000 | Loss: 0.00001692
Iteration 105/1000 | Loss: 0.00001692
Iteration 106/1000 | Loss: 0.00001691
Iteration 107/1000 | Loss: 0.00001691
Iteration 108/1000 | Loss: 0.00001691
Iteration 109/1000 | Loss: 0.00001691
Iteration 110/1000 | Loss: 0.00001691
Iteration 111/1000 | Loss: 0.00001691
Iteration 112/1000 | Loss: 0.00001690
Iteration 113/1000 | Loss: 0.00001690
Iteration 114/1000 | Loss: 0.00001690
Iteration 115/1000 | Loss: 0.00001690
Iteration 116/1000 | Loss: 0.00001690
Iteration 117/1000 | Loss: 0.00001690
Iteration 118/1000 | Loss: 0.00001690
Iteration 119/1000 | Loss: 0.00001689
Iteration 120/1000 | Loss: 0.00001689
Iteration 121/1000 | Loss: 0.00001689
Iteration 122/1000 | Loss: 0.00001689
Iteration 123/1000 | Loss: 0.00001689
Iteration 124/1000 | Loss: 0.00001689
Iteration 125/1000 | Loss: 0.00001688
Iteration 126/1000 | Loss: 0.00001688
Iteration 127/1000 | Loss: 0.00001688
Iteration 128/1000 | Loss: 0.00001688
Iteration 129/1000 | Loss: 0.00001687
Iteration 130/1000 | Loss: 0.00001687
Iteration 131/1000 | Loss: 0.00001687
Iteration 132/1000 | Loss: 0.00001687
Iteration 133/1000 | Loss: 0.00001687
Iteration 134/1000 | Loss: 0.00001687
Iteration 135/1000 | Loss: 0.00001687
Iteration 136/1000 | Loss: 0.00001687
Iteration 137/1000 | Loss: 0.00001687
Iteration 138/1000 | Loss: 0.00001686
Iteration 139/1000 | Loss: 0.00001686
Iteration 140/1000 | Loss: 0.00001686
Iteration 141/1000 | Loss: 0.00001686
Iteration 142/1000 | Loss: 0.00001686
Iteration 143/1000 | Loss: 0.00001686
Iteration 144/1000 | Loss: 0.00001685
Iteration 145/1000 | Loss: 0.00001685
Iteration 146/1000 | Loss: 0.00001685
Iteration 147/1000 | Loss: 0.00001685
Iteration 148/1000 | Loss: 0.00001685
Iteration 149/1000 | Loss: 0.00001685
Iteration 150/1000 | Loss: 0.00001685
Iteration 151/1000 | Loss: 0.00001685
Iteration 152/1000 | Loss: 0.00001685
Iteration 153/1000 | Loss: 0.00001685
Iteration 154/1000 | Loss: 0.00001685
Iteration 155/1000 | Loss: 0.00001685
Iteration 156/1000 | Loss: 0.00001685
Iteration 157/1000 | Loss: 0.00001685
Iteration 158/1000 | Loss: 0.00001685
Iteration 159/1000 | Loss: 0.00001685
Iteration 160/1000 | Loss: 0.00001685
Iteration 161/1000 | Loss: 0.00001684
Iteration 162/1000 | Loss: 0.00001684
Iteration 163/1000 | Loss: 0.00001684
Iteration 164/1000 | Loss: 0.00001684
Iteration 165/1000 | Loss: 0.00001684
Iteration 166/1000 | Loss: 0.00001684
Iteration 167/1000 | Loss: 0.00001684
Iteration 168/1000 | Loss: 0.00001684
Iteration 169/1000 | Loss: 0.00001684
Iteration 170/1000 | Loss: 0.00001684
Iteration 171/1000 | Loss: 0.00001683
Iteration 172/1000 | Loss: 0.00001683
Iteration 173/1000 | Loss: 0.00001683
Iteration 174/1000 | Loss: 0.00001683
Iteration 175/1000 | Loss: 0.00001683
Iteration 176/1000 | Loss: 0.00001683
Iteration 177/1000 | Loss: 0.00001683
Iteration 178/1000 | Loss: 0.00001683
Iteration 179/1000 | Loss: 0.00001683
Iteration 180/1000 | Loss: 0.00001683
Iteration 181/1000 | Loss: 0.00001683
Iteration 182/1000 | Loss: 0.00001683
Iteration 183/1000 | Loss: 0.00001683
Iteration 184/1000 | Loss: 0.00001683
Iteration 185/1000 | Loss: 0.00001683
Iteration 186/1000 | Loss: 0.00001683
Iteration 187/1000 | Loss: 0.00001682
Iteration 188/1000 | Loss: 0.00001682
Iteration 189/1000 | Loss: 0.00001682
Iteration 190/1000 | Loss: 0.00001682
Iteration 191/1000 | Loss: 0.00001682
Iteration 192/1000 | Loss: 0.00001682
Iteration 193/1000 | Loss: 0.00001682
Iteration 194/1000 | Loss: 0.00001682
Iteration 195/1000 | Loss: 0.00001682
Iteration 196/1000 | Loss: 0.00001682
Iteration 197/1000 | Loss: 0.00001682
Iteration 198/1000 | Loss: 0.00001682
Iteration 199/1000 | Loss: 0.00001682
Iteration 200/1000 | Loss: 0.00001682
Iteration 201/1000 | Loss: 0.00001682
Iteration 202/1000 | Loss: 0.00001682
Iteration 203/1000 | Loss: 0.00001681
Iteration 204/1000 | Loss: 0.00001681
Iteration 205/1000 | Loss: 0.00001681
Iteration 206/1000 | Loss: 0.00001681
Iteration 207/1000 | Loss: 0.00001681
Iteration 208/1000 | Loss: 0.00001681
Iteration 209/1000 | Loss: 0.00001681
Iteration 210/1000 | Loss: 0.00001681
Iteration 211/1000 | Loss: 0.00001681
Iteration 212/1000 | Loss: 0.00001681
Iteration 213/1000 | Loss: 0.00001681
Iteration 214/1000 | Loss: 0.00001681
Iteration 215/1000 | Loss: 0.00001681
Iteration 216/1000 | Loss: 0.00001681
Iteration 217/1000 | Loss: 0.00001681
Iteration 218/1000 | Loss: 0.00001681
Iteration 219/1000 | Loss: 0.00001681
Iteration 220/1000 | Loss: 0.00001681
Iteration 221/1000 | Loss: 0.00001681
Iteration 222/1000 | Loss: 0.00001681
Iteration 223/1000 | Loss: 0.00001681
Iteration 224/1000 | Loss: 0.00001681
Iteration 225/1000 | Loss: 0.00001681
Iteration 226/1000 | Loss: 0.00001681
Iteration 227/1000 | Loss: 0.00001681
Iteration 228/1000 | Loss: 0.00001681
Iteration 229/1000 | Loss: 0.00001681
Iteration 230/1000 | Loss: 0.00001681
Iteration 231/1000 | Loss: 0.00001681
Iteration 232/1000 | Loss: 0.00001681
Iteration 233/1000 | Loss: 0.00001681
Iteration 234/1000 | Loss: 0.00001681
Iteration 235/1000 | Loss: 0.00001681
Iteration 236/1000 | Loss: 0.00001681
Iteration 237/1000 | Loss: 0.00001681
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 237. Stopping optimization.
Last 5 losses: [1.680806053627748e-05, 1.680806053627748e-05, 1.680806053627748e-05, 1.680806053627748e-05, 1.680806053627748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.680806053627748e-05

Optimization complete. Final v2v error: 3.449765920639038 mm

Highest mean error: 4.574209213256836 mm for frame 67

Lowest mean error: 2.946138858795166 mm for frame 18

Saving results

Total time: 48.02957224845886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1070/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1070.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1070
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00755337
Iteration 2/25 | Loss: 0.00214645
Iteration 3/25 | Loss: 0.00162620
Iteration 4/25 | Loss: 0.00163708
Iteration 5/25 | Loss: 0.00148303
Iteration 6/25 | Loss: 0.00143626
Iteration 7/25 | Loss: 0.00140253
Iteration 8/25 | Loss: 0.00137633
Iteration 9/25 | Loss: 0.00135199
Iteration 10/25 | Loss: 0.00134672
Iteration 11/25 | Loss: 0.00134449
Iteration 12/25 | Loss: 0.00134258
Iteration 13/25 | Loss: 0.00134416
Iteration 14/25 | Loss: 0.00133873
Iteration 15/25 | Loss: 0.00134031
Iteration 16/25 | Loss: 0.00134105
Iteration 17/25 | Loss: 0.00132920
Iteration 18/25 | Loss: 0.00133092
Iteration 19/25 | Loss: 0.00132600
Iteration 20/25 | Loss: 0.00132859
Iteration 21/25 | Loss: 0.00132390
Iteration 22/25 | Loss: 0.00132293
Iteration 23/25 | Loss: 0.00132286
Iteration 24/25 | Loss: 0.00132286
Iteration 25/25 | Loss: 0.00132285

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22202110
Iteration 2/25 | Loss: 0.00117282
Iteration 3/25 | Loss: 0.00117280
Iteration 4/25 | Loss: 0.00117280
Iteration 5/25 | Loss: 0.00117280
Iteration 6/25 | Loss: 0.00117280
Iteration 7/25 | Loss: 0.00117280
Iteration 8/25 | Loss: 0.00117280
Iteration 9/25 | Loss: 0.00117280
Iteration 10/25 | Loss: 0.00117280
Iteration 11/25 | Loss: 0.00117280
Iteration 12/25 | Loss: 0.00117280
Iteration 13/25 | Loss: 0.00117280
Iteration 14/25 | Loss: 0.00117280
Iteration 15/25 | Loss: 0.00117280
Iteration 16/25 | Loss: 0.00117280
Iteration 17/25 | Loss: 0.00117280
Iteration 18/25 | Loss: 0.00117280
Iteration 19/25 | Loss: 0.00117280
Iteration 20/25 | Loss: 0.00117280
Iteration 21/25 | Loss: 0.00117280
Iteration 22/25 | Loss: 0.00117280
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.001172801828943193, 0.001172801828943193, 0.001172801828943193, 0.001172801828943193, 0.001172801828943193]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001172801828943193

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117280
Iteration 2/1000 | Loss: 0.00070851
Iteration 3/1000 | Loss: 0.00006613
Iteration 4/1000 | Loss: 0.00004281
Iteration 5/1000 | Loss: 0.00004807
Iteration 6/1000 | Loss: 0.00005542
Iteration 7/1000 | Loss: 0.00005568
Iteration 8/1000 | Loss: 0.00003263
Iteration 9/1000 | Loss: 0.00004878
Iteration 10/1000 | Loss: 0.00003832
Iteration 11/1000 | Loss: 0.00004280
Iteration 12/1000 | Loss: 0.00003722
Iteration 13/1000 | Loss: 0.00003843
Iteration 14/1000 | Loss: 0.00003177
Iteration 15/1000 | Loss: 0.00003806
Iteration 16/1000 | Loss: 0.00002973
Iteration 17/1000 | Loss: 0.00003467
Iteration 18/1000 | Loss: 0.00036710
Iteration 19/1000 | Loss: 0.00003629
Iteration 20/1000 | Loss: 0.00004483
Iteration 21/1000 | Loss: 0.00003007
Iteration 22/1000 | Loss: 0.00002677
Iteration 23/1000 | Loss: 0.00004222
Iteration 24/1000 | Loss: 0.00004121
Iteration 25/1000 | Loss: 0.00003796
Iteration 26/1000 | Loss: 0.00003654
Iteration 27/1000 | Loss: 0.00003656
Iteration 28/1000 | Loss: 0.00002095
Iteration 29/1000 | Loss: 0.00002886
Iteration 30/1000 | Loss: 0.00002310
Iteration 31/1000 | Loss: 0.00002693
Iteration 32/1000 | Loss: 0.00002612
Iteration 33/1000 | Loss: 0.00003292
Iteration 34/1000 | Loss: 0.00002567
Iteration 35/1000 | Loss: 0.00003276
Iteration 36/1000 | Loss: 0.00002883
Iteration 37/1000 | Loss: 0.00003806
Iteration 38/1000 | Loss: 0.00003219
Iteration 39/1000 | Loss: 0.00003324
Iteration 40/1000 | Loss: 0.00003162
Iteration 41/1000 | Loss: 0.00003337
Iteration 42/1000 | Loss: 0.00002836
Iteration 43/1000 | Loss: 0.00003433
Iteration 44/1000 | Loss: 0.00003054
Iteration 45/1000 | Loss: 0.00003879
Iteration 46/1000 | Loss: 0.00002867
Iteration 47/1000 | Loss: 0.00002735
Iteration 48/1000 | Loss: 0.00002906
Iteration 49/1000 | Loss: 0.00004156
Iteration 50/1000 | Loss: 0.00002814
Iteration 51/1000 | Loss: 0.00004117
Iteration 52/1000 | Loss: 0.00002775
Iteration 53/1000 | Loss: 0.00003138
Iteration 54/1000 | Loss: 0.00002920
Iteration 55/1000 | Loss: 0.00002719
Iteration 56/1000 | Loss: 0.00003064
Iteration 57/1000 | Loss: 0.00004314
Iteration 58/1000 | Loss: 0.00002061
Iteration 59/1000 | Loss: 0.00002929
Iteration 60/1000 | Loss: 0.00003017
Iteration 61/1000 | Loss: 0.00002871
Iteration 62/1000 | Loss: 0.00003177
Iteration 63/1000 | Loss: 0.00002813
Iteration 64/1000 | Loss: 0.00003088
Iteration 65/1000 | Loss: 0.00002318
Iteration 66/1000 | Loss: 0.00003587
Iteration 67/1000 | Loss: 0.00003007
Iteration 68/1000 | Loss: 0.00003062
Iteration 69/1000 | Loss: 0.00002973
Iteration 70/1000 | Loss: 0.00003041
Iteration 71/1000 | Loss: 0.00003715
Iteration 72/1000 | Loss: 0.00003111
Iteration 73/1000 | Loss: 0.00002797
Iteration 74/1000 | Loss: 0.00003304
Iteration 75/1000 | Loss: 0.00002875
Iteration 76/1000 | Loss: 0.00003073
Iteration 77/1000 | Loss: 0.00002672
Iteration 78/1000 | Loss: 0.00002756
Iteration 79/1000 | Loss: 0.00002462
Iteration 80/1000 | Loss: 0.00003163
Iteration 81/1000 | Loss: 0.00003292
Iteration 82/1000 | Loss: 0.00003132
Iteration 83/1000 | Loss: 0.00002731
Iteration 84/1000 | Loss: 0.00003413
Iteration 85/1000 | Loss: 0.00002893
Iteration 86/1000 | Loss: 0.00003228
Iteration 87/1000 | Loss: 0.00002162
Iteration 88/1000 | Loss: 0.00002872
Iteration 89/1000 | Loss: 0.00004506
Iteration 90/1000 | Loss: 0.00002912
Iteration 91/1000 | Loss: 0.00003730
Iteration 92/1000 | Loss: 0.00002757
Iteration 93/1000 | Loss: 0.00003390
Iteration 94/1000 | Loss: 0.00002662
Iteration 95/1000 | Loss: 0.00002173
Iteration 96/1000 | Loss: 0.00001962
Iteration 97/1000 | Loss: 0.00002947
Iteration 98/1000 | Loss: 0.00002804
Iteration 99/1000 | Loss: 0.00002712
Iteration 100/1000 | Loss: 0.00002829
Iteration 101/1000 | Loss: 0.00002880
Iteration 102/1000 | Loss: 0.00002647
Iteration 103/1000 | Loss: 0.00002657
Iteration 104/1000 | Loss: 0.00002842
Iteration 105/1000 | Loss: 0.00002917
Iteration 106/1000 | Loss: 0.00002823
Iteration 107/1000 | Loss: 0.00003283
Iteration 108/1000 | Loss: 0.00002233
Iteration 109/1000 | Loss: 0.00003029
Iteration 110/1000 | Loss: 0.00003267
Iteration 111/1000 | Loss: 0.00004284
Iteration 112/1000 | Loss: 0.00003223
Iteration 113/1000 | Loss: 0.00003687
Iteration 114/1000 | Loss: 0.00002822
Iteration 115/1000 | Loss: 0.00003793
Iteration 116/1000 | Loss: 0.00002846
Iteration 117/1000 | Loss: 0.00003450
Iteration 118/1000 | Loss: 0.00002796
Iteration 119/1000 | Loss: 0.00003269
Iteration 120/1000 | Loss: 0.00002656
Iteration 121/1000 | Loss: 0.00003243
Iteration 122/1000 | Loss: 0.00003060
Iteration 123/1000 | Loss: 0.00003241
Iteration 124/1000 | Loss: 0.00003061
Iteration 125/1000 | Loss: 0.00003900
Iteration 126/1000 | Loss: 0.00002966
Iteration 127/1000 | Loss: 0.00003519
Iteration 128/1000 | Loss: 0.00002967
Iteration 129/1000 | Loss: 0.00002902
Iteration 130/1000 | Loss: 0.00002938
Iteration 131/1000 | Loss: 0.00003203
Iteration 132/1000 | Loss: 0.00002882
Iteration 133/1000 | Loss: 0.00002567
Iteration 134/1000 | Loss: 0.00002925
Iteration 135/1000 | Loss: 0.00002991
Iteration 136/1000 | Loss: 0.00003023
Iteration 137/1000 | Loss: 0.00003157
Iteration 138/1000 | Loss: 0.00003016
Iteration 139/1000 | Loss: 0.00003395
Iteration 140/1000 | Loss: 0.00002143
Iteration 141/1000 | Loss: 0.00002919
Iteration 142/1000 | Loss: 0.00003144
Iteration 143/1000 | Loss: 0.00002934
Iteration 144/1000 | Loss: 0.00003279
Iteration 145/1000 | Loss: 0.00003006
Iteration 146/1000 | Loss: 0.00003115
Iteration 147/1000 | Loss: 0.00003595
Iteration 148/1000 | Loss: 0.00003219
Iteration 149/1000 | Loss: 0.00002308
Iteration 150/1000 | Loss: 0.00002835
Iteration 151/1000 | Loss: 0.00003157
Iteration 152/1000 | Loss: 0.00003313
Iteration 153/1000 | Loss: 0.00002115
Iteration 154/1000 | Loss: 0.00001922
Iteration 155/1000 | Loss: 0.00001841
Iteration 156/1000 | Loss: 0.00001803
Iteration 157/1000 | Loss: 0.00001789
Iteration 158/1000 | Loss: 0.00001783
Iteration 159/1000 | Loss: 0.00001766
Iteration 160/1000 | Loss: 0.00001756
Iteration 161/1000 | Loss: 0.00001752
Iteration 162/1000 | Loss: 0.00001739
Iteration 163/1000 | Loss: 0.00001738
Iteration 164/1000 | Loss: 0.00001738
Iteration 165/1000 | Loss: 0.00001738
Iteration 166/1000 | Loss: 0.00001738
Iteration 167/1000 | Loss: 0.00001737
Iteration 168/1000 | Loss: 0.00001736
Iteration 169/1000 | Loss: 0.00001735
Iteration 170/1000 | Loss: 0.00001734
Iteration 171/1000 | Loss: 0.00001733
Iteration 172/1000 | Loss: 0.00001733
Iteration 173/1000 | Loss: 0.00001732
Iteration 174/1000 | Loss: 0.00001731
Iteration 175/1000 | Loss: 0.00001730
Iteration 176/1000 | Loss: 0.00001729
Iteration 177/1000 | Loss: 0.00001729
Iteration 178/1000 | Loss: 0.00001729
Iteration 179/1000 | Loss: 0.00001728
Iteration 180/1000 | Loss: 0.00001728
Iteration 181/1000 | Loss: 0.00001728
Iteration 182/1000 | Loss: 0.00001728
Iteration 183/1000 | Loss: 0.00001728
Iteration 184/1000 | Loss: 0.00001728
Iteration 185/1000 | Loss: 0.00001728
Iteration 186/1000 | Loss: 0.00001728
Iteration 187/1000 | Loss: 0.00001727
Iteration 188/1000 | Loss: 0.00001727
Iteration 189/1000 | Loss: 0.00001727
Iteration 190/1000 | Loss: 0.00001727
Iteration 191/1000 | Loss: 0.00001727
Iteration 192/1000 | Loss: 0.00001727
Iteration 193/1000 | Loss: 0.00001727
Iteration 194/1000 | Loss: 0.00001726
Iteration 195/1000 | Loss: 0.00001726
Iteration 196/1000 | Loss: 0.00001725
Iteration 197/1000 | Loss: 0.00001725
Iteration 198/1000 | Loss: 0.00001724
Iteration 199/1000 | Loss: 0.00001724
Iteration 200/1000 | Loss: 0.00001724
Iteration 201/1000 | Loss: 0.00001723
Iteration 202/1000 | Loss: 0.00001723
Iteration 203/1000 | Loss: 0.00001723
Iteration 204/1000 | Loss: 0.00001723
Iteration 205/1000 | Loss: 0.00001722
Iteration 206/1000 | Loss: 0.00001722
Iteration 207/1000 | Loss: 0.00001722
Iteration 208/1000 | Loss: 0.00001721
Iteration 209/1000 | Loss: 0.00001721
Iteration 210/1000 | Loss: 0.00001720
Iteration 211/1000 | Loss: 0.00001720
Iteration 212/1000 | Loss: 0.00001719
Iteration 213/1000 | Loss: 0.00001719
Iteration 214/1000 | Loss: 0.00001719
Iteration 215/1000 | Loss: 0.00001719
Iteration 216/1000 | Loss: 0.00001718
Iteration 217/1000 | Loss: 0.00001718
Iteration 218/1000 | Loss: 0.00001718
Iteration 219/1000 | Loss: 0.00001718
Iteration 220/1000 | Loss: 0.00001718
Iteration 221/1000 | Loss: 0.00001718
Iteration 222/1000 | Loss: 0.00001718
Iteration 223/1000 | Loss: 0.00001718
Iteration 224/1000 | Loss: 0.00001718
Iteration 225/1000 | Loss: 0.00001717
Iteration 226/1000 | Loss: 0.00001717
Iteration 227/1000 | Loss: 0.00001717
Iteration 228/1000 | Loss: 0.00001717
Iteration 229/1000 | Loss: 0.00001717
Iteration 230/1000 | Loss: 0.00001717
Iteration 231/1000 | Loss: 0.00001717
Iteration 232/1000 | Loss: 0.00001716
Iteration 233/1000 | Loss: 0.00001716
Iteration 234/1000 | Loss: 0.00001716
Iteration 235/1000 | Loss: 0.00001716
Iteration 236/1000 | Loss: 0.00001716
Iteration 237/1000 | Loss: 0.00001716
Iteration 238/1000 | Loss: 0.00001716
Iteration 239/1000 | Loss: 0.00001716
Iteration 240/1000 | Loss: 0.00001716
Iteration 241/1000 | Loss: 0.00001716
Iteration 242/1000 | Loss: 0.00001715
Iteration 243/1000 | Loss: 0.00001715
Iteration 244/1000 | Loss: 0.00001715
Iteration 245/1000 | Loss: 0.00001715
Iteration 246/1000 | Loss: 0.00001715
Iteration 247/1000 | Loss: 0.00001715
Iteration 248/1000 | Loss: 0.00001715
Iteration 249/1000 | Loss: 0.00001715
Iteration 250/1000 | Loss: 0.00001715
Iteration 251/1000 | Loss: 0.00001715
Iteration 252/1000 | Loss: 0.00001715
Iteration 253/1000 | Loss: 0.00001715
Iteration 254/1000 | Loss: 0.00001715
Iteration 255/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.7148266124422662e-05, 1.7148266124422662e-05, 1.7148266124422662e-05, 1.7148266124422662e-05, 1.7148266124422662e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7148266124422662e-05

Optimization complete. Final v2v error: 3.473144054412842 mm

Highest mean error: 4.794092655181885 mm for frame 78

Lowest mean error: 3.2381324768066406 mm for frame 151

Saving results

Total time: 307.5764148235321
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447835
Iteration 2/25 | Loss: 0.00125385
Iteration 3/25 | Loss: 0.00118187
Iteration 4/25 | Loss: 0.00117314
Iteration 5/25 | Loss: 0.00117095
Iteration 6/25 | Loss: 0.00117025
Iteration 7/25 | Loss: 0.00117025
Iteration 8/25 | Loss: 0.00117025
Iteration 9/25 | Loss: 0.00117025
Iteration 10/25 | Loss: 0.00117025
Iteration 11/25 | Loss: 0.00117025
Iteration 12/25 | Loss: 0.00117025
Iteration 13/25 | Loss: 0.00117025
Iteration 14/25 | Loss: 0.00117025
Iteration 15/25 | Loss: 0.00117025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0011702498886734247, 0.0011702498886734247, 0.0011702498886734247, 0.0011702498886734247, 0.0011702498886734247]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011702498886734247

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26933110
Iteration 2/25 | Loss: 0.00096078
Iteration 3/25 | Loss: 0.00096076
Iteration 4/25 | Loss: 0.00096076
Iteration 5/25 | Loss: 0.00096076
Iteration 6/25 | Loss: 0.00096076
Iteration 7/25 | Loss: 0.00096076
Iteration 8/25 | Loss: 0.00096076
Iteration 9/25 | Loss: 0.00096076
Iteration 10/25 | Loss: 0.00096076
Iteration 11/25 | Loss: 0.00096076
Iteration 12/25 | Loss: 0.00096076
Iteration 13/25 | Loss: 0.00096076
Iteration 14/25 | Loss: 0.00096076
Iteration 15/25 | Loss: 0.00096076
Iteration 16/25 | Loss: 0.00096076
Iteration 17/25 | Loss: 0.00096076
Iteration 18/25 | Loss: 0.00096076
Iteration 19/25 | Loss: 0.00096076
Iteration 20/25 | Loss: 0.00096076
Iteration 21/25 | Loss: 0.00096076
Iteration 22/25 | Loss: 0.00096076
Iteration 23/25 | Loss: 0.00096076
Iteration 24/25 | Loss: 0.00096076
Iteration 25/25 | Loss: 0.00096076

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096076
Iteration 2/1000 | Loss: 0.00002134
Iteration 3/1000 | Loss: 0.00001543
Iteration 4/1000 | Loss: 0.00001375
Iteration 5/1000 | Loss: 0.00001287
Iteration 6/1000 | Loss: 0.00001211
Iteration 7/1000 | Loss: 0.00001164
Iteration 8/1000 | Loss: 0.00001136
Iteration 9/1000 | Loss: 0.00001135
Iteration 10/1000 | Loss: 0.00001118
Iteration 11/1000 | Loss: 0.00001098
Iteration 12/1000 | Loss: 0.00001093
Iteration 13/1000 | Loss: 0.00001090
Iteration 14/1000 | Loss: 0.00001071
Iteration 15/1000 | Loss: 0.00001068
Iteration 16/1000 | Loss: 0.00001066
Iteration 17/1000 | Loss: 0.00001064
Iteration 18/1000 | Loss: 0.00001064
Iteration 19/1000 | Loss: 0.00001063
Iteration 20/1000 | Loss: 0.00001060
Iteration 21/1000 | Loss: 0.00001059
Iteration 22/1000 | Loss: 0.00001051
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001043
Iteration 25/1000 | Loss: 0.00001043
Iteration 26/1000 | Loss: 0.00001038
Iteration 27/1000 | Loss: 0.00001038
Iteration 28/1000 | Loss: 0.00001036
Iteration 29/1000 | Loss: 0.00001035
Iteration 30/1000 | Loss: 0.00001034
Iteration 31/1000 | Loss: 0.00001033
Iteration 32/1000 | Loss: 0.00001033
Iteration 33/1000 | Loss: 0.00001033
Iteration 34/1000 | Loss: 0.00001032
Iteration 35/1000 | Loss: 0.00001030
Iteration 36/1000 | Loss: 0.00001030
Iteration 37/1000 | Loss: 0.00001029
Iteration 38/1000 | Loss: 0.00001029
Iteration 39/1000 | Loss: 0.00001029
Iteration 40/1000 | Loss: 0.00001029
Iteration 41/1000 | Loss: 0.00001028
Iteration 42/1000 | Loss: 0.00001028
Iteration 43/1000 | Loss: 0.00001027
Iteration 44/1000 | Loss: 0.00001027
Iteration 45/1000 | Loss: 0.00001027
Iteration 46/1000 | Loss: 0.00001027
Iteration 47/1000 | Loss: 0.00001026
Iteration 48/1000 | Loss: 0.00001026
Iteration 49/1000 | Loss: 0.00001025
Iteration 50/1000 | Loss: 0.00001025
Iteration 51/1000 | Loss: 0.00001024
Iteration 52/1000 | Loss: 0.00001024
Iteration 53/1000 | Loss: 0.00001023
Iteration 54/1000 | Loss: 0.00001023
Iteration 55/1000 | Loss: 0.00001023
Iteration 56/1000 | Loss: 0.00001023
Iteration 57/1000 | Loss: 0.00001023
Iteration 58/1000 | Loss: 0.00001022
Iteration 59/1000 | Loss: 0.00001021
Iteration 60/1000 | Loss: 0.00001020
Iteration 61/1000 | Loss: 0.00001018
Iteration 62/1000 | Loss: 0.00001017
Iteration 63/1000 | Loss: 0.00001017
Iteration 64/1000 | Loss: 0.00001016
Iteration 65/1000 | Loss: 0.00001014
Iteration 66/1000 | Loss: 0.00001011
Iteration 67/1000 | Loss: 0.00001008
Iteration 68/1000 | Loss: 0.00001008
Iteration 69/1000 | Loss: 0.00001008
Iteration 70/1000 | Loss: 0.00001008
Iteration 71/1000 | Loss: 0.00001008
Iteration 72/1000 | Loss: 0.00001007
Iteration 73/1000 | Loss: 0.00001007
Iteration 74/1000 | Loss: 0.00001007
Iteration 75/1000 | Loss: 0.00001007
Iteration 76/1000 | Loss: 0.00001007
Iteration 77/1000 | Loss: 0.00001007
Iteration 78/1000 | Loss: 0.00001007
Iteration 79/1000 | Loss: 0.00001007
Iteration 80/1000 | Loss: 0.00001007
Iteration 81/1000 | Loss: 0.00001007
Iteration 82/1000 | Loss: 0.00001007
Iteration 83/1000 | Loss: 0.00001006
Iteration 84/1000 | Loss: 0.00001006
Iteration 85/1000 | Loss: 0.00001005
Iteration 86/1000 | Loss: 0.00001004
Iteration 87/1000 | Loss: 0.00001004
Iteration 88/1000 | Loss: 0.00001004
Iteration 89/1000 | Loss: 0.00001003
Iteration 90/1000 | Loss: 0.00001003
Iteration 91/1000 | Loss: 0.00001003
Iteration 92/1000 | Loss: 0.00001003
Iteration 93/1000 | Loss: 0.00001003
Iteration 94/1000 | Loss: 0.00001002
Iteration 95/1000 | Loss: 0.00001002
Iteration 96/1000 | Loss: 0.00001002
Iteration 97/1000 | Loss: 0.00001002
Iteration 98/1000 | Loss: 0.00001002
Iteration 99/1000 | Loss: 0.00001002
Iteration 100/1000 | Loss: 0.00001002
Iteration 101/1000 | Loss: 0.00001002
Iteration 102/1000 | Loss: 0.00001002
Iteration 103/1000 | Loss: 0.00001002
Iteration 104/1000 | Loss: 0.00001001
Iteration 105/1000 | Loss: 0.00001001
Iteration 106/1000 | Loss: 0.00001001
Iteration 107/1000 | Loss: 0.00001000
Iteration 108/1000 | Loss: 0.00001000
Iteration 109/1000 | Loss: 0.00001000
Iteration 110/1000 | Loss: 0.00001000
Iteration 111/1000 | Loss: 0.00001000
Iteration 112/1000 | Loss: 0.00000999
Iteration 113/1000 | Loss: 0.00000999
Iteration 114/1000 | Loss: 0.00000999
Iteration 115/1000 | Loss: 0.00000999
Iteration 116/1000 | Loss: 0.00000998
Iteration 117/1000 | Loss: 0.00000998
Iteration 118/1000 | Loss: 0.00000998
Iteration 119/1000 | Loss: 0.00000997
Iteration 120/1000 | Loss: 0.00000997
Iteration 121/1000 | Loss: 0.00000997
Iteration 122/1000 | Loss: 0.00000996
Iteration 123/1000 | Loss: 0.00000996
Iteration 124/1000 | Loss: 0.00000996
Iteration 125/1000 | Loss: 0.00000996
Iteration 126/1000 | Loss: 0.00000996
Iteration 127/1000 | Loss: 0.00000996
Iteration 128/1000 | Loss: 0.00000995
Iteration 129/1000 | Loss: 0.00000995
Iteration 130/1000 | Loss: 0.00000995
Iteration 131/1000 | Loss: 0.00000995
Iteration 132/1000 | Loss: 0.00000995
Iteration 133/1000 | Loss: 0.00000995
Iteration 134/1000 | Loss: 0.00000995
Iteration 135/1000 | Loss: 0.00000995
Iteration 136/1000 | Loss: 0.00000994
Iteration 137/1000 | Loss: 0.00000994
Iteration 138/1000 | Loss: 0.00000994
Iteration 139/1000 | Loss: 0.00000994
Iteration 140/1000 | Loss: 0.00000994
Iteration 141/1000 | Loss: 0.00000994
Iteration 142/1000 | Loss: 0.00000993
Iteration 143/1000 | Loss: 0.00000993
Iteration 144/1000 | Loss: 0.00000993
Iteration 145/1000 | Loss: 0.00000993
Iteration 146/1000 | Loss: 0.00000993
Iteration 147/1000 | Loss: 0.00000993
Iteration 148/1000 | Loss: 0.00000993
Iteration 149/1000 | Loss: 0.00000993
Iteration 150/1000 | Loss: 0.00000992
Iteration 151/1000 | Loss: 0.00000992
Iteration 152/1000 | Loss: 0.00000992
Iteration 153/1000 | Loss: 0.00000992
Iteration 154/1000 | Loss: 0.00000992
Iteration 155/1000 | Loss: 0.00000992
Iteration 156/1000 | Loss: 0.00000992
Iteration 157/1000 | Loss: 0.00000992
Iteration 158/1000 | Loss: 0.00000992
Iteration 159/1000 | Loss: 0.00000991
Iteration 160/1000 | Loss: 0.00000991
Iteration 161/1000 | Loss: 0.00000991
Iteration 162/1000 | Loss: 0.00000991
Iteration 163/1000 | Loss: 0.00000990
Iteration 164/1000 | Loss: 0.00000990
Iteration 165/1000 | Loss: 0.00000990
Iteration 166/1000 | Loss: 0.00000990
Iteration 167/1000 | Loss: 0.00000989
Iteration 168/1000 | Loss: 0.00000989
Iteration 169/1000 | Loss: 0.00000988
Iteration 170/1000 | Loss: 0.00000988
Iteration 171/1000 | Loss: 0.00000988
Iteration 172/1000 | Loss: 0.00000988
Iteration 173/1000 | Loss: 0.00000987
Iteration 174/1000 | Loss: 0.00000987
Iteration 175/1000 | Loss: 0.00000987
Iteration 176/1000 | Loss: 0.00000986
Iteration 177/1000 | Loss: 0.00000986
Iteration 178/1000 | Loss: 0.00000986
Iteration 179/1000 | Loss: 0.00000986
Iteration 180/1000 | Loss: 0.00000985
Iteration 181/1000 | Loss: 0.00000985
Iteration 182/1000 | Loss: 0.00000985
Iteration 183/1000 | Loss: 0.00000985
Iteration 184/1000 | Loss: 0.00000984
Iteration 185/1000 | Loss: 0.00000984
Iteration 186/1000 | Loss: 0.00000984
Iteration 187/1000 | Loss: 0.00000984
Iteration 188/1000 | Loss: 0.00000984
Iteration 189/1000 | Loss: 0.00000984
Iteration 190/1000 | Loss: 0.00000984
Iteration 191/1000 | Loss: 0.00000984
Iteration 192/1000 | Loss: 0.00000983
Iteration 193/1000 | Loss: 0.00000983
Iteration 194/1000 | Loss: 0.00000983
Iteration 195/1000 | Loss: 0.00000983
Iteration 196/1000 | Loss: 0.00000983
Iteration 197/1000 | Loss: 0.00000983
Iteration 198/1000 | Loss: 0.00000983
Iteration 199/1000 | Loss: 0.00000983
Iteration 200/1000 | Loss: 0.00000983
Iteration 201/1000 | Loss: 0.00000983
Iteration 202/1000 | Loss: 0.00000982
Iteration 203/1000 | Loss: 0.00000982
Iteration 204/1000 | Loss: 0.00000982
Iteration 205/1000 | Loss: 0.00000982
Iteration 206/1000 | Loss: 0.00000982
Iteration 207/1000 | Loss: 0.00000982
Iteration 208/1000 | Loss: 0.00000982
Iteration 209/1000 | Loss: 0.00000982
Iteration 210/1000 | Loss: 0.00000982
Iteration 211/1000 | Loss: 0.00000981
Iteration 212/1000 | Loss: 0.00000981
Iteration 213/1000 | Loss: 0.00000981
Iteration 214/1000 | Loss: 0.00000981
Iteration 215/1000 | Loss: 0.00000981
Iteration 216/1000 | Loss: 0.00000981
Iteration 217/1000 | Loss: 0.00000981
Iteration 218/1000 | Loss: 0.00000981
Iteration 219/1000 | Loss: 0.00000981
Iteration 220/1000 | Loss: 0.00000981
Iteration 221/1000 | Loss: 0.00000981
Iteration 222/1000 | Loss: 0.00000981
Iteration 223/1000 | Loss: 0.00000981
Iteration 224/1000 | Loss: 0.00000981
Iteration 225/1000 | Loss: 0.00000981
Iteration 226/1000 | Loss: 0.00000981
Iteration 227/1000 | Loss: 0.00000981
Iteration 228/1000 | Loss: 0.00000981
Iteration 229/1000 | Loss: 0.00000981
Iteration 230/1000 | Loss: 0.00000981
Iteration 231/1000 | Loss: 0.00000981
Iteration 232/1000 | Loss: 0.00000981
Iteration 233/1000 | Loss: 0.00000981
Iteration 234/1000 | Loss: 0.00000981
Iteration 235/1000 | Loss: 0.00000981
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 235. Stopping optimization.
Last 5 losses: [9.812809366849251e-06, 9.812809366849251e-06, 9.812809366849251e-06, 9.812809366849251e-06, 9.812809366849251e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.812809366849251e-06

Optimization complete. Final v2v error: 2.701190948486328 mm

Highest mean error: 4.828272819519043 mm for frame 220

Lowest mean error: 2.4682838916778564 mm for frame 57

Saving results

Total time: 53.43827724456787
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831228
Iteration 2/25 | Loss: 0.00179817
Iteration 3/25 | Loss: 0.00136858
Iteration 4/25 | Loss: 0.00132907
Iteration 5/25 | Loss: 0.00132140
Iteration 6/25 | Loss: 0.00131670
Iteration 7/25 | Loss: 0.00131572
Iteration 8/25 | Loss: 0.00131167
Iteration 9/25 | Loss: 0.00129827
Iteration 10/25 | Loss: 0.00129112
Iteration 11/25 | Loss: 0.00128780
Iteration 12/25 | Loss: 0.00128751
Iteration 13/25 | Loss: 0.00128741
Iteration 14/25 | Loss: 0.00128740
Iteration 15/25 | Loss: 0.00128740
Iteration 16/25 | Loss: 0.00128740
Iteration 17/25 | Loss: 0.00128740
Iteration 18/25 | Loss: 0.00128740
Iteration 19/25 | Loss: 0.00128740
Iteration 20/25 | Loss: 0.00128740
Iteration 21/25 | Loss: 0.00128740
Iteration 22/25 | Loss: 0.00128740
Iteration 23/25 | Loss: 0.00128739
Iteration 24/25 | Loss: 0.00128739
Iteration 25/25 | Loss: 0.00128738

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.36775875
Iteration 2/25 | Loss: 0.00115639
Iteration 3/25 | Loss: 0.00106229
Iteration 4/25 | Loss: 0.00106229
Iteration 5/25 | Loss: 0.00106228
Iteration 6/25 | Loss: 0.00106228
Iteration 7/25 | Loss: 0.00106228
Iteration 8/25 | Loss: 0.00106228
Iteration 9/25 | Loss: 0.00106228
Iteration 10/25 | Loss: 0.00106228
Iteration 11/25 | Loss: 0.00106228
Iteration 12/25 | Loss: 0.00106228
Iteration 13/25 | Loss: 0.00106228
Iteration 14/25 | Loss: 0.00106228
Iteration 15/25 | Loss: 0.00106228
Iteration 16/25 | Loss: 0.00106228
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001062283175997436, 0.001062283175997436, 0.001062283175997436, 0.001062283175997436, 0.001062283175997436]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001062283175997436

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00106228
Iteration 2/1000 | Loss: 0.00025727
Iteration 3/1000 | Loss: 0.00003653
Iteration 4/1000 | Loss: 0.00002659
Iteration 5/1000 | Loss: 0.00002298
Iteration 6/1000 | Loss: 0.00002166
Iteration 7/1000 | Loss: 0.00002049
Iteration 8/1000 | Loss: 0.00001957
Iteration 9/1000 | Loss: 0.00001886
Iteration 10/1000 | Loss: 0.00001835
Iteration 11/1000 | Loss: 0.00001806
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001753
Iteration 14/1000 | Loss: 0.00001746
Iteration 15/1000 | Loss: 0.00001746
Iteration 16/1000 | Loss: 0.00001743
Iteration 17/1000 | Loss: 0.00001742
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001740
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001739
Iteration 22/1000 | Loss: 0.00001737
Iteration 23/1000 | Loss: 0.00001736
Iteration 24/1000 | Loss: 0.00001735
Iteration 25/1000 | Loss: 0.00001734
Iteration 26/1000 | Loss: 0.00001734
Iteration 27/1000 | Loss: 0.00001732
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001729
Iteration 31/1000 | Loss: 0.00001728
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001724
Iteration 35/1000 | Loss: 0.00001724
Iteration 36/1000 | Loss: 0.00001722
Iteration 37/1000 | Loss: 0.00001722
Iteration 38/1000 | Loss: 0.00001722
Iteration 39/1000 | Loss: 0.00001721
Iteration 40/1000 | Loss: 0.00001721
Iteration 41/1000 | Loss: 0.00001721
Iteration 42/1000 | Loss: 0.00001721
Iteration 43/1000 | Loss: 0.00001721
Iteration 44/1000 | Loss: 0.00001721
Iteration 45/1000 | Loss: 0.00001721
Iteration 46/1000 | Loss: 0.00001720
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001718
Iteration 51/1000 | Loss: 0.00001718
Iteration 52/1000 | Loss: 0.00001718
Iteration 53/1000 | Loss: 0.00001718
Iteration 54/1000 | Loss: 0.00001718
Iteration 55/1000 | Loss: 0.00001718
Iteration 56/1000 | Loss: 0.00001718
Iteration 57/1000 | Loss: 0.00001718
Iteration 58/1000 | Loss: 0.00001718
Iteration 59/1000 | Loss: 0.00001717
Iteration 60/1000 | Loss: 0.00001717
Iteration 61/1000 | Loss: 0.00001717
Iteration 62/1000 | Loss: 0.00001716
Iteration 63/1000 | Loss: 0.00001716
Iteration 64/1000 | Loss: 0.00001716
Iteration 65/1000 | Loss: 0.00001716
Iteration 66/1000 | Loss: 0.00001716
Iteration 67/1000 | Loss: 0.00001716
Iteration 68/1000 | Loss: 0.00001715
Iteration 69/1000 | Loss: 0.00001715
Iteration 70/1000 | Loss: 0.00001715
Iteration 71/1000 | Loss: 0.00001715
Iteration 72/1000 | Loss: 0.00001715
Iteration 73/1000 | Loss: 0.00001715
Iteration 74/1000 | Loss: 0.00001715
Iteration 75/1000 | Loss: 0.00001715
Iteration 76/1000 | Loss: 0.00001715
Iteration 77/1000 | Loss: 0.00001715
Iteration 78/1000 | Loss: 0.00001715
Iteration 79/1000 | Loss: 0.00001715
Iteration 80/1000 | Loss: 0.00001715
Iteration 81/1000 | Loss: 0.00001715
Iteration 82/1000 | Loss: 0.00001715
Iteration 83/1000 | Loss: 0.00001715
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 83. Stopping optimization.
Last 5 losses: [1.715391772449948e-05, 1.715391772449948e-05, 1.715391772449948e-05, 1.715391772449948e-05, 1.715391772449948e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.715391772449948e-05

Optimization complete. Final v2v error: 3.478224754333496 mm

Highest mean error: 3.8968355655670166 mm for frame 200

Lowest mean error: 3.0664191246032715 mm for frame 203

Saving results

Total time: 57.854949951171875
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1079/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1079.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1079
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00615947
Iteration 2/25 | Loss: 0.00147494
Iteration 3/25 | Loss: 0.00130384
Iteration 4/25 | Loss: 0.00127058
Iteration 5/25 | Loss: 0.00124627
Iteration 6/25 | Loss: 0.00124322
Iteration 7/25 | Loss: 0.00124605
Iteration 8/25 | Loss: 0.00124496
Iteration 9/25 | Loss: 0.00124215
Iteration 10/25 | Loss: 0.00124125
Iteration 11/25 | Loss: 0.00123346
Iteration 12/25 | Loss: 0.00122959
Iteration 13/25 | Loss: 0.00122699
Iteration 14/25 | Loss: 0.00122687
Iteration 15/25 | Loss: 0.00122686
Iteration 16/25 | Loss: 0.00122686
Iteration 17/25 | Loss: 0.00122686
Iteration 18/25 | Loss: 0.00122686
Iteration 19/25 | Loss: 0.00122686
Iteration 20/25 | Loss: 0.00122686
Iteration 21/25 | Loss: 0.00122686
Iteration 22/25 | Loss: 0.00122686
Iteration 23/25 | Loss: 0.00122686
Iteration 24/25 | Loss: 0.00122686
Iteration 25/25 | Loss: 0.00122685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.50816989
Iteration 2/25 | Loss: 0.00146414
Iteration 3/25 | Loss: 0.00146414
Iteration 4/25 | Loss: 0.00146414
Iteration 5/25 | Loss: 0.00146414
Iteration 6/25 | Loss: 0.00146413
Iteration 7/25 | Loss: 0.00146413
Iteration 8/25 | Loss: 0.00146413
Iteration 9/25 | Loss: 0.00146413
Iteration 10/25 | Loss: 0.00146413
Iteration 11/25 | Loss: 0.00146413
Iteration 12/25 | Loss: 0.00146413
Iteration 13/25 | Loss: 0.00146413
Iteration 14/25 | Loss: 0.00146413
Iteration 15/25 | Loss: 0.00146413
Iteration 16/25 | Loss: 0.00146413
Iteration 17/25 | Loss: 0.00146413
Iteration 18/25 | Loss: 0.00146413
Iteration 19/25 | Loss: 0.00146413
Iteration 20/25 | Loss: 0.00146413
Iteration 21/25 | Loss: 0.00146413
Iteration 22/25 | Loss: 0.00146413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0014641323359683156, 0.0014641323359683156, 0.0014641323359683156, 0.0014641323359683156, 0.0014641323359683156]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014641323359683156

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00146413
Iteration 2/1000 | Loss: 0.00003463
Iteration 3/1000 | Loss: 0.00002536
Iteration 4/1000 | Loss: 0.00002214
Iteration 5/1000 | Loss: 0.00002027
Iteration 6/1000 | Loss: 0.00001916
Iteration 7/1000 | Loss: 0.00001845
Iteration 8/1000 | Loss: 0.00001792
Iteration 9/1000 | Loss: 0.00001755
Iteration 10/1000 | Loss: 0.00001723
Iteration 11/1000 | Loss: 0.00001692
Iteration 12/1000 | Loss: 0.00001676
Iteration 13/1000 | Loss: 0.00001675
Iteration 14/1000 | Loss: 0.00001669
Iteration 15/1000 | Loss: 0.00001665
Iteration 16/1000 | Loss: 0.00001659
Iteration 17/1000 | Loss: 0.00001657
Iteration 18/1000 | Loss: 0.00001655
Iteration 19/1000 | Loss: 0.00001654
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001652
Iteration 22/1000 | Loss: 0.00001650
Iteration 23/1000 | Loss: 0.00001649
Iteration 24/1000 | Loss: 0.00001647
Iteration 25/1000 | Loss: 0.00001647
Iteration 26/1000 | Loss: 0.00001646
Iteration 27/1000 | Loss: 0.00001646
Iteration 28/1000 | Loss: 0.00001643
Iteration 29/1000 | Loss: 0.00001642
Iteration 30/1000 | Loss: 0.00001641
Iteration 31/1000 | Loss: 0.00001640
Iteration 32/1000 | Loss: 0.00001640
Iteration 33/1000 | Loss: 0.00001639
Iteration 34/1000 | Loss: 0.00001639
Iteration 35/1000 | Loss: 0.00001638
Iteration 36/1000 | Loss: 0.00001638
Iteration 37/1000 | Loss: 0.00001637
Iteration 38/1000 | Loss: 0.00001636
Iteration 39/1000 | Loss: 0.00001636
Iteration 40/1000 | Loss: 0.00001636
Iteration 41/1000 | Loss: 0.00001635
Iteration 42/1000 | Loss: 0.00001635
Iteration 43/1000 | Loss: 0.00001634
Iteration 44/1000 | Loss: 0.00001634
Iteration 45/1000 | Loss: 0.00001634
Iteration 46/1000 | Loss: 0.00001634
Iteration 47/1000 | Loss: 0.00001633
Iteration 48/1000 | Loss: 0.00001633
Iteration 49/1000 | Loss: 0.00001633
Iteration 50/1000 | Loss: 0.00001632
Iteration 51/1000 | Loss: 0.00001632
Iteration 52/1000 | Loss: 0.00001632
Iteration 53/1000 | Loss: 0.00001631
Iteration 54/1000 | Loss: 0.00001631
Iteration 55/1000 | Loss: 0.00001631
Iteration 56/1000 | Loss: 0.00001631
Iteration 57/1000 | Loss: 0.00001631
Iteration 58/1000 | Loss: 0.00001631
Iteration 59/1000 | Loss: 0.00001630
Iteration 60/1000 | Loss: 0.00001630
Iteration 61/1000 | Loss: 0.00001630
Iteration 62/1000 | Loss: 0.00001630
Iteration 63/1000 | Loss: 0.00001630
Iteration 64/1000 | Loss: 0.00001630
Iteration 65/1000 | Loss: 0.00001629
Iteration 66/1000 | Loss: 0.00001629
Iteration 67/1000 | Loss: 0.00001629
Iteration 68/1000 | Loss: 0.00001628
Iteration 69/1000 | Loss: 0.00001628
Iteration 70/1000 | Loss: 0.00001627
Iteration 71/1000 | Loss: 0.00001627
Iteration 72/1000 | Loss: 0.00001627
Iteration 73/1000 | Loss: 0.00001627
Iteration 74/1000 | Loss: 0.00001626
Iteration 75/1000 | Loss: 0.00001626
Iteration 76/1000 | Loss: 0.00001626
Iteration 77/1000 | Loss: 0.00001626
Iteration 78/1000 | Loss: 0.00001626
Iteration 79/1000 | Loss: 0.00001626
Iteration 80/1000 | Loss: 0.00001625
Iteration 81/1000 | Loss: 0.00001625
Iteration 82/1000 | Loss: 0.00001625
Iteration 83/1000 | Loss: 0.00001625
Iteration 84/1000 | Loss: 0.00001625
Iteration 85/1000 | Loss: 0.00001625
Iteration 86/1000 | Loss: 0.00001625
Iteration 87/1000 | Loss: 0.00001625
Iteration 88/1000 | Loss: 0.00001624
Iteration 89/1000 | Loss: 0.00001624
Iteration 90/1000 | Loss: 0.00001624
Iteration 91/1000 | Loss: 0.00001624
Iteration 92/1000 | Loss: 0.00001623
Iteration 93/1000 | Loss: 0.00001623
Iteration 94/1000 | Loss: 0.00001623
Iteration 95/1000 | Loss: 0.00001623
Iteration 96/1000 | Loss: 0.00001623
Iteration 97/1000 | Loss: 0.00001623
Iteration 98/1000 | Loss: 0.00001623
Iteration 99/1000 | Loss: 0.00001623
Iteration 100/1000 | Loss: 0.00001622
Iteration 101/1000 | Loss: 0.00001622
Iteration 102/1000 | Loss: 0.00001622
Iteration 103/1000 | Loss: 0.00001622
Iteration 104/1000 | Loss: 0.00001621
Iteration 105/1000 | Loss: 0.00001621
Iteration 106/1000 | Loss: 0.00001621
Iteration 107/1000 | Loss: 0.00001621
Iteration 108/1000 | Loss: 0.00001621
Iteration 109/1000 | Loss: 0.00001620
Iteration 110/1000 | Loss: 0.00001620
Iteration 111/1000 | Loss: 0.00001620
Iteration 112/1000 | Loss: 0.00001620
Iteration 113/1000 | Loss: 0.00001620
Iteration 114/1000 | Loss: 0.00001620
Iteration 115/1000 | Loss: 0.00001620
Iteration 116/1000 | Loss: 0.00001620
Iteration 117/1000 | Loss: 0.00001620
Iteration 118/1000 | Loss: 0.00001619
Iteration 119/1000 | Loss: 0.00001619
Iteration 120/1000 | Loss: 0.00001619
Iteration 121/1000 | Loss: 0.00001619
Iteration 122/1000 | Loss: 0.00001619
Iteration 123/1000 | Loss: 0.00001619
Iteration 124/1000 | Loss: 0.00001618
Iteration 125/1000 | Loss: 0.00001618
Iteration 126/1000 | Loss: 0.00001618
Iteration 127/1000 | Loss: 0.00001618
Iteration 128/1000 | Loss: 0.00001618
Iteration 129/1000 | Loss: 0.00001618
Iteration 130/1000 | Loss: 0.00001618
Iteration 131/1000 | Loss: 0.00001618
Iteration 132/1000 | Loss: 0.00001618
Iteration 133/1000 | Loss: 0.00001618
Iteration 134/1000 | Loss: 0.00001618
Iteration 135/1000 | Loss: 0.00001617
Iteration 136/1000 | Loss: 0.00001617
Iteration 137/1000 | Loss: 0.00001617
Iteration 138/1000 | Loss: 0.00001617
Iteration 139/1000 | Loss: 0.00001617
Iteration 140/1000 | Loss: 0.00001617
Iteration 141/1000 | Loss: 0.00001617
Iteration 142/1000 | Loss: 0.00001617
Iteration 143/1000 | Loss: 0.00001617
Iteration 144/1000 | Loss: 0.00001617
Iteration 145/1000 | Loss: 0.00001617
Iteration 146/1000 | Loss: 0.00001616
Iteration 147/1000 | Loss: 0.00001616
Iteration 148/1000 | Loss: 0.00001616
Iteration 149/1000 | Loss: 0.00001616
Iteration 150/1000 | Loss: 0.00001616
Iteration 151/1000 | Loss: 0.00001616
Iteration 152/1000 | Loss: 0.00001616
Iteration 153/1000 | Loss: 0.00001616
Iteration 154/1000 | Loss: 0.00001616
Iteration 155/1000 | Loss: 0.00001616
Iteration 156/1000 | Loss: 0.00001616
Iteration 157/1000 | Loss: 0.00001616
Iteration 158/1000 | Loss: 0.00001616
Iteration 159/1000 | Loss: 0.00001616
Iteration 160/1000 | Loss: 0.00001616
Iteration 161/1000 | Loss: 0.00001616
Iteration 162/1000 | Loss: 0.00001616
Iteration 163/1000 | Loss: 0.00001616
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.616240115254186e-05, 1.616240115254186e-05, 1.616240115254186e-05, 1.616240115254186e-05, 1.616240115254186e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.616240115254186e-05

Optimization complete. Final v2v error: 3.334927797317505 mm

Highest mean error: 4.25450325012207 mm for frame 127

Lowest mean error: 2.7912757396698 mm for frame 48

Saving results

Total time: 63.14593029022217
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00917479
Iteration 2/25 | Loss: 0.00175769
Iteration 3/25 | Loss: 0.00138641
Iteration 4/25 | Loss: 0.00132400
Iteration 5/25 | Loss: 0.00130852
Iteration 6/25 | Loss: 0.00130440
Iteration 7/25 | Loss: 0.00130281
Iteration 8/25 | Loss: 0.00130210
Iteration 9/25 | Loss: 0.00130169
Iteration 10/25 | Loss: 0.00130128
Iteration 11/25 | Loss: 0.00130329
Iteration 12/25 | Loss: 0.00129964
Iteration 13/25 | Loss: 0.00129410
Iteration 14/25 | Loss: 0.00129226
Iteration 15/25 | Loss: 0.00129172
Iteration 16/25 | Loss: 0.00129157
Iteration 17/25 | Loss: 0.00129152
Iteration 18/25 | Loss: 0.00129152
Iteration 19/25 | Loss: 0.00129151
Iteration 20/25 | Loss: 0.00129151
Iteration 21/25 | Loss: 0.00129151
Iteration 22/25 | Loss: 0.00129151
Iteration 23/25 | Loss: 0.00129151
Iteration 24/25 | Loss: 0.00129151
Iteration 25/25 | Loss: 0.00129151

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27431917
Iteration 2/25 | Loss: 0.00244578
Iteration 3/25 | Loss: 0.00244576
Iteration 4/25 | Loss: 0.00244576
Iteration 5/25 | Loss: 0.00244576
Iteration 6/25 | Loss: 0.00244576
Iteration 7/25 | Loss: 0.00244576
Iteration 8/25 | Loss: 0.00244576
Iteration 9/25 | Loss: 0.00244576
Iteration 10/25 | Loss: 0.00244576
Iteration 11/25 | Loss: 0.00244576
Iteration 12/25 | Loss: 0.00244576
Iteration 13/25 | Loss: 0.00244576
Iteration 14/25 | Loss: 0.00244576
Iteration 15/25 | Loss: 0.00244576
Iteration 16/25 | Loss: 0.00244576
Iteration 17/25 | Loss: 0.00244576
Iteration 18/25 | Loss: 0.00244576
Iteration 19/25 | Loss: 0.00244576
Iteration 20/25 | Loss: 0.00244576
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0024457573890686035, 0.0024457573890686035, 0.0024457573890686035, 0.0024457573890686035, 0.0024457573890686035]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0024457573890686035

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00244576
Iteration 2/1000 | Loss: 0.00018380
Iteration 3/1000 | Loss: 0.00010569
Iteration 4/1000 | Loss: 0.00008995
Iteration 5/1000 | Loss: 0.00008148
Iteration 6/1000 | Loss: 0.00007795
Iteration 7/1000 | Loss: 0.00007509
Iteration 8/1000 | Loss: 0.00007310
Iteration 9/1000 | Loss: 0.00007114
Iteration 10/1000 | Loss: 0.00007025
Iteration 11/1000 | Loss: 0.00006919
Iteration 12/1000 | Loss: 0.00006845
Iteration 13/1000 | Loss: 0.00006778
Iteration 14/1000 | Loss: 0.00064844
Iteration 15/1000 | Loss: 0.00592604
Iteration 16/1000 | Loss: 0.00262725
Iteration 17/1000 | Loss: 0.00010528
Iteration 18/1000 | Loss: 0.00006633
Iteration 19/1000 | Loss: 0.00005002
Iteration 20/1000 | Loss: 0.00004392
Iteration 21/1000 | Loss: 0.00003678
Iteration 22/1000 | Loss: 0.00003208
Iteration 23/1000 | Loss: 0.00002946
Iteration 24/1000 | Loss: 0.00002771
Iteration 25/1000 | Loss: 0.00002630
Iteration 26/1000 | Loss: 0.00002530
Iteration 27/1000 | Loss: 0.00002431
Iteration 28/1000 | Loss: 0.00002370
Iteration 29/1000 | Loss: 0.00002322
Iteration 30/1000 | Loss: 0.00002293
Iteration 31/1000 | Loss: 0.00002274
Iteration 32/1000 | Loss: 0.00002267
Iteration 33/1000 | Loss: 0.00002253
Iteration 34/1000 | Loss: 0.00002244
Iteration 35/1000 | Loss: 0.00002242
Iteration 36/1000 | Loss: 0.00002242
Iteration 37/1000 | Loss: 0.00002242
Iteration 38/1000 | Loss: 0.00002241
Iteration 39/1000 | Loss: 0.00002241
Iteration 40/1000 | Loss: 0.00002239
Iteration 41/1000 | Loss: 0.00002237
Iteration 42/1000 | Loss: 0.00002236
Iteration 43/1000 | Loss: 0.00002236
Iteration 44/1000 | Loss: 0.00002235
Iteration 45/1000 | Loss: 0.00002234
Iteration 46/1000 | Loss: 0.00002234
Iteration 47/1000 | Loss: 0.00002233
Iteration 48/1000 | Loss: 0.00002230
Iteration 49/1000 | Loss: 0.00002230
Iteration 50/1000 | Loss: 0.00002229
Iteration 51/1000 | Loss: 0.00002228
Iteration 52/1000 | Loss: 0.00002226
Iteration 53/1000 | Loss: 0.00002226
Iteration 54/1000 | Loss: 0.00002225
Iteration 55/1000 | Loss: 0.00002225
Iteration 56/1000 | Loss: 0.00002224
Iteration 57/1000 | Loss: 0.00002224
Iteration 58/1000 | Loss: 0.00002224
Iteration 59/1000 | Loss: 0.00002224
Iteration 60/1000 | Loss: 0.00002224
Iteration 61/1000 | Loss: 0.00002224
Iteration 62/1000 | Loss: 0.00002224
Iteration 63/1000 | Loss: 0.00002224
Iteration 64/1000 | Loss: 0.00002224
Iteration 65/1000 | Loss: 0.00002224
Iteration 66/1000 | Loss: 0.00002223
Iteration 67/1000 | Loss: 0.00002223
Iteration 68/1000 | Loss: 0.00002223
Iteration 69/1000 | Loss: 0.00002223
Iteration 70/1000 | Loss: 0.00002222
Iteration 71/1000 | Loss: 0.00002222
Iteration 72/1000 | Loss: 0.00002222
Iteration 73/1000 | Loss: 0.00002221
Iteration 74/1000 | Loss: 0.00002221
Iteration 75/1000 | Loss: 0.00002221
Iteration 76/1000 | Loss: 0.00002220
Iteration 77/1000 | Loss: 0.00002220
Iteration 78/1000 | Loss: 0.00002220
Iteration 79/1000 | Loss: 0.00002220
Iteration 80/1000 | Loss: 0.00002219
Iteration 81/1000 | Loss: 0.00002219
Iteration 82/1000 | Loss: 0.00002219
Iteration 83/1000 | Loss: 0.00002219
Iteration 84/1000 | Loss: 0.00002218
Iteration 85/1000 | Loss: 0.00002218
Iteration 86/1000 | Loss: 0.00002217
Iteration 87/1000 | Loss: 0.00002217
Iteration 88/1000 | Loss: 0.00002217
Iteration 89/1000 | Loss: 0.00002216
Iteration 90/1000 | Loss: 0.00002216
Iteration 91/1000 | Loss: 0.00002216
Iteration 92/1000 | Loss: 0.00002216
Iteration 93/1000 | Loss: 0.00002216
Iteration 94/1000 | Loss: 0.00002216
Iteration 95/1000 | Loss: 0.00002216
Iteration 96/1000 | Loss: 0.00002216
Iteration 97/1000 | Loss: 0.00002216
Iteration 98/1000 | Loss: 0.00002216
Iteration 99/1000 | Loss: 0.00002216
Iteration 100/1000 | Loss: 0.00002215
Iteration 101/1000 | Loss: 0.00002215
Iteration 102/1000 | Loss: 0.00002215
Iteration 103/1000 | Loss: 0.00002215
Iteration 104/1000 | Loss: 0.00002213
Iteration 105/1000 | Loss: 0.00002213
Iteration 106/1000 | Loss: 0.00002213
Iteration 107/1000 | Loss: 0.00002213
Iteration 108/1000 | Loss: 0.00002213
Iteration 109/1000 | Loss: 0.00002212
Iteration 110/1000 | Loss: 0.00002212
Iteration 111/1000 | Loss: 0.00002212
Iteration 112/1000 | Loss: 0.00002212
Iteration 113/1000 | Loss: 0.00002212
Iteration 114/1000 | Loss: 0.00002212
Iteration 115/1000 | Loss: 0.00002212
Iteration 116/1000 | Loss: 0.00002211
Iteration 117/1000 | Loss: 0.00002211
Iteration 118/1000 | Loss: 0.00002211
Iteration 119/1000 | Loss: 0.00002211
Iteration 120/1000 | Loss: 0.00002211
Iteration 121/1000 | Loss: 0.00002210
Iteration 122/1000 | Loss: 0.00002210
Iteration 123/1000 | Loss: 0.00002210
Iteration 124/1000 | Loss: 0.00002210
Iteration 125/1000 | Loss: 0.00002210
Iteration 126/1000 | Loss: 0.00002209
Iteration 127/1000 | Loss: 0.00002209
Iteration 128/1000 | Loss: 0.00002209
Iteration 129/1000 | Loss: 0.00002209
Iteration 130/1000 | Loss: 0.00002208
Iteration 131/1000 | Loss: 0.00002208
Iteration 132/1000 | Loss: 0.00002208
Iteration 133/1000 | Loss: 0.00002208
Iteration 134/1000 | Loss: 0.00002208
Iteration 135/1000 | Loss: 0.00002208
Iteration 136/1000 | Loss: 0.00002207
Iteration 137/1000 | Loss: 0.00002207
Iteration 138/1000 | Loss: 0.00002207
Iteration 139/1000 | Loss: 0.00002207
Iteration 140/1000 | Loss: 0.00002207
Iteration 141/1000 | Loss: 0.00002207
Iteration 142/1000 | Loss: 0.00002207
Iteration 143/1000 | Loss: 0.00002207
Iteration 144/1000 | Loss: 0.00002207
Iteration 145/1000 | Loss: 0.00002206
Iteration 146/1000 | Loss: 0.00002206
Iteration 147/1000 | Loss: 0.00002206
Iteration 148/1000 | Loss: 0.00002206
Iteration 149/1000 | Loss: 0.00002206
Iteration 150/1000 | Loss: 0.00002206
Iteration 151/1000 | Loss: 0.00002206
Iteration 152/1000 | Loss: 0.00002206
Iteration 153/1000 | Loss: 0.00002206
Iteration 154/1000 | Loss: 0.00002205
Iteration 155/1000 | Loss: 0.00002205
Iteration 156/1000 | Loss: 0.00002205
Iteration 157/1000 | Loss: 0.00002205
Iteration 158/1000 | Loss: 0.00002205
Iteration 159/1000 | Loss: 0.00002205
Iteration 160/1000 | Loss: 0.00002205
Iteration 161/1000 | Loss: 0.00002205
Iteration 162/1000 | Loss: 0.00002205
Iteration 163/1000 | Loss: 0.00002205
Iteration 164/1000 | Loss: 0.00002205
Iteration 165/1000 | Loss: 0.00002204
Iteration 166/1000 | Loss: 0.00002204
Iteration 167/1000 | Loss: 0.00002204
Iteration 168/1000 | Loss: 0.00002204
Iteration 169/1000 | Loss: 0.00002204
Iteration 170/1000 | Loss: 0.00002204
Iteration 171/1000 | Loss: 0.00002204
Iteration 172/1000 | Loss: 0.00002203
Iteration 173/1000 | Loss: 0.00002203
Iteration 174/1000 | Loss: 0.00002203
Iteration 175/1000 | Loss: 0.00002203
Iteration 176/1000 | Loss: 0.00002203
Iteration 177/1000 | Loss: 0.00002203
Iteration 178/1000 | Loss: 0.00002203
Iteration 179/1000 | Loss: 0.00002203
Iteration 180/1000 | Loss: 0.00002203
Iteration 181/1000 | Loss: 0.00002203
Iteration 182/1000 | Loss: 0.00002203
Iteration 183/1000 | Loss: 0.00002203
Iteration 184/1000 | Loss: 0.00002202
Iteration 185/1000 | Loss: 0.00002202
Iteration 186/1000 | Loss: 0.00002202
Iteration 187/1000 | Loss: 0.00002202
Iteration 188/1000 | Loss: 0.00002201
Iteration 189/1000 | Loss: 0.00002201
Iteration 190/1000 | Loss: 0.00002201
Iteration 191/1000 | Loss: 0.00002201
Iteration 192/1000 | Loss: 0.00002201
Iteration 193/1000 | Loss: 0.00002200
Iteration 194/1000 | Loss: 0.00002200
Iteration 195/1000 | Loss: 0.00002200
Iteration 196/1000 | Loss: 0.00002200
Iteration 197/1000 | Loss: 0.00002200
Iteration 198/1000 | Loss: 0.00002200
Iteration 199/1000 | Loss: 0.00002200
Iteration 200/1000 | Loss: 0.00002200
Iteration 201/1000 | Loss: 0.00002200
Iteration 202/1000 | Loss: 0.00002200
Iteration 203/1000 | Loss: 0.00002200
Iteration 204/1000 | Loss: 0.00002200
Iteration 205/1000 | Loss: 0.00002200
Iteration 206/1000 | Loss: 0.00002200
Iteration 207/1000 | Loss: 0.00002200
Iteration 208/1000 | Loss: 0.00002200
Iteration 209/1000 | Loss: 0.00002200
Iteration 210/1000 | Loss: 0.00002200
Iteration 211/1000 | Loss: 0.00002200
Iteration 212/1000 | Loss: 0.00002200
Iteration 213/1000 | Loss: 0.00002200
Iteration 214/1000 | Loss: 0.00002199
Iteration 215/1000 | Loss: 0.00002199
Iteration 216/1000 | Loss: 0.00002199
Iteration 217/1000 | Loss: 0.00002199
Iteration 218/1000 | Loss: 0.00002199
Iteration 219/1000 | Loss: 0.00002199
Iteration 220/1000 | Loss: 0.00002199
Iteration 221/1000 | Loss: 0.00002199
Iteration 222/1000 | Loss: 0.00002199
Iteration 223/1000 | Loss: 0.00002199
Iteration 224/1000 | Loss: 0.00002199
Iteration 225/1000 | Loss: 0.00002199
Iteration 226/1000 | Loss: 0.00002199
Iteration 227/1000 | Loss: 0.00002199
Iteration 228/1000 | Loss: 0.00002199
Iteration 229/1000 | Loss: 0.00002199
Iteration 230/1000 | Loss: 0.00002199
Iteration 231/1000 | Loss: 0.00002199
Iteration 232/1000 | Loss: 0.00002199
Iteration 233/1000 | Loss: 0.00002199
Iteration 234/1000 | Loss: 0.00002199
Iteration 235/1000 | Loss: 0.00002199
Iteration 236/1000 | Loss: 0.00002199
Iteration 237/1000 | Loss: 0.00002199
Iteration 238/1000 | Loss: 0.00002199
Iteration 239/1000 | Loss: 0.00002199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 239. Stopping optimization.
Last 5 losses: [2.199411937908735e-05, 2.199411937908735e-05, 2.199411937908735e-05, 2.199411937908735e-05, 2.199411937908735e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.199411937908735e-05

Optimization complete. Final v2v error: 3.9130611419677734 mm

Highest mean error: 4.263334274291992 mm for frame 7

Lowest mean error: 3.6799428462982178 mm for frame 180

Saving results

Total time: 93.33761429786682
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831618
Iteration 2/25 | Loss: 0.00152560
Iteration 3/25 | Loss: 0.00129357
Iteration 4/25 | Loss: 0.00123650
Iteration 5/25 | Loss: 0.00124103
Iteration 6/25 | Loss: 0.00124132
Iteration 7/25 | Loss: 0.00123308
Iteration 8/25 | Loss: 0.00122422
Iteration 9/25 | Loss: 0.00122026
Iteration 10/25 | Loss: 0.00121461
Iteration 11/25 | Loss: 0.00120913
Iteration 12/25 | Loss: 0.00120648
Iteration 13/25 | Loss: 0.00120439
Iteration 14/25 | Loss: 0.00120411
Iteration 15/25 | Loss: 0.00120388
Iteration 16/25 | Loss: 0.00120614
Iteration 17/25 | Loss: 0.00120337
Iteration 18/25 | Loss: 0.00120335
Iteration 19/25 | Loss: 0.00120302
Iteration 20/25 | Loss: 0.00120320
Iteration 21/25 | Loss: 0.00120257
Iteration 22/25 | Loss: 0.00120358
Iteration 23/25 | Loss: 0.00120295
Iteration 24/25 | Loss: 0.00120231
Iteration 25/25 | Loss: 0.00120358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.87049198
Iteration 2/25 | Loss: 0.00135902
Iteration 3/25 | Loss: 0.00129470
Iteration 4/25 | Loss: 0.00129470
Iteration 5/25 | Loss: 0.00129470
Iteration 6/25 | Loss: 0.00129470
Iteration 7/25 | Loss: 0.00129470
Iteration 8/25 | Loss: 0.00129469
Iteration 9/25 | Loss: 0.00129469
Iteration 10/25 | Loss: 0.00129469
Iteration 11/25 | Loss: 0.00129469
Iteration 12/25 | Loss: 0.00129469
Iteration 13/25 | Loss: 0.00129469
Iteration 14/25 | Loss: 0.00129469
Iteration 15/25 | Loss: 0.00129469
Iteration 16/25 | Loss: 0.00129469
Iteration 17/25 | Loss: 0.00129469
Iteration 18/25 | Loss: 0.00129469
Iteration 19/25 | Loss: 0.00129469
Iteration 20/25 | Loss: 0.00129469
Iteration 21/25 | Loss: 0.00129469
Iteration 22/25 | Loss: 0.00129469
Iteration 23/25 | Loss: 0.00129469
Iteration 24/25 | Loss: 0.00129469
Iteration 25/25 | Loss: 0.00129469

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129469
Iteration 2/1000 | Loss: 0.00011611
Iteration 3/1000 | Loss: 0.00015167
Iteration 4/1000 | Loss: 0.00016269
Iteration 5/1000 | Loss: 0.00017088
Iteration 6/1000 | Loss: 0.00009971
Iteration 7/1000 | Loss: 0.00015779
Iteration 8/1000 | Loss: 0.00007370
Iteration 9/1000 | Loss: 0.00019404
Iteration 10/1000 | Loss: 0.00007912
Iteration 11/1000 | Loss: 0.00010277
Iteration 12/1000 | Loss: 0.00015645
Iteration 13/1000 | Loss: 0.00011934
Iteration 14/1000 | Loss: 0.00009364
Iteration 15/1000 | Loss: 0.00007147
Iteration 16/1000 | Loss: 0.00009148
Iteration 17/1000 | Loss: 0.00007074
Iteration 18/1000 | Loss: 0.00010319
Iteration 19/1000 | Loss: 0.00011269
Iteration 20/1000 | Loss: 0.00001902
Iteration 21/1000 | Loss: 0.00001496
Iteration 22/1000 | Loss: 0.00001389
Iteration 23/1000 | Loss: 0.00001341
Iteration 24/1000 | Loss: 0.00001310
Iteration 25/1000 | Loss: 0.00001290
Iteration 26/1000 | Loss: 0.00001289
Iteration 27/1000 | Loss: 0.00001268
Iteration 28/1000 | Loss: 0.00001265
Iteration 29/1000 | Loss: 0.00001251
Iteration 30/1000 | Loss: 0.00001230
Iteration 31/1000 | Loss: 0.00001211
Iteration 32/1000 | Loss: 0.00001210
Iteration 33/1000 | Loss: 0.00001199
Iteration 34/1000 | Loss: 0.00001195
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001189
Iteration 38/1000 | Loss: 0.00001185
Iteration 39/1000 | Loss: 0.00001182
Iteration 40/1000 | Loss: 0.00001182
Iteration 41/1000 | Loss: 0.00001181
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001172
Iteration 44/1000 | Loss: 0.00001170
Iteration 45/1000 | Loss: 0.00001169
Iteration 46/1000 | Loss: 0.00001167
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001165
Iteration 49/1000 | Loss: 0.00001165
Iteration 50/1000 | Loss: 0.00001163
Iteration 51/1000 | Loss: 0.00001163
Iteration 52/1000 | Loss: 0.00001162
Iteration 53/1000 | Loss: 0.00001162
Iteration 54/1000 | Loss: 0.00001161
Iteration 55/1000 | Loss: 0.00001161
Iteration 56/1000 | Loss: 0.00001160
Iteration 57/1000 | Loss: 0.00001159
Iteration 58/1000 | Loss: 0.00001159
Iteration 59/1000 | Loss: 0.00001159
Iteration 60/1000 | Loss: 0.00001159
Iteration 61/1000 | Loss: 0.00001159
Iteration 62/1000 | Loss: 0.00001159
Iteration 63/1000 | Loss: 0.00001159
Iteration 64/1000 | Loss: 0.00001159
Iteration 65/1000 | Loss: 0.00001159
Iteration 66/1000 | Loss: 0.00001158
Iteration 67/1000 | Loss: 0.00001158
Iteration 68/1000 | Loss: 0.00001158
Iteration 69/1000 | Loss: 0.00001157
Iteration 70/1000 | Loss: 0.00001157
Iteration 71/1000 | Loss: 0.00001156
Iteration 72/1000 | Loss: 0.00001156
Iteration 73/1000 | Loss: 0.00001156
Iteration 74/1000 | Loss: 0.00001155
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001155
Iteration 78/1000 | Loss: 0.00001154
Iteration 79/1000 | Loss: 0.00001154
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001153
Iteration 87/1000 | Loss: 0.00001153
Iteration 88/1000 | Loss: 0.00001153
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001153
Iteration 95/1000 | Loss: 0.00001153
Iteration 96/1000 | Loss: 0.00001153
Iteration 97/1000 | Loss: 0.00001153
Iteration 98/1000 | Loss: 0.00001153
Iteration 99/1000 | Loss: 0.00001153
Iteration 100/1000 | Loss: 0.00001153
Iteration 101/1000 | Loss: 0.00001153
Iteration 102/1000 | Loss: 0.00001153
Iteration 103/1000 | Loss: 0.00001153
Iteration 104/1000 | Loss: 0.00001153
Iteration 105/1000 | Loss: 0.00001153
Iteration 106/1000 | Loss: 0.00001153
Iteration 107/1000 | Loss: 0.00001153
Iteration 108/1000 | Loss: 0.00001153
Iteration 109/1000 | Loss: 0.00001153
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.1529527910170145e-05, 1.1529527910170145e-05, 1.1529527910170145e-05, 1.1529527910170145e-05, 1.1529527910170145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1529527910170145e-05

Optimization complete. Final v2v error: 2.9223434925079346 mm

Highest mean error: 4.161563873291016 mm for frame 133

Lowest mean error: 2.6135807037353516 mm for frame 99

Saving results

Total time: 111.03534626960754
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01022432
Iteration 2/25 | Loss: 0.00275444
Iteration 3/25 | Loss: 0.00179050
Iteration 4/25 | Loss: 0.00163603
Iteration 5/25 | Loss: 0.00155196
Iteration 6/25 | Loss: 0.00169613
Iteration 7/25 | Loss: 0.00144732
Iteration 8/25 | Loss: 0.00136063
Iteration 9/25 | Loss: 0.00129956
Iteration 10/25 | Loss: 0.00124926
Iteration 11/25 | Loss: 0.00123440
Iteration 12/25 | Loss: 0.00121433
Iteration 13/25 | Loss: 0.00120697
Iteration 14/25 | Loss: 0.00120267
Iteration 15/25 | Loss: 0.00120075
Iteration 16/25 | Loss: 0.00120186
Iteration 17/25 | Loss: 0.00119393
Iteration 18/25 | Loss: 0.00119565
Iteration 19/25 | Loss: 0.00119519
Iteration 20/25 | Loss: 0.00119347
Iteration 21/25 | Loss: 0.00119346
Iteration 22/25 | Loss: 0.00119346
Iteration 23/25 | Loss: 0.00119345
Iteration 24/25 | Loss: 0.00119345
Iteration 25/25 | Loss: 0.00119345

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.89100468
Iteration 2/25 | Loss: 0.00124882
Iteration 3/25 | Loss: 0.00114806
Iteration 4/25 | Loss: 0.00114806
Iteration 5/25 | Loss: 0.00114806
Iteration 6/25 | Loss: 0.00114806
Iteration 7/25 | Loss: 0.00114806
Iteration 8/25 | Loss: 0.00114806
Iteration 9/25 | Loss: 0.00114806
Iteration 10/25 | Loss: 0.00114806
Iteration 11/25 | Loss: 0.00114806
Iteration 12/25 | Loss: 0.00114806
Iteration 13/25 | Loss: 0.00114806
Iteration 14/25 | Loss: 0.00114806
Iteration 15/25 | Loss: 0.00114806
Iteration 16/25 | Loss: 0.00114806
Iteration 17/25 | Loss: 0.00114806
Iteration 18/25 | Loss: 0.00114806
Iteration 19/25 | Loss: 0.00114806
Iteration 20/25 | Loss: 0.00114806
Iteration 21/25 | Loss: 0.00114806
Iteration 22/25 | Loss: 0.00114806
Iteration 23/25 | Loss: 0.00114806
Iteration 24/25 | Loss: 0.00114806
Iteration 25/25 | Loss: 0.00114806

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114806
Iteration 2/1000 | Loss: 0.00003573
Iteration 3/1000 | Loss: 0.00047071
Iteration 4/1000 | Loss: 0.00224757
Iteration 5/1000 | Loss: 0.00003117
Iteration 6/1000 | Loss: 0.00008637
Iteration 7/1000 | Loss: 0.00012772
Iteration 8/1000 | Loss: 0.00002453
Iteration 9/1000 | Loss: 0.00018812
Iteration 10/1000 | Loss: 0.00007339
Iteration 11/1000 | Loss: 0.00001785
Iteration 12/1000 | Loss: 0.00005880
Iteration 13/1000 | Loss: 0.00003439
Iteration 14/1000 | Loss: 0.00008223
Iteration 15/1000 | Loss: 0.00006277
Iteration 16/1000 | Loss: 0.00071122
Iteration 17/1000 | Loss: 0.00205433
Iteration 18/1000 | Loss: 0.00033394
Iteration 19/1000 | Loss: 0.00027670
Iteration 20/1000 | Loss: 0.00005250
Iteration 21/1000 | Loss: 0.00054346
Iteration 22/1000 | Loss: 0.00125212
Iteration 23/1000 | Loss: 0.00076310
Iteration 24/1000 | Loss: 0.00003335
Iteration 25/1000 | Loss: 0.00012558
Iteration 26/1000 | Loss: 0.00002204
Iteration 27/1000 | Loss: 0.00005947
Iteration 28/1000 | Loss: 0.00017073
Iteration 29/1000 | Loss: 0.00002403
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00008500
Iteration 32/1000 | Loss: 0.00011247
Iteration 33/1000 | Loss: 0.00017317
Iteration 34/1000 | Loss: 0.00009358
Iteration 35/1000 | Loss: 0.00008002
Iteration 36/1000 | Loss: 0.00011628
Iteration 37/1000 | Loss: 0.00001555
Iteration 38/1000 | Loss: 0.00004917
Iteration 39/1000 | Loss: 0.00001540
Iteration 40/1000 | Loss: 0.00002942
Iteration 41/1000 | Loss: 0.00001426
Iteration 42/1000 | Loss: 0.00006285
Iteration 43/1000 | Loss: 0.00009022
Iteration 44/1000 | Loss: 0.00004860
Iteration 45/1000 | Loss: 0.00005331
Iteration 46/1000 | Loss: 0.00001471
Iteration 47/1000 | Loss: 0.00003244
Iteration 48/1000 | Loss: 0.00001868
Iteration 49/1000 | Loss: 0.00001398
Iteration 50/1000 | Loss: 0.00001710
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001467
Iteration 53/1000 | Loss: 0.00002011
Iteration 54/1000 | Loss: 0.00001394
Iteration 55/1000 | Loss: 0.00001394
Iteration 56/1000 | Loss: 0.00001395
Iteration 57/1000 | Loss: 0.00005901
Iteration 58/1000 | Loss: 0.00003432
Iteration 59/1000 | Loss: 0.00005703
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001373
Iteration 62/1000 | Loss: 0.00001373
Iteration 63/1000 | Loss: 0.00001373
Iteration 64/1000 | Loss: 0.00001373
Iteration 65/1000 | Loss: 0.00001372
Iteration 66/1000 | Loss: 0.00001372
Iteration 67/1000 | Loss: 0.00001372
Iteration 68/1000 | Loss: 0.00001372
Iteration 69/1000 | Loss: 0.00001371
Iteration 70/1000 | Loss: 0.00001371
Iteration 71/1000 | Loss: 0.00001370
Iteration 72/1000 | Loss: 0.00001370
Iteration 73/1000 | Loss: 0.00001370
Iteration 74/1000 | Loss: 0.00001370
Iteration 75/1000 | Loss: 0.00001370
Iteration 76/1000 | Loss: 0.00001370
Iteration 77/1000 | Loss: 0.00001370
Iteration 78/1000 | Loss: 0.00001370
Iteration 79/1000 | Loss: 0.00001370
Iteration 80/1000 | Loss: 0.00001370
Iteration 81/1000 | Loss: 0.00001370
Iteration 82/1000 | Loss: 0.00001370
Iteration 83/1000 | Loss: 0.00001369
Iteration 84/1000 | Loss: 0.00001369
Iteration 85/1000 | Loss: 0.00001369
Iteration 86/1000 | Loss: 0.00001368
Iteration 87/1000 | Loss: 0.00001368
Iteration 88/1000 | Loss: 0.00001368
Iteration 89/1000 | Loss: 0.00001367
Iteration 90/1000 | Loss: 0.00001367
Iteration 91/1000 | Loss: 0.00001367
Iteration 92/1000 | Loss: 0.00001367
Iteration 93/1000 | Loss: 0.00001367
Iteration 94/1000 | Loss: 0.00001367
Iteration 95/1000 | Loss: 0.00001367
Iteration 96/1000 | Loss: 0.00001367
Iteration 97/1000 | Loss: 0.00001367
Iteration 98/1000 | Loss: 0.00001367
Iteration 99/1000 | Loss: 0.00001367
Iteration 100/1000 | Loss: 0.00001366
Iteration 101/1000 | Loss: 0.00001366
Iteration 102/1000 | Loss: 0.00001420
Iteration 103/1000 | Loss: 0.00001364
Iteration 104/1000 | Loss: 0.00001364
Iteration 105/1000 | Loss: 0.00001363
Iteration 106/1000 | Loss: 0.00001363
Iteration 107/1000 | Loss: 0.00001363
Iteration 108/1000 | Loss: 0.00001363
Iteration 109/1000 | Loss: 0.00001363
Iteration 110/1000 | Loss: 0.00001363
Iteration 111/1000 | Loss: 0.00001363
Iteration 112/1000 | Loss: 0.00001363
Iteration 113/1000 | Loss: 0.00001363
Iteration 114/1000 | Loss: 0.00001363
Iteration 115/1000 | Loss: 0.00001363
Iteration 116/1000 | Loss: 0.00001363
Iteration 117/1000 | Loss: 0.00001363
Iteration 118/1000 | Loss: 0.00001363
Iteration 119/1000 | Loss: 0.00001363
Iteration 120/1000 | Loss: 0.00001363
Iteration 121/1000 | Loss: 0.00001362
Iteration 122/1000 | Loss: 0.00001362
Iteration 123/1000 | Loss: 0.00001362
Iteration 124/1000 | Loss: 0.00005345
Iteration 125/1000 | Loss: 0.00001366
Iteration 126/1000 | Loss: 0.00001366
Iteration 127/1000 | Loss: 0.00001365
Iteration 128/1000 | Loss: 0.00001365
Iteration 129/1000 | Loss: 0.00001364
Iteration 130/1000 | Loss: 0.00001364
Iteration 131/1000 | Loss: 0.00001363
Iteration 132/1000 | Loss: 0.00001363
Iteration 133/1000 | Loss: 0.00001363
Iteration 134/1000 | Loss: 0.00001624
Iteration 135/1000 | Loss: 0.00005989
Iteration 136/1000 | Loss: 0.00002199
Iteration 137/1000 | Loss: 0.00001360
Iteration 138/1000 | Loss: 0.00001360
Iteration 139/1000 | Loss: 0.00001359
Iteration 140/1000 | Loss: 0.00001359
Iteration 141/1000 | Loss: 0.00001359
Iteration 142/1000 | Loss: 0.00001359
Iteration 143/1000 | Loss: 0.00001359
Iteration 144/1000 | Loss: 0.00001359
Iteration 145/1000 | Loss: 0.00001359
Iteration 146/1000 | Loss: 0.00001359
Iteration 147/1000 | Loss: 0.00001359
Iteration 148/1000 | Loss: 0.00001359
Iteration 149/1000 | Loss: 0.00001359
Iteration 150/1000 | Loss: 0.00001359
Iteration 151/1000 | Loss: 0.00001359
Iteration 152/1000 | Loss: 0.00001359
Iteration 153/1000 | Loss: 0.00001359
Iteration 154/1000 | Loss: 0.00001359
Iteration 155/1000 | Loss: 0.00001359
Iteration 156/1000 | Loss: 0.00001359
Iteration 157/1000 | Loss: 0.00001358
Iteration 158/1000 | Loss: 0.00001358
Iteration 159/1000 | Loss: 0.00001358
Iteration 160/1000 | Loss: 0.00001358
Iteration 161/1000 | Loss: 0.00001358
Iteration 162/1000 | Loss: 0.00001358
Iteration 163/1000 | Loss: 0.00001358
Iteration 164/1000 | Loss: 0.00001358
Iteration 165/1000 | Loss: 0.00002995
Iteration 166/1000 | Loss: 0.00005773
Iteration 167/1000 | Loss: 0.00003764
Iteration 168/1000 | Loss: 0.00001669
Iteration 169/1000 | Loss: 0.00001568
Iteration 170/1000 | Loss: 0.00001365
Iteration 171/1000 | Loss: 0.00001365
Iteration 172/1000 | Loss: 0.00001365
Iteration 173/1000 | Loss: 0.00001365
Iteration 174/1000 | Loss: 0.00001365
Iteration 175/1000 | Loss: 0.00001365
Iteration 176/1000 | Loss: 0.00001365
Iteration 177/1000 | Loss: 0.00001365
Iteration 178/1000 | Loss: 0.00001364
Iteration 179/1000 | Loss: 0.00005276
Iteration 180/1000 | Loss: 0.00005276
Iteration 181/1000 | Loss: 0.00006338
Iteration 182/1000 | Loss: 0.00009937
Iteration 183/1000 | Loss: 0.00001755
Iteration 184/1000 | Loss: 0.00005174
Iteration 185/1000 | Loss: 0.00008239
Iteration 186/1000 | Loss: 0.00001407
Iteration 187/1000 | Loss: 0.00004282
Iteration 188/1000 | Loss: 0.00001369
Iteration 189/1000 | Loss: 0.00001579
Iteration 190/1000 | Loss: 0.00001363
Iteration 191/1000 | Loss: 0.00001363
Iteration 192/1000 | Loss: 0.00001363
Iteration 193/1000 | Loss: 0.00001363
Iteration 194/1000 | Loss: 0.00001363
Iteration 195/1000 | Loss: 0.00001363
Iteration 196/1000 | Loss: 0.00001363
Iteration 197/1000 | Loss: 0.00001363
Iteration 198/1000 | Loss: 0.00001363
Iteration 199/1000 | Loss: 0.00001362
Iteration 200/1000 | Loss: 0.00002147
Iteration 201/1000 | Loss: 0.00001891
Iteration 202/1000 | Loss: 0.00001359
Iteration 203/1000 | Loss: 0.00001359
Iteration 204/1000 | Loss: 0.00001359
Iteration 205/1000 | Loss: 0.00001359
Iteration 206/1000 | Loss: 0.00001358
Iteration 207/1000 | Loss: 0.00001358
Iteration 208/1000 | Loss: 0.00001358
Iteration 209/1000 | Loss: 0.00001358
Iteration 210/1000 | Loss: 0.00001357
Iteration 211/1000 | Loss: 0.00001357
Iteration 212/1000 | Loss: 0.00001357
Iteration 213/1000 | Loss: 0.00001357
Iteration 214/1000 | Loss: 0.00001357
Iteration 215/1000 | Loss: 0.00001357
Iteration 216/1000 | Loss: 0.00001357
Iteration 217/1000 | Loss: 0.00001357
Iteration 218/1000 | Loss: 0.00001523
Iteration 219/1000 | Loss: 0.00001357
Iteration 220/1000 | Loss: 0.00001356
Iteration 221/1000 | Loss: 0.00001356
Iteration 222/1000 | Loss: 0.00001356
Iteration 223/1000 | Loss: 0.00001355
Iteration 224/1000 | Loss: 0.00001355
Iteration 225/1000 | Loss: 0.00001355
Iteration 226/1000 | Loss: 0.00001355
Iteration 227/1000 | Loss: 0.00001355
Iteration 228/1000 | Loss: 0.00001355
Iteration 229/1000 | Loss: 0.00001355
Iteration 230/1000 | Loss: 0.00001354
Iteration 231/1000 | Loss: 0.00001354
Iteration 232/1000 | Loss: 0.00001354
Iteration 233/1000 | Loss: 0.00001354
Iteration 234/1000 | Loss: 0.00001354
Iteration 235/1000 | Loss: 0.00001354
Iteration 236/1000 | Loss: 0.00001354
Iteration 237/1000 | Loss: 0.00001354
Iteration 238/1000 | Loss: 0.00001354
Iteration 239/1000 | Loss: 0.00001354
Iteration 240/1000 | Loss: 0.00001354
Iteration 241/1000 | Loss: 0.00001353
Iteration 242/1000 | Loss: 0.00001353
Iteration 243/1000 | Loss: 0.00001353
Iteration 244/1000 | Loss: 0.00001353
Iteration 245/1000 | Loss: 0.00001353
Iteration 246/1000 | Loss: 0.00001353
Iteration 247/1000 | Loss: 0.00001353
Iteration 248/1000 | Loss: 0.00001353
Iteration 249/1000 | Loss: 0.00001353
Iteration 250/1000 | Loss: 0.00001353
Iteration 251/1000 | Loss: 0.00001353
Iteration 252/1000 | Loss: 0.00001353
Iteration 253/1000 | Loss: 0.00001353
Iteration 254/1000 | Loss: 0.00001353
Iteration 255/1000 | Loss: 0.00001353
Iteration 256/1000 | Loss: 0.00001353
Iteration 257/1000 | Loss: 0.00001353
Iteration 258/1000 | Loss: 0.00001353
Iteration 259/1000 | Loss: 0.00001353
Iteration 260/1000 | Loss: 0.00001353
Iteration 261/1000 | Loss: 0.00001353
Iteration 262/1000 | Loss: 0.00001353
Iteration 263/1000 | Loss: 0.00001353
Iteration 264/1000 | Loss: 0.00001353
Iteration 265/1000 | Loss: 0.00001353
Iteration 266/1000 | Loss: 0.00001353
Iteration 267/1000 | Loss: 0.00001353
Iteration 268/1000 | Loss: 0.00001353
Iteration 269/1000 | Loss: 0.00001353
Iteration 270/1000 | Loss: 0.00001353
Iteration 271/1000 | Loss: 0.00001353
Iteration 272/1000 | Loss: 0.00001353
Iteration 273/1000 | Loss: 0.00001353
Iteration 274/1000 | Loss: 0.00001353
Iteration 275/1000 | Loss: 0.00001353
Iteration 276/1000 | Loss: 0.00001353
Iteration 277/1000 | Loss: 0.00001353
Iteration 278/1000 | Loss: 0.00001353
Iteration 279/1000 | Loss: 0.00001353
Iteration 280/1000 | Loss: 0.00001353
Iteration 281/1000 | Loss: 0.00001353
Iteration 282/1000 | Loss: 0.00001353
Iteration 283/1000 | Loss: 0.00001353
Iteration 284/1000 | Loss: 0.00001353
Iteration 285/1000 | Loss: 0.00001353
Iteration 286/1000 | Loss: 0.00001353
Iteration 287/1000 | Loss: 0.00001353
Iteration 288/1000 | Loss: 0.00001353
Iteration 289/1000 | Loss: 0.00001353
Iteration 290/1000 | Loss: 0.00001353
Iteration 291/1000 | Loss: 0.00001353
Iteration 292/1000 | Loss: 0.00001353
Iteration 293/1000 | Loss: 0.00001353
Iteration 294/1000 | Loss: 0.00001353
Iteration 295/1000 | Loss: 0.00001353
Iteration 296/1000 | Loss: 0.00001353
Iteration 297/1000 | Loss: 0.00001353
Iteration 298/1000 | Loss: 0.00001353
Iteration 299/1000 | Loss: 0.00001353
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 299. Stopping optimization.
Last 5 losses: [1.3529960597225e-05, 1.3529960597225e-05, 1.3529960597225e-05, 1.3529960597225e-05, 1.3529960597225e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3529960597225e-05

Optimization complete. Final v2v error: 3.131777286529541 mm

Highest mean error: 3.8131802082061768 mm for frame 143

Lowest mean error: 2.847982168197632 mm for frame 69

Saving results

Total time: 162.72972679138184
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1034/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1034.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1034
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737791
Iteration 2/25 | Loss: 0.00179558
Iteration 3/25 | Loss: 0.00136366
Iteration 4/25 | Loss: 0.00129106
Iteration 5/25 | Loss: 0.00128105
Iteration 6/25 | Loss: 0.00128047
Iteration 7/25 | Loss: 0.00128047
Iteration 8/25 | Loss: 0.00128047
Iteration 9/25 | Loss: 0.00128047
Iteration 10/25 | Loss: 0.00128047
Iteration 11/25 | Loss: 0.00128047
Iteration 12/25 | Loss: 0.00128047
Iteration 13/25 | Loss: 0.00128047
Iteration 14/25 | Loss: 0.00128047
Iteration 15/25 | Loss: 0.00128047
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0012804650468751788, 0.0012804650468751788, 0.0012804650468751788, 0.0012804650468751788, 0.0012804650468751788]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012804650468751788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.67077756
Iteration 2/25 | Loss: 0.00099719
Iteration 3/25 | Loss: 0.00099715
Iteration 4/25 | Loss: 0.00099715
Iteration 5/25 | Loss: 0.00099715
Iteration 6/25 | Loss: 0.00099715
Iteration 7/25 | Loss: 0.00099715
Iteration 8/25 | Loss: 0.00099715
Iteration 9/25 | Loss: 0.00099715
Iteration 10/25 | Loss: 0.00099715
Iteration 11/25 | Loss: 0.00099715
Iteration 12/25 | Loss: 0.00099715
Iteration 13/25 | Loss: 0.00099715
Iteration 14/25 | Loss: 0.00099715
Iteration 15/25 | Loss: 0.00099715
Iteration 16/25 | Loss: 0.00099715
Iteration 17/25 | Loss: 0.00099715
Iteration 18/25 | Loss: 0.00099715
Iteration 19/25 | Loss: 0.00099715
Iteration 20/25 | Loss: 0.00099715
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0009971465915441513, 0.0009971465915441513, 0.0009971465915441513, 0.0009971465915441513, 0.0009971465915441513]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009971465915441513

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099715
Iteration 2/1000 | Loss: 0.00003486
Iteration 3/1000 | Loss: 0.00002506
Iteration 4/1000 | Loss: 0.00002364
Iteration 5/1000 | Loss: 0.00002271
Iteration 6/1000 | Loss: 0.00002208
Iteration 7/1000 | Loss: 0.00002174
Iteration 8/1000 | Loss: 0.00002134
Iteration 9/1000 | Loss: 0.00002106
Iteration 10/1000 | Loss: 0.00002081
Iteration 11/1000 | Loss: 0.00002060
Iteration 12/1000 | Loss: 0.00002032
Iteration 13/1000 | Loss: 0.00002017
Iteration 14/1000 | Loss: 0.00002009
Iteration 15/1000 | Loss: 0.00002000
Iteration 16/1000 | Loss: 0.00001999
Iteration 17/1000 | Loss: 0.00001997
Iteration 18/1000 | Loss: 0.00001996
Iteration 19/1000 | Loss: 0.00001996
Iteration 20/1000 | Loss: 0.00001995
Iteration 21/1000 | Loss: 0.00001995
Iteration 22/1000 | Loss: 0.00001995
Iteration 23/1000 | Loss: 0.00001994
Iteration 24/1000 | Loss: 0.00001994
Iteration 25/1000 | Loss: 0.00001994
Iteration 26/1000 | Loss: 0.00001994
Iteration 27/1000 | Loss: 0.00001994
Iteration 28/1000 | Loss: 0.00001994
Iteration 29/1000 | Loss: 0.00001994
Iteration 30/1000 | Loss: 0.00001993
Iteration 31/1000 | Loss: 0.00001993
Iteration 32/1000 | Loss: 0.00001993
Iteration 33/1000 | Loss: 0.00001993
Iteration 34/1000 | Loss: 0.00001992
Iteration 35/1000 | Loss: 0.00001992
Iteration 36/1000 | Loss: 0.00001992
Iteration 37/1000 | Loss: 0.00001992
Iteration 38/1000 | Loss: 0.00001992
Iteration 39/1000 | Loss: 0.00001992
Iteration 40/1000 | Loss: 0.00001992
Iteration 41/1000 | Loss: 0.00001991
Iteration 42/1000 | Loss: 0.00001991
Iteration 43/1000 | Loss: 0.00001991
Iteration 44/1000 | Loss: 0.00001991
Iteration 45/1000 | Loss: 0.00001991
Iteration 46/1000 | Loss: 0.00001991
Iteration 47/1000 | Loss: 0.00001991
Iteration 48/1000 | Loss: 0.00001991
Iteration 49/1000 | Loss: 0.00001991
Iteration 50/1000 | Loss: 0.00001991
Iteration 51/1000 | Loss: 0.00001991
Iteration 52/1000 | Loss: 0.00001991
Iteration 53/1000 | Loss: 0.00001991
Iteration 54/1000 | Loss: 0.00001990
Iteration 55/1000 | Loss: 0.00001990
Iteration 56/1000 | Loss: 0.00001990
Iteration 57/1000 | Loss: 0.00001990
Iteration 58/1000 | Loss: 0.00001990
Iteration 59/1000 | Loss: 0.00001989
Iteration 60/1000 | Loss: 0.00001989
Iteration 61/1000 | Loss: 0.00001989
Iteration 62/1000 | Loss: 0.00001989
Iteration 63/1000 | Loss: 0.00001989
Iteration 64/1000 | Loss: 0.00001989
Iteration 65/1000 | Loss: 0.00001989
Iteration 66/1000 | Loss: 0.00001989
Iteration 67/1000 | Loss: 0.00001988
Iteration 68/1000 | Loss: 0.00001988
Iteration 69/1000 | Loss: 0.00001988
Iteration 70/1000 | Loss: 0.00001988
Iteration 71/1000 | Loss: 0.00001988
Iteration 72/1000 | Loss: 0.00001988
Iteration 73/1000 | Loss: 0.00001988
Iteration 74/1000 | Loss: 0.00001988
Iteration 75/1000 | Loss: 0.00001988
Iteration 76/1000 | Loss: 0.00001988
Iteration 77/1000 | Loss: 0.00001988
Iteration 78/1000 | Loss: 0.00001988
Iteration 79/1000 | Loss: 0.00001987
Iteration 80/1000 | Loss: 0.00001987
Iteration 81/1000 | Loss: 0.00001987
Iteration 82/1000 | Loss: 0.00001987
Iteration 83/1000 | Loss: 0.00001986
Iteration 84/1000 | Loss: 0.00001986
Iteration 85/1000 | Loss: 0.00001986
Iteration 86/1000 | Loss: 0.00001986
Iteration 87/1000 | Loss: 0.00001986
Iteration 88/1000 | Loss: 0.00001986
Iteration 89/1000 | Loss: 0.00001986
Iteration 90/1000 | Loss: 0.00001985
Iteration 91/1000 | Loss: 0.00001985
Iteration 92/1000 | Loss: 0.00001985
Iteration 93/1000 | Loss: 0.00001984
Iteration 94/1000 | Loss: 0.00001984
Iteration 95/1000 | Loss: 0.00001984
Iteration 96/1000 | Loss: 0.00001984
Iteration 97/1000 | Loss: 0.00001984
Iteration 98/1000 | Loss: 0.00001983
Iteration 99/1000 | Loss: 0.00001983
Iteration 100/1000 | Loss: 0.00001983
Iteration 101/1000 | Loss: 0.00001983
Iteration 102/1000 | Loss: 0.00001982
Iteration 103/1000 | Loss: 0.00001982
Iteration 104/1000 | Loss: 0.00001981
Iteration 105/1000 | Loss: 0.00001981
Iteration 106/1000 | Loss: 0.00001981
Iteration 107/1000 | Loss: 0.00001981
Iteration 108/1000 | Loss: 0.00001981
Iteration 109/1000 | Loss: 0.00001980
Iteration 110/1000 | Loss: 0.00001980
Iteration 111/1000 | Loss: 0.00001980
Iteration 112/1000 | Loss: 0.00001980
Iteration 113/1000 | Loss: 0.00001980
Iteration 114/1000 | Loss: 0.00001980
Iteration 115/1000 | Loss: 0.00001980
Iteration 116/1000 | Loss: 0.00001980
Iteration 117/1000 | Loss: 0.00001980
Iteration 118/1000 | Loss: 0.00001980
Iteration 119/1000 | Loss: 0.00001980
Iteration 120/1000 | Loss: 0.00001979
Iteration 121/1000 | Loss: 0.00001979
Iteration 122/1000 | Loss: 0.00001979
Iteration 123/1000 | Loss: 0.00001979
Iteration 124/1000 | Loss: 0.00001979
Iteration 125/1000 | Loss: 0.00001979
Iteration 126/1000 | Loss: 0.00001979
Iteration 127/1000 | Loss: 0.00001979
Iteration 128/1000 | Loss: 0.00001979
Iteration 129/1000 | Loss: 0.00001978
Iteration 130/1000 | Loss: 0.00001978
Iteration 131/1000 | Loss: 0.00001978
Iteration 132/1000 | Loss: 0.00001978
Iteration 133/1000 | Loss: 0.00001977
Iteration 134/1000 | Loss: 0.00001977
Iteration 135/1000 | Loss: 0.00001977
Iteration 136/1000 | Loss: 0.00001977
Iteration 137/1000 | Loss: 0.00001977
Iteration 138/1000 | Loss: 0.00001977
Iteration 139/1000 | Loss: 0.00001977
Iteration 140/1000 | Loss: 0.00001977
Iteration 141/1000 | Loss: 0.00001977
Iteration 142/1000 | Loss: 0.00001977
Iteration 143/1000 | Loss: 0.00001977
Iteration 144/1000 | Loss: 0.00001977
Iteration 145/1000 | Loss: 0.00001977
Iteration 146/1000 | Loss: 0.00001977
Iteration 147/1000 | Loss: 0.00001977
Iteration 148/1000 | Loss: 0.00001977
Iteration 149/1000 | Loss: 0.00001977
Iteration 150/1000 | Loss: 0.00001977
Iteration 151/1000 | Loss: 0.00001977
Iteration 152/1000 | Loss: 0.00001977
Iteration 153/1000 | Loss: 0.00001977
Iteration 154/1000 | Loss: 0.00001977
Iteration 155/1000 | Loss: 0.00001977
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.9770279322983697e-05, 1.9770279322983697e-05, 1.9770279322983697e-05, 1.9770279322983697e-05, 1.9770279322983697e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9770279322983697e-05

Optimization complete. Final v2v error: 3.694075584411621 mm

Highest mean error: 4.021356582641602 mm for frame 235

Lowest mean error: 3.4656360149383545 mm for frame 124

Saving results

Total time: 43.612279415130615
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_003/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_003/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00280722
Iteration 2/25 | Loss: 0.00141900
Iteration 3/25 | Loss: 0.00125575
Iteration 4/25 | Loss: 0.00122010
Iteration 5/25 | Loss: 0.00121328
Iteration 6/25 | Loss: 0.00121181
Iteration 7/25 | Loss: 0.00121128
Iteration 8/25 | Loss: 0.00121128
Iteration 9/25 | Loss: 0.00121128
Iteration 10/25 | Loss: 0.00121128
Iteration 11/25 | Loss: 0.00121128
Iteration 12/25 | Loss: 0.00121128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012112793046981096, 0.0012112793046981096, 0.0012112793046981096, 0.0012112793046981096, 0.0012112793046981096]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012112793046981096

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30419219
Iteration 2/25 | Loss: 0.00158967
Iteration 3/25 | Loss: 0.00158967
Iteration 4/25 | Loss: 0.00158967
Iteration 5/25 | Loss: 0.00158967
Iteration 6/25 | Loss: 0.00158967
Iteration 7/25 | Loss: 0.00158967
Iteration 8/25 | Loss: 0.00158967
Iteration 9/25 | Loss: 0.00158967
Iteration 10/25 | Loss: 0.00158967
Iteration 11/25 | Loss: 0.00158967
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001589669380337, 0.001589669380337, 0.001589669380337, 0.001589669380337, 0.001589669380337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001589669380337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158967
Iteration 2/1000 | Loss: 0.00004398
Iteration 3/1000 | Loss: 0.00002749
Iteration 4/1000 | Loss: 0.00002089
Iteration 5/1000 | Loss: 0.00001943
Iteration 6/1000 | Loss: 0.00001855
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001723
Iteration 9/1000 | Loss: 0.00001690
Iteration 10/1000 | Loss: 0.00001648
Iteration 11/1000 | Loss: 0.00001612
Iteration 12/1000 | Loss: 0.00001588
Iteration 13/1000 | Loss: 0.00001576
Iteration 14/1000 | Loss: 0.00001556
Iteration 15/1000 | Loss: 0.00001543
Iteration 16/1000 | Loss: 0.00001542
Iteration 17/1000 | Loss: 0.00001542
Iteration 18/1000 | Loss: 0.00001538
Iteration 19/1000 | Loss: 0.00001534
Iteration 20/1000 | Loss: 0.00001533
Iteration 21/1000 | Loss: 0.00001533
Iteration 22/1000 | Loss: 0.00001531
Iteration 23/1000 | Loss: 0.00001531
Iteration 24/1000 | Loss: 0.00001530
Iteration 25/1000 | Loss: 0.00001530
Iteration 26/1000 | Loss: 0.00001528
Iteration 27/1000 | Loss: 0.00001528
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001527
Iteration 31/1000 | Loss: 0.00001527
Iteration 32/1000 | Loss: 0.00001527
Iteration 33/1000 | Loss: 0.00001527
Iteration 34/1000 | Loss: 0.00001527
Iteration 35/1000 | Loss: 0.00001526
Iteration 36/1000 | Loss: 0.00001526
Iteration 37/1000 | Loss: 0.00001525
Iteration 38/1000 | Loss: 0.00001525
Iteration 39/1000 | Loss: 0.00001524
Iteration 40/1000 | Loss: 0.00001524
Iteration 41/1000 | Loss: 0.00001523
Iteration 42/1000 | Loss: 0.00001523
Iteration 43/1000 | Loss: 0.00001522
Iteration 44/1000 | Loss: 0.00001521
Iteration 45/1000 | Loss: 0.00001521
Iteration 46/1000 | Loss: 0.00001519
Iteration 47/1000 | Loss: 0.00001519
Iteration 48/1000 | Loss: 0.00001519
Iteration 49/1000 | Loss: 0.00001519
Iteration 50/1000 | Loss: 0.00001519
Iteration 51/1000 | Loss: 0.00001519
Iteration 52/1000 | Loss: 0.00001519
Iteration 53/1000 | Loss: 0.00001519
Iteration 54/1000 | Loss: 0.00001519
Iteration 55/1000 | Loss: 0.00001519
Iteration 56/1000 | Loss: 0.00001518
Iteration 57/1000 | Loss: 0.00001518
Iteration 58/1000 | Loss: 0.00001518
Iteration 59/1000 | Loss: 0.00001518
Iteration 60/1000 | Loss: 0.00001518
Iteration 61/1000 | Loss: 0.00001518
Iteration 62/1000 | Loss: 0.00001518
Iteration 63/1000 | Loss: 0.00001517
Iteration 64/1000 | Loss: 0.00001517
Iteration 65/1000 | Loss: 0.00001517
Iteration 66/1000 | Loss: 0.00001516
Iteration 67/1000 | Loss: 0.00001516
Iteration 68/1000 | Loss: 0.00001515
Iteration 69/1000 | Loss: 0.00001515
Iteration 70/1000 | Loss: 0.00001515
Iteration 71/1000 | Loss: 0.00001515
Iteration 72/1000 | Loss: 0.00001514
Iteration 73/1000 | Loss: 0.00001514
Iteration 74/1000 | Loss: 0.00001514
Iteration 75/1000 | Loss: 0.00001513
Iteration 76/1000 | Loss: 0.00001513
Iteration 77/1000 | Loss: 0.00001513
Iteration 78/1000 | Loss: 0.00001512
Iteration 79/1000 | Loss: 0.00001512
Iteration 80/1000 | Loss: 0.00001512
Iteration 81/1000 | Loss: 0.00001512
Iteration 82/1000 | Loss: 0.00001511
Iteration 83/1000 | Loss: 0.00001511
Iteration 84/1000 | Loss: 0.00001510
Iteration 85/1000 | Loss: 0.00001510
Iteration 86/1000 | Loss: 0.00001510
Iteration 87/1000 | Loss: 0.00001509
Iteration 88/1000 | Loss: 0.00001509
Iteration 89/1000 | Loss: 0.00001509
Iteration 90/1000 | Loss: 0.00001509
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001509
Iteration 93/1000 | Loss: 0.00001509
Iteration 94/1000 | Loss: 0.00001508
Iteration 95/1000 | Loss: 0.00001508
Iteration 96/1000 | Loss: 0.00001508
Iteration 97/1000 | Loss: 0.00001508
Iteration 98/1000 | Loss: 0.00001508
Iteration 99/1000 | Loss: 0.00001508
Iteration 100/1000 | Loss: 0.00001508
Iteration 101/1000 | Loss: 0.00001507
Iteration 102/1000 | Loss: 0.00001507
Iteration 103/1000 | Loss: 0.00001507
Iteration 104/1000 | Loss: 0.00001507
Iteration 105/1000 | Loss: 0.00001507
Iteration 106/1000 | Loss: 0.00001507
Iteration 107/1000 | Loss: 0.00001506
Iteration 108/1000 | Loss: 0.00001506
Iteration 109/1000 | Loss: 0.00001506
Iteration 110/1000 | Loss: 0.00001506
Iteration 111/1000 | Loss: 0.00001506
Iteration 112/1000 | Loss: 0.00001506
Iteration 113/1000 | Loss: 0.00001506
Iteration 114/1000 | Loss: 0.00001506
Iteration 115/1000 | Loss: 0.00001505
Iteration 116/1000 | Loss: 0.00001505
Iteration 117/1000 | Loss: 0.00001505
Iteration 118/1000 | Loss: 0.00001505
Iteration 119/1000 | Loss: 0.00001505
Iteration 120/1000 | Loss: 0.00001505
Iteration 121/1000 | Loss: 0.00001505
Iteration 122/1000 | Loss: 0.00001505
Iteration 123/1000 | Loss: 0.00001505
Iteration 124/1000 | Loss: 0.00001505
Iteration 125/1000 | Loss: 0.00001505
Iteration 126/1000 | Loss: 0.00001505
Iteration 127/1000 | Loss: 0.00001505
Iteration 128/1000 | Loss: 0.00001504
Iteration 129/1000 | Loss: 0.00001504
Iteration 130/1000 | Loss: 0.00001504
Iteration 131/1000 | Loss: 0.00001504
Iteration 132/1000 | Loss: 0.00001504
Iteration 133/1000 | Loss: 0.00001504
Iteration 134/1000 | Loss: 0.00001504
Iteration 135/1000 | Loss: 0.00001504
Iteration 136/1000 | Loss: 0.00001504
Iteration 137/1000 | Loss: 0.00001504
Iteration 138/1000 | Loss: 0.00001504
Iteration 139/1000 | Loss: 0.00001504
Iteration 140/1000 | Loss: 0.00001503
Iteration 141/1000 | Loss: 0.00001503
Iteration 142/1000 | Loss: 0.00001503
Iteration 143/1000 | Loss: 0.00001503
Iteration 144/1000 | Loss: 0.00001503
Iteration 145/1000 | Loss: 0.00001502
Iteration 146/1000 | Loss: 0.00001502
Iteration 147/1000 | Loss: 0.00001502
Iteration 148/1000 | Loss: 0.00001502
Iteration 149/1000 | Loss: 0.00001502
Iteration 150/1000 | Loss: 0.00001502
Iteration 151/1000 | Loss: 0.00001502
Iteration 152/1000 | Loss: 0.00001502
Iteration 153/1000 | Loss: 0.00001502
Iteration 154/1000 | Loss: 0.00001502
Iteration 155/1000 | Loss: 0.00001502
Iteration 156/1000 | Loss: 0.00001502
Iteration 157/1000 | Loss: 0.00001502
Iteration 158/1000 | Loss: 0.00001501
Iteration 159/1000 | Loss: 0.00001501
Iteration 160/1000 | Loss: 0.00001501
Iteration 161/1000 | Loss: 0.00001501
Iteration 162/1000 | Loss: 0.00001501
Iteration 163/1000 | Loss: 0.00001501
Iteration 164/1000 | Loss: 0.00001501
Iteration 165/1000 | Loss: 0.00001501
Iteration 166/1000 | Loss: 0.00001501
Iteration 167/1000 | Loss: 0.00001501
Iteration 168/1000 | Loss: 0.00001501
Iteration 169/1000 | Loss: 0.00001501
Iteration 170/1000 | Loss: 0.00001501
Iteration 171/1000 | Loss: 0.00001501
Iteration 172/1000 | Loss: 0.00001500
Iteration 173/1000 | Loss: 0.00001500
Iteration 174/1000 | Loss: 0.00001500
Iteration 175/1000 | Loss: 0.00001500
Iteration 176/1000 | Loss: 0.00001500
Iteration 177/1000 | Loss: 0.00001500
Iteration 178/1000 | Loss: 0.00001500
Iteration 179/1000 | Loss: 0.00001500
Iteration 180/1000 | Loss: 0.00001499
Iteration 181/1000 | Loss: 0.00001499
Iteration 182/1000 | Loss: 0.00001499
Iteration 183/1000 | Loss: 0.00001499
Iteration 184/1000 | Loss: 0.00001499
Iteration 185/1000 | Loss: 0.00001499
Iteration 186/1000 | Loss: 0.00001498
Iteration 187/1000 | Loss: 0.00001498
Iteration 188/1000 | Loss: 0.00001498
Iteration 189/1000 | Loss: 0.00001498
Iteration 190/1000 | Loss: 0.00001498
Iteration 191/1000 | Loss: 0.00001498
Iteration 192/1000 | Loss: 0.00001497
Iteration 193/1000 | Loss: 0.00001497
Iteration 194/1000 | Loss: 0.00001497
Iteration 195/1000 | Loss: 0.00001497
Iteration 196/1000 | Loss: 0.00001497
Iteration 197/1000 | Loss: 0.00001496
Iteration 198/1000 | Loss: 0.00001496
Iteration 199/1000 | Loss: 0.00001496
Iteration 200/1000 | Loss: 0.00001496
Iteration 201/1000 | Loss: 0.00001496
Iteration 202/1000 | Loss: 0.00001496
Iteration 203/1000 | Loss: 0.00001496
Iteration 204/1000 | Loss: 0.00001496
Iteration 205/1000 | Loss: 0.00001496
Iteration 206/1000 | Loss: 0.00001496
Iteration 207/1000 | Loss: 0.00001496
Iteration 208/1000 | Loss: 0.00001495
Iteration 209/1000 | Loss: 0.00001495
Iteration 210/1000 | Loss: 0.00001495
Iteration 211/1000 | Loss: 0.00001495
Iteration 212/1000 | Loss: 0.00001495
Iteration 213/1000 | Loss: 0.00001495
Iteration 214/1000 | Loss: 0.00001495
Iteration 215/1000 | Loss: 0.00001495
Iteration 216/1000 | Loss: 0.00001494
Iteration 217/1000 | Loss: 0.00001494
Iteration 218/1000 | Loss: 0.00001494
Iteration 219/1000 | Loss: 0.00001494
Iteration 220/1000 | Loss: 0.00001494
Iteration 221/1000 | Loss: 0.00001494
Iteration 222/1000 | Loss: 0.00001493
Iteration 223/1000 | Loss: 0.00001493
Iteration 224/1000 | Loss: 0.00001493
Iteration 225/1000 | Loss: 0.00001493
Iteration 226/1000 | Loss: 0.00001493
Iteration 227/1000 | Loss: 0.00001493
Iteration 228/1000 | Loss: 0.00001493
Iteration 229/1000 | Loss: 0.00001493
Iteration 230/1000 | Loss: 0.00001493
Iteration 231/1000 | Loss: 0.00001493
Iteration 232/1000 | Loss: 0.00001493
Iteration 233/1000 | Loss: 0.00001493
Iteration 234/1000 | Loss: 0.00001493
Iteration 235/1000 | Loss: 0.00001493
Iteration 236/1000 | Loss: 0.00001493
Iteration 237/1000 | Loss: 0.00001492
Iteration 238/1000 | Loss: 0.00001492
Iteration 239/1000 | Loss: 0.00001492
Iteration 240/1000 | Loss: 0.00001492
Iteration 241/1000 | Loss: 0.00001492
Iteration 242/1000 | Loss: 0.00001492
Iteration 243/1000 | Loss: 0.00001492
Iteration 244/1000 | Loss: 0.00001492
Iteration 245/1000 | Loss: 0.00001492
Iteration 246/1000 | Loss: 0.00001492
Iteration 247/1000 | Loss: 0.00001491
Iteration 248/1000 | Loss: 0.00001491
Iteration 249/1000 | Loss: 0.00001491
Iteration 250/1000 | Loss: 0.00001491
Iteration 251/1000 | Loss: 0.00001491
Iteration 252/1000 | Loss: 0.00001491
Iteration 253/1000 | Loss: 0.00001491
Iteration 254/1000 | Loss: 0.00001491
Iteration 255/1000 | Loss: 0.00001491
Iteration 256/1000 | Loss: 0.00001491
Iteration 257/1000 | Loss: 0.00001491
Iteration 258/1000 | Loss: 0.00001491
Iteration 259/1000 | Loss: 0.00001491
Iteration 260/1000 | Loss: 0.00001491
Iteration 261/1000 | Loss: 0.00001490
Iteration 262/1000 | Loss: 0.00001490
Iteration 263/1000 | Loss: 0.00001490
Iteration 264/1000 | Loss: 0.00001490
Iteration 265/1000 | Loss: 0.00001490
Iteration 266/1000 | Loss: 0.00001490
Iteration 267/1000 | Loss: 0.00001490
Iteration 268/1000 | Loss: 0.00001490
Iteration 269/1000 | Loss: 0.00001490
Iteration 270/1000 | Loss: 0.00001490
Iteration 271/1000 | Loss: 0.00001490
Iteration 272/1000 | Loss: 0.00001490
Iteration 273/1000 | Loss: 0.00001490
Iteration 274/1000 | Loss: 0.00001490
Iteration 275/1000 | Loss: 0.00001490
Iteration 276/1000 | Loss: 0.00001490
Iteration 277/1000 | Loss: 0.00001490
Iteration 278/1000 | Loss: 0.00001490
Iteration 279/1000 | Loss: 0.00001490
Iteration 280/1000 | Loss: 0.00001490
Iteration 281/1000 | Loss: 0.00001490
Iteration 282/1000 | Loss: 0.00001489
Iteration 283/1000 | Loss: 0.00001489
Iteration 284/1000 | Loss: 0.00001489
Iteration 285/1000 | Loss: 0.00001489
Iteration 286/1000 | Loss: 0.00001489
Iteration 287/1000 | Loss: 0.00001489
Iteration 288/1000 | Loss: 0.00001489
Iteration 289/1000 | Loss: 0.00001489
Iteration 290/1000 | Loss: 0.00001489
Iteration 291/1000 | Loss: 0.00001489
Iteration 292/1000 | Loss: 0.00001489
Iteration 293/1000 | Loss: 0.00001489
Iteration 294/1000 | Loss: 0.00001489
Iteration 295/1000 | Loss: 0.00001489
Iteration 296/1000 | Loss: 0.00001489
Iteration 297/1000 | Loss: 0.00001489
Iteration 298/1000 | Loss: 0.00001489
Iteration 299/1000 | Loss: 0.00001489
Iteration 300/1000 | Loss: 0.00001489
Iteration 301/1000 | Loss: 0.00001489
Iteration 302/1000 | Loss: 0.00001489
Iteration 303/1000 | Loss: 0.00001489
Iteration 304/1000 | Loss: 0.00001489
Iteration 305/1000 | Loss: 0.00001488
Iteration 306/1000 | Loss: 0.00001488
Iteration 307/1000 | Loss: 0.00001488
Iteration 308/1000 | Loss: 0.00001488
Iteration 309/1000 | Loss: 0.00001488
Iteration 310/1000 | Loss: 0.00001488
Iteration 311/1000 | Loss: 0.00001488
Iteration 312/1000 | Loss: 0.00001488
Iteration 313/1000 | Loss: 0.00001488
Iteration 314/1000 | Loss: 0.00001488
Iteration 315/1000 | Loss: 0.00001488
Iteration 316/1000 | Loss: 0.00001488
Iteration 317/1000 | Loss: 0.00001488
Iteration 318/1000 | Loss: 0.00001488
Iteration 319/1000 | Loss: 0.00001488
Iteration 320/1000 | Loss: 0.00001488
Iteration 321/1000 | Loss: 0.00001488
Iteration 322/1000 | Loss: 0.00001488
Iteration 323/1000 | Loss: 0.00001488
Iteration 324/1000 | Loss: 0.00001488
Iteration 325/1000 | Loss: 0.00001488
Iteration 326/1000 | Loss: 0.00001488
Iteration 327/1000 | Loss: 0.00001488
Iteration 328/1000 | Loss: 0.00001488
Iteration 329/1000 | Loss: 0.00001488
Iteration 330/1000 | Loss: 0.00001488
Iteration 331/1000 | Loss: 0.00001488
Iteration 332/1000 | Loss: 0.00001488
Iteration 333/1000 | Loss: 0.00001488
Iteration 334/1000 | Loss: 0.00001488
Iteration 335/1000 | Loss: 0.00001488
Iteration 336/1000 | Loss: 0.00001488
Iteration 337/1000 | Loss: 0.00001488
Iteration 338/1000 | Loss: 0.00001488
Iteration 339/1000 | Loss: 0.00001488
Iteration 340/1000 | Loss: 0.00001488
Iteration 341/1000 | Loss: 0.00001488
Iteration 342/1000 | Loss: 0.00001488
Iteration 343/1000 | Loss: 0.00001488
Iteration 344/1000 | Loss: 0.00001488
Iteration 345/1000 | Loss: 0.00001488
Iteration 346/1000 | Loss: 0.00001488
Iteration 347/1000 | Loss: 0.00001488
Iteration 348/1000 | Loss: 0.00001488
Iteration 349/1000 | Loss: 0.00001488
Iteration 350/1000 | Loss: 0.00001488
Iteration 351/1000 | Loss: 0.00001488
Iteration 352/1000 | Loss: 0.00001488
Iteration 353/1000 | Loss: 0.00001488
Iteration 354/1000 | Loss: 0.00001488
Iteration 355/1000 | Loss: 0.00001488
Iteration 356/1000 | Loss: 0.00001488
Iteration 357/1000 | Loss: 0.00001488
Iteration 358/1000 | Loss: 0.00001488
Iteration 359/1000 | Loss: 0.00001488
Iteration 360/1000 | Loss: 0.00001488
Iteration 361/1000 | Loss: 0.00001488
Iteration 362/1000 | Loss: 0.00001488
Iteration 363/1000 | Loss: 0.00001488
Iteration 364/1000 | Loss: 0.00001488
Iteration 365/1000 | Loss: 0.00001488
Iteration 366/1000 | Loss: 0.00001488
Iteration 367/1000 | Loss: 0.00001488
Iteration 368/1000 | Loss: 0.00001488
Iteration 369/1000 | Loss: 0.00001488
Iteration 370/1000 | Loss: 0.00001488
Iteration 371/1000 | Loss: 0.00001488
Iteration 372/1000 | Loss: 0.00001488
Iteration 373/1000 | Loss: 0.00001488
Iteration 374/1000 | Loss: 0.00001488
Iteration 375/1000 | Loss: 0.00001488
Iteration 376/1000 | Loss: 0.00001488
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 376. Stopping optimization.
Last 5 losses: [1.4880098206049297e-05, 1.4880098206049297e-05, 1.4880098206049297e-05, 1.4880098206049297e-05, 1.4880098206049297e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4880098206049297e-05

Optimization complete. Final v2v error: 3.223621368408203 mm

Highest mean error: 3.6499338150024414 mm for frame 87

Lowest mean error: 2.9760804176330566 mm for frame 0

Saving results

Total time: 52.888269901275635
