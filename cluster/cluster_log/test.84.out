Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=84, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 4704-4759
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00591375
Iteration 2/25 | Loss: 0.00137426
Iteration 3/25 | Loss: 0.00125686
Iteration 4/25 | Loss: 0.00123487
Iteration 5/25 | Loss: 0.00123016
Iteration 6/25 | Loss: 0.00122975
Iteration 7/25 | Loss: 0.00122975
Iteration 8/25 | Loss: 0.00122975
Iteration 9/25 | Loss: 0.00122975
Iteration 10/25 | Loss: 0.00122975
Iteration 11/25 | Loss: 0.00122975
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012297480134293437, 0.0012297480134293437, 0.0012297480134293437, 0.0012297480134293437, 0.0012297480134293437]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012297480134293437

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.73138356
Iteration 2/25 | Loss: 0.00113502
Iteration 3/25 | Loss: 0.00113502
Iteration 4/25 | Loss: 0.00113502
Iteration 5/25 | Loss: 0.00113502
Iteration 6/25 | Loss: 0.00113502
Iteration 7/25 | Loss: 0.00113502
Iteration 8/25 | Loss: 0.00113502
Iteration 9/25 | Loss: 0.00113502
Iteration 10/25 | Loss: 0.00113502
Iteration 11/25 | Loss: 0.00113502
Iteration 12/25 | Loss: 0.00113502
Iteration 13/25 | Loss: 0.00113502
Iteration 14/25 | Loss: 0.00113502
Iteration 15/25 | Loss: 0.00113502
Iteration 16/25 | Loss: 0.00113502
Iteration 17/25 | Loss: 0.00113502
Iteration 18/25 | Loss: 0.00113502
Iteration 19/25 | Loss: 0.00113502
Iteration 20/25 | Loss: 0.00113502
Iteration 21/25 | Loss: 0.00113502
Iteration 22/25 | Loss: 0.00113502
Iteration 23/25 | Loss: 0.00113502
Iteration 24/25 | Loss: 0.00113502
Iteration 25/25 | Loss: 0.00113502

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113502
Iteration 2/1000 | Loss: 0.00003263
Iteration 3/1000 | Loss: 0.00002259
Iteration 4/1000 | Loss: 0.00001872
Iteration 5/1000 | Loss: 0.00001756
Iteration 6/1000 | Loss: 0.00001650
Iteration 7/1000 | Loss: 0.00001593
Iteration 8/1000 | Loss: 0.00001552
Iteration 9/1000 | Loss: 0.00001517
Iteration 10/1000 | Loss: 0.00001497
Iteration 11/1000 | Loss: 0.00001468
Iteration 12/1000 | Loss: 0.00001466
Iteration 13/1000 | Loss: 0.00001465
Iteration 14/1000 | Loss: 0.00001462
Iteration 15/1000 | Loss: 0.00001449
Iteration 16/1000 | Loss: 0.00001445
Iteration 17/1000 | Loss: 0.00001441
Iteration 18/1000 | Loss: 0.00001438
Iteration 19/1000 | Loss: 0.00001436
Iteration 20/1000 | Loss: 0.00001428
Iteration 21/1000 | Loss: 0.00001427
Iteration 22/1000 | Loss: 0.00001419
Iteration 23/1000 | Loss: 0.00001419
Iteration 24/1000 | Loss: 0.00001418
Iteration 25/1000 | Loss: 0.00001416
Iteration 26/1000 | Loss: 0.00001416
Iteration 27/1000 | Loss: 0.00001416
Iteration 28/1000 | Loss: 0.00001416
Iteration 29/1000 | Loss: 0.00001413
Iteration 30/1000 | Loss: 0.00001413
Iteration 31/1000 | Loss: 0.00001413
Iteration 32/1000 | Loss: 0.00001413
Iteration 33/1000 | Loss: 0.00001412
Iteration 34/1000 | Loss: 0.00001412
Iteration 35/1000 | Loss: 0.00001412
Iteration 36/1000 | Loss: 0.00001412
Iteration 37/1000 | Loss: 0.00001412
Iteration 38/1000 | Loss: 0.00001412
Iteration 39/1000 | Loss: 0.00001411
Iteration 40/1000 | Loss: 0.00001411
Iteration 41/1000 | Loss: 0.00001409
Iteration 42/1000 | Loss: 0.00001409
Iteration 43/1000 | Loss: 0.00001408
Iteration 44/1000 | Loss: 0.00001408
Iteration 45/1000 | Loss: 0.00001408
Iteration 46/1000 | Loss: 0.00001408
Iteration 47/1000 | Loss: 0.00001405
Iteration 48/1000 | Loss: 0.00001405
Iteration 49/1000 | Loss: 0.00001404
Iteration 50/1000 | Loss: 0.00001403
Iteration 51/1000 | Loss: 0.00001403
Iteration 52/1000 | Loss: 0.00001403
Iteration 53/1000 | Loss: 0.00001402
Iteration 54/1000 | Loss: 0.00001402
Iteration 55/1000 | Loss: 0.00001402
Iteration 56/1000 | Loss: 0.00001401
Iteration 57/1000 | Loss: 0.00001401
Iteration 58/1000 | Loss: 0.00001400
Iteration 59/1000 | Loss: 0.00001400
Iteration 60/1000 | Loss: 0.00001399
Iteration 61/1000 | Loss: 0.00001399
Iteration 62/1000 | Loss: 0.00001399
Iteration 63/1000 | Loss: 0.00001399
Iteration 64/1000 | Loss: 0.00001399
Iteration 65/1000 | Loss: 0.00001398
Iteration 66/1000 | Loss: 0.00001398
Iteration 67/1000 | Loss: 0.00001398
Iteration 68/1000 | Loss: 0.00001398
Iteration 69/1000 | Loss: 0.00001398
Iteration 70/1000 | Loss: 0.00001398
Iteration 71/1000 | Loss: 0.00001398
Iteration 72/1000 | Loss: 0.00001398
Iteration 73/1000 | Loss: 0.00001397
Iteration 74/1000 | Loss: 0.00001397
Iteration 75/1000 | Loss: 0.00001397
Iteration 76/1000 | Loss: 0.00001397
Iteration 77/1000 | Loss: 0.00001397
Iteration 78/1000 | Loss: 0.00001396
Iteration 79/1000 | Loss: 0.00001396
Iteration 80/1000 | Loss: 0.00001396
Iteration 81/1000 | Loss: 0.00001395
Iteration 82/1000 | Loss: 0.00001395
Iteration 83/1000 | Loss: 0.00001395
Iteration 84/1000 | Loss: 0.00001395
Iteration 85/1000 | Loss: 0.00001395
Iteration 86/1000 | Loss: 0.00001395
Iteration 87/1000 | Loss: 0.00001394
Iteration 88/1000 | Loss: 0.00001394
Iteration 89/1000 | Loss: 0.00001394
Iteration 90/1000 | Loss: 0.00001394
Iteration 91/1000 | Loss: 0.00001393
Iteration 92/1000 | Loss: 0.00001393
Iteration 93/1000 | Loss: 0.00001393
Iteration 94/1000 | Loss: 0.00001393
Iteration 95/1000 | Loss: 0.00001393
Iteration 96/1000 | Loss: 0.00001393
Iteration 97/1000 | Loss: 0.00001393
Iteration 98/1000 | Loss: 0.00001392
Iteration 99/1000 | Loss: 0.00001392
Iteration 100/1000 | Loss: 0.00001392
Iteration 101/1000 | Loss: 0.00001392
Iteration 102/1000 | Loss: 0.00001392
Iteration 103/1000 | Loss: 0.00001392
Iteration 104/1000 | Loss: 0.00001392
Iteration 105/1000 | Loss: 0.00001392
Iteration 106/1000 | Loss: 0.00001392
Iteration 107/1000 | Loss: 0.00001392
Iteration 108/1000 | Loss: 0.00001392
Iteration 109/1000 | Loss: 0.00001391
Iteration 110/1000 | Loss: 0.00001391
Iteration 111/1000 | Loss: 0.00001391
Iteration 112/1000 | Loss: 0.00001391
Iteration 113/1000 | Loss: 0.00001391
Iteration 114/1000 | Loss: 0.00001391
Iteration 115/1000 | Loss: 0.00001391
Iteration 116/1000 | Loss: 0.00001391
Iteration 117/1000 | Loss: 0.00001391
Iteration 118/1000 | Loss: 0.00001391
Iteration 119/1000 | Loss: 0.00001391
Iteration 120/1000 | Loss: 0.00001391
Iteration 121/1000 | Loss: 0.00001391
Iteration 122/1000 | Loss: 0.00001391
Iteration 123/1000 | Loss: 0.00001391
Iteration 124/1000 | Loss: 0.00001391
Iteration 125/1000 | Loss: 0.00001391
Iteration 126/1000 | Loss: 0.00001391
Iteration 127/1000 | Loss: 0.00001391
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.3910906091041397e-05, 1.3910906091041397e-05, 1.3910906091041397e-05, 1.3910906091041397e-05, 1.3910906091041397e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3910906091041397e-05

Optimization complete. Final v2v error: 3.2260019779205322 mm

Highest mean error: 4.134888648986816 mm for frame 168

Lowest mean error: 2.9186899662017822 mm for frame 46

Saving results

Total time: 42.68781280517578
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00863168
Iteration 2/25 | Loss: 0.00181611
Iteration 3/25 | Loss: 0.00149187
Iteration 4/25 | Loss: 0.00143548
Iteration 5/25 | Loss: 0.00145287
Iteration 6/25 | Loss: 0.00137023
Iteration 7/25 | Loss: 0.00131843
Iteration 8/25 | Loss: 0.00129844
Iteration 9/25 | Loss: 0.00128368
Iteration 10/25 | Loss: 0.00127947
Iteration 11/25 | Loss: 0.00127176
Iteration 12/25 | Loss: 0.00127333
Iteration 13/25 | Loss: 0.00126737
Iteration 14/25 | Loss: 0.00126534
Iteration 15/25 | Loss: 0.00126457
Iteration 16/25 | Loss: 0.00126433
Iteration 17/25 | Loss: 0.00126428
Iteration 18/25 | Loss: 0.00126428
Iteration 19/25 | Loss: 0.00126428
Iteration 20/25 | Loss: 0.00126428
Iteration 21/25 | Loss: 0.00126427
Iteration 22/25 | Loss: 0.00126427
Iteration 23/25 | Loss: 0.00126427
Iteration 24/25 | Loss: 0.00126427
Iteration 25/25 | Loss: 0.00126427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89199001
Iteration 2/25 | Loss: 0.00064214
Iteration 3/25 | Loss: 0.00064214
Iteration 4/25 | Loss: 0.00064214
Iteration 5/25 | Loss: 0.00064213
Iteration 6/25 | Loss: 0.00064213
Iteration 7/25 | Loss: 0.00064213
Iteration 8/25 | Loss: 0.00064213
Iteration 9/25 | Loss: 0.00064213
Iteration 10/25 | Loss: 0.00064213
Iteration 11/25 | Loss: 0.00064213
Iteration 12/25 | Loss: 0.00064213
Iteration 13/25 | Loss: 0.00064213
Iteration 14/25 | Loss: 0.00064213
Iteration 15/25 | Loss: 0.00064213
Iteration 16/25 | Loss: 0.00064213
Iteration 17/25 | Loss: 0.00064213
Iteration 18/25 | Loss: 0.00064213
Iteration 19/25 | Loss: 0.00064213
Iteration 20/25 | Loss: 0.00064213
Iteration 21/25 | Loss: 0.00064213
Iteration 22/25 | Loss: 0.00064213
Iteration 23/25 | Loss: 0.00064213
Iteration 24/25 | Loss: 0.00064213
Iteration 25/25 | Loss: 0.00064213

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064213
Iteration 2/1000 | Loss: 0.00003905
Iteration 3/1000 | Loss: 0.00003188
Iteration 4/1000 | Loss: 0.00002916
Iteration 5/1000 | Loss: 0.00002780
Iteration 6/1000 | Loss: 0.00002703
Iteration 7/1000 | Loss: 0.00002655
Iteration 8/1000 | Loss: 0.00002614
Iteration 9/1000 | Loss: 0.00002580
Iteration 10/1000 | Loss: 0.00002554
Iteration 11/1000 | Loss: 0.00002534
Iteration 12/1000 | Loss: 0.00002517
Iteration 13/1000 | Loss: 0.00002516
Iteration 14/1000 | Loss: 0.00002516
Iteration 15/1000 | Loss: 0.00002516
Iteration 16/1000 | Loss: 0.00002515
Iteration 17/1000 | Loss: 0.00002511
Iteration 18/1000 | Loss: 0.00002511
Iteration 19/1000 | Loss: 0.00002501
Iteration 20/1000 | Loss: 0.00002501
Iteration 21/1000 | Loss: 0.00002501
Iteration 22/1000 | Loss: 0.00002500
Iteration 23/1000 | Loss: 0.00002500
Iteration 24/1000 | Loss: 0.00002500
Iteration 25/1000 | Loss: 0.00002500
Iteration 26/1000 | Loss: 0.00002500
Iteration 27/1000 | Loss: 0.00002500
Iteration 28/1000 | Loss: 0.00002500
Iteration 29/1000 | Loss: 0.00002500
Iteration 30/1000 | Loss: 0.00002500
Iteration 31/1000 | Loss: 0.00002500
Iteration 32/1000 | Loss: 0.00002500
Iteration 33/1000 | Loss: 0.00002500
Iteration 34/1000 | Loss: 0.00002499
Iteration 35/1000 | Loss: 0.00002499
Iteration 36/1000 | Loss: 0.00002499
Iteration 37/1000 | Loss: 0.00002499
Iteration 38/1000 | Loss: 0.00002499
Iteration 39/1000 | Loss: 0.00002499
Iteration 40/1000 | Loss: 0.00002499
Iteration 41/1000 | Loss: 0.00002498
Iteration 42/1000 | Loss: 0.00002498
Iteration 43/1000 | Loss: 0.00002497
Iteration 44/1000 | Loss: 0.00002497
Iteration 45/1000 | Loss: 0.00002496
Iteration 46/1000 | Loss: 0.00002496
Iteration 47/1000 | Loss: 0.00002496
Iteration 48/1000 | Loss: 0.00002495
Iteration 49/1000 | Loss: 0.00002495
Iteration 50/1000 | Loss: 0.00002495
Iteration 51/1000 | Loss: 0.00002495
Iteration 52/1000 | Loss: 0.00002495
Iteration 53/1000 | Loss: 0.00002495
Iteration 54/1000 | Loss: 0.00002495
Iteration 55/1000 | Loss: 0.00002495
Iteration 56/1000 | Loss: 0.00002495
Iteration 57/1000 | Loss: 0.00002494
Iteration 58/1000 | Loss: 0.00002494
Iteration 59/1000 | Loss: 0.00002494
Iteration 60/1000 | Loss: 0.00002494
Iteration 61/1000 | Loss: 0.00002494
Iteration 62/1000 | Loss: 0.00002494
Iteration 63/1000 | Loss: 0.00002494
Iteration 64/1000 | Loss: 0.00002494
Iteration 65/1000 | Loss: 0.00002494
Iteration 66/1000 | Loss: 0.00002494
Iteration 67/1000 | Loss: 0.00002494
Iteration 68/1000 | Loss: 0.00002494
Iteration 69/1000 | Loss: 0.00002494
Iteration 70/1000 | Loss: 0.00002494
Iteration 71/1000 | Loss: 0.00002494
Iteration 72/1000 | Loss: 0.00002494
Iteration 73/1000 | Loss: 0.00002494
Iteration 74/1000 | Loss: 0.00002494
Iteration 75/1000 | Loss: 0.00002494
Iteration 76/1000 | Loss: 0.00002494
Iteration 77/1000 | Loss: 0.00002494
Iteration 78/1000 | Loss: 0.00002494
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [2.4938603019108996e-05, 2.4938603019108996e-05, 2.4938603019108996e-05, 2.4938603019108996e-05, 2.4938603019108996e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4938603019108996e-05

Optimization complete. Final v2v error: 4.202803611755371 mm

Highest mean error: 4.3007965087890625 mm for frame 140

Lowest mean error: 4.116402626037598 mm for frame 15

Saving results

Total time: 50.88850426673889
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00412453
Iteration 2/25 | Loss: 0.00133892
Iteration 3/25 | Loss: 0.00124570
Iteration 4/25 | Loss: 0.00123418
Iteration 5/25 | Loss: 0.00123058
Iteration 6/25 | Loss: 0.00122962
Iteration 7/25 | Loss: 0.00122962
Iteration 8/25 | Loss: 0.00122962
Iteration 9/25 | Loss: 0.00122962
Iteration 10/25 | Loss: 0.00122962
Iteration 11/25 | Loss: 0.00122962
Iteration 12/25 | Loss: 0.00122962
Iteration 13/25 | Loss: 0.00122962
Iteration 14/25 | Loss: 0.00122962
Iteration 15/25 | Loss: 0.00122962
Iteration 16/25 | Loss: 0.00122962
Iteration 17/25 | Loss: 0.00122962
Iteration 18/25 | Loss: 0.00122962
Iteration 19/25 | Loss: 0.00122962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0012296218192204833, 0.0012296218192204833, 0.0012296218192204833, 0.0012296218192204833, 0.0012296218192204833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012296218192204833

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32565475
Iteration 2/25 | Loss: 0.00118474
Iteration 3/25 | Loss: 0.00118474
Iteration 4/25 | Loss: 0.00118474
Iteration 5/25 | Loss: 0.00118474
Iteration 6/25 | Loss: 0.00118474
Iteration 7/25 | Loss: 0.00118474
Iteration 8/25 | Loss: 0.00118474
Iteration 9/25 | Loss: 0.00118474
Iteration 10/25 | Loss: 0.00118474
Iteration 11/25 | Loss: 0.00118474
Iteration 12/25 | Loss: 0.00118474
Iteration 13/25 | Loss: 0.00118474
Iteration 14/25 | Loss: 0.00118474
Iteration 15/25 | Loss: 0.00118474
Iteration 16/25 | Loss: 0.00118474
Iteration 17/25 | Loss: 0.00118474
Iteration 18/25 | Loss: 0.00118474
Iteration 19/25 | Loss: 0.00118474
Iteration 20/25 | Loss: 0.00118474
Iteration 21/25 | Loss: 0.00118474
Iteration 22/25 | Loss: 0.00118474
Iteration 23/25 | Loss: 0.00118474
Iteration 24/25 | Loss: 0.00118474
Iteration 25/25 | Loss: 0.00118474

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118474
Iteration 2/1000 | Loss: 0.00002637
Iteration 3/1000 | Loss: 0.00001616
Iteration 4/1000 | Loss: 0.00001440
Iteration 5/1000 | Loss: 0.00001364
Iteration 6/1000 | Loss: 0.00001314
Iteration 7/1000 | Loss: 0.00001277
Iteration 8/1000 | Loss: 0.00001251
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001228
Iteration 11/1000 | Loss: 0.00001221
Iteration 12/1000 | Loss: 0.00001210
Iteration 13/1000 | Loss: 0.00001205
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001199
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001197
Iteration 18/1000 | Loss: 0.00001197
Iteration 19/1000 | Loss: 0.00001197
Iteration 20/1000 | Loss: 0.00001197
Iteration 21/1000 | Loss: 0.00001197
Iteration 22/1000 | Loss: 0.00001197
Iteration 23/1000 | Loss: 0.00001197
Iteration 24/1000 | Loss: 0.00001197
Iteration 25/1000 | Loss: 0.00001196
Iteration 26/1000 | Loss: 0.00001196
Iteration 27/1000 | Loss: 0.00001194
Iteration 28/1000 | Loss: 0.00001193
Iteration 29/1000 | Loss: 0.00001192
Iteration 30/1000 | Loss: 0.00001191
Iteration 31/1000 | Loss: 0.00001191
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001180
Iteration 34/1000 | Loss: 0.00001179
Iteration 35/1000 | Loss: 0.00001178
Iteration 36/1000 | Loss: 0.00001178
Iteration 37/1000 | Loss: 0.00001178
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001178
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001177
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001175
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001174
Iteration 48/1000 | Loss: 0.00001174
Iteration 49/1000 | Loss: 0.00001173
Iteration 50/1000 | Loss: 0.00001173
Iteration 51/1000 | Loss: 0.00001173
Iteration 52/1000 | Loss: 0.00001172
Iteration 53/1000 | Loss: 0.00001172
Iteration 54/1000 | Loss: 0.00001172
Iteration 55/1000 | Loss: 0.00001172
Iteration 56/1000 | Loss: 0.00001172
Iteration 57/1000 | Loss: 0.00001172
Iteration 58/1000 | Loss: 0.00001172
Iteration 59/1000 | Loss: 0.00001171
Iteration 60/1000 | Loss: 0.00001171
Iteration 61/1000 | Loss: 0.00001171
Iteration 62/1000 | Loss: 0.00001171
Iteration 63/1000 | Loss: 0.00001170
Iteration 64/1000 | Loss: 0.00001170
Iteration 65/1000 | Loss: 0.00001170
Iteration 66/1000 | Loss: 0.00001169
Iteration 67/1000 | Loss: 0.00001169
Iteration 68/1000 | Loss: 0.00001169
Iteration 69/1000 | Loss: 0.00001168
Iteration 70/1000 | Loss: 0.00001168
Iteration 71/1000 | Loss: 0.00001168
Iteration 72/1000 | Loss: 0.00001168
Iteration 73/1000 | Loss: 0.00001168
Iteration 74/1000 | Loss: 0.00001168
Iteration 75/1000 | Loss: 0.00001168
Iteration 76/1000 | Loss: 0.00001167
Iteration 77/1000 | Loss: 0.00001167
Iteration 78/1000 | Loss: 0.00001167
Iteration 79/1000 | Loss: 0.00001167
Iteration 80/1000 | Loss: 0.00001167
Iteration 81/1000 | Loss: 0.00001167
Iteration 82/1000 | Loss: 0.00001167
Iteration 83/1000 | Loss: 0.00001167
Iteration 84/1000 | Loss: 0.00001167
Iteration 85/1000 | Loss: 0.00001166
Iteration 86/1000 | Loss: 0.00001165
Iteration 87/1000 | Loss: 0.00001165
Iteration 88/1000 | Loss: 0.00001164
Iteration 89/1000 | Loss: 0.00001164
Iteration 90/1000 | Loss: 0.00001164
Iteration 91/1000 | Loss: 0.00001164
Iteration 92/1000 | Loss: 0.00001164
Iteration 93/1000 | Loss: 0.00001164
Iteration 94/1000 | Loss: 0.00001163
Iteration 95/1000 | Loss: 0.00001163
Iteration 96/1000 | Loss: 0.00001163
Iteration 97/1000 | Loss: 0.00001163
Iteration 98/1000 | Loss: 0.00001163
Iteration 99/1000 | Loss: 0.00001163
Iteration 100/1000 | Loss: 0.00001163
Iteration 101/1000 | Loss: 0.00001162
Iteration 102/1000 | Loss: 0.00001161
Iteration 103/1000 | Loss: 0.00001161
Iteration 104/1000 | Loss: 0.00001161
Iteration 105/1000 | Loss: 0.00001160
Iteration 106/1000 | Loss: 0.00001160
Iteration 107/1000 | Loss: 0.00001160
Iteration 108/1000 | Loss: 0.00001160
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001158
Iteration 113/1000 | Loss: 0.00001158
Iteration 114/1000 | Loss: 0.00001158
Iteration 115/1000 | Loss: 0.00001158
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001157
Iteration 118/1000 | Loss: 0.00001157
Iteration 119/1000 | Loss: 0.00001157
Iteration 120/1000 | Loss: 0.00001157
Iteration 121/1000 | Loss: 0.00001157
Iteration 122/1000 | Loss: 0.00001157
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001157
Iteration 125/1000 | Loss: 0.00001157
Iteration 126/1000 | Loss: 0.00001157
Iteration 127/1000 | Loss: 0.00001157
Iteration 128/1000 | Loss: 0.00001157
Iteration 129/1000 | Loss: 0.00001157
Iteration 130/1000 | Loss: 0.00001157
Iteration 131/1000 | Loss: 0.00001157
Iteration 132/1000 | Loss: 0.00001157
Iteration 133/1000 | Loss: 0.00001157
Iteration 134/1000 | Loss: 0.00001157
Iteration 135/1000 | Loss: 0.00001157
Iteration 136/1000 | Loss: 0.00001157
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 136. Stopping optimization.
Last 5 losses: [1.1566545254027005e-05, 1.1566545254027005e-05, 1.1566545254027005e-05, 1.1566545254027005e-05, 1.1566545254027005e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1566545254027005e-05

Optimization complete. Final v2v error: 2.906083822250366 mm

Highest mean error: 3.289599657058716 mm for frame 4

Lowest mean error: 2.7357585430145264 mm for frame 114

Saving results

Total time: 35.00230026245117
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01045802
Iteration 2/25 | Loss: 0.00286669
Iteration 3/25 | Loss: 0.00192777
Iteration 4/25 | Loss: 0.00175898
Iteration 5/25 | Loss: 0.00175938
Iteration 6/25 | Loss: 0.00168885
Iteration 7/25 | Loss: 0.00166944
Iteration 8/25 | Loss: 0.00165141
Iteration 9/25 | Loss: 0.00164099
Iteration 10/25 | Loss: 0.00164358
Iteration 11/25 | Loss: 0.00163660
Iteration 12/25 | Loss: 0.00162971
Iteration 13/25 | Loss: 0.00162652
Iteration 14/25 | Loss: 0.00162405
Iteration 15/25 | Loss: 0.00162401
Iteration 16/25 | Loss: 0.00162336
Iteration 17/25 | Loss: 0.00162553
Iteration 18/25 | Loss: 0.00162418
Iteration 19/25 | Loss: 0.00162488
Iteration 20/25 | Loss: 0.00162390
Iteration 21/25 | Loss: 0.00162509
Iteration 22/25 | Loss: 0.00162470
Iteration 23/25 | Loss: 0.00162431
Iteration 24/25 | Loss: 0.00162436
Iteration 25/25 | Loss: 0.00162434

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14513850
Iteration 2/25 | Loss: 0.00494010
Iteration 3/25 | Loss: 0.00430729
Iteration 4/25 | Loss: 0.00430729
Iteration 5/25 | Loss: 0.00430729
Iteration 6/25 | Loss: 0.00430729
Iteration 7/25 | Loss: 0.00430729
Iteration 8/25 | Loss: 0.00430729
Iteration 9/25 | Loss: 0.00430728
Iteration 10/25 | Loss: 0.00430728
Iteration 11/25 | Loss: 0.00430728
Iteration 12/25 | Loss: 0.00430728
Iteration 13/25 | Loss: 0.00430728
Iteration 14/25 | Loss: 0.00430728
Iteration 15/25 | Loss: 0.00430728
Iteration 16/25 | Loss: 0.00430728
Iteration 17/25 | Loss: 0.00430728
Iteration 18/25 | Loss: 0.00430728
Iteration 19/25 | Loss: 0.00430728
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.00430728355422616, 0.00430728355422616, 0.00430728355422616, 0.00430728355422616, 0.00430728355422616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00430728355422616

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00430728
Iteration 2/1000 | Loss: 0.00170731
Iteration 3/1000 | Loss: 0.00161461
Iteration 4/1000 | Loss: 0.00391597
Iteration 5/1000 | Loss: 0.00107256
Iteration 6/1000 | Loss: 0.00184768
Iteration 7/1000 | Loss: 0.00146138
Iteration 8/1000 | Loss: 0.00027080
Iteration 9/1000 | Loss: 0.00029079
Iteration 10/1000 | Loss: 0.00018501
Iteration 11/1000 | Loss: 0.00026436
Iteration 12/1000 | Loss: 0.00015316
Iteration 13/1000 | Loss: 0.00022667
Iteration 14/1000 | Loss: 0.00014102
Iteration 15/1000 | Loss: 0.00081188
Iteration 16/1000 | Loss: 0.00026992
Iteration 17/1000 | Loss: 0.00013362
Iteration 18/1000 | Loss: 0.00012454
Iteration 19/1000 | Loss: 0.00011798
Iteration 20/1000 | Loss: 0.00596501
Iteration 21/1000 | Loss: 0.00200970
Iteration 22/1000 | Loss: 0.00334700
Iteration 23/1000 | Loss: 0.00066949
Iteration 24/1000 | Loss: 0.00144365
Iteration 25/1000 | Loss: 0.00031873
Iteration 26/1000 | Loss: 0.00041330
Iteration 27/1000 | Loss: 0.00016927
Iteration 28/1000 | Loss: 0.00009553
Iteration 29/1000 | Loss: 0.00006509
Iteration 30/1000 | Loss: 0.00005351
Iteration 31/1000 | Loss: 0.00004732
Iteration 32/1000 | Loss: 0.00051650
Iteration 33/1000 | Loss: 0.00006256
Iteration 34/1000 | Loss: 0.00004530
Iteration 35/1000 | Loss: 0.00004062
Iteration 36/1000 | Loss: 0.00003862
Iteration 37/1000 | Loss: 0.00003717
Iteration 38/1000 | Loss: 0.00113503
Iteration 39/1000 | Loss: 0.00007698
Iteration 40/1000 | Loss: 0.00016775
Iteration 41/1000 | Loss: 0.00004442
Iteration 42/1000 | Loss: 0.00003874
Iteration 43/1000 | Loss: 0.00042741
Iteration 44/1000 | Loss: 0.00006751
Iteration 45/1000 | Loss: 0.00003719
Iteration 46/1000 | Loss: 0.00024933
Iteration 47/1000 | Loss: 0.00008222
Iteration 48/1000 | Loss: 0.00022522
Iteration 49/1000 | Loss: 0.00043653
Iteration 50/1000 | Loss: 0.00041140
Iteration 51/1000 | Loss: 0.00013703
Iteration 52/1000 | Loss: 0.00004629
Iteration 53/1000 | Loss: 0.00019961
Iteration 54/1000 | Loss: 0.00003909
Iteration 55/1000 | Loss: 0.00003329
Iteration 56/1000 | Loss: 0.00003137
Iteration 57/1000 | Loss: 0.00003063
Iteration 58/1000 | Loss: 0.00003017
Iteration 59/1000 | Loss: 0.00013796
Iteration 60/1000 | Loss: 0.00003090
Iteration 61/1000 | Loss: 0.00002953
Iteration 62/1000 | Loss: 0.00002947
Iteration 63/1000 | Loss: 0.00002934
Iteration 64/1000 | Loss: 0.00002932
Iteration 65/1000 | Loss: 0.00002910
Iteration 66/1000 | Loss: 0.00002905
Iteration 67/1000 | Loss: 0.00002899
Iteration 68/1000 | Loss: 0.00002878
Iteration 69/1000 | Loss: 0.00002873
Iteration 70/1000 | Loss: 0.00002872
Iteration 71/1000 | Loss: 0.00002872
Iteration 72/1000 | Loss: 0.00002869
Iteration 73/1000 | Loss: 0.00002868
Iteration 74/1000 | Loss: 0.00002867
Iteration 75/1000 | Loss: 0.00002866
Iteration 76/1000 | Loss: 0.00002866
Iteration 77/1000 | Loss: 0.00002865
Iteration 78/1000 | Loss: 0.00002864
Iteration 79/1000 | Loss: 0.00002864
Iteration 80/1000 | Loss: 0.00002863
Iteration 81/1000 | Loss: 0.00002863
Iteration 82/1000 | Loss: 0.00002862
Iteration 83/1000 | Loss: 0.00002861
Iteration 84/1000 | Loss: 0.00002860
Iteration 85/1000 | Loss: 0.00002860
Iteration 86/1000 | Loss: 0.00002860
Iteration 87/1000 | Loss: 0.00002857
Iteration 88/1000 | Loss: 0.00002856
Iteration 89/1000 | Loss: 0.00002856
Iteration 90/1000 | Loss: 0.00002855
Iteration 91/1000 | Loss: 0.00002855
Iteration 92/1000 | Loss: 0.00002854
Iteration 93/1000 | Loss: 0.00002854
Iteration 94/1000 | Loss: 0.00002854
Iteration 95/1000 | Loss: 0.00002853
Iteration 96/1000 | Loss: 0.00002853
Iteration 97/1000 | Loss: 0.00002853
Iteration 98/1000 | Loss: 0.00002852
Iteration 99/1000 | Loss: 0.00002852
Iteration 100/1000 | Loss: 0.00002851
Iteration 101/1000 | Loss: 0.00002851
Iteration 102/1000 | Loss: 0.00002849
Iteration 103/1000 | Loss: 0.00002848
Iteration 104/1000 | Loss: 0.00002848
Iteration 105/1000 | Loss: 0.00002847
Iteration 106/1000 | Loss: 0.00002846
Iteration 107/1000 | Loss: 0.00002846
Iteration 108/1000 | Loss: 0.00002845
Iteration 109/1000 | Loss: 0.00002845
Iteration 110/1000 | Loss: 0.00002844
Iteration 111/1000 | Loss: 0.00002843
Iteration 112/1000 | Loss: 0.00002842
Iteration 113/1000 | Loss: 0.00002842
Iteration 114/1000 | Loss: 0.00002842
Iteration 115/1000 | Loss: 0.00002841
Iteration 116/1000 | Loss: 0.00002841
Iteration 117/1000 | Loss: 0.00002840
Iteration 118/1000 | Loss: 0.00002840
Iteration 119/1000 | Loss: 0.00002840
Iteration 120/1000 | Loss: 0.00002839
Iteration 121/1000 | Loss: 0.00002838
Iteration 122/1000 | Loss: 0.00002838
Iteration 123/1000 | Loss: 0.00002838
Iteration 124/1000 | Loss: 0.00002837
Iteration 125/1000 | Loss: 0.00002837
Iteration 126/1000 | Loss: 0.00002836
Iteration 127/1000 | Loss: 0.00002836
Iteration 128/1000 | Loss: 0.00002836
Iteration 129/1000 | Loss: 0.00002836
Iteration 130/1000 | Loss: 0.00002835
Iteration 131/1000 | Loss: 0.00002835
Iteration 132/1000 | Loss: 0.00002835
Iteration 133/1000 | Loss: 0.00002834
Iteration 134/1000 | Loss: 0.00002834
Iteration 135/1000 | Loss: 0.00002834
Iteration 136/1000 | Loss: 0.00002834
Iteration 137/1000 | Loss: 0.00002833
Iteration 138/1000 | Loss: 0.00002833
Iteration 139/1000 | Loss: 0.00002833
Iteration 140/1000 | Loss: 0.00002833
Iteration 141/1000 | Loss: 0.00002832
Iteration 142/1000 | Loss: 0.00002832
Iteration 143/1000 | Loss: 0.00002832
Iteration 144/1000 | Loss: 0.00002832
Iteration 145/1000 | Loss: 0.00002832
Iteration 146/1000 | Loss: 0.00002832
Iteration 147/1000 | Loss: 0.00002832
Iteration 148/1000 | Loss: 0.00002832
Iteration 149/1000 | Loss: 0.00002832
Iteration 150/1000 | Loss: 0.00002832
Iteration 151/1000 | Loss: 0.00002831
Iteration 152/1000 | Loss: 0.00002831
Iteration 153/1000 | Loss: 0.00002831
Iteration 154/1000 | Loss: 0.00002831
Iteration 155/1000 | Loss: 0.00002830
Iteration 156/1000 | Loss: 0.00002830
Iteration 157/1000 | Loss: 0.00002830
Iteration 158/1000 | Loss: 0.00002830
Iteration 159/1000 | Loss: 0.00002830
Iteration 160/1000 | Loss: 0.00002830
Iteration 161/1000 | Loss: 0.00002829
Iteration 162/1000 | Loss: 0.00002829
Iteration 163/1000 | Loss: 0.00002829
Iteration 164/1000 | Loss: 0.00002829
Iteration 165/1000 | Loss: 0.00002829
Iteration 166/1000 | Loss: 0.00002829
Iteration 167/1000 | Loss: 0.00002828
Iteration 168/1000 | Loss: 0.00002828
Iteration 169/1000 | Loss: 0.00002828
Iteration 170/1000 | Loss: 0.00002828
Iteration 171/1000 | Loss: 0.00002828
Iteration 172/1000 | Loss: 0.00002828
Iteration 173/1000 | Loss: 0.00002828
Iteration 174/1000 | Loss: 0.00002828
Iteration 175/1000 | Loss: 0.00002828
Iteration 176/1000 | Loss: 0.00002828
Iteration 177/1000 | Loss: 0.00002828
Iteration 178/1000 | Loss: 0.00002828
Iteration 179/1000 | Loss: 0.00002828
Iteration 180/1000 | Loss: 0.00002828
Iteration 181/1000 | Loss: 0.00002828
Iteration 182/1000 | Loss: 0.00002828
Iteration 183/1000 | Loss: 0.00002828
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 183. Stopping optimization.
Last 5 losses: [2.8279810067033395e-05, 2.8279810067033395e-05, 2.8279810067033395e-05, 2.8279810067033395e-05, 2.8279810067033395e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8279810067033395e-05

Optimization complete. Final v2v error: 3.8406405448913574 mm

Highest mean error: 11.960957527160645 mm for frame 66

Lowest mean error: 2.8314459323883057 mm for frame 139

Saving results

Total time: 148.28538155555725
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1055/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1055.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1055
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00815683
Iteration 2/25 | Loss: 0.00129318
Iteration 3/25 | Loss: 0.00119635
Iteration 4/25 | Loss: 0.00118626
Iteration 5/25 | Loss: 0.00118457
Iteration 6/25 | Loss: 0.00118427
Iteration 7/25 | Loss: 0.00118427
Iteration 8/25 | Loss: 0.00118427
Iteration 9/25 | Loss: 0.00118427
Iteration 10/25 | Loss: 0.00118427
Iteration 11/25 | Loss: 0.00118427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011842686217278242, 0.0011842686217278242, 0.0011842686217278242, 0.0011842686217278242, 0.0011842686217278242]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011842686217278242

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31819153
Iteration 2/25 | Loss: 0.00102736
Iteration 3/25 | Loss: 0.00102734
Iteration 4/25 | Loss: 0.00102734
Iteration 5/25 | Loss: 0.00102734
Iteration 6/25 | Loss: 0.00102734
Iteration 7/25 | Loss: 0.00102734
Iteration 8/25 | Loss: 0.00102734
Iteration 9/25 | Loss: 0.00102734
Iteration 10/25 | Loss: 0.00102733
Iteration 11/25 | Loss: 0.00102733
Iteration 12/25 | Loss: 0.00102733
Iteration 13/25 | Loss: 0.00102733
Iteration 14/25 | Loss: 0.00102733
Iteration 15/25 | Loss: 0.00102733
Iteration 16/25 | Loss: 0.00102733
Iteration 17/25 | Loss: 0.00102733
Iteration 18/25 | Loss: 0.00102733
Iteration 19/25 | Loss: 0.00102733
Iteration 20/25 | Loss: 0.00102733
Iteration 21/25 | Loss: 0.00102733
Iteration 22/25 | Loss: 0.00102733
Iteration 23/25 | Loss: 0.00102733
Iteration 24/25 | Loss: 0.00102733
Iteration 25/25 | Loss: 0.00102733

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00102733
Iteration 2/1000 | Loss: 0.00002057
Iteration 3/1000 | Loss: 0.00001334
Iteration 4/1000 | Loss: 0.00001164
Iteration 5/1000 | Loss: 0.00001059
Iteration 6/1000 | Loss: 0.00001001
Iteration 7/1000 | Loss: 0.00000952
Iteration 8/1000 | Loss: 0.00000926
Iteration 9/1000 | Loss: 0.00000917
Iteration 10/1000 | Loss: 0.00000906
Iteration 11/1000 | Loss: 0.00000905
Iteration 12/1000 | Loss: 0.00000896
Iteration 13/1000 | Loss: 0.00000895
Iteration 14/1000 | Loss: 0.00000888
Iteration 15/1000 | Loss: 0.00000887
Iteration 16/1000 | Loss: 0.00000887
Iteration 17/1000 | Loss: 0.00000886
Iteration 18/1000 | Loss: 0.00000886
Iteration 19/1000 | Loss: 0.00000882
Iteration 20/1000 | Loss: 0.00000882
Iteration 21/1000 | Loss: 0.00000880
Iteration 22/1000 | Loss: 0.00000880
Iteration 23/1000 | Loss: 0.00000879
Iteration 24/1000 | Loss: 0.00000877
Iteration 25/1000 | Loss: 0.00000876
Iteration 26/1000 | Loss: 0.00000875
Iteration 27/1000 | Loss: 0.00000874
Iteration 28/1000 | Loss: 0.00000874
Iteration 29/1000 | Loss: 0.00000874
Iteration 30/1000 | Loss: 0.00000873
Iteration 31/1000 | Loss: 0.00000873
Iteration 32/1000 | Loss: 0.00000872
Iteration 33/1000 | Loss: 0.00000870
Iteration 34/1000 | Loss: 0.00000868
Iteration 35/1000 | Loss: 0.00000866
Iteration 36/1000 | Loss: 0.00000866
Iteration 37/1000 | Loss: 0.00000865
Iteration 38/1000 | Loss: 0.00000862
Iteration 39/1000 | Loss: 0.00000862
Iteration 40/1000 | Loss: 0.00000861
Iteration 41/1000 | Loss: 0.00000861
Iteration 42/1000 | Loss: 0.00000860
Iteration 43/1000 | Loss: 0.00000857
Iteration 44/1000 | Loss: 0.00000857
Iteration 45/1000 | Loss: 0.00000857
Iteration 46/1000 | Loss: 0.00000857
Iteration 47/1000 | Loss: 0.00000857
Iteration 48/1000 | Loss: 0.00000857
Iteration 49/1000 | Loss: 0.00000856
Iteration 50/1000 | Loss: 0.00000856
Iteration 51/1000 | Loss: 0.00000856
Iteration 52/1000 | Loss: 0.00000855
Iteration 53/1000 | Loss: 0.00000855
Iteration 54/1000 | Loss: 0.00000855
Iteration 55/1000 | Loss: 0.00000855
Iteration 56/1000 | Loss: 0.00000854
Iteration 57/1000 | Loss: 0.00000854
Iteration 58/1000 | Loss: 0.00000853
Iteration 59/1000 | Loss: 0.00000853
Iteration 60/1000 | Loss: 0.00000853
Iteration 61/1000 | Loss: 0.00000853
Iteration 62/1000 | Loss: 0.00000853
Iteration 63/1000 | Loss: 0.00000853
Iteration 64/1000 | Loss: 0.00000853
Iteration 65/1000 | Loss: 0.00000853
Iteration 66/1000 | Loss: 0.00000853
Iteration 67/1000 | Loss: 0.00000852
Iteration 68/1000 | Loss: 0.00000852
Iteration 69/1000 | Loss: 0.00000852
Iteration 70/1000 | Loss: 0.00000852
Iteration 71/1000 | Loss: 0.00000852
Iteration 72/1000 | Loss: 0.00000852
Iteration 73/1000 | Loss: 0.00000852
Iteration 74/1000 | Loss: 0.00000852
Iteration 75/1000 | Loss: 0.00000852
Iteration 76/1000 | Loss: 0.00000852
Iteration 77/1000 | Loss: 0.00000852
Iteration 78/1000 | Loss: 0.00000852
Iteration 79/1000 | Loss: 0.00000852
Iteration 80/1000 | Loss: 0.00000852
Iteration 81/1000 | Loss: 0.00000852
Iteration 82/1000 | Loss: 0.00000852
Iteration 83/1000 | Loss: 0.00000852
Iteration 84/1000 | Loss: 0.00000851
Iteration 85/1000 | Loss: 0.00000851
Iteration 86/1000 | Loss: 0.00000851
Iteration 87/1000 | Loss: 0.00000851
Iteration 88/1000 | Loss: 0.00000851
Iteration 89/1000 | Loss: 0.00000850
Iteration 90/1000 | Loss: 0.00000850
Iteration 91/1000 | Loss: 0.00000850
Iteration 92/1000 | Loss: 0.00000850
Iteration 93/1000 | Loss: 0.00000850
Iteration 94/1000 | Loss: 0.00000850
Iteration 95/1000 | Loss: 0.00000850
Iteration 96/1000 | Loss: 0.00000850
Iteration 97/1000 | Loss: 0.00000849
Iteration 98/1000 | Loss: 0.00000849
Iteration 99/1000 | Loss: 0.00000849
Iteration 100/1000 | Loss: 0.00000849
Iteration 101/1000 | Loss: 0.00000849
Iteration 102/1000 | Loss: 0.00000848
Iteration 103/1000 | Loss: 0.00000848
Iteration 104/1000 | Loss: 0.00000848
Iteration 105/1000 | Loss: 0.00000848
Iteration 106/1000 | Loss: 0.00000848
Iteration 107/1000 | Loss: 0.00000847
Iteration 108/1000 | Loss: 0.00000847
Iteration 109/1000 | Loss: 0.00000847
Iteration 110/1000 | Loss: 0.00000846
Iteration 111/1000 | Loss: 0.00000846
Iteration 112/1000 | Loss: 0.00000845
Iteration 113/1000 | Loss: 0.00000845
Iteration 114/1000 | Loss: 0.00000845
Iteration 115/1000 | Loss: 0.00000845
Iteration 116/1000 | Loss: 0.00000844
Iteration 117/1000 | Loss: 0.00000844
Iteration 118/1000 | Loss: 0.00000844
Iteration 119/1000 | Loss: 0.00000843
Iteration 120/1000 | Loss: 0.00000843
Iteration 121/1000 | Loss: 0.00000843
Iteration 122/1000 | Loss: 0.00000843
Iteration 123/1000 | Loss: 0.00000843
Iteration 124/1000 | Loss: 0.00000843
Iteration 125/1000 | Loss: 0.00000843
Iteration 126/1000 | Loss: 0.00000843
Iteration 127/1000 | Loss: 0.00000842
Iteration 128/1000 | Loss: 0.00000842
Iteration 129/1000 | Loss: 0.00000842
Iteration 130/1000 | Loss: 0.00000842
Iteration 131/1000 | Loss: 0.00000842
Iteration 132/1000 | Loss: 0.00000842
Iteration 133/1000 | Loss: 0.00000842
Iteration 134/1000 | Loss: 0.00000840
Iteration 135/1000 | Loss: 0.00000840
Iteration 136/1000 | Loss: 0.00000840
Iteration 137/1000 | Loss: 0.00000840
Iteration 138/1000 | Loss: 0.00000840
Iteration 139/1000 | Loss: 0.00000840
Iteration 140/1000 | Loss: 0.00000840
Iteration 141/1000 | Loss: 0.00000840
Iteration 142/1000 | Loss: 0.00000840
Iteration 143/1000 | Loss: 0.00000839
Iteration 144/1000 | Loss: 0.00000839
Iteration 145/1000 | Loss: 0.00000839
Iteration 146/1000 | Loss: 0.00000839
Iteration 147/1000 | Loss: 0.00000839
Iteration 148/1000 | Loss: 0.00000839
Iteration 149/1000 | Loss: 0.00000839
Iteration 150/1000 | Loss: 0.00000839
Iteration 151/1000 | Loss: 0.00000839
Iteration 152/1000 | Loss: 0.00000839
Iteration 153/1000 | Loss: 0.00000839
Iteration 154/1000 | Loss: 0.00000839
Iteration 155/1000 | Loss: 0.00000839
Iteration 156/1000 | Loss: 0.00000839
Iteration 157/1000 | Loss: 0.00000839
Iteration 158/1000 | Loss: 0.00000839
Iteration 159/1000 | Loss: 0.00000839
Iteration 160/1000 | Loss: 0.00000839
Iteration 161/1000 | Loss: 0.00000839
Iteration 162/1000 | Loss: 0.00000839
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [8.390551556658465e-06, 8.390551556658465e-06, 8.390551556658465e-06, 8.390551556658465e-06, 8.390551556658465e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.390551556658465e-06

Optimization complete. Final v2v error: 2.508713960647583 mm

Highest mean error: 2.6607203483581543 mm for frame 22

Lowest mean error: 2.4050395488739014 mm for frame 64

Saving results

Total time: 33.70618987083435
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00975509
Iteration 2/25 | Loss: 0.00190277
Iteration 3/25 | Loss: 0.00141724
Iteration 4/25 | Loss: 0.00134867
Iteration 5/25 | Loss: 0.00132108
Iteration 6/25 | Loss: 0.00130541
Iteration 7/25 | Loss: 0.00127035
Iteration 8/25 | Loss: 0.00126259
Iteration 9/25 | Loss: 0.00126034
Iteration 10/25 | Loss: 0.00126606
Iteration 11/25 | Loss: 0.00126080
Iteration 12/25 | Loss: 0.00126411
Iteration 13/25 | Loss: 0.00125633
Iteration 14/25 | Loss: 0.00125061
Iteration 15/25 | Loss: 0.00124922
Iteration 16/25 | Loss: 0.00125135
Iteration 17/25 | Loss: 0.00124862
Iteration 18/25 | Loss: 0.00124751
Iteration 19/25 | Loss: 0.00124684
Iteration 20/25 | Loss: 0.00124680
Iteration 21/25 | Loss: 0.00124654
Iteration 22/25 | Loss: 0.00124665
Iteration 23/25 | Loss: 0.00124708
Iteration 24/25 | Loss: 0.00124677
Iteration 25/25 | Loss: 0.00124536

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32967889
Iteration 2/25 | Loss: 0.00142618
Iteration 3/25 | Loss: 0.00141691
Iteration 4/25 | Loss: 0.00141691
Iteration 5/25 | Loss: 0.00141691
Iteration 6/25 | Loss: 0.00141691
Iteration 7/25 | Loss: 0.00141690
Iteration 8/25 | Loss: 0.00141690
Iteration 9/25 | Loss: 0.00141690
Iteration 10/25 | Loss: 0.00141690
Iteration 11/25 | Loss: 0.00141690
Iteration 12/25 | Loss: 0.00141690
Iteration 13/25 | Loss: 0.00141690
Iteration 14/25 | Loss: 0.00141690
Iteration 15/25 | Loss: 0.00141690
Iteration 16/25 | Loss: 0.00141690
Iteration 17/25 | Loss: 0.00141690
Iteration 18/25 | Loss: 0.00141690
Iteration 19/25 | Loss: 0.00141690
Iteration 20/25 | Loss: 0.00141690
Iteration 21/25 | Loss: 0.00141690
Iteration 22/25 | Loss: 0.00141690
Iteration 23/25 | Loss: 0.00141690
Iteration 24/25 | Loss: 0.00141690
Iteration 25/25 | Loss: 0.00141690
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0014169032219797373, 0.0014169032219797373, 0.0014169032219797373, 0.0014169032219797373, 0.0014169032219797373]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014169032219797373

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00141690
Iteration 2/1000 | Loss: 0.00004356
Iteration 3/1000 | Loss: 0.00003388
Iteration 4/1000 | Loss: 0.00002174
Iteration 5/1000 | Loss: 0.00002631
Iteration 6/1000 | Loss: 0.00001843
Iteration 7/1000 | Loss: 0.00001788
Iteration 8/1000 | Loss: 0.00001837
Iteration 9/1000 | Loss: 0.00002633
Iteration 10/1000 | Loss: 0.00001540
Iteration 11/1000 | Loss: 0.00001504
Iteration 12/1000 | Loss: 0.00001473
Iteration 13/1000 | Loss: 0.00001457
Iteration 14/1000 | Loss: 0.00001456
Iteration 15/1000 | Loss: 0.00001451
Iteration 16/1000 | Loss: 0.00001448
Iteration 17/1000 | Loss: 0.00001436
Iteration 18/1000 | Loss: 0.00001431
Iteration 19/1000 | Loss: 0.00001430
Iteration 20/1000 | Loss: 0.00001429
Iteration 21/1000 | Loss: 0.00001420
Iteration 22/1000 | Loss: 0.00002556
Iteration 23/1000 | Loss: 0.00001417
Iteration 24/1000 | Loss: 0.00001412
Iteration 25/1000 | Loss: 0.00001410
Iteration 26/1000 | Loss: 0.00001410
Iteration 27/1000 | Loss: 0.00001410
Iteration 28/1000 | Loss: 0.00001409
Iteration 29/1000 | Loss: 0.00001409
Iteration 30/1000 | Loss: 0.00001408
Iteration 31/1000 | Loss: 0.00001405
Iteration 32/1000 | Loss: 0.00001405
Iteration 33/1000 | Loss: 0.00001404
Iteration 34/1000 | Loss: 0.00001404
Iteration 35/1000 | Loss: 0.00001403
Iteration 36/1000 | Loss: 0.00001403
Iteration 37/1000 | Loss: 0.00001402
Iteration 38/1000 | Loss: 0.00001401
Iteration 39/1000 | Loss: 0.00001401
Iteration 40/1000 | Loss: 0.00001401
Iteration 41/1000 | Loss: 0.00001401
Iteration 42/1000 | Loss: 0.00001400
Iteration 43/1000 | Loss: 0.00001399
Iteration 44/1000 | Loss: 0.00001399
Iteration 45/1000 | Loss: 0.00001398
Iteration 46/1000 | Loss: 0.00001398
Iteration 47/1000 | Loss: 0.00001397
Iteration 48/1000 | Loss: 0.00001397
Iteration 49/1000 | Loss: 0.00001396
Iteration 50/1000 | Loss: 0.00001395
Iteration 51/1000 | Loss: 0.00001394
Iteration 52/1000 | Loss: 0.00002356
Iteration 53/1000 | Loss: 0.00001685
Iteration 54/1000 | Loss: 0.00001434
Iteration 55/1000 | Loss: 0.00001386
Iteration 56/1000 | Loss: 0.00001386
Iteration 57/1000 | Loss: 0.00001386
Iteration 58/1000 | Loss: 0.00001386
Iteration 59/1000 | Loss: 0.00001386
Iteration 60/1000 | Loss: 0.00001386
Iteration 61/1000 | Loss: 0.00001386
Iteration 62/1000 | Loss: 0.00001386
Iteration 63/1000 | Loss: 0.00001386
Iteration 64/1000 | Loss: 0.00001386
Iteration 65/1000 | Loss: 0.00001386
Iteration 66/1000 | Loss: 0.00001386
Iteration 67/1000 | Loss: 0.00001385
Iteration 68/1000 | Loss: 0.00001385
Iteration 69/1000 | Loss: 0.00001385
Iteration 70/1000 | Loss: 0.00001385
Iteration 71/1000 | Loss: 0.00001385
Iteration 72/1000 | Loss: 0.00001385
Iteration 73/1000 | Loss: 0.00001385
Iteration 74/1000 | Loss: 0.00001385
Iteration 75/1000 | Loss: 0.00001385
Iteration 76/1000 | Loss: 0.00001385
Iteration 77/1000 | Loss: 0.00001385
Iteration 78/1000 | Loss: 0.00001385
Iteration 79/1000 | Loss: 0.00001385
Iteration 80/1000 | Loss: 0.00001385
Iteration 81/1000 | Loss: 0.00001385
Iteration 82/1000 | Loss: 0.00001385
Iteration 83/1000 | Loss: 0.00001385
Iteration 84/1000 | Loss: 0.00001385
Iteration 85/1000 | Loss: 0.00001385
Iteration 86/1000 | Loss: 0.00001385
Iteration 87/1000 | Loss: 0.00001385
Iteration 88/1000 | Loss: 0.00001385
Iteration 89/1000 | Loss: 0.00001385
Iteration 90/1000 | Loss: 0.00001385
Iteration 91/1000 | Loss: 0.00001385
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 91. Stopping optimization.
Last 5 losses: [1.385024097544374e-05, 1.385024097544374e-05, 1.385024097544374e-05, 1.385024097544374e-05, 1.385024097544374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.385024097544374e-05

Optimization complete. Final v2v error: 3.0589795112609863 mm

Highest mean error: 6.030989646911621 mm for frame 0

Lowest mean error: 2.507026433944702 mm for frame 236

Saving results

Total time: 86.96389436721802
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1085/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1085.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1085
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01013594
Iteration 2/25 | Loss: 0.01013594
Iteration 3/25 | Loss: 0.00232042
Iteration 4/25 | Loss: 0.00185155
Iteration 5/25 | Loss: 0.00173759
Iteration 6/25 | Loss: 0.00166861
Iteration 7/25 | Loss: 0.00168454
Iteration 8/25 | Loss: 0.00156609
Iteration 9/25 | Loss: 0.00149206
Iteration 10/25 | Loss: 0.00141673
Iteration 11/25 | Loss: 0.00139681
Iteration 12/25 | Loss: 0.00137768
Iteration 13/25 | Loss: 0.00138149
Iteration 14/25 | Loss: 0.00137656
Iteration 15/25 | Loss: 0.00139348
Iteration 16/25 | Loss: 0.00136505
Iteration 17/25 | Loss: 0.00135862
Iteration 18/25 | Loss: 0.00135432
Iteration 19/25 | Loss: 0.00135497
Iteration 20/25 | Loss: 0.00135314
Iteration 21/25 | Loss: 0.00135204
Iteration 22/25 | Loss: 0.00135817
Iteration 23/25 | Loss: 0.00135688
Iteration 24/25 | Loss: 0.00135692
Iteration 25/25 | Loss: 0.00134922

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32188439
Iteration 2/25 | Loss: 0.00252725
Iteration 3/25 | Loss: 0.00209430
Iteration 4/25 | Loss: 0.00209430
Iteration 5/25 | Loss: 0.00209430
Iteration 6/25 | Loss: 0.00209430
Iteration 7/25 | Loss: 0.00209430
Iteration 8/25 | Loss: 0.00209430
Iteration 9/25 | Loss: 0.00209430
Iteration 10/25 | Loss: 0.00209430
Iteration 11/25 | Loss: 0.00209430
Iteration 12/25 | Loss: 0.00209430
Iteration 13/25 | Loss: 0.00209430
Iteration 14/25 | Loss: 0.00209430
Iteration 15/25 | Loss: 0.00209430
Iteration 16/25 | Loss: 0.00209430
Iteration 17/25 | Loss: 0.00209430
Iteration 18/25 | Loss: 0.00209430
Iteration 19/25 | Loss: 0.00209430
Iteration 20/25 | Loss: 0.00209430
Iteration 21/25 | Loss: 0.00209430
Iteration 22/25 | Loss: 0.00209430
Iteration 23/25 | Loss: 0.00209430
Iteration 24/25 | Loss: 0.00209430
Iteration 25/25 | Loss: 0.00209430

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00209430
Iteration 2/1000 | Loss: 0.00088206
Iteration 3/1000 | Loss: 0.00099006
Iteration 4/1000 | Loss: 0.00069168
Iteration 5/1000 | Loss: 0.00049028
Iteration 6/1000 | Loss: 0.00057701
Iteration 7/1000 | Loss: 0.00016324
Iteration 8/1000 | Loss: 0.00033613
Iteration 9/1000 | Loss: 0.00042952
Iteration 10/1000 | Loss: 0.00031232
Iteration 11/1000 | Loss: 0.00050011
Iteration 12/1000 | Loss: 0.00012279
Iteration 13/1000 | Loss: 0.00013247
Iteration 14/1000 | Loss: 0.00015082
Iteration 15/1000 | Loss: 0.00023663
Iteration 16/1000 | Loss: 0.00011625
Iteration 17/1000 | Loss: 0.00010513
Iteration 18/1000 | Loss: 0.00014710
Iteration 19/1000 | Loss: 0.00033781
Iteration 20/1000 | Loss: 0.00083035
Iteration 21/1000 | Loss: 0.00028033
Iteration 22/1000 | Loss: 0.00014966
Iteration 23/1000 | Loss: 0.00012201
Iteration 24/1000 | Loss: 0.00014703
Iteration 25/1000 | Loss: 0.00034282
Iteration 26/1000 | Loss: 0.00010903
Iteration 27/1000 | Loss: 0.00019483
Iteration 28/1000 | Loss: 0.00017220
Iteration 29/1000 | Loss: 0.00020387
Iteration 30/1000 | Loss: 0.00026270
Iteration 31/1000 | Loss: 0.00017723
Iteration 32/1000 | Loss: 0.00012820
Iteration 33/1000 | Loss: 0.00022170
Iteration 34/1000 | Loss: 0.00012026
Iteration 35/1000 | Loss: 0.00029778
Iteration 36/1000 | Loss: 0.00052559
Iteration 37/1000 | Loss: 0.00070454
Iteration 38/1000 | Loss: 0.00075547
Iteration 39/1000 | Loss: 0.00050314
Iteration 40/1000 | Loss: 0.00031641
Iteration 41/1000 | Loss: 0.00056522
Iteration 42/1000 | Loss: 0.00042070
Iteration 43/1000 | Loss: 0.00011544
Iteration 44/1000 | Loss: 0.00017104
Iteration 45/1000 | Loss: 0.00009369
Iteration 46/1000 | Loss: 0.00010728
Iteration 47/1000 | Loss: 0.00012106
Iteration 48/1000 | Loss: 0.00009979
Iteration 49/1000 | Loss: 0.00009641
Iteration 50/1000 | Loss: 0.00010591
Iteration 51/1000 | Loss: 0.00010573
Iteration 52/1000 | Loss: 0.00015694
Iteration 53/1000 | Loss: 0.00009545
Iteration 54/1000 | Loss: 0.00010900
Iteration 55/1000 | Loss: 0.00011523
Iteration 56/1000 | Loss: 0.00019232
Iteration 57/1000 | Loss: 0.00010432
Iteration 58/1000 | Loss: 0.00010204
Iteration 59/1000 | Loss: 0.00009282
Iteration 60/1000 | Loss: 0.00009926
Iteration 61/1000 | Loss: 0.00010608
Iteration 62/1000 | Loss: 0.00010679
Iteration 63/1000 | Loss: 0.00010567
Iteration 64/1000 | Loss: 0.00009777
Iteration 65/1000 | Loss: 0.00010074
Iteration 66/1000 | Loss: 0.00013805
Iteration 67/1000 | Loss: 0.00011462
Iteration 68/1000 | Loss: 0.00010052
Iteration 69/1000 | Loss: 0.00009986
Iteration 70/1000 | Loss: 0.00011222
Iteration 71/1000 | Loss: 0.00010312
Iteration 72/1000 | Loss: 0.00010397
Iteration 73/1000 | Loss: 0.00011332
Iteration 74/1000 | Loss: 0.00012326
Iteration 75/1000 | Loss: 0.00011405
Iteration 76/1000 | Loss: 0.00010578
Iteration 77/1000 | Loss: 0.00011204
Iteration 78/1000 | Loss: 0.00009900
Iteration 79/1000 | Loss: 0.00010045
Iteration 80/1000 | Loss: 0.00013421
Iteration 81/1000 | Loss: 0.00012918
Iteration 82/1000 | Loss: 0.00011208
Iteration 83/1000 | Loss: 0.00011987
Iteration 84/1000 | Loss: 0.00009150
Iteration 85/1000 | Loss: 0.00011118
Iteration 86/1000 | Loss: 0.00015273
Iteration 87/1000 | Loss: 0.00010625
Iteration 88/1000 | Loss: 0.00012300
Iteration 89/1000 | Loss: 0.00011295
Iteration 90/1000 | Loss: 0.00010906
Iteration 91/1000 | Loss: 0.00011614
Iteration 92/1000 | Loss: 0.00010450
Iteration 93/1000 | Loss: 0.00010252
Iteration 94/1000 | Loss: 0.00009909
Iteration 95/1000 | Loss: 0.00012387
Iteration 96/1000 | Loss: 0.00010306
Iteration 97/1000 | Loss: 0.00010144
Iteration 98/1000 | Loss: 0.00009715
Iteration 99/1000 | Loss: 0.00012777
Iteration 100/1000 | Loss: 0.00011311
Iteration 101/1000 | Loss: 0.00009548
Iteration 102/1000 | Loss: 0.00012427
Iteration 103/1000 | Loss: 0.00009984
Iteration 104/1000 | Loss: 0.00010551
Iteration 105/1000 | Loss: 0.00011139
Iteration 106/1000 | Loss: 0.00013583
Iteration 107/1000 | Loss: 0.00009502
Iteration 108/1000 | Loss: 0.00010241
Iteration 109/1000 | Loss: 0.00009689
Iteration 110/1000 | Loss: 0.00009596
Iteration 111/1000 | Loss: 0.00010041
Iteration 112/1000 | Loss: 0.00009888
Iteration 113/1000 | Loss: 0.00010726
Iteration 114/1000 | Loss: 0.00009831
Iteration 115/1000 | Loss: 0.00010449
Iteration 116/1000 | Loss: 0.00009750
Iteration 117/1000 | Loss: 0.00010347
Iteration 118/1000 | Loss: 0.00010086
Iteration 119/1000 | Loss: 0.00010379
Iteration 120/1000 | Loss: 0.00009880
Iteration 121/1000 | Loss: 0.00011065
Iteration 122/1000 | Loss: 0.00029950
Iteration 123/1000 | Loss: 0.00019021
Iteration 124/1000 | Loss: 0.00020376
Iteration 125/1000 | Loss: 0.00011693
Iteration 126/1000 | Loss: 0.00025029
Iteration 127/1000 | Loss: 0.00036959
Iteration 128/1000 | Loss: 0.00026590
Iteration 129/1000 | Loss: 0.00060094
Iteration 130/1000 | Loss: 0.00012513
Iteration 131/1000 | Loss: 0.00012412
Iteration 132/1000 | Loss: 0.00007695
Iteration 133/1000 | Loss: 0.00009126
Iteration 134/1000 | Loss: 0.00006858
Iteration 135/1000 | Loss: 0.00021738
Iteration 136/1000 | Loss: 0.00061307
Iteration 137/1000 | Loss: 0.00020194
Iteration 138/1000 | Loss: 0.00022438
Iteration 139/1000 | Loss: 0.00006543
Iteration 140/1000 | Loss: 0.00006695
Iteration 141/1000 | Loss: 0.00011978
Iteration 142/1000 | Loss: 0.00020366
Iteration 143/1000 | Loss: 0.00016106
Iteration 144/1000 | Loss: 0.00010196
Iteration 145/1000 | Loss: 0.00018991
Iteration 146/1000 | Loss: 0.00007145
Iteration 147/1000 | Loss: 0.00007155
Iteration 148/1000 | Loss: 0.00006123
Iteration 149/1000 | Loss: 0.00035983
Iteration 150/1000 | Loss: 0.00033518
Iteration 151/1000 | Loss: 0.00030785
Iteration 152/1000 | Loss: 0.00039293
Iteration 153/1000 | Loss: 0.00038692
Iteration 154/1000 | Loss: 0.00061635
Iteration 155/1000 | Loss: 0.00031411
Iteration 156/1000 | Loss: 0.00020054
Iteration 157/1000 | Loss: 0.00006641
Iteration 158/1000 | Loss: 0.00006176
Iteration 159/1000 | Loss: 0.00006169
Iteration 160/1000 | Loss: 0.00016348
Iteration 161/1000 | Loss: 0.00018302
Iteration 162/1000 | Loss: 0.00023694
Iteration 163/1000 | Loss: 0.00005943
Iteration 164/1000 | Loss: 0.00008698
Iteration 165/1000 | Loss: 0.00005775
Iteration 166/1000 | Loss: 0.00042631
Iteration 167/1000 | Loss: 0.00091144
Iteration 168/1000 | Loss: 0.00057788
Iteration 169/1000 | Loss: 0.00118632
Iteration 170/1000 | Loss: 0.00124175
Iteration 171/1000 | Loss: 0.00042088
Iteration 172/1000 | Loss: 0.00023309
Iteration 173/1000 | Loss: 0.00012262
Iteration 174/1000 | Loss: 0.00006249
Iteration 175/1000 | Loss: 0.00014419
Iteration 176/1000 | Loss: 0.00005610
Iteration 177/1000 | Loss: 0.00013089
Iteration 178/1000 | Loss: 0.00006734
Iteration 179/1000 | Loss: 0.00005134
Iteration 180/1000 | Loss: 0.00005045
Iteration 181/1000 | Loss: 0.00004990
Iteration 182/1000 | Loss: 0.00008917
Iteration 183/1000 | Loss: 0.00004941
Iteration 184/1000 | Loss: 0.00018086
Iteration 185/1000 | Loss: 0.00015237
Iteration 186/1000 | Loss: 0.00011743
Iteration 187/1000 | Loss: 0.00005255
Iteration 188/1000 | Loss: 0.00005074
Iteration 189/1000 | Loss: 0.00004929
Iteration 190/1000 | Loss: 0.00004829
Iteration 191/1000 | Loss: 0.00004779
Iteration 192/1000 | Loss: 0.00004748
Iteration 193/1000 | Loss: 0.00013999
Iteration 194/1000 | Loss: 0.00005214
Iteration 195/1000 | Loss: 0.00004951
Iteration 196/1000 | Loss: 0.00004792
Iteration 197/1000 | Loss: 0.00004692
Iteration 198/1000 | Loss: 0.00004624
Iteration 199/1000 | Loss: 0.00004592
Iteration 200/1000 | Loss: 0.00004580
Iteration 201/1000 | Loss: 0.00004561
Iteration 202/1000 | Loss: 0.00004559
Iteration 203/1000 | Loss: 0.00004558
Iteration 204/1000 | Loss: 0.00004557
Iteration 205/1000 | Loss: 0.00004556
Iteration 206/1000 | Loss: 0.00004555
Iteration 207/1000 | Loss: 0.00004554
Iteration 208/1000 | Loss: 0.00004554
Iteration 209/1000 | Loss: 0.00004554
Iteration 210/1000 | Loss: 0.00004554
Iteration 211/1000 | Loss: 0.00004553
Iteration 212/1000 | Loss: 0.00004553
Iteration 213/1000 | Loss: 0.00004553
Iteration 214/1000 | Loss: 0.00004552
Iteration 215/1000 | Loss: 0.00004552
Iteration 216/1000 | Loss: 0.00004551
Iteration 217/1000 | Loss: 0.00004542
Iteration 218/1000 | Loss: 0.00004541
Iteration 219/1000 | Loss: 0.00004540
Iteration 220/1000 | Loss: 0.00004540
Iteration 221/1000 | Loss: 0.00004539
Iteration 222/1000 | Loss: 0.00004539
Iteration 223/1000 | Loss: 0.00004539
Iteration 224/1000 | Loss: 0.00004539
Iteration 225/1000 | Loss: 0.00004539
Iteration 226/1000 | Loss: 0.00004539
Iteration 227/1000 | Loss: 0.00004539
Iteration 228/1000 | Loss: 0.00004539
Iteration 229/1000 | Loss: 0.00004538
Iteration 230/1000 | Loss: 0.00004538
Iteration 231/1000 | Loss: 0.00004537
Iteration 232/1000 | Loss: 0.00004537
Iteration 233/1000 | Loss: 0.00004536
Iteration 234/1000 | Loss: 0.00004536
Iteration 235/1000 | Loss: 0.00004536
Iteration 236/1000 | Loss: 0.00004536
Iteration 237/1000 | Loss: 0.00004536
Iteration 238/1000 | Loss: 0.00004535
Iteration 239/1000 | Loss: 0.00004535
Iteration 240/1000 | Loss: 0.00004535
Iteration 241/1000 | Loss: 0.00004535
Iteration 242/1000 | Loss: 0.00004535
Iteration 243/1000 | Loss: 0.00004535
Iteration 244/1000 | Loss: 0.00004535
Iteration 245/1000 | Loss: 0.00004535
Iteration 246/1000 | Loss: 0.00012598
Iteration 247/1000 | Loss: 0.00013722
Iteration 248/1000 | Loss: 0.00009869
Iteration 249/1000 | Loss: 0.00004815
Iteration 250/1000 | Loss: 0.00004591
Iteration 251/1000 | Loss: 0.00005994
Iteration 252/1000 | Loss: 0.00004464
Iteration 253/1000 | Loss: 0.00006232
Iteration 254/1000 | Loss: 0.00007003
Iteration 255/1000 | Loss: 0.00004445
Iteration 256/1000 | Loss: 0.00004402
Iteration 257/1000 | Loss: 0.00004401
Iteration 258/1000 | Loss: 0.00004391
Iteration 259/1000 | Loss: 0.00004390
Iteration 260/1000 | Loss: 0.00004894
Iteration 261/1000 | Loss: 0.00004534
Iteration 262/1000 | Loss: 0.00005285
Iteration 263/1000 | Loss: 0.00004458
Iteration 264/1000 | Loss: 0.00004993
Iteration 265/1000 | Loss: 0.00004444
Iteration 266/1000 | Loss: 0.00005262
Iteration 267/1000 | Loss: 0.00004384
Iteration 268/1000 | Loss: 0.00004380
Iteration 269/1000 | Loss: 0.00004380
Iteration 270/1000 | Loss: 0.00004380
Iteration 271/1000 | Loss: 0.00004380
Iteration 272/1000 | Loss: 0.00004380
Iteration 273/1000 | Loss: 0.00004380
Iteration 274/1000 | Loss: 0.00004379
Iteration 275/1000 | Loss: 0.00004379
Iteration 276/1000 | Loss: 0.00004378
Iteration 277/1000 | Loss: 0.00004378
Iteration 278/1000 | Loss: 0.00004378
Iteration 279/1000 | Loss: 0.00004378
Iteration 280/1000 | Loss: 0.00004378
Iteration 281/1000 | Loss: 0.00004378
Iteration 282/1000 | Loss: 0.00004378
Iteration 283/1000 | Loss: 0.00004378
Iteration 284/1000 | Loss: 0.00004378
Iteration 285/1000 | Loss: 0.00004378
Iteration 286/1000 | Loss: 0.00004378
Iteration 287/1000 | Loss: 0.00022710
Iteration 288/1000 | Loss: 0.00010014
Iteration 289/1000 | Loss: 0.00027641
Iteration 290/1000 | Loss: 0.00012861
Iteration 291/1000 | Loss: 0.00018694
Iteration 292/1000 | Loss: 0.00004720
Iteration 293/1000 | Loss: 0.00004543
Iteration 294/1000 | Loss: 0.00005744
Iteration 295/1000 | Loss: 0.00004729
Iteration 296/1000 | Loss: 0.00004500
Iteration 297/1000 | Loss: 0.00004339
Iteration 298/1000 | Loss: 0.00004672
Iteration 299/1000 | Loss: 0.00004439
Iteration 300/1000 | Loss: 0.00004717
Iteration 301/1000 | Loss: 0.00004429
Iteration 302/1000 | Loss: 0.00004471
Iteration 303/1000 | Loss: 0.00004470
Iteration 304/1000 | Loss: 0.00004631
Iteration 305/1000 | Loss: 0.00004254
Iteration 306/1000 | Loss: 0.00004253
Iteration 307/1000 | Loss: 0.00004253
Iteration 308/1000 | Loss: 0.00004253
Iteration 309/1000 | Loss: 0.00004253
Iteration 310/1000 | Loss: 0.00004253
Iteration 311/1000 | Loss: 0.00004252
Iteration 312/1000 | Loss: 0.00004252
Iteration 313/1000 | Loss: 0.00004252
Iteration 314/1000 | Loss: 0.00004252
Iteration 315/1000 | Loss: 0.00004252
Iteration 316/1000 | Loss: 0.00004251
Iteration 317/1000 | Loss: 0.00004251
Iteration 318/1000 | Loss: 0.00004249
Iteration 319/1000 | Loss: 0.00004248
Iteration 320/1000 | Loss: 0.00004247
Iteration 321/1000 | Loss: 0.00004247
Iteration 322/1000 | Loss: 0.00004247
Iteration 323/1000 | Loss: 0.00004247
Iteration 324/1000 | Loss: 0.00004247
Iteration 325/1000 | Loss: 0.00004246
Iteration 326/1000 | Loss: 0.00004245
Iteration 327/1000 | Loss: 0.00004974
Iteration 328/1000 | Loss: 0.00004246
Iteration 329/1000 | Loss: 0.00004240
Iteration 330/1000 | Loss: 0.00004240
Iteration 331/1000 | Loss: 0.00004240
Iteration 332/1000 | Loss: 0.00004240
Iteration 333/1000 | Loss: 0.00004240
Iteration 334/1000 | Loss: 0.00004240
Iteration 335/1000 | Loss: 0.00004240
Iteration 336/1000 | Loss: 0.00004240
Iteration 337/1000 | Loss: 0.00004240
Iteration 338/1000 | Loss: 0.00004240
Iteration 339/1000 | Loss: 0.00004239
Iteration 340/1000 | Loss: 0.00004239
Iteration 341/1000 | Loss: 0.00004239
Iteration 342/1000 | Loss: 0.00004239
Iteration 343/1000 | Loss: 0.00004239
Iteration 344/1000 | Loss: 0.00004239
Iteration 345/1000 | Loss: 0.00004239
Iteration 346/1000 | Loss: 0.00004239
Iteration 347/1000 | Loss: 0.00004239
Iteration 348/1000 | Loss: 0.00004238
Iteration 349/1000 | Loss: 0.00004238
Iteration 350/1000 | Loss: 0.00004237
Iteration 351/1000 | Loss: 0.00004237
Iteration 352/1000 | Loss: 0.00004237
Iteration 353/1000 | Loss: 0.00004237
Iteration 354/1000 | Loss: 0.00004237
Iteration 355/1000 | Loss: 0.00004236
Iteration 356/1000 | Loss: 0.00004236
Iteration 357/1000 | Loss: 0.00004236
Iteration 358/1000 | Loss: 0.00004236
Iteration 359/1000 | Loss: 0.00004236
Iteration 360/1000 | Loss: 0.00004236
Iteration 361/1000 | Loss: 0.00004236
Iteration 362/1000 | Loss: 0.00004236
Iteration 363/1000 | Loss: 0.00004235
Iteration 364/1000 | Loss: 0.00004235
Iteration 365/1000 | Loss: 0.00004234
Iteration 366/1000 | Loss: 0.00004234
Iteration 367/1000 | Loss: 0.00004234
Iteration 368/1000 | Loss: 0.00005784
Iteration 369/1000 | Loss: 0.00004233
Iteration 370/1000 | Loss: 0.00004232
Iteration 371/1000 | Loss: 0.00004231
Iteration 372/1000 | Loss: 0.00004230
Iteration 373/1000 | Loss: 0.00004230
Iteration 374/1000 | Loss: 0.00004230
Iteration 375/1000 | Loss: 0.00004230
Iteration 376/1000 | Loss: 0.00004230
Iteration 377/1000 | Loss: 0.00004230
Iteration 378/1000 | Loss: 0.00004230
Iteration 379/1000 | Loss: 0.00004230
Iteration 380/1000 | Loss: 0.00004230
Iteration 381/1000 | Loss: 0.00004230
Iteration 382/1000 | Loss: 0.00004230
Iteration 383/1000 | Loss: 0.00004230
Iteration 384/1000 | Loss: 0.00004229
Iteration 385/1000 | Loss: 0.00004229
Iteration 386/1000 | Loss: 0.00004229
Iteration 387/1000 | Loss: 0.00004229
Iteration 388/1000 | Loss: 0.00004229
Iteration 389/1000 | Loss: 0.00004229
Iteration 390/1000 | Loss: 0.00004229
Iteration 391/1000 | Loss: 0.00004229
Iteration 392/1000 | Loss: 0.00004229
Iteration 393/1000 | Loss: 0.00004229
Iteration 394/1000 | Loss: 0.00004229
Iteration 395/1000 | Loss: 0.00004229
Iteration 396/1000 | Loss: 0.00004229
Iteration 397/1000 | Loss: 0.00004229
Iteration 398/1000 | Loss: 0.00004229
Iteration 399/1000 | Loss: 0.00004229
Iteration 400/1000 | Loss: 0.00004229
Iteration 401/1000 | Loss: 0.00004229
Iteration 402/1000 | Loss: 0.00004229
Iteration 403/1000 | Loss: 0.00004229
Iteration 404/1000 | Loss: 0.00004229
Iteration 405/1000 | Loss: 0.00004229
Iteration 406/1000 | Loss: 0.00004229
Iteration 407/1000 | Loss: 0.00004229
Iteration 408/1000 | Loss: 0.00004229
Iteration 409/1000 | Loss: 0.00004229
Iteration 410/1000 | Loss: 0.00004229
Iteration 411/1000 | Loss: 0.00004229
Iteration 412/1000 | Loss: 0.00004229
Iteration 413/1000 | Loss: 0.00004229
Iteration 414/1000 | Loss: 0.00004229
Iteration 415/1000 | Loss: 0.00004229
Iteration 416/1000 | Loss: 0.00004229
Iteration 417/1000 | Loss: 0.00004229
Iteration 418/1000 | Loss: 0.00004229
Iteration 419/1000 | Loss: 0.00004229
Iteration 420/1000 | Loss: 0.00004229
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 420. Stopping optimization.
Last 5 losses: [4.2291925637982786e-05, 4.2291925637982786e-05, 4.2291925637982786e-05, 4.2291925637982786e-05, 4.2291925637982786e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.2291925637982786e-05

Optimization complete. Final v2v error: 3.6614317893981934 mm

Highest mean error: 11.154276847839355 mm for frame 144

Lowest mean error: 2.7200329303741455 mm for frame 26

Saving results

Total time: 434.1237347126007
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00715106
Iteration 2/25 | Loss: 0.00192628
Iteration 3/25 | Loss: 0.00140484
Iteration 4/25 | Loss: 0.00129591
Iteration 5/25 | Loss: 0.00130201
Iteration 6/25 | Loss: 0.00123872
Iteration 7/25 | Loss: 0.00123231
Iteration 8/25 | Loss: 0.00122726
Iteration 9/25 | Loss: 0.00122283
Iteration 10/25 | Loss: 0.00122366
Iteration 11/25 | Loss: 0.00121964
Iteration 12/25 | Loss: 0.00121851
Iteration 13/25 | Loss: 0.00121836
Iteration 14/25 | Loss: 0.00121835
Iteration 15/25 | Loss: 0.00121835
Iteration 16/25 | Loss: 0.00121835
Iteration 17/25 | Loss: 0.00121835
Iteration 18/25 | Loss: 0.00121835
Iteration 19/25 | Loss: 0.00121835
Iteration 20/25 | Loss: 0.00121834
Iteration 21/25 | Loss: 0.00121834
Iteration 22/25 | Loss: 0.00121834
Iteration 23/25 | Loss: 0.00121834
Iteration 24/25 | Loss: 0.00121834
Iteration 25/25 | Loss: 0.00121834

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28217852
Iteration 2/25 | Loss: 0.00116205
Iteration 3/25 | Loss: 0.00116203
Iteration 4/25 | Loss: 0.00116203
Iteration 5/25 | Loss: 0.00116203
Iteration 6/25 | Loss: 0.00116203
Iteration 7/25 | Loss: 0.00116203
Iteration 8/25 | Loss: 0.00116203
Iteration 9/25 | Loss: 0.00116203
Iteration 10/25 | Loss: 0.00116203
Iteration 11/25 | Loss: 0.00116203
Iteration 12/25 | Loss: 0.00116203
Iteration 13/25 | Loss: 0.00116203
Iteration 14/25 | Loss: 0.00116203
Iteration 15/25 | Loss: 0.00116203
Iteration 16/25 | Loss: 0.00116203
Iteration 17/25 | Loss: 0.00116203
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011620298027992249, 0.0011620298027992249, 0.0011620298027992249, 0.0011620298027992249, 0.0011620298027992249]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011620298027992249

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116203
Iteration 2/1000 | Loss: 0.00004154
Iteration 3/1000 | Loss: 0.00005021
Iteration 4/1000 | Loss: 0.00002484
Iteration 5/1000 | Loss: 0.00002088
Iteration 6/1000 | Loss: 0.00003601
Iteration 7/1000 | Loss: 0.00002046
Iteration 8/1000 | Loss: 0.00001883
Iteration 9/1000 | Loss: 0.00001786
Iteration 10/1000 | Loss: 0.00001743
Iteration 11/1000 | Loss: 0.00001697
Iteration 12/1000 | Loss: 0.00001666
Iteration 13/1000 | Loss: 0.00001622
Iteration 14/1000 | Loss: 0.00001587
Iteration 15/1000 | Loss: 0.00001566
Iteration 16/1000 | Loss: 0.00001545
Iteration 17/1000 | Loss: 0.00001544
Iteration 18/1000 | Loss: 0.00001534
Iteration 19/1000 | Loss: 0.00001528
Iteration 20/1000 | Loss: 0.00001525
Iteration 21/1000 | Loss: 0.00001524
Iteration 22/1000 | Loss: 0.00001523
Iteration 23/1000 | Loss: 0.00001523
Iteration 24/1000 | Loss: 0.00001520
Iteration 25/1000 | Loss: 0.00001519
Iteration 26/1000 | Loss: 0.00001519
Iteration 27/1000 | Loss: 0.00001519
Iteration 28/1000 | Loss: 0.00001519
Iteration 29/1000 | Loss: 0.00001517
Iteration 30/1000 | Loss: 0.00001517
Iteration 31/1000 | Loss: 0.00001516
Iteration 32/1000 | Loss: 0.00001514
Iteration 33/1000 | Loss: 0.00001514
Iteration 34/1000 | Loss: 0.00001514
Iteration 35/1000 | Loss: 0.00001514
Iteration 36/1000 | Loss: 0.00001514
Iteration 37/1000 | Loss: 0.00001514
Iteration 38/1000 | Loss: 0.00001514
Iteration 39/1000 | Loss: 0.00001513
Iteration 40/1000 | Loss: 0.00001513
Iteration 41/1000 | Loss: 0.00001513
Iteration 42/1000 | Loss: 0.00001513
Iteration 43/1000 | Loss: 0.00001513
Iteration 44/1000 | Loss: 0.00001513
Iteration 45/1000 | Loss: 0.00001513
Iteration 46/1000 | Loss: 0.00001511
Iteration 47/1000 | Loss: 0.00001511
Iteration 48/1000 | Loss: 0.00001511
Iteration 49/1000 | Loss: 0.00001510
Iteration 50/1000 | Loss: 0.00001510
Iteration 51/1000 | Loss: 0.00001510
Iteration 52/1000 | Loss: 0.00001509
Iteration 53/1000 | Loss: 0.00001509
Iteration 54/1000 | Loss: 0.00001509
Iteration 55/1000 | Loss: 0.00001509
Iteration 56/1000 | Loss: 0.00001509
Iteration 57/1000 | Loss: 0.00001509
Iteration 58/1000 | Loss: 0.00001509
Iteration 59/1000 | Loss: 0.00001508
Iteration 60/1000 | Loss: 0.00001508
Iteration 61/1000 | Loss: 0.00001508
Iteration 62/1000 | Loss: 0.00001507
Iteration 63/1000 | Loss: 0.00001507
Iteration 64/1000 | Loss: 0.00001506
Iteration 65/1000 | Loss: 0.00001506
Iteration 66/1000 | Loss: 0.00001506
Iteration 67/1000 | Loss: 0.00001504
Iteration 68/1000 | Loss: 0.00001504
Iteration 69/1000 | Loss: 0.00001502
Iteration 70/1000 | Loss: 0.00001502
Iteration 71/1000 | Loss: 0.00001501
Iteration 72/1000 | Loss: 0.00001501
Iteration 73/1000 | Loss: 0.00001501
Iteration 74/1000 | Loss: 0.00001501
Iteration 75/1000 | Loss: 0.00001501
Iteration 76/1000 | Loss: 0.00001501
Iteration 77/1000 | Loss: 0.00001501
Iteration 78/1000 | Loss: 0.00001500
Iteration 79/1000 | Loss: 0.00001500
Iteration 80/1000 | Loss: 0.00001500
Iteration 81/1000 | Loss: 0.00001500
Iteration 82/1000 | Loss: 0.00001500
Iteration 83/1000 | Loss: 0.00001500
Iteration 84/1000 | Loss: 0.00001499
Iteration 85/1000 | Loss: 0.00001499
Iteration 86/1000 | Loss: 0.00001499
Iteration 87/1000 | Loss: 0.00001499
Iteration 88/1000 | Loss: 0.00001499
Iteration 89/1000 | Loss: 0.00001499
Iteration 90/1000 | Loss: 0.00001499
Iteration 91/1000 | Loss: 0.00001499
Iteration 92/1000 | Loss: 0.00001499
Iteration 93/1000 | Loss: 0.00001499
Iteration 94/1000 | Loss: 0.00001499
Iteration 95/1000 | Loss: 0.00001499
Iteration 96/1000 | Loss: 0.00001499
Iteration 97/1000 | Loss: 0.00001498
Iteration 98/1000 | Loss: 0.00001498
Iteration 99/1000 | Loss: 0.00001498
Iteration 100/1000 | Loss: 0.00001498
Iteration 101/1000 | Loss: 0.00001498
Iteration 102/1000 | Loss: 0.00001498
Iteration 103/1000 | Loss: 0.00001498
Iteration 104/1000 | Loss: 0.00001498
Iteration 105/1000 | Loss: 0.00001498
Iteration 106/1000 | Loss: 0.00001498
Iteration 107/1000 | Loss: 0.00001497
Iteration 108/1000 | Loss: 0.00001497
Iteration 109/1000 | Loss: 0.00001497
Iteration 110/1000 | Loss: 0.00001497
Iteration 111/1000 | Loss: 0.00001497
Iteration 112/1000 | Loss: 0.00001497
Iteration 113/1000 | Loss: 0.00001497
Iteration 114/1000 | Loss: 0.00001497
Iteration 115/1000 | Loss: 0.00001497
Iteration 116/1000 | Loss: 0.00001497
Iteration 117/1000 | Loss: 0.00001497
Iteration 118/1000 | Loss: 0.00001497
Iteration 119/1000 | Loss: 0.00001497
Iteration 120/1000 | Loss: 0.00001497
Iteration 121/1000 | Loss: 0.00001497
Iteration 122/1000 | Loss: 0.00001496
Iteration 123/1000 | Loss: 0.00001496
Iteration 124/1000 | Loss: 0.00001496
Iteration 125/1000 | Loss: 0.00001496
Iteration 126/1000 | Loss: 0.00001496
Iteration 127/1000 | Loss: 0.00001496
Iteration 128/1000 | Loss: 0.00001496
Iteration 129/1000 | Loss: 0.00001496
Iteration 130/1000 | Loss: 0.00001496
Iteration 131/1000 | Loss: 0.00001495
Iteration 132/1000 | Loss: 0.00001495
Iteration 133/1000 | Loss: 0.00001495
Iteration 134/1000 | Loss: 0.00001495
Iteration 135/1000 | Loss: 0.00001495
Iteration 136/1000 | Loss: 0.00001495
Iteration 137/1000 | Loss: 0.00001495
Iteration 138/1000 | Loss: 0.00001495
Iteration 139/1000 | Loss: 0.00001495
Iteration 140/1000 | Loss: 0.00001495
Iteration 141/1000 | Loss: 0.00001495
Iteration 142/1000 | Loss: 0.00001495
Iteration 143/1000 | Loss: 0.00001494
Iteration 144/1000 | Loss: 0.00001494
Iteration 145/1000 | Loss: 0.00001494
Iteration 146/1000 | Loss: 0.00001494
Iteration 147/1000 | Loss: 0.00001494
Iteration 148/1000 | Loss: 0.00001494
Iteration 149/1000 | Loss: 0.00001494
Iteration 150/1000 | Loss: 0.00001494
Iteration 151/1000 | Loss: 0.00001494
Iteration 152/1000 | Loss: 0.00001494
Iteration 153/1000 | Loss: 0.00001494
Iteration 154/1000 | Loss: 0.00001494
Iteration 155/1000 | Loss: 0.00001493
Iteration 156/1000 | Loss: 0.00001493
Iteration 157/1000 | Loss: 0.00001493
Iteration 158/1000 | Loss: 0.00001493
Iteration 159/1000 | Loss: 0.00001493
Iteration 160/1000 | Loss: 0.00001493
Iteration 161/1000 | Loss: 0.00001493
Iteration 162/1000 | Loss: 0.00001492
Iteration 163/1000 | Loss: 0.00001492
Iteration 164/1000 | Loss: 0.00001492
Iteration 165/1000 | Loss: 0.00001492
Iteration 166/1000 | Loss: 0.00001492
Iteration 167/1000 | Loss: 0.00001492
Iteration 168/1000 | Loss: 0.00001492
Iteration 169/1000 | Loss: 0.00001492
Iteration 170/1000 | Loss: 0.00001492
Iteration 171/1000 | Loss: 0.00001492
Iteration 172/1000 | Loss: 0.00001492
Iteration 173/1000 | Loss: 0.00001492
Iteration 174/1000 | Loss: 0.00001492
Iteration 175/1000 | Loss: 0.00001492
Iteration 176/1000 | Loss: 0.00001492
Iteration 177/1000 | Loss: 0.00001492
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 177. Stopping optimization.
Last 5 losses: [1.4921497495379299e-05, 1.4921497495379299e-05, 1.4921497495379299e-05, 1.4921497495379299e-05, 1.4921497495379299e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4921497495379299e-05

Optimization complete. Final v2v error: 3.2860605716705322 mm

Highest mean error: 4.149569988250732 mm for frame 235

Lowest mean error: 3.06180477142334 mm for frame 172

Saving results

Total time: 64.37238240242004
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1038/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1038.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1038
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00726904
Iteration 2/25 | Loss: 0.00174460
Iteration 3/25 | Loss: 0.00148161
Iteration 4/25 | Loss: 0.00139368
Iteration 5/25 | Loss: 0.00140501
Iteration 6/25 | Loss: 0.00139451
Iteration 7/25 | Loss: 0.00133517
Iteration 8/25 | Loss: 0.00129227
Iteration 9/25 | Loss: 0.00128324
Iteration 10/25 | Loss: 0.00128839
Iteration 11/25 | Loss: 0.00128932
Iteration 12/25 | Loss: 0.00128333
Iteration 13/25 | Loss: 0.00127802
Iteration 14/25 | Loss: 0.00127528
Iteration 15/25 | Loss: 0.00127496
Iteration 16/25 | Loss: 0.00127384
Iteration 17/25 | Loss: 0.00127308
Iteration 18/25 | Loss: 0.00127366
Iteration 19/25 | Loss: 0.00127158
Iteration 20/25 | Loss: 0.00127134
Iteration 21/25 | Loss: 0.00127120
Iteration 22/25 | Loss: 0.00127119
Iteration 23/25 | Loss: 0.00127118
Iteration 24/25 | Loss: 0.00127118
Iteration 25/25 | Loss: 0.00127118

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.05020332
Iteration 2/25 | Loss: 0.00137851
Iteration 3/25 | Loss: 0.00137844
Iteration 4/25 | Loss: 0.00137844
Iteration 5/25 | Loss: 0.00137844
Iteration 6/25 | Loss: 0.00137844
Iteration 7/25 | Loss: 0.00137843
Iteration 8/25 | Loss: 0.00137843
Iteration 9/25 | Loss: 0.00137843
Iteration 10/25 | Loss: 0.00137843
Iteration 11/25 | Loss: 0.00137843
Iteration 12/25 | Loss: 0.00137843
Iteration 13/25 | Loss: 0.00137843
Iteration 14/25 | Loss: 0.00137843
Iteration 15/25 | Loss: 0.00137843
Iteration 16/25 | Loss: 0.00137843
Iteration 17/25 | Loss: 0.00137843
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013784338952973485, 0.0013784338952973485, 0.0013784338952973485, 0.0013784338952973485, 0.0013784338952973485]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013784338952973485

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137843
Iteration 2/1000 | Loss: 0.00005731
Iteration 3/1000 | Loss: 0.00003769
Iteration 4/1000 | Loss: 0.00003134
Iteration 5/1000 | Loss: 0.00002843
Iteration 6/1000 | Loss: 0.00002638
Iteration 7/1000 | Loss: 0.00002496
Iteration 8/1000 | Loss: 0.00014246
Iteration 9/1000 | Loss: 0.00011869
Iteration 10/1000 | Loss: 0.00011005
Iteration 11/1000 | Loss: 0.00003646
Iteration 12/1000 | Loss: 0.00009122
Iteration 13/1000 | Loss: 0.00010290
Iteration 14/1000 | Loss: 0.00008663
Iteration 15/1000 | Loss: 0.00002724
Iteration 16/1000 | Loss: 0.00002595
Iteration 17/1000 | Loss: 0.00016072
Iteration 18/1000 | Loss: 0.00010975
Iteration 19/1000 | Loss: 0.00008823
Iteration 20/1000 | Loss: 0.00016249
Iteration 21/1000 | Loss: 0.00030107
Iteration 22/1000 | Loss: 0.00044549
Iteration 23/1000 | Loss: 0.00033194
Iteration 24/1000 | Loss: 0.00003148
Iteration 25/1000 | Loss: 0.00002653
Iteration 26/1000 | Loss: 0.00002498
Iteration 27/1000 | Loss: 0.00002448
Iteration 28/1000 | Loss: 0.00002398
Iteration 29/1000 | Loss: 0.00002358
Iteration 30/1000 | Loss: 0.00002326
Iteration 31/1000 | Loss: 0.00033053
Iteration 32/1000 | Loss: 0.00010600
Iteration 33/1000 | Loss: 0.00004339
Iteration 34/1000 | Loss: 0.00004008
Iteration 35/1000 | Loss: 0.00002756
Iteration 36/1000 | Loss: 0.00003343
Iteration 37/1000 | Loss: 0.00002592
Iteration 38/1000 | Loss: 0.00002474
Iteration 39/1000 | Loss: 0.00002351
Iteration 40/1000 | Loss: 0.00002264
Iteration 41/1000 | Loss: 0.00002140
Iteration 42/1000 | Loss: 0.00002058
Iteration 43/1000 | Loss: 0.00002007
Iteration 44/1000 | Loss: 0.00001985
Iteration 45/1000 | Loss: 0.00001962
Iteration 46/1000 | Loss: 0.00001940
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001922
Iteration 49/1000 | Loss: 0.00001920
Iteration 50/1000 | Loss: 0.00001919
Iteration 51/1000 | Loss: 0.00001913
Iteration 52/1000 | Loss: 0.00001909
Iteration 53/1000 | Loss: 0.00001909
Iteration 54/1000 | Loss: 0.00001906
Iteration 55/1000 | Loss: 0.00001905
Iteration 56/1000 | Loss: 0.00001904
Iteration 57/1000 | Loss: 0.00001903
Iteration 58/1000 | Loss: 0.00001902
Iteration 59/1000 | Loss: 0.00001899
Iteration 60/1000 | Loss: 0.00001899
Iteration 61/1000 | Loss: 0.00001898
Iteration 62/1000 | Loss: 0.00001897
Iteration 63/1000 | Loss: 0.00001897
Iteration 64/1000 | Loss: 0.00001896
Iteration 65/1000 | Loss: 0.00001896
Iteration 66/1000 | Loss: 0.00001893
Iteration 67/1000 | Loss: 0.00001891
Iteration 68/1000 | Loss: 0.00001889
Iteration 69/1000 | Loss: 0.00001882
Iteration 70/1000 | Loss: 0.00001878
Iteration 71/1000 | Loss: 0.00001876
Iteration 72/1000 | Loss: 0.00001875
Iteration 73/1000 | Loss: 0.00001873
Iteration 74/1000 | Loss: 0.00001872
Iteration 75/1000 | Loss: 0.00001871
Iteration 76/1000 | Loss: 0.00001870
Iteration 77/1000 | Loss: 0.00001870
Iteration 78/1000 | Loss: 0.00001865
Iteration 79/1000 | Loss: 0.00001864
Iteration 80/1000 | Loss: 0.00001864
Iteration 81/1000 | Loss: 0.00001863
Iteration 82/1000 | Loss: 0.00001862
Iteration 83/1000 | Loss: 0.00001862
Iteration 84/1000 | Loss: 0.00001861
Iteration 85/1000 | Loss: 0.00001861
Iteration 86/1000 | Loss: 0.00001859
Iteration 87/1000 | Loss: 0.00001856
Iteration 88/1000 | Loss: 0.00001853
Iteration 89/1000 | Loss: 0.00001850
Iteration 90/1000 | Loss: 0.00001850
Iteration 91/1000 | Loss: 0.00001848
Iteration 92/1000 | Loss: 0.00001848
Iteration 93/1000 | Loss: 0.00001847
Iteration 94/1000 | Loss: 0.00001847
Iteration 95/1000 | Loss: 0.00001846
Iteration 96/1000 | Loss: 0.00001846
Iteration 97/1000 | Loss: 0.00001846
Iteration 98/1000 | Loss: 0.00001845
Iteration 99/1000 | Loss: 0.00001845
Iteration 100/1000 | Loss: 0.00001844
Iteration 101/1000 | Loss: 0.00001844
Iteration 102/1000 | Loss: 0.00001844
Iteration 103/1000 | Loss: 0.00001843
Iteration 104/1000 | Loss: 0.00001843
Iteration 105/1000 | Loss: 0.00001842
Iteration 106/1000 | Loss: 0.00001842
Iteration 107/1000 | Loss: 0.00001841
Iteration 108/1000 | Loss: 0.00001841
Iteration 109/1000 | Loss: 0.00001841
Iteration 110/1000 | Loss: 0.00001840
Iteration 111/1000 | Loss: 0.00001840
Iteration 112/1000 | Loss: 0.00001840
Iteration 113/1000 | Loss: 0.00019428
Iteration 114/1000 | Loss: 0.00011752
Iteration 115/1000 | Loss: 0.00018327
Iteration 116/1000 | Loss: 0.00017739
Iteration 117/1000 | Loss: 0.00012557
Iteration 118/1000 | Loss: 0.00003555
Iteration 119/1000 | Loss: 0.00002069
Iteration 120/1000 | Loss: 0.00001977
Iteration 121/1000 | Loss: 0.00001911
Iteration 122/1000 | Loss: 0.00001876
Iteration 123/1000 | Loss: 0.00001850
Iteration 124/1000 | Loss: 0.00001847
Iteration 125/1000 | Loss: 0.00001845
Iteration 126/1000 | Loss: 0.00001837
Iteration 127/1000 | Loss: 0.00001837
Iteration 128/1000 | Loss: 0.00001837
Iteration 129/1000 | Loss: 0.00001837
Iteration 130/1000 | Loss: 0.00001836
Iteration 131/1000 | Loss: 0.00001836
Iteration 132/1000 | Loss: 0.00001835
Iteration 133/1000 | Loss: 0.00001835
Iteration 134/1000 | Loss: 0.00001835
Iteration 135/1000 | Loss: 0.00001834
Iteration 136/1000 | Loss: 0.00001834
Iteration 137/1000 | Loss: 0.00001834
Iteration 138/1000 | Loss: 0.00001833
Iteration 139/1000 | Loss: 0.00001833
Iteration 140/1000 | Loss: 0.00001833
Iteration 141/1000 | Loss: 0.00001833
Iteration 142/1000 | Loss: 0.00001832
Iteration 143/1000 | Loss: 0.00001832
Iteration 144/1000 | Loss: 0.00001832
Iteration 145/1000 | Loss: 0.00001832
Iteration 146/1000 | Loss: 0.00001832
Iteration 147/1000 | Loss: 0.00001831
Iteration 148/1000 | Loss: 0.00001831
Iteration 149/1000 | Loss: 0.00001831
Iteration 150/1000 | Loss: 0.00001831
Iteration 151/1000 | Loss: 0.00001831
Iteration 152/1000 | Loss: 0.00001831
Iteration 153/1000 | Loss: 0.00001831
Iteration 154/1000 | Loss: 0.00001831
Iteration 155/1000 | Loss: 0.00001831
Iteration 156/1000 | Loss: 0.00001831
Iteration 157/1000 | Loss: 0.00001831
Iteration 158/1000 | Loss: 0.00001830
Iteration 159/1000 | Loss: 0.00001830
Iteration 160/1000 | Loss: 0.00001830
Iteration 161/1000 | Loss: 0.00001830
Iteration 162/1000 | Loss: 0.00001830
Iteration 163/1000 | Loss: 0.00001830
Iteration 164/1000 | Loss: 0.00001830
Iteration 165/1000 | Loss: 0.00001830
Iteration 166/1000 | Loss: 0.00001830
Iteration 167/1000 | Loss: 0.00001830
Iteration 168/1000 | Loss: 0.00001830
Iteration 169/1000 | Loss: 0.00001830
Iteration 170/1000 | Loss: 0.00001830
Iteration 171/1000 | Loss: 0.00001830
Iteration 172/1000 | Loss: 0.00001830
Iteration 173/1000 | Loss: 0.00001830
Iteration 174/1000 | Loss: 0.00001830
Iteration 175/1000 | Loss: 0.00001830
Iteration 176/1000 | Loss: 0.00001830
Iteration 177/1000 | Loss: 0.00001830
Iteration 178/1000 | Loss: 0.00001829
Iteration 179/1000 | Loss: 0.00001829
Iteration 180/1000 | Loss: 0.00001829
Iteration 181/1000 | Loss: 0.00001829
Iteration 182/1000 | Loss: 0.00001829
Iteration 183/1000 | Loss: 0.00001829
Iteration 184/1000 | Loss: 0.00001829
Iteration 185/1000 | Loss: 0.00001829
Iteration 186/1000 | Loss: 0.00001829
Iteration 187/1000 | Loss: 0.00001829
Iteration 188/1000 | Loss: 0.00001829
Iteration 189/1000 | Loss: 0.00001829
Iteration 190/1000 | Loss: 0.00001828
Iteration 191/1000 | Loss: 0.00001828
Iteration 192/1000 | Loss: 0.00001828
Iteration 193/1000 | Loss: 0.00001828
Iteration 194/1000 | Loss: 0.00001828
Iteration 195/1000 | Loss: 0.00001828
Iteration 196/1000 | Loss: 0.00001828
Iteration 197/1000 | Loss: 0.00001828
Iteration 198/1000 | Loss: 0.00001828
Iteration 199/1000 | Loss: 0.00001827
Iteration 200/1000 | Loss: 0.00001827
Iteration 201/1000 | Loss: 0.00001827
Iteration 202/1000 | Loss: 0.00001827
Iteration 203/1000 | Loss: 0.00001827
Iteration 204/1000 | Loss: 0.00001827
Iteration 205/1000 | Loss: 0.00001827
Iteration 206/1000 | Loss: 0.00001826
Iteration 207/1000 | Loss: 0.00001826
Iteration 208/1000 | Loss: 0.00001826
Iteration 209/1000 | Loss: 0.00001826
Iteration 210/1000 | Loss: 0.00001826
Iteration 211/1000 | Loss: 0.00001826
Iteration 212/1000 | Loss: 0.00001826
Iteration 213/1000 | Loss: 0.00001826
Iteration 214/1000 | Loss: 0.00001826
Iteration 215/1000 | Loss: 0.00001826
Iteration 216/1000 | Loss: 0.00001826
Iteration 217/1000 | Loss: 0.00001826
Iteration 218/1000 | Loss: 0.00001826
Iteration 219/1000 | Loss: 0.00001826
Iteration 220/1000 | Loss: 0.00001826
Iteration 221/1000 | Loss: 0.00001826
Iteration 222/1000 | Loss: 0.00001826
Iteration 223/1000 | Loss: 0.00001825
Iteration 224/1000 | Loss: 0.00001825
Iteration 225/1000 | Loss: 0.00001825
Iteration 226/1000 | Loss: 0.00001825
Iteration 227/1000 | Loss: 0.00001825
Iteration 228/1000 | Loss: 0.00001825
Iteration 229/1000 | Loss: 0.00001825
Iteration 230/1000 | Loss: 0.00001825
Iteration 231/1000 | Loss: 0.00001825
Iteration 232/1000 | Loss: 0.00001825
Iteration 233/1000 | Loss: 0.00001824
Iteration 234/1000 | Loss: 0.00001824
Iteration 235/1000 | Loss: 0.00001824
Iteration 236/1000 | Loss: 0.00001824
Iteration 237/1000 | Loss: 0.00001824
Iteration 238/1000 | Loss: 0.00001824
Iteration 239/1000 | Loss: 0.00001824
Iteration 240/1000 | Loss: 0.00001824
Iteration 241/1000 | Loss: 0.00001824
Iteration 242/1000 | Loss: 0.00001823
Iteration 243/1000 | Loss: 0.00001823
Iteration 244/1000 | Loss: 0.00001823
Iteration 245/1000 | Loss: 0.00001823
Iteration 246/1000 | Loss: 0.00001822
Iteration 247/1000 | Loss: 0.00001822
Iteration 248/1000 | Loss: 0.00001822
Iteration 249/1000 | Loss: 0.00001822
Iteration 250/1000 | Loss: 0.00001822
Iteration 251/1000 | Loss: 0.00001822
Iteration 252/1000 | Loss: 0.00001822
Iteration 253/1000 | Loss: 0.00001822
Iteration 254/1000 | Loss: 0.00001822
Iteration 255/1000 | Loss: 0.00001822
Iteration 256/1000 | Loss: 0.00001822
Iteration 257/1000 | Loss: 0.00001821
Iteration 258/1000 | Loss: 0.00001821
Iteration 259/1000 | Loss: 0.00001821
Iteration 260/1000 | Loss: 0.00001821
Iteration 261/1000 | Loss: 0.00001821
Iteration 262/1000 | Loss: 0.00001821
Iteration 263/1000 | Loss: 0.00001821
Iteration 264/1000 | Loss: 0.00001821
Iteration 265/1000 | Loss: 0.00001821
Iteration 266/1000 | Loss: 0.00001821
Iteration 267/1000 | Loss: 0.00001821
Iteration 268/1000 | Loss: 0.00001821
Iteration 269/1000 | Loss: 0.00001821
Iteration 270/1000 | Loss: 0.00001821
Iteration 271/1000 | Loss: 0.00001821
Iteration 272/1000 | Loss: 0.00013638
Iteration 273/1000 | Loss: 0.00009923
Iteration 274/1000 | Loss: 0.00002490
Iteration 275/1000 | Loss: 0.00002157
Iteration 276/1000 | Loss: 0.00002008
Iteration 277/1000 | Loss: 0.00001899
Iteration 278/1000 | Loss: 0.00001846
Iteration 279/1000 | Loss: 0.00001822
Iteration 280/1000 | Loss: 0.00001819
Iteration 281/1000 | Loss: 0.00001818
Iteration 282/1000 | Loss: 0.00001816
Iteration 283/1000 | Loss: 0.00001815
Iteration 284/1000 | Loss: 0.00001815
Iteration 285/1000 | Loss: 0.00001814
Iteration 286/1000 | Loss: 0.00001813
Iteration 287/1000 | Loss: 0.00001812
Iteration 288/1000 | Loss: 0.00001812
Iteration 289/1000 | Loss: 0.00001811
Iteration 290/1000 | Loss: 0.00001811
Iteration 291/1000 | Loss: 0.00001810
Iteration 292/1000 | Loss: 0.00001810
Iteration 293/1000 | Loss: 0.00001810
Iteration 294/1000 | Loss: 0.00001810
Iteration 295/1000 | Loss: 0.00001810
Iteration 296/1000 | Loss: 0.00001809
Iteration 297/1000 | Loss: 0.00001809
Iteration 298/1000 | Loss: 0.00001809
Iteration 299/1000 | Loss: 0.00001808
Iteration 300/1000 | Loss: 0.00001808
Iteration 301/1000 | Loss: 0.00001808
Iteration 302/1000 | Loss: 0.00001807
Iteration 303/1000 | Loss: 0.00001807
Iteration 304/1000 | Loss: 0.00001807
Iteration 305/1000 | Loss: 0.00001806
Iteration 306/1000 | Loss: 0.00001806
Iteration 307/1000 | Loss: 0.00001805
Iteration 308/1000 | Loss: 0.00001804
Iteration 309/1000 | Loss: 0.00001804
Iteration 310/1000 | Loss: 0.00001803
Iteration 311/1000 | Loss: 0.00001803
Iteration 312/1000 | Loss: 0.00001803
Iteration 313/1000 | Loss: 0.00001802
Iteration 314/1000 | Loss: 0.00001802
Iteration 315/1000 | Loss: 0.00001802
Iteration 316/1000 | Loss: 0.00001801
Iteration 317/1000 | Loss: 0.00001801
Iteration 318/1000 | Loss: 0.00001800
Iteration 319/1000 | Loss: 0.00001800
Iteration 320/1000 | Loss: 0.00001800
Iteration 321/1000 | Loss: 0.00001799
Iteration 322/1000 | Loss: 0.00001799
Iteration 323/1000 | Loss: 0.00001799
Iteration 324/1000 | Loss: 0.00001799
Iteration 325/1000 | Loss: 0.00001798
Iteration 326/1000 | Loss: 0.00001798
Iteration 327/1000 | Loss: 0.00001798
Iteration 328/1000 | Loss: 0.00001798
Iteration 329/1000 | Loss: 0.00001797
Iteration 330/1000 | Loss: 0.00001797
Iteration 331/1000 | Loss: 0.00001796
Iteration 332/1000 | Loss: 0.00001796
Iteration 333/1000 | Loss: 0.00001796
Iteration 334/1000 | Loss: 0.00001795
Iteration 335/1000 | Loss: 0.00001795
Iteration 336/1000 | Loss: 0.00001795
Iteration 337/1000 | Loss: 0.00001794
Iteration 338/1000 | Loss: 0.00001794
Iteration 339/1000 | Loss: 0.00001794
Iteration 340/1000 | Loss: 0.00001793
Iteration 341/1000 | Loss: 0.00001793
Iteration 342/1000 | Loss: 0.00001793
Iteration 343/1000 | Loss: 0.00001793
Iteration 344/1000 | Loss: 0.00001791
Iteration 345/1000 | Loss: 0.00001791
Iteration 346/1000 | Loss: 0.00001791
Iteration 347/1000 | Loss: 0.00001790
Iteration 348/1000 | Loss: 0.00001789
Iteration 349/1000 | Loss: 0.00001788
Iteration 350/1000 | Loss: 0.00001788
Iteration 351/1000 | Loss: 0.00001788
Iteration 352/1000 | Loss: 0.00001788
Iteration 353/1000 | Loss: 0.00001788
Iteration 354/1000 | Loss: 0.00001787
Iteration 355/1000 | Loss: 0.00001787
Iteration 356/1000 | Loss: 0.00001787
Iteration 357/1000 | Loss: 0.00001787
Iteration 358/1000 | Loss: 0.00001787
Iteration 359/1000 | Loss: 0.00001787
Iteration 360/1000 | Loss: 0.00001787
Iteration 361/1000 | Loss: 0.00001787
Iteration 362/1000 | Loss: 0.00001787
Iteration 363/1000 | Loss: 0.00001786
Iteration 364/1000 | Loss: 0.00001786
Iteration 365/1000 | Loss: 0.00001785
Iteration 366/1000 | Loss: 0.00001785
Iteration 367/1000 | Loss: 0.00001784
Iteration 368/1000 | Loss: 0.00001784
Iteration 369/1000 | Loss: 0.00001784
Iteration 370/1000 | Loss: 0.00001784
Iteration 371/1000 | Loss: 0.00001783
Iteration 372/1000 | Loss: 0.00001783
Iteration 373/1000 | Loss: 0.00001783
Iteration 374/1000 | Loss: 0.00001783
Iteration 375/1000 | Loss: 0.00001782
Iteration 376/1000 | Loss: 0.00001782
Iteration 377/1000 | Loss: 0.00001782
Iteration 378/1000 | Loss: 0.00001782
Iteration 379/1000 | Loss: 0.00001781
Iteration 380/1000 | Loss: 0.00001781
Iteration 381/1000 | Loss: 0.00001781
Iteration 382/1000 | Loss: 0.00001781
Iteration 383/1000 | Loss: 0.00001780
Iteration 384/1000 | Loss: 0.00001780
Iteration 385/1000 | Loss: 0.00001780
Iteration 386/1000 | Loss: 0.00001780
Iteration 387/1000 | Loss: 0.00001780
Iteration 388/1000 | Loss: 0.00001779
Iteration 389/1000 | Loss: 0.00001779
Iteration 390/1000 | Loss: 0.00001779
Iteration 391/1000 | Loss: 0.00001779
Iteration 392/1000 | Loss: 0.00001779
Iteration 393/1000 | Loss: 0.00001779
Iteration 394/1000 | Loss: 0.00001779
Iteration 395/1000 | Loss: 0.00001779
Iteration 396/1000 | Loss: 0.00001779
Iteration 397/1000 | Loss: 0.00001779
Iteration 398/1000 | Loss: 0.00001779
Iteration 399/1000 | Loss: 0.00001779
Iteration 400/1000 | Loss: 0.00001779
Iteration 401/1000 | Loss: 0.00001779
Iteration 402/1000 | Loss: 0.00001779
Iteration 403/1000 | Loss: 0.00001779
Iteration 404/1000 | Loss: 0.00001779
Iteration 405/1000 | Loss: 0.00001779
Iteration 406/1000 | Loss: 0.00001779
Iteration 407/1000 | Loss: 0.00001778
Iteration 408/1000 | Loss: 0.00001778
Iteration 409/1000 | Loss: 0.00001778
Iteration 410/1000 | Loss: 0.00001778
Iteration 411/1000 | Loss: 0.00001778
Iteration 412/1000 | Loss: 0.00001778
Iteration 413/1000 | Loss: 0.00001778
Iteration 414/1000 | Loss: 0.00001778
Iteration 415/1000 | Loss: 0.00001778
Iteration 416/1000 | Loss: 0.00001778
Iteration 417/1000 | Loss: 0.00001778
Iteration 418/1000 | Loss: 0.00001778
Iteration 419/1000 | Loss: 0.00001778
Iteration 420/1000 | Loss: 0.00001778
Iteration 421/1000 | Loss: 0.00001778
Iteration 422/1000 | Loss: 0.00001778
Iteration 423/1000 | Loss: 0.00001778
Iteration 424/1000 | Loss: 0.00001778
Iteration 425/1000 | Loss: 0.00001778
Iteration 426/1000 | Loss: 0.00001778
Iteration 427/1000 | Loss: 0.00001778
Iteration 428/1000 | Loss: 0.00001778
Iteration 429/1000 | Loss: 0.00001778
Iteration 430/1000 | Loss: 0.00001778
Iteration 431/1000 | Loss: 0.00001778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 431. Stopping optimization.
Last 5 losses: [1.777590296114795e-05, 1.777590296114795e-05, 1.777590296114795e-05, 1.777590296114795e-05, 1.777590296114795e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.777590296114795e-05

Optimization complete. Final v2v error: 3.50671648979187 mm

Highest mean error: 4.705385684967041 mm for frame 105

Lowest mean error: 2.710206985473633 mm for frame 148

Saving results

Total time: 181.1296820640564
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1025/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1025.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1025
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00528702
Iteration 2/25 | Loss: 0.00142443
Iteration 3/25 | Loss: 0.00131312
Iteration 4/25 | Loss: 0.00130047
Iteration 5/25 | Loss: 0.00129651
Iteration 6/25 | Loss: 0.00129640
Iteration 7/25 | Loss: 0.00129637
Iteration 8/25 | Loss: 0.00129637
Iteration 9/25 | Loss: 0.00129637
Iteration 10/25 | Loss: 0.00129637
Iteration 11/25 | Loss: 0.00129637
Iteration 12/25 | Loss: 0.00129637
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001296370755881071, 0.001296370755881071, 0.001296370755881071, 0.001296370755881071, 0.001296370755881071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001296370755881071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.17176342
Iteration 2/25 | Loss: 0.00104946
Iteration 3/25 | Loss: 0.00104946
Iteration 4/25 | Loss: 0.00104946
Iteration 5/25 | Loss: 0.00104946
Iteration 6/25 | Loss: 0.00104946
Iteration 7/25 | Loss: 0.00104946
Iteration 8/25 | Loss: 0.00104946
Iteration 9/25 | Loss: 0.00104946
Iteration 10/25 | Loss: 0.00104946
Iteration 11/25 | Loss: 0.00104946
Iteration 12/25 | Loss: 0.00104946
Iteration 13/25 | Loss: 0.00104946
Iteration 14/25 | Loss: 0.00104946
Iteration 15/25 | Loss: 0.00104946
Iteration 16/25 | Loss: 0.00104946
Iteration 17/25 | Loss: 0.00104946
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001049456070177257, 0.001049456070177257, 0.001049456070177257, 0.001049456070177257, 0.001049456070177257]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001049456070177257

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104946
Iteration 2/1000 | Loss: 0.00003565
Iteration 3/1000 | Loss: 0.00002395
Iteration 4/1000 | Loss: 0.00002130
Iteration 5/1000 | Loss: 0.00002030
Iteration 6/1000 | Loss: 0.00001947
Iteration 7/1000 | Loss: 0.00001901
Iteration 8/1000 | Loss: 0.00001864
Iteration 9/1000 | Loss: 0.00001824
Iteration 10/1000 | Loss: 0.00001797
Iteration 11/1000 | Loss: 0.00001774
Iteration 12/1000 | Loss: 0.00001755
Iteration 13/1000 | Loss: 0.00001750
Iteration 14/1000 | Loss: 0.00001746
Iteration 15/1000 | Loss: 0.00001746
Iteration 16/1000 | Loss: 0.00001745
Iteration 17/1000 | Loss: 0.00001735
Iteration 18/1000 | Loss: 0.00001733
Iteration 19/1000 | Loss: 0.00001732
Iteration 20/1000 | Loss: 0.00001727
Iteration 21/1000 | Loss: 0.00001727
Iteration 22/1000 | Loss: 0.00001724
Iteration 23/1000 | Loss: 0.00001722
Iteration 24/1000 | Loss: 0.00001722
Iteration 25/1000 | Loss: 0.00001720
Iteration 26/1000 | Loss: 0.00001720
Iteration 27/1000 | Loss: 0.00001720
Iteration 28/1000 | Loss: 0.00001719
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001719
Iteration 31/1000 | Loss: 0.00001719
Iteration 32/1000 | Loss: 0.00001719
Iteration 33/1000 | Loss: 0.00001719
Iteration 34/1000 | Loss: 0.00001718
Iteration 35/1000 | Loss: 0.00001718
Iteration 36/1000 | Loss: 0.00001717
Iteration 37/1000 | Loss: 0.00001716
Iteration 38/1000 | Loss: 0.00001715
Iteration 39/1000 | Loss: 0.00001715
Iteration 40/1000 | Loss: 0.00001715
Iteration 41/1000 | Loss: 0.00001715
Iteration 42/1000 | Loss: 0.00001715
Iteration 43/1000 | Loss: 0.00001714
Iteration 44/1000 | Loss: 0.00001714
Iteration 45/1000 | Loss: 0.00001714
Iteration 46/1000 | Loss: 0.00001714
Iteration 47/1000 | Loss: 0.00001714
Iteration 48/1000 | Loss: 0.00001713
Iteration 49/1000 | Loss: 0.00001713
Iteration 50/1000 | Loss: 0.00001713
Iteration 51/1000 | Loss: 0.00001713
Iteration 52/1000 | Loss: 0.00001712
Iteration 53/1000 | Loss: 0.00001712
Iteration 54/1000 | Loss: 0.00001711
Iteration 55/1000 | Loss: 0.00001711
Iteration 56/1000 | Loss: 0.00001711
Iteration 57/1000 | Loss: 0.00001710
Iteration 58/1000 | Loss: 0.00001708
Iteration 59/1000 | Loss: 0.00001708
Iteration 60/1000 | Loss: 0.00001707
Iteration 61/1000 | Loss: 0.00001707
Iteration 62/1000 | Loss: 0.00001706
Iteration 63/1000 | Loss: 0.00001706
Iteration 64/1000 | Loss: 0.00001705
Iteration 65/1000 | Loss: 0.00001705
Iteration 66/1000 | Loss: 0.00001705
Iteration 67/1000 | Loss: 0.00001705
Iteration 68/1000 | Loss: 0.00001704
Iteration 69/1000 | Loss: 0.00001704
Iteration 70/1000 | Loss: 0.00001704
Iteration 71/1000 | Loss: 0.00001704
Iteration 72/1000 | Loss: 0.00001704
Iteration 73/1000 | Loss: 0.00001703
Iteration 74/1000 | Loss: 0.00001703
Iteration 75/1000 | Loss: 0.00001703
Iteration 76/1000 | Loss: 0.00001703
Iteration 77/1000 | Loss: 0.00001703
Iteration 78/1000 | Loss: 0.00001703
Iteration 79/1000 | Loss: 0.00001703
Iteration 80/1000 | Loss: 0.00001703
Iteration 81/1000 | Loss: 0.00001703
Iteration 82/1000 | Loss: 0.00001703
Iteration 83/1000 | Loss: 0.00001703
Iteration 84/1000 | Loss: 0.00001703
Iteration 85/1000 | Loss: 0.00001703
Iteration 86/1000 | Loss: 0.00001703
Iteration 87/1000 | Loss: 0.00001703
Iteration 88/1000 | Loss: 0.00001703
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 88. Stopping optimization.
Last 5 losses: [1.7028642105287872e-05, 1.7028642105287872e-05, 1.7028642105287872e-05, 1.7028642105287872e-05, 1.7028642105287872e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7028642105287872e-05

Optimization complete. Final v2v error: 3.4916253089904785 mm

Highest mean error: 3.878967046737671 mm for frame 123

Lowest mean error: 3.247821569442749 mm for frame 150

Saving results

Total time: 36.611833810806274
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00814716
Iteration 2/25 | Loss: 0.00163720
Iteration 3/25 | Loss: 0.00137783
Iteration 4/25 | Loss: 0.00135676
Iteration 5/25 | Loss: 0.00135301
Iteration 6/25 | Loss: 0.00135273
Iteration 7/25 | Loss: 0.00135273
Iteration 8/25 | Loss: 0.00135273
Iteration 9/25 | Loss: 0.00135273
Iteration 10/25 | Loss: 0.00135273
Iteration 11/25 | Loss: 0.00135273
Iteration 12/25 | Loss: 0.00135273
Iteration 13/25 | Loss: 0.00135273
Iteration 14/25 | Loss: 0.00135273
Iteration 15/25 | Loss: 0.00135273
Iteration 16/25 | Loss: 0.00135273
Iteration 17/25 | Loss: 0.00135273
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013527346309274435, 0.0013527346309274435, 0.0013527346309274435, 0.0013527346309274435, 0.0013527346309274435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013527346309274435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24313939
Iteration 2/25 | Loss: 0.00099003
Iteration 3/25 | Loss: 0.00098997
Iteration 4/25 | Loss: 0.00098997
Iteration 5/25 | Loss: 0.00098997
Iteration 6/25 | Loss: 0.00098997
Iteration 7/25 | Loss: 0.00098997
Iteration 8/25 | Loss: 0.00098997
Iteration 9/25 | Loss: 0.00098997
Iteration 10/25 | Loss: 0.00098997
Iteration 11/25 | Loss: 0.00098997
Iteration 12/25 | Loss: 0.00098997
Iteration 13/25 | Loss: 0.00098997
Iteration 14/25 | Loss: 0.00098997
Iteration 15/25 | Loss: 0.00098997
Iteration 16/25 | Loss: 0.00098997
Iteration 17/25 | Loss: 0.00098997
Iteration 18/25 | Loss: 0.00098997
Iteration 19/25 | Loss: 0.00098997
Iteration 20/25 | Loss: 0.00098997
Iteration 21/25 | Loss: 0.00098997
Iteration 22/25 | Loss: 0.00098997
Iteration 23/25 | Loss: 0.00098997
Iteration 24/25 | Loss: 0.00098997
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0009899697033688426, 0.0009899697033688426, 0.0009899697033688426, 0.0009899697033688426, 0.0009899697033688426]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009899697033688426

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098997
Iteration 2/1000 | Loss: 0.00005109
Iteration 3/1000 | Loss: 0.00003300
Iteration 4/1000 | Loss: 0.00002816
Iteration 5/1000 | Loss: 0.00002599
Iteration 6/1000 | Loss: 0.00002474
Iteration 7/1000 | Loss: 0.00002396
Iteration 8/1000 | Loss: 0.00002340
Iteration 9/1000 | Loss: 0.00002293
Iteration 10/1000 | Loss: 0.00002251
Iteration 11/1000 | Loss: 0.00002210
Iteration 12/1000 | Loss: 0.00002180
Iteration 13/1000 | Loss: 0.00002164
Iteration 14/1000 | Loss: 0.00002156
Iteration 15/1000 | Loss: 0.00002134
Iteration 16/1000 | Loss: 0.00002127
Iteration 17/1000 | Loss: 0.00002125
Iteration 18/1000 | Loss: 0.00002119
Iteration 19/1000 | Loss: 0.00002119
Iteration 20/1000 | Loss: 0.00002118
Iteration 21/1000 | Loss: 0.00002114
Iteration 22/1000 | Loss: 0.00002112
Iteration 23/1000 | Loss: 0.00002111
Iteration 24/1000 | Loss: 0.00002111
Iteration 25/1000 | Loss: 0.00002110
Iteration 26/1000 | Loss: 0.00002110
Iteration 27/1000 | Loss: 0.00002109
Iteration 28/1000 | Loss: 0.00002106
Iteration 29/1000 | Loss: 0.00002104
Iteration 30/1000 | Loss: 0.00002098
Iteration 31/1000 | Loss: 0.00002092
Iteration 32/1000 | Loss: 0.00002092
Iteration 33/1000 | Loss: 0.00002090
Iteration 34/1000 | Loss: 0.00002088
Iteration 35/1000 | Loss: 0.00002087
Iteration 36/1000 | Loss: 0.00002087
Iteration 37/1000 | Loss: 0.00002087
Iteration 38/1000 | Loss: 0.00002087
Iteration 39/1000 | Loss: 0.00002086
Iteration 40/1000 | Loss: 0.00002086
Iteration 41/1000 | Loss: 0.00002084
Iteration 42/1000 | Loss: 0.00002081
Iteration 43/1000 | Loss: 0.00002081
Iteration 44/1000 | Loss: 0.00002080
Iteration 45/1000 | Loss: 0.00002079
Iteration 46/1000 | Loss: 0.00002078
Iteration 47/1000 | Loss: 0.00002077
Iteration 48/1000 | Loss: 0.00002076
Iteration 49/1000 | Loss: 0.00002076
Iteration 50/1000 | Loss: 0.00002076
Iteration 51/1000 | Loss: 0.00002073
Iteration 52/1000 | Loss: 0.00002073
Iteration 53/1000 | Loss: 0.00002073
Iteration 54/1000 | Loss: 0.00002072
Iteration 55/1000 | Loss: 0.00002072
Iteration 56/1000 | Loss: 0.00002071
Iteration 57/1000 | Loss: 0.00002070
Iteration 58/1000 | Loss: 0.00002070
Iteration 59/1000 | Loss: 0.00002070
Iteration 60/1000 | Loss: 0.00002070
Iteration 61/1000 | Loss: 0.00002069
Iteration 62/1000 | Loss: 0.00002069
Iteration 63/1000 | Loss: 0.00002069
Iteration 64/1000 | Loss: 0.00002069
Iteration 65/1000 | Loss: 0.00002068
Iteration 66/1000 | Loss: 0.00002068
Iteration 67/1000 | Loss: 0.00002068
Iteration 68/1000 | Loss: 0.00002068
Iteration 69/1000 | Loss: 0.00002068
Iteration 70/1000 | Loss: 0.00002067
Iteration 71/1000 | Loss: 0.00002067
Iteration 72/1000 | Loss: 0.00002067
Iteration 73/1000 | Loss: 0.00002067
Iteration 74/1000 | Loss: 0.00002067
Iteration 75/1000 | Loss: 0.00002066
Iteration 76/1000 | Loss: 0.00002066
Iteration 77/1000 | Loss: 0.00002066
Iteration 78/1000 | Loss: 0.00002066
Iteration 79/1000 | Loss: 0.00002066
Iteration 80/1000 | Loss: 0.00002065
Iteration 81/1000 | Loss: 0.00002065
Iteration 82/1000 | Loss: 0.00002065
Iteration 83/1000 | Loss: 0.00002065
Iteration 84/1000 | Loss: 0.00002065
Iteration 85/1000 | Loss: 0.00002065
Iteration 86/1000 | Loss: 0.00002065
Iteration 87/1000 | Loss: 0.00002065
Iteration 88/1000 | Loss: 0.00002064
Iteration 89/1000 | Loss: 0.00002063
Iteration 90/1000 | Loss: 0.00002062
Iteration 91/1000 | Loss: 0.00002062
Iteration 92/1000 | Loss: 0.00002062
Iteration 93/1000 | Loss: 0.00002061
Iteration 94/1000 | Loss: 0.00002061
Iteration 95/1000 | Loss: 0.00002061
Iteration 96/1000 | Loss: 0.00002060
Iteration 97/1000 | Loss: 0.00002060
Iteration 98/1000 | Loss: 0.00002059
Iteration 99/1000 | Loss: 0.00002059
Iteration 100/1000 | Loss: 0.00002059
Iteration 101/1000 | Loss: 0.00002059
Iteration 102/1000 | Loss: 0.00002059
Iteration 103/1000 | Loss: 0.00002059
Iteration 104/1000 | Loss: 0.00002058
Iteration 105/1000 | Loss: 0.00002058
Iteration 106/1000 | Loss: 0.00002058
Iteration 107/1000 | Loss: 0.00002058
Iteration 108/1000 | Loss: 0.00002058
Iteration 109/1000 | Loss: 0.00002058
Iteration 110/1000 | Loss: 0.00002058
Iteration 111/1000 | Loss: 0.00002058
Iteration 112/1000 | Loss: 0.00002058
Iteration 113/1000 | Loss: 0.00002058
Iteration 114/1000 | Loss: 0.00002057
Iteration 115/1000 | Loss: 0.00002057
Iteration 116/1000 | Loss: 0.00002057
Iteration 117/1000 | Loss: 0.00002057
Iteration 118/1000 | Loss: 0.00002057
Iteration 119/1000 | Loss: 0.00002057
Iteration 120/1000 | Loss: 0.00002057
Iteration 121/1000 | Loss: 0.00002056
Iteration 122/1000 | Loss: 0.00002056
Iteration 123/1000 | Loss: 0.00002055
Iteration 124/1000 | Loss: 0.00002055
Iteration 125/1000 | Loss: 0.00002055
Iteration 126/1000 | Loss: 0.00002055
Iteration 127/1000 | Loss: 0.00002055
Iteration 128/1000 | Loss: 0.00002055
Iteration 129/1000 | Loss: 0.00002054
Iteration 130/1000 | Loss: 0.00002054
Iteration 131/1000 | Loss: 0.00002054
Iteration 132/1000 | Loss: 0.00002054
Iteration 133/1000 | Loss: 0.00002054
Iteration 134/1000 | Loss: 0.00002054
Iteration 135/1000 | Loss: 0.00002054
Iteration 136/1000 | Loss: 0.00002053
Iteration 137/1000 | Loss: 0.00002053
Iteration 138/1000 | Loss: 0.00002053
Iteration 139/1000 | Loss: 0.00002053
Iteration 140/1000 | Loss: 0.00002053
Iteration 141/1000 | Loss: 0.00002053
Iteration 142/1000 | Loss: 0.00002052
Iteration 143/1000 | Loss: 0.00002052
Iteration 144/1000 | Loss: 0.00002052
Iteration 145/1000 | Loss: 0.00002052
Iteration 146/1000 | Loss: 0.00002052
Iteration 147/1000 | Loss: 0.00002052
Iteration 148/1000 | Loss: 0.00002052
Iteration 149/1000 | Loss: 0.00002052
Iteration 150/1000 | Loss: 0.00002052
Iteration 151/1000 | Loss: 0.00002052
Iteration 152/1000 | Loss: 0.00002052
Iteration 153/1000 | Loss: 0.00002052
Iteration 154/1000 | Loss: 0.00002052
Iteration 155/1000 | Loss: 0.00002051
Iteration 156/1000 | Loss: 0.00002051
Iteration 157/1000 | Loss: 0.00002051
Iteration 158/1000 | Loss: 0.00002051
Iteration 159/1000 | Loss: 0.00002051
Iteration 160/1000 | Loss: 0.00002051
Iteration 161/1000 | Loss: 0.00002051
Iteration 162/1000 | Loss: 0.00002051
Iteration 163/1000 | Loss: 0.00002051
Iteration 164/1000 | Loss: 0.00002051
Iteration 165/1000 | Loss: 0.00002051
Iteration 166/1000 | Loss: 0.00002051
Iteration 167/1000 | Loss: 0.00002051
Iteration 168/1000 | Loss: 0.00002051
Iteration 169/1000 | Loss: 0.00002051
Iteration 170/1000 | Loss: 0.00002051
Iteration 171/1000 | Loss: 0.00002051
Iteration 172/1000 | Loss: 0.00002051
Iteration 173/1000 | Loss: 0.00002051
Iteration 174/1000 | Loss: 0.00002051
Iteration 175/1000 | Loss: 0.00002051
Iteration 176/1000 | Loss: 0.00002051
Iteration 177/1000 | Loss: 0.00002051
Iteration 178/1000 | Loss: 0.00002051
Iteration 179/1000 | Loss: 0.00002051
Iteration 180/1000 | Loss: 0.00002051
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 180. Stopping optimization.
Last 5 losses: [2.0509010937530547e-05, 2.0509010937530547e-05, 2.0509010937530547e-05, 2.0509010937530547e-05, 2.0509010937530547e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0509010937530547e-05

Optimization complete. Final v2v error: 3.8964998722076416 mm

Highest mean error: 4.2239990234375 mm for frame 137

Lowest mean error: 3.4786760807037354 mm for frame 0

Saving results

Total time: 43.255606174468994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00796019
Iteration 2/25 | Loss: 0.00197595
Iteration 3/25 | Loss: 0.00151412
Iteration 4/25 | Loss: 0.00157779
Iteration 5/25 | Loss: 0.00141810
Iteration 6/25 | Loss: 0.00144339
Iteration 7/25 | Loss: 0.00136117
Iteration 8/25 | Loss: 0.00136547
Iteration 9/25 | Loss: 0.00133625
Iteration 10/25 | Loss: 0.00133874
Iteration 11/25 | Loss: 0.00132892
Iteration 12/25 | Loss: 0.00133135
Iteration 13/25 | Loss: 0.00133051
Iteration 14/25 | Loss: 0.00133144
Iteration 15/25 | Loss: 0.00132683
Iteration 16/25 | Loss: 0.00132237
Iteration 17/25 | Loss: 0.00131699
Iteration 18/25 | Loss: 0.00131515
Iteration 19/25 | Loss: 0.00131484
Iteration 20/25 | Loss: 0.00131475
Iteration 21/25 | Loss: 0.00131475
Iteration 22/25 | Loss: 0.00131474
Iteration 23/25 | Loss: 0.00131474
Iteration 24/25 | Loss: 0.00131474
Iteration 25/25 | Loss: 0.00131474

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.47714257
Iteration 2/25 | Loss: 0.00121724
Iteration 3/25 | Loss: 0.00121724
Iteration 4/25 | Loss: 0.00121724
Iteration 5/25 | Loss: 0.00121724
Iteration 6/25 | Loss: 0.00121724
Iteration 7/25 | Loss: 0.00121724
Iteration 8/25 | Loss: 0.00121724
Iteration 9/25 | Loss: 0.00121724
Iteration 10/25 | Loss: 0.00121724
Iteration 11/25 | Loss: 0.00121724
Iteration 12/25 | Loss: 0.00121724
Iteration 13/25 | Loss: 0.00121724
Iteration 14/25 | Loss: 0.00121724
Iteration 15/25 | Loss: 0.00121724
Iteration 16/25 | Loss: 0.00121724
Iteration 17/25 | Loss: 0.00121724
Iteration 18/25 | Loss: 0.00121724
Iteration 19/25 | Loss: 0.00121724
Iteration 20/25 | Loss: 0.00121724
Iteration 21/25 | Loss: 0.00121724
Iteration 22/25 | Loss: 0.00121724
Iteration 23/25 | Loss: 0.00121724
Iteration 24/25 | Loss: 0.00121724
Iteration 25/25 | Loss: 0.00121724

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121724
Iteration 2/1000 | Loss: 0.00005757
Iteration 3/1000 | Loss: 0.00003773
Iteration 4/1000 | Loss: 0.00002912
Iteration 5/1000 | Loss: 0.00002635
Iteration 6/1000 | Loss: 0.00002432
Iteration 7/1000 | Loss: 0.00002314
Iteration 8/1000 | Loss: 0.00002237
Iteration 9/1000 | Loss: 0.00002195
Iteration 10/1000 | Loss: 0.00002174
Iteration 11/1000 | Loss: 0.00002151
Iteration 12/1000 | Loss: 0.00002128
Iteration 13/1000 | Loss: 0.00002112
Iteration 14/1000 | Loss: 0.00002107
Iteration 15/1000 | Loss: 0.00002107
Iteration 16/1000 | Loss: 0.00002103
Iteration 17/1000 | Loss: 0.00002098
Iteration 18/1000 | Loss: 0.00002093
Iteration 19/1000 | Loss: 0.00002092
Iteration 20/1000 | Loss: 0.00002089
Iteration 21/1000 | Loss: 0.00002088
Iteration 22/1000 | Loss: 0.00002088
Iteration 23/1000 | Loss: 0.00002087
Iteration 24/1000 | Loss: 0.00002086
Iteration 25/1000 | Loss: 0.00002086
Iteration 26/1000 | Loss: 0.00002085
Iteration 27/1000 | Loss: 0.00002085
Iteration 28/1000 | Loss: 0.00002084
Iteration 29/1000 | Loss: 0.00002084
Iteration 30/1000 | Loss: 0.00002083
Iteration 31/1000 | Loss: 0.00002083
Iteration 32/1000 | Loss: 0.00002083
Iteration 33/1000 | Loss: 0.00002082
Iteration 34/1000 | Loss: 0.00002082
Iteration 35/1000 | Loss: 0.00002082
Iteration 36/1000 | Loss: 0.00002081
Iteration 37/1000 | Loss: 0.00002081
Iteration 38/1000 | Loss: 0.00002081
Iteration 39/1000 | Loss: 0.00002081
Iteration 40/1000 | Loss: 0.00002081
Iteration 41/1000 | Loss: 0.00002080
Iteration 42/1000 | Loss: 0.00002080
Iteration 43/1000 | Loss: 0.00002079
Iteration 44/1000 | Loss: 0.00002079
Iteration 45/1000 | Loss: 0.00002079
Iteration 46/1000 | Loss: 0.00002079
Iteration 47/1000 | Loss: 0.00002078
Iteration 48/1000 | Loss: 0.00002078
Iteration 49/1000 | Loss: 0.00002078
Iteration 50/1000 | Loss: 0.00002077
Iteration 51/1000 | Loss: 0.00002077
Iteration 52/1000 | Loss: 0.00002077
Iteration 53/1000 | Loss: 0.00002077
Iteration 54/1000 | Loss: 0.00002076
Iteration 55/1000 | Loss: 0.00002076
Iteration 56/1000 | Loss: 0.00002076
Iteration 57/1000 | Loss: 0.00002075
Iteration 58/1000 | Loss: 0.00002075
Iteration 59/1000 | Loss: 0.00002075
Iteration 60/1000 | Loss: 0.00002075
Iteration 61/1000 | Loss: 0.00002075
Iteration 62/1000 | Loss: 0.00002075
Iteration 63/1000 | Loss: 0.00002074
Iteration 64/1000 | Loss: 0.00002074
Iteration 65/1000 | Loss: 0.00002074
Iteration 66/1000 | Loss: 0.00002074
Iteration 67/1000 | Loss: 0.00002074
Iteration 68/1000 | Loss: 0.00002073
Iteration 69/1000 | Loss: 0.00002073
Iteration 70/1000 | Loss: 0.00002073
Iteration 71/1000 | Loss: 0.00002073
Iteration 72/1000 | Loss: 0.00002073
Iteration 73/1000 | Loss: 0.00002072
Iteration 74/1000 | Loss: 0.00002072
Iteration 75/1000 | Loss: 0.00002072
Iteration 76/1000 | Loss: 0.00002072
Iteration 77/1000 | Loss: 0.00002072
Iteration 78/1000 | Loss: 0.00002071
Iteration 79/1000 | Loss: 0.00002071
Iteration 80/1000 | Loss: 0.00002071
Iteration 81/1000 | Loss: 0.00002071
Iteration 82/1000 | Loss: 0.00002071
Iteration 83/1000 | Loss: 0.00002071
Iteration 84/1000 | Loss: 0.00002071
Iteration 85/1000 | Loss: 0.00002071
Iteration 86/1000 | Loss: 0.00002070
Iteration 87/1000 | Loss: 0.00002070
Iteration 88/1000 | Loss: 0.00002070
Iteration 89/1000 | Loss: 0.00002070
Iteration 90/1000 | Loss: 0.00002070
Iteration 91/1000 | Loss: 0.00002070
Iteration 92/1000 | Loss: 0.00002070
Iteration 93/1000 | Loss: 0.00002069
Iteration 94/1000 | Loss: 0.00002069
Iteration 95/1000 | Loss: 0.00002069
Iteration 96/1000 | Loss: 0.00002069
Iteration 97/1000 | Loss: 0.00002069
Iteration 98/1000 | Loss: 0.00002069
Iteration 99/1000 | Loss: 0.00002069
Iteration 100/1000 | Loss: 0.00002069
Iteration 101/1000 | Loss: 0.00002068
Iteration 102/1000 | Loss: 0.00002068
Iteration 103/1000 | Loss: 0.00002068
Iteration 104/1000 | Loss: 0.00002068
Iteration 105/1000 | Loss: 0.00002068
Iteration 106/1000 | Loss: 0.00002068
Iteration 107/1000 | Loss: 0.00002068
Iteration 108/1000 | Loss: 0.00002067
Iteration 109/1000 | Loss: 0.00002067
Iteration 110/1000 | Loss: 0.00002067
Iteration 111/1000 | Loss: 0.00002067
Iteration 112/1000 | Loss: 0.00002067
Iteration 113/1000 | Loss: 0.00002067
Iteration 114/1000 | Loss: 0.00002067
Iteration 115/1000 | Loss: 0.00002066
Iteration 116/1000 | Loss: 0.00002066
Iteration 117/1000 | Loss: 0.00002066
Iteration 118/1000 | Loss: 0.00002066
Iteration 119/1000 | Loss: 0.00002066
Iteration 120/1000 | Loss: 0.00002066
Iteration 121/1000 | Loss: 0.00002066
Iteration 122/1000 | Loss: 0.00002066
Iteration 123/1000 | Loss: 0.00002066
Iteration 124/1000 | Loss: 0.00002066
Iteration 125/1000 | Loss: 0.00002066
Iteration 126/1000 | Loss: 0.00002066
Iteration 127/1000 | Loss: 0.00002066
Iteration 128/1000 | Loss: 0.00002066
Iteration 129/1000 | Loss: 0.00002066
Iteration 130/1000 | Loss: 0.00002066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 130. Stopping optimization.
Last 5 losses: [2.0657780623878352e-05, 2.0657780623878352e-05, 2.0657780623878352e-05, 2.0657780623878352e-05, 2.0657780623878352e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0657780623878352e-05

Optimization complete. Final v2v error: 3.7365100383758545 mm

Highest mean error: 4.491309642791748 mm for frame 41

Lowest mean error: 2.8468706607818604 mm for frame 2

Saving results

Total time: 62.05591082572937
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00957643
Iteration 2/25 | Loss: 0.00264817
Iteration 3/25 | Loss: 0.00207793
Iteration 4/25 | Loss: 0.00196810
Iteration 5/25 | Loss: 0.00150715
Iteration 6/25 | Loss: 0.00139184
Iteration 7/25 | Loss: 0.00135132
Iteration 8/25 | Loss: 0.00133622
Iteration 9/25 | Loss: 0.00133571
Iteration 10/25 | Loss: 0.00131168
Iteration 11/25 | Loss: 0.00130725
Iteration 12/25 | Loss: 0.00130619
Iteration 13/25 | Loss: 0.00130538
Iteration 14/25 | Loss: 0.00130905
Iteration 15/25 | Loss: 0.00130354
Iteration 16/25 | Loss: 0.00130295
Iteration 17/25 | Loss: 0.00130276
Iteration 18/25 | Loss: 0.00130276
Iteration 19/25 | Loss: 0.00130275
Iteration 20/25 | Loss: 0.00130273
Iteration 21/25 | Loss: 0.00130273
Iteration 22/25 | Loss: 0.00130273
Iteration 23/25 | Loss: 0.00130273
Iteration 24/25 | Loss: 0.00130273
Iteration 25/25 | Loss: 0.00130272

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32930982
Iteration 2/25 | Loss: 0.00077628
Iteration 3/25 | Loss: 0.00077628
Iteration 4/25 | Loss: 0.00077628
Iteration 5/25 | Loss: 0.00077628
Iteration 6/25 | Loss: 0.00077628
Iteration 7/25 | Loss: 0.00077628
Iteration 8/25 | Loss: 0.00077628
Iteration 9/25 | Loss: 0.00077628
Iteration 10/25 | Loss: 0.00077628
Iteration 11/25 | Loss: 0.00077628
Iteration 12/25 | Loss: 0.00077628
Iteration 13/25 | Loss: 0.00077628
Iteration 14/25 | Loss: 0.00077628
Iteration 15/25 | Loss: 0.00077628
Iteration 16/25 | Loss: 0.00077628
Iteration 17/25 | Loss: 0.00077628
Iteration 18/25 | Loss: 0.00077628
Iteration 19/25 | Loss: 0.00077628
Iteration 20/25 | Loss: 0.00077628
Iteration 21/25 | Loss: 0.00077628
Iteration 22/25 | Loss: 0.00077628
Iteration 23/25 | Loss: 0.00077628
Iteration 24/25 | Loss: 0.00077628
Iteration 25/25 | Loss: 0.00077628

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077628
Iteration 2/1000 | Loss: 0.00003719
Iteration 3/1000 | Loss: 0.00002538
Iteration 4/1000 | Loss: 0.00002373
Iteration 5/1000 | Loss: 0.00002241
Iteration 6/1000 | Loss: 0.00002177
Iteration 7/1000 | Loss: 0.00002126
Iteration 8/1000 | Loss: 0.00002112
Iteration 9/1000 | Loss: 0.00002095
Iteration 10/1000 | Loss: 0.00002071
Iteration 11/1000 | Loss: 0.00002049
Iteration 12/1000 | Loss: 0.00002024
Iteration 13/1000 | Loss: 0.00002008
Iteration 14/1000 | Loss: 0.00001999
Iteration 15/1000 | Loss: 0.00001990
Iteration 16/1000 | Loss: 0.00001989
Iteration 17/1000 | Loss: 0.00001986
Iteration 18/1000 | Loss: 0.00001986
Iteration 19/1000 | Loss: 0.00001981
Iteration 20/1000 | Loss: 0.00001981
Iteration 21/1000 | Loss: 0.00001981
Iteration 22/1000 | Loss: 0.00001981
Iteration 23/1000 | Loss: 0.00001980
Iteration 24/1000 | Loss: 0.00001980
Iteration 25/1000 | Loss: 0.00001980
Iteration 26/1000 | Loss: 0.00001980
Iteration 27/1000 | Loss: 0.00001980
Iteration 28/1000 | Loss: 0.00001980
Iteration 29/1000 | Loss: 0.00001980
Iteration 30/1000 | Loss: 0.00001975
Iteration 31/1000 | Loss: 0.00001974
Iteration 32/1000 | Loss: 0.00001972
Iteration 33/1000 | Loss: 0.00001972
Iteration 34/1000 | Loss: 0.00001971
Iteration 35/1000 | Loss: 0.00001971
Iteration 36/1000 | Loss: 0.00001971
Iteration 37/1000 | Loss: 0.00001970
Iteration 38/1000 | Loss: 0.00001970
Iteration 39/1000 | Loss: 0.00001970
Iteration 40/1000 | Loss: 0.00001970
Iteration 41/1000 | Loss: 0.00001969
Iteration 42/1000 | Loss: 0.00001969
Iteration 43/1000 | Loss: 0.00001968
Iteration 44/1000 | Loss: 0.00001968
Iteration 45/1000 | Loss: 0.00001967
Iteration 46/1000 | Loss: 0.00001967
Iteration 47/1000 | Loss: 0.00001967
Iteration 48/1000 | Loss: 0.00001966
Iteration 49/1000 | Loss: 0.00001966
Iteration 50/1000 | Loss: 0.00001965
Iteration 51/1000 | Loss: 0.00001965
Iteration 52/1000 | Loss: 0.00001965
Iteration 53/1000 | Loss: 0.00001965
Iteration 54/1000 | Loss: 0.00001965
Iteration 55/1000 | Loss: 0.00001965
Iteration 56/1000 | Loss: 0.00001965
Iteration 57/1000 | Loss: 0.00001965
Iteration 58/1000 | Loss: 0.00001965
Iteration 59/1000 | Loss: 0.00001965
Iteration 60/1000 | Loss: 0.00001965
Iteration 61/1000 | Loss: 0.00001965
Iteration 62/1000 | Loss: 0.00001965
Iteration 63/1000 | Loss: 0.00001965
Iteration 64/1000 | Loss: 0.00001965
Iteration 65/1000 | Loss: 0.00001965
Iteration 66/1000 | Loss: 0.00001965
Iteration 67/1000 | Loss: 0.00001965
Iteration 68/1000 | Loss: 0.00001965
Iteration 69/1000 | Loss: 0.00001965
Iteration 70/1000 | Loss: 0.00001965
Iteration 71/1000 | Loss: 0.00001965
Iteration 72/1000 | Loss: 0.00001965
Iteration 73/1000 | Loss: 0.00001965
Iteration 74/1000 | Loss: 0.00001965
Iteration 75/1000 | Loss: 0.00001965
Iteration 76/1000 | Loss: 0.00001965
Iteration 77/1000 | Loss: 0.00001965
Iteration 78/1000 | Loss: 0.00001965
Iteration 79/1000 | Loss: 0.00001965
Iteration 80/1000 | Loss: 0.00001965
Iteration 81/1000 | Loss: 0.00001965
Iteration 82/1000 | Loss: 0.00001965
Iteration 83/1000 | Loss: 0.00001965
Iteration 84/1000 | Loss: 0.00001965
Iteration 85/1000 | Loss: 0.00001965
Iteration 86/1000 | Loss: 0.00001965
Iteration 87/1000 | Loss: 0.00001965
Iteration 88/1000 | Loss: 0.00001965
Iteration 89/1000 | Loss: 0.00001965
Iteration 90/1000 | Loss: 0.00001965
Iteration 91/1000 | Loss: 0.00001965
Iteration 92/1000 | Loss: 0.00001965
Iteration 93/1000 | Loss: 0.00001965
Iteration 94/1000 | Loss: 0.00001965
Iteration 95/1000 | Loss: 0.00001965
Iteration 96/1000 | Loss: 0.00001965
Iteration 97/1000 | Loss: 0.00001965
Iteration 98/1000 | Loss: 0.00001965
Iteration 99/1000 | Loss: 0.00001965
Iteration 100/1000 | Loss: 0.00001965
Iteration 101/1000 | Loss: 0.00001965
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.9645292923087254e-05, 1.9645292923087254e-05, 1.9645292923087254e-05, 1.9645292923087254e-05, 1.9645292923087254e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9645292923087254e-05

Optimization complete. Final v2v error: 3.785496950149536 mm

Highest mean error: 3.9124321937561035 mm for frame 0

Lowest mean error: 3.552978754043579 mm for frame 18

Saving results

Total time: 54.460670471191406
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1040/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1040.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1040
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00804023
Iteration 2/25 | Loss: 0.00129532
Iteration 3/25 | Loss: 0.00122533
Iteration 4/25 | Loss: 0.00120854
Iteration 5/25 | Loss: 0.00120318
Iteration 6/25 | Loss: 0.00120214
Iteration 7/25 | Loss: 0.00120214
Iteration 8/25 | Loss: 0.00120214
Iteration 9/25 | Loss: 0.00120214
Iteration 10/25 | Loss: 0.00120214
Iteration 11/25 | Loss: 0.00120214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012021410511806607, 0.0012021410511806607, 0.0012021410511806607, 0.0012021410511806607, 0.0012021410511806607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012021410511806607

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17367220
Iteration 2/25 | Loss: 0.00154378
Iteration 3/25 | Loss: 0.00154378
Iteration 4/25 | Loss: 0.00154378
Iteration 5/25 | Loss: 0.00154378
Iteration 6/25 | Loss: 0.00154378
Iteration 7/25 | Loss: 0.00154378
Iteration 8/25 | Loss: 0.00154378
Iteration 9/25 | Loss: 0.00154378
Iteration 10/25 | Loss: 0.00154378
Iteration 11/25 | Loss: 0.00154378
Iteration 12/25 | Loss: 0.00154378
Iteration 13/25 | Loss: 0.00154378
Iteration 14/25 | Loss: 0.00154378
Iteration 15/25 | Loss: 0.00154378
Iteration 16/25 | Loss: 0.00154378
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001543777296319604, 0.001543777296319604, 0.001543777296319604, 0.001543777296319604, 0.001543777296319604]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001543777296319604

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00154378
Iteration 2/1000 | Loss: 0.00004089
Iteration 3/1000 | Loss: 0.00002711
Iteration 4/1000 | Loss: 0.00002368
Iteration 5/1000 | Loss: 0.00002259
Iteration 6/1000 | Loss: 0.00002159
Iteration 7/1000 | Loss: 0.00002117
Iteration 8/1000 | Loss: 0.00002063
Iteration 9/1000 | Loss: 0.00002026
Iteration 10/1000 | Loss: 0.00001999
Iteration 11/1000 | Loss: 0.00001969
Iteration 12/1000 | Loss: 0.00001956
Iteration 13/1000 | Loss: 0.00001932
Iteration 14/1000 | Loss: 0.00001916
Iteration 15/1000 | Loss: 0.00001908
Iteration 16/1000 | Loss: 0.00001907
Iteration 17/1000 | Loss: 0.00001906
Iteration 18/1000 | Loss: 0.00001905
Iteration 19/1000 | Loss: 0.00001905
Iteration 20/1000 | Loss: 0.00001904
Iteration 21/1000 | Loss: 0.00001904
Iteration 22/1000 | Loss: 0.00001903
Iteration 23/1000 | Loss: 0.00001903
Iteration 24/1000 | Loss: 0.00001902
Iteration 25/1000 | Loss: 0.00001894
Iteration 26/1000 | Loss: 0.00001889
Iteration 27/1000 | Loss: 0.00001889
Iteration 28/1000 | Loss: 0.00001887
Iteration 29/1000 | Loss: 0.00001887
Iteration 30/1000 | Loss: 0.00001883
Iteration 31/1000 | Loss: 0.00001883
Iteration 32/1000 | Loss: 0.00001883
Iteration 33/1000 | Loss: 0.00001882
Iteration 34/1000 | Loss: 0.00001882
Iteration 35/1000 | Loss: 0.00001881
Iteration 36/1000 | Loss: 0.00001881
Iteration 37/1000 | Loss: 0.00001880
Iteration 38/1000 | Loss: 0.00001880
Iteration 39/1000 | Loss: 0.00001880
Iteration 40/1000 | Loss: 0.00001880
Iteration 41/1000 | Loss: 0.00001880
Iteration 42/1000 | Loss: 0.00001880
Iteration 43/1000 | Loss: 0.00001880
Iteration 44/1000 | Loss: 0.00001880
Iteration 45/1000 | Loss: 0.00001880
Iteration 46/1000 | Loss: 0.00001880
Iteration 47/1000 | Loss: 0.00001880
Iteration 48/1000 | Loss: 0.00001880
Iteration 49/1000 | Loss: 0.00001880
Iteration 50/1000 | Loss: 0.00001880
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001879
Iteration 53/1000 | Loss: 0.00001879
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001879
Iteration 56/1000 | Loss: 0.00001879
Iteration 57/1000 | Loss: 0.00001878
Iteration 58/1000 | Loss: 0.00001878
Iteration 59/1000 | Loss: 0.00001878
Iteration 60/1000 | Loss: 0.00001878
Iteration 61/1000 | Loss: 0.00001878
Iteration 62/1000 | Loss: 0.00001877
Iteration 63/1000 | Loss: 0.00001877
Iteration 64/1000 | Loss: 0.00001877
Iteration 65/1000 | Loss: 0.00001877
Iteration 66/1000 | Loss: 0.00001877
Iteration 67/1000 | Loss: 0.00001877
Iteration 68/1000 | Loss: 0.00001877
Iteration 69/1000 | Loss: 0.00001876
Iteration 70/1000 | Loss: 0.00001876
Iteration 71/1000 | Loss: 0.00001876
Iteration 72/1000 | Loss: 0.00001876
Iteration 73/1000 | Loss: 0.00001875
Iteration 74/1000 | Loss: 0.00001875
Iteration 75/1000 | Loss: 0.00001874
Iteration 76/1000 | Loss: 0.00001874
Iteration 77/1000 | Loss: 0.00001873
Iteration 78/1000 | Loss: 0.00001873
Iteration 79/1000 | Loss: 0.00001872
Iteration 80/1000 | Loss: 0.00001872
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001872
Iteration 83/1000 | Loss: 0.00001872
Iteration 84/1000 | Loss: 0.00001872
Iteration 85/1000 | Loss: 0.00001871
Iteration 86/1000 | Loss: 0.00001871
Iteration 87/1000 | Loss: 0.00001871
Iteration 88/1000 | Loss: 0.00001871
Iteration 89/1000 | Loss: 0.00001870
Iteration 90/1000 | Loss: 0.00001870
Iteration 91/1000 | Loss: 0.00001870
Iteration 92/1000 | Loss: 0.00001870
Iteration 93/1000 | Loss: 0.00001870
Iteration 94/1000 | Loss: 0.00001869
Iteration 95/1000 | Loss: 0.00001869
Iteration 96/1000 | Loss: 0.00001869
Iteration 97/1000 | Loss: 0.00001869
Iteration 98/1000 | Loss: 0.00001869
Iteration 99/1000 | Loss: 0.00001869
Iteration 100/1000 | Loss: 0.00001869
Iteration 101/1000 | Loss: 0.00001868
Iteration 102/1000 | Loss: 0.00001868
Iteration 103/1000 | Loss: 0.00001868
Iteration 104/1000 | Loss: 0.00001868
Iteration 105/1000 | Loss: 0.00001868
Iteration 106/1000 | Loss: 0.00001868
Iteration 107/1000 | Loss: 0.00001868
Iteration 108/1000 | Loss: 0.00001868
Iteration 109/1000 | Loss: 0.00001868
Iteration 110/1000 | Loss: 0.00001867
Iteration 111/1000 | Loss: 0.00001867
Iteration 112/1000 | Loss: 0.00001867
Iteration 113/1000 | Loss: 0.00001867
Iteration 114/1000 | Loss: 0.00001867
Iteration 115/1000 | Loss: 0.00001867
Iteration 116/1000 | Loss: 0.00001867
Iteration 117/1000 | Loss: 0.00001867
Iteration 118/1000 | Loss: 0.00001867
Iteration 119/1000 | Loss: 0.00001867
Iteration 120/1000 | Loss: 0.00001867
Iteration 121/1000 | Loss: 0.00001867
Iteration 122/1000 | Loss: 0.00001867
Iteration 123/1000 | Loss: 0.00001866
Iteration 124/1000 | Loss: 0.00001866
Iteration 125/1000 | Loss: 0.00001866
Iteration 126/1000 | Loss: 0.00001866
Iteration 127/1000 | Loss: 0.00001866
Iteration 128/1000 | Loss: 0.00001866
Iteration 129/1000 | Loss: 0.00001866
Iteration 130/1000 | Loss: 0.00001866
Iteration 131/1000 | Loss: 0.00001866
Iteration 132/1000 | Loss: 0.00001866
Iteration 133/1000 | Loss: 0.00001866
Iteration 134/1000 | Loss: 0.00001866
Iteration 135/1000 | Loss: 0.00001866
Iteration 136/1000 | Loss: 0.00001866
Iteration 137/1000 | Loss: 0.00001866
Iteration 138/1000 | Loss: 0.00001866
Iteration 139/1000 | Loss: 0.00001866
Iteration 140/1000 | Loss: 0.00001866
Iteration 141/1000 | Loss: 0.00001866
Iteration 142/1000 | Loss: 0.00001866
Iteration 143/1000 | Loss: 0.00001866
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 143. Stopping optimization.
Last 5 losses: [1.865656849986408e-05, 1.865656849986408e-05, 1.865656849986408e-05, 1.865656849986408e-05, 1.865656849986408e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.865656849986408e-05

Optimization complete. Final v2v error: 3.707787275314331 mm

Highest mean error: 4.067814350128174 mm for frame 0

Lowest mean error: 3.490779399871826 mm for frame 190

Saving results

Total time: 40.0855393409729
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01017849
Iteration 2/25 | Loss: 0.00301018
Iteration 3/25 | Loss: 0.00201896
Iteration 4/25 | Loss: 0.00194104
Iteration 5/25 | Loss: 0.00180006
Iteration 6/25 | Loss: 0.00167445
Iteration 7/25 | Loss: 0.00162109
Iteration 8/25 | Loss: 0.00159905
Iteration 9/25 | Loss: 0.00155486
Iteration 10/25 | Loss: 0.00153902
Iteration 11/25 | Loss: 0.00153609
Iteration 12/25 | Loss: 0.00152749
Iteration 13/25 | Loss: 0.00154270
Iteration 14/25 | Loss: 0.00153118
Iteration 15/25 | Loss: 0.00151825
Iteration 16/25 | Loss: 0.00150508
Iteration 17/25 | Loss: 0.00150009
Iteration 18/25 | Loss: 0.00149927
Iteration 19/25 | Loss: 0.00149912
Iteration 20/25 | Loss: 0.00149893
Iteration 21/25 | Loss: 0.00149876
Iteration 22/25 | Loss: 0.00149856
Iteration 23/25 | Loss: 0.00149836
Iteration 24/25 | Loss: 0.00149770
Iteration 25/25 | Loss: 0.00149957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.80340362
Iteration 2/25 | Loss: 0.00332656
Iteration 3/25 | Loss: 0.00332656
Iteration 4/25 | Loss: 0.00332656
Iteration 5/25 | Loss: 0.00332655
Iteration 6/25 | Loss: 0.00332655
Iteration 7/25 | Loss: 0.00332655
Iteration 8/25 | Loss: 0.00332655
Iteration 9/25 | Loss: 0.00332655
Iteration 10/25 | Loss: 0.00332655
Iteration 11/25 | Loss: 0.00332655
Iteration 12/25 | Loss: 0.00332655
Iteration 13/25 | Loss: 0.00332655
Iteration 14/25 | Loss: 0.00332655
Iteration 15/25 | Loss: 0.00332655
Iteration 16/25 | Loss: 0.00332655
Iteration 17/25 | Loss: 0.00332655
Iteration 18/25 | Loss: 0.00332655
Iteration 19/25 | Loss: 0.00332655
Iteration 20/25 | Loss: 0.00332655
Iteration 21/25 | Loss: 0.00332655
Iteration 22/25 | Loss: 0.00332655
Iteration 23/25 | Loss: 0.00332655
Iteration 24/25 | Loss: 0.00332655
Iteration 25/25 | Loss: 0.00332655

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00332655
Iteration 2/1000 | Loss: 0.00036348
Iteration 3/1000 | Loss: 0.00044656
Iteration 4/1000 | Loss: 0.00064313
Iteration 5/1000 | Loss: 0.00045117
Iteration 6/1000 | Loss: 0.00045225
Iteration 7/1000 | Loss: 0.00016452
Iteration 8/1000 | Loss: 0.00054062
Iteration 9/1000 | Loss: 0.00028956
Iteration 10/1000 | Loss: 0.00034771
Iteration 11/1000 | Loss: 0.00013583
Iteration 12/1000 | Loss: 0.00012555
Iteration 13/1000 | Loss: 0.00044486
Iteration 14/1000 | Loss: 0.00014532
Iteration 15/1000 | Loss: 0.00011849
Iteration 16/1000 | Loss: 0.00010752
Iteration 17/1000 | Loss: 0.00010458
Iteration 18/1000 | Loss: 0.00010178
Iteration 19/1000 | Loss: 0.00009990
Iteration 20/1000 | Loss: 0.00009821
Iteration 21/1000 | Loss: 0.00009682
Iteration 22/1000 | Loss: 0.00009570
Iteration 23/1000 | Loss: 0.00009490
Iteration 24/1000 | Loss: 0.00009531
Iteration 25/1000 | Loss: 0.00009406
Iteration 26/1000 | Loss: 0.00037528
Iteration 27/1000 | Loss: 0.00020205
Iteration 28/1000 | Loss: 0.00021497
Iteration 29/1000 | Loss: 0.00027838
Iteration 30/1000 | Loss: 0.00012230
Iteration 31/1000 | Loss: 0.00010517
Iteration 32/1000 | Loss: 0.00009756
Iteration 33/1000 | Loss: 0.00008930
Iteration 34/1000 | Loss: 0.00049446
Iteration 35/1000 | Loss: 0.00009703
Iteration 36/1000 | Loss: 0.00007961
Iteration 37/1000 | Loss: 0.00007720
Iteration 38/1000 | Loss: 0.00007490
Iteration 39/1000 | Loss: 0.00007245
Iteration 40/1000 | Loss: 0.00007078
Iteration 41/1000 | Loss: 0.00006984
Iteration 42/1000 | Loss: 0.00013834
Iteration 43/1000 | Loss: 0.00007946
Iteration 44/1000 | Loss: 0.00007199
Iteration 45/1000 | Loss: 0.00006917
Iteration 46/1000 | Loss: 0.00006648
Iteration 47/1000 | Loss: 0.00006520
Iteration 48/1000 | Loss: 0.00006433
Iteration 49/1000 | Loss: 0.00006711
Iteration 50/1000 | Loss: 0.00006362
Iteration 51/1000 | Loss: 0.00006331
Iteration 52/1000 | Loss: 0.00006859
Iteration 53/1000 | Loss: 0.00006297
Iteration 54/1000 | Loss: 0.00006287
Iteration 55/1000 | Loss: 0.00006283
Iteration 56/1000 | Loss: 0.00006283
Iteration 57/1000 | Loss: 0.00006276
Iteration 58/1000 | Loss: 0.00006272
Iteration 59/1000 | Loss: 0.00006265
Iteration 60/1000 | Loss: 0.00006265
Iteration 61/1000 | Loss: 0.00006255
Iteration 62/1000 | Loss: 0.00006247
Iteration 63/1000 | Loss: 0.00006240
Iteration 64/1000 | Loss: 0.00006237
Iteration 65/1000 | Loss: 0.00006228
Iteration 66/1000 | Loss: 0.00006227
Iteration 67/1000 | Loss: 0.00006225
Iteration 68/1000 | Loss: 0.00006224
Iteration 69/1000 | Loss: 0.00006223
Iteration 70/1000 | Loss: 0.00006222
Iteration 71/1000 | Loss: 0.00006221
Iteration 72/1000 | Loss: 0.00006220
Iteration 73/1000 | Loss: 0.00006220
Iteration 74/1000 | Loss: 0.00006220
Iteration 75/1000 | Loss: 0.00006220
Iteration 76/1000 | Loss: 0.00006220
Iteration 77/1000 | Loss: 0.00006220
Iteration 78/1000 | Loss: 0.00006220
Iteration 79/1000 | Loss: 0.00006220
Iteration 80/1000 | Loss: 0.00006219
Iteration 81/1000 | Loss: 0.00006219
Iteration 82/1000 | Loss: 0.00006219
Iteration 83/1000 | Loss: 0.00006219
Iteration 84/1000 | Loss: 0.00006219
Iteration 85/1000 | Loss: 0.00006219
Iteration 86/1000 | Loss: 0.00006219
Iteration 87/1000 | Loss: 0.00006218
Iteration 88/1000 | Loss: 0.00006218
Iteration 89/1000 | Loss: 0.00006217
Iteration 90/1000 | Loss: 0.00006217
Iteration 91/1000 | Loss: 0.00006217
Iteration 92/1000 | Loss: 0.00006216
Iteration 93/1000 | Loss: 0.00006216
Iteration 94/1000 | Loss: 0.00006216
Iteration 95/1000 | Loss: 0.00006215
Iteration 96/1000 | Loss: 0.00006215
Iteration 97/1000 | Loss: 0.00006215
Iteration 98/1000 | Loss: 0.00006215
Iteration 99/1000 | Loss: 0.00006215
Iteration 100/1000 | Loss: 0.00006215
Iteration 101/1000 | Loss: 0.00006215
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [6.215435132617131e-05, 6.215435132617131e-05, 6.215435132617131e-05, 6.215435132617131e-05, 6.215435132617131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.215435132617131e-05

Optimization complete. Final v2v error: 4.532984256744385 mm

Highest mean error: 11.649332046508789 mm for frame 51

Lowest mean error: 3.00242018699646 mm for frame 6

Saving results

Total time: 125.07688164710999
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00999667
Iteration 2/25 | Loss: 0.00171769
Iteration 3/25 | Loss: 0.00150637
Iteration 4/25 | Loss: 0.00138278
Iteration 5/25 | Loss: 0.00130229
Iteration 6/25 | Loss: 0.00128860
Iteration 7/25 | Loss: 0.00127740
Iteration 8/25 | Loss: 0.00127522
Iteration 9/25 | Loss: 0.00127832
Iteration 10/25 | Loss: 0.00127207
Iteration 11/25 | Loss: 0.00127034
Iteration 12/25 | Loss: 0.00127025
Iteration 13/25 | Loss: 0.00126965
Iteration 14/25 | Loss: 0.00127015
Iteration 15/25 | Loss: 0.00126990
Iteration 16/25 | Loss: 0.00127002
Iteration 17/25 | Loss: 0.00126971
Iteration 18/25 | Loss: 0.00127105
Iteration 19/25 | Loss: 0.00126978
Iteration 20/25 | Loss: 0.00126944
Iteration 21/25 | Loss: 0.00127007
Iteration 22/25 | Loss: 0.00126959
Iteration 23/25 | Loss: 0.00126964
Iteration 24/25 | Loss: 0.00126974
Iteration 25/25 | Loss: 0.00126968

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56357098
Iteration 2/25 | Loss: 0.00152879
Iteration 3/25 | Loss: 0.00152878
Iteration 4/25 | Loss: 0.00152878
Iteration 5/25 | Loss: 0.00152878
Iteration 6/25 | Loss: 0.00152878
Iteration 7/25 | Loss: 0.00152878
Iteration 8/25 | Loss: 0.00152878
Iteration 9/25 | Loss: 0.00152878
Iteration 10/25 | Loss: 0.00152878
Iteration 11/25 | Loss: 0.00152878
Iteration 12/25 | Loss: 0.00152878
Iteration 13/25 | Loss: 0.00152878
Iteration 14/25 | Loss: 0.00152878
Iteration 15/25 | Loss: 0.00152878
Iteration 16/25 | Loss: 0.00152878
Iteration 17/25 | Loss: 0.00152878
Iteration 18/25 | Loss: 0.00152878
Iteration 19/25 | Loss: 0.00152878
Iteration 20/25 | Loss: 0.00152878
Iteration 21/25 | Loss: 0.00152878
Iteration 22/25 | Loss: 0.00152878
Iteration 23/25 | Loss: 0.00152878
Iteration 24/25 | Loss: 0.00152878
Iteration 25/25 | Loss: 0.00152878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0015287809073925018, 0.0015287809073925018, 0.0015287809073925018, 0.0015287809073925018, 0.0015287809073925018]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015287809073925018

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00152878
Iteration 2/1000 | Loss: 0.00042506
Iteration 3/1000 | Loss: 0.00006408
Iteration 4/1000 | Loss: 0.00022863
Iteration 5/1000 | Loss: 0.00019024
Iteration 6/1000 | Loss: 0.00003365
Iteration 7/1000 | Loss: 0.00002447
Iteration 8/1000 | Loss: 0.00002944
Iteration 9/1000 | Loss: 0.00003641
Iteration 10/1000 | Loss: 0.00004249
Iteration 11/1000 | Loss: 0.00024592
Iteration 12/1000 | Loss: 0.00022080
Iteration 13/1000 | Loss: 0.00025471
Iteration 14/1000 | Loss: 0.00017409
Iteration 15/1000 | Loss: 0.00002445
Iteration 16/1000 | Loss: 0.00002225
Iteration 17/1000 | Loss: 0.00002120
Iteration 18/1000 | Loss: 0.00003659
Iteration 19/1000 | Loss: 0.00024407
Iteration 20/1000 | Loss: 0.00019557
Iteration 21/1000 | Loss: 0.00015936
Iteration 22/1000 | Loss: 0.00019244
Iteration 23/1000 | Loss: 0.00003677
Iteration 24/1000 | Loss: 0.00002673
Iteration 25/1000 | Loss: 0.00002488
Iteration 26/1000 | Loss: 0.00002318
Iteration 27/1000 | Loss: 0.00002153
Iteration 28/1000 | Loss: 0.00002082
Iteration 29/1000 | Loss: 0.00002049
Iteration 30/1000 | Loss: 0.00002028
Iteration 31/1000 | Loss: 0.00001996
Iteration 32/1000 | Loss: 0.00001954
Iteration 33/1000 | Loss: 0.00001920
Iteration 34/1000 | Loss: 0.00001877
Iteration 35/1000 | Loss: 0.00001845
Iteration 36/1000 | Loss: 0.00031390
Iteration 37/1000 | Loss: 0.00002361
Iteration 38/1000 | Loss: 0.00002119
Iteration 39/1000 | Loss: 0.00001912
Iteration 40/1000 | Loss: 0.00001801
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001743
Iteration 43/1000 | Loss: 0.00001733
Iteration 44/1000 | Loss: 0.00001726
Iteration 45/1000 | Loss: 0.00001723
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001720
Iteration 48/1000 | Loss: 0.00001720
Iteration 49/1000 | Loss: 0.00001719
Iteration 50/1000 | Loss: 0.00001714
Iteration 51/1000 | Loss: 0.00001707
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001704
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001701
Iteration 57/1000 | Loss: 0.00001700
Iteration 58/1000 | Loss: 0.00001700
Iteration 59/1000 | Loss: 0.00001700
Iteration 60/1000 | Loss: 0.00001699
Iteration 61/1000 | Loss: 0.00001698
Iteration 62/1000 | Loss: 0.00001698
Iteration 63/1000 | Loss: 0.00001698
Iteration 64/1000 | Loss: 0.00001698
Iteration 65/1000 | Loss: 0.00001697
Iteration 66/1000 | Loss: 0.00001697
Iteration 67/1000 | Loss: 0.00001697
Iteration 68/1000 | Loss: 0.00001697
Iteration 69/1000 | Loss: 0.00001697
Iteration 70/1000 | Loss: 0.00001696
Iteration 71/1000 | Loss: 0.00001696
Iteration 72/1000 | Loss: 0.00001696
Iteration 73/1000 | Loss: 0.00001695
Iteration 74/1000 | Loss: 0.00001695
Iteration 75/1000 | Loss: 0.00001694
Iteration 76/1000 | Loss: 0.00001694
Iteration 77/1000 | Loss: 0.00001693
Iteration 78/1000 | Loss: 0.00001692
Iteration 79/1000 | Loss: 0.00001692
Iteration 80/1000 | Loss: 0.00001692
Iteration 81/1000 | Loss: 0.00001691
Iteration 82/1000 | Loss: 0.00001690
Iteration 83/1000 | Loss: 0.00001690
Iteration 84/1000 | Loss: 0.00001690
Iteration 85/1000 | Loss: 0.00001690
Iteration 86/1000 | Loss: 0.00001689
Iteration 87/1000 | Loss: 0.00001689
Iteration 88/1000 | Loss: 0.00001689
Iteration 89/1000 | Loss: 0.00001689
Iteration 90/1000 | Loss: 0.00001688
Iteration 91/1000 | Loss: 0.00001688
Iteration 92/1000 | Loss: 0.00001688
Iteration 93/1000 | Loss: 0.00001688
Iteration 94/1000 | Loss: 0.00001688
Iteration 95/1000 | Loss: 0.00001688
Iteration 96/1000 | Loss: 0.00001688
Iteration 97/1000 | Loss: 0.00001687
Iteration 98/1000 | Loss: 0.00001687
Iteration 99/1000 | Loss: 0.00001687
Iteration 100/1000 | Loss: 0.00001687
Iteration 101/1000 | Loss: 0.00001687
Iteration 102/1000 | Loss: 0.00001687
Iteration 103/1000 | Loss: 0.00001687
Iteration 104/1000 | Loss: 0.00001686
Iteration 105/1000 | Loss: 0.00001686
Iteration 106/1000 | Loss: 0.00001686
Iteration 107/1000 | Loss: 0.00001686
Iteration 108/1000 | Loss: 0.00001686
Iteration 109/1000 | Loss: 0.00001686
Iteration 110/1000 | Loss: 0.00001686
Iteration 111/1000 | Loss: 0.00001685
Iteration 112/1000 | Loss: 0.00001685
Iteration 113/1000 | Loss: 0.00001685
Iteration 114/1000 | Loss: 0.00001685
Iteration 115/1000 | Loss: 0.00001685
Iteration 116/1000 | Loss: 0.00001685
Iteration 117/1000 | Loss: 0.00001685
Iteration 118/1000 | Loss: 0.00001684
Iteration 119/1000 | Loss: 0.00001684
Iteration 120/1000 | Loss: 0.00001684
Iteration 121/1000 | Loss: 0.00001684
Iteration 122/1000 | Loss: 0.00001684
Iteration 123/1000 | Loss: 0.00001684
Iteration 124/1000 | Loss: 0.00001684
Iteration 125/1000 | Loss: 0.00001684
Iteration 126/1000 | Loss: 0.00001684
Iteration 127/1000 | Loss: 0.00001683
Iteration 128/1000 | Loss: 0.00001683
Iteration 129/1000 | Loss: 0.00001683
Iteration 130/1000 | Loss: 0.00001683
Iteration 131/1000 | Loss: 0.00001683
Iteration 132/1000 | Loss: 0.00001683
Iteration 133/1000 | Loss: 0.00001682
Iteration 134/1000 | Loss: 0.00001682
Iteration 135/1000 | Loss: 0.00001682
Iteration 136/1000 | Loss: 0.00001682
Iteration 137/1000 | Loss: 0.00001681
Iteration 138/1000 | Loss: 0.00001681
Iteration 139/1000 | Loss: 0.00001681
Iteration 140/1000 | Loss: 0.00001681
Iteration 141/1000 | Loss: 0.00001681
Iteration 142/1000 | Loss: 0.00001681
Iteration 143/1000 | Loss: 0.00001681
Iteration 144/1000 | Loss: 0.00001680
Iteration 145/1000 | Loss: 0.00001680
Iteration 146/1000 | Loss: 0.00001680
Iteration 147/1000 | Loss: 0.00001680
Iteration 148/1000 | Loss: 0.00001680
Iteration 149/1000 | Loss: 0.00001680
Iteration 150/1000 | Loss: 0.00001680
Iteration 151/1000 | Loss: 0.00001679
Iteration 152/1000 | Loss: 0.00001679
Iteration 153/1000 | Loss: 0.00001679
Iteration 154/1000 | Loss: 0.00001679
Iteration 155/1000 | Loss: 0.00001679
Iteration 156/1000 | Loss: 0.00001679
Iteration 157/1000 | Loss: 0.00001679
Iteration 158/1000 | Loss: 0.00001679
Iteration 159/1000 | Loss: 0.00001678
Iteration 160/1000 | Loss: 0.00001678
Iteration 161/1000 | Loss: 0.00001678
Iteration 162/1000 | Loss: 0.00001678
Iteration 163/1000 | Loss: 0.00001678
Iteration 164/1000 | Loss: 0.00001678
Iteration 165/1000 | Loss: 0.00001678
Iteration 166/1000 | Loss: 0.00001678
Iteration 167/1000 | Loss: 0.00001677
Iteration 168/1000 | Loss: 0.00001677
Iteration 169/1000 | Loss: 0.00001677
Iteration 170/1000 | Loss: 0.00001677
Iteration 171/1000 | Loss: 0.00001677
Iteration 172/1000 | Loss: 0.00001676
Iteration 173/1000 | Loss: 0.00001676
Iteration 174/1000 | Loss: 0.00001676
Iteration 175/1000 | Loss: 0.00001676
Iteration 176/1000 | Loss: 0.00001676
Iteration 177/1000 | Loss: 0.00001675
Iteration 178/1000 | Loss: 0.00001675
Iteration 179/1000 | Loss: 0.00001675
Iteration 180/1000 | Loss: 0.00001675
Iteration 181/1000 | Loss: 0.00001675
Iteration 182/1000 | Loss: 0.00001675
Iteration 183/1000 | Loss: 0.00001675
Iteration 184/1000 | Loss: 0.00001675
Iteration 185/1000 | Loss: 0.00001675
Iteration 186/1000 | Loss: 0.00001675
Iteration 187/1000 | Loss: 0.00001675
Iteration 188/1000 | Loss: 0.00001675
Iteration 189/1000 | Loss: 0.00001675
Iteration 190/1000 | Loss: 0.00001675
Iteration 191/1000 | Loss: 0.00001675
Iteration 192/1000 | Loss: 0.00001675
Iteration 193/1000 | Loss: 0.00001674
Iteration 194/1000 | Loss: 0.00001674
Iteration 195/1000 | Loss: 0.00001674
Iteration 196/1000 | Loss: 0.00001674
Iteration 197/1000 | Loss: 0.00001674
Iteration 198/1000 | Loss: 0.00001674
Iteration 199/1000 | Loss: 0.00001674
Iteration 200/1000 | Loss: 0.00001674
Iteration 201/1000 | Loss: 0.00001674
Iteration 202/1000 | Loss: 0.00001674
Iteration 203/1000 | Loss: 0.00001673
Iteration 204/1000 | Loss: 0.00001673
Iteration 205/1000 | Loss: 0.00001673
Iteration 206/1000 | Loss: 0.00001673
Iteration 207/1000 | Loss: 0.00001673
Iteration 208/1000 | Loss: 0.00001673
Iteration 209/1000 | Loss: 0.00001673
Iteration 210/1000 | Loss: 0.00001673
Iteration 211/1000 | Loss: 0.00001673
Iteration 212/1000 | Loss: 0.00001673
Iteration 213/1000 | Loss: 0.00001673
Iteration 214/1000 | Loss: 0.00001673
Iteration 215/1000 | Loss: 0.00001673
Iteration 216/1000 | Loss: 0.00001673
Iteration 217/1000 | Loss: 0.00001673
Iteration 218/1000 | Loss: 0.00001673
Iteration 219/1000 | Loss: 0.00001673
Iteration 220/1000 | Loss: 0.00001673
Iteration 221/1000 | Loss: 0.00001673
Iteration 222/1000 | Loss: 0.00001673
Iteration 223/1000 | Loss: 0.00001673
Iteration 224/1000 | Loss: 0.00001673
Iteration 225/1000 | Loss: 0.00001673
Iteration 226/1000 | Loss: 0.00001673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 226. Stopping optimization.
Last 5 losses: [1.6727739421185106e-05, 1.6727739421185106e-05, 1.6727739421185106e-05, 1.6727739421185106e-05, 1.6727739421185106e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6727739421185106e-05

Optimization complete. Final v2v error: 3.1699562072753906 mm

Highest mean error: 10.422189712524414 mm for frame 123

Lowest mean error: 2.6587138175964355 mm for frame 71

Saving results

Total time: 122.62490844726562
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1073/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1073.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1073
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01011192
Iteration 2/25 | Loss: 0.00210106
Iteration 3/25 | Loss: 0.00151105
Iteration 4/25 | Loss: 0.00142859
Iteration 5/25 | Loss: 0.00143476
Iteration 6/25 | Loss: 0.00148657
Iteration 7/25 | Loss: 0.00143179
Iteration 8/25 | Loss: 0.00137692
Iteration 9/25 | Loss: 0.00129189
Iteration 10/25 | Loss: 0.00128824
Iteration 11/25 | Loss: 0.00128826
Iteration 12/25 | Loss: 0.00127592
Iteration 13/25 | Loss: 0.00126407
Iteration 14/25 | Loss: 0.00126337
Iteration 15/25 | Loss: 0.00125387
Iteration 16/25 | Loss: 0.00124747
Iteration 17/25 | Loss: 0.00124005
Iteration 18/25 | Loss: 0.00123814
Iteration 19/25 | Loss: 0.00123824
Iteration 20/25 | Loss: 0.00123762
Iteration 21/25 | Loss: 0.00123679
Iteration 22/25 | Loss: 0.00123653
Iteration 23/25 | Loss: 0.00123707
Iteration 24/25 | Loss: 0.00123772
Iteration 25/25 | Loss: 0.00124175

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.38754785
Iteration 2/25 | Loss: 0.00143513
Iteration 3/25 | Loss: 0.00143513
Iteration 4/25 | Loss: 0.00143513
Iteration 5/25 | Loss: 0.00143513
Iteration 6/25 | Loss: 0.00143513
Iteration 7/25 | Loss: 0.00143512
Iteration 8/25 | Loss: 0.00143512
Iteration 9/25 | Loss: 0.00143512
Iteration 10/25 | Loss: 0.00143512
Iteration 11/25 | Loss: 0.00143512
Iteration 12/25 | Loss: 0.00143512
Iteration 13/25 | Loss: 0.00143512
Iteration 14/25 | Loss: 0.00143512
Iteration 15/25 | Loss: 0.00143512
Iteration 16/25 | Loss: 0.00143512
Iteration 17/25 | Loss: 0.00143512
Iteration 18/25 | Loss: 0.00143512
Iteration 19/25 | Loss: 0.00143512
Iteration 20/25 | Loss: 0.00143512
Iteration 21/25 | Loss: 0.00143512
Iteration 22/25 | Loss: 0.00143512
Iteration 23/25 | Loss: 0.00143512
Iteration 24/25 | Loss: 0.00143512
Iteration 25/25 | Loss: 0.00143512

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143512
Iteration 2/1000 | Loss: 0.00003799
Iteration 3/1000 | Loss: 0.00021078
Iteration 4/1000 | Loss: 0.00022866
Iteration 5/1000 | Loss: 0.00005372
Iteration 6/1000 | Loss: 0.00002839
Iteration 7/1000 | Loss: 0.00003447
Iteration 8/1000 | Loss: 0.00002199
Iteration 9/1000 | Loss: 0.00002519
Iteration 10/1000 | Loss: 0.00003048
Iteration 11/1000 | Loss: 0.00008846
Iteration 12/1000 | Loss: 0.00327903
Iteration 13/1000 | Loss: 0.00025285
Iteration 14/1000 | Loss: 0.00152473
Iteration 15/1000 | Loss: 0.00002197
Iteration 16/1000 | Loss: 0.00001865
Iteration 17/1000 | Loss: 0.00001691
Iteration 18/1000 | Loss: 0.00001589
Iteration 19/1000 | Loss: 0.00001500
Iteration 20/1000 | Loss: 0.00001424
Iteration 21/1000 | Loss: 0.00001358
Iteration 22/1000 | Loss: 0.00001310
Iteration 23/1000 | Loss: 0.00001276
Iteration 24/1000 | Loss: 0.00001251
Iteration 25/1000 | Loss: 0.00001250
Iteration 26/1000 | Loss: 0.00001247
Iteration 27/1000 | Loss: 0.00001247
Iteration 28/1000 | Loss: 0.00001245
Iteration 29/1000 | Loss: 0.00001238
Iteration 30/1000 | Loss: 0.00001237
Iteration 31/1000 | Loss: 0.00001236
Iteration 32/1000 | Loss: 0.00001225
Iteration 33/1000 | Loss: 0.00001211
Iteration 34/1000 | Loss: 0.00001205
Iteration 35/1000 | Loss: 0.00001204
Iteration 36/1000 | Loss: 0.00001203
Iteration 37/1000 | Loss: 0.00001199
Iteration 38/1000 | Loss: 0.00001199
Iteration 39/1000 | Loss: 0.00001197
Iteration 40/1000 | Loss: 0.00001197
Iteration 41/1000 | Loss: 0.00001196
Iteration 42/1000 | Loss: 0.00001196
Iteration 43/1000 | Loss: 0.00001195
Iteration 44/1000 | Loss: 0.00001195
Iteration 45/1000 | Loss: 0.00001194
Iteration 46/1000 | Loss: 0.00001194
Iteration 47/1000 | Loss: 0.00001194
Iteration 48/1000 | Loss: 0.00001193
Iteration 49/1000 | Loss: 0.00001193
Iteration 50/1000 | Loss: 0.00001193
Iteration 51/1000 | Loss: 0.00001193
Iteration 52/1000 | Loss: 0.00001192
Iteration 53/1000 | Loss: 0.00001192
Iteration 54/1000 | Loss: 0.00001192
Iteration 55/1000 | Loss: 0.00001192
Iteration 56/1000 | Loss: 0.00001192
Iteration 57/1000 | Loss: 0.00001191
Iteration 58/1000 | Loss: 0.00001191
Iteration 59/1000 | Loss: 0.00001191
Iteration 60/1000 | Loss: 0.00001190
Iteration 61/1000 | Loss: 0.00001190
Iteration 62/1000 | Loss: 0.00001190
Iteration 63/1000 | Loss: 0.00001190
Iteration 64/1000 | Loss: 0.00001189
Iteration 65/1000 | Loss: 0.00001189
Iteration 66/1000 | Loss: 0.00001189
Iteration 67/1000 | Loss: 0.00001189
Iteration 68/1000 | Loss: 0.00001189
Iteration 69/1000 | Loss: 0.00001189
Iteration 70/1000 | Loss: 0.00001188
Iteration 71/1000 | Loss: 0.00001188
Iteration 72/1000 | Loss: 0.00001188
Iteration 73/1000 | Loss: 0.00001188
Iteration 74/1000 | Loss: 0.00001187
Iteration 75/1000 | Loss: 0.00001187
Iteration 76/1000 | Loss: 0.00001187
Iteration 77/1000 | Loss: 0.00001187
Iteration 78/1000 | Loss: 0.00001187
Iteration 79/1000 | Loss: 0.00001187
Iteration 80/1000 | Loss: 0.00001187
Iteration 81/1000 | Loss: 0.00001186
Iteration 82/1000 | Loss: 0.00001186
Iteration 83/1000 | Loss: 0.00001186
Iteration 84/1000 | Loss: 0.00001186
Iteration 85/1000 | Loss: 0.00001186
Iteration 86/1000 | Loss: 0.00001186
Iteration 87/1000 | Loss: 0.00001186
Iteration 88/1000 | Loss: 0.00001186
Iteration 89/1000 | Loss: 0.00001186
Iteration 90/1000 | Loss: 0.00001186
Iteration 91/1000 | Loss: 0.00001186
Iteration 92/1000 | Loss: 0.00001186
Iteration 93/1000 | Loss: 0.00001186
Iteration 94/1000 | Loss: 0.00001186
Iteration 95/1000 | Loss: 0.00001186
Iteration 96/1000 | Loss: 0.00001185
Iteration 97/1000 | Loss: 0.00001185
Iteration 98/1000 | Loss: 0.00001185
Iteration 99/1000 | Loss: 0.00001185
Iteration 100/1000 | Loss: 0.00001185
Iteration 101/1000 | Loss: 0.00001185
Iteration 102/1000 | Loss: 0.00001185
Iteration 103/1000 | Loss: 0.00001185
Iteration 104/1000 | Loss: 0.00001185
Iteration 105/1000 | Loss: 0.00001185
Iteration 106/1000 | Loss: 0.00001185
Iteration 107/1000 | Loss: 0.00001185
Iteration 108/1000 | Loss: 0.00001185
Iteration 109/1000 | Loss: 0.00001185
Iteration 110/1000 | Loss: 0.00001185
Iteration 111/1000 | Loss: 0.00001185
Iteration 112/1000 | Loss: 0.00001185
Iteration 113/1000 | Loss: 0.00001185
Iteration 114/1000 | Loss: 0.00001185
Iteration 115/1000 | Loss: 0.00001185
Iteration 116/1000 | Loss: 0.00001185
Iteration 117/1000 | Loss: 0.00001185
Iteration 118/1000 | Loss: 0.00001185
Iteration 119/1000 | Loss: 0.00001185
Iteration 120/1000 | Loss: 0.00001185
Iteration 121/1000 | Loss: 0.00001185
Iteration 122/1000 | Loss: 0.00001185
Iteration 123/1000 | Loss: 0.00001185
Iteration 124/1000 | Loss: 0.00001185
Iteration 125/1000 | Loss: 0.00001185
Iteration 126/1000 | Loss: 0.00001185
Iteration 127/1000 | Loss: 0.00001185
Iteration 128/1000 | Loss: 0.00001185
Iteration 129/1000 | Loss: 0.00001185
Iteration 130/1000 | Loss: 0.00001185
Iteration 131/1000 | Loss: 0.00001185
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.185434575745603e-05, 1.185434575745603e-05, 1.185434575745603e-05, 1.185434575745603e-05, 1.185434575745603e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.185434575745603e-05

Optimization complete. Final v2v error: 2.89955997467041 mm

Highest mean error: 4.125853061676025 mm for frame 62

Lowest mean error: 2.530216693878174 mm for frame 131

Saving results

Total time: 87.42500972747803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00787524
Iteration 2/25 | Loss: 0.00139206
Iteration 3/25 | Loss: 0.00121476
Iteration 4/25 | Loss: 0.00120071
Iteration 5/25 | Loss: 0.00119600
Iteration 6/25 | Loss: 0.00119465
Iteration 7/25 | Loss: 0.00119465
Iteration 8/25 | Loss: 0.00119465
Iteration 9/25 | Loss: 0.00119465
Iteration 10/25 | Loss: 0.00119465
Iteration 11/25 | Loss: 0.00119465
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001194653450511396, 0.001194653450511396, 0.001194653450511396, 0.001194653450511396, 0.001194653450511396]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001194653450511396

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16648948
Iteration 2/25 | Loss: 0.00126777
Iteration 3/25 | Loss: 0.00126777
Iteration 4/25 | Loss: 0.00126777
Iteration 5/25 | Loss: 0.00126777
Iteration 6/25 | Loss: 0.00126776
Iteration 7/25 | Loss: 0.00126776
Iteration 8/25 | Loss: 0.00126776
Iteration 9/25 | Loss: 0.00126776
Iteration 10/25 | Loss: 0.00126776
Iteration 11/25 | Loss: 0.00126776
Iteration 12/25 | Loss: 0.00126776
Iteration 13/25 | Loss: 0.00126776
Iteration 14/25 | Loss: 0.00126776
Iteration 15/25 | Loss: 0.00126776
Iteration 16/25 | Loss: 0.00126776
Iteration 17/25 | Loss: 0.00126776
Iteration 18/25 | Loss: 0.00126776
Iteration 19/25 | Loss: 0.00126776
Iteration 20/25 | Loss: 0.00126776
Iteration 21/25 | Loss: 0.00126776
Iteration 22/25 | Loss: 0.00126776
Iteration 23/25 | Loss: 0.00126776
Iteration 24/25 | Loss: 0.00126776
Iteration 25/25 | Loss: 0.00126776

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00126776
Iteration 2/1000 | Loss: 0.00004284
Iteration 3/1000 | Loss: 0.00002722
Iteration 4/1000 | Loss: 0.00002112
Iteration 5/1000 | Loss: 0.00001840
Iteration 6/1000 | Loss: 0.00001687
Iteration 7/1000 | Loss: 0.00001564
Iteration 8/1000 | Loss: 0.00001501
Iteration 9/1000 | Loss: 0.00001451
Iteration 10/1000 | Loss: 0.00001407
Iteration 11/1000 | Loss: 0.00001381
Iteration 12/1000 | Loss: 0.00001349
Iteration 13/1000 | Loss: 0.00001332
Iteration 14/1000 | Loss: 0.00001328
Iteration 15/1000 | Loss: 0.00001327
Iteration 16/1000 | Loss: 0.00001323
Iteration 17/1000 | Loss: 0.00001322
Iteration 18/1000 | Loss: 0.00001321
Iteration 19/1000 | Loss: 0.00001319
Iteration 20/1000 | Loss: 0.00001318
Iteration 21/1000 | Loss: 0.00001317
Iteration 22/1000 | Loss: 0.00001315
Iteration 23/1000 | Loss: 0.00001310
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001306
Iteration 26/1000 | Loss: 0.00001299
Iteration 27/1000 | Loss: 0.00001297
Iteration 28/1000 | Loss: 0.00001295
Iteration 29/1000 | Loss: 0.00001295
Iteration 30/1000 | Loss: 0.00001294
Iteration 31/1000 | Loss: 0.00001293
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001291
Iteration 34/1000 | Loss: 0.00001290
Iteration 35/1000 | Loss: 0.00001286
Iteration 36/1000 | Loss: 0.00001284
Iteration 37/1000 | Loss: 0.00001283
Iteration 38/1000 | Loss: 0.00001280
Iteration 39/1000 | Loss: 0.00001279
Iteration 40/1000 | Loss: 0.00001279
Iteration 41/1000 | Loss: 0.00001278
Iteration 42/1000 | Loss: 0.00001278
Iteration 43/1000 | Loss: 0.00001277
Iteration 44/1000 | Loss: 0.00001277
Iteration 45/1000 | Loss: 0.00001276
Iteration 46/1000 | Loss: 0.00001276
Iteration 47/1000 | Loss: 0.00001275
Iteration 48/1000 | Loss: 0.00001274
Iteration 49/1000 | Loss: 0.00001270
Iteration 50/1000 | Loss: 0.00001269
Iteration 51/1000 | Loss: 0.00001269
Iteration 52/1000 | Loss: 0.00001268
Iteration 53/1000 | Loss: 0.00001268
Iteration 54/1000 | Loss: 0.00001267
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001266
Iteration 57/1000 | Loss: 0.00001266
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001265
Iteration 61/1000 | Loss: 0.00001265
Iteration 62/1000 | Loss: 0.00001265
Iteration 63/1000 | Loss: 0.00001265
Iteration 64/1000 | Loss: 0.00001265
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001264
Iteration 67/1000 | Loss: 0.00001264
Iteration 68/1000 | Loss: 0.00001264
Iteration 69/1000 | Loss: 0.00001264
Iteration 70/1000 | Loss: 0.00001263
Iteration 71/1000 | Loss: 0.00001263
Iteration 72/1000 | Loss: 0.00001263
Iteration 73/1000 | Loss: 0.00001263
Iteration 74/1000 | Loss: 0.00001263
Iteration 75/1000 | Loss: 0.00001263
Iteration 76/1000 | Loss: 0.00001263
Iteration 77/1000 | Loss: 0.00001262
Iteration 78/1000 | Loss: 0.00001262
Iteration 79/1000 | Loss: 0.00001262
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001262
Iteration 84/1000 | Loss: 0.00001262
Iteration 85/1000 | Loss: 0.00001262
Iteration 86/1000 | Loss: 0.00001262
Iteration 87/1000 | Loss: 0.00001261
Iteration 88/1000 | Loss: 0.00001261
Iteration 89/1000 | Loss: 0.00001261
Iteration 90/1000 | Loss: 0.00001261
Iteration 91/1000 | Loss: 0.00001261
Iteration 92/1000 | Loss: 0.00001260
Iteration 93/1000 | Loss: 0.00001260
Iteration 94/1000 | Loss: 0.00001260
Iteration 95/1000 | Loss: 0.00001260
Iteration 96/1000 | Loss: 0.00001260
Iteration 97/1000 | Loss: 0.00001260
Iteration 98/1000 | Loss: 0.00001260
Iteration 99/1000 | Loss: 0.00001260
Iteration 100/1000 | Loss: 0.00001260
Iteration 101/1000 | Loss: 0.00001259
Iteration 102/1000 | Loss: 0.00001259
Iteration 103/1000 | Loss: 0.00001259
Iteration 104/1000 | Loss: 0.00001259
Iteration 105/1000 | Loss: 0.00001258
Iteration 106/1000 | Loss: 0.00001258
Iteration 107/1000 | Loss: 0.00001258
Iteration 108/1000 | Loss: 0.00001258
Iteration 109/1000 | Loss: 0.00001258
Iteration 110/1000 | Loss: 0.00001258
Iteration 111/1000 | Loss: 0.00001258
Iteration 112/1000 | Loss: 0.00001258
Iteration 113/1000 | Loss: 0.00001258
Iteration 114/1000 | Loss: 0.00001258
Iteration 115/1000 | Loss: 0.00001257
Iteration 116/1000 | Loss: 0.00001257
Iteration 117/1000 | Loss: 0.00001257
Iteration 118/1000 | Loss: 0.00001257
Iteration 119/1000 | Loss: 0.00001256
Iteration 120/1000 | Loss: 0.00001256
Iteration 121/1000 | Loss: 0.00001256
Iteration 122/1000 | Loss: 0.00001256
Iteration 123/1000 | Loss: 0.00001255
Iteration 124/1000 | Loss: 0.00001255
Iteration 125/1000 | Loss: 0.00001255
Iteration 126/1000 | Loss: 0.00001255
Iteration 127/1000 | Loss: 0.00001254
Iteration 128/1000 | Loss: 0.00001254
Iteration 129/1000 | Loss: 0.00001254
Iteration 130/1000 | Loss: 0.00001254
Iteration 131/1000 | Loss: 0.00001254
Iteration 132/1000 | Loss: 0.00001254
Iteration 133/1000 | Loss: 0.00001254
Iteration 134/1000 | Loss: 0.00001254
Iteration 135/1000 | Loss: 0.00001254
Iteration 136/1000 | Loss: 0.00001254
Iteration 137/1000 | Loss: 0.00001254
Iteration 138/1000 | Loss: 0.00001254
Iteration 139/1000 | Loss: 0.00001254
Iteration 140/1000 | Loss: 0.00001254
Iteration 141/1000 | Loss: 0.00001253
Iteration 142/1000 | Loss: 0.00001253
Iteration 143/1000 | Loss: 0.00001253
Iteration 144/1000 | Loss: 0.00001252
Iteration 145/1000 | Loss: 0.00001252
Iteration 146/1000 | Loss: 0.00001252
Iteration 147/1000 | Loss: 0.00001252
Iteration 148/1000 | Loss: 0.00001251
Iteration 149/1000 | Loss: 0.00001251
Iteration 150/1000 | Loss: 0.00001251
Iteration 151/1000 | Loss: 0.00001251
Iteration 152/1000 | Loss: 0.00001251
Iteration 153/1000 | Loss: 0.00001251
Iteration 154/1000 | Loss: 0.00001250
Iteration 155/1000 | Loss: 0.00001250
Iteration 156/1000 | Loss: 0.00001250
Iteration 157/1000 | Loss: 0.00001250
Iteration 158/1000 | Loss: 0.00001250
Iteration 159/1000 | Loss: 0.00001250
Iteration 160/1000 | Loss: 0.00001250
Iteration 161/1000 | Loss: 0.00001250
Iteration 162/1000 | Loss: 0.00001249
Iteration 163/1000 | Loss: 0.00001249
Iteration 164/1000 | Loss: 0.00001249
Iteration 165/1000 | Loss: 0.00001249
Iteration 166/1000 | Loss: 0.00001249
Iteration 167/1000 | Loss: 0.00001249
Iteration 168/1000 | Loss: 0.00001249
Iteration 169/1000 | Loss: 0.00001249
Iteration 170/1000 | Loss: 0.00001249
Iteration 171/1000 | Loss: 0.00001249
Iteration 172/1000 | Loss: 0.00001249
Iteration 173/1000 | Loss: 0.00001248
Iteration 174/1000 | Loss: 0.00001248
Iteration 175/1000 | Loss: 0.00001248
Iteration 176/1000 | Loss: 0.00001248
Iteration 177/1000 | Loss: 0.00001248
Iteration 178/1000 | Loss: 0.00001248
Iteration 179/1000 | Loss: 0.00001248
Iteration 180/1000 | Loss: 0.00001248
Iteration 181/1000 | Loss: 0.00001247
Iteration 182/1000 | Loss: 0.00001247
Iteration 183/1000 | Loss: 0.00001247
Iteration 184/1000 | Loss: 0.00001247
Iteration 185/1000 | Loss: 0.00001247
Iteration 186/1000 | Loss: 0.00001247
Iteration 187/1000 | Loss: 0.00001247
Iteration 188/1000 | Loss: 0.00001247
Iteration 189/1000 | Loss: 0.00001247
Iteration 190/1000 | Loss: 0.00001247
Iteration 191/1000 | Loss: 0.00001247
Iteration 192/1000 | Loss: 0.00001246
Iteration 193/1000 | Loss: 0.00001246
Iteration 194/1000 | Loss: 0.00001246
Iteration 195/1000 | Loss: 0.00001246
Iteration 196/1000 | Loss: 0.00001246
Iteration 197/1000 | Loss: 0.00001246
Iteration 198/1000 | Loss: 0.00001246
Iteration 199/1000 | Loss: 0.00001246
Iteration 200/1000 | Loss: 0.00001246
Iteration 201/1000 | Loss: 0.00001246
Iteration 202/1000 | Loss: 0.00001246
Iteration 203/1000 | Loss: 0.00001246
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 203. Stopping optimization.
Last 5 losses: [1.246021201950498e-05, 1.246021201950498e-05, 1.246021201950498e-05, 1.246021201950498e-05, 1.246021201950498e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.246021201950498e-05

Optimization complete. Final v2v error: 2.92970609664917 mm

Highest mean error: 4.245229244232178 mm for frame 76

Lowest mean error: 2.4612414836883545 mm for frame 108

Saving results

Total time: 44.47978377342224
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1084/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1084.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1084
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00777080
Iteration 2/25 | Loss: 0.00141717
Iteration 3/25 | Loss: 0.00122686
Iteration 4/25 | Loss: 0.00121105
Iteration 5/25 | Loss: 0.00120947
Iteration 6/25 | Loss: 0.00120947
Iteration 7/25 | Loss: 0.00120947
Iteration 8/25 | Loss: 0.00120947
Iteration 9/25 | Loss: 0.00120947
Iteration 10/25 | Loss: 0.00120947
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012094725389033556, 0.0012094725389033556, 0.0012094725389033556, 0.0012094725389033556, 0.0012094725389033556]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012094725389033556

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31653297
Iteration 2/25 | Loss: 0.00117492
Iteration 3/25 | Loss: 0.00117492
Iteration 4/25 | Loss: 0.00117492
Iteration 5/25 | Loss: 0.00117492
Iteration 6/25 | Loss: 0.00117492
Iteration 7/25 | Loss: 0.00117491
Iteration 8/25 | Loss: 0.00117491
Iteration 9/25 | Loss: 0.00117491
Iteration 10/25 | Loss: 0.00117491
Iteration 11/25 | Loss: 0.00117491
Iteration 12/25 | Loss: 0.00117491
Iteration 13/25 | Loss: 0.00117491
Iteration 14/25 | Loss: 0.00117491
Iteration 15/25 | Loss: 0.00117491
Iteration 16/25 | Loss: 0.00117491
Iteration 17/25 | Loss: 0.00117491
Iteration 18/25 | Loss: 0.00117491
Iteration 19/25 | Loss: 0.00117491
Iteration 20/25 | Loss: 0.00117491
Iteration 21/25 | Loss: 0.00117491
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001174913952127099, 0.001174913952127099, 0.001174913952127099, 0.001174913952127099, 0.001174913952127099]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001174913952127099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117491
Iteration 2/1000 | Loss: 0.00002309
Iteration 3/1000 | Loss: 0.00001632
Iteration 4/1000 | Loss: 0.00001476
Iteration 5/1000 | Loss: 0.00001385
Iteration 6/1000 | Loss: 0.00001324
Iteration 7/1000 | Loss: 0.00001290
Iteration 8/1000 | Loss: 0.00001263
Iteration 9/1000 | Loss: 0.00001233
Iteration 10/1000 | Loss: 0.00001221
Iteration 11/1000 | Loss: 0.00001219
Iteration 12/1000 | Loss: 0.00001212
Iteration 13/1000 | Loss: 0.00001211
Iteration 14/1000 | Loss: 0.00001200
Iteration 15/1000 | Loss: 0.00001197
Iteration 16/1000 | Loss: 0.00001192
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001190
Iteration 20/1000 | Loss: 0.00001189
Iteration 21/1000 | Loss: 0.00001189
Iteration 22/1000 | Loss: 0.00001186
Iteration 23/1000 | Loss: 0.00001185
Iteration 24/1000 | Loss: 0.00001185
Iteration 25/1000 | Loss: 0.00001184
Iteration 26/1000 | Loss: 0.00001183
Iteration 27/1000 | Loss: 0.00001183
Iteration 28/1000 | Loss: 0.00001183
Iteration 29/1000 | Loss: 0.00001182
Iteration 30/1000 | Loss: 0.00001182
Iteration 31/1000 | Loss: 0.00001182
Iteration 32/1000 | Loss: 0.00001182
Iteration 33/1000 | Loss: 0.00001181
Iteration 34/1000 | Loss: 0.00001180
Iteration 35/1000 | Loss: 0.00001180
Iteration 36/1000 | Loss: 0.00001179
Iteration 37/1000 | Loss: 0.00001179
Iteration 38/1000 | Loss: 0.00001178
Iteration 39/1000 | Loss: 0.00001177
Iteration 40/1000 | Loss: 0.00001177
Iteration 41/1000 | Loss: 0.00001176
Iteration 42/1000 | Loss: 0.00001176
Iteration 43/1000 | Loss: 0.00001176
Iteration 44/1000 | Loss: 0.00001175
Iteration 45/1000 | Loss: 0.00001174
Iteration 46/1000 | Loss: 0.00001174
Iteration 47/1000 | Loss: 0.00001173
Iteration 48/1000 | Loss: 0.00001172
Iteration 49/1000 | Loss: 0.00001172
Iteration 50/1000 | Loss: 0.00001172
Iteration 51/1000 | Loss: 0.00001171
Iteration 52/1000 | Loss: 0.00001171
Iteration 53/1000 | Loss: 0.00001170
Iteration 54/1000 | Loss: 0.00001170
Iteration 55/1000 | Loss: 0.00001169
Iteration 56/1000 | Loss: 0.00001169
Iteration 57/1000 | Loss: 0.00001169
Iteration 58/1000 | Loss: 0.00001169
Iteration 59/1000 | Loss: 0.00001168
Iteration 60/1000 | Loss: 0.00001168
Iteration 61/1000 | Loss: 0.00001168
Iteration 62/1000 | Loss: 0.00001168
Iteration 63/1000 | Loss: 0.00001167
Iteration 64/1000 | Loss: 0.00001167
Iteration 65/1000 | Loss: 0.00001167
Iteration 66/1000 | Loss: 0.00001167
Iteration 67/1000 | Loss: 0.00001167
Iteration 68/1000 | Loss: 0.00001167
Iteration 69/1000 | Loss: 0.00001166
Iteration 70/1000 | Loss: 0.00001166
Iteration 71/1000 | Loss: 0.00001166
Iteration 72/1000 | Loss: 0.00001166
Iteration 73/1000 | Loss: 0.00001165
Iteration 74/1000 | Loss: 0.00001165
Iteration 75/1000 | Loss: 0.00001165
Iteration 76/1000 | Loss: 0.00001165
Iteration 77/1000 | Loss: 0.00001164
Iteration 78/1000 | Loss: 0.00001164
Iteration 79/1000 | Loss: 0.00001164
Iteration 80/1000 | Loss: 0.00001164
Iteration 81/1000 | Loss: 0.00001164
Iteration 82/1000 | Loss: 0.00001164
Iteration 83/1000 | Loss: 0.00001163
Iteration 84/1000 | Loss: 0.00001162
Iteration 85/1000 | Loss: 0.00001162
Iteration 86/1000 | Loss: 0.00001162
Iteration 87/1000 | Loss: 0.00001162
Iteration 88/1000 | Loss: 0.00001162
Iteration 89/1000 | Loss: 0.00001162
Iteration 90/1000 | Loss: 0.00001162
Iteration 91/1000 | Loss: 0.00001162
Iteration 92/1000 | Loss: 0.00001162
Iteration 93/1000 | Loss: 0.00001162
Iteration 94/1000 | Loss: 0.00001161
Iteration 95/1000 | Loss: 0.00001161
Iteration 96/1000 | Loss: 0.00001160
Iteration 97/1000 | Loss: 0.00001160
Iteration 98/1000 | Loss: 0.00001160
Iteration 99/1000 | Loss: 0.00001160
Iteration 100/1000 | Loss: 0.00001160
Iteration 101/1000 | Loss: 0.00001160
Iteration 102/1000 | Loss: 0.00001159
Iteration 103/1000 | Loss: 0.00001159
Iteration 104/1000 | Loss: 0.00001159
Iteration 105/1000 | Loss: 0.00001159
Iteration 106/1000 | Loss: 0.00001159
Iteration 107/1000 | Loss: 0.00001159
Iteration 108/1000 | Loss: 0.00001159
Iteration 109/1000 | Loss: 0.00001159
Iteration 110/1000 | Loss: 0.00001159
Iteration 111/1000 | Loss: 0.00001159
Iteration 112/1000 | Loss: 0.00001159
Iteration 113/1000 | Loss: 0.00001159
Iteration 114/1000 | Loss: 0.00001159
Iteration 115/1000 | Loss: 0.00001159
Iteration 116/1000 | Loss: 0.00001158
Iteration 117/1000 | Loss: 0.00001158
Iteration 118/1000 | Loss: 0.00001158
Iteration 119/1000 | Loss: 0.00001158
Iteration 120/1000 | Loss: 0.00001158
Iteration 121/1000 | Loss: 0.00001157
Iteration 122/1000 | Loss: 0.00001157
Iteration 123/1000 | Loss: 0.00001157
Iteration 124/1000 | Loss: 0.00001156
Iteration 125/1000 | Loss: 0.00001156
Iteration 126/1000 | Loss: 0.00001156
Iteration 127/1000 | Loss: 0.00001156
Iteration 128/1000 | Loss: 0.00001156
Iteration 129/1000 | Loss: 0.00001156
Iteration 130/1000 | Loss: 0.00001156
Iteration 131/1000 | Loss: 0.00001155
Iteration 132/1000 | Loss: 0.00001155
Iteration 133/1000 | Loss: 0.00001155
Iteration 134/1000 | Loss: 0.00001155
Iteration 135/1000 | Loss: 0.00001155
Iteration 136/1000 | Loss: 0.00001155
Iteration 137/1000 | Loss: 0.00001154
Iteration 138/1000 | Loss: 0.00001154
Iteration 139/1000 | Loss: 0.00001154
Iteration 140/1000 | Loss: 0.00001154
Iteration 141/1000 | Loss: 0.00001154
Iteration 142/1000 | Loss: 0.00001153
Iteration 143/1000 | Loss: 0.00001153
Iteration 144/1000 | Loss: 0.00001153
Iteration 145/1000 | Loss: 0.00001152
Iteration 146/1000 | Loss: 0.00001152
Iteration 147/1000 | Loss: 0.00001152
Iteration 148/1000 | Loss: 0.00001152
Iteration 149/1000 | Loss: 0.00001151
Iteration 150/1000 | Loss: 0.00001151
Iteration 151/1000 | Loss: 0.00001151
Iteration 152/1000 | Loss: 0.00001150
Iteration 153/1000 | Loss: 0.00001150
Iteration 154/1000 | Loss: 0.00001150
Iteration 155/1000 | Loss: 0.00001149
Iteration 156/1000 | Loss: 0.00001149
Iteration 157/1000 | Loss: 0.00001149
Iteration 158/1000 | Loss: 0.00001148
Iteration 159/1000 | Loss: 0.00001148
Iteration 160/1000 | Loss: 0.00001148
Iteration 161/1000 | Loss: 0.00001148
Iteration 162/1000 | Loss: 0.00001148
Iteration 163/1000 | Loss: 0.00001147
Iteration 164/1000 | Loss: 0.00001147
Iteration 165/1000 | Loss: 0.00001147
Iteration 166/1000 | Loss: 0.00001147
Iteration 167/1000 | Loss: 0.00001146
Iteration 168/1000 | Loss: 0.00001146
Iteration 169/1000 | Loss: 0.00001146
Iteration 170/1000 | Loss: 0.00001145
Iteration 171/1000 | Loss: 0.00001145
Iteration 172/1000 | Loss: 0.00001145
Iteration 173/1000 | Loss: 0.00001145
Iteration 174/1000 | Loss: 0.00001145
Iteration 175/1000 | Loss: 0.00001145
Iteration 176/1000 | Loss: 0.00001145
Iteration 177/1000 | Loss: 0.00001145
Iteration 178/1000 | Loss: 0.00001145
Iteration 179/1000 | Loss: 0.00001145
Iteration 180/1000 | Loss: 0.00001145
Iteration 181/1000 | Loss: 0.00001145
Iteration 182/1000 | Loss: 0.00001145
Iteration 183/1000 | Loss: 0.00001145
Iteration 184/1000 | Loss: 0.00001145
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 184. Stopping optimization.
Last 5 losses: [1.1453057595645078e-05, 1.1453057595645078e-05, 1.1453057595645078e-05, 1.1453057595645078e-05, 1.1453057595645078e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1453057595645078e-05

Optimization complete. Final v2v error: 2.863126754760742 mm

Highest mean error: 3.0213239192962646 mm for frame 31

Lowest mean error: 2.623636245727539 mm for frame 236

Saving results

Total time: 42.70383310317993
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994316
Iteration 2/25 | Loss: 0.00994316
Iteration 3/25 | Loss: 0.00994316
Iteration 4/25 | Loss: 0.00994316
Iteration 5/25 | Loss: 0.00994316
Iteration 6/25 | Loss: 0.00994316
Iteration 7/25 | Loss: 0.00994316
Iteration 8/25 | Loss: 0.00994315
Iteration 9/25 | Loss: 0.00994315
Iteration 10/25 | Loss: 0.00994315
Iteration 11/25 | Loss: 0.00994315
Iteration 12/25 | Loss: 0.00994314
Iteration 13/25 | Loss: 0.00994314
Iteration 14/25 | Loss: 0.00994314
Iteration 15/25 | Loss: 0.00994314
Iteration 16/25 | Loss: 0.00994314
Iteration 17/25 | Loss: 0.00994313
Iteration 18/25 | Loss: 0.00994313
Iteration 19/25 | Loss: 0.00994313
Iteration 20/25 | Loss: 0.00994313
Iteration 21/25 | Loss: 0.00994313
Iteration 22/25 | Loss: 0.00994313
Iteration 23/25 | Loss: 0.00994312
Iteration 24/25 | Loss: 0.00994312
Iteration 25/25 | Loss: 0.00994312

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.63436878
Iteration 2/25 | Loss: 0.16137554
Iteration 3/25 | Loss: 0.15996732
Iteration 4/25 | Loss: 0.15894431
Iteration 5/25 | Loss: 0.15894425
Iteration 6/25 | Loss: 0.15894423
Iteration 7/25 | Loss: 0.15894423
Iteration 8/25 | Loss: 0.15894422
Iteration 9/25 | Loss: 0.15894419
Iteration 10/25 | Loss: 0.15894419
Iteration 11/25 | Loss: 0.15894419
Iteration 12/25 | Loss: 0.15894419
Iteration 13/25 | Loss: 0.15894419
Iteration 14/25 | Loss: 0.15894419
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.15894418954849243, 0.15894418954849243, 0.15894418954849243, 0.15894418954849243, 0.15894418954849243]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.15894418954849243

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.15894419
Iteration 2/1000 | Loss: 0.00614511
Iteration 3/1000 | Loss: 0.00252132
Iteration 4/1000 | Loss: 0.00373667
Iteration 5/1000 | Loss: 0.00077875
Iteration 6/1000 | Loss: 0.00034061
Iteration 7/1000 | Loss: 0.00292966
Iteration 8/1000 | Loss: 0.00022763
Iteration 9/1000 | Loss: 0.00083706
Iteration 10/1000 | Loss: 0.00122540
Iteration 11/1000 | Loss: 0.00044061
Iteration 12/1000 | Loss: 0.00008770
Iteration 13/1000 | Loss: 0.00007095
Iteration 14/1000 | Loss: 0.00006211
Iteration 15/1000 | Loss: 0.00058900
Iteration 16/1000 | Loss: 0.00036344
Iteration 17/1000 | Loss: 0.00029834
Iteration 18/1000 | Loss: 0.00015729
Iteration 19/1000 | Loss: 0.00022945
Iteration 20/1000 | Loss: 0.00006149
Iteration 21/1000 | Loss: 0.00006340
Iteration 22/1000 | Loss: 0.00029552
Iteration 23/1000 | Loss: 0.00003434
Iteration 24/1000 | Loss: 0.00014762
Iteration 25/1000 | Loss: 0.00016692
Iteration 26/1000 | Loss: 0.00015628
Iteration 27/1000 | Loss: 0.00003022
Iteration 28/1000 | Loss: 0.00013170
Iteration 29/1000 | Loss: 0.00009220
Iteration 30/1000 | Loss: 0.00002767
Iteration 31/1000 | Loss: 0.00026101
Iteration 32/1000 | Loss: 0.00002868
Iteration 33/1000 | Loss: 0.00005794
Iteration 34/1000 | Loss: 0.00002569
Iteration 35/1000 | Loss: 0.00002502
Iteration 36/1000 | Loss: 0.00031612
Iteration 37/1000 | Loss: 0.00015062
Iteration 38/1000 | Loss: 0.00007682
Iteration 39/1000 | Loss: 0.00003244
Iteration 40/1000 | Loss: 0.00007191
Iteration 41/1000 | Loss: 0.00003297
Iteration 42/1000 | Loss: 0.00016075
Iteration 43/1000 | Loss: 0.00002339
Iteration 44/1000 | Loss: 0.00013129
Iteration 45/1000 | Loss: 0.00002273
Iteration 46/1000 | Loss: 0.00002237
Iteration 47/1000 | Loss: 0.00002207
Iteration 48/1000 | Loss: 0.00002183
Iteration 49/1000 | Loss: 0.00002181
Iteration 50/1000 | Loss: 0.00002160
Iteration 51/1000 | Loss: 0.00002160
Iteration 52/1000 | Loss: 0.00002160
Iteration 53/1000 | Loss: 0.00002154
Iteration 54/1000 | Loss: 0.00002153
Iteration 55/1000 | Loss: 0.00002147
Iteration 56/1000 | Loss: 0.00016209
Iteration 57/1000 | Loss: 0.00009075
Iteration 58/1000 | Loss: 0.00002376
Iteration 59/1000 | Loss: 0.00002186
Iteration 60/1000 | Loss: 0.00013234
Iteration 61/1000 | Loss: 0.00002144
Iteration 62/1000 | Loss: 0.00002137
Iteration 63/1000 | Loss: 0.00002133
Iteration 64/1000 | Loss: 0.00002133
Iteration 65/1000 | Loss: 0.00002132
Iteration 66/1000 | Loss: 0.00002132
Iteration 67/1000 | Loss: 0.00002131
Iteration 68/1000 | Loss: 0.00002130
Iteration 69/1000 | Loss: 0.00002130
Iteration 70/1000 | Loss: 0.00002129
Iteration 71/1000 | Loss: 0.00002129
Iteration 72/1000 | Loss: 0.00002129
Iteration 73/1000 | Loss: 0.00002129
Iteration 74/1000 | Loss: 0.00002129
Iteration 75/1000 | Loss: 0.00002129
Iteration 76/1000 | Loss: 0.00002129
Iteration 77/1000 | Loss: 0.00002129
Iteration 78/1000 | Loss: 0.00002128
Iteration 79/1000 | Loss: 0.00002128
Iteration 80/1000 | Loss: 0.00002128
Iteration 81/1000 | Loss: 0.00002128
Iteration 82/1000 | Loss: 0.00002127
Iteration 83/1000 | Loss: 0.00002127
Iteration 84/1000 | Loss: 0.00002126
Iteration 85/1000 | Loss: 0.00002125
Iteration 86/1000 | Loss: 0.00002124
Iteration 87/1000 | Loss: 0.00002124
Iteration 88/1000 | Loss: 0.00002124
Iteration 89/1000 | Loss: 0.00002124
Iteration 90/1000 | Loss: 0.00002124
Iteration 91/1000 | Loss: 0.00002124
Iteration 92/1000 | Loss: 0.00002124
Iteration 93/1000 | Loss: 0.00002124
Iteration 94/1000 | Loss: 0.00002124
Iteration 95/1000 | Loss: 0.00002123
Iteration 96/1000 | Loss: 0.00002123
Iteration 97/1000 | Loss: 0.00002123
Iteration 98/1000 | Loss: 0.00002123
Iteration 99/1000 | Loss: 0.00002123
Iteration 100/1000 | Loss: 0.00002123
Iteration 101/1000 | Loss: 0.00002123
Iteration 102/1000 | Loss: 0.00002123
Iteration 103/1000 | Loss: 0.00002123
Iteration 104/1000 | Loss: 0.00002123
Iteration 105/1000 | Loss: 0.00002123
Iteration 106/1000 | Loss: 0.00002123
Iteration 107/1000 | Loss: 0.00002123
Iteration 108/1000 | Loss: 0.00002123
Iteration 109/1000 | Loss: 0.00002123
Iteration 110/1000 | Loss: 0.00002123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 110. Stopping optimization.
Last 5 losses: [2.123431295331102e-05, 2.123431295331102e-05, 2.123431295331102e-05, 2.123431295331102e-05, 2.123431295331102e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.123431295331102e-05

Optimization complete. Final v2v error: 3.959620952606201 mm

Highest mean error: 4.54942512512207 mm for frame 230

Lowest mean error: 3.586682081222534 mm for frame 178

Saving results

Total time: 100.6238899230957
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00741571
Iteration 2/25 | Loss: 0.00139230
Iteration 3/25 | Loss: 0.00125520
Iteration 4/25 | Loss: 0.00124292
Iteration 5/25 | Loss: 0.00123846
Iteration 6/25 | Loss: 0.00123762
Iteration 7/25 | Loss: 0.00123762
Iteration 8/25 | Loss: 0.00123762
Iteration 9/25 | Loss: 0.00123762
Iteration 10/25 | Loss: 0.00123762
Iteration 11/25 | Loss: 0.00123762
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001237622112967074, 0.001237622112967074, 0.001237622112967074, 0.001237622112967074, 0.001237622112967074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001237622112967074

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25001478
Iteration 2/25 | Loss: 0.00116461
Iteration 3/25 | Loss: 0.00116460
Iteration 4/25 | Loss: 0.00116460
Iteration 5/25 | Loss: 0.00116460
Iteration 6/25 | Loss: 0.00116460
Iteration 7/25 | Loss: 0.00116460
Iteration 8/25 | Loss: 0.00116460
Iteration 9/25 | Loss: 0.00116460
Iteration 10/25 | Loss: 0.00116460
Iteration 11/25 | Loss: 0.00116460
Iteration 12/25 | Loss: 0.00116460
Iteration 13/25 | Loss: 0.00116460
Iteration 14/25 | Loss: 0.00116460
Iteration 15/25 | Loss: 0.00116460
Iteration 16/25 | Loss: 0.00116460
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011646007187664509, 0.0011646007187664509, 0.0011646007187664509, 0.0011646007187664509, 0.0011646007187664509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011646007187664509

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116460
Iteration 2/1000 | Loss: 0.00003340
Iteration 3/1000 | Loss: 0.00002452
Iteration 4/1000 | Loss: 0.00001942
Iteration 5/1000 | Loss: 0.00001838
Iteration 6/1000 | Loss: 0.00001734
Iteration 7/1000 | Loss: 0.00001673
Iteration 8/1000 | Loss: 0.00001643
Iteration 9/1000 | Loss: 0.00001618
Iteration 10/1000 | Loss: 0.00001598
Iteration 11/1000 | Loss: 0.00001584
Iteration 12/1000 | Loss: 0.00001579
Iteration 13/1000 | Loss: 0.00001579
Iteration 14/1000 | Loss: 0.00001578
Iteration 15/1000 | Loss: 0.00001578
Iteration 16/1000 | Loss: 0.00001578
Iteration 17/1000 | Loss: 0.00001578
Iteration 18/1000 | Loss: 0.00001578
Iteration 19/1000 | Loss: 0.00001578
Iteration 20/1000 | Loss: 0.00001578
Iteration 21/1000 | Loss: 0.00001578
Iteration 22/1000 | Loss: 0.00001578
Iteration 23/1000 | Loss: 0.00001577
Iteration 24/1000 | Loss: 0.00001565
Iteration 25/1000 | Loss: 0.00001561
Iteration 26/1000 | Loss: 0.00001561
Iteration 27/1000 | Loss: 0.00001561
Iteration 28/1000 | Loss: 0.00001561
Iteration 29/1000 | Loss: 0.00001559
Iteration 30/1000 | Loss: 0.00001559
Iteration 31/1000 | Loss: 0.00001555
Iteration 32/1000 | Loss: 0.00001551
Iteration 33/1000 | Loss: 0.00001549
Iteration 34/1000 | Loss: 0.00001547
Iteration 35/1000 | Loss: 0.00001545
Iteration 36/1000 | Loss: 0.00001544
Iteration 37/1000 | Loss: 0.00001543
Iteration 38/1000 | Loss: 0.00001542
Iteration 39/1000 | Loss: 0.00001541
Iteration 40/1000 | Loss: 0.00001541
Iteration 41/1000 | Loss: 0.00001540
Iteration 42/1000 | Loss: 0.00001538
Iteration 43/1000 | Loss: 0.00001536
Iteration 44/1000 | Loss: 0.00001536
Iteration 45/1000 | Loss: 0.00001531
Iteration 46/1000 | Loss: 0.00001521
Iteration 47/1000 | Loss: 0.00001516
Iteration 48/1000 | Loss: 0.00001516
Iteration 49/1000 | Loss: 0.00001515
Iteration 50/1000 | Loss: 0.00001515
Iteration 51/1000 | Loss: 0.00001515
Iteration 52/1000 | Loss: 0.00001515
Iteration 53/1000 | Loss: 0.00001514
Iteration 54/1000 | Loss: 0.00001514
Iteration 55/1000 | Loss: 0.00001513
Iteration 56/1000 | Loss: 0.00001513
Iteration 57/1000 | Loss: 0.00001512
Iteration 58/1000 | Loss: 0.00001512
Iteration 59/1000 | Loss: 0.00001512
Iteration 60/1000 | Loss: 0.00001512
Iteration 61/1000 | Loss: 0.00001512
Iteration 62/1000 | Loss: 0.00001512
Iteration 63/1000 | Loss: 0.00001512
Iteration 64/1000 | Loss: 0.00001512
Iteration 65/1000 | Loss: 0.00001512
Iteration 66/1000 | Loss: 0.00001512
Iteration 67/1000 | Loss: 0.00001511
Iteration 68/1000 | Loss: 0.00001511
Iteration 69/1000 | Loss: 0.00001511
Iteration 70/1000 | Loss: 0.00001511
Iteration 71/1000 | Loss: 0.00001511
Iteration 72/1000 | Loss: 0.00001511
Iteration 73/1000 | Loss: 0.00001510
Iteration 74/1000 | Loss: 0.00001510
Iteration 75/1000 | Loss: 0.00001510
Iteration 76/1000 | Loss: 0.00001510
Iteration 77/1000 | Loss: 0.00001510
Iteration 78/1000 | Loss: 0.00001509
Iteration 79/1000 | Loss: 0.00001509
Iteration 80/1000 | Loss: 0.00001509
Iteration 81/1000 | Loss: 0.00001508
Iteration 82/1000 | Loss: 0.00001508
Iteration 83/1000 | Loss: 0.00001508
Iteration 84/1000 | Loss: 0.00001507
Iteration 85/1000 | Loss: 0.00001507
Iteration 86/1000 | Loss: 0.00001507
Iteration 87/1000 | Loss: 0.00001507
Iteration 88/1000 | Loss: 0.00001507
Iteration 89/1000 | Loss: 0.00001506
Iteration 90/1000 | Loss: 0.00001506
Iteration 91/1000 | Loss: 0.00001506
Iteration 92/1000 | Loss: 0.00001506
Iteration 93/1000 | Loss: 0.00001505
Iteration 94/1000 | Loss: 0.00001505
Iteration 95/1000 | Loss: 0.00001505
Iteration 96/1000 | Loss: 0.00001505
Iteration 97/1000 | Loss: 0.00001505
Iteration 98/1000 | Loss: 0.00001505
Iteration 99/1000 | Loss: 0.00001505
Iteration 100/1000 | Loss: 0.00001505
Iteration 101/1000 | Loss: 0.00001505
Iteration 102/1000 | Loss: 0.00001505
Iteration 103/1000 | Loss: 0.00001505
Iteration 104/1000 | Loss: 0.00001505
Iteration 105/1000 | Loss: 0.00001505
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001505
Iteration 108/1000 | Loss: 0.00001504
Iteration 109/1000 | Loss: 0.00001504
Iteration 110/1000 | Loss: 0.00001504
Iteration 111/1000 | Loss: 0.00001503
Iteration 112/1000 | Loss: 0.00001503
Iteration 113/1000 | Loss: 0.00001503
Iteration 114/1000 | Loss: 0.00001502
Iteration 115/1000 | Loss: 0.00001502
Iteration 116/1000 | Loss: 0.00001502
Iteration 117/1000 | Loss: 0.00001501
Iteration 118/1000 | Loss: 0.00001501
Iteration 119/1000 | Loss: 0.00001501
Iteration 120/1000 | Loss: 0.00001501
Iteration 121/1000 | Loss: 0.00001501
Iteration 122/1000 | Loss: 0.00001501
Iteration 123/1000 | Loss: 0.00001501
Iteration 124/1000 | Loss: 0.00001501
Iteration 125/1000 | Loss: 0.00001501
Iteration 126/1000 | Loss: 0.00001501
Iteration 127/1000 | Loss: 0.00001501
Iteration 128/1000 | Loss: 0.00001501
Iteration 129/1000 | Loss: 0.00001500
Iteration 130/1000 | Loss: 0.00001500
Iteration 131/1000 | Loss: 0.00001500
Iteration 132/1000 | Loss: 0.00001500
Iteration 133/1000 | Loss: 0.00001500
Iteration 134/1000 | Loss: 0.00001500
Iteration 135/1000 | Loss: 0.00001500
Iteration 136/1000 | Loss: 0.00001500
Iteration 137/1000 | Loss: 0.00001500
Iteration 138/1000 | Loss: 0.00001500
Iteration 139/1000 | Loss: 0.00001500
Iteration 140/1000 | Loss: 0.00001499
Iteration 141/1000 | Loss: 0.00001499
Iteration 142/1000 | Loss: 0.00001499
Iteration 143/1000 | Loss: 0.00001499
Iteration 144/1000 | Loss: 0.00001499
Iteration 145/1000 | Loss: 0.00001499
Iteration 146/1000 | Loss: 0.00001499
Iteration 147/1000 | Loss: 0.00001499
Iteration 148/1000 | Loss: 0.00001498
Iteration 149/1000 | Loss: 0.00001498
Iteration 150/1000 | Loss: 0.00001498
Iteration 151/1000 | Loss: 0.00001498
Iteration 152/1000 | Loss: 0.00001498
Iteration 153/1000 | Loss: 0.00001498
Iteration 154/1000 | Loss: 0.00001498
Iteration 155/1000 | Loss: 0.00001498
Iteration 156/1000 | Loss: 0.00001498
Iteration 157/1000 | Loss: 0.00001498
Iteration 158/1000 | Loss: 0.00001498
Iteration 159/1000 | Loss: 0.00001498
Iteration 160/1000 | Loss: 0.00001497
Iteration 161/1000 | Loss: 0.00001497
Iteration 162/1000 | Loss: 0.00001497
Iteration 163/1000 | Loss: 0.00001497
Iteration 164/1000 | Loss: 0.00001496
Iteration 165/1000 | Loss: 0.00001496
Iteration 166/1000 | Loss: 0.00001496
Iteration 167/1000 | Loss: 0.00001496
Iteration 168/1000 | Loss: 0.00001496
Iteration 169/1000 | Loss: 0.00001496
Iteration 170/1000 | Loss: 0.00001496
Iteration 171/1000 | Loss: 0.00001496
Iteration 172/1000 | Loss: 0.00001496
Iteration 173/1000 | Loss: 0.00001496
Iteration 174/1000 | Loss: 0.00001496
Iteration 175/1000 | Loss: 0.00001496
Iteration 176/1000 | Loss: 0.00001496
Iteration 177/1000 | Loss: 0.00001496
Iteration 178/1000 | Loss: 0.00001496
Iteration 179/1000 | Loss: 0.00001496
Iteration 180/1000 | Loss: 0.00001496
Iteration 181/1000 | Loss: 0.00001496
Iteration 182/1000 | Loss: 0.00001496
Iteration 183/1000 | Loss: 0.00001496
Iteration 184/1000 | Loss: 0.00001496
Iteration 185/1000 | Loss: 0.00001496
Iteration 186/1000 | Loss: 0.00001496
Iteration 187/1000 | Loss: 0.00001496
Iteration 188/1000 | Loss: 0.00001496
Iteration 189/1000 | Loss: 0.00001496
Iteration 190/1000 | Loss: 0.00001496
Iteration 191/1000 | Loss: 0.00001496
Iteration 192/1000 | Loss: 0.00001496
Iteration 193/1000 | Loss: 0.00001496
Iteration 194/1000 | Loss: 0.00001496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 194. Stopping optimization.
Last 5 losses: [1.4956569430069067e-05, 1.4956569430069067e-05, 1.4956569430069067e-05, 1.4956569430069067e-05, 1.4956569430069067e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4956569430069067e-05

Optimization complete. Final v2v error: 3.3133461475372314 mm

Highest mean error: 3.6502630710601807 mm for frame 3

Lowest mean error: 3.0180394649505615 mm for frame 239

Saving results

Total time: 45.41595220565796
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00677931
Iteration 2/25 | Loss: 0.00183432
Iteration 3/25 | Loss: 0.00142136
Iteration 4/25 | Loss: 0.00136787
Iteration 5/25 | Loss: 0.00135410
Iteration 6/25 | Loss: 0.00135170
Iteration 7/25 | Loss: 0.00134299
Iteration 8/25 | Loss: 0.00131286
Iteration 9/25 | Loss: 0.00130002
Iteration 10/25 | Loss: 0.00128902
Iteration 11/25 | Loss: 0.00128135
Iteration 12/25 | Loss: 0.00127382
Iteration 13/25 | Loss: 0.00127013
Iteration 14/25 | Loss: 0.00126783
Iteration 15/25 | Loss: 0.00126740
Iteration 16/25 | Loss: 0.00126650
Iteration 17/25 | Loss: 0.00126706
Iteration 18/25 | Loss: 0.00126582
Iteration 19/25 | Loss: 0.00126657
Iteration 20/25 | Loss: 0.00126633
Iteration 21/25 | Loss: 0.00126641
Iteration 22/25 | Loss: 0.00126625
Iteration 23/25 | Loss: 0.00126631
Iteration 24/25 | Loss: 0.00126646
Iteration 25/25 | Loss: 0.00126620

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.23472953
Iteration 2/25 | Loss: 0.00128225
Iteration 3/25 | Loss: 0.00128222
Iteration 4/25 | Loss: 0.00128222
Iteration 5/25 | Loss: 0.00128222
Iteration 6/25 | Loss: 0.00128222
Iteration 7/25 | Loss: 0.00128222
Iteration 8/25 | Loss: 0.00128222
Iteration 9/25 | Loss: 0.00128222
Iteration 10/25 | Loss: 0.00128222
Iteration 11/25 | Loss: 0.00128222
Iteration 12/25 | Loss: 0.00128222
Iteration 13/25 | Loss: 0.00128222
Iteration 14/25 | Loss: 0.00128222
Iteration 15/25 | Loss: 0.00128222
Iteration 16/25 | Loss: 0.00128222
Iteration 17/25 | Loss: 0.00128222
Iteration 18/25 | Loss: 0.00128222
Iteration 19/25 | Loss: 0.00128222
Iteration 20/25 | Loss: 0.00128222
Iteration 21/25 | Loss: 0.00128222
Iteration 22/25 | Loss: 0.00128222
Iteration 23/25 | Loss: 0.00128222
Iteration 24/25 | Loss: 0.00128222
Iteration 25/25 | Loss: 0.00128222

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00128222
Iteration 2/1000 | Loss: 0.00004278
Iteration 3/1000 | Loss: 0.00003869
Iteration 4/1000 | Loss: 0.00003382
Iteration 5/1000 | Loss: 0.00003114
Iteration 6/1000 | Loss: 0.00003025
Iteration 7/1000 | Loss: 0.00003177
Iteration 8/1000 | Loss: 0.00002872
Iteration 9/1000 | Loss: 0.00002319
Iteration 10/1000 | Loss: 0.00003199
Iteration 11/1000 | Loss: 0.00002554
Iteration 12/1000 | Loss: 0.00003480
Iteration 13/1000 | Loss: 0.00002587
Iteration 14/1000 | Loss: 0.00002811
Iteration 15/1000 | Loss: 0.00002730
Iteration 16/1000 | Loss: 0.00003310
Iteration 17/1000 | Loss: 0.00003294
Iteration 18/1000 | Loss: 0.00002961
Iteration 19/1000 | Loss: 0.00002456
Iteration 20/1000 | Loss: 0.00002303
Iteration 21/1000 | Loss: 0.00002223
Iteration 22/1000 | Loss: 0.00002192
Iteration 23/1000 | Loss: 0.00002128
Iteration 24/1000 | Loss: 0.00002029
Iteration 25/1000 | Loss: 0.00001996
Iteration 26/1000 | Loss: 0.00001980
Iteration 27/1000 | Loss: 0.00001978
Iteration 28/1000 | Loss: 0.00001974
Iteration 29/1000 | Loss: 0.00001960
Iteration 30/1000 | Loss: 0.00001957
Iteration 31/1000 | Loss: 0.00001957
Iteration 32/1000 | Loss: 0.00001953
Iteration 33/1000 | Loss: 0.00001953
Iteration 34/1000 | Loss: 0.00001953
Iteration 35/1000 | Loss: 0.00001952
Iteration 36/1000 | Loss: 0.00001952
Iteration 37/1000 | Loss: 0.00001951
Iteration 38/1000 | Loss: 0.00001951
Iteration 39/1000 | Loss: 0.00001949
Iteration 40/1000 | Loss: 0.00001948
Iteration 41/1000 | Loss: 0.00001947
Iteration 42/1000 | Loss: 0.00001947
Iteration 43/1000 | Loss: 0.00001946
Iteration 44/1000 | Loss: 0.00001946
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001944
Iteration 47/1000 | Loss: 0.00001944
Iteration 48/1000 | Loss: 0.00001942
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001932
Iteration 51/1000 | Loss: 0.00001931
Iteration 52/1000 | Loss: 0.00001931
Iteration 53/1000 | Loss: 0.00001929
Iteration 54/1000 | Loss: 0.00001928
Iteration 55/1000 | Loss: 0.00001927
Iteration 56/1000 | Loss: 0.00001926
Iteration 57/1000 | Loss: 0.00001926
Iteration 58/1000 | Loss: 0.00001925
Iteration 59/1000 | Loss: 0.00001925
Iteration 60/1000 | Loss: 0.00001924
Iteration 61/1000 | Loss: 0.00001924
Iteration 62/1000 | Loss: 0.00001923
Iteration 63/1000 | Loss: 0.00001923
Iteration 64/1000 | Loss: 0.00001922
Iteration 65/1000 | Loss: 0.00001922
Iteration 66/1000 | Loss: 0.00001921
Iteration 67/1000 | Loss: 0.00001921
Iteration 68/1000 | Loss: 0.00001921
Iteration 69/1000 | Loss: 0.00001921
Iteration 70/1000 | Loss: 0.00001921
Iteration 71/1000 | Loss: 0.00001921
Iteration 72/1000 | Loss: 0.00001920
Iteration 73/1000 | Loss: 0.00001919
Iteration 74/1000 | Loss: 0.00001919
Iteration 75/1000 | Loss: 0.00001919
Iteration 76/1000 | Loss: 0.00001918
Iteration 77/1000 | Loss: 0.00001918
Iteration 78/1000 | Loss: 0.00001918
Iteration 79/1000 | Loss: 0.00001918
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001917
Iteration 82/1000 | Loss: 0.00001917
Iteration 83/1000 | Loss: 0.00001917
Iteration 84/1000 | Loss: 0.00001917
Iteration 85/1000 | Loss: 0.00001917
Iteration 86/1000 | Loss: 0.00001917
Iteration 87/1000 | Loss: 0.00001917
Iteration 88/1000 | Loss: 0.00001916
Iteration 89/1000 | Loss: 0.00001916
Iteration 90/1000 | Loss: 0.00001916
Iteration 91/1000 | Loss: 0.00001916
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001915
Iteration 94/1000 | Loss: 0.00001915
Iteration 95/1000 | Loss: 0.00001915
Iteration 96/1000 | Loss: 0.00001915
Iteration 97/1000 | Loss: 0.00001915
Iteration 98/1000 | Loss: 0.00001915
Iteration 99/1000 | Loss: 0.00001915
Iteration 100/1000 | Loss: 0.00001915
Iteration 101/1000 | Loss: 0.00001914
Iteration 102/1000 | Loss: 0.00001914
Iteration 103/1000 | Loss: 0.00001914
Iteration 104/1000 | Loss: 0.00001914
Iteration 105/1000 | Loss: 0.00001914
Iteration 106/1000 | Loss: 0.00001913
Iteration 107/1000 | Loss: 0.00001913
Iteration 108/1000 | Loss: 0.00001913
Iteration 109/1000 | Loss: 0.00001913
Iteration 110/1000 | Loss: 0.00001913
Iteration 111/1000 | Loss: 0.00001913
Iteration 112/1000 | Loss: 0.00001913
Iteration 113/1000 | Loss: 0.00001913
Iteration 114/1000 | Loss: 0.00001913
Iteration 115/1000 | Loss: 0.00001913
Iteration 116/1000 | Loss: 0.00001913
Iteration 117/1000 | Loss: 0.00001913
Iteration 118/1000 | Loss: 0.00001913
Iteration 119/1000 | Loss: 0.00001913
Iteration 120/1000 | Loss: 0.00001913
Iteration 121/1000 | Loss: 0.00001913
Iteration 122/1000 | Loss: 0.00001912
Iteration 123/1000 | Loss: 0.00001912
Iteration 124/1000 | Loss: 0.00001912
Iteration 125/1000 | Loss: 0.00001912
Iteration 126/1000 | Loss: 0.00001912
Iteration 127/1000 | Loss: 0.00001912
Iteration 128/1000 | Loss: 0.00001911
Iteration 129/1000 | Loss: 0.00001911
Iteration 130/1000 | Loss: 0.00001911
Iteration 131/1000 | Loss: 0.00001911
Iteration 132/1000 | Loss: 0.00001911
Iteration 133/1000 | Loss: 0.00001911
Iteration 134/1000 | Loss: 0.00001911
Iteration 135/1000 | Loss: 0.00001910
Iteration 136/1000 | Loss: 0.00001910
Iteration 137/1000 | Loss: 0.00001910
Iteration 138/1000 | Loss: 0.00001910
Iteration 139/1000 | Loss: 0.00001910
Iteration 140/1000 | Loss: 0.00001910
Iteration 141/1000 | Loss: 0.00001910
Iteration 142/1000 | Loss: 0.00001910
Iteration 143/1000 | Loss: 0.00001910
Iteration 144/1000 | Loss: 0.00001909
Iteration 145/1000 | Loss: 0.00001909
Iteration 146/1000 | Loss: 0.00001909
Iteration 147/1000 | Loss: 0.00001909
Iteration 148/1000 | Loss: 0.00001909
Iteration 149/1000 | Loss: 0.00001909
Iteration 150/1000 | Loss: 0.00001909
Iteration 151/1000 | Loss: 0.00001909
Iteration 152/1000 | Loss: 0.00001909
Iteration 153/1000 | Loss: 0.00001909
Iteration 154/1000 | Loss: 0.00001909
Iteration 155/1000 | Loss: 0.00001909
Iteration 156/1000 | Loss: 0.00001909
Iteration 157/1000 | Loss: 0.00001909
Iteration 158/1000 | Loss: 0.00001909
Iteration 159/1000 | Loss: 0.00001909
Iteration 160/1000 | Loss: 0.00001909
Iteration 161/1000 | Loss: 0.00001909
Iteration 162/1000 | Loss: 0.00001909
Iteration 163/1000 | Loss: 0.00001909
Iteration 164/1000 | Loss: 0.00001909
Iteration 165/1000 | Loss: 0.00001909
Iteration 166/1000 | Loss: 0.00001909
Iteration 167/1000 | Loss: 0.00001909
Iteration 168/1000 | Loss: 0.00001909
Iteration 169/1000 | Loss: 0.00001909
Iteration 170/1000 | Loss: 0.00001909
Iteration 171/1000 | Loss: 0.00001909
Iteration 172/1000 | Loss: 0.00001909
Iteration 173/1000 | Loss: 0.00001909
Iteration 174/1000 | Loss: 0.00001909
Iteration 175/1000 | Loss: 0.00001909
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.9090935893473215e-05, 1.9090935893473215e-05, 1.9090935893473215e-05, 1.9090935893473215e-05, 1.9090935893473215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9090935893473215e-05

Optimization complete. Final v2v error: 3.538356304168701 mm

Highest mean error: 11.334358215332031 mm for frame 16

Lowest mean error: 2.8453636169433594 mm for frame 190

Saving results

Total time: 105.47942018508911
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00470911
Iteration 2/25 | Loss: 0.00127619
Iteration 3/25 | Loss: 0.00119585
Iteration 4/25 | Loss: 0.00118702
Iteration 5/25 | Loss: 0.00118541
Iteration 6/25 | Loss: 0.00118541
Iteration 7/25 | Loss: 0.00118541
Iteration 8/25 | Loss: 0.00118541
Iteration 9/25 | Loss: 0.00118541
Iteration 10/25 | Loss: 0.00118541
Iteration 11/25 | Loss: 0.00118541
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011854053009301424, 0.0011854053009301424, 0.0011854053009301424, 0.0011854053009301424, 0.0011854053009301424]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011854053009301424

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.78743410
Iteration 2/25 | Loss: 0.00112071
Iteration 3/25 | Loss: 0.00112070
Iteration 4/25 | Loss: 0.00112070
Iteration 5/25 | Loss: 0.00112070
Iteration 6/25 | Loss: 0.00112070
Iteration 7/25 | Loss: 0.00112070
Iteration 8/25 | Loss: 0.00112070
Iteration 9/25 | Loss: 0.00112070
Iteration 10/25 | Loss: 0.00112070
Iteration 11/25 | Loss: 0.00112070
Iteration 12/25 | Loss: 0.00112070
Iteration 13/25 | Loss: 0.00112070
Iteration 14/25 | Loss: 0.00112070
Iteration 15/25 | Loss: 0.00112070
Iteration 16/25 | Loss: 0.00112070
Iteration 17/25 | Loss: 0.00112070
Iteration 18/25 | Loss: 0.00112070
Iteration 19/25 | Loss: 0.00112070
Iteration 20/25 | Loss: 0.00112070
Iteration 21/25 | Loss: 0.00112070
Iteration 22/25 | Loss: 0.00112070
Iteration 23/25 | Loss: 0.00112070
Iteration 24/25 | Loss: 0.00112070
Iteration 25/25 | Loss: 0.00112070

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112070
Iteration 2/1000 | Loss: 0.00002211
Iteration 3/1000 | Loss: 0.00001650
Iteration 4/1000 | Loss: 0.00001465
Iteration 5/1000 | Loss: 0.00001358
Iteration 6/1000 | Loss: 0.00001288
Iteration 7/1000 | Loss: 0.00001254
Iteration 8/1000 | Loss: 0.00001216
Iteration 9/1000 | Loss: 0.00001179
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001165
Iteration 12/1000 | Loss: 0.00001155
Iteration 13/1000 | Loss: 0.00001144
Iteration 14/1000 | Loss: 0.00001142
Iteration 15/1000 | Loss: 0.00001134
Iteration 16/1000 | Loss: 0.00001124
Iteration 17/1000 | Loss: 0.00001124
Iteration 18/1000 | Loss: 0.00001124
Iteration 19/1000 | Loss: 0.00001124
Iteration 20/1000 | Loss: 0.00001124
Iteration 21/1000 | Loss: 0.00001120
Iteration 22/1000 | Loss: 0.00001111
Iteration 23/1000 | Loss: 0.00001097
Iteration 24/1000 | Loss: 0.00001096
Iteration 25/1000 | Loss: 0.00001090
Iteration 26/1000 | Loss: 0.00001082
Iteration 27/1000 | Loss: 0.00001077
Iteration 28/1000 | Loss: 0.00001077
Iteration 29/1000 | Loss: 0.00001070
Iteration 30/1000 | Loss: 0.00001070
Iteration 31/1000 | Loss: 0.00001068
Iteration 32/1000 | Loss: 0.00001065
Iteration 33/1000 | Loss: 0.00001065
Iteration 34/1000 | Loss: 0.00001064
Iteration 35/1000 | Loss: 0.00001064
Iteration 36/1000 | Loss: 0.00001063
Iteration 37/1000 | Loss: 0.00001063
Iteration 38/1000 | Loss: 0.00001062
Iteration 39/1000 | Loss: 0.00001062
Iteration 40/1000 | Loss: 0.00001059
Iteration 41/1000 | Loss: 0.00001058
Iteration 42/1000 | Loss: 0.00001058
Iteration 43/1000 | Loss: 0.00001058
Iteration 44/1000 | Loss: 0.00001058
Iteration 45/1000 | Loss: 0.00001058
Iteration 46/1000 | Loss: 0.00001058
Iteration 47/1000 | Loss: 0.00001058
Iteration 48/1000 | Loss: 0.00001058
Iteration 49/1000 | Loss: 0.00001056
Iteration 50/1000 | Loss: 0.00001048
Iteration 51/1000 | Loss: 0.00001046
Iteration 52/1000 | Loss: 0.00001042
Iteration 53/1000 | Loss: 0.00001042
Iteration 54/1000 | Loss: 0.00001042
Iteration 55/1000 | Loss: 0.00001042
Iteration 56/1000 | Loss: 0.00001042
Iteration 57/1000 | Loss: 0.00001041
Iteration 58/1000 | Loss: 0.00001041
Iteration 59/1000 | Loss: 0.00001038
Iteration 60/1000 | Loss: 0.00001038
Iteration 61/1000 | Loss: 0.00001037
Iteration 62/1000 | Loss: 0.00001037
Iteration 63/1000 | Loss: 0.00001036
Iteration 64/1000 | Loss: 0.00001032
Iteration 65/1000 | Loss: 0.00001032
Iteration 66/1000 | Loss: 0.00001032
Iteration 67/1000 | Loss: 0.00001032
Iteration 68/1000 | Loss: 0.00001031
Iteration 69/1000 | Loss: 0.00001031
Iteration 70/1000 | Loss: 0.00001030
Iteration 71/1000 | Loss: 0.00001030
Iteration 72/1000 | Loss: 0.00001030
Iteration 73/1000 | Loss: 0.00001030
Iteration 74/1000 | Loss: 0.00001030
Iteration 75/1000 | Loss: 0.00001029
Iteration 76/1000 | Loss: 0.00001029
Iteration 77/1000 | Loss: 0.00001029
Iteration 78/1000 | Loss: 0.00001029
Iteration 79/1000 | Loss: 0.00001029
Iteration 80/1000 | Loss: 0.00001029
Iteration 81/1000 | Loss: 0.00001029
Iteration 82/1000 | Loss: 0.00001029
Iteration 83/1000 | Loss: 0.00001029
Iteration 84/1000 | Loss: 0.00001029
Iteration 85/1000 | Loss: 0.00001029
Iteration 86/1000 | Loss: 0.00001029
Iteration 87/1000 | Loss: 0.00001029
Iteration 88/1000 | Loss: 0.00001029
Iteration 89/1000 | Loss: 0.00001029
Iteration 90/1000 | Loss: 0.00001029
Iteration 91/1000 | Loss: 0.00001029
Iteration 92/1000 | Loss: 0.00001029
Iteration 93/1000 | Loss: 0.00001029
Iteration 94/1000 | Loss: 0.00001029
Iteration 95/1000 | Loss: 0.00001029
Iteration 96/1000 | Loss: 0.00001029
Iteration 97/1000 | Loss: 0.00001029
Iteration 98/1000 | Loss: 0.00001029
Iteration 99/1000 | Loss: 0.00001029
Iteration 100/1000 | Loss: 0.00001029
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [1.0288159501214977e-05, 1.0288159501214977e-05, 1.0288159501214977e-05, 1.0288159501214977e-05, 1.0288159501214977e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0288159501214977e-05

Optimization complete. Final v2v error: 2.8014073371887207 mm

Highest mean error: 3.004978656768799 mm for frame 110

Lowest mean error: 2.6428580284118652 mm for frame 2

Saving results

Total time: 42.30874156951904
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518958
Iteration 2/25 | Loss: 0.00158641
Iteration 3/25 | Loss: 0.00132136
Iteration 4/25 | Loss: 0.00130187
Iteration 5/25 | Loss: 0.00129648
Iteration 6/25 | Loss: 0.00129479
Iteration 7/25 | Loss: 0.00129423
Iteration 8/25 | Loss: 0.00129423
Iteration 9/25 | Loss: 0.00129423
Iteration 10/25 | Loss: 0.00129423
Iteration 11/25 | Loss: 0.00129423
Iteration 12/25 | Loss: 0.00129423
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012942285975441337, 0.0012942285975441337, 0.0012942285975441337, 0.0012942285975441337, 0.0012942285975441337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012942285975441337

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25329530
Iteration 2/25 | Loss: 0.00114406
Iteration 3/25 | Loss: 0.00114404
Iteration 4/25 | Loss: 0.00114404
Iteration 5/25 | Loss: 0.00114404
Iteration 6/25 | Loss: 0.00114404
Iteration 7/25 | Loss: 0.00114404
Iteration 8/25 | Loss: 0.00114404
Iteration 9/25 | Loss: 0.00114404
Iteration 10/25 | Loss: 0.00114403
Iteration 11/25 | Loss: 0.00114403
Iteration 12/25 | Loss: 0.00114403
Iteration 13/25 | Loss: 0.00114403
Iteration 14/25 | Loss: 0.00114403
Iteration 15/25 | Loss: 0.00114403
Iteration 16/25 | Loss: 0.00114403
Iteration 17/25 | Loss: 0.00114403
Iteration 18/25 | Loss: 0.00114403
Iteration 19/25 | Loss: 0.00114403
Iteration 20/25 | Loss: 0.00114403
Iteration 21/25 | Loss: 0.00114403
Iteration 22/25 | Loss: 0.00114403
Iteration 23/25 | Loss: 0.00114403
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011440340895205736, 0.0011440340895205736, 0.0011440340895205736, 0.0011440340895205736, 0.0011440340895205736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011440340895205736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114403
Iteration 2/1000 | Loss: 0.00004587
Iteration 3/1000 | Loss: 0.00003049
Iteration 4/1000 | Loss: 0.00002662
Iteration 5/1000 | Loss: 0.00002473
Iteration 6/1000 | Loss: 0.00002376
Iteration 7/1000 | Loss: 0.00002302
Iteration 8/1000 | Loss: 0.00002243
Iteration 9/1000 | Loss: 0.00002212
Iteration 10/1000 | Loss: 0.00002184
Iteration 11/1000 | Loss: 0.00002159
Iteration 12/1000 | Loss: 0.00002135
Iteration 13/1000 | Loss: 0.00002116
Iteration 14/1000 | Loss: 0.00002095
Iteration 15/1000 | Loss: 0.00002083
Iteration 16/1000 | Loss: 0.00002073
Iteration 17/1000 | Loss: 0.00002071
Iteration 18/1000 | Loss: 0.00002063
Iteration 19/1000 | Loss: 0.00002061
Iteration 20/1000 | Loss: 0.00002060
Iteration 21/1000 | Loss: 0.00002055
Iteration 22/1000 | Loss: 0.00002055
Iteration 23/1000 | Loss: 0.00002053
Iteration 24/1000 | Loss: 0.00002052
Iteration 25/1000 | Loss: 0.00002052
Iteration 26/1000 | Loss: 0.00002050
Iteration 27/1000 | Loss: 0.00002049
Iteration 28/1000 | Loss: 0.00002049
Iteration 29/1000 | Loss: 0.00002046
Iteration 30/1000 | Loss: 0.00002046
Iteration 31/1000 | Loss: 0.00002043
Iteration 32/1000 | Loss: 0.00002043
Iteration 33/1000 | Loss: 0.00002043
Iteration 34/1000 | Loss: 0.00002043
Iteration 35/1000 | Loss: 0.00002043
Iteration 36/1000 | Loss: 0.00002043
Iteration 37/1000 | Loss: 0.00002043
Iteration 38/1000 | Loss: 0.00002042
Iteration 39/1000 | Loss: 0.00002042
Iteration 40/1000 | Loss: 0.00002041
Iteration 41/1000 | Loss: 0.00002040
Iteration 42/1000 | Loss: 0.00002039
Iteration 43/1000 | Loss: 0.00002039
Iteration 44/1000 | Loss: 0.00002038
Iteration 45/1000 | Loss: 0.00002038
Iteration 46/1000 | Loss: 0.00002033
Iteration 47/1000 | Loss: 0.00002033
Iteration 48/1000 | Loss: 0.00002033
Iteration 49/1000 | Loss: 0.00002032
Iteration 50/1000 | Loss: 0.00002031
Iteration 51/1000 | Loss: 0.00002030
Iteration 52/1000 | Loss: 0.00002030
Iteration 53/1000 | Loss: 0.00002030
Iteration 54/1000 | Loss: 0.00002029
Iteration 55/1000 | Loss: 0.00002029
Iteration 56/1000 | Loss: 0.00002028
Iteration 57/1000 | Loss: 0.00002028
Iteration 58/1000 | Loss: 0.00002028
Iteration 59/1000 | Loss: 0.00002028
Iteration 60/1000 | Loss: 0.00002027
Iteration 61/1000 | Loss: 0.00002027
Iteration 62/1000 | Loss: 0.00002027
Iteration 63/1000 | Loss: 0.00002027
Iteration 64/1000 | Loss: 0.00002027
Iteration 65/1000 | Loss: 0.00002027
Iteration 66/1000 | Loss: 0.00002027
Iteration 67/1000 | Loss: 0.00002027
Iteration 68/1000 | Loss: 0.00002027
Iteration 69/1000 | Loss: 0.00002027
Iteration 70/1000 | Loss: 0.00002027
Iteration 71/1000 | Loss: 0.00002027
Iteration 72/1000 | Loss: 0.00002026
Iteration 73/1000 | Loss: 0.00002026
Iteration 74/1000 | Loss: 0.00002025
Iteration 75/1000 | Loss: 0.00002025
Iteration 76/1000 | Loss: 0.00002025
Iteration 77/1000 | Loss: 0.00002025
Iteration 78/1000 | Loss: 0.00002024
Iteration 79/1000 | Loss: 0.00002024
Iteration 80/1000 | Loss: 0.00002024
Iteration 81/1000 | Loss: 0.00002024
Iteration 82/1000 | Loss: 0.00002024
Iteration 83/1000 | Loss: 0.00002024
Iteration 84/1000 | Loss: 0.00002023
Iteration 85/1000 | Loss: 0.00002023
Iteration 86/1000 | Loss: 0.00002023
Iteration 87/1000 | Loss: 0.00002023
Iteration 88/1000 | Loss: 0.00002023
Iteration 89/1000 | Loss: 0.00002023
Iteration 90/1000 | Loss: 0.00002023
Iteration 91/1000 | Loss: 0.00002023
Iteration 92/1000 | Loss: 0.00002022
Iteration 93/1000 | Loss: 0.00002022
Iteration 94/1000 | Loss: 0.00002022
Iteration 95/1000 | Loss: 0.00002022
Iteration 96/1000 | Loss: 0.00002022
Iteration 97/1000 | Loss: 0.00002022
Iteration 98/1000 | Loss: 0.00002022
Iteration 99/1000 | Loss: 0.00002022
Iteration 100/1000 | Loss: 0.00002022
Iteration 101/1000 | Loss: 0.00002022
Iteration 102/1000 | Loss: 0.00002022
Iteration 103/1000 | Loss: 0.00002022
Iteration 104/1000 | Loss: 0.00002022
Iteration 105/1000 | Loss: 0.00002022
Iteration 106/1000 | Loss: 0.00002022
Iteration 107/1000 | Loss: 0.00002022
Iteration 108/1000 | Loss: 0.00002022
Iteration 109/1000 | Loss: 0.00002022
Iteration 110/1000 | Loss: 0.00002022
Iteration 111/1000 | Loss: 0.00002022
Iteration 112/1000 | Loss: 0.00002022
Iteration 113/1000 | Loss: 0.00002022
Iteration 114/1000 | Loss: 0.00002022
Iteration 115/1000 | Loss: 0.00002022
Iteration 116/1000 | Loss: 0.00002022
Iteration 117/1000 | Loss: 0.00002022
Iteration 118/1000 | Loss: 0.00002022
Iteration 119/1000 | Loss: 0.00002021
Iteration 120/1000 | Loss: 0.00002021
Iteration 121/1000 | Loss: 0.00002021
Iteration 122/1000 | Loss: 0.00002021
Iteration 123/1000 | Loss: 0.00002021
Iteration 124/1000 | Loss: 0.00002021
Iteration 125/1000 | Loss: 0.00002021
Iteration 126/1000 | Loss: 0.00002021
Iteration 127/1000 | Loss: 0.00002021
Iteration 128/1000 | Loss: 0.00002021
Iteration 129/1000 | Loss: 0.00002021
Iteration 130/1000 | Loss: 0.00002021
Iteration 131/1000 | Loss: 0.00002021
Iteration 132/1000 | Loss: 0.00002021
Iteration 133/1000 | Loss: 0.00002021
Iteration 134/1000 | Loss: 0.00002021
Iteration 135/1000 | Loss: 0.00002021
Iteration 136/1000 | Loss: 0.00002021
Iteration 137/1000 | Loss: 0.00002021
Iteration 138/1000 | Loss: 0.00002021
Iteration 139/1000 | Loss: 0.00002021
Iteration 140/1000 | Loss: 0.00002021
Iteration 141/1000 | Loss: 0.00002021
Iteration 142/1000 | Loss: 0.00002021
Iteration 143/1000 | Loss: 0.00002021
Iteration 144/1000 | Loss: 0.00002021
Iteration 145/1000 | Loss: 0.00002021
Iteration 146/1000 | Loss: 0.00002021
Iteration 147/1000 | Loss: 0.00002021
Iteration 148/1000 | Loss: 0.00002021
Iteration 149/1000 | Loss: 0.00002021
Iteration 150/1000 | Loss: 0.00002021
Iteration 151/1000 | Loss: 0.00002021
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 151. Stopping optimization.
Last 5 losses: [2.0214614778524265e-05, 2.0214614778524265e-05, 2.0214614778524265e-05, 2.0214614778524265e-05, 2.0214614778524265e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0214614778524265e-05

Optimization complete. Final v2v error: 3.656071424484253 mm

Highest mean error: 5.64503812789917 mm for frame 58

Lowest mean error: 2.7756948471069336 mm for frame 3

Saving results

Total time: 43.87833094596863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918741
Iteration 2/25 | Loss: 0.00175452
Iteration 3/25 | Loss: 0.00135633
Iteration 4/25 | Loss: 0.00133109
Iteration 5/25 | Loss: 0.00132444
Iteration 6/25 | Loss: 0.00132248
Iteration 7/25 | Loss: 0.00132247
Iteration 8/25 | Loss: 0.00132247
Iteration 9/25 | Loss: 0.00132247
Iteration 10/25 | Loss: 0.00132247
Iteration 11/25 | Loss: 0.00132247
Iteration 12/25 | Loss: 0.00132247
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001322466298006475, 0.001322466298006475, 0.001322466298006475, 0.001322466298006475, 0.001322466298006475]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001322466298006475

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87683839
Iteration 2/25 | Loss: 0.00113842
Iteration 3/25 | Loss: 0.00113841
Iteration 4/25 | Loss: 0.00113840
Iteration 5/25 | Loss: 0.00113840
Iteration 6/25 | Loss: 0.00113840
Iteration 7/25 | Loss: 0.00113840
Iteration 8/25 | Loss: 0.00113840
Iteration 9/25 | Loss: 0.00113840
Iteration 10/25 | Loss: 0.00113840
Iteration 11/25 | Loss: 0.00113840
Iteration 12/25 | Loss: 0.00113840
Iteration 13/25 | Loss: 0.00113840
Iteration 14/25 | Loss: 0.00113840
Iteration 15/25 | Loss: 0.00113840
Iteration 16/25 | Loss: 0.00113840
Iteration 17/25 | Loss: 0.00113840
Iteration 18/25 | Loss: 0.00113840
Iteration 19/25 | Loss: 0.00113840
Iteration 20/25 | Loss: 0.00113840
Iteration 21/25 | Loss: 0.00113840
Iteration 22/25 | Loss: 0.00113840
Iteration 23/25 | Loss: 0.00113840
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0011384011013433337, 0.0011384011013433337, 0.0011384011013433337, 0.0011384011013433337, 0.0011384011013433337]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011384011013433337

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113840
Iteration 2/1000 | Loss: 0.00005309
Iteration 3/1000 | Loss: 0.00004110
Iteration 4/1000 | Loss: 0.00003506
Iteration 5/1000 | Loss: 0.00003339
Iteration 6/1000 | Loss: 0.00003181
Iteration 7/1000 | Loss: 0.00003080
Iteration 8/1000 | Loss: 0.00002994
Iteration 9/1000 | Loss: 0.00002920
Iteration 10/1000 | Loss: 0.00002884
Iteration 11/1000 | Loss: 0.00002844
Iteration 12/1000 | Loss: 0.00002816
Iteration 13/1000 | Loss: 0.00002787
Iteration 14/1000 | Loss: 0.00002763
Iteration 15/1000 | Loss: 0.00002736
Iteration 16/1000 | Loss: 0.00002718
Iteration 17/1000 | Loss: 0.00002711
Iteration 18/1000 | Loss: 0.00002706
Iteration 19/1000 | Loss: 0.00002693
Iteration 20/1000 | Loss: 0.00002677
Iteration 21/1000 | Loss: 0.00002675
Iteration 22/1000 | Loss: 0.00002672
Iteration 23/1000 | Loss: 0.00002667
Iteration 24/1000 | Loss: 0.00002657
Iteration 25/1000 | Loss: 0.00002657
Iteration 26/1000 | Loss: 0.00002655
Iteration 27/1000 | Loss: 0.00002654
Iteration 28/1000 | Loss: 0.00002649
Iteration 29/1000 | Loss: 0.00002646
Iteration 30/1000 | Loss: 0.00002645
Iteration 31/1000 | Loss: 0.00002641
Iteration 32/1000 | Loss: 0.00002641
Iteration 33/1000 | Loss: 0.00002640
Iteration 34/1000 | Loss: 0.00002640
Iteration 35/1000 | Loss: 0.00002639
Iteration 36/1000 | Loss: 0.00002639
Iteration 37/1000 | Loss: 0.00002639
Iteration 38/1000 | Loss: 0.00002639
Iteration 39/1000 | Loss: 0.00002638
Iteration 40/1000 | Loss: 0.00002638
Iteration 41/1000 | Loss: 0.00002638
Iteration 42/1000 | Loss: 0.00002638
Iteration 43/1000 | Loss: 0.00002637
Iteration 44/1000 | Loss: 0.00002637
Iteration 45/1000 | Loss: 0.00002637
Iteration 46/1000 | Loss: 0.00002637
Iteration 47/1000 | Loss: 0.00002637
Iteration 48/1000 | Loss: 0.00002637
Iteration 49/1000 | Loss: 0.00002637
Iteration 50/1000 | Loss: 0.00002637
Iteration 51/1000 | Loss: 0.00002637
Iteration 52/1000 | Loss: 0.00002637
Iteration 53/1000 | Loss: 0.00002636
Iteration 54/1000 | Loss: 0.00002636
Iteration 55/1000 | Loss: 0.00002636
Iteration 56/1000 | Loss: 0.00002636
Iteration 57/1000 | Loss: 0.00002636
Iteration 58/1000 | Loss: 0.00002635
Iteration 59/1000 | Loss: 0.00002635
Iteration 60/1000 | Loss: 0.00002635
Iteration 61/1000 | Loss: 0.00002635
Iteration 62/1000 | Loss: 0.00002634
Iteration 63/1000 | Loss: 0.00002634
Iteration 64/1000 | Loss: 0.00002634
Iteration 65/1000 | Loss: 0.00002634
Iteration 66/1000 | Loss: 0.00002634
Iteration 67/1000 | Loss: 0.00002634
Iteration 68/1000 | Loss: 0.00002634
Iteration 69/1000 | Loss: 0.00002634
Iteration 70/1000 | Loss: 0.00002634
Iteration 71/1000 | Loss: 0.00002634
Iteration 72/1000 | Loss: 0.00002634
Iteration 73/1000 | Loss: 0.00002633
Iteration 74/1000 | Loss: 0.00002633
Iteration 75/1000 | Loss: 0.00002633
Iteration 76/1000 | Loss: 0.00002633
Iteration 77/1000 | Loss: 0.00002633
Iteration 78/1000 | Loss: 0.00002633
Iteration 79/1000 | Loss: 0.00002633
Iteration 80/1000 | Loss: 0.00002633
Iteration 81/1000 | Loss: 0.00002632
Iteration 82/1000 | Loss: 0.00002632
Iteration 83/1000 | Loss: 0.00002632
Iteration 84/1000 | Loss: 0.00002632
Iteration 85/1000 | Loss: 0.00002632
Iteration 86/1000 | Loss: 0.00002632
Iteration 87/1000 | Loss: 0.00002632
Iteration 88/1000 | Loss: 0.00002632
Iteration 89/1000 | Loss: 0.00002632
Iteration 90/1000 | Loss: 0.00002632
Iteration 91/1000 | Loss: 0.00002632
Iteration 92/1000 | Loss: 0.00002632
Iteration 93/1000 | Loss: 0.00002632
Iteration 94/1000 | Loss: 0.00002632
Iteration 95/1000 | Loss: 0.00002632
Iteration 96/1000 | Loss: 0.00002632
Iteration 97/1000 | Loss: 0.00002632
Iteration 98/1000 | Loss: 0.00002632
Iteration 99/1000 | Loss: 0.00002632
Iteration 100/1000 | Loss: 0.00002632
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 100. Stopping optimization.
Last 5 losses: [2.6316520234104246e-05, 2.6316520234104246e-05, 2.6316520234104246e-05, 2.6316520234104246e-05, 2.6316520234104246e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6316520234104246e-05

Optimization complete. Final v2v error: 4.293792724609375 mm

Highest mean error: 5.115224361419678 mm for frame 132

Lowest mean error: 3.430206060409546 mm for frame 25

Saving results

Total time: 43.10235333442688
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370463
Iteration 2/25 | Loss: 0.00137184
Iteration 3/25 | Loss: 0.00124108
Iteration 4/25 | Loss: 0.00122736
Iteration 5/25 | Loss: 0.00122099
Iteration 6/25 | Loss: 0.00121929
Iteration 7/25 | Loss: 0.00121929
Iteration 8/25 | Loss: 0.00121929
Iteration 9/25 | Loss: 0.00121929
Iteration 10/25 | Loss: 0.00121929
Iteration 11/25 | Loss: 0.00121929
Iteration 12/25 | Loss: 0.00121929
Iteration 13/25 | Loss: 0.00121929
Iteration 14/25 | Loss: 0.00121929
Iteration 15/25 | Loss: 0.00121929
Iteration 16/25 | Loss: 0.00121929
Iteration 17/25 | Loss: 0.00121929
Iteration 18/25 | Loss: 0.00121929
Iteration 19/25 | Loss: 0.00121929
Iteration 20/25 | Loss: 0.00121929
Iteration 21/25 | Loss: 0.00121929
Iteration 22/25 | Loss: 0.00121929
Iteration 23/25 | Loss: 0.00121929
Iteration 24/25 | Loss: 0.00121929
Iteration 25/25 | Loss: 0.00121929

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.82780510
Iteration 2/25 | Loss: 0.00153422
Iteration 3/25 | Loss: 0.00153422
Iteration 4/25 | Loss: 0.00153422
Iteration 5/25 | Loss: 0.00153422
Iteration 6/25 | Loss: 0.00153422
Iteration 7/25 | Loss: 0.00153422
Iteration 8/25 | Loss: 0.00153422
Iteration 9/25 | Loss: 0.00153422
Iteration 10/25 | Loss: 0.00153422
Iteration 11/25 | Loss: 0.00153422
Iteration 12/25 | Loss: 0.00153422
Iteration 13/25 | Loss: 0.00153422
Iteration 14/25 | Loss: 0.00153422
Iteration 15/25 | Loss: 0.00153422
Iteration 16/25 | Loss: 0.00153422
Iteration 17/25 | Loss: 0.00153422
Iteration 18/25 | Loss: 0.00153422
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0015342168044298887, 0.0015342168044298887, 0.0015342168044298887, 0.0015342168044298887, 0.0015342168044298887]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015342168044298887

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00153422
Iteration 2/1000 | Loss: 0.00003922
Iteration 3/1000 | Loss: 0.00002679
Iteration 4/1000 | Loss: 0.00002108
Iteration 5/1000 | Loss: 0.00001960
Iteration 6/1000 | Loss: 0.00001850
Iteration 7/1000 | Loss: 0.00001754
Iteration 8/1000 | Loss: 0.00001709
Iteration 9/1000 | Loss: 0.00001674
Iteration 10/1000 | Loss: 0.00001645
Iteration 11/1000 | Loss: 0.00001617
Iteration 12/1000 | Loss: 0.00001592
Iteration 13/1000 | Loss: 0.00001572
Iteration 14/1000 | Loss: 0.00001563
Iteration 15/1000 | Loss: 0.00001561
Iteration 16/1000 | Loss: 0.00001555
Iteration 17/1000 | Loss: 0.00001552
Iteration 18/1000 | Loss: 0.00001549
Iteration 19/1000 | Loss: 0.00001549
Iteration 20/1000 | Loss: 0.00001544
Iteration 21/1000 | Loss: 0.00001544
Iteration 22/1000 | Loss: 0.00001543
Iteration 23/1000 | Loss: 0.00001543
Iteration 24/1000 | Loss: 0.00001541
Iteration 25/1000 | Loss: 0.00001533
Iteration 26/1000 | Loss: 0.00001530
Iteration 27/1000 | Loss: 0.00001529
Iteration 28/1000 | Loss: 0.00001528
Iteration 29/1000 | Loss: 0.00001527
Iteration 30/1000 | Loss: 0.00001526
Iteration 31/1000 | Loss: 0.00001524
Iteration 32/1000 | Loss: 0.00001524
Iteration 33/1000 | Loss: 0.00001518
Iteration 34/1000 | Loss: 0.00001507
Iteration 35/1000 | Loss: 0.00001506
Iteration 36/1000 | Loss: 0.00001501
Iteration 37/1000 | Loss: 0.00001501
Iteration 38/1000 | Loss: 0.00001500
Iteration 39/1000 | Loss: 0.00001500
Iteration 40/1000 | Loss: 0.00001499
Iteration 41/1000 | Loss: 0.00001499
Iteration 42/1000 | Loss: 0.00001499
Iteration 43/1000 | Loss: 0.00001499
Iteration 44/1000 | Loss: 0.00001498
Iteration 45/1000 | Loss: 0.00001498
Iteration 46/1000 | Loss: 0.00001498
Iteration 47/1000 | Loss: 0.00001498
Iteration 48/1000 | Loss: 0.00001497
Iteration 49/1000 | Loss: 0.00001497
Iteration 50/1000 | Loss: 0.00001495
Iteration 51/1000 | Loss: 0.00001495
Iteration 52/1000 | Loss: 0.00001494
Iteration 53/1000 | Loss: 0.00001493
Iteration 54/1000 | Loss: 0.00001493
Iteration 55/1000 | Loss: 0.00001492
Iteration 56/1000 | Loss: 0.00001487
Iteration 57/1000 | Loss: 0.00001484
Iteration 58/1000 | Loss: 0.00001484
Iteration 59/1000 | Loss: 0.00001484
Iteration 60/1000 | Loss: 0.00001484
Iteration 61/1000 | Loss: 0.00001484
Iteration 62/1000 | Loss: 0.00001484
Iteration 63/1000 | Loss: 0.00001484
Iteration 64/1000 | Loss: 0.00001484
Iteration 65/1000 | Loss: 0.00001482
Iteration 66/1000 | Loss: 0.00001482
Iteration 67/1000 | Loss: 0.00001481
Iteration 68/1000 | Loss: 0.00001481
Iteration 69/1000 | Loss: 0.00001481
Iteration 70/1000 | Loss: 0.00001481
Iteration 71/1000 | Loss: 0.00001481
Iteration 72/1000 | Loss: 0.00001481
Iteration 73/1000 | Loss: 0.00001481
Iteration 74/1000 | Loss: 0.00001480
Iteration 75/1000 | Loss: 0.00001479
Iteration 76/1000 | Loss: 0.00001478
Iteration 77/1000 | Loss: 0.00001478
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001478
Iteration 80/1000 | Loss: 0.00001478
Iteration 81/1000 | Loss: 0.00001477
Iteration 82/1000 | Loss: 0.00001477
Iteration 83/1000 | Loss: 0.00001477
Iteration 84/1000 | Loss: 0.00001477
Iteration 85/1000 | Loss: 0.00001477
Iteration 86/1000 | Loss: 0.00001477
Iteration 87/1000 | Loss: 0.00001477
Iteration 88/1000 | Loss: 0.00001477
Iteration 89/1000 | Loss: 0.00001477
Iteration 90/1000 | Loss: 0.00001477
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001476
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001475
Iteration 95/1000 | Loss: 0.00001474
Iteration 96/1000 | Loss: 0.00001474
Iteration 97/1000 | Loss: 0.00001473
Iteration 98/1000 | Loss: 0.00001473
Iteration 99/1000 | Loss: 0.00001473
Iteration 100/1000 | Loss: 0.00001473
Iteration 101/1000 | Loss: 0.00001473
Iteration 102/1000 | Loss: 0.00001473
Iteration 103/1000 | Loss: 0.00001473
Iteration 104/1000 | Loss: 0.00001473
Iteration 105/1000 | Loss: 0.00001472
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001472
Iteration 109/1000 | Loss: 0.00001471
Iteration 110/1000 | Loss: 0.00001471
Iteration 111/1000 | Loss: 0.00001471
Iteration 112/1000 | Loss: 0.00001471
Iteration 113/1000 | Loss: 0.00001471
Iteration 114/1000 | Loss: 0.00001471
Iteration 115/1000 | Loss: 0.00001470
Iteration 116/1000 | Loss: 0.00001470
Iteration 117/1000 | Loss: 0.00001470
Iteration 118/1000 | Loss: 0.00001470
Iteration 119/1000 | Loss: 0.00001470
Iteration 120/1000 | Loss: 0.00001469
Iteration 121/1000 | Loss: 0.00001469
Iteration 122/1000 | Loss: 0.00001469
Iteration 123/1000 | Loss: 0.00001469
Iteration 124/1000 | Loss: 0.00001469
Iteration 125/1000 | Loss: 0.00001469
Iteration 126/1000 | Loss: 0.00001469
Iteration 127/1000 | Loss: 0.00001469
Iteration 128/1000 | Loss: 0.00001469
Iteration 129/1000 | Loss: 0.00001469
Iteration 130/1000 | Loss: 0.00001469
Iteration 131/1000 | Loss: 0.00001469
Iteration 132/1000 | Loss: 0.00001468
Iteration 133/1000 | Loss: 0.00001468
Iteration 134/1000 | Loss: 0.00001468
Iteration 135/1000 | Loss: 0.00001468
Iteration 136/1000 | Loss: 0.00001467
Iteration 137/1000 | Loss: 0.00001467
Iteration 138/1000 | Loss: 0.00001467
Iteration 139/1000 | Loss: 0.00001467
Iteration 140/1000 | Loss: 0.00001466
Iteration 141/1000 | Loss: 0.00001466
Iteration 142/1000 | Loss: 0.00001466
Iteration 143/1000 | Loss: 0.00001466
Iteration 144/1000 | Loss: 0.00001466
Iteration 145/1000 | Loss: 0.00001466
Iteration 146/1000 | Loss: 0.00001466
Iteration 147/1000 | Loss: 0.00001466
Iteration 148/1000 | Loss: 0.00001466
Iteration 149/1000 | Loss: 0.00001466
Iteration 150/1000 | Loss: 0.00001466
Iteration 151/1000 | Loss: 0.00001466
Iteration 152/1000 | Loss: 0.00001466
Iteration 153/1000 | Loss: 0.00001466
Iteration 154/1000 | Loss: 0.00001465
Iteration 155/1000 | Loss: 0.00001465
Iteration 156/1000 | Loss: 0.00001465
Iteration 157/1000 | Loss: 0.00001465
Iteration 158/1000 | Loss: 0.00001465
Iteration 159/1000 | Loss: 0.00001465
Iteration 160/1000 | Loss: 0.00001465
Iteration 161/1000 | Loss: 0.00001465
Iteration 162/1000 | Loss: 0.00001465
Iteration 163/1000 | Loss: 0.00001465
Iteration 164/1000 | Loss: 0.00001464
Iteration 165/1000 | Loss: 0.00001464
Iteration 166/1000 | Loss: 0.00001464
Iteration 167/1000 | Loss: 0.00001464
Iteration 168/1000 | Loss: 0.00001464
Iteration 169/1000 | Loss: 0.00001464
Iteration 170/1000 | Loss: 0.00001463
Iteration 171/1000 | Loss: 0.00001463
Iteration 172/1000 | Loss: 0.00001463
Iteration 173/1000 | Loss: 0.00001463
Iteration 174/1000 | Loss: 0.00001463
Iteration 175/1000 | Loss: 0.00001463
Iteration 176/1000 | Loss: 0.00001462
Iteration 177/1000 | Loss: 0.00001462
Iteration 178/1000 | Loss: 0.00001462
Iteration 179/1000 | Loss: 0.00001462
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001460
Iteration 187/1000 | Loss: 0.00001460
Iteration 188/1000 | Loss: 0.00001460
Iteration 189/1000 | Loss: 0.00001460
Iteration 190/1000 | Loss: 0.00001460
Iteration 191/1000 | Loss: 0.00001460
Iteration 192/1000 | Loss: 0.00001460
Iteration 193/1000 | Loss: 0.00001460
Iteration 194/1000 | Loss: 0.00001459
Iteration 195/1000 | Loss: 0.00001459
Iteration 196/1000 | Loss: 0.00001459
Iteration 197/1000 | Loss: 0.00001459
Iteration 198/1000 | Loss: 0.00001459
Iteration 199/1000 | Loss: 0.00001459
Iteration 200/1000 | Loss: 0.00001459
Iteration 201/1000 | Loss: 0.00001459
Iteration 202/1000 | Loss: 0.00001459
Iteration 203/1000 | Loss: 0.00001459
Iteration 204/1000 | Loss: 0.00001458
Iteration 205/1000 | Loss: 0.00001458
Iteration 206/1000 | Loss: 0.00001458
Iteration 207/1000 | Loss: 0.00001458
Iteration 208/1000 | Loss: 0.00001458
Iteration 209/1000 | Loss: 0.00001457
Iteration 210/1000 | Loss: 0.00001457
Iteration 211/1000 | Loss: 0.00001457
Iteration 212/1000 | Loss: 0.00001456
Iteration 213/1000 | Loss: 0.00001456
Iteration 214/1000 | Loss: 0.00001456
Iteration 215/1000 | Loss: 0.00001456
Iteration 216/1000 | Loss: 0.00001456
Iteration 217/1000 | Loss: 0.00001455
Iteration 218/1000 | Loss: 0.00001455
Iteration 219/1000 | Loss: 0.00001455
Iteration 220/1000 | Loss: 0.00001455
Iteration 221/1000 | Loss: 0.00001455
Iteration 222/1000 | Loss: 0.00001455
Iteration 223/1000 | Loss: 0.00001455
Iteration 224/1000 | Loss: 0.00001455
Iteration 225/1000 | Loss: 0.00001455
Iteration 226/1000 | Loss: 0.00001455
Iteration 227/1000 | Loss: 0.00001455
Iteration 228/1000 | Loss: 0.00001455
Iteration 229/1000 | Loss: 0.00001454
Iteration 230/1000 | Loss: 0.00001454
Iteration 231/1000 | Loss: 0.00001454
Iteration 232/1000 | Loss: 0.00001454
Iteration 233/1000 | Loss: 0.00001454
Iteration 234/1000 | Loss: 0.00001454
Iteration 235/1000 | Loss: 0.00001454
Iteration 236/1000 | Loss: 0.00001454
Iteration 237/1000 | Loss: 0.00001454
Iteration 238/1000 | Loss: 0.00001454
Iteration 239/1000 | Loss: 0.00001454
Iteration 240/1000 | Loss: 0.00001454
Iteration 241/1000 | Loss: 0.00001454
Iteration 242/1000 | Loss: 0.00001454
Iteration 243/1000 | Loss: 0.00001453
Iteration 244/1000 | Loss: 0.00001453
Iteration 245/1000 | Loss: 0.00001453
Iteration 246/1000 | Loss: 0.00001453
Iteration 247/1000 | Loss: 0.00001453
Iteration 248/1000 | Loss: 0.00001453
Iteration 249/1000 | Loss: 0.00001453
Iteration 250/1000 | Loss: 0.00001453
Iteration 251/1000 | Loss: 0.00001453
Iteration 252/1000 | Loss: 0.00001453
Iteration 253/1000 | Loss: 0.00001453
Iteration 254/1000 | Loss: 0.00001453
Iteration 255/1000 | Loss: 0.00001453
Iteration 256/1000 | Loss: 0.00001453
Iteration 257/1000 | Loss: 0.00001453
Iteration 258/1000 | Loss: 0.00001453
Iteration 259/1000 | Loss: 0.00001453
Iteration 260/1000 | Loss: 0.00001453
Iteration 261/1000 | Loss: 0.00001453
Iteration 262/1000 | Loss: 0.00001453
Iteration 263/1000 | Loss: 0.00001453
Iteration 264/1000 | Loss: 0.00001453
Iteration 265/1000 | Loss: 0.00001453
Iteration 266/1000 | Loss: 0.00001453
Iteration 267/1000 | Loss: 0.00001453
Iteration 268/1000 | Loss: 0.00001453
Iteration 269/1000 | Loss: 0.00001453
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 269. Stopping optimization.
Last 5 losses: [1.4533729881804902e-05, 1.4533729881804902e-05, 1.4533729881804902e-05, 1.4533729881804902e-05, 1.4533729881804902e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4533729881804902e-05

Optimization complete. Final v2v error: 3.2194466590881348 mm

Highest mean error: 3.3449866771698 mm for frame 237

Lowest mean error: 3.1519088745117188 mm for frame 183

Saving results

Total time: 59.249207973480225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1049/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1049.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1049
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00370787
Iteration 2/25 | Loss: 0.00145920
Iteration 3/25 | Loss: 0.00124357
Iteration 4/25 | Loss: 0.00121236
Iteration 5/25 | Loss: 0.00120574
Iteration 6/25 | Loss: 0.00120385
Iteration 7/25 | Loss: 0.00120360
Iteration 8/25 | Loss: 0.00120360
Iteration 9/25 | Loss: 0.00120359
Iteration 10/25 | Loss: 0.00120360
Iteration 11/25 | Loss: 0.00120359
Iteration 12/25 | Loss: 0.00120360
Iteration 13/25 | Loss: 0.00120360
Iteration 14/25 | Loss: 0.00120360
Iteration 15/25 | Loss: 0.00120359
Iteration 16/25 | Loss: 0.00120359
Iteration 17/25 | Loss: 0.00120359
Iteration 18/25 | Loss: 0.00120359
Iteration 19/25 | Loss: 0.00120360
Iteration 20/25 | Loss: 0.00120360
Iteration 21/25 | Loss: 0.00120360
Iteration 22/25 | Loss: 0.00120359
Iteration 23/25 | Loss: 0.00120360
Iteration 24/25 | Loss: 0.00120360
Iteration 25/25 | Loss: 0.00120360

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37380266
Iteration 2/25 | Loss: 0.00145496
Iteration 3/25 | Loss: 0.00145496
Iteration 4/25 | Loss: 0.00145496
Iteration 5/25 | Loss: 0.00145496
Iteration 6/25 | Loss: 0.00145496
Iteration 7/25 | Loss: 0.00145496
Iteration 8/25 | Loss: 0.00145496
Iteration 9/25 | Loss: 0.00145496
Iteration 10/25 | Loss: 0.00145496
Iteration 11/25 | Loss: 0.00145496
Iteration 12/25 | Loss: 0.00145496
Iteration 13/25 | Loss: 0.00145496
Iteration 14/25 | Loss: 0.00145496
Iteration 15/25 | Loss: 0.00145496
Iteration 16/25 | Loss: 0.00145496
Iteration 17/25 | Loss: 0.00145496
Iteration 18/25 | Loss: 0.00145496
Iteration 19/25 | Loss: 0.00145496
Iteration 20/25 | Loss: 0.00145496
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0014549577608704567, 0.0014549577608704567, 0.0014549577608704567, 0.0014549577608704567, 0.0014549577608704567]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014549577608704567

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145496
Iteration 2/1000 | Loss: 0.00004371
Iteration 3/1000 | Loss: 0.00002662
Iteration 4/1000 | Loss: 0.00001720
Iteration 5/1000 | Loss: 0.00001522
Iteration 6/1000 | Loss: 0.00001426
Iteration 7/1000 | Loss: 0.00001341
Iteration 8/1000 | Loss: 0.00001298
Iteration 9/1000 | Loss: 0.00001260
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001234
Iteration 12/1000 | Loss: 0.00001219
Iteration 13/1000 | Loss: 0.00001212
Iteration 14/1000 | Loss: 0.00001204
Iteration 15/1000 | Loss: 0.00001198
Iteration 16/1000 | Loss: 0.00001197
Iteration 17/1000 | Loss: 0.00001195
Iteration 18/1000 | Loss: 0.00001192
Iteration 19/1000 | Loss: 0.00001191
Iteration 20/1000 | Loss: 0.00001183
Iteration 21/1000 | Loss: 0.00001181
Iteration 22/1000 | Loss: 0.00001181
Iteration 23/1000 | Loss: 0.00001181
Iteration 24/1000 | Loss: 0.00001178
Iteration 25/1000 | Loss: 0.00001175
Iteration 26/1000 | Loss: 0.00001175
Iteration 27/1000 | Loss: 0.00001175
Iteration 28/1000 | Loss: 0.00001175
Iteration 29/1000 | Loss: 0.00001175
Iteration 30/1000 | Loss: 0.00001174
Iteration 31/1000 | Loss: 0.00001174
Iteration 32/1000 | Loss: 0.00001173
Iteration 33/1000 | Loss: 0.00001173
Iteration 34/1000 | Loss: 0.00001173
Iteration 35/1000 | Loss: 0.00001172
Iteration 36/1000 | Loss: 0.00001170
Iteration 37/1000 | Loss: 0.00001170
Iteration 38/1000 | Loss: 0.00001170
Iteration 39/1000 | Loss: 0.00001169
Iteration 40/1000 | Loss: 0.00001169
Iteration 41/1000 | Loss: 0.00001169
Iteration 42/1000 | Loss: 0.00001168
Iteration 43/1000 | Loss: 0.00001168
Iteration 44/1000 | Loss: 0.00001167
Iteration 45/1000 | Loss: 0.00001167
Iteration 46/1000 | Loss: 0.00001166
Iteration 47/1000 | Loss: 0.00001166
Iteration 48/1000 | Loss: 0.00001165
Iteration 49/1000 | Loss: 0.00001165
Iteration 50/1000 | Loss: 0.00001165
Iteration 51/1000 | Loss: 0.00001164
Iteration 52/1000 | Loss: 0.00001164
Iteration 53/1000 | Loss: 0.00001164
Iteration 54/1000 | Loss: 0.00001163
Iteration 55/1000 | Loss: 0.00001163
Iteration 56/1000 | Loss: 0.00001163
Iteration 57/1000 | Loss: 0.00001162
Iteration 58/1000 | Loss: 0.00001162
Iteration 59/1000 | Loss: 0.00001162
Iteration 60/1000 | Loss: 0.00001161
Iteration 61/1000 | Loss: 0.00001161
Iteration 62/1000 | Loss: 0.00001160
Iteration 63/1000 | Loss: 0.00001160
Iteration 64/1000 | Loss: 0.00001160
Iteration 65/1000 | Loss: 0.00001159
Iteration 66/1000 | Loss: 0.00001159
Iteration 67/1000 | Loss: 0.00001159
Iteration 68/1000 | Loss: 0.00001159
Iteration 69/1000 | Loss: 0.00001158
Iteration 70/1000 | Loss: 0.00001158
Iteration 71/1000 | Loss: 0.00001158
Iteration 72/1000 | Loss: 0.00001158
Iteration 73/1000 | Loss: 0.00001158
Iteration 74/1000 | Loss: 0.00001158
Iteration 75/1000 | Loss: 0.00001158
Iteration 76/1000 | Loss: 0.00001157
Iteration 77/1000 | Loss: 0.00001157
Iteration 78/1000 | Loss: 0.00001157
Iteration 79/1000 | Loss: 0.00001157
Iteration 80/1000 | Loss: 0.00001157
Iteration 81/1000 | Loss: 0.00001157
Iteration 82/1000 | Loss: 0.00001157
Iteration 83/1000 | Loss: 0.00001157
Iteration 84/1000 | Loss: 0.00001157
Iteration 85/1000 | Loss: 0.00001157
Iteration 86/1000 | Loss: 0.00001157
Iteration 87/1000 | Loss: 0.00001157
Iteration 88/1000 | Loss: 0.00001157
Iteration 89/1000 | Loss: 0.00001157
Iteration 90/1000 | Loss: 0.00001157
Iteration 91/1000 | Loss: 0.00001156
Iteration 92/1000 | Loss: 0.00001156
Iteration 93/1000 | Loss: 0.00001156
Iteration 94/1000 | Loss: 0.00001156
Iteration 95/1000 | Loss: 0.00001156
Iteration 96/1000 | Loss: 0.00001156
Iteration 97/1000 | Loss: 0.00001156
Iteration 98/1000 | Loss: 0.00001156
Iteration 99/1000 | Loss: 0.00001156
Iteration 100/1000 | Loss: 0.00001156
Iteration 101/1000 | Loss: 0.00001156
Iteration 102/1000 | Loss: 0.00001156
Iteration 103/1000 | Loss: 0.00001155
Iteration 104/1000 | Loss: 0.00001155
Iteration 105/1000 | Loss: 0.00001155
Iteration 106/1000 | Loss: 0.00001155
Iteration 107/1000 | Loss: 0.00001155
Iteration 108/1000 | Loss: 0.00001155
Iteration 109/1000 | Loss: 0.00001155
Iteration 110/1000 | Loss: 0.00001155
Iteration 111/1000 | Loss: 0.00001155
Iteration 112/1000 | Loss: 0.00001155
Iteration 113/1000 | Loss: 0.00001154
Iteration 114/1000 | Loss: 0.00001154
Iteration 115/1000 | Loss: 0.00001154
Iteration 116/1000 | Loss: 0.00001154
Iteration 117/1000 | Loss: 0.00001154
Iteration 118/1000 | Loss: 0.00001154
Iteration 119/1000 | Loss: 0.00001154
Iteration 120/1000 | Loss: 0.00001154
Iteration 121/1000 | Loss: 0.00001154
Iteration 122/1000 | Loss: 0.00001154
Iteration 123/1000 | Loss: 0.00001154
Iteration 124/1000 | Loss: 0.00001153
Iteration 125/1000 | Loss: 0.00001153
Iteration 126/1000 | Loss: 0.00001153
Iteration 127/1000 | Loss: 0.00001153
Iteration 128/1000 | Loss: 0.00001153
Iteration 129/1000 | Loss: 0.00001153
Iteration 130/1000 | Loss: 0.00001153
Iteration 131/1000 | Loss: 0.00001152
Iteration 132/1000 | Loss: 0.00001152
Iteration 133/1000 | Loss: 0.00001152
Iteration 134/1000 | Loss: 0.00001152
Iteration 135/1000 | Loss: 0.00001152
Iteration 136/1000 | Loss: 0.00001151
Iteration 137/1000 | Loss: 0.00001151
Iteration 138/1000 | Loss: 0.00001151
Iteration 139/1000 | Loss: 0.00001151
Iteration 140/1000 | Loss: 0.00001151
Iteration 141/1000 | Loss: 0.00001151
Iteration 142/1000 | Loss: 0.00001151
Iteration 143/1000 | Loss: 0.00001151
Iteration 144/1000 | Loss: 0.00001151
Iteration 145/1000 | Loss: 0.00001150
Iteration 146/1000 | Loss: 0.00001150
Iteration 147/1000 | Loss: 0.00001150
Iteration 148/1000 | Loss: 0.00001150
Iteration 149/1000 | Loss: 0.00001150
Iteration 150/1000 | Loss: 0.00001150
Iteration 151/1000 | Loss: 0.00001150
Iteration 152/1000 | Loss: 0.00001150
Iteration 153/1000 | Loss: 0.00001150
Iteration 154/1000 | Loss: 0.00001149
Iteration 155/1000 | Loss: 0.00001149
Iteration 156/1000 | Loss: 0.00001149
Iteration 157/1000 | Loss: 0.00001149
Iteration 158/1000 | Loss: 0.00001148
Iteration 159/1000 | Loss: 0.00001148
Iteration 160/1000 | Loss: 0.00001148
Iteration 161/1000 | Loss: 0.00001148
Iteration 162/1000 | Loss: 0.00001148
Iteration 163/1000 | Loss: 0.00001148
Iteration 164/1000 | Loss: 0.00001147
Iteration 165/1000 | Loss: 0.00001147
Iteration 166/1000 | Loss: 0.00001147
Iteration 167/1000 | Loss: 0.00001147
Iteration 168/1000 | Loss: 0.00001146
Iteration 169/1000 | Loss: 0.00001146
Iteration 170/1000 | Loss: 0.00001146
Iteration 171/1000 | Loss: 0.00001146
Iteration 172/1000 | Loss: 0.00001146
Iteration 173/1000 | Loss: 0.00001146
Iteration 174/1000 | Loss: 0.00001146
Iteration 175/1000 | Loss: 0.00001146
Iteration 176/1000 | Loss: 0.00001146
Iteration 177/1000 | Loss: 0.00001146
Iteration 178/1000 | Loss: 0.00001146
Iteration 179/1000 | Loss: 0.00001146
Iteration 180/1000 | Loss: 0.00001146
Iteration 181/1000 | Loss: 0.00001146
Iteration 182/1000 | Loss: 0.00001146
Iteration 183/1000 | Loss: 0.00001145
Iteration 184/1000 | Loss: 0.00001145
Iteration 185/1000 | Loss: 0.00001145
Iteration 186/1000 | Loss: 0.00001145
Iteration 187/1000 | Loss: 0.00001145
Iteration 188/1000 | Loss: 0.00001145
Iteration 189/1000 | Loss: 0.00001145
Iteration 190/1000 | Loss: 0.00001145
Iteration 191/1000 | Loss: 0.00001145
Iteration 192/1000 | Loss: 0.00001145
Iteration 193/1000 | Loss: 0.00001144
Iteration 194/1000 | Loss: 0.00001144
Iteration 195/1000 | Loss: 0.00001144
Iteration 196/1000 | Loss: 0.00001144
Iteration 197/1000 | Loss: 0.00001144
Iteration 198/1000 | Loss: 0.00001144
Iteration 199/1000 | Loss: 0.00001143
Iteration 200/1000 | Loss: 0.00001143
Iteration 201/1000 | Loss: 0.00001143
Iteration 202/1000 | Loss: 0.00001143
Iteration 203/1000 | Loss: 0.00001143
Iteration 204/1000 | Loss: 0.00001143
Iteration 205/1000 | Loss: 0.00001143
Iteration 206/1000 | Loss: 0.00001142
Iteration 207/1000 | Loss: 0.00001142
Iteration 208/1000 | Loss: 0.00001142
Iteration 209/1000 | Loss: 0.00001142
Iteration 210/1000 | Loss: 0.00001142
Iteration 211/1000 | Loss: 0.00001142
Iteration 212/1000 | Loss: 0.00001141
Iteration 213/1000 | Loss: 0.00001141
Iteration 214/1000 | Loss: 0.00001141
Iteration 215/1000 | Loss: 0.00001141
Iteration 216/1000 | Loss: 0.00001140
Iteration 217/1000 | Loss: 0.00001140
Iteration 218/1000 | Loss: 0.00001140
Iteration 219/1000 | Loss: 0.00001140
Iteration 220/1000 | Loss: 0.00001140
Iteration 221/1000 | Loss: 0.00001140
Iteration 222/1000 | Loss: 0.00001140
Iteration 223/1000 | Loss: 0.00001140
Iteration 224/1000 | Loss: 0.00001139
Iteration 225/1000 | Loss: 0.00001139
Iteration 226/1000 | Loss: 0.00001139
Iteration 227/1000 | Loss: 0.00001139
Iteration 228/1000 | Loss: 0.00001139
Iteration 229/1000 | Loss: 0.00001139
Iteration 230/1000 | Loss: 0.00001139
Iteration 231/1000 | Loss: 0.00001139
Iteration 232/1000 | Loss: 0.00001139
Iteration 233/1000 | Loss: 0.00001139
Iteration 234/1000 | Loss: 0.00001139
Iteration 235/1000 | Loss: 0.00001138
Iteration 236/1000 | Loss: 0.00001138
Iteration 237/1000 | Loss: 0.00001138
Iteration 238/1000 | Loss: 0.00001138
Iteration 239/1000 | Loss: 0.00001138
Iteration 240/1000 | Loss: 0.00001138
Iteration 241/1000 | Loss: 0.00001138
Iteration 242/1000 | Loss: 0.00001138
Iteration 243/1000 | Loss: 0.00001138
Iteration 244/1000 | Loss: 0.00001138
Iteration 245/1000 | Loss: 0.00001138
Iteration 246/1000 | Loss: 0.00001138
Iteration 247/1000 | Loss: 0.00001138
Iteration 248/1000 | Loss: 0.00001138
Iteration 249/1000 | Loss: 0.00001138
Iteration 250/1000 | Loss: 0.00001138
Iteration 251/1000 | Loss: 0.00001138
Iteration 252/1000 | Loss: 0.00001138
Iteration 253/1000 | Loss: 0.00001138
Iteration 254/1000 | Loss: 0.00001138
Iteration 255/1000 | Loss: 0.00001138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 255. Stopping optimization.
Last 5 losses: [1.1382677257643081e-05, 1.1382677257643081e-05, 1.1382677257643081e-05, 1.1382677257643081e-05, 1.1382677257643081e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1382677257643081e-05

Optimization complete. Final v2v error: 2.933169364929199 mm

Highest mean error: 3.293874740600586 mm for frame 42

Lowest mean error: 2.4568028450012207 mm for frame 3

Saving results

Total time: 44.06937503814697
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1039/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1039.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1039
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00830923
Iteration 2/25 | Loss: 0.00136946
Iteration 3/25 | Loss: 0.00128310
Iteration 4/25 | Loss: 0.00126786
Iteration 5/25 | Loss: 0.00126302
Iteration 6/25 | Loss: 0.00126248
Iteration 7/25 | Loss: 0.00126248
Iteration 8/25 | Loss: 0.00126248
Iteration 9/25 | Loss: 0.00126248
Iteration 10/25 | Loss: 0.00126248
Iteration 11/25 | Loss: 0.00126248
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012624770170077682, 0.0012624770170077682, 0.0012624770170077682, 0.0012624770170077682, 0.0012624770170077682]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012624770170077682

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.29478586
Iteration 2/25 | Loss: 0.00119490
Iteration 3/25 | Loss: 0.00119489
Iteration 4/25 | Loss: 0.00119489
Iteration 5/25 | Loss: 0.00119489
Iteration 6/25 | Loss: 0.00119489
Iteration 7/25 | Loss: 0.00119489
Iteration 8/25 | Loss: 0.00119489
Iteration 9/25 | Loss: 0.00119489
Iteration 10/25 | Loss: 0.00119489
Iteration 11/25 | Loss: 0.00119489
Iteration 12/25 | Loss: 0.00119489
Iteration 13/25 | Loss: 0.00119489
Iteration 14/25 | Loss: 0.00119489
Iteration 15/25 | Loss: 0.00119489
Iteration 16/25 | Loss: 0.00119489
Iteration 17/25 | Loss: 0.00119489
Iteration 18/25 | Loss: 0.00119489
Iteration 19/25 | Loss: 0.00119489
Iteration 20/25 | Loss: 0.00119489
Iteration 21/25 | Loss: 0.00119489
Iteration 22/25 | Loss: 0.00119489
Iteration 23/25 | Loss: 0.00119489
Iteration 24/25 | Loss: 0.00119489
Iteration 25/25 | Loss: 0.00119489

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119489
Iteration 2/1000 | Loss: 0.00004181
Iteration 3/1000 | Loss: 0.00002875
Iteration 4/1000 | Loss: 0.00002435
Iteration 5/1000 | Loss: 0.00002267
Iteration 6/1000 | Loss: 0.00002149
Iteration 7/1000 | Loss: 0.00002056
Iteration 8/1000 | Loss: 0.00002012
Iteration 9/1000 | Loss: 0.00001976
Iteration 10/1000 | Loss: 0.00001951
Iteration 11/1000 | Loss: 0.00001927
Iteration 12/1000 | Loss: 0.00001912
Iteration 13/1000 | Loss: 0.00001909
Iteration 14/1000 | Loss: 0.00001901
Iteration 15/1000 | Loss: 0.00001898
Iteration 16/1000 | Loss: 0.00001895
Iteration 17/1000 | Loss: 0.00001895
Iteration 18/1000 | Loss: 0.00001895
Iteration 19/1000 | Loss: 0.00001894
Iteration 20/1000 | Loss: 0.00001893
Iteration 21/1000 | Loss: 0.00001891
Iteration 22/1000 | Loss: 0.00001890
Iteration 23/1000 | Loss: 0.00001887
Iteration 24/1000 | Loss: 0.00001886
Iteration 25/1000 | Loss: 0.00001883
Iteration 26/1000 | Loss: 0.00001881
Iteration 27/1000 | Loss: 0.00001880
Iteration 28/1000 | Loss: 0.00001879
Iteration 29/1000 | Loss: 0.00001878
Iteration 30/1000 | Loss: 0.00001877
Iteration 31/1000 | Loss: 0.00001876
Iteration 32/1000 | Loss: 0.00001875
Iteration 33/1000 | Loss: 0.00001872
Iteration 34/1000 | Loss: 0.00001872
Iteration 35/1000 | Loss: 0.00001871
Iteration 36/1000 | Loss: 0.00001870
Iteration 37/1000 | Loss: 0.00001868
Iteration 38/1000 | Loss: 0.00001867
Iteration 39/1000 | Loss: 0.00001862
Iteration 40/1000 | Loss: 0.00001859
Iteration 41/1000 | Loss: 0.00001858
Iteration 42/1000 | Loss: 0.00001858
Iteration 43/1000 | Loss: 0.00001858
Iteration 44/1000 | Loss: 0.00001857
Iteration 45/1000 | Loss: 0.00001856
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001856
Iteration 49/1000 | Loss: 0.00001855
Iteration 50/1000 | Loss: 0.00001855
Iteration 51/1000 | Loss: 0.00001854
Iteration 52/1000 | Loss: 0.00001854
Iteration 53/1000 | Loss: 0.00001853
Iteration 54/1000 | Loss: 0.00001853
Iteration 55/1000 | Loss: 0.00001852
Iteration 56/1000 | Loss: 0.00001852
Iteration 57/1000 | Loss: 0.00001852
Iteration 58/1000 | Loss: 0.00001851
Iteration 59/1000 | Loss: 0.00001851
Iteration 60/1000 | Loss: 0.00001851
Iteration 61/1000 | Loss: 0.00001850
Iteration 62/1000 | Loss: 0.00001850
Iteration 63/1000 | Loss: 0.00001850
Iteration 64/1000 | Loss: 0.00001849
Iteration 65/1000 | Loss: 0.00001849
Iteration 66/1000 | Loss: 0.00001848
Iteration 67/1000 | Loss: 0.00001848
Iteration 68/1000 | Loss: 0.00001848
Iteration 69/1000 | Loss: 0.00001847
Iteration 70/1000 | Loss: 0.00001847
Iteration 71/1000 | Loss: 0.00001847
Iteration 72/1000 | Loss: 0.00001846
Iteration 73/1000 | Loss: 0.00001846
Iteration 74/1000 | Loss: 0.00001846
Iteration 75/1000 | Loss: 0.00001845
Iteration 76/1000 | Loss: 0.00001845
Iteration 77/1000 | Loss: 0.00001845
Iteration 78/1000 | Loss: 0.00001844
Iteration 79/1000 | Loss: 0.00001844
Iteration 80/1000 | Loss: 0.00001844
Iteration 81/1000 | Loss: 0.00001843
Iteration 82/1000 | Loss: 0.00001843
Iteration 83/1000 | Loss: 0.00001843
Iteration 84/1000 | Loss: 0.00001842
Iteration 85/1000 | Loss: 0.00001842
Iteration 86/1000 | Loss: 0.00001841
Iteration 87/1000 | Loss: 0.00001841
Iteration 88/1000 | Loss: 0.00001841
Iteration 89/1000 | Loss: 0.00001841
Iteration 90/1000 | Loss: 0.00001840
Iteration 91/1000 | Loss: 0.00001840
Iteration 92/1000 | Loss: 0.00001840
Iteration 93/1000 | Loss: 0.00001840
Iteration 94/1000 | Loss: 0.00001840
Iteration 95/1000 | Loss: 0.00001840
Iteration 96/1000 | Loss: 0.00001840
Iteration 97/1000 | Loss: 0.00001840
Iteration 98/1000 | Loss: 0.00001840
Iteration 99/1000 | Loss: 0.00001840
Iteration 100/1000 | Loss: 0.00001840
Iteration 101/1000 | Loss: 0.00001840
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 101. Stopping optimization.
Last 5 losses: [1.840097684180364e-05, 1.840097684180364e-05, 1.840097684180364e-05, 1.840097684180364e-05, 1.840097684180364e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.840097684180364e-05

Optimization complete. Final v2v error: 3.5315916538238525 mm

Highest mean error: 4.528298854827881 mm for frame 173

Lowest mean error: 2.7859604358673096 mm for frame 11

Saving results

Total time: 40.68661427497864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1056/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1056.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1056
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00918939
Iteration 2/25 | Loss: 0.00201574
Iteration 3/25 | Loss: 0.00155335
Iteration 4/25 | Loss: 0.00149310
Iteration 5/25 | Loss: 0.00149651
Iteration 6/25 | Loss: 0.00149175
Iteration 7/25 | Loss: 0.00146607
Iteration 8/25 | Loss: 0.00145024
Iteration 9/25 | Loss: 0.00144178
Iteration 10/25 | Loss: 0.00143698
Iteration 11/25 | Loss: 0.00143563
Iteration 12/25 | Loss: 0.00143266
Iteration 13/25 | Loss: 0.00143266
Iteration 14/25 | Loss: 0.00143221
Iteration 15/25 | Loss: 0.00143048
Iteration 16/25 | Loss: 0.00143426
Iteration 17/25 | Loss: 0.00143129
Iteration 18/25 | Loss: 0.00143071
Iteration 19/25 | Loss: 0.00142757
Iteration 20/25 | Loss: 0.00142880
Iteration 21/25 | Loss: 0.00142867
Iteration 22/25 | Loss: 0.00142694
Iteration 23/25 | Loss: 0.00143317
Iteration 24/25 | Loss: 0.00143287
Iteration 25/25 | Loss: 0.00142818

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.31042767
Iteration 2/25 | Loss: 0.00308820
Iteration 3/25 | Loss: 0.00308819
Iteration 4/25 | Loss: 0.00308819
Iteration 5/25 | Loss: 0.00308819
Iteration 6/25 | Loss: 0.00308819
Iteration 7/25 | Loss: 0.00308819
Iteration 8/25 | Loss: 0.00308819
Iteration 9/25 | Loss: 0.00308819
Iteration 10/25 | Loss: 0.00308819
Iteration 11/25 | Loss: 0.00308819
Iteration 12/25 | Loss: 0.00308819
Iteration 13/25 | Loss: 0.00308819
Iteration 14/25 | Loss: 0.00308819
Iteration 15/25 | Loss: 0.00308819
Iteration 16/25 | Loss: 0.00308819
Iteration 17/25 | Loss: 0.00308819
Iteration 18/25 | Loss: 0.00308819
Iteration 19/25 | Loss: 0.00308819
Iteration 20/25 | Loss: 0.00308819
Iteration 21/25 | Loss: 0.00308819
Iteration 22/25 | Loss: 0.00308819
Iteration 23/25 | Loss: 0.00308819
Iteration 24/25 | Loss: 0.00308819
Iteration 25/25 | Loss: 0.00308819

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00308819
Iteration 2/1000 | Loss: 0.00049021
Iteration 3/1000 | Loss: 0.00127998
Iteration 4/1000 | Loss: 0.00135720
Iteration 5/1000 | Loss: 0.00196097
Iteration 6/1000 | Loss: 0.00061438
Iteration 7/1000 | Loss: 0.00075305
Iteration 8/1000 | Loss: 0.00029805
Iteration 9/1000 | Loss: 0.00340133
Iteration 10/1000 | Loss: 0.00151429
Iteration 11/1000 | Loss: 0.00086687
Iteration 12/1000 | Loss: 0.00058316
Iteration 13/1000 | Loss: 0.00072309
Iteration 14/1000 | Loss: 0.00225918
Iteration 15/1000 | Loss: 0.00118403
Iteration 16/1000 | Loss: 0.00058614
Iteration 17/1000 | Loss: 0.00095243
Iteration 18/1000 | Loss: 0.00008812
Iteration 19/1000 | Loss: 0.00166868
Iteration 20/1000 | Loss: 0.00057061
Iteration 21/1000 | Loss: 0.00150633
Iteration 22/1000 | Loss: 0.00035425
Iteration 23/1000 | Loss: 0.00019615
Iteration 24/1000 | Loss: 0.00016522
Iteration 25/1000 | Loss: 0.00006339
Iteration 26/1000 | Loss: 0.00005205
Iteration 27/1000 | Loss: 0.00004275
Iteration 28/1000 | Loss: 0.00011841
Iteration 29/1000 | Loss: 0.00084803
Iteration 30/1000 | Loss: 0.00014011
Iteration 31/1000 | Loss: 0.00004470
Iteration 32/1000 | Loss: 0.00017475
Iteration 33/1000 | Loss: 0.00018715
Iteration 34/1000 | Loss: 0.00005477
Iteration 35/1000 | Loss: 0.00006280
Iteration 36/1000 | Loss: 0.00015507
Iteration 37/1000 | Loss: 0.00006356
Iteration 38/1000 | Loss: 0.00010682
Iteration 39/1000 | Loss: 0.00003677
Iteration 40/1000 | Loss: 0.00056692
Iteration 41/1000 | Loss: 0.00072554
Iteration 42/1000 | Loss: 0.00061639
Iteration 43/1000 | Loss: 0.00023015
Iteration 44/1000 | Loss: 0.00015629
Iteration 45/1000 | Loss: 0.00072203
Iteration 46/1000 | Loss: 0.00076129
Iteration 47/1000 | Loss: 0.00007051
Iteration 48/1000 | Loss: 0.00003689
Iteration 49/1000 | Loss: 0.00003183
Iteration 50/1000 | Loss: 0.00020530
Iteration 51/1000 | Loss: 0.00018228
Iteration 52/1000 | Loss: 0.00003913
Iteration 53/1000 | Loss: 0.00003964
Iteration 54/1000 | Loss: 0.00003090
Iteration 55/1000 | Loss: 0.00005261
Iteration 56/1000 | Loss: 0.00005918
Iteration 57/1000 | Loss: 0.00011967
Iteration 58/1000 | Loss: 0.00005777
Iteration 59/1000 | Loss: 0.00017930
Iteration 60/1000 | Loss: 0.00014278
Iteration 61/1000 | Loss: 0.00002920
Iteration 62/1000 | Loss: 0.00057273
Iteration 63/1000 | Loss: 0.00009006
Iteration 64/1000 | Loss: 0.00006519
Iteration 65/1000 | Loss: 0.00005089
Iteration 66/1000 | Loss: 0.00023766
Iteration 67/1000 | Loss: 0.00020090
Iteration 68/1000 | Loss: 0.00013497
Iteration 69/1000 | Loss: 0.00010102
Iteration 70/1000 | Loss: 0.00010998
Iteration 71/1000 | Loss: 0.00016552
Iteration 72/1000 | Loss: 0.00061558
Iteration 73/1000 | Loss: 0.00031629
Iteration 74/1000 | Loss: 0.00012471
Iteration 75/1000 | Loss: 0.00013225
Iteration 76/1000 | Loss: 0.00008275
Iteration 77/1000 | Loss: 0.00017065
Iteration 78/1000 | Loss: 0.00050228
Iteration 79/1000 | Loss: 0.00017760
Iteration 80/1000 | Loss: 0.00014809
Iteration 81/1000 | Loss: 0.00018208
Iteration 82/1000 | Loss: 0.00004282
Iteration 83/1000 | Loss: 0.00003252
Iteration 84/1000 | Loss: 0.00002957
Iteration 85/1000 | Loss: 0.00002799
Iteration 86/1000 | Loss: 0.00002634
Iteration 87/1000 | Loss: 0.00002518
Iteration 88/1000 | Loss: 0.00002427
Iteration 89/1000 | Loss: 0.00002381
Iteration 90/1000 | Loss: 0.00036579
Iteration 91/1000 | Loss: 0.00042011
Iteration 92/1000 | Loss: 0.00096574
Iteration 93/1000 | Loss: 0.00011180
Iteration 94/1000 | Loss: 0.00006305
Iteration 95/1000 | Loss: 0.00003533
Iteration 96/1000 | Loss: 0.00006587
Iteration 97/1000 | Loss: 0.00005722
Iteration 98/1000 | Loss: 0.00006073
Iteration 99/1000 | Loss: 0.00005293
Iteration 100/1000 | Loss: 0.00005835
Iteration 101/1000 | Loss: 0.00029313
Iteration 102/1000 | Loss: 0.00004907
Iteration 103/1000 | Loss: 0.00003668
Iteration 104/1000 | Loss: 0.00003450
Iteration 105/1000 | Loss: 0.00003565
Iteration 106/1000 | Loss: 0.00002918
Iteration 107/1000 | Loss: 0.00002713
Iteration 108/1000 | Loss: 0.00003355
Iteration 109/1000 | Loss: 0.00003083
Iteration 110/1000 | Loss: 0.00006036
Iteration 111/1000 | Loss: 0.00027425
Iteration 112/1000 | Loss: 0.00009376
Iteration 113/1000 | Loss: 0.00015716
Iteration 114/1000 | Loss: 0.00003949
Iteration 115/1000 | Loss: 0.00003476
Iteration 116/1000 | Loss: 0.00008274
Iteration 117/1000 | Loss: 0.00005165
Iteration 118/1000 | Loss: 0.00005148
Iteration 119/1000 | Loss: 0.00005252
Iteration 120/1000 | Loss: 0.00002852
Iteration 121/1000 | Loss: 0.00004742
Iteration 122/1000 | Loss: 0.00004966
Iteration 123/1000 | Loss: 0.00005139
Iteration 124/1000 | Loss: 0.00005033
Iteration 125/1000 | Loss: 0.00004417
Iteration 126/1000 | Loss: 0.00005733
Iteration 127/1000 | Loss: 0.00003920
Iteration 128/1000 | Loss: 0.00005015
Iteration 129/1000 | Loss: 0.00003978
Iteration 130/1000 | Loss: 0.00004899
Iteration 131/1000 | Loss: 0.00005207
Iteration 132/1000 | Loss: 0.00005028
Iteration 133/1000 | Loss: 0.00031805
Iteration 134/1000 | Loss: 0.00024491
Iteration 135/1000 | Loss: 0.00021748
Iteration 136/1000 | Loss: 0.00046566
Iteration 137/1000 | Loss: 0.00004284
Iteration 138/1000 | Loss: 0.00003064
Iteration 139/1000 | Loss: 0.00020156
Iteration 140/1000 | Loss: 0.00016009
Iteration 141/1000 | Loss: 0.00007432
Iteration 142/1000 | Loss: 0.00009996
Iteration 143/1000 | Loss: 0.00023878
Iteration 144/1000 | Loss: 0.00028819
Iteration 145/1000 | Loss: 0.00066964
Iteration 146/1000 | Loss: 0.00013131
Iteration 147/1000 | Loss: 0.00003232
Iteration 148/1000 | Loss: 0.00003025
Iteration 149/1000 | Loss: 0.00002905
Iteration 150/1000 | Loss: 0.00009042
Iteration 151/1000 | Loss: 0.00136295
Iteration 152/1000 | Loss: 0.00041414
Iteration 153/1000 | Loss: 0.00003864
Iteration 154/1000 | Loss: 0.00004135
Iteration 155/1000 | Loss: 0.00003993
Iteration 156/1000 | Loss: 0.00002901
Iteration 157/1000 | Loss: 0.00002706
Iteration 158/1000 | Loss: 0.00002594
Iteration 159/1000 | Loss: 0.00002506
Iteration 160/1000 | Loss: 0.00016559
Iteration 161/1000 | Loss: 0.00042026
Iteration 162/1000 | Loss: 0.00005598
Iteration 163/1000 | Loss: 0.00004368
Iteration 164/1000 | Loss: 0.00010407
Iteration 165/1000 | Loss: 0.00006701
Iteration 166/1000 | Loss: 0.00002473
Iteration 167/1000 | Loss: 0.00003745
Iteration 168/1000 | Loss: 0.00019359
Iteration 169/1000 | Loss: 0.00007426
Iteration 170/1000 | Loss: 0.00003739
Iteration 171/1000 | Loss: 0.00008024
Iteration 172/1000 | Loss: 0.00007785
Iteration 173/1000 | Loss: 0.00005871
Iteration 174/1000 | Loss: 0.00004031
Iteration 175/1000 | Loss: 0.00003254
Iteration 176/1000 | Loss: 0.00003199
Iteration 177/1000 | Loss: 0.00003221
Iteration 178/1000 | Loss: 0.00002826
Iteration 179/1000 | Loss: 0.00002221
Iteration 180/1000 | Loss: 0.00002124
Iteration 181/1000 | Loss: 0.00002031
Iteration 182/1000 | Loss: 0.00001991
Iteration 183/1000 | Loss: 0.00001978
Iteration 184/1000 | Loss: 0.00001977
Iteration 185/1000 | Loss: 0.00001976
Iteration 186/1000 | Loss: 0.00001961
Iteration 187/1000 | Loss: 0.00001956
Iteration 188/1000 | Loss: 0.00001954
Iteration 189/1000 | Loss: 0.00001953
Iteration 190/1000 | Loss: 0.00001950
Iteration 191/1000 | Loss: 0.00001949
Iteration 192/1000 | Loss: 0.00001947
Iteration 193/1000 | Loss: 0.00001946
Iteration 194/1000 | Loss: 0.00001932
Iteration 195/1000 | Loss: 0.00001929
Iteration 196/1000 | Loss: 0.00001926
Iteration 197/1000 | Loss: 0.00001926
Iteration 198/1000 | Loss: 0.00001925
Iteration 199/1000 | Loss: 0.00001925
Iteration 200/1000 | Loss: 0.00001925
Iteration 201/1000 | Loss: 0.00001924
Iteration 202/1000 | Loss: 0.00001924
Iteration 203/1000 | Loss: 0.00001924
Iteration 204/1000 | Loss: 0.00001924
Iteration 205/1000 | Loss: 0.00001924
Iteration 206/1000 | Loss: 0.00001924
Iteration 207/1000 | Loss: 0.00001924
Iteration 208/1000 | Loss: 0.00001924
Iteration 209/1000 | Loss: 0.00001923
Iteration 210/1000 | Loss: 0.00001923
Iteration 211/1000 | Loss: 0.00001923
Iteration 212/1000 | Loss: 0.00001923
Iteration 213/1000 | Loss: 0.00001923
Iteration 214/1000 | Loss: 0.00001922
Iteration 215/1000 | Loss: 0.00001922
Iteration 216/1000 | Loss: 0.00001922
Iteration 217/1000 | Loss: 0.00001922
Iteration 218/1000 | Loss: 0.00001922
Iteration 219/1000 | Loss: 0.00001921
Iteration 220/1000 | Loss: 0.00022000
Iteration 221/1000 | Loss: 0.00003835
Iteration 222/1000 | Loss: 0.00002464
Iteration 223/1000 | Loss: 0.00002151
Iteration 224/1000 | Loss: 0.00002008
Iteration 225/1000 | Loss: 0.00001943
Iteration 226/1000 | Loss: 0.00001925
Iteration 227/1000 | Loss: 0.00001922
Iteration 228/1000 | Loss: 0.00001922
Iteration 229/1000 | Loss: 0.00001921
Iteration 230/1000 | Loss: 0.00001921
Iteration 231/1000 | Loss: 0.00001921
Iteration 232/1000 | Loss: 0.00001921
Iteration 233/1000 | Loss: 0.00001921
Iteration 234/1000 | Loss: 0.00001921
Iteration 235/1000 | Loss: 0.00001921
Iteration 236/1000 | Loss: 0.00001921
Iteration 237/1000 | Loss: 0.00001921
Iteration 238/1000 | Loss: 0.00001920
Iteration 239/1000 | Loss: 0.00001920
Iteration 240/1000 | Loss: 0.00001920
Iteration 241/1000 | Loss: 0.00001920
Iteration 242/1000 | Loss: 0.00001920
Iteration 243/1000 | Loss: 0.00001920
Iteration 244/1000 | Loss: 0.00001920
Iteration 245/1000 | Loss: 0.00001920
Iteration 246/1000 | Loss: 0.00001920
Iteration 247/1000 | Loss: 0.00001920
Iteration 248/1000 | Loss: 0.00001920
Iteration 249/1000 | Loss: 0.00022418
Iteration 250/1000 | Loss: 0.00003832
Iteration 251/1000 | Loss: 0.00001929
Iteration 252/1000 | Loss: 0.00001922
Iteration 253/1000 | Loss: 0.00001922
Iteration 254/1000 | Loss: 0.00001922
Iteration 255/1000 | Loss: 0.00001922
Iteration 256/1000 | Loss: 0.00001922
Iteration 257/1000 | Loss: 0.00001921
Iteration 258/1000 | Loss: 0.00001921
Iteration 259/1000 | Loss: 0.00001921
Iteration 260/1000 | Loss: 0.00001921
Iteration 261/1000 | Loss: 0.00001921
Iteration 262/1000 | Loss: 0.00001921
Iteration 263/1000 | Loss: 0.00001921
Iteration 264/1000 | Loss: 0.00001921
Iteration 265/1000 | Loss: 0.00001921
Iteration 266/1000 | Loss: 0.00001921
Iteration 267/1000 | Loss: 0.00001920
Iteration 268/1000 | Loss: 0.00001920
Iteration 269/1000 | Loss: 0.00001919
Iteration 270/1000 | Loss: 0.00001919
Iteration 271/1000 | Loss: 0.00001918
Iteration 272/1000 | Loss: 0.00001918
Iteration 273/1000 | Loss: 0.00001918
Iteration 274/1000 | Loss: 0.00001918
Iteration 275/1000 | Loss: 0.00001918
Iteration 276/1000 | Loss: 0.00001918
Iteration 277/1000 | Loss: 0.00001918
Iteration 278/1000 | Loss: 0.00001918
Iteration 279/1000 | Loss: 0.00001917
Iteration 280/1000 | Loss: 0.00001917
Iteration 281/1000 | Loss: 0.00001917
Iteration 282/1000 | Loss: 0.00001917
Iteration 283/1000 | Loss: 0.00001917
Iteration 284/1000 | Loss: 0.00001917
Iteration 285/1000 | Loss: 0.00001917
Iteration 286/1000 | Loss: 0.00001917
Iteration 287/1000 | Loss: 0.00001917
Iteration 288/1000 | Loss: 0.00001917
Iteration 289/1000 | Loss: 0.00001917
Iteration 290/1000 | Loss: 0.00001917
Iteration 291/1000 | Loss: 0.00001916
Iteration 292/1000 | Loss: 0.00001916
Iteration 293/1000 | Loss: 0.00001916
Iteration 294/1000 | Loss: 0.00001916
Iteration 295/1000 | Loss: 0.00001916
Iteration 296/1000 | Loss: 0.00001916
Iteration 297/1000 | Loss: 0.00001916
Iteration 298/1000 | Loss: 0.00001916
Iteration 299/1000 | Loss: 0.00001916
Iteration 300/1000 | Loss: 0.00001916
Iteration 301/1000 | Loss: 0.00001916
Iteration 302/1000 | Loss: 0.00001915
Iteration 303/1000 | Loss: 0.00001915
Iteration 304/1000 | Loss: 0.00001915
Iteration 305/1000 | Loss: 0.00001915
Iteration 306/1000 | Loss: 0.00001915
Iteration 307/1000 | Loss: 0.00001915
Iteration 308/1000 | Loss: 0.00001915
Iteration 309/1000 | Loss: 0.00001915
Iteration 310/1000 | Loss: 0.00001915
Iteration 311/1000 | Loss: 0.00001915
Iteration 312/1000 | Loss: 0.00001915
Iteration 313/1000 | Loss: 0.00001915
Iteration 314/1000 | Loss: 0.00001915
Iteration 315/1000 | Loss: 0.00001915
Iteration 316/1000 | Loss: 0.00001915
Iteration 317/1000 | Loss: 0.00001915
Iteration 318/1000 | Loss: 0.00001915
Iteration 319/1000 | Loss: 0.00001915
Iteration 320/1000 | Loss: 0.00001915
Iteration 321/1000 | Loss: 0.00001915
Iteration 322/1000 | Loss: 0.00001915
Iteration 323/1000 | Loss: 0.00001915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 323. Stopping optimization.
Last 5 losses: [1.9149498257320374e-05, 1.9149498257320374e-05, 1.9149498257320374e-05, 1.9149498257320374e-05, 1.9149498257320374e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9149498257320374e-05

Optimization complete. Final v2v error: 3.2535176277160645 mm

Highest mean error: 12.436196327209473 mm for frame 53

Lowest mean error: 2.6297614574432373 mm for frame 142

Saving results

Total time: 320.78816509246826
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00837027
Iteration 2/25 | Loss: 0.00183357
Iteration 3/25 | Loss: 0.00144065
Iteration 4/25 | Loss: 0.00139442
Iteration 5/25 | Loss: 0.00138949
Iteration 6/25 | Loss: 0.00139257
Iteration 7/25 | Loss: 0.00138576
Iteration 8/25 | Loss: 0.00137794
Iteration 9/25 | Loss: 0.00137170
Iteration 10/25 | Loss: 0.00136853
Iteration 11/25 | Loss: 0.00136651
Iteration 12/25 | Loss: 0.00136612
Iteration 13/25 | Loss: 0.00136601
Iteration 14/25 | Loss: 0.00136601
Iteration 15/25 | Loss: 0.00136600
Iteration 16/25 | Loss: 0.00136600
Iteration 17/25 | Loss: 0.00136600
Iteration 18/25 | Loss: 0.00136600
Iteration 19/25 | Loss: 0.00136600
Iteration 20/25 | Loss: 0.00136600
Iteration 21/25 | Loss: 0.00136600
Iteration 22/25 | Loss: 0.00136600
Iteration 23/25 | Loss: 0.00136600
Iteration 24/25 | Loss: 0.00136600
Iteration 25/25 | Loss: 0.00136600
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013659975957125425, 0.0013659975957125425, 0.0013659975957125425, 0.0013659975957125425, 0.0013659975957125425]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013659975957125425

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.24608612
Iteration 2/25 | Loss: 0.00064246
Iteration 3/25 | Loss: 0.00064246
Iteration 4/25 | Loss: 0.00064246
Iteration 5/25 | Loss: 0.00064246
Iteration 6/25 | Loss: 0.00064246
Iteration 7/25 | Loss: 0.00064246
Iteration 8/25 | Loss: 0.00064246
Iteration 9/25 | Loss: 0.00064246
Iteration 10/25 | Loss: 0.00064246
Iteration 11/25 | Loss: 0.00064246
Iteration 12/25 | Loss: 0.00064246
Iteration 13/25 | Loss: 0.00064246
Iteration 14/25 | Loss: 0.00064246
Iteration 15/25 | Loss: 0.00064246
Iteration 16/25 | Loss: 0.00064246
Iteration 17/25 | Loss: 0.00064246
Iteration 18/25 | Loss: 0.00064246
Iteration 19/25 | Loss: 0.00064246
Iteration 20/25 | Loss: 0.00064246
Iteration 21/25 | Loss: 0.00064246
Iteration 22/25 | Loss: 0.00064246
Iteration 23/25 | Loss: 0.00064246
Iteration 24/25 | Loss: 0.00064246
Iteration 25/25 | Loss: 0.00064246

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064246
Iteration 2/1000 | Loss: 0.00005071
Iteration 3/1000 | Loss: 0.00003673
Iteration 4/1000 | Loss: 0.00003414
Iteration 5/1000 | Loss: 0.00003305
Iteration 6/1000 | Loss: 0.00003212
Iteration 7/1000 | Loss: 0.00003152
Iteration 8/1000 | Loss: 0.00003105
Iteration 9/1000 | Loss: 0.00003076
Iteration 10/1000 | Loss: 0.00003056
Iteration 11/1000 | Loss: 0.00003052
Iteration 12/1000 | Loss: 0.00003037
Iteration 13/1000 | Loss: 0.00003036
Iteration 14/1000 | Loss: 0.00003036
Iteration 15/1000 | Loss: 0.00003036
Iteration 16/1000 | Loss: 0.00003035
Iteration 17/1000 | Loss: 0.00003034
Iteration 18/1000 | Loss: 0.00003032
Iteration 19/1000 | Loss: 0.00003032
Iteration 20/1000 | Loss: 0.00003032
Iteration 21/1000 | Loss: 0.00003031
Iteration 22/1000 | Loss: 0.00003031
Iteration 23/1000 | Loss: 0.00003031
Iteration 24/1000 | Loss: 0.00003031
Iteration 25/1000 | Loss: 0.00003031
Iteration 26/1000 | Loss: 0.00003031
Iteration 27/1000 | Loss: 0.00003031
Iteration 28/1000 | Loss: 0.00003030
Iteration 29/1000 | Loss: 0.00003030
Iteration 30/1000 | Loss: 0.00003030
Iteration 31/1000 | Loss: 0.00003030
Iteration 32/1000 | Loss: 0.00003030
Iteration 33/1000 | Loss: 0.00003030
Iteration 34/1000 | Loss: 0.00003030
Iteration 35/1000 | Loss: 0.00003030
Iteration 36/1000 | Loss: 0.00003029
Iteration 37/1000 | Loss: 0.00003029
Iteration 38/1000 | Loss: 0.00003029
Iteration 39/1000 | Loss: 0.00003029
Iteration 40/1000 | Loss: 0.00003029
Iteration 41/1000 | Loss: 0.00003029
Iteration 42/1000 | Loss: 0.00003029
Iteration 43/1000 | Loss: 0.00003029
Iteration 44/1000 | Loss: 0.00003029
Iteration 45/1000 | Loss: 0.00003029
Iteration 46/1000 | Loss: 0.00003029
Iteration 47/1000 | Loss: 0.00003028
Iteration 48/1000 | Loss: 0.00003028
Iteration 49/1000 | Loss: 0.00003028
Iteration 50/1000 | Loss: 0.00003028
Iteration 51/1000 | Loss: 0.00003028
Iteration 52/1000 | Loss: 0.00003028
Iteration 53/1000 | Loss: 0.00003028
Iteration 54/1000 | Loss: 0.00003028
Iteration 55/1000 | Loss: 0.00003028
Iteration 56/1000 | Loss: 0.00003028
Iteration 57/1000 | Loss: 0.00003027
Iteration 58/1000 | Loss: 0.00003027
Iteration 59/1000 | Loss: 0.00003027
Iteration 60/1000 | Loss: 0.00003027
Iteration 61/1000 | Loss: 0.00003027
Iteration 62/1000 | Loss: 0.00003027
Iteration 63/1000 | Loss: 0.00003027
Iteration 64/1000 | Loss: 0.00003027
Iteration 65/1000 | Loss: 0.00003027
Iteration 66/1000 | Loss: 0.00003027
Iteration 67/1000 | Loss: 0.00003027
Iteration 68/1000 | Loss: 0.00003027
Iteration 69/1000 | Loss: 0.00003027
Iteration 70/1000 | Loss: 0.00003026
Iteration 71/1000 | Loss: 0.00003026
Iteration 72/1000 | Loss: 0.00003026
Iteration 73/1000 | Loss: 0.00003026
Iteration 74/1000 | Loss: 0.00003026
Iteration 75/1000 | Loss: 0.00003026
Iteration 76/1000 | Loss: 0.00003026
Iteration 77/1000 | Loss: 0.00003026
Iteration 78/1000 | Loss: 0.00003026
Iteration 79/1000 | Loss: 0.00003026
Iteration 80/1000 | Loss: 0.00003025
Iteration 81/1000 | Loss: 0.00003025
Iteration 82/1000 | Loss: 0.00003025
Iteration 83/1000 | Loss: 0.00003025
Iteration 84/1000 | Loss: 0.00003025
Iteration 85/1000 | Loss: 0.00003025
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 85. Stopping optimization.
Last 5 losses: [3.025483420060482e-05, 3.025483420060482e-05, 3.025483420060482e-05, 3.025483420060482e-05, 3.025483420060482e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.025483420060482e-05

Optimization complete. Final v2v error: 4.6067728996276855 mm

Highest mean error: 5.16096305847168 mm for frame 134

Lowest mean error: 4.103449821472168 mm for frame 1

Saving results

Total time: 48.658995389938354
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00373379
Iteration 2/25 | Loss: 0.00127369
Iteration 3/25 | Loss: 0.00119028
Iteration 4/25 | Loss: 0.00117986
Iteration 5/25 | Loss: 0.00117683
Iteration 6/25 | Loss: 0.00117625
Iteration 7/25 | Loss: 0.00117625
Iteration 8/25 | Loss: 0.00117625
Iteration 9/25 | Loss: 0.00117625
Iteration 10/25 | Loss: 0.00117625
Iteration 11/25 | Loss: 0.00117625
Iteration 12/25 | Loss: 0.00117625
Iteration 13/25 | Loss: 0.00117625
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011762462090700865, 0.0011762462090700865, 0.0011762462090700865, 0.0011762462090700865, 0.0011762462090700865]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011762462090700865

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41245902
Iteration 2/25 | Loss: 0.00123122
Iteration 3/25 | Loss: 0.00123122
Iteration 4/25 | Loss: 0.00123122
Iteration 5/25 | Loss: 0.00123122
Iteration 6/25 | Loss: 0.00123122
Iteration 7/25 | Loss: 0.00123122
Iteration 8/25 | Loss: 0.00123122
Iteration 9/25 | Loss: 0.00123122
Iteration 10/25 | Loss: 0.00123122
Iteration 11/25 | Loss: 0.00123122
Iteration 12/25 | Loss: 0.00123122
Iteration 13/25 | Loss: 0.00123122
Iteration 14/25 | Loss: 0.00123122
Iteration 15/25 | Loss: 0.00123122
Iteration 16/25 | Loss: 0.00123122
Iteration 17/25 | Loss: 0.00123122
Iteration 18/25 | Loss: 0.00123122
Iteration 19/25 | Loss: 0.00123121
Iteration 20/25 | Loss: 0.00123121
Iteration 21/25 | Loss: 0.00123121
Iteration 22/25 | Loss: 0.00123121
Iteration 23/25 | Loss: 0.00123121
Iteration 24/25 | Loss: 0.00123121
Iteration 25/25 | Loss: 0.00123121

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123121
Iteration 2/1000 | Loss: 0.00002486
Iteration 3/1000 | Loss: 0.00001538
Iteration 4/1000 | Loss: 0.00001262
Iteration 5/1000 | Loss: 0.00001139
Iteration 6/1000 | Loss: 0.00001074
Iteration 7/1000 | Loss: 0.00001027
Iteration 8/1000 | Loss: 0.00000997
Iteration 9/1000 | Loss: 0.00000985
Iteration 10/1000 | Loss: 0.00000959
Iteration 11/1000 | Loss: 0.00000957
Iteration 12/1000 | Loss: 0.00000956
Iteration 13/1000 | Loss: 0.00000955
Iteration 14/1000 | Loss: 0.00000950
Iteration 15/1000 | Loss: 0.00000949
Iteration 16/1000 | Loss: 0.00000948
Iteration 17/1000 | Loss: 0.00000947
Iteration 18/1000 | Loss: 0.00000947
Iteration 19/1000 | Loss: 0.00000946
Iteration 20/1000 | Loss: 0.00000946
Iteration 21/1000 | Loss: 0.00000945
Iteration 22/1000 | Loss: 0.00000942
Iteration 23/1000 | Loss: 0.00000941
Iteration 24/1000 | Loss: 0.00000941
Iteration 25/1000 | Loss: 0.00000940
Iteration 26/1000 | Loss: 0.00000940
Iteration 27/1000 | Loss: 0.00000938
Iteration 28/1000 | Loss: 0.00000934
Iteration 29/1000 | Loss: 0.00000934
Iteration 30/1000 | Loss: 0.00000933
Iteration 31/1000 | Loss: 0.00000933
Iteration 32/1000 | Loss: 0.00000932
Iteration 33/1000 | Loss: 0.00000931
Iteration 34/1000 | Loss: 0.00000931
Iteration 35/1000 | Loss: 0.00000930
Iteration 36/1000 | Loss: 0.00000929
Iteration 37/1000 | Loss: 0.00000928
Iteration 38/1000 | Loss: 0.00000927
Iteration 39/1000 | Loss: 0.00000927
Iteration 40/1000 | Loss: 0.00000927
Iteration 41/1000 | Loss: 0.00000927
Iteration 42/1000 | Loss: 0.00000927
Iteration 43/1000 | Loss: 0.00000927
Iteration 44/1000 | Loss: 0.00000927
Iteration 45/1000 | Loss: 0.00000927
Iteration 46/1000 | Loss: 0.00000927
Iteration 47/1000 | Loss: 0.00000927
Iteration 48/1000 | Loss: 0.00000926
Iteration 49/1000 | Loss: 0.00000926
Iteration 50/1000 | Loss: 0.00000926
Iteration 51/1000 | Loss: 0.00000922
Iteration 52/1000 | Loss: 0.00000922
Iteration 53/1000 | Loss: 0.00000922
Iteration 54/1000 | Loss: 0.00000922
Iteration 55/1000 | Loss: 0.00000922
Iteration 56/1000 | Loss: 0.00000922
Iteration 57/1000 | Loss: 0.00000922
Iteration 58/1000 | Loss: 0.00000922
Iteration 59/1000 | Loss: 0.00000922
Iteration 60/1000 | Loss: 0.00000921
Iteration 61/1000 | Loss: 0.00000921
Iteration 62/1000 | Loss: 0.00000921
Iteration 63/1000 | Loss: 0.00000921
Iteration 64/1000 | Loss: 0.00000919
Iteration 65/1000 | Loss: 0.00000919
Iteration 66/1000 | Loss: 0.00000918
Iteration 67/1000 | Loss: 0.00000918
Iteration 68/1000 | Loss: 0.00000917
Iteration 69/1000 | Loss: 0.00000917
Iteration 70/1000 | Loss: 0.00000916
Iteration 71/1000 | Loss: 0.00000916
Iteration 72/1000 | Loss: 0.00000915
Iteration 73/1000 | Loss: 0.00000914
Iteration 74/1000 | Loss: 0.00000913
Iteration 75/1000 | Loss: 0.00000913
Iteration 76/1000 | Loss: 0.00000913
Iteration 77/1000 | Loss: 0.00000912
Iteration 78/1000 | Loss: 0.00000912
Iteration 79/1000 | Loss: 0.00000911
Iteration 80/1000 | Loss: 0.00000911
Iteration 81/1000 | Loss: 0.00000911
Iteration 82/1000 | Loss: 0.00000911
Iteration 83/1000 | Loss: 0.00000910
Iteration 84/1000 | Loss: 0.00000909
Iteration 85/1000 | Loss: 0.00000908
Iteration 86/1000 | Loss: 0.00000908
Iteration 87/1000 | Loss: 0.00000908
Iteration 88/1000 | Loss: 0.00000908
Iteration 89/1000 | Loss: 0.00000907
Iteration 90/1000 | Loss: 0.00000907
Iteration 91/1000 | Loss: 0.00000907
Iteration 92/1000 | Loss: 0.00000906
Iteration 93/1000 | Loss: 0.00000905
Iteration 94/1000 | Loss: 0.00000905
Iteration 95/1000 | Loss: 0.00000904
Iteration 96/1000 | Loss: 0.00000903
Iteration 97/1000 | Loss: 0.00000903
Iteration 98/1000 | Loss: 0.00000903
Iteration 99/1000 | Loss: 0.00000903
Iteration 100/1000 | Loss: 0.00000902
Iteration 101/1000 | Loss: 0.00000902
Iteration 102/1000 | Loss: 0.00000902
Iteration 103/1000 | Loss: 0.00000902
Iteration 104/1000 | Loss: 0.00000902
Iteration 105/1000 | Loss: 0.00000902
Iteration 106/1000 | Loss: 0.00000902
Iteration 107/1000 | Loss: 0.00000902
Iteration 108/1000 | Loss: 0.00000901
Iteration 109/1000 | Loss: 0.00000899
Iteration 110/1000 | Loss: 0.00000899
Iteration 111/1000 | Loss: 0.00000899
Iteration 112/1000 | Loss: 0.00000899
Iteration 113/1000 | Loss: 0.00000899
Iteration 114/1000 | Loss: 0.00000899
Iteration 115/1000 | Loss: 0.00000899
Iteration 116/1000 | Loss: 0.00000899
Iteration 117/1000 | Loss: 0.00000899
Iteration 118/1000 | Loss: 0.00000899
Iteration 119/1000 | Loss: 0.00000899
Iteration 120/1000 | Loss: 0.00000899
Iteration 121/1000 | Loss: 0.00000898
Iteration 122/1000 | Loss: 0.00000898
Iteration 123/1000 | Loss: 0.00000898
Iteration 124/1000 | Loss: 0.00000897
Iteration 125/1000 | Loss: 0.00000897
Iteration 126/1000 | Loss: 0.00000897
Iteration 127/1000 | Loss: 0.00000896
Iteration 128/1000 | Loss: 0.00000896
Iteration 129/1000 | Loss: 0.00000896
Iteration 130/1000 | Loss: 0.00000895
Iteration 131/1000 | Loss: 0.00000895
Iteration 132/1000 | Loss: 0.00000895
Iteration 133/1000 | Loss: 0.00000895
Iteration 134/1000 | Loss: 0.00000895
Iteration 135/1000 | Loss: 0.00000895
Iteration 136/1000 | Loss: 0.00000895
Iteration 137/1000 | Loss: 0.00000895
Iteration 138/1000 | Loss: 0.00000895
Iteration 139/1000 | Loss: 0.00000894
Iteration 140/1000 | Loss: 0.00000894
Iteration 141/1000 | Loss: 0.00000894
Iteration 142/1000 | Loss: 0.00000894
Iteration 143/1000 | Loss: 0.00000894
Iteration 144/1000 | Loss: 0.00000893
Iteration 145/1000 | Loss: 0.00000893
Iteration 146/1000 | Loss: 0.00000893
Iteration 147/1000 | Loss: 0.00000893
Iteration 148/1000 | Loss: 0.00000893
Iteration 149/1000 | Loss: 0.00000893
Iteration 150/1000 | Loss: 0.00000893
Iteration 151/1000 | Loss: 0.00000893
Iteration 152/1000 | Loss: 0.00000893
Iteration 153/1000 | Loss: 0.00000892
Iteration 154/1000 | Loss: 0.00000892
Iteration 155/1000 | Loss: 0.00000892
Iteration 156/1000 | Loss: 0.00000892
Iteration 157/1000 | Loss: 0.00000892
Iteration 158/1000 | Loss: 0.00000892
Iteration 159/1000 | Loss: 0.00000891
Iteration 160/1000 | Loss: 0.00000891
Iteration 161/1000 | Loss: 0.00000891
Iteration 162/1000 | Loss: 0.00000890
Iteration 163/1000 | Loss: 0.00000890
Iteration 164/1000 | Loss: 0.00000890
Iteration 165/1000 | Loss: 0.00000890
Iteration 166/1000 | Loss: 0.00000889
Iteration 167/1000 | Loss: 0.00000889
Iteration 168/1000 | Loss: 0.00000889
Iteration 169/1000 | Loss: 0.00000889
Iteration 170/1000 | Loss: 0.00000889
Iteration 171/1000 | Loss: 0.00000889
Iteration 172/1000 | Loss: 0.00000888
Iteration 173/1000 | Loss: 0.00000888
Iteration 174/1000 | Loss: 0.00000888
Iteration 175/1000 | Loss: 0.00000888
Iteration 176/1000 | Loss: 0.00000888
Iteration 177/1000 | Loss: 0.00000888
Iteration 178/1000 | Loss: 0.00000888
Iteration 179/1000 | Loss: 0.00000888
Iteration 180/1000 | Loss: 0.00000888
Iteration 181/1000 | Loss: 0.00000887
Iteration 182/1000 | Loss: 0.00000887
Iteration 183/1000 | Loss: 0.00000887
Iteration 184/1000 | Loss: 0.00000887
Iteration 185/1000 | Loss: 0.00000887
Iteration 186/1000 | Loss: 0.00000887
Iteration 187/1000 | Loss: 0.00000887
Iteration 188/1000 | Loss: 0.00000887
Iteration 189/1000 | Loss: 0.00000887
Iteration 190/1000 | Loss: 0.00000886
Iteration 191/1000 | Loss: 0.00000886
Iteration 192/1000 | Loss: 0.00000886
Iteration 193/1000 | Loss: 0.00000886
Iteration 194/1000 | Loss: 0.00000886
Iteration 195/1000 | Loss: 0.00000886
Iteration 196/1000 | Loss: 0.00000885
Iteration 197/1000 | Loss: 0.00000885
Iteration 198/1000 | Loss: 0.00000885
Iteration 199/1000 | Loss: 0.00000885
Iteration 200/1000 | Loss: 0.00000884
Iteration 201/1000 | Loss: 0.00000884
Iteration 202/1000 | Loss: 0.00000884
Iteration 203/1000 | Loss: 0.00000884
Iteration 204/1000 | Loss: 0.00000884
Iteration 205/1000 | Loss: 0.00000884
Iteration 206/1000 | Loss: 0.00000884
Iteration 207/1000 | Loss: 0.00000884
Iteration 208/1000 | Loss: 0.00000884
Iteration 209/1000 | Loss: 0.00000884
Iteration 210/1000 | Loss: 0.00000884
Iteration 211/1000 | Loss: 0.00000884
Iteration 212/1000 | Loss: 0.00000884
Iteration 213/1000 | Loss: 0.00000883
Iteration 214/1000 | Loss: 0.00000883
Iteration 215/1000 | Loss: 0.00000883
Iteration 216/1000 | Loss: 0.00000883
Iteration 217/1000 | Loss: 0.00000883
Iteration 218/1000 | Loss: 0.00000883
Iteration 219/1000 | Loss: 0.00000883
Iteration 220/1000 | Loss: 0.00000883
Iteration 221/1000 | Loss: 0.00000883
Iteration 222/1000 | Loss: 0.00000883
Iteration 223/1000 | Loss: 0.00000883
Iteration 224/1000 | Loss: 0.00000883
Iteration 225/1000 | Loss: 0.00000883
Iteration 226/1000 | Loss: 0.00000883
Iteration 227/1000 | Loss: 0.00000882
Iteration 228/1000 | Loss: 0.00000882
Iteration 229/1000 | Loss: 0.00000882
Iteration 230/1000 | Loss: 0.00000882
Iteration 231/1000 | Loss: 0.00000882
Iteration 232/1000 | Loss: 0.00000882
Iteration 233/1000 | Loss: 0.00000882
Iteration 234/1000 | Loss: 0.00000882
Iteration 235/1000 | Loss: 0.00000882
Iteration 236/1000 | Loss: 0.00000882
Iteration 237/1000 | Loss: 0.00000882
Iteration 238/1000 | Loss: 0.00000882
Iteration 239/1000 | Loss: 0.00000882
Iteration 240/1000 | Loss: 0.00000882
Iteration 241/1000 | Loss: 0.00000882
Iteration 242/1000 | Loss: 0.00000882
Iteration 243/1000 | Loss: 0.00000882
Iteration 244/1000 | Loss: 0.00000882
Iteration 245/1000 | Loss: 0.00000882
Iteration 246/1000 | Loss: 0.00000882
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 246. Stopping optimization.
Last 5 losses: [8.822521522233728e-06, 8.822521522233728e-06, 8.822521522233728e-06, 8.822521522233728e-06, 8.822521522233728e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 8.822521522233728e-06

Optimization complete. Final v2v error: 2.553945302963257 mm

Highest mean error: 3.0611181259155273 mm for frame 98

Lowest mean error: 2.4358365535736084 mm for frame 39

Saving results

Total time: 41.447532176971436
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1057/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1057.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1057
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01038671
Iteration 2/25 | Loss: 0.00322308
Iteration 3/25 | Loss: 0.00223058
Iteration 4/25 | Loss: 0.00223022
Iteration 5/25 | Loss: 0.00244220
Iteration 6/25 | Loss: 0.00237589
Iteration 7/25 | Loss: 0.00178164
Iteration 8/25 | Loss: 0.00151410
Iteration 9/25 | Loss: 0.00148140
Iteration 10/25 | Loss: 0.00147323
Iteration 11/25 | Loss: 0.00146363
Iteration 12/25 | Loss: 0.00143907
Iteration 13/25 | Loss: 0.00142187
Iteration 14/25 | Loss: 0.00142067
Iteration 15/25 | Loss: 0.00141499
Iteration 16/25 | Loss: 0.00140599
Iteration 17/25 | Loss: 0.00140368
Iteration 18/25 | Loss: 0.00140199
Iteration 19/25 | Loss: 0.00140077
Iteration 20/25 | Loss: 0.00139987
Iteration 21/25 | Loss: 0.00140058
Iteration 22/25 | Loss: 0.00139910
Iteration 23/25 | Loss: 0.00140195
Iteration 24/25 | Loss: 0.00140060
Iteration 25/25 | Loss: 0.00140081

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.62708396
Iteration 2/25 | Loss: 0.00101746
Iteration 3/25 | Loss: 0.00101746
Iteration 4/25 | Loss: 0.00101746
Iteration 5/25 | Loss: 0.00101746
Iteration 6/25 | Loss: 0.00101746
Iteration 7/25 | Loss: 0.00101746
Iteration 8/25 | Loss: 0.00101746
Iteration 9/25 | Loss: 0.00101746
Iteration 10/25 | Loss: 0.00101746
Iteration 11/25 | Loss: 0.00101746
Iteration 12/25 | Loss: 0.00101746
Iteration 13/25 | Loss: 0.00101746
Iteration 14/25 | Loss: 0.00101746
Iteration 15/25 | Loss: 0.00101745
Iteration 16/25 | Loss: 0.00101745
Iteration 17/25 | Loss: 0.00101745
Iteration 18/25 | Loss: 0.00101745
Iteration 19/25 | Loss: 0.00101745
Iteration 20/25 | Loss: 0.00101745
Iteration 21/25 | Loss: 0.00101745
Iteration 22/25 | Loss: 0.00101745
Iteration 23/25 | Loss: 0.00101745
Iteration 24/25 | Loss: 0.00101745
Iteration 25/25 | Loss: 0.00101745

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101745
Iteration 2/1000 | Loss: 0.00018001
Iteration 3/1000 | Loss: 0.00016566
Iteration 4/1000 | Loss: 0.00004919
Iteration 5/1000 | Loss: 0.00006021
Iteration 6/1000 | Loss: 0.00003833
Iteration 7/1000 | Loss: 0.00003615
Iteration 8/1000 | Loss: 0.00005337
Iteration 9/1000 | Loss: 0.00006566
Iteration 10/1000 | Loss: 0.00025880
Iteration 11/1000 | Loss: 0.00034725
Iteration 12/1000 | Loss: 0.00030915
Iteration 13/1000 | Loss: 0.00004576
Iteration 14/1000 | Loss: 0.00004937
Iteration 15/1000 | Loss: 0.00009388
Iteration 16/1000 | Loss: 0.00021354
Iteration 17/1000 | Loss: 0.00007095
Iteration 18/1000 | Loss: 0.00022114
Iteration 19/1000 | Loss: 0.00026524
Iteration 20/1000 | Loss: 0.00003475
Iteration 21/1000 | Loss: 0.00003220
Iteration 22/1000 | Loss: 0.00003208
Iteration 23/1000 | Loss: 0.00002953
Iteration 24/1000 | Loss: 0.00002892
Iteration 25/1000 | Loss: 0.00003331
Iteration 26/1000 | Loss: 0.00021961
Iteration 27/1000 | Loss: 0.00003353
Iteration 28/1000 | Loss: 0.00011102
Iteration 29/1000 | Loss: 0.00003017
Iteration 30/1000 | Loss: 0.00003494
Iteration 31/1000 | Loss: 0.00023341
Iteration 32/1000 | Loss: 0.00007196
Iteration 33/1000 | Loss: 0.00003461
Iteration 34/1000 | Loss: 0.00022120
Iteration 35/1000 | Loss: 0.00014376
Iteration 36/1000 | Loss: 0.00003190
Iteration 37/1000 | Loss: 0.00019194
Iteration 38/1000 | Loss: 0.00010551
Iteration 39/1000 | Loss: 0.00003285
Iteration 40/1000 | Loss: 0.00015672
Iteration 41/1000 | Loss: 0.00012450
Iteration 42/1000 | Loss: 0.00013751
Iteration 43/1000 | Loss: 0.00003088
Iteration 44/1000 | Loss: 0.00002841
Iteration 45/1000 | Loss: 0.00002684
Iteration 46/1000 | Loss: 0.00002612
Iteration 47/1000 | Loss: 0.00002568
Iteration 48/1000 | Loss: 0.00019578
Iteration 49/1000 | Loss: 0.00023318
Iteration 50/1000 | Loss: 0.00008663
Iteration 51/1000 | Loss: 0.00011626
Iteration 52/1000 | Loss: 0.00026223
Iteration 53/1000 | Loss: 0.00018087
Iteration 54/1000 | Loss: 0.00020396
Iteration 55/1000 | Loss: 0.00003301
Iteration 56/1000 | Loss: 0.00002840
Iteration 57/1000 | Loss: 0.00002671
Iteration 58/1000 | Loss: 0.00016594
Iteration 59/1000 | Loss: 0.00024865
Iteration 60/1000 | Loss: 0.00003081
Iteration 61/1000 | Loss: 0.00002851
Iteration 62/1000 | Loss: 0.00027373
Iteration 63/1000 | Loss: 0.00116178
Iteration 64/1000 | Loss: 0.00089922
Iteration 65/1000 | Loss: 0.00004465
Iteration 66/1000 | Loss: 0.00005995
Iteration 67/1000 | Loss: 0.00023554
Iteration 68/1000 | Loss: 0.00005695
Iteration 69/1000 | Loss: 0.00004142
Iteration 70/1000 | Loss: 0.00002736
Iteration 71/1000 | Loss: 0.00002543
Iteration 72/1000 | Loss: 0.00002515
Iteration 73/1000 | Loss: 0.00002499
Iteration 74/1000 | Loss: 0.00002475
Iteration 75/1000 | Loss: 0.00002464
Iteration 76/1000 | Loss: 0.00002464
Iteration 77/1000 | Loss: 0.00002463
Iteration 78/1000 | Loss: 0.00002463
Iteration 79/1000 | Loss: 0.00002462
Iteration 80/1000 | Loss: 0.00002448
Iteration 81/1000 | Loss: 0.00002447
Iteration 82/1000 | Loss: 0.00002447
Iteration 83/1000 | Loss: 0.00002447
Iteration 84/1000 | Loss: 0.00002446
Iteration 85/1000 | Loss: 0.00002446
Iteration 86/1000 | Loss: 0.00002446
Iteration 87/1000 | Loss: 0.00002446
Iteration 88/1000 | Loss: 0.00002445
Iteration 89/1000 | Loss: 0.00002445
Iteration 90/1000 | Loss: 0.00002445
Iteration 91/1000 | Loss: 0.00002445
Iteration 92/1000 | Loss: 0.00002445
Iteration 93/1000 | Loss: 0.00002445
Iteration 94/1000 | Loss: 0.00002445
Iteration 95/1000 | Loss: 0.00002445
Iteration 96/1000 | Loss: 0.00002445
Iteration 97/1000 | Loss: 0.00002445
Iteration 98/1000 | Loss: 0.00002445
Iteration 99/1000 | Loss: 0.00002445
Iteration 100/1000 | Loss: 0.00002442
Iteration 101/1000 | Loss: 0.00002442
Iteration 102/1000 | Loss: 0.00002441
Iteration 103/1000 | Loss: 0.00002441
Iteration 104/1000 | Loss: 0.00002440
Iteration 105/1000 | Loss: 0.00002440
Iteration 106/1000 | Loss: 0.00002440
Iteration 107/1000 | Loss: 0.00002439
Iteration 108/1000 | Loss: 0.00002439
Iteration 109/1000 | Loss: 0.00002439
Iteration 110/1000 | Loss: 0.00002438
Iteration 111/1000 | Loss: 0.00002438
Iteration 112/1000 | Loss: 0.00002438
Iteration 113/1000 | Loss: 0.00002438
Iteration 114/1000 | Loss: 0.00002438
Iteration 115/1000 | Loss: 0.00002438
Iteration 116/1000 | Loss: 0.00002438
Iteration 117/1000 | Loss: 0.00002438
Iteration 118/1000 | Loss: 0.00002438
Iteration 119/1000 | Loss: 0.00002438
Iteration 120/1000 | Loss: 0.00002437
Iteration 121/1000 | Loss: 0.00002437
Iteration 122/1000 | Loss: 0.00002437
Iteration 123/1000 | Loss: 0.00002437
Iteration 124/1000 | Loss: 0.00002437
Iteration 125/1000 | Loss: 0.00002437
Iteration 126/1000 | Loss: 0.00002437
Iteration 127/1000 | Loss: 0.00002437
Iteration 128/1000 | Loss: 0.00002437
Iteration 129/1000 | Loss: 0.00002437
Iteration 130/1000 | Loss: 0.00002437
Iteration 131/1000 | Loss: 0.00002437
Iteration 132/1000 | Loss: 0.00002437
Iteration 133/1000 | Loss: 0.00002437
Iteration 134/1000 | Loss: 0.00002437
Iteration 135/1000 | Loss: 0.00002437
Iteration 136/1000 | Loss: 0.00002437
Iteration 137/1000 | Loss: 0.00002437
Iteration 138/1000 | Loss: 0.00002437
Iteration 139/1000 | Loss: 0.00002437
Iteration 140/1000 | Loss: 0.00002437
Iteration 141/1000 | Loss: 0.00002437
Iteration 142/1000 | Loss: 0.00002437
Iteration 143/1000 | Loss: 0.00002437
Iteration 144/1000 | Loss: 0.00002437
Iteration 145/1000 | Loss: 0.00002437
Iteration 146/1000 | Loss: 0.00002437
Iteration 147/1000 | Loss: 0.00002437
Iteration 148/1000 | Loss: 0.00002437
Iteration 149/1000 | Loss: 0.00002437
Iteration 150/1000 | Loss: 0.00002437
Iteration 151/1000 | Loss: 0.00002437
Iteration 152/1000 | Loss: 0.00002437
Iteration 153/1000 | Loss: 0.00002437
Iteration 154/1000 | Loss: 0.00002437
Iteration 155/1000 | Loss: 0.00002437
Iteration 156/1000 | Loss: 0.00002437
Iteration 157/1000 | Loss: 0.00002437
Iteration 158/1000 | Loss: 0.00002437
Iteration 159/1000 | Loss: 0.00002437
Iteration 160/1000 | Loss: 0.00002437
Iteration 161/1000 | Loss: 0.00002437
Iteration 162/1000 | Loss: 0.00002437
Iteration 163/1000 | Loss: 0.00002437
Iteration 164/1000 | Loss: 0.00002437
Iteration 165/1000 | Loss: 0.00002437
Iteration 166/1000 | Loss: 0.00002437
Iteration 167/1000 | Loss: 0.00002437
Iteration 168/1000 | Loss: 0.00002437
Iteration 169/1000 | Loss: 0.00002437
Iteration 170/1000 | Loss: 0.00002437
Iteration 171/1000 | Loss: 0.00002437
Iteration 172/1000 | Loss: 0.00002437
Iteration 173/1000 | Loss: 0.00002437
Iteration 174/1000 | Loss: 0.00002437
Iteration 175/1000 | Loss: 0.00002437
Iteration 176/1000 | Loss: 0.00002437
Iteration 177/1000 | Loss: 0.00002437
Iteration 178/1000 | Loss: 0.00002437
Iteration 179/1000 | Loss: 0.00002437
Iteration 180/1000 | Loss: 0.00002437
Iteration 181/1000 | Loss: 0.00002437
Iteration 182/1000 | Loss: 0.00002437
Iteration 183/1000 | Loss: 0.00002437
Iteration 184/1000 | Loss: 0.00002437
Iteration 185/1000 | Loss: 0.00002437
Iteration 186/1000 | Loss: 0.00002437
Iteration 187/1000 | Loss: 0.00002437
Iteration 188/1000 | Loss: 0.00002437
Iteration 189/1000 | Loss: 0.00002437
Iteration 190/1000 | Loss: 0.00002437
Iteration 191/1000 | Loss: 0.00002437
Iteration 192/1000 | Loss: 0.00002437
Iteration 193/1000 | Loss: 0.00002437
Iteration 194/1000 | Loss: 0.00002437
Iteration 195/1000 | Loss: 0.00002437
Iteration 196/1000 | Loss: 0.00002437
Iteration 197/1000 | Loss: 0.00002437
Iteration 198/1000 | Loss: 0.00002437
Iteration 199/1000 | Loss: 0.00002437
Iteration 200/1000 | Loss: 0.00002437
Iteration 201/1000 | Loss: 0.00002437
Iteration 202/1000 | Loss: 0.00002437
Iteration 203/1000 | Loss: 0.00002437
Iteration 204/1000 | Loss: 0.00002437
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 204. Stopping optimization.
Last 5 losses: [2.436786780890543e-05, 2.436786780890543e-05, 2.436786780890543e-05, 2.436786780890543e-05, 2.436786780890543e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.436786780890543e-05

Optimization complete. Final v2v error: 4.112804889678955 mm

Highest mean error: 6.14255952835083 mm for frame 0

Lowest mean error: 3.6314120292663574 mm for frame 38

Saving results

Total time: 158.2194492816925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00387319
Iteration 2/25 | Loss: 0.00125310
Iteration 3/25 | Loss: 0.00119672
Iteration 4/25 | Loss: 0.00119480
Iteration 5/25 | Loss: 0.00119480
Iteration 6/25 | Loss: 0.00119480
Iteration 7/25 | Loss: 0.00119480
Iteration 8/25 | Loss: 0.00119480
Iteration 9/25 | Loss: 0.00119480
Iteration 10/25 | Loss: 0.00119480
Iteration 11/25 | Loss: 0.00119480
Iteration 12/25 | Loss: 0.00119480
Iteration 13/25 | Loss: 0.00119480
Iteration 14/25 | Loss: 0.00119480
Iteration 15/25 | Loss: 0.00119480
Iteration 16/25 | Loss: 0.00119480
Iteration 17/25 | Loss: 0.00119480
Iteration 18/25 | Loss: 0.00119480
Iteration 19/25 | Loss: 0.00119480
Iteration 20/25 | Loss: 0.00119480
Iteration 21/25 | Loss: 0.00119480
Iteration 22/25 | Loss: 0.00119480
Iteration 23/25 | Loss: 0.00119480
Iteration 24/25 | Loss: 0.00119480
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011947969906032085, 0.0011947969906032085, 0.0011947969906032085, 0.0011947969906032085, 0.0011947969906032085]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011947969906032085

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61672688
Iteration 2/25 | Loss: 0.00103011
Iteration 3/25 | Loss: 0.00103008
Iteration 4/25 | Loss: 0.00103008
Iteration 5/25 | Loss: 0.00103008
Iteration 6/25 | Loss: 0.00103008
Iteration 7/25 | Loss: 0.00103008
Iteration 8/25 | Loss: 0.00103008
Iteration 9/25 | Loss: 0.00103008
Iteration 10/25 | Loss: 0.00103008
Iteration 11/25 | Loss: 0.00103008
Iteration 12/25 | Loss: 0.00103008
Iteration 13/25 | Loss: 0.00103007
Iteration 14/25 | Loss: 0.00103008
Iteration 15/25 | Loss: 0.00103007
Iteration 16/25 | Loss: 0.00103007
Iteration 17/25 | Loss: 0.00103008
Iteration 18/25 | Loss: 0.00103008
Iteration 19/25 | Loss: 0.00103008
Iteration 20/25 | Loss: 0.00103008
Iteration 21/25 | Loss: 0.00103008
Iteration 22/25 | Loss: 0.00103008
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010300750145688653, 0.0010300750145688653, 0.0010300750145688653, 0.0010300750145688653, 0.0010300750145688653]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010300750145688653

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00103008
Iteration 2/1000 | Loss: 0.00002733
Iteration 3/1000 | Loss: 0.00001759
Iteration 4/1000 | Loss: 0.00001448
Iteration 5/1000 | Loss: 0.00001319
Iteration 6/1000 | Loss: 0.00001238
Iteration 7/1000 | Loss: 0.00001192
Iteration 8/1000 | Loss: 0.00001163
Iteration 9/1000 | Loss: 0.00001130
Iteration 10/1000 | Loss: 0.00001085
Iteration 11/1000 | Loss: 0.00001064
Iteration 12/1000 | Loss: 0.00001055
Iteration 13/1000 | Loss: 0.00001054
Iteration 14/1000 | Loss: 0.00001051
Iteration 15/1000 | Loss: 0.00001044
Iteration 16/1000 | Loss: 0.00001044
Iteration 17/1000 | Loss: 0.00001036
Iteration 18/1000 | Loss: 0.00001032
Iteration 19/1000 | Loss: 0.00001028
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001022
Iteration 22/1000 | Loss: 0.00001022
Iteration 23/1000 | Loss: 0.00001022
Iteration 24/1000 | Loss: 0.00001022
Iteration 25/1000 | Loss: 0.00001021
Iteration 26/1000 | Loss: 0.00001021
Iteration 27/1000 | Loss: 0.00001020
Iteration 28/1000 | Loss: 0.00001016
Iteration 29/1000 | Loss: 0.00001013
Iteration 30/1000 | Loss: 0.00001010
Iteration 31/1000 | Loss: 0.00001010
Iteration 32/1000 | Loss: 0.00001007
Iteration 33/1000 | Loss: 0.00001006
Iteration 34/1000 | Loss: 0.00001004
Iteration 35/1000 | Loss: 0.00001003
Iteration 36/1000 | Loss: 0.00001003
Iteration 37/1000 | Loss: 0.00001001
Iteration 38/1000 | Loss: 0.00001001
Iteration 39/1000 | Loss: 0.00001000
Iteration 40/1000 | Loss: 0.00000999
Iteration 41/1000 | Loss: 0.00000994
Iteration 42/1000 | Loss: 0.00000991
Iteration 43/1000 | Loss: 0.00000990
Iteration 44/1000 | Loss: 0.00000990
Iteration 45/1000 | Loss: 0.00000989
Iteration 46/1000 | Loss: 0.00000989
Iteration 47/1000 | Loss: 0.00000988
Iteration 48/1000 | Loss: 0.00000987
Iteration 49/1000 | Loss: 0.00000987
Iteration 50/1000 | Loss: 0.00000986
Iteration 51/1000 | Loss: 0.00000985
Iteration 52/1000 | Loss: 0.00000985
Iteration 53/1000 | Loss: 0.00000984
Iteration 54/1000 | Loss: 0.00000983
Iteration 55/1000 | Loss: 0.00000983
Iteration 56/1000 | Loss: 0.00000983
Iteration 57/1000 | Loss: 0.00000983
Iteration 58/1000 | Loss: 0.00000982
Iteration 59/1000 | Loss: 0.00000982
Iteration 60/1000 | Loss: 0.00000981
Iteration 61/1000 | Loss: 0.00000981
Iteration 62/1000 | Loss: 0.00000980
Iteration 63/1000 | Loss: 0.00000979
Iteration 64/1000 | Loss: 0.00000978
Iteration 65/1000 | Loss: 0.00000977
Iteration 66/1000 | Loss: 0.00000974
Iteration 67/1000 | Loss: 0.00000973
Iteration 68/1000 | Loss: 0.00000971
Iteration 69/1000 | Loss: 0.00000971
Iteration 70/1000 | Loss: 0.00000970
Iteration 71/1000 | Loss: 0.00000970
Iteration 72/1000 | Loss: 0.00000970
Iteration 73/1000 | Loss: 0.00000969
Iteration 74/1000 | Loss: 0.00000968
Iteration 75/1000 | Loss: 0.00000968
Iteration 76/1000 | Loss: 0.00000968
Iteration 77/1000 | Loss: 0.00000968
Iteration 78/1000 | Loss: 0.00000968
Iteration 79/1000 | Loss: 0.00000967
Iteration 80/1000 | Loss: 0.00000967
Iteration 81/1000 | Loss: 0.00000967
Iteration 82/1000 | Loss: 0.00000967
Iteration 83/1000 | Loss: 0.00000967
Iteration 84/1000 | Loss: 0.00000965
Iteration 85/1000 | Loss: 0.00000964
Iteration 86/1000 | Loss: 0.00000963
Iteration 87/1000 | Loss: 0.00000963
Iteration 88/1000 | Loss: 0.00000962
Iteration 89/1000 | Loss: 0.00000962
Iteration 90/1000 | Loss: 0.00000962
Iteration 91/1000 | Loss: 0.00000962
Iteration 92/1000 | Loss: 0.00000961
Iteration 93/1000 | Loss: 0.00000960
Iteration 94/1000 | Loss: 0.00000959
Iteration 95/1000 | Loss: 0.00000959
Iteration 96/1000 | Loss: 0.00000959
Iteration 97/1000 | Loss: 0.00000958
Iteration 98/1000 | Loss: 0.00000958
Iteration 99/1000 | Loss: 0.00000958
Iteration 100/1000 | Loss: 0.00000958
Iteration 101/1000 | Loss: 0.00000958
Iteration 102/1000 | Loss: 0.00000958
Iteration 103/1000 | Loss: 0.00000958
Iteration 104/1000 | Loss: 0.00000957
Iteration 105/1000 | Loss: 0.00000957
Iteration 106/1000 | Loss: 0.00000957
Iteration 107/1000 | Loss: 0.00000957
Iteration 108/1000 | Loss: 0.00000956
Iteration 109/1000 | Loss: 0.00000956
Iteration 110/1000 | Loss: 0.00000956
Iteration 111/1000 | Loss: 0.00000956
Iteration 112/1000 | Loss: 0.00000956
Iteration 113/1000 | Loss: 0.00000955
Iteration 114/1000 | Loss: 0.00000955
Iteration 115/1000 | Loss: 0.00000955
Iteration 116/1000 | Loss: 0.00000955
Iteration 117/1000 | Loss: 0.00000955
Iteration 118/1000 | Loss: 0.00000955
Iteration 119/1000 | Loss: 0.00000955
Iteration 120/1000 | Loss: 0.00000955
Iteration 121/1000 | Loss: 0.00000954
Iteration 122/1000 | Loss: 0.00000954
Iteration 123/1000 | Loss: 0.00000954
Iteration 124/1000 | Loss: 0.00000954
Iteration 125/1000 | Loss: 0.00000954
Iteration 126/1000 | Loss: 0.00000954
Iteration 127/1000 | Loss: 0.00000954
Iteration 128/1000 | Loss: 0.00000954
Iteration 129/1000 | Loss: 0.00000954
Iteration 130/1000 | Loss: 0.00000954
Iteration 131/1000 | Loss: 0.00000954
Iteration 132/1000 | Loss: 0.00000953
Iteration 133/1000 | Loss: 0.00000953
Iteration 134/1000 | Loss: 0.00000952
Iteration 135/1000 | Loss: 0.00000952
Iteration 136/1000 | Loss: 0.00000952
Iteration 137/1000 | Loss: 0.00000951
Iteration 138/1000 | Loss: 0.00000951
Iteration 139/1000 | Loss: 0.00000951
Iteration 140/1000 | Loss: 0.00000951
Iteration 141/1000 | Loss: 0.00000951
Iteration 142/1000 | Loss: 0.00000950
Iteration 143/1000 | Loss: 0.00000950
Iteration 144/1000 | Loss: 0.00000950
Iteration 145/1000 | Loss: 0.00000949
Iteration 146/1000 | Loss: 0.00000949
Iteration 147/1000 | Loss: 0.00000949
Iteration 148/1000 | Loss: 0.00000949
Iteration 149/1000 | Loss: 0.00000949
Iteration 150/1000 | Loss: 0.00000949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [9.493765901424922e-06, 9.493765901424922e-06, 9.493765901424922e-06, 9.493765901424922e-06, 9.493765901424922e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.493765901424922e-06

Optimization complete. Final v2v error: 2.629244089126587 mm

Highest mean error: 3.0085203647613525 mm for frame 36

Lowest mean error: 2.385676860809326 mm for frame 192

Saving results

Total time: 45.502825021743774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1091/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1091.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1091
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00578897
Iteration 2/25 | Loss: 0.00149155
Iteration 3/25 | Loss: 0.00128710
Iteration 4/25 | Loss: 0.00126296
Iteration 5/25 | Loss: 0.00126030
Iteration 6/25 | Loss: 0.00125976
Iteration 7/25 | Loss: 0.00125976
Iteration 8/25 | Loss: 0.00125976
Iteration 9/25 | Loss: 0.00125976
Iteration 10/25 | Loss: 0.00125976
Iteration 11/25 | Loss: 0.00125976
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012597590684890747, 0.0012597590684890747, 0.0012597590684890747, 0.0012597590684890747, 0.0012597590684890747]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012597590684890747

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28093123
Iteration 2/25 | Loss: 0.00094030
Iteration 3/25 | Loss: 0.00094026
Iteration 4/25 | Loss: 0.00094026
Iteration 5/25 | Loss: 0.00094026
Iteration 6/25 | Loss: 0.00094026
Iteration 7/25 | Loss: 0.00094026
Iteration 8/25 | Loss: 0.00094026
Iteration 9/25 | Loss: 0.00094026
Iteration 10/25 | Loss: 0.00094026
Iteration 11/25 | Loss: 0.00094026
Iteration 12/25 | Loss: 0.00094026
Iteration 13/25 | Loss: 0.00094026
Iteration 14/25 | Loss: 0.00094026
Iteration 15/25 | Loss: 0.00094026
Iteration 16/25 | Loss: 0.00094026
Iteration 17/25 | Loss: 0.00094026
Iteration 18/25 | Loss: 0.00094026
Iteration 19/25 | Loss: 0.00094026
Iteration 20/25 | Loss: 0.00094026
Iteration 21/25 | Loss: 0.00094026
Iteration 22/25 | Loss: 0.00094026
Iteration 23/25 | Loss: 0.00094026
Iteration 24/25 | Loss: 0.00094026
Iteration 25/25 | Loss: 0.00094026

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00094026
Iteration 2/1000 | Loss: 0.00004028
Iteration 3/1000 | Loss: 0.00002691
Iteration 4/1000 | Loss: 0.00002241
Iteration 5/1000 | Loss: 0.00002048
Iteration 6/1000 | Loss: 0.00001921
Iteration 7/1000 | Loss: 0.00001849
Iteration 8/1000 | Loss: 0.00001790
Iteration 9/1000 | Loss: 0.00001757
Iteration 10/1000 | Loss: 0.00001725
Iteration 11/1000 | Loss: 0.00001698
Iteration 12/1000 | Loss: 0.00001671
Iteration 13/1000 | Loss: 0.00001650
Iteration 14/1000 | Loss: 0.00001642
Iteration 15/1000 | Loss: 0.00001641
Iteration 16/1000 | Loss: 0.00001622
Iteration 17/1000 | Loss: 0.00001621
Iteration 18/1000 | Loss: 0.00001617
Iteration 19/1000 | Loss: 0.00001616
Iteration 20/1000 | Loss: 0.00001605
Iteration 21/1000 | Loss: 0.00001601
Iteration 22/1000 | Loss: 0.00001599
Iteration 23/1000 | Loss: 0.00001599
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001596
Iteration 29/1000 | Loss: 0.00001596
Iteration 30/1000 | Loss: 0.00001590
Iteration 31/1000 | Loss: 0.00001590
Iteration 32/1000 | Loss: 0.00001588
Iteration 33/1000 | Loss: 0.00001585
Iteration 34/1000 | Loss: 0.00001585
Iteration 35/1000 | Loss: 0.00001585
Iteration 36/1000 | Loss: 0.00001584
Iteration 37/1000 | Loss: 0.00001583
Iteration 38/1000 | Loss: 0.00001580
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001577
Iteration 44/1000 | Loss: 0.00001577
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001576
Iteration 47/1000 | Loss: 0.00001575
Iteration 48/1000 | Loss: 0.00001575
Iteration 49/1000 | Loss: 0.00001574
Iteration 50/1000 | Loss: 0.00001573
Iteration 51/1000 | Loss: 0.00001573
Iteration 52/1000 | Loss: 0.00001573
Iteration 53/1000 | Loss: 0.00001572
Iteration 54/1000 | Loss: 0.00001572
Iteration 55/1000 | Loss: 0.00001572
Iteration 56/1000 | Loss: 0.00001571
Iteration 57/1000 | Loss: 0.00001571
Iteration 58/1000 | Loss: 0.00001571
Iteration 59/1000 | Loss: 0.00001571
Iteration 60/1000 | Loss: 0.00001570
Iteration 61/1000 | Loss: 0.00001570
Iteration 62/1000 | Loss: 0.00001569
Iteration 63/1000 | Loss: 0.00001569
Iteration 64/1000 | Loss: 0.00001568
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001567
Iteration 67/1000 | Loss: 0.00001567
Iteration 68/1000 | Loss: 0.00001566
Iteration 69/1000 | Loss: 0.00001566
Iteration 70/1000 | Loss: 0.00001565
Iteration 71/1000 | Loss: 0.00001565
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001563
Iteration 75/1000 | Loss: 0.00001563
Iteration 76/1000 | Loss: 0.00001563
Iteration 77/1000 | Loss: 0.00001563
Iteration 78/1000 | Loss: 0.00001563
Iteration 79/1000 | Loss: 0.00001562
Iteration 80/1000 | Loss: 0.00001562
Iteration 81/1000 | Loss: 0.00001561
Iteration 82/1000 | Loss: 0.00001561
Iteration 83/1000 | Loss: 0.00001561
Iteration 84/1000 | Loss: 0.00001560
Iteration 85/1000 | Loss: 0.00001560
Iteration 86/1000 | Loss: 0.00001560
Iteration 87/1000 | Loss: 0.00001560
Iteration 88/1000 | Loss: 0.00001560
Iteration 89/1000 | Loss: 0.00001560
Iteration 90/1000 | Loss: 0.00001559
Iteration 91/1000 | Loss: 0.00001559
Iteration 92/1000 | Loss: 0.00001559
Iteration 93/1000 | Loss: 0.00001559
Iteration 94/1000 | Loss: 0.00001559
Iteration 95/1000 | Loss: 0.00001559
Iteration 96/1000 | Loss: 0.00001559
Iteration 97/1000 | Loss: 0.00001559
Iteration 98/1000 | Loss: 0.00001559
Iteration 99/1000 | Loss: 0.00001559
Iteration 100/1000 | Loss: 0.00001558
Iteration 101/1000 | Loss: 0.00001558
Iteration 102/1000 | Loss: 0.00001558
Iteration 103/1000 | Loss: 0.00001558
Iteration 104/1000 | Loss: 0.00001558
Iteration 105/1000 | Loss: 0.00001558
Iteration 106/1000 | Loss: 0.00001558
Iteration 107/1000 | Loss: 0.00001558
Iteration 108/1000 | Loss: 0.00001558
Iteration 109/1000 | Loss: 0.00001558
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 109. Stopping optimization.
Last 5 losses: [1.558231633680407e-05, 1.558231633680407e-05, 1.558231633680407e-05, 1.558231633680407e-05, 1.558231633680407e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.558231633680407e-05

Optimization complete. Final v2v error: 3.3407533168792725 mm

Highest mean error: 3.6358211040496826 mm for frame 69

Lowest mean error: 2.9718234539031982 mm for frame 16

Saving results

Total time: 38.76886439323425
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00643145
Iteration 2/25 | Loss: 0.00171673
Iteration 3/25 | Loss: 0.00144203
Iteration 4/25 | Loss: 0.00141751
Iteration 5/25 | Loss: 0.00141402
Iteration 6/25 | Loss: 0.00141402
Iteration 7/25 | Loss: 0.00141402
Iteration 8/25 | Loss: 0.00141402
Iteration 9/25 | Loss: 0.00141402
Iteration 10/25 | Loss: 0.00141402
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0014140214771032333, 0.0014140214771032333, 0.0014140214771032333, 0.0014140214771032333, 0.0014140214771032333]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014140214771032333

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37410808
Iteration 2/25 | Loss: 0.00119536
Iteration 3/25 | Loss: 0.00119534
Iteration 4/25 | Loss: 0.00119534
Iteration 5/25 | Loss: 0.00119534
Iteration 6/25 | Loss: 0.00119534
Iteration 7/25 | Loss: 0.00119534
Iteration 8/25 | Loss: 0.00119533
Iteration 9/25 | Loss: 0.00119533
Iteration 10/25 | Loss: 0.00119533
Iteration 11/25 | Loss: 0.00119533
Iteration 12/25 | Loss: 0.00119533
Iteration 13/25 | Loss: 0.00119533
Iteration 14/25 | Loss: 0.00119533
Iteration 15/25 | Loss: 0.00119533
Iteration 16/25 | Loss: 0.00119533
Iteration 17/25 | Loss: 0.00119533
Iteration 18/25 | Loss: 0.00119533
Iteration 19/25 | Loss: 0.00119533
Iteration 20/25 | Loss: 0.00119533
Iteration 21/25 | Loss: 0.00119533
Iteration 22/25 | Loss: 0.00119533
Iteration 23/25 | Loss: 0.00119533
Iteration 24/25 | Loss: 0.00119533
Iteration 25/25 | Loss: 0.00119533

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119533
Iteration 2/1000 | Loss: 0.00005812
Iteration 3/1000 | Loss: 0.00003825
Iteration 4/1000 | Loss: 0.00003226
Iteration 5/1000 | Loss: 0.00003013
Iteration 6/1000 | Loss: 0.00002916
Iteration 7/1000 | Loss: 0.00002836
Iteration 8/1000 | Loss: 0.00002779
Iteration 9/1000 | Loss: 0.00002742
Iteration 10/1000 | Loss: 0.00002707
Iteration 11/1000 | Loss: 0.00002684
Iteration 12/1000 | Loss: 0.00002660
Iteration 13/1000 | Loss: 0.00002640
Iteration 14/1000 | Loss: 0.00002628
Iteration 15/1000 | Loss: 0.00002626
Iteration 16/1000 | Loss: 0.00002626
Iteration 17/1000 | Loss: 0.00002623
Iteration 18/1000 | Loss: 0.00002619
Iteration 19/1000 | Loss: 0.00002618
Iteration 20/1000 | Loss: 0.00002616
Iteration 21/1000 | Loss: 0.00002616
Iteration 22/1000 | Loss: 0.00002615
Iteration 23/1000 | Loss: 0.00002614
Iteration 24/1000 | Loss: 0.00002614
Iteration 25/1000 | Loss: 0.00002614
Iteration 26/1000 | Loss: 0.00002614
Iteration 27/1000 | Loss: 0.00002614
Iteration 28/1000 | Loss: 0.00002614
Iteration 29/1000 | Loss: 0.00002613
Iteration 30/1000 | Loss: 0.00002613
Iteration 31/1000 | Loss: 0.00002613
Iteration 32/1000 | Loss: 0.00002613
Iteration 33/1000 | Loss: 0.00002613
Iteration 34/1000 | Loss: 0.00002613
Iteration 35/1000 | Loss: 0.00002612
Iteration 36/1000 | Loss: 0.00002610
Iteration 37/1000 | Loss: 0.00002609
Iteration 38/1000 | Loss: 0.00002609
Iteration 39/1000 | Loss: 0.00002609
Iteration 40/1000 | Loss: 0.00002608
Iteration 41/1000 | Loss: 0.00002607
Iteration 42/1000 | Loss: 0.00002606
Iteration 43/1000 | Loss: 0.00002606
Iteration 44/1000 | Loss: 0.00002606
Iteration 45/1000 | Loss: 0.00002606
Iteration 46/1000 | Loss: 0.00002605
Iteration 47/1000 | Loss: 0.00002604
Iteration 48/1000 | Loss: 0.00002604
Iteration 49/1000 | Loss: 0.00002603
Iteration 50/1000 | Loss: 0.00002603
Iteration 51/1000 | Loss: 0.00002603
Iteration 52/1000 | Loss: 0.00002603
Iteration 53/1000 | Loss: 0.00002602
Iteration 54/1000 | Loss: 0.00002602
Iteration 55/1000 | Loss: 0.00002602
Iteration 56/1000 | Loss: 0.00002602
Iteration 57/1000 | Loss: 0.00002602
Iteration 58/1000 | Loss: 0.00002602
Iteration 59/1000 | Loss: 0.00002602
Iteration 60/1000 | Loss: 0.00002602
Iteration 61/1000 | Loss: 0.00002602
Iteration 62/1000 | Loss: 0.00002601
Iteration 63/1000 | Loss: 0.00002601
Iteration 64/1000 | Loss: 0.00002601
Iteration 65/1000 | Loss: 0.00002601
Iteration 66/1000 | Loss: 0.00002601
Iteration 67/1000 | Loss: 0.00002601
Iteration 68/1000 | Loss: 0.00002601
Iteration 69/1000 | Loss: 0.00002601
Iteration 70/1000 | Loss: 0.00002601
Iteration 71/1000 | Loss: 0.00002601
Iteration 72/1000 | Loss: 0.00002601
Iteration 73/1000 | Loss: 0.00002601
Iteration 74/1000 | Loss: 0.00002601
Iteration 75/1000 | Loss: 0.00002600
Iteration 76/1000 | Loss: 0.00002600
Iteration 77/1000 | Loss: 0.00002600
Iteration 78/1000 | Loss: 0.00002600
Iteration 79/1000 | Loss: 0.00002600
Iteration 80/1000 | Loss: 0.00002600
Iteration 81/1000 | Loss: 0.00002600
Iteration 82/1000 | Loss: 0.00002600
Iteration 83/1000 | Loss: 0.00002600
Iteration 84/1000 | Loss: 0.00002600
Iteration 85/1000 | Loss: 0.00002599
Iteration 86/1000 | Loss: 0.00002599
Iteration 87/1000 | Loss: 0.00002599
Iteration 88/1000 | Loss: 0.00002599
Iteration 89/1000 | Loss: 0.00002599
Iteration 90/1000 | Loss: 0.00002599
Iteration 91/1000 | Loss: 0.00002599
Iteration 92/1000 | Loss: 0.00002599
Iteration 93/1000 | Loss: 0.00002599
Iteration 94/1000 | Loss: 0.00002599
Iteration 95/1000 | Loss: 0.00002598
Iteration 96/1000 | Loss: 0.00002598
Iteration 97/1000 | Loss: 0.00002598
Iteration 98/1000 | Loss: 0.00002598
Iteration 99/1000 | Loss: 0.00002598
Iteration 100/1000 | Loss: 0.00002598
Iteration 101/1000 | Loss: 0.00002598
Iteration 102/1000 | Loss: 0.00002598
Iteration 103/1000 | Loss: 0.00002598
Iteration 104/1000 | Loss: 0.00002597
Iteration 105/1000 | Loss: 0.00002597
Iteration 106/1000 | Loss: 0.00002597
Iteration 107/1000 | Loss: 0.00002596
Iteration 108/1000 | Loss: 0.00002596
Iteration 109/1000 | Loss: 0.00002596
Iteration 110/1000 | Loss: 0.00002596
Iteration 111/1000 | Loss: 0.00002596
Iteration 112/1000 | Loss: 0.00002596
Iteration 113/1000 | Loss: 0.00002596
Iteration 114/1000 | Loss: 0.00002596
Iteration 115/1000 | Loss: 0.00002596
Iteration 116/1000 | Loss: 0.00002595
Iteration 117/1000 | Loss: 0.00002595
Iteration 118/1000 | Loss: 0.00002595
Iteration 119/1000 | Loss: 0.00002595
Iteration 120/1000 | Loss: 0.00002595
Iteration 121/1000 | Loss: 0.00002595
Iteration 122/1000 | Loss: 0.00002595
Iteration 123/1000 | Loss: 0.00002595
Iteration 124/1000 | Loss: 0.00002595
Iteration 125/1000 | Loss: 0.00002595
Iteration 126/1000 | Loss: 0.00002595
Iteration 127/1000 | Loss: 0.00002595
Iteration 128/1000 | Loss: 0.00002595
Iteration 129/1000 | Loss: 0.00002595
Iteration 130/1000 | Loss: 0.00002595
Iteration 131/1000 | Loss: 0.00002595
Iteration 132/1000 | Loss: 0.00002595
Iteration 133/1000 | Loss: 0.00002595
Iteration 134/1000 | Loss: 0.00002595
Iteration 135/1000 | Loss: 0.00002595
Iteration 136/1000 | Loss: 0.00002595
Iteration 137/1000 | Loss: 0.00002595
Iteration 138/1000 | Loss: 0.00002595
Iteration 139/1000 | Loss: 0.00002595
Iteration 140/1000 | Loss: 0.00002595
Iteration 141/1000 | Loss: 0.00002595
Iteration 142/1000 | Loss: 0.00002595
Iteration 143/1000 | Loss: 0.00002595
Iteration 144/1000 | Loss: 0.00002595
Iteration 145/1000 | Loss: 0.00002595
Iteration 146/1000 | Loss: 0.00002595
Iteration 147/1000 | Loss: 0.00002595
Iteration 148/1000 | Loss: 0.00002595
Iteration 149/1000 | Loss: 0.00002595
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [2.59494008787442e-05, 2.59494008787442e-05, 2.59494008787442e-05, 2.59494008787442e-05, 2.59494008787442e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.59494008787442e-05

Optimization complete. Final v2v error: 4.240396976470947 mm

Highest mean error: 4.667042255401611 mm for frame 64

Lowest mean error: 3.740359306335449 mm for frame 175

Saving results

Total time: 37.14577913284302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764760
Iteration 2/25 | Loss: 0.00154612
Iteration 3/25 | Loss: 0.00128490
Iteration 4/25 | Loss: 0.00124294
Iteration 5/25 | Loss: 0.00123278
Iteration 6/25 | Loss: 0.00123013
Iteration 7/25 | Loss: 0.00122286
Iteration 8/25 | Loss: 0.00122146
Iteration 9/25 | Loss: 0.00122126
Iteration 10/25 | Loss: 0.00122119
Iteration 11/25 | Loss: 0.00122118
Iteration 12/25 | Loss: 0.00122118
Iteration 13/25 | Loss: 0.00122118
Iteration 14/25 | Loss: 0.00122118
Iteration 15/25 | Loss: 0.00122118
Iteration 16/25 | Loss: 0.00122118
Iteration 17/25 | Loss: 0.00122118
Iteration 18/25 | Loss: 0.00122118
Iteration 19/25 | Loss: 0.00122118
Iteration 20/25 | Loss: 0.00122118
Iteration 21/25 | Loss: 0.00122117
Iteration 22/25 | Loss: 0.00122117
Iteration 23/25 | Loss: 0.00122117
Iteration 24/25 | Loss: 0.00122117
Iteration 25/25 | Loss: 0.00122117

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28786528
Iteration 2/25 | Loss: 0.00137708
Iteration 3/25 | Loss: 0.00137708
Iteration 4/25 | Loss: 0.00137708
Iteration 5/25 | Loss: 0.00137708
Iteration 6/25 | Loss: 0.00137707
Iteration 7/25 | Loss: 0.00137707
Iteration 8/25 | Loss: 0.00137707
Iteration 9/25 | Loss: 0.00137707
Iteration 10/25 | Loss: 0.00137707
Iteration 11/25 | Loss: 0.00137707
Iteration 12/25 | Loss: 0.00137707
Iteration 13/25 | Loss: 0.00137707
Iteration 14/25 | Loss: 0.00137707
Iteration 15/25 | Loss: 0.00137707
Iteration 16/25 | Loss: 0.00137707
Iteration 17/25 | Loss: 0.00137707
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0013770732330158353, 0.0013770732330158353, 0.0013770732330158353, 0.0013770732330158353, 0.0013770732330158353]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013770732330158353

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00137707
Iteration 2/1000 | Loss: 0.00006203
Iteration 3/1000 | Loss: 0.00004092
Iteration 4/1000 | Loss: 0.00003508
Iteration 5/1000 | Loss: 0.00003242
Iteration 6/1000 | Loss: 0.00003042
Iteration 7/1000 | Loss: 0.00002896
Iteration 8/1000 | Loss: 0.00002800
Iteration 9/1000 | Loss: 0.00002712
Iteration 10/1000 | Loss: 0.00002642
Iteration 11/1000 | Loss: 0.00002591
Iteration 12/1000 | Loss: 0.00002559
Iteration 13/1000 | Loss: 0.00002524
Iteration 14/1000 | Loss: 0.00002492
Iteration 15/1000 | Loss: 0.00002472
Iteration 16/1000 | Loss: 0.00002455
Iteration 17/1000 | Loss: 0.00002452
Iteration 18/1000 | Loss: 0.00002452
Iteration 19/1000 | Loss: 0.00002450
Iteration 20/1000 | Loss: 0.00002448
Iteration 21/1000 | Loss: 0.00002445
Iteration 22/1000 | Loss: 0.00002440
Iteration 23/1000 | Loss: 0.00002436
Iteration 24/1000 | Loss: 0.00002431
Iteration 25/1000 | Loss: 0.00002430
Iteration 26/1000 | Loss: 0.00002430
Iteration 27/1000 | Loss: 0.00002429
Iteration 28/1000 | Loss: 0.00002426
Iteration 29/1000 | Loss: 0.00002424
Iteration 30/1000 | Loss: 0.00002424
Iteration 31/1000 | Loss: 0.00002422
Iteration 32/1000 | Loss: 0.00002422
Iteration 33/1000 | Loss: 0.00002421
Iteration 34/1000 | Loss: 0.00002421
Iteration 35/1000 | Loss: 0.00002421
Iteration 36/1000 | Loss: 0.00002419
Iteration 37/1000 | Loss: 0.00002417
Iteration 38/1000 | Loss: 0.00002416
Iteration 39/1000 | Loss: 0.00002416
Iteration 40/1000 | Loss: 0.00002414
Iteration 41/1000 | Loss: 0.00002413
Iteration 42/1000 | Loss: 0.00002413
Iteration 43/1000 | Loss: 0.00002413
Iteration 44/1000 | Loss: 0.00002411
Iteration 45/1000 | Loss: 0.00002411
Iteration 46/1000 | Loss: 0.00002410
Iteration 47/1000 | Loss: 0.00002410
Iteration 48/1000 | Loss: 0.00002409
Iteration 49/1000 | Loss: 0.00002408
Iteration 50/1000 | Loss: 0.00002407
Iteration 51/1000 | Loss: 0.00002407
Iteration 52/1000 | Loss: 0.00002407
Iteration 53/1000 | Loss: 0.00002407
Iteration 54/1000 | Loss: 0.00002406
Iteration 55/1000 | Loss: 0.00002406
Iteration 56/1000 | Loss: 0.00002406
Iteration 57/1000 | Loss: 0.00002406
Iteration 58/1000 | Loss: 0.00002406
Iteration 59/1000 | Loss: 0.00002406
Iteration 60/1000 | Loss: 0.00002406
Iteration 61/1000 | Loss: 0.00002406
Iteration 62/1000 | Loss: 0.00002406
Iteration 63/1000 | Loss: 0.00002405
Iteration 64/1000 | Loss: 0.00002404
Iteration 65/1000 | Loss: 0.00002403
Iteration 66/1000 | Loss: 0.00002403
Iteration 67/1000 | Loss: 0.00002403
Iteration 68/1000 | Loss: 0.00002402
Iteration 69/1000 | Loss: 0.00002402
Iteration 70/1000 | Loss: 0.00002402
Iteration 71/1000 | Loss: 0.00002402
Iteration 72/1000 | Loss: 0.00002402
Iteration 73/1000 | Loss: 0.00002401
Iteration 74/1000 | Loss: 0.00002401
Iteration 75/1000 | Loss: 0.00002401
Iteration 76/1000 | Loss: 0.00002401
Iteration 77/1000 | Loss: 0.00002401
Iteration 78/1000 | Loss: 0.00002401
Iteration 79/1000 | Loss: 0.00002400
Iteration 80/1000 | Loss: 0.00002400
Iteration 81/1000 | Loss: 0.00002400
Iteration 82/1000 | Loss: 0.00002399
Iteration 83/1000 | Loss: 0.00002399
Iteration 84/1000 | Loss: 0.00002399
Iteration 85/1000 | Loss: 0.00002398
Iteration 86/1000 | Loss: 0.00002398
Iteration 87/1000 | Loss: 0.00002398
Iteration 88/1000 | Loss: 0.00002397
Iteration 89/1000 | Loss: 0.00002397
Iteration 90/1000 | Loss: 0.00002397
Iteration 91/1000 | Loss: 0.00002397
Iteration 92/1000 | Loss: 0.00002397
Iteration 93/1000 | Loss: 0.00002396
Iteration 94/1000 | Loss: 0.00002396
Iteration 95/1000 | Loss: 0.00002396
Iteration 96/1000 | Loss: 0.00002396
Iteration 97/1000 | Loss: 0.00002396
Iteration 98/1000 | Loss: 0.00002396
Iteration 99/1000 | Loss: 0.00002396
Iteration 100/1000 | Loss: 0.00002396
Iteration 101/1000 | Loss: 0.00002395
Iteration 102/1000 | Loss: 0.00002395
Iteration 103/1000 | Loss: 0.00002395
Iteration 104/1000 | Loss: 0.00002395
Iteration 105/1000 | Loss: 0.00002395
Iteration 106/1000 | Loss: 0.00002395
Iteration 107/1000 | Loss: 0.00002395
Iteration 108/1000 | Loss: 0.00002395
Iteration 109/1000 | Loss: 0.00002395
Iteration 110/1000 | Loss: 0.00002395
Iteration 111/1000 | Loss: 0.00002394
Iteration 112/1000 | Loss: 0.00002394
Iteration 113/1000 | Loss: 0.00002394
Iteration 114/1000 | Loss: 0.00002394
Iteration 115/1000 | Loss: 0.00002394
Iteration 116/1000 | Loss: 0.00002394
Iteration 117/1000 | Loss: 0.00002393
Iteration 118/1000 | Loss: 0.00002393
Iteration 119/1000 | Loss: 0.00002393
Iteration 120/1000 | Loss: 0.00002393
Iteration 121/1000 | Loss: 0.00002393
Iteration 122/1000 | Loss: 0.00002393
Iteration 123/1000 | Loss: 0.00002393
Iteration 124/1000 | Loss: 0.00002393
Iteration 125/1000 | Loss: 0.00002393
Iteration 126/1000 | Loss: 0.00002393
Iteration 127/1000 | Loss: 0.00002393
Iteration 128/1000 | Loss: 0.00002393
Iteration 129/1000 | Loss: 0.00002393
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [2.3931941541377455e-05, 2.3931941541377455e-05, 2.3931941541377455e-05, 2.3931941541377455e-05, 2.3931941541377455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3931941541377455e-05

Optimization complete. Final v2v error: 4.053659915924072 mm

Highest mean error: 5.511744976043701 mm for frame 74

Lowest mean error: 3.111966848373413 mm for frame 4

Saving results

Total time: 57.70066475868225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00390599
Iteration 2/25 | Loss: 0.00126559
Iteration 3/25 | Loss: 0.00120173
Iteration 4/25 | Loss: 0.00119198
Iteration 5/25 | Loss: 0.00119040
Iteration 6/25 | Loss: 0.00119040
Iteration 7/25 | Loss: 0.00119040
Iteration 8/25 | Loss: 0.00119040
Iteration 9/25 | Loss: 0.00119040
Iteration 10/25 | Loss: 0.00119040
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011904038256034255, 0.0011904038256034255, 0.0011904038256034255, 0.0011904038256034255, 0.0011904038256034255]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011904038256034255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31445646
Iteration 2/25 | Loss: 0.00112740
Iteration 3/25 | Loss: 0.00112740
Iteration 4/25 | Loss: 0.00112740
Iteration 5/25 | Loss: 0.00112740
Iteration 6/25 | Loss: 0.00112740
Iteration 7/25 | Loss: 0.00112740
Iteration 8/25 | Loss: 0.00112740
Iteration 9/25 | Loss: 0.00112740
Iteration 10/25 | Loss: 0.00112740
Iteration 11/25 | Loss: 0.00112740
Iteration 12/25 | Loss: 0.00112740
Iteration 13/25 | Loss: 0.00112740
Iteration 14/25 | Loss: 0.00112740
Iteration 15/25 | Loss: 0.00112740
Iteration 16/25 | Loss: 0.00112740
Iteration 17/25 | Loss: 0.00112740
Iteration 18/25 | Loss: 0.00112740
Iteration 19/25 | Loss: 0.00112740
Iteration 20/25 | Loss: 0.00112740
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011273982236161828, 0.0011273982236161828, 0.0011273982236161828, 0.0011273982236161828, 0.0011273982236161828]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011273982236161828

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112740
Iteration 2/1000 | Loss: 0.00001824
Iteration 3/1000 | Loss: 0.00001549
Iteration 4/1000 | Loss: 0.00001414
Iteration 5/1000 | Loss: 0.00001343
Iteration 6/1000 | Loss: 0.00001332
Iteration 7/1000 | Loss: 0.00001295
Iteration 8/1000 | Loss: 0.00001249
Iteration 9/1000 | Loss: 0.00001227
Iteration 10/1000 | Loss: 0.00001214
Iteration 11/1000 | Loss: 0.00001198
Iteration 12/1000 | Loss: 0.00001197
Iteration 13/1000 | Loss: 0.00001194
Iteration 14/1000 | Loss: 0.00001192
Iteration 15/1000 | Loss: 0.00001188
Iteration 16/1000 | Loss: 0.00001185
Iteration 17/1000 | Loss: 0.00001178
Iteration 18/1000 | Loss: 0.00001176
Iteration 19/1000 | Loss: 0.00001176
Iteration 20/1000 | Loss: 0.00001176
Iteration 21/1000 | Loss: 0.00001176
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001164
Iteration 24/1000 | Loss: 0.00001163
Iteration 25/1000 | Loss: 0.00001161
Iteration 26/1000 | Loss: 0.00001160
Iteration 27/1000 | Loss: 0.00001158
Iteration 28/1000 | Loss: 0.00001157
Iteration 29/1000 | Loss: 0.00001157
Iteration 30/1000 | Loss: 0.00001155
Iteration 31/1000 | Loss: 0.00001154
Iteration 32/1000 | Loss: 0.00001154
Iteration 33/1000 | Loss: 0.00001151
Iteration 34/1000 | Loss: 0.00001150
Iteration 35/1000 | Loss: 0.00001145
Iteration 36/1000 | Loss: 0.00001145
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001142
Iteration 39/1000 | Loss: 0.00001142
Iteration 40/1000 | Loss: 0.00001141
Iteration 41/1000 | Loss: 0.00001134
Iteration 42/1000 | Loss: 0.00001133
Iteration 43/1000 | Loss: 0.00001133
Iteration 44/1000 | Loss: 0.00001133
Iteration 45/1000 | Loss: 0.00001132
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001132
Iteration 48/1000 | Loss: 0.00001131
Iteration 49/1000 | Loss: 0.00001131
Iteration 50/1000 | Loss: 0.00001131
Iteration 51/1000 | Loss: 0.00001131
Iteration 52/1000 | Loss: 0.00001131
Iteration 53/1000 | Loss: 0.00001130
Iteration 54/1000 | Loss: 0.00001127
Iteration 55/1000 | Loss: 0.00001127
Iteration 56/1000 | Loss: 0.00001127
Iteration 57/1000 | Loss: 0.00001126
Iteration 58/1000 | Loss: 0.00001126
Iteration 59/1000 | Loss: 0.00001126
Iteration 60/1000 | Loss: 0.00001126
Iteration 61/1000 | Loss: 0.00001125
Iteration 62/1000 | Loss: 0.00001125
Iteration 63/1000 | Loss: 0.00001120
Iteration 64/1000 | Loss: 0.00001118
Iteration 65/1000 | Loss: 0.00001118
Iteration 66/1000 | Loss: 0.00001118
Iteration 67/1000 | Loss: 0.00001118
Iteration 68/1000 | Loss: 0.00001118
Iteration 69/1000 | Loss: 0.00001117
Iteration 70/1000 | Loss: 0.00001115
Iteration 71/1000 | Loss: 0.00001115
Iteration 72/1000 | Loss: 0.00001114
Iteration 73/1000 | Loss: 0.00001114
Iteration 74/1000 | Loss: 0.00001114
Iteration 75/1000 | Loss: 0.00001114
Iteration 76/1000 | Loss: 0.00001113
Iteration 77/1000 | Loss: 0.00001113
Iteration 78/1000 | Loss: 0.00001113
Iteration 79/1000 | Loss: 0.00001112
Iteration 80/1000 | Loss: 0.00001112
Iteration 81/1000 | Loss: 0.00001112
Iteration 82/1000 | Loss: 0.00001112
Iteration 83/1000 | Loss: 0.00001112
Iteration 84/1000 | Loss: 0.00001111
Iteration 85/1000 | Loss: 0.00001111
Iteration 86/1000 | Loss: 0.00001111
Iteration 87/1000 | Loss: 0.00001111
Iteration 88/1000 | Loss: 0.00001110
Iteration 89/1000 | Loss: 0.00001110
Iteration 90/1000 | Loss: 0.00001110
Iteration 91/1000 | Loss: 0.00001110
Iteration 92/1000 | Loss: 0.00001110
Iteration 93/1000 | Loss: 0.00001110
Iteration 94/1000 | Loss: 0.00001110
Iteration 95/1000 | Loss: 0.00001110
Iteration 96/1000 | Loss: 0.00001110
Iteration 97/1000 | Loss: 0.00001110
Iteration 98/1000 | Loss: 0.00001110
Iteration 99/1000 | Loss: 0.00001110
Iteration 100/1000 | Loss: 0.00001110
Iteration 101/1000 | Loss: 0.00001110
Iteration 102/1000 | Loss: 0.00001110
Iteration 103/1000 | Loss: 0.00001110
Iteration 104/1000 | Loss: 0.00001110
Iteration 105/1000 | Loss: 0.00001110
Iteration 106/1000 | Loss: 0.00001109
Iteration 107/1000 | Loss: 0.00001109
Iteration 108/1000 | Loss: 0.00001109
Iteration 109/1000 | Loss: 0.00001109
Iteration 110/1000 | Loss: 0.00001109
Iteration 111/1000 | Loss: 0.00001109
Iteration 112/1000 | Loss: 0.00001109
Iteration 113/1000 | Loss: 0.00001109
Iteration 114/1000 | Loss: 0.00001109
Iteration 115/1000 | Loss: 0.00001109
Iteration 116/1000 | Loss: 0.00001109
Iteration 117/1000 | Loss: 0.00001109
Iteration 118/1000 | Loss: 0.00001109
Iteration 119/1000 | Loss: 0.00001108
Iteration 120/1000 | Loss: 0.00001108
Iteration 121/1000 | Loss: 0.00001108
Iteration 122/1000 | Loss: 0.00001108
Iteration 123/1000 | Loss: 0.00001108
Iteration 124/1000 | Loss: 0.00001108
Iteration 125/1000 | Loss: 0.00001108
Iteration 126/1000 | Loss: 0.00001108
Iteration 127/1000 | Loss: 0.00001108
Iteration 128/1000 | Loss: 0.00001108
Iteration 129/1000 | Loss: 0.00001108
Iteration 130/1000 | Loss: 0.00001107
Iteration 131/1000 | Loss: 0.00001107
Iteration 132/1000 | Loss: 0.00001107
Iteration 133/1000 | Loss: 0.00001107
Iteration 134/1000 | Loss: 0.00001107
Iteration 135/1000 | Loss: 0.00001107
Iteration 136/1000 | Loss: 0.00001107
Iteration 137/1000 | Loss: 0.00001107
Iteration 138/1000 | Loss: 0.00001107
Iteration 139/1000 | Loss: 0.00001107
Iteration 140/1000 | Loss: 0.00001107
Iteration 141/1000 | Loss: 0.00001107
Iteration 142/1000 | Loss: 0.00001107
Iteration 143/1000 | Loss: 0.00001106
Iteration 144/1000 | Loss: 0.00001106
Iteration 145/1000 | Loss: 0.00001106
Iteration 146/1000 | Loss: 0.00001106
Iteration 147/1000 | Loss: 0.00001106
Iteration 148/1000 | Loss: 0.00001106
Iteration 149/1000 | Loss: 0.00001106
Iteration 150/1000 | Loss: 0.00001106
Iteration 151/1000 | Loss: 0.00001106
Iteration 152/1000 | Loss: 0.00001106
Iteration 153/1000 | Loss: 0.00001106
Iteration 154/1000 | Loss: 0.00001106
Iteration 155/1000 | Loss: 0.00001106
Iteration 156/1000 | Loss: 0.00001106
Iteration 157/1000 | Loss: 0.00001106
Iteration 158/1000 | Loss: 0.00001106
Iteration 159/1000 | Loss: 0.00001106
Iteration 160/1000 | Loss: 0.00001106
Iteration 161/1000 | Loss: 0.00001106
Iteration 162/1000 | Loss: 0.00001106
Iteration 163/1000 | Loss: 0.00001106
Iteration 164/1000 | Loss: 0.00001106
Iteration 165/1000 | Loss: 0.00001106
Iteration 166/1000 | Loss: 0.00001106
Iteration 167/1000 | Loss: 0.00001106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [1.1059330972784664e-05, 1.1059330972784664e-05, 1.1059330972784664e-05, 1.1059330972784664e-05, 1.1059330972784664e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1059330972784664e-05

Optimization complete. Final v2v error: 2.8582468032836914 mm

Highest mean error: 2.942661762237549 mm for frame 266

Lowest mean error: 2.837339162826538 mm for frame 41

Saving results

Total time: 41.60525178909302
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00822545
Iteration 2/25 | Loss: 0.00153255
Iteration 3/25 | Loss: 0.00130553
Iteration 4/25 | Loss: 0.00128275
Iteration 5/25 | Loss: 0.00128095
Iteration 6/25 | Loss: 0.00128095
Iteration 7/25 | Loss: 0.00128095
Iteration 8/25 | Loss: 0.00128095
Iteration 9/25 | Loss: 0.00128095
Iteration 10/25 | Loss: 0.00128095
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0012809530599042773, 0.0012809530599042773, 0.0012809530599042773, 0.0012809530599042773, 0.0012809530599042773]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012809530599042773

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94317955
Iteration 2/25 | Loss: 0.00068409
Iteration 3/25 | Loss: 0.00068408
Iteration 4/25 | Loss: 0.00068408
Iteration 5/25 | Loss: 0.00068408
Iteration 6/25 | Loss: 0.00068408
Iteration 7/25 | Loss: 0.00068408
Iteration 8/25 | Loss: 0.00068408
Iteration 9/25 | Loss: 0.00068408
Iteration 10/25 | Loss: 0.00068408
Iteration 11/25 | Loss: 0.00068408
Iteration 12/25 | Loss: 0.00068408
Iteration 13/25 | Loss: 0.00068408
Iteration 14/25 | Loss: 0.00068408
Iteration 15/25 | Loss: 0.00068408
Iteration 16/25 | Loss: 0.00068408
Iteration 17/25 | Loss: 0.00068408
Iteration 18/25 | Loss: 0.00068408
Iteration 19/25 | Loss: 0.00068408
Iteration 20/25 | Loss: 0.00068408
Iteration 21/25 | Loss: 0.00068408
Iteration 22/25 | Loss: 0.00068408
Iteration 23/25 | Loss: 0.00068408
Iteration 24/25 | Loss: 0.00068408
Iteration 25/25 | Loss: 0.00068408
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0006840771529823542, 0.0006840771529823542, 0.0006840771529823542, 0.0006840771529823542, 0.0006840771529823542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006840771529823542

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00068408
Iteration 2/1000 | Loss: 0.00003312
Iteration 3/1000 | Loss: 0.00002426
Iteration 4/1000 | Loss: 0.00002189
Iteration 5/1000 | Loss: 0.00002091
Iteration 6/1000 | Loss: 0.00002034
Iteration 7/1000 | Loss: 0.00001978
Iteration 8/1000 | Loss: 0.00001934
Iteration 9/1000 | Loss: 0.00001900
Iteration 10/1000 | Loss: 0.00001874
Iteration 11/1000 | Loss: 0.00001859
Iteration 12/1000 | Loss: 0.00001858
Iteration 13/1000 | Loss: 0.00001853
Iteration 14/1000 | Loss: 0.00001851
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001837
Iteration 17/1000 | Loss: 0.00001834
Iteration 18/1000 | Loss: 0.00001834
Iteration 19/1000 | Loss: 0.00001829
Iteration 20/1000 | Loss: 0.00001829
Iteration 21/1000 | Loss: 0.00001828
Iteration 22/1000 | Loss: 0.00001828
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001828
Iteration 25/1000 | Loss: 0.00001828
Iteration 26/1000 | Loss: 0.00001828
Iteration 27/1000 | Loss: 0.00001827
Iteration 28/1000 | Loss: 0.00001827
Iteration 29/1000 | Loss: 0.00001826
Iteration 30/1000 | Loss: 0.00001826
Iteration 31/1000 | Loss: 0.00001823
Iteration 32/1000 | Loss: 0.00001822
Iteration 33/1000 | Loss: 0.00001822
Iteration 34/1000 | Loss: 0.00001822
Iteration 35/1000 | Loss: 0.00001822
Iteration 36/1000 | Loss: 0.00001822
Iteration 37/1000 | Loss: 0.00001822
Iteration 38/1000 | Loss: 0.00001817
Iteration 39/1000 | Loss: 0.00001816
Iteration 40/1000 | Loss: 0.00001816
Iteration 41/1000 | Loss: 0.00001816
Iteration 42/1000 | Loss: 0.00001816
Iteration 43/1000 | Loss: 0.00001816
Iteration 44/1000 | Loss: 0.00001816
Iteration 45/1000 | Loss: 0.00001816
Iteration 46/1000 | Loss: 0.00001816
Iteration 47/1000 | Loss: 0.00001815
Iteration 48/1000 | Loss: 0.00001815
Iteration 49/1000 | Loss: 0.00001814
Iteration 50/1000 | Loss: 0.00001814
Iteration 51/1000 | Loss: 0.00001814
Iteration 52/1000 | Loss: 0.00001813
Iteration 53/1000 | Loss: 0.00001813
Iteration 54/1000 | Loss: 0.00001813
Iteration 55/1000 | Loss: 0.00001813
Iteration 56/1000 | Loss: 0.00001813
Iteration 57/1000 | Loss: 0.00001812
Iteration 58/1000 | Loss: 0.00001811
Iteration 59/1000 | Loss: 0.00001810
Iteration 60/1000 | Loss: 0.00001810
Iteration 61/1000 | Loss: 0.00001809
Iteration 62/1000 | Loss: 0.00001806
Iteration 63/1000 | Loss: 0.00001804
Iteration 64/1000 | Loss: 0.00001803
Iteration 65/1000 | Loss: 0.00001803
Iteration 66/1000 | Loss: 0.00001802
Iteration 67/1000 | Loss: 0.00001802
Iteration 68/1000 | Loss: 0.00001802
Iteration 69/1000 | Loss: 0.00001802
Iteration 70/1000 | Loss: 0.00001801
Iteration 71/1000 | Loss: 0.00001801
Iteration 72/1000 | Loss: 0.00001801
Iteration 73/1000 | Loss: 0.00001801
Iteration 74/1000 | Loss: 0.00001800
Iteration 75/1000 | Loss: 0.00001800
Iteration 76/1000 | Loss: 0.00001800
Iteration 77/1000 | Loss: 0.00001800
Iteration 78/1000 | Loss: 0.00001800
Iteration 79/1000 | Loss: 0.00001800
Iteration 80/1000 | Loss: 0.00001800
Iteration 81/1000 | Loss: 0.00001800
Iteration 82/1000 | Loss: 0.00001799
Iteration 83/1000 | Loss: 0.00001799
Iteration 84/1000 | Loss: 0.00001799
Iteration 85/1000 | Loss: 0.00001799
Iteration 86/1000 | Loss: 0.00001799
Iteration 87/1000 | Loss: 0.00001799
Iteration 88/1000 | Loss: 0.00001796
Iteration 89/1000 | Loss: 0.00001796
Iteration 90/1000 | Loss: 0.00001796
Iteration 91/1000 | Loss: 0.00001795
Iteration 92/1000 | Loss: 0.00001795
Iteration 93/1000 | Loss: 0.00001795
Iteration 94/1000 | Loss: 0.00001794
Iteration 95/1000 | Loss: 0.00001794
Iteration 96/1000 | Loss: 0.00001794
Iteration 97/1000 | Loss: 0.00001794
Iteration 98/1000 | Loss: 0.00001794
Iteration 99/1000 | Loss: 0.00001794
Iteration 100/1000 | Loss: 0.00001793
Iteration 101/1000 | Loss: 0.00001793
Iteration 102/1000 | Loss: 0.00001793
Iteration 103/1000 | Loss: 0.00001793
Iteration 104/1000 | Loss: 0.00001793
Iteration 105/1000 | Loss: 0.00001793
Iteration 106/1000 | Loss: 0.00001793
Iteration 107/1000 | Loss: 0.00001793
Iteration 108/1000 | Loss: 0.00001793
Iteration 109/1000 | Loss: 0.00001793
Iteration 110/1000 | Loss: 0.00001793
Iteration 111/1000 | Loss: 0.00001793
Iteration 112/1000 | Loss: 0.00001792
Iteration 113/1000 | Loss: 0.00001792
Iteration 114/1000 | Loss: 0.00001792
Iteration 115/1000 | Loss: 0.00001792
Iteration 116/1000 | Loss: 0.00001792
Iteration 117/1000 | Loss: 0.00001792
Iteration 118/1000 | Loss: 0.00001792
Iteration 119/1000 | Loss: 0.00001791
Iteration 120/1000 | Loss: 0.00001791
Iteration 121/1000 | Loss: 0.00001791
Iteration 122/1000 | Loss: 0.00001791
Iteration 123/1000 | Loss: 0.00001791
Iteration 124/1000 | Loss: 0.00001791
Iteration 125/1000 | Loss: 0.00001791
Iteration 126/1000 | Loss: 0.00001791
Iteration 127/1000 | Loss: 0.00001790
Iteration 128/1000 | Loss: 0.00001790
Iteration 129/1000 | Loss: 0.00001790
Iteration 130/1000 | Loss: 0.00001790
Iteration 131/1000 | Loss: 0.00001790
Iteration 132/1000 | Loss: 0.00001790
Iteration 133/1000 | Loss: 0.00001790
Iteration 134/1000 | Loss: 0.00001790
Iteration 135/1000 | Loss: 0.00001790
Iteration 136/1000 | Loss: 0.00001790
Iteration 137/1000 | Loss: 0.00001790
Iteration 138/1000 | Loss: 0.00001790
Iteration 139/1000 | Loss: 0.00001790
Iteration 140/1000 | Loss: 0.00001790
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.7901569663081318e-05, 1.7901569663081318e-05, 1.7901569663081318e-05, 1.7901569663081318e-05, 1.7901569663081318e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7901569663081318e-05

Optimization complete. Final v2v error: 3.565077781677246 mm

Highest mean error: 3.758114814758301 mm for frame 19

Lowest mean error: 3.4092235565185547 mm for frame 85

Saving results

Total time: 35.5699360370636
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1090/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1090.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1090
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00351375
Iteration 2/25 | Loss: 0.00131262
Iteration 3/25 | Loss: 0.00119827
Iteration 4/25 | Loss: 0.00118326
Iteration 5/25 | Loss: 0.00117974
Iteration 6/25 | Loss: 0.00117912
Iteration 7/25 | Loss: 0.00117912
Iteration 8/25 | Loss: 0.00117899
Iteration 9/25 | Loss: 0.00117899
Iteration 10/25 | Loss: 0.00117899
Iteration 11/25 | Loss: 0.00117899
Iteration 12/25 | Loss: 0.00117899
Iteration 13/25 | Loss: 0.00117899
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011789901182055473, 0.0011789901182055473, 0.0011789901182055473, 0.0011789901182055473, 0.0011789901182055473]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011789901182055473

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.31348586
Iteration 2/25 | Loss: 0.00116826
Iteration 3/25 | Loss: 0.00116825
Iteration 4/25 | Loss: 0.00116825
Iteration 5/25 | Loss: 0.00116825
Iteration 6/25 | Loss: 0.00116825
Iteration 7/25 | Loss: 0.00116825
Iteration 8/25 | Loss: 0.00116825
Iteration 9/25 | Loss: 0.00116825
Iteration 10/25 | Loss: 0.00116825
Iteration 11/25 | Loss: 0.00116825
Iteration 12/25 | Loss: 0.00116825
Iteration 13/25 | Loss: 0.00116825
Iteration 14/25 | Loss: 0.00116825
Iteration 15/25 | Loss: 0.00116825
Iteration 16/25 | Loss: 0.00116825
Iteration 17/25 | Loss: 0.00116825
Iteration 18/25 | Loss: 0.00116825
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0011682473123073578, 0.0011682473123073578, 0.0011682473123073578, 0.0011682473123073578, 0.0011682473123073578]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011682473123073578

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116825
Iteration 2/1000 | Loss: 0.00002276
Iteration 3/1000 | Loss: 0.00001619
Iteration 4/1000 | Loss: 0.00001486
Iteration 5/1000 | Loss: 0.00001367
Iteration 6/1000 | Loss: 0.00001302
Iteration 7/1000 | Loss: 0.00001256
Iteration 8/1000 | Loss: 0.00001221
Iteration 9/1000 | Loss: 0.00001191
Iteration 10/1000 | Loss: 0.00001163
Iteration 11/1000 | Loss: 0.00001161
Iteration 12/1000 | Loss: 0.00001157
Iteration 13/1000 | Loss: 0.00001155
Iteration 14/1000 | Loss: 0.00001155
Iteration 15/1000 | Loss: 0.00001142
Iteration 16/1000 | Loss: 0.00001136
Iteration 17/1000 | Loss: 0.00001135
Iteration 18/1000 | Loss: 0.00001130
Iteration 19/1000 | Loss: 0.00001126
Iteration 20/1000 | Loss: 0.00001126
Iteration 21/1000 | Loss: 0.00001125
Iteration 22/1000 | Loss: 0.00001124
Iteration 23/1000 | Loss: 0.00001123
Iteration 24/1000 | Loss: 0.00001122
Iteration 25/1000 | Loss: 0.00001119
Iteration 26/1000 | Loss: 0.00001117
Iteration 27/1000 | Loss: 0.00001116
Iteration 28/1000 | Loss: 0.00001114
Iteration 29/1000 | Loss: 0.00001114
Iteration 30/1000 | Loss: 0.00001112
Iteration 31/1000 | Loss: 0.00001111
Iteration 32/1000 | Loss: 0.00001111
Iteration 33/1000 | Loss: 0.00001111
Iteration 34/1000 | Loss: 0.00001110
Iteration 35/1000 | Loss: 0.00001109
Iteration 36/1000 | Loss: 0.00001109
Iteration 37/1000 | Loss: 0.00001108
Iteration 38/1000 | Loss: 0.00001107
Iteration 39/1000 | Loss: 0.00001106
Iteration 40/1000 | Loss: 0.00001106
Iteration 41/1000 | Loss: 0.00001105
Iteration 42/1000 | Loss: 0.00001104
Iteration 43/1000 | Loss: 0.00001102
Iteration 44/1000 | Loss: 0.00001102
Iteration 45/1000 | Loss: 0.00001101
Iteration 46/1000 | Loss: 0.00001098
Iteration 47/1000 | Loss: 0.00001097
Iteration 48/1000 | Loss: 0.00001096
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001093
Iteration 52/1000 | Loss: 0.00001092
Iteration 53/1000 | Loss: 0.00001092
Iteration 54/1000 | Loss: 0.00001092
Iteration 55/1000 | Loss: 0.00001091
Iteration 56/1000 | Loss: 0.00001090
Iteration 57/1000 | Loss: 0.00001090
Iteration 58/1000 | Loss: 0.00001090
Iteration 59/1000 | Loss: 0.00001089
Iteration 60/1000 | Loss: 0.00001089
Iteration 61/1000 | Loss: 0.00001089
Iteration 62/1000 | Loss: 0.00001089
Iteration 63/1000 | Loss: 0.00001088
Iteration 64/1000 | Loss: 0.00001088
Iteration 65/1000 | Loss: 0.00001088
Iteration 66/1000 | Loss: 0.00001087
Iteration 67/1000 | Loss: 0.00001087
Iteration 68/1000 | Loss: 0.00001086
Iteration 69/1000 | Loss: 0.00001086
Iteration 70/1000 | Loss: 0.00001086
Iteration 71/1000 | Loss: 0.00001085
Iteration 72/1000 | Loss: 0.00001085
Iteration 73/1000 | Loss: 0.00001084
Iteration 74/1000 | Loss: 0.00001083
Iteration 75/1000 | Loss: 0.00001083
Iteration 76/1000 | Loss: 0.00001083
Iteration 77/1000 | Loss: 0.00001083
Iteration 78/1000 | Loss: 0.00001083
Iteration 79/1000 | Loss: 0.00001083
Iteration 80/1000 | Loss: 0.00001082
Iteration 81/1000 | Loss: 0.00001082
Iteration 82/1000 | Loss: 0.00001082
Iteration 83/1000 | Loss: 0.00001082
Iteration 84/1000 | Loss: 0.00001082
Iteration 85/1000 | Loss: 0.00001082
Iteration 86/1000 | Loss: 0.00001081
Iteration 87/1000 | Loss: 0.00001081
Iteration 88/1000 | Loss: 0.00001081
Iteration 89/1000 | Loss: 0.00001081
Iteration 90/1000 | Loss: 0.00001081
Iteration 91/1000 | Loss: 0.00001080
Iteration 92/1000 | Loss: 0.00001080
Iteration 93/1000 | Loss: 0.00001080
Iteration 94/1000 | Loss: 0.00001080
Iteration 95/1000 | Loss: 0.00001080
Iteration 96/1000 | Loss: 0.00001080
Iteration 97/1000 | Loss: 0.00001080
Iteration 98/1000 | Loss: 0.00001080
Iteration 99/1000 | Loss: 0.00001080
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.080260244634701e-05, 1.080260244634701e-05, 1.080260244634701e-05, 1.080260244634701e-05, 1.080260244634701e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.080260244634701e-05

Optimization complete. Final v2v error: 2.8226261138916016 mm

Highest mean error: 3.11126971244812 mm for frame 161

Lowest mean error: 2.56223464012146 mm for frame 59

Saving results

Total time: 39.1830837726593
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1072/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1072.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1072
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00756763
Iteration 2/25 | Loss: 0.00167155
Iteration 3/25 | Loss: 0.00130460
Iteration 4/25 | Loss: 0.00123217
Iteration 5/25 | Loss: 0.00121898
Iteration 6/25 | Loss: 0.00122736
Iteration 7/25 | Loss: 0.00122331
Iteration 8/25 | Loss: 0.00120894
Iteration 9/25 | Loss: 0.00120238
Iteration 10/25 | Loss: 0.00120017
Iteration 11/25 | Loss: 0.00119945
Iteration 12/25 | Loss: 0.00119918
Iteration 13/25 | Loss: 0.00119914
Iteration 14/25 | Loss: 0.00119914
Iteration 15/25 | Loss: 0.00119914
Iteration 16/25 | Loss: 0.00119914
Iteration 17/25 | Loss: 0.00119914
Iteration 18/25 | Loss: 0.00119914
Iteration 19/25 | Loss: 0.00119914
Iteration 20/25 | Loss: 0.00119914
Iteration 21/25 | Loss: 0.00119914
Iteration 22/25 | Loss: 0.00119914
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011991368373855948, 0.0011991368373855948, 0.0011991368373855948, 0.0011991368373855948, 0.0011991368373855948]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011991368373855948

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.81073308
Iteration 2/25 | Loss: 0.00113768
Iteration 3/25 | Loss: 0.00113768
Iteration 4/25 | Loss: 0.00113768
Iteration 5/25 | Loss: 0.00113768
Iteration 6/25 | Loss: 0.00113768
Iteration 7/25 | Loss: 0.00113768
Iteration 8/25 | Loss: 0.00113768
Iteration 9/25 | Loss: 0.00113768
Iteration 10/25 | Loss: 0.00113768
Iteration 11/25 | Loss: 0.00113768
Iteration 12/25 | Loss: 0.00113768
Iteration 13/25 | Loss: 0.00113768
Iteration 14/25 | Loss: 0.00113768
Iteration 15/25 | Loss: 0.00113768
Iteration 16/25 | Loss: 0.00113768
Iteration 17/25 | Loss: 0.00113768
Iteration 18/25 | Loss: 0.00113768
Iteration 19/25 | Loss: 0.00113768
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0011376760667189956, 0.0011376760667189956, 0.0011376760667189956, 0.0011376760667189956, 0.0011376760667189956]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011376760667189956

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00113768
Iteration 2/1000 | Loss: 0.00002177
Iteration 3/1000 | Loss: 0.00001820
Iteration 4/1000 | Loss: 0.00001672
Iteration 5/1000 | Loss: 0.00001589
Iteration 6/1000 | Loss: 0.00001525
Iteration 7/1000 | Loss: 0.00001466
Iteration 8/1000 | Loss: 0.00001428
Iteration 9/1000 | Loss: 0.00001400
Iteration 10/1000 | Loss: 0.00001377
Iteration 11/1000 | Loss: 0.00001368
Iteration 12/1000 | Loss: 0.00001365
Iteration 13/1000 | Loss: 0.00001358
Iteration 14/1000 | Loss: 0.00001351
Iteration 15/1000 | Loss: 0.00001346
Iteration 16/1000 | Loss: 0.00001345
Iteration 17/1000 | Loss: 0.00001345
Iteration 18/1000 | Loss: 0.00001345
Iteration 19/1000 | Loss: 0.00001344
Iteration 20/1000 | Loss: 0.00001344
Iteration 21/1000 | Loss: 0.00001341
Iteration 22/1000 | Loss: 0.00001326
Iteration 23/1000 | Loss: 0.00001316
Iteration 24/1000 | Loss: 0.00001314
Iteration 25/1000 | Loss: 0.00001312
Iteration 26/1000 | Loss: 0.00001311
Iteration 27/1000 | Loss: 0.00001311
Iteration 28/1000 | Loss: 0.00001311
Iteration 29/1000 | Loss: 0.00001311
Iteration 30/1000 | Loss: 0.00001311
Iteration 31/1000 | Loss: 0.00001310
Iteration 32/1000 | Loss: 0.00001310
Iteration 33/1000 | Loss: 0.00001310
Iteration 34/1000 | Loss: 0.00001309
Iteration 35/1000 | Loss: 0.00001308
Iteration 36/1000 | Loss: 0.00001307
Iteration 37/1000 | Loss: 0.00001307
Iteration 38/1000 | Loss: 0.00001307
Iteration 39/1000 | Loss: 0.00001306
Iteration 40/1000 | Loss: 0.00001306
Iteration 41/1000 | Loss: 0.00001306
Iteration 42/1000 | Loss: 0.00001305
Iteration 43/1000 | Loss: 0.00001305
Iteration 44/1000 | Loss: 0.00001304
Iteration 45/1000 | Loss: 0.00001304
Iteration 46/1000 | Loss: 0.00001303
Iteration 47/1000 | Loss: 0.00001302
Iteration 48/1000 | Loss: 0.00001302
Iteration 49/1000 | Loss: 0.00001302
Iteration 50/1000 | Loss: 0.00001302
Iteration 51/1000 | Loss: 0.00001301
Iteration 52/1000 | Loss: 0.00001301
Iteration 53/1000 | Loss: 0.00001301
Iteration 54/1000 | Loss: 0.00001300
Iteration 55/1000 | Loss: 0.00001300
Iteration 56/1000 | Loss: 0.00001300
Iteration 57/1000 | Loss: 0.00001299
Iteration 58/1000 | Loss: 0.00001299
Iteration 59/1000 | Loss: 0.00001299
Iteration 60/1000 | Loss: 0.00001299
Iteration 61/1000 | Loss: 0.00001298
Iteration 62/1000 | Loss: 0.00001298
Iteration 63/1000 | Loss: 0.00001298
Iteration 64/1000 | Loss: 0.00001298
Iteration 65/1000 | Loss: 0.00001298
Iteration 66/1000 | Loss: 0.00001298
Iteration 67/1000 | Loss: 0.00001297
Iteration 68/1000 | Loss: 0.00001297
Iteration 69/1000 | Loss: 0.00001297
Iteration 70/1000 | Loss: 0.00001297
Iteration 71/1000 | Loss: 0.00001296
Iteration 72/1000 | Loss: 0.00001296
Iteration 73/1000 | Loss: 0.00001296
Iteration 74/1000 | Loss: 0.00001295
Iteration 75/1000 | Loss: 0.00001295
Iteration 76/1000 | Loss: 0.00001295
Iteration 77/1000 | Loss: 0.00001295
Iteration 78/1000 | Loss: 0.00001295
Iteration 79/1000 | Loss: 0.00001295
Iteration 80/1000 | Loss: 0.00001294
Iteration 81/1000 | Loss: 0.00001294
Iteration 82/1000 | Loss: 0.00001294
Iteration 83/1000 | Loss: 0.00001294
Iteration 84/1000 | Loss: 0.00001294
Iteration 85/1000 | Loss: 0.00001294
Iteration 86/1000 | Loss: 0.00001293
Iteration 87/1000 | Loss: 0.00001292
Iteration 88/1000 | Loss: 0.00001292
Iteration 89/1000 | Loss: 0.00001292
Iteration 90/1000 | Loss: 0.00001292
Iteration 91/1000 | Loss: 0.00001292
Iteration 92/1000 | Loss: 0.00001292
Iteration 93/1000 | Loss: 0.00001292
Iteration 94/1000 | Loss: 0.00001291
Iteration 95/1000 | Loss: 0.00001291
Iteration 96/1000 | Loss: 0.00001291
Iteration 97/1000 | Loss: 0.00001291
Iteration 98/1000 | Loss: 0.00001291
Iteration 99/1000 | Loss: 0.00001291
Iteration 100/1000 | Loss: 0.00001291
Iteration 101/1000 | Loss: 0.00001291
Iteration 102/1000 | Loss: 0.00001291
Iteration 103/1000 | Loss: 0.00001291
Iteration 104/1000 | Loss: 0.00001290
Iteration 105/1000 | Loss: 0.00001290
Iteration 106/1000 | Loss: 0.00001290
Iteration 107/1000 | Loss: 0.00001289
Iteration 108/1000 | Loss: 0.00001289
Iteration 109/1000 | Loss: 0.00001289
Iteration 110/1000 | Loss: 0.00001289
Iteration 111/1000 | Loss: 0.00001288
Iteration 112/1000 | Loss: 0.00001288
Iteration 113/1000 | Loss: 0.00001288
Iteration 114/1000 | Loss: 0.00001288
Iteration 115/1000 | Loss: 0.00001288
Iteration 116/1000 | Loss: 0.00001287
Iteration 117/1000 | Loss: 0.00001287
Iteration 118/1000 | Loss: 0.00001287
Iteration 119/1000 | Loss: 0.00001287
Iteration 120/1000 | Loss: 0.00001287
Iteration 121/1000 | Loss: 0.00001286
Iteration 122/1000 | Loss: 0.00001286
Iteration 123/1000 | Loss: 0.00001286
Iteration 124/1000 | Loss: 0.00001286
Iteration 125/1000 | Loss: 0.00001286
Iteration 126/1000 | Loss: 0.00001286
Iteration 127/1000 | Loss: 0.00001286
Iteration 128/1000 | Loss: 0.00001285
Iteration 129/1000 | Loss: 0.00001285
Iteration 130/1000 | Loss: 0.00001285
Iteration 131/1000 | Loss: 0.00001285
Iteration 132/1000 | Loss: 0.00001285
Iteration 133/1000 | Loss: 0.00001285
Iteration 134/1000 | Loss: 0.00001285
Iteration 135/1000 | Loss: 0.00001285
Iteration 136/1000 | Loss: 0.00001284
Iteration 137/1000 | Loss: 0.00001284
Iteration 138/1000 | Loss: 0.00001284
Iteration 139/1000 | Loss: 0.00001284
Iteration 140/1000 | Loss: 0.00001284
Iteration 141/1000 | Loss: 0.00001284
Iteration 142/1000 | Loss: 0.00001284
Iteration 143/1000 | Loss: 0.00001284
Iteration 144/1000 | Loss: 0.00001284
Iteration 145/1000 | Loss: 0.00001284
Iteration 146/1000 | Loss: 0.00001284
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 146. Stopping optimization.
Last 5 losses: [1.2839346709370147e-05, 1.2839346709370147e-05, 1.2839346709370147e-05, 1.2839346709370147e-05, 1.2839346709370147e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2839346709370147e-05

Optimization complete. Final v2v error: 3.0937142372131348 mm

Highest mean error: 3.3976213932037354 mm for frame 88

Lowest mean error: 2.9062139987945557 mm for frame 38

Saving results

Total time: 52.82255744934082
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1060/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1060.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1060
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00437852
Iteration 2/25 | Loss: 0.00134036
Iteration 3/25 | Loss: 0.00124679
Iteration 4/25 | Loss: 0.00123019
Iteration 5/25 | Loss: 0.00122592
Iteration 6/25 | Loss: 0.00122546
Iteration 7/25 | Loss: 0.00122546
Iteration 8/25 | Loss: 0.00122546
Iteration 9/25 | Loss: 0.00122546
Iteration 10/25 | Loss: 0.00122546
Iteration 11/25 | Loss: 0.00122546
Iteration 12/25 | Loss: 0.00122546
Iteration 13/25 | Loss: 0.00122546
Iteration 14/25 | Loss: 0.00122546
Iteration 15/25 | Loss: 0.00122546
Iteration 16/25 | Loss: 0.00122546
Iteration 17/25 | Loss: 0.00122546
Iteration 18/25 | Loss: 0.00122546
Iteration 19/25 | Loss: 0.00122546
Iteration 20/25 | Loss: 0.00122546
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0012254554312676191, 0.0012254554312676191, 0.0012254554312676191, 0.0012254554312676191, 0.0012254554312676191]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012254554312676191

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.30647993
Iteration 2/25 | Loss: 0.00096937
Iteration 3/25 | Loss: 0.00096937
Iteration 4/25 | Loss: 0.00096937
Iteration 5/25 | Loss: 0.00096937
Iteration 6/25 | Loss: 0.00096937
Iteration 7/25 | Loss: 0.00096936
Iteration 8/25 | Loss: 0.00096936
Iteration 9/25 | Loss: 0.00096936
Iteration 10/25 | Loss: 0.00096936
Iteration 11/25 | Loss: 0.00096936
Iteration 12/25 | Loss: 0.00096936
Iteration 13/25 | Loss: 0.00096936
Iteration 14/25 | Loss: 0.00096936
Iteration 15/25 | Loss: 0.00096936
Iteration 16/25 | Loss: 0.00096936
Iteration 17/25 | Loss: 0.00096936
Iteration 18/25 | Loss: 0.00096936
Iteration 19/25 | Loss: 0.00096936
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0009693634347058833, 0.0009693634347058833, 0.0009693634347058833, 0.0009693634347058833, 0.0009693634347058833]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009693634347058833

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00096936
Iteration 2/1000 | Loss: 0.00002713
Iteration 3/1000 | Loss: 0.00002092
Iteration 4/1000 | Loss: 0.00001878
Iteration 5/1000 | Loss: 0.00001790
Iteration 6/1000 | Loss: 0.00001717
Iteration 7/1000 | Loss: 0.00001669
Iteration 8/1000 | Loss: 0.00001640
Iteration 9/1000 | Loss: 0.00001607
Iteration 10/1000 | Loss: 0.00001569
Iteration 11/1000 | Loss: 0.00001550
Iteration 12/1000 | Loss: 0.00001537
Iteration 13/1000 | Loss: 0.00001519
Iteration 14/1000 | Loss: 0.00001515
Iteration 15/1000 | Loss: 0.00001511
Iteration 16/1000 | Loss: 0.00001499
Iteration 17/1000 | Loss: 0.00001498
Iteration 18/1000 | Loss: 0.00001497
Iteration 19/1000 | Loss: 0.00001496
Iteration 20/1000 | Loss: 0.00001496
Iteration 21/1000 | Loss: 0.00001496
Iteration 22/1000 | Loss: 0.00001496
Iteration 23/1000 | Loss: 0.00001495
Iteration 24/1000 | Loss: 0.00001495
Iteration 25/1000 | Loss: 0.00001494
Iteration 26/1000 | Loss: 0.00001494
Iteration 27/1000 | Loss: 0.00001492
Iteration 28/1000 | Loss: 0.00001492
Iteration 29/1000 | Loss: 0.00001492
Iteration 30/1000 | Loss: 0.00001492
Iteration 31/1000 | Loss: 0.00001491
Iteration 32/1000 | Loss: 0.00001491
Iteration 33/1000 | Loss: 0.00001490
Iteration 34/1000 | Loss: 0.00001489
Iteration 35/1000 | Loss: 0.00001488
Iteration 36/1000 | Loss: 0.00001487
Iteration 37/1000 | Loss: 0.00001487
Iteration 38/1000 | Loss: 0.00001487
Iteration 39/1000 | Loss: 0.00001486
Iteration 40/1000 | Loss: 0.00001486
Iteration 41/1000 | Loss: 0.00001486
Iteration 42/1000 | Loss: 0.00001485
Iteration 43/1000 | Loss: 0.00001484
Iteration 44/1000 | Loss: 0.00001484
Iteration 45/1000 | Loss: 0.00001484
Iteration 46/1000 | Loss: 0.00001483
Iteration 47/1000 | Loss: 0.00001483
Iteration 48/1000 | Loss: 0.00001483
Iteration 49/1000 | Loss: 0.00001483
Iteration 50/1000 | Loss: 0.00001482
Iteration 51/1000 | Loss: 0.00001482
Iteration 52/1000 | Loss: 0.00001482
Iteration 53/1000 | Loss: 0.00001482
Iteration 54/1000 | Loss: 0.00001481
Iteration 55/1000 | Loss: 0.00001481
Iteration 56/1000 | Loss: 0.00001481
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001480
Iteration 60/1000 | Loss: 0.00001480
Iteration 61/1000 | Loss: 0.00001480
Iteration 62/1000 | Loss: 0.00001479
Iteration 63/1000 | Loss: 0.00001479
Iteration 64/1000 | Loss: 0.00001479
Iteration 65/1000 | Loss: 0.00001479
Iteration 66/1000 | Loss: 0.00001479
Iteration 67/1000 | Loss: 0.00001479
Iteration 68/1000 | Loss: 0.00001478
Iteration 69/1000 | Loss: 0.00001478
Iteration 70/1000 | Loss: 0.00001478
Iteration 71/1000 | Loss: 0.00001477
Iteration 72/1000 | Loss: 0.00001477
Iteration 73/1000 | Loss: 0.00001477
Iteration 74/1000 | Loss: 0.00001477
Iteration 75/1000 | Loss: 0.00001477
Iteration 76/1000 | Loss: 0.00001477
Iteration 77/1000 | Loss: 0.00001477
Iteration 78/1000 | Loss: 0.00001476
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001475
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001475
Iteration 84/1000 | Loss: 0.00001474
Iteration 85/1000 | Loss: 0.00001474
Iteration 86/1000 | Loss: 0.00001473
Iteration 87/1000 | Loss: 0.00001471
Iteration 88/1000 | Loss: 0.00001471
Iteration 89/1000 | Loss: 0.00001470
Iteration 90/1000 | Loss: 0.00001470
Iteration 91/1000 | Loss: 0.00001469
Iteration 92/1000 | Loss: 0.00001469
Iteration 93/1000 | Loss: 0.00001467
Iteration 94/1000 | Loss: 0.00001466
Iteration 95/1000 | Loss: 0.00001466
Iteration 96/1000 | Loss: 0.00001462
Iteration 97/1000 | Loss: 0.00001460
Iteration 98/1000 | Loss: 0.00001460
Iteration 99/1000 | Loss: 0.00001460
Iteration 100/1000 | Loss: 0.00001460
Iteration 101/1000 | Loss: 0.00001460
Iteration 102/1000 | Loss: 0.00001460
Iteration 103/1000 | Loss: 0.00001459
Iteration 104/1000 | Loss: 0.00001459
Iteration 105/1000 | Loss: 0.00001458
Iteration 106/1000 | Loss: 0.00001458
Iteration 107/1000 | Loss: 0.00001458
Iteration 108/1000 | Loss: 0.00001457
Iteration 109/1000 | Loss: 0.00001457
Iteration 110/1000 | Loss: 0.00001457
Iteration 111/1000 | Loss: 0.00001457
Iteration 112/1000 | Loss: 0.00001457
Iteration 113/1000 | Loss: 0.00001457
Iteration 114/1000 | Loss: 0.00001457
Iteration 115/1000 | Loss: 0.00001457
Iteration 116/1000 | Loss: 0.00001457
Iteration 117/1000 | Loss: 0.00001457
Iteration 118/1000 | Loss: 0.00001457
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001456
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001456
Iteration 125/1000 | Loss: 0.00001456
Iteration 126/1000 | Loss: 0.00001456
Iteration 127/1000 | Loss: 0.00001455
Iteration 128/1000 | Loss: 0.00001455
Iteration 129/1000 | Loss: 0.00001455
Iteration 130/1000 | Loss: 0.00001455
Iteration 131/1000 | Loss: 0.00001455
Iteration 132/1000 | Loss: 0.00001455
Iteration 133/1000 | Loss: 0.00001455
Iteration 134/1000 | Loss: 0.00001455
Iteration 135/1000 | Loss: 0.00001454
Iteration 136/1000 | Loss: 0.00001454
Iteration 137/1000 | Loss: 0.00001454
Iteration 138/1000 | Loss: 0.00001454
Iteration 139/1000 | Loss: 0.00001454
Iteration 140/1000 | Loss: 0.00001454
Iteration 141/1000 | Loss: 0.00001454
Iteration 142/1000 | Loss: 0.00001454
Iteration 143/1000 | Loss: 0.00001454
Iteration 144/1000 | Loss: 0.00001454
Iteration 145/1000 | Loss: 0.00001453
Iteration 146/1000 | Loss: 0.00001453
Iteration 147/1000 | Loss: 0.00001453
Iteration 148/1000 | Loss: 0.00001453
Iteration 149/1000 | Loss: 0.00001453
Iteration 150/1000 | Loss: 0.00001453
Iteration 151/1000 | Loss: 0.00001452
Iteration 152/1000 | Loss: 0.00001452
Iteration 153/1000 | Loss: 0.00001452
Iteration 154/1000 | Loss: 0.00001452
Iteration 155/1000 | Loss: 0.00001452
Iteration 156/1000 | Loss: 0.00001452
Iteration 157/1000 | Loss: 0.00001452
Iteration 158/1000 | Loss: 0.00001452
Iteration 159/1000 | Loss: 0.00001452
Iteration 160/1000 | Loss: 0.00001452
Iteration 161/1000 | Loss: 0.00001452
Iteration 162/1000 | Loss: 0.00001452
Iteration 163/1000 | Loss: 0.00001452
Iteration 164/1000 | Loss: 0.00001452
Iteration 165/1000 | Loss: 0.00001452
Iteration 166/1000 | Loss: 0.00001452
Iteration 167/1000 | Loss: 0.00001452
Iteration 168/1000 | Loss: 0.00001452
Iteration 169/1000 | Loss: 0.00001452
Iteration 170/1000 | Loss: 0.00001452
Iteration 171/1000 | Loss: 0.00001452
Iteration 172/1000 | Loss: 0.00001452
Iteration 173/1000 | Loss: 0.00001452
Iteration 174/1000 | Loss: 0.00001452
Iteration 175/1000 | Loss: 0.00001452
Iteration 176/1000 | Loss: 0.00001452
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.4521408047585282e-05, 1.4521408047585282e-05, 1.4521408047585282e-05, 1.4521408047585282e-05, 1.4521408047585282e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4521408047585282e-05

Optimization complete. Final v2v error: 3.2568325996398926 mm

Highest mean error: 3.451693534851074 mm for frame 179

Lowest mean error: 2.9298737049102783 mm for frame 191

Saving results

Total time: 47.200337648391724
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01037764
Iteration 2/25 | Loss: 0.00251208
Iteration 3/25 | Loss: 0.00197777
Iteration 4/25 | Loss: 0.00197952
Iteration 5/25 | Loss: 0.00186061
Iteration 6/25 | Loss: 0.00177241
Iteration 7/25 | Loss: 0.00165274
Iteration 8/25 | Loss: 0.00164510
Iteration 9/25 | Loss: 0.00164485
Iteration 10/25 | Loss: 0.00156836
Iteration 11/25 | Loss: 0.00151228
Iteration 12/25 | Loss: 0.00150329
Iteration 13/25 | Loss: 0.00146714
Iteration 14/25 | Loss: 0.00144017
Iteration 15/25 | Loss: 0.00140960
Iteration 16/25 | Loss: 0.00138896
Iteration 17/25 | Loss: 0.00139264
Iteration 18/25 | Loss: 0.00137058
Iteration 19/25 | Loss: 0.00137408
Iteration 20/25 | Loss: 0.00135604
Iteration 21/25 | Loss: 0.00134233
Iteration 22/25 | Loss: 0.00132872
Iteration 23/25 | Loss: 0.00133287
Iteration 24/25 | Loss: 0.00133879
Iteration 25/25 | Loss: 0.00133713

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.35923362
Iteration 2/25 | Loss: 0.00298318
Iteration 3/25 | Loss: 0.00278891
Iteration 4/25 | Loss: 0.00278886
Iteration 5/25 | Loss: 0.00278886
Iteration 6/25 | Loss: 0.00278886
Iteration 7/25 | Loss: 0.00278886
Iteration 8/25 | Loss: 0.00278886
Iteration 9/25 | Loss: 0.00278886
Iteration 10/25 | Loss: 0.00278886
Iteration 11/25 | Loss: 0.00278886
Iteration 12/25 | Loss: 0.00278886
Iteration 13/25 | Loss: 0.00278886
Iteration 14/25 | Loss: 0.00278886
Iteration 15/25 | Loss: 0.00278886
Iteration 16/25 | Loss: 0.00278886
Iteration 17/25 | Loss: 0.00278886
Iteration 18/25 | Loss: 0.00278886
Iteration 19/25 | Loss: 0.00278886
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.002788856392726302, 0.002788856392726302, 0.002788856392726302, 0.002788856392726302, 0.002788856392726302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002788856392726302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00278886
Iteration 2/1000 | Loss: 0.00310554
Iteration 3/1000 | Loss: 0.00368515
Iteration 4/1000 | Loss: 0.00382236
Iteration 5/1000 | Loss: 0.00313370
Iteration 6/1000 | Loss: 0.00324077
Iteration 7/1000 | Loss: 0.00323462
Iteration 8/1000 | Loss: 0.00351066
Iteration 9/1000 | Loss: 0.00369754
Iteration 10/1000 | Loss: 0.00371572
Iteration 11/1000 | Loss: 0.00393789
Iteration 12/1000 | Loss: 0.00397487
Iteration 13/1000 | Loss: 0.00362378
Iteration 14/1000 | Loss: 0.00449112
Iteration 15/1000 | Loss: 0.00376353
Iteration 16/1000 | Loss: 0.00382686
Iteration 17/1000 | Loss: 0.00364470
Iteration 18/1000 | Loss: 0.00349583
Iteration 19/1000 | Loss: 0.00452149
Iteration 20/1000 | Loss: 0.00319385
Iteration 21/1000 | Loss: 0.00347393
Iteration 22/1000 | Loss: 0.00382538
Iteration 23/1000 | Loss: 0.00248404
Iteration 24/1000 | Loss: 0.00415760
Iteration 25/1000 | Loss: 0.00451986
Iteration 26/1000 | Loss: 0.00709604
Iteration 27/1000 | Loss: 0.00378835
Iteration 28/1000 | Loss: 0.00326765
Iteration 29/1000 | Loss: 0.00322509
Iteration 30/1000 | Loss: 0.00299124
Iteration 31/1000 | Loss: 0.00375328
Iteration 32/1000 | Loss: 0.00435097
Iteration 33/1000 | Loss: 0.00383514
Iteration 34/1000 | Loss: 0.00283018
Iteration 35/1000 | Loss: 0.00343686
Iteration 36/1000 | Loss: 0.00356380
Iteration 37/1000 | Loss: 0.00237240
Iteration 38/1000 | Loss: 0.00284057
Iteration 39/1000 | Loss: 0.00281809
Iteration 40/1000 | Loss: 0.00311090
Iteration 41/1000 | Loss: 0.00406219
Iteration 42/1000 | Loss: 0.00370908
Iteration 43/1000 | Loss: 0.00357559
Iteration 44/1000 | Loss: 0.00357170
Iteration 45/1000 | Loss: 0.00264796
Iteration 46/1000 | Loss: 0.00300942
Iteration 47/1000 | Loss: 0.00269302
Iteration 48/1000 | Loss: 0.00203565
Iteration 49/1000 | Loss: 0.00221162
Iteration 50/1000 | Loss: 0.00286241
Iteration 51/1000 | Loss: 0.00297256
Iteration 52/1000 | Loss: 0.00369378
Iteration 53/1000 | Loss: 0.00275534
Iteration 54/1000 | Loss: 0.00332955
Iteration 55/1000 | Loss: 0.00306364
Iteration 56/1000 | Loss: 0.00298751
Iteration 57/1000 | Loss: 0.00338274
Iteration 58/1000 | Loss: 0.00295298
Iteration 59/1000 | Loss: 0.00229981
Iteration 60/1000 | Loss: 0.00293904
Iteration 61/1000 | Loss: 0.00338298
Iteration 62/1000 | Loss: 0.00341954
Iteration 63/1000 | Loss: 0.00302429
Iteration 64/1000 | Loss: 0.00310116
Iteration 65/1000 | Loss: 0.00388962
Iteration 66/1000 | Loss: 0.00376230
Iteration 67/1000 | Loss: 0.00426545
Iteration 68/1000 | Loss: 0.00222957
Iteration 69/1000 | Loss: 0.00336766
Iteration 70/1000 | Loss: 0.00324406
Iteration 71/1000 | Loss: 0.00282906
Iteration 72/1000 | Loss: 0.00347813
Iteration 73/1000 | Loss: 0.00415270
Iteration 74/1000 | Loss: 0.00318185
Iteration 75/1000 | Loss: 0.00353753
Iteration 76/1000 | Loss: 0.00325136
Iteration 77/1000 | Loss: 0.00330005
Iteration 78/1000 | Loss: 0.00320908
Iteration 79/1000 | Loss: 0.00411215
Iteration 80/1000 | Loss: 0.00326070
Iteration 81/1000 | Loss: 0.00338709
Iteration 82/1000 | Loss: 0.00369857
Iteration 83/1000 | Loss: 0.00321329
Iteration 84/1000 | Loss: 0.00321759
Iteration 85/1000 | Loss: 0.00353170
Iteration 86/1000 | Loss: 0.00402889
Iteration 87/1000 | Loss: 0.00340089
Iteration 88/1000 | Loss: 0.00311226
Iteration 89/1000 | Loss: 0.00292555
Iteration 90/1000 | Loss: 0.00252757
Iteration 91/1000 | Loss: 0.00247035
Iteration 92/1000 | Loss: 0.00266523
Iteration 93/1000 | Loss: 0.00386107
Iteration 94/1000 | Loss: 0.00329016
Iteration 95/1000 | Loss: 0.00397546
Iteration 96/1000 | Loss: 0.00355520
Iteration 97/1000 | Loss: 0.00419913
Iteration 98/1000 | Loss: 0.00443657
Iteration 99/1000 | Loss: 0.00283087
Iteration 100/1000 | Loss: 0.00311339
Iteration 101/1000 | Loss: 0.00247695
Iteration 102/1000 | Loss: 0.00264027
Iteration 103/1000 | Loss: 0.00324137
Iteration 104/1000 | Loss: 0.00323578
Iteration 105/1000 | Loss: 0.00276450
Iteration 106/1000 | Loss: 0.00323899
Iteration 107/1000 | Loss: 0.00330032
Iteration 108/1000 | Loss: 0.00315998
Iteration 109/1000 | Loss: 0.00364692
Iteration 110/1000 | Loss: 0.00317203
Iteration 111/1000 | Loss: 0.00365939
Iteration 112/1000 | Loss: 0.00242535
Iteration 113/1000 | Loss: 0.00356332
Iteration 114/1000 | Loss: 0.00327747
Iteration 115/1000 | Loss: 0.00427543
Iteration 116/1000 | Loss: 0.00420545
Iteration 117/1000 | Loss: 0.00592479
Iteration 118/1000 | Loss: 0.00254411
Iteration 119/1000 | Loss: 0.00228163
Iteration 120/1000 | Loss: 0.00329707
Iteration 121/1000 | Loss: 0.00332776
Iteration 122/1000 | Loss: 0.00449437
Iteration 123/1000 | Loss: 0.00313930
Iteration 124/1000 | Loss: 0.00198194
Iteration 125/1000 | Loss: 0.00252730
Iteration 126/1000 | Loss: 0.00265294
Iteration 127/1000 | Loss: 0.00249457
Iteration 128/1000 | Loss: 0.00204651
Iteration 129/1000 | Loss: 0.00282253
Iteration 130/1000 | Loss: 0.00285413
Iteration 131/1000 | Loss: 0.00274942
Iteration 132/1000 | Loss: 0.00336686
Iteration 133/1000 | Loss: 0.00255502
Iteration 134/1000 | Loss: 0.00283057
Iteration 135/1000 | Loss: 0.00291387
Iteration 136/1000 | Loss: 0.00257742
Iteration 137/1000 | Loss: 0.00257219
Iteration 138/1000 | Loss: 0.00196697
Iteration 139/1000 | Loss: 0.00156269
Iteration 140/1000 | Loss: 0.00224923
Iteration 141/1000 | Loss: 0.00239738
Iteration 142/1000 | Loss: 0.00224904
Iteration 143/1000 | Loss: 0.00239510
Iteration 144/1000 | Loss: 0.00247144
Iteration 145/1000 | Loss: 0.00266525
Iteration 146/1000 | Loss: 0.00259562
Iteration 147/1000 | Loss: 0.00251996
Iteration 148/1000 | Loss: 0.00358305
Iteration 149/1000 | Loss: 0.00255025
Iteration 150/1000 | Loss: 0.00495599
Iteration 151/1000 | Loss: 0.00297163
Iteration 152/1000 | Loss: 0.00244721
Iteration 153/1000 | Loss: 0.00230253
Iteration 154/1000 | Loss: 0.00255071
Iteration 155/1000 | Loss: 0.00237925
Iteration 156/1000 | Loss: 0.00247734
Iteration 157/1000 | Loss: 0.00231372
Iteration 158/1000 | Loss: 0.00347149
Iteration 159/1000 | Loss: 0.00231634
Iteration 160/1000 | Loss: 0.00254895
Iteration 161/1000 | Loss: 0.00228167
Iteration 162/1000 | Loss: 0.00249345
Iteration 163/1000 | Loss: 0.00310626
Iteration 164/1000 | Loss: 0.00258811
Iteration 165/1000 | Loss: 0.00248579
Iteration 166/1000 | Loss: 0.00260966
Iteration 167/1000 | Loss: 0.00278956
Iteration 168/1000 | Loss: 0.00276006
Iteration 169/1000 | Loss: 0.00256197
Iteration 170/1000 | Loss: 0.00278166
Iteration 171/1000 | Loss: 0.00270761
Iteration 172/1000 | Loss: 0.00282935
Iteration 173/1000 | Loss: 0.00299364
Iteration 174/1000 | Loss: 0.00300470
Iteration 175/1000 | Loss: 0.00451177
Iteration 176/1000 | Loss: 0.00327408
Iteration 177/1000 | Loss: 0.00261823
Iteration 178/1000 | Loss: 0.00286769
Iteration 179/1000 | Loss: 0.00316782
Iteration 180/1000 | Loss: 0.00273068
Iteration 181/1000 | Loss: 0.00248865
Iteration 182/1000 | Loss: 0.00260548
Iteration 183/1000 | Loss: 0.00290254
Iteration 184/1000 | Loss: 0.00283003
Iteration 185/1000 | Loss: 0.00288558
Iteration 186/1000 | Loss: 0.00257396
Iteration 187/1000 | Loss: 0.00291346
Iteration 188/1000 | Loss: 0.00278739
Iteration 189/1000 | Loss: 0.00281857
Iteration 190/1000 | Loss: 0.00326348
Iteration 191/1000 | Loss: 0.00197030
Iteration 192/1000 | Loss: 0.00196869
Iteration 193/1000 | Loss: 0.00232239
Iteration 194/1000 | Loss: 0.00254845
Iteration 195/1000 | Loss: 0.00285535
Iteration 196/1000 | Loss: 0.00310616
Iteration 197/1000 | Loss: 0.00286804
Iteration 198/1000 | Loss: 0.00363134
Iteration 199/1000 | Loss: 0.00235783
Iteration 200/1000 | Loss: 0.00243786
Iteration 201/1000 | Loss: 0.00272429
Iteration 202/1000 | Loss: 0.00405933
Iteration 203/1000 | Loss: 0.00327970
Iteration 204/1000 | Loss: 0.00191928
Iteration 205/1000 | Loss: 0.00248334
Iteration 206/1000 | Loss: 0.00311967
Iteration 207/1000 | Loss: 0.00291338
Iteration 208/1000 | Loss: 0.00265632
Iteration 209/1000 | Loss: 0.00276776
Iteration 210/1000 | Loss: 0.00321646
Iteration 211/1000 | Loss: 0.00258465
Iteration 212/1000 | Loss: 0.00256143
Iteration 213/1000 | Loss: 0.00272962
Iteration 214/1000 | Loss: 0.00252497
Iteration 215/1000 | Loss: 0.00282223
Iteration 216/1000 | Loss: 0.00296845
Iteration 217/1000 | Loss: 0.00301459
Iteration 218/1000 | Loss: 0.00294418
Iteration 219/1000 | Loss: 0.00265318
Iteration 220/1000 | Loss: 0.00231817
Iteration 221/1000 | Loss: 0.00198353
Iteration 222/1000 | Loss: 0.00172374
Iteration 223/1000 | Loss: 0.00202392
Iteration 224/1000 | Loss: 0.00288945
Iteration 225/1000 | Loss: 0.00308087
Iteration 226/1000 | Loss: 0.00312853
Iteration 227/1000 | Loss: 0.00350677
Iteration 228/1000 | Loss: 0.00295288
Iteration 229/1000 | Loss: 0.00298524
Iteration 230/1000 | Loss: 0.00275446
Iteration 231/1000 | Loss: 0.00280746
Iteration 232/1000 | Loss: 0.00229670
Iteration 233/1000 | Loss: 0.00243611
Iteration 234/1000 | Loss: 0.00335361
Iteration 235/1000 | Loss: 0.00296634
Iteration 236/1000 | Loss: 0.00248339
Iteration 237/1000 | Loss: 0.00236193
Iteration 238/1000 | Loss: 0.00236295
Iteration 239/1000 | Loss: 0.00295904
Iteration 240/1000 | Loss: 0.00366825
Iteration 241/1000 | Loss: 0.00281844
Iteration 242/1000 | Loss: 0.00206343
Iteration 243/1000 | Loss: 0.00230639
Iteration 244/1000 | Loss: 0.00201771
Iteration 245/1000 | Loss: 0.00215233
Iteration 246/1000 | Loss: 0.00237068
Iteration 247/1000 | Loss: 0.00223056
Iteration 248/1000 | Loss: 0.00341670
Iteration 249/1000 | Loss: 0.00248723
Iteration 250/1000 | Loss: 0.00212714
Iteration 251/1000 | Loss: 0.00242118
Iteration 252/1000 | Loss: 0.00195751
Iteration 253/1000 | Loss: 0.00190547
Iteration 254/1000 | Loss: 0.00196236
Iteration 255/1000 | Loss: 0.00210965
Iteration 256/1000 | Loss: 0.00167762
Iteration 257/1000 | Loss: 0.00186995
Iteration 258/1000 | Loss: 0.00177685
Iteration 259/1000 | Loss: 0.00166965
Iteration 260/1000 | Loss: 0.00296496
Iteration 261/1000 | Loss: 0.00235020
Iteration 262/1000 | Loss: 0.00194187
Iteration 263/1000 | Loss: 0.00144500
Iteration 264/1000 | Loss: 0.00160047
Iteration 265/1000 | Loss: 0.00127871
Iteration 266/1000 | Loss: 0.00150590
Iteration 267/1000 | Loss: 0.00158116
Iteration 268/1000 | Loss: 0.00149151
Iteration 269/1000 | Loss: 0.00196773
Iteration 270/1000 | Loss: 0.00186241
Iteration 271/1000 | Loss: 0.00188978
Iteration 272/1000 | Loss: 0.00149231
Iteration 273/1000 | Loss: 0.00160866
Iteration 274/1000 | Loss: 0.00180339
Iteration 275/1000 | Loss: 0.00174792
Iteration 276/1000 | Loss: 0.00150107
Iteration 277/1000 | Loss: 0.00171698
Iteration 278/1000 | Loss: 0.00193768
Iteration 279/1000 | Loss: 0.00203243
Iteration 280/1000 | Loss: 0.00260239
Iteration 281/1000 | Loss: 0.00349522
Iteration 282/1000 | Loss: 0.00204433
Iteration 283/1000 | Loss: 0.00194459
Iteration 284/1000 | Loss: 0.00182328
Iteration 285/1000 | Loss: 0.00212600
Iteration 286/1000 | Loss: 0.00116976
Iteration 287/1000 | Loss: 0.00146933
Iteration 288/1000 | Loss: 0.00141263
Iteration 289/1000 | Loss: 0.00160763
Iteration 290/1000 | Loss: 0.00197026
Iteration 291/1000 | Loss: 0.00151696
Iteration 292/1000 | Loss: 0.00168367
Iteration 293/1000 | Loss: 0.00205402
Iteration 294/1000 | Loss: 0.00210946
Iteration 295/1000 | Loss: 0.00153057
Iteration 296/1000 | Loss: 0.00219043
Iteration 297/1000 | Loss: 0.00195737
Iteration 298/1000 | Loss: 0.00182878
Iteration 299/1000 | Loss: 0.00223472
Iteration 300/1000 | Loss: 0.00300375
Iteration 301/1000 | Loss: 0.00261572
Iteration 302/1000 | Loss: 0.00162751
Iteration 303/1000 | Loss: 0.00147699
Iteration 304/1000 | Loss: 0.00192483
Iteration 305/1000 | Loss: 0.00152309
Iteration 306/1000 | Loss: 0.00187486
Iteration 307/1000 | Loss: 0.00194952
Iteration 308/1000 | Loss: 0.00228798
Iteration 309/1000 | Loss: 0.00191659
Iteration 310/1000 | Loss: 0.00262997
Iteration 311/1000 | Loss: 0.00366089
Iteration 312/1000 | Loss: 0.00252128
Iteration 313/1000 | Loss: 0.00184525
Iteration 314/1000 | Loss: 0.00191588
Iteration 315/1000 | Loss: 0.00221366
Iteration 316/1000 | Loss: 0.00197370
Iteration 317/1000 | Loss: 0.00204263
Iteration 318/1000 | Loss: 0.00178385
Iteration 319/1000 | Loss: 0.00187826
Iteration 320/1000 | Loss: 0.00277483
Iteration 321/1000 | Loss: 0.00197834
Iteration 322/1000 | Loss: 0.00194445
Iteration 323/1000 | Loss: 0.00178636
Iteration 324/1000 | Loss: 0.00191302
Iteration 325/1000 | Loss: 0.00189925
Iteration 326/1000 | Loss: 0.00189548
Iteration 327/1000 | Loss: 0.00156056
Iteration 328/1000 | Loss: 0.00185289
Iteration 329/1000 | Loss: 0.00208558
Iteration 330/1000 | Loss: 0.00178157
Iteration 331/1000 | Loss: 0.00195155
Iteration 332/1000 | Loss: 0.00195311
Iteration 333/1000 | Loss: 0.00141033
Iteration 334/1000 | Loss: 0.00147946
Iteration 335/1000 | Loss: 0.00186300
Iteration 336/1000 | Loss: 0.00184161
Iteration 337/1000 | Loss: 0.00223302
Iteration 338/1000 | Loss: 0.00174986
Iteration 339/1000 | Loss: 0.00181325
Iteration 340/1000 | Loss: 0.00125924
Iteration 341/1000 | Loss: 0.00158197
Iteration 342/1000 | Loss: 0.00173944
Iteration 343/1000 | Loss: 0.00146958
Iteration 344/1000 | Loss: 0.00160774
Iteration 345/1000 | Loss: 0.00211347
Iteration 346/1000 | Loss: 0.00145191
Iteration 347/1000 | Loss: 0.00142867
Iteration 348/1000 | Loss: 0.00168009
Iteration 349/1000 | Loss: 0.00170126
Iteration 350/1000 | Loss: 0.00167027
Iteration 351/1000 | Loss: 0.00182025
Iteration 352/1000 | Loss: 0.00183849
Iteration 353/1000 | Loss: 0.00173134
Iteration 354/1000 | Loss: 0.00134939
Iteration 355/1000 | Loss: 0.00199272
Iteration 356/1000 | Loss: 0.00209985
Iteration 357/1000 | Loss: 0.00207934
Iteration 358/1000 | Loss: 0.00189924
Iteration 359/1000 | Loss: 0.00196812
Iteration 360/1000 | Loss: 0.00193973
Iteration 361/1000 | Loss: 0.00171270
Iteration 362/1000 | Loss: 0.00162818
Iteration 363/1000 | Loss: 0.00157303
Iteration 364/1000 | Loss: 0.00169168
Iteration 365/1000 | Loss: 0.00161858
Iteration 366/1000 | Loss: 0.00163530
Iteration 367/1000 | Loss: 0.00129331
Iteration 368/1000 | Loss: 0.00159255
Iteration 369/1000 | Loss: 0.00228670
Iteration 370/1000 | Loss: 0.00137016
Iteration 371/1000 | Loss: 0.00167717
Iteration 372/1000 | Loss: 0.00145458
Iteration 373/1000 | Loss: 0.00170300
Iteration 374/1000 | Loss: 0.00154426
Iteration 375/1000 | Loss: 0.00141074
Iteration 376/1000 | Loss: 0.00129169
Iteration 377/1000 | Loss: 0.00129908
Iteration 378/1000 | Loss: 0.00103265
Iteration 379/1000 | Loss: 0.00125452
Iteration 380/1000 | Loss: 0.00143544
Iteration 381/1000 | Loss: 0.00151993
Iteration 382/1000 | Loss: 0.00150913
Iteration 383/1000 | Loss: 0.00154694
Iteration 384/1000 | Loss: 0.00187555
Iteration 385/1000 | Loss: 0.00157262
Iteration 386/1000 | Loss: 0.00180810
Iteration 387/1000 | Loss: 0.00168946
Iteration 388/1000 | Loss: 0.00165887
Iteration 389/1000 | Loss: 0.00160889
Iteration 390/1000 | Loss: 0.00252491
Iteration 391/1000 | Loss: 0.00171785
Iteration 392/1000 | Loss: 0.00189141
Iteration 393/1000 | Loss: 0.00168652
Iteration 394/1000 | Loss: 0.00187412
Iteration 395/1000 | Loss: 0.00218298
Iteration 396/1000 | Loss: 0.00211369
Iteration 397/1000 | Loss: 0.00191443
Iteration 398/1000 | Loss: 0.00270439
Iteration 399/1000 | Loss: 0.00221355
Iteration 400/1000 | Loss: 0.00249556
Iteration 401/1000 | Loss: 0.00195397
Iteration 402/1000 | Loss: 0.00163417
Iteration 403/1000 | Loss: 0.00144289
Iteration 404/1000 | Loss: 0.00226287
Iteration 405/1000 | Loss: 0.00185163
Iteration 406/1000 | Loss: 0.00149501
Iteration 407/1000 | Loss: 0.00111937
Iteration 408/1000 | Loss: 0.00104414
Iteration 409/1000 | Loss: 0.00157184
Iteration 410/1000 | Loss: 0.00142357
Iteration 411/1000 | Loss: 0.00212737
Iteration 412/1000 | Loss: 0.00208337
Iteration 413/1000 | Loss: 0.00150096
Iteration 414/1000 | Loss: 0.00116721
Iteration 415/1000 | Loss: 0.00125491
Iteration 416/1000 | Loss: 0.00140651
Iteration 417/1000 | Loss: 0.00100222
Iteration 418/1000 | Loss: 0.00066035
Iteration 419/1000 | Loss: 0.00132454
Iteration 420/1000 | Loss: 0.00124811
Iteration 421/1000 | Loss: 0.00144116
Iteration 422/1000 | Loss: 0.00119292
Iteration 423/1000 | Loss: 0.00085223
Iteration 424/1000 | Loss: 0.00089913
Iteration 425/1000 | Loss: 0.00100872
Iteration 426/1000 | Loss: 0.00128016
Iteration 427/1000 | Loss: 0.00096474
Iteration 428/1000 | Loss: 0.00110896
Iteration 429/1000 | Loss: 0.00116652
Iteration 430/1000 | Loss: 0.00110634
Iteration 431/1000 | Loss: 0.00113956
Iteration 432/1000 | Loss: 0.00089351
Iteration 433/1000 | Loss: 0.00157703
Iteration 434/1000 | Loss: 0.00174815
Iteration 435/1000 | Loss: 0.00159001
Iteration 436/1000 | Loss: 0.00109840
Iteration 437/1000 | Loss: 0.00110444
Iteration 438/1000 | Loss: 0.00114141
Iteration 439/1000 | Loss: 0.00143385
Iteration 440/1000 | Loss: 0.00134923
Iteration 441/1000 | Loss: 0.00163161
Iteration 442/1000 | Loss: 0.00162143
Iteration 443/1000 | Loss: 0.00192820
Iteration 444/1000 | Loss: 0.00147102
Iteration 445/1000 | Loss: 0.00195148
Iteration 446/1000 | Loss: 0.00186516
Iteration 447/1000 | Loss: 0.00150010
Iteration 448/1000 | Loss: 0.00133865
Iteration 449/1000 | Loss: 0.00128403
Iteration 450/1000 | Loss: 0.00208425
Iteration 451/1000 | Loss: 0.00155243
Iteration 452/1000 | Loss: 0.00174979
Iteration 453/1000 | Loss: 0.00146106
Iteration 454/1000 | Loss: 0.00181213
Iteration 455/1000 | Loss: 0.00166545
Iteration 456/1000 | Loss: 0.00122903
Iteration 457/1000 | Loss: 0.00172248
Iteration 458/1000 | Loss: 0.00105512
Iteration 459/1000 | Loss: 0.00162775
Iteration 460/1000 | Loss: 0.00135148
Iteration 461/1000 | Loss: 0.00111025
Iteration 462/1000 | Loss: 0.00121931
Iteration 463/1000 | Loss: 0.00114396
Iteration 464/1000 | Loss: 0.00122604
Iteration 465/1000 | Loss: 0.00125773
Iteration 466/1000 | Loss: 0.00120627
Iteration 467/1000 | Loss: 0.00124262
Iteration 468/1000 | Loss: 0.00124290
Iteration 469/1000 | Loss: 0.00129326
Iteration 470/1000 | Loss: 0.00119897
Iteration 471/1000 | Loss: 0.00131722
Iteration 472/1000 | Loss: 0.00125641
Iteration 473/1000 | Loss: 0.00122264
Iteration 474/1000 | Loss: 0.00117261
Iteration 475/1000 | Loss: 0.00121958
Iteration 476/1000 | Loss: 0.00135531
Iteration 477/1000 | Loss: 0.00141879
Iteration 478/1000 | Loss: 0.00138446
Iteration 479/1000 | Loss: 0.00163332
Iteration 480/1000 | Loss: 0.00113689
Iteration 481/1000 | Loss: 0.00125409
Iteration 482/1000 | Loss: 0.00140030
Iteration 483/1000 | Loss: 0.00156176
Iteration 484/1000 | Loss: 0.00167859
Iteration 485/1000 | Loss: 0.00206423
Iteration 486/1000 | Loss: 0.00117578
Iteration 487/1000 | Loss: 0.00156922
Iteration 488/1000 | Loss: 0.00143997
Iteration 489/1000 | Loss: 0.00125151
Iteration 490/1000 | Loss: 0.00120828
Iteration 491/1000 | Loss: 0.00126846
Iteration 492/1000 | Loss: 0.00111441
Iteration 493/1000 | Loss: 0.00119667
Iteration 494/1000 | Loss: 0.00125691
Iteration 495/1000 | Loss: 0.00127020
Iteration 496/1000 | Loss: 0.00124128
Iteration 497/1000 | Loss: 0.00128113
Iteration 498/1000 | Loss: 0.00161324
Iteration 499/1000 | Loss: 0.00178976
Iteration 500/1000 | Loss: 0.00158800
Iteration 501/1000 | Loss: 0.00122709
Iteration 502/1000 | Loss: 0.00125660
Iteration 503/1000 | Loss: 0.00130370
Iteration 504/1000 | Loss: 0.00089519
Iteration 505/1000 | Loss: 0.00098593
Iteration 506/1000 | Loss: 0.00122911
Iteration 507/1000 | Loss: 0.00161319
Iteration 508/1000 | Loss: 0.00132965
Iteration 509/1000 | Loss: 0.00097366
Iteration 510/1000 | Loss: 0.00095845
Iteration 511/1000 | Loss: 0.00094107
Iteration 512/1000 | Loss: 0.00083200
Iteration 513/1000 | Loss: 0.00100639
Iteration 514/1000 | Loss: 0.00090398
Iteration 515/1000 | Loss: 0.00138451
Iteration 516/1000 | Loss: 0.00151042
Iteration 517/1000 | Loss: 0.00236861
Iteration 518/1000 | Loss: 0.00194456
Iteration 519/1000 | Loss: 0.00253938
Iteration 520/1000 | Loss: 0.00173214
Iteration 521/1000 | Loss: 0.00197658
Iteration 522/1000 | Loss: 0.00164907
Iteration 523/1000 | Loss: 0.00229249
Iteration 524/1000 | Loss: 0.00197154
Iteration 525/1000 | Loss: 0.00166385
Iteration 526/1000 | Loss: 0.00152808
Iteration 527/1000 | Loss: 0.00105805
Iteration 528/1000 | Loss: 0.00141483
Iteration 529/1000 | Loss: 0.00153914
Iteration 530/1000 | Loss: 0.00113506
Iteration 531/1000 | Loss: 0.00153155
Iteration 532/1000 | Loss: 0.00119186
Iteration 533/1000 | Loss: 0.00095556
Iteration 534/1000 | Loss: 0.00068998
Iteration 535/1000 | Loss: 0.00063063
Iteration 536/1000 | Loss: 0.00067673
Iteration 537/1000 | Loss: 0.00081511
Iteration 538/1000 | Loss: 0.00127578
Iteration 539/1000 | Loss: 0.00154938
Iteration 540/1000 | Loss: 0.00120599
Iteration 541/1000 | Loss: 0.00150364
Iteration 542/1000 | Loss: 0.00084606
Iteration 543/1000 | Loss: 0.00082797
Iteration 544/1000 | Loss: 0.00074057
Iteration 545/1000 | Loss: 0.00076309
Iteration 546/1000 | Loss: 0.00072979
Iteration 547/1000 | Loss: 0.00065810
Iteration 548/1000 | Loss: 0.00066734
Iteration 549/1000 | Loss: 0.00064256
Iteration 550/1000 | Loss: 0.00090886
Iteration 551/1000 | Loss: 0.00074941
Iteration 552/1000 | Loss: 0.00093506
Iteration 553/1000 | Loss: 0.00070031
Iteration 554/1000 | Loss: 0.00069730
Iteration 555/1000 | Loss: 0.00080292
Iteration 556/1000 | Loss: 0.00072607
Iteration 557/1000 | Loss: 0.00081747
Iteration 558/1000 | Loss: 0.00076305
Iteration 559/1000 | Loss: 0.00131001
Iteration 560/1000 | Loss: 0.00134450
Iteration 561/1000 | Loss: 0.00190349
Iteration 562/1000 | Loss: 0.00232378
Iteration 563/1000 | Loss: 0.00091823
Iteration 564/1000 | Loss: 0.00105167
Iteration 565/1000 | Loss: 0.00087256
Iteration 566/1000 | Loss: 0.00082342
Iteration 567/1000 | Loss: 0.00079097
Iteration 568/1000 | Loss: 0.00078327
Iteration 569/1000 | Loss: 0.00075385
Iteration 570/1000 | Loss: 0.00087495
Iteration 571/1000 | Loss: 0.00082978
Iteration 572/1000 | Loss: 0.00079212
Iteration 573/1000 | Loss: 0.00079751
Iteration 574/1000 | Loss: 0.00083235
Iteration 575/1000 | Loss: 0.00073857
Iteration 576/1000 | Loss: 0.00073685
Iteration 577/1000 | Loss: 0.00073359
Iteration 578/1000 | Loss: 0.00085469
Iteration 579/1000 | Loss: 0.00088717
Iteration 580/1000 | Loss: 0.00074060
Iteration 581/1000 | Loss: 0.00089696
Iteration 582/1000 | Loss: 0.00078287
Iteration 583/1000 | Loss: 0.00095498
Iteration 584/1000 | Loss: 0.00069650
Iteration 585/1000 | Loss: 0.00087926
Iteration 586/1000 | Loss: 0.00088751
Iteration 587/1000 | Loss: 0.00106276
Iteration 588/1000 | Loss: 0.00095594
Iteration 589/1000 | Loss: 0.00092658
Iteration 590/1000 | Loss: 0.00085490
Iteration 591/1000 | Loss: 0.00101132
Iteration 592/1000 | Loss: 0.00108243
Iteration 593/1000 | Loss: 0.00082350
Iteration 594/1000 | Loss: 0.00079447
Iteration 595/1000 | Loss: 0.00067181
Iteration 596/1000 | Loss: 0.00046434
Iteration 597/1000 | Loss: 0.00069911
Iteration 598/1000 | Loss: 0.00065750
Iteration 599/1000 | Loss: 0.00121472
Iteration 600/1000 | Loss: 0.00101934
Iteration 601/1000 | Loss: 0.00077400
Iteration 602/1000 | Loss: 0.00080171
Iteration 603/1000 | Loss: 0.00087806
Iteration 604/1000 | Loss: 0.00092117
Iteration 605/1000 | Loss: 0.00038320
Iteration 606/1000 | Loss: 0.00080953
Iteration 607/1000 | Loss: 0.00077459
Iteration 608/1000 | Loss: 0.00056956
Iteration 609/1000 | Loss: 0.00056983
Iteration 610/1000 | Loss: 0.00063475
Iteration 611/1000 | Loss: 0.00065275
Iteration 612/1000 | Loss: 0.00067518
Iteration 613/1000 | Loss: 0.00056605
Iteration 614/1000 | Loss: 0.00064941
Iteration 615/1000 | Loss: 0.00077283
Iteration 616/1000 | Loss: 0.00070593
Iteration 617/1000 | Loss: 0.00095961
Iteration 618/1000 | Loss: 0.00069846
Iteration 619/1000 | Loss: 0.00062978
Iteration 620/1000 | Loss: 0.00064726
Iteration 621/1000 | Loss: 0.00079624
Iteration 622/1000 | Loss: 0.00067326
Iteration 623/1000 | Loss: 0.00061055
Iteration 624/1000 | Loss: 0.00059366
Iteration 625/1000 | Loss: 0.00067030
Iteration 626/1000 | Loss: 0.00059403
Iteration 627/1000 | Loss: 0.00037388
Iteration 628/1000 | Loss: 0.00038811
Iteration 629/1000 | Loss: 0.00071473
Iteration 630/1000 | Loss: 0.00127391
Iteration 631/1000 | Loss: 0.00052432
Iteration 632/1000 | Loss: 0.00124944
Iteration 633/1000 | Loss: 0.00080902
Iteration 634/1000 | Loss: 0.00069413
Iteration 635/1000 | Loss: 0.00049152
Iteration 636/1000 | Loss: 0.00038433
Iteration 637/1000 | Loss: 0.00044204
Iteration 638/1000 | Loss: 0.00067839
Iteration 639/1000 | Loss: 0.00038574
Iteration 640/1000 | Loss: 0.00043887
Iteration 641/1000 | Loss: 0.00064478
Iteration 642/1000 | Loss: 0.00039320
Iteration 643/1000 | Loss: 0.00048220
Iteration 644/1000 | Loss: 0.00026343
Iteration 645/1000 | Loss: 0.00018844
Iteration 646/1000 | Loss: 0.00016234
Iteration 647/1000 | Loss: 0.00024287
Iteration 648/1000 | Loss: 0.00032895
Iteration 649/1000 | Loss: 0.00035642
Iteration 650/1000 | Loss: 0.00033645
Iteration 651/1000 | Loss: 0.00048605
Iteration 652/1000 | Loss: 0.00045189
Iteration 653/1000 | Loss: 0.00050458
Iteration 654/1000 | Loss: 0.00060555
Iteration 655/1000 | Loss: 0.00023504
Iteration 656/1000 | Loss: 0.00009942
Iteration 657/1000 | Loss: 0.00056683
Iteration 658/1000 | Loss: 0.00043847
Iteration 659/1000 | Loss: 0.00070607
Iteration 660/1000 | Loss: 0.00072355
Iteration 661/1000 | Loss: 0.00045108
Iteration 662/1000 | Loss: 0.00059448
Iteration 663/1000 | Loss: 0.00080548
Iteration 664/1000 | Loss: 0.00028577
Iteration 665/1000 | Loss: 0.00011822
Iteration 666/1000 | Loss: 0.00019561
Iteration 667/1000 | Loss: 0.00027144
Iteration 668/1000 | Loss: 0.00078229
Iteration 669/1000 | Loss: 0.00023026
Iteration 670/1000 | Loss: 0.00025303
Iteration 671/1000 | Loss: 0.00030358
Iteration 672/1000 | Loss: 0.00029139
Iteration 673/1000 | Loss: 0.00017265
Iteration 674/1000 | Loss: 0.00020791
Iteration 675/1000 | Loss: 0.00017625
Iteration 676/1000 | Loss: 0.00012356
Iteration 677/1000 | Loss: 0.00016267
Iteration 678/1000 | Loss: 0.00018390
Iteration 679/1000 | Loss: 0.00018416
Iteration 680/1000 | Loss: 0.00014732
Iteration 681/1000 | Loss: 0.00015275
Iteration 682/1000 | Loss: 0.00017314
Iteration 683/1000 | Loss: 0.00013021
Iteration 684/1000 | Loss: 0.00015300
Iteration 685/1000 | Loss: 0.00022599
Iteration 686/1000 | Loss: 0.00018012
Iteration 687/1000 | Loss: 0.00014210
Iteration 688/1000 | Loss: 0.00020199
Iteration 689/1000 | Loss: 0.00019152
Iteration 690/1000 | Loss: 0.00014491
Iteration 691/1000 | Loss: 0.00014252
Iteration 692/1000 | Loss: 0.00017653
Iteration 693/1000 | Loss: 0.00005308
Iteration 694/1000 | Loss: 0.00010259
Iteration 695/1000 | Loss: 0.00006462
Iteration 696/1000 | Loss: 0.00014897
Iteration 697/1000 | Loss: 0.00012980
Iteration 698/1000 | Loss: 0.00014459
Iteration 699/1000 | Loss: 0.00014839
Iteration 700/1000 | Loss: 0.00016699
Iteration 701/1000 | Loss: 0.00010400
Iteration 702/1000 | Loss: 0.00013690
Iteration 703/1000 | Loss: 0.00006844
Iteration 704/1000 | Loss: 0.00005599
Iteration 705/1000 | Loss: 0.00005785
Iteration 706/1000 | Loss: 0.00009407
Iteration 707/1000 | Loss: 0.00007164
Iteration 708/1000 | Loss: 0.00005674
Iteration 709/1000 | Loss: 0.00006265
Iteration 710/1000 | Loss: 0.00006593
Iteration 711/1000 | Loss: 0.00008433
Iteration 712/1000 | Loss: 0.00008558
Iteration 713/1000 | Loss: 0.00005937
Iteration 714/1000 | Loss: 0.00011503
Iteration 715/1000 | Loss: 0.00006267
Iteration 716/1000 | Loss: 0.00015749
Iteration 717/1000 | Loss: 0.00005172
Iteration 718/1000 | Loss: 0.00006638
Iteration 719/1000 | Loss: 0.00006284
Iteration 720/1000 | Loss: 0.00006037
Iteration 721/1000 | Loss: 0.00006434
Iteration 722/1000 | Loss: 0.00007261
Iteration 723/1000 | Loss: 0.00007081
Iteration 724/1000 | Loss: 0.00007866
Iteration 725/1000 | Loss: 0.00007233
Iteration 726/1000 | Loss: 0.00004982
Iteration 727/1000 | Loss: 0.00004520
Iteration 728/1000 | Loss: 0.00006633
Iteration 729/1000 | Loss: 0.00011717
Iteration 730/1000 | Loss: 0.00007001
Iteration 731/1000 | Loss: 0.00006581
Iteration 732/1000 | Loss: 0.00006199
Iteration 733/1000 | Loss: 0.00005817
Iteration 734/1000 | Loss: 0.00005589
Iteration 735/1000 | Loss: 0.00005394
Iteration 736/1000 | Loss: 0.00005560
Iteration 737/1000 | Loss: 0.00007334
Iteration 738/1000 | Loss: 0.00005343
Iteration 739/1000 | Loss: 0.00005411
Iteration 740/1000 | Loss: 0.00005555
Iteration 741/1000 | Loss: 0.00005438
Iteration 742/1000 | Loss: 0.00005758
Iteration 743/1000 | Loss: 0.00005418
Iteration 744/1000 | Loss: 0.00006005
Iteration 745/1000 | Loss: 0.00005872
Iteration 746/1000 | Loss: 0.00005547
Iteration 747/1000 | Loss: 0.00008083
Iteration 748/1000 | Loss: 0.00005714
Iteration 749/1000 | Loss: 0.00006478
Iteration 750/1000 | Loss: 0.00007918
Iteration 751/1000 | Loss: 0.00010619
Iteration 752/1000 | Loss: 0.00005519
Iteration 753/1000 | Loss: 0.00005276
Iteration 754/1000 | Loss: 0.00005273
Iteration 755/1000 | Loss: 0.00010791
Iteration 756/1000 | Loss: 0.00007848
Iteration 757/1000 | Loss: 0.00006530
Iteration 758/1000 | Loss: 0.00006174
Iteration 759/1000 | Loss: 0.00005018
Iteration 760/1000 | Loss: 0.00009738
Iteration 761/1000 | Loss: 0.00006158
Iteration 762/1000 | Loss: 0.00007563
Iteration 763/1000 | Loss: 0.00008253
Iteration 764/1000 | Loss: 0.00013068
Iteration 765/1000 | Loss: 0.00008025
Iteration 766/1000 | Loss: 0.00008936
Iteration 767/1000 | Loss: 0.00003311
Iteration 768/1000 | Loss: 0.00006287
Iteration 769/1000 | Loss: 0.00015102
Iteration 770/1000 | Loss: 0.00007895
Iteration 771/1000 | Loss: 0.00006118
Iteration 772/1000 | Loss: 0.00005963
Iteration 773/1000 | Loss: 0.00016807
Iteration 774/1000 | Loss: 0.00008349
Iteration 775/1000 | Loss: 0.00006200
Iteration 776/1000 | Loss: 0.00008814
Iteration 777/1000 | Loss: 0.00006136
Iteration 778/1000 | Loss: 0.00025513
Iteration 779/1000 | Loss: 0.00006557
Iteration 780/1000 | Loss: 0.00008076
Iteration 781/1000 | Loss: 0.00008663
Iteration 782/1000 | Loss: 0.00012717
Iteration 783/1000 | Loss: 0.00006838
Iteration 784/1000 | Loss: 0.00006189
Iteration 785/1000 | Loss: 0.00005960
Iteration 786/1000 | Loss: 0.00006243
Iteration 787/1000 | Loss: 0.00006660
Iteration 788/1000 | Loss: 0.00005559
Iteration 789/1000 | Loss: 0.00005972
Iteration 790/1000 | Loss: 0.00016225
Iteration 791/1000 | Loss: 0.00012057
Iteration 792/1000 | Loss: 0.00019622
Iteration 793/1000 | Loss: 0.00020733
Iteration 794/1000 | Loss: 0.00031294
Iteration 795/1000 | Loss: 0.00020041
Iteration 796/1000 | Loss: 0.00017798
Iteration 797/1000 | Loss: 0.00015295
Iteration 798/1000 | Loss: 0.00012008
Iteration 799/1000 | Loss: 0.00009149
Iteration 800/1000 | Loss: 0.00010102
Iteration 801/1000 | Loss: 0.00007610
Iteration 802/1000 | Loss: 0.00006770
Iteration 803/1000 | Loss: 0.00005550
Iteration 804/1000 | Loss: 0.00008111
Iteration 805/1000 | Loss: 0.00005982
Iteration 806/1000 | Loss: 0.00005570
Iteration 807/1000 | Loss: 0.00005464
Iteration 808/1000 | Loss: 0.00005439
Iteration 809/1000 | Loss: 0.00005474
Iteration 810/1000 | Loss: 0.00005728
Iteration 811/1000 | Loss: 0.00006067
Iteration 812/1000 | Loss: 0.00004777
Iteration 813/1000 | Loss: 0.00005952
Iteration 814/1000 | Loss: 0.00006571
Iteration 815/1000 | Loss: 0.00005774
Iteration 816/1000 | Loss: 0.00009640
Iteration 817/1000 | Loss: 0.00005817
Iteration 818/1000 | Loss: 0.00007586
Iteration 819/1000 | Loss: 0.00008768
Iteration 820/1000 | Loss: 0.00005292
Iteration 821/1000 | Loss: 0.00004865
Iteration 822/1000 | Loss: 0.00007596
Iteration 823/1000 | Loss: 0.00005914
Iteration 824/1000 | Loss: 0.00005756
Iteration 825/1000 | Loss: 0.00005914
Iteration 826/1000 | Loss: 0.00005806
Iteration 827/1000 | Loss: 0.00005925
Iteration 828/1000 | Loss: 0.00006673
Iteration 829/1000 | Loss: 0.00011619
Iteration 830/1000 | Loss: 0.00005728
Iteration 831/1000 | Loss: 0.00005993
Iteration 832/1000 | Loss: 0.00005859
Iteration 833/1000 | Loss: 0.00006530
Iteration 834/1000 | Loss: 0.00006201
Iteration 835/1000 | Loss: 0.00007025
Iteration 836/1000 | Loss: 0.00006367
Iteration 837/1000 | Loss: 0.00005928
Iteration 838/1000 | Loss: 0.00007204
Iteration 839/1000 | Loss: 0.00005758
Iteration 840/1000 | Loss: 0.00006000
Iteration 841/1000 | Loss: 0.00005987
Iteration 842/1000 | Loss: 0.00005745
Iteration 843/1000 | Loss: 0.00007096
Iteration 844/1000 | Loss: 0.00005856
Iteration 845/1000 | Loss: 0.00005955
Iteration 846/1000 | Loss: 0.00010219
Iteration 847/1000 | Loss: 0.00005022
Iteration 848/1000 | Loss: 0.00010173
Iteration 849/1000 | Loss: 0.00012448
Iteration 850/1000 | Loss: 0.00006098
Iteration 851/1000 | Loss: 0.00006175
Iteration 852/1000 | Loss: 0.00005810
Iteration 853/1000 | Loss: 0.00006378
Iteration 854/1000 | Loss: 0.00005702
Iteration 855/1000 | Loss: 0.00006137
Iteration 856/1000 | Loss: 0.00005063
Iteration 857/1000 | Loss: 0.00005619
Iteration 858/1000 | Loss: 0.00005725
Iteration 859/1000 | Loss: 0.00004875
Iteration 860/1000 | Loss: 0.00005431
Iteration 861/1000 | Loss: 0.00006072
Iteration 862/1000 | Loss: 0.00005105
Iteration 863/1000 | Loss: 0.00006191
Iteration 864/1000 | Loss: 0.00006165
Iteration 865/1000 | Loss: 0.00006322
Iteration 866/1000 | Loss: 0.00005766
Iteration 867/1000 | Loss: 0.00005863
Iteration 868/1000 | Loss: 0.00006326
Iteration 869/1000 | Loss: 0.00007281
Iteration 870/1000 | Loss: 0.00006381
Iteration 871/1000 | Loss: 0.00007002
Iteration 872/1000 | Loss: 0.00006433
Iteration 873/1000 | Loss: 0.00006272
Iteration 874/1000 | Loss: 0.00006274
Iteration 875/1000 | Loss: 0.00006150
Iteration 876/1000 | Loss: 0.00006086
Iteration 877/1000 | Loss: 0.00005957
Iteration 878/1000 | Loss: 0.00006355
Iteration 879/1000 | Loss: 0.00006358
Iteration 880/1000 | Loss: 0.00006446
Iteration 881/1000 | Loss: 0.00006090
Iteration 882/1000 | Loss: 0.00006169
Iteration 883/1000 | Loss: 0.00006008
Iteration 884/1000 | Loss: 0.00005587
Iteration 885/1000 | Loss: 0.00006452
Iteration 886/1000 | Loss: 0.00005700
Iteration 887/1000 | Loss: 0.00005678
Iteration 888/1000 | Loss: 0.00006208
Iteration 889/1000 | Loss: 0.00006432
Iteration 890/1000 | Loss: 0.00006129
Iteration 891/1000 | Loss: 0.00006189
Iteration 892/1000 | Loss: 0.00005753
Iteration 893/1000 | Loss: 0.00005701
Iteration 894/1000 | Loss: 0.00005770
Iteration 895/1000 | Loss: 0.00005937
Iteration 896/1000 | Loss: 0.00005699
Iteration 897/1000 | Loss: 0.00005991
Iteration 898/1000 | Loss: 0.00005612
Iteration 899/1000 | Loss: 0.00004589
Iteration 900/1000 | Loss: 0.00006046
Iteration 901/1000 | Loss: 0.00005988
Iteration 902/1000 | Loss: 0.00005699
Iteration 903/1000 | Loss: 0.00005667
Iteration 904/1000 | Loss: 0.00005309
Iteration 905/1000 | Loss: 0.00009412
Iteration 906/1000 | Loss: 0.00006170
Iteration 907/1000 | Loss: 0.00005956
Iteration 908/1000 | Loss: 0.00005667
Iteration 909/1000 | Loss: 0.00005640
Iteration 910/1000 | Loss: 0.00005600
Iteration 911/1000 | Loss: 0.00005631
Iteration 912/1000 | Loss: 0.00005601
Iteration 913/1000 | Loss: 0.00004876
Iteration 914/1000 | Loss: 0.00005363
Iteration 915/1000 | Loss: 0.00005731
Iteration 916/1000 | Loss: 0.00005722
Iteration 917/1000 | Loss: 0.00005088
Iteration 918/1000 | Loss: 0.00005473
Iteration 919/1000 | Loss: 0.00005656
Iteration 920/1000 | Loss: 0.00005510
Iteration 921/1000 | Loss: 0.00005722
Iteration 922/1000 | Loss: 0.00005895
Iteration 923/1000 | Loss: 0.00005850
Iteration 924/1000 | Loss: 0.00005935
Iteration 925/1000 | Loss: 0.00006646
Iteration 926/1000 | Loss: 0.00006094
Iteration 927/1000 | Loss: 0.00006128
Iteration 928/1000 | Loss: 0.00005978
Iteration 929/1000 | Loss: 0.00005668
Iteration 930/1000 | Loss: 0.00006749
Iteration 931/1000 | Loss: 0.00006083
Iteration 932/1000 | Loss: 0.00005868
Iteration 933/1000 | Loss: 0.00005743
Iteration 934/1000 | Loss: 0.00005857
Iteration 935/1000 | Loss: 0.00005407
Iteration 936/1000 | Loss: 0.00005757
Iteration 937/1000 | Loss: 0.00008022
Iteration 938/1000 | Loss: 0.00006283
Iteration 939/1000 | Loss: 0.00005950
Iteration 940/1000 | Loss: 0.00005929
Iteration 941/1000 | Loss: 0.00005379
Iteration 942/1000 | Loss: 0.00005589
Iteration 943/1000 | Loss: 0.00005716
Iteration 944/1000 | Loss: 0.00005611
Iteration 945/1000 | Loss: 0.00005737
Iteration 946/1000 | Loss: 0.00005570
Iteration 947/1000 | Loss: 0.00006946
Iteration 948/1000 | Loss: 0.00006162
Iteration 949/1000 | Loss: 0.00005516
Iteration 950/1000 | Loss: 0.00005562
Iteration 951/1000 | Loss: 0.00005474
Iteration 952/1000 | Loss: 0.00006147
Iteration 953/1000 | Loss: 0.00005392
Iteration 954/1000 | Loss: 0.00005709
Iteration 955/1000 | Loss: 0.00005390
Iteration 956/1000 | Loss: 0.00005579
Iteration 957/1000 | Loss: 0.00005413
Iteration 958/1000 | Loss: 0.00006753
Iteration 959/1000 | Loss: 0.00005409
Iteration 960/1000 | Loss: 0.00006667
Iteration 961/1000 | Loss: 0.00005622
Iteration 962/1000 | Loss: 0.00006332
Iteration 963/1000 | Loss: 0.00005421
Iteration 964/1000 | Loss: 0.00005356
Iteration 965/1000 | Loss: 0.00006623
Iteration 966/1000 | Loss: 0.00005380
Iteration 967/1000 | Loss: 0.00005933
Iteration 968/1000 | Loss: 0.00005186
Iteration 969/1000 | Loss: 0.00006107
Iteration 970/1000 | Loss: 0.00006230
Iteration 971/1000 | Loss: 0.00005455
Iteration 972/1000 | Loss: 0.00005472
Iteration 973/1000 | Loss: 0.00006538
Iteration 974/1000 | Loss: 0.00006592
Iteration 975/1000 | Loss: 0.00007013
Iteration 976/1000 | Loss: 0.00005465
Iteration 977/1000 | Loss: 0.00007180
Iteration 978/1000 | Loss: 0.00005486
Iteration 979/1000 | Loss: 0.00005676
Iteration 980/1000 | Loss: 0.00005944
Iteration 981/1000 | Loss: 0.00005655
Iteration 982/1000 | Loss: 0.00005611
Iteration 983/1000 | Loss: 0.00005410
Iteration 984/1000 | Loss: 0.00006499
Iteration 985/1000 | Loss: 0.00005647
Iteration 986/1000 | Loss: 0.00005373
Iteration 987/1000 | Loss: 0.00006953
Iteration 988/1000 | Loss: 0.00005829
Iteration 989/1000 | Loss: 0.00006620
Iteration 990/1000 | Loss: 0.00005793
Iteration 991/1000 | Loss: 0.00005254
Iteration 992/1000 | Loss: 0.00005242
Iteration 993/1000 | Loss: 0.00005105
Iteration 994/1000 | Loss: 0.00005614
Iteration 995/1000 | Loss: 0.00006463
Iteration 996/1000 | Loss: 0.00005737
Iteration 997/1000 | Loss: 0.00007420
Iteration 998/1000 | Loss: 0.00006464
Iteration 999/1000 | Loss: 0.00005694
Iteration 1000/1000 | Loss: 0.00005740

Optimization complete. Final v2v error: 2.8531925678253174 mm

Highest mean error: 31.427457809448242 mm for frame 176

Lowest mean error: 2.311997175216675 mm for frame 7

Saving results

Total time: 1500.5995163917542
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00737153
Iteration 2/25 | Loss: 0.00130823
Iteration 3/25 | Loss: 0.00117119
Iteration 4/25 | Loss: 0.00116013
Iteration 5/25 | Loss: 0.00115739
Iteration 6/25 | Loss: 0.00115739
Iteration 7/25 | Loss: 0.00115739
Iteration 8/25 | Loss: 0.00115739
Iteration 9/25 | Loss: 0.00115739
Iteration 10/25 | Loss: 0.00115739
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0011573937954381108, 0.0011573937954381108, 0.0011573937954381108, 0.0011573937954381108, 0.0011573937954381108]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011573937954381108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28345823
Iteration 2/25 | Loss: 0.00091532
Iteration 3/25 | Loss: 0.00091529
Iteration 4/25 | Loss: 0.00091529
Iteration 5/25 | Loss: 0.00091529
Iteration 6/25 | Loss: 0.00091529
Iteration 7/25 | Loss: 0.00091529
Iteration 8/25 | Loss: 0.00091529
Iteration 9/25 | Loss: 0.00091529
Iteration 10/25 | Loss: 0.00091529
Iteration 11/25 | Loss: 0.00091529
Iteration 12/25 | Loss: 0.00091529
Iteration 13/25 | Loss: 0.00091529
Iteration 14/25 | Loss: 0.00091529
Iteration 15/25 | Loss: 0.00091529
Iteration 16/25 | Loss: 0.00091529
Iteration 17/25 | Loss: 0.00091529
Iteration 18/25 | Loss: 0.00091529
Iteration 19/25 | Loss: 0.00091529
Iteration 20/25 | Loss: 0.00091529
Iteration 21/25 | Loss: 0.00091529
Iteration 22/25 | Loss: 0.00091529
Iteration 23/25 | Loss: 0.00091529
Iteration 24/25 | Loss: 0.00091529
Iteration 25/25 | Loss: 0.00091529
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0009152891580015421, 0.0009152891580015421, 0.0009152891580015421, 0.0009152891580015421, 0.0009152891580015421]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009152891580015421

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00091529
Iteration 2/1000 | Loss: 0.00002428
Iteration 3/1000 | Loss: 0.00001930
Iteration 4/1000 | Loss: 0.00001734
Iteration 5/1000 | Loss: 0.00001583
Iteration 6/1000 | Loss: 0.00001483
Iteration 7/1000 | Loss: 0.00001419
Iteration 8/1000 | Loss: 0.00001359
Iteration 9/1000 | Loss: 0.00001313
Iteration 10/1000 | Loss: 0.00001277
Iteration 11/1000 | Loss: 0.00001264
Iteration 12/1000 | Loss: 0.00001231
Iteration 13/1000 | Loss: 0.00001204
Iteration 14/1000 | Loss: 0.00001176
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001149
Iteration 17/1000 | Loss: 0.00001147
Iteration 18/1000 | Loss: 0.00001147
Iteration 19/1000 | Loss: 0.00001145
Iteration 20/1000 | Loss: 0.00001144
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001143
Iteration 24/1000 | Loss: 0.00001143
Iteration 25/1000 | Loss: 0.00001142
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001141
Iteration 28/1000 | Loss: 0.00001140
Iteration 29/1000 | Loss: 0.00001140
Iteration 30/1000 | Loss: 0.00001139
Iteration 31/1000 | Loss: 0.00001139
Iteration 32/1000 | Loss: 0.00001139
Iteration 33/1000 | Loss: 0.00001139
Iteration 34/1000 | Loss: 0.00001138
Iteration 35/1000 | Loss: 0.00001138
Iteration 36/1000 | Loss: 0.00001138
Iteration 37/1000 | Loss: 0.00001137
Iteration 38/1000 | Loss: 0.00001137
Iteration 39/1000 | Loss: 0.00001137
Iteration 40/1000 | Loss: 0.00001136
Iteration 41/1000 | Loss: 0.00001136
Iteration 42/1000 | Loss: 0.00001135
Iteration 43/1000 | Loss: 0.00001135
Iteration 44/1000 | Loss: 0.00001135
Iteration 45/1000 | Loss: 0.00001134
Iteration 46/1000 | Loss: 0.00001134
Iteration 47/1000 | Loss: 0.00001134
Iteration 48/1000 | Loss: 0.00001134
Iteration 49/1000 | Loss: 0.00001133
Iteration 50/1000 | Loss: 0.00001132
Iteration 51/1000 | Loss: 0.00001132
Iteration 52/1000 | Loss: 0.00001132
Iteration 53/1000 | Loss: 0.00001132
Iteration 54/1000 | Loss: 0.00001131
Iteration 55/1000 | Loss: 0.00001131
Iteration 56/1000 | Loss: 0.00001131
Iteration 57/1000 | Loss: 0.00001131
Iteration 58/1000 | Loss: 0.00001131
Iteration 59/1000 | Loss: 0.00001131
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001130
Iteration 66/1000 | Loss: 0.00001130
Iteration 67/1000 | Loss: 0.00001130
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001128
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001127
Iteration 74/1000 | Loss: 0.00001127
Iteration 75/1000 | Loss: 0.00001126
Iteration 76/1000 | Loss: 0.00001126
Iteration 77/1000 | Loss: 0.00001126
Iteration 78/1000 | Loss: 0.00001125
Iteration 79/1000 | Loss: 0.00001124
Iteration 80/1000 | Loss: 0.00001124
Iteration 81/1000 | Loss: 0.00001123
Iteration 82/1000 | Loss: 0.00001123
Iteration 83/1000 | Loss: 0.00001123
Iteration 84/1000 | Loss: 0.00001123
Iteration 85/1000 | Loss: 0.00001123
Iteration 86/1000 | Loss: 0.00001123
Iteration 87/1000 | Loss: 0.00001123
Iteration 88/1000 | Loss: 0.00001123
Iteration 89/1000 | Loss: 0.00001122
Iteration 90/1000 | Loss: 0.00001122
Iteration 91/1000 | Loss: 0.00001122
Iteration 92/1000 | Loss: 0.00001122
Iteration 93/1000 | Loss: 0.00001122
Iteration 94/1000 | Loss: 0.00001122
Iteration 95/1000 | Loss: 0.00001122
Iteration 96/1000 | Loss: 0.00001122
Iteration 97/1000 | Loss: 0.00001122
Iteration 98/1000 | Loss: 0.00001122
Iteration 99/1000 | Loss: 0.00001122
Iteration 100/1000 | Loss: 0.00001122
Iteration 101/1000 | Loss: 0.00001121
Iteration 102/1000 | Loss: 0.00001121
Iteration 103/1000 | Loss: 0.00001121
Iteration 104/1000 | Loss: 0.00001121
Iteration 105/1000 | Loss: 0.00001121
Iteration 106/1000 | Loss: 0.00001121
Iteration 107/1000 | Loss: 0.00001120
Iteration 108/1000 | Loss: 0.00001120
Iteration 109/1000 | Loss: 0.00001117
Iteration 110/1000 | Loss: 0.00001117
Iteration 111/1000 | Loss: 0.00001117
Iteration 112/1000 | Loss: 0.00001117
Iteration 113/1000 | Loss: 0.00001117
Iteration 114/1000 | Loss: 0.00001117
Iteration 115/1000 | Loss: 0.00001116
Iteration 116/1000 | Loss: 0.00001116
Iteration 117/1000 | Loss: 0.00001114
Iteration 118/1000 | Loss: 0.00001114
Iteration 119/1000 | Loss: 0.00001114
Iteration 120/1000 | Loss: 0.00001113
Iteration 121/1000 | Loss: 0.00001113
Iteration 122/1000 | Loss: 0.00001112
Iteration 123/1000 | Loss: 0.00001112
Iteration 124/1000 | Loss: 0.00001112
Iteration 125/1000 | Loss: 0.00001111
Iteration 126/1000 | Loss: 0.00001111
Iteration 127/1000 | Loss: 0.00001111
Iteration 128/1000 | Loss: 0.00001110
Iteration 129/1000 | Loss: 0.00001110
Iteration 130/1000 | Loss: 0.00001110
Iteration 131/1000 | Loss: 0.00001110
Iteration 132/1000 | Loss: 0.00001110
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001109
Iteration 138/1000 | Loss: 0.00001109
Iteration 139/1000 | Loss: 0.00001108
Iteration 140/1000 | Loss: 0.00001108
Iteration 141/1000 | Loss: 0.00001108
Iteration 142/1000 | Loss: 0.00001108
Iteration 143/1000 | Loss: 0.00001107
Iteration 144/1000 | Loss: 0.00001107
Iteration 145/1000 | Loss: 0.00001107
Iteration 146/1000 | Loss: 0.00001107
Iteration 147/1000 | Loss: 0.00001107
Iteration 148/1000 | Loss: 0.00001107
Iteration 149/1000 | Loss: 0.00001107
Iteration 150/1000 | Loss: 0.00001107
Iteration 151/1000 | Loss: 0.00001107
Iteration 152/1000 | Loss: 0.00001107
Iteration 153/1000 | Loss: 0.00001107
Iteration 154/1000 | Loss: 0.00001107
Iteration 155/1000 | Loss: 0.00001106
Iteration 156/1000 | Loss: 0.00001106
Iteration 157/1000 | Loss: 0.00001106
Iteration 158/1000 | Loss: 0.00001106
Iteration 159/1000 | Loss: 0.00001106
Iteration 160/1000 | Loss: 0.00001106
Iteration 161/1000 | Loss: 0.00001106
Iteration 162/1000 | Loss: 0.00001106
Iteration 163/1000 | Loss: 0.00001106
Iteration 164/1000 | Loss: 0.00001106
Iteration 165/1000 | Loss: 0.00001106
Iteration 166/1000 | Loss: 0.00001106
Iteration 167/1000 | Loss: 0.00001106
Iteration 168/1000 | Loss: 0.00001106
Iteration 169/1000 | Loss: 0.00001106
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [1.106014406104805e-05, 1.106014406104805e-05, 1.106014406104805e-05, 1.106014406104805e-05, 1.106014406104805e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.106014406104805e-05

Optimization complete. Final v2v error: 2.868198871612549 mm

Highest mean error: 3.231126546859741 mm for frame 21

Lowest mean error: 2.635411500930786 mm for frame 222

Saving results

Total time: 46.46265149116516
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00793044
Iteration 2/25 | Loss: 0.00135169
Iteration 3/25 | Loss: 0.00126328
Iteration 4/25 | Loss: 0.00125059
Iteration 5/25 | Loss: 0.00124701
Iteration 6/25 | Loss: 0.00124648
Iteration 7/25 | Loss: 0.00124648
Iteration 8/25 | Loss: 0.00124648
Iteration 9/25 | Loss: 0.00124648
Iteration 10/25 | Loss: 0.00124648
Iteration 11/25 | Loss: 0.00124648
Iteration 12/25 | Loss: 0.00124648
Iteration 13/25 | Loss: 0.00124648
Iteration 14/25 | Loss: 0.00124648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012464806204661727, 0.0012464806204661727, 0.0012464806204661727, 0.0012464806204661727, 0.0012464806204661727]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012464806204661727

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42968297
Iteration 2/25 | Loss: 0.00115153
Iteration 3/25 | Loss: 0.00115153
Iteration 4/25 | Loss: 0.00115153
Iteration 5/25 | Loss: 0.00115153
Iteration 6/25 | Loss: 0.00115153
Iteration 7/25 | Loss: 0.00115152
Iteration 8/25 | Loss: 0.00115152
Iteration 9/25 | Loss: 0.00115152
Iteration 10/25 | Loss: 0.00115152
Iteration 11/25 | Loss: 0.00115152
Iteration 12/25 | Loss: 0.00115152
Iteration 13/25 | Loss: 0.00115152
Iteration 14/25 | Loss: 0.00115152
Iteration 15/25 | Loss: 0.00115152
Iteration 16/25 | Loss: 0.00115152
Iteration 17/25 | Loss: 0.00115152
Iteration 18/25 | Loss: 0.00115152
Iteration 19/25 | Loss: 0.00115152
Iteration 20/25 | Loss: 0.00115152
Iteration 21/25 | Loss: 0.00115152
Iteration 22/25 | Loss: 0.00115152
Iteration 23/25 | Loss: 0.00115152
Iteration 24/25 | Loss: 0.00115152
Iteration 25/25 | Loss: 0.00115152

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115152
Iteration 2/1000 | Loss: 0.00004061
Iteration 3/1000 | Loss: 0.00002574
Iteration 4/1000 | Loss: 0.00002098
Iteration 5/1000 | Loss: 0.00001987
Iteration 6/1000 | Loss: 0.00001889
Iteration 7/1000 | Loss: 0.00001842
Iteration 8/1000 | Loss: 0.00001797
Iteration 9/1000 | Loss: 0.00001766
Iteration 10/1000 | Loss: 0.00001739
Iteration 11/1000 | Loss: 0.00001726
Iteration 12/1000 | Loss: 0.00001704
Iteration 13/1000 | Loss: 0.00001692
Iteration 14/1000 | Loss: 0.00001684
Iteration 15/1000 | Loss: 0.00001681
Iteration 16/1000 | Loss: 0.00001679
Iteration 17/1000 | Loss: 0.00001672
Iteration 18/1000 | Loss: 0.00001660
Iteration 19/1000 | Loss: 0.00001656
Iteration 20/1000 | Loss: 0.00001654
Iteration 21/1000 | Loss: 0.00001653
Iteration 22/1000 | Loss: 0.00001653
Iteration 23/1000 | Loss: 0.00001653
Iteration 24/1000 | Loss: 0.00001653
Iteration 25/1000 | Loss: 0.00001652
Iteration 26/1000 | Loss: 0.00001652
Iteration 27/1000 | Loss: 0.00001650
Iteration 28/1000 | Loss: 0.00001649
Iteration 29/1000 | Loss: 0.00001649
Iteration 30/1000 | Loss: 0.00001648
Iteration 31/1000 | Loss: 0.00001648
Iteration 32/1000 | Loss: 0.00001648
Iteration 33/1000 | Loss: 0.00001648
Iteration 34/1000 | Loss: 0.00001648
Iteration 35/1000 | Loss: 0.00001647
Iteration 36/1000 | Loss: 0.00001647
Iteration 37/1000 | Loss: 0.00001646
Iteration 38/1000 | Loss: 0.00001646
Iteration 39/1000 | Loss: 0.00001645
Iteration 40/1000 | Loss: 0.00001645
Iteration 41/1000 | Loss: 0.00001645
Iteration 42/1000 | Loss: 0.00001644
Iteration 43/1000 | Loss: 0.00001644
Iteration 44/1000 | Loss: 0.00001644
Iteration 45/1000 | Loss: 0.00001643
Iteration 46/1000 | Loss: 0.00001643
Iteration 47/1000 | Loss: 0.00001643
Iteration 48/1000 | Loss: 0.00001643
Iteration 49/1000 | Loss: 0.00001643
Iteration 50/1000 | Loss: 0.00001642
Iteration 51/1000 | Loss: 0.00001642
Iteration 52/1000 | Loss: 0.00001642
Iteration 53/1000 | Loss: 0.00001641
Iteration 54/1000 | Loss: 0.00001641
Iteration 55/1000 | Loss: 0.00001641
Iteration 56/1000 | Loss: 0.00001641
Iteration 57/1000 | Loss: 0.00001641
Iteration 58/1000 | Loss: 0.00001641
Iteration 59/1000 | Loss: 0.00001641
Iteration 60/1000 | Loss: 0.00001641
Iteration 61/1000 | Loss: 0.00001641
Iteration 62/1000 | Loss: 0.00001640
Iteration 63/1000 | Loss: 0.00001640
Iteration 64/1000 | Loss: 0.00001640
Iteration 65/1000 | Loss: 0.00001639
Iteration 66/1000 | Loss: 0.00001639
Iteration 67/1000 | Loss: 0.00001638
Iteration 68/1000 | Loss: 0.00001638
Iteration 69/1000 | Loss: 0.00001638
Iteration 70/1000 | Loss: 0.00001638
Iteration 71/1000 | Loss: 0.00001638
Iteration 72/1000 | Loss: 0.00001638
Iteration 73/1000 | Loss: 0.00001638
Iteration 74/1000 | Loss: 0.00001638
Iteration 75/1000 | Loss: 0.00001638
Iteration 76/1000 | Loss: 0.00001637
Iteration 77/1000 | Loss: 0.00001637
Iteration 78/1000 | Loss: 0.00001637
Iteration 79/1000 | Loss: 0.00001637
Iteration 80/1000 | Loss: 0.00001636
Iteration 81/1000 | Loss: 0.00001636
Iteration 82/1000 | Loss: 0.00001636
Iteration 83/1000 | Loss: 0.00001636
Iteration 84/1000 | Loss: 0.00001635
Iteration 85/1000 | Loss: 0.00001634
Iteration 86/1000 | Loss: 0.00001634
Iteration 87/1000 | Loss: 0.00001634
Iteration 88/1000 | Loss: 0.00001634
Iteration 89/1000 | Loss: 0.00001633
Iteration 90/1000 | Loss: 0.00001633
Iteration 91/1000 | Loss: 0.00001633
Iteration 92/1000 | Loss: 0.00001633
Iteration 93/1000 | Loss: 0.00001632
Iteration 94/1000 | Loss: 0.00001632
Iteration 95/1000 | Loss: 0.00001632
Iteration 96/1000 | Loss: 0.00001632
Iteration 97/1000 | Loss: 0.00001631
Iteration 98/1000 | Loss: 0.00001631
Iteration 99/1000 | Loss: 0.00001631
Iteration 100/1000 | Loss: 0.00001631
Iteration 101/1000 | Loss: 0.00001631
Iteration 102/1000 | Loss: 0.00001631
Iteration 103/1000 | Loss: 0.00001631
Iteration 104/1000 | Loss: 0.00001630
Iteration 105/1000 | Loss: 0.00001630
Iteration 106/1000 | Loss: 0.00001630
Iteration 107/1000 | Loss: 0.00001630
Iteration 108/1000 | Loss: 0.00001630
Iteration 109/1000 | Loss: 0.00001630
Iteration 110/1000 | Loss: 0.00001630
Iteration 111/1000 | Loss: 0.00001629
Iteration 112/1000 | Loss: 0.00001629
Iteration 113/1000 | Loss: 0.00001629
Iteration 114/1000 | Loss: 0.00001629
Iteration 115/1000 | Loss: 0.00001628
Iteration 116/1000 | Loss: 0.00001628
Iteration 117/1000 | Loss: 0.00001628
Iteration 118/1000 | Loss: 0.00001628
Iteration 119/1000 | Loss: 0.00001627
Iteration 120/1000 | Loss: 0.00001627
Iteration 121/1000 | Loss: 0.00001627
Iteration 122/1000 | Loss: 0.00001627
Iteration 123/1000 | Loss: 0.00001627
Iteration 124/1000 | Loss: 0.00001627
Iteration 125/1000 | Loss: 0.00001626
Iteration 126/1000 | Loss: 0.00001626
Iteration 127/1000 | Loss: 0.00001626
Iteration 128/1000 | Loss: 0.00001625
Iteration 129/1000 | Loss: 0.00001625
Iteration 130/1000 | Loss: 0.00001625
Iteration 131/1000 | Loss: 0.00001625
Iteration 132/1000 | Loss: 0.00001625
Iteration 133/1000 | Loss: 0.00001624
Iteration 134/1000 | Loss: 0.00001624
Iteration 135/1000 | Loss: 0.00001624
Iteration 136/1000 | Loss: 0.00001623
Iteration 137/1000 | Loss: 0.00001623
Iteration 138/1000 | Loss: 0.00001623
Iteration 139/1000 | Loss: 0.00001622
Iteration 140/1000 | Loss: 0.00001622
Iteration 141/1000 | Loss: 0.00001622
Iteration 142/1000 | Loss: 0.00001622
Iteration 143/1000 | Loss: 0.00001621
Iteration 144/1000 | Loss: 0.00001621
Iteration 145/1000 | Loss: 0.00001621
Iteration 146/1000 | Loss: 0.00001621
Iteration 147/1000 | Loss: 0.00001620
Iteration 148/1000 | Loss: 0.00001620
Iteration 149/1000 | Loss: 0.00001620
Iteration 150/1000 | Loss: 0.00001620
Iteration 151/1000 | Loss: 0.00001620
Iteration 152/1000 | Loss: 0.00001620
Iteration 153/1000 | Loss: 0.00001620
Iteration 154/1000 | Loss: 0.00001619
Iteration 155/1000 | Loss: 0.00001619
Iteration 156/1000 | Loss: 0.00001619
Iteration 157/1000 | Loss: 0.00001619
Iteration 158/1000 | Loss: 0.00001619
Iteration 159/1000 | Loss: 0.00001618
Iteration 160/1000 | Loss: 0.00001618
Iteration 161/1000 | Loss: 0.00001618
Iteration 162/1000 | Loss: 0.00001618
Iteration 163/1000 | Loss: 0.00001618
Iteration 164/1000 | Loss: 0.00001618
Iteration 165/1000 | Loss: 0.00001618
Iteration 166/1000 | Loss: 0.00001618
Iteration 167/1000 | Loss: 0.00001618
Iteration 168/1000 | Loss: 0.00001617
Iteration 169/1000 | Loss: 0.00001617
Iteration 170/1000 | Loss: 0.00001617
Iteration 171/1000 | Loss: 0.00001617
Iteration 172/1000 | Loss: 0.00001617
Iteration 173/1000 | Loss: 0.00001617
Iteration 174/1000 | Loss: 0.00001617
Iteration 175/1000 | Loss: 0.00001617
Iteration 176/1000 | Loss: 0.00001617
Iteration 177/1000 | Loss: 0.00001617
Iteration 178/1000 | Loss: 0.00001617
Iteration 179/1000 | Loss: 0.00001616
Iteration 180/1000 | Loss: 0.00001616
Iteration 181/1000 | Loss: 0.00001616
Iteration 182/1000 | Loss: 0.00001616
Iteration 183/1000 | Loss: 0.00001616
Iteration 184/1000 | Loss: 0.00001616
Iteration 185/1000 | Loss: 0.00001616
Iteration 186/1000 | Loss: 0.00001616
Iteration 187/1000 | Loss: 0.00001615
Iteration 188/1000 | Loss: 0.00001615
Iteration 189/1000 | Loss: 0.00001615
Iteration 190/1000 | Loss: 0.00001615
Iteration 191/1000 | Loss: 0.00001615
Iteration 192/1000 | Loss: 0.00001615
Iteration 193/1000 | Loss: 0.00001615
Iteration 194/1000 | Loss: 0.00001615
Iteration 195/1000 | Loss: 0.00001615
Iteration 196/1000 | Loss: 0.00001615
Iteration 197/1000 | Loss: 0.00001614
Iteration 198/1000 | Loss: 0.00001614
Iteration 199/1000 | Loss: 0.00001614
Iteration 200/1000 | Loss: 0.00001614
Iteration 201/1000 | Loss: 0.00001614
Iteration 202/1000 | Loss: 0.00001614
Iteration 203/1000 | Loss: 0.00001614
Iteration 204/1000 | Loss: 0.00001614
Iteration 205/1000 | Loss: 0.00001614
Iteration 206/1000 | Loss: 0.00001614
Iteration 207/1000 | Loss: 0.00001614
Iteration 208/1000 | Loss: 0.00001614
Iteration 209/1000 | Loss: 0.00001614
Iteration 210/1000 | Loss: 0.00001613
Iteration 211/1000 | Loss: 0.00001613
Iteration 212/1000 | Loss: 0.00001613
Iteration 213/1000 | Loss: 0.00001613
Iteration 214/1000 | Loss: 0.00001613
Iteration 215/1000 | Loss: 0.00001613
Iteration 216/1000 | Loss: 0.00001613
Iteration 217/1000 | Loss: 0.00001612
Iteration 218/1000 | Loss: 0.00001612
Iteration 219/1000 | Loss: 0.00001612
Iteration 220/1000 | Loss: 0.00001612
Iteration 221/1000 | Loss: 0.00001612
Iteration 222/1000 | Loss: 0.00001612
Iteration 223/1000 | Loss: 0.00001612
Iteration 224/1000 | Loss: 0.00001612
Iteration 225/1000 | Loss: 0.00001612
Iteration 226/1000 | Loss: 0.00001612
Iteration 227/1000 | Loss: 0.00001612
Iteration 228/1000 | Loss: 0.00001612
Iteration 229/1000 | Loss: 0.00001612
Iteration 230/1000 | Loss: 0.00001612
Iteration 231/1000 | Loss: 0.00001612
Iteration 232/1000 | Loss: 0.00001612
Iteration 233/1000 | Loss: 0.00001612
Iteration 234/1000 | Loss: 0.00001612
Iteration 235/1000 | Loss: 0.00001612
Iteration 236/1000 | Loss: 0.00001612
Iteration 237/1000 | Loss: 0.00001612
Iteration 238/1000 | Loss: 0.00001612
Iteration 239/1000 | Loss: 0.00001612
Iteration 240/1000 | Loss: 0.00001612
Iteration 241/1000 | Loss: 0.00001612
Iteration 242/1000 | Loss: 0.00001612
Iteration 243/1000 | Loss: 0.00001612
Iteration 244/1000 | Loss: 0.00001612
Iteration 245/1000 | Loss: 0.00001612
Iteration 246/1000 | Loss: 0.00001612
Iteration 247/1000 | Loss: 0.00001612
Iteration 248/1000 | Loss: 0.00001612
Iteration 249/1000 | Loss: 0.00001612
Iteration 250/1000 | Loss: 0.00001612
Iteration 251/1000 | Loss: 0.00001612
Iteration 252/1000 | Loss: 0.00001612
Iteration 253/1000 | Loss: 0.00001612
Iteration 254/1000 | Loss: 0.00001612
Iteration 255/1000 | Loss: 0.00001612
Iteration 256/1000 | Loss: 0.00001612
Iteration 257/1000 | Loss: 0.00001612
Iteration 258/1000 | Loss: 0.00001612
Iteration 259/1000 | Loss: 0.00001612
Iteration 260/1000 | Loss: 0.00001612
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 260. Stopping optimization.
Last 5 losses: [1.612102823855821e-05, 1.612102823855821e-05, 1.612102823855821e-05, 1.612102823855821e-05, 1.612102823855821e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.612102823855821e-05

Optimization complete. Final v2v error: 3.3556840419769287 mm

Highest mean error: 4.29483699798584 mm for frame 122

Lowest mean error: 2.6293959617614746 mm for frame 144

Saving results

Total time: 43.459980964660645
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00781426
Iteration 2/25 | Loss: 0.00188760
Iteration 3/25 | Loss: 0.00135723
Iteration 4/25 | Loss: 0.00130363
Iteration 5/25 | Loss: 0.00130025
Iteration 6/25 | Loss: 0.00129958
Iteration 7/25 | Loss: 0.00129958
Iteration 8/25 | Loss: 0.00129958
Iteration 9/25 | Loss: 0.00129958
Iteration 10/25 | Loss: 0.00129958
Iteration 11/25 | Loss: 0.00129958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012995755532756448, 0.0012995755532756448, 0.0012995755532756448, 0.0012995755532756448, 0.0012995755532756448]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012995755532756448

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.32715201
Iteration 2/25 | Loss: 0.00109616
Iteration 3/25 | Loss: 0.00109614
Iteration 4/25 | Loss: 0.00109614
Iteration 5/25 | Loss: 0.00109614
Iteration 6/25 | Loss: 0.00109614
Iteration 7/25 | Loss: 0.00109614
Iteration 8/25 | Loss: 0.00109614
Iteration 9/25 | Loss: 0.00109614
Iteration 10/25 | Loss: 0.00109614
Iteration 11/25 | Loss: 0.00109614
Iteration 12/25 | Loss: 0.00109614
Iteration 13/25 | Loss: 0.00109614
Iteration 14/25 | Loss: 0.00109614
Iteration 15/25 | Loss: 0.00109614
Iteration 16/25 | Loss: 0.00109614
Iteration 17/25 | Loss: 0.00109614
Iteration 18/25 | Loss: 0.00109614
Iteration 19/25 | Loss: 0.00109614
Iteration 20/25 | Loss: 0.00109614
Iteration 21/25 | Loss: 0.00109614
Iteration 22/25 | Loss: 0.00109614
Iteration 23/25 | Loss: 0.00109614
Iteration 24/25 | Loss: 0.00109614
Iteration 25/25 | Loss: 0.00109614

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00109614
Iteration 2/1000 | Loss: 0.00003506
Iteration 3/1000 | Loss: 0.00002454
Iteration 4/1000 | Loss: 0.00002205
Iteration 5/1000 | Loss: 0.00002126
Iteration 6/1000 | Loss: 0.00002076
Iteration 7/1000 | Loss: 0.00002045
Iteration 8/1000 | Loss: 0.00002019
Iteration 9/1000 | Loss: 0.00001998
Iteration 10/1000 | Loss: 0.00001993
Iteration 11/1000 | Loss: 0.00001988
Iteration 12/1000 | Loss: 0.00001980
Iteration 13/1000 | Loss: 0.00001979
Iteration 14/1000 | Loss: 0.00001978
Iteration 15/1000 | Loss: 0.00001977
Iteration 16/1000 | Loss: 0.00001976
Iteration 17/1000 | Loss: 0.00001970
Iteration 18/1000 | Loss: 0.00001967
Iteration 19/1000 | Loss: 0.00001966
Iteration 20/1000 | Loss: 0.00001966
Iteration 21/1000 | Loss: 0.00001966
Iteration 22/1000 | Loss: 0.00001966
Iteration 23/1000 | Loss: 0.00001966
Iteration 24/1000 | Loss: 0.00001966
Iteration 25/1000 | Loss: 0.00001965
Iteration 26/1000 | Loss: 0.00001965
Iteration 27/1000 | Loss: 0.00001965
Iteration 28/1000 | Loss: 0.00001965
Iteration 29/1000 | Loss: 0.00001965
Iteration 30/1000 | Loss: 0.00001965
Iteration 31/1000 | Loss: 0.00001965
Iteration 32/1000 | Loss: 0.00001965
Iteration 33/1000 | Loss: 0.00001965
Iteration 34/1000 | Loss: 0.00001963
Iteration 35/1000 | Loss: 0.00001963
Iteration 36/1000 | Loss: 0.00001961
Iteration 37/1000 | Loss: 0.00001958
Iteration 38/1000 | Loss: 0.00001958
Iteration 39/1000 | Loss: 0.00001957
Iteration 40/1000 | Loss: 0.00001957
Iteration 41/1000 | Loss: 0.00001957
Iteration 42/1000 | Loss: 0.00001956
Iteration 43/1000 | Loss: 0.00001956
Iteration 44/1000 | Loss: 0.00001955
Iteration 45/1000 | Loss: 0.00001955
Iteration 46/1000 | Loss: 0.00001955
Iteration 47/1000 | Loss: 0.00001954
Iteration 48/1000 | Loss: 0.00001954
Iteration 49/1000 | Loss: 0.00001954
Iteration 50/1000 | Loss: 0.00001954
Iteration 51/1000 | Loss: 0.00001954
Iteration 52/1000 | Loss: 0.00001954
Iteration 53/1000 | Loss: 0.00001954
Iteration 54/1000 | Loss: 0.00001953
Iteration 55/1000 | Loss: 0.00001953
Iteration 56/1000 | Loss: 0.00001952
Iteration 57/1000 | Loss: 0.00001952
Iteration 58/1000 | Loss: 0.00001952
Iteration 59/1000 | Loss: 0.00001951
Iteration 60/1000 | Loss: 0.00001951
Iteration 61/1000 | Loss: 0.00001950
Iteration 62/1000 | Loss: 0.00001949
Iteration 63/1000 | Loss: 0.00001948
Iteration 64/1000 | Loss: 0.00001947
Iteration 65/1000 | Loss: 0.00001947
Iteration 66/1000 | Loss: 0.00001947
Iteration 67/1000 | Loss: 0.00001947
Iteration 68/1000 | Loss: 0.00001947
Iteration 69/1000 | Loss: 0.00001947
Iteration 70/1000 | Loss: 0.00001947
Iteration 71/1000 | Loss: 0.00001947
Iteration 72/1000 | Loss: 0.00001947
Iteration 73/1000 | Loss: 0.00001947
Iteration 74/1000 | Loss: 0.00001947
Iteration 75/1000 | Loss: 0.00001947
Iteration 76/1000 | Loss: 0.00001947
Iteration 77/1000 | Loss: 0.00001947
Iteration 78/1000 | Loss: 0.00001946
Iteration 79/1000 | Loss: 0.00001946
Iteration 80/1000 | Loss: 0.00001946
Iteration 81/1000 | Loss: 0.00001945
Iteration 82/1000 | Loss: 0.00001945
Iteration 83/1000 | Loss: 0.00001944
Iteration 84/1000 | Loss: 0.00001944
Iteration 85/1000 | Loss: 0.00001944
Iteration 86/1000 | Loss: 0.00001944
Iteration 87/1000 | Loss: 0.00001943
Iteration 88/1000 | Loss: 0.00001943
Iteration 89/1000 | Loss: 0.00001942
Iteration 90/1000 | Loss: 0.00001942
Iteration 91/1000 | Loss: 0.00001942
Iteration 92/1000 | Loss: 0.00001942
Iteration 93/1000 | Loss: 0.00001942
Iteration 94/1000 | Loss: 0.00001941
Iteration 95/1000 | Loss: 0.00001941
Iteration 96/1000 | Loss: 0.00001941
Iteration 97/1000 | Loss: 0.00001941
Iteration 98/1000 | Loss: 0.00001941
Iteration 99/1000 | Loss: 0.00001941
Iteration 100/1000 | Loss: 0.00001941
Iteration 101/1000 | Loss: 0.00001941
Iteration 102/1000 | Loss: 0.00001941
Iteration 103/1000 | Loss: 0.00001941
Iteration 104/1000 | Loss: 0.00001941
Iteration 105/1000 | Loss: 0.00001941
Iteration 106/1000 | Loss: 0.00001941
Iteration 107/1000 | Loss: 0.00001941
Iteration 108/1000 | Loss: 0.00001941
Iteration 109/1000 | Loss: 0.00001941
Iteration 110/1000 | Loss: 0.00001941
Iteration 111/1000 | Loss: 0.00001941
Iteration 112/1000 | Loss: 0.00001941
Iteration 113/1000 | Loss: 0.00001941
Iteration 114/1000 | Loss: 0.00001941
Iteration 115/1000 | Loss: 0.00001941
Iteration 116/1000 | Loss: 0.00001941
Iteration 117/1000 | Loss: 0.00001941
Iteration 118/1000 | Loss: 0.00001941
Iteration 119/1000 | Loss: 0.00001941
Iteration 120/1000 | Loss: 0.00001941
Iteration 121/1000 | Loss: 0.00001941
Iteration 122/1000 | Loss: 0.00001941
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.9409524611546658e-05, 1.9409524611546658e-05, 1.9409524611546658e-05, 1.9409524611546658e-05, 1.9409524611546658e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9409524611546658e-05

Optimization complete. Final v2v error: 3.732267379760742 mm

Highest mean error: 4.2479729652404785 mm for frame 136

Lowest mean error: 3.4686272144317627 mm for frame 109

Saving results

Total time: 31.113054990768433
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1061/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1061.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1061
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00772130
Iteration 2/25 | Loss: 0.00147888
Iteration 3/25 | Loss: 0.00128037
Iteration 4/25 | Loss: 0.00125745
Iteration 5/25 | Loss: 0.00126383
Iteration 6/25 | Loss: 0.00125587
Iteration 7/25 | Loss: 0.00124806
Iteration 8/25 | Loss: 0.00124500
Iteration 9/25 | Loss: 0.00124575
Iteration 10/25 | Loss: 0.00124238
Iteration 11/25 | Loss: 0.00124435
Iteration 12/25 | Loss: 0.00124443
Iteration 13/25 | Loss: 0.00124132
Iteration 14/25 | Loss: 0.00123997
Iteration 15/25 | Loss: 0.00123949
Iteration 16/25 | Loss: 0.00124164
Iteration 17/25 | Loss: 0.00124116
Iteration 18/25 | Loss: 0.00123934
Iteration 19/25 | Loss: 0.00123816
Iteration 20/25 | Loss: 0.00123732
Iteration 21/25 | Loss: 0.00123710
Iteration 22/25 | Loss: 0.00123703
Iteration 23/25 | Loss: 0.00123703
Iteration 24/25 | Loss: 0.00123703
Iteration 25/25 | Loss: 0.00123703

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.85057294
Iteration 2/25 | Loss: 0.00130694
Iteration 3/25 | Loss: 0.00130693
Iteration 4/25 | Loss: 0.00130693
Iteration 5/25 | Loss: 0.00130693
Iteration 6/25 | Loss: 0.00130693
Iteration 7/25 | Loss: 0.00130693
Iteration 8/25 | Loss: 0.00130693
Iteration 9/25 | Loss: 0.00130693
Iteration 10/25 | Loss: 0.00130693
Iteration 11/25 | Loss: 0.00130693
Iteration 12/25 | Loss: 0.00130693
Iteration 13/25 | Loss: 0.00130693
Iteration 14/25 | Loss: 0.00130693
Iteration 15/25 | Loss: 0.00130693
Iteration 16/25 | Loss: 0.00130693
Iteration 17/25 | Loss: 0.00130693
Iteration 18/25 | Loss: 0.00130693
Iteration 19/25 | Loss: 0.00130693
Iteration 20/25 | Loss: 0.00130693
Iteration 21/25 | Loss: 0.00130693
Iteration 22/25 | Loss: 0.00130693
Iteration 23/25 | Loss: 0.00130693
Iteration 24/25 | Loss: 0.00130693
Iteration 25/25 | Loss: 0.00130693
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0013069284614175558, 0.0013069284614175558, 0.0013069284614175558, 0.0013069284614175558, 0.0013069284614175558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013069284614175558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130693
Iteration 2/1000 | Loss: 0.00002611
Iteration 3/1000 | Loss: 0.00002065
Iteration 4/1000 | Loss: 0.00016655
Iteration 5/1000 | Loss: 0.00016928
Iteration 6/1000 | Loss: 0.00008269
Iteration 7/1000 | Loss: 0.00009953
Iteration 8/1000 | Loss: 0.00009316
Iteration 9/1000 | Loss: 0.00001814
Iteration 10/1000 | Loss: 0.00001709
Iteration 11/1000 | Loss: 0.00017245
Iteration 12/1000 | Loss: 0.00003259
Iteration 13/1000 | Loss: 0.00001679
Iteration 14/1000 | Loss: 0.00018383
Iteration 15/1000 | Loss: 0.00010172
Iteration 16/1000 | Loss: 0.00014976
Iteration 17/1000 | Loss: 0.00009465
Iteration 18/1000 | Loss: 0.00005601
Iteration 19/1000 | Loss: 0.00001791
Iteration 20/1000 | Loss: 0.00018500
Iteration 21/1000 | Loss: 0.00020756
Iteration 22/1000 | Loss: 0.00016857
Iteration 23/1000 | Loss: 0.00013443
Iteration 24/1000 | Loss: 0.00014452
Iteration 25/1000 | Loss: 0.00009471
Iteration 26/1000 | Loss: 0.00013493
Iteration 27/1000 | Loss: 0.00008871
Iteration 28/1000 | Loss: 0.00013155
Iteration 29/1000 | Loss: 0.00008129
Iteration 30/1000 | Loss: 0.00008145
Iteration 31/1000 | Loss: 0.00002008
Iteration 32/1000 | Loss: 0.00015135
Iteration 33/1000 | Loss: 0.00006230
Iteration 34/1000 | Loss: 0.00001669
Iteration 35/1000 | Loss: 0.00016610
Iteration 36/1000 | Loss: 0.00014482
Iteration 37/1000 | Loss: 0.00012402
Iteration 38/1000 | Loss: 0.00002039
Iteration 39/1000 | Loss: 0.00001610
Iteration 40/1000 | Loss: 0.00015872
Iteration 41/1000 | Loss: 0.00009149
Iteration 42/1000 | Loss: 0.00010656
Iteration 43/1000 | Loss: 0.00008950
Iteration 44/1000 | Loss: 0.00005358
Iteration 45/1000 | Loss: 0.00001901
Iteration 46/1000 | Loss: 0.00008345
Iteration 47/1000 | Loss: 0.00013231
Iteration 48/1000 | Loss: 0.00012720
Iteration 49/1000 | Loss: 0.00002326
Iteration 50/1000 | Loss: 0.00013468
Iteration 51/1000 | Loss: 0.00015924
Iteration 52/1000 | Loss: 0.00012911
Iteration 53/1000 | Loss: 0.00015893
Iteration 54/1000 | Loss: 0.00017596
Iteration 55/1000 | Loss: 0.00002024
Iteration 56/1000 | Loss: 0.00001644
Iteration 57/1000 | Loss: 0.00001570
Iteration 58/1000 | Loss: 0.00001526
Iteration 59/1000 | Loss: 0.00001483
Iteration 60/1000 | Loss: 0.00001463
Iteration 61/1000 | Loss: 0.00001439
Iteration 62/1000 | Loss: 0.00001433
Iteration 63/1000 | Loss: 0.00001431
Iteration 64/1000 | Loss: 0.00001430
Iteration 65/1000 | Loss: 0.00001429
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001423
Iteration 68/1000 | Loss: 0.00001419
Iteration 69/1000 | Loss: 0.00001419
Iteration 70/1000 | Loss: 0.00001419
Iteration 71/1000 | Loss: 0.00001419
Iteration 72/1000 | Loss: 0.00001419
Iteration 73/1000 | Loss: 0.00001418
Iteration 74/1000 | Loss: 0.00001418
Iteration 75/1000 | Loss: 0.00001418
Iteration 76/1000 | Loss: 0.00001418
Iteration 77/1000 | Loss: 0.00001418
Iteration 78/1000 | Loss: 0.00001418
Iteration 79/1000 | Loss: 0.00001418
Iteration 80/1000 | Loss: 0.00001418
Iteration 81/1000 | Loss: 0.00001418
Iteration 82/1000 | Loss: 0.00001418
Iteration 83/1000 | Loss: 0.00001417
Iteration 84/1000 | Loss: 0.00001417
Iteration 85/1000 | Loss: 0.00001416
Iteration 86/1000 | Loss: 0.00001416
Iteration 87/1000 | Loss: 0.00001415
Iteration 88/1000 | Loss: 0.00001415
Iteration 89/1000 | Loss: 0.00001415
Iteration 90/1000 | Loss: 0.00001414
Iteration 91/1000 | Loss: 0.00001414
Iteration 92/1000 | Loss: 0.00001413
Iteration 93/1000 | Loss: 0.00001412
Iteration 94/1000 | Loss: 0.00001409
Iteration 95/1000 | Loss: 0.00001409
Iteration 96/1000 | Loss: 0.00001409
Iteration 97/1000 | Loss: 0.00001409
Iteration 98/1000 | Loss: 0.00001409
Iteration 99/1000 | Loss: 0.00001409
Iteration 100/1000 | Loss: 0.00001409
Iteration 101/1000 | Loss: 0.00001409
Iteration 102/1000 | Loss: 0.00001408
Iteration 103/1000 | Loss: 0.00001408
Iteration 104/1000 | Loss: 0.00001407
Iteration 105/1000 | Loss: 0.00001406
Iteration 106/1000 | Loss: 0.00001406
Iteration 107/1000 | Loss: 0.00001405
Iteration 108/1000 | Loss: 0.00001405
Iteration 109/1000 | Loss: 0.00001404
Iteration 110/1000 | Loss: 0.00001404
Iteration 111/1000 | Loss: 0.00001403
Iteration 112/1000 | Loss: 0.00001403
Iteration 113/1000 | Loss: 0.00001401
Iteration 114/1000 | Loss: 0.00001401
Iteration 115/1000 | Loss: 0.00001401
Iteration 116/1000 | Loss: 0.00001400
Iteration 117/1000 | Loss: 0.00001400
Iteration 118/1000 | Loss: 0.00001400
Iteration 119/1000 | Loss: 0.00001400
Iteration 120/1000 | Loss: 0.00001400
Iteration 121/1000 | Loss: 0.00001400
Iteration 122/1000 | Loss: 0.00001400
Iteration 123/1000 | Loss: 0.00001400
Iteration 124/1000 | Loss: 0.00001400
Iteration 125/1000 | Loss: 0.00001400
Iteration 126/1000 | Loss: 0.00001399
Iteration 127/1000 | Loss: 0.00001399
Iteration 128/1000 | Loss: 0.00001399
Iteration 129/1000 | Loss: 0.00001399
Iteration 130/1000 | Loss: 0.00001398
Iteration 131/1000 | Loss: 0.00001398
Iteration 132/1000 | Loss: 0.00001397
Iteration 133/1000 | Loss: 0.00001397
Iteration 134/1000 | Loss: 0.00001397
Iteration 135/1000 | Loss: 0.00001397
Iteration 136/1000 | Loss: 0.00001397
Iteration 137/1000 | Loss: 0.00001397
Iteration 138/1000 | Loss: 0.00001397
Iteration 139/1000 | Loss: 0.00001397
Iteration 140/1000 | Loss: 0.00001397
Iteration 141/1000 | Loss: 0.00001396
Iteration 142/1000 | Loss: 0.00001396
Iteration 143/1000 | Loss: 0.00001396
Iteration 144/1000 | Loss: 0.00001396
Iteration 145/1000 | Loss: 0.00001396
Iteration 146/1000 | Loss: 0.00001396
Iteration 147/1000 | Loss: 0.00001396
Iteration 148/1000 | Loss: 0.00001395
Iteration 149/1000 | Loss: 0.00001395
Iteration 150/1000 | Loss: 0.00001395
Iteration 151/1000 | Loss: 0.00001395
Iteration 152/1000 | Loss: 0.00001395
Iteration 153/1000 | Loss: 0.00001395
Iteration 154/1000 | Loss: 0.00001395
Iteration 155/1000 | Loss: 0.00001395
Iteration 156/1000 | Loss: 0.00001394
Iteration 157/1000 | Loss: 0.00001394
Iteration 158/1000 | Loss: 0.00001394
Iteration 159/1000 | Loss: 0.00001394
Iteration 160/1000 | Loss: 0.00001394
Iteration 161/1000 | Loss: 0.00001394
Iteration 162/1000 | Loss: 0.00001394
Iteration 163/1000 | Loss: 0.00001394
Iteration 164/1000 | Loss: 0.00001393
Iteration 165/1000 | Loss: 0.00001393
Iteration 166/1000 | Loss: 0.00001393
Iteration 167/1000 | Loss: 0.00001393
Iteration 168/1000 | Loss: 0.00001393
Iteration 169/1000 | Loss: 0.00001393
Iteration 170/1000 | Loss: 0.00001393
Iteration 171/1000 | Loss: 0.00001393
Iteration 172/1000 | Loss: 0.00001393
Iteration 173/1000 | Loss: 0.00001393
Iteration 174/1000 | Loss: 0.00001393
Iteration 175/1000 | Loss: 0.00001393
Iteration 176/1000 | Loss: 0.00001393
Iteration 177/1000 | Loss: 0.00001393
Iteration 178/1000 | Loss: 0.00001393
Iteration 179/1000 | Loss: 0.00001392
Iteration 180/1000 | Loss: 0.00001392
Iteration 181/1000 | Loss: 0.00001392
Iteration 182/1000 | Loss: 0.00001392
Iteration 183/1000 | Loss: 0.00001392
Iteration 184/1000 | Loss: 0.00001392
Iteration 185/1000 | Loss: 0.00001392
Iteration 186/1000 | Loss: 0.00001392
Iteration 187/1000 | Loss: 0.00001392
Iteration 188/1000 | Loss: 0.00001392
Iteration 189/1000 | Loss: 0.00001392
Iteration 190/1000 | Loss: 0.00001392
Iteration 191/1000 | Loss: 0.00001392
Iteration 192/1000 | Loss: 0.00001392
Iteration 193/1000 | Loss: 0.00001392
Iteration 194/1000 | Loss: 0.00001392
Iteration 195/1000 | Loss: 0.00001391
Iteration 196/1000 | Loss: 0.00001391
Iteration 197/1000 | Loss: 0.00001391
Iteration 198/1000 | Loss: 0.00001391
Iteration 199/1000 | Loss: 0.00001391
Iteration 200/1000 | Loss: 0.00001391
Iteration 201/1000 | Loss: 0.00001391
Iteration 202/1000 | Loss: 0.00001390
Iteration 203/1000 | Loss: 0.00001390
Iteration 204/1000 | Loss: 0.00001390
Iteration 205/1000 | Loss: 0.00001390
Iteration 206/1000 | Loss: 0.00001389
Iteration 207/1000 | Loss: 0.00001389
Iteration 208/1000 | Loss: 0.00001389
Iteration 209/1000 | Loss: 0.00001389
Iteration 210/1000 | Loss: 0.00001389
Iteration 211/1000 | Loss: 0.00001389
Iteration 212/1000 | Loss: 0.00001389
Iteration 213/1000 | Loss: 0.00001389
Iteration 214/1000 | Loss: 0.00001389
Iteration 215/1000 | Loss: 0.00001389
Iteration 216/1000 | Loss: 0.00001389
Iteration 217/1000 | Loss: 0.00001389
Iteration 218/1000 | Loss: 0.00001389
Iteration 219/1000 | Loss: 0.00001388
Iteration 220/1000 | Loss: 0.00001388
Iteration 221/1000 | Loss: 0.00001388
Iteration 222/1000 | Loss: 0.00001388
Iteration 223/1000 | Loss: 0.00001388
Iteration 224/1000 | Loss: 0.00001388
Iteration 225/1000 | Loss: 0.00001388
Iteration 226/1000 | Loss: 0.00001388
Iteration 227/1000 | Loss: 0.00001388
Iteration 228/1000 | Loss: 0.00001388
Iteration 229/1000 | Loss: 0.00001388
Iteration 230/1000 | Loss: 0.00001388
Iteration 231/1000 | Loss: 0.00001388
Iteration 232/1000 | Loss: 0.00001388
Iteration 233/1000 | Loss: 0.00001388
Iteration 234/1000 | Loss: 0.00001388
Iteration 235/1000 | Loss: 0.00001388
Iteration 236/1000 | Loss: 0.00001388
Iteration 237/1000 | Loss: 0.00001388
Iteration 238/1000 | Loss: 0.00001388
Iteration 239/1000 | Loss: 0.00001388
Iteration 240/1000 | Loss: 0.00001388
Iteration 241/1000 | Loss: 0.00001388
Iteration 242/1000 | Loss: 0.00001388
Iteration 243/1000 | Loss: 0.00001388
Iteration 244/1000 | Loss: 0.00001388
Iteration 245/1000 | Loss: 0.00001388
Iteration 246/1000 | Loss: 0.00001388
Iteration 247/1000 | Loss: 0.00001388
Iteration 248/1000 | Loss: 0.00001388
Iteration 249/1000 | Loss: 0.00001388
Iteration 250/1000 | Loss: 0.00001388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 250. Stopping optimization.
Last 5 losses: [1.3882007806387264e-05, 1.3882007806387264e-05, 1.3882007806387264e-05, 1.3882007806387264e-05, 1.3882007806387264e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3882007806387264e-05

Optimization complete. Final v2v error: 3.134324550628662 mm

Highest mean error: 4.108519554138184 mm for frame 38

Lowest mean error: 2.753235101699829 mm for frame 187

Saving results

Total time: 152.4331259727478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00788620
Iteration 2/25 | Loss: 0.00148132
Iteration 3/25 | Loss: 0.00126928
Iteration 4/25 | Loss: 0.00124271
Iteration 5/25 | Loss: 0.00123357
Iteration 6/25 | Loss: 0.00123182
Iteration 7/25 | Loss: 0.00123182
Iteration 8/25 | Loss: 0.00123182
Iteration 9/25 | Loss: 0.00123182
Iteration 10/25 | Loss: 0.00123182
Iteration 11/25 | Loss: 0.00123182
Iteration 12/25 | Loss: 0.00123182
Iteration 13/25 | Loss: 0.00123182
Iteration 14/25 | Loss: 0.00123182
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0012318195076659322, 0.0012318195076659322, 0.0012318195076659322, 0.0012318195076659322, 0.0012318195076659322]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012318195076659322

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70724177
Iteration 2/25 | Loss: 0.00097784
Iteration 3/25 | Loss: 0.00097784
Iteration 4/25 | Loss: 0.00097784
Iteration 5/25 | Loss: 0.00097784
Iteration 6/25 | Loss: 0.00097784
Iteration 7/25 | Loss: 0.00097784
Iteration 8/25 | Loss: 0.00097784
Iteration 9/25 | Loss: 0.00097784
Iteration 10/25 | Loss: 0.00097784
Iteration 11/25 | Loss: 0.00097783
Iteration 12/25 | Loss: 0.00097783
Iteration 13/25 | Loss: 0.00097783
Iteration 14/25 | Loss: 0.00097783
Iteration 15/25 | Loss: 0.00097783
Iteration 16/25 | Loss: 0.00097783
Iteration 17/25 | Loss: 0.00097783
Iteration 18/25 | Loss: 0.00097783
Iteration 19/25 | Loss: 0.00097783
Iteration 20/25 | Loss: 0.00097783
Iteration 21/25 | Loss: 0.00097783
Iteration 22/25 | Loss: 0.00097783
Iteration 23/25 | Loss: 0.00097783
Iteration 24/25 | Loss: 0.00097784
Iteration 25/25 | Loss: 0.00097783

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097783
Iteration 2/1000 | Loss: 0.00003672
Iteration 3/1000 | Loss: 0.00002543
Iteration 4/1000 | Loss: 0.00002283
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00001988
Iteration 7/1000 | Loss: 0.00001933
Iteration 8/1000 | Loss: 0.00001892
Iteration 9/1000 | Loss: 0.00001850
Iteration 10/1000 | Loss: 0.00001816
Iteration 11/1000 | Loss: 0.00001794
Iteration 12/1000 | Loss: 0.00001780
Iteration 13/1000 | Loss: 0.00001779
Iteration 14/1000 | Loss: 0.00001778
Iteration 15/1000 | Loss: 0.00001773
Iteration 16/1000 | Loss: 0.00001773
Iteration 17/1000 | Loss: 0.00001772
Iteration 18/1000 | Loss: 0.00001771
Iteration 19/1000 | Loss: 0.00001771
Iteration 20/1000 | Loss: 0.00001767
Iteration 21/1000 | Loss: 0.00001761
Iteration 22/1000 | Loss: 0.00001749
Iteration 23/1000 | Loss: 0.00001742
Iteration 24/1000 | Loss: 0.00001741
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001738
Iteration 27/1000 | Loss: 0.00001734
Iteration 28/1000 | Loss: 0.00001732
Iteration 29/1000 | Loss: 0.00001730
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001729
Iteration 32/1000 | Loss: 0.00001729
Iteration 33/1000 | Loss: 0.00001728
Iteration 34/1000 | Loss: 0.00001728
Iteration 35/1000 | Loss: 0.00001728
Iteration 36/1000 | Loss: 0.00001728
Iteration 37/1000 | Loss: 0.00001728
Iteration 38/1000 | Loss: 0.00001728
Iteration 39/1000 | Loss: 0.00001728
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001727
Iteration 44/1000 | Loss: 0.00001727
Iteration 45/1000 | Loss: 0.00001726
Iteration 46/1000 | Loss: 0.00001726
Iteration 47/1000 | Loss: 0.00001726
Iteration 48/1000 | Loss: 0.00001726
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001725
Iteration 52/1000 | Loss: 0.00001725
Iteration 53/1000 | Loss: 0.00001725
Iteration 54/1000 | Loss: 0.00001724
Iteration 55/1000 | Loss: 0.00001724
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001723
Iteration 58/1000 | Loss: 0.00001723
Iteration 59/1000 | Loss: 0.00001723
Iteration 60/1000 | Loss: 0.00001723
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001723
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001722
Iteration 66/1000 | Loss: 0.00001722
Iteration 67/1000 | Loss: 0.00001722
Iteration 68/1000 | Loss: 0.00001722
Iteration 69/1000 | Loss: 0.00001722
Iteration 70/1000 | Loss: 0.00001722
Iteration 71/1000 | Loss: 0.00001722
Iteration 72/1000 | Loss: 0.00001722
Iteration 73/1000 | Loss: 0.00001722
Iteration 74/1000 | Loss: 0.00001722
Iteration 75/1000 | Loss: 0.00001722
Iteration 76/1000 | Loss: 0.00001722
Iteration 77/1000 | Loss: 0.00001721
Iteration 78/1000 | Loss: 0.00001721
Iteration 79/1000 | Loss: 0.00001721
Iteration 80/1000 | Loss: 0.00001721
Iteration 81/1000 | Loss: 0.00001720
Iteration 82/1000 | Loss: 0.00001720
Iteration 83/1000 | Loss: 0.00001720
Iteration 84/1000 | Loss: 0.00001720
Iteration 85/1000 | Loss: 0.00001719
Iteration 86/1000 | Loss: 0.00001719
Iteration 87/1000 | Loss: 0.00001719
Iteration 88/1000 | Loss: 0.00001719
Iteration 89/1000 | Loss: 0.00001719
Iteration 90/1000 | Loss: 0.00001719
Iteration 91/1000 | Loss: 0.00001719
Iteration 92/1000 | Loss: 0.00001719
Iteration 93/1000 | Loss: 0.00001719
Iteration 94/1000 | Loss: 0.00001719
Iteration 95/1000 | Loss: 0.00001719
Iteration 96/1000 | Loss: 0.00001719
Iteration 97/1000 | Loss: 0.00001718
Iteration 98/1000 | Loss: 0.00001718
Iteration 99/1000 | Loss: 0.00001718
Iteration 100/1000 | Loss: 0.00001718
Iteration 101/1000 | Loss: 0.00001718
Iteration 102/1000 | Loss: 0.00001718
Iteration 103/1000 | Loss: 0.00001718
Iteration 104/1000 | Loss: 0.00001718
Iteration 105/1000 | Loss: 0.00001718
Iteration 106/1000 | Loss: 0.00001718
Iteration 107/1000 | Loss: 0.00001718
Iteration 108/1000 | Loss: 0.00001718
Iteration 109/1000 | Loss: 0.00001718
Iteration 110/1000 | Loss: 0.00001718
Iteration 111/1000 | Loss: 0.00001718
Iteration 112/1000 | Loss: 0.00001718
Iteration 113/1000 | Loss: 0.00001718
Iteration 114/1000 | Loss: 0.00001718
Iteration 115/1000 | Loss: 0.00001718
Iteration 116/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 116. Stopping optimization.
Last 5 losses: [1.7184140233439393e-05, 1.7184140233439393e-05, 1.7184140233439393e-05, 1.7184140233439393e-05, 1.7184140233439393e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7184140233439393e-05

Optimization complete. Final v2v error: 3.5542123317718506 mm

Highest mean error: 3.8866052627563477 mm for frame 10

Lowest mean error: 3.196021318435669 mm for frame 134

Saving results

Total time: 39.236913204193115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1082/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1082.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1082
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01042080
Iteration 2/25 | Loss: 0.01042080
Iteration 3/25 | Loss: 0.00220188
Iteration 4/25 | Loss: 0.00176484
Iteration 5/25 | Loss: 0.00168249
Iteration 6/25 | Loss: 0.00146388
Iteration 7/25 | Loss: 0.00148213
Iteration 8/25 | Loss: 0.00143743
Iteration 9/25 | Loss: 0.00145226
Iteration 10/25 | Loss: 0.00140357
Iteration 11/25 | Loss: 0.00134074
Iteration 12/25 | Loss: 0.00133329
Iteration 13/25 | Loss: 0.00131377
Iteration 14/25 | Loss: 0.00131359
Iteration 15/25 | Loss: 0.00130809
Iteration 16/25 | Loss: 0.00130620
Iteration 17/25 | Loss: 0.00130348
Iteration 18/25 | Loss: 0.00130436
Iteration 19/25 | Loss: 0.00129554
Iteration 20/25 | Loss: 0.00129831
Iteration 21/25 | Loss: 0.00130038
Iteration 22/25 | Loss: 0.00130024
Iteration 23/25 | Loss: 0.00129209
Iteration 24/25 | Loss: 0.00129415
Iteration 25/25 | Loss: 0.00129348

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.37755227
Iteration 2/25 | Loss: 0.00177466
Iteration 3/25 | Loss: 0.00169584
Iteration 4/25 | Loss: 0.00169584
Iteration 5/25 | Loss: 0.00169584
Iteration 6/25 | Loss: 0.00169584
Iteration 7/25 | Loss: 0.00169584
Iteration 8/25 | Loss: 0.00169584
Iteration 9/25 | Loss: 0.00169584
Iteration 10/25 | Loss: 0.00169584
Iteration 11/25 | Loss: 0.00169584
Iteration 12/25 | Loss: 0.00169584
Iteration 13/25 | Loss: 0.00169584
Iteration 14/25 | Loss: 0.00169584
Iteration 15/25 | Loss: 0.00169584
Iteration 16/25 | Loss: 0.00169584
Iteration 17/25 | Loss: 0.00169584
Iteration 18/25 | Loss: 0.00169584
Iteration 19/25 | Loss: 0.00169584
Iteration 20/25 | Loss: 0.00169584
Iteration 21/25 | Loss: 0.00169584
Iteration 22/25 | Loss: 0.00169584
Iteration 23/25 | Loss: 0.00169584
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.001695837127044797, 0.001695837127044797, 0.001695837127044797, 0.001695837127044797, 0.001695837127044797]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001695837127044797

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00169584
Iteration 2/1000 | Loss: 0.00042018
Iteration 3/1000 | Loss: 0.00032810
Iteration 4/1000 | Loss: 0.00024668
Iteration 5/1000 | Loss: 0.00023139
Iteration 6/1000 | Loss: 0.00049047
Iteration 7/1000 | Loss: 0.00055382
Iteration 8/1000 | Loss: 0.00060547
Iteration 9/1000 | Loss: 0.00051131
Iteration 10/1000 | Loss: 0.00026101
Iteration 11/1000 | Loss: 0.00026684
Iteration 12/1000 | Loss: 0.00040309
Iteration 13/1000 | Loss: 0.00023228
Iteration 14/1000 | Loss: 0.00023555
Iteration 15/1000 | Loss: 0.00022189
Iteration 16/1000 | Loss: 0.00024255
Iteration 17/1000 | Loss: 0.00038649
Iteration 18/1000 | Loss: 0.00051188
Iteration 19/1000 | Loss: 0.00032700
Iteration 20/1000 | Loss: 0.00032266
Iteration 21/1000 | Loss: 0.00045070
Iteration 22/1000 | Loss: 0.00041653
Iteration 23/1000 | Loss: 0.00041331
Iteration 24/1000 | Loss: 0.00031207
Iteration 25/1000 | Loss: 0.00023898
Iteration 26/1000 | Loss: 0.00022141
Iteration 27/1000 | Loss: 0.00030547
Iteration 28/1000 | Loss: 0.00029937
Iteration 29/1000 | Loss: 0.00011635
Iteration 30/1000 | Loss: 0.00032407
Iteration 31/1000 | Loss: 0.00025036
Iteration 32/1000 | Loss: 0.00026453
Iteration 33/1000 | Loss: 0.00008587
Iteration 34/1000 | Loss: 0.00022125
Iteration 35/1000 | Loss: 0.00018206
Iteration 36/1000 | Loss: 0.00020804
Iteration 37/1000 | Loss: 0.00013636
Iteration 38/1000 | Loss: 0.00012999
Iteration 39/1000 | Loss: 0.00031510
Iteration 40/1000 | Loss: 0.00018228
Iteration 41/1000 | Loss: 0.00032912
Iteration 42/1000 | Loss: 0.00044467
Iteration 43/1000 | Loss: 0.00060225
Iteration 44/1000 | Loss: 0.00044064
Iteration 45/1000 | Loss: 0.00046430
Iteration 46/1000 | Loss: 0.00023575
Iteration 47/1000 | Loss: 0.00011469
Iteration 48/1000 | Loss: 0.00024252
Iteration 49/1000 | Loss: 0.00017141
Iteration 50/1000 | Loss: 0.00029077
Iteration 51/1000 | Loss: 0.00021072
Iteration 52/1000 | Loss: 0.00027637
Iteration 53/1000 | Loss: 0.00031894
Iteration 54/1000 | Loss: 0.00026990
Iteration 55/1000 | Loss: 0.00060778
Iteration 56/1000 | Loss: 0.00028679
Iteration 57/1000 | Loss: 0.00053823
Iteration 58/1000 | Loss: 0.00034701
Iteration 59/1000 | Loss: 0.00018413
Iteration 60/1000 | Loss: 0.00109365
Iteration 61/1000 | Loss: 0.00047145
Iteration 62/1000 | Loss: 0.00052287
Iteration 63/1000 | Loss: 0.00019563
Iteration 64/1000 | Loss: 0.00017132
Iteration 65/1000 | Loss: 0.00037156
Iteration 66/1000 | Loss: 0.00019504
Iteration 67/1000 | Loss: 0.00010955
Iteration 68/1000 | Loss: 0.00006539
Iteration 69/1000 | Loss: 0.00005815
Iteration 70/1000 | Loss: 0.00016837
Iteration 71/1000 | Loss: 0.00024589
Iteration 72/1000 | Loss: 0.00026658
Iteration 73/1000 | Loss: 0.00015932
Iteration 74/1000 | Loss: 0.00006039
Iteration 75/1000 | Loss: 0.00020571
Iteration 76/1000 | Loss: 0.00015363
Iteration 77/1000 | Loss: 0.00010724
Iteration 78/1000 | Loss: 0.00007165
Iteration 79/1000 | Loss: 0.00006968
Iteration 80/1000 | Loss: 0.00009086
Iteration 81/1000 | Loss: 0.00004379
Iteration 82/1000 | Loss: 0.00010501
Iteration 83/1000 | Loss: 0.00010844
Iteration 84/1000 | Loss: 0.00016755
Iteration 85/1000 | Loss: 0.00028592
Iteration 86/1000 | Loss: 0.00019369
Iteration 87/1000 | Loss: 0.00021298
Iteration 88/1000 | Loss: 0.00029534
Iteration 89/1000 | Loss: 0.00013265
Iteration 90/1000 | Loss: 0.00009996
Iteration 91/1000 | Loss: 0.00009977
Iteration 92/1000 | Loss: 0.00009606
Iteration 93/1000 | Loss: 0.00009671
Iteration 94/1000 | Loss: 0.00016957
Iteration 95/1000 | Loss: 0.00029374
Iteration 96/1000 | Loss: 0.00007953
Iteration 97/1000 | Loss: 0.00004484
Iteration 98/1000 | Loss: 0.00010317
Iteration 99/1000 | Loss: 0.00003696
Iteration 100/1000 | Loss: 0.00003500
Iteration 101/1000 | Loss: 0.00087433
Iteration 102/1000 | Loss: 0.00062012
Iteration 103/1000 | Loss: 0.00077129
Iteration 104/1000 | Loss: 0.00008891
Iteration 105/1000 | Loss: 0.00033459
Iteration 106/1000 | Loss: 0.00009641
Iteration 107/1000 | Loss: 0.00007301
Iteration 108/1000 | Loss: 0.00003465
Iteration 109/1000 | Loss: 0.00009766
Iteration 110/1000 | Loss: 0.00003082
Iteration 111/1000 | Loss: 0.00007774
Iteration 112/1000 | Loss: 0.00002899
Iteration 113/1000 | Loss: 0.00002763
Iteration 114/1000 | Loss: 0.00005836
Iteration 115/1000 | Loss: 0.00003040
Iteration 116/1000 | Loss: 0.00002627
Iteration 117/1000 | Loss: 0.00002544
Iteration 118/1000 | Loss: 0.00022852
Iteration 119/1000 | Loss: 0.00005533
Iteration 120/1000 | Loss: 0.00003247
Iteration 121/1000 | Loss: 0.00006166
Iteration 122/1000 | Loss: 0.00005848
Iteration 123/1000 | Loss: 0.00002550
Iteration 124/1000 | Loss: 0.00003157
Iteration 125/1000 | Loss: 0.00002500
Iteration 126/1000 | Loss: 0.00002459
Iteration 127/1000 | Loss: 0.00005391
Iteration 128/1000 | Loss: 0.00004671
Iteration 129/1000 | Loss: 0.00005738
Iteration 130/1000 | Loss: 0.00004487
Iteration 131/1000 | Loss: 0.00005747
Iteration 132/1000 | Loss: 0.00004474
Iteration 133/1000 | Loss: 0.00002726
Iteration 134/1000 | Loss: 0.00007096
Iteration 135/1000 | Loss: 0.00012607
Iteration 136/1000 | Loss: 0.00015220
Iteration 137/1000 | Loss: 0.00002688
Iteration 138/1000 | Loss: 0.00002538
Iteration 139/1000 | Loss: 0.00002416
Iteration 140/1000 | Loss: 0.00006169
Iteration 141/1000 | Loss: 0.00002398
Iteration 142/1000 | Loss: 0.00009898
Iteration 143/1000 | Loss: 0.00003082
Iteration 144/1000 | Loss: 0.00006272
Iteration 145/1000 | Loss: 0.00002578
Iteration 146/1000 | Loss: 0.00002491
Iteration 147/1000 | Loss: 0.00002455
Iteration 148/1000 | Loss: 0.00002433
Iteration 149/1000 | Loss: 0.00002424
Iteration 150/1000 | Loss: 0.00002403
Iteration 151/1000 | Loss: 0.00002395
Iteration 152/1000 | Loss: 0.00002386
Iteration 153/1000 | Loss: 0.00002384
Iteration 154/1000 | Loss: 0.00002358
Iteration 155/1000 | Loss: 0.00008308
Iteration 156/1000 | Loss: 0.00005344
Iteration 157/1000 | Loss: 0.00016872
Iteration 158/1000 | Loss: 0.00007158
Iteration 159/1000 | Loss: 0.00002424
Iteration 160/1000 | Loss: 0.00006221
Iteration 161/1000 | Loss: 0.00007271
Iteration 162/1000 | Loss: 0.00015191
Iteration 163/1000 | Loss: 0.00026810
Iteration 164/1000 | Loss: 0.00013157
Iteration 165/1000 | Loss: 0.00008457
Iteration 166/1000 | Loss: 0.00028729
Iteration 167/1000 | Loss: 0.00002358
Iteration 168/1000 | Loss: 0.00002295
Iteration 169/1000 | Loss: 0.00019010
Iteration 170/1000 | Loss: 0.00002474
Iteration 171/1000 | Loss: 0.00018507
Iteration 172/1000 | Loss: 0.00002813
Iteration 173/1000 | Loss: 0.00002277
Iteration 174/1000 | Loss: 0.00002125
Iteration 175/1000 | Loss: 0.00002061
Iteration 176/1000 | Loss: 0.00002035
Iteration 177/1000 | Loss: 0.00002018
Iteration 178/1000 | Loss: 0.00002012
Iteration 179/1000 | Loss: 0.00002007
Iteration 180/1000 | Loss: 0.00002007
Iteration 181/1000 | Loss: 0.00002006
Iteration 182/1000 | Loss: 0.00001999
Iteration 183/1000 | Loss: 0.00001998
Iteration 184/1000 | Loss: 0.00001998
Iteration 185/1000 | Loss: 0.00001997
Iteration 186/1000 | Loss: 0.00001996
Iteration 187/1000 | Loss: 0.00001995
Iteration 188/1000 | Loss: 0.00001990
Iteration 189/1000 | Loss: 0.00001990
Iteration 190/1000 | Loss: 0.00001990
Iteration 191/1000 | Loss: 0.00001990
Iteration 192/1000 | Loss: 0.00001990
Iteration 193/1000 | Loss: 0.00001989
Iteration 194/1000 | Loss: 0.00001989
Iteration 195/1000 | Loss: 0.00001989
Iteration 196/1000 | Loss: 0.00001988
Iteration 197/1000 | Loss: 0.00001988
Iteration 198/1000 | Loss: 0.00001986
Iteration 199/1000 | Loss: 0.00001986
Iteration 200/1000 | Loss: 0.00001986
Iteration 201/1000 | Loss: 0.00001986
Iteration 202/1000 | Loss: 0.00001986
Iteration 203/1000 | Loss: 0.00001986
Iteration 204/1000 | Loss: 0.00001986
Iteration 205/1000 | Loss: 0.00001986
Iteration 206/1000 | Loss: 0.00001986
Iteration 207/1000 | Loss: 0.00001986
Iteration 208/1000 | Loss: 0.00001985
Iteration 209/1000 | Loss: 0.00001985
Iteration 210/1000 | Loss: 0.00001985
Iteration 211/1000 | Loss: 0.00001985
Iteration 212/1000 | Loss: 0.00001984
Iteration 213/1000 | Loss: 0.00001984
Iteration 214/1000 | Loss: 0.00001984
Iteration 215/1000 | Loss: 0.00001983
Iteration 216/1000 | Loss: 0.00001983
Iteration 217/1000 | Loss: 0.00001983
Iteration 218/1000 | Loss: 0.00001982
Iteration 219/1000 | Loss: 0.00001982
Iteration 220/1000 | Loss: 0.00001982
Iteration 221/1000 | Loss: 0.00001982
Iteration 222/1000 | Loss: 0.00001981
Iteration 223/1000 | Loss: 0.00001981
Iteration 224/1000 | Loss: 0.00001981
Iteration 225/1000 | Loss: 0.00001981
Iteration 226/1000 | Loss: 0.00001981
Iteration 227/1000 | Loss: 0.00001981
Iteration 228/1000 | Loss: 0.00001981
Iteration 229/1000 | Loss: 0.00001981
Iteration 230/1000 | Loss: 0.00001981
Iteration 231/1000 | Loss: 0.00001980
Iteration 232/1000 | Loss: 0.00001980
Iteration 233/1000 | Loss: 0.00001980
Iteration 234/1000 | Loss: 0.00001980
Iteration 235/1000 | Loss: 0.00001980
Iteration 236/1000 | Loss: 0.00001979
Iteration 237/1000 | Loss: 0.00001979
Iteration 238/1000 | Loss: 0.00001979
Iteration 239/1000 | Loss: 0.00001979
Iteration 240/1000 | Loss: 0.00001979
Iteration 241/1000 | Loss: 0.00001978
Iteration 242/1000 | Loss: 0.00001978
Iteration 243/1000 | Loss: 0.00001978
Iteration 244/1000 | Loss: 0.00001978
Iteration 245/1000 | Loss: 0.00002230
Iteration 246/1000 | Loss: 0.00002024
Iteration 247/1000 | Loss: 0.00001997
Iteration 248/1000 | Loss: 0.00001976
Iteration 249/1000 | Loss: 0.00001969
Iteration 250/1000 | Loss: 0.00001968
Iteration 251/1000 | Loss: 0.00001967
Iteration 252/1000 | Loss: 0.00001967
Iteration 253/1000 | Loss: 0.00001967
Iteration 254/1000 | Loss: 0.00001967
Iteration 255/1000 | Loss: 0.00001967
Iteration 256/1000 | Loss: 0.00001967
Iteration 257/1000 | Loss: 0.00001967
Iteration 258/1000 | Loss: 0.00001967
Iteration 259/1000 | Loss: 0.00001966
Iteration 260/1000 | Loss: 0.00001966
Iteration 261/1000 | Loss: 0.00001966
Iteration 262/1000 | Loss: 0.00001966
Iteration 263/1000 | Loss: 0.00001966
Iteration 264/1000 | Loss: 0.00001966
Iteration 265/1000 | Loss: 0.00001966
Iteration 266/1000 | Loss: 0.00001966
Iteration 267/1000 | Loss: 0.00001965
Iteration 268/1000 | Loss: 0.00001965
Iteration 269/1000 | Loss: 0.00001965
Iteration 270/1000 | Loss: 0.00001965
Iteration 271/1000 | Loss: 0.00001965
Iteration 272/1000 | Loss: 0.00001965
Iteration 273/1000 | Loss: 0.00001965
Iteration 274/1000 | Loss: 0.00001965
Iteration 275/1000 | Loss: 0.00001965
Iteration 276/1000 | Loss: 0.00001965
Iteration 277/1000 | Loss: 0.00001965
Iteration 278/1000 | Loss: 0.00001964
Iteration 279/1000 | Loss: 0.00001964
Iteration 280/1000 | Loss: 0.00001964
Iteration 281/1000 | Loss: 0.00001964
Iteration 282/1000 | Loss: 0.00001964
Iteration 283/1000 | Loss: 0.00001964
Iteration 284/1000 | Loss: 0.00001964
Iteration 285/1000 | Loss: 0.00001964
Iteration 286/1000 | Loss: 0.00001964
Iteration 287/1000 | Loss: 0.00001964
Iteration 288/1000 | Loss: 0.00001964
Iteration 289/1000 | Loss: 0.00001964
Iteration 290/1000 | Loss: 0.00001964
Iteration 291/1000 | Loss: 0.00001963
Iteration 292/1000 | Loss: 0.00001963
Iteration 293/1000 | Loss: 0.00001963
Iteration 294/1000 | Loss: 0.00001963
Iteration 295/1000 | Loss: 0.00001963
Iteration 296/1000 | Loss: 0.00001963
Iteration 297/1000 | Loss: 0.00001963
Iteration 298/1000 | Loss: 0.00001963
Iteration 299/1000 | Loss: 0.00001963
Iteration 300/1000 | Loss: 0.00001963
Iteration 301/1000 | Loss: 0.00001963
Iteration 302/1000 | Loss: 0.00001963
Iteration 303/1000 | Loss: 0.00001963
Iteration 304/1000 | Loss: 0.00001962
Iteration 305/1000 | Loss: 0.00001962
Iteration 306/1000 | Loss: 0.00001962
Iteration 307/1000 | Loss: 0.00001962
Iteration 308/1000 | Loss: 0.00001962
Iteration 309/1000 | Loss: 0.00001962
Iteration 310/1000 | Loss: 0.00001962
Iteration 311/1000 | Loss: 0.00001962
Iteration 312/1000 | Loss: 0.00001962
Iteration 313/1000 | Loss: 0.00001962
Iteration 314/1000 | Loss: 0.00001962
Iteration 315/1000 | Loss: 0.00001962
Iteration 316/1000 | Loss: 0.00001962
Iteration 317/1000 | Loss: 0.00001962
Iteration 318/1000 | Loss: 0.00001962
Iteration 319/1000 | Loss: 0.00001962
Iteration 320/1000 | Loss: 0.00001962
Iteration 321/1000 | Loss: 0.00001962
Iteration 322/1000 | Loss: 0.00001962
Iteration 323/1000 | Loss: 0.00001961
Iteration 324/1000 | Loss: 0.00001961
Iteration 325/1000 | Loss: 0.00001961
Iteration 326/1000 | Loss: 0.00001961
Iteration 327/1000 | Loss: 0.00001961
Iteration 328/1000 | Loss: 0.00001961
Iteration 329/1000 | Loss: 0.00001961
Iteration 330/1000 | Loss: 0.00001961
Iteration 331/1000 | Loss: 0.00001961
Iteration 332/1000 | Loss: 0.00001961
Iteration 333/1000 | Loss: 0.00001961
Iteration 334/1000 | Loss: 0.00001961
Iteration 335/1000 | Loss: 0.00001960
Iteration 336/1000 | Loss: 0.00001960
Iteration 337/1000 | Loss: 0.00001960
Iteration 338/1000 | Loss: 0.00001960
Iteration 339/1000 | Loss: 0.00001960
Iteration 340/1000 | Loss: 0.00001960
Iteration 341/1000 | Loss: 0.00001960
Iteration 342/1000 | Loss: 0.00001960
Iteration 343/1000 | Loss: 0.00001960
Iteration 344/1000 | Loss: 0.00001960
Iteration 345/1000 | Loss: 0.00001960
Iteration 346/1000 | Loss: 0.00001960
Iteration 347/1000 | Loss: 0.00001960
Iteration 348/1000 | Loss: 0.00001960
Iteration 349/1000 | Loss: 0.00001960
Iteration 350/1000 | Loss: 0.00001960
Iteration 351/1000 | Loss: 0.00001960
Iteration 352/1000 | Loss: 0.00001960
Iteration 353/1000 | Loss: 0.00001960
Iteration 354/1000 | Loss: 0.00001960
Iteration 355/1000 | Loss: 0.00001960
Iteration 356/1000 | Loss: 0.00001959
Iteration 357/1000 | Loss: 0.00001959
Iteration 358/1000 | Loss: 0.00001959
Iteration 359/1000 | Loss: 0.00001959
Iteration 360/1000 | Loss: 0.00001959
Iteration 361/1000 | Loss: 0.00001959
Iteration 362/1000 | Loss: 0.00001959
Iteration 363/1000 | Loss: 0.00001959
Iteration 364/1000 | Loss: 0.00001959
Iteration 365/1000 | Loss: 0.00001959
Iteration 366/1000 | Loss: 0.00001959
Iteration 367/1000 | Loss: 0.00001959
Iteration 368/1000 | Loss: 0.00001959
Iteration 369/1000 | Loss: 0.00001959
Iteration 370/1000 | Loss: 0.00001959
Iteration 371/1000 | Loss: 0.00001959
Iteration 372/1000 | Loss: 0.00001958
Iteration 373/1000 | Loss: 0.00001958
Iteration 374/1000 | Loss: 0.00001958
Iteration 375/1000 | Loss: 0.00001958
Iteration 376/1000 | Loss: 0.00001958
Iteration 377/1000 | Loss: 0.00001958
Iteration 378/1000 | Loss: 0.00001958
Iteration 379/1000 | Loss: 0.00001958
Iteration 380/1000 | Loss: 0.00001958
Iteration 381/1000 | Loss: 0.00001958
Iteration 382/1000 | Loss: 0.00001958
Iteration 383/1000 | Loss: 0.00001958
Iteration 384/1000 | Loss: 0.00001958
Iteration 385/1000 | Loss: 0.00001958
Iteration 386/1000 | Loss: 0.00001958
Iteration 387/1000 | Loss: 0.00001958
Iteration 388/1000 | Loss: 0.00001957
Iteration 389/1000 | Loss: 0.00001957
Iteration 390/1000 | Loss: 0.00001957
Iteration 391/1000 | Loss: 0.00001957
Iteration 392/1000 | Loss: 0.00001957
Iteration 393/1000 | Loss: 0.00001957
Iteration 394/1000 | Loss: 0.00001957
Iteration 395/1000 | Loss: 0.00001957
Iteration 396/1000 | Loss: 0.00001957
Iteration 397/1000 | Loss: 0.00001957
Iteration 398/1000 | Loss: 0.00001957
Iteration 399/1000 | Loss: 0.00001957
Iteration 400/1000 | Loss: 0.00001957
Iteration 401/1000 | Loss: 0.00001957
Iteration 402/1000 | Loss: 0.00001957
Iteration 403/1000 | Loss: 0.00001957
Iteration 404/1000 | Loss: 0.00001957
Iteration 405/1000 | Loss: 0.00001957
Iteration 406/1000 | Loss: 0.00001956
Iteration 407/1000 | Loss: 0.00001956
Iteration 408/1000 | Loss: 0.00001956
Iteration 409/1000 | Loss: 0.00001956
Iteration 410/1000 | Loss: 0.00001956
Iteration 411/1000 | Loss: 0.00001956
Iteration 412/1000 | Loss: 0.00001956
Iteration 413/1000 | Loss: 0.00001956
Iteration 414/1000 | Loss: 0.00001956
Iteration 415/1000 | Loss: 0.00001956
Iteration 416/1000 | Loss: 0.00001956
Iteration 417/1000 | Loss: 0.00001956
Iteration 418/1000 | Loss: 0.00001956
Iteration 419/1000 | Loss: 0.00001956
Iteration 420/1000 | Loss: 0.00001956
Iteration 421/1000 | Loss: 0.00001956
Iteration 422/1000 | Loss: 0.00001956
Iteration 423/1000 | Loss: 0.00001956
Iteration 424/1000 | Loss: 0.00001956
Iteration 425/1000 | Loss: 0.00001956
Iteration 426/1000 | Loss: 0.00001955
Iteration 427/1000 | Loss: 0.00001955
Iteration 428/1000 | Loss: 0.00001955
Iteration 429/1000 | Loss: 0.00001955
Iteration 430/1000 | Loss: 0.00001955
Iteration 431/1000 | Loss: 0.00001955
Iteration 432/1000 | Loss: 0.00001955
Iteration 433/1000 | Loss: 0.00001955
Iteration 434/1000 | Loss: 0.00001955
Iteration 435/1000 | Loss: 0.00001955
Iteration 436/1000 | Loss: 0.00001955
Iteration 437/1000 | Loss: 0.00001955
Iteration 438/1000 | Loss: 0.00001955
Iteration 439/1000 | Loss: 0.00001955
Iteration 440/1000 | Loss: 0.00001955
Iteration 441/1000 | Loss: 0.00001955
Iteration 442/1000 | Loss: 0.00001955
Iteration 443/1000 | Loss: 0.00001955
Iteration 444/1000 | Loss: 0.00001955
Iteration 445/1000 | Loss: 0.00001955
Iteration 446/1000 | Loss: 0.00001955
Iteration 447/1000 | Loss: 0.00001955
Iteration 448/1000 | Loss: 0.00001955
Iteration 449/1000 | Loss: 0.00001955
Iteration 450/1000 | Loss: 0.00001955
Iteration 451/1000 | Loss: 0.00001955
Iteration 452/1000 | Loss: 0.00001955
Iteration 453/1000 | Loss: 0.00001955
Iteration 454/1000 | Loss: 0.00001955
Iteration 455/1000 | Loss: 0.00001955
Iteration 456/1000 | Loss: 0.00001955
Iteration 457/1000 | Loss: 0.00001955
Iteration 458/1000 | Loss: 0.00001955
Iteration 459/1000 | Loss: 0.00001955
Iteration 460/1000 | Loss: 0.00001955
Iteration 461/1000 | Loss: 0.00001955
Iteration 462/1000 | Loss: 0.00001955
Iteration 463/1000 | Loss: 0.00001955
Iteration 464/1000 | Loss: 0.00001955
Iteration 465/1000 | Loss: 0.00001955
Iteration 466/1000 | Loss: 0.00001955
Iteration 467/1000 | Loss: 0.00001955
Iteration 468/1000 | Loss: 0.00001955
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 468. Stopping optimization.
Last 5 losses: [1.9550267097656615e-05, 1.9550267097656615e-05, 1.9550267097656615e-05, 1.9550267097656615e-05, 1.9550267097656615e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9550267097656615e-05

Optimization complete. Final v2v error: 3.4889347553253174 mm

Highest mean error: 11.845114707946777 mm for frame 62

Lowest mean error: 2.9680004119873047 mm for frame 179

Saving results

Total time: 352.32379364967346
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00487737
Iteration 2/25 | Loss: 0.00130384
Iteration 3/25 | Loss: 0.00121991
Iteration 4/25 | Loss: 0.00121161
Iteration 5/25 | Loss: 0.00120908
Iteration 6/25 | Loss: 0.00120864
Iteration 7/25 | Loss: 0.00120864
Iteration 8/25 | Loss: 0.00120864
Iteration 9/25 | Loss: 0.00120864
Iteration 10/25 | Loss: 0.00120864
Iteration 11/25 | Loss: 0.00120864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0012086369097232819, 0.0012086369097232819, 0.0012086369097232819, 0.0012086369097232819, 0.0012086369097232819]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012086369097232819

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33264244
Iteration 2/25 | Loss: 0.00120366
Iteration 3/25 | Loss: 0.00120362
Iteration 4/25 | Loss: 0.00120361
Iteration 5/25 | Loss: 0.00120361
Iteration 6/25 | Loss: 0.00120361
Iteration 7/25 | Loss: 0.00120361
Iteration 8/25 | Loss: 0.00120361
Iteration 9/25 | Loss: 0.00120361
Iteration 10/25 | Loss: 0.00120361
Iteration 11/25 | Loss: 0.00120361
Iteration 12/25 | Loss: 0.00120361
Iteration 13/25 | Loss: 0.00120361
Iteration 14/25 | Loss: 0.00120361
Iteration 15/25 | Loss: 0.00120361
Iteration 16/25 | Loss: 0.00120361
Iteration 17/25 | Loss: 0.00120361
Iteration 18/25 | Loss: 0.00120361
Iteration 19/25 | Loss: 0.00120361
Iteration 20/25 | Loss: 0.00120361
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.001203612657263875, 0.001203612657263875, 0.001203612657263875, 0.001203612657263875, 0.001203612657263875]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001203612657263875

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120361
Iteration 2/1000 | Loss: 0.00002952
Iteration 3/1000 | Loss: 0.00001882
Iteration 4/1000 | Loss: 0.00001552
Iteration 5/1000 | Loss: 0.00001413
Iteration 6/1000 | Loss: 0.00001322
Iteration 7/1000 | Loss: 0.00001272
Iteration 8/1000 | Loss: 0.00001240
Iteration 9/1000 | Loss: 0.00001216
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001214
Iteration 12/1000 | Loss: 0.00001196
Iteration 13/1000 | Loss: 0.00001188
Iteration 14/1000 | Loss: 0.00001173
Iteration 15/1000 | Loss: 0.00001167
Iteration 16/1000 | Loss: 0.00001165
Iteration 17/1000 | Loss: 0.00001161
Iteration 18/1000 | Loss: 0.00001158
Iteration 19/1000 | Loss: 0.00001157
Iteration 20/1000 | Loss: 0.00001156
Iteration 21/1000 | Loss: 0.00001151
Iteration 22/1000 | Loss: 0.00001150
Iteration 23/1000 | Loss: 0.00001150
Iteration 24/1000 | Loss: 0.00001146
Iteration 25/1000 | Loss: 0.00001145
Iteration 26/1000 | Loss: 0.00001145
Iteration 27/1000 | Loss: 0.00001145
Iteration 28/1000 | Loss: 0.00001143
Iteration 29/1000 | Loss: 0.00001135
Iteration 30/1000 | Loss: 0.00001135
Iteration 31/1000 | Loss: 0.00001134
Iteration 32/1000 | Loss: 0.00001134
Iteration 33/1000 | Loss: 0.00001133
Iteration 34/1000 | Loss: 0.00001133
Iteration 35/1000 | Loss: 0.00001132
Iteration 36/1000 | Loss: 0.00001131
Iteration 37/1000 | Loss: 0.00001130
Iteration 38/1000 | Loss: 0.00001130
Iteration 39/1000 | Loss: 0.00001130
Iteration 40/1000 | Loss: 0.00001129
Iteration 41/1000 | Loss: 0.00001129
Iteration 42/1000 | Loss: 0.00001128
Iteration 43/1000 | Loss: 0.00001128
Iteration 44/1000 | Loss: 0.00001128
Iteration 45/1000 | Loss: 0.00001127
Iteration 46/1000 | Loss: 0.00001127
Iteration 47/1000 | Loss: 0.00001127
Iteration 48/1000 | Loss: 0.00001127
Iteration 49/1000 | Loss: 0.00001126
Iteration 50/1000 | Loss: 0.00001126
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001125
Iteration 53/1000 | Loss: 0.00001125
Iteration 54/1000 | Loss: 0.00001125
Iteration 55/1000 | Loss: 0.00001124
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001123
Iteration 58/1000 | Loss: 0.00001123
Iteration 59/1000 | Loss: 0.00001122
Iteration 60/1000 | Loss: 0.00001122
Iteration 61/1000 | Loss: 0.00001122
Iteration 62/1000 | Loss: 0.00001121
Iteration 63/1000 | Loss: 0.00001121
Iteration 64/1000 | Loss: 0.00001120
Iteration 65/1000 | Loss: 0.00001120
Iteration 66/1000 | Loss: 0.00001119
Iteration 67/1000 | Loss: 0.00001119
Iteration 68/1000 | Loss: 0.00001119
Iteration 69/1000 | Loss: 0.00001118
Iteration 70/1000 | Loss: 0.00001118
Iteration 71/1000 | Loss: 0.00001117
Iteration 72/1000 | Loss: 0.00001117
Iteration 73/1000 | Loss: 0.00001116
Iteration 74/1000 | Loss: 0.00001116
Iteration 75/1000 | Loss: 0.00001115
Iteration 76/1000 | Loss: 0.00001115
Iteration 77/1000 | Loss: 0.00001115
Iteration 78/1000 | Loss: 0.00001115
Iteration 79/1000 | Loss: 0.00001115
Iteration 80/1000 | Loss: 0.00001114
Iteration 81/1000 | Loss: 0.00001114
Iteration 82/1000 | Loss: 0.00001114
Iteration 83/1000 | Loss: 0.00001113
Iteration 84/1000 | Loss: 0.00001109
Iteration 85/1000 | Loss: 0.00001109
Iteration 86/1000 | Loss: 0.00001109
Iteration 87/1000 | Loss: 0.00001109
Iteration 88/1000 | Loss: 0.00001109
Iteration 89/1000 | Loss: 0.00001107
Iteration 90/1000 | Loss: 0.00001107
Iteration 91/1000 | Loss: 0.00001106
Iteration 92/1000 | Loss: 0.00001106
Iteration 93/1000 | Loss: 0.00001105
Iteration 94/1000 | Loss: 0.00001105
Iteration 95/1000 | Loss: 0.00001104
Iteration 96/1000 | Loss: 0.00001104
Iteration 97/1000 | Loss: 0.00001104
Iteration 98/1000 | Loss: 0.00001104
Iteration 99/1000 | Loss: 0.00001103
Iteration 100/1000 | Loss: 0.00001103
Iteration 101/1000 | Loss: 0.00001103
Iteration 102/1000 | Loss: 0.00001102
Iteration 103/1000 | Loss: 0.00001101
Iteration 104/1000 | Loss: 0.00001101
Iteration 105/1000 | Loss: 0.00001101
Iteration 106/1000 | Loss: 0.00001101
Iteration 107/1000 | Loss: 0.00001100
Iteration 108/1000 | Loss: 0.00001100
Iteration 109/1000 | Loss: 0.00001100
Iteration 110/1000 | Loss: 0.00001100
Iteration 111/1000 | Loss: 0.00001100
Iteration 112/1000 | Loss: 0.00001100
Iteration 113/1000 | Loss: 0.00001099
Iteration 114/1000 | Loss: 0.00001099
Iteration 115/1000 | Loss: 0.00001098
Iteration 116/1000 | Loss: 0.00001098
Iteration 117/1000 | Loss: 0.00001098
Iteration 118/1000 | Loss: 0.00001098
Iteration 119/1000 | Loss: 0.00001097
Iteration 120/1000 | Loss: 0.00001097
Iteration 121/1000 | Loss: 0.00001097
Iteration 122/1000 | Loss: 0.00001097
Iteration 123/1000 | Loss: 0.00001097
Iteration 124/1000 | Loss: 0.00001097
Iteration 125/1000 | Loss: 0.00001097
Iteration 126/1000 | Loss: 0.00001097
Iteration 127/1000 | Loss: 0.00001096
Iteration 128/1000 | Loss: 0.00001096
Iteration 129/1000 | Loss: 0.00001096
Iteration 130/1000 | Loss: 0.00001096
Iteration 131/1000 | Loss: 0.00001096
Iteration 132/1000 | Loss: 0.00001096
Iteration 133/1000 | Loss: 0.00001096
Iteration 134/1000 | Loss: 0.00001096
Iteration 135/1000 | Loss: 0.00001096
Iteration 136/1000 | Loss: 0.00001095
Iteration 137/1000 | Loss: 0.00001095
Iteration 138/1000 | Loss: 0.00001095
Iteration 139/1000 | Loss: 0.00001095
Iteration 140/1000 | Loss: 0.00001095
Iteration 141/1000 | Loss: 0.00001094
Iteration 142/1000 | Loss: 0.00001094
Iteration 143/1000 | Loss: 0.00001094
Iteration 144/1000 | Loss: 0.00001094
Iteration 145/1000 | Loss: 0.00001093
Iteration 146/1000 | Loss: 0.00001093
Iteration 147/1000 | Loss: 0.00001093
Iteration 148/1000 | Loss: 0.00001093
Iteration 149/1000 | Loss: 0.00001093
Iteration 150/1000 | Loss: 0.00001093
Iteration 151/1000 | Loss: 0.00001093
Iteration 152/1000 | Loss: 0.00001093
Iteration 153/1000 | Loss: 0.00001092
Iteration 154/1000 | Loss: 0.00001092
Iteration 155/1000 | Loss: 0.00001092
Iteration 156/1000 | Loss: 0.00001092
Iteration 157/1000 | Loss: 0.00001092
Iteration 158/1000 | Loss: 0.00001091
Iteration 159/1000 | Loss: 0.00001091
Iteration 160/1000 | Loss: 0.00001091
Iteration 161/1000 | Loss: 0.00001091
Iteration 162/1000 | Loss: 0.00001091
Iteration 163/1000 | Loss: 0.00001091
Iteration 164/1000 | Loss: 0.00001091
Iteration 165/1000 | Loss: 0.00001091
Iteration 166/1000 | Loss: 0.00001091
Iteration 167/1000 | Loss: 0.00001091
Iteration 168/1000 | Loss: 0.00001091
Iteration 169/1000 | Loss: 0.00001091
Iteration 170/1000 | Loss: 0.00001091
Iteration 171/1000 | Loss: 0.00001091
Iteration 172/1000 | Loss: 0.00001091
Iteration 173/1000 | Loss: 0.00001091
Iteration 174/1000 | Loss: 0.00001091
Iteration 175/1000 | Loss: 0.00001091
Iteration 176/1000 | Loss: 0.00001091
Iteration 177/1000 | Loss: 0.00001090
Iteration 178/1000 | Loss: 0.00001090
Iteration 179/1000 | Loss: 0.00001090
Iteration 180/1000 | Loss: 0.00001090
Iteration 181/1000 | Loss: 0.00001090
Iteration 182/1000 | Loss: 0.00001090
Iteration 183/1000 | Loss: 0.00001090
Iteration 184/1000 | Loss: 0.00001090
Iteration 185/1000 | Loss: 0.00001090
Iteration 186/1000 | Loss: 0.00001090
Iteration 187/1000 | Loss: 0.00001089
Iteration 188/1000 | Loss: 0.00001089
Iteration 189/1000 | Loss: 0.00001089
Iteration 190/1000 | Loss: 0.00001089
Iteration 191/1000 | Loss: 0.00001089
Iteration 192/1000 | Loss: 0.00001089
Iteration 193/1000 | Loss: 0.00001089
Iteration 194/1000 | Loss: 0.00001089
Iteration 195/1000 | Loss: 0.00001089
Iteration 196/1000 | Loss: 0.00001089
Iteration 197/1000 | Loss: 0.00001089
Iteration 198/1000 | Loss: 0.00001089
Iteration 199/1000 | Loss: 0.00001089
Iteration 200/1000 | Loss: 0.00001089
Iteration 201/1000 | Loss: 0.00001089
Iteration 202/1000 | Loss: 0.00001089
Iteration 203/1000 | Loss: 0.00001089
Iteration 204/1000 | Loss: 0.00001089
Iteration 205/1000 | Loss: 0.00001089
Iteration 206/1000 | Loss: 0.00001089
Iteration 207/1000 | Loss: 0.00001089
Iteration 208/1000 | Loss: 0.00001089
Iteration 209/1000 | Loss: 0.00001089
Iteration 210/1000 | Loss: 0.00001089
Iteration 211/1000 | Loss: 0.00001089
Iteration 212/1000 | Loss: 0.00001089
Iteration 213/1000 | Loss: 0.00001089
Iteration 214/1000 | Loss: 0.00001089
Iteration 215/1000 | Loss: 0.00001089
Iteration 216/1000 | Loss: 0.00001089
Iteration 217/1000 | Loss: 0.00001089
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 217. Stopping optimization.
Last 5 losses: [1.088579938368639e-05, 1.088579938368639e-05, 1.088579938368639e-05, 1.088579938368639e-05, 1.088579938368639e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.088579938368639e-05

Optimization complete. Final v2v error: 2.8000917434692383 mm

Highest mean error: 3.1419622898101807 mm for frame 41

Lowest mean error: 2.4725470542907715 mm for frame 13

Saving results

Total time: 42.55536651611328
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00783354
Iteration 2/25 | Loss: 0.00143730
Iteration 3/25 | Loss: 0.00125667
Iteration 4/25 | Loss: 0.00123772
Iteration 5/25 | Loss: 0.00123620
Iteration 6/25 | Loss: 0.00123620
Iteration 7/25 | Loss: 0.00123620
Iteration 8/25 | Loss: 0.00123620
Iteration 9/25 | Loss: 0.00123620
Iteration 10/25 | Loss: 0.00123620
Iteration 11/25 | Loss: 0.00123620
Iteration 12/25 | Loss: 0.00123620
Iteration 13/25 | Loss: 0.00123620
Iteration 14/25 | Loss: 0.00123620
Iteration 15/25 | Loss: 0.00123620
Iteration 16/25 | Loss: 0.00123620
Iteration 17/25 | Loss: 0.00123620
Iteration 18/25 | Loss: 0.00123620
Iteration 19/25 | Loss: 0.00123620
Iteration 20/25 | Loss: 0.00123620
Iteration 21/25 | Loss: 0.00123620
Iteration 22/25 | Loss: 0.00123620
Iteration 23/25 | Loss: 0.00123620
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0012361954431980848, 0.0012361954431980848, 0.0012361954431980848, 0.0012361954431980848, 0.0012361954431980848]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012361954431980848

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.38769388
Iteration 2/25 | Loss: 0.00110797
Iteration 3/25 | Loss: 0.00110795
Iteration 4/25 | Loss: 0.00110795
Iteration 5/25 | Loss: 0.00110795
Iteration 6/25 | Loss: 0.00110795
Iteration 7/25 | Loss: 0.00110795
Iteration 8/25 | Loss: 0.00110795
Iteration 9/25 | Loss: 0.00110795
Iteration 10/25 | Loss: 0.00110794
Iteration 11/25 | Loss: 0.00110794
Iteration 12/25 | Loss: 0.00110794
Iteration 13/25 | Loss: 0.00110794
Iteration 14/25 | Loss: 0.00110794
Iteration 15/25 | Loss: 0.00110794
Iteration 16/25 | Loss: 0.00110794
Iteration 17/25 | Loss: 0.00110794
Iteration 18/25 | Loss: 0.00110794
Iteration 19/25 | Loss: 0.00110794
Iteration 20/25 | Loss: 0.00110794
Iteration 21/25 | Loss: 0.00110794
Iteration 22/25 | Loss: 0.00110794
Iteration 23/25 | Loss: 0.00110794
Iteration 24/25 | Loss: 0.00110794
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.001107944524846971, 0.001107944524846971, 0.001107944524846971, 0.001107944524846971, 0.001107944524846971]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001107944524846971

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110794
Iteration 2/1000 | Loss: 0.00002542
Iteration 3/1000 | Loss: 0.00001757
Iteration 4/1000 | Loss: 0.00001526
Iteration 5/1000 | Loss: 0.00001399
Iteration 6/1000 | Loss: 0.00001312
Iteration 7/1000 | Loss: 0.00001269
Iteration 8/1000 | Loss: 0.00001244
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001195
Iteration 11/1000 | Loss: 0.00001178
Iteration 12/1000 | Loss: 0.00001173
Iteration 13/1000 | Loss: 0.00001165
Iteration 14/1000 | Loss: 0.00001154
Iteration 15/1000 | Loss: 0.00001150
Iteration 16/1000 | Loss: 0.00001148
Iteration 17/1000 | Loss: 0.00001148
Iteration 18/1000 | Loss: 0.00001148
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001144
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001143
Iteration 23/1000 | Loss: 0.00001143
Iteration 24/1000 | Loss: 0.00001142
Iteration 25/1000 | Loss: 0.00001142
Iteration 26/1000 | Loss: 0.00001142
Iteration 27/1000 | Loss: 0.00001141
Iteration 28/1000 | Loss: 0.00001140
Iteration 29/1000 | Loss: 0.00001140
Iteration 30/1000 | Loss: 0.00001139
Iteration 31/1000 | Loss: 0.00001139
Iteration 32/1000 | Loss: 0.00001138
Iteration 33/1000 | Loss: 0.00001138
Iteration 34/1000 | Loss: 0.00001137
Iteration 35/1000 | Loss: 0.00001137
Iteration 36/1000 | Loss: 0.00001136
Iteration 37/1000 | Loss: 0.00001136
Iteration 38/1000 | Loss: 0.00001135
Iteration 39/1000 | Loss: 0.00001135
Iteration 40/1000 | Loss: 0.00001133
Iteration 41/1000 | Loss: 0.00001132
Iteration 42/1000 | Loss: 0.00001132
Iteration 43/1000 | Loss: 0.00001132
Iteration 44/1000 | Loss: 0.00001131
Iteration 45/1000 | Loss: 0.00001130
Iteration 46/1000 | Loss: 0.00001129
Iteration 47/1000 | Loss: 0.00001129
Iteration 48/1000 | Loss: 0.00001128
Iteration 49/1000 | Loss: 0.00001128
Iteration 50/1000 | Loss: 0.00001127
Iteration 51/1000 | Loss: 0.00001126
Iteration 52/1000 | Loss: 0.00001126
Iteration 53/1000 | Loss: 0.00001125
Iteration 54/1000 | Loss: 0.00001125
Iteration 55/1000 | Loss: 0.00001125
Iteration 56/1000 | Loss: 0.00001124
Iteration 57/1000 | Loss: 0.00001124
Iteration 58/1000 | Loss: 0.00001124
Iteration 59/1000 | Loss: 0.00001123
Iteration 60/1000 | Loss: 0.00001123
Iteration 61/1000 | Loss: 0.00001123
Iteration 62/1000 | Loss: 0.00001122
Iteration 63/1000 | Loss: 0.00001122
Iteration 64/1000 | Loss: 0.00001122
Iteration 65/1000 | Loss: 0.00001122
Iteration 66/1000 | Loss: 0.00001122
Iteration 67/1000 | Loss: 0.00001121
Iteration 68/1000 | Loss: 0.00001121
Iteration 69/1000 | Loss: 0.00001121
Iteration 70/1000 | Loss: 0.00001120
Iteration 71/1000 | Loss: 0.00001120
Iteration 72/1000 | Loss: 0.00001120
Iteration 73/1000 | Loss: 0.00001120
Iteration 74/1000 | Loss: 0.00001119
Iteration 75/1000 | Loss: 0.00001119
Iteration 76/1000 | Loss: 0.00001119
Iteration 77/1000 | Loss: 0.00001119
Iteration 78/1000 | Loss: 0.00001119
Iteration 79/1000 | Loss: 0.00001119
Iteration 80/1000 | Loss: 0.00001118
Iteration 81/1000 | Loss: 0.00001118
Iteration 82/1000 | Loss: 0.00001118
Iteration 83/1000 | Loss: 0.00001118
Iteration 84/1000 | Loss: 0.00001117
Iteration 85/1000 | Loss: 0.00001117
Iteration 86/1000 | Loss: 0.00001117
Iteration 87/1000 | Loss: 0.00001116
Iteration 88/1000 | Loss: 0.00001116
Iteration 89/1000 | Loss: 0.00001116
Iteration 90/1000 | Loss: 0.00001116
Iteration 91/1000 | Loss: 0.00001116
Iteration 92/1000 | Loss: 0.00001116
Iteration 93/1000 | Loss: 0.00001116
Iteration 94/1000 | Loss: 0.00001115
Iteration 95/1000 | Loss: 0.00001115
Iteration 96/1000 | Loss: 0.00001115
Iteration 97/1000 | Loss: 0.00001115
Iteration 98/1000 | Loss: 0.00001114
Iteration 99/1000 | Loss: 0.00001114
Iteration 100/1000 | Loss: 0.00001114
Iteration 101/1000 | Loss: 0.00001113
Iteration 102/1000 | Loss: 0.00001113
Iteration 103/1000 | Loss: 0.00001113
Iteration 104/1000 | Loss: 0.00001112
Iteration 105/1000 | Loss: 0.00001112
Iteration 106/1000 | Loss: 0.00001112
Iteration 107/1000 | Loss: 0.00001112
Iteration 108/1000 | Loss: 0.00001112
Iteration 109/1000 | Loss: 0.00001112
Iteration 110/1000 | Loss: 0.00001112
Iteration 111/1000 | Loss: 0.00001112
Iteration 112/1000 | Loss: 0.00001112
Iteration 113/1000 | Loss: 0.00001112
Iteration 114/1000 | Loss: 0.00001112
Iteration 115/1000 | Loss: 0.00001111
Iteration 116/1000 | Loss: 0.00001111
Iteration 117/1000 | Loss: 0.00001111
Iteration 118/1000 | Loss: 0.00001111
Iteration 119/1000 | Loss: 0.00001111
Iteration 120/1000 | Loss: 0.00001111
Iteration 121/1000 | Loss: 0.00001110
Iteration 122/1000 | Loss: 0.00001110
Iteration 123/1000 | Loss: 0.00001109
Iteration 124/1000 | Loss: 0.00001109
Iteration 125/1000 | Loss: 0.00001109
Iteration 126/1000 | Loss: 0.00001109
Iteration 127/1000 | Loss: 0.00001109
Iteration 128/1000 | Loss: 0.00001109
Iteration 129/1000 | Loss: 0.00001109
Iteration 130/1000 | Loss: 0.00001109
Iteration 131/1000 | Loss: 0.00001109
Iteration 132/1000 | Loss: 0.00001109
Iteration 133/1000 | Loss: 0.00001109
Iteration 134/1000 | Loss: 0.00001109
Iteration 135/1000 | Loss: 0.00001109
Iteration 136/1000 | Loss: 0.00001109
Iteration 137/1000 | Loss: 0.00001109
Iteration 138/1000 | Loss: 0.00001109
Iteration 139/1000 | Loss: 0.00001109
Iteration 140/1000 | Loss: 0.00001109
Iteration 141/1000 | Loss: 0.00001109
Iteration 142/1000 | Loss: 0.00001109
Iteration 143/1000 | Loss: 0.00001109
Iteration 144/1000 | Loss: 0.00001109
Iteration 145/1000 | Loss: 0.00001109
Iteration 146/1000 | Loss: 0.00001109
Iteration 147/1000 | Loss: 0.00001109
Iteration 148/1000 | Loss: 0.00001109
Iteration 149/1000 | Loss: 0.00001109
Iteration 150/1000 | Loss: 0.00001109
Iteration 151/1000 | Loss: 0.00001109
Iteration 152/1000 | Loss: 0.00001109
Iteration 153/1000 | Loss: 0.00001109
Iteration 154/1000 | Loss: 0.00001109
Iteration 155/1000 | Loss: 0.00001109
Iteration 156/1000 | Loss: 0.00001109
Iteration 157/1000 | Loss: 0.00001109
Iteration 158/1000 | Loss: 0.00001109
Iteration 159/1000 | Loss: 0.00001109
Iteration 160/1000 | Loss: 0.00001109
Iteration 161/1000 | Loss: 0.00001109
Iteration 162/1000 | Loss: 0.00001109
Iteration 163/1000 | Loss: 0.00001109
Iteration 164/1000 | Loss: 0.00001109
Iteration 165/1000 | Loss: 0.00001109
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 165. Stopping optimization.
Last 5 losses: [1.1089981853729114e-05, 1.1089981853729114e-05, 1.1089981853729114e-05, 1.1089981853729114e-05, 1.1089981853729114e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1089981853729114e-05

Optimization complete. Final v2v error: 2.786581516265869 mm

Highest mean error: 3.2012667655944824 mm for frame 81

Lowest mean error: 2.406721591949463 mm for frame 47

Saving results

Total time: 41.93790125846863
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1031/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1031.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1031
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00860131
Iteration 2/25 | Loss: 0.00172442
Iteration 3/25 | Loss: 0.00138322
Iteration 4/25 | Loss: 0.00133175
Iteration 5/25 | Loss: 0.00138187
Iteration 6/25 | Loss: 0.00130174
Iteration 7/25 | Loss: 0.00125958
Iteration 8/25 | Loss: 0.00124096
Iteration 9/25 | Loss: 0.00123563
Iteration 10/25 | Loss: 0.00123558
Iteration 11/25 | Loss: 0.00123312
Iteration 12/25 | Loss: 0.00123244
Iteration 13/25 | Loss: 0.00123228
Iteration 14/25 | Loss: 0.00123224
Iteration 15/25 | Loss: 0.00123224
Iteration 16/25 | Loss: 0.00123224
Iteration 17/25 | Loss: 0.00123224
Iteration 18/25 | Loss: 0.00123224
Iteration 19/25 | Loss: 0.00123224
Iteration 20/25 | Loss: 0.00123224
Iteration 21/25 | Loss: 0.00123223
Iteration 22/25 | Loss: 0.00123223
Iteration 23/25 | Loss: 0.00123223
Iteration 24/25 | Loss: 0.00123223
Iteration 25/25 | Loss: 0.00123223

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40682244
Iteration 2/25 | Loss: 0.00125601
Iteration 3/25 | Loss: 0.00125601
Iteration 4/25 | Loss: 0.00125601
Iteration 5/25 | Loss: 0.00125601
Iteration 6/25 | Loss: 0.00125601
Iteration 7/25 | Loss: 0.00125601
Iteration 8/25 | Loss: 0.00125601
Iteration 9/25 | Loss: 0.00125601
Iteration 10/25 | Loss: 0.00125601
Iteration 11/25 | Loss: 0.00125601
Iteration 12/25 | Loss: 0.00125601
Iteration 13/25 | Loss: 0.00125601
Iteration 14/25 | Loss: 0.00125601
Iteration 15/25 | Loss: 0.00125601
Iteration 16/25 | Loss: 0.00125601
Iteration 17/25 | Loss: 0.00125601
Iteration 18/25 | Loss: 0.00125601
Iteration 19/25 | Loss: 0.00125601
Iteration 20/25 | Loss: 0.00125601
Iteration 21/25 | Loss: 0.00125601
Iteration 22/25 | Loss: 0.00125601
Iteration 23/25 | Loss: 0.00125601
Iteration 24/25 | Loss: 0.00125601
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0012560051400214434, 0.0012560051400214434, 0.0012560051400214434, 0.0012560051400214434, 0.0012560051400214434]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012560051400214434

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00125601
Iteration 2/1000 | Loss: 0.00003664
Iteration 3/1000 | Loss: 0.00002560
Iteration 4/1000 | Loss: 0.00018022
Iteration 5/1000 | Loss: 0.00011913
Iteration 6/1000 | Loss: 0.00003181
Iteration 7/1000 | Loss: 0.00001824
Iteration 8/1000 | Loss: 0.00001746
Iteration 9/1000 | Loss: 0.00004044
Iteration 10/1000 | Loss: 0.00001687
Iteration 11/1000 | Loss: 0.00005558
Iteration 12/1000 | Loss: 0.00023745
Iteration 13/1000 | Loss: 0.00001623
Iteration 14/1000 | Loss: 0.00001594
Iteration 15/1000 | Loss: 0.00001567
Iteration 16/1000 | Loss: 0.00004066
Iteration 17/1000 | Loss: 0.00001541
Iteration 18/1000 | Loss: 0.00022367
Iteration 19/1000 | Loss: 0.00005512
Iteration 20/1000 | Loss: 0.00001600
Iteration 21/1000 | Loss: 0.00003445
Iteration 22/1000 | Loss: 0.00009534
Iteration 23/1000 | Loss: 0.00005315
Iteration 24/1000 | Loss: 0.00005214
Iteration 25/1000 | Loss: 0.00002359
Iteration 26/1000 | Loss: 0.00001517
Iteration 27/1000 | Loss: 0.00001515
Iteration 28/1000 | Loss: 0.00001513
Iteration 29/1000 | Loss: 0.00001513
Iteration 30/1000 | Loss: 0.00001510
Iteration 31/1000 | Loss: 0.00001510
Iteration 32/1000 | Loss: 0.00001510
Iteration 33/1000 | Loss: 0.00001510
Iteration 34/1000 | Loss: 0.00001509
Iteration 35/1000 | Loss: 0.00001508
Iteration 36/1000 | Loss: 0.00006360
Iteration 37/1000 | Loss: 0.00011487
Iteration 38/1000 | Loss: 0.00003977
Iteration 39/1000 | Loss: 0.00005085
Iteration 40/1000 | Loss: 0.00001510
Iteration 41/1000 | Loss: 0.00003663
Iteration 42/1000 | Loss: 0.00001502
Iteration 43/1000 | Loss: 0.00001491
Iteration 44/1000 | Loss: 0.00001490
Iteration 45/1000 | Loss: 0.00001487
Iteration 46/1000 | Loss: 0.00001486
Iteration 47/1000 | Loss: 0.00001486
Iteration 48/1000 | Loss: 0.00001486
Iteration 49/1000 | Loss: 0.00001485
Iteration 50/1000 | Loss: 0.00001485
Iteration 51/1000 | Loss: 0.00001485
Iteration 52/1000 | Loss: 0.00001484
Iteration 53/1000 | Loss: 0.00001484
Iteration 54/1000 | Loss: 0.00001484
Iteration 55/1000 | Loss: 0.00001483
Iteration 56/1000 | Loss: 0.00001482
Iteration 57/1000 | Loss: 0.00001481
Iteration 58/1000 | Loss: 0.00001481
Iteration 59/1000 | Loss: 0.00001479
Iteration 60/1000 | Loss: 0.00001478
Iteration 61/1000 | Loss: 0.00001478
Iteration 62/1000 | Loss: 0.00001478
Iteration 63/1000 | Loss: 0.00001477
Iteration 64/1000 | Loss: 0.00001477
Iteration 65/1000 | Loss: 0.00001477
Iteration 66/1000 | Loss: 0.00001476
Iteration 67/1000 | Loss: 0.00001476
Iteration 68/1000 | Loss: 0.00001476
Iteration 69/1000 | Loss: 0.00001475
Iteration 70/1000 | Loss: 0.00006479
Iteration 71/1000 | Loss: 0.00005403
Iteration 72/1000 | Loss: 0.00001805
Iteration 73/1000 | Loss: 0.00001489
Iteration 74/1000 | Loss: 0.00005535
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001507
Iteration 77/1000 | Loss: 0.00001479
Iteration 78/1000 | Loss: 0.00001478
Iteration 79/1000 | Loss: 0.00001476
Iteration 80/1000 | Loss: 0.00001475
Iteration 81/1000 | Loss: 0.00001475
Iteration 82/1000 | Loss: 0.00001475
Iteration 83/1000 | Loss: 0.00001474
Iteration 84/1000 | Loss: 0.00001473
Iteration 85/1000 | Loss: 0.00001473
Iteration 86/1000 | Loss: 0.00001472
Iteration 87/1000 | Loss: 0.00001471
Iteration 88/1000 | Loss: 0.00001470
Iteration 89/1000 | Loss: 0.00001470
Iteration 90/1000 | Loss: 0.00001470
Iteration 91/1000 | Loss: 0.00001470
Iteration 92/1000 | Loss: 0.00001469
Iteration 93/1000 | Loss: 0.00001469
Iteration 94/1000 | Loss: 0.00001469
Iteration 95/1000 | Loss: 0.00001468
Iteration 96/1000 | Loss: 0.00001468
Iteration 97/1000 | Loss: 0.00001467
Iteration 98/1000 | Loss: 0.00001467
Iteration 99/1000 | Loss: 0.00001467
Iteration 100/1000 | Loss: 0.00001466
Iteration 101/1000 | Loss: 0.00001466
Iteration 102/1000 | Loss: 0.00001466
Iteration 103/1000 | Loss: 0.00001466
Iteration 104/1000 | Loss: 0.00001466
Iteration 105/1000 | Loss: 0.00001466
Iteration 106/1000 | Loss: 0.00001466
Iteration 107/1000 | Loss: 0.00001466
Iteration 108/1000 | Loss: 0.00001466
Iteration 109/1000 | Loss: 0.00001466
Iteration 110/1000 | Loss: 0.00001466
Iteration 111/1000 | Loss: 0.00001465
Iteration 112/1000 | Loss: 0.00001465
Iteration 113/1000 | Loss: 0.00001464
Iteration 114/1000 | Loss: 0.00001464
Iteration 115/1000 | Loss: 0.00001464
Iteration 116/1000 | Loss: 0.00001463
Iteration 117/1000 | Loss: 0.00001463
Iteration 118/1000 | Loss: 0.00001463
Iteration 119/1000 | Loss: 0.00001463
Iteration 120/1000 | Loss: 0.00001462
Iteration 121/1000 | Loss: 0.00001462
Iteration 122/1000 | Loss: 0.00001462
Iteration 123/1000 | Loss: 0.00001462
Iteration 124/1000 | Loss: 0.00001461
Iteration 125/1000 | Loss: 0.00001461
Iteration 126/1000 | Loss: 0.00001461
Iteration 127/1000 | Loss: 0.00001461
Iteration 128/1000 | Loss: 0.00001461
Iteration 129/1000 | Loss: 0.00001461
Iteration 130/1000 | Loss: 0.00001461
Iteration 131/1000 | Loss: 0.00001461
Iteration 132/1000 | Loss: 0.00001461
Iteration 133/1000 | Loss: 0.00001460
Iteration 134/1000 | Loss: 0.00001460
Iteration 135/1000 | Loss: 0.00001460
Iteration 136/1000 | Loss: 0.00001460
Iteration 137/1000 | Loss: 0.00001460
Iteration 138/1000 | Loss: 0.00001460
Iteration 139/1000 | Loss: 0.00001460
Iteration 140/1000 | Loss: 0.00001460
Iteration 141/1000 | Loss: 0.00001460
Iteration 142/1000 | Loss: 0.00001459
Iteration 143/1000 | Loss: 0.00001459
Iteration 144/1000 | Loss: 0.00001459
Iteration 145/1000 | Loss: 0.00001459
Iteration 146/1000 | Loss: 0.00001459
Iteration 147/1000 | Loss: 0.00001459
Iteration 148/1000 | Loss: 0.00001458
Iteration 149/1000 | Loss: 0.00001458
Iteration 150/1000 | Loss: 0.00001458
Iteration 151/1000 | Loss: 0.00001458
Iteration 152/1000 | Loss: 0.00001458
Iteration 153/1000 | Loss: 0.00001457
Iteration 154/1000 | Loss: 0.00001457
Iteration 155/1000 | Loss: 0.00001457
Iteration 156/1000 | Loss: 0.00001457
Iteration 157/1000 | Loss: 0.00001457
Iteration 158/1000 | Loss: 0.00001457
Iteration 159/1000 | Loss: 0.00001457
Iteration 160/1000 | Loss: 0.00001457
Iteration 161/1000 | Loss: 0.00001457
Iteration 162/1000 | Loss: 0.00001457
Iteration 163/1000 | Loss: 0.00001457
Iteration 164/1000 | Loss: 0.00001457
Iteration 165/1000 | Loss: 0.00001457
Iteration 166/1000 | Loss: 0.00001457
Iteration 167/1000 | Loss: 0.00001456
Iteration 168/1000 | Loss: 0.00001456
Iteration 169/1000 | Loss: 0.00001456
Iteration 170/1000 | Loss: 0.00001456
Iteration 171/1000 | Loss: 0.00001456
Iteration 172/1000 | Loss: 0.00001456
Iteration 173/1000 | Loss: 0.00001456
Iteration 174/1000 | Loss: 0.00001456
Iteration 175/1000 | Loss: 0.00001456
Iteration 176/1000 | Loss: 0.00001456
Iteration 177/1000 | Loss: 0.00001456
Iteration 178/1000 | Loss: 0.00001456
Iteration 179/1000 | Loss: 0.00001456
Iteration 180/1000 | Loss: 0.00001456
Iteration 181/1000 | Loss: 0.00001456
Iteration 182/1000 | Loss: 0.00001456
Iteration 183/1000 | Loss: 0.00001456
Iteration 184/1000 | Loss: 0.00001456
Iteration 185/1000 | Loss: 0.00001456
Iteration 186/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.4561369425791781e-05, 1.4561369425791781e-05, 1.4561369425791781e-05, 1.4561369425791781e-05, 1.4561369425791781e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4561369425791781e-05

Optimization complete. Final v2v error: 3.2744500637054443 mm

Highest mean error: 4.094928741455078 mm for frame 73

Lowest mean error: 2.964265823364258 mm for frame 114

Saving results

Total time: 86.2328999042511
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1068/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1068.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1068
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00625178
Iteration 2/25 | Loss: 0.00128759
Iteration 3/25 | Loss: 0.00120179
Iteration 4/25 | Loss: 0.00119091
Iteration 5/25 | Loss: 0.00118710
Iteration 6/25 | Loss: 0.00118605
Iteration 7/25 | Loss: 0.00118605
Iteration 8/25 | Loss: 0.00118605
Iteration 9/25 | Loss: 0.00118605
Iteration 10/25 | Loss: 0.00118605
Iteration 11/25 | Loss: 0.00118605
Iteration 12/25 | Loss: 0.00118605
Iteration 13/25 | Loss: 0.00118605
Iteration 14/25 | Loss: 0.00118605
Iteration 15/25 | Loss: 0.00118605
Iteration 16/25 | Loss: 0.00118605
Iteration 17/25 | Loss: 0.00118605
Iteration 18/25 | Loss: 0.00118605
Iteration 19/25 | Loss: 0.00118605
Iteration 20/25 | Loss: 0.00118605
Iteration 21/25 | Loss: 0.00118605
Iteration 22/25 | Loss: 0.00118605
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011860475642606616, 0.0011860475642606616, 0.0011860475642606616, 0.0011860475642606616, 0.0011860475642606616]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011860475642606616

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.36770368
Iteration 2/25 | Loss: 0.00116257
Iteration 3/25 | Loss: 0.00116257
Iteration 4/25 | Loss: 0.00116257
Iteration 5/25 | Loss: 0.00116257
Iteration 6/25 | Loss: 0.00116257
Iteration 7/25 | Loss: 0.00116257
Iteration 8/25 | Loss: 0.00116257
Iteration 9/25 | Loss: 0.00116257
Iteration 10/25 | Loss: 0.00116257
Iteration 11/25 | Loss: 0.00116257
Iteration 12/25 | Loss: 0.00116257
Iteration 13/25 | Loss: 0.00116257
Iteration 14/25 | Loss: 0.00116257
Iteration 15/25 | Loss: 0.00116257
Iteration 16/25 | Loss: 0.00116257
Iteration 17/25 | Loss: 0.00116257
Iteration 18/25 | Loss: 0.00116257
Iteration 19/25 | Loss: 0.00116257
Iteration 20/25 | Loss: 0.00116257
Iteration 21/25 | Loss: 0.00116257
Iteration 22/25 | Loss: 0.00116257
Iteration 23/25 | Loss: 0.00116257
Iteration 24/25 | Loss: 0.00116257
Iteration 25/25 | Loss: 0.00116257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011625677580013871, 0.0011625677580013871, 0.0011625677580013871, 0.0011625677580013871, 0.0011625677580013871]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011625677580013871

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116257
Iteration 2/1000 | Loss: 0.00001895
Iteration 3/1000 | Loss: 0.00001311
Iteration 4/1000 | Loss: 0.00001171
Iteration 5/1000 | Loss: 0.00001113
Iteration 6/1000 | Loss: 0.00001074
Iteration 7/1000 | Loss: 0.00001037
Iteration 8/1000 | Loss: 0.00001029
Iteration 9/1000 | Loss: 0.00001002
Iteration 10/1000 | Loss: 0.00000985
Iteration 11/1000 | Loss: 0.00000984
Iteration 12/1000 | Loss: 0.00000984
Iteration 13/1000 | Loss: 0.00000982
Iteration 14/1000 | Loss: 0.00000974
Iteration 15/1000 | Loss: 0.00000971
Iteration 16/1000 | Loss: 0.00000970
Iteration 17/1000 | Loss: 0.00000965
Iteration 18/1000 | Loss: 0.00000964
Iteration 19/1000 | Loss: 0.00000963
Iteration 20/1000 | Loss: 0.00000962
Iteration 21/1000 | Loss: 0.00000961
Iteration 22/1000 | Loss: 0.00000958
Iteration 23/1000 | Loss: 0.00000957
Iteration 24/1000 | Loss: 0.00000957
Iteration 25/1000 | Loss: 0.00000956
Iteration 26/1000 | Loss: 0.00000956
Iteration 27/1000 | Loss: 0.00000956
Iteration 28/1000 | Loss: 0.00000955
Iteration 29/1000 | Loss: 0.00000955
Iteration 30/1000 | Loss: 0.00000952
Iteration 31/1000 | Loss: 0.00000951
Iteration 32/1000 | Loss: 0.00000951
Iteration 33/1000 | Loss: 0.00000951
Iteration 34/1000 | Loss: 0.00000949
Iteration 35/1000 | Loss: 0.00000947
Iteration 36/1000 | Loss: 0.00000947
Iteration 37/1000 | Loss: 0.00000946
Iteration 38/1000 | Loss: 0.00000945
Iteration 39/1000 | Loss: 0.00000945
Iteration 40/1000 | Loss: 0.00000945
Iteration 41/1000 | Loss: 0.00000944
Iteration 42/1000 | Loss: 0.00000944
Iteration 43/1000 | Loss: 0.00000941
Iteration 44/1000 | Loss: 0.00000940
Iteration 45/1000 | Loss: 0.00000940
Iteration 46/1000 | Loss: 0.00000940
Iteration 47/1000 | Loss: 0.00000939
Iteration 48/1000 | Loss: 0.00000938
Iteration 49/1000 | Loss: 0.00000937
Iteration 50/1000 | Loss: 0.00000937
Iteration 51/1000 | Loss: 0.00000936
Iteration 52/1000 | Loss: 0.00000936
Iteration 53/1000 | Loss: 0.00000936
Iteration 54/1000 | Loss: 0.00000936
Iteration 55/1000 | Loss: 0.00000936
Iteration 56/1000 | Loss: 0.00000936
Iteration 57/1000 | Loss: 0.00000936
Iteration 58/1000 | Loss: 0.00000936
Iteration 59/1000 | Loss: 0.00000935
Iteration 60/1000 | Loss: 0.00000934
Iteration 61/1000 | Loss: 0.00000934
Iteration 62/1000 | Loss: 0.00000934
Iteration 63/1000 | Loss: 0.00000934
Iteration 64/1000 | Loss: 0.00000934
Iteration 65/1000 | Loss: 0.00000934
Iteration 66/1000 | Loss: 0.00000933
Iteration 67/1000 | Loss: 0.00000933
Iteration 68/1000 | Loss: 0.00000933
Iteration 69/1000 | Loss: 0.00000932
Iteration 70/1000 | Loss: 0.00000932
Iteration 71/1000 | Loss: 0.00000932
Iteration 72/1000 | Loss: 0.00000931
Iteration 73/1000 | Loss: 0.00000931
Iteration 74/1000 | Loss: 0.00000931
Iteration 75/1000 | Loss: 0.00000931
Iteration 76/1000 | Loss: 0.00000931
Iteration 77/1000 | Loss: 0.00000931
Iteration 78/1000 | Loss: 0.00000931
Iteration 79/1000 | Loss: 0.00000931
Iteration 80/1000 | Loss: 0.00000931
Iteration 81/1000 | Loss: 0.00000931
Iteration 82/1000 | Loss: 0.00000931
Iteration 83/1000 | Loss: 0.00000930
Iteration 84/1000 | Loss: 0.00000930
Iteration 85/1000 | Loss: 0.00000929
Iteration 86/1000 | Loss: 0.00000929
Iteration 87/1000 | Loss: 0.00000929
Iteration 88/1000 | Loss: 0.00000927
Iteration 89/1000 | Loss: 0.00000927
Iteration 90/1000 | Loss: 0.00000927
Iteration 91/1000 | Loss: 0.00000927
Iteration 92/1000 | Loss: 0.00000927
Iteration 93/1000 | Loss: 0.00000926
Iteration 94/1000 | Loss: 0.00000926
Iteration 95/1000 | Loss: 0.00000926
Iteration 96/1000 | Loss: 0.00000926
Iteration 97/1000 | Loss: 0.00000926
Iteration 98/1000 | Loss: 0.00000926
Iteration 99/1000 | Loss: 0.00000925
Iteration 100/1000 | Loss: 0.00000925
Iteration 101/1000 | Loss: 0.00000925
Iteration 102/1000 | Loss: 0.00000924
Iteration 103/1000 | Loss: 0.00000924
Iteration 104/1000 | Loss: 0.00000924
Iteration 105/1000 | Loss: 0.00000924
Iteration 106/1000 | Loss: 0.00000924
Iteration 107/1000 | Loss: 0.00000923
Iteration 108/1000 | Loss: 0.00000923
Iteration 109/1000 | Loss: 0.00000923
Iteration 110/1000 | Loss: 0.00000923
Iteration 111/1000 | Loss: 0.00000923
Iteration 112/1000 | Loss: 0.00000922
Iteration 113/1000 | Loss: 0.00000921
Iteration 114/1000 | Loss: 0.00000921
Iteration 115/1000 | Loss: 0.00000921
Iteration 116/1000 | Loss: 0.00000921
Iteration 117/1000 | Loss: 0.00000921
Iteration 118/1000 | Loss: 0.00000921
Iteration 119/1000 | Loss: 0.00000920
Iteration 120/1000 | Loss: 0.00000920
Iteration 121/1000 | Loss: 0.00000920
Iteration 122/1000 | Loss: 0.00000920
Iteration 123/1000 | Loss: 0.00000920
Iteration 124/1000 | Loss: 0.00000920
Iteration 125/1000 | Loss: 0.00000920
Iteration 126/1000 | Loss: 0.00000920
Iteration 127/1000 | Loss: 0.00000920
Iteration 128/1000 | Loss: 0.00000919
Iteration 129/1000 | Loss: 0.00000919
Iteration 130/1000 | Loss: 0.00000919
Iteration 131/1000 | Loss: 0.00000918
Iteration 132/1000 | Loss: 0.00000918
Iteration 133/1000 | Loss: 0.00000918
Iteration 134/1000 | Loss: 0.00000918
Iteration 135/1000 | Loss: 0.00000918
Iteration 136/1000 | Loss: 0.00000918
Iteration 137/1000 | Loss: 0.00000918
Iteration 138/1000 | Loss: 0.00000918
Iteration 139/1000 | Loss: 0.00000918
Iteration 140/1000 | Loss: 0.00000918
Iteration 141/1000 | Loss: 0.00000917
Iteration 142/1000 | Loss: 0.00000917
Iteration 143/1000 | Loss: 0.00000917
Iteration 144/1000 | Loss: 0.00000917
Iteration 145/1000 | Loss: 0.00000917
Iteration 146/1000 | Loss: 0.00000917
Iteration 147/1000 | Loss: 0.00000917
Iteration 148/1000 | Loss: 0.00000917
Iteration 149/1000 | Loss: 0.00000917
Iteration 150/1000 | Loss: 0.00000917
Iteration 151/1000 | Loss: 0.00000917
Iteration 152/1000 | Loss: 0.00000917
Iteration 153/1000 | Loss: 0.00000916
Iteration 154/1000 | Loss: 0.00000916
Iteration 155/1000 | Loss: 0.00000916
Iteration 156/1000 | Loss: 0.00000916
Iteration 157/1000 | Loss: 0.00000916
Iteration 158/1000 | Loss: 0.00000916
Iteration 159/1000 | Loss: 0.00000916
Iteration 160/1000 | Loss: 0.00000915
Iteration 161/1000 | Loss: 0.00000915
Iteration 162/1000 | Loss: 0.00000915
Iteration 163/1000 | Loss: 0.00000915
Iteration 164/1000 | Loss: 0.00000915
Iteration 165/1000 | Loss: 0.00000915
Iteration 166/1000 | Loss: 0.00000915
Iteration 167/1000 | Loss: 0.00000915
Iteration 168/1000 | Loss: 0.00000915
Iteration 169/1000 | Loss: 0.00000915
Iteration 170/1000 | Loss: 0.00000915
Iteration 171/1000 | Loss: 0.00000915
Iteration 172/1000 | Loss: 0.00000915
Iteration 173/1000 | Loss: 0.00000915
Iteration 174/1000 | Loss: 0.00000914
Iteration 175/1000 | Loss: 0.00000914
Iteration 176/1000 | Loss: 0.00000914
Iteration 177/1000 | Loss: 0.00000914
Iteration 178/1000 | Loss: 0.00000914
Iteration 179/1000 | Loss: 0.00000914
Iteration 180/1000 | Loss: 0.00000914
Iteration 181/1000 | Loss: 0.00000914
Iteration 182/1000 | Loss: 0.00000913
Iteration 183/1000 | Loss: 0.00000913
Iteration 184/1000 | Loss: 0.00000913
Iteration 185/1000 | Loss: 0.00000913
Iteration 186/1000 | Loss: 0.00000913
Iteration 187/1000 | Loss: 0.00000913
Iteration 188/1000 | Loss: 0.00000913
Iteration 189/1000 | Loss: 0.00000913
Iteration 190/1000 | Loss: 0.00000913
Iteration 191/1000 | Loss: 0.00000913
Iteration 192/1000 | Loss: 0.00000913
Iteration 193/1000 | Loss: 0.00000913
Iteration 194/1000 | Loss: 0.00000913
Iteration 195/1000 | Loss: 0.00000913
Iteration 196/1000 | Loss: 0.00000913
Iteration 197/1000 | Loss: 0.00000913
Iteration 198/1000 | Loss: 0.00000913
Iteration 199/1000 | Loss: 0.00000913
Iteration 200/1000 | Loss: 0.00000913
Iteration 201/1000 | Loss: 0.00000913
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [9.127365046879277e-06, 9.127365046879277e-06, 9.127365046879277e-06, 9.127365046879277e-06, 9.127365046879277e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.127365046879277e-06

Optimization complete. Final v2v error: 2.5925302505493164 mm

Highest mean error: 3.2172114849090576 mm for frame 80

Lowest mean error: 2.4187917709350586 mm for frame 105

Saving results

Total time: 37.092740058898926
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1081/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1081.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1081
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407404
Iteration 2/25 | Loss: 0.00135652
Iteration 3/25 | Loss: 0.00127057
Iteration 4/25 | Loss: 0.00126094
Iteration 5/25 | Loss: 0.00125750
Iteration 6/25 | Loss: 0.00125649
Iteration 7/25 | Loss: 0.00125643
Iteration 8/25 | Loss: 0.00125643
Iteration 9/25 | Loss: 0.00125643
Iteration 10/25 | Loss: 0.00125643
Iteration 11/25 | Loss: 0.00125643
Iteration 12/25 | Loss: 0.00125643
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012564281933009624, 0.0012564281933009624, 0.0012564281933009624, 0.0012564281933009624, 0.0012564281933009624]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012564281933009624

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42305088
Iteration 2/25 | Loss: 0.00123048
Iteration 3/25 | Loss: 0.00123047
Iteration 4/25 | Loss: 0.00123047
Iteration 5/25 | Loss: 0.00123047
Iteration 6/25 | Loss: 0.00123047
Iteration 7/25 | Loss: 0.00123047
Iteration 8/25 | Loss: 0.00123047
Iteration 9/25 | Loss: 0.00123047
Iteration 10/25 | Loss: 0.00123047
Iteration 11/25 | Loss: 0.00123047
Iteration 12/25 | Loss: 0.00123047
Iteration 13/25 | Loss: 0.00123047
Iteration 14/25 | Loss: 0.00123047
Iteration 15/25 | Loss: 0.00123047
Iteration 16/25 | Loss: 0.00123047
Iteration 17/25 | Loss: 0.00123047
Iteration 18/25 | Loss: 0.00123047
Iteration 19/25 | Loss: 0.00123047
Iteration 20/25 | Loss: 0.00123047
Iteration 21/25 | Loss: 0.00123047
Iteration 22/25 | Loss: 0.00123047
Iteration 23/25 | Loss: 0.00123047
Iteration 24/25 | Loss: 0.00123047
Iteration 25/25 | Loss: 0.00123047

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00123047
Iteration 2/1000 | Loss: 0.00004405
Iteration 3/1000 | Loss: 0.00002865
Iteration 4/1000 | Loss: 0.00002092
Iteration 5/1000 | Loss: 0.00001867
Iteration 6/1000 | Loss: 0.00001762
Iteration 7/1000 | Loss: 0.00001681
Iteration 8/1000 | Loss: 0.00001624
Iteration 9/1000 | Loss: 0.00001578
Iteration 10/1000 | Loss: 0.00001547
Iteration 11/1000 | Loss: 0.00001524
Iteration 12/1000 | Loss: 0.00001501
Iteration 13/1000 | Loss: 0.00001479
Iteration 14/1000 | Loss: 0.00001465
Iteration 15/1000 | Loss: 0.00001461
Iteration 16/1000 | Loss: 0.00001454
Iteration 17/1000 | Loss: 0.00001449
Iteration 18/1000 | Loss: 0.00001449
Iteration 19/1000 | Loss: 0.00001448
Iteration 20/1000 | Loss: 0.00001447
Iteration 21/1000 | Loss: 0.00001446
Iteration 22/1000 | Loss: 0.00001445
Iteration 23/1000 | Loss: 0.00001445
Iteration 24/1000 | Loss: 0.00001445
Iteration 25/1000 | Loss: 0.00001444
Iteration 26/1000 | Loss: 0.00001444
Iteration 27/1000 | Loss: 0.00001443
Iteration 28/1000 | Loss: 0.00001443
Iteration 29/1000 | Loss: 0.00001443
Iteration 30/1000 | Loss: 0.00001442
Iteration 31/1000 | Loss: 0.00001442
Iteration 32/1000 | Loss: 0.00001440
Iteration 33/1000 | Loss: 0.00001439
Iteration 34/1000 | Loss: 0.00001439
Iteration 35/1000 | Loss: 0.00001438
Iteration 36/1000 | Loss: 0.00001436
Iteration 37/1000 | Loss: 0.00001436
Iteration 38/1000 | Loss: 0.00001435
Iteration 39/1000 | Loss: 0.00001435
Iteration 40/1000 | Loss: 0.00001433
Iteration 41/1000 | Loss: 0.00001433
Iteration 42/1000 | Loss: 0.00001433
Iteration 43/1000 | Loss: 0.00001433
Iteration 44/1000 | Loss: 0.00001433
Iteration 45/1000 | Loss: 0.00001433
Iteration 46/1000 | Loss: 0.00001433
Iteration 47/1000 | Loss: 0.00001433
Iteration 48/1000 | Loss: 0.00001432
Iteration 49/1000 | Loss: 0.00001432
Iteration 50/1000 | Loss: 0.00001431
Iteration 51/1000 | Loss: 0.00001431
Iteration 52/1000 | Loss: 0.00001431
Iteration 53/1000 | Loss: 0.00001430
Iteration 54/1000 | Loss: 0.00001430
Iteration 55/1000 | Loss: 0.00001430
Iteration 56/1000 | Loss: 0.00001430
Iteration 57/1000 | Loss: 0.00001430
Iteration 58/1000 | Loss: 0.00001429
Iteration 59/1000 | Loss: 0.00001429
Iteration 60/1000 | Loss: 0.00001429
Iteration 61/1000 | Loss: 0.00001428
Iteration 62/1000 | Loss: 0.00001428
Iteration 63/1000 | Loss: 0.00001428
Iteration 64/1000 | Loss: 0.00001427
Iteration 65/1000 | Loss: 0.00001427
Iteration 66/1000 | Loss: 0.00001427
Iteration 67/1000 | Loss: 0.00001427
Iteration 68/1000 | Loss: 0.00001426
Iteration 69/1000 | Loss: 0.00001426
Iteration 70/1000 | Loss: 0.00001426
Iteration 71/1000 | Loss: 0.00001426
Iteration 72/1000 | Loss: 0.00001426
Iteration 73/1000 | Loss: 0.00001426
Iteration 74/1000 | Loss: 0.00001425
Iteration 75/1000 | Loss: 0.00001425
Iteration 76/1000 | Loss: 0.00001424
Iteration 77/1000 | Loss: 0.00001424
Iteration 78/1000 | Loss: 0.00001424
Iteration 79/1000 | Loss: 0.00001424
Iteration 80/1000 | Loss: 0.00001424
Iteration 81/1000 | Loss: 0.00001424
Iteration 82/1000 | Loss: 0.00001424
Iteration 83/1000 | Loss: 0.00001423
Iteration 84/1000 | Loss: 0.00001423
Iteration 85/1000 | Loss: 0.00001423
Iteration 86/1000 | Loss: 0.00001422
Iteration 87/1000 | Loss: 0.00001422
Iteration 88/1000 | Loss: 0.00001422
Iteration 89/1000 | Loss: 0.00001422
Iteration 90/1000 | Loss: 0.00001422
Iteration 91/1000 | Loss: 0.00001421
Iteration 92/1000 | Loss: 0.00001421
Iteration 93/1000 | Loss: 0.00001421
Iteration 94/1000 | Loss: 0.00001421
Iteration 95/1000 | Loss: 0.00001421
Iteration 96/1000 | Loss: 0.00001420
Iteration 97/1000 | Loss: 0.00001420
Iteration 98/1000 | Loss: 0.00001420
Iteration 99/1000 | Loss: 0.00001420
Iteration 100/1000 | Loss: 0.00001420
Iteration 101/1000 | Loss: 0.00001420
Iteration 102/1000 | Loss: 0.00001420
Iteration 103/1000 | Loss: 0.00001419
Iteration 104/1000 | Loss: 0.00001419
Iteration 105/1000 | Loss: 0.00001419
Iteration 106/1000 | Loss: 0.00001419
Iteration 107/1000 | Loss: 0.00001419
Iteration 108/1000 | Loss: 0.00001418
Iteration 109/1000 | Loss: 0.00001418
Iteration 110/1000 | Loss: 0.00001418
Iteration 111/1000 | Loss: 0.00001418
Iteration 112/1000 | Loss: 0.00001417
Iteration 113/1000 | Loss: 0.00001417
Iteration 114/1000 | Loss: 0.00001417
Iteration 115/1000 | Loss: 0.00001417
Iteration 116/1000 | Loss: 0.00001416
Iteration 117/1000 | Loss: 0.00001416
Iteration 118/1000 | Loss: 0.00001416
Iteration 119/1000 | Loss: 0.00001416
Iteration 120/1000 | Loss: 0.00001416
Iteration 121/1000 | Loss: 0.00001416
Iteration 122/1000 | Loss: 0.00001415
Iteration 123/1000 | Loss: 0.00001415
Iteration 124/1000 | Loss: 0.00001415
Iteration 125/1000 | Loss: 0.00001415
Iteration 126/1000 | Loss: 0.00001415
Iteration 127/1000 | Loss: 0.00001415
Iteration 128/1000 | Loss: 0.00001415
Iteration 129/1000 | Loss: 0.00001414
Iteration 130/1000 | Loss: 0.00001414
Iteration 131/1000 | Loss: 0.00001414
Iteration 132/1000 | Loss: 0.00001414
Iteration 133/1000 | Loss: 0.00001414
Iteration 134/1000 | Loss: 0.00001414
Iteration 135/1000 | Loss: 0.00001414
Iteration 136/1000 | Loss: 0.00001414
Iteration 137/1000 | Loss: 0.00001414
Iteration 138/1000 | Loss: 0.00001414
Iteration 139/1000 | Loss: 0.00001414
Iteration 140/1000 | Loss: 0.00001413
Iteration 141/1000 | Loss: 0.00001413
Iteration 142/1000 | Loss: 0.00001413
Iteration 143/1000 | Loss: 0.00001413
Iteration 144/1000 | Loss: 0.00001413
Iteration 145/1000 | Loss: 0.00001413
Iteration 146/1000 | Loss: 0.00001413
Iteration 147/1000 | Loss: 0.00001413
Iteration 148/1000 | Loss: 0.00001413
Iteration 149/1000 | Loss: 0.00001413
Iteration 150/1000 | Loss: 0.00001413
Iteration 151/1000 | Loss: 0.00001413
Iteration 152/1000 | Loss: 0.00001413
Iteration 153/1000 | Loss: 0.00001413
Iteration 154/1000 | Loss: 0.00001413
Iteration 155/1000 | Loss: 0.00001413
Iteration 156/1000 | Loss: 0.00001413
Iteration 157/1000 | Loss: 0.00001413
Iteration 158/1000 | Loss: 0.00001413
Iteration 159/1000 | Loss: 0.00001413
Iteration 160/1000 | Loss: 0.00001413
Iteration 161/1000 | Loss: 0.00001413
Iteration 162/1000 | Loss: 0.00001413
Iteration 163/1000 | Loss: 0.00001413
Iteration 164/1000 | Loss: 0.00001413
Iteration 165/1000 | Loss: 0.00001413
Iteration 166/1000 | Loss: 0.00001413
Iteration 167/1000 | Loss: 0.00001413
Iteration 168/1000 | Loss: 0.00001413
Iteration 169/1000 | Loss: 0.00001413
Iteration 170/1000 | Loss: 0.00001413
Iteration 171/1000 | Loss: 0.00001413
Iteration 172/1000 | Loss: 0.00001413
Iteration 173/1000 | Loss: 0.00001413
Iteration 174/1000 | Loss: 0.00001413
Iteration 175/1000 | Loss: 0.00001413
Iteration 176/1000 | Loss: 0.00001413
Iteration 177/1000 | Loss: 0.00001413
Iteration 178/1000 | Loss: 0.00001413
Iteration 179/1000 | Loss: 0.00001413
Iteration 180/1000 | Loss: 0.00001413
Iteration 181/1000 | Loss: 0.00001413
Iteration 182/1000 | Loss: 0.00001413
Iteration 183/1000 | Loss: 0.00001413
Iteration 184/1000 | Loss: 0.00001413
Iteration 185/1000 | Loss: 0.00001413
Iteration 186/1000 | Loss: 0.00001413
Iteration 187/1000 | Loss: 0.00001413
Iteration 188/1000 | Loss: 0.00001413
Iteration 189/1000 | Loss: 0.00001413
Iteration 190/1000 | Loss: 0.00001413
Iteration 191/1000 | Loss: 0.00001413
Iteration 192/1000 | Loss: 0.00001413
Iteration 193/1000 | Loss: 0.00001413
Iteration 194/1000 | Loss: 0.00001413
Iteration 195/1000 | Loss: 0.00001413
Iteration 196/1000 | Loss: 0.00001413
Iteration 197/1000 | Loss: 0.00001413
Iteration 198/1000 | Loss: 0.00001413
Iteration 199/1000 | Loss: 0.00001413
Iteration 200/1000 | Loss: 0.00001413
Iteration 201/1000 | Loss: 0.00001413
Iteration 202/1000 | Loss: 0.00001413
Iteration 203/1000 | Loss: 0.00001413
Iteration 204/1000 | Loss: 0.00001413
Iteration 205/1000 | Loss: 0.00001413
Iteration 206/1000 | Loss: 0.00001413
Iteration 207/1000 | Loss: 0.00001413
Iteration 208/1000 | Loss: 0.00001413
Iteration 209/1000 | Loss: 0.00001413
Iteration 210/1000 | Loss: 0.00001413
Iteration 211/1000 | Loss: 0.00001413
Iteration 212/1000 | Loss: 0.00001413
Iteration 213/1000 | Loss: 0.00001413
Iteration 214/1000 | Loss: 0.00001413
Iteration 215/1000 | Loss: 0.00001413
Iteration 216/1000 | Loss: 0.00001413
Iteration 217/1000 | Loss: 0.00001413
Iteration 218/1000 | Loss: 0.00001413
Iteration 219/1000 | Loss: 0.00001413
Iteration 220/1000 | Loss: 0.00001413
Iteration 221/1000 | Loss: 0.00001413
Iteration 222/1000 | Loss: 0.00001413
Iteration 223/1000 | Loss: 0.00001413
Iteration 224/1000 | Loss: 0.00001413
Iteration 225/1000 | Loss: 0.00001413
Iteration 226/1000 | Loss: 0.00001413
Iteration 227/1000 | Loss: 0.00001413
Iteration 228/1000 | Loss: 0.00001413
Iteration 229/1000 | Loss: 0.00001413
Iteration 230/1000 | Loss: 0.00001413
Iteration 231/1000 | Loss: 0.00001413
Iteration 232/1000 | Loss: 0.00001413
Iteration 233/1000 | Loss: 0.00001413
Iteration 234/1000 | Loss: 0.00001413
Iteration 235/1000 | Loss: 0.00001413
Iteration 236/1000 | Loss: 0.00001413
Iteration 237/1000 | Loss: 0.00001413
Iteration 238/1000 | Loss: 0.00001413
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 238. Stopping optimization.
Last 5 losses: [1.4127576832834166e-05, 1.4127576832834166e-05, 1.4127576832834166e-05, 1.4127576832834166e-05, 1.4127576832834166e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4127576832834166e-05

Optimization complete. Final v2v error: 3.197727918624878 mm

Highest mean error: 4.569453239440918 mm for frame 67

Lowest mean error: 2.8596625328063965 mm for frame 44

Saving results

Total time: 42.74031043052673
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00994409
Iteration 2/25 | Loss: 0.00994408
Iteration 3/25 | Loss: 0.00317917
Iteration 4/25 | Loss: 0.00222303
Iteration 5/25 | Loss: 0.00172538
Iteration 6/25 | Loss: 0.00179275
Iteration 7/25 | Loss: 0.00165779
Iteration 8/25 | Loss: 0.00153753
Iteration 9/25 | Loss: 0.00147712
Iteration 10/25 | Loss: 0.00147143
Iteration 11/25 | Loss: 0.00141066
Iteration 12/25 | Loss: 0.00140275
Iteration 13/25 | Loss: 0.00139227
Iteration 14/25 | Loss: 0.00139038
Iteration 15/25 | Loss: 0.00139168
Iteration 16/25 | Loss: 0.00138970
Iteration 17/25 | Loss: 0.00138958
Iteration 18/25 | Loss: 0.00138958
Iteration 19/25 | Loss: 0.00138957
Iteration 20/25 | Loss: 0.00138957
Iteration 21/25 | Loss: 0.00138957
Iteration 22/25 | Loss: 0.00138957
Iteration 23/25 | Loss: 0.00138957
Iteration 24/25 | Loss: 0.00138957
Iteration 25/25 | Loss: 0.00138957

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26551139
Iteration 2/25 | Loss: 0.00115719
Iteration 3/25 | Loss: 0.00115719
Iteration 4/25 | Loss: 0.00115719
Iteration 5/25 | Loss: 0.00115719
Iteration 6/25 | Loss: 0.00115719
Iteration 7/25 | Loss: 0.00115719
Iteration 8/25 | Loss: 0.00115719
Iteration 9/25 | Loss: 0.00115719
Iteration 10/25 | Loss: 0.00115719
Iteration 11/25 | Loss: 0.00115719
Iteration 12/25 | Loss: 0.00115719
Iteration 13/25 | Loss: 0.00115719
Iteration 14/25 | Loss: 0.00115719
Iteration 15/25 | Loss: 0.00115719
Iteration 16/25 | Loss: 0.00115719
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011571882059797645, 0.0011571882059797645, 0.0011571882059797645, 0.0011571882059797645, 0.0011571882059797645]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011571882059797645

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115719
Iteration 2/1000 | Loss: 0.00005239
Iteration 3/1000 | Loss: 0.00011653
Iteration 4/1000 | Loss: 0.00004260
Iteration 5/1000 | Loss: 0.00003758
Iteration 6/1000 | Loss: 0.00003567
Iteration 7/1000 | Loss: 0.00003422
Iteration 8/1000 | Loss: 0.00003325
Iteration 9/1000 | Loss: 0.00003226
Iteration 10/1000 | Loss: 0.00003159
Iteration 11/1000 | Loss: 0.00003090
Iteration 12/1000 | Loss: 0.00020681
Iteration 13/1000 | Loss: 0.00006812
Iteration 14/1000 | Loss: 0.00003242
Iteration 15/1000 | Loss: 0.00003071
Iteration 16/1000 | Loss: 0.00002987
Iteration 17/1000 | Loss: 0.00011969
Iteration 18/1000 | Loss: 0.00003058
Iteration 19/1000 | Loss: 0.00002886
Iteration 20/1000 | Loss: 0.00002852
Iteration 21/1000 | Loss: 0.00002835
Iteration 22/1000 | Loss: 0.00002803
Iteration 23/1000 | Loss: 0.00002782
Iteration 24/1000 | Loss: 0.00002775
Iteration 25/1000 | Loss: 0.00002758
Iteration 26/1000 | Loss: 0.00002753
Iteration 27/1000 | Loss: 0.00002750
Iteration 28/1000 | Loss: 0.00002742
Iteration 29/1000 | Loss: 0.00002741
Iteration 30/1000 | Loss: 0.00002741
Iteration 31/1000 | Loss: 0.00002741
Iteration 32/1000 | Loss: 0.00002741
Iteration 33/1000 | Loss: 0.00002740
Iteration 34/1000 | Loss: 0.00002740
Iteration 35/1000 | Loss: 0.00002740
Iteration 36/1000 | Loss: 0.00002736
Iteration 37/1000 | Loss: 0.00002731
Iteration 38/1000 | Loss: 0.00002731
Iteration 39/1000 | Loss: 0.00002731
Iteration 40/1000 | Loss: 0.00002731
Iteration 41/1000 | Loss: 0.00002731
Iteration 42/1000 | Loss: 0.00002731
Iteration 43/1000 | Loss: 0.00002731
Iteration 44/1000 | Loss: 0.00002731
Iteration 45/1000 | Loss: 0.00002730
Iteration 46/1000 | Loss: 0.00002730
Iteration 47/1000 | Loss: 0.00002730
Iteration 48/1000 | Loss: 0.00002729
Iteration 49/1000 | Loss: 0.00002729
Iteration 50/1000 | Loss: 0.00002729
Iteration 51/1000 | Loss: 0.00002729
Iteration 52/1000 | Loss: 0.00002729
Iteration 53/1000 | Loss: 0.00002729
Iteration 54/1000 | Loss: 0.00002729
Iteration 55/1000 | Loss: 0.00002729
Iteration 56/1000 | Loss: 0.00002729
Iteration 57/1000 | Loss: 0.00002729
Iteration 58/1000 | Loss: 0.00002729
Iteration 59/1000 | Loss: 0.00002729
Iteration 60/1000 | Loss: 0.00002728
Iteration 61/1000 | Loss: 0.00002728
Iteration 62/1000 | Loss: 0.00002728
Iteration 63/1000 | Loss: 0.00002728
Iteration 64/1000 | Loss: 0.00002728
Iteration 65/1000 | Loss: 0.00002728
Iteration 66/1000 | Loss: 0.00002728
Iteration 67/1000 | Loss: 0.00002728
Iteration 68/1000 | Loss: 0.00002727
Iteration 69/1000 | Loss: 0.00002727
Iteration 70/1000 | Loss: 0.00002727
Iteration 71/1000 | Loss: 0.00002727
Iteration 72/1000 | Loss: 0.00002727
Iteration 73/1000 | Loss: 0.00002727
Iteration 74/1000 | Loss: 0.00002727
Iteration 75/1000 | Loss: 0.00002727
Iteration 76/1000 | Loss: 0.00002727
Iteration 77/1000 | Loss: 0.00002727
Iteration 78/1000 | Loss: 0.00002727
Iteration 79/1000 | Loss: 0.00002726
Iteration 80/1000 | Loss: 0.00002726
Iteration 81/1000 | Loss: 0.00002726
Iteration 82/1000 | Loss: 0.00002726
Iteration 83/1000 | Loss: 0.00002726
Iteration 84/1000 | Loss: 0.00002726
Iteration 85/1000 | Loss: 0.00002725
Iteration 86/1000 | Loss: 0.00002725
Iteration 87/1000 | Loss: 0.00002725
Iteration 88/1000 | Loss: 0.00002725
Iteration 89/1000 | Loss: 0.00002725
Iteration 90/1000 | Loss: 0.00002725
Iteration 91/1000 | Loss: 0.00002725
Iteration 92/1000 | Loss: 0.00002725
Iteration 93/1000 | Loss: 0.00002724
Iteration 94/1000 | Loss: 0.00002723
Iteration 95/1000 | Loss: 0.00002723
Iteration 96/1000 | Loss: 0.00002722
Iteration 97/1000 | Loss: 0.00002722
Iteration 98/1000 | Loss: 0.00002721
Iteration 99/1000 | Loss: 0.00002721
Iteration 100/1000 | Loss: 0.00002721
Iteration 101/1000 | Loss: 0.00002721
Iteration 102/1000 | Loss: 0.00002721
Iteration 103/1000 | Loss: 0.00002721
Iteration 104/1000 | Loss: 0.00002721
Iteration 105/1000 | Loss: 0.00002721
Iteration 106/1000 | Loss: 0.00002721
Iteration 107/1000 | Loss: 0.00002721
Iteration 108/1000 | Loss: 0.00002720
Iteration 109/1000 | Loss: 0.00002720
Iteration 110/1000 | Loss: 0.00002720
Iteration 111/1000 | Loss: 0.00002720
Iteration 112/1000 | Loss: 0.00002720
Iteration 113/1000 | Loss: 0.00002720
Iteration 114/1000 | Loss: 0.00002719
Iteration 115/1000 | Loss: 0.00002719
Iteration 116/1000 | Loss: 0.00002719
Iteration 117/1000 | Loss: 0.00002719
Iteration 118/1000 | Loss: 0.00002719
Iteration 119/1000 | Loss: 0.00002719
Iteration 120/1000 | Loss: 0.00002719
Iteration 121/1000 | Loss: 0.00002719
Iteration 122/1000 | Loss: 0.00002719
Iteration 123/1000 | Loss: 0.00002719
Iteration 124/1000 | Loss: 0.00002718
Iteration 125/1000 | Loss: 0.00002718
Iteration 126/1000 | Loss: 0.00002718
Iteration 127/1000 | Loss: 0.00002718
Iteration 128/1000 | Loss: 0.00002718
Iteration 129/1000 | Loss: 0.00002718
Iteration 130/1000 | Loss: 0.00002717
Iteration 131/1000 | Loss: 0.00002717
Iteration 132/1000 | Loss: 0.00002717
Iteration 133/1000 | Loss: 0.00002716
Iteration 134/1000 | Loss: 0.00002716
Iteration 135/1000 | Loss: 0.00002716
Iteration 136/1000 | Loss: 0.00002716
Iteration 137/1000 | Loss: 0.00002716
Iteration 138/1000 | Loss: 0.00002716
Iteration 139/1000 | Loss: 0.00002716
Iteration 140/1000 | Loss: 0.00002716
Iteration 141/1000 | Loss: 0.00002715
Iteration 142/1000 | Loss: 0.00002715
Iteration 143/1000 | Loss: 0.00002715
Iteration 144/1000 | Loss: 0.00002714
Iteration 145/1000 | Loss: 0.00002714
Iteration 146/1000 | Loss: 0.00002714
Iteration 147/1000 | Loss: 0.00002714
Iteration 148/1000 | Loss: 0.00002714
Iteration 149/1000 | Loss: 0.00002714
Iteration 150/1000 | Loss: 0.00002714
Iteration 151/1000 | Loss: 0.00002713
Iteration 152/1000 | Loss: 0.00002713
Iteration 153/1000 | Loss: 0.00002713
Iteration 154/1000 | Loss: 0.00002713
Iteration 155/1000 | Loss: 0.00002713
Iteration 156/1000 | Loss: 0.00002713
Iteration 157/1000 | Loss: 0.00002712
Iteration 158/1000 | Loss: 0.00002712
Iteration 159/1000 | Loss: 0.00002712
Iteration 160/1000 | Loss: 0.00002712
Iteration 161/1000 | Loss: 0.00002712
Iteration 162/1000 | Loss: 0.00002711
Iteration 163/1000 | Loss: 0.00002711
Iteration 164/1000 | Loss: 0.00002711
Iteration 165/1000 | Loss: 0.00002711
Iteration 166/1000 | Loss: 0.00002711
Iteration 167/1000 | Loss: 0.00002711
Iteration 168/1000 | Loss: 0.00002711
Iteration 169/1000 | Loss: 0.00002711
Iteration 170/1000 | Loss: 0.00002711
Iteration 171/1000 | Loss: 0.00002711
Iteration 172/1000 | Loss: 0.00002711
Iteration 173/1000 | Loss: 0.00002711
Iteration 174/1000 | Loss: 0.00002711
Iteration 175/1000 | Loss: 0.00002711
Iteration 176/1000 | Loss: 0.00002711
Iteration 177/1000 | Loss: 0.00002711
Iteration 178/1000 | Loss: 0.00002711
Iteration 179/1000 | Loss: 0.00002711
Iteration 180/1000 | Loss: 0.00002711
Iteration 181/1000 | Loss: 0.00002711
Iteration 182/1000 | Loss: 0.00002711
Iteration 183/1000 | Loss: 0.00002711
Iteration 184/1000 | Loss: 0.00002711
Iteration 185/1000 | Loss: 0.00002711
Iteration 186/1000 | Loss: 0.00002711
Iteration 187/1000 | Loss: 0.00002711
Iteration 188/1000 | Loss: 0.00002711
Iteration 189/1000 | Loss: 0.00002711
Iteration 190/1000 | Loss: 0.00002711
Iteration 191/1000 | Loss: 0.00002711
Iteration 192/1000 | Loss: 0.00002711
Iteration 193/1000 | Loss: 0.00002711
Iteration 194/1000 | Loss: 0.00002711
Iteration 195/1000 | Loss: 0.00002711
Iteration 196/1000 | Loss: 0.00002711
Iteration 197/1000 | Loss: 0.00002711
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 197. Stopping optimization.
Last 5 losses: [2.7105294066132046e-05, 2.7105294066132046e-05, 2.7105294066132046e-05, 2.7105294066132046e-05, 2.7105294066132046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7105294066132046e-05

Optimization complete. Final v2v error: 4.450654983520508 mm

Highest mean error: 5.010937213897705 mm for frame 239

Lowest mean error: 4.252694606781006 mm for frame 23

Saving results

Total time: 85.19084978103638
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00851039
Iteration 2/25 | Loss: 0.00159208
Iteration 3/25 | Loss: 0.00131652
Iteration 4/25 | Loss: 0.00127515
Iteration 5/25 | Loss: 0.00126680
Iteration 6/25 | Loss: 0.00126526
Iteration 7/25 | Loss: 0.00126516
Iteration 8/25 | Loss: 0.00126516
Iteration 9/25 | Loss: 0.00126516
Iteration 10/25 | Loss: 0.00126516
Iteration 11/25 | Loss: 0.00126516
Iteration 12/25 | Loss: 0.00126516
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012651631841436028, 0.0012651631841436028, 0.0012651631841436028, 0.0012651631841436028, 0.0012651631841436028]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012651631841436028

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.89109427
Iteration 2/25 | Loss: 0.00081050
Iteration 3/25 | Loss: 0.00081050
Iteration 4/25 | Loss: 0.00081050
Iteration 5/25 | Loss: 0.00081050
Iteration 6/25 | Loss: 0.00081050
Iteration 7/25 | Loss: 0.00081050
Iteration 8/25 | Loss: 0.00081050
Iteration 9/25 | Loss: 0.00081050
Iteration 10/25 | Loss: 0.00081050
Iteration 11/25 | Loss: 0.00081050
Iteration 12/25 | Loss: 0.00081050
Iteration 13/25 | Loss: 0.00081050
Iteration 14/25 | Loss: 0.00081050
Iteration 15/25 | Loss: 0.00081050
Iteration 16/25 | Loss: 0.00081050
Iteration 17/25 | Loss: 0.00081050
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0008104988955892622, 0.0008104988955892622, 0.0008104988955892622, 0.0008104988955892622, 0.0008104988955892622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008104988955892622

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081050
Iteration 2/1000 | Loss: 0.00005362
Iteration 3/1000 | Loss: 0.00003752
Iteration 4/1000 | Loss: 0.00003278
Iteration 5/1000 | Loss: 0.00003012
Iteration 6/1000 | Loss: 0.00002834
Iteration 7/1000 | Loss: 0.00002739
Iteration 8/1000 | Loss: 0.00002669
Iteration 9/1000 | Loss: 0.00002630
Iteration 10/1000 | Loss: 0.00002599
Iteration 11/1000 | Loss: 0.00002570
Iteration 12/1000 | Loss: 0.00002541
Iteration 13/1000 | Loss: 0.00002526
Iteration 14/1000 | Loss: 0.00002510
Iteration 15/1000 | Loss: 0.00002495
Iteration 16/1000 | Loss: 0.00002487
Iteration 17/1000 | Loss: 0.00002482
Iteration 18/1000 | Loss: 0.00002482
Iteration 19/1000 | Loss: 0.00002481
Iteration 20/1000 | Loss: 0.00002481
Iteration 21/1000 | Loss: 0.00002478
Iteration 22/1000 | Loss: 0.00002478
Iteration 23/1000 | Loss: 0.00002478
Iteration 24/1000 | Loss: 0.00002478
Iteration 25/1000 | Loss: 0.00002478
Iteration 26/1000 | Loss: 0.00002478
Iteration 27/1000 | Loss: 0.00002477
Iteration 28/1000 | Loss: 0.00002477
Iteration 29/1000 | Loss: 0.00002477
Iteration 30/1000 | Loss: 0.00002477
Iteration 31/1000 | Loss: 0.00002477
Iteration 32/1000 | Loss: 0.00002477
Iteration 33/1000 | Loss: 0.00002476
Iteration 34/1000 | Loss: 0.00002476
Iteration 35/1000 | Loss: 0.00002476
Iteration 36/1000 | Loss: 0.00002476
Iteration 37/1000 | Loss: 0.00002476
Iteration 38/1000 | Loss: 0.00002476
Iteration 39/1000 | Loss: 0.00002475
Iteration 40/1000 | Loss: 0.00002475
Iteration 41/1000 | Loss: 0.00002475
Iteration 42/1000 | Loss: 0.00002475
Iteration 43/1000 | Loss: 0.00002475
Iteration 44/1000 | Loss: 0.00002475
Iteration 45/1000 | Loss: 0.00002475
Iteration 46/1000 | Loss: 0.00002475
Iteration 47/1000 | Loss: 0.00002475
Iteration 48/1000 | Loss: 0.00002475
Iteration 49/1000 | Loss: 0.00002475
Iteration 50/1000 | Loss: 0.00002475
Iteration 51/1000 | Loss: 0.00002474
Iteration 52/1000 | Loss: 0.00002474
Iteration 53/1000 | Loss: 0.00002474
Iteration 54/1000 | Loss: 0.00002474
Iteration 55/1000 | Loss: 0.00002474
Iteration 56/1000 | Loss: 0.00002474
Iteration 57/1000 | Loss: 0.00002474
Iteration 58/1000 | Loss: 0.00002473
Iteration 59/1000 | Loss: 0.00002473
Iteration 60/1000 | Loss: 0.00002473
Iteration 61/1000 | Loss: 0.00002473
Iteration 62/1000 | Loss: 0.00002473
Iteration 63/1000 | Loss: 0.00002473
Iteration 64/1000 | Loss: 0.00002473
Iteration 65/1000 | Loss: 0.00002472
Iteration 66/1000 | Loss: 0.00002472
Iteration 67/1000 | Loss: 0.00002472
Iteration 68/1000 | Loss: 0.00002472
Iteration 69/1000 | Loss: 0.00002472
Iteration 70/1000 | Loss: 0.00002472
Iteration 71/1000 | Loss: 0.00002472
Iteration 72/1000 | Loss: 0.00002472
Iteration 73/1000 | Loss: 0.00002472
Iteration 74/1000 | Loss: 0.00002471
Iteration 75/1000 | Loss: 0.00002471
Iteration 76/1000 | Loss: 0.00002471
Iteration 77/1000 | Loss: 0.00002471
Iteration 78/1000 | Loss: 0.00002471
Iteration 79/1000 | Loss: 0.00002470
Iteration 80/1000 | Loss: 0.00002470
Iteration 81/1000 | Loss: 0.00002470
Iteration 82/1000 | Loss: 0.00002470
Iteration 83/1000 | Loss: 0.00002469
Iteration 84/1000 | Loss: 0.00002469
Iteration 85/1000 | Loss: 0.00002469
Iteration 86/1000 | Loss: 0.00002469
Iteration 87/1000 | Loss: 0.00002469
Iteration 88/1000 | Loss: 0.00002469
Iteration 89/1000 | Loss: 0.00002469
Iteration 90/1000 | Loss: 0.00002469
Iteration 91/1000 | Loss: 0.00002468
Iteration 92/1000 | Loss: 0.00002468
Iteration 93/1000 | Loss: 0.00002468
Iteration 94/1000 | Loss: 0.00002468
Iteration 95/1000 | Loss: 0.00002468
Iteration 96/1000 | Loss: 0.00002468
Iteration 97/1000 | Loss: 0.00002468
Iteration 98/1000 | Loss: 0.00002468
Iteration 99/1000 | Loss: 0.00002468
Iteration 100/1000 | Loss: 0.00002468
Iteration 101/1000 | Loss: 0.00002468
Iteration 102/1000 | Loss: 0.00002468
Iteration 103/1000 | Loss: 0.00002468
Iteration 104/1000 | Loss: 0.00002468
Iteration 105/1000 | Loss: 0.00002468
Iteration 106/1000 | Loss: 0.00002468
Iteration 107/1000 | Loss: 0.00002468
Iteration 108/1000 | Loss: 0.00002468
Iteration 109/1000 | Loss: 0.00002468
Iteration 110/1000 | Loss: 0.00002468
Iteration 111/1000 | Loss: 0.00002468
Iteration 112/1000 | Loss: 0.00002468
Iteration 113/1000 | Loss: 0.00002468
Iteration 114/1000 | Loss: 0.00002468
Iteration 115/1000 | Loss: 0.00002468
Iteration 116/1000 | Loss: 0.00002468
Iteration 117/1000 | Loss: 0.00002468
Iteration 118/1000 | Loss: 0.00002468
Iteration 119/1000 | Loss: 0.00002468
Iteration 120/1000 | Loss: 0.00002468
Iteration 121/1000 | Loss: 0.00002468
Iteration 122/1000 | Loss: 0.00002468
Iteration 123/1000 | Loss: 0.00002468
Iteration 124/1000 | Loss: 0.00002468
Iteration 125/1000 | Loss: 0.00002468
Iteration 126/1000 | Loss: 0.00002468
Iteration 127/1000 | Loss: 0.00002468
Iteration 128/1000 | Loss: 0.00002468
Iteration 129/1000 | Loss: 0.00002468
Iteration 130/1000 | Loss: 0.00002468
Iteration 131/1000 | Loss: 0.00002468
Iteration 132/1000 | Loss: 0.00002468
Iteration 133/1000 | Loss: 0.00002468
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 133. Stopping optimization.
Last 5 losses: [2.4681354261701927e-05, 2.4681354261701927e-05, 2.4681354261701927e-05, 2.4681354261701927e-05, 2.4681354261701927e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4681354261701927e-05

Optimization complete. Final v2v error: 4.294314384460449 mm

Highest mean error: 4.5657854080200195 mm for frame 100

Lowest mean error: 3.801177978515625 mm for frame 0

Saving results

Total time: 36.331632137298584
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_celina_posed_002/1083/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1083.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_celina_posed_002/1083
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01169469
Iteration 2/25 | Loss: 0.00283503
Iteration 3/25 | Loss: 0.00209558
Iteration 4/25 | Loss: 0.00200630
Iteration 5/25 | Loss: 0.00220583
Iteration 6/25 | Loss: 0.00200980
Iteration 7/25 | Loss: 0.00192543
Iteration 8/25 | Loss: 0.00179728
Iteration 9/25 | Loss: 0.00174379
Iteration 10/25 | Loss: 0.00180951
Iteration 11/25 | Loss: 0.00172725
Iteration 12/25 | Loss: 0.00170808
Iteration 13/25 | Loss: 0.00172168
Iteration 14/25 | Loss: 0.00169300
Iteration 15/25 | Loss: 0.00168180
Iteration 16/25 | Loss: 0.00168085
Iteration 17/25 | Loss: 0.00167966
Iteration 18/25 | Loss: 0.00167839
Iteration 19/25 | Loss: 0.00167713
Iteration 20/25 | Loss: 0.00167585
Iteration 21/25 | Loss: 0.00167515
Iteration 22/25 | Loss: 0.00167468
Iteration 23/25 | Loss: 0.00167434
Iteration 24/25 | Loss: 0.00167416
Iteration 25/25 | Loss: 0.00167405

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.74984235
Iteration 2/25 | Loss: 0.00325729
Iteration 3/25 | Loss: 0.00325729
Iteration 4/25 | Loss: 0.00325729
Iteration 5/25 | Loss: 0.00325729
Iteration 6/25 | Loss: 0.00325729
Iteration 7/25 | Loss: 0.00325729
Iteration 8/25 | Loss: 0.00325729
Iteration 9/25 | Loss: 0.00325729
Iteration 10/25 | Loss: 0.00325729
Iteration 11/25 | Loss: 0.00325729
Iteration 12/25 | Loss: 0.00325729
Iteration 13/25 | Loss: 0.00325729
Iteration 14/25 | Loss: 0.00325729
Iteration 15/25 | Loss: 0.00325729
Iteration 16/25 | Loss: 0.00325729
Iteration 17/25 | Loss: 0.00325729
Iteration 18/25 | Loss: 0.00325729
Iteration 19/25 | Loss: 0.00325729
Iteration 20/25 | Loss: 0.00325729
Iteration 21/25 | Loss: 0.00325729
Iteration 22/25 | Loss: 0.00325729
Iteration 23/25 | Loss: 0.00325729
Iteration 24/25 | Loss: 0.00325729
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0032572865020483732, 0.0032572865020483732, 0.0032572865020483732, 0.0032572865020483732, 0.0032572865020483732]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0032572865020483732

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00325729
Iteration 2/1000 | Loss: 0.00109274
Iteration 3/1000 | Loss: 0.00123425
Iteration 4/1000 | Loss: 0.00023120
Iteration 5/1000 | Loss: 0.00019168
Iteration 6/1000 | Loss: 0.00017629
Iteration 7/1000 | Loss: 0.00079372
Iteration 8/1000 | Loss: 0.00192106
Iteration 9/1000 | Loss: 0.00033903
Iteration 10/1000 | Loss: 0.00055156
Iteration 11/1000 | Loss: 0.00073520
Iteration 12/1000 | Loss: 0.00097188
Iteration 13/1000 | Loss: 0.00047656
Iteration 14/1000 | Loss: 0.00014024
Iteration 15/1000 | Loss: 0.00013109
Iteration 16/1000 | Loss: 0.00142700
Iteration 17/1000 | Loss: 0.00096508
Iteration 18/1000 | Loss: 0.00368800
Iteration 19/1000 | Loss: 0.00226794
Iteration 20/1000 | Loss: 0.00273010
Iteration 21/1000 | Loss: 0.00142845
Iteration 22/1000 | Loss: 0.00189998
Iteration 23/1000 | Loss: 0.00088371
Iteration 24/1000 | Loss: 0.00012986
Iteration 25/1000 | Loss: 0.00056918
Iteration 26/1000 | Loss: 0.00014284
Iteration 27/1000 | Loss: 0.00012850
Iteration 28/1000 | Loss: 0.00177856
Iteration 29/1000 | Loss: 0.00182384
Iteration 30/1000 | Loss: 0.00291565
Iteration 31/1000 | Loss: 0.00097163
Iteration 32/1000 | Loss: 0.00183985
Iteration 33/1000 | Loss: 0.00085556
Iteration 34/1000 | Loss: 0.00076463
Iteration 35/1000 | Loss: 0.00023617
Iteration 36/1000 | Loss: 0.00020348
Iteration 37/1000 | Loss: 0.00071391
Iteration 38/1000 | Loss: 0.00054480
Iteration 39/1000 | Loss: 0.00056977
Iteration 40/1000 | Loss: 0.00066701
Iteration 41/1000 | Loss: 0.00065255
Iteration 42/1000 | Loss: 0.00013854
Iteration 43/1000 | Loss: 0.00011515
Iteration 44/1000 | Loss: 0.00010482
Iteration 45/1000 | Loss: 0.00009736
Iteration 46/1000 | Loss: 0.00009160
Iteration 47/1000 | Loss: 0.00008782
Iteration 48/1000 | Loss: 0.00008533
Iteration 49/1000 | Loss: 0.00008361
Iteration 50/1000 | Loss: 0.00008217
Iteration 51/1000 | Loss: 0.00008127
Iteration 52/1000 | Loss: 0.00008047
Iteration 53/1000 | Loss: 0.00007961
Iteration 54/1000 | Loss: 0.00007903
Iteration 55/1000 | Loss: 0.00076543
Iteration 56/1000 | Loss: 0.00008059
Iteration 57/1000 | Loss: 0.00007774
Iteration 58/1000 | Loss: 0.00007660
Iteration 59/1000 | Loss: 0.00007561
Iteration 60/1000 | Loss: 0.00007482
Iteration 61/1000 | Loss: 0.00007430
Iteration 62/1000 | Loss: 0.00007379
Iteration 63/1000 | Loss: 0.00007329
Iteration 64/1000 | Loss: 0.00007279
Iteration 65/1000 | Loss: 0.00007249
Iteration 66/1000 | Loss: 0.00007213
Iteration 67/1000 | Loss: 0.00007182
Iteration 68/1000 | Loss: 0.00007129
Iteration 69/1000 | Loss: 0.00007061
Iteration 70/1000 | Loss: 0.00007002
Iteration 71/1000 | Loss: 0.00006948
Iteration 72/1000 | Loss: 0.00006917
Iteration 73/1000 | Loss: 0.00006891
Iteration 74/1000 | Loss: 0.00006860
Iteration 75/1000 | Loss: 0.00006838
Iteration 76/1000 | Loss: 0.00006825
Iteration 77/1000 | Loss: 0.00006823
Iteration 78/1000 | Loss: 0.00006820
Iteration 79/1000 | Loss: 0.00006819
Iteration 80/1000 | Loss: 0.00006819
Iteration 81/1000 | Loss: 0.00006815
Iteration 82/1000 | Loss: 0.00006815
Iteration 83/1000 | Loss: 0.00006813
Iteration 84/1000 | Loss: 0.00006809
Iteration 85/1000 | Loss: 0.00006804
Iteration 86/1000 | Loss: 0.00006803
Iteration 87/1000 | Loss: 0.00006796
Iteration 88/1000 | Loss: 0.00006791
Iteration 89/1000 | Loss: 0.00006791
Iteration 90/1000 | Loss: 0.00006791
Iteration 91/1000 | Loss: 0.00006791
Iteration 92/1000 | Loss: 0.00006791
Iteration 93/1000 | Loss: 0.00006791
Iteration 94/1000 | Loss: 0.00006791
Iteration 95/1000 | Loss: 0.00006791
Iteration 96/1000 | Loss: 0.00006791
Iteration 97/1000 | Loss: 0.00006791
Iteration 98/1000 | Loss: 0.00006791
Iteration 99/1000 | Loss: 0.00006790
Iteration 100/1000 | Loss: 0.00006790
Iteration 101/1000 | Loss: 0.00006790
Iteration 102/1000 | Loss: 0.00006790
Iteration 103/1000 | Loss: 0.00006790
Iteration 104/1000 | Loss: 0.00006790
Iteration 105/1000 | Loss: 0.00006789
Iteration 106/1000 | Loss: 0.00006789
Iteration 107/1000 | Loss: 0.00006789
Iteration 108/1000 | Loss: 0.00006789
Iteration 109/1000 | Loss: 0.00006789
Iteration 110/1000 | Loss: 0.00006788
Iteration 111/1000 | Loss: 0.00006788
Iteration 112/1000 | Loss: 0.00006788
Iteration 113/1000 | Loss: 0.00006787
Iteration 114/1000 | Loss: 0.00006787
Iteration 115/1000 | Loss: 0.00006787
Iteration 116/1000 | Loss: 0.00006786
Iteration 117/1000 | Loss: 0.00006786
Iteration 118/1000 | Loss: 0.00006786
Iteration 119/1000 | Loss: 0.00006786
Iteration 120/1000 | Loss: 0.00006786
Iteration 121/1000 | Loss: 0.00006786
Iteration 122/1000 | Loss: 0.00006786
Iteration 123/1000 | Loss: 0.00006786
Iteration 124/1000 | Loss: 0.00006785
Iteration 125/1000 | Loss: 0.00006785
Iteration 126/1000 | Loss: 0.00006785
Iteration 127/1000 | Loss: 0.00006785
Iteration 128/1000 | Loss: 0.00006785
Iteration 129/1000 | Loss: 0.00006785
Iteration 130/1000 | Loss: 0.00006785
Iteration 131/1000 | Loss: 0.00006785
Iteration 132/1000 | Loss: 0.00006785
Iteration 133/1000 | Loss: 0.00006785
Iteration 134/1000 | Loss: 0.00006785
Iteration 135/1000 | Loss: 0.00006784
Iteration 136/1000 | Loss: 0.00006784
Iteration 137/1000 | Loss: 0.00006784
Iteration 138/1000 | Loss: 0.00006784
Iteration 139/1000 | Loss: 0.00006784
Iteration 140/1000 | Loss: 0.00006783
Iteration 141/1000 | Loss: 0.00006783
Iteration 142/1000 | Loss: 0.00006783
Iteration 143/1000 | Loss: 0.00006783
Iteration 144/1000 | Loss: 0.00006783
Iteration 145/1000 | Loss: 0.00006783
Iteration 146/1000 | Loss: 0.00006783
Iteration 147/1000 | Loss: 0.00006783
Iteration 148/1000 | Loss: 0.00006783
Iteration 149/1000 | Loss: 0.00006783
Iteration 150/1000 | Loss: 0.00006783
Iteration 151/1000 | Loss: 0.00006783
Iteration 152/1000 | Loss: 0.00006783
Iteration 153/1000 | Loss: 0.00006783
Iteration 154/1000 | Loss: 0.00006783
Iteration 155/1000 | Loss: 0.00006783
Iteration 156/1000 | Loss: 0.00006783
Iteration 157/1000 | Loss: 0.00006783
Iteration 158/1000 | Loss: 0.00006782
Iteration 159/1000 | Loss: 0.00006782
Iteration 160/1000 | Loss: 0.00006782
Iteration 161/1000 | Loss: 0.00006781
Iteration 162/1000 | Loss: 0.00006781
Iteration 163/1000 | Loss: 0.00006781
Iteration 164/1000 | Loss: 0.00006780
Iteration 165/1000 | Loss: 0.00006780
Iteration 166/1000 | Loss: 0.00006780
Iteration 167/1000 | Loss: 0.00006780
Iteration 168/1000 | Loss: 0.00006779
Iteration 169/1000 | Loss: 0.00006779
Iteration 170/1000 | Loss: 0.00006778
Iteration 171/1000 | Loss: 0.00006778
Iteration 172/1000 | Loss: 0.00006778
Iteration 173/1000 | Loss: 0.00006778
Iteration 174/1000 | Loss: 0.00006778
Iteration 175/1000 | Loss: 0.00006778
Iteration 176/1000 | Loss: 0.00006778
Iteration 177/1000 | Loss: 0.00006778
Iteration 178/1000 | Loss: 0.00006778
Iteration 179/1000 | Loss: 0.00006778
Iteration 180/1000 | Loss: 0.00006778
Iteration 181/1000 | Loss: 0.00006778
Iteration 182/1000 | Loss: 0.00006778
Iteration 183/1000 | Loss: 0.00006778
Iteration 184/1000 | Loss: 0.00006778
Iteration 185/1000 | Loss: 0.00006777
Iteration 186/1000 | Loss: 0.00006777
Iteration 187/1000 | Loss: 0.00006777
Iteration 188/1000 | Loss: 0.00006777
Iteration 189/1000 | Loss: 0.00006777
Iteration 190/1000 | Loss: 0.00006777
Iteration 191/1000 | Loss: 0.00006777
Iteration 192/1000 | Loss: 0.00006777
Iteration 193/1000 | Loss: 0.00006777
Iteration 194/1000 | Loss: 0.00006777
Iteration 195/1000 | Loss: 0.00006777
Iteration 196/1000 | Loss: 0.00006777
Iteration 197/1000 | Loss: 0.00006777
Iteration 198/1000 | Loss: 0.00006777
Iteration 199/1000 | Loss: 0.00006777
Iteration 200/1000 | Loss: 0.00006777
Iteration 201/1000 | Loss: 0.00006777
Iteration 202/1000 | Loss: 0.00006777
Iteration 203/1000 | Loss: 0.00006777
Iteration 204/1000 | Loss: 0.00006777
Iteration 205/1000 | Loss: 0.00006777
Iteration 206/1000 | Loss: 0.00006777
Iteration 207/1000 | Loss: 0.00006777
Iteration 208/1000 | Loss: 0.00006777
Iteration 209/1000 | Loss: 0.00006777
Iteration 210/1000 | Loss: 0.00006777
Iteration 211/1000 | Loss: 0.00006777
Iteration 212/1000 | Loss: 0.00006777
Iteration 213/1000 | Loss: 0.00006777
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 213. Stopping optimization.
Last 5 losses: [6.777174712624401e-05, 6.777174712624401e-05, 6.777174712624401e-05, 6.777174712624401e-05, 6.777174712624401e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.777174712624401e-05

Optimization complete. Final v2v error: 5.453193664550781 mm

Highest mean error: 11.098173141479492 mm for frame 10

Lowest mean error: 4.466198921203613 mm for frame 134

Saving results

Total time: 156.61701822280884
