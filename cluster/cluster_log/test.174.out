Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=174, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 9744-9799
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00402907
Iteration 2/25 | Loss: 0.00090534
Iteration 3/25 | Loss: 0.00074634
Iteration 4/25 | Loss: 0.00072272
Iteration 5/25 | Loss: 0.00071870
Iteration 6/25 | Loss: 0.00071727
Iteration 7/25 | Loss: 0.00071691
Iteration 8/25 | Loss: 0.00071691
Iteration 9/25 | Loss: 0.00071691
Iteration 10/25 | Loss: 0.00071691
Iteration 11/25 | Loss: 0.00071691
Iteration 12/25 | Loss: 0.00071691
Iteration 13/25 | Loss: 0.00071691
Iteration 14/25 | Loss: 0.00071691
Iteration 15/25 | Loss: 0.00071691
Iteration 16/25 | Loss: 0.00071691
Iteration 17/25 | Loss: 0.00071691
Iteration 18/25 | Loss: 0.00071691
Iteration 19/25 | Loss: 0.00071691
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0007169118034653366, 0.0007169118034653366, 0.0007169118034653366, 0.0007169118034653366, 0.0007169118034653366]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007169118034653366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60233617
Iteration 2/25 | Loss: 0.00107810
Iteration 3/25 | Loss: 0.00107809
Iteration 4/25 | Loss: 0.00107809
Iteration 5/25 | Loss: 0.00107809
Iteration 6/25 | Loss: 0.00107809
Iteration 7/25 | Loss: 0.00107809
Iteration 8/25 | Loss: 0.00107809
Iteration 9/25 | Loss: 0.00107809
Iteration 10/25 | Loss: 0.00107808
Iteration 11/25 | Loss: 0.00107808
Iteration 12/25 | Loss: 0.00107808
Iteration 13/25 | Loss: 0.00107808
Iteration 14/25 | Loss: 0.00107808
Iteration 15/25 | Loss: 0.00107808
Iteration 16/25 | Loss: 0.00107808
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010780846932902932, 0.0010780846932902932, 0.0010780846932902932, 0.0010780846932902932, 0.0010780846932902932]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010780846932902932

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107808
Iteration 2/1000 | Loss: 0.00002474
Iteration 3/1000 | Loss: 0.00001406
Iteration 4/1000 | Loss: 0.00001251
Iteration 5/1000 | Loss: 0.00001181
Iteration 6/1000 | Loss: 0.00001125
Iteration 7/1000 | Loss: 0.00001096
Iteration 8/1000 | Loss: 0.00001079
Iteration 9/1000 | Loss: 0.00001077
Iteration 10/1000 | Loss: 0.00001075
Iteration 11/1000 | Loss: 0.00001074
Iteration 12/1000 | Loss: 0.00001074
Iteration 13/1000 | Loss: 0.00001069
Iteration 14/1000 | Loss: 0.00001069
Iteration 15/1000 | Loss: 0.00001068
Iteration 16/1000 | Loss: 0.00001064
Iteration 17/1000 | Loss: 0.00001063
Iteration 18/1000 | Loss: 0.00001060
Iteration 19/1000 | Loss: 0.00001057
Iteration 20/1000 | Loss: 0.00001057
Iteration 21/1000 | Loss: 0.00001056
Iteration 22/1000 | Loss: 0.00001052
Iteration 23/1000 | Loss: 0.00001050
Iteration 24/1000 | Loss: 0.00001049
Iteration 25/1000 | Loss: 0.00001047
Iteration 26/1000 | Loss: 0.00001047
Iteration 27/1000 | Loss: 0.00001047
Iteration 28/1000 | Loss: 0.00001047
Iteration 29/1000 | Loss: 0.00001047
Iteration 30/1000 | Loss: 0.00001047
Iteration 31/1000 | Loss: 0.00001046
Iteration 32/1000 | Loss: 0.00001046
Iteration 33/1000 | Loss: 0.00001045
Iteration 34/1000 | Loss: 0.00001045
Iteration 35/1000 | Loss: 0.00001044
Iteration 36/1000 | Loss: 0.00001040
Iteration 37/1000 | Loss: 0.00001037
Iteration 38/1000 | Loss: 0.00001036
Iteration 39/1000 | Loss: 0.00001036
Iteration 40/1000 | Loss: 0.00001035
Iteration 41/1000 | Loss: 0.00001035
Iteration 42/1000 | Loss: 0.00001034
Iteration 43/1000 | Loss: 0.00001034
Iteration 44/1000 | Loss: 0.00001034
Iteration 45/1000 | Loss: 0.00001034
Iteration 46/1000 | Loss: 0.00001033
Iteration 47/1000 | Loss: 0.00001033
Iteration 48/1000 | Loss: 0.00001032
Iteration 49/1000 | Loss: 0.00001031
Iteration 50/1000 | Loss: 0.00001031
Iteration 51/1000 | Loss: 0.00001031
Iteration 52/1000 | Loss: 0.00001031
Iteration 53/1000 | Loss: 0.00001031
Iteration 54/1000 | Loss: 0.00001031
Iteration 55/1000 | Loss: 0.00001031
Iteration 56/1000 | Loss: 0.00001031
Iteration 57/1000 | Loss: 0.00001031
Iteration 58/1000 | Loss: 0.00001030
Iteration 59/1000 | Loss: 0.00001030
Iteration 60/1000 | Loss: 0.00001030
Iteration 61/1000 | Loss: 0.00001030
Iteration 62/1000 | Loss: 0.00001030
Iteration 63/1000 | Loss: 0.00001029
Iteration 64/1000 | Loss: 0.00001029
Iteration 65/1000 | Loss: 0.00001029
Iteration 66/1000 | Loss: 0.00001029
Iteration 67/1000 | Loss: 0.00001029
Iteration 68/1000 | Loss: 0.00001028
Iteration 69/1000 | Loss: 0.00001028
Iteration 70/1000 | Loss: 0.00001028
Iteration 71/1000 | Loss: 0.00001028
Iteration 72/1000 | Loss: 0.00001028
Iteration 73/1000 | Loss: 0.00001028
Iteration 74/1000 | Loss: 0.00001027
Iteration 75/1000 | Loss: 0.00001027
Iteration 76/1000 | Loss: 0.00001027
Iteration 77/1000 | Loss: 0.00001026
Iteration 78/1000 | Loss: 0.00001026
Iteration 79/1000 | Loss: 0.00001026
Iteration 80/1000 | Loss: 0.00001026
Iteration 81/1000 | Loss: 0.00001026
Iteration 82/1000 | Loss: 0.00001025
Iteration 83/1000 | Loss: 0.00001025
Iteration 84/1000 | Loss: 0.00001025
Iteration 85/1000 | Loss: 0.00001025
Iteration 86/1000 | Loss: 0.00001025
Iteration 87/1000 | Loss: 0.00001025
Iteration 88/1000 | Loss: 0.00001025
Iteration 89/1000 | Loss: 0.00001024
Iteration 90/1000 | Loss: 0.00001024
Iteration 91/1000 | Loss: 0.00001024
Iteration 92/1000 | Loss: 0.00001023
Iteration 93/1000 | Loss: 0.00001023
Iteration 94/1000 | Loss: 0.00001023
Iteration 95/1000 | Loss: 0.00001022
Iteration 96/1000 | Loss: 0.00001022
Iteration 97/1000 | Loss: 0.00001022
Iteration 98/1000 | Loss: 0.00001021
Iteration 99/1000 | Loss: 0.00001021
Iteration 100/1000 | Loss: 0.00001021
Iteration 101/1000 | Loss: 0.00001021
Iteration 102/1000 | Loss: 0.00001021
Iteration 103/1000 | Loss: 0.00001020
Iteration 104/1000 | Loss: 0.00001020
Iteration 105/1000 | Loss: 0.00001020
Iteration 106/1000 | Loss: 0.00001019
Iteration 107/1000 | Loss: 0.00001019
Iteration 108/1000 | Loss: 0.00001019
Iteration 109/1000 | Loss: 0.00001019
Iteration 110/1000 | Loss: 0.00001019
Iteration 111/1000 | Loss: 0.00001019
Iteration 112/1000 | Loss: 0.00001019
Iteration 113/1000 | Loss: 0.00001019
Iteration 114/1000 | Loss: 0.00001019
Iteration 115/1000 | Loss: 0.00001019
Iteration 116/1000 | Loss: 0.00001019
Iteration 117/1000 | Loss: 0.00001019
Iteration 118/1000 | Loss: 0.00001019
Iteration 119/1000 | Loss: 0.00001019
Iteration 120/1000 | Loss: 0.00001019
Iteration 121/1000 | Loss: 0.00001019
Iteration 122/1000 | Loss: 0.00001019
Iteration 123/1000 | Loss: 0.00001019
Iteration 124/1000 | Loss: 0.00001019
Iteration 125/1000 | Loss: 0.00001019
Iteration 126/1000 | Loss: 0.00001019
Iteration 127/1000 | Loss: 0.00001019
Iteration 128/1000 | Loss: 0.00001019
Iteration 129/1000 | Loss: 0.00001019
Iteration 130/1000 | Loss: 0.00001019
Iteration 131/1000 | Loss: 0.00001019
Iteration 132/1000 | Loss: 0.00001019
Iteration 133/1000 | Loss: 0.00001019
Iteration 134/1000 | Loss: 0.00001019
Iteration 135/1000 | Loss: 0.00001019
Iteration 136/1000 | Loss: 0.00001019
Iteration 137/1000 | Loss: 0.00001019
Iteration 138/1000 | Loss: 0.00001019
Iteration 139/1000 | Loss: 0.00001019
Iteration 140/1000 | Loss: 0.00001019
Iteration 141/1000 | Loss: 0.00001019
Iteration 142/1000 | Loss: 0.00001019
Iteration 143/1000 | Loss: 0.00001019
Iteration 144/1000 | Loss: 0.00001019
Iteration 145/1000 | Loss: 0.00001019
Iteration 146/1000 | Loss: 0.00001019
Iteration 147/1000 | Loss: 0.00001019
Iteration 148/1000 | Loss: 0.00001019
Iteration 149/1000 | Loss: 0.00001019
Iteration 150/1000 | Loss: 0.00001019
Iteration 151/1000 | Loss: 0.00001019
Iteration 152/1000 | Loss: 0.00001019
Iteration 153/1000 | Loss: 0.00001019
Iteration 154/1000 | Loss: 0.00001019
Iteration 155/1000 | Loss: 0.00001019
Iteration 156/1000 | Loss: 0.00001019
Iteration 157/1000 | Loss: 0.00001019
Iteration 158/1000 | Loss: 0.00001019
Iteration 159/1000 | Loss: 0.00001019
Iteration 160/1000 | Loss: 0.00001019
Iteration 161/1000 | Loss: 0.00001019
Iteration 162/1000 | Loss: 0.00001019
Iteration 163/1000 | Loss: 0.00001019
Iteration 164/1000 | Loss: 0.00001019
Iteration 165/1000 | Loss: 0.00001019
Iteration 166/1000 | Loss: 0.00001019
Iteration 167/1000 | Loss: 0.00001019
Iteration 168/1000 | Loss: 0.00001019
Iteration 169/1000 | Loss: 0.00001019
Iteration 170/1000 | Loss: 0.00001019
Iteration 171/1000 | Loss: 0.00001019
Iteration 172/1000 | Loss: 0.00001019
Iteration 173/1000 | Loss: 0.00001019
Iteration 174/1000 | Loss: 0.00001019
Iteration 175/1000 | Loss: 0.00001019
Iteration 176/1000 | Loss: 0.00001019
Iteration 177/1000 | Loss: 0.00001019
Iteration 178/1000 | Loss: 0.00001019
Iteration 179/1000 | Loss: 0.00001019
Iteration 180/1000 | Loss: 0.00001019
Iteration 181/1000 | Loss: 0.00001019
Iteration 182/1000 | Loss: 0.00001019
Iteration 183/1000 | Loss: 0.00001019
Iteration 184/1000 | Loss: 0.00001019
Iteration 185/1000 | Loss: 0.00001019
Iteration 186/1000 | Loss: 0.00001019
Iteration 187/1000 | Loss: 0.00001019
Iteration 188/1000 | Loss: 0.00001019
Iteration 189/1000 | Loss: 0.00001019
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 189. Stopping optimization.
Last 5 losses: [1.0185613064095378e-05, 1.0185613064095378e-05, 1.0185613064095378e-05, 1.0185613064095378e-05, 1.0185613064095378e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0185613064095378e-05

Optimization complete. Final v2v error: 2.7161178588867188 mm

Highest mean error: 3.5118916034698486 mm for frame 66

Lowest mean error: 2.5781350135803223 mm for frame 104

Saving results

Total time: 37.750086069107056
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1052/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1052.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1052
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00509616
Iteration 2/25 | Loss: 0.00098056
Iteration 3/25 | Loss: 0.00084902
Iteration 4/25 | Loss: 0.00081461
Iteration 5/25 | Loss: 0.00080585
Iteration 6/25 | Loss: 0.00080367
Iteration 7/25 | Loss: 0.00080284
Iteration 8/25 | Loss: 0.00080283
Iteration 9/25 | Loss: 0.00080283
Iteration 10/25 | Loss: 0.00080283
Iteration 11/25 | Loss: 0.00080283
Iteration 12/25 | Loss: 0.00080283
Iteration 13/25 | Loss: 0.00080283
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008028314914554358, 0.0008028314914554358, 0.0008028314914554358, 0.0008028314914554358, 0.0008028314914554358]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008028314914554358

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.63236284
Iteration 2/25 | Loss: 0.00121217
Iteration 3/25 | Loss: 0.00121213
Iteration 4/25 | Loss: 0.00121213
Iteration 5/25 | Loss: 0.00121213
Iteration 6/25 | Loss: 0.00121213
Iteration 7/25 | Loss: 0.00121213
Iteration 8/25 | Loss: 0.00121213
Iteration 9/25 | Loss: 0.00121213
Iteration 10/25 | Loss: 0.00121213
Iteration 11/25 | Loss: 0.00121213
Iteration 12/25 | Loss: 0.00121213
Iteration 13/25 | Loss: 0.00121213
Iteration 14/25 | Loss: 0.00121213
Iteration 15/25 | Loss: 0.00121213
Iteration 16/25 | Loss: 0.00121213
Iteration 17/25 | Loss: 0.00121213
Iteration 18/25 | Loss: 0.00121213
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012121290201321244, 0.0012121290201321244, 0.0012121290201321244, 0.0012121290201321244, 0.0012121290201321244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012121290201321244

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121213
Iteration 2/1000 | Loss: 0.00004443
Iteration 3/1000 | Loss: 0.00003123
Iteration 4/1000 | Loss: 0.00002664
Iteration 5/1000 | Loss: 0.00002533
Iteration 6/1000 | Loss: 0.00002427
Iteration 7/1000 | Loss: 0.00002361
Iteration 8/1000 | Loss: 0.00002306
Iteration 9/1000 | Loss: 0.00002268
Iteration 10/1000 | Loss: 0.00002244
Iteration 11/1000 | Loss: 0.00002222
Iteration 12/1000 | Loss: 0.00002218
Iteration 13/1000 | Loss: 0.00002211
Iteration 14/1000 | Loss: 0.00002209
Iteration 15/1000 | Loss: 0.00002195
Iteration 16/1000 | Loss: 0.00002192
Iteration 17/1000 | Loss: 0.00002191
Iteration 18/1000 | Loss: 0.00002191
Iteration 19/1000 | Loss: 0.00002188
Iteration 20/1000 | Loss: 0.00002187
Iteration 21/1000 | Loss: 0.00002187
Iteration 22/1000 | Loss: 0.00002185
Iteration 23/1000 | Loss: 0.00002185
Iteration 24/1000 | Loss: 0.00002185
Iteration 25/1000 | Loss: 0.00002185
Iteration 26/1000 | Loss: 0.00002185
Iteration 27/1000 | Loss: 0.00002184
Iteration 28/1000 | Loss: 0.00002184
Iteration 29/1000 | Loss: 0.00002184
Iteration 30/1000 | Loss: 0.00002183
Iteration 31/1000 | Loss: 0.00002183
Iteration 32/1000 | Loss: 0.00002183
Iteration 33/1000 | Loss: 0.00002182
Iteration 34/1000 | Loss: 0.00002182
Iteration 35/1000 | Loss: 0.00002182
Iteration 36/1000 | Loss: 0.00002181
Iteration 37/1000 | Loss: 0.00002181
Iteration 38/1000 | Loss: 0.00002181
Iteration 39/1000 | Loss: 0.00002181
Iteration 40/1000 | Loss: 0.00002180
Iteration 41/1000 | Loss: 0.00002180
Iteration 42/1000 | Loss: 0.00002180
Iteration 43/1000 | Loss: 0.00002179
Iteration 44/1000 | Loss: 0.00002179
Iteration 45/1000 | Loss: 0.00002179
Iteration 46/1000 | Loss: 0.00002179
Iteration 47/1000 | Loss: 0.00002179
Iteration 48/1000 | Loss: 0.00002179
Iteration 49/1000 | Loss: 0.00002179
Iteration 50/1000 | Loss: 0.00002179
Iteration 51/1000 | Loss: 0.00002178
Iteration 52/1000 | Loss: 0.00002178
Iteration 53/1000 | Loss: 0.00002178
Iteration 54/1000 | Loss: 0.00002178
Iteration 55/1000 | Loss: 0.00002178
Iteration 56/1000 | Loss: 0.00002177
Iteration 57/1000 | Loss: 0.00002177
Iteration 58/1000 | Loss: 0.00002177
Iteration 59/1000 | Loss: 0.00002177
Iteration 60/1000 | Loss: 0.00002176
Iteration 61/1000 | Loss: 0.00002176
Iteration 62/1000 | Loss: 0.00002176
Iteration 63/1000 | Loss: 0.00002175
Iteration 64/1000 | Loss: 0.00002175
Iteration 65/1000 | Loss: 0.00002175
Iteration 66/1000 | Loss: 0.00002175
Iteration 67/1000 | Loss: 0.00002175
Iteration 68/1000 | Loss: 0.00002174
Iteration 69/1000 | Loss: 0.00002174
Iteration 70/1000 | Loss: 0.00002174
Iteration 71/1000 | Loss: 0.00002174
Iteration 72/1000 | Loss: 0.00002174
Iteration 73/1000 | Loss: 0.00002174
Iteration 74/1000 | Loss: 0.00002174
Iteration 75/1000 | Loss: 0.00002174
Iteration 76/1000 | Loss: 0.00002174
Iteration 77/1000 | Loss: 0.00002174
Iteration 78/1000 | Loss: 0.00002174
Iteration 79/1000 | Loss: 0.00002174
Iteration 80/1000 | Loss: 0.00002173
Iteration 81/1000 | Loss: 0.00002173
Iteration 82/1000 | Loss: 0.00002173
Iteration 83/1000 | Loss: 0.00002173
Iteration 84/1000 | Loss: 0.00002173
Iteration 85/1000 | Loss: 0.00002173
Iteration 86/1000 | Loss: 0.00002173
Iteration 87/1000 | Loss: 0.00002172
Iteration 88/1000 | Loss: 0.00002172
Iteration 89/1000 | Loss: 0.00002172
Iteration 90/1000 | Loss: 0.00002172
Iteration 91/1000 | Loss: 0.00002172
Iteration 92/1000 | Loss: 0.00002172
Iteration 93/1000 | Loss: 0.00002172
Iteration 94/1000 | Loss: 0.00002172
Iteration 95/1000 | Loss: 0.00002172
Iteration 96/1000 | Loss: 0.00002172
Iteration 97/1000 | Loss: 0.00002172
Iteration 98/1000 | Loss: 0.00002172
Iteration 99/1000 | Loss: 0.00002172
Iteration 100/1000 | Loss: 0.00002171
Iteration 101/1000 | Loss: 0.00002171
Iteration 102/1000 | Loss: 0.00002171
Iteration 103/1000 | Loss: 0.00002170
Iteration 104/1000 | Loss: 0.00002170
Iteration 105/1000 | Loss: 0.00002170
Iteration 106/1000 | Loss: 0.00002170
Iteration 107/1000 | Loss: 0.00002170
Iteration 108/1000 | Loss: 0.00002170
Iteration 109/1000 | Loss: 0.00002170
Iteration 110/1000 | Loss: 0.00002170
Iteration 111/1000 | Loss: 0.00002169
Iteration 112/1000 | Loss: 0.00002169
Iteration 113/1000 | Loss: 0.00002169
Iteration 114/1000 | Loss: 0.00002169
Iteration 115/1000 | Loss: 0.00002169
Iteration 116/1000 | Loss: 0.00002169
Iteration 117/1000 | Loss: 0.00002169
Iteration 118/1000 | Loss: 0.00002168
Iteration 119/1000 | Loss: 0.00002168
Iteration 120/1000 | Loss: 0.00002168
Iteration 121/1000 | Loss: 0.00002168
Iteration 122/1000 | Loss: 0.00002168
Iteration 123/1000 | Loss: 0.00002168
Iteration 124/1000 | Loss: 0.00002168
Iteration 125/1000 | Loss: 0.00002167
Iteration 126/1000 | Loss: 0.00002167
Iteration 127/1000 | Loss: 0.00002167
Iteration 128/1000 | Loss: 0.00002167
Iteration 129/1000 | Loss: 0.00002166
Iteration 130/1000 | Loss: 0.00002166
Iteration 131/1000 | Loss: 0.00002166
Iteration 132/1000 | Loss: 0.00002166
Iteration 133/1000 | Loss: 0.00002166
Iteration 134/1000 | Loss: 0.00002166
Iteration 135/1000 | Loss: 0.00002166
Iteration 136/1000 | Loss: 0.00002166
Iteration 137/1000 | Loss: 0.00002166
Iteration 138/1000 | Loss: 0.00002166
Iteration 139/1000 | Loss: 0.00002165
Iteration 140/1000 | Loss: 0.00002164
Iteration 141/1000 | Loss: 0.00002164
Iteration 142/1000 | Loss: 0.00002164
Iteration 143/1000 | Loss: 0.00002164
Iteration 144/1000 | Loss: 0.00002164
Iteration 145/1000 | Loss: 0.00002164
Iteration 146/1000 | Loss: 0.00002163
Iteration 147/1000 | Loss: 0.00002163
Iteration 148/1000 | Loss: 0.00002163
Iteration 149/1000 | Loss: 0.00002163
Iteration 150/1000 | Loss: 0.00002162
Iteration 151/1000 | Loss: 0.00002162
Iteration 152/1000 | Loss: 0.00002162
Iteration 153/1000 | Loss: 0.00002162
Iteration 154/1000 | Loss: 0.00002162
Iteration 155/1000 | Loss: 0.00002162
Iteration 156/1000 | Loss: 0.00002162
Iteration 157/1000 | Loss: 0.00002161
Iteration 158/1000 | Loss: 0.00002161
Iteration 159/1000 | Loss: 0.00002161
Iteration 160/1000 | Loss: 0.00002161
Iteration 161/1000 | Loss: 0.00002161
Iteration 162/1000 | Loss: 0.00002161
Iteration 163/1000 | Loss: 0.00002161
Iteration 164/1000 | Loss: 0.00002161
Iteration 165/1000 | Loss: 0.00002161
Iteration 166/1000 | Loss: 0.00002161
Iteration 167/1000 | Loss: 0.00002161
Iteration 168/1000 | Loss: 0.00002161
Iteration 169/1000 | Loss: 0.00002160
Iteration 170/1000 | Loss: 0.00002160
Iteration 171/1000 | Loss: 0.00002160
Iteration 172/1000 | Loss: 0.00002160
Iteration 173/1000 | Loss: 0.00002160
Iteration 174/1000 | Loss: 0.00002160
Iteration 175/1000 | Loss: 0.00002160
Iteration 176/1000 | Loss: 0.00002160
Iteration 177/1000 | Loss: 0.00002160
Iteration 178/1000 | Loss: 0.00002160
Iteration 179/1000 | Loss: 0.00002160
Iteration 180/1000 | Loss: 0.00002160
Iteration 181/1000 | Loss: 0.00002160
Iteration 182/1000 | Loss: 0.00002160
Iteration 183/1000 | Loss: 0.00002160
Iteration 184/1000 | Loss: 0.00002160
Iteration 185/1000 | Loss: 0.00002160
Iteration 186/1000 | Loss: 0.00002160
Iteration 187/1000 | Loss: 0.00002160
Iteration 188/1000 | Loss: 0.00002160
Iteration 189/1000 | Loss: 0.00002160
Iteration 190/1000 | Loss: 0.00002160
Iteration 191/1000 | Loss: 0.00002160
Iteration 192/1000 | Loss: 0.00002160
Iteration 193/1000 | Loss: 0.00002160
Iteration 194/1000 | Loss: 0.00002160
Iteration 195/1000 | Loss: 0.00002160
Iteration 196/1000 | Loss: 0.00002160
Iteration 197/1000 | Loss: 0.00002160
Iteration 198/1000 | Loss: 0.00002160
Iteration 199/1000 | Loss: 0.00002160
Iteration 200/1000 | Loss: 0.00002160
Iteration 201/1000 | Loss: 0.00002160
Iteration 202/1000 | Loss: 0.00002160
Iteration 203/1000 | Loss: 0.00002160
Iteration 204/1000 | Loss: 0.00002160
Iteration 205/1000 | Loss: 0.00002160
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 205. Stopping optimization.
Last 5 losses: [2.1601581465802155e-05, 2.1601581465802155e-05, 2.1601581465802155e-05, 2.1601581465802155e-05, 2.1601581465802155e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1601581465802155e-05

Optimization complete. Final v2v error: 3.9129109382629395 mm

Highest mean error: 4.546186923980713 mm for frame 2

Lowest mean error: 3.512507438659668 mm for frame 131

Saving results

Total time: 42.18475651741028
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1029/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1029.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1029
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00952035
Iteration 2/25 | Loss: 0.00173331
Iteration 3/25 | Loss: 0.00127036
Iteration 4/25 | Loss: 0.00114489
Iteration 5/25 | Loss: 0.00109523
Iteration 6/25 | Loss: 0.00107367
Iteration 7/25 | Loss: 0.00104218
Iteration 8/25 | Loss: 0.00099602
Iteration 9/25 | Loss: 0.00097543
Iteration 10/25 | Loss: 0.00095522
Iteration 11/25 | Loss: 0.00094705
Iteration 12/25 | Loss: 0.00093882
Iteration 13/25 | Loss: 0.00094030
Iteration 14/25 | Loss: 0.00093334
Iteration 15/25 | Loss: 0.00093118
Iteration 16/25 | Loss: 0.00093099
Iteration 17/25 | Loss: 0.00093095
Iteration 18/25 | Loss: 0.00093095
Iteration 19/25 | Loss: 0.00093094
Iteration 20/25 | Loss: 0.00093094
Iteration 21/25 | Loss: 0.00093094
Iteration 22/25 | Loss: 0.00093094
Iteration 23/25 | Loss: 0.00093094
Iteration 24/25 | Loss: 0.00093094
Iteration 25/25 | Loss: 0.00093094

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.39405656
Iteration 2/25 | Loss: 0.00127261
Iteration 3/25 | Loss: 0.00127258
Iteration 4/25 | Loss: 0.00127258
Iteration 5/25 | Loss: 0.00127257
Iteration 6/25 | Loss: 0.00127257
Iteration 7/25 | Loss: 0.00127257
Iteration 8/25 | Loss: 0.00127257
Iteration 9/25 | Loss: 0.00127257
Iteration 10/25 | Loss: 0.00127257
Iteration 11/25 | Loss: 0.00127257
Iteration 12/25 | Loss: 0.00127257
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012725730193778872, 0.0012725730193778872, 0.0012725730193778872, 0.0012725730193778872, 0.0012725730193778872]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012725730193778872

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127257
Iteration 2/1000 | Loss: 0.00069648
Iteration 3/1000 | Loss: 0.00073397
Iteration 4/1000 | Loss: 0.00045371
Iteration 5/1000 | Loss: 0.00027045
Iteration 6/1000 | Loss: 0.00016950
Iteration 7/1000 | Loss: 0.00015998
Iteration 8/1000 | Loss: 0.00012179
Iteration 9/1000 | Loss: 0.00034043
Iteration 10/1000 | Loss: 0.00024262
Iteration 11/1000 | Loss: 0.00020634
Iteration 12/1000 | Loss: 0.00037640
Iteration 13/1000 | Loss: 0.00019586
Iteration 14/1000 | Loss: 0.00022186
Iteration 15/1000 | Loss: 0.00021392
Iteration 16/1000 | Loss: 0.00030681
Iteration 17/1000 | Loss: 0.00025666
Iteration 18/1000 | Loss: 0.00014775
Iteration 19/1000 | Loss: 0.00013040
Iteration 20/1000 | Loss: 0.00015775
Iteration 21/1000 | Loss: 0.00014246
Iteration 22/1000 | Loss: 0.00010383
Iteration 23/1000 | Loss: 0.00015260
Iteration 24/1000 | Loss: 0.00009379
Iteration 25/1000 | Loss: 0.00013467
Iteration 26/1000 | Loss: 0.00008800
Iteration 27/1000 | Loss: 0.00008069
Iteration 28/1000 | Loss: 0.00007309
Iteration 29/1000 | Loss: 0.00006669
Iteration 30/1000 | Loss: 0.00005924
Iteration 31/1000 | Loss: 0.00005523
Iteration 32/1000 | Loss: 0.00005130
Iteration 33/1000 | Loss: 0.00004914
Iteration 34/1000 | Loss: 0.00005936
Iteration 35/1000 | Loss: 0.00005086
Iteration 36/1000 | Loss: 0.00006012
Iteration 37/1000 | Loss: 0.00005056
Iteration 38/1000 | Loss: 0.00004824
Iteration 39/1000 | Loss: 0.00004596
Iteration 40/1000 | Loss: 0.00008254
Iteration 41/1000 | Loss: 0.00007657
Iteration 42/1000 | Loss: 0.00004312
Iteration 43/1000 | Loss: 0.00007802
Iteration 44/1000 | Loss: 0.00005236
Iteration 45/1000 | Loss: 0.00006828
Iteration 46/1000 | Loss: 0.00004078
Iteration 47/1000 | Loss: 0.00004014
Iteration 48/1000 | Loss: 0.00007724
Iteration 49/1000 | Loss: 0.00026947
Iteration 50/1000 | Loss: 0.00023101
Iteration 51/1000 | Loss: 0.00009647
Iteration 52/1000 | Loss: 0.00006409
Iteration 53/1000 | Loss: 0.00008816
Iteration 54/1000 | Loss: 0.00005473
Iteration 55/1000 | Loss: 0.00004636
Iteration 56/1000 | Loss: 0.00004888
Iteration 57/1000 | Loss: 0.00005501
Iteration 58/1000 | Loss: 0.00004657
Iteration 59/1000 | Loss: 0.00004341
Iteration 60/1000 | Loss: 0.00004132
Iteration 61/1000 | Loss: 0.00008812
Iteration 62/1000 | Loss: 0.00024945
Iteration 63/1000 | Loss: 0.00020912
Iteration 64/1000 | Loss: 0.00008743
Iteration 65/1000 | Loss: 0.00007304
Iteration 66/1000 | Loss: 0.00007906
Iteration 67/1000 | Loss: 0.00007220
Iteration 68/1000 | Loss: 0.00008183
Iteration 69/1000 | Loss: 0.00007061
Iteration 70/1000 | Loss: 0.00009919
Iteration 71/1000 | Loss: 0.00009936
Iteration 72/1000 | Loss: 0.00009903
Iteration 73/1000 | Loss: 0.00008409
Iteration 74/1000 | Loss: 0.00008780
Iteration 75/1000 | Loss: 0.00011587
Iteration 76/1000 | Loss: 0.00007250
Iteration 77/1000 | Loss: 0.00005683
Iteration 78/1000 | Loss: 0.00004924
Iteration 79/1000 | Loss: 0.00004712
Iteration 80/1000 | Loss: 0.00008696
Iteration 81/1000 | Loss: 0.00011550
Iteration 82/1000 | Loss: 0.00013579
Iteration 83/1000 | Loss: 0.00007054
Iteration 84/1000 | Loss: 0.00005101
Iteration 85/1000 | Loss: 0.00005669
Iteration 86/1000 | Loss: 0.00005336
Iteration 87/1000 | Loss: 0.00012915
Iteration 88/1000 | Loss: 0.00011273
Iteration 89/1000 | Loss: 0.00005227
Iteration 90/1000 | Loss: 0.00004871
Iteration 91/1000 | Loss: 0.00008243
Iteration 92/1000 | Loss: 0.00007730
Iteration 93/1000 | Loss: 0.00007123
Iteration 94/1000 | Loss: 0.00005092
Iteration 95/1000 | Loss: 0.00005084
Iteration 96/1000 | Loss: 0.00004473
Iteration 97/1000 | Loss: 0.00004379
Iteration 98/1000 | Loss: 0.00008546
Iteration 99/1000 | Loss: 0.00014869
Iteration 100/1000 | Loss: 0.00011789
Iteration 101/1000 | Loss: 0.00006673
Iteration 102/1000 | Loss: 0.00008248
Iteration 103/1000 | Loss: 0.00011655
Iteration 104/1000 | Loss: 0.00009598
Iteration 105/1000 | Loss: 0.00004834
Iteration 106/1000 | Loss: 0.00004625
Iteration 107/1000 | Loss: 0.00004148
Iteration 108/1000 | Loss: 0.00009146
Iteration 109/1000 | Loss: 0.00008734
Iteration 110/1000 | Loss: 0.00007288
Iteration 111/1000 | Loss: 0.00007196
Iteration 112/1000 | Loss: 0.00007156
Iteration 113/1000 | Loss: 0.00006915
Iteration 114/1000 | Loss: 0.00006911
Iteration 115/1000 | Loss: 0.00009395
Iteration 116/1000 | Loss: 0.00008643
Iteration 117/1000 | Loss: 0.00007338
Iteration 118/1000 | Loss: 0.00005539
Iteration 119/1000 | Loss: 0.00005017
Iteration 120/1000 | Loss: 0.00005106
Iteration 121/1000 | Loss: 0.00004282
Iteration 122/1000 | Loss: 0.00004105
Iteration 123/1000 | Loss: 0.00004450
Iteration 124/1000 | Loss: 0.00004032
Iteration 125/1000 | Loss: 0.00003940
Iteration 126/1000 | Loss: 0.00003892
Iteration 127/1000 | Loss: 0.00015493
Iteration 128/1000 | Loss: 0.00012230
Iteration 129/1000 | Loss: 0.00004584
Iteration 130/1000 | Loss: 0.00003860
Iteration 131/1000 | Loss: 0.00003790
Iteration 132/1000 | Loss: 0.00015941
Iteration 133/1000 | Loss: 0.00020452
Iteration 134/1000 | Loss: 0.00008749
Iteration 135/1000 | Loss: 0.00010015
Iteration 136/1000 | Loss: 0.00011086
Iteration 137/1000 | Loss: 0.00009343
Iteration 138/1000 | Loss: 0.00007963
Iteration 139/1000 | Loss: 0.00011047
Iteration 140/1000 | Loss: 0.00009202
Iteration 141/1000 | Loss: 0.00010923
Iteration 142/1000 | Loss: 0.00009688
Iteration 143/1000 | Loss: 0.00009688
Iteration 144/1000 | Loss: 0.00007201
Iteration 145/1000 | Loss: 0.00004802
Iteration 146/1000 | Loss: 0.00005676
Iteration 147/1000 | Loss: 0.00005572
Iteration 148/1000 | Loss: 0.00006404
Iteration 149/1000 | Loss: 0.00005253
Iteration 150/1000 | Loss: 0.00005074
Iteration 151/1000 | Loss: 0.00006130
Iteration 152/1000 | Loss: 0.00005053
Iteration 153/1000 | Loss: 0.00004981
Iteration 154/1000 | Loss: 0.00004297
Iteration 155/1000 | Loss: 0.00003840
Iteration 156/1000 | Loss: 0.00006696
Iteration 157/1000 | Loss: 0.00007805
Iteration 158/1000 | Loss: 0.00006272
Iteration 159/1000 | Loss: 0.00004721
Iteration 160/1000 | Loss: 0.00004277
Iteration 161/1000 | Loss: 0.00006102
Iteration 162/1000 | Loss: 0.00004294
Iteration 163/1000 | Loss: 0.00006865
Iteration 164/1000 | Loss: 0.00005443
Iteration 165/1000 | Loss: 0.00005251
Iteration 166/1000 | Loss: 0.00005224
Iteration 167/1000 | Loss: 0.00006597
Iteration 168/1000 | Loss: 0.00006224
Iteration 169/1000 | Loss: 0.00005334
Iteration 170/1000 | Loss: 0.00004474
Iteration 171/1000 | Loss: 0.00004642
Iteration 172/1000 | Loss: 0.00004244
Iteration 173/1000 | Loss: 0.00006199
Iteration 174/1000 | Loss: 0.00005558
Iteration 175/1000 | Loss: 0.00005309
Iteration 176/1000 | Loss: 0.00005833
Iteration 177/1000 | Loss: 0.00005234
Iteration 178/1000 | Loss: 0.00005133
Iteration 179/1000 | Loss: 0.00003960
Iteration 180/1000 | Loss: 0.00004493
Iteration 181/1000 | Loss: 0.00004707
Iteration 182/1000 | Loss: 0.00004547
Iteration 183/1000 | Loss: 0.00004978
Iteration 184/1000 | Loss: 0.00005934
Iteration 185/1000 | Loss: 0.00006784
Iteration 186/1000 | Loss: 0.00006106
Iteration 187/1000 | Loss: 0.00005256
Iteration 188/1000 | Loss: 0.00005255
Iteration 189/1000 | Loss: 0.00005042
Iteration 190/1000 | Loss: 0.00006753
Iteration 191/1000 | Loss: 0.00006844
Iteration 192/1000 | Loss: 0.00006441
Iteration 193/1000 | Loss: 0.00005108
Iteration 194/1000 | Loss: 0.00005237
Iteration 195/1000 | Loss: 0.00005426
Iteration 196/1000 | Loss: 0.00005727
Iteration 197/1000 | Loss: 0.00005129
Iteration 198/1000 | Loss: 0.00005907
Iteration 199/1000 | Loss: 0.00005733
Iteration 200/1000 | Loss: 0.00005342
Iteration 201/1000 | Loss: 0.00005723
Iteration 202/1000 | Loss: 0.00005303
Iteration 203/1000 | Loss: 0.00006317
Iteration 204/1000 | Loss: 0.00007183
Iteration 205/1000 | Loss: 0.00005203
Iteration 206/1000 | Loss: 0.00005113
Iteration 207/1000 | Loss: 0.00004355
Iteration 208/1000 | Loss: 0.00005757
Iteration 209/1000 | Loss: 0.00006943
Iteration 210/1000 | Loss: 0.00006135
Iteration 211/1000 | Loss: 0.00006679
Iteration 212/1000 | Loss: 0.00006986
Iteration 213/1000 | Loss: 0.00005519
Iteration 214/1000 | Loss: 0.00004939
Iteration 215/1000 | Loss: 0.00005056
Iteration 216/1000 | Loss: 0.00004062
Iteration 217/1000 | Loss: 0.00004937
Iteration 218/1000 | Loss: 0.00004334
Iteration 219/1000 | Loss: 0.00004528
Iteration 220/1000 | Loss: 0.00005023
Iteration 221/1000 | Loss: 0.00004963
Iteration 222/1000 | Loss: 0.00003794
Iteration 223/1000 | Loss: 0.00004833
Iteration 224/1000 | Loss: 0.00005626
Iteration 225/1000 | Loss: 0.00004668
Iteration 226/1000 | Loss: 0.00004587
Iteration 227/1000 | Loss: 0.00004663
Iteration 228/1000 | Loss: 0.00004849
Iteration 229/1000 | Loss: 0.00004289
Iteration 230/1000 | Loss: 0.00004188
Iteration 231/1000 | Loss: 0.00005307
Iteration 232/1000 | Loss: 0.00005941
Iteration 233/1000 | Loss: 0.00004567
Iteration 234/1000 | Loss: 0.00004035
Iteration 235/1000 | Loss: 0.00004198
Iteration 236/1000 | Loss: 0.00004523
Iteration 237/1000 | Loss: 0.00004309
Iteration 238/1000 | Loss: 0.00004382
Iteration 239/1000 | Loss: 0.00004089
Iteration 240/1000 | Loss: 0.00003995
Iteration 241/1000 | Loss: 0.00003794
Iteration 242/1000 | Loss: 0.00004463
Iteration 243/1000 | Loss: 0.00005810
Iteration 244/1000 | Loss: 0.00004213
Iteration 245/1000 | Loss: 0.00004103
Iteration 246/1000 | Loss: 0.00003855
Iteration 247/1000 | Loss: 0.00003985
Iteration 248/1000 | Loss: 0.00005536
Iteration 249/1000 | Loss: 0.00004116
Iteration 250/1000 | Loss: 0.00003745
Iteration 251/1000 | Loss: 0.00003783
Iteration 252/1000 | Loss: 0.00004457
Iteration 253/1000 | Loss: 0.00004139
Iteration 254/1000 | Loss: 0.00003581
Iteration 255/1000 | Loss: 0.00004564
Iteration 256/1000 | Loss: 0.00004539
Iteration 257/1000 | Loss: 0.00004440
Iteration 258/1000 | Loss: 0.00004530
Iteration 259/1000 | Loss: 0.00005402
Iteration 260/1000 | Loss: 0.00004894
Iteration 261/1000 | Loss: 0.00004154
Iteration 262/1000 | Loss: 0.00003857
Iteration 263/1000 | Loss: 0.00005224
Iteration 264/1000 | Loss: 0.00006612
Iteration 265/1000 | Loss: 0.00004528
Iteration 266/1000 | Loss: 0.00004980
Iteration 267/1000 | Loss: 0.00004208
Iteration 268/1000 | Loss: 0.00003522
Iteration 269/1000 | Loss: 0.00004157
Iteration 270/1000 | Loss: 0.00005383
Iteration 271/1000 | Loss: 0.00004677
Iteration 272/1000 | Loss: 0.00003510
Iteration 273/1000 | Loss: 0.00004129
Iteration 274/1000 | Loss: 0.00004880
Iteration 275/1000 | Loss: 0.00005502
Iteration 276/1000 | Loss: 0.00003492
Iteration 277/1000 | Loss: 0.00004474
Iteration 278/1000 | Loss: 0.00005096
Iteration 279/1000 | Loss: 0.00004735
Iteration 280/1000 | Loss: 0.00004419
Iteration 281/1000 | Loss: 0.00005487
Iteration 282/1000 | Loss: 0.00004628
Iteration 283/1000 | Loss: 0.00004251
Iteration 284/1000 | Loss: 0.00004774
Iteration 285/1000 | Loss: 0.00006825
Iteration 286/1000 | Loss: 0.00004936
Iteration 287/1000 | Loss: 0.00005761
Iteration 288/1000 | Loss: 0.00004738
Iteration 289/1000 | Loss: 0.00003829
Iteration 290/1000 | Loss: 0.00005944
Iteration 291/1000 | Loss: 0.00006125
Iteration 292/1000 | Loss: 0.00004570
Iteration 293/1000 | Loss: 0.00004742
Iteration 294/1000 | Loss: 0.00006337
Iteration 295/1000 | Loss: 0.00004919
Iteration 296/1000 | Loss: 0.00005822
Iteration 297/1000 | Loss: 0.00006710
Iteration 298/1000 | Loss: 0.00004863
Iteration 299/1000 | Loss: 0.00004527
Iteration 300/1000 | Loss: 0.00004291
Iteration 301/1000 | Loss: 0.00004452
Iteration 302/1000 | Loss: 0.00004987
Iteration 303/1000 | Loss: 0.00004202
Iteration 304/1000 | Loss: 0.00003987
Iteration 305/1000 | Loss: 0.00005341
Iteration 306/1000 | Loss: 0.00005283
Iteration 307/1000 | Loss: 0.00004935
Iteration 308/1000 | Loss: 0.00005358
Iteration 309/1000 | Loss: 0.00005118
Iteration 310/1000 | Loss: 0.00004900
Iteration 311/1000 | Loss: 0.00004694
Iteration 312/1000 | Loss: 0.00004208
Iteration 313/1000 | Loss: 0.00004280
Iteration 314/1000 | Loss: 0.00003792
Iteration 315/1000 | Loss: 0.00004517
Iteration 316/1000 | Loss: 0.00003488
Iteration 317/1000 | Loss: 0.00003488
Iteration 318/1000 | Loss: 0.00003461
Iteration 319/1000 | Loss: 0.00003430
Iteration 320/1000 | Loss: 0.00003414
Iteration 321/1000 | Loss: 0.00003410
Iteration 322/1000 | Loss: 0.00003409
Iteration 323/1000 | Loss: 0.00003407
Iteration 324/1000 | Loss: 0.00003406
Iteration 325/1000 | Loss: 0.00003406
Iteration 326/1000 | Loss: 0.00003402
Iteration 327/1000 | Loss: 0.00003377
Iteration 328/1000 | Loss: 0.00003356
Iteration 329/1000 | Loss: 0.00004739
Iteration 330/1000 | Loss: 0.00003711
Iteration 331/1000 | Loss: 0.00004625
Iteration 332/1000 | Loss: 0.00003685
Iteration 333/1000 | Loss: 0.00004661
Iteration 334/1000 | Loss: 0.00003674
Iteration 335/1000 | Loss: 0.00004632
Iteration 336/1000 | Loss: 0.00004942
Iteration 337/1000 | Loss: 0.00003816
Iteration 338/1000 | Loss: 0.00003602
Iteration 339/1000 | Loss: 0.00003505
Iteration 340/1000 | Loss: 0.00003451
Iteration 341/1000 | Loss: 0.00003583
Iteration 342/1000 | Loss: 0.00004273
Iteration 343/1000 | Loss: 0.00004036
Iteration 344/1000 | Loss: 0.00004148
Iteration 345/1000 | Loss: 0.00004143
Iteration 346/1000 | Loss: 0.00003917
Iteration 347/1000 | Loss: 0.00003754
Iteration 348/1000 | Loss: 0.00004141
Iteration 349/1000 | Loss: 0.00004140
Iteration 350/1000 | Loss: 0.00004128
Iteration 351/1000 | Loss: 0.00003976
Iteration 352/1000 | Loss: 0.00004043
Iteration 353/1000 | Loss: 0.00003812
Iteration 354/1000 | Loss: 0.00004000
Iteration 355/1000 | Loss: 0.00003544
Iteration 356/1000 | Loss: 0.00003432
Iteration 357/1000 | Loss: 0.00003308
Iteration 358/1000 | Loss: 0.00003276
Iteration 359/1000 | Loss: 0.00003275
Iteration 360/1000 | Loss: 0.00003274
Iteration 361/1000 | Loss: 0.00003273
Iteration 362/1000 | Loss: 0.00003270
Iteration 363/1000 | Loss: 0.00003269
Iteration 364/1000 | Loss: 0.00003269
Iteration 365/1000 | Loss: 0.00003269
Iteration 366/1000 | Loss: 0.00003269
Iteration 367/1000 | Loss: 0.00003269
Iteration 368/1000 | Loss: 0.00003268
Iteration 369/1000 | Loss: 0.00003268
Iteration 370/1000 | Loss: 0.00003268
Iteration 371/1000 | Loss: 0.00003268
Iteration 372/1000 | Loss: 0.00003267
Iteration 373/1000 | Loss: 0.00003267
Iteration 374/1000 | Loss: 0.00003267
Iteration 375/1000 | Loss: 0.00003266
Iteration 376/1000 | Loss: 0.00003266
Iteration 377/1000 | Loss: 0.00003266
Iteration 378/1000 | Loss: 0.00003266
Iteration 379/1000 | Loss: 0.00003266
Iteration 380/1000 | Loss: 0.00003265
Iteration 381/1000 | Loss: 0.00003265
Iteration 382/1000 | Loss: 0.00003265
Iteration 383/1000 | Loss: 0.00003265
Iteration 384/1000 | Loss: 0.00003265
Iteration 385/1000 | Loss: 0.00003265
Iteration 386/1000 | Loss: 0.00003264
Iteration 387/1000 | Loss: 0.00003264
Iteration 388/1000 | Loss: 0.00003264
Iteration 389/1000 | Loss: 0.00003264
Iteration 390/1000 | Loss: 0.00003263
Iteration 391/1000 | Loss: 0.00003263
Iteration 392/1000 | Loss: 0.00003263
Iteration 393/1000 | Loss: 0.00003263
Iteration 394/1000 | Loss: 0.00003263
Iteration 395/1000 | Loss: 0.00003263
Iteration 396/1000 | Loss: 0.00003263
Iteration 397/1000 | Loss: 0.00003263
Iteration 398/1000 | Loss: 0.00003262
Iteration 399/1000 | Loss: 0.00003262
Iteration 400/1000 | Loss: 0.00003262
Iteration 401/1000 | Loss: 0.00003262
Iteration 402/1000 | Loss: 0.00003262
Iteration 403/1000 | Loss: 0.00003262
Iteration 404/1000 | Loss: 0.00003262
Iteration 405/1000 | Loss: 0.00003262
Iteration 406/1000 | Loss: 0.00003262
Iteration 407/1000 | Loss: 0.00003262
Iteration 408/1000 | Loss: 0.00003262
Iteration 409/1000 | Loss: 0.00003262
Iteration 410/1000 | Loss: 0.00003262
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 410. Stopping optimization.
Last 5 losses: [3.2624127925373614e-05, 3.2624127925373614e-05, 3.2624127925373614e-05, 3.2624127925373614e-05, 3.2624127925373614e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.2624127925373614e-05

Optimization complete. Final v2v error: 4.435717582702637 mm

Highest mean error: 9.157198905944824 mm for frame 115

Lowest mean error: 3.2550435066223145 mm for frame 75

Saving results

Total time: 557.0101706981659
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01016397
Iteration 2/25 | Loss: 0.00164916
Iteration 3/25 | Loss: 0.00107607
Iteration 4/25 | Loss: 0.00101970
Iteration 5/25 | Loss: 0.00100219
Iteration 6/25 | Loss: 0.00099945
Iteration 7/25 | Loss: 0.00099927
Iteration 8/25 | Loss: 0.00099927
Iteration 9/25 | Loss: 0.00099927
Iteration 10/25 | Loss: 0.00099927
Iteration 11/25 | Loss: 0.00099927
Iteration 12/25 | Loss: 0.00099927
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009992672130465508, 0.0009992672130465508, 0.0009992672130465508, 0.0009992672130465508, 0.0009992672130465508]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009992672130465508

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.87655354
Iteration 2/25 | Loss: 0.00079081
Iteration 3/25 | Loss: 0.00079080
Iteration 4/25 | Loss: 0.00079080
Iteration 5/25 | Loss: 0.00079080
Iteration 6/25 | Loss: 0.00079080
Iteration 7/25 | Loss: 0.00079080
Iteration 8/25 | Loss: 0.00079080
Iteration 9/25 | Loss: 0.00079080
Iteration 10/25 | Loss: 0.00079080
Iteration 11/25 | Loss: 0.00079080
Iteration 12/25 | Loss: 0.00079080
Iteration 13/25 | Loss: 0.00079080
Iteration 14/25 | Loss: 0.00079080
Iteration 15/25 | Loss: 0.00079080
Iteration 16/25 | Loss: 0.00079080
Iteration 17/25 | Loss: 0.00079080
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007907982799224555, 0.0007907982799224555, 0.0007907982799224555, 0.0007907982799224555, 0.0007907982799224555]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007907982799224555

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00079080
Iteration 2/1000 | Loss: 0.00007413
Iteration 3/1000 | Loss: 0.00004710
Iteration 4/1000 | Loss: 0.00004276
Iteration 5/1000 | Loss: 0.00004058
Iteration 6/1000 | Loss: 0.00003952
Iteration 7/1000 | Loss: 0.00003885
Iteration 8/1000 | Loss: 0.00003810
Iteration 9/1000 | Loss: 0.00003761
Iteration 10/1000 | Loss: 0.00003725
Iteration 11/1000 | Loss: 0.00003692
Iteration 12/1000 | Loss: 0.00003668
Iteration 13/1000 | Loss: 0.00003640
Iteration 14/1000 | Loss: 0.00003620
Iteration 15/1000 | Loss: 0.00003615
Iteration 16/1000 | Loss: 0.00003604
Iteration 17/1000 | Loss: 0.00003604
Iteration 18/1000 | Loss: 0.00003589
Iteration 19/1000 | Loss: 0.00003578
Iteration 20/1000 | Loss: 0.00003577
Iteration 21/1000 | Loss: 0.00003572
Iteration 22/1000 | Loss: 0.00003571
Iteration 23/1000 | Loss: 0.00003568
Iteration 24/1000 | Loss: 0.00003564
Iteration 25/1000 | Loss: 0.00003560
Iteration 26/1000 | Loss: 0.00003559
Iteration 27/1000 | Loss: 0.00003558
Iteration 28/1000 | Loss: 0.00003557
Iteration 29/1000 | Loss: 0.00003550
Iteration 30/1000 | Loss: 0.00003550
Iteration 31/1000 | Loss: 0.00003548
Iteration 32/1000 | Loss: 0.00003548
Iteration 33/1000 | Loss: 0.00003547
Iteration 34/1000 | Loss: 0.00003547
Iteration 35/1000 | Loss: 0.00003543
Iteration 36/1000 | Loss: 0.00003539
Iteration 37/1000 | Loss: 0.00003539
Iteration 38/1000 | Loss: 0.00003539
Iteration 39/1000 | Loss: 0.00003538
Iteration 40/1000 | Loss: 0.00003537
Iteration 41/1000 | Loss: 0.00003533
Iteration 42/1000 | Loss: 0.00003531
Iteration 43/1000 | Loss: 0.00003531
Iteration 44/1000 | Loss: 0.00003530
Iteration 45/1000 | Loss: 0.00003529
Iteration 46/1000 | Loss: 0.00003528
Iteration 47/1000 | Loss: 0.00003528
Iteration 48/1000 | Loss: 0.00003528
Iteration 49/1000 | Loss: 0.00003528
Iteration 50/1000 | Loss: 0.00003528
Iteration 51/1000 | Loss: 0.00003528
Iteration 52/1000 | Loss: 0.00003528
Iteration 53/1000 | Loss: 0.00003528
Iteration 54/1000 | Loss: 0.00003527
Iteration 55/1000 | Loss: 0.00003527
Iteration 56/1000 | Loss: 0.00003527
Iteration 57/1000 | Loss: 0.00003527
Iteration 58/1000 | Loss: 0.00003527
Iteration 59/1000 | Loss: 0.00003527
Iteration 60/1000 | Loss: 0.00003527
Iteration 61/1000 | Loss: 0.00003527
Iteration 62/1000 | Loss: 0.00003527
Iteration 63/1000 | Loss: 0.00003527
Iteration 64/1000 | Loss: 0.00003527
Iteration 65/1000 | Loss: 0.00003526
Iteration 66/1000 | Loss: 0.00003526
Iteration 67/1000 | Loss: 0.00003526
Iteration 68/1000 | Loss: 0.00003525
Iteration 69/1000 | Loss: 0.00003525
Iteration 70/1000 | Loss: 0.00003524
Iteration 71/1000 | Loss: 0.00003524
Iteration 72/1000 | Loss: 0.00003524
Iteration 73/1000 | Loss: 0.00003524
Iteration 74/1000 | Loss: 0.00003523
Iteration 75/1000 | Loss: 0.00003523
Iteration 76/1000 | Loss: 0.00003523
Iteration 77/1000 | Loss: 0.00003523
Iteration 78/1000 | Loss: 0.00003523
Iteration 79/1000 | Loss: 0.00003523
Iteration 80/1000 | Loss: 0.00003523
Iteration 81/1000 | Loss: 0.00003523
Iteration 82/1000 | Loss: 0.00003523
Iteration 83/1000 | Loss: 0.00003522
Iteration 84/1000 | Loss: 0.00003522
Iteration 85/1000 | Loss: 0.00003522
Iteration 86/1000 | Loss: 0.00003522
Iteration 87/1000 | Loss: 0.00003522
Iteration 88/1000 | Loss: 0.00003521
Iteration 89/1000 | Loss: 0.00003521
Iteration 90/1000 | Loss: 0.00003521
Iteration 91/1000 | Loss: 0.00003521
Iteration 92/1000 | Loss: 0.00003520
Iteration 93/1000 | Loss: 0.00003520
Iteration 94/1000 | Loss: 0.00003520
Iteration 95/1000 | Loss: 0.00003520
Iteration 96/1000 | Loss: 0.00003520
Iteration 97/1000 | Loss: 0.00003520
Iteration 98/1000 | Loss: 0.00003519
Iteration 99/1000 | Loss: 0.00003519
Iteration 100/1000 | Loss: 0.00003519
Iteration 101/1000 | Loss: 0.00003518
Iteration 102/1000 | Loss: 0.00003518
Iteration 103/1000 | Loss: 0.00003518
Iteration 104/1000 | Loss: 0.00003518
Iteration 105/1000 | Loss: 0.00003517
Iteration 106/1000 | Loss: 0.00003517
Iteration 107/1000 | Loss: 0.00003517
Iteration 108/1000 | Loss: 0.00003517
Iteration 109/1000 | Loss: 0.00003517
Iteration 110/1000 | Loss: 0.00003516
Iteration 111/1000 | Loss: 0.00003516
Iteration 112/1000 | Loss: 0.00003516
Iteration 113/1000 | Loss: 0.00003516
Iteration 114/1000 | Loss: 0.00003516
Iteration 115/1000 | Loss: 0.00003516
Iteration 116/1000 | Loss: 0.00003516
Iteration 117/1000 | Loss: 0.00003516
Iteration 118/1000 | Loss: 0.00003516
Iteration 119/1000 | Loss: 0.00003516
Iteration 120/1000 | Loss: 0.00003516
Iteration 121/1000 | Loss: 0.00003516
Iteration 122/1000 | Loss: 0.00003516
Iteration 123/1000 | Loss: 0.00003516
Iteration 124/1000 | Loss: 0.00003515
Iteration 125/1000 | Loss: 0.00003515
Iteration 126/1000 | Loss: 0.00003515
Iteration 127/1000 | Loss: 0.00003515
Iteration 128/1000 | Loss: 0.00003515
Iteration 129/1000 | Loss: 0.00003515
Iteration 130/1000 | Loss: 0.00003515
Iteration 131/1000 | Loss: 0.00003515
Iteration 132/1000 | Loss: 0.00003514
Iteration 133/1000 | Loss: 0.00003514
Iteration 134/1000 | Loss: 0.00003514
Iteration 135/1000 | Loss: 0.00003514
Iteration 136/1000 | Loss: 0.00003514
Iteration 137/1000 | Loss: 0.00003514
Iteration 138/1000 | Loss: 0.00003514
Iteration 139/1000 | Loss: 0.00003514
Iteration 140/1000 | Loss: 0.00003513
Iteration 141/1000 | Loss: 0.00003513
Iteration 142/1000 | Loss: 0.00003513
Iteration 143/1000 | Loss: 0.00003513
Iteration 144/1000 | Loss: 0.00003513
Iteration 145/1000 | Loss: 0.00003512
Iteration 146/1000 | Loss: 0.00003512
Iteration 147/1000 | Loss: 0.00003512
Iteration 148/1000 | Loss: 0.00003512
Iteration 149/1000 | Loss: 0.00003512
Iteration 150/1000 | Loss: 0.00003512
Iteration 151/1000 | Loss: 0.00003512
Iteration 152/1000 | Loss: 0.00003512
Iteration 153/1000 | Loss: 0.00003512
Iteration 154/1000 | Loss: 0.00003512
Iteration 155/1000 | Loss: 0.00003512
Iteration 156/1000 | Loss: 0.00003512
Iteration 157/1000 | Loss: 0.00003511
Iteration 158/1000 | Loss: 0.00003511
Iteration 159/1000 | Loss: 0.00003511
Iteration 160/1000 | Loss: 0.00003511
Iteration 161/1000 | Loss: 0.00003511
Iteration 162/1000 | Loss: 0.00003511
Iteration 163/1000 | Loss: 0.00003511
Iteration 164/1000 | Loss: 0.00003511
Iteration 165/1000 | Loss: 0.00003511
Iteration 166/1000 | Loss: 0.00003511
Iteration 167/1000 | Loss: 0.00003511
Iteration 168/1000 | Loss: 0.00003511
Iteration 169/1000 | Loss: 0.00003511
Iteration 170/1000 | Loss: 0.00003511
Iteration 171/1000 | Loss: 0.00003511
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [3.511218892526813e-05, 3.511218892526813e-05, 3.511218892526813e-05, 3.511218892526813e-05, 3.511218892526813e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.511218892526813e-05

Optimization complete. Final v2v error: 4.76630163192749 mm

Highest mean error: 5.80076265335083 mm for frame 106

Lowest mean error: 3.8902838230133057 mm for frame 32

Saving results

Total time: 52.123241901397705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1033/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1033.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1033
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840831
Iteration 2/25 | Loss: 0.00113111
Iteration 3/25 | Loss: 0.00090970
Iteration 4/25 | Loss: 0.00086310
Iteration 5/25 | Loss: 0.00084873
Iteration 6/25 | Loss: 0.00084553
Iteration 7/25 | Loss: 0.00084447
Iteration 8/25 | Loss: 0.00084419
Iteration 9/25 | Loss: 0.00084419
Iteration 10/25 | Loss: 0.00084419
Iteration 11/25 | Loss: 0.00084419
Iteration 12/25 | Loss: 0.00084419
Iteration 13/25 | Loss: 0.00084419
Iteration 14/25 | Loss: 0.00084419
Iteration 15/25 | Loss: 0.00084419
Iteration 16/25 | Loss: 0.00084419
Iteration 17/25 | Loss: 0.00084419
Iteration 18/25 | Loss: 0.00084419
Iteration 19/25 | Loss: 0.00084419
Iteration 20/25 | Loss: 0.00084419
Iteration 21/25 | Loss: 0.00084419
Iteration 22/25 | Loss: 0.00084419
Iteration 23/25 | Loss: 0.00084419
Iteration 24/25 | Loss: 0.00084419
Iteration 25/25 | Loss: 0.00084419

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.74022007
Iteration 2/25 | Loss: 0.00178857
Iteration 3/25 | Loss: 0.00178855
Iteration 4/25 | Loss: 0.00178855
Iteration 5/25 | Loss: 0.00178855
Iteration 6/25 | Loss: 0.00178855
Iteration 7/25 | Loss: 0.00178855
Iteration 8/25 | Loss: 0.00178855
Iteration 9/25 | Loss: 0.00178855
Iteration 10/25 | Loss: 0.00178855
Iteration 11/25 | Loss: 0.00178855
Iteration 12/25 | Loss: 0.00178855
Iteration 13/25 | Loss: 0.00178855
Iteration 14/25 | Loss: 0.00178855
Iteration 15/25 | Loss: 0.00178855
Iteration 16/25 | Loss: 0.00178855
Iteration 17/25 | Loss: 0.00178855
Iteration 18/25 | Loss: 0.00178855
Iteration 19/25 | Loss: 0.00178855
Iteration 20/25 | Loss: 0.00178855
Iteration 21/25 | Loss: 0.00178855
Iteration 22/25 | Loss: 0.00178855
Iteration 23/25 | Loss: 0.00178855
Iteration 24/25 | Loss: 0.00178855
Iteration 25/25 | Loss: 0.00178855

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00178855
Iteration 2/1000 | Loss: 0.00005166
Iteration 3/1000 | Loss: 0.00003391
Iteration 4/1000 | Loss: 0.00002515
Iteration 5/1000 | Loss: 0.00002235
Iteration 6/1000 | Loss: 0.00002111
Iteration 7/1000 | Loss: 0.00002015
Iteration 8/1000 | Loss: 0.00001966
Iteration 9/1000 | Loss: 0.00001925
Iteration 10/1000 | Loss: 0.00001891
Iteration 11/1000 | Loss: 0.00001876
Iteration 12/1000 | Loss: 0.00001858
Iteration 13/1000 | Loss: 0.00001842
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001821
Iteration 16/1000 | Loss: 0.00001817
Iteration 17/1000 | Loss: 0.00001813
Iteration 18/1000 | Loss: 0.00001810
Iteration 19/1000 | Loss: 0.00001809
Iteration 20/1000 | Loss: 0.00001808
Iteration 21/1000 | Loss: 0.00001806
Iteration 22/1000 | Loss: 0.00001805
Iteration 23/1000 | Loss: 0.00001804
Iteration 24/1000 | Loss: 0.00001803
Iteration 25/1000 | Loss: 0.00001803
Iteration 26/1000 | Loss: 0.00001802
Iteration 27/1000 | Loss: 0.00001801
Iteration 28/1000 | Loss: 0.00001800
Iteration 29/1000 | Loss: 0.00001799
Iteration 30/1000 | Loss: 0.00001798
Iteration 31/1000 | Loss: 0.00001798
Iteration 32/1000 | Loss: 0.00001797
Iteration 33/1000 | Loss: 0.00001796
Iteration 34/1000 | Loss: 0.00001796
Iteration 35/1000 | Loss: 0.00001795
Iteration 36/1000 | Loss: 0.00001794
Iteration 37/1000 | Loss: 0.00001793
Iteration 38/1000 | Loss: 0.00001793
Iteration 39/1000 | Loss: 0.00001792
Iteration 40/1000 | Loss: 0.00001792
Iteration 41/1000 | Loss: 0.00001792
Iteration 42/1000 | Loss: 0.00001791
Iteration 43/1000 | Loss: 0.00001791
Iteration 44/1000 | Loss: 0.00001790
Iteration 45/1000 | Loss: 0.00001790
Iteration 46/1000 | Loss: 0.00001789
Iteration 47/1000 | Loss: 0.00001789
Iteration 48/1000 | Loss: 0.00001789
Iteration 49/1000 | Loss: 0.00001789
Iteration 50/1000 | Loss: 0.00001788
Iteration 51/1000 | Loss: 0.00001788
Iteration 52/1000 | Loss: 0.00001788
Iteration 53/1000 | Loss: 0.00001787
Iteration 54/1000 | Loss: 0.00001787
Iteration 55/1000 | Loss: 0.00001787
Iteration 56/1000 | Loss: 0.00001787
Iteration 57/1000 | Loss: 0.00001787
Iteration 58/1000 | Loss: 0.00001786
Iteration 59/1000 | Loss: 0.00001786
Iteration 60/1000 | Loss: 0.00001786
Iteration 61/1000 | Loss: 0.00001786
Iteration 62/1000 | Loss: 0.00001785
Iteration 63/1000 | Loss: 0.00001785
Iteration 64/1000 | Loss: 0.00001785
Iteration 65/1000 | Loss: 0.00001784
Iteration 66/1000 | Loss: 0.00001784
Iteration 67/1000 | Loss: 0.00001784
Iteration 68/1000 | Loss: 0.00001784
Iteration 69/1000 | Loss: 0.00001783
Iteration 70/1000 | Loss: 0.00001783
Iteration 71/1000 | Loss: 0.00001783
Iteration 72/1000 | Loss: 0.00001782
Iteration 73/1000 | Loss: 0.00001782
Iteration 74/1000 | Loss: 0.00001782
Iteration 75/1000 | Loss: 0.00001781
Iteration 76/1000 | Loss: 0.00001781
Iteration 77/1000 | Loss: 0.00001781
Iteration 78/1000 | Loss: 0.00001780
Iteration 79/1000 | Loss: 0.00001780
Iteration 80/1000 | Loss: 0.00001780
Iteration 81/1000 | Loss: 0.00001780
Iteration 82/1000 | Loss: 0.00001779
Iteration 83/1000 | Loss: 0.00001779
Iteration 84/1000 | Loss: 0.00001779
Iteration 85/1000 | Loss: 0.00001778
Iteration 86/1000 | Loss: 0.00001778
Iteration 87/1000 | Loss: 0.00001778
Iteration 88/1000 | Loss: 0.00001777
Iteration 89/1000 | Loss: 0.00001777
Iteration 90/1000 | Loss: 0.00001777
Iteration 91/1000 | Loss: 0.00001777
Iteration 92/1000 | Loss: 0.00001777
Iteration 93/1000 | Loss: 0.00001776
Iteration 94/1000 | Loss: 0.00001776
Iteration 95/1000 | Loss: 0.00001776
Iteration 96/1000 | Loss: 0.00001776
Iteration 97/1000 | Loss: 0.00001776
Iteration 98/1000 | Loss: 0.00001776
Iteration 99/1000 | Loss: 0.00001775
Iteration 100/1000 | Loss: 0.00001775
Iteration 101/1000 | Loss: 0.00001775
Iteration 102/1000 | Loss: 0.00001775
Iteration 103/1000 | Loss: 0.00001775
Iteration 104/1000 | Loss: 0.00001775
Iteration 105/1000 | Loss: 0.00001775
Iteration 106/1000 | Loss: 0.00001775
Iteration 107/1000 | Loss: 0.00001775
Iteration 108/1000 | Loss: 0.00001775
Iteration 109/1000 | Loss: 0.00001775
Iteration 110/1000 | Loss: 0.00001774
Iteration 111/1000 | Loss: 0.00001774
Iteration 112/1000 | Loss: 0.00001774
Iteration 113/1000 | Loss: 0.00001774
Iteration 114/1000 | Loss: 0.00001773
Iteration 115/1000 | Loss: 0.00001773
Iteration 116/1000 | Loss: 0.00001773
Iteration 117/1000 | Loss: 0.00001773
Iteration 118/1000 | Loss: 0.00001773
Iteration 119/1000 | Loss: 0.00001773
Iteration 120/1000 | Loss: 0.00001773
Iteration 121/1000 | Loss: 0.00001773
Iteration 122/1000 | Loss: 0.00001773
Iteration 123/1000 | Loss: 0.00001773
Iteration 124/1000 | Loss: 0.00001773
Iteration 125/1000 | Loss: 0.00001773
Iteration 126/1000 | Loss: 0.00001772
Iteration 127/1000 | Loss: 0.00001772
Iteration 128/1000 | Loss: 0.00001772
Iteration 129/1000 | Loss: 0.00001772
Iteration 130/1000 | Loss: 0.00001772
Iteration 131/1000 | Loss: 0.00001772
Iteration 132/1000 | Loss: 0.00001772
Iteration 133/1000 | Loss: 0.00001772
Iteration 134/1000 | Loss: 0.00001772
Iteration 135/1000 | Loss: 0.00001772
Iteration 136/1000 | Loss: 0.00001772
Iteration 137/1000 | Loss: 0.00001772
Iteration 138/1000 | Loss: 0.00001772
Iteration 139/1000 | Loss: 0.00001771
Iteration 140/1000 | Loss: 0.00001771
Iteration 141/1000 | Loss: 0.00001771
Iteration 142/1000 | Loss: 0.00001771
Iteration 143/1000 | Loss: 0.00001771
Iteration 144/1000 | Loss: 0.00001771
Iteration 145/1000 | Loss: 0.00001771
Iteration 146/1000 | Loss: 0.00001771
Iteration 147/1000 | Loss: 0.00001771
Iteration 148/1000 | Loss: 0.00001771
Iteration 149/1000 | Loss: 0.00001771
Iteration 150/1000 | Loss: 0.00001771
Iteration 151/1000 | Loss: 0.00001771
Iteration 152/1000 | Loss: 0.00001770
Iteration 153/1000 | Loss: 0.00001770
Iteration 154/1000 | Loss: 0.00001770
Iteration 155/1000 | Loss: 0.00001770
Iteration 156/1000 | Loss: 0.00001770
Iteration 157/1000 | Loss: 0.00001770
Iteration 158/1000 | Loss: 0.00001770
Iteration 159/1000 | Loss: 0.00001770
Iteration 160/1000 | Loss: 0.00001770
Iteration 161/1000 | Loss: 0.00001770
Iteration 162/1000 | Loss: 0.00001770
Iteration 163/1000 | Loss: 0.00001770
Iteration 164/1000 | Loss: 0.00001770
Iteration 165/1000 | Loss: 0.00001770
Iteration 166/1000 | Loss: 0.00001770
Iteration 167/1000 | Loss: 0.00001770
Iteration 168/1000 | Loss: 0.00001769
Iteration 169/1000 | Loss: 0.00001769
Iteration 170/1000 | Loss: 0.00001769
Iteration 171/1000 | Loss: 0.00001769
Iteration 172/1000 | Loss: 0.00001769
Iteration 173/1000 | Loss: 0.00001769
Iteration 174/1000 | Loss: 0.00001769
Iteration 175/1000 | Loss: 0.00001769
Iteration 176/1000 | Loss: 0.00001769
Iteration 177/1000 | Loss: 0.00001769
Iteration 178/1000 | Loss: 0.00001769
Iteration 179/1000 | Loss: 0.00001769
Iteration 180/1000 | Loss: 0.00001769
Iteration 181/1000 | Loss: 0.00001769
Iteration 182/1000 | Loss: 0.00001769
Iteration 183/1000 | Loss: 0.00001769
Iteration 184/1000 | Loss: 0.00001769
Iteration 185/1000 | Loss: 0.00001769
Iteration 186/1000 | Loss: 0.00001769
Iteration 187/1000 | Loss: 0.00001769
Iteration 188/1000 | Loss: 0.00001769
Iteration 189/1000 | Loss: 0.00001769
Iteration 190/1000 | Loss: 0.00001769
Iteration 191/1000 | Loss: 0.00001769
Iteration 192/1000 | Loss: 0.00001769
Iteration 193/1000 | Loss: 0.00001769
Iteration 194/1000 | Loss: 0.00001769
Iteration 195/1000 | Loss: 0.00001769
Iteration 196/1000 | Loss: 0.00001769
Iteration 197/1000 | Loss: 0.00001769
Iteration 198/1000 | Loss: 0.00001769
Iteration 199/1000 | Loss: 0.00001769
Iteration 200/1000 | Loss: 0.00001769
Iteration 201/1000 | Loss: 0.00001769
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 201. Stopping optimization.
Last 5 losses: [1.7690486856736243e-05, 1.7690486856736243e-05, 1.7690486856736243e-05, 1.7690486856736243e-05, 1.7690486856736243e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7690486856736243e-05

Optimization complete. Final v2v error: 3.499192714691162 mm

Highest mean error: 3.9293394088745117 mm for frame 53

Lowest mean error: 2.9585065841674805 mm for frame 146

Saving results

Total time: 47.205557346343994
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00906587
Iteration 2/25 | Loss: 0.00197399
Iteration 3/25 | Loss: 0.00149998
Iteration 4/25 | Loss: 0.00136129
Iteration 5/25 | Loss: 0.00117332
Iteration 6/25 | Loss: 0.00100224
Iteration 7/25 | Loss: 0.00097744
Iteration 8/25 | Loss: 0.00094298
Iteration 9/25 | Loss: 0.00092554
Iteration 10/25 | Loss: 0.00092135
Iteration 11/25 | Loss: 0.00091665
Iteration 12/25 | Loss: 0.00091702
Iteration 13/25 | Loss: 0.00091518
Iteration 14/25 | Loss: 0.00091363
Iteration 15/25 | Loss: 0.00091361
Iteration 16/25 | Loss: 0.00091318
Iteration 17/25 | Loss: 0.00091339
Iteration 18/25 | Loss: 0.00091413
Iteration 19/25 | Loss: 0.00091315
Iteration 20/25 | Loss: 0.00091333
Iteration 21/25 | Loss: 0.00091351
Iteration 22/25 | Loss: 0.00091397
Iteration 23/25 | Loss: 0.00091318
Iteration 24/25 | Loss: 0.00091359
Iteration 25/25 | Loss: 0.00091373

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59814930
Iteration 2/25 | Loss: 0.00132820
Iteration 3/25 | Loss: 0.00132817
Iteration 4/25 | Loss: 0.00132817
Iteration 5/25 | Loss: 0.00132817
Iteration 6/25 | Loss: 0.00132817
Iteration 7/25 | Loss: 0.00132817
Iteration 8/25 | Loss: 0.00132817
Iteration 9/25 | Loss: 0.00132817
Iteration 10/25 | Loss: 0.00132817
Iteration 11/25 | Loss: 0.00132817
Iteration 12/25 | Loss: 0.00132817
Iteration 13/25 | Loss: 0.00132817
Iteration 14/25 | Loss: 0.00132817
Iteration 15/25 | Loss: 0.00132817
Iteration 16/25 | Loss: 0.00132817
Iteration 17/25 | Loss: 0.00132817
Iteration 18/25 | Loss: 0.00132817
Iteration 19/25 | Loss: 0.00132817
Iteration 20/25 | Loss: 0.00132817
Iteration 21/25 | Loss: 0.00132817
Iteration 22/25 | Loss: 0.00132817
Iteration 23/25 | Loss: 0.00132817
Iteration 24/25 | Loss: 0.00132817
Iteration 25/25 | Loss: 0.00132817

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132817
Iteration 2/1000 | Loss: 0.00008647
Iteration 3/1000 | Loss: 0.00011371
Iteration 4/1000 | Loss: 0.00006256
Iteration 5/1000 | Loss: 0.00007407
Iteration 6/1000 | Loss: 0.00005606
Iteration 7/1000 | Loss: 0.00009192
Iteration 8/1000 | Loss: 0.00008951
Iteration 9/1000 | Loss: 0.00008132
Iteration 10/1000 | Loss: 0.00008280
Iteration 11/1000 | Loss: 0.00007774
Iteration 12/1000 | Loss: 0.00007620
Iteration 13/1000 | Loss: 0.00007801
Iteration 14/1000 | Loss: 0.00006330
Iteration 15/1000 | Loss: 0.00007684
Iteration 16/1000 | Loss: 0.00007807
Iteration 17/1000 | Loss: 0.00009427
Iteration 18/1000 | Loss: 0.00008857
Iteration 19/1000 | Loss: 0.00005993
Iteration 20/1000 | Loss: 0.00005256
Iteration 21/1000 | Loss: 0.00004055
Iteration 22/1000 | Loss: 0.00007587
Iteration 23/1000 | Loss: 0.00007997
Iteration 24/1000 | Loss: 0.00005775
Iteration 25/1000 | Loss: 0.00003418
Iteration 26/1000 | Loss: 0.00003660
Iteration 27/1000 | Loss: 0.00005230
Iteration 28/1000 | Loss: 0.00004235
Iteration 29/1000 | Loss: 0.00004224
Iteration 30/1000 | Loss: 0.00005149
Iteration 31/1000 | Loss: 0.00003840
Iteration 32/1000 | Loss: 0.00005092
Iteration 33/1000 | Loss: 0.00005898
Iteration 34/1000 | Loss: 0.00004652
Iteration 35/1000 | Loss: 0.00004995
Iteration 36/1000 | Loss: 0.00005326
Iteration 37/1000 | Loss: 0.00005539
Iteration 38/1000 | Loss: 0.00005147
Iteration 39/1000 | Loss: 0.00004385
Iteration 40/1000 | Loss: 0.00004183
Iteration 41/1000 | Loss: 0.00004788
Iteration 42/1000 | Loss: 0.00004622
Iteration 43/1000 | Loss: 0.00006514
Iteration 44/1000 | Loss: 0.00004162
Iteration 45/1000 | Loss: 0.00003963
Iteration 46/1000 | Loss: 0.00004510
Iteration 47/1000 | Loss: 0.00006045
Iteration 48/1000 | Loss: 0.00004577
Iteration 49/1000 | Loss: 0.00005386
Iteration 50/1000 | Loss: 0.00003726
Iteration 51/1000 | Loss: 0.00003860
Iteration 52/1000 | Loss: 0.00005104
Iteration 53/1000 | Loss: 0.00005030
Iteration 54/1000 | Loss: 0.00003826
Iteration 55/1000 | Loss: 0.00006126
Iteration 56/1000 | Loss: 0.00005128
Iteration 57/1000 | Loss: 0.00005343
Iteration 58/1000 | Loss: 0.00003928
Iteration 59/1000 | Loss: 0.00003914
Iteration 60/1000 | Loss: 0.00004753
Iteration 61/1000 | Loss: 0.00004258
Iteration 62/1000 | Loss: 0.00005574
Iteration 63/1000 | Loss: 0.00006463
Iteration 64/1000 | Loss: 0.00005248
Iteration 65/1000 | Loss: 0.00004666
Iteration 66/1000 | Loss: 0.00005066
Iteration 67/1000 | Loss: 0.00005326
Iteration 68/1000 | Loss: 0.00002709
Iteration 69/1000 | Loss: 0.00005382
Iteration 70/1000 | Loss: 0.00002855
Iteration 71/1000 | Loss: 0.00002309
Iteration 72/1000 | Loss: 0.00002150
Iteration 73/1000 | Loss: 0.00002064
Iteration 74/1000 | Loss: 0.00002024
Iteration 75/1000 | Loss: 0.00002004
Iteration 76/1000 | Loss: 0.00001986
Iteration 77/1000 | Loss: 0.00001983
Iteration 78/1000 | Loss: 0.00001980
Iteration 79/1000 | Loss: 0.00001976
Iteration 80/1000 | Loss: 0.00001976
Iteration 81/1000 | Loss: 0.00001975
Iteration 82/1000 | Loss: 0.00001974
Iteration 83/1000 | Loss: 0.00001974
Iteration 84/1000 | Loss: 0.00001974
Iteration 85/1000 | Loss: 0.00001973
Iteration 86/1000 | Loss: 0.00001973
Iteration 87/1000 | Loss: 0.00001973
Iteration 88/1000 | Loss: 0.00001973
Iteration 89/1000 | Loss: 0.00001973
Iteration 90/1000 | Loss: 0.00001972
Iteration 91/1000 | Loss: 0.00001972
Iteration 92/1000 | Loss: 0.00001972
Iteration 93/1000 | Loss: 0.00001972
Iteration 94/1000 | Loss: 0.00001972
Iteration 95/1000 | Loss: 0.00001972
Iteration 96/1000 | Loss: 0.00001971
Iteration 97/1000 | Loss: 0.00001971
Iteration 98/1000 | Loss: 0.00001970
Iteration 99/1000 | Loss: 0.00001969
Iteration 100/1000 | Loss: 0.00001969
Iteration 101/1000 | Loss: 0.00001968
Iteration 102/1000 | Loss: 0.00001967
Iteration 103/1000 | Loss: 0.00001967
Iteration 104/1000 | Loss: 0.00001966
Iteration 105/1000 | Loss: 0.00001966
Iteration 106/1000 | Loss: 0.00001965
Iteration 107/1000 | Loss: 0.00001955
Iteration 108/1000 | Loss: 0.00001954
Iteration 109/1000 | Loss: 0.00001953
Iteration 110/1000 | Loss: 0.00001946
Iteration 111/1000 | Loss: 0.00001945
Iteration 112/1000 | Loss: 0.00001944
Iteration 113/1000 | Loss: 0.00001944
Iteration 114/1000 | Loss: 0.00001944
Iteration 115/1000 | Loss: 0.00001944
Iteration 116/1000 | Loss: 0.00001944
Iteration 117/1000 | Loss: 0.00001943
Iteration 118/1000 | Loss: 0.00001943
Iteration 119/1000 | Loss: 0.00001943
Iteration 120/1000 | Loss: 0.00001942
Iteration 121/1000 | Loss: 0.00001942
Iteration 122/1000 | Loss: 0.00001942
Iteration 123/1000 | Loss: 0.00001941
Iteration 124/1000 | Loss: 0.00001941
Iteration 125/1000 | Loss: 0.00001941
Iteration 126/1000 | Loss: 0.00001941
Iteration 127/1000 | Loss: 0.00001941
Iteration 128/1000 | Loss: 0.00001940
Iteration 129/1000 | Loss: 0.00001940
Iteration 130/1000 | Loss: 0.00001940
Iteration 131/1000 | Loss: 0.00001940
Iteration 132/1000 | Loss: 0.00001940
Iteration 133/1000 | Loss: 0.00001939
Iteration 134/1000 | Loss: 0.00001939
Iteration 135/1000 | Loss: 0.00001939
Iteration 136/1000 | Loss: 0.00001939
Iteration 137/1000 | Loss: 0.00001939
Iteration 138/1000 | Loss: 0.00001939
Iteration 139/1000 | Loss: 0.00001939
Iteration 140/1000 | Loss: 0.00001939
Iteration 141/1000 | Loss: 0.00001939
Iteration 142/1000 | Loss: 0.00001939
Iteration 143/1000 | Loss: 0.00001938
Iteration 144/1000 | Loss: 0.00001938
Iteration 145/1000 | Loss: 0.00001938
Iteration 146/1000 | Loss: 0.00001938
Iteration 147/1000 | Loss: 0.00001938
Iteration 148/1000 | Loss: 0.00001937
Iteration 149/1000 | Loss: 0.00001937
Iteration 150/1000 | Loss: 0.00001937
Iteration 151/1000 | Loss: 0.00001937
Iteration 152/1000 | Loss: 0.00001936
Iteration 153/1000 | Loss: 0.00001936
Iteration 154/1000 | Loss: 0.00001936
Iteration 155/1000 | Loss: 0.00001936
Iteration 156/1000 | Loss: 0.00001935
Iteration 157/1000 | Loss: 0.00001935
Iteration 158/1000 | Loss: 0.00001934
Iteration 159/1000 | Loss: 0.00001934
Iteration 160/1000 | Loss: 0.00001934
Iteration 161/1000 | Loss: 0.00001934
Iteration 162/1000 | Loss: 0.00001934
Iteration 163/1000 | Loss: 0.00001934
Iteration 164/1000 | Loss: 0.00001933
Iteration 165/1000 | Loss: 0.00001933
Iteration 166/1000 | Loss: 0.00001933
Iteration 167/1000 | Loss: 0.00001933
Iteration 168/1000 | Loss: 0.00001933
Iteration 169/1000 | Loss: 0.00001933
Iteration 170/1000 | Loss: 0.00001933
Iteration 171/1000 | Loss: 0.00001933
Iteration 172/1000 | Loss: 0.00001933
Iteration 173/1000 | Loss: 0.00001933
Iteration 174/1000 | Loss: 0.00001933
Iteration 175/1000 | Loss: 0.00001933
Iteration 176/1000 | Loss: 0.00001933
Iteration 177/1000 | Loss: 0.00001932
Iteration 178/1000 | Loss: 0.00001932
Iteration 179/1000 | Loss: 0.00001932
Iteration 180/1000 | Loss: 0.00001932
Iteration 181/1000 | Loss: 0.00001932
Iteration 182/1000 | Loss: 0.00001932
Iteration 183/1000 | Loss: 0.00001932
Iteration 184/1000 | Loss: 0.00001932
Iteration 185/1000 | Loss: 0.00001932
Iteration 186/1000 | Loss: 0.00001932
Iteration 187/1000 | Loss: 0.00001932
Iteration 188/1000 | Loss: 0.00001932
Iteration 189/1000 | Loss: 0.00001931
Iteration 190/1000 | Loss: 0.00001931
Iteration 191/1000 | Loss: 0.00001931
Iteration 192/1000 | Loss: 0.00001931
Iteration 193/1000 | Loss: 0.00001931
Iteration 194/1000 | Loss: 0.00001931
Iteration 195/1000 | Loss: 0.00001931
Iteration 196/1000 | Loss: 0.00001931
Iteration 197/1000 | Loss: 0.00001931
Iteration 198/1000 | Loss: 0.00001931
Iteration 199/1000 | Loss: 0.00001931
Iteration 200/1000 | Loss: 0.00001931
Iteration 201/1000 | Loss: 0.00001931
Iteration 202/1000 | Loss: 0.00001931
Iteration 203/1000 | Loss: 0.00001931
Iteration 204/1000 | Loss: 0.00001931
Iteration 205/1000 | Loss: 0.00001931
Iteration 206/1000 | Loss: 0.00001931
Iteration 207/1000 | Loss: 0.00001931
Iteration 208/1000 | Loss: 0.00001931
Iteration 209/1000 | Loss: 0.00001931
Iteration 210/1000 | Loss: 0.00001931
Iteration 211/1000 | Loss: 0.00001931
Iteration 212/1000 | Loss: 0.00001931
Iteration 213/1000 | Loss: 0.00001930
Iteration 214/1000 | Loss: 0.00001930
Iteration 215/1000 | Loss: 0.00001930
Iteration 216/1000 | Loss: 0.00001930
Iteration 217/1000 | Loss: 0.00001930
Iteration 218/1000 | Loss: 0.00001930
Iteration 219/1000 | Loss: 0.00001930
Iteration 220/1000 | Loss: 0.00001930
Iteration 221/1000 | Loss: 0.00001930
Iteration 222/1000 | Loss: 0.00001930
Iteration 223/1000 | Loss: 0.00001930
Iteration 224/1000 | Loss: 0.00001930
Iteration 225/1000 | Loss: 0.00001930
Iteration 226/1000 | Loss: 0.00001930
Iteration 227/1000 | Loss: 0.00001930
Iteration 228/1000 | Loss: 0.00001930
Iteration 229/1000 | Loss: 0.00001930
Iteration 230/1000 | Loss: 0.00001930
Iteration 231/1000 | Loss: 0.00001930
Iteration 232/1000 | Loss: 0.00001929
Iteration 233/1000 | Loss: 0.00001929
Iteration 234/1000 | Loss: 0.00001929
Iteration 235/1000 | Loss: 0.00001929
Iteration 236/1000 | Loss: 0.00001929
Iteration 237/1000 | Loss: 0.00001929
Iteration 238/1000 | Loss: 0.00001929
Iteration 239/1000 | Loss: 0.00001929
Iteration 240/1000 | Loss: 0.00001929
Iteration 241/1000 | Loss: 0.00001929
Iteration 242/1000 | Loss: 0.00001929
Iteration 243/1000 | Loss: 0.00001929
Iteration 244/1000 | Loss: 0.00001929
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 244. Stopping optimization.
Last 5 losses: [1.9292756405775435e-05, 1.9292756405775435e-05, 1.9292756405775435e-05, 1.9292756405775435e-05, 1.9292756405775435e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9292756405775435e-05

Optimization complete. Final v2v error: 3.7210586071014404 mm

Highest mean error: 4.312469005584717 mm for frame 99

Lowest mean error: 3.571800708770752 mm for frame 73

Saving results

Total time: 192.10207772254944
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1043/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1043.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1043
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00403009
Iteration 2/25 | Loss: 0.00088074
Iteration 3/25 | Loss: 0.00076007
Iteration 4/25 | Loss: 0.00074123
Iteration 5/25 | Loss: 0.00073713
Iteration 6/25 | Loss: 0.00073601
Iteration 7/25 | Loss: 0.00073564
Iteration 8/25 | Loss: 0.00073560
Iteration 9/25 | Loss: 0.00073560
Iteration 10/25 | Loss: 0.00073560
Iteration 11/25 | Loss: 0.00073560
Iteration 12/25 | Loss: 0.00073560
Iteration 13/25 | Loss: 0.00073560
Iteration 14/25 | Loss: 0.00073560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0007356039714068174, 0.0007356039714068174, 0.0007356039714068174, 0.0007356039714068174, 0.0007356039714068174]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007356039714068174

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.70539701
Iteration 2/25 | Loss: 0.00122021
Iteration 3/25 | Loss: 0.00122021
Iteration 4/25 | Loss: 0.00122021
Iteration 5/25 | Loss: 0.00122021
Iteration 6/25 | Loss: 0.00122020
Iteration 7/25 | Loss: 0.00122020
Iteration 8/25 | Loss: 0.00122020
Iteration 9/25 | Loss: 0.00122020
Iteration 10/25 | Loss: 0.00122020
Iteration 11/25 | Loss: 0.00122020
Iteration 12/25 | Loss: 0.00122020
Iteration 13/25 | Loss: 0.00122020
Iteration 14/25 | Loss: 0.00122020
Iteration 15/25 | Loss: 0.00122020
Iteration 16/25 | Loss: 0.00122020
Iteration 17/25 | Loss: 0.00122020
Iteration 18/25 | Loss: 0.00122020
Iteration 19/25 | Loss: 0.00122020
Iteration 20/25 | Loss: 0.00122020
Iteration 21/25 | Loss: 0.00122020
Iteration 22/25 | Loss: 0.00122020
Iteration 23/25 | Loss: 0.00122020
Iteration 24/25 | Loss: 0.00122020
Iteration 25/25 | Loss: 0.00122020

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00122020
Iteration 2/1000 | Loss: 0.00002853
Iteration 3/1000 | Loss: 0.00001761
Iteration 4/1000 | Loss: 0.00001488
Iteration 5/1000 | Loss: 0.00001412
Iteration 6/1000 | Loss: 0.00001350
Iteration 7/1000 | Loss: 0.00001316
Iteration 8/1000 | Loss: 0.00001292
Iteration 9/1000 | Loss: 0.00001279
Iteration 10/1000 | Loss: 0.00001268
Iteration 11/1000 | Loss: 0.00001258
Iteration 12/1000 | Loss: 0.00001254
Iteration 13/1000 | Loss: 0.00001253
Iteration 14/1000 | Loss: 0.00001241
Iteration 15/1000 | Loss: 0.00001240
Iteration 16/1000 | Loss: 0.00001234
Iteration 17/1000 | Loss: 0.00001231
Iteration 18/1000 | Loss: 0.00001230
Iteration 19/1000 | Loss: 0.00001228
Iteration 20/1000 | Loss: 0.00001228
Iteration 21/1000 | Loss: 0.00001227
Iteration 22/1000 | Loss: 0.00001227
Iteration 23/1000 | Loss: 0.00001227
Iteration 24/1000 | Loss: 0.00001225
Iteration 25/1000 | Loss: 0.00001225
Iteration 26/1000 | Loss: 0.00001224
Iteration 27/1000 | Loss: 0.00001224
Iteration 28/1000 | Loss: 0.00001224
Iteration 29/1000 | Loss: 0.00001224
Iteration 30/1000 | Loss: 0.00001224
Iteration 31/1000 | Loss: 0.00001224
Iteration 32/1000 | Loss: 0.00001223
Iteration 33/1000 | Loss: 0.00001223
Iteration 34/1000 | Loss: 0.00001222
Iteration 35/1000 | Loss: 0.00001222
Iteration 36/1000 | Loss: 0.00001221
Iteration 37/1000 | Loss: 0.00001220
Iteration 38/1000 | Loss: 0.00001220
Iteration 39/1000 | Loss: 0.00001220
Iteration 40/1000 | Loss: 0.00001220
Iteration 41/1000 | Loss: 0.00001220
Iteration 42/1000 | Loss: 0.00001220
Iteration 43/1000 | Loss: 0.00001219
Iteration 44/1000 | Loss: 0.00001217
Iteration 45/1000 | Loss: 0.00001217
Iteration 46/1000 | Loss: 0.00001216
Iteration 47/1000 | Loss: 0.00001216
Iteration 48/1000 | Loss: 0.00001216
Iteration 49/1000 | Loss: 0.00001215
Iteration 50/1000 | Loss: 0.00001214
Iteration 51/1000 | Loss: 0.00001214
Iteration 52/1000 | Loss: 0.00001214
Iteration 53/1000 | Loss: 0.00001213
Iteration 54/1000 | Loss: 0.00001213
Iteration 55/1000 | Loss: 0.00001213
Iteration 56/1000 | Loss: 0.00001212
Iteration 57/1000 | Loss: 0.00001212
Iteration 58/1000 | Loss: 0.00001212
Iteration 59/1000 | Loss: 0.00001212
Iteration 60/1000 | Loss: 0.00001212
Iteration 61/1000 | Loss: 0.00001212
Iteration 62/1000 | Loss: 0.00001212
Iteration 63/1000 | Loss: 0.00001212
Iteration 64/1000 | Loss: 0.00001211
Iteration 65/1000 | Loss: 0.00001211
Iteration 66/1000 | Loss: 0.00001211
Iteration 67/1000 | Loss: 0.00001210
Iteration 68/1000 | Loss: 0.00001210
Iteration 69/1000 | Loss: 0.00001210
Iteration 70/1000 | Loss: 0.00001210
Iteration 71/1000 | Loss: 0.00001210
Iteration 72/1000 | Loss: 0.00001210
Iteration 73/1000 | Loss: 0.00001210
Iteration 74/1000 | Loss: 0.00001210
Iteration 75/1000 | Loss: 0.00001210
Iteration 76/1000 | Loss: 0.00001210
Iteration 77/1000 | Loss: 0.00001210
Iteration 78/1000 | Loss: 0.00001210
Iteration 79/1000 | Loss: 0.00001210
Iteration 80/1000 | Loss: 0.00001210
Iteration 81/1000 | Loss: 0.00001210
Iteration 82/1000 | Loss: 0.00001210
Iteration 83/1000 | Loss: 0.00001210
Iteration 84/1000 | Loss: 0.00001210
Iteration 85/1000 | Loss: 0.00001210
Iteration 86/1000 | Loss: 0.00001210
Iteration 87/1000 | Loss: 0.00001210
Iteration 88/1000 | Loss: 0.00001210
Iteration 89/1000 | Loss: 0.00001210
Iteration 90/1000 | Loss: 0.00001210
Iteration 91/1000 | Loss: 0.00001210
Iteration 92/1000 | Loss: 0.00001210
Iteration 93/1000 | Loss: 0.00001210
Iteration 94/1000 | Loss: 0.00001210
Iteration 95/1000 | Loss: 0.00001210
Iteration 96/1000 | Loss: 0.00001210
Iteration 97/1000 | Loss: 0.00001210
Iteration 98/1000 | Loss: 0.00001210
Iteration 99/1000 | Loss: 0.00001210
Iteration 100/1000 | Loss: 0.00001210
Iteration 101/1000 | Loss: 0.00001210
Iteration 102/1000 | Loss: 0.00001210
Iteration 103/1000 | Loss: 0.00001210
Iteration 104/1000 | Loss: 0.00001210
Iteration 105/1000 | Loss: 0.00001210
Iteration 106/1000 | Loss: 0.00001210
Iteration 107/1000 | Loss: 0.00001210
Iteration 108/1000 | Loss: 0.00001210
Iteration 109/1000 | Loss: 0.00001210
Iteration 110/1000 | Loss: 0.00001210
Iteration 111/1000 | Loss: 0.00001210
Iteration 112/1000 | Loss: 0.00001210
Iteration 113/1000 | Loss: 0.00001210
Iteration 114/1000 | Loss: 0.00001210
Iteration 115/1000 | Loss: 0.00001210
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 115. Stopping optimization.
Last 5 losses: [1.2096453247067984e-05, 1.2096453247067984e-05, 1.2096453247067984e-05, 1.2096453247067984e-05, 1.2096453247067984e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2096453247067984e-05

Optimization complete. Final v2v error: 2.9254298210144043 mm

Highest mean error: 3.732571601867676 mm for frame 67

Lowest mean error: 2.6817216873168945 mm for frame 120

Saving results

Total time: 33.49806594848633
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1026/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1026.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1026
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00548777
Iteration 2/25 | Loss: 0.00124645
Iteration 3/25 | Loss: 0.00094204
Iteration 4/25 | Loss: 0.00089927
Iteration 5/25 | Loss: 0.00089014
Iteration 6/25 | Loss: 0.00088790
Iteration 7/25 | Loss: 0.00088760
Iteration 8/25 | Loss: 0.00088760
Iteration 9/25 | Loss: 0.00088760
Iteration 10/25 | Loss: 0.00088760
Iteration 11/25 | Loss: 0.00088760
Iteration 12/25 | Loss: 0.00088760
Iteration 13/25 | Loss: 0.00088760
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0008876013453118503, 0.0008876013453118503, 0.0008876013453118503, 0.0008876013453118503, 0.0008876013453118503]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008876013453118503

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.69731402
Iteration 2/25 | Loss: 0.00119150
Iteration 3/25 | Loss: 0.00119149
Iteration 4/25 | Loss: 0.00119149
Iteration 5/25 | Loss: 0.00119149
Iteration 6/25 | Loss: 0.00119149
Iteration 7/25 | Loss: 0.00119149
Iteration 8/25 | Loss: 0.00119149
Iteration 9/25 | Loss: 0.00119149
Iteration 10/25 | Loss: 0.00119149
Iteration 11/25 | Loss: 0.00119149
Iteration 12/25 | Loss: 0.00119149
Iteration 13/25 | Loss: 0.00119149
Iteration 14/25 | Loss: 0.00119149
Iteration 15/25 | Loss: 0.00119149
Iteration 16/25 | Loss: 0.00119149
Iteration 17/25 | Loss: 0.00119149
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011914880014955997, 0.0011914880014955997, 0.0011914880014955997, 0.0011914880014955997, 0.0011914880014955997]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011914880014955997

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119149
Iteration 2/1000 | Loss: 0.00005021
Iteration 3/1000 | Loss: 0.00003864
Iteration 4/1000 | Loss: 0.00003485
Iteration 5/1000 | Loss: 0.00003350
Iteration 6/1000 | Loss: 0.00003191
Iteration 7/1000 | Loss: 0.00003115
Iteration 8/1000 | Loss: 0.00003044
Iteration 9/1000 | Loss: 0.00003004
Iteration 10/1000 | Loss: 0.00002982
Iteration 11/1000 | Loss: 0.00002963
Iteration 12/1000 | Loss: 0.00002943
Iteration 13/1000 | Loss: 0.00002926
Iteration 14/1000 | Loss: 0.00002922
Iteration 15/1000 | Loss: 0.00002920
Iteration 16/1000 | Loss: 0.00002919
Iteration 17/1000 | Loss: 0.00002919
Iteration 18/1000 | Loss: 0.00002918
Iteration 19/1000 | Loss: 0.00002918
Iteration 20/1000 | Loss: 0.00002917
Iteration 21/1000 | Loss: 0.00002917
Iteration 22/1000 | Loss: 0.00002916
Iteration 23/1000 | Loss: 0.00002916
Iteration 24/1000 | Loss: 0.00002915
Iteration 25/1000 | Loss: 0.00002915
Iteration 26/1000 | Loss: 0.00002911
Iteration 27/1000 | Loss: 0.00002911
Iteration 28/1000 | Loss: 0.00002910
Iteration 29/1000 | Loss: 0.00002910
Iteration 30/1000 | Loss: 0.00002909
Iteration 31/1000 | Loss: 0.00002909
Iteration 32/1000 | Loss: 0.00002909
Iteration 33/1000 | Loss: 0.00002909
Iteration 34/1000 | Loss: 0.00002909
Iteration 35/1000 | Loss: 0.00002908
Iteration 36/1000 | Loss: 0.00002908
Iteration 37/1000 | Loss: 0.00002907
Iteration 38/1000 | Loss: 0.00002907
Iteration 39/1000 | Loss: 0.00002907
Iteration 40/1000 | Loss: 0.00002906
Iteration 41/1000 | Loss: 0.00002906
Iteration 42/1000 | Loss: 0.00002906
Iteration 43/1000 | Loss: 0.00002906
Iteration 44/1000 | Loss: 0.00002906
Iteration 45/1000 | Loss: 0.00002905
Iteration 46/1000 | Loss: 0.00002905
Iteration 47/1000 | Loss: 0.00002905
Iteration 48/1000 | Loss: 0.00002905
Iteration 49/1000 | Loss: 0.00002904
Iteration 50/1000 | Loss: 0.00002904
Iteration 51/1000 | Loss: 0.00002903
Iteration 52/1000 | Loss: 0.00002903
Iteration 53/1000 | Loss: 0.00002903
Iteration 54/1000 | Loss: 0.00002903
Iteration 55/1000 | Loss: 0.00002903
Iteration 56/1000 | Loss: 0.00002903
Iteration 57/1000 | Loss: 0.00002903
Iteration 58/1000 | Loss: 0.00002902
Iteration 59/1000 | Loss: 0.00002902
Iteration 60/1000 | Loss: 0.00002902
Iteration 61/1000 | Loss: 0.00002902
Iteration 62/1000 | Loss: 0.00002902
Iteration 63/1000 | Loss: 0.00002902
Iteration 64/1000 | Loss: 0.00002902
Iteration 65/1000 | Loss: 0.00002902
Iteration 66/1000 | Loss: 0.00002902
Iteration 67/1000 | Loss: 0.00002902
Iteration 68/1000 | Loss: 0.00002901
Iteration 69/1000 | Loss: 0.00002901
Iteration 70/1000 | Loss: 0.00002901
Iteration 71/1000 | Loss: 0.00002901
Iteration 72/1000 | Loss: 0.00002901
Iteration 73/1000 | Loss: 0.00002901
Iteration 74/1000 | Loss: 0.00002901
Iteration 75/1000 | Loss: 0.00002900
Iteration 76/1000 | Loss: 0.00002900
Iteration 77/1000 | Loss: 0.00002900
Iteration 78/1000 | Loss: 0.00002900
Iteration 79/1000 | Loss: 0.00002900
Iteration 80/1000 | Loss: 0.00002900
Iteration 81/1000 | Loss: 0.00002900
Iteration 82/1000 | Loss: 0.00002900
Iteration 83/1000 | Loss: 0.00002899
Iteration 84/1000 | Loss: 0.00002899
Iteration 85/1000 | Loss: 0.00002899
Iteration 86/1000 | Loss: 0.00002899
Iteration 87/1000 | Loss: 0.00002899
Iteration 88/1000 | Loss: 0.00002899
Iteration 89/1000 | Loss: 0.00002899
Iteration 90/1000 | Loss: 0.00002899
Iteration 91/1000 | Loss: 0.00002899
Iteration 92/1000 | Loss: 0.00002899
Iteration 93/1000 | Loss: 0.00002899
Iteration 94/1000 | Loss: 0.00002899
Iteration 95/1000 | Loss: 0.00002898
Iteration 96/1000 | Loss: 0.00002898
Iteration 97/1000 | Loss: 0.00002898
Iteration 98/1000 | Loss: 0.00002898
Iteration 99/1000 | Loss: 0.00002898
Iteration 100/1000 | Loss: 0.00002898
Iteration 101/1000 | Loss: 0.00002898
Iteration 102/1000 | Loss: 0.00002898
Iteration 103/1000 | Loss: 0.00002898
Iteration 104/1000 | Loss: 0.00002898
Iteration 105/1000 | Loss: 0.00002898
Iteration 106/1000 | Loss: 0.00002898
Iteration 107/1000 | Loss: 0.00002898
Iteration 108/1000 | Loss: 0.00002898
Iteration 109/1000 | Loss: 0.00002897
Iteration 110/1000 | Loss: 0.00002897
Iteration 111/1000 | Loss: 0.00002897
Iteration 112/1000 | Loss: 0.00002897
Iteration 113/1000 | Loss: 0.00002897
Iteration 114/1000 | Loss: 0.00002897
Iteration 115/1000 | Loss: 0.00002897
Iteration 116/1000 | Loss: 0.00002897
Iteration 117/1000 | Loss: 0.00002897
Iteration 118/1000 | Loss: 0.00002897
Iteration 119/1000 | Loss: 0.00002897
Iteration 120/1000 | Loss: 0.00002897
Iteration 121/1000 | Loss: 0.00002897
Iteration 122/1000 | Loss: 0.00002897
Iteration 123/1000 | Loss: 0.00002897
Iteration 124/1000 | Loss: 0.00002896
Iteration 125/1000 | Loss: 0.00002896
Iteration 126/1000 | Loss: 0.00002896
Iteration 127/1000 | Loss: 0.00002896
Iteration 128/1000 | Loss: 0.00002896
Iteration 129/1000 | Loss: 0.00002896
Iteration 130/1000 | Loss: 0.00002896
Iteration 131/1000 | Loss: 0.00002895
Iteration 132/1000 | Loss: 0.00002895
Iteration 133/1000 | Loss: 0.00002895
Iteration 134/1000 | Loss: 0.00002895
Iteration 135/1000 | Loss: 0.00002895
Iteration 136/1000 | Loss: 0.00002895
Iteration 137/1000 | Loss: 0.00002895
Iteration 138/1000 | Loss: 0.00002895
Iteration 139/1000 | Loss: 0.00002895
Iteration 140/1000 | Loss: 0.00002895
Iteration 141/1000 | Loss: 0.00002895
Iteration 142/1000 | Loss: 0.00002895
Iteration 143/1000 | Loss: 0.00002895
Iteration 144/1000 | Loss: 0.00002895
Iteration 145/1000 | Loss: 0.00002895
Iteration 146/1000 | Loss: 0.00002894
Iteration 147/1000 | Loss: 0.00002894
Iteration 148/1000 | Loss: 0.00002894
Iteration 149/1000 | Loss: 0.00002894
Iteration 150/1000 | Loss: 0.00002894
Iteration 151/1000 | Loss: 0.00002894
Iteration 152/1000 | Loss: 0.00002894
Iteration 153/1000 | Loss: 0.00002894
Iteration 154/1000 | Loss: 0.00002893
Iteration 155/1000 | Loss: 0.00002893
Iteration 156/1000 | Loss: 0.00002893
Iteration 157/1000 | Loss: 0.00002893
Iteration 158/1000 | Loss: 0.00002893
Iteration 159/1000 | Loss: 0.00002893
Iteration 160/1000 | Loss: 0.00002893
Iteration 161/1000 | Loss: 0.00002893
Iteration 162/1000 | Loss: 0.00002893
Iteration 163/1000 | Loss: 0.00002893
Iteration 164/1000 | Loss: 0.00002893
Iteration 165/1000 | Loss: 0.00002893
Iteration 166/1000 | Loss: 0.00002893
Iteration 167/1000 | Loss: 0.00002893
Iteration 168/1000 | Loss: 0.00002893
Iteration 169/1000 | Loss: 0.00002892
Iteration 170/1000 | Loss: 0.00002892
Iteration 171/1000 | Loss: 0.00002892
Iteration 172/1000 | Loss: 0.00002892
Iteration 173/1000 | Loss: 0.00002892
Iteration 174/1000 | Loss: 0.00002892
Iteration 175/1000 | Loss: 0.00002892
Iteration 176/1000 | Loss: 0.00002892
Iteration 177/1000 | Loss: 0.00002892
Iteration 178/1000 | Loss: 0.00002892
Iteration 179/1000 | Loss: 0.00002892
Iteration 180/1000 | Loss: 0.00002892
Iteration 181/1000 | Loss: 0.00002892
Iteration 182/1000 | Loss: 0.00002892
Iteration 183/1000 | Loss: 0.00002892
Iteration 184/1000 | Loss: 0.00002892
Iteration 185/1000 | Loss: 0.00002892
Iteration 186/1000 | Loss: 0.00002892
Iteration 187/1000 | Loss: 0.00002892
Iteration 188/1000 | Loss: 0.00002891
Iteration 189/1000 | Loss: 0.00002891
Iteration 190/1000 | Loss: 0.00002891
Iteration 191/1000 | Loss: 0.00002891
Iteration 192/1000 | Loss: 0.00002891
Iteration 193/1000 | Loss: 0.00002891
Iteration 194/1000 | Loss: 0.00002891
Iteration 195/1000 | Loss: 0.00002891
Iteration 196/1000 | Loss: 0.00002891
Iteration 197/1000 | Loss: 0.00002891
Iteration 198/1000 | Loss: 0.00002891
Iteration 199/1000 | Loss: 0.00002891
Iteration 200/1000 | Loss: 0.00002891
Iteration 201/1000 | Loss: 0.00002891
Iteration 202/1000 | Loss: 0.00002891
Iteration 203/1000 | Loss: 0.00002891
Iteration 204/1000 | Loss: 0.00002891
Iteration 205/1000 | Loss: 0.00002891
Iteration 206/1000 | Loss: 0.00002891
Iteration 207/1000 | Loss: 0.00002891
Iteration 208/1000 | Loss: 0.00002891
Iteration 209/1000 | Loss: 0.00002891
Iteration 210/1000 | Loss: 0.00002891
Iteration 211/1000 | Loss: 0.00002891
Iteration 212/1000 | Loss: 0.00002891
Iteration 213/1000 | Loss: 0.00002891
Iteration 214/1000 | Loss: 0.00002891
Iteration 215/1000 | Loss: 0.00002891
Iteration 216/1000 | Loss: 0.00002891
Iteration 217/1000 | Loss: 0.00002891
Iteration 218/1000 | Loss: 0.00002891
Iteration 219/1000 | Loss: 0.00002891
Iteration 220/1000 | Loss: 0.00002891
Iteration 221/1000 | Loss: 0.00002891
Iteration 222/1000 | Loss: 0.00002891
Iteration 223/1000 | Loss: 0.00002891
Iteration 224/1000 | Loss: 0.00002891
Iteration 225/1000 | Loss: 0.00002891
Iteration 226/1000 | Loss: 0.00002891
Iteration 227/1000 | Loss: 0.00002891
Iteration 228/1000 | Loss: 0.00002891
Iteration 229/1000 | Loss: 0.00002891
Iteration 230/1000 | Loss: 0.00002891
Iteration 231/1000 | Loss: 0.00002891
Iteration 232/1000 | Loss: 0.00002891
Iteration 233/1000 | Loss: 0.00002891
Iteration 234/1000 | Loss: 0.00002891
Iteration 235/1000 | Loss: 0.00002891
Iteration 236/1000 | Loss: 0.00002891
Iteration 237/1000 | Loss: 0.00002891
Iteration 238/1000 | Loss: 0.00002891
Iteration 239/1000 | Loss: 0.00002891
Iteration 240/1000 | Loss: 0.00002891
Iteration 241/1000 | Loss: 0.00002891
Iteration 242/1000 | Loss: 0.00002891
Iteration 243/1000 | Loss: 0.00002891
Iteration 244/1000 | Loss: 0.00002891
Iteration 245/1000 | Loss: 0.00002891
Iteration 246/1000 | Loss: 0.00002891
Iteration 247/1000 | Loss: 0.00002891
Iteration 248/1000 | Loss: 0.00002891
Iteration 249/1000 | Loss: 0.00002891
Iteration 250/1000 | Loss: 0.00002891
Iteration 251/1000 | Loss: 0.00002891
Iteration 252/1000 | Loss: 0.00002891
Iteration 253/1000 | Loss: 0.00002891
Iteration 254/1000 | Loss: 0.00002891
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 254. Stopping optimization.
Last 5 losses: [2.8906451916554943e-05, 2.8906451916554943e-05, 2.8906451916554943e-05, 2.8906451916554943e-05, 2.8906451916554943e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.8906451916554943e-05

Optimization complete. Final v2v error: 4.479430198669434 mm

Highest mean error: 4.806358814239502 mm for frame 89

Lowest mean error: 4.047452449798584 mm for frame 72

Saving results

Total time: 40.87891459465027
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1063/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1063.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1063
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01061327
Iteration 2/25 | Loss: 0.01061327
Iteration 3/25 | Loss: 0.01061327
Iteration 4/25 | Loss: 0.01061327
Iteration 5/25 | Loss: 0.01061326
Iteration 6/25 | Loss: 0.01061326
Iteration 7/25 | Loss: 0.01061326
Iteration 8/25 | Loss: 0.01061326
Iteration 9/25 | Loss: 0.01061326
Iteration 10/25 | Loss: 0.01061326
Iteration 11/25 | Loss: 0.01061326
Iteration 12/25 | Loss: 0.01061326
Iteration 13/25 | Loss: 0.01061326
Iteration 14/25 | Loss: 0.01061326
Iteration 15/25 | Loss: 0.01061326
Iteration 16/25 | Loss: 0.01061326
Iteration 17/25 | Loss: 0.01061326
Iteration 18/25 | Loss: 0.01061325
Iteration 19/25 | Loss: 0.01061325
Iteration 20/25 | Loss: 0.01061325
Iteration 21/25 | Loss: 0.01061325
Iteration 22/25 | Loss: 0.01061325
Iteration 23/25 | Loss: 0.01061325
Iteration 24/25 | Loss: 0.01061325
Iteration 25/25 | Loss: 0.01061325

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.91006374
Iteration 2/25 | Loss: 0.10743590
Iteration 3/25 | Loss: 0.10637169
Iteration 4/25 | Loss: 0.10637167
Iteration 5/25 | Loss: 0.10637166
Iteration 6/25 | Loss: 0.10637166
Iteration 7/25 | Loss: 0.10637166
Iteration 8/25 | Loss: 0.10637166
Iteration 9/25 | Loss: 0.10637166
Iteration 10/25 | Loss: 0.10637166
Iteration 11/25 | Loss: 0.10637166
Iteration 12/25 | Loss: 0.10637166
Iteration 13/25 | Loss: 0.10637166
Iteration 14/25 | Loss: 0.10637166
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.10637165606021881, 0.10637165606021881, 0.10637165606021881, 0.10637165606021881, 0.10637165606021881]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.10637165606021881

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.10637166
Iteration 2/1000 | Loss: 0.00494793
Iteration 3/1000 | Loss: 0.00337570
Iteration 4/1000 | Loss: 0.00095923
Iteration 5/1000 | Loss: 0.00082095
Iteration 6/1000 | Loss: 0.00022680
Iteration 7/1000 | Loss: 0.00016216
Iteration 8/1000 | Loss: 0.00007256
Iteration 9/1000 | Loss: 0.00015526
Iteration 10/1000 | Loss: 0.00038811
Iteration 11/1000 | Loss: 0.00094330
Iteration 12/1000 | Loss: 0.00452790
Iteration 13/1000 | Loss: 0.00007065
Iteration 14/1000 | Loss: 0.00012962
Iteration 15/1000 | Loss: 0.00053002
Iteration 16/1000 | Loss: 0.00277269
Iteration 17/1000 | Loss: 0.00103253
Iteration 18/1000 | Loss: 0.00082693
Iteration 19/1000 | Loss: 0.00008067
Iteration 20/1000 | Loss: 0.00009522
Iteration 21/1000 | Loss: 0.00004402
Iteration 22/1000 | Loss: 0.00010879
Iteration 23/1000 | Loss: 0.00027081
Iteration 24/1000 | Loss: 0.00012205
Iteration 25/1000 | Loss: 0.00003302
Iteration 26/1000 | Loss: 0.00030933
Iteration 27/1000 | Loss: 0.00036293
Iteration 28/1000 | Loss: 0.00002585
Iteration 29/1000 | Loss: 0.00010230
Iteration 30/1000 | Loss: 0.00045012
Iteration 31/1000 | Loss: 0.00093863
Iteration 32/1000 | Loss: 0.00358876
Iteration 33/1000 | Loss: 0.00053869
Iteration 34/1000 | Loss: 0.00020990
Iteration 35/1000 | Loss: 0.00006367
Iteration 36/1000 | Loss: 0.00002975
Iteration 37/1000 | Loss: 0.00017359
Iteration 38/1000 | Loss: 0.00004325
Iteration 39/1000 | Loss: 0.00009759
Iteration 40/1000 | Loss: 0.00007240
Iteration 41/1000 | Loss: 0.00004481
Iteration 42/1000 | Loss: 0.00014525
Iteration 43/1000 | Loss: 0.00002650
Iteration 44/1000 | Loss: 0.00003432
Iteration 45/1000 | Loss: 0.00004717
Iteration 46/1000 | Loss: 0.00005284
Iteration 47/1000 | Loss: 0.00002524
Iteration 48/1000 | Loss: 0.00003266
Iteration 49/1000 | Loss: 0.00012935
Iteration 50/1000 | Loss: 0.00002404
Iteration 51/1000 | Loss: 0.00005450
Iteration 52/1000 | Loss: 0.00007450
Iteration 53/1000 | Loss: 0.00004296
Iteration 54/1000 | Loss: 0.00002778
Iteration 55/1000 | Loss: 0.00003630
Iteration 56/1000 | Loss: 0.00002165
Iteration 57/1000 | Loss: 0.00005794
Iteration 58/1000 | Loss: 0.00002181
Iteration 59/1000 | Loss: 0.00002294
Iteration 60/1000 | Loss: 0.00007175
Iteration 61/1000 | Loss: 0.00002953
Iteration 62/1000 | Loss: 0.00002654
Iteration 63/1000 | Loss: 0.00002654
Iteration 64/1000 | Loss: 0.00064080
Iteration 65/1000 | Loss: 0.00038603
Iteration 66/1000 | Loss: 0.00011940
Iteration 67/1000 | Loss: 0.00008093
Iteration 68/1000 | Loss: 0.00011905
Iteration 69/1000 | Loss: 0.00006132
Iteration 70/1000 | Loss: 0.00053442
Iteration 71/1000 | Loss: 0.00003051
Iteration 72/1000 | Loss: 0.00002655
Iteration 73/1000 | Loss: 0.00003177
Iteration 74/1000 | Loss: 0.00002261
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001751
Iteration 77/1000 | Loss: 0.00001750
Iteration 78/1000 | Loss: 0.00003288
Iteration 79/1000 | Loss: 0.00001991
Iteration 80/1000 | Loss: 0.00002434
Iteration 81/1000 | Loss: 0.00001764
Iteration 82/1000 | Loss: 0.00002179
Iteration 83/1000 | Loss: 0.00002816
Iteration 84/1000 | Loss: 0.00001761
Iteration 85/1000 | Loss: 0.00001746
Iteration 86/1000 | Loss: 0.00002251
Iteration 87/1000 | Loss: 0.00001741
Iteration 88/1000 | Loss: 0.00001741
Iteration 89/1000 | Loss: 0.00001741
Iteration 90/1000 | Loss: 0.00001741
Iteration 91/1000 | Loss: 0.00001741
Iteration 92/1000 | Loss: 0.00001741
Iteration 93/1000 | Loss: 0.00001740
Iteration 94/1000 | Loss: 0.00001740
Iteration 95/1000 | Loss: 0.00001740
Iteration 96/1000 | Loss: 0.00001739
Iteration 97/1000 | Loss: 0.00001739
Iteration 98/1000 | Loss: 0.00001739
Iteration 99/1000 | Loss: 0.00001779
Iteration 100/1000 | Loss: 0.00001787
Iteration 101/1000 | Loss: 0.00001786
Iteration 102/1000 | Loss: 0.00002704
Iteration 103/1000 | Loss: 0.00004289
Iteration 104/1000 | Loss: 0.00002775
Iteration 105/1000 | Loss: 0.00002132
Iteration 106/1000 | Loss: 0.00001739
Iteration 107/1000 | Loss: 0.00001739
Iteration 108/1000 | Loss: 0.00001739
Iteration 109/1000 | Loss: 0.00001739
Iteration 110/1000 | Loss: 0.00002194
Iteration 111/1000 | Loss: 0.00002107
Iteration 112/1000 | Loss: 0.00002286
Iteration 113/1000 | Loss: 0.00001811
Iteration 114/1000 | Loss: 0.00001770
Iteration 115/1000 | Loss: 0.00001819
Iteration 116/1000 | Loss: 0.00001846
Iteration 117/1000 | Loss: 0.00001765
Iteration 118/1000 | Loss: 0.00001789
Iteration 119/1000 | Loss: 0.00001752
Iteration 120/1000 | Loss: 0.00001737
Iteration 121/1000 | Loss: 0.00001737
Iteration 122/1000 | Loss: 0.00001737
Iteration 123/1000 | Loss: 0.00001737
Iteration 124/1000 | Loss: 0.00001737
Iteration 125/1000 | Loss: 0.00001737
Iteration 126/1000 | Loss: 0.00001737
Iteration 127/1000 | Loss: 0.00001737
Iteration 128/1000 | Loss: 0.00001737
Iteration 129/1000 | Loss: 0.00001737
Iteration 130/1000 | Loss: 0.00001737
Iteration 131/1000 | Loss: 0.00001737
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [1.737141610647086e-05, 1.737141610647086e-05, 1.737141610647086e-05, 1.737141610647086e-05, 1.737141610647086e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.737141610647086e-05

Optimization complete. Final v2v error: 3.505075693130493 mm

Highest mean error: 3.610882043838501 mm for frame 188

Lowest mean error: 3.3964486122131348 mm for frame 224

Saving results

Total time: 151.46357703208923
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1095/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1095.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1095
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00891113
Iteration 2/25 | Loss: 0.00090961
Iteration 3/25 | Loss: 0.00076988
Iteration 4/25 | Loss: 0.00074591
Iteration 5/25 | Loss: 0.00074275
Iteration 6/25 | Loss: 0.00074202
Iteration 7/25 | Loss: 0.00074202
Iteration 8/25 | Loss: 0.00074202
Iteration 9/25 | Loss: 0.00074202
Iteration 10/25 | Loss: 0.00074202
Iteration 11/25 | Loss: 0.00074202
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007420230540446937, 0.0007420230540446937, 0.0007420230540446937, 0.0007420230540446937, 0.0007420230540446937]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007420230540446937

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 7.11593342
Iteration 2/25 | Loss: 0.00107080
Iteration 3/25 | Loss: 0.00107080
Iteration 4/25 | Loss: 0.00107080
Iteration 5/25 | Loss: 0.00107080
Iteration 6/25 | Loss: 0.00107080
Iteration 7/25 | Loss: 0.00107080
Iteration 8/25 | Loss: 0.00107080
Iteration 9/25 | Loss: 0.00107080
Iteration 10/25 | Loss: 0.00107080
Iteration 11/25 | Loss: 0.00107080
Iteration 12/25 | Loss: 0.00107080
Iteration 13/25 | Loss: 0.00107080
Iteration 14/25 | Loss: 0.00107080
Iteration 15/25 | Loss: 0.00107080
Iteration 16/25 | Loss: 0.00107080
Iteration 17/25 | Loss: 0.00107080
Iteration 18/25 | Loss: 0.00107080
Iteration 19/25 | Loss: 0.00107080
Iteration 20/25 | Loss: 0.00107080
Iteration 21/25 | Loss: 0.00107080
Iteration 22/25 | Loss: 0.00107080
Iteration 23/25 | Loss: 0.00107080
Iteration 24/25 | Loss: 0.00107080
Iteration 25/25 | Loss: 0.00107080

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107080
Iteration 2/1000 | Loss: 0.00002100
Iteration 3/1000 | Loss: 0.00001648
Iteration 4/1000 | Loss: 0.00001530
Iteration 5/1000 | Loss: 0.00001441
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001380
Iteration 8/1000 | Loss: 0.00001379
Iteration 9/1000 | Loss: 0.00001363
Iteration 10/1000 | Loss: 0.00001362
Iteration 11/1000 | Loss: 0.00001348
Iteration 12/1000 | Loss: 0.00001343
Iteration 13/1000 | Loss: 0.00001341
Iteration 14/1000 | Loss: 0.00001338
Iteration 15/1000 | Loss: 0.00001337
Iteration 16/1000 | Loss: 0.00001337
Iteration 17/1000 | Loss: 0.00001337
Iteration 18/1000 | Loss: 0.00001337
Iteration 19/1000 | Loss: 0.00001336
Iteration 20/1000 | Loss: 0.00001336
Iteration 21/1000 | Loss: 0.00001336
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001334
Iteration 24/1000 | Loss: 0.00001333
Iteration 25/1000 | Loss: 0.00001333
Iteration 26/1000 | Loss: 0.00001332
Iteration 27/1000 | Loss: 0.00001332
Iteration 28/1000 | Loss: 0.00001332
Iteration 29/1000 | Loss: 0.00001331
Iteration 30/1000 | Loss: 0.00001330
Iteration 31/1000 | Loss: 0.00001330
Iteration 32/1000 | Loss: 0.00001330
Iteration 33/1000 | Loss: 0.00001330
Iteration 34/1000 | Loss: 0.00001330
Iteration 35/1000 | Loss: 0.00001330
Iteration 36/1000 | Loss: 0.00001330
Iteration 37/1000 | Loss: 0.00001330
Iteration 38/1000 | Loss: 0.00001330
Iteration 39/1000 | Loss: 0.00001330
Iteration 40/1000 | Loss: 0.00001329
Iteration 41/1000 | Loss: 0.00001329
Iteration 42/1000 | Loss: 0.00001329
Iteration 43/1000 | Loss: 0.00001329
Iteration 44/1000 | Loss: 0.00001329
Iteration 45/1000 | Loss: 0.00001329
Iteration 46/1000 | Loss: 0.00001329
Iteration 47/1000 | Loss: 0.00001329
Iteration 48/1000 | Loss: 0.00001329
Iteration 49/1000 | Loss: 0.00001329
Iteration 50/1000 | Loss: 0.00001329
Iteration 51/1000 | Loss: 0.00001328
Iteration 52/1000 | Loss: 0.00001328
Iteration 53/1000 | Loss: 0.00001328
Iteration 54/1000 | Loss: 0.00001328
Iteration 55/1000 | Loss: 0.00001328
Iteration 56/1000 | Loss: 0.00001328
Iteration 57/1000 | Loss: 0.00001327
Iteration 58/1000 | Loss: 0.00001327
Iteration 59/1000 | Loss: 0.00001327
Iteration 60/1000 | Loss: 0.00001326
Iteration 61/1000 | Loss: 0.00001326
Iteration 62/1000 | Loss: 0.00001326
Iteration 63/1000 | Loss: 0.00001326
Iteration 64/1000 | Loss: 0.00001326
Iteration 65/1000 | Loss: 0.00001325
Iteration 66/1000 | Loss: 0.00001325
Iteration 67/1000 | Loss: 0.00001325
Iteration 68/1000 | Loss: 0.00001324
Iteration 69/1000 | Loss: 0.00001324
Iteration 70/1000 | Loss: 0.00001324
Iteration 71/1000 | Loss: 0.00001324
Iteration 72/1000 | Loss: 0.00001324
Iteration 73/1000 | Loss: 0.00001324
Iteration 74/1000 | Loss: 0.00001324
Iteration 75/1000 | Loss: 0.00001324
Iteration 76/1000 | Loss: 0.00001323
Iteration 77/1000 | Loss: 0.00001323
Iteration 78/1000 | Loss: 0.00001323
Iteration 79/1000 | Loss: 0.00001323
Iteration 80/1000 | Loss: 0.00001323
Iteration 81/1000 | Loss: 0.00001322
Iteration 82/1000 | Loss: 0.00001322
Iteration 83/1000 | Loss: 0.00001322
Iteration 84/1000 | Loss: 0.00001322
Iteration 85/1000 | Loss: 0.00001322
Iteration 86/1000 | Loss: 0.00001322
Iteration 87/1000 | Loss: 0.00001322
Iteration 88/1000 | Loss: 0.00001322
Iteration 89/1000 | Loss: 0.00001321
Iteration 90/1000 | Loss: 0.00001321
Iteration 91/1000 | Loss: 0.00001321
Iteration 92/1000 | Loss: 0.00001321
Iteration 93/1000 | Loss: 0.00001321
Iteration 94/1000 | Loss: 0.00001320
Iteration 95/1000 | Loss: 0.00001320
Iteration 96/1000 | Loss: 0.00001320
Iteration 97/1000 | Loss: 0.00001320
Iteration 98/1000 | Loss: 0.00001320
Iteration 99/1000 | Loss: 0.00001320
Iteration 100/1000 | Loss: 0.00001320
Iteration 101/1000 | Loss: 0.00001320
Iteration 102/1000 | Loss: 0.00001319
Iteration 103/1000 | Loss: 0.00001319
Iteration 104/1000 | Loss: 0.00001319
Iteration 105/1000 | Loss: 0.00001318
Iteration 106/1000 | Loss: 0.00001318
Iteration 107/1000 | Loss: 0.00001318
Iteration 108/1000 | Loss: 0.00001318
Iteration 109/1000 | Loss: 0.00001318
Iteration 110/1000 | Loss: 0.00001317
Iteration 111/1000 | Loss: 0.00001317
Iteration 112/1000 | Loss: 0.00001317
Iteration 113/1000 | Loss: 0.00001317
Iteration 114/1000 | Loss: 0.00001317
Iteration 115/1000 | Loss: 0.00001317
Iteration 116/1000 | Loss: 0.00001317
Iteration 117/1000 | Loss: 0.00001317
Iteration 118/1000 | Loss: 0.00001317
Iteration 119/1000 | Loss: 0.00001317
Iteration 120/1000 | Loss: 0.00001316
Iteration 121/1000 | Loss: 0.00001316
Iteration 122/1000 | Loss: 0.00001316
Iteration 123/1000 | Loss: 0.00001316
Iteration 124/1000 | Loss: 0.00001315
Iteration 125/1000 | Loss: 0.00001315
Iteration 126/1000 | Loss: 0.00001315
Iteration 127/1000 | Loss: 0.00001315
Iteration 128/1000 | Loss: 0.00001315
Iteration 129/1000 | Loss: 0.00001315
Iteration 130/1000 | Loss: 0.00001315
Iteration 131/1000 | Loss: 0.00001315
Iteration 132/1000 | Loss: 0.00001315
Iteration 133/1000 | Loss: 0.00001314
Iteration 134/1000 | Loss: 0.00001314
Iteration 135/1000 | Loss: 0.00001314
Iteration 136/1000 | Loss: 0.00001314
Iteration 137/1000 | Loss: 0.00001314
Iteration 138/1000 | Loss: 0.00001314
Iteration 139/1000 | Loss: 0.00001313
Iteration 140/1000 | Loss: 0.00001313
Iteration 141/1000 | Loss: 0.00001313
Iteration 142/1000 | Loss: 0.00001313
Iteration 143/1000 | Loss: 0.00001312
Iteration 144/1000 | Loss: 0.00001312
Iteration 145/1000 | Loss: 0.00001312
Iteration 146/1000 | Loss: 0.00001312
Iteration 147/1000 | Loss: 0.00001311
Iteration 148/1000 | Loss: 0.00001311
Iteration 149/1000 | Loss: 0.00001311
Iteration 150/1000 | Loss: 0.00001311
Iteration 151/1000 | Loss: 0.00001310
Iteration 152/1000 | Loss: 0.00001310
Iteration 153/1000 | Loss: 0.00001310
Iteration 154/1000 | Loss: 0.00001310
Iteration 155/1000 | Loss: 0.00001310
Iteration 156/1000 | Loss: 0.00001310
Iteration 157/1000 | Loss: 0.00001310
Iteration 158/1000 | Loss: 0.00001310
Iteration 159/1000 | Loss: 0.00001310
Iteration 160/1000 | Loss: 0.00001310
Iteration 161/1000 | Loss: 0.00001310
Iteration 162/1000 | Loss: 0.00001310
Iteration 163/1000 | Loss: 0.00001310
Iteration 164/1000 | Loss: 0.00001310
Iteration 165/1000 | Loss: 0.00001310
Iteration 166/1000 | Loss: 0.00001310
Iteration 167/1000 | Loss: 0.00001310
Iteration 168/1000 | Loss: 0.00001309
Iteration 169/1000 | Loss: 0.00001309
Iteration 170/1000 | Loss: 0.00001309
Iteration 171/1000 | Loss: 0.00001309
Iteration 172/1000 | Loss: 0.00001309
Iteration 173/1000 | Loss: 0.00001309
Iteration 174/1000 | Loss: 0.00001309
Iteration 175/1000 | Loss: 0.00001308
Iteration 176/1000 | Loss: 0.00001308
Iteration 177/1000 | Loss: 0.00001308
Iteration 178/1000 | Loss: 0.00001308
Iteration 179/1000 | Loss: 0.00001308
Iteration 180/1000 | Loss: 0.00001308
Iteration 181/1000 | Loss: 0.00001308
Iteration 182/1000 | Loss: 0.00001308
Iteration 183/1000 | Loss: 0.00001308
Iteration 184/1000 | Loss: 0.00001308
Iteration 185/1000 | Loss: 0.00001308
Iteration 186/1000 | Loss: 0.00001308
Iteration 187/1000 | Loss: 0.00001308
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 187. Stopping optimization.
Last 5 losses: [1.3078290976409335e-05, 1.3078290976409335e-05, 1.3078290976409335e-05, 1.3078290976409335e-05, 1.3078290976409335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3078290976409335e-05

Optimization complete. Final v2v error: 3.0615551471710205 mm

Highest mean error: 3.2242095470428467 mm for frame 165

Lowest mean error: 2.944575548171997 mm for frame 182

Saving results

Total time: 38.4147264957428
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1088/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1088.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1088
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746838
Iteration 2/25 | Loss: 0.00156960
Iteration 3/25 | Loss: 0.00106097
Iteration 4/25 | Loss: 0.00092240
Iteration 5/25 | Loss: 0.00090249
Iteration 6/25 | Loss: 0.00089429
Iteration 7/25 | Loss: 0.00088148
Iteration 8/25 | Loss: 0.00087574
Iteration 9/25 | Loss: 0.00087446
Iteration 10/25 | Loss: 0.00087402
Iteration 11/25 | Loss: 0.00087378
Iteration 12/25 | Loss: 0.00087356
Iteration 13/25 | Loss: 0.00087349
Iteration 14/25 | Loss: 0.00087349
Iteration 15/25 | Loss: 0.00087349
Iteration 16/25 | Loss: 0.00087349
Iteration 17/25 | Loss: 0.00087349
Iteration 18/25 | Loss: 0.00087349
Iteration 19/25 | Loss: 0.00087349
Iteration 20/25 | Loss: 0.00087349
Iteration 21/25 | Loss: 0.00087349
Iteration 22/25 | Loss: 0.00087349
Iteration 23/25 | Loss: 0.00087349
Iteration 24/25 | Loss: 0.00087349
Iteration 25/25 | Loss: 0.00087349

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.72580194
Iteration 2/25 | Loss: 0.00147721
Iteration 3/25 | Loss: 0.00147712
Iteration 4/25 | Loss: 0.00147712
Iteration 5/25 | Loss: 0.00147712
Iteration 6/25 | Loss: 0.00147712
Iteration 7/25 | Loss: 0.00147712
Iteration 8/25 | Loss: 0.00147712
Iteration 9/25 | Loss: 0.00147712
Iteration 10/25 | Loss: 0.00147712
Iteration 11/25 | Loss: 0.00147712
Iteration 12/25 | Loss: 0.00147712
Iteration 13/25 | Loss: 0.00147712
Iteration 14/25 | Loss: 0.00147712
Iteration 15/25 | Loss: 0.00147712
Iteration 16/25 | Loss: 0.00147712
Iteration 17/25 | Loss: 0.00147712
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0014771189307793975, 0.0014771189307793975, 0.0014771189307793975, 0.0014771189307793975, 0.0014771189307793975]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014771189307793975

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00147712
Iteration 2/1000 | Loss: 0.00004456
Iteration 3/1000 | Loss: 0.00002797
Iteration 4/1000 | Loss: 0.00002552
Iteration 5/1000 | Loss: 0.00032678
Iteration 6/1000 | Loss: 0.00002722
Iteration 7/1000 | Loss: 0.00002380
Iteration 8/1000 | Loss: 0.00002230
Iteration 9/1000 | Loss: 0.00002120
Iteration 10/1000 | Loss: 0.00002065
Iteration 11/1000 | Loss: 0.00002032
Iteration 12/1000 | Loss: 0.00002011
Iteration 13/1000 | Loss: 0.00001991
Iteration 14/1000 | Loss: 0.00001980
Iteration 15/1000 | Loss: 0.00001976
Iteration 16/1000 | Loss: 0.00001971
Iteration 17/1000 | Loss: 0.00001971
Iteration 18/1000 | Loss: 0.00001970
Iteration 19/1000 | Loss: 0.00001969
Iteration 20/1000 | Loss: 0.00001968
Iteration 21/1000 | Loss: 0.00001963
Iteration 22/1000 | Loss: 0.00001961
Iteration 23/1000 | Loss: 0.00001959
Iteration 24/1000 | Loss: 0.00001956
Iteration 25/1000 | Loss: 0.00001955
Iteration 26/1000 | Loss: 0.00001953
Iteration 27/1000 | Loss: 0.00001952
Iteration 28/1000 | Loss: 0.00001951
Iteration 29/1000 | Loss: 0.00001948
Iteration 30/1000 | Loss: 0.00001948
Iteration 31/1000 | Loss: 0.00001948
Iteration 32/1000 | Loss: 0.00001948
Iteration 33/1000 | Loss: 0.00001947
Iteration 34/1000 | Loss: 0.00001947
Iteration 35/1000 | Loss: 0.00001947
Iteration 36/1000 | Loss: 0.00001947
Iteration 37/1000 | Loss: 0.00001946
Iteration 38/1000 | Loss: 0.00001946
Iteration 39/1000 | Loss: 0.00001945
Iteration 40/1000 | Loss: 0.00001945
Iteration 41/1000 | Loss: 0.00001945
Iteration 42/1000 | Loss: 0.00001945
Iteration 43/1000 | Loss: 0.00001945
Iteration 44/1000 | Loss: 0.00001945
Iteration 45/1000 | Loss: 0.00001945
Iteration 46/1000 | Loss: 0.00001944
Iteration 47/1000 | Loss: 0.00001944
Iteration 48/1000 | Loss: 0.00001944
Iteration 49/1000 | Loss: 0.00001944
Iteration 50/1000 | Loss: 0.00001944
Iteration 51/1000 | Loss: 0.00001944
Iteration 52/1000 | Loss: 0.00001944
Iteration 53/1000 | Loss: 0.00001944
Iteration 54/1000 | Loss: 0.00001944
Iteration 55/1000 | Loss: 0.00001944
Iteration 56/1000 | Loss: 0.00001944
Iteration 57/1000 | Loss: 0.00001943
Iteration 58/1000 | Loss: 0.00001943
Iteration 59/1000 | Loss: 0.00001943
Iteration 60/1000 | Loss: 0.00001942
Iteration 61/1000 | Loss: 0.00001942
Iteration 62/1000 | Loss: 0.00001942
Iteration 63/1000 | Loss: 0.00001942
Iteration 64/1000 | Loss: 0.00001942
Iteration 65/1000 | Loss: 0.00001942
Iteration 66/1000 | Loss: 0.00001942
Iteration 67/1000 | Loss: 0.00001942
Iteration 68/1000 | Loss: 0.00001942
Iteration 69/1000 | Loss: 0.00001941
Iteration 70/1000 | Loss: 0.00001941
Iteration 71/1000 | Loss: 0.00001941
Iteration 72/1000 | Loss: 0.00001941
Iteration 73/1000 | Loss: 0.00001941
Iteration 74/1000 | Loss: 0.00001941
Iteration 75/1000 | Loss: 0.00001941
Iteration 76/1000 | Loss: 0.00001940
Iteration 77/1000 | Loss: 0.00001940
Iteration 78/1000 | Loss: 0.00001940
Iteration 79/1000 | Loss: 0.00001940
Iteration 80/1000 | Loss: 0.00001940
Iteration 81/1000 | Loss: 0.00001939
Iteration 82/1000 | Loss: 0.00001939
Iteration 83/1000 | Loss: 0.00001939
Iteration 84/1000 | Loss: 0.00001939
Iteration 85/1000 | Loss: 0.00001939
Iteration 86/1000 | Loss: 0.00001939
Iteration 87/1000 | Loss: 0.00001939
Iteration 88/1000 | Loss: 0.00001939
Iteration 89/1000 | Loss: 0.00001939
Iteration 90/1000 | Loss: 0.00001938
Iteration 91/1000 | Loss: 0.00001938
Iteration 92/1000 | Loss: 0.00001938
Iteration 93/1000 | Loss: 0.00001938
Iteration 94/1000 | Loss: 0.00001938
Iteration 95/1000 | Loss: 0.00001938
Iteration 96/1000 | Loss: 0.00001938
Iteration 97/1000 | Loss: 0.00001938
Iteration 98/1000 | Loss: 0.00001937
Iteration 99/1000 | Loss: 0.00001937
Iteration 100/1000 | Loss: 0.00001937
Iteration 101/1000 | Loss: 0.00001937
Iteration 102/1000 | Loss: 0.00001937
Iteration 103/1000 | Loss: 0.00001937
Iteration 104/1000 | Loss: 0.00001937
Iteration 105/1000 | Loss: 0.00001937
Iteration 106/1000 | Loss: 0.00001936
Iteration 107/1000 | Loss: 0.00001936
Iteration 108/1000 | Loss: 0.00001936
Iteration 109/1000 | Loss: 0.00001936
Iteration 110/1000 | Loss: 0.00001936
Iteration 111/1000 | Loss: 0.00001936
Iteration 112/1000 | Loss: 0.00001936
Iteration 113/1000 | Loss: 0.00001936
Iteration 114/1000 | Loss: 0.00001935
Iteration 115/1000 | Loss: 0.00001935
Iteration 116/1000 | Loss: 0.00001935
Iteration 117/1000 | Loss: 0.00001935
Iteration 118/1000 | Loss: 0.00001935
Iteration 119/1000 | Loss: 0.00001935
Iteration 120/1000 | Loss: 0.00001935
Iteration 121/1000 | Loss: 0.00001935
Iteration 122/1000 | Loss: 0.00001935
Iteration 123/1000 | Loss: 0.00001935
Iteration 124/1000 | Loss: 0.00001935
Iteration 125/1000 | Loss: 0.00001935
Iteration 126/1000 | Loss: 0.00001935
Iteration 127/1000 | Loss: 0.00001935
Iteration 128/1000 | Loss: 0.00001935
Iteration 129/1000 | Loss: 0.00001935
Iteration 130/1000 | Loss: 0.00001934
Iteration 131/1000 | Loss: 0.00001934
Iteration 132/1000 | Loss: 0.00001934
Iteration 133/1000 | Loss: 0.00001934
Iteration 134/1000 | Loss: 0.00001934
Iteration 135/1000 | Loss: 0.00001934
Iteration 136/1000 | Loss: 0.00001934
Iteration 137/1000 | Loss: 0.00001934
Iteration 138/1000 | Loss: 0.00001934
Iteration 139/1000 | Loss: 0.00001934
Iteration 140/1000 | Loss: 0.00001934
Iteration 141/1000 | Loss: 0.00001934
Iteration 142/1000 | Loss: 0.00001934
Iteration 143/1000 | Loss: 0.00001934
Iteration 144/1000 | Loss: 0.00001934
Iteration 145/1000 | Loss: 0.00001934
Iteration 146/1000 | Loss: 0.00001934
Iteration 147/1000 | Loss: 0.00001934
Iteration 148/1000 | Loss: 0.00001934
Iteration 149/1000 | Loss: 0.00001934
Iteration 150/1000 | Loss: 0.00001934
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.934324245667085e-05, 1.934324245667085e-05, 1.934324245667085e-05, 1.934324245667085e-05, 1.934324245667085e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.934324245667085e-05

Optimization complete. Final v2v error: 3.6434214115142822 mm

Highest mean error: 4.579300880432129 mm for frame 115

Lowest mean error: 2.91361927986145 mm for frame 235

Saving results

Total time: 59.13900351524353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1096/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1096.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1096
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00522880
Iteration 2/25 | Loss: 0.00101942
Iteration 3/25 | Loss: 0.00082785
Iteration 4/25 | Loss: 0.00079807
Iteration 5/25 | Loss: 0.00079380
Iteration 6/25 | Loss: 0.00079319
Iteration 7/25 | Loss: 0.00079319
Iteration 8/25 | Loss: 0.00079319
Iteration 9/25 | Loss: 0.00079319
Iteration 10/25 | Loss: 0.00079319
Iteration 11/25 | Loss: 0.00079319
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0007931904983706772, 0.0007931904983706772, 0.0007931904983706772, 0.0007931904983706772, 0.0007931904983706772]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007931904983706772

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.94027573
Iteration 2/25 | Loss: 0.00092300
Iteration 3/25 | Loss: 0.00092299
Iteration 4/25 | Loss: 0.00092299
Iteration 5/25 | Loss: 0.00092299
Iteration 6/25 | Loss: 0.00092299
Iteration 7/25 | Loss: 0.00092298
Iteration 8/25 | Loss: 0.00092298
Iteration 9/25 | Loss: 0.00092298
Iteration 10/25 | Loss: 0.00092298
Iteration 11/25 | Loss: 0.00092298
Iteration 12/25 | Loss: 0.00092298
Iteration 13/25 | Loss: 0.00092298
Iteration 14/25 | Loss: 0.00092298
Iteration 15/25 | Loss: 0.00092298
Iteration 16/25 | Loss: 0.00092298
Iteration 17/25 | Loss: 0.00092298
Iteration 18/25 | Loss: 0.00092298
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000922984501812607, 0.000922984501812607, 0.000922984501812607, 0.000922984501812607, 0.000922984501812607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000922984501812607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00092298
Iteration 2/1000 | Loss: 0.00002523
Iteration 3/1000 | Loss: 0.00001825
Iteration 4/1000 | Loss: 0.00001739
Iteration 5/1000 | Loss: 0.00001672
Iteration 6/1000 | Loss: 0.00001620
Iteration 7/1000 | Loss: 0.00001571
Iteration 8/1000 | Loss: 0.00001554
Iteration 9/1000 | Loss: 0.00001552
Iteration 10/1000 | Loss: 0.00001533
Iteration 11/1000 | Loss: 0.00001513
Iteration 12/1000 | Loss: 0.00001492
Iteration 13/1000 | Loss: 0.00001484
Iteration 14/1000 | Loss: 0.00001483
Iteration 15/1000 | Loss: 0.00001476
Iteration 16/1000 | Loss: 0.00001474
Iteration 17/1000 | Loss: 0.00001474
Iteration 18/1000 | Loss: 0.00001474
Iteration 19/1000 | Loss: 0.00001474
Iteration 20/1000 | Loss: 0.00001473
Iteration 21/1000 | Loss: 0.00001473
Iteration 22/1000 | Loss: 0.00001473
Iteration 23/1000 | Loss: 0.00001473
Iteration 24/1000 | Loss: 0.00001473
Iteration 25/1000 | Loss: 0.00001473
Iteration 26/1000 | Loss: 0.00001473
Iteration 27/1000 | Loss: 0.00001473
Iteration 28/1000 | Loss: 0.00001473
Iteration 29/1000 | Loss: 0.00001473
Iteration 30/1000 | Loss: 0.00001472
Iteration 31/1000 | Loss: 0.00001472
Iteration 32/1000 | Loss: 0.00001472
Iteration 33/1000 | Loss: 0.00001472
Iteration 34/1000 | Loss: 0.00001472
Iteration 35/1000 | Loss: 0.00001470
Iteration 36/1000 | Loss: 0.00001469
Iteration 37/1000 | Loss: 0.00001469
Iteration 38/1000 | Loss: 0.00001467
Iteration 39/1000 | Loss: 0.00001467
Iteration 40/1000 | Loss: 0.00001467
Iteration 41/1000 | Loss: 0.00001466
Iteration 42/1000 | Loss: 0.00001466
Iteration 43/1000 | Loss: 0.00001466
Iteration 44/1000 | Loss: 0.00001465
Iteration 45/1000 | Loss: 0.00001465
Iteration 46/1000 | Loss: 0.00001465
Iteration 47/1000 | Loss: 0.00001465
Iteration 48/1000 | Loss: 0.00001464
Iteration 49/1000 | Loss: 0.00001464
Iteration 50/1000 | Loss: 0.00001463
Iteration 51/1000 | Loss: 0.00001463
Iteration 52/1000 | Loss: 0.00001463
Iteration 53/1000 | Loss: 0.00001463
Iteration 54/1000 | Loss: 0.00001463
Iteration 55/1000 | Loss: 0.00001463
Iteration 56/1000 | Loss: 0.00001463
Iteration 57/1000 | Loss: 0.00001463
Iteration 58/1000 | Loss: 0.00001463
Iteration 59/1000 | Loss: 0.00001463
Iteration 60/1000 | Loss: 0.00001462
Iteration 61/1000 | Loss: 0.00001462
Iteration 62/1000 | Loss: 0.00001462
Iteration 63/1000 | Loss: 0.00001462
Iteration 64/1000 | Loss: 0.00001462
Iteration 65/1000 | Loss: 0.00001461
Iteration 66/1000 | Loss: 0.00001461
Iteration 67/1000 | Loss: 0.00001461
Iteration 68/1000 | Loss: 0.00001461
Iteration 69/1000 | Loss: 0.00001461
Iteration 70/1000 | Loss: 0.00001460
Iteration 71/1000 | Loss: 0.00001460
Iteration 72/1000 | Loss: 0.00001460
Iteration 73/1000 | Loss: 0.00001460
Iteration 74/1000 | Loss: 0.00001460
Iteration 75/1000 | Loss: 0.00001460
Iteration 76/1000 | Loss: 0.00001460
Iteration 77/1000 | Loss: 0.00001460
Iteration 78/1000 | Loss: 0.00001460
Iteration 79/1000 | Loss: 0.00001459
Iteration 80/1000 | Loss: 0.00001459
Iteration 81/1000 | Loss: 0.00001459
Iteration 82/1000 | Loss: 0.00001459
Iteration 83/1000 | Loss: 0.00001458
Iteration 84/1000 | Loss: 0.00001458
Iteration 85/1000 | Loss: 0.00001458
Iteration 86/1000 | Loss: 0.00001458
Iteration 87/1000 | Loss: 0.00001458
Iteration 88/1000 | Loss: 0.00001458
Iteration 89/1000 | Loss: 0.00001457
Iteration 90/1000 | Loss: 0.00001457
Iteration 91/1000 | Loss: 0.00001457
Iteration 92/1000 | Loss: 0.00001457
Iteration 93/1000 | Loss: 0.00001457
Iteration 94/1000 | Loss: 0.00001456
Iteration 95/1000 | Loss: 0.00001456
Iteration 96/1000 | Loss: 0.00001456
Iteration 97/1000 | Loss: 0.00001456
Iteration 98/1000 | Loss: 0.00001456
Iteration 99/1000 | Loss: 0.00001456
Iteration 100/1000 | Loss: 0.00001456
Iteration 101/1000 | Loss: 0.00001456
Iteration 102/1000 | Loss: 0.00001456
Iteration 103/1000 | Loss: 0.00001456
Iteration 104/1000 | Loss: 0.00001456
Iteration 105/1000 | Loss: 0.00001456
Iteration 106/1000 | Loss: 0.00001456
Iteration 107/1000 | Loss: 0.00001456
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 114. Stopping optimization.
Last 5 losses: [1.4557666872860864e-05, 1.4557666872860864e-05, 1.4557666872860864e-05, 1.4557666872860864e-05, 1.4557666872860864e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4557666872860864e-05

Optimization complete. Final v2v error: 3.2578344345092773 mm

Highest mean error: 3.321963310241699 mm for frame 0

Lowest mean error: 3.182677745819092 mm for frame 263

Saving results

Total time: 36.74136281013489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1076/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1076.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1076
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01056090
Iteration 2/25 | Loss: 0.00204183
Iteration 3/25 | Loss: 0.00128385
Iteration 4/25 | Loss: 0.00112500
Iteration 5/25 | Loss: 0.00095052
Iteration 6/25 | Loss: 0.00093598
Iteration 7/25 | Loss: 0.00086499
Iteration 8/25 | Loss: 0.00085682
Iteration 9/25 | Loss: 0.00085387
Iteration 10/25 | Loss: 0.00085370
Iteration 11/25 | Loss: 0.00085088
Iteration 12/25 | Loss: 0.00084344
Iteration 13/25 | Loss: 0.00085146
Iteration 14/25 | Loss: 0.00084246
Iteration 15/25 | Loss: 0.00083884
Iteration 16/25 | Loss: 0.00083277
Iteration 17/25 | Loss: 0.00083420
Iteration 18/25 | Loss: 0.00082647
Iteration 19/25 | Loss: 0.00081831
Iteration 20/25 | Loss: 0.00081305
Iteration 21/25 | Loss: 0.00080546
Iteration 22/25 | Loss: 0.00079958
Iteration 23/25 | Loss: 0.00080188
Iteration 24/25 | Loss: 0.00079796
Iteration 25/25 | Loss: 0.00079600

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60697508
Iteration 2/25 | Loss: 0.00161822
Iteration 3/25 | Loss: 0.00161822
Iteration 4/25 | Loss: 0.00161822
Iteration 5/25 | Loss: 0.00161822
Iteration 6/25 | Loss: 0.00161822
Iteration 7/25 | Loss: 0.00161822
Iteration 8/25 | Loss: 0.00161822
Iteration 9/25 | Loss: 0.00161822
Iteration 10/25 | Loss: 0.00161822
Iteration 11/25 | Loss: 0.00161822
Iteration 12/25 | Loss: 0.00161822
Iteration 13/25 | Loss: 0.00161822
Iteration 14/25 | Loss: 0.00161822
Iteration 15/25 | Loss: 0.00161822
Iteration 16/25 | Loss: 0.00161822
Iteration 17/25 | Loss: 0.00161822
Iteration 18/25 | Loss: 0.00161822
Iteration 19/25 | Loss: 0.00161822
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016182156978175044, 0.0016182156978175044, 0.0016182156978175044, 0.0016182156978175044, 0.0016182156978175044]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016182156978175044

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00161822
Iteration 2/1000 | Loss: 0.00004069
Iteration 3/1000 | Loss: 0.00002880
Iteration 4/1000 | Loss: 0.00007157
Iteration 5/1000 | Loss: 0.00026078
Iteration 6/1000 | Loss: 0.00005438
Iteration 7/1000 | Loss: 0.00002073
Iteration 8/1000 | Loss: 0.00027230
Iteration 9/1000 | Loss: 0.00002777
Iteration 10/1000 | Loss: 0.00002180
Iteration 11/1000 | Loss: 0.00011078
Iteration 12/1000 | Loss: 0.00009650
Iteration 13/1000 | Loss: 0.00004220
Iteration 14/1000 | Loss: 0.00058474
Iteration 15/1000 | Loss: 0.00006457
Iteration 16/1000 | Loss: 0.00004249
Iteration 17/1000 | Loss: 0.00002229
Iteration 18/1000 | Loss: 0.00006908
Iteration 19/1000 | Loss: 0.00002354
Iteration 20/1000 | Loss: 0.00002979
Iteration 21/1000 | Loss: 0.00002270
Iteration 22/1000 | Loss: 0.00002927
Iteration 23/1000 | Loss: 0.00001784
Iteration 24/1000 | Loss: 0.00002997
Iteration 25/1000 | Loss: 0.00014918
Iteration 26/1000 | Loss: 0.00001882
Iteration 27/1000 | Loss: 0.00001767
Iteration 28/1000 | Loss: 0.00001747
Iteration 29/1000 | Loss: 0.00001746
Iteration 30/1000 | Loss: 0.00001746
Iteration 31/1000 | Loss: 0.00001744
Iteration 32/1000 | Loss: 0.00001742
Iteration 33/1000 | Loss: 0.00001742
Iteration 34/1000 | Loss: 0.00008096
Iteration 35/1000 | Loss: 0.00014510
Iteration 36/1000 | Loss: 0.00005759
Iteration 37/1000 | Loss: 0.00006750
Iteration 38/1000 | Loss: 0.00003370
Iteration 39/1000 | Loss: 0.00002523
Iteration 40/1000 | Loss: 0.00002600
Iteration 41/1000 | Loss: 0.00001855
Iteration 42/1000 | Loss: 0.00001926
Iteration 43/1000 | Loss: 0.00003197
Iteration 44/1000 | Loss: 0.00002012
Iteration 45/1000 | Loss: 0.00001727
Iteration 46/1000 | Loss: 0.00001727
Iteration 47/1000 | Loss: 0.00001727
Iteration 48/1000 | Loss: 0.00001727
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001725
Iteration 52/1000 | Loss: 0.00001725
Iteration 53/1000 | Loss: 0.00001724
Iteration 54/1000 | Loss: 0.00001724
Iteration 55/1000 | Loss: 0.00001724
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001723
Iteration 58/1000 | Loss: 0.00001880
Iteration 59/1000 | Loss: 0.00002345
Iteration 60/1000 | Loss: 0.00001812
Iteration 61/1000 | Loss: 0.00001803
Iteration 62/1000 | Loss: 0.00001719
Iteration 63/1000 | Loss: 0.00001718
Iteration 64/1000 | Loss: 0.00001718
Iteration 65/1000 | Loss: 0.00001718
Iteration 66/1000 | Loss: 0.00001718
Iteration 67/1000 | Loss: 0.00001718
Iteration 68/1000 | Loss: 0.00001718
Iteration 69/1000 | Loss: 0.00001718
Iteration 70/1000 | Loss: 0.00001718
Iteration 71/1000 | Loss: 0.00001718
Iteration 72/1000 | Loss: 0.00001718
Iteration 73/1000 | Loss: 0.00001718
Iteration 74/1000 | Loss: 0.00001718
Iteration 75/1000 | Loss: 0.00001718
Iteration 76/1000 | Loss: 0.00001718
Iteration 77/1000 | Loss: 0.00001718
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001718
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [1.7180334907607175e-05, 1.7180334907607175e-05, 1.7180334907607175e-05, 1.7180334907607175e-05, 1.7180334907607175e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7180334907607175e-05

Optimization complete. Final v2v error: 3.490499258041382 mm

Highest mean error: 4.117488384246826 mm for frame 154

Lowest mean error: 3.019407033920288 mm for frame 174

Saving results

Total time: 116.80098056793213
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1075/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1075.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1075
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00625321
Iteration 2/25 | Loss: 0.00091598
Iteration 3/25 | Loss: 0.00077142
Iteration 4/25 | Loss: 0.00075658
Iteration 5/25 | Loss: 0.00075318
Iteration 6/25 | Loss: 0.00075253
Iteration 7/25 | Loss: 0.00075253
Iteration 8/25 | Loss: 0.00075253
Iteration 9/25 | Loss: 0.00075253
Iteration 10/25 | Loss: 0.00075253
Iteration 11/25 | Loss: 0.00075253
Iteration 12/25 | Loss: 0.00075253
Iteration 13/25 | Loss: 0.00075253
Iteration 14/25 | Loss: 0.00075253
Iteration 15/25 | Loss: 0.00075253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007525276741944253, 0.0007525276741944253, 0.0007525276741944253, 0.0007525276741944253, 0.0007525276741944253]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007525276741944253

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.94508553
Iteration 2/25 | Loss: 0.00133173
Iteration 3/25 | Loss: 0.00133169
Iteration 4/25 | Loss: 0.00133169
Iteration 5/25 | Loss: 0.00133169
Iteration 6/25 | Loss: 0.00133169
Iteration 7/25 | Loss: 0.00133169
Iteration 8/25 | Loss: 0.00133169
Iteration 9/25 | Loss: 0.00133169
Iteration 10/25 | Loss: 0.00133169
Iteration 11/25 | Loss: 0.00133169
Iteration 12/25 | Loss: 0.00133169
Iteration 13/25 | Loss: 0.00133169
Iteration 14/25 | Loss: 0.00133169
Iteration 15/25 | Loss: 0.00133169
Iteration 16/25 | Loss: 0.00133169
Iteration 17/25 | Loss: 0.00133169
Iteration 18/25 | Loss: 0.00133169
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013316911645233631, 0.0013316911645233631, 0.0013316911645233631, 0.0013316911645233631, 0.0013316911645233631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013316911645233631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00133169
Iteration 2/1000 | Loss: 0.00002332
Iteration 3/1000 | Loss: 0.00001616
Iteration 4/1000 | Loss: 0.00001488
Iteration 5/1000 | Loss: 0.00001402
Iteration 6/1000 | Loss: 0.00001401
Iteration 7/1000 | Loss: 0.00001367
Iteration 8/1000 | Loss: 0.00001360
Iteration 9/1000 | Loss: 0.00001336
Iteration 10/1000 | Loss: 0.00001316
Iteration 11/1000 | Loss: 0.00001296
Iteration 12/1000 | Loss: 0.00001291
Iteration 13/1000 | Loss: 0.00001291
Iteration 14/1000 | Loss: 0.00001281
Iteration 15/1000 | Loss: 0.00001277
Iteration 16/1000 | Loss: 0.00001277
Iteration 17/1000 | Loss: 0.00001276
Iteration 18/1000 | Loss: 0.00001274
Iteration 19/1000 | Loss: 0.00001272
Iteration 20/1000 | Loss: 0.00001272
Iteration 21/1000 | Loss: 0.00001271
Iteration 22/1000 | Loss: 0.00001271
Iteration 23/1000 | Loss: 0.00001271
Iteration 24/1000 | Loss: 0.00001271
Iteration 25/1000 | Loss: 0.00001271
Iteration 26/1000 | Loss: 0.00001271
Iteration 27/1000 | Loss: 0.00001271
Iteration 28/1000 | Loss: 0.00001271
Iteration 29/1000 | Loss: 0.00001271
Iteration 30/1000 | Loss: 0.00001271
Iteration 31/1000 | Loss: 0.00001271
Iteration 32/1000 | Loss: 0.00001270
Iteration 33/1000 | Loss: 0.00001270
Iteration 34/1000 | Loss: 0.00001270
Iteration 35/1000 | Loss: 0.00001269
Iteration 36/1000 | Loss: 0.00001269
Iteration 37/1000 | Loss: 0.00001269
Iteration 38/1000 | Loss: 0.00001268
Iteration 39/1000 | Loss: 0.00001268
Iteration 40/1000 | Loss: 0.00001268
Iteration 41/1000 | Loss: 0.00001268
Iteration 42/1000 | Loss: 0.00001268
Iteration 43/1000 | Loss: 0.00001268
Iteration 44/1000 | Loss: 0.00001268
Iteration 45/1000 | Loss: 0.00001268
Iteration 46/1000 | Loss: 0.00001268
Iteration 47/1000 | Loss: 0.00001268
Iteration 48/1000 | Loss: 0.00001268
Iteration 49/1000 | Loss: 0.00001268
Iteration 50/1000 | Loss: 0.00001267
Iteration 51/1000 | Loss: 0.00001267
Iteration 52/1000 | Loss: 0.00001267
Iteration 53/1000 | Loss: 0.00001267
Iteration 54/1000 | Loss: 0.00001267
Iteration 55/1000 | Loss: 0.00001267
Iteration 56/1000 | Loss: 0.00001267
Iteration 57/1000 | Loss: 0.00001267
Iteration 58/1000 | Loss: 0.00001266
Iteration 59/1000 | Loss: 0.00001266
Iteration 60/1000 | Loss: 0.00001266
Iteration 61/1000 | Loss: 0.00001266
Iteration 62/1000 | Loss: 0.00001266
Iteration 63/1000 | Loss: 0.00001266
Iteration 64/1000 | Loss: 0.00001266
Iteration 65/1000 | Loss: 0.00001265
Iteration 66/1000 | Loss: 0.00001265
Iteration 67/1000 | Loss: 0.00001265
Iteration 68/1000 | Loss: 0.00001264
Iteration 69/1000 | Loss: 0.00001264
Iteration 70/1000 | Loss: 0.00001264
Iteration 71/1000 | Loss: 0.00001264
Iteration 72/1000 | Loss: 0.00001264
Iteration 73/1000 | Loss: 0.00001264
Iteration 74/1000 | Loss: 0.00001264
Iteration 75/1000 | Loss: 0.00001264
Iteration 76/1000 | Loss: 0.00001263
Iteration 77/1000 | Loss: 0.00001263
Iteration 78/1000 | Loss: 0.00001263
Iteration 79/1000 | Loss: 0.00001263
Iteration 80/1000 | Loss: 0.00001262
Iteration 81/1000 | Loss: 0.00001262
Iteration 82/1000 | Loss: 0.00001262
Iteration 83/1000 | Loss: 0.00001261
Iteration 84/1000 | Loss: 0.00001260
Iteration 85/1000 | Loss: 0.00001259
Iteration 86/1000 | Loss: 0.00001258
Iteration 87/1000 | Loss: 0.00001258
Iteration 88/1000 | Loss: 0.00001258
Iteration 89/1000 | Loss: 0.00001258
Iteration 90/1000 | Loss: 0.00001258
Iteration 91/1000 | Loss: 0.00001257
Iteration 92/1000 | Loss: 0.00001257
Iteration 93/1000 | Loss: 0.00001256
Iteration 94/1000 | Loss: 0.00001256
Iteration 95/1000 | Loss: 0.00001256
Iteration 96/1000 | Loss: 0.00001255
Iteration 97/1000 | Loss: 0.00001255
Iteration 98/1000 | Loss: 0.00001255
Iteration 99/1000 | Loss: 0.00001255
Iteration 100/1000 | Loss: 0.00001255
Iteration 101/1000 | Loss: 0.00001255
Iteration 102/1000 | Loss: 0.00001255
Iteration 103/1000 | Loss: 0.00001255
Iteration 104/1000 | Loss: 0.00001255
Iteration 105/1000 | Loss: 0.00001255
Iteration 106/1000 | Loss: 0.00001255
Iteration 107/1000 | Loss: 0.00001254
Iteration 108/1000 | Loss: 0.00001254
Iteration 109/1000 | Loss: 0.00001254
Iteration 110/1000 | Loss: 0.00001254
Iteration 111/1000 | Loss: 0.00001254
Iteration 112/1000 | Loss: 0.00001254
Iteration 113/1000 | Loss: 0.00001254
Iteration 114/1000 | Loss: 0.00001253
Iteration 115/1000 | Loss: 0.00001253
Iteration 116/1000 | Loss: 0.00001253
Iteration 117/1000 | Loss: 0.00001253
Iteration 118/1000 | Loss: 0.00001253
Iteration 119/1000 | Loss: 0.00001253
Iteration 120/1000 | Loss: 0.00001253
Iteration 121/1000 | Loss: 0.00001253
Iteration 122/1000 | Loss: 0.00001253
Iteration 123/1000 | Loss: 0.00001253
Iteration 124/1000 | Loss: 0.00001253
Iteration 125/1000 | Loss: 0.00001253
Iteration 126/1000 | Loss: 0.00001253
Iteration 127/1000 | Loss: 0.00001253
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 127. Stopping optimization.
Last 5 losses: [1.2526527825684752e-05, 1.2526527825684752e-05, 1.2526527825684752e-05, 1.2526527825684752e-05, 1.2526527825684752e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2526527825684752e-05

Optimization complete. Final v2v error: 3.0159287452697754 mm

Highest mean error: 3.327281951904297 mm for frame 118

Lowest mean error: 2.804835557937622 mm for frame 23

Saving results

Total time: 36.79474878311157
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1041/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1041.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1041
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00768543
Iteration 2/25 | Loss: 0.00138115
Iteration 3/25 | Loss: 0.00118471
Iteration 4/25 | Loss: 0.00112686
Iteration 5/25 | Loss: 0.00113360
Iteration 6/25 | Loss: 0.00108538
Iteration 7/25 | Loss: 0.00112792
Iteration 8/25 | Loss: 0.00096367
Iteration 9/25 | Loss: 0.00091665
Iteration 10/25 | Loss: 0.00091035
Iteration 11/25 | Loss: 0.00091627
Iteration 12/25 | Loss: 0.00091565
Iteration 13/25 | Loss: 0.00091619
Iteration 14/25 | Loss: 0.00091570
Iteration 15/25 | Loss: 0.00091584
Iteration 16/25 | Loss: 0.00093932
Iteration 17/25 | Loss: 0.00092469
Iteration 18/25 | Loss: 0.00089022
Iteration 19/25 | Loss: 0.00088275
Iteration 20/25 | Loss: 0.00088052
Iteration 21/25 | Loss: 0.00087977
Iteration 22/25 | Loss: 0.00087967
Iteration 23/25 | Loss: 0.00087966
Iteration 24/25 | Loss: 0.00087966
Iteration 25/25 | Loss: 0.00087966

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.68871140
Iteration 2/25 | Loss: 0.00166612
Iteration 3/25 | Loss: 0.00166611
Iteration 4/25 | Loss: 0.00166611
Iteration 5/25 | Loss: 0.00166611
Iteration 6/25 | Loss: 0.00166611
Iteration 7/25 | Loss: 0.00166611
Iteration 8/25 | Loss: 0.00166611
Iteration 9/25 | Loss: 0.00166611
Iteration 10/25 | Loss: 0.00166611
Iteration 11/25 | Loss: 0.00166611
Iteration 12/25 | Loss: 0.00166611
Iteration 13/25 | Loss: 0.00166611
Iteration 14/25 | Loss: 0.00166611
Iteration 15/25 | Loss: 0.00166611
Iteration 16/25 | Loss: 0.00166611
Iteration 17/25 | Loss: 0.00166611
Iteration 18/25 | Loss: 0.00166611
Iteration 19/25 | Loss: 0.00166611
Iteration 20/25 | Loss: 0.00166611
Iteration 21/25 | Loss: 0.00166611
Iteration 22/25 | Loss: 0.00166611
Iteration 23/25 | Loss: 0.00166611
Iteration 24/25 | Loss: 0.00166611
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0016661070985719562, 0.0016661070985719562, 0.0016661070985719562, 0.0016661070985719562, 0.0016661070985719562]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016661070985719562

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00166611
Iteration 2/1000 | Loss: 0.00006803
Iteration 3/1000 | Loss: 0.00004977
Iteration 4/1000 | Loss: 0.00004076
Iteration 5/1000 | Loss: 0.00003658
Iteration 6/1000 | Loss: 0.00003540
Iteration 7/1000 | Loss: 0.00003430
Iteration 8/1000 | Loss: 0.00003365
Iteration 9/1000 | Loss: 0.00003306
Iteration 10/1000 | Loss: 0.00003266
Iteration 11/1000 | Loss: 0.00003245
Iteration 12/1000 | Loss: 0.00003235
Iteration 13/1000 | Loss: 0.00003232
Iteration 14/1000 | Loss: 0.00003232
Iteration 15/1000 | Loss: 0.00003225
Iteration 16/1000 | Loss: 0.00003209
Iteration 17/1000 | Loss: 0.00003203
Iteration 18/1000 | Loss: 0.00003200
Iteration 19/1000 | Loss: 0.00003196
Iteration 20/1000 | Loss: 0.00003194
Iteration 21/1000 | Loss: 0.00003193
Iteration 22/1000 | Loss: 0.00003193
Iteration 23/1000 | Loss: 0.00003191
Iteration 24/1000 | Loss: 0.00003190
Iteration 25/1000 | Loss: 0.00003189
Iteration 26/1000 | Loss: 0.00003189
Iteration 27/1000 | Loss: 0.00003189
Iteration 28/1000 | Loss: 0.00003187
Iteration 29/1000 | Loss: 0.00003187
Iteration 30/1000 | Loss: 0.00003187
Iteration 31/1000 | Loss: 0.00003187
Iteration 32/1000 | Loss: 0.00003185
Iteration 33/1000 | Loss: 0.00003184
Iteration 34/1000 | Loss: 0.00003184
Iteration 35/1000 | Loss: 0.00003183
Iteration 36/1000 | Loss: 0.00003183
Iteration 37/1000 | Loss: 0.00003183
Iteration 38/1000 | Loss: 0.00003182
Iteration 39/1000 | Loss: 0.00003182
Iteration 40/1000 | Loss: 0.00003182
Iteration 41/1000 | Loss: 0.00003181
Iteration 42/1000 | Loss: 0.00003181
Iteration 43/1000 | Loss: 0.00003181
Iteration 44/1000 | Loss: 0.00003180
Iteration 45/1000 | Loss: 0.00003180
Iteration 46/1000 | Loss: 0.00003179
Iteration 47/1000 | Loss: 0.00003179
Iteration 48/1000 | Loss: 0.00003179
Iteration 49/1000 | Loss: 0.00003178
Iteration 50/1000 | Loss: 0.00003178
Iteration 51/1000 | Loss: 0.00003178
Iteration 52/1000 | Loss: 0.00003178
Iteration 53/1000 | Loss: 0.00003178
Iteration 54/1000 | Loss: 0.00003178
Iteration 55/1000 | Loss: 0.00003177
Iteration 56/1000 | Loss: 0.00003177
Iteration 57/1000 | Loss: 0.00003177
Iteration 58/1000 | Loss: 0.00003177
Iteration 59/1000 | Loss: 0.00003177
Iteration 60/1000 | Loss: 0.00003177
Iteration 61/1000 | Loss: 0.00003177
Iteration 62/1000 | Loss: 0.00003177
Iteration 63/1000 | Loss: 0.00003177
Iteration 64/1000 | Loss: 0.00003177
Iteration 65/1000 | Loss: 0.00003176
Iteration 66/1000 | Loss: 0.00003176
Iteration 67/1000 | Loss: 0.00003176
Iteration 68/1000 | Loss: 0.00003176
Iteration 69/1000 | Loss: 0.00003176
Iteration 70/1000 | Loss: 0.00003176
Iteration 71/1000 | Loss: 0.00003176
Iteration 72/1000 | Loss: 0.00003176
Iteration 73/1000 | Loss: 0.00003176
Iteration 74/1000 | Loss: 0.00003176
Iteration 75/1000 | Loss: 0.00003176
Iteration 76/1000 | Loss: 0.00003176
Iteration 77/1000 | Loss: 0.00003176
Iteration 78/1000 | Loss: 0.00003176
Iteration 79/1000 | Loss: 0.00003176
Iteration 80/1000 | Loss: 0.00003176
Iteration 81/1000 | Loss: 0.00003175
Iteration 82/1000 | Loss: 0.00003175
Iteration 83/1000 | Loss: 0.00003175
Iteration 84/1000 | Loss: 0.00003175
Iteration 85/1000 | Loss: 0.00003175
Iteration 86/1000 | Loss: 0.00003175
Iteration 87/1000 | Loss: 0.00003175
Iteration 88/1000 | Loss: 0.00003175
Iteration 89/1000 | Loss: 0.00003175
Iteration 90/1000 | Loss: 0.00003175
Iteration 91/1000 | Loss: 0.00003175
Iteration 92/1000 | Loss: 0.00003175
Iteration 93/1000 | Loss: 0.00003174
Iteration 94/1000 | Loss: 0.00003174
Iteration 95/1000 | Loss: 0.00003174
Iteration 96/1000 | Loss: 0.00003174
Iteration 97/1000 | Loss: 0.00003174
Iteration 98/1000 | Loss: 0.00003174
Iteration 99/1000 | Loss: 0.00003174
Iteration 100/1000 | Loss: 0.00003174
Iteration 101/1000 | Loss: 0.00003173
Iteration 102/1000 | Loss: 0.00003173
Iteration 103/1000 | Loss: 0.00003173
Iteration 104/1000 | Loss: 0.00003173
Iteration 105/1000 | Loss: 0.00003173
Iteration 106/1000 | Loss: 0.00003173
Iteration 107/1000 | Loss: 0.00003172
Iteration 108/1000 | Loss: 0.00003172
Iteration 109/1000 | Loss: 0.00003172
Iteration 110/1000 | Loss: 0.00003172
Iteration 111/1000 | Loss: 0.00003172
Iteration 112/1000 | Loss: 0.00003172
Iteration 113/1000 | Loss: 0.00003172
Iteration 114/1000 | Loss: 0.00003172
Iteration 115/1000 | Loss: 0.00003172
Iteration 116/1000 | Loss: 0.00003172
Iteration 117/1000 | Loss: 0.00003172
Iteration 118/1000 | Loss: 0.00003171
Iteration 119/1000 | Loss: 0.00003171
Iteration 120/1000 | Loss: 0.00003171
Iteration 121/1000 | Loss: 0.00003171
Iteration 122/1000 | Loss: 0.00003171
Iteration 123/1000 | Loss: 0.00003171
Iteration 124/1000 | Loss: 0.00003171
Iteration 125/1000 | Loss: 0.00003171
Iteration 126/1000 | Loss: 0.00003171
Iteration 127/1000 | Loss: 0.00003171
Iteration 128/1000 | Loss: 0.00003171
Iteration 129/1000 | Loss: 0.00003170
Iteration 130/1000 | Loss: 0.00003170
Iteration 131/1000 | Loss: 0.00003170
Iteration 132/1000 | Loss: 0.00003170
Iteration 133/1000 | Loss: 0.00003170
Iteration 134/1000 | Loss: 0.00003170
Iteration 135/1000 | Loss: 0.00003170
Iteration 136/1000 | Loss: 0.00003170
Iteration 137/1000 | Loss: 0.00003170
Iteration 138/1000 | Loss: 0.00003170
Iteration 139/1000 | Loss: 0.00003170
Iteration 140/1000 | Loss: 0.00003170
Iteration 141/1000 | Loss: 0.00003170
Iteration 142/1000 | Loss: 0.00003170
Iteration 143/1000 | Loss: 0.00003170
Iteration 144/1000 | Loss: 0.00003170
Iteration 145/1000 | Loss: 0.00003170
Iteration 146/1000 | Loss: 0.00003170
Iteration 147/1000 | Loss: 0.00003170
Iteration 148/1000 | Loss: 0.00003170
Iteration 149/1000 | Loss: 0.00003170
Iteration 150/1000 | Loss: 0.00003170
Iteration 151/1000 | Loss: 0.00003170
Iteration 152/1000 | Loss: 0.00003170
Iteration 153/1000 | Loss: 0.00003170
Iteration 154/1000 | Loss: 0.00003170
Iteration 155/1000 | Loss: 0.00003170
Iteration 156/1000 | Loss: 0.00003170
Iteration 157/1000 | Loss: 0.00003170
Iteration 158/1000 | Loss: 0.00003170
Iteration 159/1000 | Loss: 0.00003170
Iteration 160/1000 | Loss: 0.00003170
Iteration 161/1000 | Loss: 0.00003170
Iteration 162/1000 | Loss: 0.00003170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [3.169546471326612e-05, 3.169546471326612e-05, 3.169546471326612e-05, 3.169546471326612e-05, 3.169546471326612e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.169546471326612e-05

Optimization complete. Final v2v error: 4.601041793823242 mm

Highest mean error: 5.228336811065674 mm for frame 90

Lowest mean error: 3.7259209156036377 mm for frame 4

Saving results

Total time: 66.46481490135193
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1074/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1074.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1074
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00539501
Iteration 2/25 | Loss: 0.00114430
Iteration 3/25 | Loss: 0.00089815
Iteration 4/25 | Loss: 0.00082728
Iteration 5/25 | Loss: 0.00081231
Iteration 6/25 | Loss: 0.00077959
Iteration 7/25 | Loss: 0.00077873
Iteration 8/25 | Loss: 0.00079163
Iteration 9/25 | Loss: 0.00077685
Iteration 10/25 | Loss: 0.00076449
Iteration 11/25 | Loss: 0.00076211
Iteration 12/25 | Loss: 0.00075676
Iteration 13/25 | Loss: 0.00075512
Iteration 14/25 | Loss: 0.00075115
Iteration 15/25 | Loss: 0.00074975
Iteration 16/25 | Loss: 0.00074919
Iteration 17/25 | Loss: 0.00074797
Iteration 18/25 | Loss: 0.00074710
Iteration 19/25 | Loss: 0.00074696
Iteration 20/25 | Loss: 0.00074671
Iteration 21/25 | Loss: 0.00074615
Iteration 22/25 | Loss: 0.00074439
Iteration 23/25 | Loss: 0.00074392
Iteration 24/25 | Loss: 0.00074367
Iteration 25/25 | Loss: 0.00074355

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64166653
Iteration 2/25 | Loss: 0.00114262
Iteration 3/25 | Loss: 0.00114262
Iteration 4/25 | Loss: 0.00114262
Iteration 5/25 | Loss: 0.00114261
Iteration 6/25 | Loss: 0.00114261
Iteration 7/25 | Loss: 0.00114261
Iteration 8/25 | Loss: 0.00114261
Iteration 9/25 | Loss: 0.00114261
Iteration 10/25 | Loss: 0.00114261
Iteration 11/25 | Loss: 0.00114261
Iteration 12/25 | Loss: 0.00114261
Iteration 13/25 | Loss: 0.00114261
Iteration 14/25 | Loss: 0.00114261
Iteration 15/25 | Loss: 0.00114261
Iteration 16/25 | Loss: 0.00114261
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0011426133569329977, 0.0011426133569329977, 0.0011426133569329977, 0.0011426133569329977, 0.0011426133569329977]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011426133569329977

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00114261
Iteration 2/1000 | Loss: 0.00002949
Iteration 3/1000 | Loss: 0.00001880
Iteration 4/1000 | Loss: 0.00001562
Iteration 5/1000 | Loss: 0.00001485
Iteration 6/1000 | Loss: 0.00001417
Iteration 7/1000 | Loss: 0.00001389
Iteration 8/1000 | Loss: 0.00001356
Iteration 9/1000 | Loss: 0.00001356
Iteration 10/1000 | Loss: 0.00001344
Iteration 11/1000 | Loss: 0.00001330
Iteration 12/1000 | Loss: 0.00001321
Iteration 13/1000 | Loss: 0.00001320
Iteration 14/1000 | Loss: 0.00001319
Iteration 15/1000 | Loss: 0.00001319
Iteration 16/1000 | Loss: 0.00001315
Iteration 17/1000 | Loss: 0.00001315
Iteration 18/1000 | Loss: 0.00001312
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001310
Iteration 21/1000 | Loss: 0.00001309
Iteration 22/1000 | Loss: 0.00001308
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001307
Iteration 26/1000 | Loss: 0.00001305
Iteration 27/1000 | Loss: 0.00001305
Iteration 28/1000 | Loss: 0.00001304
Iteration 29/1000 | Loss: 0.00001303
Iteration 30/1000 | Loss: 0.00001298
Iteration 31/1000 | Loss: 0.00001294
Iteration 32/1000 | Loss: 0.00001292
Iteration 33/1000 | Loss: 0.00001292
Iteration 34/1000 | Loss: 0.00001291
Iteration 35/1000 | Loss: 0.00001290
Iteration 36/1000 | Loss: 0.00001290
Iteration 37/1000 | Loss: 0.00001289
Iteration 38/1000 | Loss: 0.00001288
Iteration 39/1000 | Loss: 0.00001288
Iteration 40/1000 | Loss: 0.00001288
Iteration 41/1000 | Loss: 0.00001288
Iteration 42/1000 | Loss: 0.00001288
Iteration 43/1000 | Loss: 0.00001287
Iteration 44/1000 | Loss: 0.00001287
Iteration 45/1000 | Loss: 0.00001286
Iteration 46/1000 | Loss: 0.00001286
Iteration 47/1000 | Loss: 0.00001286
Iteration 48/1000 | Loss: 0.00001286
Iteration 49/1000 | Loss: 0.00001285
Iteration 50/1000 | Loss: 0.00001285
Iteration 51/1000 | Loss: 0.00001285
Iteration 52/1000 | Loss: 0.00001285
Iteration 53/1000 | Loss: 0.00001285
Iteration 54/1000 | Loss: 0.00001285
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001284
Iteration 57/1000 | Loss: 0.00001284
Iteration 58/1000 | Loss: 0.00001283
Iteration 59/1000 | Loss: 0.00001283
Iteration 60/1000 | Loss: 0.00001283
Iteration 61/1000 | Loss: 0.00001283
Iteration 62/1000 | Loss: 0.00001283
Iteration 63/1000 | Loss: 0.00001282
Iteration 64/1000 | Loss: 0.00001282
Iteration 65/1000 | Loss: 0.00001282
Iteration 66/1000 | Loss: 0.00001282
Iteration 67/1000 | Loss: 0.00001282
Iteration 68/1000 | Loss: 0.00001282
Iteration 69/1000 | Loss: 0.00001282
Iteration 70/1000 | Loss: 0.00001282
Iteration 71/1000 | Loss: 0.00001282
Iteration 72/1000 | Loss: 0.00001281
Iteration 73/1000 | Loss: 0.00001281
Iteration 74/1000 | Loss: 0.00001281
Iteration 75/1000 | Loss: 0.00001281
Iteration 76/1000 | Loss: 0.00001281
Iteration 77/1000 | Loss: 0.00001281
Iteration 78/1000 | Loss: 0.00001281
Iteration 79/1000 | Loss: 0.00001280
Iteration 80/1000 | Loss: 0.00001280
Iteration 81/1000 | Loss: 0.00001280
Iteration 82/1000 | Loss: 0.00001280
Iteration 83/1000 | Loss: 0.00001280
Iteration 84/1000 | Loss: 0.00001280
Iteration 85/1000 | Loss: 0.00001280
Iteration 86/1000 | Loss: 0.00001280
Iteration 87/1000 | Loss: 0.00001280
Iteration 88/1000 | Loss: 0.00001279
Iteration 89/1000 | Loss: 0.00001279
Iteration 90/1000 | Loss: 0.00001279
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001279
Iteration 93/1000 | Loss: 0.00001279
Iteration 94/1000 | Loss: 0.00001279
Iteration 95/1000 | Loss: 0.00001279
Iteration 96/1000 | Loss: 0.00001279
Iteration 97/1000 | Loss: 0.00001279
Iteration 98/1000 | Loss: 0.00001278
Iteration 99/1000 | Loss: 0.00001278
Iteration 100/1000 | Loss: 0.00001278
Iteration 101/1000 | Loss: 0.00001278
Iteration 102/1000 | Loss: 0.00001278
Iteration 103/1000 | Loss: 0.00001278
Iteration 104/1000 | Loss: 0.00001277
Iteration 105/1000 | Loss: 0.00001277
Iteration 106/1000 | Loss: 0.00001277
Iteration 107/1000 | Loss: 0.00001277
Iteration 108/1000 | Loss: 0.00001277
Iteration 109/1000 | Loss: 0.00001277
Iteration 110/1000 | Loss: 0.00001277
Iteration 111/1000 | Loss: 0.00001277
Iteration 112/1000 | Loss: 0.00001277
Iteration 113/1000 | Loss: 0.00001277
Iteration 114/1000 | Loss: 0.00001276
Iteration 115/1000 | Loss: 0.00001276
Iteration 116/1000 | Loss: 0.00001276
Iteration 117/1000 | Loss: 0.00001276
Iteration 118/1000 | Loss: 0.00001276
Iteration 119/1000 | Loss: 0.00001276
Iteration 120/1000 | Loss: 0.00001276
Iteration 121/1000 | Loss: 0.00001275
Iteration 122/1000 | Loss: 0.00001275
Iteration 123/1000 | Loss: 0.00001275
Iteration 124/1000 | Loss: 0.00001275
Iteration 125/1000 | Loss: 0.00001274
Iteration 126/1000 | Loss: 0.00001274
Iteration 127/1000 | Loss: 0.00001274
Iteration 128/1000 | Loss: 0.00001274
Iteration 129/1000 | Loss: 0.00001274
Iteration 130/1000 | Loss: 0.00001274
Iteration 131/1000 | Loss: 0.00001274
Iteration 132/1000 | Loss: 0.00001274
Iteration 133/1000 | Loss: 0.00001273
Iteration 134/1000 | Loss: 0.00001273
Iteration 135/1000 | Loss: 0.00001273
Iteration 136/1000 | Loss: 0.00001273
Iteration 137/1000 | Loss: 0.00001273
Iteration 138/1000 | Loss: 0.00001273
Iteration 139/1000 | Loss: 0.00001273
Iteration 140/1000 | Loss: 0.00001272
Iteration 141/1000 | Loss: 0.00001272
Iteration 142/1000 | Loss: 0.00001272
Iteration 143/1000 | Loss: 0.00001272
Iteration 144/1000 | Loss: 0.00001272
Iteration 145/1000 | Loss: 0.00001272
Iteration 146/1000 | Loss: 0.00001272
Iteration 147/1000 | Loss: 0.00001272
Iteration 148/1000 | Loss: 0.00001272
Iteration 149/1000 | Loss: 0.00001272
Iteration 150/1000 | Loss: 0.00001271
Iteration 151/1000 | Loss: 0.00001271
Iteration 152/1000 | Loss: 0.00001271
Iteration 153/1000 | Loss: 0.00001271
Iteration 154/1000 | Loss: 0.00001271
Iteration 155/1000 | Loss: 0.00001271
Iteration 156/1000 | Loss: 0.00001271
Iteration 157/1000 | Loss: 0.00001271
Iteration 158/1000 | Loss: 0.00001271
Iteration 159/1000 | Loss: 0.00001271
Iteration 160/1000 | Loss: 0.00001271
Iteration 161/1000 | Loss: 0.00001271
Iteration 162/1000 | Loss: 0.00001271
Iteration 163/1000 | Loss: 0.00001271
Iteration 164/1000 | Loss: 0.00001271
Iteration 165/1000 | Loss: 0.00001271
Iteration 166/1000 | Loss: 0.00001271
Iteration 167/1000 | Loss: 0.00001271
Iteration 168/1000 | Loss: 0.00001271
Iteration 169/1000 | Loss: 0.00001271
Iteration 170/1000 | Loss: 0.00001271
Iteration 171/1000 | Loss: 0.00001271
Iteration 172/1000 | Loss: 0.00001271
Iteration 173/1000 | Loss: 0.00001271
Iteration 174/1000 | Loss: 0.00001271
Iteration 175/1000 | Loss: 0.00001271
Iteration 176/1000 | Loss: 0.00001271
Iteration 177/1000 | Loss: 0.00001271
Iteration 178/1000 | Loss: 0.00001271
Iteration 179/1000 | Loss: 0.00001271
Iteration 180/1000 | Loss: 0.00001271
Iteration 181/1000 | Loss: 0.00001271
Iteration 182/1000 | Loss: 0.00001271
Iteration 183/1000 | Loss: 0.00001271
Iteration 184/1000 | Loss: 0.00001271
Iteration 185/1000 | Loss: 0.00001271
Iteration 186/1000 | Loss: 0.00001271
Iteration 187/1000 | Loss: 0.00001271
Iteration 188/1000 | Loss: 0.00001271
Iteration 189/1000 | Loss: 0.00001271
Iteration 190/1000 | Loss: 0.00001271
Iteration 191/1000 | Loss: 0.00001271
Iteration 192/1000 | Loss: 0.00001271
Iteration 193/1000 | Loss: 0.00001271
Iteration 194/1000 | Loss: 0.00001271
Iteration 195/1000 | Loss: 0.00001271
Iteration 196/1000 | Loss: 0.00001271
Iteration 197/1000 | Loss: 0.00001271
Iteration 198/1000 | Loss: 0.00001271
Iteration 199/1000 | Loss: 0.00001271
Iteration 200/1000 | Loss: 0.00001271
Iteration 201/1000 | Loss: 0.00001271
Iteration 202/1000 | Loss: 0.00001271
Iteration 203/1000 | Loss: 0.00001271
Iteration 204/1000 | Loss: 0.00001271
Iteration 205/1000 | Loss: 0.00001271
Iteration 206/1000 | Loss: 0.00001271
Iteration 207/1000 | Loss: 0.00001271
Iteration 208/1000 | Loss: 0.00001271
Iteration 209/1000 | Loss: 0.00001271
Iteration 210/1000 | Loss: 0.00001271
Iteration 211/1000 | Loss: 0.00001271
Iteration 212/1000 | Loss: 0.00001271
Iteration 213/1000 | Loss: 0.00001271
Iteration 214/1000 | Loss: 0.00001271
Iteration 215/1000 | Loss: 0.00001271
Iteration 216/1000 | Loss: 0.00001271
Iteration 217/1000 | Loss: 0.00001271
Iteration 218/1000 | Loss: 0.00001271
Iteration 219/1000 | Loss: 0.00001271
Iteration 220/1000 | Loss: 0.00001271
Iteration 221/1000 | Loss: 0.00001271
Iteration 222/1000 | Loss: 0.00001271
Iteration 223/1000 | Loss: 0.00001271
Iteration 224/1000 | Loss: 0.00001271
Iteration 225/1000 | Loss: 0.00001271
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.2711744602711406e-05, 1.2711744602711406e-05, 1.2711744602711406e-05, 1.2711744602711406e-05, 1.2711744602711406e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2711744602711406e-05

Optimization complete. Final v2v error: 3.0499510765075684 mm

Highest mean error: 3.9344351291656494 mm for frame 39

Lowest mean error: 2.747345447540283 mm for frame 122

Saving results

Total time: 75.36550807952881
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1094/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1094.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1094
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00396688
Iteration 2/25 | Loss: 0.00096596
Iteration 3/25 | Loss: 0.00081908
Iteration 4/25 | Loss: 0.00078697
Iteration 5/25 | Loss: 0.00078167
Iteration 6/25 | Loss: 0.00078020
Iteration 7/25 | Loss: 0.00078016
Iteration 8/25 | Loss: 0.00078016
Iteration 9/25 | Loss: 0.00078016
Iteration 10/25 | Loss: 0.00078016
Iteration 11/25 | Loss: 0.00078016
Iteration 12/25 | Loss: 0.00078016
Iteration 13/25 | Loss: 0.00078016
Iteration 14/25 | Loss: 0.00078016
Iteration 15/25 | Loss: 0.00078016
Iteration 16/25 | Loss: 0.00078016
Iteration 17/25 | Loss: 0.00078016
Iteration 18/25 | Loss: 0.00078016
Iteration 19/25 | Loss: 0.00078016
Iteration 20/25 | Loss: 0.00078016
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007801567553542554, 0.0007801567553542554, 0.0007801567553542554, 0.0007801567553542554, 0.0007801567553542554]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007801567553542554

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59473836
Iteration 2/25 | Loss: 0.00132110
Iteration 3/25 | Loss: 0.00132110
Iteration 4/25 | Loss: 0.00132110
Iteration 5/25 | Loss: 0.00132110
Iteration 6/25 | Loss: 0.00132110
Iteration 7/25 | Loss: 0.00132110
Iteration 8/25 | Loss: 0.00132110
Iteration 9/25 | Loss: 0.00132110
Iteration 10/25 | Loss: 0.00132110
Iteration 11/25 | Loss: 0.00132110
Iteration 12/25 | Loss: 0.00132110
Iteration 13/25 | Loss: 0.00132110
Iteration 14/25 | Loss: 0.00132110
Iteration 15/25 | Loss: 0.00132110
Iteration 16/25 | Loss: 0.00132110
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013211006298661232, 0.0013211006298661232, 0.0013211006298661232, 0.0013211006298661232, 0.0013211006298661232]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013211006298661232

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132110
Iteration 2/1000 | Loss: 0.00003035
Iteration 3/1000 | Loss: 0.00001940
Iteration 4/1000 | Loss: 0.00001745
Iteration 5/1000 | Loss: 0.00001609
Iteration 6/1000 | Loss: 0.00001554
Iteration 7/1000 | Loss: 0.00001523
Iteration 8/1000 | Loss: 0.00001513
Iteration 9/1000 | Loss: 0.00001500
Iteration 10/1000 | Loss: 0.00001495
Iteration 11/1000 | Loss: 0.00001487
Iteration 12/1000 | Loss: 0.00001471
Iteration 13/1000 | Loss: 0.00001471
Iteration 14/1000 | Loss: 0.00001469
Iteration 15/1000 | Loss: 0.00001468
Iteration 16/1000 | Loss: 0.00001464
Iteration 17/1000 | Loss: 0.00001464
Iteration 18/1000 | Loss: 0.00001463
Iteration 19/1000 | Loss: 0.00001461
Iteration 20/1000 | Loss: 0.00001458
Iteration 21/1000 | Loss: 0.00001457
Iteration 22/1000 | Loss: 0.00001457
Iteration 23/1000 | Loss: 0.00001454
Iteration 24/1000 | Loss: 0.00001453
Iteration 25/1000 | Loss: 0.00001452
Iteration 26/1000 | Loss: 0.00001451
Iteration 27/1000 | Loss: 0.00001450
Iteration 28/1000 | Loss: 0.00001449
Iteration 29/1000 | Loss: 0.00001448
Iteration 30/1000 | Loss: 0.00001448
Iteration 31/1000 | Loss: 0.00001448
Iteration 32/1000 | Loss: 0.00001447
Iteration 33/1000 | Loss: 0.00001447
Iteration 34/1000 | Loss: 0.00001447
Iteration 35/1000 | Loss: 0.00001447
Iteration 36/1000 | Loss: 0.00001446
Iteration 37/1000 | Loss: 0.00001446
Iteration 38/1000 | Loss: 0.00001446
Iteration 39/1000 | Loss: 0.00001445
Iteration 40/1000 | Loss: 0.00001445
Iteration 41/1000 | Loss: 0.00001444
Iteration 42/1000 | Loss: 0.00001444
Iteration 43/1000 | Loss: 0.00001444
Iteration 44/1000 | Loss: 0.00001444
Iteration 45/1000 | Loss: 0.00001444
Iteration 46/1000 | Loss: 0.00001444
Iteration 47/1000 | Loss: 0.00001444
Iteration 48/1000 | Loss: 0.00001444
Iteration 49/1000 | Loss: 0.00001444
Iteration 50/1000 | Loss: 0.00001444
Iteration 51/1000 | Loss: 0.00001444
Iteration 52/1000 | Loss: 0.00001444
Iteration 53/1000 | Loss: 0.00001443
Iteration 54/1000 | Loss: 0.00001443
Iteration 55/1000 | Loss: 0.00001443
Iteration 56/1000 | Loss: 0.00001443
Iteration 57/1000 | Loss: 0.00001443
Iteration 58/1000 | Loss: 0.00001443
Iteration 59/1000 | Loss: 0.00001443
Iteration 60/1000 | Loss: 0.00001442
Iteration 61/1000 | Loss: 0.00001442
Iteration 62/1000 | Loss: 0.00001441
Iteration 63/1000 | Loss: 0.00001441
Iteration 64/1000 | Loss: 0.00001441
Iteration 65/1000 | Loss: 0.00001441
Iteration 66/1000 | Loss: 0.00001440
Iteration 67/1000 | Loss: 0.00001440
Iteration 68/1000 | Loss: 0.00001440
Iteration 69/1000 | Loss: 0.00001440
Iteration 70/1000 | Loss: 0.00001439
Iteration 71/1000 | Loss: 0.00001439
Iteration 72/1000 | Loss: 0.00001439
Iteration 73/1000 | Loss: 0.00001439
Iteration 74/1000 | Loss: 0.00001438
Iteration 75/1000 | Loss: 0.00001438
Iteration 76/1000 | Loss: 0.00001438
Iteration 77/1000 | Loss: 0.00001437
Iteration 78/1000 | Loss: 0.00001437
Iteration 79/1000 | Loss: 0.00001437
Iteration 80/1000 | Loss: 0.00001437
Iteration 81/1000 | Loss: 0.00001437
Iteration 82/1000 | Loss: 0.00001437
Iteration 83/1000 | Loss: 0.00001437
Iteration 84/1000 | Loss: 0.00001436
Iteration 85/1000 | Loss: 0.00001436
Iteration 86/1000 | Loss: 0.00001436
Iteration 87/1000 | Loss: 0.00001436
Iteration 88/1000 | Loss: 0.00001436
Iteration 89/1000 | Loss: 0.00001436
Iteration 90/1000 | Loss: 0.00001435
Iteration 91/1000 | Loss: 0.00001435
Iteration 92/1000 | Loss: 0.00001435
Iteration 93/1000 | Loss: 0.00001435
Iteration 94/1000 | Loss: 0.00001435
Iteration 95/1000 | Loss: 0.00001435
Iteration 96/1000 | Loss: 0.00001435
Iteration 97/1000 | Loss: 0.00001435
Iteration 98/1000 | Loss: 0.00001435
Iteration 99/1000 | Loss: 0.00001435
Iteration 100/1000 | Loss: 0.00001435
Iteration 101/1000 | Loss: 0.00001434
Iteration 102/1000 | Loss: 0.00001434
Iteration 103/1000 | Loss: 0.00001434
Iteration 104/1000 | Loss: 0.00001434
Iteration 105/1000 | Loss: 0.00001434
Iteration 106/1000 | Loss: 0.00001434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 106. Stopping optimization.
Last 5 losses: [1.4344538612931501e-05, 1.4344538612931501e-05, 1.4344538612931501e-05, 1.4344538612931501e-05, 1.4344538612931501e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4344538612931501e-05

Optimization complete. Final v2v error: 3.0947203636169434 mm

Highest mean error: 3.539930582046509 mm for frame 114

Lowest mean error: 2.5836009979248047 mm for frame 0

Saving results

Total time: 36.401267766952515
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1066/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1066.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1066
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01058262
Iteration 2/25 | Loss: 0.00149155
Iteration 3/25 | Loss: 0.00110550
Iteration 4/25 | Loss: 0.00103553
Iteration 5/25 | Loss: 0.00101839
Iteration 6/25 | Loss: 0.00101320
Iteration 7/25 | Loss: 0.00101219
Iteration 8/25 | Loss: 0.00101219
Iteration 9/25 | Loss: 0.00101219
Iteration 10/25 | Loss: 0.00101219
Iteration 11/25 | Loss: 0.00101219
Iteration 12/25 | Loss: 0.00101219
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0010121918749064207, 0.0010121918749064207, 0.0010121918749064207, 0.0010121918749064207, 0.0010121918749064207]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010121918749064207

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.02513897
Iteration 2/25 | Loss: 0.00119609
Iteration 3/25 | Loss: 0.00119598
Iteration 4/25 | Loss: 0.00119598
Iteration 5/25 | Loss: 0.00119598
Iteration 6/25 | Loss: 0.00119598
Iteration 7/25 | Loss: 0.00119598
Iteration 8/25 | Loss: 0.00119598
Iteration 9/25 | Loss: 0.00119598
Iteration 10/25 | Loss: 0.00119598
Iteration 11/25 | Loss: 0.00119598
Iteration 12/25 | Loss: 0.00119598
Iteration 13/25 | Loss: 0.00119598
Iteration 14/25 | Loss: 0.00119598
Iteration 15/25 | Loss: 0.00119598
Iteration 16/25 | Loss: 0.00119598
Iteration 17/25 | Loss: 0.00119598
Iteration 18/25 | Loss: 0.00119598
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001195979304611683, 0.001195979304611683, 0.001195979304611683, 0.001195979304611683, 0.001195979304611683]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001195979304611683

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119598
Iteration 2/1000 | Loss: 0.00012237
Iteration 3/1000 | Loss: 0.00008398
Iteration 4/1000 | Loss: 0.00007116
Iteration 5/1000 | Loss: 0.00006608
Iteration 6/1000 | Loss: 0.00006311
Iteration 7/1000 | Loss: 0.00006006
Iteration 8/1000 | Loss: 0.00005812
Iteration 9/1000 | Loss: 0.00005621
Iteration 10/1000 | Loss: 0.00005503
Iteration 11/1000 | Loss: 0.00005420
Iteration 12/1000 | Loss: 0.00005339
Iteration 13/1000 | Loss: 0.00005273
Iteration 14/1000 | Loss: 0.00005225
Iteration 15/1000 | Loss: 0.00005186
Iteration 16/1000 | Loss: 0.00005145
Iteration 17/1000 | Loss: 0.00005116
Iteration 18/1000 | Loss: 0.00005089
Iteration 19/1000 | Loss: 0.00005065
Iteration 20/1000 | Loss: 0.00005044
Iteration 21/1000 | Loss: 0.00005025
Iteration 22/1000 | Loss: 0.00005011
Iteration 23/1000 | Loss: 0.00005004
Iteration 24/1000 | Loss: 0.00005003
Iteration 25/1000 | Loss: 0.00005002
Iteration 26/1000 | Loss: 0.00005001
Iteration 27/1000 | Loss: 0.00005000
Iteration 28/1000 | Loss: 0.00005000
Iteration 29/1000 | Loss: 0.00004998
Iteration 30/1000 | Loss: 0.00004998
Iteration 31/1000 | Loss: 0.00004997
Iteration 32/1000 | Loss: 0.00004995
Iteration 33/1000 | Loss: 0.00004987
Iteration 34/1000 | Loss: 0.00004986
Iteration 35/1000 | Loss: 0.00004980
Iteration 36/1000 | Loss: 0.00004965
Iteration 37/1000 | Loss: 0.00004960
Iteration 38/1000 | Loss: 0.00004960
Iteration 39/1000 | Loss: 0.00004957
Iteration 40/1000 | Loss: 0.00004957
Iteration 41/1000 | Loss: 0.00004954
Iteration 42/1000 | Loss: 0.00004954
Iteration 43/1000 | Loss: 0.00004954
Iteration 44/1000 | Loss: 0.00004952
Iteration 45/1000 | Loss: 0.00004951
Iteration 46/1000 | Loss: 0.00004951
Iteration 47/1000 | Loss: 0.00004950
Iteration 48/1000 | Loss: 0.00004950
Iteration 49/1000 | Loss: 0.00004949
Iteration 50/1000 | Loss: 0.00004948
Iteration 51/1000 | Loss: 0.00004948
Iteration 52/1000 | Loss: 0.00004948
Iteration 53/1000 | Loss: 0.00004948
Iteration 54/1000 | Loss: 0.00004948
Iteration 55/1000 | Loss: 0.00004948
Iteration 56/1000 | Loss: 0.00004947
Iteration 57/1000 | Loss: 0.00004947
Iteration 58/1000 | Loss: 0.00004947
Iteration 59/1000 | Loss: 0.00004947
Iteration 60/1000 | Loss: 0.00004946
Iteration 61/1000 | Loss: 0.00004946
Iteration 62/1000 | Loss: 0.00004946
Iteration 63/1000 | Loss: 0.00004945
Iteration 64/1000 | Loss: 0.00004944
Iteration 65/1000 | Loss: 0.00004944
Iteration 66/1000 | Loss: 0.00004943
Iteration 67/1000 | Loss: 0.00004943
Iteration 68/1000 | Loss: 0.00004943
Iteration 69/1000 | Loss: 0.00004943
Iteration 70/1000 | Loss: 0.00004943
Iteration 71/1000 | Loss: 0.00004942
Iteration 72/1000 | Loss: 0.00004942
Iteration 73/1000 | Loss: 0.00004941
Iteration 74/1000 | Loss: 0.00004941
Iteration 75/1000 | Loss: 0.00004941
Iteration 76/1000 | Loss: 0.00004940
Iteration 77/1000 | Loss: 0.00004940
Iteration 78/1000 | Loss: 0.00004940
Iteration 79/1000 | Loss: 0.00004940
Iteration 80/1000 | Loss: 0.00004940
Iteration 81/1000 | Loss: 0.00004939
Iteration 82/1000 | Loss: 0.00004939
Iteration 83/1000 | Loss: 0.00004939
Iteration 84/1000 | Loss: 0.00004939
Iteration 85/1000 | Loss: 0.00004939
Iteration 86/1000 | Loss: 0.00004939
Iteration 87/1000 | Loss: 0.00004939
Iteration 88/1000 | Loss: 0.00004939
Iteration 89/1000 | Loss: 0.00004939
Iteration 90/1000 | Loss: 0.00004938
Iteration 91/1000 | Loss: 0.00004938
Iteration 92/1000 | Loss: 0.00004938
Iteration 93/1000 | Loss: 0.00004938
Iteration 94/1000 | Loss: 0.00004937
Iteration 95/1000 | Loss: 0.00004937
Iteration 96/1000 | Loss: 0.00004937
Iteration 97/1000 | Loss: 0.00004937
Iteration 98/1000 | Loss: 0.00004937
Iteration 99/1000 | Loss: 0.00004936
Iteration 100/1000 | Loss: 0.00004936
Iteration 101/1000 | Loss: 0.00004936
Iteration 102/1000 | Loss: 0.00004936
Iteration 103/1000 | Loss: 0.00004936
Iteration 104/1000 | Loss: 0.00004936
Iteration 105/1000 | Loss: 0.00004936
Iteration 106/1000 | Loss: 0.00004936
Iteration 107/1000 | Loss: 0.00004936
Iteration 108/1000 | Loss: 0.00004936
Iteration 109/1000 | Loss: 0.00004935
Iteration 110/1000 | Loss: 0.00004935
Iteration 111/1000 | Loss: 0.00004935
Iteration 112/1000 | Loss: 0.00004935
Iteration 113/1000 | Loss: 0.00004934
Iteration 114/1000 | Loss: 0.00004934
Iteration 115/1000 | Loss: 0.00004934
Iteration 116/1000 | Loss: 0.00004934
Iteration 117/1000 | Loss: 0.00004934
Iteration 118/1000 | Loss: 0.00004934
Iteration 119/1000 | Loss: 0.00004934
Iteration 120/1000 | Loss: 0.00004933
Iteration 121/1000 | Loss: 0.00004933
Iteration 122/1000 | Loss: 0.00004933
Iteration 123/1000 | Loss: 0.00004933
Iteration 124/1000 | Loss: 0.00004933
Iteration 125/1000 | Loss: 0.00004933
Iteration 126/1000 | Loss: 0.00004933
Iteration 127/1000 | Loss: 0.00004933
Iteration 128/1000 | Loss: 0.00004933
Iteration 129/1000 | Loss: 0.00004933
Iteration 130/1000 | Loss: 0.00004932
Iteration 131/1000 | Loss: 0.00004932
Iteration 132/1000 | Loss: 0.00004932
Iteration 133/1000 | Loss: 0.00004932
Iteration 134/1000 | Loss: 0.00004932
Iteration 135/1000 | Loss: 0.00004932
Iteration 136/1000 | Loss: 0.00004932
Iteration 137/1000 | Loss: 0.00004932
Iteration 138/1000 | Loss: 0.00004932
Iteration 139/1000 | Loss: 0.00004931
Iteration 140/1000 | Loss: 0.00004931
Iteration 141/1000 | Loss: 0.00004931
Iteration 142/1000 | Loss: 0.00004931
Iteration 143/1000 | Loss: 0.00004931
Iteration 144/1000 | Loss: 0.00004931
Iteration 145/1000 | Loss: 0.00004931
Iteration 146/1000 | Loss: 0.00004931
Iteration 147/1000 | Loss: 0.00004931
Iteration 148/1000 | Loss: 0.00004930
Iteration 149/1000 | Loss: 0.00004930
Iteration 150/1000 | Loss: 0.00004930
Iteration 151/1000 | Loss: 0.00004930
Iteration 152/1000 | Loss: 0.00004930
Iteration 153/1000 | Loss: 0.00004930
Iteration 154/1000 | Loss: 0.00004930
Iteration 155/1000 | Loss: 0.00004930
Iteration 156/1000 | Loss: 0.00004930
Iteration 157/1000 | Loss: 0.00004930
Iteration 158/1000 | Loss: 0.00004930
Iteration 159/1000 | Loss: 0.00004930
Iteration 160/1000 | Loss: 0.00004930
Iteration 161/1000 | Loss: 0.00004930
Iteration 162/1000 | Loss: 0.00004930
Iteration 163/1000 | Loss: 0.00004930
Iteration 164/1000 | Loss: 0.00004930
Iteration 165/1000 | Loss: 0.00004930
Iteration 166/1000 | Loss: 0.00004930
Iteration 167/1000 | Loss: 0.00004930
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [4.930205977871083e-05, 4.930205977871083e-05, 4.930205977871083e-05, 4.930205977871083e-05, 4.930205977871083e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.930205977871083e-05

Optimization complete. Final v2v error: 5.771755218505859 mm

Highest mean error: 6.302629470825195 mm for frame 182

Lowest mean error: 5.085041046142578 mm for frame 151

Saving results

Total time: 66.20528030395508
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1046/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1046.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1046
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485054
Iteration 2/25 | Loss: 0.00107299
Iteration 3/25 | Loss: 0.00079806
Iteration 4/25 | Loss: 0.00077017
Iteration 5/25 | Loss: 0.00076114
Iteration 6/25 | Loss: 0.00075884
Iteration 7/25 | Loss: 0.00075820
Iteration 8/25 | Loss: 0.00075820
Iteration 9/25 | Loss: 0.00075820
Iteration 10/25 | Loss: 0.00075820
Iteration 11/25 | Loss: 0.00075820
Iteration 12/25 | Loss: 0.00075820
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007581986719742417, 0.0007581986719742417, 0.0007581986719742417, 0.0007581986719742417, 0.0007581986719742417]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007581986719742417

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.98569238
Iteration 2/25 | Loss: 0.00104242
Iteration 3/25 | Loss: 0.00104241
Iteration 4/25 | Loss: 0.00104241
Iteration 5/25 | Loss: 0.00104241
Iteration 6/25 | Loss: 0.00104241
Iteration 7/25 | Loss: 0.00104241
Iteration 8/25 | Loss: 0.00104241
Iteration 9/25 | Loss: 0.00104241
Iteration 10/25 | Loss: 0.00104241
Iteration 11/25 | Loss: 0.00104241
Iteration 12/25 | Loss: 0.00104241
Iteration 13/25 | Loss: 0.00104241
Iteration 14/25 | Loss: 0.00104241
Iteration 15/25 | Loss: 0.00104241
Iteration 16/25 | Loss: 0.00104241
Iteration 17/25 | Loss: 0.00104241
Iteration 18/25 | Loss: 0.00104241
Iteration 19/25 | Loss: 0.00104241
Iteration 20/25 | Loss: 0.00104241
Iteration 21/25 | Loss: 0.00104241
Iteration 22/25 | Loss: 0.00104241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0010424081701785326, 0.0010424081701785326, 0.0010424081701785326, 0.0010424081701785326, 0.0010424081701785326]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010424081701785326

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00104241
Iteration 2/1000 | Loss: 0.00003184
Iteration 3/1000 | Loss: 0.00002005
Iteration 4/1000 | Loss: 0.00001823
Iteration 5/1000 | Loss: 0.00001739
Iteration 6/1000 | Loss: 0.00001678
Iteration 7/1000 | Loss: 0.00001629
Iteration 8/1000 | Loss: 0.00001585
Iteration 9/1000 | Loss: 0.00001559
Iteration 10/1000 | Loss: 0.00001543
Iteration 11/1000 | Loss: 0.00001523
Iteration 12/1000 | Loss: 0.00001507
Iteration 13/1000 | Loss: 0.00001506
Iteration 14/1000 | Loss: 0.00001505
Iteration 15/1000 | Loss: 0.00001504
Iteration 16/1000 | Loss: 0.00001502
Iteration 17/1000 | Loss: 0.00001496
Iteration 18/1000 | Loss: 0.00001494
Iteration 19/1000 | Loss: 0.00001491
Iteration 20/1000 | Loss: 0.00001488
Iteration 21/1000 | Loss: 0.00001487
Iteration 22/1000 | Loss: 0.00001487
Iteration 23/1000 | Loss: 0.00001486
Iteration 24/1000 | Loss: 0.00001484
Iteration 25/1000 | Loss: 0.00001484
Iteration 26/1000 | Loss: 0.00001484
Iteration 27/1000 | Loss: 0.00001484
Iteration 28/1000 | Loss: 0.00001483
Iteration 29/1000 | Loss: 0.00001483
Iteration 30/1000 | Loss: 0.00001482
Iteration 31/1000 | Loss: 0.00001482
Iteration 32/1000 | Loss: 0.00001482
Iteration 33/1000 | Loss: 0.00001481
Iteration 34/1000 | Loss: 0.00001481
Iteration 35/1000 | Loss: 0.00001481
Iteration 36/1000 | Loss: 0.00001478
Iteration 37/1000 | Loss: 0.00001478
Iteration 38/1000 | Loss: 0.00001476
Iteration 39/1000 | Loss: 0.00001476
Iteration 40/1000 | Loss: 0.00001476
Iteration 41/1000 | Loss: 0.00001476
Iteration 42/1000 | Loss: 0.00001475
Iteration 43/1000 | Loss: 0.00001475
Iteration 44/1000 | Loss: 0.00001475
Iteration 45/1000 | Loss: 0.00001475
Iteration 46/1000 | Loss: 0.00001475
Iteration 47/1000 | Loss: 0.00001475
Iteration 48/1000 | Loss: 0.00001475
Iteration 49/1000 | Loss: 0.00001475
Iteration 50/1000 | Loss: 0.00001474
Iteration 51/1000 | Loss: 0.00001473
Iteration 52/1000 | Loss: 0.00001473
Iteration 53/1000 | Loss: 0.00001473
Iteration 54/1000 | Loss: 0.00001472
Iteration 55/1000 | Loss: 0.00001472
Iteration 56/1000 | Loss: 0.00001471
Iteration 57/1000 | Loss: 0.00001471
Iteration 58/1000 | Loss: 0.00001470
Iteration 59/1000 | Loss: 0.00001469
Iteration 60/1000 | Loss: 0.00001469
Iteration 61/1000 | Loss: 0.00001468
Iteration 62/1000 | Loss: 0.00001468
Iteration 63/1000 | Loss: 0.00001468
Iteration 64/1000 | Loss: 0.00001468
Iteration 65/1000 | Loss: 0.00001468
Iteration 66/1000 | Loss: 0.00001467
Iteration 67/1000 | Loss: 0.00001467
Iteration 68/1000 | Loss: 0.00001466
Iteration 69/1000 | Loss: 0.00001466
Iteration 70/1000 | Loss: 0.00001465
Iteration 71/1000 | Loss: 0.00001465
Iteration 72/1000 | Loss: 0.00001465
Iteration 73/1000 | Loss: 0.00001464
Iteration 74/1000 | Loss: 0.00001464
Iteration 75/1000 | Loss: 0.00001464
Iteration 76/1000 | Loss: 0.00001463
Iteration 77/1000 | Loss: 0.00001463
Iteration 78/1000 | Loss: 0.00001462
Iteration 79/1000 | Loss: 0.00001462
Iteration 80/1000 | Loss: 0.00001461
Iteration 81/1000 | Loss: 0.00001461
Iteration 82/1000 | Loss: 0.00001461
Iteration 83/1000 | Loss: 0.00001461
Iteration 84/1000 | Loss: 0.00001460
Iteration 85/1000 | Loss: 0.00001460
Iteration 86/1000 | Loss: 0.00001460
Iteration 87/1000 | Loss: 0.00001460
Iteration 88/1000 | Loss: 0.00001459
Iteration 89/1000 | Loss: 0.00001459
Iteration 90/1000 | Loss: 0.00001459
Iteration 91/1000 | Loss: 0.00001458
Iteration 92/1000 | Loss: 0.00001458
Iteration 93/1000 | Loss: 0.00001458
Iteration 94/1000 | Loss: 0.00001458
Iteration 95/1000 | Loss: 0.00001458
Iteration 96/1000 | Loss: 0.00001458
Iteration 97/1000 | Loss: 0.00001458
Iteration 98/1000 | Loss: 0.00001458
Iteration 99/1000 | Loss: 0.00001458
Iteration 100/1000 | Loss: 0.00001458
Iteration 101/1000 | Loss: 0.00001457
Iteration 102/1000 | Loss: 0.00001457
Iteration 103/1000 | Loss: 0.00001457
Iteration 104/1000 | Loss: 0.00001457
Iteration 105/1000 | Loss: 0.00001457
Iteration 106/1000 | Loss: 0.00001457
Iteration 107/1000 | Loss: 0.00001456
Iteration 108/1000 | Loss: 0.00001456
Iteration 109/1000 | Loss: 0.00001456
Iteration 110/1000 | Loss: 0.00001456
Iteration 111/1000 | Loss: 0.00001456
Iteration 112/1000 | Loss: 0.00001456
Iteration 113/1000 | Loss: 0.00001456
Iteration 114/1000 | Loss: 0.00001456
Iteration 115/1000 | Loss: 0.00001456
Iteration 116/1000 | Loss: 0.00001456
Iteration 117/1000 | Loss: 0.00001456
Iteration 118/1000 | Loss: 0.00001456
Iteration 119/1000 | Loss: 0.00001456
Iteration 120/1000 | Loss: 0.00001456
Iteration 121/1000 | Loss: 0.00001456
Iteration 122/1000 | Loss: 0.00001456
Iteration 123/1000 | Loss: 0.00001456
Iteration 124/1000 | Loss: 0.00001456
Iteration 125/1000 | Loss: 0.00001456
Iteration 126/1000 | Loss: 0.00001456
Iteration 127/1000 | Loss: 0.00001456
Iteration 128/1000 | Loss: 0.00001456
Iteration 129/1000 | Loss: 0.00001456
Iteration 130/1000 | Loss: 0.00001456
Iteration 131/1000 | Loss: 0.00001456
Iteration 132/1000 | Loss: 0.00001456
Iteration 133/1000 | Loss: 0.00001456
Iteration 134/1000 | Loss: 0.00001456
Iteration 135/1000 | Loss: 0.00001456
Iteration 136/1000 | Loss: 0.00001456
Iteration 137/1000 | Loss: 0.00001456
Iteration 138/1000 | Loss: 0.00001456
Iteration 139/1000 | Loss: 0.00001456
Iteration 140/1000 | Loss: 0.00001456
Iteration 141/1000 | Loss: 0.00001456
Iteration 142/1000 | Loss: 0.00001456
Iteration 143/1000 | Loss: 0.00001456
Iteration 144/1000 | Loss: 0.00001456
Iteration 145/1000 | Loss: 0.00001456
Iteration 146/1000 | Loss: 0.00001456
Iteration 147/1000 | Loss: 0.00001456
Iteration 148/1000 | Loss: 0.00001456
Iteration 149/1000 | Loss: 0.00001456
Iteration 150/1000 | Loss: 0.00001456
Iteration 151/1000 | Loss: 0.00001456
Iteration 152/1000 | Loss: 0.00001456
Iteration 153/1000 | Loss: 0.00001456
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 153. Stopping optimization.
Last 5 losses: [1.455796791560715e-05, 1.455796791560715e-05, 1.455796791560715e-05, 1.455796791560715e-05, 1.455796791560715e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.455796791560715e-05

Optimization complete. Final v2v error: 3.263183832168579 mm

Highest mean error: 3.469208002090454 mm for frame 0

Lowest mean error: 3.170311689376831 mm for frame 30

Saving results

Total time: 45.30724239349365
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1087/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1087.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1087
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00887606
Iteration 2/25 | Loss: 0.00085635
Iteration 3/25 | Loss: 0.00074533
Iteration 4/25 | Loss: 0.00072183
Iteration 5/25 | Loss: 0.00071448
Iteration 6/25 | Loss: 0.00071299
Iteration 7/25 | Loss: 0.00071268
Iteration 8/25 | Loss: 0.00071268
Iteration 9/25 | Loss: 0.00071268
Iteration 10/25 | Loss: 0.00071268
Iteration 11/25 | Loss: 0.00071268
Iteration 12/25 | Loss: 0.00071268
Iteration 13/25 | Loss: 0.00071268
Iteration 14/25 | Loss: 0.00071268
Iteration 15/25 | Loss: 0.00071268
Iteration 16/25 | Loss: 0.00071268
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0007126768468879163, 0.0007126768468879163, 0.0007126768468879163, 0.0007126768468879163, 0.0007126768468879163]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007126768468879163

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.84178245
Iteration 2/25 | Loss: 0.00111253
Iteration 3/25 | Loss: 0.00111252
Iteration 4/25 | Loss: 0.00111252
Iteration 5/25 | Loss: 0.00111252
Iteration 6/25 | Loss: 0.00111252
Iteration 7/25 | Loss: 0.00111252
Iteration 8/25 | Loss: 0.00111252
Iteration 9/25 | Loss: 0.00111252
Iteration 10/25 | Loss: 0.00111252
Iteration 11/25 | Loss: 0.00111252
Iteration 12/25 | Loss: 0.00111252
Iteration 13/25 | Loss: 0.00111252
Iteration 14/25 | Loss: 0.00111252
Iteration 15/25 | Loss: 0.00111252
Iteration 16/25 | Loss: 0.00111252
Iteration 17/25 | Loss: 0.00111252
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0011125195305794477, 0.0011125195305794477, 0.0011125195305794477, 0.0011125195305794477, 0.0011125195305794477]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011125195305794477

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00111252
Iteration 2/1000 | Loss: 0.00002629
Iteration 3/1000 | Loss: 0.00001597
Iteration 4/1000 | Loss: 0.00001424
Iteration 5/1000 | Loss: 0.00001335
Iteration 6/1000 | Loss: 0.00001282
Iteration 7/1000 | Loss: 0.00001251
Iteration 8/1000 | Loss: 0.00001233
Iteration 9/1000 | Loss: 0.00001222
Iteration 10/1000 | Loss: 0.00001215
Iteration 11/1000 | Loss: 0.00001208
Iteration 12/1000 | Loss: 0.00001208
Iteration 13/1000 | Loss: 0.00001207
Iteration 14/1000 | Loss: 0.00001195
Iteration 15/1000 | Loss: 0.00001194
Iteration 16/1000 | Loss: 0.00001194
Iteration 17/1000 | Loss: 0.00001191
Iteration 18/1000 | Loss: 0.00001191
Iteration 19/1000 | Loss: 0.00001191
Iteration 20/1000 | Loss: 0.00001190
Iteration 21/1000 | Loss: 0.00001190
Iteration 22/1000 | Loss: 0.00001188
Iteration 23/1000 | Loss: 0.00001187
Iteration 24/1000 | Loss: 0.00001187
Iteration 25/1000 | Loss: 0.00001186
Iteration 26/1000 | Loss: 0.00001186
Iteration 27/1000 | Loss: 0.00001186
Iteration 28/1000 | Loss: 0.00001186
Iteration 29/1000 | Loss: 0.00001186
Iteration 30/1000 | Loss: 0.00001186
Iteration 31/1000 | Loss: 0.00001185
Iteration 32/1000 | Loss: 0.00001185
Iteration 33/1000 | Loss: 0.00001184
Iteration 34/1000 | Loss: 0.00001183
Iteration 35/1000 | Loss: 0.00001181
Iteration 36/1000 | Loss: 0.00001181
Iteration 37/1000 | Loss: 0.00001181
Iteration 38/1000 | Loss: 0.00001181
Iteration 39/1000 | Loss: 0.00001181
Iteration 40/1000 | Loss: 0.00001181
Iteration 41/1000 | Loss: 0.00001180
Iteration 42/1000 | Loss: 0.00001180
Iteration 43/1000 | Loss: 0.00001180
Iteration 44/1000 | Loss: 0.00001180
Iteration 45/1000 | Loss: 0.00001180
Iteration 46/1000 | Loss: 0.00001179
Iteration 47/1000 | Loss: 0.00001178
Iteration 48/1000 | Loss: 0.00001177
Iteration 49/1000 | Loss: 0.00001177
Iteration 50/1000 | Loss: 0.00001177
Iteration 51/1000 | Loss: 0.00001176
Iteration 52/1000 | Loss: 0.00001176
Iteration 53/1000 | Loss: 0.00001176
Iteration 54/1000 | Loss: 0.00001175
Iteration 55/1000 | Loss: 0.00001175
Iteration 56/1000 | Loss: 0.00001175
Iteration 57/1000 | Loss: 0.00001175
Iteration 58/1000 | Loss: 0.00001175
Iteration 59/1000 | Loss: 0.00001175
Iteration 60/1000 | Loss: 0.00001175
Iteration 61/1000 | Loss: 0.00001175
Iteration 62/1000 | Loss: 0.00001175
Iteration 63/1000 | Loss: 0.00001174
Iteration 64/1000 | Loss: 0.00001174
Iteration 65/1000 | Loss: 0.00001174
Iteration 66/1000 | Loss: 0.00001174
Iteration 67/1000 | Loss: 0.00001174
Iteration 68/1000 | Loss: 0.00001174
Iteration 69/1000 | Loss: 0.00001174
Iteration 70/1000 | Loss: 0.00001174
Iteration 71/1000 | Loss: 0.00001174
Iteration 72/1000 | Loss: 0.00001174
Iteration 73/1000 | Loss: 0.00001174
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001174
Iteration 77/1000 | Loss: 0.00001174
Iteration 78/1000 | Loss: 0.00001174
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001172
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001171
Iteration 87/1000 | Loss: 0.00001171
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001170
Iteration 90/1000 | Loss: 0.00001170
Iteration 91/1000 | Loss: 0.00001170
Iteration 92/1000 | Loss: 0.00001170
Iteration 93/1000 | Loss: 0.00001169
Iteration 94/1000 | Loss: 0.00001169
Iteration 95/1000 | Loss: 0.00001168
Iteration 96/1000 | Loss: 0.00001166
Iteration 97/1000 | Loss: 0.00001165
Iteration 98/1000 | Loss: 0.00001165
Iteration 99/1000 | Loss: 0.00001165
Iteration 100/1000 | Loss: 0.00001165
Iteration 101/1000 | Loss: 0.00001165
Iteration 102/1000 | Loss: 0.00001164
Iteration 103/1000 | Loss: 0.00001164
Iteration 104/1000 | Loss: 0.00001164
Iteration 105/1000 | Loss: 0.00001163
Iteration 106/1000 | Loss: 0.00001163
Iteration 107/1000 | Loss: 0.00001163
Iteration 108/1000 | Loss: 0.00001163
Iteration 109/1000 | Loss: 0.00001162
Iteration 110/1000 | Loss: 0.00001162
Iteration 111/1000 | Loss: 0.00001162
Iteration 112/1000 | Loss: 0.00001162
Iteration 113/1000 | Loss: 0.00001162
Iteration 114/1000 | Loss: 0.00001162
Iteration 115/1000 | Loss: 0.00001162
Iteration 116/1000 | Loss: 0.00001161
Iteration 117/1000 | Loss: 0.00001161
Iteration 118/1000 | Loss: 0.00001161
Iteration 119/1000 | Loss: 0.00001161
Iteration 120/1000 | Loss: 0.00001161
Iteration 121/1000 | Loss: 0.00001161
Iteration 122/1000 | Loss: 0.00001161
Iteration 123/1000 | Loss: 0.00001161
Iteration 124/1000 | Loss: 0.00001161
Iteration 125/1000 | Loss: 0.00001160
Iteration 126/1000 | Loss: 0.00001160
Iteration 127/1000 | Loss: 0.00001160
Iteration 128/1000 | Loss: 0.00001160
Iteration 129/1000 | Loss: 0.00001160
Iteration 130/1000 | Loss: 0.00001160
Iteration 131/1000 | Loss: 0.00001160
Iteration 132/1000 | Loss: 0.00001160
Iteration 133/1000 | Loss: 0.00001160
Iteration 134/1000 | Loss: 0.00001160
Iteration 135/1000 | Loss: 0.00001159
Iteration 136/1000 | Loss: 0.00001159
Iteration 137/1000 | Loss: 0.00001159
Iteration 138/1000 | Loss: 0.00001159
Iteration 139/1000 | Loss: 0.00001159
Iteration 140/1000 | Loss: 0.00001159
Iteration 141/1000 | Loss: 0.00001159
Iteration 142/1000 | Loss: 0.00001159
Iteration 143/1000 | Loss: 0.00001159
Iteration 144/1000 | Loss: 0.00001159
Iteration 145/1000 | Loss: 0.00001159
Iteration 146/1000 | Loss: 0.00001159
Iteration 147/1000 | Loss: 0.00001159
Iteration 148/1000 | Loss: 0.00001159
Iteration 149/1000 | Loss: 0.00001159
Iteration 150/1000 | Loss: 0.00001159
Iteration 151/1000 | Loss: 0.00001159
Iteration 152/1000 | Loss: 0.00001159
Iteration 153/1000 | Loss: 0.00001159
Iteration 154/1000 | Loss: 0.00001159
Iteration 155/1000 | Loss: 0.00001159
Iteration 156/1000 | Loss: 0.00001159
Iteration 157/1000 | Loss: 0.00001159
Iteration 158/1000 | Loss: 0.00001159
Iteration 159/1000 | Loss: 0.00001159
Iteration 160/1000 | Loss: 0.00001159
Iteration 161/1000 | Loss: 0.00001159
Iteration 162/1000 | Loss: 0.00001159
Iteration 163/1000 | Loss: 0.00001159
Iteration 164/1000 | Loss: 0.00001159
Iteration 165/1000 | Loss: 0.00001159
Iteration 166/1000 | Loss: 0.00001159
Iteration 167/1000 | Loss: 0.00001159
Iteration 168/1000 | Loss: 0.00001159
Iteration 169/1000 | Loss: 0.00001159
Iteration 170/1000 | Loss: 0.00001159
Iteration 171/1000 | Loss: 0.00001159
Iteration 172/1000 | Loss: 0.00001159
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 172. Stopping optimization.
Last 5 losses: [1.1587871085794177e-05, 1.1587871085794177e-05, 1.1587871085794177e-05, 1.1587871085794177e-05, 1.1587871085794177e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1587871085794177e-05

Optimization complete. Final v2v error: 2.9168436527252197 mm

Highest mean error: 3.53779673576355 mm for frame 44

Lowest mean error: 2.79940128326416 mm for frame 102

Saving results

Total time: 35.077821254730225
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1035/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1035.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1035
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00378210
Iteration 2/25 | Loss: 0.00087398
Iteration 3/25 | Loss: 0.00077390
Iteration 4/25 | Loss: 0.00075722
Iteration 5/25 | Loss: 0.00075198
Iteration 6/25 | Loss: 0.00075057
Iteration 7/25 | Loss: 0.00075024
Iteration 8/25 | Loss: 0.00075024
Iteration 9/25 | Loss: 0.00075024
Iteration 10/25 | Loss: 0.00075024
Iteration 11/25 | Loss: 0.00075024
Iteration 12/25 | Loss: 0.00075024
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007502418593503535, 0.0007502418593503535, 0.0007502418593503535, 0.0007502418593503535, 0.0007502418593503535]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007502418593503535

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60461986
Iteration 2/25 | Loss: 0.00130879
Iteration 3/25 | Loss: 0.00130879
Iteration 4/25 | Loss: 0.00130879
Iteration 5/25 | Loss: 0.00130879
Iteration 6/25 | Loss: 0.00130879
Iteration 7/25 | Loss: 0.00130879
Iteration 8/25 | Loss: 0.00130879
Iteration 9/25 | Loss: 0.00130879
Iteration 10/25 | Loss: 0.00130879
Iteration 11/25 | Loss: 0.00130878
Iteration 12/25 | Loss: 0.00130878
Iteration 13/25 | Loss: 0.00130878
Iteration 14/25 | Loss: 0.00130878
Iteration 15/25 | Loss: 0.00130878
Iteration 16/25 | Loss: 0.00130878
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0013087847037240863, 0.0013087847037240863, 0.0013087847037240863, 0.0013087847037240863, 0.0013087847037240863]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013087847037240863

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00130878
Iteration 2/1000 | Loss: 0.00002292
Iteration 3/1000 | Loss: 0.00001499
Iteration 4/1000 | Loss: 0.00001377
Iteration 5/1000 | Loss: 0.00001310
Iteration 6/1000 | Loss: 0.00001282
Iteration 7/1000 | Loss: 0.00001260
Iteration 8/1000 | Loss: 0.00001260
Iteration 9/1000 | Loss: 0.00001258
Iteration 10/1000 | Loss: 0.00001256
Iteration 11/1000 | Loss: 0.00001254
Iteration 12/1000 | Loss: 0.00001251
Iteration 13/1000 | Loss: 0.00001241
Iteration 14/1000 | Loss: 0.00001226
Iteration 15/1000 | Loss: 0.00001224
Iteration 16/1000 | Loss: 0.00001224
Iteration 17/1000 | Loss: 0.00001221
Iteration 18/1000 | Loss: 0.00001220
Iteration 19/1000 | Loss: 0.00001219
Iteration 20/1000 | Loss: 0.00001218
Iteration 21/1000 | Loss: 0.00001218
Iteration 22/1000 | Loss: 0.00001218
Iteration 23/1000 | Loss: 0.00001217
Iteration 24/1000 | Loss: 0.00001217
Iteration 25/1000 | Loss: 0.00001215
Iteration 26/1000 | Loss: 0.00001215
Iteration 27/1000 | Loss: 0.00001214
Iteration 28/1000 | Loss: 0.00001214
Iteration 29/1000 | Loss: 0.00001214
Iteration 30/1000 | Loss: 0.00001213
Iteration 31/1000 | Loss: 0.00001213
Iteration 32/1000 | Loss: 0.00001212
Iteration 33/1000 | Loss: 0.00001211
Iteration 34/1000 | Loss: 0.00001211
Iteration 35/1000 | Loss: 0.00001211
Iteration 36/1000 | Loss: 0.00001209
Iteration 37/1000 | Loss: 0.00001209
Iteration 38/1000 | Loss: 0.00001209
Iteration 39/1000 | Loss: 0.00001208
Iteration 40/1000 | Loss: 0.00001208
Iteration 41/1000 | Loss: 0.00001208
Iteration 42/1000 | Loss: 0.00001207
Iteration 43/1000 | Loss: 0.00001207
Iteration 44/1000 | Loss: 0.00001206
Iteration 45/1000 | Loss: 0.00001206
Iteration 46/1000 | Loss: 0.00001206
Iteration 47/1000 | Loss: 0.00001206
Iteration 48/1000 | Loss: 0.00001205
Iteration 49/1000 | Loss: 0.00001205
Iteration 50/1000 | Loss: 0.00001205
Iteration 51/1000 | Loss: 0.00001205
Iteration 52/1000 | Loss: 0.00001205
Iteration 53/1000 | Loss: 0.00001205
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001205
Iteration 56/1000 | Loss: 0.00001205
Iteration 57/1000 | Loss: 0.00001205
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001204
Iteration 62/1000 | Loss: 0.00001204
Iteration 63/1000 | Loss: 0.00001204
Iteration 64/1000 | Loss: 0.00001204
Iteration 65/1000 | Loss: 0.00001204
Iteration 66/1000 | Loss: 0.00001204
Iteration 67/1000 | Loss: 0.00001203
Iteration 68/1000 | Loss: 0.00001203
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001203
Iteration 72/1000 | Loss: 0.00001203
Iteration 73/1000 | Loss: 0.00001203
Iteration 74/1000 | Loss: 0.00001203
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001202
Iteration 81/1000 | Loss: 0.00001202
Iteration 82/1000 | Loss: 0.00001202
Iteration 83/1000 | Loss: 0.00001202
Iteration 84/1000 | Loss: 0.00001201
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001201
Iteration 88/1000 | Loss: 0.00001201
Iteration 89/1000 | Loss: 0.00001201
Iteration 90/1000 | Loss: 0.00001201
Iteration 91/1000 | Loss: 0.00001201
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001200
Iteration 99/1000 | Loss: 0.00001200
Iteration 100/1000 | Loss: 0.00001199
Iteration 101/1000 | Loss: 0.00001199
Iteration 102/1000 | Loss: 0.00001199
Iteration 103/1000 | Loss: 0.00001199
Iteration 104/1000 | Loss: 0.00001199
Iteration 105/1000 | Loss: 0.00001199
Iteration 106/1000 | Loss: 0.00001199
Iteration 107/1000 | Loss: 0.00001199
Iteration 108/1000 | Loss: 0.00001199
Iteration 109/1000 | Loss: 0.00001199
Iteration 110/1000 | Loss: 0.00001199
Iteration 111/1000 | Loss: 0.00001199
Iteration 112/1000 | Loss: 0.00001198
Iteration 113/1000 | Loss: 0.00001198
Iteration 114/1000 | Loss: 0.00001198
Iteration 115/1000 | Loss: 0.00001198
Iteration 116/1000 | Loss: 0.00001198
Iteration 117/1000 | Loss: 0.00001197
Iteration 118/1000 | Loss: 0.00001197
Iteration 119/1000 | Loss: 0.00001197
Iteration 120/1000 | Loss: 0.00001197
Iteration 121/1000 | Loss: 0.00001197
Iteration 122/1000 | Loss: 0.00001196
Iteration 123/1000 | Loss: 0.00001196
Iteration 124/1000 | Loss: 0.00001196
Iteration 125/1000 | Loss: 0.00001196
Iteration 126/1000 | Loss: 0.00001195
Iteration 127/1000 | Loss: 0.00001195
Iteration 128/1000 | Loss: 0.00001195
Iteration 129/1000 | Loss: 0.00001195
Iteration 130/1000 | Loss: 0.00001195
Iteration 131/1000 | Loss: 0.00001195
Iteration 132/1000 | Loss: 0.00001195
Iteration 133/1000 | Loss: 0.00001195
Iteration 134/1000 | Loss: 0.00001195
Iteration 135/1000 | Loss: 0.00001195
Iteration 136/1000 | Loss: 0.00001195
Iteration 137/1000 | Loss: 0.00001195
Iteration 138/1000 | Loss: 0.00001195
Iteration 139/1000 | Loss: 0.00001195
Iteration 140/1000 | Loss: 0.00001195
Iteration 141/1000 | Loss: 0.00001195
Iteration 142/1000 | Loss: 0.00001195
Iteration 143/1000 | Loss: 0.00001195
Iteration 144/1000 | Loss: 0.00001195
Iteration 145/1000 | Loss: 0.00001195
Iteration 146/1000 | Loss: 0.00001195
Iteration 147/1000 | Loss: 0.00001195
Iteration 148/1000 | Loss: 0.00001195
Iteration 149/1000 | Loss: 0.00001195
Iteration 150/1000 | Loss: 0.00001195
Iteration 151/1000 | Loss: 0.00001195
Iteration 152/1000 | Loss: 0.00001195
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 152. Stopping optimization.
Last 5 losses: [1.1945109690714162e-05, 1.1945109690714162e-05, 1.1945109690714162e-05, 1.1945109690714162e-05, 1.1945109690714162e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1945109690714162e-05

Optimization complete. Final v2v error: 2.9037721157073975 mm

Highest mean error: 3.1213855743408203 mm for frame 148

Lowest mean error: 2.6731762886047363 mm for frame 98

Saving results

Total time: 33.22401142120361
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1093/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1093.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1093
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00813495
Iteration 2/25 | Loss: 0.00100594
Iteration 3/25 | Loss: 0.00081838
Iteration 4/25 | Loss: 0.00078092
Iteration 5/25 | Loss: 0.00077610
Iteration 6/25 | Loss: 0.00077570
Iteration 7/25 | Loss: 0.00077570
Iteration 8/25 | Loss: 0.00077570
Iteration 9/25 | Loss: 0.00077570
Iteration 10/25 | Loss: 0.00077570
Iteration 11/25 | Loss: 0.00077570
Iteration 12/25 | Loss: 0.00077570
Iteration 13/25 | Loss: 0.00077570
Iteration 14/25 | Loss: 0.00077570
Iteration 15/25 | Loss: 0.00077570
Iteration 16/25 | Loss: 0.00077570
Iteration 17/25 | Loss: 0.00077570
Iteration 18/25 | Loss: 0.00077570
Iteration 19/25 | Loss: 0.00077570
Iteration 20/25 | Loss: 0.00077570
Iteration 21/25 | Loss: 0.00077570
Iteration 22/25 | Loss: 0.00077570
Iteration 23/25 | Loss: 0.00077570
Iteration 24/25 | Loss: 0.00077570
Iteration 25/25 | Loss: 0.00077570

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.56548548
Iteration 2/25 | Loss: 0.00112724
Iteration 3/25 | Loss: 0.00112719
Iteration 4/25 | Loss: 0.00112719
Iteration 5/25 | Loss: 0.00112719
Iteration 6/25 | Loss: 0.00112719
Iteration 7/25 | Loss: 0.00112719
Iteration 8/25 | Loss: 0.00112719
Iteration 9/25 | Loss: 0.00112719
Iteration 10/25 | Loss: 0.00112719
Iteration 11/25 | Loss: 0.00112719
Iteration 12/25 | Loss: 0.00112719
Iteration 13/25 | Loss: 0.00112719
Iteration 14/25 | Loss: 0.00112719
Iteration 15/25 | Loss: 0.00112719
Iteration 16/25 | Loss: 0.00112719
Iteration 17/25 | Loss: 0.00112719
Iteration 18/25 | Loss: 0.00112719
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001127187511883676, 0.001127187511883676, 0.001127187511883676, 0.001127187511883676, 0.001127187511883676]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001127187511883676

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112719
Iteration 2/1000 | Loss: 0.00003654
Iteration 3/1000 | Loss: 0.00002798
Iteration 4/1000 | Loss: 0.00002357
Iteration 5/1000 | Loss: 0.00002242
Iteration 6/1000 | Loss: 0.00002117
Iteration 7/1000 | Loss: 0.00002031
Iteration 8/1000 | Loss: 0.00001965
Iteration 9/1000 | Loss: 0.00001946
Iteration 10/1000 | Loss: 0.00001916
Iteration 11/1000 | Loss: 0.00001900
Iteration 12/1000 | Loss: 0.00001883
Iteration 13/1000 | Loss: 0.00001880
Iteration 14/1000 | Loss: 0.00001879
Iteration 15/1000 | Loss: 0.00001878
Iteration 16/1000 | Loss: 0.00001878
Iteration 17/1000 | Loss: 0.00001878
Iteration 18/1000 | Loss: 0.00001866
Iteration 19/1000 | Loss: 0.00001865
Iteration 20/1000 | Loss: 0.00001865
Iteration 21/1000 | Loss: 0.00001864
Iteration 22/1000 | Loss: 0.00001863
Iteration 23/1000 | Loss: 0.00001862
Iteration 24/1000 | Loss: 0.00001861
Iteration 25/1000 | Loss: 0.00001861
Iteration 26/1000 | Loss: 0.00001860
Iteration 27/1000 | Loss: 0.00001860
Iteration 28/1000 | Loss: 0.00001860
Iteration 29/1000 | Loss: 0.00001859
Iteration 30/1000 | Loss: 0.00001859
Iteration 31/1000 | Loss: 0.00001859
Iteration 32/1000 | Loss: 0.00001858
Iteration 33/1000 | Loss: 0.00001858
Iteration 34/1000 | Loss: 0.00001857
Iteration 35/1000 | Loss: 0.00001857
Iteration 36/1000 | Loss: 0.00001857
Iteration 37/1000 | Loss: 0.00001857
Iteration 38/1000 | Loss: 0.00001857
Iteration 39/1000 | Loss: 0.00001857
Iteration 40/1000 | Loss: 0.00001857
Iteration 41/1000 | Loss: 0.00001857
Iteration 42/1000 | Loss: 0.00001857
Iteration 43/1000 | Loss: 0.00001857
Iteration 44/1000 | Loss: 0.00001857
Iteration 45/1000 | Loss: 0.00001857
Iteration 46/1000 | Loss: 0.00001856
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00001856
Iteration 49/1000 | Loss: 0.00001856
Iteration 50/1000 | Loss: 0.00001856
Iteration 51/1000 | Loss: 0.00001856
Iteration 52/1000 | Loss: 0.00001856
Iteration 53/1000 | Loss: 0.00001856
Iteration 54/1000 | Loss: 0.00001856
Iteration 55/1000 | Loss: 0.00001856
Iteration 56/1000 | Loss: 0.00001856
Iteration 57/1000 | Loss: 0.00001855
Iteration 58/1000 | Loss: 0.00001855
Iteration 59/1000 | Loss: 0.00001855
Iteration 60/1000 | Loss: 0.00001855
Iteration 61/1000 | Loss: 0.00001855
Iteration 62/1000 | Loss: 0.00001855
Iteration 63/1000 | Loss: 0.00001855
Iteration 64/1000 | Loss: 0.00001855
Iteration 65/1000 | Loss: 0.00001855
Iteration 66/1000 | Loss: 0.00001855
Iteration 67/1000 | Loss: 0.00001854
Iteration 68/1000 | Loss: 0.00001854
Iteration 69/1000 | Loss: 0.00001854
Iteration 70/1000 | Loss: 0.00001854
Iteration 71/1000 | Loss: 0.00001854
Iteration 72/1000 | Loss: 0.00001854
Iteration 73/1000 | Loss: 0.00001854
Iteration 74/1000 | Loss: 0.00001854
Iteration 75/1000 | Loss: 0.00001853
Iteration 76/1000 | Loss: 0.00001853
Iteration 77/1000 | Loss: 0.00001853
Iteration 78/1000 | Loss: 0.00001853
Iteration 79/1000 | Loss: 0.00001853
Iteration 80/1000 | Loss: 0.00001853
Iteration 81/1000 | Loss: 0.00001853
Iteration 82/1000 | Loss: 0.00001852
Iteration 83/1000 | Loss: 0.00001852
Iteration 84/1000 | Loss: 0.00001852
Iteration 85/1000 | Loss: 0.00001852
Iteration 86/1000 | Loss: 0.00001852
Iteration 87/1000 | Loss: 0.00001852
Iteration 88/1000 | Loss: 0.00001852
Iteration 89/1000 | Loss: 0.00001852
Iteration 90/1000 | Loss: 0.00001852
Iteration 91/1000 | Loss: 0.00001852
Iteration 92/1000 | Loss: 0.00001852
Iteration 93/1000 | Loss: 0.00001852
Iteration 94/1000 | Loss: 0.00001852
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 94. Stopping optimization.
Last 5 losses: [1.8520531739341095e-05, 1.8520531739341095e-05, 1.8520531739341095e-05, 1.8520531739341095e-05, 1.8520531739341095e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8520531739341095e-05

Optimization complete. Final v2v error: 3.6386992931365967 mm

Highest mean error: 3.975555658340454 mm for frame 218

Lowest mean error: 3.3528244495391846 mm for frame 61

Saving results

Total time: 36.99477744102478
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1098/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1098.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1098
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407267
Iteration 2/25 | Loss: 0.00098223
Iteration 3/25 | Loss: 0.00080759
Iteration 4/25 | Loss: 0.00077216
Iteration 5/25 | Loss: 0.00076173
Iteration 6/25 | Loss: 0.00075971
Iteration 7/25 | Loss: 0.00075906
Iteration 8/25 | Loss: 0.00075905
Iteration 9/25 | Loss: 0.00075905
Iteration 10/25 | Loss: 0.00075903
Iteration 11/25 | Loss: 0.00075903
Iteration 12/25 | Loss: 0.00075903
Iteration 13/25 | Loss: 0.00075903
Iteration 14/25 | Loss: 0.00075903
Iteration 15/25 | Loss: 0.00075903
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007590338937006891, 0.0007590338937006891, 0.0007590338937006891, 0.0007590338937006891, 0.0007590338937006891]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007590338937006891

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66007364
Iteration 2/25 | Loss: 0.00145665
Iteration 3/25 | Loss: 0.00145664
Iteration 4/25 | Loss: 0.00145664
Iteration 5/25 | Loss: 0.00145664
Iteration 6/25 | Loss: 0.00145664
Iteration 7/25 | Loss: 0.00145664
Iteration 8/25 | Loss: 0.00145664
Iteration 9/25 | Loss: 0.00145664
Iteration 10/25 | Loss: 0.00145664
Iteration 11/25 | Loss: 0.00145664
Iteration 12/25 | Loss: 0.00145664
Iteration 13/25 | Loss: 0.00145664
Iteration 14/25 | Loss: 0.00145664
Iteration 15/25 | Loss: 0.00145664
Iteration 16/25 | Loss: 0.00145664
Iteration 17/25 | Loss: 0.00145664
Iteration 18/25 | Loss: 0.00145664
Iteration 19/25 | Loss: 0.00145664
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0014566396130248904, 0.0014566396130248904, 0.0014566396130248904, 0.0014566396130248904, 0.0014566396130248904]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014566396130248904

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00145664
Iteration 2/1000 | Loss: 0.00003444
Iteration 3/1000 | Loss: 0.00002496
Iteration 4/1000 | Loss: 0.00001751
Iteration 5/1000 | Loss: 0.00001607
Iteration 6/1000 | Loss: 0.00001538
Iteration 7/1000 | Loss: 0.00001454
Iteration 8/1000 | Loss: 0.00001424
Iteration 9/1000 | Loss: 0.00001390
Iteration 10/1000 | Loss: 0.00001374
Iteration 11/1000 | Loss: 0.00001364
Iteration 12/1000 | Loss: 0.00001363
Iteration 13/1000 | Loss: 0.00001360
Iteration 14/1000 | Loss: 0.00001353
Iteration 15/1000 | Loss: 0.00001349
Iteration 16/1000 | Loss: 0.00001348
Iteration 17/1000 | Loss: 0.00001346
Iteration 18/1000 | Loss: 0.00001345
Iteration 19/1000 | Loss: 0.00001341
Iteration 20/1000 | Loss: 0.00001339
Iteration 21/1000 | Loss: 0.00001335
Iteration 22/1000 | Loss: 0.00001335
Iteration 23/1000 | Loss: 0.00001331
Iteration 24/1000 | Loss: 0.00001329
Iteration 25/1000 | Loss: 0.00001328
Iteration 26/1000 | Loss: 0.00001328
Iteration 27/1000 | Loss: 0.00001327
Iteration 28/1000 | Loss: 0.00001327
Iteration 29/1000 | Loss: 0.00001326
Iteration 30/1000 | Loss: 0.00001326
Iteration 31/1000 | Loss: 0.00001325
Iteration 32/1000 | Loss: 0.00001325
Iteration 33/1000 | Loss: 0.00001324
Iteration 34/1000 | Loss: 0.00001324
Iteration 35/1000 | Loss: 0.00001323
Iteration 36/1000 | Loss: 0.00001323
Iteration 37/1000 | Loss: 0.00001323
Iteration 38/1000 | Loss: 0.00001322
Iteration 39/1000 | Loss: 0.00001322
Iteration 40/1000 | Loss: 0.00001322
Iteration 41/1000 | Loss: 0.00001321
Iteration 42/1000 | Loss: 0.00001321
Iteration 43/1000 | Loss: 0.00001321
Iteration 44/1000 | Loss: 0.00001320
Iteration 45/1000 | Loss: 0.00001320
Iteration 46/1000 | Loss: 0.00001319
Iteration 47/1000 | Loss: 0.00001319
Iteration 48/1000 | Loss: 0.00001319
Iteration 49/1000 | Loss: 0.00001319
Iteration 50/1000 | Loss: 0.00001318
Iteration 51/1000 | Loss: 0.00001318
Iteration 52/1000 | Loss: 0.00001318
Iteration 53/1000 | Loss: 0.00001317
Iteration 54/1000 | Loss: 0.00001317
Iteration 55/1000 | Loss: 0.00001317
Iteration 56/1000 | Loss: 0.00001316
Iteration 57/1000 | Loss: 0.00001315
Iteration 58/1000 | Loss: 0.00001315
Iteration 59/1000 | Loss: 0.00001314
Iteration 60/1000 | Loss: 0.00001313
Iteration 61/1000 | Loss: 0.00001312
Iteration 62/1000 | Loss: 0.00001312
Iteration 63/1000 | Loss: 0.00001312
Iteration 64/1000 | Loss: 0.00001312
Iteration 65/1000 | Loss: 0.00001312
Iteration 66/1000 | Loss: 0.00001311
Iteration 67/1000 | Loss: 0.00001311
Iteration 68/1000 | Loss: 0.00001311
Iteration 69/1000 | Loss: 0.00001310
Iteration 70/1000 | Loss: 0.00001310
Iteration 71/1000 | Loss: 0.00001310
Iteration 72/1000 | Loss: 0.00001310
Iteration 73/1000 | Loss: 0.00001310
Iteration 74/1000 | Loss: 0.00001310
Iteration 75/1000 | Loss: 0.00001309
Iteration 76/1000 | Loss: 0.00001309
Iteration 77/1000 | Loss: 0.00001309
Iteration 78/1000 | Loss: 0.00001309
Iteration 79/1000 | Loss: 0.00001309
Iteration 80/1000 | Loss: 0.00001309
Iteration 81/1000 | Loss: 0.00001309
Iteration 82/1000 | Loss: 0.00001309
Iteration 83/1000 | Loss: 0.00001309
Iteration 84/1000 | Loss: 0.00001309
Iteration 85/1000 | Loss: 0.00001309
Iteration 86/1000 | Loss: 0.00001309
Iteration 87/1000 | Loss: 0.00001308
Iteration 88/1000 | Loss: 0.00001308
Iteration 89/1000 | Loss: 0.00001308
Iteration 90/1000 | Loss: 0.00001308
Iteration 91/1000 | Loss: 0.00001308
Iteration 92/1000 | Loss: 0.00001308
Iteration 93/1000 | Loss: 0.00001308
Iteration 94/1000 | Loss: 0.00001308
Iteration 95/1000 | Loss: 0.00001308
Iteration 96/1000 | Loss: 0.00001308
Iteration 97/1000 | Loss: 0.00001308
Iteration 98/1000 | Loss: 0.00001308
Iteration 99/1000 | Loss: 0.00001308
Iteration 100/1000 | Loss: 0.00001308
Iteration 101/1000 | Loss: 0.00001307
Iteration 102/1000 | Loss: 0.00001307
Iteration 103/1000 | Loss: 0.00001307
Iteration 104/1000 | Loss: 0.00001307
Iteration 105/1000 | Loss: 0.00001307
Iteration 106/1000 | Loss: 0.00001307
Iteration 107/1000 | Loss: 0.00001307
Iteration 108/1000 | Loss: 0.00001307
Iteration 109/1000 | Loss: 0.00001307
Iteration 110/1000 | Loss: 0.00001307
Iteration 111/1000 | Loss: 0.00001307
Iteration 112/1000 | Loss: 0.00001307
Iteration 113/1000 | Loss: 0.00001307
Iteration 114/1000 | Loss: 0.00001307
Iteration 115/1000 | Loss: 0.00001306
Iteration 116/1000 | Loss: 0.00001306
Iteration 117/1000 | Loss: 0.00001306
Iteration 118/1000 | Loss: 0.00001306
Iteration 119/1000 | Loss: 0.00001306
Iteration 120/1000 | Loss: 0.00001306
Iteration 121/1000 | Loss: 0.00001306
Iteration 122/1000 | Loss: 0.00001306
Iteration 123/1000 | Loss: 0.00001306
Iteration 124/1000 | Loss: 0.00001306
Iteration 125/1000 | Loss: 0.00001306
Iteration 126/1000 | Loss: 0.00001306
Iteration 127/1000 | Loss: 0.00001306
Iteration 128/1000 | Loss: 0.00001305
Iteration 129/1000 | Loss: 0.00001305
Iteration 130/1000 | Loss: 0.00001305
Iteration 131/1000 | Loss: 0.00001305
Iteration 132/1000 | Loss: 0.00001305
Iteration 133/1000 | Loss: 0.00001305
Iteration 134/1000 | Loss: 0.00001305
Iteration 135/1000 | Loss: 0.00001305
Iteration 136/1000 | Loss: 0.00001304
Iteration 137/1000 | Loss: 0.00001304
Iteration 138/1000 | Loss: 0.00001304
Iteration 139/1000 | Loss: 0.00001304
Iteration 140/1000 | Loss: 0.00001304
Iteration 141/1000 | Loss: 0.00001304
Iteration 142/1000 | Loss: 0.00001304
Iteration 143/1000 | Loss: 0.00001304
Iteration 144/1000 | Loss: 0.00001304
Iteration 145/1000 | Loss: 0.00001304
Iteration 146/1000 | Loss: 0.00001304
Iteration 147/1000 | Loss: 0.00001303
Iteration 148/1000 | Loss: 0.00001303
Iteration 149/1000 | Loss: 0.00001303
Iteration 150/1000 | Loss: 0.00001303
Iteration 151/1000 | Loss: 0.00001303
Iteration 152/1000 | Loss: 0.00001303
Iteration 153/1000 | Loss: 0.00001303
Iteration 154/1000 | Loss: 0.00001303
Iteration 155/1000 | Loss: 0.00001303
Iteration 156/1000 | Loss: 0.00001303
Iteration 157/1000 | Loss: 0.00001303
Iteration 158/1000 | Loss: 0.00001303
Iteration 159/1000 | Loss: 0.00001303
Iteration 160/1000 | Loss: 0.00001303
Iteration 161/1000 | Loss: 0.00001303
Iteration 162/1000 | Loss: 0.00001303
Iteration 163/1000 | Loss: 0.00001303
Iteration 164/1000 | Loss: 0.00001303
Iteration 165/1000 | Loss: 0.00001302
Iteration 166/1000 | Loss: 0.00001302
Iteration 167/1000 | Loss: 0.00001302
Iteration 168/1000 | Loss: 0.00001302
Iteration 169/1000 | Loss: 0.00001302
Iteration 170/1000 | Loss: 0.00001302
Iteration 171/1000 | Loss: 0.00001302
Iteration 172/1000 | Loss: 0.00001302
Iteration 173/1000 | Loss: 0.00001302
Iteration 174/1000 | Loss: 0.00001302
Iteration 175/1000 | Loss: 0.00001302
Iteration 176/1000 | Loss: 0.00001302
Iteration 177/1000 | Loss: 0.00001302
Iteration 178/1000 | Loss: 0.00001302
Iteration 179/1000 | Loss: 0.00001302
Iteration 180/1000 | Loss: 0.00001302
Iteration 181/1000 | Loss: 0.00001302
Iteration 182/1000 | Loss: 0.00001302
Iteration 183/1000 | Loss: 0.00001302
Iteration 184/1000 | Loss: 0.00001302
Iteration 185/1000 | Loss: 0.00001302
Iteration 186/1000 | Loss: 0.00001302
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 186. Stopping optimization.
Last 5 losses: [1.3018092431593686e-05, 1.3018092431593686e-05, 1.3018092431593686e-05, 1.3018092431593686e-05, 1.3018092431593686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3018092431593686e-05

Optimization complete. Final v2v error: 3.0685038566589355 mm

Highest mean error: 3.576604127883911 mm for frame 106

Lowest mean error: 2.7635271549224854 mm for frame 151

Saving results

Total time: 40.25622797012329
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00968194
Iteration 2/25 | Loss: 0.00130335
Iteration 3/25 | Loss: 0.00095807
Iteration 4/25 | Loss: 0.00091127
Iteration 5/25 | Loss: 0.00090181
Iteration 6/25 | Loss: 0.00090035
Iteration 7/25 | Loss: 0.00090002
Iteration 8/25 | Loss: 0.00090002
Iteration 9/25 | Loss: 0.00090002
Iteration 10/25 | Loss: 0.00090002
Iteration 11/25 | Loss: 0.00090002
Iteration 12/25 | Loss: 0.00090002
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009000158170238137, 0.0009000158170238137, 0.0009000158170238137, 0.0009000158170238137, 0.0009000158170238137]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009000158170238137

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.14895439
Iteration 2/25 | Loss: 0.00118998
Iteration 3/25 | Loss: 0.00118998
Iteration 4/25 | Loss: 0.00118998
Iteration 5/25 | Loss: 0.00118998
Iteration 6/25 | Loss: 0.00118998
Iteration 7/25 | Loss: 0.00118998
Iteration 8/25 | Loss: 0.00118998
Iteration 9/25 | Loss: 0.00118998
Iteration 10/25 | Loss: 0.00118998
Iteration 11/25 | Loss: 0.00118998
Iteration 12/25 | Loss: 0.00118998
Iteration 13/25 | Loss: 0.00118998
Iteration 14/25 | Loss: 0.00118998
Iteration 15/25 | Loss: 0.00118998
Iteration 16/25 | Loss: 0.00118998
Iteration 17/25 | Loss: 0.00118998
Iteration 18/25 | Loss: 0.00118998
Iteration 19/25 | Loss: 0.00118998
Iteration 20/25 | Loss: 0.00118998
Iteration 21/25 | Loss: 0.00118998
Iteration 22/25 | Loss: 0.00118998
Iteration 23/25 | Loss: 0.00118998
Iteration 24/25 | Loss: 0.00118998
Iteration 25/25 | Loss: 0.00118998

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118998
Iteration 2/1000 | Loss: 0.00004618
Iteration 3/1000 | Loss: 0.00003100
Iteration 4/1000 | Loss: 0.00002754
Iteration 5/1000 | Loss: 0.00002638
Iteration 6/1000 | Loss: 0.00002551
Iteration 7/1000 | Loss: 0.00002508
Iteration 8/1000 | Loss: 0.00002470
Iteration 9/1000 | Loss: 0.00002435
Iteration 10/1000 | Loss: 0.00002410
Iteration 11/1000 | Loss: 0.00002394
Iteration 12/1000 | Loss: 0.00002387
Iteration 13/1000 | Loss: 0.00002378
Iteration 14/1000 | Loss: 0.00002375
Iteration 15/1000 | Loss: 0.00002374
Iteration 16/1000 | Loss: 0.00002373
Iteration 17/1000 | Loss: 0.00002371
Iteration 18/1000 | Loss: 0.00002369
Iteration 19/1000 | Loss: 0.00002369
Iteration 20/1000 | Loss: 0.00002364
Iteration 21/1000 | Loss: 0.00002364
Iteration 22/1000 | Loss: 0.00002363
Iteration 23/1000 | Loss: 0.00002362
Iteration 24/1000 | Loss: 0.00002362
Iteration 25/1000 | Loss: 0.00002362
Iteration 26/1000 | Loss: 0.00002362
Iteration 27/1000 | Loss: 0.00002361
Iteration 28/1000 | Loss: 0.00002361
Iteration 29/1000 | Loss: 0.00002361
Iteration 30/1000 | Loss: 0.00002361
Iteration 31/1000 | Loss: 0.00002360
Iteration 32/1000 | Loss: 0.00002360
Iteration 33/1000 | Loss: 0.00002360
Iteration 34/1000 | Loss: 0.00002360
Iteration 35/1000 | Loss: 0.00002360
Iteration 36/1000 | Loss: 0.00002360
Iteration 37/1000 | Loss: 0.00002360
Iteration 38/1000 | Loss: 0.00002360
Iteration 39/1000 | Loss: 0.00002360
Iteration 40/1000 | Loss: 0.00002360
Iteration 41/1000 | Loss: 0.00002360
Iteration 42/1000 | Loss: 0.00002360
Iteration 43/1000 | Loss: 0.00002360
Iteration 44/1000 | Loss: 0.00002359
Iteration 45/1000 | Loss: 0.00002359
Iteration 46/1000 | Loss: 0.00002359
Iteration 47/1000 | Loss: 0.00002359
Iteration 48/1000 | Loss: 0.00002358
Iteration 49/1000 | Loss: 0.00002358
Iteration 50/1000 | Loss: 0.00002358
Iteration 51/1000 | Loss: 0.00002358
Iteration 52/1000 | Loss: 0.00002358
Iteration 53/1000 | Loss: 0.00002358
Iteration 54/1000 | Loss: 0.00002358
Iteration 55/1000 | Loss: 0.00002358
Iteration 56/1000 | Loss: 0.00002357
Iteration 57/1000 | Loss: 0.00002357
Iteration 58/1000 | Loss: 0.00002357
Iteration 59/1000 | Loss: 0.00002357
Iteration 60/1000 | Loss: 0.00002356
Iteration 61/1000 | Loss: 0.00002356
Iteration 62/1000 | Loss: 0.00002356
Iteration 63/1000 | Loss: 0.00002355
Iteration 64/1000 | Loss: 0.00002355
Iteration 65/1000 | Loss: 0.00002355
Iteration 66/1000 | Loss: 0.00002355
Iteration 67/1000 | Loss: 0.00002355
Iteration 68/1000 | Loss: 0.00002354
Iteration 69/1000 | Loss: 0.00002354
Iteration 70/1000 | Loss: 0.00002354
Iteration 71/1000 | Loss: 0.00002354
Iteration 72/1000 | Loss: 0.00002354
Iteration 73/1000 | Loss: 0.00002353
Iteration 74/1000 | Loss: 0.00002353
Iteration 75/1000 | Loss: 0.00002353
Iteration 76/1000 | Loss: 0.00002353
Iteration 77/1000 | Loss: 0.00002353
Iteration 78/1000 | Loss: 0.00002353
Iteration 79/1000 | Loss: 0.00002353
Iteration 80/1000 | Loss: 0.00002352
Iteration 81/1000 | Loss: 0.00002352
Iteration 82/1000 | Loss: 0.00002352
Iteration 83/1000 | Loss: 0.00002352
Iteration 84/1000 | Loss: 0.00002351
Iteration 85/1000 | Loss: 0.00002351
Iteration 86/1000 | Loss: 0.00002351
Iteration 87/1000 | Loss: 0.00002351
Iteration 88/1000 | Loss: 0.00002351
Iteration 89/1000 | Loss: 0.00002351
Iteration 90/1000 | Loss: 0.00002351
Iteration 91/1000 | Loss: 0.00002350
Iteration 92/1000 | Loss: 0.00002350
Iteration 93/1000 | Loss: 0.00002350
Iteration 94/1000 | Loss: 0.00002350
Iteration 95/1000 | Loss: 0.00002350
Iteration 96/1000 | Loss: 0.00002350
Iteration 97/1000 | Loss: 0.00002349
Iteration 98/1000 | Loss: 0.00002349
Iteration 99/1000 | Loss: 0.00002349
Iteration 100/1000 | Loss: 0.00002349
Iteration 101/1000 | Loss: 0.00002349
Iteration 102/1000 | Loss: 0.00002348
Iteration 103/1000 | Loss: 0.00002348
Iteration 104/1000 | Loss: 0.00002348
Iteration 105/1000 | Loss: 0.00002348
Iteration 106/1000 | Loss: 0.00002348
Iteration 107/1000 | Loss: 0.00002347
Iteration 108/1000 | Loss: 0.00002347
Iteration 109/1000 | Loss: 0.00002347
Iteration 110/1000 | Loss: 0.00002347
Iteration 111/1000 | Loss: 0.00002347
Iteration 112/1000 | Loss: 0.00002347
Iteration 113/1000 | Loss: 0.00002347
Iteration 114/1000 | Loss: 0.00002347
Iteration 115/1000 | Loss: 0.00002347
Iteration 116/1000 | Loss: 0.00002346
Iteration 117/1000 | Loss: 0.00002346
Iteration 118/1000 | Loss: 0.00002346
Iteration 119/1000 | Loss: 0.00002346
Iteration 120/1000 | Loss: 0.00002346
Iteration 121/1000 | Loss: 0.00002346
Iteration 122/1000 | Loss: 0.00002346
Iteration 123/1000 | Loss: 0.00002345
Iteration 124/1000 | Loss: 0.00002345
Iteration 125/1000 | Loss: 0.00002345
Iteration 126/1000 | Loss: 0.00002345
Iteration 127/1000 | Loss: 0.00002345
Iteration 128/1000 | Loss: 0.00002345
Iteration 129/1000 | Loss: 0.00002344
Iteration 130/1000 | Loss: 0.00002344
Iteration 131/1000 | Loss: 0.00002343
Iteration 132/1000 | Loss: 0.00002343
Iteration 133/1000 | Loss: 0.00002343
Iteration 134/1000 | Loss: 0.00002342
Iteration 135/1000 | Loss: 0.00002342
Iteration 136/1000 | Loss: 0.00002342
Iteration 137/1000 | Loss: 0.00002342
Iteration 138/1000 | Loss: 0.00002342
Iteration 139/1000 | Loss: 0.00002342
Iteration 140/1000 | Loss: 0.00002342
Iteration 141/1000 | Loss: 0.00002342
Iteration 142/1000 | Loss: 0.00002342
Iteration 143/1000 | Loss: 0.00002342
Iteration 144/1000 | Loss: 0.00002341
Iteration 145/1000 | Loss: 0.00002341
Iteration 146/1000 | Loss: 0.00002340
Iteration 147/1000 | Loss: 0.00002340
Iteration 148/1000 | Loss: 0.00002340
Iteration 149/1000 | Loss: 0.00002340
Iteration 150/1000 | Loss: 0.00002339
Iteration 151/1000 | Loss: 0.00002339
Iteration 152/1000 | Loss: 0.00002339
Iteration 153/1000 | Loss: 0.00002338
Iteration 154/1000 | Loss: 0.00002338
Iteration 155/1000 | Loss: 0.00002338
Iteration 156/1000 | Loss: 0.00002338
Iteration 157/1000 | Loss: 0.00002337
Iteration 158/1000 | Loss: 0.00002337
Iteration 159/1000 | Loss: 0.00002337
Iteration 160/1000 | Loss: 0.00002337
Iteration 161/1000 | Loss: 0.00002337
Iteration 162/1000 | Loss: 0.00002337
Iteration 163/1000 | Loss: 0.00002337
Iteration 164/1000 | Loss: 0.00002337
Iteration 165/1000 | Loss: 0.00002337
Iteration 166/1000 | Loss: 0.00002337
Iteration 167/1000 | Loss: 0.00002336
Iteration 168/1000 | Loss: 0.00002336
Iteration 169/1000 | Loss: 0.00002336
Iteration 170/1000 | Loss: 0.00002336
Iteration 171/1000 | Loss: 0.00002335
Iteration 172/1000 | Loss: 0.00002335
Iteration 173/1000 | Loss: 0.00002335
Iteration 174/1000 | Loss: 0.00002335
Iteration 175/1000 | Loss: 0.00002335
Iteration 176/1000 | Loss: 0.00002335
Iteration 177/1000 | Loss: 0.00002335
Iteration 178/1000 | Loss: 0.00002335
Iteration 179/1000 | Loss: 0.00002335
Iteration 180/1000 | Loss: 0.00002334
Iteration 181/1000 | Loss: 0.00002334
Iteration 182/1000 | Loss: 0.00002334
Iteration 183/1000 | Loss: 0.00002334
Iteration 184/1000 | Loss: 0.00002334
Iteration 185/1000 | Loss: 0.00002333
Iteration 186/1000 | Loss: 0.00002333
Iteration 187/1000 | Loss: 0.00002333
Iteration 188/1000 | Loss: 0.00002333
Iteration 189/1000 | Loss: 0.00002333
Iteration 190/1000 | Loss: 0.00002333
Iteration 191/1000 | Loss: 0.00002333
Iteration 192/1000 | Loss: 0.00002333
Iteration 193/1000 | Loss: 0.00002332
Iteration 194/1000 | Loss: 0.00002332
Iteration 195/1000 | Loss: 0.00002332
Iteration 196/1000 | Loss: 0.00002332
Iteration 197/1000 | Loss: 0.00002332
Iteration 198/1000 | Loss: 0.00002332
Iteration 199/1000 | Loss: 0.00002332
Iteration 200/1000 | Loss: 0.00002332
Iteration 201/1000 | Loss: 0.00002332
Iteration 202/1000 | Loss: 0.00002332
Iteration 203/1000 | Loss: 0.00002332
Iteration 204/1000 | Loss: 0.00002332
Iteration 205/1000 | Loss: 0.00002332
Iteration 206/1000 | Loss: 0.00002332
Iteration 207/1000 | Loss: 0.00002332
Iteration 208/1000 | Loss: 0.00002332
Iteration 209/1000 | Loss: 0.00002332
Iteration 210/1000 | Loss: 0.00002332
Iteration 211/1000 | Loss: 0.00002332
Iteration 212/1000 | Loss: 0.00002332
Iteration 213/1000 | Loss: 0.00002332
Iteration 214/1000 | Loss: 0.00002332
Iteration 215/1000 | Loss: 0.00002332
Iteration 216/1000 | Loss: 0.00002332
Iteration 217/1000 | Loss: 0.00002332
Iteration 218/1000 | Loss: 0.00002332
Iteration 219/1000 | Loss: 0.00002332
Iteration 220/1000 | Loss: 0.00002332
Iteration 221/1000 | Loss: 0.00002332
Iteration 222/1000 | Loss: 0.00002332
Iteration 223/1000 | Loss: 0.00002332
Iteration 224/1000 | Loss: 0.00002332
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 224. Stopping optimization.
Last 5 losses: [2.332429721718654e-05, 2.332429721718654e-05, 2.332429721718654e-05, 2.332429721718654e-05, 2.332429721718654e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.332429721718654e-05

Optimization complete. Final v2v error: 3.946309804916382 mm

Highest mean error: 4.607542037963867 mm for frame 82

Lowest mean error: 3.4846436977386475 mm for frame 142

Saving results

Total time: 42.23149824142456
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00912231
Iteration 2/25 | Loss: 0.00126201
Iteration 3/25 | Loss: 0.00092584
Iteration 4/25 | Loss: 0.00088953
Iteration 5/25 | Loss: 0.00088322
Iteration 6/25 | Loss: 0.00088233
Iteration 7/25 | Loss: 0.00088233
Iteration 8/25 | Loss: 0.00088233
Iteration 9/25 | Loss: 0.00088233
Iteration 10/25 | Loss: 0.00088233
Iteration 11/25 | Loss: 0.00088233
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008823250536806881, 0.0008823250536806881, 0.0008823250536806881, 0.0008823250536806881, 0.0008823250536806881]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008823250536806881

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.09042847
Iteration 2/25 | Loss: 0.00112059
Iteration 3/25 | Loss: 0.00112058
Iteration 4/25 | Loss: 0.00112058
Iteration 5/25 | Loss: 0.00112058
Iteration 6/25 | Loss: 0.00112058
Iteration 7/25 | Loss: 0.00112058
Iteration 8/25 | Loss: 0.00112058
Iteration 9/25 | Loss: 0.00112058
Iteration 10/25 | Loss: 0.00112058
Iteration 11/25 | Loss: 0.00112058
Iteration 12/25 | Loss: 0.00112058
Iteration 13/25 | Loss: 0.00112058
Iteration 14/25 | Loss: 0.00112058
Iteration 15/25 | Loss: 0.00112058
Iteration 16/25 | Loss: 0.00112058
Iteration 17/25 | Loss: 0.00112058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00112057919614017, 0.00112057919614017, 0.00112057919614017, 0.00112057919614017, 0.00112057919614017]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00112057919614017

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00112058
Iteration 2/1000 | Loss: 0.00003536
Iteration 3/1000 | Loss: 0.00002603
Iteration 4/1000 | Loss: 0.00002299
Iteration 5/1000 | Loss: 0.00002218
Iteration 6/1000 | Loss: 0.00002129
Iteration 7/1000 | Loss: 0.00002061
Iteration 8/1000 | Loss: 0.00002010
Iteration 9/1000 | Loss: 0.00001993
Iteration 10/1000 | Loss: 0.00001974
Iteration 11/1000 | Loss: 0.00001973
Iteration 12/1000 | Loss: 0.00001973
Iteration 13/1000 | Loss: 0.00001956
Iteration 14/1000 | Loss: 0.00001949
Iteration 15/1000 | Loss: 0.00001945
Iteration 16/1000 | Loss: 0.00001944
Iteration 17/1000 | Loss: 0.00001943
Iteration 18/1000 | Loss: 0.00001942
Iteration 19/1000 | Loss: 0.00001939
Iteration 20/1000 | Loss: 0.00001938
Iteration 21/1000 | Loss: 0.00001938
Iteration 22/1000 | Loss: 0.00001938
Iteration 23/1000 | Loss: 0.00001938
Iteration 24/1000 | Loss: 0.00001938
Iteration 25/1000 | Loss: 0.00001938
Iteration 26/1000 | Loss: 0.00001938
Iteration 27/1000 | Loss: 0.00001937
Iteration 28/1000 | Loss: 0.00001937
Iteration 29/1000 | Loss: 0.00001936
Iteration 30/1000 | Loss: 0.00001936
Iteration 31/1000 | Loss: 0.00001936
Iteration 32/1000 | Loss: 0.00001936
Iteration 33/1000 | Loss: 0.00001936
Iteration 34/1000 | Loss: 0.00001936
Iteration 35/1000 | Loss: 0.00001936
Iteration 36/1000 | Loss: 0.00001936
Iteration 37/1000 | Loss: 0.00001936
Iteration 38/1000 | Loss: 0.00001936
Iteration 39/1000 | Loss: 0.00001936
Iteration 40/1000 | Loss: 0.00001935
Iteration 41/1000 | Loss: 0.00001935
Iteration 42/1000 | Loss: 0.00001935
Iteration 43/1000 | Loss: 0.00001935
Iteration 44/1000 | Loss: 0.00001934
Iteration 45/1000 | Loss: 0.00001934
Iteration 46/1000 | Loss: 0.00001934
Iteration 47/1000 | Loss: 0.00001934
Iteration 48/1000 | Loss: 0.00001934
Iteration 49/1000 | Loss: 0.00001934
Iteration 50/1000 | Loss: 0.00001934
Iteration 51/1000 | Loss: 0.00001933
Iteration 52/1000 | Loss: 0.00001933
Iteration 53/1000 | Loss: 0.00001933
Iteration 54/1000 | Loss: 0.00001933
Iteration 55/1000 | Loss: 0.00001933
Iteration 56/1000 | Loss: 0.00001933
Iteration 57/1000 | Loss: 0.00001933
Iteration 58/1000 | Loss: 0.00001933
Iteration 59/1000 | Loss: 0.00001933
Iteration 60/1000 | Loss: 0.00001933
Iteration 61/1000 | Loss: 0.00001933
Iteration 62/1000 | Loss: 0.00001933
Iteration 63/1000 | Loss: 0.00001933
Iteration 64/1000 | Loss: 0.00001933
Iteration 65/1000 | Loss: 0.00001932
Iteration 66/1000 | Loss: 0.00001932
Iteration 67/1000 | Loss: 0.00001932
Iteration 68/1000 | Loss: 0.00001932
Iteration 69/1000 | Loss: 0.00001932
Iteration 70/1000 | Loss: 0.00001931
Iteration 71/1000 | Loss: 0.00001931
Iteration 72/1000 | Loss: 0.00001931
Iteration 73/1000 | Loss: 0.00001931
Iteration 74/1000 | Loss: 0.00001931
Iteration 75/1000 | Loss: 0.00001931
Iteration 76/1000 | Loss: 0.00001931
Iteration 77/1000 | Loss: 0.00001931
Iteration 78/1000 | Loss: 0.00001931
Iteration 79/1000 | Loss: 0.00001931
Iteration 80/1000 | Loss: 0.00001930
Iteration 81/1000 | Loss: 0.00001930
Iteration 82/1000 | Loss: 0.00001930
Iteration 83/1000 | Loss: 0.00001930
Iteration 84/1000 | Loss: 0.00001929
Iteration 85/1000 | Loss: 0.00001929
Iteration 86/1000 | Loss: 0.00001929
Iteration 87/1000 | Loss: 0.00001929
Iteration 88/1000 | Loss: 0.00001929
Iteration 89/1000 | Loss: 0.00001929
Iteration 90/1000 | Loss: 0.00001929
Iteration 91/1000 | Loss: 0.00001928
Iteration 92/1000 | Loss: 0.00001928
Iteration 93/1000 | Loss: 0.00001928
Iteration 94/1000 | Loss: 0.00001928
Iteration 95/1000 | Loss: 0.00001928
Iteration 96/1000 | Loss: 0.00001928
Iteration 97/1000 | Loss: 0.00001928
Iteration 98/1000 | Loss: 0.00001928
Iteration 99/1000 | Loss: 0.00001928
Iteration 100/1000 | Loss: 0.00001928
Iteration 101/1000 | Loss: 0.00001928
Iteration 102/1000 | Loss: 0.00001928
Iteration 103/1000 | Loss: 0.00001928
Iteration 104/1000 | Loss: 0.00001928
Iteration 105/1000 | Loss: 0.00001928
Iteration 106/1000 | Loss: 0.00001928
Iteration 107/1000 | Loss: 0.00001928
Iteration 108/1000 | Loss: 0.00001927
Iteration 109/1000 | Loss: 0.00001927
Iteration 110/1000 | Loss: 0.00001927
Iteration 111/1000 | Loss: 0.00001927
Iteration 112/1000 | Loss: 0.00001927
Iteration 113/1000 | Loss: 0.00001927
Iteration 114/1000 | Loss: 0.00001927
Iteration 115/1000 | Loss: 0.00001927
Iteration 116/1000 | Loss: 0.00001927
Iteration 117/1000 | Loss: 0.00001927
Iteration 118/1000 | Loss: 0.00001927
Iteration 119/1000 | Loss: 0.00001927
Iteration 120/1000 | Loss: 0.00001927
Iteration 121/1000 | Loss: 0.00001927
Iteration 122/1000 | Loss: 0.00001927
Iteration 123/1000 | Loss: 0.00001927
Iteration 124/1000 | Loss: 0.00001927
Iteration 125/1000 | Loss: 0.00001927
Iteration 126/1000 | Loss: 0.00001926
Iteration 127/1000 | Loss: 0.00001926
Iteration 128/1000 | Loss: 0.00001926
Iteration 129/1000 | Loss: 0.00001926
Iteration 130/1000 | Loss: 0.00001926
Iteration 131/1000 | Loss: 0.00001926
Iteration 132/1000 | Loss: 0.00001926
Iteration 133/1000 | Loss: 0.00001925
Iteration 134/1000 | Loss: 0.00001925
Iteration 135/1000 | Loss: 0.00001925
Iteration 136/1000 | Loss: 0.00001925
Iteration 137/1000 | Loss: 0.00001925
Iteration 138/1000 | Loss: 0.00001925
Iteration 139/1000 | Loss: 0.00001925
Iteration 140/1000 | Loss: 0.00001925
Iteration 141/1000 | Loss: 0.00001925
Iteration 142/1000 | Loss: 0.00001925
Iteration 143/1000 | Loss: 0.00001925
Iteration 144/1000 | Loss: 0.00001925
Iteration 145/1000 | Loss: 0.00001925
Iteration 146/1000 | Loss: 0.00001925
Iteration 147/1000 | Loss: 0.00001925
Iteration 148/1000 | Loss: 0.00001925
Iteration 149/1000 | Loss: 0.00001924
Iteration 150/1000 | Loss: 0.00001924
Iteration 151/1000 | Loss: 0.00001924
Iteration 152/1000 | Loss: 0.00001924
Iteration 153/1000 | Loss: 0.00001924
Iteration 154/1000 | Loss: 0.00001924
Iteration 155/1000 | Loss: 0.00001924
Iteration 156/1000 | Loss: 0.00001924
Iteration 157/1000 | Loss: 0.00001924
Iteration 158/1000 | Loss: 0.00001924
Iteration 159/1000 | Loss: 0.00001924
Iteration 160/1000 | Loss: 0.00001924
Iteration 161/1000 | Loss: 0.00001924
Iteration 162/1000 | Loss: 0.00001924
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 162. Stopping optimization.
Last 5 losses: [1.924255229823757e-05, 1.924255229823757e-05, 1.924255229823757e-05, 1.924255229823757e-05, 1.924255229823757e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.924255229823757e-05

Optimization complete. Final v2v error: 3.686079978942871 mm

Highest mean error: 4.013474941253662 mm for frame 116

Lowest mean error: 3.4386682510375977 mm for frame 95

Saving results

Total time: 34.01306891441345
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1097/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1097.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1097
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00457664
Iteration 2/25 | Loss: 0.00104778
Iteration 3/25 | Loss: 0.00084042
Iteration 4/25 | Loss: 0.00081577
Iteration 5/25 | Loss: 0.00081005
Iteration 6/25 | Loss: 0.00080862
Iteration 7/25 | Loss: 0.00080843
Iteration 8/25 | Loss: 0.00080843
Iteration 9/25 | Loss: 0.00080843
Iteration 10/25 | Loss: 0.00080843
Iteration 11/25 | Loss: 0.00080843
Iteration 12/25 | Loss: 0.00080843
Iteration 13/25 | Loss: 0.00080843
Iteration 14/25 | Loss: 0.00080843
Iteration 15/25 | Loss: 0.00080843
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0008084255969151855, 0.0008084255969151855, 0.0008084255969151855, 0.0008084255969151855, 0.0008084255969151855]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008084255969151855

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.34727669
Iteration 2/25 | Loss: 0.00129219
Iteration 3/25 | Loss: 0.00129216
Iteration 4/25 | Loss: 0.00129216
Iteration 5/25 | Loss: 0.00129216
Iteration 6/25 | Loss: 0.00129216
Iteration 7/25 | Loss: 0.00129216
Iteration 8/25 | Loss: 0.00129216
Iteration 9/25 | Loss: 0.00129216
Iteration 10/25 | Loss: 0.00129216
Iteration 11/25 | Loss: 0.00129216
Iteration 12/25 | Loss: 0.00129216
Iteration 13/25 | Loss: 0.00129216
Iteration 14/25 | Loss: 0.00129216
Iteration 15/25 | Loss: 0.00129216
Iteration 16/25 | Loss: 0.00129216
Iteration 17/25 | Loss: 0.00129216
Iteration 18/25 | Loss: 0.00129216
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012921581510454416, 0.0012921581510454416, 0.0012921581510454416, 0.0012921581510454416, 0.0012921581510454416]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012921581510454416

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129216
Iteration 2/1000 | Loss: 0.00002634
Iteration 3/1000 | Loss: 0.00002131
Iteration 4/1000 | Loss: 0.00002034
Iteration 5/1000 | Loss: 0.00001916
Iteration 6/1000 | Loss: 0.00001855
Iteration 7/1000 | Loss: 0.00001803
Iteration 8/1000 | Loss: 0.00001773
Iteration 9/1000 | Loss: 0.00001752
Iteration 10/1000 | Loss: 0.00001745
Iteration 11/1000 | Loss: 0.00001744
Iteration 12/1000 | Loss: 0.00001739
Iteration 13/1000 | Loss: 0.00001739
Iteration 14/1000 | Loss: 0.00001738
Iteration 15/1000 | Loss: 0.00001737
Iteration 16/1000 | Loss: 0.00001733
Iteration 17/1000 | Loss: 0.00001733
Iteration 18/1000 | Loss: 0.00001731
Iteration 19/1000 | Loss: 0.00001731
Iteration 20/1000 | Loss: 0.00001731
Iteration 21/1000 | Loss: 0.00001731
Iteration 22/1000 | Loss: 0.00001730
Iteration 23/1000 | Loss: 0.00001730
Iteration 24/1000 | Loss: 0.00001730
Iteration 25/1000 | Loss: 0.00001729
Iteration 26/1000 | Loss: 0.00001729
Iteration 27/1000 | Loss: 0.00001728
Iteration 28/1000 | Loss: 0.00001728
Iteration 29/1000 | Loss: 0.00001728
Iteration 30/1000 | Loss: 0.00001727
Iteration 31/1000 | Loss: 0.00001727
Iteration 32/1000 | Loss: 0.00001727
Iteration 33/1000 | Loss: 0.00001726
Iteration 34/1000 | Loss: 0.00001726
Iteration 35/1000 | Loss: 0.00001726
Iteration 36/1000 | Loss: 0.00001726
Iteration 37/1000 | Loss: 0.00001726
Iteration 38/1000 | Loss: 0.00001725
Iteration 39/1000 | Loss: 0.00001725
Iteration 40/1000 | Loss: 0.00001724
Iteration 41/1000 | Loss: 0.00001724
Iteration 42/1000 | Loss: 0.00001724
Iteration 43/1000 | Loss: 0.00001724
Iteration 44/1000 | Loss: 0.00001724
Iteration 45/1000 | Loss: 0.00001724
Iteration 46/1000 | Loss: 0.00001723
Iteration 47/1000 | Loss: 0.00001723
Iteration 48/1000 | Loss: 0.00001723
Iteration 49/1000 | Loss: 0.00001723
Iteration 50/1000 | Loss: 0.00001722
Iteration 51/1000 | Loss: 0.00001722
Iteration 52/1000 | Loss: 0.00001722
Iteration 53/1000 | Loss: 0.00001722
Iteration 54/1000 | Loss: 0.00001722
Iteration 55/1000 | Loss: 0.00001721
Iteration 56/1000 | Loss: 0.00001721
Iteration 57/1000 | Loss: 0.00001721
Iteration 58/1000 | Loss: 0.00001721
Iteration 59/1000 | Loss: 0.00001721
Iteration 60/1000 | Loss: 0.00001721
Iteration 61/1000 | Loss: 0.00001721
Iteration 62/1000 | Loss: 0.00001721
Iteration 63/1000 | Loss: 0.00001721
Iteration 64/1000 | Loss: 0.00001721
Iteration 65/1000 | Loss: 0.00001720
Iteration 66/1000 | Loss: 0.00001720
Iteration 67/1000 | Loss: 0.00001720
Iteration 68/1000 | Loss: 0.00001720
Iteration 69/1000 | Loss: 0.00001720
Iteration 70/1000 | Loss: 0.00001720
Iteration 71/1000 | Loss: 0.00001720
Iteration 72/1000 | Loss: 0.00001720
Iteration 73/1000 | Loss: 0.00001719
Iteration 74/1000 | Loss: 0.00001719
Iteration 75/1000 | Loss: 0.00001719
Iteration 76/1000 | Loss: 0.00001719
Iteration 77/1000 | Loss: 0.00001719
Iteration 78/1000 | Loss: 0.00001718
Iteration 79/1000 | Loss: 0.00001718
Iteration 80/1000 | Loss: 0.00001718
Iteration 81/1000 | Loss: 0.00001718
Iteration 82/1000 | Loss: 0.00001718
Iteration 83/1000 | Loss: 0.00001718
Iteration 84/1000 | Loss: 0.00001717
Iteration 85/1000 | Loss: 0.00001717
Iteration 86/1000 | Loss: 0.00001717
Iteration 87/1000 | Loss: 0.00001717
Iteration 88/1000 | Loss: 0.00001717
Iteration 89/1000 | Loss: 0.00001717
Iteration 90/1000 | Loss: 0.00001717
Iteration 91/1000 | Loss: 0.00001717
Iteration 92/1000 | Loss: 0.00001716
Iteration 93/1000 | Loss: 0.00001716
Iteration 94/1000 | Loss: 0.00001716
Iteration 95/1000 | Loss: 0.00001716
Iteration 96/1000 | Loss: 0.00001716
Iteration 97/1000 | Loss: 0.00001716
Iteration 98/1000 | Loss: 0.00001716
Iteration 99/1000 | Loss: 0.00001716
Iteration 100/1000 | Loss: 0.00001716
Iteration 101/1000 | Loss: 0.00001716
Iteration 102/1000 | Loss: 0.00001716
Iteration 103/1000 | Loss: 0.00001716
Iteration 104/1000 | Loss: 0.00001716
Iteration 105/1000 | Loss: 0.00001716
Iteration 106/1000 | Loss: 0.00001716
Iteration 107/1000 | Loss: 0.00001716
Iteration 108/1000 | Loss: 0.00001716
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 108. Stopping optimization.
Last 5 losses: [1.7163214579341002e-05, 1.7163214579341002e-05, 1.7163214579341002e-05, 1.7163214579341002e-05, 1.7163214579341002e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7163214579341002e-05

Optimization complete. Final v2v error: 3.5258867740631104 mm

Highest mean error: 4.054443359375 mm for frame 62

Lowest mean error: 3.2681636810302734 mm for frame 3

Saving results

Total time: 30.422571897506714
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1062/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1062.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1062
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00764677
Iteration 2/25 | Loss: 0.00196080
Iteration 3/25 | Loss: 0.00144303
Iteration 4/25 | Loss: 0.00134039
Iteration 5/25 | Loss: 0.00124263
Iteration 6/25 | Loss: 0.00120214
Iteration 7/25 | Loss: 0.00116505
Iteration 8/25 | Loss: 0.00150869
Iteration 9/25 | Loss: 0.00131425
Iteration 10/25 | Loss: 0.00093323
Iteration 11/25 | Loss: 0.00091689
Iteration 12/25 | Loss: 0.00086513
Iteration 13/25 | Loss: 0.00084926
Iteration 14/25 | Loss: 0.00084631
Iteration 15/25 | Loss: 0.00084455
Iteration 16/25 | Loss: 0.00084235
Iteration 17/25 | Loss: 0.00084194
Iteration 18/25 | Loss: 0.00084184
Iteration 19/25 | Loss: 0.00084182
Iteration 20/25 | Loss: 0.00084182
Iteration 21/25 | Loss: 0.00084182
Iteration 22/25 | Loss: 0.00084182
Iteration 23/25 | Loss: 0.00084182
Iteration 24/25 | Loss: 0.00084182
Iteration 25/25 | Loss: 0.00084182

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.64385128
Iteration 2/25 | Loss: 0.00161779
Iteration 3/25 | Loss: 0.00156586
Iteration 4/25 | Loss: 0.00156520
Iteration 5/25 | Loss: 0.00156520
Iteration 6/25 | Loss: 0.00156519
Iteration 7/25 | Loss: 0.00156519
Iteration 8/25 | Loss: 0.00156519
Iteration 9/25 | Loss: 0.00156519
Iteration 10/25 | Loss: 0.00156519
Iteration 11/25 | Loss: 0.00156519
Iteration 12/25 | Loss: 0.00156519
Iteration 13/25 | Loss: 0.00156519
Iteration 14/25 | Loss: 0.00156519
Iteration 15/25 | Loss: 0.00156519
Iteration 16/25 | Loss: 0.00156519
Iteration 17/25 | Loss: 0.00156519
Iteration 18/25 | Loss: 0.00156519
Iteration 19/25 | Loss: 0.00156519
Iteration 20/25 | Loss: 0.00156519
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0015651930589228868, 0.0015651930589228868, 0.0015651930589228868, 0.0015651930589228868, 0.0015651930589228868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015651930589228868

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156519
Iteration 2/1000 | Loss: 0.00005693
Iteration 3/1000 | Loss: 0.00003942
Iteration 4/1000 | Loss: 0.00003439
Iteration 5/1000 | Loss: 0.00003200
Iteration 6/1000 | Loss: 0.00003042
Iteration 7/1000 | Loss: 0.00002935
Iteration 8/1000 | Loss: 0.00002871
Iteration 9/1000 | Loss: 0.00002809
Iteration 10/1000 | Loss: 0.00002760
Iteration 11/1000 | Loss: 0.00002725
Iteration 12/1000 | Loss: 0.00002702
Iteration 13/1000 | Loss: 0.00002681
Iteration 14/1000 | Loss: 0.00002664
Iteration 15/1000 | Loss: 0.00002647
Iteration 16/1000 | Loss: 0.00002639
Iteration 17/1000 | Loss: 0.00002635
Iteration 18/1000 | Loss: 0.00002634
Iteration 19/1000 | Loss: 0.00002634
Iteration 20/1000 | Loss: 0.00002631
Iteration 21/1000 | Loss: 0.00002628
Iteration 22/1000 | Loss: 0.00002626
Iteration 23/1000 | Loss: 0.00002625
Iteration 24/1000 | Loss: 0.00002623
Iteration 25/1000 | Loss: 0.00002621
Iteration 26/1000 | Loss: 0.00002621
Iteration 27/1000 | Loss: 0.00002620
Iteration 28/1000 | Loss: 0.00002620
Iteration 29/1000 | Loss: 0.00002617
Iteration 30/1000 | Loss: 0.00002616
Iteration 31/1000 | Loss: 0.00002614
Iteration 32/1000 | Loss: 0.00002610
Iteration 33/1000 | Loss: 0.00002610
Iteration 34/1000 | Loss: 0.00002606
Iteration 35/1000 | Loss: 0.00002602
Iteration 36/1000 | Loss: 0.00002602
Iteration 37/1000 | Loss: 0.00002601
Iteration 38/1000 | Loss: 0.00002601
Iteration 39/1000 | Loss: 0.00002598
Iteration 40/1000 | Loss: 0.00002598
Iteration 41/1000 | Loss: 0.00002595
Iteration 42/1000 | Loss: 0.00002595
Iteration 43/1000 | Loss: 0.00002594
Iteration 44/1000 | Loss: 0.00002594
Iteration 45/1000 | Loss: 0.00002594
Iteration 46/1000 | Loss: 0.00002592
Iteration 47/1000 | Loss: 0.00002590
Iteration 48/1000 | Loss: 0.00002589
Iteration 49/1000 | Loss: 0.00002588
Iteration 50/1000 | Loss: 0.00002588
Iteration 51/1000 | Loss: 0.00002588
Iteration 52/1000 | Loss: 0.00002585
Iteration 53/1000 | Loss: 0.00002585
Iteration 54/1000 | Loss: 0.00002585
Iteration 55/1000 | Loss: 0.00002585
Iteration 56/1000 | Loss: 0.00002585
Iteration 57/1000 | Loss: 0.00002584
Iteration 58/1000 | Loss: 0.00002584
Iteration 59/1000 | Loss: 0.00002584
Iteration 60/1000 | Loss: 0.00002583
Iteration 61/1000 | Loss: 0.00002583
Iteration 62/1000 | Loss: 0.00002583
Iteration 63/1000 | Loss: 0.00002583
Iteration 64/1000 | Loss: 0.00002583
Iteration 65/1000 | Loss: 0.00002582
Iteration 66/1000 | Loss: 0.00002582
Iteration 67/1000 | Loss: 0.00002582
Iteration 68/1000 | Loss: 0.00002582
Iteration 69/1000 | Loss: 0.00002582
Iteration 70/1000 | Loss: 0.00002581
Iteration 71/1000 | Loss: 0.00002581
Iteration 72/1000 | Loss: 0.00002581
Iteration 73/1000 | Loss: 0.00002580
Iteration 74/1000 | Loss: 0.00002580
Iteration 75/1000 | Loss: 0.00002580
Iteration 76/1000 | Loss: 0.00002579
Iteration 77/1000 | Loss: 0.00002579
Iteration 78/1000 | Loss: 0.00002579
Iteration 79/1000 | Loss: 0.00002578
Iteration 80/1000 | Loss: 0.00002578
Iteration 81/1000 | Loss: 0.00002578
Iteration 82/1000 | Loss: 0.00002577
Iteration 83/1000 | Loss: 0.00002577
Iteration 84/1000 | Loss: 0.00002577
Iteration 85/1000 | Loss: 0.00002576
Iteration 86/1000 | Loss: 0.00002576
Iteration 87/1000 | Loss: 0.00002576
Iteration 88/1000 | Loss: 0.00002575
Iteration 89/1000 | Loss: 0.00002575
Iteration 90/1000 | Loss: 0.00002574
Iteration 91/1000 | Loss: 0.00002574
Iteration 92/1000 | Loss: 0.00002574
Iteration 93/1000 | Loss: 0.00002573
Iteration 94/1000 | Loss: 0.00002573
Iteration 95/1000 | Loss: 0.00002573
Iteration 96/1000 | Loss: 0.00002572
Iteration 97/1000 | Loss: 0.00002571
Iteration 98/1000 | Loss: 0.00002571
Iteration 99/1000 | Loss: 0.00002570
Iteration 100/1000 | Loss: 0.00002570
Iteration 101/1000 | Loss: 0.00002569
Iteration 102/1000 | Loss: 0.00002569
Iteration 103/1000 | Loss: 0.00002569
Iteration 104/1000 | Loss: 0.00002568
Iteration 105/1000 | Loss: 0.00002568
Iteration 106/1000 | Loss: 0.00002568
Iteration 107/1000 | Loss: 0.00002568
Iteration 108/1000 | Loss: 0.00002567
Iteration 109/1000 | Loss: 0.00002567
Iteration 110/1000 | Loss: 0.00002567
Iteration 111/1000 | Loss: 0.00002567
Iteration 112/1000 | Loss: 0.00002567
Iteration 113/1000 | Loss: 0.00002566
Iteration 114/1000 | Loss: 0.00002566
Iteration 115/1000 | Loss: 0.00002566
Iteration 116/1000 | Loss: 0.00002565
Iteration 117/1000 | Loss: 0.00002565
Iteration 118/1000 | Loss: 0.00002565
Iteration 119/1000 | Loss: 0.00002565
Iteration 120/1000 | Loss: 0.00002564
Iteration 121/1000 | Loss: 0.00002564
Iteration 122/1000 | Loss: 0.00002564
Iteration 123/1000 | Loss: 0.00002564
Iteration 124/1000 | Loss: 0.00002563
Iteration 125/1000 | Loss: 0.00002563
Iteration 126/1000 | Loss: 0.00002563
Iteration 127/1000 | Loss: 0.00002563
Iteration 128/1000 | Loss: 0.00002562
Iteration 129/1000 | Loss: 0.00002562
Iteration 130/1000 | Loss: 0.00002562
Iteration 131/1000 | Loss: 0.00002562
Iteration 132/1000 | Loss: 0.00002562
Iteration 133/1000 | Loss: 0.00002562
Iteration 134/1000 | Loss: 0.00002562
Iteration 135/1000 | Loss: 0.00002561
Iteration 136/1000 | Loss: 0.00002561
Iteration 137/1000 | Loss: 0.00002561
Iteration 138/1000 | Loss: 0.00002561
Iteration 139/1000 | Loss: 0.00002561
Iteration 140/1000 | Loss: 0.00002561
Iteration 141/1000 | Loss: 0.00002561
Iteration 142/1000 | Loss: 0.00002560
Iteration 143/1000 | Loss: 0.00002560
Iteration 144/1000 | Loss: 0.00002560
Iteration 145/1000 | Loss: 0.00002560
Iteration 146/1000 | Loss: 0.00002560
Iteration 147/1000 | Loss: 0.00002560
Iteration 148/1000 | Loss: 0.00002559
Iteration 149/1000 | Loss: 0.00002559
Iteration 150/1000 | Loss: 0.00002559
Iteration 151/1000 | Loss: 0.00002559
Iteration 152/1000 | Loss: 0.00002559
Iteration 153/1000 | Loss: 0.00002559
Iteration 154/1000 | Loss: 0.00002559
Iteration 155/1000 | Loss: 0.00002559
Iteration 156/1000 | Loss: 0.00002558
Iteration 157/1000 | Loss: 0.00002558
Iteration 158/1000 | Loss: 0.00002558
Iteration 159/1000 | Loss: 0.00002558
Iteration 160/1000 | Loss: 0.00002558
Iteration 161/1000 | Loss: 0.00002558
Iteration 162/1000 | Loss: 0.00002557
Iteration 163/1000 | Loss: 0.00002557
Iteration 164/1000 | Loss: 0.00002557
Iteration 165/1000 | Loss: 0.00002557
Iteration 166/1000 | Loss: 0.00002557
Iteration 167/1000 | Loss: 0.00002557
Iteration 168/1000 | Loss: 0.00002557
Iteration 169/1000 | Loss: 0.00002557
Iteration 170/1000 | Loss: 0.00002557
Iteration 171/1000 | Loss: 0.00002556
Iteration 172/1000 | Loss: 0.00002556
Iteration 173/1000 | Loss: 0.00002556
Iteration 174/1000 | Loss: 0.00002556
Iteration 175/1000 | Loss: 0.00002556
Iteration 176/1000 | Loss: 0.00002556
Iteration 177/1000 | Loss: 0.00002556
Iteration 178/1000 | Loss: 0.00002556
Iteration 179/1000 | Loss: 0.00002556
Iteration 180/1000 | Loss: 0.00002555
Iteration 181/1000 | Loss: 0.00002555
Iteration 182/1000 | Loss: 0.00002555
Iteration 183/1000 | Loss: 0.00002555
Iteration 184/1000 | Loss: 0.00002555
Iteration 185/1000 | Loss: 0.00002555
Iteration 186/1000 | Loss: 0.00002555
Iteration 187/1000 | Loss: 0.00002555
Iteration 188/1000 | Loss: 0.00002555
Iteration 189/1000 | Loss: 0.00002555
Iteration 190/1000 | Loss: 0.00002555
Iteration 191/1000 | Loss: 0.00002555
Iteration 192/1000 | Loss: 0.00002555
Iteration 193/1000 | Loss: 0.00002555
Iteration 194/1000 | Loss: 0.00002555
Iteration 195/1000 | Loss: 0.00002555
Iteration 196/1000 | Loss: 0.00002554
Iteration 197/1000 | Loss: 0.00002554
Iteration 198/1000 | Loss: 0.00002554
Iteration 199/1000 | Loss: 0.00002554
Iteration 200/1000 | Loss: 0.00002554
Iteration 201/1000 | Loss: 0.00002554
Iteration 202/1000 | Loss: 0.00002554
Iteration 203/1000 | Loss: 0.00002554
Iteration 204/1000 | Loss: 0.00002554
Iteration 205/1000 | Loss: 0.00002554
Iteration 206/1000 | Loss: 0.00002554
Iteration 207/1000 | Loss: 0.00002554
Iteration 208/1000 | Loss: 0.00002554
Iteration 209/1000 | Loss: 0.00002554
Iteration 210/1000 | Loss: 0.00002554
Iteration 211/1000 | Loss: 0.00002554
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 211. Stopping optimization.
Last 5 losses: [2.5543704396113753e-05, 2.5543704396113753e-05, 2.5543704396113753e-05, 2.5543704396113753e-05, 2.5543704396113753e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5543704396113753e-05

Optimization complete. Final v2v error: 4.1108317375183105 mm

Highest mean error: 6.252689361572266 mm for frame 158

Lowest mean error: 3.265202045440674 mm for frame 200

Saving results

Total time: 86.52419114112854
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1050/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1050.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1050
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00779550
Iteration 2/25 | Loss: 0.00159293
Iteration 3/25 | Loss: 0.00096893
Iteration 4/25 | Loss: 0.00086084
Iteration 5/25 | Loss: 0.00082143
Iteration 6/25 | Loss: 0.00082141
Iteration 7/25 | Loss: 0.00080136
Iteration 8/25 | Loss: 0.00079575
Iteration 9/25 | Loss: 0.00077384
Iteration 10/25 | Loss: 0.00077037
Iteration 11/25 | Loss: 0.00077052
Iteration 12/25 | Loss: 0.00076553
Iteration 13/25 | Loss: 0.00075444
Iteration 14/25 | Loss: 0.00075158
Iteration 15/25 | Loss: 0.00075010
Iteration 16/25 | Loss: 0.00074974
Iteration 17/25 | Loss: 0.00074992
Iteration 18/25 | Loss: 0.00074914
Iteration 19/25 | Loss: 0.00074911
Iteration 20/25 | Loss: 0.00074911
Iteration 21/25 | Loss: 0.00074911
Iteration 22/25 | Loss: 0.00074911
Iteration 23/25 | Loss: 0.00074911
Iteration 24/25 | Loss: 0.00074911
Iteration 25/25 | Loss: 0.00074911

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.29992628
Iteration 2/25 | Loss: 0.00116992
Iteration 3/25 | Loss: 0.00116784
Iteration 4/25 | Loss: 0.00116784
Iteration 5/25 | Loss: 0.00116784
Iteration 6/25 | Loss: 0.00116784
Iteration 7/25 | Loss: 0.00116784
Iteration 8/25 | Loss: 0.00116784
Iteration 9/25 | Loss: 0.00116784
Iteration 10/25 | Loss: 0.00116784
Iteration 11/25 | Loss: 0.00116784
Iteration 12/25 | Loss: 0.00116784
Iteration 13/25 | Loss: 0.00116784
Iteration 14/25 | Loss: 0.00116784
Iteration 15/25 | Loss: 0.00116784
Iteration 16/25 | Loss: 0.00116784
Iteration 17/25 | Loss: 0.00116784
Iteration 18/25 | Loss: 0.00116784
Iteration 19/25 | Loss: 0.00116784
Iteration 20/25 | Loss: 0.00116784
Iteration 21/25 | Loss: 0.00116784
Iteration 22/25 | Loss: 0.00116784
Iteration 23/25 | Loss: 0.00116784
Iteration 24/25 | Loss: 0.00116784
Iteration 25/25 | Loss: 0.00116784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00116784
Iteration 2/1000 | Loss: 0.00003680
Iteration 3/1000 | Loss: 0.00002897
Iteration 4/1000 | Loss: 0.00002618
Iteration 5/1000 | Loss: 0.00001880
Iteration 6/1000 | Loss: 0.00002388
Iteration 7/1000 | Loss: 0.00019178
Iteration 8/1000 | Loss: 0.00002053
Iteration 9/1000 | Loss: 0.00002187
Iteration 10/1000 | Loss: 0.00013860
Iteration 11/1000 | Loss: 0.00002177
Iteration 12/1000 | Loss: 0.00001660
Iteration 13/1000 | Loss: 0.00001646
Iteration 14/1000 | Loss: 0.00001717
Iteration 15/1000 | Loss: 0.00001844
Iteration 16/1000 | Loss: 0.00004599
Iteration 17/1000 | Loss: 0.00001875
Iteration 18/1000 | Loss: 0.00001812
Iteration 19/1000 | Loss: 0.00001648
Iteration 20/1000 | Loss: 0.00003236
Iteration 21/1000 | Loss: 0.00001709
Iteration 22/1000 | Loss: 0.00002036
Iteration 23/1000 | Loss: 0.00001589
Iteration 24/1000 | Loss: 0.00001599
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001612
Iteration 27/1000 | Loss: 0.00002246
Iteration 28/1000 | Loss: 0.00002544
Iteration 29/1000 | Loss: 0.00001678
Iteration 30/1000 | Loss: 0.00002017
Iteration 31/1000 | Loss: 0.00001722
Iteration 32/1000 | Loss: 0.00001575
Iteration 33/1000 | Loss: 0.00001575
Iteration 34/1000 | Loss: 0.00001575
Iteration 35/1000 | Loss: 0.00001574
Iteration 36/1000 | Loss: 0.00001574
Iteration 37/1000 | Loss: 0.00001574
Iteration 38/1000 | Loss: 0.00001574
Iteration 39/1000 | Loss: 0.00001574
Iteration 40/1000 | Loss: 0.00001574
Iteration 41/1000 | Loss: 0.00001573
Iteration 42/1000 | Loss: 0.00001573
Iteration 43/1000 | Loss: 0.00001573
Iteration 44/1000 | Loss: 0.00001573
Iteration 45/1000 | Loss: 0.00001572
Iteration 46/1000 | Loss: 0.00001572
Iteration 47/1000 | Loss: 0.00001572
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001571
Iteration 50/1000 | Loss: 0.00001570
Iteration 51/1000 | Loss: 0.00001570
Iteration 52/1000 | Loss: 0.00001569
Iteration 53/1000 | Loss: 0.00001569
Iteration 54/1000 | Loss: 0.00001569
Iteration 55/1000 | Loss: 0.00001814
Iteration 56/1000 | Loss: 0.00001568
Iteration 57/1000 | Loss: 0.00001568
Iteration 58/1000 | Loss: 0.00001568
Iteration 59/1000 | Loss: 0.00001568
Iteration 60/1000 | Loss: 0.00001567
Iteration 61/1000 | Loss: 0.00001567
Iteration 62/1000 | Loss: 0.00001567
Iteration 63/1000 | Loss: 0.00001567
Iteration 64/1000 | Loss: 0.00001567
Iteration 65/1000 | Loss: 0.00001568
Iteration 66/1000 | Loss: 0.00001565
Iteration 67/1000 | Loss: 0.00001565
Iteration 68/1000 | Loss: 0.00001565
Iteration 69/1000 | Loss: 0.00001565
Iteration 70/1000 | Loss: 0.00001565
Iteration 71/1000 | Loss: 0.00001564
Iteration 72/1000 | Loss: 0.00001564
Iteration 73/1000 | Loss: 0.00001564
Iteration 74/1000 | Loss: 0.00001564
Iteration 75/1000 | Loss: 0.00001564
Iteration 76/1000 | Loss: 0.00001644
Iteration 77/1000 | Loss: 0.00001556
Iteration 78/1000 | Loss: 0.00001556
Iteration 79/1000 | Loss: 0.00001556
Iteration 80/1000 | Loss: 0.00001556
Iteration 81/1000 | Loss: 0.00001556
Iteration 82/1000 | Loss: 0.00001556
Iteration 83/1000 | Loss: 0.00001556
Iteration 84/1000 | Loss: 0.00001556
Iteration 85/1000 | Loss: 0.00001556
Iteration 86/1000 | Loss: 0.00001556
Iteration 87/1000 | Loss: 0.00001556
Iteration 88/1000 | Loss: 0.00001556
Iteration 89/1000 | Loss: 0.00001555
Iteration 90/1000 | Loss: 0.00001555
Iteration 91/1000 | Loss: 0.00001555
Iteration 92/1000 | Loss: 0.00001555
Iteration 93/1000 | Loss: 0.00001555
Iteration 94/1000 | Loss: 0.00001555
Iteration 95/1000 | Loss: 0.00001555
Iteration 96/1000 | Loss: 0.00001555
Iteration 97/1000 | Loss: 0.00001555
Iteration 98/1000 | Loss: 0.00001555
Iteration 99/1000 | Loss: 0.00001555
Iteration 100/1000 | Loss: 0.00001554
Iteration 101/1000 | Loss: 0.00001554
Iteration 102/1000 | Loss: 0.00001554
Iteration 103/1000 | Loss: 0.00001553
Iteration 104/1000 | Loss: 0.00001552
Iteration 105/1000 | Loss: 0.00001552
Iteration 106/1000 | Loss: 0.00001552
Iteration 107/1000 | Loss: 0.00001549
Iteration 108/1000 | Loss: 0.00001549
Iteration 109/1000 | Loss: 0.00001549
Iteration 110/1000 | Loss: 0.00001549
Iteration 111/1000 | Loss: 0.00001549
Iteration 112/1000 | Loss: 0.00001549
Iteration 113/1000 | Loss: 0.00001549
Iteration 114/1000 | Loss: 0.00001549
Iteration 115/1000 | Loss: 0.00001549
Iteration 116/1000 | Loss: 0.00001548
Iteration 117/1000 | Loss: 0.00001548
Iteration 118/1000 | Loss: 0.00001548
Iteration 119/1000 | Loss: 0.00001548
Iteration 120/1000 | Loss: 0.00001548
Iteration 121/1000 | Loss: 0.00001548
Iteration 122/1000 | Loss: 0.00001548
Iteration 123/1000 | Loss: 0.00001548
Iteration 124/1000 | Loss: 0.00001547
Iteration 125/1000 | Loss: 0.00001547
Iteration 126/1000 | Loss: 0.00001547
Iteration 127/1000 | Loss: 0.00001547
Iteration 128/1000 | Loss: 0.00001547
Iteration 129/1000 | Loss: 0.00001547
Iteration 130/1000 | Loss: 0.00001547
Iteration 131/1000 | Loss: 0.00001547
Iteration 132/1000 | Loss: 0.00001547
Iteration 133/1000 | Loss: 0.00001547
Iteration 134/1000 | Loss: 0.00001547
Iteration 135/1000 | Loss: 0.00001547
Iteration 136/1000 | Loss: 0.00001547
Iteration 137/1000 | Loss: 0.00001546
Iteration 138/1000 | Loss: 0.00001546
Iteration 139/1000 | Loss: 0.00001546
Iteration 140/1000 | Loss: 0.00001546
Iteration 141/1000 | Loss: 0.00001546
Iteration 142/1000 | Loss: 0.00001546
Iteration 143/1000 | Loss: 0.00001546
Iteration 144/1000 | Loss: 0.00001546
Iteration 145/1000 | Loss: 0.00001545
Iteration 146/1000 | Loss: 0.00001545
Iteration 147/1000 | Loss: 0.00001545
Iteration 148/1000 | Loss: 0.00001545
Iteration 149/1000 | Loss: 0.00001545
Iteration 150/1000 | Loss: 0.00001545
Iteration 151/1000 | Loss: 0.00001545
Iteration 152/1000 | Loss: 0.00001544
Iteration 153/1000 | Loss: 0.00001544
Iteration 154/1000 | Loss: 0.00001544
Iteration 155/1000 | Loss: 0.00001544
Iteration 156/1000 | Loss: 0.00001544
Iteration 157/1000 | Loss: 0.00001544
Iteration 158/1000 | Loss: 0.00001544
Iteration 159/1000 | Loss: 0.00001544
Iteration 160/1000 | Loss: 0.00001856
Iteration 161/1000 | Loss: 0.00001856
Iteration 162/1000 | Loss: 0.00001589
Iteration 163/1000 | Loss: 0.00001541
Iteration 164/1000 | Loss: 0.00001541
Iteration 165/1000 | Loss: 0.00001541
Iteration 166/1000 | Loss: 0.00001541
Iteration 167/1000 | Loss: 0.00001541
Iteration 168/1000 | Loss: 0.00001540
Iteration 169/1000 | Loss: 0.00001540
Iteration 170/1000 | Loss: 0.00001540
Iteration 171/1000 | Loss: 0.00001540
Iteration 172/1000 | Loss: 0.00001540
Iteration 173/1000 | Loss: 0.00001540
Iteration 174/1000 | Loss: 0.00001540
Iteration 175/1000 | Loss: 0.00001540
Iteration 176/1000 | Loss: 0.00001540
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 176. Stopping optimization.
Last 5 losses: [1.5403777069877833e-05, 1.5403777069877833e-05, 1.5403777069877833e-05, 1.5403777069877833e-05, 1.5403777069877833e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5403777069877833e-05

Optimization complete. Final v2v error: 3.334641695022583 mm

Highest mean error: 3.708935022354126 mm for frame 100

Lowest mean error: 3.0375633239746094 mm for frame 2

Saving results

Total time: 90.72208023071289
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1080/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1080.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1080
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00436421
Iteration 2/25 | Loss: 0.00096276
Iteration 3/25 | Loss: 0.00085761
Iteration 4/25 | Loss: 0.00082989
Iteration 5/25 | Loss: 0.00082391
Iteration 6/25 | Loss: 0.00082255
Iteration 7/25 | Loss: 0.00082214
Iteration 8/25 | Loss: 0.00082214
Iteration 9/25 | Loss: 0.00082214
Iteration 10/25 | Loss: 0.00082214
Iteration 11/25 | Loss: 0.00082214
Iteration 12/25 | Loss: 0.00082214
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008221414755098522, 0.0008221414755098522, 0.0008221414755098522, 0.0008221414755098522, 0.0008221414755098522]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008221414755098522

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.95044672
Iteration 2/25 | Loss: 0.00121479
Iteration 3/25 | Loss: 0.00121479
Iteration 4/25 | Loss: 0.00121479
Iteration 5/25 | Loss: 0.00121479
Iteration 6/25 | Loss: 0.00121479
Iteration 7/25 | Loss: 0.00121479
Iteration 8/25 | Loss: 0.00121479
Iteration 9/25 | Loss: 0.00121479
Iteration 10/25 | Loss: 0.00121479
Iteration 11/25 | Loss: 0.00121479
Iteration 12/25 | Loss: 0.00121479
Iteration 13/25 | Loss: 0.00121479
Iteration 14/25 | Loss: 0.00121479
Iteration 15/25 | Loss: 0.00121479
Iteration 16/25 | Loss: 0.00121479
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001214788411743939, 0.001214788411743939, 0.001214788411743939, 0.001214788411743939, 0.001214788411743939]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001214788411743939

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121479
Iteration 2/1000 | Loss: 0.00005652
Iteration 3/1000 | Loss: 0.00003364
Iteration 4/1000 | Loss: 0.00003061
Iteration 5/1000 | Loss: 0.00002915
Iteration 6/1000 | Loss: 0.00002831
Iteration 7/1000 | Loss: 0.00002762
Iteration 8/1000 | Loss: 0.00002710
Iteration 9/1000 | Loss: 0.00002676
Iteration 10/1000 | Loss: 0.00002654
Iteration 11/1000 | Loss: 0.00002645
Iteration 12/1000 | Loss: 0.00002640
Iteration 13/1000 | Loss: 0.00002636
Iteration 14/1000 | Loss: 0.00002618
Iteration 15/1000 | Loss: 0.00002618
Iteration 16/1000 | Loss: 0.00002611
Iteration 17/1000 | Loss: 0.00002608
Iteration 18/1000 | Loss: 0.00002605
Iteration 19/1000 | Loss: 0.00002605
Iteration 20/1000 | Loss: 0.00002604
Iteration 21/1000 | Loss: 0.00002603
Iteration 22/1000 | Loss: 0.00002602
Iteration 23/1000 | Loss: 0.00002601
Iteration 24/1000 | Loss: 0.00002601
Iteration 25/1000 | Loss: 0.00002600
Iteration 26/1000 | Loss: 0.00002599
Iteration 27/1000 | Loss: 0.00002599
Iteration 28/1000 | Loss: 0.00002589
Iteration 29/1000 | Loss: 0.00002585
Iteration 30/1000 | Loss: 0.00002584
Iteration 31/1000 | Loss: 0.00002584
Iteration 32/1000 | Loss: 0.00002584
Iteration 33/1000 | Loss: 0.00002584
Iteration 34/1000 | Loss: 0.00002584
Iteration 35/1000 | Loss: 0.00002584
Iteration 36/1000 | Loss: 0.00002584
Iteration 37/1000 | Loss: 0.00002584
Iteration 38/1000 | Loss: 0.00002584
Iteration 39/1000 | Loss: 0.00002584
Iteration 40/1000 | Loss: 0.00002584
Iteration 41/1000 | Loss: 0.00002584
Iteration 42/1000 | Loss: 0.00002584
Iteration 43/1000 | Loss: 0.00002582
Iteration 44/1000 | Loss: 0.00002582
Iteration 45/1000 | Loss: 0.00002581
Iteration 46/1000 | Loss: 0.00002581
Iteration 47/1000 | Loss: 0.00002580
Iteration 48/1000 | Loss: 0.00002580
Iteration 49/1000 | Loss: 0.00002580
Iteration 50/1000 | Loss: 0.00002580
Iteration 51/1000 | Loss: 0.00002580
Iteration 52/1000 | Loss: 0.00002579
Iteration 53/1000 | Loss: 0.00002579
Iteration 54/1000 | Loss: 0.00002579
Iteration 55/1000 | Loss: 0.00002579
Iteration 56/1000 | Loss: 0.00002579
Iteration 57/1000 | Loss: 0.00002579
Iteration 58/1000 | Loss: 0.00002579
Iteration 59/1000 | Loss: 0.00002578
Iteration 60/1000 | Loss: 0.00002578
Iteration 61/1000 | Loss: 0.00002578
Iteration 62/1000 | Loss: 0.00002578
Iteration 63/1000 | Loss: 0.00002578
Iteration 64/1000 | Loss: 0.00002577
Iteration 65/1000 | Loss: 0.00002577
Iteration 66/1000 | Loss: 0.00002577
Iteration 67/1000 | Loss: 0.00002577
Iteration 68/1000 | Loss: 0.00002577
Iteration 69/1000 | Loss: 0.00002577
Iteration 70/1000 | Loss: 0.00002577
Iteration 71/1000 | Loss: 0.00002577
Iteration 72/1000 | Loss: 0.00002577
Iteration 73/1000 | Loss: 0.00002577
Iteration 74/1000 | Loss: 0.00002577
Iteration 75/1000 | Loss: 0.00002576
Iteration 76/1000 | Loss: 0.00002576
Iteration 77/1000 | Loss: 0.00002576
Iteration 78/1000 | Loss: 0.00002575
Iteration 79/1000 | Loss: 0.00002575
Iteration 80/1000 | Loss: 0.00002575
Iteration 81/1000 | Loss: 0.00002575
Iteration 82/1000 | Loss: 0.00002575
Iteration 83/1000 | Loss: 0.00002575
Iteration 84/1000 | Loss: 0.00002575
Iteration 85/1000 | Loss: 0.00002574
Iteration 86/1000 | Loss: 0.00002574
Iteration 87/1000 | Loss: 0.00002574
Iteration 88/1000 | Loss: 0.00002574
Iteration 89/1000 | Loss: 0.00002573
Iteration 90/1000 | Loss: 0.00002573
Iteration 91/1000 | Loss: 0.00002573
Iteration 92/1000 | Loss: 0.00002573
Iteration 93/1000 | Loss: 0.00002573
Iteration 94/1000 | Loss: 0.00002573
Iteration 95/1000 | Loss: 0.00002573
Iteration 96/1000 | Loss: 0.00002573
Iteration 97/1000 | Loss: 0.00002573
Iteration 98/1000 | Loss: 0.00002573
Iteration 99/1000 | Loss: 0.00002572
Iteration 100/1000 | Loss: 0.00002572
Iteration 101/1000 | Loss: 0.00002572
Iteration 102/1000 | Loss: 0.00002572
Iteration 103/1000 | Loss: 0.00002572
Iteration 104/1000 | Loss: 0.00002571
Iteration 105/1000 | Loss: 0.00002571
Iteration 106/1000 | Loss: 0.00002571
Iteration 107/1000 | Loss: 0.00002571
Iteration 108/1000 | Loss: 0.00002571
Iteration 109/1000 | Loss: 0.00002571
Iteration 110/1000 | Loss: 0.00002571
Iteration 111/1000 | Loss: 0.00002571
Iteration 112/1000 | Loss: 0.00002571
Iteration 113/1000 | Loss: 0.00002571
Iteration 114/1000 | Loss: 0.00002571
Iteration 115/1000 | Loss: 0.00002571
Iteration 116/1000 | Loss: 0.00002570
Iteration 117/1000 | Loss: 0.00002570
Iteration 118/1000 | Loss: 0.00002570
Iteration 119/1000 | Loss: 0.00002570
Iteration 120/1000 | Loss: 0.00002570
Iteration 121/1000 | Loss: 0.00002570
Iteration 122/1000 | Loss: 0.00002570
Iteration 123/1000 | Loss: 0.00002570
Iteration 124/1000 | Loss: 0.00002570
Iteration 125/1000 | Loss: 0.00002570
Iteration 126/1000 | Loss: 0.00002570
Iteration 127/1000 | Loss: 0.00002570
Iteration 128/1000 | Loss: 0.00002570
Iteration 129/1000 | Loss: 0.00002570
Iteration 130/1000 | Loss: 0.00002570
Iteration 131/1000 | Loss: 0.00002570
Iteration 132/1000 | Loss: 0.00002570
Iteration 133/1000 | Loss: 0.00002570
Iteration 134/1000 | Loss: 0.00002570
Iteration 135/1000 | Loss: 0.00002570
Iteration 136/1000 | Loss: 0.00002570
Iteration 137/1000 | Loss: 0.00002570
Iteration 138/1000 | Loss: 0.00002570
Iteration 139/1000 | Loss: 0.00002570
Iteration 140/1000 | Loss: 0.00002570
Iteration 141/1000 | Loss: 0.00002570
Iteration 142/1000 | Loss: 0.00002570
Iteration 143/1000 | Loss: 0.00002570
Iteration 144/1000 | Loss: 0.00002570
Iteration 145/1000 | Loss: 0.00002570
Iteration 146/1000 | Loss: 0.00002570
Iteration 147/1000 | Loss: 0.00002570
Iteration 148/1000 | Loss: 0.00002570
Iteration 149/1000 | Loss: 0.00002570
Iteration 150/1000 | Loss: 0.00002570
Iteration 151/1000 | Loss: 0.00002570
Iteration 152/1000 | Loss: 0.00002570
Iteration 153/1000 | Loss: 0.00002570
Iteration 154/1000 | Loss: 0.00002570
Iteration 155/1000 | Loss: 0.00002570
Iteration 156/1000 | Loss: 0.00002570
Iteration 157/1000 | Loss: 0.00002570
Iteration 158/1000 | Loss: 0.00002570
Iteration 159/1000 | Loss: 0.00002570
Iteration 160/1000 | Loss: 0.00002570
Iteration 161/1000 | Loss: 0.00002570
Iteration 162/1000 | Loss: 0.00002570
Iteration 163/1000 | Loss: 0.00002570
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [2.569768184912391e-05, 2.569768184912391e-05, 2.569768184912391e-05, 2.569768184912391e-05, 2.569768184912391e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.569768184912391e-05

Optimization complete. Final v2v error: 4.189826488494873 mm

Highest mean error: 4.381222724914551 mm for frame 0

Lowest mean error: 3.743623733520508 mm for frame 48

Saving results

Total time: 37.373478412628174
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799930
Iteration 2/25 | Loss: 0.00137130
Iteration 3/25 | Loss: 0.00111980
Iteration 4/25 | Loss: 0.00099646
Iteration 5/25 | Loss: 0.00093046
Iteration 6/25 | Loss: 0.00092744
Iteration 7/25 | Loss: 0.00091202
Iteration 8/25 | Loss: 0.00090585
Iteration 9/25 | Loss: 0.00090306
Iteration 10/25 | Loss: 0.00090282
Iteration 11/25 | Loss: 0.00090280
Iteration 12/25 | Loss: 0.00090280
Iteration 13/25 | Loss: 0.00090280
Iteration 14/25 | Loss: 0.00090279
Iteration 15/25 | Loss: 0.00090279
Iteration 16/25 | Loss: 0.00090279
Iteration 17/25 | Loss: 0.00090279
Iteration 18/25 | Loss: 0.00090279
Iteration 19/25 | Loss: 0.00090279
Iteration 20/25 | Loss: 0.00090279
Iteration 21/25 | Loss: 0.00090279
Iteration 22/25 | Loss: 0.00090279
Iteration 23/25 | Loss: 0.00090279
Iteration 24/25 | Loss: 0.00090279
Iteration 25/25 | Loss: 0.00090279

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03247476
Iteration 2/25 | Loss: 0.00156669
Iteration 3/25 | Loss: 0.00156668
Iteration 4/25 | Loss: 0.00156668
Iteration 5/25 | Loss: 0.00156668
Iteration 6/25 | Loss: 0.00156668
Iteration 7/25 | Loss: 0.00156668
Iteration 8/25 | Loss: 0.00156668
Iteration 9/25 | Loss: 0.00156668
Iteration 10/25 | Loss: 0.00156668
Iteration 11/25 | Loss: 0.00156668
Iteration 12/25 | Loss: 0.00156668
Iteration 13/25 | Loss: 0.00156668
Iteration 14/25 | Loss: 0.00156668
Iteration 15/25 | Loss: 0.00156668
Iteration 16/25 | Loss: 0.00156668
Iteration 17/25 | Loss: 0.00156668
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.001566678169183433, 0.001566678169183433, 0.001566678169183433, 0.001566678169183433, 0.001566678169183433]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001566678169183433

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00156668
Iteration 2/1000 | Loss: 0.00007003
Iteration 3/1000 | Loss: 0.00004746
Iteration 4/1000 | Loss: 0.00003597
Iteration 5/1000 | Loss: 0.00003376
Iteration 6/1000 | Loss: 0.00003253
Iteration 7/1000 | Loss: 0.00003128
Iteration 8/1000 | Loss: 0.00003066
Iteration 9/1000 | Loss: 0.00003014
Iteration 10/1000 | Loss: 0.00002960
Iteration 11/1000 | Loss: 0.00002929
Iteration 12/1000 | Loss: 0.00002905
Iteration 13/1000 | Loss: 0.00002887
Iteration 14/1000 | Loss: 0.00002880
Iteration 15/1000 | Loss: 0.00002877
Iteration 16/1000 | Loss: 0.00002876
Iteration 17/1000 | Loss: 0.00002876
Iteration 18/1000 | Loss: 0.00002875
Iteration 19/1000 | Loss: 0.00002875
Iteration 20/1000 | Loss: 0.00002874
Iteration 21/1000 | Loss: 0.00002874
Iteration 22/1000 | Loss: 0.00002874
Iteration 23/1000 | Loss: 0.00002873
Iteration 24/1000 | Loss: 0.00002872
Iteration 25/1000 | Loss: 0.00002871
Iteration 26/1000 | Loss: 0.00002870
Iteration 27/1000 | Loss: 0.00002870
Iteration 28/1000 | Loss: 0.00002870
Iteration 29/1000 | Loss: 0.00002869
Iteration 30/1000 | Loss: 0.00002869
Iteration 31/1000 | Loss: 0.00002868
Iteration 32/1000 | Loss: 0.00002868
Iteration 33/1000 | Loss: 0.00002867
Iteration 34/1000 | Loss: 0.00002867
Iteration 35/1000 | Loss: 0.00002866
Iteration 36/1000 | Loss: 0.00002866
Iteration 37/1000 | Loss: 0.00002865
Iteration 38/1000 | Loss: 0.00002865
Iteration 39/1000 | Loss: 0.00002864
Iteration 40/1000 | Loss: 0.00002864
Iteration 41/1000 | Loss: 0.00002863
Iteration 42/1000 | Loss: 0.00002863
Iteration 43/1000 | Loss: 0.00002863
Iteration 44/1000 | Loss: 0.00002862
Iteration 45/1000 | Loss: 0.00002861
Iteration 46/1000 | Loss: 0.00002860
Iteration 47/1000 | Loss: 0.00002860
Iteration 48/1000 | Loss: 0.00002860
Iteration 49/1000 | Loss: 0.00002859
Iteration 50/1000 | Loss: 0.00002859
Iteration 51/1000 | Loss: 0.00002858
Iteration 52/1000 | Loss: 0.00002858
Iteration 53/1000 | Loss: 0.00002858
Iteration 54/1000 | Loss: 0.00002857
Iteration 55/1000 | Loss: 0.00002857
Iteration 56/1000 | Loss: 0.00002857
Iteration 57/1000 | Loss: 0.00002857
Iteration 58/1000 | Loss: 0.00002857
Iteration 59/1000 | Loss: 0.00002856
Iteration 60/1000 | Loss: 0.00002856
Iteration 61/1000 | Loss: 0.00002856
Iteration 62/1000 | Loss: 0.00002855
Iteration 63/1000 | Loss: 0.00002855
Iteration 64/1000 | Loss: 0.00002855
Iteration 65/1000 | Loss: 0.00002854
Iteration 66/1000 | Loss: 0.00002854
Iteration 67/1000 | Loss: 0.00002852
Iteration 68/1000 | Loss: 0.00002851
Iteration 69/1000 | Loss: 0.00002851
Iteration 70/1000 | Loss: 0.00002850
Iteration 71/1000 | Loss: 0.00002850
Iteration 72/1000 | Loss: 0.00002850
Iteration 73/1000 | Loss: 0.00002849
Iteration 74/1000 | Loss: 0.00002849
Iteration 75/1000 | Loss: 0.00002849
Iteration 76/1000 | Loss: 0.00002848
Iteration 77/1000 | Loss: 0.00002848
Iteration 78/1000 | Loss: 0.00002847
Iteration 79/1000 | Loss: 0.00002847
Iteration 80/1000 | Loss: 0.00002847
Iteration 81/1000 | Loss: 0.00002847
Iteration 82/1000 | Loss: 0.00002847
Iteration 83/1000 | Loss: 0.00002846
Iteration 84/1000 | Loss: 0.00002846
Iteration 85/1000 | Loss: 0.00002846
Iteration 86/1000 | Loss: 0.00002846
Iteration 87/1000 | Loss: 0.00002846
Iteration 88/1000 | Loss: 0.00002846
Iteration 89/1000 | Loss: 0.00002846
Iteration 90/1000 | Loss: 0.00002846
Iteration 91/1000 | Loss: 0.00002845
Iteration 92/1000 | Loss: 0.00002845
Iteration 93/1000 | Loss: 0.00002844
Iteration 94/1000 | Loss: 0.00002844
Iteration 95/1000 | Loss: 0.00002844
Iteration 96/1000 | Loss: 0.00002844
Iteration 97/1000 | Loss: 0.00002844
Iteration 98/1000 | Loss: 0.00002844
Iteration 99/1000 | Loss: 0.00002844
Iteration 100/1000 | Loss: 0.00002844
Iteration 101/1000 | Loss: 0.00002843
Iteration 102/1000 | Loss: 0.00002843
Iteration 103/1000 | Loss: 0.00002843
Iteration 104/1000 | Loss: 0.00002842
Iteration 105/1000 | Loss: 0.00002842
Iteration 106/1000 | Loss: 0.00002842
Iteration 107/1000 | Loss: 0.00002842
Iteration 108/1000 | Loss: 0.00002842
Iteration 109/1000 | Loss: 0.00002842
Iteration 110/1000 | Loss: 0.00002842
Iteration 111/1000 | Loss: 0.00002842
Iteration 112/1000 | Loss: 0.00002842
Iteration 113/1000 | Loss: 0.00002842
Iteration 114/1000 | Loss: 0.00002842
Iteration 115/1000 | Loss: 0.00002841
Iteration 116/1000 | Loss: 0.00002841
Iteration 117/1000 | Loss: 0.00002841
Iteration 118/1000 | Loss: 0.00002840
Iteration 119/1000 | Loss: 0.00002840
Iteration 120/1000 | Loss: 0.00002840
Iteration 121/1000 | Loss: 0.00002840
Iteration 122/1000 | Loss: 0.00002839
Iteration 123/1000 | Loss: 0.00002839
Iteration 124/1000 | Loss: 0.00002839
Iteration 125/1000 | Loss: 0.00002839
Iteration 126/1000 | Loss: 0.00002838
Iteration 127/1000 | Loss: 0.00002838
Iteration 128/1000 | Loss: 0.00002838
Iteration 129/1000 | Loss: 0.00002838
Iteration 130/1000 | Loss: 0.00002838
Iteration 131/1000 | Loss: 0.00002838
Iteration 132/1000 | Loss: 0.00002838
Iteration 133/1000 | Loss: 0.00002838
Iteration 134/1000 | Loss: 0.00002837
Iteration 135/1000 | Loss: 0.00002837
Iteration 136/1000 | Loss: 0.00002837
Iteration 137/1000 | Loss: 0.00002837
Iteration 138/1000 | Loss: 0.00002836
Iteration 139/1000 | Loss: 0.00002836
Iteration 140/1000 | Loss: 0.00002836
Iteration 141/1000 | Loss: 0.00002836
Iteration 142/1000 | Loss: 0.00002835
Iteration 143/1000 | Loss: 0.00002835
Iteration 144/1000 | Loss: 0.00002835
Iteration 145/1000 | Loss: 0.00002835
Iteration 146/1000 | Loss: 0.00002835
Iteration 147/1000 | Loss: 0.00002835
Iteration 148/1000 | Loss: 0.00002835
Iteration 149/1000 | Loss: 0.00002835
Iteration 150/1000 | Loss: 0.00002835
Iteration 151/1000 | Loss: 0.00002835
Iteration 152/1000 | Loss: 0.00002834
Iteration 153/1000 | Loss: 0.00002834
Iteration 154/1000 | Loss: 0.00002834
Iteration 155/1000 | Loss: 0.00002834
Iteration 156/1000 | Loss: 0.00002834
Iteration 157/1000 | Loss: 0.00002834
Iteration 158/1000 | Loss: 0.00002834
Iteration 159/1000 | Loss: 0.00002834
Iteration 160/1000 | Loss: 0.00002834
Iteration 161/1000 | Loss: 0.00002834
Iteration 162/1000 | Loss: 0.00002834
Iteration 163/1000 | Loss: 0.00002834
Iteration 164/1000 | Loss: 0.00002834
Iteration 165/1000 | Loss: 0.00002834
Iteration 166/1000 | Loss: 0.00002834
Iteration 167/1000 | Loss: 0.00002834
Iteration 168/1000 | Loss: 0.00002834
Iteration 169/1000 | Loss: 0.00002834
Iteration 170/1000 | Loss: 0.00002834
Iteration 171/1000 | Loss: 0.00002834
Iteration 172/1000 | Loss: 0.00002834
Iteration 173/1000 | Loss: 0.00002834
Iteration 174/1000 | Loss: 0.00002834
Iteration 175/1000 | Loss: 0.00002834
Iteration 176/1000 | Loss: 0.00002834
Iteration 177/1000 | Loss: 0.00002834
Iteration 178/1000 | Loss: 0.00002834
Iteration 179/1000 | Loss: 0.00002834
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [2.833651888067834e-05, 2.833651888067834e-05, 2.833651888067834e-05, 2.833651888067834e-05, 2.833651888067834e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.833651888067834e-05

Optimization complete. Final v2v error: 4.350062370300293 mm

Highest mean error: 5.232387065887451 mm for frame 2

Lowest mean error: 3.441920042037964 mm for frame 138

Saving results

Total time: 48.81607913970947
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103993
Iteration 2/25 | Loss: 0.00174840
Iteration 3/25 | Loss: 0.00132678
Iteration 4/25 | Loss: 0.00087609
Iteration 5/25 | Loss: 0.00081532
Iteration 6/25 | Loss: 0.00082673
Iteration 7/25 | Loss: 0.00079314
Iteration 8/25 | Loss: 0.00080307
Iteration 9/25 | Loss: 0.00078812
Iteration 10/25 | Loss: 0.00078070
Iteration 11/25 | Loss: 0.00078670
Iteration 12/25 | Loss: 0.00077860
Iteration 13/25 | Loss: 0.00077844
Iteration 14/25 | Loss: 0.00077840
Iteration 15/25 | Loss: 0.00077840
Iteration 16/25 | Loss: 0.00077839
Iteration 17/25 | Loss: 0.00077839
Iteration 18/25 | Loss: 0.00077839
Iteration 19/25 | Loss: 0.00077839
Iteration 20/25 | Loss: 0.00077839
Iteration 21/25 | Loss: 0.00077839
Iteration 22/25 | Loss: 0.00077839
Iteration 23/25 | Loss: 0.00077839
Iteration 24/25 | Loss: 0.00077838
Iteration 25/25 | Loss: 0.00077838

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.14344358
Iteration 2/25 | Loss: 0.00140568
Iteration 3/25 | Loss: 0.00140568
Iteration 4/25 | Loss: 0.00132435
Iteration 5/25 | Loss: 0.00132435
Iteration 6/25 | Loss: 0.00132435
Iteration 7/25 | Loss: 0.00132435
Iteration 8/25 | Loss: 0.00132435
Iteration 9/25 | Loss: 0.00132435
Iteration 10/25 | Loss: 0.00132435
Iteration 11/25 | Loss: 0.00132435
Iteration 12/25 | Loss: 0.00132435
Iteration 13/25 | Loss: 0.00132435
Iteration 14/25 | Loss: 0.00132435
Iteration 15/25 | Loss: 0.00132435
Iteration 16/25 | Loss: 0.00132435
Iteration 17/25 | Loss: 0.00132435
Iteration 18/25 | Loss: 0.00132435
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013243455905467272, 0.0013243455905467272, 0.0013243455905467272, 0.0013243455905467272, 0.0013243455905467272]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013243455905467272

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00132435
Iteration 2/1000 | Loss: 0.00007967
Iteration 3/1000 | Loss: 0.00006777
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00006750
Iteration 6/1000 | Loss: 0.00007619
Iteration 7/1000 | Loss: 0.00014586
Iteration 8/1000 | Loss: 0.00003828
Iteration 9/1000 | Loss: 0.00001774
Iteration 10/1000 | Loss: 0.00001747
Iteration 11/1000 | Loss: 0.00001745
Iteration 12/1000 | Loss: 0.00006845
Iteration 13/1000 | Loss: 0.00004703
Iteration 14/1000 | Loss: 0.00001725
Iteration 15/1000 | Loss: 0.00003661
Iteration 16/1000 | Loss: 0.00001839
Iteration 17/1000 | Loss: 0.00001993
Iteration 18/1000 | Loss: 0.00012957
Iteration 19/1000 | Loss: 0.00001728
Iteration 20/1000 | Loss: 0.00003671
Iteration 21/1000 | Loss: 0.00006921
Iteration 22/1000 | Loss: 0.00003084
Iteration 23/1000 | Loss: 0.00002542
Iteration 24/1000 | Loss: 0.00004817
Iteration 25/1000 | Loss: 0.00001716
Iteration 26/1000 | Loss: 0.00002407
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001700
Iteration 29/1000 | Loss: 0.00001699
Iteration 30/1000 | Loss: 0.00001699
Iteration 31/1000 | Loss: 0.00001715
Iteration 32/1000 | Loss: 0.00001693
Iteration 33/1000 | Loss: 0.00001693
Iteration 34/1000 | Loss: 0.00001693
Iteration 35/1000 | Loss: 0.00001693
Iteration 36/1000 | Loss: 0.00001693
Iteration 37/1000 | Loss: 0.00001692
Iteration 38/1000 | Loss: 0.00001692
Iteration 39/1000 | Loss: 0.00001692
Iteration 40/1000 | Loss: 0.00001692
Iteration 41/1000 | Loss: 0.00001692
Iteration 42/1000 | Loss: 0.00001691
Iteration 43/1000 | Loss: 0.00001691
Iteration 44/1000 | Loss: 0.00001690
Iteration 45/1000 | Loss: 0.00001690
Iteration 46/1000 | Loss: 0.00001690
Iteration 47/1000 | Loss: 0.00001689
Iteration 48/1000 | Loss: 0.00001689
Iteration 49/1000 | Loss: 0.00003746
Iteration 50/1000 | Loss: 0.00007963
Iteration 51/1000 | Loss: 0.00002235
Iteration 52/1000 | Loss: 0.00002383
Iteration 53/1000 | Loss: 0.00001683
Iteration 54/1000 | Loss: 0.00002513
Iteration 55/1000 | Loss: 0.00001809
Iteration 56/1000 | Loss: 0.00001758
Iteration 57/1000 | Loss: 0.00001675
Iteration 58/1000 | Loss: 0.00001675
Iteration 59/1000 | Loss: 0.00001675
Iteration 60/1000 | Loss: 0.00001675
Iteration 61/1000 | Loss: 0.00001675
Iteration 62/1000 | Loss: 0.00001675
Iteration 63/1000 | Loss: 0.00001675
Iteration 64/1000 | Loss: 0.00001675
Iteration 65/1000 | Loss: 0.00001675
Iteration 66/1000 | Loss: 0.00001675
Iteration 67/1000 | Loss: 0.00001675
Iteration 68/1000 | Loss: 0.00001675
Iteration 69/1000 | Loss: 0.00001674
Iteration 70/1000 | Loss: 0.00001674
Iteration 71/1000 | Loss: 0.00001674
Iteration 72/1000 | Loss: 0.00001674
Iteration 73/1000 | Loss: 0.00001674
Iteration 74/1000 | Loss: 0.00001673
Iteration 75/1000 | Loss: 0.00001673
Iteration 76/1000 | Loss: 0.00001673
Iteration 77/1000 | Loss: 0.00001673
Iteration 78/1000 | Loss: 0.00001673
Iteration 79/1000 | Loss: 0.00001673
Iteration 80/1000 | Loss: 0.00001673
Iteration 81/1000 | Loss: 0.00001673
Iteration 82/1000 | Loss: 0.00001673
Iteration 83/1000 | Loss: 0.00001673
Iteration 84/1000 | Loss: 0.00001673
Iteration 85/1000 | Loss: 0.00001672
Iteration 86/1000 | Loss: 0.00001672
Iteration 87/1000 | Loss: 0.00001672
Iteration 88/1000 | Loss: 0.00001670
Iteration 89/1000 | Loss: 0.00001670
Iteration 90/1000 | Loss: 0.00001669
Iteration 91/1000 | Loss: 0.00001669
Iteration 92/1000 | Loss: 0.00001669
Iteration 93/1000 | Loss: 0.00001669
Iteration 94/1000 | Loss: 0.00001668
Iteration 95/1000 | Loss: 0.00001668
Iteration 96/1000 | Loss: 0.00001668
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001666
Iteration 99/1000 | Loss: 0.00001666
Iteration 100/1000 | Loss: 0.00001665
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001665
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001665
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001664
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00001663
Iteration 110/1000 | Loss: 0.00001662
Iteration 111/1000 | Loss: 0.00001662
Iteration 112/1000 | Loss: 0.00001662
Iteration 113/1000 | Loss: 0.00001662
Iteration 114/1000 | Loss: 0.00001662
Iteration 115/1000 | Loss: 0.00001662
Iteration 116/1000 | Loss: 0.00001661
Iteration 117/1000 | Loss: 0.00001661
Iteration 118/1000 | Loss: 0.00001661
Iteration 119/1000 | Loss: 0.00001661
Iteration 120/1000 | Loss: 0.00001661
Iteration 121/1000 | Loss: 0.00008823
Iteration 122/1000 | Loss: 0.00001708
Iteration 123/1000 | Loss: 0.00001666
Iteration 124/1000 | Loss: 0.00001661
Iteration 125/1000 | Loss: 0.00001660
Iteration 126/1000 | Loss: 0.00001660
Iteration 127/1000 | Loss: 0.00001660
Iteration 128/1000 | Loss: 0.00001660
Iteration 129/1000 | Loss: 0.00001658
Iteration 130/1000 | Loss: 0.00001658
Iteration 131/1000 | Loss: 0.00001658
Iteration 132/1000 | Loss: 0.00001658
Iteration 133/1000 | Loss: 0.00001658
Iteration 134/1000 | Loss: 0.00006789
Iteration 135/1000 | Loss: 0.00008008
Iteration 136/1000 | Loss: 0.00001882
Iteration 137/1000 | Loss: 0.00001666
Iteration 138/1000 | Loss: 0.00001662
Iteration 139/1000 | Loss: 0.00001662
Iteration 140/1000 | Loss: 0.00001662
Iteration 141/1000 | Loss: 0.00001662
Iteration 142/1000 | Loss: 0.00001662
Iteration 143/1000 | Loss: 0.00001662
Iteration 144/1000 | Loss: 0.00001662
Iteration 145/1000 | Loss: 0.00001662
Iteration 146/1000 | Loss: 0.00001662
Iteration 147/1000 | Loss: 0.00001662
Iteration 148/1000 | Loss: 0.00001661
Iteration 149/1000 | Loss: 0.00001661
Iteration 150/1000 | Loss: 0.00001661
Iteration 151/1000 | Loss: 0.00001661
Iteration 152/1000 | Loss: 0.00001661
Iteration 153/1000 | Loss: 0.00001661
Iteration 154/1000 | Loss: 0.00001660
Iteration 155/1000 | Loss: 0.00001657
Iteration 156/1000 | Loss: 0.00001657
Iteration 157/1000 | Loss: 0.00001657
Iteration 158/1000 | Loss: 0.00001657
Iteration 159/1000 | Loss: 0.00001657
Iteration 160/1000 | Loss: 0.00001657
Iteration 161/1000 | Loss: 0.00001657
Iteration 162/1000 | Loss: 0.00001657
Iteration 163/1000 | Loss: 0.00001657
Iteration 164/1000 | Loss: 0.00001657
Iteration 165/1000 | Loss: 0.00001657
Iteration 166/1000 | Loss: 0.00001656
Iteration 167/1000 | Loss: 0.00001656
Iteration 168/1000 | Loss: 0.00001656
Iteration 169/1000 | Loss: 0.00001656
Iteration 170/1000 | Loss: 0.00001656
Iteration 171/1000 | Loss: 0.00001655
Iteration 172/1000 | Loss: 0.00001655
Iteration 173/1000 | Loss: 0.00001655
Iteration 174/1000 | Loss: 0.00001655
Iteration 175/1000 | Loss: 0.00006393
Iteration 176/1000 | Loss: 0.00001743
Iteration 177/1000 | Loss: 0.00001659
Iteration 178/1000 | Loss: 0.00001658
Iteration 179/1000 | Loss: 0.00001657
Iteration 180/1000 | Loss: 0.00001657
Iteration 181/1000 | Loss: 0.00001657
Iteration 182/1000 | Loss: 0.00001656
Iteration 183/1000 | Loss: 0.00001656
Iteration 184/1000 | Loss: 0.00001656
Iteration 185/1000 | Loss: 0.00001656
Iteration 186/1000 | Loss: 0.00001656
Iteration 187/1000 | Loss: 0.00001656
Iteration 188/1000 | Loss: 0.00001656
Iteration 189/1000 | Loss: 0.00001656
Iteration 190/1000 | Loss: 0.00001656
Iteration 191/1000 | Loss: 0.00002757
Iteration 192/1000 | Loss: 0.00001661
Iteration 193/1000 | Loss: 0.00001655
Iteration 194/1000 | Loss: 0.00001655
Iteration 195/1000 | Loss: 0.00001655
Iteration 196/1000 | Loss: 0.00001655
Iteration 197/1000 | Loss: 0.00001655
Iteration 198/1000 | Loss: 0.00001655
Iteration 199/1000 | Loss: 0.00001655
Iteration 200/1000 | Loss: 0.00001655
Iteration 201/1000 | Loss: 0.00001655
Iteration 202/1000 | Loss: 0.00001655
Iteration 203/1000 | Loss: 0.00001654
Iteration 204/1000 | Loss: 0.00001654
Iteration 205/1000 | Loss: 0.00001654
Iteration 206/1000 | Loss: 0.00001654
Iteration 207/1000 | Loss: 0.00001654
Iteration 208/1000 | Loss: 0.00001654
Iteration 209/1000 | Loss: 0.00001654
Iteration 210/1000 | Loss: 0.00001654
Iteration 211/1000 | Loss: 0.00001654
Iteration 212/1000 | Loss: 0.00001654
Iteration 213/1000 | Loss: 0.00001654
Iteration 214/1000 | Loss: 0.00001654
Iteration 215/1000 | Loss: 0.00001654
Iteration 216/1000 | Loss: 0.00001654
Iteration 217/1000 | Loss: 0.00001654
Iteration 218/1000 | Loss: 0.00001654
Iteration 219/1000 | Loss: 0.00001654
Iteration 220/1000 | Loss: 0.00001654
Iteration 221/1000 | Loss: 0.00001653
Iteration 222/1000 | Loss: 0.00001653
Iteration 223/1000 | Loss: 0.00001653
Iteration 224/1000 | Loss: 0.00001653
Iteration 225/1000 | Loss: 0.00001653
Iteration 226/1000 | Loss: 0.00001653
Iteration 227/1000 | Loss: 0.00001653
Iteration 228/1000 | Loss: 0.00003213
Iteration 229/1000 | Loss: 0.00001753
Iteration 230/1000 | Loss: 0.00001659
Iteration 231/1000 | Loss: 0.00001752
Iteration 232/1000 | Loss: 0.00002112
Iteration 233/1000 | Loss: 0.00001658
Iteration 234/1000 | Loss: 0.00001658
Iteration 235/1000 | Loss: 0.00001658
Iteration 236/1000 | Loss: 0.00001658
Iteration 237/1000 | Loss: 0.00001658
Iteration 238/1000 | Loss: 0.00001658
Iteration 239/1000 | Loss: 0.00001658
Iteration 240/1000 | Loss: 0.00001658
Iteration 241/1000 | Loss: 0.00001658
Iteration 242/1000 | Loss: 0.00001658
Iteration 243/1000 | Loss: 0.00001658
Iteration 244/1000 | Loss: 0.00001658
Iteration 245/1000 | Loss: 0.00001658
Iteration 246/1000 | Loss: 0.00001657
Iteration 247/1000 | Loss: 0.00001657
Iteration 248/1000 | Loss: 0.00001657
Iteration 249/1000 | Loss: 0.00001657
Iteration 250/1000 | Loss: 0.00001657
Iteration 251/1000 | Loss: 0.00001657
Iteration 252/1000 | Loss: 0.00001657
Iteration 253/1000 | Loss: 0.00001657
Iteration 254/1000 | Loss: 0.00001657
Iteration 255/1000 | Loss: 0.00001657
Iteration 256/1000 | Loss: 0.00001657
Iteration 257/1000 | Loss: 0.00001657
Iteration 258/1000 | Loss: 0.00001657
Iteration 259/1000 | Loss: 0.00001657
Iteration 260/1000 | Loss: 0.00001657
Iteration 261/1000 | Loss: 0.00001657
Iteration 262/1000 | Loss: 0.00001657
Iteration 263/1000 | Loss: 0.00001656
Iteration 264/1000 | Loss: 0.00001656
Iteration 265/1000 | Loss: 0.00001656
Iteration 266/1000 | Loss: 0.00001656
Iteration 267/1000 | Loss: 0.00001656
Iteration 268/1000 | Loss: 0.00001656
Iteration 269/1000 | Loss: 0.00001656
Iteration 270/1000 | Loss: 0.00001656
Iteration 271/1000 | Loss: 0.00001656
Iteration 272/1000 | Loss: 0.00001656
Iteration 273/1000 | Loss: 0.00001656
Iteration 274/1000 | Loss: 0.00001656
Iteration 275/1000 | Loss: 0.00001656
Iteration 276/1000 | Loss: 0.00001656
Iteration 277/1000 | Loss: 0.00001656
Iteration 278/1000 | Loss: 0.00001656
Iteration 279/1000 | Loss: 0.00001656
Iteration 280/1000 | Loss: 0.00001656
Iteration 281/1000 | Loss: 0.00001656
Iteration 282/1000 | Loss: 0.00001656
Iteration 283/1000 | Loss: 0.00001656
Iteration 284/1000 | Loss: 0.00001656
Iteration 285/1000 | Loss: 0.00001656
Iteration 286/1000 | Loss: 0.00001656
Iteration 287/1000 | Loss: 0.00001656
Iteration 288/1000 | Loss: 0.00001656
Iteration 289/1000 | Loss: 0.00001656
Iteration 290/1000 | Loss: 0.00001656
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 290. Stopping optimization.
Last 5 losses: [1.655728010518942e-05, 1.655728010518942e-05, 1.655728010518942e-05, 1.655728010518942e-05, 1.655728010518942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.655728010518942e-05

Optimization complete. Final v2v error: 3.4308574199676514 mm

Highest mean error: 4.006015300750732 mm for frame 85

Lowest mean error: 3.151486873626709 mm for frame 123

Saving results

Total time: 98.94092845916748
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1032/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1032.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1032
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00946774
Iteration 2/25 | Loss: 0.00164010
Iteration 3/25 | Loss: 0.00108728
Iteration 4/25 | Loss: 0.00102027
Iteration 5/25 | Loss: 0.00101855
Iteration 6/25 | Loss: 0.00097985
Iteration 7/25 | Loss: 0.00096227
Iteration 8/25 | Loss: 0.00096595
Iteration 9/25 | Loss: 0.00096379
Iteration 10/25 | Loss: 0.00095235
Iteration 11/25 | Loss: 0.00095283
Iteration 12/25 | Loss: 0.00094897
Iteration 13/25 | Loss: 0.00094441
Iteration 14/25 | Loss: 0.00094003
Iteration 15/25 | Loss: 0.00093884
Iteration 16/25 | Loss: 0.00093843
Iteration 17/25 | Loss: 0.00093828
Iteration 18/25 | Loss: 0.00093826
Iteration 19/25 | Loss: 0.00093826
Iteration 20/25 | Loss: 0.00093826
Iteration 21/25 | Loss: 0.00093826
Iteration 22/25 | Loss: 0.00093826
Iteration 23/25 | Loss: 0.00093826
Iteration 24/25 | Loss: 0.00093826
Iteration 25/25 | Loss: 0.00093826

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78950953
Iteration 2/25 | Loss: 0.00143475
Iteration 3/25 | Loss: 0.00143472
Iteration 4/25 | Loss: 0.00143472
Iteration 5/25 | Loss: 0.00143472
Iteration 6/25 | Loss: 0.00143472
Iteration 7/25 | Loss: 0.00143472
Iteration 8/25 | Loss: 0.00143472
Iteration 9/25 | Loss: 0.00143472
Iteration 10/25 | Loss: 0.00143472
Iteration 11/25 | Loss: 0.00143472
Iteration 12/25 | Loss: 0.00143472
Iteration 13/25 | Loss: 0.00143472
Iteration 14/25 | Loss: 0.00143472
Iteration 15/25 | Loss: 0.00143472
Iteration 16/25 | Loss: 0.00143472
Iteration 17/25 | Loss: 0.00143472
Iteration 18/25 | Loss: 0.00143472
Iteration 19/25 | Loss: 0.00143472
Iteration 20/25 | Loss: 0.00143472
Iteration 21/25 | Loss: 0.00143472
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001434715581126511, 0.001434715581126511, 0.001434715581126511, 0.001434715581126511, 0.001434715581126511]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001434715581126511

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00143472
Iteration 2/1000 | Loss: 0.00011705
Iteration 3/1000 | Loss: 0.00008795
Iteration 4/1000 | Loss: 0.00007681
Iteration 5/1000 | Loss: 0.00006936
Iteration 6/1000 | Loss: 0.00066890
Iteration 7/1000 | Loss: 0.00021586
Iteration 8/1000 | Loss: 0.00275049
Iteration 9/1000 | Loss: 0.00038042
Iteration 10/1000 | Loss: 0.00059850
Iteration 11/1000 | Loss: 0.00283636
Iteration 12/1000 | Loss: 0.00020681
Iteration 13/1000 | Loss: 0.00216540
Iteration 14/1000 | Loss: 0.00025009
Iteration 15/1000 | Loss: 0.00199812
Iteration 16/1000 | Loss: 0.00046105
Iteration 17/1000 | Loss: 0.00183398
Iteration 18/1000 | Loss: 0.00021685
Iteration 19/1000 | Loss: 0.00015001
Iteration 20/1000 | Loss: 0.00121794
Iteration 21/1000 | Loss: 0.00031579
Iteration 22/1000 | Loss: 0.00234422
Iteration 23/1000 | Loss: 0.00062754
Iteration 24/1000 | Loss: 0.00012198
Iteration 25/1000 | Loss: 0.00018190
Iteration 26/1000 | Loss: 0.00111439
Iteration 27/1000 | Loss: 0.00025291
Iteration 28/1000 | Loss: 0.00010714
Iteration 29/1000 | Loss: 0.00016443
Iteration 30/1000 | Loss: 0.00225778
Iteration 31/1000 | Loss: 0.00074601
Iteration 32/1000 | Loss: 0.00091597
Iteration 33/1000 | Loss: 0.00225348
Iteration 34/1000 | Loss: 0.00116423
Iteration 35/1000 | Loss: 0.00175407
Iteration 36/1000 | Loss: 0.00091999
Iteration 37/1000 | Loss: 0.00033941
Iteration 38/1000 | Loss: 0.00005521
Iteration 39/1000 | Loss: 0.00222642
Iteration 40/1000 | Loss: 0.00227409
Iteration 41/1000 | Loss: 0.00070809
Iteration 42/1000 | Loss: 0.00051051
Iteration 43/1000 | Loss: 0.00032197
Iteration 44/1000 | Loss: 0.00005990
Iteration 45/1000 | Loss: 0.00014970
Iteration 46/1000 | Loss: 0.00028076
Iteration 47/1000 | Loss: 0.00018786
Iteration 48/1000 | Loss: 0.00024360
Iteration 49/1000 | Loss: 0.00024656
Iteration 50/1000 | Loss: 0.00015832
Iteration 51/1000 | Loss: 0.00041093
Iteration 52/1000 | Loss: 0.00005089
Iteration 53/1000 | Loss: 0.00004238
Iteration 54/1000 | Loss: 0.00009502
Iteration 55/1000 | Loss: 0.00003753
Iteration 56/1000 | Loss: 0.00003503
Iteration 57/1000 | Loss: 0.00003271
Iteration 58/1000 | Loss: 0.00003137
Iteration 59/1000 | Loss: 0.00003069
Iteration 60/1000 | Loss: 0.00003003
Iteration 61/1000 | Loss: 0.00002968
Iteration 62/1000 | Loss: 0.00002940
Iteration 63/1000 | Loss: 0.00064728
Iteration 64/1000 | Loss: 0.00004399
Iteration 65/1000 | Loss: 0.00003604
Iteration 66/1000 | Loss: 0.00003207
Iteration 67/1000 | Loss: 0.00002903
Iteration 68/1000 | Loss: 0.00002794
Iteration 69/1000 | Loss: 0.00002734
Iteration 70/1000 | Loss: 0.00002681
Iteration 71/1000 | Loss: 0.00002653
Iteration 72/1000 | Loss: 0.00002645
Iteration 73/1000 | Loss: 0.00002630
Iteration 74/1000 | Loss: 0.00002630
Iteration 75/1000 | Loss: 0.00002629
Iteration 76/1000 | Loss: 0.00002629
Iteration 77/1000 | Loss: 0.00002628
Iteration 78/1000 | Loss: 0.00002628
Iteration 79/1000 | Loss: 0.00002628
Iteration 80/1000 | Loss: 0.00002627
Iteration 81/1000 | Loss: 0.00002627
Iteration 82/1000 | Loss: 0.00002627
Iteration 83/1000 | Loss: 0.00002626
Iteration 84/1000 | Loss: 0.00002626
Iteration 85/1000 | Loss: 0.00002626
Iteration 86/1000 | Loss: 0.00002626
Iteration 87/1000 | Loss: 0.00002626
Iteration 88/1000 | Loss: 0.00002626
Iteration 89/1000 | Loss: 0.00002625
Iteration 90/1000 | Loss: 0.00002625
Iteration 91/1000 | Loss: 0.00002625
Iteration 92/1000 | Loss: 0.00002625
Iteration 93/1000 | Loss: 0.00002624
Iteration 94/1000 | Loss: 0.00002624
Iteration 95/1000 | Loss: 0.00002624
Iteration 96/1000 | Loss: 0.00002624
Iteration 97/1000 | Loss: 0.00002624
Iteration 98/1000 | Loss: 0.00002624
Iteration 99/1000 | Loss: 0.00002624
Iteration 100/1000 | Loss: 0.00002623
Iteration 101/1000 | Loss: 0.00002623
Iteration 102/1000 | Loss: 0.00002623
Iteration 103/1000 | Loss: 0.00002623
Iteration 104/1000 | Loss: 0.00002623
Iteration 105/1000 | Loss: 0.00002623
Iteration 106/1000 | Loss: 0.00002622
Iteration 107/1000 | Loss: 0.00002622
Iteration 108/1000 | Loss: 0.00002622
Iteration 109/1000 | Loss: 0.00002622
Iteration 110/1000 | Loss: 0.00002622
Iteration 111/1000 | Loss: 0.00002622
Iteration 112/1000 | Loss: 0.00002622
Iteration 113/1000 | Loss: 0.00002621
Iteration 114/1000 | Loss: 0.00002621
Iteration 115/1000 | Loss: 0.00002621
Iteration 116/1000 | Loss: 0.00002621
Iteration 117/1000 | Loss: 0.00002620
Iteration 118/1000 | Loss: 0.00002620
Iteration 119/1000 | Loss: 0.00002620
Iteration 120/1000 | Loss: 0.00002620
Iteration 121/1000 | Loss: 0.00002620
Iteration 122/1000 | Loss: 0.00002620
Iteration 123/1000 | Loss: 0.00002620
Iteration 124/1000 | Loss: 0.00002620
Iteration 125/1000 | Loss: 0.00002620
Iteration 126/1000 | Loss: 0.00002619
Iteration 127/1000 | Loss: 0.00002619
Iteration 128/1000 | Loss: 0.00002619
Iteration 129/1000 | Loss: 0.00002619
Iteration 130/1000 | Loss: 0.00002619
Iteration 131/1000 | Loss: 0.00002618
Iteration 132/1000 | Loss: 0.00002618
Iteration 133/1000 | Loss: 0.00002618
Iteration 134/1000 | Loss: 0.00002618
Iteration 135/1000 | Loss: 0.00002618
Iteration 136/1000 | Loss: 0.00002618
Iteration 137/1000 | Loss: 0.00002618
Iteration 138/1000 | Loss: 0.00002617
Iteration 139/1000 | Loss: 0.00002617
Iteration 140/1000 | Loss: 0.00002617
Iteration 141/1000 | Loss: 0.00002617
Iteration 142/1000 | Loss: 0.00002617
Iteration 143/1000 | Loss: 0.00002616
Iteration 144/1000 | Loss: 0.00002616
Iteration 145/1000 | Loss: 0.00002616
Iteration 146/1000 | Loss: 0.00002616
Iteration 147/1000 | Loss: 0.00002615
Iteration 148/1000 | Loss: 0.00002615
Iteration 149/1000 | Loss: 0.00002615
Iteration 150/1000 | Loss: 0.00002614
Iteration 151/1000 | Loss: 0.00002614
Iteration 152/1000 | Loss: 0.00002614
Iteration 153/1000 | Loss: 0.00002614
Iteration 154/1000 | Loss: 0.00002614
Iteration 155/1000 | Loss: 0.00002614
Iteration 156/1000 | Loss: 0.00002614
Iteration 157/1000 | Loss: 0.00002613
Iteration 158/1000 | Loss: 0.00002613
Iteration 159/1000 | Loss: 0.00002613
Iteration 160/1000 | Loss: 0.00002613
Iteration 161/1000 | Loss: 0.00002613
Iteration 162/1000 | Loss: 0.00002612
Iteration 163/1000 | Loss: 0.00002612
Iteration 164/1000 | Loss: 0.00002612
Iteration 165/1000 | Loss: 0.00002612
Iteration 166/1000 | Loss: 0.00002612
Iteration 167/1000 | Loss: 0.00002612
Iteration 168/1000 | Loss: 0.00002612
Iteration 169/1000 | Loss: 0.00002612
Iteration 170/1000 | Loss: 0.00002612
Iteration 171/1000 | Loss: 0.00002612
Iteration 172/1000 | Loss: 0.00002611
Iteration 173/1000 | Loss: 0.00002611
Iteration 174/1000 | Loss: 0.00002611
Iteration 175/1000 | Loss: 0.00002611
Iteration 176/1000 | Loss: 0.00002611
Iteration 177/1000 | Loss: 0.00002611
Iteration 178/1000 | Loss: 0.00002611
Iteration 179/1000 | Loss: 0.00002611
Iteration 180/1000 | Loss: 0.00002610
Iteration 181/1000 | Loss: 0.00002610
Iteration 182/1000 | Loss: 0.00002610
Iteration 183/1000 | Loss: 0.00002610
Iteration 184/1000 | Loss: 0.00002610
Iteration 185/1000 | Loss: 0.00002610
Iteration 186/1000 | Loss: 0.00002610
Iteration 187/1000 | Loss: 0.00002610
Iteration 188/1000 | Loss: 0.00002610
Iteration 189/1000 | Loss: 0.00002610
Iteration 190/1000 | Loss: 0.00002610
Iteration 191/1000 | Loss: 0.00002610
Iteration 192/1000 | Loss: 0.00002610
Iteration 193/1000 | Loss: 0.00002610
Iteration 194/1000 | Loss: 0.00002610
Iteration 195/1000 | Loss: 0.00002610
Iteration 196/1000 | Loss: 0.00002609
Iteration 197/1000 | Loss: 0.00002609
Iteration 198/1000 | Loss: 0.00002609
Iteration 199/1000 | Loss: 0.00002609
Iteration 200/1000 | Loss: 0.00002609
Iteration 201/1000 | Loss: 0.00002609
Iteration 202/1000 | Loss: 0.00002609
Iteration 203/1000 | Loss: 0.00002609
Iteration 204/1000 | Loss: 0.00002609
Iteration 205/1000 | Loss: 0.00002609
Iteration 206/1000 | Loss: 0.00002609
Iteration 207/1000 | Loss: 0.00002609
Iteration 208/1000 | Loss: 0.00002608
Iteration 209/1000 | Loss: 0.00002608
Iteration 210/1000 | Loss: 0.00002608
Iteration 211/1000 | Loss: 0.00002608
Iteration 212/1000 | Loss: 0.00002608
Iteration 213/1000 | Loss: 0.00002608
Iteration 214/1000 | Loss: 0.00002608
Iteration 215/1000 | Loss: 0.00002608
Iteration 216/1000 | Loss: 0.00002608
Iteration 217/1000 | Loss: 0.00002607
Iteration 218/1000 | Loss: 0.00002607
Iteration 219/1000 | Loss: 0.00002607
Iteration 220/1000 | Loss: 0.00002607
Iteration 221/1000 | Loss: 0.00002607
Iteration 222/1000 | Loss: 0.00002607
Iteration 223/1000 | Loss: 0.00002606
Iteration 224/1000 | Loss: 0.00002606
Iteration 225/1000 | Loss: 0.00002606
Iteration 226/1000 | Loss: 0.00002606
Iteration 227/1000 | Loss: 0.00002606
Iteration 228/1000 | Loss: 0.00002606
Iteration 229/1000 | Loss: 0.00002606
Iteration 230/1000 | Loss: 0.00002606
Iteration 231/1000 | Loss: 0.00002606
Iteration 232/1000 | Loss: 0.00002606
Iteration 233/1000 | Loss: 0.00002606
Iteration 234/1000 | Loss: 0.00002606
Iteration 235/1000 | Loss: 0.00002606
Iteration 236/1000 | Loss: 0.00002606
Iteration 237/1000 | Loss: 0.00002606
Iteration 238/1000 | Loss: 0.00002606
Iteration 239/1000 | Loss: 0.00002606
Iteration 240/1000 | Loss: 0.00002606
Iteration 241/1000 | Loss: 0.00002606
Iteration 242/1000 | Loss: 0.00002606
Iteration 243/1000 | Loss: 0.00002606
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 243. Stopping optimization.
Last 5 losses: [2.6056955903186463e-05, 2.6056955903186463e-05, 2.6056955903186463e-05, 2.6056955903186463e-05, 2.6056955903186463e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6056955903186463e-05

Optimization complete. Final v2v error: 4.253180980682373 mm

Highest mean error: 5.6034417152404785 mm for frame 40

Lowest mean error: 3.574038505554199 mm for frame 84

Saving results

Total time: 163.82568073272705
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1058/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1058.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1058
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00443973
Iteration 2/25 | Loss: 0.00098592
Iteration 3/25 | Loss: 0.00083532
Iteration 4/25 | Loss: 0.00080131
Iteration 5/25 | Loss: 0.00079440
Iteration 6/25 | Loss: 0.00079248
Iteration 7/25 | Loss: 0.00079232
Iteration 8/25 | Loss: 0.00079231
Iteration 9/25 | Loss: 0.00079231
Iteration 10/25 | Loss: 0.00079231
Iteration 11/25 | Loss: 0.00079231
Iteration 12/25 | Loss: 0.00079231
Iteration 13/25 | Loss: 0.00079231
Iteration 14/25 | Loss: 0.00079231
Iteration 15/25 | Loss: 0.00079231
Iteration 16/25 | Loss: 0.00079231
Iteration 17/25 | Loss: 0.00079231
Iteration 18/25 | Loss: 0.00079231
Iteration 19/25 | Loss: 0.00079231
Iteration 20/25 | Loss: 0.00079231
Iteration 21/25 | Loss: 0.00079231
Iteration 22/25 | Loss: 0.00079231
Iteration 23/25 | Loss: 0.00079231
Iteration 24/25 | Loss: 0.00079231
Iteration 25/25 | Loss: 0.00079231

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60453403
Iteration 2/25 | Loss: 0.00144441
Iteration 3/25 | Loss: 0.00144441
Iteration 4/25 | Loss: 0.00144441
Iteration 5/25 | Loss: 0.00144441
Iteration 6/25 | Loss: 0.00144440
Iteration 7/25 | Loss: 0.00144440
Iteration 8/25 | Loss: 0.00144440
Iteration 9/25 | Loss: 0.00144440
Iteration 10/25 | Loss: 0.00144440
Iteration 11/25 | Loss: 0.00144440
Iteration 12/25 | Loss: 0.00144440
Iteration 13/25 | Loss: 0.00144440
Iteration 14/25 | Loss: 0.00144440
Iteration 15/25 | Loss: 0.00144440
Iteration 16/25 | Loss: 0.00144440
Iteration 17/25 | Loss: 0.00144440
Iteration 18/25 | Loss: 0.00144440
Iteration 19/25 | Loss: 0.00144440
Iteration 20/25 | Loss: 0.00144440
Iteration 21/25 | Loss: 0.00144440
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0014444031985476613, 0.0014444031985476613, 0.0014444031985476613, 0.0014444031985476613, 0.0014444031985476613]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014444031985476613

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00144440
Iteration 2/1000 | Loss: 0.00004106
Iteration 3/1000 | Loss: 0.00002602
Iteration 4/1000 | Loss: 0.00002138
Iteration 5/1000 | Loss: 0.00001948
Iteration 6/1000 | Loss: 0.00001843
Iteration 7/1000 | Loss: 0.00001780
Iteration 8/1000 | Loss: 0.00001730
Iteration 9/1000 | Loss: 0.00001692
Iteration 10/1000 | Loss: 0.00001670
Iteration 11/1000 | Loss: 0.00001647
Iteration 12/1000 | Loss: 0.00001627
Iteration 13/1000 | Loss: 0.00001610
Iteration 14/1000 | Loss: 0.00001601
Iteration 15/1000 | Loss: 0.00001599
Iteration 16/1000 | Loss: 0.00001597
Iteration 17/1000 | Loss: 0.00001590
Iteration 18/1000 | Loss: 0.00001587
Iteration 19/1000 | Loss: 0.00001587
Iteration 20/1000 | Loss: 0.00001586
Iteration 21/1000 | Loss: 0.00001586
Iteration 22/1000 | Loss: 0.00001585
Iteration 23/1000 | Loss: 0.00001585
Iteration 24/1000 | Loss: 0.00001585
Iteration 25/1000 | Loss: 0.00001584
Iteration 26/1000 | Loss: 0.00001584
Iteration 27/1000 | Loss: 0.00001584
Iteration 28/1000 | Loss: 0.00001583
Iteration 29/1000 | Loss: 0.00001583
Iteration 30/1000 | Loss: 0.00001583
Iteration 31/1000 | Loss: 0.00001582
Iteration 32/1000 | Loss: 0.00001582
Iteration 33/1000 | Loss: 0.00001581
Iteration 34/1000 | Loss: 0.00001581
Iteration 35/1000 | Loss: 0.00001581
Iteration 36/1000 | Loss: 0.00001580
Iteration 37/1000 | Loss: 0.00001580
Iteration 38/1000 | Loss: 0.00001580
Iteration 39/1000 | Loss: 0.00001579
Iteration 40/1000 | Loss: 0.00001579
Iteration 41/1000 | Loss: 0.00001579
Iteration 42/1000 | Loss: 0.00001578
Iteration 43/1000 | Loss: 0.00001578
Iteration 44/1000 | Loss: 0.00001578
Iteration 45/1000 | Loss: 0.00001577
Iteration 46/1000 | Loss: 0.00001577
Iteration 47/1000 | Loss: 0.00001577
Iteration 48/1000 | Loss: 0.00001576
Iteration 49/1000 | Loss: 0.00001576
Iteration 50/1000 | Loss: 0.00001575
Iteration 51/1000 | Loss: 0.00001575
Iteration 52/1000 | Loss: 0.00001574
Iteration 53/1000 | Loss: 0.00001574
Iteration 54/1000 | Loss: 0.00001574
Iteration 55/1000 | Loss: 0.00001574
Iteration 56/1000 | Loss: 0.00001573
Iteration 57/1000 | Loss: 0.00001573
Iteration 58/1000 | Loss: 0.00001573
Iteration 59/1000 | Loss: 0.00001573
Iteration 60/1000 | Loss: 0.00001573
Iteration 61/1000 | Loss: 0.00001573
Iteration 62/1000 | Loss: 0.00001572
Iteration 63/1000 | Loss: 0.00001572
Iteration 64/1000 | Loss: 0.00001572
Iteration 65/1000 | Loss: 0.00001572
Iteration 66/1000 | Loss: 0.00001571
Iteration 67/1000 | Loss: 0.00001571
Iteration 68/1000 | Loss: 0.00001570
Iteration 69/1000 | Loss: 0.00001570
Iteration 70/1000 | Loss: 0.00001570
Iteration 71/1000 | Loss: 0.00001570
Iteration 72/1000 | Loss: 0.00001569
Iteration 73/1000 | Loss: 0.00001569
Iteration 74/1000 | Loss: 0.00001569
Iteration 75/1000 | Loss: 0.00001568
Iteration 76/1000 | Loss: 0.00001568
Iteration 77/1000 | Loss: 0.00001568
Iteration 78/1000 | Loss: 0.00001568
Iteration 79/1000 | Loss: 0.00001567
Iteration 80/1000 | Loss: 0.00001567
Iteration 81/1000 | Loss: 0.00001567
Iteration 82/1000 | Loss: 0.00001567
Iteration 83/1000 | Loss: 0.00001567
Iteration 84/1000 | Loss: 0.00001567
Iteration 85/1000 | Loss: 0.00001567
Iteration 86/1000 | Loss: 0.00001566
Iteration 87/1000 | Loss: 0.00001566
Iteration 88/1000 | Loss: 0.00001566
Iteration 89/1000 | Loss: 0.00001566
Iteration 90/1000 | Loss: 0.00001566
Iteration 91/1000 | Loss: 0.00001566
Iteration 92/1000 | Loss: 0.00001566
Iteration 93/1000 | Loss: 0.00001565
Iteration 94/1000 | Loss: 0.00001565
Iteration 95/1000 | Loss: 0.00001565
Iteration 96/1000 | Loss: 0.00001565
Iteration 97/1000 | Loss: 0.00001565
Iteration 98/1000 | Loss: 0.00001565
Iteration 99/1000 | Loss: 0.00001565
Iteration 100/1000 | Loss: 0.00001564
Iteration 101/1000 | Loss: 0.00001564
Iteration 102/1000 | Loss: 0.00001564
Iteration 103/1000 | Loss: 0.00001564
Iteration 104/1000 | Loss: 0.00001564
Iteration 105/1000 | Loss: 0.00001564
Iteration 106/1000 | Loss: 0.00001563
Iteration 107/1000 | Loss: 0.00001563
Iteration 108/1000 | Loss: 0.00001563
Iteration 109/1000 | Loss: 0.00001563
Iteration 110/1000 | Loss: 0.00001563
Iteration 111/1000 | Loss: 0.00001563
Iteration 112/1000 | Loss: 0.00001563
Iteration 113/1000 | Loss: 0.00001563
Iteration 114/1000 | Loss: 0.00001563
Iteration 115/1000 | Loss: 0.00001563
Iteration 116/1000 | Loss: 0.00001563
Iteration 117/1000 | Loss: 0.00001563
Iteration 118/1000 | Loss: 0.00001563
Iteration 119/1000 | Loss: 0.00001563
Iteration 120/1000 | Loss: 0.00001563
Iteration 121/1000 | Loss: 0.00001563
Iteration 122/1000 | Loss: 0.00001563
Iteration 123/1000 | Loss: 0.00001563
Iteration 124/1000 | Loss: 0.00001563
Iteration 125/1000 | Loss: 0.00001563
Iteration 126/1000 | Loss: 0.00001563
Iteration 127/1000 | Loss: 0.00001563
Iteration 128/1000 | Loss: 0.00001563
Iteration 129/1000 | Loss: 0.00001563
Iteration 130/1000 | Loss: 0.00001563
Iteration 131/1000 | Loss: 0.00001563
Iteration 132/1000 | Loss: 0.00001563
Iteration 133/1000 | Loss: 0.00001563
Iteration 134/1000 | Loss: 0.00001563
Iteration 135/1000 | Loss: 0.00001563
Iteration 136/1000 | Loss: 0.00001563
Iteration 137/1000 | Loss: 0.00001563
Iteration 138/1000 | Loss: 0.00001563
Iteration 139/1000 | Loss: 0.00001563
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 139. Stopping optimization.
Last 5 losses: [1.5633844668627717e-05, 1.5633844668627717e-05, 1.5633844668627717e-05, 1.5633844668627717e-05, 1.5633844668627717e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5633844668627717e-05

Optimization complete. Final v2v error: 3.3158421516418457 mm

Highest mean error: 3.889829635620117 mm for frame 145

Lowest mean error: 2.67108416557312 mm for frame 160

Saving results

Total time: 44.11470079421997
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1044/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1044.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1044
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00870506
Iteration 2/25 | Loss: 0.00088945
Iteration 3/25 | Loss: 0.00076414
Iteration 4/25 | Loss: 0.00074708
Iteration 5/25 | Loss: 0.00074198
Iteration 6/25 | Loss: 0.00074085
Iteration 7/25 | Loss: 0.00074074
Iteration 8/25 | Loss: 0.00074074
Iteration 9/25 | Loss: 0.00074074
Iteration 10/25 | Loss: 0.00074074
Iteration 11/25 | Loss: 0.00074074
Iteration 12/25 | Loss: 0.00074074
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007407422526739538, 0.0007407422526739538, 0.0007407422526739538, 0.0007407422526739538, 0.0007407422526739538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007407422526739538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.78890455
Iteration 2/25 | Loss: 0.00118517
Iteration 3/25 | Loss: 0.00118517
Iteration 4/25 | Loss: 0.00118517
Iteration 5/25 | Loss: 0.00118517
Iteration 6/25 | Loss: 0.00118517
Iteration 7/25 | Loss: 0.00118517
Iteration 8/25 | Loss: 0.00118517
Iteration 9/25 | Loss: 0.00118517
Iteration 10/25 | Loss: 0.00118517
Iteration 11/25 | Loss: 0.00118516
Iteration 12/25 | Loss: 0.00118516
Iteration 13/25 | Loss: 0.00118516
Iteration 14/25 | Loss: 0.00118516
Iteration 15/25 | Loss: 0.00118516
Iteration 16/25 | Loss: 0.00118516
Iteration 17/25 | Loss: 0.00118516
Iteration 18/25 | Loss: 0.00118516
Iteration 19/25 | Loss: 0.00118516
Iteration 20/25 | Loss: 0.00118516
Iteration 21/25 | Loss: 0.00118516
Iteration 22/25 | Loss: 0.00118516
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0011851645540446043, 0.0011851645540446043, 0.0011851645540446043, 0.0011851645540446043, 0.0011851645540446043]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011851645540446043

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00118516
Iteration 2/1000 | Loss: 0.00002571
Iteration 3/1000 | Loss: 0.00001562
Iteration 4/1000 | Loss: 0.00001442
Iteration 5/1000 | Loss: 0.00001376
Iteration 6/1000 | Loss: 0.00001329
Iteration 7/1000 | Loss: 0.00001322
Iteration 8/1000 | Loss: 0.00001307
Iteration 9/1000 | Loss: 0.00001295
Iteration 10/1000 | Loss: 0.00001278
Iteration 11/1000 | Loss: 0.00001268
Iteration 12/1000 | Loss: 0.00001266
Iteration 13/1000 | Loss: 0.00001263
Iteration 14/1000 | Loss: 0.00001260
Iteration 15/1000 | Loss: 0.00001258
Iteration 16/1000 | Loss: 0.00001258
Iteration 17/1000 | Loss: 0.00001257
Iteration 18/1000 | Loss: 0.00001257
Iteration 19/1000 | Loss: 0.00001255
Iteration 20/1000 | Loss: 0.00001254
Iteration 21/1000 | Loss: 0.00001253
Iteration 22/1000 | Loss: 0.00001253
Iteration 23/1000 | Loss: 0.00001253
Iteration 24/1000 | Loss: 0.00001253
Iteration 25/1000 | Loss: 0.00001252
Iteration 26/1000 | Loss: 0.00001251
Iteration 27/1000 | Loss: 0.00001251
Iteration 28/1000 | Loss: 0.00001251
Iteration 29/1000 | Loss: 0.00001249
Iteration 30/1000 | Loss: 0.00001249
Iteration 31/1000 | Loss: 0.00001249
Iteration 32/1000 | Loss: 0.00001249
Iteration 33/1000 | Loss: 0.00001249
Iteration 34/1000 | Loss: 0.00001249
Iteration 35/1000 | Loss: 0.00001249
Iteration 36/1000 | Loss: 0.00001249
Iteration 37/1000 | Loss: 0.00001249
Iteration 38/1000 | Loss: 0.00001249
Iteration 39/1000 | Loss: 0.00001249
Iteration 40/1000 | Loss: 0.00001248
Iteration 41/1000 | Loss: 0.00001248
Iteration 42/1000 | Loss: 0.00001248
Iteration 43/1000 | Loss: 0.00001248
Iteration 44/1000 | Loss: 0.00001248
Iteration 45/1000 | Loss: 0.00001248
Iteration 46/1000 | Loss: 0.00001248
Iteration 47/1000 | Loss: 0.00001248
Iteration 48/1000 | Loss: 0.00001248
Iteration 49/1000 | Loss: 0.00001248
Iteration 50/1000 | Loss: 0.00001247
Iteration 51/1000 | Loss: 0.00001247
Iteration 52/1000 | Loss: 0.00001246
Iteration 53/1000 | Loss: 0.00001245
Iteration 54/1000 | Loss: 0.00001245
Iteration 55/1000 | Loss: 0.00001244
Iteration 56/1000 | Loss: 0.00001244
Iteration 57/1000 | Loss: 0.00001243
Iteration 58/1000 | Loss: 0.00001243
Iteration 59/1000 | Loss: 0.00001242
Iteration 60/1000 | Loss: 0.00001241
Iteration 61/1000 | Loss: 0.00001241
Iteration 62/1000 | Loss: 0.00001241
Iteration 63/1000 | Loss: 0.00001240
Iteration 64/1000 | Loss: 0.00001240
Iteration 65/1000 | Loss: 0.00001240
Iteration 66/1000 | Loss: 0.00001239
Iteration 67/1000 | Loss: 0.00001238
Iteration 68/1000 | Loss: 0.00001238
Iteration 69/1000 | Loss: 0.00001238
Iteration 70/1000 | Loss: 0.00001237
Iteration 71/1000 | Loss: 0.00001237
Iteration 72/1000 | Loss: 0.00001236
Iteration 73/1000 | Loss: 0.00001236
Iteration 74/1000 | Loss: 0.00001236
Iteration 75/1000 | Loss: 0.00001236
Iteration 76/1000 | Loss: 0.00001236
Iteration 77/1000 | Loss: 0.00001235
Iteration 78/1000 | Loss: 0.00001235
Iteration 79/1000 | Loss: 0.00001235
Iteration 80/1000 | Loss: 0.00001235
Iteration 81/1000 | Loss: 0.00001234
Iteration 82/1000 | Loss: 0.00001234
Iteration 83/1000 | Loss: 0.00001234
Iteration 84/1000 | Loss: 0.00001234
Iteration 85/1000 | Loss: 0.00001234
Iteration 86/1000 | Loss: 0.00001233
Iteration 87/1000 | Loss: 0.00001232
Iteration 88/1000 | Loss: 0.00001232
Iteration 89/1000 | Loss: 0.00001231
Iteration 90/1000 | Loss: 0.00001231
Iteration 91/1000 | Loss: 0.00001231
Iteration 92/1000 | Loss: 0.00001230
Iteration 93/1000 | Loss: 0.00001230
Iteration 94/1000 | Loss: 0.00001230
Iteration 95/1000 | Loss: 0.00001230
Iteration 96/1000 | Loss: 0.00001230
Iteration 97/1000 | Loss: 0.00001230
Iteration 98/1000 | Loss: 0.00001230
Iteration 99/1000 | Loss: 0.00001229
Iteration 100/1000 | Loss: 0.00001229
Iteration 101/1000 | Loss: 0.00001228
Iteration 102/1000 | Loss: 0.00001227
Iteration 103/1000 | Loss: 0.00001227
Iteration 104/1000 | Loss: 0.00001227
Iteration 105/1000 | Loss: 0.00001227
Iteration 106/1000 | Loss: 0.00001227
Iteration 107/1000 | Loss: 0.00001227
Iteration 108/1000 | Loss: 0.00001227
Iteration 109/1000 | Loss: 0.00001227
Iteration 110/1000 | Loss: 0.00001227
Iteration 111/1000 | Loss: 0.00001226
Iteration 112/1000 | Loss: 0.00001226
Iteration 113/1000 | Loss: 0.00001226
Iteration 114/1000 | Loss: 0.00001226
Iteration 115/1000 | Loss: 0.00001226
Iteration 116/1000 | Loss: 0.00001226
Iteration 117/1000 | Loss: 0.00001226
Iteration 118/1000 | Loss: 0.00001225
Iteration 119/1000 | Loss: 0.00001225
Iteration 120/1000 | Loss: 0.00001225
Iteration 121/1000 | Loss: 0.00001225
Iteration 122/1000 | Loss: 0.00001225
Iteration 123/1000 | Loss: 0.00001225
Iteration 124/1000 | Loss: 0.00001225
Iteration 125/1000 | Loss: 0.00001225
Iteration 126/1000 | Loss: 0.00001224
Iteration 127/1000 | Loss: 0.00001224
Iteration 128/1000 | Loss: 0.00001224
Iteration 129/1000 | Loss: 0.00001224
Iteration 130/1000 | Loss: 0.00001224
Iteration 131/1000 | Loss: 0.00001224
Iteration 132/1000 | Loss: 0.00001224
Iteration 133/1000 | Loss: 0.00001223
Iteration 134/1000 | Loss: 0.00001223
Iteration 135/1000 | Loss: 0.00001223
Iteration 136/1000 | Loss: 0.00001223
Iteration 137/1000 | Loss: 0.00001223
Iteration 138/1000 | Loss: 0.00001223
Iteration 139/1000 | Loss: 0.00001223
Iteration 140/1000 | Loss: 0.00001223
Iteration 141/1000 | Loss: 0.00001223
Iteration 142/1000 | Loss: 0.00001223
Iteration 143/1000 | Loss: 0.00001223
Iteration 144/1000 | Loss: 0.00001223
Iteration 145/1000 | Loss: 0.00001223
Iteration 146/1000 | Loss: 0.00001222
Iteration 147/1000 | Loss: 0.00001222
Iteration 148/1000 | Loss: 0.00001222
Iteration 149/1000 | Loss: 0.00001222
Iteration 150/1000 | Loss: 0.00001222
Iteration 151/1000 | Loss: 0.00001222
Iteration 152/1000 | Loss: 0.00001222
Iteration 153/1000 | Loss: 0.00001222
Iteration 154/1000 | Loss: 0.00001222
Iteration 155/1000 | Loss: 0.00001222
Iteration 156/1000 | Loss: 0.00001222
Iteration 157/1000 | Loss: 0.00001222
Iteration 158/1000 | Loss: 0.00001222
Iteration 159/1000 | Loss: 0.00001222
Iteration 160/1000 | Loss: 0.00001222
Iteration 161/1000 | Loss: 0.00001222
Iteration 162/1000 | Loss: 0.00001222
Iteration 163/1000 | Loss: 0.00001222
Iteration 164/1000 | Loss: 0.00001221
Iteration 165/1000 | Loss: 0.00001221
Iteration 166/1000 | Loss: 0.00001221
Iteration 167/1000 | Loss: 0.00001221
Iteration 168/1000 | Loss: 0.00001221
Iteration 169/1000 | Loss: 0.00001221
Iteration 170/1000 | Loss: 0.00001221
Iteration 171/1000 | Loss: 0.00001221
Iteration 172/1000 | Loss: 0.00001221
Iteration 173/1000 | Loss: 0.00001221
Iteration 174/1000 | Loss: 0.00001221
Iteration 175/1000 | Loss: 0.00001221
Iteration 176/1000 | Loss: 0.00001221
Iteration 177/1000 | Loss: 0.00001220
Iteration 178/1000 | Loss: 0.00001220
Iteration 179/1000 | Loss: 0.00001220
Iteration 180/1000 | Loss: 0.00001220
Iteration 181/1000 | Loss: 0.00001220
Iteration 182/1000 | Loss: 0.00001220
Iteration 183/1000 | Loss: 0.00001220
Iteration 184/1000 | Loss: 0.00001220
Iteration 185/1000 | Loss: 0.00001220
Iteration 186/1000 | Loss: 0.00001220
Iteration 187/1000 | Loss: 0.00001220
Iteration 188/1000 | Loss: 0.00001220
Iteration 189/1000 | Loss: 0.00001220
Iteration 190/1000 | Loss: 0.00001220
Iteration 191/1000 | Loss: 0.00001220
Iteration 192/1000 | Loss: 0.00001220
Iteration 193/1000 | Loss: 0.00001220
Iteration 194/1000 | Loss: 0.00001220
Iteration 195/1000 | Loss: 0.00001220
Iteration 196/1000 | Loss: 0.00001220
Iteration 197/1000 | Loss: 0.00001220
Iteration 198/1000 | Loss: 0.00001219
Iteration 199/1000 | Loss: 0.00001219
Iteration 200/1000 | Loss: 0.00001219
Iteration 201/1000 | Loss: 0.00001219
Iteration 202/1000 | Loss: 0.00001219
Iteration 203/1000 | Loss: 0.00001219
Iteration 204/1000 | Loss: 0.00001219
Iteration 205/1000 | Loss: 0.00001219
Iteration 206/1000 | Loss: 0.00001219
Iteration 207/1000 | Loss: 0.00001219
Iteration 208/1000 | Loss: 0.00001219
Iteration 209/1000 | Loss: 0.00001219
Iteration 210/1000 | Loss: 0.00001219
Iteration 211/1000 | Loss: 0.00001219
Iteration 212/1000 | Loss: 0.00001219
Iteration 213/1000 | Loss: 0.00001219
Iteration 214/1000 | Loss: 0.00001219
Iteration 215/1000 | Loss: 0.00001219
Iteration 216/1000 | Loss: 0.00001219
Iteration 217/1000 | Loss: 0.00001218
Iteration 218/1000 | Loss: 0.00001218
Iteration 219/1000 | Loss: 0.00001218
Iteration 220/1000 | Loss: 0.00001218
Iteration 221/1000 | Loss: 0.00001218
Iteration 222/1000 | Loss: 0.00001218
Iteration 223/1000 | Loss: 0.00001218
Iteration 224/1000 | Loss: 0.00001218
Iteration 225/1000 | Loss: 0.00001218
Iteration 226/1000 | Loss: 0.00001218
Iteration 227/1000 | Loss: 0.00001218
Iteration 228/1000 | Loss: 0.00001218
Iteration 229/1000 | Loss: 0.00001218
Iteration 230/1000 | Loss: 0.00001218
Iteration 231/1000 | Loss: 0.00001218
Iteration 232/1000 | Loss: 0.00001218
Iteration 233/1000 | Loss: 0.00001218
Iteration 234/1000 | Loss: 0.00001218
Iteration 235/1000 | Loss: 0.00001218
Iteration 236/1000 | Loss: 0.00001218
Iteration 237/1000 | Loss: 0.00001218
Iteration 238/1000 | Loss: 0.00001218
Iteration 239/1000 | Loss: 0.00001218
Iteration 240/1000 | Loss: 0.00001218
Iteration 241/1000 | Loss: 0.00001218
Iteration 242/1000 | Loss: 0.00001218
Iteration 243/1000 | Loss: 0.00001218
Iteration 244/1000 | Loss: 0.00001218
Iteration 245/1000 | Loss: 0.00001218
Iteration 246/1000 | Loss: 0.00001218
Iteration 247/1000 | Loss: 0.00001218
Iteration 248/1000 | Loss: 0.00001218
Iteration 249/1000 | Loss: 0.00001218
Iteration 250/1000 | Loss: 0.00001218
Iteration 251/1000 | Loss: 0.00001218
Iteration 252/1000 | Loss: 0.00001218
Iteration 253/1000 | Loss: 0.00001218
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 253. Stopping optimization.
Last 5 losses: [1.2182883438072167e-05, 1.2182883438072167e-05, 1.2182883438072167e-05, 1.2182883438072167e-05, 1.2182883438072167e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2182883438072167e-05

Optimization complete. Final v2v error: 2.9686942100524902 mm

Highest mean error: 3.388340950012207 mm for frame 69

Lowest mean error: 2.843470335006714 mm for frame 115

Saving results

Total time: 37.340312004089355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1077/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1077.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1077
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00839179
Iteration 2/25 | Loss: 0.00163409
Iteration 3/25 | Loss: 0.00134706
Iteration 4/25 | Loss: 0.00127493
Iteration 5/25 | Loss: 0.00144232
Iteration 6/25 | Loss: 0.00132776
Iteration 7/25 | Loss: 0.00122126
Iteration 8/25 | Loss: 0.00105645
Iteration 9/25 | Loss: 0.00091836
Iteration 10/25 | Loss: 0.00089729
Iteration 11/25 | Loss: 0.00089475
Iteration 12/25 | Loss: 0.00083899
Iteration 13/25 | Loss: 0.00084020
Iteration 14/25 | Loss: 0.00082174
Iteration 15/25 | Loss: 0.00081908
Iteration 16/25 | Loss: 0.00081337
Iteration 17/25 | Loss: 0.00081748
Iteration 18/25 | Loss: 0.00080223
Iteration 19/25 | Loss: 0.00079350
Iteration 20/25 | Loss: 0.00078838
Iteration 21/25 | Loss: 0.00078595
Iteration 22/25 | Loss: 0.00078729
Iteration 23/25 | Loss: 0.00078724
Iteration 24/25 | Loss: 0.00078572
Iteration 25/25 | Loss: 0.00078519

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58555567
Iteration 2/25 | Loss: 0.00178869
Iteration 3/25 | Loss: 0.00158131
Iteration 4/25 | Loss: 0.00158131
Iteration 5/25 | Loss: 0.00158131
Iteration 6/25 | Loss: 0.00158131
Iteration 7/25 | Loss: 0.00158131
Iteration 8/25 | Loss: 0.00158131
Iteration 9/25 | Loss: 0.00158131
Iteration 10/25 | Loss: 0.00158131
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001581309363245964, 0.001581309363245964, 0.001581309363245964, 0.001581309363245964, 0.001581309363245964]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001581309363245964

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00158131
Iteration 2/1000 | Loss: 0.00037279
Iteration 3/1000 | Loss: 0.00006751
Iteration 4/1000 | Loss: 0.00013886
Iteration 5/1000 | Loss: 0.00009700
Iteration 6/1000 | Loss: 0.00011632
Iteration 7/1000 | Loss: 0.00018319
Iteration 8/1000 | Loss: 0.00010074
Iteration 9/1000 | Loss: 0.00002717
Iteration 10/1000 | Loss: 0.00010821
Iteration 11/1000 | Loss: 0.00009238
Iteration 12/1000 | Loss: 0.00001973
Iteration 13/1000 | Loss: 0.00001859
Iteration 14/1000 | Loss: 0.00001787
Iteration 15/1000 | Loss: 0.00001733
Iteration 16/1000 | Loss: 0.00001690
Iteration 17/1000 | Loss: 0.00001657
Iteration 18/1000 | Loss: 0.00001626
Iteration 19/1000 | Loss: 0.00001621
Iteration 20/1000 | Loss: 0.00001606
Iteration 21/1000 | Loss: 0.00001600
Iteration 22/1000 | Loss: 0.00001599
Iteration 23/1000 | Loss: 0.00001598
Iteration 24/1000 | Loss: 0.00001598
Iteration 25/1000 | Loss: 0.00001598
Iteration 26/1000 | Loss: 0.00001597
Iteration 27/1000 | Loss: 0.00001597
Iteration 28/1000 | Loss: 0.00001597
Iteration 29/1000 | Loss: 0.00001597
Iteration 30/1000 | Loss: 0.00001597
Iteration 31/1000 | Loss: 0.00001597
Iteration 32/1000 | Loss: 0.00001597
Iteration 33/1000 | Loss: 0.00001596
Iteration 34/1000 | Loss: 0.00001596
Iteration 35/1000 | Loss: 0.00001596
Iteration 36/1000 | Loss: 0.00001595
Iteration 37/1000 | Loss: 0.00001595
Iteration 38/1000 | Loss: 0.00001595
Iteration 39/1000 | Loss: 0.00001595
Iteration 40/1000 | Loss: 0.00001595
Iteration 41/1000 | Loss: 0.00001595
Iteration 42/1000 | Loss: 0.00001594
Iteration 43/1000 | Loss: 0.00001594
Iteration 44/1000 | Loss: 0.00001594
Iteration 45/1000 | Loss: 0.00001594
Iteration 46/1000 | Loss: 0.00001594
Iteration 47/1000 | Loss: 0.00001594
Iteration 48/1000 | Loss: 0.00001593
Iteration 49/1000 | Loss: 0.00001593
Iteration 50/1000 | Loss: 0.00001593
Iteration 51/1000 | Loss: 0.00001592
Iteration 52/1000 | Loss: 0.00001592
Iteration 53/1000 | Loss: 0.00001592
Iteration 54/1000 | Loss: 0.00001591
Iteration 55/1000 | Loss: 0.00001591
Iteration 56/1000 | Loss: 0.00001591
Iteration 57/1000 | Loss: 0.00001591
Iteration 58/1000 | Loss: 0.00001590
Iteration 59/1000 | Loss: 0.00001590
Iteration 60/1000 | Loss: 0.00001590
Iteration 61/1000 | Loss: 0.00001590
Iteration 62/1000 | Loss: 0.00001590
Iteration 63/1000 | Loss: 0.00001590
Iteration 64/1000 | Loss: 0.00001589
Iteration 65/1000 | Loss: 0.00001589
Iteration 66/1000 | Loss: 0.00001589
Iteration 67/1000 | Loss: 0.00001589
Iteration 68/1000 | Loss: 0.00001589
Iteration 69/1000 | Loss: 0.00001589
Iteration 70/1000 | Loss: 0.00001589
Iteration 71/1000 | Loss: 0.00001589
Iteration 72/1000 | Loss: 0.00001588
Iteration 73/1000 | Loss: 0.00001588
Iteration 74/1000 | Loss: 0.00001588
Iteration 75/1000 | Loss: 0.00001588
Iteration 76/1000 | Loss: 0.00001588
Iteration 77/1000 | Loss: 0.00001588
Iteration 78/1000 | Loss: 0.00001588
Iteration 79/1000 | Loss: 0.00001588
Iteration 80/1000 | Loss: 0.00001588
Iteration 81/1000 | Loss: 0.00001587
Iteration 82/1000 | Loss: 0.00001587
Iteration 83/1000 | Loss: 0.00001587
Iteration 84/1000 | Loss: 0.00001587
Iteration 85/1000 | Loss: 0.00001587
Iteration 86/1000 | Loss: 0.00001587
Iteration 87/1000 | Loss: 0.00001587
Iteration 88/1000 | Loss: 0.00001586
Iteration 89/1000 | Loss: 0.00001586
Iteration 90/1000 | Loss: 0.00001586
Iteration 91/1000 | Loss: 0.00001586
Iteration 92/1000 | Loss: 0.00001586
Iteration 93/1000 | Loss: 0.00001586
Iteration 94/1000 | Loss: 0.00001586
Iteration 95/1000 | Loss: 0.00001585
Iteration 96/1000 | Loss: 0.00001585
Iteration 97/1000 | Loss: 0.00001585
Iteration 98/1000 | Loss: 0.00001585
Iteration 99/1000 | Loss: 0.00001584
Iteration 100/1000 | Loss: 0.00001584
Iteration 101/1000 | Loss: 0.00001584
Iteration 102/1000 | Loss: 0.00001584
Iteration 103/1000 | Loss: 0.00001583
Iteration 104/1000 | Loss: 0.00001583
Iteration 105/1000 | Loss: 0.00001583
Iteration 106/1000 | Loss: 0.00001583
Iteration 107/1000 | Loss: 0.00001582
Iteration 108/1000 | Loss: 0.00001582
Iteration 109/1000 | Loss: 0.00001582
Iteration 110/1000 | Loss: 0.00001581
Iteration 111/1000 | Loss: 0.00001581
Iteration 112/1000 | Loss: 0.00001581
Iteration 113/1000 | Loss: 0.00001581
Iteration 114/1000 | Loss: 0.00001580
Iteration 115/1000 | Loss: 0.00001580
Iteration 116/1000 | Loss: 0.00001580
Iteration 117/1000 | Loss: 0.00001579
Iteration 118/1000 | Loss: 0.00001579
Iteration 119/1000 | Loss: 0.00001579
Iteration 120/1000 | Loss: 0.00001578
Iteration 121/1000 | Loss: 0.00001578
Iteration 122/1000 | Loss: 0.00001578
Iteration 123/1000 | Loss: 0.00001578
Iteration 124/1000 | Loss: 0.00001578
Iteration 125/1000 | Loss: 0.00001578
Iteration 126/1000 | Loss: 0.00001578
Iteration 127/1000 | Loss: 0.00001578
Iteration 128/1000 | Loss: 0.00001578
Iteration 129/1000 | Loss: 0.00001578
Iteration 130/1000 | Loss: 0.00001577
Iteration 131/1000 | Loss: 0.00001577
Iteration 132/1000 | Loss: 0.00001577
Iteration 133/1000 | Loss: 0.00001577
Iteration 134/1000 | Loss: 0.00001577
Iteration 135/1000 | Loss: 0.00001576
Iteration 136/1000 | Loss: 0.00001576
Iteration 137/1000 | Loss: 0.00001576
Iteration 138/1000 | Loss: 0.00001576
Iteration 139/1000 | Loss: 0.00001576
Iteration 140/1000 | Loss: 0.00001576
Iteration 141/1000 | Loss: 0.00001576
Iteration 142/1000 | Loss: 0.00001576
Iteration 143/1000 | Loss: 0.00001576
Iteration 144/1000 | Loss: 0.00001576
Iteration 145/1000 | Loss: 0.00001575
Iteration 146/1000 | Loss: 0.00001575
Iteration 147/1000 | Loss: 0.00001575
Iteration 148/1000 | Loss: 0.00001575
Iteration 149/1000 | Loss: 0.00001575
Iteration 150/1000 | Loss: 0.00001575
Iteration 151/1000 | Loss: 0.00001575
Iteration 152/1000 | Loss: 0.00001575
Iteration 153/1000 | Loss: 0.00001575
Iteration 154/1000 | Loss: 0.00001575
Iteration 155/1000 | Loss: 0.00001575
Iteration 156/1000 | Loss: 0.00001575
Iteration 157/1000 | Loss: 0.00001575
Iteration 158/1000 | Loss: 0.00001574
Iteration 159/1000 | Loss: 0.00001574
Iteration 160/1000 | Loss: 0.00001574
Iteration 161/1000 | Loss: 0.00001574
Iteration 162/1000 | Loss: 0.00001573
Iteration 163/1000 | Loss: 0.00001573
Iteration 164/1000 | Loss: 0.00001573
Iteration 165/1000 | Loss: 0.00001573
Iteration 166/1000 | Loss: 0.00001573
Iteration 167/1000 | Loss: 0.00001573
Iteration 168/1000 | Loss: 0.00001573
Iteration 169/1000 | Loss: 0.00001573
Iteration 170/1000 | Loss: 0.00001573
Iteration 171/1000 | Loss: 0.00001572
Iteration 172/1000 | Loss: 0.00001572
Iteration 173/1000 | Loss: 0.00001572
Iteration 174/1000 | Loss: 0.00001572
Iteration 175/1000 | Loss: 0.00001572
Iteration 176/1000 | Loss: 0.00001572
Iteration 177/1000 | Loss: 0.00001572
Iteration 178/1000 | Loss: 0.00001572
Iteration 179/1000 | Loss: 0.00001572
Iteration 180/1000 | Loss: 0.00001571
Iteration 181/1000 | Loss: 0.00001571
Iteration 182/1000 | Loss: 0.00001571
Iteration 183/1000 | Loss: 0.00001571
Iteration 184/1000 | Loss: 0.00001571
Iteration 185/1000 | Loss: 0.00001571
Iteration 186/1000 | Loss: 0.00001571
Iteration 187/1000 | Loss: 0.00001571
Iteration 188/1000 | Loss: 0.00001571
Iteration 189/1000 | Loss: 0.00001571
Iteration 190/1000 | Loss: 0.00001571
Iteration 191/1000 | Loss: 0.00001571
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 191. Stopping optimization.
Last 5 losses: [1.5712719687144272e-05, 1.5712719687144272e-05, 1.5712719687144272e-05, 1.5712719687144272e-05, 1.5712719687144272e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5712719687144272e-05

Optimization complete. Final v2v error: 3.300096273422241 mm

Highest mean error: 4.3158721923828125 mm for frame 152

Lowest mean error: 2.6927196979522705 mm for frame 1

Saving results

Total time: 98.60976076126099
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1047/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1047.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1047
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01087506
Iteration 2/25 | Loss: 0.01087505
Iteration 3/25 | Loss: 0.01087505
Iteration 4/25 | Loss: 0.01087505
Iteration 5/25 | Loss: 0.01087505
Iteration 6/25 | Loss: 0.01087505
Iteration 7/25 | Loss: 0.01087505
Iteration 8/25 | Loss: 0.01087504
Iteration 9/25 | Loss: 0.01087504
Iteration 10/25 | Loss: 0.01087504
Iteration 11/25 | Loss: 0.01087504
Iteration 12/25 | Loss: 0.01087504
Iteration 13/25 | Loss: 0.01087503
Iteration 14/25 | Loss: 0.01087503
Iteration 15/25 | Loss: 0.01087503
Iteration 16/25 | Loss: 0.01087503
Iteration 17/25 | Loss: 0.01087503
Iteration 18/25 | Loss: 0.01087502
Iteration 19/25 | Loss: 0.01087502
Iteration 20/25 | Loss: 0.01087502
Iteration 21/25 | Loss: 0.01087502
Iteration 22/25 | Loss: 0.01087502
Iteration 23/25 | Loss: 0.01087501
Iteration 24/25 | Loss: 0.01087501
Iteration 25/25 | Loss: 0.01087501

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.15029144
Iteration 2/25 | Loss: 0.08744287
Iteration 3/25 | Loss: 0.08323646
Iteration 4/25 | Loss: 0.08247549
Iteration 5/25 | Loss: 0.08247548
Iteration 6/25 | Loss: 0.08247547
Iteration 7/25 | Loss: 0.08247547
Iteration 8/25 | Loss: 0.08247547
Iteration 9/25 | Loss: 0.08247547
Iteration 10/25 | Loss: 0.08247547
Iteration 11/25 | Loss: 0.08247547
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.08247546851634979, 0.08247546851634979, 0.08247546851634979, 0.08247546851634979, 0.08247546851634979]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08247546851634979

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08247547
Iteration 2/1000 | Loss: 0.00391647
Iteration 3/1000 | Loss: 0.00238496
Iteration 4/1000 | Loss: 0.00057474
Iteration 5/1000 | Loss: 0.00106658
Iteration 6/1000 | Loss: 0.00096230
Iteration 7/1000 | Loss: 0.00030666
Iteration 8/1000 | Loss: 0.00021297
Iteration 9/1000 | Loss: 0.00015129
Iteration 10/1000 | Loss: 0.00030604
Iteration 11/1000 | Loss: 0.00005087
Iteration 12/1000 | Loss: 0.00008433
Iteration 13/1000 | Loss: 0.00011637
Iteration 14/1000 | Loss: 0.00003640
Iteration 15/1000 | Loss: 0.00008703
Iteration 16/1000 | Loss: 0.00003240
Iteration 17/1000 | Loss: 0.00003088
Iteration 18/1000 | Loss: 0.00010650
Iteration 19/1000 | Loss: 0.00002857
Iteration 20/1000 | Loss: 0.00002764
Iteration 21/1000 | Loss: 0.00023733
Iteration 22/1000 | Loss: 0.00003792
Iteration 23/1000 | Loss: 0.00002750
Iteration 24/1000 | Loss: 0.00005021
Iteration 25/1000 | Loss: 0.00018505
Iteration 26/1000 | Loss: 0.00023141
Iteration 27/1000 | Loss: 0.00131118
Iteration 28/1000 | Loss: 0.00058042
Iteration 29/1000 | Loss: 0.00134984
Iteration 30/1000 | Loss: 0.00010854
Iteration 31/1000 | Loss: 0.00002514
Iteration 32/1000 | Loss: 0.00002303
Iteration 33/1000 | Loss: 0.00008158
Iteration 34/1000 | Loss: 0.00002186
Iteration 35/1000 | Loss: 0.00006359
Iteration 36/1000 | Loss: 0.00002073
Iteration 37/1000 | Loss: 0.00002010
Iteration 38/1000 | Loss: 0.00006499
Iteration 39/1000 | Loss: 0.00014683
Iteration 40/1000 | Loss: 0.00001928
Iteration 41/1000 | Loss: 0.00001873
Iteration 42/1000 | Loss: 0.00001850
Iteration 43/1000 | Loss: 0.00001819
Iteration 44/1000 | Loss: 0.00001806
Iteration 45/1000 | Loss: 0.00001806
Iteration 46/1000 | Loss: 0.00014629
Iteration 47/1000 | Loss: 0.00001856
Iteration 48/1000 | Loss: 0.00004911
Iteration 49/1000 | Loss: 0.00024647
Iteration 50/1000 | Loss: 0.00004781
Iteration 51/1000 | Loss: 0.00015924
Iteration 52/1000 | Loss: 0.00001863
Iteration 53/1000 | Loss: 0.00005044
Iteration 54/1000 | Loss: 0.00001796
Iteration 55/1000 | Loss: 0.00001785
Iteration 56/1000 | Loss: 0.00001785
Iteration 57/1000 | Loss: 0.00001785
Iteration 58/1000 | Loss: 0.00001785
Iteration 59/1000 | Loss: 0.00001785
Iteration 60/1000 | Loss: 0.00001785
Iteration 61/1000 | Loss: 0.00001785
Iteration 62/1000 | Loss: 0.00001785
Iteration 63/1000 | Loss: 0.00001785
Iteration 64/1000 | Loss: 0.00001785
Iteration 65/1000 | Loss: 0.00005458
Iteration 66/1000 | Loss: 0.00004710
Iteration 67/1000 | Loss: 0.00006006
Iteration 68/1000 | Loss: 0.00002800
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001785
Iteration 71/1000 | Loss: 0.00004522
Iteration 72/1000 | Loss: 0.00004522
Iteration 73/1000 | Loss: 0.00001829
Iteration 74/1000 | Loss: 0.00001788
Iteration 75/1000 | Loss: 0.00001778
Iteration 76/1000 | Loss: 0.00001778
Iteration 77/1000 | Loss: 0.00001777
Iteration 78/1000 | Loss: 0.00001777
Iteration 79/1000 | Loss: 0.00001777
Iteration 80/1000 | Loss: 0.00001777
Iteration 81/1000 | Loss: 0.00001777
Iteration 82/1000 | Loss: 0.00001777
Iteration 83/1000 | Loss: 0.00001777
Iteration 84/1000 | Loss: 0.00001777
Iteration 85/1000 | Loss: 0.00001777
Iteration 86/1000 | Loss: 0.00001777
Iteration 87/1000 | Loss: 0.00001777
Iteration 88/1000 | Loss: 0.00001777
Iteration 89/1000 | Loss: 0.00001776
Iteration 90/1000 | Loss: 0.00001776
Iteration 91/1000 | Loss: 0.00001776
Iteration 92/1000 | Loss: 0.00001776
Iteration 93/1000 | Loss: 0.00001776
Iteration 94/1000 | Loss: 0.00001776
Iteration 95/1000 | Loss: 0.00001776
Iteration 96/1000 | Loss: 0.00001776
Iteration 97/1000 | Loss: 0.00001775
Iteration 98/1000 | Loss: 0.00001775
Iteration 99/1000 | Loss: 0.00001775
Iteration 100/1000 | Loss: 0.00001775
Iteration 101/1000 | Loss: 0.00001775
Iteration 102/1000 | Loss: 0.00001775
Iteration 103/1000 | Loss: 0.00001775
Iteration 104/1000 | Loss: 0.00001775
Iteration 105/1000 | Loss: 0.00001775
Iteration 106/1000 | Loss: 0.00001775
Iteration 107/1000 | Loss: 0.00001775
Iteration 108/1000 | Loss: 0.00001775
Iteration 109/1000 | Loss: 0.00001774
Iteration 110/1000 | Loss: 0.00001774
Iteration 111/1000 | Loss: 0.00001774
Iteration 112/1000 | Loss: 0.00001774
Iteration 113/1000 | Loss: 0.00001773
Iteration 114/1000 | Loss: 0.00001773
Iteration 115/1000 | Loss: 0.00001773
Iteration 116/1000 | Loss: 0.00001773
Iteration 117/1000 | Loss: 0.00001773
Iteration 118/1000 | Loss: 0.00001773
Iteration 119/1000 | Loss: 0.00001773
Iteration 120/1000 | Loss: 0.00001773
Iteration 121/1000 | Loss: 0.00001772
Iteration 122/1000 | Loss: 0.00001772
Iteration 123/1000 | Loss: 0.00001772
Iteration 124/1000 | Loss: 0.00001772
Iteration 125/1000 | Loss: 0.00001772
Iteration 126/1000 | Loss: 0.00001772
Iteration 127/1000 | Loss: 0.00001772
Iteration 128/1000 | Loss: 0.00001772
Iteration 129/1000 | Loss: 0.00001772
Iteration 130/1000 | Loss: 0.00001772
Iteration 131/1000 | Loss: 0.00001772
Iteration 132/1000 | Loss: 0.00001771
Iteration 133/1000 | Loss: 0.00001771
Iteration 134/1000 | Loss: 0.00004198
Iteration 135/1000 | Loss: 0.00012435
Iteration 136/1000 | Loss: 0.00002230
Iteration 137/1000 | Loss: 0.00001785
Iteration 138/1000 | Loss: 0.00003127
Iteration 139/1000 | Loss: 0.00004779
Iteration 140/1000 | Loss: 0.00001812
Iteration 141/1000 | Loss: 0.00001791
Iteration 142/1000 | Loss: 0.00001791
Iteration 143/1000 | Loss: 0.00001791
Iteration 144/1000 | Loss: 0.00001790
Iteration 145/1000 | Loss: 0.00001790
Iteration 146/1000 | Loss: 0.00001790
Iteration 147/1000 | Loss: 0.00001790
Iteration 148/1000 | Loss: 0.00001790
Iteration 149/1000 | Loss: 0.00001790
Iteration 150/1000 | Loss: 0.00001787
Iteration 151/1000 | Loss: 0.00001781
Iteration 152/1000 | Loss: 0.00001780
Iteration 153/1000 | Loss: 0.00005657
Iteration 154/1000 | Loss: 0.00002232
Iteration 155/1000 | Loss: 0.00002224
Iteration 156/1000 | Loss: 0.00002166
Iteration 157/1000 | Loss: 0.00001939
Iteration 158/1000 | Loss: 0.00001777
Iteration 159/1000 | Loss: 0.00001777
Iteration 160/1000 | Loss: 0.00001777
Iteration 161/1000 | Loss: 0.00001777
Iteration 162/1000 | Loss: 0.00002571
Iteration 163/1000 | Loss: 0.00002307
Iteration 164/1000 | Loss: 0.00002971
Iteration 165/1000 | Loss: 0.00001823
Iteration 166/1000 | Loss: 0.00002122
Iteration 167/1000 | Loss: 0.00005418
Iteration 168/1000 | Loss: 0.00001802
Iteration 169/1000 | Loss: 0.00001780
Iteration 170/1000 | Loss: 0.00001778
Iteration 171/1000 | Loss: 0.00001778
Iteration 172/1000 | Loss: 0.00001777
Iteration 173/1000 | Loss: 0.00001909
Iteration 174/1000 | Loss: 0.00001776
Iteration 175/1000 | Loss: 0.00001775
Iteration 176/1000 | Loss: 0.00001775
Iteration 177/1000 | Loss: 0.00001775
Iteration 178/1000 | Loss: 0.00001775
Iteration 179/1000 | Loss: 0.00001775
Iteration 180/1000 | Loss: 0.00001775
Iteration 181/1000 | Loss: 0.00001775
Iteration 182/1000 | Loss: 0.00001773
Iteration 183/1000 | Loss: 0.00001773
Iteration 184/1000 | Loss: 0.00001773
Iteration 185/1000 | Loss: 0.00001773
Iteration 186/1000 | Loss: 0.00001773
Iteration 187/1000 | Loss: 0.00001773
Iteration 188/1000 | Loss: 0.00001773
Iteration 189/1000 | Loss: 0.00001772
Iteration 190/1000 | Loss: 0.00001772
Iteration 191/1000 | Loss: 0.00001771
Iteration 192/1000 | Loss: 0.00001770
Iteration 193/1000 | Loss: 0.00001769
Iteration 194/1000 | Loss: 0.00001769
Iteration 195/1000 | Loss: 0.00001769
Iteration 196/1000 | Loss: 0.00001768
Iteration 197/1000 | Loss: 0.00001768
Iteration 198/1000 | Loss: 0.00001768
Iteration 199/1000 | Loss: 0.00001768
Iteration 200/1000 | Loss: 0.00001768
Iteration 201/1000 | Loss: 0.00001767
Iteration 202/1000 | Loss: 0.00001767
Iteration 203/1000 | Loss: 0.00001766
Iteration 204/1000 | Loss: 0.00001765
Iteration 205/1000 | Loss: 0.00001764
Iteration 206/1000 | Loss: 0.00001764
Iteration 207/1000 | Loss: 0.00001764
Iteration 208/1000 | Loss: 0.00001764
Iteration 209/1000 | Loss: 0.00001763
Iteration 210/1000 | Loss: 0.00001763
Iteration 211/1000 | Loss: 0.00001763
Iteration 212/1000 | Loss: 0.00001762
Iteration 213/1000 | Loss: 0.00001762
Iteration 214/1000 | Loss: 0.00001762
Iteration 215/1000 | Loss: 0.00001761
Iteration 216/1000 | Loss: 0.00001761
Iteration 217/1000 | Loss: 0.00001761
Iteration 218/1000 | Loss: 0.00001761
Iteration 219/1000 | Loss: 0.00001761
Iteration 220/1000 | Loss: 0.00001761
Iteration 221/1000 | Loss: 0.00001761
Iteration 222/1000 | Loss: 0.00001761
Iteration 223/1000 | Loss: 0.00001761
Iteration 224/1000 | Loss: 0.00001761
Iteration 225/1000 | Loss: 0.00001761
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 225. Stopping optimization.
Last 5 losses: [1.76072062458843e-05, 1.76072062458843e-05, 1.76072062458843e-05, 1.76072062458843e-05, 1.76072062458843e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.76072062458843e-05

Optimization complete. Final v2v error: 3.5354254245758057 mm

Highest mean error: 4.335395336151123 mm for frame 212

Lowest mean error: 3.152407646179199 mm for frame 142

Saving results

Total time: 144.3489818572998
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1059/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1059.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1059
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00406888
Iteration 2/25 | Loss: 0.00106275
Iteration 3/25 | Loss: 0.00077839
Iteration 4/25 | Loss: 0.00074499
Iteration 5/25 | Loss: 0.00073579
Iteration 6/25 | Loss: 0.00073414
Iteration 7/25 | Loss: 0.00073365
Iteration 8/25 | Loss: 0.00073364
Iteration 9/25 | Loss: 0.00073364
Iteration 10/25 | Loss: 0.00073364
Iteration 11/25 | Loss: 0.00073364
Iteration 12/25 | Loss: 0.00073364
Iteration 13/25 | Loss: 0.00073364
Iteration 14/25 | Loss: 0.00073364
Iteration 15/25 | Loss: 0.00073364
Iteration 16/25 | Loss: 0.00073364
Iteration 17/25 | Loss: 0.00073364
Iteration 18/25 | Loss: 0.00073364
Iteration 19/25 | Loss: 0.00073364
Iteration 20/25 | Loss: 0.00073364
Iteration 21/25 | Loss: 0.00073364
Iteration 22/25 | Loss: 0.00073364
Iteration 23/25 | Loss: 0.00073364
Iteration 24/25 | Loss: 0.00073364
Iteration 25/25 | Loss: 0.00073364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58381045
Iteration 2/25 | Loss: 0.00120638
Iteration 3/25 | Loss: 0.00120638
Iteration 4/25 | Loss: 0.00120638
Iteration 5/25 | Loss: 0.00120638
Iteration 6/25 | Loss: 0.00120638
Iteration 7/25 | Loss: 0.00120638
Iteration 8/25 | Loss: 0.00120638
Iteration 9/25 | Loss: 0.00120638
Iteration 10/25 | Loss: 0.00120638
Iteration 11/25 | Loss: 0.00120638
Iteration 12/25 | Loss: 0.00120638
Iteration 13/25 | Loss: 0.00120638
Iteration 14/25 | Loss: 0.00120638
Iteration 15/25 | Loss: 0.00120638
Iteration 16/25 | Loss: 0.00120638
Iteration 17/25 | Loss: 0.00120638
Iteration 18/25 | Loss: 0.00120638
Iteration 19/25 | Loss: 0.00120638
Iteration 20/25 | Loss: 0.00120638
Iteration 21/25 | Loss: 0.00120638
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.0012063774047419429, 0.0012063774047419429, 0.0012063774047419429, 0.0012063774047419429, 0.0012063774047419429]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012063774047419429

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120638
Iteration 2/1000 | Loss: 0.00002276
Iteration 3/1000 | Loss: 0.00001585
Iteration 4/1000 | Loss: 0.00001410
Iteration 5/1000 | Loss: 0.00001306
Iteration 6/1000 | Loss: 0.00001248
Iteration 7/1000 | Loss: 0.00001200
Iteration 8/1000 | Loss: 0.00001189
Iteration 9/1000 | Loss: 0.00001180
Iteration 10/1000 | Loss: 0.00001179
Iteration 11/1000 | Loss: 0.00001177
Iteration 12/1000 | Loss: 0.00001175
Iteration 13/1000 | Loss: 0.00001174
Iteration 14/1000 | Loss: 0.00001174
Iteration 15/1000 | Loss: 0.00001174
Iteration 16/1000 | Loss: 0.00001173
Iteration 17/1000 | Loss: 0.00001169
Iteration 18/1000 | Loss: 0.00001169
Iteration 19/1000 | Loss: 0.00001168
Iteration 20/1000 | Loss: 0.00001165
Iteration 21/1000 | Loss: 0.00001164
Iteration 22/1000 | Loss: 0.00001164
Iteration 23/1000 | Loss: 0.00001163
Iteration 24/1000 | Loss: 0.00001163
Iteration 25/1000 | Loss: 0.00001162
Iteration 26/1000 | Loss: 0.00001158
Iteration 27/1000 | Loss: 0.00001158
Iteration 28/1000 | Loss: 0.00001157
Iteration 29/1000 | Loss: 0.00001156
Iteration 30/1000 | Loss: 0.00001152
Iteration 31/1000 | Loss: 0.00001151
Iteration 32/1000 | Loss: 0.00001147
Iteration 33/1000 | Loss: 0.00001147
Iteration 34/1000 | Loss: 0.00001146
Iteration 35/1000 | Loss: 0.00001146
Iteration 36/1000 | Loss: 0.00001146
Iteration 37/1000 | Loss: 0.00001145
Iteration 38/1000 | Loss: 0.00001144
Iteration 39/1000 | Loss: 0.00001142
Iteration 40/1000 | Loss: 0.00001142
Iteration 41/1000 | Loss: 0.00001142
Iteration 42/1000 | Loss: 0.00001142
Iteration 43/1000 | Loss: 0.00001142
Iteration 44/1000 | Loss: 0.00001142
Iteration 45/1000 | Loss: 0.00001142
Iteration 46/1000 | Loss: 0.00001141
Iteration 47/1000 | Loss: 0.00001141
Iteration 48/1000 | Loss: 0.00001141
Iteration 49/1000 | Loss: 0.00001141
Iteration 50/1000 | Loss: 0.00001140
Iteration 51/1000 | Loss: 0.00001140
Iteration 52/1000 | Loss: 0.00001140
Iteration 53/1000 | Loss: 0.00001139
Iteration 54/1000 | Loss: 0.00001139
Iteration 55/1000 | Loss: 0.00001138
Iteration 56/1000 | Loss: 0.00001138
Iteration 57/1000 | Loss: 0.00001138
Iteration 58/1000 | Loss: 0.00001138
Iteration 59/1000 | Loss: 0.00001138
Iteration 60/1000 | Loss: 0.00001137
Iteration 61/1000 | Loss: 0.00001137
Iteration 62/1000 | Loss: 0.00001137
Iteration 63/1000 | Loss: 0.00001137
Iteration 64/1000 | Loss: 0.00001137
Iteration 65/1000 | Loss: 0.00001137
Iteration 66/1000 | Loss: 0.00001137
Iteration 67/1000 | Loss: 0.00001136
Iteration 68/1000 | Loss: 0.00001136
Iteration 69/1000 | Loss: 0.00001136
Iteration 70/1000 | Loss: 0.00001136
Iteration 71/1000 | Loss: 0.00001136
Iteration 72/1000 | Loss: 0.00001136
Iteration 73/1000 | Loss: 0.00001136
Iteration 74/1000 | Loss: 0.00001136
Iteration 75/1000 | Loss: 0.00001136
Iteration 76/1000 | Loss: 0.00001136
Iteration 77/1000 | Loss: 0.00001135
Iteration 78/1000 | Loss: 0.00001135
Iteration 79/1000 | Loss: 0.00001135
Iteration 80/1000 | Loss: 0.00001135
Iteration 81/1000 | Loss: 0.00001134
Iteration 82/1000 | Loss: 0.00001134
Iteration 83/1000 | Loss: 0.00001134
Iteration 84/1000 | Loss: 0.00001134
Iteration 85/1000 | Loss: 0.00001134
Iteration 86/1000 | Loss: 0.00001134
Iteration 87/1000 | Loss: 0.00001133
Iteration 88/1000 | Loss: 0.00001133
Iteration 89/1000 | Loss: 0.00001133
Iteration 90/1000 | Loss: 0.00001133
Iteration 91/1000 | Loss: 0.00001133
Iteration 92/1000 | Loss: 0.00001133
Iteration 93/1000 | Loss: 0.00001133
Iteration 94/1000 | Loss: 0.00001133
Iteration 95/1000 | Loss: 0.00001133
Iteration 96/1000 | Loss: 0.00001133
Iteration 97/1000 | Loss: 0.00001133
Iteration 98/1000 | Loss: 0.00001133
Iteration 99/1000 | Loss: 0.00001133
Iteration 100/1000 | Loss: 0.00001133
Iteration 101/1000 | Loss: 0.00001133
Iteration 102/1000 | Loss: 0.00001132
Iteration 103/1000 | Loss: 0.00001132
Iteration 104/1000 | Loss: 0.00001132
Iteration 105/1000 | Loss: 0.00001132
Iteration 106/1000 | Loss: 0.00001132
Iteration 107/1000 | Loss: 0.00001132
Iteration 108/1000 | Loss: 0.00001132
Iteration 109/1000 | Loss: 0.00001131
Iteration 110/1000 | Loss: 0.00001131
Iteration 111/1000 | Loss: 0.00001131
Iteration 112/1000 | Loss: 0.00001131
Iteration 113/1000 | Loss: 0.00001131
Iteration 114/1000 | Loss: 0.00001131
Iteration 115/1000 | Loss: 0.00001131
Iteration 116/1000 | Loss: 0.00001131
Iteration 117/1000 | Loss: 0.00001131
Iteration 118/1000 | Loss: 0.00001131
Iteration 119/1000 | Loss: 0.00001131
Iteration 120/1000 | Loss: 0.00001130
Iteration 121/1000 | Loss: 0.00001130
Iteration 122/1000 | Loss: 0.00001130
Iteration 123/1000 | Loss: 0.00001130
Iteration 124/1000 | Loss: 0.00001130
Iteration 125/1000 | Loss: 0.00001130
Iteration 126/1000 | Loss: 0.00001130
Iteration 127/1000 | Loss: 0.00001130
Iteration 128/1000 | Loss: 0.00001130
Iteration 129/1000 | Loss: 0.00001130
Iteration 130/1000 | Loss: 0.00001129
Iteration 131/1000 | Loss: 0.00001129
Iteration 132/1000 | Loss: 0.00001129
Iteration 133/1000 | Loss: 0.00001129
Iteration 134/1000 | Loss: 0.00001129
Iteration 135/1000 | Loss: 0.00001129
Iteration 136/1000 | Loss: 0.00001129
Iteration 137/1000 | Loss: 0.00001129
Iteration 138/1000 | Loss: 0.00001129
Iteration 139/1000 | Loss: 0.00001129
Iteration 140/1000 | Loss: 0.00001129
Iteration 141/1000 | Loss: 0.00001129
Iteration 142/1000 | Loss: 0.00001129
Iteration 143/1000 | Loss: 0.00001129
Iteration 144/1000 | Loss: 0.00001129
Iteration 145/1000 | Loss: 0.00001129
Iteration 146/1000 | Loss: 0.00001129
Iteration 147/1000 | Loss: 0.00001128
Iteration 148/1000 | Loss: 0.00001128
Iteration 149/1000 | Loss: 0.00001128
Iteration 150/1000 | Loss: 0.00001128
Iteration 151/1000 | Loss: 0.00001128
Iteration 152/1000 | Loss: 0.00001128
Iteration 153/1000 | Loss: 0.00001128
Iteration 154/1000 | Loss: 0.00001128
Iteration 155/1000 | Loss: 0.00001128
Iteration 156/1000 | Loss: 0.00001128
Iteration 157/1000 | Loss: 0.00001128
Iteration 158/1000 | Loss: 0.00001128
Iteration 159/1000 | Loss: 0.00001128
Iteration 160/1000 | Loss: 0.00001128
Iteration 161/1000 | Loss: 0.00001128
Iteration 162/1000 | Loss: 0.00001128
Iteration 163/1000 | Loss: 0.00001128
Iteration 164/1000 | Loss: 0.00001128
Iteration 165/1000 | Loss: 0.00001128
Iteration 166/1000 | Loss: 0.00001128
Iteration 167/1000 | Loss: 0.00001128
Iteration 168/1000 | Loss: 0.00001128
Iteration 169/1000 | Loss: 0.00001128
Iteration 170/1000 | Loss: 0.00001128
Iteration 171/1000 | Loss: 0.00001128
Iteration 172/1000 | Loss: 0.00001128
Iteration 173/1000 | Loss: 0.00001128
Iteration 174/1000 | Loss: 0.00001128
Iteration 175/1000 | Loss: 0.00001128
Iteration 176/1000 | Loss: 0.00001128
Iteration 177/1000 | Loss: 0.00001128
Iteration 178/1000 | Loss: 0.00001128
Iteration 179/1000 | Loss: 0.00001128
Iteration 180/1000 | Loss: 0.00001128
Iteration 181/1000 | Loss: 0.00001128
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 181. Stopping optimization.
Last 5 losses: [1.1275300494162366e-05, 1.1275300494162366e-05, 1.1275300494162366e-05, 1.1275300494162366e-05, 1.1275300494162366e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1275300494162366e-05

Optimization complete. Final v2v error: 2.8853302001953125 mm

Highest mean error: 2.959249973297119 mm for frame 19

Lowest mean error: 2.799642562866211 mm for frame 97

Saving results

Total time: 35.11231875419617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00360763
Iteration 2/25 | Loss: 0.00079421
Iteration 3/25 | Loss: 0.00071119
Iteration 4/25 | Loss: 0.00069695
Iteration 5/25 | Loss: 0.00069323
Iteration 6/25 | Loss: 0.00069184
Iteration 7/25 | Loss: 0.00069163
Iteration 8/25 | Loss: 0.00069163
Iteration 9/25 | Loss: 0.00069162
Iteration 10/25 | Loss: 0.00069162
Iteration 11/25 | Loss: 0.00069162
Iteration 12/25 | Loss: 0.00069162
Iteration 13/25 | Loss: 0.00069162
Iteration 14/25 | Loss: 0.00069162
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0006916159763932228, 0.0006916159763932228, 0.0006916159763932228, 0.0006916159763932228, 0.0006916159763932228]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006916159763932228

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.66586518
Iteration 2/25 | Loss: 0.00121318
Iteration 3/25 | Loss: 0.00121318
Iteration 4/25 | Loss: 0.00121318
Iteration 5/25 | Loss: 0.00121318
Iteration 6/25 | Loss: 0.00121318
Iteration 7/25 | Loss: 0.00121318
Iteration 8/25 | Loss: 0.00121318
Iteration 9/25 | Loss: 0.00121318
Iteration 10/25 | Loss: 0.00121318
Iteration 11/25 | Loss: 0.00121318
Iteration 12/25 | Loss: 0.00121318
Iteration 13/25 | Loss: 0.00121318
Iteration 14/25 | Loss: 0.00121318
Iteration 15/25 | Loss: 0.00121318
Iteration 16/25 | Loss: 0.00121318
Iteration 17/25 | Loss: 0.00121318
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0012131757102906704, 0.0012131757102906704, 0.0012131757102906704, 0.0012131757102906704, 0.0012131757102906704]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012131757102906704

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00121318
Iteration 2/1000 | Loss: 0.00002253
Iteration 3/1000 | Loss: 0.00001289
Iteration 4/1000 | Loss: 0.00001201
Iteration 5/1000 | Loss: 0.00001117
Iteration 6/1000 | Loss: 0.00001084
Iteration 7/1000 | Loss: 0.00001060
Iteration 8/1000 | Loss: 0.00001054
Iteration 9/1000 | Loss: 0.00001053
Iteration 10/1000 | Loss: 0.00001053
Iteration 11/1000 | Loss: 0.00001051
Iteration 12/1000 | Loss: 0.00001043
Iteration 13/1000 | Loss: 0.00001043
Iteration 14/1000 | Loss: 0.00001036
Iteration 15/1000 | Loss: 0.00001035
Iteration 16/1000 | Loss: 0.00001031
Iteration 17/1000 | Loss: 0.00001029
Iteration 18/1000 | Loss: 0.00001028
Iteration 19/1000 | Loss: 0.00001028
Iteration 20/1000 | Loss: 0.00001027
Iteration 21/1000 | Loss: 0.00001022
Iteration 22/1000 | Loss: 0.00001019
Iteration 23/1000 | Loss: 0.00001017
Iteration 24/1000 | Loss: 0.00001017
Iteration 25/1000 | Loss: 0.00001017
Iteration 26/1000 | Loss: 0.00001016
Iteration 27/1000 | Loss: 0.00001016
Iteration 28/1000 | Loss: 0.00001015
Iteration 29/1000 | Loss: 0.00001014
Iteration 30/1000 | Loss: 0.00001013
Iteration 31/1000 | Loss: 0.00001013
Iteration 32/1000 | Loss: 0.00001012
Iteration 33/1000 | Loss: 0.00001012
Iteration 34/1000 | Loss: 0.00001012
Iteration 35/1000 | Loss: 0.00001011
Iteration 36/1000 | Loss: 0.00001011
Iteration 37/1000 | Loss: 0.00001008
Iteration 38/1000 | Loss: 0.00001007
Iteration 39/1000 | Loss: 0.00001006
Iteration 40/1000 | Loss: 0.00001004
Iteration 41/1000 | Loss: 0.00001004
Iteration 42/1000 | Loss: 0.00001004
Iteration 43/1000 | Loss: 0.00001003
Iteration 44/1000 | Loss: 0.00001003
Iteration 45/1000 | Loss: 0.00001003
Iteration 46/1000 | Loss: 0.00001003
Iteration 47/1000 | Loss: 0.00001003
Iteration 48/1000 | Loss: 0.00001002
Iteration 49/1000 | Loss: 0.00001002
Iteration 50/1000 | Loss: 0.00001002
Iteration 51/1000 | Loss: 0.00001001
Iteration 52/1000 | Loss: 0.00001001
Iteration 53/1000 | Loss: 0.00001000
Iteration 54/1000 | Loss: 0.00001000
Iteration 55/1000 | Loss: 0.00001000
Iteration 56/1000 | Loss: 0.00001000
Iteration 57/1000 | Loss: 0.00001000
Iteration 58/1000 | Loss: 0.00000999
Iteration 59/1000 | Loss: 0.00000999
Iteration 60/1000 | Loss: 0.00000999
Iteration 61/1000 | Loss: 0.00000999
Iteration 62/1000 | Loss: 0.00000999
Iteration 63/1000 | Loss: 0.00000999
Iteration 64/1000 | Loss: 0.00000999
Iteration 65/1000 | Loss: 0.00000999
Iteration 66/1000 | Loss: 0.00000999
Iteration 67/1000 | Loss: 0.00000999
Iteration 68/1000 | Loss: 0.00000999
Iteration 69/1000 | Loss: 0.00000999
Iteration 70/1000 | Loss: 0.00000999
Iteration 71/1000 | Loss: 0.00000998
Iteration 72/1000 | Loss: 0.00000998
Iteration 73/1000 | Loss: 0.00000998
Iteration 74/1000 | Loss: 0.00000998
Iteration 75/1000 | Loss: 0.00000998
Iteration 76/1000 | Loss: 0.00000998
Iteration 77/1000 | Loss: 0.00000998
Iteration 78/1000 | Loss: 0.00000998
Iteration 79/1000 | Loss: 0.00000998
Iteration 80/1000 | Loss: 0.00000998
Iteration 81/1000 | Loss: 0.00000998
Iteration 82/1000 | Loss: 0.00000998
Iteration 83/1000 | Loss: 0.00000998
Iteration 84/1000 | Loss: 0.00000998
Iteration 85/1000 | Loss: 0.00000998
Iteration 86/1000 | Loss: 0.00000998
Iteration 87/1000 | Loss: 0.00000998
Iteration 88/1000 | Loss: 0.00000998
Iteration 89/1000 | Loss: 0.00000997
Iteration 90/1000 | Loss: 0.00000997
Iteration 91/1000 | Loss: 0.00000997
Iteration 92/1000 | Loss: 0.00000997
Iteration 93/1000 | Loss: 0.00000997
Iteration 94/1000 | Loss: 0.00000997
Iteration 95/1000 | Loss: 0.00000997
Iteration 96/1000 | Loss: 0.00000997
Iteration 97/1000 | Loss: 0.00000997
Iteration 98/1000 | Loss: 0.00000996
Iteration 99/1000 | Loss: 0.00000996
Iteration 100/1000 | Loss: 0.00000996
Iteration 101/1000 | Loss: 0.00000996
Iteration 102/1000 | Loss: 0.00000995
Iteration 103/1000 | Loss: 0.00000995
Iteration 104/1000 | Loss: 0.00000995
Iteration 105/1000 | Loss: 0.00000995
Iteration 106/1000 | Loss: 0.00000995
Iteration 107/1000 | Loss: 0.00000994
Iteration 108/1000 | Loss: 0.00000994
Iteration 109/1000 | Loss: 0.00000994
Iteration 110/1000 | Loss: 0.00000994
Iteration 111/1000 | Loss: 0.00000994
Iteration 112/1000 | Loss: 0.00000994
Iteration 113/1000 | Loss: 0.00000994
Iteration 114/1000 | Loss: 0.00000994
Iteration 115/1000 | Loss: 0.00000994
Iteration 116/1000 | Loss: 0.00000993
Iteration 117/1000 | Loss: 0.00000993
Iteration 118/1000 | Loss: 0.00000993
Iteration 119/1000 | Loss: 0.00000993
Iteration 120/1000 | Loss: 0.00000993
Iteration 121/1000 | Loss: 0.00000993
Iteration 122/1000 | Loss: 0.00000993
Iteration 123/1000 | Loss: 0.00000993
Iteration 124/1000 | Loss: 0.00000993
Iteration 125/1000 | Loss: 0.00000993
Iteration 126/1000 | Loss: 0.00000992
Iteration 127/1000 | Loss: 0.00000992
Iteration 128/1000 | Loss: 0.00000992
Iteration 129/1000 | Loss: 0.00000992
Iteration 130/1000 | Loss: 0.00000991
Iteration 131/1000 | Loss: 0.00000991
Iteration 132/1000 | Loss: 0.00000991
Iteration 133/1000 | Loss: 0.00000990
Iteration 134/1000 | Loss: 0.00000990
Iteration 135/1000 | Loss: 0.00000990
Iteration 136/1000 | Loss: 0.00000990
Iteration 137/1000 | Loss: 0.00000989
Iteration 138/1000 | Loss: 0.00000989
Iteration 139/1000 | Loss: 0.00000989
Iteration 140/1000 | Loss: 0.00000989
Iteration 141/1000 | Loss: 0.00000989
Iteration 142/1000 | Loss: 0.00000989
Iteration 143/1000 | Loss: 0.00000989
Iteration 144/1000 | Loss: 0.00000989
Iteration 145/1000 | Loss: 0.00000989
Iteration 146/1000 | Loss: 0.00000988
Iteration 147/1000 | Loss: 0.00000988
Iteration 148/1000 | Loss: 0.00000988
Iteration 149/1000 | Loss: 0.00000988
Iteration 150/1000 | Loss: 0.00000988
Iteration 151/1000 | Loss: 0.00000988
Iteration 152/1000 | Loss: 0.00000988
Iteration 153/1000 | Loss: 0.00000988
Iteration 154/1000 | Loss: 0.00000988
Iteration 155/1000 | Loss: 0.00000988
Iteration 156/1000 | Loss: 0.00000988
Iteration 157/1000 | Loss: 0.00000988
Iteration 158/1000 | Loss: 0.00000988
Iteration 159/1000 | Loss: 0.00000988
Iteration 160/1000 | Loss: 0.00000988
Iteration 161/1000 | Loss: 0.00000988
Iteration 162/1000 | Loss: 0.00000988
Iteration 163/1000 | Loss: 0.00000987
Iteration 164/1000 | Loss: 0.00000987
Iteration 165/1000 | Loss: 0.00000987
Iteration 166/1000 | Loss: 0.00000987
Iteration 167/1000 | Loss: 0.00000987
Iteration 168/1000 | Loss: 0.00000987
Iteration 169/1000 | Loss: 0.00000987
Iteration 170/1000 | Loss: 0.00000987
Iteration 171/1000 | Loss: 0.00000987
Iteration 172/1000 | Loss: 0.00000987
Iteration 173/1000 | Loss: 0.00000987
Iteration 174/1000 | Loss: 0.00000987
Iteration 175/1000 | Loss: 0.00000987
Iteration 176/1000 | Loss: 0.00000987
Iteration 177/1000 | Loss: 0.00000987
Iteration 178/1000 | Loss: 0.00000987
Iteration 179/1000 | Loss: 0.00000987
Iteration 180/1000 | Loss: 0.00000987
Iteration 181/1000 | Loss: 0.00000986
Iteration 182/1000 | Loss: 0.00000986
Iteration 183/1000 | Loss: 0.00000986
Iteration 184/1000 | Loss: 0.00000986
Iteration 185/1000 | Loss: 0.00000986
Iteration 186/1000 | Loss: 0.00000986
Iteration 187/1000 | Loss: 0.00000986
Iteration 188/1000 | Loss: 0.00000986
Iteration 189/1000 | Loss: 0.00000986
Iteration 190/1000 | Loss: 0.00000986
Iteration 191/1000 | Loss: 0.00000986
Iteration 192/1000 | Loss: 0.00000986
Iteration 193/1000 | Loss: 0.00000986
Iteration 194/1000 | Loss: 0.00000986
Iteration 195/1000 | Loss: 0.00000986
Iteration 196/1000 | Loss: 0.00000986
Iteration 197/1000 | Loss: 0.00000986
Iteration 198/1000 | Loss: 0.00000986
Iteration 199/1000 | Loss: 0.00000986
Iteration 200/1000 | Loss: 0.00000986
Iteration 201/1000 | Loss: 0.00000986
Iteration 202/1000 | Loss: 0.00000985
Iteration 203/1000 | Loss: 0.00000985
Iteration 204/1000 | Loss: 0.00000985
Iteration 205/1000 | Loss: 0.00000985
Iteration 206/1000 | Loss: 0.00000985
Iteration 207/1000 | Loss: 0.00000985
Iteration 208/1000 | Loss: 0.00000985
Iteration 209/1000 | Loss: 0.00000985
Iteration 210/1000 | Loss: 0.00000985
Iteration 211/1000 | Loss: 0.00000985
Iteration 212/1000 | Loss: 0.00000985
Iteration 213/1000 | Loss: 0.00000985
Iteration 214/1000 | Loss: 0.00000985
Iteration 215/1000 | Loss: 0.00000985
Iteration 216/1000 | Loss: 0.00000985
Iteration 217/1000 | Loss: 0.00000985
Iteration 218/1000 | Loss: 0.00000985
Iteration 219/1000 | Loss: 0.00000985
Iteration 220/1000 | Loss: 0.00000985
Iteration 221/1000 | Loss: 0.00000985
Iteration 222/1000 | Loss: 0.00000984
Iteration 223/1000 | Loss: 0.00000984
Iteration 224/1000 | Loss: 0.00000984
Iteration 225/1000 | Loss: 0.00000984
Iteration 226/1000 | Loss: 0.00000984
Iteration 227/1000 | Loss: 0.00000984
Iteration 228/1000 | Loss: 0.00000984
Iteration 229/1000 | Loss: 0.00000984
Iteration 230/1000 | Loss: 0.00000984
Iteration 231/1000 | Loss: 0.00000984
Iteration 232/1000 | Loss: 0.00000984
Iteration 233/1000 | Loss: 0.00000984
Iteration 234/1000 | Loss: 0.00000984
Iteration 235/1000 | Loss: 0.00000984
Iteration 236/1000 | Loss: 0.00000984
Iteration 237/1000 | Loss: 0.00000984
Iteration 238/1000 | Loss: 0.00000984
Iteration 239/1000 | Loss: 0.00000984
Iteration 240/1000 | Loss: 0.00000984
Iteration 241/1000 | Loss: 0.00000984
Iteration 242/1000 | Loss: 0.00000984
Iteration 243/1000 | Loss: 0.00000984
Iteration 244/1000 | Loss: 0.00000984
Iteration 245/1000 | Loss: 0.00000984
Iteration 246/1000 | Loss: 0.00000984
Iteration 247/1000 | Loss: 0.00000984
Iteration 248/1000 | Loss: 0.00000984
Iteration 249/1000 | Loss: 0.00000984
Iteration 250/1000 | Loss: 0.00000984
Iteration 251/1000 | Loss: 0.00000984
Iteration 252/1000 | Loss: 0.00000984
Iteration 253/1000 | Loss: 0.00000984
Iteration 254/1000 | Loss: 0.00000984
Iteration 255/1000 | Loss: 0.00000984
Iteration 256/1000 | Loss: 0.00000984
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 256. Stopping optimization.
Last 5 losses: [9.839122867560945e-06, 9.839122867560945e-06, 9.839122867560945e-06, 9.839122867560945e-06, 9.839122867560945e-06]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 9.839122867560945e-06

Optimization complete. Final v2v error: 2.689854383468628 mm

Highest mean error: 2.9297142028808594 mm for frame 116

Lowest mean error: 2.6366939544677734 mm for frame 3

Saving results

Total time: 37.54462552070618
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1064/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1064.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1064
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00797052
Iteration 2/25 | Loss: 0.00114261
Iteration 3/25 | Loss: 0.00084921
Iteration 4/25 | Loss: 0.00082232
Iteration 5/25 | Loss: 0.00081777
Iteration 6/25 | Loss: 0.00081571
Iteration 7/25 | Loss: 0.00081560
Iteration 8/25 | Loss: 0.00081560
Iteration 9/25 | Loss: 0.00081560
Iteration 10/25 | Loss: 0.00081560
Iteration 11/25 | Loss: 0.00081560
Iteration 12/25 | Loss: 0.00081560
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008156001567840576, 0.0008156001567840576, 0.0008156001567840576, 0.0008156001567840576, 0.0008156001567840576]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008156001567840576

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.48616004
Iteration 2/25 | Loss: 0.00120709
Iteration 3/25 | Loss: 0.00120706
Iteration 4/25 | Loss: 0.00120706
Iteration 5/25 | Loss: 0.00120706
Iteration 6/25 | Loss: 0.00120706
Iteration 7/25 | Loss: 0.00120706
Iteration 8/25 | Loss: 0.00120706
Iteration 9/25 | Loss: 0.00120706
Iteration 10/25 | Loss: 0.00120706
Iteration 11/25 | Loss: 0.00120706
Iteration 12/25 | Loss: 0.00120706
Iteration 13/25 | Loss: 0.00120706
Iteration 14/25 | Loss: 0.00120706
Iteration 15/25 | Loss: 0.00120706
Iteration 16/25 | Loss: 0.00120706
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001207062043249607, 0.001207062043249607, 0.001207062043249607, 0.001207062043249607, 0.001207062043249607]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001207062043249607

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00120706
Iteration 2/1000 | Loss: 0.00002298
Iteration 3/1000 | Loss: 0.00001792
Iteration 4/1000 | Loss: 0.00001656
Iteration 5/1000 | Loss: 0.00001577
Iteration 6/1000 | Loss: 0.00001538
Iteration 7/1000 | Loss: 0.00001520
Iteration 8/1000 | Loss: 0.00001499
Iteration 9/1000 | Loss: 0.00001488
Iteration 10/1000 | Loss: 0.00001488
Iteration 11/1000 | Loss: 0.00001484
Iteration 12/1000 | Loss: 0.00001483
Iteration 13/1000 | Loss: 0.00001483
Iteration 14/1000 | Loss: 0.00001475
Iteration 15/1000 | Loss: 0.00001471
Iteration 16/1000 | Loss: 0.00001471
Iteration 17/1000 | Loss: 0.00001471
Iteration 18/1000 | Loss: 0.00001470
Iteration 19/1000 | Loss: 0.00001470
Iteration 20/1000 | Loss: 0.00001470
Iteration 21/1000 | Loss: 0.00001470
Iteration 22/1000 | Loss: 0.00001470
Iteration 23/1000 | Loss: 0.00001470
Iteration 24/1000 | Loss: 0.00001470
Iteration 25/1000 | Loss: 0.00001470
Iteration 26/1000 | Loss: 0.00001470
Iteration 27/1000 | Loss: 0.00001470
Iteration 28/1000 | Loss: 0.00001470
Iteration 29/1000 | Loss: 0.00001470
Iteration 30/1000 | Loss: 0.00001470
Iteration 31/1000 | Loss: 0.00001470
Iteration 32/1000 | Loss: 0.00001468
Iteration 33/1000 | Loss: 0.00001467
Iteration 34/1000 | Loss: 0.00001467
Iteration 35/1000 | Loss: 0.00001466
Iteration 36/1000 | Loss: 0.00001466
Iteration 37/1000 | Loss: 0.00001464
Iteration 38/1000 | Loss: 0.00001464
Iteration 39/1000 | Loss: 0.00001464
Iteration 40/1000 | Loss: 0.00001463
Iteration 41/1000 | Loss: 0.00001463
Iteration 42/1000 | Loss: 0.00001463
Iteration 43/1000 | Loss: 0.00001463
Iteration 44/1000 | Loss: 0.00001463
Iteration 45/1000 | Loss: 0.00001463
Iteration 46/1000 | Loss: 0.00001462
Iteration 47/1000 | Loss: 0.00001462
Iteration 48/1000 | Loss: 0.00001462
Iteration 49/1000 | Loss: 0.00001462
Iteration 50/1000 | Loss: 0.00001462
Iteration 51/1000 | Loss: 0.00001462
Iteration 52/1000 | Loss: 0.00001461
Iteration 53/1000 | Loss: 0.00001461
Iteration 54/1000 | Loss: 0.00001461
Iteration 55/1000 | Loss: 0.00001461
Iteration 56/1000 | Loss: 0.00001460
Iteration 57/1000 | Loss: 0.00001460
Iteration 58/1000 | Loss: 0.00001459
Iteration 59/1000 | Loss: 0.00001459
Iteration 60/1000 | Loss: 0.00001459
Iteration 61/1000 | Loss: 0.00001459
Iteration 62/1000 | Loss: 0.00001459
Iteration 63/1000 | Loss: 0.00001459
Iteration 64/1000 | Loss: 0.00001458
Iteration 65/1000 | Loss: 0.00001458
Iteration 66/1000 | Loss: 0.00001458
Iteration 67/1000 | Loss: 0.00001458
Iteration 68/1000 | Loss: 0.00001457
Iteration 69/1000 | Loss: 0.00001456
Iteration 70/1000 | Loss: 0.00001456
Iteration 71/1000 | Loss: 0.00001456
Iteration 72/1000 | Loss: 0.00001456
Iteration 73/1000 | Loss: 0.00001456
Iteration 74/1000 | Loss: 0.00001455
Iteration 75/1000 | Loss: 0.00001455
Iteration 76/1000 | Loss: 0.00001455
Iteration 77/1000 | Loss: 0.00001455
Iteration 78/1000 | Loss: 0.00001455
Iteration 79/1000 | Loss: 0.00001455
Iteration 80/1000 | Loss: 0.00001455
Iteration 81/1000 | Loss: 0.00001455
Iteration 82/1000 | Loss: 0.00001455
Iteration 83/1000 | Loss: 0.00001455
Iteration 84/1000 | Loss: 0.00001455
Iteration 85/1000 | Loss: 0.00001455
Iteration 86/1000 | Loss: 0.00001454
Iteration 87/1000 | Loss: 0.00001454
Iteration 88/1000 | Loss: 0.00001454
Iteration 89/1000 | Loss: 0.00001453
Iteration 90/1000 | Loss: 0.00001453
Iteration 91/1000 | Loss: 0.00001453
Iteration 92/1000 | Loss: 0.00001453
Iteration 93/1000 | Loss: 0.00001453
Iteration 94/1000 | Loss: 0.00001453
Iteration 95/1000 | Loss: 0.00001452
Iteration 96/1000 | Loss: 0.00001452
Iteration 97/1000 | Loss: 0.00001452
Iteration 98/1000 | Loss: 0.00001452
Iteration 99/1000 | Loss: 0.00001452
Iteration 100/1000 | Loss: 0.00001452
Iteration 101/1000 | Loss: 0.00001451
Iteration 102/1000 | Loss: 0.00001451
Iteration 103/1000 | Loss: 0.00001451
Iteration 104/1000 | Loss: 0.00001451
Iteration 105/1000 | Loss: 0.00001451
Iteration 106/1000 | Loss: 0.00001451
Iteration 107/1000 | Loss: 0.00001451
Iteration 108/1000 | Loss: 0.00001451
Iteration 109/1000 | Loss: 0.00001451
Iteration 110/1000 | Loss: 0.00001450
Iteration 111/1000 | Loss: 0.00001450
Iteration 112/1000 | Loss: 0.00001450
Iteration 113/1000 | Loss: 0.00001450
Iteration 114/1000 | Loss: 0.00001449
Iteration 115/1000 | Loss: 0.00001449
Iteration 116/1000 | Loss: 0.00001449
Iteration 117/1000 | Loss: 0.00001449
Iteration 118/1000 | Loss: 0.00001449
Iteration 119/1000 | Loss: 0.00001449
Iteration 120/1000 | Loss: 0.00001449
Iteration 121/1000 | Loss: 0.00001449
Iteration 122/1000 | Loss: 0.00001449
Iteration 123/1000 | Loss: 0.00001449
Iteration 124/1000 | Loss: 0.00001449
Iteration 125/1000 | Loss: 0.00001449
Iteration 126/1000 | Loss: 0.00001449
Iteration 127/1000 | Loss: 0.00001449
Iteration 128/1000 | Loss: 0.00001449
Iteration 129/1000 | Loss: 0.00001449
Iteration 130/1000 | Loss: 0.00001449
Iteration 131/1000 | Loss: 0.00001449
Iteration 132/1000 | Loss: 0.00001449
Iteration 133/1000 | Loss: 0.00001449
Iteration 134/1000 | Loss: 0.00001449
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 134. Stopping optimization.
Last 5 losses: [1.4492218724626582e-05, 1.4492218724626582e-05, 1.4492218724626582e-05, 1.4492218724626582e-05, 1.4492218724626582e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4492218724626582e-05

Optimization complete. Final v2v error: 3.262312889099121 mm

Highest mean error: 3.4800221920013428 mm for frame 29

Lowest mean error: 3.1249732971191406 mm for frame 54

Saving results

Total time: 30.860455751419067
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01053117
Iteration 2/25 | Loss: 0.00224579
Iteration 3/25 | Loss: 0.00158901
Iteration 4/25 | Loss: 0.00139626
Iteration 5/25 | Loss: 0.00121309
Iteration 6/25 | Loss: 0.00113980
Iteration 7/25 | Loss: 0.00108008
Iteration 8/25 | Loss: 0.00104206
Iteration 9/25 | Loss: 0.00101094
Iteration 10/25 | Loss: 0.00098171
Iteration 11/25 | Loss: 0.00096604
Iteration 12/25 | Loss: 0.00096202
Iteration 13/25 | Loss: 0.00093858
Iteration 14/25 | Loss: 0.00092397
Iteration 15/25 | Loss: 0.00091822
Iteration 16/25 | Loss: 0.00091654
Iteration 17/25 | Loss: 0.00091554
Iteration 18/25 | Loss: 0.00091462
Iteration 19/25 | Loss: 0.00091452
Iteration 20/25 | Loss: 0.00091228
Iteration 21/25 | Loss: 0.00091184
Iteration 22/25 | Loss: 0.00090878
Iteration 23/25 | Loss: 0.00091134
Iteration 24/25 | Loss: 0.00091127
Iteration 25/25 | Loss: 0.00091181

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.62959504
Iteration 2/25 | Loss: 0.00207145
Iteration 3/25 | Loss: 0.00206882
Iteration 4/25 | Loss: 0.00206882
Iteration 5/25 | Loss: 0.00206882
Iteration 6/25 | Loss: 0.00206882
Iteration 7/25 | Loss: 0.00206882
Iteration 8/25 | Loss: 0.00206882
Iteration 9/25 | Loss: 0.00206882
Iteration 10/25 | Loss: 0.00206882
Iteration 11/25 | Loss: 0.00206882
Iteration 12/25 | Loss: 0.00206882
Iteration 13/25 | Loss: 0.00206882
Iteration 14/25 | Loss: 0.00206882
Iteration 15/25 | Loss: 0.00206882
Iteration 16/25 | Loss: 0.00206882
Iteration 17/25 | Loss: 0.00206882
Iteration 18/25 | Loss: 0.00206882
Iteration 19/25 | Loss: 0.00206882
Iteration 20/25 | Loss: 0.00206882
Iteration 21/25 | Loss: 0.00206882
Iteration 22/25 | Loss: 0.00206882
Iteration 23/25 | Loss: 0.00206882
Iteration 24/25 | Loss: 0.00206882
Iteration 25/25 | Loss: 0.00206882

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00206882
Iteration 2/1000 | Loss: 0.00018199
Iteration 3/1000 | Loss: 0.00015216
Iteration 4/1000 | Loss: 0.00012267
Iteration 5/1000 | Loss: 0.00021403
Iteration 6/1000 | Loss: 0.00009892
Iteration 7/1000 | Loss: 0.00008349
Iteration 8/1000 | Loss: 0.00016507
Iteration 9/1000 | Loss: 0.00010430
Iteration 10/1000 | Loss: 0.00005562
Iteration 11/1000 | Loss: 0.00005973
Iteration 12/1000 | Loss: 0.00004694
Iteration 13/1000 | Loss: 0.00020443
Iteration 14/1000 | Loss: 0.00015889
Iteration 15/1000 | Loss: 0.00009202
Iteration 16/1000 | Loss: 0.00004508
Iteration 17/1000 | Loss: 0.00003951
Iteration 18/1000 | Loss: 0.00005929
Iteration 19/1000 | Loss: 0.00003770
Iteration 20/1000 | Loss: 0.00003701
Iteration 21/1000 | Loss: 0.00003637
Iteration 22/1000 | Loss: 0.00003588
Iteration 23/1000 | Loss: 0.00009238
Iteration 24/1000 | Loss: 0.00069646
Iteration 25/1000 | Loss: 0.00006517
Iteration 26/1000 | Loss: 0.00004736
Iteration 27/1000 | Loss: 0.00009664
Iteration 28/1000 | Loss: 0.00004658
Iteration 29/1000 | Loss: 0.00006544
Iteration 30/1000 | Loss: 0.00003967
Iteration 31/1000 | Loss: 0.00003444
Iteration 32/1000 | Loss: 0.00006520
Iteration 33/1000 | Loss: 0.00003307
Iteration 34/1000 | Loss: 0.00003264
Iteration 35/1000 | Loss: 0.00005420
Iteration 36/1000 | Loss: 0.00003762
Iteration 37/1000 | Loss: 0.00003212
Iteration 38/1000 | Loss: 0.00004123
Iteration 39/1000 | Loss: 0.00005158
Iteration 40/1000 | Loss: 0.00038707
Iteration 41/1000 | Loss: 0.00011628
Iteration 42/1000 | Loss: 0.00003437
Iteration 43/1000 | Loss: 0.00004178
Iteration 44/1000 | Loss: 0.00003150
Iteration 45/1000 | Loss: 0.00005000
Iteration 46/1000 | Loss: 0.00002960
Iteration 47/1000 | Loss: 0.00002917
Iteration 48/1000 | Loss: 0.00004117
Iteration 49/1000 | Loss: 0.00002883
Iteration 50/1000 | Loss: 0.00002879
Iteration 51/1000 | Loss: 0.00004597
Iteration 52/1000 | Loss: 0.00003921
Iteration 53/1000 | Loss: 0.00002863
Iteration 54/1000 | Loss: 0.00002862
Iteration 55/1000 | Loss: 0.00002862
Iteration 56/1000 | Loss: 0.00002861
Iteration 57/1000 | Loss: 0.00002860
Iteration 58/1000 | Loss: 0.00002854
Iteration 59/1000 | Loss: 0.00002850
Iteration 60/1000 | Loss: 0.00003698
Iteration 61/1000 | Loss: 0.00002907
Iteration 62/1000 | Loss: 0.00002844
Iteration 63/1000 | Loss: 0.00002842
Iteration 64/1000 | Loss: 0.00002842
Iteration 65/1000 | Loss: 0.00002842
Iteration 66/1000 | Loss: 0.00002841
Iteration 67/1000 | Loss: 0.00002841
Iteration 68/1000 | Loss: 0.00002841
Iteration 69/1000 | Loss: 0.00002840
Iteration 70/1000 | Loss: 0.00003069
Iteration 71/1000 | Loss: 0.00005215
Iteration 72/1000 | Loss: 0.00002839
Iteration 73/1000 | Loss: 0.00002888
Iteration 74/1000 | Loss: 0.00022870
Iteration 75/1000 | Loss: 0.00003498
Iteration 76/1000 | Loss: 0.00003781
Iteration 77/1000 | Loss: 0.00003053
Iteration 78/1000 | Loss: 0.00003017
Iteration 79/1000 | Loss: 0.00003302
Iteration 80/1000 | Loss: 0.00003019
Iteration 81/1000 | Loss: 0.00003009
Iteration 82/1000 | Loss: 0.00002969
Iteration 83/1000 | Loss: 0.00002969
Iteration 84/1000 | Loss: 0.00002969
Iteration 85/1000 | Loss: 0.00002969
Iteration 86/1000 | Loss: 0.00002969
Iteration 87/1000 | Loss: 0.00002968
Iteration 88/1000 | Loss: 0.00002968
Iteration 89/1000 | Loss: 0.00002968
Iteration 90/1000 | Loss: 0.00003102
Iteration 91/1000 | Loss: 0.00002960
Iteration 92/1000 | Loss: 0.00002960
Iteration 93/1000 | Loss: 0.00002960
Iteration 94/1000 | Loss: 0.00002960
Iteration 95/1000 | Loss: 0.00002960
Iteration 96/1000 | Loss: 0.00002960
Iteration 97/1000 | Loss: 0.00002960
Iteration 98/1000 | Loss: 0.00002960
Iteration 99/1000 | Loss: 0.00002960
Iteration 100/1000 | Loss: 0.00002960
Iteration 101/1000 | Loss: 0.00002960
Iteration 102/1000 | Loss: 0.00002960
Iteration 103/1000 | Loss: 0.00002959
Iteration 104/1000 | Loss: 0.00004956
Iteration 105/1000 | Loss: 0.00003731
Iteration 106/1000 | Loss: 0.00004053
Iteration 107/1000 | Loss: 0.00003021
Iteration 108/1000 | Loss: 0.00002943
Iteration 109/1000 | Loss: 0.00003495
Iteration 110/1000 | Loss: 0.00002969
Iteration 111/1000 | Loss: 0.00002935
Iteration 112/1000 | Loss: 0.00002934
Iteration 113/1000 | Loss: 0.00002934
Iteration 114/1000 | Loss: 0.00002934
Iteration 115/1000 | Loss: 0.00002933
Iteration 116/1000 | Loss: 0.00002933
Iteration 117/1000 | Loss: 0.00002933
Iteration 118/1000 | Loss: 0.00002933
Iteration 119/1000 | Loss: 0.00002933
Iteration 120/1000 | Loss: 0.00002933
Iteration 121/1000 | Loss: 0.00002932
Iteration 122/1000 | Loss: 0.00002932
Iteration 123/1000 | Loss: 0.00002931
Iteration 124/1000 | Loss: 0.00002931
Iteration 125/1000 | Loss: 0.00002931
Iteration 126/1000 | Loss: 0.00002931
Iteration 127/1000 | Loss: 0.00002930
Iteration 128/1000 | Loss: 0.00002930
Iteration 129/1000 | Loss: 0.00002929
Iteration 130/1000 | Loss: 0.00002929
Iteration 131/1000 | Loss: 0.00002928
Iteration 132/1000 | Loss: 0.00002928
Iteration 133/1000 | Loss: 0.00002927
Iteration 134/1000 | Loss: 0.00002927
Iteration 135/1000 | Loss: 0.00002926
Iteration 136/1000 | Loss: 0.00002926
Iteration 137/1000 | Loss: 0.00002925
Iteration 138/1000 | Loss: 0.00002925
Iteration 139/1000 | Loss: 0.00002925
Iteration 140/1000 | Loss: 0.00002924
Iteration 141/1000 | Loss: 0.00002924
Iteration 142/1000 | Loss: 0.00002924
Iteration 143/1000 | Loss: 0.00002924
Iteration 144/1000 | Loss: 0.00002923
Iteration 145/1000 | Loss: 0.00002923
Iteration 146/1000 | Loss: 0.00002923
Iteration 147/1000 | Loss: 0.00002923
Iteration 148/1000 | Loss: 0.00002923
Iteration 149/1000 | Loss: 0.00002922
Iteration 150/1000 | Loss: 0.00002922
Iteration 151/1000 | Loss: 0.00003284
Iteration 152/1000 | Loss: 0.00002918
Iteration 153/1000 | Loss: 0.00002918
Iteration 154/1000 | Loss: 0.00002918
Iteration 155/1000 | Loss: 0.00002918
Iteration 156/1000 | Loss: 0.00002918
Iteration 157/1000 | Loss: 0.00002917
Iteration 158/1000 | Loss: 0.00002917
Iteration 159/1000 | Loss: 0.00002917
Iteration 160/1000 | Loss: 0.00002917
Iteration 161/1000 | Loss: 0.00002917
Iteration 162/1000 | Loss: 0.00002917
Iteration 163/1000 | Loss: 0.00002917
Iteration 164/1000 | Loss: 0.00002917
Iteration 165/1000 | Loss: 0.00002916
Iteration 166/1000 | Loss: 0.00002916
Iteration 167/1000 | Loss: 0.00002916
Iteration 168/1000 | Loss: 0.00002915
Iteration 169/1000 | Loss: 0.00002915
Iteration 170/1000 | Loss: 0.00002914
Iteration 171/1000 | Loss: 0.00002914
Iteration 172/1000 | Loss: 0.00002914
Iteration 173/1000 | Loss: 0.00002914
Iteration 174/1000 | Loss: 0.00002914
Iteration 175/1000 | Loss: 0.00002914
Iteration 176/1000 | Loss: 0.00002914
Iteration 177/1000 | Loss: 0.00002914
Iteration 178/1000 | Loss: 0.00002914
Iteration 179/1000 | Loss: 0.00002914
Iteration 180/1000 | Loss: 0.00002914
Iteration 181/1000 | Loss: 0.00002913
Iteration 182/1000 | Loss: 0.00002913
Iteration 183/1000 | Loss: 0.00002913
Iteration 184/1000 | Loss: 0.00002912
Iteration 185/1000 | Loss: 0.00002912
Iteration 186/1000 | Loss: 0.00004646
Iteration 187/1000 | Loss: 0.00003579
Iteration 188/1000 | Loss: 0.00007037
Iteration 189/1000 | Loss: 0.00002924
Iteration 190/1000 | Loss: 0.00003303
Iteration 191/1000 | Loss: 0.00002881
Iteration 192/1000 | Loss: 0.00002880
Iteration 193/1000 | Loss: 0.00004243
Iteration 194/1000 | Loss: 0.00002868
Iteration 195/1000 | Loss: 0.00008108
Iteration 196/1000 | Loss: 0.00002854
Iteration 197/1000 | Loss: 0.00002848
Iteration 198/1000 | Loss: 0.00002846
Iteration 199/1000 | Loss: 0.00002846
Iteration 200/1000 | Loss: 0.00002845
Iteration 201/1000 | Loss: 0.00002844
Iteration 202/1000 | Loss: 0.00002844
Iteration 203/1000 | Loss: 0.00003068
Iteration 204/1000 | Loss: 0.00024243
Iteration 205/1000 | Loss: 0.00019024
Iteration 206/1000 | Loss: 0.00003624
Iteration 207/1000 | Loss: 0.00003112
Iteration 208/1000 | Loss: 0.00002850
Iteration 209/1000 | Loss: 0.00002850
Iteration 210/1000 | Loss: 0.00002870
Iteration 211/1000 | Loss: 0.00003391
Iteration 212/1000 | Loss: 0.00002856
Iteration 213/1000 | Loss: 0.00002839
Iteration 214/1000 | Loss: 0.00002839
Iteration 215/1000 | Loss: 0.00002839
Iteration 216/1000 | Loss: 0.00002839
Iteration 217/1000 | Loss: 0.00002839
Iteration 218/1000 | Loss: 0.00002839
Iteration 219/1000 | Loss: 0.00004332
Iteration 220/1000 | Loss: 0.00024393
Iteration 221/1000 | Loss: 0.00022148
Iteration 222/1000 | Loss: 0.00018364
Iteration 223/1000 | Loss: 0.00018360
Iteration 224/1000 | Loss: 0.00004494
Iteration 225/1000 | Loss: 0.00003007
Iteration 226/1000 | Loss: 0.00007288
Iteration 227/1000 | Loss: 0.00002866
Iteration 228/1000 | Loss: 0.00003526
Iteration 229/1000 | Loss: 0.00002805
Iteration 230/1000 | Loss: 0.00002803
Iteration 231/1000 | Loss: 0.00002780
Iteration 232/1000 | Loss: 0.00002779
Iteration 233/1000 | Loss: 0.00002777
Iteration 234/1000 | Loss: 0.00002776
Iteration 235/1000 | Loss: 0.00002775
Iteration 236/1000 | Loss: 0.00002772
Iteration 237/1000 | Loss: 0.00002770
Iteration 238/1000 | Loss: 0.00002769
Iteration 239/1000 | Loss: 0.00002769
Iteration 240/1000 | Loss: 0.00003049
Iteration 241/1000 | Loss: 0.00003591
Iteration 242/1000 | Loss: 0.00002795
Iteration 243/1000 | Loss: 0.00002785
Iteration 244/1000 | Loss: 0.00002762
Iteration 245/1000 | Loss: 0.00002761
Iteration 246/1000 | Loss: 0.00002761
Iteration 247/1000 | Loss: 0.00003247
Iteration 248/1000 | Loss: 0.00002767
Iteration 249/1000 | Loss: 0.00002747
Iteration 250/1000 | Loss: 0.00002747
Iteration 251/1000 | Loss: 0.00002747
Iteration 252/1000 | Loss: 0.00002747
Iteration 253/1000 | Loss: 0.00002747
Iteration 254/1000 | Loss: 0.00002747
Iteration 255/1000 | Loss: 0.00002747
Iteration 256/1000 | Loss: 0.00002746
Iteration 257/1000 | Loss: 0.00003017
Iteration 258/1000 | Loss: 0.00002759
Iteration 259/1000 | Loss: 0.00002745
Iteration 260/1000 | Loss: 0.00002745
Iteration 261/1000 | Loss: 0.00002745
Iteration 262/1000 | Loss: 0.00002744
Iteration 263/1000 | Loss: 0.00002744
Iteration 264/1000 | Loss: 0.00002744
Iteration 265/1000 | Loss: 0.00002758
Iteration 266/1000 | Loss: 0.00002744
Iteration 267/1000 | Loss: 0.00002744
Iteration 268/1000 | Loss: 0.00002744
Iteration 269/1000 | Loss: 0.00002744
Iteration 270/1000 | Loss: 0.00002744
Iteration 271/1000 | Loss: 0.00002744
Iteration 272/1000 | Loss: 0.00002744
Iteration 273/1000 | Loss: 0.00002744
Iteration 274/1000 | Loss: 0.00002744
Iteration 275/1000 | Loss: 0.00002744
Iteration 276/1000 | Loss: 0.00002744
Iteration 277/1000 | Loss: 0.00002744
Iteration 278/1000 | Loss: 0.00002744
Iteration 279/1000 | Loss: 0.00002744
Iteration 280/1000 | Loss: 0.00002744
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 280. Stopping optimization.
Last 5 losses: [2.7437848984845914e-05, 2.7437848984845914e-05, 2.7437848984845914e-05, 2.7437848984845914e-05, 2.7437848984845914e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.7437848984845914e-05

Optimization complete. Final v2v error: 4.116962432861328 mm

Highest mean error: 11.638203620910645 mm for frame 48

Lowest mean error: 3.4826018810272217 mm for frame 61

Saving results

Total time: 238.29915738105774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871012
Iteration 2/25 | Loss: 0.00122587
Iteration 3/25 | Loss: 0.00100851
Iteration 4/25 | Loss: 0.00092081
Iteration 5/25 | Loss: 0.00090503
Iteration 6/25 | Loss: 0.00090327
Iteration 7/25 | Loss: 0.00090320
Iteration 8/25 | Loss: 0.00090320
Iteration 9/25 | Loss: 0.00090320
Iteration 10/25 | Loss: 0.00090320
Iteration 11/25 | Loss: 0.00090320
Iteration 12/25 | Loss: 0.00090320
Iteration 13/25 | Loss: 0.00090320
Iteration 14/25 | Loss: 0.00090320
Iteration 15/25 | Loss: 0.00090320
Iteration 16/25 | Loss: 0.00090320
Iteration 17/25 | Loss: 0.00090320
Iteration 18/25 | Loss: 0.00090320
Iteration 19/25 | Loss: 0.00090320
Iteration 20/25 | Loss: 0.00090320
Iteration 21/25 | Loss: 0.00090320
Iteration 22/25 | Loss: 0.00090320
Iteration 23/25 | Loss: 0.00090320
Iteration 24/25 | Loss: 0.00090320
Iteration 25/25 | Loss: 0.00090320

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57001317
Iteration 2/25 | Loss: 0.00135219
Iteration 3/25 | Loss: 0.00135219
Iteration 4/25 | Loss: 0.00135219
Iteration 5/25 | Loss: 0.00135219
Iteration 6/25 | Loss: 0.00135219
Iteration 7/25 | Loss: 0.00135219
Iteration 8/25 | Loss: 0.00135219
Iteration 9/25 | Loss: 0.00135219
Iteration 10/25 | Loss: 0.00135219
Iteration 11/25 | Loss: 0.00135219
Iteration 12/25 | Loss: 0.00135219
Iteration 13/25 | Loss: 0.00135219
Iteration 14/25 | Loss: 0.00135219
Iteration 15/25 | Loss: 0.00135219
Iteration 16/25 | Loss: 0.00135219
Iteration 17/25 | Loss: 0.00135219
Iteration 18/25 | Loss: 0.00135219
Iteration 19/25 | Loss: 0.00135219
Iteration 20/25 | Loss: 0.00135219
Iteration 21/25 | Loss: 0.00135219
Iteration 22/25 | Loss: 0.00135219
Iteration 23/25 | Loss: 0.00135219
Iteration 24/25 | Loss: 0.00135219
Iteration 25/25 | Loss: 0.00135219

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00135219
Iteration 2/1000 | Loss: 0.00005100
Iteration 3/1000 | Loss: 0.00003115
Iteration 4/1000 | Loss: 0.00002784
Iteration 5/1000 | Loss: 0.00002632
Iteration 6/1000 | Loss: 0.00002539
Iteration 7/1000 | Loss: 0.00002479
Iteration 8/1000 | Loss: 0.00002426
Iteration 9/1000 | Loss: 0.00002406
Iteration 10/1000 | Loss: 0.00002384
Iteration 11/1000 | Loss: 0.00002367
Iteration 12/1000 | Loss: 0.00002363
Iteration 13/1000 | Loss: 0.00002357
Iteration 14/1000 | Loss: 0.00002347
Iteration 15/1000 | Loss: 0.00002347
Iteration 16/1000 | Loss: 0.00002347
Iteration 17/1000 | Loss: 0.00002344
Iteration 18/1000 | Loss: 0.00002343
Iteration 19/1000 | Loss: 0.00002342
Iteration 20/1000 | Loss: 0.00002340
Iteration 21/1000 | Loss: 0.00002340
Iteration 22/1000 | Loss: 0.00002340
Iteration 23/1000 | Loss: 0.00002340
Iteration 24/1000 | Loss: 0.00002340
Iteration 25/1000 | Loss: 0.00002340
Iteration 26/1000 | Loss: 0.00002340
Iteration 27/1000 | Loss: 0.00002339
Iteration 28/1000 | Loss: 0.00002339
Iteration 29/1000 | Loss: 0.00002339
Iteration 30/1000 | Loss: 0.00002339
Iteration 31/1000 | Loss: 0.00002339
Iteration 32/1000 | Loss: 0.00002339
Iteration 33/1000 | Loss: 0.00002339
Iteration 34/1000 | Loss: 0.00002339
Iteration 35/1000 | Loss: 0.00002339
Iteration 36/1000 | Loss: 0.00002339
Iteration 37/1000 | Loss: 0.00002339
Iteration 38/1000 | Loss: 0.00002338
Iteration 39/1000 | Loss: 0.00002338
Iteration 40/1000 | Loss: 0.00002338
Iteration 41/1000 | Loss: 0.00002338
Iteration 42/1000 | Loss: 0.00002337
Iteration 43/1000 | Loss: 0.00002337
Iteration 44/1000 | Loss: 0.00002337
Iteration 45/1000 | Loss: 0.00002337
Iteration 46/1000 | Loss: 0.00002337
Iteration 47/1000 | Loss: 0.00002337
Iteration 48/1000 | Loss: 0.00002337
Iteration 49/1000 | Loss: 0.00002336
Iteration 50/1000 | Loss: 0.00002336
Iteration 51/1000 | Loss: 0.00002336
Iteration 52/1000 | Loss: 0.00002335
Iteration 53/1000 | Loss: 0.00002335
Iteration 54/1000 | Loss: 0.00002335
Iteration 55/1000 | Loss: 0.00002335
Iteration 56/1000 | Loss: 0.00002334
Iteration 57/1000 | Loss: 0.00002334
Iteration 58/1000 | Loss: 0.00002334
Iteration 59/1000 | Loss: 0.00002333
Iteration 60/1000 | Loss: 0.00002333
Iteration 61/1000 | Loss: 0.00002333
Iteration 62/1000 | Loss: 0.00002333
Iteration 63/1000 | Loss: 0.00002332
Iteration 64/1000 | Loss: 0.00002332
Iteration 65/1000 | Loss: 0.00002332
Iteration 66/1000 | Loss: 0.00002331
Iteration 67/1000 | Loss: 0.00002331
Iteration 68/1000 | Loss: 0.00002331
Iteration 69/1000 | Loss: 0.00002330
Iteration 70/1000 | Loss: 0.00002330
Iteration 71/1000 | Loss: 0.00002330
Iteration 72/1000 | Loss: 0.00002330
Iteration 73/1000 | Loss: 0.00002330
Iteration 74/1000 | Loss: 0.00002329
Iteration 75/1000 | Loss: 0.00002329
Iteration 76/1000 | Loss: 0.00002329
Iteration 77/1000 | Loss: 0.00002329
Iteration 78/1000 | Loss: 0.00002328
Iteration 79/1000 | Loss: 0.00002328
Iteration 80/1000 | Loss: 0.00002328
Iteration 81/1000 | Loss: 0.00002328
Iteration 82/1000 | Loss: 0.00002327
Iteration 83/1000 | Loss: 0.00002327
Iteration 84/1000 | Loss: 0.00002327
Iteration 85/1000 | Loss: 0.00002327
Iteration 86/1000 | Loss: 0.00002327
Iteration 87/1000 | Loss: 0.00002327
Iteration 88/1000 | Loss: 0.00002327
Iteration 89/1000 | Loss: 0.00002327
Iteration 90/1000 | Loss: 0.00002327
Iteration 91/1000 | Loss: 0.00002327
Iteration 92/1000 | Loss: 0.00002327
Iteration 93/1000 | Loss: 0.00002327
Iteration 94/1000 | Loss: 0.00002326
Iteration 95/1000 | Loss: 0.00002326
Iteration 96/1000 | Loss: 0.00002326
Iteration 97/1000 | Loss: 0.00002326
Iteration 98/1000 | Loss: 0.00002326
Iteration 99/1000 | Loss: 0.00002326
Iteration 100/1000 | Loss: 0.00002326
Iteration 101/1000 | Loss: 0.00002326
Iteration 102/1000 | Loss: 0.00002326
Iteration 103/1000 | Loss: 0.00002326
Iteration 104/1000 | Loss: 0.00002326
Iteration 105/1000 | Loss: 0.00002325
Iteration 106/1000 | Loss: 0.00002325
Iteration 107/1000 | Loss: 0.00002325
Iteration 108/1000 | Loss: 0.00002325
Iteration 109/1000 | Loss: 0.00002325
Iteration 110/1000 | Loss: 0.00002325
Iteration 111/1000 | Loss: 0.00002325
Iteration 112/1000 | Loss: 0.00002325
Iteration 113/1000 | Loss: 0.00002324
Iteration 114/1000 | Loss: 0.00002324
Iteration 115/1000 | Loss: 0.00002324
Iteration 116/1000 | Loss: 0.00002324
Iteration 117/1000 | Loss: 0.00002324
Iteration 118/1000 | Loss: 0.00002324
Iteration 119/1000 | Loss: 0.00002324
Iteration 120/1000 | Loss: 0.00002323
Iteration 121/1000 | Loss: 0.00002323
Iteration 122/1000 | Loss: 0.00002323
Iteration 123/1000 | Loss: 0.00002323
Iteration 124/1000 | Loss: 0.00002323
Iteration 125/1000 | Loss: 0.00002323
Iteration 126/1000 | Loss: 0.00002323
Iteration 127/1000 | Loss: 0.00002323
Iteration 128/1000 | Loss: 0.00002323
Iteration 129/1000 | Loss: 0.00002322
Iteration 130/1000 | Loss: 0.00002322
Iteration 131/1000 | Loss: 0.00002322
Iteration 132/1000 | Loss: 0.00002322
Iteration 133/1000 | Loss: 0.00002322
Iteration 134/1000 | Loss: 0.00002322
Iteration 135/1000 | Loss: 0.00002322
Iteration 136/1000 | Loss: 0.00002322
Iteration 137/1000 | Loss: 0.00002322
Iteration 138/1000 | Loss: 0.00002322
Iteration 139/1000 | Loss: 0.00002322
Iteration 140/1000 | Loss: 0.00002322
Iteration 141/1000 | Loss: 0.00002322
Iteration 142/1000 | Loss: 0.00002322
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 142. Stopping optimization.
Last 5 losses: [2.3223343305289745e-05, 2.3223343305289745e-05, 2.3223343305289745e-05, 2.3223343305289745e-05, 2.3223343305289745e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3223343305289745e-05

Optimization complete. Final v2v error: 4.018518447875977 mm

Highest mean error: 4.300874710083008 mm for frame 144

Lowest mean error: 3.9084572792053223 mm for frame 34

Saving results

Total time: 34.85981488227844
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1065/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1065.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1065
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00447123
Iteration 2/25 | Loss: 0.00094099
Iteration 3/25 | Loss: 0.00083028
Iteration 4/25 | Loss: 0.00081095
Iteration 5/25 | Loss: 0.00080792
Iteration 6/25 | Loss: 0.00080738
Iteration 7/25 | Loss: 0.00080738
Iteration 8/25 | Loss: 0.00080738
Iteration 9/25 | Loss: 0.00080738
Iteration 10/25 | Loss: 0.00080738
Iteration 11/25 | Loss: 0.00080738
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0008073760545812547, 0.0008073760545812547, 0.0008073760545812547, 0.0008073760545812547, 0.0008073760545812547]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008073760545812547

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15791905
Iteration 2/25 | Loss: 0.00138708
Iteration 3/25 | Loss: 0.00138708
Iteration 4/25 | Loss: 0.00138708
Iteration 5/25 | Loss: 0.00138708
Iteration 6/25 | Loss: 0.00138708
Iteration 7/25 | Loss: 0.00138708
Iteration 8/25 | Loss: 0.00138708
Iteration 9/25 | Loss: 0.00138708
Iteration 10/25 | Loss: 0.00138708
Iteration 11/25 | Loss: 0.00138708
Iteration 12/25 | Loss: 0.00138708
Iteration 13/25 | Loss: 0.00138708
Iteration 14/25 | Loss: 0.00138708
Iteration 15/25 | Loss: 0.00138708
Iteration 16/25 | Loss: 0.00138708
Iteration 17/25 | Loss: 0.00138708
Iteration 18/25 | Loss: 0.00138708
Iteration 19/25 | Loss: 0.00138708
Iteration 20/25 | Loss: 0.00138708
Iteration 21/25 | Loss: 0.00138708
Iteration 22/25 | Loss: 0.00138708
Iteration 23/25 | Loss: 0.00138708
Iteration 24/25 | Loss: 0.00138708
Iteration 25/25 | Loss: 0.00138708

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00138708
Iteration 2/1000 | Loss: 0.00003948
Iteration 3/1000 | Loss: 0.00002525
Iteration 4/1000 | Loss: 0.00002232
Iteration 5/1000 | Loss: 0.00002101
Iteration 6/1000 | Loss: 0.00002005
Iteration 7/1000 | Loss: 0.00001946
Iteration 8/1000 | Loss: 0.00001931
Iteration 9/1000 | Loss: 0.00001900
Iteration 10/1000 | Loss: 0.00001899
Iteration 11/1000 | Loss: 0.00001891
Iteration 12/1000 | Loss: 0.00001887
Iteration 13/1000 | Loss: 0.00001867
Iteration 14/1000 | Loss: 0.00001855
Iteration 15/1000 | Loss: 0.00001851
Iteration 16/1000 | Loss: 0.00001848
Iteration 17/1000 | Loss: 0.00001848
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001848
Iteration 20/1000 | Loss: 0.00001848
Iteration 21/1000 | Loss: 0.00001848
Iteration 22/1000 | Loss: 0.00001847
Iteration 23/1000 | Loss: 0.00001847
Iteration 24/1000 | Loss: 0.00001847
Iteration 25/1000 | Loss: 0.00001847
Iteration 26/1000 | Loss: 0.00001847
Iteration 27/1000 | Loss: 0.00001846
Iteration 28/1000 | Loss: 0.00001845
Iteration 29/1000 | Loss: 0.00001844
Iteration 30/1000 | Loss: 0.00001844
Iteration 31/1000 | Loss: 0.00001844
Iteration 32/1000 | Loss: 0.00001844
Iteration 33/1000 | Loss: 0.00001843
Iteration 34/1000 | Loss: 0.00001843
Iteration 35/1000 | Loss: 0.00001842
Iteration 36/1000 | Loss: 0.00001842
Iteration 37/1000 | Loss: 0.00001842
Iteration 38/1000 | Loss: 0.00001841
Iteration 39/1000 | Loss: 0.00001841
Iteration 40/1000 | Loss: 0.00001841
Iteration 41/1000 | Loss: 0.00001841
Iteration 42/1000 | Loss: 0.00001841
Iteration 43/1000 | Loss: 0.00001840
Iteration 44/1000 | Loss: 0.00001840
Iteration 45/1000 | Loss: 0.00001840
Iteration 46/1000 | Loss: 0.00001840
Iteration 47/1000 | Loss: 0.00001839
Iteration 48/1000 | Loss: 0.00001837
Iteration 49/1000 | Loss: 0.00001836
Iteration 50/1000 | Loss: 0.00001836
Iteration 51/1000 | Loss: 0.00001836
Iteration 52/1000 | Loss: 0.00001835
Iteration 53/1000 | Loss: 0.00001835
Iteration 54/1000 | Loss: 0.00001835
Iteration 55/1000 | Loss: 0.00001835
Iteration 56/1000 | Loss: 0.00001835
Iteration 57/1000 | Loss: 0.00001835
Iteration 58/1000 | Loss: 0.00001835
Iteration 59/1000 | Loss: 0.00001835
Iteration 60/1000 | Loss: 0.00001835
Iteration 61/1000 | Loss: 0.00001835
Iteration 62/1000 | Loss: 0.00001835
Iteration 63/1000 | Loss: 0.00001835
Iteration 64/1000 | Loss: 0.00001835
Iteration 65/1000 | Loss: 0.00001835
Iteration 66/1000 | Loss: 0.00001835
Iteration 67/1000 | Loss: 0.00001835
Iteration 68/1000 | Loss: 0.00001835
Iteration 69/1000 | Loss: 0.00001835
Iteration 70/1000 | Loss: 0.00001835
Iteration 71/1000 | Loss: 0.00001835
Iteration 72/1000 | Loss: 0.00001835
Iteration 73/1000 | Loss: 0.00001835
Iteration 74/1000 | Loss: 0.00001835
Iteration 75/1000 | Loss: 0.00001835
Iteration 76/1000 | Loss: 0.00001835
Iteration 77/1000 | Loss: 0.00001835
Iteration 78/1000 | Loss: 0.00001835
Iteration 79/1000 | Loss: 0.00001835
Iteration 80/1000 | Loss: 0.00001835
Iteration 81/1000 | Loss: 0.00001835
Iteration 82/1000 | Loss: 0.00001835
Iteration 83/1000 | Loss: 0.00001835
Iteration 84/1000 | Loss: 0.00001835
Iteration 85/1000 | Loss: 0.00001835
Iteration 86/1000 | Loss: 0.00001835
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 86. Stopping optimization.
Last 5 losses: [1.8353252016822807e-05, 1.8353252016822807e-05, 1.8353252016822807e-05, 1.8353252016822807e-05, 1.8353252016822807e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8353252016822807e-05

Optimization complete. Final v2v error: 3.59720516204834 mm

Highest mean error: 3.6176609992980957 mm for frame 57

Lowest mean error: 3.5622525215148926 mm for frame 42

Saving results

Total time: 27.24329113960266
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1069/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1069.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1069
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00491615
Iteration 2/25 | Loss: 0.00092799
Iteration 3/25 | Loss: 0.00078700
Iteration 4/25 | Loss: 0.00076127
Iteration 5/25 | Loss: 0.00075456
Iteration 6/25 | Loss: 0.00075317
Iteration 7/25 | Loss: 0.00075304
Iteration 8/25 | Loss: 0.00075304
Iteration 9/25 | Loss: 0.00075304
Iteration 10/25 | Loss: 0.00075304
Iteration 11/25 | Loss: 0.00075304
Iteration 12/25 | Loss: 0.00075304
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0007530359434895217, 0.0007530359434895217, 0.0007530359434895217, 0.0007530359434895217, 0.0007530359434895217]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007530359434895217

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.89040852
Iteration 2/25 | Loss: 0.00110336
Iteration 3/25 | Loss: 0.00110335
Iteration 4/25 | Loss: 0.00110335
Iteration 5/25 | Loss: 0.00110335
Iteration 6/25 | Loss: 0.00110335
Iteration 7/25 | Loss: 0.00110335
Iteration 8/25 | Loss: 0.00110335
Iteration 9/25 | Loss: 0.00110335
Iteration 10/25 | Loss: 0.00110335
Iteration 11/25 | Loss: 0.00110335
Iteration 12/25 | Loss: 0.00110335
Iteration 13/25 | Loss: 0.00110335
Iteration 14/25 | Loss: 0.00110335
Iteration 15/25 | Loss: 0.00110335
Iteration 16/25 | Loss: 0.00110335
Iteration 17/25 | Loss: 0.00110335
Iteration 18/25 | Loss: 0.00110335
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.001103346236050129, 0.001103346236050129, 0.001103346236050129, 0.001103346236050129, 0.001103346236050129]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001103346236050129

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110335
Iteration 2/1000 | Loss: 0.00002773
Iteration 3/1000 | Loss: 0.00001668
Iteration 4/1000 | Loss: 0.00001540
Iteration 5/1000 | Loss: 0.00001427
Iteration 6/1000 | Loss: 0.00001390
Iteration 7/1000 | Loss: 0.00001359
Iteration 8/1000 | Loss: 0.00001338
Iteration 9/1000 | Loss: 0.00001320
Iteration 10/1000 | Loss: 0.00001315
Iteration 11/1000 | Loss: 0.00001306
Iteration 12/1000 | Loss: 0.00001303
Iteration 13/1000 | Loss: 0.00001302
Iteration 14/1000 | Loss: 0.00001302
Iteration 15/1000 | Loss: 0.00001300
Iteration 16/1000 | Loss: 0.00001300
Iteration 17/1000 | Loss: 0.00001299
Iteration 18/1000 | Loss: 0.00001298
Iteration 19/1000 | Loss: 0.00001298
Iteration 20/1000 | Loss: 0.00001298
Iteration 21/1000 | Loss: 0.00001298
Iteration 22/1000 | Loss: 0.00001297
Iteration 23/1000 | Loss: 0.00001297
Iteration 24/1000 | Loss: 0.00001297
Iteration 25/1000 | Loss: 0.00001297
Iteration 26/1000 | Loss: 0.00001297
Iteration 27/1000 | Loss: 0.00001296
Iteration 28/1000 | Loss: 0.00001296
Iteration 29/1000 | Loss: 0.00001296
Iteration 30/1000 | Loss: 0.00001296
Iteration 31/1000 | Loss: 0.00001296
Iteration 32/1000 | Loss: 0.00001295
Iteration 33/1000 | Loss: 0.00001295
Iteration 34/1000 | Loss: 0.00001295
Iteration 35/1000 | Loss: 0.00001295
Iteration 36/1000 | Loss: 0.00001294
Iteration 37/1000 | Loss: 0.00001294
Iteration 38/1000 | Loss: 0.00001294
Iteration 39/1000 | Loss: 0.00001294
Iteration 40/1000 | Loss: 0.00001294
Iteration 41/1000 | Loss: 0.00001294
Iteration 42/1000 | Loss: 0.00001294
Iteration 43/1000 | Loss: 0.00001294
Iteration 44/1000 | Loss: 0.00001294
Iteration 45/1000 | Loss: 0.00001293
Iteration 46/1000 | Loss: 0.00001293
Iteration 47/1000 | Loss: 0.00001293
Iteration 48/1000 | Loss: 0.00001293
Iteration 49/1000 | Loss: 0.00001292
Iteration 50/1000 | Loss: 0.00001292
Iteration 51/1000 | Loss: 0.00001292
Iteration 52/1000 | Loss: 0.00001292
Iteration 53/1000 | Loss: 0.00001292
Iteration 54/1000 | Loss: 0.00001291
Iteration 55/1000 | Loss: 0.00001291
Iteration 56/1000 | Loss: 0.00001290
Iteration 57/1000 | Loss: 0.00001290
Iteration 58/1000 | Loss: 0.00001290
Iteration 59/1000 | Loss: 0.00001290
Iteration 60/1000 | Loss: 0.00001289
Iteration 61/1000 | Loss: 0.00001289
Iteration 62/1000 | Loss: 0.00001289
Iteration 63/1000 | Loss: 0.00001289
Iteration 64/1000 | Loss: 0.00001288
Iteration 65/1000 | Loss: 0.00001288
Iteration 66/1000 | Loss: 0.00001288
Iteration 67/1000 | Loss: 0.00001288
Iteration 68/1000 | Loss: 0.00001287
Iteration 69/1000 | Loss: 0.00001287
Iteration 70/1000 | Loss: 0.00001287
Iteration 71/1000 | Loss: 0.00001286
Iteration 72/1000 | Loss: 0.00001286
Iteration 73/1000 | Loss: 0.00001286
Iteration 74/1000 | Loss: 0.00001285
Iteration 75/1000 | Loss: 0.00001285
Iteration 76/1000 | Loss: 0.00001285
Iteration 77/1000 | Loss: 0.00001285
Iteration 78/1000 | Loss: 0.00001284
Iteration 79/1000 | Loss: 0.00001284
Iteration 80/1000 | Loss: 0.00001283
Iteration 81/1000 | Loss: 0.00001282
Iteration 82/1000 | Loss: 0.00001282
Iteration 83/1000 | Loss: 0.00001282
Iteration 84/1000 | Loss: 0.00001281
Iteration 85/1000 | Loss: 0.00001281
Iteration 86/1000 | Loss: 0.00001281
Iteration 87/1000 | Loss: 0.00001281
Iteration 88/1000 | Loss: 0.00001280
Iteration 89/1000 | Loss: 0.00001280
Iteration 90/1000 | Loss: 0.00001280
Iteration 91/1000 | Loss: 0.00001279
Iteration 92/1000 | Loss: 0.00001279
Iteration 93/1000 | Loss: 0.00001279
Iteration 94/1000 | Loss: 0.00001279
Iteration 95/1000 | Loss: 0.00001279
Iteration 96/1000 | Loss: 0.00001279
Iteration 97/1000 | Loss: 0.00001279
Iteration 98/1000 | Loss: 0.00001279
Iteration 99/1000 | Loss: 0.00001279
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.2794325812137686e-05, 1.2794325812137686e-05, 1.2794325812137686e-05, 1.2794325812137686e-05, 1.2794325812137686e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.2794325812137686e-05

Optimization complete. Final v2v error: 3.0595333576202393 mm

Highest mean error: 3.3548097610473633 mm for frame 100

Lowest mean error: 2.7860212326049805 mm for frame 161

Saving results

Total time: 35.36315178871155
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00882294
Iteration 2/25 | Loss: 0.00119333
Iteration 3/25 | Loss: 0.00086274
Iteration 4/25 | Loss: 0.00078633
Iteration 5/25 | Loss: 0.00075433
Iteration 6/25 | Loss: 0.00074110
Iteration 7/25 | Loss: 0.00073882
Iteration 8/25 | Loss: 0.00074092
Iteration 9/25 | Loss: 0.00073924
Iteration 10/25 | Loss: 0.00073816
Iteration 11/25 | Loss: 0.00073805
Iteration 12/25 | Loss: 0.00073805
Iteration 13/25 | Loss: 0.00073805
Iteration 14/25 | Loss: 0.00073805
Iteration 15/25 | Loss: 0.00073805
Iteration 16/25 | Loss: 0.00073805
Iteration 17/25 | Loss: 0.00073805
Iteration 18/25 | Loss: 0.00073805
Iteration 19/25 | Loss: 0.00073805
Iteration 20/25 | Loss: 0.00073805
Iteration 21/25 | Loss: 0.00073805
Iteration 22/25 | Loss: 0.00073805
Iteration 23/25 | Loss: 0.00073805
Iteration 24/25 | Loss: 0.00073805
Iteration 25/25 | Loss: 0.00073805

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.71221066
Iteration 2/25 | Loss: 0.00127457
Iteration 3/25 | Loss: 0.00127455
Iteration 4/25 | Loss: 0.00127454
Iteration 5/25 | Loss: 0.00127454
Iteration 6/25 | Loss: 0.00127454
Iteration 7/25 | Loss: 0.00127454
Iteration 8/25 | Loss: 0.00127454
Iteration 9/25 | Loss: 0.00127454
Iteration 10/25 | Loss: 0.00127454
Iteration 11/25 | Loss: 0.00127454
Iteration 12/25 | Loss: 0.00127454
Iteration 13/25 | Loss: 0.00127454
Iteration 14/25 | Loss: 0.00127454
Iteration 15/25 | Loss: 0.00127454
Iteration 16/25 | Loss: 0.00127454
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.001274542068131268, 0.001274542068131268, 0.001274542068131268, 0.001274542068131268, 0.001274542068131268]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001274542068131268

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00127454
Iteration 2/1000 | Loss: 0.00002275
Iteration 3/1000 | Loss: 0.00001506
Iteration 4/1000 | Loss: 0.00001393
Iteration 5/1000 | Loss: 0.00001316
Iteration 6/1000 | Loss: 0.00001297
Iteration 7/1000 | Loss: 0.00001270
Iteration 8/1000 | Loss: 0.00001265
Iteration 9/1000 | Loss: 0.00001264
Iteration 10/1000 | Loss: 0.00001261
Iteration 11/1000 | Loss: 0.00001250
Iteration 12/1000 | Loss: 0.00001239
Iteration 13/1000 | Loss: 0.00001220
Iteration 14/1000 | Loss: 0.00001219
Iteration 15/1000 | Loss: 0.00001212
Iteration 16/1000 | Loss: 0.00001203
Iteration 17/1000 | Loss: 0.00001200
Iteration 18/1000 | Loss: 0.00001198
Iteration 19/1000 | Loss: 0.00001197
Iteration 20/1000 | Loss: 0.00001197
Iteration 21/1000 | Loss: 0.00001196
Iteration 22/1000 | Loss: 0.00001196
Iteration 23/1000 | Loss: 0.00001195
Iteration 24/1000 | Loss: 0.00001194
Iteration 25/1000 | Loss: 0.00001194
Iteration 26/1000 | Loss: 0.00001193
Iteration 27/1000 | Loss: 0.00001193
Iteration 28/1000 | Loss: 0.00001192
Iteration 29/1000 | Loss: 0.00001192
Iteration 30/1000 | Loss: 0.00001192
Iteration 31/1000 | Loss: 0.00001192
Iteration 32/1000 | Loss: 0.00001192
Iteration 33/1000 | Loss: 0.00001192
Iteration 34/1000 | Loss: 0.00001191
Iteration 35/1000 | Loss: 0.00001191
Iteration 36/1000 | Loss: 0.00001191
Iteration 37/1000 | Loss: 0.00001190
Iteration 38/1000 | Loss: 0.00001190
Iteration 39/1000 | Loss: 0.00001189
Iteration 40/1000 | Loss: 0.00001189
Iteration 41/1000 | Loss: 0.00001188
Iteration 42/1000 | Loss: 0.00001188
Iteration 43/1000 | Loss: 0.00001187
Iteration 44/1000 | Loss: 0.00001187
Iteration 45/1000 | Loss: 0.00001187
Iteration 46/1000 | Loss: 0.00001187
Iteration 47/1000 | Loss: 0.00001186
Iteration 48/1000 | Loss: 0.00001186
Iteration 49/1000 | Loss: 0.00001186
Iteration 50/1000 | Loss: 0.00001186
Iteration 51/1000 | Loss: 0.00001185
Iteration 52/1000 | Loss: 0.00001184
Iteration 53/1000 | Loss: 0.00001184
Iteration 54/1000 | Loss: 0.00001184
Iteration 55/1000 | Loss: 0.00001183
Iteration 56/1000 | Loss: 0.00001183
Iteration 57/1000 | Loss: 0.00001183
Iteration 58/1000 | Loss: 0.00001182
Iteration 59/1000 | Loss: 0.00001182
Iteration 60/1000 | Loss: 0.00001181
Iteration 61/1000 | Loss: 0.00001180
Iteration 62/1000 | Loss: 0.00001180
Iteration 63/1000 | Loss: 0.00001180
Iteration 64/1000 | Loss: 0.00001180
Iteration 65/1000 | Loss: 0.00001180
Iteration 66/1000 | Loss: 0.00001180
Iteration 67/1000 | Loss: 0.00001179
Iteration 68/1000 | Loss: 0.00001179
Iteration 69/1000 | Loss: 0.00001178
Iteration 70/1000 | Loss: 0.00001178
Iteration 71/1000 | Loss: 0.00001176
Iteration 72/1000 | Loss: 0.00001175
Iteration 73/1000 | Loss: 0.00001175
Iteration 74/1000 | Loss: 0.00001174
Iteration 75/1000 | Loss: 0.00001174
Iteration 76/1000 | Loss: 0.00001173
Iteration 77/1000 | Loss: 0.00001173
Iteration 78/1000 | Loss: 0.00001173
Iteration 79/1000 | Loss: 0.00001173
Iteration 80/1000 | Loss: 0.00001173
Iteration 81/1000 | Loss: 0.00001173
Iteration 82/1000 | Loss: 0.00001172
Iteration 83/1000 | Loss: 0.00001172
Iteration 84/1000 | Loss: 0.00001172
Iteration 85/1000 | Loss: 0.00001172
Iteration 86/1000 | Loss: 0.00001172
Iteration 87/1000 | Loss: 0.00001172
Iteration 88/1000 | Loss: 0.00001171
Iteration 89/1000 | Loss: 0.00001171
Iteration 90/1000 | Loss: 0.00001171
Iteration 91/1000 | Loss: 0.00001171
Iteration 92/1000 | Loss: 0.00001171
Iteration 93/1000 | Loss: 0.00001171
Iteration 94/1000 | Loss: 0.00001171
Iteration 95/1000 | Loss: 0.00001171
Iteration 96/1000 | Loss: 0.00001171
Iteration 97/1000 | Loss: 0.00001170
Iteration 98/1000 | Loss: 0.00001170
Iteration 99/1000 | Loss: 0.00001170
Iteration 100/1000 | Loss: 0.00001170
Iteration 101/1000 | Loss: 0.00001170
Iteration 102/1000 | Loss: 0.00001170
Iteration 103/1000 | Loss: 0.00001170
Iteration 104/1000 | Loss: 0.00001170
Iteration 105/1000 | Loss: 0.00001170
Iteration 106/1000 | Loss: 0.00001170
Iteration 107/1000 | Loss: 0.00001170
Iteration 108/1000 | Loss: 0.00001170
Iteration 109/1000 | Loss: 0.00001170
Iteration 110/1000 | Loss: 0.00001170
Iteration 111/1000 | Loss: 0.00001170
Iteration 112/1000 | Loss: 0.00001170
Iteration 113/1000 | Loss: 0.00001170
Iteration 114/1000 | Loss: 0.00001170
Iteration 115/1000 | Loss: 0.00001170
Iteration 116/1000 | Loss: 0.00001170
Iteration 117/1000 | Loss: 0.00001170
Iteration 118/1000 | Loss: 0.00001170
Iteration 119/1000 | Loss: 0.00001170
Iteration 120/1000 | Loss: 0.00001170
Iteration 121/1000 | Loss: 0.00001170
Iteration 122/1000 | Loss: 0.00001170
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 122. Stopping optimization.
Last 5 losses: [1.1700051800289657e-05, 1.1700051800289657e-05, 1.1700051800289657e-05, 1.1700051800289657e-05, 1.1700051800289657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1700051800289657e-05

Optimization complete. Final v2v error: 2.912461757659912 mm

Highest mean error: 3.337437152862549 mm for frame 237

Lowest mean error: 2.621277332305908 mm for frame 148

Saving results

Total time: 47.04406690597534
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1027/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1027.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1027
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00709426
Iteration 2/25 | Loss: 0.00141503
Iteration 3/25 | Loss: 0.00103010
Iteration 4/25 | Loss: 0.00094206
Iteration 5/25 | Loss: 0.00090932
Iteration 6/25 | Loss: 0.00089230
Iteration 7/25 | Loss: 0.00088897
Iteration 8/25 | Loss: 0.00088445
Iteration 9/25 | Loss: 0.00088413
Iteration 10/25 | Loss: 0.00088405
Iteration 11/25 | Loss: 0.00088405
Iteration 12/25 | Loss: 0.00088405
Iteration 13/25 | Loss: 0.00088404
Iteration 14/25 | Loss: 0.00088404
Iteration 15/25 | Loss: 0.00088404
Iteration 16/25 | Loss: 0.00088404
Iteration 17/25 | Loss: 0.00088404
Iteration 18/25 | Loss: 0.00088400
Iteration 19/25 | Loss: 0.00088400
Iteration 20/25 | Loss: 0.00088400
Iteration 21/25 | Loss: 0.00088399
Iteration 22/25 | Loss: 0.00088399
Iteration 23/25 | Loss: 0.00088399
Iteration 24/25 | Loss: 0.00088399
Iteration 25/25 | Loss: 0.00088399

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64594746
Iteration 2/25 | Loss: 0.00189753
Iteration 3/25 | Loss: 0.00189753
Iteration 4/25 | Loss: 0.00189753
Iteration 5/25 | Loss: 0.00189753
Iteration 6/25 | Loss: 0.00189753
Iteration 7/25 | Loss: 0.00189753
Iteration 8/25 | Loss: 0.00189753
Iteration 9/25 | Loss: 0.00189753
Iteration 10/25 | Loss: 0.00189752
Iteration 11/25 | Loss: 0.00189752
Iteration 12/25 | Loss: 0.00189752
Iteration 13/25 | Loss: 0.00189752
Iteration 14/25 | Loss: 0.00189752
Iteration 15/25 | Loss: 0.00189752
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0018975248094648123, 0.0018975248094648123, 0.0018975248094648123, 0.0018975248094648123, 0.0018975248094648123]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0018975248094648123

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00189752
Iteration 2/1000 | Loss: 0.00012140
Iteration 3/1000 | Loss: 0.00010771
Iteration 4/1000 | Loss: 0.00005214
Iteration 5/1000 | Loss: 0.00004445
Iteration 6/1000 | Loss: 0.00004167
Iteration 7/1000 | Loss: 0.00003984
Iteration 8/1000 | Loss: 0.00003882
Iteration 9/1000 | Loss: 0.00003782
Iteration 10/1000 | Loss: 0.00371634
Iteration 11/1000 | Loss: 0.00393973
Iteration 12/1000 | Loss: 0.00313574
Iteration 13/1000 | Loss: 0.00011824
Iteration 14/1000 | Loss: 0.00003572
Iteration 15/1000 | Loss: 0.00003205
Iteration 16/1000 | Loss: 0.00003006
Iteration 17/1000 | Loss: 0.00002906
Iteration 18/1000 | Loss: 0.00002786
Iteration 19/1000 | Loss: 0.00002736
Iteration 20/1000 | Loss: 0.00002675
Iteration 21/1000 | Loss: 0.00002624
Iteration 22/1000 | Loss: 0.00002592
Iteration 23/1000 | Loss: 0.00002565
Iteration 24/1000 | Loss: 0.00002538
Iteration 25/1000 | Loss: 0.00002517
Iteration 26/1000 | Loss: 0.00002513
Iteration 27/1000 | Loss: 0.00002513
Iteration 28/1000 | Loss: 0.00002512
Iteration 29/1000 | Loss: 0.00002511
Iteration 30/1000 | Loss: 0.00002510
Iteration 31/1000 | Loss: 0.00002508
Iteration 32/1000 | Loss: 0.00002507
Iteration 33/1000 | Loss: 0.00002507
Iteration 34/1000 | Loss: 0.00002507
Iteration 35/1000 | Loss: 0.00002506
Iteration 36/1000 | Loss: 0.00002505
Iteration 37/1000 | Loss: 0.00002504
Iteration 38/1000 | Loss: 0.00002502
Iteration 39/1000 | Loss: 0.00002502
Iteration 40/1000 | Loss: 0.00002501
Iteration 41/1000 | Loss: 0.00002500
Iteration 42/1000 | Loss: 0.00002500
Iteration 43/1000 | Loss: 0.00002499
Iteration 44/1000 | Loss: 0.00002499
Iteration 45/1000 | Loss: 0.00002498
Iteration 46/1000 | Loss: 0.00002498
Iteration 47/1000 | Loss: 0.00002498
Iteration 48/1000 | Loss: 0.00002497
Iteration 49/1000 | Loss: 0.00002497
Iteration 50/1000 | Loss: 0.00002497
Iteration 51/1000 | Loss: 0.00002496
Iteration 52/1000 | Loss: 0.00002496
Iteration 53/1000 | Loss: 0.00002496
Iteration 54/1000 | Loss: 0.00002495
Iteration 55/1000 | Loss: 0.00002495
Iteration 56/1000 | Loss: 0.00002495
Iteration 57/1000 | Loss: 0.00002495
Iteration 58/1000 | Loss: 0.00002494
Iteration 59/1000 | Loss: 0.00002494
Iteration 60/1000 | Loss: 0.00002493
Iteration 61/1000 | Loss: 0.00002493
Iteration 62/1000 | Loss: 0.00002493
Iteration 63/1000 | Loss: 0.00002492
Iteration 64/1000 | Loss: 0.00002492
Iteration 65/1000 | Loss: 0.00002491
Iteration 66/1000 | Loss: 0.00002491
Iteration 67/1000 | Loss: 0.00002491
Iteration 68/1000 | Loss: 0.00002490
Iteration 69/1000 | Loss: 0.00002490
Iteration 70/1000 | Loss: 0.00002490
Iteration 71/1000 | Loss: 0.00002490
Iteration 72/1000 | Loss: 0.00002490
Iteration 73/1000 | Loss: 0.00002489
Iteration 74/1000 | Loss: 0.00002489
Iteration 75/1000 | Loss: 0.00002489
Iteration 76/1000 | Loss: 0.00002489
Iteration 77/1000 | Loss: 0.00002489
Iteration 78/1000 | Loss: 0.00002489
Iteration 79/1000 | Loss: 0.00002488
Iteration 80/1000 | Loss: 0.00002488
Iteration 81/1000 | Loss: 0.00002488
Iteration 82/1000 | Loss: 0.00002488
Iteration 83/1000 | Loss: 0.00002488
Iteration 84/1000 | Loss: 0.00002488
Iteration 85/1000 | Loss: 0.00002488
Iteration 86/1000 | Loss: 0.00002488
Iteration 87/1000 | Loss: 0.00002487
Iteration 88/1000 | Loss: 0.00002487
Iteration 89/1000 | Loss: 0.00002487
Iteration 90/1000 | Loss: 0.00002487
Iteration 91/1000 | Loss: 0.00002487
Iteration 92/1000 | Loss: 0.00002487
Iteration 93/1000 | Loss: 0.00002487
Iteration 94/1000 | Loss: 0.00002487
Iteration 95/1000 | Loss: 0.00002486
Iteration 96/1000 | Loss: 0.00002486
Iteration 97/1000 | Loss: 0.00002486
Iteration 98/1000 | Loss: 0.00002486
Iteration 99/1000 | Loss: 0.00002486
Iteration 100/1000 | Loss: 0.00002485
Iteration 101/1000 | Loss: 0.00002485
Iteration 102/1000 | Loss: 0.00002485
Iteration 103/1000 | Loss: 0.00002485
Iteration 104/1000 | Loss: 0.00002485
Iteration 105/1000 | Loss: 0.00002485
Iteration 106/1000 | Loss: 0.00002485
Iteration 107/1000 | Loss: 0.00002485
Iteration 108/1000 | Loss: 0.00002485
Iteration 109/1000 | Loss: 0.00002485
Iteration 110/1000 | Loss: 0.00002485
Iteration 111/1000 | Loss: 0.00002485
Iteration 112/1000 | Loss: 0.00002485
Iteration 113/1000 | Loss: 0.00002485
Iteration 114/1000 | Loss: 0.00002485
Iteration 115/1000 | Loss: 0.00002485
Iteration 116/1000 | Loss: 0.00002485
Iteration 117/1000 | Loss: 0.00002485
Iteration 118/1000 | Loss: 0.00002485
Iteration 119/1000 | Loss: 0.00002485
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 119. Stopping optimization.
Last 5 losses: [2.4845639927661978e-05, 2.4845639927661978e-05, 2.4845639927661978e-05, 2.4845639927661978e-05, 2.4845639927661978e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4845639927661978e-05

Optimization complete. Final v2v error: 4.184129238128662 mm

Highest mean error: 5.015094757080078 mm for frame 118

Lowest mean error: 3.167003631591797 mm for frame 180

Saving results

Total time: 62.20227646827698
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01093181
Iteration 2/25 | Loss: 0.01093181
Iteration 3/25 | Loss: 0.01093181
Iteration 4/25 | Loss: 0.01093181
Iteration 5/25 | Loss: 0.01093181
Iteration 6/25 | Loss: 0.01093181
Iteration 7/25 | Loss: 0.01093181
Iteration 8/25 | Loss: 0.01093181
Iteration 9/25 | Loss: 0.01093180
Iteration 10/25 | Loss: 0.01093180
Iteration 11/25 | Loss: 0.01093180
Iteration 12/25 | Loss: 0.01093180
Iteration 13/25 | Loss: 0.01093180
Iteration 14/25 | Loss: 0.01093180
Iteration 15/25 | Loss: 0.01093180
Iteration 16/25 | Loss: 0.01093180
Iteration 17/25 | Loss: 0.01093180
Iteration 18/25 | Loss: 0.01093180
Iteration 19/25 | Loss: 0.01093180
Iteration 20/25 | Loss: 0.01093180
Iteration 21/25 | Loss: 0.01093180
Iteration 22/25 | Loss: 0.01093180
Iteration 23/25 | Loss: 0.01093180
Iteration 24/25 | Loss: 0.01093180
Iteration 25/25 | Loss: 0.01093179

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.79003751
Iteration 2/25 | Loss: 0.08376437
Iteration 3/25 | Loss: 0.08366511
Iteration 4/25 | Loss: 0.08366280
Iteration 5/25 | Loss: 0.08366279
Iteration 6/25 | Loss: 0.08366279
Iteration 7/25 | Loss: 0.08366279
Iteration 8/25 | Loss: 0.08366278
Iteration 9/25 | Loss: 0.08366278
Iteration 10/25 | Loss: 0.08366277
Iteration 11/25 | Loss: 0.08366277
Iteration 12/25 | Loss: 0.08366277
Iteration 13/25 | Loss: 0.08366277
Iteration 14/25 | Loss: 0.08366277
Iteration 15/25 | Loss: 0.08366277
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.08366277068853378, 0.08366277068853378, 0.08366277068853378, 0.08366277068853378, 0.08366277068853378]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.08366277068853378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.08366277
Iteration 2/1000 | Loss: 0.00228160
Iteration 3/1000 | Loss: 0.00290643
Iteration 4/1000 | Loss: 0.00043021
Iteration 5/1000 | Loss: 0.00106043
Iteration 6/1000 | Loss: 0.00008789
Iteration 7/1000 | Loss: 0.00132683
Iteration 8/1000 | Loss: 0.00097644
Iteration 9/1000 | Loss: 0.00090728
Iteration 10/1000 | Loss: 0.00023020
Iteration 11/1000 | Loss: 0.00053286
Iteration 12/1000 | Loss: 0.00047701
Iteration 13/1000 | Loss: 0.00015976
Iteration 14/1000 | Loss: 0.00073670
Iteration 15/1000 | Loss: 0.00003925
Iteration 16/1000 | Loss: 0.00003279
Iteration 17/1000 | Loss: 0.00012707
Iteration 18/1000 | Loss: 0.00016939
Iteration 19/1000 | Loss: 0.00028441
Iteration 20/1000 | Loss: 0.00002744
Iteration 21/1000 | Loss: 0.00002494
Iteration 22/1000 | Loss: 0.00002332
Iteration 23/1000 | Loss: 0.00004787
Iteration 24/1000 | Loss: 0.00002654
Iteration 25/1000 | Loss: 0.00002018
Iteration 26/1000 | Loss: 0.00007154
Iteration 27/1000 | Loss: 0.00001908
Iteration 28/1000 | Loss: 0.00001832
Iteration 29/1000 | Loss: 0.00006856
Iteration 30/1000 | Loss: 0.00009705
Iteration 31/1000 | Loss: 0.00002556
Iteration 32/1000 | Loss: 0.00001895
Iteration 33/1000 | Loss: 0.00001744
Iteration 34/1000 | Loss: 0.00001677
Iteration 35/1000 | Loss: 0.00001626
Iteration 36/1000 | Loss: 0.00001590
Iteration 37/1000 | Loss: 0.00003935
Iteration 38/1000 | Loss: 0.00001555
Iteration 39/1000 | Loss: 0.00001538
Iteration 40/1000 | Loss: 0.00001526
Iteration 41/1000 | Loss: 0.00001520
Iteration 42/1000 | Loss: 0.00001519
Iteration 43/1000 | Loss: 0.00001519
Iteration 44/1000 | Loss: 0.00001518
Iteration 45/1000 | Loss: 0.00001517
Iteration 46/1000 | Loss: 0.00001516
Iteration 47/1000 | Loss: 0.00001515
Iteration 48/1000 | Loss: 0.00001514
Iteration 49/1000 | Loss: 0.00001513
Iteration 50/1000 | Loss: 0.00001532
Iteration 51/1000 | Loss: 0.00001532
Iteration 52/1000 | Loss: 0.00001554
Iteration 53/1000 | Loss: 0.00001565
Iteration 54/1000 | Loss: 0.00008531
Iteration 55/1000 | Loss: 0.00001547
Iteration 56/1000 | Loss: 0.00001543
Iteration 57/1000 | Loss: 0.00001562
Iteration 58/1000 | Loss: 0.00001579
Iteration 59/1000 | Loss: 0.00001529
Iteration 60/1000 | Loss: 0.00001508
Iteration 61/1000 | Loss: 0.00001508
Iteration 62/1000 | Loss: 0.00001530
Iteration 63/1000 | Loss: 0.00001511
Iteration 64/1000 | Loss: 0.00001511
Iteration 65/1000 | Loss: 0.00001521
Iteration 66/1000 | Loss: 0.00001520
Iteration 67/1000 | Loss: 0.00001520
Iteration 68/1000 | Loss: 0.00001520
Iteration 69/1000 | Loss: 0.00001519
Iteration 70/1000 | Loss: 0.00001519
Iteration 71/1000 | Loss: 0.00001518
Iteration 72/1000 | Loss: 0.00001517
Iteration 73/1000 | Loss: 0.00001573
Iteration 74/1000 | Loss: 0.00001562
Iteration 75/1000 | Loss: 0.00001544
Iteration 76/1000 | Loss: 0.00001501
Iteration 77/1000 | Loss: 0.00001501
Iteration 78/1000 | Loss: 0.00001567
Iteration 79/1000 | Loss: 0.00001528
Iteration 80/1000 | Loss: 0.00001535
Iteration 81/1000 | Loss: 0.00001529
Iteration 82/1000 | Loss: 0.00001521
Iteration 83/1000 | Loss: 0.00001535
Iteration 84/1000 | Loss: 0.00001534
Iteration 85/1000 | Loss: 0.00001527
Iteration 86/1000 | Loss: 0.00001534
Iteration 87/1000 | Loss: 0.00001519
Iteration 88/1000 | Loss: 0.00001501
Iteration 89/1000 | Loss: 0.00001501
Iteration 90/1000 | Loss: 0.00001500
Iteration 91/1000 | Loss: 0.00001509
Iteration 92/1000 | Loss: 0.00001539
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00012560
Iteration 95/1000 | Loss: 0.00001691
Iteration 96/1000 | Loss: 0.00001543
Iteration 97/1000 | Loss: 0.00001547
Iteration 98/1000 | Loss: 0.00001545
Iteration 99/1000 | Loss: 0.00008748
Iteration 100/1000 | Loss: 0.00001543
Iteration 101/1000 | Loss: 0.00003949
Iteration 102/1000 | Loss: 0.00001530
Iteration 103/1000 | Loss: 0.00003618
Iteration 104/1000 | Loss: 0.00001603
Iteration 105/1000 | Loss: 0.00001778
Iteration 106/1000 | Loss: 0.00001505
Iteration 107/1000 | Loss: 0.00001943
Iteration 108/1000 | Loss: 0.00001963
Iteration 109/1000 | Loss: 0.00001500
Iteration 110/1000 | Loss: 0.00001500
Iteration 111/1000 | Loss: 0.00001498
Iteration 112/1000 | Loss: 0.00001498
Iteration 113/1000 | Loss: 0.00001493
Iteration 114/1000 | Loss: 0.00001488
Iteration 115/1000 | Loss: 0.00001488
Iteration 116/1000 | Loss: 0.00001486
Iteration 117/1000 | Loss: 0.00001486
Iteration 118/1000 | Loss: 0.00001486
Iteration 119/1000 | Loss: 0.00001486
Iteration 120/1000 | Loss: 0.00001485
Iteration 121/1000 | Loss: 0.00001542
Iteration 122/1000 | Loss: 0.00001540
Iteration 123/1000 | Loss: 0.00009194
Iteration 124/1000 | Loss: 0.00001705
Iteration 125/1000 | Loss: 0.00001539
Iteration 126/1000 | Loss: 0.00003905
Iteration 127/1000 | Loss: 0.00001544
Iteration 128/1000 | Loss: 0.00001497
Iteration 129/1000 | Loss: 0.00001496
Iteration 130/1000 | Loss: 0.00001495
Iteration 131/1000 | Loss: 0.00001495
Iteration 132/1000 | Loss: 0.00001534
Iteration 133/1000 | Loss: 0.00001520
Iteration 134/1000 | Loss: 0.00001502
Iteration 135/1000 | Loss: 0.00001501
Iteration 136/1000 | Loss: 0.00001501
Iteration 137/1000 | Loss: 0.00001501
Iteration 138/1000 | Loss: 0.00001500
Iteration 139/1000 | Loss: 0.00001500
Iteration 140/1000 | Loss: 0.00001496
Iteration 141/1000 | Loss: 0.00001493
Iteration 142/1000 | Loss: 0.00001492
Iteration 143/1000 | Loss: 0.00006955
Iteration 144/1000 | Loss: 0.00001504
Iteration 145/1000 | Loss: 0.00001543
Iteration 146/1000 | Loss: 0.00001537
Iteration 147/1000 | Loss: 0.00001496
Iteration 148/1000 | Loss: 0.00001495
Iteration 149/1000 | Loss: 0.00001525
Iteration 150/1000 | Loss: 0.00002993
Iteration 151/1000 | Loss: 0.00001706
Iteration 152/1000 | Loss: 0.00001507
Iteration 153/1000 | Loss: 0.00001509
Iteration 154/1000 | Loss: 0.00001489
Iteration 155/1000 | Loss: 0.00001489
Iteration 156/1000 | Loss: 0.00001488
Iteration 157/1000 | Loss: 0.00001488
Iteration 158/1000 | Loss: 0.00001487
Iteration 159/1000 | Loss: 0.00001487
Iteration 160/1000 | Loss: 0.00001487
Iteration 161/1000 | Loss: 0.00001487
Iteration 162/1000 | Loss: 0.00001487
Iteration 163/1000 | Loss: 0.00001528
Iteration 164/1000 | Loss: 0.00004657
Iteration 165/1000 | Loss: 0.00001920
Iteration 166/1000 | Loss: 0.00001543
Iteration 167/1000 | Loss: 0.00001542
Iteration 168/1000 | Loss: 0.00001531
Iteration 169/1000 | Loss: 0.00001506
Iteration 170/1000 | Loss: 0.00001492
Iteration 171/1000 | Loss: 0.00001525
Iteration 172/1000 | Loss: 0.00004774
Iteration 173/1000 | Loss: 0.00001604
Iteration 174/1000 | Loss: 0.00001533
Iteration 175/1000 | Loss: 0.00001532
Iteration 176/1000 | Loss: 0.00001549
Iteration 177/1000 | Loss: 0.00001516
Iteration 178/1000 | Loss: 0.00001516
Iteration 179/1000 | Loss: 0.00002451
Iteration 180/1000 | Loss: 0.00001583
Iteration 181/1000 | Loss: 0.00001544
Iteration 182/1000 | Loss: 0.00001550
Iteration 183/1000 | Loss: 0.00001537
Iteration 184/1000 | Loss: 0.00001566
Iteration 185/1000 | Loss: 0.00001504
Iteration 186/1000 | Loss: 0.00001510
Iteration 187/1000 | Loss: 0.00001528
Iteration 188/1000 | Loss: 0.00001861
Iteration 189/1000 | Loss: 0.00002040
Iteration 190/1000 | Loss: 0.00001538
Iteration 191/1000 | Loss: 0.00001566
Iteration 192/1000 | Loss: 0.00001489
Iteration 193/1000 | Loss: 0.00001489
Iteration 194/1000 | Loss: 0.00001489
Iteration 195/1000 | Loss: 0.00001523
Iteration 196/1000 | Loss: 0.00001523
Iteration 197/1000 | Loss: 0.00002106
Iteration 198/1000 | Loss: 0.00002162
Iteration 199/1000 | Loss: 0.00002614
Iteration 200/1000 | Loss: 0.00002026
Iteration 201/1000 | Loss: 0.00001600
Iteration 202/1000 | Loss: 0.00001773
Iteration 203/1000 | Loss: 0.00001956
Iteration 204/1000 | Loss: 0.00001626
Iteration 205/1000 | Loss: 0.00001614
Iteration 206/1000 | Loss: 0.00001599
Iteration 207/1000 | Loss: 0.00001551
Iteration 208/1000 | Loss: 0.00001569
Iteration 209/1000 | Loss: 0.00001687
Iteration 210/1000 | Loss: 0.00001615
Iteration 211/1000 | Loss: 0.00001633
Iteration 212/1000 | Loss: 0.00001565
Iteration 213/1000 | Loss: 0.00001553
Iteration 214/1000 | Loss: 0.00001577
Iteration 215/1000 | Loss: 0.00001508
Iteration 216/1000 | Loss: 0.00001511
Iteration 217/1000 | Loss: 0.00001840
Iteration 218/1000 | Loss: 0.00001565
Iteration 219/1000 | Loss: 0.00001516
Iteration 220/1000 | Loss: 0.00001552
Iteration 221/1000 | Loss: 0.00001801
Iteration 222/1000 | Loss: 0.00001590
Iteration 223/1000 | Loss: 0.00001556
Iteration 224/1000 | Loss: 0.00001562
Iteration 225/1000 | Loss: 0.00001681
Iteration 226/1000 | Loss: 0.00001685
Iteration 227/1000 | Loss: 0.00002271
Iteration 228/1000 | Loss: 0.00006824
Iteration 229/1000 | Loss: 0.00002158
Iteration 230/1000 | Loss: 0.00001702
Iteration 231/1000 | Loss: 0.00006296
Iteration 232/1000 | Loss: 0.00001518
Iteration 233/1000 | Loss: 0.00001542
Iteration 234/1000 | Loss: 0.00001545
Iteration 235/1000 | Loss: 0.00001501
Iteration 236/1000 | Loss: 0.00003919
Iteration 237/1000 | Loss: 0.00001495
Iteration 238/1000 | Loss: 0.00003255
Iteration 239/1000 | Loss: 0.00002344
Iteration 240/1000 | Loss: 0.00006779
Iteration 241/1000 | Loss: 0.00003865
Iteration 242/1000 | Loss: 0.00003481
Iteration 243/1000 | Loss: 0.00009589
Iteration 244/1000 | Loss: 0.00001490
Iteration 245/1000 | Loss: 0.00003316
Iteration 246/1000 | Loss: 0.00001492
Iteration 247/1000 | Loss: 0.00001489
Iteration 248/1000 | Loss: 0.00001487
Iteration 249/1000 | Loss: 0.00001486
Iteration 250/1000 | Loss: 0.00001486
Iteration 251/1000 | Loss: 0.00001485
Iteration 252/1000 | Loss: 0.00001485
Iteration 253/1000 | Loss: 0.00001486
Iteration 254/1000 | Loss: 0.00001485
Iteration 255/1000 | Loss: 0.00001484
Iteration 256/1000 | Loss: 0.00001483
Iteration 257/1000 | Loss: 0.00001483
Iteration 258/1000 | Loss: 0.00001483
Iteration 259/1000 | Loss: 0.00001483
Iteration 260/1000 | Loss: 0.00001483
Iteration 261/1000 | Loss: 0.00001483
Iteration 262/1000 | Loss: 0.00001482
Iteration 263/1000 | Loss: 0.00001482
Iteration 264/1000 | Loss: 0.00001482
Iteration 265/1000 | Loss: 0.00001481
Iteration 266/1000 | Loss: 0.00001533
Iteration 267/1000 | Loss: 0.00004703
Iteration 268/1000 | Loss: 0.00002515
Iteration 269/1000 | Loss: 0.00001483
Iteration 270/1000 | Loss: 0.00001483
Iteration 271/1000 | Loss: 0.00001483
Iteration 272/1000 | Loss: 0.00001483
Iteration 273/1000 | Loss: 0.00001482
Iteration 274/1000 | Loss: 0.00001504
Iteration 275/1000 | Loss: 0.00001484
Iteration 276/1000 | Loss: 0.00001483
Iteration 277/1000 | Loss: 0.00001482
Iteration 278/1000 | Loss: 0.00001481
Iteration 279/1000 | Loss: 0.00001481
Iteration 280/1000 | Loss: 0.00001481
Iteration 281/1000 | Loss: 0.00001481
Iteration 282/1000 | Loss: 0.00001480
Iteration 283/1000 | Loss: 0.00001480
Iteration 284/1000 | Loss: 0.00001480
Iteration 285/1000 | Loss: 0.00001480
Iteration 286/1000 | Loss: 0.00001480
Iteration 287/1000 | Loss: 0.00001480
Iteration 288/1000 | Loss: 0.00001480
Iteration 289/1000 | Loss: 0.00001480
Iteration 290/1000 | Loss: 0.00001479
Iteration 291/1000 | Loss: 0.00001479
Iteration 292/1000 | Loss: 0.00001530
Iteration 293/1000 | Loss: 0.00005296
Iteration 294/1000 | Loss: 0.00001614
Iteration 295/1000 | Loss: 0.00001490
Iteration 296/1000 | Loss: 0.00001490
Iteration 297/1000 | Loss: 0.00001490
Iteration 298/1000 | Loss: 0.00001490
Iteration 299/1000 | Loss: 0.00001490
Iteration 300/1000 | Loss: 0.00001489
Iteration 301/1000 | Loss: 0.00001489
Iteration 302/1000 | Loss: 0.00001489
Iteration 303/1000 | Loss: 0.00001489
Iteration 304/1000 | Loss: 0.00001489
Iteration 305/1000 | Loss: 0.00001489
Iteration 306/1000 | Loss: 0.00001489
Iteration 307/1000 | Loss: 0.00001489
Iteration 308/1000 | Loss: 0.00001489
Iteration 309/1000 | Loss: 0.00001489
Iteration 310/1000 | Loss: 0.00001489
Iteration 311/1000 | Loss: 0.00001489
Iteration 312/1000 | Loss: 0.00001489
Iteration 313/1000 | Loss: 0.00001489
Iteration 314/1000 | Loss: 0.00001489
Iteration 315/1000 | Loss: 0.00001489
Iteration 316/1000 | Loss: 0.00001489
Iteration 317/1000 | Loss: 0.00001489
Iteration 318/1000 | Loss: 0.00001489
Iteration 319/1000 | Loss: 0.00001489
Iteration 320/1000 | Loss: 0.00001489
Iteration 321/1000 | Loss: 0.00001489
Iteration 322/1000 | Loss: 0.00001489
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 322. Stopping optimization.
Last 5 losses: [1.4889931662764866e-05, 1.4889931662764866e-05, 1.4889931662764866e-05, 1.4889931662764866e-05, 1.4889931662764866e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4889931662764866e-05

Optimization complete. Final v2v error: 3.116849660873413 mm

Highest mean error: 9.916143417358398 mm for frame 181

Lowest mean error: 2.60174560546875 mm for frame 237

Saving results

Total time: 272.8536126613617
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1086/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1086.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1086
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00521848
Iteration 2/25 | Loss: 0.00095340
Iteration 3/25 | Loss: 0.00083475
Iteration 4/25 | Loss: 0.00081226
Iteration 5/25 | Loss: 0.00080519
Iteration 6/25 | Loss: 0.00080368
Iteration 7/25 | Loss: 0.00080364
Iteration 8/25 | Loss: 0.00080364
Iteration 9/25 | Loss: 0.00080364
Iteration 10/25 | Loss: 0.00080364
Iteration 11/25 | Loss: 0.00080364
Iteration 12/25 | Loss: 0.00080364
Iteration 13/25 | Loss: 0.00080364
Iteration 14/25 | Loss: 0.00080364
Iteration 15/25 | Loss: 0.00080364
Iteration 16/25 | Loss: 0.00080364
Iteration 17/25 | Loss: 0.00080364
Iteration 18/25 | Loss: 0.00080364
Iteration 19/25 | Loss: 0.00080364
Iteration 20/25 | Loss: 0.00080364
Iteration 21/25 | Loss: 0.00080364
Iteration 22/25 | Loss: 0.00080364
Iteration 23/25 | Loss: 0.00080364
Iteration 24/25 | Loss: 0.00080364
Iteration 25/25 | Loss: 0.00080364

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.58533108
Iteration 2/25 | Loss: 0.00108175
Iteration 3/25 | Loss: 0.00108171
Iteration 4/25 | Loss: 0.00108171
Iteration 5/25 | Loss: 0.00108171
Iteration 6/25 | Loss: 0.00108171
Iteration 7/25 | Loss: 0.00108171
Iteration 8/25 | Loss: 0.00108171
Iteration 9/25 | Loss: 0.00108171
Iteration 10/25 | Loss: 0.00108171
Iteration 11/25 | Loss: 0.00108171
Iteration 12/25 | Loss: 0.00108171
Iteration 13/25 | Loss: 0.00108171
Iteration 14/25 | Loss: 0.00108171
Iteration 15/25 | Loss: 0.00108171
Iteration 16/25 | Loss: 0.00108171
Iteration 17/25 | Loss: 0.00108171
Iteration 18/25 | Loss: 0.00108171
Iteration 19/25 | Loss: 0.00108171
Iteration 20/25 | Loss: 0.00108171
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0010817054426297545, 0.0010817054426297545, 0.0010817054426297545, 0.0010817054426297545, 0.0010817054426297545]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010817054426297545

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00108171
Iteration 2/1000 | Loss: 0.00003363
Iteration 3/1000 | Loss: 0.00002516
Iteration 4/1000 | Loss: 0.00002284
Iteration 5/1000 | Loss: 0.00002118
Iteration 6/1000 | Loss: 0.00002041
Iteration 7/1000 | Loss: 0.00001981
Iteration 8/1000 | Loss: 0.00001933
Iteration 9/1000 | Loss: 0.00001901
Iteration 10/1000 | Loss: 0.00001871
Iteration 11/1000 | Loss: 0.00001846
Iteration 12/1000 | Loss: 0.00001842
Iteration 13/1000 | Loss: 0.00001832
Iteration 14/1000 | Loss: 0.00001826
Iteration 15/1000 | Loss: 0.00001825
Iteration 16/1000 | Loss: 0.00001822
Iteration 17/1000 | Loss: 0.00001817
Iteration 18/1000 | Loss: 0.00001817
Iteration 19/1000 | Loss: 0.00001812
Iteration 20/1000 | Loss: 0.00001812
Iteration 21/1000 | Loss: 0.00001811
Iteration 22/1000 | Loss: 0.00001811
Iteration 23/1000 | Loss: 0.00001811
Iteration 24/1000 | Loss: 0.00001811
Iteration 25/1000 | Loss: 0.00001811
Iteration 26/1000 | Loss: 0.00001810
Iteration 27/1000 | Loss: 0.00001810
Iteration 28/1000 | Loss: 0.00001810
Iteration 29/1000 | Loss: 0.00001810
Iteration 30/1000 | Loss: 0.00001809
Iteration 31/1000 | Loss: 0.00001808
Iteration 32/1000 | Loss: 0.00001808
Iteration 33/1000 | Loss: 0.00001807
Iteration 34/1000 | Loss: 0.00001807
Iteration 35/1000 | Loss: 0.00001806
Iteration 36/1000 | Loss: 0.00001806
Iteration 37/1000 | Loss: 0.00001806
Iteration 38/1000 | Loss: 0.00001805
Iteration 39/1000 | Loss: 0.00001805
Iteration 40/1000 | Loss: 0.00001805
Iteration 41/1000 | Loss: 0.00001804
Iteration 42/1000 | Loss: 0.00001804
Iteration 43/1000 | Loss: 0.00001804
Iteration 44/1000 | Loss: 0.00001803
Iteration 45/1000 | Loss: 0.00001803
Iteration 46/1000 | Loss: 0.00001803
Iteration 47/1000 | Loss: 0.00001802
Iteration 48/1000 | Loss: 0.00001802
Iteration 49/1000 | Loss: 0.00001802
Iteration 50/1000 | Loss: 0.00001802
Iteration 51/1000 | Loss: 0.00001802
Iteration 52/1000 | Loss: 0.00001802
Iteration 53/1000 | Loss: 0.00001802
Iteration 54/1000 | Loss: 0.00001802
Iteration 55/1000 | Loss: 0.00001802
Iteration 56/1000 | Loss: 0.00001802
Iteration 57/1000 | Loss: 0.00001802
Iteration 58/1000 | Loss: 0.00001801
Iteration 59/1000 | Loss: 0.00001801
Iteration 60/1000 | Loss: 0.00001800
Iteration 61/1000 | Loss: 0.00001800
Iteration 62/1000 | Loss: 0.00001800
Iteration 63/1000 | Loss: 0.00001799
Iteration 64/1000 | Loss: 0.00001799
Iteration 65/1000 | Loss: 0.00001799
Iteration 66/1000 | Loss: 0.00001799
Iteration 67/1000 | Loss: 0.00001798
Iteration 68/1000 | Loss: 0.00001798
Iteration 69/1000 | Loss: 0.00001798
Iteration 70/1000 | Loss: 0.00001797
Iteration 71/1000 | Loss: 0.00001797
Iteration 72/1000 | Loss: 0.00001797
Iteration 73/1000 | Loss: 0.00001797
Iteration 74/1000 | Loss: 0.00001797
Iteration 75/1000 | Loss: 0.00001797
Iteration 76/1000 | Loss: 0.00001797
Iteration 77/1000 | Loss: 0.00001797
Iteration 78/1000 | Loss: 0.00001797
Iteration 79/1000 | Loss: 0.00001796
Iteration 80/1000 | Loss: 0.00001796
Iteration 81/1000 | Loss: 0.00001796
Iteration 82/1000 | Loss: 0.00001796
Iteration 83/1000 | Loss: 0.00001796
Iteration 84/1000 | Loss: 0.00001796
Iteration 85/1000 | Loss: 0.00001795
Iteration 86/1000 | Loss: 0.00001795
Iteration 87/1000 | Loss: 0.00001795
Iteration 88/1000 | Loss: 0.00001795
Iteration 89/1000 | Loss: 0.00001795
Iteration 90/1000 | Loss: 0.00001795
Iteration 91/1000 | Loss: 0.00001795
Iteration 92/1000 | Loss: 0.00001794
Iteration 93/1000 | Loss: 0.00001794
Iteration 94/1000 | Loss: 0.00001794
Iteration 95/1000 | Loss: 0.00001794
Iteration 96/1000 | Loss: 0.00001794
Iteration 97/1000 | Loss: 0.00001794
Iteration 98/1000 | Loss: 0.00001794
Iteration 99/1000 | Loss: 0.00001793
Iteration 100/1000 | Loss: 0.00001793
Iteration 101/1000 | Loss: 0.00001793
Iteration 102/1000 | Loss: 0.00001793
Iteration 103/1000 | Loss: 0.00001792
Iteration 104/1000 | Loss: 0.00001792
Iteration 105/1000 | Loss: 0.00001792
Iteration 106/1000 | Loss: 0.00001791
Iteration 107/1000 | Loss: 0.00001791
Iteration 108/1000 | Loss: 0.00001791
Iteration 109/1000 | Loss: 0.00001791
Iteration 110/1000 | Loss: 0.00001790
Iteration 111/1000 | Loss: 0.00001790
Iteration 112/1000 | Loss: 0.00001790
Iteration 113/1000 | Loss: 0.00001790
Iteration 114/1000 | Loss: 0.00001790
Iteration 115/1000 | Loss: 0.00001790
Iteration 116/1000 | Loss: 0.00001789
Iteration 117/1000 | Loss: 0.00001789
Iteration 118/1000 | Loss: 0.00001789
Iteration 119/1000 | Loss: 0.00001789
Iteration 120/1000 | Loss: 0.00001789
Iteration 121/1000 | Loss: 0.00001788
Iteration 122/1000 | Loss: 0.00001788
Iteration 123/1000 | Loss: 0.00001788
Iteration 124/1000 | Loss: 0.00001788
Iteration 125/1000 | Loss: 0.00001788
Iteration 126/1000 | Loss: 0.00001788
Iteration 127/1000 | Loss: 0.00001788
Iteration 128/1000 | Loss: 0.00001788
Iteration 129/1000 | Loss: 0.00001788
Iteration 130/1000 | Loss: 0.00001787
Iteration 131/1000 | Loss: 0.00001787
Iteration 132/1000 | Loss: 0.00001787
Iteration 133/1000 | Loss: 0.00001786
Iteration 134/1000 | Loss: 0.00001786
Iteration 135/1000 | Loss: 0.00001786
Iteration 136/1000 | Loss: 0.00001786
Iteration 137/1000 | Loss: 0.00001786
Iteration 138/1000 | Loss: 0.00001786
Iteration 139/1000 | Loss: 0.00001786
Iteration 140/1000 | Loss: 0.00001786
Iteration 141/1000 | Loss: 0.00001786
Iteration 142/1000 | Loss: 0.00001786
Iteration 143/1000 | Loss: 0.00001786
Iteration 144/1000 | Loss: 0.00001786
Iteration 145/1000 | Loss: 0.00001786
Iteration 146/1000 | Loss: 0.00001786
Iteration 147/1000 | Loss: 0.00001786
Iteration 148/1000 | Loss: 0.00001786
Iteration 149/1000 | Loss: 0.00001786
Iteration 150/1000 | Loss: 0.00001786
Iteration 151/1000 | Loss: 0.00001785
Iteration 152/1000 | Loss: 0.00001785
Iteration 153/1000 | Loss: 0.00001785
Iteration 154/1000 | Loss: 0.00001785
Iteration 155/1000 | Loss: 0.00001785
Iteration 156/1000 | Loss: 0.00001785
Iteration 157/1000 | Loss: 0.00001785
Iteration 158/1000 | Loss: 0.00001785
Iteration 159/1000 | Loss: 0.00001785
Iteration 160/1000 | Loss: 0.00001785
Iteration 161/1000 | Loss: 0.00001785
Iteration 162/1000 | Loss: 0.00001785
Iteration 163/1000 | Loss: 0.00001785
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [1.7853664758149534e-05, 1.7853664758149534e-05, 1.7853664758149534e-05, 1.7853664758149534e-05, 1.7853664758149534e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7853664758149534e-05

Optimization complete. Final v2v error: 3.522756814956665 mm

Highest mean error: 4.437184810638428 mm for frame 158

Lowest mean error: 2.9769036769866943 mm for frame 8

Saving results

Total time: 43.40510272979736
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00858801
Iteration 2/25 | Loss: 0.00123562
Iteration 3/25 | Loss: 0.00091117
Iteration 4/25 | Loss: 0.00087212
Iteration 5/25 | Loss: 0.00086457
Iteration 6/25 | Loss: 0.00086316
Iteration 7/25 | Loss: 0.00086316
Iteration 8/25 | Loss: 0.00086316
Iteration 9/25 | Loss: 0.00086316
Iteration 10/25 | Loss: 0.00086316
Iteration 11/25 | Loss: 0.00086316
Iteration 12/25 | Loss: 0.00086316
Iteration 13/25 | Loss: 0.00086316
Iteration 14/25 | Loss: 0.00086316
Iteration 15/25 | Loss: 0.00086316
Iteration 16/25 | Loss: 0.00086316
Iteration 17/25 | Loss: 0.00086316
Iteration 18/25 | Loss: 0.00086316
Iteration 19/25 | Loss: 0.00086316
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0008631598902866244, 0.0008631598902866244, 0.0008631598902866244, 0.0008631598902866244, 0.0008631598902866244]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008631598902866244

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14023387
Iteration 2/25 | Loss: 0.00105518
Iteration 3/25 | Loss: 0.00105517
Iteration 4/25 | Loss: 0.00105517
Iteration 5/25 | Loss: 0.00105517
Iteration 6/25 | Loss: 0.00105517
Iteration 7/25 | Loss: 0.00105517
Iteration 8/25 | Loss: 0.00105517
Iteration 9/25 | Loss: 0.00105517
Iteration 10/25 | Loss: 0.00105517
Iteration 11/25 | Loss: 0.00105517
Iteration 12/25 | Loss: 0.00105517
Iteration 13/25 | Loss: 0.00105517
Iteration 14/25 | Loss: 0.00105517
Iteration 15/25 | Loss: 0.00105517
Iteration 16/25 | Loss: 0.00105517
Iteration 17/25 | Loss: 0.00105517
Iteration 18/25 | Loss: 0.00105517
Iteration 19/25 | Loss: 0.00105517
Iteration 20/25 | Loss: 0.00105517
Iteration 21/25 | Loss: 0.00105517
Iteration 22/25 | Loss: 0.00105517
Iteration 23/25 | Loss: 0.00105517
Iteration 24/25 | Loss: 0.00105517
Iteration 25/25 | Loss: 0.00105517

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105517
Iteration 2/1000 | Loss: 0.00003110
Iteration 3/1000 | Loss: 0.00002528
Iteration 4/1000 | Loss: 0.00002378
Iteration 5/1000 | Loss: 0.00002283
Iteration 6/1000 | Loss: 0.00002220
Iteration 7/1000 | Loss: 0.00002161
Iteration 8/1000 | Loss: 0.00002133
Iteration 9/1000 | Loss: 0.00002118
Iteration 10/1000 | Loss: 0.00002104
Iteration 11/1000 | Loss: 0.00002100
Iteration 12/1000 | Loss: 0.00002098
Iteration 13/1000 | Loss: 0.00002097
Iteration 14/1000 | Loss: 0.00002097
Iteration 15/1000 | Loss: 0.00002091
Iteration 16/1000 | Loss: 0.00002086
Iteration 17/1000 | Loss: 0.00002086
Iteration 18/1000 | Loss: 0.00002086
Iteration 19/1000 | Loss: 0.00002085
Iteration 20/1000 | Loss: 0.00002085
Iteration 21/1000 | Loss: 0.00002085
Iteration 22/1000 | Loss: 0.00002085
Iteration 23/1000 | Loss: 0.00002085
Iteration 24/1000 | Loss: 0.00002085
Iteration 25/1000 | Loss: 0.00002085
Iteration 26/1000 | Loss: 0.00002084
Iteration 27/1000 | Loss: 0.00002084
Iteration 28/1000 | Loss: 0.00002084
Iteration 29/1000 | Loss: 0.00002084
Iteration 30/1000 | Loss: 0.00002084
Iteration 31/1000 | Loss: 0.00002083
Iteration 32/1000 | Loss: 0.00002083
Iteration 33/1000 | Loss: 0.00002083
Iteration 34/1000 | Loss: 0.00002083
Iteration 35/1000 | Loss: 0.00002083
Iteration 36/1000 | Loss: 0.00002083
Iteration 37/1000 | Loss: 0.00002083
Iteration 38/1000 | Loss: 0.00002083
Iteration 39/1000 | Loss: 0.00002082
Iteration 40/1000 | Loss: 0.00002082
Iteration 41/1000 | Loss: 0.00002082
Iteration 42/1000 | Loss: 0.00002082
Iteration 43/1000 | Loss: 0.00002082
Iteration 44/1000 | Loss: 0.00002080
Iteration 45/1000 | Loss: 0.00002080
Iteration 46/1000 | Loss: 0.00002078
Iteration 47/1000 | Loss: 0.00002078
Iteration 48/1000 | Loss: 0.00002078
Iteration 49/1000 | Loss: 0.00002078
Iteration 50/1000 | Loss: 0.00002078
Iteration 51/1000 | Loss: 0.00002078
Iteration 52/1000 | Loss: 0.00002078
Iteration 53/1000 | Loss: 0.00002077
Iteration 54/1000 | Loss: 0.00002077
Iteration 55/1000 | Loss: 0.00002077
Iteration 56/1000 | Loss: 0.00002077
Iteration 57/1000 | Loss: 0.00002077
Iteration 58/1000 | Loss: 0.00002077
Iteration 59/1000 | Loss: 0.00002076
Iteration 60/1000 | Loss: 0.00002076
Iteration 61/1000 | Loss: 0.00002076
Iteration 62/1000 | Loss: 0.00002076
Iteration 63/1000 | Loss: 0.00002076
Iteration 64/1000 | Loss: 0.00002076
Iteration 65/1000 | Loss: 0.00002076
Iteration 66/1000 | Loss: 0.00002075
Iteration 67/1000 | Loss: 0.00002075
Iteration 68/1000 | Loss: 0.00002075
Iteration 69/1000 | Loss: 0.00002075
Iteration 70/1000 | Loss: 0.00002075
Iteration 71/1000 | Loss: 0.00002075
Iteration 72/1000 | Loss: 0.00002075
Iteration 73/1000 | Loss: 0.00002075
Iteration 74/1000 | Loss: 0.00002075
Iteration 75/1000 | Loss: 0.00002075
Iteration 76/1000 | Loss: 0.00002075
Iteration 77/1000 | Loss: 0.00002074
Iteration 78/1000 | Loss: 0.00002073
Iteration 79/1000 | Loss: 0.00002073
Iteration 80/1000 | Loss: 0.00002073
Iteration 81/1000 | Loss: 0.00002072
Iteration 82/1000 | Loss: 0.00002072
Iteration 83/1000 | Loss: 0.00002072
Iteration 84/1000 | Loss: 0.00002072
Iteration 85/1000 | Loss: 0.00002072
Iteration 86/1000 | Loss: 0.00002072
Iteration 87/1000 | Loss: 0.00002072
Iteration 88/1000 | Loss: 0.00002071
Iteration 89/1000 | Loss: 0.00002071
Iteration 90/1000 | Loss: 0.00002071
Iteration 91/1000 | Loss: 0.00002071
Iteration 92/1000 | Loss: 0.00002071
Iteration 93/1000 | Loss: 0.00002071
Iteration 94/1000 | Loss: 0.00002071
Iteration 95/1000 | Loss: 0.00002071
Iteration 96/1000 | Loss: 0.00002071
Iteration 97/1000 | Loss: 0.00002071
Iteration 98/1000 | Loss: 0.00002071
Iteration 99/1000 | Loss: 0.00002070
Iteration 100/1000 | Loss: 0.00002070
Iteration 101/1000 | Loss: 0.00002070
Iteration 102/1000 | Loss: 0.00002070
Iteration 103/1000 | Loss: 0.00002070
Iteration 104/1000 | Loss: 0.00002070
Iteration 105/1000 | Loss: 0.00002070
Iteration 106/1000 | Loss: 0.00002070
Iteration 107/1000 | Loss: 0.00002070
Iteration 108/1000 | Loss: 0.00002070
Iteration 109/1000 | Loss: 0.00002070
Iteration 110/1000 | Loss: 0.00002070
Iteration 111/1000 | Loss: 0.00002070
Iteration 112/1000 | Loss: 0.00002069
Iteration 113/1000 | Loss: 0.00002069
Iteration 114/1000 | Loss: 0.00002069
Iteration 115/1000 | Loss: 0.00002069
Iteration 116/1000 | Loss: 0.00002069
Iteration 117/1000 | Loss: 0.00002069
Iteration 118/1000 | Loss: 0.00002069
Iteration 119/1000 | Loss: 0.00002069
Iteration 120/1000 | Loss: 0.00002069
Iteration 121/1000 | Loss: 0.00002069
Iteration 122/1000 | Loss: 0.00002069
Iteration 123/1000 | Loss: 0.00002068
Iteration 124/1000 | Loss: 0.00002068
Iteration 125/1000 | Loss: 0.00002068
Iteration 126/1000 | Loss: 0.00002068
Iteration 127/1000 | Loss: 0.00002068
Iteration 128/1000 | Loss: 0.00002068
Iteration 129/1000 | Loss: 0.00002068
Iteration 130/1000 | Loss: 0.00002068
Iteration 131/1000 | Loss: 0.00002068
Iteration 132/1000 | Loss: 0.00002068
Iteration 133/1000 | Loss: 0.00002068
Iteration 134/1000 | Loss: 0.00002068
Iteration 135/1000 | Loss: 0.00002068
Iteration 136/1000 | Loss: 0.00002068
Iteration 137/1000 | Loss: 0.00002068
Iteration 138/1000 | Loss: 0.00002068
Iteration 139/1000 | Loss: 0.00002068
Iteration 140/1000 | Loss: 0.00002068
Iteration 141/1000 | Loss: 0.00002068
Iteration 142/1000 | Loss: 0.00002068
Iteration 143/1000 | Loss: 0.00002068
Iteration 144/1000 | Loss: 0.00002068
Iteration 145/1000 | Loss: 0.00002068
Iteration 146/1000 | Loss: 0.00002068
Iteration 147/1000 | Loss: 0.00002068
Iteration 148/1000 | Loss: 0.00002068
Iteration 149/1000 | Loss: 0.00002068
Iteration 150/1000 | Loss: 0.00002068
Iteration 151/1000 | Loss: 0.00002068
Iteration 152/1000 | Loss: 0.00002068
Iteration 153/1000 | Loss: 0.00002068
Iteration 154/1000 | Loss: 0.00002068
Iteration 155/1000 | Loss: 0.00002068
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [2.0682302420027554e-05, 2.0682302420027554e-05, 2.0682302420027554e-05, 2.0682302420027554e-05, 2.0682302420027554e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0682302420027554e-05

Optimization complete. Final v2v error: 3.84890079498291 mm

Highest mean error: 4.115167140960693 mm for frame 119

Lowest mean error: 3.55238938331604 mm for frame 49

Saving results

Total time: 32.01776623725891
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00819517
Iteration 2/25 | Loss: 0.00118575
Iteration 3/25 | Loss: 0.00091021
Iteration 4/25 | Loss: 0.00083864
Iteration 5/25 | Loss: 0.00082474
Iteration 6/25 | Loss: 0.00082235
Iteration 7/25 | Loss: 0.00082221
Iteration 8/25 | Loss: 0.00082221
Iteration 9/25 | Loss: 0.00082221
Iteration 10/25 | Loss: 0.00082221
Iteration 11/25 | Loss: 0.00082221
Iteration 12/25 | Loss: 0.00082221
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0008222080650739372, 0.0008222080650739372, 0.0008222080650739372, 0.0008222080650739372, 0.0008222080650739372]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008222080650739372

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59117281
Iteration 2/25 | Loss: 0.00140610
Iteration 3/25 | Loss: 0.00140609
Iteration 4/25 | Loss: 0.00140609
Iteration 5/25 | Loss: 0.00140609
Iteration 6/25 | Loss: 0.00140608
Iteration 7/25 | Loss: 0.00140608
Iteration 8/25 | Loss: 0.00140608
Iteration 9/25 | Loss: 0.00140608
Iteration 10/25 | Loss: 0.00140608
Iteration 11/25 | Loss: 0.00140608
Iteration 12/25 | Loss: 0.00140608
Iteration 13/25 | Loss: 0.00140608
Iteration 14/25 | Loss: 0.00140608
Iteration 15/25 | Loss: 0.00140608
Iteration 16/25 | Loss: 0.00140608
Iteration 17/25 | Loss: 0.00140608
Iteration 18/25 | Loss: 0.00140608
Iteration 19/25 | Loss: 0.00140608
Iteration 20/25 | Loss: 0.00140608
Iteration 21/25 | Loss: 0.00140608
Iteration 22/25 | Loss: 0.00140608
Iteration 23/25 | Loss: 0.00140608
Iteration 24/25 | Loss: 0.00140608
Iteration 25/25 | Loss: 0.00140608

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00140608
Iteration 2/1000 | Loss: 0.00003718
Iteration 3/1000 | Loss: 0.00002774
Iteration 4/1000 | Loss: 0.00002578
Iteration 5/1000 | Loss: 0.00002423
Iteration 6/1000 | Loss: 0.00002339
Iteration 7/1000 | Loss: 0.00002262
Iteration 8/1000 | Loss: 0.00002205
Iteration 9/1000 | Loss: 0.00002164
Iteration 10/1000 | Loss: 0.00002155
Iteration 11/1000 | Loss: 0.00002132
Iteration 12/1000 | Loss: 0.00002118
Iteration 13/1000 | Loss: 0.00002104
Iteration 14/1000 | Loss: 0.00002101
Iteration 15/1000 | Loss: 0.00002100
Iteration 16/1000 | Loss: 0.00002098
Iteration 17/1000 | Loss: 0.00002097
Iteration 18/1000 | Loss: 0.00002096
Iteration 19/1000 | Loss: 0.00002093
Iteration 20/1000 | Loss: 0.00002092
Iteration 21/1000 | Loss: 0.00002091
Iteration 22/1000 | Loss: 0.00002091
Iteration 23/1000 | Loss: 0.00002090
Iteration 24/1000 | Loss: 0.00002090
Iteration 25/1000 | Loss: 0.00002090
Iteration 26/1000 | Loss: 0.00002090
Iteration 27/1000 | Loss: 0.00002090
Iteration 28/1000 | Loss: 0.00002090
Iteration 29/1000 | Loss: 0.00002089
Iteration 30/1000 | Loss: 0.00002089
Iteration 31/1000 | Loss: 0.00002088
Iteration 32/1000 | Loss: 0.00002088
Iteration 33/1000 | Loss: 0.00002088
Iteration 34/1000 | Loss: 0.00002087
Iteration 35/1000 | Loss: 0.00002087
Iteration 36/1000 | Loss: 0.00002087
Iteration 37/1000 | Loss: 0.00002087
Iteration 38/1000 | Loss: 0.00002086
Iteration 39/1000 | Loss: 0.00002086
Iteration 40/1000 | Loss: 0.00002085
Iteration 41/1000 | Loss: 0.00002085
Iteration 42/1000 | Loss: 0.00002085
Iteration 43/1000 | Loss: 0.00002084
Iteration 44/1000 | Loss: 0.00002084
Iteration 45/1000 | Loss: 0.00002081
Iteration 46/1000 | Loss: 0.00002081
Iteration 47/1000 | Loss: 0.00002080
Iteration 48/1000 | Loss: 0.00002078
Iteration 49/1000 | Loss: 0.00002077
Iteration 50/1000 | Loss: 0.00002077
Iteration 51/1000 | Loss: 0.00002077
Iteration 52/1000 | Loss: 0.00002077
Iteration 53/1000 | Loss: 0.00002076
Iteration 54/1000 | Loss: 0.00002076
Iteration 55/1000 | Loss: 0.00002075
Iteration 56/1000 | Loss: 0.00002075
Iteration 57/1000 | Loss: 0.00002075
Iteration 58/1000 | Loss: 0.00002074
Iteration 59/1000 | Loss: 0.00002074
Iteration 60/1000 | Loss: 0.00002073
Iteration 61/1000 | Loss: 0.00002073
Iteration 62/1000 | Loss: 0.00002072
Iteration 63/1000 | Loss: 0.00002072
Iteration 64/1000 | Loss: 0.00002071
Iteration 65/1000 | Loss: 0.00002071
Iteration 66/1000 | Loss: 0.00002071
Iteration 67/1000 | Loss: 0.00002070
Iteration 68/1000 | Loss: 0.00002070
Iteration 69/1000 | Loss: 0.00002069
Iteration 70/1000 | Loss: 0.00002069
Iteration 71/1000 | Loss: 0.00002069
Iteration 72/1000 | Loss: 0.00002069
Iteration 73/1000 | Loss: 0.00002069
Iteration 74/1000 | Loss: 0.00002068
Iteration 75/1000 | Loss: 0.00002068
Iteration 76/1000 | Loss: 0.00002068
Iteration 77/1000 | Loss: 0.00002068
Iteration 78/1000 | Loss: 0.00002068
Iteration 79/1000 | Loss: 0.00002068
Iteration 80/1000 | Loss: 0.00002068
Iteration 81/1000 | Loss: 0.00002068
Iteration 82/1000 | Loss: 0.00002068
Iteration 83/1000 | Loss: 0.00002067
Iteration 84/1000 | Loss: 0.00002067
Iteration 85/1000 | Loss: 0.00002067
Iteration 86/1000 | Loss: 0.00002066
Iteration 87/1000 | Loss: 0.00002066
Iteration 88/1000 | Loss: 0.00002066
Iteration 89/1000 | Loss: 0.00002066
Iteration 90/1000 | Loss: 0.00002066
Iteration 91/1000 | Loss: 0.00002066
Iteration 92/1000 | Loss: 0.00002066
Iteration 93/1000 | Loss: 0.00002066
Iteration 94/1000 | Loss: 0.00002066
Iteration 95/1000 | Loss: 0.00002066
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 95. Stopping optimization.
Last 5 losses: [2.0656658307416365e-05, 2.0656658307416365e-05, 2.0656658307416365e-05, 2.0656658307416365e-05, 2.0656658307416365e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.0656658307416365e-05

Optimization complete. Final v2v error: 3.8214282989501953 mm

Highest mean error: 4.4391021728515625 mm for frame 156

Lowest mean error: 3.4330103397369385 mm for frame 235

Saving results

Total time: 39.34703087806702
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1089/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1089.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1089
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01027169
Iteration 2/25 | Loss: 0.00214274
Iteration 3/25 | Loss: 0.00159319
Iteration 4/25 | Loss: 0.00120958
Iteration 5/25 | Loss: 0.00116909
Iteration 6/25 | Loss: 0.00124990
Iteration 7/25 | Loss: 0.00124747
Iteration 8/25 | Loss: 0.00101679
Iteration 9/25 | Loss: 0.00102374
Iteration 10/25 | Loss: 0.00094939
Iteration 11/25 | Loss: 0.00090978
Iteration 12/25 | Loss: 0.00090026
Iteration 13/25 | Loss: 0.00089784
Iteration 14/25 | Loss: 0.00089419
Iteration 15/25 | Loss: 0.00089245
Iteration 16/25 | Loss: 0.00088738
Iteration 17/25 | Loss: 0.00088197
Iteration 18/25 | Loss: 0.00087992
Iteration 19/25 | Loss: 0.00087451
Iteration 20/25 | Loss: 0.00087278
Iteration 21/25 | Loss: 0.00087658
Iteration 22/25 | Loss: 0.00087066
Iteration 23/25 | Loss: 0.00086577
Iteration 24/25 | Loss: 0.00086493
Iteration 25/25 | Loss: 0.00086824

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.03879452
Iteration 2/25 | Loss: 0.00198477
Iteration 3/25 | Loss: 0.00198477
Iteration 4/25 | Loss: 0.00198476
Iteration 5/25 | Loss: 0.00198476
Iteration 6/25 | Loss: 0.00198476
Iteration 7/25 | Loss: 0.00198476
Iteration 8/25 | Loss: 0.00198476
Iteration 9/25 | Loss: 0.00198476
Iteration 10/25 | Loss: 0.00198476
Iteration 11/25 | Loss: 0.00198476
Iteration 12/25 | Loss: 0.00198476
Iteration 13/25 | Loss: 0.00198476
Iteration 14/25 | Loss: 0.00198476
Iteration 15/25 | Loss: 0.00198476
Iteration 16/25 | Loss: 0.00198476
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0019847608637064695, 0.0019847608637064695, 0.0019847608637064695, 0.0019847608637064695, 0.0019847608637064695]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019847608637064695

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00198476
Iteration 2/1000 | Loss: 0.00034489
Iteration 3/1000 | Loss: 0.00027601
Iteration 4/1000 | Loss: 0.00032316
Iteration 5/1000 | Loss: 0.00073328
Iteration 6/1000 | Loss: 0.00052664
Iteration 7/1000 | Loss: 0.00043601
Iteration 8/1000 | Loss: 0.00035045
Iteration 9/1000 | Loss: 0.00040220
Iteration 10/1000 | Loss: 0.00034673
Iteration 11/1000 | Loss: 0.00034534
Iteration 12/1000 | Loss: 0.00031825
Iteration 13/1000 | Loss: 0.00118952
Iteration 14/1000 | Loss: 0.00032832
Iteration 15/1000 | Loss: 0.00031336
Iteration 16/1000 | Loss: 0.00041483
Iteration 17/1000 | Loss: 0.00027551
Iteration 18/1000 | Loss: 0.00039957
Iteration 19/1000 | Loss: 0.00042045
Iteration 20/1000 | Loss: 0.00081780
Iteration 21/1000 | Loss: 0.00020238
Iteration 22/1000 | Loss: 0.00023209
Iteration 23/1000 | Loss: 0.00022448
Iteration 24/1000 | Loss: 0.00004518
Iteration 25/1000 | Loss: 0.00003699
Iteration 26/1000 | Loss: 0.00002985
Iteration 27/1000 | Loss: 0.00004393
Iteration 28/1000 | Loss: 0.00048816
Iteration 29/1000 | Loss: 0.00022222
Iteration 30/1000 | Loss: 0.00002812
Iteration 31/1000 | Loss: 0.00002479
Iteration 32/1000 | Loss: 0.00002254
Iteration 33/1000 | Loss: 0.00002168
Iteration 34/1000 | Loss: 0.00002109
Iteration 35/1000 | Loss: 0.00073687
Iteration 36/1000 | Loss: 0.00053962
Iteration 37/1000 | Loss: 0.00070546
Iteration 38/1000 | Loss: 0.00057708
Iteration 39/1000 | Loss: 0.00005490
Iteration 40/1000 | Loss: 0.00043967
Iteration 41/1000 | Loss: 0.00047813
Iteration 42/1000 | Loss: 0.00065120
Iteration 43/1000 | Loss: 0.00046956
Iteration 44/1000 | Loss: 0.00017275
Iteration 45/1000 | Loss: 0.00013318
Iteration 46/1000 | Loss: 0.00017471
Iteration 47/1000 | Loss: 0.00008720
Iteration 48/1000 | Loss: 0.00012397
Iteration 49/1000 | Loss: 0.00025151
Iteration 50/1000 | Loss: 0.00017742
Iteration 51/1000 | Loss: 0.00021512
Iteration 52/1000 | Loss: 0.00014648
Iteration 53/1000 | Loss: 0.00004018
Iteration 54/1000 | Loss: 0.00002451
Iteration 55/1000 | Loss: 0.00002117
Iteration 56/1000 | Loss: 0.00001918
Iteration 57/1000 | Loss: 0.00001825
Iteration 58/1000 | Loss: 0.00001761
Iteration 59/1000 | Loss: 0.00001690
Iteration 60/1000 | Loss: 0.00001652
Iteration 61/1000 | Loss: 0.00001629
Iteration 62/1000 | Loss: 0.00001612
Iteration 63/1000 | Loss: 0.00001604
Iteration 64/1000 | Loss: 0.00001602
Iteration 65/1000 | Loss: 0.00001601
Iteration 66/1000 | Loss: 0.00001597
Iteration 67/1000 | Loss: 0.00001596
Iteration 68/1000 | Loss: 0.00001595
Iteration 69/1000 | Loss: 0.00001595
Iteration 70/1000 | Loss: 0.00001595
Iteration 71/1000 | Loss: 0.00001594
Iteration 72/1000 | Loss: 0.00001593
Iteration 73/1000 | Loss: 0.00001584
Iteration 74/1000 | Loss: 0.00001584
Iteration 75/1000 | Loss: 0.00001583
Iteration 76/1000 | Loss: 0.00001583
Iteration 77/1000 | Loss: 0.00001583
Iteration 78/1000 | Loss: 0.00001583
Iteration 79/1000 | Loss: 0.00001583
Iteration 80/1000 | Loss: 0.00001582
Iteration 81/1000 | Loss: 0.00001582
Iteration 82/1000 | Loss: 0.00001582
Iteration 83/1000 | Loss: 0.00001582
Iteration 84/1000 | Loss: 0.00001581
Iteration 85/1000 | Loss: 0.00001581
Iteration 86/1000 | Loss: 0.00001581
Iteration 87/1000 | Loss: 0.00001581
Iteration 88/1000 | Loss: 0.00001580
Iteration 89/1000 | Loss: 0.00001580
Iteration 90/1000 | Loss: 0.00001580
Iteration 91/1000 | Loss: 0.00001580
Iteration 92/1000 | Loss: 0.00001580
Iteration 93/1000 | Loss: 0.00001580
Iteration 94/1000 | Loss: 0.00001580
Iteration 95/1000 | Loss: 0.00001579
Iteration 96/1000 | Loss: 0.00001579
Iteration 97/1000 | Loss: 0.00001579
Iteration 98/1000 | Loss: 0.00001579
Iteration 99/1000 | Loss: 0.00001579
Iteration 100/1000 | Loss: 0.00001579
Iteration 101/1000 | Loss: 0.00001579
Iteration 102/1000 | Loss: 0.00001578
Iteration 103/1000 | Loss: 0.00001578
Iteration 104/1000 | Loss: 0.00001578
Iteration 105/1000 | Loss: 0.00001578
Iteration 106/1000 | Loss: 0.00001578
Iteration 107/1000 | Loss: 0.00001578
Iteration 108/1000 | Loss: 0.00001578
Iteration 109/1000 | Loss: 0.00001578
Iteration 110/1000 | Loss: 0.00001578
Iteration 111/1000 | Loss: 0.00001578
Iteration 112/1000 | Loss: 0.00001578
Iteration 113/1000 | Loss: 0.00001578
Iteration 114/1000 | Loss: 0.00001577
Iteration 115/1000 | Loss: 0.00001577
Iteration 116/1000 | Loss: 0.00001577
Iteration 117/1000 | Loss: 0.00001577
Iteration 118/1000 | Loss: 0.00001577
Iteration 119/1000 | Loss: 0.00001577
Iteration 120/1000 | Loss: 0.00001577
Iteration 121/1000 | Loss: 0.00001577
Iteration 122/1000 | Loss: 0.00001577
Iteration 123/1000 | Loss: 0.00001577
Iteration 124/1000 | Loss: 0.00001577
Iteration 125/1000 | Loss: 0.00001577
Iteration 126/1000 | Loss: 0.00001577
Iteration 127/1000 | Loss: 0.00001577
Iteration 128/1000 | Loss: 0.00001576
Iteration 129/1000 | Loss: 0.00001576
Iteration 130/1000 | Loss: 0.00001576
Iteration 131/1000 | Loss: 0.00001576
Iteration 132/1000 | Loss: 0.00001576
Iteration 133/1000 | Loss: 0.00001575
Iteration 134/1000 | Loss: 0.00001575
Iteration 135/1000 | Loss: 0.00001575
Iteration 136/1000 | Loss: 0.00001574
Iteration 137/1000 | Loss: 0.00001574
Iteration 138/1000 | Loss: 0.00001574
Iteration 139/1000 | Loss: 0.00001574
Iteration 140/1000 | Loss: 0.00001574
Iteration 141/1000 | Loss: 0.00001574
Iteration 142/1000 | Loss: 0.00001574
Iteration 143/1000 | Loss: 0.00001574
Iteration 144/1000 | Loss: 0.00001574
Iteration 145/1000 | Loss: 0.00001574
Iteration 146/1000 | Loss: 0.00001573
Iteration 147/1000 | Loss: 0.00001573
Iteration 148/1000 | Loss: 0.00001573
Iteration 149/1000 | Loss: 0.00001573
Iteration 150/1000 | Loss: 0.00001573
Iteration 151/1000 | Loss: 0.00001573
Iteration 152/1000 | Loss: 0.00001573
Iteration 153/1000 | Loss: 0.00001573
Iteration 154/1000 | Loss: 0.00001573
Iteration 155/1000 | Loss: 0.00001573
Iteration 156/1000 | Loss: 0.00001573
Iteration 157/1000 | Loss: 0.00001573
Iteration 158/1000 | Loss: 0.00001573
Iteration 159/1000 | Loss: 0.00001573
Iteration 160/1000 | Loss: 0.00001573
Iteration 161/1000 | Loss: 0.00001573
Iteration 162/1000 | Loss: 0.00001573
Iteration 163/1000 | Loss: 0.00001573
Iteration 164/1000 | Loss: 0.00001572
Iteration 165/1000 | Loss: 0.00001572
Iteration 166/1000 | Loss: 0.00001572
Iteration 167/1000 | Loss: 0.00001572
Iteration 168/1000 | Loss: 0.00001572
Iteration 169/1000 | Loss: 0.00001572
Iteration 170/1000 | Loss: 0.00001572
Iteration 171/1000 | Loss: 0.00001572
Iteration 172/1000 | Loss: 0.00001572
Iteration 173/1000 | Loss: 0.00001572
Iteration 174/1000 | Loss: 0.00001572
Iteration 175/1000 | Loss: 0.00001572
Iteration 176/1000 | Loss: 0.00001572
Iteration 177/1000 | Loss: 0.00001572
Iteration 178/1000 | Loss: 0.00001572
Iteration 179/1000 | Loss: 0.00001572
Iteration 180/1000 | Loss: 0.00001572
Iteration 181/1000 | Loss: 0.00001572
Iteration 182/1000 | Loss: 0.00001572
Iteration 183/1000 | Loss: 0.00001572
Iteration 184/1000 | Loss: 0.00001571
Iteration 185/1000 | Loss: 0.00001571
Iteration 186/1000 | Loss: 0.00001571
Iteration 187/1000 | Loss: 0.00001571
Iteration 188/1000 | Loss: 0.00001571
Iteration 189/1000 | Loss: 0.00001571
Iteration 190/1000 | Loss: 0.00001571
Iteration 191/1000 | Loss: 0.00001571
Iteration 192/1000 | Loss: 0.00001571
Iteration 193/1000 | Loss: 0.00001571
Iteration 194/1000 | Loss: 0.00001571
Iteration 195/1000 | Loss: 0.00001571
Iteration 196/1000 | Loss: 0.00001571
Iteration 197/1000 | Loss: 0.00001571
Iteration 198/1000 | Loss: 0.00001570
Iteration 199/1000 | Loss: 0.00001570
Iteration 200/1000 | Loss: 0.00001570
Iteration 201/1000 | Loss: 0.00001570
Iteration 202/1000 | Loss: 0.00001570
Iteration 203/1000 | Loss: 0.00001570
Iteration 204/1000 | Loss: 0.00001570
Iteration 205/1000 | Loss: 0.00001570
Iteration 206/1000 | Loss: 0.00001570
Iteration 207/1000 | Loss: 0.00001569
Iteration 208/1000 | Loss: 0.00001569
Iteration 209/1000 | Loss: 0.00001569
Iteration 210/1000 | Loss: 0.00001569
Iteration 211/1000 | Loss: 0.00001569
Iteration 212/1000 | Loss: 0.00001569
Iteration 213/1000 | Loss: 0.00001569
Iteration 214/1000 | Loss: 0.00001569
Iteration 215/1000 | Loss: 0.00001569
Iteration 216/1000 | Loss: 0.00001568
Iteration 217/1000 | Loss: 0.00001568
Iteration 218/1000 | Loss: 0.00001568
Iteration 219/1000 | Loss: 0.00001568
Iteration 220/1000 | Loss: 0.00001568
Iteration 221/1000 | Loss: 0.00001568
Iteration 222/1000 | Loss: 0.00001568
Iteration 223/1000 | Loss: 0.00001568
Iteration 224/1000 | Loss: 0.00001568
Iteration 225/1000 | Loss: 0.00001568
Iteration 226/1000 | Loss: 0.00001568
Iteration 227/1000 | Loss: 0.00001568
Iteration 228/1000 | Loss: 0.00001568
Iteration 229/1000 | Loss: 0.00001568
Iteration 230/1000 | Loss: 0.00001568
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 230. Stopping optimization.
Last 5 losses: [1.567813342262525e-05, 1.567813342262525e-05, 1.567813342262525e-05, 1.567813342262525e-05, 1.567813342262525e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.567813342262525e-05

Optimization complete. Final v2v error: 3.3148343563079834 mm

Highest mean error: 4.461505889892578 mm for frame 68

Lowest mean error: 2.8284928798675537 mm for frame 17

Saving results

Total time: 143.2076849937439
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1036/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1036.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1036
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00849546
Iteration 2/25 | Loss: 0.00093927
Iteration 3/25 | Loss: 0.00075985
Iteration 4/25 | Loss: 0.00073606
Iteration 5/25 | Loss: 0.00073020
Iteration 6/25 | Loss: 0.00072831
Iteration 7/25 | Loss: 0.00072779
Iteration 8/25 | Loss: 0.00072779
Iteration 9/25 | Loss: 0.00072779
Iteration 10/25 | Loss: 0.00072779
Iteration 11/25 | Loss: 0.00072779
Iteration 12/25 | Loss: 0.00072779
Iteration 13/25 | Loss: 0.00072779
Iteration 14/25 | Loss: 0.00072779
Iteration 15/25 | Loss: 0.00072779
Iteration 16/25 | Loss: 0.00072779
Iteration 17/25 | Loss: 0.00072779
Iteration 18/25 | Loss: 0.00072779
Iteration 19/25 | Loss: 0.00072779
Iteration 20/25 | Loss: 0.00072779
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0007277895347215235, 0.0007277895347215235, 0.0007277895347215235, 0.0007277895347215235, 0.0007277895347215235]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007277895347215235

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.59088218
Iteration 2/25 | Loss: 0.00119716
Iteration 3/25 | Loss: 0.00119716
Iteration 4/25 | Loss: 0.00119716
Iteration 5/25 | Loss: 0.00119716
Iteration 6/25 | Loss: 0.00119716
Iteration 7/25 | Loss: 0.00119716
Iteration 8/25 | Loss: 0.00119716
Iteration 9/25 | Loss: 0.00119716
Iteration 10/25 | Loss: 0.00119716
Iteration 11/25 | Loss: 0.00119716
Iteration 12/25 | Loss: 0.00119716
Iteration 13/25 | Loss: 0.00119716
Iteration 14/25 | Loss: 0.00119716
Iteration 15/25 | Loss: 0.00119716
Iteration 16/25 | Loss: 0.00119716
Iteration 17/25 | Loss: 0.00119716
Iteration 18/25 | Loss: 0.00119716
Iteration 19/25 | Loss: 0.00119716
Iteration 20/25 | Loss: 0.00119716
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.0011971574276685715, 0.0011971574276685715, 0.0011971574276685715, 0.0011971574276685715, 0.0011971574276685715]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011971574276685715

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00119716
Iteration 2/1000 | Loss: 0.00002155
Iteration 3/1000 | Loss: 0.00001436
Iteration 4/1000 | Loss: 0.00001309
Iteration 5/1000 | Loss: 0.00001224
Iteration 6/1000 | Loss: 0.00001182
Iteration 7/1000 | Loss: 0.00001152
Iteration 8/1000 | Loss: 0.00001145
Iteration 9/1000 | Loss: 0.00001145
Iteration 10/1000 | Loss: 0.00001138
Iteration 11/1000 | Loss: 0.00001137
Iteration 12/1000 | Loss: 0.00001134
Iteration 13/1000 | Loss: 0.00001133
Iteration 14/1000 | Loss: 0.00001132
Iteration 15/1000 | Loss: 0.00001132
Iteration 16/1000 | Loss: 0.00001132
Iteration 17/1000 | Loss: 0.00001131
Iteration 18/1000 | Loss: 0.00001116
Iteration 19/1000 | Loss: 0.00001112
Iteration 20/1000 | Loss: 0.00001112
Iteration 21/1000 | Loss: 0.00001111
Iteration 22/1000 | Loss: 0.00001110
Iteration 23/1000 | Loss: 0.00001105
Iteration 24/1000 | Loss: 0.00001105
Iteration 25/1000 | Loss: 0.00001104
Iteration 26/1000 | Loss: 0.00001104
Iteration 27/1000 | Loss: 0.00001104
Iteration 28/1000 | Loss: 0.00001103
Iteration 29/1000 | Loss: 0.00001101
Iteration 30/1000 | Loss: 0.00001101
Iteration 31/1000 | Loss: 0.00001097
Iteration 32/1000 | Loss: 0.00001093
Iteration 33/1000 | Loss: 0.00001093
Iteration 34/1000 | Loss: 0.00001092
Iteration 35/1000 | Loss: 0.00001092
Iteration 36/1000 | Loss: 0.00001091
Iteration 37/1000 | Loss: 0.00001091
Iteration 38/1000 | Loss: 0.00001090
Iteration 39/1000 | Loss: 0.00001090
Iteration 40/1000 | Loss: 0.00001089
Iteration 41/1000 | Loss: 0.00001089
Iteration 42/1000 | Loss: 0.00001089
Iteration 43/1000 | Loss: 0.00001088
Iteration 44/1000 | Loss: 0.00001088
Iteration 45/1000 | Loss: 0.00001088
Iteration 46/1000 | Loss: 0.00001087
Iteration 47/1000 | Loss: 0.00001087
Iteration 48/1000 | Loss: 0.00001087
Iteration 49/1000 | Loss: 0.00001087
Iteration 50/1000 | Loss: 0.00001087
Iteration 51/1000 | Loss: 0.00001087
Iteration 52/1000 | Loss: 0.00001087
Iteration 53/1000 | Loss: 0.00001087
Iteration 54/1000 | Loss: 0.00001087
Iteration 55/1000 | Loss: 0.00001087
Iteration 56/1000 | Loss: 0.00001087
Iteration 57/1000 | Loss: 0.00001087
Iteration 58/1000 | Loss: 0.00001087
Iteration 59/1000 | Loss: 0.00001087
Iteration 60/1000 | Loss: 0.00001087
Iteration 61/1000 | Loss: 0.00001087
Iteration 62/1000 | Loss: 0.00001087
Iteration 63/1000 | Loss: 0.00001087
Iteration 64/1000 | Loss: 0.00001087
Iteration 65/1000 | Loss: 0.00001087
Iteration 66/1000 | Loss: 0.00001087
Iteration 67/1000 | Loss: 0.00001087
Iteration 68/1000 | Loss: 0.00001087
Iteration 69/1000 | Loss: 0.00001087
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 69. Stopping optimization.
Last 5 losses: [1.0866647244256455e-05, 1.0866647244256455e-05, 1.0866647244256455e-05, 1.0866647244256455e-05, 1.0866647244256455e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0866647244256455e-05

Optimization complete. Final v2v error: 2.7728934288024902 mm

Highest mean error: 2.9944348335266113 mm for frame 94

Lowest mean error: 2.6022789478302 mm for frame 144

Saving results

Total time: 29.776264667510986
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1054/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1054.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1054
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00905985
Iteration 2/25 | Loss: 0.00128442
Iteration 3/25 | Loss: 0.00086361
Iteration 4/25 | Loss: 0.00082462
Iteration 5/25 | Loss: 0.00081829
Iteration 6/25 | Loss: 0.00081717
Iteration 7/25 | Loss: 0.00081717
Iteration 8/25 | Loss: 0.00081717
Iteration 9/25 | Loss: 0.00081717
Iteration 10/25 | Loss: 0.00081717
Iteration 11/25 | Loss: 0.00081717
Iteration 12/25 | Loss: 0.00081717
Iteration 13/25 | Loss: 0.00081717
Iteration 14/25 | Loss: 0.00081717
Iteration 15/25 | Loss: 0.00081717
Iteration 16/25 | Loss: 0.00081717
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008171743829734623, 0.0008171743829734623, 0.0008171743829734623, 0.0008171743829734623, 0.0008171743829734623]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008171743829734623

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 6.57174444
Iteration 2/25 | Loss: 0.00110413
Iteration 3/25 | Loss: 0.00110413
Iteration 4/25 | Loss: 0.00110413
Iteration 5/25 | Loss: 0.00110413
Iteration 6/25 | Loss: 0.00110413
Iteration 7/25 | Loss: 0.00110413
Iteration 8/25 | Loss: 0.00110413
Iteration 9/25 | Loss: 0.00110413
Iteration 10/25 | Loss: 0.00110413
Iteration 11/25 | Loss: 0.00110413
Iteration 12/25 | Loss: 0.00110413
Iteration 13/25 | Loss: 0.00110413
Iteration 14/25 | Loss: 0.00110413
Iteration 15/25 | Loss: 0.00110413
Iteration 16/25 | Loss: 0.00110413
Iteration 17/25 | Loss: 0.00110413
Iteration 18/25 | Loss: 0.00110413
Iteration 19/25 | Loss: 0.00110413
Iteration 20/25 | Loss: 0.00110413
Iteration 21/25 | Loss: 0.00110413
Iteration 22/25 | Loss: 0.00110413
Iteration 23/25 | Loss: 0.00110413
Iteration 24/25 | Loss: 0.00110413
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.0011041293619200587, 0.0011041293619200587, 0.0011041293619200587, 0.0011041293619200587, 0.0011041293619200587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011041293619200587

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110413
Iteration 2/1000 | Loss: 0.00002662
Iteration 3/1000 | Loss: 0.00001734
Iteration 4/1000 | Loss: 0.00001544
Iteration 5/1000 | Loss: 0.00001457
Iteration 6/1000 | Loss: 0.00001402
Iteration 7/1000 | Loss: 0.00001375
Iteration 8/1000 | Loss: 0.00001375
Iteration 9/1000 | Loss: 0.00001362
Iteration 10/1000 | Loss: 0.00001345
Iteration 11/1000 | Loss: 0.00001334
Iteration 12/1000 | Loss: 0.00001326
Iteration 13/1000 | Loss: 0.00001325
Iteration 14/1000 | Loss: 0.00001320
Iteration 15/1000 | Loss: 0.00001319
Iteration 16/1000 | Loss: 0.00001317
Iteration 17/1000 | Loss: 0.00001317
Iteration 18/1000 | Loss: 0.00001314
Iteration 19/1000 | Loss: 0.00001312
Iteration 20/1000 | Loss: 0.00001312
Iteration 21/1000 | Loss: 0.00001312
Iteration 22/1000 | Loss: 0.00001311
Iteration 23/1000 | Loss: 0.00001308
Iteration 24/1000 | Loss: 0.00001307
Iteration 25/1000 | Loss: 0.00001306
Iteration 26/1000 | Loss: 0.00001305
Iteration 27/1000 | Loss: 0.00001305
Iteration 28/1000 | Loss: 0.00001305
Iteration 29/1000 | Loss: 0.00001305
Iteration 30/1000 | Loss: 0.00001305
Iteration 31/1000 | Loss: 0.00001305
Iteration 32/1000 | Loss: 0.00001304
Iteration 33/1000 | Loss: 0.00001304
Iteration 34/1000 | Loss: 0.00001302
Iteration 35/1000 | Loss: 0.00001302
Iteration 36/1000 | Loss: 0.00001302
Iteration 37/1000 | Loss: 0.00001302
Iteration 38/1000 | Loss: 0.00001302
Iteration 39/1000 | Loss: 0.00001302
Iteration 40/1000 | Loss: 0.00001302
Iteration 41/1000 | Loss: 0.00001300
Iteration 42/1000 | Loss: 0.00001299
Iteration 43/1000 | Loss: 0.00001299
Iteration 44/1000 | Loss: 0.00001298
Iteration 45/1000 | Loss: 0.00001298
Iteration 46/1000 | Loss: 0.00001297
Iteration 47/1000 | Loss: 0.00001297
Iteration 48/1000 | Loss: 0.00001295
Iteration 49/1000 | Loss: 0.00001289
Iteration 50/1000 | Loss: 0.00001286
Iteration 51/1000 | Loss: 0.00001285
Iteration 52/1000 | Loss: 0.00001285
Iteration 53/1000 | Loss: 0.00001284
Iteration 54/1000 | Loss: 0.00001284
Iteration 55/1000 | Loss: 0.00001284
Iteration 56/1000 | Loss: 0.00001283
Iteration 57/1000 | Loss: 0.00001282
Iteration 58/1000 | Loss: 0.00001281
Iteration 59/1000 | Loss: 0.00001281
Iteration 60/1000 | Loss: 0.00001281
Iteration 61/1000 | Loss: 0.00001281
Iteration 62/1000 | Loss: 0.00001281
Iteration 63/1000 | Loss: 0.00001281
Iteration 64/1000 | Loss: 0.00001281
Iteration 65/1000 | Loss: 0.00001281
Iteration 66/1000 | Loss: 0.00001281
Iteration 67/1000 | Loss: 0.00001281
Iteration 68/1000 | Loss: 0.00001280
Iteration 69/1000 | Loss: 0.00001280
Iteration 70/1000 | Loss: 0.00001279
Iteration 71/1000 | Loss: 0.00001277
Iteration 72/1000 | Loss: 0.00001277
Iteration 73/1000 | Loss: 0.00001277
Iteration 74/1000 | Loss: 0.00001277
Iteration 75/1000 | Loss: 0.00001276
Iteration 76/1000 | Loss: 0.00001276
Iteration 77/1000 | Loss: 0.00001276
Iteration 78/1000 | Loss: 0.00001275
Iteration 79/1000 | Loss: 0.00001275
Iteration 80/1000 | Loss: 0.00001275
Iteration 81/1000 | Loss: 0.00001275
Iteration 82/1000 | Loss: 0.00001274
Iteration 83/1000 | Loss: 0.00001274
Iteration 84/1000 | Loss: 0.00001274
Iteration 85/1000 | Loss: 0.00001274
Iteration 86/1000 | Loss: 0.00001274
Iteration 87/1000 | Loss: 0.00001274
Iteration 88/1000 | Loss: 0.00001274
Iteration 89/1000 | Loss: 0.00001274
Iteration 90/1000 | Loss: 0.00001274
Iteration 91/1000 | Loss: 0.00001274
Iteration 92/1000 | Loss: 0.00001274
Iteration 93/1000 | Loss: 0.00001274
Iteration 94/1000 | Loss: 0.00001274
Iteration 95/1000 | Loss: 0.00001274
Iteration 96/1000 | Loss: 0.00001274
Iteration 97/1000 | Loss: 0.00001274
Iteration 98/1000 | Loss: 0.00001274
Iteration 99/1000 | Loss: 0.00001274
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 99. Stopping optimization.
Last 5 losses: [1.273880207008915e-05, 1.273880207008915e-05, 1.273880207008915e-05, 1.273880207008915e-05, 1.273880207008915e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.273880207008915e-05

Optimization complete. Final v2v error: 3.0060133934020996 mm

Highest mean error: 3.164564609527588 mm for frame 123

Lowest mean error: 2.8292746543884277 mm for frame 204

Saving results

Total time: 35.96486139297485
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00817347
Iteration 2/25 | Loss: 0.00115971
Iteration 3/25 | Loss: 0.00100150
Iteration 4/25 | Loss: 0.00093299
Iteration 5/25 | Loss: 0.00092214
Iteration 6/25 | Loss: 0.00091519
Iteration 7/25 | Loss: 0.00090077
Iteration 8/25 | Loss: 0.00088444
Iteration 9/25 | Loss: 0.00088047
Iteration 10/25 | Loss: 0.00087984
Iteration 11/25 | Loss: 0.00087962
Iteration 12/25 | Loss: 0.00087957
Iteration 13/25 | Loss: 0.00087957
Iteration 14/25 | Loss: 0.00087957
Iteration 15/25 | Loss: 0.00087957
Iteration 16/25 | Loss: 0.00087957
Iteration 17/25 | Loss: 0.00087957
Iteration 18/25 | Loss: 0.00087957
Iteration 19/25 | Loss: 0.00087957
Iteration 20/25 | Loss: 0.00087957
Iteration 21/25 | Loss: 0.00087957
Iteration 22/25 | Loss: 0.00087957
Iteration 23/25 | Loss: 0.00087957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0008795716566964984, 0.0008795716566964984, 0.0008795716566964984, 0.0008795716566964984, 0.0008795716566964984]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008795716566964984

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.61377501
Iteration 2/25 | Loss: 0.00163741
Iteration 3/25 | Loss: 0.00163738
Iteration 4/25 | Loss: 0.00163738
Iteration 5/25 | Loss: 0.00163738
Iteration 6/25 | Loss: 0.00163738
Iteration 7/25 | Loss: 0.00163738
Iteration 8/25 | Loss: 0.00163738
Iteration 9/25 | Loss: 0.00163738
Iteration 10/25 | Loss: 0.00163738
Iteration 11/25 | Loss: 0.00163738
Iteration 12/25 | Loss: 0.00163738
Iteration 13/25 | Loss: 0.00163738
Iteration 14/25 | Loss: 0.00163738
Iteration 15/25 | Loss: 0.00163738
Iteration 16/25 | Loss: 0.00163738
Iteration 17/25 | Loss: 0.00163738
Iteration 18/25 | Loss: 0.00163738
Iteration 19/25 | Loss: 0.00163738
Iteration 20/25 | Loss: 0.00163738
Iteration 21/25 | Loss: 0.00163738
Iteration 22/25 | Loss: 0.00163738
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0016373753314837813, 0.0016373753314837813, 0.0016373753314837813, 0.0016373753314837813, 0.0016373753314837813]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016373753314837813

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00163738
Iteration 2/1000 | Loss: 0.00007801
Iteration 3/1000 | Loss: 0.00005157
Iteration 4/1000 | Loss: 0.00004434
Iteration 5/1000 | Loss: 0.00004158
Iteration 6/1000 | Loss: 0.00003975
Iteration 7/1000 | Loss: 0.00003872
Iteration 8/1000 | Loss: 0.00003771
Iteration 9/1000 | Loss: 0.00003714
Iteration 10/1000 | Loss: 0.00003672
Iteration 11/1000 | Loss: 0.00003636
Iteration 12/1000 | Loss: 0.00003607
Iteration 13/1000 | Loss: 0.00003585
Iteration 14/1000 | Loss: 0.00003561
Iteration 15/1000 | Loss: 0.00003543
Iteration 16/1000 | Loss: 0.00003543
Iteration 17/1000 | Loss: 0.00003542
Iteration 18/1000 | Loss: 0.00003537
Iteration 19/1000 | Loss: 0.00003537
Iteration 20/1000 | Loss: 0.00003535
Iteration 21/1000 | Loss: 0.00003534
Iteration 22/1000 | Loss: 0.00003534
Iteration 23/1000 | Loss: 0.00003534
Iteration 24/1000 | Loss: 0.00003534
Iteration 25/1000 | Loss: 0.00003533
Iteration 26/1000 | Loss: 0.00003532
Iteration 27/1000 | Loss: 0.00003529
Iteration 28/1000 | Loss: 0.00003527
Iteration 29/1000 | Loss: 0.00003523
Iteration 30/1000 | Loss: 0.00003523
Iteration 31/1000 | Loss: 0.00003521
Iteration 32/1000 | Loss: 0.00003521
Iteration 33/1000 | Loss: 0.00003520
Iteration 34/1000 | Loss: 0.00003520
Iteration 35/1000 | Loss: 0.00003519
Iteration 36/1000 | Loss: 0.00003518
Iteration 37/1000 | Loss: 0.00003513
Iteration 38/1000 | Loss: 0.00003513
Iteration 39/1000 | Loss: 0.00003512
Iteration 40/1000 | Loss: 0.00003512
Iteration 41/1000 | Loss: 0.00003512
Iteration 42/1000 | Loss: 0.00003511
Iteration 43/1000 | Loss: 0.00003511
Iteration 44/1000 | Loss: 0.00003510
Iteration 45/1000 | Loss: 0.00003510
Iteration 46/1000 | Loss: 0.00003510
Iteration 47/1000 | Loss: 0.00003510
Iteration 48/1000 | Loss: 0.00003509
Iteration 49/1000 | Loss: 0.00003509
Iteration 50/1000 | Loss: 0.00003509
Iteration 51/1000 | Loss: 0.00003509
Iteration 52/1000 | Loss: 0.00003509
Iteration 53/1000 | Loss: 0.00003508
Iteration 54/1000 | Loss: 0.00003508
Iteration 55/1000 | Loss: 0.00003508
Iteration 56/1000 | Loss: 0.00003508
Iteration 57/1000 | Loss: 0.00003507
Iteration 58/1000 | Loss: 0.00003507
Iteration 59/1000 | Loss: 0.00003507
Iteration 60/1000 | Loss: 0.00003507
Iteration 61/1000 | Loss: 0.00003507
Iteration 62/1000 | Loss: 0.00003507
Iteration 63/1000 | Loss: 0.00003506
Iteration 64/1000 | Loss: 0.00003506
Iteration 65/1000 | Loss: 0.00003505
Iteration 66/1000 | Loss: 0.00003505
Iteration 67/1000 | Loss: 0.00003505
Iteration 68/1000 | Loss: 0.00003504
Iteration 69/1000 | Loss: 0.00003504
Iteration 70/1000 | Loss: 0.00003504
Iteration 71/1000 | Loss: 0.00003503
Iteration 72/1000 | Loss: 0.00003503
Iteration 73/1000 | Loss: 0.00003503
Iteration 74/1000 | Loss: 0.00003502
Iteration 75/1000 | Loss: 0.00003502
Iteration 76/1000 | Loss: 0.00003502
Iteration 77/1000 | Loss: 0.00003502
Iteration 78/1000 | Loss: 0.00003502
Iteration 79/1000 | Loss: 0.00003502
Iteration 80/1000 | Loss: 0.00003502
Iteration 81/1000 | Loss: 0.00003501
Iteration 82/1000 | Loss: 0.00003501
Iteration 83/1000 | Loss: 0.00003501
Iteration 84/1000 | Loss: 0.00003500
Iteration 85/1000 | Loss: 0.00003500
Iteration 86/1000 | Loss: 0.00003500
Iteration 87/1000 | Loss: 0.00003499
Iteration 88/1000 | Loss: 0.00003499
Iteration 89/1000 | Loss: 0.00003499
Iteration 90/1000 | Loss: 0.00003499
Iteration 91/1000 | Loss: 0.00003499
Iteration 92/1000 | Loss: 0.00003499
Iteration 93/1000 | Loss: 0.00003498
Iteration 94/1000 | Loss: 0.00003498
Iteration 95/1000 | Loss: 0.00003498
Iteration 96/1000 | Loss: 0.00003498
Iteration 97/1000 | Loss: 0.00003498
Iteration 98/1000 | Loss: 0.00003498
Iteration 99/1000 | Loss: 0.00003498
Iteration 100/1000 | Loss: 0.00003497
Iteration 101/1000 | Loss: 0.00003497
Iteration 102/1000 | Loss: 0.00003497
Iteration 103/1000 | Loss: 0.00003497
Iteration 104/1000 | Loss: 0.00003496
Iteration 105/1000 | Loss: 0.00003496
Iteration 106/1000 | Loss: 0.00003496
Iteration 107/1000 | Loss: 0.00003496
Iteration 108/1000 | Loss: 0.00003496
Iteration 109/1000 | Loss: 0.00003496
Iteration 110/1000 | Loss: 0.00003496
Iteration 111/1000 | Loss: 0.00003496
Iteration 112/1000 | Loss: 0.00003496
Iteration 113/1000 | Loss: 0.00003496
Iteration 114/1000 | Loss: 0.00003496
Iteration 115/1000 | Loss: 0.00003496
Iteration 116/1000 | Loss: 0.00003496
Iteration 117/1000 | Loss: 0.00003496
Iteration 118/1000 | Loss: 0.00003496
Iteration 119/1000 | Loss: 0.00003496
Iteration 120/1000 | Loss: 0.00003496
Iteration 121/1000 | Loss: 0.00003496
Iteration 122/1000 | Loss: 0.00003496
Iteration 123/1000 | Loss: 0.00003496
Iteration 124/1000 | Loss: 0.00003496
Iteration 125/1000 | Loss: 0.00003496
Iteration 126/1000 | Loss: 0.00003496
Iteration 127/1000 | Loss: 0.00003496
Iteration 128/1000 | Loss: 0.00003496
Iteration 129/1000 | Loss: 0.00003496
Iteration 130/1000 | Loss: 0.00003496
Iteration 131/1000 | Loss: 0.00003496
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [3.495700366329402e-05, 3.495700366329402e-05, 3.495700366329402e-05, 3.495700366329402e-05, 3.495700366329402e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.495700366329402e-05

Optimization complete. Final v2v error: 4.81614351272583 mm

Highest mean error: 5.759660243988037 mm for frame 36

Lowest mean error: 4.181927680969238 mm for frame 107

Saving results

Total time: 59.048407316207886
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1053/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1053.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1053
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00401277
Iteration 2/25 | Loss: 0.00100736
Iteration 3/25 | Loss: 0.00083660
Iteration 4/25 | Loss: 0.00078041
Iteration 5/25 | Loss: 0.00076019
Iteration 6/25 | Loss: 0.00075555
Iteration 7/25 | Loss: 0.00075447
Iteration 8/25 | Loss: 0.00075424
Iteration 9/25 | Loss: 0.00075424
Iteration 10/25 | Loss: 0.00075424
Iteration 11/25 | Loss: 0.00075424
Iteration 12/25 | Loss: 0.00075424
Iteration 13/25 | Loss: 0.00075424
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0007542390376329422, 0.0007542390376329422, 0.0007542390376329422, 0.0007542390376329422, 0.0007542390376329422]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007542390376329422

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.57849813
Iteration 2/25 | Loss: 0.00124946
Iteration 3/25 | Loss: 0.00124945
Iteration 4/25 | Loss: 0.00124945
Iteration 5/25 | Loss: 0.00124945
Iteration 6/25 | Loss: 0.00124945
Iteration 7/25 | Loss: 0.00124945
Iteration 8/25 | Loss: 0.00124945
Iteration 9/25 | Loss: 0.00124945
Iteration 10/25 | Loss: 0.00124945
Iteration 11/25 | Loss: 0.00124945
Iteration 12/25 | Loss: 0.00124945
Iteration 13/25 | Loss: 0.00124945
Iteration 14/25 | Loss: 0.00124945
Iteration 15/25 | Loss: 0.00124945
Iteration 16/25 | Loss: 0.00124945
Iteration 17/25 | Loss: 0.00124945
Iteration 18/25 | Loss: 0.00124945
Iteration 19/25 | Loss: 0.00124945
Iteration 20/25 | Loss: 0.00124945
Iteration 21/25 | Loss: 0.00124945
Iteration 22/25 | Loss: 0.00124945
Iteration 23/25 | Loss: 0.00124945
Iteration 24/25 | Loss: 0.00124945
Iteration 25/25 | Loss: 0.00124945

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00124945
Iteration 2/1000 | Loss: 0.00002178
Iteration 3/1000 | Loss: 0.00001723
Iteration 4/1000 | Loss: 0.00001591
Iteration 5/1000 | Loss: 0.00001505
Iteration 6/1000 | Loss: 0.00001470
Iteration 7/1000 | Loss: 0.00001442
Iteration 8/1000 | Loss: 0.00001422
Iteration 9/1000 | Loss: 0.00001408
Iteration 10/1000 | Loss: 0.00001387
Iteration 11/1000 | Loss: 0.00001386
Iteration 12/1000 | Loss: 0.00001385
Iteration 13/1000 | Loss: 0.00001383
Iteration 14/1000 | Loss: 0.00001382
Iteration 15/1000 | Loss: 0.00001382
Iteration 16/1000 | Loss: 0.00001381
Iteration 17/1000 | Loss: 0.00001381
Iteration 18/1000 | Loss: 0.00001381
Iteration 19/1000 | Loss: 0.00001380
Iteration 20/1000 | Loss: 0.00001380
Iteration 21/1000 | Loss: 0.00001380
Iteration 22/1000 | Loss: 0.00001376
Iteration 23/1000 | Loss: 0.00001376
Iteration 24/1000 | Loss: 0.00001376
Iteration 25/1000 | Loss: 0.00001375
Iteration 26/1000 | Loss: 0.00001374
Iteration 27/1000 | Loss: 0.00001370
Iteration 28/1000 | Loss: 0.00001370
Iteration 29/1000 | Loss: 0.00001370
Iteration 30/1000 | Loss: 0.00001370
Iteration 31/1000 | Loss: 0.00001370
Iteration 32/1000 | Loss: 0.00001370
Iteration 33/1000 | Loss: 0.00001370
Iteration 34/1000 | Loss: 0.00001369
Iteration 35/1000 | Loss: 0.00001369
Iteration 36/1000 | Loss: 0.00001369
Iteration 37/1000 | Loss: 0.00001367
Iteration 38/1000 | Loss: 0.00001367
Iteration 39/1000 | Loss: 0.00001367
Iteration 40/1000 | Loss: 0.00001366
Iteration 41/1000 | Loss: 0.00001366
Iteration 42/1000 | Loss: 0.00001366
Iteration 43/1000 | Loss: 0.00001365
Iteration 44/1000 | Loss: 0.00001365
Iteration 45/1000 | Loss: 0.00001365
Iteration 46/1000 | Loss: 0.00001364
Iteration 47/1000 | Loss: 0.00001364
Iteration 48/1000 | Loss: 0.00001364
Iteration 49/1000 | Loss: 0.00001364
Iteration 50/1000 | Loss: 0.00001364
Iteration 51/1000 | Loss: 0.00001364
Iteration 52/1000 | Loss: 0.00001364
Iteration 53/1000 | Loss: 0.00001364
Iteration 54/1000 | Loss: 0.00001364
Iteration 55/1000 | Loss: 0.00001363
Iteration 56/1000 | Loss: 0.00001363
Iteration 57/1000 | Loss: 0.00001363
Iteration 58/1000 | Loss: 0.00001362
Iteration 59/1000 | Loss: 0.00001361
Iteration 60/1000 | Loss: 0.00001361
Iteration 61/1000 | Loss: 0.00001361
Iteration 62/1000 | Loss: 0.00001361
Iteration 63/1000 | Loss: 0.00001361
Iteration 64/1000 | Loss: 0.00001361
Iteration 65/1000 | Loss: 0.00001361
Iteration 66/1000 | Loss: 0.00001361
Iteration 67/1000 | Loss: 0.00001361
Iteration 68/1000 | Loss: 0.00001361
Iteration 69/1000 | Loss: 0.00001360
Iteration 70/1000 | Loss: 0.00001360
Iteration 71/1000 | Loss: 0.00001360
Iteration 72/1000 | Loss: 0.00001359
Iteration 73/1000 | Loss: 0.00001359
Iteration 74/1000 | Loss: 0.00001358
Iteration 75/1000 | Loss: 0.00001358
Iteration 76/1000 | Loss: 0.00001358
Iteration 77/1000 | Loss: 0.00001358
Iteration 78/1000 | Loss: 0.00001357
Iteration 79/1000 | Loss: 0.00001356
Iteration 80/1000 | Loss: 0.00001356
Iteration 81/1000 | Loss: 0.00001356
Iteration 82/1000 | Loss: 0.00001355
Iteration 83/1000 | Loss: 0.00001355
Iteration 84/1000 | Loss: 0.00001354
Iteration 85/1000 | Loss: 0.00001353
Iteration 86/1000 | Loss: 0.00001353
Iteration 87/1000 | Loss: 0.00001353
Iteration 88/1000 | Loss: 0.00001353
Iteration 89/1000 | Loss: 0.00001352
Iteration 90/1000 | Loss: 0.00001352
Iteration 91/1000 | Loss: 0.00001352
Iteration 92/1000 | Loss: 0.00001352
Iteration 93/1000 | Loss: 0.00001351
Iteration 94/1000 | Loss: 0.00001351
Iteration 95/1000 | Loss: 0.00001351
Iteration 96/1000 | Loss: 0.00001351
Iteration 97/1000 | Loss: 0.00001351
Iteration 98/1000 | Loss: 0.00001351
Iteration 99/1000 | Loss: 0.00001351
Iteration 100/1000 | Loss: 0.00001351
Iteration 101/1000 | Loss: 0.00001351
Iteration 102/1000 | Loss: 0.00001351
Iteration 103/1000 | Loss: 0.00001350
Iteration 104/1000 | Loss: 0.00001350
Iteration 105/1000 | Loss: 0.00001350
Iteration 106/1000 | Loss: 0.00001350
Iteration 107/1000 | Loss: 0.00001350
Iteration 108/1000 | Loss: 0.00001350
Iteration 109/1000 | Loss: 0.00001350
Iteration 110/1000 | Loss: 0.00001349
Iteration 111/1000 | Loss: 0.00001349
Iteration 112/1000 | Loss: 0.00001349
Iteration 113/1000 | Loss: 0.00001349
Iteration 114/1000 | Loss: 0.00001349
Iteration 115/1000 | Loss: 0.00001349
Iteration 116/1000 | Loss: 0.00001349
Iteration 117/1000 | Loss: 0.00001349
Iteration 118/1000 | Loss: 0.00001349
Iteration 119/1000 | Loss: 0.00001348
Iteration 120/1000 | Loss: 0.00001348
Iteration 121/1000 | Loss: 0.00001348
Iteration 122/1000 | Loss: 0.00001348
Iteration 123/1000 | Loss: 0.00001348
Iteration 124/1000 | Loss: 0.00001348
Iteration 125/1000 | Loss: 0.00001348
Iteration 126/1000 | Loss: 0.00001348
Iteration 127/1000 | Loss: 0.00001348
Iteration 128/1000 | Loss: 0.00001348
Iteration 129/1000 | Loss: 0.00001348
Iteration 130/1000 | Loss: 0.00001348
Iteration 131/1000 | Loss: 0.00001348
Iteration 132/1000 | Loss: 0.00001348
Iteration 133/1000 | Loss: 0.00001348
Iteration 134/1000 | Loss: 0.00001348
Iteration 135/1000 | Loss: 0.00001348
Iteration 136/1000 | Loss: 0.00001348
Iteration 137/1000 | Loss: 0.00001347
Iteration 138/1000 | Loss: 0.00001347
Iteration 139/1000 | Loss: 0.00001347
Iteration 140/1000 | Loss: 0.00001347
Iteration 141/1000 | Loss: 0.00001347
Iteration 142/1000 | Loss: 0.00001347
Iteration 143/1000 | Loss: 0.00001347
Iteration 144/1000 | Loss: 0.00001347
Iteration 145/1000 | Loss: 0.00001347
Iteration 146/1000 | Loss: 0.00001347
Iteration 147/1000 | Loss: 0.00001347
Iteration 148/1000 | Loss: 0.00001347
Iteration 149/1000 | Loss: 0.00001347
Iteration 150/1000 | Loss: 0.00001347
Iteration 151/1000 | Loss: 0.00001346
Iteration 152/1000 | Loss: 0.00001346
Iteration 153/1000 | Loss: 0.00001346
Iteration 154/1000 | Loss: 0.00001346
Iteration 155/1000 | Loss: 0.00001346
Iteration 156/1000 | Loss: 0.00001346
Iteration 157/1000 | Loss: 0.00001346
Iteration 158/1000 | Loss: 0.00001346
Iteration 159/1000 | Loss: 0.00001346
Iteration 160/1000 | Loss: 0.00001346
Iteration 161/1000 | Loss: 0.00001346
Iteration 162/1000 | Loss: 0.00001346
Iteration 163/1000 | Loss: 0.00001346
Iteration 164/1000 | Loss: 0.00001346
Iteration 165/1000 | Loss: 0.00001346
Iteration 166/1000 | Loss: 0.00001346
Iteration 167/1000 | Loss: 0.00001346
Iteration 168/1000 | Loss: 0.00001346
Iteration 169/1000 | Loss: 0.00001346
Iteration 170/1000 | Loss: 0.00001346
Iteration 171/1000 | Loss: 0.00001346
Iteration 172/1000 | Loss: 0.00001346
Iteration 173/1000 | Loss: 0.00001346
Iteration 174/1000 | Loss: 0.00001346
Iteration 175/1000 | Loss: 0.00001346
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 175. Stopping optimization.
Last 5 losses: [1.3458751709549688e-05, 1.3458751709549688e-05, 1.3458751709549688e-05, 1.3458751709549688e-05, 1.3458751709549688e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.3458751709549688e-05

Optimization complete. Final v2v error: 3.092353582382202 mm

Highest mean error: 3.4176063537597656 mm for frame 141

Lowest mean error: 2.901705265045166 mm for frame 266

Saving results

Total time: 42.754929304122925
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1037/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1037.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1037
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00835106
Iteration 2/25 | Loss: 0.00099367
Iteration 3/25 | Loss: 0.00077273
Iteration 4/25 | Loss: 0.00075501
Iteration 5/25 | Loss: 0.00075155
Iteration 6/25 | Loss: 0.00075010
Iteration 7/25 | Loss: 0.00075005
Iteration 8/25 | Loss: 0.00075005
Iteration 9/25 | Loss: 0.00075005
Iteration 10/25 | Loss: 0.00075005
Iteration 11/25 | Loss: 0.00075005
Iteration 12/25 | Loss: 0.00075005
Iteration 13/25 | Loss: 0.00075005
Iteration 14/25 | Loss: 0.00075005
Iteration 15/25 | Loss: 0.00075005
Iteration 16/25 | Loss: 0.00075005
Iteration 17/25 | Loss: 0.00075005
Iteration 18/25 | Loss: 0.00075005
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.000750047795008868, 0.000750047795008868, 0.000750047795008868, 0.000750047795008868, 0.000750047795008868]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000750047795008868

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.60014713
Iteration 2/25 | Loss: 0.00131401
Iteration 3/25 | Loss: 0.00131400
Iteration 4/25 | Loss: 0.00131400
Iteration 5/25 | Loss: 0.00131400
Iteration 6/25 | Loss: 0.00131400
Iteration 7/25 | Loss: 0.00131400
Iteration 8/25 | Loss: 0.00131400
Iteration 9/25 | Loss: 0.00131400
Iteration 10/25 | Loss: 0.00131400
Iteration 11/25 | Loss: 0.00131400
Iteration 12/25 | Loss: 0.00131400
Iteration 13/25 | Loss: 0.00131400
Iteration 14/25 | Loss: 0.00131400
Iteration 15/25 | Loss: 0.00131400
Iteration 16/25 | Loss: 0.00131400
Iteration 17/25 | Loss: 0.00131400
Iteration 18/25 | Loss: 0.00131400
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0013140017399564385, 0.0013140017399564385, 0.0013140017399564385, 0.0013140017399564385, 0.0013140017399564385]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0013140017399564385

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00131400
Iteration 2/1000 | Loss: 0.00002494
Iteration 3/1000 | Loss: 0.00001554
Iteration 4/1000 | Loss: 0.00001428
Iteration 5/1000 | Loss: 0.00001343
Iteration 6/1000 | Loss: 0.00001274
Iteration 7/1000 | Loss: 0.00001242
Iteration 8/1000 | Loss: 0.00001221
Iteration 9/1000 | Loss: 0.00001219
Iteration 10/1000 | Loss: 0.00001206
Iteration 11/1000 | Loss: 0.00001189
Iteration 12/1000 | Loss: 0.00001185
Iteration 13/1000 | Loss: 0.00001182
Iteration 14/1000 | Loss: 0.00001181
Iteration 15/1000 | Loss: 0.00001173
Iteration 16/1000 | Loss: 0.00001172
Iteration 17/1000 | Loss: 0.00001172
Iteration 18/1000 | Loss: 0.00001171
Iteration 19/1000 | Loss: 0.00001170
Iteration 20/1000 | Loss: 0.00001170
Iteration 21/1000 | Loss: 0.00001169
Iteration 22/1000 | Loss: 0.00001169
Iteration 23/1000 | Loss: 0.00001169
Iteration 24/1000 | Loss: 0.00001168
Iteration 25/1000 | Loss: 0.00001168
Iteration 26/1000 | Loss: 0.00001168
Iteration 27/1000 | Loss: 0.00001168
Iteration 28/1000 | Loss: 0.00001168
Iteration 29/1000 | Loss: 0.00001167
Iteration 30/1000 | Loss: 0.00001167
Iteration 31/1000 | Loss: 0.00001167
Iteration 32/1000 | Loss: 0.00001167
Iteration 33/1000 | Loss: 0.00001166
Iteration 34/1000 | Loss: 0.00001166
Iteration 35/1000 | Loss: 0.00001165
Iteration 36/1000 | Loss: 0.00001164
Iteration 37/1000 | Loss: 0.00001164
Iteration 38/1000 | Loss: 0.00001164
Iteration 39/1000 | Loss: 0.00001163
Iteration 40/1000 | Loss: 0.00001163
Iteration 41/1000 | Loss: 0.00001163
Iteration 42/1000 | Loss: 0.00001162
Iteration 43/1000 | Loss: 0.00001162
Iteration 44/1000 | Loss: 0.00001161
Iteration 45/1000 | Loss: 0.00001160
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001159
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001158
Iteration 50/1000 | Loss: 0.00001157
Iteration 51/1000 | Loss: 0.00001157
Iteration 52/1000 | Loss: 0.00001156
Iteration 53/1000 | Loss: 0.00001156
Iteration 54/1000 | Loss: 0.00001156
Iteration 55/1000 | Loss: 0.00001155
Iteration 56/1000 | Loss: 0.00001155
Iteration 57/1000 | Loss: 0.00001155
Iteration 58/1000 | Loss: 0.00001154
Iteration 59/1000 | Loss: 0.00001154
Iteration 60/1000 | Loss: 0.00001154
Iteration 61/1000 | Loss: 0.00001154
Iteration 62/1000 | Loss: 0.00001153
Iteration 63/1000 | Loss: 0.00001153
Iteration 64/1000 | Loss: 0.00001153
Iteration 65/1000 | Loss: 0.00001153
Iteration 66/1000 | Loss: 0.00001153
Iteration 67/1000 | Loss: 0.00001153
Iteration 68/1000 | Loss: 0.00001152
Iteration 69/1000 | Loss: 0.00001152
Iteration 70/1000 | Loss: 0.00001152
Iteration 71/1000 | Loss: 0.00001152
Iteration 72/1000 | Loss: 0.00001152
Iteration 73/1000 | Loss: 0.00001152
Iteration 74/1000 | Loss: 0.00001152
Iteration 75/1000 | Loss: 0.00001151
Iteration 76/1000 | Loss: 0.00001151
Iteration 77/1000 | Loss: 0.00001151
Iteration 78/1000 | Loss: 0.00001151
Iteration 79/1000 | Loss: 0.00001151
Iteration 80/1000 | Loss: 0.00001151
Iteration 81/1000 | Loss: 0.00001150
Iteration 82/1000 | Loss: 0.00001150
Iteration 83/1000 | Loss: 0.00001150
Iteration 84/1000 | Loss: 0.00001150
Iteration 85/1000 | Loss: 0.00001150
Iteration 86/1000 | Loss: 0.00001150
Iteration 87/1000 | Loss: 0.00001150
Iteration 88/1000 | Loss: 0.00001150
Iteration 89/1000 | Loss: 0.00001150
Iteration 90/1000 | Loss: 0.00001150
Iteration 91/1000 | Loss: 0.00001149
Iteration 92/1000 | Loss: 0.00001149
Iteration 93/1000 | Loss: 0.00001149
Iteration 94/1000 | Loss: 0.00001148
Iteration 95/1000 | Loss: 0.00001148
Iteration 96/1000 | Loss: 0.00001148
Iteration 97/1000 | Loss: 0.00001148
Iteration 98/1000 | Loss: 0.00001148
Iteration 99/1000 | Loss: 0.00001148
Iteration 100/1000 | Loss: 0.00001148
Iteration 101/1000 | Loss: 0.00001148
Iteration 102/1000 | Loss: 0.00001148
Iteration 103/1000 | Loss: 0.00001148
Iteration 104/1000 | Loss: 0.00001148
Iteration 105/1000 | Loss: 0.00001148
Iteration 106/1000 | Loss: 0.00001148
Iteration 107/1000 | Loss: 0.00001148
Iteration 108/1000 | Loss: 0.00001148
Iteration 109/1000 | Loss: 0.00001148
Iteration 110/1000 | Loss: 0.00001148
Iteration 111/1000 | Loss: 0.00001148
Iteration 112/1000 | Loss: 0.00001148
Iteration 113/1000 | Loss: 0.00001148
Iteration 114/1000 | Loss: 0.00001148
Iteration 115/1000 | Loss: 0.00001148
Iteration 116/1000 | Loss: 0.00001148
Iteration 117/1000 | Loss: 0.00001148
Iteration 118/1000 | Loss: 0.00001148
Iteration 119/1000 | Loss: 0.00001148
Iteration 120/1000 | Loss: 0.00001148
Iteration 121/1000 | Loss: 0.00001148
Iteration 122/1000 | Loss: 0.00001148
Iteration 123/1000 | Loss: 0.00001148
Iteration 124/1000 | Loss: 0.00001148
Iteration 125/1000 | Loss: 0.00001148
Iteration 126/1000 | Loss: 0.00001148
Iteration 127/1000 | Loss: 0.00001148
Iteration 128/1000 | Loss: 0.00001148
Iteration 129/1000 | Loss: 0.00001148
Iteration 130/1000 | Loss: 0.00001148
Iteration 131/1000 | Loss: 0.00001148
Iteration 132/1000 | Loss: 0.00001148
Iteration 133/1000 | Loss: 0.00001148
Iteration 134/1000 | Loss: 0.00001148
Iteration 135/1000 | Loss: 0.00001148
Iteration 136/1000 | Loss: 0.00001148
Iteration 137/1000 | Loss: 0.00001148
Iteration 138/1000 | Loss: 0.00001148
Iteration 139/1000 | Loss: 0.00001148
Iteration 140/1000 | Loss: 0.00001148
Iteration 141/1000 | Loss: 0.00001148
Iteration 142/1000 | Loss: 0.00001148
Iteration 143/1000 | Loss: 0.00001148
Iteration 144/1000 | Loss: 0.00001148
Iteration 145/1000 | Loss: 0.00001148
Iteration 146/1000 | Loss: 0.00001148
Iteration 147/1000 | Loss: 0.00001148
Iteration 148/1000 | Loss: 0.00001148
Iteration 149/1000 | Loss: 0.00001148
Iteration 150/1000 | Loss: 0.00001148
Iteration 151/1000 | Loss: 0.00001148
Iteration 152/1000 | Loss: 0.00001148
Iteration 153/1000 | Loss: 0.00001148
Iteration 154/1000 | Loss: 0.00001148
Iteration 155/1000 | Loss: 0.00001148
Iteration 156/1000 | Loss: 0.00001148
Iteration 157/1000 | Loss: 0.00001148
Iteration 158/1000 | Loss: 0.00001148
Iteration 159/1000 | Loss: 0.00001148
Iteration 160/1000 | Loss: 0.00001148
Iteration 161/1000 | Loss: 0.00001148
Iteration 162/1000 | Loss: 0.00001148
Iteration 163/1000 | Loss: 0.00001148
Iteration 164/1000 | Loss: 0.00001148
Iteration 165/1000 | Loss: 0.00001148
Iteration 166/1000 | Loss: 0.00001148
Iteration 167/1000 | Loss: 0.00001148
Iteration 168/1000 | Loss: 0.00001148
Iteration 169/1000 | Loss: 0.00001148
Iteration 170/1000 | Loss: 0.00001148
Iteration 171/1000 | Loss: 0.00001148
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 171. Stopping optimization.
Last 5 losses: [1.1475200153654441e-05, 1.1475200153654441e-05, 1.1475200153654441e-05, 1.1475200153654441e-05, 1.1475200153654441e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1475200153654441e-05

Optimization complete. Final v2v error: 2.860151767730713 mm

Highest mean error: 3.2539260387420654 mm for frame 90

Lowest mean error: 2.7155582904815674 mm for frame 38

Saving results

Total time: 34.89368677139282
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/rp_aaron_posed_002/1051/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1051.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/rp_aaron_posed_002/1051
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00850867
Iteration 2/25 | Loss: 0.00096577
Iteration 3/25 | Loss: 0.00084684
Iteration 4/25 | Loss: 0.00078817
Iteration 5/25 | Loss: 0.00077845
Iteration 6/25 | Loss: 0.00077630
Iteration 7/25 | Loss: 0.00077603
Iteration 8/25 | Loss: 0.00077603
Iteration 9/25 | Loss: 0.00077603
Iteration 10/25 | Loss: 0.00077603
Iteration 11/25 | Loss: 0.00077603
Iteration 12/25 | Loss: 0.00077603
Iteration 13/25 | Loss: 0.00077603
Iteration 14/25 | Loss: 0.00077603
Iteration 15/25 | Loss: 0.00077603
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0007760301814414561, 0.0007760301814414561, 0.0007760301814414561, 0.0007760301814414561, 0.0007760301814414561]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007760301814414561

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.72293425
Iteration 2/25 | Loss: 0.00148423
Iteration 3/25 | Loss: 0.00148416
Iteration 4/25 | Loss: 0.00148416
Iteration 5/25 | Loss: 0.00148416
Iteration 6/25 | Loss: 0.00148416
Iteration 7/25 | Loss: 0.00148416
Iteration 8/25 | Loss: 0.00148416
Iteration 9/25 | Loss: 0.00148416
Iteration 10/25 | Loss: 0.00148416
Iteration 11/25 | Loss: 0.00148416
Iteration 12/25 | Loss: 0.00148416
Iteration 13/25 | Loss: 0.00148416
Iteration 14/25 | Loss: 0.00148416
Iteration 15/25 | Loss: 0.00148416
Iteration 16/25 | Loss: 0.00148416
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0014841597294434905, 0.0014841597294434905, 0.0014841597294434905, 0.0014841597294434905, 0.0014841597294434905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0014841597294434905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00148416
Iteration 2/1000 | Loss: 0.00003467
Iteration 3/1000 | Loss: 0.00002422
Iteration 4/1000 | Loss: 0.00002075
Iteration 5/1000 | Loss: 0.00001920
Iteration 6/1000 | Loss: 0.00001848
Iteration 7/1000 | Loss: 0.00001795
Iteration 8/1000 | Loss: 0.00001766
Iteration 9/1000 | Loss: 0.00001744
Iteration 10/1000 | Loss: 0.00001718
Iteration 11/1000 | Loss: 0.00001711
Iteration 12/1000 | Loss: 0.00001708
Iteration 13/1000 | Loss: 0.00001698
Iteration 14/1000 | Loss: 0.00001692
Iteration 15/1000 | Loss: 0.00001690
Iteration 16/1000 | Loss: 0.00001685
Iteration 17/1000 | Loss: 0.00001681
Iteration 18/1000 | Loss: 0.00001678
Iteration 19/1000 | Loss: 0.00001677
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001673
Iteration 22/1000 | Loss: 0.00001673
Iteration 23/1000 | Loss: 0.00001673
Iteration 24/1000 | Loss: 0.00001673
Iteration 25/1000 | Loss: 0.00001672
Iteration 26/1000 | Loss: 0.00001672
Iteration 27/1000 | Loss: 0.00001672
Iteration 28/1000 | Loss: 0.00001672
Iteration 29/1000 | Loss: 0.00001672
Iteration 30/1000 | Loss: 0.00001669
Iteration 31/1000 | Loss: 0.00001668
Iteration 32/1000 | Loss: 0.00001667
Iteration 33/1000 | Loss: 0.00001667
Iteration 34/1000 | Loss: 0.00001667
Iteration 35/1000 | Loss: 0.00001667
Iteration 36/1000 | Loss: 0.00001667
Iteration 37/1000 | Loss: 0.00001667
Iteration 38/1000 | Loss: 0.00001667
Iteration 39/1000 | Loss: 0.00001667
Iteration 40/1000 | Loss: 0.00001667
Iteration 41/1000 | Loss: 0.00001667
Iteration 42/1000 | Loss: 0.00001667
Iteration 43/1000 | Loss: 0.00001667
Iteration 44/1000 | Loss: 0.00001667
Iteration 45/1000 | Loss: 0.00001667
Iteration 46/1000 | Loss: 0.00001667
Iteration 47/1000 | Loss: 0.00001667
Iteration 48/1000 | Loss: 0.00001667
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 48. Stopping optimization.
Last 5 losses: [1.6668276657583192e-05, 1.6668276657583192e-05, 1.6668276657583192e-05, 1.6668276657583192e-05, 1.6668276657583192e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6668276657583192e-05

Optimization complete. Final v2v error: 3.4530599117279053 mm

Highest mean error: 3.9189257621765137 mm for frame 49

Lowest mean error: 3.0652010440826416 mm for frame 196

Saving results

Total time: 33.240304470062256
