Namespace(object_key=None, cluster_batch_size=56, cluster_start_idx=234, opts=[], cfg_id=0, cluster=False, bid=10, memory=64000, gpu_min_mem=12000, gpu_arch=['tesla', 'quadro', 'rtx'], num_cpus=8, input_dir='/is/cluster/sbhor/smpl_ground_truth_corr/', output_dir='/is/cluster/fast/sbhor/star_bedlam_bomoto/', cfg='/is/cluster/fast/sbhor/bomoto/configs/params_dataset.yaml')

Starting the conversion process at location /is/cluster/fast/sbhor/star_bedlam_bomoto/conversion_log.log.
running 56 files in the range 13104-13159
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00902929
Iteration 2/25 | Loss: 0.00324840
Iteration 3/25 | Loss: 0.00212329
Iteration 4/25 | Loss: 0.00177588
Iteration 5/25 | Loss: 0.00175727
Iteration 6/25 | Loss: 0.00170273
Iteration 7/25 | Loss: 0.00168813
Iteration 8/25 | Loss: 0.00169295
Iteration 9/25 | Loss: 0.00165961
Iteration 10/25 | Loss: 0.00165769
Iteration 11/25 | Loss: 0.00164872
Iteration 12/25 | Loss: 0.00164839
Iteration 13/25 | Loss: 0.00164759
Iteration 14/25 | Loss: 0.00164916
Iteration 15/25 | Loss: 0.00164676
Iteration 16/25 | Loss: 0.00164584
Iteration 17/25 | Loss: 0.00164558
Iteration 18/25 | Loss: 0.00164345
Iteration 19/25 | Loss: 0.00164526
Iteration 20/25 | Loss: 0.00164760
Iteration 21/25 | Loss: 0.00164563
Iteration 22/25 | Loss: 0.00164645
Iteration 23/25 | Loss: 0.00164799
Iteration 24/25 | Loss: 0.00164755
Iteration 25/25 | Loss: 0.00164557

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.88195634
Iteration 2/25 | Loss: 0.00449852
Iteration 3/25 | Loss: 0.00438636
Iteration 4/25 | Loss: 0.00438635
Iteration 5/25 | Loss: 0.00438635
Iteration 6/25 | Loss: 0.00438635
Iteration 7/25 | Loss: 0.00438634
Iteration 8/25 | Loss: 0.00438634
Iteration 9/25 | Loss: 0.00438634
Iteration 10/25 | Loss: 0.00438634
Iteration 11/25 | Loss: 0.00438634
Iteration 12/25 | Loss: 0.00438634
Iteration 13/25 | Loss: 0.00438634
Iteration 14/25 | Loss: 0.00438634
Iteration 15/25 | Loss: 0.00438634
Iteration 16/25 | Loss: 0.00438634
Iteration 17/25 | Loss: 0.00438634
Iteration 18/25 | Loss: 0.00438634
Iteration 19/25 | Loss: 0.00438634
Iteration 20/25 | Loss: 0.00438634
Iteration 21/25 | Loss: 0.00438634
Iteration 22/25 | Loss: 0.00438634
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.004386341664940119, 0.004386341664940119, 0.004386341664940119, 0.004386341664940119, 0.004386341664940119]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004386341664940119

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00438634
Iteration 2/1000 | Loss: 0.00047433
Iteration 3/1000 | Loss: 0.00058768
Iteration 4/1000 | Loss: 0.00069951
Iteration 5/1000 | Loss: 0.00066202
Iteration 6/1000 | Loss: 0.00425058
Iteration 7/1000 | Loss: 0.00104483
Iteration 8/1000 | Loss: 0.00120565
Iteration 9/1000 | Loss: 0.00050018
Iteration 10/1000 | Loss: 0.00039483
Iteration 11/1000 | Loss: 0.00186385
Iteration 12/1000 | Loss: 0.00117786
Iteration 13/1000 | Loss: 0.00128141
Iteration 14/1000 | Loss: 0.00253728
Iteration 15/1000 | Loss: 0.00248812
Iteration 16/1000 | Loss: 0.00079658
Iteration 17/1000 | Loss: 0.00050805
Iteration 18/1000 | Loss: 0.00064315
Iteration 19/1000 | Loss: 0.00039158
Iteration 20/1000 | Loss: 0.00044095
Iteration 21/1000 | Loss: 0.00078113
Iteration 22/1000 | Loss: 0.00027828
Iteration 23/1000 | Loss: 0.00034914
Iteration 24/1000 | Loss: 0.00044548
Iteration 25/1000 | Loss: 0.00044244
Iteration 26/1000 | Loss: 0.00019464
Iteration 27/1000 | Loss: 0.00035544
Iteration 28/1000 | Loss: 0.00016998
Iteration 29/1000 | Loss: 0.00099474
Iteration 30/1000 | Loss: 0.00497180
Iteration 31/1000 | Loss: 0.01276267
Iteration 32/1000 | Loss: 0.00420598
Iteration 33/1000 | Loss: 0.01270826
Iteration 34/1000 | Loss: 0.00353563
Iteration 35/1000 | Loss: 0.00209023
Iteration 36/1000 | Loss: 0.00575386
Iteration 37/1000 | Loss: 0.00270473
Iteration 38/1000 | Loss: 0.00244206
Iteration 39/1000 | Loss: 0.00420918
Iteration 40/1000 | Loss: 0.00377294
Iteration 41/1000 | Loss: 0.00416537
Iteration 42/1000 | Loss: 0.00386128
Iteration 43/1000 | Loss: 0.00323951
Iteration 44/1000 | Loss: 0.00507803
Iteration 45/1000 | Loss: 0.00506638
Iteration 46/1000 | Loss: 0.00203333
Iteration 47/1000 | Loss: 0.00173322
Iteration 48/1000 | Loss: 0.00420625
Iteration 49/1000 | Loss: 0.00100545
Iteration 50/1000 | Loss: 0.00169760
Iteration 51/1000 | Loss: 0.00108578
Iteration 52/1000 | Loss: 0.00143097
Iteration 53/1000 | Loss: 0.00259935
Iteration 54/1000 | Loss: 0.00523128
Iteration 55/1000 | Loss: 0.00177204
Iteration 56/1000 | Loss: 0.00132326
Iteration 57/1000 | Loss: 0.00233168
Iteration 58/1000 | Loss: 0.00279661
Iteration 59/1000 | Loss: 0.00181402
Iteration 60/1000 | Loss: 0.00065018
Iteration 61/1000 | Loss: 0.00068060
Iteration 62/1000 | Loss: 0.00063401
Iteration 63/1000 | Loss: 0.00033961
Iteration 64/1000 | Loss: 0.00162837
Iteration 65/1000 | Loss: 0.00066438
Iteration 66/1000 | Loss: 0.00130652
Iteration 67/1000 | Loss: 0.00093896
Iteration 68/1000 | Loss: 0.00041040
Iteration 69/1000 | Loss: 0.00072499
Iteration 70/1000 | Loss: 0.00047628
Iteration 71/1000 | Loss: 0.00032391
Iteration 72/1000 | Loss: 0.00107644
Iteration 73/1000 | Loss: 0.00077435
Iteration 74/1000 | Loss: 0.00083602
Iteration 75/1000 | Loss: 0.00021733
Iteration 76/1000 | Loss: 0.00037388
Iteration 77/1000 | Loss: 0.00045459
Iteration 78/1000 | Loss: 0.00061654
Iteration 79/1000 | Loss: 0.00095017
Iteration 80/1000 | Loss: 0.00007759
Iteration 81/1000 | Loss: 0.00006919
Iteration 82/1000 | Loss: 0.00013928
Iteration 83/1000 | Loss: 0.00013631
Iteration 84/1000 | Loss: 0.00005894
Iteration 85/1000 | Loss: 0.00005161
Iteration 86/1000 | Loss: 0.00020083
Iteration 87/1000 | Loss: 0.00154789
Iteration 88/1000 | Loss: 0.00011067
Iteration 89/1000 | Loss: 0.00035605
Iteration 90/1000 | Loss: 0.00004550
Iteration 91/1000 | Loss: 0.00005783
Iteration 92/1000 | Loss: 0.00004108
Iteration 93/1000 | Loss: 0.00033650
Iteration 94/1000 | Loss: 0.00016560
Iteration 95/1000 | Loss: 0.00022196
Iteration 96/1000 | Loss: 0.00029488
Iteration 97/1000 | Loss: 0.00034443
Iteration 98/1000 | Loss: 0.00032484
Iteration 99/1000 | Loss: 0.00035943
Iteration 100/1000 | Loss: 0.00012415
Iteration 101/1000 | Loss: 0.00040902
Iteration 102/1000 | Loss: 0.00031415
Iteration 103/1000 | Loss: 0.00068479
Iteration 104/1000 | Loss: 0.00010095
Iteration 105/1000 | Loss: 0.00032480
Iteration 106/1000 | Loss: 0.00023811
Iteration 107/1000 | Loss: 0.00006608
Iteration 108/1000 | Loss: 0.00004556
Iteration 109/1000 | Loss: 0.00007171
Iteration 110/1000 | Loss: 0.00008634
Iteration 111/1000 | Loss: 0.00081385
Iteration 112/1000 | Loss: 0.00042002
Iteration 113/1000 | Loss: 0.00009614
Iteration 114/1000 | Loss: 0.00015766
Iteration 115/1000 | Loss: 0.00039633
Iteration 116/1000 | Loss: 0.00069533
Iteration 117/1000 | Loss: 0.00024139
Iteration 118/1000 | Loss: 0.00016998
Iteration 119/1000 | Loss: 0.00004730
Iteration 120/1000 | Loss: 0.00018419
Iteration 121/1000 | Loss: 0.00020140
Iteration 122/1000 | Loss: 0.00011653
Iteration 123/1000 | Loss: 0.00014990
Iteration 124/1000 | Loss: 0.00003722
Iteration 125/1000 | Loss: 0.00008598
Iteration 126/1000 | Loss: 0.00015284
Iteration 127/1000 | Loss: 0.00021132
Iteration 128/1000 | Loss: 0.00013339
Iteration 129/1000 | Loss: 0.00084866
Iteration 130/1000 | Loss: 0.00024536
Iteration 131/1000 | Loss: 0.00038190
Iteration 132/1000 | Loss: 0.00022284
Iteration 133/1000 | Loss: 0.00042560
Iteration 134/1000 | Loss: 0.00038789
Iteration 135/1000 | Loss: 0.00028243
Iteration 136/1000 | Loss: 0.00014453
Iteration 137/1000 | Loss: 0.00012541
Iteration 138/1000 | Loss: 0.00014093
Iteration 139/1000 | Loss: 0.00014132
Iteration 140/1000 | Loss: 0.00007607
Iteration 141/1000 | Loss: 0.00003139
Iteration 142/1000 | Loss: 0.00005755
Iteration 143/1000 | Loss: 0.00002874
Iteration 144/1000 | Loss: 0.00002776
Iteration 145/1000 | Loss: 0.00002684
Iteration 146/1000 | Loss: 0.00002611
Iteration 147/1000 | Loss: 0.00002567
Iteration 148/1000 | Loss: 0.00002532
Iteration 149/1000 | Loss: 0.00025693
Iteration 150/1000 | Loss: 0.00029457
Iteration 151/1000 | Loss: 0.00017741
Iteration 152/1000 | Loss: 0.00019636
Iteration 153/1000 | Loss: 0.00011168
Iteration 154/1000 | Loss: 0.00016316
Iteration 155/1000 | Loss: 0.00010288
Iteration 156/1000 | Loss: 0.00083041
Iteration 157/1000 | Loss: 0.00028247
Iteration 158/1000 | Loss: 0.00004220
Iteration 159/1000 | Loss: 0.00004072
Iteration 160/1000 | Loss: 0.00002615
Iteration 161/1000 | Loss: 0.00012196
Iteration 162/1000 | Loss: 0.00007049
Iteration 163/1000 | Loss: 0.00004809
Iteration 164/1000 | Loss: 0.00002546
Iteration 165/1000 | Loss: 0.00004792
Iteration 166/1000 | Loss: 0.00017342
Iteration 167/1000 | Loss: 0.00004779
Iteration 168/1000 | Loss: 0.00018646
Iteration 169/1000 | Loss: 0.00008614
Iteration 170/1000 | Loss: 0.00015323
Iteration 171/1000 | Loss: 0.00003832
Iteration 172/1000 | Loss: 0.00010491
Iteration 173/1000 | Loss: 0.00002574
Iteration 174/1000 | Loss: 0.00002529
Iteration 175/1000 | Loss: 0.00002527
Iteration 176/1000 | Loss: 0.00002533
Iteration 177/1000 | Loss: 0.00010471
Iteration 178/1000 | Loss: 0.00003154
Iteration 179/1000 | Loss: 0.00002857
Iteration 180/1000 | Loss: 0.00017013
Iteration 181/1000 | Loss: 0.00003551
Iteration 182/1000 | Loss: 0.00002549
Iteration 183/1000 | Loss: 0.00002517
Iteration 184/1000 | Loss: 0.00002517
Iteration 185/1000 | Loss: 0.00002515
Iteration 186/1000 | Loss: 0.00002514
Iteration 187/1000 | Loss: 0.00002511
Iteration 188/1000 | Loss: 0.00002510
Iteration 189/1000 | Loss: 0.00002506
Iteration 190/1000 | Loss: 0.00002505
Iteration 191/1000 | Loss: 0.00002504
Iteration 192/1000 | Loss: 0.00002503
Iteration 193/1000 | Loss: 0.00002502
Iteration 194/1000 | Loss: 0.00002502
Iteration 195/1000 | Loss: 0.00002497
Iteration 196/1000 | Loss: 0.00008511
Iteration 197/1000 | Loss: 0.00004282
Iteration 198/1000 | Loss: 0.00009115
Iteration 199/1000 | Loss: 0.00003918
Iteration 200/1000 | Loss: 0.00009859
Iteration 201/1000 | Loss: 0.00012024
Iteration 202/1000 | Loss: 0.00007225
Iteration 203/1000 | Loss: 0.00010584
Iteration 204/1000 | Loss: 0.00004352
Iteration 205/1000 | Loss: 0.00005099
Iteration 206/1000 | Loss: 0.00002491
Iteration 207/1000 | Loss: 0.00002489
Iteration 208/1000 | Loss: 0.00002488
Iteration 209/1000 | Loss: 0.00002487
Iteration 210/1000 | Loss: 0.00002487
Iteration 211/1000 | Loss: 0.00002487
Iteration 212/1000 | Loss: 0.00002486
Iteration 213/1000 | Loss: 0.00002486
Iteration 214/1000 | Loss: 0.00002485
Iteration 215/1000 | Loss: 0.00002485
Iteration 216/1000 | Loss: 0.00002484
Iteration 217/1000 | Loss: 0.00002484
Iteration 218/1000 | Loss: 0.00002484
Iteration 219/1000 | Loss: 0.00002484
Iteration 220/1000 | Loss: 0.00002483
Iteration 221/1000 | Loss: 0.00002483
Iteration 222/1000 | Loss: 0.00002483
Iteration 223/1000 | Loss: 0.00002482
Iteration 224/1000 | Loss: 0.00002481
Iteration 225/1000 | Loss: 0.00005681
Iteration 226/1000 | Loss: 0.00008282
Iteration 227/1000 | Loss: 0.00006701
Iteration 228/1000 | Loss: 0.00003429
Iteration 229/1000 | Loss: 0.00002484
Iteration 230/1000 | Loss: 0.00007608
Iteration 231/1000 | Loss: 0.00016862
Iteration 232/1000 | Loss: 0.00011643
Iteration 233/1000 | Loss: 0.00004946
Iteration 234/1000 | Loss: 0.00008727
Iteration 235/1000 | Loss: 0.00007210
Iteration 236/1000 | Loss: 0.00012146
Iteration 237/1000 | Loss: 0.00032100
Iteration 238/1000 | Loss: 0.00006960
Iteration 239/1000 | Loss: 0.00010599
Iteration 240/1000 | Loss: 0.00003630
Iteration 241/1000 | Loss: 0.00008494
Iteration 242/1000 | Loss: 0.00025001
Iteration 243/1000 | Loss: 0.00015538
Iteration 244/1000 | Loss: 0.00028182
Iteration 245/1000 | Loss: 0.00006386
Iteration 246/1000 | Loss: 0.00003073
Iteration 247/1000 | Loss: 0.00009145
Iteration 248/1000 | Loss: 0.00006434
Iteration 249/1000 | Loss: 0.00005553
Iteration 250/1000 | Loss: 0.00006783
Iteration 251/1000 | Loss: 0.00007507
Iteration 252/1000 | Loss: 0.00009923
Iteration 253/1000 | Loss: 0.00010737
Iteration 254/1000 | Loss: 0.00013874
Iteration 255/1000 | Loss: 0.00003783
Iteration 256/1000 | Loss: 0.00020523
Iteration 257/1000 | Loss: 0.00013756
Iteration 258/1000 | Loss: 0.00010571
Iteration 259/1000 | Loss: 0.00016650
Iteration 260/1000 | Loss: 0.00012329
Iteration 261/1000 | Loss: 0.00017116
Iteration 262/1000 | Loss: 0.00029920
Iteration 263/1000 | Loss: 0.00007026
Iteration 264/1000 | Loss: 0.00003199
Iteration 265/1000 | Loss: 0.00015730
Iteration 266/1000 | Loss: 0.00003290
Iteration 267/1000 | Loss: 0.00003243
Iteration 268/1000 | Loss: 0.00009594
Iteration 269/1000 | Loss: 0.00003318
Iteration 270/1000 | Loss: 0.00002373
Iteration 271/1000 | Loss: 0.00005883
Iteration 272/1000 | Loss: 0.00005782
Iteration 273/1000 | Loss: 0.00005417
Iteration 274/1000 | Loss: 0.00002567
Iteration 275/1000 | Loss: 0.00002413
Iteration 276/1000 | Loss: 0.00002265
Iteration 277/1000 | Loss: 0.00002255
Iteration 278/1000 | Loss: 0.00002230
Iteration 279/1000 | Loss: 0.00002217
Iteration 280/1000 | Loss: 0.00002216
Iteration 281/1000 | Loss: 0.00002216
Iteration 282/1000 | Loss: 0.00002214
Iteration 283/1000 | Loss: 0.00002213
Iteration 284/1000 | Loss: 0.00002213
Iteration 285/1000 | Loss: 0.00002213
Iteration 286/1000 | Loss: 0.00002213
Iteration 287/1000 | Loss: 0.00002212
Iteration 288/1000 | Loss: 0.00002212
Iteration 289/1000 | Loss: 0.00002212
Iteration 290/1000 | Loss: 0.00002212
Iteration 291/1000 | Loss: 0.00002211
Iteration 292/1000 | Loss: 0.00002211
Iteration 293/1000 | Loss: 0.00002211
Iteration 294/1000 | Loss: 0.00002211
Iteration 295/1000 | Loss: 0.00002211
Iteration 296/1000 | Loss: 0.00002210
Iteration 297/1000 | Loss: 0.00002210
Iteration 298/1000 | Loss: 0.00002225
Iteration 299/1000 | Loss: 0.00002222
Iteration 300/1000 | Loss: 0.00002221
Iteration 301/1000 | Loss: 0.00002221
Iteration 302/1000 | Loss: 0.00002221
Iteration 303/1000 | Loss: 0.00002221
Iteration 304/1000 | Loss: 0.00002221
Iteration 305/1000 | Loss: 0.00002220
Iteration 306/1000 | Loss: 0.00002220
Iteration 307/1000 | Loss: 0.00002220
Iteration 308/1000 | Loss: 0.00002220
Iteration 309/1000 | Loss: 0.00002219
Iteration 310/1000 | Loss: 0.00002219
Iteration 311/1000 | Loss: 0.00002219
Iteration 312/1000 | Loss: 0.00002218
Iteration 313/1000 | Loss: 0.00002218
Iteration 314/1000 | Loss: 0.00002218
Iteration 315/1000 | Loss: 0.00002211
Iteration 316/1000 | Loss: 0.00002210
Iteration 317/1000 | Loss: 0.00002210
Iteration 318/1000 | Loss: 0.00002206
Iteration 319/1000 | Loss: 0.00006865
Iteration 320/1000 | Loss: 0.00002329
Iteration 321/1000 | Loss: 0.00002222
Iteration 322/1000 | Loss: 0.00002191
Iteration 323/1000 | Loss: 0.00002201
Iteration 324/1000 | Loss: 0.00002196
Iteration 325/1000 | Loss: 0.00002196
Iteration 326/1000 | Loss: 0.00002196
Iteration 327/1000 | Loss: 0.00002196
Iteration 328/1000 | Loss: 0.00002195
Iteration 329/1000 | Loss: 0.00002194
Iteration 330/1000 | Loss: 0.00002194
Iteration 331/1000 | Loss: 0.00002194
Iteration 332/1000 | Loss: 0.00002194
Iteration 333/1000 | Loss: 0.00002193
Iteration 334/1000 | Loss: 0.00002193
Iteration 335/1000 | Loss: 0.00002193
Iteration 336/1000 | Loss: 0.00002193
Iteration 337/1000 | Loss: 0.00002193
Iteration 338/1000 | Loss: 0.00002193
Iteration 339/1000 | Loss: 0.00002193
Iteration 340/1000 | Loss: 0.00002193
Iteration 341/1000 | Loss: 0.00002193
Iteration 342/1000 | Loss: 0.00002192
Iteration 343/1000 | Loss: 0.00002192
Iteration 344/1000 | Loss: 0.00002192
Iteration 345/1000 | Loss: 0.00002192
Iteration 346/1000 | Loss: 0.00002191
Iteration 347/1000 | Loss: 0.00002191
Iteration 348/1000 | Loss: 0.00002191
Iteration 349/1000 | Loss: 0.00002190
Iteration 350/1000 | Loss: 0.00002190
Iteration 351/1000 | Loss: 0.00002190
Iteration 352/1000 | Loss: 0.00002189
Iteration 353/1000 | Loss: 0.00002189
Iteration 354/1000 | Loss: 0.00002189
Iteration 355/1000 | Loss: 0.00002188
Iteration 356/1000 | Loss: 0.00002188
Iteration 357/1000 | Loss: 0.00002188
Iteration 358/1000 | Loss: 0.00002188
Iteration 359/1000 | Loss: 0.00002187
Iteration 360/1000 | Loss: 0.00002187
Iteration 361/1000 | Loss: 0.00002187
Iteration 362/1000 | Loss: 0.00002187
Iteration 363/1000 | Loss: 0.00002187
Iteration 364/1000 | Loss: 0.00002187
Iteration 365/1000 | Loss: 0.00002187
Iteration 366/1000 | Loss: 0.00002187
Iteration 367/1000 | Loss: 0.00002187
Iteration 368/1000 | Loss: 0.00002187
Iteration 369/1000 | Loss: 0.00002187
Iteration 370/1000 | Loss: 0.00002186
Iteration 371/1000 | Loss: 0.00002186
Iteration 372/1000 | Loss: 0.00002186
Iteration 373/1000 | Loss: 0.00002186
Iteration 374/1000 | Loss: 0.00002185
Iteration 375/1000 | Loss: 0.00002185
Iteration 376/1000 | Loss: 0.00002185
Iteration 377/1000 | Loss: 0.00002185
Iteration 378/1000 | Loss: 0.00002185
Iteration 379/1000 | Loss: 0.00002185
Iteration 380/1000 | Loss: 0.00002185
Iteration 381/1000 | Loss: 0.00002185
Iteration 382/1000 | Loss: 0.00002185
Iteration 383/1000 | Loss: 0.00002185
Iteration 384/1000 | Loss: 0.00002184
Iteration 385/1000 | Loss: 0.00002184
Iteration 386/1000 | Loss: 0.00002184
Iteration 387/1000 | Loss: 0.00002184
Iteration 388/1000 | Loss: 0.00002184
Iteration 389/1000 | Loss: 0.00002184
Iteration 390/1000 | Loss: 0.00002184
Iteration 391/1000 | Loss: 0.00002184
Iteration 392/1000 | Loss: 0.00002184
Iteration 393/1000 | Loss: 0.00002184
Iteration 394/1000 | Loss: 0.00002184
Iteration 395/1000 | Loss: 0.00002184
Iteration 396/1000 | Loss: 0.00002184
Iteration 397/1000 | Loss: 0.00002184
Iteration 398/1000 | Loss: 0.00002184
Iteration 399/1000 | Loss: 0.00002184
Iteration 400/1000 | Loss: 0.00002184
Iteration 401/1000 | Loss: 0.00002184
Iteration 402/1000 | Loss: 0.00002184
Iteration 403/1000 | Loss: 0.00002184
Iteration 404/1000 | Loss: 0.00002184
Iteration 405/1000 | Loss: 0.00002184
Iteration 406/1000 | Loss: 0.00002184
Iteration 407/1000 | Loss: 0.00002184
Iteration 408/1000 | Loss: 0.00002184
Iteration 409/1000 | Loss: 0.00002184
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 409. Stopping optimization.
Last 5 losses: [2.1839161490788683e-05, 2.1839161490788683e-05, 2.1839161490788683e-05, 2.1839161490788683e-05, 2.1839161490788683e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1839161490788683e-05

Optimization complete. Final v2v error: 3.6831753253936768 mm

Highest mean error: 9.69332218170166 mm for frame 56

Lowest mean error: 2.896193742752075 mm for frame 178

Saving results

Total time: 462.8194103240967
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00511428
Iteration 2/25 | Loss: 0.00164971
Iteration 3/25 | Loss: 0.00156389
Iteration 4/25 | Loss: 0.00155589
Iteration 5/25 | Loss: 0.00155370
Iteration 6/25 | Loss: 0.00155370
Iteration 7/25 | Loss: 0.00155370
Iteration 8/25 | Loss: 0.00155370
Iteration 9/25 | Loss: 0.00155370
Iteration 10/25 | Loss: 0.00155370
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001553699723444879, 0.001553699723444879, 0.001553699723444879, 0.001553699723444879, 0.001553699723444879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001553699723444879

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18764138
Iteration 2/25 | Loss: 0.00261070
Iteration 3/25 | Loss: 0.00261070
Iteration 4/25 | Loss: 0.00261070
Iteration 5/25 | Loss: 0.00261070
Iteration 6/25 | Loss: 0.00261070
Iteration 7/25 | Loss: 0.00261070
Iteration 8/25 | Loss: 0.00261069
Iteration 9/25 | Loss: 0.00261069
Iteration 10/25 | Loss: 0.00261069
Iteration 11/25 | Loss: 0.00261069
Iteration 12/25 | Loss: 0.00261069
Iteration 13/25 | Loss: 0.00261069
Iteration 14/25 | Loss: 0.00261069
Iteration 15/25 | Loss: 0.00261069
Iteration 16/25 | Loss: 0.00261069
Iteration 17/25 | Loss: 0.00261069
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.002610693918541074, 0.002610693918541074, 0.002610693918541074, 0.002610693918541074, 0.002610693918541074]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002610693918541074

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00261069
Iteration 2/1000 | Loss: 0.00004622
Iteration 3/1000 | Loss: 0.00003167
Iteration 4/1000 | Loss: 0.00002712
Iteration 5/1000 | Loss: 0.00002535
Iteration 6/1000 | Loss: 0.00002428
Iteration 7/1000 | Loss: 0.00002371
Iteration 8/1000 | Loss: 0.00002313
Iteration 9/1000 | Loss: 0.00002279
Iteration 10/1000 | Loss: 0.00002259
Iteration 11/1000 | Loss: 0.00002251
Iteration 12/1000 | Loss: 0.00002238
Iteration 13/1000 | Loss: 0.00002232
Iteration 14/1000 | Loss: 0.00002226
Iteration 15/1000 | Loss: 0.00002223
Iteration 16/1000 | Loss: 0.00002218
Iteration 17/1000 | Loss: 0.00002218
Iteration 18/1000 | Loss: 0.00002218
Iteration 19/1000 | Loss: 0.00002218
Iteration 20/1000 | Loss: 0.00002216
Iteration 21/1000 | Loss: 0.00002216
Iteration 22/1000 | Loss: 0.00002215
Iteration 23/1000 | Loss: 0.00002215
Iteration 24/1000 | Loss: 0.00002215
Iteration 25/1000 | Loss: 0.00002214
Iteration 26/1000 | Loss: 0.00002214
Iteration 27/1000 | Loss: 0.00002214
Iteration 28/1000 | Loss: 0.00002214
Iteration 29/1000 | Loss: 0.00002213
Iteration 30/1000 | Loss: 0.00002212
Iteration 31/1000 | Loss: 0.00002211
Iteration 32/1000 | Loss: 0.00002211
Iteration 33/1000 | Loss: 0.00002211
Iteration 34/1000 | Loss: 0.00002211
Iteration 35/1000 | Loss: 0.00002211
Iteration 36/1000 | Loss: 0.00002211
Iteration 37/1000 | Loss: 0.00002211
Iteration 38/1000 | Loss: 0.00002211
Iteration 39/1000 | Loss: 0.00002210
Iteration 40/1000 | Loss: 0.00002210
Iteration 41/1000 | Loss: 0.00002210
Iteration 42/1000 | Loss: 0.00002209
Iteration 43/1000 | Loss: 0.00002209
Iteration 44/1000 | Loss: 0.00002208
Iteration 45/1000 | Loss: 0.00002208
Iteration 46/1000 | Loss: 0.00002208
Iteration 47/1000 | Loss: 0.00002208
Iteration 48/1000 | Loss: 0.00002208
Iteration 49/1000 | Loss: 0.00002208
Iteration 50/1000 | Loss: 0.00002208
Iteration 51/1000 | Loss: 0.00002208
Iteration 52/1000 | Loss: 0.00002208
Iteration 53/1000 | Loss: 0.00002208
Iteration 54/1000 | Loss: 0.00002208
Iteration 55/1000 | Loss: 0.00002207
Iteration 56/1000 | Loss: 0.00002207
Iteration 57/1000 | Loss: 0.00002207
Iteration 58/1000 | Loss: 0.00002207
Iteration 59/1000 | Loss: 0.00002206
Iteration 60/1000 | Loss: 0.00002206
Iteration 61/1000 | Loss: 0.00002204
Iteration 62/1000 | Loss: 0.00002204
Iteration 63/1000 | Loss: 0.00002204
Iteration 64/1000 | Loss: 0.00002204
Iteration 65/1000 | Loss: 0.00002204
Iteration 66/1000 | Loss: 0.00002203
Iteration 67/1000 | Loss: 0.00002203
Iteration 68/1000 | Loss: 0.00002203
Iteration 69/1000 | Loss: 0.00002203
Iteration 70/1000 | Loss: 0.00002202
Iteration 71/1000 | Loss: 0.00002202
Iteration 72/1000 | Loss: 0.00002202
Iteration 73/1000 | Loss: 0.00002201
Iteration 74/1000 | Loss: 0.00002201
Iteration 75/1000 | Loss: 0.00002201
Iteration 76/1000 | Loss: 0.00002201
Iteration 77/1000 | Loss: 0.00002201
Iteration 78/1000 | Loss: 0.00002201
Iteration 79/1000 | Loss: 0.00002200
Iteration 80/1000 | Loss: 0.00002200
Iteration 81/1000 | Loss: 0.00002200
Iteration 82/1000 | Loss: 0.00002200
Iteration 83/1000 | Loss: 0.00002200
Iteration 84/1000 | Loss: 0.00002200
Iteration 85/1000 | Loss: 0.00002200
Iteration 86/1000 | Loss: 0.00002200
Iteration 87/1000 | Loss: 0.00002200
Iteration 88/1000 | Loss: 0.00002199
Iteration 89/1000 | Loss: 0.00002199
Iteration 90/1000 | Loss: 0.00002199
Iteration 91/1000 | Loss: 0.00002199
Iteration 92/1000 | Loss: 0.00002199
Iteration 93/1000 | Loss: 0.00002199
Iteration 94/1000 | Loss: 0.00002198
Iteration 95/1000 | Loss: 0.00002198
Iteration 96/1000 | Loss: 0.00002198
Iteration 97/1000 | Loss: 0.00002198
Iteration 98/1000 | Loss: 0.00002198
Iteration 99/1000 | Loss: 0.00002198
Iteration 100/1000 | Loss: 0.00002198
Iteration 101/1000 | Loss: 0.00002198
Iteration 102/1000 | Loss: 0.00002197
Iteration 103/1000 | Loss: 0.00002197
Iteration 104/1000 | Loss: 0.00002197
Iteration 105/1000 | Loss: 0.00002197
Iteration 106/1000 | Loss: 0.00002197
Iteration 107/1000 | Loss: 0.00002196
Iteration 108/1000 | Loss: 0.00002196
Iteration 109/1000 | Loss: 0.00002196
Iteration 110/1000 | Loss: 0.00002196
Iteration 111/1000 | Loss: 0.00002196
Iteration 112/1000 | Loss: 0.00002196
Iteration 113/1000 | Loss: 0.00002196
Iteration 114/1000 | Loss: 0.00002196
Iteration 115/1000 | Loss: 0.00002196
Iteration 116/1000 | Loss: 0.00002196
Iteration 117/1000 | Loss: 0.00002196
Iteration 118/1000 | Loss: 0.00002196
Iteration 119/1000 | Loss: 0.00002196
Iteration 120/1000 | Loss: 0.00002196
Iteration 121/1000 | Loss: 0.00002196
Iteration 122/1000 | Loss: 0.00002196
Iteration 123/1000 | Loss: 0.00002196
Iteration 124/1000 | Loss: 0.00002196
Iteration 125/1000 | Loss: 0.00002196
Iteration 126/1000 | Loss: 0.00002196
Iteration 127/1000 | Loss: 0.00002196
Iteration 128/1000 | Loss: 0.00002196
Iteration 129/1000 | Loss: 0.00002196
Iteration 130/1000 | Loss: 0.00002196
Iteration 131/1000 | Loss: 0.00002196
Iteration 132/1000 | Loss: 0.00002196
Iteration 133/1000 | Loss: 0.00002196
Iteration 134/1000 | Loss: 0.00002196
Iteration 135/1000 | Loss: 0.00002196
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [2.1957403077976778e-05, 2.1957403077976778e-05, 2.1957403077976778e-05, 2.1957403077976778e-05, 2.1957403077976778e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1957403077976778e-05

Optimization complete. Final v2v error: 4.00321626663208 mm

Highest mean error: 4.384488105773926 mm for frame 30

Lowest mean error: 3.6275691986083984 mm for frame 16

Saving results

Total time: 37.46410393714905
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00485996
Iteration 2/25 | Loss: 0.00168029
Iteration 3/25 | Loss: 0.00158189
Iteration 4/25 | Loss: 0.00156471
Iteration 5/25 | Loss: 0.00155748
Iteration 6/25 | Loss: 0.00155614
Iteration 7/25 | Loss: 0.00155588
Iteration 8/25 | Loss: 0.00155588
Iteration 9/25 | Loss: 0.00155588
Iteration 10/25 | Loss: 0.00155588
Iteration 11/25 | Loss: 0.00155588
Iteration 12/25 | Loss: 0.00155588
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015558847226202488, 0.0015558847226202488, 0.0015558847226202488, 0.0015558847226202488, 0.0015558847226202488]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015558847226202488

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16258132
Iteration 2/25 | Loss: 0.00306753
Iteration 3/25 | Loss: 0.00306752
Iteration 4/25 | Loss: 0.00306752
Iteration 5/25 | Loss: 0.00306752
Iteration 6/25 | Loss: 0.00306752
Iteration 7/25 | Loss: 0.00306752
Iteration 8/25 | Loss: 0.00306752
Iteration 9/25 | Loss: 0.00306752
Iteration 10/25 | Loss: 0.00306752
Iteration 11/25 | Loss: 0.00306752
Iteration 12/25 | Loss: 0.00306752
Iteration 13/25 | Loss: 0.00306752
Iteration 14/25 | Loss: 0.00306752
Iteration 15/25 | Loss: 0.00306752
Iteration 16/25 | Loss: 0.00306752
Iteration 17/25 | Loss: 0.00306752
Iteration 18/25 | Loss: 0.00306752
Iteration 19/25 | Loss: 0.00306752
Iteration 20/25 | Loss: 0.00306752
Iteration 21/25 | Loss: 0.00306752
Iteration 22/25 | Loss: 0.00306752
Iteration 23/25 | Loss: 0.00306752
Iteration 24/25 | Loss: 0.00306752
Iteration 25/25 | Loss: 0.00306752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00306752
Iteration 2/1000 | Loss: 0.00004808
Iteration 3/1000 | Loss: 0.00003437
Iteration 4/1000 | Loss: 0.00002938
Iteration 5/1000 | Loss: 0.00002681
Iteration 6/1000 | Loss: 0.00002576
Iteration 7/1000 | Loss: 0.00002501
Iteration 8/1000 | Loss: 0.00002456
Iteration 9/1000 | Loss: 0.00002425
Iteration 10/1000 | Loss: 0.00002396
Iteration 11/1000 | Loss: 0.00002376
Iteration 12/1000 | Loss: 0.00002375
Iteration 13/1000 | Loss: 0.00002375
Iteration 14/1000 | Loss: 0.00002363
Iteration 15/1000 | Loss: 0.00002359
Iteration 16/1000 | Loss: 0.00002358
Iteration 17/1000 | Loss: 0.00002357
Iteration 18/1000 | Loss: 0.00002357
Iteration 19/1000 | Loss: 0.00002356
Iteration 20/1000 | Loss: 0.00002350
Iteration 21/1000 | Loss: 0.00002348
Iteration 22/1000 | Loss: 0.00002346
Iteration 23/1000 | Loss: 0.00002346
Iteration 24/1000 | Loss: 0.00002346
Iteration 25/1000 | Loss: 0.00002346
Iteration 26/1000 | Loss: 0.00002346
Iteration 27/1000 | Loss: 0.00002346
Iteration 28/1000 | Loss: 0.00002346
Iteration 29/1000 | Loss: 0.00002346
Iteration 30/1000 | Loss: 0.00002346
Iteration 31/1000 | Loss: 0.00002342
Iteration 32/1000 | Loss: 0.00002342
Iteration 33/1000 | Loss: 0.00002342
Iteration 34/1000 | Loss: 0.00002342
Iteration 35/1000 | Loss: 0.00002342
Iteration 36/1000 | Loss: 0.00002341
Iteration 37/1000 | Loss: 0.00002340
Iteration 38/1000 | Loss: 0.00002340
Iteration 39/1000 | Loss: 0.00002340
Iteration 40/1000 | Loss: 0.00002339
Iteration 41/1000 | Loss: 0.00002339
Iteration 42/1000 | Loss: 0.00002339
Iteration 43/1000 | Loss: 0.00002339
Iteration 44/1000 | Loss: 0.00002338
Iteration 45/1000 | Loss: 0.00002338
Iteration 46/1000 | Loss: 0.00002338
Iteration 47/1000 | Loss: 0.00002338
Iteration 48/1000 | Loss: 0.00002338
Iteration 49/1000 | Loss: 0.00002337
Iteration 50/1000 | Loss: 0.00002337
Iteration 51/1000 | Loss: 0.00002337
Iteration 52/1000 | Loss: 0.00002337
Iteration 53/1000 | Loss: 0.00002337
Iteration 54/1000 | Loss: 0.00002336
Iteration 55/1000 | Loss: 0.00002336
Iteration 56/1000 | Loss: 0.00002336
Iteration 57/1000 | Loss: 0.00002336
Iteration 58/1000 | Loss: 0.00002336
Iteration 59/1000 | Loss: 0.00002336
Iteration 60/1000 | Loss: 0.00002335
Iteration 61/1000 | Loss: 0.00002335
Iteration 62/1000 | Loss: 0.00002335
Iteration 63/1000 | Loss: 0.00002335
Iteration 64/1000 | Loss: 0.00002335
Iteration 65/1000 | Loss: 0.00002334
Iteration 66/1000 | Loss: 0.00002334
Iteration 67/1000 | Loss: 0.00002334
Iteration 68/1000 | Loss: 0.00002334
Iteration 69/1000 | Loss: 0.00002334
Iteration 70/1000 | Loss: 0.00002334
Iteration 71/1000 | Loss: 0.00002334
Iteration 72/1000 | Loss: 0.00002334
Iteration 73/1000 | Loss: 0.00002333
Iteration 74/1000 | Loss: 0.00002333
Iteration 75/1000 | Loss: 0.00002333
Iteration 76/1000 | Loss: 0.00002333
Iteration 77/1000 | Loss: 0.00002333
Iteration 78/1000 | Loss: 0.00002333
Iteration 79/1000 | Loss: 0.00002333
Iteration 80/1000 | Loss: 0.00002332
Iteration 81/1000 | Loss: 0.00002332
Iteration 82/1000 | Loss: 0.00002332
Iteration 83/1000 | Loss: 0.00002332
Iteration 84/1000 | Loss: 0.00002331
Iteration 85/1000 | Loss: 0.00002331
Iteration 86/1000 | Loss: 0.00002331
Iteration 87/1000 | Loss: 0.00002331
Iteration 88/1000 | Loss: 0.00002331
Iteration 89/1000 | Loss: 0.00002331
Iteration 90/1000 | Loss: 0.00002331
Iteration 91/1000 | Loss: 0.00002331
Iteration 92/1000 | Loss: 0.00002331
Iteration 93/1000 | Loss: 0.00002330
Iteration 94/1000 | Loss: 0.00002330
Iteration 95/1000 | Loss: 0.00002330
Iteration 96/1000 | Loss: 0.00002329
Iteration 97/1000 | Loss: 0.00002329
Iteration 98/1000 | Loss: 0.00002329
Iteration 99/1000 | Loss: 0.00002328
Iteration 100/1000 | Loss: 0.00002328
Iteration 101/1000 | Loss: 0.00002328
Iteration 102/1000 | Loss: 0.00002328
Iteration 103/1000 | Loss: 0.00002328
Iteration 104/1000 | Loss: 0.00002328
Iteration 105/1000 | Loss: 0.00002328
Iteration 106/1000 | Loss: 0.00002328
Iteration 107/1000 | Loss: 0.00002327
Iteration 108/1000 | Loss: 0.00002327
Iteration 109/1000 | Loss: 0.00002327
Iteration 110/1000 | Loss: 0.00002327
Iteration 111/1000 | Loss: 0.00002327
Iteration 112/1000 | Loss: 0.00002327
Iteration 113/1000 | Loss: 0.00002327
Iteration 114/1000 | Loss: 0.00002327
Iteration 115/1000 | Loss: 0.00002327
Iteration 116/1000 | Loss: 0.00002327
Iteration 117/1000 | Loss: 0.00002327
Iteration 118/1000 | Loss: 0.00002327
Iteration 119/1000 | Loss: 0.00002327
Iteration 120/1000 | Loss: 0.00002326
Iteration 121/1000 | Loss: 0.00002326
Iteration 122/1000 | Loss: 0.00002326
Iteration 123/1000 | Loss: 0.00002326
Iteration 124/1000 | Loss: 0.00002326
Iteration 125/1000 | Loss: 0.00002326
Iteration 126/1000 | Loss: 0.00002326
Iteration 127/1000 | Loss: 0.00002326
Iteration 128/1000 | Loss: 0.00002326
Iteration 129/1000 | Loss: 0.00002326
Iteration 130/1000 | Loss: 0.00002326
Iteration 131/1000 | Loss: 0.00002326
Iteration 132/1000 | Loss: 0.00002326
Iteration 133/1000 | Loss: 0.00002326
Iteration 134/1000 | Loss: 0.00002326
Iteration 135/1000 | Loss: 0.00002326
Iteration 136/1000 | Loss: 0.00002326
Iteration 137/1000 | Loss: 0.00002326
Iteration 138/1000 | Loss: 0.00002326
Iteration 139/1000 | Loss: 0.00002326
Iteration 140/1000 | Loss: 0.00002326
Iteration 141/1000 | Loss: 0.00002326
Iteration 142/1000 | Loss: 0.00002326
Iteration 143/1000 | Loss: 0.00002326
Iteration 144/1000 | Loss: 0.00002326
Iteration 145/1000 | Loss: 0.00002326
Iteration 146/1000 | Loss: 0.00002326
Iteration 147/1000 | Loss: 0.00002326
Iteration 148/1000 | Loss: 0.00002326
Iteration 149/1000 | Loss: 0.00002326
Iteration 150/1000 | Loss: 0.00002326
Iteration 151/1000 | Loss: 0.00002326
Iteration 152/1000 | Loss: 0.00002326
Iteration 153/1000 | Loss: 0.00002326
Iteration 154/1000 | Loss: 0.00002326
Iteration 155/1000 | Loss: 0.00002326
Iteration 156/1000 | Loss: 0.00002326
Iteration 157/1000 | Loss: 0.00002326
Iteration 158/1000 | Loss: 0.00002326
Iteration 159/1000 | Loss: 0.00002326
Iteration 160/1000 | Loss: 0.00002326
Iteration 161/1000 | Loss: 0.00002326
Iteration 162/1000 | Loss: 0.00002326
Iteration 163/1000 | Loss: 0.00002326
Iteration 164/1000 | Loss: 0.00002326
Iteration 165/1000 | Loss: 0.00002326
Iteration 166/1000 | Loss: 0.00002326
Iteration 167/1000 | Loss: 0.00002326
Iteration 168/1000 | Loss: 0.00002326
Iteration 169/1000 | Loss: 0.00002326
Iteration 170/1000 | Loss: 0.00002326
Iteration 171/1000 | Loss: 0.00002326
Iteration 172/1000 | Loss: 0.00002326
Iteration 173/1000 | Loss: 0.00002326
Iteration 174/1000 | Loss: 0.00002326
Iteration 175/1000 | Loss: 0.00002326
Iteration 176/1000 | Loss: 0.00002326
Iteration 177/1000 | Loss: 0.00002326
Iteration 178/1000 | Loss: 0.00002326
Iteration 179/1000 | Loss: 0.00002326
Iteration 180/1000 | Loss: 0.00002326
Iteration 181/1000 | Loss: 0.00002326
Iteration 182/1000 | Loss: 0.00002326
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 182. Stopping optimization.
Last 5 losses: [2.3258991859620437e-05, 2.3258991859620437e-05, 2.3258991859620437e-05, 2.3258991859620437e-05, 2.3258991859620437e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.3258991859620437e-05

Optimization complete. Final v2v error: 4.005203723907471 mm

Highest mean error: 5.1864447593688965 mm for frame 30

Lowest mean error: 3.76082181930542 mm for frame 58

Saving results

Total time: 36.57666230201721
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00341574
Iteration 2/25 | Loss: 0.00178002
Iteration 3/25 | Loss: 0.00164875
Iteration 4/25 | Loss: 0.00163118
Iteration 5/25 | Loss: 0.00162631
Iteration 6/25 | Loss: 0.00162522
Iteration 7/25 | Loss: 0.00162462
Iteration 8/25 | Loss: 0.00162462
Iteration 9/25 | Loss: 0.00162462
Iteration 10/25 | Loss: 0.00162462
Iteration 11/25 | Loss: 0.00162462
Iteration 12/25 | Loss: 0.00162462
Iteration 13/25 | Loss: 0.00162462
Iteration 14/25 | Loss: 0.00162462
Iteration 15/25 | Loss: 0.00162462
Iteration 16/25 | Loss: 0.00162462
Iteration 17/25 | Loss: 0.00162462
Iteration 18/25 | Loss: 0.00162462
Iteration 19/25 | Loss: 0.00162462
Iteration 20/25 | Loss: 0.00162462
Iteration 21/25 | Loss: 0.00162462
Iteration 22/25 | Loss: 0.00162462
Iteration 23/25 | Loss: 0.00162462
Iteration 24/25 | Loss: 0.00162462
Iteration 25/25 | Loss: 0.00162462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.10585535
Iteration 2/25 | Loss: 0.00437944
Iteration 3/25 | Loss: 0.00437944
Iteration 4/25 | Loss: 0.00437944
Iteration 5/25 | Loss: 0.00437944
Iteration 6/25 | Loss: 0.00437944
Iteration 7/25 | Loss: 0.00437944
Iteration 8/25 | Loss: 0.00437944
Iteration 9/25 | Loss: 0.00437944
Iteration 10/25 | Loss: 0.00437944
Iteration 11/25 | Loss: 0.00437944
Iteration 12/25 | Loss: 0.00437944
Iteration 13/25 | Loss: 0.00437943
Iteration 14/25 | Loss: 0.00437943
Iteration 15/25 | Loss: 0.00437943
Iteration 16/25 | Loss: 0.00437943
Iteration 17/25 | Loss: 0.00437943
Iteration 18/25 | Loss: 0.00437943
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00437943497672677, 0.00437943497672677, 0.00437943497672677, 0.00437943497672677, 0.00437943497672677]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00437943497672677

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00437943
Iteration 2/1000 | Loss: 0.00008050
Iteration 3/1000 | Loss: 0.00004719
Iteration 4/1000 | Loss: 0.00003789
Iteration 5/1000 | Loss: 0.00003251
Iteration 6/1000 | Loss: 0.00003013
Iteration 7/1000 | Loss: 0.00002795
Iteration 8/1000 | Loss: 0.00002691
Iteration 9/1000 | Loss: 0.00002611
Iteration 10/1000 | Loss: 0.00002551
Iteration 11/1000 | Loss: 0.00002502
Iteration 12/1000 | Loss: 0.00002453
Iteration 13/1000 | Loss: 0.00002406
Iteration 14/1000 | Loss: 0.00002372
Iteration 15/1000 | Loss: 0.00002352
Iteration 16/1000 | Loss: 0.00002333
Iteration 17/1000 | Loss: 0.00002327
Iteration 18/1000 | Loss: 0.00002326
Iteration 19/1000 | Loss: 0.00002314
Iteration 20/1000 | Loss: 0.00002313
Iteration 21/1000 | Loss: 0.00002313
Iteration 22/1000 | Loss: 0.00002311
Iteration 23/1000 | Loss: 0.00002311
Iteration 24/1000 | Loss: 0.00002310
Iteration 25/1000 | Loss: 0.00002310
Iteration 26/1000 | Loss: 0.00002309
Iteration 27/1000 | Loss: 0.00002309
Iteration 28/1000 | Loss: 0.00002309
Iteration 29/1000 | Loss: 0.00002309
Iteration 30/1000 | Loss: 0.00002308
Iteration 31/1000 | Loss: 0.00002308
Iteration 32/1000 | Loss: 0.00002308
Iteration 33/1000 | Loss: 0.00002307
Iteration 34/1000 | Loss: 0.00002306
Iteration 35/1000 | Loss: 0.00002306
Iteration 36/1000 | Loss: 0.00002305
Iteration 37/1000 | Loss: 0.00002305
Iteration 38/1000 | Loss: 0.00002304
Iteration 39/1000 | Loss: 0.00002303
Iteration 40/1000 | Loss: 0.00002303
Iteration 41/1000 | Loss: 0.00002302
Iteration 42/1000 | Loss: 0.00002302
Iteration 43/1000 | Loss: 0.00002302
Iteration 44/1000 | Loss: 0.00002301
Iteration 45/1000 | Loss: 0.00002301
Iteration 46/1000 | Loss: 0.00002301
Iteration 47/1000 | Loss: 0.00002301
Iteration 48/1000 | Loss: 0.00002301
Iteration 49/1000 | Loss: 0.00002301
Iteration 50/1000 | Loss: 0.00002300
Iteration 51/1000 | Loss: 0.00002300
Iteration 52/1000 | Loss: 0.00002300
Iteration 53/1000 | Loss: 0.00002300
Iteration 54/1000 | Loss: 0.00002299
Iteration 55/1000 | Loss: 0.00002299
Iteration 56/1000 | Loss: 0.00002299
Iteration 57/1000 | Loss: 0.00002299
Iteration 58/1000 | Loss: 0.00002299
Iteration 59/1000 | Loss: 0.00002299
Iteration 60/1000 | Loss: 0.00002299
Iteration 61/1000 | Loss: 0.00002299
Iteration 62/1000 | Loss: 0.00002298
Iteration 63/1000 | Loss: 0.00002298
Iteration 64/1000 | Loss: 0.00002297
Iteration 65/1000 | Loss: 0.00002297
Iteration 66/1000 | Loss: 0.00002297
Iteration 67/1000 | Loss: 0.00002297
Iteration 68/1000 | Loss: 0.00002296
Iteration 69/1000 | Loss: 0.00002296
Iteration 70/1000 | Loss: 0.00002295
Iteration 71/1000 | Loss: 0.00002295
Iteration 72/1000 | Loss: 0.00002295
Iteration 73/1000 | Loss: 0.00002295
Iteration 74/1000 | Loss: 0.00002294
Iteration 75/1000 | Loss: 0.00002294
Iteration 76/1000 | Loss: 0.00002294
Iteration 77/1000 | Loss: 0.00002294
Iteration 78/1000 | Loss: 0.00002293
Iteration 79/1000 | Loss: 0.00002293
Iteration 80/1000 | Loss: 0.00002293
Iteration 81/1000 | Loss: 0.00002293
Iteration 82/1000 | Loss: 0.00002292
Iteration 83/1000 | Loss: 0.00002292
Iteration 84/1000 | Loss: 0.00002292
Iteration 85/1000 | Loss: 0.00002291
Iteration 86/1000 | Loss: 0.00002291
Iteration 87/1000 | Loss: 0.00002291
Iteration 88/1000 | Loss: 0.00002291
Iteration 89/1000 | Loss: 0.00002291
Iteration 90/1000 | Loss: 0.00002290
Iteration 91/1000 | Loss: 0.00002290
Iteration 92/1000 | Loss: 0.00002290
Iteration 93/1000 | Loss: 0.00002290
Iteration 94/1000 | Loss: 0.00002290
Iteration 95/1000 | Loss: 0.00002290
Iteration 96/1000 | Loss: 0.00002290
Iteration 97/1000 | Loss: 0.00002290
Iteration 98/1000 | Loss: 0.00002290
Iteration 99/1000 | Loss: 0.00002290
Iteration 100/1000 | Loss: 0.00002290
Iteration 101/1000 | Loss: 0.00002290
Iteration 102/1000 | Loss: 0.00002290
Iteration 103/1000 | Loss: 0.00002289
Iteration 104/1000 | Loss: 0.00002289
Iteration 105/1000 | Loss: 0.00002289
Iteration 106/1000 | Loss: 0.00002289
Iteration 107/1000 | Loss: 0.00002289
Iteration 108/1000 | Loss: 0.00002288
Iteration 109/1000 | Loss: 0.00002288
Iteration 110/1000 | Loss: 0.00002288
Iteration 111/1000 | Loss: 0.00002288
Iteration 112/1000 | Loss: 0.00002288
Iteration 113/1000 | Loss: 0.00002288
Iteration 114/1000 | Loss: 0.00002288
Iteration 115/1000 | Loss: 0.00002287
Iteration 116/1000 | Loss: 0.00002287
Iteration 117/1000 | Loss: 0.00002287
Iteration 118/1000 | Loss: 0.00002287
Iteration 119/1000 | Loss: 0.00002287
Iteration 120/1000 | Loss: 0.00002287
Iteration 121/1000 | Loss: 0.00002287
Iteration 122/1000 | Loss: 0.00002287
Iteration 123/1000 | Loss: 0.00002287
Iteration 124/1000 | Loss: 0.00002287
Iteration 125/1000 | Loss: 0.00002287
Iteration 126/1000 | Loss: 0.00002287
Iteration 127/1000 | Loss: 0.00002287
Iteration 128/1000 | Loss: 0.00002286
Iteration 129/1000 | Loss: 0.00002286
Iteration 130/1000 | Loss: 0.00002286
Iteration 131/1000 | Loss: 0.00002286
Iteration 132/1000 | Loss: 0.00002286
Iteration 133/1000 | Loss: 0.00002286
Iteration 134/1000 | Loss: 0.00002286
Iteration 135/1000 | Loss: 0.00002286
Iteration 136/1000 | Loss: 0.00002286
Iteration 137/1000 | Loss: 0.00002285
Iteration 138/1000 | Loss: 0.00002285
Iteration 139/1000 | Loss: 0.00002285
Iteration 140/1000 | Loss: 0.00002285
Iteration 141/1000 | Loss: 0.00002285
Iteration 142/1000 | Loss: 0.00002285
Iteration 143/1000 | Loss: 0.00002284
Iteration 144/1000 | Loss: 0.00002284
Iteration 145/1000 | Loss: 0.00002284
Iteration 146/1000 | Loss: 0.00002284
Iteration 147/1000 | Loss: 0.00002284
Iteration 148/1000 | Loss: 0.00002284
Iteration 149/1000 | Loss: 0.00002284
Iteration 150/1000 | Loss: 0.00002284
Iteration 151/1000 | Loss: 0.00002283
Iteration 152/1000 | Loss: 0.00002283
Iteration 153/1000 | Loss: 0.00002283
Iteration 154/1000 | Loss: 0.00002283
Iteration 155/1000 | Loss: 0.00002283
Iteration 156/1000 | Loss: 0.00002283
Iteration 157/1000 | Loss: 0.00002283
Iteration 158/1000 | Loss: 0.00002283
Iteration 159/1000 | Loss: 0.00002283
Iteration 160/1000 | Loss: 0.00002283
Iteration 161/1000 | Loss: 0.00002283
Iteration 162/1000 | Loss: 0.00002283
Iteration 163/1000 | Loss: 0.00002282
Iteration 164/1000 | Loss: 0.00002282
Iteration 165/1000 | Loss: 0.00002282
Iteration 166/1000 | Loss: 0.00002281
Iteration 167/1000 | Loss: 0.00002281
Iteration 168/1000 | Loss: 0.00002281
Iteration 169/1000 | Loss: 0.00002281
Iteration 170/1000 | Loss: 0.00002281
Iteration 171/1000 | Loss: 0.00002281
Iteration 172/1000 | Loss: 0.00002281
Iteration 173/1000 | Loss: 0.00002281
Iteration 174/1000 | Loss: 0.00002281
Iteration 175/1000 | Loss: 0.00002281
Iteration 176/1000 | Loss: 0.00002281
Iteration 177/1000 | Loss: 0.00002280
Iteration 178/1000 | Loss: 0.00002280
Iteration 179/1000 | Loss: 0.00002280
Iteration 180/1000 | Loss: 0.00002280
Iteration 181/1000 | Loss: 0.00002280
Iteration 182/1000 | Loss: 0.00002280
Iteration 183/1000 | Loss: 0.00002280
Iteration 184/1000 | Loss: 0.00002280
Iteration 185/1000 | Loss: 0.00002280
Iteration 186/1000 | Loss: 0.00002280
Iteration 187/1000 | Loss: 0.00002280
Iteration 188/1000 | Loss: 0.00002280
Iteration 189/1000 | Loss: 0.00002280
Iteration 190/1000 | Loss: 0.00002280
Iteration 191/1000 | Loss: 0.00002280
Iteration 192/1000 | Loss: 0.00002280
Iteration 193/1000 | Loss: 0.00002280
Iteration 194/1000 | Loss: 0.00002280
Iteration 195/1000 | Loss: 0.00002280
Iteration 196/1000 | Loss: 0.00002280
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 196. Stopping optimization.
Last 5 losses: [2.2802692910772748e-05, 2.2802692910772748e-05, 2.2802692910772748e-05, 2.2802692910772748e-05, 2.2802692910772748e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2802692910772748e-05

Optimization complete. Final v2v error: 4.142425537109375 mm

Highest mean error: 4.531667709350586 mm for frame 107

Lowest mean error: 3.662747621536255 mm for frame 21

Saving results

Total time: 47.23401665687561
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00831295
Iteration 2/25 | Loss: 0.00175804
Iteration 3/25 | Loss: 0.00165449
Iteration 4/25 | Loss: 0.00156833
Iteration 5/25 | Loss: 0.00156253
Iteration 6/25 | Loss: 0.00155762
Iteration 7/25 | Loss: 0.00155582
Iteration 8/25 | Loss: 0.00155491
Iteration 9/25 | Loss: 0.00155449
Iteration 10/25 | Loss: 0.00155438
Iteration 11/25 | Loss: 0.00155431
Iteration 12/25 | Loss: 0.00155428
Iteration 13/25 | Loss: 0.00155428
Iteration 14/25 | Loss: 0.00155428
Iteration 15/25 | Loss: 0.00155428
Iteration 16/25 | Loss: 0.00155428
Iteration 17/25 | Loss: 0.00155428
Iteration 18/25 | Loss: 0.00155428
Iteration 19/25 | Loss: 0.00155427
Iteration 20/25 | Loss: 0.00155427
Iteration 21/25 | Loss: 0.00155427
Iteration 22/25 | Loss: 0.00155427
Iteration 23/25 | Loss: 0.00155427
Iteration 24/25 | Loss: 0.00155427
Iteration 25/25 | Loss: 0.00155427

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76426756
Iteration 2/25 | Loss: 0.00320125
Iteration 3/25 | Loss: 0.00320125
Iteration 4/25 | Loss: 0.00320125
Iteration 5/25 | Loss: 0.00320125
Iteration 6/25 | Loss: 0.00320125
Iteration 7/25 | Loss: 0.00320125
Iteration 8/25 | Loss: 0.00320125
Iteration 9/25 | Loss: 0.00320125
Iteration 10/25 | Loss: 0.00320125
Iteration 11/25 | Loss: 0.00320125
Iteration 12/25 | Loss: 0.00320125
Iteration 13/25 | Loss: 0.00320125
Iteration 14/25 | Loss: 0.00320125
Iteration 15/25 | Loss: 0.00320125
Iteration 16/25 | Loss: 0.00320125
Iteration 17/25 | Loss: 0.00320125
Iteration 18/25 | Loss: 0.00320125
Iteration 19/25 | Loss: 0.00320125
Iteration 20/25 | Loss: 0.00320125
Iteration 21/25 | Loss: 0.00320125
Iteration 22/25 | Loss: 0.00320125
Iteration 23/25 | Loss: 0.00320125
Iteration 24/25 | Loss: 0.00320125
Iteration 25/25 | Loss: 0.00320125

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00320125
Iteration 2/1000 | Loss: 0.00003600
Iteration 3/1000 | Loss: 0.00002574
Iteration 4/1000 | Loss: 0.00002262
Iteration 5/1000 | Loss: 0.00002102
Iteration 6/1000 | Loss: 0.00002017
Iteration 7/1000 | Loss: 0.00001966
Iteration 8/1000 | Loss: 0.00001924
Iteration 9/1000 | Loss: 0.00001891
Iteration 10/1000 | Loss: 0.00001881
Iteration 11/1000 | Loss: 0.00001870
Iteration 12/1000 | Loss: 0.00001868
Iteration 13/1000 | Loss: 0.00001859
Iteration 14/1000 | Loss: 0.00001855
Iteration 15/1000 | Loss: 0.00001854
Iteration 16/1000 | Loss: 0.00001850
Iteration 17/1000 | Loss: 0.00001849
Iteration 18/1000 | Loss: 0.00001848
Iteration 19/1000 | Loss: 0.00001841
Iteration 20/1000 | Loss: 0.00001840
Iteration 21/1000 | Loss: 0.00001840
Iteration 22/1000 | Loss: 0.00001839
Iteration 23/1000 | Loss: 0.00001839
Iteration 24/1000 | Loss: 0.00001838
Iteration 25/1000 | Loss: 0.00001837
Iteration 26/1000 | Loss: 0.00001835
Iteration 27/1000 | Loss: 0.00001833
Iteration 28/1000 | Loss: 0.00001833
Iteration 29/1000 | Loss: 0.00001833
Iteration 30/1000 | Loss: 0.00001832
Iteration 31/1000 | Loss: 0.00001832
Iteration 32/1000 | Loss: 0.00001830
Iteration 33/1000 | Loss: 0.00001829
Iteration 34/1000 | Loss: 0.00001829
Iteration 35/1000 | Loss: 0.00001829
Iteration 36/1000 | Loss: 0.00001828
Iteration 37/1000 | Loss: 0.00001828
Iteration 38/1000 | Loss: 0.00001828
Iteration 39/1000 | Loss: 0.00001828
Iteration 40/1000 | Loss: 0.00001828
Iteration 41/1000 | Loss: 0.00001827
Iteration 42/1000 | Loss: 0.00001827
Iteration 43/1000 | Loss: 0.00001827
Iteration 44/1000 | Loss: 0.00001827
Iteration 45/1000 | Loss: 0.00001827
Iteration 46/1000 | Loss: 0.00001826
Iteration 47/1000 | Loss: 0.00001826
Iteration 48/1000 | Loss: 0.00001825
Iteration 49/1000 | Loss: 0.00001825
Iteration 50/1000 | Loss: 0.00001825
Iteration 51/1000 | Loss: 0.00001825
Iteration 52/1000 | Loss: 0.00001825
Iteration 53/1000 | Loss: 0.00001824
Iteration 54/1000 | Loss: 0.00001824
Iteration 55/1000 | Loss: 0.00001824
Iteration 56/1000 | Loss: 0.00001822
Iteration 57/1000 | Loss: 0.00001822
Iteration 58/1000 | Loss: 0.00001822
Iteration 59/1000 | Loss: 0.00001822
Iteration 60/1000 | Loss: 0.00001822
Iteration 61/1000 | Loss: 0.00001822
Iteration 62/1000 | Loss: 0.00001821
Iteration 63/1000 | Loss: 0.00001821
Iteration 64/1000 | Loss: 0.00001820
Iteration 65/1000 | Loss: 0.00001820
Iteration 66/1000 | Loss: 0.00001819
Iteration 67/1000 | Loss: 0.00001819
Iteration 68/1000 | Loss: 0.00001818
Iteration 69/1000 | Loss: 0.00001818
Iteration 70/1000 | Loss: 0.00001818
Iteration 71/1000 | Loss: 0.00001818
Iteration 72/1000 | Loss: 0.00001818
Iteration 73/1000 | Loss: 0.00001817
Iteration 74/1000 | Loss: 0.00001817
Iteration 75/1000 | Loss: 0.00001817
Iteration 76/1000 | Loss: 0.00001817
Iteration 77/1000 | Loss: 0.00001817
Iteration 78/1000 | Loss: 0.00001817
Iteration 79/1000 | Loss: 0.00001817
Iteration 80/1000 | Loss: 0.00001817
Iteration 81/1000 | Loss: 0.00001817
Iteration 82/1000 | Loss: 0.00001816
Iteration 83/1000 | Loss: 0.00001816
Iteration 84/1000 | Loss: 0.00001816
Iteration 85/1000 | Loss: 0.00001816
Iteration 86/1000 | Loss: 0.00001816
Iteration 87/1000 | Loss: 0.00001816
Iteration 88/1000 | Loss: 0.00001816
Iteration 89/1000 | Loss: 0.00001816
Iteration 90/1000 | Loss: 0.00001816
Iteration 91/1000 | Loss: 0.00001816
Iteration 92/1000 | Loss: 0.00001816
Iteration 93/1000 | Loss: 0.00001816
Iteration 94/1000 | Loss: 0.00001816
Iteration 95/1000 | Loss: 0.00001816
Iteration 96/1000 | Loss: 0.00001816
Iteration 97/1000 | Loss: 0.00001815
Iteration 98/1000 | Loss: 0.00001815
Iteration 99/1000 | Loss: 0.00001815
Iteration 100/1000 | Loss: 0.00001815
Iteration 101/1000 | Loss: 0.00001815
Iteration 102/1000 | Loss: 0.00001815
Iteration 103/1000 | Loss: 0.00001815
Iteration 104/1000 | Loss: 0.00001815
Iteration 105/1000 | Loss: 0.00001815
Iteration 106/1000 | Loss: 0.00001815
Iteration 107/1000 | Loss: 0.00001814
Iteration 108/1000 | Loss: 0.00001814
Iteration 109/1000 | Loss: 0.00001814
Iteration 110/1000 | Loss: 0.00001814
Iteration 111/1000 | Loss: 0.00001814
Iteration 112/1000 | Loss: 0.00001814
Iteration 113/1000 | Loss: 0.00001814
Iteration 114/1000 | Loss: 0.00001814
Iteration 115/1000 | Loss: 0.00001814
Iteration 116/1000 | Loss: 0.00001814
Iteration 117/1000 | Loss: 0.00001814
Iteration 118/1000 | Loss: 0.00001814
Iteration 119/1000 | Loss: 0.00001814
Iteration 120/1000 | Loss: 0.00001814
Iteration 121/1000 | Loss: 0.00001814
Iteration 122/1000 | Loss: 0.00001814
Iteration 123/1000 | Loss: 0.00001814
Iteration 124/1000 | Loss: 0.00001814
Iteration 125/1000 | Loss: 0.00001814
Iteration 126/1000 | Loss: 0.00001813
Iteration 127/1000 | Loss: 0.00001813
Iteration 128/1000 | Loss: 0.00001813
Iteration 129/1000 | Loss: 0.00001813
Iteration 130/1000 | Loss: 0.00001813
Iteration 131/1000 | Loss: 0.00001813
Iteration 132/1000 | Loss: 0.00001813
Iteration 133/1000 | Loss: 0.00001813
Iteration 134/1000 | Loss: 0.00001813
Iteration 135/1000 | Loss: 0.00001813
Iteration 136/1000 | Loss: 0.00001813
Iteration 137/1000 | Loss: 0.00001813
Iteration 138/1000 | Loss: 0.00001813
Iteration 139/1000 | Loss: 0.00001813
Iteration 140/1000 | Loss: 0.00001813
Iteration 141/1000 | Loss: 0.00001813
Iteration 142/1000 | Loss: 0.00001813
Iteration 143/1000 | Loss: 0.00001813
Iteration 144/1000 | Loss: 0.00001813
Iteration 145/1000 | Loss: 0.00001813
Iteration 146/1000 | Loss: 0.00001813
Iteration 147/1000 | Loss: 0.00001813
Iteration 148/1000 | Loss: 0.00001813
Iteration 149/1000 | Loss: 0.00001813
Iteration 150/1000 | Loss: 0.00001813
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 150. Stopping optimization.
Last 5 losses: [1.8128288502339274e-05, 1.8128288502339274e-05, 1.8128288502339274e-05, 1.8128288502339274e-05, 1.8128288502339274e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8128288502339274e-05

Optimization complete. Final v2v error: 3.673449754714966 mm

Highest mean error: 3.977175712585449 mm for frame 93

Lowest mean error: 3.4432101249694824 mm for frame 51

Saving results

Total time: 44.95359802246094
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00503169
Iteration 2/25 | Loss: 0.00165095
Iteration 3/25 | Loss: 0.00160375
Iteration 4/25 | Loss: 0.00159749
Iteration 5/25 | Loss: 0.00159565
Iteration 6/25 | Loss: 0.00159555
Iteration 7/25 | Loss: 0.00159555
Iteration 8/25 | Loss: 0.00159555
Iteration 9/25 | Loss: 0.00159555
Iteration 10/25 | Loss: 0.00159555
Iteration 11/25 | Loss: 0.00159555
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001595545676536858, 0.001595545676536858, 0.001595545676536858, 0.001595545676536858, 0.001595545676536858]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001595545676536858

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.92085266
Iteration 2/25 | Loss: 0.00304875
Iteration 3/25 | Loss: 0.00304872
Iteration 4/25 | Loss: 0.00304871
Iteration 5/25 | Loss: 0.00304871
Iteration 6/25 | Loss: 0.00304871
Iteration 7/25 | Loss: 0.00304871
Iteration 8/25 | Loss: 0.00304871
Iteration 9/25 | Loss: 0.00304871
Iteration 10/25 | Loss: 0.00304871
Iteration 11/25 | Loss: 0.00304871
Iteration 12/25 | Loss: 0.00304871
Iteration 13/25 | Loss: 0.00304871
Iteration 14/25 | Loss: 0.00304871
Iteration 15/25 | Loss: 0.00304871
Iteration 16/25 | Loss: 0.00304871
Iteration 17/25 | Loss: 0.00304871
Iteration 18/25 | Loss: 0.00304871
Iteration 19/25 | Loss: 0.00304871
Iteration 20/25 | Loss: 0.00304871
Iteration 21/25 | Loss: 0.00304871
Iteration 22/25 | Loss: 0.00304871
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0030487109906971455, 0.0030487109906971455, 0.0030487109906971455, 0.0030487109906971455, 0.0030487109906971455]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0030487109906971455

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00304871
Iteration 2/1000 | Loss: 0.00005179
Iteration 3/1000 | Loss: 0.00003404
Iteration 4/1000 | Loss: 0.00002994
Iteration 5/1000 | Loss: 0.00002836
Iteration 6/1000 | Loss: 0.00002715
Iteration 7/1000 | Loss: 0.00002633
Iteration 8/1000 | Loss: 0.00002603
Iteration 9/1000 | Loss: 0.00002582
Iteration 10/1000 | Loss: 0.00002556
Iteration 11/1000 | Loss: 0.00002536
Iteration 12/1000 | Loss: 0.00002530
Iteration 13/1000 | Loss: 0.00002509
Iteration 14/1000 | Loss: 0.00002498
Iteration 15/1000 | Loss: 0.00002491
Iteration 16/1000 | Loss: 0.00002488
Iteration 17/1000 | Loss: 0.00002488
Iteration 18/1000 | Loss: 0.00002488
Iteration 19/1000 | Loss: 0.00002488
Iteration 20/1000 | Loss: 0.00002488
Iteration 21/1000 | Loss: 0.00002487
Iteration 22/1000 | Loss: 0.00002487
Iteration 23/1000 | Loss: 0.00002486
Iteration 24/1000 | Loss: 0.00002484
Iteration 25/1000 | Loss: 0.00002484
Iteration 26/1000 | Loss: 0.00002481
Iteration 27/1000 | Loss: 0.00002481
Iteration 28/1000 | Loss: 0.00002481
Iteration 29/1000 | Loss: 0.00002481
Iteration 30/1000 | Loss: 0.00002481
Iteration 31/1000 | Loss: 0.00002481
Iteration 32/1000 | Loss: 0.00002481
Iteration 33/1000 | Loss: 0.00002481
Iteration 34/1000 | Loss: 0.00002481
Iteration 35/1000 | Loss: 0.00002480
Iteration 36/1000 | Loss: 0.00002480
Iteration 37/1000 | Loss: 0.00002480
Iteration 38/1000 | Loss: 0.00002480
Iteration 39/1000 | Loss: 0.00002480
Iteration 40/1000 | Loss: 0.00002480
Iteration 41/1000 | Loss: 0.00002478
Iteration 42/1000 | Loss: 0.00002478
Iteration 43/1000 | Loss: 0.00002478
Iteration 44/1000 | Loss: 0.00002478
Iteration 45/1000 | Loss: 0.00002478
Iteration 46/1000 | Loss: 0.00002477
Iteration 47/1000 | Loss: 0.00002477
Iteration 48/1000 | Loss: 0.00002477
Iteration 49/1000 | Loss: 0.00002477
Iteration 50/1000 | Loss: 0.00002477
Iteration 51/1000 | Loss: 0.00002476
Iteration 52/1000 | Loss: 0.00002476
Iteration 53/1000 | Loss: 0.00002475
Iteration 54/1000 | Loss: 0.00002475
Iteration 55/1000 | Loss: 0.00002475
Iteration 56/1000 | Loss: 0.00002474
Iteration 57/1000 | Loss: 0.00002472
Iteration 58/1000 | Loss: 0.00002472
Iteration 59/1000 | Loss: 0.00002472
Iteration 60/1000 | Loss: 0.00002472
Iteration 61/1000 | Loss: 0.00002472
Iteration 62/1000 | Loss: 0.00002472
Iteration 63/1000 | Loss: 0.00002472
Iteration 64/1000 | Loss: 0.00002472
Iteration 65/1000 | Loss: 0.00002472
Iteration 66/1000 | Loss: 0.00002472
Iteration 67/1000 | Loss: 0.00002471
Iteration 68/1000 | Loss: 0.00002470
Iteration 69/1000 | Loss: 0.00002470
Iteration 70/1000 | Loss: 0.00002470
Iteration 71/1000 | Loss: 0.00002470
Iteration 72/1000 | Loss: 0.00002470
Iteration 73/1000 | Loss: 0.00002470
Iteration 74/1000 | Loss: 0.00002470
Iteration 75/1000 | Loss: 0.00002470
Iteration 76/1000 | Loss: 0.00002470
Iteration 77/1000 | Loss: 0.00002470
Iteration 78/1000 | Loss: 0.00002470
Iteration 79/1000 | Loss: 0.00002470
Iteration 80/1000 | Loss: 0.00002469
Iteration 81/1000 | Loss: 0.00002466
Iteration 82/1000 | Loss: 0.00002465
Iteration 83/1000 | Loss: 0.00002464
Iteration 84/1000 | Loss: 0.00002464
Iteration 85/1000 | Loss: 0.00002464
Iteration 86/1000 | Loss: 0.00002463
Iteration 87/1000 | Loss: 0.00002461
Iteration 88/1000 | Loss: 0.00002461
Iteration 89/1000 | Loss: 0.00002461
Iteration 90/1000 | Loss: 0.00002461
Iteration 91/1000 | Loss: 0.00002461
Iteration 92/1000 | Loss: 0.00002461
Iteration 93/1000 | Loss: 0.00002461
Iteration 94/1000 | Loss: 0.00002460
Iteration 95/1000 | Loss: 0.00002460
Iteration 96/1000 | Loss: 0.00002460
Iteration 97/1000 | Loss: 0.00002460
Iteration 98/1000 | Loss: 0.00002460
Iteration 99/1000 | Loss: 0.00002459
Iteration 100/1000 | Loss: 0.00002459
Iteration 101/1000 | Loss: 0.00002459
Iteration 102/1000 | Loss: 0.00002458
Iteration 103/1000 | Loss: 0.00002458
Iteration 104/1000 | Loss: 0.00002458
Iteration 105/1000 | Loss: 0.00002458
Iteration 106/1000 | Loss: 0.00002458
Iteration 107/1000 | Loss: 0.00002458
Iteration 108/1000 | Loss: 0.00002458
Iteration 109/1000 | Loss: 0.00002457
Iteration 110/1000 | Loss: 0.00002457
Iteration 111/1000 | Loss: 0.00002457
Iteration 112/1000 | Loss: 0.00002457
Iteration 113/1000 | Loss: 0.00002457
Iteration 114/1000 | Loss: 0.00002457
Iteration 115/1000 | Loss: 0.00002457
Iteration 116/1000 | Loss: 0.00002457
Iteration 117/1000 | Loss: 0.00002457
Iteration 118/1000 | Loss: 0.00002456
Iteration 119/1000 | Loss: 0.00002456
Iteration 120/1000 | Loss: 0.00002456
Iteration 121/1000 | Loss: 0.00002456
Iteration 122/1000 | Loss: 0.00002456
Iteration 123/1000 | Loss: 0.00002456
Iteration 124/1000 | Loss: 0.00002456
Iteration 125/1000 | Loss: 0.00002456
Iteration 126/1000 | Loss: 0.00002456
Iteration 127/1000 | Loss: 0.00002456
Iteration 128/1000 | Loss: 0.00002456
Iteration 129/1000 | Loss: 0.00002455
Iteration 130/1000 | Loss: 0.00002455
Iteration 131/1000 | Loss: 0.00002455
Iteration 132/1000 | Loss: 0.00002455
Iteration 133/1000 | Loss: 0.00002455
Iteration 134/1000 | Loss: 0.00002455
Iteration 135/1000 | Loss: 0.00002455
Iteration 136/1000 | Loss: 0.00002455
Iteration 137/1000 | Loss: 0.00002455
Iteration 138/1000 | Loss: 0.00002455
Iteration 139/1000 | Loss: 0.00002455
Iteration 140/1000 | Loss: 0.00002454
Iteration 141/1000 | Loss: 0.00002454
Iteration 142/1000 | Loss: 0.00002454
Iteration 143/1000 | Loss: 0.00002454
Iteration 144/1000 | Loss: 0.00002453
Iteration 145/1000 | Loss: 0.00002453
Iteration 146/1000 | Loss: 0.00002453
Iteration 147/1000 | Loss: 0.00002453
Iteration 148/1000 | Loss: 0.00002452
Iteration 149/1000 | Loss: 0.00002452
Iteration 150/1000 | Loss: 0.00002452
Iteration 151/1000 | Loss: 0.00002452
Iteration 152/1000 | Loss: 0.00002451
Iteration 153/1000 | Loss: 0.00002451
Iteration 154/1000 | Loss: 0.00002451
Iteration 155/1000 | Loss: 0.00002451
Iteration 156/1000 | Loss: 0.00002451
Iteration 157/1000 | Loss: 0.00002451
Iteration 158/1000 | Loss: 0.00002451
Iteration 159/1000 | Loss: 0.00002451
Iteration 160/1000 | Loss: 0.00002451
Iteration 161/1000 | Loss: 0.00002451
Iteration 162/1000 | Loss: 0.00002451
Iteration 163/1000 | Loss: 0.00002450
Iteration 164/1000 | Loss: 0.00002450
Iteration 165/1000 | Loss: 0.00002450
Iteration 166/1000 | Loss: 0.00002449
Iteration 167/1000 | Loss: 0.00002449
Iteration 168/1000 | Loss: 0.00002449
Iteration 169/1000 | Loss: 0.00002449
Iteration 170/1000 | Loss: 0.00002449
Iteration 171/1000 | Loss: 0.00002449
Iteration 172/1000 | Loss: 0.00002449
Iteration 173/1000 | Loss: 0.00002448
Iteration 174/1000 | Loss: 0.00002447
Iteration 175/1000 | Loss: 0.00002447
Iteration 176/1000 | Loss: 0.00002447
Iteration 177/1000 | Loss: 0.00002446
Iteration 178/1000 | Loss: 0.00002446
Iteration 179/1000 | Loss: 0.00002446
Iteration 180/1000 | Loss: 0.00002446
Iteration 181/1000 | Loss: 0.00002445
Iteration 182/1000 | Loss: 0.00002445
Iteration 183/1000 | Loss: 0.00002445
Iteration 184/1000 | Loss: 0.00002444
Iteration 185/1000 | Loss: 0.00002444
Iteration 186/1000 | Loss: 0.00002444
Iteration 187/1000 | Loss: 0.00002444
Iteration 188/1000 | Loss: 0.00002444
Iteration 189/1000 | Loss: 0.00002443
Iteration 190/1000 | Loss: 0.00002443
Iteration 191/1000 | Loss: 0.00002443
Iteration 192/1000 | Loss: 0.00002443
Iteration 193/1000 | Loss: 0.00002443
Iteration 194/1000 | Loss: 0.00002443
Iteration 195/1000 | Loss: 0.00002443
Iteration 196/1000 | Loss: 0.00002443
Iteration 197/1000 | Loss: 0.00002443
Iteration 198/1000 | Loss: 0.00002442
Iteration 199/1000 | Loss: 0.00002442
Iteration 200/1000 | Loss: 0.00002442
Iteration 201/1000 | Loss: 0.00002442
Iteration 202/1000 | Loss: 0.00002442
Iteration 203/1000 | Loss: 0.00002442
Iteration 204/1000 | Loss: 0.00002442
Iteration 205/1000 | Loss: 0.00002442
Iteration 206/1000 | Loss: 0.00002442
Iteration 207/1000 | Loss: 0.00002442
Iteration 208/1000 | Loss: 0.00002441
Iteration 209/1000 | Loss: 0.00002441
Iteration 210/1000 | Loss: 0.00002441
Iteration 211/1000 | Loss: 0.00002441
Iteration 212/1000 | Loss: 0.00002441
Iteration 213/1000 | Loss: 0.00002441
Iteration 214/1000 | Loss: 0.00002441
Iteration 215/1000 | Loss: 0.00002441
Iteration 216/1000 | Loss: 0.00002441
Iteration 217/1000 | Loss: 0.00002441
Iteration 218/1000 | Loss: 0.00002441
Iteration 219/1000 | Loss: 0.00002441
Iteration 220/1000 | Loss: 0.00002441
Iteration 221/1000 | Loss: 0.00002441
Iteration 222/1000 | Loss: 0.00002441
Iteration 223/1000 | Loss: 0.00002441
Iteration 224/1000 | Loss: 0.00002441
Iteration 225/1000 | Loss: 0.00002441
Iteration 226/1000 | Loss: 0.00002441
Iteration 227/1000 | Loss: 0.00002441
Iteration 228/1000 | Loss: 0.00002441
Iteration 229/1000 | Loss: 0.00002441
Iteration 230/1000 | Loss: 0.00002441
Iteration 231/1000 | Loss: 0.00002441
Iteration 232/1000 | Loss: 0.00002441
Iteration 233/1000 | Loss: 0.00002441
Iteration 234/1000 | Loss: 0.00002441
Iteration 235/1000 | Loss: 0.00002441
Iteration 236/1000 | Loss: 0.00002441
Iteration 237/1000 | Loss: 0.00002441
Iteration 238/1000 | Loss: 0.00002441
Iteration 239/1000 | Loss: 0.00002441
Iteration 240/1000 | Loss: 0.00002441
Iteration 241/1000 | Loss: 0.00002441
Iteration 242/1000 | Loss: 0.00002441
Iteration 243/1000 | Loss: 0.00002441
Iteration 244/1000 | Loss: 0.00002441
Iteration 245/1000 | Loss: 0.00002441
Iteration 246/1000 | Loss: 0.00002441
Iteration 247/1000 | Loss: 0.00002441
Iteration 248/1000 | Loss: 0.00002441
Iteration 249/1000 | Loss: 0.00002441
Iteration 250/1000 | Loss: 0.00002441
Iteration 251/1000 | Loss: 0.00002441
Iteration 252/1000 | Loss: 0.00002441
Iteration 253/1000 | Loss: 0.00002441
Iteration 254/1000 | Loss: 0.00002441
Iteration 255/1000 | Loss: 0.00002441
Iteration 256/1000 | Loss: 0.00002441
Iteration 257/1000 | Loss: 0.00002441
Iteration 258/1000 | Loss: 0.00002441
Iteration 259/1000 | Loss: 0.00002441
Iteration 260/1000 | Loss: 0.00002441
Iteration 261/1000 | Loss: 0.00002441
Iteration 262/1000 | Loss: 0.00002441
Iteration 263/1000 | Loss: 0.00002441
Iteration 264/1000 | Loss: 0.00002441
Iteration 265/1000 | Loss: 0.00002441
Iteration 266/1000 | Loss: 0.00002441
Iteration 267/1000 | Loss: 0.00002441
Iteration 268/1000 | Loss: 0.00002441
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 268. Stopping optimization.
Last 5 losses: [2.4407974706264213e-05, 2.4407974706264213e-05, 2.4407974706264213e-05, 2.4407974706264213e-05, 2.4407974706264213e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4407974706264213e-05

Optimization complete. Final v2v error: 4.074684143066406 mm

Highest mean error: 4.127109527587891 mm for frame 11

Lowest mean error: 4.025683403015137 mm for frame 39

Saving results

Total time: 41.929924964904785
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00926441
Iteration 2/25 | Loss: 0.00186859
Iteration 3/25 | Loss: 0.00162652
Iteration 4/25 | Loss: 0.00160341
Iteration 5/25 | Loss: 0.00159428
Iteration 6/25 | Loss: 0.00159124
Iteration 7/25 | Loss: 0.00159017
Iteration 8/25 | Loss: 0.00159006
Iteration 9/25 | Loss: 0.00159006
Iteration 10/25 | Loss: 0.00159006
Iteration 11/25 | Loss: 0.00159006
Iteration 12/25 | Loss: 0.00159006
Iteration 13/25 | Loss: 0.00159006
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0015900633297860622, 0.0015900633297860622, 0.0015900633297860622, 0.0015900633297860622, 0.0015900633297860622]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015900633297860622

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.00721693
Iteration 2/25 | Loss: 0.00272382
Iteration 3/25 | Loss: 0.00272382
Iteration 4/25 | Loss: 0.00272381
Iteration 5/25 | Loss: 0.00272381
Iteration 6/25 | Loss: 0.00272381
Iteration 7/25 | Loss: 0.00272381
Iteration 8/25 | Loss: 0.00272381
Iteration 9/25 | Loss: 0.00272381
Iteration 10/25 | Loss: 0.00272381
Iteration 11/25 | Loss: 0.00272381
Iteration 12/25 | Loss: 0.00272381
Iteration 13/25 | Loss: 0.00272381
Iteration 14/25 | Loss: 0.00272381
Iteration 15/25 | Loss: 0.00272381
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.00272381235845387, 0.00272381235845387, 0.00272381235845387, 0.00272381235845387, 0.00272381235845387]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00272381235845387

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00272381
Iteration 2/1000 | Loss: 0.00015093
Iteration 3/1000 | Loss: 0.00006605
Iteration 4/1000 | Loss: 0.00005130
Iteration 5/1000 | Loss: 0.00004456
Iteration 6/1000 | Loss: 0.00004075
Iteration 7/1000 | Loss: 0.00003782
Iteration 8/1000 | Loss: 0.00003602
Iteration 9/1000 | Loss: 0.00003524
Iteration 10/1000 | Loss: 0.00003462
Iteration 11/1000 | Loss: 0.00003422
Iteration 12/1000 | Loss: 0.00003373
Iteration 13/1000 | Loss: 0.00003330
Iteration 14/1000 | Loss: 0.00003293
Iteration 15/1000 | Loss: 0.00003255
Iteration 16/1000 | Loss: 0.00003233
Iteration 17/1000 | Loss: 0.00003220
Iteration 18/1000 | Loss: 0.00003209
Iteration 19/1000 | Loss: 0.00003199
Iteration 20/1000 | Loss: 0.00003197
Iteration 21/1000 | Loss: 0.00003194
Iteration 22/1000 | Loss: 0.00003190
Iteration 23/1000 | Loss: 0.00003186
Iteration 24/1000 | Loss: 0.00003184
Iteration 25/1000 | Loss: 0.00003182
Iteration 26/1000 | Loss: 0.00003180
Iteration 27/1000 | Loss: 0.00003175
Iteration 28/1000 | Loss: 0.00003169
Iteration 29/1000 | Loss: 0.00003168
Iteration 30/1000 | Loss: 0.00003167
Iteration 31/1000 | Loss: 0.00003164
Iteration 32/1000 | Loss: 0.00003163
Iteration 33/1000 | Loss: 0.00003162
Iteration 34/1000 | Loss: 0.00003161
Iteration 35/1000 | Loss: 0.00003161
Iteration 36/1000 | Loss: 0.00003160
Iteration 37/1000 | Loss: 0.00003160
Iteration 38/1000 | Loss: 0.00003158
Iteration 39/1000 | Loss: 0.00003156
Iteration 40/1000 | Loss: 0.00003155
Iteration 41/1000 | Loss: 0.00003155
Iteration 42/1000 | Loss: 0.00003153
Iteration 43/1000 | Loss: 0.00003153
Iteration 44/1000 | Loss: 0.00003152
Iteration 45/1000 | Loss: 0.00003149
Iteration 46/1000 | Loss: 0.00003149
Iteration 47/1000 | Loss: 0.00003146
Iteration 48/1000 | Loss: 0.00003145
Iteration 49/1000 | Loss: 0.00003144
Iteration 50/1000 | Loss: 0.00003144
Iteration 51/1000 | Loss: 0.00003143
Iteration 52/1000 | Loss: 0.00003143
Iteration 53/1000 | Loss: 0.00003142
Iteration 54/1000 | Loss: 0.00003140
Iteration 55/1000 | Loss: 0.00003139
Iteration 56/1000 | Loss: 0.00003139
Iteration 57/1000 | Loss: 0.00003139
Iteration 58/1000 | Loss: 0.00003138
Iteration 59/1000 | Loss: 0.00003137
Iteration 60/1000 | Loss: 0.00003137
Iteration 61/1000 | Loss: 0.00003137
Iteration 62/1000 | Loss: 0.00003136
Iteration 63/1000 | Loss: 0.00003136
Iteration 64/1000 | Loss: 0.00003136
Iteration 65/1000 | Loss: 0.00003136
Iteration 66/1000 | Loss: 0.00003136
Iteration 67/1000 | Loss: 0.00003136
Iteration 68/1000 | Loss: 0.00003135
Iteration 69/1000 | Loss: 0.00003135
Iteration 70/1000 | Loss: 0.00003134
Iteration 71/1000 | Loss: 0.00003134
Iteration 72/1000 | Loss: 0.00003133
Iteration 73/1000 | Loss: 0.00003133
Iteration 74/1000 | Loss: 0.00003133
Iteration 75/1000 | Loss: 0.00003132
Iteration 76/1000 | Loss: 0.00003132
Iteration 77/1000 | Loss: 0.00003132
Iteration 78/1000 | Loss: 0.00003131
Iteration 79/1000 | Loss: 0.00003131
Iteration 80/1000 | Loss: 0.00003130
Iteration 81/1000 | Loss: 0.00003130
Iteration 82/1000 | Loss: 0.00003129
Iteration 83/1000 | Loss: 0.00003129
Iteration 84/1000 | Loss: 0.00003129
Iteration 85/1000 | Loss: 0.00003128
Iteration 86/1000 | Loss: 0.00003128
Iteration 87/1000 | Loss: 0.00003128
Iteration 88/1000 | Loss: 0.00003127
Iteration 89/1000 | Loss: 0.00003127
Iteration 90/1000 | Loss: 0.00003127
Iteration 91/1000 | Loss: 0.00003126
Iteration 92/1000 | Loss: 0.00003125
Iteration 93/1000 | Loss: 0.00003125
Iteration 94/1000 | Loss: 0.00003125
Iteration 95/1000 | Loss: 0.00003125
Iteration 96/1000 | Loss: 0.00003124
Iteration 97/1000 | Loss: 0.00003124
Iteration 98/1000 | Loss: 0.00003124
Iteration 99/1000 | Loss: 0.00003124
Iteration 100/1000 | Loss: 0.00003124
Iteration 101/1000 | Loss: 0.00003124
Iteration 102/1000 | Loss: 0.00003124
Iteration 103/1000 | Loss: 0.00003123
Iteration 104/1000 | Loss: 0.00003123
Iteration 105/1000 | Loss: 0.00003123
Iteration 106/1000 | Loss: 0.00003123
Iteration 107/1000 | Loss: 0.00003122
Iteration 108/1000 | Loss: 0.00003122
Iteration 109/1000 | Loss: 0.00003122
Iteration 110/1000 | Loss: 0.00003122
Iteration 111/1000 | Loss: 0.00003121
Iteration 112/1000 | Loss: 0.00003121
Iteration 113/1000 | Loss: 0.00003121
Iteration 114/1000 | Loss: 0.00003121
Iteration 115/1000 | Loss: 0.00003121
Iteration 116/1000 | Loss: 0.00003120
Iteration 117/1000 | Loss: 0.00003120
Iteration 118/1000 | Loss: 0.00003120
Iteration 119/1000 | Loss: 0.00003120
Iteration 120/1000 | Loss: 0.00003120
Iteration 121/1000 | Loss: 0.00003120
Iteration 122/1000 | Loss: 0.00003120
Iteration 123/1000 | Loss: 0.00003119
Iteration 124/1000 | Loss: 0.00003119
Iteration 125/1000 | Loss: 0.00003119
Iteration 126/1000 | Loss: 0.00003119
Iteration 127/1000 | Loss: 0.00003119
Iteration 128/1000 | Loss: 0.00003119
Iteration 129/1000 | Loss: 0.00003119
Iteration 130/1000 | Loss: 0.00003119
Iteration 131/1000 | Loss: 0.00003119
Iteration 132/1000 | Loss: 0.00003118
Iteration 133/1000 | Loss: 0.00003118
Iteration 134/1000 | Loss: 0.00003118
Iteration 135/1000 | Loss: 0.00003118
Iteration 136/1000 | Loss: 0.00003118
Iteration 137/1000 | Loss: 0.00003117
Iteration 138/1000 | Loss: 0.00003117
Iteration 139/1000 | Loss: 0.00003117
Iteration 140/1000 | Loss: 0.00003117
Iteration 141/1000 | Loss: 0.00003117
Iteration 142/1000 | Loss: 0.00003117
Iteration 143/1000 | Loss: 0.00003117
Iteration 144/1000 | Loss: 0.00003117
Iteration 145/1000 | Loss: 0.00003117
Iteration 146/1000 | Loss: 0.00003117
Iteration 147/1000 | Loss: 0.00003117
Iteration 148/1000 | Loss: 0.00003116
Iteration 149/1000 | Loss: 0.00003116
Iteration 150/1000 | Loss: 0.00003116
Iteration 151/1000 | Loss: 0.00003116
Iteration 152/1000 | Loss: 0.00003116
Iteration 153/1000 | Loss: 0.00003116
Iteration 154/1000 | Loss: 0.00003116
Iteration 155/1000 | Loss: 0.00003116
Iteration 156/1000 | Loss: 0.00003116
Iteration 157/1000 | Loss: 0.00003115
Iteration 158/1000 | Loss: 0.00003115
Iteration 159/1000 | Loss: 0.00003115
Iteration 160/1000 | Loss: 0.00003115
Iteration 161/1000 | Loss: 0.00003115
Iteration 162/1000 | Loss: 0.00003115
Iteration 163/1000 | Loss: 0.00003115
Iteration 164/1000 | Loss: 0.00003115
Iteration 165/1000 | Loss: 0.00003115
Iteration 166/1000 | Loss: 0.00003115
Iteration 167/1000 | Loss: 0.00003115
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 167. Stopping optimization.
Last 5 losses: [3.11450166918803e-05, 3.11450166918803e-05, 3.11450166918803e-05, 3.11450166918803e-05, 3.11450166918803e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.11450166918803e-05

Optimization complete. Final v2v error: 4.56923246383667 mm

Highest mean error: 6.983715057373047 mm for frame 90

Lowest mean error: 3.260525941848755 mm for frame 5

Saving results

Total time: 53.186453342437744
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00896355
Iteration 2/25 | Loss: 0.00164246
Iteration 3/25 | Loss: 0.00153939
Iteration 4/25 | Loss: 0.00152848
Iteration 5/25 | Loss: 0.00152516
Iteration 6/25 | Loss: 0.00152484
Iteration 7/25 | Loss: 0.00152484
Iteration 8/25 | Loss: 0.00152484
Iteration 9/25 | Loss: 0.00152484
Iteration 10/25 | Loss: 0.00152484
Iteration 11/25 | Loss: 0.00152484
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.001524836290627718, 0.001524836290627718, 0.001524836290627718, 0.001524836290627718, 0.001524836290627718]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001524836290627718

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.19047904
Iteration 2/25 | Loss: 0.00290100
Iteration 3/25 | Loss: 0.00290099
Iteration 4/25 | Loss: 0.00290099
Iteration 5/25 | Loss: 0.00290099
Iteration 6/25 | Loss: 0.00290099
Iteration 7/25 | Loss: 0.00290099
Iteration 8/25 | Loss: 0.00290099
Iteration 9/25 | Loss: 0.00290099
Iteration 10/25 | Loss: 0.00290099
Iteration 11/25 | Loss: 0.00290099
Iteration 12/25 | Loss: 0.00290099
Iteration 13/25 | Loss: 0.00290099
Iteration 14/25 | Loss: 0.00290099
Iteration 15/25 | Loss: 0.00290099
Iteration 16/25 | Loss: 0.00290099
Iteration 17/25 | Loss: 0.00290099
Iteration 18/25 | Loss: 0.00290099
Iteration 19/25 | Loss: 0.00290099
Iteration 20/25 | Loss: 0.00290099
Iteration 21/25 | Loss: 0.00290099
Iteration 22/25 | Loss: 0.00290099
Iteration 23/25 | Loss: 0.00290099
Iteration 24/25 | Loss: 0.00290099
Iteration 25/25 | Loss: 0.00290099

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00290099
Iteration 2/1000 | Loss: 0.00004412
Iteration 3/1000 | Loss: 0.00002721
Iteration 4/1000 | Loss: 0.00002377
Iteration 5/1000 | Loss: 0.00002145
Iteration 6/1000 | Loss: 0.00001982
Iteration 7/1000 | Loss: 0.00001895
Iteration 8/1000 | Loss: 0.00001827
Iteration 9/1000 | Loss: 0.00001786
Iteration 10/1000 | Loss: 0.00001752
Iteration 11/1000 | Loss: 0.00001726
Iteration 12/1000 | Loss: 0.00001709
Iteration 13/1000 | Loss: 0.00001702
Iteration 14/1000 | Loss: 0.00001699
Iteration 15/1000 | Loss: 0.00001699
Iteration 16/1000 | Loss: 0.00001696
Iteration 17/1000 | Loss: 0.00001695
Iteration 18/1000 | Loss: 0.00001681
Iteration 19/1000 | Loss: 0.00001678
Iteration 20/1000 | Loss: 0.00001675
Iteration 21/1000 | Loss: 0.00001675
Iteration 22/1000 | Loss: 0.00001675
Iteration 23/1000 | Loss: 0.00001674
Iteration 24/1000 | Loss: 0.00001673
Iteration 25/1000 | Loss: 0.00001673
Iteration 26/1000 | Loss: 0.00001673
Iteration 27/1000 | Loss: 0.00001672
Iteration 28/1000 | Loss: 0.00001672
Iteration 29/1000 | Loss: 0.00001672
Iteration 30/1000 | Loss: 0.00001672
Iteration 31/1000 | Loss: 0.00001672
Iteration 32/1000 | Loss: 0.00001672
Iteration 33/1000 | Loss: 0.00001671
Iteration 34/1000 | Loss: 0.00001671
Iteration 35/1000 | Loss: 0.00001671
Iteration 36/1000 | Loss: 0.00001671
Iteration 37/1000 | Loss: 0.00001671
Iteration 38/1000 | Loss: 0.00001671
Iteration 39/1000 | Loss: 0.00001671
Iteration 40/1000 | Loss: 0.00001671
Iteration 41/1000 | Loss: 0.00001671
Iteration 42/1000 | Loss: 0.00001671
Iteration 43/1000 | Loss: 0.00001671
Iteration 44/1000 | Loss: 0.00001671
Iteration 45/1000 | Loss: 0.00001670
Iteration 46/1000 | Loss: 0.00001670
Iteration 47/1000 | Loss: 0.00001670
Iteration 48/1000 | Loss: 0.00001670
Iteration 49/1000 | Loss: 0.00001670
Iteration 50/1000 | Loss: 0.00001670
Iteration 51/1000 | Loss: 0.00001669
Iteration 52/1000 | Loss: 0.00001669
Iteration 53/1000 | Loss: 0.00001667
Iteration 54/1000 | Loss: 0.00001667
Iteration 55/1000 | Loss: 0.00001667
Iteration 56/1000 | Loss: 0.00001666
Iteration 57/1000 | Loss: 0.00001666
Iteration 58/1000 | Loss: 0.00001665
Iteration 59/1000 | Loss: 0.00001665
Iteration 60/1000 | Loss: 0.00001664
Iteration 61/1000 | Loss: 0.00001664
Iteration 62/1000 | Loss: 0.00001663
Iteration 63/1000 | Loss: 0.00001663
Iteration 64/1000 | Loss: 0.00001663
Iteration 65/1000 | Loss: 0.00001663
Iteration 66/1000 | Loss: 0.00001663
Iteration 67/1000 | Loss: 0.00001662
Iteration 68/1000 | Loss: 0.00001662
Iteration 69/1000 | Loss: 0.00001662
Iteration 70/1000 | Loss: 0.00001661
Iteration 71/1000 | Loss: 0.00001660
Iteration 72/1000 | Loss: 0.00001660
Iteration 73/1000 | Loss: 0.00001660
Iteration 74/1000 | Loss: 0.00001660
Iteration 75/1000 | Loss: 0.00001659
Iteration 76/1000 | Loss: 0.00001659
Iteration 77/1000 | Loss: 0.00001659
Iteration 78/1000 | Loss: 0.00001659
Iteration 79/1000 | Loss: 0.00001658
Iteration 80/1000 | Loss: 0.00001658
Iteration 81/1000 | Loss: 0.00001658
Iteration 82/1000 | Loss: 0.00001658
Iteration 83/1000 | Loss: 0.00001658
Iteration 84/1000 | Loss: 0.00001657
Iteration 85/1000 | Loss: 0.00001657
Iteration 86/1000 | Loss: 0.00001657
Iteration 87/1000 | Loss: 0.00001657
Iteration 88/1000 | Loss: 0.00001657
Iteration 89/1000 | Loss: 0.00001657
Iteration 90/1000 | Loss: 0.00001657
Iteration 91/1000 | Loss: 0.00001657
Iteration 92/1000 | Loss: 0.00001657
Iteration 93/1000 | Loss: 0.00001657
Iteration 94/1000 | Loss: 0.00001657
Iteration 95/1000 | Loss: 0.00001657
Iteration 96/1000 | Loss: 0.00001657
Iteration 97/1000 | Loss: 0.00001657
Iteration 98/1000 | Loss: 0.00001657
Iteration 99/1000 | Loss: 0.00001657
Iteration 100/1000 | Loss: 0.00001657
Iteration 101/1000 | Loss: 0.00001657
Iteration 102/1000 | Loss: 0.00001657
Iteration 103/1000 | Loss: 0.00001657
Iteration 104/1000 | Loss: 0.00001657
Iteration 105/1000 | Loss: 0.00001657
Iteration 106/1000 | Loss: 0.00001657
Iteration 107/1000 | Loss: 0.00001657
Iteration 108/1000 | Loss: 0.00001657
Iteration 109/1000 | Loss: 0.00001657
Iteration 110/1000 | Loss: 0.00001657
Iteration 111/1000 | Loss: 0.00001657
Iteration 112/1000 | Loss: 0.00001657
Iteration 113/1000 | Loss: 0.00001657
Iteration 114/1000 | Loss: 0.00001657
Iteration 115/1000 | Loss: 0.00001657
Iteration 116/1000 | Loss: 0.00001657
Iteration 117/1000 | Loss: 0.00001657
Iteration 118/1000 | Loss: 0.00001657
Iteration 119/1000 | Loss: 0.00001657
Iteration 120/1000 | Loss: 0.00001657
Iteration 121/1000 | Loss: 0.00001657
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 121. Stopping optimization.
Last 5 losses: [1.65651181305293e-05, 1.65651181305293e-05, 1.65651181305293e-05, 1.65651181305293e-05, 1.65651181305293e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.65651181305293e-05

Optimization complete. Final v2v error: 3.4847707748413086 mm

Highest mean error: 3.6803412437438965 mm for frame 85

Lowest mean error: 3.2134437561035156 mm for frame 2

Saving results

Total time: 33.9286105632782
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00654744
Iteration 2/25 | Loss: 0.00190199
Iteration 3/25 | Loss: 0.00174541
Iteration 4/25 | Loss: 0.00173464
Iteration 5/25 | Loss: 0.00173970
Iteration 6/25 | Loss: 0.00164948
Iteration 7/25 | Loss: 0.00162433
Iteration 8/25 | Loss: 0.00162445
Iteration 9/25 | Loss: 0.00162024
Iteration 10/25 | Loss: 0.00161874
Iteration 11/25 | Loss: 0.00161851
Iteration 12/25 | Loss: 0.00161843
Iteration 13/25 | Loss: 0.00161843
Iteration 14/25 | Loss: 0.00161843
Iteration 15/25 | Loss: 0.00161842
Iteration 16/25 | Loss: 0.00161842
Iteration 17/25 | Loss: 0.00161842
Iteration 18/25 | Loss: 0.00161842
Iteration 19/25 | Loss: 0.00161842
Iteration 20/25 | Loss: 0.00161842
Iteration 21/25 | Loss: 0.00161842
Iteration 22/25 | Loss: 0.00161842
Iteration 23/25 | Loss: 0.00161842
Iteration 24/25 | Loss: 0.00161842
Iteration 25/25 | Loss: 0.00161842

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13810146
Iteration 2/25 | Loss: 0.00267494
Iteration 3/25 | Loss: 0.00267487
Iteration 4/25 | Loss: 0.00267487
Iteration 5/25 | Loss: 0.00267487
Iteration 6/25 | Loss: 0.00267487
Iteration 7/25 | Loss: 0.00267487
Iteration 8/25 | Loss: 0.00267487
Iteration 9/25 | Loss: 0.00267487
Iteration 10/25 | Loss: 0.00267487
Iteration 11/25 | Loss: 0.00267487
Iteration 12/25 | Loss: 0.00267487
Iteration 13/25 | Loss: 0.00267487
Iteration 14/25 | Loss: 0.00267487
Iteration 15/25 | Loss: 0.00267487
Iteration 16/25 | Loss: 0.00267487
Iteration 17/25 | Loss: 0.00267487
Iteration 18/25 | Loss: 0.00267487
Iteration 19/25 | Loss: 0.00267487
Iteration 20/25 | Loss: 0.00267487
Iteration 21/25 | Loss: 0.00267487
Iteration 22/25 | Loss: 0.00267487
Iteration 23/25 | Loss: 0.00267487
Iteration 24/25 | Loss: 0.00267487
Iteration 25/25 | Loss: 0.00267487

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00267487
Iteration 2/1000 | Loss: 0.00010528
Iteration 3/1000 | Loss: 0.00006363
Iteration 4/1000 | Loss: 0.00005169
Iteration 5/1000 | Loss: 0.00004823
Iteration 6/1000 | Loss: 0.00004602
Iteration 7/1000 | Loss: 0.00004388
Iteration 8/1000 | Loss: 0.00004247
Iteration 9/1000 | Loss: 0.00004119
Iteration 10/1000 | Loss: 0.00004032
Iteration 11/1000 | Loss: 0.00003954
Iteration 12/1000 | Loss: 0.00003888
Iteration 13/1000 | Loss: 0.00003841
Iteration 14/1000 | Loss: 0.00003803
Iteration 15/1000 | Loss: 0.00003771
Iteration 16/1000 | Loss: 0.00003755
Iteration 17/1000 | Loss: 0.00003738
Iteration 18/1000 | Loss: 0.00003734
Iteration 19/1000 | Loss: 0.00003728
Iteration 20/1000 | Loss: 0.00003712
Iteration 21/1000 | Loss: 0.00003705
Iteration 22/1000 | Loss: 0.00003700
Iteration 23/1000 | Loss: 0.00003698
Iteration 24/1000 | Loss: 0.00003697
Iteration 25/1000 | Loss: 0.00003696
Iteration 26/1000 | Loss: 0.00003695
Iteration 27/1000 | Loss: 0.00003694
Iteration 28/1000 | Loss: 0.00003690
Iteration 29/1000 | Loss: 0.00003690
Iteration 30/1000 | Loss: 0.00003689
Iteration 31/1000 | Loss: 0.00003686
Iteration 32/1000 | Loss: 0.00003685
Iteration 33/1000 | Loss: 0.00003682
Iteration 34/1000 | Loss: 0.00003681
Iteration 35/1000 | Loss: 0.00003681
Iteration 36/1000 | Loss: 0.00003680
Iteration 37/1000 | Loss: 0.00003680
Iteration 38/1000 | Loss: 0.00003679
Iteration 39/1000 | Loss: 0.00003679
Iteration 40/1000 | Loss: 0.00003678
Iteration 41/1000 | Loss: 0.00003678
Iteration 42/1000 | Loss: 0.00003677
Iteration 43/1000 | Loss: 0.00003677
Iteration 44/1000 | Loss: 0.00003677
Iteration 45/1000 | Loss: 0.00003677
Iteration 46/1000 | Loss: 0.00003677
Iteration 47/1000 | Loss: 0.00003677
Iteration 48/1000 | Loss: 0.00003677
Iteration 49/1000 | Loss: 0.00003676
Iteration 50/1000 | Loss: 0.00003676
Iteration 51/1000 | Loss: 0.00003676
Iteration 52/1000 | Loss: 0.00003675
Iteration 53/1000 | Loss: 0.00003675
Iteration 54/1000 | Loss: 0.00003674
Iteration 55/1000 | Loss: 0.00003674
Iteration 56/1000 | Loss: 0.00003674
Iteration 57/1000 | Loss: 0.00003674
Iteration 58/1000 | Loss: 0.00003674
Iteration 59/1000 | Loss: 0.00003673
Iteration 60/1000 | Loss: 0.00003673
Iteration 61/1000 | Loss: 0.00003673
Iteration 62/1000 | Loss: 0.00003672
Iteration 63/1000 | Loss: 0.00003672
Iteration 64/1000 | Loss: 0.00003672
Iteration 65/1000 | Loss: 0.00003671
Iteration 66/1000 | Loss: 0.00003671
Iteration 67/1000 | Loss: 0.00003671
Iteration 68/1000 | Loss: 0.00003671
Iteration 69/1000 | Loss: 0.00003671
Iteration 70/1000 | Loss: 0.00003671
Iteration 71/1000 | Loss: 0.00003670
Iteration 72/1000 | Loss: 0.00003670
Iteration 73/1000 | Loss: 0.00003670
Iteration 74/1000 | Loss: 0.00003669
Iteration 75/1000 | Loss: 0.00003669
Iteration 76/1000 | Loss: 0.00003669
Iteration 77/1000 | Loss: 0.00003668
Iteration 78/1000 | Loss: 0.00003668
Iteration 79/1000 | Loss: 0.00003668
Iteration 80/1000 | Loss: 0.00003668
Iteration 81/1000 | Loss: 0.00003667
Iteration 82/1000 | Loss: 0.00003667
Iteration 83/1000 | Loss: 0.00003667
Iteration 84/1000 | Loss: 0.00003667
Iteration 85/1000 | Loss: 0.00003667
Iteration 86/1000 | Loss: 0.00003666
Iteration 87/1000 | Loss: 0.00003666
Iteration 88/1000 | Loss: 0.00003666
Iteration 89/1000 | Loss: 0.00003666
Iteration 90/1000 | Loss: 0.00003666
Iteration 91/1000 | Loss: 0.00003665
Iteration 92/1000 | Loss: 0.00003665
Iteration 93/1000 | Loss: 0.00003665
Iteration 94/1000 | Loss: 0.00003665
Iteration 95/1000 | Loss: 0.00003665
Iteration 96/1000 | Loss: 0.00003665
Iteration 97/1000 | Loss: 0.00003665
Iteration 98/1000 | Loss: 0.00003664
Iteration 99/1000 | Loss: 0.00003664
Iteration 100/1000 | Loss: 0.00003664
Iteration 101/1000 | Loss: 0.00003664
Iteration 102/1000 | Loss: 0.00003664
Iteration 103/1000 | Loss: 0.00003664
Iteration 104/1000 | Loss: 0.00003664
Iteration 105/1000 | Loss: 0.00003664
Iteration 106/1000 | Loss: 0.00003664
Iteration 107/1000 | Loss: 0.00003664
Iteration 108/1000 | Loss: 0.00003664
Iteration 109/1000 | Loss: 0.00003664
Iteration 110/1000 | Loss: 0.00003664
Iteration 111/1000 | Loss: 0.00003664
Iteration 112/1000 | Loss: 0.00003663
Iteration 113/1000 | Loss: 0.00003663
Iteration 114/1000 | Loss: 0.00003663
Iteration 115/1000 | Loss: 0.00003663
Iteration 116/1000 | Loss: 0.00003663
Iteration 117/1000 | Loss: 0.00003663
Iteration 118/1000 | Loss: 0.00003662
Iteration 119/1000 | Loss: 0.00003662
Iteration 120/1000 | Loss: 0.00003662
Iteration 121/1000 | Loss: 0.00003662
Iteration 122/1000 | Loss: 0.00003662
Iteration 123/1000 | Loss: 0.00003662
Iteration 124/1000 | Loss: 0.00003662
Iteration 125/1000 | Loss: 0.00003662
Iteration 126/1000 | Loss: 0.00003662
Iteration 127/1000 | Loss: 0.00003661
Iteration 128/1000 | Loss: 0.00003661
Iteration 129/1000 | Loss: 0.00003661
Iteration 130/1000 | Loss: 0.00003661
Iteration 131/1000 | Loss: 0.00003661
Iteration 132/1000 | Loss: 0.00003661
Iteration 133/1000 | Loss: 0.00003661
Iteration 134/1000 | Loss: 0.00003661
Iteration 135/1000 | Loss: 0.00003661
Iteration 136/1000 | Loss: 0.00003661
Iteration 137/1000 | Loss: 0.00003660
Iteration 138/1000 | Loss: 0.00003660
Iteration 139/1000 | Loss: 0.00003660
Iteration 140/1000 | Loss: 0.00003660
Iteration 141/1000 | Loss: 0.00003660
Iteration 142/1000 | Loss: 0.00003660
Iteration 143/1000 | Loss: 0.00003660
Iteration 144/1000 | Loss: 0.00003660
Iteration 145/1000 | Loss: 0.00003660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [3.660281436168589e-05, 3.660281436168589e-05, 3.660281436168589e-05, 3.660281436168589e-05, 3.660281436168589e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.660281436168589e-05

Optimization complete. Final v2v error: 4.855448246002197 mm

Highest mean error: 6.415275573730469 mm for frame 138

Lowest mean error: 4.117262840270996 mm for frame 0

Saving results

Total time: 59.31248068809509
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00518389
Iteration 2/25 | Loss: 0.00179546
Iteration 3/25 | Loss: 0.00160161
Iteration 4/25 | Loss: 0.00159104
Iteration 5/25 | Loss: 0.00158962
Iteration 6/25 | Loss: 0.00158962
Iteration 7/25 | Loss: 0.00158962
Iteration 8/25 | Loss: 0.00158962
Iteration 9/25 | Loss: 0.00158962
Iteration 10/25 | Loss: 0.00158962
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0015896230470389128, 0.0015896230470389128, 0.0015896230470389128, 0.0015896230470389128, 0.0015896230470389128]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015896230470389128

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.15206194
Iteration 2/25 | Loss: 0.00266488
Iteration 3/25 | Loss: 0.00266488
Iteration 4/25 | Loss: 0.00266488
Iteration 5/25 | Loss: 0.00266488
Iteration 6/25 | Loss: 0.00266488
Iteration 7/25 | Loss: 0.00266488
Iteration 8/25 | Loss: 0.00266488
Iteration 9/25 | Loss: 0.00266488
Iteration 10/25 | Loss: 0.00266488
Iteration 11/25 | Loss: 0.00266488
Iteration 12/25 | Loss: 0.00266488
Iteration 13/25 | Loss: 0.00266488
Iteration 14/25 | Loss: 0.00266488
Iteration 15/25 | Loss: 0.00266488
Iteration 16/25 | Loss: 0.00266488
Iteration 17/25 | Loss: 0.00266488
Iteration 18/25 | Loss: 0.00266488
Iteration 19/25 | Loss: 0.00266488
Iteration 20/25 | Loss: 0.00266488
Iteration 21/25 | Loss: 0.00266488
Iteration 22/25 | Loss: 0.00266488
Iteration 23/25 | Loss: 0.00266488
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0026648808270692825, 0.0026648808270692825, 0.0026648808270692825, 0.0026648808270692825, 0.0026648808270692825]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026648808270692825

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266488
Iteration 2/1000 | Loss: 0.00006023
Iteration 3/1000 | Loss: 0.00004121
Iteration 4/1000 | Loss: 0.00003683
Iteration 5/1000 | Loss: 0.00003510
Iteration 6/1000 | Loss: 0.00003432
Iteration 7/1000 | Loss: 0.00003391
Iteration 8/1000 | Loss: 0.00003354
Iteration 9/1000 | Loss: 0.00003320
Iteration 10/1000 | Loss: 0.00003296
Iteration 11/1000 | Loss: 0.00003285
Iteration 12/1000 | Loss: 0.00003278
Iteration 13/1000 | Loss: 0.00003263
Iteration 14/1000 | Loss: 0.00003241
Iteration 15/1000 | Loss: 0.00003225
Iteration 16/1000 | Loss: 0.00003224
Iteration 17/1000 | Loss: 0.00003224
Iteration 18/1000 | Loss: 0.00003220
Iteration 19/1000 | Loss: 0.00003220
Iteration 20/1000 | Loss: 0.00003220
Iteration 21/1000 | Loss: 0.00003220
Iteration 22/1000 | Loss: 0.00003220
Iteration 23/1000 | Loss: 0.00003220
Iteration 24/1000 | Loss: 0.00003220
Iteration 25/1000 | Loss: 0.00003220
Iteration 26/1000 | Loss: 0.00003220
Iteration 27/1000 | Loss: 0.00003219
Iteration 28/1000 | Loss: 0.00003219
Iteration 29/1000 | Loss: 0.00003218
Iteration 30/1000 | Loss: 0.00003218
Iteration 31/1000 | Loss: 0.00003215
Iteration 32/1000 | Loss: 0.00003213
Iteration 33/1000 | Loss: 0.00003209
Iteration 34/1000 | Loss: 0.00003208
Iteration 35/1000 | Loss: 0.00003198
Iteration 36/1000 | Loss: 0.00003195
Iteration 37/1000 | Loss: 0.00003189
Iteration 38/1000 | Loss: 0.00003188
Iteration 39/1000 | Loss: 0.00003186
Iteration 40/1000 | Loss: 0.00003186
Iteration 41/1000 | Loss: 0.00003185
Iteration 42/1000 | Loss: 0.00003184
Iteration 43/1000 | Loss: 0.00003182
Iteration 44/1000 | Loss: 0.00003182
Iteration 45/1000 | Loss: 0.00003182
Iteration 46/1000 | Loss: 0.00003181
Iteration 47/1000 | Loss: 0.00003181
Iteration 48/1000 | Loss: 0.00003181
Iteration 49/1000 | Loss: 0.00003180
Iteration 50/1000 | Loss: 0.00003180
Iteration 51/1000 | Loss: 0.00003179
Iteration 52/1000 | Loss: 0.00003179
Iteration 53/1000 | Loss: 0.00003178
Iteration 54/1000 | Loss: 0.00003178
Iteration 55/1000 | Loss: 0.00003178
Iteration 56/1000 | Loss: 0.00003178
Iteration 57/1000 | Loss: 0.00003178
Iteration 58/1000 | Loss: 0.00003178
Iteration 59/1000 | Loss: 0.00003178
Iteration 60/1000 | Loss: 0.00003177
Iteration 61/1000 | Loss: 0.00003177
Iteration 62/1000 | Loss: 0.00003177
Iteration 63/1000 | Loss: 0.00003177
Iteration 64/1000 | Loss: 0.00003177
Iteration 65/1000 | Loss: 0.00003176
Iteration 66/1000 | Loss: 0.00003176
Iteration 67/1000 | Loss: 0.00003176
Iteration 68/1000 | Loss: 0.00003176
Iteration 69/1000 | Loss: 0.00003176
Iteration 70/1000 | Loss: 0.00003176
Iteration 71/1000 | Loss: 0.00003176
Iteration 72/1000 | Loss: 0.00003176
Iteration 73/1000 | Loss: 0.00003176
Iteration 74/1000 | Loss: 0.00003176
Iteration 75/1000 | Loss: 0.00003176
Iteration 76/1000 | Loss: 0.00003176
Iteration 77/1000 | Loss: 0.00003176
Iteration 78/1000 | Loss: 0.00003176
Iteration 79/1000 | Loss: 0.00003176
Iteration 80/1000 | Loss: 0.00003176
Iteration 81/1000 | Loss: 0.00003176
Iteration 82/1000 | Loss: 0.00003176
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 82. Stopping optimization.
Last 5 losses: [3.1755123927723616e-05, 3.1755123927723616e-05, 3.1755123927723616e-05, 3.1755123927723616e-05, 3.1755123927723616e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1755123927723616e-05

Optimization complete. Final v2v error: 4.5643439292907715 mm

Highest mean error: 5.222412109375 mm for frame 157

Lowest mean error: 4.134237766265869 mm for frame 83

Saving results

Total time: 40.1056592464447
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01040486
Iteration 2/25 | Loss: 0.00283948
Iteration 3/25 | Loss: 0.00202242
Iteration 4/25 | Loss: 0.00189525
Iteration 5/25 | Loss: 0.00185158
Iteration 6/25 | Loss: 0.00181978
Iteration 7/25 | Loss: 0.00181317
Iteration 8/25 | Loss: 0.00182349
Iteration 9/25 | Loss: 0.00179784
Iteration 10/25 | Loss: 0.00187601
Iteration 11/25 | Loss: 0.00177368
Iteration 12/25 | Loss: 0.00168544
Iteration 13/25 | Loss: 0.00165586
Iteration 14/25 | Loss: 0.00164143
Iteration 15/25 | Loss: 0.00163471
Iteration 16/25 | Loss: 0.00163188
Iteration 17/25 | Loss: 0.00163177
Iteration 18/25 | Loss: 0.00163114
Iteration 19/25 | Loss: 0.00163074
Iteration 20/25 | Loss: 0.00163138
Iteration 21/25 | Loss: 0.00163007
Iteration 22/25 | Loss: 0.00163072
Iteration 23/25 | Loss: 0.00163084
Iteration 24/25 | Loss: 0.00163094
Iteration 25/25 | Loss: 0.00163069

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16354787
Iteration 2/25 | Loss: 0.00403674
Iteration 3/25 | Loss: 0.00403674
Iteration 4/25 | Loss: 0.00403673
Iteration 5/25 | Loss: 0.00403673
Iteration 6/25 | Loss: 0.00403673
Iteration 7/25 | Loss: 0.00403673
Iteration 8/25 | Loss: 0.00403673
Iteration 9/25 | Loss: 0.00403673
Iteration 10/25 | Loss: 0.00403673
Iteration 11/25 | Loss: 0.00403673
Iteration 12/25 | Loss: 0.00403673
Iteration 13/25 | Loss: 0.00403673
Iteration 14/25 | Loss: 0.00403673
Iteration 15/25 | Loss: 0.00403673
Iteration 16/25 | Loss: 0.00403673
Iteration 17/25 | Loss: 0.00403673
Iteration 18/25 | Loss: 0.00403673
Iteration 19/25 | Loss: 0.00403673
Iteration 20/25 | Loss: 0.00403673
Iteration 21/25 | Loss: 0.00403673
Iteration 22/25 | Loss: 0.00403673
Iteration 23/25 | Loss: 0.00403673
Iteration 24/25 | Loss: 0.00403673
Iteration 25/25 | Loss: 0.00403673

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00403673
Iteration 2/1000 | Loss: 0.00020211
Iteration 3/1000 | Loss: 0.00012132
Iteration 4/1000 | Loss: 0.00010630
Iteration 5/1000 | Loss: 0.00024002
Iteration 6/1000 | Loss: 0.00008875
Iteration 7/1000 | Loss: 0.00045996
Iteration 8/1000 | Loss: 0.00007964
Iteration 9/1000 | Loss: 0.00007486
Iteration 10/1000 | Loss: 0.00007610
Iteration 11/1000 | Loss: 0.00007478
Iteration 12/1000 | Loss: 0.00092041
Iteration 13/1000 | Loss: 0.00043449
Iteration 14/1000 | Loss: 0.00008086
Iteration 15/1000 | Loss: 0.00076448
Iteration 16/1000 | Loss: 0.00007531
Iteration 17/1000 | Loss: 0.00011668
Iteration 18/1000 | Loss: 0.00008009
Iteration 19/1000 | Loss: 0.00008184
Iteration 20/1000 | Loss: 0.00006363
Iteration 21/1000 | Loss: 0.00005638
Iteration 22/1000 | Loss: 0.00006255
Iteration 23/1000 | Loss: 0.00006507
Iteration 24/1000 | Loss: 0.00007159
Iteration 25/1000 | Loss: 0.00006566
Iteration 26/1000 | Loss: 0.00006708
Iteration 27/1000 | Loss: 0.00010520
Iteration 28/1000 | Loss: 0.00006211
Iteration 29/1000 | Loss: 0.00008538
Iteration 30/1000 | Loss: 0.00005975
Iteration 31/1000 | Loss: 0.00007853
Iteration 32/1000 | Loss: 0.00004917
Iteration 33/1000 | Loss: 0.00004681
Iteration 34/1000 | Loss: 0.00004538
Iteration 35/1000 | Loss: 0.00004492
Iteration 36/1000 | Loss: 0.00004449
Iteration 37/1000 | Loss: 0.00004412
Iteration 38/1000 | Loss: 0.00043091
Iteration 39/1000 | Loss: 0.00072148
Iteration 40/1000 | Loss: 0.00015088
Iteration 41/1000 | Loss: 0.00007957
Iteration 42/1000 | Loss: 0.00009326
Iteration 43/1000 | Loss: 0.00004923
Iteration 44/1000 | Loss: 0.00004410
Iteration 45/1000 | Loss: 0.00003904
Iteration 46/1000 | Loss: 0.00006905
Iteration 47/1000 | Loss: 0.00004076
Iteration 48/1000 | Loss: 0.00004652
Iteration 49/1000 | Loss: 0.00003606
Iteration 50/1000 | Loss: 0.00003692
Iteration 51/1000 | Loss: 0.00003520
Iteration 52/1000 | Loss: 0.00003487
Iteration 53/1000 | Loss: 0.00003596
Iteration 54/1000 | Loss: 0.00003445
Iteration 55/1000 | Loss: 0.00003441
Iteration 56/1000 | Loss: 0.00003416
Iteration 57/1000 | Loss: 0.00003395
Iteration 58/1000 | Loss: 0.00003395
Iteration 59/1000 | Loss: 0.00003388
Iteration 60/1000 | Loss: 0.00003708
Iteration 61/1000 | Loss: 0.00003399
Iteration 62/1000 | Loss: 0.00003373
Iteration 63/1000 | Loss: 0.00003373
Iteration 64/1000 | Loss: 0.00003373
Iteration 65/1000 | Loss: 0.00003373
Iteration 66/1000 | Loss: 0.00003372
Iteration 67/1000 | Loss: 0.00003372
Iteration 68/1000 | Loss: 0.00003372
Iteration 69/1000 | Loss: 0.00003372
Iteration 70/1000 | Loss: 0.00003372
Iteration 71/1000 | Loss: 0.00003372
Iteration 72/1000 | Loss: 0.00003372
Iteration 73/1000 | Loss: 0.00003370
Iteration 74/1000 | Loss: 0.00003368
Iteration 75/1000 | Loss: 0.00003368
Iteration 76/1000 | Loss: 0.00003367
Iteration 77/1000 | Loss: 0.00003367
Iteration 78/1000 | Loss: 0.00003363
Iteration 79/1000 | Loss: 0.00003363
Iteration 80/1000 | Loss: 0.00003363
Iteration 81/1000 | Loss: 0.00003362
Iteration 82/1000 | Loss: 0.00003362
Iteration 83/1000 | Loss: 0.00003362
Iteration 84/1000 | Loss: 0.00003361
Iteration 85/1000 | Loss: 0.00003361
Iteration 86/1000 | Loss: 0.00003361
Iteration 87/1000 | Loss: 0.00003360
Iteration 88/1000 | Loss: 0.00003360
Iteration 89/1000 | Loss: 0.00003360
Iteration 90/1000 | Loss: 0.00003360
Iteration 91/1000 | Loss: 0.00003360
Iteration 92/1000 | Loss: 0.00003360
Iteration 93/1000 | Loss: 0.00003360
Iteration 94/1000 | Loss: 0.00003360
Iteration 95/1000 | Loss: 0.00003360
Iteration 96/1000 | Loss: 0.00003359
Iteration 97/1000 | Loss: 0.00003357
Iteration 98/1000 | Loss: 0.00003357
Iteration 99/1000 | Loss: 0.00003357
Iteration 100/1000 | Loss: 0.00003356
Iteration 101/1000 | Loss: 0.00003356
Iteration 102/1000 | Loss: 0.00003356
Iteration 103/1000 | Loss: 0.00003356
Iteration 104/1000 | Loss: 0.00003356
Iteration 105/1000 | Loss: 0.00003356
Iteration 106/1000 | Loss: 0.00003356
Iteration 107/1000 | Loss: 0.00003356
Iteration 108/1000 | Loss: 0.00003356
Iteration 109/1000 | Loss: 0.00003356
Iteration 110/1000 | Loss: 0.00003355
Iteration 111/1000 | Loss: 0.00003355
Iteration 112/1000 | Loss: 0.00003355
Iteration 113/1000 | Loss: 0.00003355
Iteration 114/1000 | Loss: 0.00003355
Iteration 115/1000 | Loss: 0.00003355
Iteration 116/1000 | Loss: 0.00003354
Iteration 117/1000 | Loss: 0.00003354
Iteration 118/1000 | Loss: 0.00003354
Iteration 119/1000 | Loss: 0.00003354
Iteration 120/1000 | Loss: 0.00003353
Iteration 121/1000 | Loss: 0.00003353
Iteration 122/1000 | Loss: 0.00003353
Iteration 123/1000 | Loss: 0.00003353
Iteration 124/1000 | Loss: 0.00003353
Iteration 125/1000 | Loss: 0.00003353
Iteration 126/1000 | Loss: 0.00003352
Iteration 127/1000 | Loss: 0.00003352
Iteration 128/1000 | Loss: 0.00003352
Iteration 129/1000 | Loss: 0.00003352
Iteration 130/1000 | Loss: 0.00003352
Iteration 131/1000 | Loss: 0.00003352
Iteration 132/1000 | Loss: 0.00003352
Iteration 133/1000 | Loss: 0.00003352
Iteration 134/1000 | Loss: 0.00003352
Iteration 135/1000 | Loss: 0.00003351
Iteration 136/1000 | Loss: 0.00003351
Iteration 137/1000 | Loss: 0.00003351
Iteration 138/1000 | Loss: 0.00003351
Iteration 139/1000 | Loss: 0.00003350
Iteration 140/1000 | Loss: 0.00003350
Iteration 141/1000 | Loss: 0.00003350
Iteration 142/1000 | Loss: 0.00003350
Iteration 143/1000 | Loss: 0.00003350
Iteration 144/1000 | Loss: 0.00003350
Iteration 145/1000 | Loss: 0.00003350
Iteration 146/1000 | Loss: 0.00003350
Iteration 147/1000 | Loss: 0.00003350
Iteration 148/1000 | Loss: 0.00003350
Iteration 149/1000 | Loss: 0.00003350
Iteration 150/1000 | Loss: 0.00003350
Iteration 151/1000 | Loss: 0.00003349
Iteration 152/1000 | Loss: 0.00003349
Iteration 153/1000 | Loss: 0.00003349
Iteration 154/1000 | Loss: 0.00003349
Iteration 155/1000 | Loss: 0.00003349
Iteration 156/1000 | Loss: 0.00003349
Iteration 157/1000 | Loss: 0.00003349
Iteration 158/1000 | Loss: 0.00003349
Iteration 159/1000 | Loss: 0.00003349
Iteration 160/1000 | Loss: 0.00003349
Iteration 161/1000 | Loss: 0.00003349
Iteration 162/1000 | Loss: 0.00003349
Iteration 163/1000 | Loss: 0.00003349
Iteration 164/1000 | Loss: 0.00003349
Iteration 165/1000 | Loss: 0.00003349
Iteration 166/1000 | Loss: 0.00003349
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [3.348844620632008e-05, 3.348844620632008e-05, 3.348844620632008e-05, 3.348844620632008e-05, 3.348844620632008e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.348844620632008e-05

Optimization complete. Final v2v error: 4.46514892578125 mm

Highest mean error: 12.80758285522461 mm for frame 63

Lowest mean error: 3.5569498538970947 mm for frame 24

Saving results

Total time: 132.61974906921387
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00803480
Iteration 2/25 | Loss: 0.00211822
Iteration 3/25 | Loss: 0.00165810
Iteration 4/25 | Loss: 0.00161183
Iteration 5/25 | Loss: 0.00160949
Iteration 6/25 | Loss: 0.00160949
Iteration 7/25 | Loss: 0.00160949
Iteration 8/25 | Loss: 0.00160949
Iteration 9/25 | Loss: 0.00160949
Iteration 10/25 | Loss: 0.00160949
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0016094902530312538, 0.0016094902530312538, 0.0016094902530312538, 0.0016094902530312538, 0.0016094902530312538]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016094902530312538

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.50091863
Iteration 2/25 | Loss: 0.00266913
Iteration 3/25 | Loss: 0.00266909
Iteration 4/25 | Loss: 0.00266908
Iteration 5/25 | Loss: 0.00266908
Iteration 6/25 | Loss: 0.00266908
Iteration 7/25 | Loss: 0.00266908
Iteration 8/25 | Loss: 0.00266908
Iteration 9/25 | Loss: 0.00266908
Iteration 10/25 | Loss: 0.00266908
Iteration 11/25 | Loss: 0.00266908
Iteration 12/25 | Loss: 0.00266908
Iteration 13/25 | Loss: 0.00266908
Iteration 14/25 | Loss: 0.00266908
Iteration 15/25 | Loss: 0.00266908
Iteration 16/25 | Loss: 0.00266908
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.002669082721695304, 0.002669082721695304, 0.002669082721695304, 0.002669082721695304, 0.002669082721695304]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002669082721695304

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00266908
Iteration 2/1000 | Loss: 0.00005783
Iteration 3/1000 | Loss: 0.00003602
Iteration 4/1000 | Loss: 0.00003076
Iteration 5/1000 | Loss: 0.00002850
Iteration 6/1000 | Loss: 0.00002725
Iteration 7/1000 | Loss: 0.00002661
Iteration 8/1000 | Loss: 0.00002605
Iteration 9/1000 | Loss: 0.00002539
Iteration 10/1000 | Loss: 0.00002500
Iteration 11/1000 | Loss: 0.00002477
Iteration 12/1000 | Loss: 0.00002476
Iteration 13/1000 | Loss: 0.00002465
Iteration 14/1000 | Loss: 0.00002461
Iteration 15/1000 | Loss: 0.00002457
Iteration 16/1000 | Loss: 0.00002456
Iteration 17/1000 | Loss: 0.00002450
Iteration 18/1000 | Loss: 0.00002448
Iteration 19/1000 | Loss: 0.00002448
Iteration 20/1000 | Loss: 0.00002442
Iteration 21/1000 | Loss: 0.00002442
Iteration 22/1000 | Loss: 0.00002442
Iteration 23/1000 | Loss: 0.00002442
Iteration 24/1000 | Loss: 0.00002442
Iteration 25/1000 | Loss: 0.00002441
Iteration 26/1000 | Loss: 0.00002440
Iteration 27/1000 | Loss: 0.00002440
Iteration 28/1000 | Loss: 0.00002439
Iteration 29/1000 | Loss: 0.00002438
Iteration 30/1000 | Loss: 0.00002438
Iteration 31/1000 | Loss: 0.00002437
Iteration 32/1000 | Loss: 0.00002437
Iteration 33/1000 | Loss: 0.00002436
Iteration 34/1000 | Loss: 0.00002436
Iteration 35/1000 | Loss: 0.00002435
Iteration 36/1000 | Loss: 0.00002435
Iteration 37/1000 | Loss: 0.00002435
Iteration 38/1000 | Loss: 0.00002435
Iteration 39/1000 | Loss: 0.00002435
Iteration 40/1000 | Loss: 0.00002435
Iteration 41/1000 | Loss: 0.00002435
Iteration 42/1000 | Loss: 0.00002435
Iteration 43/1000 | Loss: 0.00002435
Iteration 44/1000 | Loss: 0.00002435
Iteration 45/1000 | Loss: 0.00002435
Iteration 46/1000 | Loss: 0.00002435
Iteration 47/1000 | Loss: 0.00002434
Iteration 48/1000 | Loss: 0.00002434
Iteration 49/1000 | Loss: 0.00002433
Iteration 50/1000 | Loss: 0.00002433
Iteration 51/1000 | Loss: 0.00002433
Iteration 52/1000 | Loss: 0.00002433
Iteration 53/1000 | Loss: 0.00002433
Iteration 54/1000 | Loss: 0.00002432
Iteration 55/1000 | Loss: 0.00002432
Iteration 56/1000 | Loss: 0.00002432
Iteration 57/1000 | Loss: 0.00002431
Iteration 58/1000 | Loss: 0.00002431
Iteration 59/1000 | Loss: 0.00002431
Iteration 60/1000 | Loss: 0.00002431
Iteration 61/1000 | Loss: 0.00002430
Iteration 62/1000 | Loss: 0.00002430
Iteration 63/1000 | Loss: 0.00002430
Iteration 64/1000 | Loss: 0.00002430
Iteration 65/1000 | Loss: 0.00002430
Iteration 66/1000 | Loss: 0.00002430
Iteration 67/1000 | Loss: 0.00002430
Iteration 68/1000 | Loss: 0.00002430
Iteration 69/1000 | Loss: 0.00002430
Iteration 70/1000 | Loss: 0.00002429
Iteration 71/1000 | Loss: 0.00002429
Iteration 72/1000 | Loss: 0.00002429
Iteration 73/1000 | Loss: 0.00002429
Iteration 74/1000 | Loss: 0.00002429
Iteration 75/1000 | Loss: 0.00002429
Iteration 76/1000 | Loss: 0.00002429
Iteration 77/1000 | Loss: 0.00002429
Iteration 78/1000 | Loss: 0.00002429
Iteration 79/1000 | Loss: 0.00002429
Iteration 80/1000 | Loss: 0.00002429
Iteration 81/1000 | Loss: 0.00002429
Iteration 82/1000 | Loss: 0.00002429
Iteration 83/1000 | Loss: 0.00002429
Iteration 84/1000 | Loss: 0.00002428
Iteration 85/1000 | Loss: 0.00002428
Iteration 86/1000 | Loss: 0.00002428
Iteration 87/1000 | Loss: 0.00002428
Iteration 88/1000 | Loss: 0.00002428
Iteration 89/1000 | Loss: 0.00002428
Iteration 90/1000 | Loss: 0.00002428
Iteration 91/1000 | Loss: 0.00002428
Iteration 92/1000 | Loss: 0.00002428
Iteration 93/1000 | Loss: 0.00002427
Iteration 94/1000 | Loss: 0.00002427
Iteration 95/1000 | Loss: 0.00002427
Iteration 96/1000 | Loss: 0.00002427
Iteration 97/1000 | Loss: 0.00002427
Iteration 98/1000 | Loss: 0.00002427
Iteration 99/1000 | Loss: 0.00002427
Iteration 100/1000 | Loss: 0.00002427
Iteration 101/1000 | Loss: 0.00002427
Iteration 102/1000 | Loss: 0.00002427
Iteration 103/1000 | Loss: 0.00002427
Iteration 104/1000 | Loss: 0.00002427
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 104. Stopping optimization.
Last 5 losses: [2.4271634174510837e-05, 2.4271634174510837e-05, 2.4271634174510837e-05, 2.4271634174510837e-05, 2.4271634174510837e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4271634174510837e-05

Optimization complete. Final v2v error: 4.155719757080078 mm

Highest mean error: 4.391864776611328 mm for frame 57

Lowest mean error: 3.943781852722168 mm for frame 13

Saving results

Total time: 35.95487141609192
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00840404
Iteration 2/25 | Loss: 0.00244347
Iteration 3/25 | Loss: 0.00173636
Iteration 4/25 | Loss: 0.00162557
Iteration 5/25 | Loss: 0.00162239
Iteration 6/25 | Loss: 0.00160146
Iteration 7/25 | Loss: 0.00160541
Iteration 8/25 | Loss: 0.00161179
Iteration 9/25 | Loss: 0.00160429
Iteration 10/25 | Loss: 0.00160346
Iteration 11/25 | Loss: 0.00159733
Iteration 12/25 | Loss: 0.00158531
Iteration 13/25 | Loss: 0.00158027
Iteration 14/25 | Loss: 0.00157874
Iteration 15/25 | Loss: 0.00157916
Iteration 16/25 | Loss: 0.00157681
Iteration 17/25 | Loss: 0.00157473
Iteration 18/25 | Loss: 0.00157360
Iteration 19/25 | Loss: 0.00157375
Iteration 20/25 | Loss: 0.00157720
Iteration 21/25 | Loss: 0.00157405
Iteration 22/25 | Loss: 0.00157440
Iteration 23/25 | Loss: 0.00157209
Iteration 24/25 | Loss: 0.00157415
Iteration 25/25 | Loss: 0.00157106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16563773
Iteration 2/25 | Loss: 0.00223497
Iteration 3/25 | Loss: 0.00223497
Iteration 4/25 | Loss: 0.00223497
Iteration 5/25 | Loss: 0.00223497
Iteration 6/25 | Loss: 0.00223497
Iteration 7/25 | Loss: 0.00223497
Iteration 8/25 | Loss: 0.00223497
Iteration 9/25 | Loss: 0.00223497
Iteration 10/25 | Loss: 0.00223497
Iteration 11/25 | Loss: 0.00223497
Iteration 12/25 | Loss: 0.00223497
Iteration 13/25 | Loss: 0.00223497
Iteration 14/25 | Loss: 0.00223497
Iteration 15/25 | Loss: 0.00223497
Iteration 16/25 | Loss: 0.00223497
Iteration 17/25 | Loss: 0.00223497
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.00223496719263494, 0.00223496719263494, 0.00223496719263494, 0.00223496719263494, 0.00223496719263494]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00223496719263494

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00223497
Iteration 2/1000 | Loss: 0.00006570
Iteration 3/1000 | Loss: 0.00006037
Iteration 4/1000 | Loss: 0.00017185
Iteration 5/1000 | Loss: 0.00011098
Iteration 6/1000 | Loss: 0.00009455
Iteration 7/1000 | Loss: 0.00018594
Iteration 8/1000 | Loss: 0.00011141
Iteration 9/1000 | Loss: 0.00013808
Iteration 10/1000 | Loss: 0.00015346
Iteration 11/1000 | Loss: 0.00013455
Iteration 12/1000 | Loss: 0.00014344
Iteration 13/1000 | Loss: 0.00014829
Iteration 14/1000 | Loss: 0.00013942
Iteration 15/1000 | Loss: 0.00005699
Iteration 16/1000 | Loss: 0.00005642
Iteration 17/1000 | Loss: 0.00004708
Iteration 18/1000 | Loss: 0.00004673
Iteration 19/1000 | Loss: 0.00005294
Iteration 20/1000 | Loss: 0.00005098
Iteration 21/1000 | Loss: 0.00004212
Iteration 22/1000 | Loss: 0.00004758
Iteration 23/1000 | Loss: 0.00004771
Iteration 24/1000 | Loss: 0.00005750
Iteration 25/1000 | Loss: 0.00004651
Iteration 26/1000 | Loss: 0.00003815
Iteration 27/1000 | Loss: 0.00005156
Iteration 28/1000 | Loss: 0.00004536
Iteration 29/1000 | Loss: 0.00004039
Iteration 30/1000 | Loss: 0.00003246
Iteration 31/1000 | Loss: 0.00004086
Iteration 32/1000 | Loss: 0.00003906
Iteration 33/1000 | Loss: 0.00003571
Iteration 34/1000 | Loss: 0.00003396
Iteration 35/1000 | Loss: 0.00004660
Iteration 36/1000 | Loss: 0.00003790
Iteration 37/1000 | Loss: 0.00003941
Iteration 38/1000 | Loss: 0.00003738
Iteration 39/1000 | Loss: 0.00003292
Iteration 40/1000 | Loss: 0.00003635
Iteration 41/1000 | Loss: 0.00003569
Iteration 42/1000 | Loss: 0.00003089
Iteration 43/1000 | Loss: 0.00004157
Iteration 44/1000 | Loss: 0.00003318
Iteration 45/1000 | Loss: 0.00003723
Iteration 46/1000 | Loss: 0.00004392
Iteration 47/1000 | Loss: 0.00004071
Iteration 48/1000 | Loss: 0.00003893
Iteration 49/1000 | Loss: 0.00003891
Iteration 50/1000 | Loss: 0.00004234
Iteration 51/1000 | Loss: 0.00005202
Iteration 52/1000 | Loss: 0.00004995
Iteration 53/1000 | Loss: 0.00004560
Iteration 54/1000 | Loss: 0.00004176
Iteration 55/1000 | Loss: 0.00004667
Iteration 56/1000 | Loss: 0.00003998
Iteration 57/1000 | Loss: 0.00004286
Iteration 58/1000 | Loss: 0.00004126
Iteration 59/1000 | Loss: 0.00004103
Iteration 60/1000 | Loss: 0.00003499
Iteration 61/1000 | Loss: 0.00004003
Iteration 62/1000 | Loss: 0.00003603
Iteration 63/1000 | Loss: 0.00003859
Iteration 64/1000 | Loss: 0.00003494
Iteration 65/1000 | Loss: 0.00005053
Iteration 66/1000 | Loss: 0.00003617
Iteration 67/1000 | Loss: 0.00003628
Iteration 68/1000 | Loss: 0.00003474
Iteration 69/1000 | Loss: 0.00003762
Iteration 70/1000 | Loss: 0.00003581
Iteration 71/1000 | Loss: 0.00003995
Iteration 72/1000 | Loss: 0.00004567
Iteration 73/1000 | Loss: 0.00004830
Iteration 74/1000 | Loss: 0.00004983
Iteration 75/1000 | Loss: 0.00005718
Iteration 76/1000 | Loss: 0.00004911
Iteration 77/1000 | Loss: 0.00005695
Iteration 78/1000 | Loss: 0.00003425
Iteration 79/1000 | Loss: 0.00003056
Iteration 80/1000 | Loss: 0.00003571
Iteration 81/1000 | Loss: 0.00003891
Iteration 82/1000 | Loss: 0.00003085
Iteration 83/1000 | Loss: 0.00003415
Iteration 84/1000 | Loss: 0.00003403
Iteration 85/1000 | Loss: 0.00003982
Iteration 86/1000 | Loss: 0.00003396
Iteration 87/1000 | Loss: 0.00003820
Iteration 88/1000 | Loss: 0.00003302
Iteration 89/1000 | Loss: 0.00004067
Iteration 90/1000 | Loss: 0.00003742
Iteration 91/1000 | Loss: 0.00003559
Iteration 92/1000 | Loss: 0.00003838
Iteration 93/1000 | Loss: 0.00003371
Iteration 94/1000 | Loss: 0.00003841
Iteration 95/1000 | Loss: 0.00003625
Iteration 96/1000 | Loss: 0.00003788
Iteration 97/1000 | Loss: 0.00003525
Iteration 98/1000 | Loss: 0.00003876
Iteration 99/1000 | Loss: 0.00003588
Iteration 100/1000 | Loss: 0.00003804
Iteration 101/1000 | Loss: 0.00003727
Iteration 102/1000 | Loss: 0.00003797
Iteration 103/1000 | Loss: 0.00003797
Iteration 104/1000 | Loss: 0.00003797
Iteration 105/1000 | Loss: 0.00003426
Iteration 106/1000 | Loss: 0.00002957
Iteration 107/1000 | Loss: 0.00002814
Iteration 108/1000 | Loss: 0.00002760
Iteration 109/1000 | Loss: 0.00002735
Iteration 110/1000 | Loss: 0.00002714
Iteration 111/1000 | Loss: 0.00002700
Iteration 112/1000 | Loss: 0.00002686
Iteration 113/1000 | Loss: 0.00002683
Iteration 114/1000 | Loss: 0.00002680
Iteration 115/1000 | Loss: 0.00002679
Iteration 116/1000 | Loss: 0.00002679
Iteration 117/1000 | Loss: 0.00002679
Iteration 118/1000 | Loss: 0.00002679
Iteration 119/1000 | Loss: 0.00002679
Iteration 120/1000 | Loss: 0.00002678
Iteration 121/1000 | Loss: 0.00002678
Iteration 122/1000 | Loss: 0.00002676
Iteration 123/1000 | Loss: 0.00002676
Iteration 124/1000 | Loss: 0.00002676
Iteration 125/1000 | Loss: 0.00002676
Iteration 126/1000 | Loss: 0.00002675
Iteration 127/1000 | Loss: 0.00002675
Iteration 128/1000 | Loss: 0.00002675
Iteration 129/1000 | Loss: 0.00002675
Iteration 130/1000 | Loss: 0.00002674
Iteration 131/1000 | Loss: 0.00002674
Iteration 132/1000 | Loss: 0.00002674
Iteration 133/1000 | Loss: 0.00002674
Iteration 134/1000 | Loss: 0.00002674
Iteration 135/1000 | Loss: 0.00002674
Iteration 136/1000 | Loss: 0.00002674
Iteration 137/1000 | Loss: 0.00002673
Iteration 138/1000 | Loss: 0.00002673
Iteration 139/1000 | Loss: 0.00002673
Iteration 140/1000 | Loss: 0.00002673
Iteration 141/1000 | Loss: 0.00002673
Iteration 142/1000 | Loss: 0.00002673
Iteration 143/1000 | Loss: 0.00002673
Iteration 144/1000 | Loss: 0.00002673
Iteration 145/1000 | Loss: 0.00002673
Iteration 146/1000 | Loss: 0.00002673
Iteration 147/1000 | Loss: 0.00002673
Iteration 148/1000 | Loss: 0.00002673
Iteration 149/1000 | Loss: 0.00002673
Iteration 150/1000 | Loss: 0.00002673
Iteration 151/1000 | Loss: 0.00002673
Iteration 152/1000 | Loss: 0.00002673
Iteration 153/1000 | Loss: 0.00002673
Iteration 154/1000 | Loss: 0.00002673
Iteration 155/1000 | Loss: 0.00002673
Iteration 156/1000 | Loss: 0.00002673
Iteration 157/1000 | Loss: 0.00002673
Iteration 158/1000 | Loss: 0.00002673
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 158. Stopping optimization.
Last 5 losses: [2.6725459974841215e-05, 2.6725459974841215e-05, 2.6725459974841215e-05, 2.6725459974841215e-05, 2.6725459974841215e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6725459974841215e-05

Optimization complete. Final v2v error: 4.329931735992432 mm

Highest mean error: 9.333366394042969 mm for frame 96

Lowest mean error: 3.729339599609375 mm for frame 138

Saving results

Total time: 232.176860332489
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034204
Iteration 2/25 | Loss: 0.00333033
Iteration 3/25 | Loss: 0.00234739
Iteration 4/25 | Loss: 0.00216105
Iteration 5/25 | Loss: 0.00219018
Iteration 6/25 | Loss: 0.00209388
Iteration 7/25 | Loss: 0.00200636
Iteration 8/25 | Loss: 0.00181906
Iteration 9/25 | Loss: 0.00174571
Iteration 10/25 | Loss: 0.00173590
Iteration 11/25 | Loss: 0.00168587
Iteration 12/25 | Loss: 0.00165250
Iteration 13/25 | Loss: 0.00164649
Iteration 14/25 | Loss: 0.00163509
Iteration 15/25 | Loss: 0.00163453
Iteration 16/25 | Loss: 0.00162451
Iteration 17/25 | Loss: 0.00162629
Iteration 18/25 | Loss: 0.00162132
Iteration 19/25 | Loss: 0.00161719
Iteration 20/25 | Loss: 0.00161611
Iteration 21/25 | Loss: 0.00161734
Iteration 22/25 | Loss: 0.00161494
Iteration 23/25 | Loss: 0.00161205
Iteration 24/25 | Loss: 0.00161007
Iteration 25/25 | Loss: 0.00161108

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.13976550
Iteration 2/25 | Loss: 0.00509967
Iteration 3/25 | Loss: 0.00493040
Iteration 4/25 | Loss: 0.00493040
Iteration 5/25 | Loss: 0.00493040
Iteration 6/25 | Loss: 0.00493040
Iteration 7/25 | Loss: 0.00493040
Iteration 8/25 | Loss: 0.00493040
Iteration 9/25 | Loss: 0.00493040
Iteration 10/25 | Loss: 0.00493040
Iteration 11/25 | Loss: 0.00493040
Iteration 12/25 | Loss: 0.00493040
Iteration 13/25 | Loss: 0.00493040
Iteration 14/25 | Loss: 0.00493040
Iteration 15/25 | Loss: 0.00493040
Iteration 16/25 | Loss: 0.00493040
Iteration 17/25 | Loss: 0.00493040
Iteration 18/25 | Loss: 0.00493040
Iteration 19/25 | Loss: 0.00493040
Iteration 20/25 | Loss: 0.00493040
Iteration 21/25 | Loss: 0.00493040
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.00493039982393384, 0.00493039982393384, 0.00493039982393384, 0.00493039982393384, 0.00493039982393384]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00493039982393384

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00493040
Iteration 2/1000 | Loss: 0.00070515
Iteration 3/1000 | Loss: 0.00037198
Iteration 4/1000 | Loss: 0.00057815
Iteration 5/1000 | Loss: 0.00067476
Iteration 6/1000 | Loss: 0.00128470
Iteration 7/1000 | Loss: 0.00026090
Iteration 8/1000 | Loss: 0.00056912
Iteration 9/1000 | Loss: 0.00057279
Iteration 10/1000 | Loss: 0.00045723
Iteration 11/1000 | Loss: 0.00036184
Iteration 12/1000 | Loss: 0.00019663
Iteration 13/1000 | Loss: 0.00037105
Iteration 14/1000 | Loss: 0.00039144
Iteration 15/1000 | Loss: 0.00024110
Iteration 16/1000 | Loss: 0.00035604
Iteration 17/1000 | Loss: 0.00031113
Iteration 18/1000 | Loss: 0.00029540
Iteration 19/1000 | Loss: 0.00021093
Iteration 20/1000 | Loss: 0.00018244
Iteration 21/1000 | Loss: 0.00024371
Iteration 22/1000 | Loss: 0.00021168
Iteration 23/1000 | Loss: 0.00045006
Iteration 24/1000 | Loss: 0.00027431
Iteration 25/1000 | Loss: 0.00027810
Iteration 26/1000 | Loss: 0.00016293
Iteration 27/1000 | Loss: 0.00038060
Iteration 28/1000 | Loss: 0.00052512
Iteration 29/1000 | Loss: 0.00219190
Iteration 30/1000 | Loss: 0.00158052
Iteration 31/1000 | Loss: 0.00136438
Iteration 32/1000 | Loss: 0.00246836
Iteration 33/1000 | Loss: 0.00251740
Iteration 34/1000 | Loss: 0.00217791
Iteration 35/1000 | Loss: 0.00047319
Iteration 36/1000 | Loss: 0.00029378
Iteration 37/1000 | Loss: 0.00149127
Iteration 38/1000 | Loss: 0.00030315
Iteration 39/1000 | Loss: 0.00050673
Iteration 40/1000 | Loss: 0.00016026
Iteration 41/1000 | Loss: 0.00015617
Iteration 42/1000 | Loss: 0.00016162
Iteration 43/1000 | Loss: 0.00013522
Iteration 44/1000 | Loss: 0.00014834
Iteration 45/1000 | Loss: 0.00075176
Iteration 46/1000 | Loss: 0.00013995
Iteration 47/1000 | Loss: 0.00031595
Iteration 48/1000 | Loss: 0.00016129
Iteration 49/1000 | Loss: 0.00015431
Iteration 50/1000 | Loss: 0.00035519
Iteration 51/1000 | Loss: 0.00030922
Iteration 52/1000 | Loss: 0.00039771
Iteration 53/1000 | Loss: 0.00047987
Iteration 54/1000 | Loss: 0.00047462
Iteration 55/1000 | Loss: 0.00015280
Iteration 56/1000 | Loss: 0.00015636
Iteration 57/1000 | Loss: 0.00057566
Iteration 58/1000 | Loss: 0.00072775
Iteration 59/1000 | Loss: 0.00031047
Iteration 60/1000 | Loss: 0.00035957
Iteration 61/1000 | Loss: 0.00014509
Iteration 62/1000 | Loss: 0.00013646
Iteration 63/1000 | Loss: 0.00028719
Iteration 64/1000 | Loss: 0.00025287
Iteration 65/1000 | Loss: 0.00011363
Iteration 66/1000 | Loss: 0.00010021
Iteration 67/1000 | Loss: 0.00046502
Iteration 68/1000 | Loss: 0.00011654
Iteration 69/1000 | Loss: 0.00027720
Iteration 70/1000 | Loss: 0.00022806
Iteration 71/1000 | Loss: 0.00011893
Iteration 72/1000 | Loss: 0.00011222
Iteration 73/1000 | Loss: 0.00024142
Iteration 74/1000 | Loss: 0.00053145
Iteration 75/1000 | Loss: 0.00059414
Iteration 76/1000 | Loss: 0.00035384
Iteration 77/1000 | Loss: 0.00035042
Iteration 78/1000 | Loss: 0.00026968
Iteration 79/1000 | Loss: 0.00021820
Iteration 80/1000 | Loss: 0.00010631
Iteration 81/1000 | Loss: 0.00008192
Iteration 82/1000 | Loss: 0.00009104
Iteration 83/1000 | Loss: 0.00018781
Iteration 84/1000 | Loss: 0.00014409
Iteration 85/1000 | Loss: 0.00007480
Iteration 86/1000 | Loss: 0.00033891
Iteration 87/1000 | Loss: 0.00007866
Iteration 88/1000 | Loss: 0.00007338
Iteration 89/1000 | Loss: 0.00008457
Iteration 90/1000 | Loss: 0.00023433
Iteration 91/1000 | Loss: 0.00049924
Iteration 92/1000 | Loss: 0.00050412
Iteration 93/1000 | Loss: 0.00021405
Iteration 94/1000 | Loss: 0.00007660
Iteration 95/1000 | Loss: 0.00007212
Iteration 96/1000 | Loss: 0.00006915
Iteration 97/1000 | Loss: 0.00006549
Iteration 98/1000 | Loss: 0.00006540
Iteration 99/1000 | Loss: 0.00006496
Iteration 100/1000 | Loss: 0.00006349
Iteration 101/1000 | Loss: 0.00016962
Iteration 102/1000 | Loss: 0.00036615
Iteration 103/1000 | Loss: 0.00028650
Iteration 104/1000 | Loss: 0.00019098
Iteration 105/1000 | Loss: 0.00008133
Iteration 106/1000 | Loss: 0.00008999
Iteration 107/1000 | Loss: 0.00008373
Iteration 108/1000 | Loss: 0.00006178
Iteration 109/1000 | Loss: 0.00007000
Iteration 110/1000 | Loss: 0.00013713
Iteration 111/1000 | Loss: 0.00006003
Iteration 112/1000 | Loss: 0.00007004
Iteration 113/1000 | Loss: 0.00005739
Iteration 114/1000 | Loss: 0.00005632
Iteration 115/1000 | Loss: 0.00005602
Iteration 116/1000 | Loss: 0.00005548
Iteration 117/1000 | Loss: 0.00005640
Iteration 118/1000 | Loss: 0.00005564
Iteration 119/1000 | Loss: 0.00006409
Iteration 120/1000 | Loss: 0.00005814
Iteration 121/1000 | Loss: 0.00006501
Iteration 122/1000 | Loss: 0.00005879
Iteration 123/1000 | Loss: 0.00006637
Iteration 124/1000 | Loss: 0.00007468
Iteration 125/1000 | Loss: 0.00005586
Iteration 126/1000 | Loss: 0.00047740
Iteration 127/1000 | Loss: 0.00006431
Iteration 128/1000 | Loss: 0.00005656
Iteration 129/1000 | Loss: 0.00008222
Iteration 130/1000 | Loss: 0.00005438
Iteration 131/1000 | Loss: 0.00006059
Iteration 132/1000 | Loss: 0.00011698
Iteration 133/1000 | Loss: 0.00005885
Iteration 134/1000 | Loss: 0.00005765
Iteration 135/1000 | Loss: 0.00005662
Iteration 136/1000 | Loss: 0.00006029
Iteration 137/1000 | Loss: 0.00006052
Iteration 138/1000 | Loss: 0.00006188
Iteration 139/1000 | Loss: 0.00005541
Iteration 140/1000 | Loss: 0.00005871
Iteration 141/1000 | Loss: 0.00005442
Iteration 142/1000 | Loss: 0.00005760
Iteration 143/1000 | Loss: 0.00005401
Iteration 144/1000 | Loss: 0.00005699
Iteration 145/1000 | Loss: 0.00005971
Iteration 146/1000 | Loss: 0.00007964
Iteration 147/1000 | Loss: 0.00006195
Iteration 148/1000 | Loss: 0.00005474
Iteration 149/1000 | Loss: 0.00005989
Iteration 150/1000 | Loss: 0.00005547
Iteration 151/1000 | Loss: 0.00006448
Iteration 152/1000 | Loss: 0.00005495
Iteration 153/1000 | Loss: 0.00006187
Iteration 154/1000 | Loss: 0.00005568
Iteration 155/1000 | Loss: 0.00006787
Iteration 156/1000 | Loss: 0.00005552
Iteration 157/1000 | Loss: 0.00006237
Iteration 158/1000 | Loss: 0.00005699
Iteration 159/1000 | Loss: 0.00006211
Iteration 160/1000 | Loss: 0.00005555
Iteration 161/1000 | Loss: 0.00006451
Iteration 162/1000 | Loss: 0.00005445
Iteration 163/1000 | Loss: 0.00006354
Iteration 164/1000 | Loss: 0.00007172
Iteration 165/1000 | Loss: 0.00007907
Iteration 166/1000 | Loss: 0.00005362
Iteration 167/1000 | Loss: 0.00005362
Iteration 168/1000 | Loss: 0.00007894
Iteration 169/1000 | Loss: 0.00005586
Iteration 170/1000 | Loss: 0.00006989
Iteration 171/1000 | Loss: 0.00005460
Iteration 172/1000 | Loss: 0.00006125
Iteration 173/1000 | Loss: 0.00006009
Iteration 174/1000 | Loss: 0.00005998
Iteration 175/1000 | Loss: 0.00005497
Iteration 176/1000 | Loss: 0.00005250
Iteration 177/1000 | Loss: 0.00005238
Iteration 178/1000 | Loss: 0.00005234
Iteration 179/1000 | Loss: 0.00005492
Iteration 180/1000 | Loss: 0.00005922
Iteration 181/1000 | Loss: 0.00005468
Iteration 182/1000 | Loss: 0.00005991
Iteration 183/1000 | Loss: 0.00006164
Iteration 184/1000 | Loss: 0.00006936
Iteration 185/1000 | Loss: 0.00005753
Iteration 186/1000 | Loss: 0.00005759
Iteration 187/1000 | Loss: 0.00007526
Iteration 188/1000 | Loss: 0.00005742
Iteration 189/1000 | Loss: 0.00007157
Iteration 190/1000 | Loss: 0.00005563
Iteration 191/1000 | Loss: 0.00006276
Iteration 192/1000 | Loss: 0.00005947
Iteration 193/1000 | Loss: 0.00005898
Iteration 194/1000 | Loss: 0.00005458
Iteration 195/1000 | Loss: 0.00007610
Iteration 196/1000 | Loss: 0.00005776
Iteration 197/1000 | Loss: 0.00005305
Iteration 198/1000 | Loss: 0.00005649
Iteration 199/1000 | Loss: 0.00005467
Iteration 200/1000 | Loss: 0.00005911
Iteration 201/1000 | Loss: 0.00005880
Iteration 202/1000 | Loss: 0.00005864
Iteration 203/1000 | Loss: 0.00006112
Iteration 204/1000 | Loss: 0.00006030
Iteration 205/1000 | Loss: 0.00005688
Iteration 206/1000 | Loss: 0.00005973
Iteration 207/1000 | Loss: 0.00005692
Iteration 208/1000 | Loss: 0.00006373
Iteration 209/1000 | Loss: 0.00005271
Iteration 210/1000 | Loss: 0.00005221
Iteration 211/1000 | Loss: 0.00005216
Iteration 212/1000 | Loss: 0.00005679
Iteration 213/1000 | Loss: 0.00005506
Iteration 214/1000 | Loss: 0.00005738
Iteration 215/1000 | Loss: 0.00006586
Iteration 216/1000 | Loss: 0.00005247
Iteration 217/1000 | Loss: 0.00005950
Iteration 218/1000 | Loss: 0.00005304
Iteration 219/1000 | Loss: 0.00005966
Iteration 220/1000 | Loss: 0.00005370
Iteration 221/1000 | Loss: 0.00005974
Iteration 222/1000 | Loss: 0.00005510
Iteration 223/1000 | Loss: 0.00005942
Iteration 224/1000 | Loss: 0.00005593
Iteration 225/1000 | Loss: 0.00006601
Iteration 226/1000 | Loss: 0.00005666
Iteration 227/1000 | Loss: 0.00005851
Iteration 228/1000 | Loss: 0.00005538
Iteration 229/1000 | Loss: 0.00005900
Iteration 230/1000 | Loss: 0.00005632
Iteration 231/1000 | Loss: 0.00006667
Iteration 232/1000 | Loss: 0.00005780
Iteration 233/1000 | Loss: 0.00005345
Iteration 234/1000 | Loss: 0.00006756
Iteration 235/1000 | Loss: 0.00005841
Iteration 236/1000 | Loss: 0.00007594
Iteration 237/1000 | Loss: 0.00005834
Iteration 238/1000 | Loss: 0.00006343
Iteration 239/1000 | Loss: 0.00005773
Iteration 240/1000 | Loss: 0.00005429
Iteration 241/1000 | Loss: 0.00005597
Iteration 242/1000 | Loss: 0.00007150
Iteration 243/1000 | Loss: 0.00005845
Iteration 244/1000 | Loss: 0.00005257
Iteration 245/1000 | Loss: 0.00006098
Iteration 246/1000 | Loss: 0.00005459
Iteration 247/1000 | Loss: 0.00006996
Iteration 248/1000 | Loss: 0.00005613
Iteration 249/1000 | Loss: 0.00005564
Iteration 250/1000 | Loss: 0.00005453
Iteration 251/1000 | Loss: 0.00005232
Iteration 252/1000 | Loss: 0.00006606
Iteration 253/1000 | Loss: 0.00005582
Iteration 254/1000 | Loss: 0.00006859
Iteration 255/1000 | Loss: 0.00005705
Iteration 256/1000 | Loss: 0.00007805
Iteration 257/1000 | Loss: 0.00005678
Iteration 258/1000 | Loss: 0.00005662
Iteration 259/1000 | Loss: 0.00006606
Iteration 260/1000 | Loss: 0.00005649
Iteration 261/1000 | Loss: 0.00005229
Iteration 262/1000 | Loss: 0.00005229
Iteration 263/1000 | Loss: 0.00005220
Iteration 264/1000 | Loss: 0.00006489
Iteration 265/1000 | Loss: 0.00005517
Iteration 266/1000 | Loss: 0.00006025
Iteration 267/1000 | Loss: 0.00005454
Iteration 268/1000 | Loss: 0.00006170
Iteration 269/1000 | Loss: 0.00006351
Iteration 270/1000 | Loss: 0.00005341
Iteration 271/1000 | Loss: 0.00006247
Iteration 272/1000 | Loss: 0.00005714
Iteration 273/1000 | Loss: 0.00005814
Iteration 274/1000 | Loss: 0.00005280
Iteration 275/1000 | Loss: 0.00005337
Iteration 276/1000 | Loss: 0.00005224
Iteration 277/1000 | Loss: 0.00006670
Iteration 278/1000 | Loss: 0.00005631
Iteration 279/1000 | Loss: 0.00005888
Iteration 280/1000 | Loss: 0.00005567
Iteration 281/1000 | Loss: 0.00005551
Iteration 282/1000 | Loss: 0.00005894
Iteration 283/1000 | Loss: 0.00005485
Iteration 284/1000 | Loss: 0.00005858
Iteration 285/1000 | Loss: 0.00005619
Iteration 286/1000 | Loss: 0.00005226
Iteration 287/1000 | Loss: 0.00005820
Iteration 288/1000 | Loss: 0.00005256
Iteration 289/1000 | Loss: 0.00005208
Iteration 290/1000 | Loss: 0.00005925
Iteration 291/1000 | Loss: 0.00005257
Iteration 292/1000 | Loss: 0.00006074
Iteration 293/1000 | Loss: 0.00005308
Iteration 294/1000 | Loss: 0.00006466
Iteration 295/1000 | Loss: 0.00005288
Iteration 296/1000 | Loss: 0.00006028
Iteration 297/1000 | Loss: 0.00006325
Iteration 298/1000 | Loss: 0.00005845
Iteration 299/1000 | Loss: 0.00005287
Iteration 300/1000 | Loss: 0.00006016
Iteration 301/1000 | Loss: 0.00005293
Iteration 302/1000 | Loss: 0.00005234
Iteration 303/1000 | Loss: 0.00005653
Iteration 304/1000 | Loss: 0.00005338
Iteration 305/1000 | Loss: 0.00005313
Iteration 306/1000 | Loss: 0.00005693
Iteration 307/1000 | Loss: 0.00005707
Iteration 308/1000 | Loss: 0.00005798
Iteration 309/1000 | Loss: 0.00005772
Iteration 310/1000 | Loss: 0.00005574
Iteration 311/1000 | Loss: 0.00005871
Iteration 312/1000 | Loss: 0.00006632
Iteration 313/1000 | Loss: 0.00005686
Iteration 314/1000 | Loss: 0.00005885
Iteration 315/1000 | Loss: 0.00005526
Iteration 316/1000 | Loss: 0.00006188
Iteration 317/1000 | Loss: 0.00006096
Iteration 318/1000 | Loss: 0.00006413
Iteration 319/1000 | Loss: 0.00006055
Iteration 320/1000 | Loss: 0.00006149
Iteration 321/1000 | Loss: 0.00009992
Iteration 322/1000 | Loss: 0.00006187
Iteration 323/1000 | Loss: 0.00010786
Iteration 324/1000 | Loss: 0.00006048
Iteration 325/1000 | Loss: 0.00006145
Iteration 326/1000 | Loss: 0.00006621
Iteration 327/1000 | Loss: 0.00006389
Iteration 328/1000 | Loss: 0.00006064
Iteration 329/1000 | Loss: 0.00005530
Iteration 330/1000 | Loss: 0.00006742
Iteration 331/1000 | Loss: 0.00006421
Iteration 332/1000 | Loss: 0.00006057
Iteration 333/1000 | Loss: 0.00005286
Iteration 334/1000 | Loss: 0.00005217
Iteration 335/1000 | Loss: 0.00005213
Iteration 336/1000 | Loss: 0.00005213
Iteration 337/1000 | Loss: 0.00005213
Iteration 338/1000 | Loss: 0.00005213
Iteration 339/1000 | Loss: 0.00005212
Iteration 340/1000 | Loss: 0.00005212
Iteration 341/1000 | Loss: 0.00005212
Iteration 342/1000 | Loss: 0.00005212
Iteration 343/1000 | Loss: 0.00005212
Iteration 344/1000 | Loss: 0.00005212
Iteration 345/1000 | Loss: 0.00005212
Iteration 346/1000 | Loss: 0.00005212
Iteration 347/1000 | Loss: 0.00005212
Iteration 348/1000 | Loss: 0.00005212
Iteration 349/1000 | Loss: 0.00005211
Iteration 350/1000 | Loss: 0.00005211
Iteration 351/1000 | Loss: 0.00005208
Iteration 352/1000 | Loss: 0.00005207
Iteration 353/1000 | Loss: 0.00005207
Iteration 354/1000 | Loss: 0.00005206
Iteration 355/1000 | Loss: 0.00005206
Iteration 356/1000 | Loss: 0.00005206
Iteration 357/1000 | Loss: 0.00005205
Iteration 358/1000 | Loss: 0.00005204
Iteration 359/1000 | Loss: 0.00005204
Iteration 360/1000 | Loss: 0.00005203
Iteration 361/1000 | Loss: 0.00005202
Iteration 362/1000 | Loss: 0.00005765
Iteration 363/1000 | Loss: 0.00005322
Iteration 364/1000 | Loss: 0.00005200
Iteration 365/1000 | Loss: 0.00005200
Iteration 366/1000 | Loss: 0.00005200
Iteration 367/1000 | Loss: 0.00005200
Iteration 368/1000 | Loss: 0.00005200
Iteration 369/1000 | Loss: 0.00005200
Iteration 370/1000 | Loss: 0.00005200
Iteration 371/1000 | Loss: 0.00005200
Iteration 372/1000 | Loss: 0.00005200
Iteration 373/1000 | Loss: 0.00005200
Iteration 374/1000 | Loss: 0.00005200
Iteration 375/1000 | Loss: 0.00005199
Iteration 376/1000 | Loss: 0.00005199
Iteration 377/1000 | Loss: 0.00005199
Iteration 378/1000 | Loss: 0.00005199
Iteration 379/1000 | Loss: 0.00005199
Iteration 380/1000 | Loss: 0.00005199
Iteration 381/1000 | Loss: 0.00005199
Iteration 382/1000 | Loss: 0.00005199
Iteration 383/1000 | Loss: 0.00005199
Iteration 384/1000 | Loss: 0.00005199
Iteration 385/1000 | Loss: 0.00005199
Iteration 386/1000 | Loss: 0.00005199
Iteration 387/1000 | Loss: 0.00005199
Iteration 388/1000 | Loss: 0.00005199
Iteration 389/1000 | Loss: 0.00005199
Iteration 390/1000 | Loss: 0.00005199
Iteration 391/1000 | Loss: 0.00005199
Iteration 392/1000 | Loss: 0.00005199
Iteration 393/1000 | Loss: 0.00005199
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 393. Stopping optimization.
Last 5 losses: [5.198537837713957e-05, 5.198537837713957e-05, 5.198537837713957e-05, 5.198537837713957e-05, 5.198537837713957e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 5.198537837713957e-05

Optimization complete. Final v2v error: 4.095038890838623 mm

Highest mean error: 11.806238174438477 mm for frame 127

Lowest mean error: 2.945725202560425 mm for frame 236

Saving results

Total time: 575.7497749328613
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00601972
Iteration 2/25 | Loss: 0.00172030
Iteration 3/25 | Loss: 0.00166099
Iteration 4/25 | Loss: 0.00164471
Iteration 5/25 | Loss: 0.00164029
Iteration 6/25 | Loss: 0.00163915
Iteration 7/25 | Loss: 0.00163915
Iteration 8/25 | Loss: 0.00163915
Iteration 9/25 | Loss: 0.00163915
Iteration 10/25 | Loss: 0.00163915
Iteration 11/25 | Loss: 0.00163915
Iteration 12/25 | Loss: 0.00163915
Iteration 13/25 | Loss: 0.00163915
Iteration 14/25 | Loss: 0.00163915
Iteration 15/25 | Loss: 0.00163915
Iteration 16/25 | Loss: 0.00163915
Iteration 17/25 | Loss: 0.00163915
Iteration 18/25 | Loss: 0.00163915
Iteration 19/25 | Loss: 0.00163915
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 19. Stopping optimization.
Last 5 losses: [0.0016391533426940441, 0.0016391533426940441, 0.0016391533426940441, 0.0016391533426940441, 0.0016391533426940441]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016391533426940441

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.84744900
Iteration 2/25 | Loss: 0.00238975
Iteration 3/25 | Loss: 0.00238972
Iteration 4/25 | Loss: 0.00238972
Iteration 5/25 | Loss: 0.00238972
Iteration 6/25 | Loss: 0.00238972
Iteration 7/25 | Loss: 0.00238972
Iteration 8/25 | Loss: 0.00238972
Iteration 9/25 | Loss: 0.00238972
Iteration 10/25 | Loss: 0.00238972
Iteration 11/25 | Loss: 0.00238972
Iteration 12/25 | Loss: 0.00238972
Iteration 13/25 | Loss: 0.00238972
Iteration 14/25 | Loss: 0.00238972
Iteration 15/25 | Loss: 0.00238972
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.002389720408245921, 0.002389720408245921, 0.002389720408245921, 0.002389720408245921, 0.002389720408245921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.002389720408245921

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00238972
Iteration 2/1000 | Loss: 0.00006105
Iteration 3/1000 | Loss: 0.00004934
Iteration 4/1000 | Loss: 0.00004179
Iteration 5/1000 | Loss: 0.00003925
Iteration 6/1000 | Loss: 0.00003798
Iteration 7/1000 | Loss: 0.00003706
Iteration 8/1000 | Loss: 0.00003638
Iteration 9/1000 | Loss: 0.00003597
Iteration 10/1000 | Loss: 0.00003573
Iteration 11/1000 | Loss: 0.00003567
Iteration 12/1000 | Loss: 0.00003567
Iteration 13/1000 | Loss: 0.00003567
Iteration 14/1000 | Loss: 0.00003567
Iteration 15/1000 | Loss: 0.00003567
Iteration 16/1000 | Loss: 0.00003567
Iteration 17/1000 | Loss: 0.00003567
Iteration 18/1000 | Loss: 0.00003567
Iteration 19/1000 | Loss: 0.00003567
Iteration 20/1000 | Loss: 0.00003566
Iteration 21/1000 | Loss: 0.00003553
Iteration 22/1000 | Loss: 0.00003553
Iteration 23/1000 | Loss: 0.00003552
Iteration 24/1000 | Loss: 0.00003552
Iteration 25/1000 | Loss: 0.00003552
Iteration 26/1000 | Loss: 0.00003550
Iteration 27/1000 | Loss: 0.00003549
Iteration 28/1000 | Loss: 0.00003549
Iteration 29/1000 | Loss: 0.00003549
Iteration 30/1000 | Loss: 0.00003547
Iteration 31/1000 | Loss: 0.00003544
Iteration 32/1000 | Loss: 0.00003544
Iteration 33/1000 | Loss: 0.00003544
Iteration 34/1000 | Loss: 0.00003544
Iteration 35/1000 | Loss: 0.00003544
Iteration 36/1000 | Loss: 0.00003544
Iteration 37/1000 | Loss: 0.00003543
Iteration 38/1000 | Loss: 0.00003543
Iteration 39/1000 | Loss: 0.00003543
Iteration 40/1000 | Loss: 0.00003543
Iteration 41/1000 | Loss: 0.00003543
Iteration 42/1000 | Loss: 0.00003543
Iteration 43/1000 | Loss: 0.00003543
Iteration 44/1000 | Loss: 0.00003543
Iteration 45/1000 | Loss: 0.00003543
Iteration 46/1000 | Loss: 0.00003543
Iteration 47/1000 | Loss: 0.00003541
Iteration 48/1000 | Loss: 0.00003541
Iteration 49/1000 | Loss: 0.00003541
Iteration 50/1000 | Loss: 0.00003541
Iteration 51/1000 | Loss: 0.00003541
Iteration 52/1000 | Loss: 0.00003541
Iteration 53/1000 | Loss: 0.00003540
Iteration 54/1000 | Loss: 0.00003540
Iteration 55/1000 | Loss: 0.00003540
Iteration 56/1000 | Loss: 0.00003540
Iteration 57/1000 | Loss: 0.00003540
Iteration 58/1000 | Loss: 0.00003540
Iteration 59/1000 | Loss: 0.00003540
Iteration 60/1000 | Loss: 0.00003540
Iteration 61/1000 | Loss: 0.00003540
Iteration 62/1000 | Loss: 0.00003540
Iteration 63/1000 | Loss: 0.00003539
Iteration 64/1000 | Loss: 0.00003539
Iteration 65/1000 | Loss: 0.00003538
Iteration 66/1000 | Loss: 0.00003538
Iteration 67/1000 | Loss: 0.00003538
Iteration 68/1000 | Loss: 0.00003538
Iteration 69/1000 | Loss: 0.00003538
Iteration 70/1000 | Loss: 0.00003538
Iteration 71/1000 | Loss: 0.00003538
Iteration 72/1000 | Loss: 0.00003538
Iteration 73/1000 | Loss: 0.00003538
Iteration 74/1000 | Loss: 0.00003538
Iteration 75/1000 | Loss: 0.00003538
Iteration 76/1000 | Loss: 0.00003538
Iteration 77/1000 | Loss: 0.00003538
Iteration 78/1000 | Loss: 0.00003538
Iteration 79/1000 | Loss: 0.00003538
Iteration 80/1000 | Loss: 0.00003538
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 80. Stopping optimization.
Last 5 losses: [3.537825250532478e-05, 3.537825250532478e-05, 3.537825250532478e-05, 3.537825250532478e-05, 3.537825250532478e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.537825250532478e-05

Optimization complete. Final v2v error: 5.022695541381836 mm

Highest mean error: 5.096469402313232 mm for frame 47

Lowest mean error: 4.900578022003174 mm for frame 137

Saving results

Total time: 28.890910863876343
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00559543
Iteration 2/25 | Loss: 0.00166993
Iteration 3/25 | Loss: 0.00155141
Iteration 4/25 | Loss: 0.00153651
Iteration 5/25 | Loss: 0.00153174
Iteration 6/25 | Loss: 0.00153088
Iteration 7/25 | Loss: 0.00153088
Iteration 8/25 | Loss: 0.00153088
Iteration 9/25 | Loss: 0.00153088
Iteration 10/25 | Loss: 0.00153088
Iteration 11/25 | Loss: 0.00153088
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0015308753354474902, 0.0015308753354474902, 0.0015308753354474902, 0.0015308753354474902, 0.0015308753354474902]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015308753354474902

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 5.18847656
Iteration 2/25 | Loss: 0.00252774
Iteration 3/25 | Loss: 0.00252773
Iteration 4/25 | Loss: 0.00252773
Iteration 5/25 | Loss: 0.00252772
Iteration 6/25 | Loss: 0.00252772
Iteration 7/25 | Loss: 0.00252772
Iteration 8/25 | Loss: 0.00252772
Iteration 9/25 | Loss: 0.00252772
Iteration 10/25 | Loss: 0.00252772
Iteration 11/25 | Loss: 0.00252772
Iteration 12/25 | Loss: 0.00252772
Iteration 13/25 | Loss: 0.00252772
Iteration 14/25 | Loss: 0.00252772
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0025277226231992245, 0.0025277226231992245, 0.0025277226231992245, 0.0025277226231992245, 0.0025277226231992245]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0025277226231992245

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00252772
Iteration 2/1000 | Loss: 0.00004761
Iteration 3/1000 | Loss: 0.00002743
Iteration 4/1000 | Loss: 0.00002356
Iteration 5/1000 | Loss: 0.00002100
Iteration 6/1000 | Loss: 0.00001973
Iteration 7/1000 | Loss: 0.00001898
Iteration 8/1000 | Loss: 0.00001855
Iteration 9/1000 | Loss: 0.00001822
Iteration 10/1000 | Loss: 0.00001797
Iteration 11/1000 | Loss: 0.00001771
Iteration 12/1000 | Loss: 0.00001764
Iteration 13/1000 | Loss: 0.00001759
Iteration 14/1000 | Loss: 0.00001752
Iteration 15/1000 | Loss: 0.00001748
Iteration 16/1000 | Loss: 0.00001744
Iteration 17/1000 | Loss: 0.00001743
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001741
Iteration 21/1000 | Loss: 0.00001740
Iteration 22/1000 | Loss: 0.00001740
Iteration 23/1000 | Loss: 0.00001740
Iteration 24/1000 | Loss: 0.00001740
Iteration 25/1000 | Loss: 0.00001740
Iteration 26/1000 | Loss: 0.00001740
Iteration 27/1000 | Loss: 0.00001738
Iteration 28/1000 | Loss: 0.00001738
Iteration 29/1000 | Loss: 0.00001737
Iteration 30/1000 | Loss: 0.00001737
Iteration 31/1000 | Loss: 0.00001737
Iteration 32/1000 | Loss: 0.00001736
Iteration 33/1000 | Loss: 0.00001736
Iteration 34/1000 | Loss: 0.00001736
Iteration 35/1000 | Loss: 0.00001735
Iteration 36/1000 | Loss: 0.00001735
Iteration 37/1000 | Loss: 0.00001734
Iteration 38/1000 | Loss: 0.00001734
Iteration 39/1000 | Loss: 0.00001734
Iteration 40/1000 | Loss: 0.00001734
Iteration 41/1000 | Loss: 0.00001734
Iteration 42/1000 | Loss: 0.00001734
Iteration 43/1000 | Loss: 0.00001734
Iteration 44/1000 | Loss: 0.00001734
Iteration 45/1000 | Loss: 0.00001734
Iteration 46/1000 | Loss: 0.00001734
Iteration 47/1000 | Loss: 0.00001734
Iteration 48/1000 | Loss: 0.00001734
Iteration 49/1000 | Loss: 0.00001734
Iteration 50/1000 | Loss: 0.00001733
Iteration 51/1000 | Loss: 0.00001733
Iteration 52/1000 | Loss: 0.00001733
Iteration 53/1000 | Loss: 0.00001733
Iteration 54/1000 | Loss: 0.00001733
Iteration 55/1000 | Loss: 0.00001732
Iteration 56/1000 | Loss: 0.00001732
Iteration 57/1000 | Loss: 0.00001732
Iteration 58/1000 | Loss: 0.00001732
Iteration 59/1000 | Loss: 0.00001731
Iteration 60/1000 | Loss: 0.00001731
Iteration 61/1000 | Loss: 0.00001731
Iteration 62/1000 | Loss: 0.00001731
Iteration 63/1000 | Loss: 0.00001731
Iteration 64/1000 | Loss: 0.00001730
Iteration 65/1000 | Loss: 0.00001730
Iteration 66/1000 | Loss: 0.00001730
Iteration 67/1000 | Loss: 0.00001730
Iteration 68/1000 | Loss: 0.00001730
Iteration 69/1000 | Loss: 0.00001730
Iteration 70/1000 | Loss: 0.00001730
Iteration 71/1000 | Loss: 0.00001730
Iteration 72/1000 | Loss: 0.00001729
Iteration 73/1000 | Loss: 0.00001729
Iteration 74/1000 | Loss: 0.00001729
Iteration 75/1000 | Loss: 0.00001729
Iteration 76/1000 | Loss: 0.00001728
Iteration 77/1000 | Loss: 0.00001728
Iteration 78/1000 | Loss: 0.00001728
Iteration 79/1000 | Loss: 0.00001727
Iteration 80/1000 | Loss: 0.00001727
Iteration 81/1000 | Loss: 0.00001727
Iteration 82/1000 | Loss: 0.00001726
Iteration 83/1000 | Loss: 0.00001726
Iteration 84/1000 | Loss: 0.00001726
Iteration 85/1000 | Loss: 0.00001726
Iteration 86/1000 | Loss: 0.00001725
Iteration 87/1000 | Loss: 0.00001725
Iteration 88/1000 | Loss: 0.00001725
Iteration 89/1000 | Loss: 0.00001725
Iteration 90/1000 | Loss: 0.00001725
Iteration 91/1000 | Loss: 0.00001725
Iteration 92/1000 | Loss: 0.00001725
Iteration 93/1000 | Loss: 0.00001725
Iteration 94/1000 | Loss: 0.00001725
Iteration 95/1000 | Loss: 0.00001725
Iteration 96/1000 | Loss: 0.00001725
Iteration 97/1000 | Loss: 0.00001724
Iteration 98/1000 | Loss: 0.00001724
Iteration 99/1000 | Loss: 0.00001724
Iteration 100/1000 | Loss: 0.00001724
Iteration 101/1000 | Loss: 0.00001724
Iteration 102/1000 | Loss: 0.00001723
Iteration 103/1000 | Loss: 0.00001723
Iteration 104/1000 | Loss: 0.00001723
Iteration 105/1000 | Loss: 0.00001723
Iteration 106/1000 | Loss: 0.00001723
Iteration 107/1000 | Loss: 0.00001723
Iteration 108/1000 | Loss: 0.00001723
Iteration 109/1000 | Loss: 0.00001723
Iteration 110/1000 | Loss: 0.00001723
Iteration 111/1000 | Loss: 0.00001723
Iteration 112/1000 | Loss: 0.00001723
Iteration 113/1000 | Loss: 0.00001723
Iteration 114/1000 | Loss: 0.00001722
Iteration 115/1000 | Loss: 0.00001722
Iteration 116/1000 | Loss: 0.00001722
Iteration 117/1000 | Loss: 0.00001722
Iteration 118/1000 | Loss: 0.00001722
Iteration 119/1000 | Loss: 0.00001722
Iteration 120/1000 | Loss: 0.00001722
Iteration 121/1000 | Loss: 0.00001721
Iteration 122/1000 | Loss: 0.00001721
Iteration 123/1000 | Loss: 0.00001721
Iteration 124/1000 | Loss: 0.00001721
Iteration 125/1000 | Loss: 0.00001721
Iteration 126/1000 | Loss: 0.00001721
Iteration 127/1000 | Loss: 0.00001721
Iteration 128/1000 | Loss: 0.00001721
Iteration 129/1000 | Loss: 0.00001721
Iteration 130/1000 | Loss: 0.00001721
Iteration 131/1000 | Loss: 0.00001721
Iteration 132/1000 | Loss: 0.00001721
Iteration 133/1000 | Loss: 0.00001721
Iteration 134/1000 | Loss: 0.00001721
Iteration 135/1000 | Loss: 0.00001721
Iteration 136/1000 | Loss: 0.00001721
Iteration 137/1000 | Loss: 0.00001721
Iteration 138/1000 | Loss: 0.00001721
Iteration 139/1000 | Loss: 0.00001721
Iteration 140/1000 | Loss: 0.00001721
Iteration 141/1000 | Loss: 0.00001721
Iteration 142/1000 | Loss: 0.00001721
Iteration 143/1000 | Loss: 0.00001721
Iteration 144/1000 | Loss: 0.00001721
Iteration 145/1000 | Loss: 0.00001721
Iteration 146/1000 | Loss: 0.00001721
Iteration 147/1000 | Loss: 0.00001721
Iteration 148/1000 | Loss: 0.00001721
Iteration 149/1000 | Loss: 0.00001721
Iteration 150/1000 | Loss: 0.00001720
Iteration 151/1000 | Loss: 0.00001720
Iteration 152/1000 | Loss: 0.00001720
Iteration 153/1000 | Loss: 0.00001720
Iteration 154/1000 | Loss: 0.00001720
Iteration 155/1000 | Loss: 0.00001720
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 155. Stopping optimization.
Last 5 losses: [1.720499130897224e-05, 1.720499130897224e-05, 1.720499130897224e-05, 1.720499130897224e-05, 1.720499130897224e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.720499130897224e-05

Optimization complete. Final v2v error: 3.4973340034484863 mm

Highest mean error: 4.17485237121582 mm for frame 47

Lowest mean error: 3.0828707218170166 mm for frame 116

Saving results

Total time: 35.211310148239136
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00799305
Iteration 2/25 | Loss: 0.00178842
Iteration 3/25 | Loss: 0.00169164
Iteration 4/25 | Loss: 0.00162004
Iteration 5/25 | Loss: 0.00161009
Iteration 6/25 | Loss: 0.00160687
Iteration 7/25 | Loss: 0.00160587
Iteration 8/25 | Loss: 0.00160542
Iteration 9/25 | Loss: 0.00160536
Iteration 10/25 | Loss: 0.00160536
Iteration 11/25 | Loss: 0.00160536
Iteration 12/25 | Loss: 0.00160536
Iteration 13/25 | Loss: 0.00160536
Iteration 14/25 | Loss: 0.00160536
Iteration 15/25 | Loss: 0.00160536
Iteration 16/25 | Loss: 0.00160536
Iteration 17/25 | Loss: 0.00160536
Iteration 18/25 | Loss: 0.00160536
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0016053591389209032, 0.0016053591389209032, 0.0016053591389209032, 0.0016053591389209032, 0.0016053591389209032]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0016053591389209032

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.50236773
Iteration 2/25 | Loss: 0.00427581
Iteration 3/25 | Loss: 0.00427578
Iteration 4/25 | Loss: 0.00427578
Iteration 5/25 | Loss: 0.00427578
Iteration 6/25 | Loss: 0.00427578
Iteration 7/25 | Loss: 0.00427578
Iteration 8/25 | Loss: 0.00427578
Iteration 9/25 | Loss: 0.00427578
Iteration 10/25 | Loss: 0.00427578
Iteration 11/25 | Loss: 0.00427578
Iteration 12/25 | Loss: 0.00427578
Iteration 13/25 | Loss: 0.00427578
Iteration 14/25 | Loss: 0.00427578
Iteration 15/25 | Loss: 0.00427578
Iteration 16/25 | Loss: 0.00427578
Iteration 17/25 | Loss: 0.00427578
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.004275776445865631, 0.004275776445865631, 0.004275776445865631, 0.004275776445865631, 0.004275776445865631]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.004275776445865631

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00427578
Iteration 2/1000 | Loss: 0.00008509
Iteration 3/1000 | Loss: 0.00004336
Iteration 4/1000 | Loss: 0.00003184
Iteration 5/1000 | Loss: 0.00002796
Iteration 6/1000 | Loss: 0.00002576
Iteration 7/1000 | Loss: 0.00002447
Iteration 8/1000 | Loss: 0.00002374
Iteration 9/1000 | Loss: 0.00002311
Iteration 10/1000 | Loss: 0.00002261
Iteration 11/1000 | Loss: 0.00002217
Iteration 12/1000 | Loss: 0.00002187
Iteration 13/1000 | Loss: 0.00002172
Iteration 14/1000 | Loss: 0.00002168
Iteration 15/1000 | Loss: 0.00002165
Iteration 16/1000 | Loss: 0.00002161
Iteration 17/1000 | Loss: 0.00002156
Iteration 18/1000 | Loss: 0.00002151
Iteration 19/1000 | Loss: 0.00002146
Iteration 20/1000 | Loss: 0.00002146
Iteration 21/1000 | Loss: 0.00002145
Iteration 22/1000 | Loss: 0.00002144
Iteration 23/1000 | Loss: 0.00002144
Iteration 24/1000 | Loss: 0.00002144
Iteration 25/1000 | Loss: 0.00002144
Iteration 26/1000 | Loss: 0.00002143
Iteration 27/1000 | Loss: 0.00002142
Iteration 28/1000 | Loss: 0.00002141
Iteration 29/1000 | Loss: 0.00002140
Iteration 30/1000 | Loss: 0.00002140
Iteration 31/1000 | Loss: 0.00002140
Iteration 32/1000 | Loss: 0.00002139
Iteration 33/1000 | Loss: 0.00002138
Iteration 34/1000 | Loss: 0.00002138
Iteration 35/1000 | Loss: 0.00002138
Iteration 36/1000 | Loss: 0.00002137
Iteration 37/1000 | Loss: 0.00002137
Iteration 38/1000 | Loss: 0.00002136
Iteration 39/1000 | Loss: 0.00002136
Iteration 40/1000 | Loss: 0.00002135
Iteration 41/1000 | Loss: 0.00002134
Iteration 42/1000 | Loss: 0.00002133
Iteration 43/1000 | Loss: 0.00002133
Iteration 44/1000 | Loss: 0.00002133
Iteration 45/1000 | Loss: 0.00002133
Iteration 46/1000 | Loss: 0.00002133
Iteration 47/1000 | Loss: 0.00002133
Iteration 48/1000 | Loss: 0.00002133
Iteration 49/1000 | Loss: 0.00002133
Iteration 50/1000 | Loss: 0.00002133
Iteration 51/1000 | Loss: 0.00002132
Iteration 52/1000 | Loss: 0.00002132
Iteration 53/1000 | Loss: 0.00002132
Iteration 54/1000 | Loss: 0.00002132
Iteration 55/1000 | Loss: 0.00002132
Iteration 56/1000 | Loss: 0.00002132
Iteration 57/1000 | Loss: 0.00002132
Iteration 58/1000 | Loss: 0.00002132
Iteration 59/1000 | Loss: 0.00002132
Iteration 60/1000 | Loss: 0.00002132
Iteration 61/1000 | Loss: 0.00002132
Iteration 62/1000 | Loss: 0.00002132
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 62. Stopping optimization.
Last 5 losses: [2.1323643522919156e-05, 2.1323643522919156e-05, 2.1323643522919156e-05, 2.1323643522919156e-05, 2.1323643522919156e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.1323643522919156e-05

Optimization complete. Final v2v error: 3.9023919105529785 mm

Highest mean error: 4.323984622955322 mm for frame 96

Lowest mean error: 3.543670654296875 mm for frame 18

Saving results

Total time: 37.57084894180298
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00420401
Iteration 2/25 | Loss: 0.00160579
Iteration 3/25 | Loss: 0.00153885
Iteration 4/25 | Loss: 0.00153566
Iteration 5/25 | Loss: 0.00153475
Iteration 6/25 | Loss: 0.00153475
Iteration 7/25 | Loss: 0.00153475
Iteration 8/25 | Loss: 0.00153475
Iteration 9/25 | Loss: 0.00153475
Iteration 10/25 | Loss: 0.00153475
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.001534754759632051, 0.001534754759632051, 0.001534754759632051, 0.001534754759632051, 0.001534754759632051]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001534754759632051

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25100327
Iteration 2/25 | Loss: 0.00282564
Iteration 3/25 | Loss: 0.00282564
Iteration 4/25 | Loss: 0.00282564
Iteration 5/25 | Loss: 0.00282564
Iteration 6/25 | Loss: 0.00282564
Iteration 7/25 | Loss: 0.00282564
Iteration 8/25 | Loss: 0.00282563
Iteration 9/25 | Loss: 0.00282563
Iteration 10/25 | Loss: 0.00282563
Iteration 11/25 | Loss: 0.00282563
Iteration 12/25 | Loss: 0.00282563
Iteration 13/25 | Loss: 0.00282563
Iteration 14/25 | Loss: 0.00282563
Iteration 15/25 | Loss: 0.00282563
Iteration 16/25 | Loss: 0.00282563
Iteration 17/25 | Loss: 0.00282563
Iteration 18/25 | Loss: 0.00282563
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.00282563385553658, 0.00282563385553658, 0.00282563385553658, 0.00282563385553658, 0.00282563385553658]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00282563385553658

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00282563
Iteration 2/1000 | Loss: 0.00004864
Iteration 3/1000 | Loss: 0.00002670
Iteration 4/1000 | Loss: 0.00002377
Iteration 5/1000 | Loss: 0.00002154
Iteration 6/1000 | Loss: 0.00002055
Iteration 7/1000 | Loss: 0.00002008
Iteration 8/1000 | Loss: 0.00001965
Iteration 9/1000 | Loss: 0.00001934
Iteration 10/1000 | Loss: 0.00001924
Iteration 11/1000 | Loss: 0.00001914
Iteration 12/1000 | Loss: 0.00001909
Iteration 13/1000 | Loss: 0.00001908
Iteration 14/1000 | Loss: 0.00001906
Iteration 15/1000 | Loss: 0.00001905
Iteration 16/1000 | Loss: 0.00001904
Iteration 17/1000 | Loss: 0.00001903
Iteration 18/1000 | Loss: 0.00001901
Iteration 19/1000 | Loss: 0.00001901
Iteration 20/1000 | Loss: 0.00001900
Iteration 21/1000 | Loss: 0.00001900
Iteration 22/1000 | Loss: 0.00001900
Iteration 23/1000 | Loss: 0.00001900
Iteration 24/1000 | Loss: 0.00001900
Iteration 25/1000 | Loss: 0.00001900
Iteration 26/1000 | Loss: 0.00001900
Iteration 27/1000 | Loss: 0.00001900
Iteration 28/1000 | Loss: 0.00001900
Iteration 29/1000 | Loss: 0.00001900
Iteration 30/1000 | Loss: 0.00001900
Iteration 31/1000 | Loss: 0.00001899
Iteration 32/1000 | Loss: 0.00001899
Iteration 33/1000 | Loss: 0.00001899
Iteration 34/1000 | Loss: 0.00001899
Iteration 35/1000 | Loss: 0.00001897
Iteration 36/1000 | Loss: 0.00001894
Iteration 37/1000 | Loss: 0.00001894
Iteration 38/1000 | Loss: 0.00001894
Iteration 39/1000 | Loss: 0.00001894
Iteration 40/1000 | Loss: 0.00001894
Iteration 41/1000 | Loss: 0.00001892
Iteration 42/1000 | Loss: 0.00001891
Iteration 43/1000 | Loss: 0.00001891
Iteration 44/1000 | Loss: 0.00001891
Iteration 45/1000 | Loss: 0.00001887
Iteration 46/1000 | Loss: 0.00001885
Iteration 47/1000 | Loss: 0.00001884
Iteration 48/1000 | Loss: 0.00001882
Iteration 49/1000 | Loss: 0.00001881
Iteration 50/1000 | Loss: 0.00001881
Iteration 51/1000 | Loss: 0.00001880
Iteration 52/1000 | Loss: 0.00001880
Iteration 53/1000 | Loss: 0.00001879
Iteration 54/1000 | Loss: 0.00001879
Iteration 55/1000 | Loss: 0.00001878
Iteration 56/1000 | Loss: 0.00001878
Iteration 57/1000 | Loss: 0.00001878
Iteration 58/1000 | Loss: 0.00001878
Iteration 59/1000 | Loss: 0.00001877
Iteration 60/1000 | Loss: 0.00001877
Iteration 61/1000 | Loss: 0.00001877
Iteration 62/1000 | Loss: 0.00001877
Iteration 63/1000 | Loss: 0.00001877
Iteration 64/1000 | Loss: 0.00001877
Iteration 65/1000 | Loss: 0.00001876
Iteration 66/1000 | Loss: 0.00001876
Iteration 67/1000 | Loss: 0.00001875
Iteration 68/1000 | Loss: 0.00001875
Iteration 69/1000 | Loss: 0.00001875
Iteration 70/1000 | Loss: 0.00001875
Iteration 71/1000 | Loss: 0.00001875
Iteration 72/1000 | Loss: 0.00001875
Iteration 73/1000 | Loss: 0.00001874
Iteration 74/1000 | Loss: 0.00001874
Iteration 75/1000 | Loss: 0.00001874
Iteration 76/1000 | Loss: 0.00001873
Iteration 77/1000 | Loss: 0.00001873
Iteration 78/1000 | Loss: 0.00001873
Iteration 79/1000 | Loss: 0.00001872
Iteration 80/1000 | Loss: 0.00001872
Iteration 81/1000 | Loss: 0.00001872
Iteration 82/1000 | Loss: 0.00001872
Iteration 83/1000 | Loss: 0.00001872
Iteration 84/1000 | Loss: 0.00001872
Iteration 85/1000 | Loss: 0.00001871
Iteration 86/1000 | Loss: 0.00001871
Iteration 87/1000 | Loss: 0.00001871
Iteration 88/1000 | Loss: 0.00001871
Iteration 89/1000 | Loss: 0.00001871
Iteration 90/1000 | Loss: 0.00001870
Iteration 91/1000 | Loss: 0.00001870
Iteration 92/1000 | Loss: 0.00001870
Iteration 93/1000 | Loss: 0.00001870
Iteration 94/1000 | Loss: 0.00001870
Iteration 95/1000 | Loss: 0.00001870
Iteration 96/1000 | Loss: 0.00001869
Iteration 97/1000 | Loss: 0.00001869
Iteration 98/1000 | Loss: 0.00001869
Iteration 99/1000 | Loss: 0.00001869
Iteration 100/1000 | Loss: 0.00001869
Iteration 101/1000 | Loss: 0.00001869
Iteration 102/1000 | Loss: 0.00001869
Iteration 103/1000 | Loss: 0.00001869
Iteration 104/1000 | Loss: 0.00001869
Iteration 105/1000 | Loss: 0.00001869
Iteration 106/1000 | Loss: 0.00001869
Iteration 107/1000 | Loss: 0.00001868
Iteration 108/1000 | Loss: 0.00001868
Iteration 109/1000 | Loss: 0.00001868
Iteration 110/1000 | Loss: 0.00001868
Iteration 111/1000 | Loss: 0.00001868
Iteration 112/1000 | Loss: 0.00001868
Iteration 113/1000 | Loss: 0.00001868
Iteration 114/1000 | Loss: 0.00001868
Iteration 115/1000 | Loss: 0.00001868
Iteration 116/1000 | Loss: 0.00001868
Iteration 117/1000 | Loss: 0.00001868
Iteration 118/1000 | Loss: 0.00001868
Iteration 119/1000 | Loss: 0.00001868
Iteration 120/1000 | Loss: 0.00001868
Iteration 121/1000 | Loss: 0.00001867
Iteration 122/1000 | Loss: 0.00001867
Iteration 123/1000 | Loss: 0.00001867
Iteration 124/1000 | Loss: 0.00001867
Iteration 125/1000 | Loss: 0.00001867
Iteration 126/1000 | Loss: 0.00001867
Iteration 127/1000 | Loss: 0.00001867
Iteration 128/1000 | Loss: 0.00001867
Iteration 129/1000 | Loss: 0.00001867
Iteration 130/1000 | Loss: 0.00001867
Iteration 131/1000 | Loss: 0.00001867
Iteration 132/1000 | Loss: 0.00001867
Iteration 133/1000 | Loss: 0.00001867
Iteration 134/1000 | Loss: 0.00001867
Iteration 135/1000 | Loss: 0.00001867
Iteration 136/1000 | Loss: 0.00001867
Iteration 137/1000 | Loss: 0.00001867
Iteration 138/1000 | Loss: 0.00001867
Iteration 139/1000 | Loss: 0.00001867
Iteration 140/1000 | Loss: 0.00001867
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.8669605196919292e-05, 1.8669605196919292e-05, 1.8669605196919292e-05, 1.8669605196919292e-05, 1.8669605196919292e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8669605196919292e-05

Optimization complete. Final v2v error: 3.7215006351470947 mm

Highest mean error: 4.474566459655762 mm for frame 79

Lowest mean error: 3.382751703262329 mm for frame 0

Saving results

Total time: 33.1517539024353
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01131901
Iteration 2/25 | Loss: 0.00196907
Iteration 3/25 | Loss: 0.00177780
Iteration 4/25 | Loss: 0.00162414
Iteration 5/25 | Loss: 0.00161596
Iteration 6/25 | Loss: 0.00150013
Iteration 7/25 | Loss: 0.00146062
Iteration 8/25 | Loss: 0.00144968
Iteration 9/25 | Loss: 0.00144835
Iteration 10/25 | Loss: 0.00141523
Iteration 11/25 | Loss: 0.00141239
Iteration 12/25 | Loss: 0.00146697
Iteration 13/25 | Loss: 0.00139792
Iteration 14/25 | Loss: 0.00142196
Iteration 15/25 | Loss: 0.00138564
Iteration 16/25 | Loss: 0.00138216
Iteration 17/25 | Loss: 0.00138042
Iteration 18/25 | Loss: 0.00137884
Iteration 19/25 | Loss: 0.00138187
Iteration 20/25 | Loss: 0.00138115
Iteration 21/25 | Loss: 0.00138038
Iteration 22/25 | Loss: 0.00137942
Iteration 23/25 | Loss: 0.00138125
Iteration 24/25 | Loss: 0.00137987
Iteration 25/25 | Loss: 0.00137989

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 8.13818455
Iteration 2/25 | Loss: 0.00264086
Iteration 3/25 | Loss: 0.00264086
Iteration 4/25 | Loss: 0.00264086
Iteration 5/25 | Loss: 0.00264086
Iteration 6/25 | Loss: 0.00264086
Iteration 7/25 | Loss: 0.00264086
Iteration 8/25 | Loss: 0.00264086
Iteration 9/25 | Loss: 0.00264085
Iteration 10/25 | Loss: 0.00264085
Iteration 11/25 | Loss: 0.00264085
Iteration 12/25 | Loss: 0.00264085
Iteration 13/25 | Loss: 0.00264085
Iteration 14/25 | Loss: 0.00264085
Iteration 15/25 | Loss: 0.00264085
Iteration 16/25 | Loss: 0.00264085
Iteration 17/25 | Loss: 0.00264085
Iteration 18/25 | Loss: 0.00264085
Iteration 19/25 | Loss: 0.00264085
Iteration 20/25 | Loss: 0.00264085
Iteration 21/25 | Loss: 0.00264085
Iteration 22/25 | Loss: 0.00264085
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0026408531703054905, 0.0026408531703054905, 0.0026408531703054905, 0.0026408531703054905, 0.0026408531703054905]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0026408531703054905

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00264085
Iteration 2/1000 | Loss: 0.00119860
Iteration 3/1000 | Loss: 0.00010543
Iteration 4/1000 | Loss: 0.00005041
Iteration 5/1000 | Loss: 0.00011582
Iteration 6/1000 | Loss: 0.00016798
Iteration 7/1000 | Loss: 0.00003224
Iteration 8/1000 | Loss: 0.00003835
Iteration 9/1000 | Loss: 0.00004834
Iteration 10/1000 | Loss: 0.00003920
Iteration 11/1000 | Loss: 0.00009316
Iteration 12/1000 | Loss: 0.00003921
Iteration 13/1000 | Loss: 0.00004535
Iteration 14/1000 | Loss: 0.00009752
Iteration 15/1000 | Loss: 0.00010141
Iteration 16/1000 | Loss: 0.00007409
Iteration 17/1000 | Loss: 0.00038921
Iteration 18/1000 | Loss: 0.00055545
Iteration 19/1000 | Loss: 0.00042215
Iteration 20/1000 | Loss: 0.00079803
Iteration 21/1000 | Loss: 0.00071921
Iteration 22/1000 | Loss: 0.00038291
Iteration 23/1000 | Loss: 0.00063258
Iteration 24/1000 | Loss: 0.00022484
Iteration 25/1000 | Loss: 0.00037236
Iteration 26/1000 | Loss: 0.00012930
Iteration 27/1000 | Loss: 0.00054916
Iteration 28/1000 | Loss: 0.00007674
Iteration 29/1000 | Loss: 0.00039491
Iteration 30/1000 | Loss: 0.00043322
Iteration 31/1000 | Loss: 0.00050101
Iteration 32/1000 | Loss: 0.00003152
Iteration 33/1000 | Loss: 0.00002777
Iteration 34/1000 | Loss: 0.00002642
Iteration 35/1000 | Loss: 0.00002567
Iteration 36/1000 | Loss: 0.00002513
Iteration 37/1000 | Loss: 0.00002508
Iteration 38/1000 | Loss: 0.00002546
Iteration 39/1000 | Loss: 0.00002547
Iteration 40/1000 | Loss: 0.00055011
Iteration 41/1000 | Loss: 0.00034275
Iteration 42/1000 | Loss: 0.00063095
Iteration 43/1000 | Loss: 0.00004550
Iteration 44/1000 | Loss: 0.00002968
Iteration 45/1000 | Loss: 0.00072581
Iteration 46/1000 | Loss: 0.00002750
Iteration 47/1000 | Loss: 0.00047636
Iteration 48/1000 | Loss: 0.00013378
Iteration 49/1000 | Loss: 0.00045947
Iteration 50/1000 | Loss: 0.00041780
Iteration 51/1000 | Loss: 0.00024300
Iteration 52/1000 | Loss: 0.00002982
Iteration 53/1000 | Loss: 0.00017839
Iteration 54/1000 | Loss: 0.00043937
Iteration 55/1000 | Loss: 0.00030718
Iteration 56/1000 | Loss: 0.00043357
Iteration 57/1000 | Loss: 0.00089498
Iteration 58/1000 | Loss: 0.00042641
Iteration 59/1000 | Loss: 0.00030765
Iteration 60/1000 | Loss: 0.00017776
Iteration 61/1000 | Loss: 0.00081625
Iteration 62/1000 | Loss: 0.00094897
Iteration 63/1000 | Loss: 0.00036301
Iteration 64/1000 | Loss: 0.00070936
Iteration 65/1000 | Loss: 0.00005333
Iteration 66/1000 | Loss: 0.00003894
Iteration 67/1000 | Loss: 0.00002811
Iteration 68/1000 | Loss: 0.00002319
Iteration 69/1000 | Loss: 0.00002064
Iteration 70/1000 | Loss: 0.00001993
Iteration 71/1000 | Loss: 0.00001957
Iteration 72/1000 | Loss: 0.00001948
Iteration 73/1000 | Loss: 0.00001942
Iteration 74/1000 | Loss: 0.00001941
Iteration 75/1000 | Loss: 0.00001999
Iteration 76/1000 | Loss: 0.00001999
Iteration 77/1000 | Loss: 0.00001957
Iteration 78/1000 | Loss: 0.00001929
Iteration 79/1000 | Loss: 0.00001917
Iteration 80/1000 | Loss: 0.00001917
Iteration 81/1000 | Loss: 0.00001917
Iteration 82/1000 | Loss: 0.00001917
Iteration 83/1000 | Loss: 0.00001917
Iteration 84/1000 | Loss: 0.00001917
Iteration 85/1000 | Loss: 0.00001917
Iteration 86/1000 | Loss: 0.00001917
Iteration 87/1000 | Loss: 0.00001916
Iteration 88/1000 | Loss: 0.00001916
Iteration 89/1000 | Loss: 0.00001916
Iteration 90/1000 | Loss: 0.00001916
Iteration 91/1000 | Loss: 0.00001916
Iteration 92/1000 | Loss: 0.00001916
Iteration 93/1000 | Loss: 0.00001916
Iteration 94/1000 | Loss: 0.00001915
Iteration 95/1000 | Loss: 0.00001915
Iteration 96/1000 | Loss: 0.00001915
Iteration 97/1000 | Loss: 0.00001915
Iteration 98/1000 | Loss: 0.00001908
Iteration 99/1000 | Loss: 0.00001906
Iteration 100/1000 | Loss: 0.00001906
Iteration 101/1000 | Loss: 0.00001906
Iteration 102/1000 | Loss: 0.00001906
Iteration 103/1000 | Loss: 0.00001905
Iteration 104/1000 | Loss: 0.00001905
Iteration 105/1000 | Loss: 0.00001904
Iteration 106/1000 | Loss: 0.00001904
Iteration 107/1000 | Loss: 0.00001976
Iteration 108/1000 | Loss: 0.00001914
Iteration 109/1000 | Loss: 0.00001930
Iteration 110/1000 | Loss: 0.00001897
Iteration 111/1000 | Loss: 0.00001897
Iteration 112/1000 | Loss: 0.00001897
Iteration 113/1000 | Loss: 0.00001897
Iteration 114/1000 | Loss: 0.00001897
Iteration 115/1000 | Loss: 0.00001896
Iteration 116/1000 | Loss: 0.00001896
Iteration 117/1000 | Loss: 0.00001896
Iteration 118/1000 | Loss: 0.00001896
Iteration 119/1000 | Loss: 0.00001896
Iteration 120/1000 | Loss: 0.00001896
Iteration 121/1000 | Loss: 0.00001896
Iteration 122/1000 | Loss: 0.00001896
Iteration 123/1000 | Loss: 0.00001896
Iteration 124/1000 | Loss: 0.00001896
Iteration 125/1000 | Loss: 0.00001896
Iteration 126/1000 | Loss: 0.00001896
Iteration 127/1000 | Loss: 0.00001896
Iteration 128/1000 | Loss: 0.00001896
Iteration 129/1000 | Loss: 0.00001896
Iteration 130/1000 | Loss: 0.00001896
Iteration 131/1000 | Loss: 0.00001896
Iteration 132/1000 | Loss: 0.00001896
Iteration 133/1000 | Loss: 0.00001896
Iteration 134/1000 | Loss: 0.00001896
Iteration 135/1000 | Loss: 0.00001896
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 135. Stopping optimization.
Last 5 losses: [1.8963366528623737e-05, 1.8963366528623737e-05, 1.8963366528623737e-05, 1.8963366528623737e-05, 1.8963366528623737e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8963366528623737e-05

Optimization complete. Final v2v error: 3.425959587097168 mm

Highest mean error: 13.173140525817871 mm for frame 119

Lowest mean error: 3.0206117630004883 mm for frame 97

Saving results

Total time: 168.90537190437317
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00479025
Iteration 2/25 | Loss: 0.00167025
Iteration 3/25 | Loss: 0.00156360
Iteration 4/25 | Loss: 0.00154756
Iteration 5/25 | Loss: 0.00154260
Iteration 6/25 | Loss: 0.00154138
Iteration 7/25 | Loss: 0.00154138
Iteration 8/25 | Loss: 0.00154138
Iteration 9/25 | Loss: 0.00154138
Iteration 10/25 | Loss: 0.00154138
Iteration 11/25 | Loss: 0.00154138
Iteration 12/25 | Loss: 0.00154138
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0015413777437061071, 0.0015413777437061071, 0.0015413777437061071, 0.0015413777437061071, 0.0015413777437061071]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0015413777437061071

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.16962302
Iteration 2/25 | Loss: 0.00298738
Iteration 3/25 | Loss: 0.00298737
Iteration 4/25 | Loss: 0.00298737
Iteration 5/25 | Loss: 0.00298736
Iteration 6/25 | Loss: 0.00298736
Iteration 7/25 | Loss: 0.00298736
Iteration 8/25 | Loss: 0.00298736
Iteration 9/25 | Loss: 0.00298736
Iteration 10/25 | Loss: 0.00298736
Iteration 11/25 | Loss: 0.00298736
Iteration 12/25 | Loss: 0.00298736
Iteration 13/25 | Loss: 0.00298736
Iteration 14/25 | Loss: 0.00298736
Iteration 15/25 | Loss: 0.00298736
Iteration 16/25 | Loss: 0.00298736
Iteration 17/25 | Loss: 0.00298736
Iteration 18/25 | Loss: 0.00298736
Iteration 19/25 | Loss: 0.00298736
Iteration 20/25 | Loss: 0.00298736
Iteration 21/25 | Loss: 0.00298736
Iteration 22/25 | Loss: 0.00298736
Iteration 23/25 | Loss: 0.00298736
Iteration 24/25 | Loss: 0.00298736
Iteration 25/25 | Loss: 0.00298736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00298736
Iteration 2/1000 | Loss: 0.00004575
Iteration 3/1000 | Loss: 0.00003250
Iteration 4/1000 | Loss: 0.00002769
Iteration 5/1000 | Loss: 0.00002496
Iteration 6/1000 | Loss: 0.00002330
Iteration 7/1000 | Loss: 0.00002221
Iteration 8/1000 | Loss: 0.00002136
Iteration 9/1000 | Loss: 0.00002087
Iteration 10/1000 | Loss: 0.00002059
Iteration 11/1000 | Loss: 0.00002033
Iteration 12/1000 | Loss: 0.00002027
Iteration 13/1000 | Loss: 0.00002026
Iteration 14/1000 | Loss: 0.00002024
Iteration 15/1000 | Loss: 0.00002024
Iteration 16/1000 | Loss: 0.00002023
Iteration 17/1000 | Loss: 0.00002022
Iteration 18/1000 | Loss: 0.00002020
Iteration 19/1000 | Loss: 0.00002013
Iteration 20/1000 | Loss: 0.00002002
Iteration 21/1000 | Loss: 0.00001999
Iteration 22/1000 | Loss: 0.00001997
Iteration 23/1000 | Loss: 0.00001997
Iteration 24/1000 | Loss: 0.00001996
Iteration 25/1000 | Loss: 0.00001994
Iteration 26/1000 | Loss: 0.00001993
Iteration 27/1000 | Loss: 0.00001989
Iteration 28/1000 | Loss: 0.00001989
Iteration 29/1000 | Loss: 0.00001989
Iteration 30/1000 | Loss: 0.00001988
Iteration 31/1000 | Loss: 0.00001988
Iteration 32/1000 | Loss: 0.00001988
Iteration 33/1000 | Loss: 0.00001988
Iteration 34/1000 | Loss: 0.00001988
Iteration 35/1000 | Loss: 0.00001988
Iteration 36/1000 | Loss: 0.00001988
Iteration 37/1000 | Loss: 0.00001988
Iteration 38/1000 | Loss: 0.00001988
Iteration 39/1000 | Loss: 0.00001988
Iteration 40/1000 | Loss: 0.00001988
Iteration 41/1000 | Loss: 0.00001988
Iteration 42/1000 | Loss: 0.00001988
Iteration 43/1000 | Loss: 0.00001988
Iteration 44/1000 | Loss: 0.00001988
Iteration 45/1000 | Loss: 0.00001988
Iteration 46/1000 | Loss: 0.00001988
Iteration 47/1000 | Loss: 0.00001988
Iteration 48/1000 | Loss: 0.00001988
Iteration 49/1000 | Loss: 0.00001988
Iteration 50/1000 | Loss: 0.00001988
Iteration 51/1000 | Loss: 0.00001988
Iteration 52/1000 | Loss: 0.00001988
Iteration 53/1000 | Loss: 0.00001988
Iteration 54/1000 | Loss: 0.00001988
Iteration 55/1000 | Loss: 0.00001988
Iteration 56/1000 | Loss: 0.00001988
Iteration 57/1000 | Loss: 0.00001988
Iteration 58/1000 | Loss: 0.00001987
Iteration 59/1000 | Loss: 0.00001987
Iteration 60/1000 | Loss: 0.00001987
Iteration 61/1000 | Loss: 0.00001987
Iteration 62/1000 | Loss: 0.00001987
Iteration 63/1000 | Loss: 0.00001987
Iteration 64/1000 | Loss: 0.00001987
Iteration 65/1000 | Loss: 0.00001987
Iteration 66/1000 | Loss: 0.00001987
Iteration 67/1000 | Loss: 0.00001987
Iteration 68/1000 | Loss: 0.00001987
Iteration 69/1000 | Loss: 0.00001987
Iteration 70/1000 | Loss: 0.00001987
Iteration 71/1000 | Loss: 0.00001987
Iteration 72/1000 | Loss: 0.00001987
Iteration 73/1000 | Loss: 0.00001987
Iteration 74/1000 | Loss: 0.00001987
Iteration 75/1000 | Loss: 0.00001987
Iteration 76/1000 | Loss: 0.00001987
Iteration 77/1000 | Loss: 0.00001987
Iteration 78/1000 | Loss: 0.00001987
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 78. Stopping optimization.
Last 5 losses: [1.9873752535204403e-05, 1.9873752535204403e-05, 1.9873752535204403e-05, 1.9873752535204403e-05, 1.9873752535204403e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9873752535204403e-05

Optimization complete. Final v2v error: 3.766094207763672 mm

Highest mean error: 4.125068187713623 mm for frame 75

Lowest mean error: 3.5101921558380127 mm for frame 126

Saving results

Total time: 30.43686079978943
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_41_nl_5319/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_41_nl_5319/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01131667
Iteration 2/25 | Loss: 0.00601194
Iteration 3/25 | Loss: 0.00421920
Iteration 4/25 | Loss: 0.00325118
Iteration 5/25 | Loss: 0.00303649
Iteration 6/25 | Loss: 0.00289224
Iteration 7/25 | Loss: 0.00286663
Iteration 8/25 | Loss: 0.00265210
Iteration 9/25 | Loss: 0.00254619
Iteration 10/25 | Loss: 0.00243321
Iteration 11/25 | Loss: 0.00243700
Iteration 12/25 | Loss: 0.00237691
Iteration 13/25 | Loss: 0.00232921
Iteration 14/25 | Loss: 0.00229614
Iteration 15/25 | Loss: 0.00231188
Iteration 16/25 | Loss: 0.00228373
Iteration 17/25 | Loss: 0.00220123
Iteration 18/25 | Loss: 0.00218435
Iteration 19/25 | Loss: 0.00216512
Iteration 20/25 | Loss: 0.00209668
Iteration 21/25 | Loss: 0.00206002
Iteration 22/25 | Loss: 0.00203725
Iteration 23/25 | Loss: 0.00203836
Iteration 24/25 | Loss: 0.00203647
Iteration 25/25 | Loss: 0.00202873

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.57323855
Iteration 2/25 | Loss: 0.00609289
Iteration 3/25 | Loss: 0.00609289
Iteration 4/25 | Loss: 0.00609288
Iteration 5/25 | Loss: 0.00609288
Iteration 6/25 | Loss: 0.00609288
Iteration 7/25 | Loss: 0.00609288
Iteration 8/25 | Loss: 0.00609288
Iteration 9/25 | Loss: 0.00609288
Iteration 10/25 | Loss: 0.00609288
Iteration 11/25 | Loss: 0.00609288
Iteration 12/25 | Loss: 0.00609288
Iteration 13/25 | Loss: 0.00609288
Iteration 14/25 | Loss: 0.00609288
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0060928803868591785, 0.0060928803868591785, 0.0060928803868591785, 0.0060928803868591785, 0.0060928803868591785]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0060928803868591785

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00609288
Iteration 2/1000 | Loss: 0.00487402
Iteration 3/1000 | Loss: 0.00373257
Iteration 4/1000 | Loss: 0.00224810
Iteration 5/1000 | Loss: 0.00233507
Iteration 6/1000 | Loss: 0.00211275
Iteration 7/1000 | Loss: 0.00229191
Iteration 8/1000 | Loss: 0.00309091
Iteration 9/1000 | Loss: 0.00103246
Iteration 10/1000 | Loss: 0.00091048
Iteration 11/1000 | Loss: 0.00052658
Iteration 12/1000 | Loss: 0.00072382
Iteration 13/1000 | Loss: 0.00049782
Iteration 14/1000 | Loss: 0.00045074
Iteration 15/1000 | Loss: 0.00031457
Iteration 16/1000 | Loss: 0.00055102
Iteration 17/1000 | Loss: 0.00056136
Iteration 18/1000 | Loss: 0.00077810
Iteration 19/1000 | Loss: 0.00044979
Iteration 20/1000 | Loss: 0.00043605
Iteration 21/1000 | Loss: 0.00048470
Iteration 22/1000 | Loss: 0.00027837
Iteration 23/1000 | Loss: 0.00066766
Iteration 24/1000 | Loss: 0.00027428
Iteration 25/1000 | Loss: 0.00042069
Iteration 26/1000 | Loss: 0.00047264
Iteration 27/1000 | Loss: 0.00039352
Iteration 28/1000 | Loss: 0.00047715
Iteration 29/1000 | Loss: 0.00033013
Iteration 30/1000 | Loss: 0.00035787
Iteration 31/1000 | Loss: 0.00027073
Iteration 32/1000 | Loss: 0.00034198
Iteration 33/1000 | Loss: 0.00024683
Iteration 34/1000 | Loss: 0.00049282
Iteration 35/1000 | Loss: 0.00017723
Iteration 36/1000 | Loss: 0.00018345
Iteration 37/1000 | Loss: 0.00016416
Iteration 38/1000 | Loss: 0.00016958
Iteration 39/1000 | Loss: 0.00017391
Iteration 40/1000 | Loss: 0.00016978
Iteration 41/1000 | Loss: 0.00016551
Iteration 42/1000 | Loss: 0.00063005
Iteration 43/1000 | Loss: 0.00029620
Iteration 44/1000 | Loss: 0.00021180
Iteration 45/1000 | Loss: 0.00017059
Iteration 46/1000 | Loss: 0.00016148
Iteration 47/1000 | Loss: 0.00015760
Iteration 48/1000 | Loss: 0.00046199
Iteration 49/1000 | Loss: 0.00025762
Iteration 50/1000 | Loss: 0.00029908
Iteration 51/1000 | Loss: 0.00024322
Iteration 52/1000 | Loss: 0.00024354
Iteration 53/1000 | Loss: 0.00030518
Iteration 54/1000 | Loss: 0.00019582
Iteration 55/1000 | Loss: 0.00015375
Iteration 56/1000 | Loss: 0.00015317
Iteration 57/1000 | Loss: 0.00015637
Iteration 58/1000 | Loss: 0.00016249
Iteration 59/1000 | Loss: 0.00014656
Iteration 60/1000 | Loss: 0.00015331
Iteration 61/1000 | Loss: 0.00013664
Iteration 62/1000 | Loss: 0.00014489
Iteration 63/1000 | Loss: 0.00014330
Iteration 64/1000 | Loss: 0.00013756
Iteration 65/1000 | Loss: 0.00014138
Iteration 66/1000 | Loss: 0.00013157
Iteration 67/1000 | Loss: 0.00023281
Iteration 68/1000 | Loss: 0.00031289
Iteration 69/1000 | Loss: 0.00014108
Iteration 70/1000 | Loss: 0.00031023
Iteration 71/1000 | Loss: 0.00014271
Iteration 72/1000 | Loss: 0.00013118
Iteration 73/1000 | Loss: 0.00023706
Iteration 74/1000 | Loss: 0.00016554
Iteration 75/1000 | Loss: 0.00013708
Iteration 76/1000 | Loss: 0.00022793
Iteration 77/1000 | Loss: 0.00017178
Iteration 78/1000 | Loss: 0.00017028
Iteration 79/1000 | Loss: 0.00021413
Iteration 80/1000 | Loss: 0.00013510
Iteration 81/1000 | Loss: 0.00015803
Iteration 82/1000 | Loss: 0.00016145
Iteration 83/1000 | Loss: 0.00016348
Iteration 84/1000 | Loss: 0.00013403
Iteration 85/1000 | Loss: 0.00013357
Iteration 86/1000 | Loss: 0.00013103
Iteration 87/1000 | Loss: 0.00045815
Iteration 88/1000 | Loss: 0.00025445
Iteration 89/1000 | Loss: 0.00021835
Iteration 90/1000 | Loss: 0.00013813
Iteration 91/1000 | Loss: 0.00044711
Iteration 92/1000 | Loss: 0.00019160
Iteration 93/1000 | Loss: 0.00023970
Iteration 94/1000 | Loss: 0.00014869
Iteration 95/1000 | Loss: 0.00041062
Iteration 96/1000 | Loss: 0.00052371
Iteration 97/1000 | Loss: 0.00014914
Iteration 98/1000 | Loss: 0.00017777
Iteration 99/1000 | Loss: 0.00017104
Iteration 100/1000 | Loss: 0.00016704
Iteration 101/1000 | Loss: 0.00016308
Iteration 102/1000 | Loss: 0.00016569
Iteration 103/1000 | Loss: 0.00042675
Iteration 104/1000 | Loss: 0.00011648
Iteration 105/1000 | Loss: 0.00010987
Iteration 106/1000 | Loss: 0.00031266
Iteration 107/1000 | Loss: 0.00020584
Iteration 108/1000 | Loss: 0.00010604
Iteration 109/1000 | Loss: 0.00025514
Iteration 110/1000 | Loss: 0.00012147
Iteration 111/1000 | Loss: 0.00020429
Iteration 112/1000 | Loss: 0.00022996
Iteration 113/1000 | Loss: 0.00011048
Iteration 114/1000 | Loss: 0.00010596
Iteration 115/1000 | Loss: 0.00010171
Iteration 116/1000 | Loss: 0.00009921
Iteration 117/1000 | Loss: 0.00020971
Iteration 118/1000 | Loss: 0.00023646
Iteration 119/1000 | Loss: 0.00010811
Iteration 120/1000 | Loss: 0.00010135
Iteration 121/1000 | Loss: 0.00020585
Iteration 122/1000 | Loss: 0.00022812
Iteration 123/1000 | Loss: 0.00011312
Iteration 124/1000 | Loss: 0.00009939
Iteration 125/1000 | Loss: 0.00009568
Iteration 126/1000 | Loss: 0.00021609
Iteration 127/1000 | Loss: 0.00019139
Iteration 128/1000 | Loss: 0.00016614
Iteration 129/1000 | Loss: 0.00009350
Iteration 130/1000 | Loss: 0.00026827
Iteration 131/1000 | Loss: 0.00011235
Iteration 132/1000 | Loss: 0.00009584
Iteration 133/1000 | Loss: 0.00009184
Iteration 134/1000 | Loss: 0.00009630
Iteration 135/1000 | Loss: 0.00008798
Iteration 136/1000 | Loss: 0.00008630
Iteration 137/1000 | Loss: 0.00008526
Iteration 138/1000 | Loss: 0.00008467
Iteration 139/1000 | Loss: 0.00008559
Iteration 140/1000 | Loss: 0.00008433
Iteration 141/1000 | Loss: 0.00008405
Iteration 142/1000 | Loss: 0.00017563
Iteration 143/1000 | Loss: 0.00009875
Iteration 144/1000 | Loss: 0.00030856
Iteration 145/1000 | Loss: 0.00025866
Iteration 146/1000 | Loss: 0.00010507
Iteration 147/1000 | Loss: 0.00009026
Iteration 148/1000 | Loss: 0.00021197
Iteration 149/1000 | Loss: 0.00009383
Iteration 150/1000 | Loss: 0.00008691
Iteration 151/1000 | Loss: 0.00008386
Iteration 152/1000 | Loss: 0.00019118
Iteration 153/1000 | Loss: 0.00030262
Iteration 154/1000 | Loss: 0.00020323
Iteration 155/1000 | Loss: 0.00009517
Iteration 156/1000 | Loss: 0.00023806
Iteration 157/1000 | Loss: 0.00009206
Iteration 158/1000 | Loss: 0.00008489
Iteration 159/1000 | Loss: 0.00008025
Iteration 160/1000 | Loss: 0.00007736
Iteration 161/1000 | Loss: 0.00020353
Iteration 162/1000 | Loss: 0.00008378
Iteration 163/1000 | Loss: 0.00008450
Iteration 164/1000 | Loss: 0.00007711
Iteration 165/1000 | Loss: 0.00007513
Iteration 166/1000 | Loss: 0.00007398
Iteration 167/1000 | Loss: 0.00020861
Iteration 168/1000 | Loss: 0.00008785
Iteration 169/1000 | Loss: 0.00007688
Iteration 170/1000 | Loss: 0.00007454
Iteration 171/1000 | Loss: 0.00007298
Iteration 172/1000 | Loss: 0.00007187
Iteration 173/1000 | Loss: 0.00007124
Iteration 174/1000 | Loss: 0.00007083
Iteration 175/1000 | Loss: 0.00021609
Iteration 176/1000 | Loss: 0.00008399
Iteration 177/1000 | Loss: 0.00008201
Iteration 178/1000 | Loss: 0.00007325
Iteration 179/1000 | Loss: 0.00007143
Iteration 180/1000 | Loss: 0.00007011
Iteration 181/1000 | Loss: 0.00007000
Iteration 182/1000 | Loss: 0.00006943
Iteration 183/1000 | Loss: 0.00022284
Iteration 184/1000 | Loss: 0.00007564
Iteration 185/1000 | Loss: 0.00007130
Iteration 186/1000 | Loss: 0.00024085
Iteration 187/1000 | Loss: 0.00008428
Iteration 188/1000 | Loss: 0.00007573
Iteration 189/1000 | Loss: 0.00007045
Iteration 190/1000 | Loss: 0.00006857
Iteration 191/1000 | Loss: 0.00006756
Iteration 192/1000 | Loss: 0.00006663
Iteration 193/1000 | Loss: 0.00006607
Iteration 194/1000 | Loss: 0.00006628
Iteration 195/1000 | Loss: 0.00006591
Iteration 196/1000 | Loss: 0.00006665
Iteration 197/1000 | Loss: 0.00006575
Iteration 198/1000 | Loss: 0.00006710
Iteration 199/1000 | Loss: 0.00006561
Iteration 200/1000 | Loss: 0.00006728
Iteration 201/1000 | Loss: 0.00006555
Iteration 202/1000 | Loss: 0.00006738
Iteration 203/1000 | Loss: 0.00006571
Iteration 204/1000 | Loss: 0.00006707
Iteration 205/1000 | Loss: 0.00006603
Iteration 206/1000 | Loss: 0.00006902
Iteration 207/1000 | Loss: 0.00006589
Iteration 208/1000 | Loss: 0.00006812
Iteration 209/1000 | Loss: 0.00006737
Iteration 210/1000 | Loss: 0.00006870
Iteration 211/1000 | Loss: 0.00006688
Iteration 212/1000 | Loss: 0.00006906
Iteration 213/1000 | Loss: 0.00006650
Iteration 214/1000 | Loss: 0.00006858
Iteration 215/1000 | Loss: 0.00006661
Iteration 216/1000 | Loss: 0.00006987
Iteration 217/1000 | Loss: 0.00006710
Iteration 218/1000 | Loss: 0.00007108
Iteration 219/1000 | Loss: 0.00006668
Iteration 220/1000 | Loss: 0.00006916
Iteration 221/1000 | Loss: 0.00006693
Iteration 222/1000 | Loss: 0.00006853
Iteration 223/1000 | Loss: 0.00006821
Iteration 224/1000 | Loss: 0.00006821
Iteration 225/1000 | Loss: 0.00007002
Iteration 226/1000 | Loss: 0.00006788
Iteration 227/1000 | Loss: 0.00007202
Iteration 228/1000 | Loss: 0.00006633
Iteration 229/1000 | Loss: 0.00007204
Iteration 230/1000 | Loss: 0.00006661
Iteration 231/1000 | Loss: 0.00006660
Iteration 232/1000 | Loss: 0.00006596
Iteration 233/1000 | Loss: 0.00006622
Iteration 234/1000 | Loss: 0.00006838
Iteration 235/1000 | Loss: 0.00006636
Iteration 236/1000 | Loss: 0.00006874
Iteration 237/1000 | Loss: 0.00006730
Iteration 238/1000 | Loss: 0.00006805
Iteration 239/1000 | Loss: 0.00006686
Iteration 240/1000 | Loss: 0.00006931
Iteration 241/1000 | Loss: 0.00006913
Iteration 242/1000 | Loss: 0.00006878
Iteration 243/1000 | Loss: 0.00006865
Iteration 244/1000 | Loss: 0.00007495
Iteration 245/1000 | Loss: 0.00006873
Iteration 246/1000 | Loss: 0.00007329
Iteration 247/1000 | Loss: 0.00006960
Iteration 248/1000 | Loss: 0.00007315
Iteration 249/1000 | Loss: 0.00007321
Iteration 250/1000 | Loss: 0.00006797
Iteration 251/1000 | Loss: 0.00006849
Iteration 252/1000 | Loss: 0.00006725
Iteration 253/1000 | Loss: 0.00007023
Iteration 254/1000 | Loss: 0.00006700
Iteration 255/1000 | Loss: 0.00006984
Iteration 256/1000 | Loss: 0.00006691
Iteration 257/1000 | Loss: 0.00006795
Iteration 258/1000 | Loss: 0.00006667
Iteration 259/1000 | Loss: 0.00006598
Iteration 260/1000 | Loss: 0.00006727
Iteration 261/1000 | Loss: 0.00006640
Iteration 262/1000 | Loss: 0.00006529
Iteration 263/1000 | Loss: 0.00006526
Iteration 264/1000 | Loss: 0.00006526
Iteration 265/1000 | Loss: 0.00006526
Iteration 266/1000 | Loss: 0.00006526
Iteration 267/1000 | Loss: 0.00006526
Iteration 268/1000 | Loss: 0.00006526
Iteration 269/1000 | Loss: 0.00006526
Iteration 270/1000 | Loss: 0.00006526
Iteration 271/1000 | Loss: 0.00006526
Iteration 272/1000 | Loss: 0.00006526
Iteration 273/1000 | Loss: 0.00006526
Iteration 274/1000 | Loss: 0.00006525
Iteration 275/1000 | Loss: 0.00006524
Iteration 276/1000 | Loss: 0.00006524
Iteration 277/1000 | Loss: 0.00006524
Iteration 278/1000 | Loss: 0.00006523
Iteration 279/1000 | Loss: 0.00006523
Iteration 280/1000 | Loss: 0.00006523
Iteration 281/1000 | Loss: 0.00006523
Iteration 282/1000 | Loss: 0.00006523
Iteration 283/1000 | Loss: 0.00006522
Iteration 284/1000 | Loss: 0.00006513
Iteration 285/1000 | Loss: 0.00006511
Iteration 286/1000 | Loss: 0.00006511
Iteration 287/1000 | Loss: 0.00006511
Iteration 288/1000 | Loss: 0.00006510
Iteration 289/1000 | Loss: 0.00006510
Iteration 290/1000 | Loss: 0.00006510
Iteration 291/1000 | Loss: 0.00006510
Iteration 292/1000 | Loss: 0.00006510
Iteration 293/1000 | Loss: 0.00006510
Iteration 294/1000 | Loss: 0.00006510
Iteration 295/1000 | Loss: 0.00006510
Iteration 296/1000 | Loss: 0.00006510
Iteration 297/1000 | Loss: 0.00006510
Iteration 298/1000 | Loss: 0.00006510
Iteration 299/1000 | Loss: 0.00006510
Iteration 300/1000 | Loss: 0.00006510
Iteration 301/1000 | Loss: 0.00006509
Iteration 302/1000 | Loss: 0.00006509
Iteration 303/1000 | Loss: 0.00006509
Iteration 304/1000 | Loss: 0.00006509
Iteration 305/1000 | Loss: 0.00006509
Iteration 306/1000 | Loss: 0.00006509
Iteration 307/1000 | Loss: 0.00006509
Iteration 308/1000 | Loss: 0.00006509
Iteration 309/1000 | Loss: 0.00006508
Iteration 310/1000 | Loss: 0.00006508
Iteration 311/1000 | Loss: 0.00006508
Iteration 312/1000 | Loss: 0.00006508
Iteration 313/1000 | Loss: 0.00006508
Iteration 314/1000 | Loss: 0.00006508
Iteration 315/1000 | Loss: 0.00006508
Iteration 316/1000 | Loss: 0.00006508
Iteration 317/1000 | Loss: 0.00006508
Iteration 318/1000 | Loss: 0.00006507
Iteration 319/1000 | Loss: 0.00006507
Iteration 320/1000 | Loss: 0.00006507
Iteration 321/1000 | Loss: 0.00006507
Iteration 322/1000 | Loss: 0.00006506
Iteration 323/1000 | Loss: 0.00006506
Iteration 324/1000 | Loss: 0.00006506
Iteration 325/1000 | Loss: 0.00006506
Iteration 326/1000 | Loss: 0.00006506
Iteration 327/1000 | Loss: 0.00006506
Iteration 328/1000 | Loss: 0.00006505
Iteration 329/1000 | Loss: 0.00006505
Iteration 330/1000 | Loss: 0.00006505
Iteration 331/1000 | Loss: 0.00006505
Iteration 332/1000 | Loss: 0.00006505
Iteration 333/1000 | Loss: 0.00006505
Iteration 334/1000 | Loss: 0.00006505
Iteration 335/1000 | Loss: 0.00006505
Iteration 336/1000 | Loss: 0.00006505
Iteration 337/1000 | Loss: 0.00006505
Iteration 338/1000 | Loss: 0.00006505
Iteration 339/1000 | Loss: 0.00006505
Iteration 340/1000 | Loss: 0.00006505
Iteration 341/1000 | Loss: 0.00006505
Iteration 342/1000 | Loss: 0.00006505
Iteration 343/1000 | Loss: 0.00006504
Iteration 344/1000 | Loss: 0.00006504
Iteration 345/1000 | Loss: 0.00006504
Iteration 346/1000 | Loss: 0.00006504
Iteration 347/1000 | Loss: 0.00006504
Iteration 348/1000 | Loss: 0.00006504
Iteration 349/1000 | Loss: 0.00006504
Iteration 350/1000 | Loss: 0.00006504
Iteration 351/1000 | Loss: 0.00006504
Iteration 352/1000 | Loss: 0.00006504
Iteration 353/1000 | Loss: 0.00006504
Iteration 354/1000 | Loss: 0.00006504
Iteration 355/1000 | Loss: 0.00006504
Iteration 356/1000 | Loss: 0.00006504
Iteration 357/1000 | Loss: 0.00006504
Iteration 358/1000 | Loss: 0.00006504
Iteration 359/1000 | Loss: 0.00006504
Iteration 360/1000 | Loss: 0.00006504
Iteration 361/1000 | Loss: 0.00006504
Iteration 362/1000 | Loss: 0.00006504
Iteration 363/1000 | Loss: 0.00006504
Iteration 364/1000 | Loss: 0.00006504
Iteration 365/1000 | Loss: 0.00006504
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 365. Stopping optimization.
Last 5 losses: [6.504177144961432e-05, 6.504177144961432e-05, 6.504177144961432e-05, 6.504177144961432e-05, 6.504177144961432e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 6.504177144961432e-05

Optimization complete. Final v2v error: 5.547601222991943 mm

Highest mean error: 15.835382461547852 mm for frame 88

Lowest mean error: 4.714025974273682 mm for frame 239

Saving results

Total time: 468.1286871433258
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0023/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0023.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0023
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841184
Iteration 2/25 | Loss: 0.00133947
Iteration 3/25 | Loss: 0.00110550
Iteration 4/25 | Loss: 0.00108020
Iteration 5/25 | Loss: 0.00107338
Iteration 6/25 | Loss: 0.00107155
Iteration 7/25 | Loss: 0.00107155
Iteration 8/25 | Loss: 0.00107155
Iteration 9/25 | Loss: 0.00107155
Iteration 10/25 | Loss: 0.00107155
Iteration 11/25 | Loss: 0.00107155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010715486714616418, 0.0010715486714616418, 0.0010715486714616418, 0.0010715486714616418, 0.0010715486714616418]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010715486714616418

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 4.04236031
Iteration 2/25 | Loss: 0.00117945
Iteration 3/25 | Loss: 0.00117931
Iteration 4/25 | Loss: 0.00117931
Iteration 5/25 | Loss: 0.00117931
Iteration 6/25 | Loss: 0.00117931
Iteration 7/25 | Loss: 0.00117930
Iteration 8/25 | Loss: 0.00117930
Iteration 9/25 | Loss: 0.00117930
Iteration 10/25 | Loss: 0.00117930
Iteration 11/25 | Loss: 0.00117930
Iteration 12/25 | Loss: 0.00117930
Iteration 13/25 | Loss: 0.00117930
Iteration 14/25 | Loss: 0.00117930
Iteration 15/25 | Loss: 0.00117930
Iteration 16/25 | Loss: 0.00117930
Iteration 17/25 | Loss: 0.00117930
Iteration 18/25 | Loss: 0.00117930
Iteration 19/25 | Loss: 0.00117930
Iteration 20/25 | Loss: 0.00117930
Iteration 21/25 | Loss: 0.00117930
Iteration 22/25 | Loss: 0.00117930
Iteration 23/25 | Loss: 0.00117930
Iteration 24/25 | Loss: 0.00117930
Iteration 25/25 | Loss: 0.00117930

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00117930
Iteration 2/1000 | Loss: 0.00005800
Iteration 3/1000 | Loss: 0.00003228
Iteration 4/1000 | Loss: 0.00002568
Iteration 5/1000 | Loss: 0.00002303
Iteration 6/1000 | Loss: 0.00002158
Iteration 7/1000 | Loss: 0.00002078
Iteration 8/1000 | Loss: 0.00002030
Iteration 9/1000 | Loss: 0.00001995
Iteration 10/1000 | Loss: 0.00001959
Iteration 11/1000 | Loss: 0.00001941
Iteration 12/1000 | Loss: 0.00001939
Iteration 13/1000 | Loss: 0.00001933
Iteration 14/1000 | Loss: 0.00001926
Iteration 15/1000 | Loss: 0.00001921
Iteration 16/1000 | Loss: 0.00001914
Iteration 17/1000 | Loss: 0.00001910
Iteration 18/1000 | Loss: 0.00001910
Iteration 19/1000 | Loss: 0.00001909
Iteration 20/1000 | Loss: 0.00001908
Iteration 21/1000 | Loss: 0.00001908
Iteration 22/1000 | Loss: 0.00001908
Iteration 23/1000 | Loss: 0.00001907
Iteration 24/1000 | Loss: 0.00001907
Iteration 25/1000 | Loss: 0.00001907
Iteration 26/1000 | Loss: 0.00001907
Iteration 27/1000 | Loss: 0.00001906
Iteration 28/1000 | Loss: 0.00001905
Iteration 29/1000 | Loss: 0.00001905
Iteration 30/1000 | Loss: 0.00001904
Iteration 31/1000 | Loss: 0.00001904
Iteration 32/1000 | Loss: 0.00001904
Iteration 33/1000 | Loss: 0.00001903
Iteration 34/1000 | Loss: 0.00001903
Iteration 35/1000 | Loss: 0.00001902
Iteration 36/1000 | Loss: 0.00001902
Iteration 37/1000 | Loss: 0.00001902
Iteration 38/1000 | Loss: 0.00001901
Iteration 39/1000 | Loss: 0.00001901
Iteration 40/1000 | Loss: 0.00001901
Iteration 41/1000 | Loss: 0.00001900
Iteration 42/1000 | Loss: 0.00001900
Iteration 43/1000 | Loss: 0.00001900
Iteration 44/1000 | Loss: 0.00001899
Iteration 45/1000 | Loss: 0.00001899
Iteration 46/1000 | Loss: 0.00001899
Iteration 47/1000 | Loss: 0.00001898
Iteration 48/1000 | Loss: 0.00001898
Iteration 49/1000 | Loss: 0.00001898
Iteration 50/1000 | Loss: 0.00001897
Iteration 51/1000 | Loss: 0.00001897
Iteration 52/1000 | Loss: 0.00001897
Iteration 53/1000 | Loss: 0.00001896
Iteration 54/1000 | Loss: 0.00001896
Iteration 55/1000 | Loss: 0.00001896
Iteration 56/1000 | Loss: 0.00001895
Iteration 57/1000 | Loss: 0.00001895
Iteration 58/1000 | Loss: 0.00001895
Iteration 59/1000 | Loss: 0.00001894
Iteration 60/1000 | Loss: 0.00001894
Iteration 61/1000 | Loss: 0.00001894
Iteration 62/1000 | Loss: 0.00001894
Iteration 63/1000 | Loss: 0.00001894
Iteration 64/1000 | Loss: 0.00001894
Iteration 65/1000 | Loss: 0.00001894
Iteration 66/1000 | Loss: 0.00001893
Iteration 67/1000 | Loss: 0.00001893
Iteration 68/1000 | Loss: 0.00001893
Iteration 69/1000 | Loss: 0.00001893
Iteration 70/1000 | Loss: 0.00001893
Iteration 71/1000 | Loss: 0.00001893
Iteration 72/1000 | Loss: 0.00001892
Iteration 73/1000 | Loss: 0.00001892
Iteration 74/1000 | Loss: 0.00001892
Iteration 75/1000 | Loss: 0.00001892
Iteration 76/1000 | Loss: 0.00001892
Iteration 77/1000 | Loss: 0.00001892
Iteration 78/1000 | Loss: 0.00001892
Iteration 79/1000 | Loss: 0.00001891
Iteration 80/1000 | Loss: 0.00001891
Iteration 81/1000 | Loss: 0.00001891
Iteration 82/1000 | Loss: 0.00001891
Iteration 83/1000 | Loss: 0.00001891
Iteration 84/1000 | Loss: 0.00001891
Iteration 85/1000 | Loss: 0.00001891
Iteration 86/1000 | Loss: 0.00001891
Iteration 87/1000 | Loss: 0.00001891
Iteration 88/1000 | Loss: 0.00001891
Iteration 89/1000 | Loss: 0.00001891
Iteration 90/1000 | Loss: 0.00001891
Iteration 91/1000 | Loss: 0.00001890
Iteration 92/1000 | Loss: 0.00001890
Iteration 93/1000 | Loss: 0.00001890
Iteration 94/1000 | Loss: 0.00001890
Iteration 95/1000 | Loss: 0.00001890
Iteration 96/1000 | Loss: 0.00001890
Iteration 97/1000 | Loss: 0.00001890
Iteration 98/1000 | Loss: 0.00001890
Iteration 99/1000 | Loss: 0.00001890
Iteration 100/1000 | Loss: 0.00001889
Iteration 101/1000 | Loss: 0.00001889
Iteration 102/1000 | Loss: 0.00001889
Iteration 103/1000 | Loss: 0.00001889
Iteration 104/1000 | Loss: 0.00001889
Iteration 105/1000 | Loss: 0.00001889
Iteration 106/1000 | Loss: 0.00001889
Iteration 107/1000 | Loss: 0.00001889
Iteration 108/1000 | Loss: 0.00001889
Iteration 109/1000 | Loss: 0.00001889
Iteration 110/1000 | Loss: 0.00001889
Iteration 111/1000 | Loss: 0.00001889
Iteration 112/1000 | Loss: 0.00001889
Iteration 113/1000 | Loss: 0.00001889
Iteration 114/1000 | Loss: 0.00001889
Iteration 115/1000 | Loss: 0.00001889
Iteration 116/1000 | Loss: 0.00001889
Iteration 117/1000 | Loss: 0.00001889
Iteration 118/1000 | Loss: 0.00001889
Iteration 119/1000 | Loss: 0.00001889
Iteration 120/1000 | Loss: 0.00001889
Iteration 121/1000 | Loss: 0.00001889
Iteration 122/1000 | Loss: 0.00001889
Iteration 123/1000 | Loss: 0.00001889
Iteration 124/1000 | Loss: 0.00001889
Iteration 125/1000 | Loss: 0.00001889
Iteration 126/1000 | Loss: 0.00001889
Iteration 127/1000 | Loss: 0.00001889
Iteration 128/1000 | Loss: 0.00001889
Iteration 129/1000 | Loss: 0.00001889
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 129. Stopping optimization.
Last 5 losses: [1.8886899852077477e-05, 1.8886899852077477e-05, 1.8886899852077477e-05, 1.8886899852077477e-05, 1.8886899852077477e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8886899852077477e-05

Optimization complete. Final v2v error: 3.579521894454956 mm

Highest mean error: 4.307610511779785 mm for frame 141

Lowest mean error: 2.8847193717956543 mm for frame 127

Saving results

Total time: 40.030524492263794
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00989384
Iteration 2/25 | Loss: 0.00186342
Iteration 3/25 | Loss: 0.00122004
Iteration 4/25 | Loss: 0.00120283
Iteration 5/25 | Loss: 0.00119990
Iteration 6/25 | Loss: 0.00119900
Iteration 7/25 | Loss: 0.00119900
Iteration 8/25 | Loss: 0.00119900
Iteration 9/25 | Loss: 0.00119900
Iteration 10/25 | Loss: 0.00119900
Iteration 11/25 | Loss: 0.00119900
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0011990038910880685, 0.0011990038910880685, 0.0011990038910880685, 0.0011990038910880685, 0.0011990038910880685]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011990038910880685

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18565726
Iteration 2/25 | Loss: 0.00077567
Iteration 3/25 | Loss: 0.00077566
Iteration 4/25 | Loss: 0.00077566
Iteration 5/25 | Loss: 0.00077566
Iteration 6/25 | Loss: 0.00077566
Iteration 7/25 | Loss: 0.00077566
Iteration 8/25 | Loss: 0.00077566
Iteration 9/25 | Loss: 0.00077566
Iteration 10/25 | Loss: 0.00077566
Iteration 11/25 | Loss: 0.00077566
Iteration 12/25 | Loss: 0.00077566
Iteration 13/25 | Loss: 0.00077566
Iteration 14/25 | Loss: 0.00077566
Iteration 15/25 | Loss: 0.00077566
Iteration 16/25 | Loss: 0.00077566
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.000775662309024483, 0.000775662309024483, 0.000775662309024483, 0.000775662309024483, 0.000775662309024483]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000775662309024483

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00077566
Iteration 2/1000 | Loss: 0.00008423
Iteration 3/1000 | Loss: 0.00005731
Iteration 4/1000 | Loss: 0.00004856
Iteration 5/1000 | Loss: 0.00004542
Iteration 6/1000 | Loss: 0.00004382
Iteration 7/1000 | Loss: 0.00004265
Iteration 8/1000 | Loss: 0.00004161
Iteration 9/1000 | Loss: 0.00004103
Iteration 10/1000 | Loss: 0.00004047
Iteration 11/1000 | Loss: 0.00003971
Iteration 12/1000 | Loss: 0.00003930
Iteration 13/1000 | Loss: 0.00003876
Iteration 14/1000 | Loss: 0.00003843
Iteration 15/1000 | Loss: 0.00003817
Iteration 16/1000 | Loss: 0.00003788
Iteration 17/1000 | Loss: 0.00003760
Iteration 18/1000 | Loss: 0.00003731
Iteration 19/1000 | Loss: 0.00003708
Iteration 20/1000 | Loss: 0.00003688
Iteration 21/1000 | Loss: 0.00003672
Iteration 22/1000 | Loss: 0.00003657
Iteration 23/1000 | Loss: 0.00003644
Iteration 24/1000 | Loss: 0.00003634
Iteration 25/1000 | Loss: 0.00003632
Iteration 26/1000 | Loss: 0.00003627
Iteration 27/1000 | Loss: 0.00003626
Iteration 28/1000 | Loss: 0.00003626
Iteration 29/1000 | Loss: 0.00003626
Iteration 30/1000 | Loss: 0.00003626
Iteration 31/1000 | Loss: 0.00003626
Iteration 32/1000 | Loss: 0.00003626
Iteration 33/1000 | Loss: 0.00003625
Iteration 34/1000 | Loss: 0.00003624
Iteration 35/1000 | Loss: 0.00003624
Iteration 36/1000 | Loss: 0.00003623
Iteration 37/1000 | Loss: 0.00003623
Iteration 38/1000 | Loss: 0.00003623
Iteration 39/1000 | Loss: 0.00003621
Iteration 40/1000 | Loss: 0.00003621
Iteration 41/1000 | Loss: 0.00003621
Iteration 42/1000 | Loss: 0.00003621
Iteration 43/1000 | Loss: 0.00003621
Iteration 44/1000 | Loss: 0.00003621
Iteration 45/1000 | Loss: 0.00003621
Iteration 46/1000 | Loss: 0.00003621
Iteration 47/1000 | Loss: 0.00003621
Iteration 48/1000 | Loss: 0.00003620
Iteration 49/1000 | Loss: 0.00003620
Iteration 50/1000 | Loss: 0.00003620
Iteration 51/1000 | Loss: 0.00003620
Iteration 52/1000 | Loss: 0.00003619
Iteration 53/1000 | Loss: 0.00003619
Iteration 54/1000 | Loss: 0.00003619
Iteration 55/1000 | Loss: 0.00003619
Iteration 56/1000 | Loss: 0.00003618
Iteration 57/1000 | Loss: 0.00003618
Iteration 58/1000 | Loss: 0.00003618
Iteration 59/1000 | Loss: 0.00003618
Iteration 60/1000 | Loss: 0.00003618
Iteration 61/1000 | Loss: 0.00003618
Iteration 62/1000 | Loss: 0.00003618
Iteration 63/1000 | Loss: 0.00003618
Iteration 64/1000 | Loss: 0.00003618
Iteration 65/1000 | Loss: 0.00003618
Iteration 66/1000 | Loss: 0.00003617
Iteration 67/1000 | Loss: 0.00003617
Iteration 68/1000 | Loss: 0.00003616
Iteration 69/1000 | Loss: 0.00003616
Iteration 70/1000 | Loss: 0.00003616
Iteration 71/1000 | Loss: 0.00003615
Iteration 72/1000 | Loss: 0.00003615
Iteration 73/1000 | Loss: 0.00003615
Iteration 74/1000 | Loss: 0.00003614
Iteration 75/1000 | Loss: 0.00003614
Iteration 76/1000 | Loss: 0.00003614
Iteration 77/1000 | Loss: 0.00003614
Iteration 78/1000 | Loss: 0.00003614
Iteration 79/1000 | Loss: 0.00003614
Iteration 80/1000 | Loss: 0.00003614
Iteration 81/1000 | Loss: 0.00003613
Iteration 82/1000 | Loss: 0.00003613
Iteration 83/1000 | Loss: 0.00003613
Iteration 84/1000 | Loss: 0.00003613
Iteration 85/1000 | Loss: 0.00003613
Iteration 86/1000 | Loss: 0.00003613
Iteration 87/1000 | Loss: 0.00003613
Iteration 88/1000 | Loss: 0.00003612
Iteration 89/1000 | Loss: 0.00003612
Iteration 90/1000 | Loss: 0.00003612
Iteration 91/1000 | Loss: 0.00003612
Iteration 92/1000 | Loss: 0.00003612
Iteration 93/1000 | Loss: 0.00003611
Iteration 94/1000 | Loss: 0.00003611
Iteration 95/1000 | Loss: 0.00003611
Iteration 96/1000 | Loss: 0.00003611
Iteration 97/1000 | Loss: 0.00003611
Iteration 98/1000 | Loss: 0.00003610
Iteration 99/1000 | Loss: 0.00003610
Iteration 100/1000 | Loss: 0.00003610
Iteration 101/1000 | Loss: 0.00003610
Iteration 102/1000 | Loss: 0.00003610
Iteration 103/1000 | Loss: 0.00003610
Iteration 104/1000 | Loss: 0.00003610
Iteration 105/1000 | Loss: 0.00003609
Iteration 106/1000 | Loss: 0.00003609
Iteration 107/1000 | Loss: 0.00003609
Iteration 108/1000 | Loss: 0.00003609
Iteration 109/1000 | Loss: 0.00003608
Iteration 110/1000 | Loss: 0.00003608
Iteration 111/1000 | Loss: 0.00003608
Iteration 112/1000 | Loss: 0.00003608
Iteration 113/1000 | Loss: 0.00003608
Iteration 114/1000 | Loss: 0.00003608
Iteration 115/1000 | Loss: 0.00003608
Iteration 116/1000 | Loss: 0.00003608
Iteration 117/1000 | Loss: 0.00003608
Iteration 118/1000 | Loss: 0.00003608
Iteration 119/1000 | Loss: 0.00003608
Iteration 120/1000 | Loss: 0.00003608
Iteration 121/1000 | Loss: 0.00003608
Iteration 122/1000 | Loss: 0.00003608
Iteration 123/1000 | Loss: 0.00003608
Iteration 124/1000 | Loss: 0.00003608
Iteration 125/1000 | Loss: 0.00003608
Iteration 126/1000 | Loss: 0.00003608
Iteration 127/1000 | Loss: 0.00003608
Iteration 128/1000 | Loss: 0.00003608
Iteration 129/1000 | Loss: 0.00003608
Iteration 130/1000 | Loss: 0.00003608
Iteration 131/1000 | Loss: 0.00003608
Iteration 132/1000 | Loss: 0.00003608
Iteration 133/1000 | Loss: 0.00003608
Iteration 134/1000 | Loss: 0.00003608
Iteration 135/1000 | Loss: 0.00003608
Iteration 136/1000 | Loss: 0.00003608
Iteration 137/1000 | Loss: 0.00003608
Iteration 138/1000 | Loss: 0.00003608
Iteration 139/1000 | Loss: 0.00003608
Iteration 140/1000 | Loss: 0.00003608
Iteration 141/1000 | Loss: 0.00003608
Iteration 142/1000 | Loss: 0.00003608
Iteration 143/1000 | Loss: 0.00003608
Iteration 144/1000 | Loss: 0.00003608
Iteration 145/1000 | Loss: 0.00003608
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 145. Stopping optimization.
Last 5 losses: [3.608046245062724e-05, 3.608046245062724e-05, 3.608046245062724e-05, 3.608046245062724e-05, 3.608046245062724e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.608046245062724e-05

Optimization complete. Final v2v error: 4.752487659454346 mm

Highest mean error: 5.755173683166504 mm for frame 132

Lowest mean error: 4.340206623077393 mm for frame 59

Saving results

Total time: 50.24773073196411
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00826548
Iteration 2/25 | Loss: 0.00114906
Iteration 3/25 | Loss: 0.00098723
Iteration 4/25 | Loss: 0.00097082
Iteration 5/25 | Loss: 0.00096693
Iteration 6/25 | Loss: 0.00096679
Iteration 7/25 | Loss: 0.00096679
Iteration 8/25 | Loss: 0.00096679
Iteration 9/25 | Loss: 0.00096679
Iteration 10/25 | Loss: 0.00096679
Iteration 11/25 | Loss: 0.00096679
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009667885024100542, 0.0009667885024100542, 0.0009667885024100542, 0.0009667885024100542, 0.0009667885024100542]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009667885024100542

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26667285
Iteration 2/25 | Loss: 0.00101490
Iteration 3/25 | Loss: 0.00101490
Iteration 4/25 | Loss: 0.00101490
Iteration 5/25 | Loss: 0.00101490
Iteration 6/25 | Loss: 0.00101490
Iteration 7/25 | Loss: 0.00101490
Iteration 8/25 | Loss: 0.00101490
Iteration 9/25 | Loss: 0.00101490
Iteration 10/25 | Loss: 0.00101490
Iteration 11/25 | Loss: 0.00101490
Iteration 12/25 | Loss: 0.00101490
Iteration 13/25 | Loss: 0.00101490
Iteration 14/25 | Loss: 0.00101490
Iteration 15/25 | Loss: 0.00101490
Iteration 16/25 | Loss: 0.00101490
Iteration 17/25 | Loss: 0.00101490
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010148974834010005, 0.0010148974834010005, 0.0010148974834010005, 0.0010148974834010005, 0.0010148974834010005]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010148974834010005

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101490
Iteration 2/1000 | Loss: 0.00004097
Iteration 3/1000 | Loss: 0.00002167
Iteration 4/1000 | Loss: 0.00001737
Iteration 5/1000 | Loss: 0.00001532
Iteration 6/1000 | Loss: 0.00001385
Iteration 7/1000 | Loss: 0.00001306
Iteration 8/1000 | Loss: 0.00001247
Iteration 9/1000 | Loss: 0.00001209
Iteration 10/1000 | Loss: 0.00001166
Iteration 11/1000 | Loss: 0.00001140
Iteration 12/1000 | Loss: 0.00001135
Iteration 13/1000 | Loss: 0.00001125
Iteration 14/1000 | Loss: 0.00001121
Iteration 15/1000 | Loss: 0.00001120
Iteration 16/1000 | Loss: 0.00001120
Iteration 17/1000 | Loss: 0.00001120
Iteration 18/1000 | Loss: 0.00001119
Iteration 19/1000 | Loss: 0.00001119
Iteration 20/1000 | Loss: 0.00001118
Iteration 21/1000 | Loss: 0.00001118
Iteration 22/1000 | Loss: 0.00001116
Iteration 23/1000 | Loss: 0.00001116
Iteration 24/1000 | Loss: 0.00001116
Iteration 25/1000 | Loss: 0.00001116
Iteration 26/1000 | Loss: 0.00001116
Iteration 27/1000 | Loss: 0.00001115
Iteration 28/1000 | Loss: 0.00001115
Iteration 29/1000 | Loss: 0.00001114
Iteration 30/1000 | Loss: 0.00001113
Iteration 31/1000 | Loss: 0.00001113
Iteration 32/1000 | Loss: 0.00001112
Iteration 33/1000 | Loss: 0.00001112
Iteration 34/1000 | Loss: 0.00001112
Iteration 35/1000 | Loss: 0.00001111
Iteration 36/1000 | Loss: 0.00001111
Iteration 37/1000 | Loss: 0.00001111
Iteration 38/1000 | Loss: 0.00001111
Iteration 39/1000 | Loss: 0.00001111
Iteration 40/1000 | Loss: 0.00001111
Iteration 41/1000 | Loss: 0.00001111
Iteration 42/1000 | Loss: 0.00001110
Iteration 43/1000 | Loss: 0.00001110
Iteration 44/1000 | Loss: 0.00001110
Iteration 45/1000 | Loss: 0.00001109
Iteration 46/1000 | Loss: 0.00001109
Iteration 47/1000 | Loss: 0.00001109
Iteration 48/1000 | Loss: 0.00001109
Iteration 49/1000 | Loss: 0.00001109
Iteration 50/1000 | Loss: 0.00001109
Iteration 51/1000 | Loss: 0.00001109
Iteration 52/1000 | Loss: 0.00001109
Iteration 53/1000 | Loss: 0.00001109
Iteration 54/1000 | Loss: 0.00001109
Iteration 55/1000 | Loss: 0.00001109
Iteration 56/1000 | Loss: 0.00001109
Iteration 57/1000 | Loss: 0.00001108
Iteration 58/1000 | Loss: 0.00001108
Iteration 59/1000 | Loss: 0.00001108
Iteration 60/1000 | Loss: 0.00001108
Iteration 61/1000 | Loss: 0.00001107
Iteration 62/1000 | Loss: 0.00001107
Iteration 63/1000 | Loss: 0.00001107
Iteration 64/1000 | Loss: 0.00001107
Iteration 65/1000 | Loss: 0.00001107
Iteration 66/1000 | Loss: 0.00001107
Iteration 67/1000 | Loss: 0.00001107
Iteration 68/1000 | Loss: 0.00001107
Iteration 69/1000 | Loss: 0.00001107
Iteration 70/1000 | Loss: 0.00001106
Iteration 71/1000 | Loss: 0.00001106
Iteration 72/1000 | Loss: 0.00001106
Iteration 73/1000 | Loss: 0.00001106
Iteration 74/1000 | Loss: 0.00001106
Iteration 75/1000 | Loss: 0.00001105
Iteration 76/1000 | Loss: 0.00001105
Iteration 77/1000 | Loss: 0.00001105
Iteration 78/1000 | Loss: 0.00001105
Iteration 79/1000 | Loss: 0.00001105
Iteration 80/1000 | Loss: 0.00001104
Iteration 81/1000 | Loss: 0.00001104
Iteration 82/1000 | Loss: 0.00001104
Iteration 83/1000 | Loss: 0.00001104
Iteration 84/1000 | Loss: 0.00001104
Iteration 85/1000 | Loss: 0.00001104
Iteration 86/1000 | Loss: 0.00001103
Iteration 87/1000 | Loss: 0.00001103
Iteration 88/1000 | Loss: 0.00001103
Iteration 89/1000 | Loss: 0.00001103
Iteration 90/1000 | Loss: 0.00001103
Iteration 91/1000 | Loss: 0.00001103
Iteration 92/1000 | Loss: 0.00001102
Iteration 93/1000 | Loss: 0.00001102
Iteration 94/1000 | Loss: 0.00001102
Iteration 95/1000 | Loss: 0.00001102
Iteration 96/1000 | Loss: 0.00001102
Iteration 97/1000 | Loss: 0.00001102
Iteration 98/1000 | Loss: 0.00001102
Iteration 99/1000 | Loss: 0.00001102
Iteration 100/1000 | Loss: 0.00001102
Iteration 101/1000 | Loss: 0.00001102
Iteration 102/1000 | Loss: 0.00001102
Iteration 103/1000 | Loss: 0.00001102
Iteration 104/1000 | Loss: 0.00001102
Iteration 105/1000 | Loss: 0.00001102
Iteration 106/1000 | Loss: 0.00001102
Iteration 107/1000 | Loss: 0.00001102
Iteration 108/1000 | Loss: 0.00001102
Iteration 109/1000 | Loss: 0.00001102
Iteration 110/1000 | Loss: 0.00001102
Iteration 111/1000 | Loss: 0.00001102
Iteration 112/1000 | Loss: 0.00001102
Iteration 113/1000 | Loss: 0.00001102
Iteration 114/1000 | Loss: 0.00001102
Iteration 115/1000 | Loss: 0.00001102
Iteration 116/1000 | Loss: 0.00001102
Iteration 117/1000 | Loss: 0.00001102
Iteration 118/1000 | Loss: 0.00001102
Iteration 119/1000 | Loss: 0.00001102
Iteration 120/1000 | Loss: 0.00001102
Iteration 121/1000 | Loss: 0.00001102
Iteration 122/1000 | Loss: 0.00001102
Iteration 123/1000 | Loss: 0.00001102
Iteration 124/1000 | Loss: 0.00001102
Iteration 125/1000 | Loss: 0.00001102
Iteration 126/1000 | Loss: 0.00001102
Iteration 127/1000 | Loss: 0.00001102
Iteration 128/1000 | Loss: 0.00001102
Iteration 129/1000 | Loss: 0.00001102
Iteration 130/1000 | Loss: 0.00001102
Iteration 131/1000 | Loss: 0.00001102
Iteration 132/1000 | Loss: 0.00001102
Iteration 133/1000 | Loss: 0.00001102
Iteration 134/1000 | Loss: 0.00001102
Iteration 135/1000 | Loss: 0.00001102
Iteration 136/1000 | Loss: 0.00001102
Iteration 137/1000 | Loss: 0.00001102
Iteration 138/1000 | Loss: 0.00001102
Iteration 139/1000 | Loss: 0.00001102
Iteration 140/1000 | Loss: 0.00001102
Iteration 141/1000 | Loss: 0.00001102
Iteration 142/1000 | Loss: 0.00001102
Iteration 143/1000 | Loss: 0.00001102
Iteration 144/1000 | Loss: 0.00001102
Iteration 145/1000 | Loss: 0.00001102
Iteration 146/1000 | Loss: 0.00001102
Iteration 147/1000 | Loss: 0.00001102
Iteration 148/1000 | Loss: 0.00001102
Iteration 149/1000 | Loss: 0.00001102
Iteration 150/1000 | Loss: 0.00001102
Iteration 151/1000 | Loss: 0.00001102
Iteration 152/1000 | Loss: 0.00001102
Iteration 153/1000 | Loss: 0.00001102
Iteration 154/1000 | Loss: 0.00001102
Iteration 155/1000 | Loss: 0.00001102
Iteration 156/1000 | Loss: 0.00001102
Iteration 157/1000 | Loss: 0.00001102
Iteration 158/1000 | Loss: 0.00001102
Iteration 159/1000 | Loss: 0.00001102
Iteration 160/1000 | Loss: 0.00001102
Iteration 161/1000 | Loss: 0.00001102
Iteration 162/1000 | Loss: 0.00001102
Iteration 163/1000 | Loss: 0.00001102
Iteration 164/1000 | Loss: 0.00001102
Iteration 165/1000 | Loss: 0.00001102
Iteration 166/1000 | Loss: 0.00001102
Iteration 167/1000 | Loss: 0.00001102
Iteration 168/1000 | Loss: 0.00001102
Iteration 169/1000 | Loss: 0.00001102
Iteration 170/1000 | Loss: 0.00001102
Iteration 171/1000 | Loss: 0.00001102
Iteration 172/1000 | Loss: 0.00001102
Iteration 173/1000 | Loss: 0.00001102
Iteration 174/1000 | Loss: 0.00001102
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 174. Stopping optimization.
Last 5 losses: [1.1017073120456189e-05, 1.1017073120456189e-05, 1.1017073120456189e-05, 1.1017073120456189e-05, 1.1017073120456189e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1017073120456189e-05

Optimization complete. Final v2v error: 2.7648978233337402 mm

Highest mean error: 3.343386650085449 mm for frame 119

Lowest mean error: 2.415982246398926 mm for frame 207

Saving results

Total time: 36.20552206039429
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00424473
Iteration 2/25 | Loss: 0.00112510
Iteration 3/25 | Loss: 0.00099586
Iteration 4/25 | Loss: 0.00098338
Iteration 5/25 | Loss: 0.00098050
Iteration 6/25 | Loss: 0.00097992
Iteration 7/25 | Loss: 0.00097992
Iteration 8/25 | Loss: 0.00097992
Iteration 9/25 | Loss: 0.00097992
Iteration 10/25 | Loss: 0.00097992
Iteration 11/25 | Loss: 0.00097992
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009799159597605467, 0.0009799159597605467, 0.0009799159597605467, 0.0009799159597605467, 0.0009799159597605467]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009799159597605467

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27016163
Iteration 2/25 | Loss: 0.00100531
Iteration 3/25 | Loss: 0.00100531
Iteration 4/25 | Loss: 0.00100531
Iteration 5/25 | Loss: 0.00100531
Iteration 6/25 | Loss: 0.00100531
Iteration 7/25 | Loss: 0.00100531
Iteration 8/25 | Loss: 0.00100531
Iteration 9/25 | Loss: 0.00100531
Iteration 10/25 | Loss: 0.00100531
Iteration 11/25 | Loss: 0.00100531
Iteration 12/25 | Loss: 0.00100531
Iteration 13/25 | Loss: 0.00100531
Iteration 14/25 | Loss: 0.00100531
Iteration 15/25 | Loss: 0.00100531
Iteration 16/25 | Loss: 0.00100531
Iteration 17/25 | Loss: 0.00100531
Iteration 18/25 | Loss: 0.00100531
Iteration 19/25 | Loss: 0.00100531
Iteration 20/25 | Loss: 0.00100531
Iteration 21/25 | Loss: 0.00100531
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001005306257866323, 0.001005306257866323, 0.001005306257866323, 0.001005306257866323, 0.001005306257866323]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001005306257866323

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00100531
Iteration 2/1000 | Loss: 0.00004638
Iteration 3/1000 | Loss: 0.00002611
Iteration 4/1000 | Loss: 0.00001887
Iteration 5/1000 | Loss: 0.00001588
Iteration 6/1000 | Loss: 0.00001458
Iteration 7/1000 | Loss: 0.00001373
Iteration 8/1000 | Loss: 0.00001322
Iteration 9/1000 | Loss: 0.00001278
Iteration 10/1000 | Loss: 0.00001237
Iteration 11/1000 | Loss: 0.00001211
Iteration 12/1000 | Loss: 0.00001200
Iteration 13/1000 | Loss: 0.00001194
Iteration 14/1000 | Loss: 0.00001189
Iteration 15/1000 | Loss: 0.00001185
Iteration 16/1000 | Loss: 0.00001181
Iteration 17/1000 | Loss: 0.00001178
Iteration 18/1000 | Loss: 0.00001176
Iteration 19/1000 | Loss: 0.00001174
Iteration 20/1000 | Loss: 0.00001173
Iteration 21/1000 | Loss: 0.00001172
Iteration 22/1000 | Loss: 0.00001172
Iteration 23/1000 | Loss: 0.00001168
Iteration 24/1000 | Loss: 0.00001166
Iteration 25/1000 | Loss: 0.00001165
Iteration 26/1000 | Loss: 0.00001162
Iteration 27/1000 | Loss: 0.00001162
Iteration 28/1000 | Loss: 0.00001162
Iteration 29/1000 | Loss: 0.00001162
Iteration 30/1000 | Loss: 0.00001162
Iteration 31/1000 | Loss: 0.00001162
Iteration 32/1000 | Loss: 0.00001162
Iteration 33/1000 | Loss: 0.00001161
Iteration 34/1000 | Loss: 0.00001161
Iteration 35/1000 | Loss: 0.00001161
Iteration 36/1000 | Loss: 0.00001161
Iteration 37/1000 | Loss: 0.00001160
Iteration 38/1000 | Loss: 0.00001160
Iteration 39/1000 | Loss: 0.00001160
Iteration 40/1000 | Loss: 0.00001160
Iteration 41/1000 | Loss: 0.00001160
Iteration 42/1000 | Loss: 0.00001160
Iteration 43/1000 | Loss: 0.00001160
Iteration 44/1000 | Loss: 0.00001159
Iteration 45/1000 | Loss: 0.00001159
Iteration 46/1000 | Loss: 0.00001159
Iteration 47/1000 | Loss: 0.00001159
Iteration 48/1000 | Loss: 0.00001159
Iteration 49/1000 | Loss: 0.00001159
Iteration 50/1000 | Loss: 0.00001158
Iteration 51/1000 | Loss: 0.00001158
Iteration 52/1000 | Loss: 0.00001158
Iteration 53/1000 | Loss: 0.00001157
Iteration 54/1000 | Loss: 0.00001157
Iteration 55/1000 | Loss: 0.00001157
Iteration 56/1000 | Loss: 0.00001157
Iteration 57/1000 | Loss: 0.00001157
Iteration 58/1000 | Loss: 0.00001157
Iteration 59/1000 | Loss: 0.00001157
Iteration 60/1000 | Loss: 0.00001157
Iteration 61/1000 | Loss: 0.00001157
Iteration 62/1000 | Loss: 0.00001156
Iteration 63/1000 | Loss: 0.00001156
Iteration 64/1000 | Loss: 0.00001156
Iteration 65/1000 | Loss: 0.00001155
Iteration 66/1000 | Loss: 0.00001155
Iteration 67/1000 | Loss: 0.00001155
Iteration 68/1000 | Loss: 0.00001155
Iteration 69/1000 | Loss: 0.00001155
Iteration 70/1000 | Loss: 0.00001155
Iteration 71/1000 | Loss: 0.00001155
Iteration 72/1000 | Loss: 0.00001155
Iteration 73/1000 | Loss: 0.00001155
Iteration 74/1000 | Loss: 0.00001155
Iteration 75/1000 | Loss: 0.00001155
Iteration 76/1000 | Loss: 0.00001155
Iteration 77/1000 | Loss: 0.00001154
Iteration 78/1000 | Loss: 0.00001154
Iteration 79/1000 | Loss: 0.00001154
Iteration 80/1000 | Loss: 0.00001154
Iteration 81/1000 | Loss: 0.00001154
Iteration 82/1000 | Loss: 0.00001154
Iteration 83/1000 | Loss: 0.00001154
Iteration 84/1000 | Loss: 0.00001154
Iteration 85/1000 | Loss: 0.00001154
Iteration 86/1000 | Loss: 0.00001154
Iteration 87/1000 | Loss: 0.00001154
Iteration 88/1000 | Loss: 0.00001154
Iteration 89/1000 | Loss: 0.00001153
Iteration 90/1000 | Loss: 0.00001153
Iteration 91/1000 | Loss: 0.00001153
Iteration 92/1000 | Loss: 0.00001153
Iteration 93/1000 | Loss: 0.00001153
Iteration 94/1000 | Loss: 0.00001152
Iteration 95/1000 | Loss: 0.00001152
Iteration 96/1000 | Loss: 0.00001152
Iteration 97/1000 | Loss: 0.00001152
Iteration 98/1000 | Loss: 0.00001152
Iteration 99/1000 | Loss: 0.00001152
Iteration 100/1000 | Loss: 0.00001152
Iteration 101/1000 | Loss: 0.00001152
Iteration 102/1000 | Loss: 0.00001152
Iteration 103/1000 | Loss: 0.00001152
Iteration 104/1000 | Loss: 0.00001152
Iteration 105/1000 | Loss: 0.00001151
Iteration 106/1000 | Loss: 0.00001151
Iteration 107/1000 | Loss: 0.00001151
Iteration 108/1000 | Loss: 0.00001151
Iteration 109/1000 | Loss: 0.00001151
Iteration 110/1000 | Loss: 0.00001151
Iteration 111/1000 | Loss: 0.00001151
Iteration 112/1000 | Loss: 0.00001151
Iteration 113/1000 | Loss: 0.00001151
Iteration 114/1000 | Loss: 0.00001151
Iteration 115/1000 | Loss: 0.00001151
Iteration 116/1000 | Loss: 0.00001151
Iteration 117/1000 | Loss: 0.00001150
Iteration 118/1000 | Loss: 0.00001150
Iteration 119/1000 | Loss: 0.00001150
Iteration 120/1000 | Loss: 0.00001150
Iteration 121/1000 | Loss: 0.00001150
Iteration 122/1000 | Loss: 0.00001150
Iteration 123/1000 | Loss: 0.00001150
Iteration 124/1000 | Loss: 0.00001150
Iteration 125/1000 | Loss: 0.00001150
Iteration 126/1000 | Loss: 0.00001149
Iteration 127/1000 | Loss: 0.00001149
Iteration 128/1000 | Loss: 0.00001149
Iteration 129/1000 | Loss: 0.00001149
Iteration 130/1000 | Loss: 0.00001149
Iteration 131/1000 | Loss: 0.00001149
Iteration 132/1000 | Loss: 0.00001149
Iteration 133/1000 | Loss: 0.00001149
Iteration 134/1000 | Loss: 0.00001149
Iteration 135/1000 | Loss: 0.00001149
Iteration 136/1000 | Loss: 0.00001149
Iteration 137/1000 | Loss: 0.00001149
Iteration 138/1000 | Loss: 0.00001149
Iteration 139/1000 | Loss: 0.00001149
Iteration 140/1000 | Loss: 0.00001149
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 140. Stopping optimization.
Last 5 losses: [1.1493521924421657e-05, 1.1493521924421657e-05, 1.1493521924421657e-05, 1.1493521924421657e-05, 1.1493521924421657e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1493521924421657e-05

Optimization complete. Final v2v error: 2.8278777599334717 mm

Highest mean error: 3.858288526535034 mm for frame 65

Lowest mean error: 2.4558286666870117 mm for frame 0

Saving results

Total time: 39.7856183052063
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0016/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0016.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0016
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00838735
Iteration 2/25 | Loss: 0.00123241
Iteration 3/25 | Loss: 0.00100423
Iteration 4/25 | Loss: 0.00098167
Iteration 5/25 | Loss: 0.00097812
Iteration 6/25 | Loss: 0.00097778
Iteration 7/25 | Loss: 0.00097778
Iteration 8/25 | Loss: 0.00097778
Iteration 9/25 | Loss: 0.00097778
Iteration 10/25 | Loss: 0.00097778
Iteration 11/25 | Loss: 0.00097778
Iteration 12/25 | Loss: 0.00097778
Iteration 13/25 | Loss: 0.00097778
Iteration 14/25 | Loss: 0.00097778
Iteration 15/25 | Loss: 0.00097778
Iteration 16/25 | Loss: 0.00097778
Iteration 17/25 | Loss: 0.00097778
Iteration 18/25 | Loss: 0.00097778
Iteration 19/25 | Loss: 0.00097778
Iteration 20/25 | Loss: 0.00097778
Iteration 21/25 | Loss: 0.00097778
Iteration 22/25 | Loss: 0.00097778
Iteration 23/25 | Loss: 0.00097778
Iteration 24/25 | Loss: 0.00097778
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000977776013314724, 0.000977776013314724, 0.000977776013314724, 0.000977776013314724, 0.000977776013314724]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000977776013314724

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27138031
Iteration 2/25 | Loss: 0.00105241
Iteration 3/25 | Loss: 0.00105241
Iteration 4/25 | Loss: 0.00105241
Iteration 5/25 | Loss: 0.00105241
Iteration 6/25 | Loss: 0.00105241
Iteration 7/25 | Loss: 0.00105241
Iteration 8/25 | Loss: 0.00105241
Iteration 9/25 | Loss: 0.00105241
Iteration 10/25 | Loss: 0.00105241
Iteration 11/25 | Loss: 0.00105241
Iteration 12/25 | Loss: 0.00105241
Iteration 13/25 | Loss: 0.00105241
Iteration 14/25 | Loss: 0.00105241
Iteration 15/25 | Loss: 0.00105241
Iteration 16/25 | Loss: 0.00105241
Iteration 17/25 | Loss: 0.00105241
Iteration 18/25 | Loss: 0.00105241
Iteration 19/25 | Loss: 0.00105241
Iteration 20/25 | Loss: 0.00105241
Iteration 21/25 | Loss: 0.00105241
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 21. Stopping optimization.
Last 5 losses: [0.001052411855198443, 0.001052411855198443, 0.001052411855198443, 0.001052411855198443, 0.001052411855198443]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001052411855198443

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105241
Iteration 2/1000 | Loss: 0.00004930
Iteration 3/1000 | Loss: 0.00002901
Iteration 4/1000 | Loss: 0.00002031
Iteration 5/1000 | Loss: 0.00001702
Iteration 6/1000 | Loss: 0.00001538
Iteration 7/1000 | Loss: 0.00001427
Iteration 8/1000 | Loss: 0.00001365
Iteration 9/1000 | Loss: 0.00001332
Iteration 10/1000 | Loss: 0.00001294
Iteration 11/1000 | Loss: 0.00001258
Iteration 12/1000 | Loss: 0.00001244
Iteration 13/1000 | Loss: 0.00001229
Iteration 14/1000 | Loss: 0.00001225
Iteration 15/1000 | Loss: 0.00001223
Iteration 16/1000 | Loss: 0.00001221
Iteration 17/1000 | Loss: 0.00001219
Iteration 18/1000 | Loss: 0.00001219
Iteration 19/1000 | Loss: 0.00001218
Iteration 20/1000 | Loss: 0.00001216
Iteration 21/1000 | Loss: 0.00001215
Iteration 22/1000 | Loss: 0.00001215
Iteration 23/1000 | Loss: 0.00001214
Iteration 24/1000 | Loss: 0.00001214
Iteration 25/1000 | Loss: 0.00001213
Iteration 26/1000 | Loss: 0.00001213
Iteration 27/1000 | Loss: 0.00001213
Iteration 28/1000 | Loss: 0.00001213
Iteration 29/1000 | Loss: 0.00001213
Iteration 30/1000 | Loss: 0.00001213
Iteration 31/1000 | Loss: 0.00001213
Iteration 32/1000 | Loss: 0.00001212
Iteration 33/1000 | Loss: 0.00001212
Iteration 34/1000 | Loss: 0.00001212
Iteration 35/1000 | Loss: 0.00001212
Iteration 36/1000 | Loss: 0.00001212
Iteration 37/1000 | Loss: 0.00001211
Iteration 38/1000 | Loss: 0.00001210
Iteration 39/1000 | Loss: 0.00001210
Iteration 40/1000 | Loss: 0.00001210
Iteration 41/1000 | Loss: 0.00001209
Iteration 42/1000 | Loss: 0.00001209
Iteration 43/1000 | Loss: 0.00001209
Iteration 44/1000 | Loss: 0.00001208
Iteration 45/1000 | Loss: 0.00001208
Iteration 46/1000 | Loss: 0.00001208
Iteration 47/1000 | Loss: 0.00001207
Iteration 48/1000 | Loss: 0.00001207
Iteration 49/1000 | Loss: 0.00001207
Iteration 50/1000 | Loss: 0.00001206
Iteration 51/1000 | Loss: 0.00001206
Iteration 52/1000 | Loss: 0.00001206
Iteration 53/1000 | Loss: 0.00001206
Iteration 54/1000 | Loss: 0.00001205
Iteration 55/1000 | Loss: 0.00001205
Iteration 56/1000 | Loss: 0.00001204
Iteration 57/1000 | Loss: 0.00001204
Iteration 58/1000 | Loss: 0.00001204
Iteration 59/1000 | Loss: 0.00001204
Iteration 60/1000 | Loss: 0.00001204
Iteration 61/1000 | Loss: 0.00001203
Iteration 62/1000 | Loss: 0.00001203
Iteration 63/1000 | Loss: 0.00001203
Iteration 64/1000 | Loss: 0.00001203
Iteration 65/1000 | Loss: 0.00001203
Iteration 66/1000 | Loss: 0.00001203
Iteration 67/1000 | Loss: 0.00001203
Iteration 68/1000 | Loss: 0.00001203
Iteration 69/1000 | Loss: 0.00001203
Iteration 70/1000 | Loss: 0.00001203
Iteration 71/1000 | Loss: 0.00001202
Iteration 72/1000 | Loss: 0.00001202
Iteration 73/1000 | Loss: 0.00001202
Iteration 74/1000 | Loss: 0.00001202
Iteration 75/1000 | Loss: 0.00001202
Iteration 76/1000 | Loss: 0.00001202
Iteration 77/1000 | Loss: 0.00001202
Iteration 78/1000 | Loss: 0.00001202
Iteration 79/1000 | Loss: 0.00001202
Iteration 80/1000 | Loss: 0.00001201
Iteration 81/1000 | Loss: 0.00001201
Iteration 82/1000 | Loss: 0.00001201
Iteration 83/1000 | Loss: 0.00001201
Iteration 84/1000 | Loss: 0.00001201
Iteration 85/1000 | Loss: 0.00001201
Iteration 86/1000 | Loss: 0.00001201
Iteration 87/1000 | Loss: 0.00001200
Iteration 88/1000 | Loss: 0.00001200
Iteration 89/1000 | Loss: 0.00001200
Iteration 90/1000 | Loss: 0.00001200
Iteration 91/1000 | Loss: 0.00001200
Iteration 92/1000 | Loss: 0.00001200
Iteration 93/1000 | Loss: 0.00001200
Iteration 94/1000 | Loss: 0.00001200
Iteration 95/1000 | Loss: 0.00001200
Iteration 96/1000 | Loss: 0.00001200
Iteration 97/1000 | Loss: 0.00001200
Iteration 98/1000 | Loss: 0.00001200
Iteration 99/1000 | Loss: 0.00001200
Iteration 100/1000 | Loss: 0.00001200
Iteration 101/1000 | Loss: 0.00001200
Iteration 102/1000 | Loss: 0.00001200
Iteration 103/1000 | Loss: 0.00001200
Iteration 104/1000 | Loss: 0.00001200
Iteration 105/1000 | Loss: 0.00001200
Iteration 106/1000 | Loss: 0.00001200
Iteration 107/1000 | Loss: 0.00001200
Iteration 108/1000 | Loss: 0.00001200
Iteration 109/1000 | Loss: 0.00001200
Iteration 110/1000 | Loss: 0.00001200
Iteration 111/1000 | Loss: 0.00001200
Iteration 112/1000 | Loss: 0.00001200
Iteration 113/1000 | Loss: 0.00001200
Iteration 114/1000 | Loss: 0.00001200
Iteration 115/1000 | Loss: 0.00001200
Iteration 116/1000 | Loss: 0.00001200
Iteration 117/1000 | Loss: 0.00001200
Iteration 118/1000 | Loss: 0.00001200
Iteration 119/1000 | Loss: 0.00001200
Iteration 120/1000 | Loss: 0.00001200
Iteration 121/1000 | Loss: 0.00001200
Iteration 122/1000 | Loss: 0.00001200
Iteration 123/1000 | Loss: 0.00001200
Iteration 124/1000 | Loss: 0.00001200
Iteration 125/1000 | Loss: 0.00001200
Iteration 126/1000 | Loss: 0.00001200
Iteration 127/1000 | Loss: 0.00001200
Iteration 128/1000 | Loss: 0.00001200
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.1996175999229308e-05, 1.1996175999229308e-05, 1.1996175999229308e-05, 1.1996175999229308e-05, 1.1996175999229308e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1996175999229308e-05

Optimization complete. Final v2v error: 2.9897048473358154 mm

Highest mean error: 3.569256067276001 mm for frame 113

Lowest mean error: 2.5983293056488037 mm for frame 0

Saving results

Total time: 34.31013512611389
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00630086
Iteration 2/25 | Loss: 0.00103183
Iteration 3/25 | Loss: 0.00093106
Iteration 4/25 | Loss: 0.00092223
Iteration 5/25 | Loss: 0.00092031
Iteration 6/25 | Loss: 0.00092020
Iteration 7/25 | Loss: 0.00092020
Iteration 8/25 | Loss: 0.00092020
Iteration 9/25 | Loss: 0.00092020
Iteration 10/25 | Loss: 0.00092020
Iteration 11/25 | Loss: 0.00092020
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.000920195656362921, 0.000920195656362921, 0.000920195656362921, 0.000920195656362921, 0.000920195656362921]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000920195656362921

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.63108635
Iteration 2/25 | Loss: 0.00088965
Iteration 3/25 | Loss: 0.00088962
Iteration 4/25 | Loss: 0.00088962
Iteration 5/25 | Loss: 0.00088962
Iteration 6/25 | Loss: 0.00088962
Iteration 7/25 | Loss: 0.00088962
Iteration 8/25 | Loss: 0.00088962
Iteration 9/25 | Loss: 0.00088962
Iteration 10/25 | Loss: 0.00088962
Iteration 11/25 | Loss: 0.00088962
Iteration 12/25 | Loss: 0.00088962
Iteration 13/25 | Loss: 0.00088962
Iteration 14/25 | Loss: 0.00088962
Iteration 15/25 | Loss: 0.00088962
Iteration 16/25 | Loss: 0.00088962
Iteration 17/25 | Loss: 0.00088962
Iteration 18/25 | Loss: 0.00088962
Iteration 19/25 | Loss: 0.00088962
Iteration 20/25 | Loss: 0.00088962
Iteration 21/25 | Loss: 0.00088962
Iteration 22/25 | Loss: 0.00088962
Iteration 23/25 | Loss: 0.00088962
Iteration 24/25 | Loss: 0.00088962
Iteration 25/25 | Loss: 0.00088962

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088962
Iteration 2/1000 | Loss: 0.00003064
Iteration 3/1000 | Loss: 0.00001798
Iteration 4/1000 | Loss: 0.00001496
Iteration 5/1000 | Loss: 0.00001324
Iteration 6/1000 | Loss: 0.00001245
Iteration 7/1000 | Loss: 0.00001188
Iteration 8/1000 | Loss: 0.00001143
Iteration 9/1000 | Loss: 0.00001113
Iteration 10/1000 | Loss: 0.00001109
Iteration 11/1000 | Loss: 0.00001091
Iteration 12/1000 | Loss: 0.00001089
Iteration 13/1000 | Loss: 0.00001086
Iteration 14/1000 | Loss: 0.00001084
Iteration 15/1000 | Loss: 0.00001083
Iteration 16/1000 | Loss: 0.00001082
Iteration 17/1000 | Loss: 0.00001082
Iteration 18/1000 | Loss: 0.00001081
Iteration 19/1000 | Loss: 0.00001079
Iteration 20/1000 | Loss: 0.00001078
Iteration 21/1000 | Loss: 0.00001078
Iteration 22/1000 | Loss: 0.00001077
Iteration 23/1000 | Loss: 0.00001077
Iteration 24/1000 | Loss: 0.00001077
Iteration 25/1000 | Loss: 0.00001076
Iteration 26/1000 | Loss: 0.00001076
Iteration 27/1000 | Loss: 0.00001076
Iteration 28/1000 | Loss: 0.00001076
Iteration 29/1000 | Loss: 0.00001075
Iteration 30/1000 | Loss: 0.00001075
Iteration 31/1000 | Loss: 0.00001075
Iteration 32/1000 | Loss: 0.00001075
Iteration 33/1000 | Loss: 0.00001074
Iteration 34/1000 | Loss: 0.00001074
Iteration 35/1000 | Loss: 0.00001074
Iteration 36/1000 | Loss: 0.00001073
Iteration 37/1000 | Loss: 0.00001073
Iteration 38/1000 | Loss: 0.00001073
Iteration 39/1000 | Loss: 0.00001073
Iteration 40/1000 | Loss: 0.00001073
Iteration 41/1000 | Loss: 0.00001072
Iteration 42/1000 | Loss: 0.00001072
Iteration 43/1000 | Loss: 0.00001072
Iteration 44/1000 | Loss: 0.00001072
Iteration 45/1000 | Loss: 0.00001072
Iteration 46/1000 | Loss: 0.00001072
Iteration 47/1000 | Loss: 0.00001072
Iteration 48/1000 | Loss: 0.00001072
Iteration 49/1000 | Loss: 0.00001072
Iteration 50/1000 | Loss: 0.00001072
Iteration 51/1000 | Loss: 0.00001071
Iteration 52/1000 | Loss: 0.00001071
Iteration 53/1000 | Loss: 0.00001071
Iteration 54/1000 | Loss: 0.00001071
Iteration 55/1000 | Loss: 0.00001071
Iteration 56/1000 | Loss: 0.00001071
Iteration 57/1000 | Loss: 0.00001071
Iteration 58/1000 | Loss: 0.00001071
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 58. Stopping optimization.
Last 5 losses: [1.0706705324992072e-05, 1.0706705324992072e-05, 1.0706705324992072e-05, 1.0706705324992072e-05, 1.0706705324992072e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0706705324992072e-05

Optimization complete. Final v2v error: 2.7103681564331055 mm

Highest mean error: 3.134430170059204 mm for frame 116

Lowest mean error: 2.435584306716919 mm for frame 21

Saving results

Total time: 29.227635860443115
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0017/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0017.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0017
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00852267
Iteration 2/25 | Loss: 0.00166989
Iteration 3/25 | Loss: 0.00117494
Iteration 4/25 | Loss: 0.00107080
Iteration 5/25 | Loss: 0.00105451
Iteration 6/25 | Loss: 0.00105315
Iteration 7/25 | Loss: 0.00105312
Iteration 8/25 | Loss: 0.00105312
Iteration 9/25 | Loss: 0.00105312
Iteration 10/25 | Loss: 0.00105312
Iteration 11/25 | Loss: 0.00105312
Iteration 12/25 | Loss: 0.00105312
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001053119427524507, 0.001053119427524507, 0.001053119427524507, 0.001053119427524507, 0.001053119427524507]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001053119427524507

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25716937
Iteration 2/25 | Loss: 0.00105984
Iteration 3/25 | Loss: 0.00105983
Iteration 4/25 | Loss: 0.00105983
Iteration 5/25 | Loss: 0.00105983
Iteration 6/25 | Loss: 0.00105983
Iteration 7/25 | Loss: 0.00105983
Iteration 8/25 | Loss: 0.00105983
Iteration 9/25 | Loss: 0.00105983
Iteration 10/25 | Loss: 0.00105983
Iteration 11/25 | Loss: 0.00105983
Iteration 12/25 | Loss: 0.00105983
Iteration 13/25 | Loss: 0.00105983
Iteration 14/25 | Loss: 0.00105983
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.0010598318185657263, 0.0010598318185657263, 0.0010598318185657263, 0.0010598318185657263, 0.0010598318185657263]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010598318185657263

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105983
Iteration 2/1000 | Loss: 0.00004080
Iteration 3/1000 | Loss: 0.00002763
Iteration 4/1000 | Loss: 0.00002400
Iteration 5/1000 | Loss: 0.00002195
Iteration 6/1000 | Loss: 0.00002101
Iteration 7/1000 | Loss: 0.00002032
Iteration 8/1000 | Loss: 0.00001964
Iteration 9/1000 | Loss: 0.00001896
Iteration 10/1000 | Loss: 0.00001867
Iteration 11/1000 | Loss: 0.00001847
Iteration 12/1000 | Loss: 0.00001847
Iteration 13/1000 | Loss: 0.00001846
Iteration 14/1000 | Loss: 0.00001846
Iteration 15/1000 | Loss: 0.00001845
Iteration 16/1000 | Loss: 0.00001845
Iteration 17/1000 | Loss: 0.00001844
Iteration 18/1000 | Loss: 0.00001844
Iteration 19/1000 | Loss: 0.00001838
Iteration 20/1000 | Loss: 0.00001837
Iteration 21/1000 | Loss: 0.00001830
Iteration 22/1000 | Loss: 0.00001829
Iteration 23/1000 | Loss: 0.00001828
Iteration 24/1000 | Loss: 0.00001827
Iteration 25/1000 | Loss: 0.00001825
Iteration 26/1000 | Loss: 0.00001822
Iteration 27/1000 | Loss: 0.00001822
Iteration 28/1000 | Loss: 0.00001820
Iteration 29/1000 | Loss: 0.00001820
Iteration 30/1000 | Loss: 0.00001819
Iteration 31/1000 | Loss: 0.00001819
Iteration 32/1000 | Loss: 0.00001819
Iteration 33/1000 | Loss: 0.00001819
Iteration 34/1000 | Loss: 0.00001817
Iteration 35/1000 | Loss: 0.00001817
Iteration 36/1000 | Loss: 0.00001815
Iteration 37/1000 | Loss: 0.00001815
Iteration 38/1000 | Loss: 0.00001815
Iteration 39/1000 | Loss: 0.00001815
Iteration 40/1000 | Loss: 0.00001814
Iteration 41/1000 | Loss: 0.00001813
Iteration 42/1000 | Loss: 0.00001813
Iteration 43/1000 | Loss: 0.00001813
Iteration 44/1000 | Loss: 0.00001813
Iteration 45/1000 | Loss: 0.00001813
Iteration 46/1000 | Loss: 0.00001813
Iteration 47/1000 | Loss: 0.00001813
Iteration 48/1000 | Loss: 0.00001812
Iteration 49/1000 | Loss: 0.00001812
Iteration 50/1000 | Loss: 0.00001812
Iteration 51/1000 | Loss: 0.00001812
Iteration 52/1000 | Loss: 0.00001812
Iteration 53/1000 | Loss: 0.00001812
Iteration 54/1000 | Loss: 0.00001812
Iteration 55/1000 | Loss: 0.00001812
Iteration 56/1000 | Loss: 0.00001812
Iteration 57/1000 | Loss: 0.00001812
Iteration 58/1000 | Loss: 0.00001812
Iteration 59/1000 | Loss: 0.00001812
Iteration 60/1000 | Loss: 0.00001812
Iteration 61/1000 | Loss: 0.00001812
Iteration 62/1000 | Loss: 0.00001811
Iteration 63/1000 | Loss: 0.00001811
Iteration 64/1000 | Loss: 0.00001811
Iteration 65/1000 | Loss: 0.00001811
Iteration 66/1000 | Loss: 0.00001811
Iteration 67/1000 | Loss: 0.00001811
Iteration 68/1000 | Loss: 0.00001811
Iteration 69/1000 | Loss: 0.00001811
Iteration 70/1000 | Loss: 0.00001811
Iteration 71/1000 | Loss: 0.00001811
Iteration 72/1000 | Loss: 0.00001811
Iteration 73/1000 | Loss: 0.00001810
Iteration 74/1000 | Loss: 0.00001810
Iteration 75/1000 | Loss: 0.00001810
Iteration 76/1000 | Loss: 0.00001810
Iteration 77/1000 | Loss: 0.00001810
Iteration 78/1000 | Loss: 0.00001810
Iteration 79/1000 | Loss: 0.00001810
Iteration 80/1000 | Loss: 0.00001810
Iteration 81/1000 | Loss: 0.00001810
Iteration 82/1000 | Loss: 0.00001810
Iteration 83/1000 | Loss: 0.00001810
Iteration 84/1000 | Loss: 0.00001810
Iteration 85/1000 | Loss: 0.00001810
Iteration 86/1000 | Loss: 0.00001810
Iteration 87/1000 | Loss: 0.00001810
Iteration 88/1000 | Loss: 0.00001810
Iteration 89/1000 | Loss: 0.00001810
Iteration 90/1000 | Loss: 0.00001809
Iteration 91/1000 | Loss: 0.00001809
Iteration 92/1000 | Loss: 0.00001809
Iteration 93/1000 | Loss: 0.00001809
Iteration 94/1000 | Loss: 0.00001809
Iteration 95/1000 | Loss: 0.00001809
Iteration 96/1000 | Loss: 0.00001809
Iteration 97/1000 | Loss: 0.00001809
Iteration 98/1000 | Loss: 0.00001808
Iteration 99/1000 | Loss: 0.00001808
Iteration 100/1000 | Loss: 0.00001808
Iteration 101/1000 | Loss: 0.00001808
Iteration 102/1000 | Loss: 0.00001808
Iteration 103/1000 | Loss: 0.00001808
Iteration 104/1000 | Loss: 0.00001808
Iteration 105/1000 | Loss: 0.00001808
Iteration 106/1000 | Loss: 0.00001808
Iteration 107/1000 | Loss: 0.00001808
Iteration 108/1000 | Loss: 0.00001808
Iteration 109/1000 | Loss: 0.00001808
Iteration 110/1000 | Loss: 0.00001808
Iteration 111/1000 | Loss: 0.00001808
Iteration 112/1000 | Loss: 0.00001808
Iteration 113/1000 | Loss: 0.00001808
Iteration 114/1000 | Loss: 0.00001808
Iteration 115/1000 | Loss: 0.00001808
Iteration 116/1000 | Loss: 0.00001808
Iteration 117/1000 | Loss: 0.00001808
Iteration 118/1000 | Loss: 0.00001808
Iteration 119/1000 | Loss: 0.00001808
Iteration 120/1000 | Loss: 0.00001808
Iteration 121/1000 | Loss: 0.00001808
Iteration 122/1000 | Loss: 0.00001808
Iteration 123/1000 | Loss: 0.00001808
Iteration 124/1000 | Loss: 0.00001808
Iteration 125/1000 | Loss: 0.00001808
Iteration 126/1000 | Loss: 0.00001808
Iteration 127/1000 | Loss: 0.00001808
Iteration 128/1000 | Loss: 0.00001808
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 128. Stopping optimization.
Last 5 losses: [1.8082208043779247e-05, 1.8082208043779247e-05, 1.8082208043779247e-05, 1.8082208043779247e-05, 1.8082208043779247e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.8082208043779247e-05

Optimization complete. Final v2v error: 3.4683187007904053 mm

Highest mean error: 3.6721489429473877 mm for frame 34

Lowest mean error: 3.1044838428497314 mm for frame 16

Saving results

Total time: 38.00438833236694
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0012/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0012.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0012
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00841938
Iteration 2/25 | Loss: 0.00125077
Iteration 3/25 | Loss: 0.00108531
Iteration 4/25 | Loss: 0.00106029
Iteration 5/25 | Loss: 0.00105386
Iteration 6/25 | Loss: 0.00105222
Iteration 7/25 | Loss: 0.00105216
Iteration 8/25 | Loss: 0.00105216
Iteration 9/25 | Loss: 0.00105216
Iteration 10/25 | Loss: 0.00105216
Iteration 11/25 | Loss: 0.00105216
Iteration 12/25 | Loss: 0.00105216
Iteration 13/25 | Loss: 0.00105216
Iteration 14/25 | Loss: 0.00105216
Iteration 15/25 | Loss: 0.00105216
Iteration 16/25 | Loss: 0.00105216
Iteration 17/25 | Loss: 0.00105216
Iteration 18/25 | Loss: 0.00105216
Iteration 19/25 | Loss: 0.00105216
Iteration 20/25 | Loss: 0.00105216
Iteration 21/25 | Loss: 0.00105216
Iteration 22/25 | Loss: 0.00105216
Iteration 23/25 | Loss: 0.00105216
Iteration 24/25 | Loss: 0.00105216
Iteration 25/25 | Loss: 0.00105216

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.36949301
Iteration 2/25 | Loss: 0.00134528
Iteration 3/25 | Loss: 0.00134526
Iteration 4/25 | Loss: 0.00134526
Iteration 5/25 | Loss: 0.00134525
Iteration 6/25 | Loss: 0.00134525
Iteration 7/25 | Loss: 0.00134525
Iteration 8/25 | Loss: 0.00134525
Iteration 9/25 | Loss: 0.00134525
Iteration 10/25 | Loss: 0.00134525
Iteration 11/25 | Loss: 0.00134525
Iteration 12/25 | Loss: 0.00134525
Iteration 13/25 | Loss: 0.00134525
Iteration 14/25 | Loss: 0.00134525
Iteration 15/25 | Loss: 0.00134525
Iteration 16/25 | Loss: 0.00134525
Iteration 17/25 | Loss: 0.00134525
Iteration 18/25 | Loss: 0.00134525
Iteration 19/25 | Loss: 0.00134525
Iteration 20/25 | Loss: 0.00134525
Iteration 21/25 | Loss: 0.00134525
Iteration 22/25 | Loss: 0.00134525
Iteration 23/25 | Loss: 0.00134525
Iteration 24/25 | Loss: 0.00134525
Iteration 25/25 | Loss: 0.00134525

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00134525
Iteration 2/1000 | Loss: 0.00007072
Iteration 3/1000 | Loss: 0.00003776
Iteration 4/1000 | Loss: 0.00002744
Iteration 5/1000 | Loss: 0.00002368
Iteration 6/1000 | Loss: 0.00002153
Iteration 7/1000 | Loss: 0.00002056
Iteration 8/1000 | Loss: 0.00001985
Iteration 9/1000 | Loss: 0.00001932
Iteration 10/1000 | Loss: 0.00001890
Iteration 11/1000 | Loss: 0.00001858
Iteration 12/1000 | Loss: 0.00001833
Iteration 13/1000 | Loss: 0.00001815
Iteration 14/1000 | Loss: 0.00001815
Iteration 15/1000 | Loss: 0.00001798
Iteration 16/1000 | Loss: 0.00001794
Iteration 17/1000 | Loss: 0.00001791
Iteration 18/1000 | Loss: 0.00001788
Iteration 19/1000 | Loss: 0.00001787
Iteration 20/1000 | Loss: 0.00001786
Iteration 21/1000 | Loss: 0.00001785
Iteration 22/1000 | Loss: 0.00001784
Iteration 23/1000 | Loss: 0.00001783
Iteration 24/1000 | Loss: 0.00001782
Iteration 25/1000 | Loss: 0.00001782
Iteration 26/1000 | Loss: 0.00001781
Iteration 27/1000 | Loss: 0.00001780
Iteration 28/1000 | Loss: 0.00001779
Iteration 29/1000 | Loss: 0.00001779
Iteration 30/1000 | Loss: 0.00001776
Iteration 31/1000 | Loss: 0.00001776
Iteration 32/1000 | Loss: 0.00001776
Iteration 33/1000 | Loss: 0.00001776
Iteration 34/1000 | Loss: 0.00001776
Iteration 35/1000 | Loss: 0.00001776
Iteration 36/1000 | Loss: 0.00001776
Iteration 37/1000 | Loss: 0.00001776
Iteration 38/1000 | Loss: 0.00001776
Iteration 39/1000 | Loss: 0.00001776
Iteration 40/1000 | Loss: 0.00001776
Iteration 41/1000 | Loss: 0.00001775
Iteration 42/1000 | Loss: 0.00001775
Iteration 43/1000 | Loss: 0.00001775
Iteration 44/1000 | Loss: 0.00001775
Iteration 45/1000 | Loss: 0.00001775
Iteration 46/1000 | Loss: 0.00001775
Iteration 47/1000 | Loss: 0.00001775
Iteration 48/1000 | Loss: 0.00001775
Iteration 49/1000 | Loss: 0.00001775
Iteration 50/1000 | Loss: 0.00001775
Iteration 51/1000 | Loss: 0.00001774
Iteration 52/1000 | Loss: 0.00001773
Iteration 53/1000 | Loss: 0.00001773
Iteration 54/1000 | Loss: 0.00001772
Iteration 55/1000 | Loss: 0.00001772
Iteration 56/1000 | Loss: 0.00001771
Iteration 57/1000 | Loss: 0.00001771
Iteration 58/1000 | Loss: 0.00001771
Iteration 59/1000 | Loss: 0.00001770
Iteration 60/1000 | Loss: 0.00001770
Iteration 61/1000 | Loss: 0.00001770
Iteration 62/1000 | Loss: 0.00001770
Iteration 63/1000 | Loss: 0.00001769
Iteration 64/1000 | Loss: 0.00001769
Iteration 65/1000 | Loss: 0.00001769
Iteration 66/1000 | Loss: 0.00001769
Iteration 67/1000 | Loss: 0.00001768
Iteration 68/1000 | Loss: 0.00001768
Iteration 69/1000 | Loss: 0.00001768
Iteration 70/1000 | Loss: 0.00001768
Iteration 71/1000 | Loss: 0.00001768
Iteration 72/1000 | Loss: 0.00001767
Iteration 73/1000 | Loss: 0.00001767
Iteration 74/1000 | Loss: 0.00001766
Iteration 75/1000 | Loss: 0.00001766
Iteration 76/1000 | Loss: 0.00001766
Iteration 77/1000 | Loss: 0.00001766
Iteration 78/1000 | Loss: 0.00001765
Iteration 79/1000 | Loss: 0.00001765
Iteration 80/1000 | Loss: 0.00001765
Iteration 81/1000 | Loss: 0.00001765
Iteration 82/1000 | Loss: 0.00001764
Iteration 83/1000 | Loss: 0.00001764
Iteration 84/1000 | Loss: 0.00001764
Iteration 85/1000 | Loss: 0.00001764
Iteration 86/1000 | Loss: 0.00001763
Iteration 87/1000 | Loss: 0.00001763
Iteration 88/1000 | Loss: 0.00001763
Iteration 89/1000 | Loss: 0.00001763
Iteration 90/1000 | Loss: 0.00001763
Iteration 91/1000 | Loss: 0.00001763
Iteration 92/1000 | Loss: 0.00001763
Iteration 93/1000 | Loss: 0.00001762
Iteration 94/1000 | Loss: 0.00001762
Iteration 95/1000 | Loss: 0.00001762
Iteration 96/1000 | Loss: 0.00001762
Iteration 97/1000 | Loss: 0.00001762
Iteration 98/1000 | Loss: 0.00001762
Iteration 99/1000 | Loss: 0.00001762
Iteration 100/1000 | Loss: 0.00001762
Iteration 101/1000 | Loss: 0.00001762
Iteration 102/1000 | Loss: 0.00001762
Iteration 103/1000 | Loss: 0.00001762
Iteration 104/1000 | Loss: 0.00001762
Iteration 105/1000 | Loss: 0.00001761
Iteration 106/1000 | Loss: 0.00001761
Iteration 107/1000 | Loss: 0.00001761
Iteration 108/1000 | Loss: 0.00001761
Iteration 109/1000 | Loss: 0.00001761
Iteration 110/1000 | Loss: 0.00001761
Iteration 111/1000 | Loss: 0.00001761
Iteration 112/1000 | Loss: 0.00001761
Iteration 113/1000 | Loss: 0.00001761
Iteration 114/1000 | Loss: 0.00001761
Iteration 115/1000 | Loss: 0.00001760
Iteration 116/1000 | Loss: 0.00001760
Iteration 117/1000 | Loss: 0.00001760
Iteration 118/1000 | Loss: 0.00001760
Iteration 119/1000 | Loss: 0.00001760
Iteration 120/1000 | Loss: 0.00001760
Iteration 121/1000 | Loss: 0.00001760
Iteration 122/1000 | Loss: 0.00001760
Iteration 123/1000 | Loss: 0.00001760
Iteration 124/1000 | Loss: 0.00001760
Iteration 125/1000 | Loss: 0.00001760
Iteration 126/1000 | Loss: 0.00001760
Iteration 127/1000 | Loss: 0.00001760
Iteration 128/1000 | Loss: 0.00001759
Iteration 129/1000 | Loss: 0.00001759
Iteration 130/1000 | Loss: 0.00001759
Iteration 131/1000 | Loss: 0.00001759
Iteration 132/1000 | Loss: 0.00001759
Iteration 133/1000 | Loss: 0.00001759
Iteration 134/1000 | Loss: 0.00001759
Iteration 135/1000 | Loss: 0.00001759
Iteration 136/1000 | Loss: 0.00001759
Iteration 137/1000 | Loss: 0.00001759
Iteration 138/1000 | Loss: 0.00001759
Iteration 139/1000 | Loss: 0.00001759
Iteration 140/1000 | Loss: 0.00001759
Iteration 141/1000 | Loss: 0.00001759
Iteration 142/1000 | Loss: 0.00001759
Iteration 143/1000 | Loss: 0.00001758
Iteration 144/1000 | Loss: 0.00001758
Iteration 145/1000 | Loss: 0.00001758
Iteration 146/1000 | Loss: 0.00001758
Iteration 147/1000 | Loss: 0.00001758
Iteration 148/1000 | Loss: 0.00001758
Iteration 149/1000 | Loss: 0.00001758
Iteration 150/1000 | Loss: 0.00001758
Iteration 151/1000 | Loss: 0.00001758
Iteration 152/1000 | Loss: 0.00001758
Iteration 153/1000 | Loss: 0.00001758
Iteration 154/1000 | Loss: 0.00001758
Iteration 155/1000 | Loss: 0.00001758
Iteration 156/1000 | Loss: 0.00001758
Iteration 157/1000 | Loss: 0.00001758
Iteration 158/1000 | Loss: 0.00001758
Iteration 159/1000 | Loss: 0.00001758
Iteration 160/1000 | Loss: 0.00001758
Iteration 161/1000 | Loss: 0.00001758
Iteration 162/1000 | Loss: 0.00001758
Iteration 163/1000 | Loss: 0.00001758
Iteration 164/1000 | Loss: 0.00001758
Iteration 165/1000 | Loss: 0.00001758
Iteration 166/1000 | Loss: 0.00001758
Iteration 167/1000 | Loss: 0.00001758
Iteration 168/1000 | Loss: 0.00001758
Iteration 169/1000 | Loss: 0.00001758
Iteration 170/1000 | Loss: 0.00001758
Iteration 171/1000 | Loss: 0.00001758
Iteration 172/1000 | Loss: 0.00001758
Iteration 173/1000 | Loss: 0.00001758
Iteration 174/1000 | Loss: 0.00001758
Iteration 175/1000 | Loss: 0.00001758
Iteration 176/1000 | Loss: 0.00001758
Iteration 177/1000 | Loss: 0.00001758
Iteration 178/1000 | Loss: 0.00001758
Iteration 179/1000 | Loss: 0.00001758
Iteration 180/1000 | Loss: 0.00001758
Iteration 181/1000 | Loss: 0.00001758
Iteration 182/1000 | Loss: 0.00001758
Iteration 183/1000 | Loss: 0.00001758
Iteration 184/1000 | Loss: 0.00001758
Iteration 185/1000 | Loss: 0.00001758
Iteration 186/1000 | Loss: 0.00001758
Iteration 187/1000 | Loss: 0.00001758
Iteration 188/1000 | Loss: 0.00001758
Iteration 189/1000 | Loss: 0.00001758
Iteration 190/1000 | Loss: 0.00001758
Iteration 191/1000 | Loss: 0.00001758
Iteration 192/1000 | Loss: 0.00001758
Iteration 193/1000 | Loss: 0.00001758
Iteration 194/1000 | Loss: 0.00001758
Iteration 195/1000 | Loss: 0.00001758
Iteration 196/1000 | Loss: 0.00001758
Iteration 197/1000 | Loss: 0.00001758
Iteration 198/1000 | Loss: 0.00001758
Iteration 199/1000 | Loss: 0.00001758
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 199. Stopping optimization.
Last 5 losses: [1.7579170162207447e-05, 1.7579170162207447e-05, 1.7579170162207447e-05, 1.7579170162207447e-05, 1.7579170162207447e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7579170162207447e-05

Optimization complete. Final v2v error: 3.4987869262695312 mm

Highest mean error: 3.8532729148864746 mm for frame 0

Lowest mean error: 3.1293013095855713 mm for frame 45

Saving results

Total time: 41.20115351676941
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0006/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0006.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0006
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00945531
Iteration 2/25 | Loss: 0.00172055
Iteration 3/25 | Loss: 0.00129670
Iteration 4/25 | Loss: 0.00124084
Iteration 5/25 | Loss: 0.00120922
Iteration 6/25 | Loss: 0.00116444
Iteration 7/25 | Loss: 0.00113994
Iteration 8/25 | Loss: 0.00112096
Iteration 9/25 | Loss: 0.00110288
Iteration 10/25 | Loss: 0.00110226
Iteration 11/25 | Loss: 0.00109044
Iteration 12/25 | Loss: 0.00109622
Iteration 13/25 | Loss: 0.00109725
Iteration 14/25 | Loss: 0.00108580
Iteration 15/25 | Loss: 0.00107421
Iteration 16/25 | Loss: 0.00107053
Iteration 17/25 | Loss: 0.00106953
Iteration 18/25 | Loss: 0.00107311
Iteration 19/25 | Loss: 0.00107295
Iteration 20/25 | Loss: 0.00107278
Iteration 21/25 | Loss: 0.00106946
Iteration 22/25 | Loss: 0.00106834
Iteration 23/25 | Loss: 0.00106797
Iteration 24/25 | Loss: 0.00106788
Iteration 25/25 | Loss: 0.00106788

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 3.35533404
Iteration 2/25 | Loss: 0.00081233
Iteration 3/25 | Loss: 0.00081188
Iteration 4/25 | Loss: 0.00081188
Iteration 5/25 | Loss: 0.00081188
Iteration 6/25 | Loss: 0.00081188
Iteration 7/25 | Loss: 0.00081188
Iteration 8/25 | Loss: 0.00081188
Iteration 9/25 | Loss: 0.00081187
Iteration 10/25 | Loss: 0.00081187
Iteration 11/25 | Loss: 0.00081187
Iteration 12/25 | Loss: 0.00081187
Iteration 13/25 | Loss: 0.00081187
Iteration 14/25 | Loss: 0.00081187
Iteration 15/25 | Loss: 0.00081187
Iteration 16/25 | Loss: 0.00081187
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0008118745172396302, 0.0008118745172396302, 0.0008118745172396302, 0.0008118745172396302, 0.0008118745172396302]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0008118745172396302

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00081187
Iteration 2/1000 | Loss: 0.00033605
Iteration 3/1000 | Loss: 0.00033393
Iteration 4/1000 | Loss: 0.00024454
Iteration 5/1000 | Loss: 0.00018014
Iteration 6/1000 | Loss: 0.00013296
Iteration 7/1000 | Loss: 0.00009645
Iteration 8/1000 | Loss: 0.00008245
Iteration 9/1000 | Loss: 0.00007155
Iteration 10/1000 | Loss: 0.00014313
Iteration 11/1000 | Loss: 0.00011173
Iteration 12/1000 | Loss: 0.00014773
Iteration 13/1000 | Loss: 0.00011276
Iteration 14/1000 | Loss: 0.00009922
Iteration 15/1000 | Loss: 0.00006114
Iteration 16/1000 | Loss: 0.00004816
Iteration 17/1000 | Loss: 0.00006502
Iteration 18/1000 | Loss: 0.00004359
Iteration 19/1000 | Loss: 0.00003721
Iteration 20/1000 | Loss: 0.00003193
Iteration 21/1000 | Loss: 0.00002984
Iteration 22/1000 | Loss: 0.00002816
Iteration 23/1000 | Loss: 0.00002686
Iteration 24/1000 | Loss: 0.00002597
Iteration 25/1000 | Loss: 0.00002506
Iteration 26/1000 | Loss: 0.00002445
Iteration 27/1000 | Loss: 0.00002416
Iteration 28/1000 | Loss: 0.00002401
Iteration 29/1000 | Loss: 0.00002381
Iteration 30/1000 | Loss: 0.00002357
Iteration 31/1000 | Loss: 0.00002329
Iteration 32/1000 | Loss: 0.00002315
Iteration 33/1000 | Loss: 0.00002313
Iteration 34/1000 | Loss: 0.00002313
Iteration 35/1000 | Loss: 0.00002308
Iteration 36/1000 | Loss: 0.00002294
Iteration 37/1000 | Loss: 0.00002293
Iteration 38/1000 | Loss: 0.00002286
Iteration 39/1000 | Loss: 0.00002285
Iteration 40/1000 | Loss: 0.00002285
Iteration 41/1000 | Loss: 0.00002284
Iteration 42/1000 | Loss: 0.00002283
Iteration 43/1000 | Loss: 0.00002281
Iteration 44/1000 | Loss: 0.00002281
Iteration 45/1000 | Loss: 0.00002280
Iteration 46/1000 | Loss: 0.00002278
Iteration 47/1000 | Loss: 0.00002278
Iteration 48/1000 | Loss: 0.00002278
Iteration 49/1000 | Loss: 0.00002277
Iteration 50/1000 | Loss: 0.00002277
Iteration 51/1000 | Loss: 0.00002277
Iteration 52/1000 | Loss: 0.00002276
Iteration 53/1000 | Loss: 0.00002276
Iteration 54/1000 | Loss: 0.00002276
Iteration 55/1000 | Loss: 0.00002275
Iteration 56/1000 | Loss: 0.00002275
Iteration 57/1000 | Loss: 0.00002275
Iteration 58/1000 | Loss: 0.00002275
Iteration 59/1000 | Loss: 0.00002274
Iteration 60/1000 | Loss: 0.00002274
Iteration 61/1000 | Loss: 0.00002274
Iteration 62/1000 | Loss: 0.00002274
Iteration 63/1000 | Loss: 0.00002273
Iteration 64/1000 | Loss: 0.00002273
Iteration 65/1000 | Loss: 0.00002273
Iteration 66/1000 | Loss: 0.00002273
Iteration 67/1000 | Loss: 0.00002273
Iteration 68/1000 | Loss: 0.00002272
Iteration 69/1000 | Loss: 0.00002272
Iteration 70/1000 | Loss: 0.00002272
Iteration 71/1000 | Loss: 0.00002272
Iteration 72/1000 | Loss: 0.00002272
Iteration 73/1000 | Loss: 0.00002272
Iteration 74/1000 | Loss: 0.00002272
Iteration 75/1000 | Loss: 0.00002272
Iteration 76/1000 | Loss: 0.00002272
Iteration 77/1000 | Loss: 0.00002271
Iteration 78/1000 | Loss: 0.00002271
Iteration 79/1000 | Loss: 0.00002271
Iteration 80/1000 | Loss: 0.00002271
Iteration 81/1000 | Loss: 0.00002271
Iteration 82/1000 | Loss: 0.00002271
Iteration 83/1000 | Loss: 0.00002270
Iteration 84/1000 | Loss: 0.00002270
Iteration 85/1000 | Loss: 0.00002270
Iteration 86/1000 | Loss: 0.00002269
Iteration 87/1000 | Loss: 0.00002269
Iteration 88/1000 | Loss: 0.00002269
Iteration 89/1000 | Loss: 0.00002269
Iteration 90/1000 | Loss: 0.00002269
Iteration 91/1000 | Loss: 0.00002269
Iteration 92/1000 | Loss: 0.00002269
Iteration 93/1000 | Loss: 0.00002268
Iteration 94/1000 | Loss: 0.00002268
Iteration 95/1000 | Loss: 0.00002268
Iteration 96/1000 | Loss: 0.00002268
Iteration 97/1000 | Loss: 0.00002267
Iteration 98/1000 | Loss: 0.00002267
Iteration 99/1000 | Loss: 0.00002267
Iteration 100/1000 | Loss: 0.00002267
Iteration 101/1000 | Loss: 0.00002267
Iteration 102/1000 | Loss: 0.00002267
Iteration 103/1000 | Loss: 0.00002266
Iteration 104/1000 | Loss: 0.00002266
Iteration 105/1000 | Loss: 0.00002266
Iteration 106/1000 | Loss: 0.00002266
Iteration 107/1000 | Loss: 0.00002266
Iteration 108/1000 | Loss: 0.00002266
Iteration 109/1000 | Loss: 0.00002266
Iteration 110/1000 | Loss: 0.00002266
Iteration 111/1000 | Loss: 0.00002266
Iteration 112/1000 | Loss: 0.00002266
Iteration 113/1000 | Loss: 0.00002265
Iteration 114/1000 | Loss: 0.00002265
Iteration 115/1000 | Loss: 0.00002265
Iteration 116/1000 | Loss: 0.00002265
Iteration 117/1000 | Loss: 0.00002265
Iteration 118/1000 | Loss: 0.00002265
Iteration 119/1000 | Loss: 0.00002265
Iteration 120/1000 | Loss: 0.00002265
Iteration 121/1000 | Loss: 0.00002264
Iteration 122/1000 | Loss: 0.00002264
Iteration 123/1000 | Loss: 0.00002264
Iteration 124/1000 | Loss: 0.00002264
Iteration 125/1000 | Loss: 0.00002264
Iteration 126/1000 | Loss: 0.00002264
Iteration 127/1000 | Loss: 0.00002264
Iteration 128/1000 | Loss: 0.00002264
Iteration 129/1000 | Loss: 0.00002263
Iteration 130/1000 | Loss: 0.00002263
Iteration 131/1000 | Loss: 0.00002263
Iteration 132/1000 | Loss: 0.00002263
Iteration 133/1000 | Loss: 0.00002263
Iteration 134/1000 | Loss: 0.00002263
Iteration 135/1000 | Loss: 0.00002263
Iteration 136/1000 | Loss: 0.00002263
Iteration 137/1000 | Loss: 0.00002262
Iteration 138/1000 | Loss: 0.00002262
Iteration 139/1000 | Loss: 0.00002262
Iteration 140/1000 | Loss: 0.00002262
Iteration 141/1000 | Loss: 0.00002262
Iteration 142/1000 | Loss: 0.00002262
Iteration 143/1000 | Loss: 0.00002262
Iteration 144/1000 | Loss: 0.00002262
Iteration 145/1000 | Loss: 0.00002262
Iteration 146/1000 | Loss: 0.00002261
Iteration 147/1000 | Loss: 0.00002261
Iteration 148/1000 | Loss: 0.00002261
Iteration 149/1000 | Loss: 0.00002261
Iteration 150/1000 | Loss: 0.00002261
Iteration 151/1000 | Loss: 0.00002261
Iteration 152/1000 | Loss: 0.00002261
Iteration 153/1000 | Loss: 0.00002261
Iteration 154/1000 | Loss: 0.00002261
Iteration 155/1000 | Loss: 0.00002260
Iteration 156/1000 | Loss: 0.00002260
Iteration 157/1000 | Loss: 0.00002260
Iteration 158/1000 | Loss: 0.00002260
Iteration 159/1000 | Loss: 0.00002260
Iteration 160/1000 | Loss: 0.00002260
Iteration 161/1000 | Loss: 0.00002260
Iteration 162/1000 | Loss: 0.00002260
Iteration 163/1000 | Loss: 0.00002260
Iteration 164/1000 | Loss: 0.00002260
Iteration 165/1000 | Loss: 0.00002260
Iteration 166/1000 | Loss: 0.00002260
Iteration 167/1000 | Loss: 0.00002260
Iteration 168/1000 | Loss: 0.00002260
Iteration 169/1000 | Loss: 0.00002259
Iteration 170/1000 | Loss: 0.00002259
Iteration 171/1000 | Loss: 0.00002259
Iteration 172/1000 | Loss: 0.00002259
Iteration 173/1000 | Loss: 0.00002259
Iteration 174/1000 | Loss: 0.00002259
Iteration 175/1000 | Loss: 0.00002259
Iteration 176/1000 | Loss: 0.00002259
Iteration 177/1000 | Loss: 0.00002259
Iteration 178/1000 | Loss: 0.00002259
Iteration 179/1000 | Loss: 0.00002259
Iteration 180/1000 | Loss: 0.00002259
Iteration 181/1000 | Loss: 0.00002259
Iteration 182/1000 | Loss: 0.00002259
Iteration 183/1000 | Loss: 0.00002258
Iteration 184/1000 | Loss: 0.00002258
Iteration 185/1000 | Loss: 0.00002258
Iteration 186/1000 | Loss: 0.00002258
Iteration 187/1000 | Loss: 0.00002258
Iteration 188/1000 | Loss: 0.00002258
Iteration 189/1000 | Loss: 0.00002258
Iteration 190/1000 | Loss: 0.00002258
Iteration 191/1000 | Loss: 0.00002258
Iteration 192/1000 | Loss: 0.00002258
Iteration 193/1000 | Loss: 0.00002258
Iteration 194/1000 | Loss: 0.00002258
Iteration 195/1000 | Loss: 0.00002258
Iteration 196/1000 | Loss: 0.00002258
Iteration 197/1000 | Loss: 0.00002258
Iteration 198/1000 | Loss: 0.00002258
Iteration 199/1000 | Loss: 0.00002258
Iteration 200/1000 | Loss: 0.00002258
Iteration 201/1000 | Loss: 0.00002258
Iteration 202/1000 | Loss: 0.00002258
Iteration 203/1000 | Loss: 0.00002258
Iteration 204/1000 | Loss: 0.00002258
Iteration 205/1000 | Loss: 0.00002258
Iteration 206/1000 | Loss: 0.00002258
Iteration 207/1000 | Loss: 0.00002258
Iteration 208/1000 | Loss: 0.00002258
Iteration 209/1000 | Loss: 0.00002258
Iteration 210/1000 | Loss: 0.00002258
Iteration 211/1000 | Loss: 0.00002258
Iteration 212/1000 | Loss: 0.00002258
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 212. Stopping optimization.
Last 5 losses: [2.25836720346706e-05, 2.25836720346706e-05, 2.25836720346706e-05, 2.25836720346706e-05, 2.25836720346706e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.25836720346706e-05

Optimization complete. Final v2v error: 3.6819956302642822 mm

Highest mean error: 6.273571968078613 mm for frame 109

Lowest mean error: 2.766845941543579 mm for frame 73

Saving results

Total time: 105.02393770217896
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0024/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0024.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0024
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00986358
Iteration 2/25 | Loss: 0.00265392
Iteration 3/25 | Loss: 0.00205412
Iteration 4/25 | Loss: 0.00184493
Iteration 5/25 | Loss: 0.00169820
Iteration 6/25 | Loss: 0.00169787
Iteration 7/25 | Loss: 0.00189773
Iteration 8/25 | Loss: 0.00149577
Iteration 9/25 | Loss: 0.00140701
Iteration 10/25 | Loss: 0.00138600
Iteration 11/25 | Loss: 0.00136551
Iteration 12/25 | Loss: 0.00135254
Iteration 13/25 | Loss: 0.00133266
Iteration 14/25 | Loss: 0.00133439
Iteration 15/25 | Loss: 0.00131820
Iteration 16/25 | Loss: 0.00131344
Iteration 17/25 | Loss: 0.00130649
Iteration 18/25 | Loss: 0.00130203
Iteration 19/25 | Loss: 0.00130319
Iteration 20/25 | Loss: 0.00129978
Iteration 21/25 | Loss: 0.00130092
Iteration 22/25 | Loss: 0.00130371
Iteration 23/25 | Loss: 0.00130353
Iteration 24/25 | Loss: 0.00129762
Iteration 25/25 | Loss: 0.00129923

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.26627600
Iteration 2/25 | Loss: 0.00197124
Iteration 3/25 | Loss: 0.00197124
Iteration 4/25 | Loss: 0.00197124
Iteration 5/25 | Loss: 0.00197124
Iteration 6/25 | Loss: 0.00197124
Iteration 7/25 | Loss: 0.00197124
Iteration 8/25 | Loss: 0.00197124
Iteration 9/25 | Loss: 0.00197124
Iteration 10/25 | Loss: 0.00197124
Iteration 11/25 | Loss: 0.00197124
Iteration 12/25 | Loss: 0.00197124
Iteration 13/25 | Loss: 0.00197124
Iteration 14/25 | Loss: 0.00197124
Iteration 15/25 | Loss: 0.00197124
Iteration 16/25 | Loss: 0.00197124
Iteration 17/25 | Loss: 0.00197124
Iteration 18/25 | Loss: 0.00197124
Iteration 19/25 | Loss: 0.00197124
Iteration 20/25 | Loss: 0.00197124
Iteration 21/25 | Loss: 0.00197124
Iteration 22/25 | Loss: 0.00197124
Iteration 23/25 | Loss: 0.00197124
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 23. Stopping optimization.
Last 5 losses: [0.0019712354987859726, 0.0019712354987859726, 0.0019712354987859726, 0.0019712354987859726, 0.0019712354987859726]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0019712354987859726

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00197124
Iteration 2/1000 | Loss: 0.00023616
Iteration 3/1000 | Loss: 0.00023075
Iteration 4/1000 | Loss: 0.00084143
Iteration 5/1000 | Loss: 0.00020264
Iteration 6/1000 | Loss: 0.00020758
Iteration 7/1000 | Loss: 0.00018866
Iteration 8/1000 | Loss: 0.00018124
Iteration 9/1000 | Loss: 0.00023921
Iteration 10/1000 | Loss: 0.00022021
Iteration 11/1000 | Loss: 0.00019106
Iteration 12/1000 | Loss: 0.00059440
Iteration 13/1000 | Loss: 0.00016339
Iteration 14/1000 | Loss: 0.00011584
Iteration 15/1000 | Loss: 0.00012848
Iteration 16/1000 | Loss: 0.00015371
Iteration 17/1000 | Loss: 0.00015197
Iteration 18/1000 | Loss: 0.00014275
Iteration 19/1000 | Loss: 0.00014790
Iteration 20/1000 | Loss: 0.00015794
Iteration 21/1000 | Loss: 0.00025162
Iteration 22/1000 | Loss: 0.00079482
Iteration 23/1000 | Loss: 0.00065219
Iteration 24/1000 | Loss: 0.00020739
Iteration 25/1000 | Loss: 0.00018108
Iteration 26/1000 | Loss: 0.00012416
Iteration 27/1000 | Loss: 0.00013969
Iteration 28/1000 | Loss: 0.00012147
Iteration 29/1000 | Loss: 0.00034679
Iteration 30/1000 | Loss: 0.00020032
Iteration 31/1000 | Loss: 0.00007964
Iteration 32/1000 | Loss: 0.00007324
Iteration 33/1000 | Loss: 0.00006951
Iteration 34/1000 | Loss: 0.00006629
Iteration 35/1000 | Loss: 0.00006425
Iteration 36/1000 | Loss: 0.00006310
Iteration 37/1000 | Loss: 0.00006175
Iteration 38/1000 | Loss: 0.00006069
Iteration 39/1000 | Loss: 0.00005982
Iteration 40/1000 | Loss: 0.00005912
Iteration 41/1000 | Loss: 0.00005854
Iteration 42/1000 | Loss: 0.00005811
Iteration 43/1000 | Loss: 0.00005778
Iteration 44/1000 | Loss: 0.00015275
Iteration 45/1000 | Loss: 0.00096795
Iteration 46/1000 | Loss: 0.00011013
Iteration 47/1000 | Loss: 0.00007742
Iteration 48/1000 | Loss: 0.00006361
Iteration 49/1000 | Loss: 0.00005113
Iteration 50/1000 | Loss: 0.00004255
Iteration 51/1000 | Loss: 0.00003776
Iteration 52/1000 | Loss: 0.00003595
Iteration 53/1000 | Loss: 0.00003453
Iteration 54/1000 | Loss: 0.00003362
Iteration 55/1000 | Loss: 0.00003296
Iteration 56/1000 | Loss: 0.00003232
Iteration 57/1000 | Loss: 0.00003170
Iteration 58/1000 | Loss: 0.00003129
Iteration 59/1000 | Loss: 0.00003095
Iteration 60/1000 | Loss: 0.00003072
Iteration 61/1000 | Loss: 0.00003051
Iteration 62/1000 | Loss: 0.00003049
Iteration 63/1000 | Loss: 0.00003048
Iteration 64/1000 | Loss: 0.00003048
Iteration 65/1000 | Loss: 0.00003046
Iteration 66/1000 | Loss: 0.00003045
Iteration 67/1000 | Loss: 0.00003044
Iteration 68/1000 | Loss: 0.00003044
Iteration 69/1000 | Loss: 0.00003044
Iteration 70/1000 | Loss: 0.00003043
Iteration 71/1000 | Loss: 0.00003043
Iteration 72/1000 | Loss: 0.00003043
Iteration 73/1000 | Loss: 0.00003043
Iteration 74/1000 | Loss: 0.00003042
Iteration 75/1000 | Loss: 0.00003040
Iteration 76/1000 | Loss: 0.00003040
Iteration 77/1000 | Loss: 0.00003039
Iteration 78/1000 | Loss: 0.00003039
Iteration 79/1000 | Loss: 0.00003039
Iteration 80/1000 | Loss: 0.00003039
Iteration 81/1000 | Loss: 0.00003039
Iteration 82/1000 | Loss: 0.00003039
Iteration 83/1000 | Loss: 0.00003037
Iteration 84/1000 | Loss: 0.00003037
Iteration 85/1000 | Loss: 0.00003036
Iteration 86/1000 | Loss: 0.00003036
Iteration 87/1000 | Loss: 0.00003036
Iteration 88/1000 | Loss: 0.00003035
Iteration 89/1000 | Loss: 0.00003035
Iteration 90/1000 | Loss: 0.00003035
Iteration 91/1000 | Loss: 0.00003035
Iteration 92/1000 | Loss: 0.00003034
Iteration 93/1000 | Loss: 0.00003034
Iteration 94/1000 | Loss: 0.00003033
Iteration 95/1000 | Loss: 0.00003032
Iteration 96/1000 | Loss: 0.00003032
Iteration 97/1000 | Loss: 0.00003032
Iteration 98/1000 | Loss: 0.00003032
Iteration 99/1000 | Loss: 0.00003032
Iteration 100/1000 | Loss: 0.00003032
Iteration 101/1000 | Loss: 0.00003032
Iteration 102/1000 | Loss: 0.00003032
Iteration 103/1000 | Loss: 0.00003032
Iteration 104/1000 | Loss: 0.00003032
Iteration 105/1000 | Loss: 0.00003032
Iteration 106/1000 | Loss: 0.00003032
Iteration 107/1000 | Loss: 0.00003031
Iteration 108/1000 | Loss: 0.00003031
Iteration 109/1000 | Loss: 0.00003031
Iteration 110/1000 | Loss: 0.00003031
Iteration 111/1000 | Loss: 0.00003031
Iteration 112/1000 | Loss: 0.00003031
Iteration 113/1000 | Loss: 0.00003031
Iteration 114/1000 | Loss: 0.00003030
Iteration 115/1000 | Loss: 0.00003030
Iteration 116/1000 | Loss: 0.00003030
Iteration 117/1000 | Loss: 0.00003030
Iteration 118/1000 | Loss: 0.00003030
Iteration 119/1000 | Loss: 0.00003030
Iteration 120/1000 | Loss: 0.00003029
Iteration 121/1000 | Loss: 0.00003029
Iteration 122/1000 | Loss: 0.00003029
Iteration 123/1000 | Loss: 0.00003028
Iteration 124/1000 | Loss: 0.00003028
Iteration 125/1000 | Loss: 0.00003028
Iteration 126/1000 | Loss: 0.00003028
Iteration 127/1000 | Loss: 0.00003028
Iteration 128/1000 | Loss: 0.00003028
Iteration 129/1000 | Loss: 0.00003028
Iteration 130/1000 | Loss: 0.00003028
Iteration 131/1000 | Loss: 0.00003028
Iteration 132/1000 | Loss: 0.00003028
Iteration 133/1000 | Loss: 0.00003027
Iteration 134/1000 | Loss: 0.00003027
Iteration 135/1000 | Loss: 0.00003027
Iteration 136/1000 | Loss: 0.00003027
Iteration 137/1000 | Loss: 0.00003027
Iteration 138/1000 | Loss: 0.00003027
Iteration 139/1000 | Loss: 0.00003027
Iteration 140/1000 | Loss: 0.00003027
Iteration 141/1000 | Loss: 0.00003027
Iteration 142/1000 | Loss: 0.00003027
Iteration 143/1000 | Loss: 0.00003027
Iteration 144/1000 | Loss: 0.00003027
Iteration 145/1000 | Loss: 0.00003027
Iteration 146/1000 | Loss: 0.00003027
Iteration 147/1000 | Loss: 0.00003027
Iteration 148/1000 | Loss: 0.00003027
Iteration 149/1000 | Loss: 0.00003027
Iteration 150/1000 | Loss: 0.00003027
Iteration 151/1000 | Loss: 0.00003027
Iteration 152/1000 | Loss: 0.00003027
Iteration 153/1000 | Loss: 0.00003027
Iteration 154/1000 | Loss: 0.00003027
Iteration 155/1000 | Loss: 0.00003027
Iteration 156/1000 | Loss: 0.00003027
Iteration 157/1000 | Loss: 0.00003027
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [3.027079401363153e-05, 3.027079401363153e-05, 3.027079401363153e-05, 3.027079401363153e-05, 3.027079401363153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.027079401363153e-05

Optimization complete. Final v2v error: 4.281883239746094 mm

Highest mean error: 10.453954696655273 mm for frame 3

Lowest mean error: 3.6850922107696533 mm for frame 175

Saving results

Total time: 144.54028868675232
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00616073
Iteration 2/25 | Loss: 0.00160577
Iteration 3/25 | Loss: 0.00119037
Iteration 4/25 | Loss: 0.00116975
Iteration 5/25 | Loss: 0.00116296
Iteration 6/25 | Loss: 0.00116018
Iteration 7/25 | Loss: 0.00115962
Iteration 8/25 | Loss: 0.00115957
Iteration 9/25 | Loss: 0.00115957
Iteration 10/25 | Loss: 0.00115957
Iteration 11/25 | Loss: 0.00115957
Iteration 12/25 | Loss: 0.00115957
Iteration 13/25 | Loss: 0.00115957
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 13. Stopping optimization.
Last 5 losses: [0.0011595700634643435, 0.0011595700634643435, 0.0011595700634643435, 0.0011595700634643435, 0.0011595700634643435]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011595700634643435

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.88471168
Iteration 2/25 | Loss: 0.00088167
Iteration 3/25 | Loss: 0.00088167
Iteration 4/25 | Loss: 0.00088166
Iteration 5/25 | Loss: 0.00088166
Iteration 6/25 | Loss: 0.00088166
Iteration 7/25 | Loss: 0.00088166
Iteration 8/25 | Loss: 0.00088166
Iteration 9/25 | Loss: 0.00088166
Iteration 10/25 | Loss: 0.00088166
Iteration 11/25 | Loss: 0.00088166
Iteration 12/25 | Loss: 0.00088166
Iteration 13/25 | Loss: 0.00088166
Iteration 14/25 | Loss: 0.00088166
Iteration 15/25 | Loss: 0.00088166
Iteration 16/25 | Loss: 0.00088166
Iteration 17/25 | Loss: 0.00088166
Iteration 18/25 | Loss: 0.00088166
Iteration 19/25 | Loss: 0.00088166
Iteration 20/25 | Loss: 0.00088166
Iteration 21/25 | Loss: 0.00088166
Iteration 22/25 | Loss: 0.00088166
Iteration 23/25 | Loss: 0.00088166
Iteration 24/25 | Loss: 0.00088166
Iteration 25/25 | Loss: 0.00088166

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088166
Iteration 2/1000 | Loss: 0.00007035
Iteration 3/1000 | Loss: 0.00004801
Iteration 4/1000 | Loss: 0.00004207
Iteration 5/1000 | Loss: 0.00003924
Iteration 6/1000 | Loss: 0.00003785
Iteration 7/1000 | Loss: 0.00003664
Iteration 8/1000 | Loss: 0.00003587
Iteration 9/1000 | Loss: 0.00003541
Iteration 10/1000 | Loss: 0.00003496
Iteration 11/1000 | Loss: 0.00003457
Iteration 12/1000 | Loss: 0.00003430
Iteration 13/1000 | Loss: 0.00003410
Iteration 14/1000 | Loss: 0.00003393
Iteration 15/1000 | Loss: 0.00003377
Iteration 16/1000 | Loss: 0.00003355
Iteration 17/1000 | Loss: 0.00003339
Iteration 18/1000 | Loss: 0.00003331
Iteration 19/1000 | Loss: 0.00003330
Iteration 20/1000 | Loss: 0.00003319
Iteration 21/1000 | Loss: 0.00003311
Iteration 22/1000 | Loss: 0.00003310
Iteration 23/1000 | Loss: 0.00003309
Iteration 24/1000 | Loss: 0.00003308
Iteration 25/1000 | Loss: 0.00003307
Iteration 26/1000 | Loss: 0.00003306
Iteration 27/1000 | Loss: 0.00003306
Iteration 28/1000 | Loss: 0.00003305
Iteration 29/1000 | Loss: 0.00003305
Iteration 30/1000 | Loss: 0.00003305
Iteration 31/1000 | Loss: 0.00003304
Iteration 32/1000 | Loss: 0.00003304
Iteration 33/1000 | Loss: 0.00003304
Iteration 34/1000 | Loss: 0.00003303
Iteration 35/1000 | Loss: 0.00003303
Iteration 36/1000 | Loss: 0.00003301
Iteration 37/1000 | Loss: 0.00003301
Iteration 38/1000 | Loss: 0.00003301
Iteration 39/1000 | Loss: 0.00003301
Iteration 40/1000 | Loss: 0.00003301
Iteration 41/1000 | Loss: 0.00003301
Iteration 42/1000 | Loss: 0.00003301
Iteration 43/1000 | Loss: 0.00003300
Iteration 44/1000 | Loss: 0.00003300
Iteration 45/1000 | Loss: 0.00003300
Iteration 46/1000 | Loss: 0.00003300
Iteration 47/1000 | Loss: 0.00003300
Iteration 48/1000 | Loss: 0.00003300
Iteration 49/1000 | Loss: 0.00003300
Iteration 50/1000 | Loss: 0.00003299
Iteration 51/1000 | Loss: 0.00003298
Iteration 52/1000 | Loss: 0.00003298
Iteration 53/1000 | Loss: 0.00003298
Iteration 54/1000 | Loss: 0.00003297
Iteration 55/1000 | Loss: 0.00003297
Iteration 56/1000 | Loss: 0.00003296
Iteration 57/1000 | Loss: 0.00003295
Iteration 58/1000 | Loss: 0.00003295
Iteration 59/1000 | Loss: 0.00003294
Iteration 60/1000 | Loss: 0.00003294
Iteration 61/1000 | Loss: 0.00003293
Iteration 62/1000 | Loss: 0.00003292
Iteration 63/1000 | Loss: 0.00003292
Iteration 64/1000 | Loss: 0.00003292
Iteration 65/1000 | Loss: 0.00003292
Iteration 66/1000 | Loss: 0.00003292
Iteration 67/1000 | Loss: 0.00003292
Iteration 68/1000 | Loss: 0.00003292
Iteration 69/1000 | Loss: 0.00003292
Iteration 70/1000 | Loss: 0.00003291
Iteration 71/1000 | Loss: 0.00003291
Iteration 72/1000 | Loss: 0.00003291
Iteration 73/1000 | Loss: 0.00003291
Iteration 74/1000 | Loss: 0.00003291
Iteration 75/1000 | Loss: 0.00003291
Iteration 76/1000 | Loss: 0.00003290
Iteration 77/1000 | Loss: 0.00003290
Iteration 78/1000 | Loss: 0.00003290
Iteration 79/1000 | Loss: 0.00003290
Iteration 80/1000 | Loss: 0.00003290
Iteration 81/1000 | Loss: 0.00003290
Iteration 82/1000 | Loss: 0.00003289
Iteration 83/1000 | Loss: 0.00003289
Iteration 84/1000 | Loss: 0.00003289
Iteration 85/1000 | Loss: 0.00003289
Iteration 86/1000 | Loss: 0.00003289
Iteration 87/1000 | Loss: 0.00003289
Iteration 88/1000 | Loss: 0.00003289
Iteration 89/1000 | Loss: 0.00003289
Iteration 90/1000 | Loss: 0.00003288
Iteration 91/1000 | Loss: 0.00003288
Iteration 92/1000 | Loss: 0.00003288
Iteration 93/1000 | Loss: 0.00003288
Iteration 94/1000 | Loss: 0.00003288
Iteration 95/1000 | Loss: 0.00003288
Iteration 96/1000 | Loss: 0.00003288
Iteration 97/1000 | Loss: 0.00003288
Iteration 98/1000 | Loss: 0.00003288
Iteration 99/1000 | Loss: 0.00003288
Iteration 100/1000 | Loss: 0.00003287
Iteration 101/1000 | Loss: 0.00003287
Iteration 102/1000 | Loss: 0.00003287
Iteration 103/1000 | Loss: 0.00003287
Iteration 104/1000 | Loss: 0.00003287
Iteration 105/1000 | Loss: 0.00003287
Iteration 106/1000 | Loss: 0.00003287
Iteration 107/1000 | Loss: 0.00003287
Iteration 108/1000 | Loss: 0.00003287
Iteration 109/1000 | Loss: 0.00003287
Iteration 110/1000 | Loss: 0.00003287
Iteration 111/1000 | Loss: 0.00003287
Iteration 112/1000 | Loss: 0.00003287
Iteration 113/1000 | Loss: 0.00003286
Iteration 114/1000 | Loss: 0.00003286
Iteration 115/1000 | Loss: 0.00003286
Iteration 116/1000 | Loss: 0.00003286
Iteration 117/1000 | Loss: 0.00003286
Iteration 118/1000 | Loss: 0.00003286
Iteration 119/1000 | Loss: 0.00003286
Iteration 120/1000 | Loss: 0.00003286
Iteration 121/1000 | Loss: 0.00003286
Iteration 122/1000 | Loss: 0.00003286
Iteration 123/1000 | Loss: 0.00003286
Iteration 124/1000 | Loss: 0.00003285
Iteration 125/1000 | Loss: 0.00003285
Iteration 126/1000 | Loss: 0.00003285
Iteration 127/1000 | Loss: 0.00003285
Iteration 128/1000 | Loss: 0.00003285
Iteration 129/1000 | Loss: 0.00003285
Iteration 130/1000 | Loss: 0.00003285
Iteration 131/1000 | Loss: 0.00003285
Iteration 132/1000 | Loss: 0.00003285
Iteration 133/1000 | Loss: 0.00003285
Iteration 134/1000 | Loss: 0.00003285
Iteration 135/1000 | Loss: 0.00003285
Iteration 136/1000 | Loss: 0.00003285
Iteration 137/1000 | Loss: 0.00003285
Iteration 138/1000 | Loss: 0.00003285
Iteration 139/1000 | Loss: 0.00003285
Iteration 140/1000 | Loss: 0.00003285
Iteration 141/1000 | Loss: 0.00003285
Iteration 142/1000 | Loss: 0.00003285
Iteration 143/1000 | Loss: 0.00003285
Iteration 144/1000 | Loss: 0.00003285
Iteration 145/1000 | Loss: 0.00003285
Iteration 146/1000 | Loss: 0.00003285
Iteration 147/1000 | Loss: 0.00003285
Iteration 148/1000 | Loss: 0.00003285
Iteration 149/1000 | Loss: 0.00003285
Iteration 150/1000 | Loss: 0.00003285
Iteration 151/1000 | Loss: 0.00003285
Iteration 152/1000 | Loss: 0.00003285
Iteration 153/1000 | Loss: 0.00003285
Iteration 154/1000 | Loss: 0.00003285
Iteration 155/1000 | Loss: 0.00003285
Iteration 156/1000 | Loss: 0.00003285
Iteration 157/1000 | Loss: 0.00003285
Iteration 158/1000 | Loss: 0.00003285
Iteration 159/1000 | Loss: 0.00003285
Iteration 160/1000 | Loss: 0.00003285
Iteration 161/1000 | Loss: 0.00003285
Iteration 162/1000 | Loss: 0.00003285
Iteration 163/1000 | Loss: 0.00003285
Iteration 164/1000 | Loss: 0.00003285
Iteration 165/1000 | Loss: 0.00003285
Iteration 166/1000 | Loss: 0.00003285
Iteration 167/1000 | Loss: 0.00003285
Iteration 168/1000 | Loss: 0.00003285
Iteration 169/1000 | Loss: 0.00003285
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 169. Stopping optimization.
Last 5 losses: [3.284871854702942e-05, 3.284871854702942e-05, 3.284871854702942e-05, 3.284871854702942e-05, 3.284871854702942e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.284871854702942e-05

Optimization complete. Final v2v error: 4.429963111877441 mm

Highest mean error: 5.620702266693115 mm for frame 117

Lowest mean error: 3.268146276473999 mm for frame 6

Saving results

Total time: 46.489657163619995
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00471074
Iteration 2/25 | Loss: 0.00136726
Iteration 3/25 | Loss: 0.00107281
Iteration 4/25 | Loss: 0.00103902
Iteration 5/25 | Loss: 0.00103143
Iteration 6/25 | Loss: 0.00102968
Iteration 7/25 | Loss: 0.00102905
Iteration 8/25 | Loss: 0.00102905
Iteration 9/25 | Loss: 0.00102905
Iteration 10/25 | Loss: 0.00102905
Iteration 11/25 | Loss: 0.00102905
Iteration 12/25 | Loss: 0.00102905
Iteration 13/25 | Loss: 0.00102905
Iteration 14/25 | Loss: 0.00102905
Iteration 15/25 | Loss: 0.00102905
Iteration 16/25 | Loss: 0.00102905
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 16. Stopping optimization.
Last 5 losses: [0.0010290455538779497, 0.0010290455538779497, 0.0010290455538779497, 0.0010290455538779497, 0.0010290455538779497]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010290455538779497

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.17584419
Iteration 2/25 | Loss: 0.00101379
Iteration 3/25 | Loss: 0.00101378
Iteration 4/25 | Loss: 0.00101378
Iteration 5/25 | Loss: 0.00101378
Iteration 6/25 | Loss: 0.00101378
Iteration 7/25 | Loss: 0.00101378
Iteration 8/25 | Loss: 0.00101378
Iteration 9/25 | Loss: 0.00101378
Iteration 10/25 | Loss: 0.00101378
Iteration 11/25 | Loss: 0.00101378
Iteration 12/25 | Loss: 0.00101378
Iteration 13/25 | Loss: 0.00101378
Iteration 14/25 | Loss: 0.00101378
Iteration 15/25 | Loss: 0.00101378
Iteration 16/25 | Loss: 0.00101378
Iteration 17/25 | Loss: 0.00101378
Iteration 18/25 | Loss: 0.00101378
Iteration 19/25 | Loss: 0.00101378
Iteration 20/25 | Loss: 0.00101378
Iteration 21/25 | Loss: 0.00101378
Iteration 22/25 | Loss: 0.00101378
Iteration 23/25 | Loss: 0.00101378
Iteration 24/25 | Loss: 0.00101378
Iteration 25/25 | Loss: 0.00101378

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00101378
Iteration 2/1000 | Loss: 0.00006910
Iteration 3/1000 | Loss: 0.00004268
Iteration 4/1000 | Loss: 0.00003382
Iteration 5/1000 | Loss: 0.00002979
Iteration 6/1000 | Loss: 0.00002751
Iteration 7/1000 | Loss: 0.00002607
Iteration 8/1000 | Loss: 0.00002515
Iteration 9/1000 | Loss: 0.00002432
Iteration 10/1000 | Loss: 0.00002392
Iteration 11/1000 | Loss: 0.00002366
Iteration 12/1000 | Loss: 0.00002337
Iteration 13/1000 | Loss: 0.00002320
Iteration 14/1000 | Loss: 0.00002313
Iteration 15/1000 | Loss: 0.00002312
Iteration 16/1000 | Loss: 0.00002312
Iteration 17/1000 | Loss: 0.00002311
Iteration 18/1000 | Loss: 0.00002310
Iteration 19/1000 | Loss: 0.00002303
Iteration 20/1000 | Loss: 0.00002303
Iteration 21/1000 | Loss: 0.00002295
Iteration 22/1000 | Loss: 0.00002292
Iteration 23/1000 | Loss: 0.00002292
Iteration 24/1000 | Loss: 0.00002288
Iteration 25/1000 | Loss: 0.00002288
Iteration 26/1000 | Loss: 0.00002287
Iteration 27/1000 | Loss: 0.00002287
Iteration 28/1000 | Loss: 0.00002287
Iteration 29/1000 | Loss: 0.00002286
Iteration 30/1000 | Loss: 0.00002286
Iteration 31/1000 | Loss: 0.00002283
Iteration 32/1000 | Loss: 0.00002283
Iteration 33/1000 | Loss: 0.00002283
Iteration 34/1000 | Loss: 0.00002282
Iteration 35/1000 | Loss: 0.00002282
Iteration 36/1000 | Loss: 0.00002282
Iteration 37/1000 | Loss: 0.00002282
Iteration 38/1000 | Loss: 0.00002282
Iteration 39/1000 | Loss: 0.00002282
Iteration 40/1000 | Loss: 0.00002282
Iteration 41/1000 | Loss: 0.00002282
Iteration 42/1000 | Loss: 0.00002282
Iteration 43/1000 | Loss: 0.00002282
Iteration 44/1000 | Loss: 0.00002282
Iteration 45/1000 | Loss: 0.00002281
Iteration 46/1000 | Loss: 0.00002281
Iteration 47/1000 | Loss: 0.00002281
Iteration 48/1000 | Loss: 0.00002280
Iteration 49/1000 | Loss: 0.00002280
Iteration 50/1000 | Loss: 0.00002280
Iteration 51/1000 | Loss: 0.00002279
Iteration 52/1000 | Loss: 0.00002279
Iteration 53/1000 | Loss: 0.00002278
Iteration 54/1000 | Loss: 0.00002278
Iteration 55/1000 | Loss: 0.00002278
Iteration 56/1000 | Loss: 0.00002278
Iteration 57/1000 | Loss: 0.00002278
Iteration 58/1000 | Loss: 0.00002277
Iteration 59/1000 | Loss: 0.00002277
Iteration 60/1000 | Loss: 0.00002277
Iteration 61/1000 | Loss: 0.00002277
Iteration 62/1000 | Loss: 0.00002276
Iteration 63/1000 | Loss: 0.00002275
Iteration 64/1000 | Loss: 0.00002274
Iteration 65/1000 | Loss: 0.00002273
Iteration 66/1000 | Loss: 0.00002273
Iteration 67/1000 | Loss: 0.00002273
Iteration 68/1000 | Loss: 0.00002272
Iteration 69/1000 | Loss: 0.00002272
Iteration 70/1000 | Loss: 0.00002272
Iteration 71/1000 | Loss: 0.00002272
Iteration 72/1000 | Loss: 0.00002271
Iteration 73/1000 | Loss: 0.00002271
Iteration 74/1000 | Loss: 0.00002270
Iteration 75/1000 | Loss: 0.00002270
Iteration 76/1000 | Loss: 0.00002269
Iteration 77/1000 | Loss: 0.00002268
Iteration 78/1000 | Loss: 0.00002268
Iteration 79/1000 | Loss: 0.00002268
Iteration 80/1000 | Loss: 0.00002268
Iteration 81/1000 | Loss: 0.00002268
Iteration 82/1000 | Loss: 0.00002268
Iteration 83/1000 | Loss: 0.00002267
Iteration 84/1000 | Loss: 0.00002267
Iteration 85/1000 | Loss: 0.00002267
Iteration 86/1000 | Loss: 0.00002267
Iteration 87/1000 | Loss: 0.00002266
Iteration 88/1000 | Loss: 0.00002266
Iteration 89/1000 | Loss: 0.00002266
Iteration 90/1000 | Loss: 0.00002265
Iteration 91/1000 | Loss: 0.00002265
Iteration 92/1000 | Loss: 0.00002265
Iteration 93/1000 | Loss: 0.00002265
Iteration 94/1000 | Loss: 0.00002264
Iteration 95/1000 | Loss: 0.00002264
Iteration 96/1000 | Loss: 0.00002264
Iteration 97/1000 | Loss: 0.00002264
Iteration 98/1000 | Loss: 0.00002264
Iteration 99/1000 | Loss: 0.00002263
Iteration 100/1000 | Loss: 0.00002263
Iteration 101/1000 | Loss: 0.00002263
Iteration 102/1000 | Loss: 0.00002263
Iteration 103/1000 | Loss: 0.00002263
Iteration 104/1000 | Loss: 0.00002263
Iteration 105/1000 | Loss: 0.00002263
Iteration 106/1000 | Loss: 0.00002263
Iteration 107/1000 | Loss: 0.00002263
Iteration 108/1000 | Loss: 0.00002263
Iteration 109/1000 | Loss: 0.00002263
Iteration 110/1000 | Loss: 0.00002262
Iteration 111/1000 | Loss: 0.00002262
Iteration 112/1000 | Loss: 0.00002262
Iteration 113/1000 | Loss: 0.00002262
Iteration 114/1000 | Loss: 0.00002261
Iteration 115/1000 | Loss: 0.00002261
Iteration 116/1000 | Loss: 0.00002261
Iteration 117/1000 | Loss: 0.00002261
Iteration 118/1000 | Loss: 0.00002261
Iteration 119/1000 | Loss: 0.00002261
Iteration 120/1000 | Loss: 0.00002260
Iteration 121/1000 | Loss: 0.00002260
Iteration 122/1000 | Loss: 0.00002260
Iteration 123/1000 | Loss: 0.00002260
Iteration 124/1000 | Loss: 0.00002260
Iteration 125/1000 | Loss: 0.00002260
Iteration 126/1000 | Loss: 0.00002260
Iteration 127/1000 | Loss: 0.00002260
Iteration 128/1000 | Loss: 0.00002260
Iteration 129/1000 | Loss: 0.00002260
Iteration 130/1000 | Loss: 0.00002260
Iteration 131/1000 | Loss: 0.00002259
Iteration 132/1000 | Loss: 0.00002259
Iteration 133/1000 | Loss: 0.00002259
Iteration 134/1000 | Loss: 0.00002259
Iteration 135/1000 | Loss: 0.00002259
Iteration 136/1000 | Loss: 0.00002259
Iteration 137/1000 | Loss: 0.00002259
Iteration 138/1000 | Loss: 0.00002258
Iteration 139/1000 | Loss: 0.00002258
Iteration 140/1000 | Loss: 0.00002257
Iteration 141/1000 | Loss: 0.00002257
Iteration 142/1000 | Loss: 0.00002257
Iteration 143/1000 | Loss: 0.00002257
Iteration 144/1000 | Loss: 0.00002256
Iteration 145/1000 | Loss: 0.00002256
Iteration 146/1000 | Loss: 0.00002256
Iteration 147/1000 | Loss: 0.00002256
Iteration 148/1000 | Loss: 0.00002256
Iteration 149/1000 | Loss: 0.00002255
Iteration 150/1000 | Loss: 0.00002255
Iteration 151/1000 | Loss: 0.00002255
Iteration 152/1000 | Loss: 0.00002255
Iteration 153/1000 | Loss: 0.00002255
Iteration 154/1000 | Loss: 0.00002254
Iteration 155/1000 | Loss: 0.00002254
Iteration 156/1000 | Loss: 0.00002254
Iteration 157/1000 | Loss: 0.00002254
Iteration 158/1000 | Loss: 0.00002254
Iteration 159/1000 | Loss: 0.00002254
Iteration 160/1000 | Loss: 0.00002254
Iteration 161/1000 | Loss: 0.00002253
Iteration 162/1000 | Loss: 0.00002253
Iteration 163/1000 | Loss: 0.00002253
Iteration 164/1000 | Loss: 0.00002253
Iteration 165/1000 | Loss: 0.00002253
Iteration 166/1000 | Loss: 0.00002252
Iteration 167/1000 | Loss: 0.00002252
Iteration 168/1000 | Loss: 0.00002252
Iteration 169/1000 | Loss: 0.00002252
Iteration 170/1000 | Loss: 0.00002252
Iteration 171/1000 | Loss: 0.00002252
Iteration 172/1000 | Loss: 0.00002252
Iteration 173/1000 | Loss: 0.00002251
Iteration 174/1000 | Loss: 0.00002251
Iteration 175/1000 | Loss: 0.00002251
Iteration 176/1000 | Loss: 0.00002251
Iteration 177/1000 | Loss: 0.00002251
Iteration 178/1000 | Loss: 0.00002250
Iteration 179/1000 | Loss: 0.00002250
Iteration 180/1000 | Loss: 0.00002250
Iteration 181/1000 | Loss: 0.00002250
Iteration 182/1000 | Loss: 0.00002250
Iteration 183/1000 | Loss: 0.00002250
Iteration 184/1000 | Loss: 0.00002250
Iteration 185/1000 | Loss: 0.00002249
Iteration 186/1000 | Loss: 0.00002249
Iteration 187/1000 | Loss: 0.00002249
Iteration 188/1000 | Loss: 0.00002249
Iteration 189/1000 | Loss: 0.00002249
Iteration 190/1000 | Loss: 0.00002249
Iteration 191/1000 | Loss: 0.00002249
Iteration 192/1000 | Loss: 0.00002249
Iteration 193/1000 | Loss: 0.00002249
Iteration 194/1000 | Loss: 0.00002249
Iteration 195/1000 | Loss: 0.00002249
Iteration 196/1000 | Loss: 0.00002248
Iteration 197/1000 | Loss: 0.00002248
Iteration 198/1000 | Loss: 0.00002248
Iteration 199/1000 | Loss: 0.00002248
Iteration 200/1000 | Loss: 0.00002248
Iteration 201/1000 | Loss: 0.00002248
Iteration 202/1000 | Loss: 0.00002248
Iteration 203/1000 | Loss: 0.00002248
Iteration 204/1000 | Loss: 0.00002248
Iteration 205/1000 | Loss: 0.00002248
Iteration 206/1000 | Loss: 0.00002248
Iteration 207/1000 | Loss: 0.00002248
Iteration 208/1000 | Loss: 0.00002247
Iteration 209/1000 | Loss: 0.00002247
Iteration 210/1000 | Loss: 0.00002247
Iteration 211/1000 | Loss: 0.00002247
Iteration 212/1000 | Loss: 0.00002247
Iteration 213/1000 | Loss: 0.00002247
Iteration 214/1000 | Loss: 0.00002247
Iteration 215/1000 | Loss: 0.00002247
Iteration 216/1000 | Loss: 0.00002247
Iteration 217/1000 | Loss: 0.00002247
Iteration 218/1000 | Loss: 0.00002247
Iteration 219/1000 | Loss: 0.00002246
Iteration 220/1000 | Loss: 0.00002246
Iteration 221/1000 | Loss: 0.00002246
Iteration 222/1000 | Loss: 0.00002246
Iteration 223/1000 | Loss: 0.00002246
Iteration 224/1000 | Loss: 0.00002246
Iteration 225/1000 | Loss: 0.00002245
Iteration 226/1000 | Loss: 0.00002245
Iteration 227/1000 | Loss: 0.00002245
Iteration 228/1000 | Loss: 0.00002245
Iteration 229/1000 | Loss: 0.00002245
Iteration 230/1000 | Loss: 0.00002245
Iteration 231/1000 | Loss: 0.00002245
Iteration 232/1000 | Loss: 0.00002245
Iteration 233/1000 | Loss: 0.00002244
Iteration 234/1000 | Loss: 0.00002244
Iteration 235/1000 | Loss: 0.00002244
Iteration 236/1000 | Loss: 0.00002244
Iteration 237/1000 | Loss: 0.00002244
Iteration 238/1000 | Loss: 0.00002244
Iteration 239/1000 | Loss: 0.00002244
Iteration 240/1000 | Loss: 0.00002244
Iteration 241/1000 | Loss: 0.00002244
Iteration 242/1000 | Loss: 0.00002244
Iteration 243/1000 | Loss: 0.00002243
Iteration 244/1000 | Loss: 0.00002243
Iteration 245/1000 | Loss: 0.00002243
Iteration 246/1000 | Loss: 0.00002243
Iteration 247/1000 | Loss: 0.00002243
Iteration 248/1000 | Loss: 0.00002243
Iteration 249/1000 | Loss: 0.00002243
Iteration 250/1000 | Loss: 0.00002242
Iteration 251/1000 | Loss: 0.00002242
Iteration 252/1000 | Loss: 0.00002242
Iteration 253/1000 | Loss: 0.00002242
Iteration 254/1000 | Loss: 0.00002242
Iteration 255/1000 | Loss: 0.00002242
Iteration 256/1000 | Loss: 0.00002242
Iteration 257/1000 | Loss: 0.00002242
Iteration 258/1000 | Loss: 0.00002242
Iteration 259/1000 | Loss: 0.00002242
Iteration 260/1000 | Loss: 0.00002242
Iteration 261/1000 | Loss: 0.00002242
Iteration 262/1000 | Loss: 0.00002242
Iteration 263/1000 | Loss: 0.00002242
Iteration 264/1000 | Loss: 0.00002242
Iteration 265/1000 | Loss: 0.00002242
Iteration 266/1000 | Loss: 0.00002242
Iteration 267/1000 | Loss: 0.00002242
Iteration 268/1000 | Loss: 0.00002242
Iteration 269/1000 | Loss: 0.00002242
Iteration 270/1000 | Loss: 0.00002242
Iteration 271/1000 | Loss: 0.00002242
Iteration 272/1000 | Loss: 0.00002242
Iteration 273/1000 | Loss: 0.00002242
Iteration 274/1000 | Loss: 0.00002242
Iteration 275/1000 | Loss: 0.00002242
Iteration 276/1000 | Loss: 0.00002242
Iteration 277/1000 | Loss: 0.00002242
Iteration 278/1000 | Loss: 0.00002242
Iteration 279/1000 | Loss: 0.00002242
Iteration 280/1000 | Loss: 0.00002242
Iteration 281/1000 | Loss: 0.00002242
Iteration 282/1000 | Loss: 0.00002242
Iteration 283/1000 | Loss: 0.00002242
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 283. Stopping optimization.
Last 5 losses: [2.2422047550207935e-05, 2.2422047550207935e-05, 2.2422047550207935e-05, 2.2422047550207935e-05, 2.2422047550207935e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.2422047550207935e-05

Optimization complete. Final v2v error: 3.682295083999634 mm

Highest mean error: 5.459606647491455 mm for frame 73

Lowest mean error: 2.7439017295837402 mm for frame 35

Saving results

Total time: 49.4709095954895
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0003/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0003.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0003
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608965
Iteration 2/25 | Loss: 0.00143633
Iteration 3/25 | Loss: 0.00116544
Iteration 4/25 | Loss: 0.00113928
Iteration 5/25 | Loss: 0.00112434
Iteration 6/25 | Loss: 0.00109991
Iteration 7/25 | Loss: 0.00108166
Iteration 8/25 | Loss: 0.00107113
Iteration 9/25 | Loss: 0.00105739
Iteration 10/25 | Loss: 0.00104959
Iteration 11/25 | Loss: 0.00104636
Iteration 12/25 | Loss: 0.00104872
Iteration 13/25 | Loss: 0.00104806
Iteration 14/25 | Loss: 0.00104477
Iteration 15/25 | Loss: 0.00104313
Iteration 16/25 | Loss: 0.00104280
Iteration 17/25 | Loss: 0.00104266
Iteration 18/25 | Loss: 0.00104259
Iteration 19/25 | Loss: 0.00104259
Iteration 20/25 | Loss: 0.00104258
Iteration 21/25 | Loss: 0.00104257
Iteration 22/25 | Loss: 0.00104257
Iteration 23/25 | Loss: 0.00104256
Iteration 24/25 | Loss: 0.00104256
Iteration 25/25 | Loss: 0.00104255

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.76175070
Iteration 2/25 | Loss: 0.00110286
Iteration 3/25 | Loss: 0.00110286
Iteration 4/25 | Loss: 0.00110286
Iteration 5/25 | Loss: 0.00110286
Iteration 6/25 | Loss: 0.00110286
Iteration 7/25 | Loss: 0.00110286
Iteration 8/25 | Loss: 0.00110286
Iteration 9/25 | Loss: 0.00110286
Iteration 10/25 | Loss: 0.00110286
Iteration 11/25 | Loss: 0.00110286
Iteration 12/25 | Loss: 0.00110286
Iteration 13/25 | Loss: 0.00110286
Iteration 14/25 | Loss: 0.00110286
Iteration 15/25 | Loss: 0.00110286
Iteration 16/25 | Loss: 0.00110286
Iteration 17/25 | Loss: 0.00110286
Iteration 18/25 | Loss: 0.00110286
Iteration 19/25 | Loss: 0.00110286
Iteration 20/25 | Loss: 0.00110286
Iteration 21/25 | Loss: 0.00110286
Iteration 22/25 | Loss: 0.00110286
Iteration 23/25 | Loss: 0.00110286
Iteration 24/25 | Loss: 0.00110286
Iteration 25/25 | Loss: 0.00110286
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 25. Stopping optimization.
Last 5 losses: [0.0011028572916984558, 0.0011028572916984558, 0.0011028572916984558, 0.0011028572916984558, 0.0011028572916984558]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011028572916984558

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00110286
Iteration 2/1000 | Loss: 0.00006838
Iteration 3/1000 | Loss: 0.00004680
Iteration 4/1000 | Loss: 0.00003860
Iteration 5/1000 | Loss: 0.00003413
Iteration 6/1000 | Loss: 0.00003168
Iteration 7/1000 | Loss: 0.00003006
Iteration 8/1000 | Loss: 0.00002901
Iteration 9/1000 | Loss: 0.00002790
Iteration 10/1000 | Loss: 0.00002715
Iteration 11/1000 | Loss: 0.00002656
Iteration 12/1000 | Loss: 0.00002616
Iteration 13/1000 | Loss: 0.00002582
Iteration 14/1000 | Loss: 0.00010826
Iteration 15/1000 | Loss: 0.00004222
Iteration 16/1000 | Loss: 0.00002564
Iteration 17/1000 | Loss: 0.00010600
Iteration 18/1000 | Loss: 0.00003141
Iteration 19/1000 | Loss: 0.00002587
Iteration 20/1000 | Loss: 0.00006481
Iteration 21/1000 | Loss: 0.00002699
Iteration 22/1000 | Loss: 0.00006189
Iteration 23/1000 | Loss: 0.00002629
Iteration 24/1000 | Loss: 0.00002538
Iteration 25/1000 | Loss: 0.00002475
Iteration 26/1000 | Loss: 0.00002425
Iteration 27/1000 | Loss: 0.00002394
Iteration 28/1000 | Loss: 0.00002362
Iteration 29/1000 | Loss: 0.00002337
Iteration 30/1000 | Loss: 0.00002316
Iteration 31/1000 | Loss: 0.00002298
Iteration 32/1000 | Loss: 0.00002294
Iteration 33/1000 | Loss: 0.00002292
Iteration 34/1000 | Loss: 0.00002292
Iteration 35/1000 | Loss: 0.00002291
Iteration 36/1000 | Loss: 0.00002291
Iteration 37/1000 | Loss: 0.00002290
Iteration 38/1000 | Loss: 0.00002290
Iteration 39/1000 | Loss: 0.00002289
Iteration 40/1000 | Loss: 0.00002287
Iteration 41/1000 | Loss: 0.00002287
Iteration 42/1000 | Loss: 0.00002287
Iteration 43/1000 | Loss: 0.00002287
Iteration 44/1000 | Loss: 0.00002286
Iteration 45/1000 | Loss: 0.00002286
Iteration 46/1000 | Loss: 0.00002285
Iteration 47/1000 | Loss: 0.00002285
Iteration 48/1000 | Loss: 0.00002285
Iteration 49/1000 | Loss: 0.00002285
Iteration 50/1000 | Loss: 0.00002285
Iteration 51/1000 | Loss: 0.00002284
Iteration 52/1000 | Loss: 0.00002284
Iteration 53/1000 | Loss: 0.00002284
Iteration 54/1000 | Loss: 0.00002283
Iteration 55/1000 | Loss: 0.00002283
Iteration 56/1000 | Loss: 0.00002283
Iteration 57/1000 | Loss: 0.00002283
Iteration 58/1000 | Loss: 0.00002282
Iteration 59/1000 | Loss: 0.00002282
Iteration 60/1000 | Loss: 0.00002281
Iteration 61/1000 | Loss: 0.00002281
Iteration 62/1000 | Loss: 0.00002279
Iteration 63/1000 | Loss: 0.00002278
Iteration 64/1000 | Loss: 0.00002278
Iteration 65/1000 | Loss: 0.00002277
Iteration 66/1000 | Loss: 0.00002276
Iteration 67/1000 | Loss: 0.00002275
Iteration 68/1000 | Loss: 0.00002275
Iteration 69/1000 | Loss: 0.00002275
Iteration 70/1000 | Loss: 0.00002275
Iteration 71/1000 | Loss: 0.00002274
Iteration 72/1000 | Loss: 0.00002274
Iteration 73/1000 | Loss: 0.00002273
Iteration 74/1000 | Loss: 0.00002271
Iteration 75/1000 | Loss: 0.00002271
Iteration 76/1000 | Loss: 0.00002271
Iteration 77/1000 | Loss: 0.00002271
Iteration 78/1000 | Loss: 0.00002271
Iteration 79/1000 | Loss: 0.00002270
Iteration 80/1000 | Loss: 0.00002269
Iteration 81/1000 | Loss: 0.00002267
Iteration 82/1000 | Loss: 0.00002267
Iteration 83/1000 | Loss: 0.00002267
Iteration 84/1000 | Loss: 0.00002267
Iteration 85/1000 | Loss: 0.00002266
Iteration 86/1000 | Loss: 0.00002266
Iteration 87/1000 | Loss: 0.00002266
Iteration 88/1000 | Loss: 0.00002266
Iteration 89/1000 | Loss: 0.00002266
Iteration 90/1000 | Loss: 0.00002266
Iteration 91/1000 | Loss: 0.00002266
Iteration 92/1000 | Loss: 0.00002266
Iteration 93/1000 | Loss: 0.00002264
Iteration 94/1000 | Loss: 0.00002264
Iteration 95/1000 | Loss: 0.00002262
Iteration 96/1000 | Loss: 0.00002262
Iteration 97/1000 | Loss: 0.00002261
Iteration 98/1000 | Loss: 0.00002261
Iteration 99/1000 | Loss: 0.00002261
Iteration 100/1000 | Loss: 0.00002260
Iteration 101/1000 | Loss: 0.00002260
Iteration 102/1000 | Loss: 0.00002260
Iteration 103/1000 | Loss: 0.00002260
Iteration 104/1000 | Loss: 0.00002260
Iteration 105/1000 | Loss: 0.00002259
Iteration 106/1000 | Loss: 0.00002259
Iteration 107/1000 | Loss: 0.00002259
Iteration 108/1000 | Loss: 0.00002259
Iteration 109/1000 | Loss: 0.00002259
Iteration 110/1000 | Loss: 0.00002259
Iteration 111/1000 | Loss: 0.00002259
Iteration 112/1000 | Loss: 0.00002259
Iteration 113/1000 | Loss: 0.00002258
Iteration 114/1000 | Loss: 0.00002258
Iteration 115/1000 | Loss: 0.00002258
Iteration 116/1000 | Loss: 0.00002258
Iteration 117/1000 | Loss: 0.00002258
Iteration 118/1000 | Loss: 0.00002258
Iteration 119/1000 | Loss: 0.00002258
Iteration 120/1000 | Loss: 0.00002258
Iteration 121/1000 | Loss: 0.00002258
Iteration 122/1000 | Loss: 0.00002258
Iteration 123/1000 | Loss: 0.00002258
Iteration 124/1000 | Loss: 0.00002257
Iteration 125/1000 | Loss: 0.00002257
Iteration 126/1000 | Loss: 0.00002257
Iteration 127/1000 | Loss: 0.00002257
Iteration 128/1000 | Loss: 0.00002257
Iteration 129/1000 | Loss: 0.00002257
Iteration 130/1000 | Loss: 0.00002257
Iteration 131/1000 | Loss: 0.00002257
Iteration 132/1000 | Loss: 0.00002257
Iteration 133/1000 | Loss: 0.00002256
Iteration 134/1000 | Loss: 0.00002256
Iteration 135/1000 | Loss: 0.00002256
Iteration 136/1000 | Loss: 0.00002256
Iteration 137/1000 | Loss: 0.00002255
Iteration 138/1000 | Loss: 0.00002255
Iteration 139/1000 | Loss: 0.00002255
Iteration 140/1000 | Loss: 0.00002255
Iteration 141/1000 | Loss: 0.00002255
Iteration 142/1000 | Loss: 0.00002255
Iteration 143/1000 | Loss: 0.00002255
Iteration 144/1000 | Loss: 0.00002255
Iteration 145/1000 | Loss: 0.00002255
Iteration 146/1000 | Loss: 0.00002255
Iteration 147/1000 | Loss: 0.00002255
Iteration 148/1000 | Loss: 0.00002255
Iteration 149/1000 | Loss: 0.00002255
Iteration 150/1000 | Loss: 0.00002255
Iteration 151/1000 | Loss: 0.00002255
Iteration 152/1000 | Loss: 0.00002255
Iteration 153/1000 | Loss: 0.00002255
Iteration 154/1000 | Loss: 0.00002255
Iteration 155/1000 | Loss: 0.00002255
Iteration 156/1000 | Loss: 0.00002255
Iteration 157/1000 | Loss: 0.00002255
Iteration 158/1000 | Loss: 0.00002255
Iteration 159/1000 | Loss: 0.00002255
Iteration 160/1000 | Loss: 0.00002255
Iteration 161/1000 | Loss: 0.00002255
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 161. Stopping optimization.
Last 5 losses: [2.254503488074988e-05, 2.254503488074988e-05, 2.254503488074988e-05, 2.254503488074988e-05, 2.254503488074988e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.254503488074988e-05

Optimization complete. Final v2v error: 3.469130277633667 mm

Highest mean error: 11.815701484680176 mm for frame 105

Lowest mean error: 2.7713029384613037 mm for frame 232

Saving results

Total time: 98.17149662971497
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0019/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0019.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0019
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00971820
Iteration 2/25 | Loss: 0.00265819
Iteration 3/25 | Loss: 0.00194674
Iteration 4/25 | Loss: 0.00182757
Iteration 5/25 | Loss: 0.00162012
Iteration 6/25 | Loss: 0.00147302
Iteration 7/25 | Loss: 0.00137998
Iteration 8/25 | Loss: 0.00131420
Iteration 9/25 | Loss: 0.00132016
Iteration 10/25 | Loss: 0.00126776
Iteration 11/25 | Loss: 0.00123493
Iteration 12/25 | Loss: 0.00123157
Iteration 13/25 | Loss: 0.00122005
Iteration 14/25 | Loss: 0.00121677
Iteration 15/25 | Loss: 0.00121572
Iteration 16/25 | Loss: 0.00121514
Iteration 17/25 | Loss: 0.00121862
Iteration 18/25 | Loss: 0.00121271
Iteration 19/25 | Loss: 0.00121226
Iteration 20/25 | Loss: 0.00121221
Iteration 21/25 | Loss: 0.00121221
Iteration 22/25 | Loss: 0.00121221
Iteration 23/25 | Loss: 0.00121220
Iteration 24/25 | Loss: 0.00121220
Iteration 25/25 | Loss: 0.00121220

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27119279
Iteration 2/25 | Loss: 0.00230385
Iteration 3/25 | Loss: 0.00133751
Iteration 4/25 | Loss: 0.00129177
Iteration 5/25 | Loss: 0.00129177
Iteration 6/25 | Loss: 0.00129177
Iteration 7/25 | Loss: 0.00129177
Iteration 8/25 | Loss: 0.00129177
Iteration 9/25 | Loss: 0.00129177
Iteration 10/25 | Loss: 0.00129177
Iteration 11/25 | Loss: 0.00129177
Iteration 12/25 | Loss: 0.00129177
Iteration 13/25 | Loss: 0.00129177
Iteration 14/25 | Loss: 0.00129177
Iteration 15/25 | Loss: 0.00129177
Iteration 16/25 | Loss: 0.00129177
Iteration 17/25 | Loss: 0.00129177
Iteration 18/25 | Loss: 0.00129177
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0012917686253786087, 0.0012917686253786087, 0.0012917686253786087, 0.0012917686253786087, 0.0012917686253786087]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012917686253786087

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00129177
Iteration 2/1000 | Loss: 0.00644640
Iteration 3/1000 | Loss: 0.00071493
Iteration 4/1000 | Loss: 0.00129569
Iteration 5/1000 | Loss: 0.00044571
Iteration 6/1000 | Loss: 0.00021767
Iteration 7/1000 | Loss: 0.00026421
Iteration 8/1000 | Loss: 0.00034805
Iteration 9/1000 | Loss: 0.00011003
Iteration 10/1000 | Loss: 0.00038245
Iteration 11/1000 | Loss: 0.00021294
Iteration 12/1000 | Loss: 0.00029256
Iteration 13/1000 | Loss: 0.00010093
Iteration 14/1000 | Loss: 0.00011319
Iteration 15/1000 | Loss: 0.00038647
Iteration 16/1000 | Loss: 0.00050656
Iteration 17/1000 | Loss: 0.00011052
Iteration 18/1000 | Loss: 0.00028552
Iteration 19/1000 | Loss: 0.00014044
Iteration 20/1000 | Loss: 0.00025879
Iteration 21/1000 | Loss: 0.00056064
Iteration 22/1000 | Loss: 0.00025665
Iteration 23/1000 | Loss: 0.00012021
Iteration 24/1000 | Loss: 0.00006097
Iteration 25/1000 | Loss: 0.00005221
Iteration 26/1000 | Loss: 0.00015877
Iteration 27/1000 | Loss: 0.00004845
Iteration 28/1000 | Loss: 0.00027674
Iteration 29/1000 | Loss: 0.00004873
Iteration 30/1000 | Loss: 0.00018926
Iteration 31/1000 | Loss: 0.00004714
Iteration 32/1000 | Loss: 0.00004260
Iteration 33/1000 | Loss: 0.00003979
Iteration 34/1000 | Loss: 0.00003854
Iteration 35/1000 | Loss: 0.00005984
Iteration 36/1000 | Loss: 0.00018468
Iteration 37/1000 | Loss: 0.00008426
Iteration 38/1000 | Loss: 0.00005061
Iteration 39/1000 | Loss: 0.00016024
Iteration 40/1000 | Loss: 0.00013805
Iteration 41/1000 | Loss: 0.00015336
Iteration 42/1000 | Loss: 0.00006438
Iteration 43/1000 | Loss: 0.00014635
Iteration 44/1000 | Loss: 0.00004353
Iteration 45/1000 | Loss: 0.00003570
Iteration 46/1000 | Loss: 0.00003221
Iteration 47/1000 | Loss: 0.00003038
Iteration 48/1000 | Loss: 0.00007444
Iteration 49/1000 | Loss: 0.00005269
Iteration 50/1000 | Loss: 0.00004964
Iteration 51/1000 | Loss: 0.00003697
Iteration 52/1000 | Loss: 0.00004713
Iteration 53/1000 | Loss: 0.00003096
Iteration 54/1000 | Loss: 0.00002844
Iteration 55/1000 | Loss: 0.00002810
Iteration 56/1000 | Loss: 0.00004998
Iteration 57/1000 | Loss: 0.00004996
Iteration 58/1000 | Loss: 0.00008968
Iteration 59/1000 | Loss: 0.00005305
Iteration 60/1000 | Loss: 0.00005841
Iteration 61/1000 | Loss: 0.00006742
Iteration 62/1000 | Loss: 0.00002847
Iteration 63/1000 | Loss: 0.00004138
Iteration 64/1000 | Loss: 0.00002866
Iteration 65/1000 | Loss: 0.00002799
Iteration 66/1000 | Loss: 0.00002741
Iteration 67/1000 | Loss: 0.00002731
Iteration 68/1000 | Loss: 0.00002705
Iteration 69/1000 | Loss: 0.00004982
Iteration 70/1000 | Loss: 0.00007902
Iteration 71/1000 | Loss: 0.00006255
Iteration 72/1000 | Loss: 0.00003995
Iteration 73/1000 | Loss: 0.00006315
Iteration 74/1000 | Loss: 0.00004512
Iteration 75/1000 | Loss: 0.00003046
Iteration 76/1000 | Loss: 0.00003639
Iteration 77/1000 | Loss: 0.00003098
Iteration 78/1000 | Loss: 0.00004112
Iteration 79/1000 | Loss: 0.00003042
Iteration 80/1000 | Loss: 0.00002755
Iteration 81/1000 | Loss: 0.00002709
Iteration 82/1000 | Loss: 0.00005843
Iteration 83/1000 | Loss: 0.00002671
Iteration 84/1000 | Loss: 0.00002657
Iteration 85/1000 | Loss: 0.00002648
Iteration 86/1000 | Loss: 0.00002645
Iteration 87/1000 | Loss: 0.00002644
Iteration 88/1000 | Loss: 0.00002644
Iteration 89/1000 | Loss: 0.00002643
Iteration 90/1000 | Loss: 0.00002643
Iteration 91/1000 | Loss: 0.00002643
Iteration 92/1000 | Loss: 0.00002642
Iteration 93/1000 | Loss: 0.00002641
Iteration 94/1000 | Loss: 0.00002638
Iteration 95/1000 | Loss: 0.00002638
Iteration 96/1000 | Loss: 0.00002637
Iteration 97/1000 | Loss: 0.00002636
Iteration 98/1000 | Loss: 0.00002636
Iteration 99/1000 | Loss: 0.00002636
Iteration 100/1000 | Loss: 0.00002634
Iteration 101/1000 | Loss: 0.00002633
Iteration 102/1000 | Loss: 0.00002632
Iteration 103/1000 | Loss: 0.00002631
Iteration 104/1000 | Loss: 0.00004967
Iteration 105/1000 | Loss: 0.00004967
Iteration 106/1000 | Loss: 0.00008334
Iteration 107/1000 | Loss: 0.00003014
Iteration 108/1000 | Loss: 0.00002955
Iteration 109/1000 | Loss: 0.00002752
Iteration 110/1000 | Loss: 0.00003907
Iteration 111/1000 | Loss: 0.00008678
Iteration 112/1000 | Loss: 0.00003291
Iteration 113/1000 | Loss: 0.00003307
Iteration 114/1000 | Loss: 0.00002629
Iteration 115/1000 | Loss: 0.00002619
Iteration 116/1000 | Loss: 0.00002616
Iteration 117/1000 | Loss: 0.00004043
Iteration 118/1000 | Loss: 0.00008863
Iteration 119/1000 | Loss: 0.00004373
Iteration 120/1000 | Loss: 0.00002711
Iteration 121/1000 | Loss: 0.00002654
Iteration 122/1000 | Loss: 0.00002623
Iteration 123/1000 | Loss: 0.00002593
Iteration 124/1000 | Loss: 0.00004899
Iteration 125/1000 | Loss: 0.00011055
Iteration 126/1000 | Loss: 0.00005661
Iteration 127/1000 | Loss: 0.00005787
Iteration 128/1000 | Loss: 0.00005199
Iteration 129/1000 | Loss: 0.00003163
Iteration 130/1000 | Loss: 0.00002885
Iteration 131/1000 | Loss: 0.00005278
Iteration 132/1000 | Loss: 0.00003112
Iteration 133/1000 | Loss: 0.00004911
Iteration 134/1000 | Loss: 0.00002896
Iteration 135/1000 | Loss: 0.00004700
Iteration 136/1000 | Loss: 0.00011854
Iteration 137/1000 | Loss: 0.00005053
Iteration 138/1000 | Loss: 0.00004507
Iteration 139/1000 | Loss: 0.00004830
Iteration 140/1000 | Loss: 0.00002552
Iteration 141/1000 | Loss: 0.00002550
Iteration 142/1000 | Loss: 0.00002545
Iteration 143/1000 | Loss: 0.00003714
Iteration 144/1000 | Loss: 0.00003383
Iteration 145/1000 | Loss: 0.00002541
Iteration 146/1000 | Loss: 0.00002538
Iteration 147/1000 | Loss: 0.00002538
Iteration 148/1000 | Loss: 0.00002538
Iteration 149/1000 | Loss: 0.00002536
Iteration 150/1000 | Loss: 0.00002536
Iteration 151/1000 | Loss: 0.00002535
Iteration 152/1000 | Loss: 0.00002535
Iteration 153/1000 | Loss: 0.00002535
Iteration 154/1000 | Loss: 0.00002534
Iteration 155/1000 | Loss: 0.00002534
Iteration 156/1000 | Loss: 0.00002533
Iteration 157/1000 | Loss: 0.00002531
Iteration 158/1000 | Loss: 0.00002531
Iteration 159/1000 | Loss: 0.00002531
Iteration 160/1000 | Loss: 0.00002531
Iteration 161/1000 | Loss: 0.00002531
Iteration 162/1000 | Loss: 0.00002530
Iteration 163/1000 | Loss: 0.00002530
Iteration 164/1000 | Loss: 0.00002530
Iteration 165/1000 | Loss: 0.00002530
Iteration 166/1000 | Loss: 0.00002530
Iteration 167/1000 | Loss: 0.00002530
Iteration 168/1000 | Loss: 0.00002529
Iteration 169/1000 | Loss: 0.00002529
Iteration 170/1000 | Loss: 0.00002528
Iteration 171/1000 | Loss: 0.00002528
Iteration 172/1000 | Loss: 0.00002528
Iteration 173/1000 | Loss: 0.00002528
Iteration 174/1000 | Loss: 0.00002527
Iteration 175/1000 | Loss: 0.00002527
Iteration 176/1000 | Loss: 0.00002527
Iteration 177/1000 | Loss: 0.00002527
Iteration 178/1000 | Loss: 0.00002527
Iteration 179/1000 | Loss: 0.00002527
Iteration 180/1000 | Loss: 0.00002526
Iteration 181/1000 | Loss: 0.00002526
Iteration 182/1000 | Loss: 0.00002526
Iteration 183/1000 | Loss: 0.00002525
Iteration 184/1000 | Loss: 0.00002525
Iteration 185/1000 | Loss: 0.00002525
Iteration 186/1000 | Loss: 0.00002525
Iteration 187/1000 | Loss: 0.00002525
Iteration 188/1000 | Loss: 0.00002524
Iteration 189/1000 | Loss: 0.00002524
Iteration 190/1000 | Loss: 0.00002524
Iteration 191/1000 | Loss: 0.00002524
Iteration 192/1000 | Loss: 0.00002524
Iteration 193/1000 | Loss: 0.00002524
Iteration 194/1000 | Loss: 0.00002524
Iteration 195/1000 | Loss: 0.00002524
Iteration 196/1000 | Loss: 0.00002524
Iteration 197/1000 | Loss: 0.00002523
Iteration 198/1000 | Loss: 0.00002523
Iteration 199/1000 | Loss: 0.00002523
Iteration 200/1000 | Loss: 0.00002523
Iteration 201/1000 | Loss: 0.00002523
Iteration 202/1000 | Loss: 0.00002522
Iteration 203/1000 | Loss: 0.00002522
Iteration 204/1000 | Loss: 0.00002522
Iteration 205/1000 | Loss: 0.00002522
Iteration 206/1000 | Loss: 0.00002522
Iteration 207/1000 | Loss: 0.00002521
Iteration 208/1000 | Loss: 0.00002521
Iteration 209/1000 | Loss: 0.00002520
Iteration 210/1000 | Loss: 0.00004875
Iteration 211/1000 | Loss: 0.00003645
Iteration 212/1000 | Loss: 0.00002752
Iteration 213/1000 | Loss: 0.00007241
Iteration 214/1000 | Loss: 0.00024747
Iteration 215/1000 | Loss: 0.00011913
Iteration 216/1000 | Loss: 0.00011365
Iteration 217/1000 | Loss: 0.00004116
Iteration 218/1000 | Loss: 0.00002669
Iteration 219/1000 | Loss: 0.00002529
Iteration 220/1000 | Loss: 0.00002935
Iteration 221/1000 | Loss: 0.00002766
Iteration 222/1000 | Loss: 0.00002843
Iteration 223/1000 | Loss: 0.00002519
Iteration 224/1000 | Loss: 0.00002519
Iteration 225/1000 | Loss: 0.00002519
Iteration 226/1000 | Loss: 0.00002519
Iteration 227/1000 | Loss: 0.00002519
Iteration 228/1000 | Loss: 0.00002519
Iteration 229/1000 | Loss: 0.00002519
Iteration 230/1000 | Loss: 0.00002519
Iteration 231/1000 | Loss: 0.00002519
Iteration 232/1000 | Loss: 0.00002519
Iteration 233/1000 | Loss: 0.00002519
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 233. Stopping optimization.
Last 5 losses: [2.5185012418660335e-05, 2.5185012418660335e-05, 2.5185012418660335e-05, 2.5185012418660335e-05, 2.5185012418660335e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.5185012418660335e-05

Optimization complete. Final v2v error: 3.8474934101104736 mm

Highest mean error: 11.2210054397583 mm for frame 50

Lowest mean error: 2.825784683227539 mm for frame 1

Saving results

Total time: 236.4180850982666
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00818433
Iteration 2/25 | Loss: 0.00118317
Iteration 3/25 | Loss: 0.00103691
Iteration 4/25 | Loss: 0.00101598
Iteration 5/25 | Loss: 0.00100917
Iteration 6/25 | Loss: 0.00100687
Iteration 7/25 | Loss: 0.00100687
Iteration 8/25 | Loss: 0.00100687
Iteration 9/25 | Loss: 0.00100687
Iteration 10/25 | Loss: 0.00100687
Iteration 11/25 | Loss: 0.00100687
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010068746050819755, 0.0010068746050819755, 0.0010068746050819755, 0.0010068746050819755, 0.0010068746050819755]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010068746050819755

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.13309789
Iteration 2/25 | Loss: 0.00107512
Iteration 3/25 | Loss: 0.00107512
Iteration 4/25 | Loss: 0.00107512
Iteration 5/25 | Loss: 0.00107512
Iteration 6/25 | Loss: 0.00107512
Iteration 7/25 | Loss: 0.00107512
Iteration 8/25 | Loss: 0.00107512
Iteration 9/25 | Loss: 0.00107512
Iteration 10/25 | Loss: 0.00107512
Iteration 11/25 | Loss: 0.00107512
Iteration 12/25 | Loss: 0.00107512
Iteration 13/25 | Loss: 0.00107512
Iteration 14/25 | Loss: 0.00107512
Iteration 15/25 | Loss: 0.00107512
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.001075115636922419, 0.001075115636922419, 0.001075115636922419, 0.001075115636922419, 0.001075115636922419]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001075115636922419

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00107512
Iteration 2/1000 | Loss: 0.00005405
Iteration 3/1000 | Loss: 0.00002726
Iteration 4/1000 | Loss: 0.00002277
Iteration 5/1000 | Loss: 0.00002085
Iteration 6/1000 | Loss: 0.00001966
Iteration 7/1000 | Loss: 0.00001915
Iteration 8/1000 | Loss: 0.00001857
Iteration 9/1000 | Loss: 0.00001818
Iteration 10/1000 | Loss: 0.00001793
Iteration 11/1000 | Loss: 0.00001789
Iteration 12/1000 | Loss: 0.00001787
Iteration 13/1000 | Loss: 0.00001776
Iteration 14/1000 | Loss: 0.00001772
Iteration 15/1000 | Loss: 0.00001770
Iteration 16/1000 | Loss: 0.00001768
Iteration 17/1000 | Loss: 0.00001768
Iteration 18/1000 | Loss: 0.00001767
Iteration 19/1000 | Loss: 0.00001767
Iteration 20/1000 | Loss: 0.00001767
Iteration 21/1000 | Loss: 0.00001767
Iteration 22/1000 | Loss: 0.00001766
Iteration 23/1000 | Loss: 0.00001766
Iteration 24/1000 | Loss: 0.00001766
Iteration 25/1000 | Loss: 0.00001765
Iteration 26/1000 | Loss: 0.00001763
Iteration 27/1000 | Loss: 0.00001763
Iteration 28/1000 | Loss: 0.00001763
Iteration 29/1000 | Loss: 0.00001763
Iteration 30/1000 | Loss: 0.00001763
Iteration 31/1000 | Loss: 0.00001762
Iteration 32/1000 | Loss: 0.00001762
Iteration 33/1000 | Loss: 0.00001757
Iteration 34/1000 | Loss: 0.00001757
Iteration 35/1000 | Loss: 0.00001757
Iteration 36/1000 | Loss: 0.00001756
Iteration 37/1000 | Loss: 0.00001756
Iteration 38/1000 | Loss: 0.00001756
Iteration 39/1000 | Loss: 0.00001756
Iteration 40/1000 | Loss: 0.00001756
Iteration 41/1000 | Loss: 0.00001756
Iteration 42/1000 | Loss: 0.00001756
Iteration 43/1000 | Loss: 0.00001756
Iteration 44/1000 | Loss: 0.00001756
Iteration 45/1000 | Loss: 0.00001756
Iteration 46/1000 | Loss: 0.00001755
Iteration 47/1000 | Loss: 0.00001755
Iteration 48/1000 | Loss: 0.00001755
Iteration 49/1000 | Loss: 0.00001755
Iteration 50/1000 | Loss: 0.00001755
Iteration 51/1000 | Loss: 0.00001755
Iteration 52/1000 | Loss: 0.00001754
Iteration 53/1000 | Loss: 0.00001754
Iteration 54/1000 | Loss: 0.00001754
Iteration 55/1000 | Loss: 0.00001754
Iteration 56/1000 | Loss: 0.00001753
Iteration 57/1000 | Loss: 0.00001753
Iteration 58/1000 | Loss: 0.00001753
Iteration 59/1000 | Loss: 0.00001753
Iteration 60/1000 | Loss: 0.00001753
Iteration 61/1000 | Loss: 0.00001753
Iteration 62/1000 | Loss: 0.00001753
Iteration 63/1000 | Loss: 0.00001753
Iteration 64/1000 | Loss: 0.00001753
Iteration 65/1000 | Loss: 0.00001753
Iteration 66/1000 | Loss: 0.00001753
Iteration 67/1000 | Loss: 0.00001753
Iteration 68/1000 | Loss: 0.00001753
Iteration 69/1000 | Loss: 0.00001753
Iteration 70/1000 | Loss: 0.00001753
Iteration 71/1000 | Loss: 0.00001753
Iteration 72/1000 | Loss: 0.00001753
Iteration 73/1000 | Loss: 0.00001753
Iteration 74/1000 | Loss: 0.00001753
Iteration 75/1000 | Loss: 0.00001753
Iteration 76/1000 | Loss: 0.00001752
Iteration 77/1000 | Loss: 0.00001752
Iteration 78/1000 | Loss: 0.00001752
Iteration 79/1000 | Loss: 0.00001752
Iteration 80/1000 | Loss: 0.00001752
Iteration 81/1000 | Loss: 0.00001752
Iteration 82/1000 | Loss: 0.00001752
Iteration 83/1000 | Loss: 0.00001752
Iteration 84/1000 | Loss: 0.00001752
Iteration 85/1000 | Loss: 0.00001752
Iteration 86/1000 | Loss: 0.00001752
Iteration 87/1000 | Loss: 0.00001752
Iteration 88/1000 | Loss: 0.00001752
Iteration 89/1000 | Loss: 0.00001751
Iteration 90/1000 | Loss: 0.00001751
Iteration 91/1000 | Loss: 0.00001751
Iteration 92/1000 | Loss: 0.00001751
Iteration 93/1000 | Loss: 0.00001751
Iteration 94/1000 | Loss: 0.00001751
Iteration 95/1000 | Loss: 0.00001751
Iteration 96/1000 | Loss: 0.00001751
Iteration 97/1000 | Loss: 0.00001751
Iteration 98/1000 | Loss: 0.00001751
Iteration 99/1000 | Loss: 0.00001751
Iteration 100/1000 | Loss: 0.00001751
Iteration 101/1000 | Loss: 0.00001751
Iteration 102/1000 | Loss: 0.00001751
Iteration 103/1000 | Loss: 0.00001751
Iteration 104/1000 | Loss: 0.00001751
Iteration 105/1000 | Loss: 0.00001751
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 105. Stopping optimization.
Last 5 losses: [1.7509704775875434e-05, 1.7509704775875434e-05, 1.7509704775875434e-05, 1.7509704775875434e-05, 1.7509704775875434e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7509704775875434e-05

Optimization complete. Final v2v error: 3.5373189449310303 mm

Highest mean error: 3.665879011154175 mm for frame 64

Lowest mean error: 3.3776376247406006 mm for frame 27

Saving results

Total time: 30.629109382629395
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0018/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0018.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0018
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00854623
Iteration 2/25 | Loss: 0.00167857
Iteration 3/25 | Loss: 0.00115409
Iteration 4/25 | Loss: 0.00106375
Iteration 5/25 | Loss: 0.00105848
Iteration 6/25 | Loss: 0.00105836
Iteration 7/25 | Loss: 0.00105836
Iteration 8/25 | Loss: 0.00105836
Iteration 9/25 | Loss: 0.00105836
Iteration 10/25 | Loss: 0.00105836
Iteration 11/25 | Loss: 0.00105836
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010583563707768917, 0.0010583563707768917, 0.0010583563707768917, 0.0010583563707768917, 0.0010583563707768917]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010583563707768917

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.25574136
Iteration 2/25 | Loss: 0.00099815
Iteration 3/25 | Loss: 0.00099815
Iteration 4/25 | Loss: 0.00099815
Iteration 5/25 | Loss: 0.00099815
Iteration 6/25 | Loss: 0.00099814
Iteration 7/25 | Loss: 0.00099814
Iteration 8/25 | Loss: 0.00099814
Iteration 9/25 | Loss: 0.00099814
Iteration 10/25 | Loss: 0.00099814
Iteration 11/25 | Loss: 0.00099814
Iteration 12/25 | Loss: 0.00099814
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009981446200981736, 0.0009981446200981736, 0.0009981446200981736, 0.0009981446200981736, 0.0009981446200981736]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009981446200981736

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00099814
Iteration 2/1000 | Loss: 0.00004224
Iteration 3/1000 | Loss: 0.00002946
Iteration 4/1000 | Loss: 0.00002644
Iteration 5/1000 | Loss: 0.00002385
Iteration 6/1000 | Loss: 0.00002264
Iteration 7/1000 | Loss: 0.00002179
Iteration 8/1000 | Loss: 0.00002100
Iteration 9/1000 | Loss: 0.00002036
Iteration 10/1000 | Loss: 0.00002000
Iteration 11/1000 | Loss: 0.00001980
Iteration 12/1000 | Loss: 0.00001979
Iteration 13/1000 | Loss: 0.00001965
Iteration 14/1000 | Loss: 0.00001959
Iteration 15/1000 | Loss: 0.00001959
Iteration 16/1000 | Loss: 0.00001956
Iteration 17/1000 | Loss: 0.00001952
Iteration 18/1000 | Loss: 0.00001949
Iteration 19/1000 | Loss: 0.00001949
Iteration 20/1000 | Loss: 0.00001948
Iteration 21/1000 | Loss: 0.00001948
Iteration 22/1000 | Loss: 0.00001947
Iteration 23/1000 | Loss: 0.00001947
Iteration 24/1000 | Loss: 0.00001946
Iteration 25/1000 | Loss: 0.00001945
Iteration 26/1000 | Loss: 0.00001943
Iteration 27/1000 | Loss: 0.00001943
Iteration 28/1000 | Loss: 0.00001941
Iteration 29/1000 | Loss: 0.00001941
Iteration 30/1000 | Loss: 0.00001941
Iteration 31/1000 | Loss: 0.00001941
Iteration 32/1000 | Loss: 0.00001941
Iteration 33/1000 | Loss: 0.00001941
Iteration 34/1000 | Loss: 0.00001941
Iteration 35/1000 | Loss: 0.00001941
Iteration 36/1000 | Loss: 0.00001941
Iteration 37/1000 | Loss: 0.00001940
Iteration 38/1000 | Loss: 0.00001940
Iteration 39/1000 | Loss: 0.00001939
Iteration 40/1000 | Loss: 0.00001939
Iteration 41/1000 | Loss: 0.00001939
Iteration 42/1000 | Loss: 0.00001939
Iteration 43/1000 | Loss: 0.00001939
Iteration 44/1000 | Loss: 0.00001939
Iteration 45/1000 | Loss: 0.00001939
Iteration 46/1000 | Loss: 0.00001939
Iteration 47/1000 | Loss: 0.00001938
Iteration 48/1000 | Loss: 0.00001938
Iteration 49/1000 | Loss: 0.00001938
Iteration 50/1000 | Loss: 0.00001938
Iteration 51/1000 | Loss: 0.00001938
Iteration 52/1000 | Loss: 0.00001938
Iteration 53/1000 | Loss: 0.00001937
Iteration 54/1000 | Loss: 0.00001937
Iteration 55/1000 | Loss: 0.00001937
Iteration 56/1000 | Loss: 0.00001937
Iteration 57/1000 | Loss: 0.00001936
Iteration 58/1000 | Loss: 0.00001936
Iteration 59/1000 | Loss: 0.00001936
Iteration 60/1000 | Loss: 0.00001936
Iteration 61/1000 | Loss: 0.00001936
Iteration 62/1000 | Loss: 0.00001936
Iteration 63/1000 | Loss: 0.00001936
Iteration 64/1000 | Loss: 0.00001935
Iteration 65/1000 | Loss: 0.00001935
Iteration 66/1000 | Loss: 0.00001935
Iteration 67/1000 | Loss: 0.00001935
Iteration 68/1000 | Loss: 0.00001935
Iteration 69/1000 | Loss: 0.00001935
Iteration 70/1000 | Loss: 0.00001934
Iteration 71/1000 | Loss: 0.00001934
Iteration 72/1000 | Loss: 0.00001934
Iteration 73/1000 | Loss: 0.00001934
Iteration 74/1000 | Loss: 0.00001934
Iteration 75/1000 | Loss: 0.00001934
Iteration 76/1000 | Loss: 0.00001934
Iteration 77/1000 | Loss: 0.00001934
Iteration 78/1000 | Loss: 0.00001934
Iteration 79/1000 | Loss: 0.00001934
Iteration 80/1000 | Loss: 0.00001934
Iteration 81/1000 | Loss: 0.00001934
Iteration 82/1000 | Loss: 0.00001934
Iteration 83/1000 | Loss: 0.00001934
Iteration 84/1000 | Loss: 0.00001934
Iteration 85/1000 | Loss: 0.00001934
Iteration 86/1000 | Loss: 0.00001934
Iteration 87/1000 | Loss: 0.00001933
Iteration 88/1000 | Loss: 0.00001933
Iteration 89/1000 | Loss: 0.00001933
Iteration 90/1000 | Loss: 0.00001933
Iteration 91/1000 | Loss: 0.00001933
Iteration 92/1000 | Loss: 0.00001933
Iteration 93/1000 | Loss: 0.00001933
Iteration 94/1000 | Loss: 0.00001932
Iteration 95/1000 | Loss: 0.00001932
Iteration 96/1000 | Loss: 0.00001932
Iteration 97/1000 | Loss: 0.00001932
Iteration 98/1000 | Loss: 0.00001932
Iteration 99/1000 | Loss: 0.00001932
Iteration 100/1000 | Loss: 0.00001932
Iteration 101/1000 | Loss: 0.00001932
Iteration 102/1000 | Loss: 0.00001932
Iteration 103/1000 | Loss: 0.00001932
Iteration 104/1000 | Loss: 0.00001932
Iteration 105/1000 | Loss: 0.00001932
Iteration 106/1000 | Loss: 0.00001932
Iteration 107/1000 | Loss: 0.00001932
Iteration 108/1000 | Loss: 0.00001932
Iteration 109/1000 | Loss: 0.00001932
Iteration 110/1000 | Loss: 0.00001932
Iteration 111/1000 | Loss: 0.00001932
Iteration 112/1000 | Loss: 0.00001932
Iteration 113/1000 | Loss: 0.00001931
Iteration 114/1000 | Loss: 0.00001931
Iteration 115/1000 | Loss: 0.00001931
Iteration 116/1000 | Loss: 0.00001931
Iteration 117/1000 | Loss: 0.00001931
Iteration 118/1000 | Loss: 0.00001931
Iteration 119/1000 | Loss: 0.00001931
Iteration 120/1000 | Loss: 0.00001931
Iteration 121/1000 | Loss: 0.00001931
Iteration 122/1000 | Loss: 0.00001931
Iteration 123/1000 | Loss: 0.00001931
Iteration 124/1000 | Loss: 0.00001931
Iteration 125/1000 | Loss: 0.00001931
Iteration 126/1000 | Loss: 0.00001931
Iteration 127/1000 | Loss: 0.00001931
Iteration 128/1000 | Loss: 0.00001931
Iteration 129/1000 | Loss: 0.00001931
Iteration 130/1000 | Loss: 0.00001931
Iteration 131/1000 | Loss: 0.00001931
Iteration 132/1000 | Loss: 0.00001931
Iteration 133/1000 | Loss: 0.00001931
Iteration 134/1000 | Loss: 0.00001931
Iteration 135/1000 | Loss: 0.00001931
Iteration 136/1000 | Loss: 0.00001931
Iteration 137/1000 | Loss: 0.00001931
Iteration 138/1000 | Loss: 0.00001931
Iteration 139/1000 | Loss: 0.00001931
Iteration 140/1000 | Loss: 0.00001931
Iteration 141/1000 | Loss: 0.00001931
Iteration 142/1000 | Loss: 0.00001931
Iteration 143/1000 | Loss: 0.00001931
Iteration 144/1000 | Loss: 0.00001931
Iteration 145/1000 | Loss: 0.00001931
Iteration 146/1000 | Loss: 0.00001931
Iteration 147/1000 | Loss: 0.00001931
Iteration 148/1000 | Loss: 0.00001931
Iteration 149/1000 | Loss: 0.00001931
Iteration 150/1000 | Loss: 0.00001931
Iteration 151/1000 | Loss: 0.00001931
Iteration 152/1000 | Loss: 0.00001931
Iteration 153/1000 | Loss: 0.00001931
Iteration 154/1000 | Loss: 0.00001931
Iteration 155/1000 | Loss: 0.00001931
Iteration 156/1000 | Loss: 0.00001931
Iteration 157/1000 | Loss: 0.00001931
Iteration 158/1000 | Loss: 0.00001931
Iteration 159/1000 | Loss: 0.00001931
Iteration 160/1000 | Loss: 0.00001931
Iteration 161/1000 | Loss: 0.00001931
Iteration 162/1000 | Loss: 0.00001931
Iteration 163/1000 | Loss: 0.00001931
Iteration 164/1000 | Loss: 0.00001931
Iteration 165/1000 | Loss: 0.00001931
Iteration 166/1000 | Loss: 0.00001931
Iteration 167/1000 | Loss: 0.00001931
Iteration 168/1000 | Loss: 0.00001931
Iteration 169/1000 | Loss: 0.00001931
Iteration 170/1000 | Loss: 0.00001931
Iteration 171/1000 | Loss: 0.00001931
Iteration 172/1000 | Loss: 0.00001931
Iteration 173/1000 | Loss: 0.00001931
Iteration 174/1000 | Loss: 0.00001931
Iteration 175/1000 | Loss: 0.00001931
Iteration 176/1000 | Loss: 0.00001931
Iteration 177/1000 | Loss: 0.00001931
Iteration 178/1000 | Loss: 0.00001931
Iteration 179/1000 | Loss: 0.00001931
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 179. Stopping optimization.
Last 5 losses: [1.9305483874632046e-05, 1.9305483874632046e-05, 1.9305483874632046e-05, 1.9305483874632046e-05, 1.9305483874632046e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.9305483874632046e-05

Optimization complete. Final v2v error: 3.627645492553711 mm

Highest mean error: 3.8688228130340576 mm for frame 69

Lowest mean error: 2.887904644012451 mm for frame 1

Saving results

Total time: 39.18271780014038
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01103704
Iteration 2/25 | Loss: 0.00166813
Iteration 3/25 | Loss: 0.00126504
Iteration 4/25 | Loss: 0.00117662
Iteration 5/25 | Loss: 0.00120056
Iteration 6/25 | Loss: 0.00110719
Iteration 7/25 | Loss: 0.00105801
Iteration 8/25 | Loss: 0.00104467
Iteration 9/25 | Loss: 0.00103467
Iteration 10/25 | Loss: 0.00103096
Iteration 11/25 | Loss: 0.00102976
Iteration 12/25 | Loss: 0.00102958
Iteration 13/25 | Loss: 0.00102958
Iteration 14/25 | Loss: 0.00102958
Iteration 15/25 | Loss: 0.00102958
Iteration 16/25 | Loss: 0.00102958
Iteration 17/25 | Loss: 0.00102958
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0010295839747413993, 0.0010295839747413993, 0.0010295839747413993, 0.0010295839747413993, 0.0010295839747413993]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010295839747413993

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.33108079
Iteration 2/25 | Loss: 0.00076541
Iteration 3/25 | Loss: 0.00076541
Iteration 4/25 | Loss: 0.00076541
Iteration 5/25 | Loss: 0.00076541
Iteration 6/25 | Loss: 0.00076540
Iteration 7/25 | Loss: 0.00076540
Iteration 8/25 | Loss: 0.00076540
Iteration 9/25 | Loss: 0.00076540
Iteration 10/25 | Loss: 0.00076540
Iteration 11/25 | Loss: 0.00076540
Iteration 12/25 | Loss: 0.00076540
Iteration 13/25 | Loss: 0.00076540
Iteration 14/25 | Loss: 0.00076540
Iteration 15/25 | Loss: 0.00076540
Iteration 16/25 | Loss: 0.00076540
Iteration 17/25 | Loss: 0.00076540
Iteration 18/25 | Loss: 0.00076540
Iteration 19/25 | Loss: 0.00076540
Iteration 20/25 | Loss: 0.00076540
Iteration 21/25 | Loss: 0.00076540
Iteration 22/25 | Loss: 0.00076540
Iteration 23/25 | Loss: 0.00076540
Iteration 24/25 | Loss: 0.00076540
Iteration 25/25 | Loss: 0.00076540

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00076540
Iteration 2/1000 | Loss: 0.00003612
Iteration 3/1000 | Loss: 0.00002303
Iteration 4/1000 | Loss: 0.00001992
Iteration 5/1000 | Loss: 0.00001839
Iteration 6/1000 | Loss: 0.00002299
Iteration 7/1000 | Loss: 0.00001986
Iteration 8/1000 | Loss: 0.00001870
Iteration 9/1000 | Loss: 0.00001809
Iteration 10/1000 | Loss: 0.00001756
Iteration 11/1000 | Loss: 0.00001719
Iteration 12/1000 | Loss: 0.00001694
Iteration 13/1000 | Loss: 0.00001680
Iteration 14/1000 | Loss: 0.00001679
Iteration 15/1000 | Loss: 0.00001673
Iteration 16/1000 | Loss: 0.00001670
Iteration 17/1000 | Loss: 0.00001669
Iteration 18/1000 | Loss: 0.00001669
Iteration 19/1000 | Loss: 0.00001669
Iteration 20/1000 | Loss: 0.00001668
Iteration 21/1000 | Loss: 0.00001667
Iteration 22/1000 | Loss: 0.00001667
Iteration 23/1000 | Loss: 0.00001666
Iteration 24/1000 | Loss: 0.00001663
Iteration 25/1000 | Loss: 0.00002428
Iteration 26/1000 | Loss: 0.00001975
Iteration 27/1000 | Loss: 0.00001886
Iteration 28/1000 | Loss: 0.00001827
Iteration 29/1000 | Loss: 0.00001719
Iteration 30/1000 | Loss: 0.00001632
Iteration 31/1000 | Loss: 0.00001596
Iteration 32/1000 | Loss: 0.00001572
Iteration 33/1000 | Loss: 0.00001558
Iteration 34/1000 | Loss: 0.00001558
Iteration 35/1000 | Loss: 0.00001555
Iteration 36/1000 | Loss: 0.00001553
Iteration 37/1000 | Loss: 0.00001552
Iteration 38/1000 | Loss: 0.00001552
Iteration 39/1000 | Loss: 0.00001551
Iteration 40/1000 | Loss: 0.00001550
Iteration 41/1000 | Loss: 0.00001550
Iteration 42/1000 | Loss: 0.00001549
Iteration 43/1000 | Loss: 0.00001549
Iteration 44/1000 | Loss: 0.00001549
Iteration 45/1000 | Loss: 0.00001548
Iteration 46/1000 | Loss: 0.00001548
Iteration 47/1000 | Loss: 0.00001548
Iteration 48/1000 | Loss: 0.00001548
Iteration 49/1000 | Loss: 0.00001548
Iteration 50/1000 | Loss: 0.00001548
Iteration 51/1000 | Loss: 0.00001548
Iteration 52/1000 | Loss: 0.00001548
Iteration 53/1000 | Loss: 0.00001548
Iteration 54/1000 | Loss: 0.00001548
Iteration 55/1000 | Loss: 0.00001547
Iteration 56/1000 | Loss: 0.00001547
Iteration 57/1000 | Loss: 0.00001547
Iteration 58/1000 | Loss: 0.00001547
Iteration 59/1000 | Loss: 0.00001547
Iteration 60/1000 | Loss: 0.00001547
Iteration 61/1000 | Loss: 0.00001547
Iteration 62/1000 | Loss: 0.00001547
Iteration 63/1000 | Loss: 0.00001547
Iteration 64/1000 | Loss: 0.00001547
Iteration 65/1000 | Loss: 0.00001547
Iteration 66/1000 | Loss: 0.00001547
Iteration 67/1000 | Loss: 0.00001547
Iteration 68/1000 | Loss: 0.00001547
Iteration 69/1000 | Loss: 0.00001547
Iteration 70/1000 | Loss: 0.00001546
Iteration 71/1000 | Loss: 0.00001546
Iteration 72/1000 | Loss: 0.00001546
Iteration 73/1000 | Loss: 0.00001546
Iteration 74/1000 | Loss: 0.00001546
Iteration 75/1000 | Loss: 0.00001546
Iteration 76/1000 | Loss: 0.00001546
Iteration 77/1000 | Loss: 0.00001546
Iteration 78/1000 | Loss: 0.00001546
Iteration 79/1000 | Loss: 0.00001546
Iteration 80/1000 | Loss: 0.00001546
Iteration 81/1000 | Loss: 0.00001546
Iteration 82/1000 | Loss: 0.00001546
Iteration 83/1000 | Loss: 0.00001546
Iteration 84/1000 | Loss: 0.00001546
Iteration 85/1000 | Loss: 0.00001546
Iteration 86/1000 | Loss: 0.00001546
Iteration 87/1000 | Loss: 0.00001546
Iteration 88/1000 | Loss: 0.00001546
Iteration 89/1000 | Loss: 0.00001546
Iteration 90/1000 | Loss: 0.00001545
Iteration 91/1000 | Loss: 0.00001545
Iteration 92/1000 | Loss: 0.00001545
Iteration 93/1000 | Loss: 0.00001545
Iteration 94/1000 | Loss: 0.00001545
Iteration 95/1000 | Loss: 0.00001545
Iteration 96/1000 | Loss: 0.00001545
Iteration 97/1000 | Loss: 0.00001545
Iteration 98/1000 | Loss: 0.00001545
Iteration 99/1000 | Loss: 0.00001545
Iteration 100/1000 | Loss: 0.00001545
Iteration 101/1000 | Loss: 0.00001545
Iteration 102/1000 | Loss: 0.00001545
Iteration 103/1000 | Loss: 0.00001545
Iteration 104/1000 | Loss: 0.00001545
Iteration 105/1000 | Loss: 0.00001545
Iteration 106/1000 | Loss: 0.00001545
Iteration 107/1000 | Loss: 0.00001545
Iteration 108/1000 | Loss: 0.00001545
Iteration 109/1000 | Loss: 0.00001545
Iteration 110/1000 | Loss: 0.00001545
Iteration 111/1000 | Loss: 0.00001545
Iteration 112/1000 | Loss: 0.00001545
Iteration 113/1000 | Loss: 0.00001545
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 113. Stopping optimization.
Last 5 losses: [1.5445863027707674e-05, 1.5445863027707674e-05, 1.5445863027707674e-05, 1.5445863027707674e-05, 1.5445863027707674e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5445863027707674e-05

Optimization complete. Final v2v error: 3.3047239780426025 mm

Highest mean error: 4.009829998016357 mm for frame 119

Lowest mean error: 2.954988479614258 mm for frame 84

Saving results

Total time: 63.11203932762146
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0007/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0007.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0007
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00757561
Iteration 2/25 | Loss: 0.00123383
Iteration 3/25 | Loss: 0.00104100
Iteration 4/25 | Loss: 0.00102078
Iteration 5/25 | Loss: 0.00101690
Iteration 6/25 | Loss: 0.00101618
Iteration 7/25 | Loss: 0.00101618
Iteration 8/25 | Loss: 0.00101618
Iteration 9/25 | Loss: 0.00101618
Iteration 10/25 | Loss: 0.00101618
Iteration 11/25 | Loss: 0.00101618
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0010161794489249587, 0.0010161794489249587, 0.0010161794489249587, 0.0010161794489249587, 0.0010161794489249587]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0010161794489249587

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 9.90735435
Iteration 2/25 | Loss: 0.00098209
Iteration 3/25 | Loss: 0.00098189
Iteration 4/25 | Loss: 0.00098189
Iteration 5/25 | Loss: 0.00098189
Iteration 6/25 | Loss: 0.00098189
Iteration 7/25 | Loss: 0.00098188
Iteration 8/25 | Loss: 0.00098188
Iteration 9/25 | Loss: 0.00098188
Iteration 10/25 | Loss: 0.00098188
Iteration 11/25 | Loss: 0.00098188
Iteration 12/25 | Loss: 0.00098188
Iteration 13/25 | Loss: 0.00098188
Iteration 14/25 | Loss: 0.00098188
Iteration 15/25 | Loss: 0.00098188
Iteration 16/25 | Loss: 0.00098188
Iteration 17/25 | Loss: 0.00098188
Iteration 18/25 | Loss: 0.00098188
Iteration 19/25 | Loss: 0.00098188
Iteration 20/25 | Loss: 0.00098188
Iteration 21/25 | Loss: 0.00098188
Iteration 22/25 | Loss: 0.00098188
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 22. Stopping optimization.
Last 5 losses: [0.0009818838443607092, 0.0009818838443607092, 0.0009818838443607092, 0.0009818838443607092, 0.0009818838443607092]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009818838443607092

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00098188
Iteration 2/1000 | Loss: 0.00004874
Iteration 3/1000 | Loss: 0.00002906
Iteration 4/1000 | Loss: 0.00002449
Iteration 5/1000 | Loss: 0.00002201
Iteration 6/1000 | Loss: 0.00002054
Iteration 7/1000 | Loss: 0.00001962
Iteration 8/1000 | Loss: 0.00001889
Iteration 9/1000 | Loss: 0.00001847
Iteration 10/1000 | Loss: 0.00001800
Iteration 11/1000 | Loss: 0.00001758
Iteration 12/1000 | Loss: 0.00001754
Iteration 13/1000 | Loss: 0.00001738
Iteration 14/1000 | Loss: 0.00001728
Iteration 15/1000 | Loss: 0.00001708
Iteration 16/1000 | Loss: 0.00001707
Iteration 17/1000 | Loss: 0.00001694
Iteration 18/1000 | Loss: 0.00001693
Iteration 19/1000 | Loss: 0.00001692
Iteration 20/1000 | Loss: 0.00001690
Iteration 21/1000 | Loss: 0.00001689
Iteration 22/1000 | Loss: 0.00001689
Iteration 23/1000 | Loss: 0.00001687
Iteration 24/1000 | Loss: 0.00001684
Iteration 25/1000 | Loss: 0.00001684
Iteration 26/1000 | Loss: 0.00001684
Iteration 27/1000 | Loss: 0.00001683
Iteration 28/1000 | Loss: 0.00001683
Iteration 29/1000 | Loss: 0.00001682
Iteration 30/1000 | Loss: 0.00001681
Iteration 31/1000 | Loss: 0.00001680
Iteration 32/1000 | Loss: 0.00001680
Iteration 33/1000 | Loss: 0.00001680
Iteration 34/1000 | Loss: 0.00001680
Iteration 35/1000 | Loss: 0.00001680
Iteration 36/1000 | Loss: 0.00001679
Iteration 37/1000 | Loss: 0.00001679
Iteration 38/1000 | Loss: 0.00001679
Iteration 39/1000 | Loss: 0.00001679
Iteration 40/1000 | Loss: 0.00001679
Iteration 41/1000 | Loss: 0.00001678
Iteration 42/1000 | Loss: 0.00001678
Iteration 43/1000 | Loss: 0.00001677
Iteration 44/1000 | Loss: 0.00001676
Iteration 45/1000 | Loss: 0.00001676
Iteration 46/1000 | Loss: 0.00001676
Iteration 47/1000 | Loss: 0.00001676
Iteration 48/1000 | Loss: 0.00001676
Iteration 49/1000 | Loss: 0.00001676
Iteration 50/1000 | Loss: 0.00001676
Iteration 51/1000 | Loss: 0.00001675
Iteration 52/1000 | Loss: 0.00001675
Iteration 53/1000 | Loss: 0.00001675
Iteration 54/1000 | Loss: 0.00001675
Iteration 55/1000 | Loss: 0.00001675
Iteration 56/1000 | Loss: 0.00001675
Iteration 57/1000 | Loss: 0.00001674
Iteration 58/1000 | Loss: 0.00001674
Iteration 59/1000 | Loss: 0.00001674
Iteration 60/1000 | Loss: 0.00001674
Iteration 61/1000 | Loss: 0.00001674
Iteration 62/1000 | Loss: 0.00001674
Iteration 63/1000 | Loss: 0.00001674
Iteration 64/1000 | Loss: 0.00001673
Iteration 65/1000 | Loss: 0.00001673
Iteration 66/1000 | Loss: 0.00001673
Iteration 67/1000 | Loss: 0.00001673
Iteration 68/1000 | Loss: 0.00001672
Iteration 69/1000 | Loss: 0.00001672
Iteration 70/1000 | Loss: 0.00001672
Iteration 71/1000 | Loss: 0.00001672
Iteration 72/1000 | Loss: 0.00001671
Iteration 73/1000 | Loss: 0.00001671
Iteration 74/1000 | Loss: 0.00001671
Iteration 75/1000 | Loss: 0.00001671
Iteration 76/1000 | Loss: 0.00001671
Iteration 77/1000 | Loss: 0.00001670
Iteration 78/1000 | Loss: 0.00001670
Iteration 79/1000 | Loss: 0.00001670
Iteration 80/1000 | Loss: 0.00001670
Iteration 81/1000 | Loss: 0.00001669
Iteration 82/1000 | Loss: 0.00001669
Iteration 83/1000 | Loss: 0.00001669
Iteration 84/1000 | Loss: 0.00001668
Iteration 85/1000 | Loss: 0.00001668
Iteration 86/1000 | Loss: 0.00001667
Iteration 87/1000 | Loss: 0.00001667
Iteration 88/1000 | Loss: 0.00001667
Iteration 89/1000 | Loss: 0.00001667
Iteration 90/1000 | Loss: 0.00001667
Iteration 91/1000 | Loss: 0.00001666
Iteration 92/1000 | Loss: 0.00001666
Iteration 93/1000 | Loss: 0.00001666
Iteration 94/1000 | Loss: 0.00001666
Iteration 95/1000 | Loss: 0.00001666
Iteration 96/1000 | Loss: 0.00001666
Iteration 97/1000 | Loss: 0.00001666
Iteration 98/1000 | Loss: 0.00001666
Iteration 99/1000 | Loss: 0.00001666
Iteration 100/1000 | Loss: 0.00001666
Iteration 101/1000 | Loss: 0.00001665
Iteration 102/1000 | Loss: 0.00001665
Iteration 103/1000 | Loss: 0.00001665
Iteration 104/1000 | Loss: 0.00001665
Iteration 105/1000 | Loss: 0.00001664
Iteration 106/1000 | Loss: 0.00001664
Iteration 107/1000 | Loss: 0.00001664
Iteration 108/1000 | Loss: 0.00001664
Iteration 109/1000 | Loss: 0.00001664
Iteration 110/1000 | Loss: 0.00001664
Iteration 111/1000 | Loss: 0.00001663
Iteration 112/1000 | Loss: 0.00001663
Iteration 113/1000 | Loss: 0.00001663
Iteration 114/1000 | Loss: 0.00001663
Iteration 115/1000 | Loss: 0.00001663
Iteration 116/1000 | Loss: 0.00001663
Iteration 117/1000 | Loss: 0.00001663
Iteration 118/1000 | Loss: 0.00001663
Iteration 119/1000 | Loss: 0.00001663
Iteration 120/1000 | Loss: 0.00001663
Iteration 121/1000 | Loss: 0.00001663
Iteration 122/1000 | Loss: 0.00001663
Iteration 123/1000 | Loss: 0.00001663
Iteration 124/1000 | Loss: 0.00001662
Iteration 125/1000 | Loss: 0.00001662
Iteration 126/1000 | Loss: 0.00001662
Iteration 127/1000 | Loss: 0.00001662
Iteration 128/1000 | Loss: 0.00001662
Iteration 129/1000 | Loss: 0.00001662
Iteration 130/1000 | Loss: 0.00001662
Iteration 131/1000 | Loss: 0.00001662
Iteration 132/1000 | Loss: 0.00001661
Iteration 133/1000 | Loss: 0.00001661
Iteration 134/1000 | Loss: 0.00001661
Iteration 135/1000 | Loss: 0.00001661
Iteration 136/1000 | Loss: 0.00001661
Iteration 137/1000 | Loss: 0.00001661
Iteration 138/1000 | Loss: 0.00001661
Iteration 139/1000 | Loss: 0.00001661
Iteration 140/1000 | Loss: 0.00001660
Iteration 141/1000 | Loss: 0.00001660
Iteration 142/1000 | Loss: 0.00001660
Iteration 143/1000 | Loss: 0.00001660
Iteration 144/1000 | Loss: 0.00001660
Iteration 145/1000 | Loss: 0.00001660
Iteration 146/1000 | Loss: 0.00001660
Iteration 147/1000 | Loss: 0.00001660
Iteration 148/1000 | Loss: 0.00001660
Iteration 149/1000 | Loss: 0.00001660
Iteration 150/1000 | Loss: 0.00001660
Iteration 151/1000 | Loss: 0.00001660
Iteration 152/1000 | Loss: 0.00001660
Iteration 153/1000 | Loss: 0.00001660
Iteration 154/1000 | Loss: 0.00001660
Iteration 155/1000 | Loss: 0.00001660
Iteration 156/1000 | Loss: 0.00001660
Iteration 157/1000 | Loss: 0.00001660
Iteration 158/1000 | Loss: 0.00001660
Iteration 159/1000 | Loss: 0.00001660
Iteration 160/1000 | Loss: 0.00001660
Iteration 161/1000 | Loss: 0.00001660
Iteration 162/1000 | Loss: 0.00001660
Iteration 163/1000 | Loss: 0.00001660
Iteration 164/1000 | Loss: 0.00001660
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 164. Stopping optimization.
Last 5 losses: [1.659879126236774e-05, 1.659879126236774e-05, 1.659879126236774e-05, 1.659879126236774e-05, 1.659879126236774e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.659879126236774e-05

Optimization complete. Final v2v error: 3.4581298828125 mm

Highest mean error: 4.032402038574219 mm for frame 110

Lowest mean error: 2.8340227603912354 mm for frame 224

Saving results

Total time: 45.09419560432434
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0004/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0004.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0004
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00853406
Iteration 2/25 | Loss: 0.00140289
Iteration 3/25 | Loss: 0.00102609
Iteration 4/25 | Loss: 0.00098111
Iteration 5/25 | Loss: 0.00097366
Iteration 6/25 | Loss: 0.00097200
Iteration 7/25 | Loss: 0.00097126
Iteration 8/25 | Loss: 0.00097083
Iteration 9/25 | Loss: 0.00097042
Iteration 10/25 | Loss: 0.00097508
Iteration 11/25 | Loss: 0.00097527
Iteration 12/25 | Loss: 0.00097466
Iteration 13/25 | Loss: 0.00097446
Iteration 14/25 | Loss: 0.00097402
Iteration 15/25 | Loss: 0.00097405
Iteration 16/25 | Loss: 0.00097393
Iteration 17/25 | Loss: 0.00097474
Iteration 18/25 | Loss: 0.00097371
Iteration 19/25 | Loss: 0.00097407
Iteration 20/25 | Loss: 0.00097256
Iteration 21/25 | Loss: 0.00097354
Iteration 22/25 | Loss: 0.00097265
Iteration 23/25 | Loss: 0.00097355
Iteration 24/25 | Loss: 0.00097307
Iteration 25/25 | Loss: 0.00097305

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27262652
Iteration 2/25 | Loss: 0.00105564
Iteration 3/25 | Loss: 0.00105564
Iteration 4/25 | Loss: 0.00105564
Iteration 5/25 | Loss: 0.00105563
Iteration 6/25 | Loss: 0.00105563
Iteration 7/25 | Loss: 0.00105563
Iteration 8/25 | Loss: 0.00105563
Iteration 9/25 | Loss: 0.00105563
Iteration 10/25 | Loss: 0.00105563
Iteration 11/25 | Loss: 0.00105563
Iteration 12/25 | Loss: 0.00105563
Iteration 13/25 | Loss: 0.00105563
Iteration 14/25 | Loss: 0.00105563
Iteration 15/25 | Loss: 0.00105563
Iteration 16/25 | Loss: 0.00105563
Iteration 17/25 | Loss: 0.00105563
Iteration 18/25 | Loss: 0.00105563
Iteration 19/25 | Loss: 0.00105563
Iteration 20/25 | Loss: 0.00105563
Iteration 21/25 | Loss: 0.00105563
Iteration 22/25 | Loss: 0.00105563
Iteration 23/25 | Loss: 0.00105563
Iteration 24/25 | Loss: 0.00105563
Iteration 25/25 | Loss: 0.00105563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105563
Iteration 2/1000 | Loss: 0.00004319
Iteration 3/1000 | Loss: 0.00006228
Iteration 4/1000 | Loss: 0.00002255
Iteration 5/1000 | Loss: 0.00004004
Iteration 6/1000 | Loss: 0.00002849
Iteration 7/1000 | Loss: 0.00004762
Iteration 8/1000 | Loss: 0.00001969
Iteration 9/1000 | Loss: 0.00004894
Iteration 10/1000 | Loss: 0.00007198
Iteration 11/1000 | Loss: 0.00006429
Iteration 12/1000 | Loss: 0.00007187
Iteration 13/1000 | Loss: 0.00004540
Iteration 14/1000 | Loss: 0.00004215
Iteration 15/1000 | Loss: 0.00003956
Iteration 16/1000 | Loss: 0.00004335
Iteration 17/1000 | Loss: 0.00003917
Iteration 18/1000 | Loss: 0.00004225
Iteration 19/1000 | Loss: 0.00005105
Iteration 20/1000 | Loss: 0.00004148
Iteration 21/1000 | Loss: 0.00004427
Iteration 22/1000 | Loss: 0.00001854
Iteration 23/1000 | Loss: 0.00003623
Iteration 24/1000 | Loss: 0.00003078
Iteration 25/1000 | Loss: 0.00003150
Iteration 26/1000 | Loss: 0.00001859
Iteration 27/1000 | Loss: 0.00003293
Iteration 28/1000 | Loss: 0.00002661
Iteration 29/1000 | Loss: 0.00001732
Iteration 30/1000 | Loss: 0.00003221
Iteration 31/1000 | Loss: 0.00002509
Iteration 32/1000 | Loss: 0.00001568
Iteration 33/1000 | Loss: 0.00002045
Iteration 34/1000 | Loss: 0.00003942
Iteration 35/1000 | Loss: 0.00002007
Iteration 36/1000 | Loss: 0.00003184
Iteration 37/1000 | Loss: 0.00003926
Iteration 38/1000 | Loss: 0.00001760
Iteration 39/1000 | Loss: 0.00001516
Iteration 40/1000 | Loss: 0.00001408
Iteration 41/1000 | Loss: 0.00001285
Iteration 42/1000 | Loss: 0.00001182
Iteration 43/1000 | Loss: 0.00001120
Iteration 44/1000 | Loss: 0.00001082
Iteration 45/1000 | Loss: 0.00001073
Iteration 46/1000 | Loss: 0.00001068
Iteration 47/1000 | Loss: 0.00001067
Iteration 48/1000 | Loss: 0.00001067
Iteration 49/1000 | Loss: 0.00001066
Iteration 50/1000 | Loss: 0.00001066
Iteration 51/1000 | Loss: 0.00001065
Iteration 52/1000 | Loss: 0.00001065
Iteration 53/1000 | Loss: 0.00001064
Iteration 54/1000 | Loss: 0.00001062
Iteration 55/1000 | Loss: 0.00001062
Iteration 56/1000 | Loss: 0.00001061
Iteration 57/1000 | Loss: 0.00001061
Iteration 58/1000 | Loss: 0.00001060
Iteration 59/1000 | Loss: 0.00001060
Iteration 60/1000 | Loss: 0.00001059
Iteration 61/1000 | Loss: 0.00001059
Iteration 62/1000 | Loss: 0.00001059
Iteration 63/1000 | Loss: 0.00001058
Iteration 64/1000 | Loss: 0.00001057
Iteration 65/1000 | Loss: 0.00001057
Iteration 66/1000 | Loss: 0.00001057
Iteration 67/1000 | Loss: 0.00001057
Iteration 68/1000 | Loss: 0.00001057
Iteration 69/1000 | Loss: 0.00001056
Iteration 70/1000 | Loss: 0.00001056
Iteration 71/1000 | Loss: 0.00001056
Iteration 72/1000 | Loss: 0.00001056
Iteration 73/1000 | Loss: 0.00001056
Iteration 74/1000 | Loss: 0.00001056
Iteration 75/1000 | Loss: 0.00001055
Iteration 76/1000 | Loss: 0.00001055
Iteration 77/1000 | Loss: 0.00001054
Iteration 78/1000 | Loss: 0.00001054
Iteration 79/1000 | Loss: 0.00001054
Iteration 80/1000 | Loss: 0.00001054
Iteration 81/1000 | Loss: 0.00001054
Iteration 82/1000 | Loss: 0.00001054
Iteration 83/1000 | Loss: 0.00001054
Iteration 84/1000 | Loss: 0.00001054
Iteration 85/1000 | Loss: 0.00001054
Iteration 86/1000 | Loss: 0.00001054
Iteration 87/1000 | Loss: 0.00001053
Iteration 88/1000 | Loss: 0.00001053
Iteration 89/1000 | Loss: 0.00001053
Iteration 90/1000 | Loss: 0.00001053
Iteration 91/1000 | Loss: 0.00001053
Iteration 92/1000 | Loss: 0.00001053
Iteration 93/1000 | Loss: 0.00001053
Iteration 94/1000 | Loss: 0.00001053
Iteration 95/1000 | Loss: 0.00001053
Iteration 96/1000 | Loss: 0.00001053
Iteration 97/1000 | Loss: 0.00001052
Iteration 98/1000 | Loss: 0.00001052
Iteration 99/1000 | Loss: 0.00001052
Iteration 100/1000 | Loss: 0.00001052
Iteration 101/1000 | Loss: 0.00001051
Iteration 102/1000 | Loss: 0.00001051
Iteration 103/1000 | Loss: 0.00001051
Iteration 104/1000 | Loss: 0.00001051
Iteration 105/1000 | Loss: 0.00001051
Iteration 106/1000 | Loss: 0.00001051
Iteration 107/1000 | Loss: 0.00001051
Iteration 108/1000 | Loss: 0.00001051
Iteration 109/1000 | Loss: 0.00001051
Iteration 110/1000 | Loss: 0.00001051
Iteration 111/1000 | Loss: 0.00001051
Iteration 112/1000 | Loss: 0.00001051
Iteration 113/1000 | Loss: 0.00001050
Iteration 114/1000 | Loss: 0.00001050
Iteration 115/1000 | Loss: 0.00001050
Iteration 116/1000 | Loss: 0.00001050
Iteration 117/1000 | Loss: 0.00001050
Iteration 118/1000 | Loss: 0.00001050
Iteration 119/1000 | Loss: 0.00001050
Iteration 120/1000 | Loss: 0.00001050
Iteration 121/1000 | Loss: 0.00001050
Iteration 122/1000 | Loss: 0.00001050
Iteration 123/1000 | Loss: 0.00001050
Iteration 124/1000 | Loss: 0.00001050
Iteration 125/1000 | Loss: 0.00001050
Iteration 126/1000 | Loss: 0.00001050
Iteration 127/1000 | Loss: 0.00001050
Iteration 128/1000 | Loss: 0.00001050
Iteration 129/1000 | Loss: 0.00001050
Iteration 130/1000 | Loss: 0.00001050
Iteration 131/1000 | Loss: 0.00001050
Iteration 132/1000 | Loss: 0.00001050
Iteration 133/1000 | Loss: 0.00001050
Iteration 134/1000 | Loss: 0.00001050
Iteration 135/1000 | Loss: 0.00001050
Iteration 136/1000 | Loss: 0.00001050
Iteration 137/1000 | Loss: 0.00001050
Iteration 138/1000 | Loss: 0.00001050
Iteration 139/1000 | Loss: 0.00001050
Iteration 140/1000 | Loss: 0.00001050
Iteration 141/1000 | Loss: 0.00001050
Iteration 142/1000 | Loss: 0.00001050
Iteration 143/1000 | Loss: 0.00001050
Iteration 144/1000 | Loss: 0.00001050
Iteration 145/1000 | Loss: 0.00001050
Iteration 146/1000 | Loss: 0.00001050
Iteration 147/1000 | Loss: 0.00001050
Iteration 148/1000 | Loss: 0.00001050
Iteration 149/1000 | Loss: 0.00001050
Iteration 150/1000 | Loss: 0.00001050
Iteration 151/1000 | Loss: 0.00001050
Iteration 152/1000 | Loss: 0.00001050
Iteration 153/1000 | Loss: 0.00001050
Iteration 154/1000 | Loss: 0.00001050
Iteration 155/1000 | Loss: 0.00001050
Iteration 156/1000 | Loss: 0.00001050
Iteration 157/1000 | Loss: 0.00001050
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 157. Stopping optimization.
Last 5 losses: [1.0495222340978216e-05, 1.0495222340978216e-05, 1.0495222340978216e-05, 1.0495222340978216e-05, 1.0495222340978216e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0495222340978216e-05

Optimization complete. Final v2v error: 2.719446897506714 mm

Highest mean error: 3.9143035411834717 mm for frame 61

Lowest mean error: 2.4153964519500732 mm for frame 166

Saving results

Total time: 118.29364466667175
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0009/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0009.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0009
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00407777
Iteration 2/25 | Loss: 0.00113251
Iteration 3/25 | Loss: 0.00102231
Iteration 4/25 | Loss: 0.00100640
Iteration 5/25 | Loss: 0.00100111
Iteration 6/25 | Loss: 0.00099991
Iteration 7/25 | Loss: 0.00099982
Iteration 8/25 | Loss: 0.00099982
Iteration 9/25 | Loss: 0.00099982
Iteration 10/25 | Loss: 0.00099982
Iteration 11/25 | Loss: 0.00099982
Iteration 12/25 | Loss: 0.00099982
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0009998212335631251, 0.0009998212335631251, 0.0009998212335631251, 0.0009998212335631251, 0.0009998212335631251]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009998212335631251

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.22925138
Iteration 2/25 | Loss: 0.00115012
Iteration 3/25 | Loss: 0.00115012
Iteration 4/25 | Loss: 0.00115012
Iteration 5/25 | Loss: 0.00115012
Iteration 6/25 | Loss: 0.00115012
Iteration 7/25 | Loss: 0.00115012
Iteration 8/25 | Loss: 0.00115012
Iteration 9/25 | Loss: 0.00115012
Iteration 10/25 | Loss: 0.00115012
Iteration 11/25 | Loss: 0.00115012
Iteration 12/25 | Loss: 0.00115012
Iteration 13/25 | Loss: 0.00115012
Iteration 14/25 | Loss: 0.00115012
Iteration 15/25 | Loss: 0.00115012
Iteration 16/25 | Loss: 0.00115012
Iteration 17/25 | Loss: 0.00115012
Iteration 18/25 | Loss: 0.00115012
Iteration 19/25 | Loss: 0.00115012
Iteration 20/25 | Loss: 0.00115012
Iteration 21/25 | Loss: 0.00115012
Iteration 22/25 | Loss: 0.00115012
Iteration 23/25 | Loss: 0.00115012
Iteration 24/25 | Loss: 0.00115012
Iteration 25/25 | Loss: 0.00115012

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00115012
Iteration 2/1000 | Loss: 0.00004379
Iteration 3/1000 | Loss: 0.00003024
Iteration 4/1000 | Loss: 0.00002425
Iteration 5/1000 | Loss: 0.00002167
Iteration 6/1000 | Loss: 0.00002021
Iteration 7/1000 | Loss: 0.00001924
Iteration 8/1000 | Loss: 0.00001860
Iteration 9/1000 | Loss: 0.00001822
Iteration 10/1000 | Loss: 0.00001790
Iteration 11/1000 | Loss: 0.00001762
Iteration 12/1000 | Loss: 0.00001742
Iteration 13/1000 | Loss: 0.00001741
Iteration 14/1000 | Loss: 0.00001729
Iteration 15/1000 | Loss: 0.00001725
Iteration 16/1000 | Loss: 0.00001724
Iteration 17/1000 | Loss: 0.00001723
Iteration 18/1000 | Loss: 0.00001719
Iteration 19/1000 | Loss: 0.00001716
Iteration 20/1000 | Loss: 0.00001716
Iteration 21/1000 | Loss: 0.00001715
Iteration 22/1000 | Loss: 0.00001714
Iteration 23/1000 | Loss: 0.00001714
Iteration 24/1000 | Loss: 0.00001713
Iteration 25/1000 | Loss: 0.00001712
Iteration 26/1000 | Loss: 0.00001712
Iteration 27/1000 | Loss: 0.00001711
Iteration 28/1000 | Loss: 0.00001711
Iteration 29/1000 | Loss: 0.00001711
Iteration 30/1000 | Loss: 0.00001710
Iteration 31/1000 | Loss: 0.00001710
Iteration 32/1000 | Loss: 0.00001709
Iteration 33/1000 | Loss: 0.00001709
Iteration 34/1000 | Loss: 0.00001708
Iteration 35/1000 | Loss: 0.00001708
Iteration 36/1000 | Loss: 0.00001708
Iteration 37/1000 | Loss: 0.00001707
Iteration 38/1000 | Loss: 0.00001707
Iteration 39/1000 | Loss: 0.00001707
Iteration 40/1000 | Loss: 0.00001706
Iteration 41/1000 | Loss: 0.00001705
Iteration 42/1000 | Loss: 0.00001705
Iteration 43/1000 | Loss: 0.00001705
Iteration 44/1000 | Loss: 0.00001705
Iteration 45/1000 | Loss: 0.00001705
Iteration 46/1000 | Loss: 0.00001705
Iteration 47/1000 | Loss: 0.00001705
Iteration 48/1000 | Loss: 0.00001704
Iteration 49/1000 | Loss: 0.00001704
Iteration 50/1000 | Loss: 0.00001704
Iteration 51/1000 | Loss: 0.00001704
Iteration 52/1000 | Loss: 0.00001704
Iteration 53/1000 | Loss: 0.00001704
Iteration 54/1000 | Loss: 0.00001703
Iteration 55/1000 | Loss: 0.00001703
Iteration 56/1000 | Loss: 0.00001702
Iteration 57/1000 | Loss: 0.00001702
Iteration 58/1000 | Loss: 0.00001702
Iteration 59/1000 | Loss: 0.00001702
Iteration 60/1000 | Loss: 0.00001701
Iteration 61/1000 | Loss: 0.00001700
Iteration 62/1000 | Loss: 0.00001700
Iteration 63/1000 | Loss: 0.00001699
Iteration 64/1000 | Loss: 0.00001699
Iteration 65/1000 | Loss: 0.00001699
Iteration 66/1000 | Loss: 0.00001699
Iteration 67/1000 | Loss: 0.00001699
Iteration 68/1000 | Loss: 0.00001698
Iteration 69/1000 | Loss: 0.00001698
Iteration 70/1000 | Loss: 0.00001698
Iteration 71/1000 | Loss: 0.00001698
Iteration 72/1000 | Loss: 0.00001697
Iteration 73/1000 | Loss: 0.00001697
Iteration 74/1000 | Loss: 0.00001697
Iteration 75/1000 | Loss: 0.00001697
Iteration 76/1000 | Loss: 0.00001697
Iteration 77/1000 | Loss: 0.00001696
Iteration 78/1000 | Loss: 0.00001696
Iteration 79/1000 | Loss: 0.00001696
Iteration 80/1000 | Loss: 0.00001696
Iteration 81/1000 | Loss: 0.00001696
Iteration 82/1000 | Loss: 0.00001696
Iteration 83/1000 | Loss: 0.00001696
Iteration 84/1000 | Loss: 0.00001696
Iteration 85/1000 | Loss: 0.00001695
Iteration 86/1000 | Loss: 0.00001695
Iteration 87/1000 | Loss: 0.00001695
Iteration 88/1000 | Loss: 0.00001695
Iteration 89/1000 | Loss: 0.00001695
Iteration 90/1000 | Loss: 0.00001695
Iteration 91/1000 | Loss: 0.00001695
Iteration 92/1000 | Loss: 0.00001694
Iteration 93/1000 | Loss: 0.00001694
Iteration 94/1000 | Loss: 0.00001694
Iteration 95/1000 | Loss: 0.00001694
Iteration 96/1000 | Loss: 0.00001694
Iteration 97/1000 | Loss: 0.00001694
Iteration 98/1000 | Loss: 0.00001694
Iteration 99/1000 | Loss: 0.00001694
Iteration 100/1000 | Loss: 0.00001694
Iteration 101/1000 | Loss: 0.00001694
Iteration 102/1000 | Loss: 0.00001694
Iteration 103/1000 | Loss: 0.00001694
Iteration 104/1000 | Loss: 0.00001694
Iteration 105/1000 | Loss: 0.00001694
Iteration 106/1000 | Loss: 0.00001694
Iteration 107/1000 | Loss: 0.00001694
Iteration 108/1000 | Loss: 0.00001694
Iteration 109/1000 | Loss: 0.00001694
Iteration 110/1000 | Loss: 0.00001694
Iteration 111/1000 | Loss: 0.00001694
Iteration 112/1000 | Loss: 0.00001694
Iteration 113/1000 | Loss: 0.00001694
Iteration 114/1000 | Loss: 0.00001694
Iteration 115/1000 | Loss: 0.00001694
Iteration 116/1000 | Loss: 0.00001694
Iteration 117/1000 | Loss: 0.00001694
Iteration 118/1000 | Loss: 0.00001694
Iteration 119/1000 | Loss: 0.00001694
Iteration 120/1000 | Loss: 0.00001694
Iteration 121/1000 | Loss: 0.00001694
Iteration 122/1000 | Loss: 0.00001694
Iteration 123/1000 | Loss: 0.00001694
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 123. Stopping optimization.
Last 5 losses: [1.6943271475611255e-05, 1.6943271475611255e-05, 1.6943271475611255e-05, 1.6943271475611255e-05, 1.6943271475611255e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6943271475611255e-05

Optimization complete. Final v2v error: 3.330066680908203 mm

Highest mean error: 3.705212116241455 mm for frame 152

Lowest mean error: 2.846280574798584 mm for frame 131

Saving results

Total time: 35.49047541618347
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00811641
Iteration 2/25 | Loss: 0.00134629
Iteration 3/25 | Loss: 0.00102365
Iteration 4/25 | Loss: 0.00099340
Iteration 5/25 | Loss: 0.00099048
Iteration 6/25 | Loss: 0.00099044
Iteration 7/25 | Loss: 0.00099044
Iteration 8/25 | Loss: 0.00099044
Iteration 9/25 | Loss: 0.00099044
Iteration 10/25 | Loss: 0.00099044
Iteration 11/25 | Loss: 0.00099044
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.0009904371108859777, 0.0009904371108859777, 0.0009904371108859777, 0.0009904371108859777, 0.0009904371108859777]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009904371108859777

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.23461127
Iteration 2/25 | Loss: 0.00097520
Iteration 3/25 | Loss: 0.00097519
Iteration 4/25 | Loss: 0.00097518
Iteration 5/25 | Loss: 0.00097518
Iteration 6/25 | Loss: 0.00097518
Iteration 7/25 | Loss: 0.00097518
Iteration 8/25 | Loss: 0.00097518
Iteration 9/25 | Loss: 0.00097518
Iteration 10/25 | Loss: 0.00097518
Iteration 11/25 | Loss: 0.00097518
Iteration 12/25 | Loss: 0.00097518
Iteration 13/25 | Loss: 0.00097518
Iteration 14/25 | Loss: 0.00097518
Iteration 15/25 | Loss: 0.00097518
Iteration 16/25 | Loss: 0.00097518
Iteration 17/25 | Loss: 0.00097518
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.000975182163529098, 0.000975182163529098, 0.000975182163529098, 0.000975182163529098, 0.000975182163529098]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000975182163529098

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00097518
Iteration 2/1000 | Loss: 0.00003331
Iteration 3/1000 | Loss: 0.00001979
Iteration 4/1000 | Loss: 0.00001543
Iteration 5/1000 | Loss: 0.00001378
Iteration 6/1000 | Loss: 0.00001289
Iteration 7/1000 | Loss: 0.00001232
Iteration 8/1000 | Loss: 0.00001200
Iteration 9/1000 | Loss: 0.00001163
Iteration 10/1000 | Loss: 0.00001153
Iteration 11/1000 | Loss: 0.00001134
Iteration 12/1000 | Loss: 0.00001126
Iteration 13/1000 | Loss: 0.00001115
Iteration 14/1000 | Loss: 0.00001110
Iteration 15/1000 | Loss: 0.00001102
Iteration 16/1000 | Loss: 0.00001102
Iteration 17/1000 | Loss: 0.00001102
Iteration 18/1000 | Loss: 0.00001102
Iteration 19/1000 | Loss: 0.00001102
Iteration 20/1000 | Loss: 0.00001102
Iteration 21/1000 | Loss: 0.00001101
Iteration 22/1000 | Loss: 0.00001101
Iteration 23/1000 | Loss: 0.00001100
Iteration 24/1000 | Loss: 0.00001100
Iteration 25/1000 | Loss: 0.00001099
Iteration 26/1000 | Loss: 0.00001098
Iteration 27/1000 | Loss: 0.00001097
Iteration 28/1000 | Loss: 0.00001097
Iteration 29/1000 | Loss: 0.00001097
Iteration 30/1000 | Loss: 0.00001096
Iteration 31/1000 | Loss: 0.00001096
Iteration 32/1000 | Loss: 0.00001096
Iteration 33/1000 | Loss: 0.00001095
Iteration 34/1000 | Loss: 0.00001095
Iteration 35/1000 | Loss: 0.00001095
Iteration 36/1000 | Loss: 0.00001095
Iteration 37/1000 | Loss: 0.00001095
Iteration 38/1000 | Loss: 0.00001095
Iteration 39/1000 | Loss: 0.00001095
Iteration 40/1000 | Loss: 0.00001095
Iteration 41/1000 | Loss: 0.00001095
Iteration 42/1000 | Loss: 0.00001095
Iteration 43/1000 | Loss: 0.00001095
Iteration 44/1000 | Loss: 0.00001094
Iteration 45/1000 | Loss: 0.00001094
Iteration 46/1000 | Loss: 0.00001094
Iteration 47/1000 | Loss: 0.00001094
Iteration 48/1000 | Loss: 0.00001094
Iteration 49/1000 | Loss: 0.00001094
Iteration 50/1000 | Loss: 0.00001094
Iteration 51/1000 | Loss: 0.00001093
Iteration 52/1000 | Loss: 0.00001093
Iteration 53/1000 | Loss: 0.00001093
Iteration 54/1000 | Loss: 0.00001093
Iteration 55/1000 | Loss: 0.00001093
Iteration 56/1000 | Loss: 0.00001093
Iteration 57/1000 | Loss: 0.00001093
Iteration 58/1000 | Loss: 0.00001092
Iteration 59/1000 | Loss: 0.00001092
Iteration 60/1000 | Loss: 0.00001092
Iteration 61/1000 | Loss: 0.00001092
Iteration 62/1000 | Loss: 0.00001092
Iteration 63/1000 | Loss: 0.00001092
Iteration 64/1000 | Loss: 0.00001092
Iteration 65/1000 | Loss: 0.00001092
Iteration 66/1000 | Loss: 0.00001092
Iteration 67/1000 | Loss: 0.00001092
Iteration 68/1000 | Loss: 0.00001092
Iteration 69/1000 | Loss: 0.00001092
Iteration 70/1000 | Loss: 0.00001092
Iteration 71/1000 | Loss: 0.00001091
Iteration 72/1000 | Loss: 0.00001090
Iteration 73/1000 | Loss: 0.00001090
Iteration 74/1000 | Loss: 0.00001090
Iteration 75/1000 | Loss: 0.00001090
Iteration 76/1000 | Loss: 0.00001090
Iteration 77/1000 | Loss: 0.00001090
Iteration 78/1000 | Loss: 0.00001090
Iteration 79/1000 | Loss: 0.00001090
Iteration 80/1000 | Loss: 0.00001090
Iteration 81/1000 | Loss: 0.00001090
Iteration 82/1000 | Loss: 0.00001090
Iteration 83/1000 | Loss: 0.00001090
Iteration 84/1000 | Loss: 0.00001090
Iteration 85/1000 | Loss: 0.00001090
Iteration 86/1000 | Loss: 0.00001090
Iteration 87/1000 | Loss: 0.00001090
Iteration 88/1000 | Loss: 0.00001090
Iteration 89/1000 | Loss: 0.00001090
Iteration 90/1000 | Loss: 0.00001090
Iteration 91/1000 | Loss: 0.00001090
Iteration 92/1000 | Loss: 0.00001090
Iteration 93/1000 | Loss: 0.00001090
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 93. Stopping optimization.
Last 5 losses: [1.0900539564318024e-05, 1.0900539564318024e-05, 1.0900539564318024e-05, 1.0900539564318024e-05, 1.0900539564318024e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.0900539564318024e-05

Optimization complete. Final v2v error: 2.81689190864563 mm

Highest mean error: 3.0451810359954834 mm for frame 119

Lowest mean error: 2.6434812545776367 mm for frame 1

Saving results

Total time: 29.44746470451355
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0020/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0020.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0020
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00414641
Iteration 2/25 | Loss: 0.00107790
Iteration 3/25 | Loss: 0.00096858
Iteration 4/25 | Loss: 0.00095989
Iteration 5/25 | Loss: 0.00095726
Iteration 6/25 | Loss: 0.00095690
Iteration 7/25 | Loss: 0.00095690
Iteration 8/25 | Loss: 0.00095690
Iteration 9/25 | Loss: 0.00095690
Iteration 10/25 | Loss: 0.00095690
Iteration 11/25 | Loss: 0.00095690
Iteration 12/25 | Loss: 0.00095690
Iteration 13/25 | Loss: 0.00095690
Iteration 14/25 | Loss: 0.00095690
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 14. Stopping optimization.
Last 5 losses: [0.00095690309535712, 0.00095690309535712, 0.00095690309535712, 0.00095690309535712, 0.00095690309535712]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00095690309535712

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.27633047
Iteration 2/25 | Loss: 0.00088877
Iteration 3/25 | Loss: 0.00088877
Iteration 4/25 | Loss: 0.00088877
Iteration 5/25 | Loss: 0.00088877
Iteration 6/25 | Loss: 0.00088877
Iteration 7/25 | Loss: 0.00088877
Iteration 8/25 | Loss: 0.00088877
Iteration 9/25 | Loss: 0.00088877
Iteration 10/25 | Loss: 0.00088877
Iteration 11/25 | Loss: 0.00088877
Iteration 12/25 | Loss: 0.00088877
Iteration 13/25 | Loss: 0.00088877
Iteration 14/25 | Loss: 0.00088877
Iteration 15/25 | Loss: 0.00088877
Iteration 16/25 | Loss: 0.00088877
Iteration 17/25 | Loss: 0.00088877
Iteration 18/25 | Loss: 0.00088877
Iteration 19/25 | Loss: 0.00088877
Iteration 20/25 | Loss: 0.00088877
Iteration 21/25 | Loss: 0.00088877
Iteration 22/25 | Loss: 0.00088877
Iteration 23/25 | Loss: 0.00088877
Iteration 24/25 | Loss: 0.00088877
Iteration 25/25 | Loss: 0.00088877

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00088877
Iteration 2/1000 | Loss: 0.00003526
Iteration 3/1000 | Loss: 0.00001981
Iteration 4/1000 | Loss: 0.00001592
Iteration 5/1000 | Loss: 0.00001411
Iteration 6/1000 | Loss: 0.00001325
Iteration 7/1000 | Loss: 0.00001267
Iteration 8/1000 | Loss: 0.00001216
Iteration 9/1000 | Loss: 0.00001180
Iteration 10/1000 | Loss: 0.00001162
Iteration 11/1000 | Loss: 0.00001161
Iteration 12/1000 | Loss: 0.00001150
Iteration 13/1000 | Loss: 0.00001148
Iteration 14/1000 | Loss: 0.00001147
Iteration 15/1000 | Loss: 0.00001147
Iteration 16/1000 | Loss: 0.00001147
Iteration 17/1000 | Loss: 0.00001146
Iteration 18/1000 | Loss: 0.00001146
Iteration 19/1000 | Loss: 0.00001146
Iteration 20/1000 | Loss: 0.00001145
Iteration 21/1000 | Loss: 0.00001144
Iteration 22/1000 | Loss: 0.00001144
Iteration 23/1000 | Loss: 0.00001144
Iteration 24/1000 | Loss: 0.00001144
Iteration 25/1000 | Loss: 0.00001143
Iteration 26/1000 | Loss: 0.00001143
Iteration 27/1000 | Loss: 0.00001143
Iteration 28/1000 | Loss: 0.00001138
Iteration 29/1000 | Loss: 0.00001137
Iteration 30/1000 | Loss: 0.00001137
Iteration 31/1000 | Loss: 0.00001137
Iteration 32/1000 | Loss: 0.00001136
Iteration 33/1000 | Loss: 0.00001136
Iteration 34/1000 | Loss: 0.00001135
Iteration 35/1000 | Loss: 0.00001135
Iteration 36/1000 | Loss: 0.00001134
Iteration 37/1000 | Loss: 0.00001134
Iteration 38/1000 | Loss: 0.00001134
Iteration 39/1000 | Loss: 0.00001134
Iteration 40/1000 | Loss: 0.00001134
Iteration 41/1000 | Loss: 0.00001133
Iteration 42/1000 | Loss: 0.00001133
Iteration 43/1000 | Loss: 0.00001133
Iteration 44/1000 | Loss: 0.00001133
Iteration 45/1000 | Loss: 0.00001133
Iteration 46/1000 | Loss: 0.00001132
Iteration 47/1000 | Loss: 0.00001132
Iteration 48/1000 | Loss: 0.00001132
Iteration 49/1000 | Loss: 0.00001131
Iteration 50/1000 | Loss: 0.00001131
Iteration 51/1000 | Loss: 0.00001131
Iteration 52/1000 | Loss: 0.00001131
Iteration 53/1000 | Loss: 0.00001130
Iteration 54/1000 | Loss: 0.00001130
Iteration 55/1000 | Loss: 0.00001130
Iteration 56/1000 | Loss: 0.00001130
Iteration 57/1000 | Loss: 0.00001130
Iteration 58/1000 | Loss: 0.00001130
Iteration 59/1000 | Loss: 0.00001130
Iteration 60/1000 | Loss: 0.00001130
Iteration 61/1000 | Loss: 0.00001130
Iteration 62/1000 | Loss: 0.00001130
Iteration 63/1000 | Loss: 0.00001130
Iteration 64/1000 | Loss: 0.00001130
Iteration 65/1000 | Loss: 0.00001129
Iteration 66/1000 | Loss: 0.00001129
Iteration 67/1000 | Loss: 0.00001129
Iteration 68/1000 | Loss: 0.00001129
Iteration 69/1000 | Loss: 0.00001129
Iteration 70/1000 | Loss: 0.00001128
Iteration 71/1000 | Loss: 0.00001128
Iteration 72/1000 | Loss: 0.00001128
Iteration 73/1000 | Loss: 0.00001128
Iteration 74/1000 | Loss: 0.00001128
Iteration 75/1000 | Loss: 0.00001128
Iteration 76/1000 | Loss: 0.00001128
Iteration 77/1000 | Loss: 0.00001128
Iteration 78/1000 | Loss: 0.00001128
Iteration 79/1000 | Loss: 0.00001128
Iteration 80/1000 | Loss: 0.00001127
Iteration 81/1000 | Loss: 0.00001127
Iteration 82/1000 | Loss: 0.00001127
Iteration 83/1000 | Loss: 0.00001127
Iteration 84/1000 | Loss: 0.00001127
Iteration 85/1000 | Loss: 0.00001127
Iteration 86/1000 | Loss: 0.00001126
Iteration 87/1000 | Loss: 0.00001126
Iteration 88/1000 | Loss: 0.00001126
Iteration 89/1000 | Loss: 0.00001126
Iteration 90/1000 | Loss: 0.00001126
Iteration 91/1000 | Loss: 0.00001125
Iteration 92/1000 | Loss: 0.00001125
Iteration 93/1000 | Loss: 0.00001125
Iteration 94/1000 | Loss: 0.00001125
Iteration 95/1000 | Loss: 0.00001125
Iteration 96/1000 | Loss: 0.00001125
Iteration 97/1000 | Loss: 0.00001125
Iteration 98/1000 | Loss: 0.00001125
Iteration 99/1000 | Loss: 0.00001125
Iteration 100/1000 | Loss: 0.00001125
Iteration 101/1000 | Loss: 0.00001125
Iteration 102/1000 | Loss: 0.00001125
Iteration 103/1000 | Loss: 0.00001125
Iteration 104/1000 | Loss: 0.00001125
Iteration 105/1000 | Loss: 0.00001125
Iteration 106/1000 | Loss: 0.00001125
Iteration 107/1000 | Loss: 0.00001125
Iteration 108/1000 | Loss: 0.00001125
Iteration 109/1000 | Loss: 0.00001125
Iteration 110/1000 | Loss: 0.00001125
Iteration 111/1000 | Loss: 0.00001125
Iteration 112/1000 | Loss: 0.00001125
Iteration 113/1000 | Loss: 0.00001125
Iteration 114/1000 | Loss: 0.00001125
Iteration 115/1000 | Loss: 0.00001125
Iteration 116/1000 | Loss: 0.00001125
Iteration 117/1000 | Loss: 0.00001125
Iteration 118/1000 | Loss: 0.00001125
Iteration 119/1000 | Loss: 0.00001125
Iteration 120/1000 | Loss: 0.00001125
Iteration 121/1000 | Loss: 0.00001125
Iteration 122/1000 | Loss: 0.00001125
Iteration 123/1000 | Loss: 0.00001125
Iteration 124/1000 | Loss: 0.00001125
Iteration 125/1000 | Loss: 0.00001125
Iteration 126/1000 | Loss: 0.00001125
Iteration 127/1000 | Loss: 0.00001125
Iteration 128/1000 | Loss: 0.00001125
Iteration 129/1000 | Loss: 0.00001125
Iteration 130/1000 | Loss: 0.00001125
Iteration 131/1000 | Loss: 0.00001125
Iteration 132/1000 | Loss: 0.00001125
Iteration 133/1000 | Loss: 0.00001125
Iteration 134/1000 | Loss: 0.00001125
Iteration 135/1000 | Loss: 0.00001125
Iteration 136/1000 | Loss: 0.00001125
Iteration 137/1000 | Loss: 0.00001125
Iteration 138/1000 | Loss: 0.00001125
Iteration 139/1000 | Loss: 0.00001125
Iteration 140/1000 | Loss: 0.00001125
Iteration 141/1000 | Loss: 0.00001125
Iteration 142/1000 | Loss: 0.00001125
Iteration 143/1000 | Loss: 0.00001125
Iteration 144/1000 | Loss: 0.00001125
Iteration 145/1000 | Loss: 0.00001125
Iteration 146/1000 | Loss: 0.00001125
Iteration 147/1000 | Loss: 0.00001125
Iteration 148/1000 | Loss: 0.00001125
Iteration 149/1000 | Loss: 0.00001125
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [1.1248101145611145e-05, 1.1248101145611145e-05, 1.1248101145611145e-05, 1.1248101145611145e-05, 1.1248101145611145e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1248101145611145e-05

Optimization complete. Final v2v error: 2.821887969970703 mm

Highest mean error: 2.9700634479522705 mm for frame 15

Lowest mean error: 2.6955559253692627 mm for frame 122

Saving results

Total time: 30.57118272781372
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00871998
Iteration 2/25 | Loss: 0.00128024
Iteration 3/25 | Loss: 0.00105586
Iteration 4/25 | Loss: 0.00103517
Iteration 5/25 | Loss: 0.00103081
Iteration 6/25 | Loss: 0.00102989
Iteration 7/25 | Loss: 0.00102969
Iteration 8/25 | Loss: 0.00102969
Iteration 9/25 | Loss: 0.00102969
Iteration 10/25 | Loss: 0.00102969
Iteration 11/25 | Loss: 0.00102969
Iteration 12/25 | Loss: 0.00102969
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.001029687817208469, 0.001029687817208469, 0.001029687817208469, 0.001029687817208469, 0.001029687817208469]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.001029687817208469

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.14902139
Iteration 2/25 | Loss: 0.00070075
Iteration 3/25 | Loss: 0.00070071
Iteration 4/25 | Loss: 0.00070071
Iteration 5/25 | Loss: 0.00070071
Iteration 6/25 | Loss: 0.00070071
Iteration 7/25 | Loss: 0.00070071
Iteration 8/25 | Loss: 0.00070071
Iteration 9/25 | Loss: 0.00070071
Iteration 10/25 | Loss: 0.00070071
Iteration 11/25 | Loss: 0.00070071
Iteration 12/25 | Loss: 0.00070071
Iteration 13/25 | Loss: 0.00070071
Iteration 14/25 | Loss: 0.00070071
Iteration 15/25 | Loss: 0.00070071
Iteration 16/25 | Loss: 0.00070071
Iteration 17/25 | Loss: 0.00070071
Iteration 18/25 | Loss: 0.00070071
Iteration 19/25 | Loss: 0.00070071
Iteration 20/25 | Loss: 0.00070071
Iteration 21/25 | Loss: 0.00070071
Iteration 22/25 | Loss: 0.00070071
Iteration 23/25 | Loss: 0.00070071
Iteration 24/25 | Loss: 0.00070071
Iteration 25/25 | Loss: 0.00070071

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070071
Iteration 2/1000 | Loss: 0.00005942
Iteration 3/1000 | Loss: 0.00003367
Iteration 4/1000 | Loss: 0.00002466
Iteration 5/1000 | Loss: 0.00002044
Iteration 6/1000 | Loss: 0.00001915
Iteration 7/1000 | Loss: 0.00001845
Iteration 8/1000 | Loss: 0.00001783
Iteration 9/1000 | Loss: 0.00001732
Iteration 10/1000 | Loss: 0.00001697
Iteration 11/1000 | Loss: 0.00001662
Iteration 12/1000 | Loss: 0.00001651
Iteration 13/1000 | Loss: 0.00001633
Iteration 14/1000 | Loss: 0.00001629
Iteration 15/1000 | Loss: 0.00001627
Iteration 16/1000 | Loss: 0.00001627
Iteration 17/1000 | Loss: 0.00001627
Iteration 18/1000 | Loss: 0.00001626
Iteration 19/1000 | Loss: 0.00001624
Iteration 20/1000 | Loss: 0.00001621
Iteration 21/1000 | Loss: 0.00001620
Iteration 22/1000 | Loss: 0.00001620
Iteration 23/1000 | Loss: 0.00001620
Iteration 24/1000 | Loss: 0.00001620
Iteration 25/1000 | Loss: 0.00001620
Iteration 26/1000 | Loss: 0.00001620
Iteration 27/1000 | Loss: 0.00001619
Iteration 28/1000 | Loss: 0.00001619
Iteration 29/1000 | Loss: 0.00001619
Iteration 30/1000 | Loss: 0.00001619
Iteration 31/1000 | Loss: 0.00001618
Iteration 32/1000 | Loss: 0.00001618
Iteration 33/1000 | Loss: 0.00001615
Iteration 34/1000 | Loss: 0.00001615
Iteration 35/1000 | Loss: 0.00001615
Iteration 36/1000 | Loss: 0.00001614
Iteration 37/1000 | Loss: 0.00001609
Iteration 38/1000 | Loss: 0.00001608
Iteration 39/1000 | Loss: 0.00001608
Iteration 40/1000 | Loss: 0.00001607
Iteration 41/1000 | Loss: 0.00001607
Iteration 42/1000 | Loss: 0.00001607
Iteration 43/1000 | Loss: 0.00001607
Iteration 44/1000 | Loss: 0.00001607
Iteration 45/1000 | Loss: 0.00001607
Iteration 46/1000 | Loss: 0.00001607
Iteration 47/1000 | Loss: 0.00001607
Iteration 48/1000 | Loss: 0.00001607
Iteration 49/1000 | Loss: 0.00001606
Iteration 50/1000 | Loss: 0.00001606
Iteration 51/1000 | Loss: 0.00001606
Iteration 52/1000 | Loss: 0.00001606
Iteration 53/1000 | Loss: 0.00001606
Iteration 54/1000 | Loss: 0.00001606
Iteration 55/1000 | Loss: 0.00001606
Iteration 56/1000 | Loss: 0.00001606
Iteration 57/1000 | Loss: 0.00001606
Iteration 58/1000 | Loss: 0.00001606
Iteration 59/1000 | Loss: 0.00001605
Iteration 60/1000 | Loss: 0.00001605
Iteration 61/1000 | Loss: 0.00001605
Iteration 62/1000 | Loss: 0.00001605
Iteration 63/1000 | Loss: 0.00001605
Iteration 64/1000 | Loss: 0.00001605
Iteration 65/1000 | Loss: 0.00001604
Iteration 66/1000 | Loss: 0.00001604
Iteration 67/1000 | Loss: 0.00001604
Iteration 68/1000 | Loss: 0.00001604
Iteration 69/1000 | Loss: 0.00001604
Iteration 70/1000 | Loss: 0.00001604
Iteration 71/1000 | Loss: 0.00001604
Iteration 72/1000 | Loss: 0.00001604
Iteration 73/1000 | Loss: 0.00001604
Iteration 74/1000 | Loss: 0.00001604
Iteration 75/1000 | Loss: 0.00001604
Iteration 76/1000 | Loss: 0.00001603
Iteration 77/1000 | Loss: 0.00001603
Iteration 78/1000 | Loss: 0.00001603
Iteration 79/1000 | Loss: 0.00001602
Iteration 80/1000 | Loss: 0.00001602
Iteration 81/1000 | Loss: 0.00001602
Iteration 82/1000 | Loss: 0.00001602
Iteration 83/1000 | Loss: 0.00001602
Iteration 84/1000 | Loss: 0.00001602
Iteration 85/1000 | Loss: 0.00001602
Iteration 86/1000 | Loss: 0.00001602
Iteration 87/1000 | Loss: 0.00001601
Iteration 88/1000 | Loss: 0.00001601
Iteration 89/1000 | Loss: 0.00001601
Iteration 90/1000 | Loss: 0.00001601
Iteration 91/1000 | Loss: 0.00001601
Iteration 92/1000 | Loss: 0.00001601
Iteration 93/1000 | Loss: 0.00001601
Iteration 94/1000 | Loss: 0.00001601
Iteration 95/1000 | Loss: 0.00001601
Iteration 96/1000 | Loss: 0.00001601
Iteration 97/1000 | Loss: 0.00001601
Iteration 98/1000 | Loss: 0.00001601
Iteration 99/1000 | Loss: 0.00001600
Iteration 100/1000 | Loss: 0.00001600
Iteration 101/1000 | Loss: 0.00001600
Iteration 102/1000 | Loss: 0.00001600
Iteration 103/1000 | Loss: 0.00001600
Iteration 104/1000 | Loss: 0.00001600
Iteration 105/1000 | Loss: 0.00001600
Iteration 106/1000 | Loss: 0.00001600
Iteration 107/1000 | Loss: 0.00001600
Iteration 108/1000 | Loss: 0.00001600
Iteration 109/1000 | Loss: 0.00001600
Iteration 110/1000 | Loss: 0.00001600
Iteration 111/1000 | Loss: 0.00001600
Iteration 112/1000 | Loss: 0.00001600
Iteration 113/1000 | Loss: 0.00001600
Iteration 114/1000 | Loss: 0.00001599
Iteration 115/1000 | Loss: 0.00001599
Iteration 116/1000 | Loss: 0.00001599
Iteration 117/1000 | Loss: 0.00001599
Iteration 118/1000 | Loss: 0.00001599
Iteration 119/1000 | Loss: 0.00001599
Iteration 120/1000 | Loss: 0.00001599
Iteration 121/1000 | Loss: 0.00001599
Iteration 122/1000 | Loss: 0.00001599
Iteration 123/1000 | Loss: 0.00001599
Iteration 124/1000 | Loss: 0.00001599
Iteration 125/1000 | Loss: 0.00001599
Iteration 126/1000 | Loss: 0.00001599
Iteration 127/1000 | Loss: 0.00001599
Iteration 128/1000 | Loss: 0.00001599
Iteration 129/1000 | Loss: 0.00001599
Iteration 130/1000 | Loss: 0.00001599
Iteration 131/1000 | Loss: 0.00001599
Iteration 132/1000 | Loss: 0.00001599
Iteration 133/1000 | Loss: 0.00001599
Iteration 134/1000 | Loss: 0.00001599
Iteration 135/1000 | Loss: 0.00001599
Iteration 136/1000 | Loss: 0.00001599
Iteration 137/1000 | Loss: 0.00001599
Iteration 138/1000 | Loss: 0.00001599
Iteration 139/1000 | Loss: 0.00001599
Iteration 140/1000 | Loss: 0.00001599
Iteration 141/1000 | Loss: 0.00001599
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [1.5988078303053044e-05, 1.5988078303053044e-05, 1.5988078303053044e-05, 1.5988078303053044e-05, 1.5988078303053044e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.5988078303053044e-05

Optimization complete. Final v2v error: 3.3848235607147217 mm

Highest mean error: 3.7834670543670654 mm for frame 99

Lowest mean error: 3.0623884201049805 mm for frame 83

Saving results

Total time: 36.37576460838318
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0015/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0015.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0015
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00533161
Iteration 2/25 | Loss: 0.00109935
Iteration 3/25 | Loss: 0.00098055
Iteration 4/25 | Loss: 0.00097213
Iteration 5/25 | Loss: 0.00096923
Iteration 6/25 | Loss: 0.00096920
Iteration 7/25 | Loss: 0.00096920
Iteration 8/25 | Loss: 0.00096920
Iteration 9/25 | Loss: 0.00096920
Iteration 10/25 | Loss: 0.00096920
Iteration 11/25 | Loss: 0.00096920
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 11. Stopping optimization.
Last 5 losses: [0.00096919946372509, 0.00096919946372509, 0.00096919946372509, 0.00096919946372509, 0.00096919946372509]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00096919946372509

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 10.17544651
Iteration 2/25 | Loss: 0.00067557
Iteration 3/25 | Loss: 0.00067555
Iteration 4/25 | Loss: 0.00067555
Iteration 5/25 | Loss: 0.00067555
Iteration 6/25 | Loss: 0.00067555
Iteration 7/25 | Loss: 0.00067555
Iteration 8/25 | Loss: 0.00067555
Iteration 9/25 | Loss: 0.00067555
Iteration 10/25 | Loss: 0.00067555
Iteration 11/25 | Loss: 0.00067555
Iteration 12/25 | Loss: 0.00067555
Iteration 13/25 | Loss: 0.00067555
Iteration 14/25 | Loss: 0.00067555
Iteration 15/25 | Loss: 0.00067555
Iteration 16/25 | Loss: 0.00067555
Iteration 17/25 | Loss: 0.00067555
Iteration 18/25 | Loss: 0.00067555
Iteration 19/25 | Loss: 0.00067555
Iteration 20/25 | Loss: 0.00067555
Iteration 21/25 | Loss: 0.00067555
Iteration 22/25 | Loss: 0.00067555
Iteration 23/25 | Loss: 0.00067555
Iteration 24/25 | Loss: 0.00067555
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 24. Stopping optimization.
Last 5 losses: [0.000675550545565784, 0.000675550545565784, 0.000675550545565784, 0.000675550545565784, 0.000675550545565784]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.000675550545565784

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00067555
Iteration 2/1000 | Loss: 0.00002715
Iteration 3/1000 | Loss: 0.00001634
Iteration 4/1000 | Loss: 0.00001437
Iteration 5/1000 | Loss: 0.00001359
Iteration 6/1000 | Loss: 0.00001308
Iteration 7/1000 | Loss: 0.00001275
Iteration 8/1000 | Loss: 0.00001253
Iteration 9/1000 | Loss: 0.00001232
Iteration 10/1000 | Loss: 0.00001224
Iteration 11/1000 | Loss: 0.00001224
Iteration 12/1000 | Loss: 0.00001218
Iteration 13/1000 | Loss: 0.00001217
Iteration 14/1000 | Loss: 0.00001217
Iteration 15/1000 | Loss: 0.00001216
Iteration 16/1000 | Loss: 0.00001214
Iteration 17/1000 | Loss: 0.00001212
Iteration 18/1000 | Loss: 0.00001212
Iteration 19/1000 | Loss: 0.00001211
Iteration 20/1000 | Loss: 0.00001211
Iteration 21/1000 | Loss: 0.00001211
Iteration 22/1000 | Loss: 0.00001210
Iteration 23/1000 | Loss: 0.00001210
Iteration 24/1000 | Loss: 0.00001209
Iteration 25/1000 | Loss: 0.00001209
Iteration 26/1000 | Loss: 0.00001209
Iteration 27/1000 | Loss: 0.00001208
Iteration 28/1000 | Loss: 0.00001208
Iteration 29/1000 | Loss: 0.00001208
Iteration 30/1000 | Loss: 0.00001207
Iteration 31/1000 | Loss: 0.00001207
Iteration 32/1000 | Loss: 0.00001207
Iteration 33/1000 | Loss: 0.00001207
Iteration 34/1000 | Loss: 0.00001206
Iteration 35/1000 | Loss: 0.00001206
Iteration 36/1000 | Loss: 0.00001206
Iteration 37/1000 | Loss: 0.00001205
Iteration 38/1000 | Loss: 0.00001205
Iteration 39/1000 | Loss: 0.00001205
Iteration 40/1000 | Loss: 0.00001204
Iteration 41/1000 | Loss: 0.00001204
Iteration 42/1000 | Loss: 0.00001204
Iteration 43/1000 | Loss: 0.00001204
Iteration 44/1000 | Loss: 0.00001204
Iteration 45/1000 | Loss: 0.00001203
Iteration 46/1000 | Loss: 0.00001203
Iteration 47/1000 | Loss: 0.00001203
Iteration 48/1000 | Loss: 0.00001203
Iteration 49/1000 | Loss: 0.00001203
Iteration 50/1000 | Loss: 0.00001203
Iteration 51/1000 | Loss: 0.00001202
Iteration 52/1000 | Loss: 0.00001202
Iteration 53/1000 | Loss: 0.00001202
Iteration 54/1000 | Loss: 0.00001202
Iteration 55/1000 | Loss: 0.00001202
Iteration 56/1000 | Loss: 0.00001202
Iteration 57/1000 | Loss: 0.00001202
Iteration 58/1000 | Loss: 0.00001202
Iteration 59/1000 | Loss: 0.00001202
Iteration 60/1000 | Loss: 0.00001201
Iteration 61/1000 | Loss: 0.00001201
Iteration 62/1000 | Loss: 0.00001201
Iteration 63/1000 | Loss: 0.00001201
Iteration 64/1000 | Loss: 0.00001201
Iteration 65/1000 | Loss: 0.00001201
Iteration 66/1000 | Loss: 0.00001201
Iteration 67/1000 | Loss: 0.00001201
Iteration 68/1000 | Loss: 0.00001201
Iteration 69/1000 | Loss: 0.00001201
Iteration 70/1000 | Loss: 0.00001201
Iteration 71/1000 | Loss: 0.00001201
Iteration 72/1000 | Loss: 0.00001201
Iteration 73/1000 | Loss: 0.00001200
Iteration 74/1000 | Loss: 0.00001200
Iteration 75/1000 | Loss: 0.00001200
Iteration 76/1000 | Loss: 0.00001200
Iteration 77/1000 | Loss: 0.00001200
Iteration 78/1000 | Loss: 0.00001200
Iteration 79/1000 | Loss: 0.00001200
Iteration 80/1000 | Loss: 0.00001200
Iteration 81/1000 | Loss: 0.00001200
Iteration 82/1000 | Loss: 0.00001199
Iteration 83/1000 | Loss: 0.00001199
Iteration 84/1000 | Loss: 0.00001199
Iteration 85/1000 | Loss: 0.00001199
Iteration 86/1000 | Loss: 0.00001199
Iteration 87/1000 | Loss: 0.00001198
Iteration 88/1000 | Loss: 0.00001198
Iteration 89/1000 | Loss: 0.00001198
Iteration 90/1000 | Loss: 0.00001198
Iteration 91/1000 | Loss: 0.00001198
Iteration 92/1000 | Loss: 0.00001198
Iteration 93/1000 | Loss: 0.00001198
Iteration 94/1000 | Loss: 0.00001198
Iteration 95/1000 | Loss: 0.00001198
Iteration 96/1000 | Loss: 0.00001198
Iteration 97/1000 | Loss: 0.00001197
Iteration 98/1000 | Loss: 0.00001197
Iteration 99/1000 | Loss: 0.00001197
Iteration 100/1000 | Loss: 0.00001197
Iteration 101/1000 | Loss: 0.00001196
Iteration 102/1000 | Loss: 0.00001196
Iteration 103/1000 | Loss: 0.00001195
Iteration 104/1000 | Loss: 0.00001195
Iteration 105/1000 | Loss: 0.00001195
Iteration 106/1000 | Loss: 0.00001195
Iteration 107/1000 | Loss: 0.00001195
Iteration 108/1000 | Loss: 0.00001195
Iteration 109/1000 | Loss: 0.00001195
Iteration 110/1000 | Loss: 0.00001195
Iteration 111/1000 | Loss: 0.00001195
Iteration 112/1000 | Loss: 0.00001194
Iteration 113/1000 | Loss: 0.00001194
Iteration 114/1000 | Loss: 0.00001194
Iteration 115/1000 | Loss: 0.00001194
Iteration 116/1000 | Loss: 0.00001194
Iteration 117/1000 | Loss: 0.00001194
Iteration 118/1000 | Loss: 0.00001193
Iteration 119/1000 | Loss: 0.00001193
Iteration 120/1000 | Loss: 0.00001193
Iteration 121/1000 | Loss: 0.00001193
Iteration 122/1000 | Loss: 0.00001193
Iteration 123/1000 | Loss: 0.00001193
Iteration 124/1000 | Loss: 0.00001193
Iteration 125/1000 | Loss: 0.00001193
Iteration 126/1000 | Loss: 0.00001193
Iteration 127/1000 | Loss: 0.00001193
Iteration 128/1000 | Loss: 0.00001193
Iteration 129/1000 | Loss: 0.00001193
Iteration 130/1000 | Loss: 0.00001193
Iteration 131/1000 | Loss: 0.00001193
Iteration 132/1000 | Loss: 0.00001193
Iteration 133/1000 | Loss: 0.00001193
Iteration 134/1000 | Loss: 0.00001193
Iteration 135/1000 | Loss: 0.00001193
Iteration 136/1000 | Loss: 0.00001193
Iteration 137/1000 | Loss: 0.00001192
Iteration 138/1000 | Loss: 0.00001192
Iteration 139/1000 | Loss: 0.00001192
Iteration 140/1000 | Loss: 0.00001192
Iteration 141/1000 | Loss: 0.00001192
Iteration 142/1000 | Loss: 0.00001192
Iteration 143/1000 | Loss: 0.00001192
Iteration 144/1000 | Loss: 0.00001192
Iteration 145/1000 | Loss: 0.00001192
Iteration 146/1000 | Loss: 0.00001192
Iteration 147/1000 | Loss: 0.00001192
Iteration 148/1000 | Loss: 0.00001192
Iteration 149/1000 | Loss: 0.00001191
Iteration 150/1000 | Loss: 0.00001191
Iteration 151/1000 | Loss: 0.00001191
Iteration 152/1000 | Loss: 0.00001191
Iteration 153/1000 | Loss: 0.00001191
Iteration 154/1000 | Loss: 0.00001191
Iteration 155/1000 | Loss: 0.00001191
Iteration 156/1000 | Loss: 0.00001191
Iteration 157/1000 | Loss: 0.00001191
Iteration 158/1000 | Loss: 0.00001191
Iteration 159/1000 | Loss: 0.00001191
Iteration 160/1000 | Loss: 0.00001191
Iteration 161/1000 | Loss: 0.00001191
Iteration 162/1000 | Loss: 0.00001191
Iteration 163/1000 | Loss: 0.00001191
Iteration 164/1000 | Loss: 0.00001191
Iteration 165/1000 | Loss: 0.00001191
Iteration 166/1000 | Loss: 0.00001191
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 166. Stopping optimization.
Last 5 losses: [1.1912412446690723e-05, 1.1912412446690723e-05, 1.1912412446690723e-05, 1.1912412446690723e-05, 1.1912412446690723e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.1912412446690723e-05

Optimization complete. Final v2v error: 2.9200868606567383 mm

Highest mean error: 3.4733362197875977 mm for frame 43

Lowest mean error: 2.6707823276519775 mm for frame 29

Saving results

Total time: 36.51347756385803
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/female_30_us_2580/0010/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0010.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/female_30_us_2580/0010
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01069525
Iteration 2/25 | Loss: 0.00236565
Iteration 3/25 | Loss: 0.00180868
Iteration 4/25 | Loss: 0.00151674
Iteration 5/25 | Loss: 0.00128024
Iteration 6/25 | Loss: 0.00119446
Iteration 7/25 | Loss: 0.00116857
Iteration 8/25 | Loss: 0.00114853
Iteration 9/25 | Loss: 0.00115298
Iteration 10/25 | Loss: 0.00113200
Iteration 11/25 | Loss: 0.00113195
Iteration 12/25 | Loss: 0.00112497
Iteration 13/25 | Loss: 0.00116551
Iteration 14/25 | Loss: 0.00116199
Iteration 15/25 | Loss: 0.00116133
Iteration 16/25 | Loss: 0.00112078
Iteration 17/25 | Loss: 0.00111620
Iteration 18/25 | Loss: 0.00115320
Iteration 19/25 | Loss: 0.00110451
Iteration 20/25 | Loss: 0.00109994
Iteration 21/25 | Loss: 0.00109946
Iteration 22/25 | Loss: 0.00109936
Iteration 23/25 | Loss: 0.00109936
Iteration 24/25 | Loss: 0.00109936
Iteration 25/25 | Loss: 0.00109936

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.28230107
Iteration 2/25 | Loss: 0.00233024
Iteration 3/25 | Loss: 0.00212426
Iteration 4/25 | Loss: 0.00212426
Iteration 5/25 | Loss: 0.00212426
Iteration 6/25 | Loss: 0.00212426
Iteration 7/25 | Loss: 0.00212426
Iteration 8/25 | Loss: 0.00212426
Iteration 9/25 | Loss: 0.00212426
Iteration 10/25 | Loss: 0.00212426
Iteration 11/25 | Loss: 0.00212426
Iteration 12/25 | Loss: 0.00212426
Iteration 13/25 | Loss: 0.00212426
Iteration 14/25 | Loss: 0.00212426
Iteration 15/25 | Loss: 0.00212426
Iteration 16/25 | Loss: 0.00212426
Iteration 17/25 | Loss: 0.00212426
Iteration 18/25 | Loss: 0.00212426
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 18. Stopping optimization.
Last 5 losses: [0.0021242587827146053, 0.0021242587827146053, 0.0021242587827146053, 0.0021242587827146053, 0.0021242587827146053]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0021242587827146053

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00212426
Iteration 2/1000 | Loss: 0.00028462
Iteration 3/1000 | Loss: 0.00037839
Iteration 4/1000 | Loss: 0.00014023
Iteration 5/1000 | Loss: 0.00011570
Iteration 6/1000 | Loss: 0.00093169
Iteration 7/1000 | Loss: 0.00020951
Iteration 8/1000 | Loss: 0.00017347
Iteration 9/1000 | Loss: 0.00062009
Iteration 10/1000 | Loss: 0.00033062
Iteration 11/1000 | Loss: 0.00027161
Iteration 12/1000 | Loss: 0.00063505
Iteration 13/1000 | Loss: 0.00011346
Iteration 14/1000 | Loss: 0.00010559
Iteration 15/1000 | Loss: 0.00008892
Iteration 16/1000 | Loss: 0.00013057
Iteration 17/1000 | Loss: 0.00008243
Iteration 18/1000 | Loss: 0.00030627
Iteration 19/1000 | Loss: 0.00056987
Iteration 20/1000 | Loss: 0.00029720
Iteration 21/1000 | Loss: 0.00084157
Iteration 22/1000 | Loss: 0.00009497
Iteration 23/1000 | Loss: 0.00035217
Iteration 24/1000 | Loss: 0.00034841
Iteration 25/1000 | Loss: 0.00023620
Iteration 26/1000 | Loss: 0.00018823
Iteration 27/1000 | Loss: 0.00009467
Iteration 28/1000 | Loss: 0.00061324
Iteration 29/1000 | Loss: 0.00077114
Iteration 30/1000 | Loss: 0.00061866
Iteration 31/1000 | Loss: 0.00008006
Iteration 32/1000 | Loss: 0.00010298
Iteration 33/1000 | Loss: 0.00007643
Iteration 34/1000 | Loss: 0.00008939
Iteration 35/1000 | Loss: 0.00006052
Iteration 36/1000 | Loss: 0.00005756
Iteration 37/1000 | Loss: 0.00005595
Iteration 38/1000 | Loss: 0.00005517
Iteration 39/1000 | Loss: 0.00005448
Iteration 40/1000 | Loss: 0.00005393
Iteration 41/1000 | Loss: 0.00005351
Iteration 42/1000 | Loss: 0.00005313
Iteration 43/1000 | Loss: 0.00035104
Iteration 44/1000 | Loss: 0.00268780
Iteration 45/1000 | Loss: 0.00462796
Iteration 46/1000 | Loss: 0.00241722
Iteration 47/1000 | Loss: 0.00284090
Iteration 48/1000 | Loss: 0.00137859
Iteration 49/1000 | Loss: 0.00047540
Iteration 50/1000 | Loss: 0.00025112
Iteration 51/1000 | Loss: 0.00007405
Iteration 52/1000 | Loss: 0.00004722
Iteration 53/1000 | Loss: 0.00022794
Iteration 54/1000 | Loss: 0.00002797
Iteration 55/1000 | Loss: 0.00015934
Iteration 56/1000 | Loss: 0.00002284
Iteration 57/1000 | Loss: 0.00002063
Iteration 58/1000 | Loss: 0.00017771
Iteration 59/1000 | Loss: 0.00001862
Iteration 60/1000 | Loss: 0.00001757
Iteration 61/1000 | Loss: 0.00001686
Iteration 62/1000 | Loss: 0.00001624
Iteration 63/1000 | Loss: 0.00001577
Iteration 64/1000 | Loss: 0.00001544
Iteration 65/1000 | Loss: 0.00001524
Iteration 66/1000 | Loss: 0.00001509
Iteration 67/1000 | Loss: 0.00001491
Iteration 68/1000 | Loss: 0.00001491
Iteration 69/1000 | Loss: 0.00001490
Iteration 70/1000 | Loss: 0.00001490
Iteration 71/1000 | Loss: 0.00001490
Iteration 72/1000 | Loss: 0.00001485
Iteration 73/1000 | Loss: 0.00001485
Iteration 74/1000 | Loss: 0.00001483
Iteration 75/1000 | Loss: 0.00001482
Iteration 76/1000 | Loss: 0.00001482
Iteration 77/1000 | Loss: 0.00001482
Iteration 78/1000 | Loss: 0.00001481
Iteration 79/1000 | Loss: 0.00001481
Iteration 80/1000 | Loss: 0.00001480
Iteration 81/1000 | Loss: 0.00001480
Iteration 82/1000 | Loss: 0.00001480
Iteration 83/1000 | Loss: 0.00001480
Iteration 84/1000 | Loss: 0.00001479
Iteration 85/1000 | Loss: 0.00001479
Iteration 86/1000 | Loss: 0.00001479
Iteration 87/1000 | Loss: 0.00001478
Iteration 88/1000 | Loss: 0.00001478
Iteration 89/1000 | Loss: 0.00001478
Iteration 90/1000 | Loss: 0.00001478
Iteration 91/1000 | Loss: 0.00001477
Iteration 92/1000 | Loss: 0.00001477
Iteration 93/1000 | Loss: 0.00001476
Iteration 94/1000 | Loss: 0.00001476
Iteration 95/1000 | Loss: 0.00001475
Iteration 96/1000 | Loss: 0.00001475
Iteration 97/1000 | Loss: 0.00001475
Iteration 98/1000 | Loss: 0.00001474
Iteration 99/1000 | Loss: 0.00001474
Iteration 100/1000 | Loss: 0.00001474
Iteration 101/1000 | Loss: 0.00001474
Iteration 102/1000 | Loss: 0.00001473
Iteration 103/1000 | Loss: 0.00001473
Iteration 104/1000 | Loss: 0.00001472
Iteration 105/1000 | Loss: 0.00001472
Iteration 106/1000 | Loss: 0.00001472
Iteration 107/1000 | Loss: 0.00001472
Iteration 108/1000 | Loss: 0.00001471
Iteration 109/1000 | Loss: 0.00001471
Iteration 110/1000 | Loss: 0.00001471
Iteration 111/1000 | Loss: 0.00001471
Iteration 112/1000 | Loss: 0.00001471
Iteration 113/1000 | Loss: 0.00001470
Iteration 114/1000 | Loss: 0.00001470
Iteration 115/1000 | Loss: 0.00001470
Iteration 116/1000 | Loss: 0.00001470
Iteration 117/1000 | Loss: 0.00001469
Iteration 118/1000 | Loss: 0.00001469
Iteration 119/1000 | Loss: 0.00001469
Iteration 120/1000 | Loss: 0.00001469
Iteration 121/1000 | Loss: 0.00001468
Iteration 122/1000 | Loss: 0.00001468
Iteration 123/1000 | Loss: 0.00001468
Iteration 124/1000 | Loss: 0.00001468
Iteration 125/1000 | Loss: 0.00001468
Iteration 126/1000 | Loss: 0.00001468
Iteration 127/1000 | Loss: 0.00001467
Iteration 128/1000 | Loss: 0.00001467
Iteration 129/1000 | Loss: 0.00001467
Iteration 130/1000 | Loss: 0.00001467
Iteration 131/1000 | Loss: 0.00001466
Iteration 132/1000 | Loss: 0.00001466
Iteration 133/1000 | Loss: 0.00001466
Iteration 134/1000 | Loss: 0.00001466
Iteration 135/1000 | Loss: 0.00001466
Iteration 136/1000 | Loss: 0.00001466
Iteration 137/1000 | Loss: 0.00001466
Iteration 138/1000 | Loss: 0.00001466
Iteration 139/1000 | Loss: 0.00001466
Iteration 140/1000 | Loss: 0.00001466
Iteration 141/1000 | Loss: 0.00001466
Iteration 142/1000 | Loss: 0.00001466
Iteration 143/1000 | Loss: 0.00001466
Iteration 144/1000 | Loss: 0.00001466
Iteration 145/1000 | Loss: 0.00001466
Iteration 146/1000 | Loss: 0.00001465
Iteration 147/1000 | Loss: 0.00001465
Iteration 148/1000 | Loss: 0.00001465
Iteration 149/1000 | Loss: 0.00001465
Iteration 150/1000 | Loss: 0.00001465
Iteration 151/1000 | Loss: 0.00001465
Iteration 152/1000 | Loss: 0.00001464
Iteration 153/1000 | Loss: 0.00001464
Iteration 154/1000 | Loss: 0.00001464
Iteration 155/1000 | Loss: 0.00001464
Iteration 156/1000 | Loss: 0.00001464
Iteration 157/1000 | Loss: 0.00001463
Iteration 158/1000 | Loss: 0.00001463
Iteration 159/1000 | Loss: 0.00001463
Iteration 160/1000 | Loss: 0.00001463
Iteration 161/1000 | Loss: 0.00001463
Iteration 162/1000 | Loss: 0.00001463
Iteration 163/1000 | Loss: 0.00001463
Iteration 164/1000 | Loss: 0.00001463
Iteration 165/1000 | Loss: 0.00001463
Iteration 166/1000 | Loss: 0.00001463
Iteration 167/1000 | Loss: 0.00001463
Iteration 168/1000 | Loss: 0.00001463
Iteration 169/1000 | Loss: 0.00001462
Iteration 170/1000 | Loss: 0.00001462
Iteration 171/1000 | Loss: 0.00001462
Iteration 172/1000 | Loss: 0.00001462
Iteration 173/1000 | Loss: 0.00001462
Iteration 174/1000 | Loss: 0.00001462
Iteration 175/1000 | Loss: 0.00001462
Iteration 176/1000 | Loss: 0.00001461
Iteration 177/1000 | Loss: 0.00001461
Iteration 178/1000 | Loss: 0.00001461
Iteration 179/1000 | Loss: 0.00001461
Iteration 180/1000 | Loss: 0.00001461
Iteration 181/1000 | Loss: 0.00001461
Iteration 182/1000 | Loss: 0.00001461
Iteration 183/1000 | Loss: 0.00001461
Iteration 184/1000 | Loss: 0.00001461
Iteration 185/1000 | Loss: 0.00001461
Iteration 186/1000 | Loss: 0.00001461
Iteration 187/1000 | Loss: 0.00001461
Iteration 188/1000 | Loss: 0.00001461
Iteration 189/1000 | Loss: 0.00001461
Iteration 190/1000 | Loss: 0.00001461
Iteration 191/1000 | Loss: 0.00001460
Iteration 192/1000 | Loss: 0.00001460
Iteration 193/1000 | Loss: 0.00001460
Iteration 194/1000 | Loss: 0.00001460
Iteration 195/1000 | Loss: 0.00001460
Iteration 196/1000 | Loss: 0.00001460
Iteration 197/1000 | Loss: 0.00001460
Iteration 198/1000 | Loss: 0.00001460
Iteration 199/1000 | Loss: 0.00001460
Iteration 200/1000 | Loss: 0.00001460
Iteration 201/1000 | Loss: 0.00001460
Iteration 202/1000 | Loss: 0.00001460
Iteration 203/1000 | Loss: 0.00001460
Iteration 204/1000 | Loss: 0.00001460
Iteration 205/1000 | Loss: 0.00001460
Iteration 206/1000 | Loss: 0.00001460
Iteration 207/1000 | Loss: 0.00001460
Iteration 208/1000 | Loss: 0.00001460
Iteration 209/1000 | Loss: 0.00001460
Iteration 210/1000 | Loss: 0.00001460
Iteration 211/1000 | Loss: 0.00001460
Iteration 212/1000 | Loss: 0.00001459
Iteration 213/1000 | Loss: 0.00001459
Iteration 214/1000 | Loss: 0.00001459
Iteration 215/1000 | Loss: 0.00001459
Iteration 216/1000 | Loss: 0.00001459
Iteration 217/1000 | Loss: 0.00001459
Iteration 218/1000 | Loss: 0.00001459
Iteration 219/1000 | Loss: 0.00001459
Iteration 220/1000 | Loss: 0.00001459
Iteration 221/1000 | Loss: 0.00001459
Iteration 222/1000 | Loss: 0.00001459
Iteration 223/1000 | Loss: 0.00001459
Iteration 224/1000 | Loss: 0.00001459
Iteration 225/1000 | Loss: 0.00001459
Iteration 226/1000 | Loss: 0.00001458
Iteration 227/1000 | Loss: 0.00001458
Iteration 228/1000 | Loss: 0.00001458
Iteration 229/1000 | Loss: 0.00001458
Iteration 230/1000 | Loss: 0.00001458
Iteration 231/1000 | Loss: 0.00001458
Iteration 232/1000 | Loss: 0.00001458
Iteration 233/1000 | Loss: 0.00001458
Iteration 234/1000 | Loss: 0.00001458
Iteration 235/1000 | Loss: 0.00001458
Iteration 236/1000 | Loss: 0.00001458
Iteration 237/1000 | Loss: 0.00001458
Iteration 238/1000 | Loss: 0.00001458
Iteration 239/1000 | Loss: 0.00001458
Iteration 240/1000 | Loss: 0.00001458
Iteration 241/1000 | Loss: 0.00001458
Iteration 242/1000 | Loss: 0.00001458
Iteration 243/1000 | Loss: 0.00001458
Iteration 244/1000 | Loss: 0.00001458
Iteration 245/1000 | Loss: 0.00001458
Iteration 246/1000 | Loss: 0.00001458
Iteration 247/1000 | Loss: 0.00001458
Iteration 248/1000 | Loss: 0.00001458
Iteration 249/1000 | Loss: 0.00001457
Iteration 250/1000 | Loss: 0.00001457
Iteration 251/1000 | Loss: 0.00001457
Iteration 252/1000 | Loss: 0.00001457
Iteration 253/1000 | Loss: 0.00001457
Iteration 254/1000 | Loss: 0.00001457
Iteration 255/1000 | Loss: 0.00001457
Iteration 256/1000 | Loss: 0.00001457
Iteration 257/1000 | Loss: 0.00001457
Iteration 258/1000 | Loss: 0.00001457
Iteration 259/1000 | Loss: 0.00001457
Iteration 260/1000 | Loss: 0.00001457
Iteration 261/1000 | Loss: 0.00001457
Iteration 262/1000 | Loss: 0.00001457
Iteration 263/1000 | Loss: 0.00001457
Iteration 264/1000 | Loss: 0.00001457
Iteration 265/1000 | Loss: 0.00001457
Iteration 266/1000 | Loss: 0.00001457
Iteration 267/1000 | Loss: 0.00001457
Iteration 268/1000 | Loss: 0.00001457
Iteration 269/1000 | Loss: 0.00001457
Iteration 270/1000 | Loss: 0.00001457
Iteration 271/1000 | Loss: 0.00001457
Iteration 272/1000 | Loss: 0.00001457
Iteration 273/1000 | Loss: 0.00001457
Iteration 274/1000 | Loss: 0.00001457
Iteration 275/1000 | Loss: 0.00001457
Iteration 276/1000 | Loss: 0.00001457
Iteration 277/1000 | Loss: 0.00001457
Iteration 278/1000 | Loss: 0.00001457
Iteration 279/1000 | Loss: 0.00001457
Iteration 280/1000 | Loss: 0.00001457
Iteration 281/1000 | Loss: 0.00001457
Iteration 282/1000 | Loss: 0.00001457
Iteration 283/1000 | Loss: 0.00001457
Iteration 284/1000 | Loss: 0.00001457
Iteration 285/1000 | Loss: 0.00001457
Iteration 286/1000 | Loss: 0.00001457
Iteration 287/1000 | Loss: 0.00001457
Iteration 288/1000 | Loss: 0.00001457
Iteration 289/1000 | Loss: 0.00001457
Iteration 290/1000 | Loss: 0.00001457
Iteration 291/1000 | Loss: 0.00001457
Iteration 292/1000 | Loss: 0.00001457
Iteration 293/1000 | Loss: 0.00001457
Iteration 294/1000 | Loss: 0.00001457
Iteration 295/1000 | Loss: 0.00001457
Iteration 296/1000 | Loss: 0.00001457
Iteration 297/1000 | Loss: 0.00001457
Iteration 298/1000 | Loss: 0.00001457
Iteration 299/1000 | Loss: 0.00001457
Iteration 300/1000 | Loss: 0.00001457
Iteration 301/1000 | Loss: 0.00001457
Iteration 302/1000 | Loss: 0.00001457
Iteration 303/1000 | Loss: 0.00001457
Iteration 304/1000 | Loss: 0.00001457
Iteration 305/1000 | Loss: 0.00001457
Iteration 306/1000 | Loss: 0.00001457
Iteration 307/1000 | Loss: 0.00001457
Iteration 308/1000 | Loss: 0.00001457
Iteration 309/1000 | Loss: 0.00001457
Iteration 310/1000 | Loss: 0.00001457
Iteration 311/1000 | Loss: 0.00001457
Iteration 312/1000 | Loss: 0.00001457
Iteration 313/1000 | Loss: 0.00001457
Iteration 314/1000 | Loss: 0.00001457
Iteration 315/1000 | Loss: 0.00001457
Iteration 316/1000 | Loss: 0.00001457
Iteration 317/1000 | Loss: 0.00001457
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 317. Stopping optimization.
Last 5 losses: [1.4566245226887986e-05, 1.4566245226887986e-05, 1.4566245226887986e-05, 1.4566245226887986e-05, 1.4566245226887986e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.4566245226887986e-05

Optimization complete. Final v2v error: 2.9051506519317627 mm

Highest mean error: 10.995513916015625 mm for frame 65

Lowest mean error: 2.4711737632751465 mm for frame 57

Saving results

Total time: 145.40067839622498
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0008/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0008.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0008
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00607971
Iteration 2/25 | Loss: 0.00207318
Iteration 3/25 | Loss: 0.00132352
Iteration 4/25 | Loss: 0.00127268
Iteration 5/25 | Loss: 0.00125839
Iteration 6/25 | Loss: 0.00125516
Iteration 7/25 | Loss: 0.00125631
Iteration 8/25 | Loss: 0.00125504
Iteration 9/25 | Loss: 0.00125199
Iteration 10/25 | Loss: 0.00125261
Iteration 11/25 | Loss: 0.00125124
Iteration 12/25 | Loss: 0.00124643
Iteration 13/25 | Loss: 0.00124380
Iteration 14/25 | Loss: 0.00124166
Iteration 15/25 | Loss: 0.00124222
Iteration 16/25 | Loss: 0.00124085
Iteration 17/25 | Loss: 0.00123977
Iteration 18/25 | Loss: 0.00124046
Iteration 19/25 | Loss: 0.00124024
Iteration 20/25 | Loss: 0.00123748
Iteration 21/25 | Loss: 0.00123610
Iteration 22/25 | Loss: 0.00123481
Iteration 23/25 | Loss: 0.00123723
Iteration 24/25 | Loss: 0.00123752
Iteration 25/25 | Loss: 0.00123290

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.06913960
Iteration 2/25 | Loss: 0.00070372
Iteration 3/25 | Loss: 0.00070371
Iteration 4/25 | Loss: 0.00070371
Iteration 5/25 | Loss: 0.00070371
Iteration 6/25 | Loss: 0.00070371
Iteration 7/25 | Loss: 0.00070371
Iteration 8/25 | Loss: 0.00070371
Iteration 9/25 | Loss: 0.00070371
Iteration 10/25 | Loss: 0.00070371
Iteration 11/25 | Loss: 0.00070371
Iteration 12/25 | Loss: 0.00070371
Iteration 13/25 | Loss: 0.00070371
Iteration 14/25 | Loss: 0.00070371
Iteration 15/25 | Loss: 0.00070371
Iteration 16/25 | Loss: 0.00070371
Iteration 17/25 | Loss: 0.00070371
Iteration 18/25 | Loss: 0.00070371
Iteration 19/25 | Loss: 0.00070371
Iteration 20/25 | Loss: 0.00070371
Iteration 21/25 | Loss: 0.00070371
Iteration 22/25 | Loss: 0.00070371
Iteration 23/25 | Loss: 0.00070371
Iteration 24/25 | Loss: 0.00070371
Iteration 25/25 | Loss: 0.00070371

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00070371
Iteration 2/1000 | Loss: 0.00009717
Iteration 3/1000 | Loss: 0.00006339
Iteration 4/1000 | Loss: 0.00005621
Iteration 5/1000 | Loss: 0.00005227
Iteration 6/1000 | Loss: 0.00005074
Iteration 7/1000 | Loss: 0.00004962
Iteration 8/1000 | Loss: 0.00004859
Iteration 9/1000 | Loss: 0.00004776
Iteration 10/1000 | Loss: 0.00004718
Iteration 11/1000 | Loss: 0.00004646
Iteration 12/1000 | Loss: 0.00004568
Iteration 13/1000 | Loss: 0.00004522
Iteration 14/1000 | Loss: 0.00004489
Iteration 15/1000 | Loss: 0.00004465
Iteration 16/1000 | Loss: 0.00004448
Iteration 17/1000 | Loss: 0.00004432
Iteration 18/1000 | Loss: 0.00004430
Iteration 19/1000 | Loss: 0.00004423
Iteration 20/1000 | Loss: 0.00004413
Iteration 21/1000 | Loss: 0.00004412
Iteration 22/1000 | Loss: 0.00004411
Iteration 23/1000 | Loss: 0.00004411
Iteration 24/1000 | Loss: 0.00004411
Iteration 25/1000 | Loss: 0.00004411
Iteration 26/1000 | Loss: 0.00004410
Iteration 27/1000 | Loss: 0.00004406
Iteration 28/1000 | Loss: 0.00004406
Iteration 29/1000 | Loss: 0.00004406
Iteration 30/1000 | Loss: 0.00004404
Iteration 31/1000 | Loss: 0.00004404
Iteration 32/1000 | Loss: 0.00004403
Iteration 33/1000 | Loss: 0.00004403
Iteration 34/1000 | Loss: 0.00004403
Iteration 35/1000 | Loss: 0.00004402
Iteration 36/1000 | Loss: 0.00004401
Iteration 37/1000 | Loss: 0.00004401
Iteration 38/1000 | Loss: 0.00004401
Iteration 39/1000 | Loss: 0.00004401
Iteration 40/1000 | Loss: 0.00004401
Iteration 41/1000 | Loss: 0.00004401
Iteration 42/1000 | Loss: 0.00004400
Iteration 43/1000 | Loss: 0.00004400
Iteration 44/1000 | Loss: 0.00004399
Iteration 45/1000 | Loss: 0.00004399
Iteration 46/1000 | Loss: 0.00004399
Iteration 47/1000 | Loss: 0.00004398
Iteration 48/1000 | Loss: 0.00004398
Iteration 49/1000 | Loss: 0.00004398
Iteration 50/1000 | Loss: 0.00004397
Iteration 51/1000 | Loss: 0.00004397
Iteration 52/1000 | Loss: 0.00004397
Iteration 53/1000 | Loss: 0.00004397
Iteration 54/1000 | Loss: 0.00004397
Iteration 55/1000 | Loss: 0.00004396
Iteration 56/1000 | Loss: 0.00004396
Iteration 57/1000 | Loss: 0.00004396
Iteration 58/1000 | Loss: 0.00004395
Iteration 59/1000 | Loss: 0.00004395
Iteration 60/1000 | Loss: 0.00004395
Iteration 61/1000 | Loss: 0.00004395
Iteration 62/1000 | Loss: 0.00004394
Iteration 63/1000 | Loss: 0.00004394
Iteration 64/1000 | Loss: 0.00004394
Iteration 65/1000 | Loss: 0.00004394
Iteration 66/1000 | Loss: 0.00004394
Iteration 67/1000 | Loss: 0.00004394
Iteration 68/1000 | Loss: 0.00004393
Iteration 69/1000 | Loss: 0.00004393
Iteration 70/1000 | Loss: 0.00004393
Iteration 71/1000 | Loss: 0.00004393
Iteration 72/1000 | Loss: 0.00004393
Iteration 73/1000 | Loss: 0.00004392
Iteration 74/1000 | Loss: 0.00004392
Iteration 75/1000 | Loss: 0.00004392
Iteration 76/1000 | Loss: 0.00004392
Iteration 77/1000 | Loss: 0.00004392
Iteration 78/1000 | Loss: 0.00004392
Iteration 79/1000 | Loss: 0.00004392
Iteration 80/1000 | Loss: 0.00004391
Iteration 81/1000 | Loss: 0.00004391
Iteration 82/1000 | Loss: 0.00004391
Iteration 83/1000 | Loss: 0.00004391
Iteration 84/1000 | Loss: 0.00004391
Iteration 85/1000 | Loss: 0.00004391
Iteration 86/1000 | Loss: 0.00004391
Iteration 87/1000 | Loss: 0.00004391
Iteration 88/1000 | Loss: 0.00004391
Iteration 89/1000 | Loss: 0.00004391
Iteration 90/1000 | Loss: 0.00004391
Iteration 91/1000 | Loss: 0.00004390
Iteration 92/1000 | Loss: 0.00004390
Iteration 93/1000 | Loss: 0.00004390
Iteration 94/1000 | Loss: 0.00004390
Iteration 95/1000 | Loss: 0.00004390
Iteration 96/1000 | Loss: 0.00004390
Iteration 97/1000 | Loss: 0.00004390
Iteration 98/1000 | Loss: 0.00004390
Iteration 99/1000 | Loss: 0.00004390
Iteration 100/1000 | Loss: 0.00004390
Iteration 101/1000 | Loss: 0.00004389
Iteration 102/1000 | Loss: 0.00004389
Iteration 103/1000 | Loss: 0.00004389
Iteration 104/1000 | Loss: 0.00004389
Iteration 105/1000 | Loss: 0.00004389
Iteration 106/1000 | Loss: 0.00004389
Iteration 107/1000 | Loss: 0.00004389
Iteration 108/1000 | Loss: 0.00004389
Iteration 109/1000 | Loss: 0.00004389
Iteration 110/1000 | Loss: 0.00004389
Iteration 111/1000 | Loss: 0.00004389
Iteration 112/1000 | Loss: 0.00004389
Iteration 113/1000 | Loss: 0.00004389
Iteration 114/1000 | Loss: 0.00004388
Iteration 115/1000 | Loss: 0.00004388
Iteration 116/1000 | Loss: 0.00004388
Iteration 117/1000 | Loss: 0.00004388
Iteration 118/1000 | Loss: 0.00004388
Iteration 119/1000 | Loss: 0.00004388
Iteration 120/1000 | Loss: 0.00004388
Iteration 121/1000 | Loss: 0.00004388
Iteration 122/1000 | Loss: 0.00004388
Iteration 123/1000 | Loss: 0.00004388
Iteration 124/1000 | Loss: 0.00004388
Iteration 125/1000 | Loss: 0.00004388
Iteration 126/1000 | Loss: 0.00004388
Iteration 127/1000 | Loss: 0.00004388
Iteration 128/1000 | Loss: 0.00004388
Iteration 129/1000 | Loss: 0.00004388
Iteration 130/1000 | Loss: 0.00004388
Iteration 131/1000 | Loss: 0.00004388
Iteration 132/1000 | Loss: 0.00004388
Iteration 133/1000 | Loss: 0.00004388
Iteration 134/1000 | Loss: 0.00004388
Iteration 135/1000 | Loss: 0.00004388
Iteration 136/1000 | Loss: 0.00004388
Iteration 137/1000 | Loss: 0.00004388
Iteration 138/1000 | Loss: 0.00004388
Iteration 139/1000 | Loss: 0.00004388
Iteration 140/1000 | Loss: 0.00004388
Iteration 141/1000 | Loss: 0.00004388
Iteration 142/1000 | Loss: 0.00004388
Iteration 143/1000 | Loss: 0.00004388
Iteration 144/1000 | Loss: 0.00004388
Iteration 145/1000 | Loss: 0.00004388
Iteration 146/1000 | Loss: 0.00004388
Iteration 147/1000 | Loss: 0.00004388
Iteration 148/1000 | Loss: 0.00004388
Iteration 149/1000 | Loss: 0.00004388
Iteration 150/1000 | Loss: 0.00004388
Iteration 151/1000 | Loss: 0.00004388
Iteration 152/1000 | Loss: 0.00004388
Iteration 153/1000 | Loss: 0.00004388
Iteration 154/1000 | Loss: 0.00004388
Iteration 155/1000 | Loss: 0.00004388
Iteration 156/1000 | Loss: 0.00004388
Iteration 157/1000 | Loss: 0.00004388
Iteration 158/1000 | Loss: 0.00004388
Iteration 159/1000 | Loss: 0.00004388
Iteration 160/1000 | Loss: 0.00004388
Iteration 161/1000 | Loss: 0.00004388
Iteration 162/1000 | Loss: 0.00004388
Iteration 163/1000 | Loss: 0.00004388
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 163. Stopping optimization.
Last 5 losses: [4.387652006698772e-05, 4.387652006698772e-05, 4.387652006698772e-05, 4.387652006698772e-05, 4.387652006698772e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.387652006698772e-05

Optimization complete. Final v2v error: 5.294499397277832 mm

Highest mean error: 6.789769649505615 mm for frame 21

Lowest mean error: 4.052302360534668 mm for frame 213

Saving results

Total time: 92.07186818122864
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0014/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0014.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0014
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00889168
Iteration 2/25 | Loss: 0.00150095
Iteration 3/25 | Loss: 0.00125962
Iteration 4/25 | Loss: 0.00121702
Iteration 5/25 | Loss: 0.00120221
Iteration 6/25 | Loss: 0.00119938
Iteration 7/25 | Loss: 0.00119864
Iteration 8/25 | Loss: 0.00119864
Iteration 9/25 | Loss: 0.00119864
Iteration 10/25 | Loss: 0.00119864
Iteration 11/25 | Loss: 0.00119864
Iteration 12/25 | Loss: 0.00119864
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.00119863823056221, 0.00119863823056221, 0.00119863823056221, 0.00119863823056221, 0.00119863823056221]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00119863823056221

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.18487227
Iteration 2/25 | Loss: 0.00064197
Iteration 3/25 | Loss: 0.00064197
Iteration 4/25 | Loss: 0.00064197
Iteration 5/25 | Loss: 0.00064197
Iteration 6/25 | Loss: 0.00064197
Iteration 7/25 | Loss: 0.00064197
Iteration 8/25 | Loss: 0.00064197
Iteration 9/25 | Loss: 0.00064197
Iteration 10/25 | Loss: 0.00064197
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 10. Stopping optimization.
Last 5 losses: [0.0006419656565412879, 0.0006419656565412879, 0.0006419656565412879, 0.0006419656565412879, 0.0006419656565412879]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006419656565412879

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00064197
Iteration 2/1000 | Loss: 0.00005241
Iteration 3/1000 | Loss: 0.00004194
Iteration 4/1000 | Loss: 0.00003816
Iteration 5/1000 | Loss: 0.00003652
Iteration 6/1000 | Loss: 0.00003523
Iteration 7/1000 | Loss: 0.00003432
Iteration 8/1000 | Loss: 0.00003376
Iteration 9/1000 | Loss: 0.00003315
Iteration 10/1000 | Loss: 0.00003284
Iteration 11/1000 | Loss: 0.00003244
Iteration 12/1000 | Loss: 0.00003219
Iteration 13/1000 | Loss: 0.00003197
Iteration 14/1000 | Loss: 0.00003183
Iteration 15/1000 | Loss: 0.00003181
Iteration 16/1000 | Loss: 0.00003179
Iteration 17/1000 | Loss: 0.00003178
Iteration 18/1000 | Loss: 0.00003177
Iteration 19/1000 | Loss: 0.00003177
Iteration 20/1000 | Loss: 0.00003176
Iteration 21/1000 | Loss: 0.00003176
Iteration 22/1000 | Loss: 0.00003175
Iteration 23/1000 | Loss: 0.00003175
Iteration 24/1000 | Loss: 0.00003175
Iteration 25/1000 | Loss: 0.00003174
Iteration 26/1000 | Loss: 0.00003174
Iteration 27/1000 | Loss: 0.00003173
Iteration 28/1000 | Loss: 0.00003172
Iteration 29/1000 | Loss: 0.00003172
Iteration 30/1000 | Loss: 0.00003171
Iteration 31/1000 | Loss: 0.00003171
Iteration 32/1000 | Loss: 0.00003171
Iteration 33/1000 | Loss: 0.00003171
Iteration 34/1000 | Loss: 0.00003170
Iteration 35/1000 | Loss: 0.00003170
Iteration 36/1000 | Loss: 0.00003170
Iteration 37/1000 | Loss: 0.00003170
Iteration 38/1000 | Loss: 0.00003170
Iteration 39/1000 | Loss: 0.00003170
Iteration 40/1000 | Loss: 0.00003169
Iteration 41/1000 | Loss: 0.00003169
Iteration 42/1000 | Loss: 0.00003169
Iteration 43/1000 | Loss: 0.00003169
Iteration 44/1000 | Loss: 0.00003169
Iteration 45/1000 | Loss: 0.00003169
Iteration 46/1000 | Loss: 0.00003169
Iteration 47/1000 | Loss: 0.00003168
Iteration 48/1000 | Loss: 0.00003168
Iteration 49/1000 | Loss: 0.00003168
Iteration 50/1000 | Loss: 0.00003168
Iteration 51/1000 | Loss: 0.00003168
Iteration 52/1000 | Loss: 0.00003168
Iteration 53/1000 | Loss: 0.00003168
Iteration 54/1000 | Loss: 0.00003167
Iteration 55/1000 | Loss: 0.00003167
Iteration 56/1000 | Loss: 0.00003167
Iteration 57/1000 | Loss: 0.00003167
Iteration 58/1000 | Loss: 0.00003166
Iteration 59/1000 | Loss: 0.00003166
Iteration 60/1000 | Loss: 0.00003166
Iteration 61/1000 | Loss: 0.00003165
Iteration 62/1000 | Loss: 0.00003165
Iteration 63/1000 | Loss: 0.00003165
Iteration 64/1000 | Loss: 0.00003165
Iteration 65/1000 | Loss: 0.00003165
Iteration 66/1000 | Loss: 0.00003164
Iteration 67/1000 | Loss: 0.00003164
Iteration 68/1000 | Loss: 0.00003164
Iteration 69/1000 | Loss: 0.00003164
Iteration 70/1000 | Loss: 0.00003164
Iteration 71/1000 | Loss: 0.00003164
Iteration 72/1000 | Loss: 0.00003163
Iteration 73/1000 | Loss: 0.00003163
Iteration 74/1000 | Loss: 0.00003163
Iteration 75/1000 | Loss: 0.00003163
Iteration 76/1000 | Loss: 0.00003163
Iteration 77/1000 | Loss: 0.00003163
Iteration 78/1000 | Loss: 0.00003163
Iteration 79/1000 | Loss: 0.00003163
Iteration 80/1000 | Loss: 0.00003162
Iteration 81/1000 | Loss: 0.00003162
Iteration 82/1000 | Loss: 0.00003162
Iteration 83/1000 | Loss: 0.00003162
Iteration 84/1000 | Loss: 0.00003162
Iteration 85/1000 | Loss: 0.00003162
Iteration 86/1000 | Loss: 0.00003162
Iteration 87/1000 | Loss: 0.00003162
Iteration 88/1000 | Loss: 0.00003162
Iteration 89/1000 | Loss: 0.00003162
Iteration 90/1000 | Loss: 0.00003162
Iteration 91/1000 | Loss: 0.00003162
Iteration 92/1000 | Loss: 0.00003161
Iteration 93/1000 | Loss: 0.00003161
Iteration 94/1000 | Loss: 0.00003161
Iteration 95/1000 | Loss: 0.00003161
Iteration 96/1000 | Loss: 0.00003161
Iteration 97/1000 | Loss: 0.00003161
Iteration 98/1000 | Loss: 0.00003161
Iteration 99/1000 | Loss: 0.00003161
Iteration 100/1000 | Loss: 0.00003160
Iteration 101/1000 | Loss: 0.00003160
Iteration 102/1000 | Loss: 0.00003160
Iteration 103/1000 | Loss: 0.00003160
Iteration 104/1000 | Loss: 0.00003160
Iteration 105/1000 | Loss: 0.00003160
Iteration 106/1000 | Loss: 0.00003160
Iteration 107/1000 | Loss: 0.00003160
Iteration 108/1000 | Loss: 0.00003160
Iteration 109/1000 | Loss: 0.00003160
Iteration 110/1000 | Loss: 0.00003160
Iteration 111/1000 | Loss: 0.00003159
Iteration 112/1000 | Loss: 0.00003159
Iteration 113/1000 | Loss: 0.00003159
Iteration 114/1000 | Loss: 0.00003159
Iteration 115/1000 | Loss: 0.00003159
Iteration 116/1000 | Loss: 0.00003159
Iteration 117/1000 | Loss: 0.00003159
Iteration 118/1000 | Loss: 0.00003159
Iteration 119/1000 | Loss: 0.00003158
Iteration 120/1000 | Loss: 0.00003158
Iteration 121/1000 | Loss: 0.00003158
Iteration 122/1000 | Loss: 0.00003158
Iteration 123/1000 | Loss: 0.00003158
Iteration 124/1000 | Loss: 0.00003157
Iteration 125/1000 | Loss: 0.00003157
Iteration 126/1000 | Loss: 0.00003157
Iteration 127/1000 | Loss: 0.00003157
Iteration 128/1000 | Loss: 0.00003157
Iteration 129/1000 | Loss: 0.00003157
Iteration 130/1000 | Loss: 0.00003157
Iteration 131/1000 | Loss: 0.00003157
Iteration 132/1000 | Loss: 0.00003157
Iteration 133/1000 | Loss: 0.00003156
Iteration 134/1000 | Loss: 0.00003156
Iteration 135/1000 | Loss: 0.00003156
Iteration 136/1000 | Loss: 0.00003156
Iteration 137/1000 | Loss: 0.00003155
Iteration 138/1000 | Loss: 0.00003155
Iteration 139/1000 | Loss: 0.00003155
Iteration 140/1000 | Loss: 0.00003155
Iteration 141/1000 | Loss: 0.00003155
Iteration 142/1000 | Loss: 0.00003155
Iteration 143/1000 | Loss: 0.00003155
Iteration 144/1000 | Loss: 0.00003155
Iteration 145/1000 | Loss: 0.00003155
Iteration 146/1000 | Loss: 0.00003155
Iteration 147/1000 | Loss: 0.00003155
Iteration 148/1000 | Loss: 0.00003155
Iteration 149/1000 | Loss: 0.00003155
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 149. Stopping optimization.
Last 5 losses: [3.1549327104585245e-05, 3.1549327104585245e-05, 3.1549327104585245e-05, 3.1549327104585245e-05, 3.1549327104585245e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.1549327104585245e-05

Optimization complete. Final v2v error: 4.8038434982299805 mm

Highest mean error: 5.859786510467529 mm for frame 114

Lowest mean error: 3.9738974571228027 mm for frame 238

Saving results

Total time: 44.02420783042908
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0001/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0001.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0001
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01135226
Iteration 2/25 | Loss: 0.00251594
Iteration 3/25 | Loss: 0.00198191
Iteration 4/25 | Loss: 0.00187694
Iteration 5/25 | Loss: 0.00122892
Iteration 6/25 | Loss: 0.00107570
Iteration 7/25 | Loss: 0.00099470
Iteration 8/25 | Loss: 0.00093996
Iteration 9/25 | Loss: 0.00093228
Iteration 10/25 | Loss: 0.00091840
Iteration 11/25 | Loss: 0.00091368
Iteration 12/25 | Loss: 0.00091264
Iteration 13/25 | Loss: 0.00091173
Iteration 14/25 | Loss: 0.00090509
Iteration 15/25 | Loss: 0.00090391
Iteration 16/25 | Loss: 0.00090206
Iteration 17/25 | Loss: 0.00090357
Iteration 18/25 | Loss: 0.00090312
Iteration 19/25 | Loss: 0.00090388
Iteration 20/25 | Loss: 0.00090106
Iteration 21/25 | Loss: 0.00089937
Iteration 22/25 | Loss: 0.00089950
Iteration 23/25 | Loss: 0.00089937
Iteration 24/25 | Loss: 0.00089777
Iteration 25/25 | Loss: 0.00089920

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.64343095
Iteration 2/25 | Loss: 0.00105893
Iteration 3/25 | Loss: 0.00105893
Iteration 4/25 | Loss: 0.00105893
Iteration 5/25 | Loss: 0.00105893
Iteration 6/25 | Loss: 0.00105893
Iteration 7/25 | Loss: 0.00105893
Iteration 8/25 | Loss: 0.00105893
Iteration 9/25 | Loss: 0.00105893
Iteration 10/25 | Loss: 0.00105893
Iteration 11/25 | Loss: 0.00105893
Iteration 12/25 | Loss: 0.00105893
Iteration 13/25 | Loss: 0.00105893
Iteration 14/25 | Loss: 0.00105893
Iteration 15/25 | Loss: 0.00105893
Iteration 16/25 | Loss: 0.00105893
Iteration 17/25 | Loss: 0.00105893
Iteration 18/25 | Loss: 0.00105893
Iteration 19/25 | Loss: 0.00105893
Iteration 20/25 | Loss: 0.00105893
Iteration 21/25 | Loss: 0.00105893
Iteration 22/25 | Loss: 0.00105893
Iteration 23/25 | Loss: 0.00105893
Iteration 24/25 | Loss: 0.00105893
Iteration 25/25 | Loss: 0.00105893

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00105893
Iteration 2/1000 | Loss: 0.00552839
Iteration 3/1000 | Loss: 0.00017601
Iteration 4/1000 | Loss: 0.00011528
Iteration 5/1000 | Loss: 0.00010575
Iteration 6/1000 | Loss: 0.00015420
Iteration 7/1000 | Loss: 0.00007585
Iteration 8/1000 | Loss: 0.00004775
Iteration 9/1000 | Loss: 0.00011478
Iteration 10/1000 | Loss: 0.00007181
Iteration 11/1000 | Loss: 0.00004674
Iteration 12/1000 | Loss: 0.00004716
Iteration 13/1000 | Loss: 0.00004303
Iteration 14/1000 | Loss: 0.00004196
Iteration 15/1000 | Loss: 0.00003973
Iteration 16/1000 | Loss: 0.00003877
Iteration 17/1000 | Loss: 0.00003906
Iteration 18/1000 | Loss: 0.00003791
Iteration 19/1000 | Loss: 0.00003778
Iteration 20/1000 | Loss: 0.00004806
Iteration 21/1000 | Loss: 0.00004166
Iteration 22/1000 | Loss: 0.00004351
Iteration 23/1000 | Loss: 0.00003826
Iteration 24/1000 | Loss: 0.00004242
Iteration 25/1000 | Loss: 0.00007473
Iteration 26/1000 | Loss: 0.00007975
Iteration 27/1000 | Loss: 0.00004203
Iteration 28/1000 | Loss: 0.00003941
Iteration 29/1000 | Loss: 0.00003826
Iteration 30/1000 | Loss: 0.00003740
Iteration 31/1000 | Loss: 0.00003965
Iteration 32/1000 | Loss: 0.00003923
Iteration 33/1000 | Loss: 0.00004017
Iteration 34/1000 | Loss: 0.00003913
Iteration 35/1000 | Loss: 0.00004024
Iteration 36/1000 | Loss: 0.00003795
Iteration 37/1000 | Loss: 0.00003860
Iteration 38/1000 | Loss: 0.00003926
Iteration 39/1000 | Loss: 0.00003804
Iteration 40/1000 | Loss: 0.00003932
Iteration 41/1000 | Loss: 0.00003792
Iteration 42/1000 | Loss: 0.00003922
Iteration 43/1000 | Loss: 0.00003859
Iteration 44/1000 | Loss: 0.00003862
Iteration 45/1000 | Loss: 0.00003721
Iteration 46/1000 | Loss: 0.00003823
Iteration 47/1000 | Loss: 0.00003950
Iteration 48/1000 | Loss: 0.00004471
Iteration 49/1000 | Loss: 0.00003849
Iteration 50/1000 | Loss: 0.00004277
Iteration 51/1000 | Loss: 0.00003851
Iteration 52/1000 | Loss: 0.00004562
Iteration 53/1000 | Loss: 0.00003996
Iteration 54/1000 | Loss: 0.00004501
Iteration 55/1000 | Loss: 0.00004231
Iteration 56/1000 | Loss: 0.00004649
Iteration 57/1000 | Loss: 0.00004280
Iteration 58/1000 | Loss: 0.00004507
Iteration 59/1000 | Loss: 0.00004404
Iteration 60/1000 | Loss: 0.00004213
Iteration 61/1000 | Loss: 0.00004311
Iteration 62/1000 | Loss: 0.00004225
Iteration 63/1000 | Loss: 0.00004182
Iteration 64/1000 | Loss: 0.00004239
Iteration 65/1000 | Loss: 0.00004037
Iteration 66/1000 | Loss: 0.00004365
Iteration 67/1000 | Loss: 0.00004277
Iteration 68/1000 | Loss: 0.00004871
Iteration 69/1000 | Loss: 0.00004551
Iteration 70/1000 | Loss: 0.00004340
Iteration 71/1000 | Loss: 0.00004315
Iteration 72/1000 | Loss: 0.00004498
Iteration 73/1000 | Loss: 0.00003889
Iteration 74/1000 | Loss: 0.00003818
Iteration 75/1000 | Loss: 0.00003869
Iteration 76/1000 | Loss: 0.00003890
Iteration 77/1000 | Loss: 0.00004189
Iteration 78/1000 | Loss: 0.00004162
Iteration 79/1000 | Loss: 0.00004431
Iteration 80/1000 | Loss: 0.00004252
Iteration 81/1000 | Loss: 0.00004490
Iteration 82/1000 | Loss: 0.00004381
Iteration 83/1000 | Loss: 0.00004753
Iteration 84/1000 | Loss: 0.00004568
Iteration 85/1000 | Loss: 0.00004892
Iteration 86/1000 | Loss: 0.00004532
Iteration 87/1000 | Loss: 0.00004920
Iteration 88/1000 | Loss: 0.00004415
Iteration 89/1000 | Loss: 0.00004818
Iteration 90/1000 | Loss: 0.00004922
Iteration 91/1000 | Loss: 0.00005203
Iteration 92/1000 | Loss: 0.00004991
Iteration 93/1000 | Loss: 0.00005018
Iteration 94/1000 | Loss: 0.00004833
Iteration 95/1000 | Loss: 0.00005246
Iteration 96/1000 | Loss: 0.00004906
Iteration 97/1000 | Loss: 0.00005139
Iteration 98/1000 | Loss: 0.00005229
Iteration 99/1000 | Loss: 0.00004341
Iteration 100/1000 | Loss: 0.00004059
Iteration 101/1000 | Loss: 0.00003989
Iteration 102/1000 | Loss: 0.00003866
Iteration 103/1000 | Loss: 0.00003878
Iteration 104/1000 | Loss: 0.00003818
Iteration 105/1000 | Loss: 0.00003864
Iteration 106/1000 | Loss: 0.00003851
Iteration 107/1000 | Loss: 0.00004099
Iteration 108/1000 | Loss: 0.00003997
Iteration 109/1000 | Loss: 0.00004290
Iteration 110/1000 | Loss: 0.00004007
Iteration 111/1000 | Loss: 0.00004146
Iteration 112/1000 | Loss: 0.00004043
Iteration 113/1000 | Loss: 0.00004526
Iteration 114/1000 | Loss: 0.00004526
Iteration 115/1000 | Loss: 0.00004301
Iteration 116/1000 | Loss: 0.00004246
Iteration 117/1000 | Loss: 0.00004086
Iteration 118/1000 | Loss: 0.00004531
Iteration 119/1000 | Loss: 0.00004061
Iteration 120/1000 | Loss: 0.00003817
Iteration 121/1000 | Loss: 0.00003929
Iteration 122/1000 | Loss: 0.00004499
Iteration 123/1000 | Loss: 0.00004268
Iteration 124/1000 | Loss: 0.00003823
Iteration 125/1000 | Loss: 0.00004325
Iteration 126/1000 | Loss: 0.00004415
Iteration 127/1000 | Loss: 0.00004301
Iteration 128/1000 | Loss: 0.00004472
Iteration 129/1000 | Loss: 0.00004642
Iteration 130/1000 | Loss: 0.00004449
Iteration 131/1000 | Loss: 0.00004685
Iteration 132/1000 | Loss: 0.00004317
Iteration 133/1000 | Loss: 0.00004054
Iteration 134/1000 | Loss: 0.00003721
Iteration 135/1000 | Loss: 0.00003981
Iteration 136/1000 | Loss: 0.00004687
Iteration 137/1000 | Loss: 0.00004445
Iteration 138/1000 | Loss: 0.00004928
Iteration 139/1000 | Loss: 0.00004031
Iteration 140/1000 | Loss: 0.00003798
Iteration 141/1000 | Loss: 0.00004187
Iteration 142/1000 | Loss: 0.00004633
Iteration 143/1000 | Loss: 0.00003851
Iteration 144/1000 | Loss: 0.00004241
Iteration 145/1000 | Loss: 0.00004299
Iteration 146/1000 | Loss: 0.00003865
Iteration 147/1000 | Loss: 0.00003989
Iteration 148/1000 | Loss: 0.00003882
Iteration 149/1000 | Loss: 0.00003829
Iteration 150/1000 | Loss: 0.00003707
Iteration 151/1000 | Loss: 0.00003899
Iteration 152/1000 | Loss: 0.00003992
Iteration 153/1000 | Loss: 0.00003932
Iteration 154/1000 | Loss: 0.00003849
Iteration 155/1000 | Loss: 0.00003909
Iteration 156/1000 | Loss: 0.00004045
Iteration 157/1000 | Loss: 0.00003935
Iteration 158/1000 | Loss: 0.00003935
Iteration 159/1000 | Loss: 0.00004097
Iteration 160/1000 | Loss: 0.00004144
Iteration 161/1000 | Loss: 0.00004250
Iteration 162/1000 | Loss: 0.00004282
Iteration 163/1000 | Loss: 0.00004499
Iteration 164/1000 | Loss: 0.00004334
Iteration 165/1000 | Loss: 0.00004490
Iteration 166/1000 | Loss: 0.00004441
Iteration 167/1000 | Loss: 0.00004497
Iteration 168/1000 | Loss: 0.00004368
Iteration 169/1000 | Loss: 0.00004110
Iteration 170/1000 | Loss: 0.00003766
Iteration 171/1000 | Loss: 0.00003818
Iteration 172/1000 | Loss: 0.00003856
Iteration 173/1000 | Loss: 0.00004274
Iteration 174/1000 | Loss: 0.00004281
Iteration 175/1000 | Loss: 0.00004042
Iteration 176/1000 | Loss: 0.00004122
Iteration 177/1000 | Loss: 0.00003797
Iteration 178/1000 | Loss: 0.00003871
Iteration 179/1000 | Loss: 0.00003861
Iteration 180/1000 | Loss: 0.00004003
Iteration 181/1000 | Loss: 0.00004097
Iteration 182/1000 | Loss: 0.00004306
Iteration 183/1000 | Loss: 0.00004104
Iteration 184/1000 | Loss: 0.00003859
Iteration 185/1000 | Loss: 0.00004252
Iteration 186/1000 | Loss: 0.00004091
Iteration 187/1000 | Loss: 0.00003993
Iteration 188/1000 | Loss: 0.00003834
Iteration 189/1000 | Loss: 0.00004111
Iteration 190/1000 | Loss: 0.00003963
Iteration 191/1000 | Loss: 0.00003804
Iteration 192/1000 | Loss: 0.00003737
Iteration 193/1000 | Loss: 0.00004012
Iteration 194/1000 | Loss: 0.00004166
Iteration 195/1000 | Loss: 0.00004434
Iteration 196/1000 | Loss: 0.00004226
Iteration 197/1000 | Loss: 0.00004560
Iteration 198/1000 | Loss: 0.00003832
Iteration 199/1000 | Loss: 0.00003755
Iteration 200/1000 | Loss: 0.00003776
Iteration 201/1000 | Loss: 0.00003850
Iteration 202/1000 | Loss: 0.00003787
Iteration 203/1000 | Loss: 0.00004091
Iteration 204/1000 | Loss: 0.00003894
Iteration 205/1000 | Loss: 0.00004079
Iteration 206/1000 | Loss: 0.00003915
Iteration 207/1000 | Loss: 0.00004245
Iteration 208/1000 | Loss: 0.00003983
Iteration 209/1000 | Loss: 0.00004097
Iteration 210/1000 | Loss: 0.00003938
Iteration 211/1000 | Loss: 0.00003950
Iteration 212/1000 | Loss: 0.00003872
Iteration 213/1000 | Loss: 0.00004153
Iteration 214/1000 | Loss: 0.00004062
Iteration 215/1000 | Loss: 0.00003826
Iteration 216/1000 | Loss: 0.00003826
Iteration 217/1000 | Loss: 0.00004300
Iteration 218/1000 | Loss: 0.00004094
Iteration 219/1000 | Loss: 0.00004032
Iteration 220/1000 | Loss: 0.00003958
Iteration 221/1000 | Loss: 0.00004283
Iteration 222/1000 | Loss: 0.00004094
Iteration 223/1000 | Loss: 0.00003714
Iteration 224/1000 | Loss: 0.00003778
Iteration 225/1000 | Loss: 0.00003685
Iteration 226/1000 | Loss: 0.00003867
Iteration 227/1000 | Loss: 0.00003911
Iteration 228/1000 | Loss: 0.00003951
Iteration 229/1000 | Loss: 0.00004136
Iteration 230/1000 | Loss: 0.00003859
Iteration 231/1000 | Loss: 0.00004043
Iteration 232/1000 | Loss: 0.00003802
Iteration 233/1000 | Loss: 0.00004243
Iteration 234/1000 | Loss: 0.00003812
Iteration 235/1000 | Loss: 0.00003930
Iteration 236/1000 | Loss: 0.00003810
Iteration 237/1000 | Loss: 0.00003790
Iteration 238/1000 | Loss: 0.00004081
Iteration 239/1000 | Loss: 0.00003962
Iteration 240/1000 | Loss: 0.00004186
Iteration 241/1000 | Loss: 0.00004009
Iteration 242/1000 | Loss: 0.00003778
Iteration 243/1000 | Loss: 0.00003875
Iteration 244/1000 | Loss: 0.00003823
Iteration 245/1000 | Loss: 0.00004063
Iteration 246/1000 | Loss: 0.00003755
Iteration 247/1000 | Loss: 0.00003957
Iteration 248/1000 | Loss: 0.00003831
Iteration 249/1000 | Loss: 0.00004111
Iteration 250/1000 | Loss: 0.00003984
Iteration 251/1000 | Loss: 0.00004191
Iteration 252/1000 | Loss: 0.00004141
Iteration 253/1000 | Loss: 0.00004020
Iteration 254/1000 | Loss: 0.00004026
Iteration 255/1000 | Loss: 0.00004290
Iteration 256/1000 | Loss: 0.00004478
Iteration 257/1000 | Loss: 0.00004533
Iteration 258/1000 | Loss: 0.00004257
Iteration 259/1000 | Loss: 0.00004015
Iteration 260/1000 | Loss: 0.00004313
Iteration 261/1000 | Loss: 0.00004621
Iteration 262/1000 | Loss: 0.00004091
Iteration 263/1000 | Loss: 0.00004052
Iteration 264/1000 | Loss: 0.00004500
Iteration 265/1000 | Loss: 0.00003845
Iteration 266/1000 | Loss: 0.00004171
Iteration 267/1000 | Loss: 0.00004456
Iteration 268/1000 | Loss: 0.00004028
Iteration 269/1000 | Loss: 0.00004460
Iteration 270/1000 | Loss: 0.00004097
Iteration 271/1000 | Loss: 0.00004365
Iteration 272/1000 | Loss: 0.00004191
Iteration 273/1000 | Loss: 0.00003834
Iteration 274/1000 | Loss: 0.00003714
Iteration 275/1000 | Loss: 0.00003795
Iteration 276/1000 | Loss: 0.00003945
Iteration 277/1000 | Loss: 0.00003967
Iteration 278/1000 | Loss: 0.00004214
Iteration 279/1000 | Loss: 0.00004031
Iteration 280/1000 | Loss: 0.00003892
Iteration 281/1000 | Loss: 0.00003703
Iteration 282/1000 | Loss: 0.00003796
Iteration 283/1000 | Loss: 0.00003939
Iteration 284/1000 | Loss: 0.00003930
Iteration 285/1000 | Loss: 0.00003884
Iteration 286/1000 | Loss: 0.00004001
Iteration 287/1000 | Loss: 0.00003958
Iteration 288/1000 | Loss: 0.00003778
Iteration 289/1000 | Loss: 0.00003988
Iteration 290/1000 | Loss: 0.00004072
Iteration 291/1000 | Loss: 0.00003835
Iteration 292/1000 | Loss: 0.00004005
Iteration 293/1000 | Loss: 0.00003883
Iteration 294/1000 | Loss: 0.00004158
Iteration 295/1000 | Loss: 0.00003967
Iteration 296/1000 | Loss: 0.00004106
Iteration 297/1000 | Loss: 0.00003960
Iteration 298/1000 | Loss: 0.00003783
Iteration 299/1000 | Loss: 0.00004145
Iteration 300/1000 | Loss: 0.00003977
Iteration 301/1000 | Loss: 0.00004134
Iteration 302/1000 | Loss: 0.00003968
Iteration 303/1000 | Loss: 0.00004034
Iteration 304/1000 | Loss: 0.00003931
Iteration 305/1000 | Loss: 0.00003927
Iteration 306/1000 | Loss: 0.00004186
Iteration 307/1000 | Loss: 0.00003867
Iteration 308/1000 | Loss: 0.00004050
Iteration 309/1000 | Loss: 0.00004275
Iteration 310/1000 | Loss: 0.00003964
Iteration 311/1000 | Loss: 0.00004086
Iteration 312/1000 | Loss: 0.00003987
Iteration 313/1000 | Loss: 0.00004209
Iteration 314/1000 | Loss: 0.00004142
Iteration 315/1000 | Loss: 0.00004141
Iteration 316/1000 | Loss: 0.00003855
Iteration 317/1000 | Loss: 0.00004017
Iteration 318/1000 | Loss: 0.00003969
Iteration 319/1000 | Loss: 0.00003943
Iteration 320/1000 | Loss: 0.00003859
Iteration 321/1000 | Loss: 0.00003852
Iteration 322/1000 | Loss: 0.00004293
Iteration 323/1000 | Loss: 0.00004270
Iteration 324/1000 | Loss: 0.00004482
Iteration 325/1000 | Loss: 0.00003754
Iteration 326/1000 | Loss: 0.00003937
Iteration 327/1000 | Loss: 0.00004443
Iteration 328/1000 | Loss: 0.00003812
Iteration 329/1000 | Loss: 0.00003724
Iteration 330/1000 | Loss: 0.00004418
Iteration 331/1000 | Loss: 0.00004166
Iteration 332/1000 | Loss: 0.00003801
Iteration 333/1000 | Loss: 0.00004385
Iteration 334/1000 | Loss: 0.00004204
Iteration 335/1000 | Loss: 0.00003797
Iteration 336/1000 | Loss: 0.00003784
Iteration 337/1000 | Loss: 0.00004412
Iteration 338/1000 | Loss: 0.00004135
Iteration 339/1000 | Loss: 0.00004559
Iteration 340/1000 | Loss: 0.00004488
Iteration 341/1000 | Loss: 0.00003934
Iteration 342/1000 | Loss: 0.00004009
Iteration 343/1000 | Loss: 0.00004232
Iteration 344/1000 | Loss: 0.00004250
Iteration 345/1000 | Loss: 0.00004186
Iteration 346/1000 | Loss: 0.00004411
Iteration 347/1000 | Loss: 0.00003753
Iteration 348/1000 | Loss: 0.00003786
Iteration 349/1000 | Loss: 0.00003664
Iteration 350/1000 | Loss: 0.00004059
Iteration 351/1000 | Loss: 0.00004059
Iteration 352/1000 | Loss: 0.00003709
Iteration 353/1000 | Loss: 0.00004213
Iteration 354/1000 | Loss: 0.00004465
Iteration 355/1000 | Loss: 0.00004200
Iteration 356/1000 | Loss: 0.00003990
Iteration 357/1000 | Loss: 0.00003786
Iteration 358/1000 | Loss: 0.00003767
Iteration 359/1000 | Loss: 0.00003825
Iteration 360/1000 | Loss: 0.00003920
Iteration 361/1000 | Loss: 0.00004285
Iteration 362/1000 | Loss: 0.00003917
Iteration 363/1000 | Loss: 0.00004247
Iteration 364/1000 | Loss: 0.00004244
Iteration 365/1000 | Loss: 0.00003761
Iteration 366/1000 | Loss: 0.00004252
Iteration 367/1000 | Loss: 0.00004474
Iteration 368/1000 | Loss: 0.00004458
Iteration 369/1000 | Loss: 0.00004324
Iteration 370/1000 | Loss: 0.00003763
Iteration 371/1000 | Loss: 0.00003733
Iteration 372/1000 | Loss: 0.00004165
Iteration 373/1000 | Loss: 0.00004476
Iteration 374/1000 | Loss: 0.00004096
Iteration 375/1000 | Loss: 0.00004815
Iteration 376/1000 | Loss: 0.00003784
Iteration 377/1000 | Loss: 0.00003919
Iteration 378/1000 | Loss: 0.00003785
Iteration 379/1000 | Loss: 0.00003688
Iteration 380/1000 | Loss: 0.00004239
Iteration 381/1000 | Loss: 0.00003762
Iteration 382/1000 | Loss: 0.00004216
Iteration 383/1000 | Loss: 0.00004026
Iteration 384/1000 | Loss: 0.00004261
Iteration 385/1000 | Loss: 0.00003970
Iteration 386/1000 | Loss: 0.00003713
Iteration 387/1000 | Loss: 0.00003709
Iteration 388/1000 | Loss: 0.00003694
Iteration 389/1000 | Loss: 0.00004091
Iteration 390/1000 | Loss: 0.00004068
Iteration 391/1000 | Loss: 0.00003805
Iteration 392/1000 | Loss: 0.00003868
Iteration 393/1000 | Loss: 0.00004033
Iteration 394/1000 | Loss: 0.00004212
Iteration 395/1000 | Loss: 0.00004194
Iteration 396/1000 | Loss: 0.00004338
Iteration 397/1000 | Loss: 0.00004542
Iteration 398/1000 | Loss: 0.00004409
Iteration 399/1000 | Loss: 0.00004027
Iteration 400/1000 | Loss: 0.00003710
Iteration 401/1000 | Loss: 0.00003803
Iteration 402/1000 | Loss: 0.00003953
Iteration 403/1000 | Loss: 0.00004105
Iteration 404/1000 | Loss: 0.00004320
Iteration 405/1000 | Loss: 0.00003898
Iteration 406/1000 | Loss: 0.00004140
Iteration 407/1000 | Loss: 0.00004268
Iteration 408/1000 | Loss: 0.00004254
Iteration 409/1000 | Loss: 0.00003774
Iteration 410/1000 | Loss: 0.00004217
Iteration 411/1000 | Loss: 0.00004131
Iteration 412/1000 | Loss: 0.00003872
Iteration 413/1000 | Loss: 0.00003740
Iteration 414/1000 | Loss: 0.00003786
Iteration 415/1000 | Loss: 0.00003754
Iteration 416/1000 | Loss: 0.00004198
Iteration 417/1000 | Loss: 0.00003858
Iteration 418/1000 | Loss: 0.00003696
Iteration 419/1000 | Loss: 0.00004194
Iteration 420/1000 | Loss: 0.00004149
Iteration 421/1000 | Loss: 0.00004232
Iteration 422/1000 | Loss: 0.00004202
Iteration 423/1000 | Loss: 0.00004261
Iteration 424/1000 | Loss: 0.00003818
Iteration 425/1000 | Loss: 0.00003878
Iteration 426/1000 | Loss: 0.00004664
Iteration 427/1000 | Loss: 0.00003988
Iteration 428/1000 | Loss: 0.00004033
Iteration 429/1000 | Loss: 0.00004040
Iteration 430/1000 | Loss: 0.00004129
Iteration 431/1000 | Loss: 0.00004182
Iteration 432/1000 | Loss: 0.00004582
Iteration 433/1000 | Loss: 0.00003804
Iteration 434/1000 | Loss: 0.00003912
Iteration 435/1000 | Loss: 0.00004062
Iteration 436/1000 | Loss: 0.00004135
Iteration 437/1000 | Loss: 0.00003924
Iteration 438/1000 | Loss: 0.00003754
Iteration 439/1000 | Loss: 0.00003957
Iteration 440/1000 | Loss: 0.00004101
Iteration 441/1000 | Loss: 0.00004334
Iteration 442/1000 | Loss: 0.00003761
Iteration 443/1000 | Loss: 0.00003696
Iteration 444/1000 | Loss: 0.00003694
Iteration 445/1000 | Loss: 0.00003755
Iteration 446/1000 | Loss: 0.00003869
Iteration 447/1000 | Loss: 0.00004152
Iteration 448/1000 | Loss: 0.00004156
Iteration 449/1000 | Loss: 0.00003775
Iteration 450/1000 | Loss: 0.00004186
Iteration 451/1000 | Loss: 0.00004213
Iteration 452/1000 | Loss: 0.00003854
Iteration 453/1000 | Loss: 0.00003993
Iteration 454/1000 | Loss: 0.00003859
Iteration 455/1000 | Loss: 0.00004204
Iteration 456/1000 | Loss: 0.00004105
Iteration 457/1000 | Loss: 0.00003870
Iteration 458/1000 | Loss: 0.00003891
Iteration 459/1000 | Loss: 0.00004161
Iteration 460/1000 | Loss: 0.00004340
Iteration 461/1000 | Loss: 0.00004066
Iteration 462/1000 | Loss: 0.00004302
Iteration 463/1000 | Loss: 0.00004207
Iteration 464/1000 | Loss: 0.00004526
Iteration 465/1000 | Loss: 0.00004316
Iteration 466/1000 | Loss: 0.00003773
Iteration 467/1000 | Loss: 0.00003911
Iteration 468/1000 | Loss: 0.00003800
Iteration 469/1000 | Loss: 0.00003983
Iteration 470/1000 | Loss: 0.00004075
Iteration 471/1000 | Loss: 0.00003986
Iteration 472/1000 | Loss: 0.00003759
Iteration 473/1000 | Loss: 0.00003717
Iteration 474/1000 | Loss: 0.00004015
Iteration 475/1000 | Loss: 0.00003823
Iteration 476/1000 | Loss: 0.00003895
Iteration 477/1000 | Loss: 0.00004060
Iteration 478/1000 | Loss: 0.00003857
Iteration 479/1000 | Loss: 0.00003963
Iteration 480/1000 | Loss: 0.00004049
Iteration 481/1000 | Loss: 0.00003940
Iteration 482/1000 | Loss: 0.00003998
Iteration 483/1000 | Loss: 0.00003763
Iteration 484/1000 | Loss: 0.00003847
Iteration 485/1000 | Loss: 0.00004043
Iteration 486/1000 | Loss: 0.00003783
Iteration 487/1000 | Loss: 0.00003761
Iteration 488/1000 | Loss: 0.00003850
Iteration 489/1000 | Loss: 0.00003930
Iteration 490/1000 | Loss: 0.00003891
Iteration 491/1000 | Loss: 0.00004184
Iteration 492/1000 | Loss: 0.00004079
Iteration 493/1000 | Loss: 0.00004289
Iteration 494/1000 | Loss: 0.00004122
Iteration 495/1000 | Loss: 0.00003982
Iteration 496/1000 | Loss: 0.00004048
Iteration 497/1000 | Loss: 0.00004292
Iteration 498/1000 | Loss: 0.00004149
Iteration 499/1000 | Loss: 0.00004353
Iteration 500/1000 | Loss: 0.00004232
Iteration 501/1000 | Loss: 0.00004416
Iteration 502/1000 | Loss: 0.00004280
Iteration 503/1000 | Loss: 0.00004472
Iteration 504/1000 | Loss: 0.00004321
Iteration 505/1000 | Loss: 0.00004481
Iteration 506/1000 | Loss: 0.00004498
Iteration 507/1000 | Loss: 0.00004557
Iteration 508/1000 | Loss: 0.00004567
Iteration 509/1000 | Loss: 0.00004481
Iteration 510/1000 | Loss: 0.00004561
Iteration 511/1000 | Loss: 0.00004690
Iteration 512/1000 | Loss: 0.00004615
Iteration 513/1000 | Loss: 0.00004752
Iteration 514/1000 | Loss: 0.00004877
Iteration 515/1000 | Loss: 0.00004834
Iteration 516/1000 | Loss: 0.00004739
Iteration 517/1000 | Loss: 0.00004793
Iteration 518/1000 | Loss: 0.00004389
Iteration 519/1000 | Loss: 0.00004821
Iteration 520/1000 | Loss: 0.00004503
Iteration 521/1000 | Loss: 0.00004660
Iteration 522/1000 | Loss: 0.00004660
Iteration 523/1000 | Loss: 0.00004828
Iteration 524/1000 | Loss: 0.00004672
Iteration 525/1000 | Loss: 0.00004944
Iteration 526/1000 | Loss: 0.00004980
Iteration 527/1000 | Loss: 0.00004985
Iteration 528/1000 | Loss: 0.00005137
Iteration 529/1000 | Loss: 0.00004841
Iteration 530/1000 | Loss: 0.00005146
Iteration 531/1000 | Loss: 0.00005072
Iteration 532/1000 | Loss: 0.00004037
Iteration 533/1000 | Loss: 0.00003838
Iteration 534/1000 | Loss: 0.00003726
Iteration 535/1000 | Loss: 0.00004022
Iteration 536/1000 | Loss: 0.00004017
Iteration 537/1000 | Loss: 0.00004340
Iteration 538/1000 | Loss: 0.00003845
Iteration 539/1000 | Loss: 0.00003644
Iteration 540/1000 | Loss: 0.00004145
Iteration 541/1000 | Loss: 0.00004452
Iteration 542/1000 | Loss: 0.00004340
Iteration 543/1000 | Loss: 0.00004462
Iteration 544/1000 | Loss: 0.00004673
Iteration 545/1000 | Loss: 0.00004376
Iteration 546/1000 | Loss: 0.00005218
Iteration 547/1000 | Loss: 0.00004034
Iteration 548/1000 | Loss: 0.00003742
Iteration 549/1000 | Loss: 0.00003713
Iteration 550/1000 | Loss: 0.00003633
Iteration 551/1000 | Loss: 0.00003898
Iteration 552/1000 | Loss: 0.00004112
Iteration 553/1000 | Loss: 0.00003839
Iteration 554/1000 | Loss: 0.00003889
Iteration 555/1000 | Loss: 0.00004408
Iteration 556/1000 | Loss: 0.00004229
Iteration 557/1000 | Loss: 0.00004667
Iteration 558/1000 | Loss: 0.00004307
Iteration 559/1000 | Loss: 0.00004772
Iteration 560/1000 | Loss: 0.00004087
Iteration 561/1000 | Loss: 0.00004039
Iteration 562/1000 | Loss: 0.00003897
Iteration 563/1000 | Loss: 0.00004796
Iteration 564/1000 | Loss: 0.00004402
Iteration 565/1000 | Loss: 0.00004820
Iteration 566/1000 | Loss: 0.00004400
Iteration 567/1000 | Loss: 0.00003776
Iteration 568/1000 | Loss: 0.00003951
Iteration 569/1000 | Loss: 0.00004776
Iteration 570/1000 | Loss: 0.00004544
Iteration 571/1000 | Loss: 0.00004480
Iteration 572/1000 | Loss: 0.00004370
Iteration 573/1000 | Loss: 0.00004232
Iteration 574/1000 | Loss: 0.00004139
Iteration 575/1000 | Loss: 0.00004185
Iteration 576/1000 | Loss: 0.00004281
Iteration 577/1000 | Loss: 0.00003790
Iteration 578/1000 | Loss: 0.00004003
Iteration 579/1000 | Loss: 0.00004691
Iteration 580/1000 | Loss: 0.00004297
Iteration 581/1000 | Loss: 0.00004878
Iteration 582/1000 | Loss: 0.00004410
Iteration 583/1000 | Loss: 0.00004137
Iteration 584/1000 | Loss: 0.00003737
Iteration 585/1000 | Loss: 0.00003884
Iteration 586/1000 | Loss: 0.00004074
Iteration 587/1000 | Loss: 0.00004556
Iteration 588/1000 | Loss: 0.00004374
Iteration 589/1000 | Loss: 0.00004521
Iteration 590/1000 | Loss: 0.00004386
Iteration 591/1000 | Loss: 0.00004566
Iteration 592/1000 | Loss: 0.00004075
Iteration 593/1000 | Loss: 0.00003947
Iteration 594/1000 | Loss: 0.00004014
Iteration 595/1000 | Loss: 0.00004948
Iteration 596/1000 | Loss: 0.00004350
Iteration 597/1000 | Loss: 0.00004189
Iteration 598/1000 | Loss: 0.00003710
Iteration 599/1000 | Loss: 0.00003773
Iteration 600/1000 | Loss: 0.00003967
Iteration 601/1000 | Loss: 0.00003946
Iteration 602/1000 | Loss: 0.00003980
Iteration 603/1000 | Loss: 0.00004469
Iteration 604/1000 | Loss: 0.00004349
Iteration 605/1000 | Loss: 0.00004589
Iteration 606/1000 | Loss: 0.00003906
Iteration 607/1000 | Loss: 0.00004342
Iteration 608/1000 | Loss: 0.00003976
Iteration 609/1000 | Loss: 0.00004114
Iteration 610/1000 | Loss: 0.00004003
Iteration 611/1000 | Loss: 0.00004042
Iteration 612/1000 | Loss: 0.00004064
Iteration 613/1000 | Loss: 0.00004089
Iteration 614/1000 | Loss: 0.00004357
Iteration 615/1000 | Loss: 0.00003990
Iteration 616/1000 | Loss: 0.00004579
Iteration 617/1000 | Loss: 0.00004378
Iteration 618/1000 | Loss: 0.00004483
Iteration 619/1000 | Loss: 0.00004331
Iteration 620/1000 | Loss: 0.00004599
Iteration 621/1000 | Loss: 0.00004205
Iteration 622/1000 | Loss: 0.00004624
Iteration 623/1000 | Loss: 0.00004436
Iteration 624/1000 | Loss: 0.00004370
Iteration 625/1000 | Loss: 0.00004410
Iteration 626/1000 | Loss: 0.00004739
Iteration 627/1000 | Loss: 0.00004593
Iteration 628/1000 | Loss: 0.00005015
Iteration 629/1000 | Loss: 0.00004372
Iteration 630/1000 | Loss: 0.00004738
Iteration 631/1000 | Loss: 0.00004411
Iteration 632/1000 | Loss: 0.00004909
Iteration 633/1000 | Loss: 0.00004238
Iteration 634/1000 | Loss: 0.00004833
Iteration 635/1000 | Loss: 0.00004542
Iteration 636/1000 | Loss: 0.00004853
Iteration 637/1000 | Loss: 0.00004597
Iteration 638/1000 | Loss: 0.00004781
Iteration 639/1000 | Loss: 0.00004814
Iteration 640/1000 | Loss: 0.00004928
Iteration 641/1000 | Loss: 0.00005249
Iteration 642/1000 | Loss: 0.00004802
Iteration 643/1000 | Loss: 0.00004871
Iteration 644/1000 | Loss: 0.00004822
Iteration 645/1000 | Loss: 0.00005030
Iteration 646/1000 | Loss: 0.00004787
Iteration 647/1000 | Loss: 0.00005046
Iteration 648/1000 | Loss: 0.00004049
Iteration 649/1000 | Loss: 0.00003793
Iteration 650/1000 | Loss: 0.00003663
Iteration 651/1000 | Loss: 0.00003977
Iteration 652/1000 | Loss: 0.00004509
Iteration 653/1000 | Loss: 0.00004581
Iteration 654/1000 | Loss: 0.00004871
Iteration 655/1000 | Loss: 0.00004779
Iteration 656/1000 | Loss: 0.00004843
Iteration 657/1000 | Loss: 0.00004362
Iteration 658/1000 | Loss: 0.00004125
Iteration 659/1000 | Loss: 0.00004187
Iteration 660/1000 | Loss: 0.00004248
Iteration 661/1000 | Loss: 0.00004831
Iteration 662/1000 | Loss: 0.00003963
Iteration 663/1000 | Loss: 0.00004146
Iteration 664/1000 | Loss: 0.00004153
Iteration 665/1000 | Loss: 0.00004571
Iteration 666/1000 | Loss: 0.00004137
Iteration 667/1000 | Loss: 0.00004845
Iteration 668/1000 | Loss: 0.00003914
Iteration 669/1000 | Loss: 0.00003933
Iteration 670/1000 | Loss: 0.00003850
Iteration 671/1000 | Loss: 0.00004088
Iteration 672/1000 | Loss: 0.00003891
Iteration 673/1000 | Loss: 0.00004599
Iteration 674/1000 | Loss: 0.00004186
Iteration 675/1000 | Loss: 0.00004526
Iteration 676/1000 | Loss: 0.00004249
Iteration 677/1000 | Loss: 0.00004752
Iteration 678/1000 | Loss: 0.00004459
Iteration 679/1000 | Loss: 0.00004152
Iteration 680/1000 | Loss: 0.00004119
Iteration 681/1000 | Loss: 0.00004628
Iteration 682/1000 | Loss: 0.00004475
Iteration 683/1000 | Loss: 0.00003823
Iteration 684/1000 | Loss: 0.00003776
Iteration 685/1000 | Loss: 0.00003705
Iteration 686/1000 | Loss: 0.00003793
Iteration 687/1000 | Loss: 0.00003850
Iteration 688/1000 | Loss: 0.00003945
Iteration 689/1000 | Loss: 0.00004213
Iteration 690/1000 | Loss: 0.00004563
Iteration 691/1000 | Loss: 0.00003998
Iteration 692/1000 | Loss: 0.00003913
Iteration 693/1000 | Loss: 0.00004121
Iteration 694/1000 | Loss: 0.00003832
Iteration 695/1000 | Loss: 0.00003737
Iteration 696/1000 | Loss: 0.00003639
Iteration 697/1000 | Loss: 0.00003722
Iteration 698/1000 | Loss: 0.00003870
Iteration 699/1000 | Loss: 0.00004088
Iteration 700/1000 | Loss: 0.00004203
Iteration 701/1000 | Loss: 0.00003920
Iteration 702/1000 | Loss: 0.00003794
Iteration 703/1000 | Loss: 0.00003913
Iteration 704/1000 | Loss: 0.00004035
Iteration 705/1000 | Loss: 0.00004196
Iteration 706/1000 | Loss: 0.00004357
Iteration 707/1000 | Loss: 0.00004971
Iteration 708/1000 | Loss: 0.00004286
Iteration 709/1000 | Loss: 0.00004488
Iteration 710/1000 | Loss: 0.00004394
Iteration 711/1000 | Loss: 0.00004482
Iteration 712/1000 | Loss: 0.00004173
Iteration 713/1000 | Loss: 0.00004633
Iteration 714/1000 | Loss: 0.00003817
Iteration 715/1000 | Loss: 0.00003758
Iteration 716/1000 | Loss: 0.00003846
Iteration 717/1000 | Loss: 0.00004250
Iteration 718/1000 | Loss: 0.00004032
Iteration 719/1000 | Loss: 0.00004219
Iteration 720/1000 | Loss: 0.00004274
Iteration 721/1000 | Loss: 0.00004413
Iteration 722/1000 | Loss: 0.00004383
Iteration 723/1000 | Loss: 0.00004585
Iteration 724/1000 | Loss: 0.00004464
Iteration 725/1000 | Loss: 0.00004124
Iteration 726/1000 | Loss: 0.00004210
Iteration 727/1000 | Loss: 0.00004538
Iteration 728/1000 | Loss: 0.00004477
Iteration 729/1000 | Loss: 0.00004748
Iteration 730/1000 | Loss: 0.00004442
Iteration 731/1000 | Loss: 0.00004578
Iteration 732/1000 | Loss: 0.00004564
Iteration 733/1000 | Loss: 0.00005257
Iteration 734/1000 | Loss: 0.00004531
Iteration 735/1000 | Loss: 0.00005342
Iteration 736/1000 | Loss: 0.00004421
Iteration 737/1000 | Loss: 0.00004361
Iteration 738/1000 | Loss: 0.00004469
Iteration 739/1000 | Loss: 0.00004343
Iteration 740/1000 | Loss: 0.00004391
Iteration 741/1000 | Loss: 0.00005599
Iteration 742/1000 | Loss: 0.00004072
Iteration 743/1000 | Loss: 0.00003992
Iteration 744/1000 | Loss: 0.00004167
Iteration 745/1000 | Loss: 0.00004889
Iteration 746/1000 | Loss: 0.00004202
Iteration 747/1000 | Loss: 0.00005138
Iteration 748/1000 | Loss: 0.00004158
Iteration 749/1000 | Loss: 0.00003919
Iteration 750/1000 | Loss: 0.00003816
Iteration 751/1000 | Loss: 0.00003703
Iteration 752/1000 | Loss: 0.00003653
Iteration 753/1000 | Loss: 0.00003638
Iteration 754/1000 | Loss: 0.00003767
Iteration 755/1000 | Loss: 0.00004013
Iteration 756/1000 | Loss: 0.00004116
Iteration 757/1000 | Loss: 0.00004064
Iteration 758/1000 | Loss: 0.00004149
Iteration 759/1000 | Loss: 0.00004265
Iteration 760/1000 | Loss: 0.00004797
Iteration 761/1000 | Loss: 0.00004318
Iteration 762/1000 | Loss: 0.00004612
Iteration 763/1000 | Loss: 0.00004368
Iteration 764/1000 | Loss: 0.00003877
Iteration 765/1000 | Loss: 0.00004125
Iteration 766/1000 | Loss: 0.00004333
Iteration 767/1000 | Loss: 0.00004379
Iteration 768/1000 | Loss: 0.00003919
Iteration 769/1000 | Loss: 0.00003746
Iteration 770/1000 | Loss: 0.00003709
Iteration 771/1000 | Loss: 0.00003668
Iteration 772/1000 | Loss: 0.00003768
Iteration 773/1000 | Loss: 0.00004037
Iteration 774/1000 | Loss: 0.00004202
Iteration 775/1000 | Loss: 0.00004023
Iteration 776/1000 | Loss: 0.00004041
Iteration 777/1000 | Loss: 0.00004018
Iteration 778/1000 | Loss: 0.00004175
Iteration 779/1000 | Loss: 0.00004056
Iteration 780/1000 | Loss: 0.00004203
Iteration 781/1000 | Loss: 0.00004048
Iteration 782/1000 | Loss: 0.00004437
Iteration 783/1000 | Loss: 0.00004181
Iteration 784/1000 | Loss: 0.00004529
Iteration 785/1000 | Loss: 0.00004256
Iteration 786/1000 | Loss: 0.00004168
Iteration 787/1000 | Loss: 0.00003788
Iteration 788/1000 | Loss: 0.00003708
Iteration 789/1000 | Loss: 0.00003950
Iteration 790/1000 | Loss: 0.00004126
Iteration 791/1000 | Loss: 0.00004131
Iteration 792/1000 | Loss: 0.00003857
Iteration 793/1000 | Loss: 0.00003733
Iteration 794/1000 | Loss: 0.00003834
Iteration 795/1000 | Loss: 0.00004146
Iteration 796/1000 | Loss: 0.00004040
Iteration 797/1000 | Loss: 0.00004113
Iteration 798/1000 | Loss: 0.00004027
Iteration 799/1000 | Loss: 0.00004212
Iteration 800/1000 | Loss: 0.00004161
Iteration 801/1000 | Loss: 0.00004162
Iteration 802/1000 | Loss: 0.00004027
Iteration 803/1000 | Loss: 0.00004337
Iteration 804/1000 | Loss: 0.00004105
Iteration 805/1000 | Loss: 0.00004409
Iteration 806/1000 | Loss: 0.00004182
Iteration 807/1000 | Loss: 0.00003892
Iteration 808/1000 | Loss: 0.00004151
Iteration 809/1000 | Loss: 0.00004086
Iteration 810/1000 | Loss: 0.00004292
Iteration 811/1000 | Loss: 0.00004323
Iteration 812/1000 | Loss: 0.00004195
Iteration 813/1000 | Loss: 0.00004207
Iteration 814/1000 | Loss: 0.00004139
Iteration 815/1000 | Loss: 0.00004340
Iteration 816/1000 | Loss: 0.00004272
Iteration 817/1000 | Loss: 0.00004254
Iteration 818/1000 | Loss: 0.00004352
Iteration 819/1000 | Loss: 0.00004077
Iteration 820/1000 | Loss: 0.00003793
Iteration 821/1000 | Loss: 0.00003764
Iteration 822/1000 | Loss: 0.00003894
Iteration 823/1000 | Loss: 0.00004119
Iteration 824/1000 | Loss: 0.00004134
Iteration 825/1000 | Loss: 0.00003902
Iteration 826/1000 | Loss: 0.00004038
Iteration 827/1000 | Loss: 0.00004269
Iteration 828/1000 | Loss: 0.00004126
Iteration 829/1000 | Loss: 0.00004236
Iteration 830/1000 | Loss: 0.00004302
Iteration 831/1000 | Loss: 0.00004379
Iteration 832/1000 | Loss: 0.00003993
Iteration 833/1000 | Loss: 0.00004074
Iteration 834/1000 | Loss: 0.00004335
Iteration 835/1000 | Loss: 0.00004000
Iteration 836/1000 | Loss: 0.00004162
Iteration 837/1000 | Loss: 0.00003996
Iteration 838/1000 | Loss: 0.00004171
Iteration 839/1000 | Loss: 0.00004262
Iteration 840/1000 | Loss: 0.00004077
Iteration 841/1000 | Loss: 0.00004006
Iteration 842/1000 | Loss: 0.00004290
Iteration 843/1000 | Loss: 0.00004533
Iteration 844/1000 | Loss: 0.00004533
Iteration 845/1000 | Loss: 0.00004462
Iteration 846/1000 | Loss: 0.00004683
Iteration 847/1000 | Loss: 0.00004649
Iteration 848/1000 | Loss: 0.00004747
Iteration 849/1000 | Loss: 0.00004463
Iteration 850/1000 | Loss: 0.00004435
Iteration 851/1000 | Loss: 0.00004151
Iteration 852/1000 | Loss: 0.00004157
Iteration 853/1000 | Loss: 0.00004537
Iteration 854/1000 | Loss: 0.00004555
Iteration 855/1000 | Loss: 0.00004329
Iteration 856/1000 | Loss: 0.00004007
Iteration 857/1000 | Loss: 0.00004386
Iteration 858/1000 | Loss: 0.00004163
Iteration 859/1000 | Loss: 0.00003855
Iteration 860/1000 | Loss: 0.00003710
Iteration 861/1000 | Loss: 0.00003714
Iteration 862/1000 | Loss: 0.00003662
Iteration 863/1000 | Loss: 0.00004099
Iteration 864/1000 | Loss: 0.00004726
Iteration 865/1000 | Loss: 0.00004227
Iteration 866/1000 | Loss: 0.00004568
Iteration 867/1000 | Loss: 0.00004377
Iteration 868/1000 | Loss: 0.00004799
Iteration 869/1000 | Loss: 0.00004263
Iteration 870/1000 | Loss: 0.00004468
Iteration 871/1000 | Loss: 0.00003851
Iteration 872/1000 | Loss: 0.00003749
Iteration 873/1000 | Loss: 0.00004082
Iteration 874/1000 | Loss: 0.00004596
Iteration 875/1000 | Loss: 0.00004476
Iteration 876/1000 | Loss: 0.00004256
Iteration 877/1000 | Loss: 0.00003918
Iteration 878/1000 | Loss: 0.00003727
Iteration 879/1000 | Loss: 0.00003689
Iteration 880/1000 | Loss: 0.00003697
Iteration 881/1000 | Loss: 0.00003772
Iteration 882/1000 | Loss: 0.00003833
Iteration 883/1000 | Loss: 0.00003842
Iteration 884/1000 | Loss: 0.00004056
Iteration 885/1000 | Loss: 0.00004005
Iteration 886/1000 | Loss: 0.00003978
Iteration 887/1000 | Loss: 0.00003862
Iteration 888/1000 | Loss: 0.00003983
Iteration 889/1000 | Loss: 0.00003833
Iteration 890/1000 | Loss: 0.00003682
Iteration 891/1000 | Loss: 0.00003709
Iteration 892/1000 | Loss: 0.00003759
Iteration 893/1000 | Loss: 0.00004131
Iteration 894/1000 | Loss: 0.00003946
Iteration 895/1000 | Loss: 0.00003914
Iteration 896/1000 | Loss: 0.00003983
Iteration 897/1000 | Loss: 0.00004499
Iteration 898/1000 | Loss: 0.00004081
Iteration 899/1000 | Loss: 0.00004568
Iteration 900/1000 | Loss: 0.00003835
Iteration 901/1000 | Loss: 0.00003947
Iteration 902/1000 | Loss: 0.00003921
Iteration 903/1000 | Loss: 0.00003886
Iteration 904/1000 | Loss: 0.00003994
Iteration 905/1000 | Loss: 0.00004564
Iteration 906/1000 | Loss: 0.00004077
Iteration 907/1000 | Loss: 0.00003925
Iteration 908/1000 | Loss: 0.00004009
Iteration 909/1000 | Loss: 0.00004636
Iteration 910/1000 | Loss: 0.00004335
Iteration 911/1000 | Loss: 0.00004296
Iteration 912/1000 | Loss: 0.00003907
Iteration 913/1000 | Loss: 0.00004559
Iteration 914/1000 | Loss: 0.00004158
Iteration 915/1000 | Loss: 0.00004203
Iteration 916/1000 | Loss: 0.00004422
Iteration 917/1000 | Loss: 0.00004850
Iteration 918/1000 | Loss: 0.00004177
Iteration 919/1000 | Loss: 0.00004458
Iteration 920/1000 | Loss: 0.00003844
Iteration 921/1000 | Loss: 0.00004543
Iteration 922/1000 | Loss: 0.00004217
Iteration 923/1000 | Loss: 0.00004510
Iteration 924/1000 | Loss: 0.00004296
Iteration 925/1000 | Loss: 0.00004594
Iteration 926/1000 | Loss: 0.00004464
Iteration 927/1000 | Loss: 0.00004623
Iteration 928/1000 | Loss: 0.00004567
Iteration 929/1000 | Loss: 0.00004816
Iteration 930/1000 | Loss: 0.00004313
Iteration 931/1000 | Loss: 0.00004992
Iteration 932/1000 | Loss: 0.00004227
Iteration 933/1000 | Loss: 0.00004558
Iteration 934/1000 | Loss: 0.00004157
Iteration 935/1000 | Loss: 0.00004587
Iteration 936/1000 | Loss: 0.00004121
Iteration 937/1000 | Loss: 0.00004755
Iteration 938/1000 | Loss: 0.00004083
Iteration 939/1000 | Loss: 0.00004788
Iteration 940/1000 | Loss: 0.00004046
Iteration 941/1000 | Loss: 0.00004276
Iteration 942/1000 | Loss: 0.00004013
Iteration 943/1000 | Loss: 0.00004013
Iteration 944/1000 | Loss: 0.00004616
Iteration 945/1000 | Loss: 0.00004012
Iteration 946/1000 | Loss: 0.00004030
Iteration 947/1000 | Loss: 0.00004012
Iteration 948/1000 | Loss: 0.00004047
Iteration 949/1000 | Loss: 0.00004063
Iteration 950/1000 | Loss: 0.00004133
Iteration 951/1000 | Loss: 0.00003987
Iteration 952/1000 | Loss: 0.00004000
Iteration 953/1000 | Loss: 0.00003876
Iteration 954/1000 | Loss: 0.00004044
Iteration 955/1000 | Loss: 0.00004120
Iteration 956/1000 | Loss: 0.00004203
Iteration 957/1000 | Loss: 0.00003976
Iteration 958/1000 | Loss: 0.00004087
Iteration 959/1000 | Loss: 0.00003937
Iteration 960/1000 | Loss: 0.00004096
Iteration 961/1000 | Loss: 0.00003913
Iteration 962/1000 | Loss: 0.00004105
Iteration 963/1000 | Loss: 0.00003839
Iteration 964/1000 | Loss: 0.00003929
Iteration 965/1000 | Loss: 0.00003809
Iteration 966/1000 | Loss: 0.00003903
Iteration 967/1000 | Loss: 0.00003943
Iteration 968/1000 | Loss: 0.00003853
Iteration 969/1000 | Loss: 0.00003894
Iteration 970/1000 | Loss: 0.00004025
Iteration 971/1000 | Loss: 0.00003895
Iteration 972/1000 | Loss: 0.00004034
Iteration 973/1000 | Loss: 0.00003816
Iteration 974/1000 | Loss: 0.00004023
Iteration 975/1000 | Loss: 0.00003803
Iteration 976/1000 | Loss: 0.00004005
Iteration 977/1000 | Loss: 0.00003986
Iteration 978/1000 | Loss: 0.00003996
Iteration 979/1000 | Loss: 0.00004080
Iteration 980/1000 | Loss: 0.00004080
Iteration 981/1000 | Loss: 0.00004089
Iteration 982/1000 | Loss: 0.00003856
Iteration 983/1000 | Loss: 0.00003936
Iteration 984/1000 | Loss: 0.00004040
Iteration 985/1000 | Loss: 0.00003901
Iteration 986/1000 | Loss: 0.00003984
Iteration 987/1000 | Loss: 0.00004257
Iteration 988/1000 | Loss: 0.00004278
Iteration 989/1000 | Loss: 0.00004362
Iteration 990/1000 | Loss: 0.00004087
Iteration 991/1000 | Loss: 0.00004158
Iteration 992/1000 | Loss: 0.00004054
Iteration 993/1000 | Loss: 0.00004130
Iteration 994/1000 | Loss: 0.00003840
Iteration 995/1000 | Loss: 0.00004230
Iteration 996/1000 | Loss: 0.00003956
Iteration 997/1000 | Loss: 0.00003873
Iteration 998/1000 | Loss: 0.00003779
Iteration 999/1000 | Loss: 0.00003905
Iteration 1000/1000 | Loss: 0.00003940

Optimization complete. Final v2v error: 4.82365608215332 mm

Highest mean error: 13.084301948547363 mm for frame 16

Lowest mean error: 3.882781744003296 mm for frame 149

Saving results

Total time: 1420.2417585849762
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0011/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0011.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0011
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01034044
Iteration 2/25 | Loss: 0.00185280
Iteration 3/25 | Loss: 0.00126440
Iteration 4/25 | Loss: 0.00122364
Iteration 5/25 | Loss: 0.00120702
Iteration 6/25 | Loss: 0.00120198
Iteration 7/25 | Loss: 0.00120123
Iteration 8/25 | Loss: 0.00120123
Iteration 9/25 | Loss: 0.00120123
Iteration 10/25 | Loss: 0.00120123
Iteration 11/25 | Loss: 0.00120123
Iteration 12/25 | Loss: 0.00120123
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0012012330116704106, 0.0012012330116704106, 0.0012012330116704106, 0.0012012330116704106, 0.0012012330116704106]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0012012330116704106

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.96172196
Iteration 2/25 | Loss: 0.00045902
Iteration 3/25 | Loss: 0.00045902
Iteration 4/25 | Loss: 0.00045902
Iteration 5/25 | Loss: 0.00045902
Iteration 6/25 | Loss: 0.00045902
Iteration 7/25 | Loss: 0.00045902
Iteration 8/25 | Loss: 0.00045902
Iteration 9/25 | Loss: 0.00045902
Iteration 10/25 | Loss: 0.00045902
Iteration 11/25 | Loss: 0.00045902
Iteration 12/25 | Loss: 0.00045902
Iteration 13/25 | Loss: 0.00045902
Iteration 14/25 | Loss: 0.00045902
Iteration 15/25 | Loss: 0.00045902
Iteration 16/25 | Loss: 0.00045902
Iteration 17/25 | Loss: 0.00045902
Iteration 18/25 | Loss: 0.00045902
Iteration 19/25 | Loss: 0.00045902
Iteration 20/25 | Loss: 0.00045902
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 20. Stopping optimization.
Last 5 losses: [0.00045901743578724563, 0.00045901743578724563, 0.00045901743578724563, 0.00045901743578724563, 0.00045901743578724563]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.00045901743578724563

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00045902
Iteration 2/1000 | Loss: 0.00006550
Iteration 3/1000 | Loss: 0.00004978
Iteration 4/1000 | Loss: 0.00004720
Iteration 5/1000 | Loss: 0.00004554
Iteration 6/1000 | Loss: 0.00004460
Iteration 7/1000 | Loss: 0.00004361
Iteration 8/1000 | Loss: 0.00004292
Iteration 9/1000 | Loss: 0.00004250
Iteration 10/1000 | Loss: 0.00004214
Iteration 11/1000 | Loss: 0.00004192
Iteration 12/1000 | Loss: 0.00004190
Iteration 13/1000 | Loss: 0.00004187
Iteration 14/1000 | Loss: 0.00004185
Iteration 15/1000 | Loss: 0.00004185
Iteration 16/1000 | Loss: 0.00004184
Iteration 17/1000 | Loss: 0.00004183
Iteration 18/1000 | Loss: 0.00004182
Iteration 19/1000 | Loss: 0.00004179
Iteration 20/1000 | Loss: 0.00004179
Iteration 21/1000 | Loss: 0.00004176
Iteration 22/1000 | Loss: 0.00004172
Iteration 23/1000 | Loss: 0.00004170
Iteration 24/1000 | Loss: 0.00004170
Iteration 25/1000 | Loss: 0.00004170
Iteration 26/1000 | Loss: 0.00004170
Iteration 27/1000 | Loss: 0.00004170
Iteration 28/1000 | Loss: 0.00004169
Iteration 29/1000 | Loss: 0.00004169
Iteration 30/1000 | Loss: 0.00004169
Iteration 31/1000 | Loss: 0.00004168
Iteration 32/1000 | Loss: 0.00004168
Iteration 33/1000 | Loss: 0.00004166
Iteration 34/1000 | Loss: 0.00004165
Iteration 35/1000 | Loss: 0.00004163
Iteration 36/1000 | Loss: 0.00004162
Iteration 37/1000 | Loss: 0.00004162
Iteration 38/1000 | Loss: 0.00004160
Iteration 39/1000 | Loss: 0.00004159
Iteration 40/1000 | Loss: 0.00004159
Iteration 41/1000 | Loss: 0.00004157
Iteration 42/1000 | Loss: 0.00004156
Iteration 43/1000 | Loss: 0.00004156
Iteration 44/1000 | Loss: 0.00004156
Iteration 45/1000 | Loss: 0.00004156
Iteration 46/1000 | Loss: 0.00004156
Iteration 47/1000 | Loss: 0.00004156
Iteration 48/1000 | Loss: 0.00004156
Iteration 49/1000 | Loss: 0.00004156
Iteration 50/1000 | Loss: 0.00004156
Iteration 51/1000 | Loss: 0.00004156
Iteration 52/1000 | Loss: 0.00004155
Iteration 53/1000 | Loss: 0.00004155
Iteration 54/1000 | Loss: 0.00004155
Iteration 55/1000 | Loss: 0.00004155
Iteration 56/1000 | Loss: 0.00004155
Iteration 57/1000 | Loss: 0.00004155
Iteration 58/1000 | Loss: 0.00004155
Iteration 59/1000 | Loss: 0.00004155
Iteration 60/1000 | Loss: 0.00004155
Iteration 61/1000 | Loss: 0.00004155
Iteration 62/1000 | Loss: 0.00004154
Iteration 63/1000 | Loss: 0.00004154
Iteration 64/1000 | Loss: 0.00004154
Iteration 65/1000 | Loss: 0.00004154
Iteration 66/1000 | Loss: 0.00004154
Iteration 67/1000 | Loss: 0.00004153
Iteration 68/1000 | Loss: 0.00004153
Iteration 69/1000 | Loss: 0.00004153
Iteration 70/1000 | Loss: 0.00004152
Iteration 71/1000 | Loss: 0.00004152
Iteration 72/1000 | Loss: 0.00004152
Iteration 73/1000 | Loss: 0.00004152
Iteration 74/1000 | Loss: 0.00004152
Iteration 75/1000 | Loss: 0.00004152
Iteration 76/1000 | Loss: 0.00004152
Iteration 77/1000 | Loss: 0.00004151
Iteration 78/1000 | Loss: 0.00004151
Iteration 79/1000 | Loss: 0.00004151
Iteration 80/1000 | Loss: 0.00004151
Iteration 81/1000 | Loss: 0.00004151
Iteration 82/1000 | Loss: 0.00004151
Iteration 83/1000 | Loss: 0.00004151
Iteration 84/1000 | Loss: 0.00004151
Iteration 85/1000 | Loss: 0.00004151
Iteration 86/1000 | Loss: 0.00004151
Iteration 87/1000 | Loss: 0.00004151
Iteration 88/1000 | Loss: 0.00004151
Iteration 89/1000 | Loss: 0.00004150
Iteration 90/1000 | Loss: 0.00004150
Iteration 91/1000 | Loss: 0.00004150
Iteration 92/1000 | Loss: 0.00004149
Iteration 93/1000 | Loss: 0.00004149
Iteration 94/1000 | Loss: 0.00004149
Iteration 95/1000 | Loss: 0.00004148
Iteration 96/1000 | Loss: 0.00004148
Iteration 97/1000 | Loss: 0.00004148
Iteration 98/1000 | Loss: 0.00004148
Iteration 99/1000 | Loss: 0.00004147
Iteration 100/1000 | Loss: 0.00004147
Iteration 101/1000 | Loss: 0.00004147
Iteration 102/1000 | Loss: 0.00004147
Iteration 103/1000 | Loss: 0.00004147
Iteration 104/1000 | Loss: 0.00004147
Iteration 105/1000 | Loss: 0.00004147
Iteration 106/1000 | Loss: 0.00004147
Iteration 107/1000 | Loss: 0.00004147
Iteration 108/1000 | Loss: 0.00004147
Iteration 109/1000 | Loss: 0.00004147
Iteration 110/1000 | Loss: 0.00004146
Iteration 111/1000 | Loss: 0.00004146
Iteration 112/1000 | Loss: 0.00004146
Iteration 113/1000 | Loss: 0.00004146
Iteration 114/1000 | Loss: 0.00004145
Iteration 115/1000 | Loss: 0.00004145
Iteration 116/1000 | Loss: 0.00004145
Iteration 117/1000 | Loss: 0.00004145
Iteration 118/1000 | Loss: 0.00004145
Iteration 119/1000 | Loss: 0.00004145
Iteration 120/1000 | Loss: 0.00004145
Iteration 121/1000 | Loss: 0.00004144
Iteration 122/1000 | Loss: 0.00004144
Iteration 123/1000 | Loss: 0.00004144
Iteration 124/1000 | Loss: 0.00004144
Iteration 125/1000 | Loss: 0.00004143
Iteration 126/1000 | Loss: 0.00004143
Iteration 127/1000 | Loss: 0.00004143
Iteration 128/1000 | Loss: 0.00004143
Iteration 129/1000 | Loss: 0.00004143
Iteration 130/1000 | Loss: 0.00004143
Iteration 131/1000 | Loss: 0.00004143
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 131. Stopping optimization.
Last 5 losses: [4.1430805140407756e-05, 4.1430805140407756e-05, 4.1430805140407756e-05, 4.1430805140407756e-05, 4.1430805140407756e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 4.1430805140407756e-05

Optimization complete. Final v2v error: 5.303181171417236 mm

Highest mean error: 6.703240394592285 mm for frame 119

Lowest mean error: 4.297221660614014 mm for frame 0

Saving results

Total time: 42.23246431350708
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0000/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0000.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0000
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00908173
Iteration 2/25 | Loss: 0.00116066
Iteration 3/25 | Loss: 0.00104393
Iteration 4/25 | Loss: 0.00102556
Iteration 5/25 | Loss: 0.00102054
Iteration 6/25 | Loss: 0.00101934
Iteration 7/25 | Loss: 0.00101934
Iteration 8/25 | Loss: 0.00101934
Iteration 9/25 | Loss: 0.00101934
Iteration 10/25 | Loss: 0.00101934
Iteration 11/25 | Loss: 0.00101934
Iteration 12/25 | Loss: 0.00101934
Iteration 13/25 | Loss: 0.00101934
Iteration 14/25 | Loss: 0.00101934
Iteration 15/25 | Loss: 0.00101934
Iteration 16/25 | Loss: 0.00101934
Iteration 17/25 | Loss: 0.00101934
Iteration 18/25 | Loss: 0.00101934
Iteration 19/25 | Loss: 0.00101934
Iteration 20/25 | Loss: 0.00101934
Iteration 21/25 | Loss: 0.00101934
Iteration 22/25 | Loss: 0.00101934
Iteration 23/25 | Loss: 0.00101934
Iteration 24/25 | Loss: 0.00101934
Iteration 25/25 | Loss: 0.00101934

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.40784049
Iteration 2/25 | Loss: 0.00058059
Iteration 3/25 | Loss: 0.00058059
Iteration 4/25 | Loss: 0.00058058
Iteration 5/25 | Loss: 0.00058058
Iteration 6/25 | Loss: 0.00058058
Iteration 7/25 | Loss: 0.00058058
Iteration 8/25 | Loss: 0.00058058
Iteration 9/25 | Loss: 0.00058058
Iteration 10/25 | Loss: 0.00058058
Iteration 11/25 | Loss: 0.00058058
Iteration 12/25 | Loss: 0.00058058
Iteration 13/25 | Loss: 0.00058058
Iteration 14/25 | Loss: 0.00058058
Iteration 15/25 | Loss: 0.00058058
Iteration 16/25 | Loss: 0.00058058
Iteration 17/25 | Loss: 0.00058058
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0005805830587632954, 0.0005805830587632954, 0.0005805830587632954, 0.0005805830587632954, 0.0005805830587632954]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005805830587632954

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00058058
Iteration 2/1000 | Loss: 0.00002904
Iteration 3/1000 | Loss: 0.00002190
Iteration 4/1000 | Loss: 0.00001941
Iteration 5/1000 | Loss: 0.00001841
Iteration 6/1000 | Loss: 0.00001790
Iteration 7/1000 | Loss: 0.00001752
Iteration 8/1000 | Loss: 0.00001731
Iteration 9/1000 | Loss: 0.00001711
Iteration 10/1000 | Loss: 0.00001706
Iteration 11/1000 | Loss: 0.00001704
Iteration 12/1000 | Loss: 0.00001704
Iteration 13/1000 | Loss: 0.00001701
Iteration 14/1000 | Loss: 0.00001698
Iteration 15/1000 | Loss: 0.00001697
Iteration 16/1000 | Loss: 0.00001697
Iteration 17/1000 | Loss: 0.00001694
Iteration 18/1000 | Loss: 0.00001692
Iteration 19/1000 | Loss: 0.00001691
Iteration 20/1000 | Loss: 0.00001690
Iteration 21/1000 | Loss: 0.00001689
Iteration 22/1000 | Loss: 0.00001689
Iteration 23/1000 | Loss: 0.00001689
Iteration 24/1000 | Loss: 0.00001688
Iteration 25/1000 | Loss: 0.00001687
Iteration 26/1000 | Loss: 0.00001685
Iteration 27/1000 | Loss: 0.00001684
Iteration 28/1000 | Loss: 0.00001684
Iteration 29/1000 | Loss: 0.00001684
Iteration 30/1000 | Loss: 0.00001684
Iteration 31/1000 | Loss: 0.00001684
Iteration 32/1000 | Loss: 0.00001684
Iteration 33/1000 | Loss: 0.00001684
Iteration 34/1000 | Loss: 0.00001684
Iteration 35/1000 | Loss: 0.00001684
Iteration 36/1000 | Loss: 0.00001684
Iteration 37/1000 | Loss: 0.00001684
Iteration 38/1000 | Loss: 0.00001684
Iteration 39/1000 | Loss: 0.00001684
Iteration 40/1000 | Loss: 0.00001683
Iteration 41/1000 | Loss: 0.00001683
Iteration 42/1000 | Loss: 0.00001683
Iteration 43/1000 | Loss: 0.00001683
Iteration 44/1000 | Loss: 0.00001682
Iteration 45/1000 | Loss: 0.00001682
Iteration 46/1000 | Loss: 0.00001682
Iteration 47/1000 | Loss: 0.00001682
Iteration 48/1000 | Loss: 0.00001681
Iteration 49/1000 | Loss: 0.00001681
Iteration 50/1000 | Loss: 0.00001681
Iteration 51/1000 | Loss: 0.00001681
Iteration 52/1000 | Loss: 0.00001681
Iteration 53/1000 | Loss: 0.00001680
Iteration 54/1000 | Loss: 0.00001680
Iteration 55/1000 | Loss: 0.00001679
Iteration 56/1000 | Loss: 0.00001679
Iteration 57/1000 | Loss: 0.00001679
Iteration 58/1000 | Loss: 0.00001679
Iteration 59/1000 | Loss: 0.00001679
Iteration 60/1000 | Loss: 0.00001678
Iteration 61/1000 | Loss: 0.00001678
Iteration 62/1000 | Loss: 0.00001678
Iteration 63/1000 | Loss: 0.00001678
Iteration 64/1000 | Loss: 0.00001678
Iteration 65/1000 | Loss: 0.00001677
Iteration 66/1000 | Loss: 0.00001677
Iteration 67/1000 | Loss: 0.00001677
Iteration 68/1000 | Loss: 0.00001677
Iteration 69/1000 | Loss: 0.00001677
Iteration 70/1000 | Loss: 0.00001676
Iteration 71/1000 | Loss: 0.00001676
Iteration 72/1000 | Loss: 0.00001676
Iteration 73/1000 | Loss: 0.00001676
Iteration 74/1000 | Loss: 0.00001675
Iteration 75/1000 | Loss: 0.00001675
Iteration 76/1000 | Loss: 0.00001675
Iteration 77/1000 | Loss: 0.00001675
Iteration 78/1000 | Loss: 0.00001675
Iteration 79/1000 | Loss: 0.00001675
Iteration 80/1000 | Loss: 0.00001675
Iteration 81/1000 | Loss: 0.00001675
Iteration 82/1000 | Loss: 0.00001674
Iteration 83/1000 | Loss: 0.00001673
Iteration 84/1000 | Loss: 0.00001673
Iteration 85/1000 | Loss: 0.00001673
Iteration 86/1000 | Loss: 0.00001673
Iteration 87/1000 | Loss: 0.00001673
Iteration 88/1000 | Loss: 0.00001672
Iteration 89/1000 | Loss: 0.00001672
Iteration 90/1000 | Loss: 0.00001671
Iteration 91/1000 | Loss: 0.00001671
Iteration 92/1000 | Loss: 0.00001671
Iteration 93/1000 | Loss: 0.00001671
Iteration 94/1000 | Loss: 0.00001671
Iteration 95/1000 | Loss: 0.00001670
Iteration 96/1000 | Loss: 0.00001670
Iteration 97/1000 | Loss: 0.00001670
Iteration 98/1000 | Loss: 0.00001670
Iteration 99/1000 | Loss: 0.00001670
Iteration 100/1000 | Loss: 0.00001670
Iteration 101/1000 | Loss: 0.00001670
Iteration 102/1000 | Loss: 0.00001670
Iteration 103/1000 | Loss: 0.00001670
Iteration 104/1000 | Loss: 0.00001670
Iteration 105/1000 | Loss: 0.00001670
Iteration 106/1000 | Loss: 0.00001670
Iteration 107/1000 | Loss: 0.00001669
Iteration 108/1000 | Loss: 0.00001669
Iteration 109/1000 | Loss: 0.00001669
Iteration 110/1000 | Loss: 0.00001669
Iteration 111/1000 | Loss: 0.00001669
Iteration 112/1000 | Loss: 0.00001669
Iteration 113/1000 | Loss: 0.00001668
Iteration 114/1000 | Loss: 0.00001668
Iteration 115/1000 | Loss: 0.00001668
Iteration 116/1000 | Loss: 0.00001668
Iteration 117/1000 | Loss: 0.00001668
Iteration 118/1000 | Loss: 0.00001668
Iteration 119/1000 | Loss: 0.00001668
Iteration 120/1000 | Loss: 0.00001668
Iteration 121/1000 | Loss: 0.00001668
Iteration 122/1000 | Loss: 0.00001668
Iteration 123/1000 | Loss: 0.00001668
Iteration 124/1000 | Loss: 0.00001668
Iteration 125/1000 | Loss: 0.00001668
Iteration 126/1000 | Loss: 0.00001668
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 126. Stopping optimization.
Last 5 losses: [1.6676449376973324e-05, 1.6676449376973324e-05, 1.6676449376973324e-05, 1.6676449376973324e-05, 1.6676449376973324e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.6676449376973324e-05

Optimization complete. Final v2v error: 3.565732717514038 mm

Highest mean error: 3.7499630451202393 mm for frame 89

Lowest mean error: 3.370640277862549 mm for frame 8

Saving results

Total time: 35.965471506118774
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0002/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0002.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0002
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01014668
Iteration 2/25 | Loss: 0.00146115
Iteration 3/25 | Loss: 0.00129683
Iteration 4/25 | Loss: 0.00123721
Iteration 5/25 | Loss: 0.00122498
Iteration 6/25 | Loss: 0.00122221
Iteration 7/25 | Loss: 0.00122085
Iteration 8/25 | Loss: 0.00122082
Iteration 9/25 | Loss: 0.00122082
Iteration 10/25 | Loss: 0.00122082
Iteration 11/25 | Loss: 0.00122082
Iteration 12/25 | Loss: 0.00122082
Iteration 13/25 | Loss: 0.00122082
Iteration 14/25 | Loss: 0.00122082
Iteration 15/25 | Loss: 0.00122082
Iteration 16/25 | Loss: 0.00122082
Iteration 17/25 | Loss: 0.00122082
Iteration 18/25 | Loss: 0.00122082
Iteration 19/25 | Loss: 0.00122082
Iteration 20/25 | Loss: 0.00122082
Iteration 21/25 | Loss: 0.00122082
Iteration 22/25 | Loss: 0.00122082
Iteration 23/25 | Loss: 0.00122082
Iteration 24/25 | Loss: 0.00122082
Iteration 25/25 | Loss: 0.00122082

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 0.21300913
Iteration 2/25 | Loss: 0.00065525
Iteration 3/25 | Loss: 0.00065524
Iteration 4/25 | Loss: 0.00065524
Iteration 5/25 | Loss: 0.00065524
Iteration 6/25 | Loss: 0.00065524
Iteration 7/25 | Loss: 0.00065524
Iteration 8/25 | Loss: 0.00065524
Iteration 9/25 | Loss: 0.00065524
Iteration 10/25 | Loss: 0.00065524
Iteration 11/25 | Loss: 0.00065524
Iteration 12/25 | Loss: 0.00065524
Iteration 13/25 | Loss: 0.00065524
Iteration 14/25 | Loss: 0.00065524
Iteration 15/25 | Loss: 0.00065524
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0006552418926730752, 0.0006552418926730752, 0.0006552418926730752, 0.0006552418926730752, 0.0006552418926730752]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006552418926730752

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00065524
Iteration 2/1000 | Loss: 0.00005805
Iteration 3/1000 | Loss: 0.00004645
Iteration 4/1000 | Loss: 0.00004138
Iteration 5/1000 | Loss: 0.00003964
Iteration 6/1000 | Loss: 0.00003857
Iteration 7/1000 | Loss: 0.00003763
Iteration 8/1000 | Loss: 0.00003665
Iteration 9/1000 | Loss: 0.00003604
Iteration 10/1000 | Loss: 0.00003563
Iteration 11/1000 | Loss: 0.00003529
Iteration 12/1000 | Loss: 0.00003526
Iteration 13/1000 | Loss: 0.00003499
Iteration 14/1000 | Loss: 0.00003476
Iteration 15/1000 | Loss: 0.00003467
Iteration 16/1000 | Loss: 0.00003461
Iteration 17/1000 | Loss: 0.00003461
Iteration 18/1000 | Loss: 0.00003459
Iteration 19/1000 | Loss: 0.00003458
Iteration 20/1000 | Loss: 0.00003458
Iteration 21/1000 | Loss: 0.00003457
Iteration 22/1000 | Loss: 0.00003457
Iteration 23/1000 | Loss: 0.00003456
Iteration 24/1000 | Loss: 0.00003454
Iteration 25/1000 | Loss: 0.00003452
Iteration 26/1000 | Loss: 0.00003451
Iteration 27/1000 | Loss: 0.00003451
Iteration 28/1000 | Loss: 0.00003451
Iteration 29/1000 | Loss: 0.00003450
Iteration 30/1000 | Loss: 0.00003450
Iteration 31/1000 | Loss: 0.00003450
Iteration 32/1000 | Loss: 0.00003450
Iteration 33/1000 | Loss: 0.00003450
Iteration 34/1000 | Loss: 0.00003450
Iteration 35/1000 | Loss: 0.00003450
Iteration 36/1000 | Loss: 0.00003450
Iteration 37/1000 | Loss: 0.00003450
Iteration 38/1000 | Loss: 0.00003450
Iteration 39/1000 | Loss: 0.00003450
Iteration 40/1000 | Loss: 0.00003449
Iteration 41/1000 | Loss: 0.00003449
Iteration 42/1000 | Loss: 0.00003449
Iteration 43/1000 | Loss: 0.00003449
Iteration 44/1000 | Loss: 0.00003449
Iteration 45/1000 | Loss: 0.00003449
Iteration 46/1000 | Loss: 0.00003449
Iteration 47/1000 | Loss: 0.00003449
Iteration 48/1000 | Loss: 0.00003448
Iteration 49/1000 | Loss: 0.00003448
Iteration 50/1000 | Loss: 0.00003448
Iteration 51/1000 | Loss: 0.00003448
Iteration 52/1000 | Loss: 0.00003448
Iteration 53/1000 | Loss: 0.00003448
Iteration 54/1000 | Loss: 0.00003448
Iteration 55/1000 | Loss: 0.00003448
Iteration 56/1000 | Loss: 0.00003447
Iteration 57/1000 | Loss: 0.00003447
Iteration 58/1000 | Loss: 0.00003447
Iteration 59/1000 | Loss: 0.00003447
Iteration 60/1000 | Loss: 0.00003447
Iteration 61/1000 | Loss: 0.00003446
Iteration 62/1000 | Loss: 0.00003446
Iteration 63/1000 | Loss: 0.00003446
Iteration 64/1000 | Loss: 0.00003446
Iteration 65/1000 | Loss: 0.00003446
Iteration 66/1000 | Loss: 0.00003446
Iteration 67/1000 | Loss: 0.00003446
Iteration 68/1000 | Loss: 0.00003445
Iteration 69/1000 | Loss: 0.00003445
Iteration 70/1000 | Loss: 0.00003445
Iteration 71/1000 | Loss: 0.00003445
Iteration 72/1000 | Loss: 0.00003445
Iteration 73/1000 | Loss: 0.00003445
Iteration 74/1000 | Loss: 0.00003444
Iteration 75/1000 | Loss: 0.00003444
Iteration 76/1000 | Loss: 0.00003444
Iteration 77/1000 | Loss: 0.00003444
Iteration 78/1000 | Loss: 0.00003444
Iteration 79/1000 | Loss: 0.00003444
Iteration 80/1000 | Loss: 0.00003444
Iteration 81/1000 | Loss: 0.00003444
Iteration 82/1000 | Loss: 0.00003444
Iteration 83/1000 | Loss: 0.00003444
Iteration 84/1000 | Loss: 0.00003444
Iteration 85/1000 | Loss: 0.00003444
Iteration 86/1000 | Loss: 0.00003444
Iteration 87/1000 | Loss: 0.00003444
Iteration 88/1000 | Loss: 0.00003443
Iteration 89/1000 | Loss: 0.00003443
Iteration 90/1000 | Loss: 0.00003443
Iteration 91/1000 | Loss: 0.00003443
Iteration 92/1000 | Loss: 0.00003443
Iteration 93/1000 | Loss: 0.00003443
Iteration 94/1000 | Loss: 0.00003443
Iteration 95/1000 | Loss: 0.00003443
Iteration 96/1000 | Loss: 0.00003443
Iteration 97/1000 | Loss: 0.00003443
Iteration 98/1000 | Loss: 0.00003443
Iteration 99/1000 | Loss: 0.00003443
Iteration 100/1000 | Loss: 0.00003443
Iteration 101/1000 | Loss: 0.00003443
Iteration 102/1000 | Loss: 0.00003443
Iteration 103/1000 | Loss: 0.00003443
Iteration 104/1000 | Loss: 0.00003443
Iteration 105/1000 | Loss: 0.00003443
Iteration 106/1000 | Loss: 0.00003443
Iteration 107/1000 | Loss: 0.00003443
Iteration 108/1000 | Loss: 0.00003443
Iteration 109/1000 | Loss: 0.00003443
Iteration 110/1000 | Loss: 0.00003443
Iteration 111/1000 | Loss: 0.00003443
Iteration 112/1000 | Loss: 0.00003443
Iteration 113/1000 | Loss: 0.00003443
Iteration 114/1000 | Loss: 0.00003443
Iteration 115/1000 | Loss: 0.00003443
Iteration 116/1000 | Loss: 0.00003443
Iteration 117/1000 | Loss: 0.00003443
Iteration 118/1000 | Loss: 0.00003443
Iteration 119/1000 | Loss: 0.00003443
Iteration 120/1000 | Loss: 0.00003443
Iteration 121/1000 | Loss: 0.00003443
Iteration 122/1000 | Loss: 0.00003443
Iteration 123/1000 | Loss: 0.00003443
Iteration 124/1000 | Loss: 0.00003443
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 124. Stopping optimization.
Last 5 losses: [3.442748129600659e-05, 3.442748129600659e-05, 3.442748129600659e-05, 3.442748129600659e-05, 3.442748129600659e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.442748129600659e-05

Optimization complete. Final v2v error: 5.196324825286865 mm

Highest mean error: 5.670689582824707 mm for frame 44

Lowest mean error: 4.666648864746094 mm for frame 12

Saving results

Total time: 36.01538395881653
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0022/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0022.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0022
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00746613
Iteration 2/25 | Loss: 0.00141345
Iteration 3/25 | Loss: 0.00126175
Iteration 4/25 | Loss: 0.00120968
Iteration 5/25 | Loss: 0.00119035
Iteration 6/25 | Loss: 0.00118685
Iteration 7/25 | Loss: 0.00118593
Iteration 8/25 | Loss: 0.00118593
Iteration 9/25 | Loss: 0.00118593
Iteration 10/25 | Loss: 0.00118593
Iteration 11/25 | Loss: 0.00118593
Iteration 12/25 | Loss: 0.00118593
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 12. Stopping optimization.
Last 5 losses: [0.0011859261430799961, 0.0011859261430799961, 0.0011859261430799961, 0.0011859261430799961, 0.0011859261430799961]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0011859261430799961

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.67328405
Iteration 2/25 | Loss: 0.00093600
Iteration 3/25 | Loss: 0.00093597
Iteration 4/25 | Loss: 0.00093597
Iteration 5/25 | Loss: 0.00093596
Iteration 6/25 | Loss: 0.00093596
Iteration 7/25 | Loss: 0.00093596
Iteration 8/25 | Loss: 0.00093596
Iteration 9/25 | Loss: 0.00093596
Iteration 10/25 | Loss: 0.00093596
Iteration 11/25 | Loss: 0.00093596
Iteration 12/25 | Loss: 0.00093596
Iteration 13/25 | Loss: 0.00093596
Iteration 14/25 | Loss: 0.00093596
Iteration 15/25 | Loss: 0.00093596
Iteration 16/25 | Loss: 0.00093596
Iteration 17/25 | Loss: 0.00093596
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0009359628893435001, 0.0009359628893435001, 0.0009359628893435001, 0.0009359628893435001, 0.0009359628893435001]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0009359628893435001

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00093596
Iteration 2/1000 | Loss: 0.00005514
Iteration 3/1000 | Loss: 0.00003859
Iteration 4/1000 | Loss: 0.00003418
Iteration 5/1000 | Loss: 0.00003174
Iteration 6/1000 | Loss: 0.00003039
Iteration 7/1000 | Loss: 0.00002937
Iteration 8/1000 | Loss: 0.00002868
Iteration 9/1000 | Loss: 0.00002812
Iteration 10/1000 | Loss: 0.00002766
Iteration 11/1000 | Loss: 0.00002740
Iteration 12/1000 | Loss: 0.00002731
Iteration 13/1000 | Loss: 0.00002719
Iteration 14/1000 | Loss: 0.00002704
Iteration 15/1000 | Loss: 0.00002692
Iteration 16/1000 | Loss: 0.00002684
Iteration 17/1000 | Loss: 0.00002683
Iteration 18/1000 | Loss: 0.00002680
Iteration 19/1000 | Loss: 0.00002679
Iteration 20/1000 | Loss: 0.00002670
Iteration 21/1000 | Loss: 0.00002670
Iteration 22/1000 | Loss: 0.00002665
Iteration 23/1000 | Loss: 0.00002663
Iteration 24/1000 | Loss: 0.00002662
Iteration 25/1000 | Loss: 0.00002661
Iteration 26/1000 | Loss: 0.00002661
Iteration 27/1000 | Loss: 0.00002661
Iteration 28/1000 | Loss: 0.00002661
Iteration 29/1000 | Loss: 0.00002660
Iteration 30/1000 | Loss: 0.00002660
Iteration 31/1000 | Loss: 0.00002658
Iteration 32/1000 | Loss: 0.00002658
Iteration 33/1000 | Loss: 0.00002658
Iteration 34/1000 | Loss: 0.00002658
Iteration 35/1000 | Loss: 0.00002658
Iteration 36/1000 | Loss: 0.00002658
Iteration 37/1000 | Loss: 0.00002658
Iteration 38/1000 | Loss: 0.00002658
Iteration 39/1000 | Loss: 0.00002657
Iteration 40/1000 | Loss: 0.00002657
Iteration 41/1000 | Loss: 0.00002657
Iteration 42/1000 | Loss: 0.00002657
Iteration 43/1000 | Loss: 0.00002657
Iteration 44/1000 | Loss: 0.00002657
Iteration 45/1000 | Loss: 0.00002656
Iteration 46/1000 | Loss: 0.00002656
Iteration 47/1000 | Loss: 0.00002655
Iteration 48/1000 | Loss: 0.00002655
Iteration 49/1000 | Loss: 0.00002655
Iteration 50/1000 | Loss: 0.00002655
Iteration 51/1000 | Loss: 0.00002654
Iteration 52/1000 | Loss: 0.00002654
Iteration 53/1000 | Loss: 0.00002654
Iteration 54/1000 | Loss: 0.00002653
Iteration 55/1000 | Loss: 0.00002653
Iteration 56/1000 | Loss: 0.00002652
Iteration 57/1000 | Loss: 0.00002652
Iteration 58/1000 | Loss: 0.00002652
Iteration 59/1000 | Loss: 0.00002652
Iteration 60/1000 | Loss: 0.00002652
Iteration 61/1000 | Loss: 0.00002651
Iteration 62/1000 | Loss: 0.00002651
Iteration 63/1000 | Loss: 0.00002651
Iteration 64/1000 | Loss: 0.00002650
Iteration 65/1000 | Loss: 0.00002650
Iteration 66/1000 | Loss: 0.00002650
Iteration 67/1000 | Loss: 0.00002649
Iteration 68/1000 | Loss: 0.00002649
Iteration 69/1000 | Loss: 0.00002649
Iteration 70/1000 | Loss: 0.00002649
Iteration 71/1000 | Loss: 0.00002649
Iteration 72/1000 | Loss: 0.00002649
Iteration 73/1000 | Loss: 0.00002649
Iteration 74/1000 | Loss: 0.00002648
Iteration 75/1000 | Loss: 0.00002648
Iteration 76/1000 | Loss: 0.00002648
Iteration 77/1000 | Loss: 0.00002648
Iteration 78/1000 | Loss: 0.00002648
Iteration 79/1000 | Loss: 0.00002648
Iteration 80/1000 | Loss: 0.00002648
Iteration 81/1000 | Loss: 0.00002648
Iteration 82/1000 | Loss: 0.00002648
Iteration 83/1000 | Loss: 0.00002648
Iteration 84/1000 | Loss: 0.00002648
Iteration 85/1000 | Loss: 0.00002648
Iteration 86/1000 | Loss: 0.00002648
Iteration 87/1000 | Loss: 0.00002648
Iteration 88/1000 | Loss: 0.00002648
Iteration 89/1000 | Loss: 0.00002648
Iteration 90/1000 | Loss: 0.00002648
Iteration 91/1000 | Loss: 0.00002648
Iteration 92/1000 | Loss: 0.00002648
Iteration 93/1000 | Loss: 0.00002648
Iteration 94/1000 | Loss: 0.00002648
Iteration 95/1000 | Loss: 0.00002648
Iteration 96/1000 | Loss: 0.00002648
Iteration 97/1000 | Loss: 0.00002648
Iteration 98/1000 | Loss: 0.00002648
Iteration 99/1000 | Loss: 0.00002648
Iteration 100/1000 | Loss: 0.00002648
Iteration 101/1000 | Loss: 0.00002648
Iteration 102/1000 | Loss: 0.00002648
Iteration 103/1000 | Loss: 0.00002648
Iteration 104/1000 | Loss: 0.00002648
Iteration 105/1000 | Loss: 0.00002648
Iteration 106/1000 | Loss: 0.00002648
Iteration 107/1000 | Loss: 0.00002648
Iteration 108/1000 | Loss: 0.00002648
Iteration 109/1000 | Loss: 0.00002648
Iteration 110/1000 | Loss: 0.00002648
Iteration 111/1000 | Loss: 0.00002648
Iteration 112/1000 | Loss: 0.00002648
Iteration 113/1000 | Loss: 0.00002648
Iteration 114/1000 | Loss: 0.00002648
Iteration 115/1000 | Loss: 0.00002648
Iteration 116/1000 | Loss: 0.00002648
Iteration 117/1000 | Loss: 0.00002648
Iteration 118/1000 | Loss: 0.00002648
Iteration 119/1000 | Loss: 0.00002648
Iteration 120/1000 | Loss: 0.00002648
Iteration 121/1000 | Loss: 0.00002648
Iteration 122/1000 | Loss: 0.00002648
Iteration 123/1000 | Loss: 0.00002648
Iteration 124/1000 | Loss: 0.00002648
Iteration 125/1000 | Loss: 0.00002648
Iteration 126/1000 | Loss: 0.00002648
Iteration 127/1000 | Loss: 0.00002648
Iteration 128/1000 | Loss: 0.00002648
Iteration 129/1000 | Loss: 0.00002648
Iteration 130/1000 | Loss: 0.00002648
Iteration 131/1000 | Loss: 0.00002648
Iteration 132/1000 | Loss: 0.00002648
Iteration 133/1000 | Loss: 0.00002648
Iteration 134/1000 | Loss: 0.00002648
Iteration 135/1000 | Loss: 0.00002648
Iteration 136/1000 | Loss: 0.00002648
Iteration 137/1000 | Loss: 0.00002648
Iteration 138/1000 | Loss: 0.00002648
Iteration 139/1000 | Loss: 0.00002648
Iteration 140/1000 | Loss: 0.00002648
Iteration 141/1000 | Loss: 0.00002648
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 141. Stopping optimization.
Last 5 losses: [2.6479629013920203e-05, 2.6479629013920203e-05, 2.6479629013920203e-05, 2.6479629013920203e-05, 2.6479629013920203e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.6479629013920203e-05

Optimization complete. Final v2v error: 4.41391658782959 mm

Highest mean error: 5.2120466232299805 mm for frame 165

Lowest mean error: 3.7846992015838623 mm for frame 203

Saving results

Total time: 44.16422152519226
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0021/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0021.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0021
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00608130
Iteration 2/25 | Loss: 0.00115546
Iteration 3/25 | Loss: 0.00106094
Iteration 4/25 | Loss: 0.00104326
Iteration 5/25 | Loss: 0.00103618
Iteration 6/25 | Loss: 0.00103395
Iteration 7/25 | Loss: 0.00103366
Iteration 8/25 | Loss: 0.00103366
Iteration 9/25 | Loss: 0.00103366
Iteration 10/25 | Loss: 0.00103366
Iteration 11/25 | Loss: 0.00103366
Iteration 12/25 | Loss: 0.00103366
Iteration 13/25 | Loss: 0.00103366
Iteration 14/25 | Loss: 0.00103366
Iteration 15/25 | Loss: 0.00103366
Iteration 16/25 | Loss: 0.00103366
Iteration 17/25 | Loss: 0.00103366
Iteration 18/25 | Loss: 0.00103366
Iteration 19/25 | Loss: 0.00103366
Iteration 20/25 | Loss: 0.00103366
Iteration 21/25 | Loss: 0.00103366
Iteration 22/25 | Loss: 0.00103366
Iteration 23/25 | Loss: 0.00103366
Iteration 24/25 | Loss: 0.00103366
Iteration 25/25 | Loss: 0.00103366

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 2.75427747
Iteration 2/25 | Loss: 0.00063397
Iteration 3/25 | Loss: 0.00063397
Iteration 4/25 | Loss: 0.00063397
Iteration 5/25 | Loss: 0.00063397
Iteration 6/25 | Loss: 0.00063397
Iteration 7/25 | Loss: 0.00063397
Iteration 8/25 | Loss: 0.00063397
Iteration 9/25 | Loss: 0.00063397
Iteration 10/25 | Loss: 0.00063397
Iteration 11/25 | Loss: 0.00063397
Iteration 12/25 | Loss: 0.00063397
Iteration 13/25 | Loss: 0.00063397
Iteration 14/25 | Loss: 0.00063397
Iteration 15/25 | Loss: 0.00063397
Iteration 16/25 | Loss: 0.00063397
Iteration 17/25 | Loss: 0.00063397
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0006339661194942892, 0.0006339661194942892, 0.0006339661194942892, 0.0006339661194942892, 0.0006339661194942892]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0006339661194942892

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00063397
Iteration 2/1000 | Loss: 0.00002968
Iteration 3/1000 | Loss: 0.00002188
Iteration 4/1000 | Loss: 0.00001994
Iteration 5/1000 | Loss: 0.00001909
Iteration 6/1000 | Loss: 0.00001847
Iteration 7/1000 | Loss: 0.00001815
Iteration 8/1000 | Loss: 0.00001798
Iteration 9/1000 | Loss: 0.00001781
Iteration 10/1000 | Loss: 0.00001780
Iteration 11/1000 | Loss: 0.00001773
Iteration 12/1000 | Loss: 0.00001772
Iteration 13/1000 | Loss: 0.00001771
Iteration 14/1000 | Loss: 0.00001759
Iteration 15/1000 | Loss: 0.00001756
Iteration 16/1000 | Loss: 0.00001750
Iteration 17/1000 | Loss: 0.00001745
Iteration 18/1000 | Loss: 0.00001741
Iteration 19/1000 | Loss: 0.00001741
Iteration 20/1000 | Loss: 0.00001740
Iteration 21/1000 | Loss: 0.00001740
Iteration 22/1000 | Loss: 0.00001739
Iteration 23/1000 | Loss: 0.00001735
Iteration 24/1000 | Loss: 0.00001735
Iteration 25/1000 | Loss: 0.00001734
Iteration 26/1000 | Loss: 0.00001734
Iteration 27/1000 | Loss: 0.00001731
Iteration 28/1000 | Loss: 0.00001731
Iteration 29/1000 | Loss: 0.00001731
Iteration 30/1000 | Loss: 0.00001730
Iteration 31/1000 | Loss: 0.00001730
Iteration 32/1000 | Loss: 0.00001730
Iteration 33/1000 | Loss: 0.00001730
Iteration 34/1000 | Loss: 0.00001729
Iteration 35/1000 | Loss: 0.00001729
Iteration 36/1000 | Loss: 0.00001728
Iteration 37/1000 | Loss: 0.00001727
Iteration 38/1000 | Loss: 0.00001727
Iteration 39/1000 | Loss: 0.00001727
Iteration 40/1000 | Loss: 0.00001727
Iteration 41/1000 | Loss: 0.00001727
Iteration 42/1000 | Loss: 0.00001727
Iteration 43/1000 | Loss: 0.00001727
Iteration 44/1000 | Loss: 0.00001727
Iteration 45/1000 | Loss: 0.00001726
Iteration 46/1000 | Loss: 0.00001726
Iteration 47/1000 | Loss: 0.00001726
Iteration 48/1000 | Loss: 0.00001726
Iteration 49/1000 | Loss: 0.00001726
Iteration 50/1000 | Loss: 0.00001726
Iteration 51/1000 | Loss: 0.00001725
Iteration 52/1000 | Loss: 0.00001724
Iteration 53/1000 | Loss: 0.00001724
Iteration 54/1000 | Loss: 0.00001723
Iteration 55/1000 | Loss: 0.00001723
Iteration 56/1000 | Loss: 0.00001723
Iteration 57/1000 | Loss: 0.00001723
Iteration 58/1000 | Loss: 0.00001723
Iteration 59/1000 | Loss: 0.00001723
Iteration 60/1000 | Loss: 0.00001723
Iteration 61/1000 | Loss: 0.00001723
Iteration 62/1000 | Loss: 0.00001723
Iteration 63/1000 | Loss: 0.00001723
Iteration 64/1000 | Loss: 0.00001723
Iteration 65/1000 | Loss: 0.00001723
Iteration 66/1000 | Loss: 0.00001723
Iteration 67/1000 | Loss: 0.00001723
Iteration 68/1000 | Loss: 0.00001723
Iteration 69/1000 | Loss: 0.00001723
Iteration 70/1000 | Loss: 0.00001723
Iteration 71/1000 | Loss: 0.00001723
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 71. Stopping optimization.
Last 5 losses: [1.7227415810339153e-05, 1.7227415810339153e-05, 1.7227415810339153e-05, 1.7227415810339153e-05, 1.7227415810339153e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 1.7227415810339153e-05

Optimization complete. Final v2v error: 3.6089820861816406 mm

Highest mean error: 3.814915895462036 mm for frame 122

Lowest mean error: 3.3515868186950684 mm for frame 0

Saving results

Total time: 31.256792783737183
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0013/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0013.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0013
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.00833041
Iteration 2/25 | Loss: 0.00145668
Iteration 3/25 | Loss: 0.00123965
Iteration 4/25 | Loss: 0.00120280
Iteration 5/25 | Loss: 0.00119409
Iteration 6/25 | Loss: 0.00119329
Iteration 7/25 | Loss: 0.00119329
Iteration 8/25 | Loss: 0.00119329
Iteration 9/25 | Loss: 0.00119329
Iteration 10/25 | Loss: 0.00119329
Iteration 11/25 | Loss: 0.00119327
Iteration 12/25 | Loss: 0.00119327
Iteration 13/25 | Loss: 0.00119327
Iteration 14/25 | Loss: 0.00119327
Iteration 15/25 | Loss: 0.00119327
Iteration 16/25 | Loss: 0.00119327
Iteration 17/25 | Loss: 0.00119327
Iteration 18/25 | Loss: 0.00119327
Iteration 19/25 | Loss: 0.00119327
Iteration 20/25 | Loss: 0.00119327
Iteration 21/25 | Loss: 0.00119327
Iteration 22/25 | Loss: 0.00119327
Iteration 23/25 | Loss: 0.00119327
Iteration 24/25 | Loss: 0.00119327
Iteration 25/25 | Loss: 0.00119327

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.41099858
Iteration 2/25 | Loss: 0.00078161
Iteration 3/25 | Loss: 0.00078158
Iteration 4/25 | Loss: 0.00078158
Iteration 5/25 | Loss: 0.00078158
Iteration 6/25 | Loss: 0.00078157
Iteration 7/25 | Loss: 0.00078157
Iteration 8/25 | Loss: 0.00078157
Iteration 9/25 | Loss: 0.00078157
Iteration 10/25 | Loss: 0.00078157
Iteration 11/25 | Loss: 0.00078157
Iteration 12/25 | Loss: 0.00078157
Iteration 13/25 | Loss: 0.00078157
Iteration 14/25 | Loss: 0.00078157
Iteration 15/25 | Loss: 0.00078157
Iteration 16/25 | Loss: 0.00078157
Iteration 17/25 | Loss: 0.00078157
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 17. Stopping optimization.
Last 5 losses: [0.0007815734716132283, 0.0007815734716132283, 0.0007815734716132283, 0.0007815734716132283, 0.0007815734716132283]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0007815734716132283

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00078157
Iteration 2/1000 | Loss: 0.00005321
Iteration 3/1000 | Loss: 0.00004272
Iteration 4/1000 | Loss: 0.00003863
Iteration 5/1000 | Loss: 0.00003670
Iteration 6/1000 | Loss: 0.00003585
Iteration 7/1000 | Loss: 0.00003467
Iteration 8/1000 | Loss: 0.00003367
Iteration 9/1000 | Loss: 0.00003301
Iteration 10/1000 | Loss: 0.00003260
Iteration 11/1000 | Loss: 0.00003246
Iteration 12/1000 | Loss: 0.00003240
Iteration 13/1000 | Loss: 0.00003238
Iteration 14/1000 | Loss: 0.00003236
Iteration 15/1000 | Loss: 0.00003234
Iteration 16/1000 | Loss: 0.00003233
Iteration 17/1000 | Loss: 0.00003233
Iteration 18/1000 | Loss: 0.00003231
Iteration 19/1000 | Loss: 0.00003225
Iteration 20/1000 | Loss: 0.00003224
Iteration 21/1000 | Loss: 0.00003224
Iteration 22/1000 | Loss: 0.00003223
Iteration 23/1000 | Loss: 0.00003223
Iteration 24/1000 | Loss: 0.00003223
Iteration 25/1000 | Loss: 0.00003222
Iteration 26/1000 | Loss: 0.00003221
Iteration 27/1000 | Loss: 0.00003221
Iteration 28/1000 | Loss: 0.00003219
Iteration 29/1000 | Loss: 0.00003219
Iteration 30/1000 | Loss: 0.00003218
Iteration 31/1000 | Loss: 0.00003218
Iteration 32/1000 | Loss: 0.00003217
Iteration 33/1000 | Loss: 0.00003217
Iteration 34/1000 | Loss: 0.00003217
Iteration 35/1000 | Loss: 0.00003217
Iteration 36/1000 | Loss: 0.00003217
Iteration 37/1000 | Loss: 0.00003217
Iteration 38/1000 | Loss: 0.00003217
Iteration 39/1000 | Loss: 0.00003216
Iteration 40/1000 | Loss: 0.00003216
Iteration 41/1000 | Loss: 0.00003216
Iteration 42/1000 | Loss: 0.00003215
Iteration 43/1000 | Loss: 0.00003215
Iteration 44/1000 | Loss: 0.00003215
Iteration 45/1000 | Loss: 0.00003214
Iteration 46/1000 | Loss: 0.00003214
Iteration 47/1000 | Loss: 0.00003214
Iteration 48/1000 | Loss: 0.00003214
Iteration 49/1000 | Loss: 0.00003214
Iteration 50/1000 | Loss: 0.00003214
Iteration 51/1000 | Loss: 0.00003214
Iteration 52/1000 | Loss: 0.00003214
Iteration 53/1000 | Loss: 0.00003214
Iteration 54/1000 | Loss: 0.00003214
Iteration 55/1000 | Loss: 0.00003213
Iteration 56/1000 | Loss: 0.00003213
Iteration 57/1000 | Loss: 0.00003213
Iteration 58/1000 | Loss: 0.00003213
Iteration 59/1000 | Loss: 0.00003213
Iteration 60/1000 | Loss: 0.00003213
Iteration 61/1000 | Loss: 0.00003213
Iteration 62/1000 | Loss: 0.00003213
Iteration 63/1000 | Loss: 0.00003213
Iteration 64/1000 | Loss: 0.00003213
Iteration 65/1000 | Loss: 0.00003213
Iteration 66/1000 | Loss: 0.00003213
Iteration 67/1000 | Loss: 0.00003213
Iteration 68/1000 | Loss: 0.00003213
Iteration 69/1000 | Loss: 0.00003213
Iteration 70/1000 | Loss: 0.00003213
Iteration 71/1000 | Loss: 0.00003213
Iteration 72/1000 | Loss: 0.00003213
Iteration 73/1000 | Loss: 0.00003213
Iteration 74/1000 | Loss: 0.00003212
Iteration 75/1000 | Loss: 0.00003212
Iteration 76/1000 | Loss: 0.00003212
Iteration 77/1000 | Loss: 0.00003212
Iteration 78/1000 | Loss: 0.00003212
Iteration 79/1000 | Loss: 0.00003212
Iteration 80/1000 | Loss: 0.00003212
Iteration 81/1000 | Loss: 0.00003212
Iteration 82/1000 | Loss: 0.00003212
Iteration 83/1000 | Loss: 0.00003212
Iteration 84/1000 | Loss: 0.00003212
Iteration 85/1000 | Loss: 0.00003212
Iteration 86/1000 | Loss: 0.00003212
Iteration 87/1000 | Loss: 0.00003212
Iteration 88/1000 | Loss: 0.00003212
Iteration 89/1000 | Loss: 0.00003212
Iteration 90/1000 | Loss: 0.00003212
Iteration 91/1000 | Loss: 0.00003212
Iteration 92/1000 | Loss: 0.00003212
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 92. Stopping optimization.
Last 5 losses: [3.21238003380131e-05, 3.21238003380131e-05, 3.21238003380131e-05, 3.21238003380131e-05, 3.21238003380131e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 3.21238003380131e-05

Optimization complete. Final v2v error: 4.850759506225586 mm

Highest mean error: 5.01039981842041 mm for frame 173

Lowest mean error: 4.498002052307129 mm for frame 238

Saving results

Total time: 34.432849407196045
Loading the SMPL Parameters /is/cluster/sbhor/smpl_ground_truth_corr/male_37_us_0525/0005/motion_seq.npz
Created directory /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0005.
Output directory: /is/cluster/fast/sbhor/star_bedlam_bomoto/male_37_us_0525/0005
Using device: cuda:0
Loaded SMPL model from /is/cluster/sbhor/SMPL_models_SMPLIFYX/smpl/SMPL_NEUTRAL.pkl
Loaded STAR model from <_io.BufferedReader name='/is/cluster/sbhor/STAR/star_1_1/star/STAR_NEUTRAL.npz'>
Processing batch 1/1

Performing pose optimization using an edge loss

Iteration 1/25 | Loss: 0.01148058
Iteration 2/25 | Loss: 0.00173462
Iteration 3/25 | Loss: 0.00144190
Iteration 4/25 | Loss: 0.00128773
Iteration 5/25 | Loss: 0.00118046
Iteration 6/25 | Loss: 0.00117914
Iteration 7/25 | Loss: 0.00112375
Iteration 8/25 | Loss: 0.00114664
Iteration 9/25 | Loss: 0.00107462
Iteration 10/25 | Loss: 0.00107052
Iteration 11/25 | Loss: 0.00105075
Iteration 12/25 | Loss: 0.00104601
Iteration 13/25 | Loss: 0.00104523
Iteration 14/25 | Loss: 0.00104367
Iteration 15/25 | Loss: 0.00104734
Iteration 16/25 | Loss: 0.00103985
Iteration 17/25 | Loss: 0.00103786
Iteration 18/25 | Loss: 0.00103703
Iteration 19/25 | Loss: 0.00103654
Iteration 20/25 | Loss: 0.00103652
Iteration 21/25 | Loss: 0.00103492
Iteration 22/25 | Loss: 0.00103468
Iteration 23/25 | Loss: 0.00103463
Iteration 24/25 | Loss: 0.00103462
Iteration 25/25 | Loss: 0.00103462

Performing global translation and orientation optimization using a vertex loss

Iteration 1/25 | Loss: 1.42552924
Iteration 2/25 | Loss: 0.00063799
Iteration 3/25 | Loss: 0.00057699
Iteration 4/25 | Loss: 0.00057699
Iteration 5/25 | Loss: 0.00057699
Iteration 6/25 | Loss: 0.00057699
Iteration 7/25 | Loss: 0.00057699
Iteration 8/25 | Loss: 0.00057699
Iteration 9/25 | Loss: 0.00057699
Iteration 10/25 | Loss: 0.00057699
Iteration 11/25 | Loss: 0.00057699
Iteration 12/25 | Loss: 0.00057699
Iteration 13/25 | Loss: 0.00057699
Iteration 14/25 | Loss: 0.00057699
Iteration 15/25 | Loss: 0.00057699
Low loss delta threshold (1e-12) for 5 consecutive iterations reached at iteration 15. Stopping optimization.
Last 5 losses: [0.0005769921699538827, 0.0005769921699538827, 0.0005769921699538827, 0.0005769921699538827, 0.0005769921699538827]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 0.0005769921699538827

Optimizing all parameters using a vertex loss

Iteration 1/1000 | Loss: 0.00057699
Iteration 2/1000 | Loss: 0.00010115
Iteration 3/1000 | Loss: 0.00003245
Iteration 4/1000 | Loss: 0.00010061
Iteration 5/1000 | Loss: 0.00078453
Iteration 6/1000 | Loss: 0.00002766
Iteration 7/1000 | Loss: 0.00008340
Iteration 8/1000 | Loss: 0.00002637
Iteration 9/1000 | Loss: 0.00002564
Iteration 10/1000 | Loss: 0.00002524
Iteration 11/1000 | Loss: 0.00002496
Iteration 12/1000 | Loss: 0.00002477
Iteration 13/1000 | Loss: 0.00002461
Iteration 14/1000 | Loss: 0.00002460
Iteration 15/1000 | Loss: 0.00002454
Iteration 16/1000 | Loss: 0.00002449
Iteration 17/1000 | Loss: 0.00002448
Iteration 18/1000 | Loss: 0.00002448
Iteration 19/1000 | Loss: 0.00002446
Iteration 20/1000 | Loss: 0.00002445
Iteration 21/1000 | Loss: 0.00002443
Iteration 22/1000 | Loss: 0.00002443
Iteration 23/1000 | Loss: 0.00002442
Iteration 24/1000 | Loss: 0.00002442
Iteration 25/1000 | Loss: 0.00005442
Iteration 26/1000 | Loss: 0.00005276
Iteration 27/1000 | Loss: 0.00002452
Iteration 28/1000 | Loss: 0.00002440
Iteration 29/1000 | Loss: 0.00002438
Iteration 30/1000 | Loss: 0.00002438
Iteration 31/1000 | Loss: 0.00002438
Iteration 32/1000 | Loss: 0.00002438
Iteration 33/1000 | Loss: 0.00002438
Iteration 34/1000 | Loss: 0.00002437
Iteration 35/1000 | Loss: 0.00002437
Iteration 36/1000 | Loss: 0.00002437
Iteration 37/1000 | Loss: 0.00003276
Iteration 38/1000 | Loss: 0.00002535
Iteration 39/1000 | Loss: 0.00002545
Iteration 40/1000 | Loss: 0.00002453
Iteration 41/1000 | Loss: 0.00002514
Iteration 42/1000 | Loss: 0.00002435
Iteration 43/1000 | Loss: 0.00002435
Iteration 44/1000 | Loss: 0.00002435
Iteration 45/1000 | Loss: 0.00002435
Iteration 46/1000 | Loss: 0.00002435
Iteration 47/1000 | Loss: 0.00002435
Iteration 48/1000 | Loss: 0.00002435
Iteration 49/1000 | Loss: 0.00002435
Iteration 50/1000 | Loss: 0.00002435
Iteration 51/1000 | Loss: 0.00002435
Iteration 52/1000 | Loss: 0.00002435
Iteration 53/1000 | Loss: 0.00002435
Iteration 54/1000 | Loss: 0.00002435
Iteration 55/1000 | Loss: 0.00002435
Iteration 56/1000 | Loss: 0.00002434
Iteration 57/1000 | Loss: 0.00002434
Iteration 58/1000 | Loss: 0.00002434
Iteration 59/1000 | Loss: 0.00002434
Iteration 60/1000 | Loss: 0.00002434
Iteration 61/1000 | Loss: 0.00002434
Iteration 62/1000 | Loss: 0.00002434
Iteration 63/1000 | Loss: 0.00002434
Iteration 64/1000 | Loss: 0.00002434
Iteration 65/1000 | Loss: 0.00002434
Iteration 66/1000 | Loss: 0.00002434
Iteration 67/1000 | Loss: 0.00002434
Iteration 68/1000 | Loss: 0.00002434
Iteration 69/1000 | Loss: 0.00002434
Iteration 70/1000 | Loss: 0.00002434
Iteration 71/1000 | Loss: 0.00002434
Iteration 72/1000 | Loss: 0.00002434
Iteration 73/1000 | Loss: 0.00002434
Iteration 74/1000 | Loss: 0.00002434
Iteration 75/1000 | Loss: 0.00002434
Low loss delta threshold (1e-15) for 5 consecutive iterations reached at iteration 75. Stopping optimization.
Last 5 losses: [2.4343884433619678e-05, 2.4343884433619678e-05, 2.4343884433619678e-05, 2.4343884433619678e-05, 2.4343884433619678e-05]
Last 5 loss deltas: [0.0, 0.0, 0.0, 0.0, 0.0]
Final loss: 2.4343884433619678e-05

Optimization complete. Final v2v error: 4.221776008605957 mm

Highest mean error: 10.129722595214844 mm for frame 132

Lowest mean error: 3.8179731369018555 mm for frame 14

Saving results

Total time: 71.32733988761902
